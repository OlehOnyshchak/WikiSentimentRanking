{
  "pages": [
    {
      "title": "Multi-armed bandit",
      "url": "https://en.wikipedia.org/wiki/Multi-armed_bandit",
      "text": "[[File:Las Vegas slot machines.jpg|thumb|right|A row of slot machines in Las Vegas]]\n\nIn [[probability theory]], the '''multi-armed bandit problem''' (sometimes called the '''''K''-<ref name=\"doi10.1023/A:1013689704352\">{{Cite journal | last1 = Auer | first1 = P. | last2 = Cesa-Bianchi | first2 = N. | last3 = Fischer | first3 = P. | journal = Machine Learning | volume = 47 | issue = 2/3 | pages = 235–256 | year = 2002 |title=Finite-time Analysis of the Multiarmed Bandit Problem| doi = 10.1023/A:1013689704352 | pmid =  | pmc = }}</ref> or ''N''-armed bandit problem'''<ref>{{Cite journal | last1 = Katehakis | first1 = M. N. | last2 = Veinott | first2 = A. F. | doi = 10.1287/moor.12.2.262 | title = The Multi-Armed Bandit Problem: Decomposition and Computation | journal = Mathematics of Operations Research | volume = 12 | issue = 2 | pages = 262–268 | year = 1987 | pmid =  | pmc = }}</ref>) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.<ref name=\"Gittins89\" /><ref name=\"BF\"/> This is a classic [[reinforcement learning]] problem that exemplifies the exploration-exploitation tradeoff dilemma. The name comes from imagining a [[gambler]] at a row of [[slot machines]] (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.<ref name=\"weber\">{{citation\n | last = Weber | first = Richard\n | issue = 4\n | journal = [[Annals of Applied Probability]]\n | pages = 1024–1033\n | title = On the Gittins index for multiarmed bandits\n | volume = 2\n | year = 1992\n | jstor=2959678\n | doi = 10.1214/aoap/1177005588}}</ref> The multi-armed bandit problem also falls into the broad category of [[stochastic scheduling]].\n\nIn the problem, each machine provides a random reward from a [[probability distribution]] specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls.<ref name=\"Gittins89\"/><ref Name=\"BF\"/> The crucial tradeoff the gambler faces at each trial is between \"exploitation\" of the machine that has the highest expected payoff and \"exploration\" to get more [[Bayes' theorem|information]] about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in [[machine learning]]. In practice, multi-armed bandits have been used to model problems such as managing research projects in a large organization like a science foundation or a [[Pharmaceutical industry|pharmaceutical company]].<ref name=\"Gittins89\" /><ref name=\"BF\"/> In early versions of the problem, the gambler begins with no initial knowledge about the machines.\n\n[[Herbert Robbins]] in 1952, realizing the importance of the problem, constructed convergent population selection strategies in \"some aspects of the sequential design of experiments\".<ref>{{Cite journal | last1 = Robbins | first1 = H. | title = Some aspects of the sequential design of experiments | doi = 10.1090/S0002-9904-1952-09620-8 | journal = Bulletin of the American Mathematical Society | volume = 58 | issue = 5 | pages = 527–535 | year = 1952 | pmid =  | pmc = }}</ref> A theorem, the [[Gittins index]], first published by [[John C. Gittins]], gives an optimal policy for maximizing the expected discounted reward.<ref>{{cite journal | author = J. C. Gittins | authorlink = John C. Gittins | year = 1979 | title = Bandit Processes and Dynamic Allocation Indices | journal = Journal of the Royal Statistical Society. Series B (Methodological)  | volume = 41 | issue = 2 | pages = 148–177 | doi =  | jstor = 2985029 | url =  | format =  | accessdate = }}</ref>\n\n==Empirical motivation==\n[[File:The Jet Propulsion Laboratory (9416811752).jpg|thumb|How to distribute a given budget among these research departments to maximize results?]]\nThe multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called \"exploration\") and optimize their decisions based on existing knowledge (called \"exploitation\"). The agent attempts to balance these competing tasks in order to maximize their total value over the period of time considered. There are many practical applications of the bandit model, for example:\n\n* [[clinical trial]]s investigating the effects of different experimental treatments while minimizing patient losses,<ref name=\"Gittins89\" /><ref name=\"BF\"/><ref name=\"WHP\"/><ref name=\"KD\">Press (1986)</ref>\n* [[adaptive routing]] efforts for minimizing delays in a network,\n* [[Portfolio (finance)|financial portfolio design]]<ref name=\"BrochuHoffmandeFreitas\" /><ref name=\"ShenWangJiangZha\" />\n\nIn these practical examples, the problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the ''exploitation vs. exploration tradeoff'' in [[machine learning]].\n\nThe model has also been used to control dynamic allocation of resources to different projects, answering the question of which project to work on, given uncertainty about the difficulty and payoff of each possibility.<ref name=\"farias2011irrevocable\" />\n\nOriginally considered by Allied scientists in [[World War II]], it proved so intractable that, according to [[Peter Whittle (mathematician)|Peter Whittle]], the problem was proposed to be dropped over [[Germany]] so that German scientists could also waste their time on it.<ref name=\"Whittle79\"/>\n\nThe version of the problem now commonly analyzed was formulated by [[Herbert Robbins]] in 1952.\n\n==The multi-armed bandit model==\nThe multi-armed bandit (short: ''bandit'' or MAB) can be seen as a set of real [[Probability distribution|distributions]] <math>B = \\{R_1, \\dots ,R_K\\}</math>, each distribution being  associated with the rewards delivered by one of the <math>K \\in \\mathbb{N}^+</math> levers. Let <math>\\mu_1, \\dots , \\mu_K</math> be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon <math>H</math> is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state [[Markov decision process]]. The [[Regret (decision theory)|regret]] <math>\\rho</math> after <math>T</math> rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards: \n\n<math>\\rho = T \\mu^* - \\sum_{t=1}^T \\widehat{r}_t</math>,\n\nwhere <math>\\mu^*</math> is the maximal reward mean, <math>\\mu^* = \\max_k \\{ \\mu_k \\}</math>, and <math>\\widehat{r}_t</math> is the reward in round ''t''.\n\nA ''zero-regret strategy'' is a strategy whose average regret per round <math>\\rho / T</math> tends to zero with probability 1 when the number of played rounds tends to infinity.<ref name=\"Vermorel2005\"/> Intuitively, zero-regret strategies are guaranteed to converge to a (not necessarily unique) optimal strategy if enough rounds are played.\n\n==Variations==\nA common formulation is the ''Binary multi-armed bandit'' or ''Bernoulli multi-armed bandit,'' which issues a reward of one with probability <math>p</math>, and otherwise a reward of zero.\n\nAnother formulation of the multi-armed bandit has each arm representing an independent Markov machine. Each time a particular arm is played, the state of that machine advances to a new one, chosen according to the Markov state evolution probabilities. There is a reward depending on the current state of the machine. In a generalisation called the \"restless bandit problem\", the states of non-played arms can also evolve over time.<ref name=\"Whittle88\"/> There has also been discussion of systems where the number of choices (about which arm to play) increases over time.<ref name=\"Whittle81\"/>\n\nComputer science researchers have studied multi-armed bandits under worst-case assumptions, obtaining algorithms to minimize regret in both finite and infinite ([[asymptotic]]) time horizons for both stochastic<ref name=\"doi10.1023/A:1013689704352\"/> and non-stochastic<ref>{{Cite journal | last1 = Auer | first1 = P. | last2 = Cesa-Bianchi | first2 = N. | last3 = Freund | first3 = Y. | last4 = Schapire | first4 = R. E. | title = The Nonstochastic Multiarmed Bandit Problem | doi = 10.1137/S0097539701398375 | journal = [[SIAM Journal on Computing|SIAM J. Comput.]] | volume = 32 | issue = 1 | pages = 48–77 | year = 2002 | pmid =  | pmc = | citeseerx = 10.1.1.130.158 }}</ref> arm payoffs.\n\n==Bandit strategies==\nA major breakthrough was the construction of optimal population selection strategies, or policies (that possess uniformly maximum convergence rate  to the population with highest mean) in the work described below.\n\n===Optimal solutions===\n<!-- [[File:1966-HerbertRobbins.jpg|thumb|Herbert Robbins]] -->\nIn the paper \"Asymptotically efficient adaptive allocation rules\", Lai and Robbins<ref>{{cite journal | last1 = Lai | first1 = T.L. | last2 = Robbins | first2 = H. | year = 1985 | title = Asymptotically efficient adaptive allocation rules | url = | journal = Advances in Applied Mathematics | volume = 6 | issue = 1| pages =4–22 | doi = 10.1016/0196-8858(85)90002-8  }}</ref>  (following papers of Robbins and his co-workers going back to Robbins in the year 1952) constructed convergent population selection policies that possess the fastest rate of convergence (to the population with highest mean) for the case that the population reward distributions are the one-parameter exponential family.  Then, in [[Michael Katehakis|Katehakis]] and [[Herbert Robbins|Robbins]]<ref>{{cite journal | last1 = Katehakis | first1 = M.N. | last2 = Robbins | first2 = H.  | year = 1995 | title = Sequential choice from several populations | url = | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 92 | issue = 19| pages =8584–5 | doi = 10.1073/pnas.92.19.8584 | pmid = 11607577 | pmc = 41010  | bibcode = 1995PNAS...92.8584K}}</ref> simplifications of the policy and the main proof were given for the case of normal populations with known variances. The next notable progress was obtained by Burnetas and [[Michael Katehakis|Katehakis]]  in the paper \"Optimal adaptive policies for sequential allocation problems\",<ref>{{cite journal | last1 = Burnetas | first1 = A.N. | last2 = Katehakis | first2 = M.N. | year = 1996 | title = Optimal adaptive policies for sequential allocation problems | url = | journal = Advances in Applied Mathematics | volume = 17 | issue = 2| pages =122–142 | doi = 10.1006/aama.1996.0007  }}</ref> where index based policies  with uniformly maximum convergence rate were constructed, under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters. Burnetas and Katehakis (1996) also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary (i.e., non-parametric) discrete, univariate distributions.\n\nLater in \"Optimal adaptive policies for Markov decision processes\"<ref>{{cite journal | last1 = Burnetas | first1 = A.N. | last2 = Katehakis | first2 = M.N. | year = 1997 | title = Optimal adaptive policies for Markov decision processes | url = | journal = Math. Oper. Res. | volume = 22 | issue = 1| pages =222–255 | doi = 10.1287/moor.22.1.222  }}</ref>  Burnetas and Katehakis studied the much larger model of Markov Decision Processes under partial information,  where the transition law and/or the expected one period rewards may depend on unknown parameters. In this work the explicit form for a class of adaptive policies that possess uniformly maximum convergence rate  properties for the total expected finite horizon reward, were constructed under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of these policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. These inflations have recently been called the optimistic approach in the work of Tewari and Bartlett,<ref>{{cite journal | last1 = Tewari | first1 = A. | last2 = Bartlett | first2 = P.L. | year = 2008 | title = Optimistic linear programming gives logarithmic regret for irreducible MDPs | url = http://books.nips.cc/papers/files/nips20/NIPS2007_0673.pdf | journal = Advances in Neural Information Processing Systems | volume = 20 | issue = | pages =  | citeseerx=10.1.1.69.5482 }}</ref> Ortner<ref>{{cite journal | last1 = Ortner | first1 = R. | year = 2010 | title = Online regret bounds for Markov decision processes with deterministic transitions | url = | journal = Theoretical Computer Science | volume = 411 | issue = 29| pages =2684–2695 | doi = 10.1016/j.tcs.2010.04.005  }}</ref> Filippi,  Cappé, and Garivier,<ref>Filippi, S. and Cappé, O. and Garivier, A. (2010). \"Online regret bounds for Markov decision processes with deterministic transitions\", ''Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on'', pp. 115–122</ref> and Honda and Takemura.<ref>{{cite journal | last1=Honda | first1= J.|last2= Takemura  | first2= A. |year=2011|title=An asymptotically optimal policy for finite support models in the multi-armed bandit problem|journal=Machine Learning|volume=85|issue=3|pages= 361–391 | arxiv=0905.2776 |doi=10.1007/s10994-011-5257-4}}</ref>\n\n===Approximate solutions===\nMany strategies exist which provide an approximate solution to the bandit problem, and can be put into the four broad categories detailed below.\n\n====Semi-uniform strategies====\nSemi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a [[Greedy algorithm|greedy]] behavior where the ''best'' lever (based on previous observations) is always pulled except when a (uniformly) random action is taken.\n\n* '''Epsilon-greedy strategy''':<ref>Sutton, R. S. & Barto, A. G. 1998 Reinforcement learning: an introduction. Cambridge, MA: MIT Press.</ref> The best lever is selected for a proportion <math>1 - \\epsilon</math> of the trials, and a lever is selected at random (with uniform probability) for a proportion <math>\\epsilon</math>. A typical parameter value might be <math>\\epsilon = 0.1</math>, but this can vary widely depending on circumstances and predilections.\n* '''Epsilon-first strategy'''{{Citation needed|date=March 2015}}: A pure exploration phase is followed by a pure exploitation phase. For <math>N</math> trials in total, the exploration phase occupies <math>\\epsilon N</math> trials and the exploitation phase <math>(1 - \\epsilon) N</math> trials. During the exploration phase, a lever is randomly selected (with uniform probability); during the exploitation phase, the best lever is always selected.\n* '''Epsilon-decreasing strategy'''{{Citation needed|date=March 2015}}: Similar to the epsilon-greedy strategy, except that the value of <math>\\epsilon</math> decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.\n* '''Adaptive epsilon-greedy strategy based on value differences (VDBE)''': Similar to the epsilon-decreasing strategy, except that  epsilon is reduced on basis of the learning progress instead of manual tuning (Tokic, 2010).<ref name=\"Tokic2010\"/> High fluctuations in the value estimates lead to a high epsilon (high exploration, low exploitation); low fluctuations to a low epsilon (low exploration, high exploitation). Further improvements can be achieved by a [[softmax function|softmax]]-weighted action selection in case of exploratory actions (Tokic & Palm, 2011).<ref name=\"TokicPalm2011\"/>\n* '''Contextual-Epsilon-greedy strategy''': Similar to the epsilon-greedy strategy, except that the value of <math>\\epsilon</math> is computed regarding the situation in experiment processes, which let the algorithm be Context-Aware. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which situation is most relevant for exploration or exploitation, resulting in highly explorative behavior when the situation is not critical and highly exploitative behavior at critical situation.<ref name=\"Bouneffouf2012\"/>\n\n====Probability matching strategies====\nProbability matching strategies reflect the idea that the number of pulls for a given lever should ''match'' its actual probability of being the optimal lever.  Probability matching strategies are also known as [[Thompson sampling]] or Bayesian Bandits,<ref name=\"Scott2010\"/> and are surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative.\n\nProbability matching strategies also admit solutions to so-called contextual bandit problems.\n\n====Pricing strategies====\nPricing strategies establish a ''price'' for each lever. For example, as illustrated with the POKER algorithm,<ref name=\"Vermorel2005\"/> the price can be the sum of the expected reward plus an estimation of extra future rewards that will gain through the additional knowledge. The lever of highest price is always pulled.\n\n====Strategies with ethical constraints====\nThese strategies minimize the assignment of any patient to an inferior arm ([[Medical ethics|\"physician's duty\"]]).  In a typical case, they minimize expected successes lost (ESL), that is, the expected number of favorable outcomes that were missed because of assignment to an arm later proved to be inferior.  Another version minimizes resources wasted on any inferior, more expensive, treatment.<ref name=\"WHP\" />\n\n==Contextual bandit==\nA particularly useful version of the multi-armed bandit is the contextual multi-armed bandit problem. In this problem, in each iteration an agent has to choose between arms. Before making the choice, the agent sees a d-dimensional feature vector (context vector),\nassociated with the current iteration. The learner uses these context vectors along with the rewards of the arms played in the past to make the choice of the arm to play in\nthe current iteration. Over time, the learner's aim is to collect enough information about how the context vectors and rewards relate to each other, so that it can predict the next best arm to play by looking at the feature vectors.<ref name=\"Langford2008\" />\n\n===Approximate solutions for contextual bandit===\nMany strategies exist that provide an approximate solution to the contextual bandit problem, and can be put into two broad categories detailed below.\n\n====Online linear classifier====\n* '''LinUCB ''(Upper Confidence Bound)'' algorithm''': the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.\n*'''LinRel (Linear Associative Reinforcement Learning) algorithm''': Similar to LinUCB, but utilizes [[Singular-value decomposition]] rather than [[Ridge regression]] to obtain a better estimate of confidence.<ref>{{Cite book|last=Auer|first=P.|title=Using upper confidence bounds for online learning|journal=Proceedings 41st Annual Symposium on Foundations of Computer Science|pages=270–279|publisher=IEEE Comput. Soc|doi=10.1109/sfcs.2000.892116|isbn=978-0769508504|year=2000}}</ref><ref>{{Cite book|last=Hong|first=Tzung-Pei|last2=Song|first2=Wei-Ping|last3=Chiu|first3=Chu-Tien|date=November 2011|title=Evolutionary Composite Attribute Clustering|journal=2011 International Conference on Technologies and Applications of Artificial Intelligence|publisher=IEEE|doi=10.1109/taai.2011.59|isbn=9781457721748}}</ref>\n\n====Online non-linear classifier====\n* '''UCBogram algorithm''': The nonlinear reward functions are estimated using a piecewise constant estimator called a ''regressogram'' in [[Nonparametric regression]]. Then, UCB is employed on each constant piece. Successive refinements of the partition of the context space are scheduled or chosen adaptively.<ref name=\"RigZee10\"/><ref name=\"slivkins11\"/><ref name=\"PerRig13\"/>\n* '''NeuralBandit algorithm''':  In this algorithm several neural networks are trained to modelize the value of rewards knowing the context, and it uses a multi-experts approach to choose online the parameters of multi-layer perceptrons.<ref name=\"Robin2014\"/>\n* '''KernelUCB algorithm''': a kernelized non-linear version of linearUCB, with efficient implementation and finite-time analysis.<ref name=\"Valko2014\"/>\n* '''Bandit Forest algorithm''': a random forest is built and analyzed w.r.t the random forest built knowing the joint distribution of contexts and rewards.<ref>{{Cite journal|last=Féraud|first=Raphaël|last2=Allesiardo|first2=Robin|last3=Urvoy|first3=Tanguy|last4=Clérot|first4=Fabrice|date=2016|title=Random Forest for the Contextual Bandit Problem|url=http://jmlr.org/proceedings/papers/v51/feraud16.html|journal=Aistats|pages=93–101|doi=|pmid=|access-date=}}</ref>\n\n===Constrained contextual bandit===\nIn practice, there is usually a cost associated with the resource consumed by each action and the total cost is limited by a budget in many applications such as crowdsourcing and clinical trials. Constrained contextual bandit (CCB) is such a model that considers both the time and budget constraints in a multi-armed bandit setting.\nA. Badanidiyuru et al.<ref name=\"Badanidiyuru2014COLT\"/> first studied contextual bandits with budget constraints, also referred to as Resourceful Contextual Bandits, and show that a <math>O(\\sqrt{T})</math> regret is achievable. However, their work focuses on a finite set of policies, and the algorithm is computationally inefficient.\n\n[[File:Framework of UCB-ALP for Constrained Contextual Bandits.jpg|thumbnail|Framework of UCB-ALP for constrained contextual bandits]]\nA simple algorithm with logarithmic regret is proposed in:<ref name=\"Wu2015UCBALP\"/>\n* '''UCB-ALP algorithm''': The framework of UCB-ALP is shown in the right figure. UCB-ALP is a simple algorithm that combines the UCB method with an Adaptive Linear Programming (ALP) algorithm, and can be easily deployed in practical systems. It is the first work that show how to achieve logarithmic regret in constrained contextual bandits. Although<ref name=\"Wu2015UCBALP\"/> is devoted to a special case with single budget constraint and fixed cost, the results shed light on the design and analysis of algorithms for more general CCB problems.\n\n==Adversarial bandit==\nAnother variant of the multi-armed bandit problem is called the adversarial bandit, first introduced by Auer and Cesa-Bianchi (1998). In this variant, at each iteration, an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm. This is one of the strongest generalizations of the bandit problem<ref>Burtini, Giuseppe, Jason Loeppky, and Ramon Lawrence. \"A survey of online experiment design with the stochastic multi-armed bandit.\" arXiv preprint arXiv:1510.00757 (2015).</ref> as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems.\n\n===Example: iterated prisoner's dilemma===\nAn example often considered for adversarial bandits is the [[iterated prisoner's dilemma]]. In this example, each adversary has two arms to pull. They can either Deny or Confess. Standard stochastic bandit algorithms don't work very well with this iterations. For example, if the opponent cooperates in the first 100 rounds, defects for the next 200, then cooperate in the following 300, etc. Then algorithms such as UCB won't be able to react very quickly to these changes. This is because after a certain point sub-optimal arms are rarely pulled to limit exploration and focus on exploitation. When the environment changes the algorithm is unable to adapt or may not even detect the change.\n\n===Approximate solutions===\n====Exp3<ref>Seldin, Y., Szepesvári, C., Auer, P. and Abbasi-Yadkori, Y., 2012, December. Evaluation and Analysis of the Performance of the EXP3 Algorithm in Stochastic Environments. In EWRL (pp. 103–116).</ref>====\n=====Algorithm=====\n  '''Parameters:''' Real <math>\\gamma \\in (0,1] </math>\n  \n  '''Initialisation:''' <math>\\omega_i(1) = 1</math> for <math>i = 1,...,K</math>\n  \n  '''For each''' t = 1, 2, ..., T\n   1. Set <math>p_i(t) = (1 - \\gamma)\\frac{\\omega_i(t)}{\\sum_{j=1}^{K}\\omega_j(t)} + \\frac{\\gamma}{K}</math>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <math>i = 1,...,K</math>\n   2. Draw <math>i_t</math> randomly according to the probabilities <math>p_1(t),...,p_K(t)</math>\n   3. Receive reward <math>x_{i_t}(t) \\in [0,1]</math>\n   4. For <math>j = 1,...,K</math> set:\n   &nbsp;&nbsp;&nbsp;&nbsp;<math>\\hat{x}_j(t) = \\begin{cases}x_j(t)/p_j(t) & \\text{if }j = i_t \\\\0, & \\text{otherwise}\\end{cases}</math>\n  \n   &nbsp;&nbsp;&nbsp;&nbsp;<math>\\omega_j(t+1) = \\omega_j(t) exp(\\gamma\\hat{x}_j(t)/K)</math>\n\n=====Explanation=====\nExp3 chooses an arm at random with probability <math>(1 - \\gamma)</math> it prefers arms with higher weights (exploit), it chooses with probability &gamma; to uniformly randomly explore. After receiving the rewards the weights are updated. The exponential growth significantly increases the weight of good arms.\n\n=====Regret analysis=====\nThe (external) regret of the Exp3 algorithm is at most\n<math>O(\\sqrt{KTlog(K)})</math>\n\n====Follow the perturbed leader (FPL) algorithm====\n\n=====Algorithm=====\n  '''Parameters:''' Real <math>\\eta</math>\n  \n  '''Initialisation:''' <math>\\forall i: R_i(1) = 0</math>\n  \n  '''For each''' t = 1,2,...,T\n   1. For each arm generate a random noise from an exponential distribution <math>\\forall i : Z_i(t) \\sim Exp(\\eta)</math>\n   2. Pull arm <math>I(t)</math>: <math>I(t)=arg\\max_i\\{R_i(t) + Z_i(t)\\}</math>\n      Add noise to each arm and pull the one with the highest value\n   3. Update value: <math>R_{I(t)}(t+1) = R_{I(t)}(t) + x_{I(t)}(t)</math>\n      The rest remains the same\n\n=====Explanation=====\nWe follow the arm that we think has the best performance so far adding exponential noise to it to provide exploration.<ref>Hutter, M. and Poland, J., 2005. Adaptive online prediction by following the perturbed leader. Journal of Machine Learning Research, 6(Apr), pp.639–660.</ref>\n\n====Exp3 vs FPL====\n{| class=\"wikitable\"\n|-\n! Exp3 !! FPL\n|-\n| Maintains weights for each arm to calculate pulling probability || Doesn’t need to know the pulling probability per arm\n|-\n| Has efficient theoretical guarantees || The standard FPL does not have good theoretical guarantees\n|-\n| Might be computationally expensive (calculating the exponential terms) || Computationally quite efficient\n|}\n\n==Infinite-armed bandit==\nIn the original specification and in the above variants, the bandit problem is specified with a discrete and finite number of arms, often indicated by the variable <math>K</math>. In the infinite armed case, introduced by Agarwal (1995), the \"arms\" are a continuous variable in <math>K</math> dimensions.\n\n== Non-stationary bandit ==\nGarivier and Moulines derive some of the first results with respect to bandit problems where the underlying model can change during play. A number of algorithms were presented to deal with this case, including Discounted UCB<ref>Discounted UCB, Levente Kocsis, Csaba Szepesvári, 2006</ref> and Sliding-Window UCB.<ref>On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems, Garivier and Moulines, 2008 <https://arxiv.org/abs/0805.3415></ref>\n\nAnother work by Burtini et al. introduces a weighted least squares Thompson sampling approach (WLS-TS), which proves beneficial in both the known and unknown non-stationary cases <ref>Improving Online Marketing Experiments with Drifting Multi-armed Bandits, Giuseppe Burtini, Jason Loeppky, Ramon Lawrence, 2015 <http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=Dx2xXEB0PJE=&t=1></ref>. In the known non-stationary case, the authors in <ref name=\"AUCBDB\"/> produce an alternative solution, a variant of UCB named Adjusted Upper Confidence Bound (A-UCB) which assumes a stochastic model and provide upper-bounds of the regret.\n\n==Other variants==\nMany variants of the problem have been proposed in recent years. \n\n===Dueling bandit===\nThe dueling bandit variant was introduced by Yue et al. (2012)<ref name=\"YueEtAll2012\"/> to model the exploration-versus-exploitation tradeoff for relative feedback.\nIn this variant the gambler is allowed to pull two levers at the same time, but they only get a binary feedback telling which lever provided the best reward. The difficulty of this problem stems from the fact that the gambler has no way of directly observing the reward of their actions.\nThe earliest algorithms for this problem are InterleaveFiltering,<ref name=\"YueEtAll2012\"/> Beat-The-Mean.<ref name=\"Yue2011ICML:BTM\"/>\nThe relative feedback of dueling bandits can also lead to [[voting paradoxes]]. A solution is to take the [[Condorcet winner]] as a reference.<ref name = \"Urvoy2013ICML:SAVAGE\"/>\n\nMore recently, researchers have generalized algorithms from traditional MAB to dueling bandits: Relative Upper Confidence Bounds (RUCB),<ref name=\"Zoghi2014ICML:RUCB\"/> Relative EXponential weighing (REX3),<ref name=\"Gajane2015ICML:REX3\"/> \nCopeland Confidence Bounds (CCB),<ref name=\"Zoghi2015NIPS:CDB\"/> Relative Minimum Empirical Divergence (RMED),<ref name=\"Komiyama2015COLT:DB\"/> and Double Thompson Sampling (DTS).<ref name=\"Wu2016DTS\"/>\n\n===Collaborative bandit===\nThe collaborative filtering bandits (i.e., COFIBA) was introduced by Li and Karatzoglou and Gentile (SIGIR 2016),<ref name=\"LKG2016COFIBA\"/> where the classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, they investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings.<ref name=\"GLZ2014CLUB\"/> Their algorithm (COFIBA, pronounced as \"Coffee Bar\") takes into account the collaborative effects<ref name=\"LKG2016COFIBA\"/> that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. They provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. They also provide a regret analysis within a standard linear stochastic noise setting.\n\n===Combinatorial bandit===\nThe Combinatorial Multiarmed Bandit (CMAB) problem<ref name=\"gai2010learning\"/><ref name=\"chen2013combinatorial\"/><ref name=\"ontanon2017combinatorial\"/> arises when instead of a single discrete variable to choose from, an agent needs to choose values for a set of variables. Assuming each variable is discrete, the number of possible choices per iteration is exponential in the number of variables. Several CMAB settings have been studied in the literature, from settings where the variables are binary<ref name=\"chen2013combinatorial\"/> to more general setting where each variable can take an arbitrary set of values.<ref name=\"ontanon2017combinatorial\"/>\n\n==See also==\n* [[Gittins index]]&nbsp;– a powerful, general strategy for analyzing bandit problems.\n* [[Greedy algorithm]]\n* [[Optimal stopping]]\n* [[Search theory]]\n* [[Stochastic scheduling]]\n\n==References==\n<references>\n\n<ref name=\"slivkins11\">\n{{citation\n | last   = Slivkins\n | first  =  Aleksandrs\n | series = Conference on Learning Theory, COLT 2011\n | title  = Contextual bandits with similarity information.\n | year   = 2011\n}}\n</ref>\n\n<ref name=\"RigZee10\">\n{{citation\n | last1  = Rigollet\n | first1 = Philippe\n | last2  = Zeevi\n | first2 = Assaf\n | series = Conference on Learning Theory, COLT 2010\n | title  = Nonparametric Bandits with Covariates\n | year   = 2010\n}}\n</ref>\n\n<ref name=\"PerRig13\">\n{{citation\n | last1   = Perchet\n | first1  =  Vianney\n | last2   = Rigollet\n | first2  = Philippe\n | journal = [[Annals of Statistics]]\n | title  = The multi-armed bandit problem with covariates\n | volume = 41\n | issue  = 2\n | year   = 2013\n | doi=10.1214/13-aos1101\n | pages=693–721\n| arxiv=1110.6084\n }}\n</ref>\n\n<ref name=\"Valko2014\">\n{{citation\n | author1 = Michal Valko\n | author2 = Nathan Korda\n | author3 = Rémi Munos\n | author4 = Ilias Flaounas\n | author5 = Nello Cristianini\n | series  = 29th Conference on Uncertainty in Artificial Intelligence (UAI 2013) and (JFPDA 2013).\n | title   = Finite-Time Analysis of Kernelised Contextual Bandits\n | arxiv = 1309.6869| year    = 2013\n| bibcode= 2013arXiv1309.6869V\n }}\n</ref>\n\n<ref name=\"Gittins89\">\n{{citation\n | last        = Gittins\n | first       = J. C.\n | author-link = John C. Gittins\n | isbn        = 978-0-471-92059-5\n | location    = Chichester\n | publisher   = John Wiley & Sons, Ltd.\n | series      = Wiley-Interscience Series in Systems and Optimization.\n | title       = Multi-armed bandit allocation indices\n | year        = 1989\n}}\n</ref>\n\n<ref name=\"BF\">\n{{citation\n | last1        = Berry\n | first1       = Donald A.\n | author1-link = Don Berry (statistician)\n | last2        = Fristedt\n | first2       = Bert\n | isbn         = 978-0-412-24810-8\n | location     = London\n | publisher    = Chapman & Hall\n | series       = Monographs on Statistics and Applied Probability\n | title        = Bandit problems: Sequential allocation of experiments\n | year         = 1985\n}}\n</ref>\n\n<ref name=\"Whittle79\">\n{{citation\n | last        = Whittle\n | first       = Peter\n | author-link = Peter Whittle (mathematician)\n | journal     = [[Journal of the Royal Statistical Society]]\n | series      = Series B\n | title       = Discussion of Dr Gittins' paper\n | volume      = 41\n | issue       = 2\n | pages       = 148–177\n | year        = 1979\n | jstor       = 2985029\n}}\n</ref>\n\n<ref name=\"Whittle81\">\n{{citation\n | last        = Whittle\n | first       = Peter\n | author-link = Peter Whittle (mathematician)\n | doi         = 10.1214/aop/1176994469\n | journal     = Annals of Probability\n | pages       = 284–292\n | title       = Arm-acquiring bandits\n | volume      = 9\n | year        = 1981\n | issue       = 2\n}}\n</ref>\n\n<ref name=\"Whittle88\">\n{{citation\n | last        = Whittle\n | first       = Peter\n | author-link = Peter Whittle (mathematician)\n | mr          = 974588\n | journal     = Journal of Applied Probability\n | pages       = 287–298\n | title       = Restless bandits: Activity allocation in a changing world\n | volume      = 25A\n | year        = 1988\n | doi=10.2307/3214163\n| jstor       = 3214163\n }}\n</ref>\n\n<ref name=\"WHP\">\n{{Citation\n | first      = William H.\n | last       = Press\n | year       = 2009\n | url        = http://www.pnas.org/content/106/52/22387\n | title      = Bandit solutions provide unified ethical models for randomized clinical trials and comparative effectiveness research\n | journal    = Proceedings of the National Academy of Sciences\n | volume     = 106\n | pages      = 22387–22392\n | pmid       = 20018711\n | doi        = 10.1073/pnas.0912378106\n | issue      = 52\n | pmc        = 2793317\n | postscript = .\n| bibcode= 2009PNAS..10622387P\n }}\n</ref>\n\n<ref name=\"Scott2010\">\n{{citation\n | last    = Scott\n | first   = S.L.\n | doi     = 10.1002/asmb.874\n | journal = Applied Stochastic Models in Business and Industry\n | pages   = 639–658\n | title   = A modern Bayesian look at the multi-armed bandit\n | volume  = 26\n | year    = 2010\n | issue   = 2\n}}\n</ref>\n\n<ref name=\"Vermorel2005\">\n{{citation\n | url       = http://bandit.sourceforge.net/Vermorel2005poker.pdf\n | last1     = Vermorel\n | first1    = Joannes\n | last2     = Mohri\n | first2    = Mehryar\n | publisher = Springer\n | series    = In European Conference on Machine Learning\n | pages     = 437–448\n | title     = Multi-armed bandit algorithms and empirical evaluation\n | year      = 2005\n}}\n</ref>\n\n<ref name=\"Robin2014\">\n{{citation\n | last1        = Allesiardo\n | first1       = Robin\n | last2        = Féraud\n | first2       = Raphaël\n | last3        = Djallel\n | first3       = Bouneffouf\n | contribution = A Neural Networks Committee for the Contextual Bandit Problem\n | pages        = 374–381\n | publisher    = Springer\n | series       = [[Lecture Notes in Computer Science]]\n | title        = Neural Information Processing – 21st International Conference, ICONIP 2014, Malaisia, November 03-06,2014, Proceedings\n | volume       = 8834\n | year         = 2014\n | isbn         = 978-3-319-12636-4\n | doi=10.1007/978-3-319-12637-1_47\n| arxiv= 1409.8191\n }}\n</ref>\n\n<ref name=\"Bouneffouf2012\">\n{{Cite book | last1 = Bouneffouf | first1 = D. | last2 = Bouzeghoub | first2 = A. | last3 = Gançarski | first3 = A. L. | doi = 10.1007/978-3-642-34487-9_40 | chapter = A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System | title = Neural Information Processing | series = Lecture Notes in Computer Science | volume = 7665 | pages = 324 | year = 2012 | isbn = 978-3-642-34486-2 | pmid =  | pmc = }}\n</ref>\n\n<ref name=\"Tokic2010\">\n{{citation\n | last1     = Tokic\n | first1    = Michel\n | chapter   = Adaptive ε-greedy exploration in reinforcement learning based on value differences\n | doi       = 10.1007/978-3-642-16111-7_23\n | pages     = 203–210\n | publisher = Springer-Verlag\n | series    = Lecture Notes in Computer Science\n | title     = KI 2010: Advances in Artificial Intelligence\n | volume    = 6359\n | year      = 2010\n | chapter-url       = http://www.tokic.com/www/tokicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf\n | isbn      = 978-3-642-16110-0| citeseerx    = 10.1.1.458.464\n }}.\n</ref>\n\n<ref name=\"TokicPalm2011\">\n{{citation\n | last1     = Tokic\n | first1    = Michel\n | last2     = Palm\n | first2    = Günther\n | chapter   = Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax\n | pages     = 335–346\n | publisher = Springer-Verlag\n | series    = Lecture Notes in Computer Science\n | title     = KI 2011: Advances in Artificial Intelligence\n | volume    = 7006\n | year      = 2011\n | chapter-url       = http://www.tokic.com/www/tokicm/publikationen/papers/KI2011.pdf\n | isbn      = 978-3-642-24455-1}}.\n</ref>\n\n<ref name=\"BrochuHoffmandeFreitas\">\n{{citation\n | last1  = Brochu\n | first1 = Eric\n | last2  = Hoffman\n | first2 = Matthew W.\n | last3  = de Freitas\n | first3 = Nando\n | arxiv    = 1009.5419| date   = September 2010\n | title  = Portfolio Allocation for Bayesian Optimization| bibcode= 2010arXiv1009.5419B\n }}\n</ref>\n\n<ref name=\"ShenWangJiangZha\">\n{{citation\n | last1  = Shen\n | first1 = Weiwei\n | last2  = Wang\n | first2 = Jun\n | last3  = Jiang\n | first3 = Yu-Gang\n | last4  = Zha \n | first4 = Hongyuan\n | journal = Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI2015)\n | url    = http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPDFInterstitial/10972/10798\n | date   = 2015\n | title  = Portfolio Choices with Orthogonal Bandit Learning}}\n</ref>\n<ref name=\"Langford2008\">\n{{citation\n | chapter-url       = http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information\n | last1     = Langford\n | first1    = John\n | last2     = Zhang\n | first2    = Tong\n | publisher = Curran Associates, Inc.\n | pages     = 817–824\n | title     = Advances in Neural Information Processing Systems 20\n | chapter   = The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits\n | year      = 2008\n}}\n</ref>\n\n<ref name=\"Badanidiyuru2014COLT\">\n{{citation\n | last1     = Badanidiyuru\n | first1    = A.\n | last2     = Langford\n | first2    = J.\n | last3     = Slivkins\n | first3    = A.\n | title     = Proceeding of Conference on Learning Theory (COLT)\n | chapter   = Resourceful contextual bandits\n | year      = 2014\n}}\n</ref>\n\n<ref name=\"Wu2015UCBALP\">\n{{citation\n | url       = https://papers.nips.cc/paper/6008-algorithms-with-logarithmic-or-sublinear-regret-for-constrained-contextual-bandits\n | last1     = Wu\n | first1    = Huasen\n | last2     = Srikant\n | first2    = R.\n | last3     = Liu\n | first3    = Xin\n | last4     = Jiang\n | first4    = Chong\n | title     = Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits\n | journal   = The 29th Annual Conference on Neural Information Processing Systems (NIPS) \n | pages     = 433–441\n | year      = 2015\n}}\n</ref>\n\n<ref name=\"YueEtAll2012\">\n{{citation\n | chapter-url     = http://www.sciencedirect.com/science/article/pii/S0022000012000281\n | last1   = Yue\n | first1  = Yisong\n | last2   = Broder\n | first2  = Josef\n | last3   = Kleinberg\n | first3  = Robert\n | last4   = Joachims\n | first4  = Thorsten\n | volume  = 78\n | issue   = 5\n | pages   = 1538–1556\n | title   = Journal of Computer and System Sciences\n | journal   = Journal of Computer and System Sciences\n | chapter = The K-armed Dueling Bandits Problem\n | year    = 2012\n | doi=10.1016/j.jcss.2011.12.028\n| citeseerx   = 10.1.1.162.2764\n }}\n</ref>\n\n<ref name=\"Yue2011ICML:BTM\">\n{{citation\n | last1   = Yue\n | first1  = Yisong\n | last2   = Joachims\n | first2  = Thorsten\n | title   = Proceedings of ICML'11\n | chapter = Beat the Mean Bandit\n | year    = 2011\n}}\n</ref>\n\n<ref name=\"Urvoy2013ICML:SAVAGE\">\n{{citation\n | chapter-url     = http://www.jmlr.org/proceedings/papers/v28/urvoy13.pdf\n | last1   = Urvoy\n | first1  = Tanguy\n | last2   =  Clérot\n | first2  = Fabrice\n | last3   = Féraud\n | first3  = Raphaël\n | last4   = Naamane  \n | first4  = Sami\n | title   = Proceedings of the 30th International Conference on Machine Learning (ICML-13)\n | chapter = Generic Exploration and K-armed Voting Bandits\n | year    = 2013\n}}\n</ref>\n\n<ref name=\"Zoghi2014ICML:RUCB\">\n{{citation\n | chapter-url     = http://www.jmlr.org/proceedings/papers/v32/zoghi14.pdf\n | last1   = Zoghi\n | first1  = Masrour \n | last2   =  Whiteson\n | first2  = Shimon\n | last3   = Munos\n | first3  = Remi\n | last4   = Rijke  \n | first4  = Maarten D\n | title   = Proceedings of the 31st International Conference on Machine Learning (ICML-14)\n | chapter = Relative Upper Confidence Bound for the $K$-Armed Dueling Bandit Problem\n | year    = 2014\n}}\n</ref>\n\n<ref name=\"Gajane2015ICML:REX3\">\n{{citation\n | chapter-url     = http://jmlr.org/proceedings/papers/v37/gajane15.pdf\n | last1   = Gajane\n | first1  = Pratik\n | last2  = Urvoy\n | first2  = Tanguy\n | last3   =  Clérot\n | first3  = Fabrice\n | title   = Proceedings of the 32nd International Conference on Machine Learning (ICML-15)\n | chapter = A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits\n | year    = 2015\n}}\n</ref>\n\n<ref name=\"Zoghi2015NIPS:CDB\">\n{{citation\n | arxiv     = 1506.00312| last1   = Zoghi\n | first1  = Masrour \n | last2   =  Karnin\n | first2  =  Zohar S\n | last3   = Whiteson\n | first3  =  Shimon\n | last4   =  Rijke \n | first4  = Maarten D\n | title   = Advances in Neural Information Processing Systems, NIPS'15\n | chapter = Copeland Dueling Bandits\n | year    = 2015\n| bibcode=2015arXiv150600312Z}}\n</ref>\n\n<ref name=\"Komiyama2015COLT:DB\">\n{{citation\n | chapter-url     = http://jmlr.org/proceedings/papers/v40/Komiyama15.pdf\n | last1   = Komiyama\n | first1  = Junpei \n | last2   =  Honda\n | first2  =  Junya\n | last3   =  Kashima\n | first3  = Hisashi\n | last4   =  Nakagawa\n | first4  =  Hiroshi\n | title   = Proceedings of the 28th Conference on Learning Theory\n | chapter = Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem\n | year    = 2015\n}}\n</ref>\n\n<ref name=\"Wu2016DTS\">\n{{citation\n | arxiv       = 1604.07101| last1     = Wu\n | first1    = Huasen\n | last2     = Liu\n | first2    = Xin\n | title     = Double Thompson Sampling for Dueling Bandits\n | journal   = The 30th Annual Conference on Neural Information Processing Systems (NIPS) \n | year      = 2016\n| bibcode= 2016arXiv160407101W}}\n</ref>\n\n<ref name=\"AUCBDB\">\n{{citation\n | last1   = Bouneffouf\n | first1  = Djallel\n | last2   = Feraud\n | first2  = Raphael \n | title   = Neurocomputing\n | chapter = Multi-armed bandit problem with known trend\n | year    = 2016\n}}\n</ref>\n\n<ref name=\"GLZ2014CLUB\">\n{{citation\n | arxiv     = 1401.8257| last1   = Gentile\n | first1  = Claudio\n | last2   =  Li\n | first2  =  Shuai \n | last3   =  Zappella \n | first3  = Giovanni \n | title   = The 31st International Conference on Machine Learning, Journal of Machine Learning Research (ICML 2014)\n | chapter = Online Clustering of Bandits\n | year    = 2014\n| bibcode=2014arXiv1401.8257G}}\n</ref>\n\n\n<ref name=\"LKG2016COFIBA\">\n{{citation\n | arxiv     = 1502.03473| last1   = Li\n | first1  = Shuai\n | last2   =  Alexandros\n | first2  =  Karatzoglou \n | last3   =  Gentile\n | first3  = Claudio\n | title   = The 39th International ACM SIGIR Conference on Information Retrieval (SIGIR 2016)\n | chapter = Collaborative Filtering Bandits\n | year    = 2016\n| bibcode=2015arXiv150203473L}}\n</ref>\n\n<ref name=\"farias2011irrevocable\">\n{{citation\n | title=The irrevocable multiarmed bandit problem\n | last1 = Farias | first1 = Vivek F | first2 = Madan | last2 = Ritesh\n | journal=[[Operations Research (journal)|Operations Research]]\n | volume=59\n | number=2\n | pages=383–399\n | year= 2011\n | doi=10.1287/opre.1100.0891\n|citeseerx = 10.1.1.380.6983}}\n</ref>\n\n<ref name=\"gai2010learning\">\n{{citation\n|author=Gai, Y. and Krishnamachari, B. and Jain, R.\n|title=Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation\n|booktitle=2010 IEEE Symposium on New Frontiers in Dynamic Spectrum\n|pages=1–9\n|year=2010\n|url=http://www.academia.edu/download/30758682/DySPAN2010.pdf\n}}\n</ref>\n\n<ref name=\"chen2013combinatorial\">\n{{citation\n|author=Chen, Wei and Wang, Yajun and Yuan, Yang\n|title=Combinatorial multi-armed bandit: General framework and applications\n|booktitle=Proceedings of the 30th International Conference on Machine Learning (ICML 2013)\n|pages=151–159\n|year=2013\n|url=http://www.jmlr.org/proceedings/papers/v28/chen13a.pdf\n}}\n</ref>\n\n<ref name=\"ontanon2017combinatorial\">\n{{citation\n|author=Santiago Ontañón\n|title= Combinatorial Multi-armed Bandits for Real-Time Strategy Games\n|journal= Journal of Artificial Intelligence Research\n|volume=58\n|year=2017\n|pages=665–702\n|url=https://www.jair.org/index.php/jair/article/download/11053/26230\n}}\n</ref>\n\n</references>\n\n==Further reading==\n*{{Cite journal | last1 = Guha | first1 = S. | last2 = Munagala | first2 = K. | last3 = Shi | first3 = P. | title = Approximation algorithms for restless bandit problems | doi = 10.1145/1870103.1870106 | journal = Journal of the ACM | volume = 58 | pages = 1–50 | year = 2010 | pmid =  | pmc = | arxiv = 0711.3861 }}\n*{{citation\n | last1 = Dayanik | first1 = S.\n | last2 = Powell | first2 = W.\n | last3 = Yamazaki | first3 = K.\n | doi = 10.1239/aap/1214950209\n | issue = 2\n | journal = Advances in Applied Probability\n | pages = 377–400\n | title = Index policies for discounted bandit problems with availability constraints\n | volume = 40\n | year = 2008}}.\n*{{citation\n | last = Powell | first = Warren B.\n | contribution = Chapter 10\n | isbn = 978-0-470-17155-4\n | location = New York\n | publisher = John Wiley and Sons\n | title = Approximate Dynamic Programming: Solving the Curses of Dimensionality\n | year = 2007}}.\n*{{citation\n | last = Robbins | first = H. | author-link = Herbert Robbins\n | doi = 10.1090/S0002-9904-1952-09620-8\n | journal = [[Bulletin of the American Mathematical Society]]\n | pages = 527–535\n | title = Some aspects of the sequential design of experiments\n | volume = 58  | year = 1952  | issue = 5}}.\n*{{citation\n |last1       = Sutton\n |first1      = Richard\n |last2       = Barto\n |first2      = Andrew\n |isbn        = 978-0-262-19398-6\n |publisher   = MIT Press\n |title       = Reinforcement Learning\n |url         = http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\n |year        = 1998\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20131211192714/http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\n |archivedate = 2013-12-11\n |df          = \n}}.\n\n*{{citation\n | last = Allesiardo  | first = Robin\n | contribution = A Neural Networks Committee for the Contextual Bandit Problem\n | pages = 374–381\n | publisher = Springer\n | series = Lecture Notes in Computer Science\n | title = Neural Information Processing – 21st International Conference, ICONIP 2014, Malaisia, November 03-06,2014, Proceedings\n | volume = 8834\n | year = 2014\n | isbn = 978-3-319-12636-4\n | doi=10.1007/978-3-319-12637-1_47| arxiv = 1409.8191\n }}.\n\n*{{citation\n | last = Bouneffouf  | first = Djallel\n | contribution = A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System\n | pages = 324–331\n | publisher = Springer\n | series = Lecture Notes in Computer Science\n | title = Neural Information Processing – 19th International Conference, ICONIP 2012, Doha, Qatar, November 12–15,2012, Proceedings, Part III\n | volume = 7665\n | year = 2012\n | isbn = 978-3-642-34486-2\n | doi=10.1007/978-3-642-34487-9_40}}.\n* {{citation\n | last = Weber | first = Richard\n | issue = 4\n | journal = [[Annals of Applied Probability]]\n | pages = 1024–1033\n | title = On the Gittins index for multiarmed bandits\n | volume = 2\n | year = 1992\n | jstor=2959678\n | doi = 10.1214/aoap/1177005588}}.<!-- \"The proof from God\" according to Whittle's survey of applied probability -->\n* {{Citation\n|author=[[Michael N. Katehakis|Katehakis, M.]] and C. Derman\n|title=Computing Optimal Sequential Allocation Rules in Clinical Trials\n|journal=IMS Lecture Notes-Monograph Series\n|volume=8\n|year=1986\n|pages=29–39\n|jstor= 4355518\n|postscript=.\n|doi=10.1214/lnms/1215540286\n|series=Institute of Mathematical Statistics Lecture Notes - Monograph Series\n|isbn=978-0-940600-09-6\n}}\n* {{Citation\n|author=[[Michael N. Katehakis|Katehakis, M.]] and  A. F. Veinott, Jr.\n|title=The multi-armed bandit problem: decomposition and computation\n|journal=Mathematics of Operations Research\n|volume=12\n|year=1987\n|pages=262–268\n|jstor= 3689689\n|issue=2\n|doi=10.1287/moor.12.2.262\n|postscript=.\n}}\n\n==External links==\n*[https://github.com/fmr-llc/mabwiser MABWiser], [[Open-Source|open source]] Python implementation of bandit strategies that supports context-free, parametric and non-parametric contextual policies with built-in parallelization and simulation capability. \n*[http://mloss.org/software/view/415/ PyMaBandits], [[Open-Source|open source]] implementation of bandit strategies in Python and Matlab.\n*[https://github.com/Nth-iteration-labs/contextual Contextual], [[Open source|open source]] [[R (programming language)|R]] package facilitating the simulation and evaluation of both context-free and contextual Multi-Armed Bandit policies.\n*[http://bandit.sourceforge.net bandit.sourceforge.net Bandit project], open source implementation of bandit strategies.\n*[https://github.com/jkomiyama/banditlib Banditlib], [[Open-Source]] implementation of bandit strategies in C++.\n*[https://archive.is/20121212095047/http://www.cs.washington.edu/research/jair/volume4/kaelbling96a-html/node6.html Leslie Pack Kaelbling and Michael L. Littman (1996). Exploitation versus Exploration: The Single-State Case].\n* Tutorial: Introduction to Bandits: Algorithms and Theory. [http://techtalks.tv/talks/54451/ Part1]. [http://techtalks.tv/talks/54455/ Part2].\n* [http://www.feynmanlectures.info/exercises/Feynmans_restaurant_problem.html Feynman's restaurant problem], a classic example (with known answer) of the exploitation vs. exploration tradeoff.\n* [http://www.chrisstucchio.com/blog/2012/bandit_algorithms_vs_ab.html Bandit algorithms vs. A-B testing].\n* [http://homes.di.unimi.it/~cesabian/Pubblicazioni/banditSurvey.pdf S. Bubeck and N. Cesa-Bianchi A Survey on Bandits].\n* [https://arxiv.org/abs/1508.03326 A Survey on Contextual Multi-armed Bandits], a survey/tutorial for Contextual Bandits.\n* [https://mpatacchiola.github.io/blog/2017/08/14/dissecting-reinforcement-learning-6.html Blog post on multi-armed bandit strategies, with Python code].\n*[https://pavlov.tech/2019/03/02/animated-multi-armed-bandit-policies/ Animated, interactive plots] illustrating Epsilon-greedy, [[Thompson sampling]], and Upper Confidence Bound exploration/exploitation balancing strategies.\n\n{{DEFAULTSORT:Multi-Armed Bandit}}\n[[Category:Sequential methods]]\n[[Category:Sequential experiments]]\n[[Category:Stochastic optimization]]\n[[Category:Machine learning]]"
    },
    {
      "title": "Optimal computing budget allocation",
      "url": "https://en.wikipedia.org/wiki/Optimal_computing_budget_allocation",
      "text": "{{Multiple issues|\n{{Underlinked|date=July 2018}}\n{{Orphan|date=October 2013}}\n}}\n\n'''Optimal computing budget allocation (OCBA)''' is a concept first introduced in the mid-1990s by [[Chun-Hung Chen|Dr. Chun-Hung Chen]]. This approach intends to maximize the overall [[simulation]] efficiency for finding an optimal decision.<ref>Fu, M, C. H. Chen, and L. Shi, “[http://simulation.su/uploads/files/default/2008-fu-chun-hung-shi.pdf Some Topics for Simulation Optimization],” Proceedings of 2008 Winter Simulation Conference, pp. 27–38, Miami, FL, December 2008.</ref> Simply put, OCBA is an approach to simulation that will help determine the number of replications and/or the simulation time that is needed in order to receive acceptable or best results within a set of given parameters.<ref>Chen, and Loo H. Lee. [https://books.google.com/books?hl=en&lr=&id=wtVpDQAAQBAJ&oi=fnd&pg=PR5&dq=%22Stochastic+simulation+optimization+an+optimal+computing+budget+allocation%22&ots=evSr8zuuWn&sig=crRw81au6-FEK8S0DeoDzlEFR_w#v=onepage&q=%22Stochastic%20simulation%20optimization%20an%20optimal%20computing%20budget%20allocation%22&f=false Stochastic simulation optimization an optimal computing budget allocation]. Singapore Hackensack, NJ: World Scientific, 2011. Print..</ref> This is accomplished by using an asymptotic framework to analyze the structure of the optimal allocation.<ref>Chen, C. H. \"[https://ieeexplore.ieee.org/abstract/document/478499/ An Effective Approach to Smartly Allocate Computing Budget for Discrete Event Simulation],\" Proceedings of the 34th IEEE Conference on Decision and Control, pp. 2598–2605, December 1995.</ref> OCBA has also been shown effective in enhancing partition-based random [[Search algorithm|search algorithms]] for solving [[Deterministic global optimization|deterministic global optimization]] problems.<ref>Chen, W., S. Gao, C. H. Chen and L. Shi, \"[https://www.researchgate.net/profile/Siyang_Gao2/publication/260721040_An_Optimal_Sample_Allocation_Strategy_for_Partition-Based_Random_Search/links/582584b808aeb45b58927ed5/An-Optimal-Sample-Allocation-Strategy-for-Partition-Based-Random-Search.pdf An Optimal Sample Allocation Strategy for Partition-based Random Search],\" IEEE Transactions on Automation Science and Engineering, 11(1), 177–186, 2014.</ref>\n\n== Intuitive explanation ==\n\nOCBA’s goal is to provide a systematic approach to run a large number of simulations including only the critical alternatives in order to select the best alternative. In other words, it focuses on only part the most critical alternatives, which minimizes computation time and reduces these critical estimators’ variances. The expected result maintains the required level of accuracy, while requiring less amount of work.<ref>{{cite web|last=Chen|first=Chun-Hung|title=Optimal Computing Budget Allocation (OCBA) for Simulation-based Decision Making Under Uncertainty|url=http://seor.gmu.edu/~cchen9/ocba.html|accessdate=9 July 2013|deadurl=yes|archiveurl=https://archive.is/20131001025704/http://seor.gmu.edu/~cchen9/ocba.html|archivedate=1 October 2013|df=}}</ref> For example, we can create a simple simulation between 5 alternatives. The goal is to select an alternative with minimum average delay time. The figure below shows preliminary simulation results ( i.e. having run only a fraction of the required number of simulation replications). It is clear to see that alternative 2 and 3 have a significantly lower delay time ( highlighted in red). In order to save computation cost (which is time, resources and money spend on the process of running the simulation) OCBA suggests that more replications are required for alternative 2 and 3, and simulation can be stopped for 1, 4, and 5 much earlier without compromising results.\n \n[[File:Comparing 5 different alternatives with respect to Cost.png|framed|Observing the above graphic, it is clear that alternative 2 and 3 have the lowest cost. OCBA suggests to run further simulations on only alternatives 2 and 3 in order to minimize computation cost]]\n\n== Problem  ==\n\nThe main objective of OCBA is to maximize the probability of correct selection (PCS). PCS is subject to the sampling budget of a given stage of sampling&nbsp;''τ''.\n\n: <math>\n\\begin{align}\n \\max_{\\tau_1,\\tau_2,\\ldots,\\tau_k} &\\mathrm{ PCS} \\\\\n \\text{subject to } &\\sum_{i=1}^k \\tau_i=\\tau,\\\\\n& \\tau_i \\ge 0, i=1,2,...,k.\\qquad (1)\n\\end{align}\n</math>\n\nIn this case <math>\\sum_{i=1}^k \\tau_i=\\tau</math> stands for the total computational cost.<ref>Chen, and Loo H. Lee. [https://books.google.com/books?hl=en&lr=&id=wtVpDQAAQBAJ&oi=fnd&pg=PR5&dq=%22Stochastic+simulation+optimization+an+optimal+computing+budget+allocation%22&ots=evSr8zuuWn&sig=crRw81au6-FEK8S0DeoDzlEFR_w#v=onepage&q=%22Stochastic%20simulation%20optimization%20an%20optimal%20computing%20budget%20allocation%22&f=false Stochastic simulation optimization an optimal computing budget allocation]. Singapore Hackensack, NJ: World Scientific, 2011. Print.</ref>\n\n== Some extensions of OCBA ==\n\nExperts in the field explain that in some problems it is important to not only know the best alternative among a sample, but the top 5, 10, or even 50, because the decision maker may have other concerns that may affect the decision which are not modeled in the simulation. According to Szechtman and Yücesan (2008),<ref>Szechtman R, Yücesan E (2008) [https://calhoun.nps.edu/bitstream/handle/10945/38580/031.pdf?sequence=3 A new perspective on feasibility determination]. Proc of the 2008 Winter Simul Conf 273–280</ref> OCBA is also helpful in feasibility determination problems. This is where the decisions makers are only interested in differentiating feasible alternatives from the infeasible ones. Further, choosing an alternative that is simpler, yet similar in performance is crucial for other decision makers. In this case, the best choice is among top-r simplest alternatives, whose performance rank above desired levels.<ref>Jia QS, Zhou E, Chen CH (2012).  [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3653338/ efficient computing budget allocation for finding simplest good designs]. IIE Trans, To Appear.</ref> In addition, Trailovic<ref>Trailovic Tekin E, Sabuncuoglu I (2004) Simulation optimization: A comprehensive review on theory and applications. IIE Trans 36:1067–1081</ref> and Pao<ref>Trailovic L, Pao LY (2004) [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.1213&rep=rep1&type=pdf Computing budget allocation for efficient ranking and selection of variances with application to target tracking algorithms], IEEE Trans Autom Control 49:58–67.</ref> (2004) demonstrate an OCBA approach, where we find alternatives with minimum variance, instead of with best mean. Here, we assume unknown variances, voiding the OCBA rule (assuming that the variances are known). During 2010 research was done on an OCBA algorithm that is based on a t distribution. The results show no significant differences between those from t-distribution and normal distribution. The above presented extensions of OCBA is not a complete list and is yet to be fully explored and compiled.<ref name=\"Chen\">Chen, C. H., M. Fu, L. Shi, and L. H. Lee, “Stochastic Systems Simulation Optimization,” Frontiers of Electrical and Electronic Engineering in China, 6(3), 468–480, 2011</ref>\n\n== Multi-objective OCBA ==\n\nMulti-objective Optimal Computing Budget Allocation (MOCBA) is the OCBA concept that applies to multi-objective problems. In a typical MOCBA, the PCS is defined as\n\n<math>\\Pr\\{CS\\} \\equiv \\Pr \\left\\{ \\left( \\bigcap_{i \\in S_p} E_i \\right) \\bigcap \\left( \\bigcap_{i \\in \\overline{S}_p} E_i^c \\right)  \\right\\}, </math>\n\nin which\n* <math>S_p</math> is the observed [[Pareto set]],\n* <math>\\overline{S}_p</math> is the non-Pareto set, i.e., <math>\\overline{S}_p = \\Theta \\backslash S_p</math>,\n* <math>E_i</math> is the event that design <math>i</math> is non-dominated by all other designs,\n* <math>E_i^c</math> is the event that design <math>i</math> is dominated by at least one design.\n\nWe notice that, the Type I error <math>e_1</math> and Type II error <math>e_2</math> for identifying a correct Pareto set are respectively\n\n<math> e_1 = 1 - \\Pr\\left\\{ \\bigcap_{i \\in \\overline{S}_p} E_i^c \\right\\}</math> and <math>e_2 = 1 - \\Pr\\left\\{ \\bigcap_{i \\in S_p} E_i \\right\\}</math>.\n\nBesides, it can be proven that\n\n<math> e_1 \\leq ub_1 = H\\left|\\overline{S}_p\\right| - H\\sum_{i \\in \\overline{S}_p}{\\max_{j\\in\\Theta, j \\neq i}\\left[ \\min_{l \\in {1,\\ldots,H}} \\Pr\\left\\{ \\tilde{J}_{jl} \\leq \\tilde{J}_{il} \\right\\} \\right]}</math>\n\nand\n\n<math> e_2 \\leq ub_2 = (k-1) \\sum_{i \\in S_p}\\max_{j\\in\\Theta,j \\neq i}\\left[ \\min_{l\\in{1,\\ldots,H} } \\Pr\\left\\{ \\tilde{J}_{jl} \\leq \\tilde{J}_{il} \\right\\} \\right],</math>\n\nwhere <math>H</math> is the number of objectives, and <math>\\tilde{J}_{il}</math> follows posterior distribution <math>Normal\\left( \\bar{J}_{il}, \\frac{\\sigma_{il}^2}{N_i} \\right).</math>\nNoted that <math>\\bar{J}_{il}</math> and <math>\\sigma_{il}</math> are the average and standard deviation of the observed performance measures for objective <math>l</math> of design <math>i</math>, and <math>N_i</math> is the number of observations.\n\nThus, instead of maximizing <math>\\Pr\\{CS\\}</math>, we can maximize its lower bound, i.e., <math>APCS{-}M \\equiv 1-ub_1-ub_2.</math> Assuming <math>\\tau\\rightarrow \\infty</math>, the Lagrange method can be applied to conclude the following rules:\n\n<math> \\tau_i = \\frac{\\beta_i}{\\sum_{j\\in\\Theta}\\beta_j} \\tau,</math>\n\nin which\n\n* for a design <math>h\\in S_A</math>, <math>\\beta_h = \\frac{\\left(\\hat{\\sigma}^2_{hl_{j_h}^h} + \\hat{\\sigma}^2_{j_{h}l_{j_h}^h} / \\rho_h\\right) / {\\delta^2_{hj_{h}l_{j_h}^h}}} {\\left( \\hat{\\sigma}^2_{ml_{jm}^m} + \\hat{\\sigma}^2_{j_{m}l_{jm}^m} / \\rho_m \\right) / {\\delta^2_{mj_{m}l_{j_m}^m}}}</math>,\n* for a design <math>d\\in S_B</math>, <math>\\beta_d = \\sqrt{\\sum_{i \\in \\Theta_d^*} \\frac{\\sigma^2_{dl_d^i}}{\\sigma^2_{il_d^i}}\\beta_i^2}</math>,\n\nand <math>\\delta_{ijl} = \\bar{J}_{jl} - \\bar{J}_{il},</math>\n\n<math>j_i \\equiv \\arg \\max_{j\\in\\Theta, j \\neq i} \\prod_{l=1}^{H}{\\Pr\\left\\{ \\tilde{J}_{jl} \\leq \\tilde{J}_{il} \\right\\}},</math>\n\n<math>l_{j_i}^i \\equiv \\arg \\min_{l\\in{1,\\ldots,H}} \\Pr\\left\\{ \\tilde{J}_{jl} \\leq \\tilde{J}_{il} \\right\\},</math>\n\n<math>S_A \\equiv \\left\\{ design \\; h\\in S \\mid \\frac{\\delta^2_{hj_hl^h_{j_h}}}{\\frac{\\hat{\\sigma}^2_{hl^h_{j_h}}}{\\alpha_h}+\\frac{\\hat{\\sigma}^2_{j_hl^h_{j_h}}}{\\alpha_{j_h}}} < \\min_{i\\in \\Theta_h} \\frac{\\delta^2_{ihl^i_h}}{\\frac{\\hat{\\sigma}^2_{il^i_h}}{\\alpha_i}+\\frac{\\hat{\\sigma}^2_{hl^i_h}}{\\alpha_h}} \\right\\},</math>\n\n<math> S_B \\equiv S \\backslash S_A,</math>\n\n<math>\\Theta_h = {i | i\\in S, j_i = h},</math>\n\n<math> \\Theta_d^* = {h | h \\in S_A, j_h = d},</math>\n\n<math>\\rho_i = \\alpha_{j_i} / \\alpha_i.</math>\n\n== Constrained optimization==\nSimilar to the previous section, there are many situations with multiple performance measures. If the multiple performance measures are equally important, the decision makers can use the MOCBA. In other situations, the decision makers have one primary performance measure to be optimized while the secondary performance measures are constrained by certain limits. The primary performance measure can be called the main objective while the secondary performance measures are referred as the constraint measures. This falls into the problem of constrained optimization. When the number of alternatives is fixed, the problem is called constrained ranking and selection where the goal is to select the best feasible design given that both the main objective and the constraint measures need to be estimated via stochastic simulation. The OCBA method for constrained optimization (called OCBA-CO) can be found in Pujowidianto et al. (2009) <ref>Pujowidianto NA, Lee LH, Chen CH, Yap CM (2009) [https://core.ac.uk/download/pdf/48657520.pdf Optimal computing budget allocation for constrained optimization]. Proc of the 2009 Winter Simul Conf 584–589.</ref> and Lee et al. (2012).<ref>Lee LH, Pujowidianto NA, Li LW, Chen CH, Yap CM (2012) [https://ieeexplore.ieee.org/abstract/document/6189041/ Approximation simulation budget allocation for selecting the best design in the presence of stochastic constraints], IEEE Trans Autom Control 57:2940–2945.</ref>\n\nThe key change is in the definition of PCS. There are two components in constrained optimisation, namely optimality and feasibility. As a result, the simulation budget can be allocated to each non-best design either based on the optimality or feasibility. In other word, a non-best design will not be wrongly selected as the best feasible design if it remains either infeasible or worse than the true best feasible design. The idea is that it is not necessary to spend a large portion of the budget to determine the feasibility if the design is clearly worse than the best. Similarly, we can save the budget by allocating based on feasibility if the design is already better than the best in terms of the main objective.\n\n== Feasibility determination==\n\nThe goal of this problem is to determine all the feasible designs from a finite set of design alternatives, where the feasible designs are defined as the designs with their performance measures satisfying specified control requirements\n(constraints). With all the feasible designs selected, the decision maker can easily make the final decision by incorporating other performance considerations (e.g., deterministic criteria, such as cost, or qualitative criteria which are difficult to mathematically evaluate). Although the feasibility determination problem involves stochastic constraints too, it is distinguished from the constrained optimization problems introduced above, in that it aims to identify all the feasible designs instead of the single best feasible one.\n\nDefine\n* <math>k</math>: total number of designs;\n* <math>m</math>: total number of performance measure constraints;\n* <math>c_j</math>: control requirement of the <math>j</math>th constraint for all the designs, <math>j=1,2,...,m</math>;\n* <math>S_A</math>: set of feasible designs;\n* <math>S_B</math>: set of infeasible designs;\n* <math>\\mu_{i,j}</math>: mean of simulation samples of the <math>j</math>th constraint measure and design <math>i</math>;\n* <math>\\sigma_{i,j}^2</math>: variance of simulation samples of the <math>j</math>th constraint measure and design <math>i</math>;\n*<math>\\alpha_i</math>: proportion of the total simulation budget allocated to design <math>i</math>;\n*<math>\\bar{X}_{i,j}</math>: sample mean of simulation samples of the <math>j</math>th constraint measure and design <math>i</math>.\n\nSuppose all the constraints are provided in form of <math>\\mu_{i,j}\\leq c_j</math>, <math>i=1,2,...,k, j=1,2,...,m</math>. The probability of correctly selecting all the feasible designs is\n:<math>\n\\begin{align}\n\\mathrm{PCS}=\\mathbb{P}\\left(\\bigcap_{i\\in S_A}\\Big(\\bigcap_{j=1}^m (\\bar{X}_{i,j}\\leq c_j)\\Big) \\cap \\bigcap_{i\\in S_B}\\Big(\\bigcup_{j=1}^m (\\bar{X}_{i,j}> c_j)\\Big)\\right),\n\\end{align}\n</math>\nand the budget allocation problem for feasibility determination is given by Gao and Chen (2017)<ref>Gao, S. and W. Chen, \"[https://ieeexplore.ieee.org/abstract/document/7426351/ Efficient feasibility determination with multiple performance measure constraints],\" IEEE Transactions on Automatic Control, 62, 113–122, 2017.</ref>\n: <math>\n\\begin{align}\n \\max_{\\alpha_1,\\alpha_2,\\ldots,\\alpha_k} &\\mathrm{ PCS} \\\\\n \\text{subject to } &\\sum_{i=1}^k \\alpha_i =1,\\\\\n &\\alpha_i\\geq 0, i=1,2,...,k.\n\\end{align}\n</math>\n\nLet <math>I_{i,j}(x)=\\frac{(x-\\mu_{i,j})^2}{2\\sigma_{i,j}^2}</math> and <math>j_i\\in\\mathrm{argmin}_{j\\in\\{1,...,m\\}}I_{i,j}(c_j)</math>. The asymptotic optimal budget allocation rule is\n:<math>\n\\begin{align}\n\\frac{\\alpha_i}{\\alpha_{i'}}=\\frac{I_{i',j_{i'}}(c_{j_{i'}})}{I_{i,j_i}(c_{j_i})}, i,i'\\in\\{1,2,...,k\\}.\n\\end{align}\n</math>\n\nIntuitively speaking, the above allocation rule says that (1) for a feasible design, the dominant constraint is the most difficult one to be correctly detected among all the constraints; and (2) for an infeasible design, the dominant constraint is the easiest one to be correctly detected among all constraints.\n\n== OCBA with expected opportunity cost ==\nThe original OCBA maximizes the probability of correct selection (PCS) of the best design. In practice, another important measure is the expected opportunity cost (EOC), which quantifies how far away the mean of the selected design is from that of the real best. This measure is important because optimizing EOC not only maximizes the chance of selecting the best design but also ensures that the mean of the selected design is not too far from that of the best design, if it fails to find the best one. Compared to PCS, EOC penalizes a particularly bad choice more than a slightly incorrect selection, and is thus preferred by risk-neutral practitioners and decision makers.\n\nSpecifically, the expected opportunity cost is\n:<math>\n\\begin{align}\nEOC=\\mathbb{E}_{\\mathcal{T}}[\\mu_{\\mathcal{T}}-\\mu_t]=\\sum_{i=1,i\\neq t}^k \\delta_{i,t}\\mathbb{P}(\\mathcal{T}=i),\n\\end{align}\n</math>\nwhere,\n* <math>k</math> is the total number of designs;\n* <math>t</math> is the real best design;\n* <math>\\mathcal{T}</math> is the random variable whose realization is the observed best design;\n* <math>\\mu_i</math> is the mean of the simulation samples of design <math>i</math>, <math>i=1,2,...,k</math>;\n* <math>\\delta_{i,j}=\\mu_i-\\mu_j</math>.\n\nThe budget allocation problem with the EOC objective measure is given by Gao et al. (2017)<ref>Gao, S., W. Chen and L. Shi, \"[https://pubsonline.informs.org/doi/abs/10.1287/opre.2016.1581 A New Budget Allocation Framework for the Expected Opportunity Cost],\" Operations Research, 63, 787–803, 2017.</ref>\n: <math>\n\\begin{align}\n \\min_{\\alpha_1,\\alpha_2,\\ldots,\\alpha_k} &\\mathrm{ EOC} \\\\\n \\text{subject to } &\\sum_{i=1}^k \\alpha_i =1,\\\\\n &\\alpha_i\\geq 0, i=1,2,...,k,\n\\end{align}\n</math>\nwhere <math>\\alpha_i</math> is the proportion of the total simulation budget allocated to design <math>i</math>.\nIf we assume <math>\\alpha_t \\gg \\alpha_i</math> for all <math>i \\neq t</math>, the asymptotic optimal budget allocation rule for this problem is\n: <math>\n\\begin{align}\n& \\frac{\\alpha_t^2}{\\sigma_t^2}=\\sum_{i=1,i \\neq t}^k \\frac{\\alpha_i^2}{\\sigma_i^2},\\\\\n& \\frac{\\alpha_i}{\\alpha_j}=\\frac{\\sigma_i^2/\\delta_{i,t}^2}{\\sigma_j^2/\\delta_{j,t}^2}, i\\neq j\\neq t,\n\\end{align}\n</math>\nwhere <math>\\sigma_i^2</math> is the variance of the simulation samples of design <math>i</math>. This allocation rule is the same as the asymptotic optimal solution of problem (1). That is, asymptotically speaking, maximizing PCS and minimizing EOC are the same thing.\n\n== OCBA with input uncertainty ==\n\nAn implicit assumption for the aforementioned OCBA methods is that the true input distributions and their parameters are known, while in practice, they are typically unknown and have to be estimated from limited historical data. This may lead to uncertainty in the estimated input distributions and their parameters, which might (severely) affect the quality of the selection. Assuming that the uncertainty set contains a finite number of scenarios for the underlying input distributions and parameters, Gao et al. (2017)<ref>Gao, S., H. Xiao, E. Zhou and W. Chen, \"[https://www.sciencedirect.com/science/article/pii/S0005109817301425 Robust Ranking and Selection with Optimal Computing Budget Allocation],\" Automatica, 81, 30–36, 2017.</ref> introduces a new OCBA approach by maximizing the probability of correctly selecting the best design under a fixed simulation budget, where the performance of a design is measured by its worst-case performance among all the possible scenarios in the uncertainty set.\n\n== Web-based demonstration of OCBA ==\n\nThe following link provides an OCBA demonstration using a simple example. In the demo, you will see how OCBA performs and allocates computing budget differently as compared with traditional equal allocation approach.\n* http://seor.vse.gmu.edu/~cchen9/ocba.html\n\n== References ==\n\n<!--- See [[Wikipedia:Footnotes]] on how to create references using<ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n== External links ==\n* http://seor.vse.gmu.edu/~cchen9/ocba.html\n* [http://carschimp.com/blog/ocba/ Russian translation] of OCBA page by [http://carschimp.com Carschimp]\n* [http://sciposts.com/ocba/ Ukrainian translation] of OCBA page by [http://sciposts.com/ sciposts]\n\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Parallel tempering",
      "url": "https://en.wikipedia.org/wiki/Parallel_tempering",
      "text": "{{context|sate=December 2010|date=December 2010}}\n\n'''Parallel tempering''', also known as '''replica exchange MCMC sampling''', is a [[simulation]] method aimed at improving the dynamic properties of  [[Monte Carlo method]] simulations of physical systems, and of [[Markov chain Monte Carlo]] (MCMC) sampling methods more generally.  The replica exchange method was originally devised by Swendsen and Wang <ref>Swendsen RH and Wang JS (1986) [https://www.researchgate.net/profile/Robert_Swendsen/publication/13255490_Replica_Monte_Carlo_Simulation_of_Spin-Glasses/links/0046352309b5f54715000000.pdf Replica Monte Carlo simulation of spin glasses] Physical Review Letters 57 : 2607–2609</ref> then extended by Geyer<ref>C. J. Geyer, (1991) in ''Computing Science and Statistics'', Proceedings of the 23rd Symposium on the Interface, American Statistical Association, New York, p. 156.</ref> and later developed, among others, by [[Hukushima and Nemoto]],<ref>{{cite journal\n  |title=Exchange Monte Carlo method and application to spin glass simulations\n  |author1=Hukushima, Koji  |author2=Nemoto, Koji\n   |lastauthoramp=yes |journal=J. Phys. Soc. Jpn.\n | volume=65\n  |number=6\n  |pages=1604–1608\n  |year=1996\n|doi=10.1143/JPSJ.65.1604  |arxiv=cond-mat/9512035  }}</ref> [[Giorgio Parisi]],<ref>{{cite journal\n |author1=Marco Falcioni  |author2=Michael W. Deem\n  |lastauthoramp=yes |year=1999\n |title = A Biased Monte Carlo Scheme for Zeolite Structure Solution\n |journal = J. Chem. Phys.\n |volume = 110 |issue = 3 |pages = 1754\n |doi=10.1063/1.477812\n|arxiv = cond-mat/9809085 |bibcode = 1999JChPh.110.1754F }}</ref><ref>David J. Earl and Michael W. Deem (2005) [http://www.rsc.org/Publishing/Journals/CP/article.asp?doi=b509983h \"Parallel tempering: Theory, applications, and new perspectives\"],  ''Phys. Chem. Chem. Phys.'',  7, 3910</ref>\nSugita and Okamoto formulated a [[molecular dynamics]] version of parallel tempering:<ref>{{cite journal\n |author1=Y. Sugita  |author2=Y. Okamoto\n  |lastauthoramp=yes |year=1999\n |title = Replica-exchange molecular dynamics method for protein folding\n |journal = Chemical Physics Letters\n |volume = 314 |issue=1–2\n  |pages = 141–151\n |doi=10.1016/S0009-2614(99)01123-9\n|bibcode = 1999CPL...314..141S }}</ref> this is usually known as replica-exchange molecular dynamics or REMD.\n\nEssentially, one runs ''N'' copies of the system, randomly initialized, at different temperatures. Then, based on the Metropolis criterion  one exchanges configurations at different temperatures. The idea of this method\nis to make configurations at high temperatures available to the simulations at low temperatures and vice versa.\nThis results in a very robust ensemble which is able to sample both low and high energy configurations.\nIn this way, thermodynamical properties such as the specific heat, which is in general not well computed in the canonical ensemble, can be computed with great precision.\n\n==Background==\nTypically a [[Monte Carlo simulation]] using a [[Metropolis–Hastings]] update consists of a single [[stochastic process]] that evaluates the [[energy]] of the system and accepts/rejects updates based on the [[temperature]] ''T''.  At high temperatures updates that change the energy of the system are comparatively more probable.  When the system is highly correlated, updates are rejected and the simulation is said to suffer from critical slowing down.\n\nIf we were to run two simulations at temperatures separated by a Δ''T'', we would find that if Δ''T'' is small enough, then the energy [[histogram]]s obtained by collecting the values of the energies over a set of Monte Carlo steps N will create two distributions that will somewhat overlap. The overlap can be defined by the area of the histograms that falls over the same interval of energy values, normalized by the total number of samples. For Δ''T'' = 0 the overlap should approach 1.\n\nAnother way to interpret this overlap is to say that system configurations sampled at temperature ''T''<sub>1</sub> are likely to appear during a simulation at ''T''<sub>2</sub>. Because the [[Markov chain]] should have no memory of its past, we can create a new update for the system composed of the two systems at ''T''<sub>1</sub> and ''T''<sub>2</sub>.  At a given Monte Carlo step we can update the global system by swapping the configuration of the two systems, or alternatively trading the two temperatures. The update is accepted according to the Metropolis–Hastings criterion with probability\n\n:<math> p = \\min \\left( 1, \\frac{ \\exp \\left( -\\frac{E_j}{kT_i} - \\frac{E_i}{kT_j} \\right) }{ \\exp \\left( -\\frac{E_i}{kT_i} - \\frac{E_j}{kT_j} \\right) } \\right) = \\min \\left( 1, e^{(E_i - E_j) \\left( \\frac{1}{kT_i} - \\frac{1}{kT_j} \\right)} \\right) ,</math>\n\nand otherwise the update is rejected. The [[detailed balance]] condition has to be satisfied by ensuring that the reverse update has to be equally likely, all else being equal. This can be ensured by appropriately choosing regular Monte Carlo updates or parallel tempering updates with probabilities that are independent of the configurations of the two systems or of the Monte Carlo step.<ref>{{cite journal\n|author = Radford M. Neal\n|year=1996\n|title = Sampling from multimodal distributions using tempered transitions\n|journal = Statistics and Computing\n|volume = 6 |issue=4|pages = 353–366\n|doi=10.1007/BF00143556\n}}</ref>\n\nThis update can be generalized to more than two systems.\n\nBy a careful choice of temperatures and number of systems one can achieve an improvement in the mixing properties of a set of Monte Carlo simulations that exceeds the extra computational cost of running parallel simulations.\n\nOther considerations to be made: increasing the number of different temperatures can have a detrimental effect, as one can think of the 'lateral' movement of a given system across temperatures as a diffusion process.\nSet up is important as there must be a practical histogram overlap to achieve a reasonable probability of lateral moves.\n\nThe parallel tempering method can be used as a super [[simulated annealing]] that does not need restart, since a system at high temperature can feed new local optimizers to a system at low temperature, allowing tunneling between metastable states and improving convergence to a global optimum.\n\n== Implementations ==\n*[[Abalone (molecular mechanics)|Abalone]]\n*ACEMD\n*[[AMBER]]\n*[[CHARMM]]\n*[[Desmond (software)|Desmond]]\n*[[GROMACS]]\n*[[LAMMPS]]\n*[[Orac (MD program)|Orac]]\n\n== References ==\n<references/>\n\n{{DEFAULTSORT:Parallel Tempering}}\n[[Category:Markov chain Monte Carlo]]\n[[Category:Molecular dynamics]]\n[[Category:Heuristics]]\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Robbins' problem",
      "url": "https://en.wikipedia.org/wiki/Robbins%27_problem",
      "text": "{{about|Robbins' problem of optimal stopping|the Robbins problem on Boolean algebras|Robbins algebra}}\n\nIn [[probability theory]], '''Robbins' problem of [[optimal stopping]]''', named after [[Herbert Robbins]], is sometimes referred to as the fourth [[secretary problem]] or the problem of minimizing the [[expected value|expected]] rank with full information.  Its statement is as follows.\n\nLet ''X''<sub>1</sub>, ... , ''X''<sub>''n''</sub> be independent, identically distributed [[random variable]]s, [[continuous uniform distribution|uniform]] on [0,&nbsp;1]. We observe the ''X''<sub>''k''</sub>'s sequentially and must stop on exactly one of them. No recall of preceding observations is permitted. What stopping rule minimizes the expected rank of the selected observation, and what is its corresponding value?\n\nThe general solution to this  full-information expected rank problem is unknown. The major difficulty is that the problem is fully history-dependent, that is, the optimal rule depends at every stage on all preceding values, and not only on simpler sufficient statistics of these. Only bounds are known for the limiting value ''v'' as ''n'' goes to infinity, namely 1.908&nbsp;<&nbsp;''v''&nbsp;<&nbsp;2.329. It is known that there is some room to improve the lower bound by further computations for a truncated \nversion of the problem. It is still not known how to improve on the upper bound which stems from the subclass of memoryless threshold rules.\n\n==Importance==\nOne of the motivations to study Robbins' problem is that with its solution all classical (four) [[secretary problem]]s would be solved. But the major reason is to understand how to cope with full history dependence in a (deceptively easy-looking) problem.\nOn the Ester's Book International Conference in Israel (2006) Robbins' problem was accordingly named one of the four most important problems in the field of [[optimal stopping]] and [[sequential analysis]].\n\n==History ==\n\n[[Herbert Robbins]] presented the above described problem at the [[International Conference on Search and Selection in Real Time]] in [[Amherst, Massachusetts|Amherst]], 1990. He concluded his address with the words ''I should like to see this problem solved before I die''. Scientists working in the field of optimal stopping have since called this problem ''Robbins' problem''.\n==References==\n* {{cite journal|last1=Chow|first1=Y.S.|last2=Moriguti|first2=S.|last3=Robbins|first3=H.| last4=Samuels|first4=S.M.|title=Optimal Selection Based on Relative Rank|journal=Israel Journal of Mathematics|year=1964|volume=2|issue=2|pages=81–90|doi=10.1007/bf02759948}}\n* \"Minimizing the expected rank with full information\", [[F. Thomas Bruss]] and Thomas S. Ferguson, ''Journal of Applied Probability'' ''Volume'' '''30''', #1 (1993), pp.&nbsp;616&ndash;626\n* Half-Prophets and Robbins' Problem of Minimizing the expected rank,  F. T. Bruss and T. S. Ferguson, ''Springer Lecture Notes in Statistics'' ''Volume'' '''1''' in honor of J.M. Gani, (1996), pp.&nbsp;1&ndash;17\n* \"The secretary problem; minimizing the expected rank with i.i.d. random variables\",  D. Assaf and E. Samuel-Cahn, ''Adv. Appl. Prob.'' ''Volume'' '''28''', (1996), pp.&nbsp;828&ndash;852 [http://cat.inist.fr/?aModele=afficheN&cpsidt=3259597 Cat.Inist]\n* \"What is known about Robbins' Problem?\" [[F. Thomas Bruss]], ''Journal of Applied Probability'' ''Volume'' '''42''', #1 (2005), pp.&nbsp;108&ndash;120 [https://web.archive.org/web/20110607164808/http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.jap%2F1110381374 Euclid]\n* \"A continuous-time approach to Robbins' problem of minimizing the expected rank\", [[F. Thomas Bruss]] and Yves Caoimhin Swan, ''Journal of Applied Probability'' ''Volume 46'' #1, 1&ndash;18, (2009).\n\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Scenario optimization",
      "url": "https://en.wikipedia.org/wiki/Scenario_optimization",
      "text": "The '''scenario approach''' or '''scenario optimization approach''' is a technique for obtaining solutions to [[robust optimization]] and [[chance-constrained optimization]] problems based on a sample of the [[constraint (mathematics)|constraint]]s. It also relates to [[inductive reasoning]] in modeling and decision-making. The technique has existed for decades as a [[heuristic]] approach and has more recently been given a systematic theoretical foundation.\n\n== Description ==\n\nIn [[Optimization (mathematics)|optimization]], robustness features translate into constraints that are parameterized by the uncertain elements of the problem. In the scenario method<ref>G. Calafiore and M.C. Campi. ''Uncertain Convex Programs: Randomized Solutions and Confidence Levels.'' Mathematical Programming, 102: 25&ndash;46, 2005. [http://www.springerlink.com/content/qlcbr9eg3dne6ldb/?p=ad759ab4ef9f49049f9b5ef14e7b00ee&pi=1]</ref><ref>G. Calafiore and M.C. Campi. \"The scenario approach to robust control design,\" IEEE Transactions on Automatic Control, 51(5). 742-753, 2006. [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1632303&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1632303]</ref><ref>M.C. Campi and S. Garatti. ''The Exact Feasibility of Randomized Solutions of Uncertain Convex Programs.'' SIAM J. on Optimization, 19, no.3: 1211&ndash;1230, 2008.[http://epubs.siam.org/siopt/resource/1/sjope8/v19/i3/p1211_s1]</ref>, a solution is obtained by only looking at a random sample of constraints ([[heuristic]] approach) called ''scenarios'' and a deeply-grounded theory tells the user how “robust” the corresponding solution is related to other constraints. This theory justifies the use of [[randomization]] in robust and chance-constrained optimization. \n<!-- Deleted image removed: [[Image:risk-return.jpg|thumb|Figure 1: risk-return trade-off]] -->\n\n== Data-driven optimization ==\nAt times, scenarios are obtained as random extractions from a model. More often, however, scenarios are instances of the uncertain constraints that are obtained as observations ([[data-driven science]]). In this latter case, no model of uncertainty is needed to generate scenarios. Moreover, most remarkably, also in this case scenario optimization comes accompanied by a full-fledged theory because all scenario optimization results are distribution-free and can therefore be applied even when a model of uncertainty is not available.\n\n== Theoretical results ==\nFor constraints that are [[convex optimization|convex]] (e.g. in [[semidefinite programming|semidefinite problems]] involving [[Linear matrix inequality|LMIs, Linear Matrix Inequalities]]), a deep theoretical analysis has been established which shows that the probability that a new constraint is not satisfied follows a distribution that is dominated by a [[Beta distribution]]. This result is tight since it is exact for a whole class of convex problems.<ref>M.C. Campi and S. Garatti. ''The Exact Feasibility of Randomized Solutions of Uncertain Convex Programs.'' SIAM J. on Optimization, 19, no.3: 1211&ndash;1230, 2008.[http://epubs.siam.org/siopt/resource/1/sjope8/v19/i3/p1211_s1]</ref> More generally, various empirical levels have been shown to follow a [[Dirichlet distribution]], whose marginals are beta distribution.<ref> A. Caré, S. Garatti and M.C. Campi.''Scenario min-max optimization and the risk of empirical costs ''. SIAM Journal on Optimization, 25, no.4: 2061-2080, 2015, Mathematical Programming, published online, 2016. [http://epubs.siam.org/doi/abs/10.1137/130928546]</ref> The scenario approach with <math>L_1</math> regularization has also been considered<ref> M.C. Campi and A. Carè. ''Random convex programs with L1-regularization: sparsity and generalization''. SIAM Journal on Control and Optimization, 51, no.5: 3532-3557, 2013. [http://epubs.siam.org/doi/abs/10.1137/110856204?journalCode=sjcodc]</ref>, and handy algorithms with reduced computational complexity are available.<ref> A. Caré, S. Garatti and M.C. Campi. ''FAST - Fast Algorithm for the Scenario Technique''. Operations Research, 62, no.3: 662-671, 2014. [https://pubsonline.informs.org/doi/abs/10.1287/opre.2014.1257]</ref> Extensions to more complex, non-convex, set-ups are still objects of active investigation.\n\nAlong the scenario approach, it is also possible to pursue a risk-return trade-off.<ref>M.C. Campi and S. Garatti. ''A Sampling-and-Discarding Approach to Chance-Constrained Optimization: Feasibility and Optimality''. Journal of Optimization Theory and Applications, 148, Number 2, 257&ndash;280, 2011 (preliminary version published in Optimization Online, 2008). [http://www.springerlink.com/content/a0146681n6878894/]</ref><ref>G.C. Calafiore. ''Random Convex Programs''. SIAM J. on Optimization, 20(6) 3427-3464, 2010. [http://epubs.siam.org/doi/abs/10.1137/090773490]</ref>  Moreover, a full-fledged method can be used to apply this approach to control.<ref>S. Garatti and M.C. Campi. ''Modulating Robustness in Control Design: Principles and Algorithms.''. IEEE Control Systems Magazine, 33, 36&#x2013;51, 2013. [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6479421&filter%3DAND%28p_IS_Number%3A6479381%29]</ref> First <math>N</math> constraints are sampled and then the user starts removing some of the constraints in succession. This can be done in different ways, even according to greedy algorithms. After elimination of one more constraint, the optimal solution is updated, and the corresponding optimal value is determined. As this procedure moves on, the user constructs an empirical “curve of values”, i.e. the curve representing the value achieved after the removing of an increasing number of constraints. The scenario theory provides precise evaluations of how robust the various solutions are. \n\nA remarkable advance in the theory has been established by the recent wait-and-judge approach<ref>M.C. Campi and S. Garatti. ''Wait-and-judge scenario optimization.''. Mathematical Programming, published online, 2016. [https://link.springer.com/article/10.1007/s10107-016-1056-9?view=classic]</ref>: one assesses the complexity of the solution (as precisely defined in the referenced article) and from its value formulates precise evaluations on the robustness of the solution. These results shed light on deeply-grounded links between the concepts of complexity and risk. A related approach, named \"Repetitive Scenario Design\" aims at reducing the sample complexity of the solution by repeatedly alternating a scenario design phase (with reduced number of samples) with a randomized check of the feasibility of the ensuing solution.<ref>G.C. Calafiore. ''Repetitive Scenario Design.''  IEEE Transactions on Automatic Control, Vol. 62, Issue 3, March 2017, pp. 1125-1137. [http://ieeexplore.ieee.org/document/7484339/]</ref>\n\n== Example ==\n\nConsider a function <math>R_\\delta(x)</math> which represents the return of an [[investment]]; it depends on our vector of investment choices <math>x</math> and on the market state <math>\\delta</math> which will be experienced at the end of the investment period.\n\nGiven a stochastic model for the market conditions, we consider <math>N</math> of the possible states <math>\\delta_1, \\dots, \\delta_N</math> (randomization of uncertainty). Alternatively, the scenarios <math>\\delta_i</math> can be obtained from a record of observations. \n\nWe set out to solve the scenario optimization program\n\n: <math>\\max_x \\min_{i=1,\\dots,N} R_{\\delta_i}(x). \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)</math>\n\nThis corresponds to choosing a portfolio vector ''x'' so as to obtain the best possible return in the worst-case scenario.<ref>B.K. Pagnoncelli, D. Reich and M.C. Campi. ''Risk-Return Trade-off with the Scenario Approach in Practice: A Case Study in Portfolio Selection''. Journal of Optimization Theory and Applications, 155: 707-722, 2012. [https://link.springer.com/article/10.1007/s10957-012-0074-x]</ref><ref>G.C. Calafiore. ''Direct data-driven portfolio optimization with guaranteed shortfall probability''. Automatica, 49(2) 370-380, 2013. [http://www.sciencedirect.com/science/article/pii/S0005109812005481]</ref>\n\nAfter solving (1), an optimal investment strategy <math>x^\\ast</math> is achieved along with the corresponding optimal return <math>R^\\ast</math>. While <math>R^\\ast</math> has been obtained by looking at <math>N</math> possible market states only, the scenario theory tells us that the solution is robust up to a level <math>\\epsilon</math>, that is, the return <math>R^\\ast</math> will be achieved with probability <math>1 - \\epsilon</math> for other market states. \n\nIn quantitative finance, the worst-case approach can be overconservative. One alternative is to discard some odd situations to reduce pessimism<ref>M.C. Campi and S. Garatti. ''A Sampling-and-Discarding Approach to Chance-Constrained Optimization: Feasibility and Optimality''. Journal of Optimization Theory and Applications, 148, Number 2, 257&ndash;280, 2011 (preliminary version published in Optimization Online, 2008). [http://www.springerlink.com/content/a0146681n6878894/]</ref>; moreover, scenario optimization can be applied to other risk-measures including CVaR - Conditional Value at Risk, so adding to the flexibility of its use.<ref>M.C. Campi and Federico Alessandro Ramponi. ''Expected shortfall: Heuristics and certificates''. European Journal of Operational Research. [https://doi.org/10.1016/j.ejor.2017.11.022]</ref>\n\n== Application fields ==\n\nFields of application include: [[prediction]], [[systems theory]], [[regression analysis]], [[Actuarial science]], [[optimal control]], [[financial mathematics]], [[machine learning]], [[decision making]], [[supply chain]],  and [[management]].\n\n== References ==\n<references/>\n\n[[Category:Stochastic optimization]]\n[[Category:Optimal decisions]]\n[[Category:Control theory]]\n[[Category:Mathematical finance]]"
    },
    {
      "title": "Simulation Optimization Library: Throughput Maximization",
      "url": "https://en.wikipedia.org/wiki/Simulation_Optimization_Library%3A_Throughput_Maximization",
      "text": "\nThe problem of '''Throughput Maximization''' is a family of iterative [[stochastic optimization]] [[algorithm]]s that attempt to find the maximum expected throughput in an n-stage [[Flow line]]. According to Pichitlamken et al. (2006), there are two solutions to the discrete service-rate moderate-sized problem. With an expected throughput (defined as the limiting throughput over a long time horizon, as opposed to the approximation induced through the need for a warm-up period and ratio-estimate as described under [[#Measurement of time|Measurement of time]]. Each simulation [[Replication (statistics)|replication]] should consist of warming up the system with 2000 released jobs starting from an empty system, then recording the time T required to release the next 50 jobs, and estimating the throughput on this replication as 50=T jobs per unit time. Time is then measured in the number of simulation replications performed.\n\n==Problem statement==\nConsider an n-stage [[flow line]] with finite buffer storage in front of Stations 2, 3,..., n, denoted by b2, b3,..., bn, and an infinite number of jobs in front of Station 1. There is a single server at each station, and the service time at Station i is exponentially distributed with service rate ri, i = 1,..., n. If the buffer of Station i is full, then Station i-1 is blocked (production blocking), so that a finished job cannot be released from Station i-1, i = 2,..., n. The total buffer\nspace and the service rates are limited. The goal is to find a buffer allocation and service rates\nsuch that the throughput (average output of the flow line per unit time) is maximized. One can optionally take the service rates as integers or as continuous variables. In either case the problem is still considered as integer-ordered because the buffer allocations are integers.\nThe constraints are:\n\n''b<sub>2</sub>''+''...''+''b<sub>n</sub>''≤ B\n''r<sub>1</sub>''+''r<sub>2</sub>''+''...''+''r<sub>n</sub>''≤R\n\n==Recommended parameter settings==\n A moderate-sized problem is n = 3;B = 20;R = 20. A larger problem is n = 12;B = 80;R = 80.\n\n==Starting solution(s)==\nTake ''b<sub>2</sub>''=''...''=''b<sub>n</sub>'' as large as possible without violating the constraint.\nAllocate any residual buffer spaces to the largest-numbered stations (one per buffer). Similarly,choose uniform service rates. If multiple initial solutions are desired then sample the buffer spacings and service rates as follows. \nFor continuous service rates, sample uniformly from the simplex {r≥0:''r<sub>1</sub>''+''r<sub>2</sub>''+''...''+''r<sub>n</sub>''≤R}.This can be done by generating n-1 independent uniform random variables ''U<sub>1</sub>'',''...'',''U<sub>n-1</sub>''on [0,R], ordering them so that  ''U<sub>(1)</sub>''≤''...''≤''U<sub>(n-1)</sub>'', setting ''U<sub>(0)</sub>''=0 and ''U<sub>(n)</sub>''=R, and finally setting ''r<sub>k</sub>''=''U<sub>(k)</sub>''-''U<sub>(k-1)</sub>'', k = 1,...,n. A similar procedure can be used for assigning the buffer spaces, which can be allowed to take the value 0.\n\n==Measurement of time==\nEach simulation replication should consist of warming up the system with 2000 released jobs starting from an empty system, then recording the time T required to release the next 50 jobs, and estimating the throughput on this replication as 50=T jobs per unit time. \nTime is then measured in the number of simulation replications performed.\n\n==Optimal solutions==\nWe will consider buffer space as the number of pieces waiting to be worked on. i.e. the current job does not count as part of the buffer space Cycle through each job to determine the time at which it leaves every stage. \nThere are 2 solutions to the discreteservice-rate moderate-sized problem, namely r = (6, 7, 7), b = (12, 8) and r = (7, 7, 6), b = (8, 12)with an expected throughput (defined as the limiting throughput over a long time horizon, as opposed to the approximation induced through the need for a warm-up period and ratio-estimate as described under \"Measurement of Time\").\n\nThroughput = 50/(time last job released - time at which the 50th to last job was released)\n\n==Reported algorithm performance==\nNone.\n\n==References==\n<ref>A sequential procedure for neighborhood selection-of-the-best in optimization via simulation. European Journal of Operational Research</ref>\n<ref>Proceedings of the 1998 Winter Simulation Conference\nD.J. Medeiros, E.F. Watson, J.S. Carson and M.S. Manivannan, eds.</ref>\n<ref>The Sample Average Approximation Method for\nStochastic Programs with Integer Recourse\nShabbir Ahmed∗ and Alexander Shapiro†\nSchool of Industrial & Systems Engineering\nGeorgia Institute of Technology\n765 Ferst Drive, Atlanta, GA 30332</ref>\n{{reflist}}\n\n\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Stochastic approximation",
      "url": "https://en.wikipedia.org/wiki/Stochastic_approximation",
      "text": "{{technical|date=June 2012}}\n'''Stochastic approximation''' methods are a family of [[Iterative methods|iterative methods]] typically used for [[Root-finding|root-finding]] problems or for [[optimization problem|optimization]] problems. The recursive update rules of stochastic approximation methods can be used, among other things, for solving linear systems when the collected data is corrupted by noise, or for approximating [[Extremum|extreme values]] of functions which cannot be computed directly, but only estimated via noisy observations. \n\nFor instance in engineering many optimization problems are often of this type when you do not have a mathematical model of the system (which can be too complex) but still would like to optimize its behavior by adjusting certain parameters. For this purpose, you can do experiments or run simulations to evaluate the performance of the system at given values of the parameters. Stochastic approximation algorithms have also been used in the social sciences to describe collective dynamics: fictitious play in learning theory and consensus algorithms can be studied using their theory.<ref>{{cite web|last1=Le Ny|first1=Jerome|title=Introduction to Stochastic Approximation Algorithms|url=http://www.professeurs.polymtl.ca/jerome.le-ny/teaching/DP_fall09/notes/lec11_SA.pdf|website=Polytechnique Montreal|publisher=Teaching Notes|accessdate=16 November 2016}}</ref>. These methods are used often in statistics and machine learning that typically need to handle noisy measurements of empirical data. They are also related to [[stochastic optimization]] methods and algorithms.\n\nIn a nutshell, stochastic approximation algorithms deal with a function of the form <math display=\"inline\"> f(\\theta) = \\operatorname E_{\\xi} [F(\\theta,\\xi)] </math>\nwhich is the expected value of a function depending on a [[random variable]] <math display=\"inline\">\\xi </math>. The goal is to recover properties of such a function <math display=\"inline\">f</math> without evaluating it directly. Instead, stochastic approximation algorithms use random samples of <math display=\"inline\">F(\\theta,\\xi)</math> to efficiently approximate properties of <math display=\"inline\">f</math> such as zeros or extrema.\n\nThe earliest, and prototypical, algorithms of this kind are the '''Robbins-Monro''' and '''Kiefer-Wolfowitz''' algorithms introduced respectively in 1951 and 1952.\n\n==Robbins–Monro algorithm==\nThe Robbins–Monro algorithm, introduced in 1951 by [[Herbert Robbins]] and Sutton Monro,<ref name=\"rm\">{{Cite journal | last1 = Robbins | first1 = H. | authorlink = Herbert Robbins| last2 = Monro | first2 = S. | doi = 10.1214/aoms/1177729586 | title = A Stochastic Approximation Method | journal = The Annals of Mathematical Statistics | volume = 22 | issue = 3 | pages = 400 | year = 1951 | pmid =  | pmc = }}</ref> presented a methodology for solving a root finding problem, where the function is represented as an expected value. Assume that we have a function <math display=\"inline\">M(\\theta)</math>, and a constant <math display=\"inline\">\\alpha</math>, such that the equation <math display=\"inline\">M(\\theta) = \\alpha</math> has a unique root at <math display=\"inline\">\\theta^*</math>. It is assumed that while we cannot directly observe the function <math display=\"inline\">M(\\theta)</math>, we can instead obtain measurements of the random variable <math display=\"inline\">N(\\theta)</math> where <math display=\"inline\">\\operatorname E[N(\\theta)] = M(\\theta)</math>. The structure of the algorithm is to then generate iterates of the form:\n\n::<math>\\theta_{n+1}=\\theta_n - a_n(N(\\theta_n) - \\alpha)</math>\n\nHere, <math>a_1, a_2, \\dots</math> is a sequence of positive step sizes. [[Herbert Robbins|Robbins]] and Monro proved <ref name=\"rm\" /><sup>, Theorem 2</sup> that <math>\\theta_n</math> [[convergence of random variables|converges]] in <math>L^2</math> (and hence also in probability) to <math>\\theta</math>, and Blum<ref name=\":0\">{{Cite journal|last=Blum|first=Julius R.|date=1954-06-01|title=Approximation Methods which Converge with Probability one|journal=The Annals of Mathematical Statistics|language=EN|volume=25|issue=2|pages=382–386|doi=10.1214/aoms/1177728794|issn=0003-4851}}</ref> later proved the convergence is actually with probability one, provided that:\n* <math display=\"inline\">N(\\theta)</math> is uniformly bounded,\n* <math display=\"inline\">M(\\theta)</math> is nondecreasing,\n* <math display=\"inline\">M'(\\theta^*)</math> exists and is positive, and\n* The sequence <math display=\"inline\">a_n</math> satisfies the following requirements:\n:: <math>\\qquad \\sum^{\\infty}_{n=0}a_n = \\infty \\quad \\mbox{ and } \\quad \\sum^{\\infty}_{n=0}a^2_n < \\infty \\quad </math>\n\nA particular sequence of steps which satisfy these conditions, and was suggested by Robbins–Monro, have the form: <math display=\"inline\">a_n=a/n</math>, for <math display=\"inline\"> a > 0 </math>. Other series are possible but in order to average out the noise in <math display=\"inline\">N(\\theta)</math>, the above condition must be met.\n\n===Complexity results===\n#If <math display=\"inline\">f(\\theta)</math> is twice continuously differentiable, and strongly convex, and the minimizer of <math display=\"inline\">f(\\theta)</math> belongs to the interior of <math display=\"inline\">\\Theta</math>, then the Robbins-Monro algorithm will achieve the asymptotically optimal convergence rate, with respect to the objective function, being <math display=\"inline\">\\operatorname E[f(\\theta_n) - f^*] = O(1/n)</math>, where <math display=\"inline\">f^*</math> is the minimal value of <math display=\"inline\">f(\\theta)</math> over <math display=\"inline\">\\theta \\in \\Theta</math>.<ref name=\"jsacks\">{{Cite journal | last1 = Sacks | first1 = J. | title = Asymptotic Distribution of Stochastic Approximation Procedures | doi = 10.1214/aoms/1177706619 | journal = The Annals of Mathematical Statistics | volume = 29 | issue = 2 | pages = 373–405 | year = 1958 | jstor = 2237335| pmid =  | pmc = }}</ref><ref name=\"NJLS\">{{Cite journal | last1 = Nemirovski | first1 = A. | authorlink1 = Arkadi Nemirovski| last2 = Juditsky | first2 = A. | last3 = Lan | first3 = G. | last4 = Shapiro | first4 = A. | title = Robust Stochastic Approximation Approach to Stochastic Programming | doi = 10.1137/070704277 | journal = SIAM Journal on Optimization | volume = 19 | issue = 4 | pages = 1574 | year = 2009 | pmid =  | pmc = }}</ref>\n# Conversely, in the general convex case, where we lack both the assumption of smoothness and strong convexity, Nemirovski and Yudin <ref name=\"NYcomp\">Problem Complexity and Method Efficiency in Optimization, A. Nemirovski and D. Yudin, ''Wiley -Intersci. Ser. Discrete Math'' '''15''' ''John Wiley'' ''New York'' (1983) .</ref> have shown that the asymptotically optimal convergence rate, with respect to the objective function values, is <math display=\"inline\">O(1/\\sqrt{n})</math>. They have also proven that this rate cannot be improved.\n\n===Subsequent developments and Polyak-Ruppert Averaging===\nWhile the Robbins-Monro algorithm is theoretically able to achieve <math display=\"inline\"> O(1/n)</math> under the assumption of twice continuous differentiability and strong convexity, it can perform quite poorly upon implementation. This is primarily due to the fact that the algorithm is very sensitive to the choice of the step size sequence, and the supposed asymptotically optimal step size policy can be quite harmful in the beginning.<ref name=\"NJLS\" /><ref name=\"jcsbook\">Introduction to Stochastic Search and Optimization: Estimation, Simulation and Control, J.C. Spall, ''John Wiley'' ''Hoboken, NJ'', (2003).</ref>\n\nChung<ref>{{Cite journal|last=Chung|first=K. L.|date=1954-09-01|title=On a Stochastic Approximation Method|journal=The Annals of Mathematical Statistics|language=EN|volume=25|issue=3|pages=463–483|doi=10.1214/aoms/1177728716|issn=0003-4851}}</ref>(1954) and Fabian<ref>{{Cite journal|last=Fabian|first=Vaclav|date=1968-08-01|title=On Asymptotic Normality in Stochastic Approximation|journal=The Annals of Mathematical Statistics|language=EN|volume=39|issue=4|pages=1327–1332|doi=10.1214/aoms/1177698258|issn=0003-4851}}</ref>(1968) showed that we would achieve optimal convergence rate <math display=\"inline\">O(1/\\sqrt{n})</math> with <math display=\"inline\">a_n=\\bigtriangledown^2f(\\theta^*)^{-1}/n</math> (or <math display=\"inline\">a_n=\\frac{1}{(nM'(\\theta^*))}</math>). Lai and Robbins<ref>{{Cite journal|last=Lai|first=T. L.|last2=Robbins|first2=Herbert|date=1979-11-01|title=Adaptive Design and Stochastic Approximation|journal=The Annals of Statistics|language=EN|volume=7|issue=6|pages=1196–1221|doi=10.1214/aos/1176344840|issn=0090-5364}}</ref><ref>{{Cite journal|last=Lai|first=Tze Leung|last2=Robbins|first2=Herbert|date=1981-09-01|title=Consistency and asymptotic efficiency of slope estimates in stochastic approximation schemes|journal=Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete|language=en|volume=56|issue=3|pages=329–360|doi=10.1007/BF00536178|issn=0044-3719}}</ref> designed adaptive procedures to estimate <math display=\"inline\">M'(\\theta^*)</math> such that <math display=\"inline\">\\theta_n</math> has minimal asymptotic variance. However the application of such optimal methods requires much a priori information which is hard to obtain in most situations. To overcome this shortfall, Polyak<ref>{{Cite journal|last=Polyak|first=B T|date=1990-01-01|title=New stochastic approximation type procedures. (In Russian.)|url=https://www.researchgate.net/publication/236736759|volume=7|issue=7}}</ref>(1991) and Ruppert<ref>{{Cite journal|last=Ruppert|first=D.|title=Efficient estimators from a slowly converging robbins-monro process|url=https://www.researchgate.net/publication/242608650}}</ref>(1988) independently developed a new optimal algorithm based on the idea of averaging the trajectories. Polyak and Juditsky<ref name=\"pj\">{{Cite journal | last1 = Polyak | first1 = B. T. | last2 = Juditsky | first2 = A. B. | doi = 10.1137/0330046 | title = Acceleration of Stochastic Approximation by Averaging | journal = SIAM Journal on Control and Optimization | volume = 30 | issue = 4 | pages = 838 | year = 1992 | pmid =  | pmc = }}</ref> also presented a method of accelerating Robbins-Monro for linear and non-linear root-searching problems through the use of longer steps, and averaging of the iterates. The algorithm would have the following structure:<math display=\"block\"> \\theta_{n+1} - \\theta_n = a_n(\\alpha - N(\\theta_n)), \\qquad \\bar{\\theta}_n = \\frac{1}{n} \\sum^{n-1}_{i=0} \\theta_i </math>The convergence of <math> \\bar{\\theta}_n </math> to the unique root <math>\\theta^*</math> relies on the condition that the step sequence <math>\\{a_n\\}</math> decreases sufficiently slowly. That is\n\n'''''A1)''''' ''<math display=\"block\">  a_n \\rightarrow 0, \\qquad \\frac{a_n - a_{n+1}}{a_n} = o(a_n)</math>\n\nTherefore, the sequence <math display=\"inline\">a_n = n^{-\\alpha}</math> with <math display=\"inline\">0 < \\alpha < 1</math> satisfies this restriction, but <math display=\"inline\">\\alpha = 1</math> does not, hence the longer steps. Under the assumptions outlined in the Robbins-Monro algorithm, the resulting modification will result in the same asymptotically optimal convergence rate <math display=\"inline\">O(1/\\sqrt{n})</math> yet with a more robust step size policy.<ref name=\"pj\" /> Prior to this, the idea of using longer steps and averaging the iterates had already been proposed by Nemirovski and Yudin <ref name=\"NY\">On Cezari's convergence of the steepest descent method for approximating saddle points of convex-concave functions, A. Nemirovski and D. Yudin, ''Dokl. Akad. Nauk SSR'' '''2939''', (1978 (Russian)), Soviet Math. Dokl. '''19''' (1978 (English)).</ref> for the cases of solving the stochastic optimization problem with continuous convex objectives and for convex-concave saddle point problems. These algorithms were observed to attain the nonasymptotic rate <math display=\"inline\">O(1/\\sqrt{n})</math>.\n\nA more general result is given in Chapter 11 of Kushner and Yin<ref>{{Cite book|url=https://www.springer.com/us/book/9780387008943|title=Stochastic Approximation and Recursive Algorithms and {{!}} Harold Kushner {{!}} Springer|website=www.springer.com|access-date=2016-05-16|isbn=9780387008943|last1=Kushner|first1=Harold|last2=George Yin|first2=G.|date=2003-07-17}}</ref> by defining interpolated time <math display=\"inline\">t_n=\\sum_{i=0}^{n-1}a_i</math>, interpolated process <math display=\"inline\">\\theta^n(\\cdot)</math> and interpolated normalized process <math display=\"inline\">U^n(\\cdot)</math> as\n\n<math display=\"block\">\\theta^n(t)=\\theta_{n+i},\\quad U^n(t)=(\\theta_{n+i}-\\theta^*)/\\sqrt{a_{n+i}}\\quad\\mbox{for}\\quad t\\in[t_{n+i}-t_n,t_{n+i+1}-t_n),i\\ge0</math>Let the iterate average be <math>\\Theta_n=\\frac{a_n}{t}\\sum_{i=n}^{n+t/a_n-1}\\theta_i</math> and the associate normalized error to be <math>\\hat{U}^n(t)=\\frac{\\sqrt{a_n}}{t}\\sum_{i=n}^{n+t/a_n-1}(\\theta_i-\\theta^*)</math>.\n\nWith assumption '''A1)''' and the following '''A2)'''\n\n'''''A2)''''' ''There is a Hurwitz matrix <math display=\"inline\">A</math> and a symmetric and positive-definite matrix <math display=\"inline\">\\Sigma</math> such that <math display=\"inline\">\\{U^n(\\cdot)\\}</math> converges weakly to <math display=\"inline\">U(\\cdot)</math>, where <math display=\"inline\">U(\\cdot)</math>  is the stationary solution to<math display=\"block\">dU=AUdt+\\Sigma^{1/2}dw</math>where <math display=\"inline\">w(\\cdot)</math> is a standard Wiener process.''\n\nsatisfied, and define ''<math display=\"inline\">\\bar{V}=(A^{-1})'\\Sigma(A')^{-1}</math>''. Then for each ''<math display=\"inline\">t</math>'',\n\n''<math display=\"block\">\\hat{U}^n(t)\\stackrel{\\mathcal{D}}{\\longrightarrow}\\mathcal{N}(0,V_t),\\quad \\mbox{where}\\quad V_t=\\bar{V}/t+O(1/t^2).</math>''\n\nThe success of the averaging idea is because of the time scale separation of the original sequence ''<math display=\"inline\">\\{\\theta_n\\}</math>'' and the averaged sequence ''<math display=\"inline\">\\{\\Theta_n\\}</math>'', with the time scale of the former one being faster.\n\n=== Application in Stochastic Optimization ===\nSuppose we want to solve the following stochastic optimization problem\n\n<math display=\"block\">g(\\theta^*) = \\min_{\\theta\\in\\Theta}\\operatorname{E}[Q(\\theta,X)],</math>where <math display=\"inline\">g(\\theta) = \\operatorname{E}[Q(\\theta,X)]</math> is differentiable and convex, then this problem is equivalent to find the root <math>\\theta^*</math> of <math>\\nabla g(\\theta) = 0</math>. Here <math>Q(\\theta,X)</math> can be interpreted as some \"observed\" cost as a function of the chosen <math>\\theta</math> and random effects <math>X</math>. In practice, it might be hard to get an analytical form of <math>\\nabla g(\\theta)</math>, Robbins-Monro method manages to generate a sequence <math>(\\theta_n)_{n\\geq 0}</math> to approximate <math>\\theta^*</math> if one can generate <math>(X_n)_{n\\geq 0}\n</math> , in which the conditional expectation of <math>X_n\n\n</math> given <math>\\theta_n\n</math> is exactly <math>\\nabla g(\\theta_n)</math>, i.e. <math>X_n</math> is simulated from a conditional distribution defined by\n\n<math display=\"block\">\\operatorname{E}[H(\\theta,X)|\\theta = \\theta_n] = \\nabla g(\\theta_n).</math>\n\nHere <math>H(\\theta, X)</math> is an unbiased estimator of <math>\\nabla g(\\theta)</math>. If <math>X</math> depends on <math>\\theta</math>, there is in general no natural way of generating a random outcome <math>H(\\theta, X)</math> that is an unbiased estimator of the gradient. In some special cases when either IPA or likelihood ratio methods are applicable, then one is able to obtain an unbiased gradient estimator <math>H(\\theta, X)</math>.  If <math>X</math> is viewed as some \"fundamental\" underlying random process that is generated ''independently'' of <math>\\theta</math>, and under some regularization conditions for derivative-integral interchange operations so that <math>\\operatorname{E}\\Big[\\frac{\\partial}{\\partial\\theta}Q(\\theta,X)\\Big] = \\nabla g(\\theta)</math>, then <math>H(\\theta, X) = \\frac{\\partial}{\\partial \\theta}Q(\\theta, X)</math> gives the fundamental gradient unbiased estimate. However, for some applications we have to use finite-difference methods in which <math>H(\\theta, X)</math> has a conditional expectation close to <math>\\nabla g(\\theta)</math> but not exactly equal to it.\n\nWe then define a recursion analogously to Newton's Method in the deterministic algorithm:\n\n<math display=\"block\">\\theta_{n+1} = \\theta_n - \\epsilon_n H(\\theta_n,X_{n+1}).</math>\n\n==== Convergence of the Algorithm ====\nThe following result gives sufficient conditions on <math>\\theta_n\n\n </math> for the algorithm to converge:<ref>{{Cite book|title=Numerical Methods for stochastic Processes|last=Bouleau|first=N.|last2=Lepingle|first2=D.|publisher=John Wiley|year=1994|isbn=9780471546412|location=New York|pages=|url=https://books.google.com/books?id=9MLL2RN40asC&printsec=frontcover#v=onepage&q&f=false}}</ref>\n\nC1)   <math>\\epsilon_n \\geq 0, \\forall\\; n\\geq 0 </math>.\n\nC2)  <math>\\sum_{n=0}^{\\infty}\\epsilon_n = \\infty\n </math>\n\nC3)  <math>\\sum_{n=0}^{\\infty}\\epsilon_n^2 <\\infty\n </math>\n\nC4) <math>|X_n| \\leq B, \\text{for a fixed bound}\\; B.\n </math>\n\nC5) <math>g(\\theta)\\; \\text{is strictly convex, i.e.}\n\n </math>\n\n<math display=\"block\">\\inf_{\\delta\\leq |\\theta - \\theta^*|\\leq 1/\\delta}\\langle\\theta-\\theta^*, \\nabla g(\\theta)\\rangle > 0,\\text{for every}\\; 0< \\delta < 1.\n\n </math>\n\nThen <math>\\theta_n\n\n </math> converges to <math>\\theta^*\n\n </math> almost surely.\n\nHere are some intuitive explanations about these conditions. Suppose <math>H(\\theta_n, X_{n+1})</math> is a uniformly bounded random variables. If  C2) is not satisfied, i.e. <math>\\sum_{n=0}^{\\infty}\\epsilon_n < \\infty\n </math> , then<math display=\"block\">\\theta_n - \\theta_0 = -\\sum_{i=0}^{n-1}\\epsilon_i H(\\theta_i, X_{i+1})\n </math>is a bounded sequence, so the iteration cannot converge to <math>\\theta^*\n\n </math> if the initial guess <math>\\theta_0\n </math> is too far away from <math>\\theta^*\n\n </math>. As for C3) note that if <math>\\theta_n\n\n </math> converges to <math>\\theta^*\n\n </math> then\n\n<math display=\"block\">\\theta_{n+1} - \\theta_n = -\\epsilon_n H(\\theta_n, X_{n+1}) \\rightarrow 0,\\text{as}\\; n\\rightarrow \\infty.\n </math>so we must have <math>\\epsilon_n \\downarrow 0\n </math> ，and the condition C3) ensures it. A natural choice would be <math>\\epsilon_n = 1/n\n </math>. Condition C5) is a fairly stringent condition on the shape of <math>g(\\theta)</math>; it gives the search direction of the algorithm.\n\n==== Example (where the stochastic gradient method is appropriate)<ref name=\"jcsbook\" /> ====\nSuppose <math>Q(\\theta, X) = f(\\theta) + \\theta^T X</math>, where <math>f</math> is differentiable and <math>X\\in \\mathbb{R}^p</math> is a random variable independent of <math>\\theta</math>. Then <math>g(\\theta)=\\operatorname{E}[Q(\\theta,X)] = f(\\theta)+\\theta^T\\operatorname{E}X</math> depends on the mean of <math>X</math>, and the stochastic gradient method would be appropriate in this problem. We can choose <math>H(\\theta, X) = \\frac{\\partial}{\\partial\\theta}Q(\\theta,X) = \\frac{\\partial}{\\partial\\theta}f(\\theta) + X.</math>\n\n==Kiefer-Wolfowitz algorithm==\nThe Kiefer-Wolfowitz algorithm<ref name = \"KW\">{{Cite journal | last1 = Kiefer | first1 = J. | last2 = Wolfowitz | first2 = J. | doi = 10.1214/aoms/1177729392 | title = Stochastic Estimation of the Maximum of a Regression Function | journal = The Annals of Mathematical Statistics | volume = 23 | issue = 3 | pages = 462 | year = 1952 | pmid =  | pmc = }}</ref> was introduced in 1952 by [[Jacob Wolfowitz]] and [[Jack_Kiefer_(statistician)|Jack Kiefer]], and was motivated by the publication of the Robbins-Monro algorithm. However, the algorithm was presented as a method which would stochastically estimate the maximum of a function. Let <math>M(x) </math> be a function which has a maximum at the point <math>\\theta </math>. It is assumed that <math>M(x)</math> is unknown; however, certain observations <math>N(x)</math>, where <math>\\operatorname E[N(x)] = M(x)</math>, can be made at any point <math>x</math>. The structure of the algorithm follows a gradient-like method, with the iterates being generated as follows:\n::<math> x_{n+1} = x_n + a_n \\bigg(\\frac{N(x_n + c_n) - N(x_n -c_n)}{2 c_n} \\bigg) </math>\nwhere <math>N(x_n+c_n)</math> and <math>N(x_n-c_n)</math> are independent, and the gradient of <math>M(x)</math> is approximated using finite differences. The sequence <math>\\{c_n\\}</math> specifies the sequence of finite difference widths used for the gradient approximation, while the sequence <math>\\{a_n\\}</math> specifies a sequence of positive step sizes taken along that direction. Kiefer and Wolfowitz proved that, if <math>M(x)</math> satisfied certain regularity conditions, then <math>x_n</math> will converge to <math>\\theta</math> in probability as <math>n\\to\\infty\n </math>, and later Blum<ref name=\":0\" /> in 1954 showed <math>x_n</math> converges to <math>\\theta</math> almost surely, provided that:\n* <math>Var(N(x))\\le S<\\infty</math> for all <math>x</math>.\n* The function <math>M(x)</math> has a unique point of maximum (minimum) and is strong concave (convex)\n** The algorithm was first presented with the requirement that the function <math>M(\\cdot)</math> maintains strong global convexity (concavity) over the entire feasible space. Given this condition is too restrictive to impose over the entire domain, Kiefer and Wolfowitz proposed that it is sufficient to impose the condition over a compact set <math>C_0 \\subset \\mathbb R^d</math> which is known to include the optimal solution.\n*The function <math>M(x)</math> satisfies the regularity conditions as follows:\n** There exists <math>\\beta>0</math> and <math>B>0</math> such that <math display=\"block\">|x'-\\theta|+|x''-\\theta|<\\beta \\quad \\Longrightarrow \\quad |M(x')-M(x'')|<B|x'-x''|</math>\n** There exists <math> \\rho>0 </math> and <math> R>0 </math> such that <math display=\"block\">|x'-x''|<\\rho \\quad \\Longrightarrow \\quad |M(x')-M(x'')|<R</math> \n** For every <math> \\delta>0 </math>, there exists some <math> \\pi(\\delta)>0 </math> such that <math display=\"block\">|z-\\theta|>\\delta \\quad \\Longrightarrow \\quad \\inf_{\\delta/2>\\epsilon>0}\\frac{|M(z+\\epsilon)-M(z-\\epsilon)|}{\\epsilon}>\\pi(\\delta)</math>\n*The selected sequences <math>\\{a_n\\}</math> and <math>\\{c_n\\}</math> must be infinite sequences of positive numbers such that\n**<math>\\quad c_n \\rightarrow 0\\quad \\mbox{as}\\quad n\\to\\infty </math>\n**<math> \\sum^\\infty_{n=0} a_n =\\infty </math>\n**<math> \\sum^\\infty_{n=0} a_nc_n <\\infty </math>\n**<math> \\sum^\\infty_{n=0} a_n^2c_n^{-2} <\\infty </math>\n\nA suitable choice of sequences, as recommended by Kiefer and Wolfowitz, would be <math>a_n = 1/n</math> and <math>c_n = n^{-1/3}</math>.\n\n===Subsequent developments and important issues===\n#The Kiefer Wolfowitz algorithm requires that for each gradient computation, at least <math>d+1</math> different parameter values must be simulated for every iteration of the algorithm, where <math>d </math> is the dimension of the search space. This means that when <math>d</math> is large, the Kiefer-Wolfowitz algorithm will require substantial computational effort per iteration, leading to slow convergence.\n## To address this problem, Spall proposed the use of [[Simultaneous perturbation stochastic approximation|simultaneous perturbations]] to estimate the gradient. This method would require only two simulations per iteration, regardless of the dimension <math>d</math>.<ref name = \"Jsp\">{{Cite journal | last1 = Spall | first1 = J. C. | title = Adaptive stochastic approximation by the simultaneous perturbation method | doi = 10.1109/TAC.2000.880982 | journal = IEEE Transactions on Automatic Control | volume = 45 | issue = 10 | pages = 1839–1853 | year = 2000 | pmid =  | pmc = }}</ref>\n#In the conditions required for convergence, the ability to specify a predetermined compact set that fulfills strong convexity (or concavity) and contains the unique solution can be difficult to find. With respect to real world applications, if the domain is quite large, these assumptions can be fairly restrictive and highly unrealistic.\n\n==Further developments==\nAn extensive theoretical literature has grown up around these algorithms, concerning conditions for convergence, rates of convergence,  multivariate and other generalizations, proper choice of step size, possible noise models, and so on.<ref name=\"kushneryin\">{{Cite book | last1 = Kushner | first1 = H. J. | authorlink=Harold J. Kushner| last2 = Yin | first2 = G. G. | doi = 10.1007/978-1-4899-2696-8 | title = Stochastic Approximation Algorithms and Applications | year = 1997 | isbn = 978-1-4899-2698-2 | pmid =  | pmc = }}</ref><ref>''Stochastic Approximation and Recursive Estimation'', Mikhail Borisovich Nevel'son and Rafail Zalmanovich Has'minskiĭ, translated by Israel Program for Scientific Translations and B. Silver, Providence, RI: American Mathematical Society, 1973, 1976. {{isbn|0-8218-1597-0}}.</ref>   These methods are also applied in [[control theory]], in which case the unknown function which we wish to optimize or find the zero of may vary in time. In this case, the step size <math>a_n</math> should not converge to zero but should be chosen so as to track the function.<ref name=\"kushneryin\"/><sup>, 2nd ed., chapter 3</sup>\n\n[[C. Johan Masreliez]] and [[R. Douglas Martin]] were the first to apply \nstochastic approximation to [[Robust statistics|robust]] [[estimation]].<ref>{{Cite journal | last1 = Martin | first1 = R. | last2 = Masreliez | first2 = C. | doi = 10.1109/TIT.1975.1055386 | title = Robust estimation via stochastic approximation | journal = IEEE Transactions on Information Theory | volume = 21 | issue = 3 | pages = 263 | year = 1975 | pmid =  | pmc = }}</ref>\n\nThe main tool for analyzing stochastic approximations algorithms (including the Robbins-Monro and the Kiefer-Wolfowitz algorithms) is a theorem by [[Aryeh Dvoretzky]] published in the proceedings of the third Berkeley symposium on mathematical statistics and probability, 1956.<ref>{{Cite journal|last=Dvoretzky|first=Aryeh|date=1956-01-01|title=On Stochastic Approximation|url=http://projecteuclid.org/euclid.bsmsp/1200501645|language=EN|publisher=The Regents of the University of California}}</ref>\n\n==See also==\n*[[Stochastic gradient descent]]\n*[[Stochastic optimization]]\n*[[Simultaneous perturbation stochastic approximation]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Stochastic Approximation}}\n[[Category:Stochastic optimization]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Stochastic tunneling",
      "url": "https://en.wikipedia.org/wiki/Stochastic_tunneling",
      "text": "In [[numerical analysis]], '''stochastic tunneling''' (STUN) is an approach to [[global optimization]] based on the [[Monte Carlo method]]-[[Sampling (signal processing)|sampling]] of the function to be objective minimized in which the function is nonlinearly transformed to allow for easier tunneling among regions containing function minima. Easier tunneling allows for faster exploration of sample space and faster convergence to a good solution.\n\n== Idea ==\n[[image:stun.jpg|thumb|400px| Schematic one-dimensional test function (black) and  STUN effective potential (red & blue), where the minimum indicated by the arrows is the best minimum found so far. All [[Potential well|well]]s that lie above the best minimum found are suppressed. If the dynamical process can escape the well around the current minimum estimate it will not be trapped by other local minima that are higher. Wells with deeper minima are enhanced. The dynamical process is accelerated by that.]]\n\n[[Monte Carlo method]]-based optimization techniques sample the [[objective function]] by randomly \"hopping\" from the current solution vector to another with a difference in the function value of <math>\\Delta E</math>. The acceptance probability  of such a trial jump is in most cases chosen to be\n<math> \\min\\left(1;\\exp\\left(-\\beta\\cdot\\Delta E\\right)\\right)\n</math> ([[Nicholas Metropolis|Metropolis]] criterion) with an appropriate parameter <math>\\beta</math>.\n\nThe general idea of STUN is to circumvent the slow dynamics of ill-shaped energy functions that one encounters for example in [[spin glass]]es by tunneling through such barriers.\n\nThis goal is achieved by Monte Carlo sampling of a\ntransformed function that lacks this slow dynamics. In the \"standard-form\"\nthe transformation reads <math>f_{STUN}:=1-\\exp\\left(\n-\\gamma\\cdot\\left( E(x)-E_o\\right) \\right)</math> where <math>E_o</math>\nis the lowest function value found so far. This transformation preserves the [[Locus (mathematics)|loci]] of the minima.\n\n<math>f_{STUN}</math> is then used in place of <math>E</math> in the original algorithm giving a new acceptance probability of <math> \\min\\left(1;\\exp\\left(-\\beta\\cdot\\Delta f_{STUN}\\right)\\right)\n</math>\n\nThe effect of such a transformation is shown in the graph.\n\n== Dynamically adaptive stochastic tunneling ==\n\nA variation on always tunneling is to do so only when trapped at a local minimum. <math>\\gamma</math> is then adjusted to tunnel out of the minimum and pursue a more globally optimum solution. [[Detrended fluctuation analysis]] is the recommended way of determining if trapped at a local minimum.\n\n== Other approaches ==\n* [[Simulated annealing]]\n* [[Parallel tempering]]\n* [[Genetic algorithm]]\n* [[Differential evolution]]\n\n== References ==\n* {{Cite journal\n | author = K. Hamacher\n | title = Adaptation in Stochastic Tunneling Global Optimization of Complex Potential Energy Landscapes\n | journal = [[Europhys. Lett.]]  \n | volume = 74\n | issue = 6\n | pages = 944–950\n | year = 2006\n | doi = 10.1209/epl/i2006-10058-0\n|bibcode = 2006EL.....74..944H }}\n* {{Cite journal\n |author1=K. Hamacher  |author2=W. Wenzel\n  |lastauthoramp=yes | title = The Scaling Behaviour of Stochastic Minimization Algorithms in a Perfect Funnel Landscape\n | journal = [[Phys. Rev. E]]\n | volume = 59\n | issue = 1\n | pages = 938&ndash;941\n | year = 1999\n | doi = 10.1103/PhysRevE.59.938\n|arxiv = physics/9810035 |bibcode = 1999PhRvE..59..938H }}\n* {{Cite journal\n |author1=W. Wenzel  |author2=K. Hamacher\n  |lastauthoramp=yes | title = A Stochastic tunneling approach for global minimization\n | journal = [[Phys. Rev. Lett.]]\n | volume = 82\n | issue = 15\n | pages = 3003&ndash;3007\n | year = 1999\n | doi = 10.1103/PhysRevLett.82.3003\n| bibcode=1999PhRvL..82.3003W\n|arxiv = physics/9903008 }}\n* {{Cite journal\n | author = [[Nicholas Metropolis]], Arianna W. Rosenbluth, [[Marshall N. Rosenbluth]], Augusta H. Teller and [[Edward Teller]]\n | title = Equation of State Calculations by Fast Computing Machines\n | journal = [[The Journal of Chemical Physics]]\n | volume = 21\n |date=June 1953\n | pages = 1087&ndash;1092\n | doi = 10.1063/1.1699114\n | url = http://scienze-como.uninsubria.it/bressanini/montecarlo-history/mrt2.pdf\n | issue = 6\n | bibcode=1953JChPh..21.1087M\n}}\n* {{Cite journal\n | author = Mingjie Lin\n | title = Improving FPGA Placement with Dynamically Adaptive Stochastic Tunneling\n | journal = [[IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems]]\n | volume = 29\n |date=December 2010\n | pages = 1858&ndash;1869\n | issue = 12\n | ref = lin\n | doi=10.1109/tcad.2010.2061670\n| url = https://zenodo.org/record/1232243\n }}\n\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Haybittle–Peto boundary",
      "url": "https://en.wikipedia.org/wiki/Haybittle%E2%80%93Peto_boundary",
      "text": "The '''Haybittle–Peto boundary''' is a rule for deciding when to stop a [[clinical trial]] prematurely.<ref>{{cite journal|doi=10.1001/jama.294.17.2228|journal=JAMA|year=2005|volume=294|issue=17|pages=2228&ndash;30|title=When (not) to stop a clinical trial for benefit|author=Pocock SJ|authorlink=Stuart Pocock|pmid=16264167|citeseerx=10.1.1.498.722}}</ref> It is named for [[John Haybittle]] and [[Richard Peto]].\n\nThe typical clinical trial compares two groups of patients. One group are given a [[placebo]] or conventional treatment, while the other group of patients are given the treatment that is being tested. The investigators running the clinical trial will wish to stop the trial early for ethical reasons if the treatment group clearly shows evidence of benefit. In other words, \"when early results proved so promising it was no longer fair to keep patients on the older drugs for comparison, without giving them the opportunity to change.\"<ref>{{cite news|author=Hall C|title=Heart attacks may be cut by half|newspaper=Daily Telegraph|date=5 September 2005|page=1|url=https://www.telegraph.co.uk/news/uknews/1497648/Heart-attacks-may-be-cut-by-half.html}}</ref>\n\nThe Haybittle–Peto boundary is one such [[stopping rule]], and it states that if an interim analysis shows a probability of equal to, or less than 0.001 that a difference as extreme or more between the treatments is found, given that the null hypothesis is true, then the trial should be stopped early. The final analysis is still evaluated at the normal level of significance (usually 0.05).<ref>{{cite journal |doi=10.1259/0007-1285-44-526-793 |pmid=4940475 |last=Haybittle |first=JL|authorlink=John Haybittle |year=1971| title=Repeated assessments of results in clinical trials of cancer treatment|journal=Br. J. Radiol.|volume=44|pages=793&ndash;797| issue=526}}</ref><ref>{{cite journal |last1=Peto |first1=R |authorlink1=Richard Peto |last2=Pike |first2=MC |last3=Armitage |first3=P |display-authors=etal |year=1976|title=Design and analysis of randomized clinical trials requiring prolonged observation of each patient. I. Introduction and design|journal=Br. J. Cancer|volume=34|pages=585&ndash;612|doi=10.1038/bjc.1976.220|issue=6|pmid=795448|pmc=2025229}}</ref>  The main advantage of the Haybittle–Peto boundary is that the same threshold is used at every interim analysis, unlike the [[O'Brien–Fleming boundary]], which changes at every analysis. Also, using the Haybittle–Peto boundary means that the final analysis is performed using a 0.05 level of significance as normal, which makes it easier for investigators and readers to understand. The main argument against the Haybittle–Peto boundary is that some investigators believe that the Haybittle–Peto boundary is too conservative and makes it too difficult to stop a trial.<ref>{{cite journal|doi=10.1016/S0140-6736(05)66516-6|vauthors=Schulz KF, Grimes DA |title=Multiplicity in randomised trials, II: subgroup and interim analyses|journal=Lancet|year=2005|volume=365|pages=1657&ndash;1661|pmid=15885299|issue=9471}}</ref>\n\n{| class=\"wikitable\"\n|+List of [[p-value|''p''-values]] used at each interim analysis, assuming the overall ''p''-value for the trial is 0.05\n|-\n! Number of<br>planned analyses\n! Interim analysis\n! ''p''-value threshold\n|- style=\"background: lightgrey;\"\n| 2 || 1 || 0.001\n|- style=\"background: lightgrey\"\n|  || 2 (final) || 0.05\n|-\n| 3 || 1 || 0.001\n|-\n|  || 2 || 0.001\n|-\n|  || 3 (final) || 0.05\n|- style=\"background: lightgrey\"\n| 4 || 1 || 0.001\n|- style=\"background: lightgrey\"\n|  || 2 || 0.001\n|- style=\"background: lightgrey\"\n|  || 3 || 0.001\n|- style=\"background: lightgrey\"\n|  || 4 (final) || 0.05\n|-\n| 5 || 1 || 0.001\n|-\n|  || 2 || 0.001\n|-\n|  || 3 || 0.001\n|-\n|  || 4 || 0.001\n|-\n|  || 5 (final) || 0.05\n|}\n\n==Synonyms==\n*Peto boundary\n*Peto method\n*Peto criteria\n\n==See also==\n*[[O'Brien–Fleming boundary]]\n*[[Pocock boundary]]\n\n==References==\n{{reflist}}\n\n{{Use dmy dates|date=April 2017}}\n\n{{DEFAULTSORT:Haybittle-Peto boundary}}\n[[Category:Clinical research]]\n[[Category:Sequential experiments]]"
    },
    {
      "title": "Neyer d-optimal test",
      "url": "https://en.wikipedia.org/wiki/Neyer_d-optimal_test",
      "text": "The '''Neyer d-optimal test''' is a sensitivity test.  It can be used to answer questions such as \"How far can a carton of eggs fall, on average, before one breaks?\" If these egg cartons are very expensive, the person running the test would like to minimize the number of cartons dropped, to keep the experiment cheaper and to perform it faster. The Neyer test allows the experimenter to choose the experiment that gives the most information. In this case, given the history of egg cartons which have already been dropped, and whether those cartons broke or not, the Neyer test says \"you will learn the most if you drop the next egg carton from a height of 32.123 meters.\"\n\n== Applications ==\n\nThe Neyer test is useful in any situation when you wish to determine the average amount of a given stimulus needed in order to trigger a response.  Examples:\n\n* Material Toughness - how far does this type of bottle filled with detergent need to fall before it breaks?\n* Drug Efficacy -  how much of this drug is enough to cure this diseases?\n* Toxicology - what percentage of contaminated seed is enough to cause a bird of this species to die?\n* Sensory Threshold - how strong does the light have to be for this photodetector to sense it?\n* Damage Threshold - how loud does the sound have to be in order to damage this microphone ?\n\n== History ==\n\nThe Neyer-d optimal test was described by Barry T. Neyer in 1994. This method has replaced the earlier [[Bruceton analysis]] or \"Up and Down Test\" that was devised by Dixon and Mood in 1948 to allow computation with pencil and paper. Samples are tested at various stimulus levels, and the results (response or no response) noted. The Neyer Test guides the experimenter to pick test levels that provide the maximum amount of information. Unlike previous methods that have been developed, this method requires the use of a computer program to calculate the test levels.\n\nAlthough not directly related to the test method, the [[Likelihood-ratio test|likelihood ratio analysis]] method is often used to analyze the results of tests conducted with the Neyer D-Optimal test. The combined test and analysis methods are commonly known as the Neyer Test.\n\nDror and Steinberg (2008) suggest another [[experimental design]] method which is more efficient than Neyer's, by enabling the usage of a [[D-optimal design]] criterion from the outset of the experiment. Furthermore, their method is extended to deal with situations which are not handled by previous algorithms, including extension from fully sequential designs (updating the plan after each observation) to group-sequential designs (any partition of the experiment to blocks of numerous observations), from a binary response (\"success\" or \"failure\") to any generalized linear model, and from the univariate case to the treatment of multiple predictors (such as designing an experiment to test a response in a medical treatment where the experimenters changes doses of two different drugs).\n\n==See also==\n*[[Optimal design]]\n*[[Safety testing of explosives]]\n\n==References==\n*J. W. Dixon and A. M. Mood (1948), \"A Method for Obtaining and Analyzing Sensitivity Data,\" ''[[Journal of the American Statistical Association]]'', 43, 109-126.\n*B. T. Neyer (1994), \"A D-Optimality-Based Sensitivity Test,\" ''[[Technometrics]]'', 36, 61-70.\n*B. T. Neyer (1992), “Analysis of Sensitivity Tests,” MLM-3736, EG&G Mound Applied Technologies, Miamisburg, Ohio\n*H. A. Dror and D. M. Steinberg (2008), \"Sequential Experimental Designs for Generalized Linear Models,\" ''[[Journal of the American Statistical Association]]'', 103, Number 481, 288-298.\n\n[[Category:Explosives engineering]]\n[[Category:Sequential experiments]]"
    },
    {
      "title": "Pocock boundary",
      "url": "https://en.wikipedia.org/wiki/Pocock_boundary",
      "text": "The '''Pocock boundary''' is a method for determining whether to stop a [[clinical trial]] prematurely. The typical [[clinical trial]] compares two groups of patients. One group are given a [[placebo]] or conventional treatment, while the other group of patients are given the treatment that is being tested. The investigators running the clinical trial will wish to stop the trial early for ethical reasons if the treatment group clearly shows evidence of benefit.  In other words, \"when early results proved so promising it was no longer fair to keep patients on the older drugs for comparison, without giving them the opportunity to change.\"<ref>{{cite news|author=Hall C|title=Heart attacks may be cut by half|newspaper=Daily Telegraph|date=5 September 2005|page=1|url=https://www.telegraph.co.uk/news/uknews/1497648/Heart-attacks-may-be-cut-by-half.html}}</ref>\n\nThe concept was introduced by the medical statistician [[Stuart Pocock]] in 1977. The many reasons underlying when to stop a clinical trial for benefit were discussed in his editorial from 2005.<ref>\n{{cite journal|doi=10.1001/jama.294.17.2228|author=Pocock S|authorlink=Stuart Pocock|title=When (not) to stop a clinical trial for benefit|journal=JAMA|year=2005|volume=294|issue=17|pages=2228&ndash;2230|pmid=16264167}}</ref>\n\n==Details==\nThe Pocock boundary<ref>{{cite journal |doi=10.1093/biomet/64.2.191 |author=Pocock SJ |authorlink=Stuart Pocock |title=Group sequential methods in the design and analysis of clinical trials |journal=Biometrika |year=1977 |volume=64 |issue=2 |pages=191&ndash;9 |jstor=2335684}}</ref> gives a [[p-value|''p''-value]] threshold for each interim analysis which guides the data monitoring committee on whether to stop the trial.  The boundary used depends on the number of interim analyses.\n\n{| class=\"wikitable\"\n|+List of ''p''-values used at each interim analysis, assuming the overall ''p''-value for the trial is 0.05\n|-\n! Number of planned analyses\n! Interim analysis\n! ''p''-value threshold\n|- style=\"background: lightgrey;\"\n| 2 || 1 || 0.0294\n|- style=\"background: lightgrey\"\n|  || 2 (final) || 0.0294\n|-\n| 3 || 1 || 0.0221\n|-\n|  || 2 || 0.0221\n|-\n|  || 3 (final) || 0.0221\n|- style=\"background: lightgrey\"\n| 4 || 1 || 0.0182\n|- style=\"background: lightgrey\"\n|  || 2 || 0.0182\n|- style=\"background: lightgrey\"\n|  || 3 || 0.0182\n|- style=\"background: lightgrey\"\n|  || 4 (final) || 0.0182\n|-\n| 5 || 1 || 0.0158\n|-\n|  || 2 || 0.0158\n|-\n|  || 3 || 0.0158\n|-\n|  || 4 || 0.0158\n|-\n|  || 5 (final) || 0.0158\n|}\n\nThe Pocock boundary is simple to use in that the ''p''-value threshold is  the same at each interim analysis.  The disadvantages are that the number of interim analyses must be fixed at the start and it is not possible under this scheme to add analyses after the trial has started.  Another disadvantage is that investigators and readers frequently do not understand how the ''p''-values are reported: for example, if there are five interim analyses planned, but the trial is stopped after the third interim analysis because the ''p''-value was 0.01, then the overall ''p''-value for the trial is still reported as &lt;0.05 and not as 0.01.<ref>{{cite journal |journal=Lancet |volume=365 |issue=9471 |pages=1657&ndash;1661\n|year=2005 |doi=10.1016/S0140-6736(05)66516-6 |title=Multiplicity in randomised trials II: subgroup and interim analyses |vauthors=Schulz KF, Grimes DA |url=http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(05)66516-6/abstract |pmid=15885299}}</ref>\n\n==See also==\n*[[Haybittle–Peto boundary]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Sequential experiments]]\n[[Category:Clinical research]]\n[[Category:Design of experiments]]"
    },
    {
      "title": "Thompson sampling",
      "url": "https://en.wikipedia.org/wiki/Thompson_sampling",
      "text": "{{More citations needed|date=May 2012}}\n\nIn [[artificial intelligence]], '''Thompson sampling''',<ref name=\"ref1\"/><ref name=\"FnTTutorial\"/> named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration-exploitation dilemma in the [[multi-armed bandit]] problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.\n\n== Description ==\n{{No footnotes|section|date=May 2012}}\n\nConsider a set of contexts <math>\\mathcal{X}</math>, a set of actions <math>\\mathcal{A}</math>, and rewards in <math>\\mathbb{R}</math>. In each round, the player obtains a context <math>x \\in \\mathcal{X}</math>, plays an action <math>a \\in \\mathcal{A}</math> and receives a reward <math>r \\in \\mathbb{R}</math> following a distribution that depends on the context and the issued action. The aim of the player is to play actions such as to maximize the cumulative rewards.\n\nThe elements of Thompson sampling are as follows:\n# a likelihood function <math>P(r|\\theta,a,x)</math>;\n# a set <math>\\Theta</math> of parameters <math>\\theta</math> of the distribution of <math>r</math>;\n# a prior distribution <math>P(\\theta)</math> on these parameters;\n# past observations triplets <math>\\mathcal{D} = \\{(x; a; r)\\}</math>;\n# a posterior distribution <math>P(\\theta|\\mathcal{D}) \\propto P(\\mathcal{D}|\\theta)P(\\theta)</math>, where <math>P(\\mathcal{D}|\\theta)</math> is the likelihood function.\n\nThompson sampling consists in playing the action <math>a^\\ast \\in \\mathcal{A}</math> according to the probability that it maximizes the expected reward, i.e. \n:<math>\\int \\mathbb{I} \\left[ \\mathbb{E}(r|a^\\ast,x,\\theta) = \\max_{a'} \\mathbb{E}(r|a',x,\\theta) \\right] P(\\theta|\\mathcal{D}) d\\theta,</math>\nwhere <math>\\mathbb{I}</math> is the [[indicator function]].\n\nIn practice, the rule is implemented by sampling, in each round, parameters <math>\\theta^\\ast</math> from the posterior <math>P(\\theta|\\mathcal{D})</math>, and choosing the action <math>a^\\ast</math> that maximizes <math>\\mathbb{E}[r|\\theta^\\ast,a^\\ast,x]</math>, i.e. the expected reward given the sampled parameters, the action and the current context. Conceptually, this means that the player instantiates their beliefs randomly in each round, and then acts optimally according to them.  In most practical applications, it is computationally onerous to maintain and sample from a posterior distribution over models.  As such, Thompson sampling is often used in conjunction with approximate sampling techniques.<ref name=\"FnTTutorial\"/>\n\n== History ==\n\nThompson sampling was originally described by Thompson in 1933<ref name=\"ref1\"/> but has been largely ignored by the artificial intelligence community. It was subsequently rediscovered numerous times independently in the context of reinforcement learning.<ref name=\"ref2\"/><ref name=\"ref5\"/><ref name=\"ref6\"/><ref name=\"ref4\"/><ref name=\"ref3\"/><ref name=\"ref7\"/> A first proof of convergence for the bandit case has been shown in 1997.<ref name=\"ref2\"/> The first application to [[Markov decision process]]es was in 2000.<ref name=\"ref6\"/> A related approach (see [[Thompson sampling#Bayesian control rule|Bayesian control rule]]) was published in 2010.<ref name=\"ref5\"/> In 2010 it was also shown that Thompson sampling is ''instantaneously self-correcting''.<ref name=\"ref7\"/> Asymptotic convergence results for contextual bandits were published in 2011.<ref name=\"ref4\"/> Nowadays, Thompson Sampling has been widely used in many online learning problems: Thompson sampling has also been applied to A/B testing in website design and online advertising;<ref name=\"ref9\"/> Thompson sampling has formed the basis for accelerated learning in decentralized decision making;<ref name=\"ref8\"/> a Double Thompson Sampling (D-TS) <ref name=\"Wu2016DTS\" /> algorithm has been proposed for dueling bandits, a variant of traditional MAB, where feedbacks come in the format of pairwise comparison.\n\n== Relationship to other approaches ==\n\n=== Probability matching ===\n\n{{See also|Probability matching}}\n\nProbability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of \"positive\" on 60% of instances, and a class label of \"negative\" on 40% of instances.\n\n=== Bayesian control rule ===\n\nA generalization of Thompson sampling to arbitrary dynamical environments and causal structures, known as '''Bayesian control rule''', has been shown to be the optimal solution to the adaptive coding problem with actions and observations.<ref name=\"ref5\"/> In this formulation, an agent is conceptualized as a mixture over a set of behaviours. As the agent interacts with its environment, it learns the causal properties and adopts the behaviour that minimizes the relative entropy to the behaviour with the best prediction of the environment's behaviour. If these behaviours have been chosen according to the maximum expected utility principle, then the asymptotic behaviour of the Bayesian control rule matches the asymptotic behaviour of the perfectly rational agent.\n\nThe setup is as follows. Let <math>a_1, a_2, \\ldots, a_T</math> be the actions issued by an agent up to time <math>T</math>, and let <math>o_1, o_2, \\ldots, o_T</math> be the observations gathered by the agent up to time <math>T</math>. Then, the agent issues the action <math>a_{T+1}</math> with probability:<ref name=\"ref5\"/>\n:<math>P(a_{T+1}|\\hat{a}_{1:T}, o_{1:T}),</math>\nwhere the \"hat\"-notation <math>\\hat{a}_t</math> denotes the fact that <math>a_t</math> is a causal intervention (see [[Causality]]), and not an ordinary observation. If the agent holds beliefs <math>\\theta \\in \\Theta</math> over its behaviors, then the Bayesian control rule becomes\n:<math>P(a_{T+1}|\\hat{a}_{1:T}, o_{1:T}) = \\int_{\\Theta} P(a_{T+1}|\\theta, \\hat{a}_{1:T}, o_{1:T}) P(\\theta|\\hat{a}_{1:T}, o_{1:T}) \\, d\\theta</math>,\nwhere <math>P(\\theta|\\hat{a}_{1:T}, o_{1:T})</math> is the posterior distribution over the parameter <math>\\theta</math> given actions <math>a_{1:T}</math> and observations <math>o_{1:T}</math>.\n\nIn practice, the Bayesian control amounts to sampling, in each time step, a parameter <math>\\theta^\\ast</math> from the posterior distribution <math>P(\\theta|\\hat{a}_{1:T}, o_{1:T})</math>, where the posterior distribution is computed using Bayes' rule by only considering the (causal) likelihoods of the observations <math>o_1, o_2, \\ldots, o_T</math> and ignoring the (causal) likelihoods of the actions <math>a_1, a_2, \\ldots, a_T</math>, and then by sampling the action <math>a^\\ast_{T+1}</math> from the action distribution <math>P(a_{T+1}|\\theta^\\ast,\\hat{a}_{1:T},o_{1:T})</math>.\n\n=== Upper-Confidence-Bound (UCB) Algorithms ===\n\nThompson sampling and upper-confidence bound algorithms share a fundamental property that underlies many of their theoretical guarantees.  Roughly speaking, both algorithms allocate exploratory effort to actions that might be optimal and are in this sense \"optimistic.\"  Leveraging this property, one can translate regret bounds established for UCB algorithms to Bayesian regret bounds for Thompson sampling<ref name=\"RussoVanRoy2014\"/> or unify regret analysis across both these algorithms and many classes of problems.<ref name=\"RussoVanRoy2013\"/>\n\n== References ==\n\n{{Reflist|refs=\n<ref name=\"ref1\">\nThompson, William R. [https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf \"On the likelihood that one unknown probability exceeds another in view of the evidence of two samples\"]. ''[[Biometrika]]'', 25(3–4):285–294, 1933.\n</ref>\n<ref name=\"ref2\">\nJ. Wyatt. ''Exploration and Inference in Learning from Reinforcement''. Ph.D. thesis, Department of Artificial Intelligence, University of Edinburgh. March 1997.\n</ref>\n<ref name=\"ref3\">\nChapelle, Olivier, and Lihong Li. \"An empirical evaluation of thompson sampling.\" Advances in neural information processing systems. 2011.\nhttp://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling\n</ref>\n<ref name=\"ref4\">\nB. C. May, B. C., N. Korda, A. Lee, and D. S. Leslie. \"Optimistic Bayesian sampling in contextual-bandit problems\". Technical report, Statistics Group, Department of Mathematics, University of Bristol, 2011.\n</ref>\n<ref name=\"ref5\">\nP. A. Ortega and D. A. Braun. \"A Minimum Relative Entropy Principle for Learning and Acting\", ''Journal of Artificial Intelligence Research'', 38, pages 475–511, 2010.\n</ref>\n<ref name=\"ref6\">\nM. J. A. Strens. \"A Bayesian Framework for Reinforcement Learning\", ''Proceedings of the Seventeenth International Conference on Machine Learning'', Stanford University, California, June 29–July 2, 2000, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.1701\n</ref>\n<ref name=\"ref7\">\nO.-C. Granmo. \"Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian Learning Automaton\", ''International Journal of Intelligent Computing and Cybernetics'', 3 (2), 2010, 207-234.\n</ref>\n<ref name=\"ref8\">{{Cite journal | last1 = Granmo | first1 = O. C. | last2 = Glimsdal | first2 = S. | title = Accelerated Bayesian learning for decentralized two-armed bandit based decision making with applications to the Goore Game | doi = 10.1007/s10489-012-0346-z | journal = Applied Intelligence | year = 2012 | pmid =  | pmc = }}\n</ref>\n<ref name=\"ref9\">[[Ian Clarke (computer scientist)|Ian Clarke]].  \"Proportionate A/B testing\", September 22nd, 2011, http://blog.locut.us/2011/09/22/proportionate-ab-testing/\n</ref>\n<ref name=\"Wu2016DTS\">\n{{citation\n | arxiv       = 1604.07101| last1     = Wu\n | first1    = Huasen\n | last2     = Liu\n | first2    = Xin\n | last3     = Srikant\n | first3    = R\n | title     = Double Thompson Sampling for Dueling Bandits\n | year      = 2016\n| bibcode= 2016arXiv160407101W}}\n</ref>\n<ref name=\"FnTTutorial\">\nDaniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband and Zheng Wen (2018), \"A Tutorial on Thompson Sampling\", Foundations and Trends in Machine Learning: Vol. 11: No. 1, pp 1-96. https://www.nowpublishers.com/article/Details/MAL-070\n</ref>\n<ref name=\"RussoVanRoy2013\">\nDaniel J. Russo and Benjamin Van Roy (2013), \"Eluder Dimension and the Sample Complexity of Optimistic Exploration\", Advances in Neural Information Processing Systems 26, pp. 2256-2264. http://papers.nips.cc/paper/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration.pdf\n</ref>\n<ref name=\"RussoVanRoy2014\">\nDaniel J. Russo and Benjamin Van Roy (2014), \"Learning to Optimize Via Posterior Sampling\", Mathematics of Operations Research, Vol. 39, No. 4, pp. 1221-1243, 2014.  https://pubsonline.informs.org/doi/abs/10.1287/moor.2014.0650\n</ref>\n}}\n\n<!--- Categories --->\n[[Category:Artificial intelligence]]\n[[Category:Heuristic algorithms]]\n[[Category:Sequential methods]]\n[[Category:Sequential experiments]]"
    },
    {
      "title": "Linear–quadratic–Gaussian control",
      "url": "https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic%E2%80%93Gaussian_control",
      "text": "In [[control theory]], the '''linear–quadratic–Gaussian''' ('''LQG''') '''control problem''' is one of the most fundamental [[optimal control]] problems. It concerns [[linear system]]s driven by [[additive white Gaussian noise]]. The problem is to determine an output feedback law that is optimal in the sense of minimizing the expected value of a quadratic [[cost functional|cost]] criterion. Output measurements are assumed to be corrupted by Gaussian noise and the initial state, likewise, is assumed to be a Gaussian random vector.\n\nUnder these assumptions an optimal control scheme within the class of linear control laws can be derived by a completion-of-squares argument.<ref name=\"astrom\">{{cite book |author=Karl Johan Astrom |title=Introduction to Stochastic Control Theory |publisher=Academic Press |volume=58 |year=1970 |isbn=0-486-44531-3}}</ref> This control law which is known as the '''LQG''' controller, is unique and it is simply a combination of a [[Kalman filter]] (a linear–quadratic state estimator (LQE)) together with a [[Linear-quadratic regulator|linear–quadratic regulator]] (LQR). The [[separation principle]] states that the state estimator and the state feedback can be designed independently. LQG control applies to both [[LTI systems|linear time-invariant systems]] as well as [[Time-variant system|linear time-varying system]]s, and constitutes a linear dynamic feedback control law that is easily computed and implemented: the LQG controller itself is a dynamic system like the system it controls. Both systems have the same state dimension.\n\nA deeper statement of the separation principle is that the LQG controller is still optimal in a wider class of possibly nonlinear controllers. That is, utilizing a nonlinear control scheme will not improve the expected value of the cost functional. This version of the separation principle is a special case of the [[Separation_principle_in_stochastic_control|separation principle of stochastic control]] which states that even when the process and output noise sources are possibly non-Gaussian [[Martingale (probability theory)|martingales]], as long as the system dynamics are linear, the optimal control separates into an optimal state estimator (which may no longer be a Kalman filter) and an LQR regulator.<ref name=\"lindquist\">{{cite journal |author=Anders Lindquist|title=On Feedback Control of Linear Stochastic Systems |journal=SIAM Journal on Control |volume=11  |pages=323–343 |year=1973}}.</ref><ref name=\"GL2013\">{{cite journal |author=Tryphon T. Georgiou and Anders Lindquist |title=The Separation Principle in Stochastic Control, Redux |journal=IEEE Transactions on Automatic Control |volume=58 |issue=10 |pages=2481–2494 |year=2013 |doi=10.1109/TAC.2013.2259207|arxiv=1103.3005 }}</ref>\n\nIn the classical LQG setting, implementation of the LQG controller may be problematic when the dimension of the system state is large. The '''reduced-order LQG problem''' (fixed-order LQG problem) overcomes this by fixing ''a priori'' the number of states of the LQG controller. This problem is more difficult to solve because it is no longer separable. Also, the solution is no longer unique. Despite these facts numerical algorithms are available<ref name=\"Wil1\">{{cite journal |author1=Van Willigenburg L.G. |author2=De Koning W.L. |title=Numerical algorithms and issues concerning the discrete-time optimal projection equations |journal=European Journal of Control |volume=6 |issue=1 |pages=93–100 |year=2000 |doi=10.1016/s0947-3580(00)70917-4}} [http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=19948&objectType=file  Associated software download from Matlab Central].</ref><ref name=\"Wil2\">{{cite journal |author1=Van Willigenburg L.G. |author2=De Koning W.L. |title=Optimal reduced-order compensators for time-varying discrete-time systems with deterministic and white parameters |journal=Automatica |volume=35 |pages=129–138 |year=1999 |doi=10.1016/S0005-1098(98)00138-1}} [http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=20014&objectType=FILE  Associated software download from Matlab Central].</ref><ref name=\"Bern3\">{{cite journal |author1=Zigic D. |author2=Watson L.T. |author3=Collins E.G. |author4=Haddad W.M. |author5=Ying S. |title=Homotopy methods for solving the optimal projection equations for the H2 reduced order model problem |journal=International Journal of Control |volume=56 | issue=1 | pages=173–191 |year=1996 |doi=10.1080/00207179208934308}}</ref><ref name=\"Had1\">{{cite journal |author1=Collins Jr. E.G |author2=Haddad W.M. |author3=Ying S. |title=A homotopy algorithm for reduced-order dynamic compensation using the Hyland-Bernstein optimal projection equations |journal=Journal of Guidance Control & Dynamics |volume=19 |pages=407–417 |year=1996 |doi=10.2514/3.21633 |issue=2}}</ref> to solve the associated [[optimal projection equations]]<ref name=\"Bern1\">{{cite journal |author1=Hyland D.C |author2=Bernstein D.S. |title=The optimal projection equations for fixed order dynamic compensation |journal=IEEE Transactions on Automatic Control |volume=AC-29 | pages=1034–1037 |year=1984 |doi=10.1109/TAC.1984.1103418 |issue=11}}</ref><ref name=\"Bern2\">{{cite journal |author1=Bernstein D.S. |author2=Davis L.D. |author3=Hyland D.C. |title=The optimal projection equations for reduced-order discrete-time modeling estimation and control |journal=Journal of Guidance Control and Dynamics| volume=9 |issue=3 |pages=288–293 |year=1986 |doi=10.2514/3.20105}}</ref> which constitute necessary and sufficient conditions for a locally optimal reduced-order LQG controller.<ref name=\"Wil1\"/>\n\nLQG optimality does not automatically ensure good robustness properties.<ref>{{cite book |first=Michael |last=Green |first2=David J. N. |last2=Limebeer |title=Linear Robust Control |location=Englewood Cliffs |publisher=Prentice Hall |year=1995 |isbn=0-13-102278-4 |page=27 |url=https://books.google.com/books?id=8NdSAAAAMAAJ&pg=PA27 }}</ref> The robust stability of the closed loop system must be checked separately after the LQG controller has been designed. To promote robustness some of the system parameters may be assumed stochastic instead of deterministic. The associated more difficult control problem leads to a similar optimal controller of which only the controller parameters are different.<ref name=\"Wil2\"/>\n\nFinally, the LQG controller is also used to control perturbed non-linear systems.<ref name=\"Athans\">{{cite journal |author=Athans M. |title=The role and use of the stochastic Linear-Quadratic-Gaussian problem in control system design |journal=IEEE Transactions on Automatic Control |volume=AC-16 |issue=6 |pages=529–552 |year=1971 |doi=10.1109/TAC.1971.1099818}}</ref>\n\n==Mathematical description of the problem and solution==\n\n===Continuous time===\nConsider the [[continuous-time]] linear dynamic system\n\n: <math>\\dot{\\mathbf{x}}(t) = A(t) \\mathbf{x}(t) + B(t) \\mathbf{u}(t) +  \\mathbf{v}(t),</math>\n: <math>\\mathbf{y}(t) = C(t) \\mathbf{x}(t) + \\mathbf{w}(t),</math>\n\nwhere <math>{\\mathbf{x}}</math> represents the vector of state variables of the system, <math>{\\mathbf{u}}</math> the vector of control inputs and <math>{\\mathbf{y}}</math> the vector of measured outputs available for feedback. Both additive white Gaussian system noise <math>\\mathbf{v}(t)</math> and additive white Gaussian measurement noise <math>\\mathbf{w}(t)</math> affect the system. Given this system the objective is to find the control input history <math>{\\mathbf{u}}(t)</math> which at every time <math>{\\mathbf{}}t</math> may depend linearly only on the past measurements <math>{\\mathbf{y}}(t'), 0 \\leq t' < t</math> such that the following cost function is minimized:\n\n: <math> J = \\mathbb{E}\\left[{\\mathbf{x}^\\mathrm T}(T)F{\\mathbf{x}}(T)+ \\int_{0}^{T} {\\mathbf{x}^\\mathrm T}(t)Q(t){\\mathbf{x}}(t) + {\\mathbf{u}^\\mathrm T}(t)R(t){\\mathbf{u}}(t)\\,dt \\right],</math>\n\n: <math> F \\ge 0,\\quad Q(t) \\ge 0,\\quad R(t) > 0, </math>\n\nwhere <math>\\mathbb{E}</math> denotes the [[expected value]]. The final time (horizon) <math>{\\mathbf{}}T</math> may be either finite or infinite. If the horizon tends to infinity the first term <math>{\\mathbf{x}}^\\mathrm T(T)F{\\mathbf{x}}(T)</math> of the cost function becomes negligible and irrelevant to the problem. Also to keep the costs finite the cost function has to be taken to be <math>{\\mathbf{}}J/T</math>.\n\nThe LQG controller that solves the LQG control problem is specified by the following equations:\n\n: <math> \\dot{\\hat{\\mathbf{x}}}(t) = A(t)\\hat{\\mathbf{x}}(t) + B(t){\\mathbf{u}}(t)+L(t) \\left( {\\mathbf{y}}(t)-C(t)\\hat{\\mathbf{x}}(t) \\right),  \\quad \\hat{\\mathbf{x}}(0) = \\mathbb{E} \\left[ {\\mathbf{x}}(0) \\right], </math>\n\n: <math> {\\mathbf{u}}(t)= -K(t) \\hat{\\mathbf{x}}(t).</math>\n\nThe matrix <math>{\\mathbf{}}L(t)</math> is called the '''Kalman gain''' of the associated [[Kalman filter]] represented by the first equation. At each time <math>{\\mathbf{}}t</math> this filter generates estimates <math>\\hat{\\mathbf{x}}(t)</math> of the state <math>{\\mathbf{x}}(t)</math> using the past measurements and inputs. The Kalman gain <math>{\\mathbf{}}L(t)</math> is computed from the matrices <math>{\\mathbf{}}A(t), C(t)</math>, the two intensity matrices <math>\\mathbf{}V(t), W(t)</math> associated to the white Gaussian noises <math>\\mathbf{v}(t)</math> and <math>\\mathbf{w}(t)</math> and finally <math>\\mathbb{E}\\left[{\\mathbf{x}}(0){\\mathbf{x}}^\\mathrm T(0) \\right]</math>. These five matrices determine the Kalman gain through the following associated matrix Riccati differential equation:\n\n: <math> \\dot{P}(t) = A(t)P(t)+P(t)A^\\mathrm T(t)-P(t)C^\\mathrm T(t){\\mathbf{}}W^{-1}(t)\nC(t)P(t)+V(t),</math>\n\n: <math> P(0)= \\mathbb{E} \\left[{\\mathbf{x}}(0){\\mathbf{x}}^\\mathrm T(0) \\right].</math>\n\nGiven the solution <math>P(t), 0 \\leq t \\leq T</math> the Kalman gain equals\n\n: <math> {\\mathbf{}}L(t) = P(t)C^\\mathrm T(t)W^{-1}(t). </math>\n\nThe matrix <math>{\\mathbf{}}K(t)</math> is called the '''feedback gain''' matrix. This matrix is determined by the matrices <math>{\\mathbf{}}A(t), B(t), Q(t), R(t)</math> and <math>{\\mathbf{}}F</math> through the following associated matrix Riccati differential equation:\n\n: <math> -\\dot{S}(t) = A^\\mathrm T(t)S(t)+S(t)A(t)-S(t)B(t)R^{-1}(t)B^\\mathrm T(t)S(t)+Q(t),</math>\n\n: <math>  {\\mathbf{}}S(T) = F.</math>\n\nGiven the solution <math>{\\mathbf{}}S(t), 0 \\leq t \\leq T</math> the feedback gain equals\n\n: <math> {\\mathbf{}}K(t) = R^{-1}(t)B^\\mathrm T(t)S(t).</math>\n\nObserve the similarity of the two matrix Riccati differential equations, the first one running forward in time, the second one running backward in time. This similarity is called '''duality'''. The first matrix Riccati differential equation solves the linear–quadratic estimation problem (LQE). The second matrix Riccati differential equation solves the [[Linear-quadratic regulator|linear–quadratic regulator]] problem (LQR). These problems are dual and together they solve the linear–quadratic–Gaussian control problem (LQG). So the LQG problem separates into the LQE and LQR problem that can be solved independently. Therefore, the LQG problem is called '''separable'''.\n\nWhen <math>{\\mathbf{}}A(t), B(t), C(t), Q(t), R(t)</math> and the noise intensity matrices <math>\\mathbf{}V(t)</math>, <math>\\mathbf{}W(t)</math> do not depend on <math>{\\mathbf{}}t</math> and when <math>{\\mathbf{}}T</math> tends to infinity the LQG controller becomes a time-invariant dynamic system. In that case both matrix Riccati differential equations may be replaced by the two associated [[algebraic Riccati equation]]s.\n\n===Discrete time===\nSince the [[discrete-time]] LQG control problem is similar to the one in continuous-time, the description below focuses on the mathematical equations.\n\nThe discrete-time linear system equations are\n\n: <math>{\\mathbf{x}}_{i+1} = A_i\\mathbf{x}_i + B_i \\mathbf{u}_i +  \\mathbf{v}_i,</math>\n\n: <math>\\mathbf{y}_{i} = C_{i} \\mathbf{x}_i + \\mathbf{w}_i.</math>\n\nHere <math>\\mathbf{}i</math> represents the discrete time index and <math>\\mathbf{v}_{i}, \\mathbf{w}_{i}</math> represent discrete-time Gaussian white noise processes with covariance matrices <math>\\mathbf{}V_{i}, W_{i}</math> respectively.\n\nThe quadratic cost function to be minimized is\n\n: <math> J = \\mathbb{E}\\left[{\\mathbf{x}}^\\mathrm T_{N}F{\\mathbf{x}}_{N}+ \\sum_{i=0}^{N-1}( \\mathbf{x}_i^\\mathrm T Q_i \\mathbf{x}_i + \\mathbf{u}_i^\\mathrm T R_i \\mathbf{u}_i )\\right],</math>\n\n: <math> F \\ge 0, Q_i \\ge 0, R_i > 0. \\,  </math>\n\nThe discrete-time LQG controller is\n\n:<math>\\hat{\\mathbf{x}}_{i+1}=A_i\\hat{\\mathbf{x}}_i+B_i{\\mathbf{u}}_i+L_{i+1} \\left({\\mathbf{y}}_{i+1}-C_{i+1} \\left\\{ A_i \\hat{\\mathbf{x}}_i + B_i u_i \\right\\} \\right), \\qquad \\hat{\\mathbf{x}}_0=\\mathbb{E}[{\\mathbf{x}}_0]</math>,\n\n:<math> \\mathbf{u}_i=-K_i\\hat{\\mathbf{x}}_i. \\, </math>\n\nThe Kalman gain equals\n\n: <math> {\\mathbf{}}L_i = P_iC ^\\mathrm T _i(C_iP_iC ^\\mathrm T _i + W_i)^{-1}, </math>\n\nwhere <math>{\\mathbf{}}P_i</math> is determined by the following matrix Riccati difference equation that runs forward in time:\n\n: <math> P_{i+1} = A_i \\left( P_i - P_i C ^\\mathrm T _i \\left( C_i P_i C ^\\mathrm T _i+W_i \\right)^{-1} C_i P_i \\right) A ^\\mathrm T _i+V_i, \\qquad P_0=\\mathbb{E} [\\left( {\\mathbf{x}}_0 - \\hat{\\mathbf{x}}_0\\right)\\left({\\mathbf{x}}_0- \\hat{\\mathbf{x}}_0\\right)^\\mathrm T]. </math>\n\nThe feedback gain matrix equals\n\n: <math> {\\mathbf{}}K_i = (B^\\mathrm T_iS_{i+1}B_i + R_i)^{-1}B^\\mathrm T_iS_{i+1}A_i </math>\n\nwhere <math>{\\mathbf{}}S_i</math> is determined by the following matrix Riccati difference equation that runs backward in time:\n\n: <math> S_i = A^\\mathrm T_i \\left( S_{i+1} - S_{i+1}B_i \\left( B^\\mathrm T_iS_{i+1}B_i+R_i \\right)^{-1} B^\\mathrm T_i S_{i+1} \\right) A_i+Q_i, \\quad S_N=F.</math>\n\nIf all the matrices in the problem formulation are time-invariant and if the horizon <math>{\\mathbf{}}N</math> tends to infinity the discrete-time LQG controller becomes time-invariant. In that case the matrix Riccati difference equations may be replaced by their associated discrete-time [[algebraic Riccati equation]]s. These determine the time-invariant linear–quadratic estimator and the time-invariant [[Linear-quadratic regulator|linear–quadratic regulator]] in discrete-time. To keep the costs finite instead of <math>{\\mathbf{}}J</math> one has to consider <math>{\\mathbf{}}J/N</math> in this case.\n\n==See also==\n\n*[[Stochastic control]]\n*[[Witsenhausen's counterexample]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n* {{cite book |first=Robert F. |last=Stengel |title=Optimal Control and Estimation |location=New York |publisher=Dover |year=1994 |isbn=0-486-68200-5 |url=https://books.google.com/books?id=jDjPxqm7Lw0C }}\n\n{{DEFAULTSORT:Linear-quadratic-Gaussian control}}\n[[Category:Optimal control]]\n[[Category:Control theory]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Mabinogion sheep problem",
      "url": "https://en.wikipedia.org/wiki/Mabinogion_sheep_problem",
      "text": "In probability theory, the '''Mabinogion sheep problem''' or '''Mabinogian urn''' is a problem in [[stochastic control]] introduced by {{harvs|txt|last=Williams|first=David|authorlink=David Williams (mathematician)|year=1991|loc=15.3}}, who named it after a herd of magic sheep in the Welsh epic the ''[[Mabinogion]]''.\n\n==Statement==\n{{quotebox|width=30%|quote=\nAnd he came towards a valley, through which ran a river; and the borders of the valley were wooded, and on each side of the river were level meadows. And on one side of the river he saw a flock of white sheep, and on the other a flock of black sheep. And whenever one of the white sheep bleated, one of the black sheep would cross over and become white; and when one of the black sheep bleated, one of the white sheep would cross over and become black |source=[[Peredur son of Efrawg|Peredur the son of Evrawk]], from the [[Mabinogion]], translated by [[Lady Charlotte Guest]] (1812–1895)<ref>{{cite web|title=Peredur the Son of Evrawc|url=http://d.lib.rochester.edu/camelot/text/guest-peredur|publisher=University of Rochester|accessdate=11 May 2017}}</ref>}}\n\nAt time ''t''&nbsp;=&nbsp;0 there is a herd of sheep each of which is black or white. At each time ''t''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;... a sheep is selected at random, and a sheep of the opposite color (if one exists) is changed to be the same color as the selected sheep. At any time one may remove as many sheep (of either color) as one wishes from the flock. The problem is to do this in such a way as to maximize the expected final number of black sheep.\n\nThe optimal solution at each step is to remove just enough white sheep so that there are more black sheep than white sheep.\n\n==References==\n{{Reflist}}\n\n*{{citation\n|mr=1404309\n|last=Chan|first=Terence\n|title = Some diffusion models for the Mabinogion sheep problem of Williams\n|journal=Advances in Applied Probability\n|volume=28|year=1996|issue=3|pages=763–783\n|doi=10.2307/1428180\n}}\n*{{citation|first=David|last=Williams|title=Probability with [[Martingale (probability theory)|martingale]]s|series= Cambridge Mathematical Textbooks|publisher= Cambridge University Press |year=1991}}\n\n[[Category:Probability problems]]\n[[Category:Stochastic control]]\n[[Category:Optimal decisions]]"
    },
    {
      "title": "Merton's portfolio problem",
      "url": "https://en.wikipedia.org/wiki/Merton%27s_portfolio_problem",
      "text": "{{For|Merton’s one-period portfolio problem|Mutual fund separation theorem}}\n\n'''Merton's portfolio problem''' is a well known problem in continuous-time [[finance]] and in particular [[intertemporal portfolio choice]].  An investor must choose how much to consume and must allocate his wealth between stocks and a risk-free asset so as to maximize expected [[utility]].  The problem was formulated and solved by [[Robert C. Merton]] in 1969 both for finite lifetimes and for the infinite case.<ref>{{Cite journal| first = R. C. | authorlink1 = Robert C. Merton| title = Lifetime Portfolio Selection under Uncertainty: the Continuous-Time Case | journal = The Review of Economics and Statistics | volume = 51| issue = 3| last = Merton | pages = 247–257| date = 1 August 1969 | issn = 0034-6535 | doi = 10.2307/1926560| jstor =  1926560 }}</ref> Research has continued to extend and generalize the model to include factors like [[transaction cost]]s and bankruptcy.\n\n==Problem statement==\n\nThe investor lives from time 0 to time&nbsp;''T''; his wealth at time ''t'' is denoted ''W''<sub>''t''</sub>.  He starts with a known initial wealth ''W''<sub>0</sub> (which may include the present value of wage income).  At time ''t'' he must choose what amount of his wealth to consume, ''c''<sub>''t''</sub>, and what fraction of wealth to invest in a stock portfolio, ''&pi;''<sub>''t''</sub> (the remaining fraction 1&nbsp;&minus;&nbsp;''&pi;''<sub>''t''</sub> being invested in the risk-free asset).\n\nThe objective is\n\n: <math> \\max E \\left[ \\int_0^T e^{-\\rho s}u(c_s) \\, ds +  \\epsilon^\\gamma e^{-\\rho T}u(W_T) \\right] </math>\n\nwhere ''E'' is the expectation operator, ''u'' is a known [[utility function]] (which applies both to consumption and to the terminal wealth, or bequest, ''W''<sub>''T''</sub>), ''&epsilon;'' parameterizes the desired level of bequest, and ''&rho;'' is the subjective discount rate.\n\nThe wealth evolves according to the [[stochastic differential equation]]\n\n::<math>d W_t = [(r + \\pi_t(\\mu-r))W_t - c_t ] \\, dt +W_t \\pi_t \\sigma \\, dB_t </math>\n\nwhere ''r'' is the risk-free rate, (''&mu;'',&nbsp;''&sigma;'') are the expected return and volatility of the stock market and ''dB''<sub>''t''</sub> is the increment of the [[Wiener process]], i.e. the stochastic term of the SDE.\n\nThe utility function is of the [[constant relative risk aversion]] (CRRA) form:\n\n: <math> u(x) = \\frac{x^{1-\\gamma}}{1-\\gamma},</math>\n\nwhere <math>\\gamma</math> is a constant which expresses the investor's risk aversion: the higher the gamma, the more reluctance to own stocks.\n\nConsumption cannot be negative: ''c''<sub>''t''</sub>&nbsp;≥&nbsp;0, while ''&pi;''<sub>''t''</sub> is unrestricted (that is borrowing or shorting stocks is allowed).\n\nInvestment opportunities are assumed constant, that is ''r'',&nbsp;''&mu;'',&nbsp;''&sigma;'' are known and constant, in this (1969) version of the model, although Merton allowed them to change in his [[intertemporal CAPM]] (1973).\n\n==Solution==\n\nSomewhat surprisingly for an [[optimal control]] problem, a closed-form solution exists. The optimal consumption and stock allocation depend on wealth and time as follows:\n\n:<math>\\pi(W,t) = \\frac{\\mu-r}{\\sigma^2\\gamma}.</math>\n\n(This expression is commonly referred to as Merton's fraction. Note that ''W'' and ''t'' do not appear on the right-hand side; this implies that a constant fraction of wealth is invested in stocks, no matter what the age or prosperity of the investor).\n\n:<math>c(W,t)= \\begin{cases}\\nu \\left(1+(\\nu\\epsilon-1)e^{-\\nu(T-t)}\\right)^{-1} W&\\textrm{if}\\;T<\\infty\\;\\textrm{and}\\;\\nu\\neq0\\\\(T-t+\\epsilon)^{-1}W&\\textrm{if}\\;T<\\infty\\;\\textrm{and}\\;\\nu=0\\\\\\nu W&\\textrm{if}\\; T=\\infty\\end{cases}</math>\n\nwhere <math>0\\le\\epsilon\\ll1</math> and\n:<math>\\begin{align}\\nu&=\\left(\\rho-(1-\\gamma)\\left(\\frac{(\\mu-r)^2}{2\\sigma^2\\gamma}+r\\right)\\right)/\\gamma \\\\&=\\rho/\\gamma-(1-\\gamma)\\left(\\frac{(\\mu-r)^2}{2\\sigma^2\\gamma^2}+\\frac r{\\gamma}\\right)\\\\&=\\rho/\\gamma-(1-\\gamma)(\\pi(W,t)^2/2\\sigma^2+ r/\\gamma)\\\\&=\\rho/\\gamma-(1-\\gamma)((\\mu-r)\\pi(W,t)/2\\gamma+ r/\\gamma).\\end{align}</math>\nThe variable <math>\\rho</math> is the subjective utility discount rate.<ref>{{Cite journal | last1 = Merton | first1 = R. C. | authorlink1 = Robert C. Merton| title = Optimum consumption and portfolio rules in a continuous-time model | doi = 10.1016/0022-0531(71)90038-X | journal = Journal of Economic Theory | volume = 3 | issue = 4 | pages = 373–413 | year = 1971 | pmid =  | pmc = }}</ref>{{rp|401}})\n\n==Extensions==\n\nMany variations of the problem have been explored, but most do not lead to a simple closed-form solution.\n\n* Flexible retirement age can be taken into account.<ref>{{Cite journal | last1 = Bodie | first1 = Z. | last2 = Merton | first2 = R. C. | authorlink2 = Robert C. Merton| last3 = Samuelson | first3 = W. F. | doi = 10.1016/0165-1889(92)90044-F | title = Labor supply flexibility and portfolio choice in a life cycle model | journal = Journal of Economic Dynamics and Control | volume = 16 | issue = 3–4 | pages = 427 | year = 1992 | pmid =  | pmc = | url = http://www.people.hbs.edu/rmerton/laborsupplyflexibility.pdf}}</ref>\n* A utility function other than CRRA can be used.\n* Transaction costs can be introduced.  For ''proportional transaction costs'' the problem was solved by Davis and Norman in 1990.<ref>{{Cite journal | last1 = Davis | first1 = M. H. A. | authorlink1 = Mark H. A. Davis| last2 = Norman | first2 = A. R. | doi = 10.1287/moor.15.4.676 | title = Portfolio Selection with Transaction Costs | journal = Mathematics of Operations Research | volume = 15 | issue = 4 | pages = 676 | year = 1990 | url = http://www2.imperial.ac.uk/~mdavis/docs/Davis-Norman.pdf| jstor = 3689770| pmid =  | pmc = }}</ref> It is one of the few cases of [[stochastic singular control]] where the solution is known.  For a graphical representation, the amount invested in each of the two assets can be plotted on the ''x''- and ''y''-axes; three diagonal lines through the origin can be drawn: the upper boundary, the Merton line and the lower boundary.  The '''Merton line''' represents portfolios having the stock/bond proportion derived by Merton in the absence of transaction costs.  As long as the point which represents the current portfolio is near the Merton line, i.e. between the upper and the lower boundary, no action needs to be taken.  When the portfolio crosses above the upper or below the lower boundary, one should rebalance the portfolio to bring it back to that boundary.  In 1994 Shreve and Soner provided an analysis of the problem via the [[Hamilton–Jacobi–Bellman equation]] and its viscosity solutions.<ref>{{Cite journal | last1 = Shreve | first1 = S. E. | last2 = Soner | first2 = H. M. | doi = 10.1214/aoap/1177004966 | title = Optimal Investment and Consumption with Transaction Costs | journal = The Annals of Applied Probability | volume = 4 | issue = 3 | pages = 609 | jstor = 2245058| year = 1994 | pmid =  | pmc = | url = http://repository.cmu.edu/cgi/viewcontent.cgi?article=1466&context=math }}</ref>\n\n:When there are ''fixed transaction costs'' the problem was addressed by Eastman and Hastings in 1988.<ref>{{Cite journal | last1 = Eastham | first1 = J. F. | last2 = Hastings | first2 = K. J. | doi = 10.1287/moor.13.4.588 | title = Optimal Impulse Control of Portfolios | journal = Mathematics of Operations Research | volume = 13 | issue = 4 | pages = 588 | year = 1988 | jstor = 3689945| pmid =  | pmc = }}</ref> A numerical solution method was provided by Schroder in 1995.<ref>{{cite journal | last = Schroder | first = M. | title = Optimal Portfolio Selection with Fixed Transaction Costs: Numerical Solutions | journal = Working Paper | location =  Michigan State University | year=1995 | url = https://www.msu.edu/~schrode7/numerical.pdf }}</ref>\n\n:Finally Morton and Pliska<ref>{{Cite journal | last1 = Morton | first1 = A. J. | last2 = Pliska | first2 = S. R. | doi = 10.1111/j.1467-9965.1995.tb00071.x | title = Optimal Portfolio Management with Fixed Transaction Costs | journal = Mathematical Finance | volume = 5 | issue = 4 | pages = 337 | year = 1995 | pmid =  | pmc = }}</ref> considered trading costs that are proportional to the wealth of the investor for logarithmic utility. Although this cost structure seems unrepresentative of real life transaction costs, it can be used to find approximate solutions in cases with additional assets,<ref>http://cmcm.uni-kl.de/fileadmin/downloads/vortraege/20100329/Korn_optimal_portfolios_with_transaction_costs.pdf</ref> for example individual stocks, where it becomes difficult or intractable to give exact solutions for the problem.\n\n* The assumption of constant investment opportunities can be relaxed.  This requires a model for how <math>r,\\mu,\\sigma</math> change over time.  An interest rate model could be added and would lead to a portfolio containing bonds of different maturities.  Some authors have added a stochastic volatility model of stock market returns.\n* Bankruptcy can be incorporated. This problem was solved by Karatzas, Lehoczky, Sethi and Shreve in 1986.<ref>{{Cite book | last1 = Karatzas | first1 = I. | last2 = Lehoczky | first2 = J. P. | last3 = Sethi | first3 = S. P. | last4 = Shreve | first4 = S. E. | chapter = Explicit solution of a general consumption/investment problem | doi = 10.1007/BFb0041165 | title = Stochastic Differential Systems | series = Lecture Notes in Control and Information Sciences | volume = 78 | pages = 209 | year = 1985 | isbn = 3-540-16228-3 | pmid =  | pmc = }}</ref> Many models incorporating bankruptcy are collected in Sethi (1997).<ref>{{Cite journal | last1 = Sethi | first1 = S. P. | doi = 10.1007/978-1-4615-6257-3 | title = Optimal Consumption and Investment with Bankruptcy | year = 1997 | isbn = 978-1-4613-7871-6 | pmid =  | pmc = }}</ref>\n\n==References==\n\n{{reflist}}\n*{{Cite journal | first1 = Ioannis| last1 = Karatzas| first2 = Steven E.| last2 = Shreve| authorlink2 = Steven E. Shreve| doi = 10.1007/b98840 | title = Methods of Mathematical Finance | series = Stochastic Modelling and Applied Probability | volume = 39 | year = 1998 | isbn = 978-0-387-94839-3 | pmid =  | pmc = }}\n*Merton R.C.: ''Continuous Time Finance'', Blackwell (1990).\n\n[[Category:Financial economics]]\n[[Category:Stochastic control]]\n[[Category:Portfolio theories]]\n[[Category:Intertemporal economics]]"
    },
    {
      "title": "Multiplier uncertainty",
      "url": "https://en.wikipedia.org/wiki/Multiplier_uncertainty",
      "text": "In [[macroeconomics]], '''multiplier uncertainty''' is lack of perfect knowledge of the [[Multiplier (economics)|multiplier]] effect of a particular policy action, such as a monetary or fiscal policy change, upon the intended target of the policy. For example, a [[fiscal policy]] maker may have a prediction as to the value of the [[fiscal multiplier]]—the ratio of the effect of a [[government spending]] change on [[GDP]] to the size of the government spending change—but is not likely to know the exact value of this ratio. Similar uncertainty may surround the magnitude of effect of a change in the [[monetary base]] or its growth rate upon some target variable, which could be the [[money supply]], the [[exchange rate]], the [[inflation rate]], or GDP.\n\nThere are several policy implications of multiplier uncertainty: (1) If the multiplier uncertainty is [[correlation|uncorrelated]] with additive uncertainty, its presence causes greater cautiousness to be optimal (the policy tools should be used to a lesser extent). (2) In the presence of multiplier uncertainty, it is no longer redundant to have more policy tools than there are targeted economic variables. (3) [[certainty equivalence principle|Certainty equivalence]] no longer applies under quadratic [[loss function|loss]]: optimal policy is not equivalent to a policy of [[expected value of including uncertainty|ignoring uncertainty]].\n\n==Effect of multiplier uncertainty on the optimal magnitude of policy==\n\nFor the simplest possible case,<ref>{{cite journal |last=Brainard |first=William |year=1967 |title=Uncertainty and the effectiveness of policy |journal=[[American Economic Review]] |volume=57 |issue=2 |pages=411–425 |jstor=1821642 }}</ref> let ''P'' be the size of a policy action (a government spending change, for example), let ''y'' be the value of the target variable (GDP for example), let ''a'' be the policy multiplier, and let ''u'' be an additive term capturing both the linear intercept and all unpredictable components of the determination of ''y''.  Both ''a'' and ''u'' are random variables (assumed here for simplicity to be uncorrelated), with respective means E''a'' and E''u'' and respective variances <math>\\sigma^2_a</math> and <math>\\sigma^2_u</math>. Then\n\n:<math>y = aP + u.</math>\n\nSuppose the policy maker cares about the expected squared deviation of GDP from a preferred value <math>y_d</math>; then its [[loss function]] ''L'' is [[quadratic loss function|quadratic]] so that the objective function, expected loss, is given by:\n\n:<math>\\text{E}L = \\text{E}(y-y_d)^2 = \\text{E}(aP + u - y_d)^2 = [\\text{E}(aP + u - y_d)]^2 + \\text{var} (aP + u - y_d) = [(\\text{E}a)P + \\text{E}u - y_d]^2 + P^2 \\sigma^2_a + \\sigma^2_u.</math>\n\nwhere the last equality assumes there is no covariance between ''a'' and ''u''. Optimizing with respect to the policy variable ''P'' gives the optimal value ''P''<sup>''opt''</sup>:\n\n:<math>P^{opt} = \\frac{(\\text{E}a)(y_d - \\text{E}u)}{(\\text{E}a)^2 + \\sigma^2_a}.</math>\n\nHere the last term in the numerator is the gap between the preferred value ''y''<sub>''d''</sub> of the target variable and its expected value E''u'' in the absence of any policy action.  If there were no uncertainty about the policy multiplier, <math>\\sigma^2_a</math> would be zero, and policy would be chosen so that the contribution of policy (the policy action ''P'' times its known multiplier ''a'') would be to exactly close this gap, so that with the policy action E''y'' would equal ''y''<sub>''d''</sub>.  However, the optimal policy equation shows that, to the extent that there is multiplier uncertainty (the extent to which <math>\\sigma^2_a > 0</math>), the magnitude of the optimal policy action is diminished.\n\nThus the basic effect of multiplier uncertainty is to make policy actions more cautious, although this effect can be modified in more complicated models.\n\n==Multiple targets or policy instruments==\n\nThe above analysis of one target variable and one policy tool can readily be extended to multiple targets and tools.<ref name=Mitchell>{{cite journal |last=Mitchell |first=Douglas W. |year=1990 |title=The efficient policy frontier under parameter uncertainty and multiple tools |journal=Journal of Macroeconomics |volume=12 |issue=1 |pages=137–145 |doi=10.1016/0164-0704(90)90061-E }}</ref>  In this case a key result is that, unlike in the absence of multiplier uncertainty, it is not superfluous to have more policy tools than targets: with multiplier uncertainty, the more tools are available the lower expected loss can be driven.\n\n==Analogy to portfolio theory==\n\nThere is a mathematical and conceptual analogy between, on the one hand, policy optimization with multiple policy tools having multiplier uncertainty, and on the other hand, [[Modern portfolio theory|portfolio optimization]] involving multiple investment choices having rate-of-return uncertainty.<ref name=Mitchell/>\nThe usages of the policy variables correspond to the holdings of the risky assets, and the uncertain policy multipliers correspond to the uncertain rates of return on the assets.  In both models, [[mutual fund theorem]]s apply: under certain conditions, the optimal portfolios of all investors regardless of their preferences, or the optimal policy mixes of all policy makers regardless of their preferences, can be expressed as linear combinations of any two optimal portfolios or optimal policy mixes.\n\n==Dynamic policy optimization==\n\nThe above discussion assumed a static world in which policy actions and outcomes for only one moment in time were considered.  However, the analysis generalizes to a context of multiple time periods in which both policy actions take place and target variable outcomes matter, and in which time lags in the effects of policy actions exist.  In this dynamic [[stochastic control]] context with multiplier uncertainty,<ref>{{cite book |last=Chow |first=Gregory P. |year=1976 |title=Analysis and Control of Dynamic Economic Systems |location=New York |publisher=Wiley |isbn=0-471-15616-7 }}</ref><ref>{{cite journal |last=Turnovsky |first=Stephen |year=1976 |title=Optimal stabilization policies for stochastic linear systems: The case of correlated multiplicative and additive disturbances |journal=[[Review of Economic Studies]] |volume=43 |issue=1 |pages=191–194 |jstor=2296741 }}</ref><ref>{{cite journal |last=Turnovsky |first=Stephen |year=1974 |title=The stability properties of optimal economic policies |journal=American Economic Review |volume=64 |issue=1 |pages=136–148 |jstor=1814888 }}</ref> a key result is that the \"certainty equivalence principle\" does not apply: while in the absence of multiplier uncertainty (that is, with only additive uncertainty) the optimal policy with a quadratic loss function coincides with what would be decided if the uncertainty were ignored, this no longer holds in the presence of multiplier uncertainty.\n\n==References==\n{{reflist}}\n\n[[Category:Macroeconomic policy]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Optimal projection equations",
      "url": "https://en.wikipedia.org/wiki/Optimal_projection_equations",
      "text": "In [[control theory]],  '''optimal projection equations''' <ref name=\"Bern1\">{{cite journal |author1=Hyland D.C |author2=Bernstein D.S. |title=The optimal projection equations for fixed order dynamic compensation |journal=IEEE Transactions on Automatic Control |volume=AC-29 | pages=1034–1037 |year=1984 |doi=10.1109/TAC.1984.1103418 |issue=11}}</ref><ref name=\"Bern2\">{{cite journal |author1=Bernstein D.S. |author2=Davis L.D. |author3=Hyland D.C. |title=The optimal projection equations for reduced-order discrete-time modeling estimation and control |journal=Journal of Guidance Control and Dynamics| volume=9 |issue=3 |pages=288–293 |year=1986 |doi=10.2514/3.20105}}</ref><ref name=\"Haddad1\">{{cite journal |author1=Haddad W.M. |author2=Tadmor G. |title=Reduced-order LQG controllers for linear time-varying plants |journal=Systems & Control Letters| volume=20 |issue=2 |pages=87–97 |year=1993 |doi=10.1016/0167-6911(93)90020-7}}</ref> constitute [[necessary and sufficient conditions]] for a locally optimal reduced-order LQG controller.<ref name=\"Wil1\"/>\n\nThe [[Linear-quadratic-Gaussian control|Linear-Quadratic-Gaussian (LQG) control problem]] is one of the most fundamental [[optimal control]] problems. It concerns uncertain [[linear system]]s disturbed by [[additive white Gaussian noise]], incomplete state information (i.e. not all the state variables are measured and available for feedback) also disturbed by additive white Gaussian noise and quadratic [[cost functional|cost]]s. Moreover, the solution is unique and constitutes a linear dynamic feedback control law that is easily computed and implemented. Finally the LQG controller is also fundamental to the optimal perturbation control of non-linear systems.<ref name=\"Athans\">{{cite journal |author=Athans M. |title=The role and use of the stochastic Linear-Quadratic-Gaussian problem in control system design |journal=IEEE Transactions on Automatic Control |volume=AC-16 |issue=6 |pages=529–552 |year=1971 |doi=10.1109/TAC.1971.1099818}}</ref>\n\nThe LQG controller itself is a dynamic system like the system it controls. Both systems have the same state dimension. Therefore, implementing the LQG controller may be problematic if the dimension of the system state is large. The '''reduced-order LQG problem''' (fixed-order LQG problem) overcomes this by fixing a-priori the number of states of the LQG controller. This problem is more difficult to solve because it is no longer separable. Also the solution is no longer unique. Despite these facts numerical algorithms are available <ref name=\"Wil1\">{{cite journal |author1=Van Willigenburg L.G. |author2=De Koning W.L. |title=Numerical algorithms and issues concerning the discrete-time optimal projection equations |journal=European Journal of Control |volume=6 |issue=1 |pages=93–100 |year=2000 |doi=10.1016/s0947-3580(00)70917-4}} [http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=19948&objectType=file  Associated software download from Matlab Central].</ref><ref name=\"Wil2\">{{cite journal |author1=Van Willigenburg L.G. |author2=De Koning W.L. |title=Optimal reduced-order compensators for time-varying discrete-time systems with deterministic and white parameters |journal=Automatica |volume=35 |pages=129–138 |year=1999 |doi=10.1016/S0005-1098(98)00138-1}} [http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=20014&objectType=FILE  Associated software download from Matlab Central].</ref><ref name=\"Bern3\">{{cite journal |author1=Zigic D. |author2=Watson L.T. |author3=Collins E.G. |author4=Haddad W.M. |author5=Ying S. |title=Homotopy methods for solving the optimal projection equations for the H2 reduced order model problem |journal=International Journal of Control |volume=56 | issue=1 | pages=173–191 |year=1996 |doi=10.1080/00207179208934308}}</ref><ref name=\"Had1\">{{cite journal |author1=Collins Jr. E.G |author2=Haddad W.M. |author3=Ying S. |title=A homotopy algorithm for reduced-order dynamic compensation using the Hyland-Bernstein optimal projection equations |journal=Journal of Guidance Control & Dynamics |volume=19 |pages=407–417 |year=1996 |doi=10.2514/3.21633 |issue=2}}</ref> to solve the associated optimal projection equations.\n\n==Mathematical problem formulation and solution==\n\n===Continuous-time===\n\nThe reduced-order LQG control problem is almost identical to the [[Linear-quadratic-Gaussian control|conventional full-order LQG control problem]]. Let <math> \\hat{\\mathbf{x}}_r(t) </math> represent the state of the reduced-order LQG controller. Then the only difference is that the state dimension <math> n_r=dim(\\hat{\\mathbf{x}}_r(t)) </math> of the LQG controller is a-priori fixed to be smaller than <math> n=dim({\\mathbf{x}}(t)) </math>, the state dimension of the controlled system.\n\nThe reduced-order LQG controller is represented by the following equations:\n\n: <math> \\dot{\\hat{\\mathbf{x}}}_r(t) = A_r(t)\\hat{\\mathbf{x}}_r(t) + B_r(t){\\mathbf{u}}(t)+K_r(t) \\left( {\\mathbf{y}}(t)-C_r(t)\\hat{\\mathbf{x}}_r(t) \\right),\\hat{\\mathbf{x}}_r(0)={\\mathbf{x}}_r(0),</math>\n\n: <math> {\\mathbf{u}}(t)= -L_r(t) \\hat{\\mathbf{x}}_r(t).</math>\n\nThese equations are deliberately stated in a format that equals that of the [[Linear-quadratic-Gaussian control|conventional full-order LQG controller]]. For the reduced-order LQG control problem it is  convenient to rewrite them as\n\n: <math> \\dot{\\hat{\\mathbf{x}}}_r(t) = F_r(t)\\hat{\\mathbf{x}}_r(t) + K_r(t) {\\mathbf{y}}(t),\\hat{\\mathbf{x}}_r(0)={\\mathbf{x}}_r(0),</math>\n\n: <math> {\\mathbf{u}}(t)= -L_r(t) \\hat{\\mathbf{x}}_r(t),</math>\n\nwhere\n\n: <math>{\\mathbf{}}F_r(t)=A_r(t)-B_r(t)L_r(t)-K_r(t)C_r(t).</math>\n\nThe matrices <math>{\\mathbf{}}F_r(t), K_r(t), L_r(t) </math> and <math>{\\mathbf{x}}_r(0)</math> of the reduced-order LQG controller are determined by the so-called '''optimal projection equations''' ('''OPE''').<ref name=\"Haddad1\"/>\n\nThe square optimal projection matrix <math>{\\mathbf{}}\\tau(t)</math> with dimension <math>{\\mathbf{}}n</math> is central to the '''OPE'''. The rank of this matrix is almost everywhere equal to <math>{\\mathbf{}}n_r.</math> The associated projection is an oblique projection: <math>{\\mathbf{}}\\tau^2(t)=\\tau(t).</math> The '''OPE''' constitute four matrix differential equations. The first two equations listed below are generalizations of the matrix Riccati differential equations associated to the [[Linear-quadratic-Gaussian control|conventional full-order LQG controller]]. In these equations <math>{\\mathbf{}}\\tau_\\perp(t)</math> denotes <math>{\\mathbf{}}I_n-\\tau(t)</math> where <math>{\\mathbf{}}I_n</math> is the identity matrix of dimension <math>{\\mathbf{}}n</math>.\n\n: <math> \\dot{P}(t) = A(t)P(t)+P(t)A'(t)-P(t)C'(t)W^{-1}(t)\nC(t)P(t)+V(t)</math>\n::::<math>+\\tau_\\perp (t)P(t)C'(t)W^{-1}(t)\nC(t)P(t)\\tau'_\\perp (t),</math>\n\n: <math> P(0)= E \\left({\\mathbf{x}}(0){\\mathbf{x}}'(0) \\right),</math>\n\n: <math> -\\dot{S}(t) = A'(t)S(t)+S(t)A(t)-S(t)B(t)R^{-1}(t)B'(t)S(t)+Q(t)</math>\n::::<math>+ \\tau'_\\perp (t)S(t)B(t)R^{-1}(t)B'(t)S(t) \\tau_\\perp (t),</math>\n\n: <math>  {\\mathbf{}}S(T) = F.</math>\n\nIf the dimension of the LQG controller is not reduced, that is if <math>{\\mathbf{}}n=n_r</math>, then <math>\\tau(t)=I_n, \\tau_\\perp(t)=0</math> and the two equations above become the uncoupled matrix Riccati differential equations associated to the [[Linear-quadratic-Gaussian control|conventional full-order LQG controller]]. If <math>{\\mathbf{}}n_r<n</math> the two equations are coupled by the oblique projection <math>{\\mathbf{}}\\tau(t).</math> This reveals why the reduced-order LQG problem is not '''separable'''. The oblique projection <math>{\\mathbf{}}\\tau(t)</math> is determined from two additional matrix differential equations which involve '''rank conditions'''. Together with the previous two matrix differential equations these are the '''OPE'''. To state the additional two matrix differential equations it is convenient to introduce the following two matrices:\n\n: <math> \\Psi_1(t)=(A(t)-B(t)R^{-1}(t)B'(t)S(t))\\hat{P}(t)+\\hat{P}(t)\n(A(t)-B(t)R^{-1}(t)B'(t)S(t))'</math>\n:::: <math>{\\mathbf{}}+P(t)C'(t)W^{-1}(t)C(t)P(t),</math>\n\n: <math> \\Psi_2(t)=(A(t)-P(t)C'(t)W^{-1}(t)\nC(t))'\\hat{S}(t)+\\hat{S}(t)(A(t)-P(t)C'(t)W^{-1}(t)C(t))</math>\n:::: <math>{\\mathbf{}}+S(t)B(t)R^{-1}(t)B'(t)S(t).</math>\n\nThen the two additional matrix differential equations that complete the '''OPE''' are as follows:\n\n: <math> \\dot{\\hat{P}}(t)=1/2 \\left(\\tau(t)\\Psi_1(t)+\\Psi_1(t)\\tau'(t) \\right),\\hat{P}(0)=E({\\mathbf{x}}(0))E({\\mathbf{x}}(0))', rank(\\hat{P}(t))=n_r</math>  almost everywhere,\n\n: <math> -\\dot{\\hat{S}}(t)=1/2 \\left(\\tau'(t)\\Psi_2(t)+\\Psi_2(t)\\tau(t) \\right),\\hat{S}(T)=0, rank(\\hat{S}(t))=n_r</math> almost everywhere,\n\nwith\n\n: <math>{\\mathbf{}}\\tau(t)= \\hat{P}(t) \\hat{S}(t) \\left( \\hat{P}(t) \\hat{S}(t) \\right)^*.</math>\n\nHere * denotes the group generalized inverse or [[Drazin inverse]] that is unique and given by\n\n: <math>{\\mathbf{}}A^*=A(A^3)^+A.</math>\n\nwhere + denotes the [[Moore-Penrose pseudoinverse]].\n\nThe matrices <math>{\\mathbf{}}P(t),S(t),\\hat{P}(t),\\hat{S}(t)</math> must all be '''nonnegative symmetric'''. Then they constitute a solution of the '''OPE''' that determines the reduced-order LQG controller matrices <math>{\\mathbf{}}F_r(t), K_r(t), L_r(t) </math> and <math>{\\mathbf{x}}_r(0)</math>:\n\n: <math>{\\mathbf{}}F_r(t)=H(t)\\left( A(t)-P(t)C'(t)W^{-1}(t)\nC(t)-B(t)R^{-1}(t)B'(t)S(t) \\right)G(t)+\\dot{H}(t)G'(t),</math>\n\n: <math>{\\mathbf{}}K_r(t)=H(t)P(t)C'(t)W^{-1}(t),</math>\n\n: <math>{\\mathbf{}}L_r(t)=R^{-1}(t)B'(t)S(t)G'(t),</math>\n\n: <math>{\\mathbf{x}}_r(0)=H(0)E({\\mathbf{x}}(0)).</math>\n\nIn the equations above the matrices <math>{\\mathbf{}}G(t),H(t)</math> are two matrices with the following properties:\n\n: <math>{\\mathbf{}}G'(t)H(t)=\\tau(t),G(t)H'(t)=I_{n_r}</math> almost everywhere.\n\nThey can be obtained from a projective factorization of <math>{\\mathbf{}}\\hat{P}(t)\\hat{S}(t)</math>.<ref name=\"Wil1\"/>\n\nThe '''OPE''' can be stated in many different ways that are all equivalent. To identify the equivalent representations the following identities are especially useful:\n\n: <math>{\\mathbf{}}\\tau(t)\\hat{P}(t)=\\hat{P}(t)\\tau'(t)=\\hat{P}(t), \\tau'(t)\\hat{S}(t)=\\hat{S}(t)\\tau(t)=\\hat{S}(t)</math>\n\nUsing these identities one may for instance rewrite the first two of the optimal projection equations as follows:\n\n: <math> \\dot{P}(t) = A(t)P(t)+P(t)A'(t)-P(t)C'(t)W^{-1}(t)C(t)P(t)+V(t)+\\tau_\\perp(t)\\Psi_1(t)\\tau'_\\perp (t),</math>\n\n: <math> P(0)= E \\left({\\mathbf{x}}(0){\\mathbf{x}}'(0) \\right),</math>\n\n: <math> -\\dot{S}(t) = A'(t)S(t)+S(t)A(t)-S(t)B(t)R^{-1}(t)B'(t)S(t)+Q(t)+\\tau'_\\perp\\Psi_2(t)\\tau_\\perp(t),</math>\n\n: <math>  {\\mathbf{}}S(T) = F.</math>\n\nThis representation is both relatively simple and suitable for numerical computations.\n\nIf all the matrices in the reduced-order LQG problem formulation are time-invariant and if the horizon <math>{\\mathbf{}}T</math> tends to infinity, the optimal reduced-order LQG controller becomes time-invariant and so do the '''OPE'''.<ref name=\"Bern1\"/> In that case the derivatives on the left hand side of the '''OPE''' are zero.\n\n===Discrete-time===\n\nSimilar to the continuous-time case, in the discrete-time case the difference with the [[Linear-quadratic-Gaussian control|conventional discrete-time full-order LQG problem]] is the a-priori fixed reduced-order <math>{\\mathbf{}}n_r<n</math> of the LQG controller state dimension. As in continuous-time, to state the '''discrete-time OPE''' it is convenient to introduce the following two matrices:\n\n: <math>{\\mathbf{}}\\Psi^1_i=\\left(A_i-B_i(B'_iS_{i+1}B_i+R_i)^{-1}B'_iS_{i+1}A_i)\\right)\\hat{P}_i\n\\left(A_i-B_i(B'_iS_{i+1}B_i+R_i)^{-1}B'_iS_{i+1}A_i)\\right)'</math>\n:::: <math>{\\mathbf{}}+A_iP_iC'_i(C_iP_iC'_i+W_i)^{-1}C_iP_iA'_i</math>\n\n: <math>{\\mathbf{}}\\Psi^2_{i+1}=\\left(A_i-A_iP_iC'_i(C_iP_{i}C'_i+W_i)^{-1}C_i\\right)'\\hat{S}_{i+1}\n\\left(A_i-A_iP_iC'_i(C_iP_{i}C'_i+W_i)^{-1}C_i\\right)</math>\n:::: <math>{\\mathbf{}}+A'_iS_{i+1}B_i(B'_iS_{i+1}B_i+R_i)^{-1}B'_iS_{i+1}A_i</math>\n\nThen the '''discrete-time OPE''' is\n\n: <math>{\\mathbf{}}P_{i+1} = A_i \\left( P_i - P_i C'_i \\left( C_i P_i C'_i+W_i \\right)^{-1} C_i P_i \\right) A'_i+V_i+\\tau_{\\perp i+1}\\Psi^1_i \\tau'_{\\perp i+1}, P_0=E \\left( {\\mathbf{x}}_0{\\mathbf{x'}}_0 \\right)</math>.\n: <math>{\\mathbf{}}S_{i} = A'_i \\left( S_{i+1} - S_{i+1}B_i \\left( B'_iS_{i+1}B_i+R_i \\right)^{-1} B'_i S_{i+1} \\right) A_i+Q_i+\\tau'_{\\perp i}\\Psi^2_{i+1} \\tau_{\\perp i}, S_N=F</math>.\n:<math>{\\mathbf{}}\\hat{P}_{i+1}=1/2(\\tau_{i+1}\\Psi_i^1+\\Psi_i^1\\tau'_{i+1}),\\hat{P}_0=E({\\mathbf{x}}(0))E({\\mathbf{x}}(0))', rank(\\hat{P}_i)=n_r</math>  almost everywhere,\n:<math>{\\mathbf{}}\\hat{S}_{i}=1/2(\\tau'_{i}\\Psi_{i+1}^2+\\Psi_{i+1}^2\\tau_{i}),\\hat{S}_N=0, rank(\\hat{S}_i)=n_r</math> almost everywhere.\n\nThe oblique projection matrix is given by\n\n: <math>{\\mathbf{}}\\tau_i=\\hat{P}_i\\hat{S}_i \\left(\\hat{P}_i\\hat{S}_i \\right)^*.</math>\n\nThe '''nonnegative symmetric matrices''' <math>{\\mathbf{}}P_i,S_i,\\hat{P}_i,\\hat{S}_i</math> that solve the '''discrete-time OPE''' determine the reduced-order LQG controller matrices <math>{\\mathbf{}}F_i^r, K_i^r, L_i^r </math> and <math>{\\mathbf{x}}_0^r</math>:\n\n: <math>{\\mathbf{}}F_i^r=H_{i+1}\\left( A_i-P_i C'_i \\left( C_i P_i C'_i+W_i \\right)^{-1}C_i-B_i\\left( B'_iS_{i+1}B_i+R_i \\right)^{-1} B'_i S_{i+1}\\right)G'_i,</math>\n\n: <math>{\\mathbf{}}K_i^r=H_{i+1}P_i C'_i \\left( C_i P_i C'_i+W_i \\right)^{-1},</math>\n\n: <math>{\\mathbf{}}L_i^r=\\left( B'_iS_{i+1}B_i+R_i \\right)^{-1} B'_i S_{i+1}G'_i,</math>\n\n: <math>{\\mathbf{x}}_0^r=H_0E({\\mathbf{x}}_0).</math>\n\nIn the equations above the matrices <math>{\\mathbf{}}G_i,H_i</math> are two matrices with the following properties:\n\n: <math>{\\mathbf{}}G'_iH_i=\\tau_i, G_iH'_i=I_{n_r}</math> almost everywhere.\n\nThey can be obtained from a projective factorization of <math>{\\mathbf{}}\\hat{P}_i\\hat{S}_i</math>.<ref name=\"Wil1\"/> To identify equivalent representations of the '''discrete-time OPE''' the following identities are especially useful:\n\n: <math>{\\mathbf{}}\\tau_i\\hat{P}_i=\\hat{P}_i\\tau'_i=\\hat{P}_i, \\tau'_i\\hat{S}_i=\\hat{S}_i\\tau_i=\\hat{S}_i</math>\n\nAs in the continuous-time case if all the matrices in the problem formulation are time-invariant and if the horizon <math>{\\mathbf{}}N</math> tends to infinity the reduced-order LQG controller becomes time-invariant. Then the discrete-time OPE converge to a steady state solution that determines the time-invariant reduced-order LQG controller.<ref name=\"Bern2\"/>\n\nThe '''discrete-time OPE''' apply also to discrete-time systems with '''variable state, input and output dimensions''' (discrete-time systems with time-varying dimensions).<ref name=\"Wil2\"/> Such systems arise in the case of digital controller design if the sampling occurs asynchronously.\n\n==References==\n<references/>\n\n[[Category:Optimal control]]\n[[Category:Control theory]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Robust control",
      "url": "https://en.wikipedia.org/wiki/Robust_control",
      "text": "In [[control theory]], '''robust control''' is an approach to controller design that explicitly deals with uncertainty. Robust control methods are designed to function properly provided that uncertain parameters or disturbances are found within some (typically [[compact set|compact]]) set. Robust methods aim to achieve robust performance and/or [[Stability theory|stability]] in the presence of bounded modelling errors.\n\nThe early methods of [[Hendrik Wade Bode|Bode]] and others were fairly robust; the state-space methods invented in the 1960s and 1970s were sometimes found to lack robustness,<ref>M. Athans, Editorial on the LQG problem, IEEE Trans. Autom. Control 16 (1971), no. 6, 528.</ref> prompting research to improve them. This was the start of the theory of robust control, which took shape in the 1980s and 1990s and is still active today.\n\nIn contrast with an [[adaptive control]] policy, a robust control policy is static; rather than adapting to measurements of variations, the controller is designed to work assuming that certain variables will be unknown but\nbounded.<ref name=\"Ackermann\">{{citation|surname1=J. Ackermann|title=Robuste Regelung |publisher=Springer-Verlag|year=1993|language=German}} (Section 1.5) In German; an English version is also available</ref><ref>[http://control.ee.ethz.ch/~morari/ Manfred Morari : Homepage]</ref>\n\n==When is a control method said to be robust?==\nInformally, a controller designed for a particular set of parameters is said to be robust if it also works well under a different set of assumptions.  High-gain feedback is a simple example of a robust control method; with sufficiently high gain, the effect of any parameter variations will be negligible. From the closed loop transfer function perspective, high open loop gain leads to substantial disturbance rejection in the face of system parameter uncertainty. Other examples on robust control include [[Sliding mode control|sliding mode]] and [[terminal sliding mode]] control.\n\nThe major obstacle to achieving high loop gains is the need to maintain system closed loop stability. Loop shaping which allows stable closed loop operation can be a technical challenge.\n\nRobust control systems often incorporate advanced topologies which include multiple feedback loops and feed-forward paths. The control laws may be represented by high order transfer functions required to simultaneously accomplish desired disturbance rejection performance with robust closed loop operation.\n\nHigh-gain feedback is the principle that allows simplified models of [[operational amplifier]]s and emitter-degenerated [[bipolar transistor]]s to be used in a variety of different settings. This idea was already well understood by [[Hendrik Wade Bode|Bode]] and [[Harold Stephen Black|Black]] in 1927.\n\n==The modern theory of robust control==\nThe theory of robust control began in the late 1970s and early 1980s and soon developed a number of techniques for dealing with bounded system uncertainty.<ref>[http://routh.usc.edu/pub/safonov/safo97d.pdf Safonov: editorial]</ref><ref>[http://www.ee.lsu.edu/kemin/essentials.htm Kemin Zhou: Essentials of Robust Control]</ref>\n\nProbably the most important example of a robust control technique is [[H-infinity loop-shaping]], which was developed by [[Duncan McFarlane]] and [[Keith Glover]] of [[Cambridge University]]; this method minimizes the [[Sensitivity (control systems)|sensitivity]] of a system over its frequency spectrum, and this guarantees that the system will not greatly deviate from expected trajectories when disturbances enter the system.\n\nAn emerging area of robust control from application point of view is [[sliding mode control]] (SMC), which is a variation of [[variable structure control]] (VSC). The robustness properties of SMC with respect to matched uncertainty as well as the simplicity in design attracted a variety of applications.\n\nWhile robust control has been traditionally dealt with along deterministic approaches, in the last two decades this approach has been criticized on the basis that it is too rigid to describe real uncertainty, while it often also leads to over conservative solutions. Probabilistic robust control has been introduced as an alternative, see e.g.<ref>G. Calafiore and M.C. Campi. \"The scenario approach to robust control design,\" IEEE Transactions on Automatic Control, 51(5). 742–753, 2006. [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1632303&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1632303]</ref> that interprets robust control within the so-called [[scenario optimization]] theory.\n\nAnother example is [[Loop transformation|loop transfer recovery]] (LQG/LTR),<ref>http://www.nt.ntnu.no/users/skoge/book.html Multivariable Feedback Control\nAnalysis and Design (2nd Edition)</ref> which was developed to overcome the robustness problems of [[linear-quadratic-Gaussian control]] (LQG) control.\n\nOther robust techniques includes [[quantitative feedback theory]] (QFT), [[Passivity (engineering)|passivity based control]], [[Lyapunov stability|Lyapunov based control]], etc.\n\nWhen system behavior varies considerably in normal operation, multiple control laws may have to be devised. Each distinct control law addresses a specific system behavior mode. An example is a computer hard disk drive. Separate robust control system modes are designed in order to address the rapid magnetic head traversal operation, known as the seek, a transitional settle operation as the magnetic head approaches its destination, and a track following mode during which the disk drive performs its data access operation.\n\nOne of the challenges is to design a control system that addresses these diverse system operating modes and enables smooth transition from one mode to the next as quickly as possible.\n\nSuch state machine driven composite control system is an extension of the gain scheduling idea where the entire control strategy changes based upon changes in system behavior.\n\n==See also==\n* [[Control theory]]\n* [[Control engineering]]\n* [[Fractional-order control]]\n* [[H infinity|H-infinity control]]\n* [[H-infinity loop-shaping]]\n* [[Sliding mode control]]\n* [[Intelligent control]]\n* [[Process control]]\n* [[Robust decision making]]\n* [[Root locus]]\n* [[Servomechanism]]\n* [[Stable polynomial]]\n* [[State space (controls)]]\n* [[System identification]]\n* [[Stability radius]]\n* [[Iso-damping]]\n* [[Active Disturbance Rejection Control]]\n* [[Quantitative feedback theory]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n{{refbegin}}\n* {{cite journal\n | author = Ray, L.R.\n |author2=Stengel, R.F.\n  | year = 1991\n | title = Stochastic Robustness of Linear-Time-Invariant Control Systems\n | journal = IEEE Transactions on Automatic Control\n | volume = 36\n | issue = 1\n | pages = 82–87\n | url = http://www.princeton.edu/~stengel/RayTAC1991.pdf\n | doi = 10.1109/9.62270\n}}\n* {{Cite journal\n |author1=V. Barbu  |author2=S. S. Sritharan\n  |lastauthoramp=yes | year = 1998\n | title = H-infinity Control Theory of Fluid Dynamics\n | journal = [[Proceedings of the Royal Society A]]\n | volume = 545\n |pages= 3009–3033\n |url= http://www.nps.edu/Academics/Schools/GSEAS/SRI/R19.pdf\n |doi=10.1098/rspa.1998.0289\n}}\n* {{cite book\n | author = Dullerud, G.E.\n |author2=Paganini, F.\n  | year = 2000\n | title = A Course in Robust Control Theory: A Convex Approach\n | publisher = Springer Verlag New York\n | isbn = 0-387-98945-5\n}}\n* {{cite book\n |author=Bhattacharya |author2=Apellat |author3=Keel\n | year = 2000\n | title = Robust Control-The Parametric Approach\n | publisher = Prentice Hall PTR\n | isbn = 0-13-781576-X\n | url = http://www.ece.tamu.edu/~bhatt/books/robustcontrol/robustcontrol.pdf\n}}\n* {{cite book\n | author = Zhou, Kemin\n |author2=Doyle C., John\n | year = 1999\n | title = Essentials of Robust Control\n | publisher = Prentice Hall\n | isbn = 0-13-525833-2\n}}\n* {{cite book\n | author = Morari, Manfred\n |author2=Zafiriou, Evanghelos\n  | year = 1989\n | title = Robust Process Control\n | publisher = Prentice Hall\n | isbn = 0-13-782153-0\n | url = http://www.google.ch/books?id=HEcbgfyZEFoC&hl\n}}\n* {{cite book\n | author = Mahmoud S., Magdi\n |author2=Munro, Neil\n  | year = 1989\n | title = Robust Control and Filtering for Time-Delay Systems\n | publisher = Marcel Dekker Inc.\n | isbn = 0-8247-0327-8\n}}\n* {{cite book\n | author = Calafiore, G.\n | editor=Dabbene, F.\n | year = 2006\n | title = Probabilistic and Randomized Methods for Design under Uncertainty\n | publisher = Springer Verlag London Ltd.\n | isbn = 978-1-84628-094-8\n}}\n* {{cite book | author = Briat, Corentin | year = 2015 | title = Linear Parameter-Varying and Time-Delay Systems. Analysis, Observation, Filtering & Control | publisher = Springer Verlag Heidelberg | isbn = 978-3-662-44049-0}}\n{{refend}}\n\n{{DEFAULTSORT:Robust Control}}\n[[Category:Control theory]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Separation principle",
      "url": "https://en.wikipedia.org/wiki/Separation_principle",
      "text": "In [[control theory]], a '''separation principle''', more formally known as a '''principle of separation of estimation and control''', states that under some assumptions the problem of designing an optimal feedback controller for a stochastic system can be solved by designing an optimal [[state observer|observer]] for the state of the system, which feeds into an optimal deterministic [[controller (control theory)|controller]] for the system.  Thus the problem can be broken into two separate parts, which facilitates the design.\n\nThe first instance of such a principle is in the setting of deterministic linear systems, namely that if a stable [[state observer|observer]] and a stable state [[feedback]] are designed for a [[LTI system theory|linear time-invariant system]], then the combined observer and feedback is [[BIBO stability|stable]]. The separation principle does not hold in general for non-linear systems in general. Another instance of the separation principle arises in the setting of linear stochastic systems, namely that state estimation (possibly nonlinear) together with an optimal state feedback controller designed to minimize a quadratic cost, is optimal for the stochastic control problem with output measurements. When process and observation noise are Gaussian, the optimal solution separates into a [[Kalman filter]] and a [[linear-quadratic regulator]]. This is known as [[linear-quadratic-Gaussian control]]. More generally, under suitable conditions and when the noise is a martingale (with possible jumps), again a separation principle applies and is known as the [[separation principle in stochastic control]]<ref name=\"astrom\">{{cite book |author=Karl Johan Astrom |title=Introduction to Stochastic Control Theory |publisher=Academic Press |volume=58 |year=1970 |isbn=0-486-44531-3}}.</ref>\n<ref name=\"duncanvaraiya\">{{cite journal |author=Tyrone Duncan and Pravin Varaiya |title=On the solutions of a stochastic control system |journal=SIAM J. Control |volume=9 |issue=3 |pages=354–371 |year=1971}}.</ref>\n<ref name=\"davisvaraiya\">{{cite journal |author=M.H.A. Davis and P. Varaiya |title=Information states for stochastic systems |journal=J. Math. Anal. Applications |volume=37 |pages=384–402 |year=1972}}.</ref>\n<ref name=\"lindquist\">{{cite journal |author=Anders Lindquist|title=On Feedback Control of Linear Stochastic Systems |journal=SIAM Journal on Control |volume=11  |pages=323–343 |year=1973}}.</ref>\n<ref name=\"Bensoussan\">{{cite book |author=A. Bensoussan |title=Stochastic Control of Partially Observable Systems |publisher=Cambridge University Press |year=1992}}.</ref>\n<ref name=\"GL2013\">{{cite journal |author=Tryphon T. Georgiou and Anders Lindquist |title=The Separation Principle in Stochastic Control, Redux |journal=IEEE Transactions on Automatic Control |volume=58 |issue=10 |pages=2481–2494 |year=2013 |doi=10.1109/TAC.2013.2259207|arxiv=1103.3005 }}.</ref>. A separation principle also exists for the control of quantum systems.\n\n\n== Proof of separation principle for deterministic LTI systems ==\n\nConsider a deterministic LTI system:\n\n<math>\n\\begin{align}\n\\dot{x}(t) & = A x(t) + B u(t) \\\\\ny(t) & = C x(t)\n\\end{align}\n</math>\n\nwhere\n\n:<math>u(t)</math> represents the input signal,\n:<math>y(t)</math> represents the output signal, and\n:<math>x(t)</math> represents the internal state of the system.\n\nWe can design an observer of the form\n\n:<math>\\dot{\\hat{x}} = ( A - L C ) \\hat{x} + B u + L y \\, </math>\n\nand state feedback\n\n:<math>u(t) = - K \\hat{x} \\, .</math>\n\nDefine the error ''e'':\n\n:<math>e = x - \\hat{x} \\, .</math>\n\nThen\n\n:<math>\\dot{e} = (A - L C) e \\, </math>\n\n:<math>u(t) = - K ( x - e ) \\, .</math>\n\nNow we can write the closed-loop dynamics as\n\n: <math>\\begin{bmatrix}\n\\dot{x} \\\\\n\\dot{e} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nA - B K & BK \\\\\n0 & A - L C \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ne \\\\\n\\end{bmatrix}.</math>\n\nSince this is [[triangular matrix|triangular]], the [[eigenvalues]] are just those of ''A''&nbsp;&minus;&nbsp;''BK'' together with those of ''A''&nbsp;&minus;&nbsp;''LC''.<ref>Proof can be found in this math.stackexchange [http://math.stackexchange.com/questions/21454/prove-that-the-eigenvalues-of-a-block-matrix-are-the-combined-eigenvalues-of-its question.]</ref>  Thus the stability of the observer and feedback are [[Linear independence|independent]].\n\n==References==\n{{reflist}}\n\n* Brezinski, Claude. ''Computational Aspects of Linear Control (Numerical Methods and Algorithms)''. Springer, 2002.\n\n[[Category:Control theory]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Separation principle in stochastic control",
      "url": "https://en.wikipedia.org/wiki/Separation_principle_in_stochastic_control",
      "text": "\nThe '''separation principle''' is one of the fundamental principles of [[stochastic control theory]], which states that the problems of optimal control and state estimation can be decoupled under certain conditions. In its most basic formulation it deals with a linear stochastic system\n:<math>\\begin{align}\n   dx &  =A(t)x(t)\\,dt+B_1(t)u(t)\\,dt+B_2(t)\\,dw \\\\\n   dy &  =C(t)x(t)\\,dt +D(t)\\,dw\n\\end{align}</math>\nwith a state process <math>x</math>, an output process <math>y</math> and a control <math>u</math>, where <math>w</math> is a vector-valued [[Wiener process]], <math>x(0)</math> is a zero-mean [[Gaussian]] random vector independent of <math>w</math>, <math>y(0)=0</math>, and <math>A</math>, <math>B_1</math>, <math>B_2</math>, <math>C</math>, <math>D</math> are matrix-valued functions which generally are taken to be continuous of bounded variation. Moreover, <math>DD'</math> is  nonsingular on some interval <math>[0,T]</math>. The problem is to design an output feedback law <math>\\pi:\\,  y \\mapsto u</math> which maps the observed process <math>y</math> to the control input <math>u</math> in a nonanticipatory manner so as to minimize the functional\n:<math>\nJ(u) = \\mathbb{E}\\left\\{ \\int_0^T x(t)'Q(t)x(t)\\,dt+\\int_0^Tu(t)'R(t)u(t)\\,dt +x(T)'Sx(T)\\right\\},\n</math>\nwhere <math>\\mathbb{E}</math> denotes expected value, prime (<math>'</math>) denotes transpose. and <math>Q</math> and <math>R</math> are continuous matrix functions of bounded variation, <math> Q(t)</math> is positive semi-definite and <math>R(t)</math> is positive definite for all <math>t</math>. Under suitable conditions, which need to be properly stated,  <math>\\pi</math> can be chosen in the form\n:<math>\nu(t)=K(t)\\hat x(t),\n</math>\nwhere <math>\\hat x(t)</math> is the linear least-squares estimate of the state vector <math>x(t)</math> obtained from the [[Kalman filter]]\n:<math>\nd\\hat x=A(t)\\hat x(t)\\,dt+B_1(t)u(t)\\,dt +L(t)(dy-C(t)\\hat x(t)\\,dt),\\quad \\hat x(0)=0,\n</math>\nwhere <math>K</math> is the gain of the optimal [[linear-quadratic regulator]] obtained by taking <math>B_2=D=0</math> and <math>x(0)</math> deterministic, and where <math>L</math> is the  [[Kalman filter|Kalman gain]]. There is also a non-Gaussian version of this problem (to be discussed below) where the Wiener process  <math>w</math> is replaced by a more general square-integrable martingale with possible jumps.<ref name=\"GL2013\">{{cite journal |author=Tryphon T. Georgiou and Anders Lindquist |title=The Separation Principle in Stochastic Control, Redux |journal=IEEE Transactions on Automatic Control |volume=58 |issue=10 |pages=2481–2494 |year=2013 |doi=10.1109/TAC.2013.2259207}}.</ref>. In this case, the Kalman filter needs to be replace by a nonlinear filter providing an estimate of the (strict sense) conditional mean\n:<math>\n\\hat{x}(t)= \\operatorname E\\{ x(t)\\mid {\\cal Y}_t\\},\n</math>\nwhere\n:<math>\n{\\cal Y}_t:=\\sigma\\{ y(\\tau), \\tau\\in [0,t]\\}, \\quad 0\\leq t\\leq T,\n</math>\nis the ''filtration'' generated by the output process; i.e., the family of increasing sigma fields representing the data as it is produced.\n\nIn the early literature on the separation principle it was common to allow as admissible controls <math>u</math> all processes that are ''adapted'' to the filtration <math>\\{{\\cal Y}_t, \\, 0\\leq t\\leq T\\}</math>. This is equivalent to allowing all non-anticipatory [[Borel function]]s as feedback laws, which raises the question of existence of a unique solution to the equations of the feedback loop. Moreover, one needs to exclude the possibility that a nonlinear controller extracts more information from the data than what is possible with a linear control law<ref name=\"lindquist\">{{cite journal |author=Anders Lindquist|title=On Feedback Control of Linear Stochastic Systems |journal=SIAM Journal on Control |volume=11  |issue=2 |pages=323–343 |year=1973|doi=10.1137/0311025 }}.</ref>.\n\n==Choices of the class of admissible control laws==\n\nLinear-quadratic control problems are often solved by a completion-of-squares argument. In our present context we have\n:<math>\nJ(u)=\\operatorname{E}\\left\\{ \\int_0^T(u-Kx)'R(u-Kx) \\, dt\\right\\}\n+\\text{terms that do not depend on }u,\n</math>\nin which the first term takes the form<ref name=\"astrom\">{{cite book |author=Karl Johan Astrom |title=Introduction to Stochastic Control Theory |publisher=Academic Press |volume=58 |year=1970 |isbn=978-0-486-44531-1}}.</ref>\n:<math>\\begin{align}\n\\operatorname{E}\\left\\{ \\int_0^T(u-Kx)'R(u-Kx)\\,dt\\right\\}=\\operatorname{E}\\left\\{\\int_0^T[(u-K\\hat{x})'R(u-K\\hat{x})+\\operatorname{tr}(K'RK\\Sigma)] \\, dt\\right\\},\n\\end{align}</math>\nwhere <math>\\Sigma</math> is the covariance matrix\n:<math>\n\\Sigma(t):=\\operatorname{E}\\{[x(t)-\\hat{x}(t)][x(t)-\\hat{x}(t)]'\\}.\n</math>\nThe separation principle would now follow immediately if <math>\\begin{align}\\Sigma\\end{align}</math> were independent of the control. However this needs to be established.\n\nThe state equation can be integrated to take the form\n:<math>\nx(t)=x_0(t)+\\int_0^t \\Phi(t,s)B_1(s)u(s) \\, ds,\n</math>\nwhere <math>x_0</math> is the state process obtained by setting <math>u=0</math> and <math>\\Phi</math> is the transition matrix function. By linearity, <math>\\hat{x}(t)=\\operatorname{E}\\{x(t)\\mid {\\cal Y}_t\\}</math> equals\n:<math>\n\\hat{x}(t)=\\hat{x}_0(t)+\\int_0^t \\Phi(t,s)B_1(s)u(s)\\,ds,\n</math>\nwhere <math>\\hat{x}_0(t)=\\operatorname{E}\\{x_0(t)\\mid {\\cal Y}_t\\}</math>. Consequently,\n:<math>\n\\Sigma(t):=\\mathbb{E}\\{[x_0(t)-\\hat{x}_0(t)][x_0(t)-\\hat{x}_0(t)]'\\},\n</math>\nbut we need to establish that <math>\\begin{align}\\hat{x}_0\\end{align}</math> does not depend on the control. This would be the case if\n:<math>\n{\\cal Y}_t ={\\cal Y}_t^0:=\\sigma\\{ y_0(\\tau), \\tau\\in [0,t]\\}, \\quad 0\\leq t\\leq T,\n</math>\nwhere <math>y_0</math> is the output process obtained by setting <math>u=0</math>. This issue was discussed in detail by Lindquist<ref name=\"lindquist\" />. In fact, since the control process <math>u</math> is in general a ''nonlinear'' function of the data and thus non-Gaussian, then so is the output process <math>y</math>. To avoid these problems one might begin by uncoupling the feedback loop and determine an optimal control process in the class of stochastic processes <math>u</math> that are adapted to the family <math>\\{  {\\cal Y}_t^0\\}</math> of sigma fields. This problem, where one optimizes over the class of all control processes adapted to a fixed filtration,  is called a ''stochastic open loop (SOL) problem''<ref name=\"lindquist\" />. It is not uncommon in the literature to assume from the outset that the control is adapted to <math>\\{ {\\mathcal Y}_t^0\\}</math>; see, e.g., Section 2.3 in Bensoussan<ref name=\"Bensoussan\">{{cite book |author=A. Bensoussan |title=Stochastic Control of Partially Observable Systems |publisher=Cambridge University Press |year=1992}}.</ref>, also van Handel <ref name=\"vanHandel\">{{cite book |author=Ramon van Handel |title=Stochastic Calculus, Filtering, and Stochastic Control |publisher=unpublished notes | url=https://web.math.princeton.edu/~rvan/acm217/ACM217.pdf |year=2007}}</ref> and Willems<ref name=\"Willems78''>{{cite journal |author=Jan C. Willems.\n |title=Recursive filtering|journal=Statistica Neerlandica |volume=32 |issue=1 |pages=1–39 |year=1978|doi=10.1111/j.1467-9574.1978.tb01382.x}}.</ref>.\n\nIn Lindquist 1973<ref name=\"lindquist\" /> a procedure was proposed for how to embed the class of admissible controls in various SOL classes in a problem-dependent manner, and then construct the corresponding feedback law. The largest class <math>\\Pi</math> of admissible feedback laws <math>\\pi</math> consists  of the non-anticipatory functions <math>u:=\\pi(y)</math> such that the feedback equation  has a unique solution and the corresponding control process <math>u_\\pi</math> is adapted to <math>\\{{\\mathcal Y}_t^0\\}</math>.\nNext, we give a few examples of specific classes of feedback laws that belong to this general class, as well as some other strategies in the literature  to overcome the problems described above.\n\n===Linear control laws===\nThe admissible class <math>\\Pi</math> of control laws could be restricted to contain only certain linear ones as in Davis<ref name=\"Davis\">{{cite book |author=M.H.A. Davis |title=Linear Estimation and Stochastic Control |publisher=Chapman and Hall |year=1978}}.</ref>. More generally, the linear class\n:<math>\n({\\mathcal L})\\quad u(t)=\\bar{u}(t)+\\int_0^tF(t,\\tau)\\,dy,\n</math>\nwhere <math>\\bar{u}</math> is a deterministic function and <math>F</math> is an <math>L_2</math> kernel, ensures that <math>\\Sigma</math> is independent of the control<ref name=\"lindquist1\" /><ref name=\"lindquist\" />. In fact,  the Gaussian property will then be preserved, and <math>\\hat{x}</math> will be generated by the Kalman filter. Then  the error process <math>\\tilde{x}:= x-\\hat{x}</math> is generated by\n:<math>\nd\\tilde{x}=(A-LC)\\tilde{x}\\,dt +(B_2-LD)\\,dw, \\quad \\tilde{x}(0)=x(0),\n</math>\nwhich is clearly independent of the choice of control, and thus so is <math>\\Sigma</math>.\n\n===Lipschitz-continuous control laws===\nWonham<ref name=\"Wonham\">{{cite journal |author=Murray Wonham |title=On the separation theorem of stochastic control |journal=SIAM J. Control |volume=6 |issue=2 |pages=312–326 |year=1968|doi=10.1137/0306023 }}.</ref> proved a separation theorem for controls in the class  <math>\\begin{align}\\pi:\\, u(t)=\\psi(t,\\hat{x}(t))\\end{align}</math>, even for a more general cost functional than J(u). However, the proof is far from simple and there are many technical assumptions. For example, <math>\\begin{align}C(t)\\end{align}</math> must square and have a determinant bounded away from zero, which is a serious restriction. A later proof by Fleming and Rishel<ref name=\"FlemingRishel\">{{cite book |author=W.H. Fleming and R.W. Rishel |title=Deterministic and Stochastic Optimal Control |publisher=Springer-Verlag |year=1968}}.</ref> is considerably simpler. They also prove the separation theorem with quadratic cost functional <math>J(u)</math> for a class of Lipschitz continuous  feedback laws, namely <math>u(t)=\\phi(t,y)</math>, where <math>\\phi:\\, [0,T]\\times C^n [0,T]\\to{\\mathbb R}^m</math> is a non-anticipatory function of <math>y</math> which is Lipschitz continuous in this argument. Kushner<ref name=\"Kushner\">{{cite book |author=H. Kushner |title=Introduction to Stochastic Control |publisher=Holt, Rinehart and Winston |year=1971}}.</ref> proposed a more restricted class <math>u(t)=\\psi(t,\\hat{\\xi}(t))</math>, where the modified state process <math>\\hat{\\xi}</math> is given by\n:<math>\n\\hat{\\xi}(t)=\\operatorname{E}\\{ x_0(t)\\mid {\\mathcal Y}_t^0\\}+ \\int_0^t \\Phi(t,s)B_1(s)u(s)\\,ds,\n</math>\nleading to the identity <math>\\begin{align}\\hat{x}=\\hat{\\xi}\\end{align}</math>.\n\n===Imposing delay===\n\nIf there is a delay in the processing of the observed data so that, for each <math>t</math>, <math>u(t)</math> is a function of <math>y(\\tau); \\, 0\\leq\\tau\\leq t-\\varepsilon</math>, then <math>{\\cal Y}_t ={\\cal Y}_t^0</math>, <math>0\\leq t\\leq T</math>, see Example 3 in Georgiou and Lindquist<ref name=\"GL2013\" />.  Consequently <math>\\Sigma</math> is independent of the control. Nevertheless, the control policy <math>\\pi</math> must be such that the feedback equations have a unique solution.\n\nConsequently the problem with possibly control-dependent sigma fields  does not occur in the usual discrete-time formulation. However, a procedure used in several textbooks to construct  the continuous-time <math>\\Sigma</math> as the limit of finite difference quotients of the discrete-time <math>\\Sigma</math>, which does not depend on the control, is circular or a best incomplete; see Remark 4 in Georgiou and Lindquist<ref name=\"GL2013\" />.\n\n===Weak solutions===\n\nAn approach  introduced by Duncan and Varaiya<ref name=\"duncanvaraiya\">{{cite journal |author=Tyrone Duncan and Pravin Varaiya |title=On the solutions of a stochastic control system |journal=SIAM J. Control |volume=9 |issue=3 |pages=354–371 |year=1971|doi=10.1137/0309026 |hdl=1808/16692 }}.</ref> and  Davis and Varaiya<ref name=\"davisvaraiya\">{{cite journal |author=M.H.A. Davis and P. Varaiya |title=Information states for stochastic systems |journal=J. Math. Anal. Applications |volume=37 |pages=384–402 |year=1972|doi=10.1016/0022-247X(72)90281-8 }}.</ref>, see also Section 2.4 in Bensoussan<ref name=\"Bensoussan\" />\nis based on ''weak solutions''  of the  stochastic differential equation. Considering such solutions of\n:<math>\ndx =A(t)x(t)\\,dt+B_1(t)u(t)\\,dt+B_2(t)\\,dw\n</math>\nwe can change the probability measure (that depends on <math>\\begin{align}u\\end{align}</math>) via a [[Girsanov]] transformation so that\n:<math>\nd\\tilde{w}:= B_1(t)u(t)\\,dt+B_2(t)\\,dw\n</math>\nbecomes a new Wiener process, which (under the new probability measure) can be assumed to be unaffected by the control. The question of how this could be implemented in an engineering system is left open.\n\n===Nonlinear filtering solutions===\n\nAlthough a nonlinear control law will produce a non-Gaussian state process, it can be shown, using  nonlinear filtering theory (Chapters 16.1 in Lipster and Shirayev<ref name=\"LipsterShirayev\">{{cite book |author=R.S. Liptser and A.N. Shirayev |title=Statistics of Random Processes II, Applications |publisher=Springer-Verlag |year=1978}}.</ref>\n), that the state process is ''conditionally Gaussian'' given the filtration <math>\\begin{align}\\{{\\mathcal Y}_t\\}\\end{align}</math>. This fact can be used to show that <math>\\begin{align}\\hat{x}\\end{align}</math> is actually generated by a Kalman filter (see Chapters 11 and 12 in Lipster and Shirayev<ref name=\"LipsterShirayev\" />). However, this requires quite a sophisticated analysis and is restricted to the case where the driving noise <math>\\begin{align}w\\end{align}</math> is a Wiener process.\n\nAdditional historical perspective can be found in Mitter<ref name=\"mitter\">{{cite journal |author=S. Mitter |title=Filtering and stochastic control: A historical perspective |journal=IEEE Control Systems Magazine |volume=13 |issue=3 |pages=67–76 |year=1996}}.</ref>.\n\n==Issues on feedback in linear stochastic systems==\n\nAt this point it is suitable to consider a more general class of controlled linear stochastic systems that also covers systems with time delays, namely\n:<math>\\begin{align}\n   z(t) & =z_0(t) + \\int_0^t G(t,s)u(s)\\,ds  \\\\\n   y(t) & = Hz(t)\n\\end{align}</math>\nwith <math>\\begin{align}z_0\\end{align}</math> a stochastic vector process which does not depend on the control<ref name=\"lindquist\" />. The standard stochastic system is then obtained as a special case where <math>z=[x',y']'</math>, <math>z_0=[x_0',y_0']'</math> and <math>H=[I,0]</math>. We shall use the short-hand notation\n:<math>\nz=z_0+g\\pi Hz\n</math>\nfor the feedback system, where\n:<math>\ng\\;:\\; (t,u) \\mapsto \\int_0^t G(t,\\tau)u(\\tau)\\,d\\tau\n</math>\nis a Volterra operator.\n\nIn this more general formulation the embedding procedure of Lindquist<ref name=\"lindquist\" /> defines the class <math>\\Pi</math> of admissible feedback laws <math>\\pi</math> as the class of non-anticipatory functions <math>u:=\\pi(y)</math> such that the feedback equation <math>z=z_0+g\\pi Hz</math> has a unique solution <math>z_\\pi</math> and <math>u=\\pi(Hz_\\pi)</math> is adapted to <math>\\{{\\mathcal Y}_t^0\\}</math>.\n\nIn Georgiou and Lindquist<ref name=\"GL2013\" /> a new framework for the separation principle was proposed.  This approach considers stochastic systems as well-defined maps between sample paths rather than between stochastic processes  and allows us to extend the separation principle to systems driven by martingales with possible jumps. The approach is motivated by engineering thinking where systems and feedback loops process signals, and not stochastic processes ''per se'' or transformations of probability measures. Hence the purpose is to create a natural class of admissible control laws that make engineering sense, including those that are nonlinear and discontinuous.\n\nThe feedback equation <math>z=z_0+g\\pi Hz</math> has a unique strong solution if there exists a non-anticipating function <math>F</math> such that <math>z=F(z_0)</math> satisfies  the equation with probability one and all other solutions coincide with <math>z</math> with probability one. However, in the sample-wise setting, more is required, namely that such a unique solution exists and that <math>z=z_0+g\\pi Hz</math> holds for all <math>z_0</math>, not just almost all. The resulting feedback loop is ''deterministically well-posed''in the sense that the feedback equations admit a unique solution that causally depends on the input for ''each'' input sample path.\n\nIn this context, a ''signal'' is defined to be a sample path of a stochastic process with possible discontinuities. More precisely, signals will belong to the ''Skorohod space'' <math>D</math>, i.e., the space of functions which are continuous on the right and have a left limit at all points ([[càdlàg]] functions). In particular, the space <math>C</math> of continuous functions is a proper subspace of <math>D</math>. Hence the response of a typical nonlinear operation that involves  thresholding and switching can be modeled as a signal.  The same goes for sample paths of counting processes and other martingales.  A ''system'' is defined to be a measurable non-anticipatory map <math>D\\to D</math> sending sample paths to sample paths so that their outputs at any time <math>t</math> is a measurable function of past values of the input and  time. For example, stochastic differential equations with Lipschitz coefficients driven by a Wiener process\ninduce maps between corresponding path spaces, see page 127 in Rogers and Williams<ref name=\"RogersWilliams\">{{cite book |author=Rogers, L. Chris G., and David Williams |title=Diffusions, Markov processes and martingales: Volume 2, Itô calculus |publisher=Cambridge university press |year=2000}}.</ref>, and pages 126-128 in Klebaner<ref name=\"Klebaner\">{{cite book |author=Klebaner, Fima C. |title=Introduction to Stochastic Calculus with Applications |publisher=World Scientific Publishing Company |year=2012}}.</ref>. Also, under fairly general conditions (see e.g., Chapter V in Protter<ref name=\"Protter\">{{cite book |author=P.E Protter, Fima C. |title=Stochastic Integration and Differential Equations |publisher=Springer |year=2004}}.</ref>), stochastic differential equations driven by martingales with sample paths in <math>D</math> have strong solutions who are semi-martingales.\n\nFor the time setting <math>f(z):=g\\pi Hz</math>, the feedback system <math>z=z_0+g\\pi Hz</math> can be written <math>z=z_0+f(z)</math>, where <math>z_0</math> can be interpreted as an input.\n\n'''''Definition.''''' A feedback loop <math>z=z_0+f(z)</math> is ''deterministically well-posed'' if it has a unique solution <math>z\\in D</math> for all inputs <math>z_0\\in D</math> and  <math>(1-f)^{-1}</math> is a system.\n\nThis implies that the processes <math>z</math> and <math>z_0</math> define identical filtrations<ref name=\"GL2013\" />. Consequently no new information is created by the loop. However, what we need is that <math>{\\cal Y}_t ={\\cal Y}_t^0</math> for <math>0\\leq t\\leq T</math>. This is ensured by the following lemma (Lemma 8 in Georgiou and Lindquist<ref name=\"GL2013\" />).\n\n'''''Key Lemma.''''' If the feedback loop <math>z=z_0+g\\pi Hz</math> is deterministically well-posed, <math>g\\pi</math> is a system, and <math>H</math> is a linear system having a right inverse <math>H^{-R}</math> that is also a system, then <math>(1-Hg\\pi)^{-1}</math>  is a system and <math>{\\cal Y}_t ={\\cal Y}_t^0</math> for <math>0\\leq t\\leq T</math>.\n\nThe condition on <math>H</math> in this lemma is clearly satisfied in the standard linear stochastic system, for which <math>H=[0,I]</math>, and hence <math>H^{-R}=H'</math>. The remauining conditions are collected in the following definition.\n\n'''''Definition.''''' A feedback law <math>\\pi</math> is ''deterministically well-posed'' for the system <math>z=z_0+g\\pi Hz</math>  if <math>g\\pi</math> is a system and the feedback system <math>z=z_0+g\\pi Hz</math> deterministically well-posed.\n\nExamples of simple systems that are not deterministically  well-posed are given in Remark 12 in Georgiou and Lindquist<ref name=\"GL2013\" />.\n\n==A separation principle for physically realizable control laws==\n\nBy only considering feedback laws that are deterministically well-posed, all admissible control laws are physically realizable in the engineering sense  that they induce a signal that travels through the feedback loop.\nThe proof of the following theorem can be found in Georgiou and Lindquist 2013<ref name=\"GL2013\" />.\n\n'''''Separation theorem.'''''\nGiven the linear stochastic system\n: <math>\n\\begin{align}\n   dx &  =A(t)x(t)\\,dt+B_1(t)u(t)\\,dt+B_2(t)\\,dw \\\\\n   dy &  =C(t)x(t)\\,dt +D(t)\\,dw\n\\end{align}\n</math>\nwhere <math>w</math> is a vector-valued Wiener process, <math>x(0)</math> is a zero-mean Gaussian random vector independent of <math>w</math>, consider the problem of minimizing the quadratic functional J(u) over the class of all deterministically well-posed feedback laws <math>\\pi</math>. Then the unique optimal control law is given by <math>u(t)=K(t)\\hat{x}(t)</math>  where <math>K</math> is defined as above and <math>\\hat{x}</math> is given by the Kalman filter. More generally, if <math>w</math> is a square-integrable martingale and  <math>x(0)</math> is an arbitrary zero mean random vector, <math>u(t)=K(t)\\hat{x}(t)</math>, where <math>\\hat{x}(t)=\\operatorname{E}\\{x(t)\\mid {\\cal Y}_t\\}</math>, is the optimal control law provided it is deterministically well-posed.\n\nIn the general non-Gaussian case, which may involve counting processes, the Kalman filter needs to be replaced by an nonlinear filter.\n\n==A Separation principle for delay-differential systems==\n\nStochastic control for time-delay systems were first studied in Lindquist<ref name=\"L68\">{{cite journal |author=Anders Lindquist |title=On optimal stochastic control with smoothed information |journal=Information Sciences |volume=1 |pages=55–85 |year=1968|doi=10.1016/0020-0255(68)90007-8 }}.</ref><ref name=\"L69\">{{cite journal |author=Anders Lindquist |title=An innovations approach to optimal control of linear stochastic systems with time delay |journal=Information Sciences |volume=1 |issue=3 |pages=279–295 |year=1969|doi=10.1016/S0020-0255(69)80014-9 }}.</ref><ref name=\"lindquist1\" /><ref name=\"lindquist\" />,\nand Brooks<ref name=\"Brooks\">{{cite journal |author=R. Brooks |title=Linear Stochastic Control: An extended separation principle |journal=J. Math. Anal. Appl. |volume=38 |issue=3 |pages=569–587 |year=1972|doi=10.1016/0022-247X(72)90069-8 }}.</ref>, although Brooks relies on the strong assumption that the observation <math>y</math> is ''functionally independent'' of the control <math>u</math>, thus avoiding the key question of feedback.\n\nConsider the delay-differential system<ref name=\"lindquist1\" />\n:<math>\\begin{align}\ndx    &=\\left(\\int_{t-h}^t d_s\\,A(t,s)x(s)\\right) \\,dt + B_1(t)u(t)\\,dt+B_2(t)\\,dw   \\\\\n dy   & =\\left(\\int_{t-h}^t d_s\\,C(t,s)x(s)\\right) \\,dt +D(t)\\,dw\n\\end{align}</math>\nwhere <math>w</math> is now a (square-integrable) Gaussian (vector) martingale, and where <math>\\begin{align}A\\end{align}</math> and <math>C</math> are of bounded variation in the first argument and continuous on the right in the second, <math>x(t)=\\xi(t)</math> is deterministic for <math>-h\\leq t\\leq 0</math>, and <math>y(0)=0</math>.\nMore precisely, <math>A(t,s)=0</math> for <math>s\\geq t</math>, <math>A(t,s)=A(t,t-h)</math> for <math>t\\leq t-h</math>, and the total variation of <math>s\\mapsto A(t,s)</math> is bounded by an integrable function in the variable <math>t</math>, and the same holds for <math>C</math>.\n\nWe want to determine a control law which minimizes\n:<math>\nJ(u)=\\operatorname{E}\\left(\\int_0^T x(t)'Q(t)x(t)\\,d\\alpha(t)+\\int_0^Tu(t)'R(t)u(t)\\,dt\\right),\n</math>\nwhere <math>\\begin{align}d\\alpha\\end{align}</math> is a positive Stieltjes measure. The corresponding deterministic problem obtained by setting <math>\\begin{align}w=0\\end{align}</math> is given by\n:<math>\nu(t)=\\int_{t-h}^t d_\\tau \\, K(t,\\tau)x(\\tau),\n</math>\nwith<ref name=\"lindquist1\" /> <math>\\begin{align}K\\end{align}</math>.\n\nThe following separation principle for the delay system above can be found in Georgiou and Lindquist 2013<ref name=\"GL2013\" /> and generalizes the corresponding result in Lindquist 1973<ref name=\"lindquist1\">{{cite journal |author=Anders Lindquist|title=Optimal control of linear stochastic systems with applications to time lag systems |journal=Information Sciences|volume=5|pages=81–126|year=1973|doi=10.1016/0020-0255(73)90005-4 }}.</ref>\n\n'''''Theorem.''''' There is a unique feedback law <math>\\begin{align}\\pi:\\, y\\mapsto u\\end{align}</math>  in the class of deterministically well-posed control laws that minimizes <math>\\begin{align}J(u)\\end{align}</math>, and it is given by\n:<math>\nu(t)=\\int_{t-h}^t d_s \\, K(t,s)\\hat{x}(s\\mid t),\n</math>\nwhere <math>K</math> is the deterministic control gain and <math>\\hat{x}(s\\mid t) := E\\{ x(s)\\mid {\\cal Y}_t\\}</math> is given by the linear (distributed) filter\n:<math>\\begin{align}\n  d\\hat{x}(t\\mid t)  &  =\\int_{t-h}^t d_s \\, A(t,s)\\hat{x}(s\\mid t) \\, dt +B_1u\\,dt+ X(t,t)\\,dv \\\\\n  d\\hat{x}(t\\mid t)  &  =\\int_{t-h}^t d_s \\, A(t,s)\\hat{x}(s\\mid t) \\, dt +B_1u\\,dt+ X(t,t)\\,dv\n\\end{align}</math>\nwhere <math>v</math> is the innovation process\n:<math>\ndv=dy - \\int_{t-h}^t d_sC(t,s)\\hat{x}(s\\mid t)\\, dt, \\quad v(0)=0,\n</math>\nand the gain <math>x</math> is as defined in page 120 in Lindquist.<ref name=\"lindquist1\" />\n\n==References==\n<references />\n\n[[Category:Control theory]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Stochastic control",
      "url": "https://en.wikipedia.org/wiki/Stochastic_control",
      "text": "{{See also|Stochastic programming}}\n\n'''Stochastic control''' or '''stochastic [[optimal control]]''' is a sub field of [[control theory]] that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. The system designer assumes, in a [[Bayesian probability]]-driven fashion, that random noise with known probability distribution affects the evolution and observation of the state variables.  Stochastic control aims to design the time path of the controlled variables that performs the desired control task with minimum cost, somehow defined, despite the presence of this noise.<ref>[http://answerphone/topic/stochastic-control-theory?cat=technology Definition from Answers.com]</ref> The context may be either [[discrete time]] or [[continuous time]].\n\n==Certainty equivalence==\n\nAn extremely well-studied formulation in stochastic control is that of [[linear quadratic Gaussian control]]. Here the model is linear, the objective function is the expected value of a quadratic form, and the disturbances are purely additive.  A basic result for discrete-time centralized systems with only additive uncertainty is the '''certainty equivalence property''':<ref name=\"Chow\">{{cite book |last=[[Gregory Chow|Chow]] |first=Gregory P. |year=1976 |title=Analysis and Control of Dynamic Economic Systems |location=New York |publisher=Wiley |isbn=0-471-15616-7 }}</ref> that the optimal control solution in this case is the same as would be obtained in the absence of the additive disturbances. This property is applicable to all centralized systems with linear equations of evolution, quadratic cost function, and noise entering the model only additively; the quadratic assumption allows for the optimal control laws, which follow the certainty-equivalence property, to be linear functions of the observations of the controllers.\n\nAny deviation from the above assumptions—a nonlinear state equation, a non-quadratic objective function, [[Multiplier uncertainty|noise in the multiplicative parameters]] of the model, or decentralization of control—causes the certainty equivalence property not to hold. For example, its failure to hold for decentralized control was demonstrated in  [[Witsenhausen's counterexample]].\n\n==Discrete time==\n\nIn a discrete-time context, the decision-maker observes the state variable, possibly with observational noise, in each time period. The objective may be to optimize the sum of expected values of a nonlinear (possibly quadratic) objective function over all the time periods from the present to the final period of concern, or to optimize the value of the objective function as of the final period only. At each time period new observations are made, and the control variables are to be adjusted optimally. Finding the optimal solution for the present time may involve iterating a [[Linear-quadratic-Gaussian control#Discrete time|matrix Riccati equation]] backwards in time from the last period to the present period.\n\nIn the discrete-time case with uncertainty about the parameter values in the transition matrix (giving the effect of current values of the state variables on their own evolution) and/or the control response matrix of the state equation, but still with a linear state equation and quadratic objective function, a Riccati equation can still be obtained for iterating backward to each period's solution even though certainty equivalence does not apply.<ref name=\"Chow\"/><sup>ch.13</sup><ref name=Turnovsky>{{cite journal |last=[[Stephen J. Turnovsky|Turnovsky]] |first=Stephen |title=Optimal Stabilization Policies for Stochastic Linear Systems: The Case of Correlated Multiplicative and Additive disturbances |journal=[[Review of Economic Studies]] |volume=43 |issue=1 |year=1976 |pages=191–94 |doi= 10.2307/2296614}}</ref>  The discrete-time case of a non-quadratic loss function but only additive disturbances can also be handled, albeit with more complications.<ref>{{cite journal |last=Mitchell |first=Douglas W. |title=Tractable Risk Sensitive Control Based on Approximate Expected Utility |journal=Economic Modelling |year=1990 |volume=7 |issue=2 |pages=161–164 |doi=10.1016/0264-9993(90)90018-Y }}</ref>\n\n===Example===\n\nA typical specification of the discrete-time stochastic linear quadratic control problem is to minimize<ref name=\"Chow\" />{{rp|ch. 13;}}<ref name=Turnovsky/><ref>{{cite journal |last=Turnovsky |first=Stephen |year=1974 |title=The stability properties of optimal economic policies |journal=[[American Economic Review]] |volume=64 |issue=1 |pages=136–148 |jstor=1814888 }}</ref>\n\n:<math>\\text{E}_1\\sum_{t=1}^S [y_t^T Qy_t + u_t^T Ru_t]</math>\n\nwhere E<sub>1</sub> is the [[expected value]] operator conditional on ''y''<sub>0</sub>, superscript ''T'' indicates a [[matrix transpose]], and ''S'' is the time horizon, subject to the state equation\n\n:<math>y_t = A_ty_{t-1} + B_tu_t,</math>\n\nwhere ''y'' is an ''n'' × 1 vector of observable state variables, ''u'' is a ''k'' × 1 vector of control variables, ''A''<sub>''t''</sub> is the time ''t'' realization of the [[stochastic matrix|stochastic ''n'' × ''n'' state transition matrix]], ''B''<sub>''t''</sub> is the time ''t'' realization of the stochastic ''n'' × ''k'' matrix of control multipliers, and ''Q'' (''n'' × ''n'') and ''R'' (''k'' × ''k'') are known symmetric positive definite cost matrices. We assume that each element of ''A'' and ''B'' is jointly [[iid|independently and identically]] distributed through time, so the expected value operations need not be time-conditional.\n\n[[Bellman equation|Induction backwards in time]] can be used to obtain the optimal control solution at each time,<ref name=\"Chow\" />{{rp|ch. 13}}\n\n:<math>u_t^*=-[\\text{E}(B^TX_tB+R)]^{-1}\\text{E}(B^TX_tA)y_{t-1},</math>\n\nwith the symmetric positive definite cost-to-go matrix ''X'' evolving backwards in time from <math>X_S = Q</math> according to\n\n: <math> X_{t-1} = Q+\\text{E}[A^TX_tA] - \\text{E}[A^TX_tB][\\text{E}(B^TX_tB+R)]^{-1}\\text{E}(B^TX_tA), \\,</math>\n\nwhich is known as the discrete-time dynamic Riccati equation of this problem. The only information needed regarding the unknown parameters in the ''A'' and ''B'' matrices is the expected value and variance of each element of each matrix and the covariances among elements of the same matrix and among elements across matrices.\n\nThe optimal control solution is unaffected if zero-mean, i.i.d. additive shocks also appear in the state equation, so long as they are uncorrelated with the parameters in the ''A'' and ''B'' matrices. But if they are so correlated, then the optimal control solution for each period contains an additional additive constant vector. If an additive constant vector appears in the state equation, then again the optimal control solution for each period contains an additional additive constant vector.\n\nThe steady-state characterization of ''X'' (if it exists), relevant for the infinite-horizon problem in which ''S'' goes to infinity, can be found by iterating the dynamic equation for ''X'' repeatedly until it converges; then ''X'' is characterized by removing the time subscripts from its dynamic equation.\n\n==Continuous time==\n\nIf the model is in continuous time, the controller knows the state of the system at each instant of time. The objective is to maximize either an integral of, for example, a concave function of a state variable over a horizon from time zero (the present) to a terminal time ''T'', or a concave function of a state variable at some future date ''T''. As time evolves, new observations are continuously made and the control variables are continuously adjusted in optimal fashion.\n\n==Stochastic model predictive control==\nIn the literature, there are two types of MPCs for stochastic systems; Robust model predictive control and Stochastic Model Predictive Control (SMPC). Robust model predictive control is a more conservative method which considers the worst scenario in the optimization procedure. However, this method, similar to other robust controls, deteriorates the overall controller's performance and also is applicable only for systems with bounded uncertainties. The alternative method, SMPC, considers soft constraints which limit the risk of violation by a probabilistic inequality.<ref>{{cite journal|last1=Hashemian|last2=Armaou|title=Stochastic MPC Design for a Two-Component Granulation Process|date=2017|pages=4386–4391|arxiv=1704.04710| journal = IEEE Proceedings|bibcode=2017arXiv170404710H}}</ref>\n\n===In finance===\n\nIn a continuous time approach in a [[finance]] context, the state variable in the stochastic differential equation is usually wealth or net worth, and the controls are the shares placed at each time in the various assets. Given the [[asset allocation]] chosen at any time, the determinants of the change in wealth are usually the stochastic returns to assets and the interest rate on the risk-free asset. The field of stochastic control has developed greatly since the 1970s, particularly in its applications to finance. Robert Merton used stochastic control to study [[optimal portfolio]]s of safe and risky assets.<ref>{{cite book |first=Robert |last=Merton |title=Continuous Time Finance |location= |publisher=Blackwell |year=1990 }}</ref> [[Merton's portfolio problem|His work]] and that of [[Black–Scholes model|Black–Scholes]] changed the nature of the [[finance]] literature. Influential mathematical textbook treatments were by [[Wendell Fleming|Fleming]] and [[Raymond Rishel|Rishel]],<ref>{{cite book |first=W. |last=Fleming |first2=R. |last2=Rishel |title=Deterministic and Stochastic Optimal Control |location= |publisher= |year=1975 |isbn=0-387-90155-8 |url=https://books.google.com/books?id=kUrvAAAAMAAJ }}</ref> and by Fleming and [[Halil Mete Soner|Soner]].<ref>{{cite book |first=W. |last=Fleming |first2=M. |last2=Soner |title=Controlled Markov Processes and Viscosity Solutions |location= |publisher=Springer |year=2006 |isbn= }}</ref> These techniques were applied by [[Jerome Stein|Stein]] to the [[financial crisis of 2007–08]].<ref name=stein>{{cite book |first=J. L. |last=Stein |title=Stochastic Optimal Control and the US Financial Crisis |location= |publisher=Springer-Science |year=2012 }}</ref>\n\nThe maximization, say of the expected logarithm of net worth at a terminal date ''T'', is subject to stochastic processes on the components of wealth. In this case, in continuous time [[Itô's lemma|Itô's equation]] is the main tool of analysis. In the case where the maximization is an integral of a concave function of utility over an horizon (0,''T''), dynamic programming is used. There is no certainty equivalence as in the older literature, because the coefficients of the control variables—that is, the returns received by the chosen shares of assets—are stochastic.\n\n==See also==\n\n* [[Stochastic process]]\n* [[Control theory]]\n* [[Multiplier uncertainty]]\n* [[Stochastic scheduling]]\n\n==References==\n{{reflist}}\n\n[[Category:Control theory]]\n[[Category:Stochastic control]]\n[[Category:Stochastic processes]]"
    },
    {
      "title": "Witsenhausen's counterexample",
      "url": "https://en.wikipedia.org/wiki/Witsenhausen%27s_counterexample",
      "text": "'''Witsenhausen's counterexample''', shown in the figure below, is a deceptively simple [[toy problem]] in [[distributed control system|decentralized]] [[stochastic control]]. It was formulated by [[Hans witsenhausen|Hans Witsenhausen]] in 1968.<ref>Witsenhausen, Hans. \"A counterexample in stochastic optimum control.\" ''SIAM J. Control'', Volume 6, Issue 1, pp. 131–147 (February 1968)</ref> It is a [[counterexample]] to a natural [[conjecture]] that one can generalize a key result of centralized [[linear–quadratic–Gaussian control]] systems—that in a system with linear dynamics, Gaussian disturbance, and quadratic cost, affine (linear) control laws are optimal—to decentralized systems. Witsenhausen constructed a two-stage linear quadratic Gaussian system where two decisions are made by decision makers with decentralized information and showed that for this system, there exist nonlinear control laws that outperform all linear laws. The problem of finding the optimal control law remains unsolved.<ref name=Ho>Ho, Yu-Chi, \"Review of the Witsenhausen problem\". ''Proceedings of the 47th IEEE Conference on Decision and Control (CDC)'', pp. 1611–1613, 2008.</ref>\n\n[[File:WitsenhausenCounterexample.jpg]]\n\n==Statement of the counterexample==\n\nThe statement of the counterexample is simple: two controllers attempt to control the system by attempting to bring the state close to zero in exactly two time steps. The first controller observes the initial state <math>x_0.</math> There is a cost on the input <math>u_1</math> of the first controller, and a cost on the state <math>x_2</math> after the input of the second controller. The input <math>u_2</math> of the second controller is free, but it is based on noisy observations <math>y_1=x_1+z</math> of the state <math>x_1</math>  after the first controller's input. The second controller cannot communicate with the first controller and thus cannot observe either the original state <math>x_0</math> or the input <math>u_1</math> of the first controller. Thus the system dynamics are\n\n:<math>x_1=x_0+u_1,</math>\n:<math>x_2=x_1-u_2,</math>\n\nwith the second controller's observation equation\n\n:<math>y_1=x_1+z.</math>\n\nThe objective is to minimize an expected [[loss function|cost function]],\n\n:<math>k^2E[u_1^2]+E[x_2^2],</math>\n\nwhere the expectation is taken over the randomness in the initial state <math>x_0</math> and the observation noise <math>z</math>, which are distributed [[independence (probability theory)|independently]].  The observation noise <math>z</math> is assumed to be distributed in a [[normal distribution|Gaussian]] manner, while the distribution of the initial state value <math>x_0</math> differs depending on the particular version of the problem.\n\nThe problem is to find control functions\n\n:<math>u_1(x_0) \\quad \\text{and} \\quad u_2(y_1)</math>\n\nthat give at least as good a value of the objective function as do any other pair of control functions. Witsenhausen showed that the optimal functions <math> u_1(x_0)</math> and <math>u_2(y_1)</math> cannot be linear.\n\n==Specific results of Witsenhausen==\n\nWitsenhausen obtained the following results:\n\n*An optimum exists (Theorem 1).\n*The optimal control law of the first controller is such that <math>E(x_1)=0</math> (Lemma 9).\n*The exact solution is given for the case in which both controllers are constrained to be linear (Lemma 11).\n*If <math>x_0</math> has a Gaussian distribution and if at least one of the controllers is constrained to be linear, then it is optimal for both controllers to be linear (Lemma 13).\n*The exact nonlinear control laws are given for the case in which <math>x_0</math> has a [[two point distribution|two-point]] [[symmetric distribution]] (Lemma 15).\n*If <math>x_0</math> has a Gaussian distribution, for some values of the preference parameter <math>k</math> a non-optimal nonlinear solution for the control laws is given which gives a lower value for the expected cost function than does the best linear pair of control laws (Theorem 2).\n\n==The significance of the problem==\nThe counterexample lies at the intersection of [[control theory]] and [[information theory]]. Due to its hardness, the problem of finding the optimal control law has also received attention from the [[theoretical computer science]] community. The importance of the problem was reflected upon in the 47th IEEE Conference on Decision and Control (CDC) 2008, Cancun, Mexico,<ref name=Ho/> where an entire session was dedicated to understanding the counterexample 40 years after it was first formulated.\n\nThe problem is of conceptual significance in decentralized control because it shows that it is important for the controllers to communicate<ref>Mitterrand and Sahai. \"Information and Control: Witsenhausen revisited\". ''Learning, control and hybrid systems'', 1999, Springer.</ref> with each other implicitly in order to minimize the cost. This suggests that control actions in decentralized control may have a dual role: those of control and communication.\n\n==The hardness of the problem==\nThe hardness of the problem is attributed to the fact that information of the second controller depends on the decisions of the first controller.<ref>Ho, Yu-Chi. \"Team decision theory and information structures\". ''Proceedings of the IEEE'', Vol. 68, No.6, June 1980.</ref> Variations considered by [[Tamer Basar]] <ref>Basar, Tamer. \"Variations on the theme of the Witsenhausen counterexample\". ''47th IEEE Conference on Decision and Control Cancun'', Mexico, Dec. 9–11, 2008.</ref> show that the hardness is also because of the structure of the performance index and the coupling of different decision variables. It has also been shown that problems of the spirit of Witsenhausen's counterexample become simpler if the [[transmission delay]] along an external channel that connects the controllers is smaller than the [[propagation delay]] in the problem. However, this result requires the channels to be perfect and instantaneous,<ref>Rotkowitz, M.; Cogill, R.; Lall, S.; , \"A Simple Condition for the Convexity of Optimal Control over Networks with Delays,\" ''Decision and Control'', 2005 and 2005 ''European Control Conference. CDC-ECC '05. 44th IEEE Conference on'' , pp. 6686–6691, 12–15 December 2005.</ref> and hence is of limited applicability. In practical situations, the channel is always imperfect, and thus one can not assume that decentralized control problems are simple in presence of external channels.\n\nA justification of the failure of attempts that discretize the problem came from the computer science literature: [[Christos Papadimitriou]] and [[John Tsitsiklis]] showed that the discrete version of the counterexample is  [[NP-complete]].<ref>[[Christos Papadimitriou]] and [[John Tsitsiklis]].  \"Intractable problems in control theory.\" ''24th IEEE Conference on Decision and Control'', 1985</ref>\n\n==Attempts at obtaining a solution==\nA number of numerical attempts have been made to solve the counterexample. Focusing on a particular choice of problem parameters <math>(k=0.2,\\;\\sigma_0=5)</math>, researchers have obtained strategies by [[discretization]] and using [[neural networks]].<ref>Baglietto, Parisini, and Zoppoli. \"Numerical solutions to the Witsenhausen counterexample by approximating networks.\" ''IEEE Transactions on Automatic Control''. 2001.</ref> Further research (notably, the work of [[Yu-Chi Ho]],<ref>Lee, Lau, and Ho. \"The Witsenhausen counterexample: A hierarchical search approach for nonconvex optimization problems.\" '' IEEE Transactions on Automatic Control'', 2001</ref> and the work of Li, Marden and [[Jeff S. Shamma|Shamma]] <ref>Li, Marden, and Shamma. \"Learning approaches to the Witsenhausen counterexample from a view of potential games.\" '' IEEE Conference on Decision and Control'', 2009.</ref>) has obtained slightly improved costs for the same parameter choice. The best known numerical results for a variety of parameters, including the one mentioned previously, are obtained by a local search algorithm proposed by S.-H. Tseng and A. Tang in 2017.<ref>Tseng and Tang. \"A Local Search Algorithm for the Witsenhausen's Counterexample.\" '' IEEE Conference on Decision and Control'', 2017.</ref> The first provably approximately optimal strategies appeared in 2010 (Grover, Park, Sahai) <ref>Grover, Sahai, and Park. \"The finite-dimensional Witsenhausen counterexample.\" ''IEEE WiOpt 2010, ConCom workshop'', Seoul, Korea.</ref> where [[information theory]] is used to understand the communication in the counterexample. The optimal solution of the counterexample is still an open problem.\n\n==References==\n{{Reflist}}\n\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Witsenhausen's Counterexample}}\n[[Category:Control theory]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Calculus of variations",
      "url": "https://en.wikipedia.org/wiki/Calculus_of_variations",
      "text": "{{redirect|Variational method|the use as an approximation method in quantum mechanics|Variational method (quantum mechanics)}}\n{{Calculus |specialized}}\n\n'''Calculus of variations''' is a field of [[mathematical analysis]] that uses variations, which are small changes in [[Function (mathematics)|function]]s\nand [[functional (mathematics)|functionals]], to find maxima and minima of functionals: [[Map (mathematics)|mapping]]s from a set of [[Function (mathematics)|function]]s to the [[real number]]s.{{refn|Whereas [[Calculus|elementary calculus]] is about [[Infinitesimal|infinitesimally]] small changes in the values of functions without changes in the function itself, calculus of variations is about infinitesimally small changes in the function itself, which are called variations.<ref name='CourHilb1953P184'>{{harvnb|Courant|Hilbert|1953|p=184}}</ref>|group=\"Note\"}} Functionals are often expressed as [[definite integral]]s involving functions and their [[derivative]]s. Functions that maximize or minimize functionals may be found using the [[Euler–Lagrange equation]] of the calculus of variations.\n\nA simple example of such a problem is to find the curve of shortest length connecting two points. If there are no constraints, the solution is a straight line between the points. However, if the curve is constrained to lie on a surface in space, then the solution is less obvious, and possibly many solutions may exist. Such solutions are known as [[geodesic]]s. A related problem is posed by [[Fermat's principle]]: light follows the path of shortest optical length connecting two points, where the optical length depends upon the material of the medium. One corresponding concept in [[mechanics]] is the [[principle of least action]].\n\nMany important problems involve functions of several variables. Solutions of boundary value problems for the [[Laplace equation]] satisfy the [[Dirichlet principle]]. [[Plateau's problem]] requires finding a surface of minimal area that spans a given contour in space: a solution can often be found by dipping a frame in a solution of soap suds. Although such experiments are relatively easy to perform, their mathematical interpretation is far from simple: there may be more than one locally minimizing surface, and they may have non-trivial [[topology]].\n\n== History ==\n\nThe calculus of variations may be said to begin with [[Newton's minimal resistance problem]] in 1687, followed by the [[brachistochrone curve]] problem raised by [[Johann Bernoulli]] (1696).<ref name=GelfandFominP3>{{cite book|last1=Gelfand|first1=I. M.|authorlink1=Israel Gelfand|last2=Fomin|first2=S. V.|authorlink2=Sergei Fomin|ref=harv|title=Calculus of variations|year=2000|publisher=Dover Publications|location=Mineola, New York|isbn=978-0486414485|page=3|url=https://books.google.com/books?id=YkFLGQeGRw4C&dq=isbn:9780486414485|edition=Unabridged repr.|editor1-last=Silverman| editor1-first=Richard A.}}</ref> It immediately occupied the attention of [[Jakob Bernoulli]] and the [[Guillaume de l'Hôpital|Marquis de l'Hôpital]], but  [[Leonhard Euler]] first elaborated the subject, beginning in 1733. [[Joseph-Louis Lagrange|Lagrange]] was influenced by Euler's work to contribute significantly to the theory. After Euler saw the 1755 work of the 19-year-old Lagrange, Euler dropped his own partly geometric approach in favor of Lagrange's purely analytic approach and renamed the subject the ''calculus of variations'' in his 1756 lecture ''Elementa Calculi Variationum''.<ref name=Thiele>{{cite book |last=Thiele |first=Rüdiger |editor-last1=Bradley |editor-first1=Robert E. |editor-last2=Sandifer |editor-first2=C. Edward |title=Leonhard Euler: Life, Work and Legacy |publisher=Elsevier |year=2007 |page=249 |chapter=Euler and the Calculus of Variations |chapter-url=https://books.google.com/books?id=75vJL_Y-PvsC&pg=PA249 |isbn=9780080471297}}</ref><ref name=Goldstine> {{cite book |last=Goldstine |first=Herman H. |year=2012 |title=A History of the Calculus of Variations from the 17th through the 19th Century |url=https://books.google.com/books?id=_iTnBwAAQBAJ&q=first+real#v=onepage&q=%22Indeed%20after%20seeing%20Lagrange's%20work%2C%20Euler%20dropped%20his%20own%20method%2C%20espoused%20that%20of%20Lagrange%2C%20and%20renamed%20the%20subject%20the%20calculus%20of%20variations.%22&f=false |publisher=Springer Science & Business Media |page=110 |isbn=9781461381068 |author-link=Herman Goldstine }}</ref>{{refn|\"Euler waited until Lagrange had published on the subject in 1762 ... before he committed his lecture ... to print, so as not to rob Lagrange of his glory. Indeed, it was only Lagrange's method that Euler called Calculus of Variations.\"<ref name=Thiele/> |group=\"Note\"}} \n\n[[Adrien-Marie Legendre|Legendre]] (1786) laid down a method, not entirely satisfactory, for the discrimination of maxima and minima. [[Isaac Newton]] and [[Gottfried Leibniz]] also gave some early attention to the subject.<ref name=\"brunt\">{{cite book |last = van Brunt |first = Bruce |title = The Calculus of Variations |publisher = Springer |year = 2004  |isbn = 978-0-387-40247-5}}</ref> To this discrimination [[Vincenzo Brunacci]] (1810), [[Carl Friedrich Gauss]] (1829), [[Siméon Poisson]] (1831), [[Mikhail Ostrogradsky]] (1834), and [[Carl Gustav Jakob Jacobi|Carl Jacobi]] (1837) have been among the contributors. An important general work is that of [[Pierre Frédéric Sarrus|Sarrus]] (1842) which was condensed and improved by [[Cauchy]] (1844). Other valuable treatises and memoirs have been written by [[Strauch]] (1849), [[John Hewitt Jellett|Jellett]] (1850), [[Otto Hesse]] (1857), [[Alfred Clebsch]] (1858), and [[Carll]] (1885), but perhaps the most important work of the century is that of [[Weierstrass]]. His celebrated course on the theory is epoch-making, and it may be asserted that he was the first to place it on a firm and unquestionable foundation. The [[Hilbert's twentieth problem|20th]] and the [[Hilbert's twenty-third problem|23rd]] [[Hilbert problems|Hilbert problem]] published in 1900 encouraged further development.<ref name=\"brunt\" /> \n\nIn the 20th century [[David Hilbert]], [[Emmy Noether]], [[Leonida Tonelli]], [[Henri Lebesgue]] and [[Jacques Hadamard]] among others made significant contributions.<ref name=\"brunt\" /> [[Marston Morse]] applied calculus of variations in what is now called [[Morse theory]].<ref name=\"ferguson\">{{cite arXiv |last=Ferguson |first=James |eprint=math/0402357 |title=  Brief Survey of the History of the Calculus of Variations and its Applications |year=2004 }}</ref> [[Lev Pontryagin]], [[R. Tyrrell Rockafellar|Ralph Rockafellar]] and F. H. Clarke developed new mathematical tools for the calculus of variations in [[optimal control theory]].<ref name=\"ferguson\" /> The [[dynamic programming]] of [[Richard Bellman]] is an alternative to the calculus of variations.<ref>[[Dimitri Bertsekas]]. Dynamic programming and optimal control. Athena Scientific, 2005.\n</ref><ref name=\"bellman\">{{cite journal |last=Bellman |first=Richard E. |title= Dynamic Programming and a new formalism in the calculus of variations |year=1954 |journal= Proc. Natl. Acad. Sci. | issue=4 | pages=231–235|pmc=527981 |pmid=16589462 |volume=40 |doi=10.1073/pnas.40.4.231|bibcode=1954PNAS...40..231B }}\n</ref><ref name='Kushner2004'>{{cite news | first = Harold J. | last = Kushner|authorlink=Harold J. Kushner | title = Richard E. Bellman Control Heritage Award  | year = 2004 | url = http://a2c2.org/awards/richard-e-bellman-control-heritage-award | work = American Automatic Control Council | accessdate = 2013-07-28 }} See '''2004: Harold J. Kushner''': regarding Dynamic Programming,  \"The calculus of variations had related ideas (e.g., the work of Caratheodory, the Hamilton-Jacobi equation). This led to conflicts with the calculus of variations community.\"</ref>\n\n== Extrema ==\n\nThe calculus of variations is concerned with the maxima or minima (collectively called '''extrema''') of functionals. A functional maps [[Function (mathematics)|functions]] to [[scalar (mathematics)|scalar]]s, so functionals have been described as \"functions of functions.\"  Functionals have extrema with respect to the elements {{math|''y''}} of a given [[function space]] defined over a given [[Domain of a function|domain]]. A functional {{math|''J'' [ ''y'' ]}} is said to have an extremum at the function {{math|''f''&nbsp; }} if {{math|''ΔJ'' {{=}} ''J'' [ ''y'' ] − ''J'' [ ''f'']}} has the same [[Sign (mathematics)|sign]] for all {{math|''y''}} in an arbitrarily small neighborhood of  {{math|''f'' .}}{{refn|The neighborhood of {{math|''f''}} is the part of the given function space where {{math|<nowiki>|</nowiki> ''y'' − ''f''<nowiki>|</nowiki> < h}} over the whole domain of the functions,  with {{math|h}} a positive number that specifies the size of the neighborhood.<ref name='CourHilb1953P169'>{{cite book | last1=Courant | first1=R | authorlink1=Richard Courant | last2=Hilbert | first2=D | authorlink2=David Hilbert | title = Methods of Mathematical Physics | volume = Vol. I | edition = First English | ref=harv | publisher = Interscience Publishers, Inc. | year = 1953 | location = New York | page = 169 | isbn = 978-0471504474}}</ref> |group=\"Note\"}}  The function {{math|''f''}} is called an '''extremal''' function or extremal.{{refn | group=Note | name=ExtremalVsExtremum | Note the difference between the terms extremal and extremum. An extremal is a function that makes a functional an extremum.}} The extremum {{math|''J'' [ ''f'' ]}} is called a local maximum if {{math|''ΔJ'' ≤ 0}} everywhere in an arbitrarily small neighborhood of {{math|''f'' ,}} and a local minimum if {{math|''ΔJ'' ≥ 0}} there. For a function space of continuous functions, extrema of corresponding functionals are called '''weak extrema''' or '''strong extrema''', depending on whether the first derivatives of the continuous functions are respectively all continuous or not.<ref name='GelfandFominPP12to13'>{{harvnb|Gelfand|Fomin|2000|pp=12–13}}</ref>\n\nBoth strong and weak extrema of functionals are for a space of continuous functions but weak extrema have the additional requirement that the first derivatives of the functions in the space be continuous.  Thus a strong extremum is also a weak extremum, but the [[Converse (logic)|converse]] may not hold.  Finding strong extrema is more difficult than finding weak extrema.<ref name='GelfandFominP13'>{{harvnb | Gelfand|Fomin| 2000 | p=13 }}</ref> An example of a [[Necessity and sufficiency|necessary condition]] that is used for finding weak extrema is the [[Euler–Lagrange equation]].<ref name='GelfandFominPP14to15'>{{harvnb | Gelfand|Fomin| 2000 | pp=14–15 }}</ref> {{refn | group=Note | name=SectionVarSuffCond | For a sufficient condition, see section [[Calculus of variations#Variations and sufficient condition for a minimum|Variations and sufficient condition for a minimum]].}}\n\n== Euler–Lagrange equation ==\n{{main article|Euler–Lagrange equation}}\n\nFinding the extrema of functionals is similar to finding the maxima and minima of functions. The maxima and minima of a function may be located by finding the points where its derivative vanishes (i.e., is equal to zero).  The extrema of functionals may be obtained by finding functions where the [[functional derivative]] is equal to zero.  This leads to solving the associated [[Euler–Lagrange equation]].<ref group=\"Note\">The following derivation of the Euler–Lagrange equation corresponds to the derivation on pp. 184–5 of:<br>{{cite book | author = Courant, R. | author-link = Richard Courant | author2 = Hilbert, D. | author2-link = David Hilbert | title = Methods of Mathematical Physics | volume = Vol. I | edition = First English | publisher = Interscience Publishers, Inc. | year = 1953 | location = New York | page =  | isbn = 978-0471504474}}</ref>\n\nConsider the functional\n\n:<math> J[y] = \\int_{x_1}^{x_2}  L(x,y(x),y'(x))\\, dx  \\, .</math>\nwhere\n:{{math|''x''<sub>1</sub>, ''x''<sub>2</sub>}} are [[Constant (mathematics)|constants]],\n:{{math|''y'' (''x'')}} is twice continuously differentiable,\n:{{math|''y'' ′(''x'') {{=}} ''dy / dx'' &nbsp;}},\n:{{math|''L''(''x'', ''y'' (''x''), ''y'' ′(''x''))}} is twice continuously differentiable with respect to its arguments {{math|''x'',  &nbsp;''y'',  &nbsp;''y'' ′}}.\n\nIf the functional {{math|''J''[''y'' ]}} attains a [[local minimum]] at  {{math|''f'' ,}}  and {{math|''η''(''x'')}} is an arbitrary function that has at least one derivative and vanishes at the endpoints {{math|''x''<sub>1</sub>}} and {{math|''x''<sub>2</sub> ,}} then for any number {{math|''ε''}} close to 0,\n:<math>J[f] \\le J[f + \\varepsilon \\eta] \\, .</math>\n\nThe term {{math|''εη''}} is called the '''variation''' of the function {{math|''f''}} and is denoted by {{math|''δf'' .}}<ref name='CourHilb1953P184'/>\n\nSubstituting &nbsp;{{math|''f'' + ''εη''}}  for {{math|''y''}}&nbsp; in the functional {{math|''J''[ ''y'' ] ,}} the result is a function of {{math|''ε}},\n\n:<math> \\Phi(\\varepsilon) = J[f+\\varepsilon\\eta] \\, . </math>\nSince the functional {{math|''J''[ ''y'' ]}} has a minimum for {{math|''y'' {{=}} ''f'' ,}} the function {{math|Φ(''ε'')}} has a minimum at {{math|''ε'' {{=}} 0}} and thus,<ref group=\"Note\">The product {{math|''ε''Φ′(0)}} is called the first variation of the functional {{math|''J''}} and is denoted by {{math|''δJ''}}.  Some references define the [[first variation]] differently by leaving out the {{math|''ε''}} factor.</ref>\n:<math> \\Phi'(0) \\equiv \\left.\\frac{d\\Phi}{d\\varepsilon}\\right|_{\\varepsilon = 0} = \\int_{x_1}^{x_2} \\left.\\frac{dL}{d\\varepsilon}\\right|_{\\varepsilon = 0} dx = 0 \\, . </math>\n\nTaking the [[total derivative]] of {{math|''L''[''x'', ''y'', ''y'' ′] ,}} where {{math|''y'' {{=}} ''f'' + ''ε η''}} and {{math|''y'' ′ {{=}} ''f'' ′ + ''ε η''′}} are functions of {{math|''ε''}} but {{math|''x''}} is not,\n:<math>\n\\frac{dL}{d\\varepsilon}=\\frac{\\partial L}{\\partial y}\\frac{dy}{d\\varepsilon} + \\frac{\\partial L}{\\partial y'}\\frac{dy'}{d\\varepsilon}\n</math>\n\nand since &nbsp;{{math|''dy'' /''dε'' {{=}} ''η''}} &nbsp;and&nbsp; {{math|''dy'' ′/''dε'' {{=}} ''η' ''}},\n\n:<math>\n\\frac{dL}{d\\varepsilon}=\\frac{\\partial L}{\\partial y}\\eta + \\frac{\\partial L}{\\partial y'}\\eta'.\n</math>\n\nTherefore,\n:<math>\n\\begin{align}\n\\int_{x_1}^{x_2} \\left.\\frac{dL}{d\\varepsilon}\\right|_{\\varepsilon = 0} dx\n & = \\int_{x_1}^{x_2} \\left(\\frac{\\partial L}{\\partial f} \\eta + \\frac{\\partial L}{\\partial f'} \\eta'\\right)\\, dx \\\\\n & = \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f} \\eta \\, dx + \\left.\\frac{\\partial L}{\\partial f'} \\eta \\right|_{x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\frac{\\partial L}{\\partial f'} \\, dx \\\\\n & = \\int_{x_1}^{x_2} \\left(\\frac{\\partial L}{\\partial f} \\eta - \\eta \\frac{d}{dx}\\frac{\\partial L}{\\partial f'} \\right)\\, dx\\\\\n  \\end{align}\n</math>\nwhere {{math|''L''[''x'', ''y'', ''y'' ′] → ''L''[''x'', ''f'', ''f'' ′]}} when ''ε'' = 0 and we have used  [[integration by parts]] on the second term.  The last term vanishes because {{math|''η'' {{=}} 0}} at {{math|''x<sub>1</sub>''}} and {{math|''x<sub>2</sub>''}} by definition.  Also, as previously mentioned the left side of the equation is zero so that\n:<math> \\int_{x_1}^{x_2} \\eta (x) \\left(\\frac{\\partial L}{\\partial f} - \\frac{d}{dx}\\frac{\\partial L}{\\partial f'} \\right) \\, dx = 0 \\, . </math>\n\nAccording to the [[fundamental lemma of calculus of variations]], the part of the integrand in parentheses is zero, i.e.\n:<math> \\frac{\\partial L}{\\partial f} -\\frac{d}{dx} \\frac{\\partial L}{\\partial f'}=0 </math>\nwhich is called the '''Euler–Lagrange equation'''.  The left hand side of this equation is called the [[functional derivative]] of {{math|''J''[''f'']}} and is denoted {{math|''δJ''/''δf''(''x'') .}}\n\nIn general this gives a second-order [[ordinary differential equation]] which can be solved to obtain the extremal function {{math| ''f''(''x'') .}}  The Euler–Lagrange equation is a [[necessary condition|necessary]], but not [[sufficient condition|sufficient]], condition for an extremum {{math|''J''[''f'']}}.  A sufficient condition for a minimum is given in the section [[Calculus of variations#Variations and sufficient condition for a minimum|Variations and sufficient condition for a minimum]].\n\n=== Example ===\nIn order to illustrate this process, consider the problem of finding the extremal function {{math|''y'' {{=}} ''f'' (''x'') ,}} which is the shortest curve that connects two points {{math|(''x''<sub>1</sub>, ''y''<sub>1</sub>)}} and {{math|(''x''<sub>2</sub>, ''y''<sub>2</sub>) .}} The [[arc length]] of the curve is given by\n\n:<math> A[y] = \\int_{x_1}^{x_2} \\sqrt{1 + [ y'(x) ]^2} \\, dx \\, , </math>\n\nwith\n:<math> y\\,'(x) = \\frac{dy}{dx} \\, , \\  \\  y_1=f(x_1) \\, , \\ \\  y_2=f(x_2) \\, . </math>\n\nThe Euler–Lagrange equation will now be used to find the extremal function {{math|''f'' (''x'')}} that minimizes the functional {{math|''A''[''y'' ] .}}\n:<math> \\frac{\\partial L}{\\partial f} -\\frac{d}{dx} \\frac{\\partial L}{\\partial f'}=0 </math>\nwith\n:<math>L = \\sqrt{1 + [ f'(x) ]^2} \\, . </math>\n\nSince {{math|''f''}} does not appear explicitly in {{math|''L'' ,}} the first term in the Euler–Lagrange equation vanishes for all {{math|''f'' (''x'') }}  and thus,\n:<math> \\frac{d}{dx} \\frac{\\partial L}{\\partial f'}  = 0  \\, . </math>\nSubstituting for {{math|''L''}} and taking the derivative,\n:<math> \\frac{d}{dx} \\ \\frac{ f'(x) } {\\sqrt{1 + [ f'(x) ]^2}}  \\ = 0 \\, . </math>\n\nThus\n:<math>\\frac{f'(x)}{\\sqrt{1+[f'(x)]^2}} = c \\, , </math>\nfor some constant ''c''. Then\n:<math>\\frac{[f'(x)]^2}{1+[f'(x)]^2} = c^2 \\, , </math>\nwhere\n:<math>0\\le c^2<1.</math>\nSolving, we get\n:<math>[f'(x)]^2=\\frac{c^2}{1-c^2}\\, </math>\nwhich implies that \n:<math>f'(x)=m </math>\nis a constant and therefore\nthat the shortest curve that connects two points {{math|(''x''<sub>1</sub>, ''y''<sub>1</sub>)}} and {{math|(''x''<sub>2</sub>, ''y''<sub>2</sub>) }} is\n:<math> f(x) = m x + b \\qquad \\text{with} \\ \\  m = \\frac{y_2 - y_1}{x_2 - x_1}  \\quad \\text{and} \\quad b = \\frac{x_2 y_1 - x_1 y_2}{x_2 - x_1}</math>\nand we have thus found the extremal function {{math|''f''(''x'')}} that minimizes the functional {{math|''A''[''y'']}} so that {{math|''A''[''f'']}} is a minimum. Note that {{math|''y'' {{=}} ''f''(''x'')}} is the equation for a straight line, in other words, the shortest distance between two points is a straight line.{{refn | group=Note | name=ArchimedesStraight| As an historical note, this is an axiom of [[Archimedes]]. See e.g. {{cite book|last=Kelland|first=Philip|authorlink=Philip Kelland|title=Lectures on the principles of demonstrative mathematics|year=1843|page=58|url=https://books.google.com/books?id=yQCFAAAAIAAJ&pg=PA58}} }}\n\n== Beltrami identity ==\n\nIn physics problems it frequently turns out that {{math|∂''L'' / ∂''x'' {{=}} 0}}, i.e., the integrand only depends on ''x'' through {{math|''y''(''x''),''y'''(''x'')}} but ''x'' does not appear separately.  In that case, the Euler–Lagrange equation can be simplified to the [[Beltrami identity]]:<ref>Weisstein, Eric W. [http://mathworld.wolfram.com/Euler-LagrangeDifferentialEquation.html \"Euler–Lagrange Differential Equation.\"]  From [http://mathworld.wolfram.com/ MathWorld]—A Wolfram Web Resource. See Eq. (5).</ref>\n\n:<math>L-f'\\frac{\\partial L}{\\partial f'}=C \\, ,</math>\n\nwhere {{math|''C''}} is a constant. The left hand side is the [[Legendre transformation]] of {{math|''L''}} with respect to {{math|''f'' ′}}.\n\nThe intuition behind this result is that, if the variable {{math|''x''}} is actually time, then the statement {{math|∂''L'' / ∂''x'' {{=}} 0}} implies that the Lagrangian is time-independent. By [[Noether's theorem]], there is an associated conserved quantity: the Hamiltonian, which (often) coincides with the energy of the system. This is (minus) the constant in Beltrami's identity.\n\n== Du Bois-Reymond's theorem ==\n\nThe discussion thus far has assumed that extremal functions possess two continuous derivatives, although the existence of the integral ''J'' requires only first derivatives of trial functions. The condition that the first variation vanishes at an extremal may be regarded as a '''weak form''' of the Euler–Lagrange equation. The theorem of Du Bois-Reymond asserts that this weak form implies the strong form. If ''L'' has continuous first and second derivatives with respect to all of its arguments, and if\n\n:<math>\\frac{\\partial^2 L}{\\partial f'^2} \\ne 0, </math>\n\nthen <math>f</math> has two continuous derivatives, and it satisfies the Euler–Lagrange equation.\n\n== Lavrentiev phenomenon ==\n\nHilbert was the first to give good conditions for the Euler–Lagrange equations to give a stationary solution. Within a convex area and a positive thrice differentiable Lagrangian the solutions are composed of a countable collection of sections that either go along the boundary or satisfy the Euler–Lagrange equations in the interior.\n\nHowever [[Mikhail Lavrentyev|Lavrentiev]] in 1926 showed that there are circumstances where there is no optimum solution but one can be approached arbitrarily closely by increasing numbers of sections. For instance the following:\n:<math>L(t,x,x') = (x^3-t)^2 x'^6,\\,</math>\n:<math>x(0)=0,\\, x(1)=1.\\,</math>\nHere a zig zag path gives a better solution than any smooth path and increasing the number of sections improves the solution.\n\n== Functions of several variables ==\n\nFor example, if φ(''x'',''y'') denotes the displacement of a membrane above the domain ''D'' in the ''x'',''y'' plane, then its potential energy is proportional to its surface area:\n:<math> U[\\varphi] = \\iint_D \\sqrt{1 +\\nabla \\varphi \\cdot \\nabla \\varphi} \\,dx\\,dy.\\,</math>\n[[Plateau's problem]] consists of finding a function that minimizes the surface area while assuming prescribed values on the boundary of ''D''; the solutions are called '''minimal surfaces'''. The Euler–Lagrange equation for this problem is nonlinear:\n:<math> \\varphi_{xx}(1 + \\varphi_y^2) + \\varphi_{yy}(1 + \\varphi_x^2) - 2\\varphi_x \\varphi_y \\varphi_{xy} = 0.\\,</math>\nSee Courant (1950) for details.\n\n=== Dirichlet's principle ===\nIt is often sufficient to consider only small displacements of the membrane, whose energy difference from no displacement is approximated by\n:<math> V[\\varphi] = \\frac{1}{2}\\iint_D \\nabla \\varphi \\cdot \\nabla \\varphi \\, dx\\, dy.\\,</math>\nThe functional ''V'' is to be minimized among all trial functions φ that assume prescribed values on the boundary of ''D''. If ''u'' is the minimizing function and ''v'' is an arbitrary smooth function that vanishes on the boundary of ''D'', then the first variation of <math>V[u + \\varepsilon v]</math> must vanish:\n:<math> \\frac{d}{d\\varepsilon} V[u + \\varepsilon v]|_{\\varepsilon=0} = \\iint_D \\nabla u \\cdot \\nabla v \\, dx\\,dy = 0.\\,</math>\nProvided that u has two derivatives, we may apply the divergence theorem to obtain\n:<math> \\iint_D \\nabla \\cdot (v \\nabla u) \\,dx\\,dy = \n\\iint_D \\nabla u \\cdot \\nabla v + v \\nabla \\cdot \\nabla u \\,dx\\,dy = \\int_C v \\frac{\\partial u}{\\partial n} \\, ds,</math>\nwhere ''C'' is the boundary of ''D'', ''s'' is arclength along ''C'' and <math> \\partial u / \\partial n</math> is the normal derivative of ''u'' on ''C''. Since ''v'' vanishes on ''C'' and the first variation vanishes, the result is\n:<math>\\iint_D v\\nabla \\cdot \\nabla u \\,dx\\,dy =0 \\, </math>\nfor all smooth functions v that vanish on the boundary of ''D''. The proof for the case of one dimensional integrals may be adapted to this case to show that\n:<math> \\nabla \\cdot \\nabla u= 0 \\, </math> in ''D''.\n\nThe difficulty with this reasoning is the assumption that the minimizing function u must have two derivatives. Riemann argued that the existence of a smooth minimizing function was assured by the connection with the physical problem: membranes do indeed assume configurations with minimal potential energy. Riemann named this idea the [[Dirichlet principle]] in honor of his teacher [[Peter Gustav Lejeune Dirichlet]]. However Weierstrass gave an example of a variational problem with no solution: minimize\n:<math> W[\\varphi] = \\int_{-1}^{1} (x\\varphi')^2 \\, dx\\,</math>\namong all functions ''φ'' that satisfy <math>\\varphi(-1)=-1</math> and\n<math>\\varphi(1)=1.</math>\n<math>W</math> can be made arbitrarily small by choosing piecewise linear functions that make a transition between −1 and 1 in a small neighborhood of the origin. However, there is no function that makes <math>W=0</math>.<ref>The resulting controversy over the validity of Dirichlet's principle is explained in http://turnbull.mcs.st-and.ac.uk/~history/Biographies/Riemann.html.</ref> Eventually it was shown that Dirichlet's principle is valid, but it requires a sophisticated application of the regularity theory for [[elliptic partial differential equation]]s; see Jost and Li–Jost (1998).\n\n=== Generalization to other boundary value problems ===\nA more general expression for the potential energy of a membrane is\n:<math> V[\\varphi] = \\iint_D \\left[ \\frac{1}{2} \\nabla \\varphi \\cdot \\nabla \\varphi + f(x,y) \\varphi \\right] \\, dx\\,dy \\, + \\int_C \\left[ \\frac{1}{2} \\sigma(s) \\varphi^2 + g(s) \\varphi \\right] \\, ds.</math>\nThis corresponds to an external force density <math>f(x,y)</math> in ''D'', an external force <math>g(s)</math> on the boundary ''C'', and elastic forces with modulus <math>\\sigma(s) </math> acting on ''C''. The function that minimizes the potential energy '''with no restriction on its boundary values''' will be denoted by ''u''. Provided that ''f'' and ''g'' are continuous, regularity theory implies that the minimizing function ''u'' will have two derivatives. In taking the first variation, no boundary condition need be imposed on the increment ''v''. The first variation of\n<math>V[u + \\varepsilon v]</math> is given by\n:<math> \\iint_D \\left[ \\nabla u \\cdot \\nabla v + f v \\right] \\, dx\\, dy + \\int_C \\left[ \\sigma u v + g v \\right] \\, ds =0. \\,</math>\nIf we apply the divergence theorem, the result is\n:<math> \\iint_D \\left[ -v \\nabla \\cdot \\nabla u + v f \\right] \\, dx \\, dy + \\int_C v \\left[ \\frac{\\partial u}{\\partial n} + \\sigma u + g \\right] \\, ds =0. \\,</math>\nIf we first set ''v''&nbsp;=&nbsp;0 on ''C'', the boundary integral vanishes, and we conclude as before that\n:<math> - \\nabla \\cdot \\nabla u + f =0 \\,</math>\nin ''D''. Then if we allow ''v'' to assume arbitrary boundary values, this implies that ''u'' must satisfy the boundary condition\n:<math> \\frac{\\partial u}{\\partial n} + \\sigma u + g =0, \\,</math>\non ''C''. Note that this boundary condition is a consequence of the minimizing property of ''u'': it is not imposed beforehand. Such conditions are called '''natural boundary conditions'''.\n\nThe preceding reasoning is not valid if <math> \\sigma</math> vanishes identically on ''C''. In such a case, we could allow a trial function <math> \\varphi \\equiv c</math>, where ''c'' is a constant. For such a trial function,\n:<math> V[c] = c\\left[ \\iint_D f \\, dx\\,dy + \\int_C g \\, ds \\right].</math>\nBy appropriate choice of ''c'', ''V'' can assume any value unless the quantity inside the brackets vanishes. Therefore, the variational problem is meaningless unless\n:<math> \\iint_D f \\, dx\\,dy + \\int_C g \\, ds =0.\\,</math>\nThis condition implies that net external forces on the system are in equilibrium. If these forces are in equilibrium, then the variational problem has a solution, but it is not unique, since an arbitrary constant may be added. Further details and examples are in Courant and Hilbert  (1953).\n\n== Eigenvalue problems ==\n\nBoth one-dimensional and multi-dimensional '''eigenvalue problems''' can be formulated as variational problems.\n\n=== Sturm–Liouville problems ===\n{{See also|Sturm–Liouville theory}}\nThe Sturm–Liouville eigenvalue problem involves a general quadratic form\n:<math>Q[\\varphi] =  \\int_{x_1}^{x_2} \\left[ p(x) \\varphi'(x)^2 + q(x) \\varphi(x)^2 \\right] \\, dx, \\,</math>\nwhere <math> \\varphi </math> is restricted to functions that satisfy the boundary conditions\n:<math>\\varphi(x_1)=0, \\quad \\varphi(x_2)=0. \\,</math>\nLet ''R'' be a normalization integral\n:<math> R[\\varphi] =\\int_{x_1}^{x_2} r(x)\\varphi(x)^2 \\, dx.\\,</math>\nThe functions <math>p(x)</math>  and <math>r(x)</math> are required to be everywhere positive and bounded away from zero. The primary variational problem is to minimize the ratio ''Q''/''R'' among all φ satisfying the endpoint conditions. It is shown below that the Euler–Lagrange equation for the minimizing ''u'' is\n:<math> -(pu')' +q u -\\lambda r u =0, \\,</math>\nwhere λ is the quotient\n:<math> \\lambda = \\frac{Q[u]}{R[u]}. \\,</math>\nIt can be shown (see Gelfand and Fomin 1963) that the minimizing ''u'' has two derivatives and satisfies the Euler–Lagrange equation. The associated λ will be denoted by <math>\\lambda_1</math>; it is the lowest eigenvalue for this equation and boundary conditions. The associated minimizing function will be denoted by <math>u_1(x)</math>. This variational characterization of eigenvalues leads to the [[Rayleigh–Ritz method]]: choose an approximating ''u'' as a linear combination of basis functions (for example trigonometric functions) and carry out a finite-dimensional minimization among such linear combinations. This method is often surprisingly accurate.\n\nThe next smallest eigenvalue and eigenfunction can be obtained by minimizing ''Q'' under the additional constraint\n:<math> \\int_{x_1}^{x_2} r(x) u_1(x) \\varphi(x) \\, dx=0. \\,</math>\nThis procedure can be extended to obtain the complete sequence of eigenvalues and eigenfunctions for the problem.\n\nThe variational problem also applies to more general boundary conditions. Instead of requiring that φ vanish at the endpoints, we may not impose any condition at the endpoints, and set\n:<math>Q[\\varphi] =  \\int_{x_1}^{x_2} \\left[ p(x) \\varphi'(x)^2 + q(x)\\varphi(x)^2 \\right] \\, dx + a_1 \\varphi(x_1)^2 + a_2 \\varphi(x_2)^2, \\,</math>\nwhere <math>a_1</math> and <math>a_2</math> are arbitrary. If we set <math> \\varphi = u + \\varepsilon v </math> the first variation for the ratio <math> Q/R</math>  is\n:<math> V_1 = \\frac{2}{R[u]} \\left( \\int_{x_1}^{x_2} \\left[ p(x) u'(x)v'(x) + q(x)u(x)v(x) -\\lambda u(x) v(x) \\right] \\, dx + a_1 u(x_1)v(x_1) + a_2 u(x_2)v(x_2) \\right) , \\,</math>\nwhere λ is given by the ratio <math> Q[u]/R[u]</math> as previously.\nAfter integration by parts,\n:<math> \\frac{R[u]}{2} V_1 = \\int_{x_1}^{x_2} v(x) \\left[ -(p u')' + q u -\\lambda r u \\right] \\, dx + v(x_1)[ -p(x_1)u'(x_1) + a_1 u(x_1)] +  v(x_2) [p(x_2) u'(x_2) + a_2 u(x_2)]. \\,</math>\nIf we first require that ''v'' vanish at the endpoints, the first variation will vanish for all such ''v'' only if\n:<math> -(p u')' + q u -\\lambda r u =0 \\quad \\hbox{for} \\quad x_1 < x < x_2.\\,</math>\nIf ''u'' satisfies this condition, then the first variation will vanish for arbitrary ''v'' only if\n:<math> -p(x_1)u'(x_1) + a_1 u(x_1)=0,  \\quad \\hbox{and} \\quad p(x_2) u'(x_2) + a_2 u(x_2)=0.\\,</math>\nThese latter conditions are the '''natural boundary conditions''' for this problem, since they are not imposed on trial functions for the minimization, but are instead a consequence of the minimization.\n\n=== Eigenvalue problems in several dimensions ===\nEigenvalue problems in higher dimensions are defined in analogy with the one-dimensional case. For example, given a domain ''D'' with boundary ''B'' in three dimensions we may define\n\n:<math> Q[\\varphi] = \\iiint_D p(X) \\nabla \\varphi \\cdot \\nabla \\varphi + q(X) \\varphi^2 \\, dx \\, dy \\, dz + \\iint_B \\sigma(S) \\varphi^2 \\, dS, \\,</math>\nand\n:<math> R[\\varphi] = \\iiint_D r(X) \\varphi(X)^2 \\, dx \\, dy \\, dz.\\,</math>\nLet ''u'' be the function that minimizes the quotient <math> Q[\\varphi] / R[\\varphi], </math>\nwith no condition prescribed on the boundary ''B''. The Euler–Lagrange equation satisfied by ''u'' is\n:<math> -\\nabla \\cdot (p(X) \\nabla u) + q(x) u - \\lambda r(x) u=0,\\,</math>\nwhere\n:<math> \\lambda = \\frac{Q[u]}{R[u]}.\\,</math>\nThe minimizing ''u'' must also satisfy the natural boundary condition\n:<math> p(S) \\frac{\\partial u}{\\partial n} + \\sigma(S) u =0,</math>\non the boundary ''B''. This result depends upon the regularity theory for elliptic partial differential equations; see Jost and Li–Jost (1998) for details. Many extensions, including completeness results, asymptotic properties of the eigenvalues and results concerning the nodes of the eigenfunctions are in Courant and Hilbert (1953).\n\n== Applications ==\n{{Main article|Applications of the calculus of variations}}\n\nSome applications of the calculus of variations include:\n*The derivation of the [[Catenary]] shape\n*[[Newton's minimal resistance problem]]\n*The [[Brachistochrone curve|Brachistochrone]] problem\n*[[Isoperimetric]] problems\n*[[Geodesic]]s on surfaces\n*[[Minimal surface]]s and [[Plateau's problem]]\n*[[Optimal control]]\n\n=== Fermat's principle ===\n[[Fermat's principle]] states that light takes a path that (locally) minimizes the optical length between its endpoints. If the ''x''-coordinate is chosen as the parameter along the path, and <math>y=f(x)</math> along the path, then the optical length is given by\n\n:<math> A[f] = \\int_{x=x_0}^{x_1} n(x,f(x)) \\sqrt{1 + f'(x)^2} dx, \\,</math>\n\nwhere the refractive index <math>n(x,y)</math> depends upon the material.\nIf we try <math> f(x) = f_0 (x) + \\varepsilon f_1 (x)</math>\nthen the [[first variation]] of ''A'' (the derivative of ''A'' with respect to ε) is\n\n:<math> \\delta A[f_0,f_1] = \\int_{x=x_0}^{x_1} \\left[ \\frac{ n(x,f_0) f_0'(x) f_1'(x)}{\\sqrt{1 + f_0'(x)^2}} + n_y (x,f_0) f_1 \\sqrt{1 + f_0'(x)^2} \\right] dx.</math>\n\nAfter integration by parts of the first term within brackets, we obtain the Euler–Lagrange equation\n\n:<math> -\\frac{d}{dx} \\left[\\frac{ n(x,f_0) f_0'}{\\sqrt{1 + f_0'^2}} \\right] + n_y (x,f_0) \\sqrt{1 + f_0'(x)^2} =0. \\,</math>\n\nThe light rays may be determined by integrating this equation. This formalism is used in the context of [[Lagrangian optics]] and [[Hamiltonian optics]].\n\n==== Snell's law ====\nThere is a discontinuity of the refractive index when light enters or leaves a lens. Let\n\n:<math> n(x,y) = n_- \\quad \\hbox{if} \\quad x<0, \\,</math>\n:<math> n(x,y) = n_+ \\quad \\hbox{if} \\quad x>0,\\,</math>\n\nwhere <math>n_-</math> and <math>n_+</math> are constants. Then the Euler–Lagrange equation holds as before in the region where ''x''<0 or ''x''>0, and in fact the path is a straight line there, since the refractive index is constant. At the ''x''=0, ''f'' must be continuous, but ''f' '' may be discontinuous. After integration by parts in the separate regions and using the Euler–Lagrange equations, the first variation takes the form\n\n:<math> \\delta A[f_0,f_1] = f_1(0)\\left[ n_-\\frac{f_0'(0_-)}{\\sqrt{1 + f_0'(0_-)^2}} -n_+\\frac{f_0'(0_+)}{\\sqrt{1 + f_0'(0_+)^2}} \\right].\\,</math>\n\nThe factor multiplying <math>n_-</math> is the sine of angle of the incident ray with the ''x'' axis, and the factor multiplying <math>n_+</math> is the sine of angle of the refracted ray with the ''x'' axis.  [[Snell's law]] for refraction requires that these terms be equal. As this calculation demonstrates, Snell's law is equivalent to vanishing of the first variation of the optical path length.\n\n==== Fermat's principle in three dimensions ====\nIt is expedient to use vector notation: let <math>X=(x_1,x_2,x_3),</math> let ''t'' be a parameter,  let <math>X(t)</math> be the parametric representation of a curve ''C'', and let <math>\\dot X(t)</math> be its tangent vector. The optical length of the curve is given by\n\n:<math> A[C] = \\int_{t=t_0}^{t_1} n(X) \\sqrt{ \\dot X \\cdot \\dot X} \\, dt. \\,</math>\n\nNote that this integral is invariant with respect to changes in the parametric representation of ''C''. The Euler–Lagrange equations for a minimizing curve have the symmetric form\n\n:<math> \\frac{d}{dt} P = \\sqrt{ \\dot X \\cdot \\dot X} \\, \\nabla n, \\,</math>\n\nwhere\n:<math> P = \\frac{n(X) \\dot X}{\\sqrt{\\dot X \\cdot \\dot X} }.\\,</math>\n\nIt follows from the definition that ''P'' satisfies\n\n:<math> P \\cdot P = n(X)^2. \\,</math>\n\nTherefore, the integral may also be written as\n\n:<math> A[C] = \\int_{t=t_0}^{t_1} P \\cdot \\dot X \\, dt.\\,</math>\n\nThis form suggests that if we can find a function ψ whose gradient is given by ''P'', then the integral ''A'' is given by the difference of ψ at the endpoints of the interval of integration. Thus the problem of studying the curves that make the integral stationary can be related to the study of the level surfaces of ψ. In order to find such a function, we turn to the wave equation, which governs the propagation of light. This formalism is used in the context of [[Lagrangian optics]] and [[Hamiltonian optics]].\n\n===== Connection with the wave equation =====\nThe [[wave equation]] for an inhomogeneous medium is\n\n:<math> u_{tt} = c^2 \\nabla \\cdot \\nabla u, \\,</math>\n\nwhere ''c'' is the velocity, which generally depends upon ''X''. Wave fronts for light are characteristic surfaces for this partial differential equation: they satisfy\n\n:<math> \\varphi_t^2 = c(X)^2 \\, \\nabla \\varphi \\cdot \\nabla \\varphi. \\,</math>\n\nWe may look for solutions in the form\n\n:<math> \\varphi(t,X) = t - \\psi(X). \\,</math>\n\nIn that case, ψ satisfies\n\n:<math> \\nabla \\psi \\cdot \\nabla \\psi = n^2, \\,</math>\n\nwhere <math>n=1/c.</math> According to the theory of [[first-order partial differential equation]]s, if <math>P = \\nabla \\psi, </math> then ''P'' satisfies\n\n:<math> \\frac{dP}{ds} = n \\, \\nabla n,</math>\n\nalong a system of curves ('''the light rays''') that are given by\n\n:<math> \\frac{dX}{ds} = P. \\,</math>\n\nThese equations for solution of a first-order partial differential equation are identical to the Euler–Lagrange equations if we make the identification\n\n:<math> \\frac{ds}{dt} = \\frac{\\sqrt{ \\dot X \\cdot \\dot X} }{n}. \\,</math>\n\nWe conclude that the function ψ is the value of the minimizing integral ''A'' as a function of the upper end point. That is, when a family of minimizing curves is constructed, the values of the optical length satisfy the characteristic equation corresponding the wave equation. Hence, solving the associated partial differential equation of first order is equivalent to finding families of solutions of the variational problem. This is the essential content of the [[Hamilton–Jacobi theory]], which applies to more general variational problems.\n\n=== Action principle ===\n{{main article|Action (physics)}}\nIn classical mechanics, the action, ''S'', is defined as the time integral of the Lagrangian, ''L''.  The Lagrangian is the difference of energies,\n:<math> L = T - U, \\,</math>\nwhere ''T'' is the [[kinetic energy]] of a mechanical system and ''U'' its [[potential energy]]. [[Hamilton's principle]] (or the action principle) states that the motion of a conservative holonomic (integrable constraints) mechanical system is such that the action integral\n:<math> S  = \\int_{t=t_0}^{t_1} L(x, \\dot x, t) \\, dt </math>\nis stationary with respect to variations in the path ''x''(''t'').\nThe Euler–Lagrange equations for this system are known as Lagrange's equations:\n:<math> \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot x} = \\frac{\\partial L}{\\partial x}, \\,</math>\nand they are equivalent to Newton's equations of motion (for such systems).\n\nThe conjugate momenta ''P'' are defined by\n:<math> p = \\frac{\\partial L}{\\partial \\dot x}. \\,</math>\nFor example, if\n:<math> T = \\frac{1}{2} m \\dot x^2, \\,</math>\nthen\n:<math> p = m \\dot x. \\,</math>\n[[Hamiltonian mechanics]] results if the conjugate momenta are introduced in place of <math>\\dot x</math> by a Legendre transformation of the Lagrangian ''L'' into the Hamiltonian ''H'' defined by\n:<math> H(x, p, t) = p \\,\\dot x - L(x,\\dot x, t).\\,</math>\nThe Hamiltonian is the total energy of the system: ''H'' = ''T'' + ''U''.\nAnalogy with Fermat's principle suggests that solutions of Lagrange's equations (the particle trajectories) may be described in terms of level surfaces of some function of ''X''. This function is a solution of the [[Hamilton–Jacobi equation]]:\n:<math> \\frac{\\partial  \\psi}{\\partial t} + H\\left(x,\\frac{\\partial  \\psi}{\\partial x},t\\right) =0.\\,</math>\n\n== Variations and sufficient condition for a minimum ==\n\nCalculus of variations is concerned with variations of functionals, which are small changes in the functional's value due to small changes in the function that is its argument.  The '''first variation'''{{refn| group=Note| name=AltFirst| The first variation is also called the variation, differential, or first differential.}} is defined as the linear part of the change in the functional, and the '''second variation'''{{refn| group=Note| name=AltSecond| The second variation is also called the second differential.}} is defined as the quadratic part.<ref name='GelfandFominP11–12,99'>{{harvnb | Gelfand|Fomin| 2000 | pp=11–12, 99 }}</ref>\n\nFor example, if {{math|''J''[''y'']}} is a functional with the function {{math|''y'' {{=}} ''y''(''x'')}} as its argument, and there is a small change in its argument from {{math|''y''}} to {{math|''y'' + ''h''}}, where {{math|''h'' {{=}} ''h''(''x'')}} is a function in the same function space as {{math|''y''}}, then the corresponding change in the functional is\n\n:<math> \\Delta J[h] = J[y+h] - J[y]. </math>&nbsp;&nbsp;{{refn|group=Note|name=SimplifyNotation|Note that {{math|Δ ''J''[''h'']}} and the variations below, depend on both {{math|''y''}} and {{math|''h''}}. The argument {{math|''y''}} has been left out to simplify the notation. For example,  {{math|Δ ''J''[''h'']}} could have been written  {{math|Δ ''J''[''y'' ; ''h'']}}.<ref name='GelfandFominP12FN6'>{{harvnb | Gelfand|Fomin| 2000 | p=12, footnote 6}}</ref>}}\n\nThe functional {{math|''J''[''y'']}} is said to be '''differentiable''' if\n\n:<math> \\Delta J[h] = \\varphi [h] + \\varepsilon \\|h\\|, </math>\n\nwhere {{math|''φ''[''h'']}} is a linear functional,{{refn| group=Note| name=Linear|  A functional {{math|''φ''[''h'']}} is said to be '''linear''' if {{math|''φ''[''αh''] {{=}} ''α φ''[''h'']}} &nbsp; and &nbsp; {{math|''φ''[''h''<sub>1</sub> +''h''<sub>2</sub>] {{=}} ''φ''[''h''<sub>1</sub>] + ''φ''[''h''<sub>2</sub>]}} , where {{math|''h'', ''h''<sub>1</sub>, ''h''<sub>2</sub>}} are functions and  {{math|''α''}} is a real number.<ref name='GelfandFominP8'>{{harvnb | Gelfand|Fomin| 2000 | p=8 }}</ref>}} {{math|''<nowiki>||</nowiki>h<nowiki>||</nowiki>''}} is the norm of {{math|''h''}},{{refn| group=Note| name=Norm| For a function {{math|''h'' {{=}} ''h''(''x'')}} that is defined for {{math|''a'' ≤ ''x'' ≤ ''b''}}, where {{math|''a''}} and {{math|''b''}} are real numbers, the norm of {{math|''h''}} is its maximum absolute value, i.e.  {{math|''<nowiki>||</nowiki>h<nowiki>||</nowiki>''}} {{=}} max {{math|<nowiki>|</nowiki>''h''(''x'')<nowiki>|</nowiki>}} for {{math|''a'' ≤ ''x'' ≤ ''b''}}.<ref name='GelfandFominP6'>{{harvnb | Gelfand|Fomin| 2000 | p=6 }}</ref>}} and {{math|''ε → 0''}}  as {{math|''<nowiki>||</nowiki>h<nowiki>||</nowiki> → 0''}}. The linear functional {{math|''φ''[''h'']}} is the first variation of {{math|''J''[''y'']}} and is denoted by,<ref name='GelfandFominP11–12'>{{harvnb | Gelfand|Fomin| 2000 | pp=11–12}}</ref>\n\n:<math>\\delta J[h] = \\varphi[h]. </math>\n\nThe functional {{math|''J''[''y'']}} is said to be '''twice differentiable''' if\n\n:<math> \\Delta J[h] = \\varphi_1 [h] + \\varphi_2 [h] + \\varepsilon \\|h\\|^2, </math>\n\nwhere {{math|''φ''<sub>1</sub>[''h'']}} is a linear functional (the first variation), {{math|''φ''<sub>2</sub>[''h'']}} is a quadratic functional,{{refn| group=Note| name=Quadratic|  A functional is said to be '''quadratic''' if it is a bilinear functional with two argument functions that are equal.  A '''bilinear functional''' is a functional that depends on two argument functions and is linear when each argument function in turn is fixed while the other argument function is variable.<ref name='GelfandFominP97–98'>{{harvnb | Gelfand|Fomin| 2000 | pp=97–98 }}</ref>}} and {{math|''ε → 0''}}  as {{math|''<nowiki>||</nowiki>h<nowiki>||</nowiki> → 0''}}. The quadratic functional {{math|''φ''<sub>2</sub>[''h'']}} is the second variation of {{math|''J''[''y'']}} and is denoted by,<ref name='GelfandFominP99'>{{harvnb | Gelfand|Fomin| 2000 | p=99 }}</ref>\n\n:<math>\\delta^2 J[h] = \\varphi_2[h]. </math>\n\nThe second variation {{math|''δ''<sup>2</sup>''J''[''h'']}} is said to be '''strongly positive''' if\n:<math>\\delta^2J[h] \\ge k \\|h\\|^2, </math>\nfor all {{math|''h''}} and for some constant {{math|''k'' > ''0''}} .<ref name='GelfandFominP100'>{{harvnb | Gelfand|Fomin| 2000 | p=100 }}</ref>\n\nUsing the above definitions, especially the definitions of first variation, second variation, and strongly positive, the following sufficient condition for a minimum of a functional can be stated. \n\n{{quote box|align=left |fontsize=100% |border=2px |quote='''Sufficient condition for a minimum:'''\n\n:The functional {{math|''J''[''y'']}} has a minimum at {{math|''y'' {{=}}  ''ŷ''}} if its first variation {{math|''δJ''[''h''] {{=}} 0}} at {{math|''y'' {{=}}  ''ŷ''}} and its second variation {{math|''δ''<sup>2</sup>''J''[''h'']}} is strongly positive at {{math|''y'' {{=}}  ''ŷ''}} .<ref name='GelfandFominP100Theorem2'>{{harvnb | Gelfand|Fomin| 2000 | p=100 |loc=Theorem 2}}</ref> {{refn| group=Note| name=sufficient| For other sufficient conditions, see in {{harvnb|Gelfand|Fomin|2000}},\n*'''Chapter{{nbsp}}5: \"The Second Variation. Sufficient Conditions for a Weak Extremum\" – ''' Sufficient conditions for a weak minimum are given by the theorem on p.{{nbsp}}116.\n*'''Chapter{{nbsp}}6: \"Fields. Sufficient Conditions for a Strong Extremum\" – ''' Sufficient conditions for a strong minimum are given by the theorem on p.{{nbsp}}148.}}{{refn| group=Note| name=FuncMin| One may note the similarity to the sufficient condition for a minimum of a function, where the first derivative is zero and the second derivative is positive.}} }} \n{{clear}}\n\n== See also ==\n{{Div col|colwidth=25em}}\n*[[First variation]]\n*[[Isoperimetric inequality]]\n*[[Variational principle]]\n*[[Variational bicomplex]]\n*[[Fermat's principle]]\n*[[Principle of least action]]\n*[[Infinite-dimensional optimization]]\n*[[Functional analysis]]\n*[[Ekeland's variational principle]]\n*[[Inverse problem for Lagrangian mechanics]]\n*[[Obstacle problem]]\n*[[Perturbation methods]]\n*[[Young measure]]\n*[[Optimal control]]\n*[[Direct method in calculus of variations]]\n*[[Noether's theorem]]\n*[[De Donder–Weyl theory]]\n*[[Variational Bayesian methods]]\n*[[Chaplygin problem]]\n*[[Nehari manifold]]\n*[[Hu–Washizu principle]]\n*[[Luke's variational principle]]\n*[[Mountain pass theorem]]\n*{{cat|Variational analysts}}\n*[[Central tendency#Solutions to variational problems|Measures of central tendency as solutions to variational problems]]\n*[[Stampacchia Medal]]\n*[[Fermat Prize]]\n*[[Convenient vector space]]\n{{div col end}}\n\n== Notes ==\n{{reflist|30em|group=\"Note\"}}\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n* Benesova, B. and Kruzik, M.: [http://epubs.siam.org/doi/abs/10.1137/16M1060947] Weak Lower Semicontinuity of Integral Functionals and Applications. SIAM Review  59(4) (2017), 703–766.\n* [[:fr:Bernard Dacorogna|Dacorogna, Bernard]]. ''[http://www.worldscientific.com/worldscibooks/10.1142/p967 Introduction to the Calculus of Variations (3rd Edition)]'', 2014, World Scientific Publishing, {{isbn|978-1-78326-551-0}}. ''[http://www.worldscientific.com/doi/suppl/10.1142/p967/suppl_file/p967_chap01.pdf Introduction]''\n* [[Oskar Bolza|Bolza, O.]]: Lectures on the Calculus of Variations. Chelsea Publishing Company, 1904, available on Digital Mathematics library [http://quod.lib.umich.edu/cgi/t/text/text-idx?c=umhistmath;idno=ACM2513]. 2nd edition republished in 1961, paperback in 2005, {{isbn|978-1-4181-8201-4}}.\n* Cassel, Kevin W.: [https://books.google.com/books?id=pNHFJTKixisC Variational Methods with Applications in Science and Engineering], Cambridge University Press, 2013.\n* Clegg, J.C.: [https://books.google.com/books/about/Calculus_of_variations.html?id=qTnvAAAAMAAJ Calculus of Variations], Interscience Publishers Inc., 1968.\n* [[Richard Courant|Courant, R.]]: [https://books.google.com/books?id=GCDSBwAAQBAJ Dirichlet's principle, conformal mapping and minimal surfaces]. Interscience, 1950.\n* Elsgolc, L.E.: [https://books.google.com/books?id=MAU_AwAAQBAJ Calculus of Variations], Pergamon Press Ltd., 1962.\n* Forsyth, A.R.: [https://archive.org/stream/CalculusOfVariations/Forsyth-CalculusOfVariations#page/n3/mode/2up Calculus of Variations], Dover, 1960.\n* Fox, Charles: [https://books.google.com/books/about/An_Introduction_to_the_Calculus_of_Varia.html?id=PV2IovCWTcYC An Introduction to the Calculus of Variations], Dover Publ., 1987.\n* Jost, J. and X. Li-Jost: [https://books.google.com/books?id=QN8Iw7fUA-8C Calculus of Variations]. Cambridge University Press, 1998.\n* Lebedev, L.P. and Cloud, M.J.: [https://books.google.com/books?id=_T3ez-32YVsC The Calculus of Variations and Functional Analysis with Optimal Control and Applications in Mechanics], World Scientific, 2003, pages 1–98.\n* Logan, J. David: [https://books.google.com/books?id=nUk_AQAAIAAJ Applied Mathematics], 3rd Ed. Wiley-Interscience, 2006\n* Roubicek, T.: ''[http://www.wiley-vch.de/books/sample/3527411887_c17.pdf Calculus of variations]. Chap.17 in: [http://www.wiley-vch.de/publish/en/books/forthcomingTitles/MA00/3-527-41188-7/?sID=nrgsqk516u2v9ffab8u7io1dq4 Mathematical Tools for Physicists]. (Ed. M. Grinfeld) J. Wiley, Weinheim, 2014, {{isbn|978-3-527-41188-7}}, pp.&nbsp;551–588.\n* Sagan, Hans: [https://books.google.com/books/about/Introduction_to_the_Calculus_of_Variatio.html?id=abhS8PgpBskC Introduction to the Calculus of Variations], Dover, 1992.\n* Weinstock, Robert: [https://books.google.com/books?id=6wSVuWH1PrsC Calculus of Variations with Applications to Physics and Engineering], Dover, 1974 (reprint of 1952 ed.).\n*[https://web.archive.org/web/20070911211556/http://www.mpri.lsu.edu/textbook/Chapter8-b.htm Chapter 8: Calculus of Variations], from [https://web.archive.org/web/20070705141725/http://www.mpri.lsu.edu/bookindex.html ''Optimization for Engineering Systems''], by Ralph W. Pike, [[Louisiana State University]].\n\n== External links ==\n* [https://www.encyclopediaofmath.org/index.php/Variational_calculus Variational calculus]. ''[[Encyclopedia of Mathematics]]''.\n* [http://planetmath.org/calculusofvariations calculus of variations]. ''[[PlanetMath]]''.\n* [http://mathworld.wolfram.com/CalculusofVariations.html Calculus of Variations]. ''[[MathWorld]]''.\n* [https://web.archive.org/web/20170609215523/http://www.exampleproblems.com/wiki/index.php/Calculus_of_Variations Calculus of variations]. Example problems.\n* [https://www.youtube.com/playlist?list=PL521C2DFD15FF568C Mathematics - Calculus of Variations and Integral Equations]. Lectures on [[YouTube]].\n* Selected papers on Geodesic Fields. [http://neo-classical-physics.info/uploads/3/0/6/5/3065888/geodesic_fields_-_pt._1.pdf Part I], [http://neo-classical-physics.info/uploads/3/0/6/5/3065888/geodesic_fields_-_pt._2.pdf Part II].\n\n{{Authority control}}\n\n{{DEFAULTSORT:Calculus Of Variations}}\n[[Category:Calculus of variations| ]]\n[[Category:Optimization in vector spaces|Calculus]]"
    },
    {
      "title": "Generalized semi-infinite programming",
      "url": "https://en.wikipedia.org/wiki/Generalized_semi-infinite_programming",
      "text": "{{Refimprove|date=May 2008}}\n\nIn [[mathematics]], a [[Semi-Infinite Programming|semi-infinite programming (SIP)]] problem is an optimization problem with a finite number of variables and an infinite number of constraints. The constraints are typically parameterized. In a '''generalized semi-infinite programming''' ('''GSIP''') problem, the feasible set of the parameters depends on the variables.<ref>O. Stein and G. Still, ''On generalized semi-infinite optimization and bilevel optimization'', European J. Oper. Res., 142 (2002), pp. 444-462</ref>\n\n== Mathematical formulation of the problem ==\nThe problem can be stated simply as:\n:<math> \\min\\limits_{x \\in X}\\;\\; f(x) </math>\n\n:<math> \\mbox{subject to: }\\ </math>\n\n::<math> g(x,y) \\le 0, \\;\\;  \\forall y \\in Y(x) </math>\n\nwhere\n:<math>f: R^n \\to R</math>\n:<math>g: R^n \\times R^m \\to R</math>\n:<math>X \\subseteq R^n</math>\n:<math>Y \\subseteq R^m.</math>\n\nIn the special case that the set :<math>Y(x)</math> is nonempty for all <math>x \\in X</math> GSIP can be cast as bilevel programs ([[Multilevel programming]]).\n\n== Methods for solving the problem ==\n{{Empty section|date=July 2010}}\n\n==Examples==\n{{Empty section|date=July 2010}}\n\n== See also ==\n* [[Optimization (mathematics)|optimization]]\n* [[Semi-Infinite Programming|Semi-Infinite Programming (SIP)]]\n\n== References==\n{{Reflist}}\n\n== External links==\n*[http://glossary.computing.society.informs.org/ Mathematical Programming Glossary]\n\n[[Category:Optimization in vector spaces]]"
    },
    {
      "title": "Infinite-dimensional optimization",
      "url": "https://en.wikipedia.org/wiki/Infinite-dimensional_optimization",
      "text": "In certain [[optimization (mathematics)|optimization]] problems the unknown optimal solution might not be a number or a vector, but rather a continuous quantity, for example a [[function (mathematics)|function]] or the shape of a body. Such a problem is an '''infinite-dimensional optimization''' problem, because, a continuous quantity cannot be determined by a [[finite set|finite]] number of certain [[degrees of freedom (physics and chemistry)|degrees of freedom]].\n\n== Examples ==\n\n* Find the [[Euclidean shortest path|shortest path]] between two points in a plane. The variables in this problem are the curves connecting the two points. The optimal solution is of course the line segment joining the points, if the metric defined on the plane is the Euclidean metric.\n* Given two cities in a country with lots of hills and valleys, find the shortest road going from one city to the other. This problem is a generalization of the above, and the solution is not as obvious.\n* Given two circles which will serve as top and bottom for a cup of given height, find the shape of the side wall of the cup so that the side wall has [[minimal surface|minimal area]]. The intuition would suggest that the cup must have conical or cylindrical shape, which is false. The actual minimum surface is the [[catenoid]].\n* Find the shape of a bridge capable of sustaining given amount of traffic using the smallest amount of material.\n* Find the shape of an airplane which bounces away most of the radio waves from an enemy radar.\n\nInfinite-dimensional optimization problems can be more challenging than finite-dimensional ones. Typically one needs to employ methods from [[partial differential equation]]s to solve such problems.\n\nSeveral disciplines which study infinite-dimensional optimization problems are [[calculus of variations]], [[optimal control]] and  [[shape optimization]].\n\n==See also==\n*[[Semi-infinite programming]]\n\n==References==\n* David Luenberger (1997). ''Optimization by Vector Space Methods.'' John Wiley & Sons.  {{ISBN|0-471-18117-X}}.\n* Edward J. Anderson and Peter Nash, ''Linear Programming in Infinite-Dimensional Spaces'', Wiley, 1987.\n* M. A. Goberna and M. A. López, ''Linear Semi-Infinite Optimization'', Wiley, 1998. \n* Cassel, Kevin W.: Variational Methods with Applications in Science and Engineering, Cambridge University Press, 2013.\n\n[[Category:Functional analysis]]\n[[Category:Optimization in vector spaces]]"
    },
    {
      "title": "Variational principle",
      "url": "https://en.wikipedia.org/wiki/Variational_principle",
      "text": "The '''variational principle''' is a scientific principle used within the [[calculus of variations]], which develops general methods for finding functions which extremize the value of quantities that depend upon those functions. For example, to answer this question: \"What is the [[Catenary|shape of a chain]] suspended at both ends?\" we can use the variational principle that the shape must minimize the gravitational potential energy.\n\n==Overview==\nAny physical law which can be expressed as a variational principle describes a [[self-adjoint operator]].<ref>{{cite book |first=Cornelius |last=Lanczos |author-link=Cornelius Lanczos |orig-year=1st published 1970, [[University of Toronto Press]] |title=The Variational Principles of Mechanics |publisher=Dover |date=1974 |ISBN=0-8020-1743-6 |edition=4th, paperback}}</ref> These expressions are also called [[Hermitian]]. Such an expression describes an [[Invariant (mathematics)|invariant]] under a Hermitian transformation.\n\n==History==\n{{main|History of variational principles in physics}}\n[[Felix Klein]]'s [[Erlangen program]] attempted to identify such invariants under a group of transformations. In what is referred to in physics as [[Noether's theorem]], the [[Poincaré group]] of transformations (what is now called a [[gauge group]]) for [[general relativity]] defines symmetries under a group of transformations which depend on a variational principle, or [[Action (physics)|action principle]].\n\n==Action principle==\n{{main|Action principle}}\n{{empty section|date=May 2016}}\n\n==Examples==\n* [[Lord Rayleigh]]'s [[Rayleigh–Ritz method|variational principle]]\n* [[Ekeland's variational principle]]\n* [[Fermat's principle]] in [[geometrical optics]]\n* The [[principle of least action]] in [[mechanics]], [[electromagnetic theory]], and [[quantum mechanics]]\n* [[Maupertuis' principle]] in [[classical mechanics]]\n* The [[Einstein equation]] also involves  a variational principle, the [[Einstein–Hilbert action]]\n* [[Gauss's principle of least constraint]]\n* [[Gauss's principle of least constraint#Hertz's principle of least curvature|Hertz's principle of least curvature]]\n* [[Palatini variation]]\n* The [[variational method (quantum mechanics)|variational method]] in quantum mechanics\n* The [[finite element method]]\n\n==References==\n{{Reflist}}\n* {{cite journal|last=Ekeland|first=Ivar|authorlink=Ivar Ekeland|title=Nonconvex minimization problems|journal=Bulletin of the American Mathematical Society|series=New Series|volume=1|year=1979|number=3|pages=443–474|doi=10.1090/S0273-0979-1979-14595-6|mr=526967|ref=harv}}\n* S T Epstein  1974 \"The Variation Method in Quantum Chemistry\". (New York: Academic)\n* [[R.P. Feynman]], \"The Principle of Least Action\", an almost verbatim lecture transcript in Volume 2, Chapter 19 of ''The Feynman Lectures on Physics'', Addison-Wesley, 1965.  An introduction in Feynman's inimitable style.\n* C Lanczos, ''The Variational Principles of Mechanics'' (Dover Publications)\n* R K Nesbet 2003 \"Variational Principles and Methods In Theoretical Physics and Chemistry\". (New York: Cambridge U.P.)\n* S K Adhikari  1998 \"Variational Principles for the Numerical Solution of Scattering Problems\". (New York: Wiley)\n* C G Gray, G Karl G and V A Novikov 1996, ''Ann. Phys.'' 251 1.\n* C.G. Gray, G. Karl, and V. A. Novikov, \"[https://arxiv.org/abs/physics/0312071 Progress in Classical and Quantum Variational Principles]\". 11 December 2003. physics/0312071 Classical Physics.\n*{{cite book | author=Griffiths, David J.|title=Introduction to Quantum Mechanics (2nd ed.) | publisher=Prentice Hall |year=2004 |isbn=0-13-805326-X}}\n* [[Stephen Wolfram]], ''[[A New Kind of Science]]'' (2002), [http://www.wolframscience.com/nksonline/page-1052 p. 1052]\n* John Venables, \"[http://venables.asu.edu/quant/varprin.html The Variational Principle and some applications]\". Dept of Physics and Astronomy, Arizona State University, Tempe, Arizona (Graduate Course: Quantum Physics)\n* Andrew James Williamson, \"[http://www.tcm.phy.cam.ac.uk/~ajw29/thesis/node15.html The Variational Principle] -- Quantum monte carlo calculations of electronic excitations\". Robinson College, Cambridge,  Theory of Condensed Matter Group, Cavendish Laboratory. September 1996. (dissertation of Doctor of Philosophy)\n* Kiyohisa Tokunaga, \"[https://web.archive.org/web/20041102095610/http://www.d3.dion.ne.jp/~kiyohisa/tieca/26.htm Variational Principle for Electromagnetic Field]\". Total Integral for Electromagnetic Canonical Action, Part Two, Relativistic Canonical Theory of Electromagnetics, Chapter VI\n*[[Vadim Komkov|Komkov, Vadim]] (1986) Variational principles of continuum mechanics with engineering applications. Vol. 1. Critical points theory. Mathematics and its Applications, 24. D. Reidel Publishing Co., Dordrecht.\n* Cassel, Kevin W.: Variational Methods with Applications in Science and Engineering, Cambridge University Press, 2013.\n\n[[Category:Calculus of variations| ]]\n[[Category:Theoretical physics]]\n[[Category:Articles containing proofs]]\n[[Category:Principles]]\n[[Category:Variational principles| ]]"
    },
    {
      "title": "Action (physics)",
      "url": "https://en.wikipedia.org/wiki/Action_%28physics%29",
      "text": "In [[physics]], '''action''' is an attribute of the [[dynamics (physics)|dynamics]] of a [[physical system]] from which the [[equations of motion]] of the system can be derived. It is a [[functional (mathematics)|mathematical functional]] which takes the [[trajectory]], also called ''path'' or ''history'', of the system as its argument and has a [[real number]] as its result. Generally, the action takes different values for different paths.<ref name=\"mcgraw1\">McGraw Hill Encyclopaedia of Physics (2nd Edition), C.B. Parker, 1994, {{ISBN|0-07-051400-3}}</ref> Action has the [[dimensional analysis|dimensions]] of [[energy|[energy]]]⋅[[time|[time]]] or [[momentum|[momentum]]]⋅[[length|[length]]], and its [[SI unit]] is [[joule]]-second.\n\n== Introduction ==\n{{main article|Hamilton's principle}}\n\n[[Hamilton's principle]] states that the differential equations of motion for ''any'' physical system can be re-formulated as an equivalent [[integral equation]].  Thus, there are two distinct approaches for formulating dynamical models.\n\nIt applies not only to the [[classical mechanics]] of a single particle, but also to [[classical field]]s such as the [[electromagnetism|electromagnetic]] and [[gravity|gravitational]] [[field (physics)|fields]]. Hamilton's principle has also been extended to [[quantum mechanics]] and [[quantum field theory]]—in particular the [[path integral formulation]] of quantum mechanics makes use of the concept—where a physical system randomly follows one of the possible paths, with the phase of the probability amplitude for each path being determined by the action for the path.<ref name=\"abers1\">Quantum Mechanics, E. Abers, Pearson Ed., Addison Wesley, Prentice Hall Inc, 2004, {{ISBN|978-0-13-146100-0}}</ref>\n\n===Solution of differential equation===\nEmpirical laws are frequently expressed as [[differential equation]]s, which describe how physical quantities such as [[position vector|position]] and [[momentum]] change [[continuous function|continuously]] with [[Time in physics|time]], [[space]] or a generalization thereof. Given the [[Initial value problem|initial]] and [[Boundary value problem|boundary]] conditions for the situation, the \"solution\" to these empirical equations is one or more [[function (mathematics)|function]]s that describe the behavior of the system and are called ''[[equations of motion]]''.\n\n===Minimization of action integral===\n''Action'' is a part of an alternative approach to finding such equations of motion. Classical mechanics postulates that the path actually followed by a physical system is that for which the ''action is minimized'', or more generally, is [[Stationary point|stationary]]. In other words, the action satisfies a [[Calculus of variations|variational]] principle: the [[principle of stationary action]] (see also below). The action is defined by an [[integral (calculus)|integral]], and the classical equations of motion of a system can be derived by minimizing the value of that integral.\n\nThis simple principle provides deep insights into physics, and is an important concept in modern [[theoretical physics]].\n\n== History ==\n\n''Action'' was defined in several now obsolete ways during the development of the concept.<ref name=\"handfinch\">Analytical Mechanics, L.N. Hand, J.D. Finch, Cambridge University Press, 2008, {{ISBN|978-0-521-57572-0}}</ref>\n* [[Gottfried Leibniz]], [[Johann Bernoulli]] and [[Pierre Louis Maupertuis]] defined the action for light as the integral of its speed or inverse speed along its path length.\n* [[Leonhard Euler]] (and, possibly, Leibniz) defined action for a material particle as the integral of the particle's speed along its path through space.\n* [[Pierre Louis Maupertuis]] introduced several ''ad hoc'' and contradictory definitions of action within a single [[s:Translation:Derivation of the laws of motion and equilibrium from a metaphysical principle|article]], defining action as potential energy, as virtual kinetic energy, and as a hybrid that ensured conservation of momentum in collisions.<ref>''Œuvres de Mr de Maupertuis'' (pre-1801 Imprint Collection at the [[Library of Congress]]).</ref>\n\n== Mathematical definition ==\n\nExpressed in mathematical language, using the [[calculus of variations]], the [[time evolution|evolution]] of a physical system (i.e., how the system actually progresses from one state to another) corresponds to a [[stationary point]] (usually, a minimum) of the action.\n\nSeveral different definitions of \"the action\" are in common use in physics.<ref name=\"handfinch\" /><ref>Encyclopaedia of Physics (2nd Edition), R.G. Lerner, G.L. Trigg, VHC publishers, 1991, {{ISBN|3-527-26954-1}} (Verlagsgesellschaft), {{ISBN|0-89573-752-3}} (VHC Inc.)</ref> The action is usually an [[integral]] over time. However, when the action pertains to [[field (physics)|fields]], it may be integrated over spatial variables as well. In some cases, the action is integrated along the path followed by the physical system.\n\nThe action is typically represented as an [[integral]] over time, taken along the path of the system between the initial time and the final time of the development of the system:<ref name=\"handfinch\" />\n:<math>\\mathcal{S} = \\int_{t_1}^{t_2} L \\, dt,</math>\nwhere the integrand ''L'' is called the [[Lagrangian mechanics|Lagrangian]]. For the action integral to be well-defined, the trajectory has to be bounded in time and space.\n\nAction has the [[dimensional analysis|dimensions]] of [[energy|[energy]]]⋅[[time|[time]]], and its [[SI unit]] is [[joule]]-second, which is identical to the unit of [[angular momentum]].\n\n== Action in classical physics ==\n\nIn [[classical physics]], the term \"action\" has a number of meanings.\n\n=== Action (functional) ===\nMost commonly, the term is used for a [[functional (mathematics)|functional]] <math>\\mathcal{S}</math> which takes a [[function (mathematics)|function]] of time and (for [[field (physics)|fields]]) space as input and returns a [[scalar (physics)|scalar]].<ref name=\"penrose\">The Road to Reality, Roger Penrose, Vintage books, 2007, {{ISBN|0-679-77631-1}}</ref><ref name=\"kibble\">Classical Mechanics, T.W.B. Kibble, European Physics Series, McGraw-Hill (UK), 1973, {{ISBN|0-07-084018-0}}</ref> In [[classical mechanics]], the input function is the evolution '''q'''(''t'') of the system between two times ''t''<sub>1</sub> and ''t''<sub>2</sub>, where '''q''' represents the [[generalized coordinate]]s. The action <math>\\mathcal{S}[\\mathbf{q}(t)]</math> is defined as the [[integral]] of the [[Lagrangian mechanics|Lagrangian]] ''L'' for an input evolution between the two times:\n\n:<math>\n\\mathcal{S}[\\mathbf{q}(t)] = \\int_{t_1}^{t_2} L[\\mathbf{q}(t),\\dot{\\mathbf{q}}(t),t]\\, dt,\n</math>\n\nwhere the endpoints of the evolution are fixed and defined as <math>\\mathbf{q}_{1} = \\mathbf{q}(t_{1})</math> and <math>\\mathbf{q}_{2} = \\mathbf{q}(t_{2})</math>. According to [[Hamilton's principle]], the true evolution '''q'''<sub>true</sub>(''t'') is an evolution for which the action <math>\\mathcal{S}[\\mathbf{q}(t)]</math> is [[stationary point|stationary]] (a minimum, maximum, or a [[saddle point]]). This principle results in the equations of motion in [[Lagrangian mechanics]].\n\n===Abbreviated action (functional)=== <!-- [[Symplectic action]] redirects here -->\nUsually denoted as <math>\\mathcal{S}_{0}</math>, this is also a [[functional (mathematics)|functional]]. Here the input function is the ''path'' followed by the physical system without regard to its parameterization by time. For example, the path of a planetary orbit is an ellipse, and the path of a particle in a uniform gravitational field is a parabola; in both cases, the path does not depend on how fast the particle traverses the path. The abbreviated action <math>\\mathcal{S}_{0}</math> is defined as the integral of the generalized momenta along a path in the [[generalized coordinates]]:\n\n:<math>\n\\mathcal{S}_0 = \\int \\mathbf{p} \\cdot d\\mathbf{q} = \\int p_i \\,dq_i.\n</math>\n\nAccording to [[Maupertuis' principle]], the true path is a path for which the abbreviated action <math>\\mathcal{S}_{0}</math> is [[stationary point|stationary]].\n\n=== Hamilton's principal function ===\n{{Main article|Hamilton–Jacobi equation}}\nHamilton's principal function is defined by the [[Hamilton–Jacobi equation]]s (HJE), another alternative formulation of [[classical mechanics]].  This function ''S'' is related to the functional <math>\\mathcal{S}</math> by fixing the initial time ''t''<sub>1</sub> and the initial endpoint '''q'''<sub>1</sub> and allowing the upper limits ''t''<sub>2</sub> and the second endpoint '''q'''<sub>2</sub> to vary; these variables are the [[independent variable|arguments]] of the function ''S''.  In other words, the action function ''S'' is the [[antiderivative|indefinite integral]] of the Lagrangian with respect to time.\n\n=== Hamilton's characteristic function ===\nWhen the total energy ''E'' is conserved, the [[Hamilton–Jacobi equation]] can be solved with the [[Hamilton–Jacobi equation#Separation of variables|additive separation of variables]]:\n\n:<math>S(q_1, \\dots, q_N, t) = W(q_1, \\dots, q_N) - E \\cdot t,</math>\n\nwhere the time-independent function ''W''(''q''<sub>1</sub>, ''q''<sub>2</sub>, … ''q<sub>N</sub>'') is called ''Hamilton's characteristic function''. The physical significance of this function is understood by taking its total time derivative\n\n:<math>\\frac{d W}{d t} = \\frac{\\partial W}{\\partial q_i} \\dot q_i = p_i \\dot q_i.</math>\n\nThis can be integrated to give\n\n:<math>W(q_1, \\dots, q_N) = \\int p_i\\dot q_i \\,dt = \\int p_i \\,dq_i,</math>\n\nwhich is just the [[#Abbreviated action (functional)|abbreviated action]].\n\n=== Other solutions of Hamilton–Jacobi equations ===\nThe [[Hamilton–Jacobi equation]]s are often solved by additive separability; in some cases, the individual terms of the solution, e.g., ''S<sub>k</sub>''(''q<sub>k</sub>''), are also called an \"action\".<ref name=\"handfinch\" />\n\n=== Action of a generalized coordinate ===\nThis is a single variable ''J<sub>k</sub>'' in the [[action-angle coordinates]], defined by integrating a single generalized momentum around a closed path in [[phase space]], corresponding to rotating or oscillating motion:\n\n:<math>\nJ_k = \\oint p_k \\,dq_k\n</math>\n\nThe variable ''J<sub>k</sub>'' is called the \"action\" of the generalized coordinate ''q<sub>k</sub>''; the corresponding canonical variable conjugate to ''J<sub>k</sub>'' is its \"angle\" ''w<sub>k</sub>'', for reasons described more fully under [[action-angle coordinates]].  The integration is only over a single variable ''q<sub>k</sub>'' and, therefore, unlike the integrated dot product in the abbreviated action integral above.  The ''J<sub>k</sub>'' variable equals the change in ''S<sub>k</sub>''(''q<sub>k</sub>'') as ''q<sub>k</sub>'' is varied around the closed path.  For several physical systems of interest, J<sub>k</sub> is either a constant or varies very slowly; hence, the variable ''J<sub>k</sub>'' is often used in perturbation calculations and in determining [[adiabatic invariant]]s.\n\n=== Action for a Hamiltonian flow ===\nSee [[tautological one-form]].\n\n== Euler–Lagrange equations for the action integral ==\nAs noted above, the requirement that the action integral be [[stationary point|stationary]] under small perturbations of the evolution is equivalent to a set of [[differential equation]]s (called the [[Euler–Lagrange equations]]) that may be determined using the [[calculus of variations]]. We illustrate this derivation here using only one coordinate, ''x''; the extension to multiple coordinates is straightforward.<ref name=\"mcgraw1\" /><ref name=\"kibble\" />\n\nAdopting [[Hamilton's principle]], we assume that the Lagrangian ''L'' (the integrand of the action integral) depends only on the coordinate ''x''(''t'') and its time derivative ''dx''(''t'')/''dt'', and may also depend explicitly on time.  In that case, the action integral can be written as\n\n:<math>\n\\mathcal{S} = \\int_{t_1}^{t_2} L(x, \\dot{x}, t) \\,dt,\n</math>\n\nwhere the initial and final times (''t''<sub>1</sub> and ''t''<sub>2</sub>) and the final and initial positions are specified in advance as <math>x_1 = x(t_1)</math> and <math>x_2 = x(t_2)</math>.  Let ''x''<sub>true</sub>(''t'') represent the true evolution that we seek, and let <math>x_\\text{per}(t)</math> be a slightly perturbed version of it, albeit with the same endpoints, <math>x_\\text{per}(t_1) = x_1</math> and <math>x_\\text{per}(t_2) = x_2</math>.  The difference between these two evolutions, which we will call <math>\\varepsilon(t)</math>, is infinitesimally small at all times:\n\n:<math>\n\\varepsilon(t) = x_\\text{per}(t) - x_\\text{true}(t).\n</math>\n\nAt the endpoints, the difference vanishes, i.e., <math>\\varepsilon(t_1) = \\varepsilon(t_2) = 0</math>.\n\nExpanded to first order, the difference between the actions integrals for the two evolutions is\n\n:<math>\\begin{align}\n\\delta \\mathcal{S} &= \\int_{t_1}^{t_2}\n \\left[ L(x_\\text{true} + \\varepsilon, \\dot x_\\text{true} + \\dot\\varepsilon, t) -\n L(x_\\text{true}, \\dot x_\\text{true}, t) \\right] \\,dt \\\\\n&= \\int_{t_1}^{t_2}\n \\left(\\varepsilon\\frac{\\partial L}{\\partial x} +\n \\dot\\varepsilon\\frac{\\partial L}{\\partial \\dot x} \\right)\\,dt.\n\\end{align}</math>\n\n[[Integration by parts]] of the last term, together with the boundary conditions <math>\\varepsilon(t_1) = \\varepsilon(t_2) = 0</math>, yields the equation\n\n:<math>\n\\delta \\mathcal{S} = \\int_{t_1}^{t_2}\n \\left(\n  \\varepsilon\\frac{\\partial L}{\\partial x} -\n  \\varepsilon\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot x}\n \\right)\\,dt.\n</math>\n\nThe requirement that <math>\\mathcal{S}</math> be [[stationary point|stationary]] implies that the first-order change must be zero for ''any'' possible perturbation ε(''t'') about the true evolution:\n\n{{Equation box 1\n|indent =:\n|title= '''[[Principle of stationary action]]'''\n|equation = <math>\\delta\\mathcal{S}=0</math>\n|cellpadding\n|border=1\n|border colour = black\n|background colour = white}}\n\nThis can be true only if\n\n{{Equation box 1\n|indent =:\n|title= '''[[Euler–Lagrange equation]]'''\n|equation = <math> {\\partial L\\over\\partial x} - {d\\over dt }{\\partial L\\over\\partial \\dot{x}} = 0 </math>\n|cellpadding\n|border=1\n|border colour = black\n|background colour = white}}\n\nThe Euler–Lagrange equation is obeyed provided the [[functional derivative]] of the action integral is identically zero:\n\n:<math>\\frac{\\delta \\mathcal{S}}{\\delta x(t)} = 0.</math>\n\nThe quantity <math>\\frac{\\partial L}{\\partial\\dot x}</math> is called the ''conjugate momentum'' for the coordinate ''x''.  An important consequence of the Euler–Lagrange equations is that if ''L'' does not explicitly contain coordinate ''x'', i.e.\n\n:if <math>\\frac{\\partial L}{\\partial x} = 0</math>, then <math>\\frac{\\partial L}{\\partial\\dot x}</math> is constant in time.\n\nIn such cases, the coordinate ''x'' is called a ''cyclic'' coordinate, and its conjugate momentum is conserved.\n\n=== Example: free particle in polar coordinates ===\n\nSimple examples help to appreciate the use of the action principle via the Euler–Lagrangian equations. A free particle (mass ''m'' and velocity ''v'') in Euclidean space moves in a straight line. Using the Euler–Lagrange equations, this can be shown in [[polar coordinates]] as follows. In the absence of a potential, the Lagrangian is simply equal to the kinetic energy\n:<math>L = \\frac{1}{2} mv^2 = \\frac{1}{2}m \\left( \\dot{x}^2 + \\dot{y}^2 \\right)</math>\nin orthonormal (''x'', ''y'') coordinates, where the dot represents differentiation with respect to the curve parameter (usually the time, ''t'').\nIn polar coordinates (''r'', φ) the kinetic energy and hence the Lagrangian becomes\n\n:<math>\nL = \\frac{1}{2}m \\left( \\dot{r}^2 + r^2 \\dot\\varphi^2 \\right).\n</math>\n\nThe radial ''r'' and angular φ components of the Euler–Lagrangian equations become respectively\n\n:<math>\\begin{align}\n\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{r}} \\right) - \\frac{\\partial L}{\\partial r} &= 0\n & &\\Rightarrow &\n \\ddot{r} - r\\dot{\\varphi}^2 &= 0, \\\\\n\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{\\varphi}} \\right) - \\frac{\\partial L}{\\partial \\varphi} &= 0\n & &\\Rightarrow &\n\\ddot{\\varphi} + \\frac{2}{r}\\dot{r}\\dot{\\varphi} &= 0.\n\\end{align}</math>\n\nThe solution of these two equations is given by\n\n:<math>\\begin{align}\nr\\cos\\varphi &= a t + b, \\\\\nr\\sin\\varphi &= c t + d\n\\end{align}</math>\n\nfor a set of constants ''a'', ''b'', ''c'', ''d'' determined by initial conditions.\nThus, indeed, ''the solution is a straight line'' given in polar coordinates.\n\n== The action principle ==\n\n{{Main article|principle of stationary action}}\n\n=== Classical fields ===\n\n{{See also|Einstein–Hilbert action}}\n\nThe '''action principle''' can be extended to obtain the [[equations of motion]] for fields, such as the [[electromagnetic field]] or [[gravitational field]].\n\nThe [[Einstein equation]] utilizes the ''[[Einstein–Hilbert action]]'' as constrained by a [[variational principle]].\n\nThe [[trajectory]] (path in [[spacetime]]) of a body in a gravitational field can be found using the action principle. For a free falling body, this trajectory is a [[geodesic]].\n\n=== Conservation laws ===\n\n{{Main article|Conservation laws}}\n\nImplications of symmetries in a physical situation can be found with the action principle, together with the [[Euler–Lagrange equations]], which are derived from the action principle. An example is [[Noether's theorem]], which states that to every [[continuous symmetry]] in a physical situation there corresponds a [[conservation law (physics)|conservation law]] (and conversely). This deep connection requires that the action principle be assumed.<ref name=\"abers1\" />\n\n=== Quantum mechanics and quantum field theory ===\n\n{{Main article|Quantum mechanics|quantum field theory}}\n\nIn quantum mechanics, the system does not follow a single path whose action is stationary, but the behavior of the system depends on all permitted paths and the value of their action. The action corresponding to the various paths is used to calculate the [[Path integral formulation|path integral]], that gives the [[probability amplitude]]s of the various outcomes.\n\nAlthough equivalent in classical mechanics with [[Newton's laws]], the '''action principle''' is better suited for generalizations and plays an important role in modern physics. Indeed, this principle is one of the great generalizations in physical science. It is best understood within quantum mechanics, particularly in [[Richard Feynman]]'s [[path integral formulation]], where it arises out of [[destructive interference]] of quantum amplitudes.\n\n[[Maxwell's equations]] can also [[Electromagnetic tensor#Lagrangian formulation of classical electromagnetism|be derived as conditions of stationary action]].\n\n=== Single relativistic particle ===\n\n{{Main article|Theory of relativity}}\n\nWhen relativistic effects are significant, the action of a point particle of mass ''m'' travelling a [[world line]] ''C'' parametrized by the [[proper time]] <math>\\tau</math> is\n:<math>S = - m c^2 \\int_{C} \\, d \\tau. </math>\n\nIf instead, the particle is parametrized by the coordinate time ''t'' of the particle and the coordinate time ranges from ''t''<sub>1</sub> to ''t''<sub>2</sub>, then the action becomes\n:<math>\\int_{t1}^{t2} L \\, dt,</math>\n\nwhere the [[Lagrangian mechanics|Lagrangian]] is<ref>L. D. Landau and E. M. Lifshitz ''The Classical Theory of Fields'' Addison-Wesley 1971 Sec.&nbsp;8. p.&nbsp;24–25.</ref>\n:<math>L = -mc^2 \\sqrt{1 - \\frac{v^2}{c^2}}.</math>\n\n=== Modern extensions ===\n\nThe action principle can be generalized still further.  For example, the action need not be an integral, because [[Nonlocal Lagrangian|nonlocal actions]] are possible.  The configuration space need not even be a [[functional space]], given certain features such as [[noncommutative geometry]].  However, a physical basis for these mathematical extensions remains to be established experimentally.<ref name=\"penrose\" />\n\n== See also ==\n{{Div col}}\n* [[Calculus of variations]]\n* [[Functional derivative]]\n* [[Functional integral]]\n* [[Hamiltonian mechanics]]\n* [[Lagrangian (field theory)|Lagrangian]]\n* [[Lagrangian mechanics]]\n* [[Measure (physics)]]\n* [[Noether's theorem]]\n* [[Path integral formulation]]\n* [[Principle of least action]]\n* [[Principle of maximum entropy]]\n* Some actions:\n** [[Nambu–Goto action]]\n** [[Polyakov action]]\n** [[Bagger–Lambert–Gustavsson action]]\n** [[Einstein–Hilbert action]]\n{{Div col end}}\n\n== References ==\n\n{{reflist}}\n\n== Sources and further reading ==\n\nFor an annotated bibliography, see Edwin F. Taylor who [http://www.eftaylor.com/pub/BibliogLeastAction12.pdf lists], among other things, the following books\n* ''The Cambridge Handbook of Physics Formulas'', G. Woan, Cambridge University Press, 2010, {{ISBN|978-0-521-57507-2}}.\n* [[Cornelius Lanczos]], [https://books.google.com/books?id=cmPDAgAAQBAJ The Variational Principles of Mechanics] (Dover Publications, New York, 1986). {{ISBN|0-486-65067-7}}. ''The'' reference most quoted by all those who explore this field.\n* [[L. D. Landau]] and [[E. M. Lifshitz]], Mechanics, [[Course of Theoretical Physics]] (Butterworth-Heinenann, 1976), 3rd ed., Vol. 1. {{ISBN|0-7506-2896-0}}. Begins with the principle of least action.\n* Thomas A. Moore \"Least-Action Principle\" in Macmillan Encyclopedia of Physics (Simon & Schuster Macmillan, 1996), Volume 2, {{ISBN|0-02-897359-3}},  {{OCLC|35269891}}, pages 840 – 842.\n* [[Gerald Jay Sussman]] and [[Jack Wisdom]], [http://groups.csail.mit.edu/mac/users/gjs/6946/sicm-html/ Structure and Interpretation of Classical Mechanics] (MIT Press, 2001). Begins with the principle of least action, uses modern mathematical notation, and checks the clarity and consistency of procedures by programming them in computer language.\n* Dare A. Wells, Lagrangian Dynamics, Schaum's Outline Series (McGraw-Hill, 1967) {{ISBN|0-07-069258-0}}, A 350-page comprehensive \"outline\" of the subject.\n* Robert Weinstock, Calculus of Variations, with Applications to Physics and Engineering (Dover Publications, 1974). {{ISBN|0-486-63069-2}}.  An oldie but goodie, with the formalism carefully defined before use in physics and engineering.\n* [[Wolfgang Yourgrau]] and [[Stanley Mandelstam]], [https://books.google.com/books/about/Variational_Principles_in_Dynamics_and_Q.html?id=OwTyrJJXZbYC Variational Principles in Dynamics and Quantum Theory] (Dover Publications, 1979). A nice treatment that does not avoid the philosophical implications of the theory and lauds the Feynman treatment of quantum mechanics that reduces to the principle of least action in the limit of large mass.\n* Edwin F. Taylor's [http://www.eftaylor.com/leastaction.html page]\n\n== External links ==\n* [http://www.eftaylor.com/software/ActionApplets/LeastAction.html Principle of least action interactive] Interactive explanation/webpage\n\n{{Classical mechanics derived SI units}}\n\n[[Category:Lagrangian mechanics]]\n[[Category:Hamiltonian mechanics]]\n[[Category:Calculus of variations]]\n[[Category:Dynamics (mechanics)]]"
    },
    {
      "title": "Almgren–Pitts min-max theory",
      "url": "https://en.wikipedia.org/wiki/Almgren%E2%80%93Pitts_min-max_theory",
      "text": "In [[mathematics]], the '''Almgren–Pitts min-max theory''' (named after [[Frederick J. Almgren, Jr.]] and his student [[Jon T. Pitts]]) is an analogue of [[Morse theory]] for [[hypersurface]]s.\n\nThe theory started with the efforts for generalizing [[George David Birkhoff]]'s method for the construction of simple closed [[geodesic]]s on the sphere, to allow the construction of [[embedding|embedded]] [[minimal surface]]s in arbitrary [[3-manifold]]s.<ref>[[Tobias Colding]] and [[Camillo De Lellis]]: \"[https://arxiv.org/pdf/math/0303305v2.pdf The min-max construction of minimal surfaces]\", Surveys in Differential Geometry</ref>\n\nIt has played roles in the solutions to a number of [[conjecture]]s in [[geometry]] and [[topology]] found by Almgren and Pitts themselves and also by other mathematicians, such as [[Mikhail Leonidovich Gromov|Mikhail Gromov]], [[Richard Schoen]], [[Shing-Tung Yau]], [[Fernando Codá Marques]], [[André Neves]], [[Ian Agol]], among others.<ref>{{cite web |url=http://numdam.mathdoc.fr/numdam-bin/item?id=ASNSP_2006_5_5_4_483_0 |author1=Giaquinta, Mariano |author2=Mucci, Domenico |title=The BV-energy of maps into a manifold : relaxation and density results |publisher=Annali della Scuola Normale Superiore di Pisa – Classe di Scienze, Sér. 5, 5 |number=4 |date=2006 |pages=483–548 |access-date=2015-05-02 |archive-url=https://web.archive.org/web/20150610171642/http://numdam.mathdoc.fr/numdam-bin/item?id=ASNSP_2006_5_5_4_483_0 |archive-date=2015-06-10 |dead-url=yes |df= }}</ref><ref>Helge Holden, Ragni Piene – The Abel Prize 2008-2012, p. 203.</ref><ref>[[Robert Osserman]] – A Survey of Minimal Surfaces, p. 160.</ref><ref>{{cite web |url=http://intlpress.com/site/pub/pages/journals/items/cdm/content/vols/2013/0001/a001/index.html |title=Content Online - CDM 2013 Article 1 |publisher=Intlpress.com |accessdate=2015-05-31}}</ref><ref>{{cite web |url=http://wwwf.imperial.ac.uk/~aneves/papers/CDM-harvard3.pdf |format=PDF |title=Applications of Almgren-Pitts Min-max theory |author1=Fernando C. Marques |author2=André Neves |publisher=F.imperial.ac.uk |accessdate=2015-05-31}}</ref><ref>{{Cite arXiv |title=Degeneration of Min-Max Sequences in Three-Manifolds |author1=Daniel Ketover |arxiv=1312.2666}}<!--|accessdate=2015-05-31 --></ref><ref>{{cite web |url=http://xxx.tau.ac.il/pdf/1504.00966.pdf |format=PDF |title=Min-max hypersurface in manifold of positive Ricci curvature |author=Xin Zhou |publisher=Arvix.org |accessdate=2015-05-31}}</ref><ref>{{cite web|url=http://perso-math.univ-mlv.fr/users/sabourau.stephane/articles/hypersurface.pdf |format=PDF |title=Volume of minimal hypersurfaces in manifolds with nonnegative Ricci curvature |author=Stephane Sabourau |publisher=Arvix.org |accessdate=2015-05-31}}</ref><ref>{{Cite arXiv|title=Free boundary minimal annuli in convex three-manifolds |author1=Davi Maximo |author2=Ivaldo Nunes |author3=Graham Smith |arxiv=1312.5392 }}<!--|accessdate=2015-05-31 --></ref>\n\n==Description and basic concepts==\n{{expand section|date=May 2015}}\nThe theory allows the construction of [[Embedding|embedded]] minimal hypersurfaces though variational methods.<ref>{{cite journal | author = Zhou Xin | year = 2015 | title = Min-max minimal hypersurface  in <math>(M^{n+1}, g)</math> with <math>Ric \\geq 0</math> and <math>2 \\leq n \\leq 6</math> | url = http://projecteuclid.org/euclid.jdg/1427202766 | journal = J. Differential Geom. | volume = 100 | issue = 1 | pages = 129–160 | doi=10.4310/jdg/1427202766}}</ref>\n\n==See also==\n*[[Varifold]]\n*[[Geometric measure theory]]\n*[[Geometric analysis]]\n*[[Minimal surface]]\n*[[Freedman–He–Wang conjecture]]\n*[[Willmore conjecture]]\n*[[Yau's conjecture]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*{{cite book |author=Frederick J. Almgren |title=The Theory of Varifolds: A Variational Calculus in the Large for the K-dimensional Area Integrand |date=1964 |publisher=[[Institute for Advanced Study]]}}\n*{{cite book |author=Jon T. Pitts|title=Existence and Regularity of Minimal Surfaces on Riemannian Manifolds |date=1981 |publisher=[[Princeton University Press]] |isbn=978-0-691-08290-5}}\n*{{Cite arXiv |arxiv=1312.0792 |last1=Memarian |first1=Yashar |title=A Note on the Geometry of Positively-Curved Riemannian Manifolds |class=math.MG |date=2013 <!-- Almgren-Pitts Min-Max theory pp. 11–15 -->}}\n*[http://www.crm.umontreal.ca/rapports/bulletin/bulletin21-2.pdf Le Centre de recherches mathématiques, CRM Le Bulletin, Automne/Fall 2015 — Volume 21, No 2, pp. 10–11 Iosif Polterovich (Montréal) and Alina Stancu (Concordia), \"The 2015 Nirenberg Lectures in Geometric Analysis: Min-Max Theory and Geometry, by André Neves\"]\n\n{{DEFAULTSORT:Almgren-Pitts min-max theory}}\n[[Category:Topology]]\n[[Category:Geometry]]\n[[Category:Minimal surfaces]]\n[[Category:Calculus of variations]]\n[[Category:Measure theory]]\n\n\n{{mathematics-stub}}"
    },
    {
      "title": "Applications of the calculus of variations",
      "url": "https://en.wikipedia.org/wiki/Applications_of_the_calculus_of_variations",
      "text": "'''Applications of the [[calculus of variations]]''' include:\n\n*[[Variational method (quantum mechanics)]], one way of finding approximations to the lowest energy eigenstate or ground state, and some excited states;\n*[[Variational Bayesian methods]], a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning.\n*[[Variational methods in general relativity]], a family of techniques using calculus of variations to solve problems in Einstein's theory of general relativity. \n*[[Finite element method]] is a variational method for finding approximate solutions to boundary value problems in differential equations.\n\n[[Category:Calculus of variations]]\n\n\n{{Mathanalysis-stub}}"
    },
    {
      "title": "Beltrami identity",
      "url": "https://en.wikipedia.org/wiki/Beltrami_identity",
      "text": "The '''Beltrami identity''', named after [[Eugenio Beltrami]], is a simplified and less general version of the [[Euler–Lagrange equation]] in the [[calculus of variations]].\n\nThe Euler–Lagrange equation serves to extremize action  [[Functional (mathematics)|functional]]s of the form<ref name='CourHilb1953'>{{cite book | vauthors = Courant R, Hilbert D | author-link = Richard Courant | author2-link = David Hilbert | title = Methods of Mathematical Physics | volume = Vol. I | edition = First English | publisher = Interscience Publishers, Inc. | year = 1953 | location = New York | page = 184 | isbn = 978-0471504474}}<!--| accessdate = 2012-10-22 --></ref> \n:<math>I[u]=\\int_a^b L[x,u(x),u'(x)] \\, dx \\, ,</math>\nwhere {{math|''a'', ''b''}} are [[Constant (mathematics)|constants]] and {{math|''u''&prime;(x) {{=}} ''du'' / ''dx''}}.\n\nFor the special case of {{math|∂''L'' / ∂''x'' {{=}} 0}}, the Euler–Lagrange equation reduces to the Beltrami identity,<ref>Weisstein, Eric W. [http://mathworld.wolfram.com/Euler-LagrangeDifferentialEquation.html \"Euler-Lagrange Differential Equation.\"]  From [http://mathworld.wolfram.com/ MathWorld]--A Wolfram Web Resource. See Eq. (5).</ref>\n\n{{Equation box 1\n|indent =:\n|equation =  <math>L-u'\\frac{\\partial L}{\\partial u'}=C \\, ,</math> \n|cellpadding= 6\n|border\n|border colour = #0073CF\n|bgcolor=#F9FFF7}}\nwhere {{math|''C''}} is a constant.<ref>Thus, the [[Legendre transformation|Legendre transform]] of the [[Lagrangian mechanics|Lagrangian]], the [[Hamiltonian mechanics|Hamiltonian]], is constant on the dynamical path.</ref>\n\n==Derivation==\nThe following derivation of the Beltrami identity<ref>This derivation of the Beltrami identity corresponds to the one at — Weisstein, Eric W. [http://mathworld.wolfram.com/BeltramiIdentity.html \"Beltrami Identity.\"] From [http://mathworld.wolfram.com/ MathWorld]--A Wolfram Web Resource.</ref> starts with the Euler–Lagrange equation, \n:<math> \\frac{\\partial L}{\\partial u} =\\frac{d}{dx} \\frac{\\partial L}{\\partial u'} \\, . </math>\nMultiplying both sides by {{math|''u''&prime;}},\n:<math> u'\\frac{\\partial L}{\\partial u} =u'\\frac{d}{dx} \\frac{\\partial L}{\\partial u'} \\, . </math>\n\nAccording to the [[chain rule]],\n:<math> {dL \\over dx} = {\\partial L \\over \\partial u}u' +  {\\partial L \\over \\partial u'}u'' + {\\partial L \\over \\partial x}  \\, ,</math>\nwhere {{math|''u''&prime;&prime; {{=}} ''du''&prime;/''dx'' {{=}} ''d''<sup>2</sup>''u'' / ''dx''<sup>2</sup>}}.\n\nRearranging this yields\n:<math> u' {\\partial L \\over \\partial u} = {dL \\over dx}  - {\\partial L \\over \\partial u'}u'' - {\\partial L \\over \\partial x} \\, . </math>\nThus, substituting this expression for {{math|''u''&prime; ∂''L''/∂''u''}}  into the second equation of this derivation,  \n:<math> {dL \\over dx}  - {\\partial L \\over \\partial u'}u'' - {\\partial L \\over \\partial x} -u'\\frac{d}{dx} \\frac{\\partial L}{\\partial u'} = 0 \\, . </math>\nBy the product rule, the last term is re-expressed as\n:<math>u'\\frac{d}{dx}\\frac{\\partial L}{\\partial u'}=\\frac{d}{dx}\\left( \\frac{\\partial L}{\\partial u'}u' \\right)-\\frac{\\partial L}{\\partial u'}u'' \\, , </math>\nand rearranging,\n:<math> {d \\over dx} \\left( { L - u'\\frac{\\partial L}{\\partial u'} } \\right) = {\\partial L \\over \\partial x}  \\, . </math>\n\nFor the case of {{math|∂''L'' / ∂''x'' {{=}} 0}}, this reduces to \n:<math> {d \\over dx} \\left( { L - u'\\frac{\\partial L}{\\partial u'} } \\right) = 0 \\, , </math>\nso that taking the [[antiderivative]] results in the Beltrami identity,\n:<math>  L - u'\\frac{\\partial L}{\\partial u'}   = C \\, , </math>\nwhere {{math|''C''}} is a constant.\n\n== Application ==\n\nAn example of an application of the Beltrami identity is the [[Brachistochrone problem]], which involves finding the curve {{math|''y'' {{=}} ''y''(''x'')}} that minimizes the integral\n:<math> I[y] = \\int_0^a \\sqrt { {1+y'^{\\, 2}} \\over y } dx \\, . </math>\nThe integrand \n:<math> L(y,y') = \\sqrt{ {1+y'^{\\, 2}} \\over y } </math>\ndoes not depend explicitly on the variable of integration {{math|''x''}}, so the Beltrami identity applies,\n:<math>L-y'\\frac{\\partial L}{\\partial y'}=C \\, .</math> \nSubstituting for {{math|''L''}} and simplifying,\n:<math> y(1+y'^{\\, 2}) = 1/C^2  ~~\\text {(constant)} \\, , </math>\nwhich can be solved with the result put in the form of [[parametric equation]]s \n:<math>x = A(\\phi - \\sin \\phi) </math>\n:<math>y = A(1 - \\cos \\phi) </math>\nwith {{math|''A''}} being half the above constant, 1/(2''C'' ²), and {{math|''&phi;''}} being a variable. These are the parametric equations for a [[cycloid]].<ref name='MW307'>This solution of the Brachistochrone problem corresponds to the one in — {{cite book | last1 = Mathews | first1 = Jon | last2 = Walker | first2 = RL | title = Mathematical Methods of Physics | publisher = W. A. Benjamin, Inc. | year = 1965 | location = New York | pages = 307–9 }}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Bounded variation",
      "url": "https://en.wikipedia.org/wiki/Bounded_variation",
      "text": "{{Use dmy dates|date=June 2013}}\nIn [[mathematical analysis]], a function of '''bounded variation''', also known as '''''{{math|BV}}'' function''', is a [[real number|real]]-valued [[function (mathematics)|function]] whose [[total variation]] is bounded (finite): the [[graph of a function]] having this property is well behaved in a precise sense. For a [[continuous function]] of a single [[Variable (mathematics)|variable]], being of bounded variation means that the [[distance]] along the [[Direction (geometry, geography)|direction]] of the [[y-axis|{{math|''y''}}-axis]], neglecting the contribution of motion along [[x-axis|{{math|''x''}}-axis]], traveled by a [[point (mathematics)|point]] moving along the graph has a finite value. For a continuous function of several variables, the meaning of the definition is the same, except for the fact that the continuous path to be considered cannot be the whole graph of the given function (which is a [[Glossary of differential geometry and topology#H|hypersurface]] in this case), but can be every [[Intersection (set theory)|intersection]] of the graph itself with a [[hyperplane]] (in the case of functions of two variables, a [[Plane (mathematics)|plane]]) parallel to a fixed {{math|''x''}}-axis and to the {{math|''y''}}-axis.\n\nFunctions of bounded variation are precisely those with respect to which one may find [[Riemann–Stieltjes integral]]s of all continuous functions.\n\nAnother characterization states that the functions of bounded variation on a compact interval are exactly those {{math|''f''}} which can be written as a difference {{math|''g''&nbsp;−&nbsp;''h''}}, where both {{math|''g''}} and {{math|''h''}} are bounded [[monotonic function|monotone]].\n\nIn the case of several variables, a function {{math|''f''}} defined on an [[open subset]] {{math|Ω}} of ℝ''<sup>n</sup>'' is said to have bounded variation if its [[distribution (mathematics)|distributional derivative]] is a [[Vector-valued function|vector-valued]] finite [[Radon measure]].\n\nOne of the most important aspects of functions of bounded variation is that they form an [[Associative algebra|algebra]] of [[continuous function|discontinuous functions]] whose first derivative exists [[almost everywhere]]: due to this fact, they can and frequently are used to define [[generalized solution]]s of nonlinear problems involving [[functional (mathematics)|functional]]s, [[ordinary differential equation|ordinary]] and [[partial differential equation]]s in [[mathematics]], [[physics]] and [[engineering]].\n\nWe have the following chains of inclusions for functions over a closed, bounded interval of the real line:\n\n: '''[[Continuously differentiable]]''' ⊆ '''[[Lipschitz continuous]]''' ⊆ '''[[absolutely continuous]]''' ⊆ '''bounded variation'''  ⊆ '''[[Differentiable function|differentiable]] [[almost everywhere]]'''\n\n==History==\nAccording to Boris Golubov, ''BV'' functions of a single variable were first introduced by [[Camille Jordan]], in the paper {{Harv|Jordan|1881}} dealing with the convergence of [[Fourier series]]. The first successful step in the generalization of this concept to functions of several variables was due to [[Leonida Tonelli]],<ref>[[Leonida Tonelli|Tonelli]] introduced what is now called after him '''Tonelli plane variation''': for an analysis of this concept and its relations to other generalizations, see the entry \"[[Total variation]]\".</ref> who introduced a class of ''[[continuous function|continuous]]'' ''BV'' functions in 1926 {{Harv|Cesari|1986|pp=47–48}}, to extend his [[Direct method in the calculus of variations|direct method]] for finding solutions to problems in the [[calculus of variations]] in more than one variable. Ten years after, in {{Harv|Cesari|1936}}, [[Lamberto Cesari]] ''changed the [[continuous function|continuity]] requirement'' in Tonelli's definition ''to a less restrictive [[integral|integrability]] requirement'', obtaining for the first time the class of functions of bounded variation of several variables in its full generality: as Jordan did before him, he applied the concept to resolve of a problem concerning the convergence of [[Fourier series]], but for functions of ''two variables''. After him, several authors applied ''BV'' functions to study [[Fourier series]] in several variables, [[geometric measure theory]], [[calculus of variations]], and [[mathematical physics]]. [[Renato Caccioppoli]] and [[Ennio de Giorgi]] used them to define [[measure theory|measure]] of [[smooth function|nonsmooth]] [[boundary (topology)|boundaries]] of [[Set (mathematics)|sets]] (see the entry \"''[[Caccioppoli set]]''\" for further information). [[Olga Arsenievna Oleinik]] introduced her view of [[generalized solution]]s for [[nonlinear]] [[partial differential equation]]s as functions from the space ''BV'' in the paper {{Harv|Oleinik|1957}}, and was able to construct a generalized solution of bounded variation of a first order partial differential equation in the paper {{Harv|Oleinik|1959}}: few years later, [[Edward D. Conway]] and [[Joel A. Smoller]] applied ''BV''-functions to the study of a single [[hyperbolic equation|nonlinear hyperbolic partial differential equation]] of [[First-order partial differential equation|first order]] in the paper {{Harv|Conway|Smoller|1966}}, proving that the solution of the [[Cauchy problem]] for such equations is a function of bounded variation, provided the [[Cauchy boundary condition|initial value]] belongs to the same class. [[Aizik Isaakovich Vol'pert]] developed extensively a calculus for ''BV'' functions: in the paper {{Harv|Vol'pert|1967}} he proved the [[Bounded variation#Chain rule for BV functions|chain rule for BV functions]] and in the book {{Harv|Hudjaev|Vol'pert|1985}} he, jointly with his pupil [[Sergei Ivanovich Hudjaev]], explored extensively the properties of ''BV'' functions and their application. His chain rule formula was later extended by [[Luigi Ambrosio]] and [[Gianni Dal Maso]] in the paper {{Harv|Ambrosio|Dal Maso|1990}}.\n\n==Formal definition==\n\n=== ''BV'' functions of one variable ===\n{{EquationRef|1|Definition 1.1.}} The '''total variation'''<ref name=\"Tvar\">See the entry \"[[Total variation]]\" for further details and more information.</ref> of a continue [[real number|real]]-valued (or more generally [[complex number|complex]]-valued) [[function (mathematics)|function]] ''f'', defined on an [[interval (mathematics)|interval]] [''a'', ''b'']⊂ℝ is the quantity\n\n:<math> V_a^b(f)=\\sup_{P \\in \\mathcal{P}} \\sum_{i=0}^{n_{P}-1} | f(x_{i+1})-f(x_i) |. \\,</math>\n\nwhere the [[supremum]] is taken over the set <math> \\scriptstyle \\mathcal{P} =\\left\\{P=\\{ x_0, \\dots , x_{n_P}\\}|P\\text{ is a partition of } [a, b]\\text{ satisfying } x_i\\leq x_{i+1}\\text{ for } 0\\leq i\\leq n_P-1 \\right\\} </math> of all [[partition of an interval|partitions]] of the interval considered.\n\nIf ''f'' is [[derivative|differentiable]] and its derivative is Riemann-integrable, its total variation is the vertical component of the [[arc length|arc-length]] of its graph, that is to say,\n\n:<math> V_a^b(f) = \\int _a^b |f'(x)|\\,\\mathrm{d}x.</math>\n\n{{EquationRef|2|Definition 1.2.}} A continue real-valued function <math> f </math> on the [[real line]] is said to be of '''bounded variation''' ('''BV function''') on a chosen [[interval (mathematics)|interval]] [''a'', ''b'']⊂ℝ if its total variation is finite, ''i.e.''\n:<math> f \\in BV([a,b]) \\iff V_a^b(f) < +\\infty </math>\n\nIt can be proved that a real function ''ƒ'' is of bounded variation in <math>[a,b]</math> if and only if it can be written as the difference ''ƒ'' = ''ƒ''<sub>1</sub> − ''ƒ''<sub>2</sub> of two non-decreasing functions on <math>[a,b]</math>: this result is known as the [http://www.encyclopediaofmath.org/index.php/Jordan_decomposition_%28of_a_function%29 Jordan decomposition of a function] and it is related to the [[Hahn decomposition theorem#Jordan measure decomposition|Jordan decomposition of a measure]].\n\nThrough the [[Stieltjes integral]], any function of bounded variation on a closed interval [''a'', ''b''] defines a [[bounded linear functional]] on ''C''([''a'', ''b'']). In this special case,<ref>See for example {{harvtxt|Kolmogorov|Fomin|1969|pp=374–376}}.</ref> the [[Riesz representation theorem]] states that every bounded linear functional arises uniquely in this way. The normalised positive functionals or [[probability measure]]s correspond to positive non-decreasing lower [[semicontinuous function]]s. This point of view has been important in\n[[spectral theory]],<ref>For a general reference on this topic, see {{harvtxt|Riesz|Szőkefalvi-Nagy|1990}}</ref> in particular in its application to [[spectral theory of ordinary differential equations|ordinary differential equations]].\n\n===''BV'' functions of several variables===\nFunctions of bounded variation, BV [[function (mathematics)|functions]], are functions whose distributional [[directional derivative|derivative]] is a [[Wikt:finite|finite]]<ref>In this context, \"finite\" means that its value is never [[Infinity|infinite]], i.e. it is a [[finite measure]].</ref> [[Radon measure]]. More precisely:\n\n{{EquationRef|3|Definition 2.1.}} Let '''<math> \\Omega </math>''' be an [[open subset]] of ℝ''<sup>n</sup>''. A  function '''<math> u </math>''' belonging to '''[[Lp space|<math>L^1(\\Omega)</math>]]''' is said of '''bounded variation''' ('''BV function'''), and written\n\n:<math> u\\in BV(\\Omega)</math>\n\nif there exists a [[Finite measure|finite]] [[vector-valued function|vector]] [[Radon measure]] <math> \\scriptstyle Du\\in\\mathcal M(\\Omega,\\mathbb{R}^n)</math> such that the following equality holds\n\n:<math>\n\\int_\\Omega u(x)\\,\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x = - \\int_\\Omega \\langle\\boldsymbol{\\phi}, Du(x)\\rangle\n\\qquad \\forall\\boldsymbol{\\phi}\\in C_c^1(\\Omega,\\mathbb{R}^n)\n</math>\n\nthat is, '''<math>u</math>''' defines a [[linear functional]] on the space <math> \\scriptstyle C_c^1(\\Omega,\\mathbb{R}^n)</math> of [[Smooth function|continuously differentiable]] [[Vector-valued function|vector functions]] <math> \\scriptstyle\\boldsymbol{\\phi} </math> of [[support (mathematics)#Compact support|compact support]] contained in '''<math> \\Omega </math>''': the vector [[measure (mathematics)|measure]] '''<math>Du</math>''' represents therefore the [[Distribution (mathematics)#Formal definition|distributional]] or [[weak derivative|weak]] [[gradient]] of '''<math>u</math>'''.\n\nAn equivalent definition is the following.\n\n{{EquationRef|4|Definition 2.2.}} Given a function '''<math>u</math>''' belonging to '''<math>L^1(\\Omega)</math>''', the '''total variation of <math>u</math>'''<ref name=\"Tvar\"/> in <math>\\Omega</math> is defined as\n\n:<math> V(u,\\Omega):=\\sup\\left\\{\\int_\\Omega u(x)\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x\\colon\\boldsymbol{\\phi}\\in C_c^1(\\Omega,\\mathbb{R}^n),\\ \\Vert\\boldsymbol{\\phi}\\Vert_{L^\\infty(\\Omega)}\\le 1\\right\\}</math>\n\nwhere <math> \\scriptstyle \\Vert\\;\\Vert_{L^\\infty(\\Omega)}</math> is the [[essential supremum]] [[Norm (mathematics)|norm]]. Sometimes, especially in the theory of [[Caccioppoli set]]s, the following notation is used\n\n:<math>\\int_\\Omega\\vert D u\\vert = V(u,\\Omega)</math>\n\nin order to emphasize that <math>V(u,\\Omega)</math> is the total variation of the [[Distribution (mathematics)#Formal definition|distributional]] / [[weak derivative|weak]] [[gradient]] of '''<math>u</math>'''. This notation reminds also that if '''<math>u</math>''' is of class  '''<math>C^1</math>''' (i.e. a [[continuous function|continuous]] and [[differentiable function]] having [[continuous function|continuous]] [[derivative]]s) then its [[Total variation|variation]] is exactly the [[Integral (measure theory)|integral]] of the [[absolute value]] of its [[gradient]].\n\nThe space of '''functions of bounded variation''' ('''BV functions''') can then be defined as\n\n:<math> BV(\\Omega)=\\{ u\\in L^1(\\Omega)\\colon V(u,\\Omega)<+\\infty\\}</math>\n\nThe two definitions are equivalent since if <math> \\scriptstyle V(u,\\Omega)<+\\infty </math> then\n\n:<math>\\left|\\int_\\Omega u(x)\\,\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x \\right |\\leq V(u,\\Omega)\\Vert\\boldsymbol{\\phi}\\Vert_{L^\\infty(\\Omega)}\n\\qquad \\forall \\boldsymbol{\\phi}\\in C_c^1(\\Omega,\\mathbb{R}^n)\n</math>\n\ntherefore <math>\\scriptstyle \\boldsymbol{\\phi}\\mapsto\\,\\int_\\Omega u(x)\\,\\mathrm{div}\\boldsymbol{\\phi}(x)dx</math> defines a [[continuous linear functional]] on\nthe space <math>\\scriptstyle C_c^1(\\Omega,\\mathbb{R}^n)</math>. Since <math>\\scriptstyle C_c^1(\\Omega,\\mathbb{R}^n)\n\\subset C^0(\\Omega,\\mathbb{R}^n)</math> as a [[linear subspace]], this [[continuous linear functional]] can be extended [[continuous function|continuously]] and [[linearity|linearily]] to the whole <math>\\scriptstyle C^0(\\Omega,\\mathbb{R}^n)</math> by the [[Hahn–Banach theorem]]. Hence the continuous linear functional defines a [[Radon measure#Duality|Radon measure]] by the [[Riesz–Markov–Kakutani representation theorem|Riesz-Markov Theorem]].\n\n===Locally ''BV'' functions===\nIf the [[function space]] of [[locally integrable function]]s, i.e. [[Function (mathematics)|function]]s belonging to <math>\\scriptstyle L^1_{loc}(\\Omega)</math>, is considered in the preceding definitions {{EquationNote|2|1.2}}, {{EquationNote|3|2.1}} and {{EquationNote|4|2.2}} instead of the one of [[integrable function|globally integrable functions]], then the function space defined is that of '''functions of locally bounded variation'''. Precisely, developing this idea for {{EquationNote|4|definition 2.2}}, a '''[[local property|local]] variation''' is defined as follows,\n:<math> V(u,U):=\\sup\\left\\{\\int_\\Omega u(x)\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x\\colon\\boldsymbol{\\phi}\\in C_c^1(U,\\mathbb{R}^n),\\ \\Vert\\boldsymbol{\\phi}\\Vert_{L^\\infty(\\Omega)}\\le 1\\right\\}</math>\nfor every [[Set (mathematics)|set]] <math>\\scriptstyle U\\in\\mathcal{O}_c(\\Omega)</math>, having defined <math>\\scriptstyle \\mathcal{O}_c(\\Omega)</math> as the set of all [[Relatively compact subspace|precompact]] [[open subset]]s of '''<math>\\Omega</math>''' with respect to the standard [[topology]] of [[dimension (mathematics)|finite-dimensional]] [[vector space]]s, and correspondingly the class of functions of locally bounded variation is defined as\n:<math>BV_{loc}(\\Omega)=\\{ u\\in L^1_{loc}(\\Omega)\\colon V(u,U)<+\\infty\\; \\forall U\\in\\mathcal{O}_c(\\Omega)\\}</math>\n\n===Notation===\nThere are basically two distinct conventions for the notation of spaces of functions of locally or globally bounded variation, and unfortunately they are quite similar: the first one, which is the one adopted in this entry, is used for example in references {{Harvtxt|Giusti|1984}} (partially), {{Harvtxt|Hudjaev|Vol'pert|1985}} (partially), {{Harvtxt|Giaquinta|Modica|Souček|1998}} and is the following one\n*<math>\\scriptstyle BV(\\Omega)</math> identifies the [[Space (mathematics)|space]] of functions of globally bounded variation\n*<math>\\scriptstyle BV_{loc}(\\Omega)</math> identifies the [[Space (mathematics)|space]] of functions of locally bounded variation\nThe second one, which is adopted in references {{Harvtxt|Vol'pert|1967}} and {{Harvtxt|Maz'ya|1985}} (partially), is the following:\n*<math>\\scriptstyle \\overline{BV}(\\Omega)</math> identifies the [[Space (mathematics)|space]] of functions of globally bounded variation\n*<math>\\scriptstyle BV(\\Omega)</math> identifies the [[Space (mathematics)|space]] of functions of locally bounded variation\n\n==Basic properties==\nOnly the properties common to [[Function (mathematics)|function]]s of one variable and to [[Function (mathematics)|function]]s of several variables will be considered in the following, and [[Mathematical proof|proof]]s will be carried on only for functions of several variables since the [[Mathematical proof|proof]] for the case of one variable is a straightforward adaptation of the several variables case: also, in each section it will be stated if the property is shared also by functions of locally bounded variation or not. References {{Harv|Giusti|1984|pp=7–9}}, {{Harv|Hudjaev|Vol'pert|1985}} and {{Harv|Màlek|Nečas|Rokyta|Růžička|1996}} are extensively used.\n\n===''BV'' functions have only jump-type or removable discontinuities===\nIn the case of one variable, the assertion is clear: for each point <math>x_0</math> in the [[interval (mathematics)|interval]] <math>[a , b]\\subset\\mathbb{R}</math> of definition of the function '''<math>u</math>''', either one of the following two assertions is true\n\n:<math> \\lim_{x\\rightarrow x_{0^-}}\\!\\!\\!u(x) = \\!\\!\\!\\lim_{x\\rightarrow x_{0^+}}\\!\\!\\!u(x) </math>\n:<math> \\lim_{x\\rightarrow x_{0^-}}\\!\\!\\!u(x) \\neq \\!\\!\\!\\lim_{x\\rightarrow x_{0^+}}\\!\\!\\!u(x) </math>\n\nwhile both [[Limit of a function|limits]] exist and are finite. In the case of functions of several variables, there are some premises to understand: first of all, there is a [[Linear continuum|continuum]] of [[Direction (geometry, geography)|direction]]s along which it is possible to approach a given point '''<math>x_0</math>''' belonging to the domain '''<math>\\Omega</math>'''⊂ℝ''<sup>n</sup>''. It is necessary to make precise a suitable concept of [[Limit of a function|limit]]: choosing a [[unit vector]] <math>\\scriptstyle{\\boldsymbol{\\hat{a}}}\\in\\mathbb{R}^n</math> it is possible to divide '''<math>\\Omega</math>''' in two sets\n\n:<math>\\Omega_{({\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)} = \\Omega \\cap \\{\\boldsymbol{x}\\in\\mathbb{R}^n|\\langle\\boldsymbol{x}-\\boldsymbol{x}_0,{\\boldsymbol{\\hat{a}}}\\rangle>0\\} \\qquad\n\\Omega_{(-{\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)} = \\Omega \\cap \\{\\boldsymbol{x}\\in\\mathbb{R}^n|\\langle\\boldsymbol{x}-\\boldsymbol{x}_0,-{\\boldsymbol{\\hat{a}}}\\rangle>0\\} </math>\n\nThen for each point '''<math>x_0</math>''' belonging to the domain <math>\\scriptstyle\\Omega\\in\\mathbb{R}^n</math> of the ''BV'' function '''<math>u</math>''', only one of the following two assertions is true\n\n:<math> \\lim_{\\overset{\\boldsymbol{x}\\rightarrow \\boldsymbol{x}_0}{\\boldsymbol{x}\\in\\Omega_{({\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)}}}\\!\\!\\!\\!\\!\\!u(\\boldsymbol{x}) = \\!\\!\\!\\!\\!\\!\\!\\lim_{\\overset{\\boldsymbol{x}\\rightarrow \\boldsymbol{x}_0}{\\boldsymbol{x}\\in\\Omega_{(-{\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)}}}\\!\\!\\!\\!\\!\\!\\!u(\\boldsymbol{x})\n</math>\n:<math>  \\lim_{\\overset{\\boldsymbol{x}\\rightarrow \\boldsymbol{x}_0}{\\boldsymbol{x}\\in\\Omega_{({\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)}}}\\!\\!\\!\\!\\!\\!u(\\boldsymbol{x}) \\neq \\!\\!\\!\\!\\!\\!\\!\\lim_{\\overset{\\boldsymbol{x}\\rightarrow \\boldsymbol{x}_0}{\\boldsymbol{x}\\in\\Omega_{(-{\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)}}}\\!\\!\\!\\!\\!\\!\\!u(\\boldsymbol{x})\n</math>\n\nor '''<math>x_0</math>''' belongs to a [[subset]] of '''<math>\\Omega</math>''' having zero <math>n-1</math>-dimensional [[Hausdorff measure]]. The quantities\n\n:<math>\\lim_{\\overset{\\boldsymbol{x}\\rightarrow \\boldsymbol{x}_0}{\\boldsymbol{x}\\in\\Omega_{({\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)}}}\\!\\!\\!\\!\\!\\!u(\\boldsymbol{x})=u_{\\boldsymbol{\\hat a}}(\\boldsymbol{x}_0) \\qquad \\lim_{\\overset{\\boldsymbol{x}\\rightarrow \\boldsymbol{x}_0}{\\boldsymbol{x}\\in\\Omega_{(-{\\boldsymbol{\\hat{a}}},\\boldsymbol{x}_0)}}}\\!\\!\\!\\!\\!\\!\\!u(\\boldsymbol{x})=u_{-\\boldsymbol{\\hat a}}(\\boldsymbol{x}_0)</math>\n\nare called '''approximate limits''' of the ''BV'' function '''<math>u</math>''' at the point '''<math>x_0</math>'''.\n\n===''V''(&middot;,&nbsp;&Omega;) is lower semi-continuous on ''BV''(&Omega;)===\nThe [[functional (mathematics)|functional]] <math>\\scriptstyle V(\\cdot,\\Omega):BV(\\Omega)\\rightarrow \\mathbb{R}^+</math> is [[semi-continuity|lower semi-continuous]]:\nto see this, choose a [[Cauchy sequence]] of ''BV''-functions '''<math>\\scriptstyle\\{u_n\\}_{n\\in\\mathbb{N}}</math>''' converging to '''[[locally integrable function|<math>\\scriptstyle u\\in L^1_\\text{loc}(\\Omega)</math>]]'''. Then, since all the functions of the sequence and their limit function are [[integral|integrable]] and by the definition of [[lower limit]]\n\n:<math>\\liminf_{n\\rightarrow\\infty}V(u_n,\\Omega) \\geq \\liminf_{n\\rightarrow\\infty} \\int_\\Omega u_n(x)\\,\\mathrm{div}\\, \\boldsymbol{\\phi}\\, \\mathrm{d}x \\geq \\int_\\Omega \\lim_{n\\rightarrow\\infty} u_n(x)\\,\\mathrm{div}\\, \\boldsymbol{\\phi}\\, \\mathrm{d}x = \\int_\\Omega u(x)\\,\\mathrm{div}\\boldsymbol{\\phi}\\, \\mathrm{d}x \\qquad\\forall\\boldsymbol{\\phi}\\in C_c^1(\\Omega,\\mathbb{R}^n),\\quad\\Vert\\boldsymbol{\\phi}\\Vert_{L^\\infty(\\Omega)}\\leq 1 </math>\n\nNow considering the [[supremum]] on the set of functions <math>\\scriptstyle\\boldsymbol{\\phi}\\in C_c^1(\\Omega,\\mathbb{R}^n)</math> such that <math>\\scriptstyle \\Vert\\boldsymbol{\\phi}\\Vert_{L^\\infty(\\Omega)}\\leq 1 </math> then the following inequality holds true\n\n:<math>\\liminf_{n\\rightarrow\\infty}V(u_n,\\Omega)\\geq V(u,\\Omega)</math>\n\nwhich is exactly the definition of [[semicontinuity|lower semicontinuity]].\n\n===''BV''(&Omega;) is a Banach space===\nBy definition '''<math>BV(\\Omega)</math>''' is a [[subset]] of '''[[integrable function|<math>L^1(\\Omega)</math>]]''', while [[linearity]] follows from the linearity properties of the defining [[integral]] i.e.\n\n:<math>\\begin{align}\n\\int_\\Omega [u(x)+v(x)]\\,\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x & =\n\\int_\\Omega u(x)\\,\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x +\\int_\\Omega v(x)\\,\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x = \\\\\n& =- \\int_\\Omega \\langle\\boldsymbol{\\phi}(x), Du(x)\\rangle- \\int_\\Omega \\langle \\boldsymbol{\\phi}(x), Dv(x)\\rangle\n                                               =- \\int_\\Omega \\langle \\boldsymbol{\\phi}(x), [Du(x)+Dv(x)]\\rangle\n\\end{align}\n</math>\n\nfor all <math>\\scriptstyle\\phi\\in C_c^1(\\Omega,\\mathbb{R}^n)</math> therefore <math>\\scriptstyle u+v\\in BV(\\Omega)</math>for all <math>\\scriptstyle u,v\\in BV(\\Omega)</math>, and\n\n:<math>\n\\int_\\Omega c\\cdot u(x)\\,\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x =\nc\\!\\int_\\Omega u(x)\\,\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x =\n-c\\! \\int_\\Omega \\langle \\boldsymbol{\\phi}(x), Du(x)\\rangle\n</math>\n\nfor all <math>\\scriptstyle c\\in\\mathbb{R}</math>, therefore <math>\\scriptstyle cu\\in BV(\\Omega)</math> for all <math>\\scriptstyle u\\in BV(\\Omega)</math>, and all <math>\\scriptstyle c\\in\\mathbb{R}</math>. The proved [[vector space]] properties imply that '''<math>BV(\\Omega)</math>''' is a [[vector subspace]] of '''[[Lp space|<math>L^1(\\Omega)</math>]]'''. Consider now the function <math>\\scriptstyle\\|\\;\\|_{BV}:BV(\\Omega)\\rightarrow\\mathbb{R}^+</math> defined as\n\n:<math>\\| u \\|_{BV} := \\| u \\|_{L^1} + V(u,\\Omega)</math>\n\nwhere <math>\\scriptstyle\\| \\; \\|_{L^1}</math> is the usual '''[[Lp space#Lp spaces|<math>L^1(\\Omega)</math> norm]]''': it is easy to prove that this is a [[norm (mathematics)|norm]] on '''<math>BV(\\Omega)</math>'''. To see that '''<math>BV(\\Omega)</math>''' is [[complete metric space|complete]] respect to it, i.e. it is a [[Banach space]], consider a [[Cauchy sequence]] <math>\\scriptstyle\\{u_n\\}_{n\\in\\mathbb{N}}</math> in '''<math>BV(\\Omega)</math>'''. By definition it is also a [[Cauchy sequence]] in '''<math>L^1(\\Omega)</math>''' and therefore has a [[limit of a sequence|limit]] '''<math>u</math>''' in '''<math>L^1(\\Omega)</math>''': since '''<math>u_n</math>''' is bounded in '''<math>BV(\\Omega)</math>''' for each '''<math>n</math>''', then <math>\\scriptstyle \\Vert u \\Vert_{BV} < +\\infty </math> by [[semicontinuity|lower semicontinuity]] of the variation <math>\\scriptstyle V(\\cdot,\\Omega)</math>, therefore '''<math>u</math>''' is a ''BV'' function. Finally, again by lower semicontinuity, choosing an arbitrary small positive number '''<math>\\scriptstyle\\varepsilon</math>'''\n\n:<math>\\Vert u_j - u_k \\Vert_{BV}<\\varepsilon\\quad\\forall j,k\\geq N\\in\\mathbb{N} \\quad\\Rightarrow\\quad V(u_k-u,\\Omega)\\leq \\liminf_{j\\rightarrow +\\infty} V(u_k-u_j,\\Omega)\\leq\\varepsilon</math>\nFrom this we deduce that <math>\\scriptstyle V(\\cdot,\\Omega)</math> is continuous because it's a norm.\n\n===''BV''(&Omega;) is not separable===\nTo see this, it is sufficient to consider the following example belonging to the space '''<math>BV([0,1])</math>''':<ref>The example is taken from  {{Harvtxt|Giaquinta|Modica|Souček|1998|p=331}}: see also {{harv|Kannan|Krueger|1996|loc=example 9.4.1, p. 237}}.</ref> for each 0<''&alpha;''<1 define\n:<math>\\chi_\\alpha=\\chi_{[\\alpha,1]}=\n\\begin{cases} 0 & \\mbox{if } x \\notin\\; [\\alpha,1] \\\\ \n              1 & \\mbox{if } x \\in [\\alpha,1]\n\\end{cases}\n</math>\nas the [[indicator function|characteristic function]] of the [[Interval (mathematics)#Terminology|left-closed interval]] <math>[\\alpha,1]</math>. Then, choosing ''&alpha;,&beta;''∈<math>[0,1]</math> such that ''&alpha;''≠''&beta;'' the following relation holds true:\n:<math>\\Vert \\chi_\\alpha - \\chi_\\beta \\Vert_{BV}=2+|\\alpha-\\beta|</math>\nNow, in order to prove that every [[Dense set|dense subset]] of '''<math>BV(]0,1[)</math>''' cannot be [[countable set|countable]], it is sufficient to see that for every ''&alpha;''∈<math>[0,1]</math> it is possible to construct the [[Ball (mathematics)|ball]]s\n:<math>B_\\alpha=\\left\\{\\psi\\in BV([0,1]);\\Vert \\chi_\\alpha - \\psi \\Vert_{BV}\\leq 1\\right\\}</math>\nObviously those balls are [[Disjoint sets|pairwise disjoint]], and also are an [[indexed family]] of [[set (mathematics)|set]]s whose [[index set]] is <math>[0,1]</math>. This implies that this family has the [[cardinality of the continuum]]: now, since any dense subset of '''<math>BV([0,1])</math>''' must have at least a point inside each member of this family, its cardinality is at least that of the continuum and therefore cannot a be countable subset.<ref>The same argument is used by {{Harvtxt|Kolmogorov|Fomin|1969|loc=example 7, pp. 48–49}}, in order to prove the non [[Separable space|separability]] of the space of [[bounded sequence]]s, and also {{harvtxt|Kannan|Krueger|1996|loc=example 9.4.1, p. 237}}.</ref> This example can be obviously extended to higher dimensions, and since it involves only [[Local property|local properties]], it implies that the same property is true also for '''<math>BV_{loc}</math>'''.\n\n===Chain rule for ''BV'' functions===\n[[Chain rule]]s for [[smooth function|nonsmooth function]]s are very important in [[mathematics]] and [[mathematical physics]] since there are several important [[Mathematical model|physical model]]s whose behaviors are described by [[Function (mathematics)|functions]] or [[functional (mathematics)|functional]]s with a very limited degree of [[Smooth function|smoothness]]. The following chain rule is proved in the paper {{Harv|Vol'pert|1967|p=248}}. Note all [[partial derivative]]s must be interpreted in a generalized sense, i.e., as [[Generalized derivative#Basic idea|generalized derivative]]s.\n\n'''Theorem'''. Let <math>\\scriptstyle f:\\mathbb{R}^p\\rightarrow\\mathbb{R}</math> be a function of class '''<math>C^1</math>''' (i.e. a [[continuous function|continuous]] and [[differentiable function]] having [[continuous function|continuous]] [[derivative]]s) and let <math>\\scriptstyle\\boldsymbol{u}(\\boldsymbol{x})=(u_1(\\boldsymbol{x}),\\ldots,u_p(\\boldsymbol{x})) </math> be a function in '''<math>BV(\\Omega)</math>''' with '''<math> \\Omega </math>''' being an [[open subset]] of <math> \\scriptstyle\\mathbb{R}^n </math>.\nThen <math>\\scriptstyle f\\circ\\boldsymbol{u}(\\boldsymbol{x})=f(\\boldsymbol{u}(\\boldsymbol{x}))\\in BV(\\Omega) </math> and\n\n:<math>\\frac{\\partial f(\\boldsymbol{u}(\\boldsymbol{x}))}{\\partial x_i}=\\sum_{k=1}^p\\frac{\\partial\\bar{f}(\\boldsymbol{u}(\\boldsymbol{x}))}{\\partial u_k}\\frac{\\partial{u_k(\\boldsymbol{x})}}{\\partial x_i}\n\\qquad\\forall i=1,\\ldots,n</math>\n\nwhere <math>\\scriptstyle\\bar f(\\boldsymbol{u}(\\boldsymbol{x}))</math> is the mean value of the function at the point '''<math>\\scriptstyle x \\in\\Omega</math>''', defined as\n\n:<math>\\bar f(\\boldsymbol{u}(\\boldsymbol{x}))=\\int_0^1 f\\left(\\boldsymbol{u}_{\\boldsymbol{\\hat a}}(\\boldsymbol{x})t + \\boldsymbol{u}_{-\\boldsymbol{\\hat a}}(\\boldsymbol{x})(1-t)\\right)dt</math>\n\nA more general [[chain rule]] [[formula]] for [[lipschitz continuity|Lipschitz continuous functions]] <math>\\scriptstyle f:\\mathbb{R}^p\\rightarrow\\mathbb{R}^s</math> has been found by [[Luigi Ambrosio]] and [[Gianni Dal Maso]] and is published in the paper {{Harv|Ambrosio|Dal Maso|1990}}. However, even this formula has very important direct consequences: choosing <math>\\scriptstyle f(u)=v(\\boldsymbol{x})u(\\boldsymbol{x})</math>, where <math>\\scriptstyle v(\\boldsymbol{x})</math> is also a '''<math>BV</math>''' function, the preceding formula gives the '''''[[Product rule|Leibniz rule]]''''' for '''<math>BV</math>''' functions\n\n:<math>\\frac{\\partial v(\\boldsymbol{x})u(\\boldsymbol{x})}{\\partial x_i} = {\\bar u(\\boldsymbol{x})}\\frac{\\partial v(\\boldsymbol{x})}{\\partial x_i} +\n{\\bar v(\\boldsymbol{x})}\\frac{\\partial u(\\boldsymbol{x})}{\\partial x_i} </math>\n\nThis implies that '''the product of two functions of bounded variation is again a function of bounded variation''', therefore '''<math>BV(\\Omega)</math>''' is an [[Associative algebra|algebra]].\n\n===''BV''(&Omega;) is a Banach algebra===\nThis property follows directly from the fact that '''<math>BV(\\Omega)</math>''' is a [[Banach space]] and also an [[associative algebra]]: this implies that if '''<math>\\{v_n\\}</math>''' and '''<math>\\{u_n\\}</math>''' are [[Cauchy sequence]]s of <math>BV</math> functions converging respectively to [[function (mathematics)|function]]s '''<math>v</math>''' and  '''<math>u</math>''' in '''<math>BV(\\Omega)</math>''', then\n\n::<math>\\begin{matrix}\n  vu_n\\xrightarrow[n\\to\\infty]{} vu \\\\\n  v_nu\\xrightarrow[n\\to\\infty]{} vu\n        \\end{matrix}\\quad\\Longleftrightarrow\n\\quad vu\\in BV(\\Omega)</math>\n\ntherefore the ordinary [[Pointwise product|product of functions]] is [[continuity (mathematics)|continuous]] in '''<math>BV(\\Omega)</math>''' with respect to each argument, making this function space a [[Banach algebra]].\n\n==Generalizations and extensions==\n\n=== Weighted ''BV'' functions ===\nIt is possible to generalize the above notion of [[total variation]] so that different variations are weighted differently. More precisely, let <math>\\scriptstyle \\varphi : [0, +\\infty)\\longrightarrow [0, +\\infty)</math> be any increasing function such that <math>\\scriptstyle \\varphi(0) = \\varphi(0+) =\\lim_{x\\rightarrow 0_+}\\varphi(x) = 0</math> (the '''[[weight function]]''') and let <math>\\scriptstyle f: [0, T]\\longrightarrow X </math> be a function from the [[interval (mathematics)|interval]] <math>[0 , T]</math>⊂ℝ taking values in a [[normed vector space]] <math>X</math>. Then the <math>\\scriptstyle \\boldsymbol\\varphi</math>'''-variation''' of <math>f</math> over <math>[0, T]</math> is defined as\n\n:<math>\\mathop{\\varphi\\mbox{-Var}}_{[0, T]} (f) := \\sup \\sum_{j = 0}^{k} \\varphi \\left( | f(t_{j + 1}) - f(t_{j}) |_{X} \\right),</math>\n\nwhere, as usual, the supremum is taken over all finite [[partition of an interval|partitions]] of the interval <math>[0, T]</math>, i.e. all the [[finite set]]s of [[real number]]s <math>t_i</math> such that\n\n:<math>0 = t_{0} < t_{1} < \\ldots < t_{k} = T.</math>\n\nThe original notion of [[Total variation|variation]] considered above is the special case of <math>\\scriptstyle \\varphi</math>-variation for which the weight function is the [[identity function]]: therefore an [[integrable function]] <math>f</math> is said to be a '''weighted ''BV'' function''' (of weight <math>\\scriptstyle\\varphi</math>) if and only if its <math>\\scriptstyle \\varphi</math>-variation is finite.\n\n:<math>f\\in BV_\\varphi([0, T];X)\\iff \\mathop{\\varphi\\mbox{-Var}}_{[0, T]} (f) <+\\infty</math>\n\nThe space <math>\\scriptstyle BV_\\varphi([0, T];X)</math> is a [[topological vector space]] with respect to the [[norm (mathematics)|norm]]\n\n:<math>\\| f \\|_{BV_\\varphi} := \\| f \\|_{\\infty} + \\mathop{\\varphi \\mbox{-Var}}_{[0, T]} (f),</math>\n\nwhere <math>\\scriptstyle\\| f \\|_{\\infty}</math> denotes the usual [[supremum norm]] of ''<math>f</math>''. Weighted ''BV'' functions were introduced and studied in full generality by [[Władysław Orlicz]] and [[Julian Musielak]] in the paper {{Harvnb|Musielak|Orlicz|1959}}: [[Laurence Chisholm Young]] studied earlier the case <math>\\scriptstyle\\varphi(x)=x^p</math> where ''<math>p</math>'' is a positive integer.\n\n===''SBV'' functions===\n'''SBV functions''' ''i.e.'' ''Special functions of Bounded Variation'' were introduced by [[Luigi Ambrosio]] and [[Ennio de Giorgi]] in the paper {{Harv|Ambrosio|De Giorgi|1988}}, dealing with free discontinuity [[variational problem]]s: given an [[open subset]] '''<math> \\Omega </math>''' of ℝ''<sup>n</sup>'', the space '''<math>SBV(\\Omega)</math>''' is a proper [[linear subspace]] of '''<math>BV(\\Omega)</math>''', since the [[weak derivative|weak]] [[gradient]] of each function belonging to it consists precisely of the [[summation|sum]] of an <math>n</math>-[[dimension]]al [[Support (mathematics)|support]] and an <math>n-1</math>-[[dimension]]al [[Support (mathematics)|support]] [[Measure (mathematics)|measure]] and ''no intermediate-dimensional terms'', as seen in the following definition.\n\n'''Definition'''. Given a [[locally integrable function]] '''<math>u</math>''', then <math>\\scriptstyle u\\in {S\\!BV}(\\Omega) </math> if and only if\n\n'''1.''' There exist two [[Borel function]]s <math>f</math> and <math>g</math> of [[Domain of a function|domain]] '''<math>\\Omega</math>''' and [[codomain]] ℝ''<sup>n</sup>'' such that\n\n:<math> \\int_\\Omega\\vert f\\vert dH^n+ \\int_\\Omega\\vert g\\vert dH^{n-1}<+\\infty.</math>\n\n'''2.''' For all of [[Smooth function|continuously differentiable]] [[vector-valued function|vector functions]] <math> \\scriptstyle\\phi </math> of [[support (mathematics)#Compact support|compact support]] contained in '''<math> \\Omega </math>''', ''i.e.'' for all <math> \\scriptstyle \\phi \\in\nC_c^1(\\Omega,\\mathbb{R}^n)</math> the following formula is true:\n\n:<math> \\int_\\Omega u\\mbox{div} \\phi dH^n = \\int_\\Omega \\langle \\phi, f\\rangle dH^n +\\int_\\Omega \\langle \\phi, g\\rangle dH^{n-1}.</math>\n\nwhere <math>H^\\alpha</math> is the <math>\\alpha</math>-[[dimension]]al [[Hausdorff measure]].\n\nDetails on the properties of ''SBV'' functions can be found in works cited in the bibliography section: particularly the paper {{Harv|De Giorgi|1992}} contains a useful [[bibliography]].\n\n===''bv'' sequences===\nAs particular examples of [[Banach spaces]], {{harvtxt|Dunford|Schwartz|1958|loc=Chapter IV}} consider spaces of '''sequences of bounded variation''', in addition to the spaces of functions of bounded variation. The total variation of a [[sequence (mathematics)|sequence]] ''x''=(''x''<sub>i</sub>) of real or complex numbers is defined by\n:<math>TV(x) = \\sum_{i=1}^\\infty |x_{i+1}-x_i|.</math>\n\nThe space of all sequences of finite total variation is denoted by ''bv''. The norm on ''bv'' is given by\n:<math>\\|x\\|_{bv} = |x_1| + TV(x) = |x_1| +  \\sum_{i=1}^\\infty |x_{i+1}-x_i|.</math>\nWith this norm, the space ''bv'' is a Banach space.\n\nThe total variation itself defines a norm on a certain subspace of ''bv'', denoted by ''bv''<sub>0</sub>, consisting of sequences ''x'' = (''x''<sub>i</sub>) for which\n:<math>\\lim_{n\\to\\infty} x_n =0.</math>\nThe norm on ''bv''<sub>0</sub> is denoted\n:<math>\\|x\\|_{bv_0} = TV(x) = \\sum_{i=1}^\\infty |x_{i+1}-x_i|.</math>\nWith respect to this norm ''bv''<sub>0</sub> becomes a Banach space as well.\n\n===Measures of bounded variation===\nA [[signed measure|signed]] (or [[complex measure|complex]]) [[Measure (mathematics)|measure]] ''<math>\\mu</math>'' on a [[sigma-algebra|measurable space]] <math>(X,\\Sigma)</math> is said to be of bounded variation if its [[Total variation#Total variation in measure theory|total variation]]'' <math>\\scriptstyle\\Vert \\mu\\Vert=|\\mu|(X)</math>'' is bounded: see {{harvtxt|Halmos|1950|p=123}}, {{harvtxt|Kolmogorov|Fomin|1969|p=346}} or the entry \"[[Total variation]]\" for further details.\n\n==Examples==\n[[File:Sin x^-1.svg|right|thumb|The function ''f''(''x'')=sin(1/''x'') is ''not'' of bounded variation on the interval <math> [0,2 / \\pi] </math>.]]\nThe function\n\n:<math>f(x) = \\begin{cases} 0, & \\mbox{if }x =0 \\\\ \\sin(1/x), & \\mbox{if } x \\neq 0 \\end{cases} </math>\n\nis ''not'' of bounded variation on the interval <math> [0, 2/\\pi]</math>\n\n[[File:Xsin(x^-1).svg|thumb|right|The function ''f''(''x'')=''x''&nbsp;sin(1/''x'') is ''not'' of bounded variation on the interval <math> [0,2 / \\pi] </math>.]]\nWhile it is harder to see, the continuous function\n\n:<math>f(x) = \\begin{cases} 0, & \\mbox{if }x =0 \\\\ x \\sin(1/x), & \\mbox{if } x \\neq 0 \\end{cases} </math>\n\nis ''not'' of bounded variation on the interval <math> [0, 2/\\pi]</math> either.\n\n[[File:X^2sin(x^-1).svg|thumb|right|The function ''f''(''x'')=''x''<sup>2</sup>&nbsp;sin(1/''x'') ''is'' of bounded variation on the interval <math> [0,2 / \\pi] </math>.]]\nAt the same time, the function\n\n:<math>f(x) = \\begin{cases} 0, & \\mbox{if }x =0 \\\\ x^2 \\sin(1/x), & \\mbox{if } x \\neq 0 \\end{cases} </math>\n\nis of bounded variation on the interval <math> [0,2/\\pi]</math>. However, ''all three functions are of bounded variation on each interval'' <math>[a,b]</math> ''with'' <math>a>0</math>.\n\nThe [[Sobolev space]] '''<math> W^{1,1}(\\Omega)</math>''' is a [[proper subset]] of '''<math> BV(\\Omega)</math>'''. In fact, for each '''<math> u </math>''' in '''<math> W^{1,1}(\\Omega) </math>''' it is possible to choose a [[Measure (mathematics)|measure]] <math> \\scriptstyle \\mu:=\\nabla u \\mathcal L</math> (where <math> \\scriptstyle\\mathcal L</math> is the [[Lebesgue measure]] on '''<math>\\Omega</math>''') such that the equality\n\n:<math> \\int u\\mathrm{div}\\phi = -\\int \\phi\\, d\\mu = -\\int \\phi \\nabla u \\qquad \\forall \\phi\\in C_c^1 </math>\n\nholds, since it is nothing more than the definition of [[weak derivative]], and hence holds true. One can easily find an example of a ''BV'' function which is not '''<math>W^{1,1}</math>''':  in dimension one, any step function with a non-trivial jump will do.\n\n==Applications==\n\n=== Mathematics ===\n\nFunctions of bounded variation have been studied in connection with the set of [[classification of discontinuities|discontinuities]] of functions and differentiability of real functions, and the following results are well-known. If <math>f</math> is a [[real number|real]] [[Function (mathematics)|function]] of bounded variation on an interval <math>[a,b]</math> then\n\n* <math>f</math> is [[continuous function|continuous]] except at most on a [[countable set]];\n* <math>f</math> has [[one-sided limit]]s everywhere (limits from the left everywhere in <math>(a,b]</math>, and from the right everywhere in <math>[a,b)</math> ;\n* the [[derivative]] <math>f'(x)</math> exists [[almost everywhere]] (i.e. except for a set of [[measure zero]]).\n\nFor [[real number|real]] [[Function (mathematics)|functions]] of several real variables\n\n* the [[Indicator function|characteristic function]] of a [[Caccioppoli set]] is a ''BV'' function: ''BV'' functions lie at the basis of the modern theory of perimeters.\n* [[Minimal surface]]s are [[Graph of a function|graph]]s of ''BV'' functions: in this context, see reference {{Harv|Giusti|1984}}.\n\n===Physics and engineering===\nThe ability of ''BV'' functions to deal with discontinuities has made their use widespread in the applied sciences: solutions of problems in mechanics, physics, chemical kinetics are very often representable by functions of bounded variation. The book {{Harv|Hudjaev|Vol'pert|1985}} details a very ample set of mathematical physics applications of ''BV'' functions. Also there is some modern application which deserves a brief description.\n\n*The [[Mumford–Shah functional]]: the segmentation problem for a two-dimensional image, i.e. the problem of faithful reproduction of contours and grey scales is equivalent to the [[minimum|minimization]] of such [[Functional (mathematics)|functional]].\n\n==See also==\n{{colbegin|colwidth=20em}}\n* [[Renato Caccioppoli]]\n* [[Caccioppoli set]]\n* [[Lamberto Cesari]]\n* [[Ennio de Giorgi]]\n* [[Helly's selection theorem]]\n* [[Locally integrable function]]\n* [[Lp space|''L''<sup>''p''</sup>(&Omega;) space]]\n* [[Lebesgue–Stieltjes integral]]\n* [[Radon measure]]\n* [[Reduced derivative]]\n* [[Riemann–Stieltjes integral]]\n* [[Total variation]]\n* [[Aizik Isaakovich Vol'pert]]\n{{colend}}\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n{{refbegin}}\n\n===Research works===\n*{{Citation\n | last =Ambrosio\n | first =Luigi\n | author-link =Luigi Ambrosio\n | last2 =Fusco\n | first2 =Nicola\n | author2-link =Nicola Fusco\n | last3 =Pallara\n | first3 =Diego\n | author3-link =\n | title =Functions of bounded variation and free discontinuity problems\n | place =Oxford\n | publisher =The Clarendon Press / Oxford University Press\n | series =Oxford Mathematical Monographs\n | year =2000\n | pages =xviii+434\n | isbn =978-0-19-850245-6\n | mr =1857292\n | zbl =0957.49001\n}}. \n*{{Citation\n | last =Brudnyi\n | first =Yuri\n | author-link =Yuri Brudnyi\n | editor-last =Randrianantoanina\n | editor-first =Beata\n | editor2-last =Randrianantoanina\n | editor2-first =Narcisse\n | contribution =Multivariate functions of bounded {{math|(''k'', ''p'')}}–variation\n | contribution-url =http://www.degruyter.com/view/books/9783110918298/9783110918298.37/9783110918298.37.xml\n | title =Banach Spaces and their Applications in Analysis. Proceedings of the international conference, Miami University, Oxford, OH, USA, May 22--27, 2006. In honor of Nigel Kalton's 60th birthday\n | place =Berlin–Boston\n | publisher =Walter De Gruyter\n | year =2007\n | pages =37–58\n | language =\n | url =http://www.degruyter.com/viewbooktoc/product/175671\n | doi =10.1515/9783110918298.37\n | isbn =978-3-11-019449-4\n | mr =2374699\n | zbl = 1138.46019\n}}\n*{{Citation\n| last =  Dunford\n| first = Nelson\n| author-link = Nelson Dunford\n| last2 = Jacob T.\n| first2 = Schwartz\n| author2-link = Jacob T. Schwartz\n| title = Linear operators. Part I: General Theory\n| place = New York–London–Sydney\n| publisher = Wiley-Interscience\n| year = 1958\n| series = Pure and Applied Mathematics\n| volume = VII\n| isbn = 0-471-60848-3\n| zbl =  0084.10402\n}}. Includes a discussion of the functional-analytic properties of spaces of functions of bounded variation.\n*{{Citation\n| last = Giaquinta\n| first = Mariano\n| author-link = Mariano Giaquinta\n| last2 = Modica\n| first2 = Giuseppe\n| author2-link =\n| last3 = Souček\n| first3 = Jiří\n| author3-link =\n| title = Cartesian Currents in the Calculus of Variation I\n| place = Berlin-Heidelberg-New York\n| publisher = Springer Verlag\n| year = 1998\n| series = [[Ergebnisse der Mathematik und ihrer Grenzgebiete]]. 3. Folge. A Series of Modern Surveys in Mathematics\n| volume = 37\n| url = https://books.google.com/?id=xx2vhd_uPS0C&printsec=frontcover#v=onepage&q&f=true\n| isbn = 3-540-64009-6\n| zbl = 0914.49001}}.\n*{{Citation\n| last = Giusti\n| first = Enrico\n| author-link = Enrico Giusti\n| title = Minimal surfaces and functions of bounded variations\n| place = Basel–Boston–Stuttgart\n| publisher = Birkhäuser Verlag\n| year = 1984\n| series = Monographs in Mathematics\n| volume = 80\n| url = https://books.google.com/books?id=dNgsmArDoeQC&printsec=frontcover&dq=Minimal+surfaces+and+functions+of+bounded+variations\n| pages=XII+240\n| isbn = 978-0-8176-3153-6\n| mr=775682 \n| zbl =0545.49018}},  particularly part I, chapter 1 \"''Functions of bounded variation and Caccioppoli sets''\". A good reference on the theory of [[Caccioppoli set]]s and their application to the [[minimal surface]] problem.\n*{{Citation\n| last = Halmos\n| first = Paul\n| author-link = Paul Halmos\n| title = Measure theory\n| publisher = Van Nostrand and Co.\n| year = 1950\n| url = https://books.google.com/books?id=-Rz7q4jikxUC&printsec=frontcover&dq=halmos+measure+theory#PPP1,M1\n| isbn = 978-0-387-90088-9\n| zbl = 0040.16802\n}}. The link is to a preview of a later reprint by Springer-Verlag.\n*{{Citation\n| last = Hudjaev\n| first = Sergei Ivanovich \n| author-link =\n| last2 = Vol'pert\n| first2 = Aizik Isaakovich\n| author2-link = Aizik Isaakovich Vol'pert\n| title = Analysis in classes of discontinuous functions and equations of mathematical physics\n| place = Dordrecht–Boston–Lancaster\n| publisher = Martinus Nijhoff Publishers\n| year = 1985\n| series = Mechanics: analysis\n| volume = 8\n| url = https://books.google.com/books?id=lAN0b0-1LIYC&printsec=frontcover&dq=%22Analysis+in+classes+of+discontinuous+functions%22\n| mr = 785938 \n| isbn = 90-247-3109-7\n| zbl = 0564.46025\n}}. The whole book is devoted to the theory of {{math|''BV''}} functions and their applications to problems in [[mathematical physics]] involving [[discontinuous function]]s and geometric objects with [[smooth function|non-smooth]] [[boundary (topology)|boundaries]].\n*{{Citation\n| last = Kannan\n| first = Rangachary\n| author-link =\n| last2 = Krueger\n| first2 = Carole King\n| author2-link =\n| title = Advanced analysis on the real line\n| place = Berlin–Heidelberg–New York\n| publisher = Springer Verlag\n| year = 1996\n| series = Universitext\n| pages = x+259\n| isbn = 978-0-387-94642-9\n| mr = 1390758\n| zbl = 0855.26001\n}}. Maybe the most complete book reference for the theory of {{math|''BV''}} functions in one variable: classical results and advanced results are collected in chapter 6 \"''Bounded variation''\" along with several exercises. The first author was a collaborator of [[Lamberto Cesari]].\n*{{Citation\n| first=Andrej N.\n| last=Kolmogorov\n| author-link= Andrey Kolmogorov\n| first2=Sergej V.\n| last2=Fomin\n| author2-link=Sergei Fomin\n| title=Introductory Real Analysis\n| publisher=Dover Publications\n| pages=xii+403\n| url=https://books.google.com/books?id=z8IaHgZ9PwQC&printsec=frontcover#v=onepage&q\n| place=New York\n| year=1969\n| isbn = 0-486-61226-0\n| mr=0377445 \n| zbl=0213.07305\n}}.\n*{{Citation\n | last =Leoni\n | first =Giovanni\n | author-link =\n | title = A First Course in Sobolev Spaces: Second Edition\n | place =\n | publisher =American Mathematical Society\n | series = Graduate Studies in Mathematics\n | year =2017\n | pages =xxii+734\n | isbn = 978-1-4704-2921-8\n | mr =\n | zbl =\n}}. \n*{{Citation\n| last = Màlek\n| first = Josef\n| author-link =\n| last2 = Nečas\n| first2 = Jindřich\n| author2-link =\n| last3 = Rokyta\n| first3 = Mirko\n| last4 = Růžička\n| first4 = Michael\n| title = Weak and measure-valued solutions to evolutionary PDEs\n| place = London–Weinheim–New York–Tokyo–Melbourne–Madras\n| publisher = Chapman & Hall CRC Press\n| year = 1996\n| series = Applied Mathematics and Mathematical Computation\n| volume = 13\n| pages = xi+331\n| url = https://books.google.com/books?id=30_PBBzwSfAC&printsec=frontcover&dq=Weak+and+measure-valued+solutions+to+evolutionary+PDEs\n| isbn = 0-412-57750-X\n| mr = 1409366\n| zbl = 0851.35002}}. One of the most complete monographs on the theory of [[Young measure]]s, strongly oriented to applications in continuum mechanics of fluids.\n*{{Citation\n| last = Maz'ya\n| first = Vladimir G.\n| authorlink = Vladimir Gilelevich Maz'ya\n| title = Sobolev Spaces\n| publisher = Springer-Verlag\n| location = Berlin–Heidelberg–New York\n| year = 1985\n| isbn=0-387-13589-8\n| zbl = 0692.46023\n}}; particularly chapter 6, \"On functions in the space {{math|''BV''(Ω)}}\". One of the best monographs on the theory of [[Sobolev space]]s.\n*{{Citation\n| first = Jean Jacques\n| last = Moreau\n| author-link = Jean-Jacques Moreau\n| editor-last = Moreau\n| editor-first = J. J.\n| editor2-last = Panagiotopoulos\n| editor2-first = P. D.\n| editor3-last = Strang\n| editor3-first = G.\n| editor3-link = Gilbert Strang\n| contribution = Bounded variation in time\n| title = Topics in nonsmooth mechanics\n| year = 1988\n| pages = 1–74\n| place = Basel–Boston–Stuttgart\n| publisher = Birkhäuser Verlag\n| isbn = 3-7643-1907-0\n| zbl = 0657.28008}}\n*{{Citation\n| last = Musielak\n| first = Julian\n| author-link =\n| last2 = Orlicz\n| first2 = Władysław\n| author2-link = Władysław Orlicz\n| title = On generalized variations (I)\n| journal = [[Studia Mathematica]]\n| place = Warszawa–Wrocław\n| volume = 18\n| pages = 13–41\n| year = 1959\n| url = http://matwbn.icm.edu.pl/ksiazki/sm/sm18/sm1812.pdf\n| zbl = 0088.26901\n}}. In this paper, Musielak and Orlicz developed the concept of weighted {{math|''BV''}} functions introduced by [[Laurence Chisholm Young]] to its full generality.\n*{{Citation\n| first=Frigyes\n| last=Riesz\n| author-link=Frigyes Riesz\n| first2=Béla\n| last2=Szőkefalvi-Nagy\n| author2-link=Béla Szőkefalvi-Nagy\n| title=Functional Analysis\n| publisher=Dover Publications\n| place=New York\n| url=https://books.google.com/books?id=jlQnThDV41UC&printsec=frontcover#v=onepage&q\n| year=1990\n| isbn=0-486-66289-6\n| zbl=0732.47001\n}}\n*{{Citation\n| last = Vol'pert\n| first = Aizik Isaakovich\n| author-link =\n| title = Spaces {{math|''BV''}} and quasi-linear equations\n| journal = [[Matematicheskii Sbornik]]\n| series = (N.S.)\n| volume = 73 (115)\n| language = Russian\n| issue = 2\n| pages = 255–302\n| year = 1967\n| url = http://mi.mathnet.ru/eng/msb/v115/i2/p255\n| mr = 216338 \n| zbl = 0168.07402\n}}. A seminal paper where [[Caccioppoli set]]s and {{math|''BV''}} functions are thoroughly studied and the concept of [[functional superposition]] is introduced and applied to the theory of [[partial differential equation]]s: it was also translated in English as {{Citation\n| title = Spaces {{math|''BV''}} and quasi-linear equations\n| journal = [[Matematicheskii Sbornik|Mathematics USSR-Sbornik]]\n| volume = 2\n| issue = 2\n| pages = 225–267\n| year = 1967\n| url =\n| doi = 10.1070/SM1967v002n02ABEH002340\n| mr = 216338\n| zbl = 0168.07402\n| last1 = Vol'Pert\n| first1 = A I\n}}.\n\n===Historical references===\n*{{Citation\n| last = Adams\n| first = C. Raymond\n| author-link =Clarence Raymond Adams\n| last2 = Clarkson\n| first2 = James A.\n| author2-link =James A. Clarkson\n| title = On definitions of bounded variation for functions of two variables\n| journal = [[Transactions of the American Mathematical Society]]\n| volume = 35\n| pages = 824–854\n| year = 1933\n| url = http://www.ams.org/journals/tran/1933-035-04/S0002-9947-1933-1501718-2/home.html\n| doi = 10.1090/S0002-9947-1933-1501718-2\n| mr = 1501718 \n| zbl = 0008.00602\n| issue = 4\n}}.\n*{{Citation\n| last = Alberti\n| first = Giovanni\n| author-link =\n| last2 = Mantegazza\n| first2 = Carlo\n| author2-link =\n| title = A note on the theory of SBV functions\n| journal = [[Bollettino dell'Unione Matematica Italiana]]\n| series = IV Serie\n| volume = 11\n| issue = 2\n| pages = 375–382\n| date =\n| year = 1997\n| doi =\n| mr = 1459286\n| zbl = 0877.49001\n}}. In this paper, the authors prove the [[Compact space#Compactness of topological spaces|compactness]] of the space of SBV functions.\n*{{Citation\n| last = Ambrosio\n| first = Luigi\n| author-link = Luigi Ambrosio\n| last2 = Dal Maso\n| first2 = Gianni\n| author2-link =\n| title = A General Chain Rule for Distributional Derivatives\n| journal = [[Proceedings of the American Mathematical Society]]\n| volume = 108\n| issue = 3\n| pages = 691–691\n| year = 1990\n| url = http://www.ams.org/proc/1990-108-03/S0002-9939-1990-0969514-3/home.html\n| doi = 10.1090/S0002-9939-1990-0969514-3\n| mr = 969514 \n| zbl = 0685.49027\n}}. A paper containing a very general [[chain rule]] formula for [[Function composition|composition]] of BV functions.\n*{{Citation\n| last = Ambrosio\n| first = Luigi\n| author-link = Luigi Ambrosio\n| last2 = De Giorgi\n| first2 = Ennio\n| author2-link = Ennio de Giorgi\n| title = Un nuovo tipo di funzionale del calcolo delle variazioni\n|trans-title=A new kind of functional in the calculus of variations\n| journal = Atti della [[Accademia Nazionale dei Lincei]], Rendiconti della Classe di Scienze Fisiche, Matematiche e Naturali\n| language =Italian\n| series = VIII\n| volume = LXXXII\n| issue = 2\n| pages = 199–210\n| year = 1988\n| url=http://www.bdim.eu/item?id=RLIN_1988_8_82_2_199_0\n| mr = 1152641\n| zbl = 0715.49014\n}}. The first paper on {{math|''SBV''}} functions and related variational problems.\n*{{Citation\n| last = Cesari\n| first = Lamberto\n| author-link = Lamberto Cesari\n| title = Sulle funzioni a variazione limitata \n| journal = [[Annali della Scuola Normale Superiore]]\n| series = Serie II, \n| volume = 5\n| issue = 3–4\n| pages = 299–313\n| language = Italian\n| year = 1936\n| url = http://www.numdam.org/item?id=ASNSP_1936_2_5_3-4_299_0\n| doi = \n| mr = 1556778\n| zbl = 0014.29605\n}}. Available at [http://www.numdam.org Numdam]. In the paper \"''On the functions of bounded variation''\" (English translation of the title) Cesari he extends the now called ''[[Total variation#Tonelli plane variation|Tonelli plane variation]]'' concept to include in the definition a subclass of the class of integrable functions.\n*{{Citation\n| first = Lamberto\n| last = Cesari\n| author-link = Lamberto Cesari\n| editor-last = Montalenti\n| editor-first = G.\n| editor2-last = Amerio\n| editor2-first = L. \n| editor2-link = Luigi Amerio\n| editor3-last =Acquaro\n| editor3-first =G.\n| editor4-last = Baiada\n| editor4-first = E.\n| editor5-last = Cesari\n| editor5-first = L.\n| editor5-link =Lamberto Cesari\n| editor6-last = Ciliberto \n| editor6-first= C.\n| editor7-last = Cimmino \n| editor7-first = G.\n| editor7-link = Gianfranco Cimmino\n| editor8-last = Cinquini \n| editor8-first = S.\n| editor9-last = De Giorgi\n| editor9-first = E.\n| editor9-link = Ennio De Giorgi\n| editor10-last = Faedo\n| editor10-first = S.\n| editor10-link = Sandro Faedo\n| editor11-last = Fichera\n| editor11-first = G.\n| editor11-link = Gaetano Fichera \n| editor12-last = Galligani\n| editor12-first = I.\n| editor13-last = Ghizzetti\n| editor13-first = A.\n| editor13-link = Aldo Ghizzetti \n| editor14-last = Graffi\n| editor14-first = D.\n| editor14-link = Dario Graffi\n| editor15-last = Greco\n| editor15-first = D.\n| editor15-link = Donato Greco\n| editor16-last = Grioli\n| editor16-first = G.\n| editor16-link = Giuseppe Grioli\n| editor17-last = Magenes\n| editor17-first = E.\n| editor17-link = Enrico Magenes \n| editor18-last = Martinelli\n| editor18-first = E. \n| editor18-link = Enzo Martinelli \n| editor19-last = Pettineo\n| editor19-first = B.\n| editor20-last = Scorza\n| editor20-first = G. \n| editor20-link = Giuseppe Scorza Dragoni\n| editor21-last = Vesentini\n| editor21-first = E.\n| editor21-link = Edoardo Vesentini\n| display-editors =4\n| contribution = L'opera di Leonida Tonelli e la sua influenza nel pensiero scientifico del secolo \n| title = Convegno celebrativo del centenario della nascita di Mauro Picone e Leonida Tonelli (6–9 maggio 1985)\n| language = Italian\n| url = http://www.lincei.it/pubblicazioni/catalogo/volume.php?lg=e&rid=32847\n| series = Atti dei Convegni Lincei\n| volume = 77\n| year = 1986\n| pages = 41–73\n| place = Roma\n| publisher = [[Accademia Nazionale dei Lincei]]\n| doi = \n}}. \"''The work of Leonida Tonelli and his influence on scientific thinking in this century''\" (English translation of the title) is an ample commemorative article, reporting recollections of the Author about teachers and colleagues, and a detailed survey of his and theirs scientific work, presented at the ''International congress in occasion of the celebration of the centenary of birth of Mauro Picone and Leonida Tonelli'' (held in Rome on May 6–9, 1985).\n*{{Citation\n| last = Conway\n| first = Edward D.\n| author-link =\n| last2 = Smoller\n| first2 = Joel A.\n| author2-link =\n| title = Global solutions of the Cauchy problem for quasi–linear first–order equations in several space variables\n| journal = [[Communications on Pure and Applied Mathematics]]\n| volume = 19\n| issue = 1\n| pages = 95–105\n| year = 1966\n| doi = 10.1002/cpa.3160190107\n| mr = 0192161\n| zbl = 0138.34701\n}}. An important paper where properties of ''BV'' functions were applied to obtain a global in time [[existence theorem]] for ''single'' [[hyperbolic equation]]s of first order in any number of [[Variable (mathematics)|variables]].\n*{{Citation\n| first = Ennio\n| last = De Giorgi\n| author-link = Ennio de Giorgi\n| editor-last = Amaldi\n| editor-first = E.\n| editor-link = Edoardo Amaldi\n| editor2-last =Amerio\n| editor2-first =L. \n| editor2-link =Luigi Amerio \n| editor3-last =Fichera\n| editor3-first =G.\n| editor3-link =Gaetano Fichera\n| editor4-last =Gregory\n| editor4-first =T.\n| editor4-link =\n| editor5-last =Grioli\n| editor5-first =G.\n| editor5-link =Giuseppe Grioli\n| editor6-last =Martinelli\n| editor6-first =E.\n| editor6-link =Enzo Martinelli\n| editor7-last =Montalenti\n| editor7-first =G.\n| editor8-last =Pignedoli\n| editor8-first =A.\n| editor8-link =Antonio Pignedoli\n| editor9-last =Salvini\n| editor9-link =Giorgio Salvini\n| editor9-first =Giorgio\n| editor10-last =Scorza Dragoni\n| editor10-first =Giuseppe\n| editor10-link =Giuseppe Scorza Dragoni\n| contribution = Problemi variazionali con discontinuità libere <!-- |trans-title=Free-discontinuity variational problems -->\n| title = Convegno internazionale in memoria di Vito Volterra (8–11 ottobre 1990)\n| url = http://www.lincei.it/pubblicazioni/catalogo/volume.php?rid=32862\n| language = Italian\n| series = Atti dei Convegni Lincei\n| volume = 92\n| year = 1992\n| pages = 39–76\n| place = Roma\n| publisher = [[Accademia Nazionale dei Lincei]]\n| doi = \n| issn = 0391-805X\n| mr = 1783032\n| zbl =1039.49507\n}}. A survey paper on free-discontinuity [[calculus of variations|variational problems]] including several details on the theory of ''SBV'' functions, their applications and a rich bibliography.\n*{{Citation\n| last = Faleschini\n| first = Bruno\n| author-link =\n| title = Sulle definizioni e proprietà delle funzioni a variazione limitata di due variabili. Nota I.\n|trans-title=On the definitions and properties of functions of bounded variation of two variables. Note I\n| journal = [[Bollettino dell'Unione Matematica Italiana]]\n| series = Serie III\n| volume = 11\n| issue = 1\n| pages = 80–92\n| year = 1956a\n| language = Italian\n| url =http://www.bdim.eu/item?id=BUMI_1956_3_11_1_80_0\n| mr = 80169\n| zbl = 0071.27901\n}}. The first part of a survey of many different definitions of \"''Total variation''\" and associated functions of bounded variation.  \n*{{Citation\n| last = Faleschini\n| first = Bruno\n| author-link =\n| title = Sulle definizioni e proprietà delle funzioni a variazione limitata di due variabili. Nota II.\n|trans-title=On the definitions and properties of functions of bounded variation of two variables. Note I\n| journal = [[Bollettino dell'Unione Matematica Italiana]]\n| series = Serie III\n| volume = 11\n| issue = 2\n| pages = 260–75\n| year = 1956b\n| language =Italian\n| url =http://www.bdim.eu/item?id=BUMI_1956_3_11_2_260_0\n| mr = 80169\n| zbl = 0073.04501\n}}. The second part of a survey of many different definitions of \"''Total variation''\" and associated functions of bounded variation.\n*{{Citation\n| last = Jordan\n| first = Camille\n| author-link = Camille Jordan\n| title = Sur la série de Fourier\n|trans-title=On Fourier's series\n| journal = [[Comptes rendus hebdomadaires des séances de l'Académie des sciences]]\n| volume = 92\n| pages = 228–230\n| year = 1881\n| url = http://gallica.bnf.fr/ark:/12148/bpt6k7351t/f227.chemindefer\n}} (at [[Gallica]]). This is, according to Boris Golubov, the first paper on functions of bounded variation.\n*{{Citation\n| last = Oleinik\n| first = Olga A.\n| author-link = Olga Arsenievna Oleinik\n| title = Discontinuous solutions of non-linear differential equations\n| journal = [[Uspekhi Matematicheskikh Nauk]]\n| volume = 12\n| issue = 3(75)\n| pages = 3–73\n| year = 1957\n| url = http://mi.mathnet.ru/eng/umn/v12/i3/p3\n| zbl = 0080.07701\n}} ({{ru icon}}). An important paper where the author describes generalized solutions of [[nonlinear equation|nonlinear]] [[partial differential equation]]s as {{math|''BV''}} functions.\n*{{Citation\n| last = Oleinik\n| first = Olga A.\n| author-link = Olga Arsenievna Oleinik\n| title = Construction of a generalized solution of the Cauchy problem for a quasi-linear equation of first order by the introduction of \"vanishing viscosity\"\n| journal = [[Uspekhi Matematicheskikh Nauk]]\n| volume = 14\n| issue = 2(86)\n| pages = 159–164\n| year = 1959\n| url = http://mi.mathnet.ru/eng/umn/v14/i2/p159\n| zbl = 0096.06603\n}} ({{ru icon}}). An important paper where the author constructs a [[weak solution]] in ''BV'' for a [[nonlinear equation|nonlinear]] [[partial differential equation]] with the method of [[vanishing viscosity]].\n*[[Tony F. Chan]] and [https://sites.google.com/view/jackieshen/ Jianhong (Jackie) Shen] (2005), [https://web.archive.org/web/20080117220948/http://jackieneoshen.googlepages.com/ImagingNewEra.html ''Image Processing and Analysis - Variational, PDE, Wavelet, and Stochastic Methods''], SIAM Publisher, {{ISBN|0-89871-589-X}} (with in-depth coverage and extensive applications of Bounded Variations in modern image processing, as started by Rudin, Osher, and Fatemi).\n{{refend}}\n\n==External links==\n\n=== Theory ===\n* {{springer\n| title= Variation of a function\n| id= V/v096110\n| last= Golubov\n| first= Boris I.\n| author-link=\n| last2= Vitushkin\n| first2= Anatolii G.\n| author2-link= Anatolii Georgievich Vitushkin\n}}\n*{{planetmath reference|id=6969|title=BV function}}.\n*{{MathWorld\n|author=Rowland, Todd and Weisstein, Eric W.\n|title=Bounded Variation\n|urlname=BoundedVariation}}\n*[http://www.encyclopediaofmath.org/index.php/Function_of_bounded_variation Function of bounded variation] at [http://www.encyclopediaofmath.org/ Encyclopedia of Mathematics]\n\n===Other===\n* Luigi Ambrosio [http://cvgmt.sns.it/people/ambrosio/ home page] at the [[Scuola Normale Superiore di Pisa]]. Academic home page (with preprints and publications) of one of the contributors to the theory and applications of BV functions.\n* [http://cvgmt.sns.it/ Research Group in Calculus of Variations and Geometric Measure Theory], [[Scuola Normale Superiore di Pisa]].\n\n{{PlanetMath attribution|id=6969|title=BV function}}\n\n{{DEFAULTSORT:Bounded Variation}}\n[[Category:Real analysis]]\n[[Category:Calculus of variations]]\n[[Category:Measure theory]]"
    },
    {
      "title": "Caccioppoli set",
      "url": "https://en.wikipedia.org/wiki/Caccioppoli_set",
      "text": "In [[mathematics]], a '''Caccioppoli set''' is a [[Set (mathematics)|set]] whose [[boundary (topology)|boundary]] is [[measurable set|measurable]] and has (at least [[Local property|locally]]) a ''finite [[measure (mathematics)|measure]]''. A synonym is '''set of (locally) finite perimeter'''. Basically, a set is a Caccioppoli set if its [[Indicator function|characteristic function]] is a [[function of bounded variation]].\n\n== History ==\nThe basic concept of a Caccioppoli set was firstly introduced by the Italian mathematician [[Renato Caccioppoli]] in the paper {{Harv|Caccioppoli|1927}}: considering a plane set or a [[Surface (topology)|surface]] defined on an [[open set]] in the [[Plane (mathematics)|plane]], he defined their [[measure (mathematics)|measure]] or [[Area (geometry)|area]] as the [[total variation]] in the sense of [[Leonida Tonelli|Tonelli]] of their defining [[function (mathematics)|functions]], i.e. of their [[Surface (topology)#Extrinsically defined surfaces and embeddings|parametric equation]]s, ''provided this quantity was [[bounded variation|bounded]]''. The ''measure of the [[boundary (topology)|boundary of a set]] was defined as a '''[[functional (mathematics)|functional]]''''', precisely a [[set function]], for the first time: also, being defined on [[open set]]s, it can be defined on all [[Borel set]]s and its value can be approximated by the values it takes  on an increasing [[Net (mathematics)|net]] of [[subset]]s. Another clearly stated (and demonstrated) property of this functional was its ''[[semi-continuity|lower semi-continuity]]''.\n\nIn the paper {{Harv|Caccioppoli|1928}}, he precised  by using a ''[[Polygon mesh|triangular mesh]]'' as an increasing [[Net (mathematics)|net]] approximating the open domain, defining ''positive and negative variations'' whose sum is the total variation, i.e. the ''area functional''. His inspiring point of view, as he explicitly admitted, was those of [[Giuseppe Peano]], as expressed by the [[Jordan measure|Peano-Jordan Measure]]: ''to associate to every portion of a surface an [[Orientation (mathematics)|oriented]] plane area in a similar way as an [[Chord (geometry)|approximating chord]] is associated to a curve''. Also, another theme found in this theory was the ''extension of a [[functional (mathematics)|functional]]'' from a [[Linear subspace|subspace]] to the whole [[linear space|ambient space]]: the use of theorems generalizing the [[Hahn–Banach theorem]] is frequently encountered in Caccioppoli research. However, the restricted meaning of [[total variation]] in the sense of [[Leonida Tonelli|Tonelli]] added much complication to the formal development of the theory, and the use of a parametric description of the sets restricted its scope.\n\n[[Lamberto Cesari]] introduced the \"right\" generalization of [[Bounded variation|functions of bounded variation]] to the case of several variables only in 1936:<ref>In the paper {{harv|Cesari|1936}}. See the entries \"[[Bounded variation]]\" and \"[[Total variation]]\" for more details.</ref> perhaps, this was one of the reasons that induced Caccioppoli to present an improved version of his theory only nearly 24 years later, in the talk {{Harv|Caccioppoli|1953}} at the IV [[Italian Mathematical Union|UMI]] Congress in October 1951, followed by five notes published in the [http://www.lincei.it/pubblicazioni/rendicontiFMN/inizio_eng.html Rendiconti] of the [[Accademia Nazionale dei Lincei]]. These notes were sharply criticized by [[Laurence Chisholm Young]] in the [[Mathematical Reviews]].<ref>See {{MR|56067}}.</ref>\n\nIn 1952 [[Ennio de Giorgi]] presented his first results, developing the ideas of Caccioppoli, on the definition of the measure of boundaries of sets at the [[Salzburg]] Congress of the Austrian Mathematical Society: he obtained this results by using a smoothing operator, analogous to a [[mollifier]], constructed from the [[Gaussian function]], independently proving some results of Caccioppoli. Probably he was led to study this theory by his teacher and friend [[Mauro Picone]], who had also been the teacher of Caccioppoli and was likewise his friend. De Giorgi met Caccioppoli in 1953 for the first time: during their meeting, Caccioppoli expressed a profound appreciation of his work, starting their lifelong friendship.<ref>It lasted up to the tragic death of Caccioppoli in 1959.</ref> The same year he published his first paper on the topic i.e. {{Harv|De Giorgi|1953}}: however, this paper and the closely following one did not attracted much interest from the mathematical community. It was only with the paper {{Harv|De Giorgi|1954}}, reviewed again by Laurence Chisholm Young in the Mathematical Reviews,<ref>See {{MR|0062214}}.</ref> that his approach to sets of finite perimeter became widely known and appreciated: also, in the review, Young revised  his previous criticism on the work of Caccioppoli.\n\nThe last paper of De Giorgi on the theory of [[perimeter]]s was published in 1958: in 1959, after the death of Caccioppoli, he started to call sets of finite perimeter \"Caccioppoli sets\". Two years later [[Herbert Federer]] and [[Wendell Fleming]] published their paper {{Harv|Federer|Fleming|1960}}, changing the approach to the theory. Basically they introduced two new kind of [[current (mathematics)|currents]], respectively [[normal current]]s and [[integral current]]s: in a subsequent series of papers and in his famous treatise,<ref>See {{Harv|Federer|1969}}.</ref> Federer showed that Caccioppoli sets are normal [[current (mathematics)|currents]] of dimension <math>n</math> in <math>n</math>-dimensional [[euclidean space]]s. However, even if the theory of Caccioppoli sets can be studied within the framework of theory of [[current (mathematics)|currents]], it is customary to study it through the \"traditional\" approach using [[Bounded variation|functions of bounded variation]], as the various sections found in a lot of important [[monograph]]s in [[mathematics]] and [[mathematical physics]] testify.<ref>See the \"[[Caccioppoli set#References|References]]\" section.</ref>\n\n== Formal definition ==\nIn what follows, the definition and properties of [[Bounded variation|functions of bounded variation]] in the <math>n</math>-dimensional setting will be used.\n\n=== Caccioppoli definition ===\n'''Definition 1'''. Let ''<math>\\Omega</math>'' be an [[open subset]] of <math>\\R^n </math> and let <math>E</math> be a [[Borel set]]. The ''[[perimeter]] of <math>E</math> in <math>\\Omega</math>'' is defined as follows\n\n:<math>P(E,\\Omega) = V\\left(\\chi_E,\\Omega\\right):=\\sup\\left\\{\\int_\\Omega \\chi_E(x) \\mathrm{div}\\boldsymbol{\\phi}(x) \\, \\mathrm{d}x : \\boldsymbol{\\phi}\\in C_c^1(\\Omega,\\R^n),\\ \\|\\boldsymbol{\\phi}\\|_{L^\\infty(\\Omega)}\\le 1\\right\\}\n</math>\n\nwhere <math>\\chi_E</math> is the [[Indicator function|characteristic function]] of <math>E</math>. That is, the perimeter of <math>E</math> in an open set <math>\\Omega</math> is defined to be the [[total variation]] of its [[Indicator function|characteristic function]] on that open set. If <math>\\Omega = \\R^n</math>, then we write <math>P(E) = P(E,\\R^n)</math> for the (global) perimeter.\n\n'''Definition 2'''. The [[Borel set]] <math>E</math> is a '''Caccioppoli set''' if and only if it has finite perimeter in every [[bounded set|bounded]] [[open subset]] <math>\\Omega</math> of <math>\\R ^n </math>, i.e.\n\n:<math>P(E,\\Omega)<+\\infty</math> whenever <math>\\Omega \\subset \\R^n</math> is open and bounded.\n\nTherefore, a Caccioppoli set has a [[Indicator function|characteristic function]] whose [[total variation]] is locally bounded. From the theory of [[Bounded variation|functions of bounded variation]] it is known that this implies the existence of a [[Euclidean vector|vector-valued]] [[Radon measure]] <math>D\\chi_E</math> such that\n\n:<math>\\int_\\Omega\\chi_E(x)\\mathrm{div}\\boldsymbol{\\phi}(x)\\mathrm{d}x = \\int_E\\mathrm{div}\\boldsymbol{\\phi}(x) \\, \\mathrm{d}x = -\\int_\\Omega \\langle\\boldsymbol{\\phi}, D\\chi_E(x)\\rangle \\qquad \\forall\\boldsymbol{\\phi}\\in C_c^1(\\Omega,\\R ^n)</math>\n\nAs noted for the case of general [[Bounded variation|functions of bounded variation]], this vector [[measure (mathematics)|measure]] <math>D\\chi_E</math> is the [[Distribution (mathematics)#Formal definition|distributional]] or [[weak derivative|weak]] [[gradient]] of <math>\\chi_E</math>. The total variation measure associated with <math>D\\chi_E</math> is denoted by <math>|D\\chi_E|</math>, i.e. for every open set <math>\\Omega \\subset \\R^n</math> we write <math>|D\\chi_E|(\\Omega)</math> for <math>P(E, \\Omega) = V(\\chi_E, \\Omega)</math>.\n\n=== De Giorgi definition ===\nIn his papers {{Harv|De Giorgi|1953}} and {{Harv|De Giorgi|1954}}, [[Ennio de Giorgi]] introduces the following [[smoothing operator]], analogous to the [[Weierstrass transform]] in the one-[[Dimension (mathematics)|dimensional]] case\n\n:<math>W_\\lambda\\chi_E(x)=\\int_{\\R ^n}g_\\lambda(x-y)\\chi_E(y)\\mathrm{d}y = (\\pi\\lambda)^{-\\frac{n}{2}}\\int_Ee^{-\\frac{(x-y)^2}{\\lambda}}\\mathrm{d}y</math>\n\nAs one can easily prove, <math>W_\\lambda\\chi(x)</math> is a [[smooth function]] for all <math>x\\in\\R^n</math>, such that\n\n:<math>\\lim_{\\lambda\\to 0}W_\\lambda\\chi_E(x)=\\chi_E(x)</math>\n\nalso, its [[gradient]] is everywhere well defined, and so is its [[absolute value]]\n\n:<math>\\nabla W_\\lambda\\chi_E(x) = \\mathrm{grad}W_\\lambda\\chi_E(x) = DW_\\lambda\\chi_E(x) = \n\\begin{pmatrix}\\frac{\\partial W_\\lambda\\chi_E(x)}{\\partial x_1}\\\\ \\vdots\\\\ \\frac{\\partial W_\\lambda\\chi_E(x)}{\\partial x_n}\\\\ \\end{pmatrix} \n\\Longleftrightarrow\n\\left | DW_\\lambda\\chi_E(x)\\right | = \\sqrt{\\sum_{k=1}^n\\left|\\frac{\\partial W_\\lambda\\chi_E(x)}{\\partial x_k}\\right|^2}</math>\n\nHaving defined this function, De Giorgi gives the following definition of [[perimeter]]:\n\n'''Definition 3'''. Let '''<math> \\Omega </math>''' be an [[open subset]] of <math>\\R^n</math> and let <math>E</math> be a [[Borel set]]. The ''[[perimeter]] of <math>E</math> in <math>\\Omega</math>'' is the value\n\n:<math>P(E,\\Omega) = \\lim_{\\lambda\\to 0}\\int_\\Omega | DW_\\lambda\\chi_E(x) | \\mathrm{d}x</math>\n\nActually De Giorgi considered the case <math>\\Omega=\\R ^n</math>: however, the extension to the general case is not difficult. It can be proved that the two definitions are exactly equivalent: for a proof see the already cited De Giorgi's papers or the book {{Harv|Giusti|1984}}. Now having defined what a perimeter is, De Giorgi gives the same definition 2 of what a set of [[Local property|(locally) finite]] perimeter is.\n\n== Basic properties ==\n\nThe following properties are the ordinary properties which the general notion of a [[perimeter]] is supposed to have:\n\n* If <math>\\Omega\\subseteq\\Omega_1</math> then <math>P(E,\\Omega)\\leq P(E,\\Omega_1)</math>, with equality holding if and only if the [[Closure (topology)|closure]] of <math>E</math> is a compact subset of <math>\\Omega</math>.\n* For any two Cacciopoli sets <math>E_1</math> and <math>E_2</math>, the relation <math>P(E_1\\cup E_2,\\Omega)\\leq P(E_1,\\Omega) + P(E_2,\\Omega_1)</math> holds, with equality holding if and only if <math>d(E_1,E_2)>0</math>, where <math>d</math> is the [[Distance#Distances between sets and between a point and a set|distance between sets]] in [[euclidean space]].\n* If the [[Lebesgue measure]] of <math>E</math> is <math>0</math>, then <math>P(E)=0</math>: this implies that if the [[symmetric difference]] <math>E_1\\triangle E_2</math> of two sets has zero Lebesgue measure, the two sets have the same perimeter i.e. <math>P(E_1)=P(E_2)</math>.\n\n== Notions of boundary ==\n\nFor any given Caccioppoli set <math>E \\subset \\R ^n</math> there exist two naturally associated analytic quantities: the vector-valued [[Radon measure]] <math>D\\chi_E</math> and its [[Total variation#Total variation in measure theory|total variation measure]] <math>|D\\chi_E|</math>. Given that\n\n:<math> P(E, \\Omega) = \\int_{\\Omega} |D\\chi_E| </math>\n\nis the perimeter within any open set <math>\\Omega</math>, one should expect that <math>D\\chi_E</math> alone should somehow account for the perimeter of <math>E</math>.\n\n=== The topological boundary ===\n\nIt is natural to try to understand the relationship between the objects <math>D\\chi_E</math>, <math>|D\\chi_E|</math>, and the [[Boundary (topology)|topological boundary]] <math>\\partial E</math>. There is an elementary lemma that guarantees that the [[Distribution (mathematics)#Support of a distribution|support]] (in the sense of [[Distribution (mathematics)|distributions]]) of <math>D\\chi_E</math>, and therefore also <math>|D\\chi_E|</math>, is always '''contained''' in <math>\\partial E</math>:\n\n'''Lemma'''. The support of the vector-valued Radon measure <math>D\\chi_E</math> is a [[subset]] of the [[Boundary (topology)|topological boundary]] <math>\\partial E</math> of <math>E</math>.\n\n'''Proof'''. To see this choose <math>x_0 \\notin\\partial E</math>: then <math>x_0</math> belongs to the [[open set]] <math>\\R ^n\\setminus\\partial E</math> and this implies that it belongs to an [[open neighborhood]] <math>A</math> contained in the [[Interior (topology)|interior]] of <math>E</math> or in the interior of <math>\\R^n\\setminus E</math>. Let <math>\\phi \\in C^1_c(A; \\R ^n)</math>. If <math>A\\subseteq(\\R^n \\setminus E)^\\circ=\\R^n\\setminus E^-</math> where <math>E^-</math> is the [[Closure (topology)|closure]] of <math>E</math>, then <math>\\chi_E(x)=0</math> for <math>x \\in A</math> and\n\n:<math> \\int_\\Omega \\langle\\boldsymbol{\\phi}, D\\chi_E(x)\\rangle =- \\int_A\\chi_E(x) \\, \\operatorname{div}\\boldsymbol{\\phi}(x)\\, \\mathrm{d}x = 0</math>\n\nLikewise, if <math>A\\subseteq E^\\circ </math> then <math>\\chi_E(x)=1</math> for <math>x \\in A</math> so\n\n:<math>\\int_\\Omega \\langle\\boldsymbol{\\phi}, D\\chi_E(x)\\rangle = -\\int_A\\operatorname{div} \\boldsymbol{\\phi}(x) \\, \\mathrm{d}x = 0\n</math>\n\nWith <math>\\phi \\in C^1_c(A, \\R^n)</math> arbitrary it follows that <math>x_0</math> is outside the support of <math>D\\chi_E</math>.\n\n=== The reduced boundary ===\n\nThe topological boundary <math>\\partial E</math> turns out to be too crude for Caccioppoli sets because its [[Hausdorff measure]] overcompensates for the perimeter <math>P(E)</math> defined above. Indeed, the Caccioppoli set\n\n:<math>E = \\{ (x,y) : 0 \\leq x, y \\leq 1 \\} \\cup \\{ (x, 0) : -1 \\leq x \\leq 1 \\} \\subset \\R^2 </math>\n\nrepresenting a square together with a line segment sticking out on the left has perimeter <math>P(E) = 4</math>, i.e. the extraneous line segment is ignored, while its topological boundary\n\n:<math>\\partial E = \\{(x, 0) : -1 \\leq x \\leq 1 \\} \\cup \\{(x, 1) : 0 \\leq x \\leq 1 \\} \\cup \\{(x, y) : x \\in \\{0, 1\\}, 0 \\leq y \\leq 1 \\} </math>\n\nhas one-dimensional Hausdorff measure <math>\\mathcal{H}^1(\\partial E) = 5</math>.\n\nThe \"correct\" boundary should therefore be a subset of <math>\\partial E</math>. We define:\n\n'''Definition 4'''. The '''reduced boundary''' of a Caccioppoli set <math>E \\subset \\R ^n</math> is denoted by <math>\\partial^* E</math> and is defined to be equal to be the collection of points <math>x</math> at which the limit:\n\n:<math> \\nu_E(x) := \\lim_{\\rho \\downarrow 0} \\frac{D\\chi_E(B_\\rho(x))}{|D\\chi_E|(B_\\rho(x))} \\in \\R^n</math>\n\nexists and has length equal to one, i.e. <math>|\\nu_E(x)| = 1</math>.\n\nOne can remark that by the [[Radon-Nikodym Theorem]] the reduced boundary <math>\\partial^* E</math> is necessarily contained in the support of <math>D\\chi_E</math>, which in turn is contained in the topological boundary <math>\\partial E</math> as explained in the section above. That is:\n\n:<math>\\partial^* E \\subseteq \\operatorname{support} D\\chi_E \\subseteq \\partial E</math>\n\nThe inclusions above are not necessarily equalities as the previous example shows. In that example, <math>\\partial E</math> is the square with the segment sticking out, <math>\\operatorname{support} D\\chi_E</math> is the square, and <math>\\partial^* E</math> is the square without its four corners.\n\n=== De Giorgi's theorem ===\n\nFor convenience, in this section we treat only the case where <math>\\Omega = \\R ^n</math>, i.e. the set <math>E</math> has (globally) finite perimeter. De Giorgi's theorem provides geometric intuition for the notion of reduced boundaries and confirms that it is the more natural definition for Caccioppoli sets by showing\n\n:<math> P(E) \\left( = \\int |D\\chi_E| \\right) = \\mathcal{H}^{n-1}(\\partial^* E)</math>\n\ni.e. that its [[Hausdorff measure]] equals the perimeter of the set. The statement of the theorem is quite long because it interrelates various geometric notions in one fell swoop.\n\n'''Theorem'''. Suppose <math>E \\subset \\R^n</math> is a Caccioppoli set. Then at each point <math>x</math> of the reduced boundary <math>\\partial^* E</math> there exists a multiplicity one [[approximate tangent space]] <math>T_x</math> of <math>|D\\chi_E|</math>, i.e. a codimension-1 subspace <math>T_x</math> of <math>\\R ^n</math> such that\n\n:<math> \\lim_{\\lambda \\downarrow 0} \\int_{\\R^n} f(\\lambda^{-1}(z-x)) |D\\chi_E|(z) = \\int_{T_x} f(y) \\, d\\mathcal{H}^{n-1}(y)</math>\n\nfor every continuous, compactly supported <math>f : \\R ^n \\to \\R </math>. In fact the subspace <math>T_x</math> is the [[orthogonal complement]] of the unit vector\n\n:<math>\\nu_E(x) = \\lim_{\\rho \\downarrow 0} \\frac{D\\chi_E(B_\\rho(x))}{|D\\chi_E|(B_\\rho(x))} \\in \\R ^n</math>\n\ndefined previously. This unit vector also satisfies\n\n:<math>\\lim_{\\lambda \\downarrow 0} \\left \\{ \\lambda^{-1}(z - x) : z \\in E \\right \\} \\to \\left \\{ y \\in \\R^n : y \\cdot \\nu_E(x) > 0 \\right \\}</math>\n\nlocally in <math>L^1</math>, so it is interpreted as an approximate inward pointing [[Unit vector|unit]] [[Normal (geometry)|normal vector]] to the reduced boundary <math>\\partial^* E</math>. Finally, <math>\\partial^* E</math> is (n-1)-[[Rectifiable set|rectifiable]] and the restriction of (n-1)-dimensional [[Hausdorff measure]] <math>\\mathcal{H}^{n-1}</math> to <math>\\partial^* E</math> is <math>|D\\chi_E|</math>, i.e.\n\n:<math>|D\\chi_E|(A) = \\mathcal{H}^{n-1}(A \\cap \\partial^* E)</math> for all Borel sets <math>A \\subset \\R^n</math>.\n\nIn other words, up to <math>\\mathcal{H}^{n-1}</math>-measure zero the reduced boundary <math>\\partial^* E</math> is the smallest set on which <math>D\\chi_E</math> is supported.\n\n== Applications ==\n\n=== A Gauss–Green formula ===\nFrom the definition of the vector [[Radon measure]] <math>D\\chi_E</math> and from the properties of the perimeter, the following formula holds true:\n\n:<math>\\int_E\\operatorname{div}\\boldsymbol{\\phi}(x) \\, \\mathrm{d}x = -\\int_{\\partial E} \\langle\\boldsymbol{\\phi}, D\\chi_E(x)\\rangle \n\\qquad \\boldsymbol{\\phi}\\in C_c^1(\\Omega, \\R^n)</math>\n\nThis is one version of the [[divergence theorem]] for [[Domain (mathematics)#Real and complex analysis|domains]] with non smooth [[boundary (topology)|boundary]]. De Giorgi's theorem can be used to formulate the same identity in terms of the reduced boundary <math>\\partial^* E</math> and the approximate inward pointing unit normal vector <math>\\nu_E</math>. Precisely, the following equality holds\n\n:<math>\\int_E \\operatorname{div} \\boldsymbol{\\phi}(x) \\, \\mathrm{d}x = - \\int_{\\partial^* E} \\boldsymbol{\\phi}(x) \\cdot \\nu_E(x) \\, \\mathrm{d}\\mathcal{H}^{n-1}(x) \\qquad \\boldsymbol{\\phi} \\in C^1_c(\\Omega, \\R^n)</math>\n\n== See also ==\n{{div col}}\n* [[Geometric measure theory]]\n* [[Divergence theorem]]\n* [[Pfeffer integral]]\n{{div col end}}\n\n== Notes ==\n{{reflist|29em}}\n\n==References==\n{{refbegin}}\n\n===Historical references===\n*{{Citation\n|last = Ambrosio\n|first = Luigi\n|author-link = Luigi Ambrosio\n|title = La teoria dei perimetri di Caccioppoli–De Giorgi e i suoi più recenti sviluppi\n|trans-title=The De Giorgi-Caccioppoli theory of perimeters and its most recent developments\n|journal = [[Rendiconti Lincei - Matematica e Applicazioni]]\n|volume = 21\n|series = 9\n|issue = 3\n|pages = 275–286\n|date = \n|year = 2010\n|doi = 10.4171/RLM/572\n|mr = 2677605 \n|zbl = 1195.49052\n\n}}. A paper surveying the history of the theory of sets of finite perimeter, from the seminal paper of [[Renato Caccioppoli]] and the contributions of [[Ennio De Giorgi]] to some more recent developments and open problems in metric measure spaces, in Carnot groups and in infinite-dimensional Gaussian spaces.\n*{{Citation\n|last = Caccioppoli\n|first = Renato\n|author-link = Renato Caccioppoli\n|title = Sulla quadratura delle superfici piane e curve\n|trans-title=On the quadrature of plane and curved surfaces\n|journal = [[Atti della Accademia Nazionale dei Lincei. Rendiconti. Classe di Scienze Fisiche, Matematiche e Naturali]]\n|series = VI\n|language = Italian\n|volume = 6\n|issue = \n|pages = 142–146\n|year = 1927\n|jfm = 53.0214.02\n}}. The first paper containing the seminal concept of what a Caccioppoli set is.\n*{{Citation\n|last = Caccioppoli\n|first = Renato\n|author-link = Renato Caccioppoli\n|title = Sulle coppie di funzioni a variazione limitata\n|trans-title=On pairs of functions of bounded variation\n|journal = Rendiconti dell'Accademia di Scienze Fisiche e Matematiche di Napoli\n|language = Italian\n|volume = 34 \n|series = 3\n|pages = 83–88\n|year = 1928\n|jfm = 54.0290.04\n}}. The work where Caccioppoli made rigorous and developed the concepts introduced in the preceding paper {{harv|Caccioppoli|1927}}.\n*{{Citation\n|last = Caccioppoli\n|first = Renato\n|author-link = Renato Caccioppoli\n|contribution = Elementi di una teoria generale dell’integrazione {{mvar|k}}-dimensionale in uno spazio {{mvar|n}}-dimensionale \n|trans-title=Elements of a general theory of {{mvar|k}}-dimensional integration in a {{mvar|n}}-dimensional space\n|language = Italian\n|title = Atti IV Congresso U.M.I., [[Taormina]], October 1951\n|year = 1953\n|volume = 2\n|pages = 41–49\n|place = [[Rome|Roma]]\n|publisher = Edizioni Cremonese (distributed by [[Unione Matematica Italiana]])\n|doi = \n|mr = 0056067 \n|zbl = 0051.29402\n}}.The first paper detailing the theory of finite perimeter set in a fairly complete setting.\n*{{Citation\n|last = Caccioppoli\n|first = Renato\n|author-link = Renato Caccioppoli\n|title = Opere scelte\n|trans-title=Selected papers\n|place = [[Rome|Roma]]\n|publisher = Edizioni Cremonese (distributed by [[Unione Matematica Italiana]])\n|year = 1963\n|pages = XXX+434 (vol. 1), 350 (vol. 2)\n|doi = \n|mr = \n|zbl = 0112.28201\n|isbn = 88-7083-505-7 \n}}. A selection from Caccioppoli's scientific works with a biography and a commentary of [[Mauro Picone]].\n*{{Citation\n|last = Cesari\n|first = Lamberto\n|title = Sulle funzioni a variazione limitata\n|trans-title=On the functions of bounded variation\n|journal = [[Annali della Scuola Normale Superiore]]\n|series = Serie II, \n|volume = 5\n|issue = 3–4\n|pages = 299–313\n|date = \n|language = Italian\n|year = 1936\n|url = http://www.numdam.org/item?id=ASNSP_1936_2_5_3-4_299_0\n|doi = \n|mr = 1556778\n|zbl = 0014.29605\n}}. Available at [http://www.numdam.org Numdam]. Cesari's watershed paper, where he extends the now called ''[[Total variation#Tonelli plane variation|Tonelli plane variation]]'' concept to include in the definition a subclass of the class of integrable functions.\n*{{Citation\n|last = De Giorgi\n|first = Ennio\n|author-link = \n|title = Definizione ed espressione analitica del perimetro di un insieme \n|trans-title=Definition and analytical expression of the perimeter of a set\n|journal = [[Atti della Accademia Nazionale dei Lincei. Rendiconti. Classe di Scienze Fisiche, Matematiche e Naturali]]\n|series = VIII\n|language = Italian\n|volume = 14\n|pages = 390–393\n|year = 1953\n|doi = \n|mr = 0056066\n|zbl =0051.29403\n}}. The first note published by De Giorgi describing his approach to Caccioppoli sets.\n*{{Citation\n|last = De Giorgi\n|first = Ennio\n|author-link =  \n|title = Su una teoria generale della misura {{math|(''r''-1)}}-dimensionale in uno spazio ad {{mvar|r}} dimensioni \n|trans-title=On a general theory of {{math|(''r''-1)}}-dimensional measure in {{mvar|r}}-dimensional space\n|journal = Annali di Matematica Pura ed Applicata\n|language = Italian\n|series = Serie IV,\n|volume = 36\n|issue = 1\n|pages = 191–213\n|year = 1954\n|url = http://www.springerlink.com/content/6452044r330m1739/?p=366525395f0443be9a9565849f3fa2f4&pi=0\n|doi = 10.1007/BF02412838\n|mr = 0062214 \n|zbl = 0055.28504\n|subscription=yes\n}}. The first complete exposition by De Giorgi of the theory of Caccioppoli sets.\n*{{Citation\n|last =  Federer\n|first = Herbert\n|author-link = Herbert Federer\n|last2 = Fleming\n|first2 = Wendell H.\n|author2-link = \n|title = Normal and integral currents\n|journal = [[Annals of Mathematics]]\n|series = Series II,\n|volume = 72\n|issue = 4\n|pages = 458–520\n|year = 1960\n|doi = 10.2307/1970227\n|mr = 0123260 \n|zbl = 0187.31301\n|jstor =1970227\n}}. The first paper of Herbert Federer illustrating his approach to the theory of perimeters based on the theory of currents.\n*{{Citation\n|last = Miranda\n|first = Mario\n|author-link = \n|title = Caccioppoli sets\n|journal = [[Atti della Accademia Nazionale dei Lincei, Rendiconti Lincei, Matematica e Applicazioni]]\n|series = IX \n|volume = 14\n|issue = 3\n|pages = 173–177\n|year = 2003\n|url = http://www.lincei.it/pubblicazioni/rendicontiFMN/rol/visabs.php?lang=en&type=mat&fileId=234 \n|mr = 2064264 \n|zbl = 1072.49030\n}}. A paper sketching the history of the theory of sets of finite perimeter, from the seminal paper of [[Renato Caccioppoli]] to main discoveries.\n\n===Scientific references===\n*{{Citation\n|last = De Giorgi\n|first = Ennio\n|author-link = Ennio de Giorgi\n|last2 = Colombini\n|first2 = Ferruccio\n|author2-link = \n|last3 = Piccinini\n|first3 = Livio\n|title = Frontiere orientate di misura minima e questioni collegate\n|trans-title=Oriented boundaries of minimal measure and related questions\n|language = Italian\n|place = [[Pisa]]\n|publisher = Edizioni della Normale\n|year = 1972\n|series = Quaderni\n|page = 180\n|mr = 493669 \n|zbl = 0296.49031\n}}. An advanced text, oriented towards the theory of [[minimal surface]]s in the multi-dimensional setting, written by one of the leading contributors.\n*{{citation\n|last = Federer\n|first = Herbert\n|authorlink = Herbert Federer\n|title = Geometric measure theory\n|publisher = [[Springer-Verlag]] New York Inc.\n|series = Classics in Mathematics\n|volume = \n|location = [[Berlin]]-[[Heidelberg]]-[[New York City]]\n|year = 1996\n|origyear = 1969\n|pages = xiv+676\n|isbn = 3-540-60656-4\n|mr= 0257325\n|zbl = 0176.00801\n}}, particularly chapter 4, paragraph 4.5, sections 4.5.1 to 4.5.4 \"''Sets with locally finite perimeter''\". The absolute reference text in [[geometric measure theory]].\n*{{citation\n|last=Simon\n|first=Leon\n|authorlink = Leon Simon\n|title=Lectures on Geometric Measure Theory\n|publisher=Australian National University\n|series=Proceedings of the Centre for Mathematical Analysis\n|year=1983\n|volume=3\n}}, particularly Chapter 3, Section 14 \"''Sets of Locally Finite Perimeter''\".\n*{{Citation\n|last = Giusti\n|first = Enrico\n|author-link = Enrico Giusti\n|title = Minimal surfaces and functions of bounded variations\n|place = [[Basel]]-[[Boston]]-[[Stuttgart]]\n|publisher = [[Birkhäuser Verlag]]\n|year = 1984 \n|series = Monographs in Mathematics\n|volume = 80\n|pages = xii+240\n|url = https://books.google.com/books?id=dNgsmArDoeQC&printsec=frontcover&dq=Minimal+surfaces+and+functions+of+bounded+variations\n|mr = 0775682 \n|zbl = 0545.49018\n|isbn = 0-8176-3153-4}},  particularly part I, chapter 1 \"''Functions of bounded variation and Caccioppoli sets''\". A good reference on the theory of Caccioppoli sets and their application to the [[Minimal surface]] problem.\n*{{Citation\n|last = Hudjaev\n|first = Sergei Ivanovich\n|author-link = \n|last2 = Vol'pert\n|first2 = Aizik Isaakovich\n|author2-link = Aizik Isaakovich Vol'pert\n|title = Analysis in classes of discontinuous functions and equations of mathematical physics\n|place =  Dordrecht-Boston-Lancaster\n|publisher = Martinus Nijhoff Publishers\n|year = 1985\n|series = Mechanics: analysis \n|volume = 8\n|pages = xviii+678 \n|url = https://books.google.com/books?id=lAN0b0-1LIYC&printsec=frontcover&dq=%22Analysis+in+classes+of+discontinuous+functions%22\n|mr = 0785938\n|zbl= 0564.46025\n|isbn = 90-247-3109-7\n}}, particularly part II, chapter 4 paragraph 2 \"''Sets with finite perimeter''\". One of the best books about {{mvar|BV}}–functions and their application to problems of [[mathematical physics]], particularly [[chemical kinetics]].\n*{{Citation\n|last = Maz'ya\n|first = Vladimir G. \n|authorlink = Vladimir Gilelevich Maz'ya\n|title = Sobolev Spaces\n|publisher = [[Springer-Verlag]]\n|location = [[Berlin]]–[[Heidelberg]]–-[[New York City]]\n|year = 1985\n|pages = xix+486\n|isbn=3-540-13589-8\n|mr = 817985 \n|zbl = 0692.46023\n}}; particularly chapter 6, \"On functions in the space {{math|''BV''(Ω)}}\". One of the best monographs on the theory of [[Sobolev spaces]].\n*{{Citation\n|last = Vol'pert\n|first = Aizik Isaakovich\n|author-link = Aizik Isaakovich Vol'pert\n|title = Spaces {{mvar|BV}} and quasi-linear equations\n|language = Russian\n|journal = [[Matematicheskii Sbornik]]\n|series = (N.S.),\n|volume = 73(115)\n|issue = 2\n|pages = 255–302  \n|year = 1967\n|url = http://mi.mathnet.ru/eng/msb/v115/i2/p255\n|mr = 216338\n|zbl = 0168.07402\n}}. A seminal paper where Caccioppoli sets and {{mvar|BV}}–functions are deeply studied and the concept of [[functional superposition]] is introduced and applied to the theory of [[partial differential equation]]s.\n{{refend}}\n\n== External links ==\n*{{springer\n | title=Geometric measure theory\n | id= g/g130040\n | last= O'Neil\n | first=Toby Christopher\n | author-link=\n}}\n*{{springer\n | title=Perimeter\n | id= P/p072130\n | last=Zagaller\n | first=Victor Abramovich\n | author-link= Victor Abramovich Zalgaller\n}}\n*[http://www.encyclopediaofmath.org/index.php/Function_of_bounded_variation Function of bounded variation] at [http://www.encyclopediaofmath.org/ Encyclopedia of Mathematics]\n\n{{DEFAULTSORT:Caccioppoli Set}}\n[[Category:Mathematical analysis]]\n[[Category:Calculus of variations]]\n[[Category:Measure theory]]"
    },
    {
      "title": "Chaplygin problem",
      "url": "https://en.wikipedia.org/wiki/Chaplygin_problem",
      "text": "In [[mathematics]], particularly in the fields of [[nonlinear dynamics]] and the [[calculus of variations]], the '''Chaplygin problem''' is an isoperimetric problem with a [[derivative|differential]] [[constraint (mathematics)|constraint]].  Specifically, the problem is to determine what flight path an airplane in a constant wind field should take in order to encircle the maximum possible area in a given amount of time.  The airplane is assumed to be constrained to move in a plane, moving at a constant [[airspeed]] ''v'', for time ''T'', and the wind is assumed to move in a constant direction with speed ''w''.\n\nThe solution of the problem is that the airplane should travel in an [[ellipse]] whose major axis is perpendicular to ''w'', with eccentricity ''w''/''v''.\n\n==References==\n* {{cite book|author=Akhiezer, N. I.|author-link=Naum Akhiezer|title=The Calculus of variations|publisher=Blasidel|location=New York|year=1962|pages=206–208|isbn=3-7186-4805-9}}\n* {{cite journal|author=Klamkin, M. S.|title=On Extreme length flight paths|journal=SIAM Review|volume=18|issue=2|year=1976|pages=486–488|doi=10.1137/1018079}}\n* {{cite journal|author1=Klamkin, M. S.  |author2=Newman, D. J.|title=Flying in a wind field I, II|journal=Amer. Math. Monthly|volume=76|pages=16–23, pp. 1013–1019|year=1969|doi=10.2307/2316780|issue=1|publisher=Mathematical Association of America|jstor=2316780}}\n\n[[Category:Calculus of variations]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Convenient vector space",
      "url": "https://en.wikipedia.org/wiki/Convenient_vector_space",
      "text": "In mathematics, '''convenient vector spaces''' are  [[locally convex]] vector spaces satisfying a very mild [[Uniform space#Completeness|completeness condition]]. \n \nTraditional [[Multivariable calculus|differential calculus]] is effective in the analysis of finite-dimensional [[vector space]]s and for [[Banach space]]s. Beyond Banach spaces, difficulties begin to arise; in particular, composition of [[Continuous linear operator|continuous linear mappings]] stop being jointly continuous at the level of Banach spaces,{{refn|group=Note|An example of a composition mapping is the evaluation mapping <math>\\text{ev} : E \\times E^* \\to \\mathbb{R}</math>, where <math>E</math> is a [[Locally convex topological vector space|locally convex vector space]], and where  <math>E^*</math> is its [[Dual space|dual]] of continuous linear functionals equipped with any locally convex topology such that the evaluation mapping is separately continuous. If the evaluation is assumed to be jointly continuous, then there are neighborhoods <math>U \\subseteq E</math> and <math>V \\subseteq E^*</math> of zero such that <math> U \\times V \\subseteq [0, 1]</math>. However, this means that <math>U</math> is contained in the [[Polar set|polar]] of the open set <math>V</math>; so it is bounded in <math>E</math>. Thus <math>E</math> admits a bounded neighborhood of zero, and is thus a [[normed vector space]].}} for any compatible topology on the spaces of continuous linear mappings.\n\nMappings between convenient vector spaces are '''smooth''' or <math>C^\\infty</math> if they map smooth curves to smooth curves. This leads to a [[Cartesian closed category]] of smooth mappings between <math>c^\\infty</math>-open subsets of convenient vector spaces (see property 6 below). The corresponding calculus of smooth mappings is called ''convenient calculus''.\nIt is weaker than any other reasonable notion of differentiability, it is easy to apply, but there are smooth mappings which are not continuous (see Note 1).\nThis type of calculus alone is not useful in solving equations{{refn|group=Note|In order to be useful for solving equations like nonlinear PDE's, convenient calculus has to be supplemented by, for example,  [[a priori estimate]]s which help to create enough Banach space situation to allow convergence of some iteration procedure; for example, see the [[Nash–Moser theorem]], described in terms of convenient calculus in [KM], section 51.}}.\n\n==The <math>c^\\infty</math>-topology==\nLet <math>E</math> be a locally convex vector space. A curve <math>c : \\mathbb{R} \\to E</math> is called ''smooth'' or <math>C^\\infty</math> if all derivatives exist and are continuous. Let <math>C^\\infty(\\mathbb{R}, E)</math> be the space of smooth curves. It can be shown that the set of smooth curves does not depend entirely on the locally convex \ntopology of <math>E</math>, only on its associated [[Bornological space|bornology]] (system of bounded sets); see [KM], 2.11.\nThe final topologies with respect to the following sets of mappings into <math>E</math> coincide; see [KM], 2.13.\n* <math>C^\\infty(\\mathbb{R}, E)</math>.\n* The set of all Lipschitz curves (so that <math>\\left\\{ \\dfrac{c(t) - c(s)}{t - s} : t \\neq s {,} |t|, |s| \\leq C \\right\\}</math> is bounded in <math>E</math>, for each <math>C</math>). \n* The set of injections <math>E_B \\to E</math> where <math>B</math> runs through all bounded [[Absolutely convex set|absolutely convex]] subsets in <math>E</math>, and where <math>E_B</math> is the linear span of <math>B</math> equipped with the [[Minkowski functional]] <math>\\| x \\|_B := \\inf\\{ \\lambda > 0 : x \\in \\lambda B \\}</math>.\n* The set of all Mackey-convergent sequences <math>x_n \\to x</math> (there exists a sequence <math>0 < \\lambda_n \\to \\infty</math> with <math>\\lambda_n (x_n - x)</math> bounded).\nThis topology is called the <math>c^\\infty</math>-''topology'' on <math>E</math> and we write <math>c^\\infty E</math> for the resulting topological space. \nIn general (on the space <math>D</math> of smooth functions with compact support on the real line, for example) it is finer than the given locally convex topology, it is not a vector space topology, since addition is no longer jointly continuous. Namely, even <math>c^\\infty(D \\times D) \\neq (c^\\infty D) \\times (c^\\infty D)</math>.\nThe finest among all locally convex topologies on <math>E</math> which are coarser than <math>c^\\infty E</math> is the bornologification of the given locally convex topology. If <math>E</math> is a Fréchet space, then <math>c^\\infty E = E</math>.\n\n==Convenient vector spaces==\nA locally convex vector space <math>E</math> is said to be a ''convenient vector space'' if one of the following equivalent conditions holds (called  <math>c^\\infty</math>-completeness); see [KM], 2.14.\n* For any <math>c \\in C^\\infty(\\mathbb{R}, E)</math> the (Riemann-) integral <math>\\int_0^1 c(t)\\,dt</math> exists in <math>E</math>.\n* Any Lipschitz curve in <math>E</math> is locally Riemann integrable.\n* Any ''scalar wise'' <math>C^\\infty</math> curve is <math>C^\\infty</math>: A curve <math>c : \\mathbb{R} \\to E</math> is smooth if and only if the composition <math>\\lambda \\circ c : t \\mapsto \\lambda(c(t))</math> is in <math>C^\\infty(\\mathbb{R}, \\mathbb{R})</math> for all <math>\\lambda \\in E^*</math> where <math>E^*</math> is the dual of all continuous linear functionals on <math>E</math>.   \n** Equivalently, for all <math>\\lambda \\in E'</math>, the dual of all bounded linear functionals. \n** Equivalently, for all <math>\\lambda \\in V</math>, where <math>V</math> is a subset of <math>E'</math> which recognizes bounded subsets in <math>E</math>; see [KM], 5.22.\n* Any Mackey-Cauchy-sequence (i.e., <math>t_{n m} (x - x_m) \\to 0</math> for some <math>t_{n m} \\to \\infty</math> in <math>\\mathbb{R}</math> converges in <math>E</math>. This is visibly a mild completeness requirement.\n* If <math>B</math> is bounded closed absolutely convex, then <math>E_B</math> is a Banach space.\n* If <math>f : \\mathbb{R} \\to E</math> is scalar wise <math>\\text{Lip}^k</math>, then <math>f</math> is <math>\\text{Lip}^k</math>, for <math>k > 1</math>.\n* If <math>f : \\mathbb{R} \\to E</math> is scalar wise <math>C^\\infty</math> then <math>f</math> is differentiable at <math>0</math>.\nHere a mapping <math>f : \\mathbb{R} \\to E</math> is called <math>\\text{Lip}^k</math> if all \nderivatives up to order <math>k</math> exist and are Lipschitz, locally on <math>\\mathbb{R}</math>.\n\n==Smooth mappings==\nLet <math>E</math> and <math>F</math> be convenient vector spaces, \nand let <math>U \\subseteq E</math> be <math>c^\\infty</math>-open. \nA mapping <math>f : U \\to F</math> is called ''smooth'' or \n<math>C^\\infty</math>, if the composition <math>f \\circ c \\in C^\\infty(\\mathbb{R}, F)</math> for all <math>c \\in C^\\infty(\\mathbb{R}, U)</math>. See [KM], 3.11.\n\n==Main properties of smooth calculus==\n\n1. For maps on Fréchet spaces this notion of smoothness coincides with all other reasonable definitions. On <math>\\mathbb{R}^2</math> this is a non-trivial theorem, proved by Boman, 1967. See also [KM], 3.4.\n\n2. Multilinear mappings are smooth if and only if they are bounded ([KM], 5.5).\n\n3. If <math>f : E \\supseteq U \\to F</math> is smooth then the derivative <math>df : U \\times E \\to F</math> is smooth, and also <math>df : U \\to L(E, F)</math> is smooth where <math>L(E, F)</math> denotes the space of all bounded linear mappings with the topology of uniform convergence on bounded subsets; see [KM], 3.18.\n\n4. The chain rule holds ([KM], 3.18).\n\n5. The space <math>C^\\infty(U, F)</math> of all smooth mappings <math>U \\to F</math> is again a convenient vector space where the structure is given by the following injection, where <math>C^\\infty(\\mathbb{R}, \\mathbb{R})</math> carries the topology of compact convergence in each derivative separately; see [KM], 3.11 and 3.7.\n::::<math>\nC^\\infty(U,F) \\to\n\\prod_{c\\in C^\\infty(\\mathbb R,U), \\ell\\in F^*} \nC^\\infty(\\mathbb R,\\mathbb R),\n\\quad f\\mapsto (\\ell\\circ f\\circ c)_{c,\\ell}\\,.\n</math>\n\n6. The ''exponential law'' holds ([KM], 3.12): For <math>c^\\infty</math>-open <math>V \\subseteq F</math> the following mapping is a linear diffeomorphism of convenient vector spaces. \n:::<math>\nC^\\infty(U,C^\\infty(V,G)) \\cong C^\\infty(U\\times V, G),\\qquad f \\mapsto g,\\qquad f(u)(v) = g(u,v).\n</math>\nThis is the main assumption of variational calculus. Here it is a theorem. This property is the source of the name ''convenient'', which was borrowed from (Steenrod 1967).\n\n7. ''Smooth uniform boundedness theorem'' ([KM], theorem 5.26). \nA linear mapping <math>f : E \\to C^\\infty(V, G)</math> is smooth (by (2) equivalent to bounded) if and only if <math>\\operatorname{ev}_v \\circ f : V \\to G</math> is smooth for each <math>v \\in V</math>. \n\n8.  The following canonical mappings are smooth. This follows from the exponential law by simple categorical reasonings, see [KM], 3.13.\n\n::<math>\n\\begin{align}\n& \\operatorname{ev}: C^\\infty(E,F)\\times E\\to F,\\quad \\text{ev}(f,x) = f(x) \\\\[6pt]\n& \\operatorname{ins}: E\\to C^\\infty(F,E\\times F),\\quad\\text{ins}(x)(y) = (x,y) \\\\[6pt]\n& (\\quad)^\\wedge :C^\\infty(E,C^\\infty(F,G))\\to C^\\infty(E\\times F,G) \\\\[6pt]\n& (\\quad)^\\vee :C^\\infty(E\\times F,G)\\to C^\\infty(E,C^\\infty(F,G)) \\\\[6pt]\n& \\operatorname{comp}:C^\\infty(F,G)\\times C^\\infty(E,F)\\to C^\\infty(E,G) \\\\[6pt]\n& C^\\infty(\\quad,\\quad):C^\\infty(F,F_1)\\times C^\\infty(E_1,E)\\to C^\\infty(C^\\infty(E,F),C^\\infty(E_1,F_1)),\\quad (f,g)\\mapsto(h\\mapsto f\\circ h\\circ g) \\\\ [6pt]\n& \\prod:\\prod C^\\infty(E_i,F_i)\\to C^\\infty \\left(\\prod E_i,\\prod F_i\\right)\n\\end{align}\n</math>\n\n==Related convenient calculi==\nConvenient calculus of smooth mappings appeared for the first time in [Frölicher,  1981], [Kriegl 1982, 1983].\nConvenient calculus (having properties 6 and 7) exists also for:\n* Real analytic mappings (Kriegl, Michor, 1990; see also [KM], chapter II). \n* Holomorphic mappings (Kriegl, Nel, 1985; see also [KM], chapter II). The notion of holomorphy is that of [Fantappié, 1930-33].\n* Many classes of Denjoy Carleman ultradifferentiable functions, both of Beurling type and of  Roumieu-type [Kriegl, Michor, Rainer, 2009, 2011, 2015].\n* With some adaptations, <math>\\operatorname{Lip}^k</math>, [FK]. \n* With more adaptations, even <math>C^{k, \\alpha}</math>  (i.e., the <math>k</math>-th derivative is Hölder-continuous with index <math>\\alpha</math>) ([Faure, 1989], [Faure, These Geneve, 1991]).\nThe corresponding notion of convenient vector space is the same (for their underlying real vector space in the complex case) for all these theories.\n\n==Application: Manifolds of mappings between finite dimensional manifolds==\n\nThe exponential law 6 of convenient calculus allows for very simple proofs of the basic facts about manifolds of mappings. \nLet <math>M</math> and <math>N</math> be finite dimensional [[differentiable manifold|smooth manifolds]] where <math>M</math> is [[compact space|compact]]. We use an \nauxiliary [[Riemannian manifold|Riemann metric]] <math>\\bar g</math> on <math>N</math>. The [[Exponential map (Riemannian geometry)|Riemannian exponential mapping]] of <math>\\bar g</math> is described in the following diagram:\n:::[[file:ManifoldOfMappingsDiagram.svg]]\nIt induces an atlas of charts on the space <math>C^\\infty(M, N)</math> of all smooth mappings <math>M \\to N</math> as follows.\nA chart centered at <math>f \\in C^\\infty(M, N)</math>, is:\n:::<math>u_f : C^\\infty(M,N)\\supset U_f =\\{g: (f,g)(M)\\subset V^{N\\times N}\\} \\to \\tilde U_f \\subset \\Gamma(f^*TN),</math>\n:::<math>u_f(g) = (\\pi_N,\\exp^{\\bar g})^{-1} \\circ (f,g),\\quad u_f(g)(x) = (\\exp^{\\bar g}_{f(x)})^{-1}(g(x)),</math>\n:::<math>(u_f)^{-1}(s)  = \\exp^{\\bar g}_f\\circ s, \\qquad\\quad (u_f)^{-1}(s)(x) = \\exp^{\\bar g}_{f(x)}(s(x)).</math>\nNow the basics facts follow in easily.\nTrivializing the pull back  vector bundle <math>f^* T N</math> and applying the exponential law 6 leads to the diffeomorphism\n:::<math>C^\\infty(\\mathbb R,\\Gamma(M;f^*TN)) = \\Gamma(\\mathbb R\\times M; \\operatorname{pr_2}^* f^*TN).</math> \nAll chart change mappings are smooth (<math>C^\\infty</math>) since they map smooth curves to smooth curves:  \n:::<math>\\tilde U_{f_1}\\ni s\\mapsto (\\pi_N,\\exp^{\\bar g})\\circ s \\mapsto (\\pi_N,\\exp^{\\bar g})\\circ(f_2,\\exp^{\\bar g}_{f_1}\\circ s).</math>\nThus <math>C^\\infty(M, N)</math>is a smooth manifold modeled on Fréchet spaces. The space of all smooth curves in this manifold is given by\n:::<math>C^\\infty(\\mathbb R,C^\\infty(M,N))\\cong C^\\infty(\\mathbb R\\times M,N).</math>\nSince it visibly maps smooth curves to smooth curves, ''composition''\n:::<math>C^\\infty(P,M)\\times C^\\infty(M,N)\\to C^\\infty(P,N),\\qquad (f,g)\\mapsto g\\circ f,</math>  \nis smooth. As a consequence of the chart structure, the [[tangent bundle]] of the manifold of mappings is given by \n:::<math>\\pi_{C^\\infty(M,N)} = C^\\infty(M,\\pi_N) : TC^\\infty(M,N)= C^\\infty(M,TN) \\to  C^\\infty(M,N).</math>\n\n===Regular Lie groups===\nLet <math>G</math> be a connected smooth [[Lie group]] modeled on convenient vector spaces, with Lie algebra \n<math>\\mathfrak g=T_eG</math>. Multiplication and inversion are denoted by:\n:::<math> \\mu: G\\times G\\to G,\\quad \\mu(x,y) = x.y = \\mu_x(y) = \\mu^y(x), \\qquad \\nu: G\\to G, \\nu(x) = x^{-1}.</math>\nThe notion of a regular Lie group is originally due to Omori et al. for Fréchet Lie groups, was weakened and made more transparent by J. Milnor, and was then carried over to convenient Lie groups; see [KM], 38.4.\n\nA Lie group <math>G</math> is called ''regular'' if the following two conditions hold:\n* For each smooth curve <math>X\\in C^{\\infty}(\\mathbb R,\\mathfrak g)</math> in the Lie algebra there exists a smooth curve <math>g\\in C^{\\infty}(\\mathbb R,G)</math> in the Lie group whose right logarithmic derivative is <math>X</math>. It turn out that <math>g</math> is uniquely determined by its initial value <math>g(0)</math>, if it exists. That is,\n:::<math> g(0) = e, \\qquad \\partial_t g(t) = T_e(\\mu^{g(t)})X(t) = X(t).g(t).</math>\nIf  <math>g</math> is the unique solution for the curve <math>X</math> required above, we denote  \n:::<math>\\operatorname{evol}^r_G(X)=g(1), \\quad \\operatorname{Evol}^r_G(X)(t):= g(t) =\\operatorname{evol}^r_G(tX).</math> \n* The following mapping is required to be smooth:\n:::<math>\\operatorname{evol}^r_G: C^{\\infty}(\\mathbb R,\\mathfrak g)\\to G.</math>\nIf <math>X</math> is a constant curve in the Lie algebra, then <math>\\operatorname{evol}_G^r(X) = \\exp^G(X)</math> is the group exponential mapping.\n\n'''Theorem.''' For each compact manifold <math>M</math>, the diffeomorphism group <math>\\operatorname{Diff}(M)</math> is a regular Lie group. Its Lie algebra is the space <math>\\mathfrak X(M)</math> of all smooth vector fields on <math>M</math>, with the negative of the usual bracket as Lie bracket.\n\n''Proof:'' The diffeomorphism group <math>\\operatorname{Diff}(M)</math> is a smooth manifold since it is an open subset in <math>C^\\infty(M, M)</math>.  Composition is smooth by restriction. Inversion is smooth: If <math>t \\to f(t, \\ )</math> is a smooth curve in <math>\\operatorname{Diff}(M)</math>, then {{math|''f''(''t'',&nbsp;&nbsp;){{su|p=−1}}}}<math>f(t, \\ )^{-1}(x)</math> satisfies the implicit equation \n<math>f(t,f(t,\\quad)^{-1}(x))=x</math>, so by the finite dimensional implicit function theorem, <math>(t, x) \\mapsto f(t, \\ )^{-1}(x)</math> is smooth. So inversion maps smooth curves to smooth curves, and thus inversion is smooth.\nLet <math>X(t, x)</math> be a time dependent vector field on <math>M</math> (in <math>C^\\infty(\\mathbb R,\\mathfrak X(M))</math>).\nThen the flow operator <math>\\operatorname{Fl}</math> of the corresponding autonomous vector field <math>\\partial_t\\times X</math> on <math>\\mathbb{R} \\times M</math> induces the evolution operator via\n:::<math>\\operatorname{Fl}_s(t,x)=(t+s,\\operatorname{Evol}(X)(t,x))</math> \nwhich satisfies the ordinary differential equation \n:::<math>\\partial_t\\operatorname{Evol}(X)(t,x) = X(t,\\operatorname{Evol}(X)(t,x)).</math>\nGiven a smooth curve in the Lie algebra, <math>X(s,t,x)\\in C^\\infty(\\mathbb R^2,\\mathfrak X(M))</math>,\nthen the solution of the ordinary differential equation depends smoothly also on the further variable <math>s</math>,\nthus <math>\\operatorname{evol}_{\\operatorname{Diff}(M)}^r</math> maps smooth curves of time dependent vector fields to smooth curves of \ndiffeomorphism. QED.\n\n===The principal bundle of embeddings===\nFor finite dimensional manifolds <math>M</math> and <math>N</math> with <math>M</math> compact, the space <math>\\operatorname{Emb}(M, N)</math> of all smooth embeddings of <math>M</math> into <math>N</math>, is open in <math>C^\\infty(M, N)</math>, so it is a smooth manifold. The diffeomorphism group <math>\\operatorname{Diff}(M)</math> acts freely and smoothly from the right on <math>\\operatorname{Emb}(M, N)</math>.\n\n'''Theorem:'''  <math>\\operatorname{Emb}(M, N) \\to \\operatorname{Emb}(M, N) / \\operatorname{Diff}(M)</math> is a principal fiber bundle with structure group <math>\\operatorname{Diff}(M)</math>.\n\n''Proof:'' One uses again an auxiliary Riemannian metric <math>\\bar g</math> on <math>N</math>. Given <math>f \\in \\operatorname{Emb}(M, N)</math>, view <math>f(M)</math> as a submanifold of <math>N</math>, and split the restriction of the tangent bundle <math>T N</math> to <math>f(M)</math> into the subbundle normal to <math>f(M)</math> and tangential to <math>f(M)</math> as\n<math>TN|_{f(M)}= \\operatorname{Nor}(f(M))\\oplus Tf(M)</math>. Choose a tubular neighborhood \n:::<math>p_{f(M)} : \\operatorname{Nor}(f(M))\\supset  W_{f(M)} \\to f(M).</math>\nIf <math>g : M \\to N</math> is <math>C^1</math>-near to <math>f</math>, then \n:::<math>\\phi(g):=f^{-1}\\circ\\, p_{f(M)}\\circ\\, g\\in \\operatorname{Diff}(M)\\quad \\text{and}\\quad \ng\\circ\\, \\phi(g)^{-1}\\in \\Gamma(f^*W_{f(M)}) \\subset \\Gamma(f^*\\operatorname{Nor}(f(M))).</math>\nThis is the required local splitting. ''QED''\n\n===Further applications===\nAn overview of applications using geometry of shape spaces and diffeomorphism groups can be found in [Bauer, Bruveris, Michor, 2014].\n\n==Notes==\n{{reflist|group=Note}}\n\n==References==\n{{reflist}}\n\n* Bauer, M., Bruveris, M., Michor, P.W.: Overview of the Geometries of Shape Spaces and Diffeomorphism Groups. Journal of Mathematical Imaging and Vision, 50, 1-2, 60-97, 2014. [https://arxiv.org/abs/1305.1150 (arXiv:1305.11500)]\n* Boman, J.: Differentiability of a function and of its composition with a  function of one variable, Mathematica Scandinavia vol. 20 (1967), 249–268.\n* Faure, C.-A.: Sur un théorème de Boman, C. R. Acad. Sci., Paris}, vol. 309 (1989), 1003–1006.\n* Faure, C.-A.: Théorie de la différentiation dans les espaces convenables, These, Université de Genève, 1991.\n* Frölicher, A.: Applications lisses entre espaces et variétés de Fréchet, C. R. Acad. Sci. Paris, vol. 293 (1981), 125–127.\n* [FK] Frölicher, A., Kriegl, A.: Linear spaces and differentiation theory. Pure and Applied Mathematics,\tJ. Wiley, Chichester, 1988.\n* Kriegl, A.: Die richtigen Räume für Analysis im Unendlich – Dimensionalen,\tMonatshefte für Mathematik vol. 94 (1982) 109–124.\n* Kriegl, A.: Eine kartesisch abgeschlossene Kategorie glatter Abbildungen zwischen beliebigen lokalkonvexen Vektorräumen, Monatshefte für Mathematik vol. 95 (1983) 287–309.\n* [KM] Kriegl, A., Michor, P.W.: The Convenient Setting of Global Analysis. Mathematical Surveys and Monographs, Volume: 53, American Mathematical Society, Providence, 1997. [http://www.mat.univie.ac.at/~michor/apbookh-ams.pdf (pdf)]\n* Kriegl, A., Michor, P. W., Rainer, A.: The convenient setting for non-quasianalytic Denjoy–Carleman differentiable mappings, Journal of Functional Analysis, vol. 256 (2009), 3510–3544. [https://arxiv.org/abs/0804.2995 (arXiv:0804.2995)]\n* Kriegl, A., Michor, P. W., Rainer, A.: The convenient setting for quasianalytic Denjoy–Carleman differentiable mappings, Journal of Functional Analysis, vol. 261 (2011), 1799–1834. [https://arxiv.org/abs/0909.5632 (arXiv:0909.5632)]\n* Kriegl, A., Michor, P. W., Rainer, A.: The convenient setting for Denjoy-Carleman differentiable mappings of Beurling and Roumieu type. Revista Matemática Complutense (2015). doi:10.1007/s13163-014-0167-1. [https://arxiv.org/abs/1111.1819 (arXiv:1111.1819)]\n* Michor, P.W.: Manifolds of mappings and shapes. [https://arxiv.org/abs/1505.02359 (arXiv:1505.02359)]\n* Steenrod, N. E.: A convenient category for topological spaces, Michigan Mathematical Journal, vol. 14 (1967), 133–152.\n\n{{Functional Analysis}}\n\n[[Category:Multivariable calculus]]\n[[Category:Differential calculus]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Costate equation",
      "url": "https://en.wikipedia.org/wiki/Costate_equation",
      "text": "The '''costate equation''' is related to the state equation used in [[optimal control]].<ref>{{cite book |first=Morton I. |last=Kamien |authorlink=Morton Kamien |first2=Nancy L. |last2=Schwartz |title=Dynamic Optimization |location=London |publisher=North-Holland |edition=Second |year=1991 |isbn=0-444-01609-0 |pages=126–27 |url=https://books.google.com/books?id=0IoGUn8wjDQC&pg=PA126 }}</ref><ref>{{cite book |first=David G. |last=Luenberger |authorlink=David Luenberger |title=Optimization by Vector Space Methods |location=New York |publisher=John Wiley & Sons |year=1969 |isbn= |page=263 |url=https://books.google.com/books?id=lZU0CAH4RccC&pg=PA263 }}</ref> It is also referred to as ''auxiliary'', ''adjoint'', ''influence'', or ''multiplier equation''. It is stated as a [[vector (geometry)|vector]] of first order [[differential equation]]s\n:<math>\n\\dot{\\lambda}^{\\mathsf{T}}(t)=-\\frac{\\partial H}{\\partial x}\n</math>\nwhere the right-hand side is the vector of [[partial derivative]]s of the negative of the [[Hamiltonian (control theory)|Hamiltonian]] with respect to the state variables.\n\n== Interpretation ==\nThe costate variables <math>\\lambda(t)</math> can be interpreted as [[Lagrange multipliers]] associated with the state equations.  The state equations represent constraints of the minimization problem, and the costate variables represent the [[marginal cost]] of violating those constraints; in economic terms the costate variables are the [[shadow price]]s.<ref>{{cite book |first=Akira |last=Takayama |title=Mathematical Economics |location= |publisher=Cambridge University Press |year=1985 |page=621 |url=https://books.google.com/books?id=j6PLOBFotPQC&pg=PA621 }}</ref>\n\n== Solution ==\nThe state equation is subject to an initial condition and is solved forwards in time.  The costate equation must satisfy a [[boundary condition|terminal condition]] and is solved backwards in time, from the final time towards the beginning.  For more details see [[Pontryagin's maximum principle]].<ref>[[I. Michael Ross|Ross, I. M.]] ''A Primer on Pontryagin's Principle in Optimal Control'', Collegiate Publishers, 2009. {{ISBN|978-0-9843571-0-9}}.</ref>\n\n== See also ==\n* [[Adjoint equation]]\n* [[Covector mapping principle]]\n* [[Lagrange multiplier]]\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Costate Equation}}\n[[Category:Optimal control]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "De Donder–Weyl theory",
      "url": "https://en.wikipedia.org/wiki/De_Donder%E2%80%93Weyl_theory",
      "text": "In [[mathematical physics]], the '''De Donder–Weyl theory''' is a generalization of the [[Hamiltonian formalism]] in the [[calculus of variations]] and [[classical field theory]] over [[spacetime]] which treats the space and time coordinates on equal footing. In this framework, the [[Hamiltonian formalism]] in [[mechanics]] is generalized to field theory in the way that a [[Field (physics)|field]] is represented as a system that varies both in space and in time. This generalization is different from the [[Canonical form|canonical]] [[Hamiltonian formalism]] in field theory which treats space and time variables differently and describes classical fields as infinite-dimensional systems evolving in time. \n\n{| style=\"margin:0 1em 1em; text-align:center; border:1px solid black; padding:10px; float:right;\"\n|-\n|<u>De Donder–Weyl equations:</u>\n|-\n|<math>\\partial p^{i}_a / \\partial x^{i} = -\\partial H / \\partial y^{a}</math>\n|-\n|<math>\\partial y^{a} / \\partial x^{i} = \\partial H / \\partial p^{i}_a</math>\n|-\n|}\n\n== De Donder–Weyl formulation of field theory ==\nThe De Donder–Weyl theory is based on a change of variables known as [[Legendre transformation]]. Let ''x<sup>i</sup>'' be [[spacetime]] coordinates, for ''i'' = 1 to ''n'' (with ''n'' = 4 representing 3 + 1 dimensions of space and time), and ''y<sup>a</sup>'' field variables, for ''a'' = 1 to ''m'', and ''L'' the [[Lagrangian density]] \n:<math>L = L(y^{a},\\partial_i y^{a},x^{i})</math>\nWith the '''polymomenta''' ''p<sup>i</sup><sub>a</sub>'' defined as\n:<math>p^{i}_a = \\partial L / \\partial (\\partial_i y^{a})</math>\nand  the '''De Donder–Weyl Hamiltonian function''' ''H'' defined as\n:<math>H = p^{i}_a \\partial_i y^{a} - L</math>\nthe '''De Donder–Weyl equations''' are:<ref>Hanno Rund, \n\"Hamilton-Jacobi Theory in the Calculus of Variations: Its Role in Mathematics and Physics\", Van Nostrand, Reinhold, 1966.</ref>\n:<math>\\partial p^{i}_a / \\partial x^{i} = -\\partial H / \\partial y^{a} \\, , \\, \\partial y^{a} / \\partial x^{i} = \\partial H / \\partial p^{i}_a</math>\n\nThis De Donder-Weyl Hamiltonian form of field equations is [[Covariance and contravariance of vectors|covariant]] and it is equivalent to the [[Euler-Lagrange equations]] when the Legendre transformation to the variables ''p<sup>i</sup><sub>a</sub>'' and  ''H'' is not singular. The theory is a formulation of a [[covariant Hamiltonian field theory]] which is different from the [[Canonical form|canonical]] [[Hamiltonian formalism]] and for ''n'' = 1 it reduces to [[Hamiltonian mechanics]] (see also [[Calculus of variations#Action principle|action principle in the calculus of variations]]). \n\n[[Hermann Weyl]] in 1935 has developed the [[Hamilton-Jacobi theory]] for the De Donder–Weyl theory.<ref>Hermann Weyl, \"Geodesic Fields in the Calculus of Variation for Multiple Integrals\", Ann. Math. 36, 607 (1935). https://www.jstor.org/stable/1968645</ref>\n\nSimilarly to the [[Hamiltonian formalism]] in mechanics formulated using the [[symplectic geometry]] of [[phase space]] \nthe De Donder-Weyl theory can be formulated using the [[multisymplectic geometry]] or [[polysymplectic geometry]] and the geometry \nof [[jet bundle]]s. \n\nA generalization of the [[Poisson brackets]] to the De Donder–Weyl theory \nand the representation of De Donder–Weyl equations in terms of generalized [[Poisson brackets]] satisfying the [[Gerstenhaber algebra]] \nwas found by Kanatchikov in 1993.<ref>Igor V. Kanatchikov: [https://arxiv.org/abs/hep-th/9312162v1 ''On the Canonical Structure of the De Donder–Weyl Covariant Hamiltonian Formulation of Field Theory I. Graded Poisson brackets and equations of motion''], arXiv:hep-th/9312162v1 (submitted on 20 Dec 1993).</ref>\n\n== History ==\nThe formalism, now known as De Donder–Weyl (DW) theory, was developed by [[Théophile De Donder]]<ref>Théophile De Donder, \"Théorie invariantive du calcul des variations,\" Gauthier-Villars, 1930. [https://books.google.com/books?id=3kI7AQAAIAAJ&dq=editions:LCCN39009801]</ref><ref>Frédéric Hélein: ''Hamiltonian formalisms for multidimensional calculus of variations and perturbation theory'' \n In Haïm Brézis, Felix E. Browder, Abbas Bahri, Sergiu Klainerman, Michael Vogelius (ads.): ''Noncompact problems at the intersection of geometry, analysis, and topology'', American Mathematical Society, 2004, pp.&nbsp;127–148, [https://books.google.com/books?id=eL0_cDTqloEC&pg=PA131 p. 131], {{ISBN|0-8218-3635-8}},</ref> and [[Hermann Weyl]]. Hermann Weyl made his proposal in 1934 being inspired by the work of [[Constantin Carathéodory]], which in turn was founded on the work of [[Vito Volterra]]. The work of De Donder on the other hand started from the theory of integral [[Invariant theory|invariants]] of [[Élie Cartan]].<ref>Roger Bielawski, [[Kevin Houston (mathematician)|Kevin Houston]], Martin Speight: ''Variational Problems in Differential Geometry'', London Mathematical Society Lecture Notes Series, no.&nbsp;394, University of Leeds, 2009, {{ISBN|978-0-521-28274-1}}, [https://books.google.com/books?id=v4F7Ud8RYZoC&pg=PA104 p. 104 f.]</ref> The De Donder–Weyl theory has been a part of the calculus of variations since the 1930s and initially it found very few applications in physics. Recently it was applied in theoretical physics in the context of [[quantum field theory]]<ref>Igor V. Kanatchikov: [https://arxiv.org/abs/hep-th/9810/9810165v1 ''De Donder–Weyl theory and a hypercomplex extension of quantum mechanics to field theory''], arXiv:hep-th/9810165v1 (submitted on 21 October 1998)</ref> and [[quantum gravity]].<ref> I.V. Kanatchikov: [https://arxiv.org/abs/gr-qc/0012/0012074v2 ''Precanonical Quantum Gravity: quantization without the space-time decomposition''], arXiv:gr-qc/0012074 (submitted on 20 December 2000)</ref>\n\nIn 1970, Jedrzej Śniatycki, the author of ''Geometric quantization and quantum mechanics'', developed an invariant geometrical formulation of [[jet bundle]]s, building on the work of De Donder and Weyl.<ref>Jedrzej Śniatycki, 1970. Cited after: [[Yvette Kosmann-Schwarzbach]]: ''The Noether Theorems: Invariance and Conservation Laws in the 20th Century'', Springer, 2011, {{ISBN|978-0-387-87867-6}}, [https://books.google.com/books?id=e8F38Pu0YgEC&pg=PA111 p. 111]</ref> In 1999 Igor Kanatchikov has shown that the De Donder–Weyl covariant Hamiltonian field equations can be formulated in terms of [[Duffin–Kemmer–Petiau algebra|Duffin–Kemmer–Petiau matrices]].<ref>Igor V. Kanatchikov: [https://arxiv.org/abs/hep-th/9911/9911175v1 ''On the Duffin–Kemmer–Petiau formulation of the covariant Hamiltonian dynamics in field theory''], arXiv:hep-th/9911/9911175v1 (submitted on 23 November 1999)</ref>\n\n==See also==\n\n*[[Hamiltonian field theory]]\n*[[Covariant Hamiltonian field theory]]\n\n== Further reading ==\n* Selected papers on GEODESIC FIELDS, Translated and edited by D. H. Delphenich. Part 1 [http://mayaloop.gie.im/doc/geodesic_fields_-_pt._1.pdf], Part 2 [http://mayaloop.gie.im/doc/geodesic_fields_-_pt._2.pdf] \n* H.A. Kastrup, Canonical theories of Lagrangian dynamical systems in physics, Physics Reports, Volume 101, Issues 1–2, Pages 1-167 (1983). \n* Mark J. Gotay, James Isenberg, Jerrold E. Marsden, Richard Montgomery: \"Momentum Maps and Classical Relativistic Fields. Part I: Covariant Field Theory\" [https://arxiv.org/pdf/physics/9801019v2.pdf]\n* Cornelius Paufler, Hartmann Römer: [http://wwwthep.physik.uni-mainz.de/~paufler/publications/DWeqMultSympGeom.pdf ''De Donder–Weyl equations and multisymplectic geometry''], Reports on Mathematical Physics, vol.&nbsp;49 (2002), no.&nbsp;2–3, pp.&nbsp;325–334\n* Krzysztof Maurin: ''The Riemann legacy: Riemannian ideas in mathematics and physics'', Part II, Chapter 7.16 ''Field theories for calculus of variation for multiple integrals'', Kluwer Academic Publishers, {{ISBN|0-7923-4636-X}}, 1997, [https://books.google.com/books?id=jlll448aDLEC&pg=PA482 p.&nbsp;482 ff.]\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:De Donder-Weyl theory}}\n[[Category:Calculus of variations]]\n[[Category:Mathematical physics]]"
    },
    {
      "title": "Direct method in the calculus of variations",
      "url": "https://en.wikipedia.org/wiki/Direct_method_in_the_calculus_of_variations",
      "text": "In the [[calculus of variations]], a topic in [[mathematics]], '''the direct method''' is a general method for constructing a proof of the existence of a minimizer for a given [[Functional (mathematics)|functional]],<ref>Dacorogna, pp. 1&ndash;43.</ref> introduced by Zaremba and [[David Hilbert]] around 1900. The method relies on methods of [[functional analysis]] and [[topology]]. As well as being used to prove the existence of a solution, direct methods may be used to compute the solution to desired accuracy.<ref>{{cite book |title=Calculus of Variations |author=I. M. Gelfand |author2=S. V. Fomin |year=1991 |publisher=Dover Publications |isbn=978-0-486-41448-5}}</ref>\n\n== The method ==\nThe calculus of variations deals with functionals <math>J:V \\to \\bar{\\mathbb{R}}</math>, where <math>V</math> is some [[function space]] and <math>\\bar{\\mathbb{R}} = \\mathbb{R} \\cup \\{\\infty\\}</math>. The main interest of the subject is to find ''minimizers'' for such functionals, that is, functions <math>v \\in V</math> such that:<math>J(v) \\leq J(u)\\forall u \\in V. </math>\n\nThe standard tool for obtaining necessary conditions for a function to be a minimizer is the [[Euler&ndash;Lagrange equation]]. But seeking a minimizer amongst functions satisfying these may lead to false conclusions if the existence of a minimizer is not established beforehand.\n\nThe functional <math>J</math> must be bounded from below to have a minimizer. This means\n\n:<math>\\inf\\{J(u)|u\\in V\\} > -\\infty.\\,</math>\n\nThis condition is not enough to know that a minimizer exists, but it shows the existence of a ''minimizing sequence'', that is, a sequence <math>(u_n)</math> in <math>V</math> such that <math>J(u_n) \\to \\inf\\{J(u)|u\\in V\\}.</math>\n\nThe direct method may broken into the following steps\n# Take a minimizing sequence <math>(u_n)</math> for <math>J</math>.\n# Show that <math>(u_n)</math> admits some [[subsequence]] <math>(u_{n_k})</math>, that converges to a <math>u_0\\in V</math> with respect to a topology  <math>\\tau</math> on <math>V</math>.\n# Show that <math>J</math> is sequentially [[lower semi-continuous]] with respect to the topology <math>\\tau</math>.\n\nTo see that this shows the existence of a minimizer, consider the following characterization of sequentially lower-semicontinuous functions.\n:The function <math>J</math> is sequentially lower-semicontinuous if\n::<math>\\liminf_{n\\to\\infty} J(u_n) \\geq J(u_0)</math> for any convergent sequence <math>u_n \\to u_0</math> in <math>V</math>.\n\nThe conclusions follows from\n:<math>\\inf\\{J(u)|u\\in V\\} = \\lim_{n\\to\\infty} J(u_n) = \\lim_{k\\to \\infty} J(u_{n_k}) \\geq J(u_0) \\geq \\inf\\{J(u)|u\\in V\\}</math>,\nin other words\n:<math>J(u_0) = \\inf\\{J(u)|u\\in V\\}</math>.\n\n== Details ==\n\n=== Banach spaces ===\nThe direct method may often be applied with success when the space <math>V</math> is a subset of a [[separable space|separable]] [[reflexive space|reflexive]] [[Banach space]] <math>W</math>. In this case the  [[Banach&ndash;Alaoglu theorem#Sequential Banach–Alaoglu theorem|sequential Banach–Alaoglu theorem]] implies that any bounded sequence <math>(u_n)</math> in <math>V</math> has a subsequence that converges to some <math>u_0</math> in <math>W</math> with respect to the [[weak topology]]. If <math>V</math> is sequentially closed in <math>W</math>, so that <math>u_0</math> is in <math>V</math>, the direct method may be applied to a functional <math>J:V\\to\\bar{\\mathbb{R}}</math> by showing\n# <math>J</math> is bounded from below,\n# any minimizing sequence for <math>J</math> is bounded, and\n# <math>J</math> is weakly sequentially lower semi-continuous, i.e., for any weakly convergent sequence <math>u_n \\to u_0</math> it holds that <math>\\liminf_{n\\to\\infty} J(u_n) \\geq J(u_0)</math>.\nThe second part is usually accomplished by showing that <math>J</math> admits some growth condition. An example is\n:<math>J(x) \\geq \\alpha \\lVert x \\rVert^q - \\beta</math> for some <math>\\alpha > 0</math>, <math>q \\geq 1</math> and <math>\\beta \\geq 0</math>.\nA functional with this property is sometimes called coercive. Showing sequential lower semi-continuity is usually the most difficult part when applying the direct method. See below for some theorems for a general class of functionals.\n\n=== Sobolev spaces ===\nThe typical functional in the calculus of variations is an integral of the form\n:<math>J(u) = \\int_\\Omega F(x, u(x), \\nabla u(x))dx</math>\nwhere <math>\\Omega</math> is a subset of <math>\\mathbb{R}^n</math> and <math>F</math> is a real-valued function on <math>\\Omega \\times \\mathbb{R}^m \\times \\mathbb{R}^{mn}</math>. The argument of <math>J</math> is a differentiable function <math>u:\\Omega \\to \\mathbb{R}^m</math>, and its [[Jacobian matrix and determinant|Jacobian]] <math>\\nabla u(x)</math> is identified with a <math>mn</math>-vector.\n\nWhen deriving the Euler&ndash;Lagrange equation, the common approach is to assume <math>\\Omega</math> has a <math>C^2</math> boundary and let the domain of definition for <math>J</math> be <math>C^2(\\Omega, \\mathbb{R}^m)</math>. This space is a Banach space when endowed with the [[supremum norm]], but it is not reflexive. When applying the direct method, the functional is usually defined on a [[Sobolev space]] <math>W^{1,p}(\\Omega, \\mathbb{R}^m)</math> with <math>p > 1</math>, which is a reflexive Banach space. The derivatives of <math>u</math> in the formula for <math>J</math> must then be taken as [[weak derivative]]s. The next section presents two theorems regarding weak sequential lower semi-continuity of functionals of the above type.\n\n== Sequential lower semi-continuity of integrals ==\nAs many functionals in the calculus of variations are of the form\n:<math>J(u) = \\int_\\Omega F(x, u(x), \\nabla u(x))dx</math>,\nwhere <math>\\Omega \\subseteq \\mathbb{R}^n</math> is open, theorems characterizing functions <math>F</math> for which <math>J</math> is weakly sequentially lower-semicontinuous in <math>W^{1,p}(\\Omega, \\mathbb{R}^m)</math> is of great importance.\n\nIn general we have the following<ref>Dacorogna, pp. 74&ndash;79.</ref>\n:Assume that <math>F</math> is a function such that\n:# The function <math>(y, p) \\mapsto F(x, y, p)</math> is continuous for [[almost every]] <math>x \\in \\Omega</math>,\n:# the function <math>x \\mapsto F(x, y, p)</math> is [[measurable]] for every <math>(y, p) \\in \\mathbb{R}^m \\times \\mathbb{R}^{mn}</math>, and\n:# <math>F(x, y, p) \\geq a(x)\\cdot p + b(x)</math> for a fixed <math>a\\in L ^q(\\Omega, \\mathbb{R}^{mn})</math> where <math>1/q + 1/p = 1</math>, a fixed <math>b \\in L^1(\\Omega)</math>, for a.e. <math>x \\in \\Omega</math> and every <math>(y, p) \\in \\mathbb{R}^m \\times \\mathbb{R}^{mn}</math> (here <math>a(x) \\cdot p</math> means the inner product of <math>a(x)</math> and <math>p</math> in <math>\\mathbb{R}^{mn}</math>).\n:The following holds. If the function <math>p \\mapsto F(x, y, p)</math> is convex for a.e. <math>x \\in \\Omega</math> and every <math>y\\in \\mathbb{R}^m</math>,\n:then <math>J</math> is sequentially weakly lower semi-continuous.\nWhen <math>n = 1</math> or <math>m = 1</math> the following converse-like theorem holds<ref>Dacorogna, pp. 66&ndash;74.</ref>\n:Assume that <math>F</math> is continuous and satisfies\n::<math>| F(x, y, p) | \\leq a(x, | y |, | p |)</math>\n:for every <math>(x, y, p)</math>, and a fixed function <math>a(x, y, p)</math> increasing in <math>y</math> and <math>p</math>, and locally integrable in <math>x</math>. It then holds, if <math>J</math> is sequentially weakly lower semi-continuous, then for any given <math>(x, y) \\in \\Omega \\times \\mathbb{R}^m</math> the function <math>p \\mapsto F(x, y, p)</math> is convex.\n\nIn conclusion, when <math>m = 1</math> or <math>n = 1</math>, the functional <math>J</math>, assuming reasonable growth and boundedness on <math>F</math>, is weakly sequentially lower semi-continuous if, and only if, the function <math>p \\mapsto F(x, y, p)</math> is convex. If both <math>n</math> and <math>m</math> are greater than 1, it is possible to weaken the necessity of convexity to generalizations of convexity, namely [[polyconvex function|polyconvexity]] and quasiconvexity.<ref>Dacorogna, pp. 87&ndash;185.</ref>\n\n== Notes ==\n{{reflist}}\n\n== References and further reading ==\n* {{cite book | first = Bernard | last = Dacorogna | year = 1989 | title = Direct Methods in the Calculus of Variations | publisher = Springer-Verlag | isbn = 0-387-50491-5 }}\n* {{cite book | first = Irene | last = Fonseca | authorlink = Irene Fonseca |author2=Giovanni Leoni | year = 2007 | title = Modern Methods in the Calculus of Variations: <math>L^p</math> Spaces | publisher = Springer | isbn = 978-0-387-35784-3 }}\n* Morrey, C. B., Jr.: ''Multiple Integrals in the Calculus of Variations''. Springer, 1966 (reprinted 2008), Berlin {{ISBN|978-3-540-69915-6}}.\n* Jindřich Nečas: ''Direct Methods in the Theory of Elliptic Equations''. (Transl. from French original 1967 by A.Kufner and G.Tronel), Springer, 2012, {{ISBN|978-3-642-10455-8}}.\n*{{cite news|author=T. Roubíček|title= Direct method for parabolic problems|journal=Adv. Math. Sci. Appl.|volume=10|year=2000|pages=57–65|mr=1769181}}\n\n{{DEFAULTSORT:Direct Method In The Calculus Of Variations}}\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Dirichlet energy",
      "url": "https://en.wikipedia.org/wiki/Dirichlet_energy",
      "text": "In [[mathematics]], the '''Dirichlet energy''' is a measure of how ''variable'' a [[function (mathematics)|function]] is.  More abstractly, it is a [[quadratic function|quadratic]] [[functional (mathematics)|functional]] on the [[Sobolev space]] {{math|''H''<sup>1</sup>}}.  The Dirichlet energy is intimately connected to [[Laplace's equation]] and is named after the German mathematician [[Peter Gustav Lejeune Dirichlet]].\n\n==Definition==\n\nGiven an [[open set]] {{math|&Omega; &sube; '''R'''<sup>''n''</sup>}} and a function {{math|''u'' : &Omega; &rarr; '''R'''}} the '''Dirichlet energy''' of the function&nbsp;{{math|''u''}} is the [[real number]]\n\n:<math>E[u] = \\frac 1 2 \\int_\\Omega \\| \\nabla u(x) \\|^2 \\, dx,</math>\n\nwhere {{math|&nabla;''u'' : &Omega; &rarr; '''R'''<sup>''n''</sup>}} denotes the [[gradient]] [[vector field]] of the function&nbsp;{{math|''u''}}.\n\n==Properties and applications==\n\nSince it is the integral of a non-negative quantity, the Dirichlet energy is itself non-negative, i.e. {{math|''E''[''u''] &ge; 0}} for every function&nbsp;{{math|''u''}}.\n\nSolving Laplace's equation <math>-\\Delta u(x) = 0</math> for all <math>x \\in \\Omega</math>, subject to appropriate [[boundary conditions]], is equivalent to solving the [[calculus of variations|variational problem]] of finding a function&nbsp;{{math|''u''}} that satisfies the boundary conditions and has minimal Dirichlet energy.\n\nSuch a solution is called a [[harmonic function]] and such solutions are the topic of study in [[potential theory]].\n\n== See also ==\n* [[Dirichlet's principle]]\n* [[Total variation]]\n* [[Bounded mean oscillation|Oscillation]]\n* [[Harmonic map]]\n\n==References==\n*{{cite book | author=Lawrence C. Evans | title=Partial Differential Equations | publisher=American Mathematical Society | year=1998 | isbn=978-0821807729 }}\n\n[[Category:Calculus of variations]]\n[[Category:Partial differential equations]]"
    },
    {
      "title": "Dirichlet's principle",
      "url": "https://en.wikipedia.org/wiki/Dirichlet%27s_principle",
      "text": "{{distinguish|Pigeonhole principle}}\nIn [[mathematics]], and particularly in [[potential theory]], '''Dirichlet's principle''' is the assumption that the minimizer of a certain [[energy functional]] is a solution to [[Poisson's equation]].\n\n==Formal statement==\n'''Dirichlet's principle''' states that, if the function <math> u ( x ) </math> is the solution to [[Poisson's equation]] \n\n:<math>\\Delta u + f = 0</math>\n\non a [[domain of a function|domain]] <math>\\Omega</math> of <math>\\mathbb{R}^n</math> with [[boundary condition]] \n\n:<math>u=g\\text{ on }\\partial\\Omega,</math>\n\nthen ''u'' can be obtained as the minimizer of the [[Dirichlet's energy]] \n\n:<math>E[v(x)] = \\int_\\Omega \\left(\\frac{1}{2}|\\nabla v|^2 - vf\\right)\\,\\mathrm{d}x</math> \n\namongst all twice differentiable functions <math>v</math> such that <math>v=g</math> on <math>\\partial\\Omega</math> (provided that there exists at least one function making the Dirichlet's integral finite). This concept is named after the German mathematician [[Peter Gustav Lejeune Dirichlet]].\n\n==History==\nSince the Dirichlet's integral is bounded from below, the existence of an [[infimum]] is guaranteed. That this infimum is attained was taken for granted by [[Riemann]] (who coined the term ''Dirichlet's principle'') and others until [[Karl Weierstrass|Weierstrass]] gave an example of a functional that does not attain its minimum. [[David Hilbert|Hilbert]] later justified Riemann's use of Dirichlet's principle.\n\n==See also==\n*[[Dirichlet problem]]\n* [[Plateau's problem]]\n* [[Green's identities#Green's first identity|Green's first identity]]\n\n==References==\n*{{citation|last=Courant|first= R.|author-link=Richard Courant|title=Dirichlet's Principle, Conformal Mapping, and Minimal Surfaces. Appendix by M. Schiffer|publisher= Interscience |year= 1950}}\n* {{citation | author=Lawrence C. Evans | title=Partial Differential Equations | publisher=American Mathematical Society | year=1998 | isbn=978-0-8218-0772-9 }}\n* {{MathWorld | urlname=DirichletsPrinciple | title=Dirichlet's Principle}}\n\n[[Category:Calculus of variations]]\n[[Category:Partial differential equations]]\n[[Category:Harmonic functions]]\n[[Category:Mathematical principles]]"
    },
    {
      "title": "Energy principles in structural mechanics",
      "url": "https://en.wikipedia.org/wiki/Energy_principles_in_structural_mechanics",
      "text": "'''Energy principles in structural mechanics''' express the relationships between [[Stress (physics)|stress]]es, [[Strain (materials science)|strains]] or [[Deformation (mechanics)|deformation]]s, [[displacement (vector)|displacement]]s, material properties, and [[structural load|external effects]] in the form of energy or work done by internal and external forces. Since energy is a scalar quantity, these relationships provide convenient and alternative means for formulating the governing equations of deformable bodies in [[solid mechanics]]. They can also be used for obtaining approximate solutions of fairly complex systems, bypassing the difficult task of solving the set of governing [[partial differential equations]].\n\n==General principles==\n* [[Virtual work]] principle\n**Principle of virtual displacements\n**Principle of virtual forces\n***[[Unit dummy force method]]\n* Modified variational principles\n\n==Elastic systems==\n* [[Minimum total potential energy principle]]\n* Principle of stationary total complementary potential energy\n* [[Carlo Alberto Castigliano|Castigliano]]'s first theorem (for forces)\n\n==Linear elastic systems==\n* [[Castigliano's method|Castigliano's second theorem]] (for displacements)\n* [[Betti's theorem|Betti's reciprocal theorem]]\n* [[Müller-Breslau's principle]]\n\n==Applications==\n* Governing equations by variational principles\n* Approximate solution methods\n* [[Finite element method in structural mechanics]]\n\n==Bibliography==\n*Charlton, T.M.; ''Energy Principles in Theory of Structures'', Oxford University Press, 1973. {{ISBN|0-19-714102-1}} \n*Dym, C. L. and I. H. Shames; ''Solid Mechanics: A Variational Approach'', McGraw-Hill, 1973.\n*Hu, H. ''Variational Principles of Theory of Elasticity With Applications''; Taylor & Francis, 1984. {{ISBN|0-677-31330-6}} \n*Langhaar, H. L.; ''Energy Methods in Applied Mechanics'', Krieger, 1989.\n*Moiseiwitsch, B. L.; ''Variational Principles'', John Wiley and Sons, 1966. {{ISBN|0-470-61280-0}}\n*Mura, T.; ''Variational Methods in Mechanics'', Oxford University Press, 1992. {{ISBN|0-19-506830-0}} \n*[[J. N. Reddy|Reddy, J.N.]]; ''Energy Principles and Variational Methods in Applied Mechanics'', John Wiley, 2002. {{ISBN|0-471-17985-X}} \n*Shames, I. H. and Dym, C. L.; ''Energy and Finite Element Methods in Structural Mechanics'', Taylor & Francis, 1995, {{ISBN|0-89116-942-3}} \n*Tauchert, T.R.; ''Energy Principles in Structural Mechanics'', McGraw-Hill, 1974. {{ISBN|0-07-062925-0}}\n*Washizu, K.; ''Variational Methods in Elasticity and Plasticity'', Pergamon Pr, 1982. {{ISBN|0-08-026723-8}} \n*Wunderlich, W.; ''Mechanics of Structures: Variational and Computational Methods'', CRC, 2002. {{ISBN|0-8493-0700-7}}\n\n[[Category:Structural analysis]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Envelope theorem",
      "url": "https://en.wikipedia.org/wiki/Envelope_theorem",
      "text": "The '''envelope theorem''' is a result about the   differentiability properties of the objective function of a parameterized optimization problem. As we change parameters of the objective, the envelope theorem shows that, in a certain sense, changes in the optimizer of the objective do not contribute to the change in the objective function. The envelope theorem is an important tool for [[comparative statics]] of [[optimization]] models.<ref>{{cite book |first=Michael |last=Carter |title=Foundations of Mathematical Economics |location=Cambridge |publisher=MIT Press |year=2001 |isbn=978-0-262-53192-4 |pages=603–609 |url=https://books.google.com/books?id=KysvrGGfzq0C&pg=PA603 }}</ref>\n\n==Statement==\nLet <math>f(x,\\alpha)</math> and <math>g_{j}(x,\\alpha), j = 1,2, \\ldots, m</math> be real-valued continuously [[differentiable function]]s on <math>\\mathbb{R}^{n+l}</math>, where <math>x \\in \\mathbb{R}^{n}</math> are choice variables and <math>\\alpha \\in \\mathbb{R}^{l}</math> are parameters, and consider the problem of choosing <math>x</math>, for a given <math>\\alpha</math>, so as to:\n:<math> \\max_{x} f(x, \\alpha)</math> subject to <math>g_{j}(x,\\alpha) \\geq 0, j = 1,2, \\ldots, m</math> and <math>x \\geq 0</math>.\nThe Lagrangian expression of this problem is given by\n:<math>\\mathcal{L} (x, \\lambda, \\alpha) = f(x, \\alpha) + \\lambda \\cdot g(x, \\alpha)</math>\nwhere <math>\\lambda \\in \\mathbb{R}^{m}</math> are the Lagrange multipliers. Now let <math>x^{\\ast}(\\alpha)</math> and <math>\\lambda^{\\ast}(\\alpha)</math> together be the solution that maximizes the objective function ''f'' subject to the constraints (and hence are [[saddle point]]s of the Lagrangian),\n:<math>\\mathcal{L}^{\\ast} (\\alpha) \\equiv f(x^{\\ast}(\\alpha), \\alpha) + \\lambda^{\\ast}(\\alpha) \\cdot g(x^{\\ast}(\\alpha), \\alpha),</math>\nand define the value function\n:<math>V(\\alpha) \\equiv f(x^{\\ast}(\\alpha), \\alpha).</math>\nThen we have the following theorem.<ref name=\"Afriat, 1971\">{{cite journal |first=S. N. |last=Afriat |year=1971 |title=Theory of Maxima and the Method of Lagrange |journal=[[SIAM Journal on Applied Mathematics]] |volume=20 |issue=3 |pages=343–357 |doi=10.1137/0120037 }}</ref><ref>{{cite book |first=Akira |last=Takayama |title=Mathematical Economics |location=New York |publisher=Cambridge University Press |edition=Second |year=1985 |isbn=978-0-521-31498-5 |pages=137–138 |url=https://books.google.com/books?id=j6PLOBFotPQC&pg=PA137 }}</ref>\n\n'''Theorem:''' ''Assume that <math>V</math> and <math>\\mathcal{L}</math> are continuously differentiable. Then''\n:<math> \\frac{\\partial V(\\alpha)}{\\partial \\alpha_{k}} = \\frac{\\partial \\mathcal{L}^{\\ast} (\\alpha)}{\\partial \\alpha_{k}} = \\frac{\\partial \\mathcal{L} (x^{\\ast} (\\alpha), \\lambda^{\\ast} (\\alpha), \\alpha)}{\\partial \\alpha_{k}}, k = 1, 2, \\ldots, l</math>\n''where <math>\\partial \\mathcal{L} / \\partial \\alpha_{k} = \\partial f / \\partial \\alpha_{k} + \\lambda \\cdot \\partial g / \\partial \\alpha_{k}</math>''.\n\n==For arbitrary choice sets==\nLet <math>X</math> denote the choice set and let the relevant parameter be <math>t\\in \\lbrack 0,1]</math>. Letting <math>f:X\\times \\lbrack 0,1]\\rightarrow R</math> denote the parameterized objective function, the value function <math>V</math> and the optimal choice correspondence (set-valued function) <math>X^{\\ast }</math> are given by:\n\n:<math> V(t) =\\sup_{x\\in X}f(x,t)</math> (1)\n:<math> X^{\\ast }(t) =\\{x\\in X:f(x,t)=V(t)\\}</math> (2)\n\n\"Envelope theorems\" describe sufficient conditions for the value function <math>V</math> to be differentiable in the parameter <math>t</math> and describe its derivative as\n\n:<math> V^{\\prime }\\left( t\\right) =f_{t}\\left( x,t\\right) \\text{ for each }x\\in X^{\\ast }\\left( t\\right),</math> (3)\n\nwhere <math>f_{t}</math> denotes the partial derivative of <math>f</math> with respect to <math>t</math>. Namely, the derivative of the value function with respect to the parameter equals the partial derivative of the objective function with respect to <math>t</math> holding the maximizer fixed at its optimal level. (The term derives from describing the graph of <math>V</math> as the \"upper envelope\" of the graphs of the parameterized family of functions <math>\\left\\{ f\\left( x,\\cdot \\right) \\right\\} _{x\\in X}</math>.)\n\nTraditional envelope theorem derivations use the first-order condition for (1), which requires that the choice set <math>X</math> have the convex and topological structure, and the objective function <math>f</math> be differentiable in the variable <math>x</math>. (The argument is that changes in the maximizer have only a \"second-order effect\" at the optimum and so can be ignored.) However, in many applications such as the analysis of incentive constraints in contract theory and game theory, nonconvex production problems, and \"monotone\" or \"robust\" comparative statics, the choice sets and objective functions generally lack the topological and convexity properties required by the traditional envelope theorems.\n\n[[Paul Milgrom]] and Segal (2002) observe that the traditional envelope formula holds for optimization problems with arbitrary choice sets at any differentiability point of the value function,<ref name=\"Milgrom and Segal, 2002\" /> provided that the objective function is differentiable in the parameter:\n\n'''Theorem 1:''' Let <math>t\\in \\left( 0,1\\right) </math> and <math>x\\in X^{\\ast }\\left(t\\right) </math>. If both <math>V^{\\prime }\\left( t\\right) </math> and <math>f_{t}\\left(x,t\\right) </math> exist, the envelope formula (3) holds.\n\n'''Proof:''' (1) implies that for <math>x\\in X^{\\ast }\\left( t\\right) </math>,\n\n:<math> \\max_{s\\in \\left[ 0,1\\right] }\\left[ f\\left( x,s\\right) -V\\left( s\\right)\\right] =f\\left( x,t\\right) -V\\left( t\\right) =0. </math>\n\nUnder the assumptions, the objective function of the displayed maximization problem is differentiable at <math>s=t</math>, and the first-order condition for this maximization is exactly (3). Q.E.D.\n\nWhile differentiability of the value function in general requires strong assumptions, in many applications weaker conditions such as absolute continuity, differentiability almost everywhere, or left- and right-differentiability, suffice. In particular, Milgrom and Segal 's (2002) Theorem 2 offers a sufficient condition for <math>V</math> to be absolutely continuous,<ref name=\"Milgrom and Segal, 2002\" /> which means that it is differentiable almost everywhere and can be represented as an integral of its derivative:\n\n'''Theorem 2:''' Suppose that <math>f(x,\\cdot )</math> is absolutely continuous for all <math>x\\in X</math>. Suppose also that there exists an integrable function <math>b:[0,1]</math> <math>\\rightarrow </math> <math>\\mathbb{R}_{+}</math> such that <math>|f_{t}(x,t)|\\leq b(t)</math> for all <math>x\\in X</math> and almost all <math>t\\in \\lbrack 0,1]</math>. Then <math>V</math> is absolutely continuous. Suppose, in addition, that <math>f(x,\\cdot )</math> is differentiable for all <math>x\\in X</math>, and that <math>X^{\\ast }(t)\\neq \\varnothing </math> almost everywhere on <math>[0,1]</math>. Then for any selection <math>x^{\\ast }(t)\\in X^{\\ast }(t)</math>,\n\n:<math> V(t)=V(0)+\\int_{0}^{t}f_{t}(x^{\\ast }(s),s)ds. </math> (4)\n\n'''Proof:''' Using (1), observe that for any <math>t^{\\prime},t^{\\prime \\prime }\\in \\lbrack 0,1]</math> with <math>t^{\\prime }<t^{\\prime \\prime }</math>,\n\n:<math> |V(t^{\\prime \\prime })-V(t^{\\prime })| \\leq \\sup_{x\\in X}|f(x,t^{\\prime\\prime })-f(x,t^{\\prime })| =\\sup_{x\\in X}\\left\\vert \\int_{t^{\\prime }}^{t^{\\prime \\prime }}f_{t}(x,t)dt\\right\\vert \\leq \\int_{t^{\\prime }}^{t^{\\prime \\prime\n}}\\sup_{x\\in X}|f_{t}(x,t)|dt\\leq \\int_{t^{\\prime }}^{t^{\\prime \\prime }}b(t)dt.</math>\n\nThis implies that <math>V</math> is absolutely continuous. Therefore, <math>V</math> is differentiable almost everywhere, and using (3) yields (4). Q.E.D.\n\nThis result dispels the common misconception that nice behavior of the value function requires correspondingly nice behavior of the maximizer. Theorem 2 ensures the [[absolute continuity]] of the value function even though the maximizer may be discontinuous. In a similar vein, Milgrom and Segal's (2002) Theorem 3 implies that the value function must be differentiable at <math>t=t_{0}</math> and hence satisfy the envelope formula (3) when the family <math>\\left\\{ f\\left( x,\\cdot \\right) \\right\\} _{x\\in X}</math> is equi-differentiable at <math>t_{0}\\in \\left( 0,1\\right) </math> and <math>f_{t}\\left(X^{\\ast }\\left( t\\right) ,t_{0}\\right) </math> is single-valued and continuous at <math>t=t_{0}</math>, even if the maximizer is not differentiable at <math>t_{0}</math> (e.g., if <math>X </math> is described by a set of inequality constraints and the set of binding constraints changes at <math>t_{0}</math>).<ref name=\"Milgrom and Segal, 2002\" />\n\n==Applications==\n\n===Applications to producer theory===\nTheorem 1 implies [[Hotelling's lemma]] at any differentiability point of the profit function, and Theorem 2 implies the [[producer surplus]] formula. Formally, let <math>\\pi \\left( p\\right) </math> denote the profit function of a price-taking firm with production set <math>X\\subseteq \\mathbb{R}^{L}</math> facing prices <math>p\\in \\mathbb{R}^{L}</math>, and let <math>x^{\\ast }\\left( p\\right) </math> denote the firm's supply function, i.e.,\n\n:<math> \\pi (p)=\\max_{x\\in X}p\\cdot x=p\\cdot x^{\\ast }\\left( p\\right) \\text{.} </math>\n\nLet <math>t=p_{i}</math> (the price of good <math>i</math>) and fix the other goods' prices at <math>p_{-i}\\in \\mathbb{R}^{L-1}</math>. Applying Theorem 1 to <math>f(x,t)=tx_{i}+p_{-i}\\cdot x_{-i}</math> yields <math>\\frac{\\partial \\pi (p)}{\\partial p_{i}}=x_{i}^{\\ast }(p)</math> (the firm's optimal supply of good <math>i</math>). Applying Theorem 2 (whose assumptions are verified when <math>p_{i}</math> is restricted to a bounded interval) yields\n\n:<math>\\pi (t,p_{-i})-\\pi (0,p_{-i})=\\int_{0}^{p_{i}}x_{i}^{\\ast }(s,p_{-i})ds, </math>\n\ni.e. the producer surplus <math>\\pi (t,p_{-i})-\\pi (0,p_{-i})</math> can be obtained by integrating under the firm's supply curve for good <math>i</math>.\n\n===Applications to mechanism design and auction theory===\nConsider an agent whose utility function <math>f(x,t)</math> over outcomes <math>x\\in \\bar{X}</math> depends on his type <math>t\\in \\lbrack 0,1]</math>. Let <math>X\\subseteq \\bar{X}</math> represent the \"menu\" of possible outcomes the agent could obtain in the mechanism by sending different messages. The agent's equilibrium utility <math>V(t)</math> in the mechanism is then given by (1), and the set <math>X^{\\ast }(t)</math> of the mechanism's equilibrium outcomes is given by (2). Any selection <math>x^{\\ast }(t)\\in X^{\\ast }(t)</math> is a choice rule implemented by the mechanism. Suppose that the agent's utility function <math>f(x,t)</math> is differentiable and absolutely continuous in <math>t</math> for all <math>x\\in Y</math>, and that <math>\\sup_{x\\in \\bar{X}}|f_{t}(x,t)|</math> is integrable on <math>[0,1]</math>. Then Theorem 2 implies that the agent's equilibrium utility <math>V</math> in any mechanism implementing a given choice rule <math>x^{\\ast }</math> must satisfy the integral condition (4).\n\nThe integral condition (4) is a key step in the analysis of mechanism design problems with continuous type spaces. In particular, in Myerson's (1981) analysis of single-item auctions, the outcome from the viewpoint of one bidder can be described as <math>x=\\left( y,z\\right) </math>, where <math>y</math> is the bidder's probability of receiving the object and <math>z</math> is his expected payment, and the bidder's expected utility takes the form <math>f\\left( \\left( y,z\\right) ,t\\right) =ty-z</math>. In this case, letting <math>\\underline{t}</math> denote the bidder's lowest possible type, the integral condition (4) for the bidder's equilibrium expected utility <math>V</math> takes the form\n\n:<math> V(t)-V(\\underline{t})=\\int_{0}^{t}y^{\\ast }(s)ds. </math>\n\n(This equation can be interpreted as the producer surplus formula for the firm whose production technology for converting numeraire <math>z</math> into probability <math>y</math> of winning the object is defined by the auction and which resells the object at a fixed price <math>t</math>). This condition in turn yields Myerson's (1981) celebrated [[Revenue equivalence|revenue equivalence theorem]]: the expected revenue generated in an auction in which bidders have independent private values is fully determined by the bidders' probabilities <math>y^{\\ast }\\left( t\\right) </math> of getting the object for all types <math>t</math> as well as by the expected payoffs <math>V(\\underline{t})</math> of the bidders' lowest types. Finally, this condition is a key step in Myerson's (1981) of optimal auctions.<ref name=\"Myerson, 1981\" />\n\nFor other applications of the envelope theorem to mechanism design see Mirrlees (1971),<ref name=\"Mirrlees, 1971\">{{cite journal | author=Mirrlees, James | title= An Exploration in the Theory of Optimal Taxation| journal=Review of Economic Studies | year=2002| volume=38 | issue= 2| pages=175–208 | doi=10.2307/2296779| jstor= 2296779}}</ref> Holmstrom (1979),<ref name=\"Holmstrom, 1979\">{{cite journal | author=Holmstrom, Bengt | title=Groves Schemes on Restricted Domains| journal=Econometrica| year=1979| volume=47 | issue=5| pages=1137–1144 | doi=10.2307/1911954| jstor=1911954}}</ref> Laffont and Maskin (1980),<ref name=\"Laffont and Maskin, 1980\">{{cite journal |author1=Laffont, Jean-Jacques  |author2=Eric Maskin | title=A Differentiable Approach to Dominant Strategy Mechanisms| journal=Econometrica| year=1980| volume=48 |issue=6 | pages=1507–1520 | doi=10.2307/1912821|jstor=1912821 }}</ref> Riley and Samuelson (1981),<ref name=\"Riley and Samuelson, 1981\">{{cite journal |last1=Riley |first1=John G. |first2=William S. |last2=Samuelson | title=Optimal Auctions | journal=American Economic Review | year=1981| volume=71 | pages=381–392 | issue=3}}</ref> Fudenberg and Tirole (1991),<ref name=\"Fudenberg and Tirole, 1991\">{{cite book |author1=Fudenberg, Drew  |author2=Jean Tirole| title= Game Theory| year=1991 | publisher = Cambridge, MIT Press }}</ref> and Williams (1999).<ref name=\"Williams, 1999\">{{cite journal | author=Williams, Steven | title=A Characterization of Efficient, Bayesian Incentive Compatible Mechanism | journal=Economic Theory | year=1999| volume= 14| pages= 155–180 | doi=10.1007/s001990050286}}</ref> While these authors derived and exploited the envelope theorem by restricting attention to (piecewise) continuously differentiable choice rules or even narrower classes, it may sometimes be optimal to implement a choice rule that is not piecewise continuously differentiable. (One example is the class of trading problems with linear utility described in chapter 6.5 of Myerson (1991).<ref name=\"Myerson, 1991\">{{cite book | author= Myerson, Roger |title=Game Theory| year=1991 | publisher = Cambridge, Harvard University Press}}</ref>) Note that the integral condition (3) still holds in this setting and implies such important results as Holmstrom's lemma (Holmstrom, 1979),<ref name=\"Holmstrom, 1979\" /> Myerson's lemma (Myerson, 1981),<ref name=\"Myerson, 1981\">{{cite journal | author=Myerson, Roger | title= Optimal Auction Design| journal=Mathematics of Operations Research | year=1981| volume=6 | pages=58–73 | doi=10.1287/moor.6.1.58}}</ref> the revenue equivalence theorem (for auctions), the Green–Laffont–Holmstrom theorem (Green and Laffont, 1979; Holmstrom, 1979),<ref name=\"Green and Laffont, 1979\">{{cite book |last=Green |first=J. |last2=Laffont |first2=J. J. |title= Incentives in Public Decision Making| year=1979 | publisher = Amsterdam: North-Holland}}</ref><ref name=\"Holmstrom, 1979\" /> the Myerson–Satterthwaite inefficiency theorem (Myerson and Satterthwaite, 1983),<ref name=\"Myerson and Satterthwaite, 1983\">{{cite journal |author1=Myerson, R.  |author2=M. Satterthwaite| title=Efficient Mechanisms for Bilateral Trading | journal=Journal of Economic Theory| year=1983| volume=29 |issue=2| pages=265–281 | doi=10.1016/0022-0531(83)90048-0}}</ref> the Jehiel–Moldovanu impossibility theorems (Jehiel and Moldovanu, 2001),<ref name=\"Jehiel and Moldovanu, 2001\">{{cite journal |last=Jehiel |first=Philippe |first2=Benny |last2=Moldovanu | title=Efficient Design with Interdependent Valuations| journal=Econometrica| year=2001| volume=69 | pages=1237–1259| issue=5 | doi=10.1111/1468-0262.00240|citeseerx=10.1.1.23.7639}}</ref> the McAfee–McMillan weak-cartels theorem (McAfee and McMillan, 1992),<ref name=\"McAfee and McMillan, 1992\">{{cite journal |author1=McAfee, R. Preston  |author2=John McMillan | title=Bidding Rings| journal=American Economic Review | year=1992| volume=82 | pages=579–599| issue=3}}</ref> and Weber's martingale theorem (Weber, 1983),<ref name=\"Weber, 1983\">{{cite book | author= Weber, Robert |title=\"Multiple-Object Auctions\" in Auctions, Bidding, and Contracting: Uses and Theory| year=1983 | publisher = R. Engelbrecht-Wiggans, M. Shubik and R. M. Stark. New York, New York University Press|pages=165–191}}</ref> etc. The details of these applications are provided in Chapter 3 of Milgrom (2004),<ref name=\"Milgrom, 2004\">{{cite book | author= Milgrom, Paul |title= Putting Auction Theory to Work| year=2004 | publisher = Cambridge University Press}}</ref> who offers an elegant and unifying framework in auction and mechanism design analysis mainly based on the envelope theorem and other familiar techniques and concepts in demand theory.\n\n===Applications to multidimensional parameter spaces===\nFor a multidimensional parameter space <math>T\\subseteq \\mathbb{R}^{K}</math>, Theorem\n1 can be applied to partial and directional derivatives of the value\nfunction. If both the objective function <math>f</math> and the value function <math>V</math> are\n(totally) differentiable in <math>t</math>, Theorem 1 implies the envelope formula for\ntheir gradients: <math>\\nabla V\\left( t\\right) =\\nabla _{t}f\\left( x,t\\right) </math>\nfor each <math>x\\in X^{\\ast }\\left( t\\right) </math>. While total differentiability of\nthe value function may not be easy to ensure, Theorem 2 can be still applied\nalong any smooth path connecting two parameter values <math>t_{0}</math> and <math>t</math>.\nNamely, suppose that functions <math>f(x,\\cdot )</math> are differentiable for all <math>\nx\\in X</math> with <math>|\\nabla _{t}f(x,t)|\\leq B</math> for all <math>x\\in X,</math> <math>t\\in T</math>. A\nsmooth path from <math>t_{0}</math> to <math>t</math> is described by a differentiable mapping <math>\n\\gamma :\\left[ 0,1\\right] \\rightarrow T</math> with a bounded derivative, such\nthat <math>\\gamma \\left( 0\\right) =t_{0}</math> and <math>\\gamma \\left( 1\\right) =t</math>.\nTheorem 2 implies that for any such smooth path, the change of the value\nfunction can be expressed as the [[line integral|path integral]] of the partial gradient <math>\n\\nabla _{t}f(x^{\\ast }(t),t)</math> of the objective function along the path:\n\n:<math> V(t)-V(t_{0})=\\int_{\\gamma }\\nabla _{t}f(x^{\\ast }(s),s)\\cdot ds. </math>\n\nIn particular, for <math>t=t_{0}</math>, this establishes that cyclic path integrals\nalong any smooth path <math>\\gamma </math> must be zero:\n\n:<math>\\int \\nabla _{t}f(x^{\\ast }(s),s)\\cdot ds=0. </math>\n\nThis \"integrability condition\" plays an important role in mechanism design with multidimensional types, constraining what kind of choice rules <math>x^{\\ast }</math> can be sustained by mechanism-induced menus <math>X\\subseteq \\bar{X}</math>. In application to producer theory, with <math>x\\in X\\subseteq \\mathbb{R}^{L}</math> being the firm's production vector and <math>t\\in \\mathbb{R}^{L}</math> being the price vector, <math>f\\left( x,t\\right) =t\\cdot x</math>, and the integrability condition says that any rationalizable supply function <math>x^{\\ast }</math> must satisfy\n\n:<math> \\int x^{\\ast }(s)\\cdot ds=0. </math>\n\nWhen <math>x^{\\ast }</math> is continuously differentiable, this integrability condition is equivalent to the symmetry of the [[substitution matrix]] <math>\\left(\\partial x_{i}^{\\ast }\\left( t\\right) /\\partial t_{j}\\right) _{i,j=1}^{L}</math>. (In [[consumer theory]], the same argument applied to the expenditure minimization problem yields symmetry of the [[Slutsky matrix]].)\n\n===Applications to parameterized constraints===\nSuppose now that the feasible set <math>X\\left( t\\right) </math> depends on the parameter, i.e.,\n\n:<math> V(t) =\\sup_{x\\in X\\left( t\\right) }f(x,t) </math>\n:<math> X^{\\ast }(t) =\\{x\\in X\\left( t\\right) :f(x,t)=V(t)\\}\\text{, } </math>\n\nwhere <math> X\\left( t\\right) =\\left\\{ x\\in X:g\\left( x,t\\right) \\geq 0\\right\\}</math> for some <math> g:X\\times \\left[ 0,1\\right] \\rightarrow \\mathbb{R}^{K}. </math>\n\nSuppose that <math>X</math> is a convex set, <math>f</math> and <math>g</math> are concave in <math>x</math>, and there exists <math>\\hat{x}\\in X</math> such that <math>g\\left( \\hat{x},t\\right) >0</math> for all <math>t\\in \\left[ 0,1\\right] </math>. Under these assumptions, it is well known that the above constrained optimization program can be represented as a [[Saddle point|saddle-point problem]] for the Lagrangian <math>L\\left( x,\\lambda,t\\right) =f(x,t)+\\lambda\\cdot g\\left( x,t\\right) </math>, where <math>\\lambda \\in \\mathbb{R}_{+}^{K}</math> is the vector of [[Lagrange multiplier]]s chosen by the adversary to minimize the Lagrangian.<ref name=\"Luenberger, 1969\">{{cite book | author= Luenberger, D. G. |title= Optimization by Vector Space Methods| year=1969 | publisher = New York: John Wiley & Sons}}</ref>{{page needed|date=February 2017}}<ref name=\"Rockafellar, 1970\" />{{page needed|date=February 2017}} This allows the application of Milgrom and Segal's (2002, Theorem 4) envelope theorem for saddle-point problems,<ref name=\"Milgrom and Segal, 2002\">{{cite journal |author1=Milgrom, Paul  |author2=Ilya Segal | title= Envelope Theorems for Arbitrary Choice Sets| journal=Econometrica| year=2002| volume=70 | pages=583–601 | issue=2 | doi=10.1111/1468-0262.00296|citeseerx=10.1.1.217.4736 }}</ref> under the additional assumptions that <math>X</math> is a compact set in a normed linear space, <math>f</math> and <math>g</math> are continuous in <math>x</math>, and <math>f_{t}</math> and <math>g_{t}</math> are continuous in <math>\\left( x,t\\right) </math>. In particular, letting <math>\\left( x^{\\ast}(t),\\lambda^{\\ast }\\left( t\\right) \\right) </math> denote the Lagrangian's saddle point for parameter value <math>t</math>, the theorem implies that <math>V</math> is absolutely continuous and satisfies\n\n:<math> V(t)=V(0)+\\int_{0}^{t}L_{t}(x^{\\ast }(s),\\lambda^{\\ast }\\left( s\\right) ,s)ds. </math>\n\nFor the special case in which <math>f\\left( x,t\\right) </math> is independent of <math>t</math>, <math>K=1</math>, and <math>g\\left( x,t\\right) =h\\left( x\\right) +t</math>, the formula implies that <math>V^{\\prime }(t)=L_{t}(x^{\\ast }(t),\\lambda^{\\ast }\\left( t\\right) ,t)=\\lambda^{\\ast}\\left( t\\right) </math> for a.e. <math>t</math>. That is, the Lagrange multiplier <math>\\lambda^{\\ast}\\left( t\\right) </math> on the constraint is its \"[[shadow price]]\" in the optimization program.<ref name=\"Rockafellar, 1970\">{{cite book | author= Rockafellar, R. T. |title= Convex Analysis| year=1970 | publisher = Princeton: Princeton University Press}}</ref>{{page needed|date=February 2017}}\n\n===Other applications===\nMilgrom and Segal (2002) demonstrate that the generalized version of the envelope theorems can also be applied to convex programming, continuous optimization problems, saddle-point problems, and optimal stopping problems.<ref name=\"Milgrom and Segal, 2002\" />\n\n==See also==\n* [[Maximum theorem]]\n* [[Danskin's theorem]]\n* [[Hotelling's lemma]]\n* [[Roy's identity]]\n\n==References==\n{{Reflist|30em}}\n\n[[Category:Production economics]]\n[[Category:Calculus of variations]]\n[[Category:Economics theorems]]\n[[Category:Theorems in analysis]]"
    },
    {
      "title": "Euler–Lagrange equation",
      "url": "https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation",
      "text": "In the [[calculus of variations]], the '''Euler–Lagrange equation''', '''Euler's equation''',<ref>{{cite book|first=Charles|last=Fox|title=An introduction to the calculus of variations|publisher=Courier Dover Publications|year=1987|isbn=978-0-486-65499-7}}</ref> or '''Lagrange's equation''' (although the latter name is ambiguous—see [[Lagrange's formula (disambiguation)|disambiguation page]]), is a second-order [[partial differential equation]] whose solutions are the [[function (mathematics)|function]]s for which a given [[functional (mathematics)|functional]] is [[stationary point|stationary]]. It was developed by Swiss mathematician [[Leonhard Euler]] and Italian mathematician [[Joseph-Louis Lagrange]] in the 1750s.\n\nBecause a differentiable functional is stationary at its local [[maxima and minima]], the Euler–Lagrange equation is useful for solving [[optimization (mathematics)|optimization]] problems in which, given some functional, one seeks the function minimizing or maximizing it. This is analogous to [[Fermat's theorem (stationary points)|Fermat's theorem]] in [[calculus]], stating that at any point where a differentiable function attains a local extremum its [[derivative (mathematics)|derivative]] is zero.\n\nIn [[Lagrangian mechanics]], because of [[Hamilton's principle]] of stationary action, the evolution of a physical system is described by the solutions to the Euler–Lagrange equation for the [[action (physics)#Action (functional)|action]] of the system. In [[classical mechanics]], it is equivalent to [[Newton's laws of motion]], but it has the advantage that it takes the same form in any system of [[generalized coordinate]]s, and it is better suited to generalizations. In [[classical field theory]] there is an [[classical field theory#Lagrangian dynamics|analogous equation]] to calculate the dynamics of a [[field (physics)|field]].\n\n==History==\nThe Euler–Lagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the  [[tautochrone]] problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point.\n\nLagrange solved this problem in 1755  and sent the solution to Euler. Both further developed Lagrange's method and applied it to [[mechanics]], which led to the formulation of [[Lagrangian mechanics]]. Their correspondence ultimately led to the [[calculus of variations]], a term coined by Euler himself in 1766.<ref>[http://numericalmethods.eng.usf.edu/anecdotes/lagrange.pdf A short biography of Lagrange] {{webarchive|url=https://web.archive.org/web/20070714022022/http://numericalmethods.eng.usf.edu/anecdotes/lagrange.pdf |date=2007-07-14 }}</ref>\n\n==Statement==\nThe Euler–Lagrange equation is an equation satisfied by a function  '''''q'''''\nof a [[real number|real]] argument ''t'', which is a stationary point of the [[functional (mathematics)|functional]]\n\n:<math>\\displaystyle S(\\boldsymbol q) = \\int_a^b L(t,\\boldsymbol q(t),\\boldsymbol \\dot{q}(t))\\, \\mathrm{d}t</math>\n\nwhere:\n*<math>\\boldsymbol q</math> is the function to be found:\n:<math>\\begin{align}\n\\boldsymbol q \\colon [a, b] \\subset \\mathbb{R} & \\to     X \\\\\n                                 t & \\mapsto x = \\boldsymbol q(t)\n\\end{align}</math>\n:such that <math>\\boldsymbol q</math> is differentiable, <math>\\boldsymbol q(a) = \\boldsymbol x_a</math>, and <math>\\boldsymbol q(b) = \\boldsymbol x_{b} </math>;\n*<math>\\boldsymbol \\dot{q} </math>; is the derivative of <math>\\boldsymbol q</math>:\n*:<math>\\begin{align}\n\\dot{q}\\colon [a, b] & \\to     T_{q(t)}X \\\\\n               t & \\mapsto v = \\dot{q}(t)\n\\end{align}</math>\n:<math> T_{q(t)} X</math> denotes the [[tangent space]] to <math> X </math> at the point <math> q(t) </math>.\n* <math>L</math> is a real-valued function with [[continuous function|continuous]] first [[partial derivatives]]:\n*:<math>\\begin{align}\nL \\colon [a, b] \\times TX & \\to     \\mathbb{R} \\\\\n                         (t, x, v) & \\mapsto L(t, x, v).\n\\end{align}</math>\n:<math>TX</math> being the [[tangent bundle]] of <math>X</math> defined by \n: <math> TX = \\bigcup_{x \\in X} \\{ x \\} \\times T_{x}X </math> ;\n\nThe Euler–Lagrange equation, then, is given by\n{{Equation box 1\n|indent =:\n|equation = <math>L_x(t,q(t),\\dot{q}(t))-\\frac{\\mathrm{d}}{\\mathrm{d}t}L_v(t,q(t),\\dot{q}(t)) = 0.</math>\n|border colour = #50C878\n|background colour = #ECFCF4}} \nwhere <math>L_x</math> and <math>L_v</math> denote the partial derivatives of <math>L</math> with respect to the second and third arguments, respectively.\n\nIf the dimension of the space <math>X</math> is greater than 1, this is a system of differential equations, one for each component:\n:<math>\\frac{\\partial L}{\\partial q_i}(t,\\boldsymbol q(t),\\boldsymbol \\dot{q}(t))-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\partial L}{\\partial \\dot q_i}(t,\\boldsymbol q(t),\\boldsymbol \\dot{q}(t)) = 0\n\\quad \\text{for } i = 1, \\dots, n.</math>\n\n:{| class=\"toccolours collapsible collapsed\" width=\"60%\" style=\"text-align:left\"\n!Derivation of one-dimensional Euler–Lagrange equation\n|-\n|\nThe derivation of the one-dimensional Euler–Lagrange equation is one of the classic proofs in [[mathematics]]. It relies on the [[fundamental lemma of calculus of variations]].\n\nWe wish to find a function <math>f</math> which satisfies the boundary conditions <math>f(a) = A</math>, <math>f(b) = B</math>, and which extremizes the functional\n\n: <math> J = \\int_a^b F(x,f(x),f'(x))\\, \\mathrm{d}x\\ . </math>\n\nWe assume that <math>F</math> is twice continuously differentiable.<ref name='CourantP184'>{{harvnb|Courant|Hilbert|1953|p=184}}</ref> A weaker assumption can be used, but the proof becomes more difficult.{{Cn|date=September 2013}}\n\nIf <math>f</math> extremizes the functional subject to the boundary conditions, then any slight perturbation of <math>f</math> that preserves the boundary values must either increase <math>J</math> (if <math>f</math> is a minimizer) or decrease <math>J</math> (if <math>f</math> is a maximizer).\n\nLet <math>g_{\\varepsilon} (x) = f (x) + \\varepsilon \\eta (x)</math> be the result of such a perturbation <math>\\varepsilon \\eta (x)</math> of <math>f</math>, where <math>\\varepsilon</math> is small and <math>\\eta (x)</math> is a differentiable function satisfying <math>\\eta (a) = \\eta (b) = 0</math>. Then define\n\n: <math> J_\\varepsilon = \\int_a^b F(x,g_\\varepsilon(x), g_\\varepsilon'(x) ) \\, \\mathrm{d}x = \\int_a^b F_\\varepsilon\\, \\mathrm{d}x  </math>\n\nwhere <math> F_\\varepsilon = F(x, \\, g_\\varepsilon (x), \\, g_\\varepsilon' (x) ) </math> .\n\nWe now wish to calculate the [[total derivative]] of <math> J_\\varepsilon</math> with respect to ''ε''.\n\n: <math> \\frac{\\mathrm{d} J_\\varepsilon}{\\mathrm{d} \\varepsilon} = \\frac{\\mathrm d}{\\mathrm d\\varepsilon}\\int_a^b F_\\varepsilon\\, \\mathrm{d}x = \\int_a^b \\frac{\\mathrm{d} F_\\varepsilon}{\\mathrm{d}\\varepsilon} \\, \\mathrm{d}x </math>\n\nIt follows from the total derivative that\n\n:<math> \n\\begin{align}\n\\frac{\\mathrm d F_\\varepsilon}{\\mathrm d\\varepsilon} & =\\frac{\\mathrm d x}{\\mathrm d\\varepsilon}\\frac{\\partial F_\\varepsilon}{\\partial x} + \\frac{\\mathrm d g_\\varepsilon}{\\mathrm d\\varepsilon}\\frac{\\partial F_\\varepsilon}{\\partial g_\\varepsilon} + \\frac{\\mathrm d g_\\varepsilon'}{\\mathrm d\\varepsilon}\\frac{\\partial F_\\varepsilon}{\\partial g_\\varepsilon'} \\\\\n& = \\frac{\\mathrm d g_\\varepsilon}{\\mathrm d\\varepsilon}\\frac{\\partial F_\\varepsilon}{\\partial g_\\varepsilon}+\\frac{\\mathrm d g'_\\varepsilon}{\\mathrm d\\varepsilon}\\frac{\\partial F_\\varepsilon}{\\partial g'_\\varepsilon} \\\\\n& = \\eta(x) \\frac{\\partial F_\\varepsilon}{\\partial g_\\varepsilon} + \\eta'(x) \\frac{\\partial F_\\varepsilon}{\\partial g_\\varepsilon'} \\ . \\\\\n\\end{align}\n</math>\n\nSo\n\n: <math> \\frac{\\mathrm{d} J_\\varepsilon}{\\mathrm{d} \\varepsilon} = \\int_a^b \\left[\\eta(x) \\frac{\\partial F_\\varepsilon}{\\partial g_\\varepsilon} + \\eta'(x) \\frac{\\partial F_\\varepsilon}{\\partial g_\\varepsilon'} \\, \\right]\\,\\mathrm{d}x \\ . </math>\n\nWhen ''ε'' = 0 we have ''g''<sub>''ε''</sub> = ''f'', ''F<sub>ε</sub> =  F(x, f(x), f'(x))''   and ''J<sub>ε</sub>''&nbsp; has an [[extremum]] value, so that\n\n: <math> \\frac{\\mathrm d J_\\varepsilon}{\\mathrm d\\varepsilon}\\bigg|_{\\varepsilon=0}  = \\int_a^b \\left[ \\eta(x) \\frac{\\partial F}{\\partial f} + \\eta'(x) \\frac{\\partial F}{\\partial f'} \\,\\right]\\,\\mathrm{d}x = 0 \\ .</math>\n\nThe next step is to use [[integration by parts]] on the second term of the integrand, yielding\n\n: <math> \\int_a^b \\left[ \\frac{\\partial F}{\\partial f} - \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\partial F}{\\partial f'} \\right] \\eta(x)\\,\\mathrm{d}x + \\left[ \\eta(x) \\frac{\\partial F}{\\partial f'} \\right]_a^b = 0 \\ . </math>\n\nUsing the boundary conditions <math>\\eta (a) = \\eta (b) = 0</math>,\n\n: <math> \\int_a^b \\left[ \\frac{\\partial F}{\\partial f} - \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\partial F}{\\partial f'} \\right] \\eta(x)\\,\\mathrm{d}x = 0  \\ . </math>\n\nApplying the [[fundamental lemma of calculus of variations]] now yields the Euler–Lagrange equation\n\n: <math> \\frac{\\partial F}{\\partial f} - \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\partial F}{\\partial f'} = 0 \\ . </math>\n|}\n\n:{| class=\"toccolours collapsible collapsed\" width=\"60%\" style=\"text-align:left\"\n!Alternate derivation of one-dimensional Euler–Lagrange equation\n|-\n|\nGiven a functional\n\n:<math>J = \\int^b_aF(t, y(t), y'(t))\\,\\mathrm{d}t</math>\n\non <math>C^1([a, b])</math> with the boundary conditions <math>y(a) = A</math> and <math>y(b) = B</math>, we proceed by approximating the extremal curve by a polygonal line with <math>n</math> segments and passing to the limit as the number of segments grows arbitrarily large.\n\nDivide the interval <math>[a, b]</math> into <math>n</math> equal segments with endpoints <math>t_0 = a, t_1, t_2, \\ldots, t_n = b</math> and let <math>\\Delta t = t_k - t_{k - 1}</math>. Rather than a smooth function <math>y(t)</math> we consider the polygonal line with vertices <math>(t_0, y_0),\\ldots,(t_n, y_n)</math>, where <math>y_0 = A</math> and <math>y_n = B</math>. Accordingly, our functional becomes a real function of <math>n - 1</math> variables given by\n\n:<math>J(y_1, \\ldots, y_{n - 1}) \\approx \\sum^{n - 1}_{k = 0}F\\left(t_k, y_k, \\frac{y_{k + 1} - y_k}{\\Delta t}\\right)\\Delta t.</math>\n\nExtremals of this new functional defined on the discrete points <math>t_0,\\ldots,t_n</math> correspond to points where\n\n:<math>\\frac{\\partial J(y_1,\\ldots,y_n)}{\\partial y_m} = 0.</math>\n\nEvaluating this partial derivative gives\n\n:<math>\\frac{\\partial J}{\\partial y_m} = F_y\\left(t_m, y_m, \\frac{y_{m + 1} - y_m}{\\Delta t}\\right)\\Delta t + F_{y'}\\left(t_{m - 1}, y_{m - 1}, \\frac{y_m - y_{m - 1}}{\\Delta t}\\right) - F_{y'}\\left(t_m, y_m, \\frac{y_{m + 1} - y_m}{\\Delta t}\\right).</math>\n\nDividing the above equation by <math>\\Delta t</math> gives\n\n:<math>\\frac{\\partial J}{\\partial y_m \\Delta t} = F_y\\left(t_m, y_m, \\frac{y_{m + 1} - y_m}{\\Delta t}\\right) - \\frac{1}{\\Delta t}\\left[F_{y'}\\left(t_m, y_m, \\frac{y_{m + 1} - y_m}{\\Delta t}\\right) - F_{y'}\\left(t_{m - 1}, y_{m - 1}, \\frac{y_m - y_{m - 1}}{\\Delta t}\\right)\\right],</math>\n\nand taking the limit as <math>\\Delta t \\to 0</math> of the right-hand side of this expression yields\n\n:<math>F_y - \\frac{\\mathrm{d}}{\\mathrm{d}t}F_{y'} = 0.</math>\n\nThe left hand side of the previous equation is the [[functional derivative]] <math>\\delta J/\\delta y</math> of the functional <math>J</math>. A necessary condition for a differentiable functional to have an extremum on some function is that its functional derivative at that function vanishes, which is granted by the last equation.\n|}\n\n==Examples==\nA standard example is finding the real-valued function ''y'' on the interval [''a'', ''b''], such that ''y''(''a'') = ''c'' and ''y''(''b'') = ''d'', for which the  [[path (topology) | path]]  [[arc length|length]] along the [[Curve#length of a curve|curve]] traced by ''y'' is as short as possible. \n:<math> \\text{s} = \\int_{a}^{b} \\sqrt{1+y'^2}\\,\\mathrm{d}x,</math>\nthe integrand function being {{nowrap|1=''L''(''x'', ''y'', ''y''′) = {{radic|1 + ''y''′ ²}}}} .\n\nThe partial derivatives of ''L'' are:\n:<math>\\frac{\\partial L(x, y, y')}{\\partial y'} = \\frac{y'}{\\sqrt{1 + y'^2}} \\quad \\text{and} \\quad\n\\frac{\\partial L(x, y, y')}{\\partial y} = 0.</math>\nBy substituting these into the Euler–Lagrange equation, we obtain\n:<math> \n\\begin{align}\n\\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{y'(x)}{\\sqrt{1 + (y'(x))^2}} &= 0 \\\\ \n\\frac{y'(x)}{\\sqrt{1 + (y'(x))^2}} &= C = \\text{constant} \\\\\n\\Rightarrow y'(x)&= \\frac{C}{\\sqrt{1-C^2}} := A \\\\\n\\Rightarrow y(x) &= Ax + B\n\\end{align}\n</math>\nthat is, the function must have constant first derivative, and thus its [[graph of a function|graph]] is a [[straight line]].\n\n==Generalizations for several functions, several variables, and higher derivatives==\n\n===Single function of single variable with higher derivatives===\nThe stationary values of the functional\n:<math>\n   I[f] = \\int_{x_0}^{x_1} \\mathcal{L}(x, f, f', f'', \\dots, f^{(k)})~\\mathrm{d}x ~;~~ \n     f' := \\cfrac{\\mathrm{d}f}{\\mathrm{d}x}, ~f'' := \\cfrac{\\mathrm{d}^2f}{\\mathrm{d}x^2}, ~\n     f^{(k)} := \\cfrac{\\mathrm{d}^kf}{\\mathrm{d}x^k}\n </math>\ncan be obtained from the Euler–Lagrange equation<ref name=Courant>{{cite book | last1=Courant | first1=R | authorlink1=Richard Courant | last2=Hilbert | first2=D | authorlink2=David Hilbert | title = Methods of Mathematical Physics | volume = Vol. I | edition = First English | ref=harv | publisher = Interscience Publishers, Inc | year = 1953 | location = New York | isbn = 978-0471504474}}</ref> \n:<math>\n   \\cfrac{\\partial \\mathcal{L}}{\\partial f} - \\cfrac{\\mathrm{d}}{\\mathrm{d} x}\\left(\\cfrac{\\partial \\mathcal{L}}{\\partial f'}\\right) + \\cfrac{\\mathrm{d}^2}{\\mathrm{d} x^2}\\left(\\cfrac{\\partial \\mathcal{L}}{\\partial f''}\\right) - \\dots +\n  (-1)^k \\cfrac{\\mathrm{d}^k}{\\mathrm{d} x^k}\\left(\\cfrac{\\partial \\mathcal{L}}{\\partial f^{(k)}}\\right)  = 0 \n </math>\nunder fixed boundary conditions for the function itself as well as for the first <math>k-1</math> derivatives (i.e. for all <math>f^{(i)}, i \\in \\{0, ..., k-1\\}</math>). The endpoint values of the highest derivative <math>f^{(k)}</math> remain flexible.\n\n===Several functions of single variable with single derivative===\nIf the problem involves finding several functions (<math>f_1, f_2, \\dots, f_m</math>) of a single independent variable (<math>x</math>) that define an extremum of the functional\n:<math>\n    I[f_1,f_2, \\dots, f_m] = \\int_{x_0}^{x_1} \\mathcal{L}(x, f_1, f_2, \\dots, f_m, f_1', f_2', \\dots, f_m')~\\mathrm{d}x\n    ~;~~ f_i' := \\cfrac{\\mathrm{d}f_i}{\\mathrm{d}x}\n </math>\nthen the corresponding Euler–Lagrange equations are<ref name=Weinstock>{{cite book |last=Weinstock |first=R. |year=1952 |title=Calculus of Variations with Applications to Physics and Engineering |publisher=McGraw-Hill |location=New York }}</ref>\n:<math>\n   \\begin{align}\n     \\frac{\\partial \\mathcal{L}}{\\partial f_i} - \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_i'}\\right) = 0_i \n   \\end{align}\n</math>\n\n===Single function of several variables with single derivative===\nA multi-dimensional generalization comes from considering a function on n variables. If <math>\\Omega</math> is some surface, then\n\n: <math> \n   I[f] = \\int_{\\Omega} \\mathcal{L}(x_1, \\dots , x_n, f, f_{, 1}, \\dots , f_{, n})\\, \\mathrm{d}\\mathbf{x}\\,\\! ~;~~\n      f_{, j} := \\cfrac{\\partial f}{\\partial x_j}\n</math>\n\nis extremized only if ''f'' satisfies the [[partial differential equation]]\n\n: <math> \\frac{\\partial \\mathcal{L}}{\\partial f} - \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_j}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{, j}}\\right) = 0. </math>\n\nWhen ''n'' = 2 and functional <math>\\mathcal I</math> is the [[energy functional]], this leads to the soap-film [[minimal surface]] problem.\n\n===Several functions of several variables with single derivative===\nIf there are several unknown functions to be determined and several variables such that\n: <math> \n   I[f_1,f_2,\\dots,f_m] = \\int_{\\Omega} \\mathcal{L}(x_1, \\dots , x_n, f_1, \\dots, f_m, f_{1,1}, \\dots , f_{1,n},  \\dots, f_{m,1}, \\dots, f_{m,n}) \\, \\mathrm{d}\\mathbf{x}\\,\\! ~;~~\n      f_{i,j} := \\cfrac{\\partial f_i}{\\partial x_j}\n</math>\nthe system of Euler–Lagrange equations is<ref name=Courant/>\n: <math> \n  \\begin{align}\n    \\frac{\\partial \\mathcal{L}}{\\partial f_1} - \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_j}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{1,j}}\\right) &= 0_1 \\\\\n    \\frac{\\partial \\mathcal{L}}{\\partial f_2} - \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_j}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{2,j}}\\right) &= 0_2 \\\\\n    \\vdots \\qquad \\vdots \\qquad &\\quad \\vdots  \\\\\n    \\frac{\\partial \\mathcal{L}}{\\partial f_m} - \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_j}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{m,j}}\\right) &= 0_m.\n  \\end{align}\n </math>\n\n===Single function of two variables with higher derivatives===\nIf there is a single unknown function ''f'' to be determined that is dependent on two variables ''x''<sub>1</sub> and ''x''<sub>2</sub> and if the functional depends on higher derivatives of ''f'' up to ''n''-th order such that\n: <math>\n   \\begin{align}\n     I[f] & = \\int_{\\Omega} \\mathcal{L}(x_1, x_2, f, f_{,1}, f_{,2}, f_{,11}, f_{,12}, f_{,22},\n                                        \\dots, f_{,22\\dots 2})\\, \\mathrm{d}\\mathbf{x} \\\\\n     & \\qquad \\quad\n        f_{,i} := \\cfrac{\\partial f}{\\partial x_i} \\; , \\quad\n        f_{,ij} := \\cfrac{\\partial^2 f}{\\partial x_i\\partial x_j} \\; , \\;\\; \\dots\n   \\end{align}\n</math>\nthen the Euler–Lagrange equation is<ref name=Courant/>\n:<math>\n  \\begin{align}\n    \\frac{\\partial \\mathcal{L}}{\\partial f}\n    & - \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{,1}}\\right)\n      - \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{,2}}\\right) \n      + \\frac{\\partial^2}{\\partial x_1^2}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{,11}}\\right)\n      + \\frac{\\partial^2}{\\partial x_1\\partial x_2}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{,12}}\\right)\n      + \\frac{\\partial^2}{\\partial x_2^2}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{,22}}\\right) \\\\\n    & - \\dots\n      + (-1)^n \\frac{\\partial^n}{\\partial x_2^n}\\left(\\frac{\\partial \\mathcal{L}}{\\partial f_{,22\\dots 2}}\\right) = 0\n  \\end{align}\n </math>\nwhich can be represented shortly as:\n:<math>\n    \\frac{\\partial \\mathcal{L}}{\\partial f} +\\sum_{j=1}^n \\sum_{\\mu_1 \\leq \\ldots \\leq \\mu_j} (-1)^j \\frac{\\partial^j}{\\partial x_{\\mu_{1}}\\dots \\partial x_{\\mu_{j}}} \\left( \\frac{\\partial \\mathcal{L} }{\\partial f_{,\\mu_1\\dots\\mu_j}}\\right)=0\n </math>\nwherein <math>\\mu_1 \\dots \\mu_j</math> are indices that span the number of variables, that is, here they go from 1 to 2. Here summation over the <math>\\mu_1 \\dots \\mu_j</math> indices is only over <math>\\mu_1 \\leq \\mu_2 \\leq \\ldots \\leq \\mu_j</math> in order to avoid counting the same partial derivative multiple times, for example <math>f_{,12} = f_{,21}</math> appears only once in the previous equation.\n\n===Several functions of several variables with higher derivatives===\nIf there are ''p'' unknown functions ''f''<sub>i</sub> to be determined that are dependent on ''m'' variables ''x''<sub>1</sub> ... ''x''<sub>m</sub> and if the functional depends on higher derivatives of the ''f''<sub>i</sub> up to ''n''-th order such that\n:<math>\n   \\begin{align}\n     I[f_1,\\ldots,f_p] & = \\int_{\\Omega} \\mathcal{L}(x_1, \\ldots, x_n; f_1,\\ldots,f_p; f_{1,1},\\ldots,\n     f_{p,m}; f_{1,11},\\ldots, f_{p,mm};\\ldots; f_{p,m\\ldots m})\\, \\mathrm{d}\\mathbf{x} \\\\\n     & \\qquad \\quad\n        f_{i,\\mu} := \\cfrac{\\partial f_i}{\\partial x_\\mu} \\; , \\quad\n        f_{i,\\mu_1\\mu_2} := \\cfrac{\\partial^2 f_i}{\\partial x_{\\mu_1}\\partial x_{\\mu_2}} \\; , \\;\\; \\dots\n   \\end{align}\n</math>\n\nwhere <math>\\mu_1 \\dots \\mu_j</math> are indices that span the number of variables, that is they go from 1 to m. Then the Euler–Lagrange equation is\n\n:<math>\n    \\frac{\\partial \\mathcal{L}}{\\partial f_i} +\\sum_{j=1}^n \\sum_{\\mu_1 \\leq \\ldots \\leq \\mu_j} (-1)^j \\frac{\\partial^j}{\\partial x_{\\mu_{1}}\\dots \\partial x_{\\mu_{j}}} \\left( \\frac{\\partial \\mathcal{L} }{\\partial f_{i,\\mu_1\\dots\\mu_j}}\\right)=0\n </math>\n\nwhere the summation over the <math>\\mu_1 \\dots \\mu_j</math> is avoiding counting the same derivative <math> f_{i,\\mu_1\\mu_2} = f_{i,\\mu_2\\mu_1}</math> several times, just as in the previous subsection. This can be expressed more compactly as\n\n:<math>\n\\sum_{j=0}^n \\sum_{\\mu_1 \\leq \\ldots \\leq \\mu_j} (-1)^j \\partial_{ \\mu_{1}\\ldots \\mu_{j} }^j \\left( \\frac{\\partial \\mathcal{L} }{\\partial f_{i,\\mu_1\\dots\\mu_j}}\\right)=0\n </math>\n\n==Generalization to manifolds==\nLet <math>M</math> be a [[smooth manifold]], and let <math>C^\\infty([a,b])</math> denote the space of [[smooth functions]] <math>f:[a,b]\\to M</math>. Then, for functionals <math>S:C^\\infty ([a,b])\\to \\mathbb{R}</math> of the form\n:<math>\nS[f]=\\int_a^b (L\\circ\\dot{f})(t)\\,\\mathrm{d} t\n</math>\nwhere <math>L:TM\\to\\mathbb{R}</math> is the Lagrangian, the statement <math>\\mathrm{d} S_f=0</math> is equivalent to the statement that, for all <math>t\\in [a,b]</math>, each coordinate frame [[fiber bundle|trivialization]] <math>(x^i,X^i)</math> of a neighborhood of <math>\\dot{f}(t)</math> yields the following <math>\\dim M</math> equations:\n:<math>\n\\forall i:\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\partial L}{\\partial X^i}\\bigg|_{\\dot{f}(t)}=\\frac{\\partial L}{\\partial x^i}\\bigg|_{\\dot{f}(t)}\n</math>\n\n==See also==\n{{Wiktionary|Euler–Lagrange equation}}\n*[[Lagrangian mechanics]]\n*[[Hamiltonian mechanics]]\n*[[Analytical mechanics]]\n*[[Beltrami identity]]\n*[[Functional derivative]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{springer|title=Lagrange equations (in mechanics)|id=p/l057150}}\n* {{mathworld|urlname=Euler-LagrangeDifferentialEquation|title=Euler-Lagrange Differential Equation}}\n* {{planetmathref |id=1995|title=Calculus of Variations}}\n* {{cite book |last=Gelfand |first=Izrail Moiseevich |authorlink=Israel Gelfand |title=Calculus of Variations |publisher=Dover |year=1963 |isbn=0-486-41448-5}}\n* Roubicek, T.: ''[http://www.wiley-vch.de/books/sample/3527411887_c17.pdf Calculus of variations]. Chap.17 in: [http://www.wiley-vch.de/publish/en/books/forthcomingTitles/MA00/3-527-41188-7/?sID=nrgsqk516u2v9ffab8u7io1dq4 Mathematical Tools for Physicists]. (Ed. M. Grinfeld) J. Wiley, Weinheim, 2014, {{ISBN|978-3-527-41188-7}}, pp.551-588.\n\n{{DEFAULTSORT:Euler-Lagrange Equation}}\n[[Category:Ordinary differential equations]]\n[[Category:Partial differential equations]]\n[[Category:Calculus of variations]]\n[[Category:Articles containing proofs]]\n[[Category:Leonhard Euler]]"
    },
    {
      "title": "Fermat's principle",
      "url": "https://en.wikipedia.org/wiki/Fermat%27s_principle",
      "text": "[[Image:Snells law.svg|thumb|upright=1.14|Fermat's principle leads to [[Snell's law]]; when the sines of the angles in the different media are in the same proportion as the propagation velocities, the time to get from P to Q is minimized.]]\n\nIn [[optics]], '''Fermat's principle''' or the '''principle of least time''', named after French mathematician [[Pierre de Fermat]], is the principle that the path taken between two points by a ray of light is the path that can be traversed in the least time.  This principle is sometimes taken as the definition of a ray of light.<ref>[[Arthur Schuster]], ''An Introduction to the Theory of Optics'', London: Edward Arnold, 1904 [https://books.google.com/books?vid=OCLC03146755&id=X0AcBd-bcCwC&pg=PA41&lpg=PA41&dq=fermat%27s-principle online].</ref> However, this version of the principle is not general; a more modern statement of the principle is that rays of light traverse the path of stationary optical length with respect to variations of the path.<ref>{{Citation\n|last = Ghatak\n|first = Ajoy\n|year = 2009\n|title = Optics\n|edition = 4th\n|isbn = 0-07-338048-2\n}}\n</ref> In other words, a ray of light follows the path such that there are other paths, arbitrarily nearby on either side, along which the ray would take almost exactly the same time to traverse.\n\nFermat's principle can be used to describe the properties of light rays [[Reflection (physics)|reflected]] off mirrors, [[refraction|refracted]] through different media, or undergoing [[total internal reflection]]. It follows mathematically from [[Huygens' principle]] (at the limit of small [[wavelength]]). Fermat's text ''Analyse des réfractions'' exploits the technique of [[adequality]] to derive [[Snell's law]] of [[refraction]]<ref>{{citation\n | last1 = Katz | first1 = Mikhail G.\n | author1-link = Mikhail Katz\n | last2 = Schaps | first2 = David\n | last3 = Shnider | first3 = Steve\n | author3-link = Steve Shnider\n | arxiv = 1210.7750\n | doi = \n | issue = 3\n | journal = [[Perspectives on Science]]\n | pages = 7750\n | title = Almost Equal: The Method of Adequality from Diophantus to Fermat and Beyond\n | volume = 21\n | year = 2013|bibcode = 2012arXiv1210.7750K\n }}</ref> and the [[law of reflection]].\n\nFermat's principle has the same form as [[Hamilton's principle]] and it is the basis of [[Hamiltonian optics]].\n\n==Modern version==\nThe time T a point of the electromagnetic wave needs to cover a path between the points '''A''' and '''B''' is given by:\n\n:<math>T=\\int_{\\mathbf{t_0}}^{\\mathbf{t_1}} \\, dt = \\frac{1}{c} \\int_{\\mathbf{t_0}}^{\\mathbf{t_1}} \\frac{c}{v} \\frac{ds}{dt}\\, dt = \\frac{1}{c} \\int_{\\mathbf{A}}^{\\mathbf{B}} n\\, ds</math>\n\n''c'' is the [[speed of light]] in vacuum, ''ds'' an infinitesimal displacement along the ray, ''v'' = ''ds''/''dt'' the speed of light in a medium and ''n'' = ''c''/''v'' the [[refractive index]] of that medium, <math>t_0</math> is the starting time (the wave front is in '''A'''), <math>t_1</math> is the arrival time at '''B'''. The optical path length of a ray from a point '''A''' to a point '''B''' is defined by:\n\n:<math>S=\\int_{\\mathbf{A}}^{\\mathbf{B}} n\\, ds</math>\n\nand it is related to the travel time by ''S'' = ''cT''. The optical path length is a purely geometrical quantity since time is not considered in its calculation. An [[extremum]] in the light travel time between two points '''A''' and '''B''' is equivalent to an extremum of the optical path length between those two points. The historical form proposed by Fermat is incomplete. A complete modern statement of the variational Fermat principle is that {{quote|the optical length of the path followed by light between two fixed points, '''A''' and '''B''', is an extremum. The optical length is defined as the physical length multiplied by the refractive index of the material.\"<ref>R. Marques, F. Martin, and M. Sorolla. Metamaterials with Negative Parameters. Wiley, 2008.</ref>}} In the context of [[calculus of variations]] this can be written as\n\n:<math>\\delta S= \\delta\\int_{\\mathbf{A}}^{\\mathbf{B}} n \\, ds =0 </math>\n\nIn general, the refractive index is a [[scalar field]] of position in space, that is, <math>n=n\\left(x_1,x_2,x_3\\right)</math> in [[Three-dimensional space|3D]] [[euclidean space]]. Assuming now that light has a component that travels along the ''x''<sub>3</sub> axis, the path of a light ray may be parametrized as <math>s=\\left(x_1\\left(x_3\\right),x_2\\left(x_3\\right),x_3\\right)</math> and\n\n:<math>nds=n \\frac{\\sqrt{dx_1^2+dx_2^2+dx_3^2}}{dx_3}dx_3=n \\sqrt{1+\\dot{x}_1^2+\\dot{x}_2^2} \\ dx_3</math>\n\nwhere <math>\\dot{x}_k=dx_k/dx_3</math>. The principle of Fermat can now be written as\n\n:<math>\\begin{align}\\delta S &= \\delta\\int_{x_{3A}}^{x_{3B}} n\\left(x_1,x_2,x_3\\right) \\sqrt{1+\\dot{x}_1^2+\\dot{x}_2^2}\\, dx_3\n\\\\&= \\delta\\int_{x_{3A}}^{x_{3B}} L\\left(x_1\\left(x_3\\right),x_2\\left(x_3\\right),\\dot{x}_1\\left(x_3\\right),\\dot{x}_2\\left(x_3\\right),x_3\\right)\\, dx_3\\\\ &=0\\end{align}</math>\n\nwhich has the same form as [[Hamilton's principle]] but in which ''x''<sub>3</sub> takes the role of time in [[classical mechanics]]. Function <math>L\\left(x_1,x_2,\\dot{x}_1,\\dot{x}_2,x_3\\right)</math> is the optical [[Lagrangian mechanics|Lagrangian]] from which the Lagrangian and Hamiltonian (as in [[Hamiltonian mechanics]]) formulations of geometrical optics may be derived.<ref name=\"IntroNio2e\">{{cite book | first = Julio | last = Chaves | title = Introduction to Nonimaging Optics, Second Edition |url=https://books.google.com/books?id=e11ECgAAQBAJ | publisher = [[CRC Press]] |  year = 2015 | isbn = 978-1482206739}}</ref>\n\n==Derivation==\n\nClassically, Fermat's principle can be considered as a mathematical consequence of [[Huygens' principle]]. Indeed, of all secondary waves (along all possible paths) the waves with the extremal (stationary) paths contribute most due to constructive interference. Suppose that light waves propagate from A to B by all possible routes AB<sub>j</sub>, unrestricted initially by rules of geometrical or physical optics. The various optical paths AB<sub>j</sub> will vary by amounts greatly in excess of one wavelength, and so the waves arriving at B will have a large range of phases and will tend to interfere destructively. But if there is a shortest route AB<sub>0</sub>, and the optical path varies smoothly through it, then a considerable number of neighboring routes close to AB<sub>0</sub> will have optical paths differing from AB<sub>0</sub> by second-order amounts only and will therefore interfere constructively. Waves along and close to this shortest route will thus dominate and AB<sub>0</sub> will be the route along which the light is seen to travel.<ref>Ariel Lipson, Stephen G. Lipson, Henry Lipson, ''Optical Physics 4th Edition'', Cambridge University Press, {{ISBN|978-0-521-49345-1}}.</ref>\n\nFermat's principle is the main principle of [[quantum electrodynamics]] which states that any particle (e.g. a photon or an electron) propagates over all available, unobstructed paths and that the interference, or superposition, of its wavefunction over all those paths at the point of observation gives the probability of detecting the particle at this point. Thus, because the extremal paths (shortest, longest, or stationary) cannot be completely canceled out, they contribute most to this interference. In humans, for example, Fermat's principle can be demonstrated in a situation when a lifeguard has to find the fastest way to traverse both beach and water in order to reach a drowning swimmer.<ref>{{cite web| url=http://nautil.us/blog/to-save-drowning-people-ask-yourself-what-would-light-do | title=To Save Drowning People, Ask Yourself \"What Would Light Do?\" | author=Aatish Bhatia | date=24 March 2014 | publisher=Nautilus | access-date=11 July 2016}}</ref> The principle has been tested in studies with ants, in which the ants' nest is on one end of a container and food is on the opposite end, but the ants choose to follow the path of least time, rather than the most direct path.<ref>{{cite web| url=http://phys.org/news/2013-04-ants-fermat-principle.html | title=Ants follow Fermat's principle of least time | author=Lisa Zyga | date=1 April 2013 | publisher=Phys.org | access-date=11 July 2016}}</ref>\n\nIn the [[classic mechanics]] of [[wave]]s, Fermat's principle follows from the [[extremum principle of mechanics]] (see [[variational principle]]).\n\n==History==\n[[Euclid]], c. 320 BCE in his [http://web.calstatela.edu/faculty/hmendel/Ancient%20Mathematics/Euclid/Catoptrics/Catoptrics%201/EuclidCatoptrics.1.html Catoptrics] (on [http://web.calstatela.edu/faculty/hmendel/Ancient%20Mathematics/Euclid/Catoptrics/Catoptrics.19/Catoptrics.19.html mirrors], including [http://web.calstatela.edu/faculty/hmendel/Ancient%20Mathematics/Euclid/Catoptrics/Catoptrics.30/Catoptrics.30.html spherical mirrors]) and Optics, laid the foundations for reflection, which was repeated by [[Ptolemy]], and then in his more detailed books that have surfaced, [[Hero of Alexandria]] (Heron) (c. 60) described the principle of reflection, which stated that a ray of light that goes from point A to point B, suffering any number of reflections on flat mirrors in the same medium, has a smaller path length than any nearby path.<ref>History of Geometric Optics/Richard Fitzpatrick</ref>\n\n[[Ibn al-Haytham]] (Alhacen), in his ''[[Book of Optics]]'' (1021), expanded the principle to both reflection and refraction, and expressed an early version of the principle of least time. His experiments were based on earlier works on refraction carried out by the Greek scientist [[Ptolemy]].<ref>Pavlos Mihas (2005). [http://www.ihpst2005.leeds.ac.uk/papers/Mihas.pdf Use of History in Developing ideas of refraction, lenses and rainbow] {{webarchive|url=https://web.archive.org/web/20070927032208/http://www.ihpst2005.leeds.ac.uk/papers/Mihas.pdf |date=2007-09-27 }}, Demokritus University, Thrace, Greece.</ref>\n[[File:Pierre de Fermat2.png|thumb|140px|Pierre de Fermat]]\nThe generalized principle of least time in its modern form was stated by Fermat in a letter dated January 1, 1662, to [[Cureau de la Chambre]].<ref>[[Michael Sean Mahoney]], ''The Mathematical Career of Pierre de Fermat, 1601-1665'', 2nd edition (Princeton University Press, 1994), p. 401</ref> It was met with objections by [[Claude Clerselier]] in May 1662, an expert in optics and leading spokesman for the Cartesians at the time. Amongst his objections, Clerselier states:\n\n<blockquote>\n''... The principle which you take as the basis for your proof, namely that Nature always acts by using the simplest and shortest paths, is merely a moral, and not a physical one.  It is not, and cannot be, the cause of any effect in Nature.\n</blockquote>\nThe original French, from Mahoney, is as follows:\n\n<blockquote>\n''Le principe que vous prenez pour fondement de votre démonstration, à savoir que la nature agit toujours par les voies les plus courtes et les plus simples, n’est qu’un principe moral et non point physique, qui n’est point et qui ne peut être la cause d’aucun effet de la nature.''\n</blockquote>\n\nAlthough Fermat's principle does not hold standing alone, we now know it can be derived from earlier principles such as [[Huygens' principle]].\n\nHistorically, Fermat's principle has served as a guiding principle in the formulation of physical laws with the use of variational calculus (see [[Principle of least action]]).\n\n==See also==\n*[[Adequality]]\n*[[Eikonal equation]]\n*[[Fermat’s and energy variation principles in field theory]]\n*[[Geodesic]]\n*[[Hamilton's principle]]\n*[[Huygens' principle]]\n*[[Path integral formulation]]\n*[[Pierre de Fermat]]\n*[[Principle of least action]]\n*[[Snell's law]]\n\n==Notes==\n<references/>\n\n{{DEFAULTSORT:Fermat's Principle}}\n[[Category:Geometrical optics]]\n[[Category:Calculus of variations]]\n[[Category:Principles]]"
    },
    {
      "title": "First variation",
      "url": "https://en.wikipedia.org/wiki/First_variation",
      "text": "In applied [[mathematics]] and the [[calculus of variations]], the '''first variation''' of a [[Functional (mathematics)|functional]] ''J''(''y'') is defined as the linear functional <math> \\delta J(y) </math> mapping the function ''h'' to\n\n:<math>\\delta J(y,h) = \\lim_{\\varepsilon\\to 0} \\frac{J(y + \\varepsilon h)-J(y)}{\\varepsilon} = \\left.\\frac{d}{d\\varepsilon} J(y + \\varepsilon h)\\right|_{\\varepsilon = 0},</math>\n\nwhere ''y'' and ''h'' are functions, and ''ε'' is a scalar. This is recognizable as the [[Gateaux derivative]] of the functional.\n\n==Example==\n\nCompute the first variation of\n\n:<math>J(y)=\\int_a^b yy' dx.</math>\n\nFrom the definition above,\n\n:<math>\n\\begin{align}\n\\delta J(y,h)&=\\left.\\frac{d}{d\\varepsilon} J(y + \\varepsilon h)\\right|_{\\varepsilon = 0}\\\\\n&= \\left.\\frac{d}{d\\varepsilon} \\int_a^b (y + \\varepsilon h)(y^\\prime + \\varepsilon h^\\prime) \\ dx\\right|_{\\varepsilon = 0}\\\\\n&= \\left.\\frac{d}{d\\varepsilon} \\int_a^b (yy^\\prime + y\\varepsilon h^\\prime + y^\\prime\\varepsilon h + \\varepsilon^2 hh^\\prime) \\ dx\\right|_{\\varepsilon = 0}\\\\\n&= \\left.\\int_a^b \\frac{d}{d\\varepsilon} (yy^\\prime + y\\varepsilon h^\\prime + y^\\prime\\varepsilon h + \\varepsilon^2 hh^\\prime) \\ dx\\right|_{\\varepsilon = 0}\\\\\n&= \\left.\\int_a^b (yh^\\prime + y^\\prime h + 2\\varepsilon hh^\\prime) \\ dx\\right|_{\\varepsilon = 0}\\\\\n&= \\int_a^b (yh^\\prime + y^\\prime h) \\ dx\n\\end{align}\n</math>\n\n== See also ==\n*[[Calculus of variations]]\n*[[Functional derivative]]\n\n\n[[Category:Calculus of variations]]\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Functional derivative",
      "url": "https://en.wikipedia.org/wiki/Functional_derivative",
      "text": "In the [[calculus of variations]], a field of [[mathematical analysis]], the '''functional derivative''' (or '''variational derivative''')<ref name=\"GiaquintaHildebrandtP18\">{{harv|Giaquinta|Hildebrandt|1996|p=18}}</ref> relates a change in a [[Functional (mathematics)|functional]] to a change in a [[Function (mathematics)|function]] on which the functional depends.\n\nIn the calculus of variations, functionals are usually expressed in terms of an [[integral]] of  functions, their [[Argument of a function|arguments]],  and their [[derivative]]s. In an integral {{math|''L''}} of a functional, if a function {{math|''f''}}   is varied by adding to it another function {{math|''δf''}} that is arbitrarily small, and the resulting integrand is expanded in powers of {{math|''δf''}}, the coefficient of {{math|''δf''}} in the first order term is called the functional derivative.\n\nFor example, consider the functional\n:<math> J[f] = \\int_a^b L[ \\, x, f(x), f \\, '(x) \\, ] \\, dx \\ , </math>\nwhere {{math|''f'' &prime;(''x'') &equiv; ''df/dx''}}.  If {{math|''f''}} is varied by adding to it a function {{math|''δf''}}, and the resulting integrand {{math|''L''(''x, f +δf, f '+δf'' &prime;)}} is expanded in powers of {{math|''δf''}}, then the change in the value of {{math|''J''}} to first order in {{math|''δf''}} can be expressed as follows:<ref name=\"GiaquintaHildebrandtP18\" /><ref Group = 'Note'>According to {{harvtxt|Giaquinta|Hildebrandt|1996|p=18}}, this notation is customary in [[Physics|physical]] literature.</ref>\n:<math>  \\delta J = \\int_a^b  \\frac{\\delta J}{\\delta f(x)} {\\delta f(x)} \\, dx \\, . </math> \nThe coefficient of {{math|''δf(x)''}}, denoted as {{math|''δJ''/''δf(x)''}}, is called the '''functional derivative''' of {{math|''J''}} with respect to {{math|''f''}} at the point {{math|''x''}}.<ref name=ParrYangP246>{{harv|Parr|Yang|1989|p=246}}.</ref> For this example functional, the functional derivative is the left hand side of the [[Calculus of variations#Euler.E2.80.93Lagrange equation|Euler-Lagrange equation]],<ref name=GelfandFominP28>{{harv|Gelfand|Fomin|2000|p=28}}</ref>\n:<math> \\frac{\\delta J}{\\delta f(x)} = \\frac{\\partial L}{\\partial f} -\\frac{d}{dx} \\frac{\\partial L}{\\partial f'} \\, . </math>\n\n==Definition==\nIn this section, the functional derivative is defined. Then the functional differential is defined in terms of the functional derivative.\n\n===Functional derivative===\nGiven a [[manifold]] {{math|''M''}} representing ([[continuous function (topology)|continuous]]/[[smooth function|smooth]]) functions {{math|''ρ''}}  (with certain [[boundary condition]]s etc.), and a [[functional (mathematics)|functional]] {{math|''F''}} defined as \n:<math>F\\colon M \\rightarrow \\mathbb{R} \\quad \\mbox{or} \\quad F\\colon M \\rightarrow \\mathbb{C} \\, ,</math>\n\nthe '''functional derivative''' of {{math|''F''[}}''ρ''], denoted {{math|''δF/δρ''}}, is defined by<ref name=ParrYangP246A.2>{{harv|Parr|Yang|1989|loc= p. 246, Eq. A.2}}.</ref>\n\n:<math>\n\\begin{align}\n \\int \\frac{\\delta F}{\\delta\\rho}(x) \\phi(x) \\; dx \n&= \\lim_{\\varepsilon\\to 0}\\frac{F[\\rho+\\varepsilon \\phi]-F[\\rho]}{\\varepsilon} \\\\\n&= \\left [ \\frac{d}{d\\epsilon}F[\\rho+\\epsilon \\phi]\\right ]_{\\epsilon=0},\n\\end{align}\n</math>\n\nwhere <math>\\phi</math> is an arbitrary function. The quantity {{math|''εϕ''}} is called the variation of {{math|''ρ''}}. In other words,\n:<math>\\phi \\mapsto \\left [ \\frac{d}{d\\epsilon}F[\\rho+\\epsilon \\phi]\\right ]_{\\epsilon=0}</math>\n\nis a linear functional, so by the [[Riesz–Markov–Kakutani representation theorem]], this functional is given by integration against some [[measure (mathematics)|measure]].\nThen {{math|''δF''/''δρ''}} is defined to be the [[Radon&ndash;Nikodym derivative]] of this measure.\n\nWe think of the function {{math|''δF''/''δρ''}} as the gradient of {{math|''F''}} at the point {{math|''ρ''}} and \n:<math>\\int \\frac{\\delta F}{\\delta\\rho}(x) \\phi(x) \\; dx</math> \nas the directional derivative at point {{math|''ρ''}} in the direction of {{math|''ϕ''}}. Then analogous to vector calculus, the inner product with the gradient gives the directional derivative.\n\n===Functional differential===\nThe differential (or variation or first variation) of the functional <math>F\\left[\\rho\\right]</math> is <ref name=ParrYangP246A.1>{{harv|Parr|Yang|1989|loc= p. 246, Eq. A.1}}.</ref> <Ref Group = 'Note' > Called ''differential'' in {{harv|Parr|Yang|1989|p=246}}, ''variation'' or ''first variation'' in {{harv|Courant|Hilbert|1953|p=186}}, and ''variation'' or ''differential'' in {{harv|Gelfand|Fomin|2000|loc= p. 11, &sect; 3.2}}.</ref>\n:<math>\n\\delta F [\\rho; \\phi] = \\int   \\frac {\\delta F} {\\delta \\rho}(x) \\ \\phi(x)   \\ dx \\ .\n</math>\nHeuristically, <math>\\phi</math> is the change in <math>\\rho</math>, so we 'formally' have <math>\\phi=\\delta\\rho</math>, and then\nthis is similar in form to the [[total differential]] of a function <math>F[\\rho_1,\\rho_2,\\dots,\\rho_n]</math>,\n:<math> dF =   \\sum_{i=1} ^n  \\frac {\\partial F} {\\partial \\rho_i} \\ d\\rho_i  \\ ,</math>\nwhere <math>\\rho_1,\\rho_2,\\dots,\\rho_n</math> are independent variables. \nComparing the last two equations, the functional derivative <math>\\delta F/\\delta\\rho(x)</math> has a role similar to that of the partial derivative <math>\\partial F/\\partial\\rho_i</math>, where the variable of integration <math>x</math> is like a continuous version of the summation index <math>i</math>.<ref name=ParrYangP246 />\n\n===Formal description===\nThe definition of a functional derivative may be made more mathematically precise and formal by defining the [[topological vector space|space of functions]] more carefully. For example, when the space of functions is a [[Banach space]], the functional derivative becomes known as the [[Fréchet derivative]], while one uses the [[Gateaux derivative]] on more general [[locally convex space]]s. Note that [[Hilbert space]]s are special cases of [[Banach space]]s. The more formal treatment allows many theorems from ordinary [[calculus]] and [[Mathematical analysis|analysis]] to be generalized to corresponding theorems in [[functional analysis]], as well as numerous new theorems to be stated.\n\n==Properties==\nLike the derivative of a function, the functional derivative satisfies the following properties, where {{math|''F''}}[''ρ''] and {{math|''G''}}[''ρ''] are functionals:<ref group=\"Note\">\nHere the notation\n<math>\n\\frac{\\delta{F}}{\\delta\\rho}(x) \\equiv \\frac{\\delta{F}}{\\delta\\rho(x)}\n</math>\nis introduced.\n</ref>\n* Linearity:<ref name=ParrYangP247A.3>{{harv|Parr|Yang|1989|loc= p. 247, Eq. A.3}}.</ref>\n: <math>\\frac{\\delta(\\lambda F + \\mu G)[\\rho ]}{\\delta \\rho(x)} = \\lambda \\frac{\\delta F[\\rho]}{\\delta \\rho(x)} + \\mu \\frac{\\delta G[\\rho]}{\\delta \\rho(x)},</math>\nwhere {{math|''λ'', ''μ''}} are constants.\n\n* Product rule:<ref name=ParrYangP247A.4>{{harv|Parr|Yang|1989|loc= p. 247,  Eq. A.4}}.</ref>\n: <math>\\frac{\\delta(FG)[\\rho]}{\\delta \\rho(x)} = \\frac{\\delta F[\\rho]}{\\delta \\rho(x)} G[\\rho] + F[\\rho] \\frac{\\delta G[\\rho]}{\\delta \\rho(x)} \\, , </math>\n\n* Chain rules: \n:If {{math|''F''}} is a functional and {{math|''G''}} another functional, then<ref>{{harv|Greiner|Reinhardt|1996|loc=p. 38, Eq. 6}}.</ref>\n:<math>\\displaystyle\\frac{\\delta F[G[\\rho]] }{\\delta\\rho(y)}  = \\int dx \\frac{\\delta F[G]}{\\delta G(x)}_{G = G[\\rho]}\\cdot\\frac {\\delta G[\\rho](x)} {\\delta\\rho(y)} \\ . </math>\n:If {{math|''G''}} is an ordinary differentiable function (local functional) {{math|''g''}}, then this reduces to<ref>{{harv|Greiner|Reinhardt|1996|loc=p. 38, Eq. 7}}.</ref>\n:<math>\\displaystyle\\frac{\\delta F[g(\\rho)] }{\\delta\\rho(y)}  = \\frac{\\delta F[g(\\rho)]}{\\delta g[\\rho(y) ]} \\ \\frac {dg(\\rho)} {d\\rho(y)} \\ . </math>\n\n==Determining functional derivatives==\nWe give a formula to determine functional derivatives for a common class of functionals that can be written as the integral of a function and its derivatives. This is a generalization of the [[Euler–Lagrange equation]]: indeed, the functional derivative was introduced in [[physics]] within the derivation of the [[Joseph-Louis Lagrange|Lagrange]] equation of the second kind from the [[principle of least action]] in [[Lagrangian mechanics]] (18th century). The first three examples below are taken from [[density functional theory]] (20th century), the fourth from [[statistical mechanics]] (19th century).\n\n===Formula===\nGiven a functional \n:<math>F[\\rho] = \\int f( \\boldsymbol{r}, \\rho(\\boldsymbol{r}), \\nabla\\rho(\\boldsymbol{r}) )\\, d\\boldsymbol{r},</math>\nand a function {{math|''ϕ''}}('''{{math|''r''}}''') that vanishes on the boundary of the region of integration, from a previous section [[Functional derivative#Definition|Definition]],\n\n:<math>\n\\begin{align}\n\\int \\frac{\\delta F}{\\delta\\rho(\\boldsymbol{r})} \\, \\phi(\\boldsymbol{r}) \\, d\\boldsymbol{r}  \n&  = \\left [ \\frac{d}{d\\varepsilon} \\int f( \\boldsymbol{r}, \\rho + \\varepsilon \\phi, \\nabla\\rho+\\varepsilon\\nabla\\phi )\\, d\\boldsymbol{r} \\right ]_{\\varepsilon=0} \\\\\n&  = \\int \\left( \\frac{\\partial f}{\\partial\\rho} \\, \\phi + \\frac{\\partial f}{\\partial\\nabla\\rho} \\cdot \\nabla\\phi \\right) d\\boldsymbol{r} \\\\\n&  = \\int \\left[ \\frac{\\partial f}{\\partial\\rho} \\, \\phi + \\nabla \\cdot \\left( \\frac{\\partial f}{\\partial\\nabla\\rho} \\, \\phi \\right) - \\left( \\nabla \\cdot \\frac{\\partial f}{\\partial\\nabla\\rho} \\right) \\phi \\right] d\\boldsymbol{r} \\\\\n&  = \\int \\left[ \\frac{\\partial f}{\\partial\\rho} \\, \\phi - \\left( \\nabla \\cdot \\frac{\\partial f}{\\partial\\nabla\\rho} \\right) \\phi \\right] d\\boldsymbol{r} \\\\\n&  = \\int \\left( \\frac{\\partial f}{\\partial\\rho} -  \\nabla \\cdot \\frac{\\partial f}{\\partial\\nabla\\rho} \\right) \\phi(\\boldsymbol{r}) \\ d\\boldsymbol{r} \\, .\n\\end{align}\n</math>\n\nThe second line is obtained using the [[total derivative]], where {{math|''&part;f'' /''&part;&nabla;''}}''ρ'' is a [[Matrix calculus#Scalar-by-vector|derivative of a scalar with respect to a vector]].<ref group=\"Note\">For a three-dimensional cartesian coordinate system,\n:<math>\n\\begin{align}\n\\frac{\\partial f}{\\partial\\nabla\\rho} =   \\frac{\\partial f}{\\partial\\rho_x}  \\mathbf{\\hat{i}}   + \\frac{\\partial f}{\\partial\\rho_y}  \\mathbf{\\hat{j}} + \\frac{\\partial f}{\\partial\\rho_z}  \\mathbf{\\hat{k}}\\, ,  \\qquad \n& \\text{where} \\ \\rho_x = \\frac{\\partial \\rho}{\\partial x}\\, ,  \\ \\rho_y = \\frac{\\partial \\rho}{\\partial y}\\, , \\ \\rho_z = \\frac{\\partial \\rho}{\\partial z}\\, \\\\\n& \\text{and} \\ \\ \\mathbf{\\hat{i}}, \\ \\mathbf{\\hat{j}}, \\ \\mathbf{\\hat{k}} \\ \\ \\text {are unit vectors along the x, y, z axes.}\n\\end{align}\n</math></ref> The third line was obtained by use of a [[Divergence#Properties|product rule for divergence]]. The fourth line was obtained using the [[divergence theorem]] and the condition that {{math|''ϕ''{{=}}0}} on the boundary of the region of integration.  Since {{math|''ϕ''}} is also an arbitrary function, applying the [[fundamental lemma of calculus of variations]] to the last line, the functional derivative is\n\n:<math>\n\\frac{\\delta F}{\\delta\\rho(\\boldsymbol{r})} = \\frac{\\partial f}{\\partial\\rho} - \\nabla \\cdot \\frac{\\partial f}{\\partial\\nabla\\rho} \n</math>\n\nwhere ''ρ'' = ''ρ''('''{{math|''r''}}''') and {{math|''f'' {{=}} ''f'' ('''{{math|''r''}}'''}}, ''ρ'', &nabla;''ρ''). This formula is for the case of the functional form given by {{math|''F''}}[''ρ''] at the beginning of this section. For other functional forms,  the definition of the functional derivative can be used as the starting point for its determination. (See the example [[Functional derivative#Coulomb potential energy functional|Coulomb potential energy functional]].)\n\nThe above equation for the functional derivative can be generalized to the case that includes higher dimensions and higher order derivatives. The functional would be,\n\n:<math>\nF[\\rho(\\boldsymbol{r})] = \\int f( \\boldsymbol{r}, \\rho(\\boldsymbol{r}), \\nabla\\rho(\\boldsymbol{r}), \\nabla^{(2)}\\rho(\\boldsymbol{r}), \\dots, \\nabla^{(N)}\\rho(\\boldsymbol{r}))\\, d\\boldsymbol{r},\n</math>\n\nwhere the vector {{math|'''''r''''' &isin; ℝ<sup>''n''</sup>}}, and {{math|&nabla;<sup>(''i'')</sup>}} is a tensor whose {{math|''n<sup>i</sup>''}} components  are partial derivative operators of order {{math|''i''}},  \n:<math> \\left [ \\nabla^{(i)} \\right ]_{\\alpha_1 \\alpha_2 \\cdots \\alpha_i} = \\frac {\\partial^{\\, i}} {\\partial r_{\\alpha_1}  \\partial r_{\\alpha_2} \\cdots \\partial r_{\\alpha_i} } \\qquad \\qquad \\text{where} \\quad  \\alpha_1, \\alpha_2, \\cdots, \\alpha_i = 1, 2, \\cdots , n \\ . </math><ref group = 'Note'>For example, for the case of three dimensions ({{math|''n'' {{=}} 3}}) and second order derivatives ({{math|''i'' {{=}} 2}}), the tensor {{math|&nabla;<sup>(2)</sup>}} has components,\n:<math> \\left [ \\nabla^{(2)} \\right ]_{\\alpha \\beta} = \\frac {\\partial^{\\,2}} {\\partial r_{\\alpha} \\, \\partial r_{\\beta}} \\qquad \\qquad \\text{where} \\quad \\alpha, \\beta = 1, 2, 3 \\, . </math></ref>\n\nAn analogous application of the definition of the functional derivative yields\n\n:<math>\n\\begin{align}\n\\frac{\\delta F[\\rho]}{\\delta \\rho} &{} = \\frac{\\partial f}{\\partial\\rho} - \\nabla \\cdot \\frac{\\partial f}{\\partial(\\nabla\\rho)} + \\nabla^{(2)} \\cdot \\frac{\\partial f}{\\partial\\left(\\nabla^{(2)}\\rho\\right)} + \\dots + (-1)^N \\nabla^{(N)} \\cdot \\frac{\\partial f}{\\partial\\left(\\nabla^{(N)}\\rho\\right)} \\\\\n&{} =   \\frac{\\partial f}{\\partial\\rho} + \\sum_{i=1}^N (-1)^{i}\\nabla^{(i)} \\cdot \\frac{\\partial f}{\\partial\\left(\\nabla^{(i)}\\rho\\right)} \\ .\n\\end{align}\n</math>\n\nIn the last two equations, the {{math|''n<sup>i</sup>''}} components of the tensor <math> \\frac{\\partial f}{\\partial\\left(\\nabla^{(i)}\\rho\\right)} </math> are partial derivatives of {{math|''f''}} with respect to partial derivatives of ''ρ'',\n\n:<math> \\left [ \\frac {\\partial f} {\\partial \\left (\\nabla^{(i)}\\rho \\right ) } \\right ]_{\\alpha_1 \\alpha_2 \\cdots \\alpha_i} = \\frac {\\partial f} {\\partial \\rho_{\\alpha_1 \\alpha_2 \\cdots \\alpha_i} } \\qquad \\qquad \\text{where} \\quad \\rho_{\\alpha_1 \\alpha_2 \\cdots \\alpha_i} \\equiv \\frac {\\partial^{\\, i}\\rho} {\\partial r_{\\alpha_1} \\, \\partial r_{\\alpha_2} \\cdots \\partial r_{\\alpha_i} }   \\ ,   </math>\n\nand the tensor scalar product is,\n:<math> \\nabla^{(i)} \\cdot \\frac{\\partial f}{\\partial\\left(\\nabla^{(i)}\\rho\\right)} = \\sum_{\\alpha_1, \\alpha_2, \\cdots, \\alpha_i = 1}^n \\ \\frac {\\partial^{\\, i} } {\\partial r_{\\alpha_1} \\, \\partial r_{\\alpha_2} \\cdots \\partial r_{\\alpha_i} }  \\ \\frac {\\partial f} {\\partial \\rho_{\\alpha_1 \\alpha_2 \\cdots \\alpha_i} }   \\  .  </math> <ref group = 'Note'>For example, for the case {{math|''n'' {{=}} 3}} and {{math|''i'' {{=}} 2}}, the tensor scalar product is,\n:<math> \\nabla^{(2)} \\cdot \\frac{\\partial f}{\\partial\\left(\\nabla^{(2)}\\rho\\right)} = \\sum_{\\alpha, \\beta = 1}^3 \\ \\frac {\\partial^{\\, 2} } {\\partial r_{\\alpha} \\, \\partial r_{\\beta} }  \\ \\frac {\\partial f} {\\partial \\rho_{\\alpha \\beta} }    \\qquad \\text{where} \\ \\ \\rho_{\\alpha \\beta} \\equiv \\frac {\\partial^{\\, 2}\\rho} {\\partial r_{\\alpha} \\, \\partial r_{\\beta} } \\ . </math></ref>\n\n===Examples===\n\n====Thomas–Fermi kinetic energy functional====\nThe [[Thomas–Fermi model]] of 1927 used a kinetic energy functional for a noninteracting uniform [[free electron model|electron gas]] in a first attempt of [[density-functional theory]] of electronic structure:\n:<math>T_\\mathrm{TF}[\\rho] = C_\\mathrm{F} \\int \\rho^{5/3}(\\mathbf{r}) \\, d\\mathbf{r} \\, .</math>\nSince the integrand of {{math|''T''<sub>TF</sub>}}[''ρ''] does not involve derivatives of ''ρ''{{math|('''''r''''')}}, the functional derivative of {{math|''T''<sub>TF</sub>}}[''ρ''] is,<ref name=ParrYangP247A.6>{{harv|Parr|Yang|1989|loc=p. 247, Eq. A.6}}.</ref>\n:<math>\n\\begin{align}\n\\frac{\\delta T_{\\mathrm{TF}}}{\\delta \\rho (\\boldsymbol{r}) } \n& = C_\\mathrm{F} \\frac{\\partial \\rho^{5/3}(\\mathbf{r})}{\\partial \\rho(\\mathbf{r})}  \\\\\n& = \\frac{5}{3} C_\\mathrm{F}  \\rho^{2/3}(\\mathbf{r}) \\, .\n\\end{align}\n</math>\n\n====Coulomb potential energy functional====\nFor the '''electron-nucleus potential''', Thomas and Fermi employed the [[Coulomb's law|Coulomb]] potential energy functional\n\n:<math>V[\\rho] =  \\int \\frac{\\rho(\\boldsymbol{r})}{|\\boldsymbol{r}|} \\ d\\boldsymbol{r}.</math>\n\nApplying the definition of functional derivative,\n\n:<math>\n\\begin{align}\n\\int \\frac{\\delta V}{\\delta \\rho(\\boldsymbol{r})} \\ \\phi(\\boldsymbol{r}) \\ d\\boldsymbol{r} \n& {} = \\left [ \\frac{d}{d\\varepsilon}  \\int \\frac{\\rho(\\boldsymbol{r}) + \\varepsilon \\phi(\\boldsymbol{r})}{|\\boldsymbol{r}|} \\ d\\boldsymbol{r} \\right ]_{\\varepsilon=0} \\\\\n& {} =  \\int  \\frac {1} {|\\boldsymbol{r}|} \\, \\phi(\\boldsymbol{r}) \\ d\\boldsymbol{r} \\, .\n\\end{align}\n</math>\nSo,\n:<math> \\frac{\\delta V}{\\delta \\rho(\\boldsymbol{r})} = \\frac{1}{|\\boldsymbol{r}|} \\ . </math>\n\nFor the classical part of the '''electron-electron interaction''', Thomas and Fermi employed the [[Coulomb's law|Coulomb]] potential energy functional\n:<math>J[\\rho] = \\frac{1}{2}\\iint \\frac{\\rho(\\mathbf{r}) \\rho(\\mathbf{r}')}{\\vert \\mathbf{r}-\\mathbf{r}' \\vert}\\, d\\mathbf{r} d\\mathbf{r}' \\, .</math>\nFrom the [[Functional derivative#Functional derivative|definition of the functional derivative]], \n:<math>\n\\begin{align}\n\\int \\frac{\\delta J}{\\delta\\rho(\\boldsymbol{r})} \\phi(\\boldsymbol{r})d\\boldsymbol{r}  \n& {} = \\left [ \\frac {d \\ }{d\\epsilon} \\, J[\\rho + \\epsilon\\phi] \\right ]_{\\epsilon = 0} \\\\\n& {} = \\left [ \\frac {d \\ }{d\\epsilon} \\, \\left ( \\frac{1}{2}\\iint \\frac {[\\rho(\\boldsymbol{r}) + \\epsilon \\phi(\\boldsymbol{r})] \\,  [\\rho(\\boldsymbol{r}') + \\epsilon \\phi(\\boldsymbol{r}')]  }{\\vert \\boldsymbol{r}-\\boldsymbol{r}' \\vert}\\, d\\boldsymbol{r} d\\boldsymbol{r}'  \\right ) \\right ]_{\\epsilon = 0} \\\\\n& {} =  \\frac{1}{2}\\iint \\frac {\\rho(\\boldsymbol{r}') \\phi(\\boldsymbol{r})  }{\\vert \\boldsymbol{r}-\\boldsymbol{r}' \\vert}\\, d\\boldsymbol{r} d\\boldsymbol{r}' +\n            \\frac{1}{2}\\iint \\frac {\\rho(\\boldsymbol{r}) \\phi(\\boldsymbol{r}')  }{\\vert \\boldsymbol{r}-\\boldsymbol{r}' \\vert}\\, d\\boldsymbol{r} d\\boldsymbol{r}'   \\\\\n\\end{align}\n</math>\nThe first and second terms on the right hand side of the last equation are  equal, since {{math|'''''r'''''}} and {{math|'''''r&prime;'''''}} in the second term can be interchanged without changing the value of the integral. Therefore,\n:<math> \\int \\frac{\\delta J}{\\delta\\rho(\\boldsymbol{r})} \\phi(\\boldsymbol{r})d\\boldsymbol{r} = \\int \\left ( \\int \\frac {\\rho(\\boldsymbol{r}') }{\\vert \\boldsymbol{r}-\\boldsymbol{r}' \\vert} d\\boldsymbol{r}' \\right )  \\phi(\\boldsymbol{r}) d\\boldsymbol{r}  </math>\nand the functional derivative of the electron-electron coulomb potential energy functional {{math|''J''}}[''ρ''] is,<ref name=ParrYangP248A.11>{{harv|Parr|Yang|1989|loc=p. 248, Eq. A.11}}.</ref>\n:<math> \\frac{\\delta J}{\\delta\\rho(\\boldsymbol{r})} = \\int \\frac {\\rho(\\boldsymbol{r}') }{\\vert \\boldsymbol{r}-\\boldsymbol{r}' \\vert} d\\boldsymbol{r}' \\, . </math>\n\nThe second functional derivative is\n:<math>\\frac{\\delta^2 J[\\rho]}{\\delta \\rho(\\mathbf{r}')\\delta\\rho(\\mathbf{r})}  = \\frac{\\partial}{\\partial \\rho(\\mathbf{r}')} \\left ( \\frac{\\rho(\\mathbf{r}')}{\\vert \\mathbf{r}-\\mathbf{r}' \\vert} \\right ) = \\frac{1}{\\vert \\mathbf{r}-\\mathbf{r}' \\vert}.\n</math>\n\n====Weizsäcker kinetic energy functional====\nIn 1935 [[Carl Friedrich von Weizsacker|von Weizsäcker]] proposed to add a gradient correction to the Thomas-Fermi kinetic energy functional to make it suit better a molecular electron cloud:\n:<math>T_\\mathrm{W}[\\rho] = \\frac{1}{8} \\int \\frac{\\nabla\\rho(\\mathbf{r}) \\cdot \\nabla\\rho(\\mathbf{r})}{ \\rho(\\mathbf{r}) } d\\mathbf{r} =  \\int t_\\mathrm{W} \\ d\\mathbf{r} \\, ,</math>\nwhere\n:<math> t_\\mathrm{W} \\equiv  \\frac{1}{8}  \\frac{\\nabla\\rho \\cdot \\nabla\\rho}{ \\rho } \\qquad \\text{and} \\ \\ \\rho = \\rho(\\boldsymbol{r}) \\ .  </math>\nUsing a previously derived [[Functional derivative#Formula|formula]] for the functional derivative,\n:<math>\n\\begin{align}\n\\frac{\\delta T_\\mathrm{W}}{\\delta \\rho(\\boldsymbol{r})} \n& = \\frac{\\partial t_\\mathrm{W}}{\\partial \\rho} - \\nabla\\cdot\\frac{\\partial t_\\mathrm{W}}{\\partial \\nabla \\rho} \\\\\n& = -\\frac{1}{8}\\frac{\\nabla\\rho \\cdot \\nabla\\rho}{\\rho^2} - \\left ( \\frac {1}{4} \\frac {\\nabla^2\\rho} {\\rho} -  \\frac {1}{4} \\frac {\\nabla\\rho \\cdot \\nabla\\rho} {\\rho^2} \\right ) \\qquad \\text{where} \\ \\ \\nabla^2 = \\nabla \\cdot \\nabla \\ ,\n\\end{align}\n</math>\nand the result is,<ref name=ParrYangP247A.9>{{harv|Parr|Yang|1989|loc= p. 247, Eq. A.9}}.</ref>\n:<math> \\frac{\\delta T_\\mathrm{W}}{\\delta \\rho(\\boldsymbol{r})} =  \\ \\ \\, \\frac{1}{8}\\frac{\\nabla\\rho \\cdot \\nabla\\rho}{\\rho^2} - \\frac{1}{4}\\frac{\\nabla^2\\rho}{\\rho} \\ . </math>\n\n====Entropy====\nThe [[information entropy|entropy]] of a discrete [[random variable]] is a functional of the [[probability mass function]].\n\n:<math>\n\\begin{align}\nH[p(x)] = -\\sum_x p(x) \\log p(x)\n\\end{align}\n\n</math>\nThus,\n\n:<math>\n\\begin{align}\n\\sum_x \\frac{\\delta H}{\\delta p(x)} \\, \\phi(x) \n& {} = \\left[ \\frac{d}{d\\epsilon} H[p(x) + \\epsilon\\phi(x)] \\right]_{\\epsilon=0}\\\\\n& {} = \\left [- \\, \\frac{d}{d\\varepsilon}  \\sum_x \\, [p(x) + \\varepsilon\\phi(x)] \\ \\log [p(x) + \\varepsilon\\phi(x)] \\right]_{\\varepsilon=0} \\\\\n& {} = \\displaystyle -\\sum_x \\, [1+\\log p(x)] \\ \\phi(x) \\, .\n\\end{align}\n</math>\n\nThus,\n\n:<math>\n\\frac{\\delta H}{\\delta p(x)} = -1-\\log p(x).\n</math>\n\n==== Exponential ====\n\nLet\n:<math> F[\\varphi(x)]= e^{\\int \\varphi(x) g(x)dx}.</math>\n\nUsing the delta function as a test function,\n:<math>\n\\begin{align}\n\\frac{\\delta F[\\varphi(x)]}{\\delta \\varphi(y)} \n& {} = \\lim_{\\varepsilon\\to 0}\\frac{F[\\varphi(x)+\\varepsilon\\delta(x-y)]-F[\\varphi(x)]}{\\varepsilon}\\\\\n& {} = \\lim_{\\varepsilon\\to 0}\\frac{e^{\\int (\\varphi(x)+\\varepsilon\\delta(x-y)) g(x)dx}-e^{\\int \\varphi(x) g(x)dx}}{\\varepsilon}\\\\\n& {} = e^{\\int \\varphi(x) g(x)dx}\\lim_{\\varepsilon\\to 0}\\frac{e^{\\varepsilon \\int \\delta(x-y) g(x)dx}-1}{\\varepsilon}\\\\\n& {} = e^{\\int \\varphi(x) g(x)dx}\\lim_{\\varepsilon\\to 0}\\frac{e^{\\varepsilon g(y)}-1}{\\varepsilon}\\\\\n& {} = e^{\\int \\varphi(x) g(x)dx}g(y).\n\\end{align}\n</math>\n\nThus,\n:<math> \\frac{\\delta F[\\varphi(x)]}{\\delta \\varphi(y)} = g(y) F[\\varphi(x)]. </math>\n\nThis is particularly useful in calculating the [[Correlation function (quantum field theory)|correlation functions]] from the [[Partition function (quantum field theory)|partition function]] in [[quantum field theory]].\n\n====Functional derivative of a function====\nA function can be written in the form of  an integral like a functional. For example,\n:<math>\\rho(\\boldsymbol{r}) = F[\\rho] = \\int \\rho(\\boldsymbol{r}') \\delta(\\boldsymbol{r}-\\boldsymbol{r}')\\, d\\boldsymbol{r}'.</math>\nSince the integrand does not depend on derivatives of ''ρ'', the functional derivative of ''ρ''{{math|('''''r''''')}} is,\n:<math>\n\\begin{align} \n\\frac {\\delta \\rho(\\boldsymbol{r})} {\\delta\\rho(\\boldsymbol{r}')} \\equiv \\frac {\\delta F} {\\delta\\rho(\\boldsymbol{r}')} \n& = \\frac{\\partial \\ \\ }{\\partial \\rho(\\boldsymbol{r}')} \\, [\\rho(\\boldsymbol{r}') \\delta(\\boldsymbol{r}-\\boldsymbol{r}')] \\\\\n& = \\delta(\\boldsymbol{r}-\\boldsymbol{r}').\n\\end{align}\n</math>\n\n==== Functional derivative of iterated function====\nThe functional derivative of the iterated function <math>f(f(x))</math> is given by:\n\n:<math>\\frac{\\delta f(f(x))}{\\delta f(y) } = f'(f(x))\\delta(x-y) + \\delta(f(x)-y)</math>\n\nand \n\n:<math>\\frac{\\delta f(f(f(x)))}{\\delta f(y) } = f'(f(f(x))(f'(f(x))\\delta(x-y) + \\delta(f(x)-y)) + \\delta(f(f(x))-y)</math>\n\nIn general:\n\n:<math>\\frac{\\delta f^N(x)}{\\delta f(y)} = f'( f^{N-1}(x) ) \\frac{ \\delta f^{N-1}(x)}{\\delta f(y)} + \\delta( f^{N-1}(x) - y ) </math>\n\nPutting in N=0 gives:\n\n:<math> \\frac{\\delta f^{-1}(x)}{\\delta f(y) }  =- \\frac{ \\delta(f^{-1}(x)-y ) }{ f'(f^{-1}(x))  }</math>\n\n==Using the delta function as a test function==\nIn physics, it's common to use the [[Dirac delta function]] <math>\\delta(x-y)</math> in place of a generic test function <math>\\phi(x)</math>, for yielding the functional derivative at the point <math>y</math> (this is a point of the whole functional derivative as a [[partial derivative]] is a component of the gradient):<ref>{{harvnb|Greiner|Reinhardt|1996|p=37}}</ref>\n\n: <math>\\frac{\\delta F[\\rho(x)]}{\\delta \\rho(y)}=\\lim_{\\varepsilon\\to 0}\\frac{F[\\rho(x)+\\varepsilon\\delta(x-y)]-F[\\rho(x)]}{\\varepsilon}.\n</math>\n\nThis works in cases when <math>F[\\rho(x)+\\varepsilon f(x)]</math> formally can be expanded as a series (or at least up to first order) in <math>\\varepsilon</math>. The formula is however not mathematically rigorous, since <math>F[\\rho(x)+\\varepsilon\\delta(x-y)]</math> is usually not even defined.\n\nThe definition given in a previous section is based on a relationship that holds for all test functions {{math|''ϕ''}}, so one might think that it should hold also when {{math|''ϕ''}} is chosen to be a specific function such as the [[Dirac delta function|delta function]].  However, the latter is not a valid test function (it is not even a proper function).\n\nIn the definition, the functional derivative describes how the functional <math>F[\\varphi(x)]</math> changes as a result of a small change in the entire function <math>\\varphi(x)</math>. The particular form of the change in <math>\\varphi(x)</math> is not specified, but it should stretch over the whole interval on which <math>x</math> is defined. Employing the particular form of the perturbation given by the delta function has the meaning that <math>\\varphi(x)</math> is varied only in the point <math>y</math>. Except for this point, there is no variation in <math>\\varphi(x)</math>.\n\n==Notes==\n{{Reflist|group=Note}}\n\n==Footnotes==\n{{reflist|29em}}\n\n==References==\n*{{cite book | last1=Courant | first1=Richard | authorlink1=Richard Courant | last2=Hilbert | first2=David | authorlink2=David Hilbert | title = Methods of Mathematical Physics | volume = Vol. I | edition = First English | ref=harv | publisher = [[Interscience Publishers]], Inc | year = 1953 | location = New York, New York | chapter = Chapter IV. The Calculus of Variations | pages = 164–274 | isbn = 978-0471504474| mr = 0065391 | zbl = 0001.00501}}.\n*{{Citation\n | last = Frigyik\n | first = Béla A. \n | author-link =\n | last2 = Srivastava\n | first2 = Santosh\n | author2-link = \n | last3 = Gupta\n | first3 = Maya R.\n | author3-link =\n | title = Introduction to Functional Derivatives\n | place = Seattle, WA\n | publisher = Department of Electrical Engineering at the University of Washington\n | series = UWEE Tech Report\n | volume = UWEETR-2008-0001\n |date=January 2008\n | pages = 7\n | language =\n | url = https://www.ee.washington.edu/techsite/papers/documents/UWEETR-2008-0001.pdf\n}}. \n*{{Citation\n | last = Gelfand\n | first = I. M.\n | author-link = Israel Gelfand\n | last2 = Fomin\n | first2 = S. V.\n | author2-link = Sergei Fomin\n | title = Calculus of variations\n | place = Mineola, N.Y.\n | publisher = [[Dover Publications]]\n | series = translated and edited by Richard A. Silverman\n | origyear = 1963\n | year = 2000\n | edition = Revised English \n | url = http://store.doverpublications.com/0486414485.html\n | doi =\n | id =\n | isbn = 978-0486414485\n | mr = 0160139 \n | zbl = 0127.05402\n}}.\n*{{Citation\n | last = Giaquinta\n | first = Mariano\n | author-link = Mariano Giaquinta\n | last2 = Hildebrandt\n | first2 = Stefan\n | title = Calculus of Variations 1. The Lagrangian Formalism\n | place = Berlin\n | publisher = [[Springer-Verlag]]\n | series = Grundlehren der Mathematischen Wissenschaften\n | volume = 310\n | year = 1996\n | edition = 1st\n | url =\n | isbn = 3-540-50625-X\n | mr = 1368401\n | zbl = 0853.49001\n}}.\n*{{Citation\n | last1 = Greiner\n | first1 = Walter\n | authorlink1 = Walter Greiner\n | last2 = Reinhardt\n | first2 = Joachim\n | title = Field quantization\n | place = Berlin–Heidelberg–New York\n | publisher = Springer-Verlag\n | series = With a foreword by D. A. Bromley\n | volume =\n | origyear =\n | year = 1996\n | edition =\n | chapter = Section 2.3 – Functional derivatives\n | page =\n | pages = 36–38\n | language =\n | url =\n | doi =\n | id = \n | isbn = 3-540-59179-6\n | mr = 1383589\n | zbl = 0844.00006 \n}}.\n*{{cite book |first1=R. G.|last1=Parr|first2=W.|last2=Yang| title =  Density-Functional Theory of Atoms and Molecules | chapter = Appendix A, Functionals | pages = 246–254 | publisher = Oxford University Press | year = 1989 |location=New York| url = https://books.google.com/?id=mGOpScSIwU4C&printsec=frontcover&dq=Density-Functional+Theory+of+Atoms+and+Molecules&cd=1#v=onepage&q | ref=harv | isbn = 978-0195042795}}\n\n==External links==\n* {{springer|title=Functional derivative|id=p/f042040}}\n\n{{Functional Analysis}}\n\n[[Category:Differential calculus]]\n[[Category:Topological vector spaces]]\n[[Category:Differential operators]]\n[[Category:Calculus of variations]]\n[[Category:Variational analysis]]"
    },
    {
      "title": "Fundamental lemma of calculus of variations",
      "url": "https://en.wikipedia.org/wiki/Fundamental_lemma_of_calculus_of_variations",
      "text": "In [[mathematics]], specifically in the [[calculus of variations]], a variation {{math|''&delta;f''}} of a function {{math|''f''}} can be concentrated on an arbitrarily small interval, but not a single point.\nAccordingly, the necessary condition of extremum ([[functional derivative]] equal zero) appears in a [[weak formulation]] (variational form) integrated with an arbitrary function {{math|''&delta;f''}}. The '''fundamental lemma of the calculus of variations''' is typically used to transform this weak formulation into the strong formulation ([[differential equation]]), free of the integration with arbitrary function. The proof usually exploits the possibility to choose {{math|''&delta;f''}} concentrated on an interval on which {{math|''f''}} keeps sign (positive or negative). Several versions of the lemma are in use. Basic versions are easy to formulate and prove. More powerful versions are used when needed.\n\n==Basic version==\n\n:If a continuous function <math>f</math> on an open interval <math>(a,b)</math> satisfies the equality\n::<math> \\int_a^b f(x)h(x)\\,\\operatorname{d}x = 0 </math>\n:for all [[compactly supported]] [[smooth function]]s <math>h</math> on <math>(a,b)</math>, then <math>f</math> is identically zero.<ref name=J1>{{harvnb|Jost|Li-Jost|1998|loc=Lemma 1.1.1 on p.6}}</ref><ref name=GF1>{{harvnb|Gelfand|Fomin|1963|loc=Lemma 1 on p.9 (and Remark)}}</ref>\n\nHere \"smooth\" may be interpreted as \"infinitely differentiable\",<ref name=J1 /> but often is interpreted as \"twice continuously differentiable\" or \"continuously differentiable\" or even just \"continuous\",<ref name=GF1 /> since these weaker statements may be strong enough for a given task. \"Compactly supported\" means \"vanishes outside <math>[c,d]</math> for some <math>c</math>, <math>d</math> such that <math>a<c<d<b</math>\";<ref name=J1 /> but often a weaker statement suffices, assuming only that <math>h</math> (or <math>h</math> and a number of its derivatives) vanishes at the endpoints <math>a</math>, <math>b</math>;<ref name=GF1 /> in this case the closed interval <math>[a,b]</math> is used.\n\n==Version for two given functions==\n\n:If a pair of continuous functions ''f'', ''g'' on an interval (''a'',''b'') satisfies the equality\n::<math> \\int_a^b ( f(x) \\, h(x) + g(x) \\, h'(x) ) \\, \\mathrm{d}x = 0 </math>\n:for all compactly supported smooth functions ''h'' on (''a'',''b''), then ''g'' is differentiable, and ''g''' = ''f'' &nbsp;everywhere.<ref name=GF4>{{harvnb|Gelfand|Fomin|1963|loc=Lemma 4 on p.11}}</ref><ref name=H1>{{harvnb|Hestenes|1966|loc=Lemma 15.1 on p.50}}</ref>\n\nThe special case for ''g'' = 0 is just the basic version.\n\nHere is the special case for ''f'' = 0 (often sufficient).\n\n:If a continuous function ''g'' on an interval (''a'',''b'') satisfies the equality\n::<math> \\int_a^b g(x) \\, h'(x) \\, \\mathrm{d}x = 0 </math>\n:for all compactly supported smooth functions ''h'' on (''a'',''b''), then ''g'' is [[Constant function|constant]].<ref name=GF2>{{harvnb|Gelfand|Fomin|1963|loc=Lemma 2 on p.10}}</ref>\n\nIf, in addition, [[continuously differentiable|continuous differentiability]] of ''g'' is assumed, then [[integration by parts]] reduces both statements to the basic version; this case is attributed to [[Joseph-Louis Lagrange]], while the proof of differentiability of ''g'' is due to [[Paul du Bois-Reymond]].\n\n==Versions for discontinuous functions==\n\nThe given functions (''f'', ''g'') may be discontinuous, provided that they are [[locally integrable]] (on the given interval). In this case, [[Lebesgue integration]] is meant, the conclusions hold [[almost everywhere]] (thus, in all continuity points), and differentiability of ''g'' is interpreted as local [[absolute continuity]] (rather than continuous differentiability).<ref name=J2>{{harvnb|Jost|Li-Jost|1998|loc=Lemma 1.2.1 on p.13}}</ref><ref name=Gia>{{harvnb|Giaquinta|Hildebrandt|1996|loc=section 2.3: Mollifiers}}</ref> Sometimes the given functions are assumed to be [[piecewise continuous]], in which case [[Riemann integration]] suffices, and the conclusions are stated everywhere except the finite set of discontinuity points.<ref name=H1 />\n\n==Higher derivatives==\n\n:If a tuple of continuous functions <math>f_0,f_1,\\dots,f_n</math> on an interval (''a'',''b'') satisfies the equality\n::<math> \\int_a^b ( f_0(x) \\, h(x) + f_1(x) \\, h'(x) + \\dots + f_n(x) \\, h^{(n)}(x) ) \\, \\mathrm{d}x = 0 </math>\n:for all compactly supported smooth functions ''h'' on (''a'',''b''), then there exist continuously differentiable functions <math> u_0,u_1,\\dots,u_{n-1} </math> on (''a'',''b'') such that\n::<math> f_0 = u'_0, \\; f_1 = u_0+u'_1, \\; \\dots, \\; f_{n-1}=u_{n-2}+u'_{n-1}, \\; f_n = u_{n-1} </math>\n:everywhere.<ref name=H2>{{harvnb|Hestenes|1966|loc=Lemma 13.1 on p.105}}</ref>\n\nThis necessary condition is also sufficient, since the integrand becomes <math> (u_0 h)' + (u_1 h')' + \\dots + (u_{n-1} h^{(n-1)})'. </math>\n\nThe case ''n'' = 1 is just the version for two given functions, since <math> f=f_0=u'_0 </math> and <math> f_1=u_0, </math> thus, <math> f_0-f'_1 = 0. </math>\n\nIn contrast, the case ''n''=2 does not lead to the relation <math> f_0 - f'_1 + f''_2 = 0, </math> since the function <math> f_2 = u_1 </math> need not be differentiable twice. The sufficient condition <math> f_0 - f'_1 + f''_2 = 0 </math> is not necessary. Rather, the necessary and sufficient condition may be written as <math> f_0 - (f_1 - f'_2)' = 0 </math> for ''n''=2, <math> f_0 - (f_1 - (f_2-f'_3)')' = 0 </math> for ''n''=3, and so on; in general, the brackets cannot be opened because of non-differentiability.\n\n==Vector-valued functions==\n\nGeneralization to [[vector-valued functions]] <math>(a,b)\\to\\mathbb{R}^d</math> is straightforward; one applies the results for scalar functions to each coordinate separately,<ref>{{harvnb|Gelfand|Fomin|1963|loc=p.35}}</ref> or treats the vector-valued case from the beginning.<ref>{{harvnb|Jost|Li-Jost|1998}}</ref>\n\n==Multivariable functions==\n\n:If a continuous [[Function of several real variables|multivariable function]] ''f'' on an open set <math>\\Omega\\subset\\mathbb{R}^d</math> satisfies the equality\n::<math> \\int_\\Omega f(x) \\, h(x) \\, \\mathrm{d}x = 0 </math>\n:for all compactly supported smooth functions ''h'' on Ω, then ''f'' is identically zero.\n\nSimilarly to the basic version, one may consider a continuous function ''f'' on the closure of Ω, assuming that ''h'' vanishes on the boundary of Ω (rather than compactly supported).<ref>{{harvnb|Gelfand|Fomin|1963|loc=Lemma on p.22}}; the proof applies in both situations.</ref>\n\nHere is a version for discontinuous multivariable functions.\n\n:Let <math>\\Omega\\subset\\mathbb{R}^d</math> be an open set, and <math>f\\in L^2(\\Omega)</math> satisfy the equality\n::<math> \\int_\\Omega f(x) \\, h(x) \\, \\mathrm{d}x = 0 </math>\n:for all compactly supported smooth functions ''h'' on Ω. Then ''f''=0 (in ''L''<sup>2</sup>, that is, almost everywhere).<ref>{{harvnb|Jost|Li-Jost|1998|loc=Lemma 3.2.3 on p.170}}</ref>\n\n==Applications==\nThis lemma is used to prove that [[maxima and minima|extrema]] of the [[functional (mathematics)|functional]]\n:<math> J[y] = \\int_{x_0}^{x_1} L(t,y(t),\\dot y(t)) \\, \\mathrm{d}t </math>\nare  [[weak solution]]s <math>y:[x_0,x_1]\\to V</math> (for an appropriate vector space <math>V</math>) of the [[Euler–Lagrange equation]]\n:<math> {\\partial L(t,y(t),\\dot y(t)) \\over \\partial y} = {\\mathrm{d} \\over \\mathrm{d}t} {\\partial L(t,y(t),\\dot y(t)) \\over \\partial \\dot y} .</math>\nThe Euler–Lagrange equation plays a prominent role in [[classical mechanics]] and [[differential geometry]].\n\n==Notes==\n<references />\n\n==References==\n*{{citation|last1=Jost|first1=Jürgen|last2=Li-Jost|first2=Xianqing|title=Calculus of variations|publisher=Cambridge University|year=1998}}\n*{{citation|last1=Gelfand|first1=I.M.|last2=Fomin|first2=S.V.|title=Calculus of variations|publisher=Prentice-Hall|year=1963}} (transl. from Russian).\n*{{citation|last=Hestenes|first=Magnus R.|title=Calculus of variations and optimal control theory|publisher=John Wiley|year=1966}}\n*{{citation|last1=Giaquinta|first1=Mariano|last2=Hildebrandt|first2=Stefan|title=Calculus of Variations I|publisher=Springer|year=1996}}\n\n\n{{Fundamental theorems}}\n\n[[Category:Classical mechanics]]\n[[Category:Calculus of variations]]\n[[Category:Smooth functions]]\n[[Category:Lemmas]]\n[[Category:Fundamental theorems|Calculus of variations]]"
    },
    {
      "title": "Γ-convergence",
      "url": "https://en.wikipedia.org/wiki/%CE%93-convergence",
      "text": "{{Context|date=September 2011}}\nIn the [[calculus of variations]], '''Γ-convergence''' ('''Gamma-convergence''') is a notion of convergence for [[Functional (mathematics)|functionals]]. It was introduced by [[Ennio de Giorgi]].\n\n==Definition==\nLet <math>X</math> be a [[topological space]] and <math>F_n:X\\to[0,+\\infty)</math> a sequence of functionals on <math>X</math>. Then <math>F_n</math> are said to <math>\\Gamma</math>-converge to the <math>\\Gamma</math>-limit <math>F:X\\to[0,+\\infty)</math> if the following two conditions hold:\n* Lower bound inequality: For every sequence <math>x_n\\in X</math> such that <math>x_n\\to x</math> as <math>n\\to+\\infty</math>,\n: <math>F(x)\\le\\liminf_{n\\to\\infty} F_n(x_n).</math>\n* Upper bound inequality: For every <math>x\\in X</math>, there is a sequence <math>x_n</math> converging to <math>x</math> such that\n: <math>F(x)\\ge\\limsup_{n\\to\\infty} F_n(x_n)</math>\n\nThe first condition means that <math>F</math> provides an asymptotic common lower bound for the <math>F_n</math>. The second condition means that this lower bound is optimal.\n\n==Properties==\n* Minimizers converge to minimizers: If <math>F_n</math> <math>\\Gamma</math>-converge to <math>F</math>, and <math>x_n</math> is a minimizer for <math>F_n</math>, then every cluster point of the sequence <math>x_n</math> is a minimizer of <math>F</math>.\n* <math>\\Gamma</math>-limits are always [[Semi-continuity|lower semicontinuous]].\n* <math>\\Gamma</math>-convergence is stable under continuous perturbations: If <math>F_n</math> <math>\\Gamma</math>-converges to <math>F</math> and <math>G:X\\to[0,+\\infty)</math> is continuous, then <math>F_n+G</math> will <math>\\Gamma</math>-converge to <math>F+G</math>.\n* A constant sequence of functionals <math>F_n=F</math> does not necessarily <math>\\Gamma</math>-converge to <math>F</math>, but to the ''relaxation'' of <math>F</math>, the largest lower semicontinuous functional below <math>F</math>.\n\n==Applications==\nAn important use for <math>\\Gamma</math>-convergence is in [[homogenization (mathematics)|homogenization theory]]. It can also be used to rigorously justify the passage from discrete to continuum theories for materials, for example, in [[Elasticity (physics)|elasticity]] theory.\n\n==See also==\n* [[Mosco convergence]]\n\n==References==\n* A. Braides: ''Γ-convergence for beginners''. Oxford University Press, 2002.\n* G. Dal Maso: ''An introduction to Γ-convergence''. Birkhäuser, Basel 1993.\n\n{{DEFAULTSORT:Gamma-Convergence}}\n[[Category:Calculus of variations]]\n[[Category:Variational analysis]]\n\n\n{{Mathanalysis-stub}}"
    },
    {
      "title": "Geodesics on an ellipsoid",
      "url": "https://en.wikipedia.org/wiki/Geodesics_on_an_ellipsoid",
      "text": "<!--Because mathematics displays so badly for unregistered users who\ncan't use MathJax, this page uses plain HTML equations for inline\nmathematics.  Please stick with this convention.\n\nFootnotes are for additional explanations.  Citations use Harvard\nparenthetical referencing.\n-->\n{{Geodesy}}\n[[File:Long geodesic on an oblate ellipsoid.svg|thumb|200px|A geodesic on an oblate ellipsoid]]\nThe study of '''geodesics on an ellipsoid''' arose in connection with [[geodesy]]\nspecifically with the solution of [[triangulation network]]s.  The\n[[figure of the Earth]] is well approximated by an\n''[[oblate ellipsoid]]'', a slightly flattened sphere.  A ''[[geodesic]]''\nis the shortest path between two points on a curved surface, analogous\nto a [[straight line]] on a plane surface.  The solution of a triangulation\nnetwork on an ellipsoid is therefore a set of exercises in spheroidal\ntrigonometry {{harv|Euler|1755}}.\n\nIf the Earth is treated as a [[sphere]], the geodesics are\n[[great circles]] (all of which are closed) and the problems reduce to\nones in [[spherical trigonometry]].  However, {{harvtxt|Newton|1687}}\nshowed that the effect of the rotation of the Earth results in its\nresembling a slightly oblate ellipsoid and, in this case, the\n[[equator]] and the [[meridian (geography)|meridians]] are the only simple\nclosed geodesics.  Furthermore, the shortest path between two points on\nthe equator does not necessarily run along the equator.  Finally, if the\nellipsoid is further perturbed to become a [[triaxial ellipsoid]] (with\nthree distinct semi-axes), only three geodesics are closed.\n\n== Geodesics on an ellipsoid of revolution ==\nThere are several ways of defining geodesics\n{{harv|Hilbert|Cohn-Vossen|1952|pp=220–221}}.  A simple definition\nis as the shortest path between two points on a surface.  However, it is\nfrequently more useful to define them as paths with zero\n[[geodesic curvature]]—i.e., the analogue of [[straight lines]] on a\ncurved surface.  This definition encompasses geodesics traveling so far\nacross the ellipsoid's surface (somewhat more than half the\ncircumference) that other distinct routes require less distance.\nLocally, these geodesics are still identical to the shortest distance\nbetween two points.\n\nBy the end of the 18th century, an ellipsoid of revolution (the term\n[[spheroid]] is also used) was a well-accepted approximation to the\n[[figure of the Earth]].  The adjustment of [[triangulation network]]s\nentailed reducing all the measurements to a [[reference ellipsoid]] and\nsolving the resulting two-dimensional problem as an exercise in\nspheroidal trigonometry {{harv|Bomford|1952|loc=Chap. 3}}\n{{harv|Leick et al.|2015|loc=§4.5}}.\n\n[[File:Geodesic problem on an ellipsoid.svg|thumb|right|\nFig. 1.\nA geodesic ''AB'' on an ellipsoid of revolution.  ''N'' is the north\npole and ''EFH'' lie on the equator.]]\nIt is possible to reduce the various geodesic problems into one of two\ntypes.  Consider two points: {{math|''A''}} at latitude\n{{math|&phi;<sub>1</sub>}} and longitude {{math|&lambda;<sub>1</sub>}} and\n{{math|''B''}} at latitude {{math|&phi;<sub>2</sub>}} and longitude\n{{math|&lambda;<sub>2</sub>}} (see Fig.  1).  The connecting geodesic\n(from {{math|''A''}} to {{math|''B''}}) is {{math|''AB''}}, of length\n{{math|''s''<sub>12</sub>}}, which has [[azimuth]]s {{math|&alpha;<sub>1</sub>}} and\n{{math|&alpha;<sub>2</sub>}} at the two endpoints.{{refn|\nHere {{math|&alpha;<sub>2</sub>}} is the ''forward'' azimuth at {{math|''B''}}.\nSome authors calculate the ''back'' azimuth instead; this is given by\n{{math|&alpha;<sub>2</sub> &plusmn; &pi;}}.}}  The two geodesic problems usually\nconsidered are:\n# the ''direct geodesic problem'' or ''first geodesic problem'', given {{math|''A''}}, {{math|&alpha;<sub>1</sub>}}, and {{math|''s''<sub>12</sub>}}, determine {{math|''B''}} and {{math|&alpha;<sub>2</sub>}};\n# the ''inverse geodesic problem'' or ''second geodesic problem'', given {{math|''A''}} and {{math|''B''}}, determine {{math|''s''<sub>12</sub>}}, {{math|&alpha;<sub>1</sub>}}, and {{math|&alpha;<sub>2</sub>}}.\nAs can be seen from Fig. 1, these problems involve solving the triangle\n{{math|''NAB''}} given one angle, {{math|&alpha;<sub>1</sub>}} for the direct\nproblem and {{math|&lambda;<sub>12</sub> {{=}} &lambda;<sub>2</sub> &minus; &lambda;<sub>1</sub>}} for the\ninverse problem, and its two adjacent sides.\nFor a sphere the solutions to these problems are simple exercises in\n[[spherical trigonometry]], whose solution is given by\n[[Solution of triangles#Two sides and the included angle given|formulas\nfor solving a spherical triangle]].\n(See the article on [[great-circle navigation]].)\n\nFor an ellipsoid of revolution, the characteristic constant defining the\ngeodesic was found by {{harvtxt|Clairaut|1735}}.  A\nsystematic solution for the paths of geodesics was given by\n{{harvtxt|Legendre|1806}} and\n{{harvtxt|Oriani|1806}} (and subsequent papers in\n[[#{{harvid|Oriani|1808|}}|1808]] and\n[[#{{harvid|Oriani|1810|}}|1810]]).\nThe full solution for the direct problem (complete with computational\ntables and a worked out example) is given by {{harvtxt|Bessel|1825}}.\n\nDuring the 18th century geodesics were typically referred to as \"shortest\nlines\".\nThe term \"geodesic line\" was coined by {{harvtxt|Laplace|1799b}}:\n<blockquote>\nNous désignerons cette ligne sous le nom de ''ligne géodésique'' [We\nwill call this line the ''geodesic line''].\n</blockquote>\nThis terminology was introduced into English either as \"geodesic line\"\nor as \"geodetic line\", for example {{harv|Hutton|1811}},\n<blockquote>\nA line traced in the manner we have now been describing, or deduced from\ntrigonometrical measures, by the means we have indicated, is called\na ''geodetic'' or ''geodesic line:'' it has the property of being\nthe shortest which can be drawn between its two extremities on the\nsurface of the Earth; and it is therefore the proper itinerary\nmeasure of the distance between those two points.\n</blockquote>\nIn its adoption by other fields ''geodesic line'', frequently shortened\nto ''geodesic'', was preferred.\n\nThis section treats the problem on an ellipsoid of revolution (both\noblate and prolate).  The problem on a triaxial ellipsoid is covered in\nthe next section.\n\n=== Equations for a geodesic ===\n{{multiple image\n|align=right\n|direction=horizontal\n|width=220\n|image1=Differential element of a meridian ellipse.svg\n|caption1=Fig. 2. Differential element of a meridian ellipse.\n|image2=Differential element of a geodesic on an ellipsoid.svg\n|caption2=Fig. 3.  Differential element of a geodesic on an ellipsoid.\n}}\n\nHere the equations for a geodesic are developed; the\nderivation closely follows that of {{harvtxt|Bessel|1825}}.\n{{harvtxt|Jordan|Eggert|1941}},\n{{harvtxt|Bagratuni|1962|loc=§15}},\n{{harvtxt|Gan'shin|1967|loc=Chap. 5}},\n{{harvtxt|Krakiwsky|Thomson|1974|loc=§4}},\n{{harvtxt|Rapp|1993|loc=§1.2}},\n{{harvtxt|Jekeli|2012}}, and\n{{harvtxt|Borre|Strang|2012}} also provide derivations of these\nequations.\n\nConsider an ellipsoid of revolution with equatorial radius\n{{math|''a''}} and polar semi-axis {{math|''b''}}.  Define the\nflattening {{math|''f'' {{=}} (''a'' &minus; ''b'')/''a''}}, the eccentricity\n{{math|''e'' {{=}} {{radical|''a''<sup>2</sup> &minus; ''b''<sup>2</sup>}}/''a'' {{=}} {{radical|''f''(2 &minus; ''f'')}}}}, and the second\neccentricity {{math|''e''&prime; {{=}} {{radical|''a''<sup>2</sup> &minus; ''b''<sup>2</sup>}}/''b'' {{=}} ''e''/(1 &minus; ''f'')}}.  (In most\napplications in geodesy, the ellipsoid is taken to be oblate,\n{{math|''a'' &gt; ''b''}}; however, the theory\napplies without change to prolate ellipsoids, {{math|''a'' &lt; ''b''}}, in\nwhich case {{math|''f''}}, {{math|''e''<sup>2</sup>}}, and {{math|''e''&prime;<sup>2</sup>}} are\nnegative.)\n\nLet an elementary segment of a path on the ellipsoid have length\n{{math|''ds''}}.  From Figs. 2 and 3, we\nsee that if its azimuth is {{math|&alpha;}}, then {{math|''ds''}}\nis related to {{math|''d''&phi;}} and {{math|''d''&lambda;}} by\n:(1)<math>{\\color{white}.}\\qquad\n\\cos\\alpha\\,ds = \\rho\\,d\\varphi = - \\frac{dR}{\\sin\\varphi}, \\quad\n\\sin\\alpha\\,ds = R\\,d\\lambda,</math>\nwhere {{math|&rho;}} is the\n[[Earth radius#Meridional|meridional radius of curvature]],\n{{math|''R'' {{=}} &nu; cos&phi;}} is the radius of the circle of latitude\n{{math|&phi;}}, and {{math|&nu;}} is the\n[[Earth radius#Prime vertical|normal radius of curvature]].\nThe elementary segment is therefore given by\n:<math>ds^2 = \\rho^2\\,d\\varphi^2 + R^2\\,d\\lambda^2</math>\nor\n:<math>\\begin{align}ds &= \\sqrt{\\rho^2\\varphi'^2 + R^2}\\,d\\lambda \\\\\n&\\equiv L(\\varphi,\\varphi')\\,d\\lambda,\n\\end{align}\n</math>\nwhere {{math|&phi;&prime; {{=}} ''d''&phi;/''d''&lambda;}} and the\n[[Lagrangian_mechanics#Euler–Lagrange equations and Hamilton's principle|\nLagrangian function {{math|''L''}}]] depends on\n{{math|&phi;}} through {{math|&rho;(&phi;)}} and\n{{math|''R''(&phi;)}}.  The length of an arbitrary path between\n{{math|(&phi;<sub>1</sub>, &lambda;<sub>1</sub>)}} and {{math|(&phi;<sub>2</sub>, &lambda;<sub>2</sub>)}} is\ngiven by\n:<math> s_{12} =\n\\int_{\\lambda_1}^{\\lambda_2} L(\\varphi, \\varphi')\\,d\\lambda,</math>\nwhere {{math|&phi;}} is a function of {{math|&lambda;}} satisfying\n{{math|&phi;(&lambda;<sub>1</sub>) {{=}} &phi;<sub>1</sub>}} and\n{{math|&phi;(&lambda;<sub>2</sub>) {{=}} &phi;<sub>2</sub>}}.  The shortest path or geodesic\nentails finding that function {{math|&phi;(&lambda;)}} which minimizes\n{{math|''s''<sub>12</sub>}}.  This is an exercise in the\n[[calculus of variations]] and the minimizing condition is given by the\n[[Beltrami identity]],\n:<math>L - \\varphi' \\frac{\\partial L}{\\partial \\varphi'} = \\text{const.}\n</math>\nSubstituting for {{math|''L''}} and using Eqs. (1) gives\n:<math>R\\sin\\alpha = \\text{const.}</math>\n{{harvtxt|Clairaut|1735}} found this [[Clairaut's relation|relation]],\nusing a geometrical construction; a similar derivation is presented by\n{{harvtxt|Lyusternik|1964|loc=§10}}.{{refn|\n{{harvtxt|Laplace|1799a}} showed that a particle constrained to move on\na surface but otherwise subject to no forces moves along a geodesic for\nthat surface.  Thus, Clairaut's relation is just a consequence of\n[[conservation of angular momentum]] for a particle on a surface of\nrevolution.}}  Differentiating this\nrelation gives\n:<math>d\\alpha=\\sin\\varphi\\,d\\lambda.</math>\nThis, together with Eqs. (1), leads to a system of\n[[ordinary differential equations]] for a geodesic\n:<math>{\\color{white}.}\\qquad\n\\frac{d\\varphi}{ds} = \\frac{\\cos\\alpha}{\\rho};\\quad\n\\frac{d\\lambda}{ds} = \\frac{\\sin\\alpha}{\\nu\\cos\\varphi};\\quad\n\\frac{d\\alpha}{ds} = \\frac{\\tan\\varphi\\sin\\alpha}{\\nu}.</math>\nWe can express {{math|''R''}} in terms of the\n[[Latitude#Reduced (or parametric) latitude|parametric latitude]],\n{{math|&beta;}},\nusing\n:<math>R = a\\cos\\beta,</math>\nand Clairaut's\nrelation then becomes\n:<math>\\sin\\alpha_1\\cos\\beta_1 = \\sin\\alpha_2\\cos\\beta_2.</math>\n{{multiple image\n|align=right\n|direction=horizontal\n|width=220\n|image1=Geodesic problem on a sphere.svg\n|caption1=Fig. 4.  Geodesic problem mapped to the auxiliary sphere.\n|image2=Geodesic problem mapped to the auxiliary sphere.svg\n|caption2=Fig. 5. The elementary geodesic problem on the auxiliary sphere.\n}}\nThis is the [[Spherical trigonometry#Sine rules|sine rule]] of spherical\ntrigonometry relating two sides of the triangle {{math|''NAB''}} (see\nFig. 4), {{math|''NA'' {{=}} {{frac|1|2}}&pi; &minus; &beta;<sub>1</sub>}}, and\n{{math|''NB'' {{=}} {{frac|1|2}}&pi; &minus; &beta;<sub>2</sub>}} and their opposite angles\n{{math|''B'' {{=}} &pi; &minus; &alpha;<sub>2</sub>}} and {{math|''A'' {{=}} &alpha;<sub>1</sub>}}.\n\nIn order to find the relation for the third side\n{{math|''AB'' {{=}} &sigma;<sub>12</sub>}}, the ''spherical arc length'', and included\nangle {{math|''N'' {{=}} &omega;<sub>12</sub>}}, the ''spherical longitude'', it is\nuseful to consider the triangle {{math|''NEP''}} representing a geodesic\nstarting at the equator; see Fig.  5.  In this figure, the\nvariables referred to the auxiliary sphere are shown with the\ncorresponding quantities for the ellipsoid shown in parentheses.\nQuantities without subscripts refer to the arbitrary point\n{{math|''P''}}; {{math|''E''}}, the point at which the geodesic crosses\nthe equator in the northward direction, is used as the origin for\n{{math|&sigma;}}, {{math|''s''}} and {{math|&omega;}}.\n\n[[File:Differential element of a geodesic on a sphere.svg|thumb|\nFig. 6.\nDifferential element of a geodesic on a sphere.]]\nIf the side {{math|''EP''}} is extended by\nmoving {{math|''P''}} infinitesimally (see Fig. 6), we\nobtain\n:(2)<math>{\\color{white}.}\\qquad\n\\cos\\alpha\\,d\\sigma = d\\beta, \\quad\n\\sin\\alpha\\,d\\sigma = \\cos\\beta\\,d\\omega.</math>\nCombining Eqs. (1) and (2) gives differential\nequations for {{math|''s''}} and {{math|&lambda;}}\n:<math>\\frac1a\\frac{ds}{d\\sigma}\n= \\frac{d\\lambda}{d\\omega}\n= \\frac{\\sin\\beta}{\\sin\\varphi}.</math>\n\nThe relation between {{math|&beta;}} and {{math|&phi;}} is\n:<math>\\tan\\beta = \\sqrt{1-e^2} \\tan\\varphi = (1-f) \\tan\\varphi,</math>\nwhich gives\n:<math>\\frac{\\sin\\beta}{\\sin\\varphi} = \\sqrt{1-e^2\\cos^2\\beta},</math>\nso that the differential equations for the geodesic become\n:<math>\\frac1a\\frac{ds}{d\\sigma} = \\frac{d\\lambda}{d\\omega}\n= \\sqrt{1-e^2\\cos^2\\beta}.</math>\n\nThe last step is to use {{math|&sigma;}} as the independent\nparameter in both of\nthese differential equations and thereby to express {{math|''s''}} and\n{{math|&lambda;}} as integrals.  Applying the sine rule to the vertices\n{{math|''E''}} and {{math|''G''}} in the spherical triangle\n{{math|''EGP''}} in Fig. 5 gives\n:<math>\\sin\\beta = \\sin\\beta(\\sigma;\\alpha_0) =\n\\cos\\alpha_0 \\sin\\sigma,</math>\nwhere {{math|&alpha;<sub>0</sub>}} is the azimuth at {{math|''E''}}.\nSubstituting this into the equation for {{math|''ds''/''d''&sigma;}} and\nintegrating the result gives\n:(3)<math>{\\color{white}.}\\qquad\n\\frac sb = \\int_0^\\sigma \\sqrt{1 + k^2 \\sin^2\\sigma'}\\,d\\sigma',\n</math>\nwhere\n:<math>k = e'\\cos\\alpha_0,</math>\nand the limits on the integral are chosen so that\n{{math|''s''(&sigma; {{=}} 0) {{=}} 0}}.  {{harvtxt|Legendre|1811|p=180}} pointed out\nthat the equation for {{math|''s''}} is the same as the equation for the\n[[Meridian arc#Meridian distance on the ellipsoid|arc on an ellipse]]\nwith semi-axes {{math|''b''{{radical|1 + ''e''&prime;<sup>2</sup> cos<sup>2</sup>&alpha;<sub>0</sub>}}}} and\n{{math|''b''}}.  In order to express the equation for\n{{math|&lambda;}} in terms of {{math|&sigma;}}, we write\n:<math>d\\omega = \\frac{\\sin\\alpha_0}{\\cos^2\\beta}\\,d\\sigma,</math>\nwhich follows from Eq. (2) and Clairaut's relation.\nThis yields\n:(4)<math>{\\color{white}.}\\qquad\n\\lambda - \\lambda_0 = \\omega - f\\sin\\alpha_0\n\\int_0^\\sigma\\frac\n{2-f}{1 + (1-f)\\sqrt{1 + k^2\\sin^2\\sigma'}}\n\\,d\\sigma',\n</math>\nand the limits on the integrals are chosen\nso that {{math|&lambda; {{=}} &lambda;<sub>0</sub>}} at the equator crossing,\n{{math|&sigma; {{=}} 0}}.\n\nThis completes the solution of the path of a geodesic using the\nauxiliary sphere.  By this device a great circle can be mapped exactly\nto a geodesic on an ellipsoid of revolution.\n\nThere are also several ways of approximating geodesics on a terrestrial\nellipsoid (with small flattening) {{harv|Rapp|1991|loc=§6}}; some of\nthese are described in the article on [[geographical distance]].\nHowever, these are typically comparable in complexity to the method for\nthe exact solution {{harv|Jekeli|2012|loc=§2.1.4}}.\n\n=== Behavior of geodesics ===\n[[File:Closed geodesics on an ellipsoid of revolution.svg|thumb|\nFig. 7.  Meridians and the equator are the only closed\ngeodesics.  (For the very flattened ellipsoids, there are other closed\ngeodesics; see Figs. 11 and 12).]]\n{{multiple image\n|align=right\n|direction=horizontal\n|width=220\n|image1=Long geodesic on an oblate ellipsoid.svg\n|caption1=Fig. 8.  Following the geodesic on the ellipsoid for about 5 circuits.\n|image2=Really long geodesic on an oblate ellipsoid.svg\n|caption2=Fig. 9.  The same geodesic after about 70 circuits.\n|header=Geodesic on an oblate ellipsoid ({{math|''f'' {{=}} {{frac|50}}}}) with {{math|&alpha;<sub>0</sub> {{=}} {{val|45|u=deg}}}}.\n}}\n[[File:Long geodesic on a prolate ellipsoid.svg|thumb|\nFig. 10.\nGeodesic on a prolate ellipsoid ({{math|''f'' {{=}} &minus;{{frac|50}}}}) with\n{{math|&alpha;<sub>0</sub> {{=}} {{val|45|u=deg}}}}.  Compare with\nFig. 8.]]\nFig. 7 shows the simple closed geodesics which consist of the\nmeridians (green) and the equator (red).  (Here the qualification\n\"simple\" means that the geodesic closes on itself without an intervening\nself-intersection.)  This follows from the equations for the geodesics\ngiven in the previous section.\n\nAll other geodesics are typified by Figs. 8 and 9\nwhich show a geodesic starting on the equator with\n{{math|&alpha;<sub>0</sub> {{=}} 45°}}.  The geodesic oscillates about the equator.\nThe equatorial crossings are called ''nodes'' and the\npoints of maximum or minimum latitude are called ''vertices''; the\nvertex latitudes are given by\n{{math|&beta; {{=}} &plusmn;({{frac|1|2}}&pi; &minus; {{!}}&alpha;<sub>0</sub>{{!}})}}.\nThe geodesic completes one full oscillation in\nlatitude before the longitude has increased by {{val|360|u=deg}}.\nThus, on each successive northward crossing of the equator (see\nFig. 8), {{math|&lambda;}} falls short of a full circuit of\nthe equator by approximately {{math|2&pi; ''f'' sin&alpha;<sub>0</sub>}} (for a\nprolate ellipsoid, this quantity is negative and {{math|&lambda;}}\ncompletes more that a full circuit; see Fig. 10).  For nearly all\nvalues of {{math|&alpha;<sub>0</sub>}}, the geodesic will fill that portion of\nthe ellipsoid between the two vertex latitudes (see\nFig. 9).\n\n{{multiple image\n|align=right\n|direction=horizontal\n|width=220\n|image1=Non-standard closed geodesics on an ellipsoid of revolution 1.svg\n|caption1=Fig. 11.  Side view.\n|image2=Non-standard closed geodesics on an ellipsoid of revolution 2.svg\n|caption2=Fig. 12.  Top view.\n|header=Two additional closed geodesics for the oblate ellipsoid, {{math|{{frac|''b''|''a''}} {{=}} {{frac|2|7}}}}.\n}}\nIf the ellipsoid is sufficiently oblate, i.e.,\n{{math|{{frac|''b''|''a''}} &lt; {{frac|1|2}}}}, another class of simple closed geodesics is\npossible {{harv|Klingenberg|1982|loc=§3.5.19}}.  Two such geodesics\nare illustrated in Figs. 11 and 12.  Here\n{{math|{{frac|''b''|''a''}} {{=}} {{frac|2|7}}}} and the equatorial azimuth,\n{{math|&alpha;<sub>0</sub>}}, for the green (resp. blue) geodesic is chosen to\nbe {{val|53.175|u=deg}} (resp. {{val|75.192|u=deg}}), so that the\ngeodesic completes 2 (resp. 3) complete oscillations about the equator\non one circuit of the ellipsoid.\n\n[[File:Geodesics and geodesic circles on an oblate ellipsoid.svg|thumb|\nFig. 13.\nGeodesics (blue) from a single point for {{math|''f'' {{=}} {{frac|10}}}},\n{{math|&phi;<sub>1</sub> {{=}} {{val|-30|u=deg}}}};\ngeodesic circles are shown in green and the cut locus in red.]]\nFig.\n13 shows geodesics (in blue) emanating\n{{math|''A''}} with {{math|&alpha;<sub>1</sub>}} a multiple of\n{{val|15|u=deg}} up to the point at which they cease to be shortest\npaths.  (The flattening has been increased to\n{{frac|10}} in order to accentuate the ellipsoidal effects.)\nAlso shown (in green) are curves of constant {{math|''s''<sub>12</sub>}},\nwhich are the geodesic circles centered {{math|''A''}}.\n{{harvtxt|Gauss|1828}} showed that, on any surface, geodesics and\ngeodesic circle intersect at right angles.  The red line is the\n[[cut locus]], the locus of points which have multiple (two in this\ncase) shortest geodesics from {{math|''A''}}.  On a sphere, the cut\nlocus is a point.  On an oblate ellipsoid (shown here), it is a segment\nof the circle of latitude centered on the point [[antipodes|antipodal]]\nto {{math|''A''}}, {{math|&phi; {{=}} &minus;&phi;<sub>1</sub>}}.  The longitudinal\nextent of cut locus is approximately\n{{math|&lambda;<sub>12</sub> &isin; [&pi; &minus; ''f'' &pi; cos&phi;<sub>1</sub>, &pi; + ''f'' &pi; cos&phi;<sub>1</sub>]}}.  If\n{{math|''A''}} lies on the equator, {{math|&phi;<sub>1</sub> {{=}} 0}}, this relation\nis exact and as a consequence the equator is only a shortest geodesic if\n{{math|{{!}}&lambda;<sub>12</sub>{{!}} &le; (1 &minus; ''f'')&pi;}}.  For a prolate\nellipsoid, the cut locus is a segment of the anti-meridian centered on\nthe point antipodal to {{math|''A''}}, {{math|&lambda;<sub>12</sub> {{=}} &pi;}},\nand this means that\nmeridional geodesics stop being shortest paths before the antipodal\npoint is reached.\n\n=== Solution of the direct and inverse problems ===\nSolving the geodesic problems entails mapping the geodesic onto the\nauxiliary sphere and solving the corresponding problem in\n[[great-circle navigation]].\nWhen solving the\n\"elementary\" spherical triangle for {{math|''NEP''}} in Fig.\n5,\n[[Spherical trigonometry#Napier's rules for quadrantal triangles|\nNapier's rules for quadrantal triangles]] can be employed,\n:<math>\n\\begin{align}\n\\sin\\alpha_0 &= \\sin\\alpha \\cos\\beta = \\tan\\omega \\cot\\sigma, \\\\\n\\cos\\sigma &= \\cos\\beta \\cos\\omega  = \\tan\\alpha_0 \\cot\\alpha, \\\\\n\\cos\\alpha &= \\cos\\omega \\cos\\alpha_0 = \\cot\\sigma \\tan\\beta, \\\\\n\\sin\\beta &= \\cos\\alpha_0 \\sin\\sigma = \\cot\\alpha \\tan\\omega, \\\\\n\\sin\\omega &= \\sin\\sigma \\sin\\alpha = \\tan\\beta \\tan\\alpha_0.\n\\end{align}\n</math>\nThe mapping of the geodesic involves evaluating the\nintegrals for the distance, {{math|''s''}}, and the longitude,\n{{math|&lambda;}}, Eqs. (3) and (4) and these depend on\nthe parameter {{math|&alpha;<sub>0</sub>}}.\n\nHandling the direct problem is straightforward, because\n{{math|&alpha;<sub>0</sub>}} can be determined directly from the given\nquantities {{math|&phi;<sub>1</sub>}} and {{math|&alpha;<sub>1</sub>}}.\n\nIn the case of the inverse problem, {{math|&lambda;<sub>12</sub>}} is given;\nthis cannot be easily related to the equivalent spherical angle\n{{math|&omega;<sub>12</sub>}} because {{math|&alpha;<sub>0</sub>}} is unknown.\nThus, the solution of the problem requires that {{math|&alpha;<sub>0</sub>}} be\nfound iteratively.\n\nIn geodetic applications, where {{math|''f''}} is small, the integrals\nare typically evaluated as a series {{harv|Legendre|1806}}\n{{harv|Oriani|1806}} {{harv|Bessel|1825}} {{harv|Helmert|1880}}\n{{harv|Rainsford|1955}} {{harv|Rapp|1993}}.  For arbitrary\n{{math|''f''}}, the integrals (3) and (4) can be found by\nnumerical quadrature or by expressing them in terms of\n[[elliptic integrals]] {{harv|Legendre|1806}} {{harv|Cayley|1870}}.\n\n{{harvtxt|Vincenty|1975}} provides solutions for the direct and inverse\nproblems; these are based on a series expansion carried out to third\norder in the flattening and provide an accuracy of about\n{{val|0.1|u=mm}} for the [[WGS84]] ellipsoid; however the inverse method\nfails to converge for nearly antipodal points.  {{harvtxt|Karney|2013}}\ncontinues the expansions to sixth order which suffices to provide full\n[[double precision]] accuracy for\n{{math|{{!}}''f''{{!}} &le; {{frac|1|50}}}} and improves the solution of\nthe inverse problem so that it converges in all cases.\n{{harvtxt|Karney|2013|loc=addendum}} extends the method to use elliptic\nintegrals which can be applied to ellipsoids with arbitrary flattening.\n\n=== Differential properties of geodesics ===\nVarious problems involving geodesics require knowing their behavior\nwhen they are perturbed.  This is useful in trigonometric adjustments\n{{harv|Ehlert|1993}},\ndetermining the physical properties of signals which follow geodesics,\netc.  Consider a reference geodesic, parameterized by {{math|''s''}},\nand a second geodesic a small\ndistance {{math|''t''(''s'')}} away from it.  {{harvtxt|Gauss|1828}} showed that\n{{math|''t''(''s'')}} obeys the\n[[Differential geometry of surfaces#Gauss–Jacobi equation|\nGauss-Jacobi equation]]\n:<math>{\\color{white}.}\\qquad\n\\displaystyle\\frac{d^2t(s)}{ds^2} = K(s) t(s), </math>\n[[File:Definition of reduced length and geodesic scale.svg|thumb|\nFig. 14.\nDefinition of reduced length and geodesic scale.]]\nwhere {{math|''K''(''s'')}} is the [[Gaussian curvature]] at {{math|''s''}}.\nAs a second order, linear, homogeneous differential equation,\nits solution may be expressed as the sum of two independent solutions\n:<math> t(s_2) = C m(s_1,s_2) + D M(s_1,s_2) </math>\nwhere\n:<math>\n\\begin{align}\nm(s_1, s_1) &= 0, \\quad\n\\left.\\frac{dm(s_1,s_2)}{ds_2}\\right|_{s_2 = s_1} = 1, \\\\\nM(s_1, s_1) &= 1, \\quad\n\\left.\\frac{dM(s_1,s_2)}{ds_2}\\right|_{s_2 = s_1} = 0.\n\\end{align}\n</math>\nThe quantity {{math|''m''(''s''<sub>1</sub>, ''s''<sub>2</sub>) {{=}} ''m''<sub>12</sub>}} is the so-called\n''reduced length'', and {{math|''M''(''s''<sub>1</sub>, ''s''<sub>2</sub>) {{=}} ''M''<sub>12</sub>}} is the\n''geodesic scale''.{{refn|\n{{harvtxt|Bagratuni|1962|loc=§17}} uses the term \"coefficient of\nconvergence of ordinates\" for the geodesic scale.}}\nTheir basic definitions are illustrated in\nFig. 14.\n\nThe\n[[Ellipsoid of revolution#Curvature|Gaussian curvature for an ellipsoid of revolution]]\nis\n:<math>\nK = \\frac1{\\rho\\nu} = \\frac{\\bigl(1-e^2\\sin^2\\varphi\\bigr)^2}{b^2}\n  = \\frac{b^2}{a^4\\bigl(1-e^2\\cos^2\\beta\\bigr)^2}.\n</math>\n{{harvtxt|Helmert|1880|loc=Eq. (6.5.1.)}} solved the Gauss-Jacobi\nequation for this case enabling {{math|''m''<sub>12</sub>}} and\n{{math|''M''<sub>12</sub>}} to be expressed as integrals.\n\nAs we see from Fig. 14 (top sub-figure), the separation of two\ngeodesics starting at the same point with azimuths differing by\n{{math|''d''&alpha;<sub>1</sub>}} is {{math|''m''<sub>12</sub> ''d''&alpha;<sub>1</sub>}}.  On a closed\nsurface such as an ellipsoid, {{math|''m''<sub>12</sub>}} oscillates\nabout zero.  The point at\nwhich {{math|''m''<sub>12</sub>}} becomes zero is the point\n[[conjugate point|conjugate]] to the starting point.  In order\nfor a geodesic between {{math|''A''}} and {{math|''B''}}, of length\n{{math|''s''<sub>12</sub>}}, to be a shortest path it must satisfy the\nJacobi condition {{harv|Jacobi|1837}} {{harv|Jacobi|1866|loc=§6}}\n{{harv|Forsyth|1927|loc=§§26–27}}\n{{harv|Bliss|1916}}, that there is\nno point conjugate to {{math|''A''}} between {{math|''A''}} and\n{{math|''B''}}.  If this condition is not satisfied, then there is a\n''nearby'' path (not necessarily a geodesic) which is shorter.  Thus,\nthe Jacobi condition is a local property of the geodesic and is only a\nnecessary condition for the geodesic being a global shortest path.\nNecessary and sufficient conditions for a geodesic being the shortest\npath are:\n* for an oblate ellipsoid, {{math|{{!}}&sigma;<sub>12</sub>{{!}} &le; &pi;}};\n* for a prolate ellipsoid, {{math|{{!}}&lambda;<sub>12</sub>{{!}} &le; &pi;}}, if {{math|&alpha;<sub>0</sub> &ne; 0}}; if {{math|&alpha;<sub>0</sub> {{=}} 0}}, the supplemental condition {{math|''m''<sub>12</sub> &ge; 0}} is required if {{math|{{!}}&lambda;<sub>12</sub>{{!}} {{=}} &pi;}}.\n\n=== Envelope of geodesics ===\n{{multiple image\n|align=right\n|direction=horizontal\n|width=220\n|image1=Envelope of geodesics on an oblate ellipsoid.svg\n|caption1=Fig. 15.  The envelope of geodesics from a point {{math|''A''}} at {{math|&phi;<sub>1</sub> {{=}} {{val|-30|u=deg}}}}.\n|image2=Four geodesics connecting two points on an oblate ellipsoid.svg\n|caption2=Fig. 16.  The four geodesics connecting {{math|''A''}} and a point {{math|''B''}}, {{math|&phi;<sub>2</sub> {{=}} {{val|26|u=deg}}}}, {{math|&lambda;<sub>12</sub> {{=}} {{val|175|u=deg}}}}.\n|header=Geodesics from a single point ({{math|''f'' {{=}} {{frac|10}}}}, {{math|&phi;<sub>1</sub> {{=}} {{val|-30|u=deg}}}})\n}}\n\nThe geodesics from a particular point {{math|''A''}} if continued\npast the cut locus form an envelope illustrated in Fig. 15.\nHere the geodesics for which {{math|&alpha;<sub>1</sub>}} is a multiple of\n{{val|3|u=deg}} are shown in light blue.  (The geodesics are only\nshown for their first passage close to the antipodal point, not for\nsubsequent ones.)  Some geodesic circles are shown in green; these form\ncusps on the envelope.  The cut locus is shown in red.  The envelope is\nthe locus of points which are conjugate to {{math|''A''}}; points on the\nenvelope may be computed by finding the point at which\n{{math|''m''<sub>12</sub> {{=}} 0}} on a geodesic.\n{{harvtxt|Jacobi|1891}} calls this star-like figure\nproduced by the envelope an [[astroid]].\n\nOutside the astroid two geodesics intersect at each point; thus there\nare two geodesics (with a length approximately half the\ncircumference of the ellipsoid) between {{math|''A''}} and these points.\nThis corresponds to the situation on the sphere where there are \"short\"\nand \"long\" routes on a great circle between two points.  Inside the\nastroid four geodesics intersect at each point.  Four such geodesics are\nshown in Fig. 16 where the geodesics are numbered in order of\nincreasing length.  (This figure uses the same position for\n{{math|''A''}} as Fig. 13 and is drawn in the same projection.)\nThe two shorter geodesics are ''stable'', i.e., {{math|''m''<sub>12</sub> &gt; 0}},\nso that there is no nearby path connecting the two points which is\nshorter; the other two are unstable.  Only the shortest line (the first\none) has {{math|&sigma;<sub>12</sub> &le; &pi;}}.  All the geodesics are tangent\nto the envelope which is shown in green in the figure.\n\nThe astroid is the (exterior) [[evolute]] of the geodesic circles\ncentered at {{math|''A''}}.  Likewise, the geodesic circles are\n[[involute]]s of the astroid.\n\n=== Area of a geodesic polygon ===\nA ''geodesic polygon'' is a polygon whose sides are geodesics.  The area of\nsuch a polygon may be found by first computing the area between a\ngeodesic segment and the equator, i.e., the area of the quadrilateral\n{{math|''AFHB''}} in Fig. 1 {{harv|Danielsen|1989}}.  Once this\narea is known, the area of a polygon may be computed by summing the\ncontributions from all the edges of the polygon.\n\nHere an expression for the area {{math|''S''<sub>12</sub>}} of {{math|''AFHB''}}\nis developed following {{harvtxt|Sjöberg|2006}}.  The area of any closed\nregion of the ellipsoid is\n:<math> T = \\int dT = \\int \\frac1K \\cos\\varphi\\,d\\varphi\\,d\\lambda,\n</math>\nwhere {{math|''dT''}} is an element of surface area and {{math|''K''}}\nis the [[Gaussian curvature]].  Now the\n[[Gauss–Bonnet theorem]] applied to a geodesic polygon states\n:<math>\n\\Gamma = \\int K \\,dT = \\int \\cos\\varphi\\,d\\varphi\\,d\\lambda,\n</math>\nwhere\n:<math>\n\\Gamma = 2\\pi - \\sum_j \\theta_j\n</math>\nis the geodesic excess and {{math|&theta;<sub>''j''</sub>}} is the exterior angle at\nvertex {{math|''j''}}.  Multiplying the equation for {{math|&Gamma;}}\nby {{math|''R''<sub>2</sub><sup>2</sup>}}, where {{math|''R''<sub>2</sub>}} is the\n[[Earth radius#Authalic radius|authalic radius]], and subtracting this\nfrom the equation for {{math|''T''}} gives\n:<math>\n\\begin{align}\nT &=\nR_2^2 \\,\\Gamma + \\int \\biggl(\\frac1K - R_2^2\\biggr)\\cos\\varphi\\,d\\varphi\\,d\\lambda \\\\\n&=R_2^2 \\,\\Gamma + \\int \\Biggl(\n\\frac{b^2}{\\bigl(1 - e^2\\sin^2\\varphi\\bigr)^2} - R_2^2\n\\Biggr)\\cos\\varphi\\,d\\varphi\\,d\\lambda,\n\\end{align}\n</math>\nwhere the [[Spheroid#Curvature|value of {{math|''K''}} for an ellipsoid]]\nhas been substituted.\nApplying this formula to the quadrilateral {{math|''AFHB''}}, noting\nthat {{math|&Gamma; {{=}} &alpha;<sub>2</sub> &minus; &alpha;<sub>1</sub>}}, and performing\nthe integral over {{math|&phi;}} gives\n:<math>\nS_{12}=R_2^2 (\\alpha_2-\\alpha_1)\n+ b^2 \\int_{\\lambda_1}^{\\lambda_2} \\biggl(\n\\frac1{2\\bigl(1 - e^2\\sin^2\\varphi\\bigr)}+\n\\frac{\\tanh^{-1}(e \\sin\\varphi)}{2e \\sin\\varphi}\n- \\frac{R_2^2}{b^2}\\biggr)\\sin\\varphi\n\\,d\\lambda,\n</math>\nwhere the integral is over the geodesic line (so that {{math|&phi;}}\nis implicitly a function of {{math|&lambda;}}).\nThe integral can be expressed as a series valid for small {{math|''f''}}\n{{harv|Danielsen|1989}} {{harv|Karney|2013|loc=§6 and addendum}}.\n\nThe area of a geodesic polygon is given by summing {{math|''S''<sub>12</sub>}}\nover its edges.  This result holds provided that the polygon does not\ninclude a pole; if it does, {{math|2&pi; ''R''<sub>2</sub><sup>2</sup>}} must be added to the\nsum.   If the edges are specified by their vertices, then a\n[[Spherical excess|convenient expression]]\nfor the geodesic excess {{math|''E''<sub>12</sub> {{=}} &alpha;<sub>2</sub> &minus; &alpha;<sub>1</sub>}} is\n:<math>\n\\tan\\frac{E_{12}}2 =\n\\frac{\\sin\\tfrac12 (\\beta_2 + \\beta_1)}\n{\\cos\\tfrac12 (\\beta_2 - \\beta_1)} \\tan\\frac{\\omega_{12}}2.\n</math>\n\n== Geodesics on a triaxial ellipsoid ==\nSolving the geodesic problem for an ellipsoid of revolution is, from the\nmathematical point of view, relatively simple: because of symmetry,\ngeodesics have a constant of the motion, given by Clairaut's relation\nallowing the problem to be reduced to\n[[quadrature (mathematics)|quadrature]].  By the early 19th century\n(with the work of Legendre, [[Barnaba Oriani|Oriani]], Bessel, et al.),\nthere was a complete understanding of the properties of geodesics on an\nellipsoid of revolution.\n\nOn the other hand, geodesics on a triaxial ellipsoid (with three unequal\naxes) have no obvious constant of the motion and thus represented a\nchallenging \"unsolved\" problem in the first half of the 19th\ncentury.  In a remarkable paper, {{harvtxt|Jacobi|1839}} discovered a\nconstant of the motion allowing this problem to be reduced to quadrature\nalso {{harv|Klingenberg|1982|loc=§3.5}}.{{refn|\nThis section is adapted from the documentation for GeographicLib\n{{harv|Karney|2015|\nloc=[https://geographiclib.sourceforge.io/1.44/triaxial.html Geodesics on a triaxial ellipsoid]}}}}\n\n=== The triaxial coordinate system ===\nConsider the ellipsoid defined by\n:<math>\n  h = \\frac{X^2}{a^2} + \\frac{Y^2}{b^2} + \\frac{Z^2}{c^2} = 1,\n</math>\nwhere {{math|(''X'',''Y'',''Z'')}} are Cartesian coordinates centered on the\nellipsoid and, without loss of generality,\n{{math|''a'' &ge; ''b'' &ge; ''c'' &gt; 0}}.{{refn|1=\nThis notation for the semi-axes is incompatible with that used in the\nprevious section on ellipsoids of revolution in which {{math|''a''}} and\n{{math|''b''}} stood for the equatorial radius and polar semi-axis.\nThus the corresponding inequalities are {{math|''a'' {{=}} ''a'' &ge; ''b'' &gt; 0}} for\nan oblate ellipsoid and {{math|''b'' &ge; ''a'' {{=}} ''a'' &gt; 0}} for a prolate\nellipsoid.}}\n{{harvtxt|Jacobi|1866|loc=§§26–27}}\nemployed the ''ellipsoidal'' latitude and longitude\n{{math|(&beta;, &omega;)}} defined by\n[[File:Triaxial ellipsoid coordinate system.svg|thumb|\nFig. 17.\n[[Ellipsoidal coordinates]].]]\n:<math>\n\\begin{align}\n  X &= a \\cos\\omega\n      \\frac{\\sqrt{a^2 - b^2\\sin^2\\beta - c^2\\cos^2\\beta}}\n           {\\sqrt{a^2 - c^2}}, \\\\\n  Y &= b \\cos\\beta \\sin\\omega, \\\\\n  Z &= c \\sin\\beta\n      \\frac{\\sqrt{a^2\\sin^2\\omega + b^2\\cos^2\\omega - c^2}}\n           {\\sqrt{a^2 - c^2}}.\n\\end{align}\n</math>\nIn the limit {{math|''b'' &rarr; ''a''}}, {{math|&beta;}}\nbecomes the parametric latitude for an oblate ellipsoid, so the use of\nthe symbol {{math|&beta;}} is consistent with the previous sections.\nHowever, {{math|&omega;}} is ''different'' from the spherical\nlongitude defined above.{{refn|\nThe limit {{math|''b'' &rarr; ''c''}} gives a prolate ellipsoid with\n{{math|&omega;}} playing the role of the parametric latitude.}}\n\nGrid lines of constant {{math|&beta;}} (in blue) and\n{{math|&omega;}} (in green) are given in Fig. 17.  These\nconstitute an [[orthogonal]] coordinate system: the\ngrid lines intersect at right angles.  The principal sections of the\nellipsoid, defined by {{math|''X'' {{=}} 0}} and {{math|''Z'' {{=}} 0}} are shown in\nred.  The third principal section, {{math|''Y'' {{=}} 0}}, is covered by the\nlines {{math|&beta; {{=}} &plusmn;90°}} and {{math|&omega; {{=}} 0°}} or\n{{math|&plusmn;180°}}.  These lines meet at four\n[[umbilical point]]s (two of which are visible in this figure) where the\n[[principal curvature|principal radii of curvature]] are equal.  Here\nand in the other figures in this section the parameters of the ellipsoid\nare {{math|''a'':''b'':''c'' {{=}} 1.01:1:0.8}}, and it is viewed in an orthographic\nprojection from a point above {{math|&phi; {{=}} 40°}},\n{{math|&lambda; {{=}} 30°}}.\n\nThe grid lines of the ellipsoidal coordinates may be interpreted in three\ndifferent ways:\n# They are \"lines of curvature\" on the ellipsoid: they are parallel to the directions of principal curvature {{harv|Monge|1796}}.\n# They are also intersections of the ellipsoid with [[confocal ellipsoidal coordinates|confocal systems of hyperboloids of one and two sheets]] {{harv|Dupin|1813|loc = [https://books.google.com/books?id=j40AAAAAMAAJ&pg=PA297 Part 5]}}.\n# Finally they are geodesic ellipses and hyperbolas defined using two adjacent umbilical points {{harv|Hilbert|Cohn-Vossen|1952|p=188}}.  For example, the lines of constant {{math|&beta;}} in Fig. 17 can be generated with the familiar [[Gardener's ellipse|string construction for ellipses]] with the ends of the string pinned to the two umbilical points.\n\n=== Jacobi's solution ===\nJacobi showed that the geodesic equations, expressed in ellipsoidal\ncoordinates, are separable.  Here is how he recounted his discovery to\nhis friend and neighbor Bessel {{harv|Jacobi|1839|loc=Letter to Bessel}},\n<blockquote> The day before yesterday, I reduced to quadrature the problem of geodesic lines on an ''ellipsoid with three unequal axes''.  They are the simplest formulas in the world, [[Abelian integral]]s, which become the well known elliptic integrals if 2 axes are set equal.<br>\n[[Königsberg]], 28th Dec. '38.\n</blockquote>\n\nThe solution given by Jacobi {{harv|Jacobi|1839}}\n{{harv|Jacobi|1866|loc=§28}} is\n:<math>\n\\begin{align}\n\\delta &= \\int \\frac\n{\\sqrt{b^2\\sin^2\\beta + c^2\\cos^2\\beta}\\,d\\beta}\n{\\sqrt{a^2 - b^2\\sin^2\\beta - c^2\\cos^2\\beta}\n \\sqrt{\\bigl(b^2-c^2\\bigr)\\cos^2\\beta - \\gamma}} \\\\[6pt]\n&\\quad -\n\\int \\frac\n{\\sqrt{a^2\\sin^2\\omega + b^2\\cos^2\\omega}\\,d\\omega}\n{\\sqrt{a^2\\sin^2\\omega + b^2\\cos^2\\omega - c^2}\n \\sqrt{\\bigl(a^2-b^2\\bigr)\\sin^2\\omega + \\gamma}}.\n\\end{align}\n</math>\nAs Jacobi notes \"a function of the angle {{math|&beta;}} equals\na function of the angle {{math|&omega;}}.  These two functions are\njust Abelian integrals...\"  Two constants {{math|&delta;}} and\n{{math|&gamma;}} appear in the solution.  Typically\n{{math|&delta;}} is zero if the lower limits of the integrals are\ntaken to be the starting point of the geodesic and the direction of the\ngeodesics is determined by {{math|&gamma;}}.  However, for geodesics\nthat start at an umbilical points, we have {{math|&gamma; {{=}} 0}} and\n{{math|&delta;}} determines the direction at the umbilical point.\nThe constant {{math|&gamma;}} may be expressed as\n:<math>\n\\gamma = \\bigl(b^2-c^2\\bigr)\\cos^2\\beta\\sin^2\\alpha-\n\\bigl(a^2-b^2\\bigl)\\sin^2\\omega\\cos^2\\alpha,\n</math>\nwhere {{math|&alpha;}} is the angle the geodesic makes with lines of\nconstant {{math|&omega;}}. In the limit {{math|''b'' &rarr; ''a''}},\nthis reduces to {{math|sin&alpha; cos&beta; {{=}} const.}}, the\nfamiliar Clairaut relation.  A derivation of Jacobi's result is\ngiven by {{harvtxt|Darboux|1894|loc=§§583–584}}; he\ngives the solution found by {{harvtxt|Liouville|1846}} for general quadratic\nsurfaces.\n\n=== Survey of triaxial geodesics ===\n{{multiple image\n|align=right\n|direction=horizontal\n|width=220\n|image1=Circumpolar geodesic on a triaxial ellipsoid case A.svg\n|image2=Circumpolar geodesic on a triaxial ellipsoid case B.svg\n|header=Circumpolar geodesics, {{math|&omega;<sub>1</sub> {{=}} {{val|0|u=deg}}}}, {{math|&alpha;<sub>1</sub> {{=}} {{val|90|u=deg}}}}.\n|caption1=Fig. 18. {{math|&beta;<sub>1</sub> {{=}} {{val|45.1|u=deg}}}}.\n|caption2=Fig. 19. {{math|&beta;<sub>1</sub> {{=}} {{val|87.48|u=deg}}}}.\n}}\nOn a triaxial ellipsoid, there are only three simple closed geodesics, the\nthree principal sections of the ellipsoid given by {{math|''X'' {{=}} 0}},\n{{math|''Y'' {{=}} 0}}, and {{math|''Z'' {{=}} 0}}.{{refn|\nIf {{math|{{frac|''c''|''a''}} &lt; {{frac|1|2}}}}, there are other simple closed geodesics\nsimilar to those shown in Figs. 11 and 12\n{{harv|Klingenberg|1982|loc=§3.5.19}}.}}\nTo survey the other geodesics, it is convenient to consider geodesics\nthat intersect the middle principal section, {{math|''Y'' {{=}} 0}}, at right\nangles.  Such geodesics are shown in Figs. 18–22,\nwhich use the same ellipsoid parameters and the same viewing direction\nas Fig. 17.  In addition, the three principal ellipses are shown\nin red in each of these figures.\n\nIf the starting point is {{math|&beta;<sub>1</sub> &isin; (&minus;90°, 90°)}},\n{{math|&omega;<sub>1</sub> {{=}} 0}}, and {{math|&alpha;<sub>1</sub> {{=}} 90°}}, then\n{{math|&gamma; &gt; 0}} and the\ngeodesic encircles the ellipsoid in a \"circumpolar\" sense.  The geodesic\noscillates north and south of the equator; on each oscillation it\ncompletes slightly less than a full circuit around the ellipsoid\nresulting, in the typical case, in the geodesic filling the area bounded\nby the two latitude lines {{math|&beta; {{=}} &plusmn;&beta;<sub>1</sub>}}.  Two examples\nare given in Figs. 18 and 19.  Figure 18 shows\npractically the same behavior as for an oblate ellipsoid of revolution\n(because {{math|''a'' &asymp; ''b''}}); compare to Fig. 9.\nHowever, if the starting point is at a higher latitude (Fig. 18)\nthe distortions resulting from {{math|''a'' &ne; ''b''}} are evident.  All\ntangents to a circumpolar geodesic touch the confocal single-sheeted\nhyperboloid which intersects the ellipsoid at {{math|&beta; {{=}} &beta;<sub>1</sub>}}\n{{harv|Chasles|1846}}\n{{harv|Hilbert|Cohn-Vossen|1952|pp=223–224}}.\n\n{{multiple image\n|align=right\n|direction=horizontal\n|width=220\n|image1=Transpolar geodesic on a triaxial ellipsoid case A.svg\n|image2=Transpolar geodesic on a triaxial ellipsoid case B.svg\n|header=Transpolar geodesics, {{math|&beta;<sub>1</sub> {{=}} {{val|90|u=deg}}}}, {{math|&alpha;<sub>1</sub> {{=}} {{val|180|u=deg}}}}.\n|caption1=Fig. 20. {{math|&omega;<sub>1</sub> {{=}} {{val|39.9|u=deg}}}}.\n|caption2=Fig. 21. {{math|&omega;<sub>1</sub> {{=}} {{val|9.966|u=deg}}}}.\n}}\nIf the starting point is {{math|&beta;<sub>1</sub> {{=}} 90°}},\n{{math|&omega;<sub>1</sub> &isin; (0°, 180°)}}, and\n{{math|&alpha;<sub>1</sub> {{=}} 180°}}, then\n{{math|&gamma; &lt; 0}} and the geodesic encircles the ellipsoid\nin a \"transpolar\" sense.  The geodesic oscillates east and west of the\nellipse {{math|''X'' {{=}} 0}}; on each oscillation it completes slightly more\nthan a full circuit around the ellipsoid.  In the typical case, this results\nin the geodesic filling the area bounded by the two longitude lines\n{{math|&omega; {{=}} &omega;<sub>1</sub>}} and {{math|&omega; {{=}} 180° &minus; &omega;<sub>1</sub>}}.\nIf {{math|''a'' {{=}} ''b''}}, all meridians are geodesics; the effect of\n{{math|''a'' &ne; ''b''}} causes such geodesics to oscillate east and west.\nTwo examples are given in Figs. 20 and 21.  The constriction\nof the geodesic near the pole disappears in the limit\n{{math|''b'' &rarr; ''c''}}; in this case, the ellipsoid becomes a\nprolate ellipsoid and Fig. 20 would resemble Fig. 10\n(rotated on its side).  All tangents to a transpolar geodesic touch the\nconfocal double-sheeted hyperboloid which intersects the ellipsoid at\n{{math|&omega; {{=}} &omega;<sub>1</sub>}}.\n\n[[File:Unstable umbilical geodesic on a triaxial ellipsoid.svg|thumb|\nFig. 22.  An umbilical geodesic,\n{{math|&beta;<sub>1</sub> {{=}} {{val|90|u=deg}}}},\n{{math|&omega;<sub>1</sub> {{=}} {{val|0|u=deg}}}},\n{{math|&alpha;<sub>1</sub> {{=}} {{val|135|u=deg}}}}.]]\nIf the starting point is {{math|&beta;<sub>1</sub> {{=}} 90°}},\n{{math|&omega;<sub>1</sub> {{=}} 0°}} (an umbilical point), and\n{{math|&alpha;<sub>1</sub> {{=}} 135°}} (the geodesic leaves the ellipse\n{{math|''Y'' {{=}} 0}} at right angles), then\n{{math|&gamma; {{=}} 0}} and the geodesic repeatedly\nintersects the opposite umbilical point and returns to its starting\npoint.  However, on each circuit the angle at which it intersects\n{{math|''Y'' {{=}} 0}} becomes closer to {{val|0|u=deg}} or\n{{val|180|u=deg}} so that asymptotically the geodesic lies on the\nellipse {{math|''Y'' {{=}} 0}} {{harv|Hart|1849}} {{harv|Arnold|1989|p=265}},\nas shown in Fig. 22.  A single geodesic does not\nfill an area on the ellipsoid.  All tangents to umbilical geodesics\ntouch the confocal hyperbola that intersects the ellipsoid at the\numbilic points.\n\nUmbilical geodesic enjoy several interesting properties.\n* Through any point on the ellipsoid, there are two umbilical geodesics.\n* The geodesic distance between opposite umbilical points is the same regardless of the initial direction of the geodesic.\n* Whereas the closed geodesics on the ellipses {{math|''X'' {{=}} 0}} and {{math|''Z'' {{=}} 0}} are stable (an geodesic initially close to and nearly parallel to the ellipse remains close to the ellipse), the closed geodesic on the ellipse {{math|''Y'' {{=}} 0}}, which goes through all 4 umbilical points, is ''exponentially unstable''.  If it is perturbed, it will swing out of the plane {{math|''Y'' {{=}} 0}} and flip around before returning to close to the plane.  (This behavior may repeat depending on the nature of the initial perturbation.)\n\nIf the starting point {{math|''A''}} of a geodesic is not an umbilical\npoint, its envelope is an astroid with two cusps lying on\n{{math|&beta; {{=}} &minus;&beta;<sub>1</sub>}} and the other two on\n{{math|&omega; {{=}} &omega;<sub>1</sub> + &pi;}}.  The cut locus\nfor {{math|''A''}} is the portion\nof the line {{math|&beta; {{=}} &minus;&beta;<sub>1</sub>}} between the cusps.\n\n== Applications ==\nThe direct and inverse geodesic problems no longer play the central role\nin geodesy that they once did.  Instead of solving\n[[adjustment of observations|adjustment]] of [[geodetic networks]] as a\ntwo-dimensional problem in spheroidal trigonometry, these problems are\nnow solved by three-dimensional methods {{harv|Vincenty|Bowring|1978}}.\nNevertheless, terrestrial geodesics still play an important role in\nseveral areas:\n* for measuring distances and areas in [[geographic information systems]];\n* the definition of [[maritime boundaries]] {{harv|UNCLOS|2006}};\n* in the rules of the [[Federal Aviation Administration]] for area navigation {{harv|RNAV|2007}};\n* the method of measuring distances in the [[Fédération Aéronautique Internationale|FAI]] Sporting Code {{harv|FAI|2018}}.\n\nBy the [[principle of least action]], many problems in physics can be\nformulated as a variational problem similar to that for geodesics.  Indeed,\nthe geodesic problem is equivalent to the motion of a particle\nconstrained to move on the surface, but otherwise subject to no forces\n{{harv|Laplace|1799a}} {{harv|Hilbert|Cohn-Vossen|1952|p=222}}.\nFor this reason,\ngeodesics on simple surfaces such as ellipsoids of revolution or\ntriaxial ellipsoids are frequently used as \"test cases\" for exploring new\nmethods.  Examples include:\n* the development of elliptic integrals {{harv|Legendre|1811}} and [[elliptic functions]] {{harv|Weierstrass|1861}};\n* the development of differential geometry {{harv|Gauss|1828}} {{harv|Christoffel|1869}};\n* methods for solving systems of differential equations by a change of independent variables {{harv|Jacobi|1839}};\n* the study of [[caustic (optics)|caustics]] {{harv|Jacobi|1891}};\n* investigations into the number and stability of periodic orbits {{harv|Poincaré|1905}};\n* in the limit {{math|''c'' &rarr; 0}}, geodesics on a triaxial ellipsoid reduce to a case of [[dynamical billiards]];\n* extensions to an arbitrary number of dimensions {{harv|Knörrer|1980}};\n* geodesic flow on a surface {{harv|Berger|2010|loc=Chap. 12}}.\n\n== See also ==\n* [[Geographical distance]]\n* [[Great-circle navigation]]\n* [[Geodesics]]\n* [[Geodesy]]\n* [[Meridian arc]]\n* [[Rhumb line]]\n* [[Vincenty's formulae]]\n\n== Notes ==\n{{reflist|30em}}\n\n== References ==\n{{refbegin|30em}}\n*{{cite book\n|ref = harv |year = 1989\n|last = Arnold |first = V. I. |authorlink = Vladimir Arnold\n|title = Mathematical Methods of Classical Mechanics\n|edition = 2nd\n|publisher = Springer-Verlag\n|translator-last1 = Vogtmann |translator-first1 = K.\n|translator-last2 = Weinstein |translator-first2 = A.\n|oclc = 4037141\n|isbn = 978-0-387-96890-2\n}}\n*{{cite book\n|ref = {{harvid|Bagratuni|1962}} |year = 1967 |origyear = 1962\n|last = Bagratuni |first = G. V.\n|title = Course in Spheroidal Geodesy\n|doi = 10.5281/zenodo.32371\n|postscript = . Translation from Russian of ''Курс сфероидической геодезии'' (Moscow, 1962) by U.S. Air Force ([http://www.dtic.mil/docs/citations/AD0650520 FTD-MT-64-390])\n|oclc = 6150611\n}}\n*{{cite book\n|ref = harv |year = 2010\n|last = Berger |first = M. |authorlink = Marcel Berger\n|title = Geometry Revealed\n|publisher = Springer\n|translator-last = Senechal |translator-first = L. J.\n|isbn = 978-3-540-70996-1\n|doi = 10.1007/978-3-540-70997-8\n}}\n*{{cite journal\n|ref = {{harvid|Bessel|1825}} |year = 2010 |origyear = 1825\n|last = Bessel |first = F. W. |authorlink = Friedrich Bessel\n|title = The calculation of longitude and latitude from geodesic measurements\n|journal = Astronomische Nachrichten\n|volume = 331 |issue = 8 |pages = 852–861\n|translator-last1 = Karney |translator-first1 = C. F. F.\n|translator-last2 = Deakin |translator-first2 = R. E.\n|doi = 10.1002/asna.201011352\n|arxiv = 0908.1824\n|postscript = . English translation of [http://adsabs.harvard.edu/abs/1825AN......4..241B ''Astron. Nachr.'' '''4''', 241–254 (1825)]. [https://geographiclib.sourceforge.io/bessel-errata.html Errata].\n|bibcode = 2010AN....331..852K\n}}\n*{{cite journal\n|ref = harv |year = 1916\n|last = Bliss |first = G. A. |authorlink = Gilbert Ames Bliss\n|title = Jacobi's condition for problems of the calculus of variations in parametric form\n|journal = Transactions of the American Mathematical Society\n|volume = 17 |issue = 2 |pages = 195–206\n|doi = 10.1090/S0002-9947-1916-1501037-4\n|postscript = &nbsp;(free access).\n}}\n*{{cite book\n|ref = harv |year = 1952\n|last = Bomford |first = G.\n|title = Geodesy\n|publisher = Clarendon |location = Oxford\n|oclc = 1396190\n}}\n*{{cite book\n|ref = harv |year = 2012\n|last1 = Borre |first1 = K.\n|last2 = Strang |first2 = W. G. |authorlink2 = Gilbert Strang\n|title = Algorithms for Global Positioning\n|oclc = 795014501\n|isbn = 978-0-9802327-3-8\n|publisher = Wellesley-Cambridge Press\n|postscript = . Chapter 11, [http://old.gps.aau.dk/downloads/notes.pdf Geometry of the Ellipsoid].\n}}\n*{{cite journal\n|ref = harv |year = 1870\n|last = Cayley |first = A. |authorlink = Arthur Cayley\n|title = On the geodesic lines on an oblate spheroid\n|journal = Philosophical Magazine |series=4th series\n|volume = 40 |issue = 268 |pages = 329–340\n|doi = 10.1080/14786447008640411\n|url = https://books.google.com/books?id=Zk0wAAAAIAAJ&pg=PA329\n}}\n*{{cite journal\n|ref = harv |year = 1846\n|last = Chasles |first = M. |authorlink = Michel Chasles\n|title = Sur les lignes géodésiques et les lignes de courbure des surfaces du second degré\n|language = French\n|trans-title = Geodesic lines and the lines of curvature of the surfaces of the second degree\n|journal = Journal de Mathématiques Pures et Appliquées\n|volume = 11 |pages = 5–20\n|url = http://sites.mathdoc.fr/JMPA/PDF/JMPA_1846_1_11_A2_0.pdf\n}}\n*{{cite journal\n|ref = harv |year = 1869\n|last = Christoffel |first = E. B. |authorlink = Elwin Bruno Christoffel\n|title = Allgemeine Theorie der geodätischen Dreiecke\n|language = German\n|trans-title = General theory of geodesic triangles\n|journal = Abhandlungen Königlichen Akademie der Wissenschaft zu Berlin\n|pages = 119–176\n|url = https://books.google.com/books?id=EEtFAAAAcAAJ&pg=PA119\n}}\n*{{cite journal\n|ref = harv |year = 1735\n|last = Clairaut |first = A. C. |authorlink = Alexis Claude Clairaut\n|title = Détermination géometrique de la perpendiculaire à la méridienne tracée par M. Cassini\n|language = French\n|trans-title = Geometrical determination of the perpendicular to the meridian drawn by Jacques Cassini\n|journal = Mémoires de l'Académie Royale des Sciences de Paris 1733\n|pages = 406–416\n|url = https://books.google.com/books?id=GOAEAAAAQAAJ&pg=PA406\n}}\n*{{cite journal\n|ref = harv |year = 1989\n|last = Danielsen |first = J. S.\n|title = The Area under the Geodesic\n|journal = Survey Review\n|volume = 30 |issue = 232 |pages = 61–66\n|doi = 10.1179/003962689791474267\n}}\n*{{cite book\n|ref = harv |year = 1894\n|last = Darboux |first = J. G. |authorlink = Jean Gaston Darboux\n|title = Leçons sur la théorie générale des surfaces\n|volume = 3\n|language = French\n|trans-title = Lessons on the general theory of surfaces\n|publisher = Gauthier-Villars |location = Paris\n|url = https://books.google.com/books?id=hGMSAAAAIAAJ\n|postscript = . [https://geographiclib.sourceforge.io/geodesic-papers/darboux94.pdf PDF].\n|oclc = 8566228\n}}\n*{{cite book\n|ref = harv |year = 1813\n|last = Dupin |first = P. C. F. |authorlink = Charles Dupin\n|title = Développements de Géométrie\n|language = French\n|trans-title = Developments in geometry\n|publisher = Courcier |location = Paris\n|url = https://books.google.com/books?id=j40AAAAAMAAJ\n|oclc = 560800801\n}}\n*{{cite techreport\n|ref = harv |year = 1993\n|last = Ehlert |first = D.\n|title = Methoden der ellipsoidischen Dreiecksberechnung\n|language = German\n|trans-title = Methods for ellipsoidal triangulation\n|institution = [[:de:Deutsche Geodätische Kommission|Deutsche Geodätische Kommission]]\n|series = Reihe B: Angewandte Geodäsie, Heft Nr. 292\n|oclc = 257615376\n}}\n*{{cite journal\n|ref = harv |year = 1755\n|last = Euler |first = L. |authorlink = Leonhard Euler\n|title = Élémens de la trigonométrie sphéroïdique tirés de la méthode des plus grands et plus petits\n|language = French\n|trans-title = Elements of spheroidal trigonometry taken from the method of maxima and minima\n|journal = Mémoires de l'Académie Royale des Sciences de Berlin 1753\n|pages = 258–293\n|volume = 9\n|url = https://books.google.com/books?id=QIIfAAAAYAAJ&pg=PA258\n|postscript = . [https://books.google.com/books?id=QIIfAAAAYAAJ&pg=PA362-IA1 Figures].\n}}\n*{{cite techreport\n|ref = harv |year = 2018\n|last = FAI |authorlink = Fédération Aéronautique Internationale\n|title = FAI Sporting Code\n|institution = Fédération Aéronautique Internationale\n|location = Lausanne, Switzerland\n|url = https://www.fai.org/sites/default/files/documents/fai_sporting_code_gs_2018_final.pdf\n|postscript = . Section 8.2.3.\n}}\n*{{cite book\n|ref = harv |year = 1927\n|last = Forsyth |first = A. R. |authorlink = Andrew Forsyth\n|title = Calculus of Variations\n|publisher = Cambridge Univ. Press\n|isbn = 978-1-107-64083-2\n|oclc = 250050479\n}}\n*{{cite book\n|ref = {{harvid|Gan'shin|1967}} |year = 1969 |origyear = 1967\n|last = Gan'shin |first = V. V.\n|translator-last = Willis |translator-first = J. M.\n|title = Geometry of the Earth Ellipsoid\n|publisher = Aeronautical Chart and Information Center |location = St. Louis\n|doi = 10.5281/zenodo.32854\n|postscript = . Translation from Russian of ''Геометрия земного эллипсоида'' (Moscow, 1967).\n|oclc = 493553\n}}\n*{{cite book\n|ref = {{harvid|Gauss|1828}} |year = 1902 |origyear = 1828\n|last = Gauss |first = C. F. |authorlink = Carl Friedrich Gauss\n|title = General Investigations of Curved Surfaces of 1827 and 1825\n|publisher = Princeton Univ. Lib\n|translator-last1 = Morehead |translator-first1 = J. C.\n|translator-last2 = Hiltebeitel |translator-first2 = A. M.\n|url = https://books.google.com/books?id=a1wTJR3kHwUC\n|postscript = . [https://geographiclib.sourceforge.io/geodesic-papers/gauss28-en.pdf PDF]. English translation of [https://books.google.com/books?id=bX0AAAAAMAAJ&pg=PA3 ''Disquisitiones generales circa superficies curvas''] (Dieterich, Göttingen, 1828).\n|oclc = 7824448\n}}\n*{{cite journal\n|ref = harv |year = 1849\n|last = Hart |first = A. S. |authorlink = Andrew Searle Hart\n|title = Geometrical demonstration of some properties of geodesic lines\n|journal = Cambridge and Dublin Mathematical Journal\n|volume = 4 |pages = 80–84\n|url = https://books.google.com/books?id=ClAeAQAAMAAJ&pg=PA80\n}}\n*{{cite book\n|ref = {{harvid|Helmert|1880}} |year = 1964 |origyear = 1880\n|last = Helmert |first = F. R. |authorlink = Friedrich Robert Helmert\n|title = Mathematical and Physical Theories of Higher Geodesy\n|volume = 1\n|publisher = Aeronautical Chart and Information Center |location = St. Louis\n|doi = 10.5281/zenodo.32050\n|postscript = . English translation of [https://books.google.com/books?id=qt2CAAAAIAAJ ''Die Mathematischen und Physikalischen Theorieen der Höheren Geodäsie''], Vol. 1 (Teubner, Leipzig, 1880).\n|oclc = 17273288\n}}\n*{{cite book\n|ref = harv |year = 1952\n|last1 = Hilbert |first1 = D. |authorlink1 = David Hilbert\n|last2 = Cohn-Vossen |first2 = S. |authorlink2 = Stephan Cohn-Vossen\n|title = Geometry and the Imagination\n|publisher = Chelsea |location = New York\n|translator-last = Nemenyi |translator-first = P.\n|oclc = 301610346\n}}\n*{{cite book\n|ref = harv |year = 1811\n|last = Hutton |first = C. |authorlink = Charles Hutton\n|title = A Course of Mathematics in Three Volumes Composed for the Use of the Royal Military Academy\n|location = London\n|page = 115\n|url = https://books.google.com/books?id=BtM2AAAAMAAJ&pg=PA115\n|oclc = 18031510\n}}\n*{{cite journal\n|ref = harv |year = 1837\n|last = Jacobi |first = C. G. J. |authorlink = Carl Gustav Jacob Jacobi\n|title = Zur Theorie der Variations-Rechnung und der Differential-Gleichungen\n|language = German\n|trans-title = The theory of the calculus of variations and of differential equations\n|journal = Journal für die Reine und Angewandte Mathematik\n|volume = 1837 |issue = 17 |pages = 68–82\n|doi = 10.1515/crll.1837.17.68\n|url = https://books.google.com/books?id=FKvxAAAAMAAJ&pg=PA68\n}}\n*{{cite journal\n|ref = harv |year = 1839\n|last = Jacobi |first = C. G. J. |authorlink = Carl Gustav Jacob Jacobi\n|title = Note von der geodätischen Linie auf einem Ellipsoid und den verschiedenen Anwendungen einer merkwürdigen analytischen Substitution\n|language = German\n|trans-title = The geodesic on an ellipsoid and various applications of a remarkable analytical substitution\n|journal = Journal für die Reine und Angewandte Mathematik\n|volume = 1839 |issue = 19 |pages = 309–313\n|doi = 10.1515/crll.1839.19.309\n|url = https://books.google.com/books?id=RbwGAAAAYAAJ&pg=PA309\n|postscript = . [https://books.google.com/books?id=_09tAAAAMAAJ&pg=PA385 Letter to Bessel], Dec. 28, 1838. [http://sites.mathdoc.fr/JMPA/PDF/JMPA_1841_1_6_A20_0.pdf French translation] (1841).\n}}\n*{{cite book\n|ref = {{harvid|Jacobi|1866}} |year = 2009 |origyear = 1866\n|last = Jacobi |first = C. G. J. |authorlink = Carl Gustav Jacob Jacobi\n|title = Lectures on Dynamics\n|publisher = Hindustan Book Agency |location = New Delhi\n|editor = A. Clebsch |editorlink = Alfred Clebsch\n|translator-last = Balagangadharan |translator-first = K.\n|postscript = . English translation of [https://books.google.com/books?id=ryEOAAAAQAAJ ''Vorlesungen über Dynamik''] (Reimer, Berlin, 1866). [https://geographiclib.sourceforge.io/jacobi-errata.html Errata].\n|isbn = 978-81-85931-91-3\n|mr = 2569315\n|oclc = 440645889\n}}\n*{{cite encyclopedia\n|ref = harv |year = 1891\n|last = Jacobi |first = C. G. J. |authorlink = Carl Gustav Jacob Jacobi\n|work = Jacobi's Gesammelte Werke\n|title = Über die Curve, welche alle von einem Punkte ausgehenden geodätischen Linien eines Rotationsellipsoides berührt\n|publisher = Reimer |location = Berlin\n|language = German\n|trans-title = The envelope of geodesic lines emanating from a single point on an ellipsoid\n|pages = 72–87\n|editor = K. T. W. Weierstrass|editorlink = Karl Weierstrass\n|volume = 7\n|url = https://books.google.com/books?id=_09tAAAAMAAJ&pg=PA72\n|postscript = . Op. post., completed by [[Friedrich Heinrich Albert Wangerin|F. H. A. Wangerin]].  [https://geographiclib.sourceforge.io/geodesic-papers/jacobi-V7.pdf PDF].\n|oclc = 630416023\n}}\n*{{citation\n|ref = harv |year = 2012\n|last = Jekeli |first = C.\n|title = Geometric Reference Systems in Geodesy\n|publisher = Ohio State Univ.\n|hdl = 1811/51274\n}}\n*{{cite book\n|ref = {{harvid|Jordan|Eggert|1941}} |year = 1962 |origyear = 1941\n|last1 = Jordan |first1 = W. |authorlink1 = Wilhelm Jordan (geodesist)\n|last2 = Eggert |first2 = O. |authorlink2 = Otto Eggert\n|title = Handbook of Geodesy\n|volume = 3.2\n|publisher = Army Map Service |location = Washington, DC\n|translator-last = Carta |translator-first = M. W.\n|postscript = . English translation of ''Handbuch der Vermessungskunde'', 8th edition (Metzler, Stuttgart, 1941).\n|doi = 10.5281/zenodo.35316\n|oclc = 34429043\n|bibcode = 1962hage.book.....J }}\n*{{cite journal\n|ref = harv |year = 2013\n|last = Karney |first = C. F. F.\n|title = Algorithms for geodesics\n|journal = Journal of Geodesy\n|volume = 87 |issue = 1 |pages = 43–55\n|doi = 10.1007/s00190-012-0578-z\n|postscript = &nbsp;(open access). [https://geographiclib.sourceforge.io/geod-addenda.html Addenda].\n|arxiv = 1109.4448\n|bibcode = 2013JGeod..87...43K\n}}\n*{{cite web\n|ref = harv |year = 2015\n|title = GeographicLib\n|last = Karney |first = C. F. F.\n|url = https://geographiclib.sourceforge.io/1.44/\n|version = Version 1.44\n}}\n*{{cite book\n|ref = harv |year = 1982\n|last = Klingenberg |first = W. P. A. |authorlink = Wilhelm Klingenberg\n|title = Riemannian Geometry\n|publisher = de Gruyer\n|oclc = 8476832\n|isbn = 978-3-11-008673-7\n|mr = 666697\n}}\n*{{cite journal\n|ref = harv |year = 1980\n|last = Knörrer |first = H. |authorlink = Horst Knörrer\n|title = Geodesics on the ellipsoid\n|journal = Inventiones Mathematicae\n|volume = 59 |issue = 2 |pages = 119–143\n|doi = 10.1007/BF01390041\n|bibcode = 1980InMat..59..119K\n}}\n*{{citation\n|ref = harv |year = 1974\n|last1 = Krakiwsky |first1 = E. J.\n|last2 = Thomson |first2 = D. B.\n|title = Geodetic position computations\n|location = Fredericton, N.B.\n|publisher = Univ. of New Brunswick\n|url = http://www2.unb.ca/gge/Pubs/LN39.pdf\n|series = Dept. of Geodesy and Geomatics Engineering, Lecture Notes\n|number = 39\n|bibcode = 1974gpc..book.....K\n}}\n*{{cite book\n|ref = {{harvid|Laplace|1799a}} |year = 1829 |origyear = 1799a\n|last = Laplace |first = P. S. |authorlink = Pierre-Simon Laplace\n|title = Treatise on Celestial Mechanics\n|translator-last = Bowditch |translator-first = N.\n|translator-link = Nathaniel Bowditch\n|publisher = Hillard, Gray, Little, & Wilkins |location = Boston\n|volume = 1\n|url = https://books.google.com/books?id=k-cRAAAAYAAJ&pg=PA34\n|postscript = . Book 1, §8.\n|oclc = 1294937\n}}\n*{{cite book\n|ref = harv |year = 1799b\n|last = Laplace |first = P. S. |authorlink = Pierre-Simon Laplace\n|title = Traité de Mécanique Céleste\n|publisher = Crapelet |location = Paris\n|language = French\n|trans-title = Treatise on Celestial Mechanics\n|volume = 2 |page = 112\n|url = https://books.google.com/books?id=5FKKk5EGLJ8C&pg=PA112\n|oclc = 25448952\n}}\n*{{cite journal\n|ref = harv |year = 1806\n|last = Legendre |first = A. M. |authorlink = Adrien-Marie Legendre\n|title = Analyse des triangles tracées sur la surface d'un sphéroïde\n|language = French\n|trans-title = Analysis of spheroidal triangles\n|journal = Mémoires de l'Institut National de France\n|number = 1st semester\n|pages = 130–161\n|url = https://books.google.com/books?id=EnVFAAAAcAAJ&pg=PA130\n}}\n*{{cite book\n|ref = harv |year = 1811\n|last = Legendre |first = A. M. |authorlink = Adrien-Marie Legendre\n|title = Exercices de Calcul Intégral sur Divers Ordres de Transcendantes et sur les Quadratures\n|language = French\n|trans-title = Exercises in Integral Calculus\n|publisher = Courcier |location = Paris\n|url = https://books.google.com/books?id=riIOAAAAQAAJ\n|oclc = 312469983\n}}\n*{{cite book\n|ref = {{harvid|Leick et al.|2015}} |year = 2015\n|last1 = Leick |first1 = A.\n|last2 = Rapoport |first2 = L.\n|last3 = Tatarnikov |first3 = D.\n|title = GPS Satellite Surveying\n|url = https://books.google.com/books?id=FQlhBgAAQBAJ\n|publisher = Wiley\n|edition = 4th\n|isbn = 978-1-119-01828-5\n}}\n*{{cite journal\n|ref = harv |year = 1846\n|last = Liouville |first = J. |authorlink = Joseph Liouville\n|title = Sur quelques cas particuliers où les équations du mouvement d'un point matériel peuvent s'intégrer\n|language = French\n|trans-title = Special cases where the equations of motion are integrable\n|journal = Journal de Mathématiques Pures et Appliquées\n|volume = 11 |pages = 345–378\n|url = http://sites.mathdoc.fr/JMPA/PDF/JMPA_1846_1_11_A45_0.pdf\n}}\n*{{cite book\n|ref = harv |year = 1964\n|last = Lyusternik |first = L. |authorlink = Lazar Lyusternik\n|title = Shortest Paths: Variational Problems\n|publisher = Macmillan |location = New York\n|translator-last1 = Collins |translator-first1 = P.\n|translator-last2 = Brown |translator-first2 = R. B.\n|postscript = . Translation from Russian of ''Кратчайшие Линии: Вариационные Задачи'' (Moscow, 1955).\n|oclc = 1048605\n|series = Popular Lectures in Mathematics\n|volume = 13\n|mr = 0178386\n}}\n*{{cite encyclopedia\n|ref = {{harvid|Monge|1796}} |year = 1850 |origyear = 1796\n|last = Monge |first = G. |authorlink = Gaspard Monge\n|title = Sur les lignes de courbure de la surface de l'ellipsoïde\n|language = French\n|trans-title = On the lines of curvature on the surface of the ellipsoid\n|work = Application de l'Analyse à la Géometrie\n|pages = 139–160\n|url = https://books.google.com/books?id=Nf5zhlffjd0C&pg=PA139\n|editor = J. Liouville|editorlink = Joseph Liouville\n|edition = 5th\n|publisher = Bachelier |location = Paris\n|postscript = . [https://geographiclib.sourceforge.io/geodesic-papers/monge50-fig.pdf Figures].\n|oclc = 2829112\n}}\n*{{cite web\n|ref = {{harvid|NGS|2012}} |year = 2012\n|title = Geodesic Utilities: Inverse and Forward\n|version = Version 3.0\n|author = National Geodetic Survey |authorlink = National Geodetic Survey\n|url = http://www.ngs.noaa.gov/PC_PROD/Inv_Fwd/\n}}\n*{{cite book\n|ref = {{harvid|Newton|1687}} |year = 1848 |origyear = 1687\n|last = Newton |first = I. |authorlink = Isaac Newton\n|title = The Mathematical Principles of Natural Philosophy\n|publisher = Adee |location = New York\n|translator-last = Motte |translator-first = A.\n|postscript = . Book 3, Proposition 19, Problem 3, pp. 405–409.\n|url = https://books.google.com/books?id=KaAIAAAAIAAJ&pg=PA405\n}}\n*{{cite journal\n|ref = harv |year = 1806\n|last = Oriani |first = B. |authorlink = Barnaba Oriani\n|title = Elementi di trigonometria sferoidica, Pt. 1\n|language = Italian\n|trans-title = Elements of spheroidal trigonometry\n|journal = Memorie Dell'Istituto Nazionale Italiano\n|volume = 1 |number = 1 |pages = 118–198\n|url = https://books.google.com/books?id=SydFAAAAcAAJ&pg=PA118\n}}\n*{{cite journal\n|ref = harv |year = 1808\n|last = Oriani |first = B. |authorlink = Barnaba Oriani\n|title = Elementi di trigonometria sferoidica, Pt. 2\n|language = Italian\n|trans-title = Elements of spheroidal trigonometry\n|journal = Memorie Dell'Istituto Nazionale Italiano\n|volume = 2 |number = 1 |pages = 1–58\n|url = https://books.google.com/books?id=XSdFAAAAcAAJ&pg=PA1\n}}\n*{{cite journal\n|ref = harv |year = 1810\n|last = Oriani |first = B. |authorlink = Barnaba Oriani\n|title = Elementi di trigonometria sferoidica, Pt. 3\n|language = Italian\n|trans-title = Elements of spheroidal trigonometry\n|journal = Memorie Dell'Istituto Nazionale Italiano\n|volume = 2 |number = 2 |pages = 1–58\n|url = https://books.google.com/books?id=qaosAQAAMAAJ&pg=PA1\n}}\n*{{cite journal\n|ref = harv |year = 1905\n|last = Poincaré |first = H. |authorlink = Henri Poincaré\n|title = Sur les lignes géodésiques des surfaces convexes\n|language = French\n|trans-title = Geodesics lines on convex surfaces\n|journal = Transactions of the American Mathematical Society\n|volume = 6 |issue = 3 |pages = 237–274\n|doi = 10.2307/1986219\n|jstor = 1986219\n|url = https://zenodo.org/record/2044489\n}}\n*{{cite journal\n|ref = harv |year = 1955\n|last = Rainsford |first = H. F.\n|title = Long geodesics on the ellipsoid\n|journal = Bulletin Géodésique\n|volume = 37 |issue = 1\n|pages = 12–22\n|doi = 10.1007/BF02527187\n|bibcode = 1955BGeod..29...12R\n}}\n*{{citation\n|ref = harv |year = 1991\n|last = Rapp |first = R. H.\n|title = Geometric geodesy, part I\n|publisher = Ohio State Univ.\n|hdl = 1811/24333\n}}\n*{{citation\n|ref = harv |year = 1993\n|last = Rapp |first = R. H.\n|title = Geometric geodesy, part II\n|publisher = Ohio State Univ.\n|hdl = 1811/24409\n}}\n*{{cite techreport\n|ref = harv |year = 2007\n|last = RNAV |authorlink = RNAV\n|title = Order 8260.54A, The United States Standard for Area Navigation\n|institution = U.S. Federal Aviation Administration\n|location = Washington, D.C.\n|url = https://www.faa.gov/documentLibrary/media/Order/8260.54A.pdf\n|postscript = . Appendix 2.\n}}\n*{{cite journal\n|ref = harv |year = 2006\n|last = Sjöberg |first = L. E.\n|title = Determination of areas on the plane, sphere and ellipsoid\n|journal = Survey Review\n|volume = 38 |issue = 301 |pages = 583–593\n|doi = 10.1179/003962606780732100\n}}\n*{{cite techreport\n|ref = harv |year = 2006\n|last = UNCLOS |authorlink = UNCLOS\n|title = A Manual on Technical Aspects of the United Nations Convention on the Law of the Sea, 1982\n|institution = International Hydrographic Bureau\n|edition = 4th\n|location = Monaco\n|url = http://www.iho.int/iho_pubs/CB/C-51_Ed4-EN.pdf\n}}\n*{{cite journal\n|ref = harv |year = 1975\n|last = Vincenty |first = T. |authorlink = Thaddeus Vincenty\n|title = Direct and inverse solutions of geodesics on the ellipsoid with application of nested equations\n|journal = Survey Review\n|volume = 23 |issue = 176 |pages = 88–93\n|doi = 10.1179/sre.1975.23.176.88\n|url = https://www.ngs.noaa.gov/PUBS_LIB/inverse.pdf\n|postscript = . Addendum: Survey Review '''23''' (180): 294 (1976).\n}}\n*{{cite techreport\n|ref = harv |year = 1978\n|last1 = Vincenty |first1 = T. |authorlink1 = Thaddeus Vincenty\n|last2 = Bowring |first2 = B. R.\n|title = Application of three-dimensional geodesy to adjustments of horizontal networks\n|institution = NOAA\n|number = NOS NGS-13\n|url = https://www.ngs.noaa.gov/PUBS_LIB/ApplicationOfThreeDimensionalGeodesyToAdjustmentsOfHorizontalNetworks_TM_NOS_NGS13.pdf\n}}\n*{{cite journal\n|ref = harv |year = 1861\n|last = Weierstrass |first = K. T. W. |authorlink = Karl Weierstrass\n|title = Über die geodätischen Linien auf dem dreiaxigen Ellipsoid\n|language = German\n|trans-title = Geodesic lines on a triaxial ellipsoid\n|journal = Monatsbericht Königlichen Akademie der Wissenschaft zu Berlin\n|pages = 986–997\n|url = https://books.google.com/books?id=9O4GAAAAYAAJ&pg=PA257\n|postscript = . [https://geographiclib.sourceforge.io/geodesic-papers/weierstrass-V1.pdf PDF].\n}}\n{{refend}}\n\n== External links ==\n* [https://geographiclib.sourceforge.io/geodesic-papers/biblio.html Online geodesic bibliography] of books and articles on geodesics on ellipsoids.\n* [https://dx.doi.org/10.5281/zenodo.32156 Test set for geodesics], a set of 500000 geodesics for the WGS84 ellipsoid, computed using high-precision arithmetic.\n* [https://www.ngs.noaa.gov/TOOLS/Inv_Fwd/Inv_Fwd.html NGS tool] implementing {{harvtxt|Vincenty|1975}}.\n* [https://proj4.org/apps/geod.html geod(1)], man page for the [[PROJ.4]] utility for geodesic calculations.\n* [https://geographiclib.sourceforge.io/ GeographicLib implementation] of {{harvtxt|Karney|2013}}.\n* [https://geographiclib.sourceforge.io/scripts/geod-google.html Drawing geodesics on Google Maps.]\n\n[[Category:Geodesy]]\n[[Category:Geodesic (mathematics)]]\n[[Category:Differential geometry]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Geometric analysis",
      "url": "https://en.wikipedia.org/wiki/Geometric_analysis",
      "text": "[[File:Saddle Tower Minimal Surfaces.png|thumb|[[Saddle tower]] minimal surface. [[Minimal surface]]s are among the objects of study in geometric analysis.]]\n\n'''Geometric analysis''' is a [[mathematics|mathematical]] discipline at the interface of [[differential geometry]] and [[differential equations]]. It was founded in the early 1980's by [[Karen Uhlenbeck]] in her paper on [[minimal surface]]s.<ref>Jackson, Allyn. (2019). [https://www.sciencemag.org/news/2019/03/founder-geometric-analysis-honored-abel-prize Founder of geometric analysis honored with Abel Prize] Retrieved 20 March, 2019.</ref>\n\n==Scope==\nThe scope of geometric analysis includes both the use of geometrical methods in the study of [[partial differential equation]]s (when it is also known as \"geometric PDE\"), and the application of the theory of partial differential equations to geometry. It incorporates problems involving curves and surfaces, or domains with curved boundaries, but also the study of [[Riemannian manifold]]s in arbitrary dimension.  The [[calculus of variations]] is sometimes regarded as part of geometric analysis, because differential equations arising from [[variational principle]]s have a strong geometric content. Geometric analysis also includes [[global analysis]], which concerns the study of differential equations on [[manifold]]s, and the relationship between differential equations and [[topology]].\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite book|title=Riemannian geometry and Geometric Analysis |first=Jürgen|last=Jost|authorlink=Jürgen Jost|edition=4th|year=2005|publisher=Springer|isbn= 978-3-540-25907-7}}\n*{{cite book|title=Groups and Geometric Analysis (Integral Geometry, Invariant Differential Operators and Spherical Functions) |first=Sigurdur|last=Helgason|authorlink=Sigurdur Helgason (mathematician)|edition=2nd|year=2000|publisher=[[American Mathematical Society]]|isbn= 978-0-8218-2673-7}}\n*{{cite book|title=Geometric Analysis on Symmetric Spaces |first=Sigurdur|last=Helgason |edition=2nd|year=2008|publisher=American Mathematical Society|isbn= 978-0-8218-4530-1}}\n\n\n[[Category:Differential geometry| ]]\n[[Category:Calculus of variations]]\n[[Category:Differential equations]]"
    },
    {
      "title": "Hamilton's principle",
      "url": "https://en.wikipedia.org/wiki/Hamilton%27s_principle",
      "text": "{{classical mechanics}}\nIn [[physics]], '''Hamilton's principle''' is [[William Rowan Hamilton]]'s formulation of the [[principle of stationary action]] (see that article for historical formulations). It states that the [[dynamics (mechanics)|dynamics]] of a physical system is determined by a [[calculus of variations|variational problem]] for a [[functional (mathematics)|functional]] based on a single function, the [[Lagrangian mechanics|Lagrangian]], which contains all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the ''[[differential equation|differential]]'' [[equations of motion]] of the physical system.  Although formulated originally for [[classical mechanics]], Hamilton's principle also applies to classical [[field (physics)|fields]] such as the [[electromagnetism|electromagnetic]] and [[gravity|gravitational]] [[field (physics)|fields]], and plays an important role in [[quantum mechanics]], [[quantum field theory]] and criticality theories.\n\n[[File:Least action principle.svg|250px|thumb|As the system evolves, '''q''' traces a path through [[configuration space (physics)|configuration space]] (only some are shown). The path taken by the system (red) has a stationary action (δ''S'' = 0) under small changes in the configuration of the system (δ'''q''').<ref name=penrose>{{cite book |author=R. Penrose| title=[[The Road to Reality]]| publisher= Vintage books| year=2007 | page = 474|isbn=0-679-77631-1}}</ref>]]\n\n==Mathematical formulation==\n\nHamilton's principle states that the true evolution '''q'''(''t'') of a system described by ''N'' [[generalized coordinates]] '''q''' = (''q''<sub>1</sub>, ''q''<sub>2</sub>, ..., ''q''<sub>''N''</sub>) between two specified states '''q'''<sub>1</sub> = '''q'''(''t''<sub>1</sub>) and '''q'''<sub>2</sub> = '''q'''(''t''<sub>2</sub>) at two specified times ''t''<sub>1</sub> and ''t''<sub>2</sub> is a [[stationary point]] (a point where the [[Calculus of variations|variation]] is zero), of the [[action (physics)|action]] [[functional (mathematics)|functional]]\n\n:<math>\n\\mathcal{S}[\\mathbf{q}] \\ \\stackrel{\\mathrm{def}}{=}\\  \n\\int_{t_1}^{t_2} L(\\mathbf{q}(t),\\dot{\\mathbf{q}}(t),t)\\, dt \n</math>\n\nwhere <math>L(\\mathbf{q},\\dot{\\mathbf{q}},t)</math> is the [[Lagrangian mechanics|Lagrangian]] [[function (mathematics)|function]] for the system.  In other words, any ''first-order'' perturbation of the true evolution results in (at most) ''second-order'' changes in <math>\\mathcal{S}</math>.  The action <math>\\mathcal{S}</math> is a [[functional (mathematics)|functional]], i.e., something that takes as its input a [[function (mathematics)|function]] and returns a single number, a [[scalar (physics)|scalar]].  In terms of [[functional analysis]], Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation\n\n{{Equation box 1\n|indent =:\n|title='''Hamilton's principle'''\n|equation = <math>\n\\frac{\\delta \\mathcal{S}}{\\delta \\mathbf{q}(t)}=0\n</math>\n|border=2\n|border colour = #50C878\n|background colour = #ECFCF4}}\n\n===Euler–Lagrange equations derived from the action integral===\n\nRequiring that the true trajectory '''q'''(''t'') be a [[stationary point]] of the action functional <math>\\mathcal{S}</math> is equivalent to a set of differential equations for '''q'''(''t'') (the '''Euler–Lagrange equations'''), which may be derived as follows.\n\nLet '''q'''(''t'') represent the true evolution of the system between two specified states '''q'''<sub>1</sub> = '''q'''(''t''<sub>1</sub>) and '''q'''<sub>2</sub> = '''q'''(''t''<sub>2</sub>) at two specified times ''t''<sub>1</sub> and ''t''<sub>2</sub>, and let '''ε'''(''t'') be a small perturbation that is zero at the endpoints of the trajectory\n\n:<math>\n\\boldsymbol\\varepsilon(t_1) = \\boldsymbol\\varepsilon(t_2) \\ \\stackrel{\\mathrm{def}}{=}\\  0\n</math>\n\nTo first order in the perturbation '''ε'''(''t''), the change in the action functional <math>\\delta\\mathcal{S}</math> would be\n:<math>\n\\delta \\mathcal{S} = \n\\int_{t_1}^{t_2}\\;\n\\left[ L(\\mathbf{q}+\\boldsymbol\\varepsilon,\\dot{\\mathbf{q}} +\\dot{\\boldsymbol{\\varepsilon}})- L(\\mathbf{q},\\dot{\\mathbf{q}}) \\right]dt = \\int_{t_1}^{t_2}\\; \\left(\n\\boldsymbol\\varepsilon \\cdot \\frac{\\partial L}{\\partial \\mathbf{q}} + \n\\dot{\\boldsymbol{\\varepsilon}} \\cdot \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}}  \\right)\\,dt      \n</math>\n\nwhere we have expanded the [[Lagrangian mechanics|Lagrangian]] ''L'' to first order in the perturbation '''ε'''(''t'').\n\nApplying [[integration by parts]] to the last term results in\n\n:<math>\n\\delta \\mathcal{S} = \n\\left[ \\boldsymbol\\varepsilon \\cdot \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}}\\right]_{t_1}^{t_2} + \n\\int_{t_1}^{t_2}\\; \n\\left( \\boldsymbol\\varepsilon \\cdot \\frac{\\partial L}{\\partial \\mathbf{q}}\n- \\boldsymbol\\varepsilon \\cdot \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\right)\\,dt\n</math>\n\nThe boundary conditions <math>\n\\boldsymbol\\varepsilon(t_1) = \\boldsymbol\\varepsilon(t_2) \\ \\stackrel{\\mathrm{def}}{=}\\  0\n</math> causes the first term to vanish\n\n:<math>\\delta \\mathcal{S} = \\int_{t_1}^{t_2}\\; \\boldsymbol\\varepsilon \\cdot\\left(\\frac{\\partial L}{\\partial \\mathbf{q}} - \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\right)\\,dt</math>\n\nHamilton's principle requires that this first-order change <math>\\delta \\mathcal{S}</math> is zero for all possible perturbations '''ε'''(''t''), i.e., the true path is a [[stationary point]] of the action functional <math>\\mathcal{S}</math> (either a minimum, maximum or saddle point).  This requirement can be satisfied if and only if\n\n{{Equation box 1\n|indent =:\n|title='''[[Euler–Lagrange equations]]'''\n|equation = <math> \\frac{\\partial L}{\\partial \\mathbf{q}} - \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} = 0</math>\n|border=2\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nThese equations are called the Euler–Lagrange equations for the variational problem.\n\n===Canonical momenta and constants of motion===\n\nThe '''conjugate momentum''' ''p<sub>k</sub>'' for a generalized coordinate ''q<sub>k</sub>'' is defined by the equation\n\n:<math>p_k \\ \\stackrel{\\mathrm{def}}{=}\\ \\frac{\\partial L}{\\partial \\dot{q}_k}</math>.\n\nAn important special case of the Euler–Lagrange equation occurs when ''L'' does not contain a generalized coordinate ''q<sub>k</sub>'' explicitly,\n\n:<math>\\frac{\\partial L}{\\partial q_k}=0 \\quad \\Rightarrow \\quad \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{q}_k} = 0 \\quad \\Rightarrow \\quad \\frac{d p_k}{dt} = 0 \\,,</math>\n\nthat is, the conjugate momentum is a ''constant of the motion''.\n\nIn such cases, the coordinate ''q<sub>k</sub>'' is called a '''cyclic coordinate'''.  For example, if we use polar coordinates ''t, r, θ'' to describe the planar motion of a particle, and if ''L'' does not depend on ''θ'', the conjugate momentum is the conserved angular momentum.\n\n===Example: Free particle in polar coordinates===\n\nTrivial examples help to appreciate the use of the action principle via the Euler–Lagrange equations. A free particle (mass ''m'' and velocity ''v'') in Euclidean space moves in a straight line. Using the Euler–Lagrange equations, this can be shown in [[coordinates (elementary mathematics)|polar coordinates]] as follows. In the absence of a potential, the Lagrangian is simply equal to the kinetic energy \n:<math> L = \\frac{1}{2} mv^2= \\frac{1}{2}m \\left( \\dot{x}^2 + \\dot{y}^2 \\right)</math>\nin orthonormal (''x'',''y'') coordinates, where the dot represents differentiation with respect to the curve parameter (usually the time, ''t''). Therefore, upon application of the Euler–Lagrange equations,\n\n:<math> \\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{x}} \\right) - \\frac{\\partial L}{\\partial x} = 0 \\qquad \\Rightarrow  \\qquad m\\ddot{x} = 0 </math>\n\nAnd likewise for ''y''. Thus the Euler–Lagrange formulation can be used to derive Newton's laws.\n\nIn polar coordinates (''r'', φ) the kinetic energy and hence the Lagrangian becomes\n\n:<math> L = \\frac{1}{2}m \\left( \\dot{r}^2 + r^2\\dot{\\varphi}^2 \\right). </math>\n\nThe radial ''r'' and ''φ'' components of the Euler–Lagrange equations become, respectively\n\n:<math>\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{r}} \\right) - \\frac{\\partial L}{\\partial r} = 0  \\qquad\n\\Rightarrow  \\qquad \\ddot{r} -  r\\dot{\\varphi}^2 = 0 </math>\n\n:<math>\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{\\varphi}}  \\right)-\\frac{\\partial L}{\\partial \\varphi} = 0  \\qquad\n\\Rightarrow  \\qquad \\ddot{\\varphi} + \\frac{2}{r}\\dot{r}\\dot{\\varphi} = 0.</math>\n\nThe solution of these two equations is given by\n\n:<math> r = \\sqrt{(a t + b)^2 + c^2} </math>\n:<math> \\varphi = \\tan^{-1} \\left( \\frac{a t + b}{c} \\right) + d </math>\n\nfor a set of constants ''a, b, c, d'' determined by initial conditions.\nThus, indeed, ''the solution is a straight line'' given in polar coordinates: ''a'' is the velocity, ''c'' is the distance of the closest approach to the origin, and ''d'' is the angle of motion.\n\n==Applied to deformable bodies==\nHamilton's principle is an important variational principle in [[Linear elasticity|elastodynamics]]. As opposed to a system composed of rigid bodies, deformable bodies have an infinite number of degrees of freedom and occupy continuous regions of space; consequently, the state of the system is described by using continuous functions of space and time. The extended Hamilton Principle for such bodies is given by\n\n: <math> \\int_{t_1}^{t_2} \\left[ \\delta W_e + \\delta T - \\delta U \\right]dt = 0 </math>\n\nwhere ''T'' is the kinetic energy, ''U'' is the elastic energy, ''W<sub>e</sub>'' is the work done by\nexternal loads on the body, and ''t''<sub>1</sub>, ''t''<sub>2</sub> the initial and final times. If the system is conservative, the work done by external forces may be derived from a scalar potential ''V''. In this case,\n\n: <math> \\delta \\int_{t_1}^{t_2} \\left[ T - (U + V) \\right]dt = 0.</math>\n\nThis is called Hamilton's principle and it is invariant under coordinate transformations.\n\n==Comparison with Maupertuis' principle==\n\nHamilton's principle and [[Maupertuis' principle]] are occasionally confused and both have been called (incorrectly) the [[principle of least action]].  They differ in three important ways: \n* ''their definition of the [[action (physics)|action]]...'' \n:Maupertuis' principle uses an integral over the [[generalized coordinates]] known as the '''abbreviated action''' or [[reduced action]]\n::<math>\\mathcal{S}_{0} \\ \\stackrel{\\mathrm{def}}{=}\\  \\int \\mathbf{p} \\cdot d\\mathbf{q}</math> \n:where '''p''' = (''p''<sub>1</sub>, ''p''<sub>2</sub>, ..., ''p<sub>N</sub>'') are the conjugate momenta defined above.  By contrast, Hamilton's principle uses <math>\\mathcal{S}</math>, the integral of the [[Lagrangian mechanics|Lagrangian]] over [[time]].\n\n*''the solution that they determine...''\n:Hamilton's principle determines the trajectory '''q'''(''t'') as a function of time, whereas Maupertuis' principle determines only the shape of the trajectory in the generalized coordinates.  For example, Maupertuis' principle determines the shape of the ellipse on which a particle moves under the influence of an inverse-square central force such as [[gravitation|gravity]], but does not describe ''per se'' how the particle moves along that trajectory.  (However, this time parameterization may be determined from the trajectory itself in subsequent calculations using the [[conservation of energy]]).  By contrast, Hamilton's principle directly specifies the motion along the ellipse as a function of time.\n\n*''...and the constraints on the variation.''\n:Maupertuis' principle requires that the two endpoint states ''q''<sub>1</sub> and ''q''<sub>2</sub> be given and that energy be conserved along every trajectory (same energy for each trajectory). This forces the endpoint times to be varied as well. By contrast, Hamilton's principle does not require the conservation of energy, but does require that the endpoint times ''t''<sub>1</sub> and ''t''<sub>2</sub> be specified as well as the endpoint states ''q''<sub>1</sub> and ''q''<sub>2</sub>.\n\n==Action principle for fields==\n\n===Classical field theory===\n\n{{main|Classical field theory}}\n\nThe '''action principle''' can be extended to obtain the [[equations of motion]] for [[field (physics)|fields]], such as the [[electromagnetic field]] or [[Einstein–Hilbert action|gravity]].\n\nThe [[Einstein equation]] utilizes the ''[[Einstein–Hilbert action]]'' as constrained by a [[variational principle]].\n\nThe path of a body in a gravitational field (i.e. free fall in space time, a so-called geodesic) can be found using the action principle.\n\n===Quantum mechanics and quantum field theory===\n\n{{main|Quantum field theory}}\n\nIn [[quantum mechanics]], the system does not follow a single path whose action is stationary, but the behavior of the system depends on all imaginable paths and the value of their action. The action corresponding to the various paths is used to calculate the [[Path integral formulation|path integral]], that gives the [[probability amplitude]]s of the various outcomes.\n\nAlthough equivalent in classical mechanics with [[Newton's laws]], the '''action principle''' is better suited for generalizations and plays an important role in modern physics.  Indeed, this principle is one of the great generalizations in physical science. In particular, it is fully appreciated and best understood within [[quantum mechanics]]. [[Richard Feynman]]'s [[path integral formulation]] of quantum mechanics is based on a stationary-action principle, using path integrals. [[Maxwell's equations]] can be derived as conditions of stationary action.\n\n==See also==\n\n*[[Analytical mechanics]]\n*[[Configuration space (physics)|Configuration space]]\n*[[Hamilton–Jacobi equation]]\n*[[Phase space]]\n*[[Geodesics as Hamiltonian flows]]\n\n==References==\n{{Reflist}}\n\n* W.R. Hamilton, \"On a General Method in Dynamics.\", ''Philosophical Transactions of the Royal Society'' [http://www.emis.de/classics/Hamilton/GenMeth.pdf Part II (1834) pp. 247–308]; [http://www.emis.de/classics/Hamilton/SecEssay.pdf Part I (1835) pp. 95–144]. (''From the collection [http://www.emis.de/classics/Hamilton/ Sir William Rowan Hamilton (1805–1865): Mathematical Papers] edited by David R. Wilkins, School of Mathematics, Trinity College, Dublin 2, Ireland. (2000); also reviewed as [http://www.maths.tcd.ie/pub/HistMath/People/Hamilton/Dynamics/ On a General Method in Dynamics]'')\n* Goldstein H. (1980) ''Classical Mechanics'', 2nd ed., Addison Wesley, pp.&nbsp;35–69.\n* Landau LD and Lifshitz EM (1976) ''Mechanics'', 3rd. ed., Pergamon Press. {{ISBN|0-08-021022-8}} (hardcover) and {{ISBN|0-08-029141-4}} (softcover), pp.&nbsp;2–4.\n* Arnold VI. (1989) ''Mathematical Methods of Classical Mechanics'', 2nd ed., Springer Verlag, pp.&nbsp;59–61.\n* Cassel, Kevin W.: Variational Methods with Applications in Science and Engineering, Cambridge University Press, 2013.\n\n[[Category:Lagrangian mechanics]]\n[[Category:Calculus of variations]]\n[[Category:Principles]]\n[[Category:William Rowan Hamilton]]"
    },
    {
      "title": "Hilbert's nineteenth problem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%27s_nineteenth_problem",
      "text": "'''Hilbert's nineteenth problem''' is one of the 23 [[Hilbert problems]], set out in a list compiled in 1900 by [[David Hilbert]].<ref>See {{harv|Hilbert|1900}} or, equivalently, one of its translations.</ref> It asks whether the solutions of regular problems in the calculus of variations are always [[analytic function|analytic]].<ref>\"''Sind die Lösungen regulärer Variationsprobleme stets notwending analytisch?''\" (English translation by [[Mary Frances Winston Newson]]:-\"''Are the solutions of regular problems in the calculus of variations always necessarily analytic?''\"), formulating the problem with the same words of {{harvtxt|Hilbert|1900|p=288}}.</ref> Informally, and perhaps less directly, since Hilbert's concept of a \"''regular variational problem''\" identifies precisely a [[Calculus of variation|variational problem]] whose [[Euler–Lagrange equation]] is an [[elliptic partial differential equation]] with analytic coefficients,<ref>See {{harv|Hilbert|1900|pp=288–289}}, or the corresponding section on the nineteenth problem in any of its translation or reprint, or the subsection \"[[Hilbert's nineteenth problem#The origins of the problem|The origins of the problem]]\" in the historical section of this entry.</ref> Hilbert's nineteenth problem, despite its seemingly technical statement, simply asks whether, in this class of [[partial differential equation]]s, any solution function inherits the relatively simple and well understood structure from the solved equation.\n\n==History==\n\n===The origins of the problem===\n{{quote\n|text= Eine der begrifflich merkwürdigsten Thatsachen in den Elementen der Theorie der analytischen Funktionen erblicke ich darin, daß es Partielle Differentialgleichungen giebt, deren Integrale sämtlich notwendig analytische Funktionen der unabhängigen Variabeln sind, die also, kurz gesagt, nur analytischer Lösungen fähig sind.<ref>English translation by Mary Frances Winston Newson:-\"''One of the most remarkable facts in the elements of the theory of analytic functions appears to me to be this: that there exist partial differential equations whose integrals are all of necessity analytic functions of the independent variables, that is, in short, equations susceptible of none but analytic solutions''\".</ref>\n|sign= [[David Hilbert]]\n|source= {{harv|Hilbert|1900|p=288}}.\n}}\n\nDavid Hilbert presented the now called Hilbert's nineteenth problem in his speech at the second [[International Congress of Mathematicians]].<ref>For a detailed historical analysis, see the relevant entry \"[[Hilbert's problems]]\".</ref> In {{harv|Hilbert|1900|p=288}} he states that, in his opinion, one of the most remarkable facts of the theory of analytic functions is that there exist classes of partial differential equations which admit only such kind of functions as solutions, adducing [[Laplace's equation]], [[Liouville's equation]],<ref>Hilbert does not cite explicitly [[Joseph Liouville]] and considers the constant [[Gaussian curvature]] {{math|''K''}} as equal to {{math|-1/2}}: compare the relevant entry with {{harv|Hilbert|1900|p=288}}.</ref> the [[minimal surface equation]] and a class of linear partial differential equations studied by [[Émile Picard]] as examples.<ref>Contrary to Liouville's work, Picard's work is explicitly cited by {{harvtxt|Hilbert|1900|loc=p. 288 and footnote 1 in the same page}}.</ref> He then notes the fact that most of the partial differential equations sharing this property are the Euler–Lagrange equation of a well defined kind of variational problem, featuring the following three properties:<ref name=\"Hilbertp288\">See {{harv|Hilbert|1900|p=288}}.</ref>\n:{{EquationRef|1|(1){{spaces|5}}}}<math>{\\iint F(p,q,z;x,y) dx dy} = \\text{Minimum} \\qquad \n\\left[ \\frac{\\partial z}{\\partial x}=p \\quad;\\quad \\frac{\\partial z}{\\partial y}=q \\right]</math>,\n:{{EquationRef|2|(2){{spaces|5}}}}<math>\\frac{\\partial^2 F}{\\partial^2 p}\\cdot\\frac{\\partial^2 F}{\\partial^2 q} - \\left(\\frac{\\partial^2 F}{{\\partial p}{\\partial q}}\\right)^2 > 0</math>,\n:{{EquationRef|3|(3){{spaces|5}}}} {{math|''F''}} is an analytic function of all its arguments {{math|''p'', ''q'', ''z'', ''x''}} and {{math|''y''}}.\nHilbert calls this kind of variational problem a \"''regular variational problem''\":<ref>\"''Reguläres Variationsproblem''\", in his exact words. Hilbert's definition of a regular variational problem is stronger than the currently used one, found, for example, in {{harv|Gilbarg|Trudinger|2001|p=289}}.</ref> property {{EquationNote|(1)}} means that such kind of variational problems are [[minimum|minimum problems]], property {{EquationNote|(2)}} is the [[Elliptic partial differential equation|ellipticity condition]] on the Euler–Lagrange equations associated to the given [[Functional (mathematics)|functional]], while property {{EquationNote|(3)}} is a simple regularity assumption the function {{math|''F''}}.<ref>Since Hilbert considers all [[derivative]]s in the \"classical\", i.e. not in the [[Weak derivative|weak]] but in the [[strong derivative|strong]], sense, even before the statement of its analyticity in {{EquationNote|(3)}}, the function {{math|''F''}} is assumed to be at least {{math|{{SubSup|C||2}}}}, as the use of the [[Hessian determinant]] in {{EquationNote|(2)}} implies.</ref> Having identified the class of problems to deal with, he then poses the following question:-\"''... does every Lagrangian partial differential equation of a regular variation problem have the property of admitting analytic integrals exclusively?''\"<ref>English translation by Mary Frances Winston Newson: [[#{{harvid|Hilbert|1900}}|Hilbert's (1900]], p. 288) precise words are:-\"''... d. h. ob jede Lagrangesche partielle Differentialgleichung eines reguläres Variationsproblem die Eigenschaft at, daß sie nur analytische Integrale zuläßt''\" ([[Italic type|Italics emphasis]] by Hilbert himself).</ref> and asks further if this is the case even when the function is required to assume, as it happens for Dirichlet's problem on the [[potential theory|potential function]], boundary values which are continuous, but not analytic.<ref name=\"Hilbertp288\" />\n\n===The path to the complete solution===\nHilbert stated his nineteenth problem as a [[regularity problem]] for a class of elliptic partial differential equation with analytic coefficients,<ref name=\"Hilbertp288\" /> therefore the first efforts of the researchers who sought to solve it were directed to study the regularity of [[classical solution]]s for equations belonging to this class. For [[Continuously differentiable function|{{math|{{SubSup|C||3}}}}]] solutions Hilbert's problem was answered positively by {{harvs|txt|first=Sergei|last=Bernstein|authorlink=Sergei Natanovich Bernstein|year=1904}} in his thesis: he showed that {{math|{{SubSup|C||3}}}} solutions of nonlinear elliptic analytic equations in 2 variables are analytic. Bernstein's result was improved over the years by several authors, such as {{harvtxt|Petrowsky|1939}}, who reduced the differentiability requirements on the solution needed to prove that it is analytic. On the other hand, direct methods in the calculus of variations showed the existence of solutions with very weak differentiability properties. For many years  there was a gap between these results: the solutions that could be constructed were known to have square integrable second derivatives, which was not quite strong enough to feed into the machinery that could prove they were analytic, which needed continuity of first derivatives. This gap was filled independently by {{harvs|txt|author-link=Ennio de Giorgi|first=Ennio |last= De Giorgi|year1=1956|year2= 1957}}, and {{harvs|txt|first=John Forbes |last=Nash|author-link=John Forbes Nash|year1=1957|year2=1958}}. They were able to show the solutions had first derivatives that were [[Hölder continuous]], which by previous results implied that the solutions are analytic whenever the differential equation has analytic coefficients,  thus completing the solution of Hilbert's nineteenth problem.\n\n===Counterexamples to various generalizations of the problem===\nThe affirmative answer to Hilbert's nineteenth problem given by Ennio De Giorgi and John Forbes Nash raised the question if the same conclusion holds also for Euler-lagrange equations of more general [[Functional (mathematics)|functional]]s: at the end of the [[sixties]], {{harvtxt|Maz'ya|1968}},<ref>See {{harv|Giaquinta|1983|p=59}}, {{harv|Giusti|1994|loc=p. 7 footnote 7 and p. 353}}, {{harv|Gohberg|1999|p=1}}, {{harv|Hedberg|1999|pp=10–11}}, {{harv|Kristensen|Mingione|2011|loc=p. 5 and p. 8}}, and {{harv|Mingione|2006|p=368}}.</ref> {{harvtxt|De Giorgi|1968}} and {{harvtxt|Giusti|Miranda|1968}} constructed independently several [[counterexample]]s,<ref>See {{harv|Giaquinta|1983|pp=54–59}}, {{harv|Giusti|1994|loc=p. 7 and pp. 353}}.</ref> showing that in general there is no hope to prove such kind of regularity results without adding further hypotheses.\n\nPrecisely, {{harvtxt|Maz'ya|1968}} gave several counterexamples involving a single elliptic equation of order greater than two with analytic coefficients:<ref>See {{harv|Hedberg|1999|pp=10–11}}, {{harv|Kristensen|Mingione|2011|loc=p. 5 and p. 8}} and {{harv|Mingione|2006|p=368}}.</ref> for experts, the fact that such kind of equations could have nonanalytic and even nonsmooth solutions created a sensation.<ref>According to {{harv|Gohberg|1999|p=1}}.</ref>\n\n{{harvtxt|De Giorgi|1968}} and {{harvtxt|Giusti|Miranda|1968}} gave counterexamples showing that in the case when the solution is vector-valued rather than scalar-valued, it need not be analytic: the example of De Giorgi consists of an elliptic system with bounded coefficients, while the one of Giusti and Miranda has analytic coefficients.<ref>See {{harv|Giaquinta|1983|pp=54–59}} and {{harv|Giusti|1994|loc=p. 7, pp. 202–203 and pp. 317–318}}.</ref> Later on, {{harvtxt|Nečas|1977}} provided other, more refined, examples for the vector valued problem.<ref>For more information about the work of [[Jindřich Nečas]] see the work of {{harvtxt|Kristensen|Mingione|2011|loc=§3.3, pp. 9–12}} and {{harv|Mingione|2006|loc=§3.3, pp. 369–370}}.</ref>\n\n==De Giorgi's theorem==\nThe key theorem proved by De Giorgi is an [[a priori estimate]] stating that if ''u'' is a solution of a suitable linear second order strictly elliptic PDE of the form\n:<math> D_i(a^{ij}(x)\\,D_ju)=0</math>\nand <math>u</math> has square integrable first derivatives, then <math>u</math> is Hölder continuous.\n\n==Application of De Giorgi's theorem to Hilbert's problem==\nHilbert's problem asks whether the minimizers <math>w</math> of an energy functional such as\n:<math>\\int_UL(Dw)\\,\\mathrm{d}x</math>\nare analytic. Here <math>w</math> is a function on some compact set <math>U</math> of '''R'''<sup>''n''</sup>, <math>Dw</math> is its [[gradient]] vector, and <math>L</math> is the Lagrangian, a function of the derivatives of <math>w</math> that satisfies certain growth, smoothness, and convexity conditions. The smoothness of <math>w</math> can be shown using De Giorgi's theorem\nas follows. The [[Euler–Lagrange equation]] for this variational problem is the non-linear equation\n:<math> \\sum\\limits_{i=1}^n(L_{p_i}(Dw))_{x_i} = 0</math>\nand differentiating this with respect to <math>x_k</math> gives\n:<math> \\sum\\limits_{i=1}^n(L_{p_ip_j}(Dw)w_{x_jx_k})_{x_i} = 0</math>\nThis means that <math>u=w_{x_k}</math> satisfies the linear equation\n:<math> D_i(a^{ij}(x)D_ju)=0</math>\nwith \n:<math>a^{ij} = L_{p_ip_j}(Dw)</math>\nso by De Giorgi's result the solution ''w'' has Hölder continuous first derivatives, provided the matrix <math>L_{p_ip_j}</math> is bounded. When this is not the case, a further step is needed: one must prove that the solution <math>w</math> is Lipschitz continuous, i.e. the gradient <math>Dw</math> is an <math>L^\\infty</math> function.\n\nOnce ''w'' is known to have Hölder continuous (''n''+1)st derivatives for some ''n'' ≥ 1, then the coefficients ''a''<sup>''ij''</sup> have Hölder continuous ''n''th derivatives, so a theorem of Schauder implies that the (''n''+2)nd derivatives are also Hölder continuous, so repeating this infinitely often shows that the solution ''w'' is smooth.\n\n==Nash's theorem==\n\nNash gave a continuity estimate for solutions of the parabolic equation\n:<math> D_i(a^{ij}(x)D_ju)=D_t(u)</math>\nwhere ''u'' is a bounded function of ''x''<sub>1</sub>,...,''x''<sub>''n''</sub>, ''t'' defined for ''t'' ≥ 0. From his estimate Nash was able to deduce a continuity estimate for solutions of the elliptic equation \n:<math> D_i(a^{ij}(x)D_ju)=0</math> by considering the special case when ''u'' does not depend on ''t''.\n\n==Notes==\n{{reflist|29em}}\n\n==References==\n*{{Citation\n | last = Bernstein\n | first = S.\n | author-link = Sergei Natanovich Bernstein\n | title = Sur la nature analytique des solutions des équations aux dérivées partielles du second ordre\n | journal = [[Mathematische Annalen]]\n | issn = 0025-5831\n | volume = 59\n | issue = 1–2\n | pages = 20–76\n | year = 1904\n | language = French\n | url = http://www.digizeitschriften.de/dms/resolveppn/?PPN=GDZPPN00225977X\n | doi = 10.1007/BF01444746\n | jfm = 35.0354.01\n}}.\n*{{Citation\n | last=Bombieri\n | first=Enrico\n | author-link=Enrico Bombieri\n | editor-last =\n | editor-first =\n | editor2-last =\n | editor2-first =\n | contribution = Variational problems and elliptic equations\n | contribution-url = http://www.mathunion.org/ICM/ICM1974.1/Main/icm1974.1.0053.0064.ocr.pdf\n | title=Proceedings of the International Congress of Mathematicians, Vancouver, B.C., 1974, Vol. 1\n | series =ICM Proceedings\n | year=1975\n | pages=53–63\n | place= Montreal\n | publisher = Canadian Mathematical Congress\n | url = http://www.mathunion.org/ICM/ICM1974.1/\n | mr=0509259\n | zbl=0344.49002\n}}. Reprinted in {{Citation\n | last=Bombieri\n | first=Enrico \n | author-link=Enrico Bombieri\n | editor-last=Browder\n | editor-first=Felix E.\n | editor-link= Felix Browder\n | contribution = Variational problems and elliptic equations\n | contribution-url = \n | title=Mathematical developments arising from Hilbert problems\n | series=[[Proceedings of Symposia in Pure Mathematics]]\n | volume=XXVIII\n | year=1976\n | pages=525–535\n | place=Providence, Rhode Island\n | publisher=[[American Mathematical Society]]\n | url=https://books.google.com/books?isbn=0821814281\n | isbn=978-0-8218-1428-4\n | mr=0425740\n | zbl=0347.35032\n}}.\n*{{Citation\n | last=De Giorgi\n | first=Ennio\n | author-link=Ennio De Giorgi\n | title=Sull'analiticità delle estremali degli integrali multipli\n | journal=Atti della Accademia Nazionale dei Lincei. Rendiconti. Classe di Scienze Fisiche, Matematiche e Naturali\n | series=Serie VIII\n | volume=20\n | pages=438–441\n | year=1956\n | language=Italian\n | mr=0082045\n | zbl=0074.31503\n}}. \"''On the analyticity of extremals of multiple integrals''\" (English translation of the title) is a short research announcement disclosing the results detailed later in {{harv|De Giorgi|1957}}. While, according to the [[#{{harvid|De Giorgi|2006}}|Complete list of De Giorgi's scientific publication (De Giorgi 2006]], p.&nbsp;6), an English translation should be included in {{harv|De Giorgi|2006}}, it is unfortunately missing.\n*{{Citation\n | last=De Giorgi\n | first=Ennio\n | title=Sulla differenziabilità e l'analiticità delle estremali degli integrali multipli regolari\n | journal=Memorie della Accademia delle Scienze di Torino. Classe di Scienze Fisiche, Matematicahe e Naturali.\n | series = Serie III\n | volume=3\n | pages=25–43\n | year=1957\n | language=Italian\n | mr=0093649\n | zbl=0084.31901\n}}. Translated in English as \"''On the differentiability and the analyticity of extremals of regular multiple integrals''\" in {{harv|De Giorgi|2006|pp=149–166}}.\n*{{Citation\n | last=De Giorgi\n | first=Ennio\n | title=Un esempio di estremali discontinue per un problema variazionale di tipo ellittico\n | journal=Bollettino Dell'Unione Matematica Italiana (4)\n | series = Serie IV \n | volume=1\n | pages=135–137\n | year=1968\n | language=Italian\n | mr=0227827\n | zbl=0084.31901\n}}. Translated in English as \"''An example of discontinuous extremals for a variational problem of elliptic type''\" in {{harv|De Giorgi|2006|pp=285–287}}.\n*{{Citation\n | last=De Giorgi\n | first=Ennio\n | editor-last=Ambrosio\n | editor-first=Luigi\n | editor-link=Luigi Ambrosio\n | editor2-last=Dal Maso\n | editor2-first=Gianni\n | editor2-link=Gianni Dal Maso\n | editor3-last=Forti\n | editor3-first=Marco\n | editor4-last=Miranda\n | editor4-first=Mario\n | editor4-link=Mario Miranda (mathematician)\n | editor5-last=Spagnolo\n | editor5-first=Sergio\n | title=Selected papers\n | place=Berlin–New York\n | publisher=[[Springer-Verlag]]\n | year=2006\n | pages=x+889\n | url=https://www.springer.com/mathematics/analysis/book/978-3-540-26169-8\n | isbn=978-3-540-26169-8\n | mr=2229237\n | zbl=1096.01015\n}}.\n*{{Citation\n | last = Giaquinta\n | first = Mariano\n | author-link = Mariano Giaquinta\n | title = Multiple integrals in the calculus of variations and nonlinear elliptic systems\n | place = Princeton, New Jersey\n | publisher =Princeton University Press\n | series = [[Annals of Mathematics Studies]]\n | volume = 105\n | year = 1983\n | pages = vii+297\n | url = https://books.google.com/?id=JwSAewaYsdMC&printsec=frontcover#v=onepage&q&f=true\n | isbn = 978-0-691-08330-8\n | mr = 0717034\n | zbl = 0516.49003\n}}.\n*{{Citation\n | last = Gilbarg\n | first = David\n | author-link = David Gilbarg\n | last2 = Trudinger\n | first2 = Neil S. \n | author2-link = Neil Trudinger\n | title = Elliptic partial differential equations of second order\n | place = Berlin – Heidelberg – New York\n | publisher = Springer Verlag\n | series = Classics in Mathematics\n | origyear = 1998\n | year = 2001\n | edition = Revised 3rd printing of 2nd \n | pages = xiv+517\n | url = https://books.google.com/?id=eoiGTf4cmhwC&printsec=frontcover#v=onepage&q&f=true\n | doi =\n | id =\n | isbn = 978-3-540-41160-4\n | mr = 1814364 \n | zbl = 1042.35002\n}}.\n*{{Citation\n| last = Giusti\n| first = Enrico\n| author-link = Enrico Giusti\n| title = Metodi diretti nel calcolo delle variazioni\n| place = [[Bologna]]\n| publisher = [[Unione Matematica Italiana]]\n| year = 1994\n| language = Italian\n| series = Monografie Matematiche\n| pages = VI+422\n| url = \n| isbn = \n| mr = 1707291\n| zbl = 0942.49002}}, translated in English as {{Citation\n| title = Direct Methods in the Calculus of Variations\n| place = [[River Edge, New Jersey]] – London – Singapore\n| publisher = World Scientific Publishing\n| year = 2003\n| pages = viii+403\n| url = https://books.google.com/?id=FofhcvUZo9YC&printsec=frontcover#v=onepage&q&f=true\n| isbn = 978-981-238-043-2\n| mr = 1962933\n| zbl = 1028.49001\n}}.\n*{{Citation\n | last=Giusti\n | first=Enrico\n | author-link=Enrico Giusti\n | last2=Miranda\n | first2=Mario\n | author2-link=Mario Miranda (mathematician)\n | title=Un esempio di soluzioni discontinue per un problema di minimo relativo ad un integrale regolare del calcolo delle variazioni\n | journal=[[Bollettino dell'Unione Matematica Italiana]]\n | series = Serie IV \n | volume=2\n | pages=1–8\n | year=1968\n | language=Italian\n | mr=0232265\n | zbl=0155.44501\n}}.\n*{{Citation\n| first = Israel\n| last = Gohberg\n| author-link = Israel Gohberg\n| editor-last = Rossman\n| editor-first = Jürgen\n| editor2-last = Takáč\n| editor2-first = Peter\n| editor3-last = Wildenhain\n| editor3-first = Günther\n| contribution = Vladimir Maz'ya: Friend and Mathematician. Recollections\n| contribution-url = \n| title = The Maz'ya anniversary collection. Vol. 1: On Maz'ya's work in functional analysis, partial differential equations and applications. Based on talks given at the conference, Rostock, Germany, August 31 – September 4, 1998\n| series = Operator Theory. Advances and Applications\n| volume = 109\n| year = 1999\n| pages = 1–5\n| place = Basel\n| publisher = Birkhäuser Verlag\n| url = https://books.google.com/?id=9xPz9Mg2c_EC&printsec=frontcover#v=onepage&q\n| mr = 1747861 \n| zbl = 0939.01018\n| isbn = 978-3-7643-6201-0\n}}.\n*{{Citation\n | first = Lars Inge \n | last =Hedberg\n | author-link =\n | editor-last =Rossmann\n | editor-first =Jürgen\n | editor2-last =Takáč\n | editor2-first =Peter\n | editor3-last =Wildenhain\n | editor3-first =Günther\n | contribution =On Maz'ya's work in potential theory and the theory of function spaces\n | contribution-url =\n | title =The Maz'ya Anniversary Collection. Volume 1: On Maz'ya's work in functional analysis, partial differential equations and applications\n | series =109\n | volume =Operator Theory: Advances and Applicationsǘ\n | year =1999\n | pages =7–16\n | place =[[Basel]]\n | publisher =Birkhäuser Verlag\n | url =\n | doi =10.1007/978-3-0348-8675-8_2\n | mr =1747862\n | zbl =0939.31001\n| isbn =978-3-0348-9726-6\n }}\n*{{Citation\n | last = Hilbert\n | first = David\n | author-link = David Hilbert\n | title = Mathematische Probleme\n | journal = [[Nachrichten von der Königlichen Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse]]\n | issue = 3\n | pages = 253–297\n | year = 1900\n | language = German\n | url = http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN252457811_1900&DMDID=DMDLOG_0037\n | jfm =31.0068.03\n}} (reprinted as {{Citation\n | title = Mathematische Probleme\n | journal = [[Archiv der Mathematik und Physik]]\n | series = dritte reihe\n | volume = 1\n | pages = 44–63 and 253–297\n | year = 1900\n | language = German\n | url = https://archive.org/stream/archivdermathem02unkngoog#page/n61/mode/1up\n | jfm = 32.0084.05\n}}), translated in English by [[Mary Frances Winston Newson]] as {{Citation\n | last = Hilbert\n | first = David\n | author-link = David Hilbert\n | title = Mathematical Problems\n | journal = [[Bulletin of the American Mathematical Society]]\n | volume = 8\n | issue = 10\n | pages = 437–479\n | year = 1902\n | doi = 10.1090/S0002-9904-1902-00923-3\n | jfm = 33.0976.07\n | mr = 1557926\n}} (reprinted as {{Citation\n | last = Hilbert\n | first = David\n | author-link = David Hilbert\n | title = Mathematical Problems\n | journal = [[Bulletin of the American Mathematical Society]]\n | series = New Series\n | volume = 37\n | issue = 4\n | pages = 407–436\n | year = 2000\n | doi = 10.1090/S0273-0979-00-00881-8 \n | mr = 1779412\n | zbl = 0979.01028\n}}), and in French (with additions of Hilbert himself) by M. L. Laugel as {{Citation\n | last = Hilbert\n | first = David\n | author-link = David Hilbert\n | editor-last = Duporcq\n | editor-first = E.\n | contribution = Sur les problèmes futurs des Mathématiques\n | contribution-url = http://www.mathunion.org/ICM/ICM1900/Main/icm1900.0058.0114.ocr.pdf\n | title = Compte Rendu du Deuxième Congrès International des Mathématiciens, tenu à Paris du 6 au 12 août 1900. Procès-Verbaux et Communications\n | series = ICM Proceedings\n | year = 1902\n | pages = 58–114\n | place = Paris\n | publisher = Gauthier-Villars\n | url = http://www.mathunion.org/ICM/ICM1900/\n | jfm = 32.0084.06}}. There exists also an earlier (and shorter) resume of Hilbert's original talk, translated in French and published as {{Citation\n | last = Hilbert\n | first = D.\n | author-link = David Hilbert\n | title =Problèmes mathématiques\n | journal =[[L'Enseignement Mathématique]]\n | volume =2\n | pages =349–355\n | year =1900\n | language =French\n | url =\n | doi =10.5169/seals-3575\n | jfm = 31.0905.03}}.\n*{{Citation\n |last         = Kristensen\n |first        = Jan\n |author-link  = \n |last2        = Mingione\n |first2       = Giuseppe\n |author2-link = Giuseppe Mingione\n |title        = Sketches of Regularity Theory from The 20th Century and the Work of Jindřich Nečas\n |volume       = Report no. OxPDE-11/17\n |place        = Oxford\n |publisher    = Oxford Centre for Nonlinear PDE\n |pages        = 1–30\n |date         = October 2011\n |url          = http://www.maths.ox.ac.uk/system/files/attachments/OxPDE_11-17.pdf\n |doi          = \n |id           = \n |mr           = \n |zbl          = \n |deadurl      = yes\n |archiveurl   = https://web.archive.org/web/20140107114055/http://www.maths.ox.ac.uk/system/files/attachments/OxPDE_11-17.pdf\n |archivedate  = 2014-01-07\n |df           = \n}}.\n*{{Citation\n | last = Maz'ya\n | first = V. G.\n | author-link = Vladimir Gilelevich Maz'ya\n | script-title=ru:Примеры нерегулярных решений квазилинейных эллиптических уравнений с аналитическими коэффициентами\n | journal =[[Funktsional’nyĭ Analiz i Ego Prilozheniya]]\n | volume =2\n | issue = 3\n | pages = 53–57\n | year =1968\n | language = Russian\n | url = http://mi.mathnet.ru/eng/faa/v2/i3/p53\n | mr =0237946 \n | zbl = \n }}, translated in English as {{Citation\n | last = Maz'ya\n | first = V. G.\n | author-link = Vladimir Gilelevich Maz'ya\n | title = Examples of nonregular solutions of quasilinear elliptic equations with analytic coefficients \n | journal = [[Functional Analysis and Its Applications]]\n | volume = 2\n | issue = 3\n | pages = 230–234\n | year =1968\n | doi =10.1007/BF01076124\n | zbl = 0179.43601\n}}.\n*{{Citation\n | last =Mingione\n | first =Giuseppe\n | author-link =Giuseppe Mingione\n | title = Regularity of minima: an invitation to the Dark Side of the Calculus of Variations.\n | journal =[[Applications of Mathematics]]\n | volume =51\n | issue =4\n | pages =355–426\n | year =2006\n | url =http://dml.cz/dmlcz/134645\n | mr =2291779\n | zbl =1164.49324\n | doi=10.1007/s10778-006-0110-3\n| citeseerx =10.1.1.214.9183\n }}.\n*{{Citation\n| last=Morrey \n| first=Charles B. \n| author-link=Charles B. Morrey, Jr.\n| title= Multiple integrals in the calculus of variations \n| url=https://books.google.com/books?id=-QNKm1PBohsC \n| place = Berlin–Heidelberg–New York\n| publisher = Springer-Verlag\n| series = Die Grundlehren der mathematischen Wissenschaften\n| volume=130\n| year=1966 \n| pages = xii+506\n| isbn=978-3-540-69915-6  \n| mr=0202511 \n| zbl=0142.38701 \n}}.\n*{{Citation\n | last=Nash\n | first=John\n | author-link=John Forbes Nash\n | title=Parabolic equations\n | journal=[[Proceedings of the National Academy of Sciences of the United States of America]]\n | year=1957\n | volume=43\n | issue=8\n | pages=754–758\n | issn=0027-8424\n | jstor=89599\n | mr=0089986\n | zbl=0078.08704\n | doi=10.1073/pnas.43.8.754\n| pmid=16590082\n | pmc=528534\n }}.\n*{{Citation\n | last1=Nash\n | first1=John\n | author1-link=John Forbes Nash\n | title=Continuity of solutions of parabolic and elliptic equations\n | year=1958\n | journal=[[American Journal of Mathematics]]\n | volume=80\n | issue=4\n | pages=931–954\n | issn=0002-9327\n | jstor=2372841\n | mr=0100158\n | zbl=0096.06902\n | doi=10.2307/2372841\n}}.\n*{{Citation\n | first =Jindřich \n | last = Nečas\n | author-link =Jindřich Nečas\n | editor-last =Kluge\n | editor-first =Reinhard\n | editor2-last =Müller\n | editor2-first =Wolfdietrich\n | contribution = Example of an irregular solution to a nonlinear elliptic system with analytic coefficients and conditions for regularity\n | contribution-url =\n | title = Theory of nonlinear operators: constructive aspects. Proceedings of the fourth international summer school, held at Berlin, GDR, from September 22 to 26, 1975 \n | series = Abhandlungen der Akademie der Wissenschaften der DDR\n | volume = Nr. 1N\n | year = 1977\n | pages = 197–206\n | place = Berlin\n | publisher = Akademie-Verlag\n | url =\n | doi =\n | id = \n | mr =0509483\n | zbl=0372.35031\n}}.\n*{{Citation\n | last1=Petrowsky\n | first1=I. G.\n | author-link=Ivan Georgievich Petrovsky\n | title= Sur l'analyticité des solutions des systèmes d'équations différentielles\n | url= http://mi.mathnet.ru/eng/msb5769\n | year=1939\n | language = French\n | journal = [[Matematicheskii Sbornik|Recueil Mathématique (Matematicheskii Sbornik)]] \n | volume = 5(47)\n | issue = 1\n | pages = 3–70\n | jfm = 65.0405.02\n | mr = 0001425\n | zbl = 0022.22601\n}}.\n\n{{Hilbert's problems}}\n\n[[Category:Hilbert's problems|#19]]\n[[Category:Partial differential equations]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Hilbert's twentieth problem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%27s_twentieth_problem",
      "text": "'''Hilbert's twentieth problem''' is one of the 23 [[Hilbert problems]] set out in a celebrated list compiled in 1900 by [[David Hilbert]].  It asks whether all [[boundary value problem]]s can be solved (that is, do [[calculus of variations|variational problems]] with certain [[boundary condition]]s have solutions).\n\n==Introduction==\nHilbert noted that there existed methods for solving partial differential equations where the function's values were given at the boundary, but the problem asked for methods for solving partial differential equations with more complicated conditions on the boundary (e.g., involving derivatives of the function), or for solving calculus of variation problems in more than 1 dimension (for example, minimal surface problems or minimal curvature problems)\n\n==Problem statement==\nThe original problem statement in its entirety is as follows:\n<blockquote>\nAn important problem closely connected with the foregoing [referring to [[Hilbert's nineteenth problem]]] is the question concerning the existence of solutions of partial differential equations when the values on the boundary of the region are prescribed. This problem is solved in the main by the keen methods of H. A. Schwarz, C. Neumann, and Poincaré for the differential equation of the potential. These methods, however, seem to be generally not capable of direct extension to the case where along the boundary there are prescribed either the differential coefficients or any relations between these and the values of the function. Nor can they be extended immediately to the case where the inquiry is not for potential surfaces but, say, for surfaces of least area, or surfaces of constant positive gaussian curvature, which are to pass through a prescribed twisted curve or to stretch over a given ring surface. It is my conviction that it will be possible to prove these existence theorems by means of a general principle whose nature is indicated by Dirichlet's principle. This general principle will then perhaps enable us to approach the question: Has not every regular variation problem a solution, provided certain assumptions regarding the given boundary conditions are satisfied (say that the functions concerned in these boundary conditions are continuous and have in sections one or more derivatives), and provided also if need be that the notion of a solution shall be suitably extended?<ref>Hilbert, David, \"Mathematische Probleme\" [[Göttinger Nachrichten]], (1900), pp. 253-297, and in [[Archiv der Mathematik und Physik]], (3) '''1''' (1901), 44-63 and 213-237. Published in English translation by Dr. Maby Winton Newson, [[Bulletin of the American Mathematical Society]] '''8''' (1902), 437-479 [http://aleph0.clarku.edu/~djoyce/hilbert/problems.html#prob23] [http://www.ams.org/journals/bull/1902-08-10/S0002-9904-1902-00923-3/S0002-9904-1902-00923-3.pdf] {{doi|10.1090/S0002-9904-1902-00923-3}} . [A fuller title of the journal Göttinger Nachrichten is Nachrichten von der Königl. Gesellschaft der Wiss. zu Göttingen.]</ref>\n</blockquote>\n\n==Boundary value problems==\n{{main|Boundary value problem}}\nIn the field of [[differential equation]]s, a [[boundary value problem]] is a [[differential equation]] together with a set of additional constraints, called the '''boundary conditions'''. A solution to a boundary value problem is a solution to the differential equation which also satisfies the boundary conditions.\n\nTo be useful in applications, a boundary value problem should be [[well-posed problem|well posed]].  This means that given the input to the problem there exists a unique solution, which depends continuously on the input.  Much theoretical work in the field of [[partial differential equation]]s is devoted to proving that boundary value problems arising from scientific and engineering applications are in fact well-posed.\n\n==References==\n{{reflist}}\n*{{citation\n | last = Krzywicki | first = Andrzej\n | contribution = Hilbert's Twentieth Problem\n | language = Polish\n | mr = 1632452\n | pages = 237–245\n | publisher = Polsk. Akad. Nauk, Warsaw\n | title = Hilbert's Problems (Mi\\polhk edzyzdroje, 1993)\n | year = 1997}}.\n*{{citation\n | last = Serrin | first = James|authorlink=James Serrin\n | contribution = The solvability of boundary value problems\n | location = Providence, R. I.\n | mr = 0427784\n | pages = 507–524\n | publisher = American Mathematical Society\n | series = Proceedings of Symposia in Pure Mathematics\n | title = Mathematical developments arising from Hilbert problems (Northern Illinois Univ., De Kalb, Ill., May 1974)\n | volume = XXVIII\n | year = 1976}}.\n*{{citation\n | last = Sigalov | first = A. G.\n | contribution = On Hilbert's nineteenth and twentieth problems\n | language = Russian\n | location = Moscow\n | mr = 0251611\n | pages = 204–215\n | publisher = Izdat. “Nauka”\n | title = Hilbert's Problems\n | year = 1969}}.\n\n{{Hilbert's problems}}\n\n[[Category:Hilbert's problems|#20]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "History of variational principles in physics",
      "url": "https://en.wikipedia.org/wiki/History_of_variational_principles_in_physics",
      "text": "A '''variational principle''' in physics is an alternative method for determining the state or dynamics of a physical system, by identifying it as an extremum (minimum, maximum or saddle point) of a function or functional.  This article describes the historical development of such principles.\n\n==Variational principles before [[Modern history|modern times]]==\n\nVariational principles are found among earlier ideas in [[surveying]] and [[optics]]. The [[rope stretchers]] of [[ancient Egypt]] stretched corded ropes between two points to measure the path which minimized the distance of separation, and [[Claudius Ptolemy]], in his [[Geographia (Ptolemy)|Geographia]] (Bk 1, Ch 2), emphasized that one must correct for \"deviations from a straight course\"; in [[ancient Greece]] [[Euclid]] states in his ''Catoptrica'' that, for the path of light reflecting from a mirror, the [[angle of incidence (optics)|angle of incidence]] equals the [[angle of reflection]]; and [[Hero of Alexandria]] later showed that this path was the shortest length and least time.<ref>{{cite book|last=Kline|first=Morris|title=Mathematical Thought from Ancient to Modern Times|publisher=Oxford University Press|location=New York|year=1972|pages= 167–168|isbn=0-19-501496-0}}</ref>\n\nThis was generalized to [[refraction]] by [[Pierre de Fermat]], who, in the 17th century, refined the principle to \"light travels between two given points along the path of shortest ''time''\"; now known as the '''[[principle of least time]]''' or '''[[Fermat's principle]]'''.\n\n==Principle of extremal action==\nCredit for the formulation of the '''principle of least action''' is commonly given to [[Pierre Louis Maupertuis]], who wrote about it in 1744{{ref|Mau44}} and 1746,{{ref|Mau46}} although the true priority is less clear, as discussed below.\n\nMaupertuis felt that \"Nature is thrifty in all its actions\", and applied the principle broadly: \"The laws of movement and of rest deduced from this principle being precisely the same as those observed in nature, we can admire the application of it to all phenomena. The movement of animals, the vegetative growth of plants ... are only its consequences; and the spectacle of the universe becomes so much the grander, so much more beautiful, the worthier of its Author, when one knows that a small number of laws, most wisely established, suffice for all movements.\" {{ref|Dav98}}\n\nIn application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the \"[[vis viva]]\", twice what we now call the kinetic energy of the system.\n\n[[Leonhard Euler]] gave a formulation of the action principle in 1744, in very recognizable terms, in the ''Additamentum 2'' to his \"Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes\".{{ref|Eul44}} He begins the second paragraph:{{ref|Adit}}\n\n:\"Sit massa corporis projecti ==''M'', ejusque, dum spatiolum == ''ds'' emetitur, celeritas debita altitudini == ''v''; erit quantitas motus corporis in hoc loco == <math>M\\sqrt{v}</math> ; quae per ipsum spatiolum ''ds'' multiplicata, dabit <math>M\\,ds\\sqrt{v}</math> motum corporis collectivum per spatiolum ''ds''. Iam dico lineam a corpore descriptam ita fore comparatam, ut, inter omnes alias lineas iisdem terminis contentas, sit <math>\\int M ds \\sqrt{v}</math>, seu, ob M constans, <math>\\int ds \\sqrt{v}</math> minimum.\"\n\nA translation of this passage reads:\n\n:\"Let the mass of the projectile be ''M'', and let its squared velocity resulting from its height be <math>v</math> while being moved over a distance ''ds''.   The body will have a momentum <math>M \\sqrt{v}</math> that, when multiplied by the distance ''ds'', will give <math>M ds \\sqrt{v}</math>, the momentum of the body integrated over the distance ''ds''.  Now I assert that the curve thus described by the body to be the curve (from among all other curves connecting the same endpoints) that minimizes <math>\\int M ds \\sqrt{v}</math> or, provided that  ''M'' is constant, <math>\\int ds \\sqrt{v}</math>.\"\n\nAs Euler states, <math>\\int M ds \\sqrt{v}</math> is the integral of the momentum over distance traveled (note that here <math>v</math> contrary to usual notation denotes the ''squared'' velocity) which, in modern notation, equals the [[reduced action]] <math>\\int p\\,dq</math>.  Thus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. In rather general terms he  wrote that \"Since the fabric of the Universe is most perfect and is the work of a most wise Creator, nothing whatsoever takes place in the Universe in which some relation of maximum and minimum does not appear.\"\nHowever, Euler did not claim any priority, as the following episode shows.\n\nMaupertuis' priority was disputed in 1751 by the mathematician [[Samuel König]], who claimed that it had been invented by [[Gottfried Leibniz]] in 1707.  Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works.  König himself showed a ''copy'' of a 1707 letter from Leibniz to [[Jacob Hermann (mathematician)|Jacob Hermann]] with the principle, but the ''original'' letter has been lost. In contentious proceedings, König was accused of forgery,{{ref|Oco03}} and even the King of Prussia entered the debate, defending Maupertuis, while [[Voltaire]] defended König. Euler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752.{{ref|Oco03}} The claims of forgery were re-examined 150 years later, and archival work by [[C.I. Gerhardt]] in 1898{{ref|Ger98}} and [[W. Kabitz]] in 1913{{ref|Kab13}} uncovered other copies of the letter, and three others cited by König, in the [[Bernoulli family|Bernoulli]] archives.\n\n==Further developments of the extremal-action principle==\nEuler continued to write on the topic; in his ''Reflexions sur quelques loix generales de la nature'' (1748), he called the quantity \"effort\". His expression corresponds to what we would now call [[potential energy]], so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.\n\nThe full importance of the principle to mechanics was stated by [[Joseph Louis Lagrange]] in 1760,{{citation needed|date=February 2018}} although the variational principle was not used to derive the equations of motion until almost 75 years later, when [[William Rowan Hamilton]] in 1834 and 1835 {{ref|Ham34}} applied the variational principle to the function <math>L=T-V</math> to obtain what are now called the '''[[Lagrangian equations of motion]]'''.\n\n==Other formulations of the extremal-action principle==\nIn 1842, [[Carl Gustav Jacobi]] tackled the problem of whether the variational principle found minima or other extrema (e.g. a [[saddle point]]); most of his work focused on geodesics on two-dimensional surfaces. {{ref|Jac42}} The first clear general statements were given by [[Marston Morse]] in the 1920s and 1930s, {{ref|Mor34}} leading to what is now known as [[Morse theory]]. For example, Morse showed that the number of conjugate points in a trajectory equaled the number of negative eigenvalues in the second variation of the Lagrangian.\n\nOther extremal principles of [[classical mechanics]] have been formulated, such as [[Gauss' principle of least constraint]] and its corollary, [[Gauss' principle of least constraint|Hertz's principle of least curvature]].\n\n==Variational principles in electromagnetism==\nThe action for electromagnetism is:\n\n:<math> \\mathcal{S} =  -\\int \\frac{1}{4 \\mu_0} \\, \\mathrm{d}^4x  \\, F^{\\alpha\\beta} F_{\\alpha\\beta} - \\int \\mathrm{d}^4x \\, j^{\\alpha}A_{\\alpha} </math>\n\n==Variational principles in relativity theory==\nThe [[Einstein–Hilbert action]] which gives rise to the vacuum [[Einstein field equations]] is\n:<math>\\mathcal{S}[g] =\\frac{c^4}{16 \\pi G}\\int_{\\mathcal{M}} R \\sqrt{-g} \\, \\mathrm{d}^4 x</math>,\nwhere <math>g=\\det(g_{\\alpha\\beta})</math> is the determinant of a spacetime [[Lorentz metric]] and <math>R</math> is the [[scalar curvature]].\n\n==Variational principles in quantum mechanics==\n* Sum over possible paths, Feynman approach. See [[Path integral formulation]]\n* Dirac-Frenkel Variational Principle\n\n==Apparent teleology?==\nAlthough equivalent mathematically, there is an important ''philosophical'' difference between the [[differential equation|differential]] [[equations of motion]] and their [[integral equation|integral]] counterpart.  The differential equations are statements about quantities localized to a single point in space or single moment of time.  For example, [[Newton's laws of motion|Newton's second law]] <math>F=ma</math> states that the ''instantaneous'' force <math>F</math> applied to a mass <math>m</math> produces an acceleration <math>a</math> at the same ''instant''.  By  contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) extended region of space.  Moreover, in the usual formulation of [[classical physics|classical]] action principles, the initial and final states of the system are fixed, e.g.,\n\n:''Given that the particle begins at position <math>x_{1}</math> at time <math>t_{1}</math> and ends at position <math>x_{2}</math> at time <math>t_{2}</math>, the physical trajectory that connects these two endpoints is an extremum of the action integral.''\n\nIn particular, the fixing of the ''final'' state appears to give the action principle a [[teleology|teleological character]] which has been controversial historically.  This apparent [[teleology]] is eliminated in the [[quantum mechanics|quantum mechanical]] version of the action principle.\n\n==References==\n<references/>\n* {{note|Mau44}} P.L.N. de Maupertuis, ''Accord de différentes lois de la nature qui avaient jusqu'ici paru incompatibles.'' (1744) Mém. As. Sc. Paris p.&nbsp;417.\n* {{note|Mau46}} P.L.N. de Maupertuis, ''Le lois de mouvement et du repos, déduites d'un principe de métaphysique.'' (1746) Mém. Ac. Berlin, p.&nbsp;267.\n* {{Note|Eul44}} Leonhard Euler, ''Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes.'' (1744) Bousquet, Lausanne & Geneva. 320 pages. Reprinted in ''Leonhardi Euleri Opera Omnia: Series I vol 24.'' (1952) C. Cartheodory (ed.) Orell Fuessli, Zurich. [http://math.dartmouth.edu/~euler/pages/E065.html scanned copy of complete text] at ''[http://math.dartmouth.edu/~euler/ The Euler Archive]'', Dartmouth.\n* {{note|Ham34}} W.R. Hamilton, \"On a General Method in Dynamics.\", ''Philosophical Transactions of the Royal Society'' [http://www.emis.de/classics/Hamilton/GenMeth.pdf Part I (1834) p.247-308]; [http://www.emis.de/classics/Hamilton/SecEssay.pdf Part II (1835) p. 95-144]. (''From the collection [http://www.emis.de/classics/Hamilton/ Sir William Rowan Hamilton (1805-1865): Mathematical Papers] edited by David R. Wilkins, School of Mathematics, Trinity College, Dublin 2, Ireland. (2000); also reviewed as [http://www.maths.tcd.ie/pub/HistMath/People/Hamilton/Dynamics/ On a General Method in Dynamics]'')\n* {{note|Jac42}} G.C.J. Jacobi, ''Vorlesungen über Dynamik, gehalten an der Universität Königsberg im Wintersemester 1842-1843''. A. Clebsch (ed.) (1866); Reimer; Berlin. 290 pages, available online [http://math-doc.ujf-grenoble.fr/cgi-bin/oeitem?id=OE_JACOBI__8_1_0  Œuvres complètes volume '''8'''] at [http://math-doc.ujf-grenoble.fr/OEUVRES/ Gallica-Math] from the [http://gallica.bnf.fr/ Gallica Bibliothèque nationale de France].\n* {{note|Ger98}} Gerhardt CI. (1898) \"Über die vier Briefe von Leibniz, die Samuel König in dem Appel au public, Leide MDCCLIII, veröffentlicht hat\", ''Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften'', '''I''', 419-427.\n* {{note|Kab13}} Kabitz W. (1913) \"Über eine in Gotha aufgefundene Abschrift des von S. König in  seinem Streite mit Maupertuis und der Akademie veröffentlichten, seinerzeit für unecht erklärten Leibnizbriefes\", ''Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften'', '''II''', 632-638.\n* {{Note|Mor34}} Marston Morse (1934). \"The Calculus of Variations in the Large\", ''American Mathematical Society Colloquium Publication'' '''18'''; New York.\n* {{Note|Dav98}} Chris Davis. ''[https://web.archive.org/web/20030511071911/http://www.idlex.freeserve.co.uk/idle/evolution/ref/leastact.html Idle theory]'' (1998)\n* {{Note|Adit}} Euler, ''Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes: [http://math.dartmouth.edu/~euler/docs/originals/E065h Additamentum II]'', Ibid.\n* {{Note|Oco03}} J J O'Connor and E F Robertson, \"[http://www-history.mcs.st-andrews.ac.uk/history/HistTopics/Forgery_2.html The Berlin Academy and forgery]\", (2003), at ''[http://www-history.mcs.st-andrews.ac.uk/history/ The MacTutor History of Mathematics archive]''.\n{{Use dmy dates|date=September 2010}}\n\n* Cassel, Kevin W.: Variational Methods with Applications in Science and Engineering, Cambridge University Press, 2013.\n\n{{DEFAULTSORT:History Of Variational Principles In Physics}}\n[[Category:Calculus of variations]]\n[[Category:History of physics|Variational principles in physics]]"
    },
    {
      "title": "Homicidal chauffeur problem",
      "url": "https://en.wikipedia.org/wiki/Homicidal_chauffeur_problem",
      "text": "In [[game theory]], the '''homicidal chauffeur problem''' is a mathematical [[Pursuit-evasion|pursuit problem]] which pits a hypothetical runner, who can only move slowly, but is highly maneuverable, against the driver of a motor vehicle, which is much faster but far less maneuverable, who is attempting to run him down. Both runner and driver are assumed to never tire. The question to be solved is: under what circumstances, and with what strategy, can the driver of the car guarantee that he can always catch the pedestrian, or the pedestrian guarantee that he can indefinitely elude the car?\n\nThe problem is often used as an [[unclassified]] proxy for [[missile defence]] and other military targeting, allowing scientists to publish on it without security implications.\n\nThe problem was proposed by [[Rufus Isaacs (game theorist)|Rufus Isaacs]] in a 1951 report<ref>R. Isaacs, [http://www.rand.org/pubs/papers/P257.html Games of Pursuit], RAND Corporation (1951)</ref> for the [[RAND Corporation]], and in the book ''Differential Games''.<ref>R. Isaacs, ''Differential Games: A Mathematical Theory with Applications to Warfare and Pursuit, Control and Optimization'', John Wiley & Sons, New York (1965), PP 349&ndash;350.</ref>\n\nThe homicidal chauffeur problem is a classic example of a [[differential game]] played in [[continuous time]] in a continuous [[state space]]. The [[calculus of variations]] and [[level set]] methods can be used as a mathematical framework for investigating solutions of the problem. Although the problem is phrased as a recreational problem, it is an important [[model problem]] for mathematics used in a number of real-world applications.\n\nA discrete version of the problem was described by [[Martin Gardner]] (in his book ''Mathematical Carnival'', chapter 16), where a squad car of speed 2 chases a crook of speed 1 on a rectangular grid, where the squad car but not the crook is constrained not to make left-hand turns or U-turns.\n\n== See also ==\n* [[Variational calculus]]\n* [[Level-set method]]\n* [[Apollonius pursuit problem]]\n* Conway's [[Angel problem]], another mathematical game which pits a powerful and maneuverable adversary against a highly resourceful but less powerful foe\n* [[Princess and Monster game]]\n\n== References ==\n<references/>\n\n== External links ==\n* [http://home.imm.uran.ru/kumkov/HHCP/nice4.pdf History of the Homicidal Chauffeur Problem], presentation at the Colloquium dedicated to the 60th anniversary of Prof. Pierre Bernhard.\n* [http://www.springerlink.com/content/f466qu1820h00330/ Analytical study of a case of the homicidal chauffeur game problem]\n* [http://citeseer.ist.psu.edu/350554.html Homicidal Chauffeur Game. Computation of Level Sets of the Value Function]\n* [http://demonstrations.wolfram.com/TheHomicidalChauffeurProblem/ The Homicidal Chauffeur Problem]\n\n[[Category:Pursuit-evasion]]\n[[Category:Recreational mathematics]]\n[[Category:Calculus of variations]]\n[[Category:Game theory]]\n[[Category:Multivariable calculus]]"
    },
    {
      "title": "Hu–Washizu principle",
      "url": "https://en.wikipedia.org/wiki/Hu%E2%80%93Washizu_principle",
      "text": "In [[continuum mechanics]], and in particular in [[finite element analysis]], the '''Hu–Washizu principle''' is a [[variational principle]] which says that the action\n\n:<math>\\int_{V^e} \\left[ \\frac{1}{2} \\varepsilon^T C \\varepsilon - \\sigma^T \\varepsilon + \\sigma^T (\\nabla u) - \\bar{p}^T u \\right] dV - \\int_{S_\\sigma^e} \\bar{T}^T u\\ dS</math>\n\nis stationary, where <math>C</math> is the elastic [[stiffness tensor]]. The Hu–Washizu principle is used to develop mixed [[finite element method]]s.<ref>{{cite journal|last=Jihuan|first=He|date=June 1997|title=Equivalent theorem of Hellinger–Reissner and Hu–Washizu variational principles|journal=Journal of Shanghai University|publisher=Shanghai University Press|volume=1|issue=1|issn=1007-6417|url=http://www.springerlink.com/content/8r4812w302061273/|accessdate=2009-09-22}}</ref> The principle is named after [[Hu Haichang]] and {{ill|Kyūichirō Washizu|ja|鷲津久一郎}}.\n\n==References==\n{{reflist}}\n\n== Further reading ==\n* K. Washizu: ''Variational Methods in Elasticity & Plasticity'', Pergamon Press, New York, 3rd edition (1982)\n* O. C. Zienkiewicz, R. L. Taylor, J. Z. Zhu : ''The Finite Element Method: Its Basis and Fundamentals'', Butterworth–Heinemann, (2005).\n\n[[Category:Calculus of variations]]\n[[Category:Finite element method]]\n[[Category:Structural analysis]]\n[[Category:Principles]]\n[[Category:Continuum mechanics]]\n\n{{Applied-math-stub}}"
    },
    {
      "title": "Inverse problem for Lagrangian mechanics",
      "url": "https://en.wikipedia.org/wiki/Inverse_problem_for_Lagrangian_mechanics",
      "text": "In [[mathematics]], the '''inverse problem for Lagrangian mechanics''' is the problem of determining whether a given system of [[ordinary differential equation]]s can arise as the [[Euler&ndash;Lagrange equation]]s for some [[Lagrangian mechanics|Lagrangian]] function.\n\nThere has been a great deal of activity in the study of this problem since the early 20th century. A notable advance in this field was a 1941 paper by the [[United States|American]] [[mathematician]] [[Jesse Douglas]], in which he provided [[necessary and sufficient]] conditions for the problem to have a solution; these conditions are now known as the '''Helmholtz conditions''', after the [[Germany|German]] [[physicist]] [[Hermann von Helmholtz]].\n\n==Background and statement of the problem==\n\nThe usual set-up of [[Lagrangian mechanics]] on ''n''-[[dimension]]al [[Euclidean space]] '''R'''<sup>''n''</sup> is as follows. Consider a [[Differentiable function|differentiable]] [[path (topology)|path]] ''u''&nbsp;:&nbsp;[0,&nbsp;''T'']&nbsp;→&nbsp;'''R'''<sup>''n''</sup>. The '''action''' of the path ''u'', denoted ''S''(''u''), is given by\n:<math>S(u) = \\int_{0}^{T} L(t, u(t), \\dot{u}(t)) \\, \\mathrm{d} t,</math>\nwhere ''L'' is a function of time, position and [[velocity]] known as the '''Lagrangian'''. The [[principle of least action]] states that, given an initial state ''x''<sub>0</sub> and a final state ''x''<sub>1</sub> in '''R'''<sup>''n''</sup>, the trajectory that the system determined by ''L'' will actually follow must be a [[minimum|minimizer]] of the action [[functional (mathematics)|functional]] ''S'' satisfying the boundary conditions ''u''(0)&nbsp;=&nbsp;''x''<sub>0</sub>, ''u''(T)&nbsp;=&nbsp;''x''<sub>1</sub>. Furthermore, the [[critical point (mathematics)|critical point]]s (and hence minimizers) of ''S'' must satisfy the [[Euler&ndash;Lagrange equation]]s for ''S'':\n\n:<math>\\frac{\\mathrm{d}}{\\mathrm{d} t} \\frac{\\partial L}{\\partial \\dot{u}^{i}} - \\frac{\\partial L}{\\partial u^{i}} = 0 \\quad \\text{for } 1 \\leq i \\leq n,</math>\nwhere the upper indices ''i'' denote the components of ''u''&nbsp;=&nbsp;(''u''<sup>1</sup>,&nbsp;...,&nbsp;''u''<sup>''n''</sup>).\n\nIn the classical case\n:<math>T(\\dot{u}) = \\frac{1}{2} m | \\dot{u} |^{2},</math>\n:<math>V : [0, T] \\times \\mathbb{R}^{n} \\to \\mathbb{R},</math>\n:<math>L(t, u, \\dot{u}) = T(\\dot{u}) - V(t, u),</math>\nthe Euler&ndash;Lagrange equations are the second-order ordinary differential equations better known as [[Newton's laws of motion]]:\n:<math>m \\ddot{u}^{i} = - \\frac{\\partial V(t, u)}{\\partial u^{i}} \\quad \\text{for } 1 \\leq i \\leq n,</math>\n:<math>\\mbox{i.e. }m \\ddot{u} = - \\nabla_{u} V(t, u).</math>\n\nThe '''inverse problem of Lagrangian mechanics''' is as follows: given a system of second-order ordinary differential equations\n:<math>\\ddot{u}^{i} = f^{i} (u^{j}, \\dot{u}^{j}) \\quad \\text{for } 1 \\leq i, j \\leq n, \\quad \\mbox{(E)}</math>\nthat holds for times 0&nbsp;≤&nbsp;''t''&nbsp;≤&nbsp;''T'', does there exist a Lagrangian ''L''&nbsp;:&nbsp;[0,&nbsp;''T'']&nbsp;&times;&nbsp;'''R'''<sup>''n''</sup>&nbsp;&times;&nbsp;'''R'''<sup>''n''</sup>&nbsp;→&nbsp;'''R''' for which these ordinary differential equations (E) are the Euler&ndash;Lagrange equations? In general, this problem is posed not on Euclidean space '''R'''<sup>''n''</sup>, but on an ''n''-dimensional [[manifold]] ''M'', and the Lagrangian is a function ''L''&nbsp;:&nbsp;[0,&nbsp;''T'']&nbsp;&times;&nbsp;T''M''&nbsp;→&nbsp;'''R''', where T''M'' denotes the [[tangent bundle]] of ''M''.\n\n==Douglas' theorem and the Helmholtz conditions==\n\nTo simplify the notation, let\n\n:<math>v^{i} = \\dot{u}^{i}</math>\n\nand define a collection of ''n''<sup>2</sup> functions Φ<sub>''j''</sub><sup>''i''</sup> by\n\n:<math>\\Phi_{j}^{i} = \\frac{1}{2} \\frac{\\mathrm{d}}{\\mathrm{d} t} \\frac{\\partial f^{i}}{\\partial v^{j}} - \\frac{\\partial f^{i}}{\\partial u^{j}} - \\frac{1}{4} \\frac{\\partial f^{i}}{\\partial v^{k}} \\frac{\\partial f^{k}}{\\partial v^{j}}.</math>\n\n'''Theorem.''' (Douglas 1941) There exists a Lagrangian ''L''&nbsp;:&nbsp;[0,&nbsp;''T'']&nbsp;&times;&nbsp;T''M''&nbsp;→&nbsp;'''R''' such that the equations (E) are its Euler&ndash;Lagrange equations [[if and only if]] there exists a [[non-singular]] [[symmetric matrix]] ''g'' with entries ''g''<sub>''ij''</sub> depending on both ''u'' and ''v'' satisfying the following three '''Helmholtz conditions''':\n\n:<math>g \\Phi = (g \\Phi)^{\\top}, \\quad \\mbox{(H1)}</math>\n:<math>\\frac{\\mathrm{d} g_{ij}}{\\mathrm{d} t} + \\frac{1}{2} \\frac{\\partial f^{k}}{\\partial v^{i}} g_{kj} + \\frac{1}{2} \\frac{\\partial f^{k}}{\\partial v^{j}} g_{ki} = 0 \\mbox{ for } 1 \\leq i, j \\leq n, \\quad \\mbox{(H2)}</math>\n:<math>\\frac{\\partial g_{ij}}{\\partial v^{k}} = \\frac{\\partial g_{ik}}{\\partial v^{j}} \\mbox{ for } 1 \\leq i, j, k \\leq n. \\quad \\mbox{(H3)}</math>\n\n(The [[Einstein summation convention]] is in use for the repeated indices.)\n\n===Applying Douglas' theorem===\n\nAt first glance, solving the Helmholtz equations (H1)&ndash;(H3) seems to be an extremely difficult task. Condition (H1) is the easiest to solve: it is always possible to find a ''g'' that satisfies (H1), and it alone will not imply that the Lagrangian is singular. Equation (H2) is a system of ordinary differential equations: [[Picard&ndash;Lindelöf theorem|the usual theorems]] on the existence and uniqueness of solutions to ordinary differential equations imply that it is, ''in principle'', possible to solve (H2). Integration does not yield additional constants but instead first integrals of the system (E), so this step becomes difficult ''in practice'' unless (E) has enough explicit first integrals. In certain well-behaved cases (e.g. the [[geodesic flow]] for the [[canonical form|canonical]] [[connection (mathematics)|connection]] on a [[Lie group]]), this condition is satisfied.\n\nThe final and most difficult step is to solve equation (H3), called the ''closure conditions'' since (H3) is the condition that the [[differential form|differential 1-form]] ''g''<sub>''i''</sub> is a [[closed and exact differential forms|closed form]] for each ''i''. The reason why this is so daunting is that (H3) constitutes a large system of coupled partial differential equations: for ''n'' degrees of freedom, (H3) constitutes a system of\n\n:<math>2 \\left( \\begin{matrix} n + 1 \\\\ 3 \\end{matrix} \\right)</math>\n\npartial differential equations in the 2''n'' independent variables that are the components ''g''<sub>''ij''</sub> of ''g'', where\n\n:<math>\\left( \\begin{matrix} n \\\\ k \\end{matrix} \\right)</math>\n\ndenotes the [[binomial coefficient]]. In order to construct the most general possible Lagrangian, one must solve this huge system!\n\nFortunately, there are some auxiliary conditions that can be imposed in order to help in solving the Helmholtz conditions. First, (H1) is a purely algebraic condition on the unknown matrix ''g''. Auxiliary algebraic conditions on ''g'' can be given as follows: define functions\n\n:&Psi;<sub>''jk''</sub><sup>''i''</sup>\n\nby\n\n:<math>\\Psi_{jk}^{i} = \\frac{1}{3} \\left( \\frac{\\partial \\Phi_{j}^{i}}{\\partial v^{k}} - \\frac{\\partial \\Phi_{k}^{i}}{\\partial v^{j}} \\right).</math>\n\nThe auxiliary condition on ''g'' is then\n\n:<math>g_{mi} \\Psi_{jk}^{m} + g_{mk} \\Psi_{ij}^{m} + g_{mj} \\Psi_{ki}^{m} = 0 \\mbox{ for } 1 \\leq i, j \\leq n. \\quad \\mbox{(A)}</math>\n\nIn fact, the equations (H2) and (A) are just the first in an infinite hierarchy of similar algebraic conditions. In the case of a [[Parallelizable_manifold|parallel connection]] (such as the canonical connection on a Lie group), the higher order conditions are always satisfied, so only (H2) and (A) are of interest. Note that (A) comprises\n\n:<math>\\left( \\begin{matrix} n \\\\ 3 \\end{matrix} \\right)</math>\n\nconditions whereas (H1) comprises\n\n:<math>\\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)</math>\n\nconditions. Thus, it is possible that (H1) and (A) together imply that the Lagrangian function is singular. As of 2006, there is no general theorem to circumvent this difficulty in arbitrary dimension, although certain special cases have been resolved.\n\nA second avenue of attack is to see whether the system (E) admits a submersion onto a lower-dimensional system and to try to \"lift\" a Lagrangian for the lower-dimensional system up to the higher-dimensional one. This is not really an attempt to solve the Helmholtz conditions so much as it is an attempt to construct a Lagrangian and then show that its Euler&ndash;Lagrange equations are indeed the system (E).\n\n==References==\n\n* {{cite journal | last=Douglas | first=Jesse | title=Solution of the inverse problem in the calculus of variations | journal=Transactions of the American Mathematical Society | volume=50 | pages=71&ndash;128 | year=1941 | issn=0002-9947 | doi=10.2307/1989912 | jstor=1989912 | issue=1 | publisher=Transactions of the American Mathematical Society, Vol. 50, No. 1 | pmc=1077987 }}\n* {{cite journal | author=Rawashdeh, M., &amp; Thompson, G. | title=The inverse problem for six-dimensional codimension two nilradical Lie algebras | journal=Journal of Mathematical Physics | volume=47 | issue=11 | year=2006 | issn=0022-2488 | doi=10.1063/1.2378620 | pages=112901 }}\n\n[[Category:Calculus of variations]]\n[[Category:Lagrangian mechanics]]\n[[Category:Inverse problems]]"
    },
    {
      "title": "Lagrange multipliers on Banach spaces",
      "url": "https://en.wikipedia.org/wiki/Lagrange_multipliers_on_Banach_spaces",
      "text": "In the field of [[calculus of variations]] in [[mathematics]], the method of '''Lagrange multipliers on Banach spaces''' can be used to solve certain infinite-dimensional [[constraint (mathematics)|constrained]] [[optimization (mathematics)|optimization problems]]. The method is a generalization of the classical method of [[Lagrange multipliers]] as used to find [[extremum|extrema]] of a [[function (mathematics)|function]] of finitely many variables.\n\n==The Lagrange multiplier theorem for Banach spaces==\nLet ''X'' and ''Y'' be [[real number|real]] [[Banach space]]s. Let ''U'' be an [[open set|open subset]] of ''X'' and let ''f'' : ''U'' → '''R''' be a continuously [[differentiable function]]. Let ''g'' : ''U'' → ''Y'' be another continuously differentiable function, the ''constraint'': the objective is to find the extremal points (maxima or minima) of ''f'' subject to the constraint that ''g'' is zero.\n\nSuppose that ''u''<sub>0</sub> is a ''constrained extremum'' of ''f'', i.e. an extremum of ''f'' on\n\n:<math>g^{-1} (0) = \\{ x \\in U \\mid g(x) = 0 \\in Y \\} \\subseteq U.</math>\n\nSuppose also that the [[Fréchet derivative]] D''g''(''u''<sub>0</sub>) : ''X'' → ''Y'' of ''g'' at ''u''<sub>0</sub> is a [[surjective]] [[linear map]]. Then there exists a '''Lagrange multiplier''' ''λ'' : ''Y'' → '''R''' in ''Y''<sup>∗</sup>, the [[dual space]] to ''Y'', such that\n\n:<math>\\mathrm{D} f (u_{0}) = \\lambda \\circ \\mathrm{D} g (u_{0}). \\quad \\mbox{(L)}</math>\n\nSince D''f''(''u''<sub>0</sub>) is an element of the dual space ''X''<sup>∗</sup>, equation (L) can also be written as\n\n:<math>\\mathrm{D} f (u_{0}) = \\left( \\mathrm{D} g (u_{0}) \\right)^{*} (\\lambda),</math>\n\nwhere (D''g''(''u''<sub>0</sub>))<sup>∗</sup>(''λ'') is the [[pullback]] of ''λ'' by D''g''(''u''<sub>0</sub>), i.e. the action of the [[adjoint]] map (D''g''(''u''<sub>0</sub>))<sup>∗</sup> on ''λ'', as defined by\n\n:<math>\\left( \\mathrm{D} g (u_{0}) \\right)^{*} (\\lambda) = \\lambda \\circ \\mathrm{D} g (u_{0}).</math>\n\n==Connection to the finite-dimensional case==\nIn the case that ''X'' and ''Y'' are both finite-dimensional (i.e. [[linear isomorphism|linearly isomorphic]] to '''R'''<sup>''m''</sup> and '''R'''<sup>''n''</sup> for some [[natural numbers]] ''m'' and ''n'') then writing out equation (L) in [[matrix (mathematics)|matrix]] form shows that ''λ'' is the usual Lagrange multiplier vector; in the case ''n'' = 1, ''λ'' is the usual Lagrange multiplier, a real number.\n\n==Application==\nIn many optimization problems, one seeks to minimize a functional defined on an infinite-dimensional space such as a Banach space.\n\nConsider, for example, the [[Sobolev space]] ''X'' = ''H''<sub>0</sub><sup>1</sup>([&minus;1, +1]; '''R''') and the functional ''f'' : ''X'' → '''R''' given by\n\n:<math>f(u) = \\int_{-1}^{+1} u'(x)^{2} \\, \\mathrm{d} x.</math>\n\nWithout any constraint, the minimum value of ''f'' would be 0, attained by ''u''<sub>0</sub>(''x'') = 0 for all ''x'' between &minus;1 and +1. One could also consider the constrained optimization problem, to minimize ''f'' among all those ''u'' ∈ ''X'' such that the mean value of ''u'' is +1. In terms of the above theorem, the constraint ''g'' would be given by\n\n:<math>g(u) = \\frac{1}{2} \\int_{-1}^{+1} u(x) \\, \\mathrm{d} x - 1.</math>\n\nHowever this problem can be solved as in the finite dimensional case since the Lagrange multiplier <math> \\lambda </math> is only a scalar.\n\n==See also==\n* [[Pontryagin's minimum principle]], Hamiltonian method in calculus of variations\n\n==References==\n*{{cite book |first=David G. |last=Luenberger |authorlink=David Luenberger |title=Optimization by Vector Space Methods |location=New York |publisher=John Wiley & Sons |year=1969 |isbn=0-471-55359-X |chapter=Local Theory of Constrained Optimization |pages=239–270 }}\n* {{cite book | title=Applied functional analysis: Variational Methods and Optimization | last=Zeidler | first=Eberhard | publisher=Springer-Verlag | year=1995 | isbn=978-1-4612-9529-7 | series=Applied Mathematical Sciences 109 | location=New York, NY }} (See Section 4.14, pp.270&ndash;271.)\n\n{{PlanetMath attribution| urlname = LagrangeMultipliersOnBanachSpaces |title=Lagrange multipliers on Banach spaces}}\n\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Lagrangian (field theory)",
      "url": "https://en.wikipedia.org/wiki/Lagrangian_%28field_theory%29",
      "text": "{{Use American English|date = February 2019}}\n{{Short description|Application of Lagrangian mechanics to field theories}}\n'''Lagrangian field theory''' is a formalism in [[classical field theory]]. It is the field-theoretic analogue of [[Lagrangian mechanics]]. Lagrangian mechanics is used for discrete particles each with a finite number of [[degrees of freedom (physics and chemistry)|degrees of freedom]]. Lagrangian field theory applies to continua and fields, which have an infinite number of degrees of freedom.\n\nThis article uses <math>\\mathcal{L}</math> for the Lagrangian density, and ''L'' for the Lagrangian. \n\nThe Lagrangian mechanics formalism was generalized further to handle [[classical field theory|field theory]]. In field theory, the independent variable is replaced by an event in [[spacetime]] (''x'', ''y'', ''z'', ''t''), or more generally still by a point ''s'' on a [[manifold]]. The dependent variables (''q'') are replaced by the value of a field at that point in spacetime <math>\\varphi (x, y, z, t)</math> so that the [[equations of motion]] are obtained by means of an [[action (physics)|action]] principle, written as:\n\n:<math>\\frac{\\delta \\mathcal{S}}{\\delta \\varphi_i} = 0,\\,</math>\n\nwhere the ''action'', <math>\\mathcal{S}</math>, is a [[functional (mathematics)|functional]] of the dependent variables <math>\\varphi_i (s) </math> with their derivatives and ''s'' itself\n\n:<math>\\mathcal{S}\\left[\\varphi_i\\right] \n= \n\\int{ \\mathcal{L} \\left(\\varphi_i (s), \n\\left\\{ \\frac{\\partial\\varphi_i(s)}{\\partial s^\\alpha} \\right\\}, \n\\{ s^\\alpha \\} \\right) \\, \\mathrm{d}^n s }</math>,\n\nwhere the brackets denote <math>\\{\\cdot~\\forall\\alpha\\}</math>;\nand ''s'' = {''s<sup>α</sup>''} denotes the [[Set (mathematics)|set]] of ''n'' [[independent variable]]s of the system, including the time variable, and is indexed by ''α'' = 1, 2, 3,..., ''n''. Notice that the calligraphic typeface, <math>\\mathcal{L}</math>, is used to denote volume density, where volume is the integral measure of the domain of the field function, i.e. <math>\\mathrm{d}^n s</math>.\n\n==Definitions==\n\nIn Lagrangian field theory, the Lagrangian as a function of [[generalized coordinates]] is replaced by a Lagrangian density, a function of the fields in the system and their derivatives, and possibly the space and time coordinates themselves. In field theory, the independent variable ''t'' is replaced by an event in spacetime (''x'', ''y'', ''z'', ''t'') or still more generally by a point ''s'' on a manifold. \n\nOften, a \"Lagrangian density\" is simply referred to as a \"Lagrangian\".\n\n===Scalar fields===\n\nFor one scalar field <math>\\varphi</math>, the Lagrangian density will take the form:<ref group=nb>It is a standard abuse of notation to abbreviate all the derivatives and coordinates in the Lagrangian density as follows:\n:<math>\\mathcal{L} (\\varphi, \\partial_\\mu \\varphi, x_\\mu)</math>\nsee [[four-gradient]]. The {{math|''μ''}} is an index which takes values 0 (for the time coordinate), and 1, 2, 3 (for the spatial coordinates), so strictly only one derivative or coordinate would be present. In general, all the spatial and time derivatives will appear in the Lagrangian density, for example in Cartesian coordinates, the Lagrangian density has the full form:\n:<math>\\mathcal{L} \\left(\\varphi, \\frac{\\partial \\varphi}{\\partial x}, \\frac{\\partial \\varphi}{\\partial y}, \\frac{\\partial \\varphi}{\\partial z}, \\frac{\\partial \\varphi}{\\partial t}, x,y,z,t\\right)</math>\nHere we write the same thing, but using ∇ to abbreviate all spatial derivatives as a vector.</ref><ref name=Mandl>{{cite book |last=Mandl |first=F. |last2=Shaw |first2=G. |title=Quantum Field Theory |chapter=Lagrangian Field Theory |location= |publisher=Wiley |edition=2nd |year=2010 |isbn=978-0-471-49684-7 |page=25–38 }}</ref> \n\n:<math>\\mathcal{L}(\\varphi, \\nabla\\varphi, \\partial \\varphi/\\partial t , \\mathbf{x},t)</math>\n\nFor many scalar fields\n\n:<math>\\mathcal{L}(\\varphi_1, \\nabla\\varphi_1, \\partial \\varphi_1/\\partial t ,\\ldots,\\varphi_2, \\nabla\\varphi_2, \\partial \\varphi_2/\\partial t ,\\ldots, \\mathbf{x},t)</math>\n\n===Vector fields, tensor fields, spinor fields===\n\nThe above can be generalized for [[vector field]]s, [[tensor field]]s, and [[spinor field]]s. In physics, [[fermion]]s are described by spinor fields. [[Boson]]s are described by tensor fields, which include scalar and vector fields as special cases.\n\n===Action===\n\nThe [[time integral]] of the Lagrangian is called the [[action (physics)|action]] denoted by ''S''. In field theory, a distinction is occasionally made between the '''Lagrangian''' ''L'', of which the time integral is the action\n\n:<math>\\mathcal{S} = \\int L \\, \\mathrm{d}t \\,,</math>\n\nand the '''Lagrangian density''' <math>\\mathcal{L}</math>, which one integrates over all [[spacetime]] to get the action:\n\n:<math>\\mathcal{S} [\\varphi] = \\int \\mathcal{L} (\\varphi,\\nabla\\varphi,\\partial\\varphi/\\partial t , \\mathbf{x},t) \\, \\mathrm{d}^3 \\mathbf{x} \\, \\mathrm{d}t .</math>\n\nThe spatial [[volume integral]] of the Lagrangian density is the Lagrangian, in 3d\n\n:<math>L = \\int \\mathcal{L} \\, \\mathrm{d}^3 \\mathbf{x} \\,.</math>\n\nNote, in the presence of gravity or when using general curvilinear coordinates, the Lagrangian density <math>\\mathcal{L}</math> will include a factor of {{math|{{sqrt|''g''}}}}, making it a [[tensor density|scalar density]]. This procedure ensures that the action <math>\\mathcal{S}</math> is invariant under general coordinate transformations.\n\n===Mathematical formalism===\n\nSuppose we have an ''n''-dimensional [[manifold]], ''M'', and a target manifold, ''T''. Let <math>\\mathcal{C}</math> be the configuration space of [[smooth function]]s from ''M'' to ''T''.\n\nIn field theory, ''M'' is the [[spacetime]] manifold and the target space is the set of values the fields can take at any given point. For example, if there are <math>m</math> [[real number|real]]-valued [[scalar field]]s, <math>\\varphi_1, \\dots, \\varphi_m</math>, then the target manifold is <math>\\mathbb{R}^m</math>. If the field is a real [[vector field]], then the target manifold is [[isomorphic]] to <math>\\mathbb{R}^n</math>. Note that there is also an elegant formalism{{which|date=September 2016}} for this, using [[tangent bundle]]s over ''M''.\n\nConsider a [[Functional (mathematics)|functional]], \n:<math>\\mathcal{S}:\\mathcal{C}\\rightarrow \\mathbb{R}</math>,\ncalled the [[Action (physics)|action]].\n\nIn order for the action to be [[Functional (mathematics)#Locality|local]], we need additional restrictions on the [[action (physics)|action]]. If <math>\\varphi\\ \\in\\ \\mathcal{C}</math>, we assume <math>\\mathcal{S}[\\varphi]</math> is the [[integral]] over ''M'' of a function of <math>\\varphi</math>, its [[derivative]]s and the position called the '''Lagrangian''', <math>\\mathcal{L}(\\varphi,\\partial\\varphi,\\partial\\partial\\varphi, ...,x)</math>. In other words,\n\n:<math>\\forall\\varphi\\in\\mathcal{C}, \\ \\ \\mathcal{S}[\\varphi]\\equiv\\int_M \\mathcal{L} \\big( \\varphi(x),\\partial\\varphi(x),\\partial\\partial\\varphi(x), ...,x \\big) \\mathrm{d}^nx .</math>\n\nIt is assumed below, in addition, that the Lagrangian depends on only the field value and its first derivative but not the higher derivatives.\n\nGiven [[boundary condition]]s, basically a specification of the value of <math>\\varphi</math> at the [[Boundary (topology)|boundary]] if ''M'' is [[Compact space|compact]] or some limit on <math>\\varphi</math> as ''x'' → ∞ (this will help in doing [[integration by parts]]), the [[subspace topology|subspace]] of <math>\\mathcal{C}</math> consisting of functions, <math>\\varphi</math>, such that all [[functional derivative]]s of ''S'' at <math>\\varphi</math> are zero and <math>\\varphi</math> satisfies the given boundary conditions is the subspace of [[on shell]] solutions.\n\nFrom this we get: \n\n:<math>0 = \\frac{\\delta\\mathcal{S}}{\\delta\\varphi} = \\int_M \\left(-\\partial_\\mu\n \\left(\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\varphi)}\\right)+ \\frac{\\partial\\mathcal{L}}{\\partial\\varphi}\\right) \\mathrm{d}^nx .</math>\n\nThe left hand side is the [[functional derivative]] of the [[action (physics)|action]] with respect to <math>\\varphi</math>.\n\nHence we get the [[Euler–Lagrange equations]] (due to the [[boundary condition]]s):\n\n:<math>\\frac{\\partial\\mathcal{L}}{\\partial\\varphi}=\\partial_\\mu\n \\left(\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\varphi)}\\right) .</math>\n\n==Examples==\n\nTo go with the section on test particles above, here are the equations for the fields in which they move. The equations below pertain to the fields in which the test particles described above move and allow the calculation of those fields. The equations below will not give you the equations of motion of a test particle in the field but will instead give you the potential (field) induced by quantities such as mass or charge density at any point <math>(\\mathbf{x},t)</math>. For example, in the case of Newtonian gravity, the Lagrangian density integrated over spacetime gives you an equation which, if solved, would yield <math>\\Phi (\\mathbf{x},t)</math>. This <math>\\Phi (\\mathbf{x},t)</math>, when substituted back in equation ({{EquationNote|1}}), the Lagrangian equation for the test particle in a Newtonian gravitational field, provides the information needed to calculate the acceleration of the particle.\n\n===Newtonian gravity===\n\nThe density <math>\\mathcal{L}</math> has units of J·m<sup>−3</sup>. The interaction term ''mΦ'' is replaced by a term involving a continuous mass density ''ρ'' in kg·m<sup>−3</sup>. This is necessary because using a point source for a field would result in mathematical difficulties. The resulting Lagrangian for the classical gravitational field is:\n\n:<math>\\mathcal{L}(\\mathbf{x},t)= - \\rho (\\mathbf{x},t) \\Phi (\\mathbf{x},t) - {1 \\over 8 \\pi G} (\\nabla \\Phi (\\mathbf{x},t))^2 </math>\n\nwhere ''G'' in m<sup>3</sup>·kg<sup>−1</sup>·s<sup>−2</sup> is the [[gravitational constant]]. Variation of the integral with respect to ''Φ'' gives:\n\n:<math>\\delta \\mathcal{L}(\\mathbf{x},t) = - \\rho (\\mathbf{x},t) \\delta\\Phi (\\mathbf{x},t) - {2 \\over 8 \\pi G} (\\nabla \\Phi (\\mathbf{x},t)) \\cdot (\\nabla \\delta\\Phi (\\mathbf{x},t)) .</math>\n\nIntegrate by parts and discard the total integral. Then divide out by ''δΦ'' to get:\n\n:<math>0 = - \\rho (\\mathbf{x},t) + {1 \\over 4 \\pi G} \\nabla \\cdot \\nabla \\Phi (\\mathbf{x},t) </math>\n\nand thus\n\n:<math>4 \\pi G \\rho (\\mathbf{x},t) = \\nabla^2 \\Phi (\\mathbf{x},t) </math>\n\nwhich yields [[Gauss's law for gravity]].\n\n===Einstein gravity===\n{{main|Einstein–Hilbert action}}\nThe Lagrange density for general relativity in the presence of matter fields is\n:<math>\\mathcal{L}_\\text{GR}=\\mathcal{L}_\\text{EH}+\\mathcal{L}_\\text{matter}=\\frac{c^4}{16\\pi G}\\left(R-2\\Lambda\\right)+\\mathcal{L}_\\text{matter}</math>\n<math>R</math> is the [[curvature scalar]], which is the [[Ricci tensor]] contracted with the [[metric tensor]], and the [[Ricci tensor]] is the [[Riemann tensor]] contracted with a [[Kronecker delta]]. The integral of <math> \\mathcal{L}_\\text{EH}</math> is known as the [[Einstein-Hilbert action]]. The Riemann tensor is the [[tidal force]] tensor, and is constructed out of [[Christoffel symbols]] and derivatives of Christoffel symbols, which are the gravitational force field. Plugging this Lagrangian into the Euler-Lagrange equation and taking the metric tensor <math> g_{\\mu\\nu}</math> as the field, we obtain the [[Einstein field equations]] \n:<math> R_{\\mu\\nu}-\\frac{1}{2}Rg_{\\mu\\nu}+g_{\\mu\\nu}\\Lambda=\\frac{8\\pi G}{c^4}T_{\\mu\\nu} </math>\nThe last tensor is the [[energy momentum tensor]] and is defined by\n:<math>T_{\\mu\\nu} \\equiv \\frac{-2}{\\sqrt{-g}}\\frac{\\delta (\\mathcal{L}_{\\mathrm{matter}} \\sqrt{-g}) }{\\delta g^{\\mu\\nu}} = -2 \\frac{\\delta \\mathcal{L}_\\mathrm{matter}}{\\delta g^{\\mu\\nu}} + g_{\\mu\\nu} \\mathcal{L}_\\mathrm{matter}.</math>\n<math>g</math> is the determinant of the metric tensor when regarded as a matrix. <math>\\Lambda</math> is the [[cosmological constant]]. Generally, in general relativity, the integration measure of the action of Lagrange density is <math>\\sqrt{-g}d^4x </math>. This makes the integral coordinate independent, as the root of the metric determinant is equivalent to the [[Jacobian determinant]]. The minus sign is a consequence of the metric signature (the determinant by itself is negative).<ref>{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587|pages=344–390}}</ref>\n\n===Electromagnetism in special relativity===\n{{main|Covariant formulation of classical electromagnetism}}\n\nThe interaction terms \n:<math>- q \\phi (\\mathbf{x}(t),t) + q \\dot{\\mathbf{x}}(t) \\cdot \\mathbf{A} (\\mathbf{x}(t),t)</math>\nare replaced by terms involving a continuous charge density ρ in A·s·m<sup>−3</sup> and current density <math>\\mathbf{j}</math> in A·m<sup>−2</sup>. The resulting Lagrangian for the electromagnetic field is:\n\n:<math>\\mathcal{L}(\\mathbf{x},t) = - \\rho (\\mathbf{x},t) \\phi (\\mathbf{x},t) + \\mathbf{j} (\\mathbf{x},t) \\cdot \\mathbf{A} (\\mathbf{x},t) + {\\epsilon_0 \\over 2} {E}^2 (\\mathbf{x},t) - {1 \\over {2 \\mu_0}} {B}^2 (\\mathbf{x},t) .</math>\n\nVarying this with respect to ϕ, we get\n\n:<math>0 = - \\rho (\\mathbf{x},t) + \\epsilon_0 \\nabla \\cdot \\mathbf{E} (\\mathbf{x},t) </math>\n\nwhich yields [[Gauss' law]].\n\nVarying instead with respect to <math>\\mathbf{A}</math>, we get\n\n:<math>0 = \\mathbf{j} (\\mathbf{x},t) + \\epsilon_0 \\dot{\\mathbf{E}} (\\mathbf{x},t) - {1 \\over \\mu_0} \\nabla   \\times \\mathbf{B} (\\mathbf{x},t) </math>\n\nwhich yields [[Ampère's law]].\n\nUsing [[tensor notation]], we can write all this more compactly. The term <math> - \\rho \\phi (\\mathbf{x},t) + \\mathbf{j} \\cdot \\mathbf{A}    </math> is actually the inner product of two [[four-vector]]s. We package the charge density into the current 4-vector and the potential into the potential 4-vector. These two new vectors are\n:<math> j^\\mu = (\\rho,\\mathbf{j})\\quad\\text{and}\\quad A_\\mu = (-\\phi,\\mathbf{A}) </math>\nWe can then write the interaction term as\n:<math> - \\rho \\phi  + \\mathbf{j} \\cdot \\mathbf{A}  = j^\\mu A_\\mu </math>\nAdditionally, we can package the E and B fields into what is known as the [[electromagnetic tensor]] <math> F_{\\mu\\nu} </math>.\nWe define this tensor as\n:<math>  F_{\\mu\\nu}=\\partial_\\mu A_\\nu-\\partial_\\nu A_\\mu </math>\nThe term we are looking out for turns out to be\n:<math> {\\epsilon_0 \\over 2} {E}^2 - {1 \\over {2 \\mu_0}} {B}^2 =  -\\frac{1}{4\\mu_0} F_{\\mu\\nu}F^{\\mu\\nu}= -\\frac{1}{4\\mu_0} F_{\\mu\\nu} F_{\\rho\\sigma}\\eta^{\\mu\\rho}\\eta^{\\nu\\sigma}</math>\nWe have made use of the [[Minkowski metric]] to raise the indices on the EMF tensor. In this notation, Maxwell's equations are\n:<math> \\partial_\\mu F^{\\mu\\nu}=-\\mu_0 j^\\nu\\quad\\text{and}\\quad \\epsilon^{\\mu\\nu\\lambda\\sigma}\\partial_\\nu F_{\\lambda\\sigma}=0 </math>\nwhere ε is the [[Levi-Civita tensor]]. So the Lagrange density for electromagnetism in special relativity written in terms of Lorentz vectors and tensors is\n:<math> \\mathcal{L}(x) = j^\\mu(x) A_\\mu(x) - \\frac{1}{4\\mu_0} F_{\\mu\\nu}(x) F^{\\mu\\nu}(x) </math>\nIn this notation it is apparent that classical electromagnetism is a Lorentz-invariant theory. By the [[equivalence principle]], it becomes simple to extend the notion of electromagnetism to curved spacetime.<ref>{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587|pages=244–253}}</ref><ref>{{cite book|last1=Mexico|first1=Kevin Cahill, University of New|title=Physical mathematics|date=2013|publisher=Cambridge University Press|location=Cambridge|isbn=9781107005211|edition=Repr.}}</ref>\n\n===Electromagnetism in general relativity===\n{{main|Maxwell's equations in curved spacetime}}\nThe Lagrange density of electromagnetism in general relativity also contains the Einstein-Hilbert action from above. The pure electromagnetic Lagrangian is precisely a matter Lagrangian <math> \\mathcal{L}_\\text{matter}</math>. The Lagrangian is\n\n:<math>\\begin{align}\\mathcal{L}(x) &= j^\\mu (x) A_\\mu (x) \n - {1 \\over 4\\mu_0} F_{\\mu \\nu}(x) F_{\\rho\\sigma}(x) g^{\\mu\\rho}(x) g^{\\nu\\sigma}(x) + \\frac{c^4}{16\\pi G}R(x)\\\\ &= \\mathcal{L}_\\text{Maxwell} + \\mathcal{L}_\\text{Einstein-Hilbert}.\\end{align}</math>\n\nThis Lagrangian is obtained by simply replacing the Minkowski metric in the above flat Lagrangian with a more general (possibly curved) metric <math> g_{\\mu\\nu}(x)</math>. We can generate the Einstein Field Equations in the presence of an EM field using this lagrangian. The energy-momentum tensor is\n:<math>  T^{\\mu\\nu}(x)=\\frac{2}{\\sqrt{-g(x)}}\\frac{\\delta}{\\delta g_{\\mu\\nu}(x)}\\mathcal{S}_\\text{Maxwell}=\\frac{1}{\\mu_{0}}\\left(F^{\\mu}_{\\text{ }\\lambda}(x)F^{\\nu\\lambda}(x)-\\frac{1}{4}g^{\\mu\\nu}(x)F_{\\rho\\sigma}(x)F^{\\rho\\sigma}(x)\\right) </math>\nIt can be shown that this energy momentum tensor is traceless, i.e. that\n:<math>  T=g_{\\mu\\nu}T^{\\mu\\nu}=0 </math>\nIf we take the trace of both sides of the Einstein Field Equations, we obtain\n:<math>  R=-\\frac{8\\pi G}{c^4}T </math>\nSo the tracelessness of the energy momentum tensor implies that the curvature scalar in an electromagnetic field vanishes. The Einstein equations are then\n:<math> R^{\\mu\\nu}=\\frac{8\\pi G}{c^4}\\frac{1}{\\mu_{0}}\\left(F^{\\mu}_{\\text{ }\\lambda}(x)F^{\\nu\\lambda}(x)-\\frac{1}{4}g^{\\mu\\nu}(x)F_{\\rho\\sigma}(x)F^{\\rho\\sigma}(x)\\right) </math>\nAdditionally, Maxwell's equations are\n:<math> D_{\\mu}F^{\\mu\\nu}=-\\mu_0 j^\\nu  </math>\nwhere <math>D_\\mu</math> is the [[covariant derivative]]. For free space, we can set the current tensor equal to zero, <math> j^\\mu=0  </math>. Solving both Einstein and Maxwell's equations around a spherically symmetric mass distribution in free space leads to the [[Reissner–Nordström black hole|Reissner–Nordström charged black hole]], with the defining line element (written in [[natural units]] and with charge Q):<ref>{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587|pages=381–383, 477–478}}</ref>\n\n:<math> \\mathrm{d}s^2=\\left(1-\\frac{2M}{r}+\\frac{Q^2}{r^2}\\right)\\mathrm{d}t^2- \\left(1-\\frac{2M}{r}+\\frac{Q^2}{r^2}\\right)^{-1}\\mathrm{d}r^2 -r^2\\mathrm{d}\\Omega^2</math>\n\nOne possible way of unifying the electromagnetic and gravitational Lagrangians (by using a fifth dimension) is given by [[Kaluza-Klein theory]].\n\n===Electromagnetism using differential forms===\nUsing [[differential forms]], the electromagnetic action ''S'' in vacuum on a (pseudo-) Riemannian manifold <math>\\mathcal M</math> can be written (using [[natural units]], {{nowrap|1=''c'' = ''ε''<sub>0</sub> = 1}}) as\n:<math>\\mathcal S[\\mathbf{A}] = -\\int_{\\mathcal{M}} \\left(\\frac{1}{2}\\,\\mathbf{F} \\wedge \\star\\mathbf{F} + \\mathbf{A} \\wedge\\star \\mathbf{J}\\right) .</math>\nHere, '''A''' stands for the electromagnetic potential 1-form, '''J''' is the current 1-form, '''F''' is the field strength 2-form and the star denotes the [[Hodge star]] operator. This is exactly the same Lagrangian as in the section above, except that the treatment here is coordinate-free; expanding the integrand into a basis yields the identical, lengthy expression. Note that with forms, an additional integration measure is not necessary because forms have coordinate differentials built in. Variation of the action leads to\n:<math>\\mathrm{d} {\\star}\\mathbf{F} = {\\star}\\mathbf{J} .</math>\nThese are Maxwell's equations for the electromagnetic potential. Substituting {{nowrap|1='''F''' = d'''A'''}} immediately yields the equation for the fields,\n:<math>\\mathrm{d}\\mathbf{F} = 0</math>\nbecause '''F''' is an [[exact form]].\n\n===Dirac Lagrangian===\nThe Lagrangian density for a [[Dirac field]] is:<ref>Itzykson-Zuber, eq. 3-152</ref>\n\n:<math>\\mathcal{L} = i \\hbar c \\bar \\psi {\\partial}\\!\\!\\!/\\ \\psi - mc^2 \\bar\\psi \\psi</math>\n\nwhere ''ψ'' is a [[Dirac spinor]] ([[annihilation operator]]), <math>\\bar \\psi = \\psi^\\dagger \\gamma^0</math> is its [[Dirac adjoint]] ([[creation operator]]), and <math>{\\partial}\\!\\!\\!/</math> is [[Feynman slash notation]] for <math>\\gamma^\\sigma \\partial_\\sigma\\!</math>.\n\n===Quantum electrodynamic Lagrangian===\nThe Lagrangian density for [[Quantum electrodynamics|QED]] is:\n\n:<math>\\mathcal{L}_{\\mathrm{QED}} = i\\hbar c \\bar \\psi {D}\\!\\!\\!\\!/\\ \\psi - mc^2 \\bar\\psi \\psi - {1 \\over 4\\mu_0} F_{\\mu \\nu} F^{\\mu \\nu}</math>\n\nwhere <math>F^{\\mu \\nu}\\!</math> is the [[electromagnetic tensor]], ''D'' is the [[gauge covariant derivative]], and <math>{D}\\!\\!\\!\\!/</math> is [[Feynman slash notation|Feynman notation]] for <math>\\gamma^\\sigma D_\\sigma\\!</math> with <math> D_\\sigma = \\partial_\\sigma - i e A_\\sigma </math> where <math>A_\\sigma</math> is the [[electromagnetic four-potential]].\n\n===Quantum chromodynamic Lagrangian===\nThe Lagrangian density for [[quantum chromodynamics]] is:<ref>{{cite web|url=http://www.fuw.edu.pl/~dobaczew/maub-42w/node9.html|title=Quantum Chromodynamics (QCD)|author=|date=|website=www.fuw.edu.pl|accessdate=12 April 2018}}</ref><ref>{{cite web |first=E. R. |last=Hilf |title=Semiclassical QCD-Lagrangian for Nuclear Physics |url=http://smallsystems.isn-oldenburg.de/Docs/THEO3/publications/semiclassical.qcd.prep.pdf }}</ref><ref>{{cite web |first=Volker |last=Sluka |date=January 10, 2005 |title=Talk |url=http://www-zeus.physik.uni-bonn.de/~brock/teaching/jets_ws0405/seminar09/sluka_quark_gluon_jets.pdf |archivedate=June 26, 2007 |archiveurl=https://web.archive.org/web/20070626211526/http://www-zeus.physik.uni-bonn.de/~brock/teaching/jets_ws0405/seminar09/sluka_quark_gluon_jets.pdf }}</ref>\n\n:<math>\\mathcal{L}_{\\mathrm{QCD}} = \\sum_n \\left ( i\\hbar c\\bar\\psi_n{D}\\!\\!\\!\\!/\\ \\psi_n - m_n c^2 \\bar\\psi_n \\psi_n \\right) - {1\\over 4} G^\\alpha {}_{\\mu\\nu} G_\\alpha {}^{\\mu\\nu}</math>\n\nwhere ''D'' is the QCD [[gauge covariant derivative#Quantum chromodynamics|gauge covariant derivative]], \n''n'' = 1, 2, ...6 counts the quark types, and <math>G^\\alpha {}_{\\mu\\nu}\\!</math> is the [[gluon field strength tensor]].\n\n==See also==\n{{colbegin}}\n*[[Calculus of variations]]\n*[[Covariant classical field theory]]\n*[[Einstein–Maxwell–Dirac equations]]\n*[[Euler–Lagrange equation]]\n*[[Functional derivative]]\n*[[Functional integral]]\n*[[Generalized coordinates]]\n*[[Hamiltonian mechanics]]\n*[[Hamiltonian field theory]]\n*[[Kinetic term]]\n*[[Lagrangian and Eulerian coordinates]]\n*[[Lagrangian mechanics]]\n*[[Lagrangian point]]\n*[[Lagrangian system]]\n*[[Noether's theorem]]\n*[[Onsager–Machlup function]]\n*[[Principle of least action]]\n*[[Scalar field theory]]\n{{colend}}\n\n==Notes==\n{{reflist|group=nb}}\n\n==Citations==\n{{reflist|2}}\n\n[[Category:Theoretical physics]]\n[[Category:Mathematical physics]]\n[[Category:Classical field theory]]\n[[Category:Calculus of variations]]\n[[Category:Quantum field theory]]"
    },
    {
      "title": "Lagrangian system",
      "url": "https://en.wikipedia.org/wiki/Lagrangian_system",
      "text": "{{more footnotes|date=September 2015}}\nIn mathematics, a '''Lagrangian system''' is a pair {{math|(''Y'', ''L'')}}, consisting of a smooth [[fiber bundle]] {{math|''Y'' → ''X''}} and a Lagrangian density {{math|''L''}}, which yields the Euler–Lagrange [[differential operator]] acting on sections of {{math|''Y'' → ''X}}.\n\nIn [[classical mechanics]], many [[dynamical system]]s are Lagrangian systems. The configuration space of such a Lagrangian system is a fiber bundle {{math|''Q'' → ℝ}} over the time axis {{math|ℝ}}. In particular, {{math|''Q'' {{=}} ℝ × ''M''}} if a reference frame is fixed. In [[classical field theory]], all field systems are the Lagrangian ones.\n\n== Lagrangians and Euler–Lagrange operators ==\nA '''Lagrangian density''' {{math|''L''}} (or, simply, a [[Lagrangian (field theory)|Lagrangian]]) of order {{math|''r''}} is defined as an [[exterior form|{{math|''n''}}-form]], {{math|1=''n'' = dim ''X''}}, on the {{math|''r''}}-order [[jet bundle|jet manifold]] {{math|''J''<sup>''r''</sup>''Y''}} of {{math|''Y''}}.\n\nA Lagrangian {{math|''L''}} can be introduced as an element of the [[variational bicomplex]] of the [[differential graded algebra]] {{math|''O''<sup>∗</sup><sub>∞</sub>(''Y'')}} of [[differential form|exterior forms]] on [[jet bundle|jet manifolds]] of {{math|''Y'' → ''X''}}. The [[cohomology|coboundary operator]] of this bicomplex contains the variational operator {{math|''δ''}} which, acting on {{math|''L''}}, defines the associated Euler–Lagrange operator {{math|''δL''}}.\n\n=== In coordinates ===\nGiven bundle coordinates {{math|''x''<sup>''λ''</sup>, ''y''<sup>''i''</sup>}} on a fiber bundle {{math|''Y''}} and the adapted coordinates {{math|''x''<sup>''λ''</sup>, ''y''<sup>''i''</sup>, ''y''<sup>''i''</sup><sub>Λ</sub>}}, {{math|1=(Λ = (''λ''<sub>1</sub>, ...,''λ''<sub>''k''</sub>)}}, {{math|1={{!}}Λ{{!}} = ''k'' ≤ ''r''}}) on jet manifolds {{math|''J''<sup>''r''</sup>''Y''}}, a Lagrangian {{math|''L''}} and its Euler–Lagrange operator read\n\n: <math>L=\\mathcal{L}(x^\\lambda,y^i,y^i_\\Lambda) \\, d^nx,</math>\n\n: <math>\\delta L= \\delta_i\\mathcal{L} \\, dy^i\\wedge d^nx,\\qquad \\delta_i\\mathcal{L} =\\partial_i\\mathcal{L} +\n\\sum_{|\\Lambda|}(-1)^{|\\Lambda|} \\, d_\\Lambda\n\\, \\partial_i^\\Lambda\\mathcal{L},</math>\n\nwhere\n\n: <math>d_\\Lambda=d_{\\lambda_1}\\cdots d_{\\lambda_k}, \\qquad\nd_\\lambda=\\partial_\\lambda + y^i_\\lambda\\partial_i +\\cdots,</math>\n\ndenote the total derivatives.\n\nFor instance, a first-order Lagrangian and its second-order Euler–Lagrange operator take the form\n\n: <math>L=\\mathcal{L}(x^\\lambda,y^i,y^i_\\lambda) \\, d^nx,\\qquad\n\\delta_i L =\\partial_i\\mathcal{L} - d_\\lambda\n\\partial_i^\\lambda\\mathcal{L}.</math>\n\n=== Euler–Lagrange equations ===\nThe kernel of an Euler–Lagrange operator provides the [[Euler–Lagrange equation]]s {{math|''δL'' {{=}} 0}}.\n\n== Cohomology and Noether's theorems==\n[[Cohomology]] of the variational bicomplex leads to the so-called\nvariational formula\n\n: <math>dL=\\delta L + d_H \\Theta_L,</math>\n\nwhere\n\n: <math>d_H\\Theta_L=dx^\\lambda\\wedge d_\\lambda\\phi, \\qquad \\phi\\in\nO^*_\\infty(Y)</math>\n\nis the total differential and {{math|θ<sub>''L''</sub>}} is a Lepage equivalent of {{math|''L''}}. [[Noether's first theorem]] and [[Noether's second theorem]] are corollaries of this variational formula.\n\n== Graded manifolds ==\nExtended to [[graded manifold]]s, the [[variational bicomplex]] provides description of graded Lagrangian systems of even and odd variables.<ref>{{harvnb|Sardanashvily|2013}}</ref>\n\n== Alternative formulations ==\nIn a different way, Lagrangians, Euler–Lagrange operators and Euler–Lagrange equations are introduced in the framework of the [[calculus of variations]].\n\n== Classical mechanics ==\nIn classical mechanics equations of motion are first and second order differential equations on a manifold {{math|''M''}} or various fiber bundles {{math|''Q''}} over {{math|ℝ}}. A solution of the equations of motion is called a ''motion''.<ref>{{harvnb|Arnold|1989|p=83}}</ref><ref>{{harvnb|Giachetta|Mangiarotti|Sardanashvily|2011|p=7}}</ref>\n{{stub section|date=August 2015}}\n\n== See also ==\n*[[Lagrangian mechanics]] \n*[[Calculus of variations]] \n*[[Noether's theorem]]\n*[[Noether identities]]\n*[[Jet bundle]]\n*[[Jet (mathematics)]]\n*[[Variational bicomplex]]\n\n== References ==\n{{Reflist}}\n*{{citation|last=Arnold|first=V. I.|title=Mathematical Methods of Classical Mechanics|year=1989|edition=second|publisher=[[Springer-Verlag]]|series=[[Graduate Texts in Mathematics]]|volume=60|isbn=0-387-96890-3|authorlink=Vladimir Arnold}}\n*{{cite book|ref=harv|last1=Giachetta|first1=G.|last2=Mangiarotti|first2=L.|last3=Sardanashvily|first3=G.|authorlink3=Gennadi Sardanashvily|title=New Lagrangian and Hamiltonian Methods in Field Theory|publisher=[[World Scientific]]|year=1997|isbn=981-02-1587-8}}\n*{{cite book|ref=harv|last1=Giachetta|first1=G.|last2=Mangiarotti|first2=L.|last3=Sardanashvily|first3=G.|title=Geometric formulation of classical and quantum mechanics|publisher=World Scientific|year=2011|isbn=978-981-4313-72-8|url=http://www.worldscientific.com/worldscibooks/10.1142/7816}}\n*{{cite book|last=Olver|first=P.|title=Applications of Lie Groups to Differential Equations|edition=2|publisher=Springer-Verlag|year=1993|isbn=0-387-94007-3}}\n*{{cite journal|ref=harv|first=G.|last=Sardanashvily|title=Graded Lagrangian formalism|journal=Int. J. Geom. Methods Mod. Phys.|volume=10|year=2013|issue=5|doi=10.1142/S0219887813500163|arxiv=1206.2508|publisher=World Scientific|issn=0219-8878}}\n\n== External links ==\n*{{cite journal|ref=harv|first=G.|last=Sardanashvily|title=Fibre Bundles, Jet Manifolds and Lagrangian Theory. Lectures for Theoreticians|year=2009|arxiv=0908.1886|bibcode=2009arXiv0908.1886S}}\n\n[[Category:Differential operators]]\n[[Category:Calculus of variations]]\n[[Category:Dynamical systems]]\n[[Category:Lagrangian mechanics]]"
    },
    {
      "title": "Legendre–Clebsch condition",
      "url": "https://en.wikipedia.org/wiki/Legendre%E2%80%93Clebsch_condition",
      "text": "In the [[calculus of variations]] the '''Legendre–Clebsch condition''' is a second-order condition which a solution of the [[Euler–Lagrange equation]] must satisfy in order to be a maximum (and not a minimum or another kind of extremal).\n\nFor the problem of maximizing\n\n:<math>  \\int_{a}^{b}  L(t,x,x')\\, dt . \\,</math>\n\nthe condition is\n\n:<math>0 \\ge L_{x' x'}(t,x(t),x'(t)), \\, \\forall t \\in[a,b]</math>\n\n==Generalized Legendre-Clebsch==\n\nIn [[optimal control]], the situation is more complicated because of the possibility of a [[singular control|singular solution]]'''.  The generalized Legendre–Clebsch condition''',<ref>H.M. Robbins, A generalized Legendre-Clebsch condition for the singular cases of optimal control, IBM Journal of Research and Development, 1967</ref> also known as convexity,<ref name=Choset2005>{{cite book\n | author = Choset, H.M.\n | year = 2005\n | title = Principles of Robot Motion: Theory, Algorithms, and Implementation\n | publisher = The MIT Press\n | isbn = 0-262-03327-5\n}}</ref> is a sufficient condition for local optimality such that when the linear sensitivity of the Hamiltonian to changes in u is zero, i.e.,\n\n: <math>\\frac{\\partial H}{\\partial u} = 0</math>\n\nThe Hessian of the Hamiltonian is positive definite along the trajectory of the solution:\n\n: <math>\\frac{\\partial^2 H}{\\partial u^2} > 0</math>\n\nIn words, the generalized LC condition guarantees that over a singular arc, the Hamiltonian is minimized.\n\n==See also==\n* [[Bang–bang control]]\n\n==References==\n\n{{reflist}}\n\n{{DEFAULTSORT:Legendre-Clebsch condition}}\n[[Category:Optimal control]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "List of variational topics",
      "url": "https://en.wikipedia.org/wiki/List_of_variational_topics",
      "text": "This is a '''list of variational topics''' in from [[mathematics]] and [[physics]]. See [[calculus of variations]] for a general introduction.\n* [[Action (physics)]]\n* [[Averaged Lagrangian]]\n* [[Brachistochrone curve]]\n* [[Calculus of variations]]\n* [[Catenoid]]\n* [[Cycloid]]\n* [[Dirichlet principle]]\n* [[Euler–Lagrange equation]] cf. [[Action (physics)]]\n* [[Fermat's principle]]\n* [[Functional (mathematics)]]\n* [[Functional derivative]]\n* [[Functional integral]]\n* [[Geodesic]]\n* [[Isoperimetry]]\n* [[Lagrangian (field theory)|Lagrangian]]\n* [[Lagrangian mechanics]]\n* [[Legendre transformation]]\n* [[Luke's variational principle]]\n* [[Minimal surface]]\n* [[Morse theory]]\n* [[Noether's theorem]]\n* [[Path integral formulation]]\n* [[Plateau's problem]]\n* [[Prime geodesic]]\n* [[Principle of least action]]\n* [[Soap bubble]]\n* [[Soap film]]\n* [[Tautochrone curve]]\n\n[[Category:Mathematics-related lists|Variations]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Maupertuis's principle",
      "url": "https://en.wikipedia.org/wiki/Maupertuis%27s_principle",
      "text": "In [[classical mechanics]], '''Maupertuis's principle''' (named after [[Pierre Louis Maupertuis]]), states that the path followed by a physical system is the one of least length (with a suitable interpretation of ''path'' and ''length'').  It is a special case of the more generally stated [[principle of least action]].  Using the [[calculus of variations]], it results in an [[integral equation]] formulation of the [[equations of motion]] for the system.\n\n==Mathematical formulation==\n\nMaupertuis's principle states that the true path of a system described by <math>N</math> [[generalized coordinates]] <math>\\mathbf{q} = \\left( q_{1}, q_{2}, \\ldots, q_{N} \\right)</math> between two specified states <math>\\mathbf{q}_{1}</math> and <math>\\mathbf{q}_{2}</math> is an extremum (i.e., a [[stationary point]], a minimum, maximum or saddle point) of the  [[Action (physics)#Abbreviated action (functional)|abbreviated action functional]]\n\n:<math>\n\\mathcal{S}_{0}[\\mathbf{q}(t)] \\ \\stackrel{\\mathrm{def}}{=}\\  \n\\int \\mathbf{p} \\cdot d\\mathbf{q} \n</math>\n\nwhere <math>\\mathbf{p} = \\left( p_{1}, p_{2}, \\ldots, p_{N} \\right)</math> are the conjugate momenta of the generalized coordinates, defined by the equation \n\n:<math>\np_{k} \\ \\stackrel{\\mathrm{def}}{=}\\  \\frac{\\partial L}{\\partial\\dot{q}_{k}}\n</math>\n\nwhere <math>L(\\mathbf{q},\\dot{\\mathbf{q}},t)</math> is the [[Lagrangian mechanics|Lagrangian]] [[function (mathematics)|function]] for the system.  In other words, any ''first-order'' perturbation of the path results in (at most) ''second-order'' changes in <math>\\mathcal{S}_{0}</math>.  Note that the abbreviated action <math>\\mathcal{S}_{0}</math> is a [[Functional (mathematics)|functional]] (i.e. a function from a vector space into its underlying scalar field), which in this case takes as its input a function (i.e. the pathes between the two specified states).\n\n==Jacobi's formulation==\n\nFor many systems, the kinetic energy <math>T</math> is quadratic in the generalized velocities <math>\\dot{\\mathbf{q}}</math>\n\n:<math>\nT = \\frac{1}{2} \\frac{d\\mathbf{q}}{dt} \\cdot \\mathbf{M} \\cdot \\frac{d\\mathbf{q}}{dt}\n</math>\n\nalthough the '''mass tensor''' <math>\\mathbf{M}</math> may be a complicated function of the generalized coordinates <math>\\mathbf{q}</math>.  For such systems, a simple relation relates the kinetic energy, the generalized momenta and the generalized velocities\n\n:<math>\n2 T = \\mathbf{p} \\cdot \\dot{\\mathbf{q}}\n</math>\n\nprovided that the potential energy <math>V(\\mathbf{q})</math> does not involve the generalized velocities.  By defining a normalized distance or [[Metric (mathematics)|metric]] <math>ds</math> in the space of generalized coordinates\n\n:<math>\nds^{2} = d\\mathbf{q} \\cdot \\mathbf{M} \\cdot d\\mathbf{q}\n</math>\n\none may immediately recognize the mass tensor as a [[metric tensor]]. The kinetic energy may be written in a massless form\n\n:<math>\nT = \\frac{1}{2} \\left( \\frac{ds}{dt} \\right)^{2}\n</math>\n\nor, equivalently,\n\n:<math>\n2 T dt = \\mathbf{p} \\cdot d\\mathbf{q} = \\sqrt{2 T} \\ ds.\n</math>\n\nHence, the abbreviated action can be written\n\n:<math>\n\\mathcal{S}_0 \\ \\stackrel{\\mathrm{def}}{=}\\  \\int \\mathbf{p} \\cdot d\\mathbf{q} = \n \\int ds \\, \\sqrt{2}\\sqrt{E_\\text{tot} - V(\\mathbf{q})}\n</math>\n\nsince the kinetic energy <math>T = E_\\text{tot} - V(\\mathbf{q})</math> equals the (constant) total energy <math>E_\\text{tot}</math> minus the potential energy <math>V(\\mathbf{q})</math>.   In particular, if the potential energy is a constant, then Jacobi's principle reduces to minimizing the path length <math>s = \\int ds</math> in the space of the generalized coordinates, which is equivalent to [[Hertz's principle of least curvature]].\n\n==Comparison with Hamilton's principle==\n\n[[Hamilton's principle]] and Maupertuis's principle are occasionally confused and both have been called the [[principle of least action]].  They differ from each other in three important ways: \n* ''their definition of the [[action (physics)|action]]...'' \n:::Hamilton's principle uses <math>\\mathcal{S} \\ \\stackrel{\\mathrm{def}}{=}\\  \\int L \\, dt</math>, the integral of the [[Lagrangian mechanics|Lagrangian]] over [[time]], varied between two fixed end times <math>t_{1}</math>, <math>t_{2}</math> and endpoints <math>q_1</math>, <math>q_2</math>.  By contrast, Maupertuis's principle uses the abbreviated action integral over the [[generalized coordinates]], varied along all constant energy paths ending at <math>q_1</math> and <math>q_2</math>.\n\n*''the solution that they determine...''\n:::Hamilton's principle determines the trajectory <math>\\mathbf{q}(t)</math> as a function of time, whereas Maupertuis's principle determines only the shape of the trajectory in the generalized coordinates.  For example, Maupertuis's principle determines the shape of the ellipse on which a particle moves under the influence of an inverse-square central force such as [[gravitation|gravity]], but does not describe ''per se'' how the particle moves along that trajectory.  (However, this time parameterization may be determined from the trajectory itself in subsequent calculations using the conservation of energy.)  By contrast, Hamilton's principle directly specifies the motion along the ellipse as a function of time.\n\n*''...and the constraints on the variation.''\n:::Maupertuis's principle requires that the two endpoint states <math>q_{1}</math> and <math>q_{2}</math> be given and that energy be conserved along every trajectory.  By contrast, Hamilton's principle does not require the conservation of energy, but does require that the endpoint times <math>t_{1}</math> and <math>t_{2}</math> be  specified as well as the endpoint states <math>q_{1}</math> and <math>q_{2}</math>.\n\n==History==\n\nMaupertuis was the first to publish a ''principle of least action'', where he defined ''action'' as <math>\\int v \\, ds</math>, which was to be minimized over all paths connecting two specified points.  However, Maupertuis applied the principle only to light, not matter (see [[s:Accord between different laws of Nature that seemed incompatible|the 1744 Maupertuis reference below]]).  He arrived at the principle by considering [[Snell's law]] for the [[refraction]] of [[light]], which [[Fermat]] had explained by [[Fermat's principle]], that light follows the path of shortest ''time'', not distance.  This troubled Maupertuis, since he felt that time and distance should be on an equal footing: \"why should light prefer the path of shortest time over that of distance?\"  Accordingly, Maupertuis asserts with no further justification the principle of least action as equivalent but more fundamental than [[Fermat's principle]], and uses it to derive [[Snell's law]].  Maupertuis specifically states that light does not follow the same laws as material objects.\n\nA few months later, well before Maupertuis's work appeared in print, [[Leonhard Euler]] independently defined action in its modern abbreviated form <math>\\mathcal{S}_{0} \\ \\stackrel{\\mathrm{def}}{=}\\  \\int m v \\, ds \\ \\stackrel{\\mathrm{def}}{=}\\  \\int p \\, dq</math> and applied it to the motion of a particle, but not to light (see [[s:Methodus inveniendi/Additamentum II|the 1744 Euler reference below]]).  Euler also recognized that the principle only held when the speed was a function only of position, i.e., when the total energy was conserved.  (The mass factor in the action and the requirement for energy conservation were not relevant to Maupertuis, who was concerned only with light.)  Euler used this principle to derive the equations of motion of a particle in uniform motion, in a uniform and non-uniform force field, and in a central force field.  Euler's approach is entirely consistent with the modern understanding of Maupertuis's principle described above, except that he insisted that the action should always be a minimum, rather than a stationary point.\n\nTwo years later, Maupertuis cites Euler's 1744 work as a \"beautiful application of my principle to the motion of the planets\" and goes on to apply the principle of least action to the lever problem in mechanical equilibrium and to perfectly elastic and perfectly inelastic collisions (see [[s:Derivation of the laws of motion and equilibrium from a metaphysical principle|the 1746 publication below]]).  Thus, Maupertuis takes credit for conceiving the principle of least action as a ''general'' principle applicable to all physical systems (not merely to light), whereas the historical evidence suggests that Euler was the one to make this intuitive leap.   Notably, Maupertuis's definitions of the action and protocols for minimizing it in this paper are inconsistent with the modern approach described above.  Thus, Maupertuis's published work does not contain a single example in which he used Maupertuis's principle (as presently understood).\n\nIn 1751, Maupertuis's priority for the principle of least action was challenged in print (''Nova Acta Eruditorum'' of Leipzig) by an old acquaintance, Johann Samuel Koenig, who quoted a 1707 letter purportedly from [[Gottfried Leibniz|Leibniz]] that described results similar to those derived by Euler in 1744.  However, Maupertuis and others demanded that Koenig produce the original of the letter to authenticate its having been written by Leibniz.  Koenig only had a copy and no clue as to the whereabouts of the original.  Consequently, [[s:Translation:Investigation of the letter of Leibniz|the Berlin Academy under Euler's direction declared the letter to be a forgery]] and that its President, Maupertuis, could continue to claim priority for having invented the principle.  Koenig continued to fight for Leibniz's priority and soon luminaries such as [[Voltaire]] and the King of Prussia, [[Frederick II of Prussia|Frederick II]] were engaged in the quarrel.  However, no progress was made until the turn of the twentieth century, when other independent copies of Leibniz's letter were discovered.  \n==See also==\n* [[Analytical mechanics]]\n* [[Hamilton's principle]]\n* [[Gauss's principle of least constraint]] (also describes '''Hertz's principle of least curvature''')\n* [[Hamilton–Jacobi equation]]\n\n==References==\n\n* [[Pierre Louis Maupertuis]], [[s:fr:Accord de différentes loix de la nature qui avoient jusqu'ici paru incompatibles|Accord de différentes loix de la nature qui avoient jusqu'ici paru incompatibles]] ''(original 1744 French text)''; [[s:Accord between different laws of Nature that seemed incompatible|Accord between different laws of Nature that seemed incompatible]] ''(English translation)''\n* [[Leonhard Euler]], [[s:la:Methodus inveniendi/Additamentum II|Methodus inveniendi/Additamentum II]] ''(original 1744 Latin text)''; [[s:Methodus inveniendi/Additamentum II|Methodus inveniendi/Appendix 2]] ''(English translation)''\n* [[Pierre Louis Maupertuis]], [[s:fr:Les loix du mouvement et du repos déduites d'un principe metaphysique|Les loix du mouvement et du repos déduites d'un principe metaphysique]] ''(original 1746 French text)''; [[s:Derivation of the laws of motion and equilibrium from a metaphysical principle|Derivation of the laws of motion and equilibrium from a metaphysical principle]] ''(English translation)''\n* [[Leonhard Euler]], [[s:fr:Exposé concernant l'examen de la lettre de M. de Leibnitz|Exposé concernant l'examen de la lettre de M. de Leibnitz]] ''(original 1752 French text)''; [[s:Investigation of the letter of Leibniz|Investigation of the letter of Leibniz]] ''(English translation)''\n* König J. S. \"De universali principio aequilibrii et motus\",  ''Nova Acta Eruditorum'', '''1751''', 125–135, 162–176.\n* J. J. O'Connor and E. F. Robertson, \"[http://www-history.mcs.st-andrews.ac.uk/history/HistTopics/Forgery_2.html The Berlin Academy and forgery]\", (2003), at ''[http://www-history.mcs.st-andrews.ac.uk/history/ The MacTutor History of Mathematics archive]''.\n* C. I. Gerhardt, (1898) \"Über die vier Briefe von Leibniz, die Samuel König in dem Appel au public, Leide MDCCLIII, veröffentlicht hat\", ''Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften'', '''I''', 419–427.\n* W. Kabitz, (1913) \"Über eine in Gotha aufgefundene Abschrift des von S. König in  seinem Streite mit Maupertuis und der Akademie veröffentlichten, seinerzeit für unecht erklärten Leibnizbriefes\", ''Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften'', '''II''', 632–638.\n* H. Goldstein, (1980) ''Classical Mechanics'', 2nd ed., Addison Wesley, pp.&nbsp;362–371. {{ISBN|0-201-02918-9}}\n* L. D. Landau and E. M. Lifshitz, (1976) ''Mechanics'', 3rd. ed., Pergamon Press, pp.&nbsp;140–143.  {{ISBN|0-08-021022-8}} (hardcover) and {{ISBN|0-08-029141-4}} (softcover)\n* G. C. J. Jacobi, ''Vorlesungen über Dynamik, gehalten an der Universität Königsberg im Wintersemester 1842–1843''. A. Clebsch (ed.) (1866); Reimer; Berlin. 290 pages, available online [http://math-doc.ujf-grenoble.fr/cgi-bin/oeitem?id=OE_JACOBI__8_1_0  Œuvres complètes volume '''8'''] at [http://math-doc.ujf-grenoble.fr/OEUVRES/ Gallica-Math] from the [http://gallica.bnf.fr/ Gallica Bibliothèque nationale de France].\n* H. Hertz, (1896) ''Principles of Mechanics'', in ''Miscellaneous Papers'', vol. III, Macmillan.\n* {{springer|id=H/h047140|title=Hertz's principle of least curvature|author=V.V. Rumyantsev}}\n\n[[Category:Calculus of variations]]\n[[Category:Hamiltonian mechanics]]\n[[Category:Mathematical principles]]"
    },
    {
      "title": "Minkowski–Steiner formula",
      "url": "https://en.wikipedia.org/wiki/Minkowski%E2%80%93Steiner_formula",
      "text": "In [[mathematics]], the '''Minkowski–Steiner formula''' is a formula relating the [[area|surface area]] and [[volume]] of [[compact space|compact]] [[subset]]s of [[Euclidean space]]. More precisely, it defines the surface area as the \"derivative\" of enclosed volume in an appropriate sense.\n\nThe Minkowski–Steiner formula is used, together with the [[Brunn–Minkowski theorem]], to prove the [[isoperimetric inequality]]. It is named after [[Hermann Minkowski]] and [[Jakob Steiner]].\n\n==Statement of the Minkowski-Steiner formula==\n\nLet <math>n \\geq 2</math>, and let <math>A \\subsetneq \\mathbb{R}^{n}</math> be a compact set. Let <math>\\mu (A)</math> denote the [[Lebesgue measure]] (volume) of <math>A</math>. Define the quantity <math>\\lambda (\\partial A)</math> by the '''Minkowski–Steiner formula'''\n\n:<math>\\lambda (\\partial A) := \\liminf_{\\delta \\to 0} \\frac{\\mu \\left( A + \\overline{B_{\\delta}} \\right) - \\mu (A)}{\\delta},</math>\n\nwhere\n\n:<math>\\overline{B_{\\delta}} := \\left\\{ x = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n} \\left| | x | := \\sqrt{x_{1}^{2} + \\dots + x_{n}^{2}} \\leq \\delta \\right. \\right\\}</math>\n\ndenotes the [[closed ball]] of [[radius]] <math>\\delta > 0</math>, and\n\n:<math>A + \\overline{B_{\\delta}} := \\left\\{ a + b \\in \\mathbb{R}^{n} \\left| a \\in A, b \\in \\overline{B_{\\delta}} \\right. \\right\\}</math>\n\nis the [[Minkowski sum]] of <math>A</math> and <math>\\overline{B_{\\delta}}</math>, so that\n\n:<math>A + \\overline{B_{\\delta}} = \\left\\{ x \\in \\mathbb{R}^{n} \\mathrel|\\ \\mathopen| x - a \\mathclose| \\leq \\delta \\mbox{ for some } a \\in A \\right\\}.</math>\n\n==Remarks==\n\n===Surface measure===\n\nFor \"sufficiently regular\" sets <math>A</math>, the quantity <math>\\lambda (\\partial A)</math> does indeed correspond with the <math>(n - 1)</math>-dimensional measure of the [[boundary (topology)|boundary]] <math>\\partial A</math> of <math>A</math>. See Federer (1969) for a full treatment of this problem.\n\n===Convex sets===\n\nWhen the set <math>A</math> is a [[convex set]], the [[limit inferior|lim-inf]] above is a true [[Limit of a sequence|limit]], and one can show that\n\n:<math>\\mu \\left( A + \\overline{B_{\\delta}} \\right) = \\mu (A) + \\lambda (\\partial A) \\delta + \\sum_{i = 2}^{n - 1} \\lambda_{i} (A) \\delta^{i} + \\omega_{n} \\delta^{n},</math>\n\nwhere the <math>\\lambda_{i}</math> are some [[continuous function]]s of <math>A</math> (see [[quermassintegral]]s) and <math>\\omega_{n}</math> denotes the measure (volume) of the [[unit ball]] in <math>\\mathbb{R}^{n}</math>:\n\n:<math>\\omega_{n} = \\frac{2 \\pi^{n / 2}}{n \\Gamma (n / 2)},</math>\n\nwhere <math>\\Gamma</math> denotes the [[Gamma function]].\n\n==Example: volume and surface area of a ball==\n\nTaking <math>A = \\overline{B_{R}}</math> gives the following well-known formula for the surface area of the [[sphere]] of radius <math>R</math>, <math>S_{R} := \\partial B_{R}</math>:\n\n:<math>\\lambda (S_{R}) = \\lim_{\\delta \\to 0} \\frac{\\mu \\left( \\overline{B_{R}} + \\overline{B_{\\delta}} \\right) - \\mu \\left( \\overline{B_{R}} \\right)}{\\delta}</math>\n::<math>= \\lim_{\\delta \\to 0} \\frac{[ (R + \\delta)^{n} - R^{n} ] \\omega_{n}}{\\delta}</math>\n::<math>= n R^{n - 1} \\omega_{n},</math>\n\nwhere <math>\\omega_{n}</math> is as above.\n\n==References==\n\n* {{cite book | author=Dacorogna, Bernard | title=Introduction to the Calculus of Variations | publisher=[[Imperial College Press]] | location=London  | year=2004 | isbn=1-86094-508-2 }}\n* {{cite book | author=Federer, Herbert  | title=Geometric Measure Theory | publisher=[[Springer-Verlag]] | location=New-York | year=1969 }}\n\n{{DEFAULTSORT:Minkowski-Steiner formula}}\n[[Category:Calculus of variations]]\n[[Category:Geometry]]\n[[Category:Measure theory]]\n[[Category:Hermann Minkowski]]"
    },
    {
      "title": "Mosco convergence",
      "url": "https://en.wikipedia.org/wiki/Mosco_convergence",
      "text": "In [[mathematical analysis]], '''Mosco convergence''' is a notion of convergence for [[functional (mathematics)|functionals]] that is used in [[nonlinear|nonlinear analysis]] and [[set-valued analysis]]. It is a particular case of [[Γ-convergence]]. Mosco convergence is sometimes phrased as “weak Γ-liminf and strong Γ-limsup” convergence since it uses both the [[Weak_topology#The_strong_and_weak_topologies|weak and strong topologies]] on a [[topological vector space]] ''X''. In finite dimensional spaces, Mosco convergence coincides with [[epi-convergence]].\n\n''Mosco convergence'' is named after [[Italy|Italian]] [[mathematician]] [[Umberto Mosco]], a current Harold J. Gay<ref>http://www.wpi.edu/Campus/Faculty/Awards/Professorship/gayprofship.html</ref> professor of mathematics at [[Worcester Polytechnic Institute]].\n\n==Definition==\n\nLet ''X'' be a topological vector space and let ''X''<sup>∗</sup> denote the [[continuous dual space|dual space]] of [[continuous linear functional]]s on ''X''. Let ''F''<sub>''n''</sub>&nbsp;:&nbsp;''X''&nbsp;→&nbsp;[0,&nbsp;+∞] be functionals on ''X'' for each ''n''&nbsp;=&nbsp;1, 2, ... The sequence (or, more generally, [[net (topology)|net]]) (''F''<sub>''n''</sub>) is said to '''Mosco converge''' to another functional ''F''&nbsp;:&nbsp;''X''&nbsp;→&nbsp;[0,&nbsp;+∞] if the following two conditions hold:\n\n* lower bound inequality: for each sequence of elements ''x''<sub>''n''</sub>&nbsp;∈&nbsp;''X'' [[weak convergence of measures|converging weakly]] to ''x''&nbsp;∈&nbsp;''X'',\n\n::<math>\\liminf_{n \\to \\infty} F_{n} (x_{n}) \\geq F(x);</math>\n\n* upper bound inequality: for every ''x''&nbsp;∈&nbsp;''X'' there exists an approximating sequence of elements ''x''<sub>''n''</sub>&nbsp;∈&nbsp;''X'', converging strongly to ''x'', such that\n\n::<math>\\limsup_{n \\to \\infty} F_{n} (x_{n}) \\leq F(x).</math>\n\nSince lower and upper bound inequalities of this type are used in the definition of Γ-convergence, Mosco convergence is sometimes phrased as “weak Γ-liminf and strong Γ-limsup” convergence.  Mosco convergence is sometimes abbreviated to '''M-convergence''' and denoted by\n\n:<math>\\mathop{\\text{M-lim}}_{n \\to \\infty} F_{n} = F \\text{ or } F_{n} \\xrightarrow[n \\to \\infty]{\\mathrm{M}} F.</math>\n\n==References==\n\n* {{cite journal | last=Mosco | first=Umberto | title=Approximation of the solutions of some variational inequalities | journal=Ann. Scuola Normale Sup. | location=Pisa | volume=21 | year=1967 | pages=373&ndash;394 }}\n* {{cite journal | last=Mosco | first=Umberto | title=Convergence of convex sets and of solutions of variational inequalities | journal=Advances in Mathematics | volume=3 | year=1969 | pages=510&ndash;585 | doi=10.1016/0001-8708(69)90009-7 | issue=4 }}\n* {{cite journal | last=Borwein | first=Jonathan M. |author2=Fitzpatrick, Simon | year=1989 | title=Mosco convergence and the Kadec property | journal=Proc. Amer. Math. Soc.| volume=106 | pages=843&ndash;851 | doi=10.2307/2047444 | issue=3 | publisher=American Mathematical Society | jstor=2047444 }}\n* {{Cite web  | last = Mosco | first = Umberto  | title = Worcester Polytechnic Institute Faculty Directory | url = http://www.wpi.edu/academics/facultydir/uxm.html | publisher =  | accessdate = }}\n\n== Notes ==\n{{Reflist}}\n\n[[Category:Calculus of variations]]\n[[Category:Variational analysis]]"
    },
    {
      "title": "Mountain pass theorem",
      "url": "https://en.wikipedia.org/wiki/Mountain_pass_theorem",
      "text": "The '''mountain pass theorem''' is an [[existence theorem]] from the [[calculus of variations]].  Given certain conditions on a function, the theorem demonstrates the existence of a [[saddle point]].  The theorem is unusual in that there are many other theorems regarding the existence of [[extremum|extrema]], but few regarding saddle points.\n\n== Theorem statement ==\nThe assumptions of the theorem are:\n* <math>I</math> is a [[functional (mathematics)|functional]] from a [[Hilbert space]] ''H'' to the [[real number|reals]],\n* <math>I\\in C^1(H,\\mathbb{R})</math> and <math>I'</math> is [[Lipschitz continuous]] on bounded subsets of ''H'',\n* <math>I</math> satisfies the [[Palais-Smale compactness condition]],\n* <math>I[0]=0</math>,\n* there exist positive constants ''r'' and ''a'' such that <math>I[u]\\geq a</math> if <math>\\Vert u\\Vert =r</math>, and\n* there exists <math>v\\in H</math> with <math>\\Vert v\\Vert >r</math> such that <math>I[v]\\leq 0</math>.\nIf we define:\n:<math>\\Gamma=\\{\\mathbf{g}\\in C([0,1];H)\\,\\vert\\,\\mathbf{g}(0)=0,\\mathbf{g}(1)=v\\}</math>\nand:\n:<math>c=\\inf_{\\mathbf{g}\\in\\Gamma}\\max_{0\\leq t\\leq 1} I[\\mathbf{g}(t)],</math>\nthen the conclusion of the theorem is that ''c'' is a critical value of ''I''.\n\n== Visualization ==\n\nThe intuition behind the theorem is in the name \"mountain pass.\"  Consider ''I'' as describing elevation.  Then we know two low spots in the landscape: the origin because <math>I[0]=0</math>, and a far-off spot ''v'' where <math>I[v]\\leq 0</math>.  In between the two lies a range of mountains (at <math>\\Vert u\\Vert =r</math>) where the elevation is high (higher than ''a''>0).  In order to travel along a path ''g'' from the origin to ''v'', we must pass over the mountains — that is, we must go up and then down.  Since ''I'' is somewhat smooth, there must be a critical point somewhere in between.  (Think along the lines of the [[mean-value theorem]].)  The mountain pass lies along the path that passes at the lowest elevation through the mountains.  Note that this mountain pass is almost always a [[saddle point]].\n\nFor a proof, see section 8.5 of Evans.\n\n== Weaker formulation ==\nLet <math>X</math> be [[Banach space]]. The assumptions of the theorem are:\n* <math>\\Phi\\in C(X,\\mathbf R)</math> and have a [[Gateaux derivative]] <math>\\Phi'\\colon X\\to X^*</math> which is continuous when <math>X</math> and <math>X^*</math> are endowed with [[strong topology]] and [[weak* topology]] respectively.\n* There exists <math>r>0</math> such that one can find certain <math>\\|x'\\|>r</math> with\n:<math>\\max\\,(\\Phi(0),\\Phi(x'))<\\inf\\limits_{\\|x\\|=r}\\Phi(x)=:m(r)</math>.\n* <math>\\Phi</math> satisfies weak [[Palais-Smale condition]] on <math>\\{x\\in X\\mid m(r)\\le\\Phi(x)\\}</math>.\n\nIn this case there is a [[critical point (mathematics)|critical point]] <math>\\overline x\\in X</math> of <math>\\Phi</math> satisfying <math>m(r)\\le\\Phi(\\overline x)</math>. Moreover, if we define\n:<math>\\Gamma=\\{c\\in C([0,1],X)\\mid c\\,(0)=0,\\,c\\,(1)=x'\\}</math>\nthen\n:<math>\\Phi(\\overline x)=\\inf_{c\\,\\in\\,\\Gamma}\\max_{0\\le t\\le 1}\\Phi(c\\,(t)).</math>\n\nFor a proof, see section 5.5 of Aubin and Ekeland.\n\n== References ==\n* {{cite book | first=Youssef | last=Jabri |  title=The Mountain Pass Theorem, Variants, Generalizations and Some Applications (Encyclopedia of Mathematics and its Applications) | publisher=Cambridge University Press | year=2003 | isbn=0-521-82721-3}}\n* {{cite book | first=Lawrence C. | last=Evans | title=Partial Differential Equations | publisher=American Mathematical Society | location=Providence, Rhode Island | year=1998 | isbn=0-8218-0772-2}}\n* {{cite book | first=Jean-Pierre | last=Aubin |author2=Ivar Ekeland | title=Applied Nonlinear Analysis | publisher=Dover Books | year=2006 | isbn=0-486-45324-3}}\n* {{cite journal|last1=Bisgard|first1=James|title=Mountain Passes and Saddle Points|journal=SIAM Review|date=2015|volume=57|issue=2|pages=275–292|doi=10.1137/140963510|url=http://epubs.siam.org/doi/10.1137/140963510}}\n\n[[Category:Mathematical analysis]]\n[[Category:Calculus of variations]]\n[[Category:Theorems in analysis]]"
    },
    {
      "title": "Nehari manifold",
      "url": "https://en.wikipedia.org/wiki/Nehari_manifold",
      "text": "[[File:Nehari.png|thumb|250px|Nehari variety]]\n\nIn the [[calculus of variations]], a branch of [[mathematics]], a '''Nehari manifold''' is a manifold of functions, whose definition is motivated by the work of  {{harvs|txt|authorlink=Zeev Nehari|first=Zeev|last=Nehari|year1=1960|year2=1961}}. It is a [[differential manifold]] associated to the [[Dirichlet problem]] for the semilinear [[elliptic partial differential equation]]\n\n:<math> -\\triangle u  = |u|^{p-1}u,\\text{ with  }u\\mid_{\\partial \\Omega} = 0.</math>\n\nHere Δ is the [[Laplacian]] on a bounded domain Ω in '''R'''<sup>''n''</sup>.\n\nThere are infinitely many solutions to this problem.  Solutions are precisely the critical points for the [[energy functional]]\n\n:<math>J(v) = \\frac12\\int_{\\Omega}{|\\nabla v|^2\\,d\\mu}-\\frac1{p+1}\\int_{\\Omega}{|v|^{p+1}\\,d\\mu}</math>\n\non the [[Sobolev space]] {{nowrap|1=''H''{{su|p=1|b=0}}(&Omega;)}}. The Nehari manifold is defined to be the set of {{nowrap|1=''v''&nbsp;&isin;&nbsp;''H''{{su|p=1|b=0}}(&Omega;)}} such that\n:<math>\\|\\nabla v\\|^2_{L^2(\\Omega)} = \\|v\\|^{p+1}_{L^{p+1}(\\Omega)} > 0.</math>\nSolutions to the original variational problem that lie in the Nehari manifold are (constrained) minimizers of the energy, and so [[direct methods in the calculus of variations]] can be brought to bear.\n\nMore generally, given a suitable functional ''J'', the associated Nehari manifold is defined as the set of functions ''u'' in an appropriate function space for which\n\n:<math>\\langle J'(u), u\\rangle = 0. </math>\n\nHere ''J''&prime; is the [[functional derivative]] of ''J''.\n\n== References ==\n* A. Bahri and P. L. Lions (1988), Morse Index of Some Min-Max Critical Points. I. Applications to Multiplicity Results. Communications on Pure and Applied Mathematics. (XLI) 1027&ndash;1037.\n*{{Citation | last1=Nehari | first1=Zeev | title=On a class of nonlinear second-order differential equations | jstor=1993333 |mr=0111898 | year=1960 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=95 | pages=101–123 | doi=10.2307/1993333}}\n*{{Citation | last1=Nehari | first1=Zeev | title=Characteristic values associated with a class of non-linear second-order differential equations | doi=\t10.1007/BF02559588 |mr=0123775 | year=1961 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=105 | issue=3–4 | pages=141–175}}\n*{{Citation | last1=Willem | first1=Michel | title=Minimax theorems | publisher=Birkhäuser Boston | location=Boston, MA | series=Progress in Nonlinear Differential Equations and their Applications, 24 | isbn=978-0-8176-3913-6 |mr=1400007 | year=1996}}\n\n[[Category:Calculus of variations]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Newton's minimal resistance problem",
      "url": "https://en.wikipedia.org/wiki/Newton%27s_minimal_resistance_problem",
      "text": "'''Newton's Minimal Resistance Problem''' is a problem of finding a [[solid of revolution]] which experiences a minimum resistance when it moves through a homogeneous fluid with constant velocity in the direction of the axis of revolution, named after [[Isaac Newton]], who studied the problem in 1685 and published it in 1687 in his [[Philosophiæ Naturalis Principia Mathematica|Principia Mathematica]].<ref>Newton, Isaac. \"Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy).\" London (1687) (1987).</ref> This is the first example of a problem solved in what is now called the [[calculus of variations]], appearing a decade before the [[Brachistochrone curve|brachistochrone problem]].<ref>Goldstine, Herman Heine. A History of the Calculus of Variations from the 17th through the 19th Century. Vol. 5. Springer Science & Business Media, 2012.</ref> Newton published the solution in [[Principia Mathematica]] without his derivation and [[David Gregory (mathematician)|David Gregory]] was the first person who approached Newton and persuaded him to write an analysis for him. Then the derivation was shared with his students and peers by Gregory.<ref>Newton, I. \" Philosophiæ Naturalis Principia Mathematica, translation\" by A. Motte (1729), revised by F. Cajori (1934).\" Berkeley, CA: University of California Press 140: 175.</ref>\n\nAccording to I Bernard Cohen, in his Guide to Newton’s Principia, \"The key to Newton’s reasoning was found in the 1880s, when the earl of Portsmouth gave his family’s vast collection of Newton’s scientific and mathematical papers to Cambridge University. Among Newton’s manuscripts they found the draft text of a letter, … in which Newton elaborated his mathematical argument. [This]\nwas never fully understood, however, until the publication of the major manuscript documents by D. T. Whiteside [1974], whose analytical and historical commentary has enabled students of Newton not only to follow fully Newton’s path to discovery and proof, but also Newton’s later (1694) recomputation of the surface of least resistance\".<ref>{{cite book|last1=Cohen|first1=I. Bernard|last2=Whitman|first2=Anne|title=Principia, A New Translation|date=1999|publisher=[[University of California Press]]|pages=182}}</ref><ref>{{cite book|last1=Whiteside|first1=D. T.|title=The Mathematical Papers of Isaac Newton, Vol 6|date=1974|publisher=[[Cambridge University Press]]|pages=456, 470–480}}</ref>\n\nEventhough Newton's model for the fluid was wrong as per our current understanding, the fluid he had considered finds its application in [[Hypersonic flow]] theory as a limiting case.<ref>Hayes, W. D., & Probstein, R. F. (1967). Hypersonic flow theory: Inviscid flows. Academic Press.</ref>\n\n==Definition==\n\nIn Proposition 34 of Book 2 of the Principia, Newton wrote, ''If in a rare medium, consisting of equal particles freely disposed at equal distances from each other, a globe and a cylinder described on equal diameter move with equal velocities in the direction of the axis of the cylinder, the resistance of the globe will be but half as great as that of the cylinder.''\n\nFollowing this proposition is a scholium containing the famous condition that the curve which, when rotated about its axis, generates the solid that experiences less resistance than any other solid having a fixed length, and width.\n\nIn modern form, Newton's problem is to minimize the following integral:<ref>Chandrasekhar, Subrahmanyan. Newton's Principia for the common reader. Oxford University Press, 1995.</ref><ref>Davis, Harold Thayer. Introduction to nonlinear differential and integral equations. Courier Corporation, 1962.</ref>\n\n: <math> I = \\int \\frac{y \\dot{y}^3}{1 + \\dot{y}^2} dx </math>\n\nwhere <math>y(x)</math> represents the curve which generates a solid when it is rotated about the x-axis and <math>\\dot{y}=dy/dx</math>.\n\nI is the reduction in resistance caused by the particles impinging upon the sloping surface DNG, formed by rotating the curve, instead of perpendicularly upon the horizontal projection of DNG on the rear disc DA from the direction of motion, in Fig. 1. Note that the front of the solid is the disc BG, the triangles GBC and GBR are not part of it, but are used below by Newton to express the minimum condition.\n\nThis integral is related to the total resistance experienced by the body by the following relation: <math> \\rho = \\frac{H^2}{2} + \\int \\frac{y \\dot{y}^3}{1 + \\dot{y}^2} dx </math>\n\nThe problem is to find the curve that generates the solid that experiences less resistance than any other solid having a fixed axial length = L, and a fixed width, H.\n\nSince the solid must taper in the direction of motion, H is the radius of the disc forming the rear surface of the curve rotated about the x-axis. The units are chosen so that the constant of proportionality is unity. Also, note that <math> \\dot{y} < 0 </math>, and the integral, which is evaluated between x = 0 and x = L is negative. Let y = h when x = L.\n\nWhen the curve is the horizontal line, DK, so the solid is a cylinder, <math> y = H, \\dot{y} = 0 </math>, the integral is zero and the resistance of the cylinder is: <math> \\rho = \\frac{H^2}{2} </math>, which explains the constant term.\n\n[[File:Min Resistance Problem Diagram.png|Min Resistance Problem Diagram]]\n\n== Condition for a minimum resistance solid ==\n\nThe simplest way to apply the [[Euler–Lagrange equation]] to this problem is to rewrite the resistance as:\n\n: <math> \\rho = \\frac{H^2}{2} + \\int \\frac{y}{1 + \\dot{x}^2} dy </math> where <math> \\dot{x}=dx/dy </math>, and the integral, which is evaluated between y = H and y = h < H, is negative.\n\nSubstituting the integrand <math> F(y,\\dot{x}) = y/(1+\\dot{x}^2) </math> into the Euler-Lagrange equation\n:<math> \\frac{d}{dy} \\left(\\frac{\\partial F}{\\partial \\dot{x}}\\right) = \\frac{d}{dy} \\left(\\frac{-2\\dot{x}y}{(1+\\dot{x}^2)^2}\\right) = \\frac{\\partial F}{\\partial x} = 0 </math>, and it follows that <math> \\frac{\\dot{x}y}{(1+\\dot{x}^2)^2}</math> is constant, and this can be written as\n\n:<math> y = K_1\\frac{(1+ p^2)^2}{p^3} </math>   (1)   where <math> p = -\\dot{y} > 0 </math>, and where <math> K_1 > 0 </math> is a constant.\n\nAlthough the curves that satisfy the minimum condition cannot be described by a simple function, y = f(x), they may be plotted using p as a parameter, to obtain the corresponding coordinates (x,y) of the curves. The equation of x as a function of p is obtained from the minimum condition (1), and an equivalent of it was first found by Newton.\n\nDifferentiating: <math> dy = - pdx = K_1(1 - \\frac{2}{p^2} - \\frac{3}{p^4})dp </math>, and integrating\n\n:<math>x = K_2 - K_1 \\left(\\ln p + \\frac{1}{p^2} + \\frac{3}{4p^4}\\right) </math>, where <math> K_2 > 0 </math> is a constant.\n\nSince <math> y = H </math>, when <math> x = 0 </math>, and <math> y = h </math>, when <math> x = L </math>, the constants <math>K_1, \\ K_2</math> can be determined in terms of H, h and L. Because y from equation (1) can never be zero or negative, the front surface of any solid satisfying the minimum condition must be a disc, GB.\n\nAs this was the first example of this type of problem, Newton had to invent a completely new method of solution. Also, he went much deeper in his analysis of the problem than simply finding the condition (1).\n\n== Solid Experiencing Least Resistance ==\n\nWhile a solid of least resistance must satisfy (1), the converse is not true. Fig. 2 shows the family of curves that satisfy it for different values of <math> K_1 > 0 </math>. As <math> K_1 </math> increases the radius, Bg = h, of the disc at x = L decreases and the curve becomes steeper.\n\nDirectly before the minimum resistance problem, Newton stated that if on any elliptical or oval figure rotated about its axis, p becomes greater than unity, one with less resistance can be found. This is achieved by replacing the part of the solid that has p > 1 with the [[frustum]] of a cone whose vertex angle is a right angle, as shown in Fig. 2 for curve <math> D\\nu \\phi \\gamma B </math>. This has less resistance than <math> D\\nu \\phi \\Gamma B </math>. Newton does not prove this, but adds that it might have applications in shipbuilding. Whiteside supplies a proof and contends that Newton would have used the same reasoning.\n\nIn Fig. 2, since the solid generated from the curve Dng satisfies the minimum condition and has p < 1 at g, it experiences less resistance than that from any other curve with the same end point g. However, for the curve DνΓ, with p > 1 at end point Γ, this is not the case for although the curve satisfies the minimum condition, the resistance experienced by φγ and γΓ together is less than that by φΓ.\n\nNewton concluded that of all solids that satisfy the minimum resistance condition, the one experiencing the least resistance, DNG in Fig. 2, is the one that has p = 1 at G. This is shown schematically in Fig. 3 where the overall resistance of the solid varies against the radius of the front surface disc, the minimum occurring when h = BG, corresponding to p = 1 at G.\n\nIn the Principia, in Fig.  1 the condition for the minimum resistance solid is translated into a geometric form as follows: draw GR parallel to the tangent at N, so that <math> p = \\frac{GB}{BR} </math>, and equation (1) becomes: <math> \\frac{NM.GB^3}{ BR^3(1 + (GB/BR)^2)^2} = \\frac{NM.BR.GB^3}{GR^4} = K_1 </math>\n\nAt G, <math> NM = BG </math>, <math> BR = BC = BG </math>, and <math> GR^2 = 2BG^2 </math>, so <math> \\frac{NM.BR.GB^3}{GR^4} = \\frac{GB}{4} = K_1 </math> which appears in the Principia in the form:\n\n:<math> \\frac{NM}{GR} = \\frac{GR^3}{4BR.GB^2} </math>\n\n== Newton’s Derivation of the Minimum Resistance Condition ==\n\nAlthough this appears fairly simple, it has several subtleties that have caused much confusion.\n\nIn Fig 4, assume DNSG is the curve that when rotated about AB generates the solid whose resistance is less than any other such solid with the same heights, AD = H, BG = h and length, AB = L.\n\nFig. 5. shows the infinitesimal region of the curve about N and I in more detail. Although NI, Nj and NJ are really curved, they can be approximated by straight lines provided NH is sufficiently small.\n\n[[File:Min Resistance Surface.png|Min Resistance Surface]]\n\nLet HM = y, AM = x, NH = u, and HI = w = dx. Let the tangent at each point on the curve, <math> p = - \\frac{dy}{dx} = \\frac{u}{w} </math>. The reduction of the resistance of the sloping ring NI compared to the vertical ring NH rotated about AB is <math> r = \\frac{yp^3}{1 + p^2}dx  = \\frac{yu^3}{u^2 + w^2} </math>   (2)\n\nLet the minimum resistance solid be replaced by an identical one, except that the arc between points I and K is shifted by a small distance to the right <math> IJ = KL = o > 0 </math>, or to the left <math> Ij = Kl = o < 0 </math>, as shown in more detail in Fig. 5. In either case, HI becomes <math> HJ,Hj = w + o </math>.\n\nThe resistance of the arcs of the curve DN and SG are unchanged. Also, the resistance of the arc IK is not changed by being shifted, since the slope remains the same along its length. The only change to the overall resistance of DNSG is due to the change to the gradient of arcs NI and KS. The 2 displacements have to be equal for the slope of the arc IK to be unaffected, and the new curve to end at G.\n\nThe new resistance due to particles impinging upon NJ or Nj, rather that NI is:\n\n<math> r + \\delta r = \\frac {yu^3}{u^2 + (w + o)^2} = r - \\frac {2ywu^3 o}{(u^2 + w^2)^2} </math> + w.(terms in ascending powers of <math> \\frac{o}{w} </math>  starting with the 2nd).\n\nThe result is a change of resistance of: <math> - \\frac {2NM.HI.HN^3 o}{NI^4} </math> + higher order terms, the resistance being  reduced if o > 0 (NJ less resisted than NI).\n\nThis is the original 1685 derivation where he obtains the above result using the series expansion in powers of o. In his 1694 revisit he differentiates (2) with respect to w. He sent details of his later approach to David Gregory, and these are included as an appendix in Motte’s translation of the Principia.\n\nSimilarly, the change in resistance due to particles impinging upon SL or Sl rather that SK is: <math> + \\frac {2TS.OK.OS^3 o}{SK^4} </math> + higher order terms.\n\nThe overall change in the resistance of the complete solid, <math> \\delta \\rho = ( - \\frac {2NM.HI.HN^3}{NI^4} + \\frac {2TS.OK.OS^3}{SK^4})o </math> + w.(terms in ascending powers of <math> \\frac{o}{w} </math>  starting with the 2nd).\n \nFig 6 represents the total resistance of DNJLSG, or DNjlSG as a function of o. Since the original curve DNIKSG has the least resistance, any change o of whatever sign, must result in an increase in the resistance. This is only possible if the coefficient of o in the expansion of <math> \\rho (o) </math> is zero, so:\n\n<math> \\frac {NM.HI.HN^3}{NI^4} = \\frac {TS.OK.OS^3}{SK^4} </math>  (2)\n\nIf this was not the case, it would be possible to choose a value of o with a sign that produced a curve DNJLSG, or DNjlSG with less resistance than the original curve, contrary to the initial assumption. The approximation of taking straight lines for the finite arcs, NI and KS becomes exact in the limit as HN and OS approach zero. Also, NM and HM can be taken as equal, as can OT and ST.\n\nHowever, N and S on the original curve are arbitrary points, so for any 2 points anywhere on the curve the above equality must apply. This is only possible if in the limit of any infinitesimal arc HI, anywhere on the curve, the expression,\n\n<math> \\frac {NM.HI.HN^3}{NI^4} </math> is a constant. (3)\n\nThis has to be the case since, if <math> \\frac {NM.HI.HN^3}{NI^4} </math> was to vary along the curve, it would be possible to find 2 infinitesimal arcs NI and KS such that (2) was false, and the coefficient of o in the expansion of <math> \\delta \\rho </math> would be non-zero. Then a solid with less resistance could be produced by choosing a suitable value of o.\n\nThis is the reason for the constant term in the minimum condition in (3). As noted above, Newton went further, and claimed that the resistance of the solid is less than that of any other with the same length and width, when the slope at G is equal to unity. Therefore, in this case, the constant in (3) is equal to one quarter of the radius of the front disc of the solid, <math> \\frac{GB}{4} </math>.\n\n==References==\n{{Reflist}}\n\n{{Isaac Newton}}\n\n[[Category:Minimal surfaces]]\n[[Category:Isaac Newton]]\n[[Category:Calculus of variations]]\n[[Category:Mathematical problems]]"
    },
    {
      "title": "Noether identities",
      "url": "https://en.wikipedia.org/wiki/Noether_identities",
      "text": "In mathematics, '''Noether identities''' characterize the degeneracy of a Lagrangian system. Given a Lagrangian system and its [[Lagrangian system|Lagrangian]]&nbsp;''L'', Noether identities can be defined as a [[differential operator]] whose kernel contains a range of the [[Lagrangian system|Euler&ndash;Lagrange operator]] of&nbsp;''L''. Any  [[Lagrangian system|Euler&ndash;Lagrange operator]] obeys Noether identities which therefore are separated into the trivial and non-trivial ones. A [[Lagrangian system|Lagrangian]]&nbsp;''L'' is called degenerate if the [[Lagrangian system|Euler&ndash;Lagrange operator]] of&nbsp;''L'' satisfies non-trivial Noether identities. In this case [[Euler&ndash;Lagrange equation]]s are not independent.\n\nNoether identities need not be independent, but satisfy first-stage Noether identities, which are subject to the second-stage Noether identities and so on. Higher-stage Noether identities also are separated into the trivial and non-trivial once. A degenerate Lagrangian is called reducible if there exist non-trivial higher-stage Noether identities. [[Yang&ndash;Mills theory|Yang&ndash;Mills gauge theory]] and [[gauge gravitation theory]] exemplify irreducible Lagrangian field theories.\n\nDifferent variants of [[Noether's second theorem|second Noether’s theorem]] state the one-to-one correspondence between the non-trivial reducible Noether identities and the non-trivial reducible [[gauge symmetry (mathematics)|gauge symmetries]]. Formulated in a very general setting, [[Noether's second theorem|second Noether’s theorem]] associates to the Koszul&ndash;Tate complex of reducible Noether identities, parameterized by [[Batalin&ndash;Vilkovisky formalism|antifields]], the BRST complex of reducible gauge symmetries parameterized by [[Faddeev&ndash;Popov ghost|ghosts]]. This is the case of [[covariant classical field theory]] and Lagrangian [[BRST formalism|BRST theory]].\n\n==See also==\n*[[Noether's second theorem]]\n*[[Emmy Noether]]\n*[[Lagrangian system]]\n*[[Variational bicomplex]]\n*[[Gauge symmetry (mathematics)]]\n\n==References==\n* Gomis, J., Paris, J., Samuel, S., Antibracket, antifields and gauge theory quantization, Phys. Rep. '''259''' (1995) 1.\n* Fulp, R., Lada, T., [[Jim Stasheff|Stasheff, J.]]. Noether variational theorem II and the BV formalism, [http://xxx.lanl.gov/abs/math/0204079 arXiv: math/0204079]\n* Bashkirov, D., Giachetta, G., Mangiarotti, L., [[Gennadi Sardanashvily|Sardanashvily, G.]], The KT-BRST complex of a degenerate Lagrangian system, Lett. Math. Phys. '''83''' (2008) 237; [http://xxx.lanl.gov/abs/math-ph/0702097 arXiv: math-ph/0702097].\n* [[Gennadi Sardanashvily|Sardanashvily, G.]], Noether theorems in a general setting, [https://arxiv.org/abs/1411.2910 arXiv 1411.2910].\n\n\n[[Category:Calculus of variations]]\n[[Category:Differential equations]]\n[[Category:Theoretical physics]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Noether's second theorem",
      "url": "https://en.wikipedia.org/wiki/Noether%27s_second_theorem",
      "text": "In [[mathematics]] and [[theoretical physics]], '''Noether's second theorem''' relates symmetries of an [[action (physics)|action]] [[functional (mathematics)|functional]] with a system of [[differential equation]]s.<ref>{{Citation  |last=Noether  |first=Emmy  |year=1918  |title=Invariante Variationsprobleme  |url=https://de.wikisource.org/wiki/Invariante_Variationsprobleme  |journal=Nachr. D. König. Gesellsch. D. Wiss. Zu Göttingen, Math-phys. Klasse  |volume=1918  |pages=235–257 }}\n:Translated in {{cite journal  |last1=Noether  |first1=Emmy  |year=1971  |title=Invariant variation problems |journal=[[Transport Theory and Statistical Physics]]  |volume=1  |issue=3  |pages=186  |arxiv=physics/0503066  |bibcode=1971TTSP....1..186N  |doi=10.1080/00411457108231446}}</ref>  The action ''S'' of a physical system is an [[integral]] of a so-called [[Lagrangian mechanics|Lagrangian]] function ''L'', from which the system's behavior can be determined by the [[principle of least action]].  \n\nSpecifically, the theorem says that if the action has an infinite-dimensional [[Lie algebra]] of infinitesimal symmetries parameterized linearly by ''k'' arbitrary functions and their derivatives up to order ''m'', then the [[functional derivative]]s of ''L'' satisfy a system of ''k'' differential equations.\n\nNoether's second theorem is sometimes used in [[gauge theory]].  Gauge theories are the basic elements of all modern [[field theory (physics)|field theories]] of physics, such as the prevailing [[Standard Model]].\n\n==See also==\n* [[Noether's first theorem]]\n* [[Noether identities]]\n* [[Gauge symmetry (mathematics)]]\n* [[Emmy Noether]]\n\n==Notes==\n{{reflist|1}}\n\n==References==\n*{{cite book  |last=Kosmann-Schwarzbach  |first=Yvette   |authorlink=Yvette Kosmann-Schwarzbach  |title=The Noether theorems: Invariance and conservation laws in the twentieth century  |publisher=[[Springer Science+Business Media|Springer-Verlag]]  |series=Sources and Studies in the History of Mathematics and Physical Sciences  |year=2010  |isbn=978-0-387-87867-6}}\n*{{cite book  |last=Olver  |first=Peter  |author-link=Peter J. Olver  |title=Applications of Lie groups to differential equations  |publisher=[[Springer Science+Business Media|Springer-Verlag]]  |edition=2nd  |series=[[Graduate Texts in Mathematics]]  |volume=107  |year=1993  |isbn=0-387-95000-1}}\n*{{Cite book  |last=Sardanashvily  |first=G.  |author-link=Gennadi Sardanashvily  |title=Noether's Theorems. Applications in Mechanics and Field Theory  |publisher=[[Springer Science+Business Media|Springer-Verlag]]  |year=2016  |isbn=978-94-6239-171-0 }}\n\n==Further reading==\n*{{cite journal |last1=Noether  |first1=Emmy  |year=1971  |title=Invariant Variation Problems  |journal=[[Transport Theory and Statistical Physics]]  |volume=1  |issue=3  |pages=186–207  |arxiv=physics/0503066  |bibcode=1971TTSP....1..186N  |doi=10.1080/00411457108231446}}\n* {{cite arxiv |last1=Fulp  |first1=Ron  |last2=Lada  |first2=Tom  |last3=Stasheff  |first3=Jim  |year=2002  |title=Noether's variational theorem II and the BV formalism   |eprint=math/0204079}}\n* {{cite journal  |last1=Bashkirov |first1=D. |last2=Giachetta |first2=G. |last3=Mangiarotti |first3=L. |last4=Sardanashvily |first4=G  |year=2008  |title=The KT-BRST Complex of a Degenerate Lagrangian System |journal=Letters in Mathematical Physics |volume=83 |issue=3 |pages=237 |arxiv=math-ph/0702097  |bibcode=2008LMaPh..83..237B |doi=10.1007/s11005-008-0226-y}}\n* {{cite journal  |last1=Montesinos |first1=Merced |last2=Gonzalez|first2=Diego |last3=Celada |first3=Mariano |last4=Diaz |first4=Bogar  |year=2017  |title=Reformulation of the symmetries of first-order general relativity |journal=Classical and Quantum Gravity |volume=34 |issue=20 |pages=205002 |arxiv=1704.04248  |bibcode=2017CQGra..34t5002M |doi=10.1088/1361-6382/aa89f3}}\n* {{cite journal  |last1=Montesinos |first1=Merced |last2=Gonzalez|first2=Diego |last3=Celada |first3=Mariano |year=2018  |title=The gauge symmetries of first-order general relativity with matter fields |journal=Classical and Quantum Gravity |volume=35 |issue=20 |pages=205005 |arxiv=1809.10729 |bibcode=2018CQGra..35t5005M |doi=10.1088/1361-6382/aae10d}}\n\n\n[[Category:Theoretical physics]]\n[[Category:Calculus of variations]]\n[[Category:Partial differential equations]]\n[[Category:Conservation laws]]\n[[Category:Theorems in mathematical physics]]\n[[Category:Quantum field theory]]\n[[Category:Symmetry]]"
    },
    {
      "title": "Noether's theorem",
      "url": "https://en.wikipedia.org/wiki/Noether%27s_theorem",
      "text": "{{Use American English|date=March 2019}}{{Short description|Physical law that differentiable symmetries correspond to conservation laws}}\n{{About|Emmy Noether's first theorem, which derives conserved quantities from symmetries|}}\n[[File:Noether_theorem_1st_page.png|thumb| First page of [[Emmy Noether]]'s article \"Invariante Variationsprobleme\" (1918), where she proved her theorem.]]\n'''Noether's''' ('''first''')<ref>See also [[Noether's second theorem]].</ref> '''theorem''' states that every [[Differentiable function|differentiable]] [[Symmetry in physics|symmetry]] of the [[action (physics)|action]] of a physical system has a corresponding [[conservation law]]. The theorem was proven by mathematician [[Emmy Noether]] in 1915 and published in 1918,<ref>{{cite journal | author = Noether E | year = 1918 | title = Invariante Variationsprobleme | journal = Nachr. D. König. Gesellsch. D. Wiss. Zu Göttingen, Math-phys. Klasse | volume = 1918 | pages = 235–257 |url= https://eudml.org/doc/59024}}</ref> after a special case was proven by [[Eugène_Cosserat|E. Cosserat]] & [[François_Cosserat|F. Cosserat]] in 1909.<ref>{{cite book | author = Cosserat E., Cosserat F. |year = 1909 | title = Théorie des corps déformables | publisher = Hermann | place = Paris | url = http://ebooks.library.cornell.edu/cgi/t/text/text-idx?c=math;idno=06420001}}</ref> The action of a physical system is the [[time integral|integral over time]] of a [[Lagrangian mechanics|Lagrangian]] function (which may or may not be an [[integral over space]] of a [[Lagrangian (field theory)|Lagrangian density function]]), from which the system's behavior can be determined by the [[principle of least action]]. This theorem only applies to continuous and smooth symmetries over [[physical space]].\n\nNoether's theorem is used in [[theoretical physics]] and the [[calculus of variations]]. A generalization of the formulations on [[constants of motion]] in Lagrangian and [[Hamiltonian mechanics]] (developed in 1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian alone (e.g. systems with a [[Lagrangian_mechanics#Extensions_to_include_non-conservative_forces|Rayleigh dissipation function]]). In particular, [[dissipative]] systems with [[Continuous symmetry|continuous symmetries]] need not have a corresponding conservation law.\n\n==Basic illustrations and background==\n{{unreferenced-section|date=June 2016}}\nAs an illustration, if a physical system behaves the same regardless of how it is oriented in space, its [[Lagrangian mechanics|Lagrangian]] is symmetric under continuous rotations: from this symmetry, Noether's theorem dictates that the [[angular momentum]] of the system be conserved, as a consequence of its laws of motion. The physical system itself need not be symmetric; a jagged asteroid tumbling in space conserves angular momentum despite its asymmetry. It is the laws of its motion that are symmetric.\n\nAs another example, if a physical process exhibits  the same outcomes regardless of place or time, then its Lagrangian is symmetric under continuous translations in space and time respectively: by Noether's theorem, these symmetries account for the [[conservation law]]s of [[momentum|linear momentum]] and [[energy]] within this system, respectively.\n\nAs a final example, if the behavior of a physical system does not change upon spatial or temporal reflection, then its Lagrangian has reflection symmetry and time reversal symmetry respectively: Noether's theorem says that these symmetries result in the conservation laws of [[Parity (physics)|parity]] and [[entropy]],{{cn|date=June 2018}} {{discuss|Need to resolve the case when we are discussing entropy of a closed system, which always increases.}} respectively.\n\nNoether's theorem is important, both because of the insight it gives into conservation laws, and also as a practical calculational tool. It allows investigators to determine the conserved quantities (invariants) from the observed symmetries of a physical system. Conversely, it allows researchers to consider whole classes of hypothetical Lagrangians with given invariants, to describe a physical system. As an illustration, suppose that a physical theory is proposed which conserves a quantity ''X''. A researcher can calculate the types of Lagrangians that conserve ''X'' through a continuous symmetry.  Due to Noether's theorem, the properties of these Lagrangians provide further criteria to understand the implications and judge the fitness of the new theory.\n\nThere are numerous versions of Noether's theorem, with varying degrees of generality. There are natural quantum counterparts of this theorem, expressed in  the [[Ward–Takahashi identity|Ward–Takahashi identities]]. Generalizations of Noether's theorem to [[superspace]]s also exist.{{cn|date=December 2017}}\n\n== Informal statement of the theorem ==\nAll fine technical points aside, Noether's theorem can be stated informally\n\n{{quote|If a system has a continuous symmetry property, then there are corresponding quantities whose values are conserved in time.<ref>{{cite book |author=Thompson, W.J. |title=Angular Momentum: an illustrated guide to rotational symmetries for physical systems |publisher=Wiley |year=1994 |isbn=0-471-55264-X |volume=1 |page=5 |url=https://books.google.com/books?id=O25fXV4z0B0C&pg=PA5#v=onepage&q&f=false}}</ref>}}\n\nA more sophisticated version of the theorem involving fields states that:\n\n{{quote|To every differentiable [[Symmetry in physics|symmetry]] generated by local actions there corresponds a [[conserved current]].}}\n\nThe word \"symmetry\" in the above statement refers more precisely to the [[covariant transformation|covariance]] of the form that a physical law takes with respect to a one-dimensional [[Lie group]] of transformations satisfying certain technical criteria. The [[conservation law]] of a [[physical quantity]] is usually expressed as a [[continuity equation]].\n\nThe formal proof of the theorem utilizes the condition of invariance to derive an expression for a current associated with a conserved physical quantity. \nIn modern (since ca. 1980<ref>The term \"Noether charge\" occurs in Seligman, ''Group theory and its applications in physics, 1980: Latin American School of Physics, Mexico City'', American Institute of Physics, 1981. It comes enters wider use during the 1980s, e.g. by G. Takeda in: Errol Gotsman, Gerald Tauber (eds.) ''From SU(3) to Gravity: Festschrift in Honor of Yuval Ne'eman'', 1985, p. 196.</ref>) terminology, the  conserved quantity is called the ''Noether charge'', while the flow carrying that charge is called the ''Noether current''. The Noether current is defined [[up to]] a [[solenoidal]] (divergenceless) vector field.\n\nIn the context of gravitation, [[Felix Klein]]'s statement of Noether's theorem for action ''I'' stipulates for the invariants:<ref>Nina Byers (1998) [http://cwp.library.ucla.edu/articles/noether.asg/noether.html \"E. Noether's Discovery of the Deep Connection Between Symmetries and Conservation Laws.\"] in Proceedings of a Symposium on the Heritage of Emmy Noether, held on 2–4 December 1996, at the Bar-Ilan University, Israel, Appendix B.</ref> \n{{quote|If an integral I is invariant under a continuous group ''G''<sub>''ρ''</sub> with ''ρ'' parameters, then ''ρ'' linearly independent combinations of the Lagrangian expressions are divergences.}}\n\n==Historical context==\n{{main|Constant of motion|conservation law|conserved current}}\n\nA [[conservation law]] states that some quantity ''X'' in the mathematical description of a system's evolution remains constant throughout its motion — it is an [[Invariant (physics)|invariant]].   Mathematically, the rate of change of ''X'' (its [[derivative]] with respect to [[time]]) is zero,\n\n:<math>\\frac{dX}{dt} = \\dot{X} = 0 ~.</math>\n\nSuch quantities are said to be conserved; they are often called [[constant of motion|constants of motion]] (although motion ''per se'' need not be involved, just evolution in time). For example, if the energy of a system is conserved, its energy is invariant at all times, which imposes a constraint on the system's motion and may help in solving for it. Aside from insights that such constants of motion give into the nature of a system, they are a useful calculational tool; for example, an approximate solution can be corrected by finding the nearest state that satisfies the suitable conservation laws.\n\nThe earliest constants of motion discovered were [[momentum]] and [[energy]], which were proposed in the 17th century by [[René Descartes]] and [[Gottfried Leibniz]] on the basis of [[collision]] experiments, and refined by subsequent researchers. [[Isaac Newton]] was the first to enunciate the conservation of momentum in its modern form, and showed that it was a consequence of [[Newton's laws of motion|Newton's third law]]. According to [[general relativity]], the conservation laws of linear momentum, energy and angular momentum are only exactly true globally when expressed in terms of the  sum of the [[stress–energy tensor]] (non-gravitational stress–energy) and the [[Stress–energy–momentum pseudotensor#Landau–Lifshitz pseudotensor|Landau–Lifshitz stress–energy–momentum pseudotensor]] (gravitational stress–energy). The local conservation of non-gravitational linear momentum and energy in a free-falling reference frame is expressed by the vanishing of the covariant [[divergence]] of the [[stress–energy tensor]]. Another important conserved quantity, discovered in studies of the [[celestial mechanics]] of astronomical bodies, is the [[Laplace–Runge–Lenz vector]].\n\nIn the late 18th and early 19th centuries, physicists developed more systematic methods for discovering invariants. A major advance came in 1788 with the development of [[Lagrangian mechanics]], which is related to the [[principle of least action]]. In this approach, the state of the system can be described by any type of [[generalized coordinate]]s '''q'''; the laws of motion need not be expressed in a [[Cartesian coordinate system]], as was customary in Newtonian mechanics. The [[action (physics)|action]] is defined as the time integral ''I'' of a function known as the [[Lagrangian mechanics|Lagrangian]]&nbsp;''L''\n\n::<math>I = \\int L(\\mathbf{q}, \\dot{\\mathbf{q}}, t) \\, dt ~,</math>\n\nwhere the dot over '''q''' signifies the rate of change of the coordinates '''q''',\n\n::<math>\\dot{\\mathbf{q}} = \\frac{d\\mathbf{q}}{dt} ~.</math>\n\n[[Hamilton's principle]] states that the physical path '''q'''(''t'')—the one actually taken by the system—is a path for which infinitesimal variations in that path cause no change in ''I'', at least up to first order. This principle results in the [[Euler–Lagrange equation]]s,\n\n:<math>\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\right) = \\frac{\\partial L}{\\partial \\mathbf{q}}   ~.</math>\n\nThus, if one of the coordinates, say ''q<sub>k</sub>'', does not appear in the Lagrangian, the right-hand side of the equation is zero, and the left-hand side requires that\n\n:<math>\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{q}_k} \\right) = \\frac{dp_k}{dt} = 0~,</math>\n\nwhere the momentum\n\n:<math> p_k = \\frac{\\partial L}{\\partial \\dot{q}_k} </math>\n\nis conserved throughout the motion (on the physical path).\n\nThus, the absence of the '''ignorable''' coordinate ''q<sub>k</sub>'' from the Lagrangian implies that the Lagrangian is unaffected by changes or transformations of ''q<sub>k</sub>''; the Lagrangian is invariant, and is said to exhibit a [[symmetry in physics|symmetry]] under such transformations. This is the seed idea generalized in Noether's theorem.\n\nSeveral alternative methods for finding conserved quantities were developed in the 19th century, especially by [[William Rowan Hamilton]]. For example, he developed a theory of [[canonical transformation]]s which allowed changing coordinates so that some coordinates disappeared from the Lagrangian, as above, resulting in conserved canonical momenta. Another approach, and perhaps the most efficient for finding conserved quantities, is the [[Hamilton–Jacobi equation]].\n\n==Mathematical expression==\n{{see also|Perturbation theory}}\n\n===Simple form using perturbations===\n\nThe essence of Noether's theorem is generalizing the ignorable coordinates outlined.\n\nOne can assume that the Lagrangian ''L'' defined above is invariant under small perturbations (warpings) of the time variable ''t'' and the [[generalized coordinate]]s '''q'''. One may write\n\n:<math>t \\rightarrow t^{\\prime} = t + \\delta t</math>\n:<math>\\mathbf{q} \\rightarrow \\mathbf{q}^{\\prime} = \\mathbf{q} + \\delta \\mathbf{q} ~,</math>\n\nwhere the perturbations ''δt'' and ''δ'''''q''' are both small, but variable. For generality, assume there are (say) ''N'' such [[symmetry transformations]] of the action, i.e. transformations leaving the action unchanged; labelled by an index ''r''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;3,&nbsp;…,&nbsp;''N''.\n\nThen the resultant perturbation can be written as a linear sum of the individual types of perturbations,\n:<math>\\delta t = \\sum_r \\varepsilon_r T_r </math>\n:<math>\\delta \\mathbf{q} = \\sum_r \\varepsilon_r \\mathbf{Q}_r ~, </math>\nwhere ''ε''<sub>''r''</sub> are [[infinitesimal]] parameter coefficients corresponding to each: \n*[[Lie group#The exponential map|generator]] ''T<sub>r</sub>'' of [[time evolution]], and\n*[[Lie group#The exponential map|generator]] '''Q'''<sub>''r''</sub> of the generalized coordinates.\nFor translations, '''Q'''<sub>''r''</sub> is a constant with units of [[length]]; for rotations, it is an expression linear in the components of '''q''', and the parameters make up an  [[angle]].\n\nUsing these definitions, [[Emmy Noether|Noether]] showed that the ''N'' quantities\n\n:<math>\\left(\\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\cdot \\dot{\\mathbf{q}} - L \\right) T_r - \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\cdot \\mathbf{Q}_r</math>\n\n(which have the [[dimensional analysis|dimensions]] of [energy]·[time] + [momentum]·[length] = [action]) are conserved ([[constants of motion]]).\n\n==== Examples ====\n\n;Time invariance\n\nFor illustration, consider a Lagrangian that does not depend on time, i.e., that is invariant (symmetric) under changes ''t'' → ''t'' + δ''t'', without any change in the coordinates '''q'''. In this case, ''N''&nbsp;=&nbsp;1, ''T''&nbsp;=&nbsp;1 and '''Q'''&nbsp;=&nbsp;0; the corresponding conserved quantity is the total [[energy]] ''H''<ref name=\"energy\" >{{harvnb|Lanczos|1970|pp=401–3}}</ref>\n\n:<math>H = \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\cdot \\dot{\\mathbf{q}} - L. </math>\n\n;Translational invariance\n\nConsider a Lagrangian which does not depend on an (\"ignorable\", as above)  coordinate ''q''<sub>''k''</sub>; so  it is invariant (symmetric) under changes ''q''<sub>''k''</sub> → ''q''<sub>''k''</sub> + ''δq''<sub>''k''</sub>. In that case, ''N''&nbsp;=&nbsp;1, ''T''&nbsp;=&nbsp;0, and ''Q''<sub>''k''</sub>&nbsp;=&nbsp;1; the conserved quantity is the corresponding linear [[momentum]] ''p''<sub>''k''</sub><ref name=\"momentum\" >{{harvnb|Lanczos|1970|pp=403–4}}</ref>\n\n:<math>p_k = \\frac{\\partial L}{\\partial \\dot{q_k}}.</math>\n\nIn [[special relativity|special]] and [[general relativity]], these apparently separate conservation laws are aspects of a single conservation law, that of the [[stress–energy tensor]],<ref name=\"stress–energy_tensor\" >{{harvnb|Goldstein|1980|pp=592–3}}</ref> that is derived in the next section.\n\n;Rotational invariance\n\nThe conservation of the [[angular momentum]] '''L''' = '''r''' × '''p''' is  analogous to its linear momentum counterpart.<ref name=\"angular_momentum\" >{{harvnb|Lanczos|1970|pp=404–5}}</ref> It is assumed that the symmetry of the Lagrangian is rotational, i.e., that the Lagrangian does not depend on the absolute orientation of the physical system in space. For concreteness, assume that the Lagrangian does not change under small rotations of an angle ''δθ'' about an axis '''n'''; such a rotation transforms the [[Cartesian coordinate system|Cartesian coordinates]] by the equation\n\n:<math>\\mathbf{r} \\rightarrow \\mathbf{r} + \\delta\\theta \\, \\mathbf{n} \\times \\mathbf{r}.</math>\n\nSince time is not being transformed, ''T''=0. Taking ''δθ'' as the ''ε'' parameter and the Cartesian coordinates '''r''' as the generalized coordinates '''q''', the corresponding '''Q''' variables are given by\n\n:<math>\\mathbf{Q} = \\mathbf{n} \\times \\mathbf{r}.</math>\n\nThen Noether's theorem states that the following quantity is conserved,\n:<math>\n\\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\cdot \\mathbf{Q}_{r} = \n\\mathbf{p} \\cdot \\left( \\mathbf{n} \\times \\mathbf{r} \\right) = \n\\mathbf{n} \\cdot \\left( \\mathbf{r} \\times \\mathbf{p} \\right) = \n\\mathbf{n} \\cdot \\mathbf{L}.\n</math>\n\nIn other words, the component of the angular momentum '''L''' along the '''n''' axis is conserved.\n\nIf '''n''' is arbitrary, i.e., if the system is insensitive to any rotation, then every component of '''L''' is conserved; in short, [[angular momentum]] is conserved.\n\n===Field theory version===\nAlthough useful in its own right, the version of Noether's theorem just given is a special case of the general version derived in 1915. To give the flavor of the general theorem, a version of Noether's theorem for continuous fields in four-dimensional [[space–time]] is now given. Since field theory problems are more common in modern physics than [[mechanics]] problems, this field theory version is the most commonly used (or most often implemented) version of Noether's theorem.\n\nLet there be a set of differentiable [[Field (physics)|fields]] <math>\\varphi</math> defined over all space and time; for example, the temperature <math>T(\\mathbf{x}, t)</math> would be representative of such a field, being a number defined at every place and time. The [[principle of least action]] can be applied to such fields, but the action is now an integral over space and time\n\n:<math>\\mathcal{S} = \\int \\mathcal{L} \\left(\\varphi, \\partial_\\mu \\varphi, x^\\mu \\right) \\, d^4 x</math>\n\n(the theorem can actually be further generalized to the case where the Lagrangian depends on up to the ''n''<sup>th</sup> derivative using [[jet bundle]]s)\n\nA continuous transformation of the fields <math>\\varphi</math> can be written infinitesimally as\n\n:<math>\\varphi \\mapsto \\varphi + \\varepsilon \\Psi,</math>\n\nwhere <math>\\Psi</math> is in general a function that may depend on both <math>x^\\mu</math> and <math>\\varphi</math>. The condition for <math>\\Psi</math> to generate a physical symmetry is that the action <math>\\mathcal{S}</math> is left invariant. This will certainly be true if the Lagrangian density <math>\\mathcal{L}</math> is left invariant, but it will also be true if the Lagrangian changes by a divergence,\n\n:<math>\\mathcal{L} \\mapsto \\mathcal{L} + \\varepsilon \\partial_\\mu \\Lambda^\\mu,</math>\n\nsince the integral of a divergence becomes a boundary term according to the [[divergence theorem]]. A system described by a given action might have multiple independent symmetries of this type, indexed by <math>r = 1, 2, \\ldots, N,</math> so the most general symmetry transformation would be written as\n\n:<math>\\varphi \\mapsto \\varphi + \\varepsilon_r \\Psi_r,</math>\n\nwith the consequence\n\n:<math>\\mathcal{L} \\mapsto \\mathcal{L} + \\varepsilon_r \\partial_\\mu \\Lambda^\\mu_r.</math>\n\nFor such systems, Noether's theorem states that there are <math>N</math> conserved [[conserved current|current densities]]\n\n:<math>\nj^\\nu_r = \\Lambda^\\nu_r - \\frac{\\partial \\mathcal{L}}{\\partial \\varphi_{,\\nu}} \\cdot \\Psi_r\n</math>\n\n(where the dot product is understood to contract the ''field'' indices, not the <math>\\nu</math> index or <math>r</math> index).\n\nIn such cases, the [[conservation law]] is expressed in a four-dimensional way\n\n:<math>\\partial_\\nu j^\\nu = 0,</math>\n\nwhich expresses the idea that the amount of a conserved quantity within a sphere cannot change unless some of it flows out of the sphere. For example, [[electric charge]] is conserved; the amount of charge within a sphere cannot change unless some of the charge leaves the sphere.\n\nFor illustration, consider a physical system of fields that behaves the same under translations in time and space, as considered above; in other words, <math>L \\left(\\boldsymbol\\varphi, \\partial_\\mu{\\boldsymbol\\varphi}, x^\\mu \\right)</math> is constant in its third argument. In that case, ''N''&nbsp;=&nbsp;4, one for each dimension of space and time. An infinitesimal translation in space, <math>x^\\mu \\mapsto x^\\mu + \\varepsilon_r \\delta^\\mu_r</math> (with <math>\\delta</math> denoting the [[Kronecker delta]]), affects the fields as <math>\\varphi(x^\\mu) \\mapsto \\varphi(x^\\mu - \\varepsilon_r \\delta^\\mu_r)</math>: that is, relabelling the coordinates is equivalent to leaving the coordinates in place while translating the field itself, which in turn is equivalent to transforming the field by replacing its value at each point <math>x^\\mu</math> with the value at the point <math>x^\\mu - \\varepsilon X^\\mu</math> \"behind\" it which would be mapped onto <math>x^\\mu</math> by the infinitesimal displacement under consideration. Since this is infinitesimal, we may write this transformation as\n\n:<math>\\Psi_r = -\\delta^\\mu_r \\partial_\\mu \\varphi.</math>\n\nThe Lagrangian density transforms in the same way, <math>\\mathcal{L}(x^\\mu) \\mapsto \\mathcal{L}(x^\\mu - \\varepsilon_r \\delta^\\mu_r)</math>, so\n\n:<math>\\Lambda^\\mu_r = -\\delta^\\mu_r \\mathcal{L}</math>\n\nand thus Noether's theorem corresponds to the conservation law for the [[stress–energy tensor]] ''T''<sub>''μ''</sub><sup>''ν''</sup>,<ref name=\"stress–energy_tensor\" /> where we have used <math>\\mu</math> in place of <math>r</math>. To wit, by using the expression given earlier, and collecting the four conserved currents (one for each <math>\\mu</math>) into a tensor <math>T</math>, Noether's theorem gives\n\n:<math>\nT_\\mu{}^\\nu = -\\delta^\\nu_\\mu \\mathcal{L} + \\delta^\\sigma_\\mu \\partial_\\sigma \\varphi \\frac{\\partial \\mathcal{L}}{\\partial \\varphi_{,\\nu}} = \\left(\\frac{\\partial \\mathcal{L}}{\\partial \\varphi_{,\\nu}}\\right) \\cdot \\varphi_{,\\mu} - \\delta^\\nu_\\mu \\mathcal{L}\n</math>\n\nwith\n\n:<math>T_\\mu{}^\\nu{}_{,\\nu} = 0</math>\n\n(note that we relabelled <math>\\mu</math> as <math>\\sigma</math> at an intermediate step to avoid conflict). (However, note that the <math>T</math> obtained in this way may differ from the symmetric tensor used as the source term in general relativity; see [[Stress–energy tensor#Canonical stress.E2.80.93energy tensor|Canonical stress–energy tensor]].)\n\nThe conservation of [[electric charge]], by contrast, can be derived by considering ''Ψ'' linear in the fields ''φ'' rather than in the derivatives.<ref name=\"charge\">{{harvnb|Goldstein|1980|pp=593–4}}</ref> In [[quantum mechanics]], the [[probability amplitude]] ''ψ''('''x''') of finding a particle at a point '''x''' is a complex field ''φ'', because it ascribes a [[complex number]] to every point in space and time. The probability amplitude itself is physically unmeasurable; only the probability ''p'' = |''ψ''|<sup>2</sup> can be inferred  from a set of  measurements. Therefore, the system is invariant under transformations of the ''ψ'' field and its [[complex conjugate]] field ''ψ''<sup>*</sup> that leave |''ψ''|<sup>2</sup> unchanged, such as\n\n:<math>\\psi \\rightarrow e^{i\\theta} \\psi \\ ,\\ \\psi^{*} \\rightarrow e^{-i\\theta} \\psi^{*}~,</math>\n\na complex rotation. In the limit when the phase ''θ'' becomes infinitesimally small, ''δθ'', it may be taken as the parameter ''ε'', while the ''Ψ'' are equal to ''iψ'' and −''iψ''*, respectively. A specific example is the [[Klein–Gordon equation]], the [[special relativity|relativistically correct]] version of the [[Schrödinger equation]] for [[spin (physics)|spinless]] particles, which has the Lagrangian density\n\n:<math>L = \\psi_{,\\nu} \\psi^{*}_{,\\mu} \\eta^{\\nu \\mu} + m^2 \\psi \\psi^{*}.</math>\n\nIn this case, Noether's theorem states that the conserved (∂&nbsp;⋅&nbsp;''j''&nbsp;=&nbsp;0) current equals\n\n:<math>j^\\nu = i \\left( \\frac{\\partial \\psi}{\\partial x^\\mu} \\psi^{*} - \\frac{\\partial \\psi^{*}}{\\partial x^\\mu} \\psi \\right) \\eta^{\\nu \\mu}~,</math>\n\nwhich, when multiplied by the charge on that species of particle, equals the electric current density due to that type of particle. This \"gauge invariance\" was first noted by [[Hermann Weyl]], and is one of the prototype [[gauge symmetry|gauge symmetries]] of physics.\n\n== Derivations ==\n\n===One independent variable===\nConsider the simplest case, a system with one independent variable, time. Suppose the dependent variables '''q''' are such that the action integral\n\n:<math>I = \\int_{t_1}^{t_2} L [\\mathbf{q} [t], \\dot{\\mathbf{q}} [t], t] \\, dt </math>\n\nis invariant under brief infinitesimal variations in the dependent variables. In other words, they satisfy the [[Euler–Lagrange equation]]s\n\n:<math>\\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} [t] = \\frac{\\partial L}{\\partial \\mathbf{q}} [t].</math>\n\nAnd suppose that the integral is invariant under a continuous symmetry. Mathematically such a symmetry is represented as a [[flow (mathematics)|flow]], '''φ''', which acts on the variables as follows\n\n:<math>t \\rightarrow t' = t + \\varepsilon T </math>\n:<math>\\mathbf{q} [t] \\rightarrow \\mathbf{q}' [t'] = \\varphi [\\mathbf{q} [t], \\varepsilon] = \\varphi [\\mathbf{q} [t' - \\varepsilon T], \\varepsilon]</math>\n\nwhere ''ε'' is a real variable indicating the amount of flow, and ''T'' is a real constant (which could be zero) indicating how much the flow shifts time.\n\n:<math>\n\\dot{\\mathbf{q}} [t] \\rightarrow \\dot{\\mathbf{q}}' [t'] = \\frac{d}{dt} \\varphi [\\mathbf{q} [t], \\varepsilon] = \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} [\\mathbf{q} [t' - \\varepsilon T], \\varepsilon] \\dot{\\mathbf{q}} [t' - \\varepsilon T]\n.</math>\n\nThe action integral flows to\n\n:<math>\n\\begin{align}\nI' [\\varepsilon] & = \\int_{t_1 + \\varepsilon T}^{t_2 + \\varepsilon T} L [\\mathbf{q}'[t'], \\dot{\\mathbf{q}}' [t'], t'] \\, dt' \\\\[6pt]\n& = \\int_{t_1 + \\varepsilon T}^{t_2 + \\varepsilon T} L [\\varphi [\\mathbf{q} [t' - \\varepsilon T], \\varepsilon], \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} [\\mathbf{q} [t' - \\varepsilon T], \\varepsilon] \\dot{\\mathbf{q}} [t' - \\varepsilon T], t'] \\, dt'\n\\end{align}\n</math>\n\nwhich may be regarded as a function of ''ε''. Calculating the derivative at ''ε''' = 0 and using [[Leibniz's rule (derivatives and integrals)|Leibniz's rule]], we get\n\n:<math>\n\\begin{align}\n0 = {} & \\frac{d I'}{d \\varepsilon} [0] = L [\\mathbf{q} [t_2], \\dot{\\mathbf{q}} [t_2], t_2] T - L [\\mathbf{q} [t_1], \\dot{\\mathbf{q}} [t_1], t_1] T \\\\[6pt]\n& {} + \\int_{t_1}^{t_2} \\frac{\\partial L}{\\partial \\mathbf{q}} \\left( - \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} T + \\frac{\\partial \\varphi}{\\partial \\varepsilon} \\right) + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\left( - \\frac{\\partial^2 \\varphi}{(\\partial \\mathbf{q})^2} {\\dot{\\mathbf{q}}}^2 T + \\frac{\\partial^2 \\varphi}{\\partial \\varepsilon \\partial \\mathbf{q}} \\dot{\\mathbf{q}} -\n\\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\ddot{\\mathbf{q}} T \\right) \\, dt.\n\\end{align}\n</math>\n\nNotice that the Euler–Lagrange equations imply\n\n:<math>\n\\begin{align}\n\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} T \\right) \n& = \\left( \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\right) \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} T + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\left( \\frac{d}{dt} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\right) \\dot{\\mathbf{q}} T + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\ddot{\\mathbf{q}} \\, T \\\\[6pt]\n& = \\frac{\\partial L}{\\partial \\mathbf{q}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} T + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\left( \\frac{\\partial^2 \\varphi}{(\\partial \\mathbf{q})^2} \\dot{\\mathbf{q}} \\right) \\dot{\\mathbf{q}} T + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\ddot{\\mathbf{q}} \\, T.\n\\end{align}\n</math>\n\nSubstituting this into the previous equation, one gets\n\n:<math>\n\\begin{align}\n0 = {} & \\frac{d I'}{d \\varepsilon} [0] \\\\[6pt]\n= {} & L [\\mathbf{q} [t_2], \\dot{\\mathbf{q}} [t_2], t_2] T - L [\\mathbf{q} [t_1], \\dot{\\mathbf{q}} [t_1], t_1] T - \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} [t_2] T + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} [t_1] T \\\\[6pt]\n& {} + \\int_{t_1}^{t_2} \\frac{\\partial L}{\\partial \\mathbf{q}} \\frac{\\partial \\varphi}{\\partial \\varepsilon} + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial^2 \\varphi}{\\partial \\varepsilon \\partial \\mathbf{q}} \\dot{\\mathbf{q}} \\, dt.\n\\end{align}\n</math>\n\nAgain using the Euler–Lagrange equations we get\n\n:<math>\n\\frac{d}{d t} \\left( \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\varepsilon} \\right) \n= \\left( \\frac{d}{d t} \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\right) \\frac{\\partial \\varphi}{\\partial \\varepsilon} + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial^2 \\varphi}{\\partial \\varepsilon \\partial \\mathbf{q}} \\dot{\\mathbf{q}}\n= \\frac{\\partial L}{\\partial \\mathbf{q}} \\frac{\\partial \\varphi}{\\partial \\varepsilon} + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial^2 \\varphi}{\\partial \\varepsilon \\partial \\mathbf{q}} \\dot{\\mathbf{q}}.\n</math>\n\nSubstituting this into the previous equation, one gets\n\n:<math>\n\\begin{align}\n0 = {} & L [\\mathbf{q} [t_2], \\dot{\\mathbf{q}} [t_2], t_2] T - L [\\mathbf{q} [t_1], \\dot{\\mathbf{q}} [t_1], t_1] T - \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} [t_2] T + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} [t_1] T \\\\[6pt]\n& {} + \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\varepsilon} [t_2] - \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\varepsilon} [t_1].\n\\end{align}\n</math>\n\nFrom which one can see that\n\n:<math>\\left( \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\mathbf{q}} \\dot{\\mathbf{q}} - L \\right) T - \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\varepsilon}</math>\n\nis a constant of the motion, i.e., it is a conserved quantity. Since φ['''q''', 0] = '''q''', we get <math>\\frac{\\partial \\varphi}{\\partial \\mathbf{q}} = 1</math> and so the conserved quantity simplifies to\n\n:<math>\\left( \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\dot{\\mathbf{q}} - L \\right) T - \\frac{\\partial L}{\\partial \\dot{\\mathbf{q}}} \\frac{\\partial \\varphi}{\\partial \\varepsilon}.</math>\n\nTo avoid excessive complication of the formulas, this derivation assumed that the flow does not change as time passes. The same result can be obtained in the more general case.\n\n===Field-theoretic derivation===\nNoether's theorem may also be derived for tensor fields ''φ''<sup>''A''</sup> where the index ''A'' ranges over the various components of the various tensor fields. These field quantities are functions defined over a four-dimensional space whose points are labeled by coordinates ''x''<sup>μ</sup> where the index ''μ'' ranges over time (''μ''&nbsp;=&nbsp;0) and three spatial dimensions (''μ''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;3). These four coordinates are the independent variables; and the values of the fields at each event are the dependent variables. Under an infinitesimal transformation, the variation in the coordinates is written\n\n:<math>x^\\mu \\rightarrow \\xi^\\mu = x^\\mu + \\delta x^\\mu </math>\n\nwhereas the transformation of the field variables is expressed as\n\n:<math>\\varphi^A \\rightarrow \\alpha^A (\\xi^\\mu) = \\varphi^A (x^\\mu) + \\delta \\varphi^A (x^\\mu)\\,.</math>\n\nBy this definition, the field variations ''δφ''<sup>''A''</sup> result from two factors: intrinsic changes in the field themselves and changes in coordinates, since the transformed field ''α''<sup>''A''</sup> depends on the transformed coordinates ξ<sup>μ</sup>. To isolate the intrinsic changes, the field variation at a single point ''x''<sup>μ</sup> may be defined\n\n:<math>\\alpha^A (x^\\mu) = \\varphi^A (x^\\mu) + \\bar{\\delta} \\varphi^A (x^\\mu)\\,.</math>\n\nIf the coordinates are changed, the boundary of the region of space–time over which the Lagrangian is being integrated also changes; the original boundary and its transformed version are denoted as Ω and Ω’, respectively.\n\nNoether's theorem begins with the assumption that a specific transformation of the coordinates and field variables does not change the [[action (physics)|action]], which is defined as the integral of the Lagrangian density over the given region of spacetime. Expressed mathematically, this assumption may be written as\n\n:<math>\\int_{\\Omega^{\\prime}} L \\left( \\alpha^A, {\\alpha^A}_{,\\nu}, \\xi^{\\mu} \\right) d^{4}\\xi - \\int_{\\Omega} L \\left( \\varphi^A, {\\varphi^A}_{,\\nu}, x^{\\mu} \\right) d^{4}x = 0</math>\n\nwhere the comma subscript indicates a partial derivative with respect to the coordinate(s) that follows the comma, e.g.\n\n:<math>{\\varphi^A}_{,\\sigma} = \\frac{\\partial \\varphi^A}{\\partial x^{\\sigma}}\\,.</math>\n\nSince ξ is a dummy variable of integration, and since the change in the boundary Ω is infinitesimal by assumption, the two integrals may be combined using the four-dimensional version of the [[divergence theorem]] into the following form\n\n:<math>\n\\int_{\\Omega} \\left\\{ \n\\left[ L \\left( \\alpha^A, {\\alpha^A}_{,\\nu}, x^\\mu \\right) - \nL \\left( \\varphi^A, {\\varphi^A}_{,\\nu}, x^\\mu \\right) \\right]\n+ \\frac{\\partial}{\\partial x^\\sigma} \\left[ L \\left( \\varphi^A, {\\varphi^A}_{,\\nu}, x^\\mu \\right) \\delta x^\\sigma \\right]\n\\right\\} d^4 x = 0\n\\,.</math>\n\nThe difference in Lagrangians can be written to first-order in the infinitesimal variations as\n\n:<math>\n\\left[ L \\left( \\alpha^A, {\\alpha^A}_{,\\nu}, x^\\mu \\right) - \nL \\left( \\varphi^A, {\\varphi^A}_{,\\nu}, x^\\mu \\right) \\right] = \n\\frac{\\partial L}{\\partial \\varphi^A} \\bar{\\delta} \\varphi^A + \n\\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\bar{\\delta} {\\varphi^A}_{,\\sigma}\n\\,.</math>\n\nHowever, because the variations are defined at the same point as described above, the variation and the derivative can be done in reverse order; they [[commutativity|commute]]\n\n:<math>\n\\bar{\\delta} {\\varphi^A}_{,\\sigma} = \n\\bar{\\delta} \\frac{\\partial \\varphi^A}{\\partial x^\\sigma} = \n\\frac{\\partial}{\\partial x^\\sigma} ( \\bar{\\delta} \\varphi^A)\n\\,.</math>\n\nUsing the Euler–Lagrange field equations\n\n:<math>\n\\frac{\\partial}{\\partial x^\\sigma} \\left( \\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\right) =\n\\frac{\\partial L}{\\partial \\varphi^A}\n</math>\n\nthe difference in Lagrangians can be written neatly as\n\n:<math>\n\\begin{align}\n& \\left[ L \\left( \\alpha^A, {\\alpha^A}_{,\\nu}, x^\\mu \\right) - \nL \\left( \\varphi^A, {\\varphi^A}_{,\\nu}, x^{\\mu} \\right) \\right] \\\\[4pt]\n= {} & \\frac{\\partial}{\\partial x^\\sigma} \\left( \\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\right) \\bar{\\delta} \\varphi^A + \n\\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\bar{\\delta} {\\varphi^A}_{,\\sigma}\n= \\frac{\\partial}{\\partial x^\\sigma} \n\\left( \\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\bar{\\delta} \\varphi^A \\right).\n\\end{align}\n</math>\n\nThus, the change in the action can be written as\n\n:<math>\n\\int_\\Omega \\frac{\\partial}{\\partial x^\\sigma} \n\\left\\{ \\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\bar{\\delta} \\varphi^A + \nL \\left( \\varphi^A, {\\varphi^A}_{,\\nu}, x^\\mu \\right) \\delta x^\\sigma\n\\right\\} d^{4}x = 0\n\\,.</math>\n\nSince this holds for any region Ω, the integrand must be zero\n\n:<math>\n\\frac{\\partial}{\\partial x^\\sigma} \n\\left\\{ \\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\bar{\\delta} \\varphi^A + \nL \\left( \\varphi^A, {\\varphi^A}_{,\\nu}, x^\\mu \\right) \\delta x^\\sigma\n\\right\\} = 0\n\\,.</math>\n\nFor any combination of the various [[symmetry in physics|symmetry]] transformations, the perturbation can be written\n\n:<math>\\delta x^{\\mu} = \\varepsilon X^\\mu</math>\n:<math>\\delta \\varphi^A = \\varepsilon \\Psi^A = \\bar{\\delta} \\varphi^A + \\varepsilon \\mathcal{L}_X \\varphi^A</math>\n\nwhere <math>\\mathcal{L}_X \\varphi^A</math> is the [[Lie derivative]] of φ<sup>''A''</sup> in the ''X''<sup>''μ''</sup> direction. When ''φ''<sup>''A''</sup> is a scalar or <math>{X^\\mu}_{,\\nu} = 0 </math>,\n\n:<math>\\mathcal{L}_X \\varphi^A = \\frac{\\partial \\varphi^A}{\\partial x^{\\mu}} X^\\mu\\,.</math>\n\nThese equations imply that the field variation taken at one point equals\n\n:<math>\\bar{\\delta} \\varphi^A = \\varepsilon \\Psi^A - \\varepsilon \\mathcal{L}_X \\varphi^A\\,.</math>\n\nDifferentiating the above divergence with respect to ''ε'' at ''ε''&nbsp;=&nbsp;0 and changing the sign yields the conservation law\n\n:<math>\\frac{\\partial }{\\partial x^\\sigma} j^\\sigma = 0</math>\n\nwhere the conserved current equals\n\n:<math>\nj^\\sigma = \n\\left[\\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\mathcal{L}_X \\varphi^A - L \\, X^\\sigma\\right]\n- \\left(\\frac{\\partial L}{\\partial {\\varphi^A}_{,\\sigma}} \\right) \\Psi^A\\,.\n</math>\n\n===Manifold/fiber bundle derivation===\nSuppose we have an ''n''-dimensional oriented [[Riemannian manifold]], ''M'' and a target manifold ''T''. Let <math>\\mathcal{C}</math> be the [[Configuration space (physics)|configuration space]] of [[smooth function]]s from ''M'' to ''T''. (More generally, we can have smooth sections of a [[fiber bundle]] over ''M''.)\n\nExamples of this ''M'' in physics include:\n* In [[classical mechanics]], in the [[Hamiltonian mechanics|Hamiltonian]] formulation, ''M'' is the one-dimensional manifold '''R''', representing time and the target space is the [[cotangent bundle]] of [[space]] of generalized positions.\n* In [[field (physics)|field theory]], ''M'' is the [[spacetime]] manifold and the target space is the set of values the fields can take at any given point. For example, if there are ''m'' [[real number|real]]-valued [[scalar field]]s, <math>\\varphi_1,\\ldots,\\varphi_m</math>, then the target manifold is '''R'''<sup>m</sup>. If the field is a real vector field, then the target manifold is [[isomorphic]] to '''R'''<sup>3</sup>.\n\nNow suppose there is a [[functional (mathematics)|functional]]\n\n:<math>\\mathcal{S}:\\mathcal{C}\\rightarrow \\mathbf{R},</math>\n\ncalled the [[Action (physics)|action]]. (Note that it takes values into '''R''', rather than '''C'''; this is for physical reasons, and doesn't really matter for this proof.)\n\nTo get to the usual version of Noether's theorem, we need additional restrictions on the [[Action (physics)|action]]. We assume <math>\\mathcal{S}[\\varphi]</math> is the [[integral]] over ''M'' of a function\n\n:<math>\\mathcal{L}(\\varphi,\\partial_\\mu\\varphi,x)</math>\n\ncalled the [[Lagrangian (field theory)|Lagrangian density]], depending on ''φ'', its [[derivative]] and the position. In other words, for ''φ'' in <math>\\mathcal{C}</math>\n\n:<math> \\mathcal{S}[\\varphi]\\,=\\,\\int_M \\mathcal{L}[\\varphi(x),\\partial_\\mu\\varphi(x),x] \\, \\mathrm{d}^nx.</math>\n\nSuppose we are given [[boundary condition]]s, i.e., a specification of the value of ''φ'' at the [[Boundary (topology)|boundary]] if ''M'' is [[Compact space|compact]], or some limit on ''φ'' as ''x'' approaches ∞. Then the [[subspace topology|subspace]] of <math>\\mathcal{C}</math> consisting of functions ''φ'' such that all [[functional derivative]]s of <math>\\mathcal{S}</math> at ''φ'' are zero, that is:\n\n:<math>\\frac{\\delta \\mathcal{S}[\\varphi]}{\\delta \\varphi(x)}\\approx 0</math>\n\nand that ''φ'' satisfies the given boundary conditions, is the subspace of [[on shell]] solutions. (See [[principle of stationary action]])\n\nNow, suppose we have an [[infinitesimal transformation]] on <math>\\mathcal{C}</math>, generated by a [[functional (mathematics)|functional]] [[derivation (abstract algebra)|derivation]], ''Q'' such that\n\n:<math>Q \\left[ \\int_N \\mathcal{L} \\, \\mathrm{d}^n x \\right] \\approx \\int_{\\partial N} f^\\mu [\\varphi(x),\\partial\\varphi,\\partial\\partial\\varphi,\\ldots] \\, \\mathrm{d}s_\\mu </math>\n\nfor all compact submanifolds ''N'' or in other words,\n\n:<math>Q[\\mathcal{L}(x)]\\approx\\partial_\\mu f^\\mu(x)</math>\n\nfor all ''x'', where we set\n\n:<math>\\mathcal{L}(x)=\\mathcal{L}[\\varphi(x), \\partial_\\mu \\varphi(x),x].</math>\n\nIf this holds [[on shell]] and [[off shell]], we say ''Q'' generates an off-shell symmetry. If this only holds [[on shell]], we say ''Q'' generates an on-shell symmetry. Then, we say ''Q'' is a generator of a [[one-parameter group|one parameter]] [[symmetry]] [[Lie group]].\n\nNow, for any ''N'', because of the [[Euler–Lagrange]] theorem, [[on shell]] (and only on-shell), we have\n\n: <math>\n\\begin{align}\nQ\\left[\\int_N \\mathcal{L} \\, \\mathrm{d}^nx \\right]\n& =\\int_N \\left[\\frac{\\partial\\mathcal{L}}{\\partial\\varphi} - \\partial_\\mu \\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\varphi)} \\right]Q[\\varphi] \\, \\mathrm{d}^nx + \\int_{\\partial N} \\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\varphi)}Q[\\varphi] \\, \\mathrm{d}s_\\mu \\\\\n& \\approx\\int_{\\partial N} f^\\mu \\, \\mathrm{d}s_\\mu.\n\\end{align}\n</math>\n\nSince this is true for any ''N'', we have\n\n:<math>\\partial_\\mu\\left[\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\varphi)}Q[\\varphi]-f^\\mu\\right]\\approx 0.</math>\n\nBut this is the [[continuity equation]] for the current <math>J^\\mu</math> defined by:<ref name=Peskin>{{cite book |title=An Introduction to Quantum Field Theory |url=https://books.google.com/books?id=i35LALN0GosC&pg=PA689&dq=weinberg+%22symmetry+%22 |page=18 |author1=Michael E. Peskin |author2=Daniel V. Schroeder |publisher=Basic Books |isbn=0-201-50397-2 |year=1995 }}</ref>\n\n:<math>J^\\mu\\,=\\,\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\varphi)}Q[\\varphi]-f^\\mu,</math>\n\nwhich is called the '''Noether current''' associated with the [[symmetry]]. The continuity equation tells us that if we [[Integral|integrate]] this current over a [[space-like]] slice, we get a [[Conservation law|conserved quantity]] called the Noether charge (provided, of course, if ''M'' is noncompact, the currents fall off sufficiently fast at infinity).\n\n=== Comments ===\nNoether's theorem is an [[on shell]] theorem: it relies on use of the equations of motion—the classical path. It reflects the relation between the boundary conditions and the variational principle. Assuming no boundary terms in the action, Noether's theorem implies that\n\n:<math>\\int_{\\partial N} J^\\mu \\, \\mathrm{d}s_\\mu \\approx 0~.</math>\n\nThe quantum analogs of Noether's theorem involving expectation values, e.g. ⟨∫''d''<sup>4</sup>''x'' ∂·''J''⟩&nbsp;=&nbsp;0, probing [[off shell]] quantities as well are the [[Ward–Takahashi identity|Ward–Takahashi identities]].\n\n=== Generalization to Lie algebras ===\nSuppose we have two symmetry derivations ''Q''<sub>1</sub> and ''Q''<sub>2</sub>. Then, [''Q''<sub>1</sub>,&nbsp;''Q''<sub>2</sub>] is also a symmetry derivation. Let's see this explicitly. Let's say\n\n:<math>Q_1[\\mathcal{L}]\\approx\\partial_\\mu f_1^\\mu</math>\n\nand\n\n:<math>Q_2[\\mathcal{L}]\\approx\\partial_\\mu f_2^\\mu</math>\n\nThen,\n\n:<math>[Q_1,Q_2][\\mathcal{L}]=Q_1[Q_2[\\mathcal{L}]]-Q_2[Q_1[\\mathcal{L}]]\\approx\\partial_\\mu f_{12}^\\mu</math>\n\nwhere ''f''<sub>12</sub>&nbsp;=&nbsp;''Q''<sub>1</sub>[''f''<sub>2</sub><sup>''μ''</sup>]&nbsp;−&nbsp;''Q''<sub>2</sub>[''f''<sub>1</sub><sup>''μ''</sup>]. So,\n\n:<math>j_{12}^\\mu=\\left(\\frac{\\partial}{\\partial (\\partial_\\mu\\varphi)}\\mathcal{L}\\right)(Q_1[Q_2[\\varphi]]-Q_2[Q_1[\\varphi]])-f_{12}^\\mu.</math>\n\nThis shows we can extend Noether's theorem to larger Lie algebras in a natural way.\n\n=== Generalization of the proof ===\nThis applies to ''any'' local symmetry derivation ''Q'' satisfying ''QS''&nbsp;≈&nbsp;0, and also to more general local functional differentiable actions, including ones where the Lagrangian depends on higher derivatives of the fields. Let ''ε'' be any arbitrary smooth function of the spacetime (or time) manifold such that the closure of its support is disjoint from the boundary. ''ε''&nbsp;is a [[test function]]. Then, because of the variational principle (which does ''not'' apply to the boundary, by the way), the derivation distribution q generated by ''q''[''ε''][Φ(''x'')] = ''ε''(''x'')''Q''[Φ(''x'')] satisfies ''q''[''ε''][''S'']&nbsp;≈&nbsp;0 for every&nbsp;''ε'', or more compactly, ''q''(''x'')[''S'']&nbsp;≈ 0&nbsp;for all ''x'' not on the boundary (but remember that ''q''(''x'') is a shorthand for a derivation ''distribution'', not a derivation parametrized by ''x'' in general). This is the generalization of Noether's theorem.\n\nTo see how the generalization is related to the version given above, assume that the action is the spacetime integral of a Lagrangian that only depends on φ and its first derivatives. Also, assume\n\n:<math>Q[\\mathcal{L}]\\approx\\partial_\\mu f^\\mu</math>\n\nThen,\n\n:<math>\n\\begin{align}\nq[\\varepsilon][\\mathcal{S}] & = \\int q[\\varepsilon][\\mathcal{L}] \\, \\mathrm{d}^n x  \\\\\n& = \\int \\left\\{ \\left(\\frac{\\partial}{\\partial \\varphi}\\mathcal{L}\\right) \\varepsilon Q[\\varphi]+ \\left[\\frac{\\partial}{\\partial (\\partial_\\mu \\varphi)}\\mathcal{L}\\right]\\partial_\\mu(\\varepsilon Q[\\varphi]) \\right\\} \\, \\mathrm{d}^n x \\\\\n& = \\int \\left\\{ \\varepsilon Q[\\mathcal{L}] + \\partial_{\\mu}\\varepsilon \\left[\\frac{\\partial}{\\partial \\left( \\partial_\\mu \\varphi\\right)} \\mathcal{L} \\right] Q[\\varphi] \\right\\} \\, \\mathrm{d}^n x \\\\\n& \\approx \\int \\varepsilon \\partial_\\mu \\left\\{f^\\mu-\\left[\\frac{\\partial}{\\partial (\\partial_\\mu\\varphi)}\\mathcal{L}\\right]Q[\\varphi]\\right\\} \\, \\mathrm{d}^n x\n\\end{align}\n</math>\n\nfor all ''ε''.\n\nMore generally, if the Lagrangian depends on higher derivatives, then\n\n: <math>\n\\begin{align}\n\\partial_\\mu\\left[f^\\mu-\\left[\\frac{\\partial}{\\partial (\\partial_\\mu\\varphi)} \\mathcal{L} \\right] \\right. & \\left. Q[\\varphi] - 2\\left[\\frac{\\partial}{\\partial (\\partial_\\mu \\partial_\\nu \\varphi)}\\mathcal{L}\\right]\\partial_\\nu Q[\\varphi] \\right. \\\\[6pt]\n& \\left. {} + \\partial_\\nu\\left[\\left[\\frac{\\partial}{\\partial (\\partial_\\mu \\partial_\\nu \\varphi)}\\mathcal{L}\\right] Q[\\varphi]\\right]-\\,\\cdots\\right]\\approx 0.\n\\end{align}\n</math>\n\n== Examples ==\n\n=== Example 1: Conservation of energy ===\nLooking at the specific case of a Newtonian particle of mass ''m'', coordinate ''x'', moving under the influence of a potential ''V'', coordinatized by time ''t''. The [[Action (physics)|action]], ''S'', is:\n\n: <math>\n\\begin{align}\n\\mathcal{S}[x] & = \\int L[x(t),\\dot{x}(t)] \\, dt \\\\\n& = \\int \\left(\\frac m 2 \\sum_{i=1}^3\\dot{x}_i^2-V(x(t))\\right) \\, dt.\n\\end{align}\n</math>\n\nThe first term in the brackets is the [[kinetic energy]] of the particle, whilst the second is its [[potential energy]]. Consider the generator of [[time translation]]s ''Q'' = ''d/dt''. In other words, <math>Q[x(t)]=\\dot{x}(t)</math>. Note that ''x'' has an explicit dependence on time, whilst ''V'' does not; consequently:\n\n:<math>Q[L]=m \\sum_i\\dot{x}_i\\ddot{x}_i-\\sum_i\\frac{\\partial V(x)}{\\partial x_i}\\dot{x}_i = \\frac{d}{dt}\\left[\\frac{m}{2}\\sum_i\\dot{x}_i^2-V(x)\\right]</math>\n\nso we can set\n\n:<math>L=\\frac{m}{2} \\sum_i\\dot{x}_i^2-V(x).</math>\n\nThen,\n\n: <math>\n\\begin{align}\nj & = \\sum_{i=1}^3\\frac{\\partial L}{\\partial \\dot{x}_i}Q[x_i]-L \\\\\n& = m \\sum_i\\dot{x}_i^2 -\\left[\\frac{m}{2}\\sum_i\\dot{x}_i^2 -V(x)\\right] \\\\\n& = \\frac{m}{2}\\sum_i\\dot{x}_i^2+V(x).\n\\end{align}\n</math>\n\nThe right hand side is the energy, and Noether's theorem states that <math>\\dot{j}=0</math> (i.e. the principle of conservation of energy is a consequence of invariance under time translations.\n\nMore generally, if the Lagrangian does not depend explicitly on time, the quantity\n\n:<math>\\sum_{i=1}^3 \\frac{\\partial L}{\\partial \\dot{x}_i}\\dot{x_i}-L</math>\n\n(called the [[Hamiltonian mechanics|Hamiltonian]]) is conserved.\n\n=== Example 2: Conservation of center of momentum ===\nStill considering 1-dimensional time, let\n\n: <math>\n\\begin{align}\n\\mathcal{S}[\\vec{x}] & = \\int \\mathcal{L}[\\vec{x}(t),\\dot{\\vec{x}}(t)] \\, \\mathrm{d}t \\\\\n& = \\int \\left [\\sum^N_{\\alpha=1} \\frac{m_\\alpha}{2}(\\dot{\\vec{x}}_\\alpha)^2 -\\sum_{\\alpha<\\beta} V_{\\alpha\\beta}(\\vec{x}_\\beta-\\vec{x}_\\alpha)\\right] \\, \\mathrm{d}t\n\\end{align}\n</math>\n\ni.e. ''N'' Newtonian particles where the potential only depends pairwise upon the relative displacement.\n\nFor <math>\\vec{Q}</math>, consider the generator of Galilean transformations (i.e. a change in the frame of reference). In other words,\n\n:<math>Q_i[x^j_\\alpha(t)]=t \\delta^j_i. </math>\n\nNote that\n\n:<math>\n\\begin{align}\nQ_i[\\mathcal{L}] & = \\sum_\\alpha m_\\alpha \\dot{x}_\\alpha^i-\\sum_{\\alpha<\\beta}\\partial_i V_{\\alpha\\beta}(\\vec{x}_\\beta-\\vec{x}_\\alpha)(t-t) \\\\\n& = \\sum_\\alpha m_\\alpha \\dot{x}_\\alpha^i.\n\\end{align}\n</math>\n\nThis has the form of <math>\\frac{\\mathrm{d}}{\\mathrm{d}t}\\sum_\\alpha m_\\alpha x^i_\\alpha</math> so we can set\n\n:<math>\\vec{f}=\\sum_\\alpha m_\\alpha \\vec{x}_\\alpha.</math>\n\nThen,\n\n:<math>\n\\begin{align}\n\\vec{j} & =\\sum_\\alpha \\left(\\frac{\\partial}{\\partial \\dot{\\vec{x}}_\\alpha} \\mathcal{L}\\right)\\cdot\\vec{Q}[\\vec{x}_\\alpha]-\\vec{f} \\\\[5pt]\n& =\\sum_\\alpha (m_\\alpha \\dot{\\vec{x}}_\\alpha t-m_\\alpha \\vec{x}_\\alpha) \\\\[5pt]\n& =\\vec{P}t-M\\vec{x}_{CM}\n\\end{align}\n</math>\n\nwhere <math>\\vec{P}</math> is the total momentum, ''M'' is the total mass and <math>\\vec{x}_{CM}</math> is the center of mass. Noether's theorem states:\n\n:<math>\\dot{\\vec{j}} = 0 \\Rightarrow {\\vec{P}}-M \\dot{\\vec{x}}_{CM} = 0.</math>\n\n=== Example 3: Conformal transformation ===\n\nBoth examples 1 and 2 are over a 1-dimensional manifold (time). An example involving spacetime is a [[conformal transformation]] of a massless real scalar field with a [[Quartic interaction|quartic potential]] in (3&nbsp;+&nbsp;1)-[[Minkowski spacetime]].\n\n: <math>\n\\begin{align}\n\\mathcal{S}[\\varphi] & =\\int \\mathcal{L}[\\varphi (x),\\partial_\\mu \\varphi (x)] \\, \\mathrm{d}^4x \\\\\n& =\\int \\left( \\frac{1}{2}\\partial^\\mu \\varphi \\partial_\\mu \\varphi -\\lambda \\varphi^4\\right ) \\, \\mathrm{d}^4x\n\\end{align}\n</math>\n\nFor ''Q'', consider the generator of a spacetime rescaling. In other words,\n\n:<math>Q[\\varphi(x)]=x^\\mu\\partial_\\mu \\varphi(x)+\\varphi(x). </math>\n\nThe second term on the right hand side is due to the \"conformal weight\" of <math>\\varphi</math>. Note that\n\n:<math>Q[\\mathcal{L}]=\\partial^\\mu\\varphi\\left(\\partial_\\mu\\varphi+x^\\nu\\partial_\\mu\\partial_\\nu\\varphi+\\partial_\\mu\\varphi\\right)-4\\lambda\\varphi^3\\left(x^\\mu\\partial_\\mu\\varphi+\\varphi\\right).</math>\n\nThis has the form of\n\n:<math>\\partial_\\mu\\left[\\frac{1}{2}x^\\mu\\partial^\\nu\\varphi\\partial_\\nu\\varphi-\\lambda x^\\mu \\varphi^4 \\right] = \\partial_\\mu\\left(x^\\mu\\mathcal{L}\\right)</math>\n\n(where we have performed a change of dummy indices) so set\n\n:<math>f^\\mu=x^\\mu\\mathcal{L}.</math>\n\nThen\n\n: <math>\n\\begin{align}\nj^\\mu & =\\left[\\frac{\\partial}{\\partial\n(\\partial_\\mu\\varphi)}\\mathcal{L}\\right]Q[\\varphi]-f^\\mu \\\\\n& =\\partial^\\mu\\varphi\\left(x^\\nu\\partial_\\nu\\varphi+\\varphi\\right)-x^\\mu\\left( \\frac 1 2 \\partial^\\nu\\varphi\\partial_\\nu\\varphi-\\lambda\\varphi^4\\right).\n\\end{align}\n</math>\n\nNoether's theorem states that <math>\\partial_\\mu j^\\mu = 0 </math> (as one may explicitly check by substituting the Euler–Lagrange equations into the left hand side).\n\nNote that if one tries to find the [[Ward–Takahashi identity|Ward–Takahashi]] analog of this equation, one runs into a problem because of [[anomaly (physics)|anomalies]].\n\n== Applications ==\nApplication of Noether's theorem allows physicists to gain powerful insights into any general theory in physics, by just analyzing the various transformations that would make the form of the laws involved invariant. For example:\n\n* the invariance of physical systems with respect to spatial [[translation (physics)|translation]] (in other words, that the laws of physics do not vary with locations in space) gives the law of conservation of [[linear momentum]];\n* invariance with respect to [[rotation]] gives the law of conservation of [[angular momentum]];\n* invariance with respect to [[time]] translation gives the well-known [[law of conservation of energy]]\n\nIn [[quantum field theory]], the analog to Noether's theorem, the [[Ward–Takahashi identity]], yields further conservation laws, such as the conservation of [[electric charge]] from the invariance with respect to a change in the [[phase factor]] of the [[Complex number|complex]] field of the charged particle and the associated [[gauge invariance|gauge]] of the [[electric potential]] and [[vector potential]].\n\nThe Noether charge is also used in calculating the [[entropy]] of [[stationary black hole]]s.<ref>{{cite journal |author1=Vivek Iyer |author2=Wald |doi=10.1103/PhysRevD.52.4430 |journal=[[Physical Review D]]  |title=A comparison of Noether charge and Euclidean methods for Computing the Entropy of Stationary Black Holes |volume=52 |issue=8 |pages=4430–9 |year=1995 |arxiv=gr-qc/9503052|bibcode = 1995PhRvD..52.4430I }}</ref>\n\n==See also==\n* [[Conservation law]]\n*[[Charge (physics)]]\n*[[Gauge symmetry]]\n*[[Gauge symmetry (mathematics)]]\n*[[Invariant (physics)]]\n*[[Goldstone boson]]\n*[[Symmetry in physics]]\n\n== Notes ==\n{{reflist|37em}}\n\n==References==\n*{{cite book| ISBN=978-3-319-59694-5 |last1=Badin|first1=Gualtiero|last2=Crisciani|first2=Fulvio| title=Variational Formulation of Fluid and Geophysical Fluid Dynamics - Mechanics, Symmetries and Conservation Laws - | publisher=Springer| year=2018 | pages=218 | doi= 10.1007/978-3-319-59695-2}}\n*{{cite book |last=Goldstein |first=Herbert |authorlink=Herbert Goldstein |year=1980 |title= [[Classical Mechanics (textbook)|Classical Mechanics]] |edition=2nd |publisher=Addison-Wesley |location=Reading, MA |isbn= 0-201-02918-9 |pages=588–596 |ref=harv}}\n*{{Cite book | last = Kosmann-Schwarzbach | first = Yvette | authorlink = Yvette Kosmann-Schwarzbach | title = The Noether theorems: Invariance and conservation laws in the twentieth century | publisher = [[Springer Science+Business Media|Springer-Verlag]] | series = Sources and Studies in the History of Mathematics and Physical Sciences | year = 2010 | isbn = 978-0-387-87867-6}} [http://www.math.cornell.edu/~templier/junior/The-Noether-theorems.pdf Online copy].\n*{{cite book | authorlink= Cornelius Lanczos |last=Lanczos |first=C. | year = 1970 | title = The Variational Principles of Mechanics | edition = 4th | publisher = Dover Publications | location = New York | isbn = 0-486-65067-7 | pages = 401–5 |ref=harv}}\n*{{Cite book | last = Olver | first = Peter |author-link=Peter J. Olver | title = Applications of Lie groups to differential equations | publisher = [[Springer Science+Business Media|Springer-Verlag]] | edition = 2nd | series = [[Graduate Texts in Mathematics]] | volume = 107 | year = 1993 | isbn = 0-387-95000-1 }}\n*{{Cite book | last = Sardanashvily | first = G. | author-link=Gennadi Sardanashvily | title = Noether's Theorems. Applications in Mechanics and Field Theory | publisher = [[Springer Science+Business Media|Springer-Verlag]] | year = 2016 | isbn = 978-94-6239-171-0 }}\n\n==External links==\n*{{cite journal\n |author1=Emmy Noether |author2=Mort Tavel (translator)\n |year=1971\n |title=Invariant Variation Problems\n |journal=Transport Theory and Statistical Physics\n |volume=1 |issue=3 |pages=186–207\n |arxiv=physics/0503066\n |doi=10.1080/00411457108231446\n|bibcode = 1971TTSP....1..186N }} (Original in ''Gott. Nachr.'' 1918:235–257)\n*{{cite journal\n |author1=Emmy Noether\n |year=1918\n |title=Invariante Variationsprobleme\n |language=German\n |url=http://de.wikisource.org/wiki/Invariante_Variationsprobleme\n}}\n* {{YouTube|1_MpQG2xXVo|''Emmy Noether and The Fabric of Reality'' (video)}}\n*{{cite arXiv |eprint=physics/9807044 |first=Nina |last=Byers|title=E. Noether's Discovery of the Deep Connection Between Symmetries and Conservation Laws |year=1998}}\n*[[John Baez]] (2002) \"[http://math.ucr.edu/home/baez/noether.html Noether's Theorem in a Nutshell.]\"\n*{{cite journal |author1=Hanca, J. |author2=Tulejab, S. |author3=Hancova, M. |title=Symmetries and conservation laws: Consequences of Noether's theorem |journal=American Journal of Physics |volume=72 |issue=4 |pages=428–35 |year=2004 |doi=  10.1119/1.1591764|url=http://www.eftaylor.com/pub/symmetry.html|bibcode = 2004AmJPh..72..428H }}\n*{{cite journal |author1=Merced Montesinos |author2=Ernesto Flores |journal=Revista Mexicana de Física |title=Symmetric energy–momentum tensor in Maxwell, Yang–Mills, and Proca theories obtained using only Noether's theorem |volume=52 |pages=29–36 |year=2006 |url=http://rmf.smf.mx/pdf/rmf/52/1/52_1_29.pdf |arxiv=hep-th/0602190 |bibcode=2006RMxF...52...29M}}\n*{{cite journal |author1=Vladimir Cuesta |author2=Merced Montesinos |author3=José David Vergara |title=Gauge invariance of the action principle for gauge systems with noncanonical symplectic structures  |journal=Physical Review D |volume=76 |pages=025025 |year=2007 |doi=10.1103/PhysRevD.76.025025 |url=http://journals.aps.org/prd/abstract/10.1103/PhysRevD.76.025025|bibcode = 2007PhRvD..76b5025C }}\n*{{cite journal|author1=Sardanashvily|journal=[[International Journal of Geometric Methods in Modern Physics]]|title=Gauge conservation laws in a general setting. Superpotential |volume=6 |pages=1047 |year=2009 |arxiv=0906.1732|bibcode = 2009arXiv0906.1732S|doi=10.1142/S0219887809003862|issue=06 }}\n* {{cite book | last1 = Neuenschwander | first1 = Dwight E. | title = Emmy Noether's Wonderful Theorem | publisher = Johns Hopkins University Press | year = 2010 | isbn = 978-0-8018-9694-1}}\n<!-- Previously a referenced note; reference is lost, but we can assume this is still a valid citation -->\n*[http://www.mathpages.com/home/kmath564/kmath564.htm Noether's Theorem] at MathPages.\n\n<!-- Categories -->\n[[Category:Articles containing proofs]]\n[[Category:Calculus of variations]]\n[[Category:Conservation laws]]\n[[Category:Concepts in physics]]\n[[Category:Partial differential equations]]\n[[Category:Physics theorems]]\n[[Category:Quantum field theory]]\n[[Category:Symmetry]]\n[[Category:Theoretical physics]]"
    },
    {
      "title": "Obstacle problem",
      "url": "https://en.wikipedia.org/wiki/Obstacle_problem",
      "text": "The '''obstacle problem''' is a classic motivating example in the [[mathematical]] study of [[variational inequalities]] and [[free boundary problem]]s. The problem is to find the [[Mechanical equilibrium|equilibrium]] position of an [[Solid mechanics|elastic membrane]] whose boundary is held fixed, and which is constrained to lie above a given obstacle. It is deeply related to the study of [[minimal surfaces]] and the [[capacity of a set]] in [[potential theory]] as well. Applications include the study of fluid filtration in porous media, constrained heating, elasto-plasticity, optimal control, and financial mathematics.<ref name=\"Caf384\">See {{harvnb|Caffarelli|1998|p=384}}.</ref>\n\nThe mathematical formulation of the problem is to seek minimizers of the [[Dirichlet energy]] functional, \n:<math>J = \\int_D |\\nabla u|^2 \\mathrm{d}x</math>\nin some domain ''<math>D</math>'' where the functions ''<math>u</math>'' represent the vertical displacement of the membrane. In addition to satisfying [[Dirichlet boundary conditions]] corresponding to the fixed boundary of the membrane, the functions ''<math>u</math>'' are in addition constrained to be greater than some given ''obstacle'' function ''<math>\\phi</math>''<math>(x)</math>. The solution breaks down into a region where the solution is equal to the obstacle function, known as the ''contact set,'' and a region where the solution is above the obstacle. The interface between the two regions is the ''free boundary.''\n\nIn general, the solution is continuous and possesses [[Lipschitz continuity|Lipschitz continuous]] first derivatives, but that the solution is generally discontinuous in the second derivatives across the free boundary. The free boundary is characterized as a [[Hölder continuity|Hölder continuous]] surface except at certain singular points, which reside on a smooth manifold.\n\n==Historical note==\n{{quote\n|text=Qualche tempo dopo Stampacchia, partendo sempre dalla sua disequazione variazionale, aperse un nuovo campo di ricerche che si rivelò importante e fecondo. Si tratta di quello che oggi è chiamato il ''problema dell'ostacolo''.<ref>\"Some time after Stampacchia, starting again from his variational inequality, opened a new field of research, which revealed itself as important and fruitful. It is the now called ''obstacle problem''\" (English translation). The [[Italic type]] emphasis is due to the author himself.</ref>\n|sign=[[Sandro Faedo]]\n|source={{harv|Faedo|1986|p=107}}\n}}\n\n==Motivating problems==\n===Shape of a membrane above an obstacle===\nThe obstacle problem arises when one considers the shape taken by a soap film in a domain whose boundary position is fixed (see [[Plateau's problem]]), with the added constraint that the membrane is constrained to lie above some obstacle ''<math>\\phi</math>''<math>(x)</math> in the interior of the domain as well.<ref name=\"Caf383\">See {{harvnb|Caffarelli|1998|p=383}}.</ref> In this case, the energy functional to be minimized is the surface area integral, or\n\n:<math>J(u) = \\int_D \\sqrt{1 + |\\nabla u|^2}\\,\\mathrm{d}x.</math>\n\nThis problem can be ''linearized'' in the case of small perturbations by expanding the energy functional in terms of its [[Taylor series]] and taking the first term only, in which case the energy to be minimized is the standard [[Dirichlet energy]]\n\n:<math>J(u) = \\int_D |\\nabla u|^2 \\mathrm{d}x.</math>\n\n===Optimal stopping===\nThe obstacle problem also arises in [[control theory]], specifically the question of finding the optimal stopping time for a [[stochastic process]] with payoff function ''<math>\\phi</math>''<math>(x)</math>.\n\nIn the simple case wherein the process is [[Brownian motion]], and the process is forced to stop upon exiting the domain, the solution <math>u(x)</math> of the obstacle problem can be characterized as the expected value of the payoff, starting the process at <math>x</math>, if the optimal stopping strategy is followed. The stopping criterion is simply that one should stop upon reaching the ''contact set''.<ref>See the lecture notes by {{harvtxt|Evans|Version 1.2| pp=110–114}}.</ref>\n\n==Formal statement==\nSuppose the following data is given:\n#an [[open set|open]] [[bounded set|bounded]] [[Domain (mathematics)#Real and complex analysis|domain]] <math>D</math> ⊂ ℝ''<sup>n</sup>'' with [[smooth function|smooth]] [[boundary (topology)|boundary]]\n#a [[smooth function]] <math>f (x)</math> on '''∂'''<math>D</math> (the [[boundary (topology)|boundary]] of <math>D</math>)\n#a smooth function ''<math>\\varphi</math>''<math>(x)</math> defined on all of <math>D</math> such that <math>\\scriptstyle\\varphi|_{\\partial D}</math> < <math>f</math>, i.e. the restriction of ''<math>\\varphi</math>''<math>(x)</math> to the boundary of <math>D</math> (its [[Trace operator|trace]]) is less than <math>f</math>.\nThen consider the set\n:<math>K = \\left\\{ u(x) \\in H^1(D): u|_{\\partial D} = f(x)\\text{ and } u \\geq \\varphi \\right\\},</math>\nwhich is a [[closed set|closed]] [[convex set|convex]] [[subset]] of the [[Sobolev space]] of square [[integrable function]]s with square integrable [[weak derivative|weak first derivatives]], containing precisely those functions with the desired boundary conditions which are also above the obstacle. The solution to the obstacle problem is the function which minimizes the energy [[integral]]\n:<math>J(u) = \\int_D |\\nabla u|^2\\mathrm{d}x</math>\nover all functions <math>u(x)</math> belonging to <math>K</math>; the existence of such a minimizer is assured by considerations of [[Hilbert space]] theory.<ref name=\"Caf383\"/><ref>See {{harvnb|Kinderlehrer|Stampacchia|1980|pp=40–41}}.</ref>\n\n==Alternative formulations==\n===Variational inequality===\n{{See also|Variational inequality}}\nThe obstacle problem can be reformulated as a standard problem in the theory of [[variational inequality|variational inequalities]] on [[Hilbert space]]s. Seeking the energy minimizer in the set ''<math>K</math>'' of suitable functions is equivalent to seeking\n\n:<math> u \\in K</math>    '''such that'''   <math>\\int_D\\langle {\\nabla u} , {\\nabla (v - u)}\\rangle \\mathrm{d}x \\geq 0\\qquad\\forall v \\in K, </math>\n\nwhere ⟨ . , . ⟩ : ℝ''<sup>n</sup>'' × ℝ''<sup>n</sup>'' → ℝ is the ordinary [[scalar product]] in the [[Dimension (mathematics)|finite-dimensional]] [[Real number|real]] [[vector space]] ℝ''<sup>n</sup>''. This is a special case of the more general form for variational inequalities on Hilbert spaces, whose solutions are functions ''<math>u</math>'' in some closed convex subset ''<math>K</math>'' of the overall space, such that\n\n:<math>a(u,v-u) \\geq f(v-u)\\qquad\\forall v \\in K.\\,</math>\n\nfor [[Coercive function|coercive]], [[Real numbers|real-valued]], [[bounded operator|bounded]] [[bilinear form]]s <math>a(u,v)</math> and bounded [[linear functional]]s <math>f(v)</math>.<ref name=\"KS-chapter2\">See {{harvnb|Kinderlehrer|Stampacchia|1980|pp=23–49}}.</ref>\n\n===Least superharmonic function===\n{{See also|Superharmonic function|Viscosity solution}}\nA variational argument shows that, away from the contact set, the solution to the obstacle problem is harmonic. A similar argument which restricts itself to variations that are positive shows that the solution is superharmonic on the contact set. Together, the two arguments imply that the solution is a superharmonic function.<ref name=\"Caf384\"/>\n\nIn fact, an application of the [[maximum principle]] then shows that the solution to the obstacle problem is the least superharmonic function in the set of admissible functions.<ref name=\"KS-chapter2\"/>\n\n==Regularity properties==\n[[File:Ondulationamelioree.jpg|thumb|Solution of a one-dimensional obstacle problem. Notice how the solution stays superharmonic (concave down in 1-D), and matches derivatives with the obstacle (which is the <math>C^{1,1}</math> condition)]]\n\n===Optimal regularity===\nThe solution to the obstacle problem has <math>\\scriptstyle C^{1,1}</math> regularity, or [[bounded function|bounded]] [[Derivative#Higher derivatives|second derivative]]s, when the obstacle itself has these properties.<ref>See {{harvnb|Frehse|1972}}.</ref> More precisely, the solution's [[modulus of continuity]] and the modulus of continuity for its [[derivative]] are related to those of the obstacle. \n#If the obstacle <math>\\scriptstyle\\phi(x)</math> has modulus of continuity <math>\\scriptstyle\\sigma(r)</math>, that is to say that <math>\\scriptstyle|\\phi(x) - \\phi(y)|\\leq \\sigma(|x-y|)</math>, then the solution <math>\\scriptstyle u(x)</math> has modulus of continuity given by <math>\\scriptstyle C\\sigma(2r)</math>, where the constant depends only on the domain and not the obstacle.\n#If the obstacle's first derivative has modulus of continuity <math>\\scriptstyle\\sigma(r)</math>, then the solution's first derivative has modulus of continuity given by <math>\\scriptstyle C r \\sigma(2r)</math>, where the constant again depends only on the domain.<ref>See {{harvnb|Caffarelli|1998|p=386}}.</ref>\n\n===Level surfaces and the free boundary===\nSubject to a degeneracy condition, level sets of the difference between the solution and the obstacle, <math>\\scriptstyle\\{x: u(x) -\\phi(x) = t\\}</math> for <math>\\scriptstyle t > 0</math> are <math>\\scriptstyle C^{1,\\alpha}</math> surfaces. The free boundary, which is the boundary of the set where the solution meets the obstacle, is also <math>\\scriptstyle C^{1,\\alpha}</math> except on a set of ''singular points,'' which are themselves either isolated or locally contained on a <math>\\scriptstyle C^1</math> manifold.<ref>See {{harvnb|Caffarelli|1998|p=394 and 397}}.</ref>\n\n==Generalizations==\nThe theory of the obstacle problem is extended to other divergence form uniformly [[elliptic operator]]s,<ref name=\"KS-chapter2\"/> and their associated energy functionals. It can be generalized to degenerate elliptic operators as well.\n\nThe double obstacle problem, where the function is constrained to lie above one obstacle function and below another, is also of interest.\n\nThe [[Signorini problem]] is a variant of the obstacle problem, where the energy functional is minimized subject to a constraint which only lives on a surface of one lesser dimension, which includes the ''boundary obstacle problem'', where the constraint operates on the boundary of the domain.\n\nThe [[parabolic partial differential equation|parabolic]], time-dependent cases of the obstacle problem and its variants are also objects of study.\n\n== See also ==\n*[[Barrier option]]\n*[[Minimal surface]]\n*[[Variational inequality]]\n*[[Signorini problem]]\n\n==Notes==\n{{reflist|30em}}\n\n==Historical references==\n\n*{{Citation\n| first = Sandro\n| last = Faedo\n| author-link = Sandro Faedo\n| editor-last = Montalenti\n| editor-first = G.\n| editor2-last = Amerio\n| editor2-first = L. \n| editor2-link = Luigi Amerio\n| editor3-last =Acquaro\n| editor3-first =G.\n| editor4-last = Baiada\n| editor4-first = E.\n| editor5-last = Cesari\n| editor5-first = L.\n| editor5-link =Lamberto Cesari\n| editor6-last = Ciliberto \n| editor6-first= C.\n| editor7-last = Cimmino \n| editor7-first = G.\n| editor7-link = Gianfranco Cimmino\n| editor8-last = Cinquini \n| editor8-first = S.\n| editor9-last = De Giorgi\n| editor9-first = Ennio\n| editor9-link = Ennio De Giorgi\n| editor10-last = Faedo\n| editor10-first = S.\n| editor10-link = Sandro Faedo\n| editor11-last = Fichera\n| editor11-first = G.\n| editor11-link = Gaetano Fichera \n| editor12-last = Galligani\n| editor12-first = I.\n| editor13-last = Ghizzetti\n| editor13-first = A.\n| editor13-link = Aldo Ghizzetti \n| editor14-last = Graffi\n| editor14-first = D.\n| editor14-link = Dario Graffi\n| editor15-last = Greco\n| editor15-first = D.\n| editor15-link = Donato Greco\n| editor16-last = Grioli\n| editor16-first = G.\n| editor16-link = Giuseppe Grioli\n| editor17-last = Magenes\n| editor17-first = E.\n| editor17-link = Enrico Magenes \n| editor18-last = Martinelli\n| editor18-first = E. \n| editor18-link = Enzo Martinelli \n| editor19-last = Pettineo\n| editor19-first = B.\n| editor20-last = Scorza\n| editor20-first = G. \n| editor20-link = Giuseppe Scorza Dragoni\n| editor21-last = Vesentini\n| editor21-first = E.\n| editor21-link = Edoardo Vesentini\n| display-editors =4\n| contribution = Leonida Tonelli e la scuola matematica pisana \n| title = Convegno celebrativo del centenario della nascita di Mauro Picone e Leonida Tonelli (6–9 maggio 1985)\n| language = Italian\n| url = http://www.lincei.it/pubblicazioni/catalogo/volume.php?lg=e&rid=32847\n| series = Atti dei Convegni Lincei\n| volume = 77\n| year = 1986\n| pages = 89–109\n| place = Roma\n| publisher = [[Accademia Nazionale dei Lincei]]\n| doi = \n}}. \"''Leonida Tonelli and the Pisa mathematical school''\" is a survey of the work of Tonelli in [[Pisa]] and his influence on the development of the school, presented at the ''International congress in occasion of the celebration of the centenary of birth of Mauro Picone and Leonida Tonelli'' (held in [[Rome]] on May 6–9, 1985). The Author was one of his pupils and, after his death, held his chair of mathematical analysis at the [[University of Pisa]], becoming dean of the faculty of sciences and then rector: he exerted a strong positive influence on the development of the university.\n\n==References==\n*{{Citation\n| last=Caffarelli\n| first=Luis\n| author-link=Luis Caffarelli\n| date=July 1998\n| title=The obstacle problem revisited\n| journal=The Journal of Fourier Analysis and Applications\n| volume=4\n| issue=4–5\n| pages=383–402\n| url=http://www.springerlink.com/content/u2105854886n6282/\n| doi=10.1007/BF02498216\n| mr = 1658612\n| zbl =0928.49030\n}}\n*{{Citation\n| last=Evans\n| first=Lawrence \n| author-link=Lawrence C. Evans\n| title=An Introduction to Stochastic Differential Equations\n| url=http://math.berkeley.edu/~evans/SDE.course.pdf\n| pages=130\n| accessdate=July 11, 2011\n}}. A set of lecture notes surveying \"''without too many precise details, the basic theory of probability, random differential equations and some applications''\", as the author himself states.\n*{{Citation\n| last=Frehse\n| first=Jens\n| author-link=Jens Frehse\n| title=On the regularity of the solution of a second order variational inequality\n| year=1972\n| periodical=Bolletino della Unione Matematica Italiana\n| series = Serie IV,\n| volume=6 \n| issue=\n| pages=312–315\n| id=\n| mr=318650\n| zbl= 0261.49021\n}}.\n*{{Citation\n| last=Friedman\n| first=Avner\n| author-link=Avner Friedman\n| title=Variational principles and free boundary problems\n| publisher=[[John Wiley & Sons]]\n| place=New York\n| isbn=0-471-86849-3\n| year=1982\n| series=Pure and Applied Mathematics\n| pages=ix+710\n| mr= 0679313 \n| zbl= 0564.49002\n}}.\n*{{Citation \n| last=Kinderlehrer \n| first=David \n| author-link=David Kinderlehrer\n| last2=Stampacchia \n| first2=Guido \n| author2-link=Guido Stampacchia\n| title=An Introduction to Variational Inequalities and Their Applications \n| series= Pure and Applied Mathematics\n| volume=88\n| publisher=[[Academic Press]] \n| place=New York \n| isbn=0-12-407350-6\n| year=1980\n| pages=xiv+313\n| mr =0567696 \n| zbl=0457.35001\n}}\n*{{citation|last=Petrosyan|first=Arshak|last2=Shahgholian|first2=Henrik|last3=Uraltseva|first3=Nina|title=Regularity of Free Boundaries in Obstacle-Type Problems. Graduate Studies in Mathematics,|publisher=American Mathematical Society, Providence, RI|year=2012|isbn=0-8218-8794-7 }}\n\n== External links ==\n*{{Citation\n  | last = Caffarelli\n  | first = Luis\n  | author-link = Luis Caffarelli\n  | title = The Obstacle Problem\n  | place = \n  | publisher = \n  | series = draft from the [[Fermi Lecture]]s\n  |date=August 1998\n  | pages = 45\n  | language = \n  | url = http://www.ma.utexas.edu/users/combs/obstacle-long.pdf\n  | accessdate = July 11, 2011\n}}, delivered by the author at the [[Scuola Normale Superiore]] in 1998.\n\n[[Category:Partial differential equations]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Ostrogradsky instability",
      "url": "https://en.wikipedia.org/wiki/Ostrogradsky_instability",
      "text": "In applied mathematics, the '''Ostrogradsky instability''' is a consequence of a theorem of [[Mikhail Ostrogradsky]] in [[classical mechanics]] according to which a non-degenerate [[Lagrangian mechanics|Lagrangian]] dependent on time derivatives higher than the first corresponds to a linearly unstable [[Hamiltonian mechanics#Mathematical formalism|Hamiltonian]] associated with the Lagrangian via a [[Legendre transformation#Hamilton-Lagrange mechanics|Legendre transform]]. The Ostrogradsky instability has been proposed as an explanation as to why no differential equations of higher order than two appear to describe physical phenomena.<ref>{{Cite journal |title= Third-order equations of motion and the Ostrogradsky instability |journal=Physical Review D|volume=91|issue=8|arxiv=1411.3721|doi=10.1103/PhysRevD.91.085009|year=2015|last1=Motohashi|first1=Hayato|last2=Suyama|first2=Teruaki}}</ref>\n\n==Outline of proof <ref>{{Cite book |title=The Invisible Universe: Dark Matter and Dark Energy |volume=720 |pages=403–433 |arxiv=astro-ph/0601672|doi=10.1007/978-3-540-71013-4_14 |chapter=Avoiding Dark Energy with 1/R Modifications of Gravity |series=Lecture Notes in Physics |isbn=978-3-540-71012-7 |year=2007 |url=http://cds.cern.ch/record/925754/files/0601672.pdf|last1=Woodard |first1=R.P. }}</ref>==\n\nThe main points of the proof can be made clearer by considering a one-dimensional system with a Lagrangian <math>L(q,{\\dot q}, {\\ddot q})</math>. The [[Euler–Lagrange equation]] is\n\n:<math> \\frac{dL}{dq} - \\frac{d}{dt} \\frac{dL}{d{\\dot q}}+ \\frac{d^2}{dt^2}\\frac{dL}{d{\\ddot q}} = 0.</math>\n\nNon-degeneracy of <math>L</math> means that the [[canonical coordinates]] can be expressed in terms of the derivatives of <math>{q}</math> and vice versa. Thus, <math>dL/d{\\ddot q}</math> is a function of <math>{\\ddot q}</math> (if it was not, the [[Jacobian matrix and determinant|Jacobian]] <math>\\det[d^2 L/(d{\\ddot q_i}\\, d{\\ddot q}_j)]</math> would vanish, which would mean that <math>L</math> is degenerate), meaning that we can write <math>q^{(4)} = F(q,{\\dot q}, {\\ddot q}, q^{(3)})</math> or, inverting, <math>q = G(t, q_0, {\\dot q}_0, {\\ddot q}_0, q^{(3)}_0)</math>. Since the evolution of <math>q</math> depends upon four initial parameters, this means that there are four canonical coordinates. We can write those as\n\n:<math>Q_1 : = q</math>\n:<math>Q_2 : = {\\dot q}</math>\n\nand by using the definition of the conjugate momentum,\n\n:<math>P_1 : = \\frac{dL}{d{\\dot q}} - \\frac{d}{dt} \\frac{dL}{d{\\ddot q}}</math>\n:<math>P_2 : =  \\frac{dL}{d{\\ddot q}} </math>\n\nDue to non-degeneracy, we can write <math> {\\ddot q}</math> as <math>{\\ddot q} = a(Q_1, Q_2, P_2)</math>. Note that only ''three'' arguments are needed since the Lagrangian itself only has three free parameters. By Legendre transforming, we find the Hamiltonian to be\n\n:<math>H = P_1 Q_2 - P_2  a(Q_1, Q_2, P_2) - L </math>\n\nWe now notice that the Hamiltonian is linear in <math>P_1</math>. This is Ostrogradsky's instability, and it stems from the fact that the Lagrangian depends on fewer coordinates than there are canonical coordinates (which correspond to the initial parameters needed to specify the problem). The extension to higher dimensional systems is analogous, and the extension to higher derivatives simply means that the phase space is of even higher dimension than the configuration space, which exacerbates the instability (since the Hamiltonian is linear in even more canonical coordinates).\n\n==Notes==\n{{Reflist}}\n\n[[Category:Lagrangian mechanics]]\n[[Category:Hamiltonian mechanics]]\n[[Category:Calculus of variations]]\n[[Category:Mathematical physics]]"
    },
    {
      "title": "Palais–Smale compactness condition",
      "url": "https://en.wikipedia.org/wiki/Palais%E2%80%93Smale_compactness_condition",
      "text": "The '''Palais–Smale compactness condition''', named after [[Richard Palais]] and [[Stephen Smale]], is a hypothesis for some theorems of the [[calculus of variations]].  It is useful for guaranteeing the existence of certain kinds of [[critical point (mathematics)|critical point]]s, in particular [[saddle point]]s. The Palais-Smale condition is a condition on the [[functional (mathematics)|functional]] that one is trying to extremize.  \n\nIn finite-dimensional spaces, the Palais–Smale condition for a continuously differentiable real-valued function is satisfied automatically for [[proper map]]s: functions which do not take unbounded sets into bounded sets.  In the calculus of variations, where one is typically interested in infinite-dimensional [[function space]]s, the condition is necessary because some extra notion of [[compactness]] beyond simple boundedness is needed.  See, for example, the proof of the [[mountain pass theorem]] in section 8.5 of Evans.\n\n== Strong formulation ==\n\nA continuously [[Fréchet derivative|Fréchet differentiable]] [[functional (mathematics)|functional]] <math>I\\in C^1(H,\\mathbb{R})</math> from a [[Hilbert space]] ''H'' to the [[real number|reals]] satisfies the Palais-Smale condition if every [[sequence]] <math>\\{u_k\\}_{k=1}^\\infty\\subset H</math> such that:\n* <math>\\{I[u_k]\\}_{k=1}^\\infty</math> is bounded, and\n* <math>I'[u_k]\\rightarrow 0</math> in ''H''\nhas a convergent subsequence in ''H''.\n\n== Weak formulation ==\nLet ''X'' be a [[Banach space]] and <math>\\Phi\\colon X\\to\\mathbf R</math> be a [[Gateaux derivative|Gateaux differentiable]] functional. The functional <math>\\Phi</math> is said to satisfy the '''weak Palais-Smale condition''' if for each sequence <math>\\{x_n\\}\\subset X</math> such that\n* <math>\\sup |\\Phi(x_n)|<\\infty</math>,\n* <math>\\lim\\Phi'(x_n)=0</math> in <math>X^*</math>,\n* <math>\\Phi(x_n)\\neq0</math> for all <math>n\\in\\mathbf N</math>,\n\nthere exists a critical point <math>\\overline x\\in X</math> of <math>\\Phi</math> with\n:<math>\\liminf\\Phi(x_n)\\le\\Phi(\\overline x)\\le\\limsup\\Phi(x_n).</math>\n\n== References ==\n\n* {{cite book | first=Lawrence C. | last=Evans | title=Partial Differential Equations | publisher=American Mathematical Society | location=Providence, Rhode Island | year=1998 | isbn=0-8218-0772-2}}\n\n{{DEFAULTSORT:Palais-Smale compactness condition}}\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Path of least resistance",
      "url": "https://en.wikipedia.org/wiki/Path_of_least_resistance",
      "text": "{{For|the song \"Path of Least Resistance\" by Modest Mouse|Sad Sappy Sucker}}\n{{refimprove|date = September 2016}}\n[[Image:Cartoon_mountain_pass_symbolizing_path_of_least_resistance.png|thumb|250px|Hikers choose the easy way to cross hills.]]\n\nThe '''path of least resistance''' is the physical or metaphorical pathway that provides the least resistance to forward motion by a given object or entity, among a set of alternative paths.  The concept is often used to describe why an object or entity takes a given path. The way in which water flows is often given as an example for the idea.\n\n[[File:Path of least resistance.jpg|thumb|Bicycle traffic barrier used to slow down cyclists circumvented by a detour in the form of a [[desire path]], thereby showing a literal path of least resistance.]]\n\n==Description==\nIn [[physics]], the \"path of least resistance\" is a [[heuristic]] from [[folk physics]] that can sometimes, in very simple situations, describe approximately what happens.  It is an approximation of the tendency to the [[least energy state]].<ref name=\"Weissman2012\">{{Cite book|author=David Weissman|title=Cage, The: Must, Should, and Ought from Is|url=https://books.google.com/books?id=iyk7hognVysC&pg=PA68|date=1 February 2012|publisher=SUNY Press|isbn=978-0-7914-8119-6|page=68}}</ref> Other examples are \"what goes up must come down\" ([[gravity]]) and \"heat goes from hot to cold\" ([[second law of thermodynamics]]). But these simple descriptions are not derived from laws of physics and in more complicated cases these heuristics will fail to give even approximately correct results.\n\nIn electrical circuits, for example, the current always follows all available paths, and in some simple cases the \"path of least resistance\" will take up most of the current, but this will not be generally true in even slightly more complicated circuits. It may seem for example, that if there are three paths of approximately equal resistance, the majority of the current will flow down one of the three paths. However, due to electrons repelling each other the total path of least resistance is in fact to have approximate equal current flowing through each path. The reason for this is that three paths made of equally conductive wire will have a total resistance that is one-third of the single path. In conclusion, the current is always distributed over all possible paths inversely proportional to their resistance.\n\nThe path of least resistance is also used to describe certain human behaviors, although with much less specificity than in the strictly physical sense.  In these cases, resistance is often used as a [[metaphor]] for personal effort or confrontation; a person taking the path of least resistance avoids these.  In-library science and technical writing, information is ideally arranged for users according to the [[principle of least effort]], or the \"path of least resistance\".  Recursive navigation systems are an example of this.\n\nThe path of least resistance applies on a local, not global, reference.  For example, water always flows downhill, regardless of whether briefly flowing uphill will help it gain a lower final altitude (with certain exceptions such as [[superfluid]]s and [[siphon]]s).  In physics, this phenomenon allows the formation of [[potential well]]s, where [[potential energy]] is stored because of a barrier restricting flow to a lower energy state.\n\n==See also==\n* [[Folk physics]]\n* [[Calculus of variations]]\n* [[Mountain pass theorem]]\n* [[Principle of least action]]\n* [[Principle of least effort]]\n* [[Variational principle]]\n* [[Gradient descent]]\n* [[Natural lines of drift]]\n* [[Desire path]]\n\n==References==\n{{Reflist}}\n\n[[Category:Calculus of variations]]\n\n[[it:Meccanica classica#Principio di minima azione]]"
    },
    {
      "title": "Plateau's problem",
      "url": "https://en.wikipedia.org/wiki/Plateau%27s_problem",
      "text": "In [[mathematics]], '''Plateau's problem''' is to show the existence of a [[minimal surface]] with a given boundary, a problem raised by  [[Joseph-Louis Lagrange]] in 1760.  However, it is named after [[Joseph Plateau]] who experimented with [[soap film]]s.  The problem is considered part of the [[calculus of variations]].  The existence and regularity problems are part of [[geometric measure theory]].\n\n==History==\nVarious specialized forms of the problem were solved, but it was only in 1930 that general solutions were found in the context of mappings (immersions) independently by [[Jesse Douglas]] and [[Tibor Radó]].  Their methods were quite different; Radó's work built on the previous work of René Garnier and held only for [[rectifiable curve|rectifiable]] simple closed curves, whereas Douglas used completely new ideas with his result holding for an arbitrary simple closed curve.  Both relied on setting up minimization problems; Douglas minimized the now-named Douglas integral while Radó minimized the \"energy\".  Douglas went on to be awarded the [[Fields Medal]] in 1936 for his efforts.\n\n==In higher dimensions==\nThe extension of the problem to higher [[dimension]]s (that is, for ''k''-dimensional surfaces in ''n''-dimensional space) turns out to be much more difficult to study. Moreover, while the solutions to the original problem are always regular, it turns out that the solutions to the extended problem may have [[mathematical singularity|singularities]] if ''k''&nbsp;≤&nbsp;''n''&nbsp;&minus;&nbsp;2. In the [[hypersurface]] case where ''k''&nbsp;=&nbsp;''n''&nbsp;&minus;&nbsp;1, singularities occur only for ''n''&nbsp;≥&nbsp;8.\n\nTo solve the extended problem in special cases, the [[Caccioppoli set#De Giorgi definition|theory of perimeters]] ([[Ennio de Giorgi|De Giorgi]]) for codimension 1 and the theory of [[rectifiable current]]s ([[Herbert Federer|Federer]] and Fleming) for higher codimension have been developed. Multidimensional Plateau problem in the class of spectral  surfaces (parametrized by the spectra of the manifolds with a fixed boundary) was solved in 1969 by [[Anatoly Fomenko]].\n\n==Physical applications==\nPhysical soap films are more accurately modeled by the (M,0,delta)-minimal sets of [[Frederick Almgren]], but the lack of a compactness theorem makes it difficult to prove the existence of an area minimizer. In this context, a persistent open question has been the existence of a least-area soap film. [[Ernst Robert Reifenberg]] solved such a \"universal Plateau's problem\" for boundaries which are homeomorphic to single embedded spheres. In his book Almgren claimed to use [[varifold]]s to solve the problem for more than one sphere, as well as more general boundaries, but Allard's compactness theorem for integral varifolds produces a minimal surface, not necessarily an area minimizer. In 2015 [[Jenny Harrison]] and [[Harrison Pugh]] used cohomology to define spanning sets and Hausdorff measure weighted by a bounded Lipschitz function to define area, and solved the problem in this setting. Their paper is currently under review.\n\n==See also==\n* [[Double Bubble conjecture]]\n* [[Dirichlet principle]]\n* [[Plateau's laws]]\n* [[Stretched grid method]]\n\n==References==\n* {{cite journal\n | last = Douglas | first = Jesse\n | authorlink = Jesse Douglas\n | title = Solution of the problem of Plateau\n | journal = Trans. Amer. Math. Soc.\n | volume = 33\n | year = 1931\n | issue = 1\n | pages = 263–321\n | doi = 10.2307/1989472\n | jstor = 1989472\n }}\n* {{cite journal\n | last = Reifenberg | first = Ernst Robert\n | authorlink = Ernst Robert Reifenberg\n | title = Solution of the {Plateau} problem for m-dimensional surfaces of varying topological type\n | journal = Acta Mathematica\n | volume = 104\n | year = 1960\n | issue = 2\n | pages = 1–92\n | doi = 10.1007/bf02547186\n }}\n* {{cite book\n  | last = Fomenko | first = A.T. | title = The Plateau Problem: Historical Survey\n  | publisher = Gordon & Breach | year = 1989 | location = Williston, VT\n  | isbn = 978-2-88124-700-2}}\n* {{cite book\n  | last = Morgan | first = Frank | title = Geometric Measure Theory: a Beginner's Guide\n  | publisher = Academic Press | year = 2009\n  | isbn = 978-0-12-374444-9}}\n*{{springer|author=O'Neil, T.C.|id=G/g130040|title=Geometric Measure Theory}}\n* {{cite journal\n | first = Tibor | last = Radó\n | authorlink = Tibor Radó\n | title = On Plateau's problem\n | journal = Ann. of Math. |series =  2\n | volume = 31\n | year = 1930\n | pages = 457–469\n | doi = 10.2307/1968237\n | jstor = 1968237\n | issue = 3\n }}\n* {{cite book\n  | last = Struwe | first = Michael | title = Plateau's Problem and the Calculus of Variations\n  | publisher = Princeton University Press | year = 1989 | location = Princeton, NJ\n  | isbn = 978-0-691-08510-4\n}}\n* {{cite book\n  | last = Almgren | first = Frederick \n  | authorlink = Frederick Almgren\n  | title = Plateau's problem, an invitation to varifold geometry \n  | publisher = Benjamin | year = 1966 | location = New York-Amsterdam\n  | isbn =  978-0-821-82747-5 \n}}\n* {{cite journal\n  | last = Harrison | first = Jenny \n| title = Soap Film Solutions to Plateau's Problem\n  | journal = Journal of Geometric Analysis | year = 2012 | doi=10.1007/s12220-012-9337-x\n  | volume=24\n  | pages=271–297\n| arxiv=1106.5839}}\n\n{{PlanetMath attribution|id=4286|title=Plateau's Problem}}\n\n[[Category:Calculus of variations]]\n[[Category:Minimal surfaces]]\n[[Category:Mathematical problems]]"
    },
    {
      "title": "Principle of least action",
      "url": "https://en.wikipedia.org/wiki/Principle_of_least_action",
      "text": "{{classical mechanics}}\n\n:''This article discusses the history of the principle of least action. For the application, please refer to [[action (physics)]].''\nThe '''principle of least action''' – or, more accurately, the '''principle of stationary action''' – is a [[variational principle]] that, when applied to the [[action (physics)|action]] of a [[mechanics|mechanical]] system, can be used to obtain the [[equations of motion]] for that system. In relativity, a different action must be minimized or maximized. The principle can be used to derive [[Newtonian mechanics|Newtonian]], [[Lagrangian mechanics|Lagrangian]] and [[Hamiltonian mechanics|Hamiltonian]] [[equations of motion]], and even [[general relativity]] (see [[Einstein–Hilbert action]]). The physicist Paul Dirac<ref>P.A.M. Dirac, Physikalische Zeitschrift der Sowjetunion, Band 3, Heft 1, 64\n(1933). A translated version can be found in; Julian S. Schwinger, Selected Papers on Electrodynamics, Dover Publications Inc.(1958), pp. 312.{{ISBN|9780883076484}}</ref>, and after him Julian Schwinger and Richard Feynman, demonstrated how this principle can also be used in quantum calculations.<ref>R. Feynman, Quantum Mechanics, And Path Integrals, McGraw-Hill Companies (1965), {{ISBN|0070206503}}</ref><ref>R. J. S. Schwinger, Quantum Kinematics and Dynamics, W. A. Benjamin Publishers (1970), {{ISBN|0738203033}}</ref>\nIt was historically called \"least\" because its solution requires finding the path that has the least value.<ref name=\":0\">Chapter 19 of Volume II, [[Richard Feynman|Feynman R]], [[Robert B. Leighton|Leighton R]], and [[Matthew Sands|Sands M.]] ''The Feynman Lectures on Physics ''. 3 volumes 1964, 1966. Library of Congress Catalog Card No. 63-20717. {{ISBN|0-201-02115-3}} (1970 paperback three-volume set); {{ISBN|0-201-50064-7}} (1989 commemorative hardcover three-volume set); {{ISBN|0-8053-9045-6}} (2006 the definitive edition (2nd printing); hardcover)</ref> Its classical mechanics and electromagnetic expressions are a consequence of quantum mechanics, but the stationary action method helped in the development of quantum mechanics.<ref>[[Richard Feynman]], ''[[The Character of Physical Law]]''.</ref>\n\nThe principle remains central in [[modern physics]] and [[mathematics]], being applied in [[thermodynamics]],<ref>{{cite journal |doi=10.1016/j.aop.2008.04.007 |title=Thermodynamics based on the principle of least abbreviated action: Entropy production in a network of coupled oscillators |journal=Annals of Physics |volume=323 |issue=8 |pages=1844–58 |year=2008 |last1=García-Morales |first1=Vladimir |last2=Pellicer |first2=Julio |last3=Manzanares |first3=José A. |bibcode=2008AnPhy.323.1844G |arxiv=cond-mat/0602186 }}</ref> [[fluid mechanics]],<ref>http://www.scholarpedia.org/article/Principle_of_least_action</ref> the [[theory of relativity]], [[quantum mechanics]],<ref>{{cite journal |bibcode=1942PhDT.........5F |title=The Principle of Least Action in Quantum Mechanics |author1=Feynman |first1=Richard Phillips |year=1942 }}</ref> [[particle physics]], and [[string theory]]<ref>[http://www.damtp.cam.ac.uk/user/db275/LeastAction.pdf Principle of Least Action – damtp]</ref> and is a focus of modern mathematical investigation in [[Morse theory]]. [[Maupertuis' principle]] and [[Hamilton's principle]] exemplify the principle of stationary action.\n\nThe action principle is preceded by earlier ideas in [[optics]]. In [[ancient Greece]], [[Euclid]] wrote in his ''Catoptrica'' that, for the path of light reflecting from a mirror, the [[angle of incidence (optics)|angle of incidence]] equals the [[angle of reflection]].{{Citation needed|date=January 2015}} [[Hero of Alexandria]] later showed that this path was the shortest length and least time.<ref>{{cite book\n|last=Kline|first=Morris\n|title=Mathematical Thought from Ancient to Modern Times\n|publisher=Oxford University Press|location=New York\n|date=1972|pages= 167–68|isbn=0-19-501496-0}}</ref>\n\nScholars often credit [[Pierre Louis Maupertuis]] for formulating the principle of least action because he wrote about it in 1744<ref name=\"mau44\">P.L.M. de Maupertuis, ''[[s:fr:Accord de différentes loix de la nature qui avoient jusqu’ici paru incompatibles|Accord de différentes lois de la nature qui avaient jusqu'ici paru incompatibles.]]'' (1744) Mém. As. Sc. Paris p. 417. ([[s:Accord between different laws of Nature that seemed incompatible|English translation]])</ref> and 1746.<ref name=\"mau46\">P.L.M. de Maupertuis, ''[[s:fr:Les loix du mouvement et du repos déduites d'un principe metaphysique|Le lois de mouvement et du repos, déduites d'un principe de métaphysique.]]'' (1746) Mém. Ac. Berlin, p. 267.([[s:Derivation of the laws of motion and equilibrium from a metaphysical principle|English translation]])</ref> However, [[Leonhard Euler]] discussed the principle in 1744,<ref name=\"eul44\">Leonhard Euler, ''Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes.'' (1744) Bousquet, Lausanne &amp; Geneva. 320 pages. Reprinted in ''Leonhardi Euleri Opera Omnia: Series I vol 24.'' (1952) C. Cartheodory (ed.) Orell Fuessli, Zurich. [http://math.dartmouth.edu/~euler/pages/E065.html Scanned copy of complete text] at ''[http://math.dartmouth.edu/~euler/ The Euler Archive]'', Dartmouth.</ref> and evidence shows that [[Gottfried Leibniz]] preceded both by 39 years.<ref name=\"oco03\">J J O'Connor and E F Robertson, \"[http://www-history.mcs.st-andrews.ac.uk/history/HistTopics/Forgery_2.html The Berlin Academy and forgery]\", (2003), at ''[http://www-history.mcs.st-andrews.ac.uk/history/ The MacTutor History of Mathematics archive]''.</ref><ref name=\"ger98\">Gerhardt CI. (1898) \"Über die vier Briefe von Leibniz, die Samuel König in dem Appel au public, Leide MDCCLIII, veröffentlicht hat\", ''Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften'', '''I''', 419–427.</ref><ref name=\"kab13\">Kabitz W. (1913) \"Über eine in Gotha aufgefundene Abschrift des von S. König in seinem Streite mit Maupertuis und der Akademie veröffentlichten, seinerzeit für unecht erklärten Leibnizbriefes\", ''Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften'', '''II''', 632–638.</ref>\n\nIn 1933, [[Paul Dirac]] discerned the [[Path integral formulation#Quantum action principle|quantum mechanical underpinning]] of the principle in the [[Interference (wave propagation)#Quantum interference|quantum interference]] of amplitudes.<ref>{{cite journal |last=Dirac |first=Paul A. M. |authorlink=Paul Dirac |year=1933 |title=The Lagrangian in Quantum Mechanics |journal=Physikalische Zeitschrift der Sowjetunion |volume=3 |pages=64–72 |url=http://www.hep.anl.gov/czachos/soysoy/Dirac33.pdf}}</ref>\n\n==General statement==\n[[File:Least action principle.svg|250px|thumb|As the system evolves, '''q''' traces a path through [[configuration space (physics)|configuration space]] (only some are shown). The path taken by the system (red) has a stationary action (''δS'' = 0) under small changes in the configuration of the system (''δ'''''q''').<ref name=penrose>{{cite book |author=R. Penrose| title=[[The Road to Reality]]| publisher= Vintage books| year=2007 | page = 474|isbn=0-679-77631-1}}</ref>]]\n\nThe starting point is the ''[[action (physics)|action]]'', denoted <math> \\mathcal{S} </math> (calligraphic S), of a physical system. It is defined as the [[integral (mathematics)|integral]] of the [[Lagrangian mechanics|Lagrangian]] ''L'' between two instants of [[time in physics|time]] ''t''<sub>1</sub> and ''t''<sub>2</sub> – technically a [[functional (mathematics)|functional]] of the ''N'' [[generalized coordinates]] '''q''' = (''q''<sub>1</sub>, ''q''<sub>2</sub>, ... , ''q<sub>N</sub>'') which define the [[Configuration space (physics)|configuration]] of the system:\n\n:<math> \\mathbf{q} : \\mathbf{R} \\to \\mathbf{R}^N </math>\n:<math> \\mathcal{S}[\\mathbf{q}, t_1, t_2] \n= \\int_{t_1}^{t_2} L(\\mathbf{q}(t),\\mathbf{\\dot{q}}(t), t) dt </math>\n\nwhere the dot denotes the [[time derivative]], and ''t'' is time.\n\nMathematically the principle is<ref>Encyclopaedia of Physics (2nd Edition), R.G. Lerner, G.L. Trigg, VHC publishers, 1991, ISBN (Verlagsgesellschaft) 3-527-26954-1, ISBN (VHC Inc.) 0-89573-752-3</ref><ref>McGraw Hill Encyclopaedia of Physics (2nd Edition), C.B. Parker, 1994, {{ISBN|0-07-051400-3}}</ref><ref name=\"Analytical Mechanics 2008\">Analytical Mechanics, L.N. Hand, J.D. Finch, Cambridge University Press, 2008, {{ISBN|978-0-521-57572-0}}</ref>\n\n:<math> \\delta \\mathcal{S} = 0 ,</math>\n\nwhere ''δ'' (lowercase Greek [[Delta (letter)|delta]]) means a ''small'' change. In words this reads:<ref name=penrose/>\n\n:''The path taken by the system between times t<sub>1</sub> and t<sub>2</sub> and configurations q<sub>1</sub> and q<sub>2</sub> is the one for which the '''action''' is '''stationary (no change)''' to '''first order'''.''\n\nIn applications the statement and definition of action are taken together:<ref>Classical Mechanics, T.W.B. Kibble, European Physics Series, McGraw-Hill (UK), 1973, {{ISBN|0-07-084018-0}}</ref>\n\n:<math> \\delta \\int_{t_1}^{t_2} L(\\mathbf{q}, \\mathbf{\\dot{q}},t) dt = 0 .</math>\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the coordinates in the [[Configuration space (physics)|configuration space]], i.e. the curve '''q'''(''t''), parameterized by time (see also [[parametric equation]] for this concept).\n\n==Origins, statements, and controversy==\n\n===Fermat===\n{{main article|Fermat's principle}}\nIn the 1600s, [[Pierre de Fermat]] postulated that \"''light travels between two given points along the path of shortest time'',\" which is known as the '''principle of least time''' or '''[[Fermat's principle]]'''.<ref name=\"Analytical Mechanics 2008\"/>\n\n===Maupertuis===\n{{main article|Maupertuis principle}}\nCredit for the formulation of the '''principle of least action''' is commonly given to [[Pierre Louis Maupertuis]], who felt that \"Nature is thrifty in all its actions\", and applied the principle broadly:\n\n{{quote|The laws of movement and of rest deduced from this principle being precisely the same as those observed in nature, we can admire the application of it to all phenomena. The movement of animals, the vegetative growth of plants ... are only its consequences; and the spectacle of the universe becomes so much the grander, so much more beautiful, the worthier of its Author, when one knows that a small number of laws, most wisely established, suffice for all movements.|Pierre Louis Maupertuis<ref>Chris Davis. [http://www.idlex.freeserve.co.uk/idle/evolution/ref/leastact.html ''Idle theory''] {{webarchive|url=https://web.archive.org/web/20060615043538/http://www.idlex.freeserve.co.uk/idle/evolution/ref/leastact.html |date=2006-06-15 }} (1998)</ref>}}\n\nThis notion of Maupertuis, although somewhat deterministic today, does capture much of the essence of mechanics.\n\nIn application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the \"[[vis viva]]\",\n\n{{Equation box 1\n|indent =:\n|title='''Maupertuis' principle'''\n|equation = <math>\\delta \\int 2T(t)  dt=0</math>\n|border=2\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nwhich is the integral of twice what we now call the [[kinetic energy]] ''T'' of the system.\n\n===Euler===\n\n[[Leonhard Euler]] gave a formulation of the action principle in 1744, in very recognizable terms, in the ''Additamentum 2'' to his ''Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes''. Beginning with the second paragraph:\n\n{{cquote|Let the mass of the projectile be ''M'', and let its speed be ''v'' while being moved over an infinitesimal distance ''ds''.   The body will have a momentum ''Mv'' that, when multiplied by the distance ''ds'', will give {{nowrap|''Mv'' ''ds''}}, the momentum of the body integrated over the distance ''ds''.  Now I assert that the curve thus described by the body to be the curve (from among all other curves connecting the same endpoints) that minimizes\n:<math>\\int Mv\\,ds</math>\nor, provided that ''M'' is constant along the path,\n:<math>M\\int v\\,ds</math>.|20px|20px|Leonhard Euler<ref name=\"eul44\" /><ref>Euler, [[s:la:Methodus inveniendi/Additamentum II|Additamentum II]] ([http://math.dartmouth.edu/~euler/docs/originals/E065h external link]), ibid. ([https://en.wikisource.org/w/index.php?title=Translation:Methodus_inveniendi/Additamentum_II&oldid=6399338 English translation])</ref>}}\n\nAs Euler states, ∫''Mv''d''s'' is the integral of the momentum over distance travelled, which, in modern notation, equals the abbreviated or [[reduced action]]\n\n{{Equation box 1\n|indent =:\n|title='''Euler's principle'''\n|equation = <math>\\delta\\int p\\,dq=0</math>\n|border=2\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nThus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. Curiously, Euler did not claim any priority, as the following episode shows.\n\n===Disputed priority===\n\nMaupertuis' priority was disputed in 1751 by the mathematician [[Samuel König]], who claimed that it had been invented by [[Gottfried Leibniz]] in 1707. Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works. König himself showed a ''copy'' of a 1707 letter from Leibniz to [[Jacob Hermann (mathematician)|Jacob Hermann]] with the principle, but the ''original'' letter has been lost. In contentious proceedings, König was accused of forgery,<ref name=\"oco03\" /> and even the [[Frederick the Great|King of Prussia]] entered the debate, defending Maupertuis (the head of his Academy), while [[Voltaire]] defended König.{{Citation needed|date=July 2017}}\n\nEuler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752.<ref name=\"oco03\" /> The claims of forgery were re-examined 150 years later, and archival work by [[C.I. Gerhardt]] in 1898<ref name=\"ger98\" /> and [[W. Kabitz]] in 1913<ref name=\"kab13\" /> uncovered other copies of the letter, and three others cited by König, in the [[Bernoulli family|Bernoulli]] archives.\n\n==Further development==\n\nEuler continued to write on the topic; in his ''Reflexions sur quelques loix generales de la nature'' (1748), he called the quantity \"effort\". His expression corresponds to what we would now call [[potential energy]], so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.\n\n===Lagrange and Hamilton===\n{{main article|Hamilton's principle}}\nMuch of the calculus of variations was stated by [[Joseph-Louis Lagrange]] in 1760<ref>{{cite book|editor=D. J. Struik|title=A Source Book in Mathematics, 1200–1800|publisher=MIT Press|location=Cambridge, Mass|year=1969}} pp. 406–413</ref><ref>{{cite book|last=Kline|first=Morris|title=Mathematical Thought from Ancient to Modern Times|publisher=Oxford University Press|location=New York|year=1972|isbn=0-19-501496-0}} pp. 582-589</ref> and he proceeded to apply this to problems in dynamics. In ''Méchanique Analytique'' (1788) Lagrange derived the general [[Lagrangian equations of motion|equations of motion]] of a mechanical body.<ref>{{cite book|last=Lagrange|first=Joseph-Louis|title=Mécanique Analytique|year=1788}} p. 226</ref> [[William Rowan Hamilton]] in 1834 and 1835<ref>W. R. Hamilton, \"On a General Method in Dynamics\", ''Philosophical Transactions of the Royal Society'' [http://www.emis.de/classics/Hamilton/GenMeth.pdf Part I (1834) p.247-308]; [http://www.emis.de/classics/Hamilton/SecEssay.pdf Part II (1835) p. 95-144]. (''From the collection [http://www.emis.de/classics/Hamilton/ Sir William Rowan Hamilton (1805–1865): Mathematical Papers] edited by David R. Wilkins, School of Mathematics, Trinity College, Dublin 2, Ireland. (2000); also reviewed as [http://www.maths.tcd.ie/pub/HistMath/People/Hamilton/Dynamics/ On a General Method in Dynamics]'')</ref> applied the variational principle to the classical [[Lagrangian mechanics|Lagrangian]] [[function (mathematics)|function]]\n\n:<math>L=T-V</math>\n\nto obtain the [[Euler–Lagrange equations]] in their present form.\n\n===Jacobi and Morse===\nIn 1842, [[Carl Gustav Jacobi]] tackled the problem of whether the variational principle always found minima as opposed to other [[stationary points]] (maxima or stationary [[saddle points]]); most of his work focused on [[geodesics]] on two-dimensional surfaces.<ref>G.C.J. Jacobi, ''Vorlesungen über Dynamik, gehalten an der Universität Königsberg im Wintersemester 1842–1843''. A. Clebsch (ed.) (1866); Reimer; Berlin. 290 pages, available online [http://math-doc.ujf-grenoble.fr/cgi-bin/oeitem?id=OE_JACOBI__8_1_0  Œuvres complètes volume '''8'''] at [http://math-doc.ujf-grenoble.fr/OEUVRES/ Gallica-Math] from the [http://gallica.bnf.fr/ Gallica Bibliothèque nationale de France].</ref> The first clear general statements were given by [[Marston Morse]] in the 1920s and 1930s,<ref>Marston Morse (1934). \"The Calculus of Variations in the Large\", ''American Mathematical Society Colloquium Publication'' '''18'''; New York.</ref> leading to what is now known as [[Morse theory]]. For example, Morse showed that the number of [[conjugate points]] in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian.\n\n===Gauss and Hertz===\nOther extremal principles of [[classical mechanics]] have been formulated, such as [[Gauss's principle of least constraint]] and its corollary, [[Hertz's principle of least curvature]].\n\n==Disputes about possible teleological aspects==\n\nThe mathematical equivalence of the [[differential equation|differential]] [[equations of motion]] and their [[integral equation|integral]]\ncounterpart has important philosophical implications.  The differential equations are statements about quantities localized to a single point in space or single moment of time.  For example, [[Newton's laws of motion|Newton's second law]]\n\n:<math>\\mathbf{F}=m\\mathbf{a}</math>\n\nstates that the ''instantaneous'' force '''F''' applied to a mass ''m'' produces an acceleration '''a''' at the same ''instant''.  By contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) an extended region of space.  Moreover, in the usual formulation of [[classical physics|classical]] action principles, the initial and final states of the system are fixed, e.g.,\n\n:''Given that the particle begins at position x<sub>1</sub> at time t<sub>1</sub> and ends at position x<sub>2</sub> at time t<sub>2</sub>, the physical trajectory that connects these two endpoints is an [[extremum]] of the action integral.''\n\nIn particular, the fixing of the ''final'' state has been interpreted as giving the action principle a [[teleology|teleological character]] which has been controversial historically. However, according to W. Yourgrau and S. Mandelstam, ''the teleological approach... presupposes that the variational principles themselves have mathematical characteristics which they ''de facto'' do not possess''<ref name=\"Stöltzner1994\">{{cite book|last=Stöltzner|first=Michael|title=Inside Versus Outside: Action Principles and Teleology|year=1994|publisher=Springer|isbn=978-3-642-48649-4|pages=33–62 |doi=10.1007/978-3-642-48647-0_3}}</ref> In addition, some critics maintain this apparent [[teleology]] occurs because of the way in which the question was asked. By specifying some but not all aspects of both the initial and final conditions (the positions but not the velocities) we are making some inferences about the initial conditions from the final conditions, and it is this \"backward\" inference that can be seen as a teleological explanation. Teleology can also be overcome if we consider the classical description as a limiting case of the [[Quantum mechanics|quantum]] formalism of [[Path integral formulation|path integration]], in which stationary paths are obtained as a result of interference of amplitudes along all possible paths.<ref name=\":0\" />\n\nThe short story ''[[Story of Your Life]]'' by the speculative fiction writer [[Ted Chiang]] contains visual depictions of [[Fermat's Principle]] along with a discussion of its teleological dimension. [[Keith Devlin]]'s ''The Math Instinct'' contains a chapter, \"Elvis the Welsh Corgi Who Can Do Calculus\" that discusses the calculus \"embedded\" in some animals as they solve the \"least time\" problem in actual situations.\n\n==See also==\n{{Div col|colwidth=}}\n* [[Action (physics)]]\n* [[Path integral formulation]]\n* [[Schwinger's quantum action principle]]\n* [[Path of least resistance]]\n* [[Analytical mechanics]]\n* [[Calculus of variations]]\n* [[Hamiltonian mechanics]]\n* [[Lagrangian mechanics]]\n* [[Occam's razor]]\n{{Div col end}}\n\n==Notes and references==\n{{reflist|30em}}\n\n==External links==\n{{wikiquote}}\n* [http://www.eftaylor.com/software/ActionApplets/LeastAction.html Interactive explanation of the principle of least action]\n* [http://www.eftaylor.com/software/ActionClockTicks/ Interactive applet to construct trajectories using principle of least action]\n*{{cite book |doi=10.1007/978-3-642-28583-7_9 |chapter=A Quantitative Measure, Mechanism and Attractor for Self-Organization in Networked Complex Systems |title=Self-Organizing Systems |volume=7166 |pages=90–5 |series=Lecture Notes in Computer Science |year=2012 |last1=Georgiev |first1=Georgi Yordanov |isbn=978-3-642-28582-0 }}\n* {{Cite journal|title=The Least Action and the Metric of an Organized System |journal=Open Systems and Information Dynamics |volume=9 |issue=4 |pages=371–380 |arxiv=1004.3518 |author1=Georgiev |first1=Georgi |last2=Georgiev |first2=Iskren |year=2002 |doi=10.1023/a:1021858318296}}\n* {{Cite journal|title= Metaphysics of the Principle of Least Action|journal= Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics |volume=62  |pages=189–201 |arxiv=1511.03429| last1=Terekhovich|first1=Vladislav |year=2018 |doi=10.1016/j.shpsb.2017.09.004}}\n\n{{DEFAULTSORT:Principle Of Least Action}}\n[[Category:Concepts in physics]]\n[[Category:Calculus of variations]]\n[[Category:History of physics]]\n[[Category:Principles]]\n[[Category:Scientific laws]]\n\n[[de:Prinzip der kleinsten Wirkung]]\n[[sq:Principi i Hamiltonit]]"
    },
    {
      "title": "Pseudo-monotone operator",
      "url": "https://en.wikipedia.org/wiki/Pseudo-monotone_operator",
      "text": "In [[mathematics]], a '''pseudo-monotone operator''' from a [[reflexive space|reflexive]] [[Banach space]] into its [[continuous dual space]] is one that is, in some sense, almost as [[well-behaved]] as a [[monotone operator]]. Many problems in the [[calculus of variations]] can be expressed using operators that are pseudo-monotone, and pseudo-monotonicity in turn implies the existence of solutions to these problems.\n\n==Definition==\n\nLet (''X'',&nbsp;||&nbsp;||) be a reflexive Banach space. A map ''T''&nbsp;:&nbsp;''X''&nbsp;&rarr;&nbsp;''X''<sup>&lowast;</sup> from ''X'' into its continuous dual space ''X''<sup>&lowast;</sup> is said to be '''pseudo-monotone''' if ''T'' is a [[bounded operator]] (not necessarily continuous) and if whenever\n\n:<math>u_{j} \\rightharpoonup u \\mbox{ in } X \\mbox{ as } j \\to \\infty</math>\n\n(i.e. ''u''<sub>''j''</sub> [[weak topology|converges weakly]] to ''u'') and\n\n:<math>\\limsup_{j \\to \\infty} \\langle T(u_{j}), u_{j} - u \\rangle \\leq 0,</math>\n\nit follows that, for all ''v''&nbsp;&isin;&nbsp;''X'',\n\n:<math>\\liminf_{j \\to \\infty} \\langle T(u_{j}), u_{j} - v \\rangle \\geq \\langle T(u), u - v \\rangle.</math>\n\n==Properties of pseudo-monotone operators==\n\nUsing a very similar proof to that of the [[Browder-Minty theorem]], one can show the following:\n\nLet (''X'',&nbsp;||&nbsp;||) be a [[real number|real]], reflexive Banach space and suppose that ''T''&nbsp;:&nbsp;''X''&nbsp;&rarr;&nbsp;''X''<sup>&lowast;</sup> is [[bounded function|bounded]], [[coercive function|coercive]] and pseudo-monotone. Then, for each [[continuous linear functional]] ''g''&nbsp;&isin;&nbsp;''X''<sup>&lowast;</sup>, there exists a solution ''u''&nbsp;&isin;&nbsp;''X'' of the equation ''T''(''u'')&nbsp;=&nbsp;''g''.\n\n==References==\n\n* {{cite book\n|author1=Renardy, Michael  |author2=Rogers, Robert C.\n |lastauthoramp=yes |    title = An introduction to partial differential equations\n|   series = Texts in Applied Mathematics 13\n|  edition = Second\n|publisher = Springer-Verlag\n| location = New York\n|     year = 2004\n|    pages = 367\n|       isbn = 0-387-00444-0\n}} (Definition 9.56, Theorem 9.57)\n\n[[Category:Banach spaces]]\n[[Category:Calculus of variations]]\n[[Category:Operator theory]]"
    },
    {
      "title": "Regularized canonical correlation analysis",
      "url": "https://en.wikipedia.org/wiki/Regularized_canonical_correlation_analysis",
      "text": "{{refimprove|date=July 2011}}\n'''Regularized canonical correlation analysis''' is a way of using [[ridge regression]] to solve the [[singularity theory|singularity]] problem in the [[cross-covariance matrix|cross-covariance matrices]] of [[canonical correlation analysis]]. By converting <math>\\operatorname{cov}(X, X)</math> and <math>\\operatorname{cov}(Y, Y)</math> into <math>\\operatorname{cov}(X, X) + \\lambda I_X</math> and <math>\\operatorname{cov}(Y, Y) + \\lambda I_Y</math>, it ensures that the above matrices will have reliable [[Matrix inverse|inverses]].\n\nThe idea probably dates back to [[Hrishikesh D. Vinod]]'s publication in 1976 where he called it \"Canonical ridge\".<ref>{{Cite journal\n | author = Hrishikesh D. Vinod\n | title = Canonical ridge and econometrics of joint production\n | journal = [[Journal of Econometrics]]\n | volume = 4\n | issue = 2\n|date=May 1976\n | pages = 147–166\n | doi = 10.1016/0304-4076(76)90010-5\n}}</ref><ref>{{Cite book\n | author = [[Kanti Mardia]]| title = Multivariate Analysis\n|display-authors=etal}}</ref>\nIt has been suggested for use in the analysis of [[functional neuroimaging]] data as such data are often singular.<ref>{{Cite journal\n |author1=Finn Årup Nielsen |author2=Lars Kai Hansen |author3=Stephen C. Strother | title = Canonical ridge analysis with ridge parameter optimization\n |date=May 1998\n | journal = [[NeuroImage]]\n | volume = 7\n | pages = S758\n\n | url = http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4981/pdf/imm4981.pdf\n}}</ref>\nIt is possible to compute the regularized canonical vectors in the lower-dimensional space.<ref>{{Cite thesis\n | author = Finn Årup Nielsen\n | year = 2001\n | title = Neuroinformatics in Functional Neuroimaging\n | url = http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/201/pdf/imm201.pdf\n | publisher = [[Technical University of Denmark]]\n}} Section 3.18.5</ref>\n\n== References ==\n{{Reflist}}\n\n* {{cite journal| last1=Leurgans| first1=S.E.|author1-link=Sue Leurgans| last2=Moyeed| first2=R.A.| last3=Silverman| first3=B.W.| title=Canonical correlation analysis when the data are curves| journal=[[Journal of the Royal Statistical Society]]| series=Series B (Methodological)| year=1993| volume=55| number=3| pages=725–740| jstor=2345883}}\n\n[[Category:Mathematical analysis]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Signorini problem",
      "url": "https://en.wikipedia.org/wiki/Signorini_problem",
      "text": "The '''Signorini problem''' is an [[Linear elasticity#Elastostatics|elastostatics]] problem in [[linear elasticity]]: it consists in finding the [[Linear elasticity#Elastostatics|elastic equilibrium]] [[Continuum mechanics#Mathematical modeling of a continuum|configuration]] of an [[Anisotropy#Material science and engineering|anisotropic]] [[Homogeneous media|non-homogeneous]] [[Physical body|elastic body]], resting on a [[Rigid body|rigid]] [[friction]]less [[Surface (topology)|surface]] and subject only to its [[Weight|mass force]]s. The name was coined by [[Gaetano Fichera]] to honour his teacher, [[Antonio Signorini]]: the original name coined by him is '''problem with ambiguous [[Boundary value problem|boundary conditions]]'''.\n\n== History ==\n[[File:Classical Signorini problem.svg|thumb|400px|The classical Signorini problem: what will be the [[Linear elasticity#Elastostatics|equilibrium]] [[Continuum mechanics#Mathematical modeling of a continuum|configuration]] of the orange spherically shaped [[Physical body|elastic body]] resting on the blue [[Rigid body|rigid]] [[friction]]less [[Plane (geometry)|plane]]?]]\nThe problem was posed by [[Antonio Signorini]] during a course taught at the [[Istituto Nazionale di Alta Matematica]] in 1959, later published as the article {{harv|Signorini|1959}}, expanding a previous short exposition he gave in a note published in 1933. {{harvtxt|Signorini|1959|p=128}} himself called it ''problem with ambiguous [[Boundary value problem|boundary conditions]]'',<ref>{{lang-it|Problema con ambigue condizioni al contorno}}.</ref> since there are two alternative sets of [[Boundary value problem|boundary conditions]] the solution ''must satisfy'' on any given [[Contact (mechanics)|contact point]]. The statement of the problem involves not only [[Equality (mathematics)|equalities]] ''but also [[inequality (mathematics)|inequalities]]'', and ''it is not [[A priori and a posteriori|a priori]] known what of the two sets of boundary conditions is satisfied at each point''. Signorini asked to determine if the problem is [[Well-posed problem|well-posed]] or not in a physical sense, i.e. if its solution exists and is unique or not: he explicitly invited young [[mathematical analysis|analysts]] to study the problem.<ref>As it is stated in {{harv|Signorini|1959|p=129}}.</ref>\n\n[[Gaetano Fichera]] and [[Mauro Picone]] attended the course, and Fichera started to investigate the problem: since he found no references to similar problems in the theory of [[boundary value problem]]s,<ref>See {{harv|Fichera|1995|p=49}}.</ref> he decided to attack it by starting from [[first principle]]s, precisely from the [[virtual work principle]].\n\nDuring Fichera's researches on the problem, Signorini began to suffer serious health problems: nevertheless, he desired to know the answer to his question before his death. Picone, being tied by a strong friendship with Signorini, began to chase Fichera to find a solution: Fichera himself, being tied as well to Signorini by similar feelings, perceived the last months of 1962 as worrying days.<ref>This dramatic situation is described by {{Harvtxt|Fichera|1995|p=51}} himself.</ref> Finally, on the first days of January 1963, Fichera was able to give a complete proof of the existence and uniqueness of a solution for the problem with ambiguous boundary condition, which he called \"Signorini problem\" to honour his teacher. A preliminary research announcement, later published as {{Harv|Fichera|1963}}, was written up and submitted to Signorini exactly a week before his death: and He was very satisfied to see a positive answer to his question.\n\nA few days later, during a conversation with his [[Family doctor|family Doctor]] Damiano Aprile, Signorini told him:<ref>{{Harvtxt|Fichera|1995|p=53}} reports the episode following the remembraces of [[Mauro Picone]]: see the entry \"[[Antonio Signorini]]\" for further details.</ref>\n{{plainlist|\n*\"Il mio discepolo Fichera mi ha dato una grande soddisfazione\".<ref>{{lang-en|My disciple Fichera gave me a great contentment}}.</ref>\n*\"Ma Lei ne ha avute tante, Professore, durante la Sua vita\",<ref>{{lang-en|But you had many, Professor, during your life}}.</ref> replied Doctor Aprile, but then Signorini replied again:\n*\"Ma questa è la più grande.\"<ref>{{lang-en|But this is the greatest one}}.</ref> And those were his last words.\n}}\n\nAccording to {{Harvtxt|Antman|1983|p=282}} the solution of the Signorini problem coincides with the birth of the field of [[Variational inequality|variational inequalities]].\n\n== Formal statement of the problem ==\nThe content of this section and the following subsections follows closely the treatment of [[Gaetano Fichera]] in {{Harvnb|Fichera|1963}}, {{Harvnb|Fichera|1964b}} and also {{Harvnb|Fichera|1995}}: his derivation of the problem is different from [[Antonio Signorini|Signorini]]'s one in that he does not consider only [[incopressible body|incompressible bodies]] and a plane rest [[Surface (topology)|surface]], as Signorini does.<ref>See {{Harvnb|Signorini|1959|p=127}}) for the original approach.</ref> The problem consist in finding the [[displacement vector]] from the [[Continuum mechanics#Mathematical modeling of a continuum|natural configuration]] <math>\\scriptstyle\\boldsymbol{u}(\\boldsymbol{x})=\\left(u_1(\\boldsymbol{x}),u_2(\\boldsymbol{x}),u_3(\\boldsymbol{x})\\right)</math> of an [[Anisotropy#Material science and engineering|anisotropic]] [[Homogeneous media|non-homogeneous]] [[Physical body|elastic body]] that lies in a [[subset]] <math>A</math> of the three-[[dimension]]al [[euclidean space]] whose [[boundary (topology)|boundary]] is <math>\\scriptstyle\\partial A</math> and whose [[interior normal]] is the [[Euclidean vector|vector]] '''''<math>n</math>''''', resting on a [[Rigid body|rigid]] [[frictionless]] [[Surface (topology)|surface]] whose [[Contact (mechanics)|contact]] [[Surface (topology)|surface]] (or more generally contact [[set (mathematics)|set]]) is <math>\\Sigma</math> and subject only to its [[body force]]s <math>\\scriptstyle\\boldsymbol{f}(\\boldsymbol{x})=\\left(f_1(\\boldsymbol{x}),f_2(\\boldsymbol{x}),f_3(\\boldsymbol{x})\\right)</math>, and [[surface force]]s <math>\\scriptstyle\\boldsymbol{g}(\\boldsymbol{x})=\\left(g_1(\\boldsymbol{x}),g_2(\\boldsymbol{x}),g_3(\\boldsymbol{x})\\right)</math> applied on the free (i.e. not in contact with the rest surface) surface <math>\\scriptstyle\\partial A\\setminus\\Sigma </math>: the set <math>A</math> and the contact surface <math>\\Sigma</math> characterize the natural configuration of the body and are known a priori. Therefore, the body has to satisfy the general [[Stress (mechanics)#Equilibrium equations and symmetry of the stress tensor|equilibrium equations]]\n\n:{{EquationRef|1|(1){{spaces|5}}}}<math>\\qquad\\frac{\\partial\\sigma_{ik}}{\\partial x_k}- f_i= 0\\qquad\\text{for } i=1,2,3</math>\n\nwritten using the [[Einstein notation]] as all in the following development, the ordinary [[Boundary value problem|boundary conditions]] on <math>\\scriptstyle\\partial A\\setminus\\Sigma</math>\n\n:{{EquationRef|2|(2){{spaces|5}}}}<math>\\qquad\\sigma_{ik}n_k-g_i=0\\qquad\\text{for } i=1,2,3</math>\n\nand the following two sets of [[Boundary value problem|boundary conditions]] on <math>\\Sigma</math>, where '''<math>\\scriptstyle\\boldsymbol{\\sigma} = \\boldsymbol{\\sigma}(\\boldsymbol{u})</math>''' is the [[Cauchy stress tensor]]. Obviously, the body forces and surface forces cannot be given in arbitrary way but they must satisfy a condition in order for the body to reach an equilibrium configuration: this condition will be deduced and analized in the following development.\n\n=== The ambiguous boundary conditions ===\nIf '''''<math>\\scriptstyle\\boldsymbol{\\tau}=(\\tau_1,\\tau_2,\\tau_3)</math>''''' is any [[tangent vector]] to the [[Contact (mechanics)|contact]] [[set (mathematics)|set]] <math>\\Sigma</math>, then the ambiguous boundary condition in each [[Point (geometry)|point]] of this set are expressed by the following two systems of [[inequality (mathematics)|inequalities]]\n\n:{{EquationRef|3|(3){{spaces|5}}}}<math>\n\\quad\n\\begin{cases}\nu_i n_i & = 0 \\\\\n\\sigma_{ik} n_i n_k & \\geq 0\\\\\n\\sigma_{ik} n_i \\tau_k & = 0\n\\end{cases}\n</math>{{spaces|5}}or{{spaces|5}}{{EquationRef|4|(4){{spaces|5}}}}<math>\n\\begin{cases}\nu_i n_i & > 0 \\\\\n\\sigma_{ik} n_i n_k & = 0 \\\\\n\\sigma_{ik} n_i \\tau_k & = 0\n\\end{cases}\n</math>\n\nLet's analyze their meaning: \n*Each [[set (mathematics)|set]] of conditions consists of three [[Binary relation|relations]], [[Equality (mathematics)|equalities]] or [[inequality (mathematics)|inequalities]], and all the second members are the [[Zero function#Other uses of zero in mathematics|zero function]].\n*The [[quantity|quantities]] at first member of each first relation are [[Proportionality (mathematics)|proportional]] to the [[norm (mathematics)|norm]] of the [[vector component|component]] of the [[displacement vector]] directed along the [[normal vector]] '''<math>n</math>'''.\n*The quantities at first member of each second relation are proportional to the norm of the component of the [[Stress (mechanics)#Relationship stress vector - stress tensor|tension vector]] directed along the [[normal vector]] '''<math>n</math>''',\n*The quantities at the first member of each third relation are proportional to the norm of the component of the tension vector along any [[Euclidean vector|vector]] '''<math>\\tau</math>''' [[Tangent vector|tangent]] in the given [[Point (geometry)|point]] to the [[Contact (mechanics)|contact]] [[set (mathematics)|set]] <math>\\Sigma</math>.\n*The quantities at the first member of each of the three relations are [[Positive number|positive]] if they have the same [[Euclidean vector|sense]] of the [[Euclidean vector|vector]] they are [[Proportionality (mathematics)|proportional]] to, while they are [[negative number|negative]] if not, therefore the [[Proportionality (mathematics)|constants of proportionality]] are respectively <math>\\scriptstyle +1</math> and  <math>\\scriptstyle -1</math>.\nKnowing these facts, the set of conditions {{EquationNote|3|(3)}} applies to [[Point (geometry)|point]]s of the [[boundary (topology)|boundary]] of the body which ''do not'' leave the [[Contact (mechanics)|contact]] set <math>\\Sigma</math> in the [[Linear elasticity#Elastostatics|equilibrium configuration]], since, according to the first [[Binary relation|relation]], the [[displacement vector]] '''<math>u</math>''' ''has no [[vector component|component]]s'' directed as the [[normal vector]] '''<math>n</math>''', while, according to the second relation, the [[Stress (mechanics)#Relationship stress vector - stress tensor|tension vector]] ''may have a component'' directed as the normal vector '''<math>n</math>''' and having the same [[Euclidean vector|sense]]. In an analogous way, the set of conditions {{EquationNote|4|(4)}} applies to points of the boundary of the body which ''leave'' that set in the equilibrium configuration, since displacement vector '''<math>u</math>''' ''has a component'' directed as the normal vector '''<math>n</math>''', while the [[Stress (mechanics)#Relationship stress vector - stress tensor|tension vector]] ''has no components'' directed as the normal vector '''<math>n</math>'''. For both sets of conditions, the tension vector has no tangent component to the [[Contact (mechanics)|contact]] set, according to the [[hypothesis]] that the body rests on a rigid ''frictionless'' surface.\n\nEach system expresses a '''unilateral constraint''', in the sense that they express the physical impossibility of the [[Physical body|elastic body]] to penetrate into the surface where it rests: the ambiguity is not only in the unknown values non-[[zero]] quantities must satisfy on the [[Contact (mechanics)|contact]] set but also in the fact that it is not a priori known if a point belonging to that set satisfies the system of boundary conditions {{EquationNote|3|(3)}} or {{EquationNote|4|(4)}}. The set of points where {{EquationNote|3|(3)}} is satisfied is called the '''area of support''' of the elastic body on <math>\\Sigma</math>, while its [[Complement (set theory)#Relative complement|complement respect to <math>\\Sigma</math>]] is called the '''area of separation'''.\n\nThe above formulation is ''general'' since the [[Cauchy stress tensor]] i.e. the [[constitutive equation]] of the [[Physical body|elastic body]] has not been made explicit: it is equally valid assuming the [[hypothesis]] of [[linear elasticity]] or the ones of [[Finite strain theory|nonlinear elasticity]]. However, as it would be clear from the following developments, the problem is inherently [[Nonlinear system|nonlinear]], therefore ''assuming a [[Linear elasticity#Mathematical formulation|linear stress tensor]] does not simplify the problem''.\n\n=== The form of the stress tensor in the formulation of Signorini and Fichera ===\nThe form assumed by [[Antonio Signorini|Signorini]] and [[Gaetano Fichera|Fichera]] for the [[elastic potential energy]] is the following one (as in the previous developments, the [[Einstein notation]] is adopted)\n\n:<math>W(\\boldsymbol{\\varepsilon})=a_{ik,jh}(\\boldsymbol{x})\\varepsilon_{ik}\\varepsilon_{jh}</math>\n\nwhere \n*<math>\\scriptstyle\\boldsymbol{a}(\\boldsymbol{x})=\\left(a_{ik,jh}(\\boldsymbol{x})\\right)</math> is the [[elasticity tensor]]\n*<math>\\scriptstyle\\boldsymbol{\\varepsilon}=\\boldsymbol{\\varepsilon}(\\boldsymbol{u})=\\left(\\varepsilon_{ik}(\\boldsymbol{u})\\right)=\\left(\\frac{1}{2} \\left( \\frac{\\partial u_i}{\\partial x_k} + \\frac{\\partial u_k}{\\partial x_i} \\right)\\right)</math> is the [[Infinitesimal strain#Infinitesimal strain tensor|infinitesimal strain tensor]]\nThe [[Cauchy stress tensor]] has therefore the following form\n\n:{{EquationRef|5|(5){{spaces|5}}}}<math>\\sigma_{ik}= - \\frac{\\partial W}{\\partial \\varepsilon_{ik}} \\qquad\\text{for } i,k=1,2,3</math>\n\nand it is ''[[Linear map|linear]]'' with respect to the components of the infinitesimal strain tensor; however, it is not [[Homogeneity (physics)|homogeneous]] nor [[Isotropy|isotropic]].\n\n== Solution of the problem ==\nAs for the section on the formal statement of the Signorini problem, the contents of this section and the included subsections follow closely the treatment of [[Gaetano Fichera]] in {{Harvnb|Fichera|1963}}, {{Harvnb|Fichera|1964b}}, {{Harvnb|Fichera|1972}} and also {{Harvnb|Fichera|1995}}: obviously, the exposition focuses on the basics steps of the proof of the existence and uniqueness for the solution of problem {{EquationNote|1|(1)}}, {{EquationNote|2|(2)}}, {{EquationNote|3|(3)}}, {{EquationNote|4|(4)}} and {{EquationNote|5|(5)}}, rather than the technical details.\n\n=== The potential energy ===\nThe first step of the analysis of Fichera as well as the first step of the analysis of [[Antonio Signorini]] in {{Harvnb|Signorini|1959|}} is the analysis of the '''potential energy''', i.e. the following [[Functional (mathematics)|functional]]\n\n:{{EquationRef|6|(6){{spaces|6}}}}<math>I(\\boldsymbol{u})=\\int_A W(\\boldsymbol{x},\\boldsymbol{\\varepsilon})\\mathrm{d}x - \\int_A u_i f_i\\mathrm{d}x - \\int_{\\partial A\\setminus\\Sigma}u_i g_i \\mathrm{d}\\sigma</math>\n\nwhere '''<math>u</math>''' belongs to the [[set (mathematics)|set]] of '''admissible displacements''' <math>\\scriptstyle\\mathcal{U}_\\Sigma</math> i.e. the set of [[displacement vector]]s satisfying the system of [[boundary value problem|boundary conditions]] {{EquationNote|3|(3)}} or {{EquationNote|4|(4)}}. The meaning of each of the three terms is the following \n*the first one is the total [[elastic potential energy]] of the [[Physical body|elastic body]]\n*the second one is the total [[potential energy]] due to the [[body force]]s, for example the [[gravitational force]]\n*the third one is the potential energy due to [[surface force]]s, for example the [[Force (physics)|force]]s exerted by the [[atmospheric pressure]]\n{{Harvtxt|Signorini|1959|pp=129–133}} was able to prove that the admissible displacement '''<math>u</math>''' which [[minimum|minimize]] the integral '''<math>I(u)</math>''' is a solution of the problem with ambiguous boundary conditions {{EquationNote|1|(1)}}, {{EquationNote|2|(2)}}, {{EquationNote|3|(3)}}, {{EquationNote|4|(4)}} and {{EquationNote|5|(5)}}, provided it is a [[smooth function|<math>C^1</math> function]] [[Distribution (mathematics)#Support of a distribution|supported]] on the [[Closure (topology)|closure]] <math>\\scriptstyle \\bar A</math> of the set <math>A</math>: however [[Gaetano Fichera]] gave a class of [[counterexample]]s in {{Harv|Fichera|1964b|pp=619–620}} showing that in general, admissible displacements are \nnot [[smooth function]]s of these class. Therefore, Fichera tries to minimize the [[Functional (mathematics)|functional]] {{EquationNote|6|(6)}} in a wider [[function space]]: in doing so, he first calculates the [[first variation]] (or [[functional derivative]]) of the given functional in the [[Neighbourhood (topology)|neighbourhood]] of the sought minimizing admissible displacement <math>\\scriptstyle\\boldsymbol{u} \\in \\mathcal{U}_\\Sigma</math>, and then requires it to be greater than or equal to [[zero]]\n\n:<math>\\left. \\frac{\\mathrm{d}}{\\mathrm{d}t} I( \\boldsymbol{u} + t \\boldsymbol{v}) \\right\\vert_{t=0} = -\\int_A \\sigma_{ik}(\\boldsymbol{u})\\varepsilon_{ik}(\\boldsymbol{v})\\mathrm{d}x - \\int_A v_i f_i\\mathrm{d}x - \\int_{\\partial A\\setminus\\Sigma}\\!\\!\\!\\!\\! v_i g_i \\mathrm{d}\\sigma \\geq 0 \\qquad \\forall \\boldsymbol{v} \\in \\mathcal{U}_\\Sigma</math>\n\nDefining the following functionals\n\n:<math>B(\\boldsymbol{u},\\boldsymbol{v}) = -\\int_A \\sigma_{ik}(\\boldsymbol{u})\\varepsilon_{ik}(\\boldsymbol{v})\\mathrm{d}x \\qquad \\boldsymbol{u},\\boldsymbol{v} \\in \\mathcal{U}_\\Sigma</math>\n\nand\n\n:<math>F(\\boldsymbol{v}) = \\int_A v_i f_i\\mathrm{d}x + \\int_{\\partial A\\setminus\\Sigma}\\!\\!\\!\\!\\! v_i g_i \\mathrm{d}\\sigma\\qquad \\boldsymbol{v} \\in \\mathcal{U}_\\Sigma</math>\n\nthe preceding [[inequality (mathematics)|inequality]] is can be written as\n\n:{{EquationRef|7|(7){{spaces|6}}}}<math>B(\\boldsymbol{u},\\boldsymbol{v}) - F(\\boldsymbol{v}) \\geq 0 \\qquad \\forall \\boldsymbol{v} \\in \\mathcal{U}_\\Sigma </math>\n\nThis inequality is the '''[[variational inequality]] for the Signorini problem'''.\n\n== See also ==\n* [[Linear elasticity]]\n* [[Variational inequality]]\n\n== Notes ==\n{{Reflist|30em}}\n\n==References==\n\n=== Historical references ===\n*{{Citation\n| last = Antman\n| first = Stuart | authorlink = Stuart Antman\n| title = The influence of elasticity in analysis: modern developments\n| journal = [[Bulletin of the American Mathematical Society]]\n| volume = 9\n| issue = 3\n| pages = 267–291\n| date = \n| year = 1983\n| url = http://www.ams.org/bull/1983-09-03/S0273-0979-1983-15185-6/home.html\n| doi = 10.1090/S0273-0979-1983-15185-6 \n| mr = 714990\n| zbl = 0533.73001\n}}.\n*{{Citation\n  | last = Duvaut\n  | first = Georges\n  | author-link =Georges Duvaut\n  | contribution = Problèmes unilatéraux en mécanique des milieux continus\n  | contribution-url = http://www.mathunion.org/ICM/ICM1970.3/Main/icm1970.3.0071.0078.ocr.pdf\n  | series = [[International Congress of Mathematicians|ICM Proceedings]]\n  | title = Actes du Congrès international des mathématiciens, 1970\n  | volume = Mathématiques appliquées (E), Histoire et Enseignement (F) – Volume 3\n  | pages = 71–78\n  | year = 1971\n  | place = [[Paris]]\n  | publisher = [[Gauthier-Villars]]\n  | url = http://www.mathunion.org/ICM/ICM1970.3/\n  | id =\n  | mr =\n  | zbl =\n}}. A brief research survey describing the field.\n*{{Citation\n| last = Fichera\n| first = Gaetano\n| author-link = Gaetano Fichera\n| contribution = Boundary value problems of elasticity with unilateral constraints\n| year = 1972\n| title = Festkörpermechanik/Mechanics of Solids\n| editor-last = Flügge\n| editor-first = Siegfried \n| editor-link = Siegfried Flügge\n| editor2-last = Truesdell\n| editor2-first = Clifford A.\n| editor2-link = Clifford Truesdell\n| series = Handbuch der Physik (Encyclopedia of Physics)\n| volume = VIa/2\n| edition = paperback 1984\n| pages = 391–424\n| place = Berlin–[[Heidelberg]]–New York\n| publisher = [[Springer-Verlag]]\n| zbl = 0277.73001\n| isbn = 0-387-13161-2\n}}. The encyclopedia entry about problems with unilateral constraints (the class of [[boundary value problem]]s the Signorini problem belongs to) he wrote for the ''Handbuch der Physik'' on invitation by [[Clifford Truesdell]].\n*{{Citation\n| first = Gaetano\n| last = Fichera\n| author-link = Gaetano Fichera\n| editor-last = \n| editor-first = \n| contribution = La nascita della teoria delle disequazioni variazionali ricordata dopo trent'anni \n| title = Incontro scientifico italo-spagnolo. Roma, 21 ottobre 1993\n| url = http://www.lincei.it/pubblicazioni/catalogo/volume.php?lg=e&rid=32885\n| language = Italian\n| year = 1995\n| pages = 47–53\n| place = [[Rome|Roma]]\n| series = Atti dei Convegni Lincei\n| volume = 114\n| publisher = [[Accademia Nazionale dei Lincei]]\n}}. ''The birth of the theory of variational inequalities remembered thirty years later'' (English translation of the title) is an historical paper describing the beginning of the theory of variational inequalities from the point of view of its founder.\n* {{citation\n| last = Fichera\n| first = Gaetano\n| title = Opere storiche biografiche, divulgative\n| publisher = Giannini\n| location = [[Napoli]]\n| year = 2002\n| language = Italian\n| pages = 491\n}}. \"''Historical, biographical, divulgative works''\" in the English translation: a volume collecting almost all works of Gaetano Fichera in the fields of [[history of mathematics]] and scientific divulgation.\n* {{citation\n |last        = Fichera\n |first       = Gaetano\n |title       = Opere scelte\n |url         = http://umi.dm.unibo.it/italiano/Editoria/libri/ogm.html\n |publisher   = Edizioni Cremonese (distributed by [[Unione Matematica Italiana]])\n |location    = [[Firenze]]\n |language    = \n |year        = 2004\n |pages       = XXIX+432 (vol. 1), pp. VI+570 (vol. 2), pp. VI+583 (vol. 3)\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20091228075048/http://umi.dm.unibo.it/italiano/Editoria/libri/ogm.html\n |archivedate = 2009-12-28\n |df          = \n}}, {{isbn|88-7083-811-0}} (vol. 1), {{isbn|88-7083-812-9}} (vol. 2), {{isbn|88-7083-813-7}} (vol. 3). Gaetano Fichera's \"''Selected works''\": three volumes collecting his most important mathematical papers, with a biographical sketch of [[Olga Arsenievna Oleinik|Olga A. Oleinik]].\n* {{citation\n |last        = Signorini\n |first       = Antonio\n |author-link = Antonio Signorini\n |title       = Opere scelte\n |url         = http://umi.dm.unibo.it/italiano/Editoria/libri/ogm.html\n |publisher   = Edizioni Cremonese (distributed by [[Unione Matematica Italiana]])\n |location    = [[Firenze]]\n |year        = 1991\n |pages       = XXXI + 695\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20091228075048/http://umi.dm.unibo.it/italiano/Editoria/libri/ogm.html\n |archivedate = 2009-12-28\n |df          = \n}}. The \"''Selected works''\" of Antonio Signorini: a volume collecting his most important works with an introduction and a commentary of [[Giuseppe Grioli]].\n\n===Research works===\n*{{citation\n| last = Fichera\n| first = Gaetano\n| author-link = Gaetano Fichera\n| title = Sul problema elastostatico di Signorini con ambigue condizioni al contorno \n| journal = Rendiconti della [[Accademia Nazionale dei Lincei]], Classe di Scienze Fisiche, Matematiche e Naturali\n| language = Italian\n| volume = 34 \n| series =  8\n| issue = 2\n| year = 1963\n| pages=138–142\n| zbl= 0128.18305\n}}. \"''On the elastostatic problem of Signorini with ambiguous boundary conditions''\" (English translation of the title) is a short research note announcing and describing the solution of the Signorini problem.\n*{{citation\n| last = Fichera\n| first = Gaetano\n| author-link = Gaetano Fichera\n| title = Problemi elastostatici con vincoli unilaterali: il problema di Signorini con ambigue condizioni al contorno \n| journal = Memorie della [[Accademia Nazionale dei Lincei]], Classe di Scienze Fisiche, Matematiche e Naturali\n| language = Italian\n| volume = 7\n| series =  8\n| issue = 2\n| year = 1964a\n| pages=91–140\n| zbl = 0146.21204\n}}. \"''Elastostatic problems with unilateral constraints: the Signorini problem with ambiguous boundary conditions''\" (English translation of the title) is the first paper where aa [[Existence theorem|existence]] and [[uniqueness theorem]] for the Signorini problem is proved.\n* {{citation\n| last = Fichera\n| first = Gaetano\n| author-link = Gaetano Fichera\n| contribution = Elastostatic problems with unilateral constraints: the Signorini problem with ambiguous boundary conditions\n| title = Seminari dell'istituto Nazionale di Alta Matematica 1962–1963\n| year = 1964b\n| publisher = Edizioni Cremonese\n| place = [[Rome]]\n| pages=613–679\n}}. An English translation of the previous paper.\n*{{Citation\n| last = Signorini\n| first = Antonio\n| author-link = Antonio Signorini\n| title = Questioni di elasticità non linearizzata e semilinearizzata\n|trans-title=Topics in non linear and semilinear elasticity\n| journal = [[Rendiconti di Matematica e delle sue Applicazioni]]\n| language = Italian\n| series = 5\n| volume = 18 \n| pages = 95–139\n| year = 1959\n| zbl = 0091.38006\n}}.\n\n*{{citation|last=Petrosyan|first=Arshak|last2=Shahgholian|first2=Henrik|last3=Uraltseva|first3=Nina|title=Regularity of Free Boundaries in Obstacle-Type Problems. Graduate Studies in Mathematics,|publisher=American Mathematical Society, Providence, RI|year=2012|isbn=0-8218-8794-7 }}.\n*{{Citation\n| last = Andersson\n| first = John\n| title = Optimal regularity for the Signorini problem and its free boundary\n| journal = [[Invent. Math.]]\n| language = English\n| series = \n| volume = 1 \n| pages = 1–82\n| year = 2016\n| zbl = \n}}.\n\n== External links ==\n*{{springer\n | title= Signorini problem\n | id= S/s110130\n | last= Barbu\n | first= V.\n }}\n* [https://www.scilag.net/problem/G-180630.1  Alessio Figalli,  On global homogeneous solutions to the Signorini problem,]\n\n{{DEFAULTSORT:Signorini Problem}}\n[[Category:Calculus of variations]]\n[[Category:Continuum mechanics]]\n[[Category:Elasticity (physics)]]\n[[Category:Partial differential equations]]"
    },
    {
      "title": "Stampacchia Medal",
      "url": "https://en.wikipedia.org/wiki/Stampacchia_Medal",
      "text": "The '''Stampacchia Gold Medal''' is an international prize awarded every three years by the [[Italian Mathematical Union]] (''Unione Matematica Italiana'' - ''UMI'' {[[:it:Unione Matematica Italiana|it]]}) together with the Ettore Majorana Foundation (Erice), in recognition of outstanding contributions to the field of [[calculus of variations|Calculus of Variations]] and related applications.<ref>[http://umi.dm.unibo.it/premi/gold-medal-guido-stampacchia/ Stampacchia Medal on the site of the Italian Mathematical Union]</ref> The medal, named after the Italian mathematician [[Guido Stampacchia]], goes to a [[mathematician]] whose age does not exceed 35.\n\n\n== Prize Winners ==\n*2003 [[Tristan Rivière]] ([[ETH Zürich]])\n*2006 [[Giuseppe Mingione]] ([[University of Parma]])\n*2009 [[Camillo De Lellis]] ([[University of Zurich]])\n*2012 [[Ovidiu Savin]] ([[Columbia University]])\n*2015 [[Alessio Figalli]] ([[The University of Texas at Austin]]) <ref>[http://umi.dm.unibo.it/wp-content/uploads/2015/03/motivazioni_figalli.pdf Stampacchia Medal winner citation]</ref>\n*2018 [[Guido_de_Philippis|Guido De Philippis]] ([[International School for Advanced Studies]]) <ref>\n[http://normalenews.sns.it/il-premio-internazionale-guido-stampacchia-a-de-philippis-ex-allievo-phd-di-matematica/ Announcement of the site of [[Scuola Normale Superiore di Pisa]]]</ref>\n\n==References==\n{{reflist}}\n\n== External links ==\n*[http://www.ems-ph.org/journals/newsletter/pdf/2012-03-83.pdf [[European Mathematical Society]] newsletter with the announcement of the 2012 Stampacchia Medal (pag. 17)]\n*[http://umi.dm.unibo.it/ Official Site of the [[Italian Mathematical Union]] (UMI)]\n*[http://umi.dm.unibo.it/wp-content/uploads/2015/03/motivazioni_figalli.pdf 2015 Stampacchia Medal citation (UMI)]\n\n<!--Categories-->\n[[Category:Mathematics awards]]\n[[Category:Awards established in 2003]]\n[[Category:Awards of the Italian Mathematical Union]]\n[[Category:Calculus of variations]]\n[[Category:Awards with age limits]]\n[[Category:Triennial events]]"
    },
    {
      "title": "Tonelli's theorem (functional analysis)",
      "url": "https://en.wikipedia.org/wiki/Tonelli%27s_theorem_%28functional_analysis%29",
      "text": "{{one source|date=January 2013}}\nIn [[mathematics]], '''Tonelli's theorem in functional analysis''' is a fundamental result on the [[weak topology|weak]] [[lower semicontinuous|lower semicontinuity]] of [[nonlinear]] [[functional (mathematics)|functionals]] on [[Lp space|''L''<sup>''p''</sup> spaces]]. As such, it has major implications for [[functional analysis]] and the [[calculus of variations]]. Roughly, it shows that weak lower semicontinuity for integral functionals is equivalent to [[convex function|convexity]] of the integral kernel. The result is attributed to the [[Italy|Italian]] [[mathematician]] [[Leonida Tonelli]].\n\n==Statement of the theorem==\nLet &Omega; be a bounded [[Domain (mathematical analysis)|domain]] in ''n''-[[dimension]]al [[Euclidean space]] '''R'''<sup>''n''</sup> and let ''f''&nbsp;:&nbsp;'''R'''<sup>''m''</sup>&nbsp;&rarr;&nbsp;'''R'''&nbsp;&cup;&nbsp;{&plusmn;&infin;} be a [[continuous function|continuous]] [[extended real number line|extended real-valued]] function. Define a nonlinear functional ''F'' on functions ''u''&nbsp;:&nbsp;&Omega;&nbsp;&rarr;&nbsp;'''R'''<sup>''m''</sup> by\n\n:<math>F[u] = \\int_{\\Omega} f(u(x)) \\, \\mathrm{d} x.</math>\n\nThen ''F'' is sequentially weakly lower semicontinuous on the ''L''<sup>''p''</sup> space ''L''<sup>''p''</sup>(&Omega;;&nbsp;'''R'''<sup>''m''</sup>) for 1&nbsp;&lt;&nbsp;''p''&nbsp;&lt;&nbsp;+&infin; and weakly-&lowast; lower semicontinuous on ''L''<sup>&infin;</sup>(&Omega;;&nbsp;'''R'''<sup>''m''</sup>) [[if and only if]] the function ''f''\n\n:<math>\\mathbb{R}^{m} \\ni u \\mapsto f(u) \\in \\mathbb{R} \\cup \\{ \\pm \\infty \\}\\ </math>\n\nis convex.\n\n==References==\n* {{cite book\n|author1=Renardy, Michael  |author2=Rogers, Robert C.\n |lastauthoramp=yes |    title = An introduction to partial differential equations\n|   series = Texts in Applied Mathematics 13\n|  edition = Second\n|publisher = Springer-Verlag\n| location = New York\n|     year = 2004\n|    pages = 347\n|       isbn = 0-387-00444-0\n}} (Theorem 10.16)\n\n[[Category:Calculus of variations]]\n[[Category:Convex analysis]]\n[[Category:Theorems in functional analysis]]"
    },
    {
      "title": "Transversality (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Transversality_%28mathematics%29",
      "text": "{{Refimprove|date=December 2009}}\nIn [[mathematics]], '''transversality''' is a notion that describes how spaces can intersect; transversality can be seen as the \"opposite\" of [[tangent|tangency]], and plays a role in [[general position]]. It formalizes the idea of a generic intersection in [[differential topology]]. It is defined by considering the linearizations of the intersecting spaces at the points of intersection.\n\n==Definition==\n[[Image:Sphere-transverse.svg|thumb|Transverse curves on the surface of a sphere]]\n[[Image:Sphere-nontransverse.svg|thumb|Non-transverse curves on the surface of a sphere]]\n\nTwo [[submanifold]]s of a given finite-dimensional [[smooth manifold]] are said to '''intersect transversally''' if at every point of [[Intersection (set theory)|intersection]], their separate tangent spaces at that point together generate the [[tangent space]] of the [[ambient manifold]] at that point.<ref>Guillemin and Pollack 1974, p.30.</ref>  Manifolds that do not intersect are [[vacuously]] transverse. If the manifolds are of complementary dimension (i.e., their dimensions add up to the dimension of the [[ambient space]]), the condition means that the tangent space to the ambient manifold is the direct sum of the two smaller tangent spaces.  If an intersection is transverse, then the intersection will be a submanifold whose [[codimension]] is equal to the sums of the codimensions of the two manifolds. In the absence of the transversality condition the intersection may fail to be a submanifold, having some sort of [[Mathematical singularity|singular point]].\n\nIn particular, this means that transverse submanifolds of complementary dimension intersect in isolated points (i.e., a [[0-manifold]]). If both submanifolds and the ambient manifold are [[oriented]], their intersection is oriented.  When the intersection is zero-dimensional, the orientation is simply a plus or minus for each point.\n\nOne notation for the transverse intersection of two submanifolds ''L''<sub>1</sub> and ''L''<sub>2</sub> of a given manifold ''M'' is <math>L_{1} \\pitchfork L_{2}</math>.  This notation can be read in two ways:  either as “''L''<sub>1</sub> and ''L''<sub>2</sub> intersect transversally” or as an alternative notation for the set-theoretic intersection ''L''<sub>1</sub>&nbsp;∩&nbsp;''L''<sub>2</sub> of ''L''<sub>1</sub> and ''L''<sub>2</sub> when that intersection is transverse.  In this notation, the definition of transversality reads\n\n:<math>L_{1} \\pitchfork L_{2} \\iff \\forall p \\in L_{1} \\cap L_{2}, \\mathrm{T}_{p} M = \\mathrm{T}_{p} L_{1} + \\mathrm{T}_{p} L_{2}.</math>\n\n==Transversality of maps==\nThe notion of transversality of a pair of submanifolds is easily extended to transversality of a submanifold and a map to the ambient manifold, or to a pair of maps to the ambient manifold, by asking whether the [[pushforward (differential)|pushforwards]] of the tangent spaces along the preimage of points of intersection of the images generate the entire tangent space of the ambient manifold.<ref>Guillemin and Pollack 1974, p.28.</ref>  If the maps are [[embedding]]s, this is equivalent to transversality of submanifolds.\n\n==Meaning of transversality for different dimensions==\n[[Image:Transversality-ambient.svg|thumb|left|Transversality depends on ambient space. The two curves shown are transverse when considered as embedded in the plane, but not if we consider them as embedded in a plane in three-dimensional space]]\n\nSuppose we have transverse maps <math>f_1: L_1 \\to M</math> and <math>f_2: L_2 \\to M</math> where <math>L_1, L_2</math> and <math>M</math> are manifolds with dimensions <math>\\ell_1, \\ell_2</math> and <math>m</math> respectively.\n\nThe meaning of transversality differs a lot depending on the relative dimensions of <math>M, L_1</math> and <math>L_2</math>. The relationship between transversality and tangency is clearest when <math>\\ell_1 + \\ell_2 = m </math>.\n\nWe can consider three separate cases:\n#When <math>\\ell_1 + \\ell_2 < m </math>, it is impossible for the image of <math>L_1</math> and <math>L_2</math>'s tangent spaces to span <math>M</math>'s tangent space at any point. Thus any intersection between <math>f_1</math> and <math>f_2</math> cannot be transverse. However, non-intersecting manifolds vacuously satisfy the condition, so can be said to intersect transversely.\n#When <math>\\ell_1 + \\ell_2 = m</math>, the image of <math>L_1</math> and <math>L_2</math>'s tangent spaces must sum directly to <math>M</math>'s tangent space at any point of intersection.  Their intersection thus consists of isolated signed points, i.e. a zero-dimensional manifold.\n#When <math>\\ell_1 + \\ell_2 > m</math> this sum needn't be direct. In fact it ''cannot'' be direct if <math>f_1</math> and <math>f_2</math> are [[immersion (mathematics)|immersion]]s at their point of intersection, as happens in the case of embedded submanifolds.  If the maps are immersions, the intersection of their images will be a manifold of dimension <math>\\ell_1 + \\ell_2 - m.</math>\n\n==Intersection product==\nGiven any two smooth submanifolds, it is possible to perturb either of them by an arbitrarily small amount such that the resulting submanifold intersects transversally with the fixed submanifold. Such perturbations do not affect the [[Homology (mathematics)|homology]] class of the manifolds or of their intersections. For example, if manifolds of complementary dimension intersect transversally, the signed sum of the number of their intersection points does not change even if we [[Ambient isotopy|isotope]] the manifolds to another transverse intersection.  (The intersection points can be counted modulo 2, ignoring the signs, to obtain a coarser invariant.) This descends to a bilinear intersection product on homology classes of any dimension, which is [[Poincaré dual]] to the [[cup product]] on [[cohomology]]. Like the cup product, the intersection product is [[supercommutative|graded-commutative]].\n\n==Examples of transverse intersections==\nThe simplest non-trivial example of transversality is of arcs in a [[Surface (topology)|surface]]. An intersection point between two arcs is transverse [[if and only if]] it is not a tangency, i.e., their tangent lines inside the tangent plane to the surface are distinct.\n\nIn a three-dimensional space, transverse curves do not intersect. Curves transverse to surfaces intersect in points, and surfaces transverse to each other intersect in curves. Curves that are tangent to a surface at a point (for instance, curves lying on a surface) do not intersect the surface transversally.\n\nHere is a more specialised example: suppose that <math> G </math> is a [[simple Lie group]] and <math> \\mathfrak{g} </math> is its Lie algebra. By the [[sl2-triple|Jacobson–Morozov theorem]] every nilpotent element <math> e \\in \\mathfrak{g} </math> can be included into an <math> \\mathfrak{sl_2}</math>-triple <math> (e, h, f) </math>. The representation theory of <math> \\mathfrak{sl_2} </math> tells us that <math> \\mathfrak{g} = [\\mathfrak{g}, e] \\oplus \\mathfrak{g}_f </math>. The space <math> [\\mathfrak{g}, e] </math> is the [[tangent space]]  at <math> e </math> to the adjoint orbit <math> \\rm{Ad}(G)e </math> and so the [[affine space]] <math> e + \\mathfrak{g}_f </math> intersects the orbit of <math> e </math> transversally. The space <math> e + \\mathfrak{g}_f </math> is known as the \"Slodowy slice\" after [[Peter Slodowy]].\n\n==Applications==\n===Optimal control===\nIn fields utilizing the [[calculus of variations]] or the related [[Pontryagin maximum principle]], the transversality condition is frequently used to control the types of solutions found in optimization problems.  For example, it is a necessary condition for solution curves to problems of the form:\n\n:Minimize <math>\\int {F(x, y, y^\\prime)} dx</math> where one or both of the endpoints of the curve are not fixed.\nIn many of these problems, the solution satisfies the condition that the solution curve should cross transversally the [[nullcline]] or some other curve describing terminal conditions.\n\n===Smoothness of solution spaces===\nUsing [[Sard's theorem]], whose hypothesis is a special case of the transversality of maps, it can be shown that transverse intersections between submanifolds of a space of complementary dimensions or between submanifolds and maps to a space are themselves smooth submanifolds.  For instance, if a smooth [[Section (category theory)|section]] of an oriented manifold's [[tangent bundle]]—i.e. a [[vector field]]—is viewed as a map from the base to the total space, and intersects the zero-section (viewed either as a map or as a submanifold) transversely, then the zero set of the section—i.e. the singularities of the vector field—forms a smooth 0-dimensional submanifold of the base, i.e. a set of signed points.  The signs agree with the indices of the vector field, and thus the sum of the signs—i.e. the fundamental class of the zero set—is equal to the Euler characteristic of the manifold.  More generally, for a [[vector bundle]] over an oriented smooth closed finite-dimensional manifold, the zero set of a section transverse to the zero section will be a submanifold of the base of codimension equal to the rank of the vector bundle, and its homology class will be [[Poincaré duality|Poincaré dual]] to the [[Euler class]] of the bundle.\n\nAn extremely special case of this is the following: if a differentiable function from reals to the reals has nonzero derivative at a zero of the function, then the zero is simple, i.e. it the graph is transverse to the ''x''-axis at that zero; a zero derivative would mean a horizontal tangent to the curve, which would agree with the tangent space to the ''x''-axis.\n\nFor an infinite-dimensional example, the [[d-bar operator]] is a section of a certain [[Banach space]] bundle over the space of maps from a [[Riemann surface]] into an [[almost-complex manifold]].  The zero set of this section consists of holomorphic maps.  If the d-bar operator can be shown to be transverse to the zero-section, this [[moduli space]] will be a smooth manifold.  These considerations play a fundamental role in the theory of [[pseudoholomorphic curves]] and [[Gromov–Witten theory]]. (Note that for this example, the definition of transversality has to be refined in order to deal with [[Banach spaces]]!)\n\n==Grammar==\n<blockquote>\"Transversal\" is a noun; the adjective is \"transverse.\"</blockquote>\nquote from J.H.C. Whitehead, 1959<ref> Hirsch (1976), p.66</ref>\n\n==See also==\n*[[Transversality theorem]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite journal |first=René |last=Thom |title=Quelques propriétés globales des variétés differentiables |journal=[[Commentarii Mathematici Helvetici|Comm. Math. Helv.]] |volume=28 |year=1954 |issue=1 |pages=17–86 |doi=10.1007/BF02566923 }}\n* {{cite book |last=Guillemin |first=Victor |last2=Pollack |first2=Alan |year=1974 |title=Differential Topology |location= |publisher=Prentice-Hall |isbn=0-13-212605-2 }}\n*{{cite book | ref=harv|title = Differential Topology|authorlink=Morris Hirsch|first = Morris|last = Hirsch|publisher=Springer-Verlag|year=1976|isbn = 0-387-90148-5}}\n\n{{DEFAULTSORT:Transversality (Mathematics)}}\n[[Category:Differential topology]]\n[[Category:Calculus of variations]]\n[[Category:Geometry]]"
    },
    {
      "title": "Variational bicomplex",
      "url": "https://en.wikipedia.org/wiki/Variational_bicomplex",
      "text": "{{multiple issues|\n{{Context|date=October 2009}}\n{{no footnotes|date=June 2012}}\n}}\n\nIn mathematics, the [[Lagrangian system|Lagrangian theory]] on [[fiber bundle]]s is globally formulated in algebraic terms of the '''variational bicomplex''', without appealing to the [[calculus of variations]]. For instance, this is the case of  [[classical field theory]] on fiber bundles ([[covariant classical field theory]]).\n\nThe variational bicomplex is a [[cochain complex]] of the [[differential graded algebra]] of [[differential form|exterior forms]] on [[jet bundle|jet manifolds]] of sections of a fiber bundle. [[Lagrangian system|Lagrangian]]s and [[Lagrangian system|Euler–Lagrange operators]] on a fiber bundle are defined as elements of this bicomplex. [[Cohomology]] of the variational bicomplex leads to the global first variational formula and first [[Noether's theorem]].\n\nExtended to Lagrangian theory of even and odd fields on [[graded manifold]]s, the variational bicomplex provides strict mathematical formulation of classical field theory in a general case of reducible degenerate Lagrangians and the Lagrangian [[BRST formalism|BRST theory]].\n\n== See also ==\n\n*[[Calculus of variations]]\n*[[Lagrangian system]]\n*[[Jet bundle]]\n\n== References ==\n\n* {{Citation | last1=Takens | first1=Floris | author1-link=Floris Takens | title=A global version of the inverse problem of the calculus of variations | url=http://projecteuclid.org/getRecord?id=euclid.jdg/1214435235 | archive-url=https://archive.is/20130415170626/http://projecteuclid.org/getRecord?id=euclid.jdg/1214435235 | dead-url=yes | archive-date=2013-04-15 | mr=600611 | year=1979 | journal=Journal of Differential Geometry | issn=0022-040X | volume=14 | issue=4 | pages=543–562 }}\n* Anderson, I., \"Introduction to variational bicomplex\", ''Contemp. Math''. '''132''' (1992) 51.\n* Barnich, G., Brandt, F., Henneaux, M., \"Local BRST cohomology\", ''Phys. Rep''. '''338''' (2000) 439.\n* Giachetta, G., Mangiarotti, L., [[Gennadi Sardanashvily|Sardanashvily, G.]], ''Advanced Classical Field Theory'', World Scientific, 2009, {{isbn|978-981-283-895-7}}.\n\n== External links ==\n* Dragon, N., BRS symmetry and cohomology, [http://xxx.lanl.gov/abs/hep-th/9602163 arXiv: hep-th/9602163] \n* [[Gennadi Sardanashvily|Sardanashvily, G.]], Graded infinite-order jet manifolds, Int. G. Geom. Methods Mod. Phys. '''4''' (2007) 1335; [http://xxx.lanl.gov/abs/0708.2434 arXiv: 0708.2434v1]\n\n[[Category:Calculus of variations]]\n[[Category:Differential equations]]\n[[Category:Differential geometry]]\n\n{{physics-stub}}\n{{mathanalysis-stub}}"
    },
    {
      "title": "Variational inequality",
      "url": "https://en.wikipedia.org/wiki/Variational_inequality",
      "text": "In [[mathematics]], a '''variational inequality''' is an [[inequality (mathematics)|inequality]] involving a [[Functional (mathematics)|functional]], which has to be [[Inequality (mathematics)#Solving Inequalities|solved]] for all possible values of a given [[Variable (mathematics)|variable]], belonging usually to a [[convex set]]. The [[mathematical]] [[theory]] of variational inequalities was initially developed to deal with [[Equilibrium point|equilibrium]] problems, precisely the [[Signorini problem]]: in that model problem, the functional involved was obtained as the [[first variation]] of the involved [[Signorini_problem#The_potential_energy|potential energy]] therefore it has a [[Calculus of variation|variational origin]], recalled by the name of the general abstract problem. The applicability of the theory has since been expanded to include problems from [[economics]], [[finance]], [[Optimization (mathematics)|optimization]] and [[game theory]].\n\n== History ==\nThe first problem involving a variational inequality was the [[Signorini problem]], posed by [[Antonio Signorini]] in 1959 and solved by [[Gaetano Fichera]] in 1963, according to the references {{Harv|Antman|1983|pp=282–284}} and {{Harv|Fichera|1995}}: the first papers of the theory were {{Harv|Fichera|1963}} and {{Harv|Fichera|1964a}}, {{Harv|Fichera|1964b}}. Later on, [[Guido Stampacchia]] proved his generalization to the [[Lax–Milgram theorem]] in {{Harv|Stampacchia|1964}} in order to study the [[regularity problem]] for [[partial differential equation]]s and [[coin]]ed the name \"variational inequality\" for all the problems involving [[inequality (mathematics)|inequalities]] of this kind. [[Georges Duvaut]] encouraged his [[graduate student]]s to study and expand on Fichera's work, after attending a conference in [[Brixen]] on 1965 where Fichera presented his study of the Signorini problem, as {{Harvnb|Antman|1983|p=283}} reports: thus the theory become widely known throughout [[France]]. Also in 1965, Stampacchia and [[Jacques-Louis Lions]] extended earlier results of {{Harv|Stampacchia|1964}}, announcing them in the paper {{Harv|Lions|Stampacchia|1965}}: full proofs of their results appeared later in the paper {{Harv|Lions|Stampacchia|1967}}.\n\n== Definition ==\nFollowing {{Harvtxt|Antman|1983|p=283}}, the formal definition of a variational inequality is the following one.\n\n{{EquationRef|1|Definition 1.}} Given a [[Banach space]] <math>\\boldsymbol{E}</math>, a [[subset]] <math>\\boldsymbol{K}</math> of <math>\\boldsymbol{E}</math>, and a functional <math>F\\colon \\boldsymbol{K}\\to \\boldsymbol{E}^{\\ast}</math> from <math>\\boldsymbol{K}</math> to the [[dual space]] <math>\\boldsymbol{E}^{\\ast}</math> of the space <math>\\boldsymbol{E}</math>, \nthe variational inequality problem \nis the problem of [[Inequality (mathematics)#Solving Inequalities|solving]] \nfor the [[variable (mathematics)|variable]] <math>x</math> belonging to <math>\\boldsymbol{K}</math> the following [[inequality (mathematics)|inequality]]:\n\n:<math>\\langle F(x), y-x \\rangle \\geq 0\\qquad\\forall y \\in \\boldsymbol{K}</math>\n\nwhere <math>\\langle\\cdot,\\cdot\\rangle\\colon \n\\boldsymbol{E}^{\\ast}\\times\\boldsymbol{E}\\to\n\\mathbb{R}</math> \nis the [[Dual space|duality pairing]]. \n\nIn general, the variational inequality problem can be formulated on any [[Finite set|finite]] – or [[Infinite set|infinite]]-[[dimension]]al [[Banach space]]. The three obvious steps in the study of the problem are the following ones:\n#Prove the existence of a solution: this step implies the ''mathematical correctness'' of the problem, showing that there is at least a solution.\n#Prove the uniqueness of the given solution: this step implies the ''physical correctness'' of the problem, showing that the solution can be used to represent a physical phenomenon. It is a particularly important step since most of the problems modeled by variational inequalities are of physical origin.\n#Find the solution.\n\n==Examples==\n===The problem of finding the minimal value of a real-valued function of real variable===\nThis is a standard example problem, reported by {{Harvtxt|Antman|1983|p=283}}: consider the problem of finding the [[minimum|minimal value]] of a  [[differentiable function]] <math>f</math> over a [[closed interval]] <math>I = [a,b]</math>. Let <math>x^{\\ast}</math> be a point in <math>I</math> where the minimum occurs. Three cases can occur:\n\n# if <math>a<x^{\\ast}< b,</math> then <math>f^{\\prime}(x^{\\ast}) = 0;</math>\n# if <math>x^{\\ast}=a,</math> then <math>f^{\\prime}(x^{\\ast}) \\ge 0;</math>\n# if <math>x^{\\ast}=b,</math> then <math>f^{\\prime}(x^{\\ast}) \\le 0.</math>\n\nThese necessary conditions can be summarized as the problem of finding  <math>x^{\\ast}\\in I</math> such that\n\n:<math>f^{\\prime}(x^{\\ast})(y-x^{\\ast}) \\geq 0\\quad</math> for <math>\\quad\\forall y \\in I.</math>\n\nThe absolute minimum must be searched between the solutions (if more than one) of the preceding [[inequality (mathematics)|inequality]]: note that the solution is a [[real number]], therefore this is a finite [[Dimension (mathematics)|dimensional]] variational inequality.\n\n===The general finite-dimensional variational inequality===\nA formulation of the general problem in <math>\\mathbb{R}^n</math> \nis the following: given a [[subset]] \n<math>K</math> of \n<math>\\mathbb{R}^{n}</math> \nand a [[Map (mathematics)|mapping]] \n<math>F\\colon K\\to\\mathbb{R}^{n}</math>, \nthe [[Finite set|finite]]-[[dimension]]al variational inequality problem associated with <math>K</math> \nconsist of finding a [[Dimension|<math>n</math>-dimensional]] [[Euclidean vector|vector]] <math>x</math> belonging to \n<math>K</math> such that \n\n:<math>\\langle F(x), y-x \\rangle \\geq 0\\qquad\\forall y \\in K</math>\n\nwhere <math>\\langle\\cdot,\\cdot\\rangle\\colon\\mathbb{R}^{n}\\times\\mathbb{R}^{n}\\to\\mathbb{R}</math> \nis the standard [[inner product]] on the [[vector space]] <math>\\mathbb{R}^{n}</math>.\n\n=== The variational inequality for the Signorini problem ===\n[[File:Classical Signorini problem.svg|thumb|right|400px|The classical [[Signorini problem]]: what will be the [[Linear elasticity#Elastostatics|equilibrium]] [[Continuum mechanics#Mathematical modeling of a continuum|configuration]] of the orange spherically shaped [[Physical body|elastic body]] resting on the blue [[Rigid body|rigid]] [[friction]]less [[Plane (geometry)|plane]]?]]\nIn the historical survey {{Harv|Fichera|1995}}, [[Gaetano Fichera]] describes the genesis of his solution to the [[Signorini problem]]: the problem consist in finding the [[Linear elasticity#Elastostatics|elastic equilibrium]] [[Continuum mechanics#Mathematical modeling of a continuum|configuration]] \n<math>\\boldsymbol{u}(\\boldsymbol{x})\n=\\left(u_1(\\boldsymbol{x}),u_2(\\boldsymbol{x}),u_3(\\boldsymbol{x})\\right)</math> \nof an [[Anisotropy#Material science and engineering|anisotropic]] [[Homogeneous media|non-homogeneous]] [[Physical body|elastic body]] that lies in a [[subset]] \n<math>A</math> of the three-[[dimension]]al [[euclidean space]] whose [[boundary (topology)|boundary]] is <math>\\partial A</math>, resting on a [[Rigid body|rigid]] [[frictionless]] [[Surface (topology)|surface]] and subject only to its [[Weight|mass force]]s. \nThe solution '''<math>u</math>''' of the problem exists and is unique (under precise assumptions) in the [[set (mathematics)|set]] of '''admissible displacements''' \n<math>\\mathcal{U}_\\Sigma</math> \ni.e. the set of [[displacement vector]]s satisfying the system of [[Signorini problem#The ambiguous boundary conditions|ambiguous boundary conditions]] if and only if\n\n:<math>B(\\boldsymbol{u},\\boldsymbol{v}) - F(\\boldsymbol{v}) \\geq 0 \\qquad \\forall \\boldsymbol{v} \\in \\mathcal{U}_\\Sigma </math>\n\nwhere <math>B(\\boldsymbol{u},\\boldsymbol{v}) </math> \nand <math>F(\\boldsymbol{v}) </math> \nare the following [[Functional (mathematics)|functionals]], \nwritten using the [[Einstein notation]]\n\n:<math>B(\\boldsymbol{u},\\boldsymbol{v}) = -\\int_A \\sigma_{ik}(\\boldsymbol{u})\\varepsilon_{ik}(\\boldsymbol{v})\\,\\mathrm{d}x</math>,&nbsp;&nbsp;&nbsp;  <math>F(\\boldsymbol{v}) = \\int_A v_i f_i\\,\\mathrm{d}x + \\int_{\\partial A\\setminus\\Sigma}\\!\\!\\!\\!\\! v_i g_i \\,\\mathrm{d}\\sigma</math>,&nbsp;&nbsp;&nbsp;  <math>\\boldsymbol{u},\\boldsymbol{v} \\in \\mathcal{U}_\\Sigma </math>\n\nwhere, for all <math>\\boldsymbol{x}\\in A</math>, \n*<math>\\Sigma</math> is the [[Contact (mechanics)|contact]] [[Surface (topology)|surface]] (or more generally a contact [[set (mathematics)|set]]),\n*<math>\\boldsymbol{f}(\\boldsymbol{x}) = \\left( f_1(\\boldsymbol{x}), f_2(\\boldsymbol{x}), f_3(\\boldsymbol{x}) \\right)</math> is the ''[[body force]]'' applied to the body,\n*<math>\\boldsymbol{g}(\\boldsymbol{x})=\\left(g_1(\\boldsymbol{x}),g_2(\\boldsymbol{x}),g_3(\\boldsymbol{x})\\right)</math> is the [[surface force]] applied to <math>\\partial A\\!\\setminus\\!\\Sigma</math>,\n*<math>\\boldsymbol{\\varepsilon}=\\boldsymbol{\\varepsilon}(\\boldsymbol{u})=\\left(\\varepsilon_{ik}(\\boldsymbol{u})\\right)=\\left(\\frac{1}{2} \\left( \\frac{\\partial u_i}{\\partial x_k} + \\frac{\\partial u_k}{\\partial x_i} \\right)\\right)</math> is the [[Infinitesimal strain#Infinitesimal strain tensor|infinitesimal strain tensor]],\n*<math>\\boldsymbol{\\sigma}=\\left(\\sigma_{ik}\\right)</math> is the [[Cauchy stress tensor]], defined as\n::<math>\\sigma_{ik}= - \\frac{\\partial W}{\\partial \\varepsilon_{ik}} \\qquad\\forall i,k=1,2,3</math>\n\n:where <math>W(\\boldsymbol{\\varepsilon})=a_{ikjh}(\\boldsymbol{x})\\varepsilon_{ik}\\varepsilon_{ik}</math> <!---- may be more correct: \\varepsilon_{ik}\\varepsilon_{jh} ? ----> is the [[elastic potential energy]] and <math>\\boldsymbol{a}(\\boldsymbol{x})=\\left(a_{ikjh}(\\boldsymbol{x})\\right)</math> is the [[elasticity tensor]].\n\n==See also==\n*[[Complementarity theory]]\n*[[Differential variational inequality]]\n*[[Extended Mathematical Programming (EMP)#Equilibrium Problems|Extended Mathematical Programming for Equilibrium Problems]]\n*[[Mathematical programming with equilibrium constraints]]\n*[[Obstacle problem]]\n*[[Projected dynamical system]]\n*[[Signorini problem]]\n\n== References ==\n===Historical references===\n*{{Citation\n| last = Antman\n| first = Stuart \n| authorlink = Stuart Antman\n| author-link = \n| title = The influence of elasticity in analysis: modern developments\n| journal = [[Bulletin of the American Mathematical Society]]\n| volume = 9\n| issue = 3\n| pages = 267–291\n| date = \n| year = 1983\n| url = http://www.ams.org/bull/1983-09-03/S0273-0979-1983-15185-6/home.html\n| doi = 10.1090/S0273-0979-1983-15185-6 \n| mr = 714990 \n| zbl = 0533.73001\n}}. An historical paper about the fruitful interaction of [[elasticity theory]] and [[mathematical analysis]]: the creation of the theory of [[variational inequalities]] by [[Gaetano Fichera]] is described in §5, pages 282–284.\n*{{Citation\n  | last = Duvaut\n  | first = Georges\n  | author-link = Georges Duvaut\n  | contribution = Problèmes unilatéraux en mécanique des milieux continus\n  | contribution-url = http://www.mathunion.org/ICM/ICM1970.3/Main/icm1970.3.0071.0078.ocr.pdf\n  | series = [[International Congress of Mathematicians|ICM Proceedings]]\n  | title = Actes du Congrès international des mathématiciens, 1970\n  | volume = Mathématiques appliquées (E), Histoire et Enseignement (F) – Volume 3\n  | pages = 71–78\n  | year = 1971\n  | place = [[Paris]]\n  | publisher = [[Gauthier-Villars]]\n  | url = http://www.mathunion.org/ICM/ICM1970.3/\n  | id = \n  | mr = \n  | zbl = \n  | access-date = 2015-07-25\n  | archive-url = https://web.archive.org/web/20150725141516/http://www.mathunion.org/ICM/ICM1970.3/\n  | archive-date = 2015-07-25\n  | dead-url = yes\n  | df = \n  }}. A brief research survey describing the field of variational inequalities, precisely the sub-field of [[continuum mechanics]] problems with unilateral constraints.\n*{{Citation\n| first = Gaetano\n| last = Fichera\n| author-link = Gaetano Fichera\n| editor-last = \n| editor-first = \n| contribution = La nascita della teoria delle disequazioni variazionali ricordata dopo trent'anni \n| title = Incontro scientifico italo-spagnolo. Roma, 21 ottobre 1993\n| url = http://www.lincei.it/pubblicazioni/catalogo/volume.php?lg=e&rid=32885\n| language = Italian\n| year = 1995\n| pages = 47–53\n| place = [[Rome|Roma]]\n| series = Atti dei Convegni Lincei\n| volume = 114\n| publisher = [[Accademia Nazionale dei Lincei]]\n}}. ''The birth of the theory of variational inequalities remembered thirty years later'' (English translation of the title) is an historical paper describing the beginning of the theory of variational inequalities from the point of view of its founder.\n\n===Scientific works===\n*{{Citation\n| last1=Facchinei \n| first1=Francisco \n| author1-link=\n| last2=Pang \n| first2=Jong-Shi \n| author2-link=\n| title=Finite Dimensional Variational Inequalities and Complementarity Problems, Vol. 1\n| series = Springer Series in Operations Research\n| publisher=[[Springer-Verlag]] \n| location= [[Berlin]]–[[Heidelberg]]–[[New York City|New York]] \n| isbn=0-387-95580-1 \n| year=2003\n| zbl=1062.90001\n}}\n*{{Citation\n| last1=Facchinei \n| first1=Francisco \n| author1-link=\n| last2=Pang \n| first2=Jong-Shi \n| author2-link=\n| title=Finite Dimensional Variational Inequalities and Complementarity Problems, Vol. 2\n| series = Springer Series in Operations Research\n| publisher=[[Springer-Verlag]] \n| location= [[Berlin]]–[[Heidelberg]]–[[New York City|New York]] \n| isbn=0-387-95581-X \n| year=2003\n| zbl=1062.90001\n}}\n*{{citation\n| last = Fichera\n| first = Gaetano\n| author-link = Gaetano Fichera\n| title = Sul problema elastostatico di Signorini con ambigue condizioni al contorno \n| journal = Rendiconti della [[Accademia Nazionale dei Lincei]], Classe di Scienze Fisiche, Matematiche e Naturali\n| language = Italian\n| volume = 34 \n| series =  8\n| issue = 2\n| year = 1963\n| pages=138–142\n| zbl= 0128.18305\n}}. \"''On the elastostatic problem of Signorini with ambiguous boundary conditions''\" (English translation of the title) is a short research note announcing and describing the solution of the Signorini problem.\n*{{citation\n| last = Fichera\n| first = Gaetano\n| author-link = Gaetano Fichera\n| title = Problemi elastostatici con vincoli unilaterali: il problema di Signorini con ambigue condizioni al contorno \n| journal = Memorie della [[Accademia Nazionale dei Lincei]], Classe di Scienze Fisiche, Matematiche e Naturali\n| language = Italian\n| volume = 7\n| series =  8\n| issue = 2\n| year = 1964a\n| pages=91–140\n| zbl = 0146.21204\n}}. \"''Elastostatic problems with unilateral constraints: the Signorini problem with ambiguous boundary conditions''\" (English translation of the title) is the first paper where an [[Existence theorem|existence]] and [[uniqueness theorem]] for the Signorini problem is proved.\n* {{citation\n| last = Fichera\n| first = Gaetano\n| contribution = Elastostatic problems with unilateral constraints: the Signorini problem with ambiguous boundary conditions\n| title = Seminari dell'istituto Nazionale di Alta Matematica 1962–1963\n| year = 1964b\n| publisher = Edizioni Cremonese\n| place = [[Rome]]\n| pages=613–679\n}}. An English translation of {{Harv|Fichera|1964a}}.\n*{{Citation\n| last = Glowinski\n| first = Roland \n| author-link = Roland Glowinski\n| last2 = Lions\n| first2 = Jacques-Louis \n| author2-link = Jacques-Louis Lions\n| last3 = Trémolières\n| first3 = Raymond \n| author3-link=\n| title = Numerical analysis of variational inequalities. Translated from the French\n| place = [[Amsterdam]]–[[New York City|New York]]–[[Oxford]]\n| publisher = [[Elsevier|North-Holland]]\n| year = 1981\n| series = Studies in Mathematics and its Applications\n| volume = 8\n| mr = 635927 \n| isbn = 0-444-86199-8\n| zbl = 0463.65046\n| pages = xxix+776 }}\n*{{Citation\n| last1=Kinderlehrer \n| first1=David \n| author1-link=David Kinderlehrer\n| last2=Stampacchia \n| first2=Guido \n| author2-link=Guido Stampacchia\n| title=An Introduction to Variational Inequalities and Their Applications \n| publisher=[[Academic Press]] \n| series= Pure and Applied Mathematics\n| url = https://books.google.com/books?id=B1cPRJ3qiw0C&printsec=frontcover&dq=An+Introduction+to+Variational+Inequalities+and+Their+Applications\n| volume = 88\n| location=[[Boston]]–[[London]]–[[New York City|New York]]–[[San Diego]]–[[Sydney]]–[[Tokyo]]–[[Toronto]]\n| isbn=0-89871-466-4\n| year=1980\n| zbl=0457.35001}}.\n*{{Citation\n| last = Lions\n| first = Jacques-Louis\n| author-link = Jacques-Louis Lions\n| last2 = Stampacchia\n| first2 = Guido\n| author2-link = Guido Stampacchia\n| title = Inéquations variationnelles non coercives\n| journal = Comptes rendus hebdomadaires des séances de l'Académie des sciences\n| volume = 261\n| pages = 25–27\n| year = 1965\n| url = http://gallica.bnf.fr/ark:/12148/bpt6k4022z.image.r=Comptes+Rendus+Academie.langEN.f26.pagination\n| zbl = 0136.11906\n}}, available at [[Gallica]]. Announcements of the results of paper {{Harv|Lions|Stampacchia|1967}}.\n*{{Citation\n| last = Lions\n| first = Jacques-Louis\n| author-link = Jacques-Louis Lions\n| last2 = Stampacchia\n| first2 = Guido\n| author2-link = Guido Stampacchia\n| title = Variational inequalities\n| journal = [https://archive.today/20130105082552/http://www3.interscience.wiley.com/journal/29240/home?CRETRY=1&SRETRY=0 Communications on Pure and Applied Mathematics]\n| volume = 20\n| pages = 493–519 \n| year = 1967\n| url = http://www3.interscience.wiley.com/journal/113397217/abstract\n| archive-url = https://archive.today/20130105092629/http://www3.interscience.wiley.com/journal/113397217/abstract\n| dead-url = yes\n| archive-date = 2013-01-05\n| doi = 10.1002/cpa.3160200302\n| zbl = 0152.34601\n| issue = 3\n}}. An important paper, describing the abstract approach of the authors to the theory of variational inequalities.\n*{{citation\n|last=Roubíček\n|first= Tomáš\n|title=Nonlinear Partial Differential Equations with Applications\n|series=ISNM. International Series of Numerical Mathematics\n|volume=153\n|publisher= [[Birkhäuser Verlag]]\n|place= Basel–Boston–Berlin\n|pages= xx+476\n|edition=2nd\n|year= 2013\n|isbn= 978-3-0348-0512-4\n|mr=3014456\n|zbl=1270.35005\n|doi=10.1007/978-3-0348-0513-1}}.\n*{{Citation\n| last = Stampacchia\n| first = Guido\n| author-link = Guido Stampacchia\n| title = Formes bilineaires coercitives sur les ensembles convexes\n| journal = Comptes rendus hebdomadaires des séances de l'Académie des sciences\n| volume = 258\n| pages = 4413–4416\n| year = 1964\n| url = http://gallica.bnf.fr/ark:/12148/bpt6k4012p.image.r=Comptes+Rendus+Academie.f20.langEN\n| doi = \n| zbl = 0124.06401\n}}, available at [[Gallica]]. The paper containing Stampacchia's generalization of the [[Lax–Milgram theorem]].\n\n== External links ==\n*{{springer\n | title= Variational inequalities\n | id= V/v120010\n | last= Panagiotopoulos\n | first= P.D. \n | author-link= \n}}\n* [https://www.scilag.net/problem/G-180630.1  Alessio Figalli,  On global homogeneous solutions to the Signorini problem,]\n\n{{DEFAULTSORT:Variational Inequality}}\n[[Category:Partial differential equations]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Variational vector field",
      "url": "https://en.wikipedia.org/wiki/Variational_vector_field",
      "text": "In the [[mathematics|mathematical fields]] of the [[calculus of variations]] and [[differential geometry]], the '''variational vector field''' is a certain type of [[vector field]] defined on the [[tangent bundle]] of a [[differentiable manifold]] which gives rise to variations along a vector field in the manifold itself.\n\nSpecifically, let ''X'' be a vector field on ''M''.  Then ''X'' generates a [[one-parameter group]] of [[local diffeomorphism]]s ''Fl''<sub>X</sub><sup>t</sup>, the [[vector flow|flow]] along ''X''.  The [[pushforward (differential)|differential]] of ''Fl''<sub>X</sub><sup>t</sup> gives, for each ''t'', a mapping\n\n:<math> d\\mathrm{Fl}_X^t : TM \\to TM </math>\n\nwhere ''TM'' denotes the tangent bundle of ''M''.  This is a one-parameter group of local diffeomorphisms of the tangent bundle.  The variational vector field of ''X'', denoted by ''T''(''X'') is the tangent to the flow of ''d Fl''<sub>X</sub><sup>t</sup>.\n\n==References==\n\n* {{cite book|author=Shlomo Sternberg|authorlink=Shlomo Sternberg|title=Lectures on differential geometry|publisher=Prentice-Hall|year=1964|pages=96}}\n\n[[Category:Calculus of variations]]\n\n\n{{geometry-stub}}"
    },
    {
      "title": "Weierstrass–Erdmann condition",
      "url": "https://en.wikipedia.org/wiki/Weierstrass%E2%80%93Erdmann_condition",
      "text": "The '''Weierstrass–Erdmann condition''' is a technical tool from the [[calculus of variations]]. This condition gives the sufficient conditions for an [[extremal]] to have a corner.<ref>{{cite book |first=I. M. |last=Gelfand |authorlink=Israel Gelfand |first2=S. V. |last2=Fomin |authorlink2=Sergei Fomin |title=Calculus of Variations |location=Englewood Cliffs, NJ |publisher=Prentice-Hall |year=1963 |isbn= |pages=61–63 |url=https://books.google.com/books?id=CeC7AQAAQBAJ&pg=PA61 }}</ref>\n\n== Conditions ==\n\nThe condition says that, along a [[piecewise smooth]] extremal ''x''(''t'') (i.e. an extremal which is smooth except at a finite number of corners) for an [[integral]] <math>J=\\int f(t,x,y)\\,dt</math>, the [[partial derivative]] <math>\\partial f/\\partial x</math> must be [[Continuous function|continuous]] at a corner&nbsp;''T''. That is, if one takes the [[Limit of a function|limit]] of partials on both sides of the corner as one approaches the corner&nbsp;''T'', the result must be the same answer.\n\n== Applications ==\n\nThe condition allows one to prove that a corner exists along a given extremal. As a result, there are many applications to [[differential geometry]]. In calculations of the [[Weierstrass's elliptic functions|Weierstrass E-Function]], it is often helpful to find where corners exist along the curves. Similarly, the condition allows for one to find a minimizing curve for a given integral.\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Weierstrass-Erdmann Condition}}\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Chain of events",
      "url": "https://en.wikipedia.org/wiki/Chain_of_events",
      "text": "{{For|the 1958 film ''Chain of Events''|Chain of Events}}\nA '''chain of events''' is a number of [[Action theory (philosophy)|actions]] and their effects that are [[Contiguity (probability theory)|contiguous]] and linked together that results in a particular outcome. In the [[physical science]]s, [[chain reaction]]s are a primary example. \n\n==Determinism==\n{{Main|Determinism}}\n''Determinism'' is the [[philosophy|philosophical]] [[proposition]] that every event, including human cognition and behaviour, decision and action, is [[causality|causally]] determined by an unbroken ''chain of events''.<ref>Van Inwagen, Peter, 1983, ''An Essay on Free Will'', Oxford: Clarendon Press.</ref></blockquote> With numerous historical debates, many varieties and philosophical positions on the subject of determinism exist from traditions throughout the world.\n\n==In value theory==\nIn [[value theory]], it is the amount of cause and effects of the chain of events before generating [[intrinsic value (ethics)|intrinsic value]] that separates [[Instrumental value#High and low grades|high and low grades of instrumental value]]. The ''chain of events duration'' is the time it takes to reach the terminal event. In value theory this is generally the intrinsic value (also called terminal value). It is contrasted with [[ethic value duration]], which is the time that an object has any [[value intensity]].\n\n==Fabric of events==\nA ''fabric of events'' is an expansion of the ''chain of events'', emphasizing that chains of events are intertwined with each other, as a [[Textile|fabric]].{{CN|date=April 2014}}\n\n==In accident analysis==\n\n{{Main|Chain of events (accident analysis)}}\nIn [[accident analysis]] - for example, in the analysis of aviation accidents - a '''chain of events''' (or '''error chain''') consists of the contributing factors leading to an undesired outcome.<ref name=\"GFDPrivate\">{{cite book |author=Willits, Pat |others=Mike Abbott and Liz Kailey |title=Guided Flight Discovery: Private Pilot |url=http://www.jeppesen.com/wlcs/application/commercewf?origin=itemsummary.jsp&event=link(details)&wlcs_catalog_item_sku=JS314500&wlcs_catalog_category_id=AT1A1A8&wlcs_document_type=details |year=2007 |publisher=[[Jeppesen]] |location=[[Englewood, Colorado|Englewood]] |isbn=0-88487-429-X |oclc=145504766 |pages=10–26 |access-date=2008-05-04 |archive-url=https://web.archive.org/web/20071011033547/http://www.jeppesen.com/wlcs/application/commercewf?origin=itemsummary.jsp&event=link(details)&wlcs_catalog_item_sku=JS314500&wlcs_catalog_category_id=AT1A1A8&wlcs_document_type=details |archive-date=2007-10-11 |dead-url=yes }}</ref><ref>{{cite web|url=https://books.google.co.uk/books?id=DlgaHNs3EjoC&pg=PA21&dq=%22chain+of+events%22+accidents&hl=en&sa=X&ved=0ahUKEwievvXXuOPZAhXII8AKHcukDEEQ6AEIWjAJ#v=onepage&q=%22chain+of+events%22+accidents&f=false|title=Improving Safety-related Rules Compliance in the Public Transportation Industry|first=Judith B.|last=Gertler|date=11 March 2018|publisher=Transportation Research Board|via=Google Books}}</ref><ref>{{cite web|url=https://books.google.co.uk/books?id=4n4dBAAAQBAJ&pg=PA84&dq=%22chain+of+events%22+accidents&hl=en&sa=X&ved=0ahUKEwievvXXuOPZAhXII8AKHcukDEEQ6AEIVjAI#v=onepage&q=%22chain+of+events%22+accidents&f=false|title=International Conference on Social, Education and Management Engineering|date=9 July 2014|publisher=DEStech Publications, Inc|via=Google Books}}</ref><ref>{{cite web|url=https://books.google.co.uk/books?id=rn9Sbl9jMzIC&pg=PA133&dq=%22chain+of+events%22+accidents&hl=en&sa=X&ved=0ahUKEwievvXXuOPZAhXII8AKHcukDEEQ6AEIUDAH#v=onepage&q=%22chain+of+events%22+accidents&f=false|title=Accident/Incident Prevention Techniques, Second Edition|first=Charles D.|last=Reese|date=25 October 2011|publisher=CRC Press|via=Google Books}}</ref><ref>{{cite web|url=https://books.google.co.uk/books?id=BdtY7uQeu-YC&pg=PA16&dq=%22chain+of+events%22+accidents&hl=en&sa=X&ved=0ahUKEwievvXXuOPZAhXII8AKHcukDEEQ6AEIRTAF#v=onepage&q=%22chain+of+events%22+accidents&f=false|title=Principles of Risk-Based Decision Making|first=In c ABS|last=Consulting|date=1 February 2002|publisher=Government Institutes|via=Google Books}}</ref><ref>{{cite web|url=https://books.google.co.uk/books?id=Ceuq9P4hLJMC&pg=RA1-PT590&dq=%22chain+of+events%22+accidents&hl=en&sa=X&ved=0ahUKEwievvXXuOPZAhXII8AKHcukDEEQ6AEILDAB#v=onepage&q=%22chain+of+events%22+accidents&f=false|title=Encyclopaedia of Occupational Health and Safety|first=Jeanne Mager|last=Stellman|date=11 March 1998|publisher=International Labour Organization|via=Google Books}}</ref>\n\n==See also==\n* [[Chain reaction]]\n\n==References==\n{{Reflist}}\n{{DEFAULTSORT:Chain Of Events}}\n[[Category:Action (physics)]]\n[[Category:Determinism]]\n[[Category:Causality]]"
    },
    {
      "title": "Amoeboid movement",
      "url": "https://en.wikipedia.org/wiki/Amoeboid_movement",
      "text": "'''Amoeboid movement''' is the most common mode of locomotion in eukaryotic cells.<ref name=\"plosone.org\">{{Citation needed|date=April 2019}}journal|last1=Nishigami|first1=Yukinori|last2=Ichikawa|first2=Masatoshi|last3=Kazama|first3=Toshiya|last4=Kobayashi|first4=Ryo|last5=Shimmen|first5=Teruo|last6=Yoshikawa|first6=Kenichi|last7=Sonobe|first7=Seiji|last8=Kabla|first8=Alexandre J.|title=Reconstruction of Active Regular Motion in Amoeba Extract: Dynamic Cooperation between Sol and Gel States|journal=PLoS ONE|date=5 August 2013|volume=8|issue=8|pages=e70317|doi=10.1371/journal.pone.0070317|pmid=23940560|pmc=3734023}}</ref> It is a crawling-like type of movement accomplished by protrusion of [[cytoplasm]] of the [[Cell (biology)|cell]] involving the formation of [[pseudopod]]ia (\"false-feet\") and posterior [[Uropod (immunology)|uropods]]. One or more pseudopodia may be produced at a time depending on the organism, but all amoeboid movement is characterized by the movement of organisms with an amorphous form that possess no set motility structures. Movement occurs when the cytoplasm slides and forms a pseudopodium in front to pull the cell forward. This type of movement has been linked to changes in [[action potential]], though the exact mechanism is still unknown. Some examples of organisms that exhibit this type of locomotion are the [[amoeboids]], [[slime mold]]s and some [[protozoa]]ns such as ''[[Naegleria gruberi]]'',<ref>{{cite journal|last1=Preston|first1=TM|last2=Cooper|first2=LG|last3=King|first3=CA|title=Amoeboid locomotion of Naegleria gruberi: the effects of cytochalasin B on cell-substratum interactions and motile behavior.|journal=The Journal of Protozoology|date=Jul–Aug 1990|volume=37|issue=4|pages=6S–11S|pmid=2258833|doi=10.1111/j.1550-7408.1990.tb01139.x}}</ref> as well as some cells in humans such as [[white blood cell|leukocytes]]. [[Sarcoma]]s, or cancers arising from connective tissue cells, are particularly adept at amoeboid movement, thus leading to their high rate of [[metastasis]]. {{Citation \n\nWhile several hypotheses have been proposed to explain the mechanism of amoeboid movement, the exact mechanism is still unknown.<ref>\nR D Allen, and N S Allen.\n\"Cytoplasmic Streaming in Amoeboid Movement\"\nhttp://www.annualreviews.org/doi/abs/10.1146/annurev.bb.07.060178.002345\n{{Cite journal | last1 = Allen | first1 = R. D. | last2 = Allen | first2 = N. S. | doi = 10.1146/annurev.bb.07.060178.002345 | title = Cytoplasmic Streaming in Amoeboid Movement | journal = Annual Review of Biophysics and Bioengineering | volume = 7 | pages = 469–495 | year = 1978 | pmid =  352246| pmc = }}\n</ref>{{Better source|reason=This source is nearly 40 years old.|date=June 2015}}\n\n== Molecular mechanism of cell motion ==\n<!-- [[WP:NFCC]] violation: [[File:Collective Mechanism of Cell Motion.jpg|thumb|upright=1.5|Schematic representation of the collective biomechanical and molecular mechanism of cell motion]] -->\n\n=== Sol-gel theory ===\nThe [[protoplasm]] of an amoeba is made up of an outer layer termed the [[Ectoplasm (cell biology)|ectoplasm]] which surrounds an inner portion called the [[endoplasm]].&nbsp;The&nbsp;ectoplasm&nbsp;consists of a gelatinous semisolid called plasma gel whereas the&nbsp;endoplasm&nbsp;is made up of a less viscous fluid called plasma sol. The ectoplasm owes its highly viscous state, in part, to the cross-linking [[Myofibril|actomyosin]] complex it contains. Locomotion of an amoeba is thought to occur due to the [[sol-gel]] conversion of the protoplasm within its cell. 'Sol-gel conversion describes the contraction and relaxation events which are enforced by [[osmotic pressure]] and other ionic charges.'<ref name=\"Cell and molecular biology\">{{cite book|url=https://books.google.com/?id=iXeQ1Bi9P7cC&pg=PA461&dq=what+is+sol-gel+conversion+in+cytoplasm#v=onepage&q=what%20is%20sol-gel%20conversion%20in%20cytoplasm&f=false|title=Cell and molecular biology.|last1=Rastogi|first1=S.C.|date=2010|publisher=New Age International|isbn=9788122430790|edition=3rd|location=New Delhi|page=461|accessdate=29 October 2014}}</ref>\n\nFor example, when an amoeba moves, it extends a gelatinous, cytosolic pseudopodium, which then results in the more fluid cytosol (plasma sol) flowing after the gelatinous portion (plasma gel) where it congeals at the end of the pseudopodium. This results in the extension of this appendage. On the opposite (posterior) end of the cell, plasma gel is then converted to plasma sol and streamed toward the advancing pseudopodium. As long as the cell has a way to grapple the [[Substrate (materials science)|substratum]], repetition of this process guides the cell forward. Inside the amoeba, there are proteins that can be activated to convert the gel into the more liquid sol state.\n\nCytoplasm consist largely of actin and actin is regulated by actin binding proteins. Actin binding proteins are in turn regulated by calcium ions; hence, calcium ions are very important in the sol-gel conversion process.<ref name=\"plosone.org\" /><ref name=\"Cell and molecular biology\" />\n\n=== Amoeboid movement modalities ===\n\n==== Actin-driven motility ====\nBased on some mathematical models, recent studies hypothesize a novel biological model for collective biomechanical and molecular mechanisms of cellular motion.<ref name=\"coskun2011\">{{cite journal|last1=Coskun|first1=Hasan|last2=Coskun|first2=Huseyin.|title=Cell physician: reading cell motion. A mathematical diagnostic technique through analysis of single cell motion|journal=Bull Math Biol|date=March 2011|volume=73|issue=3|pages=658–82|doi=10.1007/s11538-010-9580-x|pmid=20878250}}</ref> It is proposed that microdomains weave the texture of [[cytoskeleton]] and their interactions mark the location for formation of new adhesion sites. According to this model, microdomain signaling dynamics organize the cytoskeleton and its interaction with the substratum. As microdomains trigger and maintain active polymerization of actin filaments, their propagation and zigzagging motion on the membrane generate a highly interlinked network of curved or linear filaments oriented at a wide spectrum of angles to the cell boundary. It has also been proposed that microdomain interaction marks the formation of new focal adhesion sites at the cell periphery. The interaction of [[myosin]] with the [[actin]] network then generates membrane retraction/ruffling, retrograde flow, and contractile forces for forward motion. Finally, continuous application of stress on the old focal adhesion sites could result in the calcium-induced activation of [[calpain]], and consequently the detachment of focal adhesions which completes the cycle.\n\nIn addition to actin polymerization, [[microtubule]]s may also play an important role in cell migration where the formation of [[Lamellipodium|lamellipodia]] is involved. One experiment showed that although microtubules are not required for actin polymerization to create lamellipodial extensions, they are needed in order to afford cellular movement.<ref>{{Cite journal|last=Ballestrem|first=Christoph|last2=Wehrle-Haller|first2=Bernhard|last3=Hinz|first3=Boris|last4=Imhof|first4=Beat A.|date=2000-09-01|title=Actin-dependent Lamellipodia Formation and Microtubule-dependent Tail Retraction Control-directed Cell Migration|url=http://www.molbiolcell.org/content/11/9/2999|journal=Molecular Biology of the Cell|language=en|volume=11|issue=9|pages=2999–3012|doi=10.1091/mbc.11.9.2999|issn=1059-1524|pmid=10982396|pmc=14971}}</ref>[[File:WikiPic4.png|thumb|Two common modes of amoeboid motility]]\n\n==== Bleb-driven motility ====\nAnother such proposed mechanism, the 'bleb-driven amoeboid locomotion' mechanism, suggests that the cell cortex actomyosin contracts to increase [[Hydrostatics|hydrostatic pressure]] inside the cell. Blebbing occurs in amoeboid cells when there is a roughly spherical protrusion in the cell membrane characterized by detachment from the actomyosin cortex. This mode of amoeboid movement requires that [[myosin II]] play a role in generating the hydrostatic pressure that causes the bleb to extend.<ref>{{Cite journal|last=Yoshida|first=Kunito|last2=Soldati|first2=Thierry|date=2006-09-15|title=Dissection of amoeboid movement into two mechanically distinct modes|journal=Journal of Cell Science|language=en|volume=119|issue=18|pages=3833–3844|doi=10.1242/jcs.03152|issn=0021-9533|pmid=16926192}}</ref> &nbsp;This is different from actin-driven locomotion where the protrusion created is by the actin polymerizing while remaining attached to the actomyosin cortex and physically pushing against the cell's barrier. During the bleb-driven amoeboid movement, the cytoplasmic sol-gel state is regulated.<ref name=\"plosone.org\" />\n\nBlebbing can also be a sign of when a cell is undergoing [[apoptosis]].<ref>{{Cite journal|last=Coleman|first=Mathew L.|last2=Sahai|first2=Erik A.|last3=Yeo|first3=Margaret|last4=Bosch|first4=Marta|last5=Dewar|first5=Ann|last6=Olson|first6=Michael F.|date=2001|title=Membrane blebbing during apoptosis results from caspase-mediated activation of ROCK I|journal=Nature Cell Biology|language=En|volume=3|issue=4|pages=339–345|doi=10.1038/35070009|pmid=11283606|issn=1476-4679}}</ref>\n\nIt has also been observed  that the blebs formed by motile cells undergo a roughly uniform life cycle that lasts approximately one minute. This includes a phase involving the initial outward expansion where the membrane breaks away from the membranous cytoskeleton. This is then followed by a short static phase where the hydrostatic pressure that has built up is just enough to maintain the size of the bleb. Following this is the last phase characterized by the bleb slowly retracting and the membrane being reintroduced to the cytoskeleton infrastructure.<ref>{{Cite journal|last=Fackler|first=Oliver T.|last2=Grosse|first2=Robert|date=2008-06-16|title=Cell motility through plasma membrane blebbing|journal=The Journal of Cell Biology|language=en|volume=181|issue=6|pages=879–884|doi=10.1083/jcb.200802081|issn=0021-9525|pmid=18541702|pmc=2426937}}</ref>\n\nCells may undergo fast transitions between blebbing and lamellipodium-based motility as a means of migration. However, the rate at which these transitions are made is still unknown. Tumor cells may also exhibit rapid transitions between amoeboid motility and mesenchymal motility, another form of cellular movement.<ref>{{Cite journal|last=Bergert|first=Martin|last2=Chandradoss|first2=Stanley D.|last3=Desai|first3=Ravi A.|last4=Paluch|first4=Ewa|date=2012-09-04|title=Cell mechanics control rapid transitions between blebs and lamellipodia during migration|journal=Proceedings of the National Academy of Sciences|language=en|volume=109|issue=36|pages=14434–14439|doi=10.1073/pnas.1207968109|issn=0027-8424|pmid=22786929|pmc=3437886}}</ref>\n\n=== Related movement mechanisms ===\nDictyostelium cells and neutrophils can also swim, using a similar mechanism as for crawling.<ref>{{cite journal|last=Van Haastert|first=Peter J. M.|author2=Hotchin, Neil A.|title=Amoeboid Cells Use Protrusions for Walking, Gliding and Swimming|journal=PLoS ONE|date=8 November 2011|volume=6|issue=11|pages=e27532|doi=10.1371/journal.pone.0027532|editor1-last=Hotchin|editor1-first=Neil A|pmid=22096590|pmc=3212573}}</ref><ref>{{cite journal|last=Bae|first=A. J.|author2=Bodenschatz, E.|title=On the swimming of Dictyostelium amoebae|journal=Proceedings of the National Academy of Sciences|date=4 October 2010|volume=107|issue=44|pages=E165–E166|doi=10.1073/pnas.1011900107|pmid=20921382|pmc=2973909}}</ref>\n\nAnother unicellular form of movement shown in [[Euglena]] is known as [[metaboly]].\n\n== See also ==\n* [[Cytoplasmic streaming]]\n* [[:Category:Cell movement|Category:Cell Movement]]\n* [[Pseudopodia|Pseudopodium]]\n* [[Amoeba]]\n\n==References==\n{{Reflist}}\n\n{{Authority control}}\n\n[[Category:Cell movement]]"
    },
    {
      "title": "Bleb (cell biology)",
      "url": "https://en.wikipedia.org/wiki/Bleb_%28cell_biology%29",
      "text": "[[Image:Apoptotic cell disassembly.png|thumb|right|500px|During [[apoptosis]], blebbing is the first phase (left) of cell disassembly.<ref>{{Cite journal|last=Smith|first=Aaron|last2=Parkes|first2=Michael AF|last3=Atkin-Smith|first3=Georgia K|last4=Tixeira|first4=Rochelle|last5=Poon|first5=Ivan KH|title=Cell disassembly during apoptosis|journal=WikiJournal of Medicine|language=en|volume=4|issue=1|doi=10.15347/wjm/2017.008|year=2017}}</ref>]]\nIn [[cell biology]], a '''bleb''' is a bulge or protrusion of the [[plasma membrane]] of a cell, human bioparticulate or abscess with an internal environment similar to that of a simple cell, characterized by a spherical, bulky morphology.<ref name=\":0\"/> It is characterized by the decoupling of the [[cytoskeleton]] from the plasma membrane, degrading the internal structure of the cell, allowing the flexibility required to allow the cell to separate into individual bulges or pockets of the intercellular matrix.<ref name=\":0\"/> Most commonly, blebs are seen in [[apoptosis]] (programmed cell death) but are also seen in other non-apoptotic functions. ''Blebbing'', or ''zeiosis'', is the formation of blebs.\n\n==Formation==\n\nBleb growth is driven by intracellular pressure generated in the cytoplasm when the [[actin cortex]] undergoes actomyosin contractions.<ref name=\"A short history of blebbing\">{{cite journal|last1=Charras|first1=G. T.|title=A short history of blebbing|journal=Journal of Microscopy|date=January 8, 2008|volume=231|issue=3|pages=466–78|doi=10.1111/j.1365-2818.2008.02059.x|pmid=18755002}}<!--|accessdate=9 November 2015--></ref> The disruption of the membrane-actin cortex interactions<ref name=\":0\">{{cite journal |vauthors=Fackler OT, Grosse R |title=Cell motility through plasma membrane blebbing |journal=J. Cell Biol. |volume=181 |issue=6 |pages=879–84 |date=Jun 2008 |pmid=18541702 |doi=10.1083/jcb.200802081 |pmc=2426937 }}</ref> are dependent on the activity of [[Myosin ATPase|myosin-ATPase]]<ref name=\":1\">{{Cite journal|title = Blebs produced by actin–myosin contraction during apoptosis release damage-associated molecular pattern proteins before secondary necrosis occurs|journal = Cell Death & Differentiation|date = 2013-10-01|issn = 1350-9047|pmc = 3770329|pmid = 23787996|pages = 1293–1305|volume = 20|issue = 10|doi = 10.1038/cdd.2013.69|first = G. R.|last = Wickman|first2 = L.|last2 = Julian|first3 = K.|last3 = Mardilovich|first4 = S.|last4 = Schumacher|first5 = J.|last5 = Munro|first6 = N.|last6 = Rath|first7 = S. Al|last7 = Zander|first8 = A.|last8 = Mleczak|first9 = D.|last9 = Sumpton}}</ref>\n\nBleb formation can be initiated in two ways: 1) through local rupture of the cortex or 2) through local detachment of the cortex from the plasma membrane.<ref>{{cite journal|last1=Charras|first1=G|last2=Paluch|first2=E|title=Blebs lead the way: how to migrate without lamellipodia.|journal=Nature Reviews Molecular Cell Biology|date=Sep 2008|volume=9|issue=9|pages=730–6|pmid=18628785|doi=10.1038/nrm2453}}</ref> This generates a weak spot through which the [[cytoplasm]] flows, leading to the expansion of the bulge of membrane by increasing the surface area through tearing of the membrane from the cortex, during which time, actin levels decrease.<ref name=\"A short history of blebbing\"/> The cytoplasmic flow is driven by hydrostatic pressure inside the cell.<ref>{{cite journal|last1=Charras|first1=GT|last2=Yarrow|first2=JC|last3=Horton|first3=MA|last4=Mahadevan|first4=L|last5=Mitchison|first5=TJ|title=Non-equilibration of hydrostatic pressure in blebbing cells.|journal=Nature|date=May 19, 2005|volume=435|issue=7040|pages=365–9|pmid=15902261|doi=10.1038/nature03550|bibcode = 2005Natur.435..365C|pmc=1564437}}</ref><ref>{{cite journal|last1=Tinevez|first1=JY|last2=Schulze|first2=U|last3=Salbreux|first3=G|last4=Roensch|first4=J|last5=Joanny|first5=JF|last6=Paluch|first6=E|title=Role of cortical tension in bleb growth.|journal=Proceedings of the National Academy of Sciences of the United States of America|date=Nov 3, 2009|volume=106|issue=44|pages=18581–6|pmid=19846787|doi=10.1073/pnas.0903353106|bibcode = 2009PNAS..10618581T|pmc=2765453}}</ref>\n\n==Physiological function==\n\n=== Apoptotic function ===\nBlebbing is one of the defined features of [[apoptosis]].<ref name=\":1\" /> During apoptosis (programmed cell death), the cell's cytoskeleton breaks up and causes the membrane to bulge outward.<ref>{{cite journal |vauthors=Vermeulen K, Van Bockstaele DR, Berneman ZN |title=Apoptosis: mechanisms and relevance in cancer |journal=Ann Hematol |volume=84 |issue=10 |pages=627–39 |date=Oct 2005 |pmid=16041532 |doi=10.1007/s00277-005-1065-x }}</ref> These bulges may separate from the cell, taking a portion of [[cytoplasm]] with them, to become known as apoptotic blebs.<ref name=\"J of T\">{{cite journal|last1=van der Pol|first1=E.|last2=Böing|first2=A. N.|last3=Gool|first3=E. L.|last4=Nieuwland|first4=R.|title=Recent developments in the nomenclature, presence, isolation, detection and clinical impact of extracellular vesicles|journal=Journal of Thrombosis and Haemostasis|pages=48–56|language=en|doi=10.1111/jth.13190|pmid=26564379|date=1 January 2016|volume=14|issue=1}}</ref>[[Phagocytic cells]] eventually consume these fragments and the components are recycled.\n\nTwo types of blebs are recognized in apoptosis. Initially, small surface blebs are formed. During later stages, larger so-called dynamic blebs may appear, which may carry larger organelle fragments such as larger parts of the fragmented apoptotic [[cell nucleus]].<ref name=\"pmid28102458\">{{cite journal | authors=Tixeira R, Caruso S, Paone S, Baxter AA, Atkin-Smith GK, Hulett MD, Poon IK |title=Defining the morphologic features and products of cell disassembly during apoptosis |journal=Apoptosis |volume=22 |issue=3 |pages=475–477 |year=2017 |pmid=28102458 |doi=10.1007/s10495-017-1345-7 |url=}}</ref>\n\n=== Nonapoptotic functions ===\nBlebbing also has important functions in other cellular processes, including cell locomotion, cell division, and physical or chemical stresses. Blebs have been seen in cultured cells in certain stages of the cell cycle. These blebs are used for cell locomotion in [[embryogenesis]].<ref>{{Cite journal|title = Apoptotic and necrotic blebs in epithelial cells display similar neck diameters but different kinase dependency|journal = Cell Death & Differentiation|date = 2003-01-01|issn = 1350-9047|pages = 687–697|volume = 10|issue = 6|doi = 10.1038/sj.cdd.4401236|pmid = 12761577|first = L. F.|last = Barros|first2 = T.|last2 = Kanaseki|first3 = R.|last3 = Sabirov|first4 = S.|last4 = Morishima|first5 = J.|last5 = Castro|first6 = C. X.|last6 = Bittner|first7 = E.|last7 = Maeno|first8 = Y.|last8 = Ando-Akatsuka|first9 = Y.|last9 = Okada}}</ref> The types of blebs vary greatly, including variations in bleb growth rates, size, contents, and [[actin]] content. It also plays an important role in all five varieties of [[necrosis]], a generally detrimental process. However, cell organelles do not spread into necrotic blebs.\n\n==Inhibition==\nIn 2004, a chemical known as [[blebbistatin]] was shown to inhibit the formation of blebs. This agent was discovered in a screen for small molecule inhibitors of [[MYH9|nonmuscle myosin IIA]] and was shown to lower the affinity of [[myosin]] with [[actin]],<ref>{{cite journal |vauthors=Straight AF, Cheung A, Limouze J, Chen I, Westwood NJ, Sellers JR, Mitchison TJ |title=Dissecting temporal and Spatial control of cytokinesis with a myosin II inhibitor |journal=Science |volume=299 |issue=5613 |pages=1743–47 |date=March 2003 |pmid=12637748|doi=10.1126/science.1081412|bibcode = 2003Sci...299.1743S }}</ref><ref>{{cite journal |vauthors=Kovács M, Tóth J, Hetényi C, Málnási-Csizmadia A, Sellers JR |title=Mechanism of blebbistatin inhibition of myosin II |journal=J Biol Chem |volume=279 |issue=34 |pages=35557–63 |date=Aug 2004 |pmid=15205456 |doi=10.1074/jbc.M405319200 }}</ref><ref>{{cite journal |vauthors=Limouze J, Straight AF, Mitchison T, Sellers JR |title=Specificity of blebbistatin, an inhibitor of myosin II |journal=J Muscle Res Cell Motil. |volume=25 |issue=4–5 |pages=337–41 |year=2004 |pmid=15548862 |doi=10.1007/s10974-004-6060-7 |url=https://zenodo.org/record/1232816 }}</ref> thus altering the contractile forces that impinge on the cytoskeleton-membrane interface.\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{cite journal |vauthors=Charras GT, Coughlin M, Mitchison TJ, Mahadevan L |title=Life and times of a cellular bleb |journal=Biophys. J. |volume=94 |issue=5 |pages=1836–53 |date=Mar 2008 |pmid=17921219 |doi=10.1529/biophysj.107.113605 |pmc=2242777 |bibcode = 2008BpJ....94.1836C }}\n*{{cite journal |vauthors=Charras GT, Hu CK, Coughlin M, Mitchison TJ |title=Reassembly of contractile actin cortex in cell blebs |journal=J. Cell Biol. |volume=175 |issue=3 |pages=477–90 |date=Nov 2006 |pmid=17088428 |pmc=2064524 |doi=10.1083/jcb.200602085 }}\n*{{cite journal |vauthors=Dai J, Sheetz MP |title=Membrane tether formation from blebbing cells |journal=Biophys. J. |volume=77 |issue=6 |pages=3363–70 |date=Dec 1999 |pmid=10585959 |pmc=1300608 |url=http://www.biophysj.org/cgi/pmidlookup?view=long&pmid=10585959 |doi=10.1016/S0006-3495(99)77168-7 |bibcode=1999BpJ....77.3363D }}{{dead link|date=July 2017 |bot=InternetArchiveBot |fix-attempted=yes }}\n* [https://web.archive.org/web/20080907102503/http://focus.hms.harvard.edu/2003/March21_2003/cell_biology.html Drug Stops Motor Protein, Shines Light on Cell Division - FOCUS March 21, 2003]. Retrieved April 8, 2008.\n*{{cite journal |vauthors=Hagmann J, Burger MM, Dagan D |title=Regulation of plasma membrane blebbing by the cytoskeleton |journal=J. Cell. Biochem. |volume=73 |issue=4 |pages=488–99 |date=Jun 1999 |pmid=10733343 |doi=10.1002/(SICI)1097-4644(19990615)73:4<488::AID-JCB7>3.0.CO;2-P}}\n\n==External links==\n{{Wiktionary|bleb}}\n*[http://www.mechanobio.info/topics/cytoskeleton-dynamics/go-0032059 MBInfo - Bleb]\n*[http://www.mechanobio.info/topics/cytoskeleton-dynamics/go-0032059/go-0032060 MBInfo - Bleb Assembly]\n\n[[Category:Cell anatomy]]\n[[Category:Programmed cell death]]\n[[Category:Cell movement]]"
    },
    {
      "title": "Cell migration",
      "url": "https://en.wikipedia.org/wiki/Cell_migration",
      "text": "'''Cell migration''' is a central process in the development and maintenance of [[multicellular organism]]s. Tissue formation during [[embryogenesis|embryonic development]], [[wound healing]] and [[immune system|immune response]]s all require the orchestrated movement of cells in particular directions to specific locations. Cells often migrate in response to specific external signals, including [[chemotaxis|chemical signals]] and [[mechanotaxis|mechanical signals]].<ref name=Mak2015>{{cite journal|last1=Mak|first1=M.|last2=Spill|first2=F.|last3=Roger|first3=K.|last4=Zaman|first4=M.|title=Single-Cell Migration in Complex Microenvironments: Mechanics and Signaling Dynamics|journal=Journal of Biomechanical Engineering|doi=10.1115/1.4032188|pmid=26639083|volume=138|issue=2|pages=021004|year=2016|pmc=4844084}}</ref> Errors during this process have serious consequences, including [[intellectual disability]], [[cardiovascular disease|vascular disease]], [[tumor|tumor formation]] and [[metastasis]]. An understanding of the mechanism by which cells migrate may lead to the development of novel therapeutic strategies for controlling, for example, invasive tumour cells.\n\nDue to the highly viscous environment (low [[Reynolds number]]), cells need to permanently produce forces in order to move. Cells achieve active movement by very different mechanisms. Many less complex prokaryotic organisms (and sperm cells) use [[flagella]] or [[cilia]] to propel themselves. [[Eukaryotic]] cell migration typically is far more complex and can consist of combinations of different migration mechanisms. It generally involves drastic changes in cell shape which are driven by the [[cytoskeleton]]. Two very distinct migration scenarios are crawling motion (most commonly studied) and blebbing motility.<ref name=Huber2013>{{cite journal |last1=Huber |first1=F |last2=Schnauss |first2=J |last3=Roenicke |first3=S |last4=Rauch |first4=P |last5=Mueller |first5=K |last6=Fuetterer |first6=C |last7=Kaes |first7=J  |title=Emergent complexity of the cytoskeleton: from single filaments to tissue |journal=Advances in Physics |volume=62 |issue=1 |pages=1–112 |year=2013 |doi=10.1080/00018732.2013.771509|pmid=24748680 |pmc=3985726}} [http://www.tandfonline.com/doi/full/10.1080/00018732.2013.771509 online]</ref> A paradigmatic example of crawling motion is the case of fish epidermal keratocytes ([http://onlinelibrary.wiley.com/store/10.1002/bmb.21071/asset/supinfo/bmb21071-sup-0001-suppvideo1.avi?v=1&s=0aef0fee627851181e8069e7d62cb3b085f466d9 videomicroscopy of crawling cultured epidermal fish keratocytes]), which have been extensively used in research and teaching.<ref>{{cite journal|last1=Prieto|first1=Daniel|last2=Aparicio|first2=Gonzalo|last3=Sotelo-Silveira|first3=Jose R.|title=Cell migration analysis: A low-cost laboratory experiment for cell and developmental biology courses using keratocytes from fish scales|journal=Biochemistry and Molecular Biology Education|volume=45|issue=6|pages=475–482|date=19 June 2017|doi=10.1002/bmb.21071|pmid=28627731}}</ref>\n\n==Cell migration studies==\n[[File:Migrating MCF-10A cells.ogv|thumb|300px\n| '''Figure 1:''' A [[Time-lapse microscopy]] video of migrating MCF-10A cells,\nimaged for 16 hours using [[quantitative phase contrast microscopy|quantitative phase microscopy]].<ref name=\"MCF-10A\">{{cite web\n| title=HoloMonitor - Non-invasive image cytometers\n| publisher=Phase Holographic Imaging AB\n| url=http://www.phiab.se/products/holomonitor}}</ref>]]\n\nThe migration of [[cell culture|cultured cells]] attached to a surface is commonly studied using [[microscopy]].\nAs cell movement is very slow, a few µm/minute, [[time-lapse microscopy]] videos are recorded of the migrating cells to\nspeed up the movement. \nSuch videos (Figure 1) reveal that the leading cell front is very active, with a characteristic behavior of successive contractions and expansions.\nIt is generally accepted that the leading front is the main motor that pulls the cell forward.\n\n===Common features===\nThe processes underlying mammalian cell migration are believed to be consistent with those of (non-[[spermatozoon|spermatozooic]]) [[Amoeboid movement|locomotion]].<ref>{{cite web|title=What is Cell Migration?|url=http://www.cellmigration.org/science/#whatis|work=Cell Migration Gateway|publisher=Cell MIgration Consortium|accessdate=24 March 2013}}</ref>  Observations in common include:\n* cytoplasmic displacement at leading edge (front)\n* laminar removal of dorsally-accumulated debris toward trailing edge (back)\nThe latter feature is most easily observed when aggregates of a surface molecule are cross-linked with a fluorescent [[antibody]] or when small beads become artificially bound to the front of the cell.<ref name=Abercrombie1971>{{cite journal |last1=Abercrombie |first1=M |last2=Heaysman |first2=JE |last3=Pegrum |first3=SM |title=The locomotion of fibroblasts in culture III. Movements of particles on the dorsal surface of the leading lamella |journal=Experimental Cell Research |volume=62 |issue=2 |pages=389–98 |year=1970 |pmid=5531377 |doi=10.1016/0014-4827(70)90570-7}}</ref>\n\nOther eukaryotic cells are observed to migrate similarly. The amoeba [[Dictyostelium discoideum]] is useful to researchers because they consistently exhibit chemotaxis in response to [[cyclic adenosine monophosphate|cyclic AMP]]; they move more quickly than cultured mammalian cells; and they have a [[haploid]] genome that simplifies the process of connecting a particular gene product with its effect on cellular behaviour.<ref>{{Cite journal|date=2006-09-27|title=Signaling pathways mediating chemotaxis in the social amoeba, Dictyostelium discoideum|journal=European Journal of Cell Biology|language=en|volume=85|issue=9–10|pages=897–904|doi=10.1016/j.ejcb.2006.06.003|pmid=16962888|issn=0171-9335|last1=Willard|first1=Stacey S|last2=Devreotes|first2=Peter N}}</ref>\n[[File:Cellmigrationmodels.png|thumb|upright=1.0|Two different models for how cells move. A) Cytoskeletal model. B) Membrane Flow Model]]\n[[File:Microtubule in Cell Migration.jpg|thumb|upright=1.0|(A) Dynamic microtubules are necessary for tail retraction and are distributed at the rear end in a migrating cell. Green, highly dynamic microtubules; yellow, moderately dynamic microtubules and red, stable microtubules. (B) Stable microtubules act as struts and prevent tail retraction and thereby inhibit cell migration.]]\n\n==Molecular processes of migration==\nThere are two main theories for how the cell advances its front edge: the cytoskeletal model and membrane flow model. It is possible that both underlying processes contribute to cell extension.\n\n===Cytoskeletal model (A)===\n\n===Leading edge===\n\nExperimentation has shown that there is rapid [[actin]] polymerisation at the cell's front edge.<ref name=Wang1985>{{cite journal |last1=Wang |first1=Y. L. |title=Exchange of actin subunits at the leading edge of living fibroblasts: possible role of treadmilling |journal=The Journal of Cell Biology |volume=101 |issue=2 |pages=597–602 |year=1985 |pmid=4040521 |pmc=2113673 |doi=10.1083/jcb.101.2.597}}</ref> This observation has led to the hypothesis that formation of actin filaments \"push\" the leading edge forward and is the main motile force for advancing the cell’s front edge.<ref name=Mitch1996>{{cite journal |last1=Mitchison |first1=T |last2=Cramer |first2=LP |title=Actin-Based Cell Motility and Cell Locomotion |journal=Cell |volume=84 |issue=3 |pages=371–9 |year=1996 |pmid=8608590 |doi=10.1016/S0092-8674(00)81281-7}}</ref><ref name=Pollard2003>{{cite journal |last1=Pollard |first1=Thomas D |last2=Borisy |first2=Gary G |title=Cellular Motility Driven by Assembly and Disassembly of Actin Filaments |journal=Cell |volume=112 |issue=4 |pages=453–65 |year=2003 |pmid=12600310 |doi=10.1016/S0092-8674(03)00120-X}}</ref> In addition, cytoskeletal elements are able to interact extensively and intimately with a cell's plasma membrane.<ref>{{cite journal |last1=Doherty |first1=Gary J. |last2=McMahon |first2=Harvey T. |title=Mediation, Modulation, and Consequences of Membrane-Cytoskeleton Interactions |journal=Annual Review of Biophysics |volume=37 |pages=65–95 |year=2008 |pmid=18573073 |doi=10.1146/annurev.biophys.37.032807.125912}}</ref>\n\n===Trailing edge===\n\nOther cytoskeletal components (like microtubules) have important functions in cell migration. It has been found that microtubules act as “struts” that counteract the contractile forces that are needed for trailing edge retraction during cell movement. When microtubules in the trailing edge of cell are dynamic, they are able to remodel to allow retraction. When dynamics are suppressed, microtubules cannot remodel and, therefore, oppose the contractile forces.<ref name=pmid20696757>{{cite journal |last1=Yang |first1=Hailing |last2=Ganguly |first2=Anutosh |last3=Cabral |first3=Fernando |title=Inhibition of Cell Migration and Cell Division Correlates with Distinct Effects of Microtubule Inhibiting Drugs |journal=The Journal of Biological Chemistry |volume=285 |issue=42 |pages=32242–50 |year=2010 |pmid=20696757 |pmc=2952225 |doi=10.1074/jbc.M110.160820}}</ref> The morphology of cells with suppressed microtubule dynamics indicate that cells can extend the front edge (polarized in the direction of movement), but have difficulty retracting their trailing edge.<ref name=\"ReferenceA\">{{cite journal |last1=Ganguly |first1=A |last2=Yang |first2=H |last3=Sharma |first3=R|last4=Patel |first4=K|last5=Cabral |first5=F |title=The Role of Microtubules and Their Dynamics in Cell Migration. |journal=J Biol Chem |volume= 287|issue=52 |pages= 43359–69|year=2012 |pmid=23135278 |doi=10.1074/jbc.M112.423905|pmc=3527923}}</ref> On the other hand, high drug concentrations, or microtubule mutations that depolymerize the microtubules, can restore cell migration but there is a loss of directionality. It can be concluded that microtubules act both to restrain cell movement and to establish directionality.\n\n===Membrane flow model (B)===\nStudies have also shown that the front of the migration is the site at which the membrane is returned to the cell surface from internal membrane pools at the end of the [[endocytic cycle]].<ref name=Bretscher1983>{{cite journal |last1=Bretscher |first1=M. S. |title=Distribution of receptors for transferrin and low density lipoprotein on the surface of giant HeLa cells |journal=Proceedings of the National Academy of Sciences |volume=80 |pages=454–8 |year=1983 |doi=10.1073/pnas.80.2.454 |pmid=6300844 |issue=2 |pmc=393396}}</ref> This has led to the hypothesis that extension of the leading edge occurs primarily by addition of membrane at the front of the cell. If so, the actin filaments that form at the front might stabilize the added membrane so that a structured extension, or lamella, is formed rather than a bubble-like structure (or bleb) at its front.<ref name=Bretscher1996>{{cite journal |last1=Bretscher |first1=M |title=Getting Membrane Flow and the Cytoskeleton to Cooperate in Moving Cells |journal=Cell |volume=87 |issue=4 |pages=601–6 |year=1996 |pmid=8929529 |doi=10.1016/S0092-8674(00)81380-X}}</ref> For a cell to move, it is necessary to bring a fresh supply of \"feet\" (proteins called [[integrins]], which attach a cell to the surface on which it is crawling) to the front. It is likely that these feet are endocytosed toward the rear of the cell and brought to the cell's front by exocytosis, to be reused to form new attachments to the substrate.\n\n[[File:Adhesion-independent migration.tif|thumb|Rearward membrane flow (red arrows) and vesicle trafficking from back to front (blue arrows) drive adhesion-independent migration.<ref name=\"O'Neill\">{{cite journal |last1=O'Neill |first1=Patrick |last2=Castillo-Badillo |first2=Jean |last3=Meshik |first3=Xenia |last4=Kalyanaraman |first4=Vani |last5=Melgarejo |first5=Krystal |last6=Gautam |first6=N |title=Membrane flow drives an adhesion-independent amoeboid cell migration mode |journal=Developmental Cell |date=2018 |volume=46 |issue=1 |pages=9-22 |doi=10.1016/j.devcel.2018.05.029}}</ref>]]\n\n===Mechanistic basis of amoeboid migration===\n\nAdhesive crawling is not the only migration mode exhibited by eukaryotic cells. Importantly, metastatic cancer cells and immune cells like [[macrophage|macrophages]] and [[neutrophil|neutrophils]] have been found to be capable of adhesion-independent migration. The mechanistic basis of this migration mode is less understood than either eukaryotic cell crawling or flagella-based swimming by microorganisms. The physicist [[Edward Mills Purcell|E. M. Purcell]] theorized that under conditions of low [[Reynolds number]] fluid dynamics, which apply at the cellular scale, rearward surface flow could provide a mechanism for microscopic objects to swim forward.<ref>{{cite journal |last1=Purcell |first1=E. M. |title=Life at Low Reynolds Number |journal=American Journal of Physics |date=1977 |volume=45 |issue=3 |doi=10.1119/1.10903}}</ref> After some decades, experimental support for this model was provided using [[optogenetics]]. It was shown that cells migrating in an amoeboid fashion without adhesions exhibit plasma membrane flow towards the cell rear that can propel cells by exerting tangential forces on the surrounding fluid.<ref name=\"O'Neill\">{{cite journal |last1=O'Neill |first1=Patrick |last2=Castillo-Badillo |first2=Jean |last3=Meshik |first3=Xenia |last4=Kalyanaraman |first4=Vani |last5=Melgarejo |first5=Krystal |last6=Gautam |first6=N |title=Membrane flow drives an adhesion-independent amoeboid cell migration mode |journal=Developmental Cell |date=2018 |volume=46 |issue=1 |pages=9-22 |doi=10.1016/j.devcel.2018.05.029}}</ref><ref name=\"Collins\">{{cite journal |last1=Bell |first1=George R. R. |last2=Collins |first2=Sean R. |title=\"Rho\"ing a cellular boat with rearward membrane flow |journal=Developmental Cell |date=2018 |volume=46 |issue=1 |pages=1-3 |doi=10.1016/j.devcel.2018.06.008}}</ref> Polarized trafficking of membrane-containing vesicles from the rear to the front of the cell helps maintain cell size.<ref name=\"O'Neill\" /> Rearward membrane flow was also observed in ''Dictyostelium discoideum'' cells.<ref>{{cite journal |last1=Tanaka |first1=Masahito |last2=Kikuchi |first2=Takeomi |last3=Uno |first3=Hiroyuki |last4=Okita |first4=Keisuke |last5=Kitanishi-Yumura |first5=Toshiko |last6=Yumura |first6=Shigehiko |title=Turnover and flow of the cell membrane for cell migration |journal=Scientific Reports |date=2017 |volume=7 |doi=10.1038/s41598-017-13438-5}}</ref> Interestingly, the migration of supracellular clusters has also been found to be supported by a similar mechanism of rearward surface flow.<ref>{{cite journal |last1=Shellard |first1=Adam |last2=Szabo |first2=Andras |last3=Trepat |first3=Xavier |last4=Mayor |first4=Roberto |title=Supracellular contraction at the rear of neural crest cell groups drives collective chemotaxis |journal=Science |date=2018 |volume=362 |issue=6412 |pages=339-343 |doi=10.1126/science.aau3301}}</ref>\n\n[[File:Collective_Mechanism_of_Cell_Motion.jpg|thumb|upright=1.5|Schematic representation of the collective biomechanical and molecular mechanism of cell motion <ref name=\"coskun2011\"/>]]\n\n===Collective biomechanical and molecular mechanism of cell motion===\n\nBased on some mathematical models, recent studies hypothesize a novel biological model for collective biomechanical and molecular mechanism of cell motion.<ref name=coskun2011>{{cite journal|last1=Coskun|first1=Hasan|last2=Coskun|first2=Huseyin.|title=Cell physician: reading cell motion. A mathematical diagnostic technique through analysis of single cell motion|journal=Bull Math Biol|date=March 2011|volume=73|issue=3|pages=658–82|doi=10.1007/s11538-010-9580-x|pmid=20878250}}</ref> It is proposed that microdomains weave the texture of cytoskeleton and their interactions mark the location for formation of new adhesion sites. According to this model, microdomain signaling dynamics organizes cytoskeleton and its interaction with substratum. As microdomains trigger and maintain active polymerization of actin filaments, their propagation and zigzagging motion on the membrane generate a highly interlinked network of curved or linear filaments oriented at a wide spectrum of angles to the cell boundary. It is also proposed that microdomain interaction marks the formation of new focal adhesion sites at the cell periphery. Myosin interaction with the actin network then generate membrane retraction/ruffling, retrograde flow, and contractile forces for forward motion. Finally, continuous application of stress on the old focal adhesion sites could result in the calcium-induced calpain activation, and consequently the detachment of focal adhesions which completes the cycle.\n\n==Polarity in migrating cells==\nMigrating cells have a [[cell polarity| polarity]]—a front and a back. Without it, they would move in all directions at once, i.e. spread. How this polarity is formulated at a molecular level inside a cell is unknown. In a cell that is meandering in a random way, the front can easily give way to become passive as some other region, or regions, of the cell form(s) a new front. In chemotaxing cells, the stability of the front appears enhanced as the cell advances toward a higher concentration of the stimulating chemical. This polarity is reflected at a molecular level by a restriction of certain molecules to particular regions of the inner [[cell membrane|cell surface]]. Thus, the phospholipid [[Phosphatidylinositol (3,4,5)-trisphosphate|PIP3]] and activated Rac and [[CDC42]] are found at the front of the cell, whereas [[RHOA|Rho GTPase]] and [[PTEN (gene)|PTEN]] are found toward the rear.<ref name=Parent1999>{{cite journal |last1=Parent |first1=C. A. |last2=Devreotes |first2=PN |title=A Cell's Sense of Direction |journal=Science |volume=284 |issue=5415 |pages=765–70 |year=1999 |pmid=10221901 |doi=10.1126/science.284.5415.765}}</ref><ref name=Ridley2003>{{cite journal |last1=Ridley |first1=A. J. |last2=Schwartz |first2=MA |last3=Burridge |first3=K |last4=Firtel |first4=RA |last5=Ginsberg |first5=MH |last6=Borisy |first6=G |last7=Parsons |first7=JT |last8=Horwitz |first8=AR |title=Cell Migration: Integrating Signals from Front to Back |journal=Science |volume=302 |issue=5651 |pages=1704–9 |year=2003 |pmid=14657486 |doi=10.1126/science.1092053}}</ref>\n\nIt is believed that  filamentous actins and [[microtubule]]s  are important for establishing and maintaining a cell’s polarity.{{cn|date=April 2019}} Drugs that destroy actin filaments have multiple and complex effects, reflecting the wide role that these filaments play in many cell processes. It may be that, as part of the locomotory process, membrane [[vesicle (biology)|vesicles]] are transported along these filaments  to the cell’s front. In chemotaxing cells, the increased persistence of migration toward the target may result from an increased stability of the arrangement of the filamentous structures inside the cell and determine its polarity. In turn, these filamentous structures may be arranged inside the cell according to how molecules like PIP3 and PTEN are arranged on the inner cell membrane. And where these are located appears in turn to be determined by the chemoattractant signals as these impinge on specific [[Receptor (biochemistry)|receptor]]s on the cell’s outer surface.\n\nAlthough microtubules have been known to influence cell migration for many years, the mechanism by which they do so has remained controversial. On a planar surface, microtubules are not needed for the movement, but they are required to provide directionality to cell movement and efficient protrusion of the leading edge.<ref name=\"ReferenceA\"/><ref name=\"Meyer2012\">{{cite journal |last1=Meyer |first1=A.S. |last2=Hughes-Alford |first2=S.K. |last3=Kay |first3=J.E. |last4=Castillo |first4=A. |last5=Wells |first5=A. |last6=Gertler | first6=F.B. |last7=Lauffenburger |first7=D.A. |title=2D protrusion but not motility predicts growth factor–induced cancer cell migration in 3D collagen |journal=J. Cell Biol. |volume=197|issue=6 |pages=721–729 |year=2012 |pmid=22665521 |doi=10.1083/jcb.201201003 |pmc=3373410}}</ref> When present, microtubules retard cell movement when their dynamics are suppressed by drug treatment or by tubulin mutations.<ref name=\"ReferenceA\"/>\n\n== Inverse problems in the context of cell motility ==\n\nAn area of research called [[inverse problem]]s in cell motility has been established. \n<ref>{{cite book|last1=Coskun|first1=Huseyin.|title=Mathematical Models for Ameboid Cell Motility and Model Based Inverse Problems|date=2006|publisher=ProQuest|url=http://search.proquest.com}}</ref><ref>{{cite journal|last1=Coskun|first1=Huseyin|last2=Li|first2=Yi|last3=Mackey|first3=Mackey A.|title=Ameboid cell motility: a model and inverse problem, with an application to live cell imaging data|journal=J Theor Biol|date=Jan 2007|volume=244|issue=2|pages=169–79|doi=10.1016/j.jtbi.2006.07.025|pmid=16997326}}</ref><ref name=\"coskun2011\"/>\nThis approach is based on the idea that behavioral or shape changes of a cell bear information about the underlying mechanisms that generate these changes. Reading cell motion, namely, understanding the underlying biophysical and mechanochemical processes, is of paramount importance.\n<ref>{{cite web|title=Profiling Cells with Math|url=http://www.maa.org/news/math-news/profiling-cells-with-math|publisher=Mathematical Association of America}}</ref>\n<ref>{{cite web|title=Mathematicians use cell 'profiling' to detect abnormalities – including cancer|url=https://www.sciencedaily.com/releases/2011/01/110125141827.htm|publisher=ScienceDaily}}</ref>\nThe mathematical models developed in these works determine some physical features and material properties of the cells locally through analysis of live cell image sequences and uses this information to make further inferences about the molecular structures, dynamics, and processes within the cells, such as the actin network, microdomains, chemotaxis, adhesion, and retrograde flow.\n\n==See also==\n{{Portal|Molecular and cellular biology}}\n* [[Cap formation]]\n* [[Chemotaxis]]\n* [[Collective cell migration]]\n* [[Durotaxis]]\n* [[Endocytic cycle]]\n* [[Mouse models of breast cancer metastasis]]\n* [[Neurophilic]]\n* [[Protein dynamics]]\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==External links==\n* [http://www.cellmigration.org/index.shtml Cell Migration Gateway] The Cell Migration Gateway is a comprehensive and regularly updated resource on cell migration\n* [http://cellix.imba.oeaw.ac.at The Cytoskeleton and Cell Migration] A tour of images and videos by the J. V. Small lab in Salzburg and Vienna\n* [http://www.phiab.se/publications/video-gallery Time-lapse microscopy videos showing proliferating and migrating cells]\n* [https://web.archive.org/web/20141224105550/http://www.phiab.se/applications/cell-tracking Cell migration tracking] by Phase Holographic Imaging AB\n\n{{swarming}}\n\n{{DEFAULTSORT:Cell Migration}}\n[[Category:Cellular processes]]\n[[Category:Cell movement]]\n[[Category:Articles containing video clips]]"
    },
    {
      "title": "Centriole",
      "url": "https://en.wikipedia.org/wiki/Centriole",
      "text": "{{Cell biology|centrosome=yes|background color=blue}}\n\nIn [[cell biology]] a '''centriole''' is a cylindrical [[organelle]] composed mainly of a protein called [[tubulin]].<ref name=edde>{{Cite journal|doi=10.1126/science.1967194|pmid=1967194|year=1990|last1=Eddé|first1=B|last2=Rossier|first2=J|last3=Le Caer|first3=JP|last4=Desbruyères|first4=E|last5=Gros|first5=F|last6=Denoulet|first6=P|title=Posttranslational glutamylation of alpha-tubulin|volume=247|issue=4938|pages=83–5|journal=Science|bibcode=1990Sci...247...83E}}</ref> Centrioles are found in most [[eukaryotic]] [[Cell (biology)|cell]]s. A bound pair of centrioles, surrounded by a shapeless mass of dense material, called the [[pericentriolar material]] (PCM), makes up a structure called a [[centrosome]].<ref name=edde/>\n\nCentrioles are present in the cells of most eukaryotes, for example those of [[animal]]s. However, they are absent from conifers ([[pinophyta]]), flowering plants ([[Flowering plant|angiosperms]]) and most [[fungi]], and are only present in the male gametes of [[charophytes]], [[bryophyte]]s, seedless [[vascular plant]]s, [[cycad]]s, and [[ginkgo]].<ref>{{Cite journal|pmid=15928206|year=2005|last1=Quarmby|first1=LM|last2=Parker|first2=JD|title=Cilia and the cell cycle?|volume=169|issue=5|pages=707–10|doi=10.1083/jcb.200503053|pmc=2171619|journal=The Journal of Cell Biology}}</ref><ref>{{Cite journal|last1=Silflow|first1=CD|last2=Lefebvre|first2=PA|title=Assembly and motility of eukaryotic cilia and flagella. Lessons from Chlamydomonas reinhardtii|journal=Plant Physiology|year=2001|volume=127|issue=4|pages=1500–1507|doi=10.1104/pp.010807|pmid=11743094|pmc=1540183}}</ref>\n\nCentrioles are typically made up of nine sets of [[microtubule|short microtubule]] triplets, arranged in a cylinder. Deviations from this structure include [[crabs]] and ''[[Drosophila melanogaster]]'' embryos, with nine doublets, and ''[[Caenorhabditis elegans]]'' [[sperm cells]] and early embryos, with nine singlets.<ref>{{Cite journal|pmid=15075224|year=2004|last1=Delattre|first1=M|last2=Gönczy|first2=P|title=The arithmetic of centrosome biogenesis|volume=117|issue=Pt 9|pages=1619–30|doi=10.1242/jcs.01128|journal=Journal of Cell Science|url=https://infoscience.epfl.ch/record/182433/files/1619.full.pdf}}</ref><ref>{{Cite journal |pmid=15665853|year=2005 |last1=Leidel|first1=S. |last2=Delattre |first2=M. |last3=Cerutti |first3=L. |last4=Baumer |first4=K. |last5=Gönczy |first5=P|title=SAS-6 defines a protein family required for centrosome duplication in ''C. elegans'' and in human cells |volume=7 |issue=2 |pages=115–25 |doi=10.1038/ncb1220 |journal=Nature Cell Biology}}</ref>\n[[File:Centriole-schema.SVG|Cross-section of a centriole showing its [[microtubule]] triplets|200px|right]]\n\nThe main function of centrioles is to produce [[Cilium|cilia]] during [[interphase]] and the [[Aster (cell biology)|aster]] and the [[spindle apparatus|spindle]] during cell division.\n\n==History==\n[[Edouard van Beneden]] made the first observation of centrosomes in 1883.<ref>{{Cite journal | year = 2002 | doi = 10.1007/s00109-002-0374-y| pmid =  12226736 | issue =  9 | pages = 545–548| volume = 80| title = JMM - Past and Present| last1 = Wunderlich| journal = Journal of Molecular Medicine| first1 = V.}}</ref> In 1895, [[Theodor Boveri]] named the organelle a \"centrosome\".<ref>Boveri, T. ''Ueber das Verhalten der Centrosomen bei der Befruchtung des Seeigel-Eies nebst allgemeinen Bemerkungen über Centrosomen und Verwandtes''. Verh. d. Phys.-Med. Ges. zu Würzburg, N. F., Bd. XXIX, 1895. [https://archive.org/details/bub_gb_UYcPAQAAMAAJ link].</ref><ref>Boveri, T. (1901). ''Zellen-Studien: Uber die Natur der Centrosomen. IV''. Fischer, Jena. [https://archive.org/details/zellenstudienbe00bovegoog link].</ref> The pattern of centriole duplication was first worked out independently by [[Etienne de Harven]] and [[Joseph G. Gall]] c. 1950.<ref>{{cite book | last = Wolfe | first = Stephen L. | authorlink = Stephen L. Wolfe | title = Biology: the foundations | publisher = Wadsworth | year = 1977|edition=First | url =https://books.google.com/books?id=Fq8TAQAAIAAJ| isbn = 9780534004903 }}</ref><ref>{{Cite book|volume=106 |year=1987 |pages=227–293|doi=10.1016/S0074-7696(08)61714-3 |title=The Centrosome and Its Role in the Organization of Microtubules |first1=I. A. |last1=Vorobjev |first2=E. S. |last2=Nadezhdina |series=International Review of Cytology|isbn=978-0-12-364506-7 |pmid=3294718}}. See also de Harven's own recollections of this work: {{Cite journal|title=Early observations of centrioles and mitotic spindle fibers by transmission electron microscopy|first=Etienne|last=de Harven |journal=Biol Cell |year=1994 |volume=80 |pages=107–109 |doi=10.1111/j.1768-322X.1994.tb00916.x|pmid=8087058|issue=2–3|df=dmy-all}}</ref>\n\n==Role in cell division==\n[[File:Centriole-en.svg|thumb|left|320px|A mother and daughter centriole, attached [[orthogonally]]]]\nCentrioles are involved in the organization of the [[spindle apparatus|mitotic spindle]] and in the completion of [[cytokinesis]].<ref name=\"Salisbury\">{{Cite journal|doi=10.1016/S0960-9822(02)01019-9|pmid=12176356|year=2002|last1=Salisbury|first1=JL|last2=Suino|first2=KM|last3=Busby|first3=R|last4=Springett|first4=M|title=Centrin-2 is required for centriole duplication in mammalian cells|volume=12|issue=15|pages=1287–92|journal=Current Biology}}</ref> Centrioles were previously thought to be required for the formation of a mitotic spindle in animal cells. However, more recent experiments have demonstrated that cells whose centrioles have been removed via [[laser]] ablation can still progress through the G<sub>1</sub> stage of [[interphase]] before centrioles can be synthesized later in a de novo fashion.<ref name=\"Terra\">{{Cite journal|pmid=15738265|year=2005|last1=La Terra|first1=S|last2=English|first2=CN|last3=Hergert|first3=P|last4=McEwen|first4=BF|last5=Sluder|first5=G|last6=Khodjakov|first6=A|title=The de novo centriole assembly pathway in HeLa cells: cell cycle progression and centriole assembly/maturation|volume=168|issue=5|pages=713–22|doi=10.1083/jcb.200411126|pmc=2171814|journal=The Journal of Cell Biology}}</ref> Additionally, mutant flies lacking centrioles develop normally, although the adult flies' cells lack [[flagella]] and [[cilia]] and as a result, they die shortly after birth.<ref name=\"Basto\">{{Cite journal|pmid=16814722|year=2006|last1=Basto|first1=R|last2=Lau|first2=J|last3=Vinogradova|first3=T|last4=Gardiol|first4=A|last5=Woods|first5=CG|last6=Khodjakov|first6=A|last7=Raff|first7=JW|title=Flies without centrioles|volume=125|issue=7|pages=1375–86|doi=10.1016/j.cell.2006.05.025|journal=Cell}}</ref>\nThe centrioles can self replicate during cell division.\n\n==Cellular organization==\nCentrioles are a very important part of [[centrosomes]], which are involved in organizing [[microtubules]] in the [[cytoplasm]].<ref name=\"PLOS\">{{Cite journal|pmid=17518519|year=2007|last1=Feldman|first1=JL|last2=Geimer|first2=S|last3=Marshall|first3=WF|title=The mother centriole plays an instructive role in defining cell geometry|volume=5|issue=6|pages=e149|doi=10.1371/journal.pbio.0050149|pmc=1872036|journal=PLoS Biology}}</ref><ref>{{Cite journal|doi=10.1016/S0955-0674(02)00017-0|pmid=12517710|year=2003|last1=Beisson|first1=J|last2=Wright|first2=M|title=Basal body/centriole assembly and continuity|volume=15|issue=1|pages=96–104|journal=Current Opinion in Cell Biology}}</ref> The position of the centriole determines the position of the nucleus and plays a crucial role in the spatial arrangement of the cell.\n\n[[File:Blausen 0214 Centrioles.png|thumb|3D rendering of centrioles]]\n\n==Fertility==\n[[Sperm]] centrioles are important for 2 functions:<ref>Avidor-Reiss, T., Khire, A., Fishman, E. L., & Jo, K. H. (2015). Atypical centrioles during sexual reproduction. Frontiers in cell and developmental biology, 3, 21.\nChicago\t</ref> (1) to form the sperm [[flagellum]] and sperm movement and (2) for the development of the embryo after fertilization.\n\n==Ciliogenesis==\nIn [[flagellate]]s and [[ciliate]]s, the position of the [[flagellum]] or [[cilium]]  is determined by the mother centriole, which becomes the [[basal body]].  An inability of cells to use centrioles to make functional flagella and cilia has been linked to a number of genetic and developmental diseases. In particular, the inability of centrioles to properly migrate prior to ciliary assembly has recently been linked to  [[Meckel-Gruber syndrome]].<ref name=\"meckel\">{{Cite journal|doi=10.1242/dmm.006262|pmc=3008963|pmid=    21045211|year=2011|author1=Cui, Cheng |author2=Chatterjee, Bishwanath |author3=Francis, Deanne |author4=Yu, Qing |author5=SanAgustin, Jovenal T. |author6=Francis, Richard |author7=Tansey, Terry |author8=Henry, Charisse |author9=Wang, Baolin |author10=Lemley, Bethan |author11=Pazour, Gregory J. |author12=Lo, Cecilia W. |title=Disruption of Mks1 localization to the mother centriole causes cilia defects and developmental malformations in Meckel-Gruber syndrome|volume=4|issue=1|pages=43–56|journal=Dis. Models Mech.}}</ref>\n\n==Animal development==\n[[File:Spindle centriole - embryonic brain mouse - TEM.jpg|thumb|Electron micrograph of a centriole from a mouse embryo.]]\nProper orientation of cilia via centriole positioning toward the posterior of embryonic node cells is critical for establishing left–right asymmetry during mammalian development.<ref>{{Cite journal|last=Babu|first=Deepak|last2=Roy|first2=Sudipto|date=2013-05-01|title=Left–right asymmetry: cilia stir up new surprises in the node|url=http://rsob.royalsocietypublishing.org/content/3/5/130052|journal=Open Biology|language=en|volume=3|issue=5|pages=130052|doi=10.1098/rsob.130052|issn=2046-2441|pmc=3866868|pmid=23720541}}</ref>\n\n==Centriole duplication==\nBefore [[DNA replication]], cells contain two centrioles. The older of the two centrioles is termed the ''mother centriole'', the other the ''daughter''. During the [[Cell cycle|cell division cycle]], a new centriole grows at the proximal end of both mother and daughter centrioles. After duplication, the two centriole pairs (freshly assembled centriole is now a daughter centriole in each pair) will remain attached to each other [[orthogonal]]ly until [[mitosis]]. At that point the mother and daughter centrioles separate dependently on an [[enzyme]] called [[separase]].<ref>{{Cite journal|pmid=16862117|year=2006|last1=Tsou|first1=MF|last2=Stearns|first2=T|title=Mechanism limiting centrosome duplication to once per cell cycle|volume=442|issue=7105|pages=947–51|doi=10.1038/nature04985|journal=Nature|bibcode = 2006Natur.442..947T }}</ref>\n\nThe two centrioles in the centrosome are tied to one another. The mother centriole has radiating appendages at the [[Anatomical terms of location#Proximal and distal|distal]] end of its long axis and is attached to its daughter at the [[Anatomical terms of location#Proximal and distal|proximal]] end. Each daughter cell formed after cell division will inherit one of these pairs. Centrioles start duplicating when DNA replicates.<ref name=\"Salisbury\" />\n\n==Origin==\nThe last common ancestor of all [[eukaryote]]s was a [[cilia]]ted cell with centrioles. Some lineages of eukaryotes, such as [[land plants]], do not have centrioles except in their motile male gametes. Centrioles are completely absent from all cells of [[Pinophyta|conifers]] and [[angiosperm|flowering plants]], which do not have ciliate or flagellate gametes.<ref>{{cite journal | last1 = Marshall | first1 = W.F. | year = 2009 | title = Centriole Evolution | url = | journal = Current Opinion in Cell Biology | volume = 21 | issue = 1| pages = 14–19 | doi = 10.1016/j.ceb.2009.01.008 | pmid=19196504 | pmc=2835302}}</ref>\nIt is unclear if the last common ancestor had one<ref name=\"Bornens07\">{{Cite book | doi = 10.1007/978-0-387-74021-8_10| last2 =  Azimzadeh| pmid =  17977464 | isbn = 978-0-387-74020-1| year = 2007| pages = 119–129 | series = Advances in Experimental Medicine and Biology| last1 = Bornens | first2 = J. | chapter = Origin and Evolution of the Centrosome| title = Eukaryotic Membranes and Cytoskeleton| volume = 607| first1 = M.}}</ref> or two cilia.<ref>{{Cite journal | doi = 10.1093/gbe/evp011 | last1 = Rogozin | first1 = I. B. | last2 = Basu | first2 = M. K. | last3 = Csürös | first3 = M. | last4 = Koonin | first4 = E. V. | title = Analysis of Rare Genomic Changes Does Not Support the Unikont-Bikont Phylogeny and Suggests Cyanobacterial Symbiosis as the Point of Primary Radiation of Eukaryotes | journal = Genome Biology and Evolution | volume = 1 | pages = 99–113 | year = 2009 | pmid = 20333181 | pmc = 2817406}}</ref> Important genes required for centriole growth, like [[centrins]], are only found in eukaryotes and not in [[bacteria]] or [[archaeans]].<ref name=\"Bornens07\"/>\n\n==Etymology and pronunciation==\nThe word ''centriole'' ({{IPAc-en|ˈ|s|ɛ|n|t|r|i|oʊ|l}}) uses [[classical compound|combining forms]] of ''centri-'' and ''[[wikt:-ole#Etymology 2|-ole]]'', yielding \"little central part\", which describes a centriole's typical location near the center of the cell.\n\n==Atypical centrioles==\nTypical centrioles are made of 9 triplets of [[microtubules]] organized with radial symmetry.<ref>{{cite journal |doi=10.1016/j.ceb.2012.10.016 |pmid=23199753 |pmc=3578074 |title=Building a centriole |journal=Current Opinion in Cell Biology |volume=25 |issue=1 |pages=72–7 |year=2013 |last1=Avidor-Reiss |first1=Tomer |last2=Gopalakrishnan |first2=Jayachandran }}</ref> Centrioles can vary the number of microtubules and can be made of 9 doublets of microtubules (as in ''[[Drosophila melanogaster]]'') or 9 singlets of microtubules as in [[Caenorhabditis elegans|''C. elegans'']]. Atypical centrioles are centrioles that do not have microtubules, such as the [[Proximal Centriole-Like]] found in ''D. melanogaster'' sperm,<ref>{{cite journal |doi=10.1534/genetics.109.101709 |pmid=19293139 |pmc=2674812 |title=A Proximal Centriole-Like Structure is Present in Drosophila Spermatids and Can Serve as a Model to Study Centriole Duplication |journal=Genetics |volume=182 |issue=1 |pages=133–44 |year=2009 |last1=Blachon |first1=S |last2=Cai |first2=X |last3=Roberts |first3=K. A |last4=Yang |first4=K |last5=Polyanovsky |first5=A |last6=Church |first6=A |last7=Avidor-Reiss |first7=T }}</ref> or that have microtubules with no radial symmetry, such as in the distal centriole of human [[spermatozoon]].<ref>{{cite journal |doi=10.1038/s41467-018-04678-8 |pmid=29880810 |pmc=5992222 |title=A novel atypical sperm centriole is functional during human fertilization |journal=Nature Communications |volume=9 |issue=1 |pages=2210 |year=2018 |last1=Fishman |first1=Emily L |last2=Jo |first2=Kyoung |last3=Nguyen |first3=Quynh P. H |last4=Kong |first4=Dong |last5=Royfman |first5=Rachel |last6=Cekic |first6=Anthony R |last7=Khanal |first7=Sushil |last8=Miller |first8=Ann L |last9=Simerly |first9=Calvin |last10=Schatten |first10=Gerald |last11=Loncarek |first11=Jadranka |last12=Mennella |first12=Vito |last13=Avidor-Reiss |first13=Tomer }}</ref>\n\n==References==\n{{Reflist}}\n\n{{Centrosome}}\n{{organelles}}\n\n{{Authority control}}\n{{Use dmy dates|date=April 2017}}\n\n[[Category:Centrosome]]\n[[Category:Protein complexes]]\n[[Category:Cell movement]]"
    },
    {
      "title": "Chemorepulsion",
      "url": "https://en.wikipedia.org/wiki/Chemorepulsion",
      "text": "{{short description|Directional movement of a cell away from a substance}}\n'''Chemorepulsion''' is the directional movement of a cell away from a substance. Of the two directional varieties of chemotaxis, [[chemoattraction]] has been studied to a much greater extent. Only recently have the key components of the chemorepulsive pathway been elucidated.<ref name=four>Vianello, F., E. Righi, et al. (2010). Methods for Quantitation of Leukocyte Chemotaxis and Fugetaxis. T-Cell Trafficking. F. M. Marelli-Berg and S. Nourshargh, Humana Press. 616: 115-124.</ref> The exact mechanism is still being investigated, and its constituents are currently being explored as likely candidates for immunotherapies.<ref name = three>IMMUNOLOGY - CHAPTER ONE > INNATE (NON-SPECIFIC) IMMUNITY Gene Mayer, Ph.D. Immunology Section of Microbiology and Immunology On-line. University of South Carolina.</ref>\n\n{| align=right border=1 width=350 cellpadding=2 cellspacing=0 class=toccolours style=\"margin-left:0.5em;\"\n! align=center style=\"background:#CCBBEE\" |Cell Migration Glossary\n|-\n| • '''Chemotaxis''' Cellular response to an environmental substance with a directional movement.\n|-\n| • '''Chemokinesis''' Cellular response to an environmental substance with a random, non-vectorial movement. \n|-\n| • '''Chemoattraction''' Directional cell movement ''towards'' a substance\n|-\n| • '''Chemorepulsion'''  Directional cell movement ''away'' from a substance\n|-\n| • '''Chemokines'''  Secreted cell-signaling proteins able to induce chemotaxis in nearby cells.\n|-\n| • '''Immunorepulsion'''  The active movement of ''immune cells'' away from a substance  \n|-\n|}\n\n== History and etymology ==\n[[File:immunorepulsion 2.jpg|thumb|left|Neutrophils being repelled from a chemokinetic agent]]\n\nThe mechanism of the chemorepulsion of immune cells was first acknowledged by medical researchers at the Massachusetts General Hospital in [[Boston]] in early 2002.<ref name=\"four\"/> The phenomenon was originally referred to as \"reverse [[chemotaxis]],\" and later, “fugetaxis” (derived from the Latin words ''fugere'', to flee from; and ''taxis'', movement).<ref name=\"four\"/> For a time, the words were used interchangeably before being replaced almost exclusively by “chemorepulsion.” While \"chemorepulsion\" applies to all cell types, the term \"immunorepulsion\" is gaining momentum as a more specific term that only applies to [[hematopoietic]] blood cell types that are involved in immune responses. Different cell types to which the term \"immunorepulsion\" could potentially be applied include: [[Myeloid]] lineage cells ([[monocytes]], [[macrophages]], [[neutrophils]], [[basophils]], [[eosinophils]], [[erythrocytes]], [[platelets]], [[dendritic cells]]) and [[Lymphoid]] lineage cells ([[T-cells]], [[B-cells]], [[NK-cells]]).\n\n==Role in physiological processes==\nThe chemorepulsion of immune cells was first postulated ''a priori'' based on the established migratory behavior of cells evidenced in several naturally occurring physiological processes: the development of the [[Central Nervous System]], the establishment of immune-privileged sites, and thymic emigration.\n\n===Central nervous system development===\nDuring the development of the [[Central Nervous System]], chemokinetic agents influence the localization of neuronal cells by either attracting or repelling the growing [[axon]].<ref>{{cite journal|last=Wu|first=W|title=Directional guidance of neuronal migration in the olfactory system by the protein slit|journal=Nature|year=1999|volume=400|issue=6742|pages=331–336|doi=10.1038/22477|pmid=10432110|display-authors=1|last2=Rao|first2=Yi|last3=Wu|first3=Wei|last4=Wong|first4=Kit|last5=Chen|first5=Jin-hui|last6=Jiang|first6=Zhi-Hong|last7=Dupuis|first7=Sophie|pmc=2041931}}</ref> This mechanism of context-dependent bidirectionality serves as a valuable model of chemorepulsion that can be studied ''in vivo''.<ref name=\"four\"/> Additionally, there is growing evidence that chemorepulsion is probably a key mechanism involved in regulating leukocyte motility.<ref>{{cite journal|last=Yoshie|first=O|title=Role of chemokines in trafficking of lymphocytes and dendritic cells|journal=Int. J. Hematol.|year=2000|volume=72|issue=4|pages=399–407|pmid=11197204}}</ref>  Many of the chemorepellents that affect neuronal cell migration, including [[netrins]], [[semaphorins]], slit ligands, and [[ephrins]] have recently been implicated in the motility of immune cells.<ref name=\"four\"/> For example, the Slit protein that mediates axonal chemorepulsion has also been shown to inhibit the directed migration of leuckocytes in response to chemoattractants.<ref>{{cite journal|last=Wu|first=J.Y.|title=The neuronal repellent Slit inhibits leukocyte chemotaxis induced by chemotactic factors|journal=Nature|year=2001|volume=410|pages=948–952|doi=10.1038/35073616|pmid=11309622|display-authors=1|last2=Feng|first2=Lili|last3=Park|first3=Hwan-Tae|last4=Havlioglu|first4=Necat|last5=Wen|first5=Leng|last6=Tang|first6=Hao|last7=Bacon|first7=Kevin B.|last8=Jiang|first8=Zhi-Hong|last9=Zhang|first9=Xiao-Chun|issue=6831|pmc=2072862}}</ref> Other factors might also provide chemorepulsive effects on immune cells, and these inhibitory effects might be regulated by the tissue microenvironment.\n\n===Immune-privileged sites===\nCertain body tissues are able to tolerate antigens without an inflammatory immune response.<ref>{{cite journal|last=Streilein|first=JW|title=Immune privilege as the result of local tissue barriers and immunosuppressive microenvironments|journal=Current Opinion in Immunology|year=1993|volume=5|issue=3|pages=428–432|doi=10.1016/0952-7915(93)90064-Y|pmid=8347303}}</ref> [[Immune privilege]] is thought to be an evolutionary adaptation to protect the most vital sensory organs and reproductive structures that would be otherwise severely impaired during an inflammatory response.<ref>{{cite journal|last=Streilein|first=JW|title=Tissue barriers, immunosuppressive microenvironments, and privileged sites: the eye's point of view|journal=Reg Immunol|year=1993|volume=5|issue=5|pages=253–268|pmid=8148235}}</ref> Although these locations are often physically isolated or segregated from access by immune cells, there are some functionally significant characteristics of such environments that are unique, and could potentially be replicated to keep immune cells away from targeted areas. Known immunologically privileged sites include the:\n# [[Brain]] and [[central nervous system]]\n# [[Eyes]]\n# [[Placenta]] and [[fetus]]\n# [[Testicles]]\nCharacteristics that are particular to immune-privileged sites should be seriously considered when investigating candidates for immunorepulsion therapy. These characteristics include: \n# Low expression of Classical [[Major histocompatibility complex|MHC]] Class IA molecules.\n# Expression of immunoregulatory Nonclassical [[Major histocompatibility complex|MHC]] Class IB molecules.\n# Increased expression of surface molecules that inhibit [[complement system|complement]] activation.\n# Local production of immunosuppressive [[cytokines]], such as [[TGF-β]]\n# Presence of [[neuropeptides]].\n# Expression of [[Fas ligand]] that controls the entry of Fas-expressing [[lymphoid]] cells.\n\n=== Thymic emigration ===\n[[T-cells]] are one of the most critical constituents of the [[adaptive immune system]] due to their ability to continue developing after activation.<ref>{{cite journal|last=Metloubian|first=M.|title=Lymphocyte egress from thymus and peripheral lymphoid organs is dependent on S1P receptor 1|journal=Nature|year=2004|volume=427|pages=355–360|doi=10.1038/nature02284|display-authors=1|last2=Lo|first2=Charles G.|last3=Cinamon|first3=Guy|last4=Lesneski|first4=Matthew J.|last5=Xu|first5=Ying|last6=Brinkmann|first6=Volker|last7=Allende|first7=Maria L.|last8=Proia|first8=Richard L.|last9=Cyster|first9=Jason G.|issue=6972|pmid=14737169}}</ref> To prevent premature instigation, it is necessary for T-cells to mature in an environment completely isolated from any potentially activating factors ([[antigens]], [[cytokines]], [[steroids]], receptor [[Receptor antagonist|antagonist]]s, adhesion molecules, etc.).<ref>{{cite journal|last=Ueno|first=T.|title=Role for CCR7 ligands in the emigration of newly generated T lymphocytes from the neonatal thy mus|journal=Immunity|year=2002|volume=16|pages=205–218|display-authors=1|doi=10.1016/S1074-7613(02)00267-4|pmid=11869682|last2=Hara|first2=K|last3=Willis|first3=MS|last4=Malin|first4=MA|last5=Höpken|first5=UE|last6=Gray|first6=DH|last7=Matsushima|first7=K|last8=Lipp|first8=M|last9=Springer|first9=TA|issue=2}}</ref> As a result, T-cells are formed in the [[bone marrow]] and subsequently migrate to the [[cortex (anatomy)|cortex]] of the [[thymus]] where they can mature in an antigen-free environment. The thymus supports the differentiation of multiple distinct T cell subsets that play unique roles in the immune system. For example, [[T-helper]], T-cytotoxic, T-memory, and T-suppressor cells all develop in the thymus and must leave it to provide their functions elsewhere in the body during an immune response.<ref>{{cite journal|last=Chaffin|first=K.E.|author2=Prelmutter, R.M|title=A pertussis toxin-sensitive process controls thymocyte emigration|journal=Eur. J. Immunol.|year=1991|volume=21|pages=2565–2573|doi=10.1002/eji.1830211038|pmid=1655469|issue=10}}</ref> ''In vitro'' models of the T-[[lymphopoiesis]] system have revealed that the emigration of mature T-cells occurs as a result of immunorepulsion away from a chemokinetic agent generated from within the thymic organ via a [[G-protein coupled receptor]].<ref>{{cite journal|last=Vianello|first=F.|title=A CXCR4-dependent chemorepellent signal contributes to the emigration of mature single-positive CD4 cel.ls from the fetal thymus|journal=Journal of Immunology|year=2005|volume=175|issue=8|pages=5115–5125|display-authors=1|author2=<Please add first missing authors to populate metadata.>|pmid=16210615 | doi = 10.4049/jimmunol.175.8.5115}}</ref>\n\n== Role in pathological processes ==\n\n===Viral and bacterial immune evasion===\n[[Pathogens]] have evolved various strategies of evasion to thwart the host’s mobilization of immune cells, some of which are relevant to immunrepulsion.<ref>{{cite journal|last=Howard|first=J|title=Molecular mimicry of the inflammation modulatory proteins (IMPs) of poxviruses: evasion of the inflammatory response to preserve viral habitat|journal=Journal of Leukocyte Biology|year=1998|volume=64|issue=1|pages=68–71|display-authors=1|author2=<Please add first missing authors to populate metadata.>|pmid=9665277|doi=10.1002/jlb.64.1.68}}</ref> For example, some [[microbes]] actively seek out and infect immune-privileged tissues where the immune response is not active.<ref>{{cite journal|last=Farrell|first=HE|author2=Degli-Esposti, Davis-Poynter NJ|title=Cytomegalovirus evasion of natural killer cell responses|journal=Immunology Reviews|year=1999|volume=168|pages=187–197|doi=10.1111/j.1600-065X.1999.tb01293.x}}</ref> Others produce immunomodulatory proteins that interfere with the host’s normal [[immune system]] response.<ref>{{cite journal|last=Murphy|first=PM|title=Molecular piracy of chemokine receptors by herpesviruses|journal=Infect Agents Dis|year=1994|volume=3|issue=2|pages=137–164|pmid=7812652}}</ref> These proteins function by modulating elements of the host:\n# [[Complement system|Complement]] system and [[inflammation|inflammatory]] response<ref>{{cite journal|last=Kotwal|first=GJ|title=Poxviral mimicry of complement and chemokine system components: what's the end game?|journal=Immunol Today|year=2000|volume=21|issue=5|pages=242–248|doi=10.1016/S0167-5699(00)01606-6|pmid=10782056}}</ref>\n# [[Cytokine]] network<ref>{{cite journal|last=Moore|first=PS|title=Molecular mimicry of human cytokine and cytokine response pathway genes by KSHV|journal=Science|year=1996|volume=274|issue=5293|pages=1739–1744|display-authors=1|doi=10.1126/science.274.5293.1739|pmid=8939871|last2=Boshoff|first2=C|last3=Weiss|first3=RA|last4=Chang|first4=Y}}</ref>\n# [[Antigen]] processing and presentation pathway<ref>{{cite journal|last=Fruh|first=K|title=A comparison of viral immune escape strategies targeting the MHC class I assembly pathway|journal=Immunol Rev|year=1999|volume=168|pages=157–166|display-authors=1|doi=10.1111/j.1600-065X.1999.tb01290.x|pmid=10399072|last2=Gruhler|first2=A|last3=Krishna|first3=RM|last4=Schoenhals|first4=GJ}}</ref>\n\nHistorically, the active sites of immunomodulatory proteins have suggested relevant targets for conventional immunotherapies.<ref>{{cite journal|last=Milne|first=RS|title=RANTES binding and down-regulation by a novel human perpesvirus-6 beta chemokine receptor|journal=Journal of Immunology|year=2000|volume=164|issue=5|pages=2396–2404|display-authors=1|pmid=10679075|last2=Mattick|first2=C|last3=Nicholson|first3=L|last4=Devaraj|first4=P|last5=Alcami|first5=A|last6=Gompels|first6=UA|doi=10.4049/jimmunol.164.5.2396}}</ref> In the current paradigm, these targets also harbor potential for innovative immunorepulsion therapies.<ref name=\"four\"/>\n\n=== Cancer immune evasion ===\n[[Cancer]] cells leverage the chemorepulsion of immune cells to evade recognition and destruction by immune cells.<ref>{{cite journal|last=Nomura|first=T|title=Enhancement of anti-tumor immunity by tumor cells transfected with the secondary lymphoid tissue chemokine EBI-1-ligand chemokine and stromal cell-derived factor-1 alpha chemokine genes|journal=Int J Cancer|year=2001|volume=91|issue=5|pages=597–606|pmid=11267967|doi=10.1002/1097-0215(200002)9999:9999<::AID-IJC1107>3.0.CO;2-J|display-authors=1|last2=Hasegawa|first2=Hitoshi|last3=Kohno|first3=Masashi|last4=Sasaki|first4=Miho|last5=Fujita|first5=Shigeru}}</ref>  Without a targeted immune response, the cancer cells can proliferate and even [[metastasize]]. Studies have been conducted to investigate which chemokines are secreted by tumors that allow them to evade response so diligently.<ref>{{cite journal|last=Rempel|first=SA|title=Identification and localization of the cytokine SDF1 and its receptor, CXC chemokine receptor 4, to regions of necrosis and angiogenesis in human glioblastoma|journal=Clin Cancer Res|year=2000|volume=6|issue=1|pages=102–111|display-authors=1|pmid=10656438|last2=Dudas|first2=S|last3=Ge|first3=S|last4=Gutiérrez|first4=JA}}</ref> One study showed that high expression of SDF-1 was responsible for the [[down-regulation]] of [[MHC class I]] molecules, which significantly interferes with tumor [[antigen]] recognition.<ref>{{cite journal|last=Zou|first=W|title=Stromal-derived factor-1 in human tumors recruits and alters the function of plasmacytoid precursor dendritic cells|journal=Nat Med|year=2001|volume=7|issue=12|pages=1339–1346|pmid=11726975|doi=10.1038/nm1201-1339|display-authors=1|last2=Machelon|first2=Véronique|last3=Coulomb-l'Hermin|first3=Aurore|last4=Borvak|first4=Jozef|last5=Nome|first5=Françoise|last6=Isaeva|first6=Tatyana|last7=Wei|first7=Shuang|last8=Krzysiek|first8=Roman|last9=Durand-Gasselin|first9=Ingrid }}</ref> Further investigations of high SDF-1 activity indicate that tumors eventually establish an immune privileged site through repulsion of tumor-specific [[lymphocytes]].<ref>{{cite journal|last=Barbero|first=S|title=Stromal cell-derived factor 1-alpha stimulates human gliblastoma cell growth through the activation of both extracellular signal-regulated kinases 1/2 and Akt|journal=Cancer Res|year=2003|volume=63|issue=8|pages=1969–1974|display-authors=1|pmid=12702590|last2=Bonavia|first2=R|last3=Bajetto|first3=A|last4=Porcile|first4=C|last5=Pirani|first5=P|last6=Ravetti|first6=JL|last7=Zona|first7=GL|last8=Spaziante|first8=R|last9=Florio|first9=T}}</ref>\n\nPotentially clinically relevant cancer [[chemokines]] include: \n# [[Interleukin 8|IL-8]]: Many cancers have been found to produce and express IL-8. Binding of IL-8 to [[CXCR1]] and [[CXCR2]] receptors has been associated with [[tumor]] establishment.<ref>{{cite journal|last=Balkwill|first=F|title=Cancer and the chemokine network|journal=Nature Reviews Cancer|year=2004|volume=4|issue=7|pages=540–550|pmid=15229479|doi=10.1038/nrc1388}}</ref>\n# [[Stromal cell-derived factor-1|SDF-1]]: Other cancers express high levels of SDF-1, which stimulates tumor growth and disrupts normal immune cell trafficking.<ref>{{cite journal|last=Strieter|first=RM|title=Chemokines: not just leukocyte chemoattractants in the promotion of cancer|journal=Nat Immunol|year=2001|volume=2|issue=4|pages=285–286|pmid=11276195|doi=10.1038/86286}}</ref>\n\n==Pharmacological relevance==\n\n===Inflammation===\n\n[[File:Neutrophil2.jpg|thumb|left|Neutrophils are critical constituents of the innate immune system]]\n\n[[Inflammation]] is one of the first responses of the immune system to infection or irritation. The response is stimulated by chemical factors released by injured cells. These chemical factors induce all associated inflammatory symptoms by sensitizing pain receptors, causing vasodilation of the blood vessels at the scene, and attracting phagocytes.<ref name = one>Stvrtinová, Viera; Ján Jakubovský and Ivan Hulín (1995), \"Inflammation and Fever from Pathophysiology: Principles of Disease\",Computing Centre, Slovak Academy of Sciences: Academic Electronic Press</ref>\n\n[[Neutrophils]] are the first to the scene, triggering other parts of the immune system by releasing factors to summon other [[leukocytes]] and [[lymphocytes]]. Other innate leukocytes include [[natural killer cells]], [[mast cells]], [[eosinophils]], [[basophils]], [[macrophages]], and [[dendritic cells]]. These cells function in concert by identifying and eliminating pathogens that might cause infection.<ref name = one/>\n\nAs first responders, the innate immune cells cannot afford to be specific, and must respond to foreign substances in a generic way.<ref name = two>Alberts, Bruce; Alexander Johnson, Julian Lewis, Martin Raff, Keith Roberts, and Peter Walters (2002), \"Molecular Biology of the Cell; Fourth Edition. New York and London: Garland Science\", \"{{ISBN|0-8153-3218-1}}\".</ref> Neutrophils, for example, contain toxic substances in their granules that kill or hinder the expansion of pathogens. The cells attack pathogens by releasing strong oxidizing agents including [[hydrogen peroxide]], free oxygen radicals, and [[hypochlorite]].<ref name = one/> Although the attack is effective against bacteria and fungi, the response can inadvertently inflict severe damage to the surrounding host tissue.  The misregulation of innate immune cells plays a key role in promulgating inflammatory conditions.\n\nChemorepulsion is currently being explored as a practicable therapy for the prevention or resolution of unwanted inflammatory responses. A chemorepellent functions by conveying chemical signals to [[immune cells]] that instruct them to leave or stay away from a targeted area or tissue in order to restore the tissue to a normal state.\n\n===Graft rejections===\nThe objective of using chemorepulsion therapy in [[Organ transplant|transplant]]ation medicine is to procure sustainable, site-specific unresponsiveness for the prevention of [[graft rejection]].<ref>{{cite journal|last=Head|first=JR|author2=Billingham RE|title=Immunologically privileged sites in transplantation immunology and oncology|journal=Perspect Biol Med|year=1985|volume=29|issue=1|pages=115–131|pmid=3906552|doi=10.1353/pbm.1985.0038}}</ref> Current therapies achieve rejection control by indiscriminately suppressing the immune response. In this approach, any benefits achieved by [[immunosuppression]] are overcome by increasing the patient's risk of deadly, [[opportunistic infections]]. If attainable, constitutive expression of chemorepellents by the donor tissue would create an inducible immune-privileged site for the [[allograft]], and would be an effective alternative treatment for graft rejection prevention.<ref>{{cite journal|last=Head|first=JR|author2=Billingham RE|title=Immune privilege in the testis:evaluation of potential local factors for transplantation|journal=Transplantation|year=1985|volume=40|issue=3|pages=269–275|doi=10.1097/00007890-198509000-00010|pmid=3898493}}</ref>\n\n==Mechanism==\n[[File:Chemotaxis Mechanism.jpg|thumb|right| Along the PIP3 gradient, the signaling pathway is highly conserved between ''D.discoideum'' and human neutrophils]]\n\nChemorepulsion is enabled by the same gradient-sensing capability that governs [[chemotaxis]]. The gradient signal of the chemokinetic agent is received through specific receptors on the cell surface and is transduced through intracellular machinery to generate the directional response. The cell moves up a [[gradient]] of a chemoattractant or down a gradient of a chemorepellent. In addition to [[axon]] growth cones, the model organism ''Dictyostelium discoideum'' has been instrumental in determining the mechanisms that mediate chemorepulsion and immunorepulsion.<ref>{{cite journal|last=Manahan|first=C.L.|title=Chemoattractant signaling in Dictyostelium discoideum|journal=Annu. Rev. Cell Dev. Biol.|year=2004|volume=20|pages=223–253|display-authors=1|doi=10.1146/annurev.cellbio.20.011303.132633|pmid=15473840|last2=Iglesias|first2=PA|last3=Long|first3=Y|last4=Devreotes|first4=PN}}</ref> The mechanisms of gradient-sensing and cell [[Dielectric polarization|polarization]] in ''D. discoideum'' are remarkably conserved in human neutrophils.<ref>{{cite journal|last=Willard|first=SS.|title=Signaling pathways mediating chemotaxis in the social amoeaba, Dictyostelium discoideum|journal=Eur. J. Cell Biol.|year=2006|volume=85|pages=897–904|doi=10.1016/j.ejcb.2006.06.003|pmid=16962888|last2=Devreotes|first2=PN|issue=9–10}}</ref>\n\n===Bidirectional decisions===\n[[File:Il88.jpg|thumb|left|Directional Decision Making Mechanism in a Human Neutrophil]]\n\n[[Leukocytes]] can exhibit active chemorepulsion away from a factor that is normally considered to stimulate chemoattraction depending on the context.<ref>{{cite journal|last=Devreotes|first=P.|author2=Janetopoulos, C.|title=Eukaryotic chemotaxis: distinctions between directional sensing and polarization|journal=J. Biol. Chem.|year=2003|volume=278|pages=20445–20448|doi=10.1074/jbc.R300010200|pmid=12672811|issue=23}}</ref> For example, [[lymphocytes]] can migrate away from a high concentration of the [[chemokine]] [[Stromal cell-derived factor-1|SDF-1]] rather than be attracted by lower concentrations of the same factor. Similar results have been reported for human [[neutrophils]] to the chemokine [[Interleukin 8|IL-8]].<ref>{{cite journal|last=Tharp|first=W.G.|title=Neutrophil chemorepulsion in defined interleukin-8 gradients in vitro and in vivo|journal=J. Leukoc. Biol.|year=2006|volume=79|pages=539–554|display-authors=1|pmid=16365152|last2=Yadav|first2=R|last3=Irimia|first3=D|last4=Upadhyaya|first4=A|last5=Samadani|first5=A|last6=Hurtado|first6=O|last7=Liu|first7=SY|last8=Munisamy|first8=S|last9=Brainard|first9=DM|issue=3|doi=10.1189/jlb.0905516}}</ref> \n: • The directional decision to move towards or away from a chemokine appears to be determined by:\n: • Differential [[receptor (biochemistry)|receptor]] occupancy\n: • Intracellular [[kinase]] activation\n: • Cyclic [[nucleotide]] concentrations\n\n===Signaling pathways===\n{| align=left border=1 width=350 cellpadding=2 cellspacing=0 class=toccolours style=\"margin-right:2.0em;\"\n! align=center style=\"background:#CCBBEE\" |Abbreviations Legend\n|-\n| • '''PI3K''' Phosphoinositide 3-kinase\n|-\n| • '''PLC''' Phospholipase C\n|-\n| • '''cAMP''' Cyclic adenosine monophosphate, a chemoattractant\n|-\n| • '''8CPT-cAMP'''  8-para-chlorphenylthio, a chemorepellent \n|-\n| • '''IP-3'''  Inositol trisphosphate \n|-\n| • '''Pt dIns(3,4,5)P<sub>3</sub>''' Phosphatidylinositol (3,4,5)-triphosphate \n|-\n| • '''SDF-1''' Stromal cell-derived factor 1\n|-\n|}\n\nIn both ''D. discoideum'' and human [[neutrophils]], there is a reversal of polarity that occurs when converting from a chemoattraction to a chemorepulsion response.<ref>{{cite book|last=Bacon|first=K.B.|title=Analysis of signal transduction following lymphocyte activation by chemokines|journal=Methods Enzymol|year=1997|volume=288|pages=340–361|doi=10.1016/S0076-6879(97)88023-8|series=Methods in Enzymology|isbn=978-0-12-182189-0}}</ref> Evidenced chemotaxis models have been observed using [[Cyclic adenosine monophosphate|cAMP]] analogs.<ref>{{cite journal|last=Dustin|first=M.L.|author2=Chakrabortk, A.K.|title=Tug of war at the exit door|journal=Immunity|year=2008|pages=15–17 | pmid = 18199414 | pmc = 2719829 | doi = 10.1016/j.immuni.2008.01.001 | volume=28 | issue = 1 }}</ref> During cAMP-mediated chemoattraction, the chemoattractant cAMP acti vates [[PI3K]] at the [[leading edge]] along with the localized activation of the small [[GTPases]] [[Rac (GTPase)|Rac]] and [[Cdc42]].<ref>{{cite journal|last=Heit|first=B.|title=An intracellular signaling hierarchy determines direction of migration in opposing chemotactic gradients|journal=J. Cell Biol.|year=2002|volume=159|pages=91–102|display-authors=1|doi=10.1083/jcb.200202114|pmid=12370241|last2=Tavener|first2=S|last3=Raharjo|first3=E|last4=Kubes|first4=P|issue=1|pmc=2173486}}</ref> This in turn activates [[Phospholipase C|PLC]] which leads to the generation  of [[Inositol trisphosphate|IP-3]], which results in a loss of PtdIns(4,5)P<sub>2</sub> at the leading edge.<ref>{{cite journal|last=Armengol|first=MP|title=Chemokines determine local lymphoneogenesis and a reduction of circulating CXCR4+ T and CCR7 B and T lymphocytes in thyroid autoimmune diseases|journal=Journal of Immunology|volume=170|issue=12|pages=6320–6328|display-authors=1|pmid=12794165|year=2003|last2=Cardoso-Schmidt|first2=CB|last3=Fernández|first3=M|last4=Ferrer|first4=X|last5=Pujol-Borrell|first5=R|last6=Juan|first6=M|doi=10.4049/jimmunol.170.12.6320}}</ref> The chemorepellent 8CPT-cAMP inhibits PLC activity and thereby increases Ptds(3,4,5)P<sub>3</sub> accumulation and activation of [[PTEN (gene)|PTEN]]. In this manner, the chemorepellant reverses the [[Chemical polarity|polarity]] of the PtdIns(3,4,5)P<sub>3</sub> gradient and induces chemorepulsion. Recent evidence also implicates a role for PI5K and [[Rho]] signaling during directional decision making and migration.<ref>{{cite journal|last=Alblas|first=J.|title=Activation of RhoA and ROCK are essential for detachment of migrating leukocytes|journal=Mol. Biol. Cell|year=2001|volume=12|pages=2137–2145|display-authors=1|pmid=11452009|last2=Ulfman|first2=L|last3=Hordijk|first3=P|last4=Koenderman|first4=L|issue=7|pmc=55668|doi=10.1091/mbc.12.7.2137}}</ref>\n\n===Inhibitors===\nUseful inhibitors have been investigated in [[T cells]]. For example, T cell chemoattration to [[Stromal cell-derived factor-1|SDF-1]] is inhibited by the [[tyrosine]] [[kinase]] inhibitors, genistein and herbimycin.<ref>{{cite journal|last=Pham|first=!.H.|title=S1P1 receptor signaling overrides retention mediated G alpha i-coupled receptors to promote T cell egress|journal=Immunity|year=2008|volume=28|pages=122–133|display-authors=1|doi=10.1016/j.immuni.2007.11.017|pmid=18164221|last2=Okada|first2=T|last3=Matloubian|first3=M|last4=Lo|first4=CG|last5=Cyster|first5=JG|issue=1|pmc=2691390}}</ref>\n\n== References ==\n<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist|2}}\n\n== External links ==\n* [http://www.celtaxsys.com/ Celtaxsys]\n\n<!--- Categories --->\n[[Category:Cell movement]]"
    },
    {
      "title": "Cilium",
      "url": "https://en.wikipedia.org/wiki/Cilium",
      "text": "{{about|organelles|fine hairs on insect wings|Cilium (entomology)}}\n{{distinguish|Psyllium}}\n{{Infobox microanatomy\n| Name        = Cilium\n| Latin       = Cilium\n| Image       = Bronchiolar epithelium 3 - SEM.jpg\n| Caption     = [[Scanning Electron Microscope|SEM]] micrograph of the cilia projecting from [[respiratory epithelium]] in the lungs \n| Width       = \n| Image2      = \n| Caption2    = \n}}\nA '''cilium''' ({{ety|la||[[eyelash]]}};<ref>Mosby’s Medical, Nursing and Allied Health Dictionary, Fourth Edition, Mosby-Year Book Inc., 1994, p. 336</ref> the plural is '''cilia''') is an [[organelle]] found on eukaryotic cells and are slender protuberances that project from the much larger [[Cell (biology)|cell body]].<ref name=\"HHMIB2005\"/>\n\nThere are two types of cilia: ''motile'' cilia and ''non-motile'', or ''primary'', cilia, which typically serve as sensory organelles. In eukaryotes, motile cilia and [[flagellum|flagella]] together make up a group of organelles known as [[undulipodium|undulipodia]].<ref name=db2004>\n[http://www.encyclopedia.com/doc/1O6-undulipodium.html A Dictionary of Biology  ], 2004, accessed 6 April 2010.</ref><!-- This is a tertiary source, which is not the best source for Wikipedia per [[Wikipedia:RS#Primary.2C_secondary.2C_and_tertiary_sources]]; however, I've not yet been able to locate a secondary source to support this claim.  N2e, 2010-04-06 -->\nEukaryotic cilia are structurally identical to eukaryotic flagella, although distinctions are sometimes made according to function and/or length.<ref name=Haimo_JCB198112>{{cite journal | vauthors = Haimo LT, Rosenbaum JL | title = Cilia, flagella, and microtubules | journal = The Journal of Cell Biology | volume = 91 | issue = 3 Pt 2 | pages = 125s–130s | date = December 1981 | pmid = 6459327 | pmc = 2112827 | doi = 10.1083/jcb.91.3.125s }}</ref> Biologists have various ideas about [[evolution of flagella|how the various flagella may have evolved]].\n\n==Types==\n\nCilia can be divided into primary forms and motile forms.<ref name=\"Murray2009\">{{cite book|author=Karen Field Murray|title=Fibrocystic Diseases of the Liver|url=https://books.google.com/books?id=CY2moAFC4GQC&pg=PA47|access-date=25 November 2010|date=1 June 2009|publisher=Springer|isbn=978-1-60327-523-1|pages=47–}}</ref>\n\n===Primary/Immotile cilia===\nIn animals, primary cilia are found on nearly every cell.<ref name=\"HHMIB2005\"/>\n\nIn comparison to ''motile'' cilia, non-motile (or ''primary'') cilia usually occur one per cell; nearly all mammalian cells have a single non-motile ''primary cilium.'' In addition, examples of specialized primary cilia can be found in human sensory organs such as the eye and the nose:\n* The outer segment of the rod [[photoreceptor cell]] in the human eye is connected to its cell body with a specialized non-motile cilium.<ref>Wolfrum, U., & Schmitt, A. (2000). Rhodopsin transport in the membrane of the connecting cilium of mammalian photoreceptor cells. Cell Motility and the Cytoskeleton, 46(2), 95-107.</ref>\n* The [[Dendrite|dendritic knob]] of the [[olfactory]] neuron, where the [[odorant receptors]] are located, also contains non-motile cilia (about 10 cilia per dendritic knob).\n\nAlthough the primary cilium was discovered in 1898, it was largely ignored for a century.{{citation needed|date=September 2018}} Only recently has great progress been made in understanding the function of the primary cilium.  Until the 1990s, the prevailing view of the primary cilium was that it was merely a [[vestigial]] organelle without important function.<ref name=\"HHMIB2005\"/>  Recent findings regarding its physiological roles in chemical sensation, signal transduction, and control of cell growth, have led scientists to acknowledge its importance in cell function, with the discovery of its role in diseases not previously recognized to involve the [[Anterior segment dysgenesis|dysgenesis]] and dysfunction of cilia, such as [[polycystic kidney disease]],<ref name=\"pmid18264930\">{{cite journal | vauthors = Wagner CA | title = News from the cyst: insights into polycystic kidney disease | journal = Journal of Nephrology | volume = 21 | issue = 1 | pages = 14–6 | year = 2008 | pmid = 18264930 | url = http://www.jnephrol.com/Article.action?cmd=navigate&urlkey=Public_Details&t=JN&uid=9A13E591-2E27-441B-8356-23BF86D9CFB0 }}</ref> [[congenital heart disease]],<ref name=\"pmid17548739\">{{cite journal | vauthors = Brueckner M | title = Heterotaxia, congenital heart disease, and primary ciliary dyskinesia | journal = Circulation | volume = 115 | issue = 22 | pages = 2793–5 | date = June 2007 | pmid = 17548739 | doi = 10.1161/CIRCULATIONAHA.107.699256 }}</ref> and an [[emergence|emerging]] group of [[Genetic disorder|genetic]] [[ciliopathy|ciliopathies]].<ref name=\"badano2006\">{{cite journal | vauthors = Badano JL, Mitsuma N, Beales PL, Katsanis N | title = The ciliopathies: an emerging class of human genetic disorders | journal = Annual Review of Genomics and Human Genetics | volume = 7 | issue =  | pages = 125–48 | year = 2006 | pmid = 16722803 | doi = 10.1146/annurev.genom.7.080505.115610 }}</ref> It is also known that the cilium must be disassembled before mitosis can occur. However, the mechanisms that control this process are still largely unknown.<ref>{{cite journal | vauthors = Pan J, Snell W | title = The primary cilium: keeper of the key to cell division | journal = Cell | volume = 129 | issue = 7 | pages = 1255–7 | date = June 2007 | pmid = 17604715 | doi = 10.1016/j.cell.2007.06.018 }}</ref> The primary cilium is now known to play an important role in the function of many human organs.<ref name=\"HHMIB2005\">{{cite journal | last = Gardiner | first = Mary Beth | name-list-format = vanc | title = The Importance of Being Cilia | journal = HHMI Bulletin | volume = 18 | issue = 2 | date = September 2005 | url = http://www.hhmi.org/sites/default/files/Bulletin/2005/September/sept2005_fulltext.pdf | doi =  | access-date = 26 July 2008 }}</ref>\nThe current scientific understanding of primary cilia views them as \"sensory [[Cell (biology)|cellular]] antennae that coordinate a large number of cellular signaling pathways, sometimes coupling the signaling to ciliary motility or alternatively to cell division and differentiation.\".<ref name=\"Satir2008\">{{cite journal | vauthors = Satir P, Christensen ST | title = Structure and function of mammalian cilia | journal = Histochemistry and Cell Biology | volume = 129 | issue = 6 | pages = 687–93 | date = June 2008 | pmid = 18365235 | pmc = 2386530 | doi = 10.1007/s00418-008-0416-9 }}</ref>\nThe primary non-motile cilia has a “9+0” axonemal structure and is divided into subdomains. The entire structure is enclosed by a plasma membrane continuous with the plasma membrane of the cell.  The basal body, where the cilia originates, is located within the ciliary pocket. The cilium membrane and the basal body microtubules are connected by transition fibers. Vesicles carrying molecules for the cilia dock at the transition fibers. The transition fibers form a transition zone where entry and exit of molecules is regulated to and from the cilia. Molecules can move to the tip of the cilia with the aid of anterograde IFT particles and the kinesin-2 motor. Molecules can also use retrograde IFT particles and the cytoplasmic dynein motor to move toward the basal body. Some of the signaling with these cilia occur through ligand binding such as Hedgehog signaling. Other forms of signaling include G-coupled receptors including the somatostatin receptor 3 in neuronal cells. \n<ref>{{cite journal | vauthors = Wheway G, Nazlamova L, Hancock JT | title = Signaling through the Primary Cilium | journal = Frontiers in Cell and Developmental Biology | volume = 6 | pages = 8 | year = 2018 | pmid = 29473038 | pmc = 5809511 | doi = 10.3389/fcell.2018.00008 }}</ref>\n[[File:Blausen 0766 RespiratoryEpithelium.png|thumb|Illustration depicting motile cilia.]]\n\n===Motile cilia===\nLarger eukaryotes, such as mammals, have ''motile'' cilia as well. Motile cilia are usually present on a cell's surface in large numbers and beat in coordinated waves.<ref name=\"Lewin2007\">{{cite book|author=Benjamin Lewin|title=Cells|url=https://books.google.com/books?id=2VEGC8j9g9wC&pg=PA359|access-date=25 November 2010|year=2007|publisher=Jones & Bartlett Learning|isbn=978-0-7637-3905-8|pages=359}}</ref>\n\n* In [[human]]s, for example, motile cilia are found in the lining of the [[vertebrate trachea|trachea]] (windpipe), where they sweep [[mucus]] and dirt out of the lungs.<ref name=\"2012-Enuka\">{{cite journal | vauthors = Enuka Y, Hanukoglu I, Edelheit O, Vaknine H, Hanukoglu A | title = Epithelial sodium channels (ENaC) are uniformly distributed on motile cilia in the oviduct and the respiratory airways | journal = Histochemistry and Cell Biology | volume = 137 | issue = 3 | pages = 339–53 | date = March 2012 | pmid = 22207244 | doi = 10.1007/s00418-011-0904-1 }}</ref>\n* In [[female mammals]], the beating of cilia in the [[Fallopian tube]]s moves the [[ovum]] from the [[ovary]] to the [[uterus]].<ref name=\"2012-Enuka\" /><ref name=\"CIN2007\">{{cite web\n| title = Cilia in nature\n| website =\n| publisher = hitech-projects.com\n| year = 2007\n| url = http://www.hitech-projects.com/euprojects/artic/index/Cilia%20in%20nature.pdf\n| doi =\n| access-date = 28 July 2008}}</ref>\n\nThe functioning of motile cilia is strongly dependent on the maintenance of optimal levels of fluid bathing the cilia. Epithelial sodium channels [[ENaC]] that are specifically expressed along the entire length of cilia apparently serve as sensors that regulate fluid level surrounding the cilia.<ref name=\"2012-Enuka\" /><ref name=\"2016-Hanukoglu\" >{{cite journal | vauthors = Hanukoglu I, Hanukoglu A | title = Epithelial sodium channel (ENaC) family: Phylogeny, structure-function, tissue distribution, and associated inherited diseases | journal = Gene | volume = 579 | issue = 2 | pages = 95–132 | date = April 2016 | pmid = 26772908 | pmc = 4756657 | doi = 10.1016/j.gene.2015.12.061 }}</ref>\n\n[[Ciliate]]s are microscopic organisms that possess ''motile'' cilia exclusively and use them for either locomotion or to simply move liquid over their surface.\n\n== Structure==\n[[File:Eukaryotic cilium diagram en.svg|thumb|300px|Eukaryotic motile cilium]]\nInside cilia and [[flagella]] is a [[microtubule]]-based [[cytoskeleton]] called the [[axoneme]].  The axoneme of primary cilia typically has a ring of nine outer microtubule doublets (called a [[Axoneme#Structure|9+0 axoneme]]),  and the axoneme of a motile cilium has two central microtubule singlets in addition to the nine outer doublets (called a [[Axoneme#Structure |9+2 axoneme]]).  The axonemal cytoskeleton acts as a scaffolding for various [[protein]] complexes and provides binding sites for [[molecular machine|molecular motor]] proteins such as [[kinesin|kinesin II]], that help carry proteins up and down the microtubules.<ref name=\"HHMIB2005\"/><ref>{{cite journal | vauthors = Rosenbaum JL, Witman GB | title = Intraflagellar transport | journal = Nature Reviews. Molecular Cell Biology | volume = 3 | issue = 11 | pages = 813–25 | date = November 2002 | pmid = 12415299 | doi = 10.1038/nrm952 }}</ref><ref>{{cite journal | vauthors = Scholey JM | title = Intraflagellar transport motors in cilia: moving along the cell's antenna | journal = The Journal of Cell Biology | volume = 180 | issue = 1 | pages = 23–9 | date = January 2008 | pmid = 18180368 | pmc = 2213603 | doi = 10.1083/jcb.200709133 }}</ref> On the outside of cilia is a membrane like the plasma membrane, but compositionally distinct due to a blocking ring (\"necklace\") around the base,<ref>{{cite journal | vauthors = Rohatgi R, Snell WJ | title = The ciliary membrane | journal = Current Opinion in Cell Biology | volume = 22 | issue = 4 | pages = 541–6 | date = August 2010 | pmid = 20399632 | pmc = 2910237 | doi = 10.1016/j.ceb.2010.03.010 }}</ref> and distinct also in its population of receptors and other integral proteins.\n\n===Ciliary rootlet===\nThe ciliary rootlet is a cytoskeleton-like structure that originates from the basal body at the proximal end of a cilium. It extends proximally toward the [[cell nucleus]]. Rootlets are typically 80-100&nbsp;nm in diameter and contain cross striae distributed at regular intervals of approximately 55-70&nbsp;nm. According to the [[Gene Ontology]], the following proteins localize to the ciliary rootlet: [[amyloid precursor protein]], [[CROCC|rootletin]], [[kinesin]]s ([[KIF5B]], [[KIF5C]], [[KLC2]], [[KLC3]]), and [[presenilin]]s ([[PSEN1]], [[PSEN2]]).<ref>{{cite web|title=Ciliary Rootlet|url=http://amigo.geneontology.org/cgi-bin/amigo/term-assoc.cgi?gptype=all&speciesdb=all&taxid=9606&evcode=all&term_assocs=all&term=GO%3A0035253&action=filter|work=Gene Ontology|access-date=13 June 2012}}</ref>\n\n===Cilia versus flagella===\nThough they have been given different names, motile cilia and flagella have nearly identical structures and have the same purpose: motion. The movement of the appendage can be described as a wave. The wave tends to originate from the cilium base and can be described in terms of frequency (ciliary beat frequency or CBF), amplitude and wave length. The beating motion is created by dynein arm structures the sliding of outer doublets, and originates in the axoneme, not at the basal body. A key difference between the two structures is that in a eukaryotic organism such as humans, flagella are used to propel the cell, while cilia are used to move substances across a surface. An example of each would be the flagellum present on a sperm cell and the cilium on the epithelial tissue of the lungs that clears out foreign particles. Motile cilia and flagella possess the same [[Axoneme#Structure|9+2 axoneme]] structure. The 9 indicates the number of doublets present around the outer edge of the appendage while the 2 refers to a central pair of independent microtubules. In primary and other non-motile cilia, the axoneme lacks a central pair, resulting in a [[Axoneme#Structure|9+0 axoneme]] structure. <ref>{{cite journal | vauthors = Foi A, Salvo FD, Doctorovich F, Huck-Iriart C, Ramallo-López JM, Dürr M, Ivanović-Burmazović I, Stirnat K, Garbe S, Klein A | title = Synthesis and structural characterisation of unprecedented primary N-nitrosamines coordinated to iridium(iv) | journal = Dalton Transactions | volume = 47 | issue = 33 | pages = 11445–11454 | date = August 2018 | pmid = 30065990 | doi = 10.1042/BCJ20170453 }}</ref>\n\n==Development==\nCilia are formed through the process of [[ciliogenesis]]. The building blocks of the cilia such as [[tubulins]] and other partially assembled axonemal proteins are added to the ciliary tips which point away from the cell body.<ref>{{cite journal | vauthors = Johnson KA, Rosenbaum JL | title = Polarity of flagellar assembly in Chlamydomonas | journal = The Journal of Cell Biology | volume = 119 | issue = 6 | pages = 1605–11 | date = December 1992 | pmid = 1281816 | pmc = 2289744 | doi = 10.1083/jcb.119.6.1605 }}</ref> In most species bi-directional motility called [[intraflagellar transport]] (IFT) plays an essential role in moving these building materials from the cell body to the assembly site.<ref>{{cite journal | vauthors = Hao L, Thein M, Brust-Mascher I, Civelekoglu-Scholey G, Lu Y, Acar S, Prevo B, Shaham S, Scholey JM | title = Intraflagellar transport delivers tubulin isotypes to sensory cilium middle and distal segments | journal = Nature Cell Biology | volume = 13 | issue = 7 | pages = 790–8 | date = June 2011 | pmid = 21642982 | pmc = 3129367 | doi = 10.1038/ncb2268 }}</ref> IFT also carries the disassembled material to be recycled from the ciliary tip back to the cell body. By regulating the equilibrium between these two IFT processes, the length  of cilia can be maintained dynamically. Disassembly of cilia requires the action of the [[Aurora A kinase]] .<ref name=\"entrez\">{{cite journal | vauthors = Pugacheva EN, Jablonski SA, Hartman TR, Henske EP, Golemis EA | title = HEF1-dependent Aurora A activation induces disassembly of the primary cilium | journal = Cell | volume = 129 | issue = 7 | pages = 1351–63 | date = June 2007 | pmid = 17604723 | pmc = 2504417 | doi = 10.1016/j.cell.2007.04.035 }}</ref>\n\nExceptions where IFT is not present include ''[[Plasmodium falciparum]]'', which is one of the species of ''[[Plasmodium]]'' that cause [[malaria]] in humans. In this parasite, cilia assemble in the cytoplasm.<ref>[http://www.pandasthumb.org/archives/2007/06/of_cilia_and_si.html Of cilia and silliness (more on Behe) - The Panda's Thumb] {{webarchive|url=https://web.archive.org/web/20071017161257/http://pandasthumb.org/archives/2007/06/of_cilia_and_si.html |date=17 October 2007 }}</ref>\n\nAt the base of the cilium where it attaches to the cell body is the microtubule organizing center, the [[basal body]]. Some basal body proteins as [[CEP164]], [[ODF2]] <ref name=\"entrez2\">{{cite journal | vauthors = Ishikawa H, Kubo A, Tsukita S, Tsukita S | title = Odf2-deficient mother centrioles lack distal/subdistal appendages and the ability to generate primary cilia | journal = Nature Cell Biology | volume = 7 | issue = 5 | pages = 517–24 | date = May 2005 | pmid = 15852003 | doi = 10.1038/ncb1251 }}</ref>\nand [[CEP170]],<ref name=\"edoc\">{{cite thesis | last = Lamla | first = Stefan | name-list-format = vanc | title = Functional characterisation of the centrosomal protein Cep170| url = http://edoc.ub.uni-muenchen.de/9783/| access-date = | date = 2009-01-22| publisher = Ludwig-Maximilians-Universität München| type = Ph.D. }}</ref> regulate the formation and the stability of the cilium.  A transition zone between the basal body and the axoneme \"serves as a docking station for intraflagellar transport and motor proteins.\"<ref name=\"HHMIB2005\"/>\n\nIn effect, the cilium is a [[Molecular machine|nanomachine]] composed of perhaps over 600 proteins in molecular complexes, many of which also function independently as nanomachines.  [[Flexible linker]]s allow the \n[[Protein domain#Domains and protein flexibility|mobile protein domains]]\nconnected by them to recruit their binding partners and induce \nlong-range [[allostery]] via \n[[Protein dynamics#Global flexibility: multiple domains|protein domain dynamics]].<ref name=\"Satir2008\"/>\n\n== Function ==\nThe [[dynein]] in the axoneme forms bridges between neighbouring microtubule doublets. When ATP activates the motor domain of dynein, it attempts to walk along the adjoining microtubule doublet. This would force the adjacent doublets to slide over one another if not for the presence of [[Nexin]] between the microtubule doublets. And thus the force generated by dynein is instead converted into a bending motion.<ref>{{Cite book |url = https://www.ncbi.nlm.nih.gov/books/NBK26888/|title = Molecular Biology of the Cell|access-date = |website = |last = Alberts|first = Bruce|year = 2002}}</ref>\n\n===Sensing the extracellular environment===\nSome primary cilia on [[epithelial]] cells in eukaryotes act as ''cellular antennae'', providing [[Molecular sensor|chemosensation]], [[thermosensation]] and [[mechanosensation]] of the extracellular environment.<ref name=\"Adams2008\"/> These cilia then play a role in mediating specific signalling cues, including soluble factors in the external cell environment, a [[Secretion|secretory]] role in which a soluble protein is released to have an effect downstream of the fluid flow, and mediation of fluid flow if the cilia are [[Motility|motile]].<ref name=Adams2008>{{cite journal | vauthors = Adams M, Smith UM, Logan CV, Johnson CA | title = Recent advances in the molecular pathology, cell biology and genetics of ciliopathies | journal = Journal of Medical Genetics | volume = 45 | issue = 5 | pages = 257–67 | date = May 2008 | pmid = 18178628 | pmc =  | doi = 10.1136/jmg.2007.054999 }}</ref> Some [[epithelial]] cells are ciliated, and they commonly exist as a sheet of polarized cells forming a tube or tubule with cilia projecting into the [[Lumen (anatomy)|lumen]]. This sensory and signalling role puts cilia in a central role for maintaining the local cellular environment and may be why [[Ciliopathy|ciliary defects]] cause such a wide range of human diseases.<ref name=pubmed20060804>{{cite journal | vauthors = Singla V, Reiter JF | title = The primary cilium as the cell's antenna: signaling at a sensory organelle | journal = Science | volume = 313 | issue = 5787 | pages = 629–33 | date = August 2006 | pmid = 16888132 | doi = 10.1126/science.1124534 | bibcode = 2006Sci...313..629S }}</ref>\n\n==Clinical significance==\n{{main|Ciliopathy}}\nCiliary defects can lead to a number of human diseases. Genetic mutations compromising the proper functioning of cilia, [[ciliopathy|ciliopathies]], can cause chronic disorders such as [[primary ciliary dyskinesia]] (PCD), [[nephronophthisis]] or [[Senior-Loken syndrome]]. In addition, a defect of the primary cilium in the [[Nephron#Renal tubule|renal tubule]] cells can lead to [[polycystic kidney disease]] (PKD). In another genetic disorder called [[Bardet-Biedl syndrome]] (BBS), the mutant gene products are the components in the basal body and cilia.<ref name=\"badano2006\"/>\n\nLack of functional cilia in the [[fallopian tubes]] can cause [[ectopic pregnancy]]. A fertilized [[ovum]] may not reach the [[uterus]] if the cilia are unable to move it there. In such a case, the ovum will implant in the fallopian tubes, causing a [[tubal pregnancy]], the most common form of ectopic pregnancy.<ref>{{cite journal | vauthors = Horne AW, Critchley HO | title = Mechanisms of disease: the endocrinology of ectopic pregnancy | journal = Expert Reviews in Molecular Medicine | volume = 14 | pages = e7 | date = March 2012 | pmid = 22380790 | doi = 10.1017/erm.2011.2 }}</ref>\n\nAs noted above, epithelial sodium channels [[ENaC]] that are expressed along the length of cilia regulate fluid level surrounding the cilia. Mutations that decrease the activity of ENaC result in multisystem [[pseudohypoaldosteronism]], that is associated with fertility problems.<ref name=\"2012-Enuka\" /> In [[cystic fibrosis]] that results from mutations in the chloride channel [[Cystic fibrosis transmembrane conductance regulator|CFTR]], ENaC activity is enhanced leading to a severe reduction of the fluid level that causes complications and infections in the respiratory airways.<ref name=\"2016-Hanukoglu\" />\n\nSince the flagellum of human sperm is actually a modified cilium, ciliary dysfunction can also be responsible for male infertility.<ref name=\"pmid16850538\">{{cite journal | vauthors = Ichioka K, Kohei N, Okubo K, Nishiyama H, Terai A | title = Obstructive azoospermia associated with chronic sinopulmonary infection and situs inversus totalis | journal = Urology | volume = 68 | issue = 1 | pages = 204.e5–7 | date = July 2006 | pmid = 16850538 | doi = 10.1016/j.urology.2006.01.072 }}</ref>\n\nOf interest, there is an association of primary ciliary dyskinesia with left-right anatomic abnormalities such as [[situs inversus]] (a combination of findings known as [[Kartagener's syndrome]]) and other heterotaxic defects. These left-right anatomic abnormalities can also result in [[congenital heart disease]].<ref name=\"pmid17515466\">{{cite journal | vauthors = Kennedy MP, Omran H, Leigh MW, Dell S, Morgan L, Molina PL, Robinson BV, Minnix SL, Olbrich H, Severin T, Ahrens P, Lange L, Morillas HN, Noone PG, Zariwala MA, Knowles MR | title = Congenital heart disease and other heterotaxic defects in a large cohort of patients with primary ciliary dyskinesia | journal = Circulation | volume = 115 | issue = 22 | pages = 2814–21 | date = June 2007 | pmid = 17515466 | doi = 10.1161/CIRCULATIONAHA.106.649038 }}</ref> It has been shown that proper cilial function is responsible for the normal left-right asymmetry in mammals.<ref name=\"pmid12888012\">{{cite journal | vauthors = McGrath J, Brueckner M | title = Cilia are at the heart of vertebrate left-right asymmetry | journal = Current Opinion in Genetics & Development | volume = 13 | issue = 4 | pages = 385–92 | date = August 2003 | pmid = 12888012 | doi = 10.1016/S0959-437X(03)00091-1 }}</ref>\n\n===Ciliopathy as an origin for many multi-symptom genetic diseases===\nEarly 2000s findings in genetic research have suggested that a large number of [[genetic disorder]]s, both [[Syndrome|genetic syndromes]] and [[Disease|genetic diseases]], that were not previously related in the medical literature, may be, in fact, highly related in the [[genotype|root cause]] of the widely varying set of medical [[phenotype|symptoms]] that are clinically visible in the [[Disorder (medicine)|disorder]].  These have been grouped as an [[emergence|emerging]] class of diseases called [[ciliopathy|ciliopathies]].  The underlying cause may be a dysfunctional molecular mechanism in the primary/immotile cilia, [[organelle]]s which are present in many diverse cellular types throughout the [[human]] body.\n\nCilia defects adversely affect numerous critical developmental signaling pathways essential to cellular development and thus offer a plausible hypothesis for the often [[pleiotropic|multi-symptom]] nature of a large set of syndromes and diseases.<ref name=\"badano2006\"/>  Known ciliopathies include [[primary ciliary dyskinesia]], [[Bardet-Biedl syndrome]], [[polycystic kidney disease|polycystic kidney]] and [[polycystic liver disease|liver disease]], [[nephronophthisis]], [[Alström syndrome]], [[Meckel-Gruber syndrome]], [[Sensenbrenner syndrome]] and some forms of [[retinopathy|retinal degeneration]].<ref name=\"badano2006\"/><ref name=Adams2008/>\n\n===Extracellular changes===\nReduction of cilia function can also result from infection. Research into biofilms has been increasing and has shown how bacteria can alter cilia. A biofilm is a community of bacteria of either the same or multiple species of bacteria. The cluster of cells secretes different factors which form an extracellular matrix. Cilia in the respiratory system is known to move mucus and pathogens out of the airways. It has been found that patients with biofilm positive infections have impaired cilia function. The impairment may present as decreased motion or reduction in the number of cilia. Though these changes result from an external source, they still effect the pathogenicity of the bacteria, progression of infection, and how it is treated.<ref>{{cite journal | vauthors = Fastenberg JH, Hsueh WD, Mustafa A, Akbar NA, Abuzeid WM | title = Biofilms in chronic rhinosinusitis: Pathophysiology and therapeutic strategies | journal = World Journal of Otorhinolaryngology - Head and Neck Surgery | volume = 2 | issue = 4 | pages = 219–229 | date = December 2016 | pmid = 29204570 | pmc = 5698538 | doi = 10.1016/j.wjorl.2016.03.002 }}</ref>\n\n== See also ==\n* [[Molecular machine#Biological|Biological machines]]\n* [[Flagellum]]\n* [[Kinocilium]]\n* [[Mucociliary clearance]]\n* [[Protein dynamics#Global flexibility: multiple domains|Protein domain dynamics]]\n* [[Protein domain#Domains and protein flexibility|Protein flexibility]]\n* [[Stereocilia]]\n* [[Undulipodia]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.hhmi.org/bulletin/sept2005/pdf/Cilia.pdf Brief summary of importance of cilia to many organs in human physiology]\n* [http://www.ciliaproteome.org The Ciliary Proteome Web Page at Johns Hopkins]\n{{Cellular structures}}\n{{Epithelial proteins}}\n{{Protist structures}}\n{{Use dmy dates|date=November 2018}}\n{{Authority control}}\n\n[[Category:Cell movement]]\n[[Category:Organelles]]"
    },
    {
      "title": "Contractility",
      "url": "https://en.wikipedia.org/wiki/Contractility",
      "text": "{{dabconcept}}\n'''Contractility''' refers to the ability for self-[[wikt:contraction|contraction]], especially of the muscles, or similar active biological tissue:\n*[[Muscle contraction]]\n**[[Myocardial contractility]]\n*[[Contractile vacuole]]\n*[[Contractile ring]] in cytokinesis\n*see [[contractile cell]] for an overview of cell types in humans\n\n==See also==\n*[[motility]]\n\n{{SIA}}\n\n[[Category:Cell movement]]"
    },
    {
      "title": "Ena/Vasp homology proteins",
      "url": "https://en.wikipedia.org/wiki/Ena%2FVasp_homology_proteins",
      "text": "{{Pfam box\n|image=Domain organisation of EVH proteins.png|caption=Domain organisation of EVH proteins\n|Symbol=VASP/EVL\n|InterPro=IPR017354\n}}\n{{Infobox protein family\n| Symbol = VASP_tetra\n| Name = VASP tetramerisation domain\n| image = PDB 1usd EBI.jpg\n| width =\n| caption = human vasp tetramerisation domain l352m\n| Pfam = PF08776\n| Pfam_clan = \n| InterPro = IPR014885\n| SMART =\n| PROSITE =\n| MEROPS =\n| SCOP =\n| TCDB =\n| OPM family =\n| OPM protein =\n| CAZy =\n| CDD =\n}}\n\n'''ENA/VASP Homology proteins''' or '''EVH''' proteins are a family of closely related [[proteins]] involved in [[cell motility]] in [[vertebrate]] and [[invertebrate]] animals. EVH proteins are modular proteins that are involved in [[actin]] polymerization, as well as interactions with other proteins. Within the cell, Ena/VASP proteins are found at the leading edge of [[Lamellipodia]] and at the tips of [[filopodia]].<ref name=\"pmid19494122\">{{cite journal | author = Bear and Gertler | title = Ena/VASP: towards resolving a controversy at the barbed end | journal = J Cell Sci | year = 2009 | pmid = 19494122 | pmc =  2723151| doi =10.1242/jcs.038125 | last2 = Gertler | first2 = FB | volume = 122 | issue = Pt 12 | pages = 1947–53  }}</ref> Ena, the founding member of the family was discovered in a [[Drosophila melanogaster|drosophila]] genetic screen for [[mutations]] that act as [[Dominance (genetics)|dominant]] suppressors of the [[Abl gene|abl]] non receptor [[tyrosine kinase]]. Invertebrate animals have one Ena homologue, whereas mammals have three, named [[ENAH|Mena]], [[Vasodilator-stimulated phosphoprotein|VASP]], and [[Enah/Vasp-like|Evl]].\n\nEna/VASP proteins promote the spatially regulated actin polymerization required for efficient chemotaxis in response to attractive and repulsive guidance cues. Mice lacking functional copies of all three family members display pleiotropic [[phenotypes]] including [[exencephaly]], [[edema]], failures in [[neurite]] formation, and embryonic lethality.\n\nA sub-domain of EVH is the [[EVH1 domain]].\n\n==VASP==\n\nVasodilator-stimulated phosphoprotein (VASP)  45-[[amino acid|residue]]-long [[tetramer]]ization [[protein domain]]  which regulates [[actin]] dynamics in the [[cytoskeleton]]. This is vital for processes such as [[cell adhesion]] and [[cell migration]].<ref>{{cite journal|first1=Yidi |last1=Wu |first2= Susan J. |last2=Gunst|title=Vasodilator-stimulated Phosphoprotein (VASP) Regulates Actin Polymerization and Contraction in Airway Smooth Muscle by a Vinculin-dependent Mechanism|journal=Journal of Biological Chemistry|volume=290|issue=18|date=1 May 2015|pages=11403–11416|doi=10.1074/jbc.M115.645788}}</ref>\n\n==Function==\nVasodilator-stimulated [[phosphoprotein]] (VASP) is an [[actin]] [[cytoskeletal]] [[gene regulation|regulatory]] [[protein]]. VASP is often found in dynamic actin structures like [[filopodia]] and [[lamellipodia]], but its precise function in their formation is controversial. VASP accelerates filament elongation by delivering [[monomer]]ic actin to the growing barbed (+) end.<ref name=\"pmid18923426\">{{cite journal|vauthors=Breitsprecher D, Kiesewetter AK, Linkner J, Urbanke C, Resch GP, Small JV, Faix J | title=Clustering of VASP actively drives processive, WH2 domain-mediated actin filament elongation. | journal=EMBO J | year= 2008 | volume= 27 | issue= 22 | pages= 2943–54 | pmid=18923426 | doi=10.1038/emboj.2008.211 | pmc=2585163 }}</ref>\n\n==Structure==\nThe tetramerisation domain has a right-handed [[alpha helix|alpha helical]] [[coiled-coil]] structure.<ref name=\"pmid15569942\">{{cite journal |vauthors=Kühnel K, Jarchau T, Wolf E, Schlichting I, Walter U, Wittinghofer A, Strelkov SV | title = The VASP tetramerization domain is a right-handed coiled coil based on a 15-residue repeat | journal = Proc. Natl. Acad. Sci. U.S.A. | volume = 101 | issue = 49 | pages = 17027–32 |date=December 2004 | pmid = 15569942 | pmc = 535362 | doi = 10.1073/pnas.0403069101 | url = }}</ref>\n\n==References==\n{{reflist}}\n{{InterPro content|IPR014885}}\n\n{{DEFAULTSORT:Ena Vasp homology proteins}}\n[[Category:EVH domain]]\n[[Category:Protein domains]]\n[[Category:Proteins]]\n[[Category:Cell movement]]\n[[Category:Cytoskeleton]]\n[[Category:Cell biology]]\n\n\n{{Molecular-cell-biology-stub}}"
    },
    {
      "title": "Filopodia",
      "url": "https://en.wikipedia.org/wiki/Filopodia",
      "text": "[[Image:Filopodia.jpg|180px|thumb|This electron micrograph shows exaggerated filopodia with club-like shape induced by formin mDia2 in cultured cells. These filopodia are filled with bundled actin filaments which were born in and converged from the [[Lamellipodia|lamellipodial network]].]]\n\n'''Filopodia,''' also called '''microspikes''', are slender [[pseudopodia|cytoplasmic projections]] that extend beyond the leading edge of [[lamellipodia]] in migrating cells.<ref name=\"pmid18464790\">{{cite journal |vauthors=Mattila PK, Lappalainen P |title=Filopodia: molecular architecture and cellular functions |journal=Nat. Rev. Mol. Cell Biol. |volume=9 |issue=6 |pages=446–54 |date=June 2008 |pmid=18464790 |doi=10.1038/nrm2406}}</ref> They contain actin filaments cross-linked into bundles by actin-binding proteins, e.g. [[fascin]] and [[fimbrin]].<ref name=\"pmid9334343\">{{cite journal |vauthors=Hanein D, Matsudaira P, DeRosier DJ |title=Evidence for a conformational change in actin induced by fimbrin (N375) binding |journal=J. Cell Biol. |volume=139 |issue=2 |pages=387–96 |date=October 1997 |pmid=9334343 |doi=10.1083/jcb.139.2.387 |pmc=2139807}}</ref> Filopodia form focal adhesions with the substratum, linking it to the cell surface.<ref>Molecular Cell Biology Fifth Edition Lodish, Berk, Matsudaira, Kaiser, Krieger, Scott, Zipursky, Darnell. pg. 821, 823 2004 by W.H. Freeman and Company.</ref> Many types of migrating [[cell (biology)|cells]] display filopodia, which are thought to be involved in both sensation of chemotropic cues, and resulting changes in directed locomotion.\n\nActivation of the Rho family of small Ras-related [[GTPase]]s, particularly [[cdc42]] and their downstream intermediates results in the polymerization of actin fibers by [[Ena/Vasp homology proteins]].<ref name=ohta>{{cite journal |vauthors=Ohta Y, Suzuki N, Nakamura S, Hartwig JH, Stossel TP |title=The small GTPase RalA targets filamin to induce filopodia |journal=Proc. Natl. Acad. Sci. U.S.A. |volume=96 |issue=5 |pages=2122–8 |date=March 1999 |pmid=10051605 |doi=10.1073/pnas.96.5.2122 |pmc=26747}}</ref> Growth factors bind to receptor tyrosine kinases resulting in the [[polymerization]] of [[actin]] filaments, which, when cross-linked, make up the supporting [[cytoskeleton|cytoskeletal]] elements of filopodia. Rho activity also results in activation by phosphorylation of [[ERM protein family|ezrin-moesin-radixin]] family proteins that link [[actin]] filaments to the filopodia membrane.<ref name=ohta/>\n\nFilopodia have roles in sensing, migration and cell-cell interaction.<ref name=\"pmid18464790\"/> To close a wound in vertebrates, growth factors stimulate the formation of filopodia in [[fibroblasts]] to direct [[fibroblast]] migration and wound closure.<ref name=\"pmid3773996\"/> In developing [[neurons]], filopodia extend from the [[growth cone]] at the leading edge. In neurons deprived of filopodia by partial inhibition of [[actin|actin filaments]] polymerization, growth cone extension continues as normal but direction of growth is disrupted and highly irregular.<ref name=\"pmid3773996\">{{cite journal |vauthors=Bentley D, Toroian-Raymond A |title=Disoriented pathfinding by pioneer neurone growth cones deprived of filopodia by cytochalasin treatment |journal=Nature |volume=323 |issue=6090 |pages=712–5 |year=1986 |pmid=3773996 |doi=10.1038/323712a0 |url=}}</ref> Filopodia-like projections have also been linked to [[dendrite]] creation when new [[synapse]]s are formed in the brain.<ref>{{cite journal|last=Beardsley|first=John|title=Getting Wired|journal=Scientific American|date=June 1999}}</ref><ref>{{cite journal|last=Maletic-Savatic|first=M.|author2=Malinow R.|title=Rapid Dendritic Morphogenesis in CA1 Hippocampal Dendrites Induced by Synaptic Activity|journal=Science|volume=283|issue=5409|pages=1923–1927|date=June 1999|doi=10.1126/science.283.5409.1923 }}</ref> In [[macrophages]], filopodia act as phagocytic tentacles and pull bound objects towards the cell for [[phagocytosis]].<ref name=\"PMID_17620618\">{{cite journal |vauthors=Kress H, Stelzer EH, Holzer D, Buss F, Griffiths G, Rohrbach A |title=Filopodia act as phagocytic tentacles and pull with discrete steps and a load-dependent velocity |journal=Proc. Natl. Acad. Sci. U.S.A. |volume=104 |issue=28 |pages= 11633–11638 |year=2007 |pmid=17620618 |doi=10.1073/pnas.0702449104 |pmc=1913848}}</ref>\n\nFilopodia are also used for movement of bacteria between cells, so as to evade the host immune system. The intracellular bacteria ''Ehrlichia'' are transported between cells through the host cell filopodia induced by the pathogen during initial stages of infection.<ref>Thomas S, Popov VL, Walker DH (2010) Exit Mechanisms of the Intracellular Bacterium Ehrlichia\" ''PLoS ONE'' 5(12)  e15775. http://www.plosone.org/article/info:doi%2F10.1371%2Fjournal.pone.0015775</ref> Viruses were shown to be transported along filopodia toward the cell body, leading to cell infection.<ref name=\"pmid16027225\">{{cite journal |vauthors=Lehmann MJ, Sherer NM, Marks CB, Pypaert M, Mothes W |title=Actin- and myosin-driven movement of viruses along filopodia precedes their entry into cells. |journal=J Cell Biol |volume=170|pages=317–325 |year=2005|pmid=16027225 |doi=10.1083/jcb.200503059  |url= |issue=2 |pmc=2171413}}</ref> Directed transport of receptor-bound [[Epidermal growth factor|epidermal growth factor (EGF)]] along filopodia has also been described, supporting the proposed sensing function of filopodia.<ref name=\"renamed_from_16103229_on_20121017231618\">{{cite journal|last=Lidke|first=DS|author2=Lidke, KA |author3=Rieger, B |author4=Jovin, TM |author5= Arndt-Jovin, DJ |title=Reaching out for signals: filopodia sense EGF and respond by directed retrograde transport of activated receptors. |journal=The Journal of Cell Biology |date=Aug 15, 2005|volume=170|issue=4|pages=619–26|pmid=16103229|doi=10.1083/jcb.200503140|pmc=2171515}}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.mechanobio.info/topics/cytoskeleton-dynamics/go-0030175 MBInfo - Filopodia]\n* [http://www.mechanobio.info/topics/cytoskeleton-dynamics/go-0030175/go-0046847 MBInfo - Filopodia Assembly]\n* [http://www.myredhotphilosophy.com/2011/03/new-form-of-cinema-cellular-film.html]\n\n{{Authority control}}\n\n[[Category:Cell movement]]\n[[Category:Cytoskeleton]]\n[[Category:Cell biology]]\n[[Category:Actin-based structures]]\n\n[[de:Filopodium]]"
    },
    {
      "title": "Flagellum",
      "url": "https://en.wikipedia.org/wiki/Flagellum",
      "text": "{{About||the insect anatomical structure|Antenna (biology)|the flagella of male Solifugae|Solifugae|Eukaryotic only|Cilia}}\n{{Use dmy dates|date=December 2012}}\n{{Infobox anatomy\n| Name        = Flagellum\n| Latin       =\n| Image       = Flagellum base diagram-en.svg\n| Caption     = Structure of bacterial flagellum.\n| Width       =\n| Image2      = Chlamydomonas (10000x).jpg\n| Caption2    = [[Scanning electron microscope|SEM]] image of flagellated ''[[Chlamydomonas]]'' sp. (10000×)\n| Precursor   =\n| System      =\n| Artery      =\n| Vein        =\n| Nerve       =\n| Lymph       =\n}}\nA '''flagellum''' ({{IPAc-en|f|l|ə|ˈ|dʒ|ɛ|l|əm}}; plural: '''flagella''') is a lash-like appendage that protrudes from the [[soma (biology)|cell body]] of certain [[bacteria]] and [[eukaryotic]] cells termed as [[flagellate]]s. A flagellate can have one or several flagella. The primary function of a flagellum is that of [[animal locomotion|locomotion]], but it also often functions as a sensory [[organelle]], being sensitive to chemicals and temperatures outside the cell.<ref>{{cite journal | vauthors = Wang Q, Suzuki A, Mariconda S, Porwollik S, Harshey RM | title = Sensing wetness: a new role for the bacterial flagellum | journal = The EMBO Journal | volume = 24 | issue = 11 | pages = 2034–42 | date = June 2005 | pmid = 15889148 | pmc = 1142604 | doi = 10.1038/sj.emboj.7600668 }}</ref><ref name=\"pmid12624192\">{{cite journal | vauthors = Bardy SL, Ng SY, Jarrell KF | title = Prokaryotic motility structures | journal = Microbiology | volume = 149 | issue = Pt 2 | pages = 295–304 | date = February 2003 | pmid = 12624192 | doi = 10.1099/mic.0.25948-0 }}</ref><ref name=\"Lefebvre_2001\">{{cite journal | vauthors = Silflow CD, Lefebvre PA | title = Assembly and motility of eukaryotic cilia and flagella. Lessons from Chlamydomonas reinhardtii | journal = Plant Physiology | volume = 127 | issue = 4 | pages = 1500–7 | date = December 2001 | pmid = 11743094 | pmc = 1540183 | doi = 10.1104/pp.010807 }}</ref><ref name= JarrellK>{{cite book | veditors = Jarrell K |year=2009 |title=Pili and Flagella: Current Research and Future Trends |publisher=Caister Academic Press |isbn= 978-1-904455-48-6}}</ref> The similar structure in the [[archaea]] functions in the same way but is structurally different and has been termed the [[archaellum]].<ref name=Albers>{{cite journal | vauthors = Albers SV, Jarrell KF | title = The archaellum: how Archaea swim | journal = Frontiers in Microbiology | volume = 6 | pages = 23 | date = 27 January 2015 | pmid = 25699024 | doi = 10.3389/fmicb.2015.00023 | pmc = 4307647 }}</ref>\n\nFlagella are organelles defined by function rather than structure. Flagella vary greatly. Both prokaryotic and eukaryotic flagella can be used for swimming but they differ greatly in protein composition, structure, and mechanism of propulsion. The word [[Wiktionary:flagellum|''flagellum'']] in [[Latin]] means [[whip]].\n\nAn example of a flagellated [[bacteria|bacterium]] is the ulcer-causing ''[[Helicobacter pylori]]'', which uses multiple flagella to propel itself through the mucus lining to reach the stomach [[epithelium]].<ref name=\"pmid11584108\">{{cite journal | vauthors = Lacy BE, Rosemore J | title = Helicobacter pylori: ulcers and more: the beginning of an era | journal = The Journal of Nutrition | volume = 131 | issue = 10 | pages = 2789S-2793S | date = October 2001 | pmid = 11584108 | doi = 10.1093/jn/131.10.2789S | url = http://jn.nutrition.org/cgi/content/abstract/131/10/2789S | format = abstract page }}</ref> An example of a eukaryotic flagellate cell is the mammalian [[spermatozoon|sperm cell]], which uses its flagellum to propel itself through the female reproductive tract.<ref name=\"pmid17148374\">{{cite journal | vauthors = Malo AF, Gomendio M, Garde J, Lang-Lenton B, Soler AJ, Roldan ER | title = Sperm design and sperm function | journal = Biology Letters | volume = 2 | issue = 2 | pages = 246–9 | date = June 2006 | pmid = 17148374 | pmc = 1618917 | doi = 10.1098/rsbl.2006.0449 }}</ref> Eukaryotic flagella are structurally identical to eukaryotic [[cilia]], although distinctions are sometimes made according to function or length.<ref name=\"pmid6459327\">{{cite journal | vauthors = Haimo LT, Rosenbaum JL | title = Cilia, flagella, and microtubules | journal = The Journal of Cell Biology | volume = 91 | issue = 3 Pt 2 | pages = 125s-130s | date = December 1981 | pmid = 6459327 | pmc = 2112827 | doi = 10.1083/jcb.91.3.125s }}</ref> [[Fimbria (bacteriology)|Fimbriae]] and [[pilus|pili]] are also thin appendages, but have different functions and are usually smaller.\n\n==Types==\n[[File:Difference Between Prokaryote and Eukaryote Flagella.svg|thumb|Prokaryotic flagella run in a rotary movement, while eukaryotic flagella run in a bending movement. The prokaryotic flagella use a rotary motor, and the eukaryotic flagella use a complex sliding filament system. Eukaryotic flagella are ATP-driven, while prokaryotic flagella can be ATP-driven (Archaea) or proton-driven (Bacteria).<ref>{{cite journal | vauthors = Streif S, Staudinger WF, Marwan W, Oesterhelt D | year = 2008 | title = Flagellar rotation in the archaeon Halobacterium salinarum depends on ATP | url = | journal = Journal of Molecular Biology | volume = 384 | issue = | pages = 1–8 }}</ref>]]\nThree types of flagella have so far been distinguished: bacterial, archaeal, and eukaryotic.\n\nThe main differences among these three types are:\n\n*Bacterial flagella are helical filaments, each with a [[rotating locomotion in living systems|rotary motor]] at its base which can turn clockwise or counterclockwise.<ref name=Silverman1>{{cite journal | vauthors = Silverman M, Simon M | title = Flagellar rotation and the mechanism of bacterial motility | journal = Nature | volume = 249 | issue = 452 | pages = 73–4 | date = May 1974 | pmid = 4598030 | doi = 10.1038/249073a0 | bibcode = 1974Natur.249...73S }}</ref><ref name=Meister1>{{cite journal |title= Rapid rotation of flagellar bundles in swimming bacteria |vauthors=Meister GL, Berg HC |journal=Nature |year=1987 |volume=325 |pages=637–640 |doi= 10.1038/325637a0 |issue=6105|bibcode=1987Natur.325..637L }}</ref><ref name=Berg1>{{cite journal | vauthors = Berg HC, Anderson RA | title = Bacteria swim by rotating their flagellar filaments | journal = Nature | volume = 245 | issue = 5425 | pages = 380–2 | date = October 1973 | pmid = 4593496 | doi = 10.1038/245380a0 | bibcode = 1973Natur.245..380B }}</ref> They provide two of several kinds of bacterial motility.<ref name=Jahn1>{{cite journal | vauthors = Jahn TL, Bovee EC | title = Movement and locomotion of microorganisms | journal = Annual Review of Microbiology | volume = 19 | pages = 21–58 | year = 1965 | pmid = 5318439 | doi = 10.1146/annurev.mi.19.100165.000321 }}</ref><ref name=Harshey1>{{cite journal | vauthors = Harshey RM | title = Bacterial motility on a surface: many ways to a common goal | journal = Annual Review of Microbiology | volume = 57 | pages = 249–73 | year = 2003 | pmid = 14527279 | doi = 10.1146/annurev.micro.57.030502.091014 }}</ref>\n*Archaeal flagella ([[archaellum|archaella]]) are superficially similar to bacterial flagella, but are different in many details and considered non-[[homology (biology)|homologous]].<ref name=Ng1>{{cite journal | vauthors = Ng SY, Chaban B, Jarrell KF | title = Archaeal flagella, bacterial flagella and type IV pili: a comparison of genes and posttranslational modifications | journal = Journal of Molecular Microbiology and Biotechnology | volume = 11 | issue = 3–5 | pages = 167–91 | year = 2006 | pmid = 16983194 | doi = 10.1159/000094053 }}</ref><ref name=Metlina1>{{cite journal | vauthors = Metlina AL | title = Bacterial and archaeal flagella as prokaryotic motility organelles | journal = Biochemistry. Biokhimiia | volume = 69 | issue = 11 | pages = 1203–12 | date = November 2004 | pmid = 15627373 | doi = 10.1007/s10541-005-0065-8 }}</ref><ref name= Jarelletal>{{cite book | vauthors = Jarrell K |year=2009|chapter=Archaeal Flagella and Pili|title=Pili and Flagella: Current Research and Future Trends|publisher=Caister Academic Press|isbn= 978-1-904455-48-6}}</ref>\n*Eukaryotic flagella—those of animal, plant, and protist cells—are complex cellular projections that lash back and forth. Eukaryotic flagella are classed along with eukaryotic [[cilium#Motile cilia|motile cilia]] as [[undulipodium|undulipodia]]<ref name=db2004>[http://www.encyclopedia.com/doc/1O6-undulipodium.html A Dictionary of Biology], 2004, accessed 2011-01-01.</ref> to emphasize their distinctive wavy appendage role in cellular function or [[motility]]. [[Cilium#Primary.2Fimmotile cilium|Primary cilia]] are immotile, and are not undulipodia; they have a [[cilium#Structure.2C assembly and maintenance.2C and function|structurally different ''9+0 axoneme'']] rather than the ''9+2 axoneme'' found in both flagella and motile cilia undulipodia.\n\n===Bacterial===\n[[File:Physical model of a bacterial flagellum.jpg|thumb|right|Physical model of a bacterial flagellum]]\n\n====Structure and composition====\nThe bacterial flagellum is made up of the [[protein]] [[flagellin]]. Its shape is a 20-[[nanometer]]-thick hollow tube. It is [[helix|helical]] and has a sharp bend just outside the outer membrane; this \"hook\" allows the axis of the helix to point directly away from the cell. A shaft runs between the hook and the [[basal body]], passing through protein rings in the cell's membrane that act as bearings. [[Gram-positive]] organisms have two of these basal body rings, one in the [[peptidoglycan]] layer and one in the [[plasma membrane]]. [[Gram-negative]] organisms have four such rings: the [[L ring]] associates with the [[lipopolysaccharides]], the [[P ring]] associates with [[peptidoglycan]] layer, the M ring is embedded in the [[plasma membrane]], and the S ring is directly attached to the plasma membrane. The filament ends with a capping protein.<ref name=MacNab1>{{cite journal | vauthors = Macnab RM | title = How bacteria assemble flagella | journal = Annual Review of Microbiology | volume = 57 | issue =  | pages = 77–100 | year = 2003 | pmid = 12730325 | doi = 10.1146/annurev.micro.57.030502.090832 }}</ref><ref name=dioszeghy1>{{cite journal | vauthors = Diószeghy Z, Závodszky P, Namba K, Vonderviszt F | title = Stabilization of flagellar filaments by HAP2 capping | journal = FEBS Letters | volume = 568 | issue = 1–3 | pages = 105–9 | date = June 2004 | pmid = 15196929 | doi = 10.1016/j.febslet.2004.05.029 }}</ref>\n\nThe flagellar filament is the long, helical screw that propels the bacterium when rotated by the motor, through the hook. In most bacteria that have been studied, including the Gram-negative ''[[Escherichia coli]], [[Salmonella typhimurium]], [[Caulobacter crescentus]]'', and ''[[Vibrio alginolyticus]]'', the filament is made up of 11 protofilaments approximately parallel to the filament axis. Each protofilament is a series of tandem protein chains. However, ''[[Campylobacter jejuni]]'' has seven protofilaments.<ref name=\"SevenProtofilamentsGalkin\">{{cite journal | vauthors = Galkin VE, Yu X, Bielnicki J, Heuser J, Ewing CP, Guerry P, Egelman EH | title = Divergence of quaternary structures among bacterial flagellar filaments | journal = Science | volume = 320 | issue = 5874 | pages = 382–5 | date = April 2008 | pmid = 18420936 | doi = 10.1126/science.1155307 | bibcode = 2008Sci...320..382G }}</ref>\n\nThe basal body has several traits in common with some types of [[secretion#Secretion in Gram negative bacteria|secretory pores]], such as the hollow, rod-like \"plug\" in their centers extending out through the plasma membrane. The similarities between bacterial flagella and bacterial secretory system structures and proteins provide scientific evidence supporting the theory that bacterial flagella evolved from the [[type three secretion system|type-three secretion system]].\n\n====Motor====\n{{Further |Rotating locomotion in living systems}}\nThe bacterial flagellum is driven by a rotary engine ([[MotA|Mot complex]]) made up of protein, located at the flagellum's anchor point on the inner cell membrane. The engine is powered by [[proton motive force]], i.e., by the flow of protons (hydrogen ions) across the bacterial cell membrane due to a [[concentration gradient]] set up by the cell's metabolism (''[[Vibrio]]'' species have two kinds of flagella, lateral and polar, and some are driven by a sodium [[ion transporter|ion pump]] rather than a [[proton pump]]<ref name=\"Atsumi2\">{{cite journal | vauthors = Atsumi T, McCarter L, Imae Y | title = Polar and lateral flagellar motors of marine Vibrio are driven by different ion-motive forces | journal = Nature | volume = 355 | issue = 6356 | pages = 182–4 | date = January 1992 | pmid = 1309599 | doi = 10.1038/355182a0 | bibcode = 1992Natur.355..182A }}</ref>). The rotor transports protons across the membrane, and is turned in the process. The rotor alone can operate at 6,000 to 17,000 [[revolutions per minute|rpm]], but with the flagellar filament attached usually only reaches 200 to 1000 rpm. The direction of rotation can be changed by the [[flagellar motor switch]] almost instantaneously, caused by a slight change in the position of a protein, FliG, in the rotor.<ref>{{cite journal | last = Dean | first = Tim | name-list-format = vanc | url =  http://lifescientist.com.au/content/molecular-biology/news/inside-nature-s-most-efficient-motor-the-flagellar-1216235209 | title = Inside nature’s most efficient motor: the flagellar | journal = Australian Life Scientist | date = 2 August 2010 }}</ref> The flagellum is highly energy efficient and uses very little energy.<ref>{{cite web | first = Yoshio | last = Nagata | name-list-format = vanc | url = http://asia.nikkei.com/Tech-Science/Tech/Unlocking-the-secrets-of-nature-s-nanomotor | title = Unlocking the secrets of nature's nanomotor | work = Nikkei Asian Review | date = June 2014 }}</ref> {{Unreliable source?|reason=The article's 200-1000 rpm speed for the flagellum is much less than the 20,000 rpm represented by footnote 22, let alone the 100,000 rpms represented by footnote 21.|date=August 2015}} The exact mechanism for torque generation is still poorly understood.<ref name=\"Thierry Mora, Howard Yu, Yoshiyuki Sowa, Ned S. Wingreen\">{{cite journal | vauthors = Mora T, Yu H, Sowa Y, Wingreen NS | title = Steps in the bacterial flagellar motor | journal = PLoS Computational Biology | volume = 5 | issue = 10 | pages = e1000540 | date = October 2009 | pmid = 19851449 | doi = 10.1371/journal.pcbi.1000540 | arxiv = 0904.0438 | bibcode = 2009PLSCB...5E0540M | pmc=2759076}}</ref> Because the flagellar motor has no on-off switch, the protein epsE is used as a mechanical clutch to disengage the motor from the rotor, thus stopping the flagellum and allowing the bacterium to remain in one place.<ref>{{cite journal|last1=Whitfield|first1=John | name-list-format = vanc |title=Bacterial engines have their own clutch|url=http://www.nature.com/news/2008/080619/full/news.2008.903.html|journal=Nature News|access-date=17 May 2017|language=en|doi=10.1038/news.2008.903|date=19 June 2008}}</ref>\n\nThe cylindrical shape of flagella is suited to locomotion of microscopic organisms; these organisms operate at a low [[Reynolds number]], where the viscosity of the surrounding water is much more important than its mass or inertia.<ref name=\"isbn0-674-03116-4\">{{cite book | vauthors = Dusenbery DB |title= Living at Micro Scale: The Unexpected Physics of Being Small |edition= |publisher= Harvard University Press |location= Cambridge |year= 2009 |isbn= 0-674-03116-4 |chapter= Chapter 13}}</ref>\n\nThe rotational speed of flagella varies in response to the intensity of the proton motive force, thereby permitting certain forms of speed control, and also permitting some types of bacteria to attain remarkable speeds in proportion to their size; some achieve roughly 60 cell lengths per second. At such a speed,  a bacterium would take about 245 days to cover 1&nbsp;km; although that may seem slow, the perspective changes when the concept of scale is introduced. In comparison to macroscopic life forms, it is very fast indeed when expressed in terms of number of body lengths per second. A cheetah, for example, only achieves about 25 body lengths per second.<ref name=\"cbn\">{{cite journal |title=Motions of the running Cheetah and Horse |first=Milton |last=Hildebrand | name-list-format = vanc |date=November 1959 |journal=[[Journal of Mammalogy]] |volume=44 |issue=4 |pages=481–495 |jstor=1376265}} Although according to Cheetah, Luke Hunter and Dave Hamman, (Struik Publishers, 2003), pp. 37–38, the cheetah's fastest recorded speed was {{convert|110|km/h|abbr=on}}.</ref>\n\nThrough use of their flagella, ''[[Escherichia coli|E. coli]]'' is able to move rapidly towards attractants and away from repellents, by means of a [[biased random walk (biochemistry)|biased random walk]], with 'runs' and 'tumbles' brought about by rotating its flagellum [[counterclockwise]] and [[clockwise]], respectively. The two directions of rotation are not identical (with respect to flagellum movement) and are selected by a molecular switch.<ref>{{cite journal | vauthors = Meadows R | title = How bacteria shift gears | journal = PLoS Biology | volume = 9 | issue = 5 | pages = e1001061 | date = May 2011 | pmid = 21572986 | pmc = 3091840 | doi = 10.1371/journal.pbio.1001061 }}</ref>\n\n====Assembly====\nDuring flagellar assembly, components of the flagellum pass through the hollow cores of the basal body and the nascent filament. During assembly, protein components are added at the flagellar tip rather than at the base.<ref name=\"MinaminoReview1\">{{cite journal | vauthors = Minamino T, Imada K, Namba K | title = Mechanisms of type III protein export for bacterial flagellar assembly | journal = Molecular BioSystems | volume = 4 | issue = 11 | pages = 1105–15 | date = November 2008 | pmid = 18931786 | doi = 10.1039/b808065h }}</ref> ''In vitro'', flagellar filaments assemble spontaneously in a solution containing purified flagellin as the sole protein.<ref name=\"Asakura1\">{{cite journal | vauthors = Asakura S, Eguchi G, Iino T | title = Reconstitution of Bacterial Flagella In Vitro | journal = Journal of Molecular Biology | volume = 10 | pages = 42–56 | date = October 1964 | pmid = 14222895 | doi = 10.1016/S0022-2836(64)80026-7 }}</ref>\n\n====Evolution====\n{{Main|Evolution of flagella}}\n\nAt least 10 protein components of the bacterial flagellum share homologous proteins with the [[type three secretion system]] (TTSS),<ref name=\"ReferenceA\">{{cite journal | vauthors = Pallen MJ, Matzke NJ | title = From The Origin of Species to the origin of bacterial flagella | journal = Nature Reviews. Microbiology | volume = 4 | issue = 10 | pages = 784–90 | date = October 2006 | pmid = 16953248 | doi = 10.1038/nrmicro1493 }}</ref> hence one likely evolved from the other. Because the TTSS has a similar number of components as a flagellar apparatus (about 25 proteins), which one evolved first is difficult to determine. However, the flagellar system appears to involve more proteins overall, including various regulators and chaperones, hence it has been argued that flagella evolved from a TTSS. However, it has also been suggested<ref name=\"Saier\">{{cite journal | vauthors = Saier MH | title = Evolution of bacterial type III protein secretion systems | journal = Trends in Microbiology | volume = 12 | issue = 3 | pages = 113–5 | date = March 2004 | pmid = 15001186 | doi = 10.1016/j.tim.2004.01.003 }}</ref> that the flagellum may have evolved first or the two structures evolved in parallel. Early single-cell organisms' need for [[motility]] (mobility) support that the more mobile flagella would be selected by evolution first,<ref name=\"Saier\"/> but the TTSS evolving from the flagellum can be seen as 'reductive evolution', and receives no topological support from the [[phylogenetic]] trees.<ref name=\"Gophna2003\">{{cite journal | vauthors = Gophna U, Ron EZ, Graur D | title = Bacterial type III secretion systems are ancient and evolved by multiple horizontal-transfer events | journal = Gene | volume = 312 | issue =  | pages = 151–63 | date = July 2003 | pmid = 12909351 | doi = 10.1016/S0378-1119(03)00612-7 | url = http://linkinghub.elsevier.com/retrieve/pii/S0378111903006127 }}</ref> The hypothesis that the two structures evolved separately from a common ancestor accounts for the protein similarities between the two structures, as well as their functional diversity.<ref>{{cite journal | vauthors = McCann HC, Guttman DS | title = Evolution of the type III secretion system and its effectors in plant-microbe interactions | journal = The New Phytologist | volume = 177 | issue = 1 | pages = 33–47 | year = 2008 | pmid = 18078471 | doi = 10.1111/J.1469-8137.2007.02293.X }}</ref>\n\n====Flagella and the intelligent design debate====\n{{Main|Intelligent design|Irreducible complexity}}\nSome authors have argued that flagella cannot have evolved, assuming that they can only function properly when all proteins are in place In other words, the flagellar apparatus is \"[[irreducible complexity|irreducibly complex]]\".<ref>Behe, M. (2007) The Edge of Evolution. Free Press, New York</ref> However, many proteins can be deleted or mutated and the flagellum still works, though sometimes at reduced efficiency.<ref>{{cite journal | vauthors = Rajagopala SV, Titz B, Goll J, Parrish JR, Wohlbold K, McKevitt MT, Palzkill T, Mori H, Finley RL, Uetz P | title = The protein network of bacterial motility | journal = Molecular Systems Biology | volume = 3 | issue =  | pages = 128 | year = 2007 | pmid = 17667950 | pmc = 1943423 | doi = 10.1038/msb4100166 }}</ref> In addition, the composition of flagella is surprisingly diverse across bacteria, with many proteins only found in some species, but not others.<ref>{{cite journal | vauthors = Titz B, Rajagopala SV, Ester C, Häuser R, Uetz P | title = Novel conserved assembly factor of the bacterial flagellum | journal = Journal of Bacteriology | volume = 188 | issue = 21 | pages = 7700–6 | date = November 2006 | pmid = 16936039 | pmc = 1636259 | doi = 10.1128/JB.00820-06 }}</ref> Hence, the flagellar apparatus is clearly very flexible in evolutionary terms and perfectly able to lose or gain protein components. For instance, a number of mutations have been found that ''increase'' the motility of ''E. coli''.<ref>{{cite journal | vauthors = Kakkanat A, Phan MD, Lo AW, Beatson SA, Schembri MA | title = Novel genes associated with enhanced motility of Escherichia coli ST131 | journal = PLOS One | volume = 12 | issue = 5 | pages = e0176290 | date = 2017-05-10 | pmid = 28489862 | doi = 10.1371/journal.pone.0176290 | url = http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0176290 | bibcode = 2017PLoSO..1276290K | pmc=5425062}}</ref> Additional evidence for the evolution of bacterial flagella includes the existence of vestigial flagella, intermediate forms of flagella and patterns of similarities among flagellar protein sequences, including the observation that almost all of the core flagellar proteins have known homologies with non-flagellar proteins.<ref name=\"ReferenceA\"/> Furthermore, several processes have been identified as playing important roles in flagellar evolution, including self-assembly of simple repeating subunits, gene duplication with subsequent divergence, recruitment of elements from other systems (‘molecular bricolage’) and recombination.<ref>{{cite journal | vauthors = Pallen MJ, Gophna U | title = Bacterial flagella and Type III secretion: case studies in the evolution of complexity | journal = Genome Dynamics | volume = 3 | pages = 30–47 | year = 2007 | pmid = 18753783 | doi = 10.1159/000107602 }}</ref>\n\n====Flagellar arrangement schemes====\n[[File:flagella.svg|thumb|Examples of bacterial flagella arrangement schemes: (A) monotrichous; (B) lophotrichous; (C) amphitrichous; (D) peritrichous.]]\n\nDifferent species of bacteria have different numbers and arrangements of flagella.\n*Monotrichous bacteria have a single flagellum (e.g., ''[[Vibrio cholerae]]'').\n*Lophotrichous bacteria have multiple flagella located at the same spot on the bacterial surfaces which act in concert to drive the bacteria in a single direction. In many cases, the bases of multiple flagella are surrounded by a specialized region of the cell membrane, called the [[polar organelle]].{{Citation needed|date=January 2009}}\n*Amphitrichous bacteria have a single flagellum on each of two opposite ends (only one flagellum operates at a time, allowing the bacterium to reverse course rapidly by switching which flagellum is active).\n*Peritrichous bacteria have flagella projecting in all directions (e.g., ''E. coli'').\n\nIn certain large forms of ''[[Selenomonad|Selenomonas]]'', more than 30 individual flagella are organized outside the cell body, helically twining about each other to form a thick structure (easily visible with the light microscope) called a \"[[wikt:fascicle|fascicle]]\".\n\n[[Spirochete]]s, in contrast, have flagella arising from opposite poles of the cell, and are located within the [[periplasmic space]] as shown by breaking the outer-membrane and more recently by [[electron cryotomography]] microscopy.<ref>{{cite journal | vauthors = Izard J, Renken C, Hsieh CE, Desrosiers DC, Dunham-Ems S, La Vake C, Gebhardt LL, Limberger RJ, Cox DL, Marko M, Radolf JD | title = Cryo-electron tomography elucidates the molecular architecture of Treponema pallidum, the syphilis spirochete | journal = Journal of Bacteriology | volume = 191 | issue = 24 | pages = 7566–80 | date = December 2009 | pmid = 19820083 | pmc = 2786590 | doi = 10.1128/JB.01031-09 }}</ref><ref>{{cite journal | vauthors = Izard J, Hsieh CE, Limberger RJ, Mannella CA, Marko M | title = Native cellular architecture of Treponema denticola revealed by cryo-electron tomography | journal = Journal of Structural Biology | volume = 163 | issue = 1 | pages = 10–7 | date = July 2008 | pmid = 18468917 | pmc = 2519799 | doi = 10.1016/j.jsb.2008.03.009 }}</ref><ref>{{cite journal | vauthors = Kudryashev M, Cyrklaff M, Baumeister W, Simon MM, Wallich R, Frischknecht F | title = Comparative cryo-electron tomography of pathogenic Lyme disease spirochetes | journal = Molecular Microbiology | volume = 71 | issue = 6 | pages = 1415–34 | date = March 2009 | pmid = 19210619 | doi = 10.1111/j.1365-2958.2009.06613.x }}</ref> The rotation of the filaments relative to the cell body causes the entire bacterium to move forward in a corkscrew-like motion, even through material viscous enough to prevent the passage of normally flagellated bacteria.\n\nCounterclockwise rotation of a monotrichous polar flagellum pushes the cell forward with the flagellum trailing behind, much like a corkscrew moving inside cork. Indeed, water on the microscopic scale is highly [[viscous]], very different from our daily experience of water.\n\nFlagella are left-handed helices, and bundle and rotate together only when rotating counterclockwise. When some of the rotors reverse direction, the flagella unwind and the cell starts \"tumbling\". Even if all flagella would rotate clockwise, they likely will not form a bundle, due to geometrical, as well as hydrodynamic reasons.<ref name=\"pmid14671319\">{{cite journal | vauthors = Kim M, Bird JC, Van Parys AJ, Breuer KS, Powers TR | title = A macroscopic scale model of bacterial flagellar bundling | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 100 | issue = 26 | pages = 15481–5 | date = December 2003 | pmid = 14671319 | pmc = 307593 | doi = 10.1073/pnas.2633596100 | arxiv = cond-mat/0312562 | bibcode = 2003PNAS..10015481K }}</ref><ref name=\"pmid264676\">{{cite journal | vauthors = Macnab RM | title = Bacterial flagella rotating in bundles: a study in helical geometry | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 74 | issue = 1 | pages = 221–5 | date = January 1977 | pmid = 264676 | pmc = 393230 | doi = 10.1073/pnas.74.1.221 | bibcode = 1977PNAS...74..221M }}</ref> Such \"tumbling\" may happen occasionally, leading to the cell seemingly thrashing about in place, resulting in the reorientation of the cell. The clockwise rotation of a flagellum is suppressed by chemical compounds favorable to the cell (e.g. food), but the motor is highly adaptive to this. Therefore, when moving in a favorable direction, the concentration of the chemical attractant increases and \"tumbles\" are continually suppressed; however, when the cell's direction of motion is unfavorable (e.g., away from a chemical attractant), tumbles are no longer suppressed and occur much more often, with the chance that the cell will be thus reoriented in the correct direction.\n\nIn some ''Vibrio'' spp. (particularly ''[[Vibrio parahemolyticus]]''<ref name=Kim1>{{cite journal | vauthors = Kim YK, McCarter LL | title = Analysis of the polar flagellar gene system of Vibrio parahaemolyticus | journal = Journal of Bacteriology | volume = 182 | issue = 13 | pages = 3693–704 | date = July 2000 | pmid = 10850984 | pmc = 94540 | doi = 10.1128/JB.182.13.3693-3704.2000 }}</ref>) and related [[proteobacteria]] such as ''[[Aeromonas]]'', two flagellar systems co-exist, using different sets of genes and different ion gradients for energy. The polar flagella are constitutively expressed and provide motility in bulk fluid, while the lateral flagella are expressed when the polar flagella meet too much resistance to turn.<ref name=Atsumi1>{{cite journal | vauthors = Atsumi T, Maekawa Y, Yamada T, Kawagishi I, Imae Y, Homma M | title = Effect of viscosity on swimming by the lateral and polar flagella of Vibrio alginolyticus | journal = Journal of Bacteriology | volume = 178 | issue = 16 | pages = 5024–6 | date = August 1996 | pmid = 8759871 | pmc = 178290 | url = http://jb.asm.org/cgi/pmidlookup?view=long&pmid=8759871 }}</ref><ref name=McCarter2>{{cite journal | vauthors = McCarter LL | title = Dual flagellar systems enable motility under different circumstances | journal = Journal of Molecular Microbiology and Biotechnology | volume = 7 | issue = 1–2 | pages = 18–29 | year = 2004 | pmid = 15170400 | doi = 10.1159/000077866 }}</ref><ref name=Merino1>{{cite journal | vauthors = Merino S, Shaw JG, Tomás JM | title = Bacterial lateral flagella: an inducible flagella system | journal = FEMS Microbiology Letters | volume = 263 | issue = 2 | pages = 127–35 | date = October 2006 | pmid = 16978346 | doi = 10.1111/j.1574-6968.2006.00403.x }}</ref><ref name=Belas1>{{cite journal | vauthors = Belas R, Simon M, Silverman M | title = Regulation of lateral flagella gene transcription in Vibrio parahaemolyticus | journal = Journal of Bacteriology | volume = 167 | issue = 1 | pages = 210–8 | date = July 1986 | pmid = 3013835 | pmc = 212863 | doi =  | url = http://jb.asm.org/cgi/content/abstract/167/1/210 }}</ref><ref name=Canals1>{{cite journal | vauthors = Canals R, Altarriba M, Vilches S, Horsburgh G, Shaw JG, Tomás JM, Merino S | title = Analysis of the lateral flagellar gene system of Aeromonas hydrophila AH-3 | journal = Journal of Bacteriology | volume = 188 | issue = 3 | pages = 852–62 | date = February 2006 | pmid = 16428388 | pmc = 1347325 | doi = 10.1128/JB.188.3.852-862.2006 }}</ref><ref name=\"Canals2\">{{cite journal | vauthors = Canals R, Ramirez S, Vilches S, Horsburgh G, Shaw JG, Tomás JM, Merino S | title = Polar flagellum biogenesis in Aeromonas hydrophila | journal = Journal of Bacteriology | volume = 188 | issue = 2 | pages = 542–55 | date = January 2006 | pmid = 16385045 | pmc = 1347287 | doi = 10.1128/JB.188.2.542-555.2006 }}</ref> These provide swarming motility on surfaces or in viscous fluids.\n\n===Archaeal===\nThe [[archaellum]] possessed by some [[archea]]e is superficially similar to the bacterial flagellum; in the 1980s, they were thought to be homologous on the basis of gross morphology and behavior.<ref name=\"Cavalier-Smith \">{{cite journal | vauthors = Cavalier-Smith T | title = The origin of eukaryotic and archaebacterial cells | journal = Annals of the New York Academy of Sciences | volume = 503 | issue = 1 | pages = 17–54 | year = 1987 | pmid = 3113314 | doi = 10.1111/j.1749-6632.1987.tb40596.x | url = http://www.annalsnyas.org/cgi/content/citation/503/1/17 | bibcode = 1987NYASA.503...17C }}</ref> Both flagella and archaella consist of filaments extending outside the cell, and rotate to propel the cell. Archaeal flagella have a unique structure which lacks a central channel. Similar to bacterial type IV [[pilin]]s, the archaeal flagellins (archaellins) are made with class 3 signal peptides and they are processed by a type IV prepilin peptidase-like enzyme. The archaellins are typically modified by the addition of N-linked [[glycan]]s which are necessary for proper assembly or function.<ref name=\"JarrellK\"/>\n\nDiscoveries in the 1990s revealed numerous detailed differences between the archaeal and bacterial flagella. These include:\n\n*Bacterial flagella are motorized by a flow of [[hydron (chemistry)|H<sup>+</sup> ion]]s (or occasionally [[sodium|Na<sup>+</sup>]] ions); archaeal flagella are almost certainly powered by [[adenosine triphosphate|ATP]]. The [[torque]]-generating motor that powers rotation of the archaeal flagellum has not been identified.\n*While bacterial cells often have many flagellar filaments, each of which rotates independently, the archaeal flagellum is composed of a bundle of many filaments that rotates as a single assembly.\n*Bacterial flagella grow by the addition of flagellin subunits at the tip; archaeal flagella grow by the addition of subunits to the base.\n*Bacterial flagella are thicker than archaella, and the bacterial filament has a large enough hollow \"tube\" inside that the flagellin subunits can flow up the inside of the filament and get added at the tip; the archaellum is too thin (12-15&nbsp;nm) to allow this.<ref name=\"Archaeal flagella\">{{cite journal | vauthors = Ghosh A, Albers SV | title = Assembly and function of the archaeal flagellum | journal = Biochemical Society Transactions | volume = 39 | issue = 1 | pages = 64–9 | date = January 2011 | pmid = 21265748 | doi = 10.1042/BST0390064 }}</ref>\n*Many components of bacterial flagella share sequence similarity to components of the [[type three secretion system|type III secretion systems]], but the components of bacterial flagella and archaella share no sequence similarity. Instead, some components of archaella share sequence and morphological similarity with components of [[type IV pili]], which are assembled through the action of [[secretion#Type II secretion system|type II secretion systems]] (the nomenclature of pili and protein secretion systems is not consistent).<ref name=\"Archaeal flagella\"/>\n\nThese differences could mean that the bacterial flagella and archaella could be a classic case of biological [[analogy]], or [[convergent evolution]], rather than [[homology (biology)|homology]] <ref>{{cite journal | vauthors = Thomas NA, Bardy SL, Jarrell KF | title = The archaeal flagellum: a different kind of prokaryotic motility structure | journal = FEMS Microbiology Reviews | volume = 25 | issue = 2 | pages = 147–74 | date = April 2001 | pmid = 11250034 | doi = 10.1111/j.1574-6976.2001.tb00575.x }}</ref><ref>{{Cite web|url=https://www.uniprot.org/locations/SL-0306|title=Archaeal flagellum|website=www.uniprot.org|access-date=2019-06-24}}</ref>.However, in comparison to the decades of well-publicized study of bacterial flagella (e.g. by [[Howard Berg]]),<ref>{{cite book|last=Berg|first=Howard C. | name-list-format = vanc |title=E. coli in motion|year=2003|publisher=Springer|location=New York|isbn=9780387008882|edition=1. Aufl.}}</ref> archaella have only recently{{when|date=February 2013}} begun to garner scientific attention.{{citation needed|date=February 2013}}\n\n===Eukaryotic===\n[[File:Eukarya Flagella.svg|thumb|left|Eukaryotic flagella. 1–axoneme, 2–cell membrane, 3–IFT (IntraFlagellar Transport), 4–Basal body, 5–Cross section of flagella, 6–Triplets of microtubules of basal body]]\n\n[[File:Eukaryotic flagellum.svg|thumb|200px|Cross section of an axoneme]]\n\n[[File:Chlamydomonas TEM 09.jpg|thumb|left|Longitudinal section through the flagella area in ''[[Chlamydomonas reinhardtii]]''. In the cell apex is the basal body that is the anchoring site for a flagellum. Basal bodies originate from and have a substructure similar to that of centrioles, with nine peripheral microtubule triplets (see structure at bottom center of image).]]\n\n[[File:Chlamydomonas TEM 17.jpg|thumb|200px|The \"9+2\" structure is visible in this cross-section micrograph of axoneme.]]\n\n====Terminology====\nAiming to emphasize the distinction between the bacterial flagella and the eukaryotic cilia and flagella, some authors attempted to replace the name of these two eukaryotic structures with \"[[undulipodium|undulipodia]]\" (e.g., all papers by [[Lynn Margulis|Margulis]] since the 1970s)<ref name=\"pmid14657097\">{{cite journal | vauthors = Taylor FJ | title = The collapse of the two-kingdom system, the rise of protistology and the founding of the International Society for Evolutionary Protistology (ISEP) | journal = International Journal of Systematic and Evolutionary Microbiology | volume = 53 | issue = Pt 6 | pages = 1707–14 | date = November 2003 | pmid = 14657097 | doi = 10.1099/ijs.0.02587-0 }}</ref> or \"cilia\" for both (e.g., Hülsmann, 1992;<ref>{{cite journal | vauthors = Hülsmann N | title = Undulipodium: End of a useless discussion | journal = European Journal of Protistology | volume = 28 | issue = 3 | pages = 253–7 | date = August 1992 | pmid = 23195228 | doi = 10.1016/s0932-4739(11)80231-2 }}</ref> Adl et al., 2012;<ref name = \"Adl_2012\">{{cite journal | vauthors = Adl SM, Simpson AG, Lane CE, Lukeš J, Bass D, Bowser SS, Brown MW, Burki F, Dunthorn M, Hampl V, Heiss A, Hoppenrath M, Lara E, Le Gall L, Lynn DH, McManus H, Mitchell EA, Mozley-Stanridge SE, Parfrey LW, Pawlowski J, Rueckert S, Shadwick L, Shadwick L, Schoch CL, Smirnov A, Spiegel FW | display-authors = 6 | title = The revised classification of eukaryotes | journal = The Journal of Eukaryotic Microbiology | volume = 59 | issue = 5 | pages = 429–93 | date = September 2012 | pmid = 23020233 | pmc = 3483872 | doi = 10.1111/j.1550-7408.2012.00644.x | df = dmy }}</ref> most papers of [[Cavalier-Smith]]), preserving \"flagella\" for the bacterial structure. However, the discriminative usage of the terms \"cilia\" and \"flagella\" for eukaryotes adopted in this article is still common (e.g., Andersen et al., 1991;<ref>{{cite journal | vauthors = Andersen RA, Barr DJ, Lynn DH, Melkonian M, Moestrup Ø, Sleigh MA | year = 1991 | title = Terminology and nomenclature of the cytoskeletal elements associated with the flagellar/ciliary apparatus in protists | journal = Protoplasma | volume = 164 | issue = 1–3| pages = 1–8 | doi=10.1007/bf01320809}}</ref> Leadbeater et al., 2000).<ref>{{cite journal | veditors = Leadbeater BS, Green JC | title = The Flagellates. Unity, diversity, and evolution | location = London | publisher = Taylor and Francis }}</ref>\n\n====Internal structure====\nA eukaryotic flagellum is a bundle of nine fused pairs of [[microtubule]] doublets surrounding two central single microtubules. The so-called \"9&thinsp;+&thinsp;2\" structure is characteristic of the core of the eukaryotic flagellum called an [[axoneme]]. At the base of a eukaryotic flagellum is a [[basal body]], \"blepharoplast\" or kinetosome, which is the [[microtubule organizing center]]  for flagellar microtubules and is about 500 nanometers long. Basal bodies are structurally identical to [[centriole]]s. The flagellum is encased within the cell's [[plasma membrane]], so that the interior of the flagellum is accessible to the cell's [[cytoplasm]].\n\nBesides the axoneme and basal body, relatively constant in morphology, other internal structures of the flagellar apparatus are the transition zone (where the axoneme and basal body meet) and the root system (microtubular or fibrilar structures which extends from the basal bodies into the cytoplasm), more variable and useful as indicators of phylogenetic relationships of eukaryotes. Other structures, more uncommon, are the paraflagellar (or paraxial, paraxonemal) rod, the R fiber, and the S fiber.<ref name = \"Barsanti_2006\" />{{rp|63–84}} For surface structures, see below.\n\n====Mechanism====\nEach of the outer 9 doublet microtubules extends a pair of [[dynein]] arms (an \"inner\" and an \"outer\" arm) to the adjacent microtubule; these produce force through ATP hydrolysis. The flagellar axoneme also contains [[radial spoke]]s, polypeptide complexes extending from each of the outer nine microtubule doublets towards the central pair, with the \"head\" of the spoke facing inwards. The radial spoke is thought to be involved in the regulation of flagellar motion, although its exact function and method of action are not yet understood.\n\n====Flagella versus cilia====\n[[File:Flagellum-beating.svg|thumb|350px|Difference of beating pattern of flagellum and cilium]]\n\nThe regular beat patterns of eukaryotic [[cilia]] and flagella generate motion on a cellular level. Examples range from the propulsion of single cells such as the swimming of [[spermatozoa]] to the transport of fluid along a stationary layer of cells such as in the [[respiratory epithelium#Mucociliary Escalator|respiratory tract]]. Though eukaryotic flagella and motile cilia are ultrastructurally identical, the beating pattern of the two organelles can be different. In the case of flagella, the motion is often planar and wave-like, whereas the motile cilia often perform a more complicated three-dimensional motion with a power and recovery stroke.{{Citation needed|date=December 2008}}\n\n====Intraflagellar transport====\n[[Intraflagellar transport]], the process by which axonemal subunits, [[transmembrane receptors]], and other proteins are moved up and down the length of the flagellum, is essential for proper functioning of the flagellum, in both motility and signal transduction.<ref name=\"pmid15466257\">{{cite journal | vauthors = Pazour GJ | title = Intraflagellar transport and cilia-dependent renal disease: the ciliary hypothesis of polycystic kidney disease | journal = Journal of the American Society of Nephrology | volume = 15 | issue = 10 | pages = 2528–36 | date = October 2004 | pmid = 15466257 | doi = 10.1097/01.ASN.0000141055.57643.E0 }}</ref>\n\n====Evolution and occurrence====\n{{further|Evolution of flagella}}\n\nEukaryotic flagella or cilia, probably an ancestral characteristic,<ref>{{cite journal | vauthors = Yubuki N, Leander BS | title = Evolution of microtubule organizing centers across the tree of eukaryotes | journal = The Plant Journal | volume = 75 | issue = 2 | pages = 230–44 | date = July 2013 | pmid = 23398214 | doi = 10.1111/tpj.12145 }}</ref> are widespread in almost all groups of eukaryotes, as a relatively perennial condition, or as a flagellated life cycle stage (e.g., [[zoid]]s, [[gamete]]s, [[zoospore]]s, which may be produced continually or not).<ref>{{cite book | vauthors = Raven JA | year = 2000 | chapter = The flagellate condition | veditors = Leadbeater BS, Green JC | title = The flagellates. Unity, diversity, and evolution | volume = The Systematics Association Special Volume 59 | publisher = Taylor and Francis | location = London | pages = 269–287 }}</ref><ref name = \"Webster_Weber_2007\">{{cite book | vauthors = Webster J, Weber R | title = 2007 | chapter = Spores of Fungi | edition = 3rd | location = Cambridge | publisher = Cambridge University Press | pages = 23–24 | url = https://books.google.com/books?id=SApIn7IEnucC&lpg=PP1&hl=&pg=PT64#v=onepage&q&f=false }}</ref><ref name = \"Adl_2012\" />\n\nThe first situation is found either in specialized cells of multicellular organisms (e.g., the [[choanocyte]]s of [[sponges]], or the ciliated [[epithelia]] of [[metazoan]]s), as in [[ciliate]]s and many eukaryotes with a \"flagellate condition\" (or \"monadoid [[:de:Organisationsstufe|level of organization]]\", see [[flagellate#Flagellates as organisms: the Flagellata|Flagellata]], an artificial group).\n\nFlagellated lifecycle stages are found in many groups, e.g., many [[green algae]] (zoospores and male gametes), [[bryophyte]]s (male gametes), [[pteridophyte]]s (male gametes), some [[gymnosperm]]s ([[cycad]]s and ''[[Ginkgo]]'', as male gametes), centric [[diatom]]s (male gametes), [[brown algae]] (zoospores and gametes), [[oomycete]]s (assexual zoospores and gametes), [[hyphochytrid]]s (zoospores), [[labyrinthulomycetes]] (zoospores), some [[apicomplexan]]s (gametes), some [[radiolarian]]s (probably gametes),<ref>{{cite journal | vauthors = Lahr DJ, Parfrey LW, Mitchell EA, Katz LA, Lara E | title = The chastity of amoebae: re-evaluating evidence for sex in amoeboid organisms | journal = Proceedings. Biological Sciences | volume = 278 | issue = 1715 | pages = 2081–90 | date = July 2011 | pmid = 21429931 | pmc = 3107637 | doi = 10.1098/rspb.2011.0289 }}</ref> [[foraminiferan]]s (gametes), [[Phytomyxea|plasmodiophoromycete]]s (zoospores and gametes), [[myxogastrid]]s (zoospores), [[metazoan]]s (male gametes), and [[chytrid]] fungi (zoospores and gametes).\n\nFlagella or cilia are completely absent in some groups, probably due to a loss rather than being a primitive condition. The loss of cilia occurred in [[red algae]], some green algae ([[Zygnematophyceae]]), the [[gymnosperm]]s except cycads and ''Ginkgo'', [[angiosperm]]s, pennate [[diatom]]s, some [[apicomplexan]]s, some [[amoebozoan]]s, in the sperm of some [[metazoan]]s,<ref name = \"Austin_1995\">{{cite book | vauthors = Austin CR | date = 1995 | chapter = Evolution of human gametes: spermatozoa. | veditors = Grudzinskas JG, Yovich JL | title = Gametes: the spermatozoon | publisher = Cambridge University Press | chapter-url = https://books.google.com/books?id=UEvbQcZ7e1oC }}</ref> and in [[fungi]] (except [[chytrid]]s).\n\n====Typology====\nA number of terms related to flagella or cilia are used to characterize eukaryotes.<ref name = \"Webster_Weber_2007\" /><ref>{{cite book | vauthors = South GR, Whittick A | year = 1987 | title = Introduction to Phycology | publisher = Blackwell Scientific Publications | location = Oxford | page = 65 | url = https://books.google.com/books?id=dOaODP4Oo5kC&lpg=PP1&hl=&pg=PA65#v=onepage&q&f=false }}</ref><ref name = \"Barsanti_2006\">{{cite book | last1 = Barsanti | first1 = Laura | last2 =  Gualtieri | first2 = Paolo  | name-list-format = vanc | year = 2006 | title = Algae: Anatomy, Biochemistry, and Biotechnology | location = Florida, USA | publisher = CRC Press | url = https://books.google.com/books?id=t4ZQRWvr510C&lpg=PP1&hl=&pg=PA60#v=onepage&q&f=false }}</ref>{{rp|60–63}}<ref>{{cite book | vauthors = Dodge JD | year = 1973 | title = The Fine Structure of Algal Cells | publisher = Academic Press | location = London | pages = 57–79 | url = https://books.google.com/books?id=5e6FqXpRlv8C&pg=PA57 }}</ref><ref>{{cite book | vauthors = Lee RE | year = 2008 | title = Phycology | edition = 4th | publisher = Cambridge University Press | page = 7 | url = https://books.google.com/books?id=gfoIAFHgusgC&lpg=PA7&dq=lee%20tubular%20hairs&hl=&pg=PA7#v=onepage&q=lee%20tubular%20hairs&f=false }}</ref> According to surface structures present, flagella may be:\n*whiplash flagella (= smooth, acronematic flagella): without hairs, e.g., in [[Opisthokonta]]\n*hairy flagella (= tinsel, flimmer, pleuronematic flagella): with hairs (= [[mastigoneme]]s ''sensu lato''), divided in:\n**with fine hairs (= non tubular, or simple hairs): occurs in [[Euglenophyceae]], [[Dinoflagellata]], some [[Haptophyceae]] ([[Pavlovales]])\n**with stiff hairs (= tubular hairs, retronemes, mastigonemes ''sensu stricto''), divided in:\n***bipartite hairs: with two regions. Occurs in [[Cryptophyceae]], [[Prasinophyceae]], and some [[Heterokonta]]\n***tripartite (= straminipilous) hairs: with three regions (a base, a tubular shaft, and one or more terminal hairs). Occurs in most [[Heterokonta]]\n*stichonematic flagella: with a single row of hairs\n*pantonematic flagella: with two rows of hairs\n*acronematic: flagella with a single, terminal mastigoneme or flagellar hair (e.g., [[Bodonida|bodonid]]s);<ref>{{cite book | vauthors = Corliss JO, Lom J | year = 2002 | chapter = An annotated glossary of protozoological terms | veditors = Lee JJ, Leedale GF, Bradbury P | title = An illustrated guide to the protozoa | edition = second | pages = 1346–1385 | publisher = Society of Protozoologists | location = Lawrence }}</ref> some authors use the term as synonym of whiplash\n*with scales: e.g., [[Prasinophyceae]]\n*with spines: e.g., some [[brown algae]]\n*with undulating membrane: e.g., some [[kinetoplastid]]s, some [[parabasalid]]s\n*with proboscis (trunk-like protrusion of the cell): e.g., [[apusomonad]]s, some [[Bodonida|bodonid]]s<ref name=\"Jeuck\"/>\n\nAccording to the number of flagella, cells may be (remembering that some authors use \"ciliated\" instead of \"flagellated\":<ref name=\"Adl_2012\"/><ref>{{cite book | vauthors = Sleigh M | year = 1989 | title = Protozoa and other Protists | publisher = Edward Arnold | location = London | pages = 98–99 | url = https://books.google.com/books?id=K2Y4AAAAIAAJ&lpg=PA98&hl=&pg=PA98&redir_esc=y#v=onepage&q&f=false }}</ref>\n*uniflagellated: e.g., most [[Opisthokonta]]\n*biflagellated: e.g., all [[Dinoflagellata]], the gametes of [[Charophyceae]], of most [[bryophyte]]s and of some [[metazoan]]s<ref name = \"Austin_1995\" />\n*triflagellated: e.g., the gametes of some [[Foraminifera]]\n*quadriflagellated: e.g., some [[Prasinophyceae]], [[Collodictyonidae]]\n*octoflagellated: e.g., some [[Diplomonads|Diplomonada]], some [[Prasinophyceae]]\n*multiflagellated: e.g., [[Opalinata]], [[Ciliophora]], ''[[Stephanopogon]]'', [[Parabasalid]]a, [[Hemimastigophora]], [[Caryoblastea]], ''[[Multicilia]]'', the gametes (or [[zoid]]s) of [[Oedogoniales]] ([[Chlorophyta]]), some [[pteridophyte]]s and some [[gymnosperm]]s\n\nAccording to the place of insertion of the flagella:<ref>{{cite book | vauthors = Sparrow FK | year = 1960 | title = Aquatic phycomycetes | edition = 2nd | location = Ann Arbor | publisher = Michigan: University of Michigan Press | page = 15 | url = https://archive.org/details/aquaticphycomyce00spar }}</ref>\n*opisthokont: cells with flagella inserted posteriorlly, e.g., in [[Opisthokonta]] (Vischer, 1945). In [[Haptophyceae]], flagella are laterally to terminally inserted, but are directed posteriorly during rapid swimming.<ref>{{cite journal | vauthors = Hibberd DJ | year = 1976 | title = The ultrastructure and taxonomy of the Chrysophyceae and Prymnesiophyceae (Haptophyceae): a survey with some new observations on the ultrastructure of the Chrysophyceae | journal = Journal of the Linnean Society of London, Botany | volume = 72 | issue = | pages = 55–80 | doi=10.1111/j.1095-8339.1976.tb01352.x }}</ref>\n*akrokont: cells with flagella inserted apically\n*subakrokont: cells with flagella inserted subapically\n*pleurokont: cells with flagella inserted laterally\n\nAccording to the beating pattern:\n*gliding: a flagellum that trails on the substrate<ref name=\"Jeuck\">{{cite journal | vauthors = Jeuck A, Arndt H | title = A short guide to common heterotrophic flagellates of freshwater habitats based on the morphology of living organisms | journal = Protist | volume = 164 | issue = 6 | pages = 842–60 | date = November 2013 | pmid = 24239731 | doi = 10.1016/j.protis.2013.08.003 }}</ref>\n*heterodynamic: flagella with different beating patterns (usually with one flagellum functioning in food capture and the other functioning in gliding, anchorage, propulsion or “steering”)<ref>{{cite journal | vauthors = Sleigh MA | year = 1985 | title = Origin and evolution of flagellar movement | url = http://opensample.info/fundamental-problems-of-movement-of-cilia-eukaryotic-flagella-and-related-systems-a-seminar-held-under-the-u-s-japan-cooperative-science-program | journal = Cell Motil | volume = 5 | issue = | pages = 137–138 }}</ref>\n*isodynamic: flagella beating with the same patterns\n\nOther terms related to the flagellar type:\n*isokont: cells with flagella of equal length. It was also formerly used to refer to the [[Chlorophyta]]\n*anisokont: cells with flagella of unequal length, e.g., some [[Euglenophyceae]] and [[Prasinophyceae]]\n*heterokont: term introduced by Luther (1899) to refer to the [[Xanthophyceae]], due to the pair of flagella of unequal length. It has taken on a specific meaning in referring to cells with an anterior straminipilous flagellum (with tripartite mastigonemes, in one or two rows) and a posterior usually smooth flagellum. It is also used to refer to the taxon [[Heterokonta]]\n*stephanokont: cells with a crown of flagella near its anterior end, e.g., the gametes and spores of [[Oedogoniales]], the spores of some [[Bryopsidales]]. Term introduced by Blackman & Tansley (1902) to refer to the [[Oedogoniales]]\n*akont: cells without flagella. It was also used to refer to taxonomic groups, as Aconta or Akonta: the [[Zygnematophyceae]] and [[Bacillariophyceae]] (Oltmanns, 1904), or the [[Rhodophyceae]] (Christensen, 1962)\n\n== See also ==\n*[[Ciliopathy]]\n*[[RpoF]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n{{refbegin}}\n* {{cite journal |first=Howard C. |last=Berg | name-list-format = vanc |title=Motile Behavior of Bacteria |journal=Physics Today |volume=53 |issue=1 |pages=24–29 |date=January 2000 |doi=10.1063/1.882934 |url=http://www.physicstoday.org/resource/1/phtoad/v53/i1/p24_s1?bypassSSO=1|bibcode=2000PhT....53a..24B }}\n* {{cite web |url= http://www2.oakland.edu/biology/lindemann/ |title= Mechanisms of sperm motility | first = Charles | last = Lindemann | name-list-format = vanc |date= 2008-04-04 |publisher= Oakland University |access-date= 2008-05-18}}\n* {{cite journal |vauthors = Purcell EM |title=Life at Low Reynolds Number |journal=American Journal of Physics |volume=45 |issue=1 |pages=3–11 |year=1977 |doi=10.1119/1.10903 |url=http://www.nada.kth.se/~annak/Low_RE.pdf |format=PDF|bibcode=1977AmJPh..45....3P }}\n* {{cite web |url= http://www.talkdesign.org/faqs/flagellum.html |title= Evolution in (Brownian) space: a model for the origin of the bacterial flagellum | vauthors = Matzke NJ |date= 2003-11-10 |publisher= www.talkdesign.org}}\n{{refend}}\n\n== External links ==\n{{Commons category|Flagella}}\n\n*[http://cellimagelibrary.org/images?k=flagella&simple_search=Search Cell Image Library - Flagella]\n\n{{Cellular structures}}\n{{Plankton}}\n{{Protist structures}}\n{{Cyclopaedia 1728}}\n\n{{Authority control}}\n\n[[Category:Cell movement]]\n[[Category:Organelles]]\n[[Category:Protein complexes]]\n[[Category:Bacteria]]"
    },
    {
      "title": "Focal adhesion",
      "url": "https://en.wikipedia.org/wiki/Focal_adhesion",
      "text": "[[Image:Focaladhesiondetail.jpg|thumb|300px|right|Immunofluorescence coloration of actin (green) and the focal adhesion protein vinculin (red) in a fibroblast. Focal adhesions are visible as red dots at the end of the green bundles.]]\n\nIn [[cell biology]], '''focal adhesions''' (also '''cell–matrix adhesions''' or '''FAs''') are large [[macromolecular assemblies]] through which mechanical force and regulatory signals are transmitted between the [[extracellular matrix]] (ECM) and an interacting cell. More precisely, focal adhesions are the sub-cellular structures that mediate the regulatory effects (i.e., signaling events) of a cell in response to ECM adhesion.<ref>{{cite journal | last1 = Chen | first1 = CS | last2 = Alonso | first2 = JL | last3 = Ostuni | first3 = E | last4 = Whitesides | first4 = GM | last5 = Ingber | first5 = DE | year = 2003 | title = Cell shape provides global control of focal adhesion assembly | url = | journal = Biochemical and Biophysical Research Communications | volume = 307 | issue = 2| pages = 355–61 | doi=10.1016/s0006-291x(03)01165-3| pmid = 12859964 }}</ref>\n\nFocal adhesions serve as the mechanical linkages to the ECM, and as a biochemical signaling hub to concentrate and direct numerous signaling proteins at sites of [[integrin]] binding and clustering.  \n<!-- Deleted image removed: [[Image:cellmatadhes.png|thumb|300px|Focal Adhesions are complicated macromolecular assemblies]] -->\n\n==Structure and function==\nFocal adhesions are integrin-containing, multi-protein structures that form mechanical links between intracellular actin bundles and the extracellular substrate in many cell types. Focal adhesions are large, dynamic [[protein complexes]] through which the [[cytoskeleton]] of a cell connects to the ECM. They are limited to clearly defined ranges of the cell, at which the plasma membrane closes to within 15&nbsp;nm of the ECM substrate.<ref>{{cite journal | last1 = Zaidel-Bar | first1 = R | last2 = Cohen | first2 = M | last3 = Addadi | first3 = L | last4 = Geiger | first4 = B | year = 2004 | title = Hierarchical assembly of cell matrix adhesion complexes | url = | journal = Biochemical Society Transactions | volume = 32 | issue = 3| pages = 416–20 | doi=10.1042/bst0320416 | pmid=15157150| citeseerx = 10.1.1.624.3354 }}</ref> Focal adhesions are in a state of constant flux: proteins associate and disassociate with it continually as signals are transmitted to other parts of the cell, relating to anything from [[cell motility]] to [[cell cycle]]. Focal adhesions can contain over 100 different proteins, which suggests a considerable functional diversity.<ref>{{cite journal | last1 = Zamir | first1 = E | last2 = Geiger | first2 = B | year = 2001 | title = Molecular complexity and dynamics of cell–matrix adhesions | url = | journal = Journal of Cell Science | volume = 114 | issue = 20| pages = 3583–90 }}</ref> More than anchoring the cell, they function as signal carriers (sensors), which inform the cell about the condition of the ECM and thus affect their behavior.<ref>{{cite journal | last1 = Riveline | first1 = D | last2 = Zamir | first2 = E | last3 = Balaban | first3 = NQ | last4 = Schwarz | first4 = US | last5 = Ishizaki | first5 = T | last6 = Narumiya | first6 = S | last7 = Kam | first7 = Z | last8 = Geiger | first8 = B | last9 = Bershadsky | first9 = AD | year = 2001 | title = Focal contacts as mechanosensors: externally applied local mechanical force induces growth of focal contacts by an mDia1-dependent and ROCK-independent mechanism | url = | journal = Journal of Cell Biology | volume = 153 | issue = 6| pages = 1175–86 | doi=10.1083/jcb.153.6.1175 | pmid=11402062 | pmc=2192034}}</ref> In [[wiktionary:sessile|sessile]] cells, focal adhesions are quite stable under normal conditions, while in moving cells their stability is diminished: this is because in motile cells, focal adhesions are being constantly assembled and disassembled as the cell establishes new contacts at the leading edge, and breaks old contacts at the trailing edge of the cell. One example of their important role is in the [[immune system]], in which [[white blood cells]] migrate along the connective [[endothelium]] following cellular signals to damaged [[biological tissue]].\n\n==Morphology==\n\nConnection between focal adhesions and proteins of the [[extracellular matrix]] generally involves [[integrins]]. Integrins bind to extra-cellular proteins via short amino acid sequences, such as the [[RGD motif]] (found in proteins such as [[fibronectin]], [[laminin]], or [[vitronectin]]), or the DGEA and GFOGER motifs found in [[collagen]]. Integrins are [[heterodimers]] which are formed from one beta and one alpha subunit. These subunits are present in different forms, their corresponding ligands classify these receptors into four groups: RGD receptors, laminin receptors, leukocyte-specific receptors and collagen receptors. Within the cell, the intracellular domain of integrin binds to the cytoskeleton via adapter proteins such as [[Talin (protein)|talin]], [[Actinin, alpha 1|α-actinin]], [[filamin]], [[vinculin]] and [[tensin]]. Many other intracellular signalling proteins, such as [[focal adhesion kinase]], bind to and associate with this integrin-adapter protein–cytoskeleton complex, and this forms the basis of a focal adhesion.\n\n==Adhesion dynamics with migrating cells==\nThe dynamic assembly and disassembly of focal adhesions plays a central role in [[cell migration]]. During cell migration, both the composition and the morphology of the focal adhesion change. Initially, small (0.25μm²) focal adhesions called focal complexes (FXs) are formed at the leading edge of the cell in [[lamellipodia]]: they consist of integrin, and some of the adapter proteins, such as [[talin protein|talin]], [[paxillin]] and [[tensin]]. Many of these focal complexes fail to mature and are disassembled as the lamellipodia withdraw. However, some focal complexes mature into larger and stable focal adhesions, and recruit many more proteins such as [[zyxin]]. Recruitment of components to the focal adhesion occurs in an ordered, sequential manner.<ref>{{cite journal|last1=Zaidel-Bar|first1=R|last2=Cohen|first2=M|last3=Addadi|first3=L|last4=Geiger|first4=B|title=Hierarchical assembly of cell-matrix adhesion complexes.|journal=Biochemical Society Transactions|date=June 2004|volume=32|issue=Pt3|pages=416–20|pmid=15157150|doi=10.1042/bst0320416|citeseerx=10.1.1.624.3354}}</ref> Once in place, a focal adhesion remains stationary with respect to the extracellular matrix, and the cell uses this as an anchor on which it can push or pull itself over the ECM. As the cell progresses along its chosen path, a given focal adhesion moves closer and closer to the trailing edge of the cell. At the trailing edge of the cell the focal adhesion must be dissolved. The mechanism of this is poorly understood and is probably instigated by a variety of different methods depending on the circumstances of the cell. One possibility is that the calcium-dependent protease [[calpain]] is involved: it has been shown that the inhibition of calpain leads to the inhibition of focal adhesion-ECM separation. Focal adhesion components are amongst the known calpain substrates, and it is possible that calpain degrades these components to aid in focal adhesion disassembly<ref>{{cite journal | last1 = Huttenlocher | first1 = A | last2 = Palecek | first2 = SP | last3 = Lu | first3 = Q | last4 = Zhang | first4 = W | last5 = Mellgren | first5 = RL | last6 = Lauffenburger | first6 = DA | last7 = Ginsberg | first7 = MH | last8 = Horwitz | first8 = AF | year = 1997 | title = Regulation of cell migration by the calcium-dependent protease calpain | url = | journal = Journal of Biological Chemistry | volume = 272 | issue = 52| pages = 32719–22 | doi=10.1074/jbc.272.52.32719}}</ref>\n\n===Actin retrograde flow===\nThe assembly of nascent focal adhesions is highly dependent on the process of retrograde actin flow. This is the phenomenon in a migrating cell where actin filaments polymerize at the leading edge and flow back towards the cell body. This is the source of traction required for migration; the focal adhesion acts as a molecular clutch when it tethers to the ECM and impedes the retrograde movement of actin, thus generating the pulling (traction) force at the site of the adhesion that is necessary for the cell to move forward. This traction can be visualized with [[traction force microscopy]]. A common metaphor to explain actin retrograde flow is a large number of people being washed downriver, and as they do so, some of them hang on to rocks and branches along the bank to stop their downriver motion. Thus, a pulling force is generated onto the rock or branch that they are hanging on to. These forces are necessary for the successful assembly, growth, and maturation of focal adhesions.<ref>{{cite journal|last1=Gardel|first1=M. L.|last2=Sabass|first2=B.|last3=Ji|first3=L.|last4=Danuser|first4=G.|last5=Schwarz|first5=U. S.|last6=Waterman|first6=C. M.|title=Traction stress in focal adhesions correlates biphasically with actin retrograde flow speed|journal=The Journal of Cell Biology|date=8 December 2008|volume=183|issue=6|pages=999–1005|doi=10.1083/jcb.200810060|pmid=19075110|pmc=2600750}}</ref>\n\n===Natural biomechanical sensor===\nExtracellular mechanical forces, which are exerted through focal adhesions, can activate [[Src kinase]] and stimulate the growth of the adhesions. As well as, intracellular force generated by actomyosin contractility is sensed by focal adhesion via adhesion proteins such as talin and vinculin <ref name=\":0\">{{Cite journal|last=Kumar|first=Abhishek|last2=Ouyang|first2=Mingxing|last3=Dries|first3=Koen Van den|last4=McGhee|first4=Ewan James|last5=Tanaka|first5=Keiichiro|last6=Anderson|first6=Marie D.|last7=Groisman|first7=Alexander|last8=Goult|first8=Benjamin T.|last9=Anderson|first9=Kurt I.|date=2016-05-09|title=Talin tension sensor reveals novel features of focal adhesion force transmission and mechanosensitivity|journal=J Cell Biol|language=en|volume=213|issue=3|pages=371–383|doi=10.1083/jcb.201510012|issn=0021-9525|pmid=27161398|pmc=4862330}}</ref><ref>{{Cite journal|last=Grashoff|first=Carsten|last2=Hoffman|first2=Brenton D.|last3=Brenner|first3=Michael D.|last4=Zhou|first4=Ruobo|last5=Parsons|first5=Maddy|last6=Yang|first6=Michael T.|last7=McLean|first7=Mark A.|last8=Sligar|first8=Stephen G.|last9=Chen|first9=Christopher S.|date=2010-07-08|title=Measuring mechanical tension across vinculin reveals regulation of focal adhesion dynamics|journal=Nature|language=En|volume=466|issue=7303|pages=263–266|doi=10.1038/nature09198|pmid=20613844|issn=1476-4687|pmc=2901888}}</ref>. Decreasing actomyosin contractility, using the drug blebbistatin, leads to a decrease in talin tension as well as focal adhesion size <ref name=\":0\" />. This indicates that focal adhesions may function as mechanical sensors, and suggests that force generated from [[myosin]] fibers could contribute to maturing the focal complexes.<ref>{{cite journal | last1 = Wang | first1 = Y | last2 = Botvinick | first2 = EL | last3 = Zhao | first3 = Y | last4 = Berns | first4 = MW | last5 = Usami | first5 = S | last6 = Tsien | first6 = RY | last7 = Chien | first7 = S | year = 2005 | title = Visualizing the mechanical activation of Src | url = http://www.escholarship.org/uc/item/8k05k56b| journal = Nature | volume = 434 | issue = 7036| pages = 1040–5 | doi=10.1038/nature03469 | pmid=15846350}}</ref>\nThis gains further support from the fact that inhibition of myosin-generated forces leads to slow disassembly of focal adhesions, by changing the turnover kinetics of the focal adhesion proteins.<ref>{{cite journal | last1 = Wolfenson | first1 = H. | last2 = Bershadsky | first2 = A. | last3 = Henis | first3 = Y. I. | last4 = Geiger | first4 = B. | year = 2011 | title = Actomyosin-generated tension controls the molecular kinetics of focal adhesions | url = | journal = J Cell Sci | volume = 124 | issue = 9| pages = 1425–32 | doi=10.1242/jcs.077388| pmid = 21486952 | pmc = 3078811 }}</ref>\n\nThe relationship between forces on focal adhesions and their compositional maturation, however, remains unclear. For instance, preventing focal adhesion maturation by inhibiting myosin activity or stress fiber assembly does not prevent forces sustained by focal adhesions, nor does it prevent cells from migrating.<ref>{{Cite journal|title = Nascent Focal Adhesions Are Responsible for the Generation of Strong Propulsive Forces in Migrating Fibroblasts|journal = The Journal of Cell Biology|date = 2001-05-14|issn = 0021-9525|pmid = 11352946|pages = 881–888|volume = 153|issue = 4|doi = 10.1083/jcb.153.4.881|first = Karen A.|last = Beningo|first2 = Micah|last2 = Dembo|first3 = Irina|last3 = Kaverina|first4 = J. Victor|last4 = Small|first5 = Yu-li|last5 = Wang|pmc=2192381}}</ref><ref>{{Cite journal|title = Spatiotemporal constraints on the force-dependent growth of focal adhesions|journal = Biophysical Journal|date = 2011-06-22|issn = 1542-0086|pmc = 3123981|pmid = 21689521|pages = 2883–2893|volume = 100|issue = 12|doi = 10.1016/j.bpj.2011.05.023|first = Jonathan|last = Stricker|first2 = Yvonne|last2 = Aratyn-Schaus|first3 = Patrick W.|last3 = Oakes|first4 = Margaret L.|last4 = Gardel}}</ref> Thus force propagation through focal adhesions may not be sensed directly by cells at all time and force scales.\n\nTheir role in mechanosensing is important for [[durotaxis]].\n\n==See also==\n\n*[[Actin]]\n*[[TES (protein)]]\n*[[Paxillin]]\n\n==References==\n{{Reflist|2}}\n\n==External links==\n*[http://www.mechanobio.info/topics/signaling/go-0007160/go-0005925 MBInfo - Focal Adhesion]\n*[http://www.mechanobio.info/topics/signaling/go-0007160/go-0005925/go-0048041 MBInfo - Focal Adhesion Assembly]\n*[http://www.mechanobio.info/topics/signaling/go-0007160/go-0005925/go-0051893 MBInfo - Regulation of Focal Adhesion Assembly]\n*[http://www.adhesome.org/ AdhesomeFAnetwork] Database with all known focal adhesion proteins and their biochemical interactions\n* [http://www.unifr.ch/histologie/elearningfree/allemand/epithel/epithel05.html Intercellular Connections]\n* [http://celladhesionlab.com Zaidel-Bar Cell Adhesion Lab]\n\n{{Epithelial tissue}}\n\n[[Category:Cell biology]]\n[[Category:Cell movement]]\n[[Category:Cell signaling]]\n[[Category:Actin-based structures]]"
    },
    {
      "title": "Gliding motility",
      "url": "https://en.wikipedia.org/wiki/Gliding_motility",
      "text": "\n[[File:Gliding Motility.svg|thumb|<u>Types of Gliding Motility in Bacteria</u>\na) Type IV Pili: A cell attaches its pili to a surface or object in the direction it is traveling. The proteins in the pili are then broken down to shrink the pili pulling the cell closer to the surface or object that was it was attached to.<ref>{{Cite journal|last=Strom|first=M S|last2=Lory|first2=S|date=1993-10-01|title=Structure-Function and Biogenesis of the Type Iv Pili|journal=Annual Review of Microbiology|volume=47|issue=1|pages=565–596|doi=10.1146/annurev.mi.47.100193.003025|pmid=7903032|issn=0066-4227}}</ref>\nb) Specific Motility Membrane Proteins: Transmembrane proteins are attached to the host surface. This adhesion complex can either be specific to a certain type of surface like a certain cell type or generic for any solid surface. Motor proteins attached to an inner membrane force the movement of the internal cell structures in relation to the transmembrane proteins creating net movement.<ref>{{Cite journal|last=McBride|first=Mark J.|date=2001-10-01|title=Bacterial Gliding Motility: Multiple Mechanisms for Cell Movement over Surfaces|journal=Annual Review of Microbiology|volume=55|issue=1|pages=49–75|doi=10.1146/annurev.micro.55.1.49|pmid=11544349|issn=0066-4227}}</ref> This is driven by the proton motive force.<ref>{{Cite journal|last=Dzink-Fox|first=J. L.|last2=Leadbetter|first2=E. R.|last3=Godchaux|first3=W.|date=December 1997|title=Acetate acts as a protonophore and differentially affects bead movement and cell migration of the gliding bacterium Cytophaga johnsonae (Flavobacterium johnsoniae)|journal=Microbiology|volume=143|pages=3693–3701|doi=10.1099/00221287-143-12-3693|issn=1350-0872|pmid=9421895|issue=12}}</ref> The proteins involved differ between species. An example of a bacterium that uses this mechanism would be ''Flavobacterium''. This mechanism is still being studied and is not well understood.<ref>{{Cite journal|last=Braun|first=Timothy F.|last2=Khubbar|first2=Manjeet K.|last3=Saffarini|first3=Daad A.|last4=McBride|first4=Mark J.|date=September 2005|title=Flavobacterium johnsoniae Gliding Motility Genes Identified by mariner Mutagenesis|journal=Journal of Bacteriology|volume=187|issue=20|pages=6943–6952|doi=10.1128/JB.187.20.6943-6952.2005|issn=0021-9193|pmc=1251627|pmid=16199564|via=}}</ref>\nc) Polysaccharide Jet: The cell releases a 'jet' of polysaccharide material behind it propelling it forward. This polysaccharide material is left behind.<ref>{{Cite journal|last=Hoiczyk|first=E.|last2=Baumeister|first2=W.|date=1998-10-22|title=The junctional pore complex, a prokaryotic secretion organelle, is the molecular motor underlying gliding motility in cyanobacteria|journal=Current Biology|volume=8|issue=21|pages=1161–1168|issn=0960-9822|pmid=9799733|doi=10.1016/s0960-9822(07)00487-3}}</ref>\n]]\n\n'''Gliding motility''' is a type of translocation used by [[microorganism]]s that is independent of propulsive structures such as [[flagellum|flagella]], [[pilus|pili]], and [[Fimbria (bacteriology)|fimbriae]].<ref name=\":0\">{{cite journal|last1=Nan|first1=Beiyan|title=Bacterial Gliding Motility: Rolling Out a Consensus Model|journal=Current Biology|date=February 2017|volume=27|issue=4|pages=R154–R156|doi=10.1016/j.cub.2016.12.035|pmid=28222296}}</ref> Gliding allows microorganisms to travel along the surface of low aqueous films. The mechanisms of this motility are only partially known.\n\n[[Twitching motility]] also allows microorganisms to travel along a surface, but this type of movement is jerky and uses [[Pilus#Type IV pili|pili]] as its means of transport. Bacterial gliding is a type of gliding motility that can also use pili for propulsion. \n\nThe speed of gliding varies between organisms, and the reversal of direction is seemingly regulated by some sort of internal clock.<ref name=\":1\">{{cite journal|last1=Nan|first1=Beiyan|last2=McBride|first2=Mark J.|last3=Chen|first3=Jing|last4=Zusman|first4=David R.|last5=Oster|first5=George|date=February 2014|title=Bacteria that Glide with Helical Tracks|journal=Current Biology|volume=24|issue=4|pages=169–174|doi=10.1016/j.cub.2013.12.034|pmid=24556443|pmc=3964879}}</ref>\n\nOther microorganisms use gliding motility, with some for example the [[apicomplexan]]s able to travel at fast rates between 1–10 μm/s. In contrast the ''[[Myxococcus xanthus]]'' bacteria glide at a rate of 5 μm/min.<ref>{{Cite journal|last=Sibley|first=L.David|last2=Håkansson|first2=Sebastian|last3=Carruthers|first3=Vern B|date=1998-01-01|title=Gliding motility: An efficient mechanism for cell penetration|journal=Current Biology|volume=8|issue=1|pages=R12–R14|doi=10.1016/S0960-9822(98)70008-9}}</ref><ref name=\":1b\">{{cite journal|last1=Sibley|first1=LDI|date=Oct 2010|title=How apicomplexan parasites move in and out of cells|journal=Curr Opin Biotechnol|volume=21|issue=5|pages=592–8|doi=10.1016/j.copbio.2010.05.009|pmc=2947570|pmid=20580218}}<!--|accessdate=May 2015--></ref>\n\nCell-invasion and gliding motility have TRAP ([[thrombospondin]]-related anonymous protein), a surface protein, as a common molecular basis that is both essential for infection and locomotion of the invasive apicomplexan parasite.<ref>{{Cite journal|last=Sultan|first=Ali A|last2=Thathy|first2=Vandana|last3=Frevert|first3=Ute|last4=Robson|first4=Kathryn J.H|last5=Crisanti|first5=Andrea|last6=Nussenzweig|first6=Victor|last7=Nussenzweig|first7=Ruth S|last8=Ménard|first8=Robert|title=TRAP Is Necessary for Gliding Motility and Infectivity of Plasmodium Sporozoites|journal=Cell|volume=90|issue=3|pages=511–522|doi=10.1016/s0092-8674(00)80511-5|year=1997}}</ref> [[Microneme]]s are secretory organelles on the apical surface of the apicomplexans used for gliding motility.\n\n==Types of motility==\n'''Bacterial gliding''' is a process of motility whereby a [[bacterium]] can move under its own power.   Generally, the process occurs whereby the bacterium moves along a surface in the general direction of its long axis.<ref name=\":2\">{{Cite journal|last=Spormann|first=Alfred M.|date=September 1999|title=Gliding Motility in Bacteria: Insights from Studies of Myxococcus xanthus|pmc=103748|journal=Microbiology and Molecular Biology Reviews|volume=63|issue=3|pages=621–641|issn=1092-2172|pmid=10477310}}</ref> Gliding may occur via distinctly different mechanisms, depending on the type of bacterium. This type of movement has been observed in phylogenetically diverse bacteria<ref name=\"McBride2\">{{Cite journal|last1=McBride|first1=M. .|year=2001|title=Bacterial gliding motility: multiple mechanisms for cell movement over surfaces|journal=Annual Review of Microbiology|volume=55|pages=49–75|doi=10.1146/annurev.micro.55.1.49|pmid=11544349}}</ref> such as [[cyanobacteria]], [[myxobacteria]], [[cytophaga]], [[flavobacteria]], and [[mycoplasma]].\n\nBacteria move in response to varying climates, water content, presence of other organisms, and firmness of surfaces or media. Gliding has been observed in a wide variety of phyla, and though the mechanisms may vary between bacteria, it is currently understood that it takes place in environments with common characteristics, such as firmness and low-water, which enables the bacterium to still have motility in its surroundings. Such environments with low-water content include [[biofilm]]s, [[soil]] or [[Humus#Benefits of soil organic matter and humus|soil crumbs]] in [[tilth]], and [[microbial mat]]s.<ref name=\":2\" />\n\n===Purpose===\nGliding, as a form of motility, appears to allow for interactions between bacteria, [[pathogenesis]], and increased social behaviours. It may play an important role in [[biofilm]] formation, bacterial [[virulence]], and [[chemoreceptor|chemosensing]].<ref name=\"Science12\">{{Cite journal|last1=Mignot|first1=T.|last2=Shaevitz|first2=J.|last3=Hartzell|first3=P.|last4=Zusman|first4=D.|year=2007|title=Evidence that focal adhesion complexes power bacterial gliding motility|journal=Science|volume=315|issue=5813|pages=853–856|bibcode=2007Sci...315..853M|doi=10.1126/science.1137223|pmid=17289998|pmc=4095873}}</ref>\n\n===Swarming motility===\n[[Swarming motility]] occurs on softer semi-solid and solid surfaces (which usually involves movement of a bacterial population in a coordinated fashion via [[quorum sensing]], using flagella to propel them), or [[twitching motility]]<ref name=\"McBride2\" /> on solid surfaces (which involves extension and retraction of [[Pilus#Type IV pili|type IV pili]] to drag the bacterium forward).<ref>{{Cite journal|last=Nan|first=Beiyan|last2=Zusman|first2=David R.|date=July 2016|title=Novel mechanisms power bacterial gliding motility|journal=Molecular Microbiology|volume=101|issue=2|pages=186–193|doi=10.1111/mmi.13389|issn=1365-2958|pmc=5008027|pmid=27028358}}</ref>\n\n==Proposed mechanisms==\nThe mechanism of gliding might differ between species. Examples of such mechanisms include:  \n\n*Motor proteins found within the inner membrane of the bacteria utilize a proton-conducting channel to transduce a mechanical force to the cell surface.<ref name=\":0\" />  The movement of the [[Cytoskeleton|cytoskeletal filaments]] causes a mechanical force which travels to the adhesion complexes on the substrate to move the cell forward.<ref name=\":3b\">{{Cite journal|last=Sun|first=Mingzhai |last2=Wartel|first2=Morgane |last3=Cascales|first3=Eric|last4=Shaevitz|first4=Joshua W.|last5=Mignot|first5=Tâm|date=2011-05-03|title=Motor-driven intracellular transport powers bacterial gliding motility|journal=Proceedings of the National Academy of Sciences|language=en|volume=108|issue=18|pages=7559–7564|doi=10.1073/pnas.1101101108|issn=0027-8424|pmid=21482768|pmc=3088616}}</ref> Motor and regulatory proteins that convert intracellular motion into mechanical forces like traction force have been discovered to be a conserved class of intracellular motors in bacteria that have been adapted to produce cell motility.<ref name=\":3b\" />\n*A-motility (adventurous motility)<ref name=\":2\" /><ref name=\"Science12\" /><ref name=\"J o B\">{{cite journal|last1=Sliusarenko|first1=O.|last2=Zusman|first2=D. R.|last3=Oster|first3=G.|title=The Motors Powering A-Motility in Myxococcus xanthus Are Distributed along the Cell Body|journal=Journal of Bacteriology|date=17 August 2007|volume=189|issue=21|pages=7920–7921|doi=10.1128/JB.00923-07|pmid=17704221|pmc=2168729}}</ref> as a proposed type of gliding motility, involving transient adhesion complexes fixed to the substrate while the organism moves forward.<ref name=\"Science12\" /> For example, in ''[[Myxococcus xanthus]]'',<ref name=\":2\" /><ref name=\"McBride2\" /><ref name=\"Science12\" /><ref name=\":3\">{{Cite journal|last=Luciano|first=Jennifer|last2=Agrebi|first2=Rym|last3=Gall|first3=Anne Valérie Le|last4=Wartel|first4=Morgane|last5=Fiegna|first5=Francesca|last6=Ducret|first6=Adrien|last7=Brochier-Armanet|first7=Céline|last8=Mignot|first8=Tâm|date=2011-09-08|title=Emergence and Modular Evolution of a Novel Motility Machinery in Bacteria|journal=PLOS Genetics|volume=7|issue=9|pages=e1002268|doi=10.1371/journal.pgen.1002268|pmid=21931562|pmc=3169522|issn=1553-7404}}</ref> a social bacterium.\n*ejection or secretion of a [[polysaccharide]] slime from nozzles at either end of the cell body.<ref name=\":0b\">{{Citation|title=Bacteria use slime jets to get around|date=3 April 2006|url=https://www.newscientist.com/article/dn8933-bacteria-use-slime-jets-to-get-around.html|last1=Merali|first1=Zeeya|newspaper=New Scientist|accessdate=17 January 2010}}</ref>\n*energized nano-machinery or large macromolecular assemblies located on the bacterium's cell body,<ref name=\":3b\" /> \n*\"[[focal adhesion]] complexes\" and \"treadmilling\" of surface adhesins distributed along the cell body.<ref name=\"Science12\" /><ref name=\":1\" />\n*the gliding motility of ''[[Flavobacterium johnsoniae]]'' uses a helical track superficially similar to ''M. xanthus'', but via a different mechanism. Here the adhesin SprB is propelled along the cell surface (spiraling from pole to pole), pulling the bacterium along 25 times faster than ''M. xanthus''.<ref>{{Cite journal| doi = 10.1016/j.cub.2013.12.034| pmid = 24556443| year = 2015| last1 = Nan | first = Beiyan | title = Bacteria that glide with helical tracks | volume = 24 | issue = 4| pages = R169–173 | journal = Curr Biol | pmc=3964879}}</ref> ''[[Flavobacterium johnsoniae]]'' move via a screw-like mechanism and are powered by a proton motive force.<ref>{{Cite journal|  doi = 10.1016/j.bpj.2016.07.043| pmid = 27602728| year = 2016| last1 = Shrivastava | first = Abhishek | title = The Screw-Like Movement of a Gliding Bacterium Is Powered by Spiral Motion of Cell-Surface Adhesins | volume = 111 | issue = 5| pages = 1008–13 | journal = Biophys. J. | pmc=5018149}}</ref>\n\n==See also==\n* [[Extracellular polymeric substance]]\n* [[Microneme]]\n* [[Mucilage]]\n\n==References==\n{{reflist}}\n\n[[Category:Cell movement]]"
    },
    {
      "title": "Haptotaxis",
      "url": "https://en.wikipedia.org/wiki/Haptotaxis",
      "text": "'''Haptotaxis''' (from [[Greek language|Greek]] ἅπτω (hapto, \"touch, fasten\") and  τάξις (taxis, \"arrangement, order\")) is the directional [[motility]] or outgrowth of cells, e.g. in the case of [[axonal]] outgrowth, usually up a [[gradient]] of cellular adhesion sites or substrate-bound chemoattractants (the gradient of the chemoattractant being expressed or bound on a surface, in contrast to the classical model of [[chemotaxis]], in which the gradient develops in a soluble fluid.). These gradients are naturally present in the [[extracellular matrix]] (ECM) of the body during processes such as angiogenesis or artificially present in [[biomaterials]] where gradients are established by altering the concentration of adhesion sites on a [[polymer]] substrate.<ref>{{cite journal |vauthors=McCarthy JB, Palm SL, Furcht LT | title= Migration by haptotaxis of a Schwann cell tumor line to the basement membrane glycoprotein laminin | journal= J Cell Biol | year= 1983| volume=97| pages= 772–7| doi= 10.1083/jcb.97.3.772| pmid= 6885918 | issue= 3 | pmc= 2112555}}</ref><ref>{{cite journal |author1=Cattaruzza S |author2=Perris R. | title= Proteoglycan control of cell movement during wound healing and cancer spreading| journal= Matrix Biol | year= 2005| volume=24| pages= 400–17 | doi= 10.1016/j.matbio.2005.06.005 | pmid= 16055321 | issue= 6}}</ref>\n\n== Clinical Significance. ==\nHaptotaxis plays a major role in the efficient healing of wounds.<ref name=\"iovs.org\">{{cite journal|last1=Blanco-Mezquita|first1=Jose|last2=Hutcheon|first2=Audrey E.K|last3=Zieske|first3=James D.|title=Role of Thrombospondin-1 in Repair of Penetrating Corneal Wounds|journal=Invest Ophthalmology|date=January 28, 2013|pages=6262–6268|doi=10.1167/iovs.13-11710|pmid=23963165|url=http://www.iovs.org/content/54/9/6262.full.pdf+html|ref=52|volume=54|issue=9|pmc=3776713}}</ref><ref>{{cite journal|last1=Basan|first1=Markus|last2=Elgeti|first2=Jens|last3=Hannezo|first3=Edouardo|last4=Rappel|first4=Wouter-Jan|last5=Levine|first5=Herbert|title=lignment of cellular motility forces with tissue flow as a mechanism for efficient wound healing|journal=Proceedings of the National Academy of Sciences of the United States of America|volume=110|date=2012-09-09 |issue=PNAS 2013 110:2452–2459|pages=2452–2459|ref=38|doi=10.1073/pnas.1219937110|pmid=23345440|pmc=3574962}}</ref>  For example, when corneal integrity is compromised, [[epithelial cells]] quickly cover the damaged area by proliferation and migration (haptotaxis). In the [[corneal stroma]], [[keratocytes]] within the wounded area undergo [[apoptosis]], leaving the stroma devoid of cells that must be replaced. [[Keratocytes]] surrounding the wounded area proliferate and become [[fibroblasts]] that migrate to fill the wounded area.  This creates a healthy environment with [[myofibroblasts]] and extracellular matrix. This is known as light backscattering or subepithial haze.<ref name=\"iovs.org\"/>\nWhen there is injury to an [[epithelial cell]] heptotaxis occurs, which is highly influenced by the cell's velocity, which is in turn influenced by direction of cell [[motility]]. Cells migrate easily and quickly in packs{{Citation needed|date=March 2015}}, so when one cell moves the rest follow in response to the gradient and initial cell movement. Mechanical effects like the buildup of tensile forces may play an important role for both division as well as [[motility]] of cells in tissue.<ref>{{cite journal|last1=Basan|first1=Markus|last2=Elgeti|first2=Jens|last3=Hannezo|first3=Edouardo|last4=Rappel|first4=Wouter-Jan|last5=Levine|first5=Herbert|title=lignment of cellular motility forces with tissue flow as a mechanism for efficient wound healing|journal=Proceedings of the National Academy of Sciences of the United States of America|volume=110|date=2012-09-09|issue=PNAS 2013 110:2452–2459|pages=2452–2459|ref=38|doi=10.1073/pnas.1219937110|pmid=23345440|pmc=3574962}}</ref>\n\n==Methods of Study==\nAs defined above, haptotaxis is the motility of cells up a gradient of substrate bound molecules. There is a wide variety of procedures to set up this gradient ''in vitro'' for the study of haptotaxis. The two main categories can be classified into either continuous or digital.<ref name=\"one\">[ Ricoult, S. G., Kennedy, T. E., & Juncker, D. (2015). Substrate-bound protein gradients to study haptotaxis. Frontiers in Bioengineering and Biotechnology, 3]</ref> Both types are relatively easy to produce, but digital gradients give more accurate concentration calculations. Overall, the methods in use currently can be improved to mirror the ''in vivo'' environment more, as the resolution of the gradients is not as sharp ''in vitro'' as they are ''in vivo''. Also, biological gradients have the ability to change geometry, which current models ''in vitro'' cannot mimic.<ref name=\"one\" /> These gradients are useful in gaining understanding of the basics of haptotaxis, but because of the complex and fluid nature of these gradients, a deeper understanding of the ''in vivo'' condition is difficult to ascertain.\n\n==Tumor Cells and Haptotaxis==\nA characteristic of many cancers is the ability to move throughout the body. These are [[malignant cells]], and pose a serious threat to the health of an individual. It has been indicated that haptotaxis plays a role in the ability of malignant cells to [[metastasize]].   One factor that was initially found to influence haptotaxis is serum spreading factor, which is present in blood serum and interstitial tissues.<ref name=\"two\">{{cite journal | last1 = Hayman | first1 = E. G. | last2 = Pierschbacher | first2 = M. D. | last3 = Ohgren | first3 = Y. | last4 = Ruoslahti | first4 = E. | year = 1983 | title = Serum spreading factor (vitronectin) is present at the cell surface and in tissues | url = | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 80 | issue = 13| pages = 4003–4007 | doi=10.1073/pnas.80.13.4003| pmid = 6191326 | pmc = 394188 }}</ref> The presence of serum spreading factor was shown to influence directed migration along a gradient of substrate molecules in a few types of cancer cells.<ref name=\"three\">{{cite journal | last1 = Basara | first1 = M. L. | last2 = McCarthy | first2 = J. B. | last3 = Barnes | first3 = D. W. | last4 = Furcht | first4 = L. T. | year = 1985 | title = Stimulation of haptotaxis and migration of tumor cells by serum spreading factor | url = | journal = Cancer Research | volume = 45 | issue = 6| pages = 2487–2494 }}</ref>  Another component important in the haptotaxis of tumor cells is MenaINV, which is an [[actin]] regulatory protein that becomes increasingly expressed in tumor cells. This actin regulatory protein binds to [[fibronectin]] receptors and aids in the haptotactic and chemotactic processes of tumor cells.<ref name=\"four\">{{cite journal | last1 = Oudin | first1 = M. J. | last2 = Jonas | first2 = O. | last3 = Kosciuk | first3 = T. | last4 = Broye | first4 = L. C. | last5 = Wyckoff | first5 = J. | last6 = Miller | first6 = M. A. | display-authors = etal  | year = 2015 | title = Mena at the nexus of chemotaxis and haptotaxis during tumor progression | url = | journal = Cancer Research | volume = 75 | issue = 15 Supplement | pages = 437 | doi=10.1158/1538-7445.am2015-437}}</ref>\n\n==Pathology==\nHaptotaxis plays a role in several kinds of diseases where the movement or aggregation of cells causes the symptoms. As mentioned before, cancers that are metastatic have the ability to perform haptotaxis in order to spread throughout the body. This ability is not limited to tumor cells. [[Idiopathic pulmonary fibrosis]] (IPF) is a disease marked by fibrosis in lung [[mesothelial]] cells. TGF-β1 is a cytokine found in higher concentrations of lungs from patients who have IPF, and induces haptotaxis of pleural mesothelial cells. At the same time, TGF-β1 causes the mesothelial cells to develop into [[myofibroblasts]], which contribute to the symptoms in IPF.<ref name=\"five\">{{cite journal | last1 = Nasreen | first1 = N. | last2 = Mohammed | first2 = K. A. | last3 = Mubarak | first3 = K. K. | last4 = Baz | first4 = M. A. | last5 = Akindipe | first5 = O. A. | last6 = Fernandez-Bussy | first6 = S. | last7 = Antony | first7 = V. B. | year = 2009 | title = Pleural mesothelial cell transformation into myofibroblasts and haptotactic migration in response to TGF-β1 in vitro | url = | journal = American Journal of Physiology. Lung Cellular and Molecular Physiology | volume = 297 | issue = 1| pages = L115–L124 | doi=10.1152/ajplung.90587.2008| pmid = 19411308 | pmc = 2711818 }}</ref> The result is that there becomes an aggregation of myofibroblasts in the lungs, which leads to fibrosis of the mesothelial cells. During [[nephritis]], VCAM-1 is expressed in higher levels on the tubules of nephrons, which leads to increased leukocyte migration via the gradient established by VCAM-1.<ref name=\"seven\" /> It is important to note that this increased expression was not found on the capillary endothelial cells. This migration of leukocytes leads to inflammation and tissue destruction characteristic of an [[inflammatory response]].\n\n==Immune System==\nMovement of cells is vital for the function of the [[immune system]], and especially for antigen presenting cells. [[Dendritic cells]] (one of the main [[antigen presenting cells]] in the immune system), move towards the lymph nodes after phagocytizing an antigen in order to present the antigen to [[T cells]]. Chemokines influence these movements, especially CCL21, which is bound to lymphatic endothelial cell membranes. The influence is short range, but causes movement of the dendritic cells up a fixed chemical gradient.<ref name=\"six\">[ Weber, M., Hauschild, R., Schwarz, J., Moussion, C., de Vries, I., Legler, D. F., et al. (2013). Interstitial dendritic cell guidance by haptotactic chemokine gradients. Science, 339(6117), 328-332. doi:10.1126/science.1228456 [doi]</ref> Other [[leukocytes]] also exhibit haptotactic movement: neutrophils undergo IL-8 mediated migration, while [[monocytes]], [[basophils]], [[eosinophils]] and some T cells are influenced by RANTES chemokines.<ref name=\"seven\">[ Dal Canton, A. (1995). Adhesion molecules in renal disease. Kidney International, 48, 1687-1696.]</ref> In the autoimmune disorder [[rheumatoid arthritis]] and in [[osteoarthritis]], the associated swelling and migration of neutrophils to the affected site has been shown to be linked to membrane bound midkine cytokine. This cytokine operates in a haptotactic fashion, attracting local neutrophils to the site of expression.<ref name=\"eight\">{{cite journal | last1 = Takada | first1 = T. | last2 = Toriyama | first2 = K. | last3 = Muramatsu | first3 = H. | last4 = Song | first4 = X. J. | last5 = Torii | first5 = S. | last6 = Muramatsu | first6 = T. | year = 1997 | title = Midkine, a retinoic acid-inducible heparin-binding cytokine in inflammatory responses: chemotactic activity to neutrophils and association with inflammatory synovitis | url = | journal = Journal of Biochemistry | volume = 122 | issue = 2| pages = 453–458 | doi=10.1093/oxfordjournals.jbchem.a021773}}</ref>\n\n==Tissue Development==\nHaptotaxis plays a role in organizing cells to form tissues and specific regions of those tissues. [[Fibronectin]] and [[laminin]] both play a role in adrenocyte mutation into distinctive distribution in the adrenal gland.<ref name=\"nine\">[ Feige, J. J., Keramidas, M., & Chambaz, E. M. (1997). Hormonally regulated components of the adrenocortical cell environment and the control of adrenal cortex homeostasis. Hormone and metabolic research= Hormon-und Stoffwechselforschung= Hormones et metabolisme, 30(6-7), 421-425.]</ref> The adrenocytes migrate centripetally as they mature towards the medulla of the adrenal gland.,<ref name=\"ten\">{{cite journal | last1 = Zajicek | first1 = G. | last2 = Ariel | first2 = I. | last3 = Arber | first3 = N. | year = 1986 | title = The streaming adrenal cortex: direct evidence of centripetal migration of adrenocytes by estimation of cell turnover rate | url = | journal = Journal of Endocrinology | volume = 111 | issue = 3| pages = 477–482 | doi=10.1677/joe.0.1110477}}</ref> and this movement may be a result of haptotactic forces mediated by fibronectin and laminin.<ref name=\"nine\" /> In nerve cells, axonal growth is mediated by nerve growth factor in a haptotactic manner, where the axon of nerve cells grows along the gradient.<ref name= \"eleven\">{{cite journal | last1 = Taniuchi | first1 = M. | last2 = Clark | first2 = H. B. | last3 = Johnson | first3 = E. M. | year = 1986 | title = Induction of nerve growth factor receptor in Schwann cells after axotomy | url = | journal = Proceedings of the National Academy of Sciences | volume = 83 | issue = 11| pages = 4094–4098 | doi=10.1073/pnas.83.11.4094}}</ref> This information could be used to possibly develop methods to promote nerve regeneration in patients that have nerve damage. \nAnother regenerative strategy is the use of [[mesenchymal stem cells]], which can differentiate into different kinds of connective tissue in the wound healing process.<ref name= \"twelve\">[ Thibault, M. M., Hoemann, C. D., & Buschmann, M. D. (2007). Fibronectin, vitronectin, and collagen I induce chemotaxis and haptotaxis of human and rabbit mesenchymal stem cells in a standardized transmembrane assay. Stem cells and development, 16(3), 489-502.]</ref> The haptotaxis is mediated by fibronectin, vitronectin, and type I collagen. A recent study has tentatively proposed the idea that the structures on cells responsible for sensing the membrane protein gradients are attenuated [[filopodia]].<ref name= \"thirteen\">[Amarachintha, S. P., Ryan, K. J., Cayer, M., Boudreau, N. S., Johnson, N. M., & Heckman, C. A. (2015). Effect of Cdc42 domains on filopodia sensing, cell orientation, and haptotaxis. Cellular signalling, 27(3), 683-693.]</ref> Also, the more amount of filopodia present on the leading edge of the migrating cell, the more responsive the cell is to the haptotactic gradient. This is important because there is the possibility that all motile cells that display filopodia may be responding to haptotactic gradients. Further research is required in the subject, but it is clear that more and more kinds of cell undergo haptotaxis than originally believed.\n\n==Therapeutic Uses==\nThe placement of haptotactic molecules would benefit most in situations where increased numbers of cells are required to move to a desired location to help the healing process either directly or by their cell products. The introduction of haptotactic peptides may help in healing several diseases such as [[diabetes mellitus]], [[hemophilia]] A and B deficienceies, and [[Parkinson’s disease]]. The haptoctatic molecules would play a role in healing by restricting other bioengineered cells that have the ability to produce the needed cell products to the desired area of the body where therapy is needed.<ref name=\"fourteen\">[ Gorodetsky, R., & Marx, G. (2006). U.S. Patent Application 11/490,033.]</ref> This application can also be used in wound healing, where increased numbers of  fibroblasts and keratinocytes aid in wound re-granularization, thus decreasing overall healing time.<ref name=\"fourteen\" /> In regard to [[prosthetics]], making the prosthetic device incorporate successfully with the tissue is a challenge. When the prosthetic’s surface is coated with haptotactic materials, the prosthetic is aided in forming covalent bonds with the cells and becomes securely attached to the cell layer.<ref name= \"fifteen\">[ Gorodetsky, R. (2013). U.S. Patent No. 8,354,111. Washington, DC: U.S. Patent and Trademark Office.]</ref> While haptotaxis may not be occurring in this process, it demonstrates the diversity with which this knowledge about haptotaxis can be used.\n\n==See also==\n* [[Chemotaxis]]\n* [[Durotaxis]]\n* [[Mechanotaxis]]\n* [[Plithotaxis]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [https://web.archive.org/web/20060902072639/http://ctelab.berkeley.edu/research/migration.htm \"Cellular Migration\"] - University of California, Berkeley, 2003.  Cell and Tissue Engineering website.\n\n[[Category:Cell movement]]"
    }
  ]
}