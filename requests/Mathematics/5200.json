{
  "pages": [
    {
      "title": "Two's complement",
      "url": "https://en.wikipedia.org/wiki/Two%27s_complement",
      "text": "{{cleanup|date=June 2019|reason=See [[Talk:Two's complement]]}}\n'''Two's complement''' is a [[mathematical operation]] on [[binary number]]s, and is an example of a [[Method of complements|radix complement]]. It is used in [[computing]] as a method of [[signed number representation]].\n\nThe two's complement of an {{mvar|N}}-bit number is defined as its [[method of complements|complement]] with respect to {{math|2<sup>''N''</sup>}}. For instance, for the three-bit number 010, the two's complement is 110, because {{math|010 + 110 {{=}} 1000}}. The two's complement is calculated by inverting the digits and adding one.\n\n{|class=\"wikitable sortable floatright\" style=\"text-align: center;\"\n|+ Three-bit signed integers\n! Decimal<br />value\n! Binary<br /><small>(two's-complement</small><br /><small>representation)</small>\n! Two's<br />complement<br /><small>{{nowrap|(2<sup>3</sup> &minus; ''n'')<sub>2</sub>}}</small>\n|-\n| 0\n| 000\n| 000\n|-\n| 1\n| 001\n| 111\n|-\n| 2\n| 010\n| 110\n|-\n| 3\n| 011\n| 101\n|-\n| −4\n| 100\n| 100\n|-\n| −3\n| 101\n| 011\n|-\n| −2\n| 110\n| 010\n|-\n| −1\n| 111\n| 001\n|}\n\n{|class=\"wikitable sortable floatright\" style=\"text-align: center;\"\n|+ Eight-bit signed integers\n! Decimal<br />value\n! Binary<br /><small>(two's-complement</small><br /><small>representation)</small>\n! Two's<br />complement<br /><small>{{nowrap|(2<sup>8</sup> &minus; ''n'')<sub>2</sub>}}</small>\n|-\n| 0\n| 0000&nbsp;0000\n| 0000&nbsp;0000\n|-\n| 1\n| 0000 0001\n| 1111 1111\n|-\n| 2\n| 0000 0010\n| 1111 1110\n|-\n| 126\n| 0111 1110\n| 1000 0010\n|-\n| 127\n| 0111 1111\n| 1000 0001\n|-\n| −128\n| 1000 0000\n| 1000 0000\n|-\n| −127\n| 1000 0001\n| 0111 1111\n|-\n| −126\n| 1000 0010\n| 0111 1110\n|-\n| −2\n| 1111 1110\n| 0000 0010\n|-\n| −1\n| 1111 1111\n| 0000 0001\n|}\n\nTwo's complement is the most common method of representing signed [[Integer (computer science)|integers]] on computers,<ref>E.g. \"Signed integers are two's complement binary values that can be used to represent both positive and negative integer values.\", Section 4.2.1 in Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 1: Basic Architecture, November 2006</ref> and more generally, [[Fixed-point arithmetic|fixed point binary]] values. In this scheme, if the binary number 010<sub>2</sub> encodes the signed integer 2<sub>10</sub>, then its two's complement, 110<sub>2</sub>, encodes the inverse: &minus;2<sub>10</sub>. In other words, to reverse the sign of any integer in this scheme, you can take the two's complement of its binary representation.<ref>David J. Lilja and Sachin S. Sapatnekar, ''Designing Digital Computer Systems with Verilog'', Cambridge University Press, 2005 [https://books.google.com/books?vid=ISBN052182866X&id=5BvW0hYhxkQC&pg=PA37&lpg=PA37&ots=l-E0VjyPt8&dq=%22two%27s+complement+arithmetic%22&sig=sS5_swrfrzcQI2nHWest75sIjgg online]</ref> The tables at right illustrate this property.\n\nCompared to other systems for representing signed numbers (''e.g.,'' [[ones' complement]]), two's complement has the advantage that the fundamental arithmetic operations of [[addition]], [[subtraction]], and [[multiplication]] are identical to those for unsigned binary numbers (as long as the inputs are represented in the same number of bits - as the output, and any [[integer overflow|overflow]] beyond those bits is discarded from the result). This property makes the system simpler to implement, especially for higher-precision arithmetic. Unlike ones' complement systems, two's complement has no representation for [[Ones'_complement#Negative_zero|negative zero]], and thus does not suffer from its associated difficulties.\n\nConveniently, another way of finding the two's complement of a number is to take its ones' complement and add one: the sum of a number and its ones' complement is all '1' bits, or {{math|2<sup>''N''</sup> &minus; 1}}; and by definition, the sum of a number and its ''two's'' complement is {{math|2<sup>''N''</sup>}}.\n\n==History==\nThe [[method of complements]] had long been used to perform subtraction in decimal [[adding machine]]s and [[mechanical calculator]]s. [[John von Neumann]] suggested use of two's complement binary representation in his 1945 ''[[First Draft of a Report on the EDVAC]]'' proposal for an electronic stored-program digital computer.<ref name = \"von Neumann 1945\">{{Citation | last = von Neumann | first = John | author-link = John von Neumann | title = First Draft of a Report on the EDVAC | year = 1945 | url = https://sites.google.com/site/michaeldgodfrey/vonneumann/vnedvac.pdf?attredirects=0&d=1 | accessdate = February 20, 2015 }}</ref> The 1949 [[EDSAC]], which was inspired by the ''First Draft'', used two's complement representation of binary numbers.\n\nMany early computers, including the [[CDC 6600]], the [[LINC]], the [[PDP-1]], and the UNIVAC 1107, use [[ones' complement]] notation; the descendants of the UNIVAC 1107, the [[UNIVAC 1100/2200 series]], continue to do so. The [[IBM 700/7000 series]] scientific machines use sign/magnitude notation, except for the index registers which are two's complement. Early commercial two's complement computers include the [[Digital Equipment Corporation]] PDP-5 and the 1963 [[PDP-6]]. The [[IBM System/360|System/360]], introduced in 1964 by [[IBM]], then the dominant player in the computer industry, made two's complement the most widely used binary representation in the computer industry. The first minicomputer, the [[PDP-8]] introduced in 1965, uses two's complement arithmetic as do the 1969 [[Data General Nova]], the 1970 [[PDP-11]], and almost all subsequent minicomputers and microcomputers.\n\n==Potential ambiguities of terminology==\nThe term ''two's complement'' can mean either a number format or a mathematical operator. For example, 0111 represents decimal 7 in two's-complement ''notation'', but the two's complement of 7 in a [[nibble|4-bit register]] is actually the \"1001\" [[bit string]] (the same as represents {{math|1=9 = 2<sup>4</sup> − 7}} in unsigned arithmetics) which is the two's complement ''representation'' of −7. The statement \"convert {{mvar|x}} to two's complement\" may be ambiguous, since it could describe either the process of representing {{mvar|x}} in two's-complement notation without changing its value, or the calculation of the two's complement, which is the [[additive inverse|arithmetic negative]] of {{mvar|x}} if two's complement representation is used.\n\n==Converting from two's complement representation==\nA two's-complement number system encodes positive and negative numbers in a binary number representation. The weight of each bit is a power of two, except for the [[most significant bit]], whose weight is the negative of the corresponding power of two.\n\nThe value&nbsp;{{mvar|w}} of an {{mvar|N}}-bit integer <math>a_{N-1} a_{N-2} \\dots a_0</math> is given by the following formula:\n:<math>w = -a_{N-1} 2^{N-1} + \\sum_{i=0}^{N-2} a_i 2^i.</math>\nThe most significant bit determines the sign of the number and is sometimes called the [[sign bit]]. Unlike in [[sign-and-magnitude]] representation, the sign bit also has the weight {{math|−(2<sup>''N'' − 1</sup>)}} shown above. Using {{mvar|N}} bits, all integers from {{math|−(2<sup>''N'' − 1</sup>)}} to {{math|2<sup>''N'' − 1</sup> − 1}} can be represented.\n\nThe following [[Python (programming language)|Python]] code shows a simple function which will convert an unsigned input integer '''to''' a two's complement signed integer using the above logic with [[Bitwise operation|bitwise operators]]:\n<syntaxhighlight lang=\"python\">\ndef twos_complement(input_value, num_bits):\n\t'''Calculates a two's complement integer from the given input value's bits'''\n\tmask = 2**(num_bits - 1)\n\treturn -(input_value & mask) + (input_value & ~mask)\n</syntaxhighlight>\n\n==Converting to two's complement representation==\n<!-- This section contains one footnote that is wrapped in a <ref></ref> tag, but this footnote does not list any external references or sources. -->\nIn two's complement notation, a ''non-negative'' number is represented by its ordinary [[Binary numeral system|binary representation]]; in this case, the most significant bit is 0. Though, the range of numbers represented is not the same as with unsigned binary numbers.  For example, an 8-bit unsigned number can represent the values 0 to 255 (11111111).  However a two's complement 8-bit number can only represent positive integers from 0 to 127 (01111111), because the rest of the bit combinations with the most significant bit as '1' represent the negative integers −1 to −128.\n\nThe two's complement operation is the [[additive inverse]] operation, so negative numbers are represented by the two's complement of the [[absolute value]].\n\n===From the ones' complement===\nTo get the two's complement of a binary number, the [[bit]]s are inverted, or \"flipped\", by using the [[bitwise NOT]] operation; the value of 1 is then added to the resulting value, ignoring the overflow which occurs when taking the two's complement of 0.\n\nFor example, using 1 byte (=8 bits), the decimal number 5 is represented by\n:0000 0101<sub>2</sub>\nThe most significant bit is 0, so the pattern represents a non-negative value. To convert to −5 in two's-complement notation, first, the bits are inverted, that is: 0 becomes 1 and 1 becomes 0:\n:1111 1010<sub>2</sub>\nAt this point, the representation is the [[ones' complement]] of the decimal value −5. To obtain the two's complement, 1 is added to the result, giving:\n:1111 1011<sub>2</sub>\nThe result is a signed binary number representing the decimal value −5 in two's-complement form. The most significant bit is 1, so the value represented is negative.\n\nThe two's complement of a negative number is the corresponding positive value. For example, inverting the bits of −5 (above) gives:\n:0000 0100<sub>2</sub>\nAnd adding one gives the final value:\n:0000 0101<sub>2</sub>\n\nThe two's complement of zero is zero: inverting gives all ones, and adding one changes the ones back to zeros (since the overflow is ignored). Furthermore, the two's complement of the most negative number representable (e.g. a one as the most-significant bit and all other bits zero) is itself. Hence, there appears to be an 'extra' negative number.\n\n===Subtraction from 2<sup>''N''</sup>===\n\nThe sum of a number and its ones' complement is an {{mvar|N}}-bit word with all 1 bits, which is (reading as an unsigned binary number) {{math|2<sup>''N''</sup> − 1}}. Then adding a number to its two's complement results in the {{mvar|N}} lowest bits set to 0 and the carry bit 1, where the latter has the weight (reading it as an unsigned binary number) of {{math|2<sup>''N''</sup>}}. Hence, in the unsigned binary arithmetic the value of two's-complement negative number {{math|''x''*}} of a positive {{mvar|x}} satisfies the equality {{math|1=''x''* = 2<sup>''N''</sup> − ''x''}}.<ref>For {{math|1=''x'' = 0}} we have {{math|1=2<sup>''N''</sup> − 0 = 2<sup>''N''</sup>}}, which is equivalent to {{math|1=0* = 0}} modulo {{math|2<sup>''N''</sup>}} (i.e. after restricting to {{mvar|N}} least significant bits).</ref>\n\nFor example, to find the 4-bit representation of −5 (subscripts denote the [[radix|base of the representation]]):\n:{{math|1=''x'' = 5<sub>10</sub>}} therefore {{math|size=120%|1=''x'' = 0101<sub>2</sub>}}\nHence, with {{math|1=''N'' = 4}}:\n:{{math|1=''x''* = 2<sup>''N''</sup> − ''x'' = 2<sup>4</sup> − 5<sub>10</sub> = 16<sub>10</sub> - 5<sub>10</sub> = 10000<sub>2</sub> − 0101<sub>2</sub> = 1011<sub>2</sub>}}\nThe calculation can be done entirely in base 10, converting to base 2 at the end:\n:{{math|1=''x''* = 2<sup>''N''</sup> − ''x'' = 2<sup>4</sup> − 5<sub>10</sub> = 11<sub>10</sub> = 1011<sub>2</sub>}}\n\n===Working from LSB towards MSB===\nA shortcut to manually convert a [[binary number]] into its two's complement is to start at the [[least significant bit]] (LSB), and copy all the zeros, working from LSB toward the most significant bit (MSB) until the first&nbsp;1 is reached; then copy that&nbsp;1, and flip all the remaining bits (Leave the MSB as a 1 if the initial number was in sign-and-magnitude representation). This shortcut allows a person to convert a number to its two's complement without first forming its ones' complement. For example: the two's complement of \"0011&nbsp;1100\" is \"1100&nbsp;0<u>100</u>\", where the underlined digits were unchanged by the copying operation (while the rest of the digits were flipped).\n\nIn computer circuitry, this method is no faster than the \"complement and add one\" method; both methods require working sequentially from right to left, propagating logic changes. The method of complementing and adding one can be sped up by a standard [[carry look-ahead adder]] circuit; the LSB towards MSB method can be sped up by a similar logic transformation.\n\n==Sign extension==\n{{Main|Sign extension}}\n\n{|class=\"wikitable floatright\" style=\"margin-left: 1.5em;\"\n|+ Sign-bit repetition in 7- and 8-bit integers using two's complement\n|-\n!Decimal\n!7-bit notation\n!8-bit notation\n|-\n|align=\"right\"| −42 ||align=\"center\"| 1010110 ||align=\"center\"| 1101 0110\n|-\n|align=\"right\"| 42 ||align=\"center\"| 0101010 ||align=\"center\"| 0010 1010\n|}\n\nWhen turning a two's-complement number with a certain number of bits into one with more bits (e.g., when copying from a 1-byte variable to a 2-byte variable), the most-significant bit must be repeated in all the extra bits.  Some processors do this in a single instruction; on other processors, a conditional must be used followed by code to set the relevant bits or bytes.\n\nSimilarly, when a two's-complement number is shifted to the right, the most-significant bit, which contains magnitude and the sign information, must be maintained. However, when shifted to the left, a 0 is shifted in. These rules preserve the common semantics that left shifts multiply the number by two and right shifts divide the number by two.\n\nBoth shifting and doubling the precision are important for some multiplication algorithms. Note that unlike addition and subtraction, width extension and right shifting are done differently for signed and unsigned numbers.\n\n==Most negative number==\nWith only one exception, starting with any number in two's-complement representation, if all the bits are flipped and 1 added, the two's-complement representation of the negative of that number is obtained. Positive 12 becomes negative 12, positive 5 becomes negative 5, zero becomes zero(+overflow), etc.\n\n{|class=\"wikitable floatright\" style=\"width:18em;\"\n|+ The two's complement of −128 results in the same 8-bit binary number.\n|-\n|align=\"center\"| −128 || align=\"center\"| 1000 0000\n|-\n|align=\"center\"| invert bits || align=\"center\"| 0111 1111\n|-\n|align=\"center\"| add one ||align=\"center\"| 1000 0000\n|-\n|}\n\nThe two's complement of the minimum number in the range will not have the desired effect of negating the number. For example, the two's complement of −128 in an 8-bit system results in the same binary number. This is because a positive value of 128 cannot be represented with an 8-bit signed binary numeral.\n\nThis phenomenon is fundamentally about the mathematics of binary numbers, not the details of the representation as two's complement. Mathematically, this is complementary to the fact that the negative of 0 is again 0. For a given number of bits ''k'' there is an even number of binary numbers 2<sup>''k''</sup>, taking negatives is a [[Group action (mathematics)|group action]] (of the group of order 2) on binary numbers, and since the [[Orbit (group theory)|orbit]] of zero has order 1, at least one other number must have an orbit of order 1 for the orders of the orbits to add up to the order of the set. Thus some other number must be invariant under taking negatives (formally, by the [[orbit-stabilizer theorem]]). Geometrically, one can view the ''k''-bit binary numbers as the [[cyclic group]] <math>\\mathbf Z/2^k</math>, which can be visualized as a circle (or properly a regular 2<sup>''k''</sup>-gon), and taking negatives is a reflection, which fixes the elements of order dividing 2: 0 and the opposite point, or visually the zenith and nadir.\n\nNote that this negative being the same number is detected as an overflow condition since there was a carry into but not out of the most-significant bit. This can lead to unexpected bugs in that an unchecked implementation of [[absolute value]] could return a negative number in the case of the minimum negative. The ''abs'' family of integer functions in [[C (programming language)|C]] typically has this behaviour. This is also true for Java.<ref>{{cite web | url = http://docs.oracle.com/javase/7/docs/api/java/lang/Math.html | title = Math ( Java Platform SE 7 )}}</ref> In this case it is for the developer to decide if there will be a check for the minimum negative value before the call of the function.\n\nThe most negative number in two's complement is sometimes called \"the weird number,\" because it is the only exception.<ref>{{cite web | url = http://www.ipl.t.u-tokyo.ac.jp/jssst2006/papers/Affeldt.pdf | title = Formal Verification of Arithmetic Functions in SmartMIPS Assembly | author = Reynald Affeldt | author2 = Nicolas Marti | last-author-amp = yes | deadurl = yes | archiveurl = https://web.archive.org/web/20110722080531/http://www.ipl.t.u-tokyo.ac.jp/jssst2006/papers/Affeldt.pdf | archivedate = 2011-07-22 | df =  }}</ref><ref>\n[https://books.google.com/books?id=5X7JV5-n0FIC&pg=PA19&dq=%22weird+number%22+binary google.com]; \"Digital Design and Computer Architecture\", by David Harris, David Money Harris, Sarah L. Harris. 2007. Page 18.</ref>\n\nAlthough the number is an exception, it is a valid number in regular two's complement systems. All arithmetic operations work with it both as an operand and (unless there was an overflow) a result.\n\n==Why it works==\nGiven a set of all possible {{mvar|N}}-bit values, we can assign the lower (by the binary value) half to be the integers from 0 to {{math|(2<sup>''N'' − 1</sup> − 1)}} inclusive and the upper half to be {{math|−2<sup>''N'' − 1</sup>}} to −1 inclusive.  The upper half (again, by the binary value) can be used to represent negative integers from {{math|−2<sup>''N'' − 1</sup>}} to −1 because, under addition modulo {{math|2<sup>''N''</sup>}} they behave the same way as those negative integers.  That is to say that because {{math|1= ''i'' + ''j'' mod 2<sup>''N''</sup> = ''i'' + (''j'' + 2<sup>''N''</sup>) mod 2<sup>''N''</sup>}} any value in the set {{math|1= { ''j'' + ''k'' 2<sup>''N''</sup> {{!}} ''k'' is an integer } }} can be used in place of&nbsp;{{mvar|j}}.<ref>{{cite web\n | url = http://www.cs.uwm.edu/~cs151/Bacon/Lecture/HTML/ch03s09.html\n | title = 3.9. Two's Complement | work = Chapter 3. Data Representation\n | date = 2012-12-03 | accessdate = 2014-06-22\n | publisher = cs.uwm.edu\n}}</ref>\n\nFor example, with eight bits, the unsigned bytes are 0 to 255. Subtracting 256 from the top half (128 to 255) yields the signed bytes −128 to −1.\n\nThe relationship to two's complement is realised by noting that {{math|1=256 = 255 + 1}}, and {{math|(255 − ''x'')}} is the [[Signed number representations|ones' complement]] of&nbsp;{{mvar|x}}.\n\n{|class=\"wikitable floatright\" style=\"width:14em;\"\n|+ Some special numbers to note\n!Decimal\n!Two's complement\n|-\n|align=\"right\"|  127 ||0111 1111\n|-\n|align=\"right\"|   64 ||0100 0000\n|-\n|align=\"right\"|    1 ||0000 0001\n|-\n|align=\"right\"|    0 ||0000 0000\n|-\n|align=\"right\"|   −1 ||1111 1111\n|-\n|align=\"right\"|  −64 ||1100 0000\n|-\n|align=\"right\"| −127 ||1000 0001\n|-\n|align=\"right\"| −128 ||1000 0000\n|}\n\n===Example===\n&nbsp;−95 modulo 256 is equivalent to 161 since\n\n:−95 + 256\n:= −95 + 255 + 1\n:= 255 − 95 + 1\n:= 160 + 1\n:= 161\n<pre style=\"width:25em\">\n   1111 1111                       255\n − 0101 1111                     −  95\n ===========                     =====\n   1010 0000  (ones' complement)   160\n +         1                     +   1\n ===========                     =====\n   1010 0001  (two's complement)   161\n</pre>\n{|class=\"wikitable floatright\" style=\"width:14em;\"\n|+ Two's complement using a 4-bit integer\n!Two's complement\n!Decimal\n|-\n|align=\"center\"| 0111 ||align=\"right\"|  7 \n|-\n|align=\"center\"| 0110 ||align=\"right\"|  6 \n|-\n|align=\"center\"| 0101 ||align=\"right\"|  5 \n|-\n|align=\"center\"| 0100 ||align=\"right\"|  4 \n|-\n|align=\"center\"| 0011 ||align=\"right\"|  3 \n|-\n|align=\"center\"| 0010 ||align=\"right\"|  2 \n|-\n|align=\"center\"| 0001 ||align=\"right\"|  1 \n|-\n|align=\"center\"| 0000 ||align=\"right\"|  0 \n|-\n|align=\"center\"| 1111 ||align=\"right\"| −1 \n|-\n|align=\"center\"| 1110 ||align=\"right\"| −2 \n|-\n|align=\"center\"| 1101 ||align=\"right\"| −3 \n|-\n|align=\"center\"| 1100 ||align=\"right\"| −4 \n|-\n|align=\"center\"| 1011 ||align=\"right\"| −5 \n|-\n|align=\"center\"| 1010 ||align=\"right\"| −6 \n|-\n|align=\"center\"| 1001 ||align=\"right\"| −7 \n|-\n|align=\"center\"| 1000 ||align=\"right\"| −8 \n|}\n\nFundamentally, the system represents negative integers by counting backward and [[modulo arithmetic|wrapping around]]. The boundary between positive and negative numbers is arbitrary, but by [[Convention (norm)|convention]] all negative numbers have a left-most bit ([[most significant bit]]) of one. Therefore, the most positive 4-bit number is 0111 (7) and the most negative is 1000 (&minus;8). Because of the use of the left-most bit as the sign bit, the absolute value of the most negative number (|&minus;8| = 8) is too large to represent. For example, an 8-bit number can only represent every integer from &minus;128 to 127 ({{math|1=2<sup>8 − 1</sup> = 128}}) inclusive. Negating a two's complement number is simple: Invert all the bits and add one to the result. For example, negating 1111, we get 0000&nbsp;+&nbsp;1&nbsp;=&nbsp;1. Therefore, 1111 must represent &minus;1.<ref>{{cite web\n | url = http://www.cs.cornell.edu/~tomf/notes/cps104/twoscomp.html\n | title = Two's Complement\n | date = April 2000 | accessdate = 2014-06-22\n | author = Thomas Finley | publisher = cs.cornell.edu\n}}</ref>\n\nThe system is useful in simplifying the implementation of arithmetic on computer hardware. Adding 0011&nbsp;(3) to 1111&nbsp;(&minus;1) at first seems to give the incorrect answer of 10010. However, the hardware can simply ignore the left-most bit to give the correct answer of 0010&nbsp;(2). Overflow checks still must exist to catch operations such as summing 0100 and 0100.\n\nThe system therefore allows addition of negative operands without a subtraction circuit or a circuit that detects the sign of a number. Moreover, that addition circuit can also perform subtraction by taking the two's complement of a number (see below), which only requires an additional cycle or its own adder circuit. To perform this, the circuit merely pretends an extra left-most bit of 1 exists.\n\n==Arithmetic operations==\n\n===Addition===\nAdding two's-complement numbers requires no special processing even if the operands have opposite signs: the sign of the result is determined automatically.  For example, adding 15 and −5:\n<pre style=\"width:25em\">\n  11111 111   (carry)\n   0000 1111  (15)\n + 1111 1011  (−5)\n ===========\n   0000 1010  (10)\n</pre>\nThis process depends upon restricting to 8 bits of precision; a carry to the (nonexistent) 9th most significant bit is ignored, resulting in the arithmetically correct result of 10<sub>10</sub>.\n\nThe last two bits of the [[Carry flag|carry]] row (reading right-to-left) contain vital information: whether the calculation resulted in an [[arithmetic overflow]], a number too large for the binary system to represent (in this case greater than 8 bits).  An overflow condition exists when these last two bits are different from one another.  As mentioned above, the sign of the number is encoded in the MSB of the result.\n\nIn other terms, if the left two carry bits (the ones on the far left of the top row in these examples) are both 1s or both 0s, the result is valid; if the left two carry bits are \"1 0\" or \"0 1\", a sign overflow has occurred.  '''Conveniently, an [[XOR]] operation on these two bits can quickly determine if an overflow condition exists.'''  As an example, consider the signed 4-bit addition of 7 and 3:\n<pre style=\"width:25em\">\n  0111   (carry)\n   0111  (7)\n + 0011  (3)\n ======\n   1010  (−6)  invalid!\n</pre>\nIn this case, the far left two (MSB) carry bits are \"01\", which means there was a two's-complement addition overflow.  That is, 1010<sub>2</sub> = 10<sub>10</sub> is outside the permitted range of &minus;8 to 7. The result would be correct if treated as unsigned integer.\n\nIn general, any two {{mvar|N}}-bit numbers may be added ''without'' overflow, by first sign-extending both of them to {{math|''N'' + 1}} bits, and then adding as above. The {{math|''N'' + 1}} bits result is large enough to represent any possible sum ({{math|1=''N'' = 5}} two's complement can represent values in the range &minus;16 to 15) so overflow will never occur. It is then possible, if desired, to 'truncate' the result back to {{mvar|N}} bits while preserving the value if and only if the discarded bit is a proper sign extension of the retained result bits. This provides another method of detecting overflow&mdash;which is equivalent to the method of comparing the carry bits&mdash;but which may be easier to implement in some situations, because it does not require access to the internals of the addition.\n\n===Subtraction===\nComputers usually use the [[method of complements]] to implement subtraction. Using complements for subtraction is closely related to using complements for representing negative numbers, since the combination allows all signs of operands and results; direct subtraction works with two's-complement numbers as well. Like addition, the advantage of using two's complement is the elimination of examining the signs of the operands to determine whether addition or subtraction is needed. For example, subtracting −5 from 15 is really adding 5 to 15, but this is hidden by the two's-complement representation:\n<pre style=\"width:25em\">\n  11110 000   (borrow)\n   0000 1111  (15)\n − 1111 1011  (−5)\n ===========\n   0001 0100  (20)\n</pre>\nOverflow is detected the same way as for addition, by examining the two leftmost (most significant) bits of the borrows; overflow has occurred if they are different.\n\nAnother example is a subtraction operation where the result is negative: 15&nbsp;−&nbsp;35 = −20:\n<pre style=\"width:25em\">\n  11100 000   (borrow)\n   0000 1111  (15)\n − 0010 0011  (35)\n ===========\n   1110 1100  (−20)\n</pre>\nAs for addition, overflow in subtraction may be avoided (or detected after the operation) by first sign-extending both inputs by an extra bit.\n\n===Multiplication===\nThe product of two {{mvar|N}}-bit numbers requires {{math|2''N''}} bits to contain all possible values.<ref>Bruno Paillard. ''An Introduction To Digital Signal Processors'', Sec. 6.4.2. Génie électrique et informatique Report, Université de Sherbrooke, April 2004.</ref>\n\nIf the precision of the two operands using two's complement is doubled before the multiplication, direct multiplication (discarding any excess bits beyond that precision) will provide the correct result.<ref>{{cite web\n |url         = http://pages.cs.wisc.edu/~cs354-1/beyond354/int.mult.html\n |title       = Two's Complement Multiplication\n |date        = August 24, 2007\n |accessdate  = April 13, 2015\n |author      = Karen Miller\n |website     = cs.wisc.edu\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20150213203512/http://pages.cs.wisc.edu/~cs354-1/beyond354/int.mult.html\n |archivedate = February 13, 2015\n |df          = \n}}</ref> For example, take {{math|1=6 &times; &minus;5 = &minus;30}}. First, the precision is extended from four bits to eight. Then the numbers are multiplied, discarding the bits beyond the eighth bit (as shown by \"{{Mono|x}}\"):\n<pre style=\"width:25em\">\n     00000110  (6)\n *   11111011  (−5)\n ============\n          110\n         1100\n        00000\n       110000\n      1100000\n     11000000\n    x10000000\n + xx00000000\n ============\n   xx11100010\n</pre>\nThis is very inefficient; by doubling the precision ahead of time, all additions must be double-precision and at least twice as many partial products are needed than for the more efficient algorithms actually implemented in computers. Some multiplication algorithms are designed for two's complement, notably [[Booth's multiplication algorithm]]. Methods for multiplying sign-magnitude numbers don't work with two's-complement numbers without adaptation. There isn't usually a problem when the multiplicand (the one being repeatedly added to form the product) is negative; the issue is setting the initial bits of the product correctly when the multiplier is negative. Two methods for adapting algorithms to handle two's-complement numbers are common:\n\n* First check to see if the multiplier is negative. If so, negate (''i.e.'', take the two's complement of) both operands before multiplying. The multiplier will then be positive so the algorithm will work. Because both operands are negated, the result will still have the correct sign.\n* Subtract the partial product resulting from the MSB (pseudo sign bit) instead of adding it like the other partial products. This method requires the multiplicand's sign bit to be extended by one position, being preserved during the shift right actions.<ref>{{cite book |first=John F. |last=Wakerly |title=Digital Design Principles & Practices |location= |publisher=Prentice Hall |edition=3rd |year=2000 |page=47 |isbn=0-13-769191-2 }}</ref>\n\nAs an example of the second method, take the common add-and-shift algorithm for multiplication. Instead of shifting partial products to the left as is done with pencil and paper, the accumulated product is shifted right, into a second register that will eventually hold the least significant half of the product. Since the [[least significant bit]]s are not changed once they are calculated, the additions can be single precision, accumulating in the register that will eventually hold the most significant half of the product. In the following example, again multiplying 6 by &minus;5, the two registers and the extended sign bit are separated by \"|\":\n<pre style=\"width:50em\">\n  0 0110  (6)  (multiplicand with extended sign bit)\n  × 1011 (−5)  (multiplier)\n  =|====|====\n  0|0110|0000  (first partial product (rightmost bit is 1))\n  0|0011|0000  (shift right, preserving extended sign bit)\n  0|1001|0000  (add second partial product (next bit is 1))\n  0|0100|1000  (shift right, preserving extended sign bit)\n  0|0100|1000  (add third partial product: 0 so no change)\n  0|0010|0100  (shift right, preserving extended sign bit)\n  1|1100|0100  (subtract last partial product since it's from sign bit)\n  1|1110|0010  (shift right, preserving extended sign bit)\n   |1110|0010  (discard extended sign bit, giving the final answer, −30)\n</pre>\n\n===Comparison (ordering)===\n[[Comparison (computer programming)|Comparison]] is often implemented with a dummy subtraction, where the flags in the computer's [[status register]] are checked, but the main result is ignored. The [[zero flag]] indicates if two values compared equal. If the exclusive-or of the [[Sign flag|sign]] and [[Overflow flag|overflow]] flags is 1, the subtraction result was less than zero, otherwise the result was zero or greater. These checks are often implemented in computers in [[conditional branch]] instructions.\n\nUnsigned binary numbers can be ordered by a simple [[lexicographic ordering]], where the bit value 0 is defined as less than the bit value 1.  For two's complement values, the meaning of the most significant bit is reversed (i.e. 1 is less than 0).\n\nThe following algorithm (for an {{mvar|n}}-bit two's complement architecture) sets the result register R to −1 if A < B, to +1 if A > B, and to 0 if A and B are equal:\n\n<syntaxhighlight lang=\"pascal\">\n // reversed comparison of the sign bit\n\n if A(n-1) == 0 and B(n-1) == 1 then\n    R := +1\n    break\n else if A(n-1) == 1 and B(n-1) == 0 then\n    R := -1\n    break\n end\n \n // comparison of remaining bits\n\n for i = n-2...0 do\n   if A(i) == 0 and B(i) == 1 then\n      R := -1\n      break\n   else if A(i) == 1 and B(i) == 0 then\n      R := +1\n      break\n   end\n end\n \n R := 0\n</syntaxhighlight>\n\n==Two's complement and 2-adic numbers==\nIn a classic ''[[HAKMEM]]'' published by the [[MIT AI Lab]] in 1972, [[Bill Gosper]] noted that whether or not a machine's internal representation was two's-complement could be determined by summing the successive powers of two. In a flight of fancy, he noted that the result of doing this algebraically indicated that \"algebra is run on a machine (the universe) which is two's-complement.\"<ref>[http://www.inwap.com/pdp10/hbaker/hakmem/hacks.html#item154 Hakmem - Programming Hacks - Draft, Not Yet Proofed<!-- Bot generated title -->]</ref>\n\nGosper's end conclusion is not necessarily meant to be taken seriously, and it is akin to a [[mathematical joke]]. The critical step is \"...110 = ...111&nbsp;−&nbsp;1\", i.e., \"2''X'' = ''X''&nbsp;−&nbsp;1\", and thus ''X''&nbsp;=&nbsp;...111&nbsp;=&nbsp;−1. This presupposes a method by which an infinite string of 1s is considered a number, which requires an extension of the finite place-value concepts in elementary arithmetic.<!--Does this interpretation take into account a sign bit?--> It is meaningful either as part of a two's-complement notation for all integers, as a typical [[p-adic number|2-adic number]], or even as one of the generalized sums defined for the [[divergent series]] of real numbers [[1 + 2 + 4 + 8 + …|1 + 2 + 4 + 8 + ···]].<ref>For the summation of 1 + 2 + 4 + 8 + ··· without recourse to the 2-adic metric, see {{cite book |last=Hardy |first=G.H. |authorlink=G. H. Hardy |title=Divergent Series |year=1949 |publisher=Clarendon Press |id={{LCC|QA295|.H29|1967}}}} (pp. 7–10)</ref> Digital arithmetic circuits, idealized to operate with infinite (extending to positive powers of 2) bit strings, produce 2-adic addition and multiplication compatible with two's complement representation.<ref>{{cite book |title=On circuits and numbers |last=Vuillemin |first=Jean |year=1993 |publisher=[[Digital Equipment Corp.]] |location=Paris |page=19 |url=http://www.hpl.hp.com/techreports/Compaq-DEC/PRL-RR-25.pdf |accessdate=2012-01-24}}, Chapter 7, especially 7.3 for multiplication.</ref> [[continuous function|Continuity]] of binary arithmetical and [[bitwise operation]]s in 2-adic [[metric space|metric]] also has some use in cryptography.<ref>{{cite web |url=http://crypto.rsuh.ru/ |title=ABC Stream Cipher |last1=Anashin|first1=Vladimir |last2=Bogdanov|first2=Andrey |last3=Kizhvatov|first3=Ilya |year=2007 |publisher=[[Russian State University for the Humanities]] |accessdate=24 January 2012}}</ref>\n\n== Fractions conversion ==\nTo convert a fraction, for instance; .0101 you must convert starting from right to left the 1s to decimal as in a normal conversion. In this example 0101 is equal to 5 in decimal. Each digit after the floating point represents a fraction where the denominator is a multiplier of 2. So, the first is 1/2, the second is 1/4 and so on. Having already calculated the decimal value as mentioned above, you use only the denominator of the LSB (LSB = starting from right). As a result, we have 5/16.\n\nFor instance, having the floating value of .0110 for this method to work, one should not consider the last 0 from the right. Hence, instead of calculating the decimal value for 0110, we calculate the value 011, which is 3 in decimal (by leaving the \"0\" in the end, the result would have been 6, together with the denominator 2<sup>4</sup> = 16 reduces to 3/8). So the denominator is 8. So, the final result is 3/8.\n\n==See also==\n*[[Division algorithm]], including restoring and non-restoring division in two's-complement representations\n*[[Offset binary]]\n*[[p-adic number]]\n*[[Method of complements]], generalisation to other number bases, used on mechanical calculators\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n*{{cite book |first=Israel |last=Koren |title=Computer Arithmetic Algorithms |location= |publisher=A.K. Peters |year=2002 |isbn=1-56881-160-8 }}\n*{{cite book |first=Ivan |last=Flores |title=The Logic of Computer Arithmetic |location= |publisher=Prentice-Hall |year=1963 }}\n\n==External links==\n*[http://www.vb-helper.com/tutorial_twos_complement.html Tutorial: Two's Complement Numbers]\n*[http://www.ecs.umass.edu/ece/koren/arith/simulator/ArrMlt/ Two's complement array multiplier JavaScript simulator]\n\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "XOR linked list",
      "url": "https://en.wikipedia.org/wiki/XOR_linked_list",
      "text": "{{Refimprove|date=October 2009}}\n\nAn '''XOR linked list''' is a type of [[data structure]] used in [[computer programming]]. It takes advantage of the [[Bitwise operation#XOR|bitwise XOR]] operation to decrease storage requirements for [[doubly linked list]]s.\n\n== Description ==\nAn ordinary doubly linked list stores addresses of the previous and next list items in each list node, requiring two address fields:\n\n  ...  A       B         C         D         E  ...\n           –>  next –>  next  –>  next  –>\n           <–  prev <–  prev  <–  prev  <–\n\nAn XOR linked list compresses the same information into ''one'' address field by storing the bitwise XOR (here denoted by ⊕) of the address for ''previous'' and the address for ''next'' in one field:\n\n  ...  A        B         C         D         E  ...\n          <–>  A⊕C  <->  B⊕D  <->  C⊕E  <->\n\nMore formally: \n   link(B) = addr(A)⊕addr(C), link(C) = addr(B)⊕addr(D), ...\n\nWhen traversing the list from left to right: supposing the cursor is at C, the previous item, B may be XORed with the value in the link field (B⊕D). The address for D will then be obtained and list traversal may resume. The same pattern applies in the other direction.\n     i.e.  addr(D) = link(C) ⊕ addr(B)\n     where\n           link(C) = addr(B)⊕addr(D)\n      so  \n           addr(D) = addr(B)⊕addr(D) ⊕ addr(B)           \n       \n           addr(D) = addr(B)⊕addr(B) ⊕ addr(D) \n     since \n            X⊕X = 0                 \n            => addr(D) = 0 ⊕ addr(D)\n     since\n            X⊕0 = x\n            => addr(D) = addr(D)\n     The XOR operation cancels addr(B) appearing twice in the equation and all we are left with is the addr(D).\n\nTo start traversing the list in either direction from some point, the address of two consecutive items is required. If the addresses of the two consecutive items are reversed, list traversal will occur in the opposite direction.<ref>{{Cite news|url=https://www.geeksforgeeks.org/xor-linked-list-a-memory-efficient-doubly-linked-list-set-1/|title=XOR Linked List - A Memory Efficient Doubly Linked List {{!}} Set 1 - GeeksforGeeks|date=2011-05-23|work=GeeksforGeeks|access-date=2018-10-29|language=en-US}}</ref>\n\n===Theory of operation===\nThe key is the first operation, and the properties of XOR: \n*X⊕X = 0\n*X⊕0 = X\n*X⊕Y = Y⊕X\n*(X⊕Y)⊕Z = X⊕(Y⊕Z)\n\nThe R2 register always contains the XOR of the address of current item C with the address of the predecessor item P: C⊕P. The Link fields in the records contain the XOR of the left and right successor addresses, say L⊕R. XOR of R2 (C⊕P) with the current link field (L⊕R) yields C⊕P⊕L⊕R. \n* If the predecessor was L, the P(=L) and L ''cancel out''  leaving C⊕R. \n* If the predecessor had been R, the P(=R) and R  cancel, leaving C⊕L. \n\nIn each case, the result is the XOR of the current address with the next address. XOR of this with the current address in R1 leaves the next address. R2 is left with the requisite XOR pair of the (now) current address and the predecessor.\n\n==Features==\n* Two XOR operations suffice to do the traversal from one item to the next, the same instructions sufficing in both cases. Consider a list with items <code>{…B C D…}</code> and with R1 and R2 being [[Processor register|registers]] containing, respectively, the address of the current (say C) list item and a work register containing the XOR of the current address with the previous address (say C⊕D).  Cast as [[System/360]] instructions:\n\n X  R2,Link    R2 <- C⊕D ⊕ B⊕D (i.e. B⊕C, \"Link\" being the link field\n                                   in the current record, containing B⊕D)\n XR R1,R2      R1 <- C ⊕ B⊕C    (i.e. B, voilà: the next record)\n\n* End of list is signified by imagining a list item at address zero placed adjacent to an end point, as in <code>{0 A B C…}</code>. The link field at A would be 0⊕B. An additional instruction is needed in the above sequence after the two XOR operations to detect a zero result in developing the address of the current item,\n* A list end point can be made reflective by making the link pointer be zero. A zero pointer is a ''mirror''. (The XOR of the left and right neighbor addresses, being the same, is zero.)\n\n== Drawbacks ==\n\n* General-purpose debugging tools cannot follow the XOR chain, making debugging more difficult; <ref>{{cite web |url=http://www.iecc.com/gclist/GC-faq.html#GC,%20C,%20and%20C++ |title=GC [garbage collection] FAQ – draft |first=David |last=Gadbois |display-authors=etal|accessdate=5 December 2018}}</ref>\n* The price for the decrease in memory usage is an increase in code complexity, making maintenance more expensive;\n* Most [[garbage collection (computer science)|garbage collection]] schemes do not work with data structures that do not contain literal [[pointer (computer programming)|pointer]]s;\n* Not all languages support [[type conversion]] between pointers and integers, XOR on pointers is not defined in some contexts;\n* While traversing the list the address of the previously accessed node is needed to calculate the next node's address and the pointers will be unreadable if one isn't traversing the list&mdash;for example, if the pointer to a list item was contained in another data structure;\n* XOR linked lists do not provide some of the important advantages of doubly linked lists, such as the ability to delete a node from the list knowing only its address or the ability to insert a new node before or after an existing node when knowing only the address of the existing node.\n\nComputer systems have increasingly cheap and plentiful memory, therefore storage overhead is not generally an overriding issue outside specialized [[embedded system]]s. Where it is still desirable to reduce the overhead of a linked list, [[unrolled linked list|unrolling]] provides a more practical approach (as well as other advantages, such as increasing cache performance and speeding [[random access]]).\n\n==Variations==\nThe underlying principle of the XOR linked list can be applied to any reversible binary operation. Replacing XOR by addition or subtraction gives slightly different, but largely equivalent, formulations:\n\n===Addition linked list===\n\n  ...  A        B         C         D         E  ...\n          <–>  A+C  <->  B+D  <->  C+E  <->\n\nThis kind of list has exactly the same properties as the XOR linked list, except that a zero link field is not a \"mirror\". The address of the next node in the list is given by subtracting the previous node's address from the current node's link field.\n\n===Subtraction linked list===\n\n  ...  A        B         C         D         E  ...\n          <–>  C-A  <->  D-B  <->  E-C  <->\n\nThis kind of list differs from the standard \"traditional\" XOR linked list in that the instruction sequences needed to traverse the list forwards is different from the sequence needed to traverse the list in reverse. The address of the next node, going forwards, is given by ''adding'' the link field to the previous node's address; the address of the preceding node is given by ''subtracting'' the link field from the next node's address.\n\nThe subtraction linked list is also special in that the entire list can be relocated in memory without needing any patching of pointer values, since adding a constant offset to each address in the list will not require any changes to the values stored in the link fields. (See also [[serialization]].) This is an advantage over both XOR linked lists and traditional linked lists.\n\n==See also==\n*[[XOR swap algorithm]]\n\n==References==\n<references/>\n\n==External links==\n* Prokash Sinha, [http://www.linuxjournal.com/article/6828 A Memory-Efficient Doubly Linked List] // LinuxJournal, Dec 01, 2004\n{{Data structures}}\n\n[[Category:Binary arithmetic]]\n[[Category:Linked lists]]"
    },
    {
      "title": "XOR swap algorithm",
      "url": "https://en.wikipedia.org/wiki/XOR_swap_algorithm",
      "text": "{{Refimprove|date=February 2012}}\n[[Image:XOR Swap.svg|thumb|upright=2|alt=With three XOR operations the binary values 1010 and 0011 are exchanged between variables.|Using the XOR swap algorithm to exchange [[nibble]]s between variables without the use of temporary storage]]\n\nIn [[computer programming]], the '''XOR swap''' is an [[algorithm]] that uses the [[exclusive disjunction|XOR]] [[bitwise operation]] to [[swap (computer science)|swap]] values of distinct [[variable (programming)|variable]]s having the same [[data type]] without using a temporary variable. \"Distinct\" means that the variables are stored at different, non-overlapping, memory addresses; the actual values of the variables do not have to be different.\n\n==The algorithm==\nConventional swapping requires the use of a temporary storage variable. Using the XOR swap algorithm, however, no temporary storage is needed. The algorithm is as follows:<ref>{{cite web|url=http://www.cs.umd.edu/class/sum2003/cmsc311/Notes/BitOp/xor.html |title=The Magic of XOR |publisher=Cs.umd.edu |date= |accessdate=2014-04-02}}</ref><ref>{{cite web|url=http://graphics.stanford.edu/~seander/bithacks.html#SwappingValuesXOR|title=Swapping Values with XOR|publisher=graphics.stanford.edu|accessdate=2014-05-02}}</ref>\n\n<syntaxhighlight lang=\"pascal\">\nX := X XOR Y\nY := Y XOR X\nX := X XOR Y\n</syntaxhighlight>\n\nThe algorithm typically corresponds to three [[machine code|machine-code]] instructions. Since XOR is a [[commutative operation]], X XOR Y can be replaced with Y XOR X in any of the lines. When coded in assembly language, this commutativity is often exercised in the second line:\n\n{| class=\"wikitable\" style=\"width: 45em;\"\n|-\n! Pseudocode !! IBM [[System/370]] assembly !! x86 assembly\n|-\n| {{code|1=X := X XOR Y|2=pascal}} || {{code|1=XR    R1,R2|2=asm}} || {{code|1=xor    eax, ebx|2=asm}}\n|-\n| {{code|1=Y := Y XOR X|2=pascal}} || {{code|1=XR    R2,R1|2=asm}} || {{code|1=xor    ebx, eax|2=asm}}\n|-\n| {{code|1=X := X XOR Y|2=pascal}} || {{code|1=XR    R1,R2|2=asm}} || {{code|1=xor    eax, ebx|2=asm}}\n|}\n\nIn the above System/370 assembly code sample, R1 and R2 are distinct [[processor register|register]]s, and each XR operation leaves its result in the register named in the first argument. Using x86 assembly, values X and Y are in registers eax and ebx (respectively), and {{code|xor|2=asm}} places the result of the operation in the first register.\n\nHowever, the algorithm fails if ''x'' and ''y'' use the same storage location, since the value stored in that location will be zeroed out by the first XOR instruction, and then remain zero; it will not be \"swapped with itself\". Note that this is ''not'' the same as if ''x'' and ''y'' have the same values.  The trouble only comes when ''x'' and ''y'' use the same storage location, in which case their values must already be equal. That is, if ''x'' and ''y'' use the same storage location, then the line:\n<syntaxhighlight lang=\"pascal\">\nX := X XOR Y\n</syntaxhighlight>\nsets ''x'' to zero (because ''x'' = ''y'' so X XOR Y is zero) ''and'' sets ''y'' to zero (since it uses the same storage location), causing ''x'' and ''y'' to lose their original values.\n\n==Proof of correctness==\nThe [[binary operation]] XOR over bit strings of length <math>N</math> exhibits the following properties (where <math>\\oplus</math> denotes XOR):{{Efn|The first three properties, along with the existence of an inverse for each element, are the definition of an [[abelian group]]. The last property is the statement that every element is an [[Involution (mathematics)|involution]], that is, having [[Order (group theory)|order]] 2, which is not true of all abelian groups.}}\n\n* '''L1.''' [[Commutative operation|Commutativity]]: <math>A \\oplus B = B \\oplus A</math>\n* '''L2.''' [[Associativity]]: <math>(A \\oplus B) \\oplus C = A \\oplus (B \\oplus C)</math>\n* '''L3.''' [[Identity element|Identity exists]]: there is a bit string, 0, (of length ''N'') such that <math>A \\oplus 0 = A</math> for any <math>A</math>\n* '''L4.''' Each element is its own [[inverse element|inverse]]: for each <math>A</math>, <math>A \\oplus A = 0</math>.\n\nSuppose that we have two distinct registers <code>R1</code> and <code>R2</code> as in the table below, with initial values ''A'' and ''B'' respectively. We perform the operations below in sequence, and reduce our results using the properties listed above.\n\n{| class=\"wikitable\"\n|- \n! Step\n! Operation\n! Register 1\n! Register 2\n! Reduction\n|-\n| 0 || Initial value || <math>A</math> || <math>B</math> || — \n|-\n| 1 || <code>R1 := R1 XOR R2</code>  || <math>A \\oplus B</math> || <math>\\ B</math> || — \n|- \n| 2 || <code>R2 := R1 XOR R2</code>  || <math>A \\oplus B</math> || <math>(A \\oplus B) \\oplus B = A \\oplus (B \\oplus B)</math><br><math>= A \\oplus 0</math><br><math>=A</math> || '''L2<br> L4<br> L3'''\n|-\n| 3 || <code>R1 := R1 XOR R2</code>  || <math>(A \\oplus B) \\oplus A = A \\oplus (A \\oplus B)</math><br><math> = (A \\oplus A) \\oplus B</math><br><math>= 0 \\oplus B </math><br><math>= B \\oplus 0 </math><br><math> = B </math> || <math>\\ A</math> || '''L1<br> L2<br> L4<br> L1<br> L3'''\n|}\n\n=== Linear algebra interpretation ===\nAs XOR can be interpreted as binary addition and a pair of bits can be interpreted as a vector in a two-dimensional [[vector space]] over the [[field with two elements]], the steps in the algorithm can be interpreted as multiplication by 2&times;2 matrices over the field with two elements. For simplicity, assume initially that ''x'' and ''y'' are each single bits, not bit vectors.\n\nFor example, the step:\n<syntaxhighlight lang=\"pascal\">\nX := X XOR Y\n</syntaxhighlight>\nwhich also has the implicit:\n<syntaxhighlight lang=\"pascal\">\nY := Y\n</syntaxhighlight>\ncorresponds to the matrix <math>\\left(\\begin{smallmatrix}1 & 1\\\\0 & 1\\end{smallmatrix}\\right)</math> as\n:<math>\\begin{pmatrix}1 & 1\\\\0 & 1\\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix}\n= \\begin{pmatrix}x+y\\\\y\\end{pmatrix}.\n</math>\nThe sequence of operations is then expressed as:\n:<math>\n\\begin{pmatrix}1 & 1\\\\0 & 1\\end{pmatrix}\n\\begin{pmatrix}1 & 0\\\\1 & 1\\end{pmatrix}\n\\begin{pmatrix}1 & 1\\\\0 & 1\\end{pmatrix}\n=\n\\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\n</math>\n(working with binary values, so <math>1 + 1 = 0</math>), which expresses the [[elementary matrix]] of switching two rows (or columns) in terms of the [[Shear mapping|transvections]] (shears) of adding one element to the other.\n\nTo generalize to where X and Y are not single bits, but instead bit vectors of length ''n'', these 2&times;2 matrices are replaced by 2''n''&times;2''n'' [[block matrices]] such as <math>\\left(\\begin{smallmatrix}I_n & I_n\\\\0 & I_n\\end{smallmatrix}\\right).</math>\n\nNote that these matrices are operating on ''values,'' not on ''variables'' (with storage locations), hence this interpretation abstracts away from issues of storage location and the problem of both variables sharing the same storage location.\n\n==Code example==\nA [[C (programming language)|C]] function that implements the XOR swap algorithm:\n<!-- This display template is broken in IE 6, the top half of the pretty bordered box is cut off. -->\n<source lang=\"c\">\n void xorSwap (int *x, int *y) {\n     if (x != y) {\n         *x ^= *y;\n         *y ^= *x;\n         *x ^= *y;\n     }\n }\n</source>\nNote that the code does not swap the integers passed immediately, but first checks if their addresses are distinct. This is because, if the addresses are equal, the algorithm will fold to a triple *x ^= *x resulting in zero.\n\nThe XOR swap algorithm can also be defined with a macro:\n<source lang=\"c\">\n#define XORSWAP_UNSAFE(a, b)\t((a)^=(b),(b)^=(a),(a)^=(b)) /* Doesn't work when a and b are the same object - assigns zero (0) to the object in that case */\n#define XORSWAP(a, b)   ((&(a) == &(b)) ? (a) : ((a)^=(b),(b)^=(a),(a)^=(b))) /* checks that the addresses of a and b are different before XOR-ing */\n</source>\n\n==Reasons for use in practice==\nIn most practical scenarios, the trivial swap algorithm using a temporary register is more efficient. Limited situations in which XOR swapping may be practical include:\n\n* on a processor where the instruction set encoding permits the XOR swap to be encoded in a smaller number of bytes\n* in a region with high [[register pressure]], it may allow the [[register allocator]] to avoid [[spilling a register]]\n* in [[microcontrollers]] where available RAM is very limited.\n* in cryptographic applications which need constant time functions to prevent time-based side-channel attacks<ref>{{cite book|last1=Schneier|first1=Tadayoshi Kohno, Niels Ferguson, Bruce|title=Cryptography engineering : design principles and practical applications|date=2010|publisher=Wiley Pub., inc.|location=Indianapolis, IN|isbn=978-0-470-47424-2|page=251 ff.}}</ref>\n\nBecause these situations are rare, most optimizing compilers do not generate XOR swap code.\n\n==Reasons for avoidance in practice==\nMost modern compilers can optimize away the temporary variable in the three-way swap, in which case it will use the same amount of memory and the same number of registers as the XOR swap and is at least as fast, and often faster. In addition to that, the XOR swap is completely opaque to anyone unfamiliar with the technique.\n\nOn modern [[CPU architecture]]s, the XOR technique can be slower than using a temporary variable to do swapping. At least on recent x86 CPUs, both by AMD and Intel, moving between registers regularly incurs zero latency. (This is called MOV-elimination.) Even if there isn’t any achitectural register available to use, the <code>XCHG</code> instruction will be at least as fast as the three XORs taken together. Another reason is that modern CPUs strive to execute instructions in parallel via [[instruction pipeline]]s. In the XOR technique, the inputs to each operation depend on the results of the previous operation, so they must be executed in strictly sequential order, negating any benefits of [[instruction-level parallelism]].<ref>{{cite web |first1=Saman |last1=Amarasinghe |first2=Charles |last2=Leiserson |title=6.172 Performance Engineering of Software Systems, Lecture 2 |year=2010 |publisher=Massachusetts Institute of Technology |website=MIT OpenCourseWare |url=http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2010/video-lectures/lecture-2-bit-hacks/ |accessdate=27 January 2015}}</ref>\n\nA historical reason was that it used to be patented (US4197590).  Even then, this was only for computer graphics.\n\n===Aliasing===\nThe XOR swap is also complicated in practice by [[aliasing (computing)|aliasing]]. As noted above, if an attempt is made to XOR-swap the contents of some location with itself, the result is that the location is zeroed out and its value lost. Therefore, XOR swapping must not be used blindly in a high-level language if aliasing is possible.\n\nSimilar problems occur with [[call by name]], as in [[Jensen's Device]], where swapping <code>i</code> and <code>A[i]</code> via a temporary variable yields incorrect results due to the arguments being related: swapping via <code>temp = i; i = A[i]; A[i] = temp</code> changes the value for <code>i</code> in the second statement, which then results in the incorrect i value for <code>A[i]</code> in the third statement.\n\n==Variations==\nThe underlying principle of the XOR swap algorithm can be applied to any operation meeting criteria L1 through L4 above. Replacing XOR by addition and subtraction gives a slightly different, but largely equivalent, formulation:\n\n<source lang=\"c\">\n void addSwap (unsigned int *x, unsigned int *y)\n {\n     if (x != y) {\n         *x = *x + *y;\n         *y = *x - *y;\n         *x = *x - *y;\n     }\n }\n</source>\n\nUnlike the XOR swap, this variation requires that the underlying processor or programming language uses a method such as [[modular arithmetic]] or [[bignum]]s to guarantee that the computation of <code>X + Y</code> cannot cause an error due to [[integer overflow]]. Therefore, it is seen even more rarely in practice than the XOR swap.\n\nNote, however, that the implementation of <code>addSwap</code> above in the C programming language always works even in case of integer overflow, since, according to the C standard, addition and  subtraction of unsigned integers follow the rules of [[modular arithmetic]], i. e. are done in the [[cyclic group]] <math>\\mathbb Z/2^s\\mathbb Z</math> where <math>s</math> is the number of bits of <code>unsigned int</code>. Indeed, the correctness of the algorithm follows from the fact that the formulas <math>(x + y) - y = x</math> and <math>(x + y) - ((x + y) - y) = y</math> hold in any [[abelian group]]. This is actually a generalization of the proof for the XOR swap algorithm: XOR is both the addition and subtraction in the abelian group <math>(\\mathbb Z/2\\mathbb Z)^{s}</math> (which is the [[direct sum]] of ''s'' copies of <math>\\mathbb Z/2\\mathbb Z</math>).\n\nPlease note that the above doesn't hold when dealing with the <code>signed int</code> type (the default for <code>int</code>). Signed integer overflow is an undefined behavior in C and thus modular arithmetic is not guaranteed by the standard (a standard-conforming compiler might optimize out such code, which leads to incorrect results).\n\n== See also ==\n* [[Symmetric difference]]\n* [[XOR linked list]]\n* [[Feistel cipher]] (the XOR swap algorithm is a degenerate form of a Feistel cypher)\n\n== Notes ==\n{{Notelist}}\n\n== References ==\n{{Reflist}}\n\n[[Category:Algorithms]]\n[[Category:Articles with example C code]]\n[[Category:Binary arithmetic]]\n\n[[pl:Zamiana wartości zmiennych]]\n[[fr:Échange (informatique)#En_utilisant_l.27op.C3.A9ration_XOR]]"
    },
    {
      "title": "XS-3 code",
      "url": "https://en.wikipedia.org/wiki/XS-3_code",
      "text": "#redirect [[Excess-3]] {{R from abbreviation}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Adder (electronics)",
      "url": "https://en.wikipedia.org/wiki/Adder_%28electronics%29",
      "text": "{{ALUSidebar|expand=Components|expand-components=Adder}}\nAn '''adder''' is a [[digital circuit]] that performs [[addition]] of numbers.\nIn many [[computer]]s and other kinds of [[microprocessor|processor]]s adders are used in the [[arithmetic logic unit]]s or ALU. They are also used in other parts of the processor, where they are used to calculate [[address (computing)|address]]es, table indices, [[increment and decrement operators]], and similar operations.\n\nAlthough adders can be constructed for many [[number representation]]s, such as [[binary-coded decimal]] or [[excess-3]], the most common adders operate on [[binary number]]s.\nIn cases where [[two's complement]] or [[ones' complement]] is being used to represent [[negative number]]s, it is trivial to modify an adder into an [[adder–subtractor]].\nOther [[signed number representations]] require more logic around the basic adder.\n\n==Binary adders==\n===Half adder===\n[[File:half Adder.svg|right|thumb|Half adder logic diagram]] \n[[File:Halfadder.gif|right|thumb|Half adder in action]]\n\nThe '''half adder''' adds two single binary digits ''A'' and ''B''. It has two outputs, sum (''S'') and carry (''C''). The carry signal represents an [[Integer overflow|overflow]] into the next digit of a multi-digit addition. The value of the sum is {{nobreak|2''C'' + ''S''}}. The simplest half-adder design, pictured on the right, incorporates an [[XOR gate]] for ''S'' and an [[AND gate]] for ''C''. The Boolean logic for the sum (in this case ''S'') will be {{nobreak|''A′B'' + ''AB′''}} whereas for the carry (''C'') will be ''AB''. With the addition of an [[OR gate]] to combine their carry outputs, two half adders can be combined to make a full adder.<ref name=\"Lancaster_2004\"/> The half adder adds two input bits and generates a carry and sum, which are the two outputs of a half adder. The input [[Variable (computer science)|variables]] of a half adder are called the augend and addend bits. The output variables are the sum and carry. The [[truth table]] for the half adder is:\n\n:{| class=\"wikitable\" style=\"text-align:center\"\n|- \n! colspan=\"2\"| '''Inputs''' || colspan=\"2\"| '''Outputs'''\n|- style=\"background:#def; text-align:center;\"\n| '''A''' || '''B''' || '''C'''  || '''S'''\n|- style=\"background:#dfd; text-align:center;\"\n| 0 || 0 || 0  || 0\n|- style=\"background:#dfd; text-align:center;\"\n| 1 || 0 || 0  || 1\n|- style=\"background:#dfd; text-align:center;\"\n| 0 || 1 || 0  || 1\n|- style=\"background:#dfd; text-align:center;\"\n| 1 || 1 || 1  || 0\n|-\n|}\n[[File:Half adder using NAND gates only.jpg|alt=half adder circuit using NAND gates only|thumb|311x311px|Half adder using NAND gates only.]]\n\n===Full adder===\n[[File:Full-adder logic diagram.svg|thumbnail|Logic diagram for a full adder.]]\n\n[[File:Fulladder.gif|thumbnail|right|Full adder in action. A full adder gives the number of 1s in the input in binary representation.]]\n\n[[File:1-bit full-adder.svg|thumb|right|Schematic symbol for a 1-bit full adder with ''C''<sub>in</sub> and ''C''<sub>out</sub> drawn on sides of block to emphasize their use in a multi-bit adder]]\n\nA '''full adder''' adds binary numbers and accounts for values carried in as well as out. A one-bit full-adder adds three one-bit numbers, often written as ''A'', ''B'', and ''C''<sub>in</sub>; ''A'' and ''B'' are the operands, and ''C''<sub>in</sub> is a bit carried in from the previous less-significant stage.<ref name=\"Mano_1979\"/> The full adder is usually a component in a cascade of adders, which add 8, 16, 32, etc. bit binary numbers. The circuit produces a two-bit output. Output carry and sum typically represented by the signals ''C''<sub>out</sub> and ''S'', where the sum equals {{nowrap|2''C''<sub>out</sub> + ''S''}}.\n\nA full adder can be implemented in many different ways such as with a custom [[transistor]]-level circuit or composed of other gates. One example implementation is with {{nowrap|''S'' {{=}} ''A'' ⊕ ''B'' ⊕ ''C''<sub>in</sub>}} and {{nowrap|''C''<sub>out</sub> {{=}} (''A'' ⋅ ''B'') + (''C''<sub>in</sub> ⋅ (''A'' ⊕ ''B''))}}.\n\nIn this implementation, the final [[OR gate]] before the carry-out output may be replaced by an [[XOR gate]] without altering the resulting logic. Using only two types of gates is convenient if the circuit is being implemented using simple [[integrated circuit]] chips which contain only one gate type per chip.\n[[File:NorAdder.svg|thumb|right|150px|NOR [[Full adder]]]]\nA full adder can also be constructed from two half adders by connecting ''A'' and ''B'' to the input of one half adder, then taking its sum-output ''S'' as one of the inputs to the second half adder and ''C''<sub>in</sub> as its other input, and finally the carry outputs from the two half-adders are connected to an OR gate. The sum-output from the second half adder is the final sum output (''S'') of the full adder and the output from the OR gate is the final carry output (''C''<sub>out</sub>). The critical path of a full adder runs through both XOR gates and ends at the sum bit ''s''. Assumed that an XOR gate takes 1 delays to complete, the delay imposed by the critical path of a full adder is equal to\n:<math>T_\\text{FA} = 2 \\cdot T_\\text{XOR} = 2 D.</math>\nThe critical path of a carry runs through one XOR gate in adder and through 2 gates (AND and OR) in carry-block and therefore, if AND or OR gates take 1 delay to complete, has a delay of\n:<math>T_\\text{c} = T_\\text{XOR} + T_\\text{AND} + T_\\text{OR} = D + D + D = 3D.</math>\n\nThe [[truth table]] for the full adder is:\n\n:{| class=\"wikitable\" style=\"text-align:center\"\n|- style=\"background:#def; text-align:center;\"\n!colspan=\"3\"| '''Inputs''' || colspan=\"2\"| '''Outputs'''\n|- style=\"background:#def; text-align:center;\"\n| '''A''' || '''B''' || '''C'''<sub>in</sub> || '''C'''<sub>out</sub>  || '''S'''\n|- style=\"background:#dfd; text-align:center;\"\n| 0 || 0 || 0  || 0 || 0\n|- style=\"background:#dfd; text-align:center;\"\n| 0 || 0 || 1  || 0 || 1\n|- style=\"background:#dfd; text-align:center;\"\n| 0 || 1 || 0  || 0 || 1\n|- style=\"background:#dfd; text-align:center;\"\n| 0 || 1 || 1  || 1 || 0\n|- style=\"background:#dfd; text-align:center;\"\n| 1 || 0 || 0  || 0 || 1\n|- style=\"background:#dfd; text-align:center;\"\n| 1 || 0 || 1  || 1 || 0\n|- style=\"background:#dfd; text-align:center;\"\n| 1 || 1 || 0  || 1 || 0\n|- style=\"background:#dfd; text-align:center;\"\n| 1 || 1 || 1  || 1 || 1\n|-\n|}\n\n===Adders supporting multiple bits===\n====Ripple-carry adder====\n[[File:4-bit ripple carry adder.svg|thumb|4-bit adder with logical block diagram shown|alt=4-bit adder with logical block diagram shown]]\n[[File:RippleCarry2.gif|thumbnail|right|Decimal 4-digit ripple carry adder. FA = full adder, HA = half adder.]]\n\nIt is possible to create a logical circuit using multiple full adders to add ''N''-bit numbers. Each full adder inputs a ''C''<sub>in</sub>, which is the ''C''<sub>out</sub> of the previous adder. This kind of adder is called a '''ripple-carry adder''' (RCA), since each carry bit \"ripples\" to the next full adder. Note that the first (and only the first) full adder may be replaced by a half adder (under the assumption that ''C''<sub>in</sub> = 0).\n\nThe layout of a ripple-carry adder is simple, which allows fast design time; however, the ripple-carry adder is relatively slow, since each full adder must wait for the carry bit to be calculated from the previous full adder. The [[gate delay]] can easily be calculated by inspection of the full adder circuit. Each full adder requires three levels of logic. In a 32-bit ripple-carry adder, there are 32 full adders, so the critical path (worst case) delay is 3 (from input to carry in first adder) + 31 × 2 (for carry propagation in latter adders) = 65 gate delays.<ref name=\"Adder\"/>\nThe general equation for the worst-case delay for a ''n''-bit carry-ripple adder, accounting for both the sum and carry bits, is\n:<math>T_\\text{CRA}(n) = T_\\text{HA} + (n-1) \\cdot T_\\text{c} + T_\\text{s} = T_\\text{FA} + (n-1) \\cdot T_c = 3 D + (n-1) \\cdot 2 D = (2n+1) \\cdot D.</math>\n\nA design with alternating carry polarities and optimized [[AND-OR-Invert]] gates can be about twice as fast.<ref name=\"Burgess_2011\"/>[[File:4-bit carry lookahead adder.svg|thumb|right|4-bit adder with carry lookahead]]\n\n====Carry-lookahead adder====\n{{main|Carry-lookahead adder}}\n\nTo reduce the computation time, engineers devised faster ways to add two binary numbers by using [[carry-lookahead adder]]s (CLA). They work by creating two signals (''P'' and ''G'') for each bit position, based on whether a carry is propagated through from a less significant bit position (at least one input is a 1), generated in that bit position (both inputs are 1), or killed in that bit position (both inputs are 0).  In most cases, ''P'' is simply the sum output of a half adder and ''G'' is the carry output of the same adder.  After ''P'' and ''G'' are generated, the carries for every bit position are created.  Some advanced carry-lookahead architectures are the [[Manchester carry chain]], [[Brent–Kung adder]] (BKA),<ref name=\"Brent-Kung_1982\"/> and the [[Kogge–Stone adder]] (KSA).<ref name=\"Kogge-Stone_1973\"/><ref name=\"ULVD_2015\"/>\n\nSome other multi-bit adder architectures break the adder into blocks.  It is possible to vary the length of these blocks based on the [[propagation delay]] of the circuits to optimize computation time.  These block based adders include the [[carry-skip adder|carry-skip (or carry-bypass) adder]] which will determine ''P'' and ''G'' values for each block rather than each bit, and the [[carry-select adder]] which pre-generates the sum and carry values for either possible carry input (0 or 1) to the block, using multiplexers to select the appropriate result ''when'' the carry bit is known.\n\n[[File:64-bit lookahead carry unit.svg|thumb|right|A 64-bit adder]]\n\nBy combining multiple carry-lookahead adders, even larger adders can be created. This can be used at multiple levels to make even larger adders. For example, the following adder is a 64-bit adder that uses four 16-bit CLAs with two levels of LCUs.\n\nOther adder designs include the [[carry-select adder]], [[conditional sum adder]], [[carry-skip adder]], and carry-complete adder.\n\n====Carry-save adders====\n{{main|Carry-save adder}}\n\nIf an adding circuit is to compute the sum of three or more numbers, it can be advantageous to not propagate the carry result. Instead, three-input adders are used, generating two results: a sum and a carry. The sum and the carry may be fed into two inputs of the subsequent 3-number adder without having to wait for propagation of a carry signal. After all stages of addition, however, a conventional adder (such as the ripple-carry or the lookahead) must be used to combine the final sum and carry results.\n\n===3:2 compressors===\nA full adder can be viewed as a ''3:2 lossy compressor'': it sums three one-bit inputs and returns the result as a single two-bit number; that is, it maps 8 input values to 4 output values. Thus, for example, a binary input of 101 results in an output of {{nobr|1 + 0 + 1 {{=}} 10}} (decimal number 2). The carry-out represents bit one of the result, while the sum represents bit zero.  Likewise, a half adder can be used as a ''2:2 lossy compressor'', compressing four possible inputs into three possible outputs.\n\nSuch compressors can be used to speed up the summation of three or more addends.  If the addends are exactly three, the layout is known as the [[carry-save adder]].  If the addends are four or more, more than one layer of compressors is necessary, and there are various possible design for the circuit: the most common are [[Dadda tree|Dadda]] and [[Wallace tree]]s.  This kind of circuit is most notably used in multipliers, which is why these circuits are also known as Dadda and Wallace multipliers.\n\n==See also==\n* [[Subtractor]]\n* [[Electronic mixer]] — for adding analog signals\n\n==References==\n{{reflist|refs=\n<ref name=\"Lancaster_2004\">{{cite book |title=Excel HSC Software Design and Development |author-first=Geoffrey A. |author-last=Lancaster |publisher=Pascal Press |date=2004 |isbn=978-1-74125175-3 |page=180 |url=https://books.google.com/books?id=PZkDpS4m0fMC&pg=PA180}}</ref>\n<ref name=\"Mano_1979\">{{cite book |author-first=M. Morris |author-last=Mano |title=Digital Logic and Computer Design |publisher=[[Prentice-Hall]] |date=1979 |isbn=978-0-13-214510-7 |pages=119–123}}</ref>\n<ref name=\"Adder\">{{cite book |title=Design and Implementation of Carry Select Adder Using T-Spice |author-first=Pinaki |author-last=Satpathy |publisher=Anchor Academic Publishing |date=2016 |isbn=978-3-96067058-2 |page=22 |url=https://books.google.com/books?id=F_AWDQAAQBAJ&pg=PA22}}</ref>\n<ref name=\"Burgess_2011\">{{cite conference |conference=[[20th IEEE Symposium on Computer Arithmetic]] |title=Fast Ripple-Carry Adders in Standard-Cell CMOS VLSI |author-last=Burgess |author-first=Neil |pages=103–111 |date=2011 |url=http://ieeexplore.ieee.org/Xplore/login.jsp?url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F5991607%2F5992089%2F05992115.pdf%3Farnumber%3D5992115&authDecision=-203}}</ref>\n<ref name=\"Brent-Kung_1982\">{{cite journal |author-first1=Richard Peirce |author-last1=Brent |author-link1=Richard Peirce Brent |author-first2=Hsiang Te |author-last2=Kung |title=A Regular Layout for Parallel Adders |journal=[[IEEE Transactions on Computers]] |volume=C-31 |issue=3 |date=March 1982 |pages=260–264 |issn=0018-9340 |doi=10.1109/TC.1982.1675982|url=http://www.dtic.mil/get-tr-doc/pdf?AD=ADA074455 }}</ref>\n<ref name=\"Kogge-Stone_1973\">{{cite journal |author-last1=Kogge |author-first1=Peter Michael |author-link=Peter Michael Kogge |author-last2=Stone |author-first2=Harold S. |title=A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations |journal=[[IEEE Transactions on Computers]] |date=August 1973 |volume=C-22 |issue=8 |pages=786–793 |doi=10.1109/TC.1973.5009159}}</ref>\n<ref name=\"ULVD_2015\">{{cite book |title=Ultra-Low-Voltage Design of Energy-Efficient Digital Circuits |author-first1=Nele |author-last1=Reynders |author-first2=Wim |author-last2=Dehaene |series=Analog Circuits And Signal Processing (ACSP) |date=2015 |edition=1 |location=Heverlee, Belgium |publisher=[[Springer International Publishing AG Switzerland]] |publication-place=Cham, Switzerland |isbn=978-3-319-16135-8 |issn=1872-082X |doi=10.1007/978-3-319-16136-5 |lccn=2015935431}}</ref>\n}}\n\n==Further reading==\n* {{cite journal |title=Optimal One-Bit Full-Adders with Different Types of Gates |author-first1=Tso-Kai |author-last1=Liu |author-first2=Keith R. |author-last2=Hohulin |author-first3=Lih-Er |author-last3=Shiau |author-first4=Saburo |author-last4=Muroga |date=January 1974 |journal=[[IEEE Transactions on Computers]] |volume=C-23 |issue=1 |issn=0018-9340 |publisher=[[IEEE]] |location=Bell Laboratories |pages=63–70 |doi=10.1109/T-C.1974.223778 |url=https://ieeexplore.ieee.org/document/1672371/}}\n* {{cite journal |title=Minimum Binary Parallel Adders with NOR (NAND) Gates |author-first1=Hung Chi |author-last1=Lai |author-first2=Saburo |author-last2=Muroga |date=September<!-- October --> 1979 |journal=[[IEEE Transactions on Computers]] |volume=C-28 |issue=9 |publisher=[[IEEE]] |pages=648–659 |doi=10.1109/TC.1979.1675433 |url=https://ieeexplore.ieee.org/document/1675433/ |access-date=2018-05-12}}\n* {{cite book |title=Introduction to VLSI Systems |author-first1=Carver  |author-last1=Mead |author-first2=Lynn |author-last2=Conway |date=1980 |orig-year=December 1979 |edition=1 |publisher=[[Addison-Wesley]] |location=Reading, MA, USA |isbn=978-0-20104358-7 |url=https://archive.org/details/introductiontovl00mead |access-date=2018-05-12}}\n* {{cite book |title=Digital Systems, with algorithm implementation |author-first1=Marc |author-last1=Davio |author-first2=Jean-Pierre |author-last2=Dechamps |author-first3=André |author-last3=Thayse |date=1983 |edition=1 |publisher=[[John Wiley & Sons]], a Wiley-Interscience Publication |location=[[Philips Research Laboratory]], Brussels, Belgium |isbn=978-0-471-10413-1 |lccn=82-2710}}\n\n==External links==\n* [https://www.ecsis.riec.tohoku.ac.jp/topics/amg/i-amg/doc/algorithm Hardware algorithms for arithmetic modules], includes description of several adder layouts with figures.\n* [http://dev.code.ultimater.net/electronics/8-bit-full-adder-and-subtractor/ 8-bit Full Adder and Subtractor], a demonstration of an interactive Full Adder built in JavaScript solely for learning purposes.\n* [http://teahlab.com/Full_Adder/ Interactive Full Adder Simulation] (requires Java), Interactive Full Adder circuit constructed with Teahlab's online circuit simulator.\n* [http://teahlab.com/Half_Adder/ Interactive Half Adder Simulation] (requires Java),  Half Adder circuit built  with Teahlab's circuit simulator.\n* [http://www.edaplayground.com/s/example/368 4-bit Full Adder Simulation] built in Verilog, and the accompanying [https://www.youtube.com/watch?v=bL3ihMA8_Gs&hd=1 Ripple Carry Full Adder Video Tutorial]\n\n{{CPU technologies|state=collapsed}}\n\n{{DEFAULTSORT:Adder (Electronics)}}\n[[Category:Computer arithmetic]]\n[[Category:Adders (electronics)| ]]\n[[Category:Binary logic]]"
    },
    {
      "title": "Binary decoder",
      "url": "https://en.wikipedia.org/wiki/Binary_decoder",
      "text": "{{Unreferenced|date=May 2009}}\n\nIn [[digital electronics]], a '''binary decoder''' is a [[combinational logic]] circuit that converts binary information from the n coded inputs to a maximum of 2<sup>n</sup> unique outputs. They are used in a wide variety of applications, including data [[multiplexing|demultiplexing]], seven segment displays, and [[memory]] address decoding.\n\nThere are several types of binary decoders, but in all cases a decoder is an electronic circuit with multiple input and multiple output signals, which converts every unique combination of input states to a specific combination of output states. In addition to integer data inputs, some decoders also have one or more \"enable\" inputs. When the enable input is negated (disabled), all decoder outputs are forced to their inactive states.\n\nDepending on its function, a binary decoder will convert binary information from n input signals to as many as 2<sup>n</sup> unique output signals. Some decoders have less than 2<sup>n</sup> output lines; in such cases, at least one output pattern will be repeated for different input values.\n\nA binary decoder is usually implemented as either a stand-alone [[integrated circuit]] (IC) or as part of a more complex IC. In the latter case the decoder may be synthesized by means of a [[hardware description language]] such as [[VHDL]] or [[Verilog]]. Widely used decoders are often available in the form of standardized ICs.\n\n==Types of decoders==\n\n===1-of-n decoder===\n[[File:Decoder Example.svg|thumb|A 2-to-4 line decoder]]\n\nA 1-of-n binary decoder has n output bits. This type of decoder asserts exactly one of its n output bits, or none of them, for every integer input value. The \"address\" (bit number) of the activated output is specified by the integer input value. For example, output bit number 0 is selected when the integer value 0 is applied to the inputs.\n\nExamples of this type of decoder include:\n\n* A ''3-to-8 line decoder'' activates one of eight output bits for each input value from 0 to 7 — the range of integer values that can be expressed in three bits. Similarly, a ''4-to-16 line decoder'' activates one of 16 outputs for each 4-bit input in the integer range [0,15].\n* A ''BCD to decimal decoder'' has ten output bits. It accepts an input value consisting of a [[binary-coded decimal]] integer value and activates one specific, unique output for every input value in the range [0,9]. All outputs are held inactive when a non-decimal value is applied to the inputs.\n* A [[demultiplexer]] is a 1-of-n binary decoder that is used to route a data bit to one of its n outputs while all other outputs remain inactive.\n\n===Code translator===\n\nCode translators differ from 1-of-n decoders in that multiple output bits may be active at the same time. An example of this is a ''seven-segment decoder'', which converts an integer into the combination of segment control signals needed to display the integer's value on a [[seven-segment display]] digit.\n\nOne variant of seven-segment decoder is the ''BCD to seven-segment decoder'', which translates a binary-coded decimal value into the corresponding segment control signals for input integer values 0 to 9. This decoder function is available in standard ICs such as the CMOS [[4511]].\n\n==See also==\n{{Wiktionary|decoder}}\n* [[Sum addressed decoder]]\n* [[Multiplexer]]\n* [[Priority encoder]]\n\n[[Category:Digital circuits]]\n[[Category:Binary logic]]"
    },
    {
      "title": "Subtractor",
      "url": "https://en.wikipedia.org/wiki/Subtractor",
      "text": "{{Refimprove|date=December 2009}}\n{{ALUSidebar|expand=Components|expand-components=Subtractor}}\nIn [[electronics]], a '''subtractor''' can be designed using the same approach as that of an [[adder (electronics)|adder]]. The [[binary numeral system|binary]] subtraction process is summarized below. As with an adder, in the general case of calculations on multi-bit numbers, three [[bit]]s are involved in performing the subtraction for each bit of the [[Difference (mathematics)|difference]]: the [[minuend]] (<math>X_{i}</math>), [[subtrahend]] (<math>Y_{i}</math>), and a borrow in from the previous (less significant) bit order position (<math>B_{i}</math>). The outputs are the difference bit (<math>D_{i}</math>) and borrow bit <math>B_{i+1}</math>. The subtractor is best understood by considering that the subtrahend and both borrow bits have negative weights, whereas the X and D bits are positive. The operation performed by the subtractor is to rewrite <math>X_{i}-Y_{i}-B_{i}</math> (which can take the values -2, -1, 0, or 1) as the sum <math>-2B_{i+1}+D_{i}</math>.\n\n:<math> D_{i} = X_{} \\oplus Y_{i} \\oplus B_{i}</math>\n:<math> B_{i+1} = X_{i} < (Y_{i} + B_{i})</math>\n\nSubtractors are usually implemented within a binary adder for only a small cost when using the standard [[two's complement]] notation, by providing an addition/subtraction selector to the carry-in and to invert the second operand.\n\n:<math>-B = \\bar{B} + 1</math> (definition of two's complement notation)\n\n:<math>\\begin{alignat}{2}\nA - B & = A + (-B) \\\\\n& = A + \\bar{B} + 1 \\\\\n\\end{alignat}</math>\n\n==Half subtractor==\n[[File:Half subtractor corrected.png|thumbnail|Logic diagram for a half subtractor]]\nThe half subtractor is a [[logic circuit|combinational circuit]] which is used to perform subtraction of two bits. It has two inputs, the [[minuend]] <math>X</math> and [[subtrahend]] <math>Y</math> and two outputs the difference <math>D</math> and borrow out <math>B_\\text{out}</math>. The borrow out signal is set when the subtractor needs to borrow from the next digit in a multi-digit subtraction. That is, <math>B_{\\text{out}} = 1</math> when <math>X < Y</math>. Since <math>X</math> and <math>Y</math> are bits, <math>B_\\text{out} = 1</math> if and only if <math>X = 0</math> and <math>Y = 1</math>. An important point worth mentioning is that the half subtractor diagram aside implements <math>X - Y</math> and not <math>Y-X</math> since <math>B_\\text{out}</math> on the diagram is given by\n:<math>B_{\\text{out}} = \\overline{X} \\cdot Y</math>.\nThis is an important distinction to make since subtraction itself is not [[commutative]], but the difference bit <math>D</math> is calculated using an [[XOR gate]] which is commutative.\n[[File:Half subtractor using NAND.jpg|alt=Half-subtractor using NAND gate only.|thumb|333x333px|Half-subtractor using NAND gate only.]]\nThe [[truth table]] for the half subtractor is:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n!colspan=\"2\"|Inputs\n!colspan=\"2\"|Outputs\n|-\n|-\n! ''X''\n! ''Y''\n! ''D''\n! ''B''<sub>out</sub>\n|-\n| 0\n| 0\n| 0\n| 0\n|-\n| 0\n| 1\n| 1\n| 1\n|-\n| 1\n| 0\n| 1\n| 0\n|-\n| 1\n| 1\n| 0\n| 0\n|-\n|}\n\nUsing the table above and a [[Karnaugh map]], we find the following logic equations for <math>D</math> and <math>B_\\text{out}</math>:\n\n:<math>D = X \\oplus Y</math>\n:<math>B_\\text{out} = \\overline X \\cdot Y</math>.\n\nConsequently, a simplified half-subtract circuit, advantageously avoiding crossed traces in particular as well as a negate gate is:\n<pre>\n      X ── XOR ─┬─────── |X-Y|,  is 0 if X equals Y, 1 otherwise\n         ┌──┘   └──┐  \n      Y ─┴─────── AND ── borrow, is 1 if Y > X, 0 otherwise\n</pre> \nwhere lines to the right are outputs and others (from the top, bottom or left) are inputs.\n\n==Full subtractor==\nThe full subtractor is a [[logic circuit|combinational circuit]] which is used to perform subtraction of three input [[bit]]s: the minuend <math>X</math>, subtrahend <math>Y</math>, and borrow in <math>B_\\text{in}</math>. The full subtractor generates two output bits: the difference <math>D</math> and borrow out <math>B_\\text{out}</math>. <math>B_\\text{in}</math> is set when the previous digit is borrowed from <math>X</math>. Thus, <math>B_\\text{in}</math> is also subtracted from <math>X</math> as well as the subtrahend <math>Y</math>. Or in symbols: <math>X - Y - B_\\text{in}</math>. Like the half subtractor, the full subtractor generates a borrow out when it needs to borrow from the next digit. Since we are subtracting <math>Y</math> and <math>B_\\text{in}</math> from <math>X</math>, a borrow out needs to be generated when <math>X < Y + B_\\text{in}</math>. When a borrow out is generated, 2 is added in the current digit. (This is similar to the subtraction algorithm in decimal. Instead of adding 2, we add 10 when we borrow.) Therefore, <math>D = X - Y - B_\\text{in} + 2B_\\text{out}</math>.\n[[File:Full subtractor circuit .jpg|alt=Full subtractor circuit|thumb|473x473px|Full subtractor circuit]]\nThe truth table for the full subtractor is:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n!colspan=\"3\"| Inputs \n!colspan=\"2\"| Outputs\n|-\n! ''X''\n! ''Y''\n! ''B''<sub>in</sub>\n! ''D''\n! ''B''<sub>out</sub>\n|-\n| 0 || 0 || 0 || 0 || 0\n|-\n| 0 || 0 || 1 || 1 || 1\n|-\n| 0 || 1 || 0 || 1 || 1\n|-\n| 0 || 1 || 1 || 0 || 1\n|-\n| 1 || 0 || 0 || 1 || 0\n|-\n| 1 || 0 || 1 || 0 || 0\n|-\n| 1 || 1 || 0 || 0 || 0\n|-\n| 1 || 1 || 1 || 1 || 1\n|}\n\nTherefore the equation is D=X⊕Y⊕B<sub>in</sub> and ''B''<sub>out</sub>=X'B<sub>in</sub>+X'Y+YB<sub>in</sub>\n\n==See also==\n* [[Adder (electronics)]]\n* [[Carry-lookahead adder]]\n* [[Carry-save adder]]\n* [[Adding machine]]\n* [[Adder-subtractor]]\n\n==References==\n{{Reflist}}\n* Foundations Of Digital Electronics by Elijah Mwangi\n\n== External links ==\n* [http://www.fullchipdesign.com/binary_adder_subtractor.htm N bit Binary addition or subtraction using single circuit.]\n\n[[Category:Computer arithmetic]]\n[[Category:Binary logic]]\n[[Category:Adders (electronics)|4]]\n[[Category:Subtraction]]\n\n{{CPU technologies|state=collapsed}}"
    },
    {
      "title": "Bit array",
      "url": "https://en.wikipedia.org/wiki/Bit_array",
      "text": "{{refimprove|date=December 2010}}\n\nA '''bit array''' (also known as '''bit map''', '''bit set''', '''bit string''', or '''bit vector''') is an [[array data structure]] that compactly stores [[bit]]s. It can be used to implement a simple [[set data structure]]. A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores ''kw'' bits, where ''w'' is the number of bits in the unit of storage, such as a [[byte]] or [[Word (computer architecture)|word]], and ''k'' is some nonnegative integer. If ''w'' does not divide the number of bits to be stored, some space is wasted due to [[Fragmentation (computing)|internal fragmentation]].\n\n== Definition ==\nA bit array is a mapping from some domain (almost always a range of integers) to values in the set {0, 1}. The values can be interpreted as dark/light, absent/present, locked/unlocked, valid/invalid, et cetera. The point is that there are only two possible values, so they can be stored in one bit. As with other arrays, the access to a single bit can be managed by applying an index to the array. Assuming its size (or length) to be ''n'' bits, the array can be used to specify a subset of the domain (e.g. {0, 1, 2, ..., ''n''&minus;1}), where a 1-bit indicates the presence and a 0-bit the absence of a number in the set. This set data structure uses about ''n''/''w'' words of space, where ''w'' is the number of bits in each [[Word (computer architecture)|machine word]]. Whether the least significant bit (of the word) or the most significant bit indicates the smallest-index number is largely irrelevant, but the former tends to be preferred (on [[Endianness|little-endian]] machines).\n\n== Basic operations ==\nAlthough most machines are not able to address individual bits in memory, nor have instructions to manipulate single bits, each bit in a word can be singled out and manipulated using [[bitwise operation]]s. In particular:\n* OR can be used to set a bit to one: 11101010 OR 00000100 = 11101110\n* AND can be used to set a bit to zero: 11101010 AND 11111101 = 11101000\n* AND together with zero-testing can be used to determine if a bit is set:\n::11101010 AND 00000001 = 00000000 = 0\n::11101010 AND 00000010 = 00000010 ≠ 0\n* XOR can be used to invert or toggle a bit:\n::11101010 XOR 00000100 = 11101110\n::11101110 XOR 00000100 = 11101010\n* NOT can be used to invert all bits.\n::NOT 10110010 = 01001101\n\nTo obtain the [[Mask_(computing)|bit mask]] needed for these operations, we can use a [[Bitwise operation#Bit shifts|bit shift]] operator to shift the number 1 to the left by the appropriate number of places, as well as [[bitwise negation]] if necessary.\n\nGiven two bit arrays of the same size representing sets, we can compute their [[union (set theory)|union]], [[intersection (set theory)|intersection]], and [[complement (set theory)|set-theoretic difference]] using ''n''/''w'' simple bit operations each (2''n''/''w'' for difference), as well as the [[Signed number representations#Ones' complement|complement]] of either:\n<syntaxhighlight lang=\"text\">\nfor i from 0 to n/w-1\n    complement_a[i] := not a[i]\n    union[i]        := a[i] or b[i]\n    intersection[i] := a[i] and b[i]\n    difference[i]   := a[i] and (not b[i])\n</syntaxhighlight>\n\nIf we wish to iterate through the bits of a bit array, we can do this efficiently using a doubly nested loop that loops through each word, one at a time. Only ''n''/''w'' memory accesses are required:\n<syntaxhighlight lang=\"text\">\nfor i from 0 to n/w-1\n    index := 0    // if needed\n    word := a[i]\n    for b from 0 to w-1\n        value := word and 1 ≠ 0\n        word := word shift right 1\n        // do something with value\n        index := index + 1    // if needed\n</syntaxhighlight>\nBoth of these code samples exhibit ideal [[locality of reference]], which will subsequently receive large performance boost from a data cache. If a cache line is ''k'' words, only about ''n''/''wk'' cache misses will occur.\n\n== More complex operations ==\n\nAs with [[String (computer science)|character strings]] it is straightforward to define ''length'', ''substring'', [[lexicographical order|lexicographical]] ''compare'', ''concatenation'', ''reverse'' operations. The implementation of some of these operations is sensitive to [[endianness]].\n\n=== Population / Hamming weight ===\nIf we wish to find the number of 1 bits in a bit array, sometimes called the population count or Hamming weight, there are efficient branch-free algorithms that can compute the number of bits in a word using a series of simple bit operations. We simply run such an algorithm on each word and keep a running total. Counting zeros is similar. See the [[Hamming weight]] article for examples of an efficient implementation.\n\n=== Inversion ===\n\nVertical flipping of a one-bit-per-pixel image, or some FFT algorithms, requires flipping the bits of individual words (so <code>b31 b30 ... b0</code> becomes <code>b0 ... b30 b31</code>).\nWhen this operation is not available on the processor, it's still possible to proceed by successive passes, in this example on 32 bits:\n\n<syntaxhighlight lang=\"c\">\nexchange two 16bit halfwords\nexchange bytes by pairs (0xddccbbaa -> 0xccddaabb)\n...\nswap bits by pairs\nswap bits (b31 b30 ... b1 b0 -> b30 b31 ... b0 b1)\n\nThe last operation can be written ((x&0x55555555)<<1) | (x&0xaaaaaaaa)>>1)).\n</syntaxhighlight>\n\n=== Find first one ===\nThe [[find first set]] or ''find first one'' operation identifies the index or position of the 1-bit with the smallest index in an array, and has widespread hardware support (for arrays not larger than a word) and efficient algorithms for its computation. When a [[priority queue]] is stored in a bit array, find first one can be used to identify the highest priority element in the queue. To expand a word-size ''find first one'' to longer arrays, one can find the first nonzero word and then run ''find first one'' on that word. The related operations ''find first zero'', ''count leading zeros'', ''count leading ones'', ''count trailing zeros'', ''count trailing ones'', and ''log base 2'' (see [[find first set]]) can also be extended to a bit array in a straightforward manner.\n\n== Compression ==\n\nA bit array is the most dense storage for \"random\" bits, that is, where each bit is equally likely to be 0 or 1, and each one is independent. But most data is not random, so it may be possible to store it more compactly. For example, the data of a typical fax image is not random and can be compressed. [[Run-length encoding]] is commonly used to compress these long streams. However, most compressed data formats are not so easy to access randomly; also by compressing bit arrays too aggressively we run the risk of losing the benefits due to bit-level parallelism ([[Array programming|vectorization]]). Thus, instead of compressing bit arrays as streams of bits, we might compress them as streams of bytes or words (see [[Bitmap_index#Compression|Bitmap index (compression)]]).\n\n== Advantages and disadvantages ==\nBit arrays, despite their simplicity, have a number of marked advantages over other data structures for the same problems:\n* They are extremely compact; no other data structures can store ''n'' independent pieces of data in ''n''/''w'' words.\n* They allow small arrays of bits to be stored and manipulated in the register set for long periods of time with no memory accesses.\n* Because of their ability to exploit bit-level parallelism, limit memory access, and maximally use the [[data cache]], they often outperform many other data structures on practical data sets, even those that are more asymptotically efficient.\nHowever, bit arrays aren't the solution to everything. In particular:\n* Without compression, they are wasteful set data structures for sparse sets (those with few elements compared to their range) in both time and space. For such applications, compressed bit arrays, [[Judy array]]s, [[trie]]s, or even [[Bloom filter]]s should be considered instead.\n* Accessing individual elements can be expensive and difficult to express in some languages. If random access is more common than sequential and the array is relatively small, a byte array may be preferable on a machine with byte addressing. A word array, however, is probably not justified due to the huge space overhead and additional cache misses it causes, unless the machine only has word addressing.\n\n== Applications ==\nBecause of their compactness, bit arrays have a number of applications in areas where space or efficiency is at a premium. Most commonly, they are used to represent a simple group of boolean flags or an ordered sequence of boolean values.\n\nBit arrays are used for [[priority queue]]s, where the bit at index ''k'' is set if and only if ''k'' is in the queue; this data structure is used, for example, by the [[Linux kernel]], and benefits strongly from a find-first-zero operation in hardware.\n\nBit arrays can be used for the allocation of [[page (computing)|memory pages]], [[inode]]s, disk sectors, etc. In such cases, the term ''bitmap'' may be used. However, this term is frequently used to refer to [[raster graphics|raster images]], which may use multiple [[color depth|bits per pixel]].\n\nAnother application of bit arrays is the [[Bloom filter]], a probabilistic [[set data structure]] that can store large sets in a small space in exchange for a small probability of error. It is also possible to build probabilistic [[hash table]]s based on bit arrays that accept either false positives or false negatives.\n\nBit arrays and the operations on them are also important for constructing [[succinct data structure]]s, which use close to the minimum possible space. In this context, operations like finding the ''n''th 1 bit or counting the number of 1 bits up to a certain position become important.\n\nBit arrays are also a useful abstraction for examining streams of [[data compression|compressed]] data, which often contain elements that occupy portions of bytes or are not byte-aligned. For example, the compressed [[Huffman coding]] representation of a single 8-bit character can be anywhere from 1 to 255 bits long.\n\nIn [[information retrieval]], bit arrays are a good representation for the [[posting list]]s of very frequent terms. If we compute the gaps between adjacent values in a list of strictly increasing integers and encode them using [[unary coding]], the result is a bit array with a 1 bit in the ''n''th position if and only if ''n'' is in the list. The implied probability of a gap of ''n'' is 1/2<sup>''n''</sup>. This is also the special case of [[Golomb coding]] where the parameter M is 1; this parameter is only normally selected when -log(2-''p'')/log(1-''p'') ≤ 1, or roughly the term occurs in at least 38% of documents.\n\n== Language support ==\nThe [[APL (programming language)|APL programming language]] fully supports bit arrays of arbitrary shape and size as a Boolean datatype distinct from integers. All major implementations ([[APL_(programming_language)#Execution|Dyalog APL, APL2, APL Next, NARS2000, Gnu APL]], etc.) pack the bits densely into whatever size the machine word is. Bits may be accessed individually via the usual indexing notation (A[3]) as well as through all of the usual primitive functions and operators where they are often operated on using a special case algorithm such as summing the bits via a table lookup of bytes.\n\nThe [[C (programming language)|C programming language]]'s ''[[bit field]]s'', pseudo-objects found in structs with size equal to some number of bits, are in fact small bit arrays; they are limited in that they cannot span words. Although they give a convenient syntax, the bits are still accessed using bitwise operators on most machines, and they can only be defined statically (like C's static arrays, their sizes are fixed at compile-time). It is also a common idiom for C programmers to use words as small bit arrays and access bits of them using bit operators. A widely available header file included in the [[X11]] system, xtrapbits.h, is “a portable way for systems to define bit field manipulation of arrays of bits.” A more explanatory description of aforementioned approach can be found in the [http://c-faq.com/misc/bitsets.html comp.lang.c faq].\n\nIn [[C++]], although individual <code>bool</code>s typically occupy the same space as a byte or an integer, the [[Standard Template Library|STL]] type <code>vector&lt;bool&gt;</code> is a [[partial template specialization]] in which bits are packed as a space efficiency optimization. Since bytes (and not bits) are the smallest addressable unit in C++, the [] operator does ''not'' return a reference to an element, but instead returns a [[Proxy pattern|proxy reference]]. This might seem a minor point, but it means that <code>vector&lt;bool&gt;</code> is ''not'' a standard STL container, which is why the use of <code>vector&lt;bool&gt;</code> is generally discouraged. Another unique STL class, <code>bitset</code>,<ref name=\"c++\" /> creates a vector of bits fixed at a particular size at compile-time, and in its interface and syntax more resembles the idiomatic use of words as bit sets by C programmers. It also has some additional power, such as the ability to efficiently count the number of bits that are set. The [[Boost C++ Libraries]] provide a <code>dynamic_bitset</code> class<ref name=\"boost\" /> whose size is specified at run-time.\n\nThe [[D programming language]] provides bit arrays in its standard library, Phobos, in <code> std.bitmanip</code>. As in C++, the [] operator does not return a reference, since individual bits are not directly addressable on most hardware, but instead returns a <code>bool</code>.\n\nIn [[Java (programming language)|Java]], the class {{Javadoc:SE|java/util|BitSet}} creates a bit array that is then manipulated with functions named after bitwise operators familiar to C programmers. Unlike the <code>bitset</code> in C++, the Java <code>BitSet</code> does not have a \"size\" state (it has an effectively infinite size, initialized with 0 bits); a bit can be set or tested at any index. In addition, there is a class {{Javadoc:SE|java/util|EnumSet}}, which represents a Set of values of an [[enumerated type]] internally as a bit vector, as a safer alternative to bit fields.\n\nThe [[.NET Framework]] supplies a <code>BitArray</code> collection class. It stores boolean values, supports random access and bitwise operators, can be iterated over, and its <code>Length</code> property can be changed to grow or truncate it.\n\nAlthough [[Standard ML]] has no support for bit arrays, Standard ML of New Jersey has an extension, the <code>BitArray</code> structure, in its SML/NJ Library. It is not fixed in size and supports set operations and bit operations, including, unusually, shift operations.\n\n[[Haskell (programming language)|Haskell]] likewise currently lacks standard support for bitwise operations, but both GHC and Hugs provide a <code>Data.Bits</code> module with assorted bitwise functions and operators, including shift and rotate operations and an \"unboxed\" array over boolean values may be used to model a Bit array, although this lacks support from the former module.\n\nIn [[Perl]], strings can be used as expandable bit arrays. They can be manipulated using the usual bitwise operators (<code>~ | & ^</code>),<ref>{{cite web|url=http://perldoc.perl.org/perlop.html#Bitwise-String-Operators|title=perlop - perldoc.perl.org|website=perldoc.perl.org}}</ref> and individual bits can be tested and set using the ''vec'' function.<ref>{{cite web|url=http://perldoc.perl.org/functions/vec.html|title=vec - perldoc.perl.org|website=perldoc.perl.org}}</ref>\n\nIn [[Ruby (programming language)|Ruby]], you can access (but not set) a bit of an integer (<code>Fixnum</code> or <code>Bignum</code>) using the bracket operator (<code>[]</code>), as if it were an array of bits.\n\nApple's [[Core Foundation]] library contains [https://developer.apple.com/library/mac/#documentation/CoreFoundation/Reference/CFBitVectorRef/Reference/reference.html CFBitVector] and [https://developer.apple.com/library/mac/#documentation/CoreFoundation/Reference/CFMutableBitVectorRef/Reference/reference.html#//apple_ref/doc/uid/20001500 CFMutableBitVector] structures.\n\n[[PL/I]] supports arrays of ''bit strings'' of arbitrary length, which may be either fixed-length or varying. The array elements may be ''aligned''&mdash; each element begins on a byte or word boundary&mdash; or ''unaligned''&mdash; elements immediately follow each other with no padding.\n\n[[PL/pgSQL]] and PostgreSQL's SQL support ''bit strings'' as native type. There are two SQL bit types: <code>bit(''<code>n</code>'')</code> and <code>bit varying(''<code>n</code>'')</code>, where ''<code>n</code>'' is a positive integer.<ref>https://www.postgresql.org/docs/current/datatype-bit.html</ref>\n\nHardware description languages such as [[VHDL]], [[Verilog]], and [[SystemVerilog]] natively support bit vectors as these are used to model storage elements like [[Flip-flop (electronics)|flip-flops]], hardware busses and hardware signals in general. In hardware verification languages such as [[OpenVera]], [[e (verification language)|''e'']] and [[SystemVerilog]], bit vectors are used to sample values from the hardware models, and to represent data that is transferred to hardware during simulations.\n\n==See also==\n* [[Binary code]]\n* [[Bit field]]\n* [[Arithmetic logic unit]]\n* [[Bitboard]] Chess and similar games.\n* [[Bitmap index]]\n* [[Binary numeral system]]\n* [[Bitstream]]\n* [[Judy array]]\n\n==References==\n{{Reflist | refs =\n<ref name=\"c++\">{{cite web|url=http://www.sgi.com/tech/stl/bitset.html|title=SGI.com Tech Archive Resources now retired|date=2 January 2018|publisher=[[Silicon Graphics|SGI]]}}</ref>\n<ref name=\"boost\">{{cite web|url=http://www.boost.org/libs/dynamic_bitset/dynamic_bitset.html|title=dynamic_bitset<Block, Allocator> - 1.66.0|website=www.boost.org}}</ref>\n}}\n\n==External links==\n* [http://www-cs-faculty.stanford.edu/~knuth/fasc1a.ps.gz mathematical bases] by Pr. D.E.Knuth\n* [http://www.gotw.ca/publications/N1185.pdf vector&lt;bool&gt; Is Nonconforming, and Forces Optimization Choice]\n* [http://www.gotw.ca/publications/N1211.pdf vector&lt;bool&gt;: More Problems, Better Solutions]\n\n{{Data structures}}\n\n[[Category:Arrays]]\n[[Category:Bit data structures]]"
    },
    {
      "title": "Bit field",
      "url": "https://en.wikipedia.org/wiki/Bit_field",
      "text": "A '''bit field''' is a [[data structure]] used in [[computer programming]].  It consists of a number of adjacent [[computer memory]] locations which have been allocated to hold a sequence of [[bit]]s, stored so that any single bit or group of bits within the set can be addressed.<ref name=\"BrummBrumm1988\">{{cite book|author1=Penn Brumm|author2=Don Brumm|title=80386 Assembly Language: A Complete Tutorial and Subroutine Library|url=https://books.google.com/books?id=qjkiAQAAIAAJ|date=August 1988|publisher=McGraw-Hill School Education Group|isbn=978-0-8306-9047-3|page=606}}</ref><ref name=\"Oualline1997\">{{cite book|author=Steve Oualline|title=Practical C Programming|url=https://books.google.com/books?id=RzmsANQ4gaAC&pg=PA403|year=1997|publisher=\"O'Reilly Media, Inc.\"|isbn=978-1-56592-306-5|pages=403–}}</ref> A bit field is most commonly used to represent [[Primitive data type|integral type]]s of known, fixed bit-width.\n\nThe meaning of the individual bits within the field is determined by the programmer; for example, the first bit in a bit field (located at the field's [[base address]]) is sometimes used to determine the state of a particular attribute associated with the bit field.<ref name=\"Miller1992\">{{cite book|author=Michael A. Miller|title=The 68000 Microprocessor Family: Architecture, Programming, and Applications|url=https://books.google.com/books?id=6qAkAQAAIAAJ|date=January 1992|publisher=Merrill|isbn=978-0-02-381560-7|page=323}}</ref>\n\nWithin [[microprocessor]]s and other logic devices, collections of bit fields called \"flags\" are commonly used to control or to indicate the intermediate state or outcome of particular operations.<ref name=\"GriffithsAdams2010\">{{cite book|author1=Ian Griffiths|author2=Matthew Adams|author3=Jesse Liberty|title=Programming C# 4.0: Building Windows, Web, and RIA Applications for the .NET 4.0 Framework|url=https://books.google.com/books?id=8graYKZ7rhIC&pg=PA81|date=30 July 2010|publisher=\"O'Reilly Media, Inc.\"|isbn=978-1-4493-9972-6|pages=81–}}</ref> Microprocessors typically have a [[status register]] that is composed of such flags, used to indicate various post-operation conditions, for example an [[arithmetic overflow]]. The flags can be read and used to decide subsequent operations, such as in processing conditional [[Branch (computer science)|jump instruction]]s. For example, a [[Branch (computer science)|<syntaxhighlight lang=\"asm\" inline>JE ...</syntaxhighlight>]] (Jump if Equal) instruction in the [[x86 assembly language#Programming flow|x86 assembly language]] will result in a jump if [[Status register|the Z (zero) flag]] was set by some previous operation.\n\nA bit field is distinguished from a [[bit array]] in that the latter is used to store a large set of bits indexed by integers and is often wider than any integral type supported by the language.{{cn|date=January 2019}} Bit fields, on the other hand, typically fit within a machine [[Word (data type)|word]],<ref name=\"Miller1992\" /> and the denotation of bits is independent of their numerical index.<ref name=\"Oualline1997\" />\n\n== Implementation ==\n\nBit fields can be used to reduce memory consumption when a program requires a number of integer variables which always will have low values. For example, in many systems storing an integer value requires two bytes (16-bits) of memory; sometimes the values to be stored actually need only one or two bits. Having a number of these tiny variables share a bit field allows efficient packaging of data in the memory.<ref name=\"Mimar1991\">{{cite book|author=Tibet Mimar|title=Programming and Designing with the 68000 Family: Including 68000, 68010/12, 68020, and the 68030|url=https://books.google.com/books?id=arFQAAAAMAAJ|year=1991|publisher=Prentice Hall|isbn=978-0-13-731498-0|page=275}}</ref>  \n\nIn C and C++, native implementation-defined bit fields can be created using unsigned int, signed int, or (in C99:) _Bool. In this case, the programmer can declare a structure for a bit field which labels and determines the width of several subfields.<ref>{{cite book|last=Prata|first=Stephen|title=C primer plus|year=2007|publisher=Sams|location=Indianapolis, Ind|isbn=0-672-32696-5|edition=5th}}</ref> Adjacently declared bit fields of the same type can then be packed by the compiler into a reduced number of words, compared with the memory used if each 'field' were to be declared separately.\n\nFor languages lacking native bitfields, or where the programmer wants strict control over the resulting bit representation, it is possible to manually manipulate bits within a larger word type. In this case, the programmer can set, test, and change the bits in the field using combinations of [[Mask (computing)|masking]] and [[bitwise operations]]. <ref name=\"Daggett2013\">{{cite book|author=Mark E. Daggett|title=Expert JavaScript|url=https://books.google.com/books?id=MPBZAgAAQBAJ&pg=PA68|date=13 November 2013|publisher=Apress|isbn=978-1-4302-6097-4|pages=68–}}</ref>\n\n== Examples ==\n===C programming language===\nDeclaring a bit field in [[C (programming language)|C]] and [[C++]]:\n<source lang=\"c\">\n// opaque and show\n#define YES 1\n#define NO  0\n\n// line styles\n#define SOLID  1\n#define DOTTED 2\n#define DASHED 3\n\n// primary colors\n#define BLUE  4  /* 100 */\n#define GREEN 2  /* 010 */\n#define RED   1  /* 001 */\n\n// mixed colors\n#define BLACK   0                    /* 000 */\n#define YELLOW  (RED | GREEN)        /* 011 */\n#define MAGENTA (RED | BLUE)         /* 101 */\n#define CYAN    (GREEN | BLUE)       /* 110 */\n#define WHITE   (RED | GREEN | BLUE) /* 111 */\n\nconst char * colors[8] = {\"Black\", \"Red\", \"Green\", \"Yellow\", \"Blue\", \"Magenta\", \"Cyan\", \"White\"};\n\n// bit field box properties\nstruct box_props {\n     unsigned int opaque       : 1;\n     unsigned int fill_color   : 3;\n     unsigned int              : 4; // fill to 8 bits\n     unsigned int show_border  : 1;\n     unsigned int border_color : 3;\n     unsigned int border_style : 2;\n     unsigned char             : 0; // fill to nearest byte (16 bits)\n     unsigned char width       : 4, // Split a byte into 2 fields of 4 bits\n                   height      : 4;\n};\n</source>\n<ref>{{cite book|last=Prata|first=Stephen|title=C primer plus|year=2007|publisher=Sams|location=Indianapolis, Ind|isbn=0-672-32696-5|edition=5th}}</ref>\n\nThe layout of bit fields in a C <code>struct</code> is [[Unspecified behavior|implementation-defined]]. For behavior that remains predictable across compilers, it may be preferable to emulate bit fields with a primitive and bit operators:\n<source lang=\"c\">\n/* Each of these preprocessor directives defines a single bit,\n   corresponding to one button on the controller.  Button order\n   matches that of the Nintendo Entertainment System. */\n#define KEY_RIGHT    (1 << 0)  /* 00000001 */\n#define KEY_LEFT     (1 << 1)  /* 00000010 */\n#define KEY_DOWN     (1 << 2)  /* 00000100 */\n#define KEY_UP       (1 << 3)  /* 00001000 */\n#define KEY_START    (1 << 4)  /* 00010000 */\n#define KEY_SELECT   (1 << 5)  /* 00100000 */\n#define KEY_B        (1 << 6)  /* 01000000 */\n#define KEY_A        (1 << 7)  /* 10000000 */\n\nint gameControllerStatus = 0;\n\n/* Sets the gameControllerStatus using OR */\nvoid keyPressed(int key) \n{\n     gameControllerStatus |= key;\n}\n\n/* Turns the key in gameControllerStatus off using AND and ~ (binary NOT)*/\nvoid keyReleased(int key) \n{\n    gameControllerStatus &= ~key;\n}\n\n/* Tests whether a bit is set using AND */\nint isPressed(int key) \n{\n    return gameControllerStatus & key;\n}\n</source>\n\n{{cleanup|reason=moved from [[Flag field]]|date=May 2016}}\n\n===Processor status register===\nA simple example of a bitfield [[status register]] is included in the design of the eight-bit [[MOS Technology 6502|6502]] processor.  One eight-bit field held seven pieces of information:<ref>{{cite book|title=InCider|url=https://books.google.com/books?id=AFpRAAAAYAAJ|date=January 1986|publisher=W. Green|page=108}}</ref>\n* Bit 7. Negative flag\n* Bit 6. Overflow flag\n* Bit 5. Unused\n* Bit 4. Break flag\n* Bit 3. Decimal flag\n* Bit 2. Interrupt-disable flag\n* Bit 1. Zero flag\n* Bit 0. Carry flag\n\n===Extracting bits from flag words===\n\nA subset of flags in a flag field may be extracted by [[Logical conjunction|AND]]ing with a [[Mask (computing)|mask]]. In addition, a large number of languages, due to the [[logical shift|shift]] operator's (&lt;&lt;) use in performing power-of-two (<code>(1 << n)</code> evaluates to <math>2^n</math>) exponentiation, also support the use of the [[logical shift|shift]] operator (&lt;&lt;) in combination with the [[Logical conjunction|AND]] operator (&) to determine the value of one or more bits.\n\nSuppose that the status-byte 103 (decimal) is returned, and that within the status-byte we want to check the 5th flag bit. The flag of interest (literal bit-position 6) is the 5th one - so the mask-byte will be <math>2^5 = 32</math>. [[Logical conjunction|AND]]ing the status-byte 103 (<code>0110&nbsp;0111</code> in binary) with the mask-byte 32 (<code>0010&nbsp;0000</code> in binary) evaluates to 32, our original mask-byte, which means the flag bit is set; alternatively, if the flag-bit had not been set, this operation would have evaluated to 0.\n\nThus, to check the '''n'''th bit from a variable '''v''', we can perform the operation:\n bool nth_is_set = ('''v''' & (1 << '''n''')) != 0;\n bool nth_is_set = ('''v''' >> '''n''') & 1;\n\n===Changing bits in flag words===\n\nWriting, reading or toggling bits in flags can be done only using the OR, AND and NOT operations - operations which can be performed quickly in the processor. To set a bit, [[Logical operator|OR]] the status byte with a mask byte. Any bits set in the mask byte or the status byte will be set in the result.\n\nTo toggle a bit, [[Exclusive disjunction|XOR]] the status byte and the mask byte. This will set a bit if it is cleared or clear a bit if it is set.\n\n==See also==\n* [[binary code]]\n* [[Bitboard]], used in chess and similar games.\n* [[Bit array]] (or bit string)\n* [[Word (computer architecture)]]\n* [[Mask (computing)]]\n* [[Program status word]]\n* [[Status register]]\n* [[FLAGS register (computing)]]\n* [[Control register]]\n\n== External links ==\n* [http://publications.gbdirect.co.uk/c_book/chapter6/bitfields.html Explanation from a book]\n* [http://c2.com/cgi/wiki?BitField Description from another wiki]\n* [https://web.archive.org/web/20080203070726/http://www.informit.com/guides/content.aspx?g=cplusplus&seqNum=131 Use case in a C++ guide]\n* [https://web.archive.org/web/20070428213243/http://libbit.sourceforge.net/ C++ libbit bit library] ([http://sourceforge.net/projects/libbit/ alternative URL])\n\n== References ==\n<references />\n\n[[Category:Bit data structures]]\n[[Category:Articles with example C code]]"
    },
    {
      "title": "Bit plane",
      "url": "https://en.wikipedia.org/wiki/Bit_plane",
      "text": "{{about|the digital information term|the company|Bitplane|the Bureau of Inverse Technology|BIT plane}}\n[[File:Lichtenstein bitplanes.png|thumb|right|400px|The 8 bit-planes of a gray-scale image (the one on left). There are eight because the original image uses eight bits per pixel.]]\n\nA '''bit plane''' of a [[Digital data|digital]] [[discrete signal]] (such as image or sound) is a set of [[bit]]s corresponding to a given bit position in each of the [[binary number]]s representing the signal.<ref>{{cite web\n  | last =\n  | first =\n  | authorlink =\n  | title =Bit Plane\n  | work =\n  | publisher =PC Magazine\n  | url =https://www.pcmag.com/encyclopedia_term/0,2542,t=bit+plane&i=38689,00.asp\n  | format =\n  | doi =\n  | accessdate =2007-05-02  }}</ref>\n\nFor example, for [[16-bit]] data representation there are 16 bit planes: the first bit plane contains the set of the most significant bit, and the 16th contains the least significant bit.\n\nIt is possible to see that the first bit plane gives the roughest but the most critical approximation of values of a medium, and the higher the number of the bit plane, the less is its contribution to the final stage. Thus, adding a bit plane gives a better approximation.\n\nIf a bit on the nth bit plane on an m-bit dataset is set to 1, it contributes a value of 2<sup>(m-n)</sup>, otherwise it contributes nothing. Therefore, bit planes can contribute half of the value of the previous bit plane. For example, in the 8-bit value 10110101 (181 in decimal) the bit planes work as follows:\n\n{| class=\"wikitable\"\n|-\n! Bit Plane !! Value !! Contribution !! Running Total\n|-\n| 1st || 1 || 1 × 2<sup>7</sup> = 128|| 128\n|-\n| 2nd || 0 || 0 × 2<sup>6</sup> = 0 || 128\n|-\n| 3rd || 1 || 1 × 2<sup>5</sup> = 32|| 160\n|-\n| 4th || 1 || 1 × 2<sup>4</sup> = 16 || 176\n|-\n| 5th || 0 || 0 × 2<sup>3</sup> = 0 || 176\n|-\n| 6th || 1 || 1 × 2<sup>2</sup> = 4 || 180\n|-\n| 7th || 0 || 0 × 2<sup>1</sup> = 0|| 180\n|-\n| 8th || 1 || 1 × 2<sup>0</sup> = 1 || 181\n|}\n\nBit plane is sometimes used as synonymous to [[Bitmap]]; however, technically the former refers to the location of the data in memory and the latter to the data itself.<ref>{{cite web\n  | last =\n  | first =\n  | authorlink =\n  | title =Bit Plane\n  | work =\n  | publisher =FOLDOC\n  | url =http://foldoc.org/foldoc.cgi?bit+plane\n  | format =\n  | doi =\n  | accessdate =2007-05-02  }}</ref>\n\nOne aspect of using bit-planes is determining whether a bit-plane is random noise or contains significant information.\n\nOne method for calculating this is compare each pixel (X,Y) to three adjacent pixels (X-1,Y), (X,Y-1) and (X-1,Y-1). If the pixel is the same as at least two of the three adjacent pixels, it is not noise. A noisy bit-plane will have 49% to 51% pixels that are noise.<ref>{{cite journal\n  | last =Strutz\n  | first =Tilo\n  | authorlink =\n  | title =Fast Noise Suppression for Lossless Image Coding\n  | journal =Proceedings of Picture Coding Symposium (PCS'2001), Seoul, Korea\n  | volume =\n  | issue =\n  | pages =\n  | publisher =\n  | year =2001\n  | url =http://citeseer.ist.psu.edu/strutz01fast.html\n  | doi =\n  | id =\n  | accessdate =2008-01-15  }}</ref>\n\n== Applications ==\n\n=== Media File Formats ===\nAs an example, in [[Pulse-code modulation|PCM]] sound [[code|encoding]] the first bit in the sample denotes the sign of the function, or in other words defines the half of the whole [[amplitude]] values range, and the last bit defines the precise value. Replacement of more significant bits result in more distortion than replacement of less significant bits. In lossy [[Data compression|media compression]] that uses bit-planes it gives more freedom to encode less significant bit-planes and it is more critical to preserve the more significant ones.<ref>{{cite journal\n  | last =Cho\n  | first =Chuan-Yu\n  | authorlink =\n  | author2 =Chen, Hong-Sheng | author3 = Wang, Jia-Shung\n  | title =Smooth Quality Streaming With Bit-Plane Labelling\n  | journal =Visual Communications and Image Processing\n  | volume =5690\n  | issue =\n  | pages =2184–2195\n  | publisher =The International Society for Optical Engineering\n  | date =July 2006\n  | bibcode =2005SPIE.5960.2184C\n  | doi =10.1117/12.633501\n  | id =\n  \n  | type =abstract  | series =Visual Communications and Image Processing 2005\n  }}</ref>\n\nAs illustrated in the image above, the early bitplanes, particularly the first, may have constant runs of bits, and thus can be efficiently encoded by [[run-length encoding]]. This is done (in the transform domain) in the [[Progressive Graphics File]] image format, for instance.\n\n=== Bitmap displays ===\nSome computers displayed graphics in [[Planar (computer graphics)|bit-plane format]], most notably the [[Amiga]] and [[Atari ST]], contrasting with the more common [[Packed pixel|packed format]]. This allowed certain classes of image manipulation to be performed using bitwise operations (especially by a [[blitter]] chip), and parallax scrolling effects.\n\n=== Video Motion Estimation ===\n\nSome [[motion estimation]] algorithms can be performed using bit planes (e.g. after the application of a filter to turn salient edge features into binary values).<ref>{{cite web|title=bitlane motion estimation|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.1755&rep=rep1&type=pdf}}</ref> This can sometimes provide a good enough approximation for correlation operations with minimal computational cost. This relies on an observation that the spatial information is more significant than the actual values. Convolutions may be reduced to [[bit shift]] and [[popcount]] operations, or performed in dedicated hardware.\n\n=== Neural Nets ===\n\nBitplane formats may be used for passing images to [[Spiking neural networks]], or low precision approximations to [[neural networks]]/[[convolutional neural networks]].<ref>{{cite arXiv|title=xnor net|eprint=1603.05279|last1=Rastegari|first1=Mohammad|last2=Ordonez|first2=Vicente|last3=Redmon|first3=Joseph|last4=Farhadi|first4=Ali|class=cs.CV|year=2016}}</ref>\n\n== Programs ==\nMany image processing packages can split an image into bit-planes. Open source tools such as Pamarith from [[Netpbm]] and Convert from [[ImageMagick]] can be used to generate bit-planes.\n\n== See also ==\n* [[Color depth]]\n* [[Planar (computer graphics)|Planar]]\n* [[Binary image]]\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Bit Plane}}\n[[Category:Bit data structures]]"
    },
    {
      "title": "Bitboard",
      "url": "https://en.wikipedia.org/wiki/Bitboard",
      "text": "{{Unreferenced|date=March 2008}}\n\nA '''bitboard''' is a [[data structure]] commonly used in [[Game AI|computer systems that play]] [[board game]]s.\n\nA bitboard, often used for board games such as [[chess]], [[checkers]], [[reversi|othello]] and [[Word game#Letter arrangement games|word games]], is a specialization of the [[bit array]] data structure, where each bit represents a game position or state, designed for optimization of speed and/or memory or disk use in mass calculations. Bits in the same bitboard relate to each other in the rules of the game, often forming a game position when taken together.  Other bitboards are commonly used as masks to transform or answer queries about positions.  The \"game\" may be any game-like system where information is tightly packed in a structured form with \"rules\" affecting how the individual units or pieces relate.\n\n==Short description==\nBitboards are used in many of the world's highest-rated chess playing programs such as [[Houdini (chess)|Houdini]], [[Stockfish (chess)|Stockfish]], and [[Critter (chess)|Critter]]. They help the programs analyze chess positions with few CPU instructions and hold a massive number of positions in memory efficiently.\n\nBitboards allow the computer to answer some questions about game state with one logical operation.  For example, if a chess program wants to know if the white player has any pawns in the center of the board (center four squares) it can just compare a bitboard for the player's pawns with one for the center of the board using a logical AND operation.  If there are no center pawns then the result will be zero.\n\nQuery results can also be represented using bitboards. For example, the query \"What are the squares between X and Y?\" can be represented as a bitboard. These query results are generally pre-calculated, so that a program can simply retrieve a query result with one memory load.\n\nHowever, as a result of the massive compression and encoding, bitboard programs are not easy for software developers to either write or debug.\n\n== History ==\nThe bitboard method for holding a board game appears to have been invented in the mid-1950s, by Arthur Samuel and was used in his checkers program.  The method was published in 1959 as \"Some Studies in Machine Learning Using the Game of Checkers\" in the IBM Journal of Research and Development.\n\nFor the more complicated game of chess, it appears the method was independently rediscovered later by the [[Kaissa]] team in the [[Soviet Union]] in the late 1960s, published in 1970  \"Programming a computer to play chess\" in Russ. Math. Surv.\n, and again by the authors of the U.S. Northwestern University program \"[[Chess (Northwestern University)|Chess]]\" in the early 1970s, and documented in 1977 in \"Chess Skill in Man and Machine\".\n\n== Description for all games or applications ==\n{{main article|Bit field}}\nA bitboard or bit field is a format that stuffs a whole group of related boolean variables into the same machine word, typically representing positions on a board game.  Each bit is a position, and when the bit is positive, a property of that position is true.  In chess, for example, there would be a bitboard for black knights.  There would be 64-bits where each bit represents a chess square.  Another bitboard might be a constant representing the center four squares of the board.  By comparing the two numbers with a bitwise logical AND instruction, we get a third bitboard which represents the black knights on the center four squares, if any. This format is generally more CPU and memory friendly than others.\n\n== General technical advantages and disadvantages ==\n\n=== Processor use ===\n\n==== Pros ====\nThe advantage of the bitboard representation is that it takes advantage of the essential logical [[bitwise operation|bitwise]] operations available on nearly all [[CPU]]s that complete in one cycle and are fully pipelined and cached etc.  Nearly all CPUs have [[Logical conjunction|AND]], [[Logical disjunction|OR]], [[Logical nor|NOR]], and [[Exclusive or|XOR]].  Many CPUs have additional bit instructions, such as finding the \"first\" bit, that make bitboard operations even more efficient.  If they do not have instructions well known algorithms can perform some \"magic\" transformations that do these quickly.\n\nFurthermore, modern CPUs have [[instruction pipeline]]s that queue instructions for execution.  A processor with multiple execution units can perform more than one instruction per cycle if more than one instruction is available in the pipeline.  Branching (the use of conditionals like if) makes it harder for the processor to fill its pipeline(s) because the CPU cannot tell what it needs to do in advance.  Too much branching makes the pipeline less effective and potentially reduces the number of instructions the processor can execute per cycle.  Many bitboard operations require fewer conditionals and therefore increase pipelining and make effective use of multiple execution units on many CPUs.\n\nCPUs have a bit width which they are designed toward and can carry out bitwise operations in one cycle in this width.  So, on a 64-bit or more CPU, 64-bit operations can occur in one instruction.  There may be support for higher or lower width instructions.  Many 32-bit CPUs may have some 64-bit instructions and those may take more than one cycle or otherwise be handicapped compared to their 32-bit instructions.\n\nIf the bitboard is larger than the width of the instruction set, then a performance hit will be the result.  So a program using 64-bit bitboards would run faster on a real 64-bit processor than on a 32-bit processor.\n\n==== Cons ====\nSome queries are going to take longer than they would with perhaps arrays, so bitboards are generally used in conjunction with array boards in chess programs.\n\n=== Memory use ===\n\n==== Pros ====\nBitboards require more memory than mailbox or piece-list board data structures, but are more execution efficient because many loop-and-compare operations are reduced to bitwise operations. For example, in mailbox, to determine whether <piece> attacks <space> requires generating and looping thru legal moves of <piece> and comparing them to <space>.  With bitboards, the legal moves of <piece> are stored in a bitmap, and that map is ANDed with the bitmap for <space>.  A non-zero result means <piece> attacks <space>.\n\n==== Cons ====\nFor some games, writing a suitable bitboard engine requires a fair amount of source code that will be longer than the straightforward implementation. For limited devices (such as cell phones) with a limited number of registers or processor instruction cache, this can cause a problem. For full-sized computers, it may cause cache misses between level-one and level-two cache. This is only a potential problem, not a major drawback, as most machines will have enough instruction cache for this not to be an issue.\n\n== Chess bitboards ==\n\n=== Standard ===\n[[File:SCD algebraic notation.svg|right|[[Algebraic notation (chess)|Algebraic notation]]|frame]]\nThe first bit may represent the square a1, and the 64th bit the square h8, in which case a 1 bit followed by 63 0 bits indicates a single piece on the a1 square. Though stored as a 64 bit word ([[bit array]]), it can be helpful to think of the words as 8 blocks of 8 bits, each representing a rank (row) on the chess board.\n\nThere are twelve types of pieces, and each type gets its own bitboard.  Black pawns get a board, white pawns, etc.  Together these twelve boards can represent a position, with each board often called a plane. Conceptually, if the 12 planes are stacked and viewed from the top with the appropriate piece entered at each 1 bit, they correspond to the usual human-viewed chess board.  Some additional information also needs to be tracked elsewhere: the programmer may use boolean variables for whether each side is in check, can castle, etc.\n\nConstants are likely available, such as WHITE_SQUARES, BLACK_SQUARES, FILE_A, RANK_4, CENTER, CORNERS, CASTLE_SQUARES, etc. For example, if the first bit represents the a1 square, the RANK_1 bitboard constant is 8 1 bits followed by 56 0 bits. These constants allow rapid positional assessment by [[bitwise operation]]s on the piece bitboards.\n\nExamples of variables would be WHITE_ATTACKING, ATTACKED_BY_PAWN, WHITE_PASSED_PAWN, etc. Such bit boards are variables, because their values change during the game, whereas the constants listed above are fixed.\n\n=== Rotated ===\n\"Rotated\" bitboards are usually used in programs that use bitboards.  Rotated bitboards make certain operations more efficient.  While engines are simply referred to as \"rotated bitboard engines,\" this is a misnomer as rotated boards are used in ''addition'' to normal boards making these hybrid standard/rotated bitboard engines.\n\nThese bitboards rotate the bitboard positions by 90 degrees, 45 degrees, and/or 315 degrees. A typical bitboard will have one byte per rank of the chess board. With this bitboard it's easy to determine rook attacks across a rank, using a table indexed by the occupied square and the occupied positions in the rank (because rook attacks stop at the first occupied square). By rotating the bitboard 90 degrees, rook attacks across a file can be examined the same way. Adding bitboards rotated 45 degrees and 315 degrees (-45 degrees) produces bitboards in which the diagonals are easy to examine. The queen can be examined by combining rook and bishop attacks. Rotated bitboards appear to have been developed separately and (essentially) simultaneously by the developers of the [[DarkThought]] and [[Crafty]] programs.{{citation needed|date=October 2012}}\n\n== Other bitboards ==\nMany other games besides chess benefit from bitboards.\n\n* In [[Connect Four]], they allow for very efficient testing for four consecutive discs, by just two shift+and operations per direction.\n* In the [[Conway's Game of Life]], they are a possible alternative to arrays.\n* Othello/Reversi (see the [[Reversi]] article).\n* In [[Word game#Letter arrangement games|word games]], they allow for very efficient generation of valid moves by simple logical operations.\n\n==See also==\n{{columns-list|colwidth=30em|\n*[[Bit array]]\n*[[Bit field]]\n*[[Bit manipulation]]\n*[[Bitwise operation]]\n*[[Board representation (chess)]]\n*[[Boolean algebra (logic)]]\n*[[Instruction set]]\n*[[Instruction pipeline]]\n*[[Opcode]]\n*[[Bytecode]]\n}}\n\n==External links==\n\n===Calculators===\n*[http://cinnamonchess.altervista.org/bitboard_calculator/Calc.html 64 bits representation and manipulation]\n\n===Checkers===\n*[http://www.3dkingdoms.com/checkers/bitboards.htm Checkers Bitboard Tutorial] by Jonathan Kreuzer\n\n===Chess===\n\n====Articles====\n*[http://www.frayn.net/beowulf/theory.html Programming area of the Beowulf project]\n*[http://supertech.lcs.mit.edu/~heinz/dt/node2.html Heinz, Ernst A. How DarkThought plays chess. ''ICCA Journal'', Vol. 20(3), pp. 166-176, Sept. 1997]\n*[https://web.archive.org/web/20030820201803/http://www.gamedev.net/reference/programming/features/chess2/page3.asp Laramee, Francois-Dominic.  Chess Programming Part 2: Data Structures.]\n*[https://web.archive.org/web/20060224211021/http://chess.verhelst.org/1997/03/10/representations/ Verhelst, Paul. Chess Board Representations]\n*[https://web.archive.org/web/20051202091925/http://www.cis.uab.edu/info/faculty/hyatt/boardrep.html Hyatt, Robert. Chess program board representations]\n*[https://web.archive.org/web/20050428023318/http://www.cis.uab.edu/info/faculty/hyatt/bitmaps.html Hyatt, Robert. Rotated bitmaps, a new twist on an old idea]\n*[http://www.frayn.net/beowulf/theory.html#bitboards Frayn, Colin. How to implement bitboards in a chess engine (chess programming theory)]\n*[http://www.onjava.com/pub/a/onjava/2005/02/02/bitsets.html Pepicelli, Glen.  ''Bitfields, Bitboards, and Beyond'']  -(Example of bitboards in the Java Language and a discussion of why this optimization works with the Java Virtual Machine  (www.OnJava.com publisher: O'Reilly 2005))\n*[http://www.pradu.us/old/Nov27_2008/Buzz/research/magic/Bitboards.pdf Magic Move-Bitboard Generation in Computer Chess. Pradyumna Kannan]\n\n====Code examples====\n*[https://web.archive.org/web/20061204195709/http://www.geocities.com/ruleren/sources.html ] The author of the Frenzee engine had posted some source examples.\n*[https://tromp.github.io/c4/Connect4.java] A 155 line java Connect-4 program demonstrating the use of bitboards.\n\n====Implementations====\n\n===== Open source =====\n*[http://www.frayn.net/beowulf/index.html Beowulf] Unix, Linux, Windows.  Rotated bitboards.\n*[[Crafty]]  See the Crafty article.  Written in straight C.  Rotated bitboards in the old versions, now uses magic bitboards. \n*[[GNU Chess]]  See the GNU Chess Article.\n*[[Stockfish (chess)|Stockfish]] UCI chess engine ranking second in Elo as of 2010\n*[https://code.google.com/p/gray-matter/ Gray Matter] C++, rotated bitboards.\n*[[KnightCap]] GPL.  ELO of 2300.\n*[http://www.quarkchess.de/pepito/ Pepito] C. Bitboard, by Carlos del Cacho. Windows and Linux binaries as well as source available.\n*[http://simontacchi.sourceforge.net/ Simontacci] Rotated bitboards.\n\n=====Closed source=====\n*DarkThought  [http://supertech.lcs.mit.edu/~heinz/dt/ Home Page]\n<!-- Add engines here.  Please include a reference where the owner states the software uses bitboards. -->\n\n===Othello===\n*[http://www.radagast.se/othello/ A complete discussion] of Othello ([[Reversi]]) engines with some source code including an Othello bitboard in C and assembly.\n*[[Edax (computing)]]  See the Edax article. An Othello ([[Reversi]]) engine with source code based on bitboard.\n\n===Word Games===\n*[http://boardword.com/static/bitboards.html Overview of bit board usage in word games.]\n\n[[Category:Game artificial intelligence]]\n[[Category:Board game-style video games]]\n[[Category:Computer chess]]\n[[Category:Bit data structures]]"
    },
    {
      "title": "Free space bitmap",
      "url": "https://en.wikipedia.org/wiki/Free_space_bitmap",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|File system implementation structure}}\n{{refimprove|date=July 2011}}\n'''Free-Space Bitmaps''' are one method used to track allocated [[Disk sector|sector]]s<!-- is it a fact that free space bitmap are never used for allocation units greater than a sector? --> by some [[file system]]s.  While the most simplistic design is highly inefficient, advanced or hybrid implementations of free space bitmaps are used by some modern file systems.\n\n==Example==\nThe simplest form of free space bitmap is a [[bit array]], i.e. a block of [[bit]]s.  In this example, a zero would indicate a free sector, while a one indicates a sector in use. Each sector would be of fixed size. For explanatory purposes, we will use a 4&nbsp;[[gibibyte|GiB]] [[hard drive]] with 4096 [[byte]] sectors, and assume the bitmap itself is stored elsewhere. The example disk would require 1,048,576 bits, one for each sector, or 128&nbsp;[[kibibyte|KiB]]. Increasing the size of the drive will proportionately increase the size of the bitmap, while multiplying the sector size will produce a proportionate reduction.\n\nWhen the [[operating system]] (OS) needs to write a file, it will scan the bitmap until it finds enough free locations to fit the file.  If a 12&nbsp;KiB file were stored on the example drive, three zero bits would be found, changed to ones, and the data would be written across the three sectors represented by those bits.  If the file were subsequently truncated down to 8&nbsp;KiB, the final sector's bit would be set back to zero, indicating that it is again available for use.\n\n===Advantages===\n{{unreferenced section|date=July 2011}}\n* Simple: Each bit directly corresponds to a sector\n* Fast random access allocation check:  Checking if a sector is free is as simple as checking the corresponding bit\n* Fast deletion: Data need not be overwritten on delete,{{clarify|date=July 2011}}<!-- actual file data are never overwritten on delete except, maybe, filesystems with embedded security features.  and some meta-data but a bitmap itself must be (also) altered, so there is no advantage. --> flipping the corresponding bit is sufficient\n* Fixed cost: Both an advantage and disadvantage.  Other techniques to store free space information have a variable amount of overhead depending on the number and size of the free space extents.  Bitmaps can never do as well as other techniques in their respective ideal circumstances, but don't suffer pathological cases either.  Since the bitmap never grows, shrinks or moves, fewer lookups are required to find the desired information\n* Low storage overhead as a percentage of the drive size: Even with relatively small sector sizes, the storage space required for the bitmap is small.  A 2&nbsp;[[tebibyte|TiB]] drive could be fully represented with a mere 64&nbsp;[[mebibyte|MiB]] bitmap.\n\n===Disadvantages===\n* Wasteful on larger disks: The simplistic design starts wasting large amounts of space (in an absolute sense) for extremely large volumes<ref name=\"bonwick\">{{cite web|url=http://blogs.sun.com/bonwick/entry/space_maps |title=Space Maps |first=Jeff |last=Bonwick |date=2007-09-14 |accessdate=2009-10-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20090401232331/http://blogs.sun.com/bonwick/entry/space_maps |archivedate=April 1, 2009 }}</ref>\n* Poor scalability: While the size remains negligible as a percentage of the disk size, finding free space becomes slower as the disk fills. If the bitmap is larger than available [[Random-access memory|memory]], performance drops precipitously on all operations<ref name=\"bonwick\" />\n* [[File system fragmentation|Fragmentation]]: If free sectors are taken as they are found, drives with frequent file creation and deletion will rapidly become fragmented.  If the search attempts to find contiguous blocks, finding free space becomes much slower for even moderately full disks.\n\n==Advanced techniques==\nAs the drive size grows, the amount of time needed to scan for free space can become unreasonable.  To address this, real world implementations of free space bitmaps will find ways to centralize information on free space.  One approach is to split the bitmap into many chunks.  A separate array then stores the number of free sectors in each chunk, so chunks with insufficient space can be easily skipped over, and the total amount of free space is easier to compute. Finding free space now entails searching the summary array first, then searching the associated bitmap chunk for the exact sectors available.<ref name=\"bonwick\"/>\n\nThis approach drastically reduces the cost of finding free space, but it doesn't help with the process of freeing space.  If the combined size of the summary array and bitmap is greater than can readily be stored in memory and a large number of files with scattered sectors are freed, an enormous amount of disk access is necessary to find all the sectors, decrement the summary counter and flip the bits back to zero.  This greatly reduces the benefits of the bitmap, as it is no longer performing its function of summarizing the free space rapidly without reading from the disk.\n\n==See also==\n* [[Block availability map]]\n* [[High Performance File System]] (HPFS)\n* [[exFAT]]\n* [[Bitmap index]] - A means of indexing databases that frequently overlaps with efficient free space bitmap designs\n* [[B-tree]] - An alternate means of tracking free space by storing a sorted set of free space extents\n\n==References==\n{{reflist}}\n\n[[Category:Bit data structures]]\n[[Category:Computer file systems]]<!-- this is not an overgeneral category, because most FS-related concepts, including architectural, reside here -->"
    },
    {
      "title": "Bitmap index",
      "url": "https://en.wikipedia.org/wiki/Bitmap_index",
      "text": "A '''bitmap index''' is a special kind of [[Index (database)|database index]] that uses [[Bit array|bitmap]]s.\n\nBitmap indexes have traditionally been considered to work well for ''low-[[cardinality]] [[Column (database)|columns]]'', which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is [[Boolean data]] (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use [[bit array]]s (commonly called bitmaps) and answer queries by performing [[bitwise operation|bitwise logical operation]]s on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional [[B-tree]] indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for [[online transaction processing]] applications.\n\nSome researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the [[Bitwise AND|AND]], [[Bitwise OR|OR]] or [[XOR]] operators extensively.<ref name=\"sharma\">[http://www.oracle.com/technetwork/articles/sharma-indexes-093638.html Bitmap Index vs. B-tree Index: Which and When?], Vivek Sharma, Oracle Technical Network.</ref>\n\nBitmap indexes are also useful in [[data warehousing]] applications for joining a large [[fact table]] to smaller [[dimension table]]s such as those arranged in a [[star schema]].\n\nBitmap based representation can also be used for representing a data structure which is labeled and directed attributed multigraph, used for queries in [[graph databases]].<code>[https://www.researchgate.net/publication/236593640_Efficient_graph_management_based_on_bitmap_indices Efficient graph management based on bitmap indices]</code> article shows how bitmap index representation can be used to manage large dataset (billions of data points) and answer queries related to graph efficiently.\n\n==Example==\nContinuing the internet access example, a bitmap index may be logically viewed as follows:\n{| class=\"wikitable\" style=\"text-align:center; float:left\"\n|-\n!rowspan=2| Identifier\n!rowspan=2| HasInternet\n!colspan=2| Bitmaps\n|-\n!Y !! N\n|-\n|1 || Yes || 1|| 0\n|-\n|2 || No || 0 || 1\n|-\n|3 || No || 0 || 1\n|-\n|4 || Unspecified || 0 || 0\n|-\n|5 || Yes || 1 || 0\n|}\n\nOn the left, [[Identifier]] refers to the unique number assigned to each resident, HasInternet is the data to be indexed, the content of the bitmap index is shown as two columns under the heading ''bitmaps''. Each column in the left illustration is a ''bitmap'' in the bitmap index. In this case, there are two such bitmaps, one for \"has internet\" ''Yes'' and one for \"has internet\" ''No''. It is easy to see that each bit in bitmap ''Y'' shows whether a particular row refers to a person who has internet access. This is the simplest form of bitmap index. Most columns will have more distinct values. For example, the sales amount is likely to have a much larger number of distinct values. Variations on the bitmap index can effectively index this data as well. We briefly review three such variations.\n\nNote: Many of the references cited here are reviewed at ([[#JohnWu2007|John Wu (2007)]]).<ref>{{cite web|ref=JohnWu2007|author=John Wu |year=2007 |title=Annotated References on Bitmap Index |url=http://www.cs.umn.edu/~kewu/annotated.html}}</ref> For those who might be interested in experimenting with some of the ideas mentioned here, many of them are implemented in open source software such as FastBit,<ref>[http://codeforge.lbl.gov/projects/fastbit/ FastBit]</ref> the Lemur Bitmap Index C++ Library,<ref>[https://code.google.com/p/lemurbitmapindex/ Lemur Bitmap Index C++ Library]</ref> the Roaring Bitmap Java library<ref>[http://roaringbitmap.org/ Roaring bitmaps]</ref> and the [[Apache Hive]] Data Warehouse system.\n\n{{Clear}}\n\n==Compression==\nSoftware can [[data compression|compress]] each bitmap in a bitmap index to save space.  There has been considerable amount of work on this subject.<ref>{{cite book |author=T. Johnson |editor1=Malcolm P. Atkinson |editor2=[[Maria Orłowska|Maria E. Orlowska]] |editor3=Patrick Valduriez |editor4=Stanley B. Zdonik |editor5=Michael L. Brodie | title = VLDB'99, Proceedings of 25th International Conference on Very Large Data Bases, September 7–10, 1999, Edinburgh, Scotland, UK | publisher = Morgan Kaufmann | year = 1999 | isbn = 978-1-55860-615-9 | chapter =Performance Measurements of Compressed Bitmap Indices | pages=278–89 | chapter-url=http://www.vldb.org/conf/1999/P29.pdf }}</ref><ref>{{cite web |vauthors=Wu K, Otoo E, Shoshani A | title=On the performance of bitmap indices for high cardinality attributes | date=March 5, 2004 | url=http://www.osti.gov/energycitations/servlets/purl/822860-LOzkmz/native/822860.pdf }}</ref>\nThough there are exceptions such as Roaring bitmaps,<ref name=roaring>{{Cite journal | last1 = Chambi | first1 = S. | last2 = Lemire | first2 = D. | last3 = Kaser | first3 = O. | last4 = Godin | first4 = R.  | title = Better bitmap performance with Roaring bitmaps | doi = 10.1002/spe.2325 | journal = Software: Practice & Experience | volume = 46 | issue = 5 | pages = 709–719 | year = 2016 | pmid =  | pmc = | arxiv = 1402.6407 }}</ref> Bitmap compression algorithms typically employ [[run-length encoding]], such as the Byte-aligned Bitmap Code,<ref>{{US Patent|5363098|Byte aligned data compression}}</ref> the Word-Aligned Hybrid code,<ref>{{US Patent|6831575|Word aligned bitmap compression method, data structure, and apparatus}}</ref> the Partitioned Word-Aligned Hybrid (PWAH) compression,<ref>{{cite conference |url=http://dl.acm.org/citation.cfm?doid=1989323.1989419 |title=A memory efficient reachability data structure through bit vector compression | last1=van Schaik | first1=Sebastiaan |last2=de Moor |first2=Oege |year=2011 |publisher=ACM |booktitle=Proceedings of the 2011 international conference on Management of data |pages=913–924 |location=Athens, Greece |doi=10.1145/1989323.1989419 |conference=SIGMOD '11 |isbn=978-1-4503-0661-4 }}</ref> the Position List Word Aligned Hybrid,<ref name=\"doi_10.1145/1739041.1739071\">{{cite book | chapter = Position list word aligned hybrid: optimizing space and performance for compressed bitmaps |vauthors=Deliège F, Pedersen TB |editor1=Ioana Manolescu |editor2=Stefano Spaccapietra |editor3=Jens Teubner |editor4=Masaru Kitsuregawa |editor5=Alain Leger |editor6=Felix Naumann |editor7=Anastasia Ailamaki |editor8=Fatma Ozcan | title = EDBT '10, Proceedings of the 13th International Conference on Extending Database Technology | publisher = ACM | location = New York, NY, USA | year = 2010 | pages = 228–39 | isbn = 978-1-60558-945-9 | doi = 10.1145/1739041.1739071 | chapter-url = http://alpha.uhasselt.be/icdt/edbticdt2010proc/edbt/papers/p0228-Deliege.pdf }}</ref> the Compressed Adaptive Index (COMPAX),<ref name=\"autogenerated1382\">{{cite journal|author1=F. Fusco |author2=M. Stoecklin |author3=M. Vlachos |title=NET-FLi: on-the-fly compression, archiving and indexing of streaming network traffic |date=September 2010 | volume = 3 | issue = 1–2 | pages = 1382–93 | journal=Proc. VLDB Endow | url=http://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/I01.pdf |doi=10.14778/1920841.1921011 }}</ref> Enhanced Word-Aligned Hybrid (EWAH) <ref name=ewah>{{Cite journal | last1 = Lemire | first1 = D. | last2 = Kaser | first2 = O. | last3 = Aouiche | first3 = K. | title = Sorting improves word-aligned bitmap indexes | doi = 10.1016/j.datak.2009.08.006 | journal = Data & Knowledge Engineering | volume = 69 | pages = 3–28 | year = 2010 | pmid =  | pmc = | arxiv = 0901.3751 }}</ref> and the COmpressed 'N' Composable Integer SEt.<ref>[http://ricerca.mat.uniroma3.it/users/colanton/concise.html Concise: Compressed 'n' Composable Integer Set] {{webarchive |url=https://web.archive.org/web/20110528033714/http://ricerca.mat.uniroma3.it/users/colanton/concise.html |date=May 28, 2011 }}</ref><ref name=\"doi_10.1016/j.ipl.2010.05.018\" /> These compression methods require very little effort to compress and decompress. More importantly, bitmaps compressed with BBC, WAH, COMPAX, PLWAH, EWAH and CONCISE can directly participate in [[bitwise operation]]s without decompression. This gives them considerable advantages over generic compression techniques such as [[LZ77]]. BBC compression and its derivatives are used in a commercial [[database management system]]. BBC is effective in both reducing index sizes and maintaining [[database query|query]] performance. BBC encodes the bitmaps in [[bytes]], while WAH encodes in words, better matching current [[CPU]]s. \"On both synthetic data and real application data, the new word aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC.\"<ref>{{cite book |vauthors=Wu K, Otoo EJ, Shoshani A |editor1=Henrique Paques |editor2=Ling Liu |editor3=David Grossman | chapter =A Performance comparison of bitmap indexes | year=2001 | title = CIKM '01 Proceedings of the tenth international conference on Information and knowledge management | publisher = ACM | location = New York, NY, USA | pages = 559–61 | isbn = 978-1-58113-436-0 | doi = 10.1145/502585.502689 | chapter-url = http://crd.lbl.gov/~kewu/ps/LBNL-48975.pdf }}</ref> PLWAH bitmaps were reported to take 50% of the storage space consumed by WAH bitmaps and offer up to 20% faster performance on [[logical operation]]s.<ref name=\"doi_10.1145/1739041.1739071\" /> Similar considerations can be done for CONCISE <ref name=\"doi_10.1016/j.ipl.2010.05.018\">{{cite journal | vauthors = Colantonio A, Di Pietro R | title = Concise: Compressed 'n' Composable Integer Set | journal = Information Processing Letters | volume = 110 | issue = 16 | date = 31 July 2010 | doi = 10.1016/j.ipl.2010.05.018 | url = http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf | pages = 644–50 | arxiv = 1004.0403 | access-date = 2 February 2011 | archive-url = https://web.archive.org/web/20110722064102/http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf | archive-date = 22 July 2011 | dead-url = yes }}</ref> and Enhanced Word-Aligned Hybrid.<ref name=\"ewah\"/>\n\nThe performance of schemes such as BBC, WAH, PLWAH, EWAH, COMPAX and CONCISE is dependent on the order of the rows. A simple lexicographical sort can divide the index size by 9 and make indexes several times faster.<ref>{{cite journal|author1=D. Lemire |author2=O. Kaser |author3=K. Aouiche |title=Sorting improves word-aligned bitmap indexes |journal=Data & Knowledge Engineering | volume=69 | issue=1 |date=January 2010 |arxiv=0901.3751 | doi = 10.1016/j.datak.2009.08.006|pages=3–28 }}</ref> The larger the table, the more important it is to sort the rows. Reshuffling techniques have also been proposed to achieve the same results of sorting when indexing streaming data.<ref name=\"autogenerated1382\"/>\n\n==Encoding==\nBasic bitmap indexes use one bitmap for each distinct value. It is possible to reduce the number of bitmaps used by using a different [[Code|encoding]] method.<ref name=\"autogenerated355\">{{cite conference |title=Bitmap index design and evaluation |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  | year=1998 |book-title = Proceedings of the 1998 ACM SIGMOD international conference on Management of data (SIGMOD '98) |editor1=Ashutosh Tiwary |editor2=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 355–6 | doi=10.1145/276304.276336 | url = http://www.comp.nus.edu.sg/~chancy/sigmod98.pdf }}</ref><ref>{{cite conference |title=An efficient bitmap encoding scheme for selection queries |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  | year=1999 |book-title = Proceedings of the 1999 ACM SIGMOD international conference on Management of data (SIGMOD '99) | publisher = ACM | location = New York, NY, USA | pages = 215–26 | doi = 10.1145/304182.304201 | url = http://www.ist.temple.edu/~vucetic/cis616spring2005/papers/P4%20p215-chan.pdf }}</ref> For example, it is possible to encode C distinct values using log(C) bitmaps with [[Binary code|binary encoding]].<ref>{{cite conference |author1=P. E. O'Neil  |author2=D. Quass |title=Improved Query Performance with Variant Indexes |book-title=Proceedings of the 1997 ACM SIGMOD international conference on Management of data (SIGMOD '97) |year=1997 |editor1=Joan M. Peckman |editor2=Sudha Ram |editor3=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 38–49| doi=10.1145/253260.253268 }}</ref>\n\nThis reduces the number of bitmaps, further saving space, but to answer any query, most of the bitmaps have to be accessed. This makes it potentially not as effective as scanning a vertical projection of the base data, also known as a [[materialized view]] or projection index. Finding the optimal encoding method that balances (arbitrary) query performance, index size and index maintenance remains a challenge.\n\nWithout considering compression, Chan and Ioannidis analyzed a class of multi-component encoding methods and came to the conclusion that two-component encoding sits at the kink of the performance vs. index size curve and therefore represents the best trade-off between index size and query performance.<ref name=\"autogenerated355\"/>\n\n==Binning==\nFor high-cardinality columns, it is useful to bin the values, where each bin covers multiple values and build the bitmaps to represent the values in each bin. This approach reduces the number of bitmaps used regardless of encoding method.<ref>{{cite book |chapter = Space efficient bitmap indexing| title= Proceedings of the ninth international conference on Information and knowledge management (CIKM '00) | year=2000 | author =N. Koudas | publisher = ACM | location = New York, NY, USA | pages = 194–201 | doi=10.1145/354756.354819 | isbn= 978-1581133202 }}</ref> However, binned indexes can only answer some queries without examining the base data. For example, if a bin covers the range from 0.1 to 0.2, then when the user asks for all values less than 0.15, all rows that fall in the bin are possible hits and have to be checked to verify whether they are actually less than 0.15. The process of checking the base data is known as the candidate check. In most cases, the time used by the candidate check is significantly longer than the time needed to work with the bitmap index. Therefore, binned indexes exhibit irregular performance. They can be very fast for some queries, but much slower if the query does not exactly match a bin.\n\n==History==\nThe concept of bitmap index was first introduced by Professor Israel Spiegler and Rafi Maayan in their research \"Storage and Retrieval Considerations of Binary Data Bases\", published in 1985.<ref>{{cite journal | title = Storage and retrieval considerations of binary data bases | author1 = Spiegler I | author2 = Maayan R | journal = Information Processing and Management: An International Journal | volume = 21 | issue = 3 | year = 1985 | doi = 10.1016/0306-4573(85)90108-6 | pages = 233–54   }}</ref> The first commercial database product to implement a bitmap index was Computer Corporation of America's [[Model 204]]. [[Patrick O'Neil]] published a paper about this implementation in 1987.<ref name=\"model204\">{{cite conference | last = O'Neil | first = Patrick | title = Model 204 Architecture and Performance | booktitle = Proceedings of the 2nd International Workshop on High Performance Transaction Systems | pages = 40–59 | publisher = Springer-Verlag | year = 1987 | location = London, UK | editor = Dieter Gawlick |editor2=Mark N. Haynie |editor3=Andreas Reuter}}</ref> This implementation is a hybrid between the basic bitmap index (without compression) and the list of Row Identifiers (RID-list). Overall, the index is organized as a [[B+tree]]. When the column cardinality is low, each leaf node of the B-tree would contain long list of RIDs. In this case, it requires less space to represent the RID-lists as bitmaps. Since each bitmap represents one distinct value, this is the basic bitmap index. As the column cardinality increases, each bitmap becomes sparse and it may take more disk space to store the bitmaps than to store the same content as RID-lists. In this case, it switches to use the RID-lists, which makes it a [[B+tree]] index.<ref>{{cite conference | title=Bit-sliced index arithmetic | booktitle= Proceedings of the 2001 ACM SIGMOD international conference on Management of data (SIGMOD '01) | year = 2001 |author1=D. Rinfret |author2=P. O'Neil |author3=E. O'Neil | editor = Timos Sellis | publisher = ACM | location = New York, NY, USA | pages = 47–57 | doi = 10.1145/375663.375669 }}</ref><ref>{{cite conference |author1=E. O'Neil |author2=P. O'Neil |author3=K. Wu | title = Bitmap Index Design Choices and Their Performance Implications | booktitle = 11th International Database Engineering and Applications Symposium (IDEAS 2007) | year = 2007 | pages = 72–84 | url=http://crd.lbl.gov/~kewu/ps/LBNL-62756.pdf | isbn = 0-7695-2947-X | doi = 10.1109/IDEAS.2007.19 }}</ref>\n\n==In-memory bitmaps==\nOne of the strongest reasons for using bitmap indexes is that the intermediate results produced from them are also bitmaps and can be efficiently reused in further operations to answer more complex queries. Many programming languages support this as a bit array data structure. For example, Java has the <code>[http://download.oracle.com/javase/6/docs/api/java/util/BitSet.html BitSet]</code> class.\n\nSome database systems that do not offer persistent bitmap indexes use bitmaps internally to speed up query processing. For example, [[PostgreSQL]] versions 8.1 and later implement a \"bitmap index scan\" optimization to speed up arbitrarily complex [[logical operation]]s between available indexes on a single table.\n\nFor tables with many columns, the total number of distinct indexes to satisfy all possible queries (with equality filtering conditions on either of the fields) grows very fast, being defined by this formula:\n\n:<math> \\mathbf{C}_n^\\left [ \\frac{n}{2} \\right ] \\equiv \\frac{n!}{\\left(n-\\left [ \\frac{n}{2} \\right ]\\right)! \\left [ \\frac{n}{2} \\right ]!}</math>.<ref>{{cite web|author=Alex Bolenok|date=2009-05-09|title=Creating indexes|url=http://explainextended.com/2009/05/09/creating-indexes/}}</ref><ref>{{cite web|author=Egor Timoshenko|title=On minimal collections of indexes|url=http://explainextended.com/files/index-en.pdf }}</ref>\n\nA bitmap index scan combines expressions on different indexes, thus requiring only one index per column to support all possible queries on a table.\n\nApplying this access strategy to B-tree indexes can also combine range queries on multiple columns. In this approach, a temporary in-memory bitmap is created with one [[bit]] for each row in the table (1 [[MiB]] can thus store over 8 million entries). Next, the results from each index are combined into the bitmap using [[bitwise operation]]s. After all conditions are evaluated, the bitmap contains a \"1\" for rows that matched the expression. Finally, the bitmap is traversed and matching rows are retrieved. In addition to efficiently combining indexes, this also improves [[locality of reference]] of table accesses, because all rows are fetched sequentially from the main table.<ref>{{cite web |author=Tom Lane |date=2005-12-26 |title=Re: Bitmap indexes etc. |publisher=PostgreSQL mailing lists |url=http://archives.postgresql.org/pgsql-performance/2005-12/msg00623.php |accessdate=2007-04-06 }}</ref> The internal bitmap is discarded after the query. If there are too many rows in the table to use 1 bit per row, a \"lossy\" bitmap is created instead, with a single bit per disk page. In this case, the bitmap is just used to determine which pages to fetch; the filter criteria are then applied to all rows in matching pages.\n\n==References==\n;Notes\n{{Reflist|30em}}\n\n;Bibliography\n*{{Cite journal|last=O'Connell|first=S.|year=2005|title=Advanced Databases Course Notes|location=[[Southampton]]|publisher=[[University of Southampton]]|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n*{{Cite journal|last1=O'Neil|first1=P.|last2=O'Neil|first2=E.|year=2001|title=Database Principles, Programming, and Performance|location=[[San Francisco]]|publisher=[[Morgan Kaufmann Publishers]]|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n*{{Cite journal|last1=Zaker|first1=M.|last2=Phon-Amnuaisuk|first2=S.|last3=Haw|first3=S.C.|year=2008|issue=2|volume=2|title=An Adequate Design for Large Data Warehouse Systems: Bitmap index versus B-tree index|journal=[[International Journal of Computers and Communications]]|url=http://www.universitypress.org.uk/journals/cc/cc-21.pdf|accessdate=2010-01-07|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n\n{{DEFAULTSORT:Bitmap Index}}\n[[Category:Bit data structures|Index]]\n[[Category:Data management]]\n[[Category:Database index techniques]]"
    },
    {
      "title": "Bitmap",
      "url": "https://en.wikipedia.org/wiki/Bitmap",
      "text": "{{Other uses}}\nIn [[computing]], a '''bitmap''' is a mapping from some domain (for example, a range of integers) to [[bit]]s. It is also called a [[bit array]] or [[bitmap index]].\n\nThe more general term '''pix-map''' refers to a map of [[pixel]]s, where each one may store more than two colors, thus using more than one bit per pixel. Often ''bitmap'' is used for this as well. In some contexts, the term ''bitmap'' implies one bit per pixel, while ''pixmap'' is used for images with multiple bits per pixel.<ref>{{cite book |quote= The term bitmap, strictly speaking, applies only to 1-bit-per-pixel bilevel systems; for multiple-bit-per-pixel systems, we use the more general term pix-map (short for pixel map). |title= Computer Graphics: Principles and Practice |author= James D. Foley |pages= 13 |publisher= Addison-Wesley Professional |year= 1995 |isbn= 0-201-84840-6 |url= https://books.google.com/books?id=A4k29b0BdVMC&pg=PA13&dq=bitmap+pix map+%22short+for+pixel+map%22&as_brr=3&ei=JyjwRpvsFYHSpgLN0LyeDA&sig=fcHA65Hg-o819ClX-ueoRJJTk-Q}}</ref><ref>{{cite book |title= Comprehensive Computer Graphics: Including C++ |author= V.K. Pachghare |publisher= Laxmi Publications |pages= 93 |year= 2005 |isbn= 81-7008-185-8 |url= https://books.google.com/books?id=xIKK9RcSTR4C&pg=PA93&dq=bitmap+pixmap+one-bit+date:2004-2007&as_brr=0&ei=PkXwRpD0H4bs7gLC9-jrCQ&sig=ebuyyYGL6FKBWHDjV62IR9MEJA8}}</ref>\n\nA bitmap is a type of [[computer storage|memory]] organization or [[image file format]] used to store [[digital image]]s. The term ''bitmap'' comes from the [[computer programming]] terminology, meaning just a ''map of bits'', a spatially mapped [[bit array|array of bits]]. Now, along with ''pixmap'', it commonly refers to the similar concept of a spatially mapped array of pixels. [[Raster graphics|Raster]] images in general may be referred to as bitmaps or pixmaps, whether synthetic or photographic, in files or memory.\n\nMany [[graphical user interface]]s use bitmaps in their built-in graphics subsystems;<ref>{{cite book |title= Cross-Platform GUI Programming with Wxwidgets |author1=Julian Smart |author2=Stefan Csomor |author3=Kevin Hock |last-author-amp=yes |url= https://books.google.com/books?id=CyMsvtgnq0QC&pg=PA265&dq=bitmap+pixmap+gui&as_brr=3&ei=4SjwRrTpHYSipgL63NS3BA&sig=4_ev_R-Xs8tXCVONCaiJEnFLtI0 |publisher= Prentice Hall |year= 2006 |isbn= 0-13-147381-6}}</ref> for example, the [[Microsoft Windows]] and [[OS/2]] platforms' [[Graphics Device Interface|GDI]] subsystem, where the specific format used is the ''Windows and OS/2 bitmap file format'', usually named with the [[file extension]] of <code>.BMP</code> (or <code>.DIB</code> for ''device-independent bitmap''). Besides [[BMP file format|BMP]], other file formats that store literal bitmaps include [[ILBM|InterLeaved Bitmap (ILBM)]], [[Portable pixmap|Portable Bitmap (PBM)]], [[X BitMap|X Bitmap (XBM)]], and [[Wireless Application Protocol Bitmap Format|Wireless Application Protocol Bitmap (WBMP)]]<!-- please keep this list short; detailed list should be below -->. Similarly, most other image file formats, such as [[JPEG]], [[TIFF]], [[Portable Network Graphics|PNG]], and [[GIF]], also store bitmap images (as opposed to [[vector graphics]]), but they are not usually referred to as ''bitmaps'', since they use [[Image compression|compressed]] formats internally.\n\n==Pixel storage==\nIn typical [[image compression|uncompressed]] bitmaps, image [[pixel]]s are generally stored with a variable number of bits per pixel which identify its color, the [[color depth]]. Pixels of  8 bits and fewer can represent either [[grayscale]] or [[indexed color]]. An [[alpha channel]] (for [[transparency (graphic)|transparency]]) may be stored in a separate bitmap, where it is similar to a grayscale bitmap, or in a fourth channel that, for example, converts 24-bit images to 32 bits per pixel.\n\nThe bits representing the bitmap pixels may be [[packed]] or unpacked (spaced out to byte or word boundaries), depending on the format or device requirements. Depending on the color depth, a pixel in the picture will occupy at least '''n/8''' bytes, where n is the bit depth.\n\nFor an uncompressed, packed within rows, bitmap, such as is stored in Microsoft DIB or [[BMP file format]], or in uncompressed [[TIFF]] format, a lower bound on storage size for a n-bit-per-pixel (2<sup>n</sup> colors) bitmap, in [[byte]]s, can be calculated as:\n:size = width • height • n/8, where height and width are given in pixels.\n\nIn the formula above, header size and [[Palette (computing)|color palette]] size, if any, are not included. Due to effects of row padding to align each row start to a storage unit boundary such as a [[word (computer architecture)|word]], additional bytes may be needed.\n\n==Device-independent bitmaps and BMP file format==\n{{Main|BMP file format}}\n\nMicrosoft has defined a particular representation of color bitmaps of different [[color depth]]s, as an aid to exchanging bitmaps between devices and applications with a variety of internal representations. They called these device-independent bitmaps or DIBs, and the file format for them is called DIB file format or [[BMP file format]]. According to Microsoft support:<ref name=DIBhelp>{{cite web |url= http://support.microsoft.com/kb/q81498/ |title= DIBs and Their Uses |work= Microsoft Help and Support |date= 2005-02-11}}</ref>\n\n<blockquote>\nA device-independent bitmap (DIB) is a format used to define device-independent bitmaps in various [[color resolution]]s. The main purpose of DIBs is to allow bitmaps to be moved from one device to another (hence, the device-independent part of the name). A DIB is an external format, in contrast to a device-dependent bitmap, which appears in the system as a bitmap object (created by an application...). A DIB is normally transported in metafiles (usually using the StretchDIBits() function), BMP files, and the Clipboard (CF_DIB data format).\n</blockquote>\n\nHere, \"device independent\" refers to the format, or storage arrangement, and should not be confused with [[color management|device-independent color]].\n\n===Other bitmap file formats===\n{{Main|Image file formats}}\n\nThe [[X Window System]] uses a similar [[X Bitmap|XBM]] format for [[black-and-white]] images, and [[X PixMap|XPM]] (''pixelmap'') for [[color]] images. Numerous other uncompressed bitmap file formats are in use, though most not widely.<ref>{{cite web |url= http://www.file-extensions.org/filetype/extensions/name/Bitmap+image/\n |title= List of bitmap file types |work= Search File-Extensions.org}}</ref> For most purposes standardized compressed bitmap files such as [[GIF]], [[Portable Network Graphics|PNG]], [[TIFF]], and [[JPEG]] are used; lossless compression in particular provides the same information as a bitmap in a smaller file size.<ref>{{cite book |title= Communicating Science Effectively: a practical handbook for integrating visual elements |author1=J. Thomas |author2=A. Jones |publisher= IWA Publishing |year= 2006 |isbn= 1-84339-125-2 |url= https://books.google.com/books?id=xrgkojGgwDYC&pg=PA26&dq=gif+png+tiff+jpeg+common&ei=9AP2RrD6GajupQLft8HOAQ&sig=mcbmyJU0LRNc-kTEPTme0708lvY}}</ref> TIFF and JPEG have various options. JPEG is usually [[lossy compression]]. TIFF is usually either uncompressed, or lossless [[Lempel-Ziv-Welch]] compressed like [[GIF]]. PNG uses [[deflate]] lossless compression, another [[Lempel-Ziv]] variant.\n\nThere are also a variety of \"raw\" image files, which store raw bitmaps with no other information; such raw files are just bitmaps in files, often with no header or size information (they are distinct from photographic [[raw image format]]s, which store raw unprocessed sensor data in a structured container such as [[TIFF]] format along with extensive image [[metadata]]).\n\n==See also==\n{{Portal|Computer graphics|left=}}\n*[[Free space bitmap]], an array of bits that tracks which disk storage blocks are in-use\n*[[Raster graphics]]\n*[[Raster scan]]\n*[[Rasterization]]\n*[[Tilemap]]\n*[[Voxels]]\n*[[Vector graphics]]\n\n==References==\n{{Reflist}}\n\n{{Compression formats}}\n\n[[Category:Bit data structures|Map]]\n[[Category:Graphics file formats]]\n\n[[ca:Mapa de bits]]\n[[es:Gráfico rasterizado]]\n[[fr:Bitmap]]\n[[it:Bitmap]]"
    },
    {
      "title": "User Electronic Signature",
      "url": "https://en.wikipedia.org/wiki/User_Electronic_Signature",
      "text": "{{Unreferenced|date=November 2015}}\nThe '''User Electronic Signature (UES)''' is a [[bitfield]] with user-defined content in a [[Programmable logic device|Programmable logic device (PLD)]]. It is usually used for the storage of the ID and/or version number of the PLD, suited for [[Serial communication|serial]] identification. Examples for PLD's with a UES are the industry standards GAL16V8 and GAL22V10.\n\n\n\n[[Category:Bit data structures]]"
    },
    {
      "title": "Boolean algebra",
      "url": "https://en.wikipedia.org/wiki/Boolean_algebra",
      "text": "{{short description|Algebra involving variables containing only \"true\" and \"false\" (or 1 and 0) as values}}\n{{other uses}}\nIn [[mathematics]] and [[mathematical logic]], '''Boolean algebra''' is the branch of [[algebra]] in which the values of the [[variable (mathematics)|variables]] are the [[truth value]]s ''true'' and ''false'', usually denoted 1 and 0 respectively. Instead of [[elementary algebra]] where the values of the variables are numbers, and the prime operations are addition and multiplication, the main operations of Boolean algebra are the [[Logical conjunction|conjunction]] ''and'' denoted as ∧, the [[Logical disjunction|disjunction]] ''or'' denoted as ∨, and the [[negation]] ''not'' denoted as ¬. It is thus a formalism for describing logical relations in the same way that elementary algebra describes numeric relations.\n\nBoolean algebra was introduced by [[George Boole]] in his first book ''The Mathematical Analysis of Logic'' (1847), and set forth more fully in his ''[[The Laws of Thought|An Investigation of the Laws of Thought]]'' (1854).<ref>{{Cite book\n  | last = Boole\n  | first = George\n  | authorlink = George Boole\n  | title = An Investigation of the Laws of Thought\n  | publisher = Prometheus Books\n  | origyear = 1854\n  | year = 2003\n  | isbn = 978-1-59102-089-9 }}</ref>\nAccording to [[Edward Vermilye Huntington|Huntington]], the term \"Boolean algebra\" was first suggested by [[Henry M. Sheffer|Sheffer]] in 1913,<ref>\"The name Boolean algebra (or Boolean 'algebras') for the calculus originated by Boole, extended by Schröder, and perfected by Whitehead seems to have been first suggested by Sheffer, in 1913.\"  E. V. Huntington, \"[http://www.ams.org/journals/tran/1933-035-01/S0002-9947-1933-1501684-X/S0002-9947-1933-1501684-X.pdf New sets of independent postulates for the algebra of logic, with special reference to Whitehead and Russell's ''Principia mathematica'']\", in\n''Trans. Amer. Math. Soc.'' '''35''' (1933), 274-304; footnote, page 278.</ref>\nalthough [[Charles Sanders Peirce]] in 1880 gave the title \"A Boolian Algebra with One Constant\" to the first chapter of his \"The Simplest Mathematics\".<ref name=\"Peirce1931\">{{cite book|first=Charles S. |last=Peirce|author-link=Charles Sanders Peirce|title=Collected Papers|url=https://books.google.com/books?id=3JJgOkGmnjEC&pg=RA1-PA13|volume=3|year=1931|publisher=Harvard University Press|isbn=978-0-674-13801-8|page=13}}</ref>\nBoolean algebra has been fundamental in the development of [[digital electronics]], and is provided for in all modern programming languages. It is also used in [[set theory]] and [[statistics]].<ref name=\"givhal\">{{Cite book\n | last1 = Givant\n | first1 = Steven\n | first2 = Paul\n | last2 = Halmos\n | year = 2009\n | title = Introduction to Boolean Algebras\n | publisher = Undergraduate Texts in Mathematics, [[Springer Science+Business Media|Springer]]\n | isbn = 978-0-387-40293-2}}</ref>\n\n==History==\nBoole's algebra predated the modern developments in [[abstract algebra]] and mathematical logic; it is however seen as connected to the origins of both fields.<ref name=\"DunnHardegree2001\">{{cite book|author1=J. Michael Dunn|author2=Gary M. Hardegree|title=Algebraic methods in philosophical logic|url=https://books.google.com/books?id=-AokWhbILUIC&pg=PA2|year=2001|publisher=Oxford University Press US|isbn=978-0-19-853192-0|page=2}}</ref> In an abstract setting, Boolean algebra was perfected in the late 19th century by [[William Stanley Jevons|Jevons]], [[Ernst Schröder|Schröder]], [[Edward Vermilye Huntington|Huntington]], and others until it reached the modern conception of an (abstract) [[mathematical structure]].<ref name=\"DunnHardegree2001\"/> For example, the empirical observation that one can manipulate expressions in the [[algebra of sets]] by translating them into expressions in Boole's algebra is explained in modern terms by saying that the algebra of sets is ''a'' [[Boolean algebra (structure)|Boolean algebra]] (note the [[indefinite article]]). In fact, [[M. H. Stone]] [[Stone's representation theorem for Boolean algebras|proved in 1936]] that every Boolean algebra is [[isomorphic]] to a [[field of sets]].\n\nIn the 1930s, while studying [[switching circuit]]s, [[Claude Shannon]] observed that one could also apply the rules of Boole's algebra in this setting, and he introduced '''switching algebra''' as a way to analyze and design circuits by algebraic means in terms of [[logic gate]]s. Shannon already had at his disposal the abstract mathematical apparatus, thus he cast his switching algebra as the [[two-element Boolean algebra]]. In circuit engineering settings today, there is little need to consider other Boolean algebras, thus \"switching algebra\" and \"Boolean algebra\" are often used interchangeably.<ref name=\"BalabanianCarlson2001\">{{cite book|author1=Norman Balabanian|author2=Bradley Carlson|title=Digital logic design principles|year=2001|publisher=John Wiley|isbn=978-0-471-29351-4|pages=39–40}}, [http://www.wiley.com/college/engin/balabanian293512/pdf/ch02.pdf online sample]</ref><ref name=\"Radhakrishnan\">{{cite book|author=Rajaraman & Radhakrishnan|title=Introduction To Digital Computer Design|url=https://books.google.com/books?id=-8MvcOgsSjcC&pg=PA65|publisher=PHI Learning Pvt. Ltd.|isbn=978-81-203-3409-0|page=65|date=2008-03-01}}</ref><ref name=\"Camara2010\">{{cite book|author=John A. Camara|title=Electrical and Electronics Reference Manual for the Electrical and Computer PE Exam|url=https://books.google.com/books?id=rfHWHeU0jfsC&pg=SA41-PA3|year=2010|publisher=www.ppi2pass.com|isbn=978-1-59126-166-7|page=41}}</ref> [[Logic optimization|Efficient implementation]] of [[Boolean function]]s is a fundamental problem in the [[logic design|design]] of [[combinational logic]] circuits. Modern [[electronic design automation]] tools for [[Very-large-scale integration|VLSI circuits]] often rely on an efficient representation of Boolean functions known as (reduced ordered) [[binary decision diagram]]s (BDD) for [[logic synthesis]] and [[formal verification]].<ref name=\"Chen2007\">{{cite book|editor=Wai-Kai Chen|title=The VLSI handbook|year=2007|publisher=CRC Press|isbn=978-0-8493-4199-1|edition=2nd|author=Shin-ichi Minato, Saburo Muroga|chapter=Binary Decision Diagrams|id=chapter 29}}</ref>\n\nLogic sentences that can be expressed in classical [[propositional calculus]] have an [[algebraic semantics (mathematical logic)|equivalent expression]] in Boolean algebra. Thus, '''Boolean logic''' is sometimes used to denote propositional calculus performed in this way.<ref name=\"Parkes2002\">{{cite book|author=Alan Parkes|title=Introduction to languages, machines and logic: computable languages, abstract machines and formal logic|url=https://books.google.com/books?id=sUQXKy8KPcQC&pg=PA276|year=2002|publisher=Springer|isbn=978-1-85233-464-2|page=276}}</ref><ref name=\"BarwiseEtchemendy1999\">{{cite book|author1=[[Jon Barwise]]|author2=[[John Etchemendy]]|author3=Gerard Allwein |author4=Dave Barker-Plummer |author5=Albert Liu|title=Language, proof, and logic|year=1999|publisher=CSLI Publications|isbn=978-1-889119-08-3}}</ref><ref name=\"Goertzel1994\">{{cite book|author=Ben Goertzel|title=Chaotic logic: language, thought, and reality from the perspective of complex systems science|url=https://books.google.com/books?id=zVOWoXDunp8C&pg=PA48|year=1994|publisher=Springer|isbn=978-0-306-44690-0|page=48}}</ref><!-- \"sometimes\" because Halmos uses it for a precise but different meaning, so he can speak of more than one Boolean logic, but we don't need to say this here. --> Boolean algebra is not sufficient to capture logic formulas using [[Quantifier (logic)|quantifiers]], like those from [[first order logic]].<!-- we could mentions [[cylndric algebras]], but the interested reader can go to the AAL page from the next sentence. --> Although the development of mathematical logic did not follow Boole's program, the connection between his algebra and logic was later put on firm ground in the setting of [[algebraic logic]], which also studies the algebraic systems of many other logics.<ref name=\"DunnHardegree2001\"/> The [[Decision problem|problem of determining whether]] the variables of a given Boolean (propositional) formula can be assigned in such a way as to make the formula evaluate to true is called the [[Boolean satisfiability problem]] (SAT), and is of importance to [[theoretical computer science]], being the first problem shown to be [[NP-complete]]. The closely related [[model of computation]] known as a [[Boolean circuit]] relates [[time complexity]] (of an [[algorithm]]) to [[circuit complexity]].\n<!-- the above two paragraphs may be in ahistorical order because Shannon made the connection via propositional calculus, but it's easier to explain things way, and most EE text don't bother to explain how Shannon connected the dots via propositional calculus. The paragraph about logic may also be harder to comprehend, so it's better not to lose the reader per WP:MTAA -->\n\n==Values==\nWhereas in elementary algebra expressions denote mainly [[number]]s, in Boolean algebra they denote the [[truth values]] ''false'' and ''true''. These values are represented with the [[bit]]s (or binary digits), namely 0 and 1. They do not behave like the [[integer]]s 0 and 1, for which 1 + 1 = 2, but may be identified with the elements of the [[GF(2)|two-element field GF(2)]], that is, [[modular arithmetic|integer arithmetic modulo 2]], for which 1 + 1 = 0. Addition and multiplication then play the Boolean roles of XOR (exclusive-or) and AND (conjunction) respectively, with disjunction ''x''∨''y'' (inclusive-or) definable as ''x'' + ''y'' - ''xy''.\n\nBoolean algebra also deals with [[function (mathematics)|functions]] which have their values in the set {0, 1}.\nA [[bit vector|sequence of bits]] is a commonly used such function. Another common example is the subsets of a set ''E'': to a subset ''F'' of ''E'' is associated the [[indicator function]] that takes the value 1 on ''F'' and 0 outside ''F''. The most general example is the elements of a [[Boolean algebra (structure)|Boolean algebra]], with all of the foregoing being instances thereof.\n\nAs with elementary algebra, the purely equational part of the theory may be developed without considering explicit values for the variables.<ref>Halmos, Paul (1963). Lectures on Boolean Algebras. van Nostrand.</ref>\n\n==Operations==\n{{More citations needed|section|date=April 2019}}\n\n===Basic operations===\n\nThe basic operations of Boolean algebra are as follows:\n*'''AND''' ([[Logical conjunction|conjunction]]), denoted ''x''∧''y'' (sometimes ''x'' AND ''y'' or K''xy''), satisfies ''x''∧''y'' = 1 if ''x'' = ''y'' = 1, and ''x''∧''y'' = 0 otherwise.\n*'''OR'''  ([[Logical disjunction|disjunction]]), denoted ''x''∨''y'' (sometimes ''x'' OR ''y'' or A''xy''), satisfies ''x''∨''y'' = 0 if ''x'' = ''y'' = 0, and ''x''∨''y'' = 1 otherwise.\n*'''NOT''' ([[negation]]), denoted ¬''x'' (sometimes NOT ''x'', N''x'' or !''x''), satisfies ¬''x'' = 0 if ''x'' = 1 and ¬''x'' = 1 if ''x'' = 0.\n\nAlternatively the values of ''x''∧''y'', ''x''∨''y'', and ¬''x'' can be expressed by tabulating their values with [[truth tables]] as follows:\n\n{{col-begin}}\n{{col-break|width=9%}}\n{| class=\"wikitable\" style=\"text-align: center\"\n|-\n!<math>x</math>\n!<math>y</math>\n!<math>x \\wedge y</math>\n!<math>x \\vee y</math>\n|-\n!0\n!0\n| 0 || 0\n|-\n!1\n!0\n| 0 || 1\n|-\n!0\n!1\n| 0 || 1\n|-\n!1\n!1\n| 1 || 1\n|}\n{{col-break}}\n{| class=\"wikitable\" style=\"text-align: center\"\n|-\n!<math>x</math>\n!<math>\\neg x</math>\n|-\n!0\n|1\n|-\n!1\n|0\n|}\n{{col-end}}\n\nIf the truth values 0 and 1 are interpreted as integers, these operations may be expressed with the ordinary operations of arithmetic (where ''x'' + ''y'' uses addition and ''xy'' uses multiplication), or by the minimum/maximum functions:\n:<math>\n\\begin{align}\nx \\wedge y & = xy = \\min(x,y)\\\\ \nx \\vee y & = x + y - xy = \\max(x,y)\\\\ \n\\neg x & = 1 - x\n\\end{align}\n</math>\n\nOne might consider that only negation and one of the two other operations are basic, because of the following identities that allow one to define conjunction in terms of negation and the disjunction, and vice versa ([[De Morgan's laws]]):\n\n:<math>\n\\begin{align}\nx \\wedge y & = \\neg(\\neg x \\vee \\neg y) \\\\\nx \\vee y & = \\neg(\\neg x \\wedge \\neg y)\n\\end{align}\n</math>\n\n===Secondary operations===\nThe three Boolean operations described above are referred to as basic, meaning that they can be taken as a basis for other Boolean operations that can be built up from them by '''composition,''' the manner in which operations are combined or compounded. Operations composed from the basic operations include the following examples:\n\n:<math>x \\rightarrow y = \\neg{x} \\vee y</math>\n:<math>x \\oplus y = (x \\vee y) \\wedge \\neg{(x \\wedge y)} = (x \\wedge \\neg y) \\vee (\\neg x \\wedge y)</math>\n:<math>x \\equiv y = \\neg{(x \\oplus y)} = (x \\wedge y) \\vee (\\neg x \\wedge \\neg y)</math>\n\nThese definitions give rise to the following truth tables giving the values of these operations for all four possible inputs.\n\n:{| class=\"wikitable\" style=\"text-align: center\"\n|+Secondary operations. Table 1\n|-\n!<math>x</math>\n!<math>y</math>\n!<math>x \\rightarrow y</math>\n!<math>x \\oplus y</math>\n!<math>x \\equiv y</math>\n|-\n!0\n!0\n| 1 || 0 || 1\n|-\n!1\n!0\n|0 || 1 || 0\n|-\n!0\n!1\n|1 || 1 || 0\n|-\n!1\n!1\n| 1 || 0 || 1\n|}\n\nThe first operation, ''x''&nbsp;→&nbsp;''y'', or C''xy'', is called '''material implication'''. If ''x'' is true then the value of ''x''&nbsp;→&nbsp;''y'' is taken to be that of ''y''.{{Clarify|reason=If so then the table above is incorrect|date=April 2019}} But if ''x'' is false then the value of ''y'' can be ignored; however the operation must return ''some'' boolean value and there are only two choices.  So by definition, ''x''&nbsp;→&nbsp;''y'' is ''true'' when x is false.  ([[Relevance logic]] suggests this definition by viewing an implication with a [[false premise]] as something other than either true or false.)\n\nThe second operation, ''x''&nbsp;⊕&nbsp;''y'', or J''xy'', is called '''[[exclusive or]]''' (often abbreviated as XOR) to distinguish it from disjunction as the inclusive kind. It excludes the possibility of both ''x'' and ''y being'' true (e.g. see table): if both are true then result is false. Defined in terms of arithmetic it is addition mod&nbsp;2 where 1&nbsp;+&nbsp;1 =&nbsp;0.\n\nThe third operation, the complement of exclusive or, is '''equivalence''' or Boolean equality: ''x''&nbsp;≡&nbsp;''y'', or E''xy'', is true just when ''x'' and ''y'' have the same value. Hence ''x''&nbsp;⊕&nbsp;''y'' as its complement can be understood as ''x''&nbsp;≠&nbsp;''y'', being true just when ''x'' and ''y'' are different. Equivalence's counterpart in arithmetic mod 2 is ''x'' + ''y'' + 1.\n\nGiven two operands, each with two possible values, there are 2<sup>2</sup> = 4 possible combinations of inputs. Because each output can have two possible values, there are a total of 2<sup>4</sup> = [[Boolean algebras canonically defined#Truth tables|16 possible binary Boolean operations]]. Any such operation or function (as well as any Boolean function with more inputs) can be expressed with the basic operations from above. Hence the basic operations are [[Functional completeness|functionally complete]].\n\n==Laws==\nA '''law''' of Boolean algebra is an [[identity (mathematics)|identity]] such as ''x'' ∨ (''y'' ∨ ''z'') = (''x'' ∨ ''y'') ∨ ''z'' between two Boolean terms, where a '''Boolean term''' is defined as an expression built up from variables and the constants 0 and 1 using the operations ∧, ∨, and ¬. The concept can be extended to terms involving other Boolean operations such as ⊕, →, and ≡, but such extensions are unnecessary for the purposes to which the laws are put. Such purposes include the definition of a [[Boolean algebra#Boolean algebras|Boolean algebra]] as any [[model (logic)|model]] of the Boolean laws, and as a means for deriving new laws from old as in the derivation of ''x''∨(''y''∧''z'') = ''x''∨(''z''∧''y'') from ''y''∧''z'' = ''z''∧''y'' as treated in the section on [[Boolean algebra#Axiomatizing Boolean algebra|axiomatization]].\n\n===Monotone laws===\nBoolean algebra satisfies many of the same laws as ordinary algebra when one matches up ∨ with addition and ∧ with multiplication. In particular the following laws are common to both kinds of algebra:<ref name=\"O'Regan_p33\">{{cite book|author=O'Regan, Gerard|title=A brief history of computing|publisher=Springer|year=2008|isbn=978-1-84800-083-4|page=33|url=https://books.google.com/books?id=081H96F1enMC&pg=PA33}}</ref>\n\n:{|\n|-\n| Associativity of <math>\\vee</math>: ||style=\"width:2em\"| ||style=\"text-align: right\"| <math>x \\vee (y \\vee z)</math> || <math>= (x \\vee y) \\vee z</math>\n|-\n| Associativity of <math>\\wedge</math>: || ||style=\"text-align: right\"| <math>x \\wedge (y \\wedge z)</math> || <math>= (x \\wedge y) \\wedge z</math>\n|-\n| Commutativity of <math>\\vee</math>: || ||style=\"text-align: right\"| <math>x \\vee y</math> || <math>= y \\vee x</math>\n|-\n| Commutativity of <math>\\wedge</math>: || ||style=\"text-align: right\"| <math>x \\wedge y</math> || <math>= y \\wedge x</math>\n|-\n| Distributivity of <math>\\wedge</math> over <math>\\vee</math>: || ||style=\"text-align: right\"| <math>x \\wedge (y \\vee z)</math> || <math>= (x \\wedge y) \\vee (x \\wedge z)</math>\n|-\n| Identity for <math>\\vee</math>: || ||style=\"text-align: right\"| <math>x \\vee 0</math> || <math>= x</math>\n|-\n| Identity for <math>\\wedge</math>: || ||style=\"text-align: right\"| <math>x \\wedge 1</math> || <math>= x</math>\n|-\n| Annihilator for <math>\\wedge</math>: || ||style=\"text-align: right\"| <math>x \\wedge 0</math> || <math>= 0</math>\n|-\n|}\n\nThe following laws hold in Boolean Algebra, but not in ordinary algebra:\n:{|\n|- Annihilator for <math>\\vee</math>: || ||style=\"text-align: right\"| <math>x \\vee 1</math> || <math>= 1</math>\n|-\n|Annihilator for <math>\\vee</math>: || ||style=\"text-align: right\"| <math>x \\vee 1</math>\n|<math>= 1</math>\n|-\n| Idempotence of <math>\\vee</math>: || ||style=\"text-align: right\"| <math>x \\vee x</math> || <math>= x</math>\n|-\n| Idempotence of <math>\\wedge</math>: || ||style=\"text-align: right\"| <math>x \\wedge x</math> || <math>= x</math>\n|-\n| Absorption 1: || ||style=\"text-align: right\"| <math>x \\wedge (x \\vee y)</math> || <math>= x</math>\n|-\n| Absorption 2: || ||style=\"text-align: right\"| <math>x \\vee (x \\wedge y)</math> || <math>= x</math>\n|-\n|Distributivity of <math>\\vee</math> over <math>\\wedge</math>:\n|\n|<math>x \\vee (y \\wedge z)</math>\n|<math>=(x \\vee y) \\wedge (x \\vee z)</math>\n|-\n| Distributivity of <math>\\vee</math> over <math>\\wedge</math>:  | \n|}\n\nTaking x = 2 in the third law above shows that it is not an ordinary algebra law, since 2×2 = 4. The remaining five laws can be falsified in ordinary algebra by taking all variables to be 1, for example in Absorption Law 1 the left hand side would be 1(1+1) = 2 while the right hand side would be 1, and so on.\n\nAll of the laws treated so far have been for conjunction and disjunction. These operations have the property that changing either argument either leaves the output unchanged or the output changes in the same way as the input. Equivalently, changing any variable from 0 to 1 never results in the output changing from 1 to 0. Operations with this property are said to be '''monotone'''. Thus the axioms so far have all been for monotonic Boolean logic. Nonmonotonicity enters via complement ¬ as follows.<ref name=\"givhal\"/>\n\n===Nonmonotone laws===\nThe complement operation is defined by the following two laws.\n\n:<math>\n\\begin{align}\n&\\text{Complementation 1} & x \\wedge \\neg x & = 0 \\\\\n&\\text{Complementation 2} & x \\vee \\neg x   & = 1\n\\end{align}\n</math>\n\nAll properties of negation including the laws below follow from the above two laws alone.<ref name=\"givhal\"/>\n\nIn both ordinary and Boolean algebra, negation works by exchanging pairs of elements, whence in both algebras it satisfies the double negation law (also called involution law)\n\n:<math>\n\\begin{align}\n&\\text{Double negation} & \\neg{(\\neg{x})} & = x\n\\end{align}\n</math>\n\nBut whereas ''ordinary algebra'' satisfies the two laws\n\n:<math>\n\\begin{align}\n(-x)(-y) & = xy \\\\\n(-x) + (-y) & = -(x + y)\n\\end{align}\n</math>\n\nBoolean algebra satisfies [[De Morgan's laws]]:\n\n:<math>\n\\begin{align}\n&\\text{De Morgan 1} & \\neg x \\wedge \\neg y & = \\neg{(x \\vee y)} \\\\\n&\\text{De Morgan 2} & \\neg x \\vee \\neg y & = \\neg{(x \\wedge y)}\n\\end{align}\n</math>\n\n===Completeness===\nThe laws listed above define Boolean algebra, in the sense that they entail the rest of the subject. The laws ''Complementation'' 1 and 2, together with the monotone laws, suffice for this purpose and can therefore be taken as one possible ''complete'' set of laws or [[axiomatization]] of Boolean algebra. Every law of Boolean algebra follows logically from these axioms. Furthermore, Boolean algebras can then be defined as the [[model (logic)|models]] of these axioms as treated in the [[Boolean algebra#Boolean algebras|section thereon]].\n\nTo clarify, writing down further laws of Boolean algebra cannot give rise to any new consequences of these axioms, nor can it rule out any model of them. In contrast, in a list of some but not all of the same laws, there could have been Boolean laws that did not follow from those on the list, and moreover there would have been models of the listed laws that were not Boolean algebras.\n\nThis axiomatization is by no means the only one, or even necessarily the most natural given that we did not pay attention to whether some of the axioms followed from others but simply chose to stop when we noticed we had enough laws, treated further in the section on [[Boolean algebra#Axiomatizing Boolean algebra|axiomatizations]]. Or the intermediate notion of axiom can be sidestepped altogether by defining a Boolean law directly as any '''tautology''', understood as an equation that holds for all values of its variables over 0 and 1. All these definitions of Boolean algebra can be shown to be equivalent.\n\n===Duality principle===\n<!-- linked from redirect [[Duality principle (Boolean algebra)]] -->\nPrinciple: If {X, R} is a [[partially ordered set|poset]], then {X, R(inverse)} is also a poset.\n\nThere is nothing magical about the choice of symbols for the values of Boolean algebra. We could rename 0 and 1 to say α and β, and as long as we did so consistently throughout it would still be Boolean algebra, albeit with some obvious cosmetic differences.\n\nBut suppose we rename 0 and 1 to 1 and 0 respectively. Then it would still be Boolean algebra, and moreover operating on the same values. However it would not be identical to our original Boolean algebra because now we find ∨ behaving the way ∧ used to do and vice versa. So there are still some cosmetic differences to show that we've been fiddling with the notation, despite the fact that we're still using 0s and 1s.\n\nBut if in addition to interchanging the names of the values we also interchange the names of the two binary operations, ''now'' there is no trace of what we have done. The end product is completely indistinguishable from what we started with. We might notice that the columns for ''x''∧''y'' and ''x''∨''y'' in the truth tables had changed places, but that switch is immaterial.\n\nWhen values and operations can be paired up in a way that leaves everything important unchanged when all pairs are switched simultaneously, we call the members of each pair '''dual''' to each other. Thus 0 and 1 are dual, and ∧ and ∨ are dual. The '''Duality Principle''', also called [[De Morgan's laws|De Morgan duality]], asserts that Boolean algebra is unchanged when all dual pairs are interchanged.\n\nOne change we did not need to make as part of this interchange was to complement. We say that complement is a '''self-dual''' operation. The identity or do-nothing operation ''x'' (copy the input to the output) is also self-dual. A more complicated example of a self-dual operation is (''x''∧''y'') ∨ (''y''∧''z'') ∨ (''z''∧''x''). There is no self-dual binary operation that depends on both its arguments. A composition of self-dual operations is a self-dual operation. For example, if ''f''(''x'', ''y'', ''z'') = (''x''∧''y'') ∨ (''y''∧''z'') ∨ (''z''∧''x''), then ''f''(''f''(''x'', ''y'', ''z''), ''x'', ''t'') is a self-dual operation of four arguments ''x,y,z,t''.\n\nThe principle of duality can be explained from a [[group theory]] perspective by the fact that there are exactly four functions that are one-to-one mappings ([[automorphism]]s) of the set of Boolean [[polynomial]]s back to itself: the identity function, the complement function, the dual function and the contradual function (complemented dual). These four functions form a [[group (mathematics)|group]] under [[function composition]], isomorphic to the [[Klein four-group]], [[Group action (mathematics)|acting]] on the set of Boolean polynomials. [[Walter Gottschalk]] remarked that consequently a more appropriate name for the phenomenon would be the ''principle'' (or ''square'') ''of quaternality''.<ref name=\"GivantHalmos2009\">{{cite book|author1=Steven R. Givant|author2=Paul Richard Halmos|title=Introduction to Boolean algebras|url=https://books.google.com/books?id=ORILyf8sF2sC&pg=PA22|year=2009|publisher=Springer|isbn=978-0-387-40293-2|pages=21–22}}</ref>\n\n==Diagrammatic representations==\n\n===Venn diagrams===\nA [[Venn diagram]]<ref>{{cite journal |author-last=Venn |author-first=John |author-link=John Venn |title=I. On the Diagrammatic and Mechanical Representation of Propositions and Reasonings |journal=[[The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science]] |volume=10 |issue=59 |date=July 1880 |series=5 |doi=10.1080/14786448008626877 |pages=1–18 |url=https://www.cis.upenn.edu/~bhusnur4/cit592_fall2014/venn%20diagrams.pdf |dead-url=no |archive-url=https://web.archive.org/web/20170516204620/https://www.cis.upenn.edu/~bhusnur4/cit592_fall2014/venn%20diagrams.pdf |archive-date=2017-05-16}} [http://www.tandfonline.com/doi/abs/10.1080/14786448008626877] [https://books.google.com/books?id=k68vAQAAIAAJ&pg=PA1]</ref> is a representation of a Boolean operation using shaded overlapping regions. There is one region for each variable, all circular in the examples here. The interior and exterior of region ''x'' corresponds respectively to the values 1 (true) and 0 (false) for variable ''x''. The shading indicates the value of the operation for each combination of regions, with dark denoting 1 and light 0 (some authors use the opposite convention).\n\nThe three Venn diagrams in the figure below represent respectively conjunction ''x''∧''y'', disjunction ''x''∨''y'', and complement ¬''x''.\n[[File:Vennandornot.svg|center|500px|thumb|Figure 2. Venn diagrams for conjunction, disjunction, and complement]]\nFor conjunction, the region inside both circles is shaded to indicate that ''x''∧''y'' is 1 when both variables are 1. The other regions are left unshaded to indicate that ''x''∧''y'' is 0 for the other three combinations.\n\nThe second diagram represents disjunction ''x''∨''y'' by shading those regions that lie inside either or both circles. The third diagram represents complement ¬''x'' by shading the region ''not'' inside the circle.\n\nWhile we have not shown the Venn diagrams for the constants 0 and 1, they are trivial, being respectively a white box and a dark box, neither one containing a circle. However we could put a circle for ''x'' in those boxes, in which case each would denote a function of one argument, ''x'', which returns the same value independently of ''x'', called a constant function. As far as their outputs are concerned, constants and constant functions are indistinguishable; the difference is that a constant takes no arguments, called a ''zeroary'' or ''nullary'' operation, while a constant function takes one argument, which it ignores, and is a ''unary'' operation.\n\nVenn diagrams are helpful in visualizing laws. The commutativity laws for ∧ and ∨ can be seen from the symmetry of the diagrams: a binary operation that was not commutative would not have a symmetric diagram because interchanging ''x'' and ''y'' would have the effect of reflecting the diagram horizontally and any failure of commutativity would then appear as a failure of symmetry.\n\n[[Idempotence]] of ∧ and ∨ can be visualized by sliding the two circles together and noting that the shaded area then becomes the whole circle, for both ∧ and ∨.\n\nTo see the first absorption law, ''x''∧(''x''∨''y'') = ''x'', start with the diagram in the middle for ''x''∨''y'' and note that the portion of the shaded area in common with the ''x'' circle is the whole of the ''x'' circle. For the second absorption law, ''x''∨(''x''∧''y'') = ''x'', start with the left diagram for ''x''∧''y'' and note that shading the whole of the ''x'' circle results in just the ''x'' circle being shaded, since the previous shading was inside the ''x'' circle.\n\nThe double negation law can be seen by complementing the shading in the third diagram for ¬''x'', which shades the ''x'' circle.\n\nTo visualize the first De Morgan's law, (¬''x'')∧(¬''y'') = ¬(''x''∨''y''), start with the middle diagram for ''x''∨''y'' and complement its shading so that only the region outside both circles is shaded, which is what the right hand side of the law describes. The result is the same as if we shaded that region which is both outside the ''x'' circle ''and'' outside the ''y'' circle, i.e. the conjunction of their exteriors, which is what the left hand side of the law describes.\n\nThe second De Morgan's law, (¬''x'')∨(¬''y'') = ¬(''x''∧''y''), works the same way with the two diagrams interchanged.\n\nThe first complement law, ''x''∧¬''x'' = 0, says that the interior and exterior of the ''x'' circle have no overlap. The second complement law, ''x''∨¬''x'' = 1, says that everything is either inside or outside the ''x'' circle.\n\n===Digital logic gates===\nDigital logic is the application of the Boolean algebra of 0 and 1 to electronic hardware consisting of [[logic gates]] connected to form a [[circuit diagram]]. Each gate implements a Boolean operation, and is depicted schematically by a shape indicating the operation. The shapes associated with the gates for conjunction (AND-gates), disjunction (OR-gates), and complement (inverters) are as follows.<ref>{{Cite journal\n  | last = Shannon\n  | first = Claude\n  | authorlink = Claude Shannon\n  | title = The Synthesis of Two-Terminal Switching Circuits\n  | journal = Bell System Technical Journal\n  | volume = 28\n  | pages = 59–98\n  | year = 1949\n  | id =\n  | doi=10.1002/j.1538-7305.1949.tb03624.x}}</ref>\n\n[[File:LogicGates.GIF|center|400px|thumb|From left to right: [[AND gate|AND]], [[OR gate|OR]], and [[Inverter (logic gate)|NOT]] gates.]]\nThe lines on the left of each gate represent input wires or ''ports''. The value of the input is represented by a voltage on the lead. For so-called \"active-high\" logic, 0 is represented by a voltage close to zero or \"ground\", while 1 is represented by a voltage close to the supply voltage; active-low reverses this.  The line on the right of each gate represents the output port, which normally follows the same voltage conventions as the input ports.\n\nComplement is implemented with an inverter gate. The triangle denotes the operation that simply copies the input to the output; the small circle on the output denotes the actual inversion complementing the input. The convention of putting such a circle on any port means that the signal passing through this port is complemented on the way through, whether it is an input or output port.\n\nThe [[Boolean Algebra#Duality principle|Duality Principle]], or [[De Morgan's laws]], can be understood as asserting that complementing all three ports of an AND gate converts it to an OR gate and vice versa, as shown in Figure 4 below. Complementing both ports of an inverter however leaves the operation unchanged.\n[[File:DeMorganGates.GIF|center|400px|thumb]]\n\nMore generally one may complement any of the eight subsets of the three ports of either an AND or OR gate. The resulting sixteen possibilities give rise to only eight Boolean operations, namely those with an odd number of 1's in their truth table. There are eight such because the \"odd-bit-out\" can be either 0 or 1 and can go in any of four positions in the truth table. There being sixteen binary Boolean operations, this must leave eight operations with an even number of 1's in their truth tables. Two of these are the constants 0 and 1 (as binary operations that ignore both their inputs); four are the operations that depend nontrivially on exactly one of their two inputs, namely ''x'', ''y'', ¬''x'', and ¬''y''; and the remaining two are ''x''⊕''y'' (XOR) and its complement ''x''≡''y''.\n\n==Boolean algebras==\n{{Main|Boolean algebra (structure)}}\nThe term \"algebra\" denotes both a subject, namely the subject of [[algebra]], and an object, namely an [[algebraic structure]]. Whereas the foregoing has addressed the subject of Boolean algebra, this section deals with mathematical objects called Boolean algebras, defined in full generality as any model of the Boolean laws. We begin with a special case of the notion definable without reference to the laws, namely concrete Boolean algebras, and then give [[#Boolean algebras: the definition|the formal definition]] of the general notion.\n\n===Concrete Boolean algebras===\nA '''concrete Boolean algebra''' or [[field of sets]] is any nonempty set of subsets of a given set ''X'' closed under the set operations of [[union (set theory)|union]], [[intersection (set theory)|intersection]], and [[complement (set theory)#Relative complement|complement]] relative to ''X''.<ref name=\"givhal\"/>\n\n(As an aside, historically ''X'' itself was required to be nonempty as well to exclude the degenerate or one-element Boolean algebra, which is the one exception to the rule that all Boolean algebras satisfy the same equations since the degenerate algebra satisfies every equation. However this exclusion conflicts with the preferred purely equational definition of \"Boolean algebra,\" there being no way to rule out the one-element algebra using only equations—&nbsp;0&nbsp;≠&nbsp;1 does not count, being a negated equation. Hence modern authors allow the degenerate Boolean algebra and let ''X'' be empty.)\n\n'''Example 1.''' The [[power set]] 2<sup>''X''</sup> of ''X'', consisting of all subsets of ''X''. Here ''X'' may be any set: empty, finite, infinite, or even [[uncountable]].\n\n'''Example 2.''' The empty set and ''X''. This two-element algebra shows that a concrete Boolean algebra can be finite even when it consists of subsets of an infinite set. It can be seen that every field of subsets of ''X'' must contain the empty set and ''X''. Hence no smaller example is possible, other than the degenerate algebra obtained by taking ''X'' to be empty so as to make the empty set and ''X'' coincide.\n\n'''Example 3.''' The set of finite and [[cofinite]] sets of integers, where a cofinite set is one omitting only finitely many integers. This is clearly closed under complement, and is closed under union because the union of a cofinite set with any set is cofinite, while the union of two finite sets is finite.  Intersection behaves like union with \"finite\" and \"cofinite\" interchanged.\n\n'''Example 4.''' For a less trivial example of the point made by Example 2, consider a [[Venn diagram]] formed by ''n'' closed curves [[Partition of a set|partitioning]] the diagram into 2<sup>''n''</sup> regions, and let ''X'' be the (infinite) set of all points in the plane not on any curve but somewhere within the diagram. The interior of each region is thus an infinite subset of ''X'', and every point in ''X'' is in exactly one region.  Then the set of all 2<sup><span>2<sup>''n''</sup></span></sup> possible unions of regions (including the empty set obtained as the union of the empty set of regions and ''X'' obtained as the union of all 2<sup>''n''</sup> regions) is closed under union, intersection, and complement relative to ''X'' and therefore forms a concrete Boolean algebra. Again we have finitely many subsets of an infinite set forming a concrete Boolean algebra, with Example 2 arising as the case ''n'' = 0 of no curves.\n\n===Subsets as bit vectors===\nA subset ''Y'' of ''X'' can be identified with an [[indexed family]] of bits with [[index set]] ''X'', with the bit indexed by ''x'' ∈ ''X'' being 1 or 0 according to whether or not ''x'' ∈ ''Y''. (This is the so-called [[Indicator function|characteristic function]] notion of a subset.)  For example, a 32-bit computer word consists of 32 bits indexed by the set {0,1,2,…,31}, with 0 and 31 indexing the low and high order bits respectively. For a smaller example, if ''X'' = {''a,b,c''} where ''a, b, c'' are viewed as bit positions in that order from left to right, the eight subsets {}, {''c''}, {''b''}, {''b'',''c''}, {''a''}, {''a'',''c''}, {''a'',''b''}, and {''a'',''b'',''c''} of ''X'' can be identified with the respective bit vectors 000, 001, 010, 011, 100, 101, 110, and 111. Bit vectors indexed by the set of natural numbers are infinite [[sequence]]s of bits, while those indexed by the [[real number|reals]] in the [[unit interval]] [0,1] are packed too densely to be able to write conventionally but nonetheless form well-defined indexed families (imagine coloring every point of the interval [0,1] either black or white independently; the black points then form an arbitrary subset of [0,1]).\n\nFrom this bit vector viewpoint, a concrete Boolean algebra can be defined equivalently as a nonempty set of bit vectors all of the same length (more generally, indexed by the same set) and closed under the bit vector operations of [[Bitwise operations|bitwise]] ∧, ∨, and ¬, as in 1010∧0110 = 0010, 1010∨0110 = 1110, and ¬1010 = 0101, the bit vector realizations of intersection, union, and complement respectively.\n\n===The prototypical Boolean algebra===\n{{main|two-element Boolean algebra}}\nThe set {0,1} and its Boolean operations as treated above can be understood as the special case of bit vectors of length one, which by the identification of bit vectors with subsets can also be understood as the two subsets of a one-element set. We call this the '''prototypical''' Boolean algebra, justified by the following observation.\n:The laws satisfied by all nondegenerate concrete Boolean algebras coincide with those satisfied by the prototypical Boolean algebra.\nThis observation is easily proved as follows. Certainly any law satisfied by all concrete Boolean algebras is satisfied by the prototypical one since it is concrete. Conversely any law that fails for some concrete Boolean algebra must have failed at a particular bit position, in which case that position by itself furnishes a one-bit counterexample to that law. Nondegeneracy ensures the existence of at least one bit position because there is only one empty bit vector.\n\nThe final goal of the next section can be understood as eliminating \"concrete\" from the above observation. We shall however reach that goal via the surprisingly stronger observation that, up to isomorphism, all Boolean algebras are concrete.\n\n===Boolean algebras: the definition===\nThe Boolean algebras we have seen so far have all been concrete, consisting of bit vectors or equivalently of subsets of some set. Such a Boolean algebra consists of a set and operations on that set which can be ''shown'' to satisfy the laws of Boolean algebra.\n\nInstead of showing that the Boolean laws are satisfied, we can instead postulate a set ''X'', two binary operations on ''X'', and one unary operation, and ''require'' that those operations satisfy the laws of Boolean algebra. The elements of ''X'' need not be bit vectors or subsets but can be anything at all. This leads to the more general ''abstract'' definition.\n:A '''Boolean algebra''' is any set with binary operations ∧ and ∨ and a unary operation ¬ thereon satisfying the Boolean laws.<ref>{{Cite book\n  | last = Koppelberg\n  | first = Sabine\n  | chapter = General Theory of Boolean Algebras\n  | title = Handbook of Boolean Algebras, Vol. 1 (ed. J. Donald Monk with Robert Bonnet)\n  | publisher = North Holland\n  | year = 1989\n  | location = Amsterdam\n  | isbn = 978-0-444-70261-6 <!--0-444-70261-X--> }}</ref>\n\nFor the purposes of this definition it is irrelevant how the operations came to satisfy the laws, whether by fiat or proof. All concrete Boolean algebras satisfy the laws (by proof rather than fiat), whence every concrete Boolean algebra is a Boolean algebra according to our definitions. This axiomatic definition of a Boolean algebra as a set and certain operations satisfying certain laws or axioms ''by fiat'' is entirely analogous to the abstract definitions of [[Group (mathematics)|group]], [[Ring (mathematics)|ring]], [[Field (mathematics)|field]] etc. characteristic of modern or [[abstract algebra]].\n\nGiven any complete axiomatization of Boolean algebra, such as the axioms for a [[Complemented lattice|complemented]] [[distributive lattice]], a sufficient condition for an [[algebraic structure]] of this kind to satisfy all the Boolean laws is that it satisfy just those axioms. The following is therefore an equivalent definition.\n:A '''Boolean algebra''' is a complemented distributive lattice.\n\nThe section on [[#Axiomatizing Boolean algebra|axiomatization]] lists other axiomatizations, any of which can be made the basis of an equivalent definition.\n\n===Representable Boolean algebras===\nAlthough every concrete Boolean algebra is a Boolean algebra, not every Boolean algebra need be concrete. Let ''n'' be a [[Square-free integer|square-free]] positive integer, one not divisible by the square of an integer, for example 30 but not 12. The operations of [[greatest common divisor]], [[least common multiple]], and division into ''n'' (that is, ¬''x'' = ''n''/''x''), can be shown to satisfy all the Boolean laws when their arguments range over the positive divisors of ''n''. Hence those divisors form a Boolean algebra. These divisors are not subsets of a set, making the divisors of ''n'' a Boolean algebra that is not concrete according to our definitions.\n\nHowever, if we ''represent'' each divisor of ''n'' by the set of its prime factors, we find that this nonconcrete Boolean algebra is [[isomorphic]] to the concrete Boolean algebra consisting of all sets of prime factors of ''n'', with union corresponding to least common multiple, intersection to greatest common divisor, and complement to division into ''n''. So this example while not technically concrete is at least \"morally\" concrete via this representation, called an [[isomorphism]]. This example is an instance of the following notion.\n:A Boolean algebra is called '''representable''' when it is isomorphic to a concrete Boolean algebra.\n\nThe obvious next question is answered positively as follows.\n:Every Boolean algebra is representable.\n\nThat is, up to isomorphism, abstract and concrete Boolean algebras are the same thing. This quite nontrivial result depends on the [[Boolean prime ideal theorem]], a choice principle slightly weaker than the [[axiom of choice]], and is treated in more detail in the article [[Stone's representation theorem for Boolean algebras]]. This strong relationship implies a weaker result strengthening the observation in the previous subsection to the following easy consequence of representability.\n:The laws satisfied by all Boolean algebras coincide with those satisfied by the prototypical Boolean algebra.\n\nIt is weaker in the sense that it does not of itself imply representability. Boolean algebras are special here, for example a [[relation algebra]] is a Boolean algebra with additional structure but it is not the case that every relation algebra is representable in the sense appropriate to relation algebras.\n\n==Axiomatizing Boolean algebra==\n{{Main|Axiomatization of Boolean algebras|Boolean algebras canonically defined}}\nThe above definition of an abstract Boolean algebra as a set and operations satisfying \"the\" Boolean laws raises the question, what are those laws?  A simple-minded answer is \"all Boolean laws,\" which can be defined as all equations that hold for the Boolean algebra of 0 and 1. Since there are infinitely many such laws this is not a terribly satisfactory answer in practice, leading to the next question: does it suffice to require only finitely many laws to hold?\n\nIn the case of Boolean algebras the answer is yes. In particular the finitely many equations we have listed above suffice. We say that Boolean algebra is '''finitely axiomatizable''' or '''finitely based.'''\n\nCan this list be made shorter yet? Again the answer is yes. To begin with, some of the above laws are implied by some of the others. A sufficient subset of the above laws consists of the pairs of associativity, commutativity, and absorption laws, distributivity of ∧ over ∨ (or the other distributivity law—one suffices), and the two complement laws. In fact this is the traditional axiomatization of Boolean algebra as a [[Complemented lattice|complemented]] [[distributive lattice]].\n\nBy introducing additional laws not listed above it becomes possible to shorten the list yet further. In 1933, [[Edward Huntington]] showed that if the basic operations are taken to be ''x''∨''y'' and ¬''x'', with ''x''∧''y'' considered a derived operation (e.g. via De Morgan's law in the form ''x''∧''y'' = ¬(¬''x''∨¬''y'')), then the equation\n¬(¬''x''∨¬''y'')∨¬(¬''x''∨''y'') = ''x'' along with the two equations expressing associativity and commutativity of ∨ completely axiomatized Boolean algebra. When the only basic operation is the binary [[Sheffer stroke|NAND operation]] ¬(''x''∧''y''), [[Stephen Wolfram]] has proposed in his book ''[[A New Kind of Science]]'' the single axiom ((''xy'')''z'')(''x''((''xz'')''x'')) = ''z'' as a one-equation axiomatization of Boolean algebra, where for convenience here ''xy'' denotes the NAND rather than the AND of ''x'' and ''y''.\n\n==Propositional logic==\n{{Main|Propositional calculus}}\n'''[[Propositional logic]]''' is a [[logical system]] that is intimately connected to Boolean algebra.<ref name=\"givhal\"/>  Many syntactic concepts of Boolean algebra carry over to propositional logic with only minor changes in notation and terminology, while the semantics of propositional logic are defined via Boolean algebras in a way that the tautologies (theorems) of propositional logic correspond to equational theorems of Boolean algebra.\n\nSyntactically, every Boolean term corresponds to a '''[[propositional formula]]''' of propositional logic. In this translation between Boolean algebra and propositional logic, Boolean variables ''x,y''… become '''[[propositional variable]]s''' (or '''atoms''') ''P,Q'',…, Boolean terms such as ''x''∨''y'' become propositional formulas ''P''∨''Q'', 0 becomes ''false'' or '''⊥''', and 1 becomes ''true'' or '''T'''. It is convenient when referring to generic propositions to use Greek letters Φ, Ψ,… as metavariables (variables outside the language of propositional calculus, used when talking ''about'' propositional calculus) to denote propositions.\n\nThe semantics of propositional logic rely on '''[[truth assignment]]'''s. The essential idea of a truth assignment is that the propositional variables are mapped to elements of a fixed Boolean algebra, and then the '''[[truth value]]''' of a propositional formula using these letters is the element of the Boolean algebra that is obtained by computing the value of the Boolean term corresponding to the formula. In classical semantics, only the two-element Boolean algebra is used, while in [[Boolean-valued semantics]] arbitrary Boolean algebras are considered. A '''[[tautology (logic)|tautology]]''' is a propositional formula that is assigned truth value ''1'' by every truth assignment of its propositional variables to an arbitrary Boolean algebra (or, equivalently, every truth assignment to the two element Boolean algebra).\n\nThese semantics permit a translation between tautologies of propositional logic and equational theorems of Boolean algebra. Every tautology Φ of propositional logic can be expressed as the Boolean equation Φ = 1, which will be a theorem of Boolean algebra. Conversely every theorem Φ = Ψ of Boolean algebra corresponds to the tautologies (Φ∨¬Ψ) ∧ (¬Φ∨Ψ) and (Φ∧Ψ) ∨ (¬Φ∧¬Ψ). If → is in the language these last tautologies can also be written as (Φ→Ψ) ∧ (Ψ→Φ), or as two separate theorems Φ→Ψ and Ψ→Φ; if ≡ is available then the single tautology Φ ≡ Ψ can be used.\n\n===Applications===\nOne motivating application of propositional calculus is the analysis of propositions and deductive arguments in natural language. Whereas the proposition \"if ''x'' = 3 then ''x''+1 = 4\" depends on the meanings of such symbols as + and 1, the proposition \"if ''x'' = 3 then ''x'' = 3\" does not; it is true merely by virtue of its structure, and remains true whether \"''x'' = 3\" is replaced by \"''x'' = 4\" or \"the moon is made of green cheese.\"  The generic or abstract form of this tautology is \"if ''P'' then ''P''\", or in the language of Boolean algebra, \"''P'' → ''P''\".\n\nReplacing ''P'' by ''x'' = 3 or any other proposition is called '''instantiation''' of ''P'' by that proposition. The result of instantiating ''P'' in an abstract proposition is called an '''instance''' of the proposition.  Thus \"''x'' = 3 → ''x'' = 3\" is a tautology by virtue of being an instance of the abstract tautology \"''P'' → ''P''\". All occurrences of the instantiated variable must be instantiated with the same proposition, to avoid such nonsense as ''P'' → ''x'' = 3 or ''x'' = 3 → ''x'' = 4.\n\nPropositional calculus restricts attention to abstract propositions, those built up from propositional variables using Boolean operations. Instantiation is still possible within propositional calculus, but only by instantiating propositional variables by abstract propositions, such as instantiating ''Q'' by ''Q''→''P'' in ''P''→(''Q''→''P'') to yield the instance ''P''→((''Q''→''P'')→''P'').\n\n(The availability of instantiation as part of the machinery of propositional calculus avoids the need for metavariables within the language of propositional calculus, since ordinary propositional variables can be considered within the language to denote arbitrary propositions. The metavariables themselves are outside the reach of instantiation, not being part of the language of propositional calculus but rather part of the same language for talking about it that this sentence is written in, where we need to be able to distinguish propositional variables and their instantiations as being distinct syntactic entities.)\n\n===Deductive systems for propositional logic===\nAn axiomatization of propositional calculus is a set of tautologies called [[axioms]] and one or more inference rules for producing new tautologies from old. A ''proof'' in an axiom system ''A'' is a finite nonempty sequence of propositions each of which is either an instance of an axiom of ''A'' or follows by some rule of ''A'' from propositions appearing earlier in the proof (thereby disallowing circular reasoning). The last proposition is the '''theorem''' proved by the proof. Every nonempty initial segment of a proof is itself a proof, whence every proposition in a proof is itself a theorem. An axiomatization is '''sound''' when every theorem is a tautology, and '''complete''' when every tautology is a theorem.<ref>{{cite book | first=Alan | last=Hausman |author2=Howard Kahane|author3=Paul Tidman | title=Logic and Philosophy: A Modern Introduction | publisher=Wadsworth Cengage Learning | year=2010 | origyear=2007 | isbn=978-0-495-60158-6}}</ref>\n\n====Sequent calculus====\n{{Main|Sequent calculus}}\nPropositional calculus is commonly organized as a [[Hilbert system]], whose operations are just those of Boolean algebra and whose theorems are Boolean tautologies, those Boolean terms equal to the Boolean constant 1. Another form is [[sequent calculus]], which has two sorts, propositions as in ordinary propositional calculus, and pairs of lists of propositions called [[sequent]]s, such as ''A''∨''B'', ''A''∧''C'',… <math>\\vdash</math> ''A'', ''B''→''C'',…. The two halves of a sequent are called the antecedent and the succedent respectively. The customary metavariable denoting an antecedent or part thereof is Γ, and for a succedent Δ; thus Γ,''A'' <math>\\vdash</math> Δ would denote a sequent whose succedent is a list Δ and whose antecedent is a list Γ with an additional proposition ''A'' appended after it. The antecedent is interpreted as the conjunction of its propositions, the succedent as the disjunction of its propositions, and the sequent itself as the [[entailment]] of the succedent by the antecedent.\n\nEntailment differs from implication in that whereas the latter is a binary ''operation'' that returns a value in a Boolean algebra, the former is a binary ''relation'' which either holds or does not hold. In this sense entailment is an ''external'' form of implication, meaning external to the Boolean algebra, thinking of the reader of the sequent as also being external and interpreting and comparing antecedents and succedents in some Boolean algebra. The natural interpretation of <math>\\vdash</math> is as ≤ in the partial order of the Boolean algebra defined by ''x'' ≤ ''y'' just when ''x''∨''y'' = ''y''. This ability to mix external implication <math>\\vdash</math> and internal implication → in the one logic is among the essential differences between sequent calculus and propositional calculus.<ref>{{cite book | first=Jean-Yves | last=Girard | authorlink=Jean-Yves Girard |author2=Paul Taylor|author3=Yves Lafont | title=Proofs and Types | publisher=Cambridge University Press (Cambridge Tracts in Theoretical Computer Science, 7) | year=1990 | origyear=1989 | isbn=978-0-521-37181-0 | url= http://www.paultaylor.eu/stable/Proofs%2BTypes.html}}</ref>\n\n==Applications==\nBoolean algebra as the calculus of two values is fundamental to computer circuits, computer programming, and mathematical logic, and is also used in other areas of mathematics such as set theory and statistics.<ref name=\"givhal\"/>\n\n===Computers===\nIn the early 20th century, several electrical engineers intuitively recognized that Boolean algebra was analogous to the behavior of certain types of electrical circuits. [[Claude Shannon]] formally proved such behavior was logically equivalent to Boolean algebra in his 1937 master's thesis, ''[[A Symbolic Analysis of Relay and Switching Circuits]]''.\n\nToday, all modern general purpose [[computer]]s perform their functions using two-value Boolean logic; that is, their electrical circuits are a physical manifestation of two-value Boolean logic. They achieve this in various ways: as [[Digital signal|voltages on wires]] in high-speed circuits and capacitive storage devices, as orientations of a [[Magnetic storage|magnetic domain]] in ferromagnetic storage devices, as holes in [[punched card]]<nowiki/>s or [[Punched tape|paper tape]], and so on. (Some early computers used decimal circuits or mechanisms instead of two-valued logic circuits.)\n\nOf course, it is possible to code more than two symbols in any given medium. For example, one might use respectively 0, 1, 2, and 3 volts to code a four-symbol alphabet on a wire, or holes of different sizes in a punched card. In practice, the tight constraints of high speed, small size, and low power combine to make noise a major factor. This makes it hard to distinguish between symbols when there are several possible symbols that could occur at a single site. Rather than attempting to distinguish between four voltages on one wire, digital designers have settled on two voltages per wire, high and low.\n\nComputers use two-value Boolean circuits for the above reasons. The most common computer architectures use ordered sequences of Boolean values, called bits, of 32 or 64 values, e.g. 01101000110101100101010101001011. When programming in [[machine code]], [[assembly language]], and certain other [[programming languages]], programmers work with the low-level digital structure of the [[Word (data type)|data registers]]. These registers operate on voltages, where zero volts represents Boolean 0, and a reference voltage (often +5V, +3.3V, +1.8V) represents Boolean 1. Such languages support both numeric operations and logical operations. In this context, \"numeric\" means that the computer treats sequences of bits as [[binary number]]s (base two numbers) and executes arithmetic operations like add, subtract, multiply, or divide. \"Logical\" refers to the Boolean logical operations of disjunction, conjunction, and negation between two sequences of bits, in which each bit in one sequence is simply compared to its counterpart in the other sequence. Programmers therefore have the option of working in and applying the rules of either numeric algebra or Boolean algebra as needed. A core differentiating feature between these families of operations is the existence of the [[Carry (arithmetic)|carry]] operation in the first but not the second.\n\n===Two-valued logic===\nOther areas where two values is a good choice are the law and mathematics. In everyday relaxed conversation, nuanced or complex answers such as \"maybe\" or \"only on the weekend\" are acceptable. In more focused situations such as a court of law or theorem-based mathematics however it is deemed advantageous to frame questions so as to admit a simple yes-or-no answer—is the defendant guilty or not guilty, is the proposition true or false—and to disallow any other answer. However much of a straitjacket this might prove in practice for the respondent, the principle of the simple yes-no question has become a central feature of both judicial and mathematical logic, making two-valued logic deserving of organization and study in its own right.\n\nA central concept of set theory is membership. Now an organization may permit multiple degrees of membership, such as novice, associate, and full. With sets however an element is either in or out. The candidates for membership in a set work just like the wires in a digital computer: each candidate is either a member or a nonmember, just as each wire is either high or low.\n\nAlgebra being a fundamental tool in any area amenable to mathematical treatment, these considerations combine to make the algebra of two values of fundamental importance to computer hardware, mathematical logic, and set theory.\n\n[[Two-valued logic]] can be extended to [[multi-valued logic]], notably by replacing the Boolean domain {0,&nbsp;1} with the unit interval [0,1], in which case rather than only taking values 0 or 1, any value between and including 0 and 1 can be assumed. Algebraically, negation (NOT) is replaced with 1&nbsp;−&nbsp;''x'', conjunction (AND) is replaced with multiplication (<math>xy</math>), and disjunction (OR) is defined via [[De Morgan's law]]. Interpreting these values as logical [[truth value]]s yields a multi-valued logic, which forms the basis for [[fuzzy logic]] and [[probabilistic logic]]. In these interpretations, a value is interpreted as the \"degree\" of truth – to what extent a proposition is true, or the probability that the proposition is true.\n\n===Boolean operations===\nThe original application for Boolean operations was [[mathematical logic]], where it combines the truth values, true or false, of individual formulas.\n\nNatural languages such as English have words for several Boolean operations, in particular conjunction (''and''), disjunction (''or''), negation (''not''), and implication (''implies''). ''But not'' is synonymous with ''and not''. When used to combine situational assertions such as \"the block is on the table\" and \"cats drink milk,\" which naively are either true or false, the meanings of these [[logical connective]]s often have the meaning of their logical counterparts. However, with descriptions of behavior such as \"Jim walked through the door\", one starts to notice differences such as failure of commutativity, for example the conjunction of \"Jim opened the door\" with \"Jim walked through the door\" in that order is not equivalent to their conjunction in the other order, since ''and'' usually means ''and then'' in such cases. Questions can be similar: the order \"Is the sky blue, and why is the sky blue?\" makes more sense than the reverse order. Conjunctive commands about behavior are like behavioral assertions, as in ''get dressed and go to school''. Disjunctive commands such ''love me or leave me'' or ''fish or cut bait'' tend to be asymmetric via the implication that one alternative is less preferable. Conjoined nouns such as ''tea and milk'' generally describe aggregation as with set union while ''tea or milk'' is a choice. However context can reverse these senses, as in ''your choices are coffee and tea'' which usually means the same as ''your choices are coffee or tea'' (alternatives). Double negation as in \"I don't not like milk\" rarely means literally \"I do like milk\" but rather conveys some sort of hedging, as though to imply that there is a third possibility. \"Not not P\" can be loosely interpreted as \"surely P\", and although ''P'' necessarily implies \"not not ''P''\" the converse is suspect in English, much as with [[intuitionistic logic]]. In view of the highly idiosyncratic usage of conjunctions in natural languages, Boolean algebra cannot be considered a reliable framework for interpreting them.\n\nBoolean operations are used in [[digital logic]] to combine the bits carried on individual wires, thereby interpreting them over {0,1}. When a vector of ''n'' identical binary gates are used to combine two bit vectors each of ''n'' bits, the individual bit operations can be understood collectively as a single operation on values from a [[Boolean algebra (structure)|Boolean algebra]] with 2<sup>''n''</sup> elements.\n\n[[Naive set theory]] interprets Boolean operations as acting on subsets of a given set ''X''. As we saw earlier this behavior exactly parallels the coordinate-wise combinations of bit vectors, with the union of two sets corresponding to the disjunction of two bit vectors and so on.\n\nThe 256-element free Boolean algebra on three generators is deployed in [[computer displays]] based on [[raster graphics]], which use [[bit blit]] to manipulate whole regions consisting of [[pixels]], relying on Boolean operations to specify how the source region should be combined with the destination, typically with the help of a third region called the [[Mask (computing)|mask]]. Modern [[video cards]] offer all 2<sup>2<span><sup>3</sup></span></sup>&nbsp;=&nbsp;256 ternary operations for this purpose, with the choice of operation being a one-byte (8-bit) parameter. The constants SRC = 0xaa or 10101010, DST = 0xcc or 11001100, and MSK = 0xf0 or 11110000 allow Boolean operations such as (SRC^DST)&MSK (meaning XOR the source and destination and then AND the result with the mask) to be written directly as a constant denoting a byte calculated at compile time, 0x60 in the (SRC^DST)&MSK example, 0x66 if just SRC^DST, etc. At run time the video card interprets the byte as the raster operation indicated by the original expression in a uniform way that requires remarkably little hardware and which takes time completely independent of the complexity of the expression.\n\n[[Solid modeling]] systems for [[computer aided design]] offer a variety of methods for building objects from other objects, combination by Boolean operations being one of them. In this method the space in which objects exist is understood as a set ''S'' of [[voxel]]s (the three-dimensional analogue of pixels in two-dimensional graphics) and shapes are defined as subsets of ''S'', allowing objects to be combined as sets via union, intersection, etc. One obvious use is in building a complex shape from simple shapes simply as the union of the latter. Another use is in sculpting understood as removal of material: any grinding, milling, routing, or drilling operation that can be performed with physical machinery on physical materials can be simulated on the computer with the Boolean operation ''x''&nbsp;∧&nbsp;¬''y'' or ''x''&nbsp;−&nbsp;''y'', which in set theory is set difference, remove the elements of ''y'' from those of ''x''. Thus given two shapes one to be machined and the other the material to be removed, the result of machining the former to remove the latter is described simply as their set difference.\n\n====Boolean searches====\n<!-- \"Boolean search\" redirects here. -->\nSearch engine queries also employ Boolean logic. For this application, each web page on the Internet may be considered to be an \"element\" of a \"set\". The following examples use a syntax previously supported by [[Google]].<ref>Not all search engines support the same query syntax. Additionally, some organizations (such as Google) provide \"specialized\" search engines that support alternate or extended syntax. (See e.g.,[https://www.google.com/help/cheatsheet.html Syntax cheatsheet], [https://www.google.com/intl/en/help/faq_codesearch.html#regexp Google codesearch supports regular expressions]).</ref>\n* Doublequotes are used to combine whitespace-separated words into a single search term.<ref>Doublequote-delimited search terms are called \"exact phrase\" searches in the Google documentation.</ref>\n* Whitespace is used to specify logical AND, as it is the default operator for joining search terms:\n\n \"Search term 1\" \"Search term 2\"\n* The OR keyword is used for logical OR:\n\n \"Search term 1\" OR \"Search term 2\"\n* A prefixed minus sign is used for logical NOT:\n\n \"Search term 1\" −\"Search term 2\"\n<!-- (let's get this speculation resolved before including it in the article)\n* Parentheses do not appear to be supported for explicitly specifying the order of operations.\n\n[[Google Scholar]] appears to perform the XOR operation when the OR keyword is used.\n-->\n\n==See also==\n{{portal|Mathematics}}\n{{Div col|colwidth=30em}}\n* [[Binary number]]\n* [[Boolean algebra (structure)]]\n* [[Boolean algebras canonically defined]]\n* [[Boolean differential calculus]]\n* [[Booleo]]\n* [[Heyting algebra]]\n* [[Intuitionistic logic]]\n* [[List of Boolean algebra topics]]\n* [[Logic design]]\n* [[Propositional calculus]]\n* [[Relation algebra]]\n* [[Three-valued logic]]\n* [[Vector logic]]\n{{div col end}}\n\n==References==\n{{Reflist|30em}}\n\n===General===\n{{cite book|last1=Mano|first1=Morris|last2=Ciletti|first2=Michael D.|title=Digital Design|year=2013|publisher=Pearson|isbn=978-0-13-277420-8}}\n\n==Further reading==\n* {{cite book|author=J. Eldon Whitesitt|title=Boolean algebra and its applications|year=1995|publisher=Courier Dover Publications|isbn=978-0-486-68483-3}} Suitable introduction for students in applied fields.\n* {{Cite book\n  | last = Dwinger\n  | first = Philip\n  | title = Introduction to Boolean algebras\n  | publisher = Physica Verlag\n  | location = Würzburg\n  | year = 1971 }}\n* {{Cite book\n  | last = Sikorski\n  | first = Roman\n  | authorlink = Roman Sikorski\n  | title = Boolean Algebras\n  | publisher = Springer-Verlag\n  | location = Berlin\n  | edition = 3/e\n  | year = 1969\n  | isbn = 978-0-387-04469-9 }}\n* [[Józef Maria Bocheński|Bocheński, Józef Maria]] (1959). ''A Précis of Mathematical Logic''. Translated from the French and German editions by Otto Bird. Dordrecht, South Holland:  D. Reidel.\n\n'''Historical perspective'''\n* [[George Boole]] (1848). \"[http://www.maths.tcd.ie/pub/HistMath/People/Boole/CalcLogic/CalcLogic.html The Calculus of Logic,]\" ''Cambridge and Dublin Mathematical Journal III: 183–98.\n* {{cite book|author=Theodore Hailperin|title=Boole's logic and probability: a critical exposition from the standpoint of contemporary algebra, logic, and probability theory|year=1986|publisher=Elsevier|isbn=978-0-444-87952-3|edition=2nd}}\n* {{cite book|editor=Dov M. Gabbay, John Woods|title=The rise of modern logic: from Leibniz to Frege|volume=3|series=Handbook of the History of Logic|year=2004|publisher=Elsevier|isbn=978-0-444-51611-4}}, several relevant chapters by Hailperin, Valencia, and Grattan-Guinesss\n* {{cite book|author=Calixto Badesa|title=The birth of model theory: Löwenheim's theorem in the frame of the theory of relatives|year=2004|publisher=Princeton University Press|isbn=978-0-691-05853-5}}, chapter 1, \"Algebra of Classes and Propositional Calculus\"\n* Burris, Stanley, 2009. [http://plato.stanford.edu/entries/algebra-logic-tradition/ The Algebra of Logic Tradition]. [[Stanford Encyclopedia of Philosophy]].\n* {{cite book|author1=Radomir S. Stankovic|author2=Jaakko Astola|title=From Boolean Logic to Switching Circuits and Automata: Towards Modern Information Technology|url=https://books.google.com/books?id=uagvEc2jGTIC|year=2011|publisher=Springer|isbn=978-3-642-11681-0}}\n\n== External links ==\n{{Wikibooks|How To Search|Boolean Logic}}\n{{Wikibooks|Electronics|Boolean Algebra}}\n* [http://www.allaboutcircuits.com/vol_4/chpt_7/1.html Boolean Algebra] chapter on All About Circuits\n* [http://computer.howstuffworks.com/boolean.htm How Stuff Works – Boolean Logic]\n* [http://oscience.info/mathematics/boolean-algebra-2/ Science and Technology - Boolean Algebra] contains a list and proof of Boolean theorems and laws.\n\n{{Digital systems}}\n{{Computer science}}\n{{Areas of mathematics |collapsed}}\n{{Mathematical logic}}\n\n{{Authority control}}\n\n[[Category:1847 introductions]]\n[[Category:Boolean algebra| ]]\n[[Category:Algebraic logic]]\n[[Category:Articles with example code]]"
    },
    {
      "title": "List of Boolean algebra topics",
      "url": "https://en.wikipedia.org/wiki/List_of_Boolean_algebra_topics",
      "text": "{{short description|Wikimedia list article}}\nThis is a list of topics around '''Boolean algebra''' and '''propositional logic'''.\n\n== Articles with a wide scope and introductions ==\n\n* [[Algebra of sets]] [[Talk:Algebra of sets| ]]\n* [[Boolean algebra (structure)]] [[Talk:Boolean algebra (structure)| ]]\n* [[Boolean algebra]] [[Talk:Boolean algebra (logic)| ]]\n* [[Field of sets]] [[Talk:Field of sets| ]]\n* [[Logical connective]] [[Talk:logical connective|  ]]\n* [[Propositional calculus]] [[Talk:propositional calculus| ]]\n\n== Boolean functions and connectives ==\n\n* [[Ampheck]] [[Talk:Ampheck| ]]\n* [[Boolean algebras canonically defined]] [[Talk:Boolean algebras canonically defined| ]]\n* [[Conditioned disjunction]] [[Talk:Conditioned disjunction| ]]\n* [[Evasive Boolean function]] [[Talk:Evasive Boolean function| ]]\n* [[Exclusive or]] [[Talk:Exclusive or| ]]\n* [[Functional completeness]] [[Talk:Functional completeness| ]]\n* [[Logical biconditional]] [[Talk:Logical biconditional| ]]\n* [[Logical conjunction]] [[Talk:Logical conjunction| ]]\n* [[Logical disjunction]] [[Talk:Logical disjunction| ]]\n* [[Logical equality]] [[Talk:Logical equality| ]]\n* [[Logical implication]] [[Talk:Logical implication| ]]\n* [[Logical negation]] [[Talk:Logical negation| ]]\n* [[Logical NOR]] [[Talk:Logical NOR| ]]\n* [[Lupanov representation]] [[Talk:Lupanov representation| ]]\n* [[Majority function]] [[Talk:Majority function| ]]\n* [[Material conditional]] [[Talk:Material conditional| ]]\n* [[Peirce arrow]] [[Talk:Peirce arrow| ]]\n* [[Sheffer stroke]] [[Talk:Sheffer stroke| ]]\n* [[Sole sufficient operator]] [[Talk:Sole sufficient operator| ]]\n* [[Symmetric Boolean function]] [[Talk:Symmetric Boolean function| ]]\n* [[Symmetric difference]] [[Talk:Symmetric difference| ]]\n* [[Zhegalkin polynomial]] [[Talk:Zhegalkin polynomial| ]]\n\n== Examples of Boolean algebras ==\n\n* [[Boolean domain]] [[Talk:Boolean domain| ]]\n* [[Interior algebra]] [[Talk:interior algebra| ]]\n* [[Lindenbaum–Tarski algebra]] [[Talk:Lindenbaum–Tarski algebra| ]]\n* [[Two-element Boolean algebra]] [[Talk:Two-element Boolean algebra| ]]\n\n== Extensions and generalizations ==\n\n* [[Complete Boolean algebra]] [[Talk:Complete Boolean algebra| ]]\n* [[Derivative algebra (abstract algebra)]] [[Talk:Derivative algebra (abstract algebra)| ]]\n* [[First-order logic]] [[Talk:First-order logic| ]]\n* [[Free Boolean algebra]] [[Talk:free Boolean algebra| ]]\n* [[De Morgan algebra]] [[Talk:De Morgan algebra| ]]\n* [[Heyting algebra]] [[Talk:Heyting algebra| ]]\n* [[Monadic Boolean algebra]] [[Talk:monadic Boolean algebra| ]]\n* [[Skew lattice#Skew Boolean algebras|skew Boolean algebra]] [[Talk:Skew lattice| ]]\n\n== Syntax ==\n\n* [[Algebraic normal form]] [[Talk:Algebraic normal form| ]]\n* [[Boolean conjunctive query]] [[Talk:Boolean conjunctive query| ]]\n* [[Canonical form (Boolean algebra)]] [[Talk:normal form (Boolean algebra)| ]]\n* [[Conjunctive normal form]] [[Talk:Conjunctive normal form| ]]\n* [[Disjunctive normal form]] [[Talk:Disjunctive normal form| ]]\n* [[Formal system]] [[Talk:Formal system| ]]\n\n== Technical applications ==\n\n* [[And-inverter graph]] [[Talk:And-inverter graph| ]]\n* [[Logic gate]] [[Talk:logic gate| ]]\n* [[Boolean analysis]] [[Talk:Boolean analysis| ]]\n\n== Theorems and specific laws ==\n\n* [[Boolean prime ideal theorem]] [[Talk:Boolean prime ideal theorem| ]]\n* [[Compactness theorem]] [[Talk:compactness theorem| ]]\n* [[Consensus theorem]] [[Talk:Consensus theorem| ]]\n* [[De Morgan's laws]] [[Talk:de Morgan's laws| ]]\n* [[Duality (order theory)]] [[Talk:duality (order theory)| ]]\n* [[Laws of classical logic]] [[Talk:Laws of classical logic| ]]\n* [[Peirce's law]] [[Talk:Peirce's law| ]]\n* [[Stone's representation theorem for Boolean algebras]] [[Talk:Stone's representation theorem for Boolean algebras| ]]\n\n== People ==\n\n* [[George Boole|Boole, George]] [[Talk:George Boole| ]]\n* [[Augustus De Morgan|De Morgan, Augustus]] [[Talk:Augustus De Morgan| ]]\n* [[William Stanley Jevons|Jevons, William Stanley]] [[Talk:William Stanley Jevons| ]]\n* [[Charles Sanders Peirce|Peirce, Charles Sanders]] [[Talk:Charles Peirce| ]]\n* [[Marshall Harvey Stone|Stone, Marshall Harvey]] [[Talk:Marshall Harvey Stone| ]]\n* [[John Venn|Venn, John]] [[Talk:John Venn| ]]\n* [[Ivan Ivanovich Zhegalkin|Zhegalkin, Ivan Ivanovich]] [[Talk:Ivan Ivanovich Zhegalkin| ]]\n\n== Philosophy ==\n\n* [[Boole's syllogistic]] [[Talk:Boole's syllogistic| ]]\n* [[Implicant|Boolean implicant]] [[Talk:Implicant| ]]\n* [[Entitative graph]] [[Talk:Entitative graph| ]]\n* [[Existential graph]] [[Talk:Existential graph| ]]\n* ''[[Laws of Form]]'' [[Talk:Laws of Form| ]]\n* [[Logical graph]] [[Talk:Logical graph| ]]\n\n== Visualization ==\n\n* [[Truth table]] [[Talk:truth table| ]]\n* [[Karnaugh map]] [[Talk:Karnaugh map| ]]\n* [[Venn diagram]] [[Talk:Venn diagram| ]]\n\n== Unclassified ==\n\n* [[Boolean function]] [[Talk:Boolean function| ]]\n* [[Boolean-valued function]] [[Talk:Boolean-valued function| ]]\n* [[Boolean-valued model]] [[Talk:Boolean-valued model| ]]\n* [[Boolean satisfiability problem]] [[Talk:Boolean satisfiability problem| ]]\n* [[Boolean differential calculus]]\n* [[Indicator function]] [[Talk:Indicator function| ]] (also called the ''characteristic function'', but that term is used in probability theory for a different concept)\n* [[Minilog|Espresso heuristic logic minimizer]]\n* [[Logical matrix]] [[Talk:Logical matrix| ]]\n* [[Logical value]] [[Talk:Logical value| ]]\n* [[Stone duality]] [[Talk:Stone duality| ]]\n* [[Stone's representation theorem for Boolean algebras|Stone space]]\n* [[Topological Boolean algebra (disambiguation)|Topological Boolean algebra]] [[Talk:topological Boolean algebra| ]]\n\n{{logic}}\n\n[[Category:Mathematics-related lists|Boolean algebra]]\n[[Category:Boolean algebra| ]]\n[[Category:Wikipedia outlines|Boolean algebra]]"
    },
    {
      "title": "2-valued morphism",
      "url": "https://en.wikipedia.org/wiki/2-valued_morphism",
      "text": "'''2-valued morphism''' is a term used in [[mathematics]]<ref>{{citation\n | last = Fleischer | first = Isidore\n | contribution = A Boolean formalization of predicate calculus\n | mr = 1233791\n | pages = 193–198\n | publisher = Kluwer Acad. Publ., Dordrecht\n | series = NATO Adv. Sci. Inst. Ser. C Math. Phys. Sci.\n | title = Algebras and orders (Montreal, PQ, 1991)\n | volume = 389\n | year = 1993}}.</ref> to describe a [[morphism]] that sends a [[Boolean algebra (structure)|Boolean algebra]] ''B'' onto a [[two-element Boolean algebra]] '''2''' = {0,1}. It is essentially the same thing as an [[ultrafilter]] on ''B''.\n\nA 2-valued morphism can be interpreted as representing a particular state of ''B''. All [[Proposition (mathematics)|proposition]]s of ''B'' which are mapped to 1 are considered true, all propositions mapped to 0 are considered false. Since this morphism conserves the Boolean operators ([[negation]], [[Logical conjunction|conjunction]], etc.), the set of true propositions will not be inconsistent but will correspond to a particular maximal conjunction of propositions, denoting the (atomic) state. \n\nThe transition between two states ''s''<sub>1</sub> and ''s''<sub>2</sub> of ''B'', represented by 2-valued morphisms, can then be represented by an [[automorphism]] ''f'' from ''B'' to ''B'', such tuhat ''s''<sub>2</sub> o ''f'' = ''s''<sub>1</sub>.\n\nThe possible states of different objects defined in this way can be conceived as representing potential events. The set of events can then be structured in the same way as invariance of causal structure, or local-to-global causal connections or even formal properties of global causal connections.\n\nThe morphisms between (non-trivial) objects could be viewed as representing causal connections leading from one event to another one. For example, the morphism ''f'' above leads form event ''s''<sub>1</sub> to event ''s''<sub>2</sub>. The sequences or \"paths\" of morphisms for which there is no inverse morphism, could then be interpreted as defining [[horismotic]] or chronological precedence relations. These relations would then determine a temporal [[Sequence|order]], a [[topology]], and possibly a [[Metric (mathematics)|metric]].\n\nAccording to,<ref name=\"Heyligen\">{{cite book\n| last = Heylighen\n| first = Francis\n| year = 1990\n| title = A Structural Language for the Foundations of Physics\n| publisher = International Journal of General Systems 18, p. 93-112\n| location = Brussels\n}}</ref> \"A minimal realization of such a relationally determined space-time structure can be found\". In this model there are, however, no explicit distinctions. This is equivalent to a model where each object is characterized by only one distinction: (presence, absence) or (existence, non-existence) of an event.   In this manner, \"the 'arrows' or the 'structural language' can then be interpreted as morphisms which conserve this unique distinction\".<ref name=\"Heyligen\"/>\n\nIf more than one distinction is considered, however, the model becomes much more complex, and the interpretation of distinctional states as events, or morphisms as processes, is much less straightforward.\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://pcp.vub.ac.be/books/Rep&Change.pdf \"Representation and Change - A metarepresentational framework for the foundations of physical and cognitive science\"]\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Algebraic normal form",
      "url": "https://en.wikipedia.org/wiki/Algebraic_normal_form",
      "text": "{{mergefrom|Zhegalkin polynomial|date=April 2019}}\n{{refimprove|date=July 2013}}\n\nIn [[Boolean algebra]], the '''algebraic normal form''' ('''ANF'''), '''ring sum normal form''' ('''RSNF''' or '''RNF'''), ''[[Zhegalkin normal form]]'', or ''[[Reed–Muller expansion]]'' is a way of writing logical formulas in one of three subforms:\n\n* The entire formula is purely true or false:\n*: 1\n*: 0\n* One or more variables are [[logical conjunction|AND]]ed together into a term, then one or more terms are [[exclusive or|XOR]]ed together into ANF. No [[negation|NOT]]s are permitted:\n*: a ⊕ b ⊕ ab ⊕ abc\n:or in standard propositional logic symbols:\n::<math> a \\veebar b \\veebar \\left(a \\wedge b\\right) \\veebar \\left(a \\wedge b \\wedge c\\right) </math>\n* The previous subform with a purely true term:\n*: 1 ⊕ a ⊕ b ⊕ ab ⊕ abc\n\n{{anchor|PPRM}}Formulas written in ANF are also known as [[Zhegalkin polynomial]]s ({{lang-ru|полиномы Жегалкина}}) and Positive Polarity (or Parity) [[Reed–Muller code|Reed–Muller expressions]] (PPRM).<ref name=\"Steinbach_2009\"/>\n\n== Common uses ==\nANF is a [[canonical form (Boolean algebra)|normal form]], which means that two equivalent formulas will convert to the same ANF, easily showing whether two formulas are equivalent for [[automated theorem proving]]. Unlike other normal forms, it can be represented as a simple list of lists of variable names. [[Conjunctive normal form|Conjunctive]] and [[disjunctive normal form|disjunctive]] normal forms also require recording whether each variable is negated or not. [[Negation normal form]] is unsuitable for that purpose, since it doesn't use equality as its equivalence relation: a &or; ¬a isn't reduced to the same thing as 1, even though they're equal.\n\nPutting a formula into ANF also makes it easy to identify [[linearity|linear]] functions (used, for example, in [[linear feedback shift register]]s): a linear function is one that is a sum of single literals.  Properties of nonlinear feedback [[shift register]]s can also be deduced from certain properties of the feedback function in ANF.\n\n== Performing operations within algebraic normal form ==\nThere are straightforward ways to perform the standard boolean operations on ANF inputs in order to get ANF results.\n\nXOR (logical exclusive disjunction) is performed directly:\n: ({{fontcolor|red|1 ⊕ x}}) ⊕ ({{fontcolor|green|1 ⊕ x ⊕ y}})\n: {{fontcolor|red|1 ⊕ x}} ⊕ {{fontcolor|green|1 ⊕ x ⊕ y}}\n: 1 ⊕ 1 ⊕ x ⊕ x ⊕ y\n: y\n\nNOT (logical negation) is XORing 1:<ref name=\"not-equiv\">[http://www.wolframalpha.com/input/?i=simplify+1+xor+a WolframAlpha NOT-equivalence demonstration: ¬a = 1 ⊕ a]</ref>\n: {{fontcolor|red|¬}}{{fontcolor|green|(1 ⊕ x ⊕ y)}}\n: {{fontcolor|red|1 ⊕ }}{{fontcolor|green|(1 ⊕ x ⊕ y)}}\n: 1 ⊕ 1 ⊕ x ⊕ y\n: x ⊕ y\n\nAND (logical conjunction) is [[distributive property|distributed algebraically]]<ref name=\"and-equiv\">[http://www.wolframalpha.com/input/?i=%28a+xor+b%29+and+%28c+xor+d%29+in+anf WolframAlpha AND-equivalence demonstration: (a ⊕ b)(c ⊕ d) = ac ⊕ ad ⊕ bc ⊕ bd]</ref>\n: ({{fontcolor|red|1}} ⊕ {{fontcolor|red|x}}){{fontcolor|green|(1 ⊕ x ⊕ y)}}\n: {{fontcolor|red|1}}{{fontcolor|green|(1 ⊕ x ⊕ y)}} ⊕ {{fontcolor|red|x}}{{fontcolor|green|(1 ⊕ x ⊕ y)}}\n: (1 ⊕ x ⊕ y) ⊕ (x ⊕ x ⊕ xy)\n: 1 ⊕ x ⊕ x ⊕ x ⊕ y ⊕ xy\n: 1 ⊕ x ⊕ y ⊕ xy\n\nOR (logical disjunction) uses either 1 ⊕ (1 ⊕ a)(1 ⊕ b)<ref name=\"or-demorgans\">From [[De Morgan's laws]]</ref> (easier when both operands have purely true terms) or a ⊕ b ⊕ ab<ref name=\"or-equiv\">[http://www.wolframalpha.com/input/?i=simplify+a+xor+b+xor+%28a+and+b%29 WolframAlpha OR-equivalence demonstration: a + b = a ⊕ b ⊕ ab]</ref> (easier otherwise):\n: ({{fontcolor|red|1 ⊕ x}}) + ({{fontcolor|green|1 ⊕ x ⊕ y}})\n: 1 ⊕ (1 ⊕ {{fontcolor|red|1 ⊕ x}})(1 ⊕ {{fontcolor|green|1 ⊕ x ⊕ y}})\n: 1 ⊕ x(x ⊕ y)\n: 1 ⊕ x ⊕ xy\n\n== Converting to algebraic normal form ==\nEach variable in a formula is already in pure ANF, so you only need to perform the formula's boolean operations as shown above to get the entire formula into ANF. For example:\n: x + (y &middot; ¬z)\n: x + (y(1 ⊕ z))\n: x + (y ⊕ yz)\n: x ⊕ (y ⊕ yz) ⊕ x(y ⊕ yz)\n: x ⊕ y ⊕ xy ⊕ yz ⊕ xyz\n\n== Formal representation ==\nANF is sometimes described in an equivalent way:\n:{| cellpadding=\"4\"\n|-\n|<math>f(x_1, x_2, \\ldots, x_n) = \\!</math>\n|<math>a_0 \\oplus \\!</math>\n|-\n|\n|<math>a_1x_1 \\oplus a_2x_2 \\oplus \\cdots \\oplus a_nx_n \\oplus \\!</math>\n|-\n|\n|<math>a_{1,2}x_1x_2 \\oplus \\cdots \\oplus a_{n-1,n}x_{n-1}x_n \\oplus \\!</math>\n|-\n|\n|<math>\\cdots \\oplus \\!</math>\n|-\n|\n|<math>a_{1,2,\\ldots,n}x_1x_2\\ldots x_n \\!</math>\n|}\n:where <math>a_0, a_1, \\ldots, a_{1,2,\\ldots,n} \\in \\{0,1\\}^*</math> fully describes <math>f</math>.\n\n=== Recursively deriving multiargument Boolean functions ===\nThere are only four functions with one argument:\n* <math>f(x)=0</math>\n* <math>f(x)=1</math>\n* <math>f(x)=x</math>\n* <math>f(x)=1 \\oplus x</math>\n\nTo represent a function with multiple arguments one can use the following equality:\n: <math>f(x_1,x_2,\\ldots,x_n) = g(x_2,\\ldots,x_n) \\oplus x_1 h(x_2,\\ldots,x_n)</math>, where\n:* <math>g(x_2,\\ldots,x_n) = f(0,x_2,\\ldots,x_n)</math>\n:* <math>h(x_2,\\ldots,x_n) = f(0,x_2,\\ldots,x_n) \\oplus f(1,x_2,\\ldots,x_n)</math>\n\nIndeed,\n* if <math>x_1=0</math> then <math>x_1 h = 0</math> and so <math>f(0,\\ldots) = f(0,\\ldots)</math>\n* if <math>x_1=1</math> then <math>x_1 h = h</math> and so <math>f(1,\\ldots) = f(0,\\ldots) \\oplus f(0,\\ldots) \\oplus f(1,\\ldots)</math>\n\nSince both <math>g</math> and <math>h</math> have fewer arguments than <math>f</math> it follows that using this process recursively we will finish with functions with one variable. For example, let us construct ANF of <math>f(x,y)= x \\lor y</math> (logical or):\n* <math>f(x,y) = f(0,y) \\oplus x(f(0,y) \\oplus f(1,y))</math>\n* since <math>f(0,y)=0 \\lor y = y</math> and <math>f(1,y)=1 \\lor y = 1</math>\n* it follows that <math>f(x,y) = y \\oplus x (y \\oplus 1)</math>\n* by distribution, we get the final ANF: <math>f(x,y) = y \\oplus x y \\oplus x = x \\oplus y \\oplus x y</math>\n\n==See also==\n* [[Reed–Muller expansion]]\n* [[Zhegalkin normal form]]\n* [[Boolean function]]\n* [[Logical graph]]\n* [[Zhegalkin polynomial]]\n* [[Negation normal form]]\n* [[Conjunctive normal form]]\n* [[Disjunctive normal form]]\n* [[Karnaugh map]]\n* [[Boolean ring]]\n\n== References ==\n{{reflist|refs=\n<ref name=\"Steinbach_2009\">{{cite book |author-first1=Bernd |author-last1=Steinbach |author-link1=:de:Bernd Steinbach |author-first2=Christian |author-last2=Posthoff |title=Logic Functions and Equations - Examples and Exercises |chapter=Preface |publisher=[[Springer Science + Business Media B. V.]] |date=2009 |edition=1st |isbn=978-1-4020-9594-8 |lccn=2008941076 |page=xv}}</ref>\n}}\n\n== Further reading==\n* {{cite book |author-first=Ingo |author-last=Wegener |title=The complexity of Boolean functions |publisher=[[Wiley-Teubner]] |date=1987 |isbn=3-519-02107-2 |page=6 |url=http://ls2-www.cs.uni-dortmund.de/monographs/bluebook}}\n* {{cite web |title=Presentation |publisher=[[University of Duisburg-Essen]] |language=German |url=http://www.is.informatik.uni-duisburg.de/courses/infoa_ss03/slides/02-slides.pdf#page=34 |access-date=2017-04-19 |dead-url=no |archive-url=https://web.archive.org/web/20170420000915/http://www.is.informatik.uni-duisburg.de/courses/infoa_ss03/slides/02-slides.pdf#page=34 |archive-date=2017-04-19}}\n* {{cite web |title=Reed-Muller Logic |work=Logic 101 |at=Part 3 |author-first=Clive \"Max\" |author-last=Maxfield |date=2006-11-29 |publisher=[[EETimes]] |url=http://www.eetimes.com/author.asp?section_id=216&doc_id=1274545 |access-date=2017-04-19 |dead-url=no |archive-url=https://web.archive.org/web/20170419235904/http://www.eetimes.com/author.asp?section_id=216&doc_id=1274545 |archive-date=2017-04-19}}\n\n[[Category:Boolean algebra]]\n[[Category:Normal forms (logic)]]\n\n[[ru:Полином Жегалкина]]"
    },
    {
      "title": "Balanced boolean function",
      "url": "https://en.wikipedia.org/wiki/Balanced_boolean_function",
      "text": "In [[mathematics]] and [[computer science]], a '''balanced boolean function''' is a [[boolean function]] whose output yields as many '''0'''s as '''1'''s over its [[Domain of a function|input set]]. This means that for a uniformly random input string of bits, the probability of getting a '''1''' is 1/2.\n\nExamples of balanced boolean functions are the function that copies the first bit of its input to the output,\nand the function that produces the [[exclusive or]] of the input bits.\n\n== Usage ==\nBalanced boolean functions are primarily used in [[cryptography]].  If a function is not balanced, it will have a [[statistical bias]], making it subject to [[cryptanalysis]] such as the [[correlation attack]].\n\n== See also ==\n* [[Bent function]]\n\n== References ==\n* [http://portal.acm.org/citation.cfm?id=1060627 Balanced boolean functions that can be evaluated so that every input bit is unlikely to be read], Annual ACM Symposium on Theory of Computing\n\n[[Category:Boolean algebra]]\n\n{{comp-sci-theory-stub}}\n{{crypto-stub}}"
    },
    {
      "title": "Bent function",
      "url": "https://en.wikipedia.org/wiki/Bent_function",
      "text": "[[File:Boolean functions like 1000 nonlinearity.svg|thumb|The four 2-ary Boolean functions with [[Hamming weight]] 1 are bent, i.e. their nonlinearity is 1 <small>(which is what this diagram shows)</small>.<br><br>The following formula shows that a 2-ary function is bent when its nonlinearity is 1:<br><math>2^{2-1} - 2^{\\frac{2}{2}-1} = 2-1 = 1</math>]]\n[[File:0001 0001 0001 1110 nonlinearity.svg|thumb|The Boolean function <math>x_1 x_2~+~x_3 x_4</math> is bent, i.e. its nonlinearity is 6 <small>(which is what this diagram shows)</small>.<br><br>The following formula shows that a 4-ary function is bent when its nonlinearity is 6:<br><math>2^{4-1} - 2^{\\frac{4}{2}-1} = 8-2 = 6</math><br><br><math>~+~</math> stands for the [[exclusive or]]<br>(compare [[algebraic normal form]])]]\n\nIn the [[mathematics|mathematical]] field of [[combinatorics]], a '''bent function''' is a special type of [[Boolean function]]. This means it takes several inputs and gives one output, each of which has two possible values (such as ''0'' and ''1'', or ''true'' and ''false''). The name is figurative. Bent functions are so called because they are as different as possible from all [[linear map|linear functions]] (the simplest or \"straight-line\" functions) and from all [[affine function]]s (which preserve parallel lines). This makes the bent functions naturally hard to approximate. Bent functions were defined and named in the 1960s by [[Oscar Rothaus]] in research not published until 1976.<ref name=rothaus/> They have been extensively studied for their applications in [[cryptography]], but have also been applied to [[spread spectrum]], [[coding theory]], and [[combinatorial design]]. The definition can be extended in several ways, leading to different classes of generalized bent functions that share many of the useful properties of the original.\n\nIt is known that V. A. Eliseev and O. P. Stepchenkov studied bent functions, which they called ''minimal functions'', in the USSR in 1962.<ref name=bent-book/> However, their results have still not been declassified.\n\n== Walsh transform ==\nBent functions are defined in terms of the [[Walsh transform]]. The Walsh transform of a Boolean function <math>f:\\Z_2^n \\to \\Z_2</math> is the function <math>\\hat{f}:\\Z_2^n \\to \\Z</math> given by\n:<math> \\hat{f}(a) =\\sum_{\\scriptstyle{x \\in \\Z_2^n}} (-1)^{f(x) + a \\cdot x} </math>\nwhere {{nowrap|1=''a'' · ''x'' = ''a''<sub>1</sub>''x''<sub>1</sub> + ''a''<sub>2</sub>''x''<sub>2</sub> + ... + ''a''<sub>''n''</sub>''x''<sub>''n''</sub> (mod 2)}} is the [[dot product]] in '''Z'''{{sup sub|''n''|2}}.<ref name=bool/> Alternatively, let\n{{nowrap|1=''S''<sub>0</sub>(''a'') = { ''x'' ∈ '''Z'''{{sup sub|''n''|2}} : ''f''(''x'') = ''a'' · ''x'' } }} and\n{{nowrap|1=''S''<sub>1</sub>(''a'') = { ''x'' ∈ '''Z'''{{sup sub|''n''|2}} : ''f''(''x'') ≠ ''a'' · ''x'' } }}.\nThen {{nowrap|1={{abs|''S''<sub>0</sub>(''a'')}} + {{abs|''S''<sub>1</sub>(''a'')}} = 2<sup>''n''</sup>}} and hence\n:<math> \\hat{f}(a) = |S_0(a)| - |S_1(a)| = 2 |S_0(a)| - 2^n. </math>\nFor any Boolean function ''f'' and {{nowrap|''a'' ∈ '''Z'''{{sup sub|''n''|2}}}} the transform lies in the range\n:<math> -2^n \\leq \\hat{f}(a) \\leq 2^n. </math>\nMoreover, the linear function\n{{nowrap|1=''f''<sub>0</sub>(''x'') = ''a'' · ''x''}}\nand the affine function\n{{nowrap|1=''f''<sub>1</sub>(''x'') = ''a'' · ''x'' + 1}}\ncorrespond to the two extreme cases, since\n:<math>\n  \\hat{f}_0(a) = 2^n,~\n  \\hat{f}_1(a) = -2^n.\n</math>\nThus, for each {{nowrap|''a'' ∈ '''Z'''{{sup sub|''n''|2}}}} the value of <math>\\hat{f}(a)</math> characterizes where the function ''f''(''x'') lies in the range from ''f''<sub>0</sub>(''x'') to ''f''<sub>1</sub>(''x'').\n\n== Definition and properties ==\n\nRothaus defined a '''bent function''' as a Boolean function <math>f:\\Z_2^n \\to \\Z_2</math> whose [[Walsh transform]] has constant [[absolute value]]. Bent functions are in a sense equidistant from all the affine functions, so they are equally hard to approximate with any affine function.\n\nThe simplest examples of bent functions, written in [[algebraic normal form]], are ''F''(''x''<sub>1</sub>,''x''<sub>2</sub>) =\n''x''<sub>1</sub>''x''<sub>2</sub> and ''G''(''x''<sub>1</sub>,''x''<sub>2</sub>,''x''<sub>3</sub>,''x''<sub>4</sub>) =\n''x''<sub>1</sub>''x''<sub>2</sub> + ''x''<sub>3</sub>''x''<sub>4</sub>. This pattern continues:\n''x''<sub>1</sub>''x''<sub>2</sub> + ''x''<sub>3</sub>''x''<sub>4</sub> + ... + ''x''<sub>''n''&nbsp;−&nbsp;1</sub>''x''<sub>''n''</sub> is a bent function <math>\\Z_2^n \\to \\Z_2</math> for every even ''n'', but there is a wide variety of different types of bent functions as ''n'' increases.<ref name=nonlin/> The sequence of values (−1)<sup>''f''(''x'')</sup>, with {{nowrap|''x'' ∈ '''Z'''{{sup sub|''n''|2}}}} taken in [[lexicographical order]], is called a '''bent sequence'''; bent functions and bent sequences have equivalent\nproperties. In this ±1 form, the Walsh transform is easily computed as\n:<math> \\hat{f}(a) = W(2^n) (-1)^{f(a)}, </math>\nwhere ''W''(2<sup>''n''</sup>) is the natural-ordered [[Walsh matrix]] and the sequence is treated as a [[column vector]].<ref name=dual/>\n\nRothaus proved that bent functions exist only for even ''n'', and that for a bent function ''f'',\n<math>|\\hat{f}(a)|=2^{n/2}</math> for all {{nowrap|''a'' ∈ '''Z'''{{sup sub|''n''|2}}}}.<ref name=bool/> In fact, <math>\\hat{f}(a)=2^{n/2}(-1)^{g(a)}</math>, where ''g'' is also bent. In this case,\n<math>\\hat{g}(a)=2^{n/2}(-1)^{f(a)}</math>, so ''f'' and ''g'' are considered [[duality (mathematics)|dual]] functions.<ref name=dual/>\n\nEvery bent function has a [[Hamming weight]] (number of times it takes the value 1) of {{nowrap|2<sup>''n'' − 1</sup> ± 2<sup>''n''/2 − 1</sup>}}, and in fact agrees with any affine function at one of those two numbers of points. So the ''nonlinearity'' of ''f'' (minimum number of times it equals any affine function) is {{nowrap|2<sup>''n'' − 1</sup> − 2<sup>''n''/2 − 1</sup>}}, the maximum possible. Conversely, any Boolean function with nonlinearity {{nowrap|2<sup>''n'' − 1</sup> − 2<sup>''n''/2 − 1</sup>}} is bent.<ref name=bool/> The [[Degree of a polynomial|degree]] of ''f'' in algebraic normal form (called the ''nonlinear order'' of ''f'') is at most ''n''/2 (for {{nowrap|''n'' > 2}}).<ref name=nonlin/>\n\nAlthough bent functions are vanishingly rare among Boolean functions of many variables, they come\nin many different kinds. There has been detailed research into special classes of bent functions,\nsuch as the [[homogeneous polynomial|homogeneous]] ones<ref name=homo/> or those arising from\na [[monomial]] over a [[finite field]],<ref name=mono/> but so far the bent functions have defied\nall attempts at a complete enumeration or classification.\n\n== Constructions ==\n\nThere are several types of constructions for bent functions.<ref name=bent-book/>\n* combinatorial constructions: iterative constructions, Maiorana-McFarland construction, Partial Spreads, Dillon's and Dobbertin's bent functions, minterm bent functions, Bent Iterative functions\n* algebraic constructions: monomial bent functions with exponents of Gold, Dillon, Kasami, Canteaut-Leander and Canteaut-Charpin-Kuyreghyan; Niho bent functions, etc.\n\n== Applications ==\n\nAs early as 1982 it was discovered that [[maximum length sequence]]s based on bent functions have [[cross-correlation]] and [[autocorrelation]] properties rivalling those of the [[Gold code]]s and [[Kasami code]]s for use in [[CDMA]].<ref name=seq/> These sequences have several applications in [[spread spectrum]] techniques.\n\nThe properties of bent functions are naturally of interest in modern digital [[cryptography]], which seeks to obscure relationships between input and output. By 1988 Forré recognized that the Walsh transform of a function can be used to show that it satisfies the [[strict avalanche criterion]] (SAC) and higher-order generalizations, and recommended this tool to select candidates for good [[S-box]]es achieving near-perfect [[confusion and diffusion|diffusion]].<ref name=spectral/> Indeed, the functions satisfying the SAC to the highest possible order are always bent.<ref name=sac/> Furthermore, the bent functions are as far as possible from having what are called ''linear structures'', nonzero vectors a such that {{nowrap|''f''(''x'' + ''a'') + ''f''(''x'')}} is a constant. In the language of [[differential cryptanalysis]] (introduced after this property was discovered) the ''derivative'' of a bent function ''f'' at every nonzero point ''a'' (that is, {{nowrap|1=''f''<sub>''a''</sub>(''x'') = ''f''(''x'' + ''a'') + ''f''(''x''))}} is a [[balanced boolean function|''balanced'']] Boolean function, taking on each value exactly half of the time. This property is called ''perfect nonlinearity''.<ref name=nonlin/>\n\nGiven such good diffusion properties, apparently perfect resistance to differential cryptanalysis, and resistance by definition to [[linear cryptanalysis]], bent functions might at first seem the ideal choice for secure cryptographic functions such as S-boxes. Their fatal flaw is that they fail to be balanced. In particular, an invertible S-box cannot be constructed directly from bent functions, and a [[stream cipher]] using a bent combining function is vulnerable to a [[correlation attack]]. Instead, one might start with a bent function and randomly complement appropriate values until the result is balanced. The modified function still has high nonlinearity, and as such functions are very rare the process should be much faster than a brute-force search.<ref name=nonlin/> But functions produced in this way may lose other desirable properties, even failing to satisfy the SAC – so careful testing is necessary.<ref name=sac/> A number of cryptographers have worked on techniques for generating balanced functions that preserve as many of the good cryptographic qualities of bent functions as possible.<ref name=nyberg/><ref name=highly/><ref name=cast/>\n\nSome of this theoretical research has been incorporated into real cryptographic algorithms. The ''CAST'' design procedure, used by [[Carlisle Adams]] and [[Stafford Tavares]] to construct the S-boxes for the [[block ciphers]] [[CAST-128]] and [[CAST-256]], makes use of bent functions.<ref name=cast/> The [[cryptographic hash function]] [[HAVAL]] uses Boolean functions built from representatives of all four of the [[equivalence class]]es of bent functions on six variables.<ref name=haval/> The stream cipher [[Grain (cipher)|Grain]] uses an [[NLFSR]] whose nonlinear feedback polynomial is, by design, the sum of a bent function and a linear function.<ref name=grain/>\n\nApplications of bent functions are listed in a 2015 monograph by N. Tokareva, ''Bent functions: results and applications to cryptography''.<ref name=bent-book/>\n\n== Generalizations ==\n\nMore than 25 different generalizations of bent functions are described in Tokareva's 2015 monograph.<ref name=bent-book/> There are algebraic generalizations (q-valued bent functions, p-ary bent functions, bent functions over a finite field, generalized Boolean bent functions of Schmidt, bent functions from a finite Abelian group into the set of complex numbers on the unit circle, bent functions from a finite Abelian group into a finite Abelian group, non Abelian \nbent functions, vectorial G-bent functions, multidimensional bent functions on a finite Abelian group), combinatorial generalizations (symmetric bent functions, homogeneous bent functions, rotation symmetric bent functions, normal bent functions, self-dual and anti-self-dual bent functions, partially defined bent functions, plateaued functions, Z-bent functions and quantum bent functions) and cryptographic generalizations (semi-bent functions, balanced bent functions, partially bent functions, hyper-bent functions, bent functions of higher order, k-bent functions).\n\nThe most common class of ''generalized bent functions'' is the [[modular arithmetic|mod ''m'']] type,\n<math>f:\\mathbb{Z}_m^n \\to \\mathbb{Z}_m</math> such that\n:<math> \\hat{f}(a) = \\sum_{x \\in \\mathbb{Z}_m^n} e^{\\frac{2 \\pi i}{m} (f(x) - a \\cdot x)} </math>\nhas constant absolute value ''m''<sup>''n''/2</sup>. Perfect nonlinear functions <math>f:\\mathbb{Z}_m^n \\to \\mathbb{Z}_m</math>, those such that for all nonzero ''a'', {{nowrap|''f''(''x'' + ''a'') − ''f''(''a'')}} takes on each value {{nowrap|''m''<sup>''n'' − 1</sup>}} times, are generalized bent. If ''m'' is [[prime number|prime]], the converse is true. In most cases only prime ''m'' are considered. For odd prime ''m'', there are generalized bent functions for every positive ''n'', even and odd. They have many of the same good cryptographic properties as the binary bent functions.<ref name=nyberg2/>,<ref name=gbf2/>\n\n'''Semi-bent functions''' are an odd-order counterpart to bent functions. A semi-bent function is\n<math>f:\\mathbb{Z}_m^n \\to \\mathbb{Z}_m</math> with ''n'' odd, such that <math>|\\hat{f}|</math> takes only the values 0 and ''m''<sup>(''n''+1)/2</sup>. They also have good cryptographic characteristics, and some of them are balanced, taking on all possible values equally often.<ref name=semi/>\n\nThe '''partially bent functions''' form a large class defined by a condition on the Walsh transform and autocorrelation functions. All affine and bent functions are partially bent. This is in turn a proper subclass of the ''plateaued functions''.<ref name=plat/>\n\nThe idea behind the '''hyper-bent functions''' is to maximize the minimum distance to ''all'' Boolean\nfunctions coming from [[bijection|bijective]] monomials on the finite field GF(2<sup>''n''</sup>), not just the affine functions. For these functions this distance is constant, which may make them resistant to an [[interpolation attack]].\n\nOther related names have been given to cryptographically important classes of functions <math>f:\\Z_2^n \\to \\Z_2^n</math>, such as '''almost bent functions''' and '''crooked functions'''. While not bent functions themselves (these are not even Boolean functions), they are closely related to the bent functions and have good nonlinearity properties.\n\n== References ==\n\n{{Reflist|2|refs=\n\n<ref name=rothaus>{{Cite journal | author = O. S. Rothaus | title = On \"Bent\" Functions | journal = Journal of Combinatorial Theory, Series A | issn = 0097-3165 | volume = 20 | issue = 3 | pages = 300–305 |date=May 1976 | url = https://www.sciencedirect.com/science/article/pii/0097316576900248 | accessdate = 16 December 2013 | doi=10.1016/0097-3165(76)90024-8}}</ref>\n\n<ref name=bent-book>{{Cite book | author = N. Tokareva | title = Bent functions: results and applications to cryptography | \npublisher= Academic Press |year=2015 |isbn=9780128023181}}</ref>\n\n<ref name=bool>{{Cite journal | author = C. Qu |author2=[[Jennifer Seberry|J. Seberry]] |author3=T. Xia | date=29 December 2001| title = Boolean Functions in Cryptography | url = http://citeseer.ist.psu.edu/old/700097.html | accessdate = 14 September 2009}}</ref>\n\n<ref name=nonlin>{{cite conference | author = W. Meier |author2= O. Staffelbach | title = Nonlinearity Criteria for Cryptographic Functions | conference = [[Eurocrypt]] '89 |date=April 1989 | pages = 549–562}}</ref>\n\n<ref name=dual>{{cite conference | author = C. Carlet |author2=L.E. Danielsen|author3=M.G. Parker|author4=P. Solé | title = Self Dual Bent Functions | conference = Fourth International Workshop on Boolean Functions: Cryptography and Applications (BFCA '08) | url = http://www.ii.uib.no/~matthew/bfcasdb.pdf | date = 19 May 2008 | accessdate = 21 September 2009}}</ref>\n\n<ref name=homo>{{cite journal | author = T. Xia |author2=J. Seberry |author3=[[Josef Pieprzyk|J. Pieprzyk]] |author4=C. Charnes | title = Homogeneous bent functions of degree n in 2n variables do not exist for n > 3 | journal = Discrete Applied Mathematics | issn = 0166-218X | volume = 142 | issue = 1–3 | pages = 127–132 |date=June 2004 | url = http://ro.uow.edu.au/infopapers/291/ | accessdate = 21 September 2009 | doi = 10.1016/j.dam.2004.02.006}}</ref>\n\n<ref name=mono>{{cite journal | author = A. Canteaut|author2=P. Charpin|author3=G. Kyureghyan | title = A new class of monomial bent functions | journal = Finite Fields and Their Applications | issn = 1071-5797 | volume = 14 | issue = 1 | pages = 221–241 |date=January 2008 | url = http://www-roc.inria.fr/secret/Anne.Canteaut/Publications/CanChaKuy07.pdf | accessdate = 21 September 2009 | doi = 10.1016/j.ffa.2007.02.004}}</ref>\n\n<ref name=seq>{{cite journal | author = J. Olsen | author2 = R. Scholtz | author3 = L. Welch | title = Bent-Function Sequences | journal = [[IEEE Transactions on Information Theory]] | issn = 0018-9448 | volume = IT-28 | issue = 6 | pages = 858–864 | date = November 1982 | url = http://www.costasarrays.org/costasrefs/b2hd-olsen82bent-function.html | accessdate = 24 September 2009 | doi = 10.1109/tit.1982.1056589 | deadurl = yes | archiveurl = https://web.archive.org/web/20110722055600/http://www.costasarrays.org/costasrefs/b2hd-olsen82bent-function.html | archivedate = 22 July 2011}}</ref>\n\n<ref name=spectral>{{cite conference | author = R. Forré | title = The Strict Avalanche Criterion: Spectral Properties of Boolean Functions and an Extended Definition | conference = [[CRYPTO]] '88 |date=August 1988 | pages = 450–468}}</ref>\n\n<ref name=sac>{{Cite journal | author = [[Carlisle Adams|C. Adams]] |author2=S. Tavares |authorlink2=Stafford Tavares | title = The Use of Bent Sequences to Achieve Higher-Order Strict Avalanche Criterion in S-Box Design | version = Technical Report TR 90-013 | publisher = [[Queen's University]] |date=January 1990 | citeseerx = 10.1.1.41.8374 }}</ref>\n\n<ref name=nyberg>{{cite conference | author = [[Kaisa Nyberg|K. Nyberg]] | title = Perfect nonlinear S-boxes | conference = Eurocrypt '91 |date=April 1991 | pages = 378–386}}</ref>\n\n<ref name=highly>{{cite conference | author = J. Seberry |author2= X. Zhang | title = Highly Nonlinear 0–1 Balanced Boolean Functions Satisfying Strict Avalanche Criterion | conference = [[AUSCRYPT]] '92 |date=December 1992 | pages = 143–155 | citeseerx = 10.1.1.57.4992 }}</ref>\n\n<ref name=cast>{{cite journal | author = C. Adams | title = Constructing Symmetric Ciphers Using the CAST Design Procedure | journal = Designs, Codes and Cryptography | issn = 0925-1022 | volume = 12 | issue = 3 | pages = 283–316 | date = November 1997 | url = http://jya.com/cast.html | accessdate = 20 September 2009 | doi = 10.1023/A:1008229029587 | deadurl = yes | archiveurl = https://web.archive.org/web/20081026081723/http://jya.com/cast.html | archivedate = 26 October 2008}}</ref>\n\n<ref name=haval>{{cite conference | author = [[Yuliang Zheng|Y. Zheng]] |author2=J. Pieprzyk|author3=J. Seberry | title = HAVAL – a one-way hashing algorithm with variable length of output | conference = AUSCRYPT '92 |date=December 1992 | pages = 83–104 | url = http://works.bepress.com/jseberry/192/ | accessdate = 20 June 2015}}</ref>\n\n<ref name=grain>{{Cite journal | author = M. Hell |author2=T. Johansson|author3=A. Maximov|author4=W. Meier | title = A Stream Cipher Proposal: Grain-128 | url = http://www.ecrypt.eu.org/stream/p2ciphers/grain/Grain128_p2.pdf | accessdate = 24 September 2009}}</ref>\n\n<ref name=nyberg2>{{cite conference | author = K. Nyberg | title = Constructions of bent functions and difference sets | conference = Eurocrypt '90 |date=May 1990 | pages = 151–160}}</ref>\n\n<ref name=gbf2>{{cite journal | author = Shashi Kant Pandey|author2=B.K. Dass | title = On Walsh Spectrum of Cryptographic Boolean Function | journal = Defence Science Journal | issn = 0011-748X | volume = 67 | issue = 5 | pages = 536–541 |date=September 2017 | doi=10.14429/dsj.67.10638\t}}</ref>\n\n<ref name=semi>{{cite journal | author = K. Khoo |author2=G. Gong |author3=[[Doug Stinson|D. Stinson]] | title = A new characterization of semi-bent and bent functions on finite fields | journal = Designs, Codes and Cryptography | issn = 0925-1022 | volume = 38 | issue = 2 | pages = 279–295 |date=February 2006 | url = http://www.cacr.math.uwaterloo.ca/~dstinson/papers/dcc-final.ps | format = [[PostScript]] | accessdate = 24 September 2009 | doi = 10.1007/s10623-005-6345-x|citeseerx=10.1.1.10.6303 }}</ref>\n\n<ref name=plat>{{cite conference | author = Y. Zheng |author2= X. Zhang | title = Plateaued Functions | conference = Second International Conference on Information and Communication Security (ICICS '99) |date=November 1999 | pages = 284–300 | url = http://citeseer.ist.psu.edu/old/291018.html | accessdate = 24 September 2009}}</ref>\n\n<ref name=plat>{{cite conference | author = Y. Zheng |author2= X. Zhang | title = Plateaued Functions | conference = Second International Conference on Information and Communication Security (ICICS '99) |date=November 1999 | pages = 284–300 | url = http://citeseer.ist.psu.edu/old/291018.html | accessdate = 24 September 2009}}</ref>\n\n\n\n}}\n\n== Further reading ==\n\n* {{cite conference | author = C. Carlet | title = Two New Classes of Bent Functions | conference = Eurocrypt '93 |date=May 1993 | pages = 77–101}}\n* {{cite journal | author = J. Seberry |author2= X. Zhang | title = Constructions of Bent Functions from Two Known Bent Functions | journal = Australasian Journal of Combinatorics | issn = 1034-4942 | volume = 9 | pages = 21–35 |date=March 1994 | citeseerx = 10.1.1.55.531 }}<!--| accessdate = 17 September 2009-->\n* {{Cite journal | author = T. Neumann | title = Bent Functions |date=May 2006 | citeseerx = 10.1.1.85.8731 }}\n* {{cite book | first1 = Charles J. | last1 = Colbourn | author1-link = Charles Colbourn | first2 = Jeffrey H. | last2 = Dinitz | author2-link = Jeff Dinitz | title = Handbook of Combinatorial Designs | edition = 2nd | publisher = [[CRC Press]] | year = 2006 | isbn = 978-1-58488-506-1 | pages = 337–339}}\n* {{cite book | first1 = T.W. | last1 = Cusick | first2 = P. | last2 = Stanica | title = Cryptographic Boolean Functions and Applications | date = 2009 | publisher = Academic Press | isbn = 9780123748904}}\n\n{{DEFAULTSORT:Bent Function}}\n[[Category:Boolean algebra]]\n[[Category:Combinatorics]]\n[[Category:Symmetric-key cryptography]]\n[[Category:Theory of cryptography]]"
    },
    {
      "title": "Binary decision diagram",
      "url": "https://en.wikipedia.org/wiki/Binary_decision_diagram",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\nIn [[computer science]], a '''binary decision diagram''' ('''BDD''') or '''branching program''' is a [[data structure]] that is used to represent a [[Boolean function]]. On a more abstract level, BDDs can be considered as a [[data compression|compressed]] representation of [[set (mathematics)|sets]] or [[relation (mathematics)|relation]]s. Unlike other compressed representations, operations are performed directly on the compressed representation, i.e. without decompression. Other [[data structure]]s used to represent [[Boolean function]]s include [[negation normal form]] (NNF), [[Zhegalkin polynomial]]s, and [[propositional directed acyclic graph]]s (PDAG).\n\n== Definition ==\n\nA Boolean function can be represented as a [[Rooted graph|rooted]], directed, acyclic [[graph theory|graph]], which consists of several decision nodes and terminal nodes. There are two types of terminal nodes called 0-terminal and 1-terminal. Each decision node <math>N</math> is labeled by  Boolean variable <math>V_N</math> and has two [[child node]]s called low child and high child. The edge from node <math>V_N</math> to a low (or high) child represents an assignment of <math>V_N</math> to 0 (respectively 1).\nSuch a '''BDD''' is called 'ordered' if different variables appear in the same order on all paths from the root. A BDD is said to be 'reduced' if the following two rules have been applied to its graph:\n* Merge any [[Graph isomorphism|isomorphic]] subgraphs.\n* Eliminate any node whose two children are isomorphic.\n\nIn popular usage, the term '''BDD''' almost always refers to '''Reduced Ordered Binary Decision Diagram''' ('''ROBDD''' in the literature, used when the ordering and reduction aspects need to be emphasized). The advantage of an ROBDD is that it is canonical (unique) for a particular function and variable order.<ref name=\"Bryant1986\"/> This property makes it useful in functional equivalence checking and other operations like functional technology mapping.\n\nA path from the root node to the 1-terminal represents a (possibly partial) variable assignment for which the represented Boolean function is true. As the path descends to a low (or high) child from a node, then that node's variable is assigned to 0 (respectively 1).\n\n=== Example ===\nThe left figure below shows a binary [[decision tree|decision ''tree'']] (the reduction rules are not applied), and a [[truth table]], each representing the function f (x1, x2, x3). In the tree on the left, the value of the function can be determined for a given variable assignment by following a path down the graph to a terminal. In the figures below, dotted lines represent edges to a low child, while solid lines represent edges to a high child. Therefore, to find (x1=0, x2=1, x3=1), begin at x1, traverse down the dotted line to x2 (since x1 has an assignment to 0), then down two solid lines (since x2 and x3 each have an assignment to one). This leads to the terminal 1, which is the value of f (x1=0, x2=1, x3=1).\n\nThe binary decision ''tree'' of the left figure can be transformed into a binary decision ''diagram'' by maximally reducing it according to the two reduction rules. The resulting '''BDD''' is shown in the right figure.\n\n{| align=\"center\"\n|-\n| [[File:BDD.png|thumb|546px|Binary decision tree and [[truth table]] for the function <math>f(x_1, x_2, x_3)=\\bar{x}_1 \\bar{x}_2 \\bar{x}_3 + x_1 x_2 + x_2 x_3</math>]]\n| [[File:BDD simple.svg|thumb|189px|BDD for the function f]]\n|}\n\n== History ==\nThe basic idea from which the data structure was created is the [[Shannon expansion]]. A [[switching function]] is split into two sub-functions (cofactors) by assigning one variable (cf. ''if-then-else normal form''). If such a sub-function is considered as a sub-tree, it can be represented by a ''[[binary decision tree]]''. Binary decision diagrams (BDD) were introduced by Lee,<ref name=\"Lee\"/> and further studied and made known by Akers<ref name=\"Akers\"/> and Boute.<ref name=\"Boute_1976\"/>\n\nThe full potential for efficient algorithms based on the data structure was investigated by [[Randal Bryant]] at [[Carnegie Mellon University]]: his key extensions were to use a fixed variable ordering (for canonical representation) and shared sub-graphs (for compression). Applying these two concepts results in an efficient data structure and algorithms for the representation of sets and relations.<ref name=\"Bryant-1986\"/><ref name=\"Bryant-1992\"/> By extending the sharing to several BDDs, i.e. one sub-graph is used by several BDDs, the data structure ''Shared Reduced Ordered Binary Decision Diagram'' is defined.<ref name=\"Brace\"/> The notion of a BDD is now generally used to refer to that particular data structure.\n\nIn his video lecture ''Fun With Binary Decision Diagrams (BDDs)'',<ref name=\"Stanford\"/> [[Donald Knuth]] calls BDDs \"one of the only really fundamental data structures that came out in the last twenty-five years\" and mentions that Bryant's 1986 paper was for some time one of the most-cited papers in computer science.\n\n[[Adnan Darwiche (computer scientist)|Adnan Darwiche]] and his collaborators have shown that BDDs are one of several normal forms for Boolean functions, each induced by a different combination of requirements. Another important normal form identified by Darwiche is [[Decomposable Negation Normal Form]] or DNNF.\n\n== Applications ==\n\nBDDs are extensively used in [[Computer Aided Design|CAD]] software to synthesize circuits ([[logic synthesis]]) and in [[formal verification]]. There are several lesser known applications of BDD, including [[fault tree]] analysis, [[Bayesian probability|Bayesian]] reasoning, product configuration, and [[private information retrieval]].<ref name=\"Jensen\"/><ref name=\"Lipmaa\"/>{{Citation needed|reason=Please provide examples of these applications in the literature.|date=June 2010}}\n\nEvery arbitrary BDD (even if it is not reduced or ordered) can be directly implemented in hardware by replacing each node with a 2 to 1 [[Multiplexer#Digital multiplexers|multiplexer]]; each multiplexer can be directly implemented by a 4-LUT in a [[FPGA]]. It is not so simple to convert from an arbitrary network of logic gates to a BDD{{Citation needed|date=March 2008}} (unlike the [[and-inverter graph]]).\n\n== Variable ordering ==\nThe size of the BDD is determined both by the function being represented and the chosen ordering of the variables. There exist Boolean functions <math>f(x_1,\\ldots, x_{n})</math> for which depending upon the ordering of the variables we would end up getting a graph whose number of nodes would be linear (in&nbsp;''n'') at the best and exponential at the worst case (e.g., a [[ripple carry adder]]). Consider the Boolean function <math>f(x_1,\\ldots, x_{2n}) = x_1x_2 + x_3x_4 + \\cdots + x_{2n-1}x_{2n}.</math>\nUsing the variable ordering <math>x_1 < x_3 < \\cdots < x_{2n-1} < x_2 < x_4 < \\cdots < x_{2n}</math>, the BDD needs 2<sup>''n''+1</sup> nodes to represent the function.  Using the ordering <math>x_1 < x_2 < x_3 < x_4 < \\cdots < x_{2n-1} < x_{2n}</math>, the BDD consists of 2''n''&nbsp;+&nbsp;2 nodes.\n\n{| align=\"center\"\n|-\n| [[File:BDD Variable Ordering Bad.svg|thumb|638px|BDD for the function ''ƒ''(''x''<sub>1</sub>, ..., ''x''<sub>8</sub>) = ''x''<sub>1</sub>''x''<sub>2</sub> + ''x''<sub>3</sub>''x''<sub>4</sub> + ''x''<sub>5</sub>''x''<sub>6</sub> + ''x''<sub>7</sub>''x''<sub>8</sub> using bad variable ordering]]\n| [[File:BDD Variable Ordering Good.svg|thumb|156px|Good variable ordering]]\n|}\n\nIt is of crucial importance to care about variable ordering when applying this data structure in practice.\nThe problem of finding the best variable ordering is [[NP-hard]].<ref name=\"Bollig\"/> For any constant ''c''&nbsp;>&nbsp;1 it is even NP-hard to compute a variable ordering resulting in an OBDD with a size that is at most c times larger than an optimal one.<ref name=\"Sieling\"/> However, there exist efficient heuristics to tackle the problem.<ref name=\"Rice\"/>\n\nThere are functions for which the graph size is always exponential — independent of variable ordering. This holds e.g. for the multiplication function.<ref name=\"Bryant1986\"/> In fact, the function computing the middle bit of the product of two <math>n</math>-bit numbers does not have an OBDD smaller than <math>2^{\\lfloor n/2 \\rfloor} / 61 - 4</math> vertices.<ref name=\"Woelfel_2005\"/> (If the multiplication function had polynomial-size OBDDs, it would show that [[integer factorization]] is in [[P/poly]], which is not known to be true.<ref name=\"Lipton_2009\"/>)\n\nFor [[cellular automata]] with simple behavior, the minimal BDD typically grows linearly on successive steps. For rule 254, for example, it is 8t+2, while for [[rule 90]] it is 4t+2. For cellular automata with more complex behavior, it typically grows roughly exponentially. Thus for [[rule 30]] it is {7, 14, 29, 60, 129} and for [[rule 110]] {7, 15, 27, 52, 88}. The size of the minimal BDD can depend on the order in which variables are specified; thus for example, just reflecting rule 30 to give rule 86 yields {6, 11, 20, 36, 63}.\n\nResearchers have suggested refinements on the BDD data structure giving way to a number of related graphs, such as BMD ([[binary moment diagram]]s), ZDD ([[zero-suppressed decision diagram]]), FDD ([[free binary decision diagram]]s), PDD ([[parity decision diagram]]s), and MTBDDs (multiple terminal BDDs).\n\n== Logical operations on BDDs ==\nMany logical operations on BDDs can be implemented by polynomial-time graph manipulation algorithms:<ref name=\"Andersen_1999\"/>{{rp|20}}\n* [[logical conjunction|conjunction]]\n* [[logical disjunction|disjunction]]\n* [[negation]]\n\nHowever, repeating these operations several times, for example forming the conjunction or disjunction of a set of BDDs, may in the worst case result in an exponentially big BDD. This is because any of the preceding operations for two BDDs may result in a BDD with a size proportional to the product of the BDDs' sizes, and consequently for several BDDs the size may be exponential. Also, since constructing the BDD of a Boolean function solves the NP-complete [[Boolean satisfiability problem]] and the co-NP-complete [[Tautology (logic)|tautology problem]], constructing the BDD can take exponential time in the size of the Boolean formula even when the resulting BDD is small.\n\nComputing existential abstraction over multiple variables of reduced BDDs is NP-complete.<ref name=\"Huth\"/>\n\n== See also ==\n* [[Boolean satisfiability problem]], the canonical [[NP-complete]] [[computational problem]]\n* [[L/poly]], a [[complexity class]] that captures the complexity of problems with polynomially sized BDDs{{Citation needed|date=December 2018}}\n* [[Model checking]]\n* [[Radix tree]]\n* [[NC (complexity)#Barrington's theorem|Barrington's theorem]]\n* [[Hardware acceleration]]\n\n== References ==\n{{reflist|refs=\n<ref name=\"Bryant1986\">Graph-Based Algorithms for Boolean Function Manipulation, Randal E. Bryant, 1986</ref>\n<ref name=\"Lee\">C. Y. Lee. \"Representation of Switching Circuits by Binary-Decision Programs\". Bell System Technical Journal, 38:985–999, 1959.</ref>\n<ref name=\"Akers\">[[Sheldon B. Akers|Sheldon B.<!-- Buckingham --> Akers, Jr.]]. Binary Decision Diagrams, IEEE Transactions on Computers, C-27(6):509–516, June 1978.</ref>\n<ref name=\"Boute_1976\">Raymond T. Boute, \"The Binary Decision Machine as a programmable controller\". [[EUROMICRO]] Newsletter, Vol. 1(2):16–22, January 1976.</ref>\n<ref name=\"Bryant-1986\">Randal E. Bryant. \"[http://www.cs.cmu.edu/~bryant/pubdir/ieeetc86.ps Graph-Based Algorithms for Boolean Function Manipulation]\". IEEE Transactions on Computers, C-35(8):677–691, 1986.</ref>\n<ref name=\"Bryant-1992\">Randal E. Bryant, \"[http://www.cs.cmu.edu/~bryant/pubdir/acmcs92.ps Symbolic Boolean Manipulation with Ordered Binary Decision Diagrams\"], ACM Computing Surveys, Vol. 24, No. 3 (September, 1992), pp. 293–318.\n</ref>\n<ref name=\"Brace\">Karl S. Brace, Richard L. Rudell and Randal E. Bryant. \"[http://portal.acm.org/citation.cfm?id=123222&coll=portal&dl=ACM Efficient Implementation of a BDD Package\"]. In Proceedings of the 27th ACM/IEEE Design Automation Conference (DAC 1990), pages 40–45. IEEE Computer Society Press, 1990.</ref>\n<ref name=\"Stanford\">{{cite web |url=http://scpd.stanford.edu/knuth/index.jsp |title=Stanford Center for Professional Development |website=scpd.stanford.edu |access-date=23 April 2018}}</ref>\n<ref name=\"Jensen\">R. M. Jensen. [http://www.cs.cmu.edu/~runej/data/papers/JSW04.pdf \"CLab: A C+ + library for fast backtrack-free interactive product configuration\"]. Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming, 2004.</ref>\n<ref name=\"Lipmaa\">H. L. Lipmaa. [http://eprint.iacr.org/2009/395.pdf \"First CPIR Protocol with Data-Dependent Computation\"]. ICISC 2009.</ref>\n<ref name=\"Bollig\">Beate Bollig, Ingo Wegener. {{doi-inline|10.1109/12.537122|Improving the Variable Ordering of OBDDs Is NP-Complete}}, IEEE Transactions on Computers, 45(9):993–1002, September 1996.</ref>\n<ref name=\"Sieling\">Detlef Sieling. \"The nonapproximability of OBDD minimization.\" Information and Computation 172, 103–138. 2002.\n</ref>\n<ref name=\"Rice\">{{cite web |author-last=Rice |author-first=Michael |title=A Survey of Static Variable Ordering Heuristics for Efficient BDD/MDD Construction |url=http://alumni.cs.ucr.edu/~skulhari/StaticHeuristics.pdf}}</ref>\n<ref name=\"Woelfel_2005\">Philipp Woelfel. \"[http://www.sciencedirect.com/science/article/pii/S002200000500067X Bounds on the OBDD-size of integer multiplication via universal hashing].\" ''Journal of Computer and System Sciences'' 71, pp. 520-534, 2005.</ref>\n<ref name=\"Lipton_2009\">[[Richard J. Lipton]]. [https://rjlipton.wordpress.com/2009/06/16/bdds-and-factoring/ \"BDD's and Factoring\"]. ''Gödel's Lost Letter and P=NP'', 2009.</ref>\n<ref name=\"Andersen_1999\">{{cite web |author-first=H. R. |author-last=Andersen |url=http://www.cmi.ac.in/~madhavan/courses/verification-2011/andersen-bdd.pdf |title=An Introduction to Binary Decision Diagrams |work=Lecture Notes |date=1999 |publisher=IT University of Copenhagen}}</ref>\n<ref name=\"Huth\">{{Cite book |title=Logic in computer science: modelling and reasoning about systems |author-last1=Huth<!-- 1962 --> |author-first1=Michael |date=2004 |publisher=Cambridge University Press |author-last2=Ryan<!-- 1962- --> |author-first2=Mark |isbn=978-0-52154310-1 |edition=2 |location=Cambridge [U.K.] |pages=380– |oclc=54960031}}</ref>\n}}\n\n== Further reading ==\n* R. Ubar, \"Test Generation for Digital Circuits Using Alternative Graphs (in Russian)\", in Proc. Tallinn Technical University, 1976, No. 409, Tallinn Technical University, Tallinn, Estonia, pp.&nbsp;75–81.\n* D. E. Knuth, \"[[The Art of Computer Programming]] Volume 4, Fascicle 1: Bitwise tricks & techniques; Binary Decision Diagrams\" (Addison–Wesley Professional, March 27, 2009) viii+260pp, {{ISBN|0-321-58050-8}}. [http://www-cs-faculty.stanford.edu/~knuth/fasc1b.ps.gz Draft of Fascicle 1b] available for download.\n* Ch. Meinel, T. Theobald, \"[http://www.hpi.uni-potsdam.de/fileadmin/hpi/FG_ITS/books/OBDD-Book.pdf Algorithms and Data Structures in VLSI-Design: OBDD – Foundations and Applications\"], Springer-Verlag, Berlin, Heidelberg, New York, 1998. Complete textbook available for download.\n* {{cite book |author-first1=Rüdiger |author-last1=Ebendt |author-first2=Görschwin |author-last2=Fey |author-first3=Rolf |author-last3=Drechsler |title=Advanced BDD optimization |date=2005 |publisher=Springer |isbn=978-0-387-25453-1}}\n* {{cite book |author-first1=Bernd |author-last1=Becker |author-first2=Rolf |author-last2=Drechsler |title=Binary Decision Diagrams: Theory and Implementation |date=1998 |publisher=Springer |isbn=978-1-4419-5047-5}}\n\n== External links ==\n{{Commons category|Binary decision diagrams}}\n* [https://web.archive.org/web/20111101143950/http://myvideos.stanford.edu/player/slplayer.aspx?coll=ea60314a-53b3-4be2-8552-dcf190ca0c0b&co=18bcd3a8-965a-4a63-a516-a1ad74af1119&o=true Fun With Binary Decision Diagrams (BDDs)], lecture by [[Donald Knuth]]\n* [https://github.com/johnyf/tool_lists/blob/master/bdd.md List of BDD software libraries] for several programming languages.\n\n{{Data structures}}\n\n{{DEFAULTSORT:Binary Decision Diagram}}\n[[Category:Diagrams]]\n[[Category:Graph data structures]]\n[[Category:Model checking]]\n[[Category:Articles with example code]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "George Boole",
      "url": "https://en.wikipedia.org/wiki/George_Boole",
      "text": "{{redirect|Boole}}\n{{distinguish|George Boolos}}\n{{Use British English|date=September 2015}}\n{{Use dmy dates|date=May 2013}}\n{{Infobox philosopher\n|region           = [[Western philosophy]]\n|era              = [[19th-century philosophy]]\n|image            = George Boole color.jpg\n| image_size = 200\n|caption          = Boole, {{circa|1860}}\n|name             = George Boole\n|birth_date       = {{birth date|df=yes|1815|11|02}}\n|birth_place      = [[Lincoln, England|Lincoln]], [[Lincolnshire]], England\n|death_date       = {{death date and age|1864|12|08|1815|11|02|df=y}}\n|death_place      = [[Ballintemple, Cork|Ballintemple]], [[Cork (city)|Cork]], Ireland\n|nationality      = \n|spouse           = [[Mary Everest Boole]]\n|education        = Bainbridge's Commercial Academy<ref name=MacTutor/>\n|institutions     = [[Lincoln, England|Lincoln]] [[Mechanics' Institutes|Mechanics' Institute]]<ref name=Hill149/><br>[[Queen's College, Cork]]\n|school_tradition = Mathematical foundations of [[computing]]\n|main_interests   = Mathematics, [[Logic]], [[Philosophy of mathematics]]\n|religion         = [[Unitarianism|Unitarian]]\n|influences       = [[Aristotle]], [[Baruch Spinoza|Spinoza]], [[Isaac Newton|Newton]]\n|influenced       = Modern computer scientists, [[William Stanley Jevons|Jevons]], [[Augustus De Morgan|De Morgan]], [[John Maynard Keynes|Keynes]], [[Charles Sanders Peirce|Peirce]], [[William Ernest Johnson|Johnson]], [[Claude Shannon|Shannon]], [[Victor Shestakov|Shestakov]]\n|notable_ideas    = [[Boolean algebra]]\n}}\n\n'''George Boole''' ({{IPAc-en|b|uː|l}}; 2 November 1815&nbsp;– 8 December 1864) was a largely self-taught English mathematician, philosopher and logician, most of whose short career was spent as the first professor of mathematics at [[Queen's College, Cork]] in Ireland.  He worked in the fields of [[Differential equation|differential equations]] and [[algebraic logic]], and is best known as the author of ''[[The Laws of Thought]]'' (1854) which contains [[Boolean algebra]]. Boolean logic is credited with laying the foundations for the [[information age]].<ref name=\"Commemoration\"/> Boole maintained that:\n{{Quote|text= No general method for the solution of questions in the theory of probabilities can be established which does not explicitly recognise, not only the special numerical bases of the science, but also those universal laws of thought which are the basis of all reasoning, and which, whatever they may be as to their essence, are at least mathematical as to their form.<ref name=\"Boole Studies in Logic p 273\">{{cite book\n| last = Boole | first = George\n| title = Studies in Logic and Probability\n| publisher = Dover Publications | location = Mineola, NY\n| date = 2012 | edition = Reprint | isbn=978-0-486-48826-4\n| orig-year= Originally published by Watts & Co., London, in 1952\n| url = https://books.google.com/books?id=pr35F8N9eaoC&lpg=PP1&pg=PA273 <!-- URL for p. 273 -->\n| page = 273\n| editor-last= Rhees |editor-first= Rush |editor-link= Rush Rhees\n| access-date=27 October 2015}}</ref>}}\n\n==Early life==\n[[File:3 Pottergate - geograph.org.uk - 657140.jpg|thumb|Boole's House and School at 3 Pottergate in [[Lincoln, England|Lincoln]]]]\nBoole was born in [[Lincoln, England|Lincoln]], [[Lincolnshire]], England, the son of John Boole senior (1779–1848), a shoemaker<ref>{{cite web|url=http://www.lincolnboolefoundation.org/john-boole/ |title=John Boole |publisher=Lincoln Boole Foundation |date= |accessdate=6 November 2015}}</ref> and Mary Ann Joyce.<ref>{{Cite EB1911|wstitle=Boole, George}}</ref> He had a primary school education, and received lessons from his father, but due to a serious decline in business, he had little further formal and academic teaching.<ref>{{Cite book|url=https://www.worldcat.org/oclc/41497065|title=Math and mathematicians : the history of math discoveries around the world|last=C.|first=Bruno, Leonard|date=2003|origyear=1999|publisher=U X L|others=Baker, Lawrence W.|year=|isbn=0787638137|location=Detroit, Mich.|pages=49|oclc=41497065}}</ref> William Brooke, a bookseller in Lincoln, may have helped him with Latin, which he may also have learned at the school of Thomas Bainbridge. He was self-taught in modern languages.<ref name=Hill149>Hill, p. 149; [https://books.google.com/books?id=-A89AAAAIAAJ&pg=PA149 Google Books].</ref> In fact, when a local newspaper printed his translation of a Latin poem, a scholar accused him of plagiarism under the pretence that he was not capable of such achievements.<ref>{{Cite book|url=https://www.worldcat.org/oclc/41497065|title=Math and mathematicians : the history of math discoveries around the world|last=C.|first=Bruno, Leonard|date=2003|origyear=1999|publisher=U X L|others=Baker, Lawrence W.|year=|isbn=0787638137|location=Detroit, Mich.|pages=49–50|oclc=41497065}}</ref> At age 16, Boole became the breadwinner for his parents and three younger siblings, taking up a junior teaching position in [[Doncaster]] at Heigham's School.<ref name=Rhees1954>[[Rush Rhees|Rhees, Rush]]. (1954) \"George Boole as Student and Teacher. By Some of His Friends and Pupils\", ''Proceedings of the Royal Irish Academy. Section A: Mathematical and Physical Sciences''. Vol. 57. Royal Irish Academy</ref> He taught briefly in [[Liverpool]].<ref name=MacTutor>{{MacTutor Biography|id=Boole}}</ref> \n\n[[File:Greyfriars, Lincoln - geograph.org.uk - 106215.jpg|thumb|left|Greyfriars, Lincoln, which housed the Mechanic's Institute]]\nBoole participated in the [[Mechanics Institute]], in the [[Greyfriars, Lincoln]], which was founded in 1833.<ref name=Hill149/><ref>[http://www.freewebs.com/sochistastro/lincolnshire.htm Society for the History of Astronomy, ''Lincolnshire''.]</ref> [[Edward Bromhead]], who knew John Boole through the institution, helped George Boole with mathematics books<ref>{{ODNBweb|id=37224|title=Bromhead, Sir Edward Thomas French|first=A. W. F.|last=Edwards}}</ref> and he was given the [[calculus]] text of [[Sylvestre François Lacroix]] by the Rev. George Stevens Dickson of [[St Swithin's Church, Lincoln|St Swithin's, Lincoln]].<ref name=SED>{{cite SEP |url-id=boole |title=George Boole |last=Burris |first=Stanley}}</ref> Without a teacher, it took him many years to master calculus.<ref name=MacTutor/>\n\nAt age 19, Boole successfully established his own school in Lincoln.<ref>[https://georgeboole200.ucc.ie/boole/life/lincoln/selfeducation/ George Boole: Self-Education & Early Career] University College Cork</ref>  He continued making his living by running schools until he was in his thirties.<ref>{{cite book|last=Wolfram|first=Stephen|title=Idea Makers: Personal Perspectives on the Lives & Ideas of Some Notable People|publisher=Wolfram Media, Inc.|year=2016|page=36|isbn=978-1-5795-5-003-5}}</ref> Four years later he took over Hall's Academy in [[Waddington, Lincolnshire|Waddington]], outside Lincoln, following the death of Robert Hall. In 1840 he moved back to Lincoln, where he ran a boarding school.<ref name=MacTutor/> Boole immediately became involved in the Lincoln Topographical Society, serving as a member of the committee, and presenting a paper entitled, ''On the origin, progress, and tendencies of Polytheism, especially amongst the ancient Egyptians and Persians, and in modern India.''<ref>A Selection of Papers relative to the County of Lincoln, read before the Lincolnshire Topographical Society, 1841–1842. Printed by W. and B. Brooke, High-Street,  Lincoln, 1843.</ref> on 30 November 1841.\n\nBoole became a prominent local figure, an admirer of [[John Kaye (bishop)|John Kaye]], the bishop.<ref>Hill, p. 172 note 2; [https://books.google.com/books?id=-A89AAAAIAAJ&pg=PA172 Google Books].</ref> He took part in the local [[Early Closing Association|campaign for early closing]].<ref name=Hill149/> With [[Edmund Larken]] and others he set up a [[building society]] in 1847.<ref>Hill, p. 130 note 1; [https://books.google.com/books?id=-A89AAAAIAAJ&pg=PA130 Google Books].</ref> He associated also with the [[Chartism|Chartist]] [[Thomas Cooper (poet)|Thomas Cooper]], whose wife was a relation.<ref>Hill, p. 148; [https://books.google.com/books?id=-A89AAAAIAAJ&pg=PA148 Google Books].</ref>\n[[File:BoolePlacque.jpg|thumb|240px|Plaque from the house in Lincoln]]\n\nFrom 1838 onwards Boole was making contacts with sympathetic British academic mathematicians and reading more widely. He studied [[algebra]] in the form of symbolic methods, as far as these were understood at the time, and began to publish research papers.<ref name=MacTutor/> After receiving positive feedback on his publications, he considered attending the [[University of Cambridge]], but decided against attending when told he would have to start with the standard undergraduate courses and discontinue his own research.<ref>{{cite book|last=Wolfram|first=Stephen|title=Idea Makers: Personal Perspectives on the Lives & Ideas of Some Notable People|publisher=Wolfram Media, Inc.|year=2016|page=37|isbn=978-1-5795-5-003-5}}</ref>\n\n==Professor at Cork==\n[[File:Boole House Cork 2012.jpg|thumb|The house at 5 Grenville Place in [[Cork (city)|Cork]], in which Boole lived between 1849 and 1855, and where he wrote ''[[The Laws of Thought]]'' ''(Picture taken during renovation.)'']]\nBoole's status as mathematician was recognised by his appointment in 1849 as the first professor of mathematics at [[Queen's College, Cork]] (now [[University College Cork]] (UCC)) in Ireland. He met his future wife, [[Mary_Everest_Boole|Mary Everest]], there in 1850 while she was visiting her uncle John Ryall who was Professor of Greek. They married some years later in 1855.<ref>Ronald Calinger, ''Vita mathematica: historical research and integration with teaching'' (1996), p. 292; [https://books.google.com/books?id=D21wKHoYGg0C&pg=PA292 Google Books].</ref> He maintained his ties with Lincoln, working there with E. R. Larken in a campaign to reduce prostitution.<ref>Hill, p. 138 note 4; [https://books.google.com/books?id=-A89AAAAIAAJ&pg=PA138 Google Books].</ref>\n\n==Honours and awards==\nIn 1844 Boole's paper ''On a General Method for Analysis'' won the first gold prize for  mathematics awarded by the [[Royal Society]].<ref>{{cite book | title=The Life and Work of George Boole: A Prelude to the Digital Age | last=MacHale | first=Desmond | page=97}}</ref> He was awarded the [[Keith Medal]] by the [[Royal Society of Edinburgh]] in 1855<ref>{{cite web|url = http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=8735503|title= Keith Awards 1827–1890|publisher=Cambridge Journals Online|accessdate = 29 November 2014}}</ref> and was elected a [[List of Fellows of the Royal Society elected in 1857|Fellow of the Royal Society (FRS) in 1857]].<ref name=SED/> He received [[honorary degrees]] of [[LL.D.]] from the [[University of Dublin]] and the [[University of Oxford]].<ref>[[Ivor Grattan-Guinness]], [[Gérard Bornet]], ''George Boole: Selected manuscripts on logic and its philosophy'' (1997), p. xiv; [https://books.google.com/books?id=pzg7UFsIVJIC&pg=PR14 Google Books].</ref>\n[[File:Grave of George Boole in Ireland.jpg|thumb|Boole's gravestone in [[Blackrock, Cork|Blackrock]], Cork, Ireland]]\n[[File:BooleWindow(bottom third).jpg|thumb|Detail of stained glass window in [[Lincoln Cathedral]] dedicated to Boole]]\n[[File:BoolePlaque2.jpg|thumb|Plaque beneath Boole's window in Lincoln Cathedral]]\n\n==Works==\nBoole's first published paper was ''Researches in the theory of analytical transformations, with a special application to the reduction of the general equation of the second order'', printed in the ''[[Cambridge Mathematical Journal]]'' in February 1840 (Volume 2, № 8, pp.&nbsp;64–73), and it led to a friendship between Boole and [[Duncan Farquharson Gregory]], the editor of the journal. His works are in about 50 articles and a few separate publications.<ref>A list of Boole's memoirs and papers is in the ''Catalogue of Scientific Memoirs'' published by the [[Royal Society]], and in the supplementary volume on differential equations, edited by [[Isaac Todhunter]]. To the ''Cambridge Mathematical Journal'' and its successor, the ''[[Cambridge and Dublin Mathematical Journal]]'', Boole contributed 22 articles in all. In the third and fourth series of the ''[[Philosophical Magazine]]'' are found 16 papers. The Royal Society printed six memoirs in the ''[[Philosophical Transactions]]'', and a few other memoirs are to be found in the ''Transactions'' of the [[Royal Society of Edinburgh]] and of the [[Royal Irish Academy]], in the ''Bulletin de l'Académie de St-Pétersbourg'' for 1862 (under the name G. Boldt, vol. iv. pp.&nbsp;198–215), and in ''[[Crelle's Journal]]''. Also included is a paper on the mathematical basis of logic, published in the ''[[Mechanic's Magazine]]'' in 1848.</ref><ref>Hill, p. 138 note 4; [https://books.google.com/books?id=-A89AAAAIAAJ&pg=PA138 Google Books].</ref>\n\nIn 1841 Boole published an influential paper in early [[invariant theory]].<ref name=SED/> He received a medal from the [[Royal Society]] for his memoir of 1844, ''On A General Method of Analysis''. It was a contribution to the theory of [[linear differential equation]]s, moving from the case of constant coefficients on which he had already published, to variable coefficients.<ref>[[Andrei Nikolaevich Kolmogorov]], [[Adolf Pavlovich Yushkevich]] (editors), ''Mathematics of the 19th Century: function theory according to Chebyshev, ordinary differential equations, calculus of variations, theory of finite differences'' (1998), pp. 130–2; [https://books.google.com/books?id=Mw6JMdZQO-wC&pg=PA130 Google Books].</ref> The innovation in operational methods is to admit that operations may not [[commutative law|commute]].<ref>[[Jeremy Gray]], [[Karen Hunger Parshall]], ''Episodes in the History of Modern Algebra (1800–1950)'' (2007), p. 66; [https://books.google.com/books?id=zMSl6QLlJZsC&pg=PA66 Google Books].</ref> In 1847 Boole published ''The Mathematical Analysis of Logic'', the first of his works on symbolic logic.<ref>George Boole, [https://books.google.com/books?id=zv4YAQAAIAAJ&pg=PP9#v=onepage&q&f=false ''The Mathematical Analysis of Logic, Being an Essay towards a Calculus of Deductive Reasoning''] (London, England: Macmillan, Barclay, & Macmillan, 1847).</ref>\n\n===Differential equations===\nBoole completed two systematic treatises on mathematical subjects during his lifetime. The ''Treatise on Differential Equations''<ref>George Boole, ''A treatise on differential equations'' (1859), [https://archive.org/details/atreatiseondiff06boolgoog Internet Archive].</ref> appeared in 1859, and was followed, the next year, by a ''Treatise on the [[Calculus]] of Finite Differences'',<ref>George Boole, ''A treatise on the calculus of finite differences'' (1860), [https://archive.org/details/treatiseoncalcul00geor  Internet Archive].</ref> a sequel to the former work.\n\n===Analysis===\nIn 1857, Boole published the treatise ''On the Comparison of Transcendents, with Certain Applications to the Theory of Definite Integrals'',<ref>{{cite journal |title=On the Comparison of Transcendents, with Certain Applications to the Theory of Definite Integrals |first=George |last=Boole |journal=Philosophical Transactions of the Royal Society of London |volume=147 |year=1857 |pages=745–803 |jstor=108643 |doi=10.1098/rstl.1857.0037}}</ref> in which he studied the sum of [[residue (complex analysis)|residues]] of a [[rational function]]. Among other results, he proved what is now called Boole's identity: \n\n:<math>\\mathrm{mes} \\left\\{ x \\in \\mathbb{R} \\, \\mid \\, \\Re \\frac{1}{\\pi} \\sum \\frac{a_k}{x - b_k} \\geq t \\right\\} = \\frac{\\sum a_k}{\\pi t} </math>\n\nfor any real numbers ''a''<sub>''k''</sub>&nbsp;>&nbsp;0, ''b''<sub>''k''</sub>, and ''t''&nbsp;>&nbsp;0.<ref name=cmr>{{cite book|mr=2129737|last1=Cima|first1=Joseph A.|last2=Matheson|first2=Alec|last3=Ross|first3=William T.|chapter=The Cauchy transform|title=Quadrature domains and their applications|pages=79–111|series=Oper. Theory Adv. Appl.|volume=156|publisher=Birkhäuser|location=Basel|year=2005}}</ref> Generalisations of this identity play an important role in the theory of the [[Hilbert transform]].<ref name=cmr/>\n\n===Symbolic logic===\n{{main article|Boolean algebra}}\nIn 1847 Boole published the pamphlet ''Mathematical Analysis of Logic''. He later regarded it as a flawed exposition of his logical system, and wanted ''[[The Laws of Thought|An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities]]'' to be seen as the mature statement of his views. Contrary to widespread belief, Boole never intended to criticise or disagree with the main principles of Aristotle's logic. Rather he intended to systematise it, to provide it with a foundation, and to extend its range of applicability.<ref>[[John Corcoran (logician)|John Corcoran]], Aristotle's Prior Analytics and Boole's Laws of Thought, History and Philosophy of Logic, vol.  24 (2003), pp. 261–288.</ref> Boole's initial involvement in logic was prompted by a current debate on [[quantification (logic)|quantification]], between [[Sir William Hamilton, 9th Baronet|Sir William Hamilton]] who supported the theory of \"quantification of the predicate\", and Boole's supporter [[Augustus De Morgan]] who advanced a version of [[De Morgan duality]], as it is now called. Boole's approach was ultimately much further reaching than either sides' in the controversy.<ref name=ODNB>{{ODNBweb|id=2868|title=Boole, George|first=I.|last=Grattan-Guinness}}</ref> It founded what was first known as the \"algebra of logic\" tradition.<ref name=Marc>Witold Marciszewski (editor), ''Dictionary of Logic as Applied in the Study of Language'' (1981), pp. 194–5.</ref>\n\nAmong his many innovations is his principle of [[wholistic reference]], which was later, and probably independently, adopted by [[Gottlob Frege]] and by logicians who subscribe to standard first-order logic. A 2003 article<ref>[[John Corcoran (logician)|Corcoran, John]] (2003). \"Aristotle's Prior Analytics and Boole's Laws of Thought\". ''History and Philosophy of Logic'', '''24''': 261–288. Reviewed by Risto Vilkko. ''Bulletin of Symbolic Logic'', '''11'''(2005) 89–91. Also by Marcel Guillaume, ''Mathematical Reviews'' 2033867 (2004m:03006).</ref> provides a systematic comparison and critical evaluation of [[Aristotelian logic]] and [[Boolean logic]]; it also reveals the centrality of [[wholistic reference]] in Boole's [[philosophy of logic]].\n\n====1854 definition of universe of discourse====\n<blockquote>In every discourse, whether of the mind conversing with its own thoughts, or of the individual in his intercourse with others, there is an assumed or expressed limit within which the subjects of its operation are confined. The most unfettered discourse is that in which the words we use are understood in the widest possible application, and for them the limits of discourse are co-extensive with those of the universe itself. But more usually we confine ourselves to a less spacious field. Sometimes, in discoursing of men we imply (without expressing the limitation) that it is of men only under certain circumstances and conditions that we speak, as of civilised men, or of men in the vigour of life, or of men under some other condition or relation. Now, whatever may be the extent of the field within which all the objects of our discourse are found, that field may properly be termed the [[universe of discourse]]. Furthermore, this universe of discourse is in the strictest sense the ultimate subject of the discourse.<ref>George Boole. 1854/2003. ''The Laws of Thought'', facsimile of 1854 edition, with an introduction by [[John Corcoran (logician)|John Corcoran]]. Buffalo: Prometheus Books (2003). Reviewed by James van Evra in Philosophy in Review.24 (2004) 167–169.</ref></blockquote>\n\n====Treatment of addition in logic====\nBoole conceived of \"elective symbols\" of his kind as an [[algebraic structure]]. But this general concept was not available to him: he did not have the segregation standard in [[abstract algebra]] of postulated (axiomatic) properties of operations, and deduced properties.<ref name=KY>[[Andrei Nikolaevich Kolmogorov]], [[Adolf Pavlovich Yushkevich]], ''Mathematics of the 19th Century: mathematical logic, algebra, number theory, probability theory'' (2001), pp. 15 (note 15)–16; [https://books.google.com/books?id=X3u5hJCkobYC&pg=PA15 Google Books].</ref> His work was a beginning to the [[algebra of sets]], again not a concept available to Boole as a familiar model. His pioneering efforts encountered specific difficulties, and the treatment of addition was an obvious difficulty in the early days.\n\nBoole replaced the operation of multiplication by the word \"and\" and addition by the word \"or\". But in Boole's original system, + was a [[partial operation]]: in the language of [[set theory]] it would correspond only to [[disjoint union]] of subsets. Later authors changed the interpretation, commonly reading it as [[exclusive or]], or in set theory terms [[symmetric difference]]; this step means that addition is always defined.<ref name=Marc/><ref>{{cite SEP |url-id=algebra-logic-tradition |title=The Algebra of Logic Tradition |last=Burris |first=Stanley}}</ref>\n\nIn fact there is the other possibility, that + should be read as [[disjunction]].<ref name=KY/> This other possibility extends from the disjoint union case, where exclusive or and non-exclusive or both give the same answer. Handling this ambiguity was an early problem of the theory, reflecting the modern use of both [[Boolean ring]]s and Boolean algebras (which are simply different aspects of one type of structure). Boole and [[William Stanley Jevons|Jevons]] struggled over just this issue in 1863, in the form of the correct evaluation of ''x'' + ''x''. Jevons argued for the result ''x'', which is correct for + as disjunction. Boole kept the result as something undefined. He argued against the result 0, which is correct for exclusive or, because he saw the equation ''x'' + ''x'' = 0 as implying ''x'' = 0, a false analogy with ordinary algebra.<ref name=SED/>\n\n===Probability theory===\nThe second part of the ''Laws of Thought'' contained a corresponding attempt to discover a general method in probabilities. Here the goal was algorithmic: from the given probabilities of any system of events, to determine the consequent probability of any other event logically connected with those events.<ref>{{cite book |last=Boole |first=George |title=An Investigation of the Laws of Thought |publisher=Walton & Maberly |year=1854 |location=London |pages=265–275 |url=https://archive.org/stream/investigationofl00boolrich#page/264/mode/2up}}</ref>\n\n==Death==\nIn late November 1864, Boole walked, in heavy rain, from his home at Lichfield Cottage in [[Ballintemple, Cork|Ballintemple]]<ref>{{cite web|url=http://www.buildingsofireland.ie/niah/search.jsp?type=record&county=CC&regno=20868114|title=Dublin City Quick Search: Buildings of Ireland: National Inventory of Architectural Heritage|publisher=}}</ref> to the university, a distance of three miles, and lectured wearing his wet clothes.<ref name=\"irishexaminer.com\">{{cite news|url=http://www.irishexaminer.com/property/features/have-a-look-inside-the-home-of-ucc-maths-professor-george-boole-336830.html|title=Have a look inside the home of UCC maths professor George Boole|last=Barker|first=Tommy|date=13 June 2015|newspaper=Irish Examiner|accessdate=6 November 2015}}</ref> He soon became ill, developing pneumonia.  As his wife believed that remedies should resemble their cause, she wrapped him in wet blankets – the wet having brought on his illness.<ref name=\"irishexaminer.com\"/><ref>{{Cite book|url=https://www.worldcat.org/oclc/41497065|title=Math and mathematicians : the history of math discoveries around the world|last=C.|first=Bruno, Leonard|date=2003|origyear=1999|publisher=U X L|others=Baker, Lawrence W.|year=|isbn=0787638137|location=Detroit, Mich.|pages=52|oclc=41497065}}</ref><ref>[https://plato.stanford.edu/entries/boole/ Stanford Encyclopedia of Philosophy]</ref>  Boole's condition worsened and on 8 December 1864,<ref>{{cite encyclopedia|title=George Boole|encyclopedia= Encyclopædia Britannica|date=30 January 2017|publisher=Encyclopædia Britannica, inc.|url=https://www.britannica.com/biography/George-Boole|accessdate=7 December 2017}}</ref> he died of fever-induced [[pleural effusion]]. \n\nHe was buried in the [[Church of Ireland]] cemetery of St Michael's, Church Road, [[Blackrock, Cork|Blackrock]] (a suburb of [[Cork (city)|Cork]]). There is a commemorative plaque inside the adjoining church.<ref>{{cite web|url=http://georgeboole.com/boole/life/ucc/death/|title=Death-His Life-- George Boole 200|publisher=}}</ref>\n\n==Legacy==\n[[File:Bust of George Boole at University College Cork - 133760 (38218465931) (2).jpg|thumb|Bust of Boole at [[University College Cork]]]]\nBoole is the namesake of the branch of [[algebra]] known as [[Boolean algebra]], as well as the namesake of the [[lunar craters|lunar crater]] [[Boole (crater)|Boole]]. The keyword ''Bool'' represents a [[Boolean datatype]] in many programming languages, though [[Pascal (programming language)|Pascal]] and [[Java (programming language)|Java]], among others, both use the full name ''Boolean''.<ref>P. J. Brown, ''Pascal from Basic'', Addison-Wesley, 1982. {{isbn|0-201-13789-5}}, page 72</ref> The library, underground lecture theatre complex and the Boole Centre for Research in Informatics<ref>{{cite web|url=http://www.bcri.ucc.ie|title=Boole Centre for Research in Informatics|publisher=}}</ref> at [[University College Cork]] are named in his honour. A road called ''Boole Heights'' in Bracknell, Berkshire is named after him.\n\n===19th-century development===\nBoole's work was extended and refined by a number of writers, beginning with [[William Stanley Jevons]]. [[Augustus De Morgan]] had worked on the [[logic of relations]], and [[Charles Sanders Peirce]] integrated his work with Boole's during the 1870s.<ref name=GGB>[[Ivor Grattan-Guinness]], [[Gérard Bornet]], ''George Boole: Selected manuscripts on logic and its philosophy'' (1997), p. xlvi; [https://books.google.com/books?id=pzg7UFsIVJIC&pg=PR46 Google Books].</ref> Other significant figures were [[Platon Sergeevich Poretskii]], and [[William Ernest Johnson]]. The conception of a Boolean algebra structure on equivalent statements of a [[propositional calculus]] is credited to [[Hugh MacColl]] (1877), in work surveyed 15 years later by Johnson.<ref name=GGB/> Surveys of these developments were published by [[Ernst Schröder]], [[Louis Couturat]], and [[Clarence Irving Lewis]].\n\n===20th-century development===\n[[File:Hasse2Free.png|thumb|In modern notation, the [[free Boolean algebra]] on basic propositions ''p'' and ''q'' arranged in a [[Hasse diagram]]. The Boolean combinations make up 16 different propositions, and the lines show which are logically related.]]\n\nIn 1921 the economist [[John Maynard Keynes]] published a book on probability theory, ''A Treatise of Probability''. Keynes believed that Boole had made a fundamental error in his definition of independence which vitiated much of his analysis.<ref>Chapter XVI, p. 167, section 6 of ''A treatise on probability'', volume 4: \"The central error in his system of probability arises out of his giving two inconsistent definitions of 'independence' (2) He first wins the reader's acquiescence by giving a perfectly correct definition: \"Two events are said to be independent when the probability of either of them is unaffected by our ''expectation'' of the occurrence or failure of the other.\" (3) But a moment later he interprets the term in quite a different sense; for, according to Boole's second definition, we must regard the events as independent unless we are told either that they ''must'' concur or that they ''cannot'' concur. That is to say, they are independent unless we know for certain that there is, in fact, an invariable connection between them. \"The simple events, ''x'', ''y'', ''z'', will be said to be ''conditioned'' when they are not free to occur in every possible combination; in other words, when some compound event depending upon them is precluded from occurring. ... Simple unconditioned events are by definition independent.\" (1) In fact as long as ''xz'' is ''possible'', ''x'' and ''z'' are independent. This is plainly inconsistent with Boole's first definition, with which he makes no attempt to reconcile it. The consequences of his employing the term independence in a double sense are far-reaching. For he uses a method of reduction which is only valid when the arguments to which it is applied are independent in the first sense, and assumes that it is valid if they are independent in second sense. While his theorems are true if all propositions or events involved are independent in the first sense, they are not true, as he supposes them to be, if the events are independent only in the second sense.\"</ref> In his book ''The Last Challenge Problem'', David Miller provides a general method in accord with Boole's system and attempts to solve the problems recognised earlier by Keynes and others. Theodore Hailperin showed much earlier that Boole had used the correct mathematical definition of independence in his worked out problems.<ref name=Miller>{{cite web|url=http://zeteticgleanings.com/boole.html|title=ZETETIC GLEANINGS|publisher=}}</ref>\n\nBoole's work and that of later logicians initially appeared to have no engineering uses. [[Claude Shannon]] attended a philosophy class at the [[University of Michigan]] which introduced him to Boole's studies. Shannon recognised that Boole's work could form the basis of mechanisms and processes in the real world and that it was therefore highly relevant. In 1937 Shannon went on to write a master's thesis, at the [[Massachusetts Institute of Technology]], in which he showed how Boolean algebra could optimise the design of systems of electromechanical [[relay]]s then used in telephone routing switches. He also proved that circuits with relays could solve Boolean algebra problems. Employing the properties of electrical switches to process logic is the basic concept that underlies all modern electronic [[digital computer]]s. [[Victor Shestakov]] at Moscow State University (1907–1987) proposed a theory of electric switches based on Boolean logic even earlier than [[Claude Shannon]] in 1935 on the testimony of Soviet logicians and mathematicians [[Sofya Yanovskaya]], Gaaze-Rapoport, [[Roland Dobrushin]], Lupanov, Medvedev and Uspensky, though they presented their academic theses in the same year, 1938.{{Clarify|date=June 2009}} But the first publication of Shestakov's result took place only in 1941 (in Russian). Hence, Boolean algebra became the foundation of practical [[digital circuit]] design; and Boole, via Shannon and Shestakov, provided the theoretical grounding for the [[Information Age]].<ref>\"That dissertation has since been hailed as one of the most significant master's theses of the 20th century. To all intents and purposes, its use of binary code and Boolean algebra paved the way for the digital circuitry that is crucial to the operation of modern computers and telecommunications equipment.\"{{cite news |url=https://www.theguardian.com/science/2001/mar/08/obituaries.news |newspaper=The Guardian |location=United Kingdom |date=8 March 2001 |title=Claude Shannon |first=Andrew |last=Emerson}}</ref>{{Clear}}\n\n===21st-century celebration===\n{{Quote box|width=30%|align=right|quote=\"Boole's legacy surrounds us everywhere, in the computers, information storage and retrieval, electronic circuits and controls that support life, learning and communications in the 21st century. His pivotal advances in mathematics, logic and probability provided the essential groundwork for modern mathematics, microelectronic engineering and computer science.\"|source=—University College Cork.<ref name=\"Commemoration\">{{cite news|title=Who is George Boole: the mathematician behind the Google doodle|url=http://www.smh.com.au/technology/technology-news/who-is-george-boole-the-mathematician-behind-the-google-doodle-20151102-gkofyg.html#ixzz3qIcv6ii2|newspaper=Sydney Morning Herald|date=2 November 2015}}</ref>}}\n2015 saw the 200th anniversary of George Boole's birth. To mark the bicentenary year, [[University College Cork]] joined admirers of Boole around the world to celebrate his life and legacy.\n\nUCC's George Boole 200<ref>{{cite web|url=http://georgeboole.com|title=George Boole 200 –  George Boole Bicentenary Celebrations|publisher=}}</ref> project, featured events, student outreach activities and academic conferences on Boole's legacy in the digital age, including a new edition of [[Desmond MacHale]]'s 1985 biography '' The Life and Work of George Boole: A Prelude to the Digital Age'',<ref>[http://www.corkuniversitypress.com/SearchResults.asp?Search=boole&Submit= Cork University Press]</ref> 2014).\n\nThe search engine [[Google]] marked the 200th anniversary of his birth on 2 November 2015 with an algebraic reimaging of its [[Google Doodle]].<ref name=\"Commemoration\"/>\n[[File:George Boole House 2017.jpg|thumb|5, Grenville Place in 2017 following restoration by UCC]]\nLitchfield Cottage in Ballintemple, Cork, where Boole lived for the last two years of his life, bears a memorial plaque. His former residence, in Grenville Place, is being restored through a collaboration between UCC and Cork City Council, as the George Boole House of Innovation, after the city council acquired the premises under the Derelict Sites Act.<ref>{{cite web|url=http://www.irishtimes.com/life-and-style/homes-and-property/boolean-logic-meets-victorian-gothic-in-leafy-cork-suburb-1.2277521|title=Boolean logic meets Victorian gothic in leafy Cork suburb|publisher=}}</ref>\n\n==Views==\nBoole's views were given in four published addresses: ''The Genius of Sir Isaac Newton''; ''The Right Use of Leisure''; ''The Claims of Science''; and ''The Social Aspect of Intellectual Culture''.<ref>1902 ''Britannica'' article by Jevons; [http://www.1902encyclopedia.com/B/BOO/george-boole.html online text.]</ref> The first of these was from 1835, when [[Charles Anderson-Pelham, 1st Earl of Yarborough]] gave a bust of Newton to the Mechanics' Institute in Lincoln.<ref>James Gasser, ''A Boole Anthology: recent and classical studies in the logic of George Boole'' (2000), p. 5; [https://books.google.com/books?id=A2Q5Yghl000C&pg=PA5 Google Books].</ref> The second justified and celebrated in 1847 the outcome of the successful campaign for early closing in Lincoln, headed by Alexander Leslie-Melville, of [[Branston Hall]].<ref>Gasser, p. 10; [https://books.google.com/books?id=A2Q5Yghl000C&pg=PA10 Google Books].</ref> ''The Claims of Science'' was given in 1851 at Queen's College, Cork.<ref>{{cite book|last=Boole |first=George |title=The Claims of Science, especially as founded in its relations to human nature; a lecture |url=https://books.google.com/books?id=BAlcAAAAQAAJ |accessdate=4 March 2012 |year=1851}}</ref> ''The Social Aspect of Intellectual Culture'' was also given in Cork, in 1855 to the Cuvierian Society.<ref>{{cite book |last=Boole |first=George |title=The Social Aspect of Intellectual Culture: an address delivered in the Cork Athenæum, May 29th, 1855 : at the soirée of the Cuvierian Society |url=https://books.google.com/books?id=PFWkZwEACAAJ |accessdate=4 March 2012 |year=1855 |publisher=George Purcell & Co.}}</ref>\n\nThough his biographer Des MacHale describes Boole as an \"agnostic deist\",<ref>{{cite book|title=Semiotica, Volume 105|year=1995|publisher=Mouton|page=56|author1=International Association for Semiotic Studies |author2=International Council for Philosophy and Humanistic Studies |author3=International Social Science Council|accessdate=31 March 2013|chapter=A tale of two amateurs|quote=MacHale's biography calls George Boole 'an agnostic deist'. Both Booles' classification of 'religious philosophies' as monistic, dualistic, and trinitarian left little doubt about their preference for 'the unity religion', whether Judaic or Unitarian.}}</ref><ref>{{cite book|title=Semiotica, Volume 105|year=1996|publisher=Mouton|page=17|author1=International Association for Semiotic Studies |author2=International Council for Philosophy and Humanistic Studies |author3=International Social Science Council|accessdate=31 March 2013|quote=MacHale does not repress this or other evidence of the Boole's nineteenth-century beliefs and practices in the paranormal and in religious mysticism. He even concedes that George Boole's many distinguished contributions to logic and mathematics may have been motivated by his distinctive religious beliefs as an \"agnostic deist\" and by an unusual personal sensitivity to the sufferings of other people.}}</ref> Boole read a wide variety of Christian theology. Combining his interests in mathematics and theology, he compared the [[Trinity|Christian trinity of Father, Son, and Holy Ghost]] with the three dimensions of space, and was attracted to the Hebrew conception of God as an absolute unity. Boole considered converting to [[Judaism]] but in the end was said to have chosen [[Unitarianism]]. Boole came to speak against a what he saw as \"prideful\" scepticism, and instead, favoured the belief in a \"Supreme Intelligent Cause.\"<ref>Boole, George. Studies in Logic and Probability. 2002. Courier Dover Publications. p. 201-202</ref> He  also declared \"I firmly believe, for the accomplishment of a purpose of the [[Divinity|Divine]] Mind.\"<ref>Boole, George. Studies in Logic and Probability. 2002. Courier Dover Publications. p. 451</ref><ref>Some-Side of a Scientific Mind (2013). pp. 112–3. The University Magazine, 1878. London: Forgotten Books. (Original work published 1878)</ref> In addition, he stated that he perceived \"teeming evidences of surrounding [[Teleological argument|design]]\" and concluded that \"the course of this world is not abandoned to chance and inexorable fate.\"<ref>Concluding remarks of his treatise of \"Clarke and Spinoza\", as found in Boole, George (2007). An Investigation of the Laws of Thought. Cosimo, Inc. Chap . XIII. p. 217-218. (Original work published 1854)</ref><ref>Boole, George (1851). The claims of science, especially as founded in its relations to human nature; a lecture, Volume 15. p. 24</ref>\n\nTwo influences on Boole were later claimed by his wife, [[Mary Everest Boole]]: a universal mysticism tempered by [[Jews|Jewish]] thought, and [[Indian logic]].<ref name=Ganeri>Jonardon Ganeri (2001), ''Indian Logic: a reader'', Routledge, p. 7, {{isbn|0-7007-1306-9}}; [https://books.google.com/books?id=t_nOiqFmxOIC&pg=PA7 Google Books].</ref> Mary Boole stated that an adolescent mystical experience provided for his life's work:\n<blockquote>My husband told me that when he was a lad of seventeen a thought struck him suddenly, which became the foundation of all his future discoveries. It was a flash of psychological insight into the conditions under which a mind most readily accumulates knowledge [...] For a few years he supposed himself to be convinced of the truth of \"the Bible\" as a whole, and even intended to take orders as a clergyman of the English Church. But by the help of a learned [[Jews|Jew]] in Lincoln he found out the true nature of the discovery which had dawned on him. This was that man's mind works by means of some mechanism which \"functions normally towards [[Monism]].\"<ref name=MaryBoole>Boole, Mary Everest ''Indian Thought and Western Science in the Nineteenth Century'', Boole, Mary Everest ''Collected Works'' eds. E. M. Cobham and E. S. Dummer, London, Daniel 1931 pp.947–967</ref></blockquote>\n\nIn Ch. 13 of ''Laws of Thought'' Boole used examples of propositions from [[Baruch Spinoza]] and [[Samuel Clarke]]. The work contains some remarks on the relationship of logic to religion, but they are slight and cryptic.<ref>Grattan-Guinness and Bornet, p. 16; [https://books.google.com/books?id=pzg7UFsIVJIC&pg=PR16 Google Books].</ref> Boole was apparently disconcerted at the book's reception just as a mathematical toolset:\n<blockquote>George afterwards learned, to his great joy, that the same conception of the basis of Logic was held by [[Gottfried Wilhelm Leibniz|Leibnitz]], the contemporary of Newton. De Morgan, of course, understood the formula in its true sense; he was Boole's collaborator all along. Herbert Spencer, Jowett, and [[Robert Leslie Ellis]] understood, I feel sure; and a few others, but nearly all the logicians and mathematicians ignored [953] the statement that the book was meant to throw light on the nature of the human mind; and treated the formula entirely as a wonderful new method of reducing to logical order masses of evidence about external fact.<ref name=MaryBoole/></blockquote>\n\nMary Boole claimed that there was profound influence – via her uncle [[George Everest]] – of [[India]]n thought in general and [[Indian logic]], in particular, on George Boole, as well as on [[Augustus De Morgan]] and [[Charles Babbage]]<ref>Kak, S. (2018) George Boole’s Laws of Thought and Indian logic. Current Science, vol. 114, 2570-2573</ref>:\n<blockquote>Think what must have been the effect of the intense Hinduizing of three such men as Babbage, De Morgan, and George Boole on the mathematical atmosphere of 1830–65. What share had it in generating the [[Vector Analysis]] and the mathematics by which investigations in physical science are now conducted?<ref name=MaryBoole/></blockquote>\n\n==Family==\nIn 1855 he married [[Mary Everest Boole|Mary Everest]] (niece of [[George Everest]]), who later wrote several educational works on her husband's principles.\n\nThe Booles had five daughters:\n* Mary Ellen (1856–1908)<ref>{{cite web|url=http://georgeboole.com/boole/life/family/ |title=Family and Genealogy – His Life George Boole 200 |publisher=Georgeboole.com |date= |accessdate=7 March 2016}}</ref> who married the mathematician and author [[Charles Howard Hinton]] and had four children: George (1882–1943), Eric (*1884), William (1886–1909)<ref>''Smothers In Orchard'' in ''The Los Angeles Times'' v. 27 February 1909.</ref> and Sebastian (1887–1923), inventor of the [[Jungle gym]]. After the sudden death of her husband, Mary Ellen committed suicide in [[Washington, D.C.]] in May 1908.<ref>''`My Right To Die´, Woman Kills Self'' in ''The Washington Times'' v. 28 May 1908 ([http://chroniclingamerica.loc.gov/lccn/sn84026749/1908-05-28/ed-1/seq-1.pdf PDF]); ''Mrs. Mary Hinton A Suicide'' in ''The New York Times'' v. 29 May 1908 ([https://timesmachine.nytimes.com/timesmachine/1908/05/29/104804272.pdf PDF]).</ref> Sebastian had three children:\n** Jean Hinton (married name Rosner) (1917–2002), a peace activist.\n** [[William H. Hinton]] (1919–2004) visited China in the 1930s and 40s and wrote an influential account of the Communist land reform.\n** [[Joan Hinton]] (1921–2010) worked for the [[Manhattan Project]] and lived in China from 1948 until her death on 8 June 2010; she was married to [[Sid Engst]].\n* Margaret (1858–1935), married [[Edward Ingram Taylor]], an artist.\n** Their elder son [[Geoffrey Ingram Taylor]] became a mathematician and a Fellow of the [[Royal Society]].\n** Their younger son [[Julian Taylor (surgeon)|Julian]] was a professor of surgery.\n* [[Alicia Boole Stott|Alicia]] (1860–1940), who made important contributions to [[Four-dimensional space|four-dimensional geometry]].\n* [[Lucy Everest Boole|Lucy Everest]] (1862–1904), who was the first female professor of chemistry in England.\n* [[Ethel Lilian Voynich|Ethel Lilian]] (1864–1960), who married the Polish scientist and revolutionary [[Wilfrid Michael Voynich]] and was the author of the novel ''[[The Gadfly]]''.\n\n==See also==\n{{Portal|Logic|Mathematics|Biography}}\n* [[Boolean algebra]], a logical calculus of truth values or set membership\n* [[Boolean algebra (structure)]], a set with operations resembling logical ones\n* [[Boolean ring]], a ring consisting of idempotent elements\n* [[Boolean circuit]], a mathematical model for digital logical circuits.\n* [[Boolean data type]] is a data type, having two values (usually denoted true and false)\n* [[Boolean expression]], an expression in a programming language that produces a Boolean value when evaluated\n* [[Boolean function]], a function that determines Boolean values or operators\n* [[Boolean model (probability theory)]], a model in stochastic geometry\n* [[Boolean network]], a certain network consisting of a set of Boolean variables whose state is determined by other variables in the network\n* [[Boolean processor]], a 1-bit variables computing unit\n* [[Boolean satisfiability problem]]\n* [[Boole's syllogistic]] is a logic invented by 19th-century British mathematician George Boole, which attempts to incorporate the \"empty set.\"\n* [[List of Boolean algebra topics]]\n* [[List of pioneers in computer science]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n{{Refbegin}}\n* [[University College Cork]], ''George Boole 200 Bicentenary Celebration'', [http://georgeboole.com GeorgeBoole.com].\n* {{Cite EB1911|wstitle=Boole, George}}\n* [[Ivor Grattan-Guinness]], ''The Search for Mathematical Roots 1870–1940''. Princeton University Press. 2000.\n* [[Francis Hill]] (1974), ''Victorian Lincoln''; [https://books.google.com/books?id=-A89AAAAIAAJ&pg=PA149 Google Books].\n* [[Des MacHale]], '' George Boole: His Life and Work''. [http://boolepress.com/ Boole Press]. 1985.\n* [[Des MacHale]], '' The Life and Work of George Boole: A Prelude to the Digital Age'' (new edition). [http://www.corkuniversitypress.com/SearchResults.asp?Search=boole&Submit= Cork University Press]. 2014\n* [[Stephen Hawking]], ''[[God Created the Integers]]''. Running Press, Philadelphia. 2007.\n{{Refend}}\n\n==External links==\n{{Sister project links| wikt=no | commons=Category:George Boole | b=no | n=no | q=George Boole | s=Author:George Boole | v=no | voy=no | species=no | d=q134661}}\n* [http://www.rogerparsons.info/george/boole.html Roger Parsons' article on Boole]\n* [http://blog.stephenwolfram.com/2015/11/george-boole-a-200-year-view/ George Boole: A 200-Year View] by [[Stephen Wolfram]] November 2015\n* {{Gutenberg author |id=Boole,+George | name=George Boole}}\n* {{Internet Archive author |sname=George Boole}}\n* [https://www.maths.tcd.ie/pub/HistMath/People/Boole/CalcLogic/CalcLogic.html ''The Calculus of Logic''] by George Boole; a transcription of an article which originally appeared in ''Cambridge and Dublin Mathematical Journal'', Vol. III (1848), pp. 183–98.\n* [http://undersci.ucc.ie/wp-content/uploads/sites/12/2014/11/George_Boole.pdf George Boole's work as first Professor of Mathematics in University College, Cork, Ireland]\n* [http://georgeboole.com George Boole website]\n* [https://zbmath.org/authors/?q=ai:boole.george Author profile] in the database [[Zentralblatt MATH|zbMATH]]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Boole, George}}\n[[Category:1815 births]]\n[[Category:1864 deaths]]\n[[Category:Academics of Queens College Cork]]\n[[Category:Boolean algebra]]\n[[Category:British deists]]\n[[Category:English logicians]]\n[[Category:19th-century English philosophers]]\n[[Category:English Unitarians]]\n[[Category:Fellows of the Royal Society]]\n[[Category:Mathematical logicians]]\n[[Category:19th-century English writers]]\n[[Category:19th-century English mathematicians]]\n[[Category:People from Lincoln, England]]\n[[Category:Royal Medal winners]]\n[[Category:Victorian writers]]\n[[Category:History of logic]]\n[[Category:Innovators]]\n[[Category:Deaths from respiratory disease]]\n[[Category:19th-century British philosophers]]"
    },
    {
      "title": "Boole's expansion theorem",
      "url": "https://en.wikipedia.org/wiki/Boole%27s_expansion_theorem",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\n'''Boole's expansion theorem''', often referred to as the '''Shannon expansion''' or '''decomposition''', is the [[Identity (mathematics)|identity]]: <math>F = x \\cdot F_x + x' \\cdot F_{x'}</math>, where <math>F</math> is any [[Boolean function]], <math>x</math> is a variable, <math>x'</math> is the complement of <math>x</math>, and <math>F_x</math>and <math>F_{x'}</math> are <math>F</math> with the argument <math>x</math> set equal to <math>1</math> and to <math>0</math> respectively.\n\nThe terms <math>F_x</math> and <math>F_{x'}</math> are sometimes called the positive and negative '''Shannon cofactors''', respectively, of <math>F</math> with respect to <math>x</math>. These are functions, computed by '''restrict''' operator, <math>\\operatorname{restrict}(F, x, 0)</math> and <math>\\operatorname{restrict}(F, x, 1)</math> (see [[valuation (logic)]] and [[partial application]]).\n\nIt has been called the \"fundamental theorem of Boolean algebra\".<ref>Paul C. Rosenbloom, ''The Elements of Mathematical Logic'', 1950, p. 5</ref> Besides its theoretical importance, it paved the way for [[binary decision diagram]]s, [[Boolean satisfiability problem|satisfiability solvers]], and many other techniques relevant to [[computer engineering]] and [[formal verification]] of digital circuits.\n\n==Statement of the theorem==\nA more explicit way of stating the theorem is:\n\n: <math>f(X_1, X_2, \\dots , X_n) = X_1 \\cdot f(1, X_2, \\dots , X_n) + X_1' \\cdot f(0, X_2, \\dots , X_n)</math>\n\n== Variations and implications ==\n; XOR-Form : The statement also holds when the [[disjunction]] \"+\" is replaced by the [[Exclusive or|XOR]] operator:\n: <math>f(X_1, X_2, \\dots , X_n) = X_1 \\cdot f(1, X_2, \\dots , X_n) \\oplus X_1' \\cdot f(0, X_2, \\dots , X_n)</math>\n; Dual form: There is a dual form of the Shannon expansion (which does not have a related XOR form):\n: <math>f(X_1, X_2, \\dots , X_n) = (X_1 + f(0, X_2, \\dots , X_n)) \\cdot (X_1' + f(1, X_2, \\dots , X_n))</math>\n\nRepeated application for each argument leads to the [[Sum of Products]] (SoP) canonical form of the Boolean function <math>f</math>. For example for <math>n=2</math> that would be\n\n:<math>\\begin{align}f(X_1, X_2) & = X_1 \\cdot f(1, X_2) + X_1' \\cdot f(0, X_2)\\\\\n                                & = X_1 X_2 \\cdot f(1, 1) + X_1 X_2' \\cdot f(1, 0) + X_1' X_2 \\cdot f(0, 1) + X_1' X_2' \\cdot f(0, 0)\n\\end{align}</math>\n\nLikewise, application of the dual form leads to the [[Product of Sums]] (PoS) canonical form (using the [[Distributive property#Truth functional connectives|distributivity law]] of <math>+</math> over <math>\\cdot</math>):\n\n:<math>\\begin{align}f(X_1, X_2) & = (X_1 + f(0, X_2)) \\cdot (X_1' + f(1, X_2))\\\\\n                                & = (X_1 + X_2 + f(0, 0)) \\cdot (X_1 + X_2' + f(0, 1)) \\cdot (X_1' + X_2 + f(1, 0)) \\cdot (X_1' + X_2' + f(1, 1))\n\\end{align}</math>\n\n== Properties of Cofactors ==\n\n; '''Linear Properties of Cofactors:'''\n: For a boolean function ''F'' which is made up of two boolean functions ''G'' and ''H'' the following are true:\n: If <math>F = H'</math> then <math>F_x = H'_x</math>\n: If <math>F = G \\cdot H</math> then <math>F_x = G_x \\cdot H_x</math>\n: If <math>F = G + H</math> then <math>F_x = G_x + H_x</math>\n: If <math>F = G \\oplus H</math> then <math>F_x = G_x \\oplus H_x</math>\n\n; '''Characteristics of Unate Functions:'''\n: If ''F'' is a [[unate function]] and...\n: If ''F'' is positive unate then <math>F = x \\cdot F_x + F_{x'}</math>\n: If ''F'' is negative unate then <math>F = F_x + x' \\cdot F_{x'}</math>\n\n== Operations with Cofactors ==\n\n; '''Boolean Difference:'''\n: The boolean difference or [[Boolean differential calculus|boolean derivative]] of the function F with respect to the literal x is defined as:\n: <math> \\frac{\\partial F}{\\partial x} = F_x \\oplus F_{x'}</math>\n\n; '''Universal Quantification:'''\n: The universal quantification of F is defined as:\n: <math> \\forall x F = F_x \\cdot F_{x'}</math>\n\n; '''Existential Quantification:'''\n: The existential quantification of F is defined as:\n: <math> \\exists x F = F_x + F_{x'}</math>\n\n==History==\n[[George Boole]] presented this expansion as his Proposition II, \"To expand or develop a function involving any number of logical symbols\", in his ''[[The Laws of Thought|Laws of Thought]]'' (1854),<ref>George Boole, ''An Investigation of the Laws of Thought: On which are Founded the Mathematical Theories of Logic and Probabilities'', 1854, p. 72 [https://books.google.com/books?id=SWgLVT0otY8C&pg=PA73&dq=to+expand+or+develop+a+function full text at Google Books]</ref> and it was \"widely applied by Boole and other nineteenth-century logicians\".<ref name=\"brown\">Frank Markham Brown, ''Boolean Reasoning: The Logic of Boolean Equations'', 2nd edition, 2003, p. 42</ref>\n\n[[Claude Shannon]] mentioned this expansion, among other Boolean identities, in a 1948 paper,<ref>Claude Shannon, \"The Synthesis of Two-Terminal Switching Circuits\", ''Bell System Technical Journal'' '''28''':59–98, [https://archive.org/download/bstj28-1-59/bstj28-1-59.pdf full text], p. 62</ref> and showed the switching network interpretations of the identity.  In the literature of computer design and switching theory, the identity is often incorrectly attributed to Shannon.<ref name=\"brown\" />\n\n==Application to switching circuits==\n# [[Binary decision diagrams]] follow from systematic use of this theorem\n# Any Boolean function can be implemented directly in a [[switching circuit theory|switching circuit]] using a hierarchy of basic [[multiplexer]] by repeated application of this theorem.\n==Notes==\n<references/>\n\n==See also==\n* [[Reed–Muller expansion]]\n\n==External links==\n* [https://web.archive.org/web/20070927201537/http://homepages.ius.edu/JFDOYLE/c421/html/Chapter6.htm Shannon’s Decomposition] Example with multiplexers.\n* [http://www1.cs.columbia.edu/~sedwards/papers/soviani2007optimizing.pdf Optimizing Sequential Cycles Through Shannon Decomposition and Retiming (PDF)] Paper on application.\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean algebras canonically defined",
      "url": "https://en.wikipedia.org/wiki/Boolean_algebras_canonically_defined",
      "text": "{{no footnotes|date=May 2015}}\n:''Boolean algebras are models of the equational theory of two values; this definition is equivalent to the lattice and ring definitions.''\n\n[[Boolean algebra]] is a mathematically rich branch of [[abstract algebra]]. Just as [[group theory]] deals with [[Group (mathematics)|groups]], and [[linear algebra]] with [[vector spaces]], [[Boolean algebras]] are models of the [[equational theory]] of the two values 0 and 1 (whose interpretation need not be numerical). Common to Boolean algebras, groups, and vector spaces is the notion of an [[algebraic structure]], a [[Set (mathematics)|set]] closed under zero or more [[operation (mathematics)|operations]]  satisfying certain equations.\n\nJust as there are basic examples of groups, such as the group '''Z''' of [[integer]]s and the [[permutation group]] ''S''<sub>''n''</sub> of [[permutation]]s of ''n'' objects, there are also basic examples of Boolean algebra such as the following.\n* The algebra of [[binary digit]]s or bits 0 and 1 under the logical operations including disjunction, conjunction, and negation. Applications include the [[propositional calculus]] and the theory of [[digital circuits]].\n* The [[algebra of sets]] under the set operations including [[Union (set theory)|union]], [[intersection (set theory)|intersection]], and [[Complement (set theory)|complement]]. Applications include any area of mathematics for which sets form a natural [[foundation of mathematics|foundation]].\nBoolean algebra thus permits applying the methods of [[abstract algebra]] to [[mathematical logic]], [[digital logic]], and the set-theoretic [[foundations of mathematics]].\n\nUnlike [[Group (mathematics)|groups]] of finite [[order (group theory)|order]], which exhibit complexity and diversity and whose [[first-order logic|first-order]] theory is [[Decidability (logic)|decidable]] only in special cases, all finite Boolean algebras share the same theorems and have a decidable first-order theory. Instead the intricacies of Boolean algebra are divided between the structure of infinite algebras and the [[algorithm]]ic complexity of their [[syntax|syntactic]] structure.\n\n==Definition==\nBoolean algebra treats the [[equational theory]] of the maximal two-element [[finitary]] algebra, called the '''Boolean prototype''', and the models of that theory, called '''Boolean algebras'''. These terms are defined as follows.\n\nAn [[Universal algebra|algebra]] is a [[Indexed family|family]] of operations on a set, called the underlying set of the algebra. We take the underlying set of the Boolean prototype to be {0,1}.\n\nAn algebra is '''[[finitary]]''' when each of its operations takes only finitely many arguments. For the prototype each argument of an operation is either 0 or 1, as is the result of the operation. The maximal such algebra consists of all finitary operations on {0,1}.\n\nThe number of arguments taken by each operation is called the [[arity]] of the operation. An operation on {0,1} of arity ''n'', or ''n''-ary operation, can be applied to any of 2<sup>''n''</sup> possible values for its ''n'' arguments. For each choice of arguments the operation may return 0 or 1, whence there are 2<sup><span>2<sup>''n''</sup></span></sup> ''n''-ary operations.\n\nThe prototype therefore has two operations taking no arguments, called zeroary or '''nullary''' operations, namely zero and one. It has four [[unary operation]]s, two of which are constant operations, another is the identity, and the most commonly used one, called ''negation'', returns the opposite of its argument: 1 if 0, 0 if 1. It has sixteen [[binary operation]]s; again two of these are constant, another returns its first argument, yet another returns its second, one is called ''conjunction'' and returns 1 if both arguments are 1 and otherwise 0, another is called ''disjunction'' and returns 0 if both arguments are 0 and otherwise 1, and so on. The number of (''n''+1)-ary operations in the prototype is the square of the number of ''n''-ary operations, so there are 16<sup>2</sup> = 256 ternary operations, 256<sup>2</sup> = 65,536 quaternary operations, and so on.\n\nA [[Indexed family|family]] is indexed by an [[index set]]. In the case of a family of operations forming an algebra, the indices are called '''operation symbols''', constituting the '''language''' of that algebra. The operation indexed by each symbol is called the denotation or '''[[interpretation (model theory)|interpretation]]''' of that symbol. Each operation symbol specifies the arity of its interpretation, whence all possible interpretations of a symbol have the same arity. In general it is possible for an algebra to interpret distinct symbols with the same operation, but this is not the case for the prototype, whose symbols are in one-one correspondence with its operations. The prototype therefore has 2<sup><span>2<sup>''n''</sup></span></sup> ''n''-ary operation symbols, called the '''Boolean operation symbols''' and forming the language of Boolean algebra. Only a few operations have conventional symbols, such as ¬ for negation, ∧ for conjunction, and ∨ for disjunction. It is convenient to consider the ''i''-th ''n''-ary symbol to be <sup>''n''</sup>''f''<sub>''i''</sub> as done below in the section on [[Boolean algebras canonically defined#Truth tables|truth tables]].\n\nAn [[equational theory]] in a given language consists of equations between terms built up from variables using symbols of that language. Typical equations in the language of Boolean algebra are ''x''∧''y'' = ''y''∧''x'', ''x''∧''x'' = ''x'', ''x''∧¬''x'' = ''y''∧¬''y'', and ''x''∧''y'' = ''x''.\n\nAn algebra '''[[satisfiability|satisfies]]''' an equation when the equation holds for all possible values of its variables in that algebra when the operation symbols are interpreted as specified by that algebra. The laws of Boolean algebra are the equations in the language of Boolean algebra satisfied by the prototype. The first three of the above examples are Boolean laws, but not the fourth since 1∧0 ≠ 1.\n\nThe [[equational theory]] of an algebra is the set of all equations satisfied by the algebra. The laws of Boolean algebra therefore constitute the equational theory of the Boolean prototype.\n\nA [[model (model theory)|model of a theory]] is an algebra interpreting the operation symbols in the language of the theory and satisfying the equations of the theory.\n: ''A Boolean algebra is any model of the laws of Boolean algebra.''\n\nThat is, a Boolean algebra is a set and a family of operations thereon interpreting the Boolean operation symbols and satisfying the same laws as the Boolean prototype.\n\nIf we define a homologue of an algebra to be a model of the equational theory of that algebra, then a Boolean algebra can be defined as any homologue of the prototype.\n\n'''Example 1'''. The Boolean prototype is a Boolean algebra, since trivially it satisfies its own laws. It is thus the prototypical Boolean algebra. We did not call it that initially in order to avoid any appearance of circularity in the definition.\n\n==Basis==\nThe operations need not be all explicitly stated. A ''basis'' is any set from which the remaining operations can be obtained by composition. A \"Boolean algebra\" may be defined from any of several different bases. Three bases for Boolean algebra are in common use, the lattice basis, the ring basis, and the [[Sheffer stroke]] or NAND basis. These bases impart respectively a logical, an arithmetical, and a parsimonious character to the subject.\n* The [[lattice (order)|lattice]] basis originated in the 19th century with the work of [[George Boole|Boole]], [[Charles Sanders Peirce|Peirce]], and others seeking an algebraic formalization of logical thought processes.\n* The [[Boolean ring|ring]] basis emerged in the 20th century with the work of [[Ivan Ivanovich Zhegalkin|Zhegalkin]] and [[Marshall Stone|Stone]] and became the basis of choice for algebraists coming to the subject from a background in [[abstract algebra]]. Most treatments of Boolean algebra assume the lattice basis, a notable exception being [[Paul Halmos|Halmos]][1963] whose linear algebra background evidently endeared the ring basis to him.\n* Since all finitary operations on {0,1} can be defined in terms of the [[Sheffer stroke]] NAND (or its dual NOR), the resulting economical basis has become the basis of choice for analyzing [[digital circuit]]s, in particular [[gate array]]s in [[digital electronics]].\n\nThe common elements of the lattice and ring bases are the constants 0 and 1, and an [[associative]] [[commutative]] [[binary operation]], called [[meet (mathematics)|meet]] ''x''∧''y'' in the lattice basis, and [[multiplication]] ''xy'' in the ring basis. The distinction is only terminological. The lattice basis has the further operations of [[join (mathematics)|join]], ''x''∨''y'', and [[Complement (order theory)|complement]], ¬''x''. The ring basis has instead the arithmetic operation ''x''⊕''y'' of [[addition]] (the symbol ⊕ is used in preference to + because the latter is sometimes given the Boolean reading of join).\n\nTo be a basis is to yield all other operations by composition, whence any two bases must be intertranslatable. The lattice basis translates ''x''∨''y'' to the ring basis as ''x''⊕''y''⊕''xy'', and ¬''x'' as ''x''⊕1. Conversely the ring basis translates ''x''⊕''y'' to the lattice basis as (''x''∨''y'')∧¬(''x''∧''y'').\n\nBoth of these bases allow Boolean algebras to be defined via a subset of the equational properties of the Boolean operations. For the lattice basis, it suffices to define a Boolean algebra as a [[distributive lattice]] satisfying ''x''∧¬''x'' = 0 and ''x''∨¬''x'' = 1, called a [[Complemented lattice|complemented]] distributive lattice. The ring basis turns a Boolean algebra into a [[Boolean ring]], namely a ring satisfying ''x''<sup>2</sup> = ''x''.\n\n[[Emil Post]] gave a necessary and sufficient condition for a set of operations to be a basis for the nonzeroary Boolean operations. A ''nontrivial'' property is one shared by some but not all operations making up a basis. Post listed five nontrivial properties of operations, identifiable with the five [[Post's class]]es, each preserved by composition, and showed that a set of operations formed a basis if, for each property, the set contained an operation lacking that property. (The converse of Post's theorem, extending \"if\" to \"[[iff|if and only if]],\" is the easy observation that a property from among these five holding of every operation in a candidate basis will also hold of every operation formed by composition from that candidate, whence by nontriviality of that property the candidate will fail to be a basis.) Post's five properties are:\n* [[Monotonic function|monotone]], no 0-1 input transition can cause a 1-0 output transition;\n* [[affine transformation|affine]], representable with [[Zhegalkin polynomial]]s that lack bilinear or higher terms, e.g. ''x''⊕''y''⊕1 but not ''xy'';\n* [[De Morgan's laws|self-dual]], so that complementing all inputs complements the output, as with ''x'', or the [[median operator]] ''xy''⊕''yz''⊕''zx'', or their negations;\n* [[strict function|strict]] (mapping the all-zeros input to zero);\n* costrict (mapping all-ones to one).\nThe [[Sheffer stroke|NAND]] (dually NOR) operation lacks all these, thus forming a basis by itself.\n\n==Truth tables==\nThe finitary operations on {0,1} may be exhibited as [[truth table]]s, thinking of 0 and 1 as the [[truth value]]s '''false''' and '''true'''. They can be laid out in a uniform and application-independent way that allows us to name, or at least number, them individually. These names provide a convenient shorthand for the Boolean operations. The names of the ''n''-ary operations are binary numbers of 2<sup>''n''</sup> bits. There being 2<sup>2<span><sup>''n''</sup></span></sup> such operations, one cannot ask for a more succinct nomenclature. Note that each finitary operation can be called a [[switching function]].\n\nThis layout and associated naming of operations is illustrated here in full for arities from 0 to 2.\n\n<div>\n<center>\n::{| border=\"0\" style=\"border:4px\"\n|+ '''Truth tables for the Boolean operations of arity up to 2'''\n|- valign=\"top\"\n|\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ Constants\n|-\n!<math>{}^0\\!f_0</math>\n!<math>{}^0\\!f_1</math>\n|-\n|0 || 1\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ Unary operations\n|-\n!<math>x_0</math>\n!<math>{}^1\\!f_0</math>\n!<math>{}^1\\!f_1</math>\n!<math>{}^1\\!f_2</math>\n!<math>{}^1\\!f_3</math>\n|-\n!0\n|0 || 1 || 0 || 1\n|-\n!1\n|0 || 0 || 1 || 1\n|}\n|-\n| colspan=\"5\" |\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ Binary operations\n|-\n!<math>x_0</math>\n!<math>x_1</math>\n!<math>{}^2\\!f_0</math>\n!<math>{}^2\\!f_1</math>\n!<math>{}^2\\!f_2</math>\n!<math>{}^2\\!f_3</math>\n!<math>{}^2\\!f_4</math>\n!<math>{}^2\\!f_5</math>\n!<math>{}^2\\!f_6</math>\n!<math>{}^2\\!f_7</math>\n!<math>{}^2\\!f_8</math>\n!<math>{}^2\\!f_9</math>\n!<math>{}^2\\!f_{10}</math>\n!<math>{}^2\\!f_{11}</math>\n!<math>{}^2\\!f_{12}</math>\n!<math>{}^2\\!f_{13}</math>\n!<math>{}^2\\!f_{14}</math>\n!<math>{}^2\\!f_{15}</math>\n|-\n!0\n!0\n|0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1\n|-\n!1\n!0\n|0 || 0 || 1 || 1 || 0 || 0 || 1 || 1 || 0 || 0 || 1 || 1 || 0 || 0 || 1 || 1\n|-\n!0\n!1\n|0 || 0 || 0 || 0 || 1 || 1 || 1 || 1 || 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1\n|-\n!1\n!1\n|0 || 0 || 0 || 0 || 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1 || 1 || 1 || 1 || 1\n|}\n|}\n</center>\n</div>\n\nThese tables continue at higher arities, with 2<sup>''n''</sup> rows at arity ''n'', each row giving a valuation or binding of the ''n'' variables ''x''<sub>0</sub>,...''x''<sub>''n''−1</sub> and each column headed <sup>''n''</sup>''f''<sub>''i''</sub> giving the value <sup>''n''</sup>''f''<sub>''i''</sub>(''x''<sub>0</sub>,...,''x''<sub>''n''−1</sub>) of the ''i''-th ''n''-ary operation at that valuation. The operations include the variables, for example <sup>1</sup>''f''<sub>2</sub> is ''x''<sub>0</sub> while <sup>2</sup>''f''<sub>10</sub> is ''x''<sub>0</sub> (as two copies of its unary counterpart) and <sup>2</sup>''f''<sub>12</sub> is ''x''<sub>1</sub> (with no unary counterpart). Negation or complement ¬''x''<sub>0</sub> appears as <sup>1</sup>''f''<sub>1</sub> and again as <sup>2</sup>''f''<sub>5</sub>, along with <sup>2</sup>''f''<sub>3</sub> (¬''x''<sub>1</sub>, which did not appear at arity 1), disjunction or union ''x''<sub>0</sub>∨''x''<sub>1</sub> as <sup>2</sup>''f''<sub>14</sub>, conjunction or intersection ''x''<sub>0</sub>∧''x''<sub>1</sub> as <sup>2</sup>''f''<sub>8</sub>, implication ''x''<sub>0</sub>→''x''<sub>1</sub> as <sup>2</sup>''f''<sub>13</sub>, exclusive-or symmetric difference ''x''<sub>0</sub>⊕''x''<sub>1</sub>  as <sup>2</sup>''f''<sub>6</sub>, set difference ''x''<sub>0</sub>−''x''<sub>1</sub> as <sup>2</sup>''f''<sub>2</sub>, and so on.\n\nAs a minor detail important more for its form than its content, the operations of an algebra are traditionally organized as a list. Although we are here indexing the operations of a Boolean algebra by the finitary operations on {0,1}, the truth-table presentation above serendipitously orders the operations first by arity and second by the layout of the tables for each arity. This permits organizing the set of all Boolean operations in the traditional list format. The list order for the operations of a given arity is determined by the following two rules.\n\n: (i) The ''i''-th row in the left half of the table is the binary representation of ''i'' with its least significant or 0-th bit on the left (\"little-endian\" order, originally proposed by [[Alan Turing]], so it would not be unreasonable to call it Turing order).\n\n: (ii)  The ''j''-th column in the right half of the table is the binary representation of ''j'', again in little-endian order. In effect the subscript of the operation ''is'' the truth table of that operation. By analogy with [[Gödel number]]ing of computable functions one might call this numbering of the Boolean operations the Boole numbering.\n\nWhen programming in C or Java, bitwise disjunction is denoted <tt>''x''|''y''</tt>, conjunction <tt>''x''&''y''</tt>, and negation <tt>~''x''</tt>. A program can therefore represent for example the operation ''x''∧(''y''∨''z'') in these languages as <tt>''x''&(''y''|''z'')</tt>, having previously set <tt>''x''&nbsp;= 0xaa</tt>, <tt>''y''&nbsp;= 0xcc</tt>, and <tt>''z''&nbsp;= 0xf0</tt> (the \"<tt>0x</tt>\" indicates that the following constant is to be read in hexadecimal or base 16), either by assignment to variables or defined as macros. These one-byte (eight-bit) constants correspond to the columns for the input variables in the extension of the above tables to three variables. This technique is almost universally used in raster graphics hardware to provide a flexible variety of ways of combining and masking images, the typical operations being ternary and acting simultaneously on source, destination, and mask bits.\n\n==Examples==\n\n===Bit vectors===\n'''Example 2'''. All [[bit vector]]s of a given length form a Boolean algebra \"pointwise\", meaning that any ''n''-ary Boolean operation can be applied to ''n'' bit vectors one bit position at a time. For example, the ternary OR of three bit vectors each of length 4 is the bit vector of length 4 formed by oring the three bits in each of the four bit positions, thus 0100∨1000∨1001&nbsp;= 1101. Another example is the truth tables above for the ''n''-ary operations, whose columns are all the bit vectors of length 2<sup>''n''</sup> and which therefore can be combined pointwise whence the ''n''-ary operations form a Boolean algebra.\nThis works equally well for bit vectors of finite and infinite length, the only rule being that the bit positions all be indexed by the same set in order that \"corresponding position\" be well defined.\n\nThe '''[[atom (order theory)|atoms]]''' of such an algebra are the bit vectors containing exactly one 1. In general the atoms of a Boolean algebra are those elements ''x'' such that ''x''∧''y'' has only two possible values, ''x'' or 0.\n\n===Power set algebra===\n'''Example 3'''. The  '''power set algebra''', the set 2<sup>''W''</sup> of all subsets of a given set ''W''. This is just Example 2 in disguise, with ''W'' serving to index the bit positions. Any subset ''X'' of ''W'' can be viewed as the bit vector having 1's in just those bit positions indexed by elements of ''X''. Thus the all-zero vector is the empty subset of ''W'' while the all-ones vector is ''W'' itself, these being the constants 0 and 1 respectively of the power set algebra. The counterpart of disjunction ''x''∨''y'' is union ''X''∪''Y'', while that of conjunction ''x''∧''y'' is intersection ''X''∩''Y''. Negation ¬''x'' becomes ~''X'', complement relative to ''W''. There is also set difference ''X''∖''Y''&nbsp;= ''X''∩~''Y'', symmetric difference (''X''∖''Y'')∪(''Y''∖''X''), ternary union ''X''∪''Y''∪''Z'', and so on. The atoms here are the singletons, those subsets with exactly one element.\n\nExamples 2 and 3 are special cases of a general construct of algebra called [[direct product]], applicable not just to Boolean algebras but all kinds of algebra including groups, rings, etc. The direct product of any family ''B''<sub>i</sub> of Boolean algebras where ''i'' ranges over some index set ''I'' (not necessarily finite or even countable) is a Boolean algebra consisting of all ''I''-tuples (...''x''<sub>i</sub>,...) whose ''i''-th element is taken from ''B''<sub>''i''</sub>. The operations of a direct product are the corresponding operations of the constituent algebras acting within their respective coordinates; in particular operation <sup>''n''</sup>''f''<sub>''j''</sub> of the product operates on ''n'' ''I''-tuples by applying operation <sup>''n''</sup>''f''<sub>''j''</sub> of ''B''<sub>''i''</sub> to the ''n'' elements in the ''i''-th coordinate of the ''n'' tuples, for all ''i'' in ''I''.\n\nWhen all the algebras being multiplied together in this way are the same algebra ''A'' we call the direct product a ''direct power'' of ''A''. The Boolean algebra of all 32-bit bit vectors is the two-element Boolean algebra raised to the 32nd power, or power set algebra of a 32-element set, denoted 2<sup>32</sup>. The Boolean algebra of all sets of integers is 2<sup>'''Z'''</sup>. All Boolean algebras we have exhibited thus far have been direct powers of the two-element Boolean algebra, justifying the name \"power set algebra\".\n\n===Representation theorems===\nIt can be shown that every finite Boolean algebra is [[isomorphic]] to some power set algebra. Hence the cardinality (number of elements) of a finite Boolean algebra is a power of 2, namely one of 1,2,4,8,...,2<sup>''n''</sup>,... This is called a '''representation theorem''' as it gives insight into the nature of finite Boolean algebras by giving a [[representation (mathematics)|representation]] of them as power set algebras.\n\nThis representation theorem does not extend to infinite Boolean algebras: although every power set algebra is a Boolean algebra, not every Boolean algebra need be isomorphic to a power set algebra. In particular, whereas there can be no [[countably infinite]] power set algebras (the smallest infinite power set algebra is the power set algebra 2<sup>''N''</sup> of sets of natural numbers, [[Cantor's diagonal argument|shown]] by [[Georg Cantor|Cantor]] to be [[uncountable]]), there exist various countably infinite Boolean algebras.\n\nTo go beyond power set algebras we need another construct. A [[subalgebra]] of an algebra ''A'' is any subset of ''A'' closed under the operations of ''A''. Every subalgebra of a Boolean algebra ''A'' must still satisfy the equations holding of ''A'', since any violation would constitute a violation for ''A'' itself. Hence every subalgebra of a Boolean algebra is a Boolean algebra.\n\nA [[subalgebra]] of a power set algebra is called a [[field of sets]]; equivalently a field of sets is a set of subsets of some set ''W'' including the empty set and ''W'' and closed under finite union and complement with respect to ''W'' (and hence also under finite intersection). Birkhoff's [1935] representation theorem for Boolean algebras states that every Boolean algebra is isomorphic to a field of sets. Now [[Birkhoff's HSP theorem]] for varieties can be stated as, every class of models of the equational theory of a class ''C'' of algebras is the Homomorphic image of a [[Subalgebra]] of a [[direct product|direct Product]] of algebras of ''C''. Normally all three of H, S, and P are needed; what the first of these two Birkhoff theorems shows is that for the special case of the variety of Boolean algebras [[Homomorphism]] can be replaced by [[Isomorphism]]. Birkhoff's HSP theorem for varieties in general therefore becomes Birkhoff's ISP theorem for the [[variety (universal algebra)|variety]] of Boolean algebras.\n\n===Other examples===\nIt is convenient when talking about a set ''X'' of natural numbers to view it as a sequence ''x''<sub>0</sub>,''x''<sub>1</sub>,''x''<sub>2</sub>,... of bits, with ''x''<sub>''i''</sub>&nbsp;= 1 if and only if ''i'' ∈ ''X''. This viewpoint will make it easier to talk about [[subalgebra]]s of the power set algebra 2<sup>''N''</sup>, which this viewpoint makes the Boolean algebra of all sequences of bits. It also fits well with the columns of a truth table: when a column is read from top to bottom it constitutes a sequence of bits, but at the same time it can be viewed as the set of those valuations (assignments to variables in the left half of the table) at which the function represented by that column evaluates to 1.\n\n'''Example 4'''. ''Ultimately constant sequences''. Any Boolean combination of ultimately constant sequences is ultimately constant; hence these form a Boolean algebra. We can identify these with the integers by viewing the ultimately-zero sequences as nonnegative binary numerals (bit 0 of the sequence being the low-order bit) and the ultimately-one sequences as negative binary numerals (think [[two's complement]] arithmetic with the all-ones sequence being −1). This makes the integers a Boolean algebra, with union being bit-wise OR and complement being ''−x−1''. There are only countably many integers, so this infinite Boolean algebra is countable. The atoms are the powers of two, namely 1,2,4,.... Another way of describing this algebra is as the set of all finite and cofinite sets of natural numbers, with the ultimately all-ones sequences corresponding to the cofinite sets, those sets omitting only finitely many natural numbers.\n\n'''Example 5'''. ''Periodic sequence''. A sequence is called ''periodic'' when there exists some number ''n'' &gt; 0, called a witness to periodicity, such that ''x''<sub>''i''</sub>&nbsp;= ''x''<sub>''i''+''n''</sub> for all ''i'' ≥ 0. The period of a periodic sequence is its least witness. Negation leaves period unchanged, while the disjunction of two periodic sequences is periodic, with period at most the least common multiple of the periods of the two arguments (the period can be as small as 1, as happens with the union of any sequence and its complement). Hence the periodic sequences form a Boolean algebra.\n\nExample 5 resembles Example 4 in being countable, but differs in being atomless. The latter is because the conjunction of any nonzero periodic sequence ''x'' with a sequence of greater period is neither 0 nor ''x''. It can be shown that all countably infinite atomless Boolean algebras are isomorphic, that is, up to isomorphism there is only one such algebra.\n\n'''Example 6'''. ''Periodic sequence with period a power of two''. This is a proper [[subalgebra]] of Example 5 (a proper subalgebra equals the intersection of itself with its algebra). These can be understood as the finitary operations, with the first period of such a sequence giving the truth table of the operation it represents. For example, the truth table of ''x''<sub>0</sub> in the table of binary operations, namely <sup>2</sup>''f''<sub>10</sub>, has period 2 (and so can be recognized as using only the first variable) even though 12 of the binary operations have period 4. When the period is 2<sup>''n''</sup> the operation only depends on the first ''n'' variables, the sense in which the operation is finitary. This example is also a countably infinite atomless Boolean algebra. Hence Example 5 is isomorphic to a proper subalgebra of itself! Example 6, and hence Example 5, constitutes the free Boolean algebra on countably many generators, meaning the Boolean algebra of all finitary operations on a countably infinite set of generators or variables.\n\n'''Example 7'''. ''Ultimately periodic sequences'', sequences that become periodic after an initial finite bout of lawlessness. They constitute a proper extension of Example 5 (meaning that Example 5 is a proper [[subalgebra]] of Example 7) and also of Example 4, since constant sequences are periodic with period one. Sequences may vary as to when they settle down, but any finite set of sequences will all eventually settle down no later than their slowest-to-settle member, whence ultimately periodic sequences are closed under all Boolean operations and so form a Boolean algebra. This example has the same atoms and coatoms as Example 4, whence it is not atomless and therefore not isomorphic to Example 5/6. However it contains an infinite atomless [[subalgebra]], namely Example 5, and so is not isomorphic to Example 4, every [[subalgebra]] of which must be a Boolean algebra of finite sets and their complements and therefore atomic. This example is isomorphic to the direct product of Examples 4 and 5, furnishing another description of it.\n\n'''Example 8'''. The [[direct product]] of a Periodic Sequence (Example 5) with any finite but nontrivial Boolean algebra. (The trivial one-element Boolean algebra is the unique finite atomless Boolean algebra.) This resembles Example 7 in having both atoms and an atomless [[subalgebra]], but differs in having only finitely many atoms. Example 8 is in fact an infinite family of examples, one for each possible finite number of atoms.\n\nThese examples by no means exhaust the possible Boolean algebras, even the countable ones. Indeed, there are uncountably many nonisomorphic countable Boolean algebras, which Jussi Ketonen [1978] classified completely in terms of invariants representable by certain hereditarily countable sets.\n\n==Boolean algebras of Boolean operations==\nThe ''n''-ary Boolean operations themselves constitute a power set algebra 2<sup>''W''</sup>, namely when ''W'' is taken to be the set of 2<sup>''n''</sup> valuations of the ''n'' inputs. In terms of the naming system of operations <sup>''n''</sup>''f''<sub>''i''</sub> where ''i'' in binary is a column of a truth table, the columns can be combined with Boolean operations of any arity to produce other columns present in the table. That is, we can apply any Boolean operation of arity ''m'' to ''m'' Boolean operations of arity ''n'' to yield a Boolean operation of arity ''n'', for any ''m'' and ''n''.\n\nThe practical significance of this convention for both software and hardware is that ''n''-ary Boolean operations can be represented as words of the appropriate length. For example, each of the 256 ternary Boolean operations can be represented as an unsigned byte. The available logical operations such as AND and OR can then be used to form new operations. If we take ''x'', ''y'', and ''z'' (dispensing with subscripted variables for now) to be 10101010, 11001100, and 11110000 respectively (170, 204, and 240 in decimal, 0xaa, 0xcc, and 0xf0 in hexadecimal), their pairwise conjunctions are ''x''∧''y''&nbsp;= 10001000, ''y''∧''z''&nbsp;= 11000000, and ''z''∧''x''&nbsp;= 10100000, while their pairwise disjunctions are ''x''∨''y''&nbsp;= 11101110, ''y''∨''z''&nbsp;= 11111100, and ''z''∨''x''&nbsp;= 11111010. The disjunction of the three conjunctions is 11101000, which also happens to be the conjunction of three disjunctions. We have thus calculated, with a dozen or so logical operations on bytes, that the two ternary operations\n: (''x''∧''y'')∨(''y''∧''z'')∨(''z''∧''x'')\n\nand\n: (''x''∨''y'')∧(''y''∨''z'')∧(''z''∨''x'')\n\nare actually the same operation. That is, we have proved the equational identity\n: (''x''∧''y'')∨(''y''∧''z'')∨(''z''∧''x'')&nbsp;= (''x''∨''y'')∧(''y''∨''z'')∧(''z''∨''x''),\n\nfor the two-element Boolean algebra. By the definition of \"Boolean algebra\" this identity must therefore hold in every Boolean algebra.\n\nThis ternary operation incidentally formed the basis for Grau's [1947] ternary Boolean algebras, which he axiomatized in terms of this operation and negation. The operation is symmetric, meaning that its value is independent of any of the 3!&nbsp;= 6 permutations of its arguments. The two halves of its truth table 11101000 are the truth tables for ∨, 1110, and ∧, 1000, so the operation can be phrased as '''if''' ''z'' '''then''' ''x''∨''y'' '''else''' ''x''∧''y''. Since it is symmetric it can equally well be phrased as either of '''if''' ''x'' '''then''' ''y''∨''z'' '''else''' ''y''∧''z'', or '''if''' ''y'' '''then''' ''z''∨''x'' '''else''' ''z''∧''x''. Viewed as a labeling of the 8-vertex 3-cube, the upper half is labeled 1 and the lower half 0; for this reason it has been called the [[median operator]], with the evident generalization to any odd number of variables (odd in order to avoid the tie when exactly half the variables are 0).\n\n==Axiomatizing Boolean algebras==\nThe technique we just used to prove an identity of Boolean algebra can be generalized to all identities in a systematic way that can be taken as a sound and complete [[axiomatization]] of, or [[axiomatic system]] for, the equational laws of [[Boolean logic]]. The customary formulation of an axiom system consists of a set of axioms that \"prime the pump\" with some initial identities, along with a set of inference rules for inferring the remaining identities from the axioms and previously proved identities. In principle it is desirable to have finitely many axioms; however as a practical matter it is not necessary since it is just as effective to have a finite [[axiom schema]] having infinitely many instances each of which when used in a proof can readily be verified to be a legal instance, the approach we follow here.\n\nBoolean identities are assertions of the form ''s''&nbsp;= ''t'' where ''s'' and ''t'' are ''n''-ary terms, by which we shall mean here terms whose variables are limited to ''x''<sub>''0''</sub> through ''x''<sub>''n-1''</sub>. An ''n''-ary '''term''' is either an atom or an application. An application <sup>''m''</sup>''f''<sub>''i''</sub>(''t''<sub>0</sub>,...,''t''<sub>''m''-1</sub>) is a pair consisting of an ''m''-ary operation <sup>''m''</sup>''f''<sub>''i''</sub> and a list or ''m''-tuple (''t''<sub>0</sub>,...,''t''<sub>''m''-1</sub>) of ''m'' ''n''-ary terms called '''operands'''.\n\nAssociated with every term is a natural number called its '''height'''. Atoms are of zero height, while applications are of height one plus the height of their highest operand.\n\nNow what is an atom? Conventionally an atom is either a constant (0 or 1) or a variable ''x''<sub>''i''</sub> where 0 ≤ ''i'' &lt; ''n''. For the proof technique here it is convenient to define atoms instead to be ''n''-ary operations <sup>''n''</sup>''f''<sub>''i''</sub>, which although treated here as atoms nevertheless mean the same as ordinary terms of the exact form <sup>''n''</sup>''f''<sub>''i''</sub>(''x''<sub>0</sub>,...,''x''<sub>''n''-1</sub>) (exact in that the variables must listed in the order shown without repetition or omission). This is not a restriction because atoms of this form include all the ordinary atoms, namely the constants 0 and 1, which arise here as the ''n''-ary operations <sup>''n''</sup>''f''<sub>0</sub> and <sup>''n''</sup>''f''<sub>−1</sub> for each ''n'' (abbreviating 2<sup>2<span><sup>''n''</sup></span></sup>−1 to −1), and the variables ''x''<sub>0</sub>,...,''x''<sub>''n''-1</sub> as can be seen from the truth tables where ''x''<sub>0</sub> appears as both the unary operation <sup>1</sup>''f''<sub>2</sub> and the binary operation <sup>2</sup>''f''<sub>10</sub> while ''x''<sub>1</sub> appears as <sup>2</sup>''f''<sub>12</sub>.\n\nThe following axiom schema and three inference rules axiomatize the Boolean algebra of ''n''-ary terms.\n: '''A1'''. <sup>''m''</sup>''f''<sub>''i''</sub>(<sup>''n''</sup>''f''<sub>''j''<span><sub>0</sub></span></sub>,...,<sup>''n''</sup>''f''<sub>''j''<span><sub>''m''-1</sub></span></sub>)&nbsp;= <sup>''n''</sup>''f''<sub>''i''o''ĵ''</sub> where (''i''<small>o</small>''ĵ'')<sub>''v''</sub>&nbsp;= ''i''<sub>''ĵ''<span><sub>''v''</sub></span></sub>, with ''ĵ'' being ''j'' transpose, defined by (''ĵ''<sub>''v''</sub>)<sub>''u''</sub>&nbsp;= (''j''<sub>''u''</sub>)<sub>''v''</sub>.\n: '''R1'''. With no premises infer ''t''&nbsp;= ''t''.\n: '''R2'''. From ''s''&nbsp;= ''u'' and ''t''&nbsp;= ''u'' infer ''s''&nbsp;= ''t'' where ''s'', ''t'', and ''u'' are ''n''-ary terms.\n: '''R3'''. From ''s''<sub>0</sub>&nbsp;= ''t''<sub>0</sub>,...,''s''<sub>''m''-1</sub>&nbsp;= ''t''<sub>''m''-1</sub> infer <sup>''m''</sup>''f''<sub>''i''</sub>(''s''<sub>0</sub>,...,''s''<sub>''m''-1</sub>)&nbsp;= <sup>''m''</sup>''f''<sub>''i''</sub>(''t''<sub>0</sub>,...,''t''<sub>''m''-1</sub>), where all terms ''s''<sub>i</sub>, ''t''<sub>i</sub> are ''n''-ary.\n\nThe meaning of the side condition on '''A1''' is that ''i''<small>o</small>''ĵ'' is that 2<sup>''n''</sup>-bit number whose ''v''-th bit is the ''ĵ''<sub>''v''</sub>-th bit of ''i'', where the ranges of each quantity are ''u'': ''m'', ''v'': 2<sup>''n''</sup>, ''j''<sub>''u''</sub>: 2<sup>2<span><sup>''n''</sup></span></sup>, and ''ĵ''<sub>''v''</sub>: 2<sup>''m''</sup>. (So ''j'' is an ''m''-tuple of 2<sup>''n''</sup>-bit numbers while ''ĵ'' as the transpose of ''j'' is a 2<sup>''n''</sup>-tuple of ''m''-bit numbers. Both ''j'' and ''ĵ'' therefore contain ''m''2<sup>''n''</sup> bits.)\n\n'''A1''' is an axiom schema rather than an axiom by virtue of containing '''metavariables''', namely ''m'', ''i'', ''n'', and ''j<sub>0</sub>'' through ''j<sub>m-1</sub>''. The actual axioms of the axiomatization are obtained by setting the metavariables to specific values. For example, if we take ''m''&nbsp;= ''n''&nbsp;= ''i''&nbsp;= ''j''<sub>0</sub>&nbsp;= 1, we can compute the two bits of ''i''<small>o</small>''ĵ'' from ''i''<sub>1</sub>&nbsp;= 0 and ''i''<sub>0</sub>&nbsp;= 1, so ''i''<small>o</small>''ĵ''&nbsp;= 2 (or 10 when written as a two-bit number). The resulting instance, namely <sup>1</sup>''f''<sub>1</sub>(<sup>1</sup>''f''<sub>1</sub>)&nbsp;= <sup>1</sup>''f''<sub>2</sub>, expresses the familiar axiom ¬¬''x''&nbsp;= ''x'' of double negation. Rule '''R3''' then allows us to infer ¬¬¬''x''&nbsp;= ¬''x'' by taking ''s<sub>0</sub>'' to be <sup>1</sup>''f''<sub>1</sub>(<sup>1</sup>''f''<sub>1</sub>) or ¬¬''x''<sub>0</sub>, ''t<sub>0</sub>'' to be <sup>1</sup>''f''<sub>2</sub> or ''x''<sub>0</sub>, and <sup>''m''</sup>''f''<sub>''i''</sub> to be <sup>''1''</sup>''f''<sub>''1''</sub> or ¬.\n\nFor each ''m'' and ''n'' there are only finitely many axioms instantiating '''A1''', namely 2<sup>2<span><sup>''m''</sup></span></sup> × (2<sup>2<span><sup>''n''</sup></span></sup>)<sup>''m''</sup>. Each instance is specified by 2<sup>''m''</sup>+''m''2<sup>''n''</sup> bits.\n\nWe treat '''R1''' as an inference rule, even though it is like an axiom in having no premises, because it is a domain-independent rule along with '''R2''' and '''R3''' common to all equational axiomatizations, whether of groups, rings, or any other variety. The only entity specific to Boolean algebras is axiom schema '''A1'''. In this way when talking about different equational theories we can push the rules to one side as being independent of the particular theories, and confine attention to the axioms as the only part of the axiom system characterizing the particular equational theory at hand.\n\nThis axiomatization is complete, meaning that every Boolean law ''s''&nbsp;= ''t'' is provable in this system. One first shows by induction on the height of ''s'' that every Boolean law for which ''t'' is atomic is provable, using '''R1''' for the base case (since distinct atoms are never equal) and '''A1''' and '''R3''' for the induction step (''s'' an application). This proof strategy amounts to a recursive procedure for evaluating ''s'' to yield an atom. Then to prove ''s''&nbsp;= ''t'' in the general case when ''t'' may be an application, use the fact that if ''s''&nbsp;= ''t'' is an identity then ''s'' and ''t'' must evaluate to the same atom, call it ''u''. So first prove ''s''&nbsp;= ''u'' and ''t''&nbsp;= ''u'' as above, that is, evaluate ''s'' and ''t'' using '''A1''', '''R1''', and '''R3''', and then invoke '''R2''' to infer ''s''&nbsp;= ''t''.\n\nIn '''A1''', if we view the number ''n''<sup>''m''</sup> as the function type ''m''→''n'', and ''m''<sub>''n''</sub> as the application ''m''(''n''), we can reinterpret the numbers ''i'', ''j'', ''ĵ'', and ''i''<small>o</small>''ĵ'' as functions of type ''i'':&nbsp;(''m''→2)→2, ''j'':&nbsp;''m''→((''n''→2)→2), ''ĵ'':&nbsp;(''n''→2)→(''m''→2), and ''i''<small>o</small>''ĵ'':&nbsp;(''n''→2)→2. The definition (''i''<small>o</small>''ĵ'')<sub>''v''</sub>&nbsp;= ''i''<sub>''ĵ''<span><sub>''v''</sub></span></sub> in '''A1''' then translates to (''i''<small>o</small>''ĵ'')(''v'')&nbsp;= ''i''(''ĵ''(''v'')), that is, ''i''<small>o</small>''ĵ'' is defined to be composition of ''i'' and ''ĵ'' understood as functions. So the content of '''A1''' amounts to defining term application to be essentially composition, modulo the need to transpose the ''m''-tuple ''j'' to make the types match up suitably for composition. This composition is the one in Lawvere's previously mentioned category of power sets and their functions. In this way we have translated the commuting diagrams of that category, as the equational theory of Boolean algebras, into the equational consequences of '''A1''' as the logical representation of that particular composition law.\n\n==Underlying lattice structure==\nUnderlying every Boolean algebra ''B'' is a [[partially ordered set]] or '''poset''' (''B'',≤). The '''partial order''' relation is defined by ''x'' ≤ ''y'' just when ''x''&nbsp;= ''x''∧''y'', or equivalently when ''y''&nbsp;= ''x''∨''y''. Given a set ''X'' of elements of a Boolean algebra, an '''upper bound''' on ''X'' is an element ''y'' such that for every element ''x'' of ''X'', ''x'' ≤ ''y'', while a lower bound on ''X'' is an element ''y'' such that for every element ''x'' of ''X'', ''y'' ≤ ''x''.\n\nA '''sup''' ([[supremum]]) of ''X'' is a least upper bound on ''X'', namely an upper bound on ''X'' that is less or equal to every upper bound on ''X''. Dually an '''inf''' ([[infimum]]) of ''X'' is a greatest lower bound on ''X''. The sup of ''x'' and ''y'' always exists in the underlying poset of a Boolean algebra, being ''x''∨''y'', and likewise their inf exists, namely ''x''∧''y''. The empty sup is 0 (the bottom element) and the empty inf is 1 (top). It follows that every finite set has both a sup and an inf. Infinite subsets of a Boolean algebra may or may not have a sup and/or an inf; in a power set algebra they always do.\n\nAny poset (''B'',≤) such that every pair ''x'',''y'' of elements has both a sup and an inf is called a '''[[lattice (order)|lattice]]'''. We write ''x''∨''y'' for the sup and ''x''∧''y'' for the inf. The underlying poset of a Boolean algebra always forms a lattice. The lattice is said to be '''distributive''' when ''x''∧(''y''∨''z'')&nbsp;= (''x''∧''y'')∨(''x''∧''z''), or equivalently when ''x''∨(''y''∧''z'')&nbsp;= (''x''∨''y'')∧(''x''∨''z''), since either law implies the other in a lattice. These are laws of Boolean algebra whence the underlying poset of a Boolean algebra forms a distributive lattice.\n\nGiven a lattice with a bottom element 0 and a top element 1, a pair ''x'',''y'' of elements is called '''complementary''' when ''x''∧''y''&nbsp;= 0 and ''x''∨''y''&nbsp;= 1, and we then say that ''y'' is a complement of ''x'' and vice versa. Any element ''x'' of a distributive lattice with top and bottom can have at most one complement. When every element of a lattice has a complement the lattice is called complemented. It follows that in a complemented distributive lattice, the complement of an element always exists and is unique, making complement a unary operation. Furthermore, every complemented distributive lattice forms a Boolean algebra, and conversely every Boolean algebra forms a complemented distributive lattice. This provides an alternative definition of a Boolean algebra, namely as any complemented distributive lattice. Each of these three properties can be axiomatized with finitely many equations, whence these equations taken together constitute a finite axiomatization of the equational theory of Boolean algebras.\n\nIn a class of algebras defined as all the models of a set of equations, it is usually the case that some algebras of the class satisfy more equations than just those needed to qualify them for the class. The class of Boolean algebras is unusual in that, with a single exception, every Boolean algebra satisfies exactly the Boolean identities and no more. The exception is the one-element Boolean algebra, which necessarily satisfies every equation, even ''x''&nbsp;= ''y'', and is therefore sometimes referred to as the inconsistent Boolean algebra.\n\n==Boolean homomorphisms==\nA Boolean [[homomorphism]] is a function ''h'': ''A''→''B'' between Boolean algebras ''A'', ''B'' such that for every Boolean operation <sup>''m''</sup>''f''<sub>''i''</sub>,\n: ''h''(<sup>''m''</sup>''f''<sub>''i''</sub>(''x''<sub>0</sub>,...,''x''<sub>''m''−1</sub>))&nbsp;= <sup>''m''</sup>''f''<sub>''i''</sub>(''h''(''x''<sub>0</sub>),...,''h''(''x''<sub>''m''−1</sub>)).\n\nThe [[category (mathematics)|category]] '''Bool''' of Boolean algebras has as objects all Boolean algebras and as morphisms the Boolean homomorphisms between them.\n\nThere exists a unique homomorphism from the two-element Boolean algebra '''2''' to every Boolean algebra, since homomorphisms must preserve the two constants and those are the only elements of '''2'''. A Boolean algebra with this property is called an '''initial''' Boolean algebra. It can be shown that any two initial Boolean algebras are isomorphic, so up to isomorphism '''2''' is ''the'' initial Boolean algebra.\n\nIn the other direction, there may exist many homomorphisms from a Boolean algebra ''B'' to '''2'''. Any such homomorphism partitions ''B'' into those elements mapped to 1 and those to 0. The subset of ''B'' consisting of the former is called an [[ultrafilter]] of ''B''. When ''B'' is finite its ultrafilters pair up with its atoms; one atom is mapped to 1 and the rest to 0. Each ultrafilter of ''B'' thus consists of an atom of ''B'' and all the elements above it; hence exactly half the elements of ''B'' are in the ultrafilter, and there as many ultrafilters as atoms.\n\nFor infinite Boolean algebras the notion of ultrafilter becomes considerably more delicate. The elements greater or equal than an atom always form an ultrafilter but so do many other sets; for example in the Boolean algebra of finite and cofinite sets of integers the cofinite sets form an ultrafilter even though none of them are atoms. Likewise the powerset of the integers has among its ultrafilters the set of all subsets containing a given integer; there are countably many of these \"standard\" ultrafilters, which may be identified with the integers themselves, but there are uncountably many more \"nonstandard\" ultrafilters. These form the basis for [[nonstandard analysis]], providing representations for such classically inconsistent objects as infinitesimals and delta functions.\n\n==Infinitary extensions==\nRecall the definition of sup and inf from the section above on the underlying partial order of a Boolean algebra. A [[complete Boolean algebra]] is one every subset of which has both a sup and an inf, even the infinite subsets. Gaifman [1964] and [[Alfred W. Hales|Hales]] [1964] independently showed that infinite [[free object|free]] [[complete Boolean algebra]]s do not exist. This suggests that a logic with set-sized-infinitary operations may have class-many terms—just as a logic with finitary operations may have infinitely many terms.\n\nThere is however another approach to introducing infinitary Boolean operations: simply drop \"finitary\" from the definition of Boolean algebra. A model of the equational theory of the algebra of ''all'' operations on {0,1} of arity up to the cardinality of the model is called a complete atomic Boolean algebra, or ''CABA''.  (In place of this awkward restriction on arity we could allow any arity, leading to a different awkwardness, that the signature would then be larger than any set, that is, a proper class. One benefit of the latter approach is that it simplifies the definition of homomorphism between CABAs of different [[cardinality]].)  Such an algebra can be defined equivalently as a [[complete Boolean algebra]] that is '''atomic''', meaning that every element is a sup of some set of atoms. Free CABAs exist for all cardinalities of a set ''V'' of [[generating set of an algebra|generators]], namely the [[power set]] algebra 2<sup>2<span><sup>''V''</sup></span></sup>, this being the obvious generalization of the finite free Boolean algebras. This neatly rescues infinitary Boolean logic from the fate the Gaifman–Hales result seemed to consign it to.\n\nThe nonexistence of [[free object|free]] [[complete Boolean algebra]]s can be traced to failure to extend the equations of Boolean logic suitably to all laws that should hold for infinitary conjunction and disjunction, in particular the neglect of distributivity in the definition of complete Boolean algebra. A complete Boolean algebra is called '''completely distributive''' when arbitrary conjunctions distribute over arbitrary disjunctions and vice versa. A Boolean algebra is a CABA if and only if it is complete and completely distributive, giving a third definition of CABA. A fourth definition is as any Boolean algebra isomorphic to a power set algebra.\n\nA complete homomorphism is one that preserves all sups that exist, not just the finite sups, and likewise for infs. The category '''CABA''' of all CABAs and their complete homomorphisms is dual to the category of sets and their functions, meaning that it is equivalent to the opposite of that category (the category resulting from reversing all morphisms). Things are not so simple for the category '''Bool''' of Boolean algebras and their homomorphisms, which [[Marshall Stone]] showed in effect (though he lacked both the language and the conceptual framework to make the duality explicit) to be dual to the category of [[totally disconnected space|totally disconnected]] [[compact Hausdorff space]]s, subsequently called [[Stone space]]s.\n\nAnother infinitary class intermediate between Boolean algebras and [[complete Boolean algebra]]s is the notion of a [[sigma-algebra]]. This is defined analogously to complete Boolean algebras, but with [[supremum|sups]] and [[infimum|infs]] limited to countable arity. That is, a [[sigma-algebra]] is a Boolean algebra with all countable sups and infs. Because the sups and infs are of bounded [[cardinality]], unlike the situation with [[complete Boolean algebra]]s, the Gaifman-Hales result does not apply and [[free object|free]] [[sigma-algebra]]s do exist. Unlike the situation with CABAs however, the free countably generated sigma algebra is not a power set algebra.\n\n==Other definitions of Boolean algebra==\nWe have already encountered several definitions of Boolean algebra, as a model of the equational theory of the two-element algebra, as a complemented distributive lattice, as a Boolean ring, and as a product-preserving functor from a certain category (Lawvere). Two more definitions worth mentioning are:.\n\n; [[Marshall Stone|Stone]] (1936): A Boolean algebra is the set of all [[clopen set]]s of a [[topological space]]. It is no limitation to require the space to be a totally disconnected compact [[Hausdorff space]], or [[Stone space]], that is, every Boolean algebra arises in this way, up to [[isomorphism]]. Moreover, if the two Boolean algebras formed as the clopen sets of two Stone spaces are isomorphic, so are the Stone spaces themselves, which is not the case for arbitrary topological spaces. This is just the reverse direction of the duality mentioned earlier from Boolean algebras to [[Stone space]]s. This definition is fleshed out by the next definition.\n\n; Johnstone (1982): A Boolean algebra is a [[filtered colimit]] of finite Boolean algebras.\n\n(The circularity in this definition can be removed by replacing \"finite Boolean algebra\" by \"finite power set\" equipped with the Boolean operations standardly interpreted for power sets.)\n\nTo put this in perspective, infinite sets arise as filtered colimits of finite sets, infinite CABAs as filtered limits of finite power set algebras, and infinite Stone spaces as filtered limits of finite sets. Thus if one starts with the finite sets and asks how these generalize to infinite objects, there are two ways: \"adding\" them gives ordinary or inductive sets while \"multiplying\" them gives [[Stone space]]s or [[profinite set]]s. The same choice exists for finite power set algebras as the duals of finite sets: addition yields Boolean algebras as inductive objects while multiplication yields CABAs or power set algebras as profinite objects.\n\nA characteristic distinguishing feature is that the underlying topology of objects so constructed, when defined so as to be [[Hausdorff space|Hausdorff]], is [[discrete space|discrete]] for inductive objects and [[compact space|compact]] for profinite objects. The topology of finite Hausdorff spaces is always both discrete and compact, whereas for infinite spaces \"discrete\"' and \"compact\" are mutually exclusive. Thus when generalizing finite algebras (of any kind, not just Boolean) to infinite ones, \"discrete\" and \"compact\" part company, and one must choose which one to retain. The general rule, for both finite and infinite algebras, is that finitary algebras are discrete, whereas their duals are compact and feature infinitary operations. Between these two extremes, there are many intermediate infinite Boolean algebras whose topology is neither discrete nor compact.\n\n==See also==\n{{col-begin}}\n{{col-break}}\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean-valued function]]\n* [[Boolean-valued model]]\n* [[Cartesian closed category]]\n* [[Closed monoidal category]]\n* [[Complete Boolean algebra]]\n* [[Topos#Elementary topoi (topoi in logic)|Elementary topos]]\n{{col-break}}\n* [[Field of sets]]\n* [[Filter (mathematics)]]\n* [[Finitary boolean function]]\n* [[Free Boolean algebra]]\n* [[Functional completeness]]\n* [[Ideal (order theory)]]\n* [[Lattice (order)]]\n* [[Lindenbaum–Tarski algebra]]\n{{col-break}}\n* [[Monoidal category]]\n* [[Propositional calculus]]\n* [[Robbins algebra]]\n* [[Truth table]]\n* [[Ultrafilter]]\n* [[Universal algebra]]\n{{col-end}}\n\n==References==\n<!-- Use [[WP:CITET|templates]] for consistency -->\n* {{cite journal\n | last = Birkhoff\n | first = Garrett\n | authorlink = Garrett Birkhoff\n | title = On the structure of abstract algebras\n | journal = Proc. Camb. Phil. Soc.\n | volume = 31\n | pages = 433–454\n | year = 1935\n | issn = 0008-1981\n | doi=10.1017/s0305004100013463}}\n* {{cite book\n | last = Boole\n | first = George\n | authorlink = George Boole\n | title = An Investigation of the Laws of Thought\n | publisher = Prometheus Books\n | origyear = 1854\n | year = 2003\n | isbn = 978-1-59102-089-9 }}\n* {{cite book\n | last = Dwinger\n | first = Philip\n | title = Introduction to Boolean algebras\n | publisher = Physica Verlag\n | location = Würzburg\n | year = 1971 }}\n* {{cite journal\n | last = Gaifman\n | first = Haim\n | title = Infinite Boolean Polynomials, I\n | journal = Fundamenta Mathematicae\n | volume = 54\n | pages = 229–250\n | year = 1964\n | issn = 0016-2736 }}\n* {{Cite book\n | last1 = Givant\n | first1 = Steven\n | first2 = Paul\n | last2 = Halmos\n | year = 2009\n | title = Introduction to Boolean Algebras\n | series = [[Undergraduate Texts in Mathematics]]\n | publisher = Springer\n | isbn = 978-0-387-40293-2\n | postscript =}}.\n* {{cite journal\n | last = Grau\n | first = A.A.\n | title = Ternary Boolean algebra\n | journal = Bull. Am. Math. Soc.\n | volume = 33\n | pages = 567–572\n | year = 1947\n | doi = 10.1090/S0002-9904-1947-08834-0\n | issue = 6 }}\n* {{cite journal\n | last = Hales\n | first = Alfred W. | authorlink = Alfred W. Hales\n | title = On the Non-Existence of Free Complete Boolean Algebras\n | journal = Fundamenta Mathematicae\n | volume = 54\n | pages = 45–66\n | year = 1964\n | issn = 0016-2736 }}\n* {{cite book\n | last = Halmos\n | first = Paul\n | authorlink = Paul Halmos\n | title = Lectures on Boolean Algebras\n | publisher = van Nostrand\n | year = 1963\n | isbn = 0-387-90094-2 }}\n* --------, and Givant, Steven (1998) ''Logic as Algebra''. Dolciani Mathematical Exposition, No. 21. [[Mathematical Association of America]].\n* {{cite book\n | last = Johnstone\n | first = Peter T.\n | title = Stone Spaces\n | authorlink = Peter Johnstone (mathematician)\n | publisher = Cambridge University Press\n | year = 1982\n | location = Cambridge, UK\n | isbn = 978-0-521-33779-3 }}\n* {{cite journal\n | last = Ketonen\n | first = Jussi\n | title = The structure of countable Boolean algebras\n | jstor = 1970929\n | journal = Annals of Mathematics\n | volume = 108\n | issue = 1\n | pages = 41–89\n | year = 1978\n | doi = 10.2307/1970929 }}\n* Koppelberg, Sabine (1989) \"General Theory of Boolean Algebras\" in Monk, J. Donald, and Bonnet, Robert, eds., ''Handbook of Boolean Algebras, Vol. 1''. North Holland. {{ISBN|978-0-444-70261-6}}.\n<!-- The following alternatives do not work; still searching for good template\n* {{cite book\n  | last = Stone\n  | first = Marshall\n  | chapter = General Theory of Boolean Algebras\n  | title = The Theory of Representations for Boolean Algebras\n  | publisher = North Holland\n  | year = 1936\n  | location = Amsterdam\n  | isbn = 978-0-444-70261-6 --> <!--0-444-70261-X-->\n<!--\n* {{cite conference\n | first = Koppelberg\n | last = Sabine\n | year = 1989\n | title = General Theory of Boolean Algebras\n | booktitle = Handbook of Boolean Algebras, Vol. 1\n | editor = J. Donald Monk with Robert Bonnet\n | publisher = North Holland\n | location = Amsterdam\n | pages = \n | isbn = 978-0-444-70261-6\n}}-->\n* [[Charles Sanders Peirce|Peirce, C. S.]] (1989) ''Writings of Charles S. Peirce: A Chronological Edition: 1879–1884''. Kloesel, C. J. W., ed. Indianapolis: Indiana University Press. {{ISBN|978-0-253-37204-8}}.\n* {{cite journal\n | last = Lawvere\n | first = F. William\n | authorlink = William Lawvere\n | title = Functorial semantics of algebraic theories\n | journal = Proceedings of the National Academy of Sciences\n | volume = 50\n | issue = 5\n | pages = 869–873\n | year = 1963\n | url = http://www.tac.mta.ca/tac/reprints/articles/5/tr5abs.html\n | doi = 10.1073/pnas.50.5.869 | pmc = 221940\n }}\n* {{cite book\n | last = Schröder\n | first = Ernst\n | authorlink = Ernst Schröder\n | title = Vorlesungen über die Algebra der Logik (exakte Logik), I–III<!--, III.1-->\n | publisher = B.G. Teubner\n | location = Leipzig\n | date = 1890–1910<!--1895-->\n }}\n* {{cite book\n | last = Sikorski\n | first = Roman\n | authorlink = Roman Sikorski\n | title = Boolean Algebras\n | publisher = Springer-Verlag\n | location = Berlin\n | edition = 3rd.\n | year = 1969\n | isbn = 978-0-387-04469-9 }}\n* {{cite journal\n | authorlink = Marshall Harvey Stone\n | journal = Transactions of the American Mathematical Society\n | volume = 40\n | issue = 1\n | pages = 37–111\n | jstor = 1989664\n | issn = 0002-9947\n | doi = 10.2307/1989664\n | title = The Theory of Representation for Boolean Algebras\n | year = 1936\n | author = Stone, M. H. }}\n* [[Alfred Tarski|Tarski, Alfred]] (1983). ''Logic, Semantics, Metamathematics'', Corcoran, J., ed. Hackett. 1956 1st edition edited and translated by J. H. Woodger, Oxford Uni. Press. Includes English translations of the following two articles:\n** {{cite journal\n | last = Tarski\n | first = Alfred\n | authorlink = Alfred Tarski\n | title = Sur les classes closes par rapport à certaines opérations élémentaires\n | journal = Fundamenta Mathematicae\n | volume = 16\n | pages = 195–97\n | year = 1929\n | issn = 0016-2736 }}\n** {{cite journal\n | last = Tarski\n | first = Alfred\n | authorlink = Alfred Tarski\n | title = Zur Grundlegung der Booleschen Algebra, I\n | journal = Fundamenta Mathematicae\n | volume = 24\n | pages = 177–98\n | year = 1935\n | issn = 0016-2736 }}\n* {{cite book\n | last = Vladimirov\n | first = D.A.\n | title = булевы алгебры (Boolean algebras, in Russian, German translation Boolesche Algebren 1974)\n  | publisher = Nauka (German translation Akademie-Verlag)\n | year = 1969 }}\n\n[[Category:Articles with inconsistent citation formats]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean conjunctive query",
      "url": "https://en.wikipedia.org/wiki/Boolean_conjunctive_query",
      "text": "In the theory of [[relational databases]], a '''Boolean conjunctive query'''  is a [[conjunctive query]] without distinguished predicates, i.e., a query in the form <math>R_1(t_1) \\wedge \\cdots \\wedge R_n(t_n)</math>, where each <math>R_i</math> is a relation symbol and each <math>t_i</math> is a [[tuple]] of variables and constants; the number of elements in <math>t_i</math> is equal to the [[arity]] of <math>R_i</math>. Such a query evaluates to either true or false depending on whether the relations in the database contain the appropriate tuples of values, i.e. the conjunction is [[Validity (logic)|valid]] according to the facts in the database.\n\nAs an example, if a database schema contains the relation symbols {{mvar|Father}} (binary, who's the father of whom) and {{mvar|Employed}} (unary, who is employed), a conjunctive query could be <math>Father(\\text{Mark}, x) \\wedge Employed(x)</math>. This query evaluates to true if there exists an individual {{mvar|x}} who is a child of Mark and employed. In other words, this query expresses the question: \"does Mark have an employed child?\"\n\n==See also==\n*[[Logical conjunction]]\n*[[Conjunctive query]]\n\n==References==\n\n* {{cite journal\n|author1=G. Gottlob |author2=N. Leone |author3=F. Scarcello | title = The complexity of acyclic conjunctive queries\n| journal = Journal of the ACM\n| volume = 48\n| issue = 3\n| pages = 431–498\n| year = 2001\n| doi = 10.1145/382780.382783\n}}\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean data type",
      "url": "https://en.wikipedia.org/wiki/Boolean_data_type",
      "text": "In [[computer science]], the '''Boolean data type''' is a [[data type]] that has one of two possible values (usually denoted ''true'' and ''false''), intended to represent the two [[truth value]]s of [[logic]] and [[Boolean algebra]]. It is named after [[George Boole]], who first defined an algebraic system of logic in the mid 19th century. The Boolean data type is primarily associated with [[Conditional (computer programming)|conditional]] statements, which allow different actions by changing [[control flow]] depending on whether a programmer-specified Boolean ''condition'' evaluates to true or false. It is a special case of a more general ''logical data type (see [[probabilistic logic]])—''logic doesn't always need to be Boolean.\n\n== Generalities ==\nIn [[programming language]]s with a built-in Boolean data type, such as [[Pascal (programming language)|Pascal]] and [[Java (programming language)|Java]], the [[comparison operator]]s such as <code>></code> and <code>≠</code> are usually defined to return a Boolean value. [[if-then-else|Conditional]] and [[while loop|iterative]] commands may be defined to test Boolean-valued expressions.\n\nLanguages with no explicit Boolean data type, like [[ANSI C#C90|C90]] and [[Lisp (programming language)|Lisp]], may still represent truth values by some other data type. [[Common Lisp]] uses an empty list for false, and any other value for true. The C programming language uses an [[Integer (computer science)|integer]] type, where relational expressions like <code>i > j</code> and logical expressions connected by <code>&&</code> and <code>||</code> are defined to have value 1 if true and 0 if false, whereas the test parts of <code>if</code>, <code>while</code>, <code>for</code>, etc., treat any non-zero value as true.<ref name=\"k&r1e\">{{cite book |first1= Brian W |last1= Kernighan |authorlink1= Brian Kernighan |first2= Dennis M |last2= Ritchie |authorlink2= Dennis Ritchie |page= 41 |title= [[The C Programming Language]] |edition= 1st |publisher= [[Prentice Hall]] |year= 1978 |location= [[Englewood Cliffs, NJ]] |ISBN= 0-13-110163-3}}</ref><ref>{{cite book |pages=86–93 |first1= PJ |last1= Plauger |authorlink1= P. J. Plauger |first2= Jim |last2= Brodie |title= ANSI and ISO Standard C Programmer's reference |publisher= [[Microsoft Press]] |origyear= 1989 |year= 1992 |ISBN= 1-55615-359-7}}</ref> Indeed, a Boolean variable may be regarded (and implemented) as a numerical variable with one binary digit ([[bit]]), which can store only two values. The implementation of Booleans in computers are most likely represented as a full [[Word (computer architecture)|word]], rather than a bit; this is usually due to the ways computers transfer blocks of information.\n\nMost programming languages, even those with no explicit Boolean type, have support for Boolean algebraic operations such as [[Logical conjunction|conjunction]] (<code>AND</code>, <code>&</code>, <code>*</code>), [[Logical disjunction|disjunction]] (<code>OR</code>, <code>|</code>, <code>+</code>), [[Logical equivalence|equivalence]] (<code>EQV</code>, <code>=</code>, <code>==</code>), [[exclusive or]]/non-equivalence (<code>XOR</code>, <code>NEQV</code>, <code>^</code>, <code>!=</code>), and [[negation]] (<code>NOT</code>, <code>~</code>, <code>!</code>).\n\nIn some languages, like [[Ruby (programming language)|Ruby]], [[Smalltalk]], and [[Alice (software)|Alice]] the ''true'' and ''false'' values belong to separate [[Class (computer programming)|classes]], i.e., <code>True</code> and <code>False</code>, respectively, so there is no one Boolean ''type''.\n\nIn [[SQL]], which uses a [[three-valued logic]] for explicit comparisons because of its special treatment of [[Null (SQL)|Null]]s, the Boolean data type (introduced in [[SQL:1999]]) is also defined to include more than two truth values, so that SQL ''Booleans'' can store all logical values resulting from the evaluation of predicates in SQL. A column of Boolean type can also be restricted to just <code>TRUE</code> and <code>FALSE</code> though.\n\n== ALGOL and the built-in boolean type ==\nOne of the earliest programming languages to provide an explicit ''boolean'' data type was [[ALGOL 60]] (1960) with values ''true'' and ''false'' and logical operators denoted by symbols '<math>\\wedge</math>' (and), '<math>\\vee</math>' (or), '<math>\\supset</math>' (implies), '<math>\\equiv</math>' (equivalence), and '<math>\\neg</math>' (not).  Due to input device and [[character set]] limits on many computers of the time, however, most compilers used alternative representations for many of the operators, such as <code>AND</code> or <code>'AND'</code>.\n\nThis approach with ''boolean'' as a built-in (either [[Primitive data type|primitive]] or otherwise predefined) [[data type]] was adopted by many later programming languages, such as [[Simula]] 67 (1967), [[ALGOL 68]] (1970),<ref>{{cite web |url=http://www.fh-jena.de/~kleine/history/languages/Algol68-Report.pdf |title=Report on the Algorithmic Language ALGOL 68, Section 10.2.2. |date=August 1968 |format=PDF |access-date=30 April 2007 |deadurl=no |archiveurl=https://web.archive.org/web/20080406061108/http://www.fh-jena.de/~kleine/history/languages/Algol68-Report.pdf |archivedate=6 April 2008 |df= }}</ref> [[Pascal (programming language)|Pascal]] (1970), [[Ada (programming language)|Ada]] (1980), [[Java (programming language)|Java]] (1995), and [[C Sharp (programming language)|C#]] (2000), among others.\n\n== Fortran ==\nThe first version of [[Fortran|FORTRAN]] (1957) and its successor FORTRAN II (1958) had no logical values or operations; even the conditional <code>IF</code> statement took an arithmetic expression and branched to one of three locations according to its sign; see [[arithmetic IF]].  FORTRAN IV (1962), however, followed the ALGOL 60 example by providing a Boolean data type (<code>LOGICAL</code>), truth literals (<code>.TRUE.</code> and <code>.FALSE.</code>), Boolean-valued numeric comparison operators (<code>.EQ.</code>, <code>.GT.</code>, etc.), and logical operators (<code>.NOT.</code>, <code>.AND.</code>, <code>.OR.</code><!-- , <code>.XOR.</code>, <code>.EQV.</code>-->).  In <code>FORMAT</code> statements, a specific control character ('<code>L</code>') was provided for the parsing or formatting of logical values.<ref>Digital Equipment Corporation, ''DECSystem10 FORTRAN IV Programmers Reference Manual''. Reprinted in ''Mathematical Languages Handbook''. [http://www.bitsavers.org/pdf/tymshare/tymcom-x/Tymcom-X_Reference_Series_Fortran_IV_Jan73.pdf Online version] {{webarchive|url=https://web.archive.org/web/20110814003524/http://www.bitsavers.org/pdf/tymshare/tymcom-x/Tymcom-X_Reference_Series_Fortran_IV_Jan73.pdf |date=2011-08-14 }} accessed 2011-11-16.</ref><!-- Fortrans IV and 66 lack XOR and EQV. -->\n\n== Lisp and Scheme ==\nThe language [[Lisp (programming language)|Lisp]] (1958) never had a built-in Boolean data type. Instead, conditional constructs like <code>cond</code> assume that the logical value ''false'' is represented by the empty list <code>()</code>, which is defined to be the same as the special atom <code>nil</code> or <code>NIL</code>; whereas any other [[s-expression]] is interpreted as ''true''. For convenience, most modern dialects of Lisp predefine the atom <code>t</code> to have value <code>t</code>, so that <code>t</code> can be used as a mnemonic notation for ''true''.\n\nThis approach (''any value can be used as a Boolean value'') was retained in most Lisp dialects ([[Common Lisp]], [[Scheme (programming language)|Scheme]], [[Emacs Lisp]]), and similar models were adopted by many [[scripting language]]s, even ones having a distinct Boolean type or Boolean values; although which values are interpreted as ''false'' and which are ''true'' vary from language to language. In Scheme, for example, the ''false'' value is an atom distinct from the empty list, so the latter is interpreted as ''true''.\n\n== Pascal, Ada, and Haskell ==\nThe language [[Pascal (programming language)|Pascal]] (1970) introduced the concept of programmer-defined [[enumerated type]]s. A built-in <code>Boolean</code> data type was then provided as a predefined enumerated type with values <code>FALSE</code> and <code>TRUE</code>. By definition, all comparisons, logical operations, and conditional statements applied to and/or yielded <code>Boolean</code> values.  Otherwise, the <code>Boolean</code> type had all the facilities which were available for enumerated types in general, such as ordering and use as indices. In contrast, converting between <code>Boolean</code>s and integers (or any other types) still required explicit tests or function calls, as in ALGOL 60. This approach (''Boolean is an enumerated type'') was adopted by most later languages which had enumerated types, such as [[Modula]], [[Ada (programming language)|Ada]], and [[Haskell (programming language)|Haskell]].\n\n== C, C++, Objective-C, AWK ==\nInitial implementations of the language [[C (programming language)|C]] (1972) provided no Boolean type, and to this day Boolean values are commonly represented by integers (<code>int</code>s) in C programs. The comparison operators (<code>&gt;</code>, <code>==</code>, etc.) are defined to return a signed integer (<code>int</code>) result, either 0 (for false) or 1 (for true). Logical operators (<code>&amp;&amp;</code>, <code>||</code>, <code>!</code>, etc.) and condition-testing statements (<code>if</code>, <code>while</code>) assume that zero is false and all other values are true.\n\nAfter enumerated types (<code>enum</code>s) were added to the [[American National Standards Institute]] version of C, [[ANSI C]] (1989), many C programmers got used to defining their own Boolean types as such, for readability reasons. However, enumerated types are equivalent to integers according to the language standards; so the effective identity between Booleans and integers is still valid for C programs.\n\nStandard [[C (programming language)|C]] (since [[C99]]) provides a boolean type, called <code>_Bool</code>. By including the header <code>stdbool.h</code>, one can use the more intuitive name <code>bool</code> and the constants <code>true</code> and <code>false</code>. The language guarantees that any two true values will compare equal (which was impossible to achieve before the introduction of the type). Boolean values still behave as integers, can be stored in integer variables, and used anywhere integers would be valid, including in indexing, arithmetic, parsing, and formatting. This approach (''Boolean values are just integers'') has been retained in all later versions of C.\n\n[[C++]] has a separate Boolean data type <code>bool</code>, but with automatic conversions from scalar and pointer values that are very similar to those of C. This approach was adopted also by many later languages, especially by some [[scripting language]]s such as [[AWK]].\n\n[[Objective-C]] also has a separate Boolean data type <code>BOOL</code>, with possible values being <code>YES</code> or <code>NO</code>, equivalents of true and false respectively.<ref>{{cite web|url=https://developer.apple.com/library/ios/#documentation/cocoa/conceptual/ProgrammingWithObjectiveC/FoundationTypesandCollections/FoundationTypesandCollections.html|title=Guides and Sample Code|author=|date=|website=developer.apple.com|accessdate=1 May 2018|deadurl=no|archiveurl=https://web.archive.org/web/20110907013839/http://developer.apple.com/library/iOS/#documentation/cocoa/conceptual/ProgrammingWithObjectiveC/FoundationTypesandCollections/FoundationTypesandCollections.html|archivedate=7 September 2011|df=}}</ref> Also, in Objective-C compilers that support C99, C's <code>_Bool</code> type can be used, since Objective-C is a [[superset]] of C.\n\n== Perl and Lua ==\n[[Perl]] has no boolean data type. Instead, any value can behave as boolean in boolean context (condition of <code>if</code> or <code>while</code> statement, argument of <code>&&</code> or <code>||</code>, etc.). The number <code>0</code>, the strings <code>\"0\"</code> and <code>\"\"</code>, the empty list <code>()</code>, and the special value <code>undef</code> evaluate to false.<ref>{{cite web |url=http://perldoc.perl.org/perlsyn.html#Truth-and-Falsehood |title=perlsyn - Perl Syntax / Truth and Falsehood |access-date=10 September 2013 |deadurl=no |archiveurl=https://web.archive.org/web/20130826100652/http://perldoc.perl.org/perlsyn.html#Truth-and-Falsehood |archivedate=26 August 2013 |df= }}</ref> All else evaluates to true.\n\n[[Lua (programming language)|Lua]] has a boolean data type, but non-boolean values can also behave as booleans. The non-value <code>nil</code> evaluates to false, whereas every other data type always evaluates to true, regardless of value.\n\n== Tcl ==\n[[Tcl]] has no separate Boolean type. Like in C, the integers 0 (false) and 1 (true - in fact any nonzero integer) are used.<ref>\n{{cite web |title= PEP 285 -- Adding a bool type |date= 4 May 2011 |url= https://wiki.tcl.tk/16235 |access-date= 28 March 2018 |deadurl= no |archiveurl= https://web.archive.org/web/20180328231245/https://wiki.tcl.tk/16235 |archivedate= 28 March 2018 |df=  }}</ref>\n\nExamples of coding:\n\n<code>\nset v 1\nif { $v } { puts \"V is 1 or true\" }\n</code>\n\nThe above will show \"V is 1 or true\" since the expression evaluates to '1'\n\n<code>\nset v \"\"\nif { $v } ....\n</code>\n\nThe above will render an error as variable 'v' cannot be evaluated as '0' or '1'\n\n== Python, Ruby, and JavaScript ==\n[[Python (programming language)|Python]], from version 2.3 forward, has a <code>bool</code> type which is a [[Subclass (computer science)|subclass]] of <code>int</code>, the standard integer type.<ref>{{cite web |last= Van Rossum |first= Guido |authorlink= Guido van Rossum |title= PEP 285 -- Adding a bool type |date= 3 April 2002 |url= https://www.python.org/dev/peps/pep-0285/ |access-date= 15 May 2013 |deadurl= no |archiveurl= https://web.archive.org/web/20130501225326/http://www.python.org/dev/peps/pep-0285/ |archivedate= 1 May 2013 |df=  }}</ref> It has two possible values: <code>True</code> and <code>False</code>, which are ''special versions'' of 1 and 0 respectively and behave as such in arithmetic contexts. Also, a numeric value of zero (integer or fractional), the null value (<code>None</code>), the empty [[String (computer science)|string]], and empty containers (i.e. [[List (computing)|lists]], [[Set (computing)|sets]], etc.) are considered Boolean false; all other values are considered Boolean true by default.<ref>{{cite web |url= https://docs.python.org/3.3/reference/expressions.html |title= Expressions |website= Python v3.3.2 documentation |access-date= 15 May 2013 |deadurl= no |archiveurl= https://web.archive.org/web/20130522082703/http://docs.python.org/3.3/reference/expressions.html |archivedate= 22 May 2013 |df=  }}</ref> Classes can define how their instances are treated in a Boolean context through the special method <code>__nonzero__</code> (Python 2) or <code>__bool__</code> (Python 3). For containers, <code>__len__</code> (the special method for determining the length of containers) is used if the explicit Boolean conversion method is not defined.\n\nIn [[Ruby (programming language)|Ruby]], in contrast, only <code>nil</code> (Ruby's null value) and a special <code>false</code> object are ''false'', all else (including the integer 0 and empty arrays) is ''true''.\n\nIn [[JavaScript]], the empty string (<code>\"\"</code>), <code>null</code>, <code>undefined</code>, <code>[[NaN]]</code>, +0, [[−0]] and <code>false</code><ref>{{cite web |url=http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-262.pdf |title=ECMAScript Language Specification |page=43 |deadurl=no |archiveurl=https://web.archive.org/web/20150412040502/http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-262.pdf |archivedate=2015-04-12 |df= }}</ref>\nare sometimes called ''falsy'' (of which the [[Complement (set theory)|complement]] is ''truthy'') to distinguish between strictly [[Type safety|type-checked]] and [[Type conversion|coerced]] Booleans.<ref>{{cite web |url=http://javascript.crockford.com/style2.html |title=The Elements of JavaScript Style |publisher=Douglas Crockford |access-date=5 March 2011 |deadurl=no |archiveurl=https://web.archive.org/web/20110317074944/http://javascript.crockford.com/style2.html |archivedate=17 March 2011 |df= }}</ref> Languages such as [[PHP]] also use this approach.\n\n== SQL ==\n{{main|Null (SQL)#Comparisons with NULL and the three-valued logic (3VL)}}\n\nBooleans appear in SQL when a condition is needed, such as WHERE clause, in form of predicate which is produced by using operators such as comparison operators, IN operator, IS (NOT) NULL etc. However, apart from TRUE and FALSE, these operators can also yield a third state, called UNKNOWN, when comparison with NULL is made.\n\nThe treatment of boolean values differs between SQL systems.\n\nFor example, in [[Microsoft SQL Server]], boolean value is not supported at all, neither as a standalone data type nor representable as an integer. It shows an error message \"An expression of non-boolean type specified in a context where a condition is expected\" if a column is directly used in the WHERE clause, e.g. <code>SELECT a FROM t WHERE a</code>, while statement such as <code>SELECT column IS NOT NULL FROM t</code> yields a syntax error. The BIT data type, which can only store integers 0 and 1 apart from NULL, is commonly used as a workaround to store Boolean values, but workarounds need to be used such as <code>UPDATE t SET flag = IIF(col IS NOT NULL, 1, 0) WHERE flag = 0</code> to convert between the integer and boolean expression.\n\nIn [[PostgreSQL]], there is a distinct BOOLEAN type as in the standard<ref>[https://www.postgresql.org/docs/9.1/datatype-boolean.html]</ref> which allows predicates to be stored directly into a BOOLEAN column, and allows using a BOOLEAN column directly as a predicate in WHERE clause.\n\nIn [[MySQL]], BOOLEAN is treated as an alias as TINYINT(1)<ref>[https://dev.mysql.com/doc/refman/8.0/en/numeric-type-overview.html]</ref>, TRUE is the same as integer 1 and FALSE is the same is integer 0.<ref>[https://dev.mysql.com/doc/refman/8.0/en/boolean-literals.html]</ref>, and treats any non-zero integer as true when evaluating conditions.\n\nThe SQL92 standard introduced IS (NOT) TRUE, IS (NOT) FALSE, IS (NOT) UNKNOWN operators which evaluate a predicate, which predated the introduction of boolean type in [[SQL:1999]]\n\nThe [[SQL:1999]] standard introduced a BOOLEAN data type as an optional feature (T031). When restricted by a <code>NOT NULL</code> constraint, a SQL BOOLEAN behaves like Booleans in other languages, which can store only TRUE and FALSE values. However, if it is nullable, which is the default like all other SQL data types, it can have the special [[Null (SQL)|null]] value also. Although the SQL standard defines three [[Literal (computer programming)|literals]] for the BOOLEAN type – TRUE, FALSE, and UNKNOWN – it also says that the NULL BOOLEAN and UNKNOWN \"may be used interchangeably to mean exactly the same thing\".<ref name=\"Date2011\">{{cite book|author=C. Date|title=SQL and Relational Theory: How to Write Accurate SQL Code|url=https://books.google.com/books?id=Ew06OZtjuJEC&pg=PA83|year=2011|publisher=O'Reilly Media, Inc.|isbn=978-1-4493-1640-2|page=83}}</ref><ref>ISO/IEC 9075-2:2011 §4.5</ref> This has caused some controversy because the identification subjects UNKNOWN to the equality comparison rules for NULL. More precisely UNKNOWN = UNKNOWN is not TRUE but UNKNOWN/NULL.<ref name=\"Prigmore2007\">{{cite book|author=Martyn Prigmore|title=Introduction to Databases With Web Applications|url=https://books.google.com/books?id=PKggKqIZnN0C&pg=PA197|year=2007|publisher=Pearson Education Canada|isbn=978-0-321-26359-9|page=197}}</ref> As of 2012 few major SQL systems implement the T031 feature.<ref>Troels Arvin, [http://troels.arvin.dk/db/rdbms/#data_types-boolean Survey of BOOLEAN data type implementation] {{webarchive|url=https://web.archive.org/web/20050309010315/http://troels.arvin.dk/db/rdbms/ |date=2005-03-09 }}</ref> Firebird and [[PostgreSQL]] are notable exceptions, although PostgreSQL implements no UNKNOWN literal; NULL can be used instead.<ref>{{cite web|url=http://www.postgresql.org/docs/current/static/datatype-boolean.html|title=PostgreSQL: Documentation: 10: 8.6. Boolean Type|author=|date=|website=www.postgresql.org|accessdate=1 May 2018|deadurl=no|archiveurl=https://web.archive.org/web/20180309053449/https://www.postgresql.org/docs/current/static/datatype-boolean.html|archivedate=9 March 2018|df=}}</ref>\n\n==See also==\n* [[true and false (commands)]], for [[shell scripting]]\n* [[Shannon's expansion]]\n* [[stdbool.h]], C99 definitions for boolean\n* [[Boolean differential calculus]]\n\n{{Data types}}\n\n==References==\n{{Reflist|2}}\n\n{{DEFAULTSORT:Boolean Data Type}}\n[[Category:Boolean algebra]]\n[[Category:Data types]]\n[[Category:Primitive types]]\n[[Category:Articles with example ALGOL 68 code]]"
    },
    {
      "title": "Boolean domain",
      "url": "https://en.wikipedia.org/wiki/Boolean_domain",
      "text": "In [[mathematics]] and [[abstract algebra]], a '''Boolean domain''' is a [[Set (mathematics)|set]] consisting of exactly two elements whose interpretations include ''false'' and ''true''. In [[logic]], mathematics and [[theoretical computer science]], a Boolean domain is usually written as {0, 1},<ref>[[Dirk van Dalen]], ''Logic and Structure''. Springer (2004), page 15.</ref><ref>[[David Makinson]], ''Sets, Logic and Maths for Computing''.  Springer (2008), page 13.</ref><ref>[[George Boolos|George S. Boolos]] and [[Richard C. Jeffrey]], ''Computability and Logic''. Cambridge University Press (1980), page 99.</ref> {false, true}, {F, T},<ref>[[Elliott Mendelson]], ''Introduction to Mathematical Logic (4th. ed.)''.  Chapman & Hall/CRC (1997), page 11.</ref> <math>\\left \\{ \\bot,\\top \\right \\}</math><ref>[[Eric C. R. Hehner]], ''A Practical Theory of Programming''.  Springer (1993, 2010), page 3.</ref> or <math>\\mathbb{B}.</math><ref name=\"Parberry1994\">{{cite book|author=Ian Parberry|title=Circuit Complexity and Neural Networks|year=1994|publisher=MIT Press|isbn=978-0-262-16148-0|pages=65}}</ref><ref name=\"Cortadella2002\">{{cite book|author=Jordi Cortadella|title=Logic Synthesis for Asynchronous Controllers and Interfaces|year=2002|publisher=Springer Science & Business Media|isbn=978-3-540-43152-7|page=73|display-authors=etal}}</ref>\n\nThe [[algebraic structure]] that naturally builds on a Boolean domain is the [[two-element Boolean algebra|Boolean algebra with two elements]]. The [[initial object]] in the [[category (mathematics)|category]] of [[bounded lattice]]s is a Boolean domain.\n\nIn [[computer science]], a Boolean variable is a [[Variable (programming)|variable]] that takes values in some Boolean domain. Some [[programming language]]s feature [[reserved word]]s or symbols for the elements of the Boolean domain, for example <code>false</code> and <code>true</code>. However, many programming languages do not have a [[Boolean datatype]] in the strict sense. In [[C (programming language)|C]] or [[BASIC]], for example, falsity is represented by the number 0 and truth is represented by the number 1 or −1, and all variables that can take these values can also take any other numerical values.\n\n== Generalizations ==\nThe Boolean domain {0, 1} can be replaced by the [[unit interval]] {{closed-closed|0,1}}, in which case rather than only taking values 0 or 1, any value between and including 0 and 1 can be assumed. Algebraically, negation (NOT) is replaced with <math>1-x,</math> conjunction (AND) is replaced with multiplication (<math>xy</math>), and disjunction (OR) is defined via [[De Morgan's law]] to be <math>1-(1-x)(1-y)</math>.\n\nInterpreting these values as logical [[truth value]]s yields a [[multi-valued logic]], which forms the basis for [[fuzzy logic]] and [[probabilistic logic]]. In these interpretations, a value is interpreted as the \"degree\" of truth – to what extent a proposition is true, or the probability that the proposition is true.\n\n==See also==\n* [[Boolean-valued function]]\n\n==Notes==\n{{reflist}}\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean expression",
      "url": "https://en.wikipedia.org/wiki/Boolean_expression",
      "text": "In [[computer science]], a '''Boolean expression''' is used [[Expression (programming)|expression]] in a [[programming language]] that produces a [[Boolean value]] when evaluated, that is one of '''true''' or '''false'''. A Boolean expression may be composed of a combination of the Boolean constants '''true''' or '''false''', [[Boolean data type|Boolean-typed]] variables, Boolean-valued operators, and [[Boolean-valued function]]s.<ref>{{citation\n | last1 = Gries | first1 = David | author1-link = David Gries\n | last2 = Schneider | first2 = Fred B. | author2-link = Fred B. Schneider\n | contribution = Chapter 2. Boolean Expressions\n | isbn = 9780387941158\n | page = 25ff\n | publisher = Springer\n | series = Monographs in Computer Science\n | title = A Logical Approach to Discrete Math\n | url = https://books.google.com/books?id=ZWTDQ6H6gsUC&pg=PA25\n | year = 1993}}.</ref>\n\nBoolean expressions correspond to [[propositional formula]]s in logic and are a [[special case]] of [[Boolean circuit]]s.<ref>{{citation\n | last = van Melkebeek | first = Dieter\n | isbn = 9783540414926\n | page = 22\n | publisher = Springer\n | series = [[Lecture Notes in Computer Science]]\n | title = Randomness and Completeness in Computational Complexity\n | url = https://books.google.com/books?id=-S0zCjOAIVwC&pg=PA22\n | volume = 1950\n | year = 2000}}.</ref>\n\n==Boolean operators==\nMost [[programming language]]s have the Boolean operators [[Logical disjunction|OR]], [[Logical conjunction|AND]] and [[Negation|''not'']]; in [[C (programming language)|C]] and some newer languages, these are represented by \"||\" (double pipe character), \"&&\" (double [[ampersand]]) and \"!\" ([[Exclamation mark|exclamation point]]) respectively, while the corresponding [[bitwise operation]]s are represented by \"|\", \"&\" and \"~\" (tilde).<ref>E.g. for [[Java (programming language)|Java]] see {{citation\n | last1 = Brogden | first1 = William B.\n | last2 = Green | first2 = Marcus\n | isbn = 9780789728616\n | page = 45\n | publisher = Que Publishing\n | title = Java 2 Programmer\n | url = https://books.google.com/books?id=24nPZw9Wsf4C&pg=PA45\n | year = 2003}}.</ref>  In the mathematical literature the symbols used are often \"+\" ([[Plus sign|plus]]), \"'''·'''\" ([[Full stop|dot]]) and [[overbar]], or \"∨\" (cup), \"∧\" (cap) and \"¬\" or \"′\" (prime).\n\n==Examples==\n\n*The expression \"5 > 3\" is evaluated as '''true'''.\n*The expression \"3 > 5\" is evaluated as '''false'''. \n*\"5>=3\" and \"3<=5\" are equivalent Boolean expressions, both of which are evaluated as '''true'''.\n*\"typeof true\" returns \"boolean\" and \"typeof false\" returns \"boolean\"\n*Of course, most Boolean expressions will contain at least one variable (X > 3), and often more (X > Y).\n\n==See also==\n*[[Expression (computer science)]]\n*[[Expression (mathematics)]]\n\n==References==\n{{reflist}}\n \t\n==External links==\n*[http://www.maths.tcd.ie/pub/HistMath/People/Boole/CalcLogic/CalcLogic.html The Calculus of Logic], by George Boole, Cambridge and Dublin Mathematical Journal Vol. III (1848), pp.&nbsp;183–98.\n\n{{DEFAULTSORT:Boolean Expression}}\n[[Category:Boolean algebra]]\n[[Category:Operators (programming)]]"
    },
    {
      "title": "Boolean function",
      "url": "https://en.wikipedia.org/wiki/Boolean_function",
      "text": "{{distinguish|Binary function}}\n{{no footnotes|date=October 2012}}\nIn [[mathematics]] and [[logic]], a '''([[finitary]]) Boolean function''' (or switching function) is a [[function (mathematics)|function]] of the form ''ƒ''&nbsp;:&nbsp;'''B'''<sup>''k''</sup>&nbsp;→&nbsp;'''B''', where '''B'''&nbsp;=&nbsp;{0,&nbsp;1} is a ''[[Boolean domain]]'' and ''k'' is a non-negative integer called the [[arity]] of the function.  In the case where ''k''&nbsp;=&nbsp;0, the \"function\" is essentially a constant element of '''B'''.\n\nEvery ''k''-ary Boolean function can be expressed as a [[propositional formula]] in ''k'' variables ''x''<sub>1</sub>,&nbsp;…,&nbsp;''x''<sub>''k''</sub>, and two propositional formulas are [[logical equivalence|logically equivalent]] if and only if they express the same Boolean function. There are 2<sup>2<sup>''k''</sup></sup> ''k''-ary functions for every&nbsp;''k''.\n\n==Boolean functions in applications==\nA function that can be utilized to evaluate any Boolean output in relation to its Boolean input by logical type of calculations. Such functions play a basic role in questions of [[Computational complexity theory|complexity theory]] as well as the design of circuits and chips for [[digital computer]]s. The properties of Boolean functions play a critical role in [[cryptography]], particularly in the design of [[symmetric key algorithm]]s (see [[substitution box]]).\n\nBoolean functions are often represented by sentences in [[propositional logic]], and sometimes as multivariate [[polynomial]]s over [[finite field|GF]](2), but more efficient representations are [[binary decision diagram]]s (BDD), [[negation normal form]]s, and [[propositional directed acyclic graph]]s (PDAG).\n\nIn [[cooperative game]] theory, monotone Boolean functions are called '''simple games''' (voting games); this notion is applied to solve problems in [[social choice theory]].\n\n==See also==\n{{Portal|Logic}}\n{{div col}}\n* [[Algebra of sets]]\n* [[Boolean algebra]]\n* [[List of Boolean algebra topics|Boolean algebra topics]]\n* [[Boolean domain]]\n* [[Boolean differential calculus]]\n* [[Boolean-valued function]]\n* [[Logical connective]]\n* [[Truth function]]\n* [[Truth table]]\n* [[Symmetric Boolean function]]\n* [[Decision tree model]]\n* [[Evasive Boolean function]]\n* [[Indicator function]]\n* [[Balanced boolean function]]\n* [[Read-once function]]\n* [[Pseudo-Boolean function]]\n* [[File:Wikiversity-logo-en.svg|20px]] [[v:3-ary Boolean functions|3-ary Boolean functions]]\n{{div col end}}\n\n==References==\n* {{citation| last1= Crama | first1= Y | last2= Hammer | first2= P. L. | year= 2011 | title= Boolean Functions : Theory, Algorithms, and Applications | publisher= Cambridge University Press | doi = 10.1017/CBO9780511852008 | isbn = 9780511852008}}.\n* {{springer|title=Boolean function|id=p/b016940}}\n* {{cite journal | url = http://www.doiserbia.nb.rs/img/doi/1451-4869/2003/1451-48690301071J.pdf | journal = Serbian Journal of Electrical Engineering | volume = 1 | date = November 2003 | issue = 71-80, number 1 | title = Arithmetic expressions optimisation using dual polarity property | first1 = Dragan | last1 = Janković | first2 = Radomir S. | last2 = Stanković | first3 = Claudio | last3 = Moraga | format = PDF | accessdate = 2015-06-07 | deadurl = yes | archiveurl = https://web.archive.org/web/20160305044628/http://www.doiserbia.nb.rs/img/doi/1451-4869/2003/1451-48690301071J.pdf | archivedate = 2016-03-05 | df =  }}\n*{{cite book|author=Bradford Henry Arnold|title=Logic and Boolean Algebra|url=https://books.google.com/books?id=KoAsmTsOK9IC&printsec=frontcover#v=onepage&q=%22boolean%20function%22&f=false|date=1 January 2011|publisher=Courier Corporation|isbn=978-0-486-48385-6}}\n* {{citation| last1=Mano | first1= M. M. | last2= Ciletti | first2= M. D. | year= 2013 | title= Digital Design | publisher= Pearson}}.\n\n\n{{Mathematical logic}}\n\n{{DEFAULTSORT:Boolean Function}}\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean matrix",
      "url": "https://en.wikipedia.org/wiki/Boolean_matrix",
      "text": "In [[mathematics]], a '''Boolean matrix''' is a [[matrix (mathematics)|matrix]] with entries from a [[Boolean algebra]]. When the Boolean algebra has just two elements {0,1} the Boolean matrix is called a [[logical matrix]].\n\nLet ''U'' be a Boolean algebra with at least two elements. Intersection, union, complementation, and containment of elements is expressed in ''U''. Let ''V'' be the collection of ''n'' × ''n'' matrices that have entries taken from ''U''. Complementation of such a matrix is obtained by complementing each element. The intersection or union of two such matrices is obtained by applying the operation to entries of each pair of elements to obtain the corresponding matrix intersection or union. A matrix is contained in another if each entry of the first is contained in the corresponding entry of the second.\n\nThe product of two Boolean matrices is expressed as follows:\n:<math>(AB)_{ij} = \\bigcup_{k=1}^n (A_{ik} \\cap B_{kj} ) .</math>\n\nAccording to one author, \"Matrices over an arbitrary Boolean algebra &beta; satisfy most of the properties over &beta;<sub>0</sub> = {0, 1}. The reason is that any Boolean algebra is a sub-Boolean algebra of <math>\\beta_0^S </math> for some set ''S'', and we have an isomorphism from ''n'' × ''n'' matrices over <math>\\beta_0^S \\ \\text{to} \\ \\beta_n^S .</math>\"<ref>Ki Hang Kim (1982) ''Boolean Matrix Theory and Applications'', page 249, Appendix: Matrices over arbitrary Boolean Algebras, [[Marcel Dekker]] {{ISBN|0-8247-1788-0}}</ref>\n\n==References==\n{{Reflist}}\n* [[R. Duncan Luce]] (1952) \"A Note on Boolean Matrices\", [[Proceedings of the American Mathematical Society]] 3: 382–8, [https://www.jstor.org/stable/2031888?seq=1#page_scan_tab_contents Jstor link] {{mr|id=0050559}}\n* [[Jacques Riguet]] (1954) \"Sur l’extension du calcul des relations binaires au calcul des matrices a éléments dans une algèbre de Boole\", [[Comptes Rendus]] 238 : 2382 to 5\n\n==Further reading==\n* Stan Gudder & Frédéric Latrémolière (2009) \"Boolean inner-product spaces and Boolean matrices\", [[Linear Algebra and Its Applications]] 431: 274–96 {{mr|id=2522576}}\n* D.E. Rutherford (1963) \"Inverses of Boolean matrices\", ''Proceedings of the Glasgow Mathematical Association'' 6: 49–63 {{mr|id=0148585}}\n* T.S. Blythe (1967) \"Eigenvectors of Boolean Matrices\", [[Proceedings of the Royal Society of Edinburgh]] 67: 196–204 {{mr|id=0210727}}\n* Steven Kirkland & [[Norman J. Pullman]] (1993) \"Linear Operators Preserving Invariants of Non-binary Boolean Matrices\", ''Linear and Multilinear Algebra'' 33: 295–300 {{doi|10.1080/03081089308818200}} {{mr|id=1334678}}\n* Kyung-Kae Kang, Seok-Zun Song & Young-Bae Jung (2011) \"Linear Preservers of Regular Matrices over General Boolean Algebras\", ''Bulletin of the Malaysian Mathematical Sciences Society'', second series, 34(1): 113–25 {{mr|id=2783783}}\n\n\n[[Category:Matrices]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean ring",
      "url": "https://en.wikipedia.org/wiki/Boolean_ring",
      "text": "In [[mathematics]], a '''Boolean ring''' ''R'' is a [[ring (mathematics)|ring]] for which ''x''<sup>2</sup> = ''x'' for all ''x'' in ''R'',<ref>{{harvtxt|Fraleigh|1976|p=200}}</ref><ref>{{harvtxt|Herstein|1975|p=130}}</ref><ref>{{harvtxt|McCoy|1968|p=46}}</ref> such as the ring of [[modular arithmetic#Integers modulo n|integers modulo 2]].  That is, ''R'' consists only of [[idempotent element (ring theory)|idempotent element]]s.<ref>{{harvtxt|Fraleigh|1976|p=25}}</ref><ref>{{harvtxt|Herstein|1975|p=268}}</ref>\n\nEvery Boolean ring gives rise to a [[Boolean algebra (structure)|Boolean algebra]], with ring multiplication corresponding to [[logical conjunction|conjunction]] or [[meet (mathematics)|meet]] ∧, and ring addition to [[exclusive or|exclusive disjunction]] or [[symmetric difference]] ([http://math.stackexchange.com/questions/1621618/disjunction-as-sum-operation-in-boolean-ring ''not''] [[logical disjunction|disjunction]] ∨, which would constitute a [[semiring]]).  Boolean rings are named after the founder of Boolean algebra, [[George Boole]].\n\n==Notations==\nThere are at least four different and incompatible systems of notation for Boolean rings and algebras. \n*In commutative algebra the standard notation is to use ''x''&nbsp;+&nbsp;''y'' = (''x''&nbsp;∧&nbsp;¬&nbsp;''y'')&nbsp;∨&nbsp;(¬&nbsp;''x''&nbsp;∧&nbsp;''y'') for the ring sum of ''x'' and ''y'', and use ''xy'' = ''x''&nbsp;∧&nbsp;''y'' for their product.\n*In logic, a common notation is to use ''x''&nbsp;∧&nbsp;''y'' for the meet (same as the ring product) and use ''x''&nbsp;∨&nbsp;''y'' for the join, given in terms of ring notation (given just above) by ''x''&nbsp;+&nbsp;''y''&nbsp;+&nbsp;''xy''.\n*In set theory and logic it is also common to use ''x''&nbsp;·&nbsp;''y'' for the meet, and ''x''&nbsp;+&nbsp;''y'' for the join ''x''&nbsp;∨&nbsp;''y''. This use of + is different from the use in ring theory.\n*A rare convention is to use ''xy'' for the product and ''x''&nbsp;⊕&nbsp;''y'' for the ring sum, in an effort to avoid the ambiguity of +.\n\nHistorically, the term \"Boolean ring\" has been used to mean a \"Boolean ring possibly without an identity\", and \"Boolean algebra\" has been used to mean a Boolean ring with an identity.  The existence of the identity is necessary to consider the ring as an algebra over the [[GF(2)|field of two elements]]: otherwise there cannot be a (unital) ring homomorphism of the field of two elements into the Boolean ring. (This is the same as the old use of the terms \"ring\" and \"algebra\" in [[measure theory]].{{efn|When a Boolean ring has an identity, then a complement operation becomes definable on it, and a key characteristic of the modern definitions of both Boolean algebra and [[sigma-algebra]] is that they have complement operations.}})\n\n==Examples==\nOne example of a Boolean ring is the [[power set]] of any set ''X'', where the addition in the ring is [[symmetric difference]], and the multiplication is [[intersection (set theory)|intersection]]. As another example, we can also consider the set of all [[finite set|finite]] or cofinite subsets of ''X'', again with symmetric difference and intersection as operations. More generally with these operations any [[field of sets]] is a Boolean ring. By [[Stone's representation theorem for Boolean algebras|Stone's representation theorem]] every Boolean ring is isomorphic to a field of sets (treated as a ring with these operations).\n\n== Relation to Boolean algebras ==\n\n[[File:Vennandornot.svg|center|500px|thumb|[[Venn diagram]]s for the Boolean operations of conjunction, disjunction, and complement]]\nSince the join operation ∨ in a Boolean algebra is often written additively, it makes sense in this context to denote ring addition by ⊕, a symbol that is often used to denote [[exclusive or]].\n\nGiven a Boolean ring ''R'', for ''x'' and ''y'' in ''R'' we can define\n\n:''x'' ∧ ''y'' = ''xy'',\n\n:''x'' ∨ ''y'' = ''x'' ⊕ ''y'' ⊕ ''xy'',\n\n:¬''x'' = 1 ⊕ ''x''.\n\nThese operations then satisfy all of the axioms for meets, joins, and complements in a [[Boolean algebra (structure)|Boolean algebra]]. Thus every Boolean ring becomes a Boolean algebra.  Similarly, every Boolean algebra becomes a Boolean ring thus:\n\n:''xy'' = ''x'' ∧ ''y'',\n\n:''x'' ⊕ ''y'' = (''x'' ∨ ''y'') ∧ ¬(''x'' ∧ ''y'').\n\nIf a Boolean ring is translated into a Boolean algebra in this way, and then the Boolean algebra is translated into a ring, the result is the original ring. The analogous result holds beginning with a Boolean algebra.\n\nA map between two Boolean rings is a [[ring homomorphism]] [[if and only if]] it is a homomorphism of the corresponding Boolean algebras. Furthermore, a subset of a Boolean ring is a [[ring ideal]] (prime ring ideal, maximal ring ideal) if and only if it is an [[order ideal]] (prime order ideal, maximal order ideal) of the Boolean algebra. The [[quotient ring]] of a Boolean ring modulo a ring ideal corresponds to the factor algebra of the corresponding Boolean algebra modulo the corresponding order ideal.\n\n== Properties of Boolean rings==\n\nEvery Boolean ring ''R'' satisfies ''x'' ⊕ ''x'' = 0 for all ''x'' in ''R'', because we know\n\n:''x'' ⊕ ''x'' = (''x'' ⊕ ''x'')<sup>2</sup> = ''x''<sup>2</sup> ⊕ ''x''<sup>2</sup> ⊕ ''x''<sup>2</sup> ⊕ ''x''<sup>2</sup> = ''x'' ⊕ ''x'' ⊕ ''x'' ⊕ ''x''\n\nand since (''R'',⊕) is an abelian group, we can subtract ''x'' ⊕ ''x'' from both sides of this equation, which gives ''x'' ⊕ ''x'' = 0. A similar proof shows that every Boolean ring is [[commutative]]:\n\n:''x'' ⊕ ''y'' = (''x'' ⊕ ''y'')<sup>2</sup> = ''x''<sup>2</sup> ⊕ ''xy'' ⊕ ''yx'' ⊕ ''y''<sup>2</sup> = ''x'' ⊕ ''xy'' ⊕ ''yx'' ⊕ ''y''\n\nand this yields ''xy'' ⊕ ''yx'' = 0, which means ''xy''  = ''yx'' (using the first property above).\n\nThe property ''x'' ⊕ ''x'' = 0 shows that any Boolean ring is an [[associative algebra]] over the [[field (mathematics)|field]] '''F'''<sub>2</sub> with two elements, in just one way. In particular, any finite Boolean ring has as [[cardinality]] a [[power of two]]. Not every associative algebra with one over '''F'''<sub>2</sub> is a Boolean ring: consider for instance the [[polynomial ring]] '''F'''<sub>2</sub>[''X''].\n\nThe quotient ring ''R''/''I'' of any Boolean ring ''R'' modulo any ideal ''I'' is again a Boolean ring. Likewise, any [[subring]] of a Boolean ring is a Boolean ring.\n\nAny [[localization_of_a_ring|localization]] <math>RS^{-1}</math> of a Boolean ring ''R'' by a set <math>S\\subseteq R</math> is a Boolean ring, since every element in the localization is idempotent.\n\nThe maximal ring of quotients <math>Q(R)</math> (in the sense of Utumi and Lambek) of a Boolean ring ''R'' is a Boolean ring, since every partial endomorphism is idempotent<ref>{{cite journal|last1=B. Brainerd, J. Lambek|title=On the ring of quotients of a Boolean ring|journal=[[Canadian Mathematical Bulletin]] |date=1959|volume=2|page=25–29|doi=10.4153/CMB-1959-006-x}} Corollary 2.</ref>.\n\nEvery [[prime ideal]] ''P'' in a Boolean ring ''R'' is [[maximal ideal|maximal]]: the [[quotient ring]] ''R''/''P'' is an [[integral domain]] and also a Boolean ring, so it is isomorphic to the [[field (mathematics)|field]] '''F'''<sub>2</sub>, which shows the maximality of ''P''. Since maximal ideals are always prime,  prime ideals and maximal ideals coincide in Boolean rings.\n\nBoolean rings are [[von Neumann regular ring]]s.\n\nBoolean rings are absolutely flat: this means that every module over them is [[flat module|flat]].\n\nEvery finitely generated ideal of a Boolean ring is [[principal ideal|principal]] (indeed, ''(x,y)=(x+y+xy)'').\n\n== Unification ==\n[[Unification (logic)|Unification]] in Boolean rings is [[Decidability (logic)|decidable]],<ref>{{cite book|author1=Martin, U. |author2=Nipkow, T. | chapter=Unification in Boolean Rings| title=Proc. 8th CADE| year=1986| volume=230| pages=506–513| publisher=Springer| editor=Jörg H. Siekmann| series=LNCS|url=https://link.springer.com/chapter/10.1007/3-540-16780-3_115}}</ref> that is, algorithms exist to solve arbitrary equations over Boolean rings. Both unification and matching in [[finitely generated algebra|finitely generated]] free Boolean rings are [[NP-complete]], and [[NP-hard]] in [[finitely presented algebra|finitely presented]] Boolean rings.<ref>Kandri-Rody, A., Kapur, D., and Narendran, P., \"[https://link.springer.com/chapter/10.1007/3-540-15976-2_17 An ideal-theoretic approach to word problems and unification problems over finitely presented commutative algebras]\", ''Proc. of the first Conference on Rewriting Techniques and Applications, Dijon, France, May 1985'', LNCS 202, Springer Verlag, 345-364.</ref> (In fact, as any unification problem ''f''(''X'') = ''g''(''X'') in a Boolean ring can be rewritten as the matching problem ''f''(''X'') + ''g''(''X'') = 0, the problems are equivalent.)\n\nUnification in Boolean rings is unitary if all the uninterpreted function symbols are nullary and finitary otherwise (i.e. if the function symbols not occurring in the signature of Boolean rings are all constants then there exists a [[most general unifier]], and otherwise the [[Unification (computer science)#Unification problem, solution set|minimal complete set of unifiers]] is finite).<ref>{{cite journal| author=A. Boudet| author2=J.-P. Jouannaud| author2-link=J.-P. Jouannaud| author3=M. Schmidt-Schauß| title=Unification of Boolean Rings and Abelian Groups| journal=[[Journal of Symbolic Computation]] | year=1989| volume=8| issue=5| pages=449–477 |url=http://www.sciencedirect.com/science/article/pii/S0747717189800549/pdf?md5=713ed362e4b6f2db53923cc5ed47c818&pid=1-s2.0-S0747717189800549-main.pdf| doi=10.1016/s0747-7171(89)80054-9}}</ref>\n\n== See also ==\n* [[Ring-sum normal form]]\n\n== Notes ==\n{{notelist}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*{{Citation | last1=Atiyah | first1=Michael Francis | authorlink1=Michael Atiyah | last2=Macdonald | first2=I. G. | authorlink2=Ian G. Macdonald | title=Introduction to Commutative Algebra | publisher=Westview Press | isbn=978-0-201-40751-8 | year=1969}}\n*{{citation | first = John B. | last = Fraleigh | year = 1976 | isbn = 978-0-201-01984-1 | title = A First Course In Abstract Algebra | edition = 2nd | publisher = [[Addison-Wesley]]  }}\n*{{citation | first = I. N. | last = Herstein | authorlink= Israel Nathan Herstein | year = 1975 | title = Topics In Algebra | edition= 2nd | publisher = [[John Wiley & Sons]] }}\n*{{citation | first=Neal H. |last=McCoy |year=1968 |title=Introduction To Modern Algebra | edition = Revised |publisher=[[Allyn and Bacon]] |lccn=68015225}}\n*{{springerEOM |id=Boolean_ring |oldid=18972 |first=Yu. M. |last=Ryabukhin}}\n\n== External links ==\n*John Armstrong, [http://unapologetic.wordpress.com/2010/08/04/boolean-rings Boolean Rings]\n\n[[Category:Ring theory]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean satisfiability algorithm heuristics",
      "url": "https://en.wikipedia.org/wiki/Boolean_satisfiability_algorithm_heuristics",
      "text": "{{Orphan|date=February 2016}}\nGiven a Boolean expression <Math>B</Math> with <Math>V = \\{v_0, \\ldots , v_n\\}</Math> variables, finding an assignment <Math>V^*</Math> of the variables such that <Math>B(V^*)</Math> is true is called the [[Boolean satisfiability problem]], frequently abbreviated SAT, and is seen as the canonical [[NP-complete]] problem.\n\nAlthough no known algorithm is known to solve SAT in polynomial time, there are classes of SAT problems which do have efficient algorithms that solve them. These classes of problems arise from many practical problems in [[AI planning]], [[Circuit satisfiability problem|circuit testing]], and software verification.<ref>Aloul, Fadi A., \"On Solving Optimization Problems Using Boolean Satisfiability\", American University of Sharjah (2005), http://www.aloul.net/Papers/faloul_icmsao05.pdf</ref><ref name = \"princeton\">Zhang, Lintao. Malik, Sharad. \"The Quest for Efficient Boolean Satisfiability Solvers\", Department of Electrical Engineering, Princeton University. https://www.princeton.edu/~chaff/publication/cade_cav_2002.pdf</ref> Research on constructing efficient SAT solvers has been based on various principles such as resolution, search, local search and random walk, binary decisions, and Stalmarck's algorithm.<ref name = \"princeton\"/>\n\nSome of these algorithms are [[Deterministic algorithm|deterministic]], while others may be [[Randomized algorithm|stochastic]].\n\nAs there exist polynomial-time algorithms to convert any Boolean expression to conjunctive normal form such as [[Tseytin transformation|Tseitin's algorithm]], posing SAT problems in CNF does not change their computational difficulty. SAT problems are canonically expressed in CNF because CNF has certain properties that can help prune the search space and speed up the search process.<ref name = \"princeton\"/>\n\n== Branching heuristics in conflict-driven algorithms <ref name = \"princeton\"/> ==\n\nOne of the cornerstone [[Conflict-Driven Clause Learning]] SAT solver algorithms is the [[DPLL algorithm]]. The algorithm works by iteratively assigning free variables, and when the algorithm encounters a bad assignment, then it backtracks to a previous iteration and chooses a different assignment of variables. It relies on a Branching Heuristic to pick the next free variable assignment; the branching algorithm effectively makes choosing the variable assignment into a decision tree. Different implementations of this heuristic produce markedly different decision trees, and thus have significant effect on the efficiency of the solver.\n\nEarly branching Heuristics (Bohm's Heuristic, Maximum Occurrences on Minimum sized clauses heuristic, and Jeroslow-Wang heuristic) can be regarded as [[greedy algorithms]]. Their basic premise is to choose a free variable assignment that will satisfy the most already unsatisfied clauses in the Boolean expression. However, as Boolean expressions get larger, more complicated, or more structured, these heuristics fail to capture useful information about these problems that could improve efficiency; they often get stuck in local maxima or do not consider the distribution of variables. Additionally, larger problems require more processing, as the operation of counting free variables in unsatisfied clauses dominates the run-time.\n\nAnother heuristic called Variable State Independent Decaying Sum (VSIDS) attempts to score each variable. VSIDS starts by looking at small portions of the Boolean expression and assigning each phase of a variable (a variable and its negated complement) a score proportional to the number of clauses that variable phase is in. As VSIDS progresses and searches more parts of the Boolean expression, periodically, all scores are divided by a constant. This discounts the effect of the presence of variables in earlier-found clauses in favor of variables with a greater presence in more recent clauses. VSIDS will select the variable phase with the highest score to determine where to branch.\n\nVSIDS is quite effective because the scores of variable phases is independent of the current variable assignment, so backtracking is much easier. Further, VSIDS guarantees that each variable assignment satisfies the greatest number of recently searched segments of the Boolean expression.\n\n== Stochastic solvers <ref>Sung, Phil. \"Maximum Satisfiability\" (2006) http://math.mit.edu/~goemans/18434S06/max-sat-phil.pdf</ref> ==\n\n[[MAX-SAT]] (the version of SAT in which the number of satisfied clauses is maximized) solvers can also be solved using probabilistic algorithms. If we{{who?|date=November 2018}} are given a Boolean expression <Math>B</Math>, with <Math>V = \\{v_0, \\ldots , v_n\\}</Math> variables and we{{who?|date=November 2018}} set each variable randomly, then each clause <Math>c</Math>, with <Math>|c|</Math> variables, has a chance of being satisfied by a particular variable assignment Pr(<Math>c</Math> is satisfied) = <Math>1-2^{-|c|}</Math>. This is because each variable in <Math>c</Math> has <Math>\\frac{1}{2}</Math> probability of being satisfied, and we{{who?|date=November 2018}} only need one variable in <Math>c</Math> to be satisfied. This works <Math>\\forall ~|c| \\geq 1</Math>, so Pr(<Math>c</Math> is satisfied) = <Math>1-2^{-|c|} \\geq \\frac{1}{2}</Math>.\n\nNow we{{who?|date=November 2018}} show that randomly assigning variable values is a <Math>\\frac{1}{2}</Math>-approximation algorithm, which means that is an optimal approximation algorithm unless P = NP. Suppose we{{who?|date=November 2018}} are given a Boolean expression <Math>B = \\{c_i\\}_{i=1}^n</Math> and\n\n: <math>\\delta_{ij} = \\begin{cases}\n0 &\\text{if } c_i \\text{ is satisfied},   \\\\\n1 &\\text{if } c_i \\text{ is not satisfied}. \\end{cases}</math>\n\n: <Math>\n\\begin{align}\nE[\\text{Num Clauses Satsified}] & = \\sum_i E[\\delta_i] = \\sum_{i} 1-2^{-|c_i|} \\\\\n& \\geq \\sum_i \\frac{1}{2} = \\frac{1}{2}|i| = \\frac{1}{2} OPT\n\\end{align}\n</Math>\n\nThis algorithm cannot be further optimized by the [[PCP theorem]] unless P = NP.\n\nOther Stochastic SAT solvers, such as [[WalkSAT]] and [[GSAT]] are an improvement to the above procedure. They start by randomly assigning values to each variable and then traverse the given Boolean expression to identify which variables to flip to minimize the number of unsatisfied clauses. They may randomly select a variable to flip or select a new random variable assignment to escape local maxima, much like a [[simulated annealing]] algorithm.\n\n== 2-SAT heuristics ==\n\nUnlike general SAT problems, [[2-SAT]] problems are [[Tractable problem|tractable]]. There exist algorithms that can compute the satisfiability of a 2-SAT problem in polynomial time. This is a result of the constraint that each clause has only two variables, so when an algorithm sets a variable <Math>v_i</Math>, the satisfaction of clauses, which contain <Math>v_i</Math> but are not satisfied by that variable assignment, depend on the satisfaction of the second variable in those clauses, which leaves only one possible assignment for those variables.\n\n=== Backtracking ===\n\nSuppose we{{who?|date=November 2018}} are given a Boolean expressions:\n\n: <Math>B_1 = (v_3 \\lor \\neg v_2) \\wedge (\\neg v_1 \\lor \\neg v_3)</Math>\n\n: <Math>B_2 = (v_3 \\lor \\neg v_2) \\wedge (\\neg v_1 \\lor \\neg v_3) \\wedge (\\neg v_1 \\lor v_2). </Math>\n\nWith <Math>B_1</Math>, the algorithm can select <Math>v_1 = \\text{true}</Math>, so to satisfy the second clause, the algorithm will need to set <Math>v_3 = \\text{false}</Math>, and resultantly to satisfy the first clause, the algorithm will set <Math>v_2 = \\text{false}</Math>.\n\nIf the algorithm tries to satisfy <Math>B_2</Math> in the same way it tried to solve <Math>B_1</Math>, then the third clause will remain unsatisfied. This will cause the algorithm to backtrack and set <Math>v_1 = \\text{false}</Math> and continue assigning variables further.\n\n=== Graph reduction <ref>Griffith, Richard. \"Strongly Connected Components and the 2-SAT Problem in Dart\". http://www.greatandlittle.com/studios/index.php?post/2013/03/26/Strongly-Connected-Components-and-the-2-SAT-Problem-in-Dart</ref> ===\n\n2-SAT problems can also be reduced to running a [[depth-first search]] on a [[strongly connected components]] of a graph. Each variable phase (a variable and its negated complement) is connected to other variable phases based on implications. In the same way when the algorithm above tried to solve\n\n: <Math>B_1, v_1 = \\text{true} \\implies v_3 = \\text{false} \\implies v_2 = \\text{false} \\implies v_1 = \\text{true}.</Math>\n\nHowever, when the algorithm tried solve\n\n: <Math>\n\\begin{align}\nB_2, v_1 = \\text{true} & \\implies v_3 = \\text{false} \\implies v_2 = \\text{false} \\\\ & \\implies v_1 = \\text{false} \\implies \\cdots \\implies v_1 = \\text{true},\n\\end{align}\n</Math>\n\nwhich is a contradiction.\n\nOnce a 2-SAT problem is reduced to a graph, then if a depth first search finds a strongly connected component with both phases of a variable, then the 2-SAT problem is not satisfiable. Likewise, if the depth first search does not find a strongly connected component with both phases of a variable, then the 2-SAT problem is satisfiable.\n\n== Weighted SAT problems ==\nNumerous weighted SAT problems exist as the [[Optimization problem|optimization versions]] of the general SAT problem. In this class of problems, each clause in a CNF Boolean expression is given a weight. The objective is the maximize or minimize the total sum of the weights of the satisfied clauses given a Boolean expression. weighted Max-SAT is the maximization version of this problem, and [[Maximum satisfiability problem|Max-SAT]] is an instance of weighted MAX-SAT problem where the weights of each clause are the same. The partial Max-SAT problem is the problem where some clauses necessarily must be satisfied (hard clauses) and the sum total of weights of the rest of the clauses (soft clauses) are to be maximized or minimized, depending on the problem. Partial Max-SAT represents an intermediary between Max-SAT (all clauses are soft) and SAT (all clauses are hard).\n\nNote that the stochastic probabilistic solvers can also be used to find optimal approximations for Max-SAT.\n\n=== Variable splitting<ref>Pipatsrisawat, Knot. Palyan, Akop. et. al. \"Solving Weighted Max-SAT Problems in a Reduced Search Space: A Performance Analysis\". University of California Computer Science Department http://reasoning.cs.ucla.edu/fetch.php?id=86&type=pdf</ref> ===\nVariable splitting is a tool to find upper and lower bounds on a Max-SAT problem. It involves splitting a variable <Math>a</Math> into new variables for all but once occurrence of <Math>a</Math> in the original Boolean expression. For example, given the Boolean expression:\n<Math>B = (a \\lor b \\lor c) \\wedge (\\neg a \\lor e \\lor \\neg b) \\wedge (a \\lor \\neg c \\lor f)</Math>\nwill become:\n<Math>B^* = (a \\lor b \\lor c) \\wedge (\\neg a_1 \\lor e \\lor \\neg b) \\wedge (a_2 \\lor \\neg c \\lor f)</Math>,\nwith <Math>a,a_1,a_2,\\ldots,a_n</Math> being all distinct variables.\n\nThis relaxes the problem by introducing new variables into the Boolean expression, which has the effect of removing many of the constraints in the expression. Because any assignment of variables in <Math>B</Math> can be represented by an assignment of variables in <Math>B^*</Math>, the minimization and maximization of the weights of <Math>B^*</Math> represent lower and upper bounds on the minimization and maximization of the weights of <Math>B</Math>.\n\n=== Partial Max-SAT ===\nPartial Max-SAT can be solved by first considering all of the hard clauses and solving them as an instance of SAT. The total maximum (or minimum) weight of the soft clauses can be evaluated given the variable assignment necessary to satisfy the hard clauses and trying to optimize the free variables (the variables that the satisfaction of the hard clauses does not depend on). The latter step is an implementation of Max-SAT given some pre-defined variables. Of course, different variable assignments that satisfy the hard clauses might have different optimal free variable assignments, so it is necessary to check different hard clause satisfaction variable assignments.\n\n== Data structures for storing clauses <ref name = \"princeton\"/> ==\n\nAs SAT solvers and practical SAT problems (e.g. circuit verification) get more advanced, the Boolean expressions of interest may exceed millions of variables with several million clauses; therefore, efficient data structures to store and evaluate the clauses must be used.\n\nExpressions can be stored as a list of clauses, where each clause is a list of variables, much like an [[adjacency list]]. Though these data structures are convenient for manipulation (adding elements, deleting elements, etc.), they rely on many pointers, which increases their memory overhead, decreases [[cache locality]], and increases [[cache misses]], which renders them impractical for problems with large clause counts and large clause sizes.\n\nWhen clause sizes are large, more efficient analogous implementations include storing expressions as a list of clauses, where each clause is represented as a matrix that represents the clauses and the variables present in that clause, much like an [[adjacency matrix]]. The elimination of pointers and the contiguous memory occupation of arrays serve to decrease memory usage and increase cache locality and cache hits, which offers a run-time speed up compared to the aforesaid implementation.\n\n== References ==\n{{reflist}}\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean satisfiability problem",
      "url": "https://en.wikipedia.org/wiki/Boolean_satisfiability_problem",
      "text": "{{short description|Problem of determining if a Boolean formula could be made true}}\n{{Redirect|3SAT|the Central European television network|3sat}}\n\nIn [[computer science]], the '''Boolean satisfiability problem''' (sometimes called '''propositional satisfiability problem''' and abbreviated '''SATISFIABILITY''' or '''SAT''') is the problem of determining if there exists an [[Interpretation (logic)|interpretation]] that [[Satisfiability|satisfies]] a given [[Boolean logic|Boolean]] [[Formula (mathematical logic)|formula]]. In other words, it asks whether the variables of a given Boolean formula can be consistently replaced by the values TRUE or FALSE in such a way that the formula [[Validity (logic)|evaluates to TRUE]]. If this is the case, the formula is called ''satisfiable''. On the other hand, if no such assignment exists, the function expressed by the formula is [[Contradiction#Contradiction in formal logic|FALSE]] for all possible variable assignments and the formula is ''unsatisfiable''. For example, the formula \"''a'' AND NOT ''b''\" is satisfiable because one can find the values ''a''&nbsp;=&nbsp;TRUE and ''b''&nbsp;=&nbsp;FALSE, which make (''a'' AND NOT ''b'')&nbsp;=&nbsp;TRUE. In contrast, \"''a'' AND NOT ''a''\" is unsatisfiable.\n\nSAT is the first problem that was proven to be [[NP-complete]]; see [[Cook–Levin theorem]]. This means that all problems in the complexity class [[NP (complexity)|NP]], which includes a wide range of natural decision and optimization problems, are at most as difficult to solve as SAT. There is no known algorithm that efficiently solves each SAT problem, and it is generally believed that no such algorithm exists; yet this belief has not been proven mathematically, and resolving the question of whether SAT has a [[polynomial-time]] algorithm is equivalent to the [[P versus NP problem]], which is a famous open problem in the theory of computing.\n\nNevertheless, as of 2007, heuristic SAT-algorithms are able to solve problem instances involving tens of thousands of variables and <!---\"clauses\" hasn't been introduced here:--->formulas consisting of millions of symbols,<ref name=\"Codish.Ohrimenko.Stuckey.2007\"/> which is sufficient for many practical SAT problems from, e.g., [[artificial intelligence]], [[circuit design]], and [[automatic theorem proving]].\n\n==Basic definitions and terminology==\nA ''[[propositional logic]] formula'', also called ''Boolean expression'', is built from [[Variable (mathematics)|variables]], operators AND ([[Logical conjunction|conjunction]], also denoted by ∧), OR ([[logical disjunction|disjunction]], ∨), NOT ([[negation]], ¬), and parentheses.\nA formula is said to be ''satisfiable'' if it can be made TRUE by assigning appropriate [[logical value]]s (i.e. TRUE, FALSE) to its variables.\nThe ''Boolean satisfiability problem'' (SAT) is, given a formula, to check whether it is satisfiable.\nThis [[decision problem]] is of central importance in many areas of [[computer science]], including [[theoretical computer science]], [[computational complexity theory|complexity theory]], [[algorithmics]], [[cryptography]] and [[artificial intelligence]].\n\nThere are several special cases of the Boolean satisfiability problem in which the formulas are required to have a particular structure. A ''literal'' is either a variable, called ''positive literal'', or the negation of a variable, called ''negative literal''.\nA ''clause'' is a disjunction of literals (or a single literal). A clause is called a ''[[Horn clause]]'' if it contains at most one positive literal.\nA formula is in ''[[conjunctive normal form]]'' (CNF) if it is a conjunction of clauses (or a single clause).\nFor example, {{math|size=100%|''x''<sub>1</sub>}} is a positive literal, {{math|size=100%|¬''x''<sub>2</sub>}} is a negative literal, {{math|size=100%|''x''<sub>1</sub> ∨ ¬''x''<sub>2</sub>}} is a clause, and {{math|size=100%|(''x''<sub>1</sub> ∨ ¬''x''<sub>2</sub>) ∧ (¬''x''<sub>1</sub> ∨ ''x''<sub>2</sub> ∨ ''x''<sub>3</sub>) ∧ ¬''x''<sub>1</sub>}} is a formula in conjunctive normal form; its first and third clauses are Horn clauses, but its second clause is not. The formula is satisfiable, by choosing ''x''<sub>1</sub>&nbsp;=&nbsp;FALSE, ''x''<sub>2</sub>&nbsp;=&nbsp;FALSE, and ''x''<sub>3</sub> arbitrarily, since (FALSE ∨ ¬FALSE) ∧ (¬FALSE ∨ FALSE ∨ ''x''<sub>3</sub>) ∧ ¬FALSE evaluates to (FALSE ∨ TRUE) ∧ (TRUE ∨ FALSE ∨ ''x''<sub>3</sub>) ∧ TRUE, and in turn to TRUE ∧ TRUE ∧ TRUE (i.e. to TRUE). In contrast, the CNF formula ''a'' ∧ ¬''a'', consisting of two clauses of one literal, is unsatisfiable, since for ''a''=TRUE or ''a''=FALSE it evaluates to TRUE ∧ ¬TRUE (i.e., FALSE) or FALSE ∧ ¬FALSE (i.e., again FALSE), respectively.\n\nFor some versions of the SAT problem,<!---need not list them in detail here---(viz. [[#Exactly-1 3-satisfiability|Exactly-1 3-satisfiability]], [[#XOR-satisfiability|XOR-satisfiability]], and, more general, [[#Schaefer's dichotomy theorem|Schaefer's dichotomy theorem]], discussed below),---> it is useful to define the notion of a ''generalized conjunctive normal form'' formula, viz. as a conjunction of arbitrarily many ''generalized clauses'', the latter being of the form {{math|''R''(''l''<sub>1</sub>,...,''l''<sub>''n''</sub>)}} for some boolean operator ''R'' and (ordinary) literals {{mvar|''l''<sub>''i''</sub>}}. Different sets of allowed boolean operators lead to different problem versions.<!---, see [[#Complexity and restricted versions|below]].---> As an example, ''R''(¬''x'',''a'',''b'') is a generalized clause, and ''R''(¬''x'',''a'',''b'') ∧ ''R''(''b'',''y'',''c'') ∧ ''R''(''c'',''d'',¬''z'') is a generalized conjunctive normal form. This formula is used [[#Exactly-1 3-satisfiability|below]], with ''R'' being the ternary operator that is TRUE just when exactly one of its arguments is.\n<!---need not explain that already here---If ''R'' is the ternary operator that is TRUE just if exactly one of its arguments is, then a satisfying assignment for the latter formula can be found starting from every possible combination of truth values for ''x'', ''y'', ''z'', except ''x''&nbsp;=&nbsp;''y''&nbsp;=&nbsp;''z''&nbsp;=&nbsp;FALSE, and choosing the values of ''a'', ''b'', ''c'', ''d'' appropriately; cf. the left table under [[#Exactly-1 3-satisfiability|Exactly-1 3-satisfiability]] below.--->\n\nUsing the laws of [[Boolean algebra (structure)|Boolean algebra]], every propositional logic formula can be transformed into an equivalent conjunctive normal form, which may, however, be exponentially longer. For example, transforming the formula\n(''x''<sub>1</sub>∧''y''<sub>1</sub>) ∨ (''x''<sub>2</sub>∧''y''<sub>2</sub>) ∨ ... ∨ (''x''<sub>''n''</sub>∧''y''<sub>''n''</sub>)\ninto conjunctive normal form yields\n:{{math|(''x''<sub>1</sub>&nbsp;∨&nbsp;''x''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''x''<sub>''n''</sub>) ∧}}\n:{{math|(''y''<sub>1</sub>&nbsp;∨&nbsp;''x''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''x''<sub>''n''</sub>) ∧}}\n:{{math|(''x''<sub>1</sub>&nbsp;∨&nbsp;''y''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''x''<sub>''n''</sub>) ∧}}\n:{{math|(''y''<sub>1</sub>&nbsp;∨&nbsp;''y''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''x''<sub>''n''</sub>) ∧ ... ∧}}\n:{{math|(''x''<sub>1</sub>&nbsp;∨&nbsp;''x''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''y''<sub>''n''</sub>) ∧}}\n:{{math|(''y''<sub>1</sub>&nbsp;∨&nbsp;''x''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''y''<sub>''n''</sub>) ∧}}\n:{{math|(''x''<sub>1</sub>&nbsp;∨&nbsp;''y''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''y''<sub>''n''</sub>) ∧}}\n:{{math|(''y''<sub>1</sub>&nbsp;∨&nbsp;''y''<sub>2</sub>&nbsp;∨&nbsp;…&nbsp;∨&nbsp;''y''<sub>''n''</sub>)}};\nwhile the former is a disjunction of ''n'' conjunctions of 2 variables, the latter consists of 2<sup>''n''</sup> clauses of ''n'' variables.\n\n==Complexity and restricted versions==\n\n===Unrestricted satisfiability (SAT)===\n{{Main article|Cook–Levin theorem}}\n\nSAT was the first known [[NP-complete]] problem, as proved by [[Stephen Cook]] at the [[University of Toronto]] in 1971<ref>{{Cite journal | last1 = Cook | first1 = Stephen A. | authorlink1=Stephen Cook| title = The Complexity of Theorem-Proving Procedures | pages = 151–158 | year = 1971 | url=http://www.cs.toronto.edu/~sacook/homepage/1971.pdf| doi = 10.1145/800157.805047| journal = Proceedings of the 3rd Annual ACM [[Symposium on Theory of Computing]]}}</ref> and independently by [[Leonid Levin]] at the [[Russian Academy of Sciences#The Academy of Sciences of the USSR|National Academy of Sciences]] in 1973.<ref>{{cite journal|last=Levin|first=Leonid|authorlink=Leonid Levin|title = Universal search problems (Russian: Универсальные задачи перебора, Universal'nye perebornye zadachi)|journal = Problems of Information Transmission (Russian: Проблемы передачи информа́ции, Problemy Peredachi Informatsii)|volume = 9|issue = 3|pages = 115–116|year = 1973}} [http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&paperid=914&volume=9&year=1973&issue=3&fpage=115&what=fullt&option_lang=eng (pdf)] {{ru icon}}, translated into English by {{cite journal|last=Trakhtenbrot|first=B. A.|title = A survey of Russian approaches to ''perebor'' (brute-force searches) algorithms|journal = Annals of the History of Computing |volume = 6|issue = 4|pages = 384–400|year = 1984|doi=10.1109/MAHC.1984.10036}}</ref> Until that time, the concept of an NP-complete problem did not even exist.\nThe proof shows how every decision problem in the [[complexity class]] [[NP (complexity)|NP]] can be [[reduction (complexity)|reduced]] to the SAT problem for CNF<ref group=note>The SAT problem for ''arbitrary'' formulas is NP-complete, too, since it is easily shown to be in NP, and it cannot be easier than SAT for CNF formulas.</ref> formulas, sometimes called '''CNFSAT'''.\nA useful property of Cook's reduction is that it preserves the number of accepting answers. For example, deciding whether a given [[Graph (discrete mathematics)|graph]] has a [[Graph coloring#Vertex coloring|3-coloring]] is another problem in NP; if a graph has 17 valid 3-colorings, the SAT formula produced by the Cook–Levin reduction will have 17 satisfying assignments.\n\nNP-completeness only refers to the run-time of the worst case instances.  Many of the instances that occur in practical applications can be solved much more quickly.  See [[#Algorithms for solving SAT|Algorithms for solving SAT]] below.\n\nSAT is trivial if the formulas are restricted to those in '''[[disjunctive normal form]]''', that is, they are disjunction of conjunctions of literals. Such a formula is indeed satisfiable if and only if at least one of its conjunctions is satisfiable, and a conjunction is satisfiable if and only if it does not contain both ''x'' and NOT ''x'' for some variable ''x''. This can be checked in linear time. Furthermore, if they are restricted to being in '''full disjunctive normal form''', in which every variable appears exactly once in every conjunction, they can be checked in constant time (each conjunction represents one satisfying assignment).  But it can take exponential time and space to convert a general SAT problem to disjunctive normal form; for an example exchange \"∧\" and \"∨\" in the [[#Basic definitions and terminology|above]] exponential blow-up example for conjunctive normal forms.\n\n===3-satisfiability===\n[[File:Sat reduced to Clique from Sipser.svg|thumb|The 3-SAT instance (''x''∨''x''∨''y'') ∧ (¬''x''∨¬''y''∨¬''y'') ∧ (¬''x''∨''y''∨''y'') reduced to a [[clique problem]]. The green vertices form a 3-clique and correspond to the satisfying assignment ''x''=FALSE, ''y''=TRUE.]]\nLike the satisfiability problem for arbitrary formulas, determining the satisfiability of a formula in conjunctive normal form where each clause is limited to at most three literals is NP-complete also; this problem is called '''3-SAT''', '''3CNFSAT''', or '''3-satisfiability'''.\nTo reduce the unrestricted SAT problem to 3-SAT, transform each clause {{math|''l''<sub>1</sub> ∨ ⋯ ∨ ''l''<sub>''n''</sub>}} to a conjunction of ''n'' − 2 clauses\n:{{math|(''l''<sub>1</sub> ∨ ''l''<sub>2</sub> ∨ ''x''<sub>2</sub>) ∧ }}\n:{{math|(¬''x''<sub>2</sub> ∨ ''l''<sub>3</sub> ∨ ''x''<sub>3</sub>) ∧ }}\n:{{math|(¬''x''<sub>3</sub> ∨ ''l''<sub>4</sub> ∨ ''x''<sub>4</sub>) ∧ ⋯ ∧ }}\n:{{math|(¬''x''<sub>''n'' − 3</sub> ∨ ''l''<sub>''n'' − 2</sub> ∨ ''x''<sub>''n'' − 2</sub>) ∧ }}\n:{{math|(¬''x''<sub>''n'' − 2</sub> ∨ ''l''<sub>''n'' − 1</sub> ∨ ''l''<sub>''n''</sub>)}}\nwhere {{math|''x''<sub>2</sub>,&thinsp;⋯&thinsp;,&thinsp;''x''<sub>''n'' − 2</sub>}} are fresh variables not occurring elsewhere.\nAlthough the two formulas are not [[logically equivalent]], they are [[equisatisfiable]]. The formula resulting from transforming all clauses is at most 3 times as long as its original, i.e. the length growth is polynomial.<ref name=\"Aho.Hopcroft.Ullman.1974\">{{cite book|author1=Alfred V. Aho |author2=John E. Hopcroft |author3=Jeffrey D. Ullman | title=The Design and Analysis of Computer Algorithms| year=1974| publisher=Addison-Wesley}}; here: Thm.10.4</ref>\n\n3-SAT is one of [[Karp's 21 NP-complete problems]], and it is used as a starting point for proving that other problems are also [[NP-hard]].<ref group=note>i.e. at least as hard as every other problem in NP. A decision problem is NP-complete if and only if it is in NP and is NP-hard.</ref> This is done by [[polynomial-time reduction]] from 3-SAT to the other problem. An example of a problem where this method has been used is the [[clique problem]]: given a CNF formula consisting of ''c'' clauses, the corresponding [[Graph (discrete mathematics)|graph]] consists of a vertex for each literal, and an edge between each two non-contradicting<ref group=note>i.e. such that one literal is not the negation of the other</ref> literals from different clauses, cf. picture. The graph has a ''c''-clique if and only if the formula is satisfiable.{{refn|Aho, Hopcroft, Ullman<ref name=\"Aho.Hopcroft.Ullman.1974\"/> (1974); Thm.10.5}}\n\nThere is a simple randomized algorithm due to Schöning (1999) that runs in time (4/3)<sup>''n''</sup> where ''n'' is the number of variables in the 3-SAT proposition, and succeeds with high probability to correctly decide 3-SAT.<ref name=\"Schoning.1999\">{{cite book | last1 = Schöning | first1 = Uwe| chapter = A Probabilistic Algorithm for ''k''-SAT and Constraint Satisfaction Problems | title = Proc. 40th Ann. Symp. Foundations of Computer Science| pages = 410–414 | date=Oct 1999 | url=http://homepages.cwi.nl/~rdewolf/schoning99.pdf| doi = 10.1109/SFFCS.1999.814612}}</ref>\n\nThe [[exponential time hypothesis]] asserts that no algorithm can solve 3-SAT (or indeed ''k''-SAT for any ''k > 2'') in {{math|exp([[Small o notation#Little-o notation|''o'']](''n''))}} time (i.e., fundamentally faster than exponential in ''n'').\n\nSelman, Mitchell, and Levesque (1996) give empirical data on the difficulty of randomly generated 3-SAT formulas, depending on their size parameters.\nDifficulty is measured in number recursive calls made by a [[#Algorithms for solving SAT|DPLL algorithm]].<ref>{{cite journal|author1=Bart Selman |author2=David Mitchell |author3=Hector Levesque | title=Generating Hard Satisfiability Problems| journal=Artificial Intelligence| year=1996| volume=81| pages=17–29| url=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=3CBEAB7E11BF4B2283E9F383810060C1?doi=10.1.1.37.7362&rep=rep1&type=pdf| doi=10.1016/0004-3702(95)00045-3}}</ref>\n\n3-satisfiability can be generalized to '''k-satisfiability''' ('''k-SAT''', also '''k-CNF-SAT'''), when formulas in CNF are considered with each clause containing up to ''k'' literals.\nHowever, since for any ''k''≥3, this problem can neither be easier than 3-SAT nor harder than SAT, and the latter two are NP-complete, so must be k-SAT.\n\nSome authors restrict k-SAT to CNF formulas with '''exactly k literals'''. This doesn't lead to a different complexity class either, as each clause {{math|''l''<sub>1</sub> ∨ ⋯ ∨ ''l''<sub>''j''</sub>}} with ''j''<''k'' literals can be padded with fixed dummy variables to\n{{math|''l''<sub>1</sub> ∨ ⋯ ∨ ''l''<sub>''j''</sub> ∨ ''d''<sub>''j''+1</sub> ∨ ⋯ ∨ ''d''<sub>''k''</sub>}}.\nAfter padding all clauses, 2<sup>''k''</sup>-1 extra clauses<ref group=note>viz. all [[Canonical form (Boolean algebra)#Maxterms|maxterms]] that can be built with {{math|''d''<sub>1</sub>,⋯,''d''<sub>''k''</sub>}}, except {{math|''d''<sub>1</sub>∨⋯∨''d''<sub>''k''</sub>}}</ref> have to be appended to ensure that only {{math|1=''d''<sub>1</sub>=⋯=''d''<sub>''k''</sub>=FALSE}} can lead to a satisfying assignment. Since ''k'' doesn't depend on the formula length, the extra clauses lead to a constant increase in length. For the same reason, it does not matter whether '''duplicate literals''' are allowed in clauses (like e.g. {{math|¬''x'' ∨ ¬''y'' ∨ ¬''y''}}), or not.\n\n===Exactly-1 3-satisfiability===\n[[File:Schaefer's 3-SAT to 1-in-3-SAT reduction.gif|thumb|x100px|'''Left:''' Schaefer's reduction of a 3-SAT clause ''x''∨''y''∨''z''. The result of ''R'' is {{fontcolor|#00a000|TRUE (1)}} if exactly one of its arguments is TRUE, and {{fontcolor|#a00000|FALSE (0)}} otherwise. All 8 combinations of values for ''x'',''y'',''z'' are examined, one per line. The fresh variables ''a'',...,''f'' can be chosen to satisfy all clauses (exactly one {{fontcolor|#00a000|green}} argument for each ''R'') in all lines except the first, where ''x''∨''y''∨''z'' is FALSE. '''Right:''' A simpler reduction with the same properties.]]\nA variant of the 3-satisfiability problem is the '''one-in-three 3-SAT''' (also known variously as '''1-in-3-SAT''' and '''exactly-1 3-SAT''').\nGiven a conjunctive normal form with three literals per clause, the problem is to determine whether there exists a truth assignment to the variables so that each clause has ''exactly'' one TRUE literal (and thus exactly two FALSE literals). In contrast, ordinary 3-SAT requires that every clause has ''at least'' one TRUE literal.\nFormally, a one-in-three 3-SAT problem is given as a generalized conjunctive normal form with all generalized clauses using a ternary operator ''R'' that is TRUE just if exactly one of its arguments is. When all literals of a one-in-three 3-SAT formula are positive, the satisfiability problem is called '''one-in-three positive 3-SAT'''.\n\nOne-in-three 3-SAT, together with its positive case, is listed as NP-complete problem \"LO4\" in the standard reference, ''Computers and Intractability: A Guide to the Theory of NP-Completeness''\nby [[Michael R. Garey]] and [[David S. Johnson]].  One-in-three 3-SAT was proved to be NP-complete by [[Thomas Jerome Schaefer]] as a special case of [[Schaefer's dichotomy theorem]], which asserts that any problem generalizing Boolean satisfiability in a certain way is either in the class P or is NP-complete.<ref name=\"schaefer\">{{Cite conference | last1 = Schaefer | first1 = Thomas J. | last2 = | first2 =  | year = 1978 | title = The complexity of satisfiability problems | booktitle = Proceedings of the 10th Annual ACM Symposium on Theory of Computing | place = San Diego, California | journal =  | volume =  | issue =  | pages = 216–226 | publisher =  | jstor =  | url = http://www.ccs.neu.edu/home/lieber/courses/csg260/f06/materials/papers/max-sat/p216-schaefer.pdf | accessdate = }}</ref>\n\nSchaefer gives a construction allowing an easy polynomial-time reduction from 3-SAT to one-in-three 3-SAT.  Let \"(''x'' or ''y'' or ''z'')\" be a clause in a 3CNF formula.  Add six fresh boolean variables ''a'', ''b'', ''c'', ''d'', ''e'', and ''f'', to be used to simulate this clause and no other.\n<!---now introduced already above---Let ''R''(''u'',''v'',''w'') be a predicate that is TRUE if and only if exactly one of the booleans ''u'', ''v'', and ''w''\nis TRUE.--->\nThen the formula ''R''(''x'',''a'',''d'') ∧ ''R''(''y'',''b'',''d'') ∧ ''R''(''a'',''b'',''e'') ∧ ''R''(''c'',''d'',''f'') ∧ ''R''(''z'',''c'',FALSE) is satisfiable by some setting of the fresh variables if and only if at least one of ''x'', ''y'', or ''z'' is TRUE, see picture (left).  Thus any 3-SAT instance with ''m'' clauses and ''n'' variables may be converted into an [[equisatisfiable]] one-in-three 3-SAT instance with 5''m'' clauses and ''n''+6''m'' variables.<ref>(Schaefer, 1978), p.222, Lemma 3.5</ref>\nAnother reduction involves only four fresh variables and three clauses: ''R''(¬''x'',''a'',''b'') ∧ ''R''(''b'',''y'',''c'') ∧ R(''c'',''d'',¬''z''), see picture (right).\n\n===Not-all-equal 3-satisfiability===\n{{main|Not-all-equal 3-satisfiability}}\nAnother variant is the '''not-all-equal 3-satisfiability''' problem (also called '''NAE3SAT''').\nGiven a conjunctive normal form with three literals per clause, the problem is to determine if an assignment to the variables exists such that in no clause all three literals have the same truth value. This problem is NP-complete, too, even if no negation symbols are admitted, by Schaefer's dichotomy theorem.<ref name=\"schaefer\"/>\n\n===2-satisfiability===\n{{Main article|2-satisfiability}}\n\nSAT is easier if the number of literals in a clause is limited to at most 2, in which case the problem is called '''[[2-SAT]]'''. This problem can be solved in polynomial time, and in fact is [[NL-complete|complete]] for the complexity class [[NL (complexity)|NL]]. If additionally all OR operations in literals are changed to [[Exclusive or|XOR]] operations, the result is called '''exclusive-or 2-satisfiability''', which is a problem complete for the complexity class [[SL (complexity)|SL]] = [[L (complexity)|L]].\n\n===Horn-satisfiability===\n{{Main article|Horn-satisfiability}}\n\nThe problem of deciding the satisfiability of a given conjunction of Horn clauses is called '''Horn-satisfiability''', or '''HORN-SAT'''.\nIt can be solved in polynomial time by a single step of the [[Unit propagation]] algorithm, which produces the single minimal model of the set of Horn clauses (w.r.t. the set of literals assigned to TRUE).\nHorn-satisfiability is [[P-complete]]. It can be seen as [[P (complexity)|P's]] version of the Boolean satisfiability problem.\nAlso, deciding the truth of quantified Horn formulas can be done in polynomial time.\n<ref name=\"buningkarpinski\">{{Cite journal | last1 = Buning | first1 = H.K. | last2 = Karpinski| first2 =  Marek| last3=Flogel|first3=A.|year = 1995 | title = Resolution for Quantified Boolean Formulas | place =  | journal = Information and Computation | volume = 117 | issue = 1 | pages = 12–18 | publisher = Elsevier | jstor =  | doi= 10.1006/inco.1995.1025| accessdate = }}</ref>\n\nHorn clauses are of interest because they are able to express [[Entailment|implication]] of one variable from a set of other variables. Indeed, one such clause ¬''x''<sub>1</sub> ∨ ... ∨ ¬''x''<sub>''n''</sub> ∨ ''y'' can be rewritten as ''x''<sub>1</sub> ∧ ... ∧ ''x''<sub>''n''</sub> → ''y'', that is, if ''x''<sub>1</sub>,...,''x''<sub>''n''</sub> are all TRUE, then ''y'' needs to be TRUE as well.\n\nA generalization of the class of Horn formulae is that of renameable-Horn formulae, which is the set of formulae that can be placed in Horn form by replacing some variables with their respective negation.\nFor example, (''x''<sub>1</sub> ∨ ¬''x''<sub>2</sub>) ∧ (¬''x''<sub>1</sub> ∨ ''x''<sub>2</sub> ∨ ''x''<sub>3</sub>) ∧ ¬''x''<sub>1</sub> is not a Horn formula, but can be renamed to the Horn formula (''x''<sub>1</sub> ∨ ¬''x''<sub>2</sub>) ∧ (¬''x''<sub>1</sub> ∨ ''x''<sub>2</sub> ∨ ¬''y''<sub>3</sub>) ∧ ¬''x''<sub>1</sub> by introducing ''y''<sub>3</sub> as negation of ''x''<sub>3</sub>.\nIn contrast, no renaming of (''x''<sub>1</sub> ∨ ¬''x''<sub>2</sub> ∨ ¬''x''<sub>3</sub>) ∧ (¬''x''<sub>1</sub> ∨ ''x''<sub>2</sub> ∨ ''x''<sub>3</sub>) ∧ ¬''x''<sub>1</sub> leads to a Horn formula.\nChecking the existence of such a replacement can be done in linear time; therefore, the satisfiability of such formulae is in P as it can be solved by first performing this replacement and then checking the satisfiability of the resulting Horn formula.\n\n{| style=\"float:right\"\n| [[File:Boolean satisfiability vs true literal counts.png|thumb|x200px|A formula with 2 clauses may be unsatisfied (red), 3-satisfied (green), xor-3-satisfied (blue), or/and 1-in-3-satisfied (yellow), depending on the TRUE-literal count in the 1st (hor) and 2nd (vert) clause.]]\n|}\n\n===XOR-satisfiability===\n\n{| align=\"right\" class=\"wikitable collapsible collapsed\" style=\"text-align:left; margin: 1em\"\n|-\n! Solving an XOR-SAT example<BR>by [[Gaussian elimination]]\n|-\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n|-\n! Given formula\n|-\n| (\"⊕\" means XOR, the {{color|#ff8080|red clause}} is optional)\n|-\n| (''a''⊕''c''⊕''d'') ∧ (''b''⊕¬''c''⊕''d'') ∧ (''a''⊕''b''⊕¬''d'') ∧ (''a''⊕¬''b''⊕¬''c'') {{color|#ff8080|∧ (¬''a''⊕''b''⊕''c'')}}\n|}\n|-\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n|-\n! colspan=\"9\" | Equation system\n|-\n| colspan=\"9\" | (\"1\" means TRUE, \"0\" means FALSE)\n|-\n| colspan=\"9\" | Each clause leads to one equation.\n|-\n|  || ''a'' || ⊕ ||   || ''c'' || ⊕ ||   || ''d'' || = 1\n|-\n|  || ''b'' || ⊕ || ¬ || ''c'' || ⊕ ||   || ''d'' || = 1\n|-\n|  || ''a'' || ⊕ ||   || ''b'' || ⊕ || ¬ || ''d'' || = 1\n|-\n|  || ''a'' || ⊕ || ¬ || ''b'' || ⊕ || ¬ || ''c'' || = 1\n|-\n|  {{color|#ff8080|¬}} || {{color|#ff8080|''a''}} || {{color|#ff8080|⊕}} || || {{color|#ff8080|''b''}} || {{color|#ff8080|⊕}} || || {{color|#ff8080|''c''}} || {{color|#ff8080| ≃ 1}}\n|}\n|-\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n|-\n! colspan=\"6\" | Normalized equation system\n|-\n| colspan=\"6\" | using properties of [[Boolean rings]] (¬''x''=1⊕''x'', ''x''⊕''x''=0)\n|-\n| ''a'' || ⊕ || ''c'' || ⊕ || ''d'' || = '''1'''\n|-\n| ''b'' || ⊕ || ''c'' || ⊕ || ''d'' || = '''0'''\n|-\n| ''a'' || ⊕ || ''b'' || ⊕ || ''d'' || = '''0'''\n|-\n| ''a'' || ⊕ || ''b'' || ⊕ || ''c'' || = '''1'''\n|-\n| {{color|#ff8080|''a''}} || {{color|#ff8080|⊕}}  || {{color|#ff8080|''b''}} || {{color|#ff8080|⊕}} || {{color|#ff8080|''c''}} || {{color|#ff8080| ≃ '''0'''}}\n|-\n| colspan=\"6\" | (If the {{color|#ff8080|red equation}} is present, {{color|#ff8080|it}} contradicts\n|-\n| colspan=\"6\" | the last black one, so the system is unsolvable.\n|-\n| colspan=\"6\" | Therefore, Gauss' algorithm is\n|-\n| colspan=\"6\" | used only for the black equations.)\n|}\n|-\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n|-\n! colspan=\"6\" | Associated coefficient matrix\n|-\n| &nbsp;\n|-\n! ''a'' !! ''b'' !! ''c'' !! ''d'' !!   !! line\n|-\n| &nbsp;\n|-\n| 1 || 0 || 1 || 1\n! 1\n| A\n|-\n| 0 || 1 || 1 || 1\n! 0\n| B\n|-\n| 1 || 1 || 0 || 1\n! 0\n| C\n|-\n| 1 || 1 || 1 || 0\n! 1\n| D\n|}\n|-\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n|-\n! colspan=\"6\" |Transforming to echelon form\n|-\n| &nbsp;\n|-\n! ''a'' !! ''b'' !! ''c'' !! ''d'' !!   !! operation\n|-\n| &nbsp;\n|-\n| 1 || 0 || 1 || 1\n! 1\n| A\n|-\n| 1 || 1 || 0 || 1\n! 0\n| C\n|-\n| 1 || 1 || 1 || 0\n! 1\n| D\n|-\n| 0 || 1 || 1 || 1\n! 0\n| B (swapped)\n|-\n| &nbsp;\n|-\n| 1 || 0 || 1 || 1\n! 1\n| A\n|-\n| 0 || 1 || 1 || 0\n! 1\n| E = C⊕A\n|-\n| 0 || 1 || 0 || 1\n! 0\n| F = D⊕A\n|-\n| 0 || 1 || 1 || 1\n! 0\n| B\n|-\n| &nbsp;\n|-\n| 1 || 0 || 1 || 1\n! 1\n| A\n|-\n| 0 || 1 || 1 || 0\n! 1\n| E\n|-\n| 0 || 0 || 1 || 1\n! 1\n| G = F⊕E\n|-\n| 0 || 0 || 0 || 1\n! 1\n| H = B⊕E\n|}\n|-\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n|-\n! colspan=\"6\" | Transforming to diagonal form\n|-\n| &nbsp;\n|-\n! ''a'' !! ''b'' !! ''c'' !! ''d'' !!   !! operation\n|-\n| &nbsp;\n|-\n| 1 || 0 || 1 || 0\n! 0\n| I = A⊕H\n|-\n| 0 || 1 || 1 || 0\n! 1\n| E\n|-\n| 0 || 0 || 1 || 0\n! 0\n| J = G⊕H\n|-\n| 0 || 0 || 0 || 1\n! 1\n| H\n|-\n| &nbsp;\n|-\n| 1 || 0 || 0 || 0\n! 0\n| K = I⊕J\n|-\n| 0 || 1 || 0 || 0\n! 1\n| L = E⊕J\n|-\n| 0 || 0 || 1 || 0\n! 0\n| J\n|-\n| 0 || 0 || 0 || 1\n! 1\n| H\n|-\n|}\n|-\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n|-\n! Solution:\n|-\n| If the {{color|#ff8080|red clause}} is present: || Unsolvable\n|-\n| Else: || ''a'' = 0 = FALSE\n|-\n| || ''b'' = 1 = TRUE\n|-\n| || ''c'' = 0 = FALSE\n|-\n| || ''d'' = 1 = TRUE\n|-\n| colspan=\"2\" | '''As a consequence:'''\n|-\n| colspan=\"2\" | ''R''(''a'',''c'',''d'') ∧ ''R''(''b'',¬''c'',''d'') ∧ ''R''(''a'',''b'',¬''d'') ∧ ''R''(''a'',¬''b'',¬''c'') {{color|#ff8080|∧ ''R''(¬''a'',''b'',''c'')}}\n|-\n| colspan=\"2\" | is not 1-in-3-satisfiable,\n|-\n| colspan=\"2\" | while (''a''∨''c''∨''d'') ∧ (''b''∨¬''c''∨''d'') ∧ (''a''∨''b''∨¬''d'') ∧ (''a''∨¬''b''∨¬''c'')\n|-\n| colspan=\"2\" |  is 3-satisfiable with ''a''=''c''=FALSE and ''b''=''d''=TRUE.\n|}\n|}\n\nAnother special case is the class of problems where each clause contains XOR (i.e. [[exclusive or]]) rather than (plain) OR operators.<ref group=note>Formally, generalized conjunctive normal forms with a ternary boolean operator ''R'' are employed, which is TRUE just if 1 or 3 of its arguments is. An input clause with more than 3 literals can be transformed into an equisatisfiable conjunction of clauses á 3 literals similar to [[#3-satisfiability|above]]; i.e. XOR-SAT can be reduced to XOR-3-SAT.</ref>\nThis is in [[P (complexity class)|P]], since an XOR-SAT formula can also be viewed as a system of linear equations mod 2, and can be solved in cubic time by [[Gaussian elimination]];<ref>{{citation|title=The Nature of Computation|first1=Cristopher|last1=Moore|author1-link=Cristopher Moore|first2=Stephan|last2=Mertens|publisher=Oxford University Press|year=2011|isbn=9780199233212|page=366|url=https://books.google.com/books?id=z4zMiZyAE1kC&pg=PA366}}.</ref> see the box for an example. This recast is based on the [[Boolean algebra (structure)#Boolean rings|kinship between Boolean algebras and Boolean rings]], and the fact that arithmetic modulo two forms a [[finite field]]. Since ''a'' XOR ''b'' XOR ''c'' evaluates to TRUE if and only if exactly 1 or 3 members of {''a'',''b'',''c''} are TRUE, each solution of the 1-in-3-SAT problem for a given CNF formula is also a solution of the XOR-3-SAT problem, and in turn each solution of XOR-3-SAT is a solution of 3-SAT, cf. picture. As a consequence, for each CNF formula, it is possible to solve the XOR-3-SAT problem defined by the formula, and based on the result infer either that the 3-SAT problem is solvable or that the 1-in-3-SAT problem is unsolvable.\n\nProvided that the [[P = NP problem|complexity classes P and NP are not equal]], neither 2-, nor Horn-, nor XOR-satisfiability is NP-complete, unlike SAT.\n\n===Schaefer's dichotomy theorem===\n{{Main article|Schaefer's dichotomy theorem}}\nThe restrictions above (CNF, 2CNF, 3CNF, Horn, XOR-SAT) bound the considered formulae to be conjunctions of subformulae; each restriction states a specific form for all subformulae: for example, only binary clauses can be subformulae in 2CNF.\n\nSchaefer's dichotomy theorem states that, for any restriction to Boolean operators that can be used to form these subformulae, the corresponding satisfiability problem is in P or NP-complete.  The membership in P of the satisfiability of 2CNF, Horn, and XOR-SAT formulae are special cases of this theorem.<ref name=\"schaefer\"/>\n\n==Extensions of SAT==\nAn extension that has gained significant popularity since 2003 is '''[[satisfiability modulo theories]]''' ('''SMT''') that can enrich CNF formulas with linear constraints, arrays, all-different constraints, [[uninterpreted function]]s,<ref name=\"Bryant.German.Velev.1999\">R. E. Bryant, S. M. German, and M. N. Velev, [http://portal.acm.org/citation.cfm?id=709275 Microprocessor Verification Using Efficient Decision Procedures for a Logic of Equality with Uninterpreted Functions], in Analytic Tableaux and Related Methods, pp.&nbsp;1–13, 1999.</ref> ''etc.'' Such extensions typically remain NP-complete, but very efficient solvers are now available that can handle many such kinds of constraints.\n\nThe satisfiability problem becomes more difficult if both \"for all\" ([[∀]]) and \"there exists\" ([[∃]]) [[Quantifier (logic)|quantifier]]s are allowed to bind the Boolean variables.\nAn example of such an expression would be {{math|size=100%|∀''x'' ∀''y'' ∃''z'' (''x'' ∨ ''y'' ∨ ''z'') ∧ (¬''x'' ∨ ¬''y'' ∨ ¬''z'')}}; it is valid, since for all values of ''x'' and ''y'', an appropriate value of ''z'' can be found, viz. ''z''=TRUE if both ''x'' and ''y'' are FALSE, and ''z''=FALSE else.\nSAT itself (tacitly) uses only ∃ quantifiers.\nIf only ∀ quantifiers are allowed instead, the so-called '''[[Tautology (logic)|tautology]] problem''' is obtained, which is [[co-NP-complete]].\nIf both quantifiers are allowed, the problem is called the '''[[quantified Boolean formula problem]]''' ('''QBF'''), which can be shown to be [[PSPACE-complete]]. It is widely believed that PSPACE-complete problems are strictly harder than any problem in NP, although this has not yet been proved. Using highly parallel ''[[P system]]s'', QBF-SAT problems can be solved in linear time.<ref>{{Cite journal | last1 = Alhazov | first1 = Artiom | last2 = Martín-Vide | first2 = Carlos | last3 = Pan | first3 = Linqiang | title = Solving a PSPACE-Complete Problem by Recognizing P Systems with Restricted Active Membranes | url = http://dl.acm.org/citation.cfm?id=2371013 | journal = Fundamenta Informaticae | volume = 58 | pages = 67–77 | year = 2003 }}</ref>\n\nOrdinary SAT asks if there is at least one variable assignment that makes the formula true. A variety of variants deal with the number of such assignments:\n* '''MAJ-SAT''' asks if the majority of all assignments make the formula TRUE. It is known to be complete for [[PP (complexity)|PP]], a probabilistic class.\n* '''[[Sharp-SAT|#SAT]]''', the problem of counting how many variable assignments satisfy a formula, is a counting problem, not a decision problem, and is [[Sharp-P-complete|#P-complete]].\n* '''UNIQUE-SAT''' is the problem of determining whether a formula has exactly one assignment. It is complete for [[US (complexity)|US]], the [[complexity class]] describing problems solvable by a non-deterministic polynomial time [[Turing machine]] that accepts when there is exactly one nondeterministic accepting path and rejects otherwise.\n* '''UNAMBIGUOUS-SAT''' is the name given to the satisfiability problem when the input is [[Promise problem|restricted]] to formulas having at most one satisfying assignment. A solving algorithm for UNAMBIGUOUS-SAT is allowed to exhibit any behavior, including endless looping, on a formula having several satisfying assignments. Although this problem seems easier, Valiant and Vazirani have [[Valiant-Vazirani theorem|shown]]<ref>{{Cite journal | last1 = Valiant | first1 = L. | last2 = Vazirani | first2 = V.| doi = 10.1016/0304-3975(86)90135-0 | title = NP is as easy as detecting unique solutions | url = http://www.cs.princeton.edu/courses/archive/fall05/cos528/handouts/NP_is_as.pdf| journal = Theoretical Computer Science | volume = 47 | pages = 85–93 | year = 1986 | pmid =  | pmc = }}</ref> that if there is a practical (i.e. [[Bounded-error probabilistic polynomial|randomized polynomial-time]]) algorithm to solve it, then all problems in [[NP (complexity class)|NP]] can be solved just as easily.\n* '''MAX-SAT''', the [[maximum satisfiability problem]], is an [[FNP (complexity)|FNP]] generalization of SAT. It asks for the maximum number of clauses, which can be satisfied by any assignment. It has efficient [[approximation algorithm]]s, but is NP-hard to solve exactly. Worse still, it is [[APX]]-complete, meaning there is no [[polynomial-time approximation scheme]] (PTAS) for this problem unless P=NP.\n\nOther generalizations include satisfiability for [[first-order predicate calculus|first]]- and [[second-order logic]], [[constraint satisfaction problem]]s, [[0-1 integer programming]].\n\n==Self-reducibility==\nThe SAT problem is '''self-reducible''', that is, each algorithm which correctly answers if an instance of SAT is solvable can be used to find a satisfying assignment. First, the question is asked on the given formula Φ. If the answer is \"no\", the formula is unsatisfiable. Otherwise, the question is asked on the partly instantiated formula Φ[[substitution (logic)|{''x''<sub>1</sub>=TRUE}]], i.e. Φ with the first variable ''x''<sub>1</sub> replaced by TRUE, and simplified accordingly. If the answer is \"yes\", then ''x''<sub>1</sub>=TRUE, otherwise ''x''<sub>1</sub>=FALSE. Values of other variables can be found subsequently in the same way. In total, ''n''+1 runs of the algorithm are required, where ''n'' is the number of distinct variables in Φ.\n\nThis property of self-reducibility is used in several theorems in complexity theory:\n\n: [[NP (complexity)|NP]] ⊆ [[P/poly]] ⇒ [[PH (complexity)|PH]] = [[Polynomial hierarchy#Definitions|Σ<sub>2</sub>]] &nbsp; ([[Karp–Lipton theorem]])\n: [[NP (complexity)|NP]] ⊆ [[BPP (complexity)|BPP]] ⇒ [[NP (complexity)|NP]] = [[RP (complexity)|RP]]\n: [[P (complexity)|P]] = [[NP (complexity)|NP]] ⇒ [[FP (complexity)|FP]] = [[FNP (complexity)|FNP]]\n\n==Algorithms for solving SAT==\n\nSince the SAT problem is NP-complete, only algorithms with exponential worst-case complexity are known for it. In spite of this, efficient and scalable algorithms for SAT were developed during the 2000s and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints (i.e. clauses).<ref name=\"Codish.Ohrimenko.Stuckey.2007\">{{citation|title=Principles and Practice of Constraint Programming – CP 2007|series=Lecture Notes in Computer Science|volume=4741|year=2007|pages=544–558|contribution=Propagation = Lazy Clause Generation|first1=Olga|last1=Ohrimenko|first2=Peter J.|last2=Stuckey|first3=Michael|last3=Codish|doi=10.1007/978-3-540-74970-7_39|quote=modern SAT solvers can often handle problems with millions of constraints and hundreds of thousands of variables|citeseerx=10.1.1.70.5471}}.</ref> Examples of such problems in [[electronic design automation]] (EDA) include [[formal equivalence checking]], [[model checking]], [[formal verification]] of [[microprocessor|pipelined microprocessors]],<ref name=\"Bryant.German.Velev.1999\"/> [[automatic test pattern generation]], [[routing (electronic design automation)|routing]] of [[FPGA]]s,<ref>{{Cite journal | last1 = Gi-Joon Nam | last2 = Sakallah | first2 = K. A. | last3 = Rutenbar | first3 = R. A. | title = A new FPGA detailed routing approach via search-based Boolean satisfiability | journal = IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems | volume = 21 | issue = 6 | pages = 674 | year = 2002 | url = http://cs-rutenbar.web.engr.illinois.edu/wp-content/uploads/2012/10/rutenbar-sattranscad02.pdf| doi = 10.1109/TCAD.2002.1004311}}</ref> [[Automated planning and scheduling|planning]], and [[Scheduling algorithm|scheduling problems]], and so on. A SAT-solving engine is now considered to be an essential component in the [[electronic design automation|EDA]] toolbox.\n\nThere are two classes of high-performance [[algorithms]] for solving instances of SAT in practice: the [[conflict-driven clause learning]] algorithm, which can be viewed as a modern variant of the [[DPLL algorithm]] (well known implementations include [[Chaff algorithm|Chaff]]<ref>{{Cite book | last1 = Moskewicz | first1 = M. W. | last2 = Madigan | first2 = C. F. | last3 = Zhao | first3 = Y. | last4 = Zhang | first4 = L. | last5 = Malik | first5 = S. | chapter = Chaff: Engineering an Efficient SAT Solver| title = Proceedings of the 38th conference on Design automation (DAC)| pages = 530 | year = 2001 | isbn = 1581132972 | chapter-url = http://www.princeton.edu/~chaff/publication/DAC2001v56.pdf| doi = 10.1145/378239.379017}}</ref> and [[GRASP (SAT solver)|GRASP]]<ref>{{Cite journal | last1 = Marques-Silva | first1 = J. P. | last2 = Sakallah | first2 = K. A. | title = GRASP: a search algorithm for propositional satisfiability | journal = IEEE Transactions on Computers | volume = 48 | issue = 5 | pages = 506 | year = 1999 | url = http://embedded.eecs.berkeley.edu/Alumni/wjiang/ee219b/grasp.pdf| doi = 10.1109/12.769433}}</ref>) and [[stochastic]] [[Local search (constraint satisfaction)|local search]] algorithms, such as [[WalkSAT]].\n\nA DPLL SAT solver employs a systematic backtracking search procedure to explore the (exponentially sized) space of variable assignments looking for satisfying assignments. The basic search procedure was proposed in two seminal papers in the early 1960s (see references below) and is now commonly referred to as the [[Davis–Putnam–Logemann–Loveland algorithm]] (\"DPLL\" or \"DLL\").<ref>{{Cite journal | last1 = Davis | first1 = M. | last2 = Putnam | first2 = H. | title = A Computing Procedure for Quantification Theory | journal = Journal of the ACM | volume = 7 | issue = 3 | pages = 201 | year = 1960 | doi = 10.1145/321033.321034}}</ref><ref>{{Cite journal | last1 = Davis | first1 = M. |authorlink1=Martin Davis (mathematician)| last2 = Logemann | first2 = G. | last3 = Loveland | first3 = D. | title = A machine program for theorem-proving | journal = [[Communications of the ACM]]| volume = 5 | issue = 7 | pages = 394–397 | year = 1962 | url = http://www.ensiie.fr/~blazy/ipr/article2.pdf| doi = 10.1145/368273.368557}}</ref> Theoretically, exponential lower bounds have been proved for the DPLL family of algorithms.\n\nIn contrast, randomized algorithms like the PPSZ algorithm by Paturi, Pudlak, Saks, and Zane set variables in a random order according to some heuristics, for example bounded-width [[Resolution (logic)|resolution]]. If the heuristic can't find the correct setting, the variable is assigned randomly. The PPSZ algorithm has a runtime of <math>O(2^{0.386n})</math> for 3-SAT with a single satisfying assignment. Currently this is the best-known runtime for this problem. In the setting with many satisfying assignments the randomized algorithm by Schöning has a better bound.<ref name=\"Schoning.1999\"/><ref name=\"ppsz_algorithm\">[http://dl.acm.org/citation.cfm?id=1066101 \"An improved exponential-time algorithm for k-SAT\"], Paturi, Pudlak, Saks, Zani</ref>\n\nModern SAT solvers (developed in the 2000s) come in two flavors: \"conflict-driven\" and \"look-ahead\".{{clarify|reason=3 paragraphs before here, 'two classes', viz. 'conflict-driven clause learning DPLL' and 'stochastic local search' were named. Please indicate if the latter essentially means the same as 'look-ahead' here.|date=September 2013}} Conflict-driven solvers augment the basic DPLL search algorithm with efficient conflict analysis, clause learning, non-[[chronological backtracking]] (a.k.a. [[backjumping]]), as well as [[\"two-watched-literals\" unit propagation]], adaptive branching, and random restarts. These \"extras\" to the basic systematic search have been empirically shown to be essential for handling the large SAT instances that arise in [[electronic design automation]] (EDA).<ref>{{Cite journal | last1 = Vizel | first1 = Y. | last2 = Weissenbacher | first2 = G. | last3 = Malik | first3 = S. | journal = Proceedings of the IEEE | volume = 103 | issue = 11 | year = 2015 | doi = 10.1109/JPROC.2015.2455034|title=Boolean Satisfiability Solvers and Their Applications in Model Checking}}</ref> Look-ahead solvers have especially strengthened reductions (going beyond unit-clause propagation) and the heuristics, and they are generally stronger than conflict-driven solvers on hard instances (while conflict-driven solvers can be much better on large instances which actually have an easy instance inside).\n\nModern SAT solvers are also having significant impact on the fields of software verification, constraint solving in artificial intelligence, and operations research, among others. Powerful solvers are readily available as [[free and open source software]]. In particular, the conflict-driven [http://minisat.se/ MiniSAT], which was relatively successful at the [http://www.satcompetition.org/ 2005 SAT competition], only has about 600 lines of code. A modern Parallel SAT solver is ManySAT<ref>http://www.cril.univ-artois.fr/~jabbour/manysat.htm</ref>. It can achieve super linear speed-ups on important classes of problems. An example for look-ahead solvers is [http://www.st.ewi.tudelft.nl/sat/march_dl.php march_dl], which won a prize at the [http://www.satcompetition.org/ 2007 SAT competition].\n\nCertain types of large random satisfiable instances of SAT can be solved by [[survey propagation]] (SP). Particularly in [[hardware design]] and [[hardware verification|verification]] applications, satisfiability and other logical properties of a given propositional formula are sometimes decided based on a representation of the formula as a [[binary decision diagram]] (BDD).\n\nAlmost all SAT solvers include time-outs, so they will terminate in reasonable time even if they cannot find a solution.\nDifferent SAT solvers will find different instances easy or hard, and some excel at proving unsatisfiability, and others at finding solutions.\nAll of these behaviors can be seen in the SAT solving contests.<ref>{{cite web|url=http://www.satcompetition.org/ |title=The international SAT Competitions web page|accessdate=2007-11-15}}</ref>\n\n==See also==\n*[[Unsatisfiable core]]\n*[[Satisfiability modulo theories]]\n*[[Sharp-SAT|Counting SAT]]\n*[[Karloff–Zwick algorithm]]\n*[[Circuit satisfiability]]\n\n==Notes==\n{{Reflist|group=note}}\n\n==References==\n{{Reflist|30em}}\nReferences are ordered by date of publication:\n{{refbegin|colwidth=30em}}\n* {{cite book|author = Michael R. Garey|author2 = David S. Johnson|author-link = Michael R. Garey|author2-link = David S. Johnson|last-author-amp = yes| year = 1979 | title = [[List of important publications in theoretical computer science#Computers and Intractability: A Guide to the Theory of NP-Completeness|Computers and Intractability: A Guide to the Theory of NP-Completeness]] | publisher = W.H. Freeman | isbn = 0-7167-1045-5}} A9.1: LO1 – LO7, pp.&nbsp;259 – 260.\n* {{Cite book | last1 = Marques-Silva | first1 = J. | last2 = Glass | first2 = T. | chapter = Combinational equivalence checking using satisfiability and recursive learning | title = Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078) | pages = 145 | year = 1999 | isbn = 0-7695-0078-1 | chapter-url= http://eprints.soton.ac.uk/265003/1/jpms-date99a.pdf| doi = 10.1109/DATE.1999.761110}}\n* {{Cite journal | last1 = Clarke | first1 = E. | last2 = Biere | first2 = A. | last3 = Raimi | first3 = R. | last4 = Zhu | first4 = Y. | journal = Formal Methods in System Design | volume = 19 | pages = 7 | year = 2001 | doi = 10.1023/A:1011276507260|title=Bounded Model Checking Using Satisfiability Solving}}\n* {{Cite journal | last1 = Giunchiglia | first1 = E. | last2 = Tacchella | first2 = A. | editor1-last = Giunchiglia | editor1-first = Enrico | editor2-first = Armando | title = Theory and Applications of Satisfiability Testing | series = Lecture Notes in Computer Science | volume = 2919 | year = 2004 | isbn = 978-3-540-20851-8 | doi = 10.1007/b95238 | editor2-last = Tacchella}}\n* {{Cite journal | last1 = Babic | first1 = D. | last2 = Bingham | first2 = J. | last3 = Hu | first3 = A. J. | title = B-Cubing: New Possibilities for Efficient SAT-Solving | journal = IEEE Transactions on Computers | volume = 55 | issue = 11 | pages = 1315 | year = 2006 | url = http://www.domagoj-babic.com/uploads/Pubs/TCOM06/tcom06.pdf| doi = 10.1109/TC.2006.175}}\n* {{Cite book | last1 = Rodriguez | first1 = C. | last2 = Villagra | first2 = M. | last3 = Baran | first3 = B. | chapter = Asynchronous team algorithms for Boolean Satisfiability | title = 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems | pages = 66 | year = 2007 | chapter-url = http://www.cc.pol.una.py/lcca/publicaciones/optimizacion/2007/Asynchronous%20Team%20Algorithms%20for%20Boolean%20Satisfiability.pdf| doi = 10.1109/BIMNICS.2007.4610083}}\n* {{cite book|editor1=Frank Van Harmelen |editor2=Vladimir Lifschitz |editor3=Bruce Porter |title=Handbook of knowledge representation|year=2008|publisher=Elsevier|isbn=978-0-444-52211-5|pages=89–134|author1=Carla P. Gomes|author1-link=Carla Gomes |author2=Henry Kautz |author3=Ashish Sabharwal |author4=Bart Selman |chapter=Satisfiability Solvers|doi=10.1016/S1574-6526(07)03002-7|series=Foundations of Artificial Intelligence|volume=3}}\n* {{Cite journal | last1 = Vizel | first1 = Y. | last2 = Weissenbacher | first2 = G. | last3 = Malik | first3 = S. | journal = Proceedings of the IEEE | volume = 103 | issue = 11 | year = 2015 | doi = 10.1109/JPROC.2015.2455034|title=Boolean Satisfiability Solvers and Their Applications in Model Checking}}\n{{refend}}\n\n==External links==\n{{commons category}}\n[http://www.cril.univ-artois.fr/~roussel/satgame/satgame.php?lang=eng SAT Game] - try solving a Boolean satisfiability problem yourself\n\n===SAT problem format===\nA SAT problem is often described in the [http://www.domagoj-babic.com/uploads/ResearchProjects/Spear/dimacs-cnf.pdf DIMACS-CNF] format: an input file in which each line represents a single disjunction. For example, a file with the two lines\n 1 -5 4 0\n -1 5 3 4 0\nrepresents the formula \"(''x''<sub>1</sub> ∨ ¬''x''<sub>5</sub> ∨ ''x''<sub>4</sub>) ∧ (¬''x''<sub>1</sub> ∨ ''x''<sub>5</sub> ∨ ''x''<sub>3</sub> ∨ ''x''<sub>4</sub>)\".\n\nAnother common format for this formula is the 7-bit [[ASCII]] representation \"(x1 | ~x5 | x4) & (~x1 | x5 | x3 | x4)\".\n* [http://users.ics.aalto.fi/tjunttil/bcsat/ BCSAT] is a tool that converts input files in human-readable format to the DIMACS-CNF format.\n\n===Online SAT solvers===\n* BoolSAT – Solves formulas in the DIMACS-CNF format or in a more human-friendly format: \"a and not b or a\". Runs on a server.\n* [http://logictools.org Logictools] – Provides different solvers in javascript for learning, comparison and hacking. Runs in the browser.\n* [http://www.msoos.org/2013/09/minisat-in-your-browser/ minisat-in-your-browser] – Solves formulas in the DIMACS-CNF format. Runs in the browser.\n* [http://satrennespa.irisa.fr/ SATRennesPA] – Solves formulas written in a user-friendly way. Runs on a server.\n* [http://somerby.net/mack/logic somerby.net/mack/logic] – Solves formulas written in symbolic logic. Runs in the browser.\n\n===Offline SAT solvers===\n* [http://minisat.se/ MiniSAT] – DIMACS-CNF format and OPB format for it's companion Pseudo-Boolean constraint solver MiniSat+\n* [http://fmv.jku.at/lingeling/ Lingeling] – won a gold medal in a 2011 SAT competition.\n** [http://fmv.jku.at/picosat/ PicoSAT] – an earlier solver from the Lingeling group.\n* [http://www.sat4j.org/ Sat4j] – DIMACS-CNF format. Java source code available.\n* [http://www.labri.fr/~lsimon/glucose Glucose] – DIMACS-CNF format.\n* [http://reasoning.cs.ucla.edu/rsat/home.html RSat] – won a gold medal in a 2007 SAT competition.\n* [http://ubcsat.dtompkins.com/ UBCSAT]. Supports unweighted and weighted clauses, both in the DIMACS-CNF format. C source code hosted on [https://github.com/dtompkins/ubcsat GitHub].\n* [http://www.msoos.org/cryptominisat2 CryptoMiniSat] – won a gold medal in a 2011 SAT competition. C++ source code hosted on [https://github.com/msoos/cryptominisat GitHub]. Tries to put many useful features of MiniSat 2.0 core, PrecoSat ver 236, and Glucose into one package, adding many new features\n* [http://www.domagoj-babic.com/index.php/ResearchProjects/Spear Spear] – Supports bit-vector arithmetic. Can use the DIMACS-CNF format or the [http://www.domagoj-babic.com/uploads/ResearchProjects/Spear/spear-format.pdf Spear format].\n** [http://www.domagoj-babic.com/index.php/ResearchProjects/HyperSAT HyperSAT] – Written to experiment with B-cubing search space pruning. Won 3rd place in a 2005 SAT competition. An earlier and slower solver from the developers of Spear.\n* [http://logic.pdmi.ras.ru/~basolver/ BASolver]\n* [http://argo.matf.bg.ac.rs/?content=downloads ArgoSAT]\n* [http://dudka.cz/fss Fast SAT Solver] – based on [[genetic algorithm]]s.\n* [http://www.princeton.edu/~chaff/zchaff.html zChaff] – not supported anymore.\n* [http://users.ics.aalto.fi/tjunttil/bcsat/ BCSAT] – human-readable boolean circuit format (also converts this format to the DIMACS-CNF format and automatically links to MiniSAT or zChaff).\n* [https://github.com/IRIFrance/gini gini] – Golang sat solver with related tools.\n* [https://github.com/crillab/gophersat gophersat] – Golang SAT solver with can also deal with pseudo-boolean and MAXSAT problems.\n* CLP(B) – Boolean Constraint Logic Programming, for example [[SWI-Prolog#Constraint Logic Programming Libraries (CLP)|SWI-Prolog]].\n\n===SAT applications===\n* [http://www.mqasem.net/sat/winsat/ WinSAT v2.04]: A Windows-based SAT application made particularly for researchers.\n\n===Conferences===\n* [http://www.satisfiability.org/ International Conference on Theory and Applications of Satisfiability Testing]\n\n===Publications===\n* [https://web.archive.org/web/20060219180520/http://jsat.ewi.tudelft.nl/ Journal on Satisfiability, Boolean Modeling and Computation]\n* [https://web.archive.org/web/20060210192113/http://www.ictp.trieste.it/~zecchina/SP/ Survey Propagation]\n\n===Benchmarks===\n* [http://www.nlsde.buaa.edu.cn/~kexu/benchmarks/benchmarks.htm Forced Satisfiable SAT Benchmarks]\n* [http://www.cs.ubc.ca/~babic/index_benchmarks.htm Software Verification Benchmarks]\n* [http://www.aloul.net/benchmarks.html Fadi Aloul SAT Benchmarks]\nSAT solving in general:\n* http://www.satlive.org\n* http://www.satisfiability.org\n\n===Evaluation of SAT solvers===\n* [http://www.maxsat.udl.cat/ Yearly evaluation of SAT solvers]\n* [http://www.maxsat.udl.cat/08/ms08.pdf SAT solvers evaluation results for 2008]\n* [http://www.satcompetition.org International SAT Competitions]\n* [http://www.satcompetition.org/2002/onlinereport/node2.html History]\n\nMore information on SAT:\n* [http://www.mqasem.net/sat/sat SAT and MAX-SAT for the Lay-researcher]\n* [https://yurichev.com/writings/SAT_SMT_by_example.pdf SAT/SMT by Example]\n----\n\n''This article includes material from a column in the ACM [http://www.sigda.org SIGDA] [https://web.archive.org/web/20070208034716/http://www.sigda.org/newsletter/index.html e-newsletter] by [http://www.eecs.umich.edu/~karem Prof. Karem Sakallah] <br />\nOriginal text is available [https://web.archive.org/web/20070708233347/http://www.sigda.org/newsletter/2006/eNews_061201.html here]''\n\n{{Logic}}\n\n{{DEFAULTSORT:Boolean Satisfiability Problem}}\n[[Category:Boolean algebra]]\n[[Category:Electronic design automation]]\n[[Category:Formal methods]]\n[[Category:Logic in computer science]]\n[[Category:NP-complete problems]]\n[[Category:Satisfiability problems]]"
    },
    {
      "title": "Boolean-valued function",
      "url": "https://en.wikipedia.org/wiki/Boolean-valued_function",
      "text": "{{Cleanup rewrite|date=March 2011}}\n{{Functions}}\n\nA '''Boolean-valued function''' (sometimes called a [[Predicate (logic)|predicate]] or a [[proposition]]) is a [[function (mathematics)|function]] of the type f : X → '''B''', where X is an arbitrary [[Set (mathematics)|set]] and where '''B''' is a [[Boolean domain]], i.e. a generic two-element set, (for example '''B''' = {0, 1}), whose elements are interpreted as [[logical value]]s, for example, 0 = [[false (logic)|false]] and 1 = [[truth value|true]], i.e., a single [[bit]] of [[information]].\n\nIn the [[formal science]]s, [[mathematics]], [[mathematical logic]], [[statistics]], and their applied disciplines, a Boolean-valued function may also be referred to as a characteristic function, [[indicator function]], predicate, or proposition. In all of these uses, it is understood that the various terms refer to a mathematical object and not the corresponding [[semiotic]] sign or syntactic expression.\n\nIn [[semantics|formal semantic]] theories of [[truth]], a '''truth predicate''' is a predicate on the [[Sentence (mathematical logic)|sentence]]s of a [[formal language]], interpreted for logic, that formalizes the intuitive concept that is normally expressed by saying that a sentence is true. A truth predicate may have additional domains beyond the formal language domain, if that is what is required to determine a final [[truth value]].\n\n==References==\n\n* [[Frank Markham Brown|Brown, Frank Markham]] (2003), ''Boolean Reasoning:  The Logic of Boolean Equations'', 1st edition, Kluwer Academic Publishers, Norwell, MA.  2nd edition, Dover Publications, Mineola, NY, 2003.\n* [[Zvi Kohavi|Kohavi, Zvi]] (1978), ''Switching and Finite Automata Theory'', 1st edition, McGraw–Hill, 1970.  2nd edition, McGraw–Hill, 1978.\n* [[Robert R. Korfhage|Korfhage, Robert R.]] (1974), ''Discrete Computational Structures'', Academic Press, New York, NY.\n* [[Mathematical Society of Japan]], ''Encyclopedic Dictionary of Mathematics'', 2nd edition, 2 vols., Kiyosi Itô (ed.), MIT Press, Cambridge, MA, 1993.  Cited as EDM.\n* [[Marvin L. Minsky|Minsky, Marvin L.]], and [[Seymour A. Papert|Papert, Seymour, A.]] (1988), ''[[Perceptrons]], An Introduction to Computational Geometry'', MIT Press, Cambridge, MA, 1969.  Revised, 1972.  Expanded edition, 1988.\n\n==See also==\n{{colbegin|colwidth=18em}}\n* [[Bit]]\n* [[Boolean data type]]\n* [[Boolean algebra (logic)]]\n* [[Boolean domain]]\n* [[Boolean logic]]\n* [[Propositional calculus]]\n* [[Truth table]]\n* [[Logic minimization]]\n* [[Indicator function]]\n* [[Predicate (logic)|Predicate]]\n* [[Proposition]]\n* [[Finitary boolean function]]\n* [[Boolean function]]\n{{colend}}\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Boolean-valued model",
      "url": "https://en.wikipedia.org/wiki/Boolean-valued_model",
      "text": "{{short description|set theory concept}}\nIn [[mathematical logic]], a '''Boolean-valued model''' is a generalization of the ordinary [[Alfred Tarski|Tarskian]] notion of [[structure (mathematical logic)|structure]] from [[model theory]]. In a Boolean-valued model, the [[truth value]]s of [[proposition]]s are not limited to \"true\" and \"false\", but instead take values in some fixed [[complete Boolean algebra]].\n\nBoolean-valued models were introduced by [[Dana Scott]], [[Robert M. Solovay]], and [[Petr Vopěnka]] in the 1960s in order to help understand [[Paul Cohen (mathematician)|Paul Cohen]]'s method of [[forcing (mathematics)|forcing]]. They are also related to [[Heyting algebra]] semantics in [[intuitionistic logic]].\n\n==Definition==\n\nFix a complete Boolean algebra ''B''<ref name=\"trivial_ba\">''B'' here is assumed to be ''nondegenerate''; that is, 0 and 1 must be distinct elements of ''B''. Authors writing on Boolean-valued models typically take this requirement to be part of the definition of \"Boolean algebra\", but authors writing on Boolean algebras in general often do not.</ref> and a [[first-order language]] ''L''; the [[signature (mathematical logic)|signature]] of ''L'' will consist of a collection of constant symbols, function symbols, and relation symbols.\n\nA Boolean-valued model for the language ''L'' consists of a universe ''M'', which is a set of elements (or '''''names'''''), together with interpretations for the symbols. Specifically, the model must assign to each constant symbol of ''L'' an element of ''M'', and to each ''n''-ary function symbol ''f'' of ''L'' and each ''n''-tuple &lt;a<sub>0</sub>,...,a<sub>''n''-1</sub>&gt; of elements of ''M'', the model must assign an element of ''M'' to the term ''f''(a<sub>0</sub>,...,a<sub>''n''-1</sub>).\n\nInterpretation of the [[atomic formula]]s of ''L'' is more complicated. To each pair ''a'' and ''b'' of elements of ''M'', the model must assign a truth value ||''a''=''b''|| to the expression ''a''=''b''; this truth value is taken from the Boolean algebra ''B''. Similarly, for each ''n''-ary relation symbol ''R'' of ''L'' and each ''n''-tuple &lt;a<sub>0</sub>,...,a<sub>''n''-1</sub>&gt; of elements of ''M'', the model must assign an element of ''B'' to be the truth value ||''R''(a<sub>0</sub>,...,a<sub>''n''-1</sub>)||.\n\n==Interpretation of other formulas and sentences==\n\nThe truth values of the atomic formulas can be used to reconstruct the truth values of more complicated formulas, using the structure of the Boolean algebra. For propositional connectives, this is easy; one simply applies the corresponding Boolean operators to the truth values of the subformulae. For example, if φ(''x'') and ψ(''y'',''z'') are formulas with one and two [[free variable]]s, respectively, and if ''a'', ''b'', ''c'' are elements of the model's universe to be substituted for ''x'', ''y'', and ''z'', then the truth value of\n: <math>\\phi(a)\\land\\psi(b,c)</math>\nis simply\n: <math>\\|\\phi(a)\\land\\psi(b,c)\\|=\\|\\phi(a)\\|\\ \\land\\ \\|\\psi(b,c)\\| </math>\n\nThe completeness of the Boolean algebra is required to define truth values for quantified formulas. If φ(''x'') is a formula with free variable ''x'' (and possibly other free variables that are suppressed), then\n: <math>\\|\\exists x\\phi(x)\\|=\\bigvee_{a\\in M}\\|\\phi(a)\\|,</math>\nwhere the right-hand side is to be understood as the [[supremum]] in ''B'' of the set of all truth values ||φ(''a'')|| as ''a'' ranges over ''M''.\n\nThe truth value of a formula is sometimes referred to as its [[probability]]. However, these are not probabilities in the ordinary sense, because they are not [[real number]]s, but rather elements of the complete Boolean algebra ''B''.\n\n==Boolean-valued models of set theory==\nGiven a complete Boolean algebra ''B''<ref name=\"trivial_ba\"/> there is a Boolean-valued model denoted by ''V<sup>B</sup>'', which is the Boolean-valued analogue of the [[von Neumann universe]] ''V''. (Strictly speaking, ''V<sup>B</sup>'' is a [[proper class]], so we need to reinterpret what it means to be a [[model theory|model]] appropriately.) Informally, the elements of ''V<sup>B</sup>'' are \"Boolean-valued sets\". Given an ordinary set ''A'', every set either is or is not a member; but given a Boolean-valued set, every set has a certain, fixed \"probability\" of being a member of ''A''. Again, the \"probability\" is an element of ''B'', not a real number. The concept of Boolean-valued sets resembles, but is not the same as, the notion of a [[fuzzy set]].\n\nThe (\"probabilistic\") elements of the Boolean-valued set, in turn, are also Boolean-valued sets, whose elements are also Boolean-valued sets, and so on. In order to obtain a non-circular definition of Boolean-valued set, they are defined inductively in a hierarchy similar to the [[cumulative hierarchy]]. For each ordinal α of ''V'', the set ''V<sup>B</sup><sub>α</sub>'' is defined as follows. \n* ''V''<sup>B</sup><sub>0</sub> is the empty set. \n*''V<sup>B</sup><sub>α+1</sub>'' is the set of all functions from ''V<sup>B</sup><sub>α</sub>'' to ''B''. (Such a function represents a \"probabilistic\" [[subset]] of ''V<sup>B</sup><sub>α</sub>''; if ''f'' is such a function, then for any ''x''∈''V<sup>B</sup><sub>α</sub>'', ''f''(''x'') is the probability that ''x'' is in the set.)\n* If α is a limit ordinal, ''V<sup>B</sup><sub>α</sub>'' is the union of ''V<sup>B</sup><sub>β</sub>'' for β&lt;α \nThe class ''V<sup>B</sup>'' is defined to be the union of all sets ''V<sup>B</sup><sub>α</sub>''.\n\nIt is also possible to relativize this entire construction to some transitive model ''M'' of [[Zermelo-Fraenkel set theory|ZF]] (or sometimes a fragment thereof). The Boolean-valued model ''M''<sup>''B''</sup> is obtained by applying the above construction ''inside'' ''M''. The restriction to transitive models is not serious, as the [[Mostowski collapse|Mostowski collapsing theorem]] implies that every \"reasonable\" (well-founded, extensional) model is isomorphic to a transitive one. (If the model ''M'' is not transitive things get messier, as ''M'''s interpretation of what it means to be a \"function\" or an \"ordinal\" may differ from  the \"external\" interpretation.)\n\nOnce the elements of ''V''<sup>B</sup> have been defined as above, it is necessary to define ''B''-valued  relations of equality and membership on ''V<sup>B</sup>''. Here a ''B''-valued relation on ''V<sup>B</sup>'' is a function from ''V<sup>B</sup>''&times;''V<sup>B</sup>'' to ''B''. To avoid confusion with the usual equality and membership, these are denoted by ||''x''=''y''|| and ||''x''∈''y''||  for ''x'' and ''y'' in ''V<sup>B</sup>''. They are defined as follows:\n:||''x''∈''y''|| is defined to be ∑<sub>''t''∈Dom(''y'')</sub> ||''x''=''t''|| ∧ ''y''(''t'')   (\"''x'' is in ''y'' if it is equal to something in ''y''\").\n:||''x''=''y''|| is defined to be ||''x''⊆''y''||∧||y⊆''x''||   (\"''x'' equals ''y'' if ''x'' and ''y'' are both subsets of each other\"), where\n:||''x''⊆''y''|| is defined to be ∏<sub>''t''∈Dom(''x'')</sub> ''x''(''t'')⇒||''t''∈''y''||   (\"''x'' is a subset of ''y'' if all elements of ''x'' are in ''y''\")\n\nThe symbols ∑ and ∏ denote the least upper bound and greatest lower bound operations, respectively, in the complete Boolean algebra ''B''. At first sight the definitions above appear to be circular: ||&nbsp; ∈&nbsp;|| depends on ||&nbsp;=&nbsp;||, which depends on ||&nbsp;⊆&nbsp;||, which depends on ||&nbsp;∈&nbsp;||. However, a close examination shows that the definition of ||&nbsp;∈&nbsp;|| only depends on ||&nbsp;∈&nbsp;|| for elements of smaller rank, so ||&nbsp;∈&nbsp;|| and ||&nbsp; =&nbsp;|| are well defined functions from ''V<sup>B</sup>''&times;''V<sup>B</sup>'' to ''B''.\n\nIt can be shown that the ''B''-valued  relations ||&nbsp;∈&nbsp;|| and ||&nbsp;=&nbsp;|| on ''V<sup>B</sup>'' make ''V<sup>B</sup>'' into a Boolean-valued model of set theory. Each sentence of first order set theory with no free variables has a truth value in ''B''; it must be shown that the axioms for equality and all the axioms of ZF set theory (written without free variables) have truth value 1 (the largest element of ''B''). This proof is straightforward, but it is long because there are many different axioms that need to be checked.\n\n==Relationship to forcing==\nSet theorists use a technique called [[forcing (mathematics)|forcing]]\nto obtain [[independence (mathematical logic)|independence results]] and to construct models of set theory for other purposes. The method was originally developed by [[Paul Cohen (mathematician)|Paul Cohen]] but has been greatly extended since then. In one form, forcing \"adds to the universe\" a [[generic filter|generic]] subset of a [[poset]], the poset being designed to impose interesting properties on the newly added object. The wrinkle is that (for interesting posets) it can be proved that there simply ''is'' no such generic subset of the poset. There are three usual ways of dealing with this:\n* '''syntactic forcing''' A ''forcing relation'' <math> p\\Vdash\\phi</math> is defined between elements ''p'' of the poset and formulas φ of the ''forcing language''. This relation is defined syntactically and has no semantics; that is, no model is ever produced. Rather, starting with the assumption that ZFC (or some other axiomatization of set theory) proves the independent statement, one shows that ZFC must also be able to prove a contradiction. However, the forcing is \"over ''V''\"; that is, it is not necessary to start with a countable transitive model. See Kunen (1980) for an exposition of this method. \n* '''countable transitive models''' One starts with a [[countable set|countable]] [[transitive set|transitive]] model ''M'' of as much of set theory as is needed for the desired purpose, and that contains the poset. Then there ''do'' exist filters on the poset that are generic ''over M''; that is, that meet all dense open subsets of the poset that happen also to be elements of ''M''. \n* '''fictional generic objects''' Commonly, set theorists will simply ''pretend'' that the poset has a subset that is generic over all of ''V''. This generic object, in nontrivial cases, cannot be an element of ''V'', and therefore \"does not really exist\". (Of course, it is a point of philosophical contention whether ''any'' sets \"really exist\", but that is outside the scope of the current discussion.) Perhaps surprisingly, with a little practice this method is useful and reliable, but it can be philosophically unsatisfying.\n\n===Boolean-valued models and syntactic forcing===\nBoolean-valued models can be used to give semantics to syntactic forcing; the price paid is that the semantics is not 2-valued (\"true or false\"), but assigns truth values from some complete Boolean algebra. Given a forcing poset ''P'', there is a corresponding complete Boolean algebra ''B'', often obtained as the collection of [[regular open set|regular open subsets]] of ''P'', where the [[topology]] on ''P'' is defined by declaring all [[lower set]]s open (and all [[upper set]]s closed). (Other approaches to constructing ''B'' are discussed below.)\n\nNow the order on ''B'' (after removing the zero element) can replace ''P'' for forcing purposes, and the forcing relation can be interpreted semantically by saying that, for ''p'' an element of ''B'' and φ a formula of the forcing language,\n:<math>p\\Vdash\\phi\\iff p\\leq||\\phi||</math>\nwhere ||φ|| is the truth value of φ in ''V''<sup>''B''</sup>.\n\nThis approach succeeds in assigning a semantics to forcing over ''V'' without resorting to fictional generic objects. The disadvantages are that the semantics is not 2-valued, and that the combinatorics of ''B'' are often more complicated than those of the underlying poset ''P''.\n\n===Boolean-valued models and generic objects over countable transitive models===\nOne interpretation of forcing starts with a countable transitive model ''M'' of ZF set theory, a partially ordered set ''P'', and a \"generic\" subset ''G'' of ''P'', and constructs a new model of ZF set theory from these objects. (The conditions that the model be countable and transitive simplify some technical problems, but are not essential.) Cohen's construction can be carried out using Boolean-valued models as follows. \n* Construct a complete Boolean algebra ''B'' as the complete Boolean algebra \"generated by\" the poset ''P''. \n* Construct an ultrafilter ''U'' on ''B'' (or equivalently a homomorphism from ''B'' to the Boolean algebra {true, false}) from the generic subset ''G'' of ''P''.\n* Use the homomorphism  from ''B'' to {true, false} to turn the Boolean-valued model ''M<sup>B</sup>'' of the section above into an ordinary model of ZF.\n\nWe now explain these steps in more detail.\n\nFor any poset ''P'' there is a complete Boolean algebra ''B'' and a map ''e'' from ''P'' to ''B''<sup>+</sup> (the non-zero elements of ''B'') such that the image is dense, ''e''(''p'')≤''e''(''q'') whenever ''p''≤''q'', and ''e''(''p'')''e''(''q'')=0 whenever ''p'' and ''q'' are incompatible. This Boolean algebra is unique up to isomorphism. It can be constructed as the algebra of regular open sets in the topological space of ''P'' (with underlying set ''P'', and a base given by the sets ''U''<sub>''p''</sub> of elements ''q'' with ''q''≤''p'').\n\nThe map from the poset ''P'' to the complete Boolean algebra ''B'' is not injective in general. The map is injective if and only if ''P'' has the following property: if every ''r''≤''p'' is compatible with ''q'', then ''p''≤''q''.\n\nThe ultrafilter ''U'' on ''B'' is defined to be  the set of elements ''b'' of ''B'' that are greater than some element of (the image of) ''G''. Given an ultrafilter ''U'' on a Boolean algebra, we get a homomorphism to {true, false}\nby mapping ''U'' to true and its complement to false. Conversely, given such a homomorphism, the inverse image of true is an ultrafilter, so ultrafilters are essentially the same as homomorphisms to {true, false}. (Algebraists might prefer to use maximal ideals instead of ultrafilters: the complement of an ultrafilter is a maximal ideal, and conversely the complement of a maximal ideal is an ultrafilter.)\n\nIf ''g'' is a homomorphism from a Boolean algebra ''B'' to a Boolean algebra ''C'' and ''M<sup>B</sup>'' is any \n''B''-valued model of ZF (or of any other theory for that matter) we can turn ''M<sup>B</sup>''  into a ''C'' -valued model by applying the homomorphism ''g'' to the value of all formulas. In particular if ''C'' is {true, false} we get a {true, false}-valued model. This is almost the same as an ordinary model: in fact we get an ordinary model on the set of equivalence classes under ||&nbsp;=&nbsp;|| of a {true, false}-valued model. So we get an ordinary model of ZF set theory by starting from ''M'', a Boolean algebra ''B'', and an ultrafilter ''U'' on ''B''.\n(The model of ZF constructed like this is not transitive. In practice one  applies the [[Mostowski collapse|Mostowski collapsing theorem]] to turn this into a transitive model.)\n\nWe have seen that forcing can be done using Boolean-valued models, by constructing a Boolean algebra with ultrafilter from a poset with a generic subset. It is also possible to go back the other way: given a Boolean algebra ''B'', we can form a poset ''P'' of all the nonzero elements of ''B'', and a generic ultrafilter on ''B'' restricts to a generic set on ''P''. So the techniques of forcing and Boolean-valued models are essentially equivalent.\n\n==Notes==\n<references/>\n\n==References==\n* Bell, J. L. (1985) ''Boolean-Valued Models and Independence Proofs in Set Theory'', Oxford. {{ISBN|0-19-853241-5}}\n*{{springer|id=b/b016990|first=V.N.|last= Grishin}}\n* {{cite book|authorlink=Thomas Jech|author=Jech, Thomas|title=Set theory, third millennium edition (revised and expanded)|publisher=Springer|year=2002|isbn=3-540-44085-2|oclc=174929965}}\n* {{cite book|title=Set Theory: An Introduction to Independence Proofs|author=Kunen, Kenneth|publisher=North-Holland|year=1980|isbn=0-444-85401-0|oclc=12808956}}\n* {{cite book|author=Kusraev, A. G. and  [[S. S. Kutateladze]]|title=Boolean Valued Analysis|\npublisher=Kluwer Academic Publishers|year=1999|isbn=0-7923-5921-6|oclc=41967176}} Contains an account of Boolean-valued models and applications to Riesz spaces, Banach spaces and algebras. \n* {{cite book|author=Manin, Yu. I.|title=A Course in Mathematical Logic|publisher=Springer|year=1977|isbn=0-387-90243-0|oclc=2797938}} Contains an account of forcing and Boolean-valued models written for mathematicians who are not set theorists.\n* {{cite book|author=Rosser, J. Barkley|title=Simplified Independence Proofs, Boolean valued models of set theory|publisher=Academic Press|year=1969}}\n\n[[Category:Model theory]]\n[[Category:Boolean algebra]]\n[[Category:Forcing (mathematics)]]"
    },
    {
      "title": "Booleo",
      "url": "https://en.wikipedia.org/wiki/Booleo",
      "text": "{{more sources|date=January 2013}}\n'''Booleo''' (stylized ''bOOleO'') is a strategy card game using [[George Boole|boolean]] [[logic gate]]s. It was developed by Jonathan Brandt and Chris Kampf with Sean P. Dennis in 2008, and it was first published by Tessera Games LLC in 2009.<ref>http://boardgamegeek.com/boardgame/40943/booleo</ref>\n\n==Game==\nThe deck consists of 64 cards:\n:* 48 “Gate” cards using three Boolean operators [[AND gate|AND]], [[OR gate|OR]], and [[XOR Gate|XOR]]\n:::8 '''OR''' cards resolving to 1\n:::8 '''OR''' cards resolving to 0\n:::8 '''AND''' cards resolving to 1\n:::8 '''AND''' cards resolving to 0\n:::8 '''XOR''' cards resolving to 1\n:::8 '''XOR''' cards resolving to 0\n\n:* 8 '''[[NOT gate|NOT]]''' cards\n\n:* 6 '''Initial Binary''' cards, each displaying a “0” and a “1” aligned to the two short ends of the card\n\n:* 2 '''[[Truth table|Truth Tables]]''' (used for reference, not in play)\n\n==Play==\nStarting with a line of Initial Binary cards laid perpendicular to two facing players, the object of the game is to be the first to complete a logical pyramid whose final output equals that of the rightmost Initial Binary card facing that player.\n\nThe game is played in “draw one play one” format. The pyramid consists of decreasing rows of gate cards, where the outputs of any contiguous pair of cards comprise the input values to a single card in the following row. The pyramid, therefore, has Initial Binary values as its base and tapers to a single card closest to the player. By tracing the “flow” of values through any series of gate, every card placed in the pyramid must make “logical sense”, i.e. the inputs and output value of every gate card must conform to the rule of that gate card.\n\nThe '''NOT''' cards are played against any of the Initial Binary cards in play, causing that card to be rotated 180 degrees, literally “flipping” the value of that card from 0 to 1 or vice versa.\n\nBy changing the value of any Initial Binary, any and all gate cards which “flow” from it must be re-evaluated to ensure its placement makes “logical sense”. If it does not, that gate card is removed from the player's pyramid.\n\nSince both players' pyramids share the Initial Binary cards as a base, “flipping” an Initial Binary has an effect on both players' pyramids. A principal strategy during game play is to invalidate gate cards in the opponent's logic pyramid while rendering as little damage to one’s own pyramid in the process.\n\nSome logic gates are more robust than others to a change to their inputs. Therefore, not all logic gate cards have the same strategic value. \n\nThe standard edition of the game does not contain NAND, NOR, or XNOR gates. It is possible, therefore, for a player to arrive at an unresolvable pair of inputs.<ref>{{cite web|last1=Somma|first1=Ryan|title=A Game of Boolean Logic Gates with an Ambiguous Spelling|url=http://ideonexus.com/2010/02/15/b00le0-booleo-booleo-a-game-of-boolean-logic-gates-with-an-ambiguous-spelling|website=Geeking Out|accessdate=9 August 2017}}</ref>\n\n==Variations==\nThe number of cards in Booleo will comfortably support a match between two players whose logic pyramids are six cards wide at their base. By combining decks, it is possible to construct larger pyramids or to have matches among more than two players. For example:\n:* Four players may play individually or as facing teams by arranging a cross of Initial Binary cards, where four logic pyramids extend like compass points in four directions\n:* Four or more players may build partially overlapping pyramids from a long base of Initial Binary cards\n\nTessera Games also published ''bOOleO-N Edition'', which is identical to Booleo with the exception that it uses the inverse set of logic gates: [[NAND gate|NAND]], [[NOR Gate|NOR]], and [[XNOR Gate|XNOR]]. bOOleO-N Edition may be played on its own, or it may be combined with Booleo.\n\n==References==\n{{reflist}}\n\n[[Category:Card games introduced in 2009]]\n[[Category:Boolean algebra]]\n[[Category:Matching card games]]"
    },
    {
      "title": "Canonical normal form",
      "url": "https://en.wikipedia.org/wiki/Canonical_normal_form",
      "text": "{{multiple issues|\n{{Tone|date=February 2009}}\n{{Refimprove|date=October 2010}}\n* This article '''presents an incomplete view of the subject'''.\n}}\n{{anchor|Minterm|Maxterm}}In [[Boolean algebra (logic)|Boolean algebra]], any [[Boolean function]] can be put into the '''canonical disjunctive normal form''' ('''[[Disjunctive normal form|CDNF]]''')<ref name=\"PahlDamrath2012\">{{cite book|author1=Peter J. Pahl|author2=Rudolf Damrath|title=Mathematical Foundations of Computational Engineering: A Handbook|url=https://books.google.com/books?id=FRfrCAAAQBAJ&pg=PA15&dq=%22Canonical+disjunctive+normal+form%22&hl=en&sa=X&ved=0ahUKEwiKpd3r__PiAhWBv54KHTDlDMMQ6AEIKjAA#v=onepage&q=%22Canonical%20disjunctive%20normal%20form%22&f=false|date=6 December 2012|publisher=Springer Science & Business Media|isbn=978-3-642-56893-0|pages=15–}}</ref> or '''minterm canonical form''' and its dual '''canonical conjunctive normal form''' ('''[[Conjunctive normal form|CCNF]]''') or '''maxterm canonical form'''.  Other canonical forms include the complete sum of prime implicants or [[Blake canonical form]] (and its dual), and the [[algebraic normal form]]  (also called Zhegalkin or Reed–Muller).\n\n''Minterms'' are called products because they are the [[logical AND]] of a set of variables, and ''maxterms'' are called sums because they are the [[logical OR]] of a set of variables. These concepts are dual because of their complementary-symmetry relationship as expressed by [[De Morgan's laws]].\n\nTwo dual canonical forms of ''any'' Boolean function are a \"sum of minterms\" and a \"product of maxterms.\" The term \"'''Sum of Products'''\" or \"'''SoP'''\" is widely used for the canonical form that is a disjunction (OR) of minterms. Its [[De Morgan dual]] is a \"'''Product of Sums'''\" or \"'''PoS'''\" for the canonical form that is  a conjunction (AND) of maxterms. These forms can be useful for the simplification of these functions, which is of great importance in the optimization of Boolean formulas in general and digital circuits in particular.\n\n==Summary==\nOne application of Boolean algebra is digital circuit design.  The goal may be to minimize the number of gates, to minimize the settling time, etc.\n\nThere are sixteen possible functions of two variables, but in digital logic hardware, the simplest gate circuits implement only four of them: ''[[logical conjunction|conjunction]]'' (AND), ''[[logical disjunction|disjunction]]'' (inclusive OR), and the respective complements of those (NAND and NOR).\n\nMost gate circuits accept more than 2 input variables; for example, the spaceborne [[Apollo Guidance Computer]], which pioneered the application of integrated circuits in the 1960s, was built with only one type of gate, a 3-input NOR, whose output is true only when all 3 inputs are false.<ref>{{cite book |first= Eldon C. |last= Hall |title= Journey to the Moon: The History of the Apollo Guidance Computer |publisher= AIAA |date= 1996 |isbn= 1-56347-185-X }}</ref>\n\n==Minterms==\nFor a [[boolean function]] of <math>n</math> variables <math>{x_1,\\dots,x_n}</math>, a [[product term]] in which each of the <math>n</math> variables appears '''once''' (either in its complemented or uncomplemented form) is called a ''minterm''. Thus, a ''minterm'' is a logical expression of ''n'' variables that employs only the ''complement'' operator and the ''conjunction'' operator.\n\nFor example, <math>abc</math>, <math>ab'c</math> and <math>abc'</math> are 3 examples of the 8 minterms for a Boolean function of the three variables <math>a</math>, <math>b</math>, and <math>c</math>. The customary reading of the last of these is ''a AND b AND NOT-c''.\n\nThere are 2<sup>''n''</sup> minterms of ''n'' variables, since a variable in the minterm expression can be in either its direct or its complemented form—two choices per variable.\n\n=== Indexing minterms ===\n\nMinterms are often numbered by a binary encoding of the complementation pattern of the variables, where the variables are written in a standard order, usually alphabetical. This convention assigns the value 1 to the direct form (<math>x_i</math>) and 0 to the complemented form (<math>x'_i</math>); the minterm is then <math>\\sum\\limits_{i=1}^n2^i\\operatorname{value}(x_i)</math>. For example, minterm <math>a b c'</math> is numbered 110<sub>2</sub>&nbsp;=&nbsp;6<sub>10</sub> and denoted <math>m_6</math>.\n\n===Functional equivalence===\nA given minterm ''n'' gives a true value (i.e., 1) for just one combination of the input variables. For example, minterm 5, ''a'' ''b''<nowiki>'</nowiki> ''c'', is true only when ''a'' and ''c'' both are true and ''b'' is false—the input arrangement where ''a'' = 1, ''b'' = 0, ''c'' = 1 results in 1.\n\nGiven the [[truth table]] of a logical function, it is possible to write the function as a \"sum of products\". This is a special form of [[disjunctive normal form]]. For example, if given the truth table for the arithmetic sum bit ''u'' of one bit position's logic of an adder circuit, as a function of ''x'' and ''y'' from the addends and the carry in, ''ci'':\n\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|u(ci,x,y)\n|-\n|0||0||0||0\n|-\n|0||0||1||1\n|-\n|0||1||0||1\n|-\n|0||1||1||0\n|-\n|1||0||0||1\n|-\n|1||0||1||0\n|-\n|1||1||0||0\n|-\n|1||1||1||1\n|}\n\nObserving that the rows that have an output of 1 are the 2nd, 3rd, 5th, and 8th, we can write ''u'' as a sum of minterms <math>m_1, m_2, m_4,</math> and <math>m_7</math>. If we wish to verify this: <math> u(ci,x,y) = m_1 + m_2 + m_4 + m_7 = (ci',x',y)+(ci',x,y') + (ci,x',y')+(ci,x,y)</math> evaluated for all 8 combinations of the three variables will match the table.\n\n== Maxterms ==\nFor a [[boolean function]] of {{mvar|n}} variables <math>{x_1,\\dots,x_n}</math>, a sum term in which each of the {{mvar|n}} variables appears '''once''' (either in its complemented or uncomplemented form) is called a ''maxterm''. Thus, a ''maxterm'' is a logical expression of {{mvar|n}} variables that employs only the ''complement''  operator and the ''disjunction'' operator. Maxterms are a dual of the minterm idea (i.e., exhibiting a complementary symmetry in all respects). Instead of using ANDs and complements, we use ORs and complements and proceed similarly.\n\nFor example, the following are two of the eight maxterms of three variables:\n: ''a'' + ''b''&prime; + ''c''\n: ''a''&prime; + ''b'' + ''c''\n\nThere are again 2<sup>''n''</sup> maxterms of {{mvar|n}} variables, since a variable in the maxterm expression can also be in either its direct or its complemented form—two choices per variable.\n\n=== Indexing maxterms===\nEach maxterm is assigned an index based on the opposite conventional binary encoding used for minterms. The maxterm convention assigns the value 0 to the direct form <math>(x_i)</math> and 1 to the complemented form <math>(x'_i)</math>. For example, we assign the index 6 to the maxterm <math>a' + b' + c</math> (110) and denote that maxterm as ''M''<sub>6</sub>. Similarly ''M''<sub>0</sub> of these three variables is <math>a + b + c</math> (000) and ''M''<sub>7</sub> is <math>a' + b' + c'</math> (111).\n\n===Functional equivalence===\nIt is apparent that maxterm ''n'' gives a ''false'' value (i.e., 0) for just one combination of the input variables. For example, maxterm 5, ''a''&prime; + ''b'' + ''c''&prime;, is false only when ''a'' and ''c'' both are true and ''b'' is false—the input arrangement where a = 1, b = 0, c = 1 results in 0.\n\nIf one is given a [[truth table]] of a logical function, it is possible to write the function as a \"product of sums\". This is a special form of [[conjunctive normal form]]. For example, if given the truth table for the carry-out bit ''co'' of one bit position's logic of an adder circuit, as a function of ''x'' and ''y'' from the addends and the carry in, ''ci'':\n\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|co(ci,x,y)\n|-\n|0||0||0||0\n|-\n|0||0||1||0\n|-\n|0||1||0||0\n|-\n|0||1||1||1\n|-\n|1||0||0||0\n|-\n|1||0||1||1\n|-\n|1||1||0||1\n|-\n|1||1||1||1\n|}\n\nObserving that the rows that have an output of 0 are the 1st, 2nd, 3rd, and 5th, we can write ''co'' as a product of maxterms <math>M_0, M_1, M_2</math> and <math>M_4</math>. If we wish to verify this:\n:<math>co(ci, x, y) = M_0 M_1 M_2 M_4 = (ci + x + y) (ci + x + y') (ci + x' + y) (ci' + x + y)</math>\nevaluated for all 8 combinations of the three variables will match the table.\n\n==Dualization==\nThe complement of a minterm is the respective maxterm. This can be easily verified by using [[de Morgan's law]]. For example:\n<math>M_5 = a' + b + c' = (a b' c)' = m_5'</math>\n\n==Non-canonical PoS and SoP forms==\nIt is often the case that the canonical minterm form can be simplified to an equivalent SoP form.\nThis simplified form would still consist of a sum of product terms. However, in the simplified form,\nit is possible to have fewer product terms and/or product terms that contain fewer variables.\nFor example, the following 3-variable function:\n\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|a\n!width=\"50\"|b\n!width=\"50\"|c\n!width=\"50\"|f(a,b,c)\n|-\n|0||0||0||0\n|-\n|0||0||1||0\n|-\n|0||1||0||0\n|-\n|0||1||1||1\n|-\n|1||0||0||0\n|-\n|1||0||1||0\n|-\n|1||1||0||0\n|-\n|1||1||1||1\n|}\n\nhas the canonical minterm representation:\n<math>f = a'bc + abc</math>, but it has an equivalent simplified form:\n<math>f = bc</math>.\nIn this trivial example, it is obvious that <math>bc = a'bc + abc</math>, but the simplified form has both fewer product terms,\nand the term has fewer variables.\nThe most simplified SoP representation of a function is referred to as a ''minimal SoP form''.\n\nIn a similar manner, a canonical maxterm form can have a simplified PoS form.\n\nWhile this example was easily simplified by applying normal algebraic methods [<math>f = (a' + a) b c</math>], in less obvious cases a convenient method for finding the minimal PoS/SoP form of a function with up to four variables is using a [[Karnaugh map]].\n\nThe minimal PoS and SoP forms are very important for finding optimal implementations of boolean functions\nand minimizing logic circuits.\n\n==Application example==\nThe sample truth tables for minterms and maxterms above are sufficient to establish the canonical form for a single bit position in the addition of binary numbers, but are not sufficient to design the digital logic unless your inventory of gates includes AND and OR. Where performance is an issue (as in the Apollo Guidance Computer), the available parts are more likely to be NAND and NOR because of the complementing action inherent in transistor logic. The values are defined as voltage states, one near ground and one near the DC supply voltage V<sub>cc</sub>, e.g. +5 VDC. If the higher voltage is defined as the 1 \"true\" value, a NOR gate is the simplest possible useful logical element.\n\nSpecifically, a 3-input NOR gate may consist of 3 bipolar junction transistors with their emitters all grounded, their collectors tied together and linked to V<sub>cc</sub> through a load impedance. Each base is connected to an input signal, and the common collector point presents the output signal.  Any input that is a 1 (high voltage) to its base shorts its transistor's emitter to its collector, causing current to flow through the load impedance, which brings the collector voltage (the output) very near to ground. That result is independent of the other inputs. Only when all 3 input signals are 0 (low voltage) do the emitter-collector impedances of all 3 transistors remain very high.  Then very little current flows, and the voltage-divider effect with the load impedance imposes on the collector point a high voltage very near to V<sub>cc</sub>.\n\nThe complementing property of these gate circuits may seem like a drawback when trying to implement a function in canonical form, but there is a compensating bonus: such a gate with only one input implements the complementing function, which is required frequently in digital logic.\n\nThis example assumes the Apollo parts inventory: 3-input NOR gates only, but the discussion is simplified by supposing that 4-input NOR gates are also available (in Apollo, those were compounded out of pairs of 3-input NORs).\n\n===Canonical and non-canonical consequences of NOR gates===\nFact #1: a set of 8 NOR gates, if their inputs are all combinations of the direct and complement forms of the 3 input variables ''ci, x,'' and ''y'', always produce minterms, never maxterms—that is, of the 8 gates required to process all combinations of 3 input variables, only one has the output value 1.  That's because a NOR gate, despite its name, could better be viewed (using De Morgan's law) as the AND of the complements of its input signals.\n\nFact #2: the reason Fact #1 is not a problem is the duality of minterms and maxterms, i.e. each maxterm is the complement of the like-indexed minterm, and vice versa.\n\nIn the minterm example above, we wrote <math>u(ci, x, y) = m_1 + m_2 + m_4 + m_7</math> but to perform this with a 4-input NOR gate we need to restate it as a product of sums (PoS), where the sums are the opposite maxterms.  That is,\n\n:<math>u(ci, x, y) = \\mathrm{AND}(M_0,M_3,M_5,M_6) = \\mathrm{NOR}(m_0,m_3,m_5,m_6).</math>\n{| style=\"margin: 1em auto 1em auto\"\n|+ '''Truth tables'''\n|\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|M<sub>0</sub>\n!width=\"50\"|M<sub>3</sub>\n!width=\"50\"|M<sub>5</sub>\n!width=\"50\"|M<sub>6</sub>\n!width=\"50\"|AND\n!width=\"50\"|u(ci,x,y)\n|-\n|0||0||0||0||1||1||1||0||0\n|-\n|0||0||1||1||1||1||1||1||1\n|-\n|0||1||0||1||1||1||1||1||1\n|-\n|0||1||1||1||0||1||1||0||0\n|-\n|1||0||0||1||1||1||1||1||1\n|-\n|1||0||1||1||1||0||1||0||0\n|-\n|1||1||0||1||1||1||0||0||0\n|-\n|1||1||1||1||1||1||1||1||1\n|}\n|-\n|\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|m<sub>0</sub>\n!width=\"50\"|m<sub>3</sub>\n!width=\"50\"|m<sub>5</sub>\n!width=\"50\"|m<sub>6</sub>\n!width=\"50\"|NOR\n!width=\"50\"|u(ci,x,y)\n|-\n|0||0||0||1||0||0||0||0||0\n|-\n|0||0||1||0||0||0||0||1||1\n|-\n|0||1||0||0||0||0||0||1||1\n|-\n|0||1||1||0||1||0||0||0||0\n|-\n|1||0||0||0||0||0||0||1||1\n|-\n|1||0||1||0||0||1||0||0||0\n|-\n|1||1||0||0||0||0||1||0||0\n|-\n|1||1||1||0||0||0||0||1||1\n|}\n|}\nIn the maxterm example above, we wrote <math>co(ci, x, y) = M_0 M_1 M_2 M_4</math> but to perform this with a 4-input NOR gate we need to notice the equality to the NOR of the same minterms.  That is,\n\n:<math>co(ci, x, y) = \\mathrm{AND}(M_0,M_1,M_2,M_4) = \\mathrm{NOR}(m_0,m_1,m_2,m_4).</math>\n\n{| style=\"margin: 1em auto 1em auto\"\n|+ '''Truth tables'''\n|\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|M<sub>0</sub>\n!width=\"50\"|M<sub>1</sub>\n!width=\"50\"|M<sub>2</sub>\n!width=\"50\"|M<sub>4</sub>\n!width=\"50\"|AND\n!width=\"50\"|co(ci,x,y)\n|-\n|0||0||0||0||1||1||1||0||0\n|-\n|0||0||1||1||0||1||1||0||0\n|-\n|0||1||0||1||1||0||1||0||0\n|-\n|0||1||1||1||1||1||1||1||1\n|-\n|1||0||0||1||1||1||0||0||0\n|-\n|1||0||1||1||1||1||1||1||1\n|-\n|1||1||0||1||1||1||1||1||1\n|-\n|1||1||1||1||1||1||1||1||1\n|}\n|-\n|\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|m<sub>0</sub>\n!width=\"50\"|m<sub>1</sub>\n!width=\"50\"|m<sub>2</sub>\n!width=\"50\"|m<sub>4</sub>\n!width=\"50\"|NOR\n!width=\"50\"|co(ci,x,y)\n|-\n|0||0||0||1||0||0||0||0||0\n|-\n|0||0||1||0||1||0||0||0||0\n|-\n|0||1||0||0||0||1||0||0||0\n|-\n|0||1||1||0||0||0||0||1||1\n|-\n|1||0||0||0||0||0||1||0||0\n|-\n|1||0||1||0||0||0||0||1||1\n|-\n|1||1||0||0||0||0||0||1||1\n|-\n|1||1||1||0||0||0||0||1||1\n|}\n|}\n\n===Design trade-offs considered in addition to canonical forms===\nOne might suppose that the work of designing an adder stage is now complete, but we haven't addressed the fact that all 3 of the input variables have to appear in both their direct and complement forms.  There's no difficulty about the addends ''x'' and ''y'' in this respect, because they are static throughout the addition and thus are normally held in latch circuits that routinely have both direct and complement outputs. (The simplest latch circuit made of NOR gates is a pair of gates cross-coupled to make a flip-flop: the output of each is wired as one of the inputs to the other.) There is also no need to create the complement form of the sum ''u''. However, the carry out of one bit position must be passed as the carry into the next bit position in both direct and complement forms.  The most straightforward way to do this is to pass ''co'' through a 1-input NOR gate and label the output ''co''&prime;, but that would add a gate delay in the worst possible place, slowing down the rippling of carries from right to left. An additional 4-input NOR gate building the canonical form of ''co''&prime; (out of the opposite minterms as ''co'') solves this problem.\n\n: <math>co'(ci, x, y) = \\mathrm{AND}(M_3,M_5,M_6,M_7) = \\mathrm{NOR}(m_3,m_5,m_6,m_7).</math>\n{| style=\"margin: 1em auto 1em auto\"\n|+ '''Truth tables'''\n|\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|M<sub>3</sub>\n!width=\"50\"|M<sub>5</sub>\n!width=\"50\"|M<sub>6</sub>\n!width=\"50\"|M<sub>7</sub>\n!width=\"50\"|AND\n!width=\"50\"|co'(ci,x,y)\n|-\n|0||0||0||1||1||1||1||1||1\n|-\n|0||0||1||1||1||1||1||1||1\n|-\n|0||1||0||1||1||1||1||1||1\n|-\n|0||1||1||0||1||1||1||0||0\n|-\n|1||0||0||1||1||1||1||1||1\n|-\n|1||0||1||1||0||1||1||0||0\n|-\n|1||1||0||1||1||0||1||0||0\n|-\n|1||1||1||1||1||1||0||0||0\n|}\n|-\n|\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|ci\n!width=\"50\"|x\n!width=\"50\"|y\n!width=\"50\"|m<sub>3</sub>\n!width=\"50\"|m<sub>5</sub>\n!width=\"50\"|m<sub>6</sub>\n!width=\"50\"|m<sub>7</sub>\n!width=\"50\"|NOR\n!width=\"50\"|co'(ci,x,y)\n|-\n|0||0||0||0||0||0||0||1||1\n|-\n|0||0||1||0||0||0||0||1||1\n|-\n|0||1||0||0||0||0||0||1||1\n|-\n|0||1||1||1||0||0||0||0||0\n|-\n|1||0||0||0||0||0||0||1||1\n|-\n|1||0||1||0||1||0||0||0||0\n|-\n|1||1||0||0||0||1||0||0||0\n|-\n|1||1||1||0||0||0||1||0||0\n|}\n|}\nThe trade-off to maintain full speed in this way includes an unexpected cost (in addition to having to use a bigger gate). If we'd just used that 1-input gate to complement ''co'', there would have been no use for the minterm <math>m_7</math>, and the gate that generated it could have been eliminated. Nevertheless, it's still a good trade.\n\nNow we could have implemented those functions exactly according to their SoP and PoS canonical forms, by turning NOR gates into the functions specified.  A NOR gate is made into an OR gate by passing its output through a 1-input NOR gate; and it is made into an AND gate by passing each of its inputs through a 1-input NOR gate.  However, this approach not only increases the number of gates used, but also doubles the number of gate delays processing the signals, cutting the processing speed in half.  Consequently, whenever performance is vital, going beyond canonical forms and doing the Boolean algebra to make the unenhanced NOR gates do the job is well worthwhile.\n\n===Top-down vs. bottom-up design===\nWe have now seen how the minterm/maxterm tools can be used to design an adder stage in canonical form with the addition of some Boolean algebra, costing just 2 gate delays for each of the outputs. That's the \"top-down\" way to design the digital circuit for this function, but is it the best way? The discussion has focused on identifying \"fastest\" as \"best,\" and the augmented canonical form meets that criterion flawlessly, but sometimes other factors predominate. The designer may have a primary goal of minimizing the number of gates, and/or of minimizing the fanouts of signals to other gates since big fanouts reduce resilience to a degraded power supply or other environmental factors. In such a case, a designer may develop the canonical-form design as a baseline, then try a bottom-up development, and finally compare the results.\n\nThe bottom-up development involves noticing that ''u = ci'' XOR (''x'' XOR ''y''), where XOR means eXclusive OR [true when either input is true but not when both are true], and that ''co'' = ''ci x'' + ''x y'' + ''y ci''. One such development takes twelve NOR gates in all: six 2-input gates and two 1-input gates to produce ''u'' in 5 gate delays, plus three 2-input gates and one 3-input gate to produce ''co''&prime; in 2 gate delays. The canonical baseline took eight 3-input NOR gates plus three 4-input NOR gates to produce ''u, co'' and ''co''&prime; in 2 gate delays. If the circuit inventory actually includes 4-input NOR gates, the top-down canonical design looks like a winner in both gate count and speed. But if (contrary to our convenient supposition) the circuits are actually 3-input NOR gates, of which two are required for each 4-input NOR function, then the canonical design takes 14 gates compared to 12 for the bottom-up approach, but still produces the sum digit ''u'' considerably faster.  The fanout comparison is tabulated as:\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n!width=\"50\"|Variables\n!width=\"50\"|Top-down\n!width=\"50\"|Bottom-up\n|-\n! x\n|4||1\n|-\n! x'\n|4||3\n|-\n! y\n|4||1\n|-\n! y'\n|4||3\n|-\n! ci\n|4||1\n|-\n! ci'\n|4||3\n|-\n! M or m\n|4@1,4@2||N/A\n|-\n! x XOR y\n|N/A||2\n|-\n! Misc\n|N/A||5@1\n|-\n! Max\n|4||3\n|}\nWhat's a decision-maker to do?  An observant one will have noticed that the description of the bottom-up development mentions ''co''&prime; as an output but not ''co''.  Does that design simply never need the direct form of the carry out? Well, yes and no. At each stage, the calculation of ''co''&prime; depends only on ''ci''&prime;, ''x''&prime; and ''y''&prime;, which means that the carry propagation ripples along the bit positions just as fast as in the canonical design without ever developing ''co''. The calculation of ''u'', which does require ''ci'' to be made from ''ci''&prime; by a 1-input NOR, is slower but for any word length the design only pays that penalty once (when the leftmost sum digit is developed). That's because those calculations overlap, each in what amounts to its own little pipeline without affecting when the next bit position's sum bit can be calculated. And, to be sure, the ''co''&prime; out of the leftmost bit position will probably have to be complemented as part of the logic determining whether the addition overflowed. But using 3-input NOR gates, the bottom-up design is very nearly as fast for doing parallel addition on a non-trivial word length, cuts down on the gate count, and uses lower fanouts ... so it wins if gate count and/or fanout are paramount!\n\nWe'll leave the exact circuitry of the bottom-up design of which all these statements are true as an exercise for the interested reader, assisted by one more algebraic formula: ''u'' = ''ci''(''x'' XOR ''y'') + ''ci''&prime;(''x'' XOR ''y'')&prime;]&prime;. Decoupling the carry propagation from the sum formation in this way is what elevates the performance of a ''carry-lookahead adder'' over that of a ''ripple carry adder''.\n\nTo see how NOR gate logic was used in the Apollo Guidance Computer's ALU, visit http://klabs.org/history/ech/agc_schematics/index.htm, select any of the 4-BIT MODULE entries in the Index to Drawings, and expand images as desired.\n\n==See also==\n* [[Algebraic normal form]]\n* [[Canonical form]]\n* [[Blake canonical form]]\n* [[List of Boolean algebra topics]]\n\n== Footnotes ==\n{{reflist}}\n\n== References ==\n* {{cite book |first1= Edward A. |last1= Bender |first2= S. Gill |last2= Williamson |date= 2005 |title= A Short Course in Discrete Mathematics |publisher= Dover Publications, Inc. |location= Mineola, NY |isbn= 0-486-43946-1 |quote= <br />The authors demonstrate a proof that any Boolean (logic) function can be expressed in either disjunctive or conjunctive normal form (cf pages 5–6); the proof simply proceeds by creating all 2<sup>''N''</sup> rows of ''N'' Boolean variables and demonstrates that each row (\"minterm\" or \"maxterm\") has a unique Boolean expression. Any Boolean function of the ''N'' variables can be derived from a composite of the rows whose minterm or maxterm are logical 1s (\"trues\") }}\n* {{cite book |first= E. J. |last= McCluskey |date= 1965 |title= Introduction to the Theory of Switching Circuits |publisher= McGraw–Hill Book Company |location= NY |lccn= 65-17394 |quote= Canonical expressions are defined and described |page= 78 }}\n* {{cite book |first1= Fredrick J. |last1= Hill |first2= Gerald R. |last2= Peterson |date= 1974 |title= Introduction to Switching Theory and Logical Design |edition= 2nd |publisher= John Wiley & Sons |location= NY |isbn= 0-471-39882-9 |quote= Minterm and maxterm designation of functions |page= 101 }}\n\n==External links==\n{{wikibooks|Electronics|Boolean Algebra}}\n* {{cite journal |author-link= George Boole |first= George |last= Boole |date= 1848 |url= https://www.maths.tcd.ie/pub/HistMath/People/Boole/CalcLogic/CalcLogic.html |title= The Calculus of Logic |journal= Cambridge and Dublin Mathematical Journal |volume= III |pages= 183&mdash;198 |translator-first= David R. |translator-last= Wilkins }}\n\n{{Digital systems}}\n\n{{DEFAULTSORT:Canonical Form (Boolean Algebra)}}\n[[Category:Boolean algebra]]\n[[Category:Logic]]\n[[Category:Algebraic logic]]\n[[Category:Articles with example code]]"
    },
    {
      "title": "Cantor algebra",
      "url": "https://en.wikipedia.org/wiki/Cantor_algebra",
      "text": "{{for|the algebras encoding a bijection from an infinite set ''X'' onto the product ''X''×''X'', sometimes called Cantor algebras |Jónsson–Tarski algebra}}\nIn mathematics, a '''Cantor algebra''', named after [[Georg Cantor]], is one of two closely related Boolean algebras, one countable and one complete.\n\nThe countable Cantor algebra is the  Boolean algebra of all clopen subsets of the [[Cantor set]]. This is the free Boolean algebra on a countable number of generators. Up to isomorphism, this is the only nontrivial Boolean algebra that is both countable and atomless.\n\nThe complete Cantor algebra is the complete Boolean algebra of [[Borel subset]]s of the reals modulo [[meager set]]s {{harv|Balcar|Jech|2006}}. It is isomorphic to the completion of the countable Cantor algebra. (The complete Cantor algebra is sometimes called the Cohen algebra, though \"[[Cohen algebra]]\" usually refers to a different type of Boolean algebra.) The complete Cantor algebra was studied by von Neumann in 1935 (later published as {{harv|von Neumann|1998}}), who showed that it is not isomorphic to the [[random algebra]] of Borel subsets modulo measure zero sets.\n\n==References==\n*{{citation\n | last1 = Balcar | first1 = Bohuslav | author1-link = Bohuslav Balcar\n | last2 = Jech | first2 = Thomas | author2-link = Thomas Jech\n | issue = 2\n | journal = [[Bulletin of Symbolic Logic]]\n | mr = 2223923\n | pages = 241–266\n | title = Weak distributivity, a problem of von Neumann and the mystery of measurability\n | url = http://www.math.ucla.edu/~asl/bsl/1202-toc.htm\n | volume = 12\n | year = 2006}}\n*{{Citation | last1=von Neumann | first1=John | author1-link=John von Neumann | title=Continuous geometry | origyear=1960 | url=https://books.google.com/books?id=onE5HncE-HgC | publisher=[[Princeton University Press]] | series=Princeton Landmarks in Mathematics | isbn=978-0-691-05893-1 | mr=0120174 | year=1998}}\n \n[[Category:Forcing (mathematics)]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Chaff algorithm",
      "url": "https://en.wikipedia.org/wiki/Chaff_algorithm",
      "text": "{{no footnotes|date=July 2017}}\n'''Chaff''' is an [[algorithm]] for solving instances of the [[Boolean satisfiability problem]] in programming. It was designed by researchers at [[Princeton University]], United States. The algorithm is an instance of the [[DPLL algorithm]] with a number of enhancements for efficient implementation.\n\n== Implementations ==\n\nSome available implementations of the algorithm in software are mChaff and '''zChaff''', the latter one being the most widely known and used. zChaff was originally written by Dr. Lintao Zhang, {{clarify span|now|reason=Since when?|date=April 2019}} at [[Microsoft Research]], hence the “z”. It is now maintained by researchers at [[Princeton University]] and available for [[download]] as both source code and binaries on [[Linux]]. zChaff is free for non-commercial use.\n\n== References ==\n{{reflist}}\n* M. Moskewicz, C. Madigan, Y. Zhao, L. Zhang, S. Malik. ''[http://www.princeton.edu/~chaff/publication/DAC2001v56.pdf Chaff: Engineering an Efficient SAT Solver]'', 39th Design Automation Conference (DAC 2001), Las Vegas, ACM 2001.\n* {{Cite journal | last1 = Vizel | first1 = Y. | last2 = Weissenbacher | first2 = G. | last3 = Malik | first3 = S. | journal = Proceedings of the IEEE | volume = 103 | issue = 11 | year = 2015 | doi = 10.1109/JPROC.2015.2455034|title=Boolean Satisfiability Solvers and Their Applications in Model Checking}}\n{{refend}}\n\n== External links ==\n* [http://www.princeton.edu/~chaff/zchaff.html Web page about zChaff]\n\n[[Category:SAT solvers]]\n[[Category:Boolean algebra]]\n[[Category:Automated theorem proving]]\n[[Category:Constraint programming]]\n\n\n{{formalmethods-stub}}"
    },
    {
      "title": "Cohen algebra",
      "url": "https://en.wikipedia.org/wiki/Cohen_algebra",
      "text": "{{distinguish|Cohen ring|Rankin–Cohen algebra}}\n{{for|the quotient of the algebra of Borel sets by the ideal of meager sets, sometimes called the Cohen algebra|Cantor algebra}}\n\nIn mathematical [[set theory]], a '''Cohen algebra''', named after [[Paul Cohen (mathematician)|Paul Cohen]], is a type of [[Boolean algebra (structure)|Boolean algebra]] used in the theory of [[Forcing (mathematics)|forcing]].  A Cohen algebra is a Boolean algebra whose [[Completion (ring theory)|completion]] is isomorphic to the completion of a [[free Boolean algebra]] {{harv|Koppelberg|1993}}.  \n\n==References==\n\n*{{citation\n | last = Koppelberg | first = Sabine\n | contribution = Characterizations of Cohen algebras\n | doi = 10.1111/j.1749-6632.1993.tb52525.x\n | mr = 1277859\n | pages = 222–237\n | publisher = [[New York Academy of Sciences]]\n | series = Annals of the New York Academy of Sciences\n | title = Papers on general topology and applications (Madison, WI, 1991)\n | volume = 704\n | year = 1993}}\n \n[[Category:Forcing (mathematics)]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Collapsing algebra",
      "url": "https://en.wikipedia.org/wiki/Collapsing_algebra",
      "text": "In mathematics, a '''collapsing algebra''' is a type of [[Boolean algebra (structure)|Boolean algebra]] sometimes used in [[Forcing (mathematics)|forcing]] to reduce (\"collapse\") the size of [[Cardinal number|cardinals]]. The [[poset]]s used to generate collapsing algebras were introduced by {{harvs|txt|first=Azriel|last= Lévy|authorlink=Azriel Lévy|year=1963}}.\n\nThe collapsing algebra of λ<sup>ω</sup> is a [[complete Boolean algebra]] with at least λ elements but generated by a countable number of elements. As the size of countably generated complete Boolean algebras is unbounded, this shows that there is no [[Free algebra|free]] complete Boolean algebra on a countable number of elements.\n\n==Definition==\n\nThere are several slightly different sorts of collapsing algebras. \n\nIf κ and λ are cardinals, then the Boolean algebra of [[regular open set]]s of the [[product space]] κ<sup>λ</sup> is a collapsing algebra. Here κ and λ are both given the [[discrete topology]]. There are several different options for the topology of \nκ<sup>λ</sup>. The simplest option is to take the usual product topology. Another option is to take the topology generated by open sets consisting of functions whose value is specified on less than λ elements of λ.\n\n==References==\n\n* {{cite book | last=Bell | first=J. L. | year=1985 | title=Boolean-Valued Models and Independence Proofs in Set Theory | edition=2nd | location=Oxford | publisher=Oxford University Press (Clarendon Press) | series=Oxford Logic Guides | volume=12 | isbn=0-19-853241-5 | zbl=0585.03021 }}\n* {{cite book | authorlink=Thomas Jech|last=Jech | first=Thomas| title=Set theory | edition=third millennium (revised and expanded) | publisher=[[Springer-Verlag]] | year=2003 | isbn=3-540-44085-2 | oclc=174929965 | zbl=1007.03002}}\n* {{cite journal | last=Lévy | first=Azriel |title=Independence results in set theory by Cohen's method. IV, | journal=Notices Amer. Math. Soc. |volume=10 |year=1963|page= 593}} \n\n[[Category:Boolean algebra]]\n[[Category:Forcing (mathematics)]]"
    },
    {
      "title": "Complete Boolean algebra",
      "url": "https://en.wikipedia.org/wiki/Complete_Boolean_algebra",
      "text": "{{About|a type of mathematical structure|complete sets of Boolean operators|Functional completeness}}\n\nIn [[mathematics]], a '''complete Boolean algebra''' is a [[Boolean algebra (structure)|Boolean algebra]] in which every [[subset]] has a [[supremum]] (least [[upper bound]]).  Complete Boolean algebras are used to construct [[Boolean-valued model]]s of set theory in the theory of [[forcing (mathematics)|forcing]].  Every Boolean algebra ''A'' has an essentially unique completion, which is a complete Boolean algebra containing ''A'' such that every element is the supremum of some subset of ''A''.  As a [[partially ordered set]], this completion of ''A'' is the [[Dedekind–MacNeille completion]].\n\nMore generally, if κ is a cardinal then a Boolean algebra is called '''κ-complete''' if every subset of cardinality less than κ has a supremum.\n\n==Examples==\n*Every [[Finite set|finite]] Boolean algebra is complete.\n*The [[algebra of sets|algebra of subsets]] of a given set is a complete Boolean algebra.\n*The [[regular open set]]s of any [[topological space]] form a complete Boolean algebra. This example is of particular importance because every forcing [[poset]] can be considered as a topological space (a [[base (topology)|base]] for the topology consisting of sets that are the set of all elements less than or equal to a given element). The corresponding regular open algebra can be used to form [[Boolean-valued model]]s which are then equivalent to [[generic extension]]s by the given forcing poset.\n*The algebra of all measurable subsets of a σ-finite measure space, modulo null sets, is a complete Boolean algebra. When the measure space is the unit interval with the σ-algebra of Lebesgue measurable sets, the Boolean algebra is called the [[random algebra]].\n*The algebra of all measurable subsets of a  measure space is a ℵ<sub>1</sub>-complete Boolean algebra, but is not usually complete.\n*The algebra of all subsets of an infinite set that are finite or have finite complement is  a Boolean algebra but is not complete.\n*The Boolean algebra of all [[Baire set]]s modulo [[meager set]]s in a  topological space with a countable base is complete; when the topological space is the real numbers the algebra is sometimes called the [[Cantor algebra]].\n*Another example of a Boolean algebra that is not complete is the Boolean algebra P(ω) of all sets of [[natural number]]s, quotiented out by the ideal ''Fin'' of finite subsets. The resulting object, denoted P(ω)/Fin, consists of all [[equivalence class]]es of sets of naturals, where the relevant [[equivalence relation]] is that two sets of naturals are equivalent if their [[symmetric difference]] is finite. The Boolean operations are defined analogously, for example, if ''A'' and ''B'' are two equivalence classes in P(ω)/Fin, we define <math>A\\land B</math> to be the equivalence class of <math>a\\cap b</math>, where ''a'' and ''b'' are some (any) elements of ''A'' and ''B'' respectively.\n\n:Now let a<sub>0</sub>, a<sub>1</sub>,... be pairwise disjoint infinite sets of naturals, and let ''A''<sub>0</sub>, ''A''<sub>1</sub>,... be their corresponding equivalence classes in P(ω)/Fin .  Then given any upper bound ''X'' of ''A''<sub>0</sub>, ''A''<sub>1</sub>,... in P(ω)/Fin, we can find a ''lesser'' upper bound, by removing from a representative for ''X'' one element of each ''a''<sub>''n''</sub>. Therefore the ''A''<sub>''n''</sub> have no supremum.\n\n*A Boolean algebra is complete if and only if its [[Stone space]] of prime ideals is [[extremally disconnected]].\n\n==Properties of complete Boolean algebras==\n*Sikorski's extension theorem states that\nif ''A'' is a subalgebra of a Boolean algebra ''B'', then any homomorphism from ''A'' to a complete Boolean algebra ''C'' can be extended to a morphism from ''B'' to ''C''.\n\n*Every subset of a complete Boolean algebra has a supremum, by definition; it follows that every subset also has an [[infimum]] (greatest lower bound).\n* For a complete boolean algebra both infinite distributive laws hold.\n* For a complete boolean algebra [[infinite de-Morgan's laws]] hold.\n\n==The completion of a Boolean algebra==\nThe completion of a Boolean algebra can be defined in several equivalent ways:\n*The completion of ''A'' is (up to isomorphism) the unique complete Boolean algebra ''B'' containing ''A'' such that ''A'' is dense in ''B''; this means that for every nonzero element of ''B'' there is a smaller non-zero element of ''A''.\n*The completion of ''A'' is (up to isomorphism) the unique complete Boolean algebra ''B'' containing ''A'' such that every element of ''B'' is the supremum of some subset of ''A''.\n\nThe completion of a Boolean algebra ''A'' can be constructed in several ways:\n*The completion is the Boolean algebra of regular open sets in the [[Stone space]] of prime ideals of ''A''. Each element ''x'' of ''A'' corresponds to the  open set of prime ideals not containing ''x'' (which is open and closed, and therefore regular).\n*The completion is the Boolean algebra of regular cuts of ''A''. Here a ''cut'' is a subset ''U'' of ''A''<sup>+</sup> (the non-zero elements of ''A'') such that if ''q'' is in ''U'' and ''p''≤''q'' then ''p'' is in ''U'', and is called ''regular'' if whenever ''p'' is not in ''U'' there is some ''r'' ≤ ''p'' such that ''U'' has no elements ≤''r''. Each element ''p'' of ''A'' corresponds to the cut of elements ≤''p''.\n\nIf ''A'' is a metric space and ''B'' its completion then any isometry from ''A'' to a complete metric space ''C'' can be extended to a unique isometry from ''B'' to ''C''. The analogous statement for complete Boolean algebras is not true: a homomorphism from a Boolean algebra ''A'' to a complete Boolean algebra ''C'' cannot necessarily be extended to a (supremum preserving) homomorphism of complete Boolean algebras from the completion ''B'' of ''A'' to ''C''. (By Sikorski's extension theorem it can be extended to a homomorphism of Boolean algebras from ''B'' to ''C'', but this will not in general be a homomorphism of complete Boolean algebras; in other words, it need not preserve suprema.)\n\n==Free &kappa;-complete Boolean algebras==\n\nUnless the [[Axiom of Choice]] is relaxed,<ref name=\"Stavi, 1974\">{{Citation| first= Jonathan | last=Stavi| year=1974| url=http://www.springerlink.com/content/d5710380t753621u/ |format=reprint|title=A model of ZF with an infinite free complete Boolean algebra| journal=Israel Journal of Mathematics| volume=20| issue= 2| pages=149–163|doi=10.1007/BF02757883| postscript= .}}</ref> [[Free Boolean algebra|free]] complete boolean algebras generated by a set do not exist (unless the set is finite). More precisely, for any cardinal κ, there is a complete Boolean algebra of cardinality 2<sup>κ</sup> greater than κ that is generated as a complete Boolean algebra by a countable subset; for example the Boolean algebra of regular open sets in the product space κ<sup>ω</sup>, where κ has the discrete topology. A countable generating set consists of all sets ''a''<sub>''m'',''n''</sub> for ''m'', ''n'' integers, consisting of the elements ''x''∈κ<sup>ω</sup> such that ''x''(''m'')<''x''(''n''). (This boolean algebra is called a [[collapsing algebra]], because forcing with it collapses the cardinal κ onto ω.)\n\nIn particular the forgetful functor from complete Boolean algebras to sets has no left adjoint, even though it is continuous and the category of Boolean algebras is small-complete. This shows that the \"solution set condition\" in [[Freyd's adjoint functor theorem]] is necessary.\n\nGiven a set ''X'', one can form the free Boolean algebra ''A'' generated by this set and then take its completion ''B''. However ''B'' is not a \"free\" complete Boolean algebra generated by ''X'' (unless ''X'' is finite or AC is omitted), because a function from ''X'' to a free Boolean algebra ''C'' cannot in general be extended to a (supremum-preserving) morphism of Boolean algebras from ''B'' to ''C''.\n\nOn the other hand, for any fixed cardinal κ, there is a free (or universal) κ-complete Boolean algebra generated by any given set.\n\n==See also==\n\n* [[Complete lattice]]\n* [[Complete Heyting algebra]]\n\n==References==\n<references/>\n*{{citation|first=Peter T.|last= Johnstone\n|year=1982\n|title=Stone spaces\n|publisher=Cambridge University Press\n|isbn =0-521-33779-8}}\n*{{citation|mr=0991565\n|last=Koppelberg|first= Sabine\n|title=Handbook of Boolean algebras |volume=1\n|editor-first= J. Donald|editor-last= Monk|editor2-first= Robert|editor2-last= Bonnet\n|publisher=North-Holland Publishing Co.|publication-place= Amsterdam|year= 1989|pages= xx+312 |isbn= 0-444-70261-X }}\n*{{citation|mr=0991595\n|title=Handbook of Boolean algebras|volume=  2\n|editor-first= J. Donald|editor-last= Monk|editor2-first= Robert|editor2-last= Bonnet\n|publisher=North-Holland Publishing Co.|publication-place= Amsterdam|year= 1989|isbn= 0-444-87152-7 }}\n*{{citation|mr=0991607\n|title=Handbook of Boolean algebras|volume=  3\n|editor-first= J. Donald|editor-last= Monk|editor2-first= Robert|editor2-last= Bonnet\n|publisher=North-Holland Publishing Co.|publication-place= Amsterdam|year= 1989|isbn= 0-444-87153-5  }}\n*{{Springer|id=b/b016920|title=Boolean algebra|first=D.A.|last= Vladimirov}}\n\n[[Category:Boolean algebra]]\n[[Category:Forcing (mathematics)]]\n[[Category:Order theory]]"
    },
    {
      "title": "Consensus theorem",
      "url": "https://en.wikipedia.org/wiki/Consensus_theorem",
      "text": "<!-- This is the truth table shown on the right. Scroll down to edit the rest of the article. !-->\n{| class=\"wikitable\" align=right\n|- bgcolor=\"#ddeeff\" align=\"center\"\n|colspan=3|'''Variable inputs'''\n|colspan=2| '''Function values'''\n|- bgcolor=\"#ddeeff\" align=\"center\"\n| ''x'' || ''y'' || ''z'' || <math>xy \\vee \\bar{x}z \\vee yz</math> || <math>xy \\vee \\bar{x}z</math>\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 0 || 0 || 0 || 0 || 0\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 0 || 0 || 1 || 1 || 1\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 0 || 1 || 0 || 0 || 0\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 0 || 1 || 1 || 1 || 1\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 1 || 0 || 0 || 0 || 0\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 1 || 0 || 1 || 0 || 0\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 1 || 1 || 0 || 1 || 1\n|- bgcolor=\"#ddffdd\" align=\"center\"\n| 1 || 1 || 1 || 1 || 1\n|}\nIn [[Boolean algebra (logic)|Boolean algebra]], the '''consensus theorem''' or '''rule of consensus'''<ref>Frank Markham Brown, ''Boolean Reasoning: The Logic of Boolean Equations'', 2nd edition 2003, p. 44</ref> is the identity:\n\n:<math>xy \\vee \\bar{x}z \\vee yz = xy \\vee \\bar{x}z</math>\n\nThe '''consensus''' or '''resolvent''' of the terms <math>xy</math> and <math>\\bar{x}z</math> is <math>yz</math>.  It is the conjunction of all the unique literals of the terms, excluding the literal that appears unnegated in one term and negated in the other. If <math>y</math> includes a term which is negated in <math>z</math> (or vice versa), the consensus term <math>yz</math> is false; in other words, there is no consensus term.\n\nThe conjunctive [[De Morgan's laws|dual]] of this equation is:\n\n:<math>(x \\vee y)(\\bar{x} \\vee z)(y \\vee z) = (x \\vee y)(\\bar{x} \\vee z)</math>\n\n==Proof==\n:<math> \\begin{align}\n     xy \\vee \\bar{x}z \\vee yz &=  xy \\vee \\bar{x}z \\vee (x \\vee \\bar{x})yz \\\\\n            &=  xy \\vee \\bar{x}z \\vee xyz \\vee \\bar{x}yz \\\\\n            &=  (xy \\vee xyz) \\vee (\\bar{x}z \\vee \\bar{x}yz) \\\\\n            &= xy(1\\vee z)\\vee\\bar{x}z(1\\vee y) \\\\\n            &=  xy \\vee \\bar{x}z\n      \\end{align}     \n</math>\n\n==Consensus==\n{{anchor|Consensus}}{{anchor|Opposition}}\nThe '''consensus''' or '''consensus term''' of two conjunctive terms of a disjunction is defined when one term contains the literal <math>a</math> and the other the literal <math>\\bar{a}</math>, an '''opposition'''.  The consensus is the conjunction of the two terms, omitting both <math>a</math> and <math>\\bar{a}</math>, and repeated literals. For example, the consensus of <math>\\bar{x}yz</math> and <math>w\\bar{y}z</math> is <math>w\\bar{x}z</math>.<ref name=\"brown81\">Frank Markham Brown, ''Boolean Reasoning: The Logic of Boolean Equations'', 2nd edition 2003, p. 81</ref> The consensus is undefined if there is more than one opposition.\n\nFor the conjunctive dual of the rule, the consensus <math>y \\vee z</math> can be derived from <math>(x\\vee y)</math> and <math>(\\bar{x} \\vee z)</math> through the [[resolution (logic)|resolution]] [[inference rule]].  This shows that the LHS is derivable from the RHS (if ''A'' → ''B'' then ''A'' → ''AB''; replacing ''A'' with RHS and ''B'' with (''y'' ∨ ''z'') ).  The RHS can be derived from the LHS simply through the [[conjunction elimination]] inference rule.  Since RHS → LHS and LHS → RHS (in [[propositional calculus]]), then LHS = RHS (in Boolean algebra).\n\n==Applications==\nIn Boolean algebra, repeated consensus is the core of one algorithm for calculating the [[Blake canonical form]] of a formula.<ref name=\"brown81\"/>\n\nIn [[digital logic]], including the consensus term in a circuit can eliminate [[race hazard]]s.<ref>M. Rafiquzzaman, ''Fundamentals of Digital Logic and Microcontrollers'', 6th edition (2014), {{ISBN|1118855795}}, p. 75</ref>\n\n==History==\nThe concept of consensus was introduced by Archie Blake in 1937, related to the [[Blake canonical form]].<ref name=\"blake\">\"Canonical expressions in Boolean algebra\", Dissertation, Department of Mathematics, University of Chicago, 1937, reviewed in J. C. C. McKinsey, ''The Journal of Symbolic Logic'' '''3''':2:93 (June 1938) {{DOI|10.2307/2267634}} {{jstor|2267634}}</ref> It was rediscovered by Samson and Mills in 1954<ref>Edward W. Samson, Burton E. Mills, Air Force Cambridge Research Center, Technical Report 54-21, April 1954</ref> and by [[Willard van Orman Quine|Quine]] in 1955.<ref>[[Willard van Orman Quine]], \"The problem of simplifying truth functions\", ''American Mathematical Monthly'' '''59''':521-531, 1952 {{jstor|2308219}}</ref> Quine coined the term 'consensus'. [[John Alan Robinson|Robinson]] used it for clauses in 1965 as the basis of his \"[[resolution (logic)|resolution principle]]\".<ref>[[John Alan Robinson]], \"A Machine-Oriented Logic Based on the Resolution Principle\", ''Journal of the ACM'' '''12''':1: 23–41.</ref><ref>[[Donald Ervin Knuth]], ''[[The Art of Computer Programming]]'' '''4A''': ''Combinatorial Algorithms'', part 1, p. 539</ref>\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* Roth, Charles H. Jr. and Kinney, Larry L. (2004, 2010). \"Fundamentals of Logic Design\", 6th Ed., p.&nbsp;66ff.\n\n{{DEFAULTSORT:Consensus Theorem}}\n[[Category:Boolean algebra]]\n[[Category:Theorems in propositional logic]]"
    },
    {
      "title": "Correlation immunity",
      "url": "https://en.wikipedia.org/wiki/Correlation_immunity",
      "text": "In mathematics, the '''correlation immunity''' of a [[Boolean function]] is a measure of the degree to which its outputs are uncorrelated with some subset of its inputs. Specifically, a Boolean function is said to be correlation-immune ''of order m'' if every subset of ''m'' or fewer variables in <math>x_1,x_2,\\ldots,x_n</math> is [[statistically independent]] of the value of <math>f(x_1,x_2,\\ldots,x_n)</math>.\n\n== Definition ==\nA function <math>f:\\mathbb{F}_2^n\\rightarrow\\mathbb{F}_2</math> is <math>k</math>-th order correlation immune if for any independent <math>n</math> binary random variables <math>X_0\\ldots X_{n-1}</math>, the random variable <math>Z=f(X_0,\\ldots,X_{n-1})</math> is independent from any random vector <math>(X_{i_1}\\ldots X_{i_k})</math> with <math>0\\leq i_1<\\ldots<i_k<n</math>.\n\n== Results in cryptography ==\nWhen used in a [[stream cipher]] as a combining function for [[linear feedback shift register]]s, a Boolean function with '''low-order''' correlation-immunity is '''more susceptible''' to a [[correlation attack]] than a function with correlation immunity of '''high order'''.\n\nSiegenthaler showed that the correlation immunity ''m'' of a Boolean function of [[algebraic degree]] ''d'' of ''n'' variables satisfies ''m''&nbsp;+&nbsp;''d''&nbsp;≤&nbsp;''n''; for a given set of input variables, this means that a high algebraic degree will restrict the maximum possible correlation immunity.  Furthermore, if the function is balanced then ''m''&nbsp;+&nbsp;''d''&nbsp;≤&nbsp;''n''&nbsp;&minus;&nbsp;1.<ref name=\"Siegenthaler\">{{cite journal | author=T. Siegenthaler | title=Correlation-Immunity of Nonlinear Combining Functions for Cryptographic Applications | journal=IEEE Transactions on Information Theory |date=September 1984 | volume=30 | issue=5 | pages=776–780 | doi=10.1109/TIT.1984.1056949 }}</ref>\n\n==References==\n{{reflist}}\n\n===Further reading===\n# Cusick, Thomas W. & Stanica, Pantelimon (2009). \"Cryptographic Boolean functions and applications\". Academic Press. {{ISBN|9780123748904}}.\n\n{{Cryptography navbox | block | hash | stream}}\n\n[[Category:Cryptography]]\n[[Category:Boolean algebra]]\n\n\n{{crypto-stub}}"
    },
    {
      "title": "Davis–Putnam algorithm",
      "url": "https://en.wikipedia.org/wiki/Davis%E2%80%93Putnam_algorithm",
      "text": "The '''Davis–Putnam algorithm''' was developed by [[Martin Davis (mathematician)|Martin Davis]] and [[Hilary Putnam]] for checking the validity of a [[first-order logic]] formula using a [[Resolution (logic)|resolution]]-based decision procedure for [[propositional logic]]. Since the set of valid first-order formulas is [[recursively enumerable]] but not [[Recursive set|recursive]], there exists no general algorithm to solve this problem. Therefore, the Davis–Putnam algorithm only terminates on valid formulas. Today, the term \"Davis–Putnam algorithm\" is often used synonymously with the resolution-based propositional decision procedure that is actually only one of the steps of the original algorithm.\n\n==Overview==\n[[File:Resolution.png|thumb|400px|Two runs of the [[:en:Davis-Putnam procedure|Davis-Putnam procedure]] on example propositional ground instances.\n''Top to bottom, Left:'' Starting from the formula <math>(a \\lor b \\lor c) \\land (b \\lor \\lnot c \\lor \\lnot f) \\land (\\lnot b \\lor e)</math>, the algorithm resolves on <math>b</math>, and then on <math>c</math>. Since no further resolution is possible, the algorithm stops; since the [[empty clause]] couldn't be derived, the result is \"''satisfiable''\".\n''Right:'' Resolving the given formula on <math>b</math>, then on <math>a</math>, then on <math>c</math> yields the empty clause; hence the algorithm returns \"''unsatisfiable''\".]]\n\nThe procedure is based on [[Herbrand's theorem]], which implies that an [[satisfiable|unsatisfiable]] formula has an unsatisfiable [[ground instance]], and on the fact that a formula is valid if and only if its negation is unsatisfiable. Taken together, these facts imply that to prove the validity of ''φ'' it is enough to prove that a ground instance of ''¬φ'' is unsatisfiable. If ''φ'' is not valid, then the search for an unsatisfiable ground instance will not terminate.\n\nThe procedure roughly consists of these three parts:\n* put the formula in [[prenex]] form and eliminate quantifiers \n* generate all propositional ground instances, one by one\n* check if each instance is satisfiable\n\nThe last part is probably the most innovative one, and works as follows (cf. picture):\n\n* for every variable in the formula\n** for every clause <math>c</math> containing the variable and every clause <math>n</math> containing the negation of the variable\n*** [[Resolution (logic)|resolve]] ''c'' and ''n'' and add the resolvent to the formula\n** remove all original clauses containing the variable or its negation\n\nAt each step, the intermediate formula generated is [[equisatisfiable]], but possibly not [[Logical equivalence|equivalent]], to the original formula. The resolution step leads to a worst-case exponential blow-up in the size of the formula. \n\nThe [[Davis–Putnam–Logemann–Loveland algorithm]] is a 1962 refinement of the propositional satisfiability step of the Davis–Putnam procedure which requires only a linear amount of memory in the worst case. It still forms the basis for today's (as of 2015) most efficient complete [[SAT solver]]s.\n\n==See also==\n*[[Herbrandization]]\n\n==References==\n* {{cite journal\n|last=Davis\n|first=Martin\n|author2=Putnam, Hilary\n| title=A Computing Procedure for Quantification Theory\n| journal =[[Journal of the ACM]]\n| volume = 7 \n| issue = 3\n| pages = 201–215\n| year = 1960\n| url = http://portal.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=321034\n| doi=10.1145/321033.321034}}\n*{{cite journal\n| last=Beckford\n| first=Jahbrill |author2=Logemann, George |author3=Loveland, Donald\n| title=A Machine Program for Theorem Proving\n| journal =[[Communications of the ACM]]\n| volume=5\n| issue=7\n| pages = 394–397\n| year=1962\n| url=http://portal.acm.org/citation.cfm?doid=368273.368557\n| doi=10.1145/368273.368557}}\n* {{cite conference\n | author = R. Dechter\n |author2=I. Rish\n | editor = J. Doyle and E. Sandewall and P. Torasso\n | title = Directional Resolution: The Davis–Putnam Procedure, Revisited\n | conference = \n | booktitle = Principles of Knowledge Representation and Reasoning: Proc. of the Fourth International Conference (KR'94)\n | pages = 134–145\n | publisher = Starswager18\n | url = \n | conferenceurl = \n }}\n* {{cite book|author=John Harrison|title=Handbook of practical logic and automated reasoning|year=2009|publisher=Cambridge University Press|isbn=978-0-521-89957-4|pages=79–90}}\n\n{{DEFAULTSORT:Davis-Putnam algorithm}}\n[[Category:Boolean algebra]]\n[[Category:Constraint programming]]\n[[Category:Automated theorem proving]]\n\n\n{{formalmethods-stub}}"
    },
    {
      "title": "De Morgan's laws",
      "url": "https://en.wikipedia.org/wiki/De_Morgan%27s_laws",
      "text": "[[File:Demorganlaws.svg|thumb|De Morgan's laws represented with [[Venn diagrams]]. In each case, the resultant set is the set of all points in any shade of blue.]]\n\n{{Transformation rules}}\n\nIn [[propositional calculus|propositional logic]] and [[boolean algebra]], '''De Morgan's laws'''<ref>Copi and Cohen{{Full citation needed|date=January 2015}}</ref><ref>{{citation|first=Patrick J.|last=Hurley|title=A Concise Introduction to Logic|edition=12th|publisher=Cengage Learning|year=2015|isbn=978-1-285-19654-1}}</ref><ref>Moore and Parker{{Full citation needed|date=January 2015}}</ref> are a pair of transformation rules that are both [[Validity (logic)|valid]] [[rule of inference|rules of inference]]. They are named after [[Augustus De Morgan]], a 19th-century British mathematician. The rules allow the expression of [[Logical conjunction|conjunctions]] and [[Logical disjunction|disjunctions]] purely in terms of each other via [[logical negation|negation]].\n\nThe rules can be expressed in English as:\n:the negation of a disjunction is the conjunction of the negations; and\n:the negation of a conjunction is the disjunction of the negations;\nor\n:the [[Complement (set theory)|complement]] of the union of two sets is the same as the intersection of their complements; and\n:the complement of the intersection of two sets is the same as the union of their complements.\nor\n:not (A or B) = not A and not B; and\n:not (A and B) = not A or not B\n\nIn [[Set theory|set theory]] and [[Boolean algebra (logic)|Boolean algebra]], these are written formally as\n:<math>\\begin{align}\n  \\overline{A \\cup B} &= \\overline{A} \\cap \\overline{B}, \\\\\n  \\overline{A \\cap B} &= \\overline{A} \\cup \\overline{B},\n\\end{align}</math>\nwhere\n* ''A'' and ''B'' are sets,\n* {{overline|''A''}} is the complement of A,\n* ∩ is the [[Intersection (set theory)|intersection]], and\n* ∪ is the [[Union (set theory)|union]].\n\nIn [[formal language]], the rules are written as\n:<math>\\neg(P\\lor Q)\\iff(\\neg P)\\land(\\neg Q),</math>\nand\n:<math>\\neg(P\\land Q)\\iff(\\neg P)\\lor(\\neg Q)</math>\nwhere\n* ''P'' and ''Q are propositions,''\n* [[Negation|<math>\\neg</math>]] is the negation logic operator (NOT),\n* [[Logical_conjunction|<math>\\land</math>]] is the conjunction logic operator (AND),\n* [[Logical_disjunction|<math>\\lor</math>]] is the disjunction logic operator (OR),\n* [[If_and_only_if|<math>\\iff</math>]] is a [[metalogic]]al symbol meaning \"can be replaced in a [[formal proof|logical proof]] with\".\n\nApplications of the rules include simplification of logical [[Expression (computer science)|expressions]] in [[computer program]]s and digital circuit designs. De Morgan's laws are an example of a more general concept of [[duality (mathematics)|mathematical duality]].\n\n==Formal notation==\nThe ''negation of conjunction'' rule may be written in [[sequent]] notation:\n:<math>\\neg(P \\land Q) \\vdash (\\neg P \\lor \\neg Q).</math>\n\nThe ''negation of disjunction'' rule may be written as:\n:<math>\\neg(P \\lor Q) \\vdash (\\neg P \\land \\neg Q).</math>\n\nIn [[Rule of inference|rule form]]: ''negation of conjunction''\n:<math>\\frac{\\neg (P \\land Q)}{\\therefore \\neg P \\lor \\neg Q}</math>\n\nand ''negation of disjunction''\n:<math>\\frac{\\neg (P \\lor Q)}{\\therefore \\neg P \\land \\neg Q}</math>\n\nand expressed as a truth-functional [[Tautology (logic)|tautology]] or [[theorem]] of propositional logic:\n\n:<math>\\begin{align}\n  \\neg (P \\land Q) &\\to (\\neg P \\lor \\neg Q), \\\\\n   \\neg (P \\lor Q) &\\to (\\neg P \\land \\neg Q),\n\\end{align}</math>\n\nwhere <math>P</math> and <math>Q</math> are propositions expressed in some formal system.\n\n===Substitution form===\nDe Morgan's laws are normally shown in the compact form above, with negation of the output on the left and negation of the inputs on the right. A clearer form for substitution can be stated as:\n\n:<math>\\begin{align}\n  (P \\land Q) &\\equiv \\neg (\\neg P \\lor \\neg Q), \\\\\n   (P \\lor Q) &\\equiv \\neg (\\neg P \\land \\neg Q).\n\\end{align}</math>\n\nThis emphasizes the need to invert both the inputs and the output, as well as change the operator, when doing a substitution.\n\n===Set theory and Boolean algebra===\nIn set theory and [[Boolean algebra (logic)|Boolean algebra]], it is often stated as \"union and intersection interchange under complementation\",<ref>''Boolean Algebra'' by R. L. Goodstein. {{ISBN|0-486-45894-6}}</ref> which can be formally expressed as:\n:<math>\\begin{align}\n  \\overline{A \\cup B} &= \\overline{A} \\cap \\overline{B}, \\\\\n  \\overline{A \\cap B} &= \\overline{A} \\cup \\overline{B},\n\\end{align}</math>\n\nwhere:\n* {{overline|''A''}} is the negation of A, the [[overline]] being written above the terms to be negated,\n* ∩ is the [[Intersection (set theory)|intersection]] operator (AND),\n* ∪ is the [[Union (set theory)|union]] operator (OR).\n\nThe generalized form is:\n: <math>\\begin{align}\n  \\overline{\\bigcap_{i \\in I} A_{i}} &\\equiv \\bigcup_{i \\in I} \\overline{A_{i}}, \\\\\n  \\overline{\\bigcup_{i \\in I} A_{i}} &\\equiv \\bigcap_{i \\in I} \\overline{A_{i}},\n\\end{align}</math>\n\nwhere {{math|''I''}} is some, possibly uncountable, indexing set.\n\nIn set notation, De Morgan's laws can be remembered using the [[mnemonic]] \"break the line, change the sign\".<ref>[https://books.google.com/books?id=NdAjEDP5mDsC&pg=PA81&lpg=PA81&dq=break+the+line+change+the+sign&source=web&ots=BtUl4oQOja&sig=H1Wz9e6Uv_bNeSbTvN6lr3s47PQ#PPA81,M1 ''2000 Solved Problems in Digital Electronics''] by S. P. Bali</ref>\n\n===Engineering===\nIn [[electrical and computer engineering]], De Morgan's laws are commonly written as:\n: <math>\\overline{A \\cdot B} \\equiv \\overline {A} + \\overline {B}</math>\n\nand\n: <math>\\overline{A + B} \\equiv \\overline {A} \\cdot \\overline {B},</math>\n\nwhere:\n* <math> \\cdot </math> is the logical AND,\n* <math>+</math> is the logical OR,\n* the {{overline|overbar}} is the logical NOT of what is underneath the overbar.\n\n===Text searching===\nDe Morgan’s laws commonly apply to text searching using Boolean operators AND, OR, and NOT.  Consider a set of documents containing the words “cars” and “trucks”. De Morgan’s laws hold that these two searches will return the same set of documents:\n\n:Search A: NOT (cars OR trucks)\n:Search B: (NOT cars) AND (NOT trucks)\n\nThe corpus of documents containing “cars” or “trucks” can be represented by four documents:\n:Document 1: Contains only the word “cars”.\n:Document 2: Contains only “trucks”.\n:Document 3: Contains both “cars” and “trucks”.\n:Document 4: Contains neither “cars” nor “trucks”.\n\nTo evaluate Search A, clearly the search “(cars OR trucks)” will hit on Documents 1, 2, and 3. So the negation of that search (which is Search A) will hit everything else, which is Document 4.\n\nEvaluating Search B, the search “(NOT cars)” will hit on documents that do not contain “cars”, which is Documents 2 and 4.  Similarly the search “(NOT trucks)” will hit on Documents 1 and 4. Applying the AND operator to these two searches (which is Search B) will hit on the documents that are common to these two searches, which is Document 4.\n\nA similar evaluation can be applied to show that the following two searches will return the same set of documents (Documents 1, 2, 4):\n:Search C: NOT (cars AND trucks),\n:Search D: (NOT cars) OR (NOT trucks).\n\n==History==\nThe laws are named after [[Augustus De Morgan]] (1806–1871),<ref>''[http://www.mtsu.edu/~phys2020/Lectures/L19-L25/L3/DeMorgan/body_demorgan.html DeMorgan’s Theorems]'' at mtsu.edu</ref> who introduced a formal version of the laws to classical [[propositional logic]]. De Morgan's formulation was influenced by algebraization of logic undertaken by [[George Boole]], which later cemented De Morgan's claim to the find. Nevertheless, a similar observation was made by [[Aristotle]], and was known to Greek and Medieval logicians.<ref>Bocheński's ''History of Formal Logic''</ref> For example, in the 14th century, [[William of Ockham]] wrote down the words that would result by reading the laws out.<ref>William of Ockham, ''Summa Logicae'', part II, sections 32 and 33.</ref> [[Jean Buridan]], in his ''Summulae de Dialectica'', also describes rules of conversion that follow the lines of De Morgan's laws.<ref>Jean Buridan, ''Summula de Dialectica''. Trans. Gyula Klima. New Haven: Yale University Press, 2001. See especially Treatise 1, Chapter 7, Section 5. {{ISBN|0-300-08425-0}}</ref> Still, De Morgan is given credit for stating the laws in the terms of modern formal logic, and incorporating them into the language of logic. De Morgan's laws can be proved easily, and may even seem trivial.<ref>[http://www.engr.iupui.edu/~orr/webpages/cpt120/mathbios/ademo.htm Augustus De Morgan (1806–1871)] {{webarchive|url=https://web.archive.org/web/20100715185655/http://www.engr.iupui.edu/~orr/webpages/cpt120/mathbios/ademo.htm |date=2010-07-15 }} by Robert H. Orr</ref> Nonetheless, these laws are helpful in making valid inferences in proofs and deductive arguments.\n\n==Informal proof==\nDe Morgan's theorem may be applied to the negation of a [[disjunction]] or the negation of a [[Logical conjunction|conjunction]] in all or part of a formula.\n\n===Negation of a disjunction===\nIn the case of its application to a disjunction, consider the following claim: \"it is false that either of A or B is true\", which is written as:\n:<math>\\neg(A\\lor B).</math>\nIn that it has been established that ''neither'' A nor B is true, then it must follow that both A is not true [[logical AND|and]] B is not true, which may be written directly as:\n:<math>(\\neg A)\\wedge(\\neg B).</math>\nIf either A or B ''were'' true, then the disjunction of A and B would be true, making its negation false. Presented in English, this follows the logic that \"since two things are both false, it is also false that either of them is true\".\n\nWorking in the opposite direction, the second expression asserts that A is false and B is false (or equivalently that \"not A\" and \"not B\" are true). Knowing this, a disjunction of A and B must be false also. The negation of said disjunction must thus be true, and the result is identical to the first claim.\n\n===Negation of a conjunction===\nThe application of De Morgan's theorem to a conjunction is very similar to its application to a disjunction both in form and rationale.  Consider the following claim: \"it is false that A and B are both true\", which is written as:\n:<math>\\neg(A\\land B).</math>\nIn order for this claim to be true, either or both of A or B must be false, for if they both were true, then the conjunction of A and B would be true, making its negation false. Thus, [[inclusive or|one (at least) or more]] of A and B must be false (or equivalently, one or more of \"not A\" and \"not B\" must be true). This may be written directly as,\n:<math>(\\neg A)\\lor(\\neg B).</math>\nPresented in English, this follows the logic that \"since it is false that two things are both true, at least one of them must be false\".\n\nWorking in the opposite direction again, the second expression asserts that at least one of \"not A\" and \"not B\" must be true, or equivalently that at least one of A and B must be false. Since at least one of them must be false, then their conjunction would likewise be false. Negating said conjunction thus results in a true expression, and this expression is identical to the first claim.\n\n==Formal proof==\nThe proof that <math>(A\\cap B)^\\complement = A^\\complement \\cup B^\\complement</math> is completed in 2 steps by proving both <math>(A\\cap B)^\\complement \\subseteq A^\\complement \\cup B^\\complement</math> and <math>A^\\complement \\cup B^\\complement \\subseteq (A\\cap B)^\\complement</math>.\n\n===Part 1===\n\nLet <math>x \\in (A \\cap B)^\\complement</math>. Then, <math>x \\not\\in A \\cap B</math>.\n\nBecause <math>A \\cap B = \\{y | y \\in A \\land y \\in B\\}</math>, it must be the case that <math>x \\not\\in A</math> or <math>x \\not\\in B</math>.\n\nIf <math>x \\not\\in A</math>, then <math>x \\in A^\\complement</math>, so <math>x \\in A^\\complement \\cup B^\\complement</math>.\n\nSimilarly, if <math>x \\not\\in B</math>, then <math>x \\in B^\\complement</math>, so <math>x \\in A^\\complement\\cup B^\\complement</math>.\n\nThus, <math>\\forall x( x \\in (A\\cap B)^\\complement \\rarr x \\in A^\\complement \\cup B^\\complement)</math>;\n\nthat is, <math>(A\\cap B)^\\complement \\subseteq A^\\complement \\cup B^\\complement</math>.\n\n===Part 2===\n\nTo prove the reverse direction, let <math>x \\in A^\\complement \\cup B^\\complement</math>, and assume <math>x \\not\\in (A\\cap B)^\\complement</math>.\n\nUnder that assumption, it must be the case that <math>x \\in A\\cap B</math>,\n\nso it follows that <math>x \\in A</math> and <math>x \\in B</math>, and thus <math>x \\not\\in A^\\complement</math> and <math>x \\not\\in B^\\complement</math>.\n\nHowever, that means <math>x \\not\\in A^\\complement \\cup B^\\complement</math>, in contradiction to the hypothesis that <math>x \\in A^\\complement \\cup B^\\complement</math>,\n\ntherefore, the assumption <math>x \\not\\in (A\\cap B)^\\complement</math> must not be the case, meaning that <math>x \\in (A\\cap B)^\\complement</math>.\n\nHence, <math>\\forall x( x \\in A^\\complement \\cup B^\\complement \\rarr x \\in (A\\cap B)^\\complement)</math>,\n\nthat is, <math>A^\\complement \\cup B^\\complement \\subseteq (A\\cap B)^\\complement</math>.\n\n===Conclusion===\n\nIf <math>A^\\complement \\cup B^\\complement \\subseteq (A\\cap B)^\\complement</math> ''and'' <math>(A \\cap B)^\\complement \\subseteq A^\\complement \\cup B^\\complement</math>, then <math>(A\\cap B)^\\complement = A^\\complement \\cup B^\\complement</math>; this concludes the proof of De Morgan's law.\n\nThe other De Morgan's law, <math>(A \\cup B)^\\complement = A^\\complement \\cap B^\\complement</math>, is proven similarly.\n\n==Generalising De Morgan duality==\n[[File:DeMorgan Logic Circuit diagram DIN.svg|thumb|De Morgan's Laws represented as a circuit with logic gates]]\n\nIn extensions of classical propositional logic, the duality still holds (that is, to any logical operator one can always find its dual), since in the presence of the identities governing negation, one may always introduce an operator that is the De Morgan dual of another. This leads to an important property of logics based on [[classical logic]], namely the existence of [[negation normal form]]s: any formula is equivalent to another formula where negations only occur applied to the non-logical atoms of the formula. The existence of negation normal forms drives many applications, for example in [[digital circuit]] design, where it is used to manipulate the types of [[logic gate]]s, and in formal logic, where it is needed to find the [[conjunctive normal form]] and [[disjunctive normal form]] of a formula.  Computer programmers use them to simplify or properly negate complicated [[Conditional (programming)|logical conditions]]. They are also often useful in computations in elementary [[probability theory]].\n\nLet one define the dual of any propositional operator P(''p'', ''q'', ...) depending on elementary propositions ''p'', ''q'', ... to be the operator <math>\\mbox{P}^d</math> defined by\n\n:<math>\\mbox{P}^d(p, q, ...) = \\neg P(\\neg p, \\neg q, \\dots).</math>\n\n==Extension to predicate and modal logic==\n\nThis duality can be generalised to quantifiers, so for example the [[universal quantifier]] and [[existential quantifier]] are duals:\n\n:<math> \\forall x \\, P(x) \\equiv \\neg [ \\exists x \\, \\neg P(x)] </math>\n\n:<math> \\exists x \\, P(x) \\equiv \\neg [ \\forall x \\, \\neg P(x)] </math>\n\nTo relate these quantifier dualities to the De Morgan laws, set up a [[model theory|model]] with some small number of elements in its domain ''D'', such as\n\n:''D'' = {''a'', ''b'', ''c''}.\n\nThen\n\n:<math> \\forall x \\, P(x) \\equiv P(a) \\land P(b) \\land P(c) </math>\n\nand\n\n:<math> \\exists x \\, P(x) \\equiv P(a) \\lor P(b) \\lor P(c).</math>\n\nBut, using De Morgan's laws,\n\n:<math> P(a) \\land P(b) \\land P(c) \\equiv \\neg (\\neg P(a) \\lor \\neg P(b) \\lor \\neg P(c)) </math>\n\nand\n\n:<math> P(a) \\lor P(b) \\lor P(c) \\equiv \\neg (\\neg P(a) \\land \\neg P(b) \\land \\neg P(c)), </math>\n\nverifying the quantifier dualities in the model.\n\nThen, the quantifier dualities can be extended further to [[modal logic]], relating the box (\"necessarily\") and diamond (\"possibly\") operators:\n\n:<math> \\Box p \\equiv \\neg \\Diamond \\neg p, </math>\n:<math> \\Diamond p \\equiv \\neg \\Box \\neg p.</math>\n\nIn its application to the [[alethic modalities]] of possibility and necessity, [[Aristotle]] observed this case, and in the case of [[normal modal logic]], the relationship of these modal operators to the quantification can be understood by setting up models using [[Kripke semantics]].\n\n==See also==\n* [[Isomorphism]] – NOT operator as isomorphism between positive logic and negative logic\n* [[List of Boolean algebra topics]]\n* [[Positive logic]]\n\n==References==\n{{reflist}}\n\n==External links==\n* {{springer|title=Duality principle|id=p/d034130}}\n* {{MathWorld | urlname=deMorgansLaws | title=de Morgan's Laws}}\n* {{PlanetMath | urlname=DeMorgansLaws | title=de Morgan's laws | id=2308}}\n\n{{Classical logic}}\n{{Set theory}}\n\n[[Category:Boolean algebra]]\n[[Category:Duality theories]]\n[[Category:Rules of inference]]\n[[Category:Articles containing proofs]]\n[[Category:Theorems in propositional logic]]"
    },
    {
      "title": "DiVincenzo's criteria",
      "url": "https://en.wikipedia.org/wiki/DiVincenzo%27s_criteria",
      "text": "{{Short description|Criteria for a usable quantum computer}}\n\nThe '''DiVincenzo criteria''' are conditions necessary for constructing a [[Quantum computing|quantum computer]], conditions proposed in 2000 by the theoretical physicist [[David P. DiVincenzo]],<ref name=\"David\">{{cite journal| arxiv=quant-ph/0002077|title=The Physical Implementation of Quantum Computation|last=DiVincenzo |first=David P.|date=2000-04-13|doi=10.1002/1521-3978(200009)48:9/11<771::AID-PROP771>3.0.CO;2-E|volume=48|issue=9–11|journal=Fortschritte der Physik|pages=771–783}}</ref> as being those necessary to construct such a computer—a computer first proposed by mathematician [[Yuri Manin]], in 1980,<ref name=\"manin1980vychislimoe\">{{cite book| author=Manin, Yu. I.| title=Vychislimoe i nevychislimoe |trans-title=Computable and Noncomputable | year=1980| publisher=Sov.Radio| url=https://web.archive.org/web/20130510173823/http://publ.lib.ru/ARCHIVES/M/MANIN_Yuriy_Ivanovich/Manin_Yu.I._Vychislimoe_i_nevychislimoe.(1980).%5Bdjv%5D.zip| pages=13–15| language=Russian| accessdate=2013-03-04}}</ref> and physicist [[Richard Feynman]], in 1982<ref>{{cite journal|date=June 1982|title=Simulating physics with computers|journal=[[International Journal of Theoretical Physics]]|volume=21|issue=6|pages=467–488|doi=10.1007/BF02650179|last1=Feynman|first1=R. P.|bibcode=1982IJTP...21..467F|citeseerx=10.1.1.45.9310}}</ref>—as a means to efficiently simulate [[Quantum mechanics|quantum]] systems, such as in solving the [[quantum many-body problem]].\n\nThere have been many proposals for how to construct a quantum computer, all of which meet with varying degrees of success against the different challenges of constructing quantum devices. Some of these proposals involve using [[Superconducting quantum computing|superconducting qubits]], [[Trapped ion quantum computer|trapped ions]], [[Nuclear magnetic resonance quantum computer|liquid and solid state nuclear magnetic resonance]], or [[One-way quantum computer|optical cluster states]], all of which show good prospects but also have issues that prevent their practical implementation.\n\nThe DiVincenzo criteria consist of seven conditions an experimental setup must satisfy to successfully implement [[quantum algorithm]]s such as [[Grover's algorithm|Grover's search algorithm]] or [[Shor's algorithm|Shor factorization]]. The first five conditions regard quantum computation itself. Two additional conditions regard implementing [[quantum communication]], such as that used in [[quantum key distribution]]. One can demonstrate that DiVincenzo's criteria are satisfied by a classical computer. Comparing the ability of classical and quantum regimes to satisfy the criteria highlights both the complications that arise in dealing with quantum systems and the source of the [[Deutsch–Jozsa algorithm|quantum speed up]].\n\n== Statement of the criteria ==\nAccording to DiVincenzo's criteria, constructing a quantum computer requires that the experimental setup meet seven conditions. The first five are necessary for quantum computation:\n# A scalable physical system with well characterized [[qubit]]\n# The ability to initialize the state of the qubits to a simple fiducial state\n# Long relevant [[Quantum decoherence|decoherence]] [[Relaxation (NMR)|times]]\n# A \"universal\" set of [[quantum gate]]s\n# A qubit-specific [[Measurement in quantum mechanics|measurement]] capability\n\nThe remaining two are necessary for [[Quantum channel|quantum communication]]:\n# The ability to interconvert stationary and flying qubits\n# The ability to faithfully transmit flying qubits between specified locations\n\n==Why the DiVincenzo criteria?==\nDiVincenzo proposed his criteria after many attempts to construct a quantum computer. Below describes why these statements are important, and presents examples.\n\n===Scalability with well-characterised qubits===\nMost models of quantum computation require the use of qubits. Quantum mechanically, a [[qubit]] is defined as a 2-level system with some energy gap. This can sometimes be difficult to implement physically, and so we focus on a particular transition of atomic levels. Whatever the system we choose, we require that the system remain almost always in the subspace of these two levels, and in doing so we can say it is a well-characterised qubit. An example of a system that is not well characterised would be 2 one-electron [[quantum dot]]s, with potential wells each occupied by a single electron [[Two-state quantum system|in one well or the other]], which is properly characterised as a single qubit. However, in considering a state such as <math>|00\\rangle +|11\\rangle</math>, such a system would correspond to a two-qubit state.\n\nWith today's technology, [[Physical qubit|a system]] that has a well characterised qubit can be created, but it is a challenge to create a system that has an arbitrary number of well-characterised qubits. Currently, one of the biggest problems being faced is that we require exponentially larger experimental setups to accommodate a greater number of qubits. The quantum computer is capable of exponential speed-ups in computing classical algorithms for prime factorisation of numbers; but if this requires an exponentially large setup, then our advantage is lost. In the case of using liquid-state [[nuclear magnetic resonance]] (NMR), it was found that increased macroscopic size led to system initialisation that left computational qubits in a highly [[Density matrix|mixed state]].<ref>{{cite journal |author=Menicucci NC, Caves CM |year=2002 |title=Local realistic model for the dynamics of bulk-ensemble NMR information processing |journal=Physical Review Letters |volume=88 |issue=16 |doi=10.1103/PhysRevLett.88.167901|arxiv = quant-ph/0111152 |bibcode = 2002PhRvL..88p7901M |pmid=11955265 |page=167901}}</ref> In spite of this, a computation model was found that could still use these mixed states for computation, but the more mixed these states are the weaker the induction signal corresponding to a quantum measurement is. If this signal is below the noise threshold, a solution is to increase the size of the sample to boost the signal strength; and this is the source of the non-scalability of liquid-state NMR as a means for quantum computation. One could say that as the number of computational qubits increases they become less well characterised until we reach a threshold at which they are no longer useful.\n\n===Initialising qubits to a simple fiducial state===\nAll models of quantum and classical computation are based on performing operations on a states maintained by qubits or bits and measuring and reporting a result, a procedure that is dependent on the initial state of the system. In particular, the [[unitarity (physics)|unitarity]] nature of quantum mechanics makes initialisation of the qubits extremely important. In many cases, initialisation is accomplished by letting the system [[Quantum annealing|anneal]] to the ground state. This is of particular importance when you consider [[quantum error correction]], a procedure to perform quantum processes that are robust against certain types of noise and that require a large supply of freshly initialised qubits, which places restrictions on how fast the initialisation can be.\n\nAn example of annealing is described in a 2005 paper by Petta, et al., where a [[Bell state|Bell pair]] of electrons is prepared in quantum dots. This procedure relies on ''T''<sub>1</sub> to anneal the system, and the paper focuses on measuring the ''T''<sub>2</sub> relaxation time of the quantum-dot system and gives an idea of the timescales involved (milliseconds), which would be a fundamental roadblock, given that then the decoherence time is shorter than the initialisation time.<ref name=\"Petta2005\">{{cite journal|last1=Petta|first1=J. R.|last2=Johnson|first2=A. C.|last3=Taylor|first3=J. M.|last4=Laird|first4=E. A.|last5=Yacoby|first5=A.|last6=Lukin|first6=M. D.|last7=Marcus|first7=C. M.|last8=Hanson|first8=M. P.|last9=Gossard|first9=A. C.|title=Coherent Manipulation of Coupled Electron Spins in Semiconductor Quantum Dots|journal=Science|date=September 2005|volume=309|issue=5744|pages=2180–2184|doi=10.1126/science.1116955|pmid=16141370|url=http://science.sciencemag.org/content/309/5744/2180|bibcode=2005Sci...309.2180P|citeseerx=10.1.1.475.4833}}</ref> Alternate approaches (usually involving [[optical pumping]]<ref>{{cite journal|last1=Atatüre|first1=Mete|last2=Dreiser|first2=Jan|last3=Badolato|first3=Antonio|last4=Högele|first4=Alexander|last5=Karrai|first5=Khaled|last6=Imamoglu|first6=Atac|title=Quantum-Dot Spin-State Preparation with Near-Unity Fidelity|journal=Science|date=April 2006|volume=312|issue=5773|pages=551–553|doi=10.1126/science.1126074|pmid=16601152|bibcode=2006Sci...312..551A}}</ref>) have been developed to reduce the initialisation time and improve the fidelity of the procedure.\n\n===Long relevant decoherence times===\nDecoherence is a problem experienced in large, macroscopic quantum computation systems. The quantum resources used by quantum computing models ([[Quantum superposition|superposition]] or [[Quantum entanglement|entanglement]]) are quickly destroyed by decoherence. Long decoherence times are desired, much longer than the average [[Logic gate|gate]] time, so that decoherence can be combated with error correction or [[dynamical decoupling]]. In solid-state NMR using [[nitrogen-vacancy center]]s, the orbital electron experiences short decoherence times, making computations problematic; the proposed solution has been to encode the qubit in the nuclear spin of the nitrogen atom, thus increasing the decoherence time. In other systems, such as the quantum dot, issues with strong environmental effects limit the ''T''<sub>2</sub> decoherence time. Systems that can be manipulated quickly (through strong interactions) tend to experience decoherence via these very same strong interactions, and so there is a trade-off between ability to implement control and increased decoherence.\n\n===A \"universal\" set of quantum gates===\nBoth in classical and quantum computing the algorithms that we can compute are restricted by the number of gates we can implement. In the case of quantum computing, a universal quantum computer (a [[quantum Turing machine]]) can be constructed using a very small set of 1- and 2-qubit gates. Any experimental setup that manages to have well-characterised qubits; quick, faithful initialisation; and long decoherence times must also be capable of influencing the [[Hamiltonian (quantum mechanics)|Hamiltonian]] (total energy) of the system, in order to effect coherent changes capable of implementing a [[Quantum gate#Universal quantum gates|universal set of gates]]. A perfect implementation of gates is not always necessary, as gate sequences can be created that are more robust against certain systematic and random noise models.<ref>{{cite journal|last1=Green|first1=Todd J.|last2=Sastrawan|first2=Jarrah|last3=Uys|first3=Hermann|last4=Biercuk|first4=Michael J.|title=Arbitrary quantum control of qubits in the presence of universal noise|journal=New Journal of Physics|date=September 2013|volume=15|issue=9|page=095004|doi=10.1088/1367-2630/15/9/095004|arxiv=1211.1163|bibcode=2013NJPh...15i5004G}}</ref> Liquid-state NMR was one of the first setups capable of implementing a universal set of gates, through the use of precise timing and magnetic field pulses. However, as mentioned above, this system was not scalable.\n\n===A qubit-specific measurement capability===\nFor any process modifying the quantum states of qubits, the final measurement of those states is of fundamental importance when performing computations. If our system allows for non-destructive projective measurements, then, in principle, this can be used for state preparation. Measurement is at the foundation of all quantum algorithms, especially in concepts such as [[quantum teleportation]]. Measurement techniques that are not 100% efficient are typically repeated to increase the success rate. Examples of reliable measurement devices are found in optical systems where [[Homodyne detection|homodyne detectors]] have reached the point of reliably counting how many photons have passed through the detecting cross-section. More challenging is the measurement of quantum dots, where the [[energy gap]] between the <math>|01\\rangle +|10\\rangle</math> and <math>|01\\rangle -|10\\rangle</math> (the [[singlet state]]) is used to measure the relative spins of the 2 electrons.<ref name=\"Petta2005\"/>\n\n===Interconverting stationary and flying qubits and faithfully transmitting flying qubits between specified locations===\nInterconverting and transmitting are necessary when considering quantum communication protocols, such as quantum key distribution, that involve the exchange of coherent quantum states or entangled qubits (for example, the [[BB84]] protocol). When creating pairs of entangled qubits in experimental setups, these qubits are usually \"stationary\" and cannot be moved from the laboratory. If these qubits can be sent as flying qubits, such as being encoded into the polarisation of a photon, then sending entangled photons to a third party and having them extract that information, leaving two entangled stationary qubits at two different locations, can be considered. The ability to transmit the flying qubit without decoherence is a major problem. Currently, at the Institute for Quantum Computing there are efforts to produce a pair of entangled photons and transmit one of the photons to some other part of the world by reflecting it off a satellite. The main issue now is the decoherence the photon experiences whilst interacting with particles in the atmosphere. Similarly, some attempts have been made to use optical fibres, although the attenuation of the signal has kept this from becoming a reality.\n\n==See also==\n* [[Quantum computing]]\n* [[Nuclear magnetic resonance quantum computer]]\n* [[Trapped ion quantum computer]]\n\n==References==\n{{Reflist|30em}}\n\n{{Quantum computing}}\n\n{{DEFAULTSORT:DiVincenzo's criteria}}\n[[Category:Boolean algebra]]\n[[Category:Propositional calculus]]\n[[Category:Logic in computer science]]"
    },
    {
      "title": "Evasive Boolean function",
      "url": "https://en.wikipedia.org/wiki/Evasive_Boolean_function",
      "text": "{{unreferenced|date=November 2014}}\nIn [[mathematics]], an '''evasive Boolean function''' ''&fnof;'' (of ''n'' variables) is a [[Boolean function]] for which every [[Decision tree model|decision tree algorithm]] has running time of exactly&nbsp;''n''.  Consequently, every [[Decision tree model|decision tree algorithm]] that represents the function has, at worst case, a running time of&nbsp;''n''.\n\n== Examples ==\n\n=== An example for a non-evasive boolean function ===\nThe following is a Boolean function on the three variables ''x'',&nbsp;''y'',&nbsp;''z'':\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~f(x,y,z)~</math>\n|<math>~=~</math>\n|<math>~(x \\land y)~</math>\n|<math>~\\lor~</math>\n|<math>~(\\neg x \\land z)~</math>\n|-\n|[[File:Venn 0001 1011.svg|50px]]\n|<math>~=~</math>\n|[[File:Venn 0001 0001.svg|50px]]\n|<math>~\\lor~</math>\n|[[File:Venn 0000 1010.svg|50px]]\n|}\n\n(where <math>\\land</math> is the bitwise \"and\", <math>\\lor</math> is the bitwise \"or\", and <math>\\neg </math> is the bitwise \"not\").\n\nThis function is not evasive, because there is a decision tree that solves it by checking exactly two variables:  The algorithm first checks the value of&nbsp;''x''. If ''x'' is true, the algorithm checks the value of ''y'' and returns it.\n\n:( &nbsp;&nbsp;&nbsp;&nbsp;<math>(\\neg x = \\text{false}) ~~\\Rightarrow~~ ((\\neg x \\land z) = \\text{false})</math>&nbsp;&nbsp;&nbsp;&nbsp; )\n\nIf ''x'' is false, the algorithm checks the value of ''z'' and returns it.\n\n=== A simple example for an evasive boolean function ===\n\nConsider this simple \"and\" function on three variables:\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~f(x,y,z)~</math>\n|<math>~= (x \\wedge y \\wedge z)~</math>\n|-\n|[[File:Venn 0000 0001.svg|50px]]\n|\n|}\n\nA worst-case input (for every algorithm) is&nbsp;1,&nbsp;1,&nbsp;1. In every order we choose to check the variables, we have to check all of them. (Note that in general there could be a different worst-case input for every decision tree algorithm.)  Hence the functions: \"and\", \"or\" (on ''n'' variables) are evasive.\n\n== Binary zero-sum games  ==\n\nFor the case of binary [[zero-sum game]]s, every [[evaluation function]] is evasive.\n\nIn every zero-sum game, the value of the game is achieved by the [[minimax]] algorithm (player 1 tries to maximize the profit, and player 2 tries to minimize the cost).\n\nIn the binary case, the max function equals the bitwise&nbsp;\"or\", and the min function equals the bitwise&nbsp;\"and\".\n\nA decision tree for this game will be of this form:\n* every leaf will have value in {0,&nbsp;1}.\n* every node is connected to one of {\"and\",&nbsp;\"or\"}\n\nFor every such tree with ''n'' leaves, the running time in the worst case is ''n'' (meaning that the algorithm must check all the leaves):\n\nWe will exhibit an [[Adversary (online algorithm)|adversary]] that produces a worst-case input &ndash; for every leaf that the algorithm checks, the adversary will answer 0 if the leaf's parent is an Or node, and 1 if the parent is an And node.\n\nThis input (0 for all Or nodes' children, and 1 for all And nodes' children) forces the algorithm to check all nodes:\n\nAs in the second example\n* in order to calculate the ''Or'' result, if all children are 0 we must check them all. \n* In order to calculate the ''And'' result, if all children are 1 we must check them all.\n\n==See also==\n*[[Aanderaa–Karp–Rosenberg conjecture]], the conjecture that every nontrivial monotone graph property is evasive.\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Field of sets",
      "url": "https://en.wikipedia.org/wiki/Field_of_sets",
      "text": "{{Short description|Algebraic concept in measure theory}}{{redirect|Set algebra|the basic properties and laws of sets|Algebra of sets}}\n{{expert|mathematics|talk=Confusing}}\n\nIn [[mathematics]] a '''field of sets''' is a pair <math>\\langle X, \\mathcal{F} \\rangle</math> where <math>X </math> is a [[Set (mathematics)|set]] and <math>\\mathcal{F}</math> is an '''algebra over <math>X</math>''' i.e., a non-empty subset of the [[power set]] of <math>X </math> closed under the [[intersection (set theory)|intersection]] and [[union (set theory)|union]] of pairs of sets and under [[complement (set theory)|complements]] of individual sets. In other words, <math>\\mathcal{F}</math> forms a [[subalgebra]] of the [[power set]] [[Boolean algebra (structure)|Boolean algebra]] of <math>X </math>. (Many authors refer to <math>\\mathcal{F}</math> itself as a field of sets.) Elements of <math>X</math> are called '''points''' and those of <math>\\mathcal{F}</math> are called '''complexes''' and are said to be the '''admissible sets''' of <math>X </math>. \n\nFields of sets should not be confused with [[field (mathematics)|field]]s in [[ring theory]] nor with [[field (physics)|fields in physics]]. Similarly the term \"algebra over <math>X</math>\" is used in the sense of a [[Boolean algebra]] and should not be confused with [[algebra over a field|algebras over fields or rings]] in ring theory.\n\nFields of sets play an essential role in the [[representation theory]] of Boolean algebras. Every Boolean algebra can be represented as a field of sets.\n\n== Fields of sets in the representation theory of Boolean algebras ==\n\n=== Stone representation ===\nEvery finite Boolean algebra can be represented as a whole power set - the power set of its set of [[atomic (order theory)|atoms]]; each element of the Boolean algebra corresponds to the set of atoms below it (the join of which is the element). This '''power set representation''' can be constructed more generally for any [[Complete Boolean algebra|complete]] [[atomic (order theory)|atomic]] Boolean algebra.\n\nIn the case of Boolean algebras which are not complete and atomic we can still generalize the power set representation by considering fields of sets instead of whole power sets. To do this we first observe that the atoms of a finite Boolean algebra correspond to its [[ultrafilter]]s and that an atom is below an element of a finite Boolean algebra if and only if that element is contained in the ultrafilter corresponding to the atom. This leads us to construct a representation of a Boolean algebra by taking its set of ultrafilters and forming complexes by associating with each element of the Boolean algebra the set of ultrafilters containing that element. This construction does indeed produce a representation of the Boolean algebra as a field of sets and is known as the '''Stone representation'''. It is the basis of [[Stone's representation theorem for Boolean algebras]] and an example of a completion procedure in [[order theory]] based on [[Ideal (order theory)#Applications|ideal]]s or [[Filter (mathematics)|filter]]s, similar to [[Dedekind cut]]s.\n\nAlternatively one can consider the set of [[homomorphism]]s onto the two element Boolean algebra and form complexes by associating each element of the Boolean algebra with the set of such homomorphisms that map it to the top element. (The approach is equivalent as the ultrafilters of a Boolean algebra are precisely the pre-images of the top elements under these homomorphisms.) With this approach one sees that Stone representation can also be regarded as a generalization of the representation of finite Boolean algebras by [[truth table]]s.\n\n=== Separative and compact fields of sets: towards Stone duality ===\n\n* A field of sets is called '''separative''' (or '''differentiated''') if and only if for every pair of distinct points there is a complex containing one and not the other.\n* A field of sets is called '''compact''' if and only if for every proper [[filter (mathematics)|filter]] over <math>X\\ </math> the intersection of all the complexes contained in the filter is non-empty.\n\nThese definitions arise from considering the [[topological space|topology]] generated by the complexes of a field of sets. Given a field of sets <math>\\mathbf{X}= \\langle X, \\mathcal{F} \\rangle</math> the complexes form a [[base (topology)|base]] for a topology. We denote by <math>T(\\mathbf{X})</math> the corresponding topological space, <math>\\langle X, \\mathcal{T} \\rangle</math> where <math>\\mathcal{T}</math> is the topology formed by taking arbitrary unions of complexes. Then\n\n* <math>T(\\mathbf{X})</math> is always a [[zero-dimensional space]].\n* <math>T(\\mathbf{X})</math> is a [[Hausdorff space]] if and only if <math>\\mathbf{X}</math> is separative.\n* <math>T(\\mathbf{X})</math> is a [[compact space]] with compact open sets <math>\\mathcal{F}</math> if and only if <math>\\mathbf{X}</math> is compact.\n* <math>T(\\mathbf{X})</math> is a [[Boolean space]] with clopen sets <math>\\mathcal{F}</math> if and only if <math>\\mathbf{X}</math> is both separative and compact (in which case it is described as being '''descriptive''')\n\nThe Stone representation of a Boolean algebra is always separative and compact; the corresponding Boolean space is known as the [[Stone space]] of the Boolean algebra. The clopen sets of the Stone space are then precisely the complexes of the Stone representation. The area of mathematics known as [[Stone duality]] is founded on the fact that the Stone representation of a Boolean algebra can be recovered purely from the corresponding Stone space whence a [[Duality (mathematics)|duality]] exists between Boolean algebras and Boolean spaces.\n\n== Fields of sets with additional structure ==\n\n=== Sigma algebras and measure spaces ===\n\nIf an algebra over a set is closed under [[countable]] [[intersection (set theory)|intersections]] and countable [[union (set theory)|unions]], it is called a [[sigma algebra]] and the corresponding field of sets is called a '''measurable space'''. The complexes of a measurable space are called '''measurable sets'''. The [[Lynn Harold Loomis|Loomis]]-Sikorski theorem provides a Stone-type duality between countably complete Boolean algebras (which may be called '''abstract sigma algebras''') and measurable spaces.\n\nA '''measure space''' is a triple <math>\\langle X, \\mathcal{F}, \\mu  \\rangle</math> where <math>\\langle X, \\mathcal{F} \\rangle</math> is a measurable space and <math>\\mu</math> is a [[measure theory|measure]] defined on it. If <math>\\mu</math> is in fact a [[probability theory|probability measure]] we speak of a '''probability space''' and call its underlying measurable space a '''sample space'''. The points of a sample space are called '''samples''' and represent potential outcomes while the measurable sets (complexes) are called '''events''' and represent properties of outcomes for which we wish to assign probabilities. (Many use the term '''sample space''' simply for the underlying set of a probability space, particularly in the case where every subset is an event.) Measure spaces and probability spaces play a foundational role in [[measure theory]] and [[probability theory]] respectively.\n\nIn applications to [[Physics]] we often deal with measure spaces and probability spaces derived from rich mathematical structures such as [[inner product space]]s or [[topological group]]s which already have a topology associated with them - this should not be confused with the topology generated by taking arbitrary unions of complexes.\n\n=== Topological fields of sets ===\n\nA '''topological field of sets''' is a triple <math>\\langle X, \\mathcal{T}, \\mathcal{F} \\rangle</math> where <math>\\langle X, \\mathcal{T} \\rangle</math> is a [[topological space]] and <math>\\langle X, \\mathcal{F} \\rangle</math> is a field of sets which is closed under the [[closure operator]] of <math>\\mathcal{T}</math> or equivalently under the [[interior operator]] i.e. the closure and interior of every complex is also a complex. In other words, <math>\\mathcal{F}</math> forms a subalgebra of the power set [[interior algebra]] on <math>\\langle X, \\mathcal{T} \\rangle</math>.\n\nTopological fields of sets play a fundamental role in the representation theory of interior algebras and [[Heyting algebra]]s. These two classes of algebraic structures provide the [[Algebraic semantics (mathematical logic)|algebraic semantics]] for the [[modal logic]] ''S4'' (a formal mathematical abstraction of [[epistemic|epistemic logic]]) and [[intuitionistic logic]] respectively. Topological fields of sets representing these algebraic structures provide a related topological [[semantics of logic|semantics]] for these logics.\n\nEvery interior algebra can be represented as a topological field of sets with the underlying Boolean algebra of the interior algebra corresponding to the complexes of the topological field of sets and the interior and closure operators of the interior algebra corresponding to those of the topology. Every [[Heyting algebra]] can be represented by a topological field of sets with the underlying lattice of the Heyting algebra corresponding to the lattice of complexes of the topological field of sets that are open in the topology. Moreover the topological field of sets representing a Heyting algebra may be chosen so that the open complexes generate all the complexes as a Boolean algebra. These related representations provide a well defined mathematical apparatus for studying the relationship between truth modalities (possibly true vs necessarily true, studied in modal logic) and notions of provability and refutability (studied in intuitionistic logic) and is thus deeply connected to the theory of [[modal companion]]s of [[intermediate logic]]s.\n\nGiven a topological space the [[topology glossary|clopen]] sets trivially form a topological field of sets as each clopen set is its own interior and closure. The Stone representation of a Boolean algebra can be regarded as such a topological field of sets, however in general the topology of a topological field of sets can differ from the topology generated by taking arbitrary unions of complexes and in general the complexes of a topological field of sets need not be open or closed in the topology.\n\n==== Algebraic fields of sets and Stone fields ====\n\nA topological field of sets is called '''algebraic''' if and only if there is a base for its topology consisting of complexes.\n\nIf a topological field of sets is both compact and algebraic then its topology is compact and its compact open sets are precisely the open complexes. Moreover, the open complexes form a base for the topology.\n\nTopological fields of sets that are separative, compact and algebraic are called '''Stone fields''' and provide a generalization of the Stone representation of Boolean algebras. Given an interior algebra we can form the Stone representation of its underlying Boolean algebra and then extend this to a topological field of sets by taking the topology generated by the complexes corresponding to the [[Interior algebra#Open and closed elements|open elements]] of the interior algebra (which form a base for a topology). These complexes are then precisely the open complexes and the construction produces a Stone field representing the interior algebra - the '''Stone representation'''. (The topology of the Stone representation is also known as the '''McKinsey-Tarski Stone topology''' after the mathematicians who first generalized Stone's result for Boolean algebras to interior algebras and should not be confused with the Stone topology of the underlying Boolean algebra of the interior algebra which will be a finer topology).\n\n=== Preorder fields ===\n\nA '''preorder field''' is a triple <math>\\langle X, \\leq , \\mathcal{F} \\rangle</math> where <math>\\langle X, \\leq \\rangle</math> is a [[preorder|preordered set]] and <math>\\langle X, \\mathcal{F}\\rangle </math> is a field of sets.\n\nLike the topological fields of sets, preorder fields play an important role in the representation theory of interior algebras. Every interior algebra can be represented as a preorder field with its interior and closure operators corresponding to those of the [[Alexandrov topology]] induced by the preorder. In other words,\n\n:<math>\\mbox{Int}(S) = \\{x \\in X :</math> there exists a <math>y \\in S </math> with <math>y \\leq x \\}</math> and\n:<math>\\mbox{Cl}(S) = \\{ x \\in X :</math> there exists a <math>y \\in S </math> with <math>x \\leq y \\}</math> for all <math>S \\in \\mathcal{F}</math>\n\nSimilarly to topological fields of sets, preorder fields arise naturally in modal logic where the points represent the ''possible worlds'' in the [[Kripke semantics]] of a theory in the modal logic ''S4'', the preorder represents the accessibility relation on these possible worlds in this semantics, and the complexes represent sets of possible worlds in which individual sentences in the theory hold, providing a representation of the [[Lindenbaum–Tarski algebra]] of the theory. They are a special case of the [[general frame|general modal frames]] which are fields of sets with an additional accessibility relation providing representations of modal algebras.\n\n==== Algebraic and canonical preorder fields ====\n\nA preorder field is called '''algebraic''' (or '''tight''') if and only if it has a set of complexes <math>\\mathcal{A}</math> which determines the preorder in the following manner: <math>x \\leq y</math> if and only if for every complex <math>S \\in \\mathcal{A}</math>, <math>x \\in S</math> implies <math>y \\in S</math>. The preorder fields obtained from ''S4'' theories are always algebraic, the complexes determining the preorder being the sets of possible worlds in which the sentences of the theory closed under necessity hold.\n\nA separative compact algebraic preorder field is said to be '''canonical'''. Given an interior algebra, by replacing the topology of its Stone representation with the corresponding [[specialization (pre)order|canonical preorder]] (specialization preorder) we obtain a representation of the interior algebra as a canonical preorder field. By replacing the preorder by its corresponding [[Alexandrov topology#Duality with preordered sets|Alexandrov topology]] we obtain an alternative representation of the interior algebra as a topological field of sets. (The topology of this \"'''Alexandrov representation'''\" is just the [[Alexandrov topology#Categorical description of the duality|Alexandrov bi-coreflection]] of the topology of the Stone representation.) While representation of modal algebras by general modal frames is possible for any normal modal algebra, it is only in the case of interior algebras (which correspond to the modal logic ''S4'') that the general modal frame corresponds to topological field of sets in this manner.\n\n=== Complex algebras and fields of sets on relational structures===\n\nThe representation of interior algebras by preorder fields can be generalized to a representation theorem for arbitrary (normal) [[Boolean algebras with operators]]. For this we consider structures <math>\\langle X, ( R_i )_I, \\mathcal{F} \\rangle </math> where <math>\\langle X, ( R_i )_I \\rangle </math> is a [[relational structure]] i.e. a set with an indexed family of [[relation (mathematics)|relation]]s defined on it, and <math>\\langle X, \\mathcal{F} \\rangle </math> is a field of sets. The '''complex algebra''' (or '''algebra of complexes''') determined by a field of sets <math>\\mathbf{X} = \\langle X, ( R_i )_I, \\mathcal{F} \\rangle </math> on a relational structure, is the Boolean algebra with operators\n\n:<math>\\mathcal{C}(\\mathbf{X}) = \\langle \\mathcal{F}, \\cap, \\cup, \\prime, \\empty, X, ( f_i )_I \\rangle</math>\n\nwhere for all <math>i \\in I</math>, if <math>R_i\\ </math> is a relation of arity <math>n+1</math>, then <math>f_i\\ </math> is an operator of arity <math>n</math> and for all <math>S_1,...,S_n \\in \\mathcal{F}</math>\n\n:<math>f_i(S_1,...,S_n) = \\{ x \\in X :</math> there exist <math>x_1 \\in S_1 ,...,x_n \\in S_n</math> such that <math>R_i(x_1,...,x_n,x) \\}\\ </math>\n\nThis construction can be generalized to fields of sets on arbitrary [[algebraic structure]]s having both [[Operation (mathematics)|operators]] and relations as operators can be viewed as a special case of relations. If <math>\\mathcal{F}</math> is the whole power set of <math>X\\ </math> then <math>\\mathcal{C}(\\mathbf{X})</math> is called a '''full complex algebra''' or '''power algebra'''.\n\nEvery (normal) Boolean algebra with operators can be represented as a field of sets on a relational structure in the sense that it is [[isomorphism|isomorphic]] to the complex algebra corresponding to the field.\n\n(Historically the term '''complex''' was first used in the case where the algebraic structure was a [[group (mathematics)|group]] and has its origins in 19th century [[group theory]] where a subset of a group was called a '''complex'''.)\n\n== See also==\n\n* [[List of Boolean algebra topics]]\n* [[Algebra of sets]]\n* [[Sigma algebra]]\n* [[Measure theory]]\n* [[Probability theory]]\n* [[Interior algebra]]\n* [[Alexandrov topology]]\n* [[Stone's representation theorem for Boolean algebras]]\n* [[Stone duality]]\n* [[Boolean ring]]\n* [[Preordered field]]\n* [[General frame]]\n\n==References==\n\n* [[Robert Goldblatt|Goldblatt, R.]], ''Algebraic Polymodal Logic: A Survey'', Logic Journal of the IGPL, Volume 8, Issue 4, p.&nbsp;393-450, July 2000\n* Goldblatt, R., ''Varieties of complex algebras'', Annals of Pure and Applied Logic, 44, p.&nbsp;173-242, 1989\n* {{cite book\n | last = Johnstone\n | first = Peter T.\n | authorlink = Peter T. Johnstone\n | year = 1982\n | title = Stone spaces\n | edition = 3rd\n | publisher = Cambridge University Press\n | location = Cambridge\n | isbn = 0-521-33779-8\n}}\n* Naturman, C.A., ''Interior Algebras and Topology'', Ph.D. thesis, University of Cape Town Department of Mathematics, 1991\n* Patrick Blackburn, Johan F.A.K. van Benthem, Frank Wolter ed., ''Handbook of Modal Logic, Volume 3 of Studies in Logic and Practical Reasoning'', Elsevier, 2006\n\n==External links==\n*{{springer|title=Algebra of sets|id=p/a011400}}\n\n[[Category:Boolean algebra]]\n[[Category:Set families]]"
    },
    {
      "title": "Formula game",
      "url": "https://en.wikipedia.org/wiki/Formula_game",
      "text": "A '''formula game''' is an artificial game represented by a [[True quantified Boolean formula|fully quantified Boolean formula]]. Players' turns alternate and the space of possible moves is denoted by [[bound variables]]. If a variable is [[Universal quantification|universally quantified]], the formula following it has the same [[truth value]] as the formula beginning with the universal quantifier regardless of the move taken. If a variable is [[Existential quantification|existentially quantified]], the formula following it has the same truth value as the formula beginning with the existential quantifier for at least one move available at the turn. Turns alternate, and a player loses if he cannot move at his turn. In [[Computational complexity theory|computational complexity]] theory, the language FORMULA-GAME is defined as all formulas <math>\\Phi</math> such that Player 1 has a winning strategy in the game represented by <math>\\Phi</math>. FORMULA-GAME is [[PSPACE-complete]].\n\n==References==\n\n* Sipser, Michael. (2006).  ''Introduction to the Theory of Computation''.  Boston: Thomson Course Technology.\n\n[[Category:Satisfiability problems]]\n[[Category:Boolean algebra]]\n[[Category:PSPACE-complete problems]]\n\n\n\n{{comp-sci-stub}}"
    },
    {
      "title": "Free Boolean algebra",
      "url": "https://en.wikipedia.org/wiki/Free_Boolean_algebra",
      "text": "In [[mathematics]], a '''free Boolean algebra''' is a [[Boolean algebra (structure)|Boolean algebra]] with a distinguished set of elements, called '''''generators''''', such that:\n#Each element of the Boolean algebra can be expressed as a finite combination of generators, using the Boolean operations, and\n#The generators are as ''independent'' as possible, in the sense that there are no relationships among them (again in terms of finite expressions using the Boolean operations) that do not hold in ''every'' Boolean algebra no matter ''which'' elements are chosen.\n\n==A simple example==\n[[File:Free-boolean-algebra-hasse-diagram.svg|right|300px|The [[Hasse diagram]] of the free Boolean algebra on two generators, ''p'' and ''q''. Take ''p (left circle)'' to be \"John is tall\" and ''q (right circle)''to be \"Mary is rich\". The atoms are the four elements in the row just above FALSE.]]\nThe [[generating set|generators]] of a free Boolean algebra can represent independent [[proposition]]s. Consider, for example, the propositions \"John is tall\" and \"Mary is rich\". These generate a Boolean algebra with four [[atomic (order theory)|atoms]], namely:\n*John is tall, and Mary is rich;\n*John is tall, and Mary is not rich;\n*John is not tall, and Mary is rich;\n*John is not tall, and Mary is not rich.\n\nOther elements of the Boolean algebra are then [[logical disjunction]]s of the atoms, such as \"John is tall and Mary is not rich, or John is not tall and Mary is rich\". In addition there is one more element, FALSE, which can be thought of as the empty disjunction; that is, the disjunction of no atoms.\n\nThis example yields a Boolean algebra with 16 elements; in general, for finite ''n'', the free Boolean algebra with ''n'' generators has 2<sup>''n''</sup>  [[atomic (order theory)|atoms]], and therefore <math>2^{2^n}</math> elements.\n\nIf there are [[infinite set|infinitely]] many [[generating set|generators]], a similar situation prevails except that now there are no [[atomic (order theory)|atoms]]. Each element of the Boolean algebra is a combination of finitely many of the generating propositions, with two such elements deemed identical if they are [[logical equivalence|logically equivalent]].\n\n==Category-theoretic definition==\nIn the language of [[category theory]], free Boolean algebras can be defined simply in terms of an [[adjoint functors|adjunction]] between the category of sets and functions, '''Set''', and the category of Boolean algebras and Boolean algebra homomorphisms, '''BA'''. In fact, this approach generalizes to any algebraic structure definable in the framework of [[universal algebra]].\n\nAbove, we said that a free Boolean algebra is a Boolean algebra with a set of generators that behave a certain way; alternatively, one might start with a set and ask which algebra it generates. Every set ''X'' generates a free Boolean algebra ''FX'' defined as the algebra such that for every algebra ''B'' and function ''f'' : ''X'' → ''B'', there is a unique Boolean algebra homomorphism ''f''&prime; : ''FX'' → ''B'' that extends ''f''. [[Commutative diagram|Diagrammatically]],\n[[File:Free-Boolean-algebra-unit-sloppy.png|center]]\nwhere ''i''<sub>''X''</sub> is the inclusion, and the dashed arrow denotes uniqueness. The idea is that once one chooses where to send the elements of ''X'', the [[Boolean_algebra_(structure)#Homomorphisms_and_isomorphisms|laws for Boolean algebra homomorphisms]] determine where to send everything else in the free algebra ''FX''. If ''FX'' contained elements inexpressible as combinations of elements of ''X'', then ''f''&prime; wouldn't be unique, and if the elements of ''X'' weren't sufficiently independent, then ''f''&prime; wouldn't be well defined! It is easily shown that ''FX'' is unique (up to isomorphism), so this definition makes sense. It is also easily shown that a free Boolean algebra with generating set X, as defined originally, is isomorphic to ''FX'', so the two definitions agree.\n\nOne shortcoming of the above definition is that the diagram doesn't capture that ''f''&prime; is a homomorphism; since it is a diagram in '''Set''' each arrow denotes a mere function. We can fix this by separating it into two diagrams, one in '''BA''' and one in '''Set'''. To relate the two, we introduce a [[functor]] ''U'' : '''BA''' → '''Set''' that \"[[Forgetful functor|forgets]]\" the algebraic structure, mapping algebras and homomorphisms to their underlying sets and functions.\n[[File:Free-Boolean-algebra-unit.png|center]]\nIf we interpret the top arrow as a diagram in '''BA''' and the bottom triangle as a diagram in '''Set''', then this diagram properly expresses that every function ''f'' : ''X'' → ''UB'' extends to a unique Boolean algebra homomorphism ''f''&prime; : ''FX'' → ''B''. The functor ''U'' can be thought of as a device to pull the homomorphism ''f''&prime; back into '''Set''' so it can be related to ''f''.\n\nThe remarkable aspect of this is that the latter diagram is one of the various (equivalent) definitions of when two functors are [[adjoint functors|adjoint]]. Our ''F'' easily extends to a functor '''Set''' → '''BA''', and our definition of ''X'' generating a free Boolean algebra ''FX'' is precisely that ''U'' has a [[left adjoint]] ''F''.\n\n==Topological realization==\nThe free Boolean algebra with κ [[generating set|generators]], where κ is a finite or infinite [[cardinal number]], may be realized as the collection of all [[clopen set|clopen]] [[subset]]s of {0,1}<sup>κ</sup>, given the [[product topology]] assuming that {0,1} has the [[discrete topology]]. For each α&lt;κ, the α''th'' generator is the set of all elements of {0,1}<sup>κ</sup> whose α''th'' coordinate is 1. In particular, the free Boolean algebra with <math>\\aleph_0</math> generators is the collection of all [[clopen set|clopen subsets]] of a [[Cantor space]], sometimes called the [[Cantor algebra]]. Surprisingly, this collection is [[countable set|countable]]. In fact, while the free Boolean algebra with ''n'' generators, ''n'' finite, has [[cardinality]] <math>2^{2^n}</math>, the free Boolean algebra with <math>\\aleph_0</math> generators, as for any free algebra with <math>\\aleph_0</math> generators and countably many finitary operations, has cardinality <math>\\aleph_0</math>.\n\nFor more on this [[topology|topological]] approach to free Boolean algebra, see [[Stone's representation theorem for Boolean algebras]].\n\n==See also==\n* [[Boolean algebra (structure)]]\n* [[Generating set]]\n\n==References==\n* Steve Awodey (2006) ''Category Theory'' (Oxford Logic Guides 49). Oxford University Press.\n* [[Paul Halmos]] and Steven Givant (1998) ''Logic as Algebra''. [[Mathematical Association of America]].\n* [[Saunders Mac Lane]] (1998) ''[[Categories for the Working Mathematician]]''. 2nd ed. (Graduate Texts in Mathematics 5). Springer-Verlag.\n* [[Saunders Mac Lane]] (1999) ''Algebra'', 3d. ed. [[American Mathematical Society]]. {{ISBN|0-8218-1646-2}}.\n* Robert R. Stoll, 1963. ''Set Theory and Logic'', chpt. 6.7. Dover reprint 1979.\n\n[[Category:Boolean algebra]]\n[[Category:Free algebraic structures]]"
    },
    {
      "title": "Functional completeness",
      "url": "https://en.wikipedia.org/wiki/Functional_completeness",
      "text": "In [[Mathematical logic|logic]], a '''functionally complete''' set of [[logical connective]]s or [[Boolean function|Boolean operators]] is one which can be used to express all possible [[truth table]]s by combining members of the [[Set (mathematics)|set]] into a [[Boolean expression]].<ref name=\"Enderton2001\">{{citation | last1=Enderton | first1=Herbert | title=A mathematical introduction to logic | publisher=[[Academic Press]] | location=Boston, MA | edition=2nd | isbn=978-0-12-238452-3 | year=2001}}. (\"Complete set of logical connectives\").</ref><ref name=\"Nolt1998\">{{citation | last1=Nolt | first1=John | last2=Rohatyn | first2=Dennis | last3=Varzi | first3=Achille | title=Schaum's outline of theory and problems of logic | publisher=[[McGraw–Hill]] | location=New York | edition=2nd | isbn=978-0-07-046649-4 | year=1998}}. (\"[F]unctional completeness of [a] set of logical operators\").</ref> A well-known complete set of connectives is {&nbsp;AND,&nbsp;NOT&nbsp;}, consisting of binary [[logical conjunction|conjunction]] and [[logical negation|negation]]. Each of the [[singleton (mathematics)|singleton]] sets {&nbsp;[[Sheffer stroke|NAND]]&nbsp;} and {&nbsp;[[Logical NOR|NOR]]&nbsp;} is functionally complete.\n\nIn a context of [[propositional logic]], functionally complete sets of connectives are also called '''(expressively) adequate'''.<ref name=\"Smith2003\">{{Citation | last1=Smith | first1=Peter | title=An introduction to formal logic | publisher=[[Cambridge University Press]] | isbn=978-0-521-00804-4 | year=2003}}. (Defines \"expressively adequate\", shortened to \"adequate set of connectives\" in a section heading.)</ref>\n\nFrom the point of view of [[digital electronics]], functional completeness means that every possible [[logic gate]] can be realized as a network of gates of the types prescribed by the set. In particular, all logic gates can be assembled from either only binary [[NAND gate]]s, or only binary [[NOR gate]]s.\n\n==Introduction==\nModern texts on logic typically take as primitive some subset of the connectives:  [[logical conjunction|conjunction]] (<math>\\land</math>); [[logical disjunction|disjunction]] (<math>\\lor</math>); [[negation]] (<math>\\neg</math>); [[material conditional]] (<math>\\to</math>); and possibly the [[Logical biconditional|biconditional]] (<math>\\leftrightarrow</math>). Further connectives can be defined, if so desired, by defining them in terms of these primitives. For example, NOR (sometimes denoted <math>\\downarrow</math>, the negation of the disjunction) can be expressed as conjunction of two negations:\n\n:<math>A \\downarrow B := \\neg A \\land \\neg B</math>\n\nSimilarly, the negation of the conjunction, NAND (sometimes denoted as <math>\\uparrow</math>), can be defined in terms of disjunction and negation. It turns out that every binary connective can be defined in terms of <math>\\{ \\neg, \\land, \\lor, \\to, \\leftrightarrow \\}</math>, so this set is functionally complete.\n\nHowever, it still contains some redundancy: this set is not a ''minimal'' functionally complete set, because the conditional and biconditional can be defined in terms of the other connectives as\n\n:<math>\\begin{align}\n  A \\to B &:= \\neg A \\lor B\\\\\n  A \\leftrightarrow B &:= (A \\to B) \\land (B \\to A).\n\\end{align}</math>\n\nIt follows that the smaller set <math>\\{\\neg, \\land, \\lor\\}</math> is also functionally complete. But this is still not minimal, as <math>\\lor</math> can be defined as\n\n:<math>A \\lor B := \\neg(\\neg A \\land \\neg B).</math>\n\nAlternatively, <math>\\land</math> may be defined in terms of <math>\\lor</math> in a similar manner, or <math>\\lor</math> may be defined in terms of <math> \\rightarrow </math>:\n\n:<math> \\ A \\vee B := \\neg A \\rightarrow B. </math>\n\nNo further simplifications are possible. Hence, every two-element set of connectives containing <math>\\neg</math> and one of <math>\\{\\land, \\lor, \\rightarrow\\}</math> is a minimal functionally complete [[subset]] of <math>\\{\\neg, \\land, \\lor, \\to, \\leftrightarrow\\}</math>.\n\n==Formal definition==\nGiven the [[Boolean domain]] '''B'''&nbsp;=&nbsp;{0,1}, a set ''F'' of Boolean functions ''ƒ''<sub>i</sub>:&nbsp;'''B'''<sup>''n<sub>i</sub>''</sup>&nbsp;→&nbsp;'''B''' is '''functionally complete''' if the [[clone (algebra)|clone]] on '''B''' generated by the basic functions ''ƒ''<sub>i</sub> contains all functions ''ƒ'':&nbsp;'''B'''<sup>''n''</sup>&nbsp;→&nbsp;'''B''', for all ''strictly positive'' integers {{nowrap|''n'' ≥ 1}}. In other words, the set is functionally complete if every Boolean function that takes at least one variable can be expressed in terms of the functions ''ƒ''<sub>i</sub>. Since every Boolean function of at least one variable can be expressed in terms of binary Boolean functions, ''F'' is functionally complete if and only if every binary Boolean function can be expressed in terms of the functions in ''F''.\n\nA more natural condition would be that the clone generated by ''F'' consist of all functions ''ƒ'':&nbsp;'''B'''<sup>''n''</sup>&nbsp;→&nbsp;'''B''', for all integers {{nowrap|''n'' ≥ 0}}. However, the examples given above are not functionally complete in this stronger sense because it is not possible to write a [[arity|nullary]] function, i.e. a constant expression, in terms of ''F'' if ''F'' itself does not contain at least one nullary function. With this stronger definition, the smallest functionally complete sets would have 2 elements.\n\nAnother natural condition would be that the clone generated by ''F'' together with the two nullary constant functions be functionally complete or, equivalently, functionally complete in the strong sense of the previous paragraph. The example of the Boolean function given by ''S''(''x'',&nbsp;''y'',&nbsp;''z'')&nbsp;=&nbsp;''z'' if ''x''&nbsp;=&nbsp;''y'' and ''S''(''x'',&nbsp;''y'',&nbsp;''z'')&nbsp;=&nbsp;''x'' otherwise shows that this condition is strictly weaker than functional completeness.<ref name=Wesselkamper1975a>{{citation\n | title = A sole sufficient operator\n | url = http://projecteuclid.org/euclid.ndjfl/1093891614\n | year = 1975\n | author = Wesselkamper, T.C.\n | journal = Notre Dame Journal of Formal Logic\n | volume = 16\n | pages = 86–88\n | doi = 10.1305/ndjfl/1093891614\n}}</ref><ref name=Massey1975>{{citation\n | title = Concerning an alleged Sheffer function\n | url = http://projecteuclid.org/euclid.ndjfl/1093891898\n | year = 1975\n | author = Massey, G.J.\n | journal = Notre Dame Journal of Formal Logic\n | volume = 16\n | pages = 549–550\n | doi = 10.1305/ndjfl/1093891898\n | issue = 4\n}}</ref><ref name=Wesselkamper1975b>{{citation\n | title = A Correction To My Paper\" A. Sole Sufficient Operator\n | url = http://projecteuclid.org/euclid.ndjfl/1093891899\n | year = 1975\n | author = Wesselkamper, T.C.\n | journal = Notre Dame Journal of Formal Logic\n | volume = 16\n | pages = 551\n | doi = 10.1305/ndjfl/1093891899\n | issue = 4\n}}</ref>\n\n==Characterization of functional completeness==\n{{further|Post's lattice}}\n[[Emil Leon Post|Emil Post]] proved that a set of logical connectives is functionally complete if and only if it is not a subset of any of the following sets of connectives:\n\n* The [[monotonic]] connectives; changing the truth value of any connected variables from '''F''' to '''T''' without changing any from '''T''' to '''F''' never makes these connectives change their return value from '''T''' to '''F''', e.g. <math>\\vee, \\wedge, \\top, \\bot</math>.\n* The [[affine transformation|affine]] connectives, such that each connected variable either always or never affects the truth value these connectives return, e.g. <math>\\neg, \\top, \\bot, \\leftrightarrow, \\nleftrightarrow</math>.\n* The '''self-dual''' connectives, which are equal to their own [[de Morgan dual]]; if the truth values of all variables are reversed, so is the truth value these connectives return, e.g. <math>\\neg</math>, ''[[majority function|MAJ]]''(''p'',''q'',''r'').\n* The '''truth-preserving''' connectives; they return the [[truth value]] '''T''' under any interpretation which assigns '''T''' to all variables, e.g. <math>\\vee, \\wedge, \\top, \\rightarrow, \\leftrightarrow</math>.\n* The '''falsity-preserving''' connectives;  they return the truth value '''F''' under any interpretation which assigns '''F''' to all variables, e.g. <math>\\vee, \\wedge, \\bot, \\nrightarrow, \\nleftrightarrow</math>.\n\nIn fact, Post gave a complete description of the [[lattice (order)|lattice]] of all [[clone (algebra)|clone]]s (sets of operations closed under composition and containing all projections) on the two-element set {'''T''', '''F'''}, nowadays called [[Post's lattice]], which implies the above result as a simple corollary: the five mentioned sets of connectives are exactly the maximal clones.\n\n==Minimal functionally complete operator sets==\nWhen a single logical connective or Boolean operator is functionally complete by itself, it is called a '''Sheffer function'''<ref name=Martin1989>The term was originally restricted to ''binary'' operations, but since the end of the 20th century it is used more generally. {{citation\n | title =  Systems of logic\n | year = 1989\n | author = Martin, N.M.\n | publisher = Cambridge University Press\n | isbn = 978-0-521-36770-7\n | page = 54\n}}.</ref> or sometimes a '''sole sufficient operator'''. There are no [[unary operation|unary]] operators with this property. [[Logical NAND|NAND]] and [[Logical NOR|NOR]] , which are [[Boolean algebra#Duality principle|dual to each other]], are the only two binary Sheffer functions. These were discovered, but not published, by [[Charles Sanders Peirce]] around 1880, and rediscovered independently and published by [[Henry M. Sheffer]] in 1913.<ref name=Scharle1965>{{Citation\n | title = Axiomatization of propositional calculus with Sheffer functors\n | url = http://projecteuclid.org/euclid.ndjfl/1093958259\n | year = 1965\n | author = Scharle, T.W.\n | journal = Notre Dame J. Formal Logic\n | pages = 209–217\n | volume = 6\n | doi = 10.1305/ndjfl/1093958259\n | issue = 3\n}}.</ref>\nIn digital electronics terminology, the binary [[NAND gate]] and the binary [[NOR gate]] are the only binary [[universal logic gate]]s.\n\nThe following are the minimal functionally complete sets of logical connectives with [[arity]] ≤&nbsp;2:<ref name=\"Wernick\">Wernick, William (1942) \"Complete Sets of Logical Functions,\" ''Transactions of the American Mathematical Society 51'': 117&ndash;32. In his list on the last page of the article, Wernick does not distinguish between ← and →, or between <math>\\nleftarrow</math> and <math>\\nrightarrow</math>.</ref>\n\n;One element: {↑}, {↓}.\n;Two elements: <math>\\{\\vee, \\neg\\}</math>, <math>\\{\\wedge, \\neg\\}</math>, <math>\\{\\to, \\neg\\}</math>, <math>\\{\\gets, \\neg\\}</math>, <math>\\{\\to, \\bot\\}</math>, <math>\\{\\gets, \\bot\\}</math>, <math>\\{\\to, \\nleftrightarrow\\}</math>, <math>\\{\\gets, \\nleftrightarrow\\}</math>, <math>\\{\\to, \\nrightarrow\\}</math>, <math>\\{\\to, \\nleftarrow\\}</math>, <math>\\{\\gets, \\nrightarrow\\}</math>, <math>\\{\\gets, \\nleftarrow\\}</math>, <math>\\{\\nrightarrow, \\neg\\}</math>, <math>\\{\\nleftarrow, \\neg\\}</math>, <math>\\{\\nrightarrow, \\top\\}</math>, <math>\\{\\nleftarrow, \\top\\}</math>, <math>\\{\\nrightarrow, \\leftrightarrow\\}</math>, <math>\\{\\nleftarrow, \\leftrightarrow\\}</math>.\n;Three elements: <math>\\{\\lor, \\leftrightarrow, \\bot\\}</math>, <math>\\{\\lor, \\leftrightarrow, \\nleftrightarrow\\}</math>, <math>\\{\\lor, \\nleftrightarrow, \\top\\}</math>, <math>\\{\\land, \\leftrightarrow, \\bot\\}</math>, <math>\\{\\land, \\leftrightarrow, \\nleftrightarrow\\}</math>, <math>\\{\\land, \\nleftrightarrow, \\top\\}</math>.\n\nThere are no minimal functionally complete sets of more than three at most binary logical connectives.<ref name=\"Wernick\" /> In order to keep the lists above readable, operators that ignore one or more inputs have been omitted.  For example, an operator that ignores the first input and outputs the negation of the second could be substituted for a unary negation.\n\n== Examples ==\n\n* Examples of using the <code>NAND</code>(&uarr;) completeness. As illustrated by,<ref>\"NAND Gate Operations\" at http://hyperphysics.phy-astr.gsu.edu/hbase/electronic/nand.html</ref>\n** ¬A = A &uarr; A\n** A &and; B = ¬(A &uarr; B) = (A &uarr; B) &uarr; (A &uarr; B)\n** A &or; B = (A &uarr; A) &uarr; (B &uarr; B)\n* Examples of using the <code>NOR</code>(&darr;) completeness. As illustrated by,<ref>\"NOR Gate Operations\" at http://hyperphysics.phy-astr.gsu.edu/hbase/electronic/nor.html</ref>\n** ¬A = A &darr; A\n** A &or; B = ¬(A &darr; B) = (A &darr; B) &darr; (A &darr; B)\n** A &and; B = (A &darr; A) &darr; (B &darr; B)\n\nNote that, an electronic circuit or a software function is optimized by the reuse, that reduce the number of gates.  For instance, the \"A &and; B\" operation, when expressed by &uarr; gates, is implemented with the reuse of \"A &uarr; B\",\n: X = (A &uarr; B);  A &and; B = X &uarr; X\n\n== In other domains ==\nApart from logical connectives (Boolean operators), functional completeness can be introduced in other domains. For example, a set of [[reversible computation|reversible]] gates is called functionally complete, if it can express every reversible operator.\n\nThe 3-input [[Fredkin gate]] is functionally complete reversible gate by itself&nbsp;– a sole sufficient operator. There are many other [[three-input universal logic gate]]s, such as the [[Toffoli gate]].\n\nIn [[quantum computing]], the [[Hadamard gate]] and the [[quantum logic gate#Phase shift gates|T gate]] are universal, albeit with a [[quantum logic gate#Universal quantum gates|slightly more restrictive definition]] than that of functional completeness.\n\n== Set theory ==\nThere is an [[isomorphism]] between the [[algebra of sets]] and the [[Boolean algebra]], that is, they have the same [[Boolean algebra (structure)|structure]]. Then, if we map boolean operators into set operators, the \"translated\" above text are valid also for sets: there are many \"minimal complete set of set-theory operators\" that can generate any other set relations. The more popular \"Minimal complete operator sets\" are {&not;, &cap;} and {&not;, &cup;}. If the [[universal set]] [[Russell's Paradox|is forbidden]], set operators are restricted to being falsity- (Ø) preserving, and cannot be equivalent to functionally complete Boolean algebra.\n\n==See also==\n* [[Completeness (logic)]]\n* [[Algebra of sets]]\n* [[Boolean algebra]]\n* [[NAND logic]]\n* [[NOR logic]]\n\n==References==\n{{Reflist|30em}}\n\n[[Category:Boolean algebra]]\n[[Category:Propositional calculus]]\n[[Category:Logic in computer science]]"
    },
    {
      "title": "Implicant",
      "url": "https://en.wikipedia.org/wiki/Implicant",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\nIn [[Boolean logic]], the term '''implicant''' has either a generic or a particular meaning.  In the generic use, it refers to the hypothesis of an implication ([[wiktionary:implicant]]).  In the particular use, it refers to specific instance of this generic meaning, which occurs relative to a formula that is either a sum of products or a product of sums, as explained further below.\n\nIn its particular use, an '''implicant''' is a \"covering\" (sum term or product term) of one or more [[minterm]]s in a [[sum of products]] (or maxterms in product of sums) of a Boolean function. Formally, a [[product term]] ''P'' in a [[sum of products]] is an '''implicant''' of the [[Boolean function]] ''F'' if ''P'' [[Logical implication|implies]] ''F''. More precisely:\n\n: ''P'' implies ''F'' (and thus is an implicant of ''F'') if ''F'' also takes the value 1 whenever ''P'' equals 1.\nwhere\n* ''F'' is a [[Boolean function]] of ''n'' variables.\n* ''P'' is a [[product term]].\n\nThis means that ''P'' <math>\\le</math> ''F'' with respect to the natural ordering of the Boolean space. For instance, the function\n\n:<math>f(x,y,z,w)=xy+yz+w</math>\n\nis implied by <math>xy</math>, by <math>xyz</math>, by <math>xyzw</math>, by <math>w</math> and many others; these are the implicants of <math>f</math>.\n\n== {{anchor|Essential prime implicant}}Prime implicant ==<!-- Section header \"Prime implicant\" used by redirects -->\nA '''prime implicant''' of a function is an implicant that cannot be covered by a more general, (more reduced - meaning with fewer [[literal (computer programming)|literal]]s) implicant. [[W.&nbsp;V. Quine]] defined a ''prime implicant'' of ''F'' to be an implicant that is minimal - that is, the removal of any literal from ''P'' results in a non-implicant for ''F''.  '''Essential prime implicants''' (aka '''core prime implicants''') are prime implicants that cover an output of the function that no combination of other prime implicants is able to cover.\n\nUsing the example above, one can easily see that while <math>xy</math> (and others) is a prime implicant, <math>xyz</math> and <math>xyzw</math> are not. From the latter, multiple literals can be removed to make it prime:\n\n*<math>x</math>, <math>y</math> and <math>z</math> can be removed, yielding <math>w</math>.\n*Alternatively, <math>z</math> and <math>w</math> can be removed, yielding <math>xy</math>.\n*Finally, <math>x</math> and <math>w</math> can be removed, yielding <math>yz</math>.\n\nThe process of removing literals from a Boolean term is called '''expanding''' the term. Expanding by one literal doubles the number of input combinations for which the term is true (in binary Boolean algebra). Using the example function above, we may expand <math>xyz</math> to <math>xy</math> or to <math>yz</math> without changing the cover of <math>f</math>.<ref>De Micheli, Giovanni. ''Synthesis and Optimization of Digital Circuits''. McGraw-Hill, Inc.,  1994</ref>\n\nThe sum of all prime implicants of a Boolean function is called its '''complete sum''', '''minimal covering sum''', or  [[Blake canonical form]].\n\n==See also==\n* [[Quine&ndash;McCluskey algorithm]]\n* [[Karnaugh map]]\n* [[Petrick's method]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.cs.ualberta.ca/~amaral/courses/329/webslides/Topic4-KarnaughMaps/sld021.htm Slides explaining implicants, prime implicants and essential prime implicants]\n* [http://web.cecs.pdx.edu/~mcnames/ECE171/Lectures/Lecture10.html Examples of finding essential prime implicants using K-map]\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Implication graph",
      "url": "https://en.wikipedia.org/wiki/Implication_graph",
      "text": "[[Image:Implication graph.svg|thumb|upright=1.35|An implication graph representing the [[2-satisfiability]] instance <math>\\scriptscriptstyle (x_0\\lor x_2)\\land(x_0\\lor\\lnot x_3)\\land(x_1\\lor\\lnot x_3)\\land(x_1\\lor\\lnot x_4)\\land(x_2\\lor\\lnot x_4)\\land{}\\atop\\quad\\scriptscriptstyle(x_0\\lor\\lnot x_5)\\land (x_1\\lor\\lnot x_5)\\land (x_2\\lor\\lnot x_5)\\land (x_3\\lor x_6)\\land (x_4\\lor x_6)\\land (x_5\\lor x_6).</math>]]\nIn [[mathematical logic]], an '''implication graph''' is a [[skew-symmetric graph|skew-symmetric]] [[directed graph]] ''G = (V, E)'' composed of vertex set ''V'' and directed edge set ''E''. Each vertex in ''V'' represents the truth status of a [[Boolean literal]], and each directed edge from vertex ''u'' to vertex ''v'' represents the [[material conditional|material implication]] \"If the literal ''u'' is true then the literal ''v'' is also true\". Implication graphs were originally used for analyzing complex [[Boolean expression]]s.\n\n==Applications==\nA [[2-satisfiability]] instance in [[conjunctive normal form]] can be transformed into an implication graph by replacing each of its [[disjunction]]s by a pair of implications. For example, the statement <math>(x_0\\lor x_1)</math> can be rewritten as the pair <math>(\\neg x_0 \\rightarrow x_1), (\\neg x_1 \\rightarrow x_0)</math>. An instance is satisfiable if and only if no literal and its negation belong to the same [[strongly connected component]] of its implication graph; this characterization can be used to solve 2-satisfiability instances in linear time.<ref>{{cite journal|author1= Aspvall, Bengt |author2=[[Michael Plass|Plass, Michael F.]] |author3=[[Robert Tarjan|Tarjan, Robert E.]] |title = A linear-time algorithm for testing the truth of certain quantified boolean formulas|journal = Information Processing Letters | volume = 8 | issue = 3 | pages = 121–123|year = 1979|doi = 10.1016/0020-0190(79)90002-4}}</ref>\n\nIn [[CDCL]] [[Boolean satisfiability problem|SAT]]-solvers, [[unit propagation]] can be naturally associated with an implication graph that captures all possible ways of deriving all implied literals from decision literals,<ref>{{cite conference|author1=Paul Beame |author2=Henry Kautz |author3=Ashish Sabharwal |title = Understanding the Power of Clause Learning| conference = IJCAI |  pages = 1194–1201|year = 2003|url=https://www.cs.rochester.edu/u/kautz/papers/learnIjcai.pdf}}</ref> which is then used for clause learning.\n\n==References==\n<references/>\n\n[[Category:Boolean algebra]]\n[[Category:Application-specific graphs]]\n[[Category:Directed graphs]]\n[[Category:Graph families]]"
    },
    {
      "title": "Inclusion (Boolean algebra)",
      "url": "https://en.wikipedia.org/wiki/Inclusion_%28Boolean_algebra%29",
      "text": "In [[Boolean algebra (structure)]], the '''inclusion relation''' <math>a\\le b</math> is defined as <math>ab'=0</math> and is the Boolean analogue to the [[subset]] relation in [[set theory]]. Inclusion is a [[partial order]].\n\nThe inclusion relation <math>a<b</math> can be expressed in many ways:\n* <math>a<b</math>\n* <math>ab'=0</math>\n* <math>a'+b=1</math>\n* <math>b'<a'</math>\n* <math>a+b=b</math>\n* <math>ab=a</math>\n\nThe inclusion relation has a natural interpretation in various Boolean algebras: in the subset algebra, the [[subset]] relation; in arithmetic Boolean algebra, [[divisor|divisibility]]; in the [[Propositional formula#An algebra of propositions.2C the propositional calculus|algebra of propositions]], [[Material conditional|material implication]]; in the two-element algebra, the set { (0,0), (0,1), (1,1) }.\n\nSome useful properties of the inclusion relation are:\n* <math>a\\le a+b</math>\n* <math>ab\\le a</math>\n\nThe inclusion relation may be used to define '''Boolean intervals''' such that <math>a\\le x\\le b</math> A Boolean algebra whose carrier set is restricted to the elements in an interval is itself a Boolean algebra.\n\n==References==\n\n* Frank Markham Brown, ''Boolean Reasoning: The Logic of Boolean Equations'', 2nd edition, 2003, p. 52\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Karnaugh map",
      "url": "https://en.wikipedia.org/wiki/Karnaugh_map",
      "text": "{{Expand German|Karnaugh-Veitch-Diagramm|date=February 2018}}\n{{bots|deny=Citation bot}}\n{{Use dmy dates|date=April 2019|cs1-dates=y}}\n[[File:K-map 6,8,9,10,11,12,13,14 anti-race.svg|thumb|right|An example Karnaugh map. This image actually shows two Karnaugh maps: for the function ''ƒ'', using [[minterm]]s (colored rectangles) and for its complement, using [[maxterm]]s (gray rectangles). In the image, ''E''() signifies a sum of minterms, denoted in the article as <math>\\sum m_i</math>.]]\n\nThe '''Karnaugh map''' ('''KM''' or '''K-map''') is a method of simplifying [[Boolean algebra]] expressions. [[Maurice Karnaugh]] introduced it in 1953<ref name=\"Karnaugh_1953\"/><ref name=\"Curtis_1962\"/> as a refinement of [[Edward Veitch]]'s 1952 '''Veitch chart''',<ref name=\"Veitch_1952\"/><ref name=\"Brown_2012\"/> which actually was a rediscovery of [[Allan Marquand]]'s 1881 ''logical diagram''<ref name=\"Marquand_1881\"/> aka '''Marquand diagram'''<ref name=\"Brown_2012\"/> but with a focus now set on its utility for switching circuits.<ref name=\"Brown_2012\"/> Veitch charts are therefore also known as '''Marquand–Veitch diagrams''',<ref name=\"Brown_2012\"/> and Karnaugh maps as '''Karnaugh–Veitch maps''' ('''KV maps''').\n\nThe Karnaugh map reduces the need for extensive calculations by taking advantage of humans' pattern-recognition capability.<ref name=\"Karnaugh_1953\"/> It also permits the rapid identification and elimination of potential [[race condition]]s.\n\nThe required Boolean results are transferred from a [[truth table]] onto a two-dimensional grid where, in Karnaugh maps, the cells are ordered in [[Gray code]],<ref name=\"Wakerly_1994\"/><ref name=\"Brown_2012\"/> and each cell position represents one combination of input conditions, while each cell value represents the corresponding output value. Optimal groups of 1s or 0s are identified, which represent the terms of a [[Canonical form (Boolean algebra)|canonical form]] of the logic in the original truth table.<ref name=\"Belton_1998\"/> These terms can be used to write a minimal Boolean expression representing the required logic.\n\nKarnaugh maps are used to simplify real-world logic requirements so that they can be implemented using a minimum number of physical logic gates. A [[Disjunctive normal form|sum-of-products expression]] can always be implemented using [[AND gate]]s feeding into an [[OR gate]], and a [[Conjunctive normal form|product-of-sums expression]] leads to OR gates feeding an AND gate.<ref name=\"Dodge_2016\"/> Karnaugh maps can also be used to simplify logic expressions in software design. Boolean conditions, as used for example in [[Conditional (programming)|conditional statements]], can get very complicated, which makes the code difficult to read and to maintain. Once minimised, canonical sum-of-products and product-of-sums expressions can be implemented directly using AND and OR logic operators.<ref name=\"Cook_2012\"/> Diagrammatic and mechanical methods for minimizing simple logic expressions have existed since at least the medieval times. More systematic methods for minimizing complex expressions began to be developed in the early 1950s, but until the mid to late 1980's the Karnaugh map was the most common used in practice.<ref name=\"Wolfram_2002\"/>\n\n==Example==\nKarnaugh maps are used to facilitate the simplification of [[Boolean algebra (logic)|Boolean algebra]] functions. For example, consider the Boolean function described by the following [[truth table]].\n\n{| class=\"wikitable\" style=\"text-align: center\"\n|+ Truth table of a function\n|-\n! &nbsp; !! ''A'' !! ''B'' !! ''C'' !! ''D'' !! {{tmath|f(A, B, C, D)}}</font>\n|-\n! scope=\"row\" |  0 \n| 0 || 0 || 0 || 0 || 0\n|-\n! scope=\"row\" |  1 \n| 0 || 0 || 0 || 1 || 0\n|-\n! scope=\"row\" |  2 \n| 0 || 0 || 1 || 0 || 0\n|-\n! scope=\"row\" |  3 \n| 0 || 0 || 1 || 1 || 0\n|-\n! scope=\"row\" |  4 \n| 0 || 1 || 0 || 0 || 0\n|-\n! scope=\"row\" |  5 \n| 0 || 1 || 0 || 1 || 0\n|-\n! scope=\"row\" |  6 \n| 0 || 1 || 1 || 0 || 1\n|-\n! scope=\"row\" |  7 \n| 0 || 1 || 1 || 1 || 0\n|-\n! scope=\"row\" |  8 \n| 1 || 0 || 0 || 0 || 1\n|-\n! scope=\"row\" |  9 \n| 1 || 0 || 0 || 1 || 1\n|-\n! scope=\"row\" | 10 \n| 1 || 0 || 1 || 0 || 1\n|-\n! scope=\"row\" | 11 \n| 1 || 0 || 1 || 1 || 1\n|-\n! scope=\"row\" | 12 \n| 1 || 1 || 0 || 0 || 1\n|-\n! scope=\"row\" | 13 \n| 1 || 1 || 0 || 1 || 1\n|-\n! scope=\"row\" | 14 \n| 1 || 1 || 1 || 0 || 1\n|-\n! scope=\"column\" | 15 \n| 1 || 1 || 1 || 1 || 0\n|}\n\nFollowing are two different notations describing the same function in unsimplified Boolean algebra, using the Boolean variables {{mvar|A}}, {{mvar|B}}, {{mvar|C}}, {{mvar|D}}, and their inverses.\n\n*<math>f(A, B, C, D) = \\sum_{}m_i, i \\in \\{6, 8, 9, 10, 11, 12, 13, 14\\}</math> where <math>m_i</math> are the [[minterm]]s to map (i.e., rows that have output 1 in the truth table).\n*<math>f(A, B, C, D) = \\prod_{}M_i, i \\in \\{0, 1, 2, 3, 4, 5, 7, 15\\}</math> where <math>M_i</math> are the [[maxterm]]s to map (i.e., rows that have output 0 in the truth table).\n\n===Karnaugh map===\n[[File:Karnaugh6.gif|center|thumb|400px|K-map drawn on a torus, and in a plane. The dot-marked cells are adjacent.]]\n[[File:K-map minterms A.svg|thumb|right|K-map construction. Instead of the output values (the rightmost values in the truth table), this diagram shows a decimal representation of the input ABCD (the leftmost values in the truth table), therefore it is not a Karnaugh map.]]\n[[File:Torus from rectangle.gif|thumb|right|In three dimensions, one can bend a rectangle into a torus.]]\n\nIn the example above, the four input variables can be combined in 16 different ways, so the truth table has 16 rows, and the Karnaugh map has 16 positions. The Karnaugh map is therefore arranged in a 4&nbsp;×&nbsp;4 grid.\n\nThe row and column indices (shown across the top, and down the left side of the Karnaugh map) are ordered in [[Gray code]] rather than binary numerical order. Gray code ensures that only one variable changes between each pair of adjacent cells. Each cell of the completed Karnaugh map contains a binary digit representing the function's output for that combination of inputs.\n\nAfter the Karnaugh map has been constructed, it is used to find one of the simplest possible forms — a [[Canonical form (Boolean algebra)|canonical form]] — for the information in the truth table. Adjacent 1s in the Karnaugh map represent opportunities to simplify the expression. The minterms ('minimal terms') for the final expression are found by encircling groups of 1s in the map. Minterm groups must be rectangular and must have an area that is a power of two (i.e., 1,&nbsp;2,&nbsp;4,&nbsp;8…). Minterm rectangles should be as large as possible without containing any 0s. Groups may overlap in order to make each one larger. The optimal groupings in the example below are marked by the green, red and blue lines, and the red and green groups overlap. The red group is a 2&nbsp;×&nbsp;2 square, the green group is a 4&nbsp;×&nbsp;1 rectangle, and the overlap area is indicated in brown.\n\nThe cells are often denoted by a shorthand which describes the logical value of the inputs that the cell covers. For example, {{mvar|AD}} would mean a cell which covers the 2x2 area where {{mvar|A}} and {{mvar|D}} are true, i.e. the cells numbered 13, 9, 15, 11 in the diagram above. On the other hand, {{mvar|A{{overline|D}}}} would mean the cells where {{mvar|A}} is true and {{mvar|D}} is false (that is, {{mvar|{{overline|D}}}} is true).\n\nThe grid is [[torus|toroidally]] connected, which means that rectangular groups can wrap across the edges (see picture). Cells on the extreme right are actually 'adjacent' to those on the far left, in the sense that the corresponding input values only differ by one bit; similarly, so are those at the very top and those at the bottom. Therefore, {{mvar|A{{overline|D}}}} can be a valid term—it includes cells 12 and 8 at the top, and wraps to the bottom to include cells 10 and 14—as is {{mvar|{{overline|B}}, {{overline|D}}}}, which includes the four corners.\n\n===Solution===\n[[File:K-map 6,8,9,10,11,12,13,14.svg|thumb|Diagram showing two K-maps. The K-map for the function f(A, B, C, D) is shown as colored rectangles which correspond to minterms. The brown region is an overlap of the red 2×2 square and the green 4×1 rectangle. The K-map for the inverse of f is shown as gray rectangles, which correspond to maxterms.]]\n\nOnce the Karnaugh map has been constructed and the adjacent 1s linked by rectangular and square boxes, the algebraic minterms can be found by examining which variables stay the same within each box.\n\nFor the red grouping:\n* ''A'' is the same and is equal to 1 throughout the box, therefore it should be included in the algebraic representation of the red minterm.\n* ''B'' does not maintain the same state (it shifts from 1 to 0), and should therefore be excluded.\n* ''C'' does not change. It is always 0, so its complement, NOT-C, should be included. Thus, {{mvar|{{overline|C}}}} should be included.\n* ''D'' changes, so it is excluded.\n\nThus the first minterm in the Boolean sum-of-products expression is {{mvar|A{{overline|C}}}}.\n\nFor the green grouping, ''A'' and ''B'' maintain the same state, while ''C'' and ''D'' change. ''B'' is 0 and has to be negated before it can be included. The second term is therefore {{mvar|A{{overline|B}}}}. Note that it is acceptable that the green grouping overlaps with the red one.\n\nIn the same way, the blue grouping gives the term {{mvar|BC{{overline|D}}}}.\n\nThe solutions of each grouping are combined: the normal form of the circuit is <math>A\\overline{C} + A\\overline{B} + BC\\overline{D}</math>.\n\nThus the Karnaugh map has guided a simplification of\n:<math>\\begin{align}\n              f(A, B, C, D) = {} &\\overline{A}BC\\overline{D} + A\\overline{B}\\,\\overline{C}\\,\\overline{D} + A\\overline{B}\\,\\overline{C}D + A\\overline{B}C\\overline{D} + {}\\\\\n                                 &A\\overline{B}CD + AB\\overline{C}\\,\\overline{D} + AB\\overline{C}D + ABC\\overline{D}\\\\\n                            = {} &A\\overline{C} + A\\overline{B} + BC\\overline{D}\n\\end{align}</math>\n\nIt would also have been possible to derive this simplification by carefully applying the [[Boolean algebra (logic)#Axiomatizing Boolean algebra|axioms of boolean algebra]], but the time it takes to do that grows exponentially with the number of terms.\n\n===Inverse===\nThe inverse of a function is solved in the same way by grouping the 0s instead.\n\nThe three terms to cover the inverse are all shown with grey boxes with different colored borders:\n*brown: {{mvar|{{overline|A}}, {{overline|B}}}}\n*gold: {{mvar|{{overline|A}}, {{overline|C}}}}\n*blue: {{mvar|BCD}}\n\nThis yields the inverse:\n:<math>\\overline{f(A,B,C,D)} = \\overline{A}\\,\\overline{B} + \\overline{A}\\,\\overline{C} + BCD</math>\n\nThrough the use of [[De Morgan's laws]], the [[product of sums]] can be determined:\n:<math>\\begin{align}\n f(A,B,C,D) &= \\overline{\\overline{f(A,B,C,D)}} \\\\\n            &= \\overline{\\overline{A}\\,\\overline{B} + \\overline{A}\\,\\overline{C} + BCD} \\\\\n            &= \\left(\\overline{\\overline{A}\\,\\overline{B}}\\right) \\left(\\overline{\\overline{A}\\,\\overline{C}}\\right) \\left(\\overline{BCD}\\right) \\\\\n            &= \\left(A + B\\right)\\left(A + C\\right)\\left(\\overline{B} + \\overline{C} + \\overline{D}\\right)\n\\end{align}</math>\n\n===Don't cares===\n[[File:K-map 6,8,9,10,11,12,13,14 don't care.svg|thumb|The value of {{tmath|f(A,B,C,D)}} for ''ABCD'' = 1111 is replaced by a \"don't care\". This removes the green term completely and allows the red term to be larger. It also allows blue inverse term to shift and become larger]]\n\nKarnaugh maps also allow easy minimizations of functions whose truth tables include \"[[Don't-care (logic)|don't care]]\" conditions. A \"don't care\" condition is a combination of inputs for which the designer doesn't care what the output is. Therefore, \"don't care\" conditions can either be included in or excluded from any rectangular group, whichever makes it larger. They are usually indicated on the map with a dash or X.\n\nThe example on the right is the same as the example above but with the value of ''f''(1,1,1,1) replaced by a \"don't care\". This allows the red term to expand all the way down and, thus, removes the green term completely.\n\nThis yields the new minimum equation:\n:<math>f(A,B,C,D) = A + BC\\overline{D}</math>\n\nNote that the first term is just {{mvar|A}}, not {{mvar|A{{overline|C}}}}. In this case, the don't care has dropped a term (the green rectangle); simplified another (the red one); and removed the race hazard (removing the yellow term as shown in the following section on race hazards).\n\nThe inverse case is simplified as follows:\n:<math>\\overline{f(A,B,C,D)} = \\overline{A}\\,\\overline{B} + \\overline{A}\\,\\overline{C} + \\overline{A}D</math>\n\n==Race hazards==\n===Elimination===\nKarnaugh maps are useful for detecting and eliminating [[race condition]]s. Race hazards are very easy to spot using a Karnaugh map, because a race condition may exist when moving between any pair of adjacent, but disjoint, regions circumscribed on the map. However, because of the nature of Gray coding, ''adjacent'' has a special definition explained above – we're in fact moving on a torus, rather than a rectangle, wrapping around the top, bottom, and the sides.\n\n* In the example [[Karnaugh map#Solution|above]], a potential race condition exists when ''C'' is 1 and ''D'' is 0, ''A'' is 1, and ''B'' changes from 1 to 0 (moving from the blue state to the green state). For this case, the output is defined to remain unchanged at 1, but because this transition is not covered by a specific term in the equation, a potential for a ''glitch'' (a momentary transition of the output to 0) exists.\n* There is a second potential glitch in the same example that is more difficult to spot: when ''D'' is 0 and ''A'' and ''B'' are both 1, with C changing from 1 to 0 (moving from the blue state to the red state). In this case the glitch wraps around from the top of the map to the bottom.\n\n[[File:K-map 6,8,9,10,11,12,13,14.svg|thumb|Race hazards are present in this diagram.]]\n[[File:K-map 6,8,9,10,11,12,13,14 anti-race.svg|thumb|Above diagram with consensus terms added to avoid race hazards.]]\n\nWhether glitches will actually occur depends on the physical nature of the implementation, and whether we need to worry about it depends on the application. In clocked logic, it is enough that the logic settles on the desired value in time to meet the timing deadline. In our example, we are not considering clocked logic.\n\nIn our case, an additional term of <math>A\\overline{D}</math> would eliminate the potential race hazard, bridging between the green and blue output states or blue and red output states: this is shown as the yellow region (which wraps around from the bottom to the top of the right half) in the adjacent diagram.\n\nThe term is [[logic redundancy|redundant]] in terms of the static logic of the system, but such redundant, or [[consensus theorem|consensus terms]], are often needed to assure race-free dynamic performance.\n\nSimilarly, an additional term of <math>\\overline{A}D</math> must be added to the inverse to eliminate another potential race hazard. Applying De Morgan's laws creates another product of sums expression for ''f'', but with a new factor of <math>\\left(A + \\overline{D}\\right)</math>.\n\n===2-variable map examples===\nThe following are all the possible 2-variable, 2&nbsp;×&nbsp;2 Karnaugh maps. Listed with each is the minterms as a function of <math>\\sum m()</math> and the race hazard free (''see [[#Race hazards|previous section]]'') minimum equation. A minterm is defined as an expression that gives the most minimal form of expression of the mapped variables. All possible horizontal and vertical interconnected blocks can be formed. These blocks must be of the size of the powers of 2 (1, 2, 4, 8, 16, 32, ...). These expressions create a minimal logical mapping of the minimal logic variable expressions for the binary expressions to be mapped. Here are all the blocks with one field.\n\nA block can be continued across the bottom, top, left, or right of the chart. That can even wrap beyond the edge of the chart for variable minimization. This is because each logic variable corresponds to each vertical column and horizontal row. A visualization of the k-map can be considered cylindrical. The fields at edges on the left and right are adjacent, and the top and bottom are adjacent. K-Maps for four variables must be depicted as a donut or torus shape. The four corners of the square drawn by the k-map are adjacent. Still more complex maps are needed for 5 variables and more. \n<gallery perrow=\"5\">\nFile:K-map 2x2 none.svg | &Sigma;''m''(0); ''K'' = 0\nFile:K-map 2x2 1.svg | &Sigma;''m''(1); ''K'' = ''A''′''B''′\nFile:K-map 2x2 2.svg | &Sigma;''m''(2); ''K'' = ''AB''′\nFile:K-map 2x2 3.svg | &Sigma;''m''(3); ''K'' = ''A''′''B''\nFile:K-map 2x2 4.svg | &Sigma;''m''(4); ''K'' = ''AB''\nFile:K-map 2x2 1,2.svg | &Sigma;''m''(1,2); ''K'' = ''B''′\nFile:K-map 2x2 1,3.svg | &Sigma;''m''(1,3); ''K'' = ''A''′\nFile:K-map 2x2 1,4.svg | &Sigma;''m''(1,4); ''K'' = ''A''′''B''′ + ''AB''\nFile:K-map 2x2 2,3.svg | &Sigma;''m''(2,3); ''K'' = ''AB''′ + ''A''′''B''\nFile:K-map 2x2 2,4.svg | &Sigma;''m''(2,4); ''K'' = ''A''\nFile:K-map 2x2 3,4.svg | &Sigma;''m''(3,4); ''K'' = ''B''\nFile:K-map 2x2 1,2,3.svg | &Sigma;''m''(1,2,3); ''K'' = ''A''' + ''B''′\nFile:K-map 2x2 1,2,4.svg | &Sigma;''m''(1,2,4); ''K'' = ''A'' + ''B''′\nFile:K-map 2x2 1,3,4.svg | &Sigma;''m''(1,3,4); ''K'' = ''A''′ + ''B''\nFile:K-map 2x2 2,3,4.svg | &Sigma;''m''(2,3,4); ''K'' = ''A'' + ''B''\nFile:K-map 2x2 1,2,3,4.svg | &Sigma;''m''(1,2,3,4); ''K'' = 1\n</gallery>\n\n=={{anchor|Marquand|Harvard|Veitch|Svoboda|Händler|Kortum}}Other graphical methods==\nAlternative graphical minimization methods include:\n* '''Marquand diagram''' (1881) by [[Allan Marquand]] (1853–1924)<ref name=\"Marquand_1881\"/><ref name=\"Brown_2012\"/><!-- a precursor to Karnaugh maps, needs to be covered explicitly inhere or in a separate article - until then parked here for completeness -->\n* ''[[Harvard minimizing chart]]'' (1951) by [[Howard H. Aiken]] and Martha L. Whitehouse of the [[Harvard Computation Laboratory]]<ref name=\"Aiken_1952\"/><ref name=\"Karnaugh_1953\"/><ref name=\"Phister_1959\"/><ref name=\"Curtis_1962\"/>\n* ''[[Veitch chart]]'' (1952) by [[Edward Veitch]] (1924–2013)<ref name=\"Veitch_1952\"/><ref name=\"Brown_2012\"/><!-- a precursor to Karnaugh maps, needs to be covered explicitly inhere or in a separate article -  until then parked here for completeness -->\n* Svoboda's graphical aids (1956) and ''[[triadic map]]'' by [[Antonín Svoboda (computer scientist)|Antonín Svoboda]] (1907–1980)<ref name=\"Svoboda_1956_1\"/><ref name=\"Svoboda_1956_2\"/><ref name=\"Steinbuch-Weber_1974\"/><ref name=\"Svoboda_1979\"/>\n* ''[[Händler circle graph]]'' (aka <!-- Händler-circle graph, Handler-circle graph, -->{{lang|de|Händler'scher Kreisgraph}}, {{lang|de|Kreisgraph nach Händler}}, {{lang|de|Händler-Kreisgraph}}, {{lang|de|Händler-Diagramm}}, ''{{sic|{{lang|de|Minimisierungsgraph}}|expected={{lang|de|Minimierungsgraph}}}}'') (1958) by [[Wolfgang Händler]] (1920–1998)<ref name=\"Händler_1958\"/><ref name=\"Colloquium_1960\"/><ref name=\"Steinbuch-Wagner_1967\"/><ref name=\"Steinbuch-Weber_1974\"/><ref name=\"Hotz_1974\"/><ref name=\"ISER_1\"/><ref name=\"ISER_2\"/><ref name=\"Broy_1990\"/><ref name=\"Bauer-Wirsing_1991\"/>\n* Graph method (1965) by {{ill|Herbert Kortum|de}} (1907–1979)<ref name=\"Kortum_1964_12\"/><ref name=\"Kortum_1965_1\"/><ref name=\"Kortum_1965_3\"/><ref name=\"Kortum_1965_5\"/><ref name=\"Kortum_1967_6\"/><ref name=\"Kortum_1966_12\"/><ref name=\"Tafel_1971\"/>\n\n==See also==\n* [[Circuit minimization]]\n* [[Espresso heuristic logic minimizer]]\n* [[List of Boolean algebra topics]]\n* [[Quine–McCluskey algorithm]]\n* [[Algebraic normal form]] (ANF)\n* [[Ring sum normal form]] (RSNF)\n* [[Zhegalkin normal form]]\n* [[Reed–Muller expansion]]\n* [[Venn diagram]]\n* [[Punnett square]] (a similar diagram in biology)\n\n==References==\n{{reflist|refs=\n<ref name=\"Aiken_1952\">{{cite book |title=Synthesis of electronic computing and control circuits |orig-year=January 1951 |date=1952 |edition=second printing, revised |chapter=Chapter V: Minimizing charts |pages=preface, 50–67 |author-first1=Howard H. |author-last1=Aiken |author-link1=Howard H. Aiken |author-first2=Gerrit |author-last2=Blaauw |author-link2=Gerrit Blaauw |author-first3=William |author-last3=Burkhart |author-first4=Robert J. |author-last4=Burns |author-first5=Lloyd |author-last5=Cali |author-first6=Michele |author-last6=Canepa |author-first7=Carmela M. |author-last7=Ciampa |author-first8=Charles A. |author-last8=Coolidge, Jr. |author-first9=Joseph R. |author-last9=Fucarile |author-first10=J. Orten |author-last10=Gadd, Jr. |author-first11=Frank F. |author-last11=Gucker |author-first12=John A. |author-last12=Harr |author-first13=Robert L. |author-last13=Hawkins |author-first14=Miles V. |author-last14=Hayes |author-first15=Richard |author-last15=Hofheimer |author-first16=William F. |author-last16=Hulme |author-first17=Betty L. |author-last17=Jennings |author-first18=Stanley A. |author-last18=Johnson |author-first19=Theodore |author-last19=Kalin |author-first20=Marshall |author-last20=Kincaid |author-first21=E. Edward |author-last21=Lucchini |author-first22=William |author-last22=Minty |author-first23=Benjamin L. |author-last23=Moore |author-first24=Joseph |author-last24=Remmes |author-first25=Robert J. |author-last25=Rinn |author-first26=John W. |author-last26=Roche |author-first27=Jacquelin |author-last27=Sanbord |author-first28=Warren L. |author-last28=Semon |author-first29=Theodore |author-last29=Singer |author-first30=Dexter |author-last30=Smith |author-first31=Leonard |author-last31=Smith |author-first32=Peter F. |author-last32=Strong |author-first33=Helene V. |author-last33=Thomas |author-first34=An |author-last34=Wang |author-link34=An Wang |author-first35=Martha L. |author-last35=Whitehouse |author-first36=Holly B. |author-last36=Wilkins |author-first37=Robert E. |author-last37=Wilkins |author-first38=Way Dong |author-last38=Woo |author-first39=Elbert P. |author-last39=Little |author-first40=M. Scudder |author-last40=McDowell |location=Write-Patterson Air Force Base |publisher=[[Harvard University Press]] (Cambridge, Massachusetts, USA) / Geoffrey Cumberlege Oxford University Press (London) |url=https://archive.org/stream/in.ernet.dli.2015.509288/2015.509288.Synthesis-Of#page/n6/mode/1up |access-date=2017-04-16 |quote=[…] Martha Whitehouse constructed the minimizing charts used so profusely throughout this book, and in addition  prepared minimizing charts of seven and eight variables for experimental purposes. […] Hence, the present writer is obliged to record that the general algebraic approach, the switching function, the vacuum-tube operator, and the minimizing chart are his proposals, and that he is responsible for their inclusion herein. […]}} (NB. Work commenced in April 1948.)</ref>\n<ref name=\"Phister_1959\">{{cite book |title=Logical design of digital computers |author-first=Montgomery |author-last=Phister, Jr. |publisher=[[John Wiley & Sons Inc.]] |date=1959 |orig-year=December 1958 |location=New York, USA |isbn=0471688053<!-- |id=ISBN978-0471688051? --> |pages=75–83 |url=https://archive.org/stream/in.ernet.dli.2015.74854/2015.74854.Logical-Design-Of-Digital-Computers#page/n0/mode/1up}}</ref>\n<ref name=\"Curtis_1962\">{{cite book |title=A new approach to the design of switching circuits |author-first=H. Allen |author-last=Curtis |publisher=[[D. van Nostrand Company, Inc.]] |date=1962 |location=Princeton, New Jersey, USA |series=Bell Laboratories Series}}</ref>\n<ref name=\"Karnaugh_1953\">{{cite journal |author-last=Karnaugh |author-first=Maurice |author-link=Maurice Karnaugh |title=The Map Method for Synthesis of Combinational Logic Circuits |journal=[[Transactions of the American Institute of Electrical Engineers]] part I |volume=72 |issue=9 |pages=593–599 |date=November 1953 |orig-year=1953-04-23<!-- available for printing -->, 1953-03-17<!-- sent in --> |id=Paper 53-217 |doi=10.1109/TCE.1953.6371932 |url=http://philectrosophy.com/documents/The%20Map%20Method%20For%20Synthesis%20of.pdf |access-date=2017-04-16 |dead-url=no |archive-url=https://web.archive.org/web/20170416232229/http://philectrosophy.com/documents/The%20Map%20Method%20For%20Synthesis%20of.pdf |archive-date=2017-04-16}} (NB. Also contains a short review by [[Samuel H. Caldwell]].)</ref>\n<ref name=\"Brown_2012\">{{cite book |title=Boolean Reasoning - The Logic of Boolean Equations |author-first=Frank Markham |author-last=Brown |edition=<!-- 2012 -->reissue of 2nd |publisher=[[Dover Publications, Inc.]] |location=Mineola, New York |date=2012 |orig-year=2003, 1990 |isbn=978-0-486-42785-0}} [<!-- 1st edition -->http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf<!-- https://web.archive.org/web/20170416231752/http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf -->]</ref>\n<ref name=\"Wakerly_1994\">{{cite book |title=Digital Design: Principles & Practices |author-last=Wakerly |author-first=John F. |year=1994 |publisher=[[Prentice Hall]] |location=New Jersey, USA |isbn=0-13-211459-3 |pages=222, 48–49}} (NB. The two page sections taken together say that K-maps are labeled with [[Gray code]]. The first section says that they are labeled with a code that changes only one bit between entries and the second section says that such a code is called Gray code.)</ref>\n<ref name=\"Belton_1998\">{{cite web |author-first=David |author-last=Belton |date=April 1998 |url=http://www.ee.surrey.ac.uk/Projects/Labview/minimisation/karrules.html |title=Karnaugh Maps – Rules of Simplification |access-date=2009-05-30 |dead-url=no |archive-url=https://web.archive.org/web/20170418140519/http://www.ee.surrey.ac.uk/Projects/Labview/minimisation/karrules.html |archive-date=2017-04-18}}</ref>\n<ref name=\"Dodge_2016\">{{cite web |title=Simplifying Logic Circuits with Karnaugh Maps |author-first=Nathan B. |author-last=Dodge |date=September 2015 |publisher=[[The University of Texas at Dallas]], [[Erik Jonsson School of Engineering and Computer Science]] |url=http://www.utdallas.edu/~dodge/EE2310/lec5.pdf |access-date=2017-04-18 |dead-url=no |archive-url=https://web.archive.org/web/20170418140824/https://www.utdallas.edu/~dodge/EE2310/lec5.pdf |archive-date=2017-04-18}}</ref>\n<ref name=\"Cook_2012\">{{cite web |author-last=Cook |author-first=Aaron |title=Using Karnaugh Maps to Simplify Code |publisher=Quantum Rarity |url=http://www.quantumrarity.com/archives/255 |access-date=2012-10-07 |dead-url=no |archive-url=https://web.archive.org/web/20170418141624/http://www.quantumrarity.com/archives/255 |archive-date=2017-04-18}}</ref>\n<ref name=\"Marquand_1881\">{{cite journal |title=XXXIII: On Logical Diagrams for ''n'' terms |author-first=Allan |author-last=Marquand |author-link=Allan Marquand |journal=[[The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science]] |issue=75 |series=5 |date=1881 |volume=12 |doi=10.1080/14786448108627104 |pages=266–270 |url=http://www.tandfonline.com/doi/abs/10.1080/14786448108627104 |access-date=2017-05-15}} (NB. Quite many secondary sources erroneously cite this work as \"A logical diagram for ''n'' terms\" or \"On a logical diagram for ''n'' terms\".)</ref>\n<ref name=\"Veitch_1952\">{{cite journal |author-last=Veitch |author-first=Edward W. |author-link=Edward Veitch |title=A Chart Method for Simplifying Truth Functions |journal=ACM Annual Conference/Annual Meeting: Proceedings of the 1952 ACM Annual Meeting (Pittsburg) |publisher=[[Association for Computing Machinery|ACM]] |location=New York, USA |pages=127–133 |date=1952-05-03 |orig-year=1952-05-02 |doi=10.1145/609784.609801}}</ref>\n<ref name=\"Svoboda_1956_1\">{{cite book |title=Graficko-mechanické pomůcky užívané při analyse a synthese kontaktových obvodů |trans-title=Utilization of graphical-mechanical aids for the analysis and synthesis of contact circuits |journal=Stroje na zpracování informací [Symphosium IV on information processing machines] |author-first=Antonín |author-last=Svoboda |author-link=Antonín Svoboda (computer scientist) |location=Prague |publisher=Czechoslovak Academy of Sciences, Research Institute of Mathematical Machines |language=Czech |date=1956 |volume=IV |pages=9–21}}</ref>\n<ref name=\"Svoboda_1956_2\">{{cite book |title=Graphical Mechanical Aids for the Synthesis of Relay Circuits |journal=Nachrichtentechnische Fachberichte (NTF), Beihefte der Nachrichtentechnischen Zeitschrift (NTZ) |publisher=[[Vieweg-Verlag]] |location=Braunschweig, Germany |author-first=Antonín |author-last=Svoboda |author-link=Antonín Svoboda (computer scientist) |date=1956}}</ref>\n<ref name=\"Svoboda_1979\">{{cite book |title=Advanced Logical Circuit Design Techniques |author-first1=Antonín |author-last1=Svoboda |author-link1=Antonín Svoboda (computer scientist) |author-first2=Donnamaie E. |author-last2=White |date=2016 |orig-year=1979-08-01 |edition=retyped electronic reissue |publisher=[[Garland STPM Press]] (original issue) / WhitePubs (reissue) |isbn=978-0-8240-7014-4<!-- 1990 1st issue --> |url=http://www.donnamaie.com/Advanced_logic_ckt/Advanced_Logical_Circuit_Design_Techniques%20sm.pdf |access-date=2017-04-15 |dead-url=no |archive-url=https://web.archive.org/web/20170414163013/http://www.donnamaie.com/Advanced_logic_ckt/Advanced_Logical_Circuit_Design_Techniques%20sm.pdf |archive-date=2017-04-14}} [http://www.donnamaie.com/<!-- https://web.archive.org/web/20170415220158/http://www.donnamaie.com/ -->] [https://books.google.com/books?id=g3uzAAAAIAAJ]</ref>\n<ref name=\"Händler_1958\">{{cite book |title=Ein Minimisierungsverfahren zur Synthese von Schaltkreisen (Minimisierungsgraphen) |language=German |author-first=Wolfgang |author-last=Händler |author-link=Wolfgang Händler |publisher=[[Technische Hochschule Darmstadt]] |date=1958 |id=D&nbsp;17 |type=Dissertation |url=https://books.google.com/books?id=D58TAQAAIAAJ}} [https://www.tib.eu/de/suchen/id/TIBKAT%3A044782241/Ein-Minimisierungsverfahren-zur-Synthese-von-Schaltkreisen/] (NB. Although written by a German, the title contains an [[anglicism]]; the correct German term would be \"Minimierung\" instead of \"Minimisierung\".)</ref>\n<ref name=\"Hotz_1974\">{{cite book |title=Schaltkreistheorie |language=German |trans-title=Switching circuit theory |author-first=Günter |author-last=Hotz |publisher=[[Walter de Gruyter & Co.]] |series=DeGruyter Lehrbuch |date=1974 |isbn=3-11-00-2050-5 |page=117 |quote=[…] Der Kreisgraph von ''[[Wolfgang Händler|Händler]]'' ist für das Auffinden von [[prime implicant|Primimplikanten]] gut brauchbar. Er hat den Nachteil, daß er schwierig zu zeichnen ist. Diesen Nachteil kann man allerdings durch die Verwendung von Schablonen verringern. […] [The circle graph by ''Händler'' is well suited to find [[prime implicant]]s. A disadvantage is that it is difficult to draw. This can be remedied using stencils.]}}</ref>\n<ref name=\"ISER_1\">{{cite web |title=Informatik Sammlung Erlangen (ISER) |date=2012-03-13 |publisher=[[Friedrich-Alexander Universität]] |location=Erlangen, Germany |language=German |url=https://www.rrze.fau.de/wir-ueber-uns/kooperationen/iser.shtml |access-date=2017-04-12 |dead-url=yes |archive-url=https://web.archive.org/web/20170516154655/https://www.rrze.fau.de/wir-ueber-uns/kooperationen/iser.shtml |archive-date=2017-05-16}} (NB. Shows a picture of a {{lang|de|Kreisgraph}} by ''[[Wolfgang Händler|Händler]]''.)</ref>\n<ref name=\"ISER_2\">{{cite web |title=Informatik Sammlung Erlangen (ISER) - Impressum |date=2012-03-13 |publisher=[[Friedrich-Alexander Universität]] |location=Erlangen, Germany |language=German |url=http://www.iser.uni-erlangen.de:80/index.php?ort_id=327&tree=0 |access-date=2017-04-15 |dead-url=no |archive-url=https://web.archive.org/web/20120226004316/http://www.iser.uni-erlangen.de/index.php?ort_id=327&tree=0 |archive-date=2012-02-26}} (NB. Shows a picture of a {{lang|de|Kreisgraph}} by ''[[Wolfgang Händler|Händler]]''.)</ref>\n<ref name=\"Broy_1990\">{{cite book |title=Informatik und Mathematik |language=German |trans-title=Computer Sciences and Mathematics |chapter=Geschichte der Schaltalgebra |trans-chapter=History of circuit switching algebra |author-first=Heinz |author-last=Zemanek |author-link=Heinz Zemanek |editor-first=Manfred |editor-last=Broy |editor-link=Manfred Broy |orig-year=1990 |date=2013 |publisher=[[Springer-Verlag]] |isbn=9783642766770 |pages=43–72 |url=https://books.google.com/books?id=y5GfBgAAQBAJ |quote=Einen Weg besonderer Art, der damals zu wenig beachtet wurde, wies [[Wolfgang Händler|W. Händler]] in seiner Dissertation […] mit einem Kreisdiagramm. […]}} [https://link.springer.com/chapter/10.1007%2F978-3-642-76677-0_3] (NB. Collection of papers at a colloquium held at the [[Bayerische Akademie der Wissenschaften]],\n1989-06-12/14, in honor of [[Friedrich L. Bauer]].)</ref>\n<ref name=\"Bauer-Wirsing_1991\">{{cite book |author-first1=Friedrich Ludwig |author-last1=Bauer |author-link1=Friedrich Ludwig Bauer |author-first2=Martin |author-last2=Wirsing |author-link2=Martin Wirsing |title=Elementare Aussagenlogik |publisher=[[Springer-Verlag]] |language=German |location=Berlin / Heidelberg |date=March 1991 |isbn=978-3-540-52974-3 |pages=54–56, 71, 112–113, 138–139 |url=https://books.google.com/books?id=Ff58BwAAQBAJ |quote=[…] handelt es sich um ein [[Wolfgang Händler|Händler]]-Diagramm […], mit den Würfelecken als Ecken eines 2<sup>m</sup>-gons. […] Abb. […] zeigt auch Gegenstücke für andere Dimensionen. Durch waagerechte Linien sind dabei Tupel verbunden, die sich nur in der ersten Komponente unterscheiden; durch senkrechte Linien solche, die sich nur in der zweiten Komponente unterscheiden; durch 45°-Linien und 135°-Linien solche, die sich nur in der dritten Komponente unterscheiden usw. Als Nachteil der Händler-Diagramme wird angeführt, daß sie viel Platz beanspruchen. […]}}</ref>\n<ref name=\"Colloquium_1960\">{{cite book |editor-first1=Ernst Ferdinand |editor-last1=Peschl |editor-link1=Ernst Ferdinand Peschl |editor-first2=Heinz |editor-last2=Unger |editor-link2=:de:Heinz Unger (Mathematiker) |title=Colloquium über Schaltkreis- und Schaltwerk-Theorie - Vortragsauszüge vom 26. bis 28. Oktober 1960 in Bonn - Band 3 von Internationale Schriftenreihe zur Numerischen Mathematik [International Series of Numerical Mathematics] (ISNM) |language=German |volume=3 |chapter=Zum Gebrauch von Graphen in der Schaltkreis- und Schaltwerktheorie |author-first=Wolfgang |author-last=Händler |author-link=Wolfgang Händler |location=Institut für Angewandte Mathematik, [[Universität Saarbrücken]], Rheinisch-Westfälisches Institut für Instrumentelle Mathematik |publisher=[[Springer Basel AG]] / [[Birkhäuser Verlag Basel]] |date=2013 |orig-year=1961 |isbn=978-3-0348-5771-0 |doi=10.1007/978-3-0348-5770-3 |url=https://books.google.com/books?id=myTnoAEACAAJ |pages=169–198}} [https://link.springer.com/chapter/10.1007%2F978-3-0348-5770-3_10]</ref>\n<ref name=\"Kortum_1964_12\">{{cite journal |title=Minimierung von Kontaktschaltungen durch Kombination von Kürzungsverfahren und Graphenmethoden |trans-title=Minimization of contact circuits by combination of reduction procedures and graphical methods |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1965 |volume=8 |issue=12 |pages=421–425 |issn=0026-0347 |id={{CODEN|MSRGAN}} |url=https://www.tib.eu/en/search/id/ceaba%3ACEAB1966001170/MINIMIERUNG-VON-KONTAKTSCHALTUNGEN-DURCH-KOMBINATION/}} [https://www.tib.eu/en/search/id/ei-backfile%3Ac84_64eb63f914c231eeM6f1319817173212/Minimization-of-contact-circuits-by-combination/]</ref>\n<ref name=\"Kortum_1965_1\">{{cite journal |title=Konstruktion und Minimierung von Halbleiterschaltnetzwerken mittels Graphentransformation |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1966 |volume=9 |issue=1 |pages=9–12 |issn=0026-0347 |id={{CODEN|MSRGAN}} |url=https://www.tib.eu/en/search/id/ceaba%3ACEAB1966002519/KONSTRUKTION-UND-MINIMIERUNG-VON-HALBLEITER-SCHALTNETZWERKEN/}}</ref>\n<ref name=\"Kortum_1965_3\">{{cite journal |title=Weitere Bemerkungen zur Minimierung von Schaltnetzwerken mittels Graphenmethoden |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1966 |volume=9 |issue=3 |pages=96–102 |issn=0026-0347 |id={{CODEN|MSRGAN}} |url=https://www.tib.eu/en/search/id/ceaba%3ACEAB1966002896/WEITERE-BEMERKUNGEN-ZUR-MINIMIERUNG-VON-SCHALTNETZWERKEN/}}</ref>\n<ref name=\"Kortum_1965_5\">{{cite journal |title=Weitere Bemerkungen zur Behandlung von Schaltnetzwerken mittels Graphen. Konstruktion von vermaschten Netzwerken (Brückenschaltungen) |trans-title=Further remarks on treatment of switching networks by means of graphs |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1966 |volume=9 |issue=5 |pages=151–157 |issn=0026-0347 |id={{CODEN|MSRGAN}}}} {{cite journal |title=Weitere Bemerkungen zur Behandlung von Schaltnetzwerken mittels Graphen |trans-title=Further remarks on treatment of switching networks by means of graphs |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=Regelungstechnik |language=German |publisher= |date=1965 |volume=10 |issue=5 |pages=33–39 |location=10. Internationales Wissenschaftliches Kolloquium, Ilmenau. Technische Hochschule |url=https://www.tib.eu/en/search/id/ei-backfile%3Ac84_a3574af8cb8d6293M72eb19817173212/Further-remarks-on-treatment-of-switching-networks/}}</ref>\n<ref name=\"Kortum_1966_12\">{{cite journal |title=Zur Minimierung von Schaltsystemen |trans-title=Minimization of switching circuits |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=Wissenschaftliche Zeitschrift der TU Ilmenau |location=Jena |language=German |publisher=Technische Hochschule für Elektrotechnik Ilmenau / Forschungsstelle für Meßtechnik und Automatisierung der Deutschen Akademie der Wissenschaften |date=1966<!-- one source states: 1965 --> |volume=12 |issue=2<!-- one sources states: 2, 3. --> |pages=181–186 |url=https://www.tib.eu/en/search/id/ei-backfile%3Ac84_125e37df8f0a3cd4eM7a8919817173212/Minimization-of-switching-circuits/}}</ref>\n<ref name=\"Kortum_1967_6\">{{cite journal |title=Über zweckmäßige Anpassung der Graphenstruktur diskreter Systeme an vorgegebene Aufgabenstellungen |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1967 |volume=10 |issue=6 |pages=208–211 |issn=0026-0347 |id={{CODEN|MSRGAN}}}}</ref>\n<ref name=\"Tafel_1971\">{{cite book |title=Einführung in die digitale Datenverarbeitung |language=German |trans-title=Introduction to digital information processing |chapter=4.3.5. Graphenmethode zur Vereinfachung von Schaltfunktionen |author-first=Hans Jörg |author-last=Tafel |publisher=[[Carl Hanser Verlag]] |date=1971 |location=[[RWTH]], Aachen, Germany |publication-place=Munich, Germany |isbn=3-446-10569-7 |pages=98–105, 107–113}}</ref>\n<ref name=\"Steinbuch-Wagner_1967\">{{cite book |title=Taschenbuch der Nachrichtenverarbeitung |language=German |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Siegfried W. |editor-last2=Wagner |author-first1=Erich R. |author-last1=Berger |author-first2=Wolfgang |author-last2=Händler |author-link2=Wolfgang Händler |date=1967 |orig-year=1962 |edition=2 |publisher=[[Springer-Verlag OHG]] |location=Berlin, Germany |id=Title No. 1036 |lccn=67-21079 |pages=64, 1034–1035, 1036, 1038 |quote=[…] Übersichtlich ist die Darstellung nach ''[[Wolfgang Händler|Händler]]'', die sämtliche Punkte, numeriert nach dem ''[[Gray-Code]]'' […], auf dem Umfeld eines Kreises anordnet. Sie erfordert allerdings sehr viel Platz. […] [''Händler's'' illustration, where all points, numbered according to the ''[[Gray code]]'', are arranged on the circumference of a circle, is easily comprehensible. It needs, however, a lot of space.]}}</ref>\n<ref name=\"Steinbuch-Weber_1974\">{{cite book |title=Taschenbuch der Informatik - Band II - Struktur und Programmierung von EDV-Systemen |language=German |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Wolfgang |editor-last2=Weber |editor-first3=Traute |editor-last3=Heinemann |date=1974 |orig-year=1967 |edition=3 |volume=2 |work=Taschenbuch der Nachrichtenverarbeitung |publisher=[[Springer-Verlag]] |location=Berlin, Germany |isbn=3-540-06241-6 |lccn=73-80607 |pages=25, 62, 96, 122–123, 238}}</ref>\n<ref name=\"Wolfram_2002\">{{cite book |author-last=Wolfram |author-first=Stephen |author-link=Stephen Wolfram |title=A New Kind of Science |publisher=Wolfram Media, Inc. |date=2002 |page=1097 |isbn=1-57955-008-8}}</ref>\n}}\n\n==Further reading==\n* {{cite book |author-last=Katz |author-first=Randy Howard |author-link=Randy Howard Katz |title=Contemporary Logic Design |orig-year=1994 |publisher=[[The Benjamin/Cummings Publishing Company]] |isbn=0-8053-2703-7 |doi=10.1016/0026-2692(95)90052-7 |pages=70–85 |year=1998}}\n* {{cite book |author-last=Vingron |author-first=Shimon Peter |title=Switching Theory: Insight Through Predicate Logic |orig-year=2003-11-05 |publisher=[[Springer-Verlag]] |location=Berlin, Heidelberg, New York |isbn=3-540-40343-4 |pages=57–76 |chapter=Karnaugh Maps |year=2004}}\n* {{cite book |author-last=Wickes |author-first=William E. |title=Logic Design with Integrated Circuits |year=1968 |publisher=[[John Wiley & Sons]] |location=New York, USA |lccn=68-21185 |pages=36–49 |quote=A refinement of the [[Venn diagram]] in that circles are replaced by squares and arranged in a form of matrix. The Veitch diagram labels the squares with the [[minterm]]s. [[Maurice Karnaugh|Karnaugh]] assigned 1s and 0s to the squares and their labels and deduced the numbering scheme in common use.}}\n* {{cite web |title=Reed-Muller Logic |work=Logic 101 |at=Part 3 |author-first=Clive \"Max\" |author-last=Maxfield |date=2006-11-29 |publisher=[[EETimes]] |url=http://www.eetimes.com/author.asp?section_id=216&doc_id=1274545 |access-date=2017-04-19 |dead-url=no |archive-url=https://web.archive.org/web/20170419235904/http://www.eetimes.com/author.asp?section_id=216&doc_id=1274545 |archive-date=2017-04-19}}\n* {{Cite book |last1=Lind |first1=L.F. |title=Analysis and Design of Sequential Digital Systems |date=1977 |publisher=Macmillan Press |isbn=0333192664 |url=https://archive.org/details/AnalysisDesignOfSequentialDigitalSystems/}}; 146 pages. <small>(Section 2.3)</small>\n\n== External links ==\n{{sisterlinks}}\n* [http://gandraxa.com/detect_overlapping_subrectangles.xml Detect Overlapping Rectangles], by Herbert Glarner.\n* [http://www.sccs.swarthmore.edu/users/06/adem/engin/e15/lab1/ Using Karnaugh maps in practical applications], Circuit design project to control traffic lights.\n* [http://fullchipdesign.com/kmap2v.htm K-Map Tutorial for 2,3,4 and 5 variables ]\n* [http://www.generalnumbers.com/karnaugh_application1.html Karnaugh Map Example]\n* [http://iris.elf.stuba.sk/JEEEC/data/pdf/07-08_105-08.pdf POCKET–PC BOOLEAN FUNCTION SIMPLIFICATION, Ledion Bitincka — George E. Antoniou]\n\n[[Category:Boolean algebra]]\n[[Category:Diagrams]]\n[[Category:Electronics optimization]]\n[[Category:Logic in computer science]]"
    },
    {
      "title": "Logic alphabet",
      "url": "https://en.wikipedia.org/wiki/Logic_alphabet",
      "text": "The '''logic alphabet''', also called the X-stem Logic Alphabet (XLA), constitutes an iconic set of [[Symbol (formal)|symbol]]s that systematically represents the sixteen possible binary [[truth function]]s of [[logic]]. The logic alphabet was developed by [[Shea Zellweger]]. The major emphasis of his iconic \"logic alphabet\" is to provide a more cognitively ergonomic notation for logic. Zellweger's visually iconic system more readily reveals, to the novice and expert alike, the underlying [[symmetry]] relationships and [[geometric]] properties of the sixteen binary connectives within [[Boolean algebra (logic)|Boolean algebra]].\n\n==Truth functions==\n[[Truth function]]s are functions from [[sequence]]s of [[truth value]]s to truth values. A [[unary function|unary]] truth function, for example, takes a single truth value and maps it onto another truth value. Similarly, a [[binary function|binary]] truth function maps [[ordered pair]]s of truth values onto truth values, while a [[arity|ternary]] truth function maps ordered triples of truth values onto truth values, and so on.\n\nIn the unary case, there are two possible inputs, viz. '''T''' and '''F''', and thus four possible unary truth functions: one mapping '''T''' to '''T''' and '''F''' to '''F''', one mapping '''T''' to '''F''' and '''F''' to '''F''', one mapping '''T''' to '''T''' and '''F''' to '''T''', and finally one mapping '''T''' to '''F''' and '''F''' to '''T''', this last one corresponding to the familiar operation of [[logical negation]]. In the form of a table, the four unary truth functions may be represented as follows.\n\n{| border=\"1\" class=\"wikitable\" style=\"text-align:center;\"\n|+ Unary truth functions\n! style=\"width:40px;background:#aaaaaa;\" | p\n! style=\"width:40px\" | p\n! style=\"width:40px\" | F\n! style=\"width:40px\" | T\n! style=\"width:40px\" | ~p\n|-\n| T || T || F || T || F\n|-\n| F || F || F || T || T\n|}\n\nIn the binary case, there are four possible inputs, viz. ('''T''','''T'''), ('''T''','''F'''), ('''F''','''T'''), and ('''F''','''F'''), thus yielding sixteen possible binary truth functions. Quite generally, for any number ''n'', there are <math>2^{2^n}</math> possible ''n''-[[arity|ary]] truth functions. The sixteen possible binary truth functions are listed in the table below.\n\n{| border=\"1\" class=\"wikitable\" style=\"text-align:center;\"\n|+ Binary truth functions\n! style=\"width:35px;background:#aaaaaa;\" | p\n! style=\"width:35px;background:#aaaaaa;\" | q\n! style=\"width:35px\" | T\n! style=\"width:35px\" | NAND\n! style=\"width:35px\" | →\n! style=\"width:35px\" | NOT p\n! style=\"width:35px\" | ←\n! style=\"width:35px\" | NOT q\n! style=\"width:35px\" | ↔\n! style=\"width:35px\" | NOR\n! style=\"width:35px\" | OR\n! style=\"width:35px\" | XOR\n! style=\"width:35px\" | q\n! style=\"width:35px\" | NOT ←\n! style=\"width:35px\" | p\n! style=\"width:35px\" | NOT →\n! style=\"width:35px\" | AND\n! style=\"width:35px\" | F\n|-\n| T || T || T || F || T || F || T || F || T || F || T || F || T || F || T || F || T || F\n|-\n| T || F || T || T || F || F || T || T || F || F || T || T || F || F || T || T || F || F\n|-\n| F || T || T || T || T || T || F || F || F || F || T || T || T || T || F || F || F || F\n|-\n| F || F || T || T || T || T || T || T || T || T || F || F || F || F || F || F || F || F\n|}\n\n==Content==\n[[Shea Zellweger|Zellweger's]] logic alphabet offers a visually systematic way of representing each of the sixteen binary truth functions. The idea behind the logic alphabet is to first represent the sixteen binary truth functions in the form of a [[square matrix]] rather than the more familiar tabular format seen in the table above, and then to assign a [[letter (alphabet)|letter]] shape to each of these matrices. Letter shapes are derived from the distribution of '''T'''s in the matrix. When drawing a logic symbol, one passes through each square with assigned '''F''' values while stopping in a square with assigned '''T''' values. In the extreme examples, the symbol for [[tautology (logic)|tautology]] is a X (stops in all four squares), while the symbol for [[contradiction]] is an O (passing through all squares without stopping). The square matrix corresponding to each binary truth function, as well as its corresponding letter shape, are displayed in the table below.\n\n<!-- Deleted image removed: [[Image:Zellweger-LogicGarnet.jpg|thumb|200px]] -->\n{| border=\"1\" class=\"wikitable\" style=\"text-align:center;\"\n|+ Symbols\n! Conventional symbol\n! Matrix\n! Logic alphabet shape\n|-\n| T                           || [[Image:LAlphabet T table.jpg|70px]]      || [[Image:LAlphabet T.jpg|45px]]\n|-\n| [[Sheffer stroke|NAND]]     || [[Image:LAlphabet NAND table.jpg|70px]]   || [[Image:LAlphabet NAND.jpg|45px]]\n|-\n| [[Material conditional|→]]  || [[Image:LAlphabet IFTHEN table.jpg|70px]] || [[Image:LAlphabet IFTHEN.jpg|45px]]\n|-\n| NOT p                       || [[Image:LAlphabet NOTP table.jpg|70px]]   || [[Image:LAlphabet NOTP.jpg|45px]]\n|-\n| ←                           || [[Image:LAlphabet FI table.jpg|70px]]     || [[Image:LAlphabet FI.jpg|45px]]\n|-\n| NOT q                       || [[Image:LAlphabet NOTQ table.jpg|70px]]   || [[Image:LAlphabet NOTQ.jpg|45px]]\n|-\n| [[Biconditional|↔]]         || [[Image:LAlphabet IFF table.jpg|70px]]    || [[Image:LAlphabet IFF.jpg|45px]]\n|-\n| [[Logical NOR|NOR]]         || [[Image:LAlphabet NOR table.jpg|70px]]    || [[Image:LAlphabet NOR.jpg|45px]]\n|-\n| [[logical disjunction|OR]]  || [[Image:LAlphabet OR table.jpg|70px]]     || [[Image:LAlphabet OR.jpg|45px]]\n|-\n| [[XOR]]                     || [[Image:LAlphabet XOR table.jpg|70px]]    || [[Image:LAlphabet XOR.jpg|45px]]\n|-\n| q                           || [[Image:LAlphabet Q table.jpg|70px]]      || [[Image:LAlphabet Q.jpg|45px]]\n|-\n| NOT ←                       || [[Image:LAlphabet NFI table.jpg|70px]]    || [[Image:LAlphabet NFI.jpg|45px]]\n|-\n| p                           || [[Image:LAlphabet P table.jpg|70px]]      || [[Image:LAlphabet P.jpg|45px]]\n|-\n| NOT →                       || [[Image:LAlphabet NIF table.jpg|70px]]    || [[Image:LAlphabet NIF.jpg|45px]]\n|-\n| [[Logical conjunction|AND]] || [[Image:LAlphabet AND table.jpg|70px]]    || [[Image:LAlphabet AND.jpg|45px]]\n|-\n| F                           || [[Image:LAlphabet F table.jpg|70px]]      || [[Image:LAlphabet F.jpg|45px]]\n|}\n\n==Significance==\nThe interest of the logic alphabet lies in its [[aesthetic]], symmetric, and geometric qualities. These qualities combine to allow an individual to more easily, rapidly and visually manipulate the relationships between entire truth tables. A logic operation performed on a two dimensional logic alphabet connective, with its geometric qualities, produces a symmetry transformation.  When a symmetry transformation occurs, each input symbol, without any further thought, immediately changes into the correct output symbol. For example, by reflecting the symbol for [[Sheffer stroke|NAND]] (viz. 'h') across the vertical axis we produce the symbol for ←, whereas by reflecting it across the horizontal axis we produce the symbol for [[Material conditional|→]], and by reflecting it across both the horizontal and vertical axes we produce the symbol for [[logical disjunction|∨]]. Similar symmetry transformations can be obtained by operating upon the other symbols.\n\nIn effect, the X-stem Logic Alphabet is derived from three disciplines that have been stacked and combined: (1) mathematics, (2) logic, and (3) semiotics. This happens because, in keeping with the mathelogical semiotics, the connectives have been custom designed in the form of geometric letter shapes that serve as iconic replicas of their corresponding square-framed truth tables. Logic cannot do it alone.  Logic is sandwiched between mathematics and semiotics. Indeed, [[Shea Zellweger|Zellweger]] has constructed intriguing structures involving the symbols of the logic alphabet on the basis of these symmetries ([http://www.logic-alphabet.net/images/logicbug_2345_2.jpg] [http://www.logic-alphabet.net/images/clockcompass_2353_2.jpg]). The considerable aesthetic appeal of the logic alphabet has led to exhibitions of [[Shea Zellweger|Zellweger's]] work at the [[Museum of Jurassic Technology]] in [[Los Angeles]], among other places.\n\nThe value of the logic alphabet lies in its use as a visually simpler pedagogical tool than the traditional system for logic notation. The logic alphabet eases the introduction to the fundamentals of logic, especially for children, at much earlier stages of cognitive development. Because the logic notation system, in current use today, is so deeply embedded in our computer culture, the \"logic alphabets\" adoption and value by the field of [[logic]] itself, at this juncture, is questionable. Additionally, systems of [[natural deduction]], for example, generally require introduction and elimination rules for each connective, meaning that the use of all sixteen binary connectives would result in a highly complex [[Mathematical proof|proof]] system. Various subsets of the sixteen binary connectives (e.g., {∨,&,→,~}, {∨,~}, {&, ~}, {→,~}) are themselves [[functional completeness|functionally complete]] in that they suffice to define the remaining connectives. In fact, both [[Sheffer stroke|NAND]] and [[Logical NOR|NOR]] are [[sole sufficient operator]]s, meaning that the remaining connectives can all be defined solely in terms of either of them. Nonetheless, the logic alphabet’s 2-dimensional geometric letter shapes along with its group symmetry properties can help ease the learning curve for children and adult students alike, as they become familiar with the interrelations and operations on all 16 binary connectives.  Giving children and students this advantage is a decided gain.\n\n==See also==\n* [[Polish notation]]\n* [[Propositional logic]]\n* [[Boolean function]]\n* [[Boolean algebra (logic)]]\n* [[Logic gate]]\n\n==External links==\n* [http://www.logic-alphabet.net/ Page dedicated to Zellweger's logic alphabet]\n* Exhibition in a [[Museum of Jurassic Technology|small museum]]: [https://www.flickr.com/photos/43992178@N00/387339135/ Flickr photopage], including a discussion between Tilman Piesk and probably [[Shea Zellweger]]\n\n{{DEFAULTSORT:Logic Alphabet}}\n[[Category:Binary operations]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Logic optimization",
      "url": "https://en.wikipedia.org/wiki/Logic_optimization",
      "text": "{{other uses|Minimisation (disambiguation){{!}}Minimisation}}\n{{bots|deny=Citation bot}}\n{{Use dmy dates|date=May 2019|cs1-dates=y}}\n'''Logic optimization''', a part of [[logic synthesis]] in [[electronics]], is the process of finding an equivalent representation of the specified [[logic circuit]] under one or more specified constraints. Generally the circuit is constrained to minimum chip area meeting a prespecified delay.\n\n==Introduction==\nWith the advent of [[logic synthesis]], one of the biggest challenges faced by the [[electronic design automation]] (EDA) industry was to find the best [[netlist]] representation of the given design description. While [[two-level logic optimization]] had long existed in the form of the [[Quine–McCluskey algorithm]], later followed by the [[Espresso heuristic logic minimizer]], the rapidly improving chip densities, and the wide adoption of [[Hardware description language|HDLs]] for circuit description, formalized the logic optimization domain as it exists today.\n\nToday, logic optimization is divided into various categories:\n\n'''Based on circuit representation''' \n* Two-level logic optimization\n* Multi-level logic optimization\n\n'''Based on circuit characteristics'''\n* Sequential logic optimization\n* Combinational logic optimization\n\n'''Based on type of execution'''\n*Graphical optimization methods\n*Tabular optimization methods\n*Algebraic optimization methods\n\nWhile a [[two-level circuit representation]] of circuits strictly refers to the flattened view of the circuit in terms of SOPs ([[Canonical form (Boolean algebra)|sum-of-products]]) &mdash; which is more applicable to a [[Programmable logic array|PLA]] implementation of the design{{Clarify|date=February 2010}} &mdash; a [[multi-level representation]] is a more generic view of the circuit in terms of arbitrarily connected SOPs, POSs ([[Canonical form (Boolean algebra)|product-of-sums]]), factored form etc. Logic optimization algorithms generally work either on the structural (SOPs, factored form) or functional ([[Binary decision diagram|BDDs]], ADDs) representation of the circuit.{{Clarify|date=February 2010}}\n\n==Two-level versus multi-level representations==\nIf we have two functions ''F''<sub>1</sub> and ''F''<sub>2</sub>:\n\n: <math>F_1 = AB + AC + AD,\\,</math>\n\n: <math>F_2 = A'B + A'C + A'E.\\,</math>\n\nThe above 2-level representation takes six product terms and 24 transistors in CMOS Rep.{{Why|date=February 2010}}\n\nA functionally equivalent representation in multilevel can be:\n\n: ''P'' = ''B'' + ''C''.\n\n: ''F''<sub>1</sub> = ''AP'' + ''AD''.\n\n: ''F''<sub>2</sub> = ''A<nowiki>'</nowiki>P'' + ''A<nowiki>'</nowiki>E''.\n\nWhile the number of levels here is 3, the total number of product terms and literals reduce {{Quantify|date=February 2010}} because of the sharing of the term B + C.\n\nSimilarly, we distinguish between [[Sequential logic|sequential]] and [[Combinational logic|combinational circuits]], whose behavior can be described in terms of [[finite-state machine]] state tables/diagrams or by Boolean functions and relations respectively.{{Clarify|date=February 2010}}\n\n==Circuit minimization in Boolean algebra==\nIn [[Boolean algebra (logic)|Boolean algebra]], '''circuit minimization''' is the problem of obtaining the smallest [[logic circuit]] (Boolean formula) that represents a given [[Boolean function]] or [[truth table]].  For the case when the boolean function is specified by a circuit (that is, we want to find an equivalent circuit of minimum size possible), the unbounded circuit minimization problem was long-conjectured to be [[polynomial hierarchy|<math>\\Sigma_2^P</math>-complete]], a result finally proved in 2008,<ref name=\"Buchfuhrer_2011\"/> but there are effective heuristics such as [[Karnaugh map]]s and the [[Quine–McCluskey algorithm]] that facilitate the process.\n\n===Purpose===\nThe problem with having a complicated [[Electronic circuit|circuit]] (i.e. one with many elements, such as [[logic gate]]s) is that each element takes up physical space in its implementation and costs time and money to produce in itself. Circuit minimization may be one form of logic optimization used to reduce the area of complex logic in [[integrated circuit]]s.\n\n===Example===\nWhile there are many ways to minimize a circuit, this is an example that minimizes (or simplifies) a boolean function. Note that the boolean function carried out by the circuit is directly related to the algebraic expression from which the function is implemented.<ref name=\"Mano_2014\"/>\nConsider the circuit used to represent <math>(A \\wedge \\bar{B}) \\vee (\\bar{A} \\wedge B)</math>. It is evident that two negations, two conjunctions, and a disjunction are used in this statement. This means that to build the circuit one would need two [[Inverter (logic gate)|inverters]], two [[AND gate]]s, and an [[OR gate]].\n\nWe can simplify (minimize) the circuit by applying logical identities or using intuition. Since the example states that A is true when B is false or the other way around, we can conclude that this simply means <math>A \\neq B</math>. In terms of logical gates, [[inequality (mathematics)|inequality]] simply means an [[XOR gate]] (exclusive or). Therefore, <math>(A \\wedge \\bar{B}) \\vee (\\bar{A} \\wedge B) \\iff A \\neq B</math>. Then the two circuits shown below are equivalent:\n\n[[File:Circuit-minimization.svg]]\n\nYou can additionally check the correctness of the result using a [[truth table]].\n\n=={{anchor|Harvard|Svoboda|Händler|Kortum}}Graphical two-level logic minimization methods==\nGraphical minimization methods for two-level logic include:\n* ''[[Marquand diagram]]'' (1881) by [[Allan Marquand]] (1853–1924)<ref name=\"Marquand_1881\"/><ref name=\"Brown_2012\"/><!-- a precursor to Karnaugh maps, needs to be covered explicitly inhere or in a separate article - until then parked here for completeness -->\n* ''[[Harvard minimizing chart]]'' (1951) by [[Howard H. Aiken]] and Martha L. Whitehouse of the [[Harvard Computation Laboratory]]<ref name=\"Aiken_1952\"/><ref name=\"Karnaugh_1953\"/><ref name=\"Phister_1959\"/><ref name=\"Curtis_1962\"/>\n* ''[[Veitch chart]]'' (1952) by [[Edward Veitch]] (1924–2013)<ref name=\"Veitch_1952\"/><ref name=\"Brown_2012\"/><!-- a precursor to Karnaugh maps, needs to be covered explicitly inhere or in a separate article - until then parked here for completeness -->\n* ''[[Karnaugh map]]'' (1953) by [[Maurice Karnaugh]] (1924–)<ref name=\"Karnaugh_1953\"/><ref name=\"Curtis_1962\"/>\n* Svoboda's graphical aids (1956) and ''[[triadic map]]'' by [[Antonín Svoboda (computer scientist)|Antonín Svoboda]] (1907–1980)<ref name=\"Svoboda_1956_1\"/><ref name=\"Svoboda_1956_2\"/><ref name=\"Steinbuch-Weber_1974\"/><ref name=\"Svoboda_1979\"/>\n* ''[[Händler circle graph]]'' (aka <!-- Händler-circle graph, Handler-circle graph, -->{{lang|de|Händler'scher Kreisgraph}}, {{lang|de|Kreisgraph nach Händler}}, {{lang|de|Händler-Kreisgraph}}, {{lang|de|Händler-Diagramm}}, ''{{sic|{{lang|de|Minimisierungsgraph}}|expected={{lang|de|Minimierungsgraph}}}}'') (1958) by [[Wolfgang Händler]] (1920–1998)<ref name=\"Händler_1958\"/><ref name=\"Colloquium_1960\"/><ref name=\"Steinbuch-Wagner_1967\"/><ref name=\"Steinbuch-Weber_1974\"/><ref name=\"Hotz_1974\"/><ref name=\"ISER_1\"/><ref name=\"ISER_2\"/><ref name=\"Broy_1990\"/><ref name=\"Bauer-Wirsing_1991\"/>\n*Graph method (1965) by {{ill|Herbert Kortum|de}} (1907–1979)<ref name=\"Kortum_1964_12\"/><ref name=\"Kortum_1965_1\"/><ref name=\"Kortum_1965_3\"/><ref name=\"Kortum_1965_5\"/><ref name=\"Kortum_1967_6\"/><ref name=\"Kortum_1966_12\"/><ref name=\"Tafel_1971\"/>\n\n== See also ==\n* [[Binary decision diagram]]\n* [[Circuit minimization]]\n* [[Espresso heuristic logic minimizer]]\n* [[Karnaugh map]]\n* [[Petrick's method]]\n* [[Prime implicant]]\n* [[Circuit complexity]]\n* [[Function composition]]\n* [[Function decomposition]]\n* [[Circuit underutilization|Gate underutilization]]\n\n== References ==\n{{reflist|refs=\n<ref name=\"Buchfuhrer_2011\">{{cite journal |doi=10.1016/j.jcss.2010.06.011 |title=The complexity of Boolean formula minimization |journal=[[Journal of Computer and System Sciences]] (JCSS) |volume=77 |issue=1 |pages=142–153 |date=January 2011 |location=Computer Science Department, [[California Institute of Technology]], Pasadena, CA, USA |author-last1=Buchfuhrer |author-first1=David |author-last2=Umans |author-first2=Christopher |author-link2=Christopher Umans |publisher=[[Elsevier Inc.]] |url=https://ac.els-cdn.com/S0022000010000954/1-s2.0-S0022000010000954-main.pdf?_tid=045f1450-f937-11e7-ae63-00000aab0f26&acdnat=1515940215_dae38335610ea5f94fd299e5e7c95ffb}} This is an extended version of the conference paper: {{cite book |doi=10.1007/978-3-540-70575-8_3 |chapter=The Complexity of Boolean Formula Minimization |title=Proceedings of Automata, Languages and Programming |location=35th International Colloquium (ICALP) |volume=5125 |pages=24–35 |publisher=[[Springer-Verlag]] |publication-place=Berlin / Heidelberg, Germany |series=[[Lecture Notes in Computer Science]] (LNCS) |date=2008 |author-last1=Buchfuhrer |author-first1=David |author-last2=Umans |author-first2=Christopher |author-link2=Christopher Umans |isbn=978-3-540-70574-1 |url=http://users.cms.caltech.edu/~umans/papers/BU07.pdf |access-date=2018-01-14 |dead-url=no |archive-url=https://web.archive.org/web/20180114141842/http://users.cms.caltech.edu/~umans/papers/BU07.pdf |archive-date=2018-01-14}}</ref>\n<ref name=\"Mano_2014\">{{cite book |author-first1=M. Morris |author-last1=Mano |author-first2=Charles R. |author-last2=Kime |title=Logic and Computer Design Fundamentals |edition=4th new international |publisher=[[Pearson Education Limited]] |date=2014 |page=54 |isbn=978-1-292-02468-4}}</ref>\n<ref name=\"Aiken_1952\">{{cite book |title=Synthesis of electronic computing and control circuits |orig-year=January 1951 |date=1952 |edition=second printing, revised |chapter=Chapter V: Minimizing charts |pages=preface, 50–67 |author-first1=Howard H. |author-last1=Aiken |author-link1=Howard H. Aiken |author-first2=Gerrit |author-last2=Blaauw |author-link2=Gerrit Blaauw |author-first3=William |author-last3=Burkhart |author-first4=Robert J. |author-last4=Burns |author-first5=Lloyd |author-last5=Cali |author-first6=Michele |author-last6=Canepa |author-first7=Carmela M. |author-last7=Ciampa |author-first8=Charles A. |author-last8=Coolidge, Jr. |author-first9=Joseph R. |author-last9=Fucarile |author-first10=J. Orten |author-last10=Gadd, Jr. |author-first11=Frank F. |author-last11=Gucker |author-first12=John A. |author-last12=Harr |author-first13=Robert L. |author-last13=Hawkins |author-first14=Miles V. |author-last14=Hayes |author-first15=Richard |author-last15=Hofheimer |author-first16=William F. |author-last16=Hulme |author-first17=Betty L. |author-last17=Jennings |author-first18=Stanley A. |author-last18=Johnson |author-first19=Theodore |author-last19=Kalin |author-first20=Marshall |author-last20=Kincaid |author-first21=E. Edward |author-last21=Lucchini |author-first22=William |author-last22=Minty |author-first23=Benjamin L. |author-last23=Moore |author-first24=Joseph |author-last24=Remmes |author-first25=Robert J. |author-last25=Rinn |author-first26=John W. |author-last26=Roche |author-first27=Jacquelin |author-last27=Sanbord |author-first28=Warren L. |author-last28=Semon |author-first29=Theodore |author-last29=Singer |author-first30=Dexter |author-last30=Smith |author-first31=Leonard |author-last31=Smith |author-first32=Peter F. |author-last32=Strong |author-first33=Helene V. |author-last33=Thomas |author-first34=An |author-last34=Wang |author-link34=An Wang |author-first35=Martha L. |author-last35=Whitehouse |author-first36=Holly B. |author-last36=Wilkins |author-first37=Robert E. |author-last37=Wilkins |author-first38=Way Dong |author-last38=Woo |author-first39=Elbert P. |author-last39=Little |author-first40=M. Scudder |author-last40=McDowell |location=Write-Patterson Air Force Base |publisher=[[Harvard University Press]] (Cambridge, Massachusetts, USA) / Geoffrey Cumberlege Oxford University Press (London) |url=https://archive.org/stream/in.ernet.dli.2015.509288/2015.509288.Synthesis-Of#page/n6/mode/1up |access-date=2017-04-16 |quote=[…] Martha Whitehouse constructed the minimizing charts used so profusely throughout this book, and in addition prepared minimizing charts of seven and eight variables for experimental purposes. […] Hence, the present writer is obliged to record that the general algebraic approach, the switching function, the vacuum-tube operator, and the minimizing chart are his proposals, and that he is responsible for their inclusion herein. […]}} (NB. Work commenced in April 1948.)</ref>\n<ref name=\"Phister_1959\">{{cite book |title=Logical design of digital computers |author-first=Montgomery |author-last=Phister, Jr. |publisher=[[John Wiley & Sons Inc.]] |date=1959 |orig-year=December 1958 |location=New York, USA |isbn=0471688053<!-- |id={{ISBN|978-0471688051}}? --> |pages=75–83 |url=https://archive.org/stream/in.ernet.dli.2015.74854/2015.74854.Logical-Design-Of-Digital-Computers#page/n0/mode/1up}}</ref>\n<ref name=\"Curtis_1962\">{{cite book |title=A new approach to the design of switching circuits |author-first=H. Allen |author-last=Curtis |publisher=[[D. van Nostrand Company, Inc.]] |date=1962 |location=Princeton, New Jersey, USA |series=The Bell Laboratories Series}}</ref>\n<ref name=\"Karnaugh_1953\">{{cite journal |author-last=Karnaugh |author-first=Maurice |author-link=Maurice Karnaugh |title=The Map Method for Synthesis of Combinational Logic Circuits |journal=[[Transactions of the American Institute of Electrical Engineers]] part I |volume=72 |issue=9 |pages=593–599 |date=November 1953 |orig-year=1953-04-23<!-- available for printing -->, 1953-03-17<!-- sent in --> |id=Paper 53-217 |doi=10.1109/TCE.1953.6371932 |url=http://philectrosophy.com/documents/The%20Map%20Method%20For%20Synthesis%20of.pdf |access-date=2017-04-16 |dead-url=no |archive-url=https://web.archive.org/web/20170416232229/http://philectrosophy.com/documents/The%20Map%20Method%20For%20Synthesis%20of.pdf |archive-date=2017-04-16}} (NB. Also contains a short review by [[Samuel H. Caldwell]].)</ref>\n<ref name=\"Brown_2012\">{{cite book |title=Boolean Reasoning - The Logic of Boolean Equations |author-first=Frank Markham |author-last=Brown |edition=<!-- 2012 -->reissue of 2nd |publisher=[[Dover Publications, Inc.]] |location=Mineola, New York |date=2012 |orig-year=2003, 1990 |isbn=978-0-486-42785-0 |id={{ISBN|0-486-42785-4}}}} [<!-- 1st edition -->http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf<!-- https://web.archive.org/web/20170416231752/http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf -->]</ref>\n<ref name=\"Marquand_1881\">{{cite journal |title=XXXIII: On Logical Diagrams for ''n'' terms |author-first=Allan |author-last=Marquand |author-link=Allan Marquand |journal=[[The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science]] |issue=75 |series=5 |date=1881 |volume=12 |doi=10.1080/14786448108627104 |pages=266–270 |url=http://www.tandfonline.com/doi/abs/10.1080/14786448108627104 |access-date=2017-05-15}} (NB. Quite many secondary sources erroneously cite this work as \"A logical diagram for ''n'' terms\" or \"On a logical diagram for ''n'' terms\".)</ref>\n<ref name=\"Veitch_1952\">{{cite journal |author-last=Veitch |author-first=Edward W. |author-link=Edward Veitch |title=A Chart Method for Simplifying Truth Functions |journal=ACM Annual Conference/Annual Meeting: Proceedings of the 1952 ACM Annual Meeting (Pittsburg) |publisher=[[Association for Computing Machinery|ACM]] |location=New York, USA |pages=127–133 |date=1952-05-03 |orig-year=1952-05-02 |doi=10.1145/609784.609801}}</ref>\n<ref name=\"Svoboda_1956_1\">{{cite book |title=Graficko-mechanické pomůcky užívané při analyse a synthese kontaktových obvodů |trans-title=Utilization of graphical-mechanical aids for the analysis and synthesis of contact circuits |journal=Stroje na zpracování informací [Symphosium IV on information processing machines] |author-first=Antonín |author-last=Svoboda |author-link=Antonín Svoboda (computer scientist) |location=Prague |publisher=Czechoslovak Academy of Sciences, Research Institute of Mathematical Machines |language=Czech |date=1956 |volume=IV |pages=9–21}}</ref>\n<ref name=\"Svoboda_1956_2\">{{cite book |title=Graphical Mechanical Aids for the Synthesis of Relay Circuits |journal=Nachrichtentechnische Fachberichte (NTF), Beihefte der Nachrichtentechnischen Zeitschrift (NTZ) |publisher=[[Vieweg-Verlag]] |location=Braunschweig, Germany |author-first=Antonín |author-last=Svoboda |author-link=Antonín Svoboda (computer scientist) |date=1956}}</ref>\n<ref name=\"Svoboda_1979\">{{cite book |title=Advanced Logical Circuit Design Techniques |author-first1=Antonín |author-last1=Svoboda |author-link1=Antonín Svoboda (computer scientist) |author-first2=Donnamaie E. |author-last2=White |date=2016 |orig-year=1979-08-01 |edition=retyped electronic reissue |publisher=[[Garland STPM Press]] (original issue) / WhitePubs (reissue) |isbn=0-8240-7014-3<!-- 1990 1st issue --> |id={{ISBN|978-0-8240-7014-4}}<!-- 1990 1st issue --> |url=http://www.donnamaie.com/Advanced_logic_ckt/Advanced_Logical_Circuit_Design_Techniques%20sm.pdf |access-date=2017-04-15 |dead-url=no |archive-url=https://web.archive.org/web/20160315001009/http://donnamaie.com/Advanced_logic_ckt/Advanced_Logical_Circuit_Design_Techniques%20sm.pdf |archive-date=2016-03-15}} [http://www.donnamaie.com/<!-- https://web.archive.org/web/20170415220158/http://www.donnamaie.com/ -->] [https://books.google.com/books?id=g3uzAAAAIAAJ]</ref>\n<ref name=\"Händler_1958\">{{cite book |title=Ein Minimisierungsverfahren zur Synthese von Schaltkreisen (Minimisierungsgraphen) |language=German |author-first=Wolfgang |author-last=Händler |author-link=Wolfgang Händler |publisher=[[Technische Hochschule Darmstadt]] |date=1958 |id=D&nbsp;17 |type=Dissertation |url=https://books.google.com/books?id=D58TAQAAIAAJ}} [https://www.tib.eu/de/suchen/id/TIBKAT%3A044782241/Ein-Minimisierungsverfahren-zur-Synthese-von-Schaltkreisen/] (NB. Although written by a German, the title contains an [[anglicism]]; the correct German term would be \"Minimierung\" instead of \"Minimisierung\".)</ref>\n<ref name=\"Hotz_1974\">{{cite book |title=Schaltkreistheorie |language=German |trans-title=Switching circuit theory |author-first=Günter |author-last=Hotz |publisher=[[Walter de Gruyter & Co.]] |series=DeGruyter Lehrbuch |date=1974 |isbn=3-11-00-2050-5 |page=117 |quote=[…] Der Kreisgraph von ''[[Wolfgang Händler|Händler]]'' ist für das Auffinden von [[prime implicant|Primimplikanten]] gut brauchbar. Er hat den Nachteil, daß er schwierig zu zeichnen ist. Diesen Nachteil kann man allerdings durch die Verwendung von Schablonen verringern. […] [The circle graph by ''Händler'' is well suited to find [[prime implicant]]s. A disadvantage is that it is difficult to draw. This can be remedied using stencils.]}}</ref>\n<ref name=\"ISER_1\">{{cite web |title=Informatik Sammlung Erlangen (ISER) |date=2012-03-13 |publisher=[[Friedrich-Alexander Universität]] |location=Erlangen, Germany |language=German |url=https://www.rrze.fau.de/wir-ueber-uns/kooperationen/iser.shtml |access-date=2017-04-12 |dead-url=yes |archive-url=https://web.archive.org/web/20170516154655/https://www.rrze.fau.de/wir-ueber-uns/kooperationen/iser.shtml |archive-date=2017-05-16}} (NB. Shows a picture of a {{lang|de|Kreisgraph}} by ''[[Wolfgang Händler|Händler]]''.)</ref>\n<ref name=\"ISER_2\">{{cite web |title=Informatik Sammlung Erlangen (ISER) - Impressum |date=2012-03-13 |publisher=[[Friedrich-Alexander Universität]] |location=Erlangen, Germany |language=German |url=http://www.iser.uni-erlangen.de:80/index.php?ort_id=327&tree=0 |access-date=2017-04-15 |dead-url=no |archive-url=https://web.archive.org/web/20120226004316/http://www.iser.uni-erlangen.de/index.php?ort_id=327&tree=0 |archive-date=2012-02-26}} (NB. Shows a picture of a {{lang|de|Kreisgraph}} by ''[[Wolfgang Händler|Händler]]''.)</ref>\n<ref name=\"Broy_1990\">{{cite book |title=Informatik und Mathematik |language=German |trans-title=Computer Sciences and Mathematics |chapter=Geschichte der Schaltalgebra |trans-chapter=History of circuit switching algebra |author-first=Heinz |author-last=Zemanek |author-link=Heinz Zemanek |editor-first=Manfred |editor-last=Broy |editor-link=Manfred Broy |orig-year=1990 |date=2013 |publisher=[[Springer-Verlag]] |isbn=9783642766770 |id={{ISBN|3642766773}} |pages=43–72 |url=https://books.google.com/books?id=y5GfBgAAQBAJ |quote=Einen Weg besonderer Art, der damals zu wenig beachtet wurde, wies [[Wolfgang Händler|W. Händler]] in seiner Dissertation […] mit einem Kreisdiagramm. […]}} [https://link.springer.com/chapter/10.1007%2F978-3-642-76677-0_3] (NB. Collection of papers at a colloquium held at the [[Bayerische Akademie der Wissenschaften]],\n1989-06-12/14, in honor of [[Friedrich L. Bauer]].)</ref>\n<ref name=\"Bauer-Wirsing_1991\">{{cite book |author-first1=Friedrich Ludwig |author-last1=Bauer |author-link1=Friedrich Ludwig Bauer |author-first2=Martin |author-last2=Wirsing |author-link2=Martin Wirsing |title=Elementare Aussagenlogik |publisher=[[Springer-Verlag]] |language=German |location=Berlin / Heidelberg |date=March 1991 |isbn=3-540-52974-8 |id={{ISBN|978-3-540-52974-3}} |pages=54–56, 71, 112–113, 138–139 |url=https://books.google.com/books?id=Ff58BwAAQBAJ |quote=[…] handelt es sich um ein [[Wolfgang Händler|Händler]]-Diagramm […], mit den Würfelecken als Ecken eines 2<sup>m</sup>-gons. […] Abb. […] zeigt auch Gegenstücke für andere Dimensionen. Durch waagerechte Linien sind dabei Tupel verbunden, die sich nur in der ersten Komponente unterscheiden; durch senkrechte Linien solche, die sich nur in der zweiten Komponente unterscheiden; durch 45°-Linien und 135°-Linien solche, die sich nur in der dritten Komponente unterscheiden usw. Als Nachteil der Händler-Diagramme wird angeführt, daß sie viel Platz beanspruchen. […]}}</ref>\n<ref name=\"Colloquium_1960\">{{cite book |editor-first1=Ernst Ferdinand |editor-last1=Peschl |editor-link1=Ernst Ferdinand Peschl |editor-first2=Heinz |editor-last2=Unger |editor-link2=:de:Heinz Unger (Mathematiker) |title=Colloquium über Schaltkreis- und Schaltwerk-Theorie - Vortragsauszüge vom 26. bis 28. Oktober 1960 in Bonn - Band 3 von Internationale Schriftenreihe zur Numerischen Mathematik [International Series of Numerical Mathematics] (ISNM) |language=German |volume=3 |chapter=Zum Gebrauch von Graphen in der Schaltkreis- und Schaltwerktheorie |author-first=Wolfgang |author-last=Händler |author-link=Wolfgang Händler |location=Institut für Angewandte Mathematik, [[Universität Saarbrücken]], Rheinisch-Westfälisches Institut für Instrumentelle Mathematik |publisher=[[Springer Basel AG]] / [[Birkhäuser Verlag Basel]] |date=2013 |orig-year=1961 |isbn=978-3-0348-5771-0 |id={{ISBN|3-0348-5771-3}} |doi=10.1007/978-3-0348-5770-3 |url=https://books.google.com/books?id=myTnoAEACAAJ |pages=169–198}} [https://link.springer.com/chapter/10.1007%2F978-3-0348-5770-3_10]</ref>\n<ref name=\"Kortum_1964_12\">{{cite journal |title=Minimierung von Kontaktschaltungen durch Kombination von Kürzungsverfahren und Graphenmethoden |trans-title=Minimization of contact circuits by combination of reduction procedures and graphical methods |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1965 |volume=8 |issue=12 |pages=421–425 |issn=0026-0347 |id={{CODEN|MSRGAN}} |url=https://www.tib.eu/en/search/id/ceaba%3ACEAB1966001170/MINIMIERUNG-VON-KONTAKTSCHALTUNGEN-DURCH-KOMBINATION/}} [https://www.tib.eu/en/search/id/ei-backfile%3Ac84_64eb63f914c231eeM6f1319817173212/Minimization-of-contact-circuits-by-combination/]</ref>\n<ref name=\"Kortum_1965_1\">{{cite journal |title=Konstruktion und Minimierung von Halbleiterschaltnetzwerken mittels Graphentransformation |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1966 |volume=9 |issue=1 |pages=9–12 |issn=0026-0347 |id={{CODEN|MSRGAN}} |url=https://www.tib.eu/en/search/id/ceaba%3ACEAB1966002519/KONSTRUKTION-UND-MINIMIERUNG-VON-HALBLEITER-SCHALTNETZWERKEN/}}</ref>\n<ref name=\"Kortum_1965_3\">{{cite journal |title=Weitere Bemerkungen zur Minimierung von Schaltnetzwerken mittels Graphenmethoden |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1966 |volume=9 |issue=3 |pages=96–102 |issn=0026-0347 |id={{CODEN|MSRGAN}} |url=https://www.tib.eu/en/search/id/ceaba%3ACEAB1966002896/WEITERE-BEMERKUNGEN-ZUR-MINIMIERUNG-VON-SCHALTNETZWERKEN/}}</ref>\n<ref name=\"Kortum_1965_5\">{{cite journal |title=Weitere Bemerkungen zur Behandlung von Schaltnetzwerken mittels Graphen. Konstruktion von vermaschten Netzwerken (Brückenschaltungen) |trans-title=Further remarks on treatment of switching networks by means of graphs |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1966 |volume=9 |issue=5 |pages=151–157 |issn=0026-0347 |id={{CODEN|MSRGAN}}}} {{cite journal |title=Weitere Bemerkungen zur Behandlung von Schaltnetzwerken mittels Graphen |trans-title=Further remarks on treatment of switching networks by means of graphs |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=Regelungstechnik |language=German |publisher= |date=1965 |volume=10 |issue=5 |pages=33–39 |location=10. Internationales Wissenschaftliches Kolloquium, Ilmenau. Technische Hochschule |url=https://www.tib.eu/en/search/id/ei-backfile%3Ac84_a3574af8cb8d6293M72eb19817173212/Further-remarks-on-treatment-of-switching-networks/}}</ref>\n<ref name=\"Kortum_1966_12\">{{cite journal |title=Zur Minimierung von Schaltsystemen |trans-title=Minimization of switching circuits |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=Wissenschaftliche Zeitschrift der TU Ilmenau |location=Jena |language=German |publisher=Technische Hochschule für Elektrotechnik Ilmenau / Forschungsstelle für Meßtechnik und Automatisierung der Deutschen Akademie der Wissenschaften |date=1966<!-- one source states: 1965 --> |volume=12 |issue=2<!-- one sources states: 2, 3. --> |pages=181–186 |url=https://www.tib.eu/en/search/id/ei-backfile%3Ac84_125e37df8f0a3cd4eM7a8919817173212/Minimization-of-switching-circuits/}}</ref>\n<ref name=\"Kortum_1967_6\">{{cite journal |title=Über zweckmäßige Anpassung der Graphenstruktur diskreter Systeme an vorgegebene Aufgabenstellungen |author-first=Herbert |author-last=Kortum |author-link=:de:Herbert Kortum |journal=messen-steuern-regeln (msr) |language=German |publisher={{ill|Verlag Technik|de}} |date=1967 |volume=10 |issue=6 |pages=208–211 |issn=0026-0347 |id={{CODEN|MSRGAN}}}}</ref>\n<ref name=\"Tafel_1971\">{{cite book |title=Einführung in die digitale Datenverarbeitung |language=German |trans-title=Introduction to digital information processing |chapter=4.3.5. Graphenmethode zur Vereinfachung von Schaltfunktionen |author-first=Hans Jörg |author-last=Tafel |publisher=[[Carl Hanser Verlag]] |date=1971 |location=[[RWTH]], Aachen, Germany |publication-place=Munich, Germany |isbn=3-446-10569-7 |pages=98–105, 107–113}}</ref>\n<ref name=\"Steinbuch-Wagner_1967\">{{cite book |title=Taschenbuch der Nachrichtenverarbeitung |language=German |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Siegfried W. |editor-last2=Wagner |author-first1=Erich R. |author-last1=Berger |author-first2=Wolfgang |author-last2=Händler |author-link2=Wolfgang Händler |date=1967 |orig-year=1962 |edition=2 |publisher=[[Springer-Verlag OHG]] |location=Berlin, Germany |id=Title No. 1036 |lccn=67-21079 |pages=64, 1034–1035, 1036, 1038 |quote=[…] Übersichtlich ist die Darstellung nach ''[[Wolfgang Händler|Händler]]'', die sämtliche Punkte, numeriert nach dem ''[[Gray-Code]]'' […], auf dem Umfeld eines Kreises anordnet. Sie erfordert allerdings sehr viel Platz. […] [''Händler's'' illustration, where all points, numbered according to the ''[[Gray code]]'', are arranged on the circumference of a circle, is easily comprehensible. It needs, however, a lot of space.]}}</ref>\n<ref name=\"Steinbuch-Weber_1974\">{{cite book |title=Taschenbuch der Informatik - Band II - Struktur und Programmierung von EDV-Systemen |language=German |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Wolfgang |editor-last2=Weber |editor-first3=Traute |editor-last3=Heinemann |date=1974 |orig-year=1967 |edition=3 |volume=2 |work=Taschenbuch der Nachrichtenverarbeitung |publisher=[[Springer-Verlag]] |location=Berlin, Germany |isbn=3-540-06241-6 |lccn=73-80607 |pages=25, 62, 96, 122–123, 238}}</ref>\n}}\n\n== Further reading ==\n* {{cite book |title=Synthesis and Optimization of Digital Circuits |author-first=Giovanni |author-last=De Micheli |author-link=Giovanni De Micheli |date=1994 |publisher=[[McGraw-Hill]] |isbn=0-07-016333-2}} (NB. Chapters 7-9 cover combinatorial two-level, combinatorial multi-level, and respectively sequential circuit optimization.)\n* {{cite book |author-first1=Gary D. |author-last1=Hachtel |author-first2=Fabio |author-last2=Somenzi |title=Logic Synthesis and Verification Algorithms |date=2006 |orig-year=1996 |publisher=[[Springer Science & Business Media]] |isbn=978-0-387-31005-3}}\n* {{cite book |author-first1=Zvi |author-last1=Kohavi |author-first2=Niraj K. |author-last2=Jha |title=Switching and Finite Automata Theory |edition=3rd |publisher=[[Cambridge University Press]] |date=2009 |isbn=978-0-521-85748-2 |chapter=4–6}}\n* {{cite book |title=The Art of Computer Programming |title-link=The Art of Computer Programming |date=2010 |author-last=Knuth |author-first=Donald Ervin |author-link=Donald Ervin Knuth |volume=4A |chapter=chapter 7.1.2: Boolean Evaluation |publisher=[[Addison-Wesley]] |pages=96–133 |isbn=0-201-03804-8}}\n* {{cite book |author-first=Rob A. |author-last=Rutenbar |title=Multi-level minimization, Part I: Models & Methods |type=lecture slides |publisher=[[Carnegie Mellon University]] (CMU) |id=Lecture 7 |url=https://www.ece.cmu.edu/~ee760/760docs/lec07.pdf |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115125725/https://www.ece.cmu.edu/~ee760/760docs/lec07.pdf |archive-date=2018-01-15}}; {{cite book |author-first=Rob A. |author-last=Rutenbar |title=Multi-level minimization, Part II: Cube/Cokernel Extract |type=lecture slides |publisher=[[Carnegie Mellon University]] (CMU) |id=Lecture 8 |url=https://www.ece.cmu.edu/~ee760/760docs/lec08.pdf |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115125733/https://www.ece.cmu.edu/~ee760/760docs/lec08.pdf |archive-date=2018-01-15}}\n* {{cite journal |url=http://matwbn.icm.edu.pl/ksiazki/amc/amc13/amc13414.pdf |author-last1=Tomaszewski |author-first1=Sebastian P. |author-last2=Celik |author-first2=Ilgaz U. |author-last3=Antoniou |author-first3=George E. |title=WWW-based Boolean function minimization |journal=[[International Journal of Applied Mathematics and Computer Science]] |volume=13 |issue=part 4 |pages=577–584 |date=2003 |access-date=2018-01-15 |dead-url=no |archive-url=https://archive.is/20180115131301/http://matwbn.icm.edu.pl/ksiazki/amc/amc13/amc13414.pdf |archive-date=2018-01-15}}\n\n{{digital electronics}}\n\n[[Category:Electronic engineering]]\n[[Category:Electronic design]]\n[[Category:Digital electronics]]\n[[Category:Electronic design automation]]\n[[Category:Electronics optimization]]\n[[Category:Boolean algebra]]\n[[Category:Circuit complexity]]\n[[Category:Logic in computer science]]"
    },
    {
      "title": "Logic redundancy",
      "url": "https://en.wikipedia.org/wiki/Logic_redundancy",
      "text": "'''Logic redundancy''' occurs in a [[logic gate|digital gate]] network containing circuitry that does not affect the static logic function. There are several reasons why logic redundancy may exist. One reason is that it may have been added deliberately to suppress transient glitches (thus causing a [[race condition]]) in the output signals by having two or more product terms overlap with a third one.\n\nConsider the following equation:\n\n:<math>\nY = A B + \\overline{A} C + B C.\n</math>\n\nThe third product term <math>BC</math> is a redundant [[Consensus theorem|consensus term]]. If <math>A</math> switches from 1 to 0 while <math>B = 1</math> and <math>C = 1</math>, <math>Y</math> remains 1.  During the transition of signal <math>A</math> in logic gates, both the first and second term may be 0 momentarily. The third term prevents a glitch since its value of 1 in this case is not affected by the transition of signal <math>A</math>.\n\nAnother reason for logic redundancy is poor design practices which unintentionally result in logically redundant terms. This causes an unnecessary increase in network complexity, and possibly hampering the ability to test manufactured designs using traditional test methods (single stuck-at fault models). (Note: testing might be possible using [[Iddq testing|IDDQ]] models.)\n\n==Removing logic redundancy==\nLogic redundancy is, in general, not desired.\nRedundancy, by definition, requires extra parts (in this case: logical terms) which raises the cost of implementation (either actual cost of physical parts or [[CPU time]] to process).\nLogic redundancy can be removed by several well-known techniques, such as [[Karnaugh map]]s, the [[Quine–McCluskey algorithm]], and the [[Espresso heuristic logic minimizer|heuristic computer method]].\n\n==Adding logic redundancy==\n{{Main|hazard (logic)}}\n\n[[Image:K-map 6,8,9,10,11,12,13,14.svg|thumb|A k-map showing a particular logic function]]\n\nIn some cases it may be desirable to ''add'' logic redundancy.  One of those cases is to avoid [[race condition]]s whereby an output can fluctuate because different terms are \"racing\" to turn off and on.  To explain this in more concrete terms the [[Karnaugh map]] to the right shows the minterms and maxterms for the following function:\n\n:<math>f(A, B, C, D) = E(6, 8, 9, 10, 11, 12, 13, 14).\\ </math>\n\nThe boxes represent the minimal AND/OR terms needed to implement this function:\n\n:<math>F = A\\overline{C} + A\\overline{B} + BC\\overline{D}.</math>\n\nThe k-map visually shows where [[race condition]]s occur in the minimal expression by having gaps between minterms or gaps between maxterms.  For example, the gap between the blue and green rectangles.  If the input <math>ABCD=1110</math> were to change to <math>ABCD=1010</math> then a race will occur between <math>BC\\overline{D}</math> turning off and <math>A\\overline{B}</math> turning off.\nIf the blue term switches off before the green turns on then the output will fluctuate and may register as 0.\nAnother race condition is between the blue and the red for transition of <math>ABCD=1110</math> to <math>ABCD=1100</math>.\n\n[[Image:K-map 6,8,9,10,11,12,13,14 anti-race.svg|thumb|left|Above k-map with the <math>A\\overline{D}</math> term added to avoid race hazards]]\n\nThe race condition is removed by adding in logic redundancy, which is contrary to the aims of using a k-map in the first place.\nBoth minterm race conditions are covered by addition of the yellow term <math>A\\overline{D}</math>.\n(The maxterm race condition is covered by addition of the green-bordered grey term <math>A+\\overline{D}</math>.)\n\nIn this case, the addition of logic redundancy has stabilized the output to avoid output fluctuations because terms are racing each other to change state.\n\n== See also ==\n{{Portal|Computer Science}} 3\n\n{{clear}}\n\n[[Category:Boolean algebra]]\n[[Category:Electronic engineering]]\n[[Category:Digital electronics]]"
    },
    {
      "title": "Logical matrix",
      "url": "https://en.wikipedia.org/wiki/Logical_matrix",
      "text": "A '''logical matrix''', '''binary matrix''', '''relation matrix''', '''Boolean matrix''', or '''(0,1) matrix''' is a [[matrix (mathematics)|matrix]] with entries from the [[Boolean domain]] {{nowrap|1='''B''' = {0, 1}.}}  Such a matrix can be used to represent a [[binary relation]] between a pair of [[finite set]]s.\n\n==Matrix representation of a relation==\nIf ''R'' is a [[binary relation]] between the finite [[indexed set]]s ''X'' and ''Y'' (so {{nowrap| ''R'' ⊆ ''X''×''Y''}}), then ''R'' can be represented by the logical matrix ''M'' whose row and column indices index the elements of ''X'' and ''Y'', respectively, such that the entries of ''M'' are defined by:\n\n:<math>M_{i,j} =\n \\begin{cases}\n   1 & (x_i, y_j) \\in R \\\\\n   0 & (x_i, y_j) \\not\\in R \n \\end{cases}\n </math>\n\nIn order to designate the row and column numbers of the matrix, the sets ''X'' and ''Y'' are indexed with positive integers: ''i'' ranges from 1 to the [[cardinality]] (size) of ''X'' and ''j'' ranges from 1 to the cardinality of ''Y''. See the entry on [[indexed set]]s for more detail.\n\n===Example===\nThe binary relation ''R'' on the set {{nowrap|{1, 2, 3, 4}{{null}}}} is defined so that ''aRb'' holds if and only if ''a'' [[divides]] ''b'' evenly, with no remainder.  For example, 2''R''4 holds because 2 divides 4 without leaving a remainder, but 3''R''4 does not hold because when 3 divides 4 there is a remainder of 1. The following set is the set of pairs for which the relation ''R'' holds. \n:{(1, 1), (1, 2), (1, 3), (1, 4), (2, 2), (2, 4), (3, 3), (4, 4)}.\nThe corresponding representation as a logical matrix is:\n\n:<math>\\begin{pmatrix}\n   1 & 1 & 1 & 1 \\\\\n   0 & 1 & 0 & 1 \\\\\n   0 & 0 & 1 & 0 \\\\\n   0 & 0 & 0 & 1\n \\end{pmatrix}</math> which includes a diagonal of ones since each number divides itself.\n\n==Other examples==\n\n* A [[permutation matrix]] is a (0,1)-matrix, all of whose columns and rows each have exactly one nonzero element. \n** A [[Costas array]] is a special case of a permutation matrix\n* An [[incidence matrix]] in [[combinatorics]] and [[finite geometry]] has ones to indicate incidence between points (or vertices) and lines of a geometry, blocks of a [[block design]], or edges of a [[graph (discrete mathematics)]]\n* A [[design matrix]] in [[analysis of variance]] is a (0,1)-matrix with constant row sums.\n* A logical matrix may represent an [[adjacency matrix]] in [[graph theory]]: non-symmetric matrices correspond to [[directed graph]]s, symmetric matrices to ordinary [[graph (discrete mathematics)|graph]]s, and a 1 on the diagonal corresponds to a [[loop (graph theory)|loop]] at the corresponding vertex.\n* The [[biadjacency matrix]] of a simple, undirected [[bipartite graph]] is a (0,1)-matrix, and any (0,1)-matrix arises in this way.\n* The prime factors of a list of ''m'' [[square-free integer|square-free]], [[smooth number|''n''-smooth]] numbers can be described as a ''m''&times;π(''n'') (0,1)-matrix, where π is the [[prime-counting function]] and ''a''<sub>''ij''</sub> is 1 if and only if the ''j''th prime divides the ''i''th number. This representation is useful in the [[quadratic sieve]] factoring algorithm.\n*A [[Raster graphics|bitmap image]] containing [[pixel]]s in only two colors can be represented as a (0,1)-matrix in which the 0's represent pixels of one color and the 1's represent pixels of the other color.\n* A binary matrix can be used to check the game rules in the game of [[Go (game)|Go]] <ref>{{cite web|url=http://senseis.xmp.net/?BinMatrix|title=Binmatrix|date=February 8, 2013|access-date=August 11, 2017|first=Kjeld|last=Petersen}}</ref>\n\n==Some properties==\nThe matrix representation of the [[Equality (mathematics)|equality relation]] on a finite set is the [[identity matrix]] I, that is, the matrix whose entries on the diagonal are all 1, while the others are all 0. More generally, if relation ''R'' satisfies I ⊂ ''R'', then R is a [[reflexive relation]].\n\nIf the Boolean domain is viewed as a [[semiring]], where addition corresponds to [[logical OR]] and multiplication to [[logical AND]], the matrix representation of the [[composition of relations|composition]] of two relations is equal to the [[matrix product]] of the matrix representations of these relations.\nThis product can be computed in [[Expected value|expected]] time O(''n''<sup>2</sup>).<ref>{{cite journal| author=Patrick E. O'Neil, Elizabeth J. O'Neil| title=A Fast Expected Time Algorithm for Boolean Matrix Multiplication and Transitive Closure| journal=Information and Control| year=1973| volume=22| issue=2 |pages=132–138| url=http://www.sciencedirect.com/science/article/pii/S0019995873902283/pdf?md5=37b2d0aafda0b0639e48370c73d5e82a&pid=1-s2.0-S0019995873902283-main.pdf| doi=10.1016/s0019-9958(73)90228-3}} &mdash; The algorithm relies on addition being [[idempotent]], cf. p.134 (bottom).</ref>\n\nFrequently operations on binary matrices are defined in terms of [[modular arithmetic]] mod 2&mdash;that is, the elements are treated as elements of the [[Galois field]] {{nowrap|1='''GF'''(2) = ℤ<sub>2</sub>}}. They arise in a variety of representations and have a number of more restricted special forms. They are applied e.g. in [[XOR-satisfiability]].<!---more links to applications should go here--->\n\nThe number of distinct ''m''-by-''n'' binary matrices is equal to 2<sup>''mn''</sup>, and is thus finite.\n\n==Lattice==\nLet ''n'' and ''m'' be given and let ''U'' denote the set of all logical ''m'' × ''n'' matrices. Then ''U'' has a [[partial order]] given by\n:<math>m \\subset n \\quad \\text{when} \\quad \\forall i,j \\quad m_{ij} = 1 \\implies n_{ij} = 1 .</math>\n\nIn fact, ''U'' forms a [[Boolean algebra]] with the operations [[and (logic)|and]] and [[or (logic)|or]] between two matrices applied component-wise. The complement of a logical matrix is obtained by swapping all zeros and ones for their opposite.\n\nEvery logical matrix a = ( a <sub>i j </sub> ) has an '''transpose''' a<sup>T</sup> = ( a <sub>j i</sub> ). Suppose ''a'' is a logical matrix with no columns or rows identically zero. Then the matrix product, using Boolean arithmetic, a<sup>T</sup> a contains the ''m'' × ''m'' [[identity matrix]], and the product a a<sup>T</sup> contains the ''n'' × ''n'' identity.\n\nAs a mathematical structure, the Boolean algebra ''U'' forms a [[lattice (order)|lattice]] ordered by [[inclusion (logic)|inclusion]]; additionally it is a '''multiplicative lattice''' due to matrix multiplication. \n\nEvery logical matrix in ''U'' corresponds to a binary relation. These listed operations on ''U'', and ordering, correspond to a [[algebraic logic#Calculus of relations|calculus of relations]], where the matrix multiplication represents [[composition of relations]].<ref>[[Irving Copilowish]] (December 1948) \"Matrix development of the calculus of relations\", [[Journal of Symbolic Logic]] 13(4): 193–203 [https://www.jstor.org/stable/2267134?seq=1#page_scan_tab_contents Jstor link]</ref>\n\n==Logical vectors==\nIf ''m'' or ''n'' equals one, then the ''m'' × ''n'' logical matrix (M<sub>i j</sub>) is a logical vector. If ''m'' = 1 the vector is a row vector, and if ''n'' = 1 it is a column vector. In either case the index equaling one is dropped from denotation of the vector.\n\nSuppose <math>(P_i), \\quad i = 1, 2,... m \\ \\ \\text{and}\\ \\ (Q_j), \\quad j = 1,2,... n</math> are two logical vectors. The [[outer product]] of ''P'' and ''Q'' results in an ''m'' × ''n'' [[rectangular relation]]:\n:<math>M_{i j} = P_i \\land Q_j .</math> A re-ordering of the rows and columns of such a matrix can assemble all the ones into a rectangular part of the matrix. <ref name=GS>{{cite book | doi=10.1017/CBO9780511778810 | isbn=9780511778810 | author=[[Gunther Schmidt]] | page=91 | title=Relational Mathematics | chapter=6: Relations and Vectors |location= | publisher=Cambridge University Press | series= | volume= | edition= | month= | year=2013 }}</ref>\n\nLet ''h'' be the vector of all ones. Then if ''v'' is an arbitrary logical vector, the relation ''R'' = ''v h''<sup>T</sup> has constant rows determined by ''v''. In the [[calculus of relations]] such an ''R'' is called a '''vector'''.<ref name=GS/> A particular instance is the universal relation ''h h''<sup>T</sup>.\n\nFor a given relation ''R'', a maximal, rectangular relation contained in ''R'' is called a '''concept in R'''. Relations may be studied by decomposing into concepts, and then noting the [[heterogeneous relation#Induced concept lattice|induced concept lattice]].\n\n==Row and column sums==\nAdding up all the 1’s in a logical matrix may be accomplished in two ways, first summing the rows or first summing the columns. When the row-sums are added, the sum is the same as when the column-sums are added. In [[incidence geometry]], the matrix is interpreted as an [[incidence matrix]] with the rows corresponding to \"points\" and the columns as \"blocks\" (generalizing lines made of points). A row-sum is called its ''point degree'' and a column-sum is the ''block degree''. Proposition 1.6 in ''Design Theory''<ref name=BJL>{{cite book|first1=Thomas|last1=Beth|first2=Dieter|last2=Jungnickel|authorlink2=Dieter Jungnickel|first3=Hanfried|last3=Lenz|authorlink3=Hanfried Lenz|title=Design Theory|publisher=[[Cambridge University Press]]|page=18|year=1986}}. 2nd ed. (1999) {{ISBN|978-0-521-44432-3}}</ref> says that the sum of point degrees equals the sum of block degrees.\n\nAn early problem in the area was \"to find necessary and sufficient conditions for the existence of an [[incidence structure]] with given point degrees and block degrees (or in matrix language, for the existence of a (0,1)-matrix of type ''v'' × ''b'' with given row and column sums.\"<ref name=BJL/> Such a structure is a [[block design]].\n\n==See also==\n{{Commons category|Binary matrix}}\n* [[List of matrices]]\n* [[De Bruijn torus|Binatorix]] (a binary De Bruijn torus)\n* [[Bit array]]\n* [[Redheffer matrix]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{Citation | last1=Hogben | first1=Leslie | title=Handbook of Linear Algebra (Discrete Mathematics and Its Applications) | publisher=Chapman & Hall/CRC | location=Boca Raton | isbn=978-1-58488-510-8 | year=2006}}, § 31.3, Binary Matrices\n* {{Citation | last1=Kim | first1=Ki Hang | title=Boolean Matrix Theory and Applications |year=1982| isbn=978-0-8247-1788-9}}\n* [[H. J. Ryser]] (1957) \"Combinatorial properties of matrices of zeroes and ones\", [[Canadian Journal of Mathematics]] 9: 371–7.\n* Ryser, H.J. (1960) \"Traces of matrices of zeroes and ones\", ''Canadian Journal of Mathematics'' 12: 463–76.\n* Ryser, H.J. (1960) \"Matrices of Zeros and Ones\", [[Bulletin of the American Mathematical Society]] 66: 442–64.\n* [[D. R. Fulkerson]] (1960) \"Zero-one matrices with zero trace\", [[Pacific Journal of Mathematics]] 10; 831–6\n* D. R. Fulkerson & H. J. Ryser (1961) \"Widths and heights of (0, 1)-matrices\", ''Canadian Journal of Mathematics'' 13: 239–55.\n* [[L. R. Ford Jr.]] & D. R. Fulkerson (1962) § 2.12 \"Matrices composed of 0's and 1's\", pages 79 to 91 in ''Flows in Networks'', [[Princeton University Press]] {{mr|id=0159700}}\n\n==External links==\n* {{springer|title=Logical matrix|id=p/l060740}}\n\n{{DEFAULTSORT:Logical Matrix}}\n[[Category:Boolean algebra]]\n[[Category:Matrices]]"
    },
    {
      "title": "Lupanov representation",
      "url": "https://en.wikipedia.org/wiki/Lupanov_representation",
      "text": "{{unreferenced|date=December 2009}}\n{{cleanup|reason=Incomplete definition|date=August 2014}}\n'''Lupanov's (''k'',&nbsp;''s'')-representation''', named after [[Oleg Lupanov]], is a way of representing [[Boolean circuit]]s so as to show that the reciprocal of the [[Claude Shannon|Shannon effect]]. Shannon had showed that almost all [[Boolean functions]] of ''n'' variables need a [[circuit complexity|circuit]] of size at least&nbsp;2<sup>''n''</sup>''n''<sup>&minus;1</sup>. The reciprocal is that:\n\n<blockquote>\nAll Boolean functions of ''n'' variables can be computed with a circuit of at most 2<sup>''n''</sup>''n''<sup>&minus;1</sup>&nbsp;+&nbsp;o(2<sup>''n''</sup>''n''<sup>&minus;1</sup>) gates.\n</blockquote>\n\n==Definition==\n\nThe idea is to represent the values of a boolean function ''&fnof;'' in a table of 2<sup>''k''</sup> rows, representing the possible values of the ''k'' first variables ''x''<sub>1</sub>,&nbsp;...,&nbsp;,''x''<sub>''k''</sub>, and 2<sup>''n''&minus;''k''</sup> columns representing the values of the other variables.\n\nLet ''A''<sub>1</sub>,&nbsp;...,&nbsp;''A''<sub>''p''</sub> be a partition of the rows of this table such that for ''i''&nbsp;<&nbsp;''p'', |''A''<sub>''i''</sub>|&nbsp;=&nbsp;''s'' and <math>|A_p|=s'\\leq s</math>.\nLet ''&fnof;''<sub>''i''</sub>(''x'')&nbsp;=&nbsp;''&fnof;''(''x'') [[iff]]&nbsp;''x''&nbsp;&isin;&nbsp;''A''<sub>''i''</sub>.\n\nMoreover, let <math>B_{i,w}</math> be the set of the columns whose intersection with <math>A_i</math> is <math>w</math>.\n\n== See also ==\n\n* [http://courses.cs.vt.edu/cs4124/fall2005/resources/lupanov.pdf Course material describing the Lupanov representation]\n* [http://courses.cs.vt.edu/cs4124/fall2005/resources/lupanov_example.pdf An additional example from the same course material]\n\n[[Category:Boolean algebra]]\n[[Category:Circuit complexity]]"
    },
    {
      "title": "Maharam algebra",
      "url": "https://en.wikipedia.org/wiki/Maharam_algebra",
      "text": "In mathematics, a '''Maharam algebra''' is a [[complete Boolean algebra]] with a continuous submeasure. They were introduced by {{harvs|txt|last=Maharam|authorlink=Dorothy Maharam|year=1947}}.\n\n==Definitions==\n\nA '''continuous submeasure''' or '''Maharam submeasure'''  on a Boolean algebra is a [[real-valued function]] ''m'' such that\n* ''m''(0)&nbsp;=&nbsp;0, ''m''(1)&nbsp;=&nbsp;1, ''m''(''x'')&nbsp;>&nbsp;0 if ''x''&nbsp;≠&nbsp;0.\n* If ''x''&nbsp;&le;&nbsp;''y'' then ''m''(''x'')&nbsp;&le;&nbsp;''m''(''y'')\n* ''m''(''x''&nbsp;∨&nbsp;''y'') ≤&nbsp;''m''(''x'')&nbsp;+&nbsp;''m''(''y'')\n* If ''x''<sub>''n''</sub> is a [[decreasing sequence]] with intersection 0, then the sequence ''m''(''x''<sub>''n''</sub>) has [[Limit (mathematics)|limit]]&nbsp;0.\n\nA '''Maharam algebra''' is a [[complete Boolean algebra]] with a continuous submeasure.\n\n==Examples==\n\nEvery [[probability measure]] is a continuous submeasure, so as the corresponding Boolean algebra of [[measurable set]]s modulo [[Null set|measure zero sets]] is complete it is a Maharam algebra.\n\n{{harvs|txt|last=Talagrand|authorlink=Michel Talagrand|year=2008}} solved a long-standing problem by constructing a Maharam algebra that is not a [[measure algebra]], ''i.e.'' that does not admit any countably-additive strictly positive finite measure.\n\n==References==\n\n*{{citation\n | last1 = Balcar | first1 = Bohuslav | author1-link = Bohuslav Balcar\n | last2 = Jech | first2 = Thomas | author2-link = Thomas Jech\n | issue = 2\n | journal = [[Bulletin of Symbolic Logic]]\n | mr = 2223923 | zbl=1120.03028 | doi=10.2178/bsl/1146620061\n | pages = 241–266\n | title = Weak distributivity, a problem of von Neumann and the mystery of measurability\n | url = http://www.math.ucla.edu/~asl/bsl/1202-toc.htm\n | volume = 12\n | year = 2006}}\n*{{citation|first=Dorothy|last= Maharam|authorlink=Dorothy Maharam|title=An algebraic characterization of measure algebras|journal= [[Annals of Mathematics]] |series=Second Series|volume= 48 |year=1947|pages= 154–167|mr=0018718|jstor=1969222 | doi=10.2307/1969222 | zbl=0029.20401 }} \n*{{citation\n | last=Talagrand | first=Michel | author-link= Michel Talagrand\n | issue = 3\n | journal = [[Annals of Mathematics]]\n | mr = 2456888 | zbl = 1185.28002 | doi= 10.4007/annals.2008.168.981 | jstor = 40345433\n | pages = 981-1009\n | title = Maharam's Problem\n | volume = 168\n | series = Second Series\n | year = 2008}}\n*{{citation|mr=2166361 | zbl=1118.03046\n|last=Velickovic|first= Boban\n|title=ccc forcing and splitting reals\n|journal=Israel J. Math.|volume= 147 |year=2005|pages=209–220|doi=10.1007/BF02785365}}\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Majority function",
      "url": "https://en.wikipedia.org/wiki/Majority_function",
      "text": "In [[Boolean logic]], the '''majority function''' (also called the '''median operator''')  is a [[function (mathematics)|function]] from ''n'' inputs to one output. The value of the operation is false when ''n''/2 or more arguments are false, and true otherwise.\nAlternatively, representing true values as 1 and false values as 0, we may use the formula\n\n:<math>\\operatorname{Majority} \\left ( p_1,\\dots,p_n \\right ) =  \\left \\lfloor \\frac{1}{2} +  \\frac{\\left(\\sum_{i=1}^n  p_i\\right) - 1/2}{n} \\right \\rfloor. </math>\n\nThe \"&minus;1/2\" in the formula serves to break ties in favor of zeros when ''n'' is even. If the term \"&minus;1/2\" is omitted, the formula can be used for a function that breaks ties in favor of ones.\n\n== Boolean circuits ==\n[[File:Four-Bit Majority Circuit.png|thumb|Four bit majority circuit]]\n\nA ''majority gate'' is a [[logical gate]] used in [[circuit complexity]] and other applications of [[Boolean circuits]].  A majority gate returns true if and only if more than 50% of its inputs are true.\n\nFor instance, in a [[Adder (electronics)|full adder]], the carry output is found by applying a majority function to the three inputs, although frequently this part of the adder is broken down into several simpler logical gates.\n\nMany systems have [[triple modular redundancy]]; they use the majority function for [[majority logic decoding]] to implement [[error correction]].\n\nA major result in [[circuit complexity]] asserts that the majority function cannot be computed by [[AC0|AC0 circuits]] of subexponential size.\n\n== Monotone formulae for majority ==\n\nFor ''n'' = 1 the median operator is just the unary identity operation ''x''.  For ''n'' = 3 the ternary median operator can be expressed using conjunction and disjunction as ''xy'' + ''yz'' + ''zx''.  Remarkably this expression denotes the same operation independently of whether the symbol + is interpreted as [[disjunction|inclusive or]] or [[exclusive or]].\n\nFor an arbitrary ''n'' there exists a monotone formula for majority of size O(''n''<sup>5.3</sup>).<ref>{{Cite journal | first = Leslie | last = Valiant | authorlink = Leslie Valiant | title = Short monotone formulae for the majority function | journal = Journal of Algorithms | volume = 5 | issue = 3 | year = 1984 | pages = 363–366 | doi = 10.1016/0196-6774(84)90016-6}}</ref> This is proved using [[probabilistic method]]. Thus, this formula is non-constructive. However, one can obtain an explicit formula for majority of polynomial size using a [[sorting network]] of [[Miklós Ajtai|Ajtai]], [[János Komlós (mathematician)|Komlós]], and [[Endre Szemerédi|Szemerédi]].\n\nThe majority function produces \"1\" when more than half of the inputs are 1; it produces \"0\" when more than half the inputs are 0. Most applications deliberately force an odd number of inputs so they don't have to deal with the question of what happens when exactly half the inputs are 0 and exactly half the inputs are 1. The few systems that calculate the majority function on an even number of inputs are often biased towards \"0\"—they produce \"0\" when exactly half the inputs are 0 -- for example, a 4-input majority gate has a 0 output only when two or more 0's appear at its inputs.<ref>{{cite book | title=Error-correcting Codes | last1=Peterson | first1=William Wesley | last2=Weldon | first2=E.J. | isbn=9780262160391 | year=1972 | publisher=MIT Press}}</ref> In a few systems, a 4-input majority network randomly chooses \"1\" or \"0\" when exactly two 0's appear at its inputs.<ref>{{cite news | last1=Chaouiya | first1=Claudine | last2=Ourrad | first2=Ouerdia | last3=Lima | first3=Ricardo | journal=PLoS ONE | publisher=Public Library of Science | title=Majority Rules with Random Tie-Breaking in Boolean Gene Regulatory Networks |date=July 2013  | volume=8 | number=7 | doi=10.1371/journal.pone.0069626}}</ref>\n\n== Properties ==\n\nFor any ''x'', ''y'', and ''z'', the ternary median operator &lang;''x'', ''y'', ''z''&rang; satisfies the following equations.\n* &lang;''x'', ''y'', ''y''&rang; = ''y''\n* &lang;''x'', ''y'', ''z''&rang; = &lang;''z'', ''x'', ''y''&rang;\n* &lang;''x'', ''y'', ''z''&rang; = &lang;''x'', ''z'', ''y''&rang;\n* &lang;&lang;''x'', ''w'', ''y''&rang;, ''w'', ''z''&rang; = &lang;''x'', ''w'', &lang;''y'', ''w'', ''z''&rang;&rang;\nAn abstract system satisfying these as axioms is a [[median algebra]].\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n* {{cite book | last=Knuth | first=Donald E. | authorlink=Donald Knuth | title=Introduction to combinatorial algorithms and Boolean functions | series=[[The Art of Computer Programming]] | volume=4a | year=2008 | isbn=0-321-53496-4 | pages=64–74 | publisher=Addison-Wesley | location=Upper Saddle River, NJ }}\n\n==See also==\n{{Commonscat-inline|Majority functions}}\n* [[Boolean algebra (structure)]]\n* [[Boolean algebras canonically defined]]\n* [[Boyer–Moore majority vote algorithm]]\n* [[Majority problem (cellular automaton)]]\n\n[[Category:Logic gates]]\n[[Category:Circuit complexity]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Marquand diagram",
      "url": "https://en.wikipedia.org/wiki/Marquand_diagram",
      "text": "#REDIRECT [[Karnaugh map#Marquand]]\n\n{{Redirect category shell|1=\n{{R with possibilities}}\n{{R to related topic}}\n}}\n\n[[Category:Boolean algebra]]\n[[Category:Diagrams]]\n[[Category:Electronics optimization]]\n[[Category:Logic in computer science]]"
    },
    {
      "title": "Minimal axioms for Boolean algebra",
      "url": "https://en.wikipedia.org/wiki/Minimal_axioms_for_Boolean_algebra",
      "text": "In [[mathematical logic]], '''minimal axioms for Boolean algebra''' are assumptions which are equivalent to the axioms of [[Boolean algebra]] (or [[propositional calculus]]), chosen to be as short as possible. For example, an axiom with six [[Logical NAND|NAND]] operations and three variables is equivalent to Boolean algebra:\n\n: <math>((a\\mid b)\\mid c) \\mid (a\\mid((a\\mid c)\\mid a)) = c</math>\n\nwhere the vertical bar represents the NAND logical operation (also known as the [[Sheffer stroke]]).\n\n[[Stephen Wolfram]], and separately, a group of researchers including [[William McCune]], [[Branden Fitelson]], and [[Larry Wos]], identified this axiom by testing 25 candidate axioms, the set of Sheffer identities of length less or equal to 15 elements (excluding mirror images) that have no noncommutative models with four or fewer variables.<ref>{{cite book |last1=Wolfram |first1=Stephen |authorlink=Stephen Wolfram |title=[[A New Kind of Science]] |date=2002 |publisher=Wolfram Media |isbn=978-1579550080 }}</ref><ref name=mccune>{{citation\n | last1 = McCune | first1 = William |authorlink1 = William McCune\n | last2 = Veroff | first2 = Robert\n | last3 = Fitelson | first3 = Branden |authorlink3 = Branden Fitelson\n | last4 = Harris | first4 = Kenneth\n | last5 = Feist | first5 = Andrew\n | last6 = Wos | first6 = Larry |authorlink6 = Larry Wos\n | doi = 10.1023/A:1020542009983\n | issue = 1\n | journal = [[Journal of Automated Reasoning]]\n | mr = 1940227\n | pages = 1–16\n | title = Short single axioms for Boolean algebra\n | volume = 29\n | year = 2002}}</ref> [[MathWorld]], a site associated with Wolfram, has named the axiom the \"Wolfram axiom\".<ref>{{mathworld|id=WolframAxiom|title=Wolfram Axiom|author=Rowland, Todd; Weisstein, Eric W.}}</ref> McCune et al. also found a longer single axiom for Boolean algebra based on disjunction and negation.<ref name=mccune/>\n\nIn 1933, [[Edward Vermilye Huntington]] identified the axiom\n:<math>{\\neg({\\neg x} \\lor {y})} \\lor {\\neg({\\neg x} \\lor {\\neg y})} = x</math>\nas being equivalent to Boolean algebra, when combined with the commutativity of the [[Logical OR|OR]] operation, <math>x \\lor y = y \\lor x</math>, and the assumption of associativity, <math>(x \\lor y) \\lor z = x \\lor (y \\lor z)</math>.<ref>{{cite journal|last=Huntington |first=E. V. |title=New Sets of Independent Postulates for the Algebra of Logic, with Special Reference to Whitehead and Russell's ''Principia Mathematica'' |journal=[[Trans. Amer. Math. Soc.]] |volume=25 |pages=247–304 |year=1933}}</ref> [[Herbert Robbins]] conjectured that Huntington's axiom could be replaced by\n:<math>\\neg(\\neg(x \\lor y) \\lor \\neg(x \\lor {\\neg y})) = x,</math>\nwhich requires one fewer use of the logical negation operator <math>\\neg</math>. Neither Robbins nor Huntington could prove this conjecture; nor could [[Alfred Tarski]], who took considerable interest in it later. The conjecture was eventually proved in 1996 with the aid of [[automated theorem proving|theorem-proving software]].<ref>{{cite book|last1=Henkin |first1=Leon |authorlink1=Leon Henkin |last2=Monk |first2=J. Donald |last3=Tarski |first3=Alfred |authorlink3=Alfred Tarski |title=Cylindric Algebras, Part I |publisher=[[North-Holland Publishing Company|North-Holland]] |isbn=978-0-7204-2043-2 |year=1971 |oclc=1024041028}}</ref><ref>{{cite journal|last=McCune |first=William |title=Solution of the Robbins Problem |journal=[[Journal of Automated Reasoning]] |volume=19 |year=1997 |pages=263–276 |doi=10.1023/A:1005843212881}}</ref><ref>{{cite news|last=Kolata |first=Gina |authorlink=Gina Kolata |title=Computer Math Proof Shows Reasoning Power |work=[[The New York Times]] |date=1996-12-10 |url=https://archive.nytimes.com/www.nytimes.com/library/cyber/week/1210math.html}} For errata, see {{cite web|url=http://www.mcs.anl.gov/home/mccune/nyt-corrections.html |archive-url=https://web.archive.org/web/19970605011316/http://www.mcs.anl.gov/home/mccune/nyt-corrections.html |date=1997-01-23 |archive-date=1997-06-05 |last=McCune |first=William |website=[[Argonne National Laboratory]] |title=Comments on Robbins Story}}</ref> This proof established that the Robbins axiom, together with associativity and commutativity, form a 3-[[Basis (universal algebra)|basis]] for Boolean algebra. The existence of a 2-basis was established in 1967 by [[Carew Arthur Meredith]]:<ref>{{cite journal|last1=Meredith |first1=C. A. |authorlink1=Carew Arthur Meredith |last2=Prior |first2=A. N. |authorlink2=Arthur Prior |title=Equational logic |journal=[[Notre Dame J. Formal Logic]] |year=1968 |volume=9 |pages=212–226 |doi=10.1305/ndjfl/1093893457 |mr=0246753}}</ref>\n:<math>\\neg({\\neg x} \\lor y) \\lor x = x,</math>\n:<math>\\neg({\\neg x} \\lor {\\neg y}) \\lor (z \\lor y) = y \\lor (z \\lor x).</math>\nThe following year, Meredith found a 2-basis in terms of the Sheffer stroke:<ref>{{cite journal|last=Meredith |first=C. A. |authorlink1=Carew Arthur Meredith |title=Equational postulates for the Sheffer stroke |journal=[[Notre Dame J. Formal Logic]] |volume=10 |year=1969 |pages=266–270 |doi=10.1305/ndjfl/1093893713 |mr=0245423}}</ref>\n:<math>(x\\mid x) \\mid (y\\mid x) = x,</math>\n:<math>x|(y\\mid(x\\mid z)) = ((z\\mid y)\\mid y)\\mid x.</math>\n\nIn 1973, Padmanabhan and Quackenbush demonstrated a method that, in principle, would yield a 1-basis for Boolean algebra.<ref>{{cite journal |last1=Padmanabhan |first1=R. |last2=Quackenbush |first2=R. W. |title=Equational theories of algebras with distributive congruences |journal=[[Proc. Amer. Math. Soc.]] |date=1973 |volume=41 |pages=373-377 |doi=10.1090/S0002-9939-1973-0325498-2}}</ref> Applying this method in a straightforward manner yielded \"axioms of enormous length\",<ref name=mccune /> thereby prompting the question of how shorter axioms might be found. This search yielded the 1-basis in terms of the Sheffer stroke given above, as well as the 1-basis\n:<math>\\neg(\\neg(\\neg(x \\lor y) \\lor z) \\lor \\neg(x \\lor \\neg(\\neg z \\lor \\neg(z \\lor u)))) = z,</math>\nwhich is written in terms of OR and NOT.<ref name=mccune />\n\n==References==\n\n{{Reflist}}\n\n{{Logical connectives}}\n\n[[Category:Boolean algebra]]\n[[Category:History of logic]]\n[[Category:Logic gates|NAND gate]]\n[[Category:Propositional calculus]]"
    },
    {
      "title": "Modal algebra",
      "url": "https://en.wikipedia.org/wiki/Modal_algebra",
      "text": "In [[abstract algebra|algebra]] and [[logic]], a '''modal algebra''' is a structure <math>\\langle A,\\land,\\lor,-,0,1,\\Box\\rangle</math> such that\n*<math>\\langle A,\\land,\\lor,-,0,1\\rangle</math> is a [[Boolean algebra (structure)|Boolean algebra]],\n*<math>\\Box</math> is a unary operation on ''A'' satisfying <math>\\Box1=1</math> and <math>\\Box(x\\land y)=\\Box x\\land\\Box y</math> for all ''x'', ''y'' in ''A''.\nModal algebras provide models of [[propositional logic|propositional]] [[modal logic]]s in the same way as Boolean algebras are models of [[classical logic]]. In particular, the [[variety (universal algebra)|variety]] of all modal algebras is the equivalent algebraic semantics of the modal logic ''K'' in the sense of [[abstract algebraic logic]], and the [[lattice (order)|lattice]] of its subvarieties is dually [[isomorphic]] to the lattice of [[normal modal logic]]s.\n\n[[Stone's representation theorem]] can be generalized to the [[Jónsson–Tarski duality]], which ensures that each modal algebra can be [[representation theorem|represented]] as the algebra of admissible sets in a modal [[general frame]].\n\n==See also==\n*[[interior algebra]]\n*[[Heyting algebra]]\n\n==References==\nA. Chagrov and M. Zakharyaschev, ''Modal Logic'', Oxford Logic Guides vol. 35, Oxford University Press, 1997. {{ISBN|0-19-853779-4}}\n\n{{algebra-stub}}\n[[Category:Modal logic]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Monadic Boolean algebra",
      "url": "https://en.wikipedia.org/wiki/Monadic_Boolean_algebra",
      "text": "In [[abstract algebra]], a '''monadic Boolean algebra''' is an [[algebraic structure]] ''A'' with [[signature (logic)|signature]] \n\n:&lang;&middot;, +, ', 0, 1, &exist;&rang;   of [[signature (logic)|type]]   &lang;2,2,1,0,0,1&rang;,\n\nwhere &lang;''A'', &middot;, +, ', 0, 1&rang; is a [[Boolean algebra (structure)|Boolean algebra]].\n\nThe [[monadic (arity)|monadic]]/[[unary operator]] &exist; denotes the [[existential quantifier]], which satisfies the identities (using the received [[prefix]] notation for ∃):\n* &exist;0 = 0\n* &exist;''x'' &ge; ''x''\n* &exist;(''x'' + ''y'') = &exist;''x'' + &exist;''y''\n* &exist;''x''&exist;''y'' = &exist;(''x''&exist;''y''). \n&exist;''x'' is the ''existential closure'' of ''x''. [[duality (order theory)|Dual]] to &exist; is the [[unary operator]] &forall;, the [[universal quantifier]], defined as &forall;''x'' := (&exist;''x' '')'. \n\nA monadic Boolean algebra has a dual definition and notation that take &forall; as primitive and &exist; as defined, so that  &exist;''x'' := (&forall;''x''&nbsp;'&nbsp;)'&nbsp;. (Compare this with the definition of the [[duality (order theory)|dual]] Boolean algebra.) Hence, with this notation, an algebra ''A'' has signature &lang;&middot;, +, ', 0, 1, &forall;&rang;, with &lang;''A'', &middot;, +, ', 0, 1&rang; a Boolean algebra, as before. Moreover, &forall; satisfies the following [[duality (order theory)| dualized]] version of the above identities:\n# &forall;1 = 1\n# &forall;''x'' &le; ''x''\n# &forall;(''xy'') = &forall;''x''&forall;''y''\n# &forall;''x'' + &forall;''y'' = &forall;(''x'' + &forall;''y'').\n&forall;''x'' is the ''universal closure'' of ''x''.\n\n==Discussion==\nMonadic Boolean algebras have an important connection to [[topology]]. If &forall; is interpreted as the [[interior operator]] of topology, (1)–(3) above plus the axiom &forall;(&forall;''x'') = &forall;''x'' make up the axioms for an [[interior algebra]]. But &forall;(&forall;''x'') = &forall;''x'' can be proved from (1)–(4). Moreover, an alternative axiomatization of monadic Boolean algebras consists of the (reinterpreted) axioms for an [[interior algebra]], plus &forall;(&forall;''x'')' = (&forall;''x'')' (Halmos 1962: 22). Hence monadic Boolean algebras are the [[Semisimple algebra|semisimple]] interior/[[closure algebra]]s such that:\n*The universal (dually, existential) quantifier interprets the [[interior operator|interior]] ([[closure operator|closure]]) operator;\n*All open (or closed) elements are also [[clopen set|clopen]].\nA more concise axiomatization of monadic Boolean algebra is (1) and (2) above, plus &forall;(''x''&or;&forall;''y'') = &forall;''x''&or;&forall;''y'' (Halmos 1962: 21). This axiomatization obscures the connection to topology.\n\nMonadic Boolean algebras form a [[variety (universal algebra)|variety]]. They are to [[monadic logic|monadic predicate logic]] what [[Lindenbaum–Tarski algebra|Boolean algebra]]s are to [[propositional logic]], and what [[polyadic algebra]]s are to [[first-order logic]]. [[Paul Halmos]] discovered monadic Boolean algebras while working on polyadic algebras; Halmos (1962) reprints the relevant papers. Halmos and Givant (1998) includes an undergraduate treatment of monadic Boolean algebra.\n\nMonadic Boolean algebras also have an important connection to [[modal logic]]. The modal logic [[modal logic|S5]], viewed as a theory in ''S4'', is a model of monadic Boolean algebras in the same way that [[modal logic|S4]] is a model of interior algebra. Likewise, monadic Boolean algebras supply the algebraic semantics for ''S5''. Hence '''S5-algebra''' is a [[synonym]] for monadic Boolean algebra.\n\n==See also==\n*[[clopen set]]\n*[[cylindric algebra]]\n*[[interior algebra]]\n*[[Kuratowski closure axioms]]\n*[[Łukasiewicz–Moisil algebra]]\n*[[modal logic]]\n*[[monadic logic]]\n\n==References==\n* [[Paul Halmos]], 1962. ''Algebraic Logic''. New York: Chelsea.\n* ------ and Steven Givant, 1998. ''Logic as Algebra''. Mathematical Association of America.\n\n[[Category:Algebraic logic]]\n[[Category:Boolean algebra]]\n[[Category:Closure operators]]\n\n{{logic-stub}}"
    },
    {
      "title": "Parity function",
      "url": "https://en.wikipedia.org/wiki/Parity_function",
      "text": "In [[Boolean algebra (logic)|Boolean algebra]],  a '''parity function''' is a [[Boolean function]] whose value is 1 [[if and only if]] the input vector has an odd number of ones. The parity function of two inputs is also known as the [[XOR gate|XOR]] function.\n\nThe parity function is notable for its role in theoretical investigation of [[circuit complexity]] of Boolean functions. \n\nThe output of the Parity Function is the [[Parity bit]]. \n\n==Definition==\nThe <math>n</math>-variable parity function is the [[Boolean function]] <math>f:\\{0,1\\}^n\\to\\{0,1\\}</math> with the property that <math>f(x)=1</math> [[if and only if]] the number of ones in the vector <math>x\\in\\{0,1\\}^n</math> is odd.\nIn other words, <math>f</math> is defined as follows:\n:<math>f(x)=x_1\\oplus x_2 \\oplus \\dots \\oplus x_n</math>.\n\n==Properties==\nParity only depends on the number of ones and is therefore a [[symmetric Boolean function]].\n\nThe ''n''-variable parity function and its negation are the only Boolean functions for which all [[disjunctive normal form]]s have the maximal number of 2<sup>&nbsp;''n''&nbsp;&minus;&nbsp;1</sup> [[monomial]]s of length ''n'' and all [[conjunctive normal form]]s have  the maximal number of 2<sup>&nbsp;''n''&nbsp;&minus;&nbsp;1</sup> clauses of length ''n''.<ref name=wp> [[Ingo Wegener]], Randall J. Pruim, ''Complexity Theory'', 2005, {{isbn|3-540-21045-8}}, [https://books.google.com/books?id=u7DZSDSUYlQC&pg=PA261&lpg=PA261&dq=%22parity+function%22+H%C3%A5stad&source=bl&ots=HNQ-Jx67yy&sig=qOg_lAiE3JbqsDdQO0rrmgxJmDs&hl=en&ei=U9PjSfGYDYjaswOCkNysCQ&sa=X&oi=book_result&ct=result&resnum=3#PPA260,M1 p. 260]</ref>\n\n==Computational complexity==\nSome of the earliest work in computational complexity was 1961 bound of [[Bella Subbotovskaya]] showing the size of a [[Boolean expression|Boolean formula]] computing parity must be at least <math>O(n^{3/2})</math>. This work uses the method of random restrictions. This exponent of <math>3/2</math> has been increased through careful analysis to <math>1.63</math> by [[Mike Paterson|Paterson]] and [[Uri Zwick|Zwick]] (1993) and then to <math>2</math> by [[Johan Håstad|Håstad]] (1998). <ref name=\"jukna\">{{cite book|last1=Jukna|first1=Stasys|title=Boolean Function Complexity: Advances and Frontiers|date=Jan 6, 2012|publisher=Springer Science & Business Media|isbn=978-3642245084|pages=167–173}}</ref>\n\nIn the early 1980s, [[Merrick L. Furst|Merrick Furst]], [[James Saxe]] and [[Michael Sipser]]<ref name=FSS>Merrick Furst, James Saxe and Michael Sipser, \"Parity, Circuits, and the Polynomial-Time Hierarchy\", Annu. Intl. Symp. Found.Coimputer Sci., 1981, ''[[Theory of Computing Systems]]'', vol. 17, no. 1, 1984, pp. 13&ndash;27, {{doi|10.1007/BF01744431}}</ref>  and independently [[Miklós Ajtai]]<ref>Miklós Ajtai, \"<math>\\Sigma^1_1</math>-Formulae on Finite Structures\", ''[[Annals of Pure and Applied Logic]]'', 24 (1983) 1&ndash;48.</ref> established super-polynomial [[lower bound]]s on the size of constant-depth [[Boolean circuits]] for the parity function, i.e., they showed that polynomial-size constant-depth circuits cannot compute the parity function. Similar results were also established for the majority, multiplication and transitive closure functions, by reduction from the parity function.<ref name=FSS/>\n\n{{harvtxt|Håstad|1987}} established tight exponential lower bounds on the size of constant-depth [[Boolean circuits]] for the parity function. [[Håstad's Switching Lemma]] is the key technical tool used for these lower bounds and [[Johan Håstad]] was awarded the [[Gödel Prize]] for this work in 1994.\nThe precise result is that depth-{{mvar|k}} circuits with AND, OR, and NOT gates require size <math>\\exp(\\Omega(n^{\\frac{1}{k-1}}))</math> to compute the parity function.\nThis is asymptotically almost optimal as there are depth-{{mvar|k}} circuits computing parity which have size <math>\\exp(O(n^{\\frac{1}{k-1}})t)</math>.\n\n==Infinite version==\nAn infinite parity function is a function <math>f\\colon \\{0, 1\\}^{\\omega} \\to \\{0, 1\\}</math> mapping every infinite binary string to 0 or 1, having the following property: if <math>w</math> and <math>v</math> are infinite binary strings differing only on finite number of coordinates then <math>f(w) = f(v)</math> if and only if <math>w</math> and <math>v</math> differ on even number of coordinates.\n\nAssuming [[axiom of choice]] it can be easily proved that parity functions exist and there are <math>2^{\\mathfrak{c}}</math> many of them - as many as the number of all functions from <math>\\{0, 1\\}^{\\omega}</math> to <math>\\{0, 1\\}</math>. It is enough to take one representative per equivalence class of relation <math>\\approx</math> defined as follows: <math>w \\approx v</math> if <math>w</math> and <math>v</math> differ at finite number of coordinates. Having such representatives, we can map all of them to 0; the rest of <math>f</math> values are deducted unambiguously.\n\nInfinite parity functions are often used in theoretical Computer Science and Set Theory because of their simple definition and - on the other hand - their descriptive complexity. For example, it can be shown that an inverse image <math>f^{-1}[0]</math> is a [[non-Borel set]].\n\n== See also ==\nRelated topics\n*[[Error Correction]]\n*[[Error Detection]]\n\nThe output of the function\n*[[Parity bit]]\n\n==References==\n{{refbegin}}\n{{reflist}}\n* {{citation|first=Johan|last=Håstad|authorlink=Johan Håstad|title=Computational limitations of small depth circuits|year=1987|publisher=Ph.D. thesis, Massachusetts Institute of Technology|url=http://www.nada.kth.se/~johanh/thesis.pdf|ref=harv|postscript=.}}\n{{refend}}\n\n[[Category:Boolean algebra]]\n[[Category:Circuit complexity]]\n[[Category:Functions and mappings]]"
    },
    {
      "title": "Petrick's method",
      "url": "https://en.wikipedia.org/wiki/Petrick%27s_method",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\nIn [[Boolean algebra (logic)|Boolean algebra]], '''Petrick's method''' (also known as the ''branch-and-bound'' method) is a technique described by [[Stanley R. Petrick]] (1931<!-- *1931-08-16 or 1931-08-06 Cedar Rapids, Iowa -->–2006<!-- +2006-07-27 Denver, Colorado -->)<ref name=\"Petrick_Bio\"/><ref name=\"Petrick_Obit\"/> in 1956<ref name=\"Petrick_1956\"/> for determining all minimum sum-of-products solutions from a [[Quine–McCluskey algorithm|prime implicant chart]]. Petrick's method is very tedious for large charts, but it is easy to implement on a computer.\n\n==Algorithm==\n# Reduce the prime implicant chart by eliminating the essential prime implicant rows and the corresponding columns.\n# Label the rows of the reduced prime implicant chart <math>P_1</math>, <math>P_2</math>, <math>P_3</math>, <math>P_4</math>, etc.\n# Form a logical function <math>P</math> which is true when all the columns are covered. ''P'' consists of a product of sums where each sum term has the form <math>(P_{i0} + P_{i1} + </math><math>\\cdots</math><math> + P_{iN})</math>, where each <math>P_{ij}</math> represents a row covering column <math>i</math>.\n# Reduce <math>P</math> to a minimum sum of products by multiplying out and applying <math>X + XY = X</math>.\n# Each term in the result represents a solution, that is, a set of rows which covers all of the minterms in the table. To determine the minimum solutions, first find those terms which contain a minimum number of prime implicants.\n# Next, for each of the terms found in step five, count the number of literals in each prime implicant and find the total number of literals.\n# Choose the term or terms composed of the minimum total number of literals, and write out the corresponding sums of prime implicants.\n\n==Example of Petrick's method==\n\nFollowing is the function we want to reduce:<ref name=\"Example\"/>\n\n:<math>f(A,B,C) =\\sum m(0,1,2,5,6,7)\\,</math>\n\nThe prime implicant chart from the [[Quine-McCluskey algorithm]] is as follows:\n\n                 | 0 1 2 5 6 7\n  ---------------|------------\n    K (0,1) a'b' | X X\n    L (0,2) a'c' | X   X\n    M (1,5) b'c  |   X   X\n    N (2,6) bc'  |     X   X\n    P (5,7) ac   |       X   X\n    Q (6,7) ab   |         X X\n\nBased on the X marks in the table above, build a product of sums of the rows where each row is added, and columns are multiplied together:\n\n  (K+L)(K+M)(L+N)(M+P)(N+Q)(P+Q)\n\nUse the distributive law to turn that expression into a sum of products.  Also use the following equivalences to simplify the final expression:     X + XY = X         and         XX = X       and       X+X=X\n\n  = (K+L)(K+M)(L+N)(M+P)(N+Q)(P+Q)\n  = (K+LM)(N+LQ)(P+MQ)\n  = (KN+KLQ+LMN+LMQ)(P+MQ)\n  = KNP + KLPQ + LMNP + LMPQ + KMNQ + KLMQ + LMNQ + LMQ\n\nNow use again the following equivalence to further reduce the equation:    X + XY = X\n\n  = KNP + KLPQ + LMNP + LMQ + KMNQ\n\nChoose products with fewest terms, in this example, there are two products with three terms:\n\n  KNP\n  LMQ\n\nChoose term or terms with fewest total literals. In our example, the two products both expand to six literals total each:\n\n KNP    expands to    a'b' + bc' + ac\n LMQ    expands to    a'c' + b'c + ab\n\nSo either one can be used. In general, application of Petrick's method is tedious for large charts, but it is easy to implement on a computer.\n\n==References==\n{{reflist|refs=\n<ref name=\"Petrick_1956\">{{cite book |author-last=Petrick |author-first=Stanley R. |date=1956-04-10 |title=A Direct Determination of the Irredundant Forms of a Boolean Function from the Set of Prime Implicants |id=AFCRC Technical Report TR-56-110 |publisher=[[Air Force Cambridge Research Center]] |location=Bedford, Cambridge, MA, USA}}</ref>\n<ref name=\"Petrick_Bio\">{{cite web |author=(Unknown) |title=Biographical note |url=https://i.stack.imgur.com/3GGYU.png |access-date=2017-04-12 |quote=Stanley R. Petrick was born in Cedar Rapids, Iowa on August 16, 1931. He attended the Roosevelt High School and received a B. S. degree in Mathematics from the [[Iowa State University]] in 1953. During 1953 to 1955 he attended MIT while on active duty as an Air Force officer and received the S. M. degree from the Department of Electrical Engineering in 1955. He was elected to [[Sigma Xi]] in 1955.<br/>Mr. Petrick has been associated with the Applied Mathematics Board of the Data Sciences Laboratory at the [[Air Force Cambridge Research Laboratories]] since 1955 and his recent studies at [[MIT]] have been partially supported by AFCRL. During 1959-1962 he held the position of Lecturer in Mathematics in the Evening Graduate Division of [[Northeastern University]].<br/>Mr. Petrick is currently a member of the [[Linguistic Society of America]], [[The Linguistic Circle of New York]], The American Mathematical Association, [[The Association for Computing Machinery]], and the [[Association for Machine Translation and Computational Linguistics]].}}</ref>\n<ref name=\"Petrick_Obit\">{{cite web |title=Obituaries - Cedar Rapids - Stanley R. Petrick |publisher=[[The Gazette (Cedar Rapids)|The Gazette]] |date=2006-08-05 |url=https://newspaperarchive.com/cedar-rapids-gazette-aug-05-2006-p-16/ |access-date=2017-04-12 |page=16 |quote=[…] CEDAR RAPIDS Stanley R. Petrick, 74, formerly of Cedar Rapids, died July 27, 2006, in Presbyterian/St. Luke's Hospital, Denver, Colo., following a 13-year battle with leukemia. A memorial service will be held Aug. 14 at the United Presbyterian Church in Laramie, Wyo., where he lived for many years. <!-- <br/>Survivors include his wife, Mary Ethel, his daughters, Susan Petrick, Ellen Petrick, Cathleen Petrick and husband Paul Zeitz, and Jane Lavino and husband Ed; six grandchildren; his mother Catherine Dawson; his sisters, Evelyn Clifford, Pam Colgate, Joan Green and Janice Carson; and his brother, John Elliott.<br/>He was preceded in death by his father, Fred Petrick (Mombo), and his stepmother, Emma Elliott Petrick.<br/> -->[…] Stan Petrick was born in Cedar Rapids on Aug. 6, 1931 to Catherine Hunt Petrick and Fred Petrick. He graduated from Roosevelt High School in 1949 and received a B.S. degree in mathematics from [[Iowa State University]]. Stan married Mary Ethel Buxton in 1953.<br/>He joined the U.S. Air Force and was assigned as a student officer studying digital computation at [[MIT]], where he earned an M.S. degree. He was then assigned to the Applied Mathematics Branch of the [[Air Force Cambridge Research Laboratory]] and while there earned a Ph.D. in linguistics.<br/>He spent 20 years in the Theoretical and Computational Linguistics Group of the Mathematical Sciences Department at [[IBM]]'s [[T.J. Watson Research Center]], conducting research in formal language theory. He had served as an assistant director of the Mathematical Sciences Department, chairman of the Special Interest Group on Symbolic and Algebraic Manipulation of the [[Association for Computing Machinery]] and president of the [[Association for Computational Linguistics]]. He authored many technical publications.<br/>He taught three years at [[Northeastern University]] and 13 years at the Pratt Institute. Dr. Petrick joined the [[University of Wyoming]] in 1987, where he was instrumental in developing and implementing the Ph.D. program in the department and served as a thesis adviser for many graduate students. He retired in 1995. <!-- <br/>Stan served as an elder and choir member in the Presbyterian Church, was a former treasurer of the University of Wyoming Symphony Association Board, and was a member of [[Sigma Xi]], [[Phi Eta Sigma]], [[Pi Mu Epsilon]] and [[Phi Kappa Phi]] honorary societies. He was a distance runner in both cross country and track in high school and college. He continued to run for fitness for many years and also loved tennis and table tennis. -->[…]}} (NB. Includes a photo of Stanley R. Petrick.)</ref>\n<ref name=\"Example\">http://www.mrc.uidaho.edu/mrc/people/jff/349/lect.10 Lecture #10: Petrick's Method<!-- https://web.archive.org/web/20170412071126/http://www.mrc.uidaho.edu/mrc/people/jff/349/lect.10 --></ref>\n}}\n\n==Further reading==\n* Roth, Jr., Charles H. ''Fundamentals of Logic Design''\n\n==External links==\n* [https://sites.google.com/site/simpogical/download/qm.pdf<!-- https://d403fe19-a-62cb3a1a-s-sites.googlegroups.com/site/simpogical/download/qm.pdf --> Tutorial on Quine-McCluskey and Petrick's method]<!-- https://web.archive.org/web/20170412071609/https://d403fe19-a-62cb3a1a-s-sites.googlegroups.com/site/simpogical/download/qm.pdf?attachauth=ANoY7cqOUBAsLO6wiVSQKsBj0PGW5k-UaTWzufvjC0SUONIEnUE_ryij-m2BfvpSVWnKIX5c1rWlsESb0duGmCWcm0EN8o4me1ABMVlQEeru5-UuWNnEoTEnJcAPAMf9YBtKc_9-C_2EI4N0mE5kUfxiqdoLOS20oS7VLDNQPzbZA6WLetW25rG17B1voUQBZekKpQ3_cpz51xd9sxOH_YlswqUm4htfpg%3D%3D&attredirects=0&d=1 -->\n* [http://www.allaboutcircuits.com/technical-articles/prime-implicant-simplification-using-petricks-method/ Prime Implicant Simplification Using Petrick’s Method]<!-- 2016-02-17 Donald Krambeck https://web.archive.org/web/20170412071958/https://www.allaboutcircuits.com/technical-articles/prime-implicant-simplification-using-petricks-method/ -->\n\n{{DEFAULTSORT:Petrick's Method}}\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Poretsky's law of forms",
      "url": "https://en.wikipedia.org/wiki/Poretsky%27s_law_of_forms",
      "text": "In [[Boolean algebra]], '''Poretsky's law of forms''' shows that the single Boolean equation <math>f(X)=0</math> is equivalent to <math>g(X)=h(X)</math> if and only if <math>g=f\\oplus h</math>, where <math>\\oplus</math> represents [[exclusive or]].\n\nThe law of forms was discovered by [[Platon Poretsky]].\n\n==References==\n\n* Frank Markham Brown, ''Boolean Reasoning: The Logic of Boolean Equations'', 2nd edition, 2003, p.&nbsp;100\n* Louis Couturat, ''The Algebra Of Logic'', 1914, p.&nbsp;53, section 0.43\n* Clarence Irving Lewis, ''A Survey of Symbolic Logic'', 1918, p.&nbsp;145, section 7.15\n\n==External links==\n* [http://www.transhumanreflections.org/boolean_tools.html \"Transhuman Reflections - Poretsky Form to Solve\"]\n\n\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Product term",
      "url": "https://en.wikipedia.org/wiki/Product_term",
      "text": "In [[Boolean logic]], a '''product term''' is a conjunction of literals, where each literal is\neither a variable or its negation.\n\n==Examples==\nExamples of product terms include:\n\n:<math>A \\wedge B</math>\n:<math>A \\wedge (\\neg B) \\wedge (\\neg C)</math>\n:<math>\\neg A</math>\n\n==Origin==\nThe terminology comes from the similarity of AND\nto multiplication as in the ring structure of [[Boolean ring]]s.\n\n==Minterms==\nFor a [[boolean function]] of <math>n</math> variables <math>{x_1,\\dots,x_n}</math>, a product term in which each of the <math>n</math> variables appears '''once''' (in either its complemented or uncomplemented form) is called a ''minterm''. Thus, a ''minterm'' is a logical expression of ''n'' variables that employs only the ''complement'' operator and the ''conjunction'' operator.\n\n==References==\n*Fredrick J. Hill, and Gerald R. Peterson, 1974, ''Introduction to Switching Theory and Logical Design, Second Edition'', John Wiley & Sons, NY, {{isbn|0-471-39882-9}}\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Propositional calculus",
      "url": "https://en.wikipedia.org/wiki/Propositional_calculus",
      "text": "{{short description|Logical study of propositions (whether they are true or false) that are formed by other propositions with the use of logical connectives}}\n{{Distinguish|Propositional analysis}}\n{{Use dmy dates|date=May 2012}}\n{{Transformation rules}}\n'''Propositional calculus''' is a branch of [[logic]].  It is also called '''propositional logic''', '''statement logic''', '''sentential calculus''', '''sentential logic''', or sometimes '''[[zeroth-order logic]]'''. It deals with [[propositions]] (which can be true or false) and argument flow. Compound propositions are formed by connecting propositions by [[logical connective]]s. The propositions without logical connectives are called atomic propositions.\n\nUnlike [[first-order logic]], propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. However, all the machinery of propositional logic is included in first-order logic and higher-order logics. In this sense, propositional logic is the foundation of first-order logic and higher-order logic.\n\n==Explanation==\nLogical connectives are found in natural languages. In English for example, some examples are \"and\" ([[logical conjunction|conjunction]]), \"or\" ([[logical disjunction|disjunction]]),  \"not” ([[negation]]) and \"if\" (but only when used to denote [[material conditional]]).\n\nThe following is an example of a very simple inference within the scope of propositional logic:\n\n:Premise 1: If it's raining then it's cloudy.\n:Premise 2: It's raining.\n:Conclusion: It's cloudy.\n\nBoth premises and the conclusion are propositions. The premises are taken for granted and then with the application of [[modus ponens]] (an [[inference rule]]) the conclusion follows.\n\nAs propositional logic is not concerned with the structure of propositions beyond the point where they can't be decomposed any more by logical connectives, this inference can be restated replacing those ''atomic'' statements with statement letters, which are interpreted as variables representing statements:\n\n:Premise 1: <math>P \\to Q</math>\n:Premise 2: <math>P</math>\n:Conclusion: <math>Q</math>\n\nThe same can be stated succinctly in the following way:\n\n:<math>P \\to Q, P \\vdash Q</math>\n\nWhen {{mvar|P}} is interpreted as “It's raining” and {{mvar|Q}} as “it's cloudy” the above symbolic expressions can be seen to exactly correspond with the original expression in natural language. Not only that, but they will also correspond with any other inference of this ''form'', which will be valid on the same basis that this inference is.\n\nPropositional logic may be studied through a [[formal system]] in which [[well-formed formula|formulas]] of a [[formal language]] may be [[interpretation (logic)|interpreted]] to represent [[propositions]]. A [[deductive system|system]] of [[rule of inference|inference rules]] and [[axiom]]s allows certain formulas to be derived. These derived formulas are called [[theorem]]s and may be interpreted to be true propositions. A constructed sequence of such formulas is known as a ''[[formal proof|derivation]]'' or ''proof'' and the last formula of the sequence is the theorem. The derivation may be interpreted as proof of the proposition represented by the theorem.\n\nWhen a [[formal system]] is used to represent formal logic, only statement letters are represented directly. The natural language propositions that arise when they're interpreted are outside the scope of the system, and the relation between the formal system and its interpretation is likewise outside the formal system itself.\n\nIn classical '''truth-functional propositional logic''', formulas are interpreted as having precisely one of two possible [[truth value|truth values]], the truth value of ''true'' or the truth value of ''false''. The [[principle of bivalence]] and the [[law of excluded middle]] are upheld. Truth-functional propositional logic defined as such and systems [[isomorphism|isomorphic]] to it are considered to be '''[[zeroth-order logic]]'''. However, alternative propositional logics are possible. See [[Propositional_calculus#Alternative_calculus|Other logical calculi]] below.\n\n==History==\n{{main|History of logic}}\nAlthough propositional logic (which is interchangeable with propositional calculus) had been hinted by earlier philosophers, it was developed into a formal logic ([[Stoic logic]]) by [[Chrysippus]] in the 3rd century BC<ref>{{cite book|url=http://plato.stanford.edu/archives/spr2016/entries/logic-ancient/|title=The Stanford Encyclopedia of Philosophy|first=Susanne|last=Bobzien|editor-first=Edward N.|editor-last=Zalta|date=1 January 2016|publisher=|via=Stanford Encyclopedia of Philosophy}}</ref> and expanded by his successor [[Stoics]]. The logic was focused on [[proposition]]s. This advancement was different from the traditional [[syllogism|syllogistic logic]] which was focused on [[Syllogisms#Terms in syllogism|terms]]. However, later in antiquity, the propositional logic developed by the Stoics was no longer understood {{Who|date=October 2014}}. Consequently, the system was essentially reinvented by [[Peter Abelard]] in the 12th century.<ref>{{cite book |title=Medieval philosophy: an historical and philosophical introduction |last=Marenbon |first=John |year=2007 |publisher=Routledge |isbn= |page=137}}</ref>\n\nPropositional logic was eventually refined using [[symbolic logic]]. The 17th/18th-century mathematician [[Gottfried Leibniz]] has been credited with being the founder of symbolic logic for his work with the [[calculus ratiocinator]]. Although his work was the first of its kind, it was unknown to the larger logical community. Consequently, many of the advances achieved by Leibniz were recreated by logicians like [[George Boole]] and [[Augustus De Morgan]] completely independent of Leibniz.<ref>{{cite book|url=http://plato.stanford.edu/archives/spr2014/entries/leibniz-logic-influence/|title=The Stanford Encyclopedia of Philosophy|first=Volker|last=Peckhaus|editor-first=Edward N.|editor-last=Zalta|date=1 January 2014|publisher=|via=Stanford Encyclopedia of Philosophy}}</ref>\n\nJust as propositional logic can be considered an advancement from the earlier syllogistic logic, [[Gottlob Frege|Gottlob Frege's]] [[predicate logic]] was an advancement from the earlier propositional logic. One author describes predicate logic as combining \"the distinctive features of syllogistic logic and propositional logic.\"<ref>{{cite book |title=A Concise Introduction to Logic 10th edition |last=Hurley |first=Patrick |year=2007 |publisher=Wadsworth Publishing |page=392 }}</ref> Consequently, predicate logic ushered in a new era in logic's history; however, advances in propositional logic were still made after Frege, including [[natural deduction|Natural Deduction]], [[Method of analytic tableaux|Truth-Trees]] and [[truth-table|Truth-Tables]]. Natural deduction was invented by [[Gerhard Gentzen]] and [[Jan Łukasiewicz]]. Truth-Trees were invented by [[Evert Willem Beth]].<ref>Beth, Evert W.; \"Semantic entailment and formal derivability\", series: Mededlingen van de Koninklijke Nederlandse Akademie van Wetenschappen, Afdeling Letterkunde, Nieuwe Reeks, vol. 18, no. 13, Noord-Hollandsche Uitg. Mij., Amsterdam, 1955, pp. 309–42. Reprinted in Jaakko Intikka (ed.) ''The Philosophy of Mathematics'', Oxford University Press, 1969</ref> The invention of truth-tables, however, is of uncertain attribution.\n\nWithin works by Frege<ref name=\"Truth in Frege\">[https://web.archive.org/web/20120807235445/http://frege.brown.edu/heck/pdf/unpublished/TruthInFrege.pdf Truth in Frege]</ref> and [[Bertrand Russell]],<ref name=\"Russell Truth-Tables\">{{cite web|url=http://digitalcommons.mcmaster.ca/cgi/viewcontent.cgi?article=1219&context=russelljournal|title=Russell: the Journal of Bertrand Russell Studies|publisher=}}</ref> are ideas influential to the invention of truth tables. The actual tabular structure (being formatted as a table), itself, is generally credited to either [[Ludwig Wittgenstein]] or [[Emil Post]] (or both, independently).<ref name=\"Truth in Frege\" /> Besides Frege and Russell, others credited with having ideas preceding truth-tables include Philo, Boole, [[Charles Sanders Peirce]]<ref>{{cite journal|last1=Anellis|first1=Irving H.|title=Peirce's Truth-functional Analysis and the Origin of the Truth Table|journal=History and Philosophy of Logic|date=2012|volume=33|pages=87–97|doi=10.1080/01445340.2011.621702}}</ref>, and [[Ernst Schröder]]. Others credited with the tabular structure include [[Jan Łukasiewicz]], [[Ernst Schröder]], [[Alfred North Whitehead]], [[William Stanley Jevons]], [[John Venn]], and [[Clarence Irving Lewis]].<ref name=\"Russell Truth-Tables\" />  Ultimately, some have concluded, like John Shosky, that \"It is far from clear that any one person should be given the title of 'inventor' of truth-tables.\".<ref name=\"Russell Truth-Tables\" />\n\n==Terminology==\nIn general terms, a calculus is a [[formal system]] that consists of a set of syntactic expressions (''[[well-formed formula]]s''), a distinguished subset of these expressions (axioms), plus a set of formal rules that define a specific [[binary relation]], intended to be interpreted as [[logical equivalence]], on the space of expressions.\n\nWhen the formal system is intended to be a [[logical system]], the expressions are meant to be interpreted as statements, and the rules, known to be ''inference rules'', are typically intended to be truth-preserving. In this setting, the rules (which may include [[axioms]]) can then be used to derive (\"infer\") formulas representing true statements from given formulas representing true statements.\n\nThe set of axioms may be empty, a nonempty finite set, a countably infinite set, or be given by [[axiom schema]]ta. A [[formal grammar]] recursively defines the expressions and well-formed formulas of the [[formal language|language]]. In addition a [[semantics]] may be given which defines truth and [[valuation (logic)|valuation]]s (or [[interpretation (logic)|interpretations]]).\n\nThe [[formal language|language]] of a propositional calculus consists of\n# a set of primitive symbols, variously referred to as ''[[atomic formula]]s'', ''placeholders'', ''proposition letters'', or ''variables'', and\n# a set of operator symbols, variously interpreted as ''[[logical operator]]s'' or ''logical connectives''.\n\nA ''[[well-formed formula]]'' is any atomic formula, or any formula that can be built up from atomic formulas by means of operator symbols according to the rules of the grammar.\n\nMathematicians sometimes distinguish between propositional constants, propositional variables, and schemata.  Propositional constants represent some particular proposition, while propositional variables range over the set of all atomic propositions.  Schemata, however, range over all propositions. It is common to represent propositional constants by {{mvar|A}}, {{mvar|B}}, and {{mvar|C}}, propositional variables by {{mvar|P}}, {{mvar|Q}}, and {{mvar|R}}, and schematic letters are often Greek letters, most often {{mvar|φ}}, {{mvar|ψ}}, and {{mvar|χ}}.\n\n==Basic concepts==\nThe following outlines a standard propositional calculus. Many different formulations exist which are all more or less equivalent but differ in the details of:\n# their language, that is, the particular collection of primitive symbols and operator symbols,\n# the set of axioms, or distinguished formulas, and\n# the set of inference rules.\n\nAny given proposition may be represented with a letter called a 'propositional constant', analogous to representing a number by a letter in mathematics, for instance, {{math|''a'' {{=}} 5}}. All propositions require exactly one of two truth-values:  true or false. For example, let {{mvar|P}} be the proposition that it is raining outside.  This will be true ({{mvar|P}}) if it is raining outside and false otherwise ({{math|¬''P''}}).\n\n*We then define [[truth-functional]] operators, beginning with negation.  {{math|¬''P''}} represents the negation of {{mvar|P}}, which can be thought of as the denial of {{mvar|P}}.  In the example above, {{math|¬''P''}} expresses that it is not raining outside, or by a more standard reading: \"It is not the case that it is raining outside.\"  When {{mvar|P}} is true, {{math|¬''P''}} is false; and when {{mvar|P}} is false, {{math|¬''P''}} is true. {{math|¬¬''P''}} always has the same truth-value as {{mvar|P}}.\n*Conjunction is a truth-functional connective which forms a proposition out of two simpler propositions, for example, {{mvar|P}} and {{mvar|Q}}.  The conjunction of {{mvar|P}} and {{mvar|Q}} is written {{math|''P'' ∧ ''Q''}}, and expresses that each are true.  We read {{math|''P'' ∧ ''Q''}} for \"{{mvar|P}} and {{mvar|Q}}\".  For any two propositions, there are four possible assignments of truth values:\n*# {{mvar|P}} is true and {{mvar|Q}} is true\n*# {{mvar|P}} is true and {{mvar|Q}} is false\n*# {{mvar|P}} is false and {{mvar|Q}} is true\n*# {{mvar|P}} is false and {{mvar|Q}} is false\n:The conjunction of {{mvar|P}} and {{mvar|Q}} is true in case 1 and is false otherwise.  Where {{mvar|P}} is the proposition that it is raining outside and {{mvar|Q}} is the proposition that a cold-front is over Kansas, {{math|''P'' ∧ ''Q''}} is true when it is raining outside and there is a cold-front over Kansas.  If it is not raining outside, then {{mvar|P ∧ Q}} is false; and if there is no cold-front over Kansas, then {{math|''P'' ∧ ''Q''}} is false.\n*Disjunction resembles conjunction in that it forms a proposition out of two simpler propositions.  We write it {{math|''P'' ∨ ''Q''}}, and it is read \"{{mvar|P}} or {{mvar|Q}}\".  It expresses that either {{mvar|P}} or {{mvar|Q}} is true.  Thus, in the cases listed above, the disjunction of {{mvar|P}} with {{mvar|Q}} is true in all cases except case 4.  Using the example above, the disjunction expresses that it is either raining outside or there is a cold front over Kansas.  (Note, this use of disjunction is supposed to resemble the use of the English word \"or\".  However, it is most like the English [[inclusive disjunction|inclusive]] \"or\", which can be used to express the truth of at least one of two propositions.  It is not like the English [[exclusive disjunction|exclusive]] \"or\", which expresses the truth of exactly one of two propositions.  That is to say, the exclusive \"or\" is false when both {{mvar|P}} and {{mvar|Q}} are true (case 1).  An example of the exclusive or is:  You may have a bagel or a pastry, but not both. Often in natural language, given the appropriate context, the addendum \"but not both\" is omitted but implied. In mathematics, however, \"or\" is always inclusive or; if exclusive or is meant it will be specified, possibly by \"xor\".)\n*Material conditional also joins two simpler propositions, and we write {{math|''P'' → ''Q''}}, which is read \"if {{mvar|P}} then {{mvar|Q}}\".  The proposition to the left of the arrow is called the antecedent and the proposition to the right is called the consequent.  (There is no such designation for conjunction or disjunction, since they are commutative operations.)  It expresses that {{mvar|Q}} is true whenever {{mvar|P}} is true.  Thus {{math|''P'' → ''Q''}} is true in every case above except case 2, because this is the only case when {{mvar|P}} is true but {{mvar|Q}} is not. Using the example, if {{mvar|P}} then {{mvar|Q}} expresses that if it is raining outside then there is a cold-front over Kansas. The material conditional is often confused with physical causation.  The material conditional, however, only relates two propositions by their truth-values—which is not the relation of cause and effect.  It is contentious in the literature whether the material implication represents logical causation.\n*Biconditional joins two simpler propositions, and we write {{math|''P'' ↔ ''Q''}}, which is read \"{{mvar|P}} if and only if {{mvar|Q}}\".  It expresses that {{mvar|P}} and {{mvar|Q}} have the same truth-value, and so, in cases 1 and 4, {{mvar|P}} is true if and only if {{mvar|Q}} is true, and false otherwise.\n\nIt is extremely helpful to look at the [[truth table]]s for these different operators, as well as the [[method of analytic tableaux]].\n\n===Closure under operations===\nPropositional logic is closed under truth-functional connectives.  That is to say, for any proposition {{mvar|φ}}, {{math|¬''φ''}} is also a proposition.  Likewise, for any propositions {{mvar|φ}} and {{mvar|ψ}}, {{math|''φ'' ∧ ''ψ''}} is a proposition, and similarly for disjunction, conditional, and biconditional.  This implies that, for instance, {{math|''φ'' ∧ ''ψ''}} is a proposition, and so it can be conjoined with another proposition.  In order to represent this, we need to use parentheses to indicate which proposition is conjoined with which.  For instance, {{math|''P'' ∧ ''Q'' ∧ ''R''}} is not a well-formed formula, because we do not know if we are conjoining {{math|''P'' ∧ ''Q''}} with {{mvar|R}} or if we are conjoining {{mvar|P}} with {{math|''Q'' ∧ ''R''}}.  Thus we must write either {{math|(''P'' ∧ ''Q'') ∧ ''R''}} to represent the former, or {{math|''P'' ∧ (''Q'' ∧ ''R'')}} to represent the latter.  By evaluating the truth conditions, we see that both expressions have the same truth conditions (will be true in the same cases), and moreover that any proposition formed by arbitrary conjunctions will have the same truth conditions, regardless of the location of the parentheses.  This means that conjunction is associative, however, one should not assume that parentheses never serve a purpose.  For instance, the sentence {{math|''P'' ∧ (''Q'' ∨ ''R'')}} does not have the same truth conditions of {{math|(''P'' ∧ ''Q'') ∨ ''R''}}, so they are different sentences distinguished only by the parentheses.  One can verify this by the truth-table method referenced above.\n\nNote:  For any arbitrary number of propositional constants, we can form a finite number of cases which list their possible truth-values.  A simple way to generate this is by truth-tables, in which one writes {{mvar|P}}, {{mvar|Q}}, ..., {{mvar|Z}}, for any list of {{mvar|k}} propositional constants—that is to say, any list of propositional constants with {{mvar|k}} entries.  Below this list, one writes {{math|2<sup>''k''</sup>}} rows, and below {{mvar|P}} one fills in the first half of the rows with true (or T) and the second half with false (or F).  Below {{mvar|Q}} one fills in one-quarter of the rows with T, then one-quarter with F, then one-quarter with T and the last quarter with F.  The next column alternates between true and false for each eighth of the rows, then sixteenths, and so on, until the last propositional constant varies between T and F for each row.  This will give a complete listing of cases or truth-value assignments possible for those propositional constants.\n\n===Argument===\nThe propositional calculus then defines an ''[[argument]]'' to be a list of propositions.  A valid argument is a list of propositions, the last of which follows from—or is implied by—the rest.  All other arguments are invalid.  The simplest valid argument is [[modus ponens]], one instance of which is the following list of propositions:\n\n:<math>\n\\begin{array}{rl}\n1. & P \\to Q \\\\\n2. & P \\\\\n\\hline\n\\therefore & Q\n\\end{array}\n</math>\n\nThis is a list of three propositions, each line is a proposition, and the last follows from the rest.  The first two lines are called premises, and the last line the conclusion. We say that any proposition {{mvar|C}} follows from any set of propositions <math>(P_1, ..., P_n)</math>, if {{mvar|C}} must be true whenever every member of the set <math>(P_1, ..., P_n)</math> is true.  In the argument above, for any {{mvar|P}} and {{mvar|Q}}, whenever {{math|''P'' → ''Q''}} and {{mvar|P}} are true, necessarily {{mvar|Q}} is true.  Notice that, when {{mvar|P}} is true, we cannot consider cases 3 and 4 (from the truth table).  When {{math|''P'' → ''Q''}} is true, we cannot consider case 2.  This leaves only case 1, in which {{mvar|Q}} is also true.  Thus {{mvar|Q}} is implied by the premises.\n\nThis generalizes schematically.  Thus, where {{mvar|φ}} and {{mvar|ψ}} may be any propositions at all,\n\n:<math>\n\\begin{array}{rl}\n1. & \\varphi \\to \\psi \\\\\n2. & \\varphi \\\\\n\\hline\n\\therefore & \\psi\n\\end{array}\n</math>\n\nOther argument forms are convenient, but not necessary.  Given a complete set of axioms (see below for one such set), modus ponens is sufficient to prove all other argument forms in propositional logic, thus they may be considered to be a derivative.  Note, this is not true of the extension of propositional logic to other logics like [[first-order logic]].  First-order logic requires at least one additional rule of inference in order to obtain [[Completeness (logic)|completeness]].\n\nThe significance of argument in formal logic is that one may obtain new truths from established truths.  In the first example above, given the two premises, the truth of {{mvar|Q}} is not yet known or stated.  After the argument is made, {{mvar|Q}} is deduced.  In this way, we define a deduction system to be a set of all propositions that may be deduced from another set of propositions.  For instance, given the set of propositions <math>A = \\{ P \\lor Q, \\neg Q \\land R, (P \\lor Q) \\to R \\}</math>, we can define a deduction system, {{math|Γ}}, which is the set of all propositions which follow from {{mvar|A}}. [[deduction theorem#Virtual rules of inference|Reiteration]] is always assumed, so <math>P \\lor Q, \\neg Q \\land R, (P \\lor Q) \\to R \\in \\Gamma</math>.  Also, from the first element of {{mvar|A}}, last element, as well as modus ponens, {{mvar|R}} is a consequence, and so <math>R \\in \\Gamma</math>.  Because we have not included sufficiently complete axioms, though, nothing else may be deduced.  Thus, even though most deduction systems studied in propositional logic are able to deduce <math>(P \\lor Q) \\leftrightarrow (\\neg P \\to Q)</math>, this one is too weak to prove such a proposition.\n\n==Generic description of a propositional calculus==\nA '''propositional calculus''' is a [[formal system]] <math>\\mathcal{L} = \\mathcal{L} \\left( \\Alpha,\\ \\Omega,\\ \\Zeta,\\ \\Iota \\right)</math>, where:\n\n*The ''alpha set'' <math>\\Alpha</math> is a countably infinite set of elements called ''proposition symbols'' or ''[[propositional variable]]s''. Syntactically speaking, these are the most basic elements of the formal language <math>\\mathcal{L}</math>, otherwise referred to as ''[[atomic formula]]s'' or ''terminal elements''. In the examples to follow, the elements of <math>\\Alpha</math> are typically the letters {{mvar|p}}, {{mvar|q}}, {{mvar|r}}, and so on.\n\n*The ''omega set'' {{math|Ω}} is a finite set of elements called ''[[Operation (mathematics)|operator symbols]]'' or ''[[logical connective]]s''. The set {{math|Ω}} is [[Partition of a set|partitioned]] into disjoint subsets as follows:\n\n:::<math>\\Omega = \\Omega_0 \\cup \\Omega_1 \\cup \\ldots \\cup \\Omega_j \\cup \\ldots \\cup \\Omega_m.</math>\n\n:In this partition, <math>\\Omega_j</math> is the set of operator symbols of ''[[arity]]'' {{mvar|j}}.\n\n:In the more familiar propositional calculi, {{math|Ω}} is typically partitioned as follows:\n\n:::<math>\\Omega_1 = \\{ \\lnot \\},</math>\n\n:::<math>\\Omega_2 \\subseteq \\{ \\land, \\lor, \\to, \\leftrightarrow \\}.</math>\n\n:A frequently adopted convention treats the constant [[logical value]]s as operators of arity zero, thus:\n\n:::<math>\\Omega_0 = \\{ \\bot, \\top \\}.</math>\n\n:Some writers use the [[tilde]] (~), or N, instead of {{math|¬}}; and some use the [[ampersand]] (&), the prefixed K, or <math>\\cdot</math> instead of <math>\\wedge</math>. Notation varies even more for the set of logical values, with symbols like {false, true}, {F, T}, or {0, 1} all being seen in various contexts instead of <math>\\{ \\bot, \\top \\}</math>.\n\n*The ''zeta set'' <math>\\Zeta</math> is a finite set of ''transformation rules'' that are called ''[[inference rule]]s'' when they acquire logical applications.\n\n*The ''iota set'' <math>\\Iota</math> is a countable set of ''initial points'' that are called ''[[axiom]]s'' when they receive logical interpretations.\n\nThe ''language'' of <math>\\mathcal{L}</math>, also known as its set of ''formulas'', ''[[well-formed formula]]s'', is [[inductive definition|inductively defined]] by the following rules:\n\n#Base: Any element of the alpha set <math>\\Alpha</math> is a formula of <math>\\mathcal{L}</math>.\n#If <math>p_1, p_2, \\ldots, p_j</math> are formulas and <math>f</math> is in <math>\\Omega_j</math>, then <math>\\left( f(p_1, p_2, \\ldots, p_j) \\right)</math> is a formula.\n#Closed: Nothing else is a formula of <math>\\mathcal{L}</math>.\n\nRepeated applications of these rules permits the construction of complex formulas. For example:\n\n#By rule 1, {{mvar|p}} is a formula.\n#By rule 2, <math>\\neg p</math> is a formula.\n#By rule 1, {{mvar|q}} is a formula.\n#By rule 2, <math>( \\neg p \\lor q )</math> is a formula.\n\n==Example 1. Simple axiom system==\nLet <math>\\mathcal{L}_1 = \\mathcal{L}(\\Alpha,\\Omega,\\Zeta,\\Iota)</math>, where <math>\\Alpha</math>, <math>\\Omega</math>, <math>\\Zeta</math>, <math>\\Iota</math> are defined as follows:\n\n*The alpha set <math>\\Alpha</math>, is a countably infinite set of symbols, for example:\n\n:::<math>\\Alpha = \\{p, q, r, s, t, u, p_2, \\ldots \\}.</math>\n\n*Of the three connectives for conjunction, disjunction, and implication (<math>\\wedge, \\lor</math>, and {{math|→}}), one can be taken as primitive and the other two can be defined in terms of it and negation ({{math|¬}}).<ref name=\"Wernick\">Wernick, William (1942) \"Complete Sets of Logical Functions,\" ''Transactions of the American Mathematical Society'' '''51''', pp. 117&ndash;132.</ref> Indeed, all of the logical connectives can be defined in terms of a [[sole sufficient operator]]. The biconditional ({{math|↔}}) can of course be defined in terms of conjunction and implication, with <math>a \\leftrightarrow b</math> defined as <math>(a \\to b) \\land (b \\to a)</math>.\n:Adopting negation and implication as the two primitive operations of a propositional calculus is tantamount to having the omega set <math>\\Omega = \\Omega_1 \\cup \\Omega_2</math> partition as follows:\n\n::: <math>\\Omega_1 = \\{ \\lnot \\},</math>\n::: <math>\\Omega_2 = \\{ \\to \\}.</math>\n\n*An axiom system discovered by [[Jan Łukasiewicz]] formulates a propositional calculus in this language as follows. The axioms are all [[substitution instance]]s of:\n\n::* <math>(p \\to (q \\to p))</math>\n\n::* <math>((p \\to (q \\to r)) \\to ((p \\to q) \\to (p \\to r)))</math>\n\n::* <math>((\\neg p \\to \\neg q) \\to (q \\to p))</math>\n\n*The rule of inference is [[modus ponens]] (i.e., from {{mvar|p}} and <math>(p \\to q)</math>, infer {{mvar|q}}). Then <math>a \\lor b</math> is defined as <math>\\neg a \\to b</math>, and <math>a \\land b</math> is defined as <math>\\neg(a \\to \\neg b)</math>. This system is used in [[Metamath]] [http://us.metamath.org/mpegif/mmset.html#scaxioms set.mm] formal proof database.\n\n==Example 2. Natural deduction system==\nLet <math>\\mathcal{L}_2 = \\mathcal{L}(\\Alpha, \\Omega, \\Zeta, \\Iota)</math>, where <math>\\Alpha</math>, <math>\\Omega</math>, <math>\\Zeta</math>, <math>\\Iota</math> are defined as follows:\n\n* The alpha set <math>\\Alpha</math>, is a countably infinite set of symbols, for example:\n*: <math>\\Alpha = \\{p, q, r, s, t, u, p_2, \\ldots \\}.</math>\n* The omega set <math>\\Omega = \\Omega_1 \\cup \\Omega_2</math> partitions as follows:\n*: <math>\\Omega_1 = \\{ \\lnot \\},</math>\n*: <math>\\Omega_2 = \\{ \\land, \\lor, \\to, \\leftrightarrow \\}.</math>\n\nIn the following example of a propositional calculus, the transformation rules are intended to be interpreted as the inference rules of a so-called ''[[natural deduction system]]''. The particular system presented here has no initial points, which means that its interpretation for logical applications derives its [[theorem]]s from an empty axiom set.\n\n* The set of initial points is empty, that is, <math>\\Iota = \\varnothing</math>.\n* The set of transformation rules, <math>\\Zeta</math>, is described as follows:\n\nOur propositional calculus has eleven inference rules. These rules allow us to derive other true formulas given a set of formulas that are assumed to be true. The first ten simply state that we can infer certain well-formed formulas from other well-formed formulas. The last rule however uses hypothetical reasoning in the sense that in the premise of the rule we temporarily assume an (unproven) hypothesis to be part of the set of inferred formulas to see if we can infer a certain other formula. Since the first ten rules don't do this they are usually described as ''non-hypothetical'' rules, and the last one as a ''hypothetical'' rule.\n\nIn describing the transformation rules, we may introduce a metalanguage symbol <math>\\vdash</math>. It is basically a convenient shorthand for saying \"infer that\". The format is <math>\\Gamma \\vdash \\psi</math>, in which {{math|Γ}} is a (possibly empty) set of formulas called premises, and {{mvar|ψ}} is a formula called conclusion. The transformation rule <math>\\Gamma \\vdash \\psi</math> means that if every proposition in {{math|Γ}} is a theorem (or has the same truth value as the axioms), then {{mvar|ψ}} is also a theorem. Note that considering the following rule [[Conjunction introduction]], we will know whenever {{math|Γ}} has more than one formula, we can always safely reduce it into one formula using conjunction. So for short, from that time on we may represent {{math|Γ}} as one formula instead of a set. Another omission for convenience is when {{math|Γ}} is an empty set, in which case {{math|Γ}} may not appear.\n\n; [[Negation introduction]]: From <math>(p \\to q)</math> and <math>(p \\to \\neg q)</math>, infer <math>\\neg p</math>.\n: That is, <math>\\{ (p \\to q), (p \\to \\neg q) \\} \\vdash \\neg p</math>.\n; [[Negation elimination]]: From <math>\\neg p</math>, infer <math>(p \\to r)</math>.\n: That is, <math>\\{ \\neg p \\} \\vdash (p \\to r)</math>.\n; [[Double negation elimination]]: From <math>\\neg \\neg p</math>, infer {{mvar|p}}.\n: That is, <math>\\neg \\neg p \\vdash p</math>.\n; [[Conjunction introduction]]: From {{mvar|p}} and {{mvar|q}}, infer <math>(p \\land q)</math>.\n: That is, <math>\\{ p, q \\} \\vdash (p \\land q)</math>.\n; [[Conjunction elimination]]: From <math>(p \\land q)</math>, infer {{mvar|p}}.\n: From <math>(p \\land q)</math>, infer {{mvar|q}}.\n: That is, <math>(p \\land q) \\vdash p</math> and <math>(p \\land q) \\vdash q</math>.\n; [[Disjunction introduction]]: From {{mvar|p}}, infer <math>(p \\lor q)</math>.\n: From {{mvar|q}}, infer <math>(p \\lor q)</math>.\n: That is, <math>p \\vdash (p \\lor q)</math> and <math>q \\vdash (p \\lor q)</math>.\n; [[Disjunction elimination]]: From <math>(p \\lor q)</math> and <math>(p \\to r)</math> and <math>(q \\to r)</math>, infer {{mvar|r}}.\n: That is, <math>\\{p \\lor q, p \\to r, q \\to r\\} \\vdash r</math>.\n; [[Biconditional introduction]]: From <math>(p \\to q)</math> and <math>(q \\to p)</math>, infer <math>(p \\leftrightarrow q)</math>.\n: That is, <math>\\{p \\to q, q \\to p\\} \\vdash (p \\leftrightarrow q)</math>.\n;[[Biconditional elimination]]: From <math>(p \\leftrightarrow q)</math>, infer <math>(p \\to q)</math>.\n: From <math>(p \\leftrightarrow q)</math>, infer <math>(q \\to p)</math>.\n: That is, <math>(p \\leftrightarrow q) \\vdash (p \\to q)</math> and <math>(p \\leftrightarrow q) \\vdash (q \\to p)</math>.\n;[[Modus ponens]] (conditional elimination) : From {{mvar|p}} and <math>(p \\to q)</math>, infer {{mvar|q}}.\n: That is, <math>\\{ p, p \\to q\\} \\vdash q</math>.\n; [[Conditional proof]] (conditional introduction) : From [accepting {{mvar|p}} allows a proof of {{mvar|q}}], infer <math>(p \\to q)</math>.\n: That is, <math>(p \\vdash q) \\vdash (p \\to q)</math>.\n\n==Basic and derived argument forms==\n\n{| style=\"margin:auto;\" class=\"wikitable\"\n|-\n|+ Basic and Derived Argument Forms\n|- \"\n! Name\n! Sequent\n! Description\n|-\n| [[Modus Ponens]]\n| <math>((p \\to q) \\land p) \\vdash q</math>\n| If {{mvar|p}} then {{mvar|q}}; {{mvar|p}}; therefore {{mvar|q}}\n|-\n| [[Modus Tollens]]\n| <math>((p \\to q) \\land \\neg q) \\vdash \\neg p</math>\n| If {{mvar|p}} then {{mvar|q}}; not {{mvar|q}}; therefore not {{mvar|p}}\n|-\n| [[Hypothetical Syllogism]]\n| <math>((p \\to q) \\land (q \\to r)) \\vdash (p \\to r)</math>\n| If {{mvar|p}} then {{mvar|q}}; if {{mvar|q}} then {{mvar|r}}; therefore, if {{mvar|p}} then {{mvar|r}}\n|-\n| [[Disjunctive syllogism|Disjunctive Syllogism]]\n| <math>((p \\lor q) \\land \\neg p) \\vdash q</math>\n| Either {{mvar|p}} or {{mvar|q}}, or both; not {{mvar|p}}; therefore, {{mvar|q}}\n|-\n| [[Constructive dilemma|Constructive Dilemma]]\n| <math>((p \\to q) \\land (r \\to s) \\land (p \\lor r)) \\vdash (q \\lor s)</math>\n| If {{mvar|p}} then {{mvar|q}}; and if {{mvar|r}} then {{mvar|s}}; but {{mvar|p}} or {{mvar|r}}; therefore {{mvar|q}} or {{mvar|s}}\n|-\n| [[Destructive dilemma|Destructive Dilemma]]\n| <math>((p \\to q) \\land (r \\to s) \\land(\\neg q \\lor \\neg s)) \\vdash (\\neg p \\lor \\neg r)</math>\n| If {{mvar|p}} then {{mvar|q}}; and if {{mvar|r}} then {{mvar|s}}; but not {{mvar|q}} or not {{mvar|s}}; therefore not {{mvar|p}} or not {{mvar|r}}\n|-\n| Bidirectional Dilemma\n| <math>((p \\to q) \\land (r \\to s) \\land(p \\lor \\neg s)) \\vdash (q \\lor \\neg r)</math>\n| If {{mvar|p}} then {{mvar|q}}; and if {{mvar|r}} then {{mvar|s}}; but {{mvar|p}} or not {{mvar|s}}; therefore {{mvar|q}} or not {{mvar|r}}\n|-\n| [[Conjunction elimination|Simplification]]\n| <math>(p \\land q) \\vdash p</math>\n| {{mvar|p}} and {{mvar|q}} are true; therefore {{mvar|p}} is true\n|-\n| [[Logical conjunction|Conjunction]]\n| <math>p, q \\vdash (p \\land q)</math>\n| {{mvar|p}} and {{mvar|q}} are true separately; therefore they are true conjointly\n|-\n| [[Logical disjunction|Addition]]\n| <math>p \\vdash (p \\lor q)</math>\n| {{mvar|p}} is true; therefore the disjunction ({{mvar|p}} or {{mvar|q}}) is true\n|-\n| [[Distributive property|Composition]]\n| <math>((p \\to q) \\land (p \\to r)) \\vdash (p \\to (q \\land r))</math>\n| If {{mvar|p}} then {{mvar|q}}; and if {{mvar|p}} then {{mvar|r}}; therefore if {{mvar|p}} is true then {{mvar|q}} and {{mvar|r}} are true\n|-\n| [[De Morgan's laws|De Morgan's Theorem]] (1)\n| <math>\\neg (p \\land q) \\vdash (\\neg p \\lor \\neg q)</math>\n| The negation of ({{mvar|p}} and {{mvar|q}}) is equiv. to (not {{mvar|p}} or not {{mvar|q}})\n|-\n| [[De Morgan's laws|De Morgan's Theorem]] (2)\n| <math>\\neg (p \\lor q) \\vdash (\\neg p \\land \\neg q)</math>\n| The negation of ({{mvar|p}} or {{mvar|q}}) is equiv. to (not {{mvar|p}} and not {{mvar|q}})\n|-\n| [[Commutative property|Commutation]] (1)\n| <math>(p \\lor q) \\vdash (q \\lor p)</math>\n| ({{mvar|p}} or {{mvar|q}}) is equiv. to ({{mvar|q}} or {{mvar|p}})\n|-\n| [[Commutative property|Commutation]] (2)\n| <math>(p \\land q) \\vdash (q \\land p)</math>\n| ({{mvar|p}} and {{mvar|q}}) is equiv. to ({{mvar|q}} and {{mvar|p}})\n|-\n| [[Commutative property|Commutation]] (3)\n| <math>(p \\leftrightarrow q) \\vdash (q \\leftrightarrow p)</math>\n| ({{mvar|p}} is equiv. to {{mvar|q}}) is equiv. to ({{mvar|q}} is equiv. to {{mvar|p}})\n|-\n| [[Associative property|Association]] (1)\n| <math>(p \\lor (q \\lor r)) \\vdash ((p \\lor q) \\lor r)</math>\n| {{mvar|p}} or ({{mvar|q}} or {{mvar|r}}) is equiv. to ({{mvar|p}} or {{mvar|q}}) or {{mvar|r}}\n|-\n| [[Associative property|Association]] (2)\n| <math>(p \\land (q \\land r)) \\vdash ((p \\land q) \\land r)</math>\n| {{mvar|p}} and ({{mvar|q}} and {{mvar|r}}) is equiv. to ({{mvar|p}} and {{mvar|q}}) and {{mvar|r}}\n|-\n| [[Distributive property|Distribution]] (1)\n| <math>(p \\land (q \\lor r)) \\vdash ((p \\land q) \\lor (p \\land r))</math>\n| {{mvar|p}} and ({{mvar|q}} or {{mvar|r}}) is equiv. to ({{mvar|p}} and {{mvar|q}}) or ({{mvar|p}} and {{mvar|r}})\n|-\n| [[Distributive property|Distribution]] (2)\n| <math>(p \\lor (q \\land r)) \\vdash ((p \\lor q) \\land (p \\lor r))</math>\n| {{mvar|p}} or ({{mvar|q}} and {{mvar|r}}) is equiv. to ({{mvar|p}} or {{mvar|q}}) and ({{mvar|p}} or {{mvar|r}})\n|-\n| [[Double negation elimination|Double Negation]]\n| <math>p \\vdash \\neg \\neg p</math>\n| {{mvar|p}} is equivalent to the negation of not {{mvar|p}}\n|-\n| [[Transposition (logic)|Transposition]]\n| <math>(p \\to q) \\vdash (\\neg q \\to \\neg p)</math>\n| If {{mvar|p}} then {{mvar|q}} is equiv. to if not {{mvar|q}} then not {{mvar|p}}\n|-\n| [[Material implication (rule of inference)|Material Implication]]\n| <math>(p \\to q) \\vdash (\\neg p \\lor q)</math>\n| If {{mvar|p}} then {{mvar|q}} is equiv. to not {{mvar|p}} or {{mvar|q}}\n|-\n| [[Material equivalence|Material Equivalence]] (1)\n| <math>(p \\leftrightarrow q) \\vdash ((p \\to q) \\land (q \\to p))</math>\n| ({{mvar|p}} iff {{mvar|q}}) is equiv. to (if {{mvar|p}} is true then {{mvar|q}} is true) and (if {{mvar|q}} is true then {{mvar|p}} is true)\n|-\n| [[Material equivalence|Material Equivalence]] (2)\n| <math>(p \\leftrightarrow q) \\vdash ((p \\land q) \\lor (\\neg p \\land \\neg q))</math>\n| ({{mvar|p}} iff {{mvar|q}}) is equiv. to either ({{mvar|p}} and {{mvar|q}} are true) or (both {{mvar|p}} and {{mvar|q}} are false)\n|-\n| [[Material equivalence|Material Equivalence]] (3)\n| <math>(p \\leftrightarrow q) \\vdash ((p \\lor \\neg q) \\land (\\neg p \\lor q))</math>\n| ({{mvar|p}} iff {{mvar|q}}) is equiv to., both ({{mvar|p}} or not {{mvar|q}} is true) and (not {{mvar|p}} or {{mvar|q}} is true)\n|-\n| [[Exportation (logic)|Exportation]]<ref>{{cite web|url = http://www.cs.odu.edu/~toida/nerzic/content/logic/prop_logic/implications/implication_proof.html | title = Proof of Implications | work = CS381 Discrete Structures/Discrete Mathematics Web Course Material | date = 2 August 2009 | accessdate =10 March 2010 | first = Shunichi | last = Toida | publisher = Department Of Computer Science, [[Old Dominion University]]}}</ref>\n| <math>((p \\land q) \\to r) \\vdash (p \\to (q \\to r))</math>\n| from (if {{mvar|p}} and {{mvar|q}} are true then {{mvar|r}} is true) we can prove (if {{mvar|q}} is true then {{mvar|r}} is true, if {{mvar|p}} is true)\n|-\n| [[Exportation (logic)|Importation]]\n| <math>(p \\to (q \\to r)) \\vdash ((p \\land q) \\to r)</math>\n| If {{mvar|p}} then (if {{mvar|q}} then {{mvar|r}}) is equivalent to if {{mvar|p}} and {{mvar|q}} then {{mvar|r}}\n|-\n| [[Tautology (rule of inference)|Tautology]] (1)\n| <math>p \\vdash (p \\lor p)</math>\n| {{mvar|p}} is true is equiv. to {{mvar|p}} is true or {{mvar|p}} is true\n|-\n| [[Tautology (rule of inference)|Tautology]] (2)\n| <math>p \\vdash (p \\land p)</math>\n| {{mvar|p}} is true is equiv. to {{mvar|p}} is true and {{mvar|p}} is true\n|-\n| [[Law of excluded middle|Tertium non datur (Law of Excluded Middle)]]\n| <math>\\vdash (p \\lor \\neg p)</math>\n| {{mvar|p}} or not {{mvar|p}} is true\n|-\n| [[Law of noncontradiction|Law of Non-Contradiction]]\n| <math>\\vdash \\neg (p \\land \\neg p)</math>\n| {{mvar|p}} and not {{mvar|p}} is false, is a true statement\n|}\n\n==Proofs in propositional calculus==\nOne of the main uses of a propositional calculus, when interpreted for logical applications, is to determine relations of logical equivalence between propositional formulas. These relationships are determined by means of the available transformation rules, sequences of which are called ''derivations'' or ''proofs''.\n\nIn the discussion to follow, a proof is presented as a sequence of numbered lines, with each line consisting of a single formula followed by a ''reason'' or ''justification'' for introducing that formula. Each premise of the argument, that is, an assumption introduced as an hypothesis of the argument, is listed at the beginning of the sequence and is marked as a \"premise\" in lieu of other justification. The conclusion is listed on the last line. A proof is complete if every line follows from the previous ones by the correct application of a transformation rule. (For a contrasting approach, see [[Method of analytic tableaux|proof-trees]]).\n\n===Example of a proof===\n* To be shown that {{math|''A'' → ''A''}}.\n* One possible proof of this (which, though valid, happens to contain more steps than are necessary) may be arranged as follows:\n\n{| style=\"margin:auto;\" class=\"wikitable\"\n|-\n|+ Example of a Proof\n|-\n! Number\n! Formula\n! Reason\n|-\n| {{EquationRef|1}} || <math>A</math> || premise\n|-\n| {{EquationRef|2}} || <math>A \\lor A</math> || From ({{EquationNote|1}}) by disjunction introduction\n|-\n| {{EquationRef|3}} || <math>(A \\lor A) \\land A</math> || From ({{EquationNote|1}}) and ({{EquationNote|2}}) by conjunction introduction\n|-\n| {{EquationRef|4}} || <math>A</math> || From ({{EquationNote|3}}) by conjunction elimination\n|-\n| {{EquationRef|5}} || <math>A \\vdash A</math> || Summary of ({{EquationNote|1}}) through ({{EquationNote|4}})\n|-\n| {{EquationRef|6}} || <math>\\vdash A \\to A</math> || From ({{EquationNote|5}}) by conditional proof\n|}\n\nInterpret <math>A \\vdash A</math> as \"Assuming {{mvar|A}}, infer {{mvar|A}}\". Read <math>\\vdash A \\to A</math> as \"Assuming nothing, infer that {{mvar|A}} implies {{mvar|A}}\", or \"It is a tautology that {{mvar|A}} implies {{mvar|A}}\", or \"It is always true that {{mvar|A}} implies {{mvar|A}}\".\n\n==Soundness and completeness of the rules==\nThe crucial properties of this set of rules are that they are ''[[soundness|sound]]'' and ''complete''. Informally this means that the rules are correct and that no other rules are required. These claims can be made more formal as follows.\n\nWe define a ''truth assignment'' as a [[function (mathematics)|function]] that maps propositional variables to '''true''' or '''false'''. Informally such a truth assignment can be understood as the description of a possible [[State of affairs (philosophy)|state of affairs]] (or [[possible world]]) where certain statements are true and others are not. The semantics of formulas can then be formalized by defining for which \"state of affairs\" they are considered to be true, which is what is done by the following definition.\n\nWe define when such a truth assignment {{mvar|A}} satisfies a certain [[well-formed formula]] with the following rules:\n* {{mvar|A}} satisfies the propositional variable {{mvar|P}} [[if and only if]] {{math|''A''(''P'') {{=}} true}}\n* {{mvar|A}} satisfies {{math|¬''φ''}} if and only if {{mvar|A}} does not satisfy {{mvar|φ}}\n* {{mvar|A}} satisfies {{math|(''φ'' ∧ ''ψ'')}} if and only if {{mvar|A}} satisfies both {{mvar|φ}} and {{mvar|ψ}}\n* {{mvar|A}} satisfies {{math|(''φ'' ∨ ''ψ'')}} if and only if {{mvar|A}} satisfies at least one of either {{mvar|φ}} or {{mvar|ψ}}\n* {{mvar|A}} satisfies {{math|(''φ'' → ''ψ'')}} if and only if it is not the case that {{mvar|A}} satisfies {{mvar|φ}} but not {{mvar|ψ}}\n* {{mvar|A}} satisfies {{math|(''φ'' ↔ ''ψ'')}} if and only if {{mvar|A}} satisfies both {{mvar|φ}} and {{mvar|ψ}} or satisfies neither one of them\n\nWith this definition we can now formalize what it means for a formula {{mvar|φ}} to be implied by a certain set {{mvar|S}} of formulas. Informally this is true if in all worlds that are possible given the set of formulas {{mvar|S}} the formula {{mvar|φ}} also holds. This leads to the following formal definition: We say that a set {{mvar|S}} of well-formed formulas ''semantically entails'' (or ''implies'') a certain well-formed formula {{mvar|φ}} if all truth assignments that satisfy all the formulas in {{mvar|S}} also satisfy {{mvar|φ}}.\n\nFinally we define ''syntactical entailment'' such that {{mvar|φ}} is syntactically entailed by {{mvar|S}} if and only if we can derive it with the inference rules that were presented above in a finite number of steps. This allows us to formulate exactly what it means for the set of inference rules to be sound and complete:\n\n'''Soundness:''' If the set of well-formed formulas {{mvar|S}} syntactically entails the well-formed formula {{mvar|φ}} then {{mvar|S}} semantically entails {{mvar|φ}}.\n\n'''Completeness:''' If the set of well-formed formulas {{mvar|S}} semantically entails the well-formed formula {{mvar|φ}} then {{mvar|S}} syntactically entails {{mvar|φ}}.\n\nFor the above set of rules this is indeed the case.\n\n===Sketch of a soundness proof===\n(For most [[logical system]]s, this is the comparatively \"simple\" direction of proof)\n\nNotational conventions: Let {{mvar|G}} be a variable ranging over sets of sentences. Let {{mvar|A, B}} and {{mvar|C}} range over sentences. For \"{{mvar|G}} syntactically entails {{mvar|A}}\" we write \"{{mvar|G}} proves {{mvar|A}}\". For \"{{mvar|G}} semantically entails {{mvar|A}}\" we write \"{{mvar|G}} implies {{mvar|A}}\".\n\nWe want to show: {{math|(''A'')(''G'')}} (if {{mvar|G}} proves {{mvar|A}}, then {{mvar|G}} implies {{mvar|A}}).\n\nWe note that \"{{mvar|G}} proves {{mvar|A}}\" has an inductive definition, and that gives us the immediate resources for demonstrating claims of the form \"If {{mvar|G}} proves {{mvar|A}}, then ...\". So our proof proceeds by induction.\n\n{{ordered list|list-style-type=upper-roman\n|1= Basis. Show: If {{mvar|A}} is a member of {{mvar|G}}, then {{mvar|G}} implies {{mvar|A}}. \n|2= Basis. Show: If {{mvar|A}} is an axiom, then {{mvar|G}} implies {{mvar|A}}. \n|3= Inductive step (induction on {{mvar|n}}, the length of the proof):\n{{ordered list|list-style-type=lower-alpha\n | Assume for arbitrary {{mvar|G}} and {{mvar|A}} that if {{mvar|G}} proves {{mvar|A}} in {{mvar|n}} or fewer steps, then {{mvar|G}} implies {{mvar|A}}. \n | For each possible application of a rule of inference at step {{math|''n'' + 1}}, leading to a new theorem {{mvar|B}}, show that {{mvar|G}} implies {{mvar|B}}. \n }}\n}}\nNotice that Basis Step II can be omitted for [[natural deduction]] systems because they have no axioms. When used, Step II involves showing that each of the axioms is a (semantic) [[logical truth]].\n\nThe Basis steps demonstrate that the simplest provable sentences from {{mvar|G}} are also implied by {{mvar|G}}, for any {{mvar|G}}. (The proof is simple, since the semantic fact that a set implies any of its members, is also trivial.) The Inductive step will systematically cover all the further sentences that might be provable—by considering each case where we might reach a logical conclusion using an inference rule—and shows that if a new sentence is provable, it is also logically implied. (For example, we might have a rule telling us that from \"{{mvar|A}}\" we can derive \"{{mvar|A}} or {{mvar|B}}\". In III.a We assume that if {{mvar|A}} is provable it is implied. We also know that if {{mvar|A}} is provable then \"{{mvar|A}} or {{mvar|B}}\" is provable. We have to show that then \"{{mvar|A}} or {{mvar|B}}\" too is implied. We do so by appeal to the semantic definition and the assumption we just made. {{mvar|A}} is provable from {{mvar|G}}, we assume. So it is also implied by {{mvar|G}}. So any semantic valuation making all of {{mvar|G}} true makes {{mvar|A}} true. But any valuation making {{mvar|A}} true makes \"{{mvar|A}} or {{mvar|B}}\" true, by the defined semantics for \"or\". So any valuation which makes all of {{mvar|G}} true makes \"{{mvar|A}} or {{mvar|B}}\" true. So \"{{mvar|A}} or {{mvar|B}}\" is implied.) Generally, the Inductive step will consist of a lengthy but simple [[Proof by cases|case-by-case analysis]] of all the rules of inference, showing that each \"preserves\" semantic implication.\n\nBy the definition of provability, there are no sentences provable other than by being a member of {{mvar|G}}, an axiom, or following by a rule; so if all of those are semantically implied, the deduction calculus is sound.\n\n===Sketch of completeness proof===\n(This is usually the much harder direction of proof.)\n\nWe adopt the same notational conventions as above.\n\nWe want to show: If {{mvar|G}} implies {{mvar|A}}, then {{mvar|G}} proves {{mvar|A}}. We proceed by [[contraposition]]: We show instead that if {{mvar|G}} does '''not''' prove {{mvar|A}} then {{mvar|G}} does '''not''' imply {{mvar|A}}. If we show that there is a [[Mathematical model|model]] where {{mvar|A}} does not hold despite {{mvar|G}} being true, then obviously {{mvar|G}} does not imply {{mvar|A}}. The idea is to build such a model out of our very assumption that {{mvar|G}} does not prove {{mvar|A}}.\n\n{{ordered list|list-style-type=upper-roman\n|1= {{mvar|G}} does not prove {{mvar|A}}. (Assumption) \n|2= If {{mvar|G}} does not prove {{mvar|A}}, then we can construct an (infinite) '''Maximal Set''', {{math|''G''<sup>∗</sup>}}, which is a superset of {{mvar|G}} and which also does not prove {{mvar|A}}.\n {{ordered list|list-style-type=lower-latin\n |1= Place an ordering (with [[order type]] &omega;) on all the sentences in the language (e.g., shortest first, and equally long ones in extended alphabetical ordering), and number them {{math|(''E''<sub>1</sub>, ''E''<sub>2</sub>, ...)}} \n |2= Define a series {{mvar|G<sub>n</sub>}} of sets {{math|(''G''<sub>0</sub>, ''G''<sub>1</sub>, ...)}} inductively:\n  {{ordered list|list-style-type=lower-roman\n  |1= <math>G_0 = G</math> \n  |2= If <math>G_k \\cup \\{ E_{k+1} \\}</math> proves {{mvar|A}}, then <math>G_{k+1} = G_k</math> \n  |3= If <math>G_k \\cup \\{ E_{k+1} \\}</math> does '''not''' prove {{mvar|A}}, then <math>G_{k+1} = G_k \\cup \\{ E_{k+1} \\}</math> \n  }}\n |3= Define {{math|''G''<sup>∗</sup>}} as the union of all the {{mvar|G<sub>n</sub>}}. (That is, {{math|''G''<sup>∗</sup>}} is the set of all the sentences that are in any {{mvar|G<sub>n</sub>}}.) \n |4= It can be easily shown that\n  {{ordered list|list-style-type=lower-roman\n  |1= {{math|''G''<sup>∗</sup>}} contains (is a superset of) {{mvar|G}} (by (b.i)); \n  |2= {{math|''G''<sup>∗</sup>}} does not prove {{mvar|A}} (because the proof would contain only finitely many sentences and when the last of them is introduced in some {{mvar|G<sub>n</sub>}}, that {{mvar|G<sub>n</sub>}} would prove {{mvar|A}} contrary to the definition of {{mvar|G<sub>n</sub>}}); and \n  |3= {{math|''G''<sup>∗</sup>}} is a Maximal Set with respect to {{mvar|A}}: If any more sentences whatever were added to {{math|''G''<sup>∗</sup>}}, it would prove {{mvar|A}}. (Because if it were possible to add any more sentences, they should have been added when they were encountered during the construction of the {{mvar|G<sub>n</sub>}}, again by definition) \n  }}\n }}\n|3= If {{math|''G''<sup>∗</sup>}} is a Maximal Set with respect to {{mvar|A}}, then it is '''truth-like'''. This means that it contains {{mvar|C}} only if it does '''not''' contain {{mvar|¬C}}; If it contains {{mvar|C}} and contains \"If {{mvar|C}} then {{mvar|B}}\" then it also contains {{mvar|B}}; and so forth. \n|4= If {{math|''G''<sup>∗</sup>}} is truth-like there is a {{math|''G''<sup>∗</sup>}}-Canonical valuation of the language: one that makes every sentence in {{math|''G''<sup>∗</sup>}} true and everything outside {{math|''G''<sup>∗</sup>}} false while still obeying the laws of semantic composition in the language. \n|5= A {{math|''G''<sup>∗</sup>}}-canonical valuation will make our original set {{mvar|G}} all true, and make {{mvar|A}} false. \n|6= If there is a valuation on which {{mvar|G}} are true and {{mvar|A}} is false, then {{mvar|G}} does not (semantically) imply {{mvar|A}}. \n}}\n\n[[Q.E.D.|QED]]\n\n===Another outline for a completeness proof===\nIf a formula is a [[Tautology (logic)|tautology]], then there is a [[truth table]] for it which shows that each valuation yields the value true for the formula. Consider such a valuation. By mathematical induction on the length of the subformulas, show that the truth or falsity of the subformula follows from the truth or falsity (as appropriate for the valuation) of each propositional variable in the subformula. Then combine the lines of the truth table together two at a time by using \"({{mvar|P}} is true implies {{mvar|S}}) implies (({{mvar|P}} is false implies {{mvar|S}}) implies {{mvar|S}})\". Keep repeating this until all dependencies on propositional variables have been eliminated. The result is that we have proved the given tautology. Since every tautology is provable, the logic is complete.\n\n==Interpretation of a truth-functional propositional calculus==\nAn '''interpretation of a truth-functional propositional calculus''' <math>\\mathcal{P}</math> is an [[assignment (mathematical logic)|assignment]] to each [[Propositional variable|propositional symbol]] of <math>\\mathcal{P}</math> of one or the other (but not both) of the [[truth value]]s [[truth]] ('''T''') and [[false (logic)|falsity]] ('''F'''), and an assignment to the [[logical connective|connective symbols]] of <math>\\mathcal{P}</math> of their usual truth-functional meanings. An interpretation of a truth-functional propositional calculus may also be expressed in terms of [[truth tables]].<ref name=\"metalogic\">{{Cite book| last = Hunter | first = Geoffrey | title = Metalogic: An Introduction to the Metatheory of Standard First-Order Logic | publisher = University of California Pres | year = 1971 | isbn = 0-520-02356-0}}</ref>\n\nFor <math>n</math> distinct propositional symbols there are <math>2^n</math> distinct possible interpretations. For any particular symbol <math>a</math>, for example, there are <math>2^1=2</math> possible interpretations:\n# <math>a</math> is assigned '''T''', or\n# <math>a</math> is assigned '''F'''.\nFor the pair <math>a</math>, <math>b</math> there are <math>2^2=4</math> possible interpretations:\n# both are assigned '''T''',\n# both are assigned '''F''',\n# <math>a</math> is assigned '''T''' and <math>b</math> is assigned '''F''', or\n# <math>a</math> is assigned '''F''' and <math>b</math> is assigned '''T'''.<ref name=\"metalogic\"/>\n\nSince <math>\\mathcal{P}</math> has <math>\\aleph_0</math>, that is, [[Denumerably infinite|denumerably]] many propositional symbols, there are <math>2^{\\aleph_0}=\\mathfrak c</math>, and therefore [[Cardinality of the continuum|uncountably many]] distinct possible interpretations of <math>\\mathcal{P}</math>.<ref name=\"metalogic\"/>\n\n===Interpretation of a sentence of truth-functional propositional logic===\n{{Main|Interpretation (logic)}}\n\nIf {{mvar|φ}} and {{mvar|ψ}} are [[formula (mathematical logic)|formulas]] of <math>\\mathcal{P}</math> and <math>\\mathcal{I}</math> is an interpretation of <math>\\mathcal{P}</math> then:\n\n* A sentence of propositional logic is ''true under an interpretation'' <math>\\mathcal{I}</math> iff <math>\\mathcal{I}</math> assigns the truth value '''T''' to that sentence. If a sentence is [[logical truth|true]] under an interpretation, then that interpretation is called a ''model'' of that sentence.\n* {{mvar|φ}} is ''false under an interpretation'' <math>\\mathcal{I}</math> iff {{mvar|φ}} is not true under <math>\\mathcal{I}</math>.<ref name=\"metalogic\"/>\n* A sentence of propositional logic is ''logically valid'' if it is true under every interpretation.\n:<math>\\models</math> {{mvar|φ}} means that {{mvar|φ}} is logically valid.\n* A sentence {{mvar|ψ}} of propositional logic is a ''[[Logical consequence|semantic consequence]]'' of a sentence {{mvar|φ}} iff there is no interpretation under which {{mvar|φ}} is true and {{mvar|ψ}} is false.\n* A sentence of propositional logic is ''[[Consistency|consistent]]'' iff it is true under at least one interpretation. It is inconsistent if it is not consistent.\n\nSome consequences of these definitions:\n\n*For any given interpretation a given formula is either true or false.<ref name=\"metalogic\"/>\n*No formula is both true and false under the same interpretation.<ref name=\"metalogic\"/>\n*{{mvar|φ}} is false for a given interpretation iff <math>\\neg\\phi</math> is true for that interpretation; and {{mvar|φ}} is true under an interpretation iff <math>\\neg\\phi</math> is false under that interpretation.<ref name=\"metalogic\"/>\n*If {{mvar|φ}} and <math>(\\phi \\to \\psi)</math> are both true under a given interpretation, then {{mvar|ψ}} is true under that interpretation.<ref name=\"metalogic\"/>\n*If <math>\\models_{\\mathrm P}\\phi</math> and <math>\\models_{\\mathrm P}(\\phi \\to \\psi)</math>, then <math>\\models_{\\mathrm P}\\psi</math>.<ref name=\"metalogic\"/>\n*<math>\\neg\\phi</math> is true under <math>\\mathcal{I}</math> iff {{mvar|φ}} is not true under <math>\\mathcal{I}</math>.\n*<math>(\\phi \\to \\psi)</math> is true under <math>\\mathcal{I}</math> iff either {{mvar|φ}} is not true under <math>\\mathcal{I}</math> or {{mvar|ψ}} is true under <math>\\mathcal{I}</math>.<ref name=\"metalogic\"/>\n* A sentence {{mvar|ψ}} of propositional logic is a semantic consequence of a sentence {{mvar|φ}} iff <math>(\\phi \\to \\psi)</math> is [[logically valid]], that is, <math>\\phi \\models_{\\mathrm P} \\psi</math> iff <math> \\models_{\\mathrm P}(\\phi \\to \\psi)</math>.<ref name=\"metalogic\"/>\n\n==Alternative calculus==\nIt is possible to define another version of propositional calculus, which defines most of the syntax of the logical operators by means of axioms, and which uses only one inference rule.\n\n===Axioms===\nLet {{mvar|φ}}, {{mvar|χ}}, and {{mvar|ψ}} stand for well-formed formulas.  (The well-formed formulas themselves would not contain any Greek letters, but only capital Roman letters, connective operators, and parentheses.) Then the axioms are as follows:\n\n{| style=\"margin:auto;\" class=\"wikitable\"\n|-\n|+ Axioms\n|-\n! Name\n! Axiom Schema\n! Description\n|-\n| {{EquationRef|THEN-1}}\n| <math>\\phi \\to (\\chi \\to \\phi)</math>\n| Add hypothesis {{mvar|χ}}, implication introduction\n|-\n| {{EquationRef|THEN-2}}\n| <math>(\\phi \\to (\\chi \\to \\psi)) \\to ((\\phi \\to \\chi) \\to (\\phi \\to \\psi))</math>\n| Distribute hypothesis <math>\\phi</math> over implication\n|-\n| {{EquationRef|AND-1}}\n| <math>\\phi \\land \\chi \\to \\phi</math>\n| Eliminate conjunction\n|-\n| {{EquationRef|AND-2}}\n| <math>\\phi \\land \\chi \\to \\chi</math>\n| &nbsp;\n|-\n| {{EquationRef|AND-3}}\n| <math>\\phi \\to (\\chi \\to (\\phi \\land \\chi))</math>\n| Introduce conjunction\n|-\n| {{EquationRef|OR-1}}\n| <math>\\phi \\to \\phi \\lor \\chi</math>\n| Introduce disjunction\n|-\n| {{EquationRef|OR-2}}\n| <math>\\chi \\to \\phi \\lor \\chi</math>\n| &nbsp;\n|-\n| {{EquationRef|OR-3}}\n| <math>(\\phi \\to \\psi) \\to ((\\chi \\to \\psi) \\to (\\phi \\lor \\chi \\to \\psi))</math>\n| Eliminate disjunction\n|-\n| {{EquationRef|NOT-1}}\n| <math>(\\phi \\to \\chi) \\to ((\\phi \\to \\neg \\chi) \\to \\neg \\phi)</math>\n| Introduce negation\n|-\n| {{EquationRef|NOT-2}}\n| <math>\\phi \\to (\\neg \\phi \\to \\chi)</math>\n| Eliminate negation\n|-\n| {{EquationRef|NOT-3}}\n| <math>\\phi \\lor \\neg \\phi</math>\n| Excluded middle, classical logic\n|-\n| {{EquationRef|IFF-1}}\n| <math>(\\phi \\leftrightarrow \\chi) \\to (\\phi \\to \\chi)</math>\n| Eliminate equivalence\n|-\n| {{EquationRef|IFF-2}}\n| <math>(\\phi \\leftrightarrow \\chi) \\to (\\chi \\to \\phi)</math>\n| &nbsp;\n|-\n| {{EquationRef|IFF-3}}\n| <math>(\\phi \\to \\chi) \\to ((\\chi \\to \\phi) \\to (\\phi \\leftrightarrow \\chi))</math>\n| Introduce equivalence\n|}\n\n*Axiom {{EquationNote|THEN-2}} may be considered to be a \"distributive property of implication with respect to implication.\"\n*Axioms {{EquationNote|AND-1}} and {{EquationNote|AND-2}} correspond to \"conjunction elimination\". The relation between {{EquationNote|AND-1}} and {{EquationNote|AND-2}} reflects the commutativity of the conjunction operator.\n*Axiom {{EquationNote|AND-3}} corresponds to \"conjunction introduction.\"\n*Axioms {{EquationNote|OR-1}} and {{EquationNote|OR-2}} correspond to \"disjunction introduction.\" The relation between {{EquationNote|OR-1}} and {{EquationNote|OR-2}} reflects the commutativity of the disjunction operator.\n*Axiom {{EquationNote|NOT-1}} corresponds to \"reductio ad absurdum.\"\n*Axiom {{EquationNote|NOT-2}} says that \"anything can be deduced from a contradiction.\"\n*Axiom {{EquationNote|NOT-3}} is called \"[[law of excluded middle|tertium non datur]]\" ([[Latin]]: \"a third is not given\") and reflects the semantic valuation of propositional formulas: a formula can have a truth-value of either true or false. There is no third truth-value, at least not in classical logic. [[Intuitionistic logic]]ians do not accept the axiom {{EquationNote|NOT-3}}.\n\n===Inference rule===\nThe inference rule is [[modus ponens]]:\n:<math> \\phi, \\ \\phi \\to \\chi \\vdash \\chi </math>.\n\n===Meta-inference rule===\nLet a demonstration be represented by a sequence, with hypotheses to the left of the [[Turnstile (symbol)|turnstile]] and the conclusion to the right of the turnstile. Then the [[deduction theorem]] can be stated as follows:\n: ''If the sequence''\n::<math> \\phi_1, \\ \\phi_2, \\ ... , \\ \\phi_n, \\ \\chi \\vdash \\psi </math>\n: ''has been demonstrated, then it is also possible to demonstrate the sequence''\n::<math> \\phi_1, \\ \\phi_2, \\ ..., \\ \\phi_n \\vdash \\chi \\to \\psi </math>.\n\nThis deduction theorem (DT) is not itself formulated with propositional calculus: it is not a theorem of propositional calculus, but a theorem about propositional calculus. In this sense, it is a [[meta-theorem]], comparable to theorems about the soundness or completeness of propositional calculus.\n\nOn the other hand, DT is so useful for simplifying the syntactical proof process that it can be considered and used as another inference rule, accompanying modus ponens. In this sense, DT corresponds to the natural [[conditional proof]] inference rule which is part of the first version of propositional calculus introduced in this article.\n\nThe converse of DT is also valid:\n: ''If the sequence''\n::<math> \\phi_1, \\ \\phi_2, \\ ..., \\ \\phi_n \\vdash \\chi \\to \\psi </math>\n: ''has been demonstrated, then it is also possible to demonstrate the sequence''\n::<math> \\phi_1, \\ \\phi_2, \\ ... , \\ \\phi_n, \\ \\chi \\vdash \\psi </math>\nin fact, the validity of the converse of DT is almost trivial compared to that of DT:\n: ''If''\n:: <math> \\phi_1, \\ ... , \\ \\phi_n \\vdash \\chi \\to \\psi </math>\n: ''then''\n:: 1: <math> \\phi_1, \\ ... , \\ \\phi_n, \\ \\chi \\vdash \\chi \\to \\psi </math>\n:: 2: <math> \\phi_1, \\ ... , \\ \\phi_n, \\ \\chi \\vdash \\chi </math>\n: ''and from (1) and (2) can be deduced''\n:: 3: <math> \\phi_1, \\ ... , \\ \\phi_n, \\ \\chi \\vdash \\psi </math>\n: ''by means of modus ponens, Q.E.D.''\n\nThe converse of DT has powerful implications: it can be used to convert an axiom into an inference rule. For example, the axiom AND-1,\n: <math> \\vdash \\phi \\wedge \\chi \\to \\phi </math>\ncan be transformed by means of the converse of the deduction theorem into the inference rule\n: <math> \\phi \\wedge \\chi \\vdash \\phi </math>\nwhich is [[conjunction elimination]], one of the ten inference rules used in the first version (in this article) of the propositional calculus.\n\n===Example of a proof===\nThe following is an example of a (syntactical) demonstration, involving only axioms {{EquationNote|THEN-1}} and {{EquationNote|THEN-2}}:\n\n'''Prove:''' <math>A \\to A</math> (Reflexivity of implication).\n\n'''Proof:'''\n# <math>(A \\to ((B \\to A) \\to A)) \\to ((A \\to (B \\to A)) \\to (A \\to A))</math>\n#: Axiom {{EquationNote|THEN-2}} with <math>\\phi = A, \\chi = B \\to A, \\psi = A</math>\n# <math>A \\to ((B \\to A) \\to A)</math>\n#: Axiom {{EquationNote|THEN-1}} with <math>\\phi = A, \\chi = B \\to A</math>\n# <math>(A \\to (B \\to A)) \\to (A \\to A)</math>\n#: From (1) and (2) by modus ponens.\n# <math>A \\to (B \\to A)</math>\n#: Axiom {{EquationNote|THEN-1}} with <math>\\phi = A, \\chi = B</math>\n# <math>A \\to A</math>\n#: From (3) and (4) by modus ponens.\n\n==Equivalence to equational logics==\nThe preceding alternative calculus is an example of a [[Hilbert-style deduction system]]. In the case of propositional systems the axioms are terms built with logical connectives and the only inference rule is modus ponens. Equational logic as standardly used informally in high school algebra is a different kind of calculus from Hilbert systems. Its theorems are equations and its inference rules express the properties of equality, namely that it is a congruence on terms that admits substitution.\n\nClassical propositional calculus as described above is equivalent to [[Boolean algebra (logic)|Boolean algebra]], while [[Intuitionistic logic|intuitionistic propositional calculus]] is equivalent to [[Heyting algebra]]. The equivalence is shown by translation in each direction of the theorems of the respective systems. Theorems <math>\\phi</math> of classical or intuitionistic propositional calculus are translated as equations <math>\\phi = 1</math> of Boolean or Heyting algebra respectively. Conversely theorems <math>x = y</math> of Boolean or Heyting algebra are translated as theorems <math>(x \\to y) \\land (y \\to x)</math> of classical or intuitionistic calculus respectively, for which <math>x \\equiv y</math> is a standard abbreviation. In the case of Boolean algebra <math>x = y</math> can also be translated as <math>(x \\land y) \\lor (\\neg x \\land \\neg y)</math>, but this translation is incorrect intuitionistically.\n\nIn both Boolean and Heyting algebra, inequality <math>x \\le y</math> can be used in place of equality. The equality <math>x = y</math> is expressible as a pair of inequalities <math>x \\le y</math> and <math>y \\le x</math>. Conversely the inequality <math>x \\le y</math> is expressible as the equality <math>x \\land y = x</math>, or as <math>x \\lor y = y</math>. The significance of inequality for Hilbert-style systems is that it corresponds to the latter's deduction or [[entailment]] symbol <math>\\vdash</math>. An entailment\n::<math> \\phi_1, \\ \\phi_2, \\ \\dots, \\ \\phi_n \\vdash \\psi</math>\n\nis translated in the inequality version of the algebraic framework as\n::<math> \\phi_1\\ \\land\\ \\phi_2\\ \\land\\ \\dots\\ \\land \\ \\phi_n\\ \\ \\le\\ \\ \\psi</math>\n\nConversely the algebraic inequality <math>x \\le y</math> is translated as the entailment\n::<math>x\\ \\vdash\\ y</math>.\n\nThe difference between implication <math>x \\to y</math> and inequality or [[entailment]] <math>x \\le y</math> or <math>x\\ \\vdash\\ y</math> is that the former is internal to the logic while the latter is external. Internal implication between two terms is another term of the same kind. Entailment as external implication between two terms expresses a metatruth outside the language of the logic, and is considered part of the [[metalanguage]]. Even when the logic under study is intuitionistic, entailment is ordinarily understood classically as two-valued: either the left side entails, or is less-or-equal to, the right side, or it is not.\n\nSimilar but more complex translations to and from algebraic logics are possible for natural deduction systems as described above and for the [[sequent calculus]]. The entailments of the latter can be interpreted as two-valued, but a more insightful interpretation is as a set, the elements of which can be understood as abstract proofs organized as the morphisms of a [[Category (mathematics)|category]]. In this interpretation the cut rule of the sequent calculus corresponds to composition in the category. Boolean and Heyting algebras enter this picture as special categories having at most one morphism per homset, i.e., one proof per entailment, corresponding to the idea that existence of proofs is all that matters: any proof will do and there is no point in distinguishing them.\n\n==Graphical calculi==\n{{Unreferenced section|date=March 2011}}\nIt is possible to generalize the definition of a formal language from a set of finite sequences over a finite basis to include many other sets of mathematical structures, so long as they are built up by finitary means from finite materials. What's more, many of these families of formal structures are especially well-suited for use in logic.\n\nFor example, there are many families of [[Graph (discrete mathematics)|graphs]] that are close enough analogues of formal languages that the concept of a calculus is quite easily and naturally extended to them. Indeed, many species of graphs arise as ''[[parse graph]]s'' in the syntactic analysis of the corresponding families of text structures. The exigencies of practical computation on formal languages frequently demand that text strings be converted into [[pointer structure]] renditions of parse graphs, simply as a matter of checking whether strings are well-formed formulas or not. Once this is done, there are many advantages to be gained from developing the graphical analogue of the calculus on strings. The mapping from strings to parse graphs is called ''[[parsing]]'' and the inverse mapping from parse graphs to strings is achieved by an operation that is called ''[[graph traversal|traversing]]'' the graph.\n\n==Other logical calculi==\nPropositional calculus is about the simplest kind of logical calculus in current use. It can be extended in several ways. ([[Term logic|Aristotelian \"syllogistic\" calculus]], which is largely supplanted in modern logic, is in ''some'' ways simpler – but in other ways more complex – than propositional calculus.) The most immediate way to develop a more complex logical calculus is to introduce rules that are sensitive to more fine-grained details of the sentences being used.\n\n[[First-order logic]] (a.k.a. first-order predicate logic) results when the \"atomic sentences\" of propositional logic are broken up into [[singular term|terms]], [[variable (mathematics)|variable]]s, [[Predicate (logic)|predicates]], and [[Quantifier (logic)|quantifier]]s, all keeping the rules of propositional logic with some new ones introduced. (For example, from \"All dogs are mammals\" we may infer \"If Rover is a dog then Rover is a mammal\".) With the tools of first-order logic it is possible to formulate a number of theories, either with explicit axioms or by rules of inference, that can themselves be treated as logical calculi. [[Arithmetic]] is the best known of these; others include [[set theory]] and [[mereology]]. [[Second-order logic]] and other [[higher-order logic]]s are formal extensions of first-order logic. Thus, it makes sense to refer to propositional logic as ''\"zeroth-order logic\"'', when comparing it with these logics.\n\n[[Modal logic]] also offers a variety of inferences that cannot be captured in propositional calculus. For example, from \"Necessarily {{mvar|p}}\" we may infer that {{mvar|p}}. From {{mvar|p}} we may infer \"It is possible that {{mvar|p}}\". The translation between modal logics and algebraic logics concerns classical and intuitionistic logics but with the introduction of a unary operator on Boolean or Heyting algebras, different from the Boolean operations, interpreting the possibility modality, and in the case of Heyting algebra a second operator interpreting necessity (for Boolean algebra this is redundant since necessity is the De Morgan dual of possibility). The first operator preserves 0 and disjunction while the second preserves 1 and conjunction.\n\n[[Many-valued logic]]s are those allowing sentences to have values other than ''true'' and ''false''. (For example, ''neither'' and ''both'' are standard \"extra values\"; \"continuum logic\" allows each sentence to have any of an infinite number of \"degrees of truth\" between ''true'' and ''false''.)  These logics often require calculational devices quite distinct from propositional calculus. When the values form a Boolean algebra (which may have more than two or even infinitely many values), many-valued logic reduces to classical logic; many-valued logics are therefore only of independent interest when the values form an algebra that is not Boolean.\n\n==Solvers==\nFinding solutions to propositional logic formulas is an [[NP-complete]] problem. However, practical methods exist (e.g., [[DPLL algorithm]], 1962; [[Chaff algorithm]], 2001) that are very fast for many useful cases. Recent work has extended the [[SAT solver]] algorithms to work with propositions containing [[arithmetic expression]]s; these are the [[SMT solver]]s.\n\n==See also==\n{{Portal|Logic}}\n\n===Higher logical levels===\n* [[First-order logic]]\n* [[Second-order propositional logic]]\n* [[Second-order logic]]\n* [[Higher-order logic]]\n\n===Related topics===\n{{col-begin}}\n{{col-break}}\n* [[Boolean algebra (logic)]]\n* [[Boolean algebra (structure)]]\n* [[Boolean algebra topics]]\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean-valued function]]\n* [[Categorical logic]]\n* [[Combinational logic]]\n* [[Combinatory logic]]\n* [[Conceptual graph]]\n* [[Disjunctive syllogism]]\n* [[Entitative graph]]\n{{col-break}}\n* [[Equational logic]]\n* [[Existential graph]]\n* [[Frege's propositional calculus]]\n* [[Implicational propositional calculus]]\n* [[Intuitionistic propositional calculus]]\n* [[Jean Buridan]] \n* [[Laws of Form]]\n* [[Logical graph]]\n* [[Logical NOR]]\n* [[Logical value]]\n{{col-break}}\n* [[Operation (mathematics)|Operation]]\n* [[Paul of Venice]] \n* [[Peirce's law]]\n* [[Peter of Spain (author)|Peter of Spain]] \n* [[Propositional formula]]\n* [[Symmetric difference]]\n* [[Truth function]] \n* [[Truth table]]\n* [[Walter Burley]] \n* [[William of Sherwood]]\n{{col-end}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* Brown, Frank Markham (2003), ''Boolean Reasoning: The Logic of Boolean Equations'', 1st edition, Kluwer Academic Publishers, Norwell, MA. 2nd edition, Dover Publications, Mineola, NY.\n* [[Chen Chung Chang|Chang, C.C.]] and [[Howard Jerome Keisler|Keisler, H.J.]] (1973), ''Model Theory'', North-Holland, Amsterdam, Netherlands.\n* Kohavi, Zvi (1978), ''Switching and Finite Automata Theory'', 1st edition, McGraw–Hill, 1970. 2nd edition, McGraw–Hill, 1978.\n* [[Robert R. Korfhage|Korfhage, Robert R.]] (1974), ''Discrete Computational Structures'', Academic Press, New York, NY.\n* [[Joachim Lambek|Lambek, J.]] and Scott, P.J. (1986), ''Introduction to Higher Order Categorical Logic'', Cambridge University Press, Cambridge, UK.\n* Mendelson, Elliot (1964), ''Introduction to Mathematical Logic'', D. Van Nostrand Company.\n\n===Related works===\n* {{Cite book|last=Hofstadter |first=Douglas |authorlink=Douglas Hofstadter |title=[[Gödel, Escher, Bach|Gödel, Escher, Bach: An Eternal Golden Braid]] |year=1979 |publisher=[[Basic Books]] |isbn=978-0-465-02656-2 }}\n\n==External links==\n{{Commonscat}}\n*[[Kevin C. Klement|Klement, Kevin C.]] (2006), \"Propositional Logic\", in James Fieser and Bradley Dowden (eds.), ''[[Internet Encyclopedia of Philosophy]]'', [http://www.iep.utm.edu/p/prop-log.htm Eprint].\n*[http://www.qedeq.org/current/doc/math/qedeq_formal_logic_v1_en.pdf Formal Predicate Calculus], contains a systematic formal development along the lines of [[Propositional calculus#Alternative calculus|Alternative calculus]]\n* ''[http://www.fecundity.com/logic/ forall x: an introduction to formal logic]'', by [[P.D. Magnus]], covers formal semantics and [[proof theory]] for sentential logic.\n*[http://logicinaction.org/docs/ch2.pdf Chapter 2 / Propositional Logic] from [http://logicinaction.org Logic In Action]\n*[https://www.nayuki.io/page/propositional-sequent-calculus-prover Propositional sequent calculus prover] on Project Nayuki. (''note'': implication can be input in the form <tt>!X|Y</tt>, and a sequent can be a single formula prefixed with <tt>></tt> and having no commas)\n*[https://docs.google.com/document/d/1DhtRAPcMwJmiQnbdmFcHWaOddQ7kuqqDnWp2LZcGlnY/edit?usp=sharing Propositional Logic - A Generative Grammar]\n\n{{Classical logic}}\n{{Formal Fallacy}}\n{{Mathematical logic}}\n\n[[Category:Propositional calculus| ]]\n[[Category:Systems of formal logic]]\n[[Category:Logical calculi]]\n[[Category:Boolean algebra]]\n[[Category:Classical logic]]"
    },
    {
      "title": "Propositional directed acyclic graph",
      "url": "https://en.wikipedia.org/wiki/Propositional_directed_acyclic_graph",
      "text": "A '''propositional directed acyclic graph (PDAG)''' is a [[data structure]] that is used to represent a [[Boolean function]].  A Boolean function can be represented as a rooted, [[directed acyclic graph]] of the following form:\n* Leaves are labeled with <math>\\top</math> (true), <math>\\bot</math> (false), or a Boolean variable.\n* Non-leaves are <math>\\bigtriangleup</math> (logical and), <math>\\bigtriangledown</math> (logical or) and <math>\\Diamond</math> (logical not).\n* <math>\\bigtriangleup</math>- and <math>\\bigtriangledown</math>-nodes have at least one child.\n* <math>\\Diamond</math>-nodes have exactly one child.\n\nLeaves labeled with <math>\\top</math> (<math>\\bot</math>) represent the constant Boolean function which always evaluates to 1 (0). A leaf labeled with a Boolean variable <math>x</math> is interpreted as the assignment <math>x=1</math>, i.e. it represents the Boolean function which evaluates to 1 if and only if <math>x=1</math>. The Boolean function represented by a <math>\\bigtriangleup</math>-node is the one that evaluates to 1, if and only if the Boolean function of all its children evaluate to 1. Similarly, a <math>\\bigtriangledown</math>-node represents the Boolean function that evaluates to 1, if and only if the Boolean function of at least one child evaluates to 1. Finally, a <math>\\Diamond</math>-node represents the complemenatary Boolean function its child, i.e. the one that evaluates to 1, if and only if the Boolean function of its child evaluates to 0.\n\n== PDAG, BDD, and NNF ==\nEvery '''[[Binary decision diagram|binary decision diagram (BDD)]]''' and every '''[[Negation normal form|negation normal form (NNF)]]''' are also a PDAG with some particular properties. The following pictures represent the Boolean function  <math>f(x1, x2, x3) = -x1 * -x2 * -x3  +  x1 * x2  +  x2 * x3</math>:\n\n{| align=\"center\"\n|-\n| [[File:BDD simple.svg|thumb|189px|BDD for the function f]]\n| [[File:BDD2pdag.png|thumb|189px|PDAG for the function f obtained from the BDD]]\n| [[File:BDD2pdag simple.svg|thumb|189px|PDAG for the function f]]\n|}\n\n== See also ==\n* [[Data structure]]\n* [[Boolean satisfiability problem]]\n* [[Proposition (mathematics)|Proposition]]\n\n== References ==\n*  M. Wachter & R. Haenni, \"Propositional DAGs: a New Graph-Based Language for Representing Boolean Functions\", KR'06, 10th International Conference on Principles of Knowledge Representation and Reasoning, Lake District, UK, 2006.\n*  M. Wachter & R. Haenni, \"Probabilistic Equivalence Checking with Propositional DAGs\", Technical Report iam-2006-001, Institute of Computer Science and Applied Mathematics, University of Bern, Switzerland, 2006.\n*  M. Wachter, R. Haenni & J. Jonczy,  \"Reliability and Diagnostics of Modular Systems: a New Probabilistic Approach\", DX'06, 18th International Workshop on Principles of Diagnosis, Peñaranda de Duero, Burgos, Spain, 2006.\n\n[[Category:Graph data structures]]\n[[Category:Directed graphs]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Propositional formula",
      "url": "https://en.wikipedia.org/wiki/Propositional_formula",
      "text": "In [[propositional logic]], a '''propositional formula''' is a type of syntactic [[Formula (mathematical logic)|formula]] which is [[well formed formula|well formed]] and has a [[truth value]]. If the values of all variables in a propositional formula are given, it determines a unique truth value. A propositional formula may also be called a '''propositional expression''', a '''sentence''', or a '''sentential formula'''.\n\nA propositional formula is constructed from simple [[Proposition (logic)|proposition]]s, such as \"five is greater than three\" or [[propositional variable]]s such as ''P'' and ''Q'', using connectives such as NOT, AND, OR, or IMPLIES; for example:\n\n: (''P'' AND NOT ''Q'') IMPLIES (''P'' OR ''Q'').\n\nIn [[mathematics]], a propositional formula is often more briefly referred to as a \"'''proposition'''\", but, more precisely, a propositional formula is not a proposition but a [[formal expression]] that ''denotes'' a [[Proposition (mathematics)|proposition]], a [[formal object]] under discussion, just like an expression such as \"{{nowrap|''x'' + ''y''}}\" is not a value, but denotes a value. In some contexts, maintaining the distinction may be of importance.\n\n==Propositions==\nFor the purposes of the propositional calculus, '''propositions''' (utterances, sentences, assertions) are considered to be either '''simple''' or '''compound'''.<ref>Hamilton 1978:1</ref> Compound propositions are considered to be linked by '''sentential connectives''', some of the most common of which are \"AND\", \"OR\", \"IF … THEN …\", \"NEITHER … NOR …\", \"… IS EQUIVALENT TO …\" . The linking semicolon \";\", and connective \"BUT\" are considered to be expressions of \"AND\". A sequence of discrete sentences are considered to be linked by \"AND\"s, and formal analysis applies a [[Recursion|recursive]] \"parenthesis rule\" with respect to sequences of simple propositions (see more [[#Well-formed formulas (wffs)|below]] about well-formed formulas).\n: For example: The assertion: \"This cow is blue. That horse is orange but this horse here is purple.\" is actually a compound proposition linked by \"AND\"s: ( (\"This cow is blue\" AND \"that horse is orange\") AND \"this horse here is purple\" ) .\n\nSimple propositions are declarative in nature, that is, they make assertions about the condition or nature of a ''particular'' object of sensation e.g. \"This cow is blue\", \"There's a coyote!\" (\"That coyote IS ''there'', behind the rocks.\").<ref>[[Principia Mathematica]] (PM) p. 91 eschews \"the\" because they require a clear-cut \"object of sensation\"; they stipulate the use of \"this\"</ref> Thus the simple \"primitive\" [[Logical assertion|assertion]]s must be about specific objects or specific states of mind. Each must have at least a '''subject''' (an immediate object of thought or observation), a verb (in the active voice and present tense preferred), and perhaps an adjective or adverb. \"Dog!\" probably implies \"I see a dog\" but should be rejected as too ambiguous.\n\n: Example: \"That purple dog is running\", \"This cow is blue\", \"Switch M31 is closed\", \"This cap is off\", \"Tomorrow is Friday\".\n\nFor the purposes of the propositional calculus a compound proposition can usually be reworded into a series of simple sentences, although the result will probably sound stilted.\n\n=== Relationship between propositional and predicate formulas ===\nThe [[predicate calculus]] goes a step further than the propositional calculus to an \"analysis of the ''inner structure'' of propositions\"<ref>(italics added) Reichenbach{{clarify|reason=There is no 'Reichenbach' entry under 'References', not even a link to an article about Reichenbach.|date=October 2013}} p.80.</ref> It breaks a simple sentence down into two parts (i) its '''subject''' (the object (singular or plural) of discourse) and (ii) a [[Predicate (grammar)|predicate]] (a verb or possibly verb-clause that asserts a quality or attribute of the object(s)). The predicate calculus then generalizes the \"subject|predicate\" form (where | symbolizes [[concatenation]] (stringing together) of symbols) into a form with the following blank-subject structure \" ___|predicate\", and the predicate in turn generalized to all things with that property.\n\n: Example: \"This blue pig has wings\" becomes two sentences in the ''propositional calculus'': \"This pig has wings\" AND \"This pig is blue\", whose internal structure is not considered. In contrast, in the predicate calculus, the first sentence breaks into \"this pig\" as the subject, and \"has wings\" as the predicate. Thus it asserts that object \"this pig\" is a member of the class (set, collection) of \"winged things\". The second sentence asserts that object \"this pig\" has an attribute \"blue\" and thus is a member of the class of \"blue things\". One might choose to write the two sentences connected with AND as:\n:: p|W AND p|B\n\nThe generalization of \"this pig\" to a (potential) member of two classes \"winged things\" and \"blue things\" means that it has a truth-relationship with both of these classes. In other words, given a '''domain of discourse''' \"winged things\", either we find p to be a member of this domain or not. Thus we have a relationship W (wingedness) between p (pig) and { T, F }, W(p) evaluates to { T, F } where { T, F } is the set of the [[Boolean data type|boolean values]] \"true\" and \"false\". Likewise for B (blueness) and p (pig) and { T, F }: B(p) evaluates to { T, F }. So we now can analyze the connected assertions \"B(p) AND W(p)\" for its overall truth-value, i.e.:\n: ( B(p) AND W(p) ) evaluates to { T, F }\n\nIn particular, simple sentences that employ notions of \"all\", \"some\", \"a few\", \"one of\", etc. are treated by the predicate calculus. Along with the new function symbolism \"F(x)\" two new symbols are introduced: ∀ (For all), and ∃ (There exists …, At least one of … exists, etc.). The predicate calculus, but not the propositional calculus, can establish the formal validity of the following statement:\n: \"All blue pigs have wings but some pigs have no wings, hence some pigs are not blue\".\n\n=== Identity ===\nTarski asserts that the notion of IDENTITY (as distinguished from LOGICAL EQUIVALENCE) lies outside the propositional calculus; however, he notes that if a logic is to be of use for mathematics and the sciences it must contain a \"theory\" of IDENTITY.<ref>Tarski p.54-68. Suppes calls IDENTITY a \"further rule of inference\" and has a brief development around it; Robbin, Bender and Williamson, and Goodstein introduce the sign and its usage without comment or explanation. Hamilton p. 37 employs two signs ≠ and = with respect to the '''valuation''' of a formula in a formal calculus. Kleene p. 70 and Hamilton p. 52 place it in the predicate calculus, in particular with regards to the arithmetic of natural numbers.</ref> Some authors refer to \"predicate logic with identity\" to emphasize this extension. See more about this below.\n\n==An algebra of propositions, the propositional calculus==\nAn '''algebra''' (and there are many different ones), loosely defined, is a method by which a collection of '''symbols''' called '''variables''' together with some other symbols such as parentheses (, ) and some sub-set of symbols such as *, +, ~, &, &or;, =, ≡, &and;, ￢ are manipulated within a '''system''' of rules. These symbols, and '''well-formed''' strings of them, are said to represent '''objects''', but in a specific algebraic system these objects do not have '''meanings'''. Thus work inside the algebra becomes an exercise in obeying certain '''laws''' ('''rules''') of the algebra's [[syntax]] (symbol-formation) rather than in [[semantics]] (meaning) of the symbols. The meanings are to be found outside the algebra.\n\nFor a well-formed sequence of symbols in the algebra —a '''formula'''— to have some usefulness outside the algebra the symbols are assigned meanings and eventually the variables are assigned '''values'''; then by a series of rules the formula is '''evaluated'''.\n\nWhen the values are restricted to just two and applied to the notion of '''simple sentences''' (e.g. spoken utterances or written assertions) linked by '''propositional connectives''' this whole algebraic system of symbols and rules and evaluation-methods is usually called the [[propositional calculus]] or the sentential calculus.\n\nWhile some of the familiar rules of arithmetic algebra continue to hold in the algebra of propositions (e.g. the commutative and associative laws for AND and OR), some do not (e.g. the distributive laws for AND, OR and NOT).\n\n=== Usefulness of propositional formulas ===\n'''Analysis''': In [[deductive reasoning]], philosophers, rhetoricians and mathematicians reduce arguments to formulas and then study them (usually with [[truth table]]s) for correctness (soundness). For example: Is the following argument sound?\n: \"Given that consciousness is sufficient for an [[artificial intelligence]] and only conscious entities can pass the [[Turing test]], before we can conclude that a robot is an artificial intelligence the robot must pass the Turing test.\"\n\nEngineers analyze the [[logic circuits]] they have designed using synthesis techniques and then apply various reduction and minimization techniques to simplify their designs.\n\n'''Synthesis''': Engineers in particular synthesize propositional formulas (that eventually end up as '''circuits''' of symbols) from [[truth table]]s. For example, one might write down a truth table for how [[binary addition]] should behave given the addition of variables \"b\" and \"a\" and \"carry_in\" \"ci\", and the results \"carry_out\" \"co\" and \"sum\" Σ:\n* Example: in row 5, ( (b+a) + ci ) = ( (1+0) + 1 ) = the number \"2\". written as a binary number this is 10<sub>2</sub>, where \"co\"=1 and Σ=0 as shown in the right-most columns.\n{| class=\"wikitable\" style=\"text-align:center; margin-left: auto; margin-right: auto; border: none;\"\n|-\n! row\n! b !! a !! ci !! !! (b+a)+ci !! co !! Σ\n|-\n! 0\n| 0 || 0 || 0 || || 0 || 0 || 0\n|-\n! 1\n| 0 || 0 || 1 || || 1 || 0 || 1\n|-\n! 2\n| 0 || 1 || 0 || || 1 || 0 || 1\n|-\n! 3\n| 0 || 1 || 1 || || 2 || 1 || 0\n|-\n! 4\n| 1 || 0 || 0 || || 1 || 0 || 1\n|-\n! 5\n| 1 || 0 || 1 || || 2 || 1 || 0\n|-\n! 6\n| 1 || 1 || 0 || || 2 || 1 || 0\n|-\n! 7\n| 1 || 1 || 1 || || 3 || 1 || 1\n|}\n\n=== Propositional variables ===\nThe simplest type of propositional formula is a '''[[propositional variable]]'''. Propositions that are simple ([[atomic formula|atomic]]), symbolic expressions are often denoted by variables named ''a'', ''b'', or ''A'', ''B'', etc. A propositional variable is intended to represent an atomic proposition (assertion), such as \"It is Saturday\" = ''a'' (here the symbol = means \" … is assigned the variable named …\") or \"I only go to the movies on Monday\" = ''b''.\n\n=== Truth-value assignments, formula evaluations ===\n'''Evaluation''' of a propositional formula begins with assignment of a '''truth value''' to each variable. Because each variable represents a simple sentence, the truth values are being applied to the \"truth\" or \"falsity\" of these simple sentences.\n\n'''Truth values in rhetoric, philosophy and mathematics''': The truth values are only two: { TRUTH \"T\",  FALSITY \"F\" }. An [[empiricist]] puts all propositions into two broad classes: ''analytic''—true no matter what (e.g. [[tautology (logic)|tautology]]), and ''synthetic''—derived from experience and thereby susceptible to confirmation by third parties (the [[verification theory]] of meaning).<ref>Empiricits eschew the notion of ''a priori'' (built-in, born-with) knowledge. \"Radical reductionists\" such as [[John Locke]] and [[David Hume]] \"held that every idea must either originate directly in sense experience or else be compounded of ideas thus originating\"; quoted from Quine reprinted in 1996 ''The Emergence of Logical Empriricism'', Garland Publishing Inc. http://www.marxists.org/reference/subject/philosophy/works/us/quine.htm</ref> Empiricits hold that, in general, to arrive at the truth-value of a [[synthetic proposition]], meanings (pattern-matching templates) must first be applied to the words, and then these meaning-templates must be matched against whatever it is that is being asserted. For example, my utterance \"That cow is ''{{blue|blue}}''!\" Is this statement a TRUTH? Truly I said it. And maybe I ''am'' seeing a blue cow—unless I am lying my statement is a TRUTH relative to the object of my (perhaps flawed) perception. But is the blue cow \"really there\"? What do you see when you look out the same window? In order to proceed with a verification, you will need a prior notion (a template) of both \"cow\" and \"{{blue|blue}}\", and an ability to match the templates against the object of sensation (if indeed there is one).{{cn|date=October 2016}}\n\n'''Truth values in engineering''': Engineers try to avoid notions of truth and falsity that bedevil philosophers, but in the final analysis engineers must trust their measuring instruments. In their quest for [[Robust statistics|robustness]], engineers prefer to pull known objects from a small library—objects that have well-defined, predictable behaviors even in large combinations, (hence their name for the propositional calculus: \"combinatorial logic\"). The fewest behaviors of a single object are two (e.g. { OFF, ON }, { open, shut }, { UP, DOWN } etc.), and these are put in correspondence with { 0, 1 }. Such elements are called [[Digital data|digital]]; those with a continuous range of behaviors are called [[analog signal|analog]]. Whenever decisions must be made in an analog system, quite often an engineer will convert an analog behavior (the door is 45.32146% UP) to digital (e.g. DOWN=0 ) by use of a [[comparator]].<ref>[[Neural net]] modelling offers a good mathematical model for a comparator as follows: Given a signal S and a threshold \"thr\", subtract \"thr\" from S and substitute this difference d to a [[sigmoid function]]: For large \"gains\" k, e.g. k=100, 1/( 1 + e<sup>−k*d</sup> ) = 1/( 1 + e<sup>−k*(S-thr)</sup> ) = { ≃0, ≃1 }.{{clarify|What is the meaning of the curly braces here? Denoting set comprehension wouldn't make sense.|date=October 2016}} For example, if \"The door is DOWN\" means \"The door is less than 50% of the way up\", then a threshold thr=0.5 corresponding to 0.5*5.0 = +2.50 volts could be applied to a \"linear\" measuring-device with an output of 0 volts when fully closed and +5.0 volts when fully open.</ref>\n\nThus an assignment of '''meaning''' of the variables and the two value-symbols { 0, 1 } comes from \"outside\" the formula that represents the behavior of the (usually) compound object. An example is a garage door with two \"limit switches\", one for UP labelled SW_U and one for DOWN labelled SW_D, and whatever else is in the door's circuitry. Inspection of the circuit (either the diagram or the actual objects themselves—door, switches, wires, circuit board, etc.) might reveal that, on the circuit board \"node 22\" goes to +0 volts when the contacts of switch \"SW_D\" are mechanically in contact (\"closed\") and the door is in the \"down\" position (95% down), and \"node 29\" goes to +0 volts when the door is 95% UP and the contacts of switch SW_U are in mechanical contact (\"closed\").<ref>In actuality the digital 1 and 0 are defined over non-overlapping ranges e.g. { \"1\" = +5/+0.2/−1.0 volts, 0 = +0.5/−0.2 volts }{{clarify|Explain the meaning of curly braces and slash here.|date=October 2016}}. When a value falls outside the defined range(s) the value becomes \"u\" -- unknown; e.g. +2.3 would be \"u\".</ref> The engineer must define the meanings of these voltages and all possible combinations (all 4 of them), including the \"bad\" ones (e.g. both nodes 22 and 29 at 0 volts, meaning that the door is open and closed at the same time). The circuit mindlessly responds to whatever voltages it experiences without any awareness of TRUTH or FALSEHOOD, RIGHT or WRONG, SAFE or DANGEROUS.{{cn|date=October 2016}}\n\n== Propositional connectives ==\n\nArbitrary propositional formulas are built from propositional variables and other propositional formulas using [[logical connective|propositional connective]]s. Examples of connectives include:\n* The unary negation connective. If <math>\\alpha</math> is a formula, then <math>\\lnot \\alpha</math> is a formula.\n* The classical binary connectives <math>\\land, \\lor, \\to, \\leftrightarrow</math>. Thus, for example, if <math>\\alpha</math> and <math>\\beta</math> are formulas, so is <math>(\\alpha \\to \\beta)</math>.\n* Other binary connectives, such as NAND, NOR, and XOR\n* The ternary connective IF … THEN … ELSE …\n* Constant 0-ary connectives ⊤ and ⊥ (alternately, constants { T, F }, { 1, 0 } etc. )\n* The \"theory-extension\" connective EQUALS (alternately, IDENTITY, or the sign \" = \" as distinguished from the \"logical connective\" <math>\\leftrightarrow</math>)\n\n=== Connectives of rhetoric, philosophy and mathematics ===\nThe following are the connectives common to rhetoric, philosophy and mathematics together with their [[truth table]]s. The symbols used will vary from author to author and between fields of endeavor. In general the abbreviations \"T\" and \"F\" stand for the evaluations TRUTH and FALSITY applied to the variables in the propositional formula (e.g. the assertion: \"That cow is blue\" will have the truth-value \"T\" for Truth or \"F\" for Falsity, as the case may be.).\n\nThe connectives go by a number of different word-usages, e.g. \"a IMPLIES b\" is also said \"IF a THEN b\". Some of these are shown in the table.\n\n{|class=\"wikitable\"\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| width=\"48\" Height=\"12\" | \n| width=\"43.5\" | \n| width=\"45\" | \n| width=\"42\" | \n| width=\"60\" | \n| width=\"57.75\" | \n|style=\"background-color:#E5E0EC\" width=\"114.75\" | b only if a\n| width=\"187.5\" | \n| width=\"93\" | \n| width=\"87.75\" | \n| width=\"48\" | \n| width=\"63\" | \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| \n| \n| \n| \n| \n|style=\"background-color:#E5E0EC\" | b IS SUFFICIENT FOR a\n|style=\"background-color:#F2F2F2\" | b PRECISELY WHEN a\n| \n| \n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| \n| \n| \n| \n| \n|style=\"background-color:#E5E0EC\" | a IS NECESSARY FOR b\n|style=\"background-color:#F2F2F2\" | b IF AND ONLY IF a;  b IFF a\n| \n| \n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| \n| \n| \n| \n|style=\"background-color:#FDE9D9\" | inclusive OR\n|style=\"background-color:#E5E0EC\" | IF b THEN a\n|style=\"background-color:#F2F2F2\" | b IS NECESSARY AND SUFFICIENT FOR a\n| \n| \n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| \n|style=\"background-color:#EAF1DD\" | negation\n|style=\"background-color:#EAF1DD\" | negation\n|style=\"background-color:#DBE5F1\" | conjunction\n|style=\"background-color:#FDE9D9\" | disjunction\n|style=\"background-color:#E5E0EC\" | implication\n|style=\"background-color:#F2F2F2\" | biconditional\n| \n| \n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n! Height=\"12\" colspan=\"2\" | variables\n|style=\"background-color:#EAF1DD\" | NOT b\n|style=\"background-color:#EAF1DD\" | NOT a\n|style=\"background-color:#DBE5F1\" | b AND a\n|style=\"background-color:#FDE9D9\" | b OR a\n|style=\"background-color:#E5E0EC\" | b IMPLIES a\n|style=\"background-color:#F2F2F2\" | b IS [[Logical equivalence|logically equivalent]] TO a ***\n| f IS A tautology\n| NEITHER a NOR b\n| b stroke a\n| exclusive OR\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"font-weight:bold\" Height=\"12\" | b\n|style=\"font-weight:bold\" | a\n|style=\"background-color:#EAF1DD;font-weight:bold\" | &not;(b)\n|style=\"background-color:#EAF1DD;font-weight:bold\" | &not;(a)\n|style=\"background-color:#DBE5F1;font-weight:bold\" | (b &and; a)\n|style=\"background-color:#FDE9D9;font-weight:bold\" | (b &or; a)\n|style=\"background-color:#E5E0EC;font-weight:bold\" | (b &rarr; a)\n|style=\"background-color:#F2F2F2;font-weight:bold\" | (b &harr; a)\n| (f = formula)\n| (a NOR b)\n|style=\"font-weight:bold\" | (b|a)\n|style=\"font-weight:bold\" | various\n|-  align=\"center\"\n| Height=\"12\" | F\n| F\n|style=\"background-color:#EAF1DD\" | T\n|style=\"background-color:#EAF1DD\" | T\n|style=\"background-color:#DBE5F1\" | F\n|style=\"background-color:#FDE9D9\" | F\n|style=\"background-color:#E5E0EC;font-size:9pt\" | T\n|style=\"background-color:#F2F2F2;font-size:9pt\" | T\n|style=\"font-size:9pt\" | T\n|style=\"font-size:9pt\" | T\n| T\n|style=\"font-size:9pt\" | F\n|-  align=\"center\"\n| Height=\"12\" | F\n|style=\"font-size:9pt\" | T\n|style=\"background-color:#EAF1DD\" | T\n|style=\"background-color:#EAF1DD;font-size:9pt\" | F\n|style=\"background-color:#DBE5F1\" | F\n|style=\"background-color:#FDE9D9;font-size:9pt\" | T\n|style=\"background-color:#E5E0EC;font-size:9pt\" | T\n|style=\"background-color:#F2F2F2;font-size:9pt\" | F\n|style=\"font-size:9pt\" | T\n|style=\"font-size:9pt\" | F\n| T\n|style=\"font-size:9pt\" | T\n|-  align=\"center\"\n|style=\"font-size:9pt\" Height=\"12\" | T\n| F\n|style=\"background-color:#EAF1DD\" | F\n|style=\"background-color:#EAF1DD\" | T\n|style=\"background-color:#DBE5F1\" | F\n|style=\"background-color:#FDE9D9;font-size:9pt\" | T\n|style=\"background-color:#E5E0EC;font-size:9pt\" | F\n|style=\"background-color:#F2F2F2;font-size:9pt\" | F\n|style=\"font-size:9pt\" | T\n|style=\"font-size:9pt\" | F\n| T\n|style=\"font-size:9pt\" | T\n|-  align=\"center\"\n|style=\"font-size:9pt\" Height=\"12\" | T\n|style=\"font-size:9pt\" | T\n|style=\"background-color:#EAF1DD\" | F\n|style=\"background-color:#EAF1DD;font-size:9pt\" | F\n|style=\"background-color:#DBE5F1;font-size:9pt\" | T\n|style=\"background-color:#FDE9D9;font-size:9pt\" | T\n|style=\"background-color:#E5E0EC;font-size:9pt\" | T\n|style=\"background-color:#F2F2F2;font-size:9pt\" | T\n|style=\"font-size:9pt\" | T\n|style=\"font-size:9pt\" | F\n|style=\"font-size:9pt\" | F\n|style=\"font-size:9pt\" | F\n|}\n\n=== Engineering connectives ===\n[[File:Propositional formula connectives 1.png|313px|thumb|right| Engineering symbols have varied over the years, but these are commonplace. Sometimes they appear simply as boxes with symbols in them. \"a\" and \"b\" are called \"the inputs\" and \"c\" is called \"the output\". An output will typically \"connect to\" an input (unless it is the final connective); this accomplishes the mathematical notion of '''substitution'''.]]\n\nIn general, the engineering connectives are just the same as the mathematics connectives excepting they tend to evaluate with \"1\" = \"T\" and \"0\" = \"F\". This is done for the purposes of analysis/minimization and synthesis of formulas by use of the notion of ''minterms'' and [[Karnaugh map]]s (see below). Engineers also use the words '''logical product''' from [[Boole]]'s notion (a*a = a) and '''logical sum''' from [[William Stanley Jevons|Jevons]]' notion (a+a = a).<ref>While the notion of logical product is not so peculiar (e.g. 0*0=0, 0*1=0, 1*0=0, 1*1=1), the notion of (1+1=1 ''is'' peculiar; in fact (a \"+\" b) = (a + (b - a*b)) where \"+\" is the \"logical sum\" but + and - are the true arithmetic counterparts. Occasionally all four notions do appear in a formula: A AND B = 1/2*( A plus B minus ( A XOR B ) ] (cf p. 146 in John Wakerly 1978, ''Error Detecting Codes, Self-Checking Circuits and Applications, North-Holland, New York, {{isbn|0-444-00259-6}} pbk.)</ref>\n\n{|class=\"wikitable\" style=\"text-align:center\"\n|- style=\"font-size:9pt\" \n|  Height=\"12\" | \n|  | \n|  | \n|  | \n|  | \n! rowspan=\"2\"  | logical product\n! rowspan=\"2\"  | logical sum\n|  | \n|  | \n!  | half-adder (no carry)\n|- style=\"font-size:9pt\" \n| Height=\"12\" | \n| \n| \n| \n| \n| \n| \n! exclusive OR\n|- style=\"font-size:9pt\" \n! Height=\"12\" | row number\n! colspan=\"2\" | variables\n|style=\"background-color:#EAF1DD\" | NOT\n|style=\"background-color:#EAF1DD\" | NOT\n|style=\"background-color:#DBE5F1\" | AND\n|style=\"background-color:#FDE9D9\" | OR\n| NAND\n| NOR\n| XOR\n|-  \n|style=\"font-size:9pt\" Height=\"12\" | b*2<sup>1</sup>+a*2<sup>0</sup>\n|style=\"font-size:9pt;font-weight:bold\" | b\n|style=\"font-size:9pt;font-weight:bold\" | a\n|style=\"background-color:#EAF1DD;font-size:9pt;font-weight:bold\" | ~(b)\n|style=\"background-color:#EAF1DD;font-size:9pt;font-weight:bold\" | ~(a)\n|style=\"background-color:#DBE5F1;font-size:9pt;font-weight:bold\" | (b & a)\n|style=\"background-color:#FDE9D9;font-size:9pt;font-weight:bold\" | (b &or; a)\n|style=\"font-size:9pt;font-weight:bold\" | ~(b & a)\n|style=\"font-size:9pt;font-weight:bold\" | ~(b &or; a)\n|style=\"font-size:14pt\" | ⊕\n|-  \n|style=\"font-size:9pt\" Height=\"12\" | 0\n| 0\n| 0\n|style=\"background-color:#EAF1DD\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#FDE9D9\" | 0\n|style=\"font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 0\n|-  \n|style=\"font-size:9pt\" Height=\"12\" | 1\n| 0\n|style=\"font-size:9pt\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n|style=\"background-color:#EAF1DD;font-size:9pt\" | 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#FDE9D9;font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 0\n|style=\"font-size:9pt\" | 1\n|-  \n|style=\"font-size:9pt\" Height=\"12\" | 2\n|style=\"font-size:9pt\" | 1\n| 0\n|style=\"background-color:#EAF1DD\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#FDE9D9;font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 0\n|style=\"font-size:9pt\" | 1\n|-  \n|style=\"font-size:9pt\" Height=\"12\" | 3\n|style=\"font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 1\n|style=\"background-color:#EAF1DD\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n|style=\"background-color:#DBE5F1;font-size:9pt\" | 1\n|style=\"background-color:#FDE9D9;font-size:9pt\" | 1\n|style=\"font-size:9pt\" | 0\n|style=\"font-size:9pt\" | 0\n|style=\"font-size:9pt\" | 0\n|}\n\n===  CASE connective: IF … THEN … ELSE … ===\nThe IF … THEN … ELSE … connective appears as the simplest form of CASE operator of [[recursion theory]] and [[computation theory]] and is the connective responsible for conditional goto's (jumps, branches). From this one connective all other connectives can be constructed (see more below). Although \" IF c THEN b ELSE a \" sounds like an implication it is, in its most reduced form, a '''switch''' that makes a decision and offers as outcome only one of two alternatives \"a\" or \"b\" (hence the name [[switch statement]] in the [[C (programming language)|C]] programming language).<ref>A careful look at its Karnaugh map shows that IF...THEN...ELSE can also be expressed, in a rather round-about way, in terms of two exclusive-ORs: ( (b AND (c XOR a)) OR (a AND (c XOR b)) ) = d.</ref>\n\nThe following three propositions are equivalent (as indicated by the logical equivalence sign ≡ ):\n\n# ( IF 'counter is zero' THEN 'go to instruction ''b'' ' ELSE 'go to instruction ''a'' ') ≡\n# ( (c → b) & (~c → a) ) ≡  ( ( IF 'counter is zero' THEN 'go to instruction ''b'' ' ) AND ( IF 'It is NOT the case that counter is zero' THEN 'go to instruction ''a'' ) \"  ≡\n# ( (c & b) &or; (~c & a) ) ≡  \" ( 'Counter is zero' AND 'go to instruction ''b'' ) OR ( 'It is NOT the case that 'counter is zero' AND 'go to instruction ''a'' ) \"\n\nThus IF … THEN … ELSE—unlike implication—does not evaluate to an ambiguous \"TRUTH\" when the first proposition is false i.e. c = F in (c → b). For example, most people would reject the following compound proposition as a nonsensical ''non sequitur'' because the second sentence is ''not connected in meaning'' to the first.<ref>Robbin p. 3.</ref>\n: Example: The proposition \" IF 'Winston Churchill was Chinese' THEN 'The sun rises in the east' \" evaluates as a TRUTH given that 'Winston Churchill was Chinese' is a FALSEHOOD and 'The sun rises in the east' evaluates as a TRUTH.\n\nIn recognition of this problem, the sign → of formal implication in the propositional calculus is called [[material conditional|material implication]] to distinguish it from the everyday, intuitive implication.<ref>Rosenbloom p. 30 and p. 54ff discusses this problem of implication at some length. Most philosophers and mathematicians just accept the material definition as given above. But some do not, including the [[Intuitionism|intuitionists]]; they consider it a form of the law of excluded middle misapplied.</ref>\n\nThe use of the IF … THEN … ELSE construction avoids controversy because it offers a completely deterministic choice between two stated alternatives; it offers two \"objects\" (the two alternatives b and a), and it ''selects'' between them exhaustively and unambiguously.<ref>Indeed, exhaustive selection between alternatives -- '''mutual exclusion''' -- is required by the definition that Kleene gives the CASE operator (Kleene 1952229)</ref> In the truth table below, d1 is the formula: ( (IF c THEN b) AND (IF NOT-c THEN a) ). Its fully reduced form d2 is the formula: ( (c AND b) OR (NOT-c AND a). The two formulas are equivalent as shown by the columns \"=d1\" and \"=d2\". Electrical engineers call the fully reduced formula the AND-OR-SELECT operator. The CASE (or SWITCH) operator is an extension of the same idea to ''n'' possible, but mutually exclusive outcomes. Electrical engineers call the CASE operator a [[multiplexer]].\n\n{|\n|- style=\"font-size:9pt\" align=\"center\"\n| width=\"27.75\" Height=\"12\" | \n| width=\"20.25\" | \n| width=\"18.75\" | \n| width=\"18.75\" | \n| width=\"6.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"16.5\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n|style=\"background-color:#FDE9D9\" width=\"17.25\" | d1\n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"18\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"24.75\" | \n| width=\"5.25\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n|style=\"background-color:#FDE9D9\" width=\"15.75\" | d2\n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n| width=\"27\" | \n|- style=\"font-size:9pt;font-weight:bold\" align=\"center\"\n!style=\"background-color:#F2F2F2\" Height=\"12\" | row\n! c\n! b\n! a\n|style=\"background-color:#A5A5A5\" | \n| (\n| (\n| c\n| →\n| b\n| )\n|style=\"background-color:#FDE9D9\" | &\n| (\n|style=\"background-color:#EAF1DD\" | ~\n| (\n| c\n| )\n| →\n| a\n| )\n| )\n|style=\"background-color:#FDE9D9\" |  =d1\n|style=\"background-color:#A5A5A5\" | \n| (\n| (\n| c\n|style=\"background-color:#DBEEF3\" | &\n| b\n| )\n|style=\"background-color:#FDE9D9\" | &or;\n| (\n|style=\"background-color:#EAF1DD\" | ~\n| (\n| c\n| )\n|style=\"background-color:#DBE5F1\" | &\n| a\n| )\n| )\n|style=\"background-color:#FDE9D9\" |  =d2\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 0\n| 0\n| 0\n|style=\"background-color:#C5D9F1\" | 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 1\n| 0\n| \n|style=\"background-color:#B8CCE4\" | 0\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 0\n|style=\"background-color:#B8CCE4\" | 0\n| \n| \n|style=\"background-color:#B8CCE4\" | 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 0\n| 0\n| \n|style=\"background-color:#B8CCE4\" | 0\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 0\n|style=\"background-color:#B8CCE4\" | 0\n| \n| \n|style=\"background-color:#B8CCE4\" | 0\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 1\n| 0\n| 0\n|style=\"background-color:#C5D9F1\" | 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 1\n| 0\n| \n|style=\"background-color:#B8CCE4\" | 1\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 1\n|style=\"background-color:#B8CCE4\" | 1\n| \n| \n|style=\"background-color:#B8CCE4\" | 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 0\n| 0\n| \n|style=\"background-color:#B8CCE4\" | 1\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 1\n|style=\"background-color:#B8CCE4\" | 1\n| \n| \n|style=\"background-color:#B8CCE4\" | 1\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 2\n| 0\n| 1\n|style=\"background-color:#C5D9F1\" | 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 1\n| 1\n| \n|style=\"background-color:#B8CCE4\" | 0\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 0\n|style=\"background-color:#B8CCE4\" | 0\n| \n| \n|style=\"background-color:#B8CCE4\" | 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 0\n| 1\n| \n|style=\"background-color:#B8CCE4\" | 0\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 0\n|style=\"background-color:#B8CCE4\" | 0\n| \n| \n|style=\"background-color:#B8CCE4\" | 0\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 3\n| 0\n| 1\n|style=\"background-color:#C5D9F1\" | 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 1\n| 1\n| \n|style=\"background-color:#B8CCE4\" | 1\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 1\n|style=\"background-color:#B8CCE4\" | 1\n| \n| \n|style=\"background-color:#B8CCE4\" | 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 0\n| 0\n| 1\n| \n|style=\"background-color:#B8CCE4\" | 1\n| \n| 1\n| \n| 0\n| \n|style=\"background-color:#B8CCE4\" | 1\n|style=\"background-color:#B8CCE4\" | 1\n| \n| \n|style=\"background-color:#B8CCE4\" | 1\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 4\n| 1\n|style=\"background-color:#DBEEF3\" | 0\n| 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 0\n|style=\"background-color:#DBEEF3\" | 0\n| \n|style=\"background-color:#DBEEF3\" | 0\n| \n| 0\n| \n| 1\n| \n| 1\n| 0\n| \n| \n|style=\"background-color:#DBEEF3\" | 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 0\n|style=\"background-color:#DBEEF3\" | 0\n| \n|style=\"background-color:#DBEEF3\" | 0\n| \n| 0\n| \n| 1\n| \n| 0\n| 0\n| \n| \n|style=\"background-color:#DBEEF3\" | 0\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 5\n| 1\n|style=\"background-color:#DBEEF3\" | 0\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 0\n|style=\"background-color:#DBEEF3\" | 0\n| \n|style=\"background-color:#DBEEF3\" | 0\n| \n| 0\n| \n| 1\n| \n| 1\n| 1\n| \n| \n|style=\"background-color:#DBEEF3\" | 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 0\n|style=\"background-color:#DBEEF3\" | 0\n| \n|style=\"background-color:#DBEEF3\" | 0\n| \n| 0\n| \n| 1\n| \n| 0\n| 1\n| \n| \n|style=\"background-color:#DBEEF3\" | 0\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 6\n| 1\n|style=\"background-color:#DBEEF3\" | 1\n| 0\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 1\n|style=\"background-color:#DBEEF3\" | 1\n| \n|style=\"background-color:#DBEEF3\" | 1\n| \n| 0\n| \n| 1\n| \n| 1\n| 0\n| \n| \n|style=\"background-color:#DBEEF3\" | 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 1\n|style=\"background-color:#DBEEF3\" | 1\n| \n|style=\"background-color:#DBEEF3\" | 1\n| \n| 0\n| \n| 1\n| \n| 0\n| 0\n| \n| \n|style=\"background-color:#DBEEF3\" | 1\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 7\n| 1\n|style=\"background-color:#DBEEF3\" | 1\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 1\n|style=\"background-color:#DBEEF3\" | 1\n| \n|style=\"background-color:#DBEEF3\" | 1\n| \n| 0\n| \n| 1\n| \n| 1\n| 1\n| \n| \n|style=\"background-color:#DBEEF3\" | 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| 1\n|style=\"background-color:#DBEEF3\" | 1\n|style=\"background-color:#DBEEF3\" | 1\n| \n|style=\"background-color:#DBEEF3\" | 1\n| \n| 0\n| \n| 1\n| \n| 0\n| 1\n| \n| \n|style=\"background-color:#DBEEF3\" | 1\n|}\n\n=== IDENTITY and evaluation ===\n\nThe first table of this section stars *** the entry logical equivalence to note the fact that \"[[Logical equivalence]]\" is not the same thing as \"identity\". For example, most would agree that the assertion \"That cow is blue\" is identical to the assertion \"That cow is blue\". On the other hand, ''logical'' equivalence sometimes appears in speech as in this example: \" 'The sun is shining' means 'I'm biking' \" Translated into a propositional formula the words become: \"IF 'the sun is shining' THEN 'I'm biking', AND IF 'I'm biking' THEN 'the sun is shining'\":<ref>The use of quote marks around the expressions is not accidental. Tarski comments on the use of quotes in his \"18. Identity of things and identity of their designations; use of quotation marks\" p. 58ff.</ref>\n: \"IF 's' THEN 'b' AND IF 'b' THEN 's' \" is written as ((s → b) & (b → s)) or in an abbreviated form as (s ↔ b). As the rightmost symbol string is a '''definition''' for a new symbol in terms of the symbols on the left, the use of the IDENTITY sign = is appropriate:\n:: ((s → b) & (b → s)) = (s ↔ b)\n\nDifferent authors use different signs for logical equivalence: ↔ (e.g. Suppes, Goodstein, Hamilton), ≡ (e.g. Robbin), ⇔ (e.g. Bender and Williamson). Typically identity is written as the equals sign =. One exception to this rule is found in ''Principia Mathematica''. For more about the philosophy of the notion of IDENTITY see [[Identity of indiscernibles|Leibniz's law]].\n\nAs noted above, Tarski considers IDENTITY to lie outside the propositional calculus, but he asserts that without the notion, \"logic\" is insufficient for mathematics and the deductive sciences. In fact the sign comes into the propositional calculus when a formula is to be evaluated.<ref>Hamilton p. 37. Bender and Williamson p. 29 state \"In what follows, we'll replace \"equals\" with the symbol \" ⇔ \" (equivalence) which is usually used in logic. We use the more familiar \" = \" for assigning meaning and values.\"</ref>\n\nIn some systems there are no truth tables, but rather just formal axioms (e.g. strings of symbols from a set { ~, →, (, ), variables p<sub>1</sub>, p<sub>2</sub>, p<sub>3</sub>, … } and formula-formation rules (rules about how to make more symbol strings from previous strings by use of e.g. substitution and [[modus ponens]]). the result of such a calculus will be another formula (i.e. a well-formed symbol string). Eventually, however, if one wants to use the calculus to study notions of validity and truth, one must add axioms that define the behavior of the symbols called \"the truth values\" {T, F} ( or {1, 0}, etc.) relative to the other symbols.\n\nFor example, Hamilton uses two symbols = and ≠ when he defines the notion of a '''valuation v''' of any [[Well-formed_formula|wffs]] ''A'' and ''B'' in his \"formal statement calculus\" L. A valuation '''v''' is a ''[[Function (mathematics)|function]]'' from the wffs of his system L to the range (output) { T, F }, given that each variable  p<sub>1</sub>, p<sub>2</sub>, p<sub>3</sub> in a wff is assigned an arbitrary truth value { T, F }.\n{{NumBlk|*|  '''v'''(''A'') ≠ '''v'''(~''A'')|{{EquationRef|i}}}}\n{{NumBlk|*|  '''v'''(''A'' → ''B'') {{=}} F if and only if '''v'''(''A'') {{=}} T and '''v'''(''B'') {{=}} F|{{EquationRef|ii}}}}\n\nThe two definitions ({{EquationNote|i}}) and ({{EquationNote|ii}}) define the equivalent of the truth tables for the ~ (NOT) and → (IMPLICATION) connectives of his system. The first one derives F ≠ T and T ≠ F, in other words \" '''v'''(''A'') does not '''mean''' '''v'''(~''A'')\". Definition ({{EquationNote|ii}}) specifies the third row in the truth table, and the other three rows then come from an application of definition ({{EquationNote|i}}). In particular ({{EquationNote|ii}}) '''assigns''' the value F (or a meaning of \"F\") to the entire expression. The definitions also serve as formation rules that allow substitution of a value previously derived into a formula:\n{|\n|- style=\"font-size:9pt\" align=\"center\"\n| width=\"8.25\" Height=\"12\" | \n| width=\"25.5\" | \n|style=\"background-color:#E5E0EC\" width=\"50\" | v(A→B)\n| width=\"29.25\" | \n| width=\"6.75\" | \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | (\n| v(A)\n|style=\"background-color:#E5E0EC\" |  →\n| v(B)\n| )\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| F\n|style=\"background-color:#E5E0EC\" | T\n| F\n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| F\n|style=\"background-color:#E5E0EC\" | T\n| T\n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| T\n|style=\"background-color:#CCC0DA\" | F\n| F\n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | \n| T\n|style=\"background-color:#E5E0EC\" | T\n| T\n| \n|}\n\nSome [[formal system]]s specify these valuation axioms at the outset in the form of certain formulas such as the [[law of contradiction]] or laws of identity and nullity. The choice of which ones to use, together with laws such as commutation and distribution, is up to the system's designer as long as the set of axioms is '''complete''' (i.e. sufficient to form and to evaluate any well-formed formula created in the system).\n\n== More complex formulas ==\nAs shown above, the CASE (IF c THEN b ELSE a ) connective is constructed either from the 2-argument connectives IF … THEN … and AND or from OR and AND and the 1-argument NOT. Connectives such as the n-argument AND (a & b & c & … & n), OR (a &or; b &or; c &or; … &or; n) are constructed from strings of two-argument AND and OR and written in abbreviated form without the parentheses. These, and other connectives as well, can then used as building blocks for yet further connectives. Rhetoricians, philosophers, and mathematicians use truth tables and the various theorems to analyze and simplify their formulas.\n\nElectrical engineering uses drawn symbols and connect them with lines that stand for the mathematicals act of '''substitution''' and '''replacement'''. They then verify their drawings with truth tables and simplify the expressions as shown below by use of [[Karnaugh map]]s or the theorems. In this way engineers have created a host of \"combinatorial logic\" (i.e. connectives without feedback) such as \"decoders\", \"encoders\", \"mutifunction gates\", \"majority logic\", \"binary adders\", \"arithmetic logic units\", etc.\n\n=== Definitions ===\nA definition creates a new symbol and its behavior, often for the purposes of abbreviation. Once the definition is presented, either form of the equivalent symbol or formula can be used. The following symbolism =<sub>Df</sub> is following the convention of Reichenbach.<ref>Reichenbach p. 20-22 and follows the conventions of PM. The symbol =<sub>Df</sub> is in the [[metalanguage]] and is not a formal symbol with the following meaning: \"by symbol ' s ' is to have the same meaning as the formula '(c & d)' \".</ref> Some examples of convenient definitions drawn from the symbol set { ~, &, (, ) } and variables. Each definition is producing a logically equivalent formula that can be used for substitution or replacement.\n:* definition of a new variable: (c & d) =<sub>Df</sub> s\n:* OR: ~(~a & ~b) =<sub>Df</sub> (a &or; b)\n:* IMPLICATION: (~a &or; b) =<sub>Df</sub> (a → b)\n:* XOR: (~a & b) &or; (a & ~b) =<sub>Df</sub> (a ⊕ b)\n:* LOGICAL EQUIVALENCE: ( (a → b) & (b → a) ) =<sub>Df</sub> ( a ≡ b )\n\n===Axiom and definition ''schemas''===\nThe definitions above for OR, IMPLICATION, XOR, and logical equivalence are actually [[axiom schema|schema]]s (or \"schemata\"), that is, they are ''models'' (demonstrations, examples) for a general formula ''format'' but shown (for illustrative purposes) with specific letters a, b, c for the variables, whereas any variable letters can go in their places as long as the letter substitutions follow the rule of substitution below.\n: Example: In the definition (~a &or; b) =<sub>Df</sub> (a → b), other variable-symbols such as \"SW2\" and \"CON1\" might be used, i.e. formally:\n:: a =<sub>Df</sub> SW2, b =<sub>Df</sub> CON1, so we would have as an ''instance'' of the definition schema (~SW2 &or; CON1) =<sub>Df</sub> (SW2 → CON1)\n\n=== Substitution versus replacement ===\n'''Substitution''': The variable or sub-formula to be substituted with another variable, constant, or sub-formula must be replaced in all instances throughout the overall formula.\n: Example: (c & d) &or; (p & ~(c & ~d)), but  (q1 & ~q2) ≡ d. Now wherever variable \"d\" occurs, substitute (q<sub>1</sub> & ~q<sub>2</sub>):\n:: (c & (q<sub>1</sub> & ~q<sub>2</sub>)) &or; (p & ~(c & ~(q<sub>1</sub> & ~q<sub>2</sub>)))\n\n'''Replacement''': (i) the formula to be replaced must be within a tautology, i.e. ''logically equivalent'' ( connected by ≡ or ↔) to the formula that replaces it, and (ii) unlike substitution its permissible for the replacement to occur only in one place (i.e. for one formula).\n: Example: Use this set of formula schemas/equivalences: \n:# ( (a &or; 0) ≡ a ). \n:# ( (a & ~a) ≡ 0 ). \n:# ( (~a &or; b) =<sub>Df</sub> (a → b) ). \n:# <li value=\"6\"> ( ~(~a) ≡ a )</li>\n:{{ordered list|list-style-type=lower-alpha\n| start with \"a\": a\n| Use 1 to replace \"a\" with (a &or; 0): (a &or; 0)\n| Use the notion of \"schema\" to substitute b for a in 2: ( (a & ~a) ≡ 0 )\n| Use 2 to replace 0 with (b & ~b): ( a &or; (b & ~b) )\n| (see below for how to distribute \"a &or;\" over (b & ~b), etc.)\n}}\n\n== Inductive definition ==\n\nThe classical presentation of propositional logic (see [[Herbert Enderton|Enderton]] 2002) uses the connectives <math>\\lnot, \\land, \\lor, \\to, \\leftrightarrow</math>. The set of formulas over a given set of propositional variables is [[inductive definition|inductively defined]] to be the smallest set of expressions such that:\n* Each propositional variable in the set is a formula,\n* <math>(\\lnot \\alpha)</math> is a formula whenever <math>\\alpha</math> is, and\n* <math> (\\alpha\\,\\Box\\,\\beta)</math> is a formula whenever <math>\\alpha</math> and <math>\\beta</math> are formulas and <math>\\Box</math> is one of the binary connectives <math>\\land, \\lor, \\to, \\leftrightarrow</math>.\nThis inductive definition can be easily extended to cover additional connectives.\n\nThe inductive definition can also be rephrased in terms of a [[closure (mathematics)|closure]] operation (Enderton 2002). Let ''V'' denote a set of propositional variables and let ''X<sub>V</sub>'' denote the set of all strings from an alphabet including symbols in ''V'', left and right parentheses, and all the logical connectives under consideration. Each logical connective corresponds to a formula building operation, a function from ''XX<sub>V</sub>'' to ''XX<sub>V</sub>'':\n* Given a string ''z'', the operation <math>\\mathcal{E}_\\lnot(z)</math> returns <math>(\\lnot z)</math>.\n* Given strings ''y'' and ''z'', the operation <math>\\mathcal{E}_\\land(y,z)</math> returns <math>(y\\land x)</math>. There are similar operations <math>\\mathcal{E}_\\lor</math>, <math>\\mathcal{E}_\\to</math>, and <math>\\mathcal{E}_\\leftrightarrow</math> corresponding to the other binary connectives.\nThe set of formulas over ''V'' is defined to be the smallest subset of ''XX<sub>V</sub>'' containing ''V'' and closed under all the formula building operations.\n\n== Parsing formulas ==\nThe following \"laws\" of the propositional calculus are used to \"reduce\" complex formulas. The \"laws\" can be verified easily with truth tables. For each law, the principal (outermost) connective is associated with logical equivalence ≡ or identity =. A complete analysis of all 2<sup>n</sup> combinations of truth-values for its ''n'' distinct variables will result in a column of 1's (T's) underneath this connective. This finding makes each law, by definition, a tautology. And, for a given law, because its formula on the left and right are equivalent (or identical) they can be substituted for one another.\n* Example: The following truth table is De Morgan's law for the behavior of NOT over OR: ~(a &or; b) ≡ (~a & ~b). To the left of the principal connective ≡ (yellow column labelled \"taut\") the formula ~(b &or; a) evaluates to (1, 0, 0, 0) under the label \"P\". On the right of \"taut\" the formula (~(b) &or; ~(a)) also evaluates to (1, 0, 0, 0) under the label \"Q\". As the two columns have equivalent evaluations, the logical equivalence ≡ under \"taut\" evaluates to (1, 1, 1, 1), i.e. P ≡ Q. Thus either formula can be substituted for the other if it appears in a larger formula.\n{| style=\"margin-left: auto; margin-right: auto; border: none;\"\n|- style=\"font-size:9pt; text-align:center\"\n| width=\"18.75\" Height=\"12\" | \n| width=\"18.75\" | \n| width=\"4.5\" | \n| width=\"10.5\" | \n|style=\"background-color:#D7E4BC\" width=\"10.5\" | P\n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"11.25\" | \n|style=\"background-color:#FFFF99\" width=\"21.75\" | taut\n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"12\" | \n|style=\"background-color:#DBE5F1\" width=\"12.75\" | Q\n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n|- style=\"font-weight:bold\" align=\"center\"\n|style=\"font-size:9pt\" Height=\"15\" | b\n|style=\"font-size:9pt\" | a\n|style=\"background-color:#A5A5A5;font-size:9pt\" | \n|style=\"font-size:9pt\" | (\n|style=\"background-color:#D7E4BC;font-size:9pt\" | ~\n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | b\n|style=\"background-color:#FDE9D9;font-size:9pt\" | V\n|style=\"font-size:9pt\" | a\n|style=\"font-size:9pt\" | )\n|style=\"background-color:#FFFF99\" | ≡\n|style=\"font-size:9pt\" | (\n|style=\"background-color:#EAF1DD;font-size:9pt\" | ~\n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | b\n|style=\"font-size:9pt\" | )\n|style=\"background-color:#DBE5F1;font-size:9pt\" | &\n|style=\"background-color:#EAF1DD;font-size:9pt\" | ~\n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | a\n|style=\"font-size:9pt\" | )\n|style=\"font-size:9pt\" | )\n|style=\"font-size:9pt\" | )\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 0\n|style=\"background-color:#A5A5A5\" | \n| \n|style=\"background-color:#D7E4BC\" | 1\n| \n| 0\n|style=\"background-color:#FDE9D9\" | 0\n| 0\n| \n|style=\"background-color:#FFFF99\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n|style=\"background-color:#D7E4BC\" | 0\n| \n| 0\n|style=\"background-color:#FDE9D9\" | 1\n| 1\n| \n|style=\"background-color:#FFFF99\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 0\n|style=\"background-color:#A5A5A5\" | \n| \n|style=\"background-color:#D7E4BC\" | 0\n| \n| 1\n|style=\"background-color:#FDE9D9\" | 1\n| 0\n| \n|style=\"background-color:#FFFF99\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n|style=\"background-color:#D7E4BC\" | 0\n| \n| 1\n|style=\"background-color:#FDE9D9\" | 1\n| 1\n| \n|style=\"background-color:#FFFF99\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n| \n|}\n\nEnterprising readers might challenge themselves to invent an \"axiomatic system\" that uses the symbols { &or;, &, ~, (, ), variables a, b, c }, the formation rules specified above, and as few as possible of the laws listed below, and then derive as theorems the others as well as the truth-table valuations for &or;, &, and ~. One set attributed to Huntington (1904) (Suppes:204) uses eight of the laws defined below.\n\nNote that if used in an axiomatic system, the symbols 1 and 0 (or T and F) are considered to be wffs and thus obey all the same rules as the variables. Thus the laws listed below are actually [[axiom schema]]s, that is, they stand in place of an infinite number of instances. Thus ( x &or; y ) ≡ ( y &or; x ) might be used in one instance, ( p &or; 0 ) ≡ ( 0 &or; p ) and in another instance ( 1 &or; q ) ≡ ( q &or; 1 ), etc.\n\n=== Connective seniority (symbol rank) ===\nIn general, to avoid confusion during analysis and evaluation of propositional formulas make liberal use parentheses. However, quite often authors leave them out. To parse a complicated formula one first needs to know the '''seniority''', or '''rank''', that each of the connectives (excepting *) has over the other connectives. To \"well-form\" a formula, start with the connective with the highest rank and add parentheses around its components, then move down in rank (paying close attention to the connective's '''scope''' over which the it is working). From most- to least-senior, with the predicate signs ∀x and ∃x, the IDENTITY = and arithmetic signs added for completeness:<ref>Rosenbloom 1950:32. Kleene 1952:73-74 ranks all 11 symbols.</ref>\n:; ≡: (LOGICAL EQUIVALENCE)\n:; →: (IMPLICATION)\n:; &: (AND)\n:; &or;: (OR)\n:; ~: (NOT)\n:; ∀x: (FOR ALL x)\n:; ∃x: (THERE EXISTS AN x)\n:; =: (IDENTITY)\n:; +: (arithmetic sum)\n:;<nowiki>*</nowiki>: (arithmetic multiply)\n:; ' : (s, arithmetic successor).\n\nThus the formula can be parsed—but note that, because NOT does not obey the distributive law, the parentheses around the inner formula (~c & ~d) is mandatory:\n: Example: \" d & c &or; w \" rewritten is ( (d & c) &or; w )\n: Example: \" a & a → b ≡ a & ~a &or; b \" rewritten (rigorously) is\n::* ≡ has seniority: ( ( a & a → b ) ≡ ( a & ~a &or; b ) )\n::* → has seniority: ( ( a & (a → b) ) ≡ ( a & ~a &or; b ) )\n::* & has seniority both sides: ( ( ( (a) & (a → b) ) ) ≡ ( ( (a) & (~a &or; b) ) )\n::* ~ has seniority: ( ( ( (a) & (a → b) ) ) ≡ ( ( (a) & (~(a) &or; b) ) )\n::* check 9 ( -parenthesis and 9 ) -parenthesis: ( ( ( (a) & (a → b) ) ) ≡ ( ( (a) & (~(a) &or; b) ) )\n: Example:\n:: d & c &or; p & ~(c & ~d) ≡ c & d &or; p & c &or; p & ~d rewritten is ( ( (d & c) &or; ( p & ~((c & ~(d)) ) ) ) ≡ ( (c & d) &or; (p & c) &or; (p & ~(d)) ) )\n\n=== Commutative and associative laws ===\n\nBoth AND and OR obey the [[commutative law]] and [[associative law]]:\n* Commutative law for OR: ( a &or; b ) ≡ ( b &or; a )\n* Commutative law for AND: ( a & b ) ≡ ( b & a )\n* Associative law for OR: (( a &or; b ) &or; c ) ≡ ( a &or; (b &or; c) )\n* Associative law for AND: (( a & b ) & c ) ≡ ( a & (b & c) )\n\n'''Omitting parentheses in strings of AND and OR''': The connectives are considered to be unary (one-variable, e.g. NOT) and binary (i.e. two-variable AND, OR, IMPLIES). For example:\n: ( (c & d) &or; (p & c) &or; (p & ~d) ) above should be written ( ((c & d) &or; (p & c)) &or; (p & ~(d) ) ) or possibly ( (c & d) &or; ( (p & c) &or; (p & ~(d)) ) )\nHowever, a truth-table demonstration shows that the form without the extra parentheses is perfectly adequate.\n\n'''Omitting parentheses with regards to a single-variable NOT''': While ~(a) where a is a single variable is perfectly clear, ~a is adequate and is the usual way this [[literal (mathematical logic)|literal]] would appear. When the NOT is over a formula with more than one symbol, then the parentheses are mandatory, e.g. ~(a &or; b).\n\n=== Distributive laws ===\nOR distributes over AND and AND distributes over OR. NOT does not distribute over AND or OR. See below about De Morgan's law:\n* Distributive law for OR: ( c &or; ( a & b) ) ≡ ( (c &or; a) & (c &or; b) )\n* Distributive law for AND: ( c & ( a &or; b) ) ≡ ( (c & a) &or; (c & b) )\n\n=== De Morgan's laws ===\nNOT, when distributed over OR or AND, does something peculiar (again, these can be verified with a truth-table):\n* De Morgan's law for OR: ¬(a &or; b) ≡ (¬a ^ ¬b)\n* De Morgan's law for AND: ¬(a ^ b) ≡ (¬a &or; ¬b)\n\n=== Laws of absorption ===\nAbsorption, in particular the first one, causes the \"laws\" of logic to differ from the \"laws\" of arithmetic:\n* Absorption (idempotency) for OR: (a &or; a) ≡ a\n* Absorption (idempotency) for AND: (a & a) ≡ a\n\n=== Laws of evaluation: Identity, nullity, and complement ===\nThe sign \" = \" (as distinguished from logical equivalence ≡, alternately ↔ or ⇔) symbolizes the assignment of value or meaning. Thus the string (a & ~(a)) symbolizes \"0\", i.e. it '''means''' the same thing as symbol \"0\" \". In some \"systems\" this will be an axiom (definition) perhaps shown as ( (a & ~(a)) =<sub>Df</sub> 0 ); in other systems, it may be derived in the truth table below:\n{| style=\"margin-left: auto; margin-right: auto; border: none;\"\n|- style=\"font-size:9pt; text-align:center\"\n| width=\"18.75\" Height=\"12\" | \n| width=\"4.5\" | \n| width=\"9\" | \n| width=\"9\" | \n| width=\"10.5\" | \n|style=\"background-color:#C5D9F1\" width=\"10.5\" | c\n| width=\"9.75\" | \n| width=\"9.75\" | \n| width=\"15.75\" | \n| width=\"8.25\" | \n| width=\"8.25\" | \n|style=\"background-color:#FFFF99\" width=\"18\" | taut\n| width=\"13.5\" | c\n| width=\"11.25\" | \n|- style=\"font-weight:bold\" align=\"center\"\n|style=\"font-size:9pt\" Height=\"15\" | a\n|style=\"font-size:9pt\" | \n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | a\n|style=\"background-color:#C5D9F1;font-size:9pt\" | &\n|style=\"background-color:#EAF1DD;font-size:9pt\" | ~\n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | a\n|style=\"font-size:9pt\" | )\n|style=\"font-size:9pt\" | )\n|style=\"background-color:#FFFF99\" | ≡\n|style=\"font-size:9pt\" | 0\n|style=\"font-size:9pt\" | )\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| \n| \n| \n| 0\n|style=\"background-color:#C5D9F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n|style=\"background-color:#FFFF99\" | 1\n| 0\n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| \n| \n| \n| 1\n|style=\"background-color:#C5D9F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n|style=\"background-color:#FFFF99\" | 1\n| 0\n| \n|}\n\n* Commutation of equality: (a = b) ≡ (b = a)\n* Identity for OR: (a &or; 0) = a or (a &or; F) = a\n* Identity for AND: (a & 1) = a or (a & T) = a\n* Nullity for OR: (a &or; 1) = 1  or (a &or; T) = T\n* Nullity for AND: (a & 0) = 0  or (a & F) = F\n* Complement for OR: (a &or; ~a) = 1 or (a &or; ~a) = T, [[law of excluded middle]]\n* Complement for AND: (a & ~a) = 0 or (a & ~a) = F, [[law of contradiction]]\n\n=== Double negative (involution) ===\n* ¬(¬a) ≡ a\n\n== Well-formed formulas (wffs) ==\nA key property of formulas is that they can be uniquely parsed to determine the structure of the formula in terms of its propositional variables and logical connectives. When formulas are written in [[infix notation]], as above, unique readability is ensured through an appropriate use of parentheses in the definition of formulas. Alternatively, formulas can be written in [[Polish notation]] or [[reverse Polish notation]], eliminating the need for parentheses altogether.\n\nThe inductive definition of infix formulas in the previous section can be converted to a [[formal grammar]] in [[Backus-Naur form]]:\n\n<source lang=\"bnf\">\n<formula> ::= <propositional variable>\n| ( ¬ <formula> )\n| ( <formula> ∧ <formula>)\n| ( <formula> ∨ <formula> )\n| ( <formula> → <formula> )\n| ( <formula> ↔ <formula> )\n</source>\nIt can be shown that any expression matched by the grammar has a balanced number of left and right parentheses, and any nonempty initial segment of a formula has more left than right parentheses.<ref>cf Minsky 1967:75, section 4.2.3 \"The method of parenthesis counting\". Minsky presents a state machine that will do the job, and by use of induction (recursive definition) Minsky proves the \"method\" and presents a theorem as the result. A fully generalized \"parenthesis grammar\" requires an infinite state machine (e.g. a Turing machine) to do the counting.</ref> This fact can be used to give an algorithm for parsing formulas. For example, suppose that an expression ''x'' begins with <math>( \\lnot</math>. Starting after the second symbol, match the shortest subexpression ''y'' of ''x'' that has balanced parentheses. If ''x'' is a formula, there is exactly one symbol left after this expression, this symbol is a closing parenthesis, and ''y'' itself is a formula. This idea can be used to generate a [[recursive descent parser]] for formulas.\n\n'''Example of parenthesis counting''':\n\nThis method locates as \"1\" the '''principal connective''' {{--}} the connective under which the overall evaluation of the formula occurs for the outer-most parentheses (which are often omitted).<ref>Robbin p. 7</ref> It also locates the inner-most connective where one would begin evaluatation of the formula without the use of a truth table, e.g. at \"level 6\".\n{|class=\"wikitable\"\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| width=\"32.25\" Height=\"12\" | \n! width=\"27.75\" | start\n|style=\"background-color:#99FF99\" width=\"12\" | (\n|style=\"background-color:#99FF99\" width=\"12\" | (\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | c\n| width=\"12\" | &\n| width=\"12\" | d\n|style=\"background-color:#FFC000\" width=\"12\" | )\n| width=\"12\" | V\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | p\n| width=\"12\" | &\n| width=\"12\" | ~\n|style=\"background-color:#99FF99\" width=\"12\" | (\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | c\n| width=\"12\" | &\n| width=\"12\" | ~\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | d\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n| width=\"12\" | = \n|style=\"background-color:#99FF99\" width=\"12\" | (\n|style=\"background-color:#99FF99\" width=\"12\" | (\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | c\n| width=\"12\" | &\n| width=\"12\" | d\n|style=\"background-color:#FFC000\" width=\"12\" | )\n| width=\"12\" | V\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | p\n| width=\"12\" | &\n| width=\"12\" | d\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n| width=\"12\" | V\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | p\n| width=\"12\" | &\n| width=\"12\" | ~\n|style=\"background-color:#99FF99\" width=\"12\" | (\n| width=\"12\" | c\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|style=\"background-color:#FFC000\" width=\"12\" | )\n|- style=\"font-size:9pt\"\n! Height=\"13.5\" align=\"right\" valign=\"bottom\" | count\n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 2\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 2\n| align=\"center\" valign=\"bottom\" | 2\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 4\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 5\n| align=\"center\" valign=\"bottom\" | 5\n| align=\"center\" valign=\"bottom\" | 5\n| align=\"center\" valign=\"bottom\" | 5\n|style=\"background-color:#99FF99;font-weight:bold\" align=\"center\" valign=\"bottom\" | 6\n| align=\"center\" valign=\"bottom\" | 6\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 5\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 4\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 2\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 4\n| align=\"center\" valign=\"bottom\" | 4\n| align=\"center\" valign=\"bottom\" | 4\n| align=\"center\" valign=\"bottom\" | 4\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 4\n| align=\"center\" valign=\"bottom\" | 4\n| align=\"center\" valign=\"bottom\" | 4\n| align=\"center\" valign=\"bottom\" | 4\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 2\n| align=\"center\" valign=\"bottom\" | 2\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#99FF99\" align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 3\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 2\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#FFC000\" align=\"center\" valign=\"bottom\" | 0\n|}\n\n=== Wffs versus valid formulas in inferences ===\nThe notion of '''valid argument''' is usually applied to [[inference]]s in arguments, but arguments reduce to propositional formulas and can be evaluated the same as any other propositional formula. Here a '''valid''' inference means: \"The formula that represents the inference evaluates to \"truth\" beneath its principal connective, no matter what truth-values are assigned to its variables\", i.e. the formula is a tautology.<ref>cf Reichenbach p. 68 for a more involved discussion: \"If the inference is valid and the premises are true, the inference is called ''conclusive''.</ref>\nQuite possibly a formula will be ''well-formed'' but not '''valid'''. Another way of saying this is: \"Being well-formed is ''necessary'' for a formula to be valid but it is not ''sufficient''.\" The only way to find out if it is ''both'' well-formed ''and'' valid is to submit it to verification with a truth table or by use of the \"laws\":\n\n* Example 1: What does one make of the following difficult-to-follow assertion? Is it valid? \"If it's sunny, but if the frog is croaking then it's not sunny, then it's the same as saying that the frog isn't croaking.\" Convert this to a propositional formula as follows:\n*:: \" IF (a AND (IF b THEN NOT-a) THEN NOT-a\" where \" a \" represents \"its sunny\" and \" b \" represents \"the frog is croaking\":\n*:: ( ( (a) & ( (b) → ~(a) ) ≡ ~(b) )\n*: This is well-formed, but is it ''valid''? In other words, when evaluated will this yield a tautology (all T) beneath the logical-equivalence symbol ≡ ? The answer is NO, it is not valid. However, if reconstructed as an ''implication'' then the argument ''is'' valid.\n*: \"Saying it's sunny, but if the frog is croaking then it's not sunny, ''implies'' that the frog isn't croaking.\"\n*: Other circumstances may be preventing the frog from croaking: perhaps a crane ate it.\n\n* Example 2 (from Reichenbach via Bertrand Russell):\n*: \"If pigs have wings, some winged animals are good to eat. Some winged animals are good to eat, so pigs have wings.\"\n*: ( ((a) → (b)) & (b) → (a) ) is well formed, but an invalid argument as shown by the red evaluation under the principal implication:\n{|style=\"margin-left: auto; margin-right: auto; border: none;\"\n|- style=\"font-size:9pt\" align=\"center\"\n| width=\"18.75\" Height=\"12\" | W\n| width=\"18.75\" | G\n| width=\"4.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n| width=\"16.5\" | \n| width=\"10.5\" | \n| width=\"11.25\" | \n| width=\"16.5\" | \n| width=\"10.5\" | \n| width=\"10.5\" | \n|style=\"background-color:#CCC0DA\" width=\"19.5\" | arg\n| width=\"14.25\" | \n| width=\"12\" | \n|- style=\"font-weight:bold\" align=\"center\"\n|style=\"font-size:9pt\" Height=\"12.75\" | a\n|style=\"font-size:9pt\" | b\n|style=\"font-size:9pt\" | \n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | (\n|style=\"font-size:9pt\" | a\n|style=\"background-color:#F2DDDC;font-size:9pt\" |  ->\n|style=\"font-size:9pt\" | b\n|style=\"font-size:9pt\" | )\n|style=\"background-color:#DBE5F1\" | &\n|style=\"font-size:9pt\" | b\n|style=\"font-size:9pt\" | )\n|style=\"background-color:#CCC0DA;font-size:9pt\" |  ->\n|style=\"font-size:9pt\" | a\n|style=\"font-size:9pt\" | )\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 0\n| \n| \n| \n| \n| 0\n|style=\"background-color:#F2DDDC\" | 1\n| 0\n| \n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#CCC0DA\" | 1\n| 0\n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 1\n| \n| \n| \n| \n| 0\n|style=\"background-color:#F2DDDC\" | 1\n| 1\n| \n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n|style=\"background-color:red\" | 0\n| 0\n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 0\n| \n| \n| \n| \n| 1\n|style=\"background-color:#F2DDDC\" | 0\n| 0\n| \n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#CCC0DA\" | 1\n| 1\n| \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 1\n| \n| \n| \n| \n| 1\n|style=\"background-color:#F2DDDC\" | 1\n| 1\n| \n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n|style=\"background-color:#CCC0DA\" | 1\n| 1\n| \n|}\n\n== Reduced sets of connectives ==\n\n[[File:Propositional formula NANDs.png|400px|thumb|right|The engineering symbol for the NAND connective (the 'stroke') can be used to build any propositional formula. The notion that truth (1) and falsity (0) can be defined in terms of this connective is shown in the sequence of NANDs on the left, and the derivations of the four evaluations of a NAND b are shown along the bottom. The more common method is to use the definition of the NAND from the truth table.]]\n\nA set of logical connectives is called '''complete''' if every propositional formula is tautologically equivalent to a formula with just the connectives in that set. There are many complete sets of connectives, including <math>\\{\\land, \\lnot\\}</math>, <math>\\{\\lor, \\lnot\\}</math>, and <math>\\{\\to, \\lnot\\}</math>. There are two binary connectives that are complete on their own, corresponding to NAND and NOR, respectively.<ref>As well as the first three, Hamilton pp.19-22 discusses logics built from only | (NAND), and ↓ (NOR).</ref> Some pairs are not complete, for example <math>\\{\\land, \\lor\\}</math>.\n\n=== The stroke (NAND) ===\nThe binary connective corresponding to NAND is called the [[Sheffer stroke]], and written with a vertical bar | or vertical arrow ↑. The completeness of this connective was noted in ''Principia Mathematica'' (1927:xvii). Since it is complete on its own, all other connectives can be expressed using only the stroke. For example, where the symbol \" ≡ \" represents ''logical equivalence'':\n: ~p ≡ p|p\n: p → q ≡ p|~q\n: p &or; q ≡ ~p|~q\n: p & q ≡ ~(p|q)\nIn particular, the zero-ary connectives <math>\\top</math> (representing truth) and <math>\\bot</math> (representing falsity) can be expressed using the stroke:\n: <math>\\top \\equiv (a|(a|a))</math>\n: <math>\\bot \\equiv (\\top | \\top)</math>\n\n===  IF … THEN … ELSE ===\n\nThis connective together with { 0, 1 }, ( or { F, T } or { <math>\\bot</math>, <math>\\top</math> } ) forms a complete set. In the following the IF...THEN...ELSE [[Relation (mathematics)|relation]] (c, b, a) = d represents ( (c → b) &or; (~c → a) ) ≡ ( (c & b) &or; (~c & a) ) = d\n: (c, b, a):\n: (c, 0, 1) ≡ ~c\n: (c, b, 1) ≡ (c → b)\n: (c, c, a) ≡ (c &or; a)\n: (c, b, c) ≡ (c & b)\n\nExample: The following shows how a theorem-based proof of \"(c, b, 1) ≡ (c → b)\" would proceed, below the proof is its truth-table verification. ( Note: (c → b) is ''defined'' to be (~c &or; b) ):\n:* Begin with the reduced form: ( (c & b) &or; (~c & a) )\n:* Substitute \"1\" for a: ( (c & b) &or; (~c & 1) )\n:* Identity (~c & 1) = ~c: ( (c & b) &or; (~c) )\n:* Law of commutation for V:  ( (~c) &or; (c & b)  )\n:* Distribute \"~c V\" over (c & b): ( ((~c) &or; c ) & ((~c) &or; b )\n:* Law of excluded middle (((~c) &or; c ) = 1 ): ( (1) & ((~c) &or; b ) )\n:* Distribute \"(1) &\" over ((~c) &or; b): ( ((1) & (~c)) &or; ((1) & b )) )\n:* Commutivity and Identity (( 1 & ~c) = (~c & 1) = ~c, and (( 1 & b) ≡ (b & 1) ≡ b: ( ~c &or; b )\n:* ( ~c &or; b ) is defined as '''c → b''' Q. E. D.\n\nIn the following truth table the column labelled \"taut\" for tautology evaluates '''logical equivalence''' (symbolized here by ≡) between the two columns labelled d. Because all four rows under \"taut\" are 1's, the equivalence indeed represents a tautology.\n{|style=\"margin-left: auto; margin-right: auto; border: none;\"\n|- style=\"font-size:9pt; text-align:center\"\n| width=\"27.75\" Height=\"12\" | \n| width=\"20.25\" | \n| width=\"18.75\" | \n| width=\"18.75\" | \n| width=\"6.75\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n|style=\"background-color:#FDE9D9\" width=\"11.25\" | d\n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"12.75\" | \n|style=\"background-color:#DDD9C3\" width=\"19.5\" | taut\n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n| width=\"11.25\" | \n|style=\"background-color:#FDE9D9\" width=\"11.25\" | d\n| width=\"11.25\" | \n| width=\"12.75\" | \n| width=\"12.75\" | \n|- style=\"font-size:9pt;font-weight:bold\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | rows\n| c\n| b\n| a\n|style=\"background-color:#A5A5A5\" | \n| (\n| (\n| (\n| c\n|style=\"background-color:#DBE5F1\" | &\n| b\n| )\n|style=\"background-color:#FDE9D9\" | V\n| (\n|style=\"background-color:#EAF1DD\" | ~\n| (\n| c\n| )\n|style=\"background-color:#DBE5F1\" | &\n| a\n| )\n| )\n|style=\"background-color:#DDD9C3\" |  ≡\n| (\n|style=\"background-color:#EAF1DD\" | ~\n| (\n| c\n| )\n|style=\"background-color:#FDE9D9\" | V\n| b\n| )\n| )\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 0,1\n| 0\n| 0\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n| \n|style=\"background-color:#DDD9C3\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n|style=\"background-color:#FDE9D9\" | 1\n| 0\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 2,3\n| 0\n| 1\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n| \n|style=\"background-color:#DDD9C3\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n|style=\"background-color:#FDE9D9\" | 1\n| 1\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 4,5\n| 1\n| 0\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| \n| 1\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n| \n|style=\"background-color:#DDD9C3\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n|style=\"background-color:#FDE9D9\" | 0\n| 0\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 6,7\n| 1\n| 1\n| 1\n|style=\"background-color:#A5A5A5\" | \n| \n| \n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n| \n|style=\"background-color:#DDD9C3\" | 1\n| \n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| 1\n| \n| \n|}\n\n== Normal forms ==\n\nAn arbitrary propositional formula may have a very complicated structure. It is often convenient to work with formulas that have simpler forms, known as '''normal forms'''. Some common normal forms include [[conjunctive normal form]] and [[disjunctive normal form]]. Any propositional formula can be reduced to its conjunctive or disjunctive normal form.\n\n=== Reduction to normal form ===\n\n[[File:Propositional formula maps 1.png|450px|thumb|right| A truth table will contain 2<sup>n</sup> rows, where n is the number of variables (e.g. three variables \"p\", \"d\", \"c\" produce 2<sup>3</sup> rows). Each row represents a minterm. Each minterm can be found on the Hasse diagram, on the Veitch diagram, and on the Karnaugh map. (The evaluations of \"p\" shown in the truth table are not shown in the Hasse, Veitch and Karnaugh diagrams; these are shown in the Karnaugh map of the following section.)<!-- For example, row 2 represents the minterm (~p & d & ~c). If \"~v\" (where v is any variable) is thought of as \"0\" and \"v\" is thought of as \"1\", then the minterm can be thought of as a binary number, e.g. (~p & d & ~c) = 010<sub>2</sub> = 2<sub>10</sub>. A formula (e.g. the formula for q) evaluated for variabiles ''p'' = 0, ''d'' = 1, ''c'' = 0 will produce an output (e.g. q). -->]]\n\nReduction to normal form is relatively simple once a truth table for the formula is prepared. But further attempts to minimize the number of '''literals''' (see below) requires some tools: reduction by De Morgan's laws and [[truth table]]s can be unwieldy, but [[Karnaugh map]]s are very suitable a small number of variables (5 or less). Some sophisticated tabular methods exist for more complex circuits with multiple outputs but these are beyond the scope of this article; for more see [[Quine&ndash;McCluskey algorithm]].\n\n==== Literal, term and alterm ====\n\nIn electrical engineering a variable x or its negation ~(x) is lumped together into a single notion called a [[literal (mathematical logic)|literal]]. A string of literals connected by ANDs is called a '''term'''. A string of literals connected by OR is called an '''alterm'''. Typically the literal ~(x) is abbreviated ~x. Sometimes the &-symbol is omitted altogether in the manner of algebraic multiplication.\n\n* Examples\n*# a, b, c, d are variables. ((( a & ~(b) ) & ~(c)) & d) is a term. This can be abbreviated as (a & ~b & ~c & d), or a~b~cd.\n*# p, q, r, s are variables. (((p & ~(q) ) & r) & ~(s) ) is an alterm. This can be abbreviated as (p &or; ~q &or; r &or; ~s).\n\n==== Minterms ====\nIn the same way that a 2<sup>n</sup>-row truth table displays the evaluation of a propositional formula for all 2<sup>n</sup> possible values of its variables, n variables produces a 2<sup>n</sup>-square Karnaugh map (even though we cannot draw it in its full-dimensional realization). For example, 3 variables produces 2<sup>3</sup> = 8 rows and 8 Karnaugh squares; 4 variables produces 16 truth-table rows and 16 squares and therefore 16 [[minterms]]. Each Karnaugh-map square and its corresponding truth-table evaluation represents one minterm.\n\nAny propositional formula can be reduced to the \"logical sum\" (OR) of the active (i.e. \"1\"- or \"T\"-valued) minterms. When in this form the formula is said to be in [[disjunctive normal form]]. But even though it is in this form, it is not necessarily minimized with respect to either the number of terms or the number of literals.\n\nIn the following table, observe the peculiar numbering of the rows: (0, 1, 3, 2, 6, 7, 5, 4, 0). The first column is the decimal equivalent of the binary equivalent of the digits \"cba\", in other words:\n* Example\n*: cba<sub>2</sub> = c*2<sup>2</sup> + b*2<sup>1</sup> + a*2<sup>0</sup>:\n*: cba = (c=1, b=0, a=0) = 101<sub>2</sub> = 1*2<sup>2</sup> + 0*2<sup>1</sup> + 1*2<sup>0</sup> = 5<sub>10</sub>\n\nThis numbering comes about because as one moves down the table from row to row only one variable at a time changes its value. [[Gray code]] is derived from this notion. This notion can be extended to three and four-dimensional [[hypercube]]s called [[Hasse diagram]]s where each corner's variables change only one at a time as one moves around the edges of the cube. Hasse diagrams (hypercubes) flattened into two dimensions are either [[Veitch diagram]]s or [[Karnaugh map]]s (these are virtually the same thing).\n\nWhen working with Karnaugh maps one must always keep in mind that the top edge \"wrap arounds\" to the bottom edge, and the left edge wraps around to the right edge—the Karnaugh diagram is really a three- or four- or n-dimensional flattened object.\n\n{|class=\"wikitable\"\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n! width=\"60\" Height=\"39\" | decimal equivalent of (c, b, a)\n! c\n! b\n! a\n! minterm\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"15\" | 0\n| 0\n| 0\n| 0\n|style=\"background-color:#FDE9D9\" | (~c  &  ~b  &  ~a)\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"15\" | 1\n| 0\n| 0\n| 1\n|style=\"background-color:#FDE9D9\" | (~c  &  ~b  &  a)\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"12\" | 3\n| 0\n| 1\n| 1\n|style=\"background-color:#FDE9D9\" | (~c  &  b  &  a)\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"12\" | 2\n| 0\n| 1\n| 0\n|style=\"background-color:#FDE9D9\" | (~c &  b  &  ~a)\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"12\" | 6\n| 1\n| 1\n| 0\n|style=\"background-color:#FDE9D9\" | (c  &  b  &  ~a)\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"12\" | 7\n| 1\n| 1\n| 1\n|style=\"background-color:#FDE9D9\" | (c &  b &  a)\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"12\" | 5\n| 1\n| 0\n| 1\n|style=\"background-color:#FDE9D9\" | (c  &  ~b  &  a)\n|- style=\"font-size:9pt\" align=\"center\" valign=\"bottom\"\n| Height=\"12\" | 4\n| 1\n| 0\n| 0\n|style=\"background-color:#FDE9D9\" | (c  &  ~b  &  ~a)\n|- style=\"font-size:9pt;color:#A5A5A5\" align=\"center\" valign=\"bottom\"\n| Height=\"12\" | 0\n|style=\"font-weight:bold\" | 0\n| 0\n| 0\n| (~a  &  ~b  &  ~c)\n|}\n\n=== Reduction by use of the map method (Veitch, Karnaugh) ===\nVeitch improved the notion of [[Venn diagram]]s by converting the circles to abutting squares, and Karnaugh simplified the Veitch diagram by converting the minterms, written in their literal-form (e.g. ~abc~d) into numbers.<ref>Wickes 1967:36ff. Wickes offers a good example of 8 of the 2 x 4 (3-variable maps) and 16 of the 4 x 4 (4-variable) maps. As an arbitrary 3-variable map could represent any one of 2<sup>8</sup>=256 2x4 maps, and an arbitrary 4-variable map could represent any one of 2<sup>16</sup> = 65,536 different formula-evaluations, writing down every one is infeasible.</ref> The method proceeds as follows:\n\n==== Produce the formula's truth table ====\n\nProduce the formula's truth table. Number its rows using the binary-equivalents of the variables (usually just sequentially 0 through n-1) for n variables.\n\n: ''Technically, the [[propositional function]] has been reduced to its (unminimized) conjunctive normal form: each row has its minterm expression and these can be OR'd to produce the formula in its (unminimized) conjunctive normal form.''\n\nExample: ((c & d) &or; (p & ~(c & (~d)))) = q in conjunctive normal form is:\n::: ( (~p & d & c ) &or; (p & d & c) &or; (p & d & ~c) &or; (p & ~d & ~c) ) = q\n\nHowever, this formula be reduced both in the number of terms (from 4 to 3) and in the total count of its literals (12 to 6).\n\n{|\n|- style=\"font-size:9pt;font-weight:bold\" align=\"center\"\n!style=\"background-color:#F2F2F2\" width=\"25.5\" Height=\"24\" | row\n! Minterms\n! width=\"21\" | p\n! width=\"21\" | d\n! width=\"21\" | c\n! width=\"10.5\" | (\n! width=\"10.5\" | (\n! width=\"10.5\" | c\n! width=\"10.5\" | &\n! width=\"10.5\" | d\n! width=\"10.5\" | )\n!style=\"background-color:#FDE9D9\" width=\"10.5\" | &or;\n! width=\"10.5\" | (\n! width=\"10.5\" | p\n! width=\"10.5\" | &\n! width=\"10.5\" | ~\n! width=\"10.5\" | (\n! width=\"10.5\" | (\n! width=\"10.5\" | c\n! width=\"10.5\" | &\n! width=\"10.5\" | ~\n! width=\"10.5\" | (\n! width=\"10.5\" | d\n! width=\"10.5\" | )\n! width=\"10.5\" | )\n! width=\"10.5\" | )\n! width=\"10.5\" | )\n! width=\"10.5\" | )\n! {{Active}} minterms\n! Formula in conjunctive normal form\n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | ( ~p & ~d & ~c )\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FDE9D9\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | ( ~p & ~d & c)\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FDE9D9\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 2\n| align=\"center\" valign=\"bottom\" | ( ~p & d & ~c )\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FDE9D9\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 3\n| align=\"center\" valign=\"bottom\" | ( ~p & d & c )\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" | (~p & d & c)\n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 4\n| align=\"center\" valign=\"bottom\" | ( p & ~d & ~c )\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" | (~p & d & c)\n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 5\n| align=\"center\" valign=\"bottom\" | ( p & ~d & c )\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FDE9D9\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" | \n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 6\n| align=\"center\" valign=\"bottom\" | ( p & d & ~c )\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" | (p & d & ~c)\n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" align=\"center\" valign=\"bottom\" | 7\n| align=\"center\" valign=\"bottom\" | ( p & d & c )\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n|style=\"background-color:#DBE5F1\" align=\"center\" valign=\"bottom\" | 0\n|style=\"background-color:#EAF1DD\" align=\"center\" valign=\"bottom\" | 0\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | 1\n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|style=\"background-color:#FCD5B4\" align=\"center\" | ( p & d & c )\n|  valign=\"bottom\" | \n|- style=\"font-size:9pt\"\n| Height=\"16.5\"  valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|style=\"background-color:#FFA7A9;font-weight:bold\" align=\"center\" | q\n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|style=\"background-color:#FFA7A9\" align=\"center\" |  = (~p&d&c) &or; (~p&d&c) &or; (p&d&~c ) &or; (p&d&c )\n|}\n\n==== Create the formula's Karnaugh map ====\n\n[[File:Propositional formula maps 2.png|400px|thumb|right| Steps in the reduction using a Karnaugh map. The final result is the OR (logical \"sum\") of the three reduced terms.]]\n\nUse the values of the formula (e.g. \"p\") found by the truth-table method and place them in their into their respective (associated) Karnaugh squares (these are numbered per the Gray code convention). If values of \"d\" for \"don't care\" appear in the table, this adds flexibility during the reduction phase.\n\n==== Reduce minterms ====\n\nMinterms of adjacent (abutting) 1-squares (T-squares) can be reduced with respect to the number of their [[literal (mathematical logic)|literal]]s, and the number terms also will be reduced in the process. Two abutting squares (2 x 1 horizontal or 1 x 2 vertical, even the edges represent abutting squares) lose one literal, four squares in a 4 x 1 rectangle (horizontal or vertical) or 2 x 2 square (even the four corners represent abutting squares) lose two literals, eight squares in a rectangle lose 3 literals, etc. (One seeks out the largest square or rectangles and ignores the smaller squares or rectangles contained totally within it. ) This process continues until all abutting squares are accounted for, at which point the propositional formula is minimized.\n\nFor example, squares #3 and #7 abut. These two abutting squares can lose one literal (e.g. \"p\" from squares #3 and #7), four squares in a rectangle or square lose two literals, eight squares in a rectangle lose 3 literals, etc. (One seeks out the largest square or rectangles.) This process continues until all abutting squares are accounted for, at which point the propositional formula is said to be minimized.\n\nExample:  The map method usually is done by inspection. The following example expands the algebraic method to show the \"trick\" behind the combining of terms on a Karnaugh map:\n: Minterms #3 and #7 abut, #7 and #6 abut, and #4 and #6 abut (because the table's edges wrap around). So each of these pairs can be reduced.\n\nObserve that by the Idempotency law (A &or; A) = A, we can create more terms. Then by association and distributive laws the variables to disappear can be paired, and then \"disappeared\" with the Law of contradiction (x & ~x)=0. The following uses brackets [ and ] only to keep track of the terms; they have no special significance:\n* Put the formula in conjunctive normal form with the formula to be reduced:\n::: '''q = ( (~p & d & c ) &or; (p & d & c) &or; (p & d & ~c) &or; (p & ~d & ~c) )''' = ( #3 &or; #7 &or; #6 &or; #4 )\n* Idempotency (absorption) [ A &or; A) = A:\n::: ( #3 &or; [ #7 &or; #7 ] &or; [ #6 &or; #6 ] &or; #4 )\n* Associative law (x &or; (y &or; z)) = ( (x &or; y) &or; z )\n::: ( [ #3 &or; #7 ] &or; [ #7 &or; #6 ] &or; [ #6 &or; #4]  )\n::: '''[''' (~p & d & c ) &or; (p & d & c) ''']''' &or; '''[''' (p & d & c) &or; (p & d & ~c) ''']''' &or; '''[''' (p & d & ~c) &or; (p & ~d & ~c) ''']'''.\n* Distributive law ( x & (y &or; z) ) = ( (x & y) &or; (x & z) ) :\n::: ( [ (d & c) &or; (~p & p) ]  &or; [ (p & d) &or; (~c & c) ] &or; [ (p & ~c) &or; (c & ~c) ] )\n* Commutative law and law of contradiction (x & ~x) = (~x & x) = 0:\n::: ( [ (d & c) &or; (0) ] &or; [ (p & d) &or; (0) ] &or; [ (p & ~c) &or; (0) ] )\n* Law of identity ( x &or; 0 ) = x leading to the reduced form of the formula:\n::: '''q = ( (d & c) &or; (p & d)  &or; (p & ~c) )'''\n\n==== Verify reduction with a truth table ====\n\n{|\n|- style=\"font-size:9pt;font-weight:bold\" align=\"center\"\n|style=\"background-color:#F2F2F2\" width=\"25.5\" Height=\"12\" | row\n! Minterms\n! width=\"21\" | p\n! width=\"21\" | d\n! width=\"21\" | c\n! width=\"10.5\" | (\n! width=\"10.5\" | (\n! width=\"10.5\" | d\n!style=\"background-color:#DBE5F1\" width=\"10.5\" | &\n! width=\"10.5\" | c\n! width=\"10.5\" | )\n!style=\"background-color:#FDE9D9\" width=\"10.5\" | &or;\n! width=\"10.5\" | (\n! width=\"10.5\" | p\n!style=\"background-color:#DBE5F1\" width=\"10.5\" | &\n! width=\"10.5\" | d\n! width=\"10.5\" | )\n!style=\"background-color:#FDE9D9\" width=\"10.5\" | &or;\n! width=\"10.5\" | (\n! width=\"10.5\" | p\n!style=\"background-color:#DBE5F1\" width=\"10.5\" | &\n!style=\"background-color:#EAF1DD\" width=\"10.5\" | ~\n! width=\"10.5\" | (\n! width=\"10.5\" | c\n! width=\"10.5\" | )\n! width=\"10.5\" | )\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 0\n| ( ~p & ~d & ~c )\n| 0\n| 0\n| 0\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 1\n| ( ~p & ~d & c)\n| 0\n| 0\n| 1\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 2\n| ( ~p & d & ~c )\n| 0\n| 1\n| 0\n| \n| \n| 1\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 3\n| ( ~p & d & c )\n| 0\n| 1\n| 1\n| \n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n|style=\"background-color:#FAC090\" | 1\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 4\n| ( p & ~d & ~c )\n| 1\n| 0\n| 0\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FAC090\" | 1\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 5\n| ( p & ~d & c )\n| 1\n| 0\n| 1\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 6\n| ( p & d & ~c )\n| 1\n| 1\n| 0\n| \n| \n| 1\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n|style=\"background-color:#FAC090\" | 1\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 7\n| ( p & d & c )\n| 1\n| 1\n| 1\n| \n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n| 1\n| \n|style=\"background-color:#FAC090\" | 1\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n|- style=\"font-size:9pt\"\n| Height=\"12\"  valign=\"bottom\" | \n| align=\"center\" valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|style=\"background-color:#FAC090;font-weight:bold\" align=\"center\" | q\n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|  valign=\"bottom\" | \n|}\n\n== Impredicative propositions ==\n\nGiven the following examples-as-definitions, what does one make of the subsequent reasoning:\n: (1) \"This sentence is simple.\" (2) \"This sentence is complex, and it is conjoined by AND.\"\n\nThen assign the variable \"s\" to the left-most sentence \"This sentence is simple\". Define \"compound\" c = \"not simple\" ~s, and assign c = ~s to \"This sentence is compound\"; assign \"j\" to \"It [this sentence] is conjoined by AND\". The second sentence can be expressed as:\n: ( NOT(s) AND j )\n\nIf truth values are to be placed on the sentences c = ~s and j, then all are clearly FALSEHOODS: e.g. \"This sentence is complex\" is a FALSEHOOD (it is ''simple'', by definition). So their conjunction (AND) is a falsehood. But when taken in its assembed form, the sentence a TRUTH.\n\nThis is an example of the [[paradox]]es that result from an [[impredicative definition]]—that is, when an object m has a property P, but the object m is defined in terms of property P.<ref>This definition is given by [[Stephen Kleene]]. Both [[Kurt Gödel]] and Kleene believed that the classical paradoxes are uniformly examples of this sort of definition. But Kleene went on to assert that the problem has not been solved satisfactorily and impredicative definitions can be found in [[analysis]]. He gives as example the definition of the [[least upper bound]] (l.u.b) '''u''' of '''M'''. Given a [[Dedekind cut]] of the number line '''C''' and the two parts into which the number line is cut, i.e. '''M''' and ('''C''' - '''M'''), l.u.b. = '''u''' is defined in terms of the notion '''M''', whereas '''M''' is defined in terms of '''C'''. Thus the definition of '''u''', an element of '''C''', is defined in terms of the totality '''C''' and this makes its definition impredicative. Kleene asserts that attempts to argue this away can be used to uphold the impredicative definitions in the paradoxes.(Kleene 1952:43).</ref> The best advice for a rhetorician or one involved in deductive analysis is avoid impredicative definitions but at the same time be on the lookout for them because they can indeed create paradoxes. Engineers, on the other hand, put them to work in the form of propositional formulas with feedback.\n\n== Propositional formula with \"feedback\" ==\n\nThe notion of a propositional formula appearing as one of its own variables requires a formation rule that allows the assignment of the formula to a variable. In general there is no stipulation (either axiomatic or truth-table systems of objects and relations) that forbids this from happening.<ref>McCluskey comments that \"it could be argued that the analysis is still incomplete because the word statement \"The outputs are equal to the previous values of the inputs\" has not been obtained\"; he goes on to dismiss such worries because \"English is not a formal language in a mathematical sense, [and] it is not really possible to have a ''formal'' procedure for obtaining word statements\" (p. 185).</ref>\n\nThe simplest case occurs when an OR formula becomes one its own inputs e.g. p = q. Begin with (p &or; s) = q, then let p = q. Observe that q's \"definition\" depends on itself \"q\" as well as on \"s\" and the OR connective; this definition of q is thus '''impredicative'''.\nEither of two conditions can result:<ref>More precisely, given enough \"loop gain\", either '''oscillation''' or '''memory''' will occur (cf McCluskey p. 191-2). In abstract (idealized) mathematical systems adequate loop gain is not a problem.</ref> oscillation or memory.\n\nIt helps to think of the formula as a [[black box]]. Without knowledge of what is going on \"inside\" the formula-\"box\" from the outside it would appear that the output is no longer a [[Function (mathematics)|function]] of the inputs alone. That is, sometimes one looks at q and sees 0 and other times 1. To avoid this problem one has to know the '''state''' (condition) of the \"hidden\" variable p inside the box (i.e. the value of q fed back and assigned to p). When this is known the apparent inconsistency goes away.\n\nTo understand [predict] the behavior of formulas with feedback requires the more sophisticated analysis of [[sequential circuit]]s. Propositional formulas with feedback lead, in their simplest form, to state machines; they also lead to memories in the form of Turing tapes and counter-machine counters. From combinations of these elements one can build any sort of bounded computational model (e.g. [[Turing machine]]s, [[counter machine]]s, [[register machine]]s, [[Macintosh computer]]s, etc.).\n\n=== Oscillation ===\n\nIn the abstract (ideal) case the simplest oscillating formula is a NOT fed back to itself: ~(~(p=q)) = q. Analysis of an abstract (ideal) propositional formula in a truth-table reveals an inconsistency for both p=1 and p=0 cases: When p=1, q=0, this cannot be because p=q; ditto for when p=0 and q=1.\n\n{|\n|- style=\"font-size:9pt\" align=\"center\"\n!  width=\"14.25\" Height=\"12\" | \n!  width=\"5.25\" | \n! style=\"background-color:#EAF1DD;font-weight:bold\" width=\"14.25\" | q\n! style=\"font-weight:bold\" width=\"12.75\" | \n!  width=\"14.25\" | \n!  width=\"12.75\" | \n!  width=\"23.25\" | \n!  width=\"111\" | \n|- style=\"font-size:9pt\" align=\"center\"\n! style=\"font-weight:bold\" Height=\"12\" | p\n!  \n! style=\"background-color:#EAF1DD;font-weight:bold\" | ~\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | p\n! style=\"font-weight:bold\" | )\n! style=\"font-weight:bold\" |  = q\n!  \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| \n|style=\"background-color:red\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:red\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| \n|style=\"background-color:red\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:red\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|}\n\n[[File:Propositional formula oscillator 1.png|400px|thumb|right]]\n\n'''Oscillation with delay''': If an delay<ref>The notion of delay and the principle of local causation as caused ultimately by the speed of light appears in Robin Gandy (1980), \"Church's thesis and Principles for Mechanisms\", in J. Barwise, H. J. Keisler and K. Kunen, eds., ''The Kleene Symposium'', North-Holland Publishing Company (1980) 123-148. Gandy considered this to be the most important of his principles: \"Contemporary physics rejects the possibility of instantaneous action at a distance\" (p. 135). Gandy was [[Alan Turing]]'s student and close friend.</ref> (ideal or non-ideal) is inserted in the abstract formula between p and q then p will oscillate between 1 and 0: 101010...101... ''ad infinitum''. If either of the delay and NOT are not abstract (i.e. not ideal), the type of analysis to be used will be dependent upon the exact nature of the objects that make up the oscillator; such things fall outside mathematics and into engineering.\n\nAnalysis requires a delay to be inserted and then the loop cut between the delay and the input \"p\". The delay must be viewed as a kind of proposition that has \"qd\" (q-delayed) as output for \"q\" as input. This new proposition adds another column to the truth table. The inconsistency is now between \"qd\" and \"p\" as shown in red; two stable states resulting:\n\n{|\n|- style=\"font-size:9pt\" align=\"center\"\n!  width=\"16.5\" Height=\"12\" | \n!  width=\"14.25\" | \n!  width=\"8.25\" | \n! style=\"background-color:#EAF1DD;font-weight:bold\" width=\"14.25\" | q\n! style=\"font-weight:bold\" width=\"12.75\" | \n!  width=\"14.25\" | \n!  width=\"12.75\" | \n!  width=\"23.25\" | \n!  width=\"111\" | \n|- style=\"font-size:9pt\" align=\"center\"\n! style=\"font-weight:bold\" Height=\"12\" | qd\n! style=\"font-weight:bold\" | p\n! style=\"font-weight:bold\" | (\n! style=\"background-color:#EAF1DD;font-weight:bold\" | ~\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | p\n! style=\"font-weight:bold\" | )\n! style=\"background-color:#EAF1DD;font-weight:bold\" |  = q\n!  \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 0\n| \n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n|style=\"background-color:#EAF1DD\" | 1\n| state 1\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:red\" Height=\"12\" | 0\n|style=\"background-color:red\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | qd & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:red\" Height=\"12\" | 1\n|style=\"background-color:red\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | qd & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 1\n| \n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n|style=\"background-color:#EAF1DD\" | 0\n| state 0\n|}\n\n=== Memory ===\n[[File:Propositional formula flip flops 1.png|400px|thumb|right| About the simplest memory results when the output of an OR feeds back to one of its inputs, in this case output \"q\" feeding back into \"p\". The next simplest is the \"flip-flop\" shown below the once-flip. Analysis of these sorts of formulas can be done by either cutting the feedback path(s) or inserting (ideal) delay in the path. A cut path and an assumption that no delay occurs anywhere in the \"circuit\" results in inconsistencies for some of the '''total states''' (combination of inputs and outputs, e.g. (p=0, s=1, r=1) results in an inconsistency). When delay is present these inconsistencies are merely '''transient''' and expire when the delay(s) expire. The drawings on the right are called [[state diagram]]s.]]\n\n[[File:Propositional formula 3.png|400px|thumb|right| A \"clocked flip-flop\" memory (\"c\" is the \"clock\" and \"d\" is the \"data\"). The data can change at any time when clock c=0; when clock c=1 the output q \"tracks\" the value of data d. When c goes from 1 to 0 it \"traps\" d = q's value and this continues to appear at q no matter what d does (as long as c remains 0).]]\n\nWithout delay, inconsistencies must be eliminated from a truth table analysis. With the notion of \"delay\", this condition presents itself as a momentary inconsistency between the fed-back output variable q and p = q<sub>delayed</sub>.\n\nA truth table reveals the rows where inconsistencies occur between p = q<sub>delayed</sub> at the input and q at the output. After \"breaking\" the feed-back,<ref>McKlusky p. 194-5 discusses \"breaking the loop\" and inserts \"amplifiers\" to do this; Wickes (p. 118-121) discuss inserting delays. McCluskey p. 195ff discusses the problem of \"races\" caused by delays.</ref> the truth table construction proceeds in the conventional manner. But afterwards, in every row the output q is compared to the now-independent input p and any inconsistencies between p and q are noted (i.e. p=0 together with q=1, or p=1 and q=0); when the \"line\" is \"remade\" both are rendered impossible by the Law of contradiction ~(p & ~p)). Rows revealing inconsistencies are either considered '''transient states''' or just eliminated as inconsistent and hence \"impossible\".\n\n==== Once-flip memory ====\nAbout the simplest memory results when the output of an OR feeds back to one of its inputs, in this case output \"q\" feeds back into \"p\". Given that the formula is first evaluated (initialized) with p=0 & q=0, it will \"flip\" once when \"set\" by s=1. Thereafter, output \"q\" will sustain \"q\" in the \"flipped\" condition (state q=1). This behavior, now time-dependent, is shown by the [[state diagram]] to the right of the once-flip.\n\n{|\n|- style=\"font-size:9pt\" align=\"center\"\n!  width=\"16.5\" Height=\"12\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n! style=\"background-color:#FDE9D9;font-weight:bold\" width=\"14.25\" | q\n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"23.25\" | \n!  width=\"153\" | \n|- style=\"font-size:9pt\" align=\"center\"\n! style=\"font-weight:bold\" Height=\"12\" | p\n! style=\"font-weight:bold\" | s\n! style=\"font-weight:bold\" | (\n! style=\"background-color:#FCFF7F;font-weight:bold\" | s\n! style=\"background-color:#FDE9D9;font-weight:bold\" | &or;\n! style=\"font-weight:bold\" | p\n! style=\"font-weight:bold\" | )\n! style=\"background-color:#FDE9D9;font-weight:bold\" |  = q\n!  \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 0\n| \n| 0\n|style=\"background-color:#FDE9D9\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| state 0, s=0\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#BFBFBF\" Height=\"12\" | 0\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:red\" | 1\n|style=\"background-color:red\" | 0\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 0\n| \n| 0\n|style=\"background-color:#FDE9D9\" | 1\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| state 1 with s = 0\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 1\n| \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#FDE9D9\" | 1\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| state 1 with s = 1\n|}\n\n==== Flip-flop memory ====\nThe next simplest case is the \"set-reset\" [[Flip-flop (electronics)|flip-flop]] shown below the once-flip. Given that r=0 & s=0 and q=0 at the outset, it is \"set\" (s=1) in a manner similar to the once-flip. It however has a provision to \"reset\" q=0 when \"r\"=1. And additional complication occurs if both set=1 and reset=1. In this formula, the set=1 ''forces'' the output q=1 so when and if (s=0 & r=1) the flip-flop will be reset. Or, if (s=1 & r=0) the flip-flop will be set. In the abstract (ideal) instance in which s=1 &rArr; s=0 & r=1 &rArr; r=0 simultaneously, the formula q will be indeterminate (undecidable). Due to delays in \"real\" OR, AND and NOT the result will be unknown at the outset but thereafter predicable.\n\n{|\n|- style=\"font-size:9pt\" align=\"center\"\n!  width=\"19.5\" Height=\"12\" | \n!  width=\"19.5\" | \n!  width=\"19.5\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n! style=\"background-color:#FDE9D9;font-weight:bold\" width=\"14.25\" | q\n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"16.5\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"14.25\" | \n!  width=\"23.25\" | \n!  width=\"153\" | \n|- style=\"font-size:9pt\" align=\"center\"\n! style=\"font-weight:bold\" Height=\"12\" | p\n! style=\"font-weight:bold\" | s\n! style=\"font-weight:bold\" | r\n! style=\"font-weight:bold\" | (\n! style=\"background-color:#FCFF7F;font-weight:bold\" | s\n! style=\"background-color:#FDE9D9;font-weight:bold\" | &or;\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | p\n! style=\"background-color:#DBE5F1;font-weight:bold\" | &\n! style=\"background-color:#EAF1DD;font-weight:bold\" | ~\n! style=\"font-weight:bold\" | (\n! style=\"background-color:#FCFF7F;font-weight:bold\" | r\n! style=\"font-weight:bold\" | )\n! style=\"font-weight:bold\" | )\n! style=\"font-weight:bold\" | )\n! style=\"background-color:#FDE9D9;font-weight:bold\" |  = q\n!  \n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 0\n| 0\n| \n| 0\n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 0\n| state 0 with ( s=0 & r=0 )\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 0\n| 0\n| 1\n| \n| 0\n|style=\"background-color:#FDE9D9\" | 0\n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n|style=\"background-color:#FCFF7F\" | 1\n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 0\n| state 0 with ( s=0 & r=1 )\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#D8D8D8\" Height=\"12\" | 0\n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:red\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:red\" | 0\n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#D8D8D8\" Height=\"12\" | 0\n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:red\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:red\" | 0\n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 0\n| 0\n| \n| 0\n|style=\"background-color:#FDE9D9\" | 1\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 1\n| state 1 with ( s=0 & r=0 )\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#D8D8D8\" Height=\"12\" | 1\n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:red\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:red\" | 1\n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | 0\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | 1\n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | \n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n| Height=\"12\" | 1\n| 1\n| 0\n| \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#FDE9D9\" | 1\n| \n| 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 1\n| state 1 with ( s=1 & r=0 )\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2\" Height=\"12\" | 1\n|style=\"background-color:#F2F2F2\" | 1\n|style=\"background-color:#F2F2F2\" | 1\n|style=\"background-color:#F2F2F2\" | \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#FDE9D9\" | 1\n|style=\"background-color:#F2F2F2\" | \n|style=\"background-color:#F2F2F2\" | 1\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n|style=\"background-color:#F2F2F2\" | \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#F2F2F2\" | \n|style=\"background-color:#F2F2F2\" | \n|style=\"background-color:#F2F2F2\" | \n|style=\"background-color:#FDE9D9\" | 1\n|style=\"background-color:#F2F2F2\" | state 1 with s & r simultaneously 1\n|}\n\n==== Clocked flip-flop memory ====\nThe formula known as \"clocked flip-flop\" memory (\"c\" is the \"clock\" and \"d\" is the \"data\") is given below. It works as follows: When c = 0 the data d (either 0 or 1) cannot \"get through\" to affect output q. When c = 1 the data d \"gets through\" and output q \"follows\" d's value. When c goes from 1 to 0 the last value of the data remains \"trapped\" at output \"q\". As long as c=0, d can change value without causing q to change.\n\n* Examples\n*# ( ( c & d ) &or; ( '''p''' & ( ~( c & ~( d ) ) ) ) = '''q''', but now let p = q:\n*# ( ( c & d ) &or; ( '''q''' & ( ~( c & ~( d ) ) ) ) = '''q'''\n\nThe state diagram is similar in shape to the flip-flop's state diagram, but with different labelling on the '''transitions'''.\n\n{|\n|- style=\"font-size:9pt\"\n!  width=\"26.25\" Height=\"12\" align=\"center\" | \n!  width=\"16.5\" align=\"center\" | \n!  width=\"16.5\" align=\"center\" | \n!  width=\"16.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n! style=\"background-color:#DBE5F1\" width=\"10.5\" align=\"center\" | s\n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n! style=\"background-color:#FDE9D9;font-weight:bold\" width=\"10.5\" align=\"center\" | q\n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n! style=\"background-color:#DBE5F1\" width=\"10.5\" align=\"center\" | w\n! style=\"background-color:#EAF1DD\" width=\"10.5\" align=\"center\" | v\n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n! style=\"background-color:#DBE5F1\" width=\"10.5\" align=\"center\" | r\n! style=\"background-color:#EAF1DD\" width=\"10.5\" align=\"center\" | u\n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"10.5\" align=\"center\" | \n!  width=\"17.25\" align=\"center\" | \n!  width=\"201\"  | \n|- style=\"font-size:9pt\" align=\"center\"\n! style=\"background-color:#F2F2F2\" Height=\"14.25\" | row\n! style=\"font-weight:bold\" | q\n! style=\"font-weight:bold\" | d\n! style=\"font-weight:bold\" | c\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | c\n! style=\"background-color:#DBE5F1;font-weight:bold\" | &\n! style=\"font-weight:bold\" | d\n! style=\"font-weight:bold\" | )\n! style=\"background-color:#FDE9D9;font-weight:bold\" | &or;\n! style=\"font-weight:bold\" | (\n! style=\"background-color:#FDE9D9;font-weight:bold\" | q\n! style=\"background-color:#DBE5F1;font-weight:bold\" | &\n! style=\"background-color:#EAF1DD;font-weight:bold\" | ~\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | c\n! style=\"background-color:#DBE5F1;font-weight:bold\" | &\n! style=\"background-color:#EAF1DD;font-weight:bold\" | ~\n! style=\"font-weight:bold\" | (\n! style=\"font-weight:bold\" | d\n! style=\"font-weight:bold\" | )\n! style=\"font-weight:bold\" | )\n! style=\"font-weight:bold\" | )\n! style=\"font-weight:bold\" | )\n! style=\"font-weight:bold\" | )\n! style=\"background-color:#FDE9D9;font-weight:bold\" |  =q\n! style=\"font-weight:bold\" | Description\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 0\n| 0\n| 0\n| 0\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 0\n| state 0 with ( s=0 & r=0 ), 0 is trapped\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 1\n| 0\n| 0\n| 1\n| \n| \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 0\n| state 0 with ( d=0 & c=1 ):<br/>q=0 is following d=0\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 2\n| 0\n| 1\n| 0\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 0\n| \n|style=\"background-color:#FDE9D9\" | 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 0\n| state 0 with ( d=1 & r=0 ), 0 is trapped\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#BFBFBF;font-weight:bold\" Height=\"12\" | 3\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:red\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:red\" | 0\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 4\n| 1\n| 0\n| 0\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 0\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 1\n| \n| 0\n| \n| \n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 1\n| state 1 with (d =0 & c=0 ),  1 is trapped\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#BFBFBF;font-weight:bold\" Height=\"12\" | 5\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:red\" | 0\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:red\" | 1\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | 1\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | 0\n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#BFBFBF\" | \n|style=\"background-color:#D8D8D8\" | q & p inconsistent\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 6\n| 1\n| 1\n| 0\n| \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n| 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n|style=\"background-color:#EAF1DD\" | \n| \n| 0\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n| 1\n| \n| \n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 1\n| state 1 with (d =1 & c=0 ), 1 is trapped\n|- style=\"font-size:9pt\" align=\"center\"\n|style=\"background-color:#F2F2F2;font-weight:bold\" Height=\"12\" | 7\n| 1\n| 1\n| 1\n| \n| \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#FCFF7F\" | 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n| \n|style=\"background-color:#FDE9D9\" | 1\n|style=\"background-color:#DBE5F1\" | 1\n|style=\"background-color:#EAF1DD\" | 1\n| \n| \n|style=\"background-color:#FCFF7F\" | 1\n|style=\"background-color:#DBE5F1\" | 0\n|style=\"background-color:#EAF1DD\" | 0\n| \n|style=\"background-color:#FCFF7F\" | 1\n| \n| \n| \n| \n| \n|style=\"background-color:#FDE9D9\" | 1\n| state 1 with ( d=1 & c=1 ):<br/>q=1 is following d=1\n|}\n\n== Historical development ==\n[[Bertrand Russell]] (1912:74) lists three laws of thought that derive from [[Aristotle]]: (1) The law of identity: \"Whatever is, is.\", (2) The [[law of contradiction]]: \"Nothing cannot both be and not be\", and (3) The [[law of excluded middle]]: \"Everything must be or not be.\"\n* Example: Here O is an expression about an objects BEING or QUALITY:\n*# Law of Identity: O = O\n*# Law of contradiction: ~(O & ~(O))\n*# Law of excluded middle: (O &or; ~(O))\n\nThe use of the word \"everything\" in the law of excluded middle renders Russell's expression of this law open to debate. If restricted to an expression about BEING or QUALITY with reference to a finite collection of objects (a finite \"universe of discourse\") -- the members of which can be investigated one after another for the presence or absence of the assertion—then the law is considered intuitionistically appropriate. Thus an assertion such as: \"This object must either BE or NOT BE (in the collection)\", or \"This object must either have this QUALITY or NOT have this QUALITY (relative to the objects in the collection)\" is acceptable. See more at [[Venn diagram]].\n\nAlthough a propositional calculus originated with Aristotle, the notion of an ''algebra'' applied to propositions had to wait until the early 19th century. In an (adverse) reaction to the 2000 year tradition of Aristotle's [[syllogism]]s, [[John Locke]]'s ''Essay concerning human understanding (1690)'' used the word [[semiotics]] (theory of the use of symbols). By 1826 [[Richard Whately]] had critically analyzed the syllogistic logic with a sympathy toward Locke's semiotics. [[George Bentham]]'s work (1827) resulted in the notion of \"quantification of the predicate\" (1827) (nowadays symbolized as ∀ ≡ \"for all\"). A \"row\" instigated by [[Sir William Hamilton, 9th Baronet|William Hamilton]] over a priority dispute with [[Augustus De Morgan]] \"inspired [[George Boole]] to write up his ideas on logic, and to publish them as MAL [Mathematical Analysis of Logic] in 1847\" (Grattin-Guinness and Bornet 1997:xxviii).\n\nAbout his contribution Grattin-Guinness and Bornet comment:\n: \"Boole's principal single innovation was [the] law [ x<sup>n</sup> = x ] for logic: it stated that the mental acts of choosing the property x and choosing x again and again is the same as choosing x once... As consequence of it he formed the equations x•(1-x)=0 and x+(1-x)=1 which for him expressed respectively the law of contradiction and the law of excluded middle\" (p. xxviiff). For Boole \"1\" was the [[universe of discourse]] and \"0\" was nothing.\n\n[[Gottlob Frege]]'s massive undertaking (1879) resulted in a formal calculus of propositions, but his symbolism is so daunting that it had little influence excepting on one person: [[Bertrand Russell]]. First as the student of [[Alfred North Whitehead]] he studied Frege's work and suggested a (famous and notorious) emendation with respect to it (1904) around the problem of an [[antinomy]] that he discovered in Frege's treatment ( cf [[Russell's paradox]] ). Russell's work led to a collatoration with Whitehead that, in the year 1912, produced the first volume of ''Principia Mathematica'' (PM). It is here that what we consider \"modern\" propositional logic first appeared. In particular, PM introduces NOT and OR and the assertion symbol ⊦ as primitives. In terms of these notions they define IMPLICATION → ( def. *1.01: ~p &or; q ), then AND (def. *3.01: ~(~p &or; ~q) ), then EQUIVALENCE p ←→ q (*4.01: (p → q) & ( q → p ) ).\n\n* [[Henry M. Sheffer]] (1921) and [[Jean Nicod]] demonstrate that only one connective, the \"stroke\" | is sufficient to express all propositional formulas.\n* [[Emil Post]] (1921) develops the truth-table method of analysis in his \"Introduction to a general theory of elementary propositions\". He notes Nicod's stroke | .\n* Whitehead and Russell add an introduction to their 1927 re-publication of PM adding, in part, a favorable treatment of the \"stroke\".\n\n'''Computation and switching logic''':\n* [[William Eccles]] and [[F. W. Jordan]] (1919) describe a \"trigger relay\" made from a vacuum tube.\n* [[George Stibitz]] (1937) invents the binary adder using mechanical relays. He builds this on his kitchen table.\n: Example: Given binary [[bit]]s a<sub>i</sub> and b<sub>i</sub> and carry-in ( c_in<sub>i</sub>), their summation Σ<sub>i</sub> and carry-out (c_out<sub>i</sub>) are:\n:* ( ( a<sub>i</sub> XOR b<sub>i</sub> ) XOR c_in<sub>i</sub> )= Σ<sub>i</sub>\n:* ( a<sub>i</sub> & b<sub>i</sub> ) &or; c_in<sub>i</sub> ) = c_out<sub>i</sub>;\n* [[Alan Turing]] builds a multiplier using relays (1937–1938). He has to hand-wind his own relay coils to do this.\n* Textbooks about \"switching circuits\" appear in early 1950s.\n* [[Willard Quine]] 1952 and 1955, [[Edward W. Veitch|E. W. Veitch]] 1952, and [[Maurice Karnaugh|M. Karnaugh]] (1953) develop map-methods for simplifying propositional functions.\n* [[George H. Mealy]] (1955) and [[Edward F. Moore]] (1956) address the theory of sequential (i.e. switching-circuit) \"machines\".\n* E. J. McCluskey and H. Shorr develop a method for simplifying propositional (switching) circuits (1962).\n\n== Footnotes ==\n{{reflist}}\n\n== References ==\n* {{aut|Bender, Edward A.}} and {{aut|Williamson, S. Gill}}, 2005, ''A Short Course in Discrete Mathematics'', Dover Publications, Mineola NY, {{isbn|0-486-43946-1}}. This text is used in a \"lower division two-quarter [computer science] course\" at UC San Diego.\n* {{aut|[[Herbert Enderton|Enderton, H. B.]]}},  2002, ''A Mathematical Introduction to Logic.'' Harcourt/Academic Press. {{isbn|0-12-238452-0}}\n* {{aut|Goodstein, R. L.}}, (Pergamon Press 1963), 1966, (Dover edition 2007), ''Boolean Algebra'', Dover Publications, Inc. Minola, New York, {{isbn|0-486-45894-6}}. Emphasis on the notion of \"algebra of classes\" with set-theoretic symbols such as ∩, ∪, ' (NOT), ⊂ (IMPLIES). Later Goldstein replaces these with &, ∨, ￢, → (respectively) in his treatment of \"Sentence Logic\" pp.&nbsp;76–93.\n* {{aut|[[Ivor Grattan-Guinness]]}} and Gérard Bornet 1997, ''George Boole: Selected Manuscripts on Logic and its Philosophy'', Birkhäuser Verlag, Basil, {{isbn|978-0-8176-5456-6}} (Boston).\n* {{aut|A. G. Hamilton}} 1978, ''Logic for Mathematicians'', Cambridge University Press, Cambridge UK, {{isbn|0-521-21838-1}}.\n* {{aut|E. J. [[McCluskey]]}} 1965, ''Introduction to the Theory of Switching Circuits'', McGraw-Hill Book Company, New York. No ISBN. Library of Congress Catalog Card Number 65-17394. McCluskey was a student of [[Willard Quine]] and developed some notable theorems with Quine and on his own. For those interested in the history, the book contains a wealth of references.\n* {{aut|[[Marvin L. Minsky]]}} 1967, ''Computation: Finite and Infinite Machines'', Prentice-Hall, Inc, Englewood Cliffs, N.J.. No ISBN. Library of Congress Catalog Card Number 67-12342. Useful especially for computability, plus good sources.\n* {{aut|[[Paul C. Rosenbloom]]}} 1950, Dover edition 2005, ''The Elements of Mathematical Logic'', Dover Publications, Inc., Mineola, New York, {{isbn|0-486-44617-4}}.\n* {{aut|[[Joel W. Robbin]]}} 1969, 1997, ''Mathematical Logic: A First Course'', Dover Publications, Inc., Mineola, New York, {{isbn|0-486-45018-X}} (pbk.).\n* {{aut|[[Patrick Suppes]]}} 1957 (1999 Dover edition), ''Introduction to Logic'', Dover Publications, Inc., Mineola, New York. {{isbn|0-486-40687-3}} (pbk.). This book is in print and readily available.\n* On his page 204 in a footnote he references his set of axioms to [[Edward Vermilye Huntington|E. V. Huntington]], \"Sets of Independent Postulates for the Algebra of Logic\", ''Transactions of the American Mathematical Society, Vol. 5 91904) pp. 288-309.\n* {{aut|[[Alfred Tarski]]}} 1941 (1995 Dover edition), ''Introduction to Logic and to the Methodology of Deductive Sciences'', Dover Publications, Inc., Mineola, New York. {{isbn|0-486-28462-X}} (pbk.). This book is in print and readily available.\n* {{aut|[[Jean van Heijenoort]]}} 1967, 3rd printing with emendations 1976, ''From Frege to Gödel: A Source Book in Mathematical Logic, 1879-1931'', Harvard University Press, Cambridge, Massachusetts. {{isbn|0-674-32449-8}} (pbk.) Translation/reprints of Frege (1879), Russell's letter to Frege (1902) and Frege's letter to Russell (1902), Richard's paradox (1905), Post (1921) can be found here.\n* {{aut|[[Alfred North Whitehead]]}} and {{aut|[[Bertrand Russell]]}} 1927 2nd edition, paperback edition to *53 1962, ''Principia Mathematica'', Cambridge University Press, no ISBN. In the years between the first edition of 1912 and the 2nd edition of 1927, H. M. [[Sheffer]] 1921 and M. Jean [[Nicod]] (no year cited) brought to Russell's and Whitehead's attention that what they considered their primitive propositions (connectives) could be reduced to a single |, nowadays known as the \"stroke\" or NAND (NOT-AND, NEITHER ... NOR...). Russell-Whitehead discuss this in their \"Introduction to the Second Edition\" and makes the definitions as discussed above.\n* {{aut|William E. Wickes}} 1968, ''Logic Design with Integrated Circuits'', John Wiley & Sons, Inc., New York. No ISBN. Library of Congress Catalog Card Number: 68-21185. Tight presentation of engineering's analysis and synthesis methods, references McCluskey 1965. Unlike Suppes, Wickes' presentation of \"Boolean algebra\" starts with a set of postulates of a truth-table nature and then derives the customary theorems of them (p.&nbsp;18ff).\n\n{{Mathematical logic}}\n\n{{DEFAULTSORT:Propositional Formula}}\n[[Category:Propositional calculus]]\n[[Category:Boolean algebra]]\n[[Category:Statements]]\n[[Category:Syntax (logic)]]\n[[Category:Propositions]]\n[[Category:Logical expressions]]"
    },
    {
      "title": "Quine–McCluskey algorithm",
      "url": "https://en.wikipedia.org/wiki/Quine%E2%80%93McCluskey_algorithm",
      "text": "The '''Quine–McCluskey algorithm''' (or '''the method of prime implicants''') is a method used for [[Minimization of Boolean functions|minimization]] of [[Boolean function|Boolean functions]] that was developed by [[Willard Van Orman Quine|Willard V. Quine]]<ref name=\"Quine_1952\"/><ref name=\"Quine_1955\"/> and extended by [[Edward J. McCluskey]].<ref name=\"McCluskey_1956\"/> It is functionally identical to [[Karnaugh mapping]], but the tabular form makes it more efficient for use in computer algorithms, and it also gives a deterministic way to check that the minimal form of a Boolean function has been reached. It is sometimes referred to as the tabulation method.\n\nThe method involves two steps:\n# Finding all [[implicant|prime implicants]] of the function.\n# Use those prime implicants in a ''prime implicant chart'' to find the essential prime implicants of the function, as well as other prime implicants that are necessary to cover the function.\n\n==Complexity==\nAlthough more practical than [[Karnaugh mapping]] when dealing with more than four variables, the Quine–McCluskey algorithm also has a limited range of use since the [[Boolean satisfiability problem|problem]] it solves is [[NP-complete]].<ref name=\"Masek_1979\"/><ref name=\"Czort_1999\"/><ref name=\"Umans_2006\"/> The [[running time]] of the Quine–McCluskey algorithm grows [[exponential growth|exponentially]] with the number of variables. For a function of ''n'' variables the number of prime implicants can be as large as 3<sup>''n''</sup>ln(''n''), e.g. for 32 variables there may be over 534 * 10<sup>12</sup> prime implicants. Functions with a large number of variables have to be minimized with potentially non-optimal [[Heuristic algorithm|heuristic]] methods, of which the [[Espresso heuristic logic minimizer]] was the de facto standard in 1995.{{update inline|date=May 2017|reason=The reference correctly describes the situation in 1995. We need to expand this to include the changes of the past twenty years, however.}}<ref name=\"Nelson_1995\"/>\n\nStep two of the algorithm amounts to solving the [[set cover problem]];<ref name=\"Feldman_2009\"/>[[NP-hard]] instances of this problem may occur in this algorithm step.<ref name=\"Gimpel_1965\"/><ref name=\"Paul_1974\"/>\n\n==Example==\n===Input===\n\nIn this example, the input is a Boolean function in four variables, <math>f :\\{0,1\\}^4 \\to \\{0,1\\}</math> which evaluates to <math>1</math> on the values <math>4,8,10,11,12</math> and <math>15</math>, evaluates to an unknown value on <math>9</math> and <math>14</math>, and to <math>0</math> everywhere else (where these integers are interpreted in their binary form for input to <math>f</math> for succinctness of notation). The inputs that evaluate to <math>1</math> are called 'minterms'. We encode all of this information by writing\n\n:<math>f(A,B,C,D) =\\sum m(4,8,10,11,12,15) + d(9,14). \\,</math>\nThis expression says that the output function f will be 1 for the minterms <math>4,8,10,11,12</math> and <math>15</math> (denoted by the 'm' term) and that we don't care about the output for <math>9</math> and <math>14</math> combinations (denoted by the 'd' term).\n\n===Step 1: finding prime implicants===\nFirst, we write the function as a table (where 'x' stands for don't care):\n:{| class=\"wikitable\"\n|-\n!   !! A !! B !! C !! D !! f\n|-\n| m0 || 0 || 0 || 0 || 0 || 0\n|-\n| m1 || 0 || 0 || 0 || 1 || 0\n|-\n| m2 || 0 || 0 || 1 || 0 || 0\n|-\n| m3 || 0 || 0 || 1 || 1 || 0\n|-\n| m4 || 0 || 1 || 0 || 0 || 1\n|-\n| m5 || 0 || 1 || 0 || 1 || 0\n|-\n| m6 || 0 || 1 || 1 || 0 || 0\n|-\n| m7 || 0 || 1 || 1 || 1 || 0\n|-\n| m8 || 1 || 0 || 0 || 0 || 1\n|-\n| m9 || 1 || 0 || 0 || 1 || x\n|-\n| m10 || 1 || 0 || 1 || 0 || 1\n|-\n| m11 || 1 || 0 || 1 || 1 || 1\n|-\n| m12 || 1 || 1 || 0 || 0 || 1\n|-\n| m13 || 1 || 1 || 0 || 1 || 0\n|-\n| m14 || 1 || 1 || 1 || 0 || x\n|-\n| m15 || 1 || 1 || 1 || 1 || 1\n|}\n\nOne can easily form the canonical [[sum of products]] expression from this table, simply by summing the [[minterm]]s (leaving out [[Don't-care (logic)|don't-care terms]]) where the function evaluates to one:\n\n:''f''{{sub|A,B,C,D}} = A'BC'D' + AB'C'D' + AB'CD' + AB'CD + ABC'D' + ABCD.\n\nwhich is not minimal. So to optimize, all minterms that evaluate to one are first placed in a minterm table. Don't-care terms are also added into this table, so they can be combined with minterms:\n\n:{| class=\"wikitable\"\n|-\n! Number<br/>of 1s !! Minterm !! Binary<br/>Representation\n|-\n| rowspan=\"2\" | 1\n| m4 || {{mono|0100}}\n|-\n| m8 || {{mono|1000}}\n|-\n| rowspan=\"3\" | 2\n| m9 || {{mono|1001}}\n|-\n| m10 || {{mono|1010}}\n|-\n| m12 || {{mono|1100}}\n|-\n| rowspan=\"2\" | 3\n| m11 || {{mono|1011}}\n|-\n| m14 || {{mono|1110}}\n|-\n|| 4\n| m15 || {{mono|1111}}\n|}\n\nAt this point, one can start combining minterms with other minterms. If two terms vary by only a single digit changing, that digit can be replaced with a dash indicating that the digit doesn't matter. Terms that can't be combined any more are marked with an asterisk (*). When going from Size 2 to Size 4, treat '-' as a third bit value. For instance, -110 and -100 can be combined, as well as -110 and -11-, but -110 and 011- cannot. (Trick: Match up the '-' first.)\n\n:{| class=\"wikitable\"\n|-\n! Number<br/>of 1s !! Minterm !! 0-Cube !! colspan = \"2\" | Size 2 Implicants !! colspan = \"2\" | Size 4 Implicants\n|-\n| rowspan=\"4\" | 1\n| m4 || {{mono|0100}} || m(4,12) || {{mono|-100*}} || m(8,9,10,11) || {{mono|10--*}}\n|-\n| m8 || {{mono|1000}} || m(8,9) || {{mono|100-}} || m(8,10,12,14)  || {{mono|1--0*}}\n|-\n| {{sdash}} || {{sdash}} || m(8,10) || {{mono|10-0}} ||colspan = \"2\" {{sdash}}\n|-\n| {{sdash}} || {{sdash}} || m(8,12) || {{mono|1-00}} ||colspan = \"2\" {{sdash}}\n|-\n| rowspan=\"4\" | 2\n| m9 || {{mono|1001}} || m(9,11) || {{mono|10-1}} || m(10,11,14,15) || {{mono|1-1-*}}\n|-\n| m10 || {{mono|1010}} || m(10,11) || {{mono|101-}} ||colspan = \"2\" {{sdash}}\n|-\n| {{sdash}} || {{sdash}} || m(10,14) || {{mono|1-10}} ||colspan = \"2\" {{sdash}}\n|-\n| m12 || {{mono|1100}} || m(12,14) || {{mono|11-0}} ||colspan = \"2\" {{sdash}}\n|-\n| rowspan=\"2\" | 3\n| m11 || {{mono|1011}} || m(11,15) || {{mono|1-11}} ||colspan = \"2\" {{sdash}}\n|-\n| m14 || {{mono|1110}} || m(14,15) || {{mono|111-}} ||colspan = \"2\" {{sdash}}\n|-\n| rowspan=\"1\" | 4\n| m15 || {{mono|1111}} ||\n| {{sdash}}\n| colspan = \"2\" {{sdash}}\n|}\n\nNote: In this example, none of the terms in the size 4 implicants table can be combined any further. Be aware that this processing should be continued otherwise (size 8 etc.).\n\n===Step 2: prime implicant chart===\nNone of the terms can be combined any further than this, so at this point we construct an essential prime implicant table. Along the side goes the prime implicants that have just been generated, and along the top go the minterms specified earlier. The don't care terms are not placed on top—they are omitted from this section because they are not necessary inputs.\n\n:{| class=\"wikitable\" {{ts|ac}}\n|-\n!                             || 4 || 8 || 10 || 11 || 12 || 15 || &rArr; || A || B || C || D\n|-\n| {{ts|al}} | m(4,12)*        || {{ya}} || || || || {{ya}} ||   || &rArr; || {{sdash}} || 1 || 0 || 0\n|-\n| {{ts|al}} | m(8,9,10,11)    || || {{ya}} || {{ya}} || {{ya}} || || || &rArr; || 1 || 0 || {{sdash}} || {{sdash}}\n|-\n| {{ts|al}} | m(8,10,12,14)   || || {{ya}} || {{ya}} || || {{ya}} || || &rArr; || 1 || {{sdash}} || {{sdash}} || 0\n|-\n| {{ts|al}} | m(10,11,14,15)* || || || {{ya}} || {{ya}} || || {{ya}} || &rArr; || 1 || {{sdash}} || 1 || {{sdash}}\n|}\n\nTo find the essential prime implicants, we run along the top row. We have to look for columns with only 1 \"✓\". If a column has only 1 \"X\", this means that the minterm can only be covered by 1 prime implicant. This prime implicant is ''essential''.\n\nFor example: in the first column, with minterm 4, there is only 1 \"✓\". This means that m(4,12) is essential. So we place a star next to it. Minterm 15 also has only 1 \"✓\", so m(10,11,14,15) is also essential. Now all columns with 1 \"✓\" are covered.\n\nThe second prime implicant can be 'covered' by the third and fourth, and the third prime implicant can be 'covered' by the second and first, and neither is thus essential. If a prime implicant is essential then, as would be expected, it is necessary to include it in the minimized boolean equation. In some cases, the essential prime implicants do not cover all minterms, in which case additional procedures for chart reduction can be employed. The simplest \"additional procedure\" is trial and error, but a more systematic way is [[Petrick's method]]. In the current example, the essential prime implicants do not handle all of the minterms, so, in this case, one can combine the essential implicants with one of the two non-essential ones to yield one equation:\n\n:''f''{{sub|A,B,C,D}} = BC'D' + AB' + AC<ref name=\"Logic_Friday\"/>\nor\n:''f''{{sub|A,B,C,D}} = BC'D' + AD' + AC\n\nBoth of those final equations are functionally equivalent to the original, verbose equation:\n:''f''{{sub|A,B,C,D}} = A'BC'D' + AB'C'D' + AB'C'D + AB'CD' + AB'CD + ABC'D' + ABCD' + ABCD.\n\n==See also==\n* [[Buchberger's algorithm]] &ndash; analogous algorithm for algebraic geometry\n* [[Petrick's method]]\n\n==References==\n{{reflist|refs=\n<ref name=\"Quine_1952\">{{cite journal |author-last=Quine |author-first=Willard Van Orman |author-link=Willard Van Orman Quine |date=October 1952 |title=The Problem of Simplifying Truth Functions |jstor=2308219 |journal=[[The American Mathematical Monthly]] |volume=59 |issue=8 |pages=521–531 |doi=10.2307/2308219}}</ref>\n<ref name=\"Quine_1955\">{{cite journal |author-last=Quine |author-first=Willard Van Orman |author-link=Willard Van Orman Quine |date=November 1955 |title=A Way to Simplify Truth Functions |jstor=2307285 |journal=[[The American Mathematical Monthly]] |volume=62 |issue=9 |pages=627–631 |doi=10.2307/2307285}}</ref>\n<ref name=\"McCluskey_1956\">{{cite journal |author-last=McCluskey, Jr. |author-first=Edward J. |author-link=Edward J. McCluskey |date=November 1956 |title=Minimization of Boolean Functions |journal=[[Bell System Technical Journal]] |volume=35 |issue=6 |pages=1417–1444 |doi=10.1002/j.1538-7305.1956.tb03835.x |url=https://archive.org/details/bstj35-6-1417 |access-date=2014-08-24}}</ref>\n<ref name=\"Nelson_1995\">{{cite book |author-last=Nelson |author-first=Victor P. |date=1995 |title=Digital Logic Circuit Analysis and Design |url=https://books.google.com/books?id=8V5TAAAAMAAJ |publisher=[[Prentice Hall]] |page=234 |access-date=2014-08-26 |display-authors=etal}}</ref>\n<ref name=\"Logic_Friday\">[[Logic Friday]] program</ref>\n<ref name=\"Masek_1979\">{{cite book |url= |author-first=W. |author-last=Masek |title=Some NP-complete set covering problems |publisher=unpublished |location= |pages= |date=1979}}</ref>\n<ref name=\"Czort_1999\">{{cite thesis |type=Master's thesis |url= |author-first=S. |author-last=Czort |title=The complexity of minimizing disjunctive normal form formulas |institution=University of Aarhus |date=1999}}</ref>\n<ref name=\"Umans_2006\">{{cite journal |url= |author-first1=C. |author-last1=Umans |author-first2=T. |author-last2=Villa |author-first3=A. L. |author-last3=Sangiovanni-Vincentelli |title=Complexity of two-level logic minimization |journal=[[IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems]] |volume=25 |number=7 |pages=1230-1246 |date=2006}}</ref>\n<ref name=\"Feldman_2009\">{{cite journal |url=https://www.sciencedirect.com/science/article/pii/S002200000800069X/pdf?md5=0adeeefb189024849aaf393af8c98634&pid=1-s2.0-S002200000800069X-main.pdf |author-first=Vitaly |author-last=Feldman |title=Hardness of Approximate Two-Level Logic Minimization and PAC Learning with Membership Queries |journal=[[Journal of Computer and System Sciences]] |volume=75 |number= |pages=13-25 |date=2009}} Specifically: p.13-14</ref>\n<ref name=\"Gimpel_1965\">{{cite journal |author-first=J. F. |author-last=Gimpel |title=A Method for Producing a Boolean Function Having an Arbitrary Prescribed Prime Implicant Table |journal=[[IEEE Transactions on Computers]] |volume=14 |number= |pages=485-488 |date=1965}}</ref>\n<ref name=\"Paul_1974\">{{cite journal |author-first=W. J. |author-last=Paul |title=Boolesche Minimalpolynome und Überdeckungsprobleme |language=de |journal=Acta Inform. |volume=4 |number= |pages=321-336 |date=1974}}</ref>\n}}\n\n== Further reading==\n* {{cite book |title=A new approach to the design of switching circuits |author-first=H. Allen |author-last=Curtis |publisher=[[D. van Nostrand Company, Inc.]] |date=1962 |location=Princeton, New Jersey, USA |series=The Bell Laboratories Series |chapter=Chapter 2.3. McCluskey's Method |pages=90-160}}\n\n==External links==\n* [http://www.quinemccluskey.com Quine-McCluskey Solver], by Hatem Hassan.\n* [http://quine-mccluskey-frederic-carpon-implementation.e-geii.eu Quine-McCluskey algorithm implementation with a search of all solutions], by Frédéric Carpon.\n* [https://arxiv.org/abs/1203.2289 Modified Quine-McCluskey Method], by Vitthal Jadhav, Amar Buchade.\n* [http://www.embedded.com/electronics-blogs/programmer-s-toolbox/4025004/All-about-Quine-McClusky All about Quine-McClusky], article by Jack Crenshaw comparing Quine-McClusky to Karnaugh maps\n* [http://www.inf.ufrgs.br/logics/ Karċma 3], A set of logic synthesis tools including Karnaugh maps, Quine-McCluskey minimization, BDDs, probabilities, teaching module and more. Logic Circuits Synthesis Labs (LogiCS) - [[UFRGS]], Brazil.\n* A. Costa [http://accmdq.org/acc/bfunc/ BFunc], QMC based boolean logic simplifiers supporting up to 64 inputs / 64 outputs (independently) or 32 outputs (simultaneously)\n* [[Python (programming language)|Python]] [http://cheeseshop.python.org/pypi/qm/0.2 Implementation] by Robert Dick, with an [http://shiftlock.wordpress.com/2011/05/17/quine-mccluskey-algorithm-implementation-in-python/ optimized version].\n* [[Python (programming language)|Python]] [http://symlog.git.sourceforge.net/git/gitweb.cgi?p=symlog/symlog;a=blob_plain;f=symlog/logic.py;hb=HEAD Implementation] for symbolically reducing Boolean expressions.\n* [http://sourceforge.net/projects/quinessence/ Quinessence], an open source implementation written in Free Pascal by Marco Caminati.\n* [https://cran.r-project.org/web/packages/QCA/index.html QCA] an open source, R based implementation used in the social sciences, by Adrian Duşa\n* A series of two articles describing the algorithm(s) implemented in R: [https://web.archive.org/web/20120313031147/http://www.compasss.org/files/WPfiles/Dusa2007.pdf first article] and [https://web.archive.org/web/20120313031159/http://www.compasss.org/files/WPfiles/Dusa2007a.pdf second article]. The R implementation is exhaustive and it offers complete and exact solutions. It processes up to 20 input variables.\n* [http://www.p0p0v.com/science/#_minBool minBool] an implementation by Andrey Popov.\n* [http://www-ihs.theoinf.tu-ilmenau.de/~sane/projekte/qmc/embed_qmc.html QMC applet], an applet for a step by step analyze of the QMC- algorithm by Christian Roth\n* [http://sourceforge.net/projects/qmcs C++ implementation] SourceForge.net C++ program implementing the algorithm.\n* [https://metacpan.org/module/Algorithm::QuineMcCluskey Perl Module] by Darren M. Kulp.\n* [http://sites.google.com/site/simpogical/download Tutorial] Tutorial on Quine-McCluskey and Petrick's method (pdf).\n* [https://code.google.com/p/quine-mccluskey-petrick/source/browse/ Petrick] C++ implementation (including Petrick) based on the tutorial above \n* [http://sourceforge.net/projects/mini-qmc C program] Public Domain console based C program on SourceForge.net.\n<!-- * George Vastianos. [http://www.seattlerobotics.org/encoder/200106/qmccmin.htm Boolean functions' minimisation software based on the Quine-McCluskey method]. ''Encoder''. -->\n* [http://matwbn.icm.edu.pl/ksiazki/amc/amc13/amc13414.pdf Tomaszewski, S. P., Celik, I. U., Antoniou, G. E., \"WWW-based Boolean function minimization\" INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE, VOL 13; PART 4, pages 577-584, 2003.]\n* For a fully worked out example visit: http://www.cs.ualberta.ca/~amaral/courses/329/webslides/Topic5-QuineMcCluskey/sld024.htm\n* An excellent resource detailing each step: [http://www.ocoudert.com/papers/pdf/int94.pdf Olivier Coudert \"Two-level logic minimization: an overview\" INTEGRATION, the VLSI journal, 17-2, pp. 97–140, October 1994]\n* The Boolean Bot: A JavaScript implementation for the web: http://booleanbot.com/\n* [http://sourceforge.net/projects/qmclm/ open source gui QMC minimizer]\n* [https://arxiv.org/abs/1404.3349 Computer Simulation Codes for the Quine-McCluskey Method], by Sourangsu Banerji.\n\n{{DEFAULTSORT:Quine-Mccluskey Algorithm}}\n\n[[Category:Boolean algebra]]\n[[Category:Willard Van Orman Quine]]"
    },
    {
      "title": "Random algebra",
      "url": "https://en.wikipedia.org/wiki/Random_algebra",
      "text": "In set theory, the '''random algebra''' or '''random real algebra''' is the Boolean algebra of Borel sets of the unit interval modulo the ideal of measure zero sets. It is used in '''random forcing''' to add '''random reals''' to a model of set theory. The random algebra was studied by [[John von Neumann]] in 1935 (in work later published as {{harvtxt|Neumann|1998|loc=p. 253}}) who showed that it is not isomorphic to the [[Cantor algebra]] of Borel sets modulo [[meager set]]s. Random forcing was introduced by {{harvtxt|Solovay|1970}}.\n\n==References==\n\n*{{citation|mr=2768686 \n|last=Bartoszyński|first= Tomek|author-link= Tomek Bartoszyński\n|chapter=Invariants of measure and category|title= Handbook of set theory|volume= 2|pages= 491–555|publisher= Springer|year= 2010}} \n*{{citation|mr=0485358 |last= Bukowský|first= Lev|chapter= Random forcing|title= Set theory and hierarchy theory, V (Proc. Third Conf., Bierutowice, 1976)|pages= 101–117|series= Lecture Notes in Math.|volume= 619|publisher= Springer|place= Berlin|year= 1977}}\n*{{Citation | last1=Solovay | first1=Robert M. | author1-link=Robert M. Solovay | title=A model of set-theory in which every set of reals is Lebesgue measurable | jstor=1970696 | mr=0265151 | year=1970 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=92 | pages=1–56 | doi=10.2307/1970696}}\n*{{Citation | last1=Neumann | first1=John von| author1-link=John von Neumann | title=Continuous geometry | origyear=1960 | url=https://books.google.com/books?id=onE5HncE-HgC | publisher=[[Princeton University Press]] | series=Princeton Landmarks in Mathematics | isbn=978-0-691-05893-1 | mr=0120174 | year=1998}}\n\n[[Category:Boolean algebra]]\n[[Category:Forcing (mathematics)]]"
    },
    {
      "title": "Read-once function",
      "url": "https://en.wikipedia.org/wiki/Read-once_function",
      "text": "In mathematics, a '''read-once function''' is a special type of [[Boolean function]] that can be described by a [[Boolean expression]] in which each [[Variable (mathematics)|variable]] appears only once.\n\nMore precisely, the expression is required to use only the operations of [[logical conjunction]], [[logical disjunction]], and [[negation]]. By applying [[De Morgan's laws]], such an expression can be transformed into one in which negation is used only on individual variables (still with each variable appearing only once). By replacing each negated variable with a new positive variable representing its negation, such a function can be transformed into an equivalent [[Monotonic function|positive]] read-once Boolean function, represented by a read-once expression without negations.<ref>{{harvtxt|Golumbic|Gurvich|2011}}, p.&nbsp;519.</ref>\n\n==Examples==\nFor example, for three variables {{mvar|a}}, {{mvar|b}}, and {{mvar|c}}, the expressions\n:<math>a \\wedge b \\wedge c</math>\n:<math>a \\wedge (b \\vee c)</math>\n:<math>(a \\wedge b)\\vee c</math>, and\n:<math>a\\vee b\\vee c</math>\nare all read-once (as are the other functions obtained by permuting the variables in these expressions). However, the Boolean [[Median algebra|median]] operation, given by the expression\n:<math>(a\\vee b)\\wedge (a\\vee c)\\wedge (b\\wedge c)</math>\nis not read-once: this formula has more than one copy of each variable, and there is no equivalent formula that uses each variable only once.<ref>{{harvtxt|Golumbic|Gurvich|2011}}, p.&nbsp;520.</ref>\n\n==Characterization==\nThe [[disjunctive normal form]] of a (positive) read-once function is not generally itself read-once. Nevertheless, it carries important information about the function. In particular, if one forms a ''co-occurrence graph'' in which the vertices represent variables, and edges connect pairs of variables that both occur in the same clause of the conjunctive normal form, then the co-occurrence graph of a read-once function is necessarily a [[cograph]]. More precisely, a positive Boolean function is read-once if and only if its co-occurrence graph is a cograph, and in addition every [[maximal clique]] of the co-occurrence graph forms one of the conjunctions (prime implicants) of the disjunctive normal form.<ref>{{harvtxt|Golumbic|Gurvich|2011}}, Theorem 10.1, p.&nbsp;521; {{harvtxt|Golumbic|Mintz|Rotics|2006}}.</ref> That is, when interpreted as a function on sets of vertices of its co-occurrence graph, a read-once function is true for sets of vertices that contain a maximal clique, and false otherwise. \nFor instance the median function has the same co-occurrence graph as the conjunction of three variables, a [[triangle graph]], but the three-vertex complete subgraph of this graph (the whole graph) forms a subset of a clause only for the conjunction and not for the median.<ref>{{harvtxt|Golumbic|Gurvich|2011}}, Examples {{math|''f''<sub>2</sub>}} and {{math|''f''<sub>3</sub>}}, p.&nbsp;521.</ref>\nTwo variables of a positive read-once expression are adjacent in the co-occurrence graph if and only if their [[lowest common ancestor]] in the expression is a conjunction,<ref>{{harvtxt|Golumbic|Gurvich|2011}}, Lemma 10.1, p.&nbsp;529.</ref> so the expression tree can be interpreted as a cotree for the corresponding cograph.<ref>{{harvtxt|Golumbic|Gurvich|2011}}, Remark 10.4, pp.&nbsp;540–541.</ref>\n\nAnother alternative characterization of positive read-once functions combines their disjunctive and [[conjunctive normal form]]. A positive function of a given system of variables, that uses all of its variables, is read-once if and only if every prime implicant of the disjunctive normal form and every clause of the conjunctive normal form have exactly one variable in common.<ref>{{harvtxt|Gurvič|1977}}; {{harvtxt|Mundici|1989}}; {{harvtxt|Karchmer|Linial|Newman|Saks|1993}}.</ref>\n\n==Recognition==\nIt is possible to recognize read-once functions from their disjunctive normal form expressions in [[polynomial time]].<ref>{{harvtxt|Golumbic|Gurvich|2011}}, Theorem 10.8, p.&nbsp;541; {{harvtxt|Golumbic|Mintz|Rotics|2006}}; {{harvtxt|Golumbic|Mintz|Rotics|2008}}.</ref>\nIt is also possible to find a read-once expression for a positive read-once function, given access to the function only through a \"black box\" that allows its evaluation at any [[truth assignment]], using only a quadratic number of function evaluations.<ref>{{harvtxt|Golumbic|Gurvich|2011}}, Theorem 10.9, p.&nbsp;548; {{harvtxt|Angluin|Hellerstein|Karpinski|1993}}.</ref>\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n{{refbegin|30em}}\n*{{citation\n | last1 = Angluin | first1 = Dana | author1-link = Dana Angluin\n | last2 = Hellerstein | first2 = Lisa\n | last3 = Karpinski | first3 = Marek | author3-link = Marek Karpinski\n | doi = 10.1145/138027.138061\n | issue = 1\n | journal = [[Journal of the ACM]]\n | mr = 1202143\n | pages = 185–210\n | title = Learning read-once formulas with queries\n | volume = 40\n | year = 1993| citeseerx = 10.1.1.7.5033 }}.\n*{{citation\n | last1 = Golumbic | first1 = Martin C. | author1-link = Martin Charles Golumbic\n | last2 = Gurvich | first2 = Vladimir\n | editor1-last = Crama | editor1-first = Yves\n | editor2-last = Hammer | editor2-first = Peter L.\n | contribution = Read-once functions\n | contribution-url = http://www.cs.haifa.ac.il/~golumbic/courses/algorithmic-graph-theory/slides_and_notes_of_lectures/Lecture%207%20-%20Cographs%20and%20their%20Applications/readonce-chapter-final.pdf\n | doi = 10.1017/CBO9780511852008\n | isbn = 978-0-521-84751-3\n | mr = 2742439\n | pages = 519–560\n | publisher = Cambridge University Press, Cambridge\n | series = Encyclopedia of Mathematics and its Applications\n | title = Boolean functions\n | volume = 142\n | year = 2011}}.\n*{{citation\n | last1 = Golumbic | first1 = Martin Charles | author1-link = Martin Charles Golumbic\n | last2 = Mintz | first2 = Aviad\n | last3 = Rotics | first3 = Udi\n | doi = 10.1016/j.dam.2005.09.016\n | issue = 10\n | journal = [[Discrete Applied Mathematics]]\n | mr = 2222833\n | pages = 1465–1477\n | title = Factoring and recognition of read-once functions using cographs and normality and the readability of functions associated with partial {{mvar|k}}-trees\n | volume = 154\n | year = 2006}}.\n*{{citation\n | last1 = Golumbic | first1 = Martin Charles | author1-link = Martin Charles Golumbic\n | last2 = Mintz | first2 = Aviad\n | last3 = Rotics | first3 = Udi\n | doi = 10.1016/j.dam.2008.02.011\n | issue = 10\n | journal = [[Discrete Applied Mathematics]]\n | mr = 2432929\n | pages = 1633–1636\n | title = An improvement on the complexity of factoring read-once Boolean functions\n | volume = 156\n | year = 2008}}.\n*{{citation\n | last = Gurvič | first = V. A.\n | issue = 1(193)\n | journal = [[Russian Mathematical Surveys|Uspekhi Matematicheskikh Nauk]]\n | mr = 0441560\n | pages = 183–184\n | title = Repetition-free Boolean functions\n | url = http://mi.mathnet.ru/eng/umn3055\n | volume = 32\n | year = 1977}}.\n*{{citation\n | last1 = Karchmer | first1 = M.\n | last2 = Linial | first2 = N. | author2-link = Nati Linial\n | last3 = Newman | first3 = I.\n | last4 = Saks | first4 = M. | author4-link = Michael Saks (mathematician)\n | last5 = Wigderson | first5 = A. | author5-link = Avi Wigderson\n | doi = 10.1016/0012-365X(93)90372-Z\n | issue = 1–3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | mr = 1217758\n | pages = 275–282\n | title = Combinatorial characterization of read-once formulae\n | volume = 114\n | year = 1993}}.\n*{{citation\n | last = Mundici | first = Daniele\n | doi = 10.1016/0304-3975(89)90150-3\n | issue = 1\n | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]\n | mr = 1018849\n | pages = 113–114\n | title = Functions computed by monotone Boolean formulas with no repeated variables\n | volume = 66\n | year = 1989}}.\n{{refend}}\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Reed–Muller expansion",
      "url": "https://en.wikipedia.org/wiki/Reed%E2%80%93Muller_expansion",
      "text": "{{Technical|date=December 2018}}\nIn [[Boolean logic]], a '''Reed–Muller expansion''' (or '''Davio expansion''') is a [[Decomposition (computer science)|decomposition]] of a [[Boolean function]].\n\nFor a Boolean function <math>f(x_1,\\ldots,x_n) : \\mathbb{B}^n \\to \\mathbb{B}</math> we call\n\n:<math>\n\\begin{align}\nf_{{x_i}}(x) & = f(x_1,\\ldots,x_{i-1},1,x_{i+1},\\ldots,x_n) \\\\\nf_{\\overline{x}_i}(x)& = f(x_1,\\ldots,x_{i-1},0,x_{i+1},\\ldots,x_n)\n\\end{align}\n</math>\n\nthe positive and negative [[Shannon expansion|cofactor]]s of <math>f</math> with respect to <math>x_i</math>, and\n:<math>\n\\begin{align}\n\\frac{\\partial f}{\\partial x_i} & = f_{x_i}(x) \\oplus f_{\\overline{x}_i}(x)\n\\end{align}\n</math>\nthe boolean derivation of <math>f</math> with respect to <math>x_i</math>, where <math>{\\oplus}</math> denotes the [[Exclusive or|XOR]] operator.\n\nThen we have for the Reed–Muller or positive Davio expansion:\n\n:<math>\nf = f_{\\overline{x}_i} \\oplus x_i \\frac{\\partial f}{\\partial x_i}.\n</math>\n\n==Description==\n\nThis equation is written in a way that it resembles a [[Taylor series|Taylor expansion]] of <math>f</math> about <math>x_i=0</math>. There is a similar decomposition corresponding to an expansion about <math>x_i=1</math> (negative Davio):\n\n:<math>\nf = f_{x_i} \\oplus \\overline{x}_i \\frac{\\partial f}{\\partial x_i}.\n</math>\n\nRepeated application of the Reed–Muller expansion results in an XOR polynomial in <math>x_1,\\ldots,x_n</math>:\n\n:<math>\nf = a_1 \\oplus a_2 x_1 \\oplus a_3 x_2 \\oplus a_4 x_1 x_2 \\oplus \\ldots \\oplus a_{2^n} x_1\\cdots x_n\n</math>\n\nThis representation is unique and sometimes also called Reed–Muller expansion.<ref name=\"Kebschull_1992\"/>\n\nE.g. for <math>n=2</math> the result would be\n\n:<math>\nf(x_1, x_2) = f_{\\overline{x}_1 \\overline{x}_2} \\oplus \\frac{\\partial f_{\\overline{x}_2}}{\\partial x_1} x_1 \\oplus \\frac{\\partial f_{\\overline{x}_1}}{\\partial x_2} x_2 \\oplus \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} x_1 x_2\n</math>\n\nwhere \n\n:<math> {\\partial^2 f \\over \\partial x_1 \\partial x_2} = f_{\\bar x_1 \\bar x_2} \\oplus f_{\\bar x_1 x_2} \\oplus f_{x_1 \\bar x_2} \\oplus f_{x_1 x_2} </math>.\n\nFor <math>n = 3</math> the result would be\n\n:<math> f(x_1, x_2, x_3) = f_{\\bar x_1 \\bar x_2 \\bar x_3} \\oplus {\\partial f_{\\bar x_2 \\bar x_3} \\over \\partial x_1} x_1 \\oplus {\\partial f_{\\bar x_1 \\bar x_3} \\over \\partial x_2} x_2 \\oplus {\\partial f_{\\bar x_1 \\bar x_2} \\over \\partial x_3} x_3 \\oplus {\\partial^2 f_{\\bar x_3} \\over \\partial x_1 \\partial x_2} x_1 x_2 \\oplus {\\partial^2 f_{\\bar x_2} \\over \\partial x_1 \\partial x_3} x_1 x_3 \\oplus {\\partial^2 f_{\\bar x_1} \\over \\partial x_2 \\partial x_3} x_2 x_3 \\oplus {\\partial^3 f \\over \\partial x_1 \\partial x_2 \\partial x_3} x_1 x_2 x_3 </math>\n\nwhere\n\n:<math> {\\partial^3 f \\over \\partial x_1 \\partial x_2 \\partial x_3} = f_{\\bar x_1 \\bar x_2 \\bar x_3} \\oplus f_{\\bar x_1 \\bar x_2 x_3} \\oplus f_{\\bar x_1 x_2 \\bar x_3} \\oplus f_{\\bar x_1  x_2  x_3} \\oplus f_{x_1 \\bar x_2 \\bar x_3} \\oplus f_{x_1 \\bar x_2 x_3} \\oplus f_{x_1 x_2 \\bar x_3} \\oplus f_{x_1 x_2 x_3} </math>.\n\n==Geometric interpretation==\n\nThis <math>n = 3</math> case can be given a cubical geometric interpretation (or a graph-theoretic interpretation) as follows: when moving along the edge from <math>\\bar x_1 \\bar x_2 \\bar x_3</math> to <math>x_1 \\bar x_2 \\bar x_3</math>, XOR up the functions of the two end-vertices of the edge in order to obtain the coefficient of <math>x_1</math>. To move from <math>\\bar x_1 \\bar x_2 \\bar x_3</math> to <math>x_1 x_2 \\bar x_3</math> there are two shortest paths: one is a two-edge path passing through <math>x_1 \\bar x_2 \\bar x_3</math> and the other one a two-edge path passing through <math>\\bar x_1 x_2 \\bar x_3</math>. These two paths encompass four vertices of a square, and XORing up the functions of these four vertices yields the coefficient of <math>x_1 x_2</math>. Finally, to move from <math>\\bar x_1 \\bar x_2 \\bar x_3</math> to <math>x_1 x_2 x_3</math> there are six shortest paths which are three-edge paths, and these six paths encompass all the vertices of the cube, therefore the coefficient of <math>x_1 x_2 x_3</math> can be obtained by XORing up the functions of all eight of the vertices. (The other, unmentioned coefficients can be obtained by symmetry.)\n\n==Paths==\nThe shortest paths all involve monotonic changes to the values of the variables, whereas non-shortest paths all involve non-monotonic changes of such variables; or, to put it another way, the shortest paths all have lengths equal to the Hamming distance between the starting and destination vertices. This means that it should be easy to generalize an algorithm for obtaining coefficients from a truth table by XORing up values of the function from appropriate rows of a truth table, even for hyperdimensional cases (<math>n = 4</math> and above). Between the starting and destination rows of a truth table, some variables have their values remaining fixed: find all the rows of the truth table such that those variables likewise remain fixed at those given values, then XOR up their functions and the result should be the coefficient for the monomial corresponding to the destination row. (In such monomial, include any variable whose value is 1 (at that row) and exclude any variable whose value is 0 (at that row), instead of including the negation of the variable whose value is 0, as in the minterm style.)\n\nSimilar to [[binary decision diagram]]s (BDDs), where nodes represent [[Shannon expansion]] with respect to the according variable, we can define a \n[[decision diagram]] based on the Reed–Muller expansion. These decision diagrams are called functional BDDs (FBDDs).\n\n== Derivations ==\nThe Reed–Muller expansion can be derived from the XOR-form of the Shannon decomposition, using the identity <math>\\overline{x} = 1 \\oplus x</math>:\n\n:<math>\n\\begin{align}\nf & = x_i f_{x_i} \\oplus \\overline{x}_i f_{\\overline{x}_i} \\\\\n  & = x_i f_{x_i} \\oplus (1 \\oplus x_i) f_{\\overline{x}_i} \\\\\n  & = x_i f_{x_i} \\oplus f_{\\overline{x}_i} \\oplus x_i f_{\\overline{x}_i} \\\\\n  & = f_{\\overline{x}_i} \\oplus x_i \\frac{\\partial f}{\\partial x_i}.\n\\end{align}\n</math>\n\n\nDerivation of the expansion for <math>n = 2</math>:\n\n:<math>\\begin{align}\n f & = f_{\\bar x_1} \\oplus x_1 {\\partial f \\over \\partial x_1} \\\\\n    & = \\Big( f_{\\bar x_2} \\oplus x_2 {\\partial f \\over \\partial x_2} \\Big)_{\\bar x_1} \\oplus x_1 {\\partial \\Big(f_{\\bar x_2} \\oplus x_2 {\\partial f \\over \\partial x_2} \\Big) \\over \\partial x_1} \\\\\n     & = f_{\\bar x_1 \\bar x_2} \\oplus x_2 {\\partial f_{\\bar x_1} \\over \\partial x_2} \\oplus x_1 \\Big({\\partial f_{\\bar x_2} \\over \\partial x_1} \\oplus x_2 {\\partial^2 f \\over \\partial x_1 \\partial x_2}\\Big) \\\\\n      & = f_{\\bar x_1 \\bar x_2} \\oplus x_2 {\\partial f_{\\bar x_1} \\over \\partial x_2} \\oplus x_1 {\\partial f_{\\bar x_2} \\over \\partial x_1} \\oplus x_1 x_2 {\\partial^2 f \\over \\partial x_1 \\partial x_2}. \n \\end{align} </math>\n\n\nDerivation of the second-order boolean derivative:\n\n:<math> \\begin{align}\n{\\partial^2 f \\over \\partial x_1 \\partial x_2} & = {\\partial \\over \\partial x_1} \\Big( {\\partial f \\over \\partial x_2} \\Big) = {\\partial \\over \\partial x_1} (f_{\\bar x_2} \\oplus f_{x_2}) \\\\\n  & = (f_{\\bar x_2} \\oplus f_{x_2})_{\\bar x_1} \\oplus (f_{\\bar x_2} \\oplus f_{x_2})_{x_1} \\\\\n  & = f_{\\bar x_1 \\bar x_2} \\oplus f_{\\bar x_1 x_2} \\oplus f_{x_1 \\bar x_2} \\oplus f_{x_1 x_2}.\n  \\end{align}</math>\n\n==See also==\n* [[Algebraic normal form]] (ANF)\n* [[Ring sum normal form]] (RSNF)\n* [[Zhegalkin normal form]]\n* [[Karnaugh map]]\n* [[Irving Stoy Reed]]\n* [[David Eugene Muller]]\n* [[Reed–Muller code]]\n\n==References==\n{{reflist|refs=\n<ref name=\"Kebschull_1992\">{{cite journal |author-first1=U. |author-last1=Kebschull |author-first2=E. |author-last2=Schubert |author-first3=W. |author-last3=Rosenstiel |title=Multilevel logic synthesis based on functional decision diagrams |journal=Proceedings 3rd European Conference on Design Automation |date=1992}}</ref>\n}}\n\n==Further reading==\n* {{cite journal |author-last1=Kebschull |author-first1=U. |author-last2=Rosenstiel |author-first2=W. |title=Efficient graph-based computation and manipulation of functional decision diagrams |journal=Proceedings 4th European Conference on Design Automation |date=1993 |pages=278–282}}\n* {{cite web |title=Reed-Muller Logic |work=Logic 101 |at=Part 3 |author-first=Clive \"Max\" |author-last=Maxfield |date=2006-11-29 |publisher=[[EETimes]] |url=http://www.eetimes.com/author.asp?section_id=216&doc_id=1274545 |access-date=2017-04-19 |dead-url=no |archive-url=https://web.archive.org/web/20170419235904/http://www.eetimes.com/author.asp?section_id=216&doc_id=1274545 |archive-date=2017-04-19}}\n* {{cite book |author-first1=Bernd |author-last1=Steinbach |author-link1=:de:Bernd Steinbach |author-first2=Christian |author-last2=Posthoff |title=Logic Functions and Equations - Examples and Exercises |chapter=Preface |publisher=[[Springer Science + Business Media B. V.]] |date=2009 |edition=1st |isbn=978-1-4020-9594-8 |lccn=2008941076 |page=xv}}\n\n{{DEFAULTSORT:Reed-Muller expansion}}\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Relation algebra",
      "url": "https://en.wikipedia.org/wiki/Relation_algebra",
      "text": "{{for|the concept related to databases|Relational algebra}}\n\nIn [[mathematics]] and [[abstract algebra]], a '''relation algebra''' is a [[residuated Boolean algebra]] [[reduct|expanded]] with an [[involution (mathematics)|involution]] called '''converse''', a unary operation. The motivating example of a relation algebra is the algebra 2<sup>''X''²</sup> of all [[binary relation]]s on a set ''X'', that is, subsets of the [[cartesian square]] ''X''<sup>2</sup>, with ''R''•''S'' interpreted as the usual [[Composition of relations|composition of binary relations]] ''R'' and ''S'', and with the converse of ''R'' as the [[converse relation]].\n\nRelation algebra emerged in the 19th-century work of [[Augustus De Morgan]] and [[Charles Sanders Peirce|Charles Peirce]], which culminated in the [[algebraic logic]] of [[Ernst Schröder]]. The equational form of relation algebra treated here was developed by [[Alfred Tarski]] and his students, starting in the 1940s. Tarski and Givant (1987) applied relation algebra to a variable-free treatment of [[axiomatic set theory]], with the implication that mathematics founded on set theory could itself be conducted without variables.\n\n==Definition==\nA '''relation algebra''' (''L'', ∧, ∨, <sup>&minus;</sup>, 0, 1, •, '''I''', ˘) is an algebraic structure equipped with the [[Introduction to Boolean algebra|Boolean operations]] of conjunction ''x''∧''y'', disjunction ''x''∨''y'', and negation ''x''<sup>&minus;</sup>, the Boolean constants 0 and 1, the relational operations of [[composition of relations|composition]] ''x''•''y'' and [[converse relation|converse]] ''x''˘, and the relational constant '''I''', such that these operations and constants satisfy certain equations constituting an axiomatization of a [[algebraic logic#Calculus of relations|calculus of relations]]. Roughly, a relation algebra is to a system of binary relations on a set containing the [[empty relation|empty]] (0), [[universal relation|complete]] (1), and [[identity relation|identity]] ('''I''') relations and closed under these five operations as a [[group (mathematics)|group]] is to a system of [[permutation]]s of a set containing the identity permutation and closed under composition and inverse. However, the [[first-order logic|first order]] [[theory (logic)|theory]] of relation algebras is not [[completeness (logic)|complete]] for such systems of binary relations.\n\nFollowing Jónsson and Tsinakis (1993) it is convenient to define additional operations ''x''◁''y'' = ''x''•''y''˘, and, dually,  ''x''▷''y'' = ''x''˘•''y'' .  Jónsson and Tsinakis showed that '''I'''◁''x'' = ''x''▷'''I''', and that both were equal to ''x''˘.  Hence a relation algebra can equally well be defined as an algebraic structure (''L'', ∧, ∨, <sup>&minus;</sup>, 0, 1, •, '''I''', ◁, ▷).  The advantage of this [[signature (logic)|signature]] over the usual one is that a relation algebra can then be defined in full simply as a [[residuated Boolean algebra]] for which '''I'''◁''x'' is an involution, that is, '''I'''◁('''I'''◁''x'') = ''x'' .  The latter condition can be thought of as the relational counterpart of the equation 1/(1/''x'') = ''x'' for ordinary arithmetic [[multiplicative inverse|reciprocal]], and some authors use reciprocal as a synonym for converse.\n\nSince residuated Boolean algebras are axiomatized with finitely many identities, so are relation algebras. Hence the latter form a [[Variety (universal algebra)|variety]], the variety '''RA''' of relation algebras.  Expanding the above definition as equations yields the following finite axiomatization.\n\n===Axioms===\nThe axioms '''B1-B10''' below are adapted from Givant (2006: 283), and were first set out by [[alfred Tarski|Tarski]] in 1948.<ref>[[Alfred Tarski]] (1948) \"Abstract: Representation Problems for Relation Algebras,\" ''Bulletin of the AMS'' 54: 80.</ref>\n\n''L'' is a [[Boolean algebra (structure)|Boolean algebra]] under binary [[disjunction]], ∨, and unary [[Complement (order theory)|complementation]] ()<sup>−</sup>:\n:'''B1''': ''A'' ∨ ''B'' = ''B'' ∨ ''A''\n:'''B2''': ''A'' ∨ (''B'' ∨ ''C'') = (''A'' ∨ ''B'') ∨ ''C''\n:'''B3''': (''A''<sup>−</sup> ∨ ''B'')<sup>−</sup> ∨ (''A''<sup>−</sup> ∨ ''B''<sup>−</sup>)<sup>−</sup> = ''A''\nThis axiomatization of Boolean algebra is due to [[Edward Vermilye Huntington|Huntington]] (1933).  Note that the meet of the implied Boolean algebra is ''not'' the • operator (even though it distributes over <math>\\vee</math> like a meet does), nor is the 1 of the Boolean algebra the '''I''' constant.\n\n''L'' is a [[monoid]] under binary [[composition of relations|composition]] (•) and [[nullary]] identity '''I''':\n:'''B4''': ''A''•(''B''•''C'') = (''A''•''B'')•''C''\n:'''B5''': ''A''•'''I''' = ''A''\n\nUnary [[converse relation|converse]] ()˘ is an [[semigroup with involution|involution with respect to composition]]:\n:'''B6''': ''A''˘˘ = ''A'' \n:'''B7''': (''A''•''B'')˘ = ''B''˘•''A''˘\n\nAxiom B6 defines conversion as an [[involution (mathematics)|involution]], whereas B7 expresses the [[antidistributive]] property of conversion relative to composition.<ref name=\"BrinkKahl1997\">{{cite book|author1=Chris Brink|author2=Wolfram Kahl|author3=Gunther Schmidt|title=Relational Methods in Computer Science|date=1997|publisher=Springer|isbn=978-3-211-82971-4|pages=4 and 8}}</ref>\n\nConverse and composition [[distributive law|distribute]] over disjunction:\n:'''B8''': (''A''∨''B'')˘ = ''A''˘∨''B''˘\n:'''B9''': (''A''∨''B'')•''C'' = (''A''•''C'')∨(''B''•''C'')\n\n'''B10''' is Tarski's equational form of the fact, discovered by [[Augustus De Morgan]], that ''A''•''B'' &le; ''C''<sup>−</sup>  {{eqv}} ''A''˘•''C'' &le; ''B''<sup>−</sup>  {{eqv}} ''C''•''B''˘ &le; ''A''<sup>−</sup>.\n:'''B10''': (''A''˘•(''A''•''B'')<sup>−</sup>)∨''B''<sup>−</sup> = ''B''<sup>−</sup>\n\nThese axioms are [[ZFC]] theorems; for the purely Boolean '''B1-B3''', this fact is trivial. After each of the following axioms is shown the number of the corresponding theorem in Chapter 3 of Suppes (1960), an exposition of ZFC: '''B4''' 27, '''B5''' 45, '''B6''' 14, '''B7''' 26, '''B8''' 16, '''B9''' 23.\n\n==Expressing properties of binary relations in RA==\nThe following table shows how many of the usual properties of [[binary relation]]s can be expressed as succinct '''RA''' equalities or inequalities. Below, an inequality of the form ''A''≤''B'' is shorthand for the Boolean equation ''A''∨''B'' = ''B''.\n\nThe most complete set of results of this nature is Chapter C of Carnap (1958), where the notation is rather distant from that of this entry. Chapter  3.2 of Suppes (1960) contains fewer results, presented as [[ZFC]] theorems and using a notation that more resembles that of this entry. Neither Carnap nor Suppes formulated their results using the '''RA''' of this entry, or in an equational manner.\n{| class=wikitable\n|-\n!''R'' is!![[If and only if]]:\n|-\n|- style=\"border-top:1px solid #999;\"\n|-\n|[[Functional relation|Functional]]||''R''˘•''R'' ≤ '''I'''  \n|-\n|[[Binary relation#Special types of binary relations|Left-total]]||'''I''' ≤ ''R''•''R''˘ (''R''˘ is surjective)\n|-\n|[[Function (mathematics)|Function]]||functional and left-total.\n|-\n|[[Injective]]<br>|| ''R''•''R''˘ ≤ '''I''' (''R''˘ is functional) \n|-\n|[[Surjective]]|| '''I''' ≤ ''R''˘•''R'' (''R''˘ is left-total)\n|-\n|[[Bijection]]|| ''R''˘•''R'' = ''R''•''R''˘ = '''I''' (Injective surjective function)\n|-\n|[[Transitive relation|Transitive]]||''R''•''R'' ≤ ''R''\n|-\n|[[Reflexive relation|Reflexive]]||'''I''' ≤ ''R''\n|-\n|[[Coreflexive relation|Coreflexive]]||''R'' ≤ '''I'''\n|-\n|[[Irreflexive relation|Irreflexive]]||''R'' &and; '''I''' = 0\n|-\n|[[Symmetric relation|Symmetric]]||''R''˘ = ''R''\n|-\n|[[Antisymmetric relation|Antisymmetric]]||''R'' &and; ''R''˘ ≤ '''I'''\n|-\n|[[Asymmetric relation|Asymmetric]]||''R'' &and; ''R''˘ = 0\n|-\n|[[Total relation|Total]]|| ''R'' ∨ ''R''˘ = 1 \n|-\n|[[Total relation|Connex]]|| '''I''' ∨ ''R'' ∨ ''R''˘ = 1 \n|-\n|[[Idempotent relation|Idempotent]]||''R''•''R'' = ''R'' \n|-\n|[[Preorder]]|| ''R'' is transitive and reflexive.\n|-\n|[[Equivalence relation|Equivalence]]||''R'' is a symmetric preorder.\n|-\n|[[Partial order]]|| ''R'' is an antisymmetric preorder.\n|-\n|[[Total order]]|| ''R'' is a total partial order.\n|-\n|[[Strict partial order]]||''R'' is transitive and irreflexive.\n|-\n|[[Total order|Strict total order]]|| ''R'' is a connex strict partial order.\n|-\n|[[Dense order|Dense]]|| ''R'' &and; '''I'''<sup>−</sup> ≤ (''R'' &and; '''I'''<sup>−</sup>)•(''R'' &and; '''I'''<sup>−</sup>).\n|}\n\n==Expressive power==\nThe [[metamathematics]] of '''RA''' are discussed at length in Tarski and Givant (1987), and more briefly in Givant (2006).\n\n'''RA''' consists entirely of equations manipulated using nothing more than uniform replacement and the substitution of equals for equals. Both rules are wholly familiar from school mathematics and from [[abstract algebra]] generally. Hence '''RA''' proofs are carried out in a manner familiar to all mathematicians, unlike the case in [[mathematical logic]] generally.\n\n'''RA''' can express any (and up to [[logical equivalence]], exactly the) [[first-order logic]] (FOL) formulas containing no more than three variables. (A given variable can be quantified multiple times and hence quantifiers can be nested arbitrarily deeply by \"reusing\" variables.){{cn|reason=See section 'Quantifier nesting' on talk page. Moreover, the treatment of ternary, etc., relations should be made clear.|date=March 2019}} Surprisingly, this fragment of FOL suffices to express [[Peano arithmetic]] and almost all [[axiomatic set theory|axiomatic set theories]] ever proposed. Hence '''RA''' is, in effect, a way of algebraizing nearly all mathematics, while dispensing with FOL and its [[Logical connective|connectives]], [[Quantifier (logic)|quantifier]]s, [[turnstile (symbol)|turnstiles]], and [[modus ponens]]. Because '''RA''' can express Peano arithmetic and set theory, [[Gödel's incompleteness theorems]] apply to it; '''RA''' is [[Gödel's incompleteness theorems|incomplete]], incompletable, and [[undecidable problem|undecidable]].{{Citation needed|date=April 2012}} (N.B. The Boolean algebra fragment of '''RA''' is complete and decidable.)\n\nThe '''representable relation algebras''', forming the class '''RRA''', are those relation algebras isomorphic to some relation algebra consisting of binary relations on some set, and closed under the intended interpretation of the '''RA''' operations. It is easily shown, e.g. using the method of [[pseudoelementary class]]es, that '''RRA''' is a [[quasivariety]], that is, axiomatizable by a [[universal Horn theory]]. In 1950, [[Roger Lyndon]] proved the existence of equations holding in '''RRA''' that did not hold in '''RA'''. Hence the variety generated by '''RRA''' is a proper subvariety of the variety '''RA'''. In 1955, [[Alfred Tarski]] showed that '''RRA''' is itself a variety. In 1964, Donald Monk showed that '''RRA''' has no finite axiomatization, unlike '''RA''', which is finitely axiomatized by definition.\n\n===Q-relation algebras===\nAn '''RA''' is a Q-relation algebra ('''QRA''') if, in addition to '''B1-B10''', there exist some ''A'' and ''B'' such that (Tarski and Givant 1987: §8.4):\n\n:'''Q0''': ''A''˘•''A'' ≤ '''I'''\n:'''Q1''': ''B''˘•''B'' ≤ '''I'''\n:'''Q2''': ''A''˘•''B'' = 1\n\nEssentially these axioms imply that the universe has a (non-surjective) pairing relation whose projections are ''A'' and ''B''. It is a theorem that every '''QRA''' is a '''RRA''' (Proof by Maddux, see Tarski & Givant 1987: 8.4(iii) ).\n\nEvery '''QRA''' is representable (Tarski and Givant 1987). That not every relation algebra is representable is a fundamental way '''RA''' differs from '''QRA''' and [[Boolean algebra (structure)|Boolean algebras]], which, by [[Stone's representation theorem for Boolean algebras]], are always representable as sets of subsets of some set, closed under union, intersection, and complement.\n\n== Examples ==\n1.  Any Boolean algebra can be turned into a '''RA''' by interpreting conjunction as composition (the monoid multiplication •), i.e. ''x''•''y'' is defined as ''x''∧''y''.  This interpretation requires that converse interpret identity (''ў'' = ''y''), and that both residuals ''y''\\''x'' and ''x''/''y'' interpret the conditional ''y''→''x''  (i.e., ¬''y''∨''x'').\n\n2.  The motivating example of a relation algebra depends on the definition of a binary relation ''R'' on a set ''X'' as any subset ''R'' ⊆ ''X''², where  ''X''² is the [[Cartesian square]] of ''X''. The power set 2<sup>''X''²</sup> consisting of all binary relations on ''X'' is a Boolean algebra. While  2<sup>''X''²</sup> can be made a relation algebra by taking ''R''•''S'' = ''R''∧''S'', as per example (1) above, the standard interpretation of • is instead ''x''(''R''•''S'')''z'' = ∃''y'':''xRy.ySz''.  That is, the [[ordered pair]] (''x'',''z'') belongs to the relation ''R''•''S'' just when there exists ''y'' ∈ ''X'' such that (''x'',''y'') ∈ ''R'' and (''y'',''z'') ∈ ''S''. This interpretation uniquely determines ''R''\\''S'' as consisting of all pairs (''y'',''z'') such that for all ''x'' ∈ ''X'', if ''xRy'' then ''xSz''. Dually, ''S''/''R'' consists of all pairs (''x'',''y'') such that for all ''z'' ∈ ''X'', if ''yRz'' then ''xSz''. The translation ''ў'' = ¬(y\\¬'''I''') then establishes the converse ''R''˘ of ''R'' as consisting of all pairs (''y'',''x'') such that (''x'',''y'') ∈ ''R''.\n\n3.  An important generalization of the previous example is the power set 2<sup>''E''</sup> where ''E'' ⊆ ''X''² is any [[equivalence relation]] on the set ''X''. This is a generalization because ''X''² is itself an equivalence relation, namely the complete relation consisting of all pairs. While 2<sup>''E''</sup> is not a subalgebra of 2<sup>''X''²</sup> when ''E'' ≠ ''X''² (since in that case it does not contain the relation ''X''², the top element 1 being ''E'' instead of ''X''²), it is nevertheless turned into a relation algebra using the same definitions of the operations. Its importance resides in the definition of a ''representable relation algebra'' as any relation algebra isomorphic to a subalgebra of the relation algebra 2<sup>''E''</sup> for some equivalence relation ''E'' on some set. The previous section says more about the relevant metamathematics.\n\n4. Let <math>G</math> be group. Then the power set <math>2^G</math> is a relation algebra with the obvious boolean algebra operations, composition given by the [[product of group subsets]], the converse by the inverse subset (<math>A^{-1} = \\{a^{-1}\\mid a\\in A\\}</math>), and the identity by the singleton subset <math>\\{e\\}</math>. There is a relation algebra homomorphism embedding <math>2^G</math> in <math>2^{G\\times G}</math> which sends each subset <math>A\\subset G</math> to the relation\n<math>R_A = \\{(g, h)\\in G \\times G\\mid h\\in A g\\}</math>. The image of this homomorphism is the set of all right-invariant relations on <math>G</math>. \n\n5. If group sum or product interprets composition, [[Group_(mathematics)#Definition|group inverse]] interprets converse, group identity interprets '''I''', and if ''R'' is a [[one-to-one correspondence]], so that ''R''˘•''R'' = ''R•R''˘ = '''I''',<ref>[[Alfred Tarski|Tarski, A.]] (1941), p. 87.</ref> then ''L'' is a [[group (mathematics)|group]] as well as a [[monoid]]. '''B4'''-'''B7''' become well-known theorems of [[group theory]], so that '''RA''' becomes a [[proper extension]] of [[group theory]] as well as of Boolean algebra.\n\n==Historical remarks==\n[[augustus De Morgan|De Morgan]] founded '''RA''' in 1860, but [[Charles Sanders Peirce|C. S. Peirce]] took it much further and became fascinated with its philosophical power. The work of DeMorgan and Peirce came to be known mainly in the extended and definitive form [[Ernst Schröder]] gave it in Vol. 3 of his ''Vorlesungen'' (1890–1905). ''[[Principia Mathematica]]'' drew strongly on Schröder's '''RA''', but acknowledged him only as the inventor of the notation. In 1912, [[Alwin Korselt]] proved that a particular formula in which the quantifiers were nested four deep had no '''RA''' equivalent.<ref>Korselt did not publish his finding. It was first published in [[Leopold Loewenheim]] (1915) \"Über Möglichkeiten im Relativkalkül,\" ''[[Mathematische Annalen]]'' 76: 447–470. Translated as \"On possibilities in the calculus of relatives\" in [[Jean van Heijenoort]], 1967. ''A Source Book in Mathematical Logic, 1879–1931''. Harvard Univ. Press: 228–251.</ref> This fact led to a loss of interest in '''RA''' until Tarski (1941) began writing about it. His students have continued to develop '''RA''' down to the present day. Tarski returned to '''RA''' in the 1970s with the help of Steven Givant; this collaboration resulted in the monograph by Tarski and Givant (1987), the definitive reference for this subject. For more on the history of '''RA''', see Maddux (1991, 2006).\n\n== Software ==\n* [http://relmics.mcmaster.ca/html/index.html RelMICS / Relational Methods in Computer Science] maintained by [http://www.cas.mcmaster.ca/~kahl/ Wolfram Kahl]\n* Carsten Sinz: [https://web.archive.org/web/20070627003141/http://www-sr.informatik.uni-tuebingen.de/~sinz/ARA/ ARA / An Automatic Theorem Prover for Relation Algebras]\n* [https://www.researchgate.net/profile/Stef_Joosten Stef Joosten], Relation Algebra as programming language using the Ampersand compiler, [https://www.sciencedirect.com/science/article/pii/S2352220817301499 Journal of Logical and Algebraic Methods in Programming], Volume 100, April 2018, Pages 113-129. (see also https://ampersandtarski.gitbook.io/documentation)\n\n==See also==\n{{col-begin}}\n{{col-break}}\n* [[Algebraic logic]]\n* [[Allegory (category theory)]]\n* [[Binary relation]]\n* [[Cartesian product]]\n* [[Cartesian square]]\n* [[Cylindric algebra]]s\n{{col-break}}\n* [[Extension (predicate logic)|Extension in logic]]\n* [[Involution (mathematics)|Involution]]\n* [[Logic of relatives]]\n* [[Logical matrix]]\n* [[Predicate functor logic]]\n* [[Quantale]]\n* [[Relation (mathematics)|Relation]]\n* [[Relation construction]]\n{{col-break}}\n* [[Relational calculus]]\n* [[Relational algebra]]\n* [[Residuated Boolean algebra]]\n* [[Spatial-temporal reasoning]]\n* [[Theory of relations]]\n* [[Triadic relation]]\n{{col-end}}\n\n==Footnotes==\n<references />\n\n==References==\n* {{cite book|last=Carnap|first=Rudolf|authorlink=Rudolf Carnap|year=1958|title=Introduction to Symbolic Logic and its Applications|publisher=Dover Publications}}\n* {{cite journal | first1=Steven | last1=Givant | year=2006 | title=The calculus of relations as a foundation for mathematics | journal=Journal of Automated Reasoning | volume=37 | issue=4 | pages=277–322 | doi=10.1007/s10817-006-9062-x}}\n* {{cite book|authorlink=Paul Richard Halmos|last=Halmos|first=P. R.|year=1960|title=Naive Set Theory|publisher=Van Nostrand.}}\n* {{cite book|last1=Henkin|first1=Leon|author1link=Leon Henkin|last2=Tarski|first2=Alfred|author2link=Alfred Tarski|last3=Monk|first3=J. D.|year=1971|title=Cylindric Algebras, Part 1|publisher=North Holland}}\n* {{cite book|last1=Henkin|first1=Leon|author1link=Leon Henkin|last2=Tarski|first2=Alfred|author2link=Alfred Tarski|last3=Monk|first3=J. D.|year=1985|title=Cylindric Algebras, Part 2|publisher=North Holland}}\n* {{cite book|last1=Hirsch|first1=R.|last2=Hodkinson|first2=I.|year=2002|title=Relation Algebra by Games|volume=147|series=Studies in Logic and the Foundations of Mathematics|publisher=Elsevier Science}}\n* {{cite journal | authorlink1=Bjarni Jónsson | last1=Jónsson | first1=Bjarni | first2=Constantine | last2=Tsinakis | year=1993 | title=Relation algebras as residuated Boolean algebras | journal=Algebra Universalis | volume=30 | issue=4 | pages=469–78 | doi=10.1007/BF01195378}}\n* {{cite journal | authorlink=Roger Maddux | last1=Maddux | first1=Roger | year=1991 | url=http://orion.math.iastate.edu/maddux/papers/Maddux1991.pdf | title=The Origin of Relation Algebras in the Development and Axiomatization of the Calculus of Relations | journal=Studia Logica | volume=50 | number=3–4 | pages=421–455 | doi=10.1007/BF00370681| citeseerx=10.1.1.146.5668 }}\n* {{cite book|last=Maddux|first=Roger|year=2006|title=Relation Algebras|volume=150|series=Studies in Logic and the Foundations of Mathematics|publisher=Elsevier Science|isbn=9780444520135}}\n*{{cite book |last=Suppes |first=Patrick |authorlink= Patrick Suppes |origyear=1960 |title= Axiomatic Set Theory |publisher= Van Nostrand |edition= Dover reprint |year=1972 |chapter= Chapter 3}}\n*{{cite book|last=Schmidt|first=Gunther|authorlink=Gunther Schmidt|year=2010|title=Relational Mathematics|publisher=Cambridge University Press}}\n*{{cite journal | authorlink=Alfred Tarski | last1=Tarski | first1=Alfred | year=1941 | title=On the calculus of relations | journal=Journal of Symbolic Logic | volume=6 | issue=3 | pages=73–89 | jstor=2268577 | doi=10.2307/2268577}}\n*{{cite book|last1=Tarski | first1=Alfred |last2=Givant|first2=Steven|year=1987|title=A Formalization of Set Theory without Variables|location=Providence RI|publisher=American Mathematical Society}}\n\n==External links==\n*Yohji AKAMA, Yasuo Kawahara, and Hitoshi Furusawa, \"[https://web.archive.org/web/19980713070139/http://nicosia.is.s.u-tokyo.ac.jp/pub/staff/akama/repr.ps Constructing Allegory from Relation Algebra and Representation Theorems.]\"\n*Richard Bird, Oege de Moor, Paul Hoogendijk, \"[http://citeseer.ist.psu.edu/bird99generic.html Generic Programming with Relations and Functors.]\" \n* R.P. de Freitas and Viana, \"[https://web.archive.org/web/20070927201527/http://www.cos.ufrj.br/~naborges/fv02.ps A Completeness Result for Relation Algebra with Binders.]\"\n*[http://www1.chapman.edu/~jipsen/ Peter Jipsen]:\n**[https://web.archive.org/web/20110614180042/http://math.chapman.edu/structuresold/files/Relation_algebras.pdf Relation algebras]<!--[http://math.chapman.edu/structuresold/files/Relation_algebras.pdf Relation algebras]. In [http://math.chapman.edu/cgi-bin/structures Mathematical structures.] If there are problems with LaTeX, see an old HTML version [http://math.chapman.edu/cgi-bin/structures.pl?Relation_algebras here.] broken links-->\n** \"[http://math.chapman.edu/~jipsen/talks/RelMiCS2006/JipsenRAKAtutorial.pdf Foundations of Relations and Kleene Algebra.]\"\n** \"[http://www1.chapman.edu/~jipsen/dissertation/ Computer Aided Investigations of Relation Algebras.]\"\n** \"[http://citeseer.ist.psu.edu/337149.html A Gentzen System And Decidability For Residuated Lattices.\"]\n*[[Vaughan Pratt]]:\n** \"[http://boole.stanford.edu/pub/ocbr.pdf Origins of the Calculus of Binary Relations.]\" A historical treatment.\n** \"[http://boole.stanford.edu/pub/scbr.pdf The Second Calculus of Binary Relations.]\"\n* Priss, Uta: \n** \"[http://www.upriss.org.uk/papers/fcaic06.pdf An FCA interpretation of Relation Algebra.]\"\n** \"[http://www.upriss.org.uk/fca/relalg.html Relation Algebra and FCA]\" Links to publications and software\n*[http://www.cas.mcmaster.ca/~kahl/ Kahl, Wolfram] and [[Gunther Schmidt]]: [http://relmics.mcmaster.ca/~kahl/Publications/TR/2000-02/ Exploring (Finite) Relation Algebras Using Tools Written in Haskell.] and [http://relmics.mcmaster.ca/tools/RATH/index.html Relation Algebra Tools with Haskell] from [[McMaster University]].\n\n[[Category:Boolean algebra]]\n[[Category:Algebraic logic]]\n[[Category:Mathematical axioms]]\n[[Category:Mathematical logic]]\n[[Category:Mathematical relations]]"
    },
    {
      "title": "Residuated Boolean algebra",
      "url": "https://en.wikipedia.org/wiki/Residuated_Boolean_algebra",
      "text": "In [[mathematics]], a '''residuated Boolean algebra''' is a [[residuated lattice]] whose lattice structure is that of a [[Boolean algebra (structure)|Boolean algebra]]. Examples include Boolean algebras with the monoid taken to be conjunction, the set of all formal languages over a given alphabet Σ under concatenation, the set of all binary relations on a given set ''X'' under relational composition, and more generally the power set of any equivalence relation, again under relational composition. The original application was to [[relation algebra]]s as a finitely axiomatized generalization of the binary relation example, but there exist interesting examples of residuated Boolean algebras that are not relation algebras, such as the language example.\n\n==Definition==\nA '''residuated Boolean algebra''' is an algebraic structure (''L'', ∧, ∨, ¬, 0, 1, •, '''I''', \\, /) such that\n: (i) (''L'', &and;, &or;, •, '''I''', \\, /) is a [[residuated lattice]], and\n:(ii) (''L'', &and;, &or;, &not;, 0, 1) is a Boolean algebra.\n\nAn equivalent signature better suited to the [[relation algebra]] application is (''L'', ∧, ∨, ¬, 0, 1, •, '''I''', ▷, ◁) where the unary operations ''x''\\ and ''x''▷ are intertranslatable in the manner of [[De Morgan's laws]] via\n:''x''\\''y'' = &not;(''x''&#9655;&not;''y''), &nbsp; ''x''&#9655;''y'' = &not;(''x''\\&not;''y''), \n\nand dually /''y'' and &#9665;''y'' as\n\n: ''x''/''y'' = &not;(&not;''x''&#9665;''y''), &nbsp; ''x''&#9665;''y'' = &not;(&not;''x''/''y''),\n\nwith the residuation axioms in the [[residuated lattice]] article reorganized accordingly (replacing ''z'' by ¬''z'') to read\n:(''x''&#9655;''z'')&and;''y'' = 0 &nbsp; &hArr; &nbsp; (''x''•''y'')&and;''z'' = 0 &nbsp; &hArr; &nbsp; (''z''&#9665;''y'')&and;''x'' = 0\n\nThis [[De Morgan's laws|De Morgan dual]] reformulation is motivated and discussed in more detail in the section below on conjugacy.\n\nSince residuated lattices and Boolean algebras are each definable with finitely many equations, so are residuated Boolean algebras, whence they form a finitely axiomatizable [[Variety (universal algebra)|variety]].\n\n==Examples==\n# Any Boolean algebra, with the monoid multiplication • taken to be conjunction and both residuals taken to be [[material conditional|material implication]] ''x''→''y''. Of the remaining 15 binary Boolean operations that might be considered in place of conjunction for the monoid multiplication, only five meet the monotonicity requirement, namely 0, 1, ''x'', ''y'', and ''x''∨''y''. Setting ''y'' = ''z'' = 0 in the residuation axiom ''y'' ≤ ''x''\\''z'' &nbsp; ⇔ &nbsp; ''x''•''y'' ≤ ''z'', we have 0 ≤ ''x''\\0 &nbsp; ⇔ &nbsp; ''x''•0 ≤ 0, which is falsified by taking ''x'' = 1 when ''x''•''y'' = 1, ''x'', or ''x''∨''y''. The dual argument for ''z''/''y'' rules out ''x''•''y'' = ''y''. This just leaves ''x''•''y'' = 0 (a constant binary operation independent of ''x'' and ''y''), which satisfies almost all the axioms when the residuals are both taken to be the constant operation ''x''/''y'' = ''x''\\''y'' = 1. The axiom it fails is ''x''•'''I''' = ''x'' = '''I'''•''x'', for want of a suitable value for '''I'''. Hence conjunction is the only binary Boolean operation making the monoid multiplication that of a residuated Boolean algebra.\n# The power set 2<sup>''X''²</sup> made a Boolean algebra as usual with ∩, ∪ and complement relative to ''X''², and made a monoid with relational composition. The monoid unit '''I''' is the identity relation {(''x'',''x'')|''x'' ∈ ''X''}. The right residual ''R''\\''S'' is defined by ''x''(''R''\\''S'')''y'' if and only if for all ''z'' in ''X'', ''zRx'' implies ''zSy''. Dually the left residual ''S''/''R'' is defined by ''y''(''S''/''R'')''x'' if and only if for all ''z'' in ''X'', ''xRz'' implies ''ySz''.\n# The power set 2<sup>Σ*</sup> made a Boolean algebra as for Example 2, but with language concatenation for the monoid. Here the set Σ is used as an alphabet while Σ* denotes the set of all finite (including empty) words over that alphabet. The concatenation ''LM'' of languages ''L'' and ''M'' consists of all words ''uv'' such that ''u'' ∈ ''L'' and ''v'' ∈ ''M''. The monoid unit is the language {ε} consisting of just the empty word ε. The right residual ''M''\\''L'' consists of all words ''w'' over Σ such that ''Mw'' ⊆ ''L''. The left residual ''L''/''M'' is the same with ''wM'' in place of ''Mw''.\n\n==Conjugacy==\nThe De Morgan duals ▷ and ◁ of residuation arise as follows. Among residuated lattices, Boolean algebras are special by virtue of having a complementation operation ¬. This permits an alternative expression of the three inequalities\n:''y'' &le; ''x''\\''z'' &nbsp; &hArr; &nbsp; ''x''•''y'' &le; ''z'' &nbsp; &hArr; &nbsp; ''x'' &le; ''z''/''y''\n\nin the axiomatization of the two residuals in terms of disjointness, via the equivalence ''x'' ≤ ''y'' ⇔ ''x''∧¬''y'' = 0. Abbreviating ''x''∧''y'' = 0 to ''x'' # ''y'' as the expression of their disjointness, and substituting ¬''z'' for ''z'' in the axioms, they become with a little Boolean manipulation\n:&not;(''x''\\&not;''z'') # ''y'' &nbsp; &hArr; &nbsp; ''x''•''y'' # ''z'' &nbsp; &hArr; &nbsp; &not;(&not;''z''/''y'') # ''x''\n\nNow ¬(''x''\\¬''z'') is reminiscent of [[De Morgan's laws|De Morgan duality]], suggesting that ''x''\\ be thought of as a unary operation ''f'', defined by ''f''(y) = ''x''\\''y'', that has a De Morgan dual ¬''f''(¬''y''), analogous to ∀''x''φ(''x'') = ¬∃''x''¬φ(''x''). Denoting this dual operation as ''x''▷, we define ''x''▷''z'' as ¬(''x''\\¬''z''). Similarly we define another operation ''z''◁''y'' as ¬(¬''z''/''y''). By analogy with ''x''\\ as the residual operation associated with the operation ''x''•, we refer to ''x''▷ as the conjugate operation, or simply '''conjugate''', of ''x''•. Likewise ◁''y'' is the '''conjugate''' of •''y''. Unlike residuals, conjugacy is an equivalence relation between operations: if ''f'' is the conjugate of ''g'' then ''g'' is also the conjugate of ''f'', i.e. the conjugate of the conjugate of ''f'' is ''f''. Another advantage of conjugacy is that it becomes unnecessary to speak of right and left conjugates, that distinction now being inherited from the difference between ''x''• and •''x'', which have as their respective conjugates ''x''▷ and ◁''x''.  (But this advantage accrues also to residuals when ''x''\\ is taken to be the residual operation to ''x''•.)\n\nAll this yields (along with the Boolean algebra and monoid axioms) the following equivalent axiomatization of a residuated Boolean algebra.\n:''y'' # ''x''&#9655;''z'' &nbsp; &hArr; &nbsp; ''x''•''y'' # ''z'' &nbsp; &hArr; &nbsp; ''x'' # ''z''&#9665;''y''\n\nWith this signature it remains the case that this axiomatization can be expressed as finitely many equations.\n\n==Converse==\nIn Examples 2 and 3 it can be shown that ''x''▷'''I''' = '''I'''◁''x''. In Example 2 both sides equal the converse ''x''˘ of ''x'', while in Example 3, both sides are '''I''' when ''x'' contains the empty word and 0 otherwise. In the former case ''x''˘ = ''x''. This is impossible for the latter because ''x''▷'''I''' retains hardly any information about ''x''. Hence in Example 2 we can substitute ''x''˘ for ''x'' in ''x''▷'''I''' = ''x''˘ = '''I'''◁''x'' and cancel (soundly) to give\n:''x''˘&#9655;'''I''' = ''x'' =  '''I'''&#9665;''x''˘.\n\n''x''˘˘ = ''x'' can be proved from these two equations. [[Alfred Tarski|Tarski]]'s notion of a '''[[relation algebra]]''' can be defined as a residuated Boolean algebra having an operation ''x''˘ satisfying these two equations.\n\nThe cancellation step in the above is not possible for Example 3, which therefore is not a relation algebra, ''x''˘ being uniquely determined as ''x''▷'''I'''.\n\nConsequences of this axiomatization of converse include ''x''˘˘ = ''x'', ¬(''x''˘) = (¬''x'')˘, (''x'' ∨ ''y'')˘ = ''x''˘ ∨ ''y''˘, and (''x''•''y'')˘ = ''y''˘•''x''˘.\n\n== References ==\n* Bjarni Jónsson and Constantine Tsinakis, Relation algebras as residuated Boolean algebras, Algebra Universalis, 30 (1993) 469-478.\n* Peter Jipsen, ''[http://www1.chapman.edu/~jipsen/dissertation/ Computer aided investigations of relation algebras]'', Ph.D. Thesis, Vanderbilt University, May 1992.\n\n[[Category:Boolean algebra]]\n[[Category:Mathematical logic]]\n[[Category:Fuzzy logic]]\n[[Category:Algebraic logic]]"
    },
    {
      "title": "Robbins algebra",
      "url": "https://en.wikipedia.org/wiki/Robbins_algebra",
      "text": "{{no footnotes|date=June 2015}}\nIn [[abstract algebra]], a '''Robbins algebra''' is an [[Universal algebra#Basic idea|algebra]] containing a single [[binary operation]], usually denoted by <math>\\lor</math>, and a single [[unary operation]] usually denoted by <math>\\neg</math>. These operations satisfy the following [[Universal algebra#Equations|axioms]]:\n\nFor all elements ''a'', ''b'', and ''c'':\n# [[Associativity]]: <math>a \\lor \\left(b \\lor c \\right) = \\left(a \\lor b \\right) \\lor c</math>\n# [[Commutativity]]: <math>a \\lor b = b \\lor a</math>\n# ''Robbins equation'': <math>\\neg \\left( \\neg \\left(a \\lor b \\right) \\lor \\neg \\left(a \\lor \\neg b \\right) \\right) = a</math>\n\nFor many years, it was conjectured, but unproven, that all Robbins algebras are [[Boolean algebra (structure)|Boolean algebra]]s.  This was proved in 1996, so the term \"Robbins algebra\" is now simply a synonym for \"Boolean algebra\".\n\n== History ==\nIn 1933, [[Edward Huntington]] proposed a new set of axioms for Boolean algebras, consisting of (1) and (2) above, plus: \n*''Huntington's equation'': <math>\\neg(\\neg a \\lor b) \\lor \\neg(\\neg a \\lor \\neg b) = a.</math>\nFrom these axioms, Huntington derived the usual axioms of Boolean algebra.\n\nVery soon thereafter, [[Herbert Robbins]] posed the \"Robbins conjecture\", namely that the Huntington equation could be replaced with what came to be called the Robbins equation, and the result would still be [[Boolean algebra (structure)|Boolean algebra]]. <math>\\lor</math> would interpret Boolean [[Boolean algebra (structure)#Definition|join]] and <math>\\neg</math> Boolean [[Boolean algebra (structure)#Definition|complement]]. Boolean [[Boolean algebra (structure)#Definition|meet]] and the constants 0 and 1 are easily defined from the Robbins algebra primitives. Pending verification of the conjecture, the system of Robbins was called \"Robbins algebra.\"\n\nVerifying the Robbins conjecture required proving Huntington's equation, or some other axiomatization of a Boolean algebra, as theorems of a Robbins algebra. Huntington, Robbins, [[Alfred Tarski]], and others worked on the problem, but failed to find a proof or counterexample.\n\n[[William McCune]] proved the conjecture in 1996, using the [[automated theorem proving|automated theorem prover]] [[EQP]]. For a complete proof of the Robbins conjecture in one consistent notation and following McCune closely, see Mann (2003). Dahn (1998) simplified McCune's machine proof.\n\n==See also==\n* [[Algebraic structure]]\n* [[Minimal axioms for Boolean algebra]]\n\n==References==\n* Dahn, B. I. (1998) Abstract to \"[https://www.sciencedirect.com/science/article/pii/S0021869398974671 Robbins Algebras Are Boolean: A Revision of McCune's Computer-Generated Solution of Robbins Problem,]\" ''Journal of Algebra'' 208(2): 526–32.\n* Mann, Allen (2003) \"[http://math.colgate.edu/~amann/MA/robbins_complete.pdf A Complete Proof of the Robbins Conjecture.]\"\n* [[William McCune]], \"[http://calculemus.org/MathUniversalis/4/6robbins.html Robbins Algebras Are Boolean,]\" With links to proofs and other papers.\n\n{{DEFAULTSORT:Robbins Algebra}}\n[[Category:Boolean algebra]]\n[[Category:Formal methods]]\n[[Category:Computer-assisted proofs]]"
    },
    {
      "title": "Sigma-algebra",
      "url": "https://en.wikipedia.org/wiki/Sigma-algebra",
      "text": "{{redirect|Σ-algebra|an algebraic structure admitting a given signature Σ of operations|Universal algebra}}\n\nIn [[mathematical analysis]] and in [[probability theory]], a '''σ-algebra''' (also '''σ-field''') on a set ''X'' is a collection Σ of [[subset]]s of ''X'' that\nincludes ''X'' itself, is [[Closure (mathematics)|closed]] under [[complement (set theory)|complement]], and is closed under [[ Countable set | countable]] [[union (set theory)|unions]].\n\nThe definition implies that it also includes \nthe [[Empty set|empty subset]] and that it is closed under countable [[intersection (set theory)|intersections]].\n \nThe pair (''X'', Σ) is called a [[measurable space]] or Borel space.\n\nA σ-algebra is a type of [[algebra of sets]]. An algebra of sets needs only to be closed under the union or intersection of ''finitely'' many subsets, which is a weaker condition.<ref>{{cite web|title=Probability, Mathematical Statistics, Stochastic Processes|url=http://www.math.uah.edu/stat/foundations/Measure.html|website=Random|publisher=University of Alabama in Huntsville, Department of Mathematical Sciences|accessdate=30 March 2016}}</ref>\n\nThe main use of σ-algebras is in the definition of [[measure (mathematics)|measures]]; specifically, the collection of those subsets for which a given measure is defined is necessarily a σ-algebra. This concept is important in [[mathematical analysis]] as the foundation for [[Lebesgue integration]], and in [[probability theory]], where it is interpreted as the collection of events which can be assigned probabilities. Also, in probability, σ-algebras are pivotal in the definition of [[conditional expectation]].\n\nIn [[statistics]], (sub) σ-algebras are needed for the formal mathematical definition of a [[sufficient statistic]],<ref name=sufficiency>{{cite book |last1=Billingsley |first1=Patrick |authorlink=Patrick Billingsley |title=Probability and Measure |date=2012 |publisher=Wiley |isbn=978-1-118-12237-2 |edition=Anniversary}}</ref> particularly when the statistic is a function or a random process and the notion of [[Conditional probability distribution|conditional density]] is not applicable.\n\nIf {{math|''X'' {{=}} {''a'', ''b'', ''c'', ''d''},}} one possible σ-algebra on ''X'' is {{math|Σ {{=}} { ∅, {''a'', ''b''}, {''c'', ''d''}, {''a'', ''b'', ''c'', ''d''} },}} where ∅ is the [[empty set]]. In general, a finite algebra is always a σ-algebra.\n\nIf {''A''<sub>1</sub>, ''A''<sub>2</sub>, ''A''<sub>3</sub>, …} is a countable [[partition of a set|partition]] of ''X'' then the collection of all unions of sets in the partition (including the empty set) is a σ-algebra.\n\nA more useful example is the set of subsets of the [[real line]] formed by starting with all [[open interval]]s and adding in all countable unions, countable intersections, and relative complements and continuing this process (by [[transfinite iteration]] through all [[countable ordinal]]s) until the relevant closure properties are achieved - the σ-algebra produced by this process is known as the [[Borel set|Borel algebra]] on the real line, and can also be conceived as the smallest (i.e. \"coarsest\") σ-algebra containing all the open sets, or equivalently containing all the closed sets. It is foundational to [[Measure (mathematics)|measure theory]], and therefore modern [[probability theory]], and a related construction known as the [[Borel hierarchy]] is of relevance to [[Descriptive set theory|descriptive set theory.]]\n\n==Motivation==\nThere are at least three key motivators for σ-algebras: defining measures, manipulating limits of sets, and managing partial information characterized by sets.\n\n===Measure===\nA [[Measure (mathematics)|measure]] on ''X'' is a [[function (mathematics)|function]] that assigns a non-negative [[real number]] to subsets of ''X''; this can be thought of as making precise a notion of \"size\" or \"volume\" for sets. We want the size of the union of disjoint sets to be the sum of their individual sizes, even for an infinite sequence of [[disjoint sets]].\n\nOne would like to assign a size to ''every'' subset of ''X'', but in many natural settings, this is not possible. For example, the [[axiom of choice]] implies that, when the size under consideration is the ordinary notion of length for subsets of the real line, then there exist sets for which no size exists, for example, the [[Vitali set]]s. For this reason, one considers instead a smaller collection of privileged subsets of ''X''. These subsets will be called the measurable sets. They are closed under operations that one would expect for measurable sets; that is, the complement of a measurable set is a measurable set and the countable union of measurable sets is a measurable set.  Non-empty collections of sets with these properties are called σ-algebras.\n\n===Limits of sets===\nMany uses of measure, such as the probability concept of [[convergence of random variables|almost sure convergence]], involve [[set-theoretic limit|limits of sequences of sets]]. For this, closure under countable unions and intersections is paramount. Set limits are defined as follows on σ-algebras.\n* The limit supremum of a sequence ''A''<sub>1</sub>, ''A''<sub>2</sub>, ''A''<sub>3</sub>, ..., each of which is a subset of ''X'', is\n::<math>\\limsup_{n\\to\\infty}A_n = \\bigcap_{n=1}^\\infty\\bigcup_{m=n}^\\infty A_m.</math>\n* The limit infimum of a sequence ''A''<sub>1</sub>, ''A''<sub>2</sub>, ''A''<sub>3</sub>, ..., each of which is a subset of ''X'', is\n::<math>\\liminf_{n\\to\\infty}A_n = \\bigcup_{n=1}^\\infty\\bigcap_{m=n}^\\infty A_m.</math>\n* If, in fact,\n::<math>\\liminf_{n\\to\\infty}A_n = \\limsup_{n\\to\\infty}A_n,</math>\n:then the <math>\\lim_{n\\to\\infty}A_n</math> exists as that common set.\n\n===Sub σ-algebras===\nIn much of probability, especially when [[conditional expectation]] is involved, one is concerned with sets that represent only part of all the possible information that can be observed. This partial information can be characterized with a smaller σ-algebra which is a subset of the principal σ-algebra; it consists of the collection of subsets relevant only to and determined only by the partial information. A simple example suffices to illustrate this idea.\n\nImagine you and another person are betting on a game that involves flipping a coin repeatedly and observing whether it comes up Heads (''H'') or Tails (''T''). Since you and your opponent are each infinitely wealthy, there is no limit to how long the game can last. This means the [[sample space]] Ω must consist of all possible infinite sequences of ''H'' or ''T'':\n\n:<math>\\Omega = \\{H, T\\}^\\infty = \\{(x_1, x_2, x_3, \\dots) : x_i \\in \\{H, T\\}, i \\ge 1\\}.</math>\n\nHowever, after ''n'' flips of the coin, you may want to determine or revise your betting strategy in advance of the next flip. The observed information at that point can be described in terms of the 2<sup>n</sup> possibilities for the first ''n'' flips. Formally, since you need to use subsets of Ω, this is codified as the σ-algebra\n\n:<math>\\mathcal{G}_n = \\{A \\times \\{H, T\\}^\\infty : A \\subset \\{H, T\\}^n\\}.</math>\n\nObserve that then\n\n:<math>\\mathcal{G}_1 \\subset \\mathcal{G}_2 \\subset \\mathcal{G}_3 \\subset \\cdots \\subset \\mathcal{G}_\\infty,</math>\n\nwhere <math>\\mathcal{G}_\\infty</math> is the smallest σ-algebra containing all the others.\n\n==Definition and properties==\n\n===Definition===\nLet ''X'' be some set, and let <math>\\mathcal{P}(X)</math> represent its [[power set]]. Then a subset <math>\\Sigma \\subseteq \\mathcal{P}(X)</math> is called a '''''σ''-algebra''' if it satisfies the following three properties:<ref>{{cite book | last=Rudin | first=Walter | authorlink=Walter Rudin | title=Real & Complex Analysis | publisher=[[McGraw-Hill]] | year=1987 | isbn=978-0-07-054234-1}}</ref>\n\n# ''X'' is in Σ, and ''X'' is considered to be the [[universal set]] in the following context.\n# Σ is ''closed under complementation'': If ''A'' is in Σ, then so is its [[complement (set theory)|complement]], {{math|''X'' \\ ''A''}}.\n# Σ is ''closed under countable unions'': If ''A''<sub>1</sub>, ''A''<sub>2</sub>, ''A''<sub>3</sub>, ... are in Σ, then so is ''A'' = ''A''<sub>1</sub> ∪ ''A''<sub>2</sub> ∪ ''A''<sub>3</sub> ∪ … .\n\nFrom these properties, it follows that the σ-algebra is also closed under countable [[intersection (set theory)|intersections]] (by applying [[De Morgan's laws]]).\n\nIt also follows that the [[empty set]] ∅ is in Σ, since by '''(1)''' ''X'' is in Σ and '''(2)''' asserts that its complement, the empty set, is also in Σ.  Moreover, since {{math|{''X'', ∅}}} satisfies condition '''(3)''' as well, it follows that {{math|{''X'', ∅}}} is the smallest possible σ-algebra on ''X''.  The largest possible σ-algebra on ''X'' is 2<sup>''X''</sup>:=<math>\\mathcal{P}(X)</math>.\n\nElements of the ''σ''-algebra are called [[measurable set]]s. An ordered pair {{math|(''X'', Σ)}}, where ''X'' is a set and Σ is a ''σ''-algebra over ''X'', is called a '''measurable space'''. A function between two measurable spaces is called a [[measurable function]] if the [[preimage]] of every measurable set is measurable. The collection of measurable spaces forms a [[category (mathematics)|category]], with the [[measurable function]]s as [[morphism]]s. [[Measure (mathematics)|Measures]] are defined as certain types of functions from a ''σ''-algebra to [0, ∞].\n\nA σ-algebra is both a [[pi-system|π-system]] and a [[Dynkin system]] (λ-system). The converse is true as well, by Dynkin's theorem (below).\n\n===Dynkin's π-λ theorem===\nThis theorem (or the related [[monotone class theorem]]) is an essential tool for proving many results about properties of specific σ-algebras. It capitalizes on the nature of two simpler classes of sets, namely the following.\n: A [[pi-system|π-system]] ''P'' is a collection of subsets of X that is closed under finitely many intersections, and\n: a [[Dynkin system]] (or λ-system) ''D'' is a collection of subsets of X that contains X and is closed under complement and under countable unions of ''disjoint'' subsets.\n\nDynkin's π-λ theorem says, if ''P'' is a π-system and ''D'' is a Dynkin system that contains ''P'' then the σ-algebra σ(''P'') [[sigma-algebra#σ-algebra generated by an arbitrary family|generated]] by ''P'' is contained in ''D''.  Since certain π-systems are relatively simple classes, it may not be hard to verify that all sets in ''P'' enjoy the property under consideration while, on the other hand, showing that the collection ''D'' of all subsets with the property is a Dynkin system can also be straightforward. Dynkin's π-λ Theorem then implies that all sets in σ(''P'') enjoy the property, avoiding the task of checking it for an arbitrary set in σ(''P'').\n\nOne of the most fundamental uses of the π-λ theorem is to show equivalence of separately defined measures or integrals. For example, it is used to equate a probability for a random variable ''X'' with the [[Lebesgue-Stieltjes integral]] typically associated with computing the probability:\n\n:<math>\\mathbb{P}(X\\in A)=\\int_A \\,F(dx)</math> for all ''A'' in the Borel σ-algebra on '''R''',\n\nwhere ''F''(''x'') is the [[cumulative distribution function]] for ''X'', defined on '''R''', while <math>\\mathbb{P}</math> is a [[probability measure]], defined on a σ-algebra Σ of subsets of some [[sample space]] Ω.\n\n===Combining σ-algebras===\nSuppose <math>\\textstyle\\{\\Sigma_\\alpha:\\alpha\\in\\mathcal{A}\\}</math> is a collection of σ-algebras on a space ''X''.\n\n* The intersection of a collection of σ-algebras is a σ-algebra. To emphasize its character as a σ-algebra, it often is denoted by:\n::<math>\\bigwedge_{\\alpha\\in\\mathcal{A}}\\Sigma_\\alpha.</math>\n:'''Sketch of Proof:''' Let {{math|Σ<sup>∗</sup>}} denote the intersection. Since ''X'' is in every {{math|Σ<sub>''α''</sub>, Σ<sup>∗</sup>}} is not empty. Closure under complement and countable unions for every {{math|Σ<sub>''α''</sub>}} implies the same must be true for {{math|Σ<sup>∗</sup>}}. Therefore, {{math|Σ<sup>∗</sup>}} is a σ-algebra.\n\n* The union of a collection of σ-algebras is not generally a σ-algebra, or even an algebra, but it [[sigma-algebra#σ-algebra generated by an arbitrary family|generates]] a σ-algebra known as the [[join (sigma algebra)|join]] which typically is denoted\n::<math>\\bigvee_{\\alpha\\in\\mathcal{A}}\\Sigma_\\alpha=\\sigma\\left(\\bigcup_{\\alpha\\in\\mathcal{A}}\\Sigma_\\alpha\\right).</math>\n:A π-system that generates the join is\n::<math>\\mathcal{P}=\\left \\{\\bigcap_{i=1}^nA_i:A_i\\in\\Sigma_{\\alpha_i},\\alpha_i\\in\\mathcal{A},\\ n\\ge1 \\right\\}.</math>\n:'''Sketch of Proof:''' By the case ''n'' = 1, it is seen that each <math>\\Sigma_\\alpha\\subset\\mathcal{P}</math>, so\n::<math>\\bigcup_{\\alpha\\in\\mathcal{A}}\\Sigma_\\alpha\\subset\\mathcal{P}.</math>\n:This implies\n::<math>\\sigma\\left(\\bigcup_{\\alpha\\in\\mathcal{A}}\\Sigma_\\alpha\\right)\\subset\\sigma(\\mathcal{P})</math>\n:by the definition of a σ-algebra [[sigma-algebra#σ-algebra generated by an arbitrary family|generated]] by a collection of subsets. On the other hand,\n::<math>\\mathcal{P}\\subset\\sigma\\left(\\bigcup_{\\alpha\\in\\mathcal{A}}\\Sigma_\\alpha\\right)</math>\n:which, by Dynkin's π-λ theorem, implies\n::<math>\\sigma(\\mathcal{P})\\subset\\sigma\\left(\\bigcup_{\\alpha\\in\\mathcal{A}}\\Sigma_\\alpha\\right).</math>\n\n===σ-algebras for subspaces===\nSuppose ''Y'' is a subset of ''X'' and let (''X'', Σ) be a measurable space.\n* The collection {''Y'' ∩ ''B'': ''B'' ∈ Σ} is a σ-algebra of subsets of ''Y''.\n* Suppose (''Y'', Λ) is a measurable space. The collection {''A'' ⊂ ''X'' : ''A'' ∩ ''Y'' ∈ Λ} is a σ-algebra of subsets of ''X''.\n\n===Relation to σ-ring===\nA ''σ''-algebra Σ is just a [[Sigma-ring|''σ''-ring]] that contains the universal set ''X''.<ref name=Vestrup2009>{{cite book| last=Vestrup |first=Eric M. |title=The Theory of Measures and Integration |year=2009 |publisher=John Wiley & Sons |isbn=978-0-470-31795-2 |page=12}}</ref> A ''σ''-ring need not be a ''σ''-algebra, as for example measurable subsets of zero Lebesgue measure in the real line are a ''σ''-ring, but not a ''σ''-algebra since the real line has infinite measure and thus cannot be obtained by their countable union. If, instead of zero measure, one takes measurable subsets of finite Lebesgue measure, those are a [[Ring of sets|ring]] but not a ''σ''-ring, since the real line can be obtained by their countable union yet its measure is not finite.\n\n===Typographic note===\n''σ''-algebras are sometimes denoted using [[calligraphic]] capital letters, or the [[Fraktur (typeface)|Fraktur typeface]]. Thus {{math|(''X'', Σ)}} may be denoted as <math>\\scriptstyle(X,\\,\\mathcal{F})</math> or  <math>\\scriptstyle(X,\\,\\mathfrak{F})</math>.\n\n==Particular cases and examples==\n\n===Separable σ-algebras===\nA '''separable &sigma;-algebra''' (or '''separable &sigma;-field''') is a &sigma;-algebra <math>\\mathcal{F}</math> that is a [[separable space]] when considered as a [[metric space]] with [[metric (mathematics)|metric]] <math>\\rho(A,B) = \\mu(A \\mathbin{\\triangle} B)</math> for <math>A,B \\in \\mathcal{F}</math> and a given [[measure (mathematics)|measure]] <math>\\mu</math> (and with <math>\\triangle</math> being the [[symmetric difference]] operator).<ref>{{cite journal|last1=Džamonja|first1=Mirna|last2=Kunen|first2=Kenneth|authorlink2=Kenneth Kunen|title=Properties of the class of measure separable compact spaces| journal=[[Fundamenta Mathematicae]]|year=1995|pages=262|url=https://archive.uea.ac.uk/~h020/fundamenta.pdf|quote=If <math>\\mu</math> is a Borel measure on <math>X</math>, the measure algebra of <math>(X,\\mu)</math> is the Boolean algebra of all Borel sets modulo <math>\\mu</math>-null sets. If <math>\\mu</math> is finite, then such a measure algebra is also a metric space, with the distance between the two sets being the measure of their symmetric difference. Then, we say that <math>\\mu</math> is ''separable'' [[if and only if|iff]] this metric space is separable as a topological space.}}</ref>  Note that any &sigma;-algebra generated by a [[countable]] collection of [[Set (mathematics)|sets]] is separable, but the converse need not hold. For example, the Lebesgue &sigma;-algebra is separable (since every Lebesgue measurable set is equivalent to some Borel set) but not countably generated (since its cardinality is higher than continuum).\n\nA separable measure space has a natural [[pseudometric space|pseudometric]] that renders it [[separable space|separable]] as a [[pseudometric space]].  The distance between two sets is defined as the measure of the [[symmetric difference]] of the two sets.  Note that the symmetric difference of two distinct sets can have measure zero; hence the pseudometric as defined above need not to be a true metric.  However, if sets whose symmetric difference has measure zero are identified into a single [[equivalence class]], the resulting [[equivalence class|quotient set]] can be properly metrized by the induced metric.  If the measure space is separable, it can be shown that the corresponding metric space is, too.\n\n===Simple set-based examples===\nLet ''X'' be any set.\n* The family consisting only of the empty set and the set ''X'', called the minimal or '''trivial σ-algebra''' over ''X''.\n* The [[power set]] of ''X'', called the '''discrete σ-algebra'''.\n* The collection {∅, ''A'', ''A''<sup>c</sup>, ''X''} is a simple σ-algebra generated by the subset ''A''.\n* The collection of subsets of ''X'' which are countable or whose complements are countable is a σ-algebra (which is distinct from the power set of ''X'' if and only if ''X'' is uncountable). This is the σ-algebra generated by the [[Singleton (mathematics)|singletons]] of ''X''. Note: \"countable\" includes finite or empty.\n* The collection of all unions of sets in a countable [[partition of a set|partition]] of ''X'' is a σ-algebra.\n\n===Stopping time sigma-algebras===\n{{main article|Σ-Algebra of τ-past}}\nA [[stopping time]] <math>\\tau</math> can define a <math>\\sigma</math>-algebra <math>\\mathcal{F}_{\\tau}</math>, the\nso-called <math>\\sigma </math>-Algebra of τ-past, which in a [[Filtration (probability theory)|filtered probability space]] describes the information up to the random time <math>\\tau</math> in the sense that, if the filtered probability space is interpreted as a random experiment, the maximum information that can be found out about the experiment from arbitrarily often repeating it until the time <math>\\tau</math> is <math>\\mathcal{F}_{\\tau}</math>.<ref name=\"Fischer (2013)\">{{cite journal|last=Fischer|first=Tom|title=On simple representations of stopping times and stopping time sigma-algebras|journal=Statistics and Probability Letters|year=2013|volume=83|issue=1|pages=345–349|doi=10.1016/j.spl.2012.09.024|arxiv=1112.1603}}</ref>\n\n==σ-algebras generated by families of sets==\n\n===σ-algebra generated by an arbitrary family===\n{{main article|Generated σ-algebra (by sets)}}\n{{anchor}}\nLet ''F'' be an arbitrary family of subsets of ''X''. Then there exists a unique smallest σ-algebra which contains every set in ''F'' (even though ''F'' may or may not itself be a σ-algebra). It is, in fact, the intersection of all σ-algebras containing ''F''. (See intersections of σ-algebras above.) This σ-algebra is denoted σ(''F'') and is called '''the σ-algebra generated by ''F'''''.\n\nIf ''F'' is empty, then σ(''F'')={{math|{''X'', ∅}}}.  Otherwise σ(''F'') consists of all the subsets of ''X'' that can be made from elements of ''F'' by a countable number of complement, union and intersection operations.\n\nFor a simple example, consider the set ''X'' = {1, 2, 3}. Then the σ-algebra generated by the single subset {1} is {{math|σ(<nowiki>{{1}}</nowiki>) {{=}} {∅, {1}, {2, 3}, {1, 2, 3<nowiki>}}</nowiki>}}. By an [[abuse of notation]], when a collection of subsets contains only one element, ''A'', one may write σ(''A'') instead of σ({''A''}); in the prior example σ({1}) instead of σ(<nowiki>{{1}}</nowiki>). Indeed, using {{math|σ(''A''<sub>1</sub>, ''A''<sub>2</sub>, ...)}} to mean {{math|σ({''A''<sub>1</sub>, ''A''<sub>2</sub>, ...})}} is also quite common.\n\nThere are many families of subsets that generate useful σ-algebras. Some of these are presented here.\n\n===σ-algebra generated by a function===\nIf <math>f</math> is a function from a set <math>X</math> to a set <math>Y</math> and <math>B</math> is a <math>\\sigma</math>-algebra of subsets of <math>Y</math>, then the <math>\\sigma</math>'''-algebra generated by the function''' <math>f</math>, denoted by <math>\\sigma (f)</math>, is the collection of all inverse images <math>f^{-1} (S)</math> of the sets <math>S</math> in <math>B</math>. i.e.\n\n:<math> \\sigma (f) = \\{ f^{-1}(S) \\, | \\, S\\in B \\}. </math>\n\nA function ''f'' from a set ''X'' to a set ''Y'' is [[Measurable function|measurable]] with respect to a σ-algebra Σ of subsets of ''X'' if and only if σ(''f'') is a subset of Σ.\n\nOne common situation, and understood by default if ''B'' is not specified explicitly, is when ''Y'' is a [[metric space|metric]] or [[topological space]] and ''B'' is the collection of [[Borel set]]s on ''Y''.\n\nIf ''f'' is a function from ''X'' to '''R'''<sup>''n''</sup> then σ(''f'') is generated by the family of subsets which are inverse images of intervals/rectangles in '''R'''<sup>''n''</sup>:\n\n:<math>\\sigma(f)=\\sigma\\left(\\{f^{-1}((a_1,b_1]\\times\\cdots\\times(a_n,b_n]):a_i,b_i\\in\\mathbb{R}\\}\\right).</math>\n\nA useful property is the following. Assume ''f'' is a measurable map from (''X'', Σ<sub>''X''</sub>) to (''S'', Σ<sub>''S''</sub>) and ''g'' is a measurable map from (''X'', Σ<sub>''X''</sub>) to (''T'', Σ<sub>''T''</sub>). If there exists a measurable map ''h'' from (''T'', Σ<sub>''T''</sub>) to (''S'', Σ<sub>''S''</sub>) such that ''f''(''x'') = ''h''(''g''(''x'')) for all ''x'', then σ(''f'') ⊂ σ(''g''). If ''S'' is finite or countably infinite or, more generally, (''S'', Σ<sub>''S''</sub>) is a [[standard Borel space]] (e.g., a separable complete metric space with its associated Borel sets), then the converse is also true.<ref>{{cite book | last=Kallenberg | first=Olav | authorlink=Olav Kallenberg | title=Foundations of Modern Probability |edition=2nd | publisher=[[Springer Nature|Springer]] | year=2001 | isbn=978-0-387-95313-7|page=7}}</ref> Examples of standard Borel spaces include '''R'''<sup>''n''</sup> with its Borel sets and '''R'''<sup>∞</sup> with the cylinder σ-algebra described below.\n\n===Borel and Lebesgue σ-algebras===\nAn important example is the [[Borel algebra]] over any [[topological space]]: the σ-algebra generated by the [[open set]]s (or, equivalently, by the [[closed set]]s). Note that this σ-algebra is not, in general, the whole power set. For a non-trivial example that is not a Borel set, see the [[Vitali set]] or [[Borel set#Non-Borel sets|Non-Borel sets]].\n\nOn the [[Euclidean space]] '''R'''<sup>''n''</sup>, another σ-algebra is of importance: that of all [[Lebesgue measure|Lebesgue measurable]] sets. This σ-algebra contains more sets than the Borel σ-algebra on '''R'''<sup>''n''</sup> and is preferred in [[Integral|integration]] theory, as it gives a [[complete measure|complete measure space]].\n\n===Product σ-algebra===\nLet <math>(X_1,\\Sigma_1)</math> and <math>(X_2,\\Sigma_2)</math> be two measurable spaces. The σ-algebra for the corresponding [[product space]] <math>X_1\\times X_2</math> is called the '''product σ-algebra''' and is defined by\n\n:<math>\\Sigma_1\\times\\Sigma_2=\\sigma(\\{B_1\\times B_2:B_1\\in\\Sigma_1,B_2\\in\\Sigma_2\\}).</math>\n\nObserve that <math>\\{B_1\\times B_2:B_1\\in\\Sigma_1,B_2\\in\\Sigma_2\\}</math> is a π-system.\n\nThe Borel σ-algebra for '''R'''<sup>''n''</sup> is generated by half-infinite rectangles and by finite rectangles. For example,\n\n:<math>\\mathcal{B}(\\mathbb{R}^n)=\\sigma \\left(\\left \\{(-\\infty,b_1]\\times\\cdots\\times(-\\infty,b_n]:b_i\\in\\mathbb{R} \\right \\}\\right) = \\sigma\\left(\\left \\{(a_1,b_1]\\times\\cdots\\times(a_n,b_n]:a_i,b_i\\in\\mathbb{R} \\right \\}\\right).</math>\n\nFor each of these two examples, the generating family is a π-system.\n\n===σ-algebra generated by cylinder sets===\nSuppose\n\n:<math>X\\subset\\mathbb{R}^{\\mathbb{T}}=\\{f:f \\text{ is a function from } \\mathbb{T} \\text{ to } \\mathbb{R}\\}</math>\n\nis a set of real-valued functions on <math>\\mathbb{T}</math>. Let <math>\\mathcal{B}(\\mathbb{R})</math> denote the Borel subsets of '''R'''. A [[cylinder set|cylinder subset]] of {{mvar|X}} is a finitely restricted set defined as\n\n:<math>C_{t_1,\\dots,t_n}(B_1,\\dots,B_n)=\\{f\\in X:f(t_i)\\in B_i, 1\\le i \\le n\\}.</math>\n\nEach\n\n:<math>\\{C_{t_1,\\dots,t_n}(B_1,\\dots,B_n):B_i\\in\\mathcal{B}(\\mathbb{R}), 1\\le i \\le n\\}</math>\n\nis a π-system that generates a σ-algebra <math>\\textstyle\\Sigma_{t_1,\\dots,t_n}</math>. Then the family of subsets\n\n:<math>\\mathcal{F}_X=\\bigcup_{n=1}^\\infty\\bigcup_{t_i\\in\\mathbb{T},i\\le n}\\Sigma_{t_1,\\dots,t_n}</math>\n\nis an algebra that generates the '''cylinder σ-algebra''' for {{mvar|X}}. This σ-algebra is a subalgebra of the Borel σ-algebra determined by the [[product topology]] of <math>\\mathbb{R}^{\\mathbb{T}}</math> restricted to {{mvar|X}}.\n\nAn important special case is when <math>\\mathbb{T}</math> is the set of natural numbers and {{mvar|X}} is a set of real-valued sequences. In this case, it suffices to consider the cylinder sets\n\n:<math>C_n(B_1,\\dots,B_n)=(B_1\\times\\cdots\\times B_n\\times\\mathbb{R}^\\infty)\\cap X=\\{(x_1,x_2,\\dots,x_n,x_{n+1},\\dots)\\in X:x_i\\in B_i,1\\le i\\le n\\},</math>\n\nfor which\n\n:<math>\\Sigma_n=\\sigma(\\{C_n(B_1,\\dots,B_n):B_i\\in\\mathcal{B}(\\mathbb{R}), 1\\le i \\le n\\})</math>\n\nis a non-decreasing sequence of σ-algebras.\n\n===σ-algebra generated by random variable or vector===\nSuppose <math>(\\Omega,\\Sigma,\\mathbb{P})</math> is a [[probability space]]. If <math>\\textstyle Y:\\Omega\\to\\mathbb{R}^n</math> is measurable with respect to the Borel σ-algebra on '''R'''<sup>''n''</sup> then {{mvar|Y}} is called a '''[[random variable]]''' (''n = 1'') or '''random vector''' (''n'' > 1). The σ-algebra generated by {{mvar|Y}} is\n\n:<math> \\sigma (Y) = \\{ Y^{-1}(A): A\\in\\mathcal{B}(\\mathbb{R}^n) \\}.</math>\n\n===σ-algebra generated by a stochastic process===\nSuppose <math>(\\Omega,\\Sigma,\\mathbb{P})</math> is a [[probability space]] and <math>\\mathbb{R}^\\mathbb{T}</math> is the set of real-valued functions on <math>\\mathbb{T}</math>. If <math>\\textstyle Y:\\Omega\\to X\\subset\\mathbb{R}^\\mathbb{T}</math> is measurable with respect to the cylinder σ-algebra <math>\\sigma(\\mathcal{F}_X)</math> (see above) for {{mvar|X}}, then {{mvar|Y}} is called a '''[[stochastic process]]''' or '''random process'''. The σ-algebra generated by {{mvar|Y}} is\n\n:<math>\\sigma(Y) = \\left \\{ Y^{-1}(A): A\\in\\sigma(\\mathcal{F}_X) \\right \\}= \\sigma(\\{ Y^{-1}(A): A\\in\\mathcal{F}_X\\}),</math>\n\nthe σ-algebra generated by the inverse images of cylinder sets.\n\n==See also==\n*[[Join (sigma algebra)]]\n*[[Measurable function]]\n*[[Sample space]]\n*[[Sigma ring]]\n*[[Sigma additivity]]\n\n==References==\n{{reflist}}\n\n==External links==\n*{{springer|title=Algebra of sets|id=p/a011400}}\n*[[PlanetMath:950|Sigma Algebra]] from PlanetMath.\n<!-- http://planetmath.org/encyclopedia/SigmaAlgebra.html -->\n\n[[Category:Measure theory]]\n[[Category:Experiment (probability theory)]]\n[[Category:Set families]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Stone functor",
      "url": "https://en.wikipedia.org/wiki/Stone_functor",
      "text": "[[File:Stone_functor.svg|thumb|250px]]\nIn [[mathematics]], the '''Stone functor''' is a [[functor]] ''S'': '''Top'''<sup>op</sup> → '''Bool''', where '''Top''' is the [[category of topological spaces]] and '''Bool''' is the [[Category (mathematics)|category]] of [[Boolean algebra (structure)|Boolean algebra]]s and [[Boolean homomorphism]]s. It assigns to each [[topological space]] ''X'' the Boolean algebra ''S''(''X'') of its [[clopen set|clopen]] subsets, and to each morphism ''f''<sup>op</sup>: ''X'' → ''Y'' in '''Top'''<sup>op</sup> (i.e., a [[continuous map]] ''f'': ''Y'' → ''X'') the homomorphism ''S''(''f''): ''S''(''X'') → ''S''(''Y'') given by ''S''(''f'')(''Z'') = ''f''<sup>−1</sup>[''Z''].\n\n==See also==\n*[[Stone's representation theorem for Boolean algebras]]\n*[[Pointless topology]]\n\n== References ==\n*[http://katmat.math.uni-bremen.de/acc/acc.pdf ''Abstract and Concrete Categories. The Joy of Cats'']. Jiri Adámek, Horst Herrlich, George E. Strecker.\n* Peter T. Johnstone, ''Stone Spaces''. (1982) Cambridge university Press {{isbn|0-521-23893-5}}\n\n[[Category:Functors]]\n[[Category:Boolean algebra]]\n[[Category:General topology]]\n\n{{cattheory-stub}}\n{{topology-stub}}"
    },
    {
      "title": "Stone space",
      "url": "https://en.wikipedia.org/wiki/Stone_space",
      "text": "In [[topology]] and related areas of [[mathematics]], a '''Stone space''', also known as a '''profinite space''','''<ref name=\":0\">{{nlab|id=Stone+space|title=Stone space}}</ref>''' is a [[compact space|compact]] [[totally disconnected]] [[Hausdorff space]].<ref name=\":1\">{{SpringerEOM\n | title=Stone space\n | id= Stone_space\n | last=\n | first=\n | author-link=\n | last2=\n | first2=\n | author2-link=\n | ref =\n}}</ref> Stone spaces are named after [[Marshall Harvey Stone]] who introduced and studied them in the 1930s in the course of his investigation of representing [[Boolean algebra (structure)|Boolean algebras]].\n\n== Equivalent conditions ==\nThe following conditions on the topological space ''X'' are equivalent:<ref name=\":1\" /><ref name=\":0\" />\n\n*''X'' is a Stone space;\n*''X'' is [[Homeomorphism|homeomorphic]] to the [[Inverse limit|projective limit]] (in the [[category of topological spaces]]) of an inverse system of finite [[Discrete space|discrete spaces]];\n*''X'' is compact and [[Connected space#Disconnected spaces|totally separated]];\n* X is compact, [[Kolmogorov space|T<sub>0</sub>]] , and [[Zero-dimensional space|zero-dimensional]] (in the sense of the [[small inductive dimension]]);\n*''X'' is [[Spectral space|coherent]] and Hausdorff.\n\n== Examples ==\nImportant examples of Stone spaces include the [[Cantor set]] and the space '''Z'''<sub>''p''</sub> of [[P-adic integers|''p''-adic integers]], where ''p'' is any [[prime number]].  Generalizing the first example, any [[Product topology|product]] of finite discrete spaces is a Stone space; generalizing the second example, the topological space underlying any [[profinite group]] is a Stone space. The [[Stone–Čech compactification]] of the natural numbers, or of any discrete space, is a Stone space.\n\n== Stone's representation theorem for Boolean algebras ==\n{{main|Stone's representation theorem for Boolean algebras}}\n\nTo every [[Boolean algebra (structure)|Boolean algebra]] ''B'' we can associate a Stone space ''S''(''B'') as follows: the elements of ''S''(''B'') are the [[ultrafilter]]s on ''B'', and the topology on ''S''(''B'') is generated by the sets of the form {''F''&isin;''S''(''B'')&nbsp;:&nbsp;''b''&isin;''F''}, where ''b'' is an element of ''B''.\n\n[[Stone's representation theorem for Boolean algebras]] states that every Boolean algebra is isomorphic to the Boolean algebra of [[clopen set]]s of the Stone space ''S''(''B''); and furthermore, every Stone space ''X'' is homeomorphic to the Stone space belonging to the Boolean algebra of clopen sets of ''X''. These assignments are [[Functor|functorial]], and we obtain a [[dual category|category-theoretic duality]] between the category of Boolean algebras (with homomorphisms as morphisms) and the category of Stone spaces (with continuous maps as morphisms).\n\n== Further reading ==\n\n* [[Peter Johnstone (mathematician)|Peter Johnstone]], ''Stone Spaces'', Cambridge University Press, 1982\n\n==References==\n{{reflist}}\n\n[[Category:General topology]]\n[[Category:Boolean algebra]]\n[[Category:Categorical logic]]"
    },
    {
      "title": "Suslin algebra",
      "url": "https://en.wikipedia.org/wiki/Suslin_algebra",
      "text": "In mathematics, a '''Suslin algebra''' is a [[Boolean algebra (structure)|Boolean algebra]] that is [[complete Boolean algebra|complete]], [[Atom (order theory)|atomless]], countably [[Distributive property|distributive]], and satisfies the [[countable chain condition]]. They are named after [[Mikhail Yakovlevich Suslin]].\n\nThe existence of Suslin algebras is independent of the axioms of [[ZFC]], and is equivalent to the existence of [[Suslin tree]]s or [[Suslin line]]s.\n\n==References==\n\n* {{cite book | authorlink=Thomas Jech|last=Jech | first=Thomas| title=Set theory | edition=third millennium (revised and expanded) | publisher=[[Springer-Verlag]] | year=2003 | isbn=3-540-44085-2 | oclc=174929965 | zbl=1007.03002}}\n\n[[Category:Boolean algebra]]\n[[Category:Forcing (mathematics)]]"
    },
    {
      "title": "Symmetric Boolean function",
      "url": "https://en.wikipedia.org/wiki/Symmetric_Boolean_function",
      "text": "In [[mathematics]], a '''symmetric Boolean function''' is a [[Boolean function]] whose value does not depend on the [[permutation]] of its input bits, i.e., it depends only on the number of ones in the input.<ref name=weg>[[Ingo Wegener]], \"The Complexity of Symmetric Boolean Functions\", in: ''Computation Theory and Logic'', ''[[Lecture Notes in Computer Science]]'', vol. 270, 1987, pp. 433–442</ref>\n\nFrom the definition follows, that there are 2<sup>''n''+1</sup> symmetric ''n''-ary Boolean functions. It implies that instead of the [[truth table]], traditionally used to represent Boolean functions, one may use a more compact representation for an ''n''-variable symmetric Boolean function: the (''n''&nbsp;+&nbsp;1)-vector, whose ''i''-th entry (''i''&nbsp;=&nbsp;0,&nbsp;...,&nbsp;''n'') is the value of the function on an input vector with ''i'' ones.\n\n==Special cases==\n\nA number of special cases are recognized.<ref name=weg/>\n\n*'''Threshold functions''': their value is 1 on input vectors with ''k'' or more ones for a fixed ''k''\n*'''Exact-value functions''': their value is 1 on input vectors with ''k'' ones for a fixed ''k''\n* '''Counting functions''' : their value is 1 on input vectors with the number of ones congruent to ''k''&nbsp;mod&nbsp;''m'' for fixed ''k'',&nbsp;''m''\n*'''[[Parity function]]s''': their value is 1 if the input vector has odd number of ones.\n\n==References==\n{{reflist}}\n\n==See also==\n* [[Majority function]]\n\n[[Category:Boolean algebra]]\n[[Category:Cryptography]]"
    },
    {
      "title": "Total operating characteristic",
      "url": "https://en.wikipedia.org/wiki/Total_operating_characteristic",
      "text": "The '''Total Operating Characteristic (TOC)'''  is a [[statistical method]] to compare a [[Boolean variable]] versus a [[Rank statistics|rank variable]]. TOC can measure the ability of an index variable to diagnose either presence or absence of a characteristic. The diagnosis of presence or absence depends on whether the value of the index is above a threshold. TOC considers multiple possible thresholds. Each threshold generates a two-by-two [[contingency table]], which contains four entries: hits, misses, false alarms, and correct rejections.<ref name=\"Si\">{{cite journal|last1=Pontius|first1=Robert Gilmore|last2=Si|first2=Kangping|title=The total operating characteristic to measure diagnostic ability for multiple thresholds| journal=International Journal of Geographical Information Science|volume=28|issue=3|year=2014|pages=570–583|doi=10.1080/13658816.2013.862623}}</ref>\n\nThe [[Receiver Operating Characteristic]] (ROC) also characterizes diagnostic ability, although ROC reveals less information than the TOC. For each threshold, ROC reveals two ratios, hits/(hits + misses) and false alarms/(false alarms + correct rejections), while TOC shows the total information in the contingency table for each threshold.<ref>{{cite journal|last1=Pontius|first1=Robert Gilmore|last2=Parmentier|first2=Benoit|title=Recommendations for using the Relative Operating Characteristic (ROC)|journal=Landscape Ecology|date=2014}}</ref> The TOC method reveals all of the information that the ROC method provides, plus additional important information that ROC does not reveal, i.e. the size of every entry in the contingency table for each threshold. TOC also provides the popular [[Receiver operating characteristic|area under the curve]] (AUC) of the ROC. \n\nTOC is applicable to measure diagnostic ability in many fields including but not limited to:  land change science, [[medical imaging]], [[weather forecasting]], [[remote sensing]], and [[materials testing]]. \n\n{{tocright}}\n\n==Basic concept==\n{{See also|Receiver operating characteristic}}\nThe procedure to construct the TOC curve compares the Boolean variable to the index variable by diagnosing each observation as either presence or absence, depending on how the index relates to various thresholds. If an observation’s index is greater than or equal to a threshold, then the observation is diagnosed as presence, otherwise the observation is diagnosed as absence. \nThe contingency table that results from the comparison between the Boolean variable and the diagnosis for a single threshold has four central entries. The four central entries are hits (''H''), misses (''M''), false alarms (''F''), and correct rejections (''C''). The total number of observations is&nbsp;''P''&nbsp;+&nbsp;''Q''. The terms “true positives”, “false negatives”, “false positives” and “true negatives” are equivalent to hits, misses, false alarms and correct rejections, respectively. The entries can be formulated in a two-by-two contingency table or confusion matrix, as follows:\n\n{| class=\"wikitable\" style=\"border:none; float:left; margin-top:0;\"\n!style=\"background:white; border:none;\" colspan=\"2\" rowspan=\"2\"|\n!colspan=\"2\" | Boolean\n!style=\"background:white; border:none;\" colspan=\"1\" rowspan=\"1\"|\n|-\n!Presence\n!Absence\n!style=\"background:white; border:none;\"|Diagnosis Total\n|-\n!rowspan=\"2\" style=\"height:6em;\"|<div style=\"{{rotate|-90}}\">Diagnosis<br> </div>\n!Presence\n|Hits (''H'')\n|False Alarms (''F'')\n|style=\"background:white; border:none;\"|''H'' + ''F''\n|-\n!Absence\n|Misses (''M'')\n|Correct Rejections (''C'')\n|style=\"background:white; border:none;\"|''M'' + ''C''\n|-\n!style=\"background:white; border:none;\" colspan=\"1\" rowspan=\"1\"|\n!style=\"background:white; border:none;\"|Boolean Total\n|style=\"background:white; border:none;\"|''H'' + ''M'' = ''P''\n|style=\"background:white; border:none;\"|''F'' + ''C'' = ''Q''\n|style=\"background:white; border:none;\"|''P'' + ''Q''\n|-\n|}\n\n{{-}}\n\nFour bits of information determine all the entries in the contingency table, including its marginal totals. For example, if we know ''H'', ''M'', ''F'', and ''C'', then we can compute all the marginal totals for any threshold. Alternatively, if we know ''H''/''P'', ''F''/''Q'', ''P'', and ''Q'', then we can compute all the entries in the table. <ref name= \"Si\"/> Two bits of information are not sufficient to complete the contingency table. For example, if we know only ''H''/''P'' and ''F''/''Q'', which is what ROC shows, then it is impossible to know all the entries in the table.<ref name=\"Si\" />\n==History==\n\n[http://www2.clarku.edu/faculty/facultybio.cfm?id=104 Robert Gilmore Pontius Jr], professor of Geography at [[Clark University]], and [https://www.linkedin.com/in/kangpingsi/ Kangping Si] in 2014 first developed the TOC for application in land change science.\n\n==TOC space==\n[[File:TOC labeled.png|thumb|left|TOC labeled]]\nThe TOC curve with four boxes indicates how a point on the TOC curve reveals the Hits, Misses, False Alarms, and Correct Rejections. The TOC curve is an effective way to show the total information in the contingency table for all thresholds. The data used to create this TOC curve is available for download [http://www2.clarku.edu/~rpontius/TOCexample2.xlsx here]. This dataset has 30 observations, each of which consists of values for a Boolean variable and an index variable. The observations are ranked from the greatest to the least value of the index. There are 31 thresholds, consisting of the 30 values of the index and one additional threshold that is greater than all the index values, which creates the point at the origin (0,0). Each point is labeled to indicate the value of each threshold. The horizontal axes ranges from 0 to 30 which is the number of observations in the dataset (''P''&nbsp;+&nbsp;''Q''). The vertical axis ranges from 0 to 10, which is the Boolean variable’s number of presence observations ''P'' (i.e. Hits&nbsp;+&nbsp;Misses). TOC curves also show the threshold at which the diagnosed amount of presence matches the Boolean amount of presence, which is the threshold point that lies directly under the point where the Maximum line meets the hits&nbsp;+&nbsp;misses line, as the TOC curve on the left illustrates. For a more detailed explanation of the construction of the TOC curve, please see Pontius Jr, Robert Gilmore; Si, Kangping (2014). \"The total operating characteristic to measure diagnostic ability for multiple thresholds.\" ''International Journal of Geographical Information Science'' '''28''' (3): 570–583.”<ref name=\"Si\" />\n\nThe following four pieces of information are the central entries in the contingency table for each threshold:\n\n#The number of hits at each threshold is the distance between the threshold’s point and the horizontal axis.\n#The number of misses at each threshold is the distance between the threshold’s point and the hits&nbsp;+&nbsp;misses horizontal line across the top of the graph.\n#The number of false alarms at each threshold is the distance between threshold’s point and the blue dashed Maximum line that bounds the left side of the TOC space.\n#The number of correct rejections at each threshold is the distance between the threshold’s point and the purple dashed Minimum line that bounds the right side of the TOC space.\n\n==TOC vs. ROC curves== \n[[File:TOC.png|thumb|left|TOC Curve]]\n[[File:ROC.png|thumb|left|ROC Curve]]\nThese figures are the TOC and ROC curves using the same data and thresholds.\nConsider the point that corresponds to a threshold of 74. The TOC curve shows the number of hits, which is 3, and hence the number of misses, which is 7. Additionally, the TOC curve shows that the number of false alarms is 4 and the number of correct rejections is 16. At any given point in the ROC curve, it is possible to glean values for the ratios of false alarms/(false alarms+correct rejections) and hits/(hits+misses). For example, at threshold 74, it is evident that the x coordinate is 0.3 and the y coordinate is 0.2. However, these two values are insufficient to construct all entries of the underlying two-by-two contingency table.\n\n{{-}}\n\n==Interpreting TOC curves==\n\nIt is common to report the area under the curve (AUC) to summarize a TOC or ROC curve. However, condensing diagnostic ability into a single number fails to appreciate the shape of the curve. The following three TOC curves are TOC curves that have an AUC of 0.75 but have different shapes.\n\n[[File:TOC1.png|thumb|left|TOC curve with higher accuracy at high thresholds.]]\nThis TOC curve on the left exemplifies an instance in which the index variable has a high diagnostic ability at high thresholds near the origin, but random diagnostic ability at low thresholds near the upper right of the curve. The curve shows  accurate diagnosis of presence until the curve reaches a threshold of 86. The curve then levels off and predicts around the random line.\n\n{{-}}\n\n[[File:TOC2.png|thumb|left|TOC curve with medium accuracy at all thresholds.]]\nThis TOC curve exemplifies an instance in which the index variable has a medium diagnostic ability at all thresholds. The curve is consistently above the random line.\n\n{{-}}\n\n[[File:TOC3.png|thumb|left|TOC curve with higher accuracy at lower thresholds.]]\nThis TOC curve exemplifies an instance in which the index variable has random diagnostic ability at high thresholds and high diagnostic ability at low thresholds. The curve follows the random line at the highest thresholds near the origin, then the index variable diagnoses absence correctly as thresholds decrease near the upper right corner.\n\n{{-}}\n\n==Area under the curve==\n\nWhen measuring diagnostic ability, a commonly reported measure is the Area Under the Curve (AUC). The AUC is calculable from the TOC and the ROC. The AUC indicates the probability that the diagnosis ranks a randomly chosen observation of Boolean presence higher than a randomly chosen observation of Boolean absence.<ref name=\"Halligan2015\" />\nThe AUC is appealing to many researchers because AUC summarizes diagnostic ability in a single number, however, the AUC has come under critique as a potentially misleading measure, especially for spatially explicit analyses.<ref name=\"Halligan2015\" /><ref name=\"Powers2012\" />\nSome features of the AUC that draw criticism include the fact that 1) AUC ignores the thresholds; 2) AUC summarizes the test performance over regions of the TOC or ROC space in which one would rarely operate; 3) AUC weighs omission and commission errors equally; 4) AUC does not give information about the spatial distribution of model errors; and, 5) the selection of spatial extent highly influences the rate of accurately diagnosed absences and the AUC scores.<ref name=\"Lobo2007\" />\nHowever, most of those criticisms apply to many other metrics.\n\n==References==\n{{reflist|refs=\n<ref name=\"Halligan2015\">{{cite journal |last1=Halligan |first1=Steve |last2=Altman |first2=Douglas G. |last3=Mallett |first3=Susan |title=Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: A discussion and proposal for an alternative approach |journal=European Radiology |date=2015 |volume=25 |issue=4 |pages=932–939 |doi=10.1007/s00330-014-3487-0}}</ref>\n<ref name=\"Lobo2007\">{{cite journal |last1=Lobo |first1=Jorge M. |last2=Jiménez-Valverde |first2=Alberto |last3=Real |first3=Raimundo |title=AUC: a misleading measure of the performance of predictive distribution models |journal=Global Ecology and Biogeography |date=2008 |volume=17 |issue=2 |pages=145–151 |doi=10.1111/j.1466-8238.2007.00358.x}}</ref>\n<ref name=\"Powers2012\">{{cite journal |last1=Powers |first1=David Martin Ward |title=The problem of Area Under the Curve |journal=2012 IEEE International Conference on Information Science and Technology |date=2012 |doi=10.1109/ICIST.2012.6221710}}</ref>\n}}\n==Further reading==\n*Pontius Jr, Robert Gilmore; Si, Kangping (2014). \"The total operating characteristic to measure diagnostic ability for multiple thresholds.\" ''International Journal of Geographical Information Science'' '''28''' (3): 570–583.\n*Pontius Jr, Robert Gilmore; Parmentier, Benoit (2014). Recommendations for using the Relative Operating Characteristic (ROC). ''Landscape Ecology'' '''29''' (3): 367–382.\n*Mas, Jean-François; Filho, Britaldo Soares; Pontius Jr, Robert Gilmore; Gutiérrez, Michelle Farfán; Rodrigues, Hermann (2013). A suite of tools for ROC analysis of spatial models. ''ISPRS International Journal of Geo-Information'' '''2''' (3): 869–887.\n*Pontius Jr, Robert Gilmore; Pacheco, Pablo (2004). Calibration and validation of a model of forest disturbance in the Western Ghats, India 1920–1990. ''GeoJournal'' '''61''' (4): 325–334.\n*Pontius Jr, Robert Gilmore; Batchu, Kiran (2003). Using the relative operating characteristic to quantify certainty in prediction of location of land cover change in India. ''Transactions in GIS'' '''7''' (4) pp. 467–484.\n*Pontius Jr, Robert Gilmore; Schneider, Laura (2001). Land-use change model validation by a ROC method for the Ipswich watershed, Massachusetts, USA. ''Agriculture, Ecosystems & Environment'' '''85''' (1-3): 239–248.\n\n==External links==\n*[https://www.youtube.com/watch?v=KKVC3GT5EPw An Introduction to the Total Operating Characteristic: Utility in Land Change Model Evaluation]\n*[https://www.youtube.com/watch?v=1JRwVOi0FSE How to run the TOC Package in R]\n*[https://github.com/amsantac/TOC TOC R package on Github]\n*[http://www2.clarku.edu/~rpontius/TOCexample2.xlsx Excel Workbook for generating TOC curves]\n\n[[Category:Boolean algebra]]"
    },
    {
      "title": "True quantified Boolean formula",
      "url": "https://en.wikipedia.org/wiki/True_quantified_Boolean_formula",
      "text": "In [[computational complexity theory]], the language '''TQBF''' is a [[formal language]] consisting of the '''true quantified Boolean formulas'''.  A (fully) quantified Boolean formula is a formula in [[Quantification (logic)|quantified]] [[propositional logic]] where every variable is quantified (or [[Bound variable|bound]]), using either [[existential quantification|existential]] or [[universal quantification|universal]] quantifiers, at the beginning of the sentence. Such a formula is equivalent to either true or false (since there are no [[Bound variable|free]] variables). If such a formula evaluates to true, then that formula is in the language TQBF. It is also known as '''QSAT''' (Quantified [[Boolean satisfiability problem|SAT]]).\n\n==Overview==\nIn computational complexity theory, the '''quantified Boolean formula problem''' ('''QBF''') is a generalization of the [[Boolean satisfiability problem]] in which both [[existential quantification|existential quantifiers]] and [[universal quantification|universal quantifiers]] can be applied to each variable. Put another way, it asks whether a quantified sentential form over a set of Boolean variables is true or false. For example, the following is an instance of QBF:\n\n: <math>\\forall x\\  \\exists y\\  \\exists z\\  ((x  \\lor z) \\land y)</math>\n\nQBF is the canonical [[complete problem]] for [[PSPACE]], the class of problems solvable by a deterministic or nondeterministic [[Turing machine]] in polynomial space and unlimited time.<ref>{{cite book |author1=M. Garey  |author2=D. Johnson  |lastauthoramp=yes | title = [[Computers and Intractability: A Guide to the Theory of NP-Completeness]] | publisher = W. H. Freeman, San Francisco, California | year = 1979 | isbn = 0-7167-1045-5}}</ref> Given the formula in the form of an [[abstract syntax tree]], the problem can be solved easily by a set of mutually recursive procedures which evaluate the formula. Such an algorithm uses space proportional to the height of the tree, which is linear in the worst case, but uses time exponential in the number of quantifiers.\n\nProvided that [[MA (complexity)|MA]] ⊊ PSPACE, which is widely believed, QBF cannot be solved, nor can a given solution even be verified, in either deterministic or [[probabilistic Turing machine|probabilistic]] polynomial time (in fact, unlike the satisfiability problem, there's no known way to specify a solution succinctly). It can be solved using an [[alternating Turing machine]] in linear time, since [[AP (complexity)|AP]] = PSPACE, where AP is the class of problems alternating machines can solve in polynomial time.<ref>{{cite journal | author = A. Chandra, D. Kozen, and [[Larry Stockmeyer|L. Stockmeyer]] | url = http://portal.acm.org/citation.cfm?id=322243 | title = Alternation | journal = Journal of the ACM | volume = 28 | issue = 1 | pages = 114&ndash;133 | year = 1981 | doi = 10.1145/322234.322243}}</ref>\n\nWhen the seminal result [[IP (complexity)|IP]] = PSPACE was shown (see [[interactive proof system]]), it was done by exhibiting an interactive proof system that could solve QBF by solving a particular arithmetization of the problem.<ref>{{cite journal | author = Adi Shamir| url = http://portal.acm.org/citation.cfm?doid=146585.146609 | title = Ip = Pspace | journal = Journal of the ACM | volume = 39 | issue = 4 | pages = 869&ndash;877 | year = 1992 | doi = 10.1145/146585.146609}}</ref>\n\nQBF formulas have a number of useful canonical forms. For example, it can be shown that there is a [[polynomial-time many-one reduction]] that will move all quantifiers to the front of the formula and make them alternate between universal and existential quantifiers. There is another reduction that proved useful in the IP = PSPACE proof where no more than one universal quantifier is placed between each variable's use and the quantifier binding that variable. This was critical in limiting the number of products in certain subexpressions of the arithmetization.\n\n== Prenex normal form ==\n\nA fully quantified Boolean formula can be assumed to have a very specific form, called [[prenex normal form]]. It has two basic parts: a portion containing only quantifiers and a portion containing an unquantified Boolean formula usually denoted as <math>\\displaystyle \\phi</math>. If there are <math>\\displaystyle n</math> Boolean variables, the entire formula can be written as\n\n:<math>\\displaystyle  \\exists x_1 \\forall x_2 \\exists x_3 \\cdots Q_n x_n \\phi(x_1, x_2, x_3, \\dots, x_n)</math>\n\nwhere every variable falls within the [[Free variables and bound variables|scope]] of some quantifier. By introducing dummy variables, any formula in prenex normal form can be converted into a sentence where existential and universal quantifiers alternate. Using the dummy variable <math>\\displaystyle y_1</math>,\n\n:<math>\\displaystyle  \\exists x_1 \\exists x_2 \\phi(x_1, x_2) \\quad \\mapsto \\quad\n\\exists x_1 \\forall y_1 \\exists x_2 \\phi(x_1, x_2)</math>\n\nThe second sentence has the same [[truth value]] but follows the restricted syntax. Assuming fully quantified Boolean formulas to be in prenex normal form is a frequent feature of proofs.\n\n== Solving ==\n\nThere is a simple recursive algorithm for determining whether a QBF is in TQBF (i.e. is true).  Given some QBF\n\n:<math>Q_1 x_1 Q_2 x_2 \\cdots Q_n x_n \\phi(x_1, x_2, \\dots, x_n).</math>\n\nIf the formula contains no quantifiers, we can just return the formula. Otherwise, we take off the first quantifier and check both possible values for the first variable:\n\n:<math>A = Q_2 x_2 \\cdots Q_n x_n \\phi(0, x_2, \\dots, x_n),</math>\n:<math>B = Q_2 x_2 \\cdots Q_n x_n \\phi(1, x_2, \\dots, x_n).</math>\n\nIf <math>Q_1 = \\exists</math>, then return <math>A \\lor B</math>. If <math>Q_1 = \\forall</math>, then return <math>A \\land B</math>.\n\nHow fast does this algorithm run?\nFor every quantifier in the initial QBF, the algorithm makes two recursive calls on only a linearly smaller subproblem. This gives the algorithm an exponential runtime [[Big O notation|O(2<sup>''n''</sup>)]].\n\nHow much space does this algorithm use?\nWithin each invocation of the algorithm, it needs to store the intermediate results of computing A and B.  Every recursive call takes off one quantifier, so the total recursive depth is linear in the number of quantifiers.  Formulas that lack quantifiers can be evaluated in space logarithmic in the number of variables. The initial QBF was fully quantified, so there are at least as many quantifiers as variables.  Thus, this algorithm uses ''O''(''n'' + log ''n'') = ''O''(''n'') space. This makes the TQBF language part of the [[PSPACE]] [[complexity class]].\n\n== PSPACE-completeness ==\n\nThe TQBF language serves in [[computational complexity theory|complexity theory]] as the canonical [[PSPACE-complete]] problem. Being PSPACE-complete means that a language is in PSPACE and that the language is also [[PSPACE-hard]].  The algorithm above shows that TQBF is in PSPACE.\nShowing that TQBF is PSPACE-hard requires showing that any language in the complexity class PSPACE can be reduced to TQBF in polynomial time. I.e.,\n\n:<math>\\forall L\\in \\mathsf{PSPACE}, L\\leq_p \\mathrm{TQBF}.</math>\n\nThis means that, for a PSPACE language L, whether an input {{mvar|x}} is in L can be decided by checking whether <math>f(x)</math> is in TQBF, for some function {{mvar|f}} that is required to run in polynomial time (relative to the length of the input). Symbolically,\n\n:<math>x\\in L\\iff f(x)\\in \\mathrm{TQBF}.</math>\n\nProving that TQBF is PSPACE-hard, requires specification of {{mvar|f}}.\n\nSo, suppose that L is a PSPACE language. This means that L can be decided by a polynomial space deterministic [[Turing machine]] (DTM). This is very important for the reduction of L to TQBF, because the configurations of any such Turing Machine can be represented as Boolean formulas, with Boolean variables representing the state of the machine as well as the contents of each cell on the Turing Machine tape, with the position of the Turing Machine head encoded in the formula by the formula's ordering. In particular, our reduction will use the variables <math>c_1</math> and <math>c_2</math>, which represent two possible configurations of the DTM for L, and a natural number t, in constructing a QBF <math>\\phi_{c_1,c_2,t}</math> which is true if and only if the DTM for L can go from the configuration encoded in <math>c_1</math> to the configuration encoded in <math>c_2</math> in no more than t steps. The function {{mvar|f}}, then, will construct from the DTM for L a QBF <math>\\phi_{c_{start},c_{accept},T}</math>, where <math>c_{start}</math> is the DTM's starting configuration, <math>c_{accept}</math> is the DTM's accepting configuration, and T is the maximum number of steps the DTM could need to move from one configuration to the other. We know that ''T'' = [[Big-o notation|''O''(exp(''n''))]], where n is the length of the input, because this bounds the total number of possible configurations of the relevant DTM. Of course, it cannot take the DTM more steps than there are possible configurations to reach <math>c_\\mathrm{accept}</math> unless it enters a loop, in which case it will never reach <math>c_\\mathrm{accept}</math> anyway.\n\nAt this stage of the proof, we have already reduced the question of whether an input formula {{mvar|w}} (encoded, of course, in <math>c_{start}</math>) is in L to the question of whether the QBF <math>\\phi_{c_{start},c_{accept},T}</math>, i.e., <math>f(w)</math>, is in TQBF. The remainder of this proof proves that {{mvar|f}} can be computed in polynomial time.\n\nFor <math>t=1</math>, computation of <math>\\phi_{c_1,c_2,t}</math> is straightforward—either one of the configurations changes to the other in one step or it does not. Since the Turing Machine that our formula represents is deterministic, this presents no problem.\n\nFor <math>t>1</math>, computation of <math>\\phi_{c_1,c_2,t}</math> involves a recursive evaluation, looking for a so-called \"middle point\" <math>m_1</math>. In this case, we rewrite the formula as follows:\n\n:<math>\\phi_{c_1,c_2,t}=\\exists m_1(\\phi_{c_1,m_1,\\lceil t/2\\rceil}\\wedge\\phi_{m_1,c_2,\\lceil t/2\\rceil}).</math>\n\nThis converts the question of whether <math>c_1</math> can reach <math>c_2</math> in t steps to the question of whether <math>c_1</math> reaches a middle point <math>m_1</math> in <math>t/2</math> steps, which itself reaches <math>c_2</math> in <math>t/2</math> steps. The answer to the latter question of course gives the answer to the former.\n\nNow, t is only bounded by T, which is exponential (and so not polynomial) in the length of the input. Additionally, each recursive layer virtually doubles the length of the formula. (The variable <math>m_1</math> is only one midpoint—for greater t, there are more stops along the way, so to speak.) So the time required to recursively evaluate <math>\\phi_{c_1,c_2,t}</math> in this manner could be exponential as well, simply because the formula could become exponentially large. This problem is solved by universally quantifying using variables <math>c_3</math> and <math>c_4</math> over the configuration pairs (e.g., <math>\\{ (c_1,m_1),(m_1,c_2)\\}</math>), which prevents the length of the formula from expanding due to recursive layers. This yields the following interpretation of <math>\\phi_{c_1,c_2,t}</math>:\n\n:<math>\\phi_{c_1,c_2,t}=\\exists m_1\\forall (c_3,c_4)\\in \\{ (c_1,m_1),(m_1,c_2)\\}(\\phi_{c_3,c_4,\\lceil t/2\\rceil}).</math>\n\nThis version of the formula can indeed be computed in polynomial time, since any one instance of it can be computed in polynomial time. The universally quantified ordered pair simply tells us that whichever choice of <math>(c_3,c_4)</math> is made, <math>\\phi_{c_1,c_2,t}\\iff\\phi_{c_3,c_4,\\lceil t/2\\rceil}</math>.\n\nThus, <math>\\forall L\\in \\mathsf{PSPACE}, L\\leq_p \\mathrm{TQBF}</math>, so TQBF is PSPACE-hard. Together with the above result that TQBF is in PSPACE, this completes the proof that TQBF is a PSPACE-complete language.\n\n(This proof follows Sipser 2006 pp.&nbsp;310–313 in all essentials. Papadimitriou 1994 also includes a proof.)\n\n==Miscellany==\n\n*One important subproblem in TQBF is the [[Boolean satisfiability problem]].  In this problem, you wish to know whether a given Boolean formula <math>\\phi</math> can be made true with some assignment of variables.  This is equivalent to the TQBF using only existential quantifiers:\n\n:: <math>\\exists x_1 \\cdots \\exists x_n \\phi(x_1, \\ldots, x_n)</math>\n\n:This is also an example of the larger result NP <math>\\subseteq</math> PSPACE which follows directly from the observation that a polynomial time   verifier for a proof of a language accepted by a NTM ([[Non-deterministic Turing machine]]) requires polynomial space to store the proof.\n*Any class in the [[polynomial hierarchy]] ([[PH (complexity)|PH]]) has TQBF as a hard problem. In other words, for the class comprising all languages L for which there exists a poly-time TM V, a verifier, such that for all input x and some constant i,\n\n:: <math>x \\in L \\Leftrightarrow \\exists y_1 \\forall y_2 \\cdots Q_i y_i \\  V(x,y_1,y_2,\\dots,y_i)\\ =\\ 1</math>\n\n: which has a specific QBF formulation that is given as\n\n:: <math>\\exists \\phi</math>  such that <math>\\exists \\vec{x_1} \\forall \\vec{x_2} \\cdots Q_i \\vec{x_i}\\  \\phi(\\vec{x_1},\\vec{x_2},\\dots,\\vec{x_i})\\ =\\ 1</math>\n\n:where the <math>\\vec{x_i}</math>'s are vectors of Boolean variables.\n*It is important to note that while TQBF the language is defined as the collection of true quantified Boolean formulas, the abbreviation TQBF is often used (even in this article) to stand for a totally quantified Boolean formula, often simply called a QBF (quantified Boolean formula, understood as \"fully\" or \"totally\" quantified). It is important to distinguish contextually between the two uses of the abbreviation TQBF in reading the literature.\n*A TQBF can be thought of as a game played between two players, with alternating moves. Existentially quantified variables are equivalent to the notion that a move is available to a player at a turn. Universally quantified variables mean that the outcome of the game does not depend on what move a player makes at that turn. Also, a TQBF whose first quantifier is existential corresponds to a [[formula game]] in which the first player has a winning strategy.\n*A TQBF for which the quantified formula is in [[conjunctive normal form|2-CNF]] may be solved in [[linear time]], by an algorithm involving [[strongly connected component|strong connectivity analysis]] of its [[implication graph]]. The [[2-satisfiability]] problem is a special case of TQBF for these formulas, in which every quantifier is existential.<ref>{{Cite journal\n| last1 = Krom | first1 = Melven R. \n| title = The Decision Problem for a Class of First-Order Formulas in Which all Disjunctions are Binary \n| journal = Zeitschrift für Mathematische Logik und Grundlagen der Mathematik \n| volume = 13 \n| pages = 15–20 \n| year = 1967 \n| doi = 10.1002/malq.19670130104\n| postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}\n}}.</ref><ref>{{Cite journal\n | last1 = Aspvall | first1 = Bengt\n | last2 = Plass | first2 = Michael F.\n | authorlink3 = Robert Tarjan | last3 = Tarjan | first3 = Robert E.\n | title = A linear-time algorithm for testing the truth of certain quantified boolean formulas\n | url = http://www.math.ucsd.edu/~sbuss/CourseWeb/Math268_2007WS/2SAT.pdf\n | journal = [[Information Processing Letters]]\n | volume = 8 | issue = 3 | pages = 121–123 | year = 1979\n | doi = 10.1016/0020-0190(79)90002-4\n | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}.</ref>\n*There is a systematic treatment of restricted versions of quantified boolean formulas (giving Schaefer-type classifications) provided in an expository paper by Hubie Chen.<ref>{{cite journal|first1=Hubie|last1=Chen|date=December 2009|title=A Rendezvous of Logic, Complexity, and Algebra|journal=ACM Computing Surveys|publisher=ACM|doi=10.1145/1592451.1592453|pages=1 |volume=42|issue=1|arxiv=cs/0611018}}</ref>\n\n== Notes and references ==\n<references/>\n* Fortnow & Homer (2003) provides some historical background for PSPACE and TQBF.\n* Zhang (2003) provides some historical background of Boolean formulas.\n* Arora, Sanjeev. (2001). [http://www.cs.princeton.edu/~arora/pubs/aroracom.ps ''COS 522: Computational Complexity''].  Lecture Notes, Princeton University.  Retrieved October 10, 2005.\n* Fortnow, Lance & Steve Homer. (2003, June).  [http://people.cs.uchicago.edu/~fortnow/beatcs/column80.pdf A short history of computational complexity].  ''The Computational Complexity Column,'' 80. Retrieved October 9, 2005.\n* Papadimitriou, C. H.  (1994).  ''Computational Complexity.''  Reading: Addison-Wesley.\n* Sipser, Michael.  (2006).  ''Introduction to the Theory of Computation.''  Boston: Thomson Course Technology.\n* Zhang, Lintao. (2003).  [http://research.microsoft.com/users/lintaoz/thesis_lintao_zhang.pdf ''Searching for truth: Techniques for satisfiability of boolean formulas''].  Retrieved October 10, 2005.\n\n== See also ==\n* [[Cook–Levin theorem]], stating that [[Boolean satisfiability problem|SAT]] is [[NP-complete]]\n* [[Generalized geography]]\n\n==External links==\n* The Quantified Boolean Formulas Library [http://www.qbflib.org (QBFLIB)]\n* [http://fmv.jku.at/qbf15/ International Workshop on Quantified Boolean Formulas]\n* [https://lonsing.github.io/depqbf/ DepQBF] - a search-based solver for quantified Boolean formulas\n* [https://github.com/ltentrup/caqe CAQE] - a CEGAR-based solver for quantified Boolean formulas; winner of the recent editions of the yearly competition of QBF solvers [http://www.qbflib.org/qbfeval/ QBFEVAL].\n* [https://github.com/MarkusRabe/cadet CADET] - a solver for quantified Boolean formulas restricted to one quantifier alternation with the ability to compute [[Skolem normal form|Skolem functions]]\n\n[[Category:Satisfiability problems]]\n[[Category:Boolean algebra]]\n[[Category:PSPACE-complete problems]]"
    },
    {
      "title": "Truth table",
      "url": "https://en.wikipedia.org/wiki/Truth_table",
      "text": "A '''truth table''' is a [[mathematical table]] used in [[logic]]—specifically in connection with [[Boolean algebra (logic)|Boolean algebra]], [[boolean function]]s, and [[propositional calculus]]—which sets out the functional values of logical [[expression (mathematics)|expressions]] on each of their functional arguments, that is, for each combination of values taken by their logical variables ([[Herbert Enderton|Enderton]], 2001). In particular, truth tables can be used to show whether a propositional expression is true for all legitimate input values, that is, [[Validity (logic)|logically valid]].\n\nA truth table has one column for each input variable (for example, P and Q), and one final column showing all of the possible results of the logical operation that the table represents (for example, P [[XOR]] Q). Each row of the truth table contains one possible configuration of the input variables (for instance, P=true Q=false), and the result of the operation for those values. See the examples below for further clarification. [[Ludwig Wittgenstein]] is often credited with inventing the truth table in his ''[[Tractatus Logico-Philosophicus]]'',<ref>{{cite journal | author = [[Georg Henrik von Wright]] | title = Ludwig Wittgenstein, A Biographical Sketch | journal = The Philosophical Review | volume = 64 | issue = 4 | year = 1955 | pages = 527–545 (p. 532, note 9) | jstor = 2182631 | doi=10.2307/2182631}}</ref> though it appeared at least a year earlier in a paper on propositional logic by [[Emil Leon Post]].<ref>{{cite journal | author=[[Emil Post]]|title=Introduction to a general theory of elementary propositions|journal=American Journal of Mathematics|date=July 1921|volume=43|issue=3|pages=163–185|jstor= 2370324|doi=10.2307/2370324}}</ref>\n\n==Unary operations==\n\nThere are 4 unary operations:\n*Always true\n*Never true, unary ''[[falsum]]''\n*Unary ''Identity'' \n*Unary ''negation'' \n\n===Logical true===\nThe output value is always true, regardless of the input value of p\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical True'''\n|-\n! style=\"width:80px\" | ''p''\n! style=\"width:80px\" | <span class=\"texhtml\">''T''</span>\n|-\n| T ||  T\n|-\n| style=\"background:papayawhip\" | F || T\n|}\n===Logical false===\nThe output value is never true: that is, always false,  regardless of the input value of p\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical False'''\n|-\n! style=\"width:80px\" | ''p''\n! style=\"width:80px\" | <span class=\"texhtml\">''F''</span>\n|-\n| T || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F ||  style=\"background:papayawhip\" | F\n|}\n\n===Logical identity===\n[[Identity function|Logical identity]] is an [[logical operation|operation]] on one [[logical value]] p, for which the output value remains p.\n\nThe truth table for the logical identity operator is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical Identity'''\n|-\n! style=\"width:80px\" | ''p''\n! style=\"width:80px\" | <span class=\"texhtml\">''p''</span>\n|-\n| T || T\n|-\n| style=\"background:papayawhip\" | F ||  style=\"background:papayawhip\" | F\n|}\n\n===Logical negation===\n[[Logical negation]] is an [[logical operation|operation]] on one [[logical value]], typically the value of a [[proposition]], that produces a value of ''true'' if its operand is false and a value of ''false'' if its operand is true.\n\nThe truth table for '''NOT p''' (also written as '''¬p''', '''Np''', '''Fpq''', or '''~p''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical Negation'''\n|-\n! style=\"width:80px\" | ''p''\n! style=\"width:80px\" | <span class=\"texhtml\">''¬p''</span>\n|-\n| T || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T\n|}\n\n\n==Binary operations==\n\nThere are 16  possible [[truth function]]s of two [[binary variable]]s:\n\n===Truth table for all binary logical operators===\nHere is an extended truth table giving definitions of all possible truth functions of two Boolean variables P and Q:<ref group=note>Information about notation may be found in [[Józef Maria Bocheński|Bocheński]] (1959), [[Herbert Enderton|Enderton]] (2001), and [[W. V. Quine|Quine]] (1982).</ref>\n\n{| class=\"wikitable\" style=\"margin:1em auto 1em auto; text-align:center;\"\n|-\n! ''p'' || ''q''\n|\n! &nbsp;[[Contradiction|F]]<sup>0</sup>&nbsp; || &nbsp;[[Logical NOR|NOR]]<sup>1</sup>&nbsp; || &nbsp;[[Converse nonimplication|↚]]<sup>2</sup>&nbsp; || &nbsp;[[Negation|'''¬p''']]<sup>3</sup>&nbsp; || &nbsp;[[Material nonimplication|↛]]<sup>4</sup>&nbsp; || &nbsp;[[Negation|'''¬q''']]<sup>5</sup>&nbsp;  || &nbsp;[[Exclusive disjunction|XOR]]<sup>6</sup>&nbsp; || &nbsp;[[Logical NAND|NAND]]<sup>7</sup>&nbsp;\n|\n!| &nbsp;[[Logical conjunction|AND]]<sup>8</sup>&nbsp; || &nbsp;[[Logical biconditional|XNOR]]<sup>9</sup>&nbsp; || [[Projection function|q]]<sup>10</sup> || [[Material_conditional|→]]<sup>11</sup> || [[Projection function|p]]<sup>12</sup> || [[Converse implication|←]]<sup>13</sup> || [[Logical disjunction|OR]]<sup>14</sup> || [[Tautology (logic)|T]]<sup>15</sup>\n|-\n! T || T\n| || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || || T || T || T || T || T || T || T || T\n|-\n! T || style=\"background:papayawhip\" | F\n| || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || T || T || || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || T || T\n|-\n! style=\"background:papayawhip\" | F || T\n| || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T\n|-\n! style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n| || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T || || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T\n|-\n! colspan=\"2\" style=\"background: #ffdead;\" |Com  \n| || ✓ || ✓ || || || || || ✓ || ✓ || || ✓ || ✓ || || || || || ✓ || ✓\n|-\n! colspan=\"2\" style=\"background: #ffdead;\" |L id  \n| || || || F || || || || F || || || T || T || T,F || T || || || F ||\n|-\n! colspan=\"2\" style=\"background: #ffdead;\" |R id  \n| || || || || || F || || F || || || T || T || || || T,F || T || F ||\n|}\n\nwhere\n\n:T = true.\n:F = false.\n:The '''Com''' row indicates whether an operator, '''op''', is [[Commutative property|commutative]] - '''P op Q = Q op P'''.\n:The '''L id''' row shows the operator's [[left identity|left identities]] if it has any - values '''I''' such that '''I op Q = Q'''.\n:The '''R id''' row shows the operator's [[right identity|right identities]] if it has any - values '''I''' such that '''P op I = P'''.<ref group=note>The operators here with equal left and right identities (XOR, AND, XNOR, and OR) are also [[monoid#Commutative_monoid|commutative monoids]] because they are also [[Associative_property|associative]]. While this distinction may be irrelevant in a simple discussion of logic, it can be quite important in more advanced mathematics. For example, in [[category theory]] an [[enriched category]] is described as a base [[category (mathematics)|category]] enriched over a monoid, and any of these operators can be used for enrichment.</ref> \n\nThe four combinations of input values for p, q, are read by row from the table above.\nThe output function for each  p, q combination, can be read, by row, from the table.\n\n'''Key:'''\n\nThe following table is oriented by column, rather than by row. There are four columns rather than four rows, to display the four combinations of p, q, as input. \n\n'''p''': T T F F<br>\n'''q''': T F T F\n\nThere are 16 rows in this key, one row for each binary function of the two binary variables, p, q. For example, in row 2 of this Key, the value of [[Converse nonimplication]] ('<math>\\nleftarrow</math>') is solely T, for the column denoted by the unique combination p=F, q=T; while  in row 2, the value of that '<math>\\nleftarrow</math>' operation is F for the three remaining columns of p, q. The output row for <math>\\nleftarrow</math> is thus\n\n2: F F T F\n\nand the 16-row<ref name=tlp5.101/> key is\n{| class=\"wikitable\" style=\"margin:1em auto 1em auto; text-align:left;\"\n|-\n! ||<ref name=tlp5.101>[[Ludwig Wittgenstein]] (1922) ''[[Tractatus Logico-Philosophicus]]'' [http://www.gutenberg.org/files/5740/5740-pdf.pdf Proposition 5.101] </ref>|| || operator || Operation name\n|-\n| 0 ||(F F F F)(p, q)|| ⊥ || [[falsum|false]], '''Opq''' || [[Contradiction]]\n|-\n| 1 ||(F F F T)(p, q)|| NOR || '''p''' ↓ '''q''', '''Xpq''' || [[Logical NOR]]\n|-\n| 2 ||(F F T F)(p, q)|| ↚ || '''p''' ↚ '''q''', '''Mpq''' || [[Converse nonimplication]]\n|-\n| 3 ||(F F T T)(p, q)|| '''¬p''', '''~p''' || '''¬p''', '''Np''', '''Fpq''' || [[Negation]]\n|-\n| 4 ||(F T F F)(p, q)|| ↛ || '''p''' ↛ '''q''', '''Lpq''' || [[Material nonimplication]]\n|-\n| 5 ||(F T F T)(p, q)|| '''¬q''', '''~q''' || '''¬q''', '''Nq''', '''Gpq''' || Negation\n|-\n| 6 ||(F T T F)(p, q)|| XOR ||'''p''' ⊕ '''q''', '''Jpq''' || [[Exclusive disjunction]]\n|-\n| 7 || (F T T T)(p, q)|| NAND || '''p''' ↑ '''q''', '''Dpq''' || [[Logical NAND]]\n|-\n| 8 || (T F F F)(p, q)|| AND || '''p''' ∧ '''q''', '''Kpq''' || [[Logical conjunction]]\n|-\n| 9 || (T F F T)(p, q)|| XNOR || '''p'''  [[If and only if]] '''q''', '''Epq'''  || [[Logical biconditional]]\n|-\n| 10 || (T F T F)(p, q)|| '''q''' || '''q''', '''Hpq''' || [[Projection function]]\n|-\n| 11 || (T F T T)(p, q)|| '''p''' &rarr; '''q'''  || if '''p''' then '''q''', '''Cpq''' || [[Material conditional|Material implication]]\n|-\n| 12 || (T T F F)(p, q)|| '''p''' || '''p''', '''Ipq''' || Projection function\n|-\n| 13 || (T T F T)(p, q)|| '''p''' &larr; '''q''' || '''p''' if '''q''', '''Bpq''' || [[Converse implication]]\n|-\n| 14 || (T T T F)(p, q)|| OR || '''p''' ∨ '''q''', '''Apq''' || [[Logical disjunction]]\n|-\n| 15 || (T T T T)(p, q)|| ⊤ || [[Tee (symbol)|true]], '''Vpq''' || [[Tautology (logic)|Tautology]]\n|}\nLogical operators can also be visualized using [[Venn diagram#Overview|Venn diagram]]s.\n\n===Logical conjunction (AND)===\n[[Logical conjunction]] is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if both of its operands are true.\n\nThe truth table for '''p AND q''' (also written as '''p ∧ q''', '''Kpq''', '''p & q''', or '''p''' <math>\\cdot</math> '''q''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical conjunction'''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p'' ∧ ''q''\n|-\n| T || T || T\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|}\n\nIn ordinary language terms, if both ''p'' and ''q'' are true, then the conjunction ''p'' ∧ ''q'' is true.  For all other assignments of logical values to ''p'' and to ''q'' the conjunction ''p''&nbsp;∧&nbsp;''q'' is false.\n\nIt can also be said that if ''p'', then ''p'' ∧ ''q'' is ''q'', otherwise ''p'' ∧ ''q'' is ''p''.\n\n===Logical disjunction (OR)===\n[[Logical disjunction]] is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if at least one of its operands is true.\n\nThe truth table for '''p OR q''' (also written as '''p ∨ q''', '''Apq''', '''p || q''', or '''p + q''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical disjunction'''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p'' ∨ ''q''\n|-\n| T || T || T\n|-\n| T || style=\"background:papayawhip\" | F || T\n|-\n| style=\"background:papayawhip\" | F || T || T\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|}\n\nStated in English, if ''p'', then ''p'' ∨ ''q'' is ''p'', otherwise ''p'' ∨ ''q'' is ''q''.\n\n===Logical implication===\nLogical implication and the [[material conditional]] are both associated with an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, which produces a value of ''false'' if the first operand is true and the second operand is false, and  a value of ''true'' otherwise.\n\nThe truth table associated with the logical implication '''p implies q''' (symbolized as '''p&nbsp;⇒&nbsp;q''', or more rarely '''Cpq''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical implication'''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p'' ⇒ ''q''\n|-\n| T || T || T\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || T\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T\n|}\n\nThe truth table associated with the material conditional '''if p then q''' (symbolized as '''p&nbsp;→&nbsp;q''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Material conditional '''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p'' → ''q''\n|-\n| T || T || T\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || T\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T\n|}\n\nIt may also be useful to note that '''p&nbsp;⇒&nbsp;q''' and '''p&nbsp;→&nbsp;q''' are equivalent to '''¬p&nbsp;∨&nbsp;q'''.\n\n===Logical equality===\n[[Logical equality]] (also known as biconditional) is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if both operands are false or both operands are true.\n\nThe truth table for '''p XNOR q''' (also written as '''p ↔ q''', '''Epq''', '''p = q''', or '''p ≡ q''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical equality'''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p'' ↔ ''q''\n|-\n| T || T || T\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T\n|}\n\nSo p EQ q is true if p and q have the same [[truth value]] (both true or both false), and false if they have different truth values.\n\n===Exclusive disjunction===\n[[Exclusive disjunction]] is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if one but not both of its operands is true.\n\nThe truth table for '''p XOR q''' (also written as '''p ⊕ q''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Exclusive disjunction'''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''¬p''v''¬q''\n|-\n| T || T || style=\"background:papayawhip\" | F\n|-\n| T || style=\"background:papayawhip\" | F || T\n|-\n| style=\"background:papayawhip\" | F || T || T\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|}\n\nFor two propositions, '''XOR''' can also be written as (p ∧ ¬q) ∨ (¬p ∧ q).\n\n===Logical NAND===\nThe [[logical NAND]] is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''false'' if both of its operands are true.  In other words, it produces a value of ''true'' if at least one of its operands is false.\n\nThe truth table for '''p NAND q''' (also written as '''p ↑ q''', '''Dpq''', or '''p | q''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical NAND'''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p'' ↑ ''q''\n|-\n| T || T || style=\"background:papayawhip\" | F\n|-\n| T || style=\"background:papayawhip\" | F || T\n|-\n| style=\"background:papayawhip\" | F || T || T\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T\n|}\n\nIt is frequently useful to express a logical operation as a compound operation, that is, as an operation that is built up or composed from other operations.  Many such compositions are possible, depending on the operations that are taken as basic or \"primitive\" and the operations that are taken as composite or \"derivative\".\n\nIn the case of logical NAND, it is clearly expressible as a compound of NOT and AND.\n\nThe negation of a conjunction: ¬(''p''&nbsp;∧&nbsp;''q''), and the disjunction of negations: (¬''p'')&nbsp;∨&nbsp;(¬''q'') can be tabulated as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p''&nbsp;∧&nbsp;''q''\n! style=\"width:15%\" | ¬(''p''&nbsp;∧&nbsp;''q'')\n! style=\"width:15%\" | ¬''p''\n! style=\"width:15%\" | ¬''q''\n! style=\"width:15%\" | (¬''p'')&nbsp;∨&nbsp;(¬''q'')\n|-\n| T || T || T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T || T\n|-\n| style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || T || T || style=\"background:papayawhip\" | F || T\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || T || T\n|}\n\n===Logical NOR===\nThe [[logical NOR]] is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if both of its operands are false. In other words, it produces a value of ''false'' if at least one of its operands is true. ↓ is also known as the [[Peirce arrow]] after its inventor, [[Charles Sanders Peirce]], and is a [[Sole sufficient operator]].\n\nThe truth table for '''p NOR q''' (also written as '''p ↓ q''', or '''Xpq''') is as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical NOR'''\n|-\n! style=\"width:15%\" | ''p''\n! style=\"width:15%\" | ''q''\n! style=\"width:15%\" | ''p'' ↓ ''q''\n|-\n| T || T || style=\"background:papayawhip\" | F\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T\n|}\n\nThe negation of a disjunction ¬(''p''&nbsp;∨&nbsp;''q''), and the conjunction of negations (¬''p'')&nbsp;∧&nbsp;(¬''q'') can be tabulated as follows:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|-\n! style=\"width:10%\" | ''p''\n! style=\"width:10%\" | ''q''\n! style=\"width:10%\" | ''p''&nbsp;∨&nbsp;''q''\n! style=\"width:10%\" | ¬(''p''&nbsp;∨&nbsp;''q'')\n! style=\"width:10%\" | ¬''p''\n! style=\"width:10%\" | ¬''q''\n! style=\"width:10%\" | (¬''p'')&nbsp;∧&nbsp;(¬''q'')\n|-\n| T || T || T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| T || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || T || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || T || T\n|}\n\nInspection of the tabular derivations for NAND and NOR, under each assignment of logical values to the functional arguments ''p'' and ''q'', produces the identical patterns of functional values for ¬(''p''&nbsp;∧&nbsp;''q'') as for (¬''p'')&nbsp;∨&nbsp;(¬''q''), and for ¬(''p''&nbsp;∨&nbsp;''q'') as for (¬''p'')&nbsp;∧&nbsp;(¬''q'').  Thus the first and second expressions in each pair are logically equivalent, and may be substituted for each other in all contexts that pertain solely to their logical values.\n\nThis equivalence is one of [[De Morgan's laws]].\n\n==Applications==\nTruth tables can be used to prove many other [[logical equivalence]]s.  For example, consider the following truth table:\n\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center;\"\n|+ '''Logical equivalence : <math>(p \\Rightarrow q) \\equiv (\\lnot p \\lor q)</math>'''\n|- style=\"background:paleturquoise\"\n! style=\"width:12%\" | <math>p</math>\n! style=\"width:12%\" | <math>q</math>\n! style=\"width:12%\" | <math>\\lnot p</math>\n! style=\"width:12%\" | <math>\\lnot p \\lor q</math>\n! style=\"width:12%\" | <math>p \\Rightarrow q</math>\n|-\n| T || T || style=\"background:papayawhip\" | F || T || T\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || T || T || T\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || T\n|}\n\nThis demonstrates the fact that <math>p \\Rightarrow q</math> is [[logically equivalent]] to <math>\\lnot p \\lor q</math>.\n\n===Truth table for most commonly used logical operators===\nHere is a truth table that gives definitions of the 6 most commonly used out of [[Tractatus Logico-Philosophicus#Propositions 4.*-5.*|the 16 possible truth functions of two Boolean variables P and Q]]:\n\n{| class=\"wikitable\" style=\"margin:1em auto 1em auto; text-align:center;\"\n|-\n! {{nobold|{{mvar|P}}}} || {{nobold|{{mvar|Q}}}} || <math>P \\land Q</math> || <math>P \\lor Q</math> || <math>P\\ \\underline{\\lor}\\ Q</math> || <math>P\\ \\underline{\\land}\\ Q</math> || <math>P \\Rightarrow Q</math> || <math>P \\Leftarrow Q</math>  ||  <math>P \\Leftrightarrow Q</math>\n|-\n| T || T ||  T  ||  T  ||  style=\"background:papayawhip\" | F   ||  T  ||  T  ||  T  ||  T\n|-\n| T || style=\"background:papayawhip\" | F  ||  style=\"background:papayawhip\" | F   ||  T  ||  T  ||  style=\"background:papayawhip\" | F   ||  style=\"background:papayawhip\" | F   ||  T  ||  style=\"background:papayawhip\" | F \n|-\n| style=\"background:papayawhip\" | F  || T ||  style=\"background:papayawhip\" | F   ||  T  ||  T  ||  style=\"background:papayawhip\" | F   ||  T  ||  style=\"background:papayawhip\" | F   ||  style=\"background:papayawhip\" | F \n|-\n| style=\"background:papayawhip\" | F  || style=\"background:papayawhip\" | F  ||  style=\"background:papayawhip\" | F   ||  style=\"background:papayawhip\" | F   ||  style=\"background:papayawhip\" | F   ||  T  ||  T  ||  T  ||  T\n|}\n\nwhere\n\n:; T : true\n:; F : false\n:; <math>\\land</math> : [[logical conjunction|AND]] (logical conjunction)\n:; <math>\\lor</math> : [[logical disjunction|OR]] (logical disjunction)\n:; <math>\\underline{\\lor}</math> : [[Exclusive or|XOR]] (exclusive or) <!-- this could be \"+\" instead according to other articles -->\n:; <math>\\underline{\\land}</math> : [[Exclusive nor|XNOR]] (exclusive nor)\n:; <math>\\Rightarrow</math> : [[logical conditional|conditional \"if-then\"]]\n:; <math>\\Leftarrow</math> : conditional \"then-if\"\n:; <math>\\Leftrightarrow</math> : [[if and only if|biconditional \"if-and-only-if\"]].\n\n===Condensed truth tables for binary operators===\nFor binary operators, a condensed form of truth table is also used, where the row headings and the column headings specify the operands and the table cells specify the result. For example, [[Boolean logic]] uses this condensed truth table notation:\n\n{|\n|-\n| style=\"width:80px;\"|\n|\n{| class=\"wikitable\" style=\"margin:1em auto 1em auto; text-align:center;\"\n|-\n! ∧ || F || T\n|-\n! F\n| F || F\n|-\n! T\n| F || T\n|}\n| style=\"width:80px;\"|\n|\n{| class=\"wikitable\" style=\"margin:1em auto 1em auto; text-align:center;\"\n|-\n! ∨ || F || T\n|-\n! F\n| F || T\n|-\n! T\n| T || T\n|}\n|}\n\nThis notation is useful especially if the operations are commutative, although one can additionally specify that the rows are the first operand and the columns are the second operand. This condensed notation is particularly useful in discussing multi-valued extensions of logic, as it significantly cuts down on combinatoric explosion of the number of rows otherwise needed. It also provides for quickly recognizable characteristic \"shape\" of the distribution of the values in the table which can assist the reader in grasping the rules more quickly.\n\n===Truth tables in digital logic===\nTruth tables are also used to specify the function of [[Lookup table#Hardware LUTs|hardware look-up tables (LUTs)]] in [[Digital circuit|digital logic circuitry]]. For an n-input LUT, the truth table will have 2^''n'' values (or rows in the above tabular format), completely specifying a boolean function for the LUT. By representing each boolean value as a [[bit]] in a [[Binary numeral system|binary number]], truth table values can be efficiently encoded as [[integer]] values in [[Electronic design automation|electronic design automation (EDA)]] [[software]]. For example, a 32-bit integer can encode the truth table for a LUT with up to 5 inputs.\n\nWhen using an integer representation of a truth table, the output value of the LUT can be obtained by calculating a bit index ''k'' based on the input values of the LUT, in which case the LUT's output value is the ''k''th bit of the integer. For example, to evaluate the output value of a LUT given an [[Array data structure|array]] of ''n'' boolean input values, the bit index of the truth table's output value can be computed as follows: if the ''i''th input is true, let <math>V_i = 1</math>, else let <math>V_i = 0</math>. Then the ''k''th bit of the binary representation of the truth table is the LUT's output value, where <math>k = V_0 \\times 2^0 + V_1 \\times 2^1 + V_2 \\times 2^2 + \\dots + V_n \\times 2^n</math>.\n\nTruth tables are a simple and straightforward way to encode boolean functions, however given the [[exponential growth]] in size as the number of inputs increase, they are not suitable for functions with a large number of inputs. Other representations which are more memory efficient are text equations and [[binary decision diagram]]s.\n\n===Applications of truth tables in digital electronics===\nIn digital electronics and computer science (fields of applied logic engineering and mathematics), truth tables can be used to reduce basic boolean operations to simple correlations of inputs to outputs, without the use of [[logic gate]]s or code. For example, a binary addition can be represented with the truth table:\n\n<pre>\nA B | C R\n1 1 | 1 0\n1 0 | 0 1\n0 1 | 0 1\n0 0 | 0 0\n\nwhere\n\nA = First Operand\nB = Second Operand\nC = Carry\nR = Result\n</pre>\n\nThis truth table is read left to right:\n* Value pair (A,B) equals value pair (C,R).\n* Or for this example, A plus B equal result R, with the Carry C.\n\nNote that this table does not describe the logic operations necessary to implement this operation, rather it simply specifies the function of inputs to output values.\n\nWith respect to the result, this example may be arithmetically viewed as modulo 2 binary addition, and as logically equivalent to the exclusive-or (exclusive disjunction) binary logic operation.\n\nIn this case it can be used for only very simple inputs and outputs, such as 1s and 0s. However, if the number of types of values one can have on the inputs increases, the size of the truth table will  increase.\n\nFor instance, in an addition operation, one needs two operands, A and B. Each can have one of two values, zero or one. The number of combinations of these two values is 2×2, or four. So the result is four possible outputs of C and R.  If one were to use base 3, the size would increase to 3×3, or nine possible outputs.\n\nThe first \"addition\" example above is called a half-adder. A full-adder is when the carry from the previous operation is provided as input to the next adder. Thus, a truth table of eight rows would be needed to describe a [[full adder]]'s logic:\n\n<pre>\nA B C* | C R\n0 0 0  | 0 0\n0 1 0  | 0 1\n1 0 0  | 0 1\n1 1 0  | 1 0\n0 0 1  | 0 1\n0 1 1  | 1 0\n1 0 1  | 1 0\n1 1 1  | 1 1\n\nSame as previous, but..\nC* = Carry from previous adder\n</pre>\n\n==History==\nIrving Anellis’s research shows that [[C.S. Peirce]] appears to be the earliest logician (in 1893) to devise a truth table matrix.<ref>{{cite journal|last1=Anellis|first1=Irving H.|title=Peirce's Truth-functional Analysis and the Origin of the Truth Table|journal=History and Philosophy of Logic|date=2012|volume=33|pages=87–97|doi=10.1080/01445340.2011.621702}}</ref> From the summary of his paper:\n<blockquote> In 1997, John Shosky discovered, on the verso of a page of the typed transcript of Bertrand Russell's 1912 lecture on \"The Philosophy of Logical Atomism\" truth table matrices. The matrix for negation is Russell's, alongside of which is the matrix for material implication in the hand of Ludwig Wittgenstein. It is shown that an unpublished manuscript identified as composed by Peirce in 1893 includes a truth table matrix that is equivalent to the matrix for material implication discovered by John Shosky. An unpublished manuscript by Peirce identified as having been composed in 1883–84 in connection with the composition of Peirce's \"On the Algebra of Logic: A Contribution to the Philosophy of Notation\" that appeared in the ''[[American Journal of Mathematics]]'' in 1885 includes an example of an indirect truth table for the conditional. </blockquote>\n\n==Notes==\n{{reflist|group=note}}\n\n==See also==\n{{Portal|Thinking|Logic}}\n{{div col|colwidth=20em}}\n* [[Boolean domain]]\n* [[Boolean-valued function]]\n* [[Publicad|Espresso heuristic logic minimizer]]\n* [[Excitation table]]\n* [[First-order logic]]\n* [[Functional completeness]]\n* [[Karnaugh maps]]\n* [[Logic gate]]\n* [[Logical connective]]\n* [[Logical graph]]\n* [[Method of analytic tableaux]]\n* [[Propositional calculus]]\n* [[Truth function]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* [[Bocheński, Józef Maria]] (1959), ''A Précis of Mathematical Logic'', translated from the French and German editions by Otto Bird, Dordrecht, South Holland:  D. Reidel.\n* [[Herbert Enderton|Enderton, H.]] (2001). ''A Mathematical Introduction to Logic'', second edition, New York:  Harcourt Academic Press. {{isbn|0-12-238452-0}}\n* [[W.V. Quine|Quine, W.V.]] (1982), ''Methods of Logic'', 4th edition, Cambridge, MA:  Harvard University Press.\n\n==External links==\n{{Commons category|Truth tables}}\n* {{springer|title=Truth table|id=p/t094370}}\n*[http://sites.millersville.edu/bikenaga/math-proof/truth-tables/truth-tables.html Truth Tables, Tautologies, and Logical Equivalence]\n*[https://arxiv.org/ftp/arxiv/papers/1108/1108.2429.pdf PEIRCE'S TRUTH-FUNCTIONAL ANALYSIS AND THE ORIGIN OF TRUTH TABLES] by Irving H. Anellis\n*[http://www.allaboutcircuits.com/vol_4/chpt_7/9.html Converting truth tables into Boolean expressions]\n{{Classical logic}}\n{{Mathematical logic}}\n\n{{Authority control}}\n{{DEFAULTSORT:Truth Table}}\n[[Category:Boolean algebra]]\n[[Category:Mathematical tables]]\n[[Category:Semantics]]\n[[Category:Propositional calculus]]\n[[Category:Conceptual models]]"
    },
    {
      "title": "Two-element Boolean algebra",
      "url": "https://en.wikipedia.org/wiki/Two-element_Boolean_algebra",
      "text": "In [[mathematics]] and [[abstract algebra]], the '''two-element Boolean algebra''' is the [[Boolean algebra (structure)| Boolean algebra]] whose ''underlying set'' (or [[Universe_(mathematics)|universe]] or ''carrier'') ''B'' is the [[Boolean domain]]. The elements of the Boolean domain are 1 and 0 by convention, so that ''B''&nbsp;=&nbsp;{0,&nbsp;1}. [[Paul Halmos]]'s name for this algebra \"'''2'''\" has some following in the literature, and will be employed here.\n\n==Definition==\n''B'' is a [[partial order|partially ordered set]] and the elements of ''B'' are also its [[bounded set|bounds]].\n\nAn [[operation (mathematics)|operation]] of [[arity]] ''n'' is a [[map (mathematics)|mapping]] from ''B''<sup>n</sup> to ''B''. Boolean algebra consists of two [[binary operation]]s and [[unary operation|unary]] [[Complement (order theory)|complementation]]. The binary operations have been named and notated in various ways. Here they are called  'sum' and 'product', and notated by infix '+' and '∙', respectively. Sum and product [[commutativity|commute]] and [[associativity|associate]], as in the usual [[elementary algebra|algebra of real numbers]]. As for the [[order of operations]], brackets are decisive if present. Otherwise '∙' precedes '+'. Hence ''A∙B&nbsp;+&nbsp;C'' is parsed as ''(A∙B)&nbsp;+&nbsp;C'' and not as ''A∙(B&nbsp;+&nbsp;C)''. [[Boolean algebra (logic)|Complementation]] is denoted by writing an overbar over its argument. The numerical analog of the complement of ''X'' is 1&nbsp;&minus;&nbsp;''X''. In the language of [[universal algebra]], a Boolean algebra is a <math>\\langle B,+,.,\\overline{..},1,0\\rangle</math> [[algebraic structure|algebra]] of [[arity|type]] <math>\\langle 2,2,1,0,0\\rangle</math>.\n\nEither [[one-to-one correspondence]] between {0,1} and {''True'',''False''} yields classical [[bivalent logic]] in equational form, with complementation read as [[logical NOT|NOT]]. If 1 is read as ''True'', '+' is read as [[logical OR|OR]], and '∙' as [[logical AND|AND]], and vice versa if 1 is read as ''False''. These two operations define a commutative [[semiring]], known as the [[Boolean semiring]].\n\n==Some basic identities==\n'''2''' can be seen as grounded in the following trivial \"Boolean\" arithmetic:\n\n:<math>\n\\begin{align}\n&1 + 1 = 1 + 0 = 0 + 1 = 1 \\\\\n&0 + 0 = 0 \\\\\n&0\\cdot0 = 0\\cdot1 = 1\\cdot0 = 0 \\\\\n&1\\cdot1 = 1 \\\\\n&\\overline{1} = 0 \\\\\n&\\overline{0} = 1\n\\end{align}\n</math>\n\nNote that:\n* '+' and '∙' work exactly as in numerical arithmetic, except that 1+1=1. '+' and '∙' are derived by analogy from numerical arithmetic; simply set any nonzero number to 1.\n* Swapping 0 and 1, and '+' and '∙' preserves truth; this is the essence of the [[Duality (order theory)|duality]] pervading all Boolean algebras.\nThis Boolean arithmetic suffices to verify any equation of '''2''', including the axioms, by examining every possible assignment of 0s and 1s to each variable (see [[decision procedure]]).\n\nThe following equations may now be verified:\n\n:<math>\n\\begin{align}\n&A + A = A \\\\\n&A \\cdot A = A \\\\\n&A + 0 = A \\\\\n&A + 1 = 1 \\\\\n&A \\cdot 0 = 0 \\\\\n&\\overline{\\overline{A}} = A\n\\end{align}\n</math>\n\nEach of '+' and '∙' [[distributivity|distributes]] over the other:\n*<math>\\ A \\cdot (B+C) = A \\cdot B + A \\cdot C;</math>\n*<math>\\ A+(B \\cdot C) = (A+B) \\cdot (A+C).</math>\nThat '∙' distributes over '+' agrees with [[elementary algebra]], but not '+' over '∙'. For this and other reasons, a sum of products (leading to a [[Sheffer stroke|NAND]] synthesis) is more commonly employed than a product of sums (leading to a [[Logical NOR|NOR]] synthesis).\n\nEach of '+' and '∙' can be defined in terms of the other and complementation:\n* <math>A \\cdot B=\\overline{\\overline{A}+\\overline{B}}</math>\n* <math>A+B=\\overline{\\overline{A} \\cdot \\overline{B}}.</math>\nWe only need one binary operation, and [[concatenation]] suffices to denote it. Hence concatenation and overbar suffice to notate '''2'''. This notation is also that of [[Willard Van Orman Quine|Quine]]'s [[Boolean term schemata]]. Letting (''X'') denote the complement of ''X'' and \"()\" denote either 0 or 1 yields the [[syntax]] of the [[laws of form|primary algebra]].\n\nA ''basis'' for '''2''' is a set of equations, called [[axiom]]s, from which all of the above equations (and more) can be derived. There are many known bases for all Boolean algebras and hence for '''2'''. An elegant basis notated using only concatenation and overbar is:\n# <math>\\ ABC = BCA</math> (Concatenation commutes, associates)\n# <math>\\overline{A}A = 1</math> ('''2''' is a [[Complement (order theory)|complemented]] lattice, with an [[bounded set|upper bound]] of 1)\n#<math>\\ A0 = A</math> (0 is the [[bounded set|lower bound]]).\n# <math>A\\overline{AB} = A\\overline{B}</math> ('''2''' is a [[distributive lattice]])\n\nWhere concatenation = OR, 1 = true, and 0 = false, or concatenation = AND, 1 = false, and 0 = true. (overbar is negation in both cases.)\n\nIf 0=1, (1)-(3) are the axioms for an [[abelian group]].\n\n(1) only serves to prove that concatenation commutes and associates. First assume that (1) associates from either the left or the right, then prove commutativity. Then prove association from the other direction. Associativity is simply association from the left and right combined.\n\nThis basis makes for an easy approach to proof, called [[Laws of Form|calculation]], that proceeds by simplifying expressions to 0 or 1, by invoking axioms (2)&ndash;(4), and the elementary identities <math>AA=A, \\overline{\\overline{A}}=A, 1+A = 1</math>, and the distributive law.\n\n==Metatheory==\n[[De Morgan's theorem]] states that if one does the following, in the given order, to any [[Boolean function]]:\n* Complement every variable;\n* Swap '+' and '∙' operators (taking care to add brackets to ensure the order of operations remains the same);\n* Complement the result,\nthe result is [[Logical equivalence|logically equivalent]] to what you started with. Repeated application of De Morgan's theorem to parts of a function can be used to drive all complements down to the individual variables.\n\nA powerful and nontrivial [[metatheorem]] states that any theorem of '''2''' holds for all Boolean algebras.<ref>{{Cite book |doi = 10.1007/978-0-387-68436-9|title = Introduction to Boolean Algebras|series = Undergraduate Texts in Mathematics|year = 2009|last1 = Halmos|first1 = Paul|last2 = Givant|first2 = Steven|isbn = 978-0-387-40293-2}}</ref> Conversely, an identity that holds for an arbitrary nontrivial Boolean algebra also holds in '''2'''. Hence all the mathematical content of Boolean algebra is captured by '''2'''. This theorem is useful because any equation in '''2''' can be verified by a [[decision procedure]]. Logicians refer to this fact as \"'''2''' is [[decidability (logic)|decidable]]\". All known [[decision procedure]]s require a number of steps that is an [[exponential function]] of the number of variables ''N'' appearing in the equation to be verified. Whether there exists a decision procedure whose steps are a [[polynomial function]] of ''N'' falls under the [[P&nbsp;=&nbsp;NP]] conjecture.\n<!--==Minterms and minimum two level forms==\nAny Boolean expression can be written as a series of [[minterm]]s added together-->\n\n==See also==\n*[[Boolean algebra]]\n*[[Boolean semiring]]\n*[[Bounded set]]\n*[[Lattice (order)]]\n*[[Order theory]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\nMany elementary texts on Boolean algebra were published in the early years of the computer era. Perhaps the best of the lot, and one still in print, is:\n* Mendelson, Elliot, 1970. ''Schaum's Outline of Boolean Algebra''. McGraw&ndash;Hill.\n\nThe following items reveal how the two-element Boolean algebra is mathematically nontrivial.\n* [[Stanford Encyclopedia of Philosophy]]: \"[http://plato.stanford.edu/entries/boolalg-math/ The Mathematics of Boolean Algebra,]\" by J. Donald Monk.\n* Burris, Stanley N., and H.P. Sankappanavar, H. P., 1981. ''[http://www.thoralf.uwaterloo.ca/htdocs/ualg.html A Course in Universal Algebra.]''  Springer-Verlag. {{isbn|3-540-90578-2}}.\n\n[[Category:Elementary algebra]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Vector logic",
      "url": "https://en.wikipedia.org/wiki/Vector_logic",
      "text": "'''Vector logic'''<ref name=\"miz92\">Mizraji, E. (1992). [http://www.sciencedirect.com/science/article/pii/016501149290216Q Vector logics: the matrix-vector representation of logical calculus.] Fuzzy Sets and Systems, 50, 179–185</ref><ref name=\"miz08\">Mizraji, E. (2008) [http://logcom.oxfordjournals.org/content/18/1/97.full.pdf Vector logic: a natural algebraic representation of the fundamental logical gates.] Journal of Logic and Computation, 18, 97–121</ref> is an [[algebra]]ic [[Mathematical model|model]] of elementary [[logic]] based on [[Matrix (mathematics)|matrix algebra]]. Vector logic assumes that the [[truth value]]s map on [[Vector (mathematics and physics)|vectors]], and that the [[Monadic predicate calculus|monadic]] and [[Binary function|dyadic]] operations are executed by matrix operators. \"Vector logic\" has also been used to refer to the representation of classical propositional logic as a vector space, <ref>Westphal, J. and Hardy, J. (2005) Logic as a Vector System. Journal of Logic and Computation, 751-765</ref><ref>Westphal, J.  Caulfield, H.J. Hardy, J. and Qian, L.(2005) Optical Vector Logic Theorem-Proving. Proceedings of the Joint Conference on Information Systems, Photonics, Networking and Computing Division.</ref> in which the unit vectors are propositional variables. Predicate logic can be represented as a vector space of the same type in which the axes represent the predicate letters <math>S</math> and <math>P</math>.<ref>Westphal, J (2010). The Application of Vector Theory to Syllogistic Logic. New Perspectives on the Square of Opposition, Bern, Peter Lang.</ref> In the vector space for propositional logic the origin represents the false, F, and the infinite periphery represents the true, T, whereas in the space for predicate logic the origin represents \"nothing\" and the periphery represents the flight from nothing, or \"something\".\n\n== Overview ==\nClassic [[Truth value#Classical logic|binary]] logic is represented by a small set of mathematical functions depending on one (monadic ) or two (dyadic) variables. In the binary set, the value 1 corresponds to ''[[True (logic)|true]]'' and the value 0 to ''[[False (logic)|false]]''. A two-valued vector logic requires a correspondence between the truth-values ''true'' (t)  and ''false'' (f), and two ''q''-dimensional normalized [[column vector]]s composed by real numbers ''s'' and ''n'', hence:\n\n: <math>t\\mapsto s</math>&nbsp;&nbsp;&nbsp; and &nbsp;&nbsp;&nbsp;<math>f\\mapsto n</math>\n\n(where <math> q \\geq 2</math> is an arbitrary natural number, and “normalized” means that the [[Euclidean norm|length]] of the vector is 1; usually s and n are orthogonal vectors).  This correspondence generates a space of vector truth-values: ''V''<sub>2</sub>&nbsp;=&nbsp;{''s'',''n''}. The basic logical operations defined using this set of vectors lead to matrix operators.\n\nThe operations of vector logic are based on the scalar product between ''q''-dimensional column vectors: <math>u^Tv=\\langle u,v\\rangle</math>: the orthonormality between vectors ''s'' and ''n'' implies that <math>\\langle u,v\\rangle=1</math> if <math>u = v</math>, and <math>\\langle u,v\\rangle=0</math> if <math>u \\ne v</math>, where <math>u, v \\in \\{s, n\\}</math>.\n\n===Monadic operators===\nThe monadic operators result from the application <math>Mon: V_2 \\to V_2</math>, and the associated matrices have ''q'' rows and ''q'' columns. The two basic monadic operators for this two-valued vector logic are the [[Identity function|identity]] and the [[Logical negation|negation]]:\n\n* '''Identity''': A logical identity ID(''p'') is represented by matrix <math>I=ss^T + nn^T</math>, where the juxtapositions are [[Kronecker product]]s. This matrix operates as follows:  ''Ip''&nbsp;=&nbsp;''p'', ''p''&nbsp;∈&nbsp;''V''<sub>2</sub>; due to the orthogonality of ''s'' respect to ''n'', we have <math>Is=ss^Ts+nn^Ts=s\\langle s,s\\rangle+n\\langle n,s\\rangle=s</math>, &nbsp; and conversely <math>In=n</math>.  It is important to note that this vector logic identity matrix is not generally an [[identity matrix]] in the sense of matrix algebra.\n* '''Negation''': A logical negation ¬''p'' is represented by matrix <math>N=ns^T + sn^T</math> Consequently, ''Ns''&nbsp;=&nbsp;''n''  and ''Nn''&nbsp;=&nbsp;''s''. The [[Involution (mathematics)|involutory]] behavior of the logical negation, namely that ¬(¬''p'') equals ''p'',  corresponds with the fact that ''N''<sup>2</sup>&nbsp;=&nbsp;''I''.\n\n=== Dyadic operators ===\nThe 16 two-valued dyadic operators correspond to functions of the type <math>Dyad: V_2 \\otimes V_2\\to V_2</math>; the dyadic matrices have ''q''<sup>2</sup> rows and ''q'' columns.\nThe matrices that execute these dyadic operations are based on the properties of the [[Kronecker product]]. (Multiplying such a dyadic matrix by a <math>q \\times q</math> matrix yields a <math>q \\times 1</math> column whose entries are [[Frobenius inner product]]s of the square matrix by blocks of its same size within the dyadic matrix.)\n\nTwo properties of this product are essential for the formalism of vector logic:\n{{ordered list\n|1= '''The mixed-product property'''\nIf '''A''', '''B''', '''C''' and '''D''' are matrices of such size that one can form the matrix products '''AC''' and '''BD''', then\n:<math> (A \\otimes B)(C \\otimes D) = AC \\otimes BD </math>\n|2= '''Distributive transpose''' The operation of transposition is distributive over the Kronecker product:\n:<math>(A\\otimes B)^T = A^T \\otimes B^T.</math>\n}}\n\nUsing these properties, expressions for dyadic logic functions can be obtained:\n\n* '''[[∧|Conjunction]]'''. The conjunction (p∧q) is executed by a matrix that acts on two vector truth-values: <math>C(u\\otimes v)</math> .This matrix reproduces the features of the classical conjunction truth-table in its formulation:\n\n::<math>C=s(s\\otimes s)^T + n(s\\otimes n)^T + n(n\\otimes s)^T + n(n\\otimes n)^T </math>\n\n::and verifies\n\n::<math>C(s\\otimes s)=s,</math> and\n\n::<math>C(s\\otimes n)=C(n\\otimes s)=C(n\\otimes n)=n.</math>\n\n* '''[[∨|Disjunction]]'''. The disjunction (p∨q) is executed by the matrix\n::<math>D=s(s\\otimes s)^T+s(s\\otimes n)^T+s(n\\otimes s)^T+n(n\\otimes n)^T,</math> resulting in\n::<math>D(s\\otimes s)=D(s\\otimes n)=D(n\\otimes s)=s</math> and\n::<math>D(n\\otimes n)=n.</math>\n\n* '''[[Logical implication|Implication]]'''. The implication corresponds in classical logic to the expression p&nbsp;→&nbsp;q&nbsp;≡&nbsp;¬p&nbsp;∨&nbsp;q. The vector logic version of this equivalence leads to a matrix that represents this implication in vector logic: <math>L=D(N\\otimes I)</math>. The explicit expression for this implication is:\n\n::<math>L=s(s\\otimes s)^T+n(s\\otimes n)^T+s(n\\otimes s)^T+s(n\\otimes n)^T,</math>\n\n::and the properties of classical implication are satisfied:\n::<math>L(s\\otimes s)=L(n\\otimes s)=L(n\\otimes n)=s</math> and\n::<math>L(s\\otimes n)=n.</math>\n\n* '''[[Logical equivalence|Equivalence]] and [[Exclusive or]]'''. In vector logic the equivalence p≡q is represented by the following matrix:\n::<math>E=s(s\\otimes s)^T+n(s\\otimes n)^T+n(n\\otimes s)^T+s(n\\otimes n)^T</math> with\n\n::<math>E(s\\otimes s)=E(n\\otimes n)=s</math> and\n\n::<math>E(s\\otimes n)=E(n\\otimes s)=n.</math>\n\n::The Exclusive or is the negation of the equivalence, ¬(p≡q); it corresponds with the matrix <math>X=NE</math> given by\n\n::<math>X=n(s\\otimes s)^T+s(s\\otimes n)^T+s(n\\otimes s)^T+n(n\\otimes n)^T,</math>\n\n::with <math>X(s\\otimes s)=X(n\\otimes n)=n</math> and\n\n::<math>X(s\\otimes n)=X(n\\otimes s)=s.</math>\n\n* '''[[Sheffer stroke|NAND]] and [[Peirce arrow|NOR]]'''\nThe matrices ''S'' and ''P'' correspond to the [[Sheffer stroke|Sheffer]] (NAND) and the [[Logical NOR|Peirce]] (NOR) operations, respectively:\n::<math>S=NC</math>   \n::<math>P=ND</math>\n\n=== De Morgan's law ===\nIn the two-valued logic, the conjunction and the disjunction operations satisfy the [[De Morgan's Laws|De Morgan's law]]: p∧q≡¬(¬p∨¬q), and its dual: p∨q≡¬(¬p∧¬q)). For the two-valued vector logic this Law is also verified:\n\n::<math>C(u\\otimes v)=ND(Nu\\otimes Nv)</math>, where ''u'' and ''v'' are two logic vectors.\n\nThe Kronecker product implies the following factorization:\n\n::<math>C(u\\otimes v)=ND(N\\otimes N)(u\\otimes v).</math>\n\nThen it can be proved that in the two–dimensional vector logic the De Morgan's law is a law involving operators, and not only a law concerning operations:<ref name=\"miz96\">Mizraji, E. (1996) The operators of vector logic. Mathematical Logic Quarterly, 42, 27–39</ref>\n\n::<math>C=ND(N\\otimes N)</math>\n\n=== Law of contraposition ===\nIn the classical propositional calculus, the [[Contraposition (traditional logic)|Law of Contraposition]] ''p''&nbsp;→&nbsp;''q''&nbsp;≡&nbsp;¬''q''&nbsp;→&nbsp;¬''p'' is proved because the equivalence holds for all the possible combinations of truth-values of ''p'' and ''q''.<ref name=\"suppes\">Suppes, P. (1957) Introduction to Logic, Van Nostrand Reinhold, New York.</ref> Instead, in vector logic, the law of contraposition emerges from a chain of equalities within the rules of matrix algebra and Kronecker products, as shown in what follows:\n\n::<math>L(u\\otimes v)=D(N\\otimes I)(u\\otimes v)=D(Nu\\otimes v)=D(Nu\\otimes NNv)=</math>\n::<math> D(NNv\\otimes Nu)=D(N\\otimes I)(Nv\\otimes Nu)=L(Nv\\otimes Nu)</math>\n\nThis result is based in the fact that ''D'', the disjunction matrix, represents a commutative operation.\n\n== Many-valued two-dimensional logic ==\n[[Many-valued logic]] was developed by many researchers, particularly by [[Jan Łukasiewicz]] and allows extending logical operations to truth-values that include uncertainties.<ref>Łukasiewicz, J. (1980) Selected Works. L. Borkowski, ed., pp. 153–178. North-Holland,\nAmsterdam, 1980</ref> In the case of two-valued vector logic, uncertainties in the truth values can be introduced using vectors with ''s'' and ''n'' weighted by probabilities.\n\nLet <math>f=\\epsilon s + \\delta n</math>, with <math>\\epsilon, \\delta \\in [0,1], \\epsilon + \\delta = 1</math> be this kind of “probabilistic” vectors. Here, the many-valued character of the logic is introduced [[A priori and a posteriori|''a posteriori'']] via the uncertainties introduced in the inputs.<ref name=\"miz92\"/>\n\n===Scalar projections of vector outputs===\nThe outputs of this many-valued logic can be projected on scalar functions and generate a particular class of probabilistic logic with similarities with the many-valued logic of Reichenbach.<ref>Rescher, N. (1969) Many-Valued Logic. McGraw–Hill, New York</ref><ref>Blanché, R. (1968) Introduction à la Logique Contemporaine, Armand Colin, Paris</ref><ref>Klir, G.J., Yuan, G. (1995) Fuzzy Sets and Fuzzy Logic. Prentice–Hall, New Jersey</ref>  Given two vectors <math>u=\\alpha s + \\beta n</math> and <math>v=\\alpha's + \\beta'n</math> and a dyadic logical matrix <math>G</math>, a scalar probabilistic logic is provided by the projection over vector&nbsp;''s'':\n\n::<math>Val(\\mathrm{scalars}) = s^TG(\\mathrm{vectors})</math>\n\nHere are the main results of these projections:\n\n::<math>NOT(\\alpha)=s^TNu=1-\\alpha</math>\n::<math>OR(\\alpha,\\alpha')=s^TD(u\\otimes v)=\\alpha + \\alpha' - \\alpha\\alpha'</math>\n::<math>AND(\\alpha,\\alpha')=s^TC(u\\otimes v)=\\alpha\\alpha'</math>\n::<math>IMPL(\\alpha,\\alpha')=s^TL(u\\otimes v)=1-\\alpha(1-\\alpha')</math>\n::<math>XOR(\\alpha,\\alpha')=s^TX(u\\otimes v)=\\alpha+\\alpha'-2\\alpha\\alpha'</math>\n\nThe associated negations are:\n::<math>NOR(\\alpha,\\alpha')=1-OR(\\alpha,\\alpha')</math>\n::<math>NAND(\\alpha,\\alpha')=1-AND(\\alpha,\\alpha')</math>\n::<math>EQUI(\\alpha,\\alpha')=1-XOR(\\alpha,\\alpha')</math>\n\nIf the scalar values belong to the set {0, ½, 1}, this many-valued scalar logic is for many of the operators almost identical to the 3-valued logic of Łukasiewicz. Also, it has been proved that when the monadic or dyadic operators act over probabilistic vectors belonging to this set, the output is also an element of this set.<ref name=\"miz96\"/>\n\n== History ==\nEarly attempts to use linear algebra to represent logic operations can be referred to [[Charles Sanders Peirce|Peirce]] and [[Irving Copi|Copilowish]],<ref>Copilowish, I.M. (1948) Matrix development of the calculus of relations. Journal of Symbolic Logic, 13, 193–203</ref> particularly in the use of [[logical matrix|logical matrices]] to interpret the [[algebraic logic#Calculus of relations|calculus of relations]].\n\nThe approach has been inspired in [[neural network]] models based on the use of high-dimensional matrices and vectors.<ref>Kohonen, T. (1977) Associative Memory: A System-Theoretical Approach. Springer-Verlag, New York</ref><ref>Mizraji, E. (1989) [https://link.springer.com/article/10.1007%2FBF02458441 Context-dependent associations in linear distributed memories]. Bulletin of Mathematical Biology, 50, 195–205</ref> Vector logic is a direct translation into a matrix-vector formalism of the classical [[Boolean algebra|Boolean polynomials]].<ref name=\"boole\">Boole, G. (1854) An Investigation of the Laws of Thought, on which are Founded the Theories of Logic and Probabilities. Macmillan, London, 1854; Dover, New York Reedition, 1958</ref> This kind of formalism has been applied to develop a [[fuzzy logic]] in terms of [[complex numbers]].<ref>Dick, S. (2005) Towards complex fuzzy logic. IEEE Transactions on Fuzzy Systems, 15,405–414, 2005</ref> Other matrix and vector approaches to logical calculus have been developed in the framework of [[quantum physics]], [[computer science]] and [[optics]].<ref>Mittelstaedt, P. (1968) Philosophische Probleme der Modernen Physik, Bibliographisches Institut, Mannheim</ref><ref>Stern, A. (1988) Matrix Logic: Theory and Applications. North-Holland, Amsterdam</ref>\n\nThe [[Indian people|Indian]] biophysicist [[G.N. Ramachandran]] developed a formalism using algebraic matrices and vectors to represent many operations of classical Jain Logic known as Syad and Saptbhangi. [[Indian logic]].<ref>Jain, M.K. (2011) Logic of evidence-based inference propositions, Current Science, 1663–1672, 100</ref> It requires independent affirmative evidence for each assertion in a proposition, and does not make the assumption for binary complementation.\n\n== Boolean polynomials ==\n\n[[George Boole]] established the development of logical operations as polynomials.<ref name=\"boole\"/> For the case of monadic operators (such as [[Identity function|identity]] or\n[[Logical negation|negation]]), the Boolean polynomials look as follows:\n\n::<math>f(x) = f(1)x + f(0)(1-x) </math>\n\nThe four different monadic operations result from the different binary values for the coefficients. Identity operation requires ''f''(1)&nbsp;=&nbsp;1 and ''f''(0)&nbsp;=&nbsp;0, and negation occurs if ''f''(1)&nbsp;=&nbsp;0 and ''f''(0)&nbsp;=&nbsp;1.  For the 16 dyadic operators, the Boolean polynomials are of the form:\n\n::<math>f(x,y) = f(1,1)xy + f(1,0)x(1-y) +f(0,1)(1-x)y + f(0,0)(1-x)(1-y)</math>\n\nThe dyadic operations can be translated to this polynomial format when the coefficients ''f'' take the values indicated in the respective [[truth table]]s. For instance: the [[Sheffer stroke|NAND]] operation requires that:\n::<math> f(1,1)=0</math>    and    <math>f(1,0)=f(0,1)=f(0,0)=1</math>. \nThese Boolean polynomials can be immediately extended to any number of variables, producing a large potential variety of logical operators.\nIn vector logic, the matrix-vector structure of logical operators is an exact translation to the format of linear algebra of these Boolean polynomials, where the ''x'' and 1&minus;''x'' correspond to vectors ''s'' and ''n'' respectively (the same for ''y'' and 1&minus;''y''). In the example of NAND, ''f''(1,1)=''n'' and ''f''(1,0)=''f''(0,1)=''f''(0,0)=''s'' and the matrix version becomes:\n\n::<math>S=n(s\\otimes s)^T + s[(s\\otimes n)^T+(n\\otimes s)^T+(n\\otimes n)^T]</math>\n\n==Extensions==\n\n* Vector logic can be extended to include many truth values since large dimensional vector spaces allow to create many orthogonal truth values and the corresponding logical matrices.<ref name=\"miz08\"/>\n* Logical modalities can be fully represented in this context, with recursive process inspired in [[Artificial neuron|neural models]]<ref name=\"miz08\"/><ref>Mizraji, E. (1994) [http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.ndjfl/1094061864&page=record Modalities in vector logic] {{Webarchive|url=https://web.archive.org/web/20140811163306/http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.ndjfl%2F1094061864&page=record |date=2014-08-11 }}. Notre Dame Journal of Formal Logic, 35, 272–283</ref>\n* Some cognitive problems about logical computations can be analyzed using this formalism, in particular recursive decisions. Any logical expression of classical propositional calculus can be naturally represented by a [[tree structure]].<ref name=\"suppes\"/> This fact is retained by vector logic, and has been partially used in neural models focused in the investigation of the branched structure of natural languages.<ref>Mizraji, E., Lin, J. (2002) The dynamics of logical decisions. Physica D, 168–169, 386–396</ref><ref>beim Graben, P., Potthast, R. (2009). Inverse problems in dynamic cognitive modeling. Chaos, 19, 015103</ref><ref>beim Graben, P., Pinotsis, D., Saddy, D., Potthast, R. (2008). Language processing with dynamic fields. Cogn. Neurodyn., 2, 79–88</ref><ref>beim Graben, P., Gerth, S., Vasishth, S.(2008) Towards dynamical system models of language-related brain potentials. Cogn. Neurodyn., 2, 229–255</ref><ref>beim Graben, P., Gerth, S. (2012) Geometric representations for minimalist grammars. Journal of Logic, Language and Information, 21, 393-432 . \n</ref><ref>Binazzi, A.(2012) [http://www.fupress.net/index.php/sf/article/view/11649 Cognizione logica e modelli mentali.] Studi sulla formazione, 1–2012, pag. 69–84</ref>\n* The computation via reversible operations as the [[Fredkin gate]] can be implemented in vector logic. Such an implementation provides explicit expressions for matrix operators that produce the input format and the output filtering necessary for obtaining computations<ref name=\"miz08\"/><ref name=\"miz96\"/>\n* [[Elementary cellular automaton|Elementary cellular automata]] can be analyzed using the operator structure of vector logic; this analysis leads to a spectral decomposition of the laws governing its dynamics<ref>Mizraji, E. (2006) The parts and the whole: inquiring how the interaction of simple subsystems generates complexity. International Journal of General Systems, 35, pp. 395–415.</ref><ref>Arruti, C., Mizraji, E. (2006) Hidden potentialities. International Journal of General Systems, 35, 461–469.</ref> \n* In addition, based on this formalism, a discrete differential and integral calculus has been developed<ref>Mizraji, E. (2015) [http://logcom.oxfordjournals.org/content/25/3/613.full.pdf+html Differential and integral calculus for logical operations. A matrix–vector approach] Journal of Logic and Computation 25, 613-638, 2015</ref>\n\n== See also ==\n* [[Algebraic logic]]\n* [[Boolean algebra]]\n* [[Propositional calculus]]\n* [[Quantum logic]]\n* [[Jonathan Westphal]]\n\n== References ==\n\n{{Reflist}}\n\n[[Category:Logic]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Veitch chart",
      "url": "https://en.wikipedia.org/wiki/Veitch_chart",
      "text": "#REDIRECT [[Karnaugh map#Veitch]]\n\n{{Redirect category shell|1=\n{{R with possibilities}}\n{{R to related topic}}\n}}\n\n[[Category:Boolean algebra]]\n[[Category:Diagrams]]\n[[Category:Electronics optimization]]\n[[Category:Logic in computer science]]"
    },
    {
      "title": "Zhegalkin polynomial",
      "url": "https://en.wikipedia.org/wiki/Zhegalkin_polynomial",
      "text": "{{mergeto|Algebraic normal form|date=April 2019}}\n\n'''Zhegalkin''' (also '''Žegalkin''', '''Gégalkine''' or '''Shegalkin'''<ref name=\"Steinbach_2009\"/>) '''polynomials''' form one of many possible representations of the operations of [[Boolean algebra]].  Introduced by the Russian mathematician [[Ivan Ivanovich Zhegalkin]] in 1927, they are the [[polynomial]]s of interpreted over the integers mod 2.  The resulting degeneracies of [[modular arithmetic]] result in Zhegalkin polynomials being simpler than ordinary polynomials, requiring neither coefficients nor exponents.  Coefficients are redundant because 1 is the only nonzero coefficient.  Exponents are redundant because in arithmetic mod 2, ''x''<sup>2</sup> = ''x''.  Hence a polynomial such as 3''x''<sup>2</sup>''y''<sup>5</sup>''z'' is congruent to, and can therefore be rewritten as, ''xyz''.\n\n__TOC__\n\n==Boolean equivalent==\nPrior to 1927 Boolean algebra had been considered a calculus of [[logical value]]s with logical operations of [[logical conjunction|conjunction]], [[logical disjunction|disjunction]], [[logical negation|negation]], etc.  Zhegalkin showed that all Boolean operations could be written as ordinary numeric polynomials, thinking of the logical constants 0 and 1 as integers mod 2.  The logical operation of conjunction is realized as the arithmetic operation of multiplication ''xy'', and logical [[Exclusive or|exclusive-or]] as arithmetic addition mod 2, (written here as ''x''⊕''y'' to avoid confusion with the common use of + as a synonym for inclusive-or ∨).  Logical complement ¬''x'' is then derived from 1 and ⊕ as ''x''⊕1.  Since ∧ and ¬ form a sufficient basis for the whole of Boolean algebra, meaning that all other logical operations are obtainable as composites of these basic operations, it follows that the polynomials of ordinary algebra can represent all Boolean operations, allowing Boolean reasoning to be performed reliably by appealing to the familiar laws of [[elementary algebra]] without the distraction of the differences from high school algebra that arise with disjunction in place of addition mod 2.\n\nAn example application is the representation of the Boolean 2-out-of-3 threshold or [[median operation]] as the Zhegalkin polynomial ''xy''⊕''yz''⊕''zx'', which is 1 when at least two of the variables are 1 and 0 otherwise.\n\n==Formal properties==\nFormally a ''Zhegalkin monomial'' is the product of a finite set of distinct variables (hence [[Square-free polynomial|square-free]]), including the empty set whose product is denoted 1.  There are 2<sup>''n''</sup> possible Zhegalkin monomials in ''n'' variables, since each monomial is fully specified by the presence or absence of each variable.  A ''Zhegalkin polynomial'' is the sum (exclusive-or) of a set of Zhegalkin monomials, with the empty set denoted by 0.  A given monomial's presence or absence in a polynomial corresponds to that monomial's coefficient being 1 or 0 respectively.  The Zhegalkin monomials, being [[linearly independent]], span a 2<sup>''n''</sup>-dimensional [[vector space]] over the [[Galois field]] '''GF'''(2) (NB: not '''GF'''(2<sup>''n''</sup>), whose multiplication is quite different).  The 2<sup>2<span><sup>''n''</sup></span></sup> vectors of this space, i.e. the linear combinations of those monomials as unit vectors, constitute the Zhegalkin polynomials.  The exact agreement with the number of [[Boolean functions|Boolean operations]] on ''n'' variables, which exhaust the ''n''-ary operations on {0,1}, furnishes a direct counting argument for completeness of the Zhegalkin polynomials as a Boolean basis.\n\nThis vector space is not equivalent to the [[free Boolean algebra]] on ''n'' generators because it lacks complementation (bitwise logical negation) as an operation (equivalently, because it lacks the top element as a constant).  This is not to say that the space is not closed under complementation or lacks top (the [[all-ones vector]]) as an element, but rather that the linear transformations of this and similarly constructed spaces need not preserve complement and top.  Those that do preserve them correspond to the Boolean homomorphisms, e.g. there are four linear transformations from the vector space of Zhegalkin polynomials over one variable to that over none, only two of which are Boolean homomorphisms.\n\n== Method of computation ==\nThere are various known methods generally used for the computation of the Zhegalkin polynomial. \n* Using the method of indeterminate coefficients\n* By constructing the [[canonical disjunctive normal form]]\n* By using tables\n* Pascal method\n* Summation method\n* Using a Karnaugh map\n\n=== The method of indeterminate coefficients ===\nUsing the method of indeterminate coefficients, a linear system consisting of all the tuples of the function and their values is generated. Solving the linear system gives the coefficients of the Zhegalkin polynomial.\n\n==== Example ====\nGiven the Boolean function <math>f(A,B,C) = \\bar A \\bar B \\bar C + \\bar A B \\bar C + A \\bar B \\bar C + A B C</math> it is wanted to express it as a Zhegalkin polynomial. This function can be expressed as a column vector\n:<math>\\vec f = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} </math>.\n\nThis vector should be the output of left-multiplying a vector of undetermined coefficients\n:<math> \\vec c = \\begin{pmatrix} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ c_4 \\\\ c_5 \\\\ c_6 \\\\ c_7 \\end{pmatrix}</math>\nby an 8x8 [[logical matrix]] which represents the possible values that all the possible conjunctions of A, B, C can take. These possible values are given in the following truth table:\n:<math> \\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|} A & B & C & \\;\\;\\;\\;\\;\\;\\;\\; & 1 & C & B & BC & A & AC & AB & ABC \\\\ \\hline 0&0&0&&1&0&0&0&0&0&0&0 \\\\ 0&0&1&&1&1&0&0&0&0&0&0 \\\\ 0&1&0&&1&0&1&0&0&0&0&0 \\\\ 0&1&1&&1&1&1&1&0&0&0&0 \\\\ 1&0&0&&1&0&0&0&1&0&0&0 \\\\ 1&0&1&&1&1&0&0&1&1&0&0 \\\\ 1&1&0&&1&0&1&0&1&0&1&0 \\\\ 1&1&1&&1&1&1&1&1&1&1&1 \\end{array}</math>.\n\nThe information in the above truth table can be encoded in the following logical matrix:\n:<math> S_3 = \\begin{pmatrix} 1&0&0&0&0&0&0&0 \\\\ 1&1&0&0&0&0&0&0 \\\\ 1&0&1&0&0&0&0&0 \\\\ 1&1&1&1&0&0&0&0 \\\\ 1&0&0&0&1&0&0&0 \\\\ 1&1&0&0&1&1&0&0 \\\\ 1&0&1&0&1&0&1&0 \\\\ 1&1&1&1&1&1&1&1 \\end{pmatrix}</math>\nwhere the &lsquo;S&rsquo; here stands for &ldquo;Sierpiński&rdquo;, as in [[Sierpiński triangle]], and the subscript 3 gives the exponents of its size: <math>2^3 \\times 2^3</math>.\n\nIt can be proven through mathematical induction and block-matrix multiplication that any such &ldquo;Sierpiński matrix&rdquo; <math>S_n</math> is its own inverse.<ref group=note>As base case,\n:<math> S_0 := \\begin{pmatrix} 1 \\end{pmatrix}</math>\n:<math> (S_0)^2 = \\begin{pmatrix} 1 \\end{pmatrix} = I_0 </math>\nwhere <math>I_n</math> is here taken to denote the [[identity matrix]] of size <math>2^n \\times 2^n</math>. The inductive assumption is\n:<math> (S_n)^2 = I_n </math>.\nThen the inductive step is:\n:<math> S_{n+1} := \\begin{pmatrix} S_n & O \\\\ S_n & S_n \\end{pmatrix} \\equiv \\begin{pmatrix} 1 & 0 \\\\ 1 & 1\\end{pmatrix} \\otimes S_n</math>,\nwhere <math>\\otimes</math> denotes the [[Kronecker product]],\n:<math> (S_{n+1})^2 = \\begin{pmatrix} S_n & O \\\\ S_n & S_n \\end{pmatrix} \\begin{pmatrix} S_n & O \\\\ S_n & S_n \\end{pmatrix} = \\begin{pmatrix}S_n S_n \\oplus O S_n & S_n O \\oplus O S_n \\\\ S_n S_n \\oplus S_n S_n & S_n O \\oplus S_n S_n \\end{pmatrix} = \\begin{pmatrix} I_n & O \\\\ I_n \\oplus I_n & I_n \\end{pmatrix} = \\begin{pmatrix}I_n & O \\\\ O & I_n\\end{pmatrix} = I_{n+1}</math>,\nor, in terms of the Kronecker product:\n:<math> S_{n+1}^2 = (S_1 \\otimes S_n) (S_1 \\otimes S_n) = S_1^2 \\otimes S_n^2 = I_1 \\otimes I_n = I_{n+1}</math>. &#8718;</ref>\n\nThen the linear system is\n:<math> S_3 \\vec c = \\vec f</math>\nwhich can be solved for <math>\\vec c</math>:\n:<math> \\vec c = S_3^{-1} \\vec f = S_3 \\vec f = \\begin{pmatrix} 1&0&0&0&0&0&0&0 \\\\ 1&1&0&0&0&0&0&0 \\\\ 1&0&1&0&0&0&0&0 \\\\ 1&1&1&1&0&0&0&0 \\\\ 1&0&0&0&1&0&0&0 \\\\ 1&1&0&0&1&1&0&0 \\\\ 1&0&1&0&1&0&1&0 \\\\ 1&1&1&1&1&1&1&1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\oplus 1 \\\\ 1 \\oplus 1 \\\\ 1 \\oplus 1 \\\\ 1 \\oplus 1 \\\\ 1 \\oplus 1 \\oplus 1 \\\\ 1 \\oplus 1 \\oplus 1 \\oplus 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}</math>,\nand the Zhegalkin polynomial corresponding to <math>\\vec c</math> is <math>1 \\oplus C \\oplus AB</math>.\n\n=== Using the canonical disjunctive normal form ===\nUsing this method, the [[canonical disjunctive normal form]] (a fully expanded [[disjunctive normal form]]) is computed first. Then the negations in this expression are replaced by an equivalent expression using the mod 2 sum of the variable and 1. The disjunction signs are changed to addition mod 2, the brackets are opened, and the resulting Boolean expression is simplified. This simplification results in the Zhegalkin polynomial.\n\n=== Using tables ===\n\n[[File:Преобразование таблицы истинности в полином Жегалкина методом треугольника.gif|thumb|Computing the Zhegalkin polynomial for an example function ''P'' by the table method|250px]]\nLet <math>c_0, ... , c_{2^n-1}</math> be the outputs of a truth table for the function ''P'' of ''n'' variables, such that the index of the <math>c_i</math>'s corresponds to the binary indexing of the {{clarify span|minterms|reason=This notion has not been explained or defined before. There is an article (in fact, a redirect) called 'minterm', but I am not sure whether it agrees with the notion used here. Moreover, the 'minterm indexing' used by Zhegalkin may deviate from that explained in the article.|date=April 2019}}. Define a function &zeta; recursively by:\n:<math> \\zeta(c_i) := c_i</math>\n:<math> \\zeta(c_0, ... , c_k) := \\zeta(c_0, ... , c_{k - 1}) \\oplus \\zeta(c_1, ... , c_k) </math>.\nNote that\n:<math> \\zeta(c_0, ... , c_m) = \\bigoplus_{k = 0}^m {m \\choose k}_2 c_k </math>\nwhere <math>{m \\choose k}_2</math> is the [[binomial coefficient]] reduced [[modular arithmetic|modulo]] 2.\n \nThen\n:<math> g_i = \\zeta(c_0, ... , c_i) </math>\nis the ''i''<sup> th</sup> coefficient of a Zhegalkin polynomial whose literals in the ''i''<sup> th</sup> monomial are the same as the literals in the ''i''<sup> th</sup> minterm, except that the negative literals are removed (or replaced by 1).\n\nThe &zeta;-transformation is its own inverse, so the same kind of table can be used to compute the coefficients <math>c_0, ... , c_{2^n-1}</math> given the coefficients <math>g_0, ... , g_{2^n-1}</math>. Just let\n:<math> c_i = \\zeta(g_0, ... , g_i) </math>.\n\nIn terms of the table in the figure, copy the outputs of the truth table (in the column labeled ''P'') into the leftmost column of the triangular table. Then successively compute columns from left to right by applying XOR to each pair of vertically adjacent cells in order to fill the cell immediately to the right of the top cell of each pair. When the entire triangular table is filled in then the top row reads out the coefficients of a linear combination which, when simplified (removing the zeroes), yields the Zhegalkin polynomial.\n\nTo go from a Zhegalkin polynomial to a truth-table, it is possible to fill out the top row of the triangular table with the coefficients of the Zhegalkin polynomial (putting in zeroes for any combinations of positive literals not in the polynomial). Then successively compute rows from top to bottom by applying XOR to each pair of horizontally adjacent cells in order to fill the cell immediately to the bottom of the leftmost cell of each pair. When the entire triangular table is filled then the leftmost column of it can be copied to column ''P'' of the truth table.\n\nAs an aside, note that this method of calculation corresponds to the method of operation of the [[elementary cellular automaton]] called [http://mathworld.wolfram.com/Rule102.html Rule 102]. For example, start such a cellular automaton with eight cells set up with the outputs of the truth table (or the coefficients of the canonical disjunctive normal form) of the Boolean expression: 10101001. Then run the cellular automaton for seven more generations while keeping a record of the state of the leftmost cell. The history of this cell then turns out to be: 11000010, which shows the coefficients of the corresponding Zhegalkin polynomial.\n<ref>В.П. Супрун. Табличный метод полиномиального разложения булевых функций // Кибернетика. — 1987. — № 1. — С. 116-117. <br>''Transliteration'': V.P. Suprun. Tablichnyy metod polinomial'nogo razlozheniya bulevykh funktsiy // Kibernetika. — 1987. — № 1. — S. 116-117. <br>''Translation'': V.P. Suprun The tabular method of polynomial decomposition of Boolean functions // Cybernetics. - 1987. - № 1. - p. 116-117.</ref><ref>В.П. Супрун. Основы теории булевых функций. — М.: Ленанд / URSS. — 2017. - 208 с.<br>''Transliteration'': V.P. Suprun. Osnovy teorii bulevykh funktsiy. — M.: Lenand / URSS. — 2017. - 208 s.<br>''Translation'': V.P. Suprun Fundamentals of the theory of Boolean functions. - M .: Lenand / URSS. - 2017. - 208 p.</ref>\n\n=== The Pascal method ===\n[[File:Построение полинома Жегалкина методом Паскаля.PNG|thumb|Using the Pascal method to compute the Zhegalkin polynomial for the Boolean function <math>\\bar a \\bar b \\bar c + \\bar a b \\bar c + \\bar a b c + a b \\bar c</math>. The line in Russian at the bottom says:<br><math>\\oplus</math> – bitwise operation &ldquo;Exclusive OR&rdquo;|250px]]\nThe most economical in terms of the amount of computation and expedient for constructing the Zhegalkin polynomial manually is the Pascal method.\n\nWe build a table consisting of <math>2^N</math> columns and <math>N + 1</math> rows, where ''N'' is the number of variables in the function. In the top row of the table we place the vector of function values, that is, the last column of the truth table.\n\nEach row of the resulting table is divided into blocks (black lines in the figure). In the first line, the block occupies one cell, in the second line — two, in the third — four, in the fourth — eight, and so on. Each block in a certain line, which we will call “lower block”, always corresponds to exactly two blocks in the previous line. We will call them \"left upper block\" and \"right upper block\".\n\nThe construction starts from the second line. The contents of the left upper blocks are transferred without change into the corresponding cells of the lower block (green arrows in the figure). Then, the operation “addition modulo two” is performed bitwise over the right upper and left upper blocks and the result is transferred to the corresponding cells of the right side of the lower block (red arrows in the figure). This operation is performed with all lines from top to bottom and with all blocks in each line. After the construction is completed, the bottom line contains a string of numbers, which are the coefficients of the Zhegalkin polynomial, written in the same sequence as in the triangle method described above.\n\n=== The summation method ===\n[[File:Коэффициенты полинома Жегалкина.PNG|thumb|Graphic representation of the coefficients of the Zhegalkin polynomial for functions with different numbers of variables.|250px]]\nAccording to the truth table, it is easy to calculate the individual coefficients of the Zhegalkin polynomial. To do this, sum up modulo 2 the values of the function in those rows of the truth table where variables that are not in the conjunction (that corresponds to the coefficient being calculated) take zero values.\n\nSuppose, for example, that we need to find the coefficient of the ''xz'' conjunction for the function of three variables <math>f(x, y, z)</math>. There is no variable ''y'' in this conjunction. Find the input sets in which the variable ''y'' takes a zero value. These are the sets 0, 1, 4, 5 (000, 001, 100, 101). Then the coefficient at conjunction ''xz'' is\n\n:<math>a_5 = f_0 \\oplus f_1 \\oplus f_4 \\oplus f_5 = f(0,0,0) \\oplus f(0,0,1) \\oplus f(1,0,0) \\oplus f(1,0,1) </math>\n\nSince there are no variables with the constant term,\n\n:<math>a_0 = f_0</math>.\n\nFor a term which includes all variables, the sum includes all values of the function:\n\n:<math>a_{N - 1} = f_0 \\oplus f_1 \\oplus f_2 \\oplus ... \\oplus f_{N-2} \\oplus f_{N-1} </math>\n\nLet us graphically represent the coefficients of the Zhegalkin polynomial as sums modulo 2 of values of functions at certain points. To do this, we construct a square table, where each column represents the value of the function at one of the points, and the row is the coefficient of the Zhegalkin polynomial. The point at the intersection of some column and row means that the value of the function at this point is included in the sum for the given coefficient of the polynomial (see figure). We call this table <math>T_N</math>, where ''N'' is the number of variables of the function. \n\nThere is a pattern that allows you to get a table for a function of ''N'' variables, having a table for a function of <math>N-1</math> variables. The new table <math>T_N + 1</math> is arranged as a 2 × 2 matrix of <math>T_N</math> tables, and the right upper block of the matrix is cleared.\n\n==== Lattice-theoretic interpretation ====\nConsider the columns of a table <math>T_N</math> as corresponding to elements of a [[Boolean lattice]] of size <math>2^N</math>. For each column <math>f_M</math> express number ''M'' as a binary number <math>M_2</math>, then <math>f_M \\le f_K</math> if and only if <math>M_2 \\vee K_2 = K_2</math>, where <math>\\vee</math> denotes bitwise OR.\n\nIf the rows of table <math>T_N</math> are numbered, from top to bottom, with the numbers from 0 to <math>2^N - 1</math>, then the tabular content of row number ''R'' is the [[Ideal (order theory)|ideal]] generated by element <math>f_R</math> of the lattice.\n\nNote incidentally that the overall pattern of a table <math>T_N</math> is that of a [[logical matrix]] [[Sierpiński triangle]]. Also, the pattern corresponds to an [[elementary cellular automaton]] called [http://mathworld.wolfram.com/Rule60.html Rule 60], starting with the leftmost cell set to 1 and all other cells cleared.\n\n=== Using a Karnaugh map ===\n[[File:Преобразование карты Карно в полином Жегалкина.gif|thumb|Converting a Karnaugh map to a Zhegalkin polynomial.|250px]]\nThe figure shows a function of three variables, ''P''(''A'', ''B'', ''C'') represented as a [[Karnaugh map]], which the reader may consider as an example of how to convert such maps into Zhegalkin polynomials; the general procedure is given in the following steps:\n* We consider all the cells of the Karnaugh map in ascending order of the number of units in their codes. For the function of three variables, the sequence of cells will be 000–100–010–001–110–101–011–111. Each cell of the Karnaugh map is associated with a member of the Zhegalkin polynomial depending on the positions of the code in which there are ones. For example, cell 111 corresponds to the member ABC, cell 101 corresponds to the member AC, cell 010 corresponds to the member B, and cell 000 corresponds to member 1.\n* If the cell in question is 0, go to the next cell.\n* If the cell in question is 1, add the corresponding term to the Zhegalkin polynomial, then invert all cells in the Karnaugh map where this term is 1 (or belonging to the [[ideal (order theory)|ideal]] generated by this term, in a Boolean lattice of monomials) and go to the next cell. For example, if, when examining cell 110, a one appears in it, then the term AB is added to the Zhegalkin polynomial and all cells of the Karnaugh map are inverted, for which A = 1 and B = 1. If unit is in cell 000, then a term 1 is added to the Zhegalkin polynomial and the entire Karnaugh map is inverted.\n* The transformation process can be considered complete when, after the next inversion, all cells of the Karnaugh map become zero.\n\n==Related work==\nIn the same year as Zhegalkin's paper (1927) the American mathematician [[Eric Temple Bell]] published a sophisticated arithmetization of Boolean algebra based on [[Richard Dedekind]]'s ideal theory and general modular arithmetic (as opposed to arithmetic mod 2).  The much simpler arithmetic character of Zhegalkin polynomials was first noticed in the west (independently, communication between Soviet and Western mathematicians being very limited in that era) by the American mathematician [[Marshall Stone]] in 1936 when he observed while writing up his celebrated [[Stone duality]] theorem that the supposedly loose analogy between [[Boolean algebras]] and [[Ring (mathematics)|rings]] could in fact be formulated as an exact equivalence holding for both finite and infinite algebras, leading him to substantially reorganize his paper.\n\n==See also==\n* [[Algebraic normal form]] (ANF)\n* [[Ring sum normal form]] (RSNF)\n* [[Reed-Muller expansion]]\n* [[Boolean domain]]\n* [[Boolean-valued function]]\n\n==Notes==\n{{reflist|group=note}}\n==References==\n{{Reflist|refs=\n<ref name=\"Steinbach_2009\">{{cite book |author-first1=Bernd |author-last1=Steinbach |author-link1=:de:Bernd Steinbach |author-first2=Christian |author-last2=Posthoff |title=Logic Functions and Equations - Examples and Exercises |chapter=Preface |publisher=[[Springer Science + Business Media B. V.]] |publication-place=Dordrecht, Netherlands |location=Freiberg, Germany |date=2009 |edition=1st |isbn=978-1-4020-9594-8 |lccn=2008941076 |page=xv}}</ref>\n}}\n* {{cite journal |author-last=Bell |author-first=Eric Temple |author-link=Eric Temple Bell |title=Arithmetic of Logic |journal=Transactions of the American Mathematical Society |volume=29 |pages=597–611 |date=1927 |doi=10.2307/1989098 |number=3 |publisher=[[Transactions of the American Mathematical Society]] |jstor=1989098}}\n* {{cite book |author-last=Gindikin |author-first=S. G. |trans-title=Algebraic Logic |title=алгебра логики в задачах |language=Russian |publisher=[[Nauka (publisher)|Наука [Nauka]]] (English translation: [[Springer-Verlag]] 1985) |date=1972 |location=Moscow, Russia |isbn=0-387-96179-8}}\n* {{cite journal |author-last=Stone |author-first=Marshall |author-link=Marshall Harvey Stone |title=The Theory of Representations for Boolean Algebras |journal=[[Transactions of the American Mathematical Society]] |volume=40 |pages=37–111 |date=1936 |issn=0002-9947 |doi=10.2307/1989664 |number=1 |jstor=1989664}}\n* {{cite journal |author-last=Жега́лкин [Zhegalkin] |author-first=Ива́н Ива́нович [Ivan Ivanovich] |author-link=Ivan Ivanovich Zhegalkin |title=О технике вычислений предложений в символической логике |language=Russian, French |trans-title=On the technique of calculating propositions in symbolic logic (Sur le calcul des propositions dans la logique symbolique) |journal=[[Matematicheskii Sbornik]] |location=Moscow, Russia |volume=34 |issue=1 |pages=9–28 |date=1927 |id={{mathnet|msb7433}} |url=http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=sm&paperid=7433&option_lang=eng |access-date=2017-10-12 |dead-url=no |archive-url=https://web.archive.org/web/20171012193119/http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=sm&paperid=7433&option_lang=eng |archive-date=2017-10-12}}\n\n[[Category:Boolean algebra]]\n[[Category:Logic]]"
    },
    {
      "title": "Conditioned disjunction",
      "url": "https://en.wikipedia.org/wiki/Conditioned_disjunction",
      "text": "{{Infobox logical connective\n| title        = Conditioned disjunction\n| other titles = \n| Venn diagram = Venn 0100 0111.svg\n| definition   = <math>(q \\rightarrow p) \\land (\\neg q \\rightarrow r)</math>\n| truth table  = <math>(0100 0111)</math>\n| logic gate   = \n| DNF          = <math>\\overline p \\overline q r + p \\overline q r + pq\\overline r + pqr</math>\n| CNF          = <math>(\\overline q + p) (q + r)</math>\n| Zhegalkin    = <math>p \\oplus qr \\oplus r</math>\n| 0-preserving = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| 1-preserving = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| monotone     = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| affine       = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| self-dual    = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n}}\n\nIn logic, '''conditioned disjunction''' (sometimes called '''conditional disjunction''') is a [[Ternary operation|ternary]] [[logical connective]] introduced by [[Alonzo Church|Church]].<ref>{{cite book |title=Introduction to Mathematical Logic |last=Church |first=Alonzo |authorlink=Alonzo Church |year=1956 |publisher=Princeton University Press}}</ref> Given operands ''p'', ''q'', and ''r'', which represent [[truth-value]]d [[proposition]]s, the meaning of the conditioned disjunction {{nowrap|[''p'', ''q'', ''r'']}} is given by:\n:<math>[p, q, r] ~\\leftrightarrow~(q \\rightarrow p) \\land (\\neg q \\rightarrow r)</math>\nIn words, {{nowrap|[''p'', ''q'', ''r'']}} is equivalent to: \"if ''q'' then ''p'', else ''r''\", or \"''p'' or ''r'', according as ''q'' or not ''q''\". This may also be stated as \"''q'' implies ''p'', and not ''q'' implies ''r''\". So, for any values of ''p'', ''q'', and ''r'', the value of {{nowrap|[''p'', ''q'', ''r'']}} is the value of ''p'' when ''q'' is true, and is the value of ''r'' otherwise.\n\nThe conditioned disjunction is also equivalent to:\n:<math>(q \\land p) \\lor (\\neg q \\land r)</math>\nand has the same truth table as the \"ternary\" ([[?:]]) operator in many programming languages. In electronic logic terms, it may also be viewed as a single-bit [[multiplexer]].\n\nIn conjunction with truth constants denoting each truth-value, conditioned disjunction is [[truth-functionally complete]] for [[classical logic]].<ref>Wesselkamper, T., \"A sole sufficient operator\", ''Notre Dame Journal of Formal Logic'', Vol. XVI, No. 1 (1975), pp. 86-88.</ref> Its [[truth table]] is the following:\n\n{| align=\"center\" border=\"1\" cellpadding=\"8\" cellspacing=\"0\" style=\"background:#f9f9f9; font-weight:bold; text-align:center; width:45%\"\n|+ '''Conditioned disjunction'''\n|- style=\"background:#efefef\"\n! style=\"width:15%\" | p\n! style=\"width:15%\" | q\n! style=\"width:15%\" | r\n! style=\"width:15%\" | [p,q,r]\n|-\n| T || T || T || T\n|-\n| T || T || F || T\n|-\n| T || F || T || T\n|-\n| T || F || F || F\n|-\n| F || T || T || F\n|-\n| F || T || F || F\n|-\n| F || F || T || T\n|-\n| F || F || F || F\n|}\n\nThere are other truth-functionally complete  ternary connectives.\n\n==References==\n{{reflist}}\n\n[[Category:Logical connectives]]\n[[Category:Ternary operations]]\n\n{{logic-stub}}"
    },
    {
      "title": "Converse implication",
      "url": "https://en.wikipedia.org/wiki/Converse_implication",
      "text": "[[File:Venn1101.svg|220px|thumb|[[Venn diagram]] of <math>A \\leftarrow B</math> <br> (the white area shows where the statement is false)]]\n\n'''Converse implication''' is the [[conversion (logic)|converse]] of [[Material implication (rule of inference)|implication]], written ←. That is to say; that for any two [[proposition]]s <math>P</math> and <math>Q</math>, if <math>Q</math> implies <math>P</math>, then <math>P</math> is the converse implication of <math>Q</math>.\n\nIt is written <math>P \\leftarrow Q</math>, but may also be notated <math>P \\subset Q</math>, or \"B''pq''\" (in [[Józef_Maria_Bocheński|Bocheński notation]]).\n\n==Definition==\n\n===Truth table===\nThe [[truth table]] of <math> P \\leftarrow Q </math>\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math> P </math> || <math> Q </math> || <math> P \\leftarrow Q </math>\n|-\n| T || T || T\n|-\n| T || F || T\n|-\n| F || T || F\n|-\n| F || F || T\n|}\n\n===Logical Equivalences===\n\nConverse implication is logically equivalent to the disjunction of <math>P</math> and <math>\\neg Q</math>\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n| <math>P \\leftarrow Q</math>\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>P</math>\n| <math>\\or</math>\n| <math>\\neg Q</math>\n|-\n| [[File:Venn1101.svg|50px]]\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| [[File:Venn0101.svg|50px]]\n| <math>\\or</math> \n| [[File:Venn1100.svg|50px]]\n|}\n\n==Properties==\n\n'''truth-preserving''': The interpretation under which all variables are assigned a [[truth value]] of 'true' produces a truth value of 'true' as a result of converse implication.\n\n==Symbol==\n{{Empty section|date=July 2010}}\n&larr;, &lArr; \n\n==Natural language==\n\n\"Not q without p.\"\n\n\"p if q.\"\n\n== See also ==\n*[[Logical connective]]\n*[[Material conditional]]\n\n{{Logical connectives}}\n\n[[Category:Logical connectives]]\n\n{{logic-stub}}"
    },
    {
      "title": "Converse nonimplication",
      "url": "https://en.wikipedia.org/wiki/Converse_nonimplication",
      "text": "[[File:Venn0010.svg|thumb|240px|[[Venn diagram]] of <math>P \\nleftarrow Q</math><br>(the red area is true)]]\n\nIn [[logic]], '''converse nonimplication'''<ref>Lehtonen, Eero, and Poikonen, J.H.</ref> is a [[logical connective]] which is the [[negation]] of [[converse implication]] (equivalently, the [[negation]] of the [[conversion (logic)|converse]] of [[material conditional|implication]]).\n\n==Definition==\nConverse nonimplication is notated <math>P \\nleftarrow Q</math>, or <math>P \\not \\subset Q</math>, and is logically equivalent to <math>\\neg (P \\leftarrow Q)</math>\n\n===Truth table===\nThe [[truth table]] of <math> P \\nleftarrow Q </math>.<ref name=\"Knuth\">{{harvnb|Knuth|2011|p=49}}</ref>\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math> P </math> || <math> Q </math> || <math> P \\nleftarrow Q </math>\n|-\n| T || T || F\n|-\n| T || F || F\n|-\n| F || T || T\n|-\n| F || F || F\n|}\n\n==Notation==\nConverse nonimplication is notated <math>\\textstyle{p \\nleftarrow q}</math>, which is the left arrow from [[converse implication]] (<math>\\textstyle{\\leftarrow}</math>), negated with a stroke ({{math|size=100%|/}}).\n\nAlternatives include\n* <math>\\textstyle{p \\not\\subset q}</math>, which combines [[Converse implication|converse implication's]] <math>\\subset</math>, negated with a stroke ({{math|size=100%|/}}).\n* <math>\\textstyle{p\\tilde{\\leftarrow}q}</math>, which combines [[Converse implication|converse implication's]] left arrow(<math>\\textstyle{\\leftarrow}</math>) with [[Negation|negation's]] tilde(<math>\\textstyle{\\sim}</math>).\n* M''pq'', in [[Józef_Maria_Bocheński|Bocheński notation]]\n\n==Properties==\n\n'''falsehood-preserving''': The interpretation under which all variables are assigned a [[truth value]] of 'false' produces a truth value of 'false' as a result of converse nonimplication\n\n==Natural language==\n===Grammatical===\n{{Empty section|date=February 2011}}\nClassic passive aggressive: \"yeah, no\"\n\n===Rhetorical===\n\"not A but B\"\n\n===Colloquial===\n{{Empty section|date=February 2011}}\n\n==Boolean algebra==\n<div id=\"Definition\">\nConverse Nonimplication in a general [[Boolean algebra (structure)|Boolean algebra]] is defined as <math display=\"inline\">{q \\nleftarrow p=q'p}\\!</math>.\n\n<div id=\"TwoElements\">\nExample of a 2-element Boolean algebra: the 2 elements {0,1} with 0 as zero and 1 as unity element, operators <math display=\"inline\">\\sim</math> as complement operator, <math display=\"inline\">\\vee</math> as join operator and <math display=\"inline\">\\wedge</math> as meet operator, build the Boolean algebra of [[propositional logic]].\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;\"\n| <math display=\"inline\">{}\\sim x</math>\n| style=\"background-color:#DDFFDD\"| {{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"| {{math|size=100%|0}}\n|-\n| {{mvar|size=100%|x}}\n! {{math|size=100%|0}}\n! {{math|size=100%|1}}\n|}\n| style=\"border:none;\" |and\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|{{mvar|size=100%|y}}\n|style=\"border:none;\" |\n|-\n!{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n|-\n!{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n|-\n| style=\"text-align:center;\" |<math display=\"inline\">y_\\vee x</math>\n!{{math|size=100%|0}}\n!{{math|size=100%|1}}\n|{{mvar|size=100%|x}}\n|}\n| style=\"border:none;\" |and\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|{{mvar|size=100%|y}}\n|style=\"border:none;\" |\n|-\n!{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n|-\n!{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|0}}\n|-\n| style=\"text-align:center;\" |<math display=\"inline\">y_\\wedge x</math>\n!{{math|size=100%|0}}\n!{{math|size=100%|1}}\n|{{mvar|size=100%|x}}\n|}\n| style=\"border:none;\" |then <math>\\scriptstyle{y \\nleftarrow x}\\!</math> means\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|{{mvar|size=100%|y}}\n|style=\"border:none;\" |\n|-\n!{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|0}}\n|-\n!{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|0}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n|-\n| style=\"text-align:center;\" |<math>\\scriptstyle{y \\nleftarrow x}\\!</math>\n!{{math|size=100%|0}}\n!{{math|size=100%|1}}\n|{{mvar|size=100%|x}}\n|}\n|-\n| style=\"border:none;\" |''(Negation)''\n| style=\"border:none;\" |\n| style=\"border:none;\" |''(Inclusive or)''\n| style=\"border:none;\" |\n| style=\"border:none;\" |''(And)''\n| style=\"border:none;\" |\n| style=\"border:none;\" |''(Converse nonimplication)''\n|}\n</div>\n<div id=\"DivisorsOfSix\">\n{{anchor|s4}}\nExample of a 4-element Boolean algebra: the 4 divisors {1,2,3,6} of 6 with 1 as zero and 6 as unity element, operators <math>\\scriptstyle{ ^{c}}\\!</math> (codivisor of 6) as complement operator, <math>\\scriptstyle{_\\vee}\\!</math> (least common multiple) as join operator and <math>\\scriptstyle{_\\wedge}\\!</math> (greatest common divisor) as meet operator, build a Boolean algebra.\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;\"\n| <math>\\scriptstyle{x^c}\\!</math>\n| style=\"background-color:#DDFFDD\"| {{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"| {{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"| {{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"| {{math|size=100%|1}}\n|-\n| {{mvar|size=100%|x}}\n! {{math|size=100%|1}}\n! {{math|size=100%|2}}\n! {{math|size=100%|3}}\n! {{math|size=100%|6}}\n|}\n| style=\"border:none;\" |and\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|{{mvar|size=100%|y}}\n|style=\"border:none;\" |\n|-\n!{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n|-\n!{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n|-\n!{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n|-\n!{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n|-\n| style=\"text-align:center;\" |<math>\\scriptstyle{y_\\vee x}\\!</math>\n!{{math|size=100%|1}}\n!{{math|size=100%|2}}\n!{{math|size=100%|3}}\n!{{math|size=100%|6}}\n|{{mvar|size=100%|x}}\n|}\n| style=\"border:none;\" |and\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|{{mvar|size=100%|y}}\n|style=\"border:none;\" |\n|-\n!{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n|-\n!{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n|-\n!{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n|-\n!{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n|-\n| style=\"text-align:center;\" |<math>\\scriptstyle{y_\\wedge x}\\!</math>\n!{{math|size=100%|1}}\n!{{math|size=100%|2}}\n!{{math|size=100%|3}}\n!{{math|size=100%|6}}\n|{{mvar|size=100%|x}}\n|}\n| style=\"border:none;\" |then <math>\\scriptstyle{y \\nleftarrow x}\\!</math> means\n|style=\"border:none;\" |\n{| class=\"wikitable\" style=\"border:none; background:transparent;text-align:center;\"\n|{{mvar|size=100%|y}}\n|style=\"border:none;\" |\n|-\n!{{math|size=100%|6}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n|-\n!{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n|-\n!{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n|-\n!{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|1}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|2}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|3}}\n| style=\"background-color:#DDFFDD\"|{{math|size=100%|6}}\n|-\n| style=\"text-align:center;\" |<math>\\scriptstyle{y \\nleftarrow x}\\!</math>\n!{{math|size=100%|1}}\n!{{math|size=100%|2}}\n!{{math|size=100%|3}}\n!{{math|size=100%|6}}\n|{{mvar|size=100%|x}}\n|}\n|-\n| style=\"border:none;\" |''(Codivisor 6)''\n| style=\"border:none;\" |\n| style=\"border:none;\" |''(Least common multiple)''\n| style=\"border:none;\" |\n| style=\"border:none;\" |''(Greatest common divisor)''\n| style=\"border:none;\" |\n| style=\"border:none;\" |''(x's greatest divisor [[coprime]] with y)''\n|}\n</div>\n\n===Properties===\n====Non-associative====\n<math>\\scriptstyle{r \\nleftarrow (q \\nleftarrow p)=(r \\nleftarrow q) \\nleftarrow p}</math> iff <math>\\scriptstyle{rp=0}</math> [[#NonAssociative|#s5]] (In a [[two-element Boolean algebra]] the latter condition is reduced to <math>\\scriptstyle{r=0}</math> or <math>\\scriptstyle{p=0}</math>). Hence in a nontrivial Boolean algebra Converse Nonimplication is '''nonassociative'''.\n\n::<math>\n\\begin{align}\n(r \\nleftarrow q) \\nleftarrow p &= r'q \\nleftarrow p & \\text{(by definition)} \\\\\n&= (r'q)'p & \\text{(by definition)} \\\\\n&= (r + q')p & \\text{(De Morgan's laws)} \\\\\n&= (r + r'q')p & \\text{(Absorption law)} \\\\\n&= rp + r'q'p \\\\\n&= rp + r'(q \\nleftarrow p) & \\text{(by definition)} \\\\\n&= rp + r \\nleftarrow (q \\nleftarrow p) & \\text{(by definition)} \\\\\n\\end{align}\n</math>\n\nClearly, it is associative iff <math>\\scriptstyle{rp=0}</math>.\n\n====Non-commutative====\n\n* <math>\\scriptstyle{q \\nleftarrow p=p \\nleftarrow q\\,}\\!</math> iff <math>\\scriptstyle{q=p\\,}\\!</math> [[#NonCommutative|#s6]]. Hence Converse Nonimplication is '''noncommutative'''.\n\n====Neutral and absorbing elements====\n\n* {{math|size=100%|0}} is a left [[neutral element]] (<math>\\scriptstyle{0 \\nleftarrow p=p}\\!</math>) and a right [[absorbing element]] (<math>\\scriptstyle{p \\nleftarrow 0=0}\\!</math>).\n* <math>\\scriptstyle{1 \\nleftarrow p=0}\\!</math>, <math>\\scriptstyle{p \\nleftarrow 1=p'}\\!</math>, and <math>\\scriptstyle{p \\nleftarrow p=0}\\!</math>.\n* Implication <math>\\scriptstyle{q \\rightarrow p}\\!</math> is the dual of converse nonimplication <math>\\scriptstyle{q \\nleftarrow p}\\!</math> [[#Dual|#s7]].\n</div>\n\n<div id=\"NonCommutative\">\n{{anchor|s6}}\n{| style=\"background-color:white;\"\n!colspan=\"5\"| Converse Nonimplication is noncommutative\n|-\n! Step\n! Make use of\n! colspan=\"3\"|Resulting in\n|-\n| <math>\\scriptstyle\\mathrm{s.1 }</math>\n| [[#Definition|Definition]]\n|colspan=\"3\"|<math>\\scriptstyle{q\\tilde{\\leftarrow}p=q'p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.2 }</math>\n| [[#Definition|Definition]]\n|colspan=\"3\"|<math>\\scriptstyle{p\\tilde{\\leftarrow}q=p'q\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.3 }</math>\n| <math>\\scriptstyle\\mathrm{s.1\\ s.2 }</math>\n|colspan=\"3\"|<math>\\scriptstyle{q\\tilde{\\leftarrow}p=p\\tilde{\\leftarrow}q\\ \\Leftrightarrow\\ q'p=qp'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.4 }</math>\n|\n|<math>\\scriptstyle{q\\,}\\!</math>\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{q.1\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.5 }</math>\n| <math>\\scriptstyle\\mathrm{s.4.right}</math> - expand Unit element\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{q.(p+p')\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.6 }</math>\n| <math>\\scriptstyle\\mathrm{s.5.right}</math> - evaluate expression\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{qp+qp'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.7 }</math>\n| <math>\\scriptstyle\\mathrm{s.4.left=s.6.right }</math>\n|colspan=\"3\"|<math>\\scriptstyle{q=qp+qp'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.8 }</math>\n|\n|<math>\\scriptstyle{q'p=qp'\\,}\\!</math>\n|<math>\\scriptstyle{\\Rightarrow\\,}\\!</math>\n|<math>\\scriptstyle{qp+qp'=qp+q'p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.9 }</math>\n| <math>\\scriptstyle\\mathrm{s.8 }</math> - regroup common factors\n|\n|<math>\\scriptstyle{\\Rightarrow\\,}\\!</math>\n|<math>\\scriptstyle{q.(p+p')=(q+q').p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.10 }</math>\n| <math>\\scriptstyle\\mathrm{s.9 }</math> - join of complements equals unity\n|\n|<math>\\scriptstyle{\\Rightarrow\\,}\\!</math>\n|<math>\\scriptstyle{q.1=1.p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.11 }</math>\n| <math>\\scriptstyle\\mathrm{s.10.right }</math> - evaluate expression\n|\n|<math>\\scriptstyle{\\Rightarrow\\,}\\!</math>\n|<math>\\scriptstyle{q=p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.12 }</math>\n| <math>\\scriptstyle\\mathrm{s.8\\ s.11}</math>\n|colspan=\"3\"|<math>\\scriptstyle{q'p=qp'\\ \\Rightarrow\\ q=p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.13 }</math>\n|\n|colspan=\"3\"|<math>\\scriptstyle{q=p\\ \\Rightarrow\\ q'p=qp'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.14 }</math>\n|<math>\\scriptstyle\\mathrm{s.12\\ s.13 }</math>\n|colspan=\"3\"|<math>\\scriptstyle{q=p\\ \\Leftrightarrow\\ q'p=qp'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.15 }</math>\n| <math>\\scriptstyle\\mathrm{s.3\\ s.14 }</math>\n|colspan=\"3\"|<math>\\scriptstyle{q\\tilde{\\leftarrow}p=p\\tilde{\\leftarrow}q\\ \\Leftrightarrow\\ q=p\\,}\\!</math>\n|-\n|}\n</div>\n<div id=\"Dual\">\n{{anchor|s7}}\n{| style=\"background-color:white;\"\n!colspan=\"5\"| Implication is the dual of Converse Nonimplication\n|-\n! Step\n! Make use of\n! colspan=\"3\"|Resulting in\n|-\n| <math>\\scriptstyle\\mathrm{s.1 }</math>\n| [[#Definition|Definition]]\n|<math>\\scriptstyle{\\operatorname{dual}(q\\tilde{\\leftarrow}p)\\,}\\!</math>\n|<math>\\scriptstyle{=\\,}\\!</math>\n|<math>\\scriptstyle{\\operatorname{dual}(q'p)\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.2 }</math>\n|<math>\\scriptstyle\\mathrm{s.1.right}</math> - .'s [[Duality (mathematics)|dual]] is +\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{q'+p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.3 }</math>\n| <math>\\scriptstyle\\mathrm{s.2.right}</math> - [[Involution (mathematics)|Involution]] complement\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{(q'+p)''\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.4 }</math>\n| <math>\\scriptstyle\\mathrm{s.3.right}</math> - [[De Morgan's laws]] applied once\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{(qp')'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.5 }</math>\n| <math>\\scriptstyle\\mathrm{s.4.right}</math> - [[Commutativity| Commutative law]]\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{(p'q)'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.6 }</math>\n| <math>\\scriptstyle\\mathrm{s.5.right}</math>\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{(p\\tilde{\\leftarrow}q)'\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.7 }</math>\n| <math>\\scriptstyle\\mathrm{s.6.right}</math>\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{p\\leftarrow q\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.8 }</math>\n| <math>\\scriptstyle\\mathrm{s.7.right}</math>\n|\n| <math>\\scriptstyle{=\\,}\\!</math>\n| <math>\\scriptstyle{q\\rightarrow p\\,}\\!</math>\n|-\n| <math>\\scriptstyle\\mathrm{s.9 }</math>\n| <math>\\scriptstyle\\mathrm{s.1.left=s.8.right }</math>\n|colspan=\"3\"|<math>\\scriptstyle{\\operatorname{dual}(q\\tilde{\\leftarrow}p)=q\\rightarrow p\\,}\\!</math>\n|-\n|}\n</div>\n\n==Computer science==\nAn example for converse nonimplication in computer science can be found when performing a [[Join (SQL)#Right outer join|right outer join]] on a set of tables from a [[database]], if records not matching the join-condition from the \"left\" table are being excluded.<ref>http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html</ref>\n\n==References==\n{{Reflist}}\n*{{cite book|last=Knuth|first=Donald E.|authorlink=Donald Knuth|year=2011|title=[[The Art of Computer Programming]], Volume 4A: Combinatorial Algorithms, Part 1|edition=1st|publisher=Addison-Wesley Professional|isbn=0-201-03804-8|ref=harv}}\n\n{{Logical connectives}}\n\n{{DEFAULTSORT:Converse Nonimplication}}\n[[Category:Logical connectives]]"
    },
    {
      "title": "Exclusive or",
      "url": "https://en.wikipedia.org/wiki/Exclusive_or",
      "text": "{{short description|True when either but not both inputs are true}}\n{{pp|small=yes}} \n{{Redirect|XOR|the logic gate|XOR gate|other uses|XOR (disambiguation)}}\n{{refimprove|date=May 2013}}\n\n{{Infobox logical connective\n| title        = Exclusive or\n| other titles = XOR\n| Venn diagram = Venn0110.svg\n| definition   = \n| truth table  = <math>(0110)</math>\n| logic gate   = XOR_ANSI.svg\n| DNF          = <math>\\overline{x} \\cdot y + x \\cdot \\overline{y}</math>\n| CNF          = <math>( \\overline{x} + \\overline{y} ) \\cdot ( x + y )</math>\n| Zhegalkin    = <math>x \\oplus y </math>\n| 0-preserving = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| 1-preserving = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| monotone     = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| affine       = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| self-dual    = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n}}\n\n[[File:Venn 0110 1001.svg|220px|[[Venn diagram]] of <math>\\scriptstyle A \\oplus B \\oplus C</math>|thumb|right]]\n'''Exclusive or''' or '''exclusive disjunction''' is a [[Logical connective|logical operation]] that outputs true only when inputs differ (one is true, the other is false).<ref name=wolfram>{{cite web|last1=Germundsson|first1=Roger|last2=Weisstein|first2=Eric|title=XOR|url=http://mathworld.wolfram.com/XOR.html|website=[[MathWorld]]|publisher=[[Wolfram Research]]|accessdate=17 June 2015}}</ref>\n\nIt is [[Table of logic symbols|symbolized]] by the prefix operator '''J'''<ref>{{citation|title=Routledge Encyclopedia of Philosophy|volume=10|editor-first=Edward|editor-last=Craig|publisher=Taylor & Francis|year=1998|isbn=9780415073103|page=496|url=https://books.google.com/books?id=HP9O6OM4iOgC&pg=PA496}}</ref> and by the [[infix]] operators '''XOR''' ({{IPAc-en|ˌ|ɛ|k|s|_|ˈ|ɔr}} or {{IPAc-en|ˈ|z|ɔɹ}}), '''EOR''', '''EXOR''', '''<span style=\"font-size:120%;\">⊻</span>''', '''<span style=\"font-size:120%;\">⩒</span>''', '''<span style=\"font-size:120%;\">⩛</span>''', '''<span style=\"font-size:100%;\">⊕</span>''', <span style=\"font-size:150%;\">↮</span>, and <span style=\"font-size:150%;\">'''&#8802;'''</span>. The [[negation]] of XOR is [[logical biconditional]], which outputs true only when both inputs are the same.\n\nIt gains the name \"exclusive or\" because the meaning of \"or\" is ambiguous when both [[operand]]s are true; the exclusive or operator ''excludes'' that case. This is sometimes thought of as \"one or the other but not both\". This could be written as \"A or B, but not, A and B\".\n\nMore generally, XOR is true only when an odd number of inputs are true. A chain of XORs—''a'' XOR ''b'' XOR ''c'' XOR ''d'' (and so on)—is true whenever an odd number of the inputs are true and is false whenever an even number of inputs are true.\n\n==Truth table==\n[[File:Multigrade operator XOR.svg|thumb|220px|Arguments on the left combined by XOR. This is a binary [[Walsh matrix]] (cf. [[Hadamard code]]).]]\nThe [[truth table]] of A XOR B shows that it outputs true whenever the inputs differ:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ XOR truth table\n|-\n!colspan=\"2\" | Input || rowspan=\"2\" | Output\n|-\n!A || B\n|-\n| 0 || 0 || 0\n|-\n| 0 || 1 || 1\n|-\n| 1 || 0 || 1\n|-\n| 1 || 1 || 0\n|}\n\n* 0, false\n* 1, true\n\n==Equivalences, elimination, and introduction==\nExclusive disjunction essentially means 'either one, but not both nor none'. In other words, the statement is true [[if and only if]] one is true and the other is false. For example, if two horses are racing, then one of the two will win the race, but not both of them. The exclusive disjunction <math>p \\oplus q</math>, also denoted by <math>p</math> <span style=\"font-size:120%;\">'''⩛'''</span> <math>q</math> or <math>\\operatorname{J}pq</math>, can be expressed in terms of the [[logical conjunction]] (\"logical and\", <math>\\wedge</math>), the [[disjunction]] (\"logical or\", <math>\\lor</math>), and the [[negation]] (<math>\\lnot</math>) as follows:\n: <math>\\begin{matrix}\n  p \\oplus q & = & (p \\lor q) \\land  \\lnot (p \\land q)\n\\end{matrix}</math>\n\nThe exclusive disjunction <math>p \\oplus q</math> can also be expressed in the following way:\n: <math>\\begin{matrix}\n  p \\oplus q & = & (p \\land \\lnot q) \\lor (\\lnot p \\land q)\n\\end{matrix}</math>\n\nThis representation of XOR may be found useful when constructing a circuit or network, because it has only one <math>\\lnot</math> operation and small number of <math>\\wedge</math> and <math>\\lor</math> operations. A proof of this identity is given below:\n: <math>\\begin{matrix}\n  p \\oplus q & = & (p \\land \\lnot q) & \\lor & (\\lnot p \\land q) \\\\[3pt]\n             & = & ((p \\land \\lnot q) \\lor \\lnot p) & \\land & ((p \\land \\lnot q) \\lor q) \\\\[3pt]\n             & = & ((p \\lor \\lnot p) \\land (\\lnot q \\lor \\lnot p)) & \\land & ((p \\lor q) \\land (\\lnot q \\lor q)) \\\\[3pt]\n             & = & (\\lnot p \\lor \\lnot q) & \\land & (p \\lor q) \\\\[3pt]\n             & = & \\lnot (p \\land q) & \\land & (p \\lor q)\n\\end{matrix}</math>\n\nIt is sometimes useful to write <math>p \\oplus q</math> in the following way:\n: <math>\\begin{matrix}\n  p \\oplus q & = & \\lnot ((p \\land q) \\lor (\\lnot p \\land \\lnot q))\n\\end{matrix}</math>\nor:\n: <math>\\begin{matrix}\n  p \\oplus q & = & (p \\lor q) \\land (\\lnot p \\lor \\lnot q)\n\\end{matrix}</math>\n\nThis equivalence can be established by applying [[De Morgan's laws]] twice to the fourth line of the above proof.\n\nThe exclusive or is also equivalent to the negation of a [[logical biconditional]], by the rules of material implication (a [[material conditional]] is equivalent to the disjunction of the negation of its [[Antecedent (logic)|antecedent]] and its consequence) and [[If and only if|material equivalence]].\n\nIn summary, we have, in mathematical and in engineering notation:\n: <math>\\begin{matrix}\n  p \\oplus q & = & (p \\land \\lnot q) & \\lor & (\\lnot p \\land q) & = & p\\overline{q} + \\overline{p}q \\\\[3pt]\n             & = & (p \\lor q) & \\land & (\\lnot p \\lor \\lnot q) & = & (p + q)(\\overline{p} + \\overline{q}) \\\\[3pt]\n             & = & (p \\lor q) & \\land & \\lnot (p \\land q) & = & (p + q)(\\overline{pq})\n\\end{matrix}</math>\n\n==Relation to modern algebra==\n\nAlthough the [[Operation (mathematics)|operators]] <math>\\wedge</math> ([[Logical conjunction|conjunction]]) and <math>\\lor</math> ([[Logical disjunction|disjunction]]) are very useful in logic systems, they fail a more generalizable structure in the following way:\n\nThe systems <math>(\\{T, F\\}, \\wedge)</math> and <math>(\\{T, F\\}, \\lor)</math> are [[monoid]]s, but neither is a [[group (mathematics)|group]]. This unfortunately prevents the combination of these two systems into larger structures, such as a [[Ring (mathematics)|mathematical ring]].\n\nHowever, the system using exclusive or <math>(\\{T, F\\}, \\oplus)</math> '''is''' an [[abelian group]].  The combination of operators <math>\\wedge</math> and <math>\\oplus</math> over elements <math>\\{T, F\\}</math> produce the well-known [[field (mathematics)|field]] [[GF(2)|<math>F_2</math>]].  This field can represent any logic obtainable with the system <math>(\\land, \\lor)</math> and has the added benefit of the arsenal of algebraic analysis tools for fields.\n\nMore specifically, if one associates <math>F</math> with 0 and <math>T</math> with 1, one can interpret the logical \"AND\" operation as multiplication on <math>F_2</math> and the \"XOR\" operation as addition on <math>F_2</math>:\n:<math>\\begin{matrix}\n   r = p \\land q & \\Leftrightarrow & r = p \\cdot q \\pmod 2 \\\\[3pt]\n  r = p \\oplus q & \\Leftrightarrow & r = p + q \\pmod 2 \\\\\n\\end{matrix}</math>\n\nUsing this basis to describe a boolean system is referred to as [[algebraic normal form]].\n\n==Exclusive \"or\" in English==\n{{essay|section|date=May 2013}}\n\nThe Oxford English Dictionary explains \"either ... or\" as follows:\n{{quote|The primary function of ''either'', etc., is to emphasize the [[Mutually independent|perfect indifference]] of the two (or more) things or courses ... ; but a secondary function is to emphasize the mutual exclusiveness, {{=}} either of the two, but not both.<ref>or, conj.2 (adv.3) 2a ''Oxford English Dictionary'', second edition (1989). OED Online.</ref>}}\n\nThe exclusive-or explicitly states \"one or the other, but not neither nor both.\" However, the mapping correspondence between formal [[Boolean algebra|Boolean]] operators and natural language conjunctions is far from simple or one-to-one, and has been studied for decades in [[linguistics]] and [[analytic philosophy]].{{citation needed|date=January 2013}}\n\nFollowing this kind of common-sense intuition about \"or\", it is sometimes argued that in many natural languages, [[English language|English]] included, the word \"or\" has an \"exclusive\" sense.<ref>Jennings quotes numerous authors saying that the word \"or\" has an exclusive sense. See Chapter 3, \"The First Myth of 'Or'\":<br/>{{cite book |last=Jennings |first=R. E. |date=1994 |title=The Genealogy of Disjunction |url= |location=New York |publisher=Oxford University Press |isbn= |accessdate= }}</ref>  The '''exclusive disjunction''' of a pair of propositions, (''p'', ''q''), is supposed to mean  that ''p'' is true or ''q'' is true, but not both.  For example, it might be argued that the normal intention of a statement like \"You may have coffee, or you may have tea\" is to stipulate that exactly one of the conditions can be true. Certainly under some circumstances a sentence like this example should be taken as forbidding the possibility of one's accepting both options. Even so, there is good reason to suppose that this sort of sentence is not disjunctive at all. If all we know about some disjunction is that it is true overall, we cannot be sure which of its disjuncts is true. For example, if a woman has been told that her friend is either at the snack bar or on the tennis court, she cannot validly infer that he is on the tennis court. But if her waiter tells her that she may have coffee or she may have tea, she can validly infer that she may have tea. Nothing classically thought of as a disjunction has this property. This is so even given that she might reasonably take her waiter as having denied her the possibility of having both coffee and tea.{{citation needed|date=June 2012}}\n\nIn English, the construct \"either ... or\" is usually used to indicate exclusive or and \"or\" generally used for inclusive.{{dubious|date=January 2013}}  But in Spanish, the word \"o\" (or) can be used in the form \"''p'' o ''q''\" (inclusive) or the form \"o ''p'' o ''q''\" (exclusive). Some may contend that any binary or other [[arity|n-ary]] exclusive \"or\" is true if and only if it has an odd number of true inputs (this is not, however, the only reasonable definition; for example, digital xor gates with multiple inputs typically do not use that definition), and that there is no conjunction in English that has this general property.  For example, Barrett and Stenner contend in the 1971 article \"The Myth of the Exclusive 'Or{{' \"}} (Mind, 80 (317), 116–121) that no author has produced an example of an English or-sentence that appears to be false because both of its inputs are true, and brush off or-sentences such as \"The light bulb is either on or off\" as reflecting particular facts about the world rather than the nature of the word \"or\".  However, the \"[[barber paradox]]\"—Everybody in town shaves himself or is shaved by the barber, who shaves the barber? -- would not be paradoxical if \"or\" could not be exclusive (although a purist could say that \"either\" is required in the statement of the paradox).\n\nWhether these examples can be considered \"natural language\" is another question.{{dubious|date=January 2013}}  Certainly when one sees a menu stating \"Lunch special: sandwich and soup or salad\" (parsed as \"sandwich and (soup or salad)\" according to common usage in the restaurant trade), one would not expect to be permitted to order both soup and salad.  Nor would one expect to order neither soup nor salad, because that belies the nature of the \"special\", that ordering the two items together is cheaper than ordering them a la carte.  Similarly, a lunch special consisting of one meat, French fries or mashed potatoes and vegetable would consist of three items, only one of which would be a form of potato.  If one wanted to have meat and both kinds of potatoes, one would ask if it were possible to substitute a second order of potatoes for the vegetable.  And, one would not expect to be permitted to have both types of potato and vegetable, because the result would be a vegetable plate rather than a meat plate.{{citation needed|date=June 2012}}\n\n==Alternative symbols==\nThe symbol used for exclusive disjunction varies from one field of application to the next, and even depends on the properties being emphasized in a given context of discussion.  In addition to the abbreviation \"XOR\", any of the following symbols may also be seen:\n* '''<samp>+</samp>''', a plus sign, which has the advantage that all of the ordinary algebraic properties of mathematical [[ring (mathematics)|rings]] and [[field (mathematics)|fields]] can be used without further ado; but the plus sign is also used for inclusive disjunction in some notation systems; note that exclusive disjunction corresponds to [[addition]] [[modular arithmetic|modulo]] 2, which has the following addition table, clearly [[isomorphism|isomorphic]] to the one above:\n<!--\n{{aligned table\n | class=wikitable style=\"margin-left: auto\"\n | style=text-align:center;\n | title=Addition modulo 2\n | row1header=yes\n | col1style=width:5em;\n | col2style=width:5em;\n | col3style=width:5em;\n | cols=3\n | <math>p</math>\n | <math>q</math>\n | <math>p + q</math>\n | 0 | 0 | 0\n | 0 | 1 | 1\n | 1 | 0 | 1\n | 1 | 1 | 0\n }}\n-->\n{| class=\"wikitable\" style=\"margin-left: 20px; text-align: center\"\n|-\n! &nbsp;<math>p</math>&nbsp;\n! &nbsp;<math>q</math>&nbsp;\n! <math>p+q</math>\n|-\n| 0 || 0 || 0\n|-\n| 0 || 1 || 1\n|-\n| 1 || 0 || 1\n|-\n| 1 || 1 || 0\n|}\n* '''<math>\\oplus</math>''', a modified plus sign; this symbol is also used in mathematics for the ''[[Direct sum of modules|direct sum]]'' of algebraic structures\n* '''<samp>J</samp>''', as in J''pq''\n* An inclusive disjunction symbol (<math>\\lor</math>) that is modified in some way, such as \n**'''<math>\\underline\\lor</math>'''\n**'''<math>\\dot\\vee</math>'''\n* '''<samp>^</samp>''', the [[caret]], used in several [[programming language]]s, such as [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[D (programming language)|D]], [[Java (programming language)|Java]], [[Perl]], [[Ruby (programming language)|Ruby]], [[PHP]] and [[Python (programming language)|Python]], denoting the [[bitwise operation|bitwise]] XOR operator; not used outside of programming contexts because it is too easily confused with other uses of the caret\n* [[File:X-or.svg|24px]], sometimes written as \n** '''<samp>><</samp>'''\n** '''<samp>>-<</samp>'''\n* '''<samp>=1</samp>''', in IEC symbology\n\n==Properties==\n{{glossary}}\n{{term|[[Commutative property|Commutativity]]: yes}}{{defn|\n{{aligned table\n | style=text-align:center; border:1px solid darkgrey;\n | rowstyle=;\n | cols=3\n | <math>A \\oplus B</math>\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | <math>B \\oplus A</math>\n | [[File:Venn0110.svg|50px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn0110.svg|50px]]\n}}\n}}\n{{term|[[Associative property|Associativity]]: yes}}{{defn|\n{{aligned table\n | style=text-align:center; border:1px solid darkgrey;\n | rowstyle=;\n | cols=9\n | <math>~A</math>\n | <math>~~~\\oplus~~~</math>\n | <math>(B \\oplus C)</math>\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n |\n |\n | <math>(A \\oplus B)</math>\n | <math>~~~\\oplus~~~</math>\n | <math>~C</math>\n | [[File:Venn 0101 0101.svg|50px]]\n | <math>~~~\\oplus~~~</math>\n | [[File:Venn 0011 1100.svg|50px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn 0110 1001.svg|50px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn 0110 0110.svg|50px]]\n | <math>~~~\\oplus~~~</math>\n | [[File:Venn 0000 1111.svg|50px]]\n }}\n}}\n{{term|[[Distributive property|Distributivity]]:}}{{defn|The exclusive or doesn't distribute over any binary function (not even itself), but [[Logical conjunction#Properties|logical conjunction distributes over exclusive or]]. <math>C\\land(A \\oplus B )= (C\\land A) \\oplus (C\\land B)</math> (Conjunction and exclusive or form the multiplication and addition operations of a [[Field (mathematics)|field]] [[GF(2)]], and as in any field they obey the distributive law.)}}\n{{term|[[Idempotence|Idempotency]]: no}}{{defn|\n {{aligned table\n | style=text-align:center; border:1px solid darkgrey;\n | rowstyle=;\n | cols=7\n | <math>~A~</math>  \n | <math>~\\oplus~</math> \n | <math>~A~</math> \n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | <math>~0~</math> \n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\nLeftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | <math>~A~</math>\n | [[File:Venn01.svg|36px]]\n | <math>~\\oplus~</math>\n | [[File:Venn01.svg|36px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn00.svg|36px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\nLeftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn01.svg|36px]]\n }}\n}}\n{{term|[[Monotone boolean function|Monotonicity]]: no}}{{defn|\n {{aligned table\n | style=text-align:center; border:1px solid darkgrey;\n | rowstyle=;\n | cols=7\n | <math>A \\rightarrow B</math>\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n |\n |\n | <math>(A \\oplus C)</math>\n | <math>\\rightarrow</math>\n | <math>(B \\oplus C)</math>\n | [[File:Venn 1011 1011.svg|50px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn 1011 1101.svg|50px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn 0101 1010.svg|50px]]\n | <math>\\rightarrow</math>\n | [[File:Venn 0011 1100.svg|50px]]\n }}\n}}\n{{term|Truth-preserving: no}}{{defn|When all inputs are true, the output is not true.\n {{aligned table\n | style=text-align:center; border:1px solid darkgrey;\n | rowstyle=;\n | cols=3\n | <math>A \\land B</math>\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | <math>A \\oplus B</math>\n | [[File:Venn0001.svg|50px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn0110.svg|60px]]\n }}\n}}\n{{term|Falsehood-preserving: yes}}{{defn|When all inputs are false, the output is false.\n {{aligned table\n | style=text-align:center; border:1px solid darkgrey;\n | rowstyle=;\n | cols=3\n | <math>A \\oplus B</math>\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | <math>A \\lor B</math>\n | [[File:Venn0110.svg|60px]]\n | &nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n | [[File:Venn0111.svg|50px]]\n }}\n}}\n{{term|[[Hadamard transform|Walsh spectrum]]: (2,0,0,−2)}}\n{{term|Non-[[Linear#Boolean functions|linearity]]: 0}}{{defn|The function is linear.}}\n{{glossary end}}\n\nIf using [[binary numeral system|binary]] values for true (1) and false (0), then ''exclusive or'' works exactly like [[addition]] [[Modular arithmetic|modulo]] 2.\n\n==Computer science==\n[[File:XOR ANSI Labelled.svg|thumb|right|114px|Traditional symbolic representation of an XOR [[logic gate]]]]\n\n===Bitwise operation===\n{{Main article|Bitwise operation}}\n[[File:Z2^4; Cayley table; binary.svg|thumb|[[Nimber]] addition is the ''exclusive or'' of [[natural number|nonnegative integers]] in [[w:binary numeral system|binary]] representation. This is also the vector addition in <math>(\\Z/2\\Z)^4</math>.]]\nExclusive disjunction is often used for bitwise operations. Examples:\n* 1 XOR 1 = 0\n* 1 XOR 0 = 1\n* 0 XOR 1 = 1\n* 0 XOR 0 = 0\n* {{n-ary|1110|2}} XOR {{n-ary|1001|2}} = {{n-ary|0111|2}} (this is equivalent to addition without [[carry (arithmetic)|carry]])\n\nAs noted above, since exclusive disjunction is identical to addition modulo 2, the bitwise exclusive disjunction of two ''n''-bit strings is identical to the standard vector of addition in the [[vector space]] <math>(\\Z/2\\Z)^n</math>.\n\nIn computer science, exclusive disjunction has several uses:\n* It tells whether two bits are unequal.\n* It is an optional bit-flipper (the deciding input chooses whether to invert the data input).\n* It tells whether there is an [[Parity (mathematics)|odd]] number of 1&nbsp;bits (<math>A \\oplus B \\oplus C \\oplus D \\oplus E</math> is true [[If and only if|iff]] an odd number of the variables are true).\n\nIn logical circuits, a simple [[adder (electronics)|adder]] can be made with an [[XOR gate]] to add the numbers, and a series of AND, OR and NOT gates to create the carry output.\n\nOn some computer architectures, it is more efficient to store a zero in a register by XOR-ing the register with itself (bits XOR-ed with themselves are always zero) instead of loading and storing the value zero.\n\nIn simple threshold activated [[neural network]]s, modeling the XOR function requires a second layer because XOR is not a linearly separable function.\n\nExclusive-or is sometimes used as a simple mixing function in [[cryptography]], for example, with [[one-time pad]] or [[Feistel cipher|Feistel network]] systems.{{citation needed|date=June 2015}}\n\nExclusive-or is also heavily used in block ciphers such as AES (Rijndael) or Serpent and in block cipher implementation (CBC, CFB, OFB or CTR).\n\nSimilarly, XOR can be used in generating [[entropy pool]]s for [[hardware random number generator]]s.  The XOR operation preserves randomness, meaning that a random bit XORed with a non-random bit will result in a random bit.  Multiple sources of potentially random data can be combined using XOR, and the unpredictability of the output is guaranteed to be at least as good as the best individual source.<ref>{{cite web|last=Davies|first=Robert B|title=Exclusive OR (XOR) and hardware random number generators|url=http://www.robertnz.net/pdf/xor2.pdf|accessdate=28 August 2013|date=28 February 2002}}</ref>\n\nXOR is used in [[RAID]] 3–6 for creating parity information.  For example, RAID can \"back up\" bytes {{n-ary|10011100|2}} and {{n-ary|01101100|2}} from two (or more) hard drives by XORing the just mentioned bytes, resulting in ({{n-ary|11110000|2}}) and writing it to another drive.  Under this method, if any one of the three hard drives are lost, the lost byte can be re-created by XORing bytes from the remaining drives. For instance,  if the drive containing {{n-ary|01101100|2}} is lost, {{n-ary|10011100|2}} and {{n-ary|11110000|2}} can be XORed to recover the lost byte.<ref>{{cite web|last=Nobel|first=Rickard|title=How RAID 5 actually works|accessdate=23 March 2017|date=26 July 2011|url=http://rickardnobel.se/how-raid5-works}}</ref>\n\nXOR is also used to detect an overflow in the result of a signed binary arithmetic operation. If the leftmost retained bit of the result is not the same as the infinite number of digits to the left, then that means overflow occurred. XORing those two bits will give a \"1\" if there is an overflow.\n\nXOR can be used to swap two numeric variables in computers, using the [[XOR swap algorithm]]; however this is regarded as more of a curiosity and not encouraged in practice.\n\n[[XOR linked list]]s leverage XOR properties in order to save space to represent [[doubly linked list]] data structures.\n\nIn [[computer graphics]], XOR-based drawing methods are often used to manage such items as [[bounding volume|bounding boxes]] and [[cursor (computers)|cursors]] on systems without [[alpha compositing|alpha channels]] or overlay planes.\n\n==Encodings==\nApart from the ASCII codes, the operator is encoded at {{unichar|22BB|XOR|html=}} and {{unichar|2295|CIRCLED PLUS|html=}}, both in block [[Mathematical operators and symbols in Unicode#Mathematical Operators|mathematical operators]].\n\n==See also==\n\n{{columns-list|colwidth=22em|\n*[[Material conditional]] &bull; [[Paradoxes of material implication|(Paradox)]]\n* [[Affirming a disjunct]]\n* [[Ampheck]]\n* [[Boolean algebra (logic)]]\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean-valued function]]\n* [[Controlled NOT gate]]\n* [[Disjunctive syllogism]]\n* [[First-order logic]]\n* [[Inclusive or]]\n* [[involution (mathematics)|Involution]]\n* [[List of Boolean algebra topics]]\n* [[Logical graph]]\n* [[Logical value]]\n* [[Operation (mathematics)|Operation]]\n* [[Parity bit]]\n* [[Propositional calculus]]\n* [[Rule 90]]\n* [[Symmetric difference]]\n* [[XOR cipher]]\n* [[XOR gate]]\n* [[XOR linked list]]\n}}\n\n==Notes==\n<references/>\n\n==External links==\n*[http://www.codeplex.com/rexor An example of XOR being used in cryptography]\n\n{{Logical connectives}}\n\n[[Category:Logical connectives]]\n[[Category:Dichotomies]]"
    },
    {
      "title": "False (logic)",
      "url": "https://en.wikipedia.org/wiki/False_%28logic%29",
      "text": "{{see also|Falsity}}\n{{Refimprove|date=March 2012}}\nIn [[logic]], '''false''' or '''[[logical truth|untrue]]''' is the state of possessing negative [[truth value]] or a [[nullary]] [[logical connective]]. In a [[truth function|truth-functional]] [[propositional calculus|system of propositional logic]] it is one of two postulated [[truth value]]s, along with its [[negation]], [[logical truth|truth]].<ref>Jennifer Fisher, ''On the Philosophy of Logic'', Thomson Wadsworth, 2007, {{ISBN|0-495-00888-5}}, [https://books.google.com/books?id=k8L_YW-lEEQC&pg=PT27 p. 17.]</ref>  Usual notations of the false are [[0 (number)|0]] (especially in Boolean logic and [[computer science]]), O (in [[Polish notation|prefix notation]], O''pq''), and the [[up tack]] symbol&nbsp;⊥.<ref>[[Willard Van Orman Quine]], ''Methods of Logic'', 4th ed, Harvard University Press, 1982, {{ISBN|0-674-57176-2}}, [https://books.google.com/books?id=liHivlUYWcUC&pg=PA34 p. 34.]</ref>\n\nAnother approach is used for several [[theory (mathematical logic)|formal theories]] (for example, [[intuitionistic propositional calculus]]) where the false is a propositional constant (i.e. a nullary connective)&nbsp;⊥, the truth value of this constant being always false in the sense above.<ref>[[George Edward Hughes]] and D.E. Londey, ''The Elements of Formal Logic'', Methuen, 1965, [https://books.google.com/books?id=JbwOAAAAQAAJ&pg=PA151 p. 151.]</ref><ref>Leon Horsten and Richard Pettigrew, ''Continuum Companion to Philosophical Logic'', Continuum International Publishing Group, 2011, {{ISBN|1-4411-5423-X}}, [https://books.google.com/books?id=w_abLTXIFkcC&pg=PA199 p. 199.]</ref><ref>[[Graham Priest]], ''An Introduction to Non-Classical Logic: From If to Is'', 2nd ed, Cambridge University Press, 2008, {{ISBN|0-521-85433-4}}, [https://books.google.com/books?id=rMXVbmAw3YwC&pg=PA105 p. 105.]</ref>\n\n== In classical logic and Boolean logic ==<!-- linked from #Consistency -->\nIn [[Boolean logic]] each variable denotes a [[Truth value]] which can be either true (1), or false (0).\nIn a [[classical logic|classical]] [[propositional calculus]] each [[proposition]] will be assigned a truth value of either true or false.\nSome systems of classical logic include dedicated symbols for false (0 or ⊥), others instead rely upon formulas such as {{math|{{mvar|p}} ∧ ¬{{mvar|p}}}} and {{math|¬({{mvar|p}} → {{mvar|p}})}}.\n\nIn both Boolean logic and Classical logic systems, true and false are opposite with respect to [[negation]]; The negation of false gives true, and the negation of true gives false.\n\n{| class=\"wikitable\" style=\"text-align: center\"\n|-\n!<math>x</math>\n!<math>\\neg x</math>\n|-\n!true\n|false\n|-\n!false\n|true\n|}\n\nThe negation of false is equivalent to the truth not only in classical logic and Boolean logic, but also in most other logical systems, as explained below.\n\n{{expand section|date=February 2012}}\n\n== False, negation and contradiction ==\nIn most logical systems, [[negation]], [[material conditional]] and false are related as:\n: {{math|¬{{mvar|p}} ⇔ ({{mvar|p}} → ⊥)}}\nThis is the definition of negation in some systems,<ref>Dov M. Gabbay and Franz Guenthner (eds), ''Handbook of Philosophical Logic, Volume 6'', 2nd ed, Springer, 2002, {{ISBN|1-4020-0583-0}}, [https://books.google.com/books?id=JyewdfGhNAsC&pg=PA12 p. 12.]</ref> such as [[intuitionistic logic]], and can be proven in propositional calculi where negation is a fundamental connective.  Because {{math|{{mvar|p}} → {{mvar|p}}}} is usually a theorem or axiom, a consequence is that the negation of false ({{math|¬ ⊥}}) is true.\n\nThe [[contradiction]] is a [[statement (logic)|statement]] which [[entailment|entails]] the false, i.e. {{math|φ ⊢ ⊥}}. Using the equivalence above, the fact that φ is a contradiction may be derived, for example, from {{math|⊢ ¬φ}}.  Contradiction and the false are sometimes not distinguished, especially due to [[Latin]] term ''[[wikt:falsum|falsum]]'' denoting both. \nContradiction means a statement is [[mathematical proof|proven]] to be false, but the false itself is a [[proposition]] which is defined to be opposite to the truth.\n\nLogical systems may or may not contain the [[principle of explosion]] (in [[Latin]], ''ex falso quodlibet''), {{math|⊥ ⊢ φ}}.\n\n== Consistency ==\n{{main|Consistency}}\nA [[theory (mathematical logic)|formal theory]] using \"⊥\" connective is defined to be consistent if and only if the false is not among its [[theorem]]s. In the absence of propositional constants, some substitutes such as [[#In classical logic and Boolean logic|mentioned above]] may be used instead to define consistency.\n\n==See also==  \n* [[Contradiction]]  \n* [[Logical truth]]  \n* [[Tautology (logic)]] (for symbolism of logical truth)\n\n== References ==\n{{reflist}}\n\n{{Logical connectives}}\n{{Logical truth}}\n\n[[Category:Logical connectives]]"
    },
    {
      "title": "If and only if",
      "url": "https://en.wikipedia.org/wiki/If_and_only_if",
      "text": "{{short description|Logical connective}}\n{{Use dmy dates|date=February 2015}}\n{{Redirect|Iff|other uses|IFF (disambiguation)}}\n{{Redirect-distinguish|↔|Bidirectional traffic}}\n{{refimprove|date=July 2013}}\n{{quote box |quote = {{resize|400%|↔<!-- Unicode 2194 \"<->\"\n-->⇔<!-- Unicode 21d4 \"<=>\"\n-->≡<!-- Unicode 2261, \"Identical to\"\n-->⟺<!-- Unicode 27FA long \"<==>\"-->}}\n<br/>Logical symbols representing ''iff''\n}}\nIn [[logic]] and related fields such as [[mathematics]] and [[philosophy]], '''if and only if''' (shortened '''iff''') is a [[biconditional]] [[logical connective]] between statements, where either both statements are true or both are false.\n\nThe connective is [[biconditional]] (a statement of '''material equivalence'''<!--boldface per WP:R#PLA-->),<ref>{{cite book |last=Copi |first=I. M. |last2=Cohen |first2=C. |last3=Flage |first3=D. E. |year=2006 |title=Essentials of Logic |edition=Second |location=Upper Saddle River, NJ |publisher=Pearson Education |page=197 |isbn=978-0-13-238034-8 }}</ref> and can be likened to the standard [[material conditional]] (\"only if\", equal to \"if ... then\") combined with its reverse (\"if\"); hence the name. The result is that the truth of either one of the connected statements requires the truth of the other (i.e. either both statements are true, or both are false). It is controversial whether the connective thus defined is properly rendered by the English \"if and only if\", with its pre-existing meaning.\n\nIn writing, phrases commonly used as alternatives to P \"if and only if\" Q include: ''Q is [[Necessary and sufficient conditions#Simultaneous necessity and sufficiency|necessary and sufficient]] for P'', ''P is equivalent (or materially equivalent) to Q'' (compare [[material conditional|material implication]]), ''P precisely if Q'', ''P precisely (or exactly) when Q'', ''P exactly in case Q'', and ''P just in case Q''.<ref>Weisstein, Eric W. \"Iff.\" From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/Iff.html</ref> Some authors regard \"iff\" as unsuitable in formal writing;<ref>E.g. {{citation |title=Reading, Writing, and Proving: A Closer Look at Mathematics |series=[[Undergraduate Texts in Mathematics]] |first1=Ulrich |last1=Daepp |first2=Pamela |last2=Gorkin|author2-link=Pamela Gorkin |publisher=Springer |year=2011 |isbn=9781441994790 |url=https://books.google.com/books?id=4QKcaXrVZb0C&pg=PA52 |page=52 |quote=While it can be a real time-saver, we don't recommend it in formal writing.}}</ref> others consider it a \"borderline case\" and tolerate its use.<ref>{{citation |title=Engineering Writing by Design: Creating Formal Documents of Lasting Value |first1=Edward J. |last1=Rothwell |first2=Michael J. |last2=Cloud |publisher=CRC Press |year=2014 |isbn=9781482234312 |page=98 |url=https://books.google.com/books?id=muXMAwAAQBAJ&pg=PA98 |quote=It is common in mathematical writing}}</ref>\n\nIn [[formula (mathematical logic)|logical formulae]], logical symbols are used instead of these phrases; see the [[If_and_only_if#Notation|discussion of notation]].\n\n==Definition==\nThe [[truth table]] of ''P'' <math>\\Leftrightarrow</math> ''Q'' is as follows:<ref>[http://www.wolframalpha.com/input/?i=p+%3C%3D%3E+q p <=> q]. [[Wolfram Alpha|Wolfram&#124;Alpha]]</ref><ref>{{citation |title=If and only if |first1= |last1= |first2= |last2= |publisher=UHM Department of Mathematics |year= |isbn= |page= |url=http://www.math.hawaii.edu/~ramsey/Logic/Iff.html |quote=Theorems which have the form \"P if and only Q\" are much prized in mathematics. They give what are called \"necessary and sufficient\" conditions, and give completely equivalent and hopefully interesting new ways to say exactly the same thing.}}</ref>\n{| class=\"wikitable\" style=\"margin:1em auto; text-align:center; float:left\"\n|+ Truth table\n|-\n! scope=\"col\"  style=\"width:20%\" | ''P''\n! scope=\"col\"  style=\"width:20%\" | ''Q''\n! scope=\"col\"  style=\"width:20%\" | {{nowrap|[[Material conditional|''P'' <math>\\Rightarrow</math> ''Q'']]}}\n! scope=\"col\"  style=\"width:20%\" | {{nowrap|''P'' <math>\\Leftarrow</math> ''Q''}}\n! scope=\"col\"  style=\"width:20%\" | {{nowrap|''P''&nbsp;<math>\\Leftrightarrow</math> ''Q''}}\n|-\n| T || T || T || T || T\n|-\n| T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || T || T || style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F\n|-\n| style=\"background:papayawhip\" | F || style=\"background:papayawhip\" | F || T || T || T\n|}\n{{clear}}\nIt is equivalent to that produced by the [[XNOR gate]], and opposite to that produced by the [[XOR gate]].\n\n==Usage==\n\n===Notation===\nThe corresponding logical symbols are \"↔\", \"<math>\\Leftrightarrow</math>\", and \"[[Triple bar|≡]]\", and sometimes \"iff\". These are usually treated as equivalent. However, some texts of [[mathematical logic]] (particularly those on [[first-order logic]], rather than [[propositional logic]]) make a distinction between these, in which the first, ↔, is used as a symbol in logic formulas, while ⇔ is used in reasoning about those logic formulas (e.g., in [[metalogic]]). In [[Jan Łukasiewicz|Łukasiewicz]]'s notation, it is the prefix symbol 'E'.\n\nAnother term for this [[logical connective]] is [[exclusive nor]].\n\nIn [[TeX]] \"if and only if\" is shown as a long double arrow: <math>\\iff</math> via command \\iff.\n\n===Proofs===\nIn most [[logical system]]s, one [[Proof theory|proves]] a statement of the form \"P iff Q\" by proving \"if P, then Q\" and \"if Q, then P\".  Proving this pair of statements sometimes leads to a more natural proof since there are not obvious conditions in which one would infer a biconditional directly. An alternative is to prove the [[disjunction]] \"(P and Q) or (not-P and not-Q)\", which itself can be inferred directly from either of its disjuncts—that is, because \"iff\" is [[truth-function]]al, \"P iff Q\" follows if P and Q have both been shown true, or both false.\n\n===Origin of iff and pronunciation===\nUsage of the abbreviation \"iff\" first appeared in print in [[John L. Kelley]]'s 1955 book ''General Topology''.<ref>''General Topology,'' reissue {{ISBN|978-0-387-90125-1}}</ref>\nIts invention is often credited to [[Paul Halmos]], who wrote \"I invented 'iff,' for  'if and only if'—but I could never believe I was really its first inventor.\"<ref name=\"Higham1998\">{{cite book |author=Nicholas J. Higham |title=Handbook of writing for the mathematical sciences |url=https://books.google.com/books?id=9gQd2fJA7Y4C&pg=PA24 |year=1998 |publisher=SIAM |isbn=978-0-89871-420-3 |page=24 |edition=2nd}}</ref>  \n\nIt is somewhat unclear how \"iff\" was meant to be pronounced. In current practice, the single 'word' \"iff\" is almost always read as the four words \"if and only if\". However, in the preface of ''General Topology'', Kelley suggests that it should be read differently: \"In some cases where mathematical content requires 'if and only if' and [[euphony]] demands something less I use Halmos' 'iff'\". The authors of one discrete mathematics textbook suggest:<ref>{{cite book |title=Discrete Algorithmic Mathematics |last=Maurer |first=Stephen B. |last2=Ralston |first2=Anthony |publisher=CRC Press |year=2005 |isbn=1568811667 |edition=3rd |location=Boca Raton, Fla. |pages=60}}</ref> \"Should you need to pronounce iff, really [[Consonant gemination|hang on to the 'ff']] so that people hear the difference from 'if'\", implying that \"iff\" could be pronounced as {{IPA|[ɪfː]}}.\n\n=== Usage in definitions ===\nTechnically, definitions are always \"if and only if\" statements; many texts such as Kelley's ''General Topology'' follow the strict demands of logic, and use \"if and only if\" or ''iff'' in definitions of new terms (for instance, from ''General Topology'', p. 25: \"A set is '''countable''' iff it is finite or countably infinite\" [boldface in original]). However, this usage of \"if and only if\" is not universal; often, mathematical definitions follow the special convention that \"if\" is interpreted to mean \"if and only if\" (for example, one might say, \"A topological space is compact if every open cover has a finite subcover\").<ref>{{citation |page=71 |first=Steven G. |last=Krantz |title=A Primer of Mathematical Writing |year=1996 |publisher=American Mathematical Society |isbn=978-0-8218-0635-7}}</ref>\n\n==Distinction from \"if\" and \"only if\"==\n{{unreferenced section|date=June 2013}}\n# '''\"Madison will eat the fruit <u>if</u> it is an apple.\"''' (equivalent to '''\"<u>Only if</u> Madison will eat the fruit, can it be an apple\"''' or '''\"Madison will eat the fruit ''←'' the fruit is an apple\"''')\n#: This states that Madison will eat fruits that are apples. It does not, however, exclude the possibility that Madison might also eat bananas or other types of fruit. All that is known for certain is that she will eat any and all apples that she happens upon. That the fruit is an apple is a ''sufficient'' condition for Madison to eat the fruit.\n# '''\"Madison will eat the fruit <u>only if</u> it is an apple.\"''' (equivalent to '''\"<u>If</u> Madison will eat the fruit, then it is an apple\"''' or '''\"Madison will eat the fruit ''→'' the fruit is an apple\"''')\n#: This states that the only fruit Madison will eat is an apple. It does not, however, exclude the possibility that Madison will refuse an apple if it is made available, in contrast with (1), which requires Madison to eat any available apple. In this case, that a given fruit is an apple is a ''necessary'' condition for Madison to be eating it. It is not a sufficient condition since Madison might not eat all the apples she is given.\n# '''\"Madison will eat the fruit <u>if and only if</u> it is an apple\"''' (equivalent to '''\"Madison will eat the fruit ''↔'' the fruit is an apple\"''')\n#: This statement makes it clear that Madison will eat all and only those fruits that are apples. She will not leave any apple uneaten, and she will not eat any other type of fruit. That a given fruit is an apple is both a ''necessary'' and a ''sufficient'' condition for Madison to eat the fruit.\n\nSufficiency is the converse of necessity. That is to say, given ''P''→''Q'' (i.e. if ''P'' then ''Q''), ''P'' would be a sufficient condition for ''Q'', and ''Q'' would be a necessary condition for ''P''. Also, given ''P''→''Q'', it is true that ''¬Q''→''¬P'' (where ¬ is the negation operator, i.e. \"not\"). This means that the relationship between ''P'' and ''Q'', established by ''P''→''Q'', can be expressed in the following, all equivalent, ways:\n:''P'' is sufficient for ''Q''\n:''Q'' is necessary for ''P''\n:''¬Q'' is sufficient for ''¬P''\n:''¬P'' is necessary for ''¬Q''\nAs an example, take (1), above, which states ''P''→''Q'', where ''P'' is \"the fruit in question is an apple\" and ''Q'' is \"Madison will eat the fruit in question\". The following are four equivalent ways of expressing this very relationship:\n:If the fruit in question is an apple, then Madison will eat it.\n:Only if Madison will eat the fruit in question, is it an apple.\n:If Madison will not eat the fruit in question, then it is not an apple.\n:Only if the fruit in question is not an apple, will Madison not eat it.\nSo we see that (2), above, can be restated in the form of ''if...then'' as \"If Madison will eat the fruit in question, then it is an apple\"; taking this in conjunction with (1), we find that (3) can be stated as \"If the fruit in question is an apple, then Madison will eat it; ''and'' if Madison will eat the fruit, then it is an apple\".\n\n==In terms of Euler diagrams==\n<gallery widths=\"270\">\nFile:Example of A is a proper subset of B.svg|A is a proper subset of B. A number is in A only if it is in B; a number is in B if it is in A.\nFile:Example of C is no proper subset of B.svg|C is a subset but not a proper subset of B. A number is in B if and only if it is in C, and a number is in C if and only if it is in B.\n</gallery>\n\n[[Euler diagram]]s show logical relationships among events, properties, and so forth. \"P only if Q\", \"if P then Q\", and  \"P→Q\" all mean that P is a [[subset]], either proper or improper, of Q. \"P if Q\", \"if Q then P\", and Q→P all mean that Q is a proper or improper subset of P. \"P if and only if Q\" and \"Q if and only if P\" both mean that the sets P and Q are identical to each other.\n\n==More general usage==\nIff is used outside the field of logic. Wherever logic is applied, especially in [[mathematics|mathematical]] discussions, it has the same meaning as above: it is an abbreviation for ''if and only if'', indicating that one statement is both [[Necessary and sufficient conditions|necessary and sufficient]] for the other. This is an example of [[mathematical jargon]]. (However, as noted above, ''if'', rather than ''iff'', is more often used in statements of definition.)\n\nThe elements of ''X'' are ''all and only'' the elements of ''Y'' is used to mean: \"for any ''z'' in the [[domain of discourse]], ''z'' is in ''X'' if and only if ''z'' is in ''Y''.\"\n\n==See also==\n{{Portal|Thinking}}\n* [[Covariance]]\n* [[Logical biconditional]]\n* [[Logical equality]]\n* [[Necessary and sufficient condition]]\n* [[Polysyllogism]]\n\n== References ==\n{{Reflist|30em}}\n\n==External links==\n* {{cite web |url= http://www.math.hawaii.edu/~ramsey/Logic/Iff.html |title= Tables of truth for if and only if |archive-url= https://web.archive.org/web/20000505112920/http://www.math.hawaii.edu/~ramsey/Logic/Iff.html |archive-date= May 5, 2000 |deadurl= }}\n* [http://itre.cis.upenn.edu/~myl/languagelog/archives/003470.html Language Log: \"Just in Case\"]\n* [http://hesperusphosphorus.wordpress.com/2009/12/08/just-in-case Southern California Philosophy for philosophy graduate students: \"Just in Case\"]\n\n[[Category:Logical connectives]]\n[[Category:Mathematical terminology]]\n[[Category:Necessity and sufficiency]]"
    }
  ]
}