{
  "pages": [
    {
      "title": "Section (United States land surveying)",
      "url": "https://en.wikipedia.org/wiki/Section_%28United_States_land_surveying%29",
      "text": "[[Image:Theoreticaltownshipmap.gif|thumb|Sectioning a [[survey township|township]] (36 sections).]]\n[[File:Indy farmland.jpg|thumb|right|Perfectly square full sections of farmland cover Central [[Indiana]].]]\n[[Image:Crops Kansas AST 20010624.jpg|thumb|Satellite image of crops growing in [[Kansas]], mainly using [[center pivot irrigation]]. The primary grid pattern is of quarter sections ({{convert|1/2|x|1/2|mi|m|abbr=on}}).]]\n<!-- Deleted image removed: [[File:Snowbank Trail (36).JPG|thumbnail|A brass disc in concrete marking a quarter section corner.]] -->\nIn U.S. land surveying under the [[Public Land Survey System]] (PLSS), a '''section''' is an area nominally {{convert|1|sqmi|km2|sp=us|abbr=off|lk=in|spell=in}}, containing {{convert|640|acre|abbr=off}}, with 36 sections making up one [[survey township]] on a rectangular grid.<ref>{{cite book |title= A History of the Rectangular Survey System |first= C. Albert |last= White |year= 1983 |location= Washington, D.C.|publisher= [[United States Bureau of Land Management]] |url=https://www.blm.gov/sites/blm.gov/files/histrect.pdf}}</ref>\n\nThe legal description of a tract of land under the PLSS includes the name of the [[U.S. state|state]], name of the [[County (United States)|county]], township number, range number, section number, and portion of a section. Sections are customarily [[Surveying|surveyed]] into smaller squares by repeated halving and quartering. A quarter section is {{convert|160|acre|abbr=on}} and a \"quarter-quarter section\" is {{convert|40|acre|abbr=on}}. In 1832 the smallest area of land that could be acquired was reduced to the {{convert|40|acre|ha|adj=on}} quarter-quarter section, and this size parcel became entrenched in American mythology. After the [[American Civil War|Civil War]], [[Freedmen]] (freed slaves) were reckoned to be self-sufficient with \"[[40 acres and a mule]].\" In the 20th century real estate developers preferred working with {{convert|40|acre|ha|adj=on}} parcels.<ref name=linklater>{{Cite book|last= Linklater |first= Andro |title= Measuring America: How the United States Was Shaped By the Greatest Land Sale in History |publisher= Plume |year= 2002 |isbn= 0-452-28459-7 |pages= 72, 166, 234}}</ref> The phrases \"front 40\" and \"[[wikt:back forty|back 40]],\" referring to farm fields, indicate the front and back quarter-quarter sections of land.\n\nOne of the reasons for creating sections of {{convert|640|acre|ha}} was the ease of dividing into halves and quarters while still maintaining a whole number of acres. A section can be halved seven times in this way, down to a {{convert|5|acre|adj=on|0}} parcel, or half of a quarter-quarter-quarter section—an easily surveyed 50-square-[[chain (unit)|chain]] (2&nbsp;ha) area. This system was of great practical value on the American frontier, where surveyors often had a shaky grasp of mathematics and were required to work quickly.<ref name=linklater/>\n\nA description of a quarter-quarter section in standard abbreviated form, might look like \"NW 1/4, NE 1/4, Sec. 34, T.3S, R.1W, 1st P.M.\" or, alternatively, \"34-3-1 NW4NE4 1PM\". In expanded form this would read \"the Northwest quarter of the Northeast quarter of Section 34 of Township 3 South, Range 1 West, first Principal Meridian\".<ref name=mapuse/>\n\n==History==\nThe existence of [[section line]]s made property descriptions far more straightforward than the old [[metes and bounds]] system.  The establishment of standard east-west and north-south lines (\"township\" and \"range lines\") meant that deeds could be written without regard to temporary terrain features such as trees, piles of rocks, fences, and the like, and be worded in the style such as \"Lying and being in Township 4 North; Range 7 West; and being the northwest quadrant of the southwest quadrant of said section,\" an exact description in this case of 40 acres, as there are {{convert|640|acre|abbr=on}} in a square mile.\n\nThe importance of \"sections\" was greatly enhanced by the passage of \"An Ordinance for ascertaining the mode of disposing of lands in the Western Territory\" of 1785 by the [[Congress of the United States|U.S. Congress]] (see [[Land Ordinance of 1785]]). This law provided that lands outside the then-existing states could not be sold, otherwise distributed, or opened for settlement prior to being surveyed. The standard way of doing this was to divide the land into sections.  An area six sections by six sections would define a township. Within this area, one section was designated as school land. As the entire parcel would not be necessary for the school and its grounds, the balance of it was to be sold, with the monies to go into the construction and upkeep of the school.\n\n===Roads and urban planning===\n{{Main|Section line road}}\n\n==Numbering within a township==\nEvery township is divided into 36 sections, each usually {{convert|1|mi}} square.  Sections are numbered [[boustrophedon]]ically within townships<ref name=mapuse/> as follows (north at top):\n{| class=\"wikitable\"\n|-\n| 6\n| 5\n| 4\n| 3\n| 2\n| 1\n|-\n| 7\n| 8\n| 9\n| 10\n| 11\n| 12\n|-\n| 18\n| 17\n| 16\n| 15\n| 14\n| 13\n|-\n| 19\n| 20\n| 21\n| 22\n| 23\n| 24\n|-\n| 30\n| 29\n| 28\n| 27\n| 26\n| 25\n|-\n| 31\n| 32\n| 33\n| 34\n| 35\n| 36\n|}\n\n==Subdivision of a section==\nSections are (often) broken up into {{convert|40|acre|ha|adj=on}} blocks, or quarter quarter sections.  These are labeled as follows:\n<ref>{{cite book |url= https://books.google.com/books?id=QHJRR3IhIloC&pg=PA52&lpg=PA52 |title= Farm Appraisal and Valuation |first= William Gordon |last= Murray |location= Ames |publisher= Iowa State University Press |year= 1969 |edition= 5th |page= 53 |oclc= 246381719}}</ref>\n\n{| class=\"wikitable\"\n|-\n|NWNW\n| NENW\n| NWNE\n| NENE\n|-\n|SWNW\n| SENW\n| SWNE\n| SENE\n|-\n|NWSW\n| NESW\n| NWSE\n| NESE\n|-\n|SWSW\n| SESW\n| SWSE\n| SESE\n|}\n\n==Measurement anomalies==\nThe curvature of the earth makes it impossible to superimpose a regular grid on its surface, as the [[meridian (geography)|meridian]]s converge toward the [[Geographical pole|North Pole]]. As the U.S. is in the [[Northern Hemisphere]], if a section's or township's east and west sides are parallel, its north side is shorter than its south side. As sections were surveyed from south and east to north and west, accumulated errors and distortions resulted on the north and west lines, and north and west sections diverge the most from the ideal shape and size.\n\nThe entire township grid shifts to account for the earth's curvature. Where the grid is corrected, or where two grids based on different [[principal meridian]]s meet, section shapes are irregular.\n\nSections also differ from the PLSS ideal of one square mile for other reasons, including errors and sloppy work by surveyors, poor instrumentation, and difficult terrain. In addition, the primary survey tool was the magnetic compass, which is influenced by local irregularities.\n\nOnce established, even an imperfect grid remains in force, mainly because the monuments of the original survey, when recovered, hold legal precedent over subsequent resurveys.<ref name=mapuse>{{Cite book|last= Muehrcke |first= Phillip C. |first2=Juliana O. |last2= Muehrcke |first3= A. Jon |last3= Kimerling  |title= Map Use: Reading, Analysis, and Interpretation |edition= 4th |publisher= JP Publications |year= 2001 |isbn= 0-9602978-5-5 |pages= 234–239}}</ref>\n\n==Alternatives and legacy systems==\nThe Public Land Survey System was not the first to define and implement a survey grid. A number of similar systems were established, often using terms like section and township but not necessarily in the same way. For example, the lands of the [[Holland Purchase]] in western [[New York (state)|New York]] were surveyed into a township grid before the PLSS was established. In colonial [[New England]] land was often divided into squares called towns or townships, and further subdivided into parcels called lots or sections.<ref name=linklater/>\n\nSections are also used in land descriptions in the portion of northwestern [[Georgia (U.S. state)|Georgia]] that was formerly part of the territory of the [[Cherokee]] Nation. They are not, however, part of the PLSS and are irregular in shape and size. See [[Cherokee County, Georgia]] for more information on the historical reasons for this.\n\nAnother exception to the usual use of sections and section numbering occurs when most of a parcel, or [[Lot (real estate)|lot]], falls under a body of water. The term \"government lot\" is used for such parcels and they are usually described separately from the rest of the section using single numbers (such as \"Government Lot 5 of Section 15\"). Also, parcels within a [[plat]]ted [[Subdivision (land)|subdivision]] are often specified by lot number rather than using PLSS descriptions.<ref name=mapuse/>\n\nWhere [[Spanish land grants in Florida]] have descriptions that predate PLSS or even the U.S. itself, deviation from typical section numbering and size and shape often takes place. In an effort to honor these land grants after the U.S. took control of Florida, surveyors would use descriptions from confirmed land grants to establish their initial boundaries and created PLSS sections that extrapolated from those lines. Often, the amount of land left over in areas immediately surrounding the grants was grossly undersized or awkwardly shaped. Those tracts are referred to as \"fractional sections\" and often are not subject to township or range definitions. An example of such a legal description's beginning would read \"Being a portion of Fractional Sec. 59, Township 0 South, Range 0 West\".\n\nAlso, land north of the [[Watson Line]] near the Georgia border was not subject to the standard U.S. section, township and range designations, since the State of Georgia had claimed and laid out counties and surveyed its public lands south to that line into what eventually became part of the State of Florida.  The exact location of the Georgia–Florida state line was ultimately confirmed by an Act of Congress, approved April 9, 1872.\n\n==See also==\n*Canadian [[Dominion Land Survey]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite book |last=Raymond |first=William Galt |title=Plane Surveying for Use in the Classroom and Field |date=1914 |publisher=American Book Company |location=New York |url=https://books.google.com/books?id=68I3AAAAMAAJ&printsec=frontcover#v=onepage&q&f=false}}\n* Johnson, Hildegard Binder. Order Upon the Land: The U. S. Rectangular Land Survey and the Upper Mississippi Country. Oxford University Press, 1976.\n\n{{DEFAULTSORT:Section (United States Land Surveying)}}\n[[Category:Land surveying of the United States]]\n[[Category:Subdivisions of the United States]]\n[[Category:Units of area]]"
    },
    {
      "title": "Sokha (unit)",
      "url": "https://en.wikipedia.org/wiki/Sokha_%28unit%29",
      "text": "{{Unreferenced|date=December 2012}}\n\n'''Sokha''' (big plow) – a unit of land measure in [[Russia]] in 13th-17th centuries. Sokha was used as unit of taxation for worked land owned by someone else or the State. The term originated when [[Tatar]]s took rent from the actual plows - the land working tool which used 2-3 horses. \n\nAt the end of 15th century [[Novgorod Republic|Novgorod]] sokha equalled 3 [[обж]]s. A [[Moscow]] sokha was equal to 10 Novgorod ones and was paid according to the different sizes in various regions of the state.\n\nIn 1550 a new land-measure unit was established which kept the old name ''sokha''. The name was then used for a certain amount of exploited land. A sokha was divided into quarters.\n\nThere were several types of sokha:\n\n# '''government''' – 800 quarters of best land;\n# '''church''' – 600 quarters of best land;\n# '''black''' – 400 quarters of best land.\n\nHere, ''government'' meant someone who was working in government sectors, and ''black'' meant an average person (''\"the masses\"''), ''quarter'' or ''quart'' meant an area of land that is planted with a quarter of [[rye]]. The land was planted in such a way that the area of land  seeded with two ''quarters'' was equal to a [[:ru:Десятина|desyatina]] (an area of land approximately equal to one [[hectare]]).\n\nBecause of the different types of sokha, land owned by lower classes was worth less towards the tax, and most of the tax was paid by the peasants.  Given the same amount of land, they paid twice as much as government officials, and 1.5 times as much as church officials. \n\nIn order to find out how many sokhas were in the state, a census measured and counted the rented lands. Quarters of land that were planted for personal use were free from payment. Until the beginning of the 17th century, a peasant who lived and worked on the land owned by a ''government'' employee, just like a peasant living on their own land or state land, paid equally for each exploited quarter of land. This section of land was called ''living''. At the beginning of the 17th century, the term ''living quarter'' was used not for the actual quarter of worked land, but for several whole farms in which more than a single quarter was planted. \n\nThis was done in the interest of government officials - their lands were used to establish ''living quarters'' which constituted 10-16 yards. This means that if every yard was divided into 4 quarters (two desyatinas), then such a ''living quarter'' was then 40 to 64 quarters of ''actual'' quarters of utilized land. Therefore, 40 to 64 quarters of government official land paid the same amount of tax as one peasant paid from the single quarter that he lived on (whether self- or state-owned). \n\nIn 1678–1679 the sokha was replaced with the ''yard count'', meaning taxes were to be paid for the number of peasant yards, rather than the area of the land occupied by the yards.\n\nTaxes from small villages was also collected in sokhas. The term sokha remained but now it referred to a certain number of yards/houses. There were several types of sokha again: ''best'', ''average'', ''lesser'', and ''worst'' villagers. Sokha of the best villages was equal to 40 yards, average - 80, lesser - 160, and worst - 320 yards. Every sokha was charged the same amount of tax. Therefore, best villages paid the same amount for half the number of farms than the average ones. If there were not enough lesser or worst sokhas, then the tax was charged from the people that lived in average or best sokhas.\n\n== Literature==\n* Knyazkov S. Stories from history of Peter the Great and his time.\n* Pushkino: Culture, 1990 (reprint of 1914 issue).\n\n[[Category:Units of area]]\n[[Category:Geography of Russia]]"
    },
    {
      "title": "Square (unit)",
      "url": "https://en.wikipedia.org/wiki/Square_%28unit%29",
      "text": "{{about|a definite unit of area|other units |square (disambiguation)}}\n{{Infobox unit\n| bgcolor      = \n| name         = square\n| image        = \n| caption      = \n| standard     = \n| quantity     = [[area]]\n| symbol       = square\n| symbol2      = \n| namedafter   = \n| extralabel   = \n| extradata    = \n| units1 = [[square foot]]\n| inunits1     = 100\n| units2 = [[square metre]]\n| inunits2     = 9.290304\n}}\nThe '''square''' is an [[Imperial unit]] of [[area]] that is used in the construction industry in the [[United States]] and Canada,<ref>{{cite web|title=What Is A \"Square\" In Roofing? |date=March 31, 2016 |website=Dallas Fort Worth Roofing Blog |url=http://www.dfwbestroofing.com/blogs/post/What-Is-A-Square-In-Roofing |accessdate=2016-04-01}}</ref> and was historically used in [[Australia]]. One square is equal to 100 [[Square foot|square feet]]. Examples where the unit is used are roofing shingles, metal roofing, vinyl siding, and fibercement siding products. Some home builders use squares as a unit in floor plans to customers.\n\nWhen used in reference to material that is applied in an overlapped fashion, such as roof shingles or siding, a square refers to the amount of material needed to cover 100 square feet when installed according to a certain lap pattern.  For example, for a shingle product designed to be installed so that each course has 5\" of exposure, a square would actually consist of more than 100 square feet of shingles in order to allow for overlapping of courses to yield the proper exposed surface.\n\nBuildings in Australia no longer use the square as a unit of measure, and has been replaced by [[square metre]]s. The measurement was often used by estate agents to make the building sound larger as the measure includes the areas outside under the eaves, and so cannot be directly compared to the internal floor area.{{Fact|date=April 2008}} Residential buildings in the state of [[Victoria, Australia]] are sometimes still advertised in squares.\n\n== Conversions ==\n1 square equals\n* 100 [[square feet]]\n* 9.290304 [[square metres]]\n\n== See also ==\n* [[List of unusual units of measurement]]\n\n==References==\n{{Reflist}}\n\n[[Category:Architecture of Australia]]\n[[Category:Units of area]]"
    },
    {
      "title": "Square foot",
      "url": "https://en.wikipedia.org/wiki/Square_foot",
      "text": "{{References|date=October 2018}}{{Comparison_area_units.svg}}\nThe '''square foot''' (plural '''square feet'''; abbreviated sq. ft, sf, ft<sup>2</sup>) is an [[imperial unit]] and [[U.S. customary unit]] (non-[[SI]], non-[[metric system|metric]]) of [[area]], used mainly in the United States and partially in Bangladesh, Canada, Ghana, Hong Kong, India, Malaysia, Nepal, Pakistan, Singapore and the United Kingdom.{{citation needed|date=August 2013}} It is defined as the area of a [[Square (geometry)|square]] with sides of 1 [[foot (length)|foot]]. \n\nAlthough the pluralisation is regular in the noun form, when used as an adjective, the singular is preferred. So, a flat measuring 700 square feet could be described as a 700 square-foot flat. This corresponds to common linguistic usage of ''foot.''\n\n== Conversions ==\n1 square foot is equivalent to:\n* 144 [[square inch]]es (sq in)\n* {{frac|1|9}} [[square yard]] (sq yd)\n* ≈0.09290304 [[square meter]]s (symbol: m<sup>2</sup>)\n\n1 [[acre]] is equivalent to 43,560 square feet.\n\n== See also ==\n{{div col}}\n* [[Area (geometry)]]\n* [[Conversion of units]]\n* [[Cubic foot]]\n* [[Metrication in Canada]]\n* [[Miscellaneous Technical (Unicode)]] for a list of miscellaneous technical symbols and fonts which support the ''square foot'' symbol\n* [[Orders_of_magnitude_(area)#10%E2%88%928_to_10%E2%88%921_square_metres|Orders of magnitude (area)]]\n* [[Square (algebra)]], [[square root]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Square Foot}}\n[[Category:Units of area]]\n[[Category:Imperial units]]"
    },
    {
      "title": "Square inch",
      "url": "https://en.wikipedia.org/wiki/Square_inch",
      "text": "{{Other uses|Square (disambiguation)}}\n{{Unreferenced|date=December 2009}}\nA '''square inch''' (plural: '''square inches''') is a [[Units of measurement|unit]] of [[area]], equal to the area of a [[Square (geometry)|square]] with sides of one [[inch]].  The following symbols are used to denote square inches:\n*square in\n*sq inches, sq inch, sq in\n*inches/-2, inch/-2, in/-2\n*inches^2, inch^2, in^2\n*inches<sup>2</sup>, inch<sup>2</sup>, in<sup>2</sup>\nor in some cases  ″^2\n*historic engineering drawings <sup>□</sup>″ (number with a square & a double apostrophe, both as an exponent)\n\nThe square inch is a common unit of measurement in the [[United States]] and the [[United Kingdom]].\n\n==Equivalence with other units of area==\n1 square inch (assuming an international inch) is equal to: (the overbars indicate [[repeating decimal]]s)\n* {{Nowrap|0.006 9{{Overbar|4}}}} [[square feet]] (1 square foot is equal to 144 square inches)\n* {{Nowrap|0.000 7{{Overbar|71 604 9382}}}} [[square yard]]s (1 square yard is equal to 1,296 square inches)\n* {{Nowrap|6.4516}} [[square centimetre]]s (1 square centimetre is equal to {{Nowrap|0.155 000 310&hellip;}} square inches)\n* {{Nowrap|0.000 645 16}} [[square metre]]s (1 square metre is equal to {{Nowrap|1,550.003 100&hellip;}} square inches)\n\n{{DEFAULTSORT:Square Inch}}\n[[Category:Units of area]]\n[[Category:Imperial units]]\n[[Category:Customary units of measurement in the United States]]\n\n[[ja:インチ#平方インチ]]"
    },
    {
      "title": "Square kilometre",
      "url": "https://en.wikipedia.org/wiki/Square_kilometre",
      "text": "'''Square [[kilometre]]''' ([[American and British English spelling differences#-re, -er|International spelling]] as used by the [[International Bureau of Weights and Measures]]) or '''square kilometer''' (American spelling), symbol '''km<sup>2</sup>''', is a multiple of the [[square metre]], the  [[International System of Units|SI]] [[Units of measurement|unit]] of [[area]] or [[surface area]].\n\n1&nbsp;km<sup>2</sup> is equal to:\n* 1,000,000 square metres (m<sup>2</sup>)\n* 100 [[hectare]]s (ha)\nIt is also approximately equal to:\n* 0.3861 [[square mile]]s <ref>There are 0.386102159 international square miles in a square kilometer while there are 0.386100614 US Survey square miles in the same measure. This is because the US Survey measures are very slightly larger than the international measures. {{cite web |url=http://www.unitconversion.org/unit_converter/area-s.html |title=UnitConversion.org Area Converter|year=2009 |work=web page |publisher=UnitConversion.org |accessdate=4 July 2012}}</ref>\n* 247.1 [[acre]]s <ref>There are 247.105381467 international acres in a square kilometre while there are only 247.104393047 of the very slightly larger US Survey acres.  {{cite web |url=http://www.unitconversion.org/unit_converter/area-s.html |title=UnitConversion.org Area Converter|year=2009 |work=web page |publisher=UnitConversion.org |accessdate=4 July 2012}}</ref>\n\nConversely:\n*1&nbsp;m<sup>2</sup> = 0.000001 (10<sup>−6</sup>) km<sup>2</sup>\n*1 hectare = 0.01 (10<sup>−2</sup>) km<sup>2</sup>\n*1 square mile = {{gaps|2.5899|km<sup>2</sup>}} <ref>An International square mile equals 2.58998811|km<sup>2</sup> while the slightly larger US Survey square mile equals 2.58999847|km<sup>2</sup>.  {{cite web |url=http://www.unitconversion.org/unit_converter/area-s.html |title=UnitConversion.org Area Converter|year=2009 |work=web page |publisher=UnitConversion.org |accessdate=4 July 2012}}</ref>\n*1 acre = about {{gaps|0.004047|km<sup>2</sup>}} <ref>1 acre (International) = {{gaps|0.004046856|km<sup>2</sup>}} while 1 acre (US Survey) = {{gaps|0.004046873|km<sup>2</sup>}} {{cite web|url=http://www.unitconversion.org/unit_converter/area-s.html |title=UnitConversion.org Area Converter|year=2009 |work=web page |publisher=UnitConversion.org |accessdate=13 June 2012}}</ref>\n\nThe symbol \"km<sup>2</sup>\" means (km)<sup>2</sup>, square kilometre or kilometre squared and not k(m<sup>2</sup>), kilo–square metre. For example, 3&nbsp;km<sup>2</sup> is equal to {{gaps|3|×|(1,000|m)<sup>2</sup>}} = 3,000,000&nbsp;m<sup>2</sup>, not 3,000&nbsp;m<sup>2</sup>.\n\n==Examples of areas of 1 square kilometre==\n\n===Topographical Map grids===\n[[File:Caldey Island map 1952.jpg|thumb|300px|Part of an Ordnance Survey map, published 1952. The grid lines are at one kilometre intervals giving each square an area of one square kilometre. The map shows that the area of the island is about two square kilometres.]]Topographical map grids are worked out in metres, with the grid lines being 1,000 metres apart.\n*  1:100,000 maps are divided into squares representing 1&nbsp;km<sup>2</sup>, each square on the map being one square centimetre in area and representing 1&nbsp;km<sup>2</sup> on the surface of the earth.\n*  For 1:50,000 maps, the grid lines are 2&nbsp;cm apart. Each square on the map is 2&nbsp;cm by 2&nbsp;cm (4&nbsp;cm<sup>2</sup>) and represents 1&nbsp;km<sup>2</sup> on the surface of the earth.\n*  For 1:25,000 maps, the grid lines are 4&nbsp;cm apart. Each square on the map is 4&nbsp;cm by 4&nbsp;cm (16&nbsp;cm<sup>2</sup>) and represents 1&nbsp;km<sup>2</sup> on the surface of the earth.\nIn each case, the grid lines enclose one square kilometre.\n\n===Medieval city centres===\n\n[[File:Dimensions of the old city of Delft.svg|260px|right|thumb|Map of [[Delft]], Netherlands dated 1659.  The walls enclosed an area of about 1 square kilometre]]\nThe area enclosed by the walls of many European medieval cities were about one square kilometre. These walls are often either still standing or the route they followed is still clearly visible, such as in [[Brussels]], where the wall has been replaced by a ring road, or in [[Frankfurt-am-Main|Frankfurt]], where the wall has been replaced by gardens. The approximate area of the old walled cities can often be worked out by fitting the course of the wall to a [[rectangle]] or an oval ([[ellipse]]). Examples include\n*[[Delft]], Netherlands (See map alongside) {{coord|52|0|54|N|4|21|34|E}}\n:The walled city of Delft was approximately rectangular.\n::The approximate length of rectangle was about {{convert|1.30|km|mi}}.<ref name=GE>Measurements taken from [[Google Earth]]</ref>\n::The approximate width of the rectangle was about {{convert|0.75|km|mi}}.<ref name=GE/>\n::A perfect rectangle with these measurements has an area of 1.30×0.75 = 0.9&nbsp;km<sup>2</sup>\n\n*[[Lucca]] (Italy) {{coord|43|50|38|N|10|30|2|E}}\n:The medieval city is roughly rectangular with rounded north-east and north-west corners.\n::The maximum distance from east to west is {{convert|1.36|km|mi}}.<ref name=GE/>\n::The maximum distance from north to south is {{convert|0.80|km|mi}}.<ref name=GE/>\n::A perfect rectangle of these dimensions would be 1.36×0.80 = 1.088&nbsp;km<sup>2</sup>.\n\n*[[Brugge]] (Belgium) {{coord|51|12|39|N|3|13|28|E}}\n:The medieval city of Brugge, a major centre in [[Flanders]], was roughly oval or [[ellipse|elliptical]] in shape with the longer or [[Semi-major axis|semi-major]] axis running north and south.\n::The maximum distance from north to south (semi-major axis) is {{convert|2.53|km|mi}}.<ref name=GE/>\n::The maximum distance from east to west (semi-minor axis) is {{convert|1.81|km|mi}}.<ref name=GE/>\n::A perfect ellipse of these dimensions would be 2.53 × 1.81 × (π/4) = 3.597&nbsp;km<sup>2</sup>.\n\n*[[Chester]] United Kingdom {{coord|53|12|1|N|2|52|45|W}}\n:Chester is one of the smaller English cities that has a near-intact city wall.<ref>{{cite web\n|url = http://www.chesterwalls.info/chestermap.html\n|title = Chester: A Virtual Stroll around the Walls\n|first = Steve\n|last = Howe\n|accessdate = 7 October 2012}}</ref>\n::The distance from [[Northgate, Chester|Northgate]] to [[Watergate, Chester|Watergate]] is about 855&nbsp;metres.<ref name=GE/>\n::The distance from [[Eastgate and Eastgate Clock|Eastgate]] to Westgate is about 589&nbsp;metres.<ref name=GE/>\n::A perfect rectangle of these dimensions would be (855/1000) × (589/1000) = 0.504&nbsp;km<sup>2</sup>.\n\n===Parks===\nParks come in all sizes; a few are almost exactly one square kilometre in area. Here are some examples:\n* [[Riverside Country Park]], UK.<ref>{{cite web |url=http://www.medway.gov.uk/environmentandplanning/countrysidesites/riversidecountrypark.aspx |title=Medway Council Riverside Country Park |year=2012 |work=web page |publisher=Medway Council |accessdate=9 June 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120419013431/http://www.medway.gov.uk/environmentandplanning/countrysidesites/riversidecountrypark.aspx |archivedate=19 April 2012 |df= }}</ref>\n* Brierley Forest Park, UK.<ref>{{cite web |url=http://www.ashfield-dc.gov.uk/ccm/navigation/leisure-and-culture/tourism-and-heritage/country-parks-and-trails/brierley-forest-park-visitor-centre-/;jsessionid=F08C5878102681F9392CAAFF7EAB8DF7 |title=Ashfield Brierley Forest Park |year=2012 |work=web page |publisher=Ashfield District Council |accessdate=9 June 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20121108034331/http://www.ashfield-dc.gov.uk/ccm/navigation/leisure-and-culture/tourism-and-heritage/country-parks-and-trails/brierley-forest-park-visitor-centre-/;jsessionid=F08C5878102681F9392CAAFF7EAB8DF7 |archivedate=8 November 2012 |df= }}</ref>\n* [[Rio de Los Angeles State Park]], California, USA <ref>{{cite web |url=http://www.parks.ca.gov/?page_id=22277 |title=California Department of Parks and Recreation Rio de Los Angeles State Park |year=2012 |work=web page |publisher=State of California|accessdate=9 June 2012}}</ref>\n* Jones County Central Park, Iowa, USA.<ref>{{cite web |url=http://www.jonescountytourism.com/parks/centralpark.html |title=Parks & Recreation Central Park, Center Junction, Iowa |work=web page |publisher=Jones County Tourism Association |accessdate=9 June 2012}}</ref>\n* Kiest Park, Dallas, Texas, USA <ref>{{cite web |url=http://friendsofoakcliffparks.org/kiesthistory.htm |title=A History of Kiest Park |year=2010 |work=web page |publisher=Friends of Oak Cliff Parks |accessdate=9 June 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120605053208/http://friendsofoakcliffparks.org/kiesthistory.htm |archivedate=5 June 2012 |df= }}</ref>\n* Hole-in-the-Wall Park & Campground, Grand Manan Island, Bay of Fundy, New Brunswick, Canada <ref>{{cite web |url=http://www.grandmanancamping.com/maps.php |title=Hole-in-the-Wall Park & Campground  |year=2012 |work=web page |publisher=Hole-in-the-Wall Park & Campground|accessdate=9 June 2012}}</ref>\n* Downing Provincial Park, British Columbia, Canada <ref>{{cite web |url=http://www.env.gov.bc.ca/bcparks/explore/parkpgs/downing/ |title=BC Parks Downing Provincial Park |work=web page |publisher=British Columbia Ministry of Environment |accessdate=9 June 2012}}</ref>\n* Citadel Park, Poznan, Poland <ref>{{cite web |url=http://culture.poland.travel/poznan/citadel-park |title=Citadel Park |work=web page |publisher=Poland Travel |accessdate=9 June 2012}}</ref>\n* Sydney Olympic Park, Sydney, Australia, contains 6.63 square kilometres of wetlands and waterways.<ref>{{cite web |url=http://www.sopa.nsw.gov.au/media/pdfs/publications/fact_sheets/parklands_fact_sheet |title=Sydney Olympic Park Parklands Fact Sheet |work=web page |publisher=Sydney Olympic Park Authority |accessdate=9 June 2012}}</ref>\n\n===Golf courses===\nUsing the figures published by golf course architects Crafter and Mogford, a course should have a fairway width of 120 metres and 40 metres clear beyond the hole. Assuming a {{convert|6000|m|yd}} 18-hole course, an area of 80 hectares (0.8 square kilometre) needs to be allocated for the course itself.<ref>{{cite web\n|url = http://www.golfstrategies.com.au/graphics/articles/FS.6.Safety.pdf\n|title = Golf Course Safety\n|publisher = Crafter + Mogford, golf course architects\n|accessdate = 5 October 2012}}</ref><ref group=Note>Assume that each hole requires (6000÷18 + 40) = 373 metres in length. The area needed is (18 × 373 × 120 ÷ 10,000) = 80.64 ha (1 hectare = 10,000 square metres).</ref>  Examples of golf courses that are about one square kilometre include:\n* Manchester Golf Club, UK <ref name=\"Manchester Golf Club Ltd\">{{cite web |url=http://www.mangc.co.uk/visitors.htm |title=Visitor Information [Manchester Golf Club] |work=web page |publisher=Manchester Golf Club Ltd |accessdate=11 June 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120525133739/http://www.mangc.co.uk/visitors.htm |archivedate=25 May 2012 |df= }}</ref>\n*  Northop Country Park, Wales, UK <ref name=\"Manchester Golf Club Ltd\"/>\n*  The Trophy Club, Lebanon, Indiana, US <ref>{{cite web |url=http://thetrophyclubgolf.com/ |title=Welcome to the Trophy Club |work=web page |publisher=The Trophy Club |accessdate=11 June 2012}}</ref>\n* Qingdao International Country Golf Course, Qingdao, Shandong, China\n* Arabian Ranches Golf Club, Dubai <ref>{{cite web |url=http://www.7daysindubai.com/sport-and-leisure/golf-courses/arabian-ranches-golf-club/business-15488895-detail/business.html |archive-url=https://web.archive.org/web/20120908003417/http://www.7daysindubai.com/sport-and-leisure/golf-courses/arabian-ranches-golf-club/business-15488895-detail/business.html |archive-date=2012-09-08 |title=Arabian Ranches Golf Club |work=7 Days in Dubai |publisher= Catchpole Communications FZ-LLC, Al Sidra Media LLC. |accessdate=1 April 2018}}</ref>\n* Sharm el Sheikh Golf Courses: Sharm el Sheikh, South Sinai, Egypt <ref>{{cite web |url=http://www.sharm-el-sheikh.world-guides.com/sharm_el_sheikh_golf.html|title=Sharm el Sheikh Golf Courses: Sharm el Sheikh, South Sinai, Egypt |work=web page |publisher=TravelSmart Ltd: World Guides |accessdate=11 June 2012}}</ref>\n* Belmont Golf Club, Lake Macquarie, NSW, Australia <ref>{{cite web |url=http://www.belmontgolf.com.au/layouts/mp_standard/Template.aspx?page=History|title=Belmont Golf Club, Lake Macquarie, History |work=web page |publisher=Belmont Golf Club |accessdate=11 June 2012}}</ref>\n\n===Other areas of one square kilometre or thereabouts===\n* [[Old City (Jerusalem)|The Old City of Jerusalem]] is almost 1 square kilometre in area.<ref>{{cite web |url=https://www.jewishvirtuallibrary.org/jsource/vie/Jerusalem2.html |title=Jerusalem - The Old City |year=2012  |work=web page |publisher=The American-Israeli Cooperative Enterprise |accessdate=6 July 2012}} Actually, about 89 hectares.</ref>\n*Milton Science Park, Oxfordshire, UK.<ref>{{cite web |url=http://www.sciencevale.com/milton/ |title=Science Vale UK |work=web page |publisher= Abbey House|accessdate=7 July 2012}}\n</ref>\n* Mielec Industrial Park,  Mielec, Poland <ref>{{cite web |url=http://www.paiz.gov.pl/investment_support/industrial_and_technology_parks/mielec |title=Invest in Poland |date= |work=web page |publisher=Polish information and foreign investment agency  |accessdate=7 July 2012}}</ref>\n* The Guildford Campus of Guildford Grammar School, South Guildford, Western Australia<ref>{{cite web |url=http://www.waterhall.com.au/lifestyle/history/guildford |archive-url=https://web.archive.org/web/20130410123141/http://www.waterhall.com.au/lifestyle/history/guildford |archive-date=10 April 2013 |title=Guildford Grammar School|work=waterhall.com.au |access-date=2 April 2018 }}</ref>\n* Sardar Vallabhbhai National Institute of Technology (SVNIT), Surat, India <ref>{{cite web |url=http://www.svnit.ac.in/conferences/fmfp2012/about%20us.html |title=39th National Conference on Fluid Mechanics and Fluid Power |work=web page |publisher=Department of Mechanical Engineering, Sardar Vallabhbhai National Institute of Technology (SVNIT) |accessdate=7 July 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120719081320/http://svnit.ac.in/conferences/fmfp2012/about%20us.html |archivedate=19 July 2012 |df= }}</ref>\n* Île aux Cerfs Island,  near the east coast of Mauritius.<ref>{{cite web |url=http://www.amity.edu/mauritius/MauritiusAttraction.asp |title=MauritiusAttraction |date=2011 |website=Amity Institute of Higher Education, Mauritius |publisher=Amity Institute of Higher Education |accessdate=27 June 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20131012085638/http://www.amity.edu/Mauritius/MauritiusAttraction.asp |archivedate=12 October 2013 |df= }}</ref>\n*[[Peng Chau]] Island, Hong Kong<ref>{{cite web|url=http://e-cgo.org.hk/travel/en/trip-detail?id=10|title=Peng Chau<Leisure Line - Barrier-Free Information Website - Hong Kong Federation of Handicapped Youth|website=e-cgo.org.hk}}</ref>\n\n==See also==\n*[[Conversion of units]]\n*[[SI prefix]] for the precise meaning of the prefix \"k\"\n*[[Square Kilometre Array]], a proposed [[radio telescope]] in [[South Africa]] or [[Australia]], which is intended to have a collecting area of approximately 1&nbsp;km<sup>2</sup>\n\n==Notes==\n{{Reflist|group = Note}}\n\n==References==\n<references/>\n\n[[Category:Units of area]]\n[[Category:SI derived units]]"
    },
    {
      "title": "Square metre",
      "url": "https://en.wikipedia.org/wiki/Square_metre",
      "text": "{{Redirect|m^2||m² (disambiguation)}}\n{{Comparison_area_units.svg}}\nThe '''square metre''' ([[American and British English spelling differences#-re, -er|international spelling]] as used by the [[International Bureau of Weights and Measures]]) or '''square meter''' ([[American and British English spelling differences#-re, -er|American spelling]]) is the [[SI derived unit]] of [[area]] with symbol '''m<sup>2</sup>'''.<ref>{{citation | editor=David R. Lide | chapter=INTERNATIONAL SYSTEM OF UNITS | title=[[CRC Handbook of Chemistry and Physics]] | edition=90th | year=2010}}</ref>\n\nAdding and subtracting [[SI prefix]]es creates multiples and submultiples; however, as the unit is [[exponentiate]]d, the [[quantities]] [[geometric growth|grow geometrically]] by the corresponding [[power of 10]]. For example, a [[kilometre]] is 10<sup>3</sup> (a [[thousand]]) times the length of a metre, but a square kilometre is 10<sup>3<sup>2</sup></sup> (10<sup>6</sup>, a [[million]]) times the area of a square metre, and a cubic kilometre is 10<sup>3<sup>3</sup></sup> (10<sup>9</sup>, a [[billion]]) cubic metres.\n\n==SI prefixes applied==\nThe square metre may be used with all SI prefixes used with the metre.\n\n{|class=\"wikitable\"\n! Multiplication        !! Name       !! Symbol\n\n|rowspan=\"12\"|\n! Multiplication        !! Name       !! Symbol\n|-\n|10<sup>0</sup>\n| '''square metre''' ([[Hectare#Centiare|centiare]])\n|'''m<sup>2</sup>'''\n|10<sup>0</sup>\n| '''square metre''' ([[Hectare#Centiare|centiare]])\n|'''m<sup>2</sup>'''\n|-\n|10<sup>2</sup>|| square decametre ([[Hectare#Are|are]]) || dam<sup>2</sup>\n|10<sup>−2</sup>|| square decimetre || dm<sup>2</sup>\n|-\n|10<sup>4</sup>|| square hectometre ([[hectare]])|| hm<sup>2</sup> \n|10<sup>−4</sup>|| square centimetre|| cm<sup>2</sup>\n|-\n|10<sup>6</sup>|| [[square kilometre]]|| km<sup>2</sup>\n|10<sup>−6</sup>|| square millimetre|| mm<sup>2</sup>\n|-\n|10<sup>12</sup>|| square megametre|| Mm<sup>2</sup>\n|10<sup>−12</sup>|| square micrometre|| µm<sup>2</sup>\n|-\n|10<sup>18</sup>|| square gigametre|| Gm<sup>2</sup>\n|10<sup>−18</sup>|| square nanometre|| nm<sup>2</sup>\n|-\n|10<sup>24</sup>|| square terametre  || Tm<sup>2</sup>\n|10<sup>−24</sup>|| square picometre|| pm<sup>2</sup>\n|-\n|10<sup>30</sup>|| square petametre|| Pm<sup>2</sup>\n|10<sup>−30</sup>|| square femtometre|| fm<sup>2</sup>\n|-\n|10<sup>36</sup>|| square exametre|| Em<sup>2</sup>\n|10<sup>−36</sup>|| square attometre|| am<sup>2</sup>\n|-\n|10<sup>42</sup>|| square zettametre|| Zm<sup>2</sup>\n|10<sup>−42</sup>|| square zeptometre|| zm<sup>2</sup>\n|-\n|10<sup>48</sup>|| square yottametre|| Ym<sup>2</sup>\n|10<sup>−48</sup>|| square yoctometre|| ym<sup>2</sup>\n|-\n|-\n|}\n\n== Unicode characters ==\n{| class=\"wikitable\"\n! colspan=\"3\" style=\"font-weight:bold;\" |Square metre: Unicode characters.<ref name=\"Unicode-U3300\">{{cite web |url=https://www.unicode.org/charts/PDF/U3300.pdf |access-date=May 24, 2019 |title=The Unicode Standard 12.0 – CJK Compatibility ❰ Range: 3300—33FF ❱ |author=Unicode Consortium |author-link=Unicode Consortium |date=2019 |website=Unicode.org}}</ref>\n|-\n| style=\"font-weight:bold;\" | Symbol\n| style=\"font-weight:bold;\" | Name\n| style=\"font-weight:bold;\" | Unicode number\n|-\n| ㎡\n| Square metre (Square M Squared)\n| U+33A1\n|-\n| ㎢\n| Square kilometre (Square KM Squared)\n| U+33A2\n|-\n| ㎠\n| Square centimetre (Square CM Squared)\n| U+33A0\n|-\n| ㎟\n| Square millimetre (Square MM Squared)\n| U+339F\n|}\n\n==Conversions==\n\nA square metre is equal to:\n{{Div col|colwidth=19em}}\n* {{val|0.000001}} [[square kilometre]] (km<sup>2</sup>)\n* {{val|10000}} square centimetres (cm<sup>2</sup>)\n* {{val|0.0001}} [[hectare]]s (ha)\n* {{val|0.001}} decares (daa)\n* {{val|0.01}} [[hectare#Are|are]]s (a)\n* {{val|0.1}} deciares (da)\n* {{val|1}} [[centiare]] (ca)\n* {{val|0.000247105381}} [[acre]]s\n* {{val|0.024710538}} [[Acre|cent]]s\n* {{val|1.195990}} [[square yard]]s\n* {{val|10.763911}} [[square foot|square feet]]\n* {{val|1550.0031}} [[square inch]]es\n{{Div col end}}\n\n==See also==\n*[[Conversion of units#Area|Conversion of units § Area]]\n*[[Orders of magnitude (area)]]\n*[[SI]]\n*[[SI prefix]]\n\n== Notes ==\n{{Reflist}}\n\n== External links ==\n*[http://www.bipm.fr/en/si/ BIPM (SI maintenance agency)] (home page)\n*[http://www.bipm.org/en/si/si_brochure/ BIPM brochure] (SI reference)\n\n[[Category:Units of area]]\n[[Category:SI derived units]]"
    },
    {
      "title": "Square mil",
      "url": "https://en.wikipedia.org/wiki/Square_mil",
      "text": "{{Unreferenced|date=December 2009}}\nA '''square mil''' is a [[Units of measurement|unit]] of [[area]], equal to the area of a square with sides of length one [[Thou (unit of length)|mil]]. A mil is one thousandth of an international [[inch]]. This unit of area is usually used in [[Area of a circle|specifying the area]] of the cross section of a wire or cable.\n\n==Equivalence to other units of area==\n1 square mil is equal to:\n* 1 millionth of a square inch (1 square inch is equal to 1 million square mils)\n* 6.4516×10<sup>−10</sup> square [[metre]]s\n* about 1.273 [[circular mil]]s (1 circular mil is equal to about 0.7854 square mils). Where 1.273 is (1÷(π÷4)) and where 0.7854 is (π÷4).\n\n==See also==\n* [[Circular mil]]\n* [[Area of a circle]]\n\n{{DEFAULTSORT:Square Mil}}\n[[Category:Units of area]]\n[[Category:Decimalisation]]"
    },
    {
      "title": "Square mile",
      "url": "https://en.wikipedia.org/wiki/Square_mile",
      "text": "{{Other uses}}\n\nThe '''square mile''' (abbreviated as '''sq&nbsp;mi''' and sometimes as '''mi²''')<ref name=\"RussRowlett\">Rowlett, Russ (September 1, 2004). [http://www.unc.edu/~rowlett/units/dictS.html \"S\", ''How Many? A Dictionary of Units of Measurement'']. [[University of North Carolina at Chapel Hill]]. Retrieved February 22, 2012.</ref> is an [[Imperial system|imperial]] and [[US customary system|US unit]] of measure for an [[area]] equal to the area of a [[square (geometry)|square]] with a side length of one [[mile|statute mile]].<ref Name=\"Davies\">{{cite book \n|title= Mathematical dictionary and cyclopedia of mathematical science\n|last= Davies\n|first= Charles \n|authorlink= \n|coauthors= \n|year= 1872\n|publisher= A.S. Barnes and co\n|location= Original from Harvard University\n|isbn= \n|page= 582 \n|url= https://books.google.com/books?id=DoUMAAAAYAAJ }}</ref> It should not be confused with ''miles square'', which refers to a square region with each side having the specified length. For instance, 20 miles square (20 × 20 miles) has an area equal to 400 square miles; a rectangle of 10 × 40 miles likewise has an area of 400 square miles, but it is not 20 miles square. \n\nOne square mile is equal to:\n*4,014,489,600 [[square inch]]es<ref name=\"Cardarelli2003\">{{cite book|author=François Cardarelli|title=Encyclopaedia of scientific units, weights, and measures: their SI equivalences and origins|url=https://books.google.com/books?id=GV1aV7V7I00C&pg=SA3-PA39|accessdate=22 February 2012|year=2003|publisher=Springer|isbn=978-1-85233-682-0|page=3}}</ref>\n*27,878,400 [[square foot|square feet]]<ref name=\"Cardarelli2003\" />\n*3,097,600 [[square yard]]s<ref name=\"Cardarelli2003\" />\n*640 [[acre]]s<ref name=\"RussRowlett\" />\n*2560 [[rood (measurement)|rood]]s<ref name=\"Zupko1985\">{{cite book|last=Zupko|first=Ronald Edward|authorlink=Ronald Edward Zupko|title=A dictionary of weights and measures for the British Isles: the Middle Ages to the twentieth century|url=https://books.google.com/books?id=0l_k-XMIiQIC&pg=PA353|accessdate=22 February 2012|year=1985|publisher=American Philosophical Society|isbn=978-0-87169-168-2|page=353}}</ref>\n\nSince one inch has been standardized to 2.54&nbsp;cm by international agreement, a square mile is equivalent to the following [[metric system|metric]] measures:\n*25,899,881,103.36 [[square centimetre]]s\n*2,589,988.110336 [[square metre]]s\n*258.9988110336 [[hectare]]s\n*2.589988110336 [[square kilometre]]s\n\nWhen applied to a portion of the earth's surface, \"square mile\" is an informal synonym for [[Section (United States land surveying)|section]].\n\nRomans derived measurements from marching. Five feet was equal to one pace (which is appropriately a double step). One thousand paces measured a Roman mile, which was somewhat smaller than the English statute mile. This Roman system was adopted, with local variations, throughout Europe as the Roman Empire spread.<ref>{{cite web |title=The history of measurement |url=http://www-history.mcs.st-and.ac.uk/HistTopics/Measurement.html |access-date=9 August 2015}}</ref>\n\nTotal population in a square mile is derived by dividing the total number of residents by the number of square miles of land area in the specified geographic area. The population per square kilometer is derived by multiplying the population per square mile by 0.3861.<ref name=\"Cardarelli\">{{cite web |work=Census.gov |title=Land Area and Persons Per Square Mile |url=http://quickfacts.census.gov/qfd/meta/long_LND110210.html|access-date=9 August 2015}}{{dead link|date=March 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\nLand area measurements are originally recorded as whole square meters. Square meters are converted to square kilometers by dividing by 1,000,000; square kilometers are converted to square miles by dividing by 2.58999; square meters are converted to square miles by dividing by 2,589,988.<ref name=\"Cardarelli\"/>\n\n==References==\n{{Reflist}}\n\n* [http://www.formulaconversion.com/formulaconversioncalculator.php?convert=squaremiles_to_acres Formula Conversion, Square miles to numerous area units with algebraic steps and unit cancellation shown]\n\n{{DEFAULTSORT:Square Mile}}\n[[Category:Units of area]]\n[[Category:Customary units of measurement in the United States]]\n[[Category:Imperial units]]\n\n\n{{Measurement-stub}}"
    },
    {
      "title": "Square yard",
      "url": "https://en.wikipedia.org/wiki/Square_yard",
      "text": "{{refimprove|date=December 2009}}\n{{Comparison_area_units.svg}}\nThe '''square yard''' ([[Indian English|India]]: '''[[guz|gaj]]''') is an [[imperial unit]] of [[area]], formerly used in most of the [[English language|English]]-speaking world but now generally replaced by the [[square metre]], however it is still in widespread use in the U.S., Canada, the U.K. and India. It is defined as the area of a [[square (geometry)|square]] with sides of one [[yard]] (three [[Foot (length)|feet]], thirty-six [[inch]]es, 0.9144 [[metre]]s) in [[length]].\n\n==Symbols==\nThere is no universally agreed symbol but the following are used:\n*square yards, square yard, square yds, square yd \n*sq yards, sq yard, sq yds, sq yd, sq.yd.\n*yards/-2, yard/-2, yds/-2, yd/-2\n*yards^2, yard^2, yds^2, yd^2\n*yards², yard², yds², yd²\n\n==Conversions==\nOne square yard is equivalent to:\n* 1,296 [[square inch]]es\n* 9 [[square foot|square feet]]\n* ≈0.00020661157 [[acre]]s\n* ≈0.000000322830579 [[square mile]]s\n* 836 127.36 square millimetres\n* 8 361.2736 [[square centimetre]]s\n* 0.83612736 [[square metre]]s\n* 0.000083612736 [[hectare]]s\n* 0.00000083612736 [[square kilometre]]s\n* 1.00969 [[Guz|gaj]] <ref>{{cite web|title=google search|url=https://www.google.co.in/search?num=100&newwindow=1&q=1+sq+yard+%3D+gaj&oq=1+square+yard+%3D+gaj&gs_l=serp.1.5.0i71l8.0.0.0.50302.0.0.0.0.0.0.0.0..0.0....0...1c..64.serp..0.0.0.xrqFdeYz1KA}}</ref>\n\n==See also==\n{{div col}}\n* [[1 E-1 m²]] for a comparison with other areas\n* [[Area (geometry)]]\n* [[Conversion of units]]\n* [[Cubic yard]]\n* [[Metrication in Canada]]\n* [[Orders of magnitude (area)]]\n* [[Square (algebra)]], [[Square root]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Square Yard}}\n[[Category:Units of area]]\n[[Category:Imperial units]]\n[[Category:Customary units of measurement in the United States]]"
    },
    {
      "title": "Stremma",
      "url": "https://en.wikipedia.org/wiki/Stremma",
      "text": "The '''stremma''' ({{abbr|pl.|plural}}&nbsp;stremmata; {{lang-grc-gre|στρέμμα}}, ''strémma'') is a [[Greece|Greek]] unit of land [[area]] now equal to precisely 1,000 [[square meter]]s. Its area can be visualized as equal to the area enclosed by a square having side length of 31.62 meters, or 103.7 imperial [[Foot_(unit)|feet]]. \n__NOTOC__\n==History==\nThe [[Ancient Greece|ancient Greek]] [[Greek units|equivalent]] was the [[square&nbsp;plethron]], which served as the Greeks' form of the [[acre (unit)|acre]]. It was originally defined as the distance plowed by a team of [[ox]]en in a day<ref>{{Citation |last=Pryce |first=Frederick Norman |author2-last=Lang |author2-first=Mabel L. |author3-last=Vickers |author3-first=Michael |display-authors=1 |editor-last=Spawforth |editor-first=Antony |editor2-last=Hornblower |editor2-first=Simon |editor3-last=Eidinow |editor3-first=Esther |display-editors=0 |contribution=measures |contribution-url=https://books.google.co.uk/books?id=bVWcAQAAQBAJ&pg=PA917 |title=The Oxford Classical Dictionary, ''4th&nbsp;ed.'' |p=917 |location=Oxford |publisher=Oxford University Press |date=2012 |isbn=978-0-19-954556-8 |ref={{harvid|Pryce|2012}} }}.</ref> but nominally standardized as the area enclosed by a square 100&nbsp;[[Greek feet]] (''[[pous]]'') to a side. This area was also used as the size of a [[Greek wrestling]] square.\n\nThe '''[[Byzantine units|Byzantine]]''' or '''[[Morea]]n&nbsp;stremma''' continued to vary depending on the period and the quality of the land, but usually enclosed an area between {{convert|900|-|1900|m2|sp=us|abbr=on}}.<ref>Siriol Davis, \"Pylos Regional Archaeological Project, Part VI: administration and settlement in Venetian Navarino\", ''Hesperia'', Winter, 2004 [http://www.findarticles.com/p/articles/mi_m0SDG/is_1_73/ai_n13493303/pg_17]</ref> It was originally also known as the \"plethron\" but this was eventually replaced by \"stremma\", derived from the verb for \"turning\" the ground with the simple Byzantine plow.<ref>Λεξικό της κοινής Νεοελληνικής (Dictionary of Modern Greek), Ινστιτούτο Νεοελληνικών Σπουδών, Θεσσαλονίκη, 1998. {{ISBN|960-231-085-5}}</ref>\n\nThe '''old''', '''[[Turkish units of measurement|Turkish]]''', or '''[[Ottoman units of measurement|Ottoman]]&nbsp;stremma''' is the Greek (and occasionally English) name for the \"[[dunam]]\", which probably derived from the Byzantine unit.<ref>V.L. Ménage, Review of Speros Vryonis, Jr. ''The decline of medieval Hellenism in Asia Minor and the process of islamization from the eleventh through the fifteenth century'', Berkeley, 1971; in ''Bulletin of the School of Oriental and African Studies'' (University of London) '''36''':3 (1973), pp. 659-661. [https://www.jstor.org/stable/613605 at JSTOR (subscription required)]; see also Erich Schilbach, ''Byzantinische Metrologie''.</ref> Again, this varied by region: ''The Dictionary of Modern Greek'' gives a value of {{convert|1,270|m2|sp=us|abbr=on}},<ref>Λεξικό, 1998</ref> but [[Costas Lapavitsas]] used the value of 1,600 m<sup>2</sup> for the region of [[Naousa, Imathia|Naoussa]] in the early 20th century.<ref>[[Costas Lapavitsas]], {{cite web|url=http://eprints.soas.ac.uk/8828/1/Lapavitsas4112005.pdf |journal=''Ηλεκτρονικό Δελτίο Οικονομικής Ιστορίας'' |title=Social and Economic Underpinning of Industrial Development: Evidence from Ottoman Macedonia |accessdate=2012-08-29|df= }}</ref>\n\n==Conversions==\nOne modern stremma is equivalent to:\n\n;Metric\n* 1,000 [[square metres]]\n* 10 [[hectare#Are|are]]s\n* 1 [[decare]]\n* 0.1 [[hectare]]\n* 0.001 [[square kilometre]]s\n\n;Imperial\n* 10,763.9 [[square foot|square feet]]\n* 0.247 105 38 [[acre]]s\n* 0.247 104 39 [[U.S. survey acre]]s (1 acre ≈ 4.047 stremmata)\n* 0.000 386 102 [[square mile]]s\n\n==See also==\n*[[1 E3 m²]] for further comparisons\n* [[Byzantine units]]\n*[[Conversion of units]]\n* [[Greek units]]\n* Streema ([[streaming radio]])\n\n==Bibliography==\n\n{{reflist}}\n\n[[Category:Units of area]]\n[[Category:Human-based units of measurement]]\n[[Category:Metricated units]]"
    },
    {
      "title": "Survey township",
      "url": "https://en.wikipedia.org/wiki/Survey_township",
      "text": "{{distinguish|civil township}}\n[[File:Western Reserve Including the Fire Lands 1826.jpg|thumb|250px|right|upright=2|1826 map of the [[Connecticut Western Reserve]] in northern [[Ohio]] showing both survey and civil townships. The survey townships are represented by the numbers (horizontal \"town\" and vertical \"range\" numbers), and the [[civil township]]s using the same boundaries are represented by the names.]]\n[[File:Warren County, Indiana map from 1877 atlas.png|thumb|250px|right|upright=2|1877 map of [[Warren County, Indiana]]. Of the civil townships shown on this map, only [[Pine Township, Warren County, Indiana|Pine Township]] exactly matches a survey township with 36 sections.]]\n'''Survey township''', sometimes called '''Congressional township''', as used by the [[United States]] [[Public Land Survey System]], refers to a square [[Conversion of units#Area|unit]] of land, that is nominally six (U.S. Survey) [[mile]]s (~9.7&nbsp;km) on a side. Each 36-square-mile (~93&nbsp;km<sup>2</sup>) township is divided into 36 one-square-mile (~2.6&nbsp;km<sup>2</sup>) [[Section (land)|sections]], that can be further subdivided for sale, and each section covers a nominal {{Convert|640|acre|km2}}. The townships are referenced by a numbering system that locates the township in relation to a [[principal meridian]] (north-south) and a [[Baseline (surveying)|base line]] (east-west). For example, Township 2 North, Range 4 East is the 4th township east of the principal meridian and the 2nd township north of the base line. Township (exterior) lines were originally surveyed and platted by the US [[General Land Office]] using contracted private survey crews. Later survey crews subdivided the townships into sections (interior) lines. Virtually all lands covered by this system were sold according to these boundaries. They are marked on the [[U.S. Geological Survey]] [[topographic map]]s.\n\n== History ==\nPrior to standardization, some of the [[Ohio Lands]] (the [[United States Military District]], the [[Firelands]] and the [[Connecticut Western Reserve]]) were surveyed into townships of {{Convert|5|mi|km}} on each side. These are often known as Congressional Townships.<ref>A History of the Rectangular Survey System by C. Albert White, 1983, Pub: Washington, D.C. : U.S. Dept. of the Interior, Bureau of Land Management : For sale by Supt. of Docs., U.S. G.P.O.,</ref><ref>{{cite web |url=http://www.blm.gov/wo/st/en/prog/more/cadastralsurvey/cadastral_history.html |title=Archived copy |accessdate=2014-11-19 |deadurl=yes |archiveurl=https://web.archive.org/web/20141113090104/http://www.blm.gov/wo/st/en/prog/more/cadastralsurvey/cadastral_history.html |archivedate=2014-11-13 |df= }}</ref>\n\nSections are divided into quarter-sections of {{Convert|160|acre|ha}} each and quarter-quarter sections of {{Convert|40|acre|ha}} each. In the [[Homestead Act]] of 1862, one quarter-section of land was the amount allocated to each settler. Stemming from this are the [[idiomatic expression]]s, \"the lower 40\", which is the 40 acres on a settler's land that is lowest in elevation, in the direction towards which water drains toward a stream, and the \"[[back forty]]\", the portion farthest from the settler's dwelling.\n\n== Survey township vs. civil township ==\nSurvey townships are distinct from [[civil township]]s. A survey township is used to establish boundaries for land ownership, while a civil township is a form of [[local government]]. In states with civil townships, the two types of townships often coincide. County lines, especially in western states, usually follow survey township lines, leading to the large number of rectangular counties in the Midwest, which are agglomerations of survey townships.<ref>{{cite book|title=Geological Survey Circular|url=https://books.google.com/books?id=XfckAQAAIAAJ|year=1933|publisher=The Survey|page=[https://books.google.com/books?id=XfckAQAAIAAJ&pg=PA24 24]}}</ref>\n\nIn western Canada, the [[Dominion Land Survey]] adopted a similar format for survey townships, which do not form administrative units. These townships also have the area of 36 square miles (six miles by six miles).\n\n==See also==\n* [[Public Land Survey System]]\n* [[Township]]\n* [[Township (United States)]]\n* [[Civil township]]\n* [[Paper township]]\n* [[Dominion Land Survey]] ([[Western Canada]])\n\n==References==\n{{Reflist}}\n\n{{Terms for types of country subdivisions}}\n\n{{DEFAULTSORT:Survey Township}}\n[[Category:Land surveying of the United States]]\n[[Category:Subdivisions of the United States]]\n[[Category:Surveying]]\n[[Category:Townships of the United States]]\n[[Category:Units of area]]"
    },
    {
      "title": "Tarang wa",
      "url": "https://en.wikipedia.org/wiki/Tarang_wa",
      "text": "A '''tarang wa''' ({{lang-th|ตารางวา}}, {{RTGS|''tarang wa''}}, {{IPA-th|tāːrāːŋ wāː|IPA}}) or '''square [[wa (unit)|wa]]''', sometimes transliterated as 'waa' or 'wah' is a unit of [[area]] used in [[Thailand]] for measuring land or property.  It is defined as the area of a [[Square (geometry)|square]] whose sides measure exactly one wa (two metres), equivalent to four [[square metre]]s.<ref>{{cite web|title=Measurements in Thailand|url=http://www.thailawonline.com/en/others/measurements-in-thailand-rai-ngan-etc.html|website=Isaan Lawyers|accessdate=20 September 2016}}</ref> Although its current size is precisely derived from the metre, it is neither part of nor recognized by the modern [[metric system]], the [[SI|International System]] (SI).\n\nThe square wa equals 1/100 [[ngaan]] or 1/400 [[Rai (area)|rai]], two units of area frequently used in Thailand. It also equals 1/25 [[Hectare#Are|are]], another metre-derived unit of area not officially part of the SI.\n\nAs with many terms normally written with the [[Thai alphabet]], there are many variant [[transliteration]]s into English, e.g. '''dta-raang waa''' and '''tarang wah'''.\n\n== See also ==\n* [[Thai units of measurement]]\n* [[Orders of magnitude (area)]] for a comparison with other areas\n\n==References==\n{{reflist}}\n\n== External links==\n* [http://www.convert-me.com/en/convert/area Area metric conversion], British and U.S., Japanese, Chinese, Thai, old French, others.\n\n[[Category:Geography of Thailand]]\n[[Category:Units of area]]\n[[Category:Thai words and phrases]]"
    },
    {
      "title": "Tatami",
      "url": "https://en.wikipedia.org/wiki/Tatami",
      "text": "{{short description|Straw mat used as flooring in Japan}}\n{{about|the Japanese flooring|the Japanese armour|Tatami-dō}}\n{{Italic title|reason=[[:Category:Japanese words and phrases]]}}\n[[Image:Youkoukan06n4592.jpg|thumb|right|Room with tatami flooring in an inauspicious layout and [[shōji]] ]]\nA {{nihongo|'''''tatami'''''|畳}} is a type of [[mat]] used as a flooring material in traditional [[Japan]]ese-style rooms. Traditionally made using [[rice]] [[straw]]  to form the core, the cores of contemporary tatami are sometimes composed of compressed [[Woodchips|wood chip]] boards or [[polystyrene|polystyrene foam]]. With a covering of woven {{nihongo|[[Juncus effusus|soft rush]]|藺草|igusa}} [[straw]], tatami are made in standard sizes, with the length exactly twice the width, an [[aspect ratio]] of 2:1. Usually, on the long sides, they have {{nihongo|edging|縁|heri}} of [[brocade]] or plain cloth, although some tatami have no edging.<ref>{{cite web |url=http://www.kyo-tatami.com/english/about|title=Understanding Tatami|accessdate=2016-10-31 |publisher=[[Motoyama Tatami shop]] }}</ref> In martial arts the tatami is the floor of the training ground in a [[dojo]] and also the floor for competition within a martial arts tournament.{{cn|date=January 2019}}\n\n==History==\n[[Image:Men Making Tatami Mats, 1860 - ca. 1900.jpg|thumb|Men making tatami mats, late 19th century.]]\nThe term ''tatami'' is derived from the verb {{nihongo|''tatamu''|畳む}}, meaning to fold or pile. This indicates that the early tatami were thin and could be folded up when not used or piled in layers.<ref name=\"KoandaTatami\">Kodansha Encyclopedia of Japan, entry for \"tatami.\"</ref> Tatami were originally a luxury item for the nobility. During the [[Heian period]], when the [[shinden-zukuri]] architectural style of aristocratic residences was consummated, the flooring of shinden-zukuri palatial rooms were mainly wooden, and tatami were only used as seating for the highest aristocrats.<ref name=\"Sato Osamu 1994\"/> In the [[Kamakura period]], there arose the [[shoin-zukuri]] architectural style of residence for the samurai and priests who had gained power. This architectural style reached its peak of development in the [[Muromachi period]], when tatami gradually came to be spread over whole rooms, beginning with small rooms. Rooms completely spread with tatami came to be known as {{nihongo|''zashiki''|座敷||{{lit.}} room spread out for sitting}}, and rules concerning seating and etiquette determined the arrangement of the tatami in the rooms.<ref name=\"Sato Osamu 1994\"/> It is said that prior to the mid-16th century, the ruling nobility and samurai slept on tatami or woven mats called {{nihongo|''goza''|茣蓙}}, while commoners used straw mats or loose straw for bedding.<ref>Kodansha Encyclopedia of Japan, entry for \"bedding\"</ref>\n\nThe lower classes had mat-covered earth floors.<ref name=\"tatamiluxury\">{{cite web |url=http://www.yoshinoantiques.com/Interior-article.html |title=The Yoshino Newsletter |accessdate=2007-03-28 |work=Floors/Tatami |publisher=[[Yoshino Japanese Antiques]] |deadurl=yes |archiveurl=https://web.archive.org/web/20070331133235/http://www.yoshinoantiques.com/Interior-article.html |archivedate=2007-03-31 |df= }}</ref>\n\nTatami were gradually popularized and finally reached the homes of commoners toward the end of the 17th century.<ref name=\"tatamicommoners\">{{cite web |url=http://www.kcif.or.jp/archive/en/newsletter/lik/archives/0304/04_2003.htm |title=Kyoto International Community House Newsletter |accessdate=2007-03-28 |work=2nd section titled History of tatami |publisher=[[Kyoto City International Foundation]] }}</ref>\n\nHouses built in Japan today often have very few tatami-floored rooms, if any. Having just one is not uncommon. The rooms having tatami flooring and other such traditional architectural features are referred to as ''nihonma'' or ''[[washitsu]]'', \"Japanese-style rooms\".\n\n==Size==\nThe size of tatami traditionally differs between regions in Japan:\n\n* [[Kyoto]]: 0.955&nbsp;m by 1.91&nbsp;m, called {{nihongo||京間|Kyōma}} tatami\n* [[Nagoya]]: 0.91&nbsp;m by 1.82&nbsp;m, called {{nihongo||合の間|Ainoma|{{lit.}} \"in-between\" size}} tatami\n* [[Tokyo]]: 0.88&nbsp;m by 1.76&nbsp;m, called {{nihongo||江戸間|Edoma}} or {{nihongo||関東間|Kantōma}} tatami\n\nIn terms of thickness, 5.5&nbsp;cm is average for a ''Kyōma'' tatami, while 6.0&nbsp;cm is the norm for a ''Kantōma'' tatami.<ref name=\"Sato Osamu 1994\">Sato Osamu, \"A History of Tatami,\" in Chanoyu Quarterly no. 77 (1994).</ref> A half mat is called a {{nihongo||半畳|hanjō}}, and a mat of three-quarter length, which is used in tea-ceremony rooms (''[[chashitsu]]''), is called ''daimedatami'' ({{lang|ja|大目}} or {{lang|ja|台目}}).<ref name=\"KoandaTatami\"/> In terms of [[Japanese units of measurement#Length|traditional Japanese length units]], a tatami is (allowing for regional variation) 1&nbsp;''[[Ken (unit)|ken]]'' by 0.5&nbsp;''ken'', or equivalently 6&nbsp;''[[Shaku (unit)|shaku]]'' by 3&nbsp;''shaku''&nbsp;– formally this is {{convert|1.81818|×|0.90909|m|ft}}, the size of Nagoya tatami. Note that a ''shaku'' is almost the same length as one&nbsp;foot in the traditional English-American measurement system.\n\n[[Image:Tatami layout 1.svg|thumb|One possible auspicious layout of a {{frac|4|1|2}}&nbsp;mat room {{legend|#ff7f7f|Half mat}}{{legend|#4992ff|Full mat}}]]\n\nIn Japan, the size of a room is often measured by the number of {{nihongo|''tatami'' mats|-畳|-jō}}, about 1.653&nbsp;square&nbsp;meters (for a standard Nagoya size tatami). Alternatively, in terms of [[Japanese units of measurement#Area|traditional Japanese area units]], room area (and especially house floor area) is measured in terms of ''[[tsubo]],'' where one ''tsubo'' is the area of two tatami mats (a square); formally 1&nbsp;''ken'' by 1&nbsp;''ken'' or a 1.81818&nbsp;meter square, about 3.306&nbsp;square meters.\n\nSome common room sizes in the Nagoya region are:\n*{{frac|4|1|2}} mats = 9 shaku × 9 shaku ≈ 2.73&nbsp;m × 2.73&nbsp;m\n*6 mats = 9 shaku × 12 shaku ≈ 2.73&nbsp;m × 3.64&nbsp;m\n*8 mats = 12 shaku × 12 shaku ≈ 3.64&nbsp;m × 3.64&nbsp;m\n\nShops were traditionally designed to be {{frac|5|1|2}}&nbsp;mats, and [[chashitsu|tea rooms]] are frequently {{frac|4|1|2}}&nbsp;mats.{{Citation needed|date=July 2009}}\n\n==Layout==\nThere are rules concerning the number of tatami mats and the layout of the tatami mats in a room. In the [[Edo period]], {{nihongo|\"auspicious\"|祝儀敷き|shūgijiki}} tatami arrangements and {{nihongo|\"inauspicious\"|不祝儀敷き|fushūgijiki}} tatami arrangements were distinctly differentiated, and the tatami accordingly would be rearranged depending on the occasion. In modern practice, the \"auspicious\" layout is ordinarily used. In this arrangement, the junctions of the tatami form a \"T\" shape; in the \"inauspicious\" arrangement, the tatami are in a grid pattern wherein the junctions form a \"+\" shape.<ref name=\"Sato Osamu 1994\"/> An auspicious tiling often requires the use of {{frac|1|2}} mats to tile a room.<ref name=\"tilings\">{{cite book|last1=Erickson|first1=Alejandro|title=Computing and Combinatorics|volume=6196|pages=288–297|last2=Ruskey|first2=Frank|last3=Woodcock|first3=Jennifer|last4=Schurch|first4=Mark|publisher=Springer|doi=10.1007/978-3-642-14031-0_32|series=Lecture Notes in Computer Science|year=2010|isbn=978-3-642-14030-3|arxiv=1103.3309}}</ref>\n\nAn inauspicious layout is said to bring bad fortune.<ref>{{Cite journal|last=Kalland|first=Arne|date=April 1999|title=Houses, People and Good Fortune: Geomancy and Vernacular Architecture in Japan|journal=Worldviews|volume=3|issue=1|pages=33–50|jstor=43809122|doi=10.1163/156853599X00036}}</ref>\n\n[[Image:Tearoom layout.svg|thumb|Typical layout of a {{frac|4|1|2}}&nbsp;mat tea room in the cold season, when the hearth built into the floor is in use. The room has a ''[[tokonoma]]'' and ''[[Mizuya#The special mizuya dōko|mizuya dōko]]'']]\n\n==See also==\n* [[Higashiyama Bunka]] in [[Muromachi period]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{commons category-inline|Tatami}}\n\n{{Japanese architectural elements}}\n\n{{Authority control}}\n\n[[Category:Floors]]\n[[Category:Japanese architectural features]]\n[[Category:Units of area]]\n[[Category:Interior design]]\n[[Category:Japanese home]]"
    },
    {
      "title": "Tetrad (area)",
      "url": "https://en.wikipedia.org/wiki/Tetrad_%28area%29",
      "text": "A '''tetrad''' is an area 2&nbsp;km x 2&nbsp;km square. The term has a particular use in connection with the British [[Ordnance Survey]] [[British national grid reference system|national grid]], and then refers to any of the 25 such squares which make up a standard [[hectad (unit of area)|hectad]].<ref name=\"KMBRC\">{{cite web|url=http://www.kmbrc.org.uk/recording/help/gridrefhelp.php?page=6|title=DINTY Tetrads|publisher=Kent and Medway Biological Records Centre|accessdate=23 December 2008|deadurl=yes|archiveurl=https://web.archive.org/web/20110527152140/http://www.kmbrc.org.uk/recording/help/gridrefhelp.php?page=6|archivedate=27 May 2011|df=}}</ref>\n\nTetrads are sometimes used by biologists for reporting the distribution of species to maintain a degree of confidentiality about their data,<ref>{{cite web|url=http://www.sxbrc.org.uk/biodiversity/recording/ngr.php|title=National Grid References|date=2008|publisher=Sussex Biodiversity Record Centre|accessdate=23 December 2008}}</ref> though the system is not in universal use.<ref name=\"KMBRC\" />\n\nThe tetrads are labelled from ''A'' to ''Z'' (omitting ''O'') according to the \"DINTY\" system as shown in the grid below, which takes its name from the letters of the second line.<ref name=\"KMBRC\" />\n\n{| border=\"1\" cellpadding=\"5\" cellspacing=\"0\"\n| E\n| J\n| P\n| U\n| Z\n|- \n| D\n| I \n| N \n| T\n| Y\n|- \n| C\n| H\n| M\n| S\n| X\n|- \n| B\n| G\n| L\n| R\n| W\n|- \n| A\n| F\n| K\n| Q\n| V\n|}\n\n==References==\n{{Reflist}}\n\n[[Category:Units of area]]\n\n\n{{Cartography-stub}}\n{{measurement-stub}}"
    },
    {
      "title": "Toise",
      "url": "https://en.wikipedia.org/wiki/Toise",
      "text": "A '''toise''' ({{IPA-fr|twaz}}; symbol: '''T''') is a unit of measure for [[length]], [[area]] and [[volume]] originating in pre-revolutionary France.  In North America, it was used in colonial French establishments in early [[New France]], [[Louisiana (New France)|French Louisiana]] (''Louisiane''), and [[Quebec]]. The related '''''toesa''''' ({{IPA-pt|tuˈezɐ}}) was used in [[Portugal]], [[Brazil]] and other parts of the [[Portuguese Empire]] until the adoption of the [[Metric system]].\n\n==Definition==\n===Unit of length===\n* 1 Toise was divided in  6 ''[[French units of measurement|pied]]s'' (feet) or 72 [[Pouce|''pouces'']] (inches) or 864 [[Units of measurement in France before the French Revolution|''lignes'']] (lines) in France until 1812.\n\n:In 1799 the meter was defined to be exactly 443.296 [[ligne|lignes]] or 13 853 / 27 000 Toise, with the intention that the meter should equal 1/10 000 000 of the distance from the pole to the equator. This had the effect of making the Toise ≈ 1949.03631mm.\n\n:According to an article written in 1866, during measurement of various standard length artifacts from several countries, the toise was measured as 1,949.03632&nbsp;mm.<ref>{{Cite journal |first1=A. R. |last1=Clarke |first2=Henry |last2=James |year=1867 |title=Abstract of the Results of the Comparisons of the Standards of Length of England, France, Belgium, Prussia, Russia, India, Australia, Made at the Ordnance Survey Office, Southampton |journal=Philosophical Transactions of the Royal Society of London |volume=157 |pages=161&ndash;180 |doi=10.1098/rstl.1867.0010}}</ref>{{Rp|180}}\n\n:Since before 1394, the standard for the Toise of Paris was an iron bar embedded in the wall of the [[Grand Châtelet]]. But a little before 1667 the pillar in which the standard was embedded bent and distorted the standard. In 1667 officials constructed a new standard, but there were complaints that the new standard was about 0.5% shorter than the previous one. Nevertheless, the new standard was mandated. The old standard was since called \"toise de l'Ecritoire\"\n\n:From 1668 to 1776 the French standard of length was the Toise of Châtelet which was fixed outside the Grand Châtelet in Paris. In 1735 two geodetic standards were calibrated against the Toise of Châtelet. One of them, the Toise of Peru, was used for the [[French Geodesic Mission|Spanish-French Geodesic Mission]]. In 1766 the Toise of Peru became the official standard of length in France and was renamed Toise of the [[French Academy of Sciences|Academy]] (French: ''Toise de l'Académie''). In 1799, after the remeasurement of the [[Paris meridian]] arc (French: ''Méridienne de France'') between [[Dunkirk]] and [[Barcelona]] by [[Jean Baptiste Joseph Delambre|Delambre]] and [[Pierre Méchain|Mechain]], the metre was defined as 3 ''pieds'' (feet) and 11.296 ''lignes'' (lines) of the Toise of the Academy.<ref>{{Cite news|url=https://www.entreprises.gouv.fr/metrologie/histoire-metre|title=Histoire du mètre|work=Direction Générale des Entreprises (DGE)|access-date=2017-12-27}}</ref>\n\n\n* 1 Toise was exactly 2 metres in France between 1812 and 1 January 1840 ([[mesures usuelles]]).\n* 1 Toise = 1.8 metres in [[Switzerland]].\n* 1 ''Toesa'' = 6 ''[[Portuguese customary units|pés]]'' (feet) = 1.98 m in Portugal.\n\n===Unit of area===\n* 1 Toise was about 3.799 square metres or, of course, a square French toise, as a measure for land and masonry area in [[France]] before 10 December 1799.\n\n===Unit of volume===\n* 1 Toise = 8.0 [[cubic metre]]s (20th century [[Haiti]])\n\n==Origin==\nHistorical [[France|French]] unit.  Early [[Louisiana]] in the [[United States]].\n\n==See also==\n* [[Units of measurement in France before the French Revolution]]\n* [[Portuguese customary units]]\n* [[Fathom]] and [[Klafter]], similar units\n\n==References==\n{{reflist}}\n==External links==\n*[http://www.sizes.com/units/toise.htm Sizes.com ''toise'' page]\n*[http://www.sizes.com/units/majsource.htm#UN1966 Reference from UN] United Nations. Department of Economic and Social Affairs. Statistical Office of the United Nations\n\n[[Category:Units of length]]\n[[Category:Units of area]]\n[[Category:Obsolete units of measurement]]\n[[Category:Units of volume]]"
    },
    {
      "title": "Vergée",
      "url": "https://en.wikipedia.org/wiki/Verg%C3%A9e",
      "text": "[[File:Almanach Nouvelle Chronique de Jersey 1891 vergees acres conversion table.jpg|thumb|Tables of conversion between Jersey vergées and English acres from an 1891 almanac]]\n\nA '''vergée''' ({{IPA-fr|vɛʁʒe}}, alternative spellings '''vergie''',  '''vrégie''') is a unit of land area, a quarter of the old French ''[[arpent]]''.  The term derives from Latin ''virga'' (rod).<ref name=\"Cooper\">{{cite news|last1=Cooper|first1=Trevor|title=Out in the fields of gold|url=https://guernseypress.com/news/property/2012/04/19/out-in-the-fields-of-gold/|accessdate=6 December 2017|work=Guernsey Press|date=19 April 2012}}</ref>  Compare [[French language|French]] ''verge'' (yard).\n\nIn the [[Channel Islands]], it is a standard measure of land, but the statutory definition differs between the [[bailiwick]]s.\n\n*In [[France]], a vergée was 12,100 square Paris feet (1,276.8 m²), equal to 25 square perches. The [[surveying]] perch measured 22 French feet. \n*In [[French North America]], it was also equal to 25 square perches, but the royal perch of 18 feet was used, yielding a ''vergée'' of 8100 square feet (854.7 m²) \n*In [[Guernsey]], a vergée ([[Guernésiais]]: vergie) is 17,640 square feet (1,639 m²). It is 40 (square) Guernsey perches. A Guernsey perch (also spelt perque) is 21 feet by 21 feet.<ref name=\"Cooper\" /><ref>{{cite book|last1=De Garis|first1=Marie|title=Dictiounnaire Angllais-Guernésiais|date=1 January 1982|publisher=Phillimore & Co Ltd|isbn=978-0850334623}}</ref>\n*In [[Jersey]], a vergée ([[Jèrriais]]: vrégie) is 19,360 square feet (1,798.6 m²). It is 40 (square) Jersey perches. A Jersey perch (also spelt pèrque) is a square 24 ''pied de perche'' on each side (i.e. a square 22 imperial feet on each side).<ref>[http://www.gov.je/EconomicDevelopment/TradingStandards/Weights+and+Measures/Unusual+Jersey+Measures.htm Unusual Jersey Measures] {{webarchive |url=https://web.archive.org/web/20060721143239/http://www.gov.je/EconomicDevelopment/TradingStandards/Weights+and+Measures/Unusual+Jersey+Measures.htm |date=July 21, 2006 }}</ref>\n[[File:Don Jeanne Gruchy Sainte Mathie Jèrri.jpg|thumb|An inscribed stone describes this 11 vergée 25 perch ''clos des pauvres'' in Jersey]]\n\n==Conversions==\n1 vergée (Guernsey) is equivalent to:\n*1 638.80963 m²\n*0.404958678 acres \n\n1 vergée (Jersey) is equivalent to:\n*1,798.60285 m²\n*0.444444444 acres\n\n== See also ==\n*[[Units of measurement in France before the French Revolution]]\n*[[Virgate]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Vergee}}\n[[Category:Channel Islands]]\n[[Category:Customary units of measurement]]\n[[Category:Units of area]]\n[[Category:Standards of France]]"
    },
    {
      "title": "Virgate",
      "url": "https://en.wikipedia.org/wiki/Virgate",
      "text": "{{hatnote|'Yardland' redirects here. Not to be confused with [[yard (land)]].<br>For the use of 'virgate' in reference to rod-like stems and ribs, see [[virgate (botany)]].}}\n{{Anthropic_Farm_Units}}\nThe '''virgate''', '''yardland''', or '''yard of land''' ({{lang-la|virgāta}} [''{{lang|la|terrae}}'']) was an [[English units|English unit]] of land. Primarily a measure of [[tax assessment]] rather than [[area]], the virgate was usually (but not always) reckoned as {{1/4}}&nbsp;[[hide (unit)|hide]] and notionally (but seldom exactly) equal to 30 [[acre]]s. It was equivalent to two of the [[Danelaw]]'s [[oxgang]]s.\n__NOTOC__\n{{anchor|Etymology}}\n\n==Name==\nThe name derives from the Old English ''{{lang|ang|gyrd landes}}'' (\"yard of land\"),<ref>''Oxford English Dictionary'', 1st&nbsp;ed. \"yardland, ''n.''\". Oxford University Press (Oxford), 1921.</ref> from \"yard\"'s former meaning as a measuring stick employed in reckoning [[acre]]s (cf. [[rod (unit)|rod]]). The word is etymologically unrelated to the [[yard (land)|yard]] of land around a dwelling.<ref>''Oxford English Dictionary'', 1st&nbsp;ed. \"yard, ''n.<sup>2</sup>''\". Oxford University Press (Oxford), 1921.</ref> \"Virgate\" is a much later [[retronym]], [[anglicizing]] the yardland's [[Latinisation of names|latinized]] form ''virgāta'' after the advent of the [[yard]] rendered the original name ambiguous.<ref>''Oxford English Dictionary'', 1st&nbsp;ed. \"virgate, ''n.''\". Oxford University Press (Oxford), 1917.</ref>\n\n==History==\nThe virgate was reckoned as the amount of land that a team of two [[oxen]] could plough in a single annual season. It was equivalent to a quarter of a [[Hide (unit)|hide]], so was nominally thirty [[Acre#Historical origin|acres]].<ref>D. Hey ed., ''Oxford Companion to Local and Family History'' (Oxford University Press, Oxford, 1996), 476.</ref> In some parts of England, it was divided into four nooks ({{lang-enm|noke}}; {{lang-lat-med|noca}}).<ref>\"Noca - nook (measure of land)\" R. W. Latham, ''Revised Medieval Latin Word-list'' (Oxford University Press, London: for British Academy 1965), 312.</ref>  Nooks were occasionally further divided into a farundel ({{lang-enm|ferthendel}}; {{lang-ang|fēorþan dǣl}}, \"fourth deal, fourth share\").<ref>{{cite book |title=An Anglo-Saxon Dictionary |last=Bosworth |first=Joseph |authorlink=Joseph Bosworth |author2=T. Northcote Toller  |year=1882 |publisher=Oxford University Press |page=281 |url=https://books.google.com/?id=oXlii1KgDngC&printsec=frontcover#v=onepage&q=&f=false}}</ref>\n\nThe [[Danelaw]] equivalent of a virgate was two [[oxgang]]s or ‘bovates’.<ref>Stephen Friar, ''Batsford Companion to Local History'' (Batsford, London 1991), 270.</ref> These were considered to represent the amount of land that could be worked in a single annual season by a single ox and therefore equated to half a virgate. As such, the oxgang represented a parallel division of the [[carucate]].\n\n==References==\n{{Reflist}}\n\n[[Category:Units of area]]\n[[Category:Obsolete units of measurement]]"
    },
    {
      "title": "Volok (unit)",
      "url": "https://en.wikipedia.org/wiki/Volok_%28unit%29",
      "text": "'''Volok''' ({{lang-lt|valakas}}, {{lang-pl|włóka}}, {{lang-ru|Волока}}) was a late medieval unit of land measurement in the [[Grand Duchy of Lithuania]], [[Kingdom of Poland (Jagiellon)|Kingdom of Poland]] and later, the [[Polish-Lithuanian Commonwealth]]. It was equal, on average, to {{convert|21.368|ha}} in Lithuania or to {{convert|17.955|ha}} in Poland.<ref name=tarvy/> It was subdivided into 30 or 33 [[morgen]]s.<ref name=baltr/> \n\nVolok was also a unit that determined taxation and other duties to the state.<ref name=baltr/> Previously, taxes and duties were based on the number of households (see, for example, [[1528 census of the Grand Duchy of Lithuania]] that recorded households as a measure of military duty) or number of horses/bulls needed to work the land. Such system was not exact: households varied greatly in size and in wealth. Therefore, the introduction of voloks marked an important transition to taxes based on area (width times length).<ref name=baltr/>\n\nIn Lithuania, it was introduced during the [[Volok Reform]] that began in 1547. Officially, voloks were abandoned as a unit after the [[emancipation reform of 1861]], but survived in everyday use until the introduction of the [[metric system]] in 1921.<ref name=gudav/> \n\n==References==\n{{reflist|refs=\n<ref name=baltr>{{cite journal |first=Aleksandras |last=Baltrūnas |title=Kaip senovėje lietuviai žemę matuodavo |language=lt |url=http://ausis.gf.vu.lt/mg/nr/2000/056/5mat.html |archiveurl=https://web.archive.org/web/20081204081258/http://ausis.gf.vu.lt/mg/nr/2000/056/5mat.html |archivedate=2008-12-04 |journal=Mokslas Ir Gyvenimas |year=2000 |volume=5–6 |issue=509–510 |issn=0134-3084}}</ref>\n<ref name=gudav>{{cite encyclopedia |url=https://www.vle.lt/Straipsnis/valakas-99080 |title=Valakas |first=Edvardas |last=Gudavičius |encyclopedia=Visuotinė lietuvių enciklopedija |publisher=Mokslo ir enciklopedijų leidybos centras |language=lt |date=2013-12-16}}</ref>\n<ref name=tarvy>{{cite book |url=http://www.lzuu.lt/file.doc?id=17317 |format=PDF |first=Marytė Elena |last=Tarvydienė |title=Žemėtvarkos pagrindai |publisher=[[Lithuanian University of Agriculture]] |year=2007 |page=29 |language=lt |isbn=978-9955-896-11-1 |access-date=2008-10-31 |archive-url=https://web.archive.org/web/20110722151716/http://www.lzuu.lt/file.doc?id=17317 |archive-date=2011-07-22 |dead-url=yes |df= }}</ref>\n}}\n\n\n[[Category:Units of area]]\n[[Category:Obsolete units of measurement]]\n[[Category:Polish–Lithuanian Commonwealth]]\n[[Category:Grand Duchy of Lithuania]]\n{{Lithuania-stub}}\n{{Poland-stub}}"
    },
    {
      "title": "1 + 2 + 4 + 8 + ⋯",
      "url": "https://en.wikipedia.org/wiki/1_%2B_2_%2B_4_%2B_8_%2B_%E2%8B%AF",
      "text": "In [[mathematics]], '''{{nowrap|1 + 2 + 4 + 8 + ⋯}}''' is the [[infinite series]] whose terms are the successive [[powers of two]]. As a [[geometric series]], it is characterized by its first term, 1, and its [[Geometric series#Common ratio|common ratio]], 2. As a series of [[real number]]s it [[divergent series|diverges]] to [[infinity]], so in the usual sense it has no sum. In a much broader sense, the series is associated with another value besides ∞, namely −1, which is the limit of the series using the [[p-adic numbers|2-adic metric]].\n\n==Summation==\nThe partial sums of {{nowrap|1 + 2 + 4 + 8 + ⋯}} are {{nowrap|1, 3, 7, 15, …;}} since these diverge to infinity, so does the series. \n\n:<math>2^0+2^1+\\cdots+2^k = 2^{k+1}-1</math>\n\nTherefore, any [[totally regular summation method]] gives a sum of infinity, including the [[Cesàro summation|Cesàro sum]] and [[Abel summation|Abel sum]].<ref>Hardy p.&nbsp;10</ref> On the other hand, there is at least one generally useful method that sums {{nowrap|1 + 2 + 4 + 8 + ⋯}} to the finite value of −1. The associated [[power series]]\n\n:<math>f(x) = 1+2x+4x^2+8x^3+\\cdots+2^n{}x^n+\\cdots = \\frac{1}{1-2x}</math> \n\nhas a [[radius of convergence]] around 0 of only {{sfrac|1|2}}, so it does not converge at {{nowrap|1=''x'' = 1}}. Nonetheless, the so-defined function ''f'' has a unique [[analytic continuation]] to the [[complex plane]] with the point {{nowrap|1=''x'' = {{sfrac|1|2}}}} deleted, and it is given by the same rule {{nowrap|1=''f''(x) = {{sfrac|1|1 − 2''x''}}}}. Since {{nowrap|1=''f''(1) = −1}}, the original series {{nowrap|1 + 2 + 4 + 8 + ⋯}} is said to be summable ([[Euler summation|E]]) to −1, and −1 is the (E) sum of the series. (The notation is due to [[G. H. Hardy]] in reference to [[Leonhard Euler]]'s approach to divergent series).<ref>Hardy pp.8, 10</ref>\n\nAn almost identical approach (the one taken by Euler himself) is to consider the power series whose coefficients are all 1, i.e.\n\n:<math>1+y+y^2+y^3+\\cdots = \\frac{1}{1-y}</math>\n\nand plugging in ''y''&nbsp;=&nbsp;2. These two series are related by the substitution ''y''&nbsp;=&nbsp;2''x''.\n\nThe fact that (E) summation assigns a finite value to {{nowrap|1 + 2 + 4 + 8 + …}} shows that the general method is not totally regular. On the other hand, it possesses some other desirable qualities for a summation method, including stability and linearity. These latter two axioms actually force the sum to be −1, since they make the following manipulation valid:\n\n:<math>\\begin{array}{rcl}\ns & = &\\displaystyle 1+2+4+8+16+\\cdots \\\\\n  & = &\\displaystyle 1+2(1+2+4+8+\\cdots) \\\\\n  & = &\\displaystyle 1+2s\n\\end{array}</math>\n\nIn a useful sense, ''s''&nbsp;=&nbsp;∞ is a root of the equation {{nowrap|1=''s'' = 1 + 2''s''.}} (For example, ∞ is one of the two [[Fixed point (mathematics)|fixed point]]s of the [[Möbius transformation]] {{nowrap|1=''z'' → 1 + 2''z''}} on the [[Riemann sphere]]). If some summation method is known to return an ordinary number for ''s'', ''i.e.'' not ∞, then it is easily determined. In this case ''s'' may be subtracted from both sides of the equation, yielding {{nowrap|1=0 = 1 + ''s''}}, so {{nowrap|1=''s'' = −1}}.<ref>The two roots of {{nowrap|1=''s'' = 1 + 2''s''}} are briefly touched on by Hardy p.&nbsp;19.</ref>\n\nThe above manipulation might be called on to produce −1 outside the context of a sufficiently powerful summation procedure. For the most well-known and straightforward sum concepts, including the fundamental convergent one, it is absurd that a series of positive terms could have a negative value. A similar phenomenon occurs with the divergent geometric series [[Grandi's series|1 − 1 + 1 − 1 + ⋯]], where a series of [[integer]]s appears to have the non-integer sum {{sfrac|1|2}}. These examples illustrate the potential danger in applying similar arguments to the series implied by such [[recurring decimal]]s as 0.111… and most notably [[0.999...|0.999…]]. The arguments are ultimately justified for these convergent series, implying that {{nowrap|1=0.111… = {{sfrac|1|9}}}} and {{nowrap|1=0.999… = 1}}, but the underlying [[mathematical proof|proof]]s demand careful thinking about the interpretation of endless sums.<ref>Gardiner pp.&nbsp;93–99; the argument on p.&nbsp;95 for {{nowrap|1 + 2 + 4 + 8 + ⋯}} is slightly different but has the same spirit.</ref>\n\nIt is also possible to view this series as convergent in a number system different from the real numbers, namely, the [[P-adic number|2-adic numbers]]. As a series of 2-adic numbers this series converges to the same sum, −1, as was derived above by analytic continuation.<ref>{{cite book|author = Koblitz |first=Neal|title = p-adic Numbers, p-adic Analysis, and Zeta-Functions|series = Graduate Texts in Mathematics, vol. 58|publisher = Springer-Verlag|isbn = 0-387-96017-1|year = 1984|pages = chapter I, exercise 16, p. 20}}</ref>\n\n== See also ==\n* [[1 − 1 + 2 − 6 + 24 − 120 + · · ·]]\n* [[Grandi's series]]\n* [[1 + 1 + 1 + 1 + · · ·]]\n* [[1 − 2 + 3 − 4 + · · ·]]\n* [[1 + 2 + 3 + 4 + · · ·]]\n* [[1 − 2 + 4 − 8 + ⋯]]\n* [[Two's complement]], a data convention for representing negative numbers where −1 is represented as if it were {{nowrap|1 + 2 + 4 + ⋯ + 2<sup>''n''−1</sup>}}.\n\n== Notes ==\n{{reflist}}\n\n==References==\n{{refbegin}}\n*{{cite journal |last=Euler |first=Leonhard |authorlink=Leonhard Euler |title=De seriebus divergentibus |journal=Novi Commentarii academiae scientiarum Petropolitanae |volume=5 |year=1760 |pages=205–237 |url=http://www.math.dartmouth.edu/~euler/pages/E247.html}}\n*{{cite book |last=Gardiner |first=A. |authorlink=Anthony Gardiner (mathematician) |title=Understanding infinity: the mathematics of infinite processes |year=2002 |origyear=1982 |edition=Dover |publisher=Dover  |isbn=0-486-42538-X}}\n*{{cite book |last=Hardy |first=G. H. |authorlink=G. H. Hardy |title=Divergent Series |year=1949 |publisher=Clarendon Press |id={{LCC|QA295|.H29|1967}}}}\n{{refend}}\n\n==Further reading==\n{{refbegin}}\n*{{cite journal |author =Barbeau |first=E. J. |first2 =P. J.|last2= Leah |title=Euler's 1760 paper on divergent series |journal=Historia Mathematica |volume=3 |issue=2 |pages=141–160 |doi=10.1016/0315-0860(76)90030-6|date=May 1976}}\n*{{cite journal |last=Ferraro |first=Giovanni |title=Convergence and Formal Manipulation of Series from the Origins of Calculus to About 1730 |journal=Annals of Science |volume=59 |year=2002 |pages=179–199 |doi=10.1080/00033790010028179}}\n*{{cite journal |last=Kline |first=Morris |authorlink=Morris Kline |title=Euler and Infinite Series |journal=Mathematics Magazine |volume=56 |issue=5 |pages=307–314 |doi=10.2307/2690371 |jstor=2690371|date=November 1983}}\n*{{cite web |last=Sandifer |first=Ed |title=Divergent series |work=How Euler Did It |publisher=MAA Online |url=http://www.maa.org/editorial/euler/How%20Euler%20Did%20It%2032%20divergent%20series.pdf|format=PDF|date=June 2006}}\n*{{cite journal |last=Sierpińska |first=Anna |title=Humanities students and epistemological obstacles related to limits |journal=Educational Studies in Mathematics |volume=18 |issue=4 |pages=371–396 |doi=10.1007/BF00240986 |jstor=3482354|date=November 1987}}\n{{refend}}\n\n{{Series (mathematics)}}\n\n{{DEFAULTSORT:1 + 2 + 4 + 8 + ...}}\n[[Category:Binary arithmetic]]\n[[Category:Divergent series]]\n[[Category:Geometric series]]"
    },
    {
      "title": "Adder–subtractor",
      "url": "https://en.wikipedia.org/wiki/Adder%E2%80%93subtractor",
      "text": "{{ALUSidebar|expand=Components|expand-components=AdderSubtractor}}\n{{unreferenced|date=June 2009|bot=yes}}\n\nIn [[digital circuit]]s, an '''adder–subtractor''' is a circuit that is capable of [[adder (electronics)|adding]] or [[subtractor|subtracting]] numbers (in particular, [[binary numeral system|binary]]). Below is a circuit that does adding ''or'' subtracting depending on a control signal. It is also possible to construct a circuit that performs both addition and subtraction at the same time.\n\n==Construction==\n\n[[Image:4-bit ripple carry adder-subtracter.svg|thumb|400px|A 4-bit ripple-carry adder–subtractor based on a 4-bit adder that performs [[two's complement]] on ''A'' when {{nowrap|1=''D'' = 1}} to yield {{nowrap|1=''S'' = ''B'' − ''A''}}.]]\n\nHaving an ''n''-bit adder for ''A'' and ''B'', then {{nowrap|1=''S'' = ''A'' + ''B''}}. Then, assume the numbers are in [[two's complement]]. Then to perform {{nowrap|''B'' − ''A''}}, two's complement theory says to invert each bit with a [[NOT gate]] then add one. This yields {{nowrap|1=''S'' = ''B'' + {{overline|''A''}} + 1}}, which is easy to do with a slightly modified adder.\n\nBy preceding each ''A'' input bit on the adder with a 2-to-1 [[multiplexer]] where:\n* Input 0 (''I''<sub>0</sub>) is ''A''\n* Input 1 (''I''<sub>1</sub>) is {{overline|''A''}}\nthat has control input ''D'' that is also connected to the initial carry, then the modified adder performs\n* addition when {{nowrap|1=''D'' = 0}}, or\n* subtraction when {{nowrap|1=''D'' = 1}}.\nThis works because when {{nowrap|1=''D'' = 1}} the ''A'' input to the adder is really {{overline|''A''}} and the carry in is 1.  Adding ''B'' to {{overline|''A''}} and 1 yields the desired subtraction of {{nowrap|''B'' − ''A''}}.\n\nA way you can mark number ''A'' as positive or negative without using a multiplexer on each bit is to use an [[XOR gate]] to precede each bit instead. \n* The first input to the XOR gate is the actual input bit\n* The second input to the XOR gate for each is the control input ''D''\nThis produces the same [[truth table]] for the bit arriving at the adder as the multiplexer solution does since the XOR gate output will be what the input bit is when {{nowrap|1=''D'' = 0}} and the inverted input bit when {{nowrap|1=''D'' = 1}}.\n\n==Role in the arithmetic logic unit==\n{{main|Arithmetic logic unit}}\n\nAdders are a part of the core of an [[arithmetic logic unit]] (ALU). The [[control unit]] decides which operations an ALU should perform (based on the [[op code]] being executed) and sets the ALU operation. The ''D'' input to the adder–subtractor above would be one such control line from the control unit.\n\nThe adder–subtractor above could easily be extended to include more functions. For example, a 2-to-1 multiplexer could be introduced on each ''B<sub>i</sub>'' that would switch between zero and ''B<sub>i</sub>''; this could be used (in conjunction with {{nowrap|1=''D'' = 1}}) to yield the [[two's complement]] of ''A'' since {{nowrap|1=−''A'' = {{overline|''A''}} + 1}}.\n\nA further step would be to change the 2-to-1 multiplex on ''A'' to a 4-to-1 with the third input being zero, then replicating this on ''B<sub>i</sub>'' thus yielding the following output functions:\n* 0 (with both the ''A<sub>i</sub>'' and ''B<sub>i</sub>'' inputs set to zero and {{nowrap|1=''D'' = 0}})\n* 1 (with both the ''A<sub>i</sub>'' and ''B<sub>i</sub>'' inputs set to zero and {{nowrap|1=''D'' = 1}})\n* ''A'' (with the ''B<sub>i</sub>'' input set to zero)\n* ''B'' (with the ''A<sub>i</sub>'' input set to zero)\n* {{nowrap|''A'' + 1}} (with the ''B<sub>i</sub>'' input set to zero and {{nowrap|1=''D'' = 1}})\n* {{nowrap|''B'' + 1}} (with the ''A<sub>i</sub>'' input set to zero and {{nowrap|1=''D'' = 1}})\n* {{nowrap|''A'' + ''B''}}\n* {{nowrap|''A'' − ''B''}}\n* {{nowrap|''B'' − ''A''}}\n* {{overline|''A''}} (with ''A<sub>i</sub>'' set to invert; ''B<sub>i</sub>'' set to zero; and {{nowrap|1=''D'' = 0}})\n* −''A'' (with ''A<sub>i</sub>'' set to invert; ''B<sub>i</sub>'' set to zero; and {{nowrap|1=''D'' = 1}})\n* {{overline|''B''}} (with ''B<sub>i</sub>'' set to invert; ''A<sub>i</sub>'' set to zero; and {{nowrap|1=''D'' = 0}})\n* −'B'' (with ''B<sub>i</sub>'' set to invert; ''A<sub>i</sub>'' set to zero; and {{nowrap|1=''D'' = 1}})\n\nBy adding more logic in front of the adder, a single adder can be converted into much more than just an adder—an ALU.\n\n==See also==\n\n* [[Adder (electronics)]]\n* [[Carry-lookahead adder]]\n* [[Carry-save adder]]\n* [[Adding machine]]\n* [[Subtractor]]\n\n{{DEFAULTSORT:Adder-subtractor}}\n[[Category:Telecommunications equipment]]\n[[Category:Binary arithmetic]]\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Adjust flag",
      "url": "https://en.wikipedia.org/wiki/Adjust_flag",
      "text": "{| class=\"infobox\" style=\"font-size:88%;\"\n|-\n|\n{| style=\"font-size:88%;\"\n|- \n|colspan=\"17\" | '''Intel CPU status register'''\n|- \n|style=\"width:10px\" align=\"center\" | 15\n|style=\"width:10px\" align=\"center\" | 14\n|style=\"width:10px\" align=\"center\" | 13\n|style=\"width:10px\" align=\"center\" | 12\n|style=\"width:10px\" align=\"center\" | 11\n|style=\"width:10px\" align=\"center\" | 10\n|style=\"width:10px\" align=\"center\" | 9\n|style=\"width:10px\" align=\"center\" | 8\n|style=\"width:10px\" align=\"center\" | 7\n|style=\"width:10px\" align=\"center\" | 6\n|style=\"width:10px\" align=\"center\" | 5\n|style=\"width:10px\" align=\"center\" | 4\n|style=\"width:10px\" align=\"center\" | 3\n|style=\"width:10px\" align=\"center\" | 2\n|style=\"width:10px\" align=\"center\" | 1\n|style=\"width:10px\" align=\"center\" | 0\n|style=\"width:160px; background:white; color:black\" | (bit position)\n|-\n|- style=\"background:silver;color:black\"\n|style=\"width:10px\" align=\"center\" | -\n|style=\"width:10px\" align=\"center\" | -\n|style=\"width:10px\" align=\"center\" | -\n|style=\"width:10px\" align=\"center\" | -\n|style=\"width:10px\" align=\"center\" | [[Overflow flag|O]]\n|style=\"width:10px\" align=\"center\" | [[Direction flag|D]]\n|style=\"width:10px\" align=\"center\" | [[Interrupt flag|I]]\n|style=\"width:10px\" align=\"center\" | [[Trap flag|T]]\n|style=\"width:10px\" align=\"center\" | [[Sign flag|S]]\n|style=\"width:10px\" align=\"center\" | [[Zero flag|Z]]\n|style=\"width:10px\" align=\"center\" | -\n|style=\"width:10px\" align=\"center\" | A\n|style=\"width:10px\" align=\"center\" | -\n|style=\"width:10px\" align=\"center\" | [[Parity flag|P]]\n|style=\"width:10px\" align=\"center\" | -\n|style=\"width:10px\" align=\"center\" | [[Carry flag|C]]\n|style=\"width:160px; background:white; color:black\" | Flags\n|}\n|}\nThe '''Adjust flag''' is a [[status register|CPU flag]] in the [[FLAGS register]] of all [[x86]]-compatible [[CPU]]s, and the preceding [[Intel 8080#8080_CPU_family|8080-family]]; it is also called the '''Auxiliary flag''' and the '''Auxiliary Carry''' flag (AC). The flag bit is located at [[bit position|position]]&nbsp;4 in the CPU flag register. It indicates when an [[arithmetic]] [[Carry (arithmetic)|carry]] or borrow has been generated out of the four least significant bits, or lower nibble. It is primarily used to support [[binary-coded decimal]] (BCD) arithmetic.\n\nThe Auxiliary flag is set (to 1) if during an \"'''add'''\" operation there is a carry from the low nibble (lowest four bits) to the high nibble (upper four bits), or a borrow from the high nibble to the low nibble, in the low-order 8-bit portion, during a subtraction. Otherwise, if no such carry or borrow occurs, the flag is cleared or \"reset\" (set to 0).\n\n==See also==\n* [[Intel BCD opcode]]\n* [[Half-carry flag]]\n\n{{Microcompu-stub}}\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:X86 architecture]]"
    },
    {
      "title": "Arithmetic shift",
      "url": "https://en.wikipedia.org/wiki/Arithmetic_shift",
      "text": "<!--This article is in Commonwealth English-->\n[[Image:Rotate right arithmetically.svg|thumb|300px|A right arithmetic shift of a binary number by 1. The empty position in the most significant bit is filled with a copy of the original MSB.]]\n[[Image:Rotate left logically.svg|thumb|300px|A left arithmetic shift of a binary number by 1. The empty position in the [[least significant bit]] is filled with a zero.]]\n\n<!-- Table arranged in rough alphabetic order of shifts. -->\n{| class=\"wikitable\" style=\"float:right; clear:right;\"\n|+ Arithmetic shift operators in various programming languages and processors\n! Language or processor !! Left !! Right\n|-\n| [[ActionScript]] 3, [[Java (programming language)|Java]], [[JavaScript]], [[Python (programming language)|Python]], [[PHP]], [[Ruby (programming language)|Ruby]];<br />[[C (programming language)|C]], [[C++]],<ref>{{Cite web|url=https://tour.dlang.org/tour/en/gems/bit-manipulation|title=Bit manipulation - Dlang Tour|website=tour.dlang.org|access-date=2019-06-23}}</ref>[[D (programming language)|D]], [[C_Sharp_(programming_language)|C#]], [[Go (programming language)|Go]], [[Julia (programming language)|Julia]], [[Swift (programming language)|Swift]] (signed types only)<ref group=\"note\">The <code> >> </code> operator in C and C++ is not necessarily an arithmetic shift.  Usually it is only an arithmetic shift if used with a signed integer type on its left-hand side.  If it is used on an unsigned integer type instead, it will be a ''logical'' shift.</ref>||<code> << </code> || <code> >> </code>\n|-\n| [[Standard ML]] || <code> << </code> || <code> ~>> </code>\n|-\n| [[Verilog]] || <code> <<< </code> || <code> >>> </code><ref group=\"note\">The Verilog arithmetic right shift operator only actually performs an arithmetic shift if the first operand is signed.  If the first operand is unsigned, the operator actually performs a ''logical'' right shift.</ref>\n|-\n| [[OpenVMS]] macro language\n| colspan=\"2\" align=\"center\" | @{{#tag:ref|In the [[OpenVMS]] macro language, whether an arithmetic shift is left or right is determined by whether the second operand is positive or negative. This is unusual. In most programming languages the two directions have distinct operators, with the operator specifying the direction, and the second operand is implicitly positive. (Some languages, such as Verilog, require that negative values be converted to unsigned positive values. Some languages, such as C and C++, have no defined behaviour if negative values are used.){{sfn|HP|2001}}{{Page needed|date=March 2012}}|group=\"note\"}}\n|-\n| [[Scheme (programming language)|Scheme]]\n| colspan=\"2\" align=\"center\" | <code>arithmetic-shift</code><ref group=\"note\" name=\"scheme\">In Scheme <code>arithmetic-shift</code> can be both left and right shift, depending on the second operand, very similar to the OpenVMS macro language, although R6RS Scheme adds both <code>-right</code> and <code>-left</code> variants.</ref>\n|-\n| [[Common Lisp]]\n| colspan=\"2\" align=\"center\" | <code>ash</code>\n|-\n| [[OCaml]] || <code>lsl</code> || <code>asr</code>\n|-\n| [[Haskell (programming language)|Haskell]]\n| colspan=\"2\" align=\"center\" | <code>Data.Bits.shift</code>{{#tag:ref|The <code>Bits</code> class from Haskell's <code>Data.Bits</code> module defines both <code>shift</code> taking a signed argument and <code>shiftL</code>/<code>shiftR</code> taking unsigned arguments. These are [[isomorphic]]; for new definitions the programmer need provide only one of the two forms and the other form will be automatically defined in terms of the provided one.|group=\"note\"}}\n|-\n| Assembly, [[Motorola 68000 series|68k]] || <code>ASL</code> || <code>ASR</code>\n|-\n| [[x86 assembly language|Assembly, x86]] || <code>SAL</code> || <code>SAR</code>\n|-\n| [[VHDL]] || <code>sla</code><ref group=\"note\">The VHDL arithmetic left shift operator is unusual.  Instead of filling the LSB of the result with zero, it copies the original LSB into the new LSB.  While this is an exact mirror image of the arithmetic right shift, it is not the conventional definition of the operator, and is not equivalent to multiplication by a power of 2.  In the VHDL 2008 standard this strange behavior was left unchanged (for backward compatibility) for argument types that do not have forced numeric interpretation (e.g., BIT_VECTOR) but 'SLA' for ''unsigned'' and ''signed'' argument types behaves in the expected way (i.e., rightmost positions are filled with zeros).  VHDL's shift left logical (SLL) function does implement the aforementioned 'standard' arithmetic shift.</ref> || <code>sra</code>\n|-\n| [[Z80]] || <code>SLA</code><ref>{{Cite web|url=http://www.z80.info/z80syntx.htm#SLA|title=Z80 Assembler Syntax}}</ref> || <code>SRA</code>\n|}\nIn [[computer programming]], an '''arithmetic shift''' is a [[shift operator]], sometimes termed a '''signed shift''' (though it is not restricted to signed operands).  The two basic types are the '''arithmetic left shift''' and the '''arithmetic right shift'''. For [[binary numeral system|binary number]]s it is a [[bitwise operation]] that shifts all of the bits of its operand; every bit in the operand is simply moved a given number of bit positions, and the vacant bit-positions are filled in.  Instead of being filled with all 0s, as in [[logical shift]], when shifting to the right, the leftmost bit (usually the [[sign bit]] in signed integer representations) is replicated to fill in all the vacant positions (this is a kind of [[sign extension]]).\n\nSome authors prefer the terms ''sticky right-shift'' and ''zero-fill right-shift'' for arithmetic and logical shifts respectively.<ref>\nThomas R. Cain and Alan T. Sherman.\n[https://web.archive.org/web/20140109191811/http://www.cisa.umbc.edu/papers/ShermanCryptologia97.pdf \"How to break Gifford's cipher\"].\nSection 8.1: \"Sticky versus Non-Sticky Bit-shifting\".\nCryptologia.\n1997.\n</ref>\n\nArithmetic shifts can be useful as efficient ways to perform multiplication or division of signed integers by powers of two. Shifting left by ''n'' bits on a signed or unsigned binary number has the effect of multiplying it by 2<sup>''n''</sup>. Shifting right by ''n'' bits on a [[two's complement]] ''signed'' binary number has the effect of dividing it by 2<sup>''n''</sup>, but it always rounds down (towards negative infinity). This is different from the way rounding is usually done in signed integer division (which rounds towards 0). This discrepancy has led to bugs in more than one compiler.<ref>{{cite web|last=Steele Jr|first=Guy|title=Arithmetic Shifting Considered Harmful|url=http://dspace.mit.edu/bitstream/handle/1721.1/6090/AIM-378.pdf|publisher=MIT AI Lab|accessdate=20 May 2013}}</ref>\n\nFor example, in the [[x86 instruction listings|x86 instruction set]], the SAR instruction (arithmetic right shift) divides a signed number by a power of two, rounding towards negative infinity.{{sfn|Hyde|1996|loc=&sect; 6.6.2.2 SAR}} However, the IDIV instruction (signed divide) divides a signed number, rounding towards zero. So a SAR instruction cannot be substituted for an IDIV by power of two instruction nor vice versa.\n\n== Formal definition ==\nThe formal definition of an arithmetic shift, from [[Federal Standard 1037C]] is that it is:\n:A shift, applied to the representation of a number in a fixed [[radix]] numeration system and in a [[fixed-point arithmetic|fixed-point]] representation system, and in which only the characters representing the fixed-point part of the number are moved. An arithmetic shift is usually equivalent to multiplying the number by a positive or a negative integral power of the radix, except for the effect of any rounding; compare the [[logical shift]] with the arithmetic shift, especially in the case of [[floating-point]] representation.\n\nAn important word in the FS 1073C definition is \"usually\".\n\n=== Equivalence of arithmetic and logical left shifts and multiplication ===\nArithmetic ''left'' shifts are equivalent to multiplication by a (positive, integral) power of the radix (e.g., a multiplication by a power of 2 for binary numbers).  Arithmetic left shifts are, with one exception, identical in effect to logical left shifts. Arithmetic shifts may trigger [[arithmetic overflow]] whereas logical shifts do not. This exception matters only if a trigger signal for such an overflow is needed.\n\n=== Non-equivalence of arithmetic right shift and division ===\nHowever, arithmetic ''right'' shifts are major traps for the unwary, specifically in treating rounding of negative integers. For example, in the usual [[two's complement]] representation of negative integers, −1 is represented as all 1's. For an 8-bit signed integer this is 1111&nbsp;1111. An arithmetic right-shift by 1 (or 2, 3, …, 7) yields 1111&nbsp;1111 again, which is still −1. This corresponds to rounding down (towards negative infinity), but is not the usual convention for division.\n\nIt is frequently stated that arithmetic right shifts are equivalent to [[division (mathematics)|division]] by a (positive, integral) power of the radix (e.g., a division by a power of 2 for binary numbers), and hence that division by a power of the radix can be optimized by implementing it as an arithmetic right shift.  (A shifter is much simpler than a divider.  On most processors, shift instructions will execute faster than division instructions.) Large number of 1960s and 1970s programming handbooks, manuals, and other specifications from companies and institutions such as [[Digital Equipment Corporation|DEC]], [[IBM]], [[Data General]], and [[American National Standards Institute|ANSI]] make such incorrect statements {{sfn|Steele|1977|p=}}{{Page needed|date=March 2012}}.\n\nLogical right shifts are equivalent to division by a power of the radix (usually 2) only for positive or unsigned numbers. Arithmetic right shifts are equivalent to logical right shifts for positive signed numbers. Arithmetic right shifts for negative numbers in N&minus;1's complement (usually [[two's complement]]) is roughly equivalent to division by a power of the radix (usually 2), where for odd numbers rounding downwards is applied (not towards 0 as usually expected).\n\nArithmetic right shifts for negative numbers are equivalent to division using rounding towards 0 in [[one's complement]] representation of signed numbers as was used by some historic computers, but this is no longer in general use.\n\n==== Handling the issue in programming languages ====\n\nThe (1999) ISO standard for the programming language [[C (programming language)|C]] defines the right shift operator in terms of divisions by powers of 2.{{sfn|ISOIEC9899|1999|loc=&sect; 6.5.7 Bitwise shift operators}} Because of the above-stated non-equivalence, the standard explicitly excludes from that definition the right shifts of signed numbers that have negative values.  It does not specify the behaviour of the right shift operator in such circumstances, but instead requires each individual C compiler to define the behaviour of shifting negative values right.{{#tag:ref|The C standard was intended to not restrict the C language to either ones' complement or two's complement architectures.  In cases where the behaviours of ones' complement and two's complement representations differ, such as this, the standard requires individual C compilers to document the behaviour of their target architectures.  The documentation for [[GNU Compiler Collection]] (GCC), for example, documents its behaviour as employing sign-extension.{{sfn|FSF|2008|loc=&sect; 4.5 Integers implementation}}|group=\"note\"}}\n\n== Applications ==\nIn applications where consistent rounding down is desired, arithmetic right shifts for signed values are useful. An example is in [[downscaling]] raster coordinates by a power of two, which maintains even spacing. For example, right shift by 1 sends 0, 1, 2, 3, 4, 5, … to 0, 0, 1, 1, 2, 2, …, and −1, −2, −3, −4, … to −1, −1, −2, −2, …, maintaining even spacing as −2, −2, −1, −1, 0, 0, 1, 1, 2, 2, … In contrast, integer division with rounding towards zero sends −1, 0, and 1 all to 0 (3 points instead of 2), yielding −2, −1, −1, 0, 0, 0, 1, 1, 2, 2, … instead, which is irregular at 0.\n\n== Notes ==\n<references group=\"note\"/>\n\n== References ==\n\n=== Cross-reference ===\n{{Reflist|30em}}\n\n=== Sources used ===\n{{FS1037C}}\n{{Refbegin}}\n* {{cite book|ref=harv|first=Donald|last=Knuth|title=[[The Art of Computer Programming]], Volume 2 &mdash; Seminumerical algorithms|pages=169&ndash;170|location=Reading, Mass.|publisher=Addison-Wesley|year=1969|authorlink=Donald Knuth}}\n* {{cite journal|ref=harv|title=Arithmetic shifting considered harmful|journal=ACM SIGPLAN Notices archive|volume=12|issue=11|date=November 1977|pages=61&ndash;69|first=Guy L.|last=Steele|publisher=ACM Press|location=New York|doi=10.1145/956641.956647|url=http://www.dtic.mil/get-tr-doc/pdf?AD=ADA031883}}\n* {{cite book |ref=CITEREFHP2001|chapter-url=http://h71000.www7.hp.com/doc/73FINAL/4515/4515pro_002.html#8_arithmeticshiftoperator |work=HP OpenVMS Systems Documentation|title=VAX MACRO and Instruction Set Reference Manual |chapter=3.7.1 Arithmetic Shift Operator|publisher=Hewlett-Packard Development Company|date=April 2001 |archive-url=https://web.archive.org/web/20110808085326/http://h71000.www7.hp.com/doc/73final/4515/4515pro_002.html#8_arithmeticshiftoperator |archive-date=2011-08-08}}\n* {{Cite journal|ref=CITEREFISOIEC98991999|title=Programming languages &mdash; C|publisher=[[International Organization for Standardization]]|year=1999|version=ISO/IEC 9899:1999}}\n* {{cite book|ref=harv|chapter=CHAPTER SIX: THE 80x86 INSTRUCTION SET (Part 3)|date=1996-09-26|title=The Art of ASSEMBLY LANGUAGE PROGRAMMING|first=Randall|last=Hyde|url=http://www.arl.wustl.edu/~lockwood/class/cs306/books/artofasm/Chapter_6/CH06-3.html#HEADING3-120|access-date=2007-11-28|archive-url=https://web.archive.org/web/20071123223102/http://www.arl.wustl.edu/~lockwood/class/cs306/books/artofasm/Chapter_6/CH06-3.html#HEADING3-120|archive-date=2007-11-23|dead-url=yes|df=}}\n* {{cite web|ref=CITEREFFSF2008|year=2008|publisher=[[Free Software Foundation]]|url=https://gcc.gnu.org/onlinedocs/gcc-4.3.3/gcc/Integers-implementation.html#Integers-implementation|work=GCC manual|title=C Implementation}}\n{{refend}}\n\n{{DEFAULTSORT:Arithmetic Shift}}\n[[Category:Binary arithmetic]]\n[[Category:Operators (programming)]]"
    },
    {
      "title": "Barrel shifter",
      "url": "https://en.wikipedia.org/wiki/Barrel_shifter",
      "text": "[[File:crossbar barrel shifter.svg|thumb|250px|Schematic of a 4-bit crossbar barrel shifter. ''x''&nbsp;denotes input bits and ''y'' denotes output bits.]]\n\nA ''' barrel shifter''' is a [[digital circuit]] that can [[Bit shift|shift]] a [[Word (data type)|data word]] by a specified number of [[bit]]s without the use of any [[sequential logic]], only pure [[combinational logic]]. One way to implement it is as a sequence of [[multiplexer]]s where the output of one multiplexer is connected to the input of the next multiplexer in a way that depends on the shift distance. A barrel shifter is often used to shift and rotate n-bits in modern microprocessors, typically within a single [[clock cycle]].\n\nFor example, take a four-bit barrel shifter, with inputs A, B, C and D. The shifter can cycle the order of the bits ''ABCD'' as ''DABC'', ''CDAB'', or ''BCDA''; in this case, no bits are lost. That is, it can shift all of the outputs up to three positions to the right (and thus make any cyclic combination of A, B, C and D). The barrel shifter has a variety of applications, including being a useful component in [[microprocessor]]s (alongside the [[Arithmetic logic unit|ALU]]).\n\n== Implementation ==\n\nA barrel shifter is often implemented as a cascade of parallel 2×1 multiplexers.  For an 8-bit barrel shifter, two intermediate signals are used which shifts by four and two bits, or passes the same data, based on the value of S[2] and S[1].  This signal is then shifted by another multiplexer, which is controlled by S[0]:\n\n  int1  = IN       , if S[2] == 0\n        = IN   << 4, if S[2] == 1\n  int2  = int1     , if S[1] == 0\n        = int1 << 2, if S[1] == 1\n  OUT   = int2     , if S[0] == 0\n        = int2 << 1, if S[0] == 1\n\nLarger barrel shifters have additional stages.\n\n== Cost ==\n\nThe number of multiplexers required for an ''n''-bit word is <math>n\\log_2 n</math>.<ref>{{cite book\n| title=Decision Procedures\n| first=Daniel\n| last=Kroening\n| first2=Ofer\n| last2=Strichman\n| publisher=[[Springer Science+Business Media|Springer]]\n| year=2008\n| page=159\n| isbn=978-3-540-74104-6}}</ref>  Five common [[word size]]s and the number of multiplexers needed are listed below:\n* 128-bit &mdash; <math>128 \\times \\log_2 128 = 128 \\times 7 = 896</math>\n* 64-bit &mdash; <math>64 \\times \\log_2 64 = 64 \\times 6 = 384</math>\n* 32-bit &mdash; <math>32 \\times \\log_2 32 = 32 \\times 5 = 160</math>\n* 16-bit &mdash; <math>16 \\times \\log_2 16 = 16 \\times 4 = 64</math>\n* 8-bit &mdash; <math>8 \\times \\log_2 8 = 8 \\times 3 = 24</math>\n\nCost of critical path in [[FO4]] (estimated, without wire delay):\n* 32-bit: from 18 FO4 to 14 FO4<ref>{{cite web\n| url=http://www.realworldtech.com/fo4-metric/4/\n| title=Revisiting the FO4 Metric\n| first=David T.\n| last=Wang\n| date=2002-08-15\n| accessdate=2016-05-19}}</ref>\n\n== Uses ==\n\nA common usage of a barrel shifter is in the hardware implementation of [[floating-point arithmetic]]. For a floating-point add or subtract operation, the [[significand]]s of the two numbers must be aligned, which requires shifting the smaller number to the right, increasing its [[exponent]], until it matches the exponent of the larger number. This is done by subtracting the exponents and using the barrel shifter to shift the smaller number to the right by the difference, in one cycle. If a simple shifter were used, shifting by ''n'' bit positions would require ''n'' clock cycles.\n\n==See also==\n*[[Circular shift]]\n\n==References==\n\n{{reflist}}\n\n==External links==\n\n* [https://tams.informatik.uni-hamburg.de/applets/hades/webdemos/10-gates/60-barrel/shifter8.html Barrel-shifter (8 bit)], [[University of Hamburg]]\n* [http://www.xilinx.com/support/documentation/application_notes/xapp195.pdf Implementing Barrel Shifters Using Multipliers] (Paul Gigliotti, 2004-08-17)\n\n==Further reading==\n* {{cite book\n| title=Decision Procedures\n| first=Daniel\n| last=Kroening\n| first2=Ofer\n| last2=Strichman\n| publisher=[[Springer Science+Business Media|Springer]]\n| year=2008\n| isbn=978-3-540-74104-6}}\n\n{{FOLDOC}}\n\n{{CPU technologies}}\n\n[[Category:Digital circuits]]\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Unary operations]]<!-- bits.shiftLeftByOneBit() -->\n[[Category:Binary operations]]<!-- bits.shiftLeftBy(n bits) -->"
    },
    {
      "title": "Bfloat16 floating-point format",
      "url": "https://en.wikipedia.org/wiki/Bfloat16_floating-point_format",
      "text": "{{lowercase title}}\n\n{{Floating-point}}\n\nThe '''bfloat16 (Brain Floating Point) floating-point format''' is a [[computer number format]] occupying [[16-bit|16 bits]] in [[computer memory]]; it represents a wide [[dynamic range]] of numeric values by using a [[floating point|floating radix point]]. This format is a truncated (16-bit) version of the 32-bit [[Single-precision floating-point format|IEEE 754 single-precision floating-point format]] (binary32) with the intent of [[Hardware acceleration|accelerating]] [[machine learning]] and [[Intelligent sensor|near-sensor computing]].<ref>{{Cite book |doi=10.23919/DATE.2018.8342167|chapter=A transprecision floating-point platform for ultra-low power computing|title=2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)|pages=1051–1056|year=2018|last1=Tagliavini|first1=Giuseppe|last2=Mach|first2=Stefan|last3=Rossi|first3=Davide|last4=Marongiu|first4=Andrea|last5=Benin|first5=Luca|isbn=978-3-9819263-0-9|arxiv=1711.10374}}</ref> It preserves the approximate dynamic range of 32-bit floating-point numbers by retaining 8 [[Exponent bias|exponent bits]], but supports only an 8-bit precision rather than the 24-bit [[significand]] of the binary32 format. More so than single-precision 32-bit floating-point numbers, bfloat16 numbers are unsuitable for integer calculations, but this is not their intended use.\n\nThe bfloat16 format is utilized in upcoming Intel [[AI accelerator|AI processors]], such as [[Nervana Systems|Nervana]] NNP-L1000, [[Xeon]] processors, and Intel [[FPGA]]s,<ref name=\"vent_Inte\">{{Cite web | title = Intel unveils Nervana Neural Net L-1000 for accelerated AI training | author = Khari Johnson | work = VentureBeat | date = 2018-05-23 | accessdate = 2018-05-23 | url = https://venturebeat.com/2018/05/23/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/ |quote = ...Intel will be extending bfloat16 support across our AI product lines, including Intel Xeon processors and Intel FPGAs. }}</ref><ref name=\"top5_Inte\">{{Cite web | title = Intel Lays Out New Roadmap for AI Portfolio | author = Michael Feldman | work = TOP500 Supercomputer Sites | date = 2018-05-23 | accessdate = 2018-05-23 | url = https://www.top500.org/news/intel-lays-out-new-roadmap-for-ai-portfolio/ | quote = Intel plans to support this format across all their AI products, including the Xeon and FPGA lines }}</ref><ref name=\"toms_Inte\">{{Cite web | title = Intel To Launch Spring Crest, Its First Neural Network Processor, In 2019 | author = Lucian Armasu | work = Tom's Hardware | date = 2018-05-23 | accessdate = 2018-05-23 | url = https://www.tomshardware.com/news/intel-neural-network-processor-lake-crest,37105.html | quote = Intel said that the NNP-L1000 would also support bfloat16, a numerical format that’s being adopted by all the ML industry players for neural networks. The company will also support bfloat16 in its FPGAs, Xeons, and other ML products. The Nervana NNP-L1000 is scheduled for release in 2019. }}</ref> Google Cloud [[Tensor processing unit|TPU]]s,<ref name=\"clou_Avai\">{{Cite web | title = Available TensorFlow Ops {{!}} Cloud TPU {{!}} Google Cloud | author =  | work = Google Cloud | date =  | accessdate = 2018-05-23 | url = https://cloud.google.com/tpu/docs/tensorflow-ops | quote = This page lists the TensorFlow Python APIs and graph operators available on Cloud TPU. }}</ref><ref name=\"blog_Comp\">{{Cite web | title = Comparing Google's TPUv2 against Nvidia's V100 on ResNet-50 | author = Elmar Haußmann | work = RiseML Blog | date = 2018-04-26 | accessdate = 2018-05-23 | url = https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e | language =  | quote = For the Cloud TPU, Google recommended we use the bfloat16 implementation from the official TPU repository with TensorFlow 1.7.0. Both the TPU and GPU implementations make use of mixed-precision computation on the respective architecture and store most tensors with half-precision. | archive-url = https://web.archive.org/web/20180426200043/https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e | archive-date = 2018-04-26 | dead-url = yes }}</ref><ref name=\"gith_tens\">{{Cite web | title = ResNet-50 using BFloat16 on TPU | author = Tensorflow Authors | work = Google | date = 2018-07-23 | accessdate = 2018-11-06 | url = https://github.com/tensorflow/tpu/tree/0ece10f6f4e523eab79aba0247b513fe57d38ae6/models/experimental/resnet_bfloat16 | quote =  }}</ref> and [[TensorFlow]].<ref name=\"gith_tens\"/><ref name=\"arxiv_1711.10604\">{{cite report |title= TensorFlow Distributions |author= Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, Rif A. Saurous |date= 2017-11-28 |id= Accessed 2018-05-23 |arxiv= 1711.10604 |quote= All operations in TensorFlow Distributions are numerically stable across half, single, and double floating-point precisions (as TensorFlow dtypes: tf.bfloat16 (truncated floating point), tf.float16, tf.float32, tf.float64). Class constructors have a validate_args flag for numerical asserts |bibcode= 2017arXiv171110604D }}</ref>\n\n== bfloat16 floating-point format ==\n<!-- \"significand\", with a d at the end, is a technical term, please do not confuse with \"significant\" -->\n\n'''bfloat16''' has the following format:\n* [[Sign bit]]: 1 bit\n* [[Exponent]] width: 8 bits\n* [[Significand]] [[precision (arithmetic)|precision]]: 8 bits (7 explicitly stored), as opposed to 24 bits in a classical single-precision floating-point format\n\nThe bfloat16 format, being a truncated [[Single-precision floating-point format|IEEE 754 single-precision]] 32-bit float, allows for fast [[Type conversion|conversion]] to and from an IEEE 754 single-precision 32-bit float; in conversion to the bfloat16 format, the exponent bits are preserved while the significand field can be reduced by truncation (thus corresponding to [[IEEE 754#Rounding rules|round toward 0]]), ignoring the [[NaN]] special case. Preserving the exponent bits maintains the 32-bit float's range of ≈ 10<sup>−38</sup> to ≈ 3 × 10<sup>38</sup>.<ref name=\"googleio18-day1-time2575\">{{Cite web | title = Livestream Day 1: Stage 8 (Google I/O '18) - YouTube | author =  | work = Google | date = 2018-05-08 | accessdate = 2018-05-23 | url = https://www.youtube.com/watch?v=vm67WcLzfvc&t=2555 | language =   | quote = In many models this is a drop-in replacement for float-32 }}</ref>\n\nThe bits are laid out as follows:\n\n[[Image:Bfloat16 format.svg|Structure of the bfloat16 floating-point format|upright=1.2|frameless]]\n\nCompare to an [[Single-precision floating-point format|IEEE 754 single-precision]] 32-bit float:\n\n[[Image:Float example.svg]]\n\nAnd contrast with an [[Half-precision floating-point format|IEEE half-precision]] 16-bit float:\n\n[[Image:IEEE 754r Half Floating Point Format.svg|upright=1.2|frameless]]\n\n=== Exponent encoding ===\nThe bfloat16 binary floating-point exponent is encoded using an [[Offset binary|offset-binary]] representation, with the zero offset being 127; also known as exponent bias in the IEEE 754 standard.\n\n* E<sub>min</sub> = 01<sub>H</sub>−7F<sub>H</sub> = −126\n* E<sub>max</sub> = FE<sub>H</sub>−7F<sub>H</sub> = 127\n* [[Exponent bias]] = 7F<sub>H</sub> = 127\n\nThus, in order to get the true exponent as defined by the offset-binary representation, the offset of 127 has to be subtracted from the value of the exponent field.\n\nThe minimum and maximum values of the exponent field (00<sub>H</sub> and FF<sub>H</sub>) are interpreted specially, like in the IEEE 754 standard formats.\n{| class=\"wikitable\" style=\"text-align:center\"\n!Exponent\n!Significand zero\n!Significand non-zero\n!Equation\n|-\n|00<sub>H</sub>\n|[[0 (number)|zero]], [[−0]]\n|[[Denormal number|subnormal numbers]]\n|(−1)<sup>signbit</sup>×2<sup>−126</sup>× 0.significandbits\n|-\n|01<sub>H</sub>, …, FE<sub>H</sub>\n| colspan=\"2\" |normalized value\n|(−1)<sup>signbit</sup>×2<sup>exponentbits−127</sup>× 1.significandbits\n|-\n|FF<sub>H</sub>\n|±[[infinity]]\n|[[NaN]] (quiet, signaling)\n|\n|}\nThe minimum positive normal value is 2<sup>−126</sup> ≈ 1.18 × 10<sup>−38</sup> and the minimum positive (subnormal) value is 2<sup>−126−7</sup> = 2<sup>−133</sup> ≈ 9.2 × 10<sup>−41</sup>.\n\n== Encoding of special values ==\n\n=== Positive and negative infinity ===\nJust as in [[IEEE 754]], positive and negative infinity are represented with their corresponding [[sign bit]]s, all 8 exponent bits set (FF<sub>hex</sub>) and all significand bits zero. Explicitly,<syntaxhighlight lang=\"text\">\nval    s_exponent_signcnd\n+inf = 0_11111111_0000000\n-inf = 1_11111111_0000000\n</syntaxhighlight>\n\n=== NaN ===\nJust as in [[IEEE 754]], [[NaN]] values are represented with either sign bit, all 8 exponent bits set (FF<sub>hex</sub>) and not all significand bits zero. Explicitly,<syntaxhighlight lang=\"text\">\nval    s_exponent_signcnd\n+NaN = 0_11111111_klmnopq\n-NaN = 1_11111111_klmonpq\n</syntaxhighlight>where at least one of ''k, l, m, n, o, p,'' or ''q'' is 1. As with IEEE 754, NaN values can be quiet or signaling, although there are no known uses of signaling bfloat16 NaNs as of September 2018.\n\n== Range and precision ==\n\nBfloat16 is designed to maintain the number range from the 32-bit [[Single-precision floating-point format|IEEE 754 single-precision floating-point format]] (binary32), while reducing the precision from a 24 bits to a 8 bits.\n\n== Examples ==\n\nThese examples are given in bit ''representation'', in [[hexadecimal]] and [[Binary number|binary]], of the floating-point value. This includes the sign, (biased) exponent, and significand.\n 3f80 = 0 01111111 0000000 = 1\n c000 = 1 10000000 0000000 = −2\n\n 7f7f = 0 11111110 1111111 = (2<sup>8</sup> − 1) × 2<sup>−7</sup> × 2<sup>127</sup> ≈ 3.38953139 × 10<sup>38</sup> (max finite positive value in bfloat16 precision)\n 0080 = 0 00000001 0000000 = 2<sup>−126</sup> ≈ 1.175494351 × 10<sup>−38</sup> (min normalized positive value in bfloat16 precision and single-precision floating point)\nThe maximum positive finite value of a normal bfloat16 number is 3.38953139 × 10<sup>38</sup>, slightly below (2<sup>24</sup> − 1) × 2<sup>−23</sup> × 2<sup>127</sup> = 3.402823466 × 10<sup>38</sup>, the max finite positive value representable in single precision.\n\n=== Zeros and infinities ===\n 0000 = 0 00000000 0000000 = 0\n 8000 = 1 00000000 0000000 = −0\n\n 7f80 = 0 11111111 0000000 = infinity\n ff80 = 1 11111111 0000000 = −infinity\n\n=== Special values ===\n 4049 = 0 10000000 1001001 = 3.140625 ≈ π ( pi )\n 3eab = 0 01111101 0101011 = 0.333984375 ≈ 1/3\n\n=== NaNs ===\n ffc1 = x 11111111 1000001 => qNaN\n ff81 = x 11111111 0000001 => sNaN\n\n== See also ==\n* [[Half-precision floating-point format]]: 16-bit float w/ 1-bit sign, 5-bit exponent, and 11-bit significand, as defined by [[IEEE 754]]\n* [[ISO/IEC 10967]], Language Independent Arithmetic\n* [[Primitive data type]]\n*[[Minifloat]]\n\n==References==\n{{Reflist}}\n\n{{data types}}\n\n{{DEFAULTSORT:bfloat16 floating-point format}}\n[[Category:Binary arithmetic]]\n[[Category:Floating point types]]"
    },
    {
      "title": "Biased representation (arithmetics)",
      "url": "https://en.wikipedia.org/wiki/Biased_representation_%28arithmetics%29",
      "text": "#redirect [[Offset binary]] {{R from other name}}\n\n[[Category:Numeral systems]]\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Binary adder",
      "url": "https://en.wikipedia.org/wiki/Binary_adder",
      "text": "#REDIRECT [[Adder (electronics)]]\n\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Binary Angular Measurement System",
      "url": "https://en.wikipedia.org/wiki/Binary_Angular_Measurement_System",
      "text": "#redirect [[Binary scaling#BAM]]\n[[Category:Binary arithmetic]]\n{{R to section}}"
    },
    {
      "title": "Binary digit",
      "url": "https://en.wikipedia.org/wiki/Binary_digit",
      "text": "#REDIRECT [[Bit]]\n\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Binary multiplier",
      "url": "https://en.wikipedia.org/wiki/Binary_multiplier",
      "text": "{{ALUSidebar|expand=Components|expand-components=Multiplier}}\nA '''binary multiplier''' is an [[electronic circuit]] used in [[digital electronics]], such as a [[computer]], to [[Multiplication|multiply]] two [[binary number]]s. It is built using [[binary adder]]s.\n\nA variety of [[:Category:computer arithmetic|computer arithmetic]] techniques can be used to implement a digital multiplier.  Most techniques involve computing a set of ''partial products'', and then summing the partial products together.  This process is similar to the method taught to primary schoolchildren for conducting long multiplication on base-10 integers, but has been modified here for application to a base-2 ([[binary numeral system|binary]]) [[numeral system]].\n\n==History==\nBetween 1947-1949 Arthur Alec Robinson worked for English Electric Ltd, as a student apprentice, and then as a development engineer. Crucially during this period he studied for a PhD degree at the University of Manchester, where he worked on the design of the hardware multiplier for the early Mark 1 computer.\n[https://issuu.com/clarealumni/docs/mu16632_clare_annual_-_web]\nHowever, until the late 1970s, most [[minicomputers]] did not have a multiply instruction, and so programmers used a \"multiply routine\"<ref>\"The Evolution of Forth\" by Elizabeth D. Rather et al.\n[http://www.forth.com/resources/evolution/evolve_2.html]\n[http://www.forth.com/resources/evolution/evolve_1.html]\n</ref><ref>[https://dx.doi.org/10.1016/0308-5953(77)90004-6 \"Interfacing a hardware multiplier to a general-purpose microprocessor\"]</ref>\nwhich repeatedly [[multiplication algorithm#Shift and add|shifts and accumulates]] partial results,\noften written using [[loop unwinding]].  [[Mainframe computer]]s had multiply instructions, but they did the same sorts of shifts and adds as a \"multiply routine\".\n\nEarly [[microprocessor]]s also had no multiply instruction.  Though the multiply instruction is usually associated with the 16-bit microprocessor generation,<ref>{{cite book \n | title = Fundamentals of Digital Logic and Microcomputer Design\n | author = M. Rafiquzzaman\n | publisher = [[John Wiley & Sons]]\n | page = 251\n | year = 2005\n | isbn = 978-0-47173349-2\n | url = https://books.google.com/?id=1QZEawDm9uAC&pg=PA251&dq=6809+multiply+instruction#v=onepage&q=6809%20multiply%20instruction&f=false}}</ref> \nat least two \"enhanced\" 8-bit micro have a multiply instruction:   the [[Motorola 6809]], introduced in 1978,<ref>{{cite book \n | title = Microprocessors and Microcontrollers: Architecture, Programming and System Design 8085, 8086, 8051, 8096\n | author = Krishna Kant\n | publisher = PHI Learning Pvt. Ltd.\n | page = 57\n | year = 2007\n | isbn = 9788120331914\n | url = https://books.google.com/?id=P-n3kelycHQC&pg=PA57&dq=6809+multiply+instruction#v=onepage&q=6809%20multiply%20instruction&f=false\n}}</ref><ref>{{cite book \n | title = Microprocessor-Based Agri Instrumentation\n | author = Krishna Kant\n | publisher = PHI Learning Pvt. Ltd.\n | page = 139\n | year = 2010\n | isbn = 9788120340862\n | url = https://books.google.com/?id=k56RJCu07ZQC&pg=PA139&dq=6809+multiply+instruction#v=onepage&q=6809%20multiply%20instruction&f=false\n}}</ref> and [[Intel MCS-51]] family, developed in 1980, and later the modern [[Atmel AVR]] 8-bit microprocessors present in the ATMega, ATTiny and ATXMega microcontrollers.\n\nAs more [[transistor count|transistors per chip]] became available due to larger-scale integration, it became possible to put enough adders on a single chip to sum all the partial products at once, rather than reuse a single adder to handle each partial product one at a time.\n\nBecause some common [[digital signal processing]] algorithms spend most of their time multiplying, [[digital signal processor]] designers sacrifice a lot of chip area in order to make the multiply as fast as possible; a single-cycle [[multiply–accumulate]] unit often used up most of the chip area of early DSPs.\n\n{{Anchor|Multiplication basics}}\n\n==Basics==\nThe method taught in school for multiplying decimal numbers is based on calculating partial products, shifting them to the left and then adding them together. The most difficult part is to obtain the partial products, as that involves multiplying a long number by one digit (from 0 to 9):\n\n  <nowiki>    123\n   x 456\n   =====\n     738  (this is 123 x 6)\n    615   (this is 123 x 5, shifted one position to the left)\n + 492    (this is 123 x 4, shifted two positions to the left)\n   =====\n   56088</nowiki>\n\n== Signed numbers ==\nA binary computer does exactly the same multiplication as decimal numbers do, but with binary numbers. In binary encoding each long number is multiplied by one digit (either 0 or 1), and that is much easier than in decimal, as the product by 0 or 1 is just 0 or the same number. Therefore, the multiplication of two binary numbers comes down to calculating partial products (which are 0 or the first number), [[Logical shift|shifting]] them left, and then adding them together (a binary addition, of course):\n\n <nowiki>\n       1011   (this is 11 in binary)\n     x 1110   (this is 14 in binary)\n     ======\n       0000   (this is 1011 x 0)\n      1011    (this is 1011 x 1, shifted one position to the left)\n     1011     (this is 1011 x 1, shifted two positions to the left)\n  + 1011      (this is 1011 x 1, shifted three positions to the left)\n  =========\n   10011010   (this is 154 in binary)</nowiki>\n\nThis is much simpler than in the decimal system, as there is no table of multiplication to remember: just shifts and adds.\n\nThis method is mathematically correct and has the advantage that a small CPU may perform the multiplication by using the shift and add features of its arithmetic logic unit rather than a specialized circuit. The method is slow, however, as it involves many intermediate additions. These additions take a lot of time. Faster multipliers may be engineered in order to do fewer additions; a modern processor can multiply two 64-bit numbers with 6 additions (rather than 64), and can do several steps in parallel.{{citation needed|date=August 2017|reason=}}\n\nThe second problem is that the basic school method handles the sign with a separate rule (\"+ with + yields +\", \"+ with − yields −\", etc.). Modern computers embed the sign of the number in the number itself, usually in the [[two's complement]] representation. That forces the multiplication process to be adapted to handle two's complement numbers, and that complicates the process a bit more. Similarly, processors that use [[ones' complement]], [[sign-and-magnitude]], [[IEEE-754]] or other binary representations require specific adjustments to the multiplication process.\n\n==Unsigned numbers==\nFor example, suppose we want to multiply two [[signedness|unsigned]] eight bit integers together: ''a''[7:0] and ''b''[7:0].  We can produce eight partial products by performing eight one-bit multiplications, one for each bit in multiplicand ''a'':\n  <nowiki>p0[7:0] = a[0] &times; b[7:0] = {8{a[0]}} & b[7:0]\n p1[7:0] = a[1] &times; b[7:0] = {8{a[1]}} & b[7:0]\n p2[7:0] = a[2] &times; b[7:0] = {8{a[2]}} & b[7:0]\n p3[7:0] = a[3] &times; b[7:0] = {8{a[3]}} & b[7:0]\n p4[7:0] = a[4] &times; b[7:0] = {8{a[4]}} & b[7:0] \n p5[7:0] = a[5] &times; b[7:0] = {8{a[5]}} & b[7:0]\n p6[7:0] = a[6] &times; b[7:0] = {8{a[6]}} & b[7:0]\n p7[7:0] = a[7] &times; b[7:0] = {8{a[7]}} & b[7:0]</nowiki>\n\nwhere <nowiki>{8{a[0]}}</nowiki> means repeating a[0] (the 0th bit of a) 8 times ([[Verilog]] notation).\n\nTo produce our product, we then need to add up all eight of our partial products, as shown here:\n                                                 p0[7] p0[6] p0[5] p0[4] p0[3] p0[2] p0[1] p0[0]\n                                         + p1[7] p1[6] p1[5] p1[4] p1[3] p1[2] p1[1] p1[0] 0\n                                   + p2[7] p2[6] p2[5] p2[4] p2[3] p2[2] p2[1] p2[0] 0     0\n                             + p3[7] p3[6] p3[5] p3[4] p3[3] p3[2] p3[1] p3[0] 0     0     0\n                       + p4[7] p4[6] p4[5] p4[4] p4[3] p4[2] p4[1] p4[0] 0     0     0     0\n                 + p5[7] p5[6] p5[5] p5[4] p5[3] p5[2] p5[1] p5[0] 0     0     0     0     0\n           + p6[7] p6[6] p6[5] p6[4] p6[3] p6[2] p6[1] p6[0] 0     0     0     0     0     0\n     + p7[7] p7[6] p7[5] p7[4] p7[3] p7[2] p7[1] p7[0] 0     0     0     0     0     0     0\n -------------------------------------------------------------------------------------------\n P[15] P[14] P[13] P[12] P[11] P[10]  P[9]  P[8]  P[7]  P[6]  P[5]  P[4]  P[3]  P[2]  P[1]  P[0]\n\nIn other words, ''P''[15:0] is produced by summing ''p''0, ''p''1 << 1, ''p''2 << 2, and so forth, to produce our final unsigned 16-bit product.\n\n==Signed integers==\nIf ''b'' had been a [[signedness|signed]] integer instead of an [[signedness|unsigned]] integer, then the partial products would need to have been sign-extended up to the width of the product before summing.  If ''a'' had been a signed integer, then partial product ''p7'' would need to be subtracted from the final sum, rather than added to it.\n\nThe above array multiplier can be modified to support [[two's complement notation]] signed numbers by inverting several of the product terms and inserting a one to the left of the first partial product term:\n\n                                                     1  ~p0[7]  p0[6]  p0[5]  p0[4]  p0[3]  p0[2]  p0[1]  p0[0]\n                                                 ~p1[7] +p1[6] +p1[5] +p1[4] +p1[3] +p1[2] +p1[1] +p1[0]   0\n                                          ~p2[7] +p2[6] +p2[5] +p2[4] +p2[3] +p2[2] +p2[1] +p2[0]   0      0\n                                   ~p3[7] +p3[6] +p3[5] +p3[4] +p3[3] +p3[2] +p3[1] +p3[0]   0      0      0\n                            ~p4[7] +p4[6] +p4[5] +p4[4] +p4[3] +p4[2] +p4[1] +p4[0]   0      0      0      0\n                     ~p5[7] +p5[6] +p5[5] +p5[4] +p5[3] +p5[2] +p5[1] +p5[0]   0      0      0      0      0\n              ~p6[7] +p6[6] +p6[5] +p6[4] +p6[3] +p6[2] +p6[1] +p6[0]   0      0      0      0      0      0\n    1  +p7[7] ~p7[6] ~p7[5] ~p7[4] ~p7[3] ~p7[2] ~p7[1] ~p7[0]   0      0      0      0      0      0      0\n  ------------------------------------------------------------------------------------------------------------\n P[15]  P[14]  P[13]  P[12]  P[11]  P[10]   P[9]   P[8]   P[7]   P[6]   P[5]   P[4]   P[3]   P[2]   P[1]  P[0]\n\nWhere ~p represents the complement (opposite value) of p.\n\nThere are a lot of simplifications in the bit array above that are not shown and are not obvious.  The sequences of one complemented bit followed by noncomplemented bits are implementing a two's complement trick to avoid sign extension.  The sequence of p7 (noncomplemented bit followed by all complemented bits) is because we're subtracting this term so they were all negated to start out with (and a 1 was added in the least significant position).  For both types of sequences, the last bit is flipped and an implicit -1 should be added directly below the MSB.  When the +1 from the two's complement negation for p7 in bit position 0 (LSB) and all the -1's in bit columns 7 through 14 (where each of the MSBs are located) are added together, they can be simplified to the single 1 that \"magically\" is floating out to the left.  For an explanation and proof of why flipping the MSB saves us the sign extension, see a computer arithmetic book.<ref>Parhami, Behrooz, Computer Arithmetic: Algorithms and Hardware Designs, [[Oxford University Press]], New York, 2000 ({{ISBN|0-19-512583-5}},  490 + xx pp.)</ref>\n\n== Floating point numbers ==\n{{Expand section|date=May 2019}}\n\n==Implementations==\nOlder multiplier architectures employed a shifter and accumulator to sum each partial product, often one partial product per cycle, trading off speed for die area.  Modern multiplier architectures use the (Modified) [[Baugh&ndash;Wooley algorithm]],<ref name=\"Baugh-Wooley_1973\"/><ref name=\"Hatamian-Cash_1986\"/><ref name=\"Gebali_2003\"/><ref name=\"ULVD_2015\"/> [[Wallace tree]]s, or [[Dadda multiplier]]s to add the partial products together in a single cycle.  The performance of the [[Wallace tree]] implementation is sometimes improved by ''modified'' [[Booth encoding]] one of the two multiplicands, which reduces the number of partial products that must be summed.\n\n==Example circuits==\n[[Image:binary multi1.jpg|center|thumb|500px|2-bit by 2-bit binary multiplier]]\n<!-- Deleted image removed: [[Image:eightbitmult.jpg|center|thumb|500px|4 Bit by 4 Bit Binary Multiplier<br> '''Using 4 Bit + 4 Bit Adders''']] -->\n\n==See also==\n{{div col}}\n* [[Booth's multiplication algorithm]]\n* [[Fused multiply–add]]\n* [[Wallace tree]]\n* [[BKM algorithm]] for complex logarithms and exponentials\n* [[Kochanski multiplication]] for [[modular arithmetic|modular]] multiplication\n* [[Logical shift left]]\n{{div col end}}\n==References==\n{{Reflist|refs=\n<ref name=\"Baugh-Wooley_1973\">{{cite journal |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1672241&sortType%3Dasc_p_Sequence%26filter%3DAND%28p_IS_Number%3A35072%29 |author-first1=Charles Richmond |author-last1=Baugh |author-first2=Bruce A. |author-last2=Wooley |title=A Two's Complement Parallel Array Multiplication Algorithm |journal=[[IEEE Transactions on Computers]] |volume=C-22 |issue=12 |pages=1045–1047 |date=December 1973|doi=10.1109/T-C.1973.223648 }}</ref>\n<ref name=\"Gebali_2003\">{{cite web |url=http://www.ece.uvic.ca/~fayez/courses/ceng465/lab_465/project2/multiplier.pdf |author-first=Fayez |author-last=Gebali |title=Baugh–Wooley Multiplier |publisher=[[University of Victoria]], CENG 465 Lab 2 |date=2003 |access-date=2018-04-14 |dead-url=no |archive-url=https://web.archive.org/web/20180414230734/http://www.ece.uvic.ca/~fayez/courses/ceng465/lab_465/project2/multiplier.pdf |archive-date=2018-04-14}}</ref>\n<ref name=\"Hatamian-Cash_1986\">{{cite journal |author-last1=Hatamian |author-first1= Mehdi |author-last2=Cash |author-first2=Glenn |date=1986 |title=A 70-MHz 8-bit×8-bit parallel pipelined multiplier in 2.5-µm CMOS |journal=[[IEEE Journal of Solid-State Circuits]] |volume=21 |issue=4 |pages=505–513 |doi=10.1109/ISSC.1986.1052564 |url=https://www.researchgate.net/publication/2981745|doi-broken-date= 2019-02-15 }}</ref>\n<ref name=\"ULVD_2015\">{{cite book |title=Ultra-Low-Voltage Design of Energy-Efficient Digital Circuits |author-first1=Nele |author-last1=Reynders |author-first2=Wim |author-last2=Dehaene |series=Analog Circuits And Signal Processing (ACSP) |date=2015 |edition=1 |location=Heverlee, Belgium |publisher=[[Springer International Publishing AG Switzerland]] |publication-place=Cham, Switzerland |isbn=978-3-319-16135-8 |issn=1872-082X |doi=10.1007/978-3-319-16136-5 |lccn=2015935431}}</ref>\n}}\n* {{cite book |title=Computer Architecture: A quantitative Approach |author-first1=John L. |author-last1=Hennessy |author-first2=David A. |author-last2=Patterson |isbn=978-0-12383872-8 |date=1990 |publisher=[[Morgan Kaufmann Publishers, Inc.]] |chapter=Section A.2, section A.9 |pages=A–3..A–6, A–39..A–49}}\n\n==External links==\n* [http://www.andraka.com/multipli.php Multiplier Designs] targeted at [[FPGA]]s\n* [http://www.fullchipdesign.com/binary_multiplier_digital.htm Binary Multiplier circuit using Half -Adders and digital gates.]\n\n{{DEFAULTSORT:Binary Multiplier}}\n[[Category:Digital circuits]]\n[[Category:Binary arithmetic]]\n[[Category:Multiplication]]"
    },
    {
      "title": "Binary number",
      "url": "https://en.wikipedia.org/wiki/Binary_number",
      "text": "{{Use dmy dates|date=October 2012}}\n{{Table Numeral Systems}}\nIn mathematics and [[digital electronics]], a '''binary number''' is a [[number]] expressed in the '''base-2 numeral system''' or '''binary numeral system''', which uses only two symbols: typically \"0\" ([[zero]]) and \"1\" ([[one]]).\n\nThe base-2 numeral system is a [[positional notation]] with a [[radix]] of 2. Each digit is referred to as a [[bit]]. Because of its straightforward implementation in [[Digital electronics|digital electronic circuitry]] using [[logic gate]]s, the binary system is used by almost all modern [[computer|computers and computer-based devices]].\n\n==History==\nThe modern binary number system was studied in Europe in the 16th and 17th centuries by [[Thomas Harriot]], [[Juan Caramuel y Lobkowitz]], and [[Gottfried Leibniz]]. However, systems related to binary numbers have appeared earlier in multiple cultures including ancient Egypt, China, and India. Leibniz was specifically inspired by the Chinese [[I Ching]].\n\n===Egypt===\n{{See also|Ancient Egyptian mathematics}}\n[[File:Oudjat.SVG|thumb|240px|left|Arithmetic values represented by parts of the Eye of Horus]]\nThe scribes of ancient Egypt used two different systems for their fractions, [[Egyptian fraction]]s (not related to the binary number system) and [[Eye of Horus|Horus-Eye]] fractions (so called because many historians of mathematics believe that the symbols used for this system could be arranged to form the eye of [[Horus]], although this has been disputed).<ref>{{citation|title=The Oxford Handbook of the History of Mathematics|editor1-first=Eleanor|editor1-last=Robson|editor1-link=Eleanor Robson|editor2-first=Jacqueline|editor2-last=Stedall|editor2-link=Jackie Stedall|publisher=Oxford University Press|year=2009|isbn=9780199213122|page=790|url=https://books.google.com/books?id=xZMSDAAAQBAJ&pg=PA790|contribution=Myth No. 2: the Horus eye fractions}}</ref> Horus-Eye fractions are a binary numbering system for fractional quantities of grain, liquids, or other measures, in which a fraction of a [[hekat]] is expressed as a sum of the binary fractions 1/2, 1/4, 1/8, 1/16, 1/32, and 1/64. Early forms of this system can be found in documents from the [[Fifth Dynasty of Egypt]], approximately 2400 BC, and its fully developed hieroglyphic form dates to the [[Nineteenth Dynasty of Egypt]], approximately 1200 BC.<ref>{{citation|title=Numerical Notation: A Comparative History|first=Stephen|last=Chrisomalis|publisher=Cambridge University Press|year=2010|isbn=9780521878180|pages=42–43|url=https://books.google.com/books?id=ux--OWgWvBQC&pg=PA42}}.</ref>\n\nThe method used for [[ancient Egyptian multiplication]] is also closely related to binary numbers. In this method, multiplying one number by a second is performed by a sequence of steps in which a value (initially the first of the two numbers) is either doubled or has the first number added back into it; the order in which these steps are to be performed is given by the binary representation of the second number. This method can be seen in use, for instance, in the [[Rhind Mathematical Papyrus]], which dates to around 1650 BC.<ref>{{citation|title=How Mathematics Happened: The First 50,000 Years|first=Peter Strom|last=Rudman|publisher=Prometheus Books|year=2007|isbn=9781615921768|pages=135–136|url=https://books.google.com/books?id=BtcQq4RUfkUC&pg=PA135}}.</ref>\n\n===China===\n[[File:Bagua-name-earlier.svg|thumb|160px|Daoist Bagua]]\nThe [[I Ching]] dates from the 9th century BC in China.<ref name=\"HackerMoore2002\">{{cite book|author1=Edward Hacker|author2=Steve Moore|author3=Lorraine Patsco|title=I Ching: An Annotated Bibliography|url=https://books.google.com/books?id=S5hLpfFiMCQC&pg=PR13|year=2002|publisher=Routledge|isbn=978-0-415-93969-0|page=13}}</ref> The binary notation in the ''I Ching'' is used to interpret its [[quaternary numeral system|quaternary]]  [[I Ching divination|divination]] technique.{{sfnp|Redmond|Hon|2014|p=227}}\n\nIt is based on taoistic duality of [[yin and yang]].<ref name=\"scientific\">{{cite book|author1=Jonathan Shectman|title=Groundbreaking Scientific Experiments, Inventions, and Discoveries of the 18th Century|url=https://books.google.com/books?id=SsbChdIiflsC&pg=PA29|year=2003|publisher=Greenwood Publishing|isbn=978-0-313-32015-6|page=29}}</ref>\n[[Ba gua|eight trigrams (Bagua)]] and a set of [[Hexagram (I Ching)|64 hexagrams (\"sixty-four\" gua)]], analogous to the three-bit and six-bit binary numerals, were in use at least as early as the [[Zhou Dynasty|Zhou Dynasty of ancient China]].<ref name=\"HackerMoore2002\"/>\n\nThe [[Song Dynasty]] scholar [[Shao Yong]] (1011–1077) rearranged the hexagrams in a format that resembles modern binary numbers, although he did not intend his arrangement to be used mathematically.{{sfnp|Redmond|Hon|2014|p=227}} Viewing the [[least significant bit]] on top of single hexagrams in [http://www.biroco.com/yijing/sequence.htm Shao Yong's square] and reading along rows either from bottom right to top left with solid lines as 0 and broken lines as 1 or from top left to bottom right with solid lines as 1 and broken lines as 0 hexagrams can be interpreted as sequence from 0 to 63.\n<ref name=\"Shao Yong’s ”Xiantian Tu‘‘\">{{cite book|last1=Zhonglian|first1=Shi|last2=Wenzhao|first2=Li|last3=Poser|first3=Hans|title=Leibniz’ Binary System and Shao Yong’s ”Xiantian Tu‘‘ in :Das Neueste über China: G.W. Leibnizens Novissima Sinica von 1697 : Internationales Symposium, Berlin 4. bis 7. Oktober 1997|date=2000|\npublisher=Franz Steiner Verlag|location=Stuttgart|isbn=3515074481|pages=165–170|url=https://books.google.de/books?id=DkIpP2SsGlIC&pg=PA165|ref=ID3515074481}}</ref>\n\n===India===\nThe Indian scholar [[Pingala]] (c. 2nd century BC) developed a binary system for describing [[prosody (poetry)|prosody]].<ref>{{Cite book|last1=Sanchez|first1=Julio|last2=Canton|first2=Maria P.|title=Microcontroller programming: the microchip PIC|year=2007|publisher=CRC Press|location=Boca Raton, Florida|isbn=0-8493-7189-9|page=37|postscript=<!--None-->}}</ref><ref>W. S. Anglin and J. Lambek, ''The Heritage of Thales'', Springer, 1995, {{ISBN|0-387-94544-X}}</ref> He used binary numbers in the form of short and long syllables (the latter equal in length to two short syllables), making it similar to [[Morse code]].<ref name = india>[http://home.ica.net/~roymanju/Binary.htm Binary Numbers in Ancient India]</ref><ref>[http://www.sju.edu/~rhall/Rhythms/Poets/arcadia.pdf Math for Poets and Drummers] (pdf, 145KB)</ref> Pingala's Hindu classic titled [[Chandah-shastra|Chandaḥśāstra]] (8.23) describes the formation of a matrix in order to give a unique value to each meter. The binary representations in Pingala's system increases towards the right, and not to the left like in the binary numbers of the modern, Western [[positional notation]].<ref name = india/><ref>{{Cite book|title=The mathematics of harmony: from Euclid to contemporary mathematics and computer science|first1=Alexey|last1=Stakhov|author1-link=Alexey Stakhov|first2=Scott Anthony|last2=Olsen|isbn=978-981-277-582-5|year=2009|url=https://books.google.com/books?id=K6fac9RxXREC}}</ref>\n\n===Other cultures===\nThe residents of the island of [[Mangareva]] in [[French Polynesia]] were using a hybrid binary-[[decimal]] system before 1450.<ref>{{Cite journal|last=Bender|first=Andrea|last2=Beller|first2=Sieghard|title=Mangarevan invention of binary steps for easier calculation|journal=Proceedings of the National Academy of Sciences|volume=111|issue=|date=16 December 2013|doi=10.1073/pnas.1309160110|pages=1322–1327|pmid=24344278|pmc=3910603}}</ref> [[Slit drum]]s with binary tones are used to encode messages across Africa and Asia.<ref name=\"scientific\"/>\nSets of binary combinations similar to the I Ching have also been used in traditional African divination systems such as [[Ifá]] as well as in [[Middle Ages|medieval]] Western [[geomancy]].\n\n===Western predecessors to Leibniz===\nIn the late 13th century [[Ramon Llull]] had the ambition to account for all wisdom in every branch of human knowledge of the time. For that purpose he developed a general method or ‘Ars generalis’ based on binary combinations of a number of simple basic principles or categories, for which he has been considered a predecessor of computing science and artificial intelligence.<ref>(see  Bonner 2007 [http://lullianarts.net/], Fidora et al. 2011 [https://www.iiia.csic.es/es/publications/ramon-llull-ars-magna-artificial-intelligence/])</ref>\n\nIn 1605 [[Francis Bacon]] discussed a system whereby letters of the alphabet could be reduced to sequences of binary digits, which could then be encoded as scarcely visible variations in the font in any random text.<ref name=\"Bacon1605\" /> Importantly for the general theory of binary encoding, he added that this method could be used with any objects at all: \"provided those objects be capable of a twofold difference only; as by Bells, by Trumpets, by Lights and Torches, by the report of Muskets, and any instruments of like nature\".<ref name=\"Bacon1605\">{{Cite web\n|last=Bacon\n|first=Francis\n|authorlink=Francis Bacon\n|title=The Advancement of Learning\n|url=http://home.hiwaay.net/~paul/bacon/advancement/book6ch1.html\n|year=1605\n|volume=6\n|location=London\n|pages=Chapter 1\n|postscript=<!--None-->\n}}\n</ref> (See [[Bacon's cipher]].)\n\n[[John Napier]] in 1617 described a system he called [[location arithmetic]] for doing binary calculations using a non-positional representation by letters.\n[[Thomas Harriot]] investigated several positional numbering systems, including binary, but did not publish his results; they were found later among his papers.<ref>{{cite journal|last=Shirley|first=John W.|title=Binary numeration before Leibniz|journal=American Journal of Physics|volume=19|year=1951|issue=8|pages=452–454|doi=10.1119/1.1933042}}</ref>\nPossibly the first publication of the system in Europe was by [[Juan Caramuel y Lobkowitz]], in 1700.<ref>{{cite journal|last=Ineichen|first=R.|title=Leibniz, Caramuel, Harriot und das Dualsystem|language=German|journal=Mitteilungen der deutschen Mathematiker-Vereinigung|volume=16|year=2008|issue=1|pages=12–15|url=http://page.math.tu-berlin.de/~mdmv/archive/16/mdmv-16-1-12-ineichen.pdf}}</ref>\n\n===Leibniz and the I Ching===\n[[File:Gottfried Wilhelm Leibniz, Bernhard Christoph Francke.jpg|thumb|upright|Gottfried Leibniz]]\nLeibniz studied binary numbering in 1679; his work appears in his article ''Explication de l'Arithmétique Binaire'' (published in 1703)\nThe full title of Leibniz's article is translated into English as the ''\"Explanation of Binary Arithmetic, which uses only the characters 1 and 0, with some remarks on its usefulness, and on the light it throws on the ancient Chinese figures of [[Fu Xi]]\"''.<ref name=lnz>Leibniz G., Explication de l'Arithmétique Binaire, Die Mathematische Schriften, ed. C. Gerhardt, Berlin 1879, vol.7, p.223; Engl. transl.[http://www.leibniz-translations.com/binary.htm]</ref> (1703). Leibniz's system uses 0 and 1, like the modern binary numeral system. An example of Leibniz's binary numeral system is as follows:<ref name=lnz/>\n: 0 0 0 1 &nbsp; numerical value 2<sup>0</sup>\n: 0 0 1 0 &nbsp; numerical value 2<sup>1</sup>\n: 0 1 0 0 &nbsp; numerical value 2<sup>2</sup>\n: 1 0 0 0 &nbsp; numerical value 2<sup>3</sup>\nLeibniz interpreted the hexagrams of the I Ching as evidence of binary calculus.<ref name=\"smith\"/>\nAs a [[Sinophile]], Leibniz was aware of the I Ching, noted with fascination how its hexagrams correspond to the binary numbers from 0 to 111111, and concluded that this mapping was evidence of major Chinese accomplishments in the sort of philosophical [[mathematics]] he admired.<ref>{{Cite book\n|last=Aiton\n|first=Eric J.\n|title=Leibniz: A Biography\n|year=1985\n|publisher=Taylor & Francis\n|isbn=0-85274-470-6\n|pages=245–8\n|postscript=<!--None-->\n}}</ref>\nLeibniz was first introduced to the ''[[I Ching]]'' through his contact with the French Jesuit [[Joachim Bouvet]], who visited China in 1685 as a missionary. Leibniz saw the ''I Ching'' hexagrams as an affirmation of the [[Universality (philosophy)|universality]] of his own religious beliefs as a Christian.<ref name=\"smith\">{{cite book|author1=J.E.H. Smith|title=Leibniz: What Kind of Rationalist?: What Kind of Rationalist?|url=https://books.google.com/books?id=Da_oP3sJs1oC&pg=PA4153|year=2008|publisher=Springer|isbn=978-1-4020-8668-7|page=415}}</ref> Binary numerals were central to Leibniz's theology. He believed that binary numbers were symbolic of the Christian idea of ''[[ex nihilo|creatio ex nihilo]]'' or creation out of nothing.<ref name=\"lniz\">{{cite book|author1=Yuen-Ting Lai|title=Leibniz, Mysticism and Religion|url=https://books.google.com/books?id=U9dOmVt81UAC&pg=PA149|year=1998|publisher=Springer|isbn=978-0-7923-5223-5|pages=149–150}}</ref>\n\n{{quote|[A concept that] is not easy to impart to the pagans, is the creation ''ex nihilo'' through God's almighty power. Now one can say that nothing in the world can better present and demonstrate this power than the origin of numbers, as it is presented here through the simple and unadorned presentation of One and Zero or Nothing.|Leibniz's letter to the [[Rudolph Augustus, Duke of Brunswick-Lüneburg|Duke of Brunswick]] attached with the ''I Ching'' hexagrams<ref name=\"smith\"/>}}\n\n===Later developments===\n[[File:George Boole color.jpg|thumb|left|160px|George Boole]]\nIn 1854, British mathematician [[George Boole]] published a landmark paper detailing an [[algebra]]ic system of [[logic]] that would become known as [[Boolean algebra (logic)|Boolean algebra]]. His logical calculus was to become instrumental in the design of digital electronic circuitry.<ref>{{cite book |last=Boole |first=George |origyear=1854 |url=http://www.gutenberg.org/etext/15114 |title=An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities |publisher=Cambridge University Press |edition=Macmillan, Dover Publications, reprinted with corrections [1958] |location=New York |year=2009 |isbn=978-1-108-00153-3}}</ref>\n\nIn 1937, [[Claude Shannon]] produced his master's thesis at [[MIT]] that implemented Boolean algebra and binary arithmetic using electronic relays and switches for the first time in history. Entitled ''[[A Symbolic Analysis of Relay and Switching Circuits]]'', Shannon's thesis essentially founded practical [[digital circuit]] design.<ref>{{cite book |title=A symbolic analysis of relay and switching circuits |last=Shannon |first=Claude Elwood |publisher=Massachusetts Institute of Technology |location=Cambridge |year=1940 |url=http://hdl.handle.net/1721.1/11173}}</ref>\n\nIn November 1937, [[George Stibitz]], then working at [[Bell Labs]], completed a relay-based computer he dubbed the \"Model K\" (for \"'''K'''itchen\", where he had assembled it), which calculated using binary addition.<ref>{{cite web |url=http://www.invent.org/hall_of_fame/140.html |title=National Inventors Hall of Fame – George R. Stibitz |date=20 August 2008 |accessdate=5 July 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20100709213530/http://www.invent.org/hall_of_fame/140.html |archivedate=9 July 2010 |df=dmy-all }}</ref> Bell Labs authorized a full research program in late 1938 with Stibitz at the helm. Their Complex Number Computer, completed 8 January 1940, was able to  calculate [[complex numbers]]. In a demonstration to the [[American Mathematical Society]] conference at [[Dartmouth College]] on 11 September 1940, Stibitz was able to send the Complex Number Calculator remote commands over telephone lines by a [[teletype]]. It was the first computing machine ever used remotely over a phone line. Some participants of the conference who witnessed the demonstration were [[John von Neumann]], [[John Mauchly]] and [[Norbert Wiener]], who wrote about it in his memoirs.<ref>{{cite web|url=http://stibitz.denison.edu/bio.html |title=George Stibitz : Bio |publisher=Math & Computer Science Department, Denison University |date=30 April 2004 |accessdate=5 July 2010 }}</ref><ref>{{cite web|url=http://www.kerryr.net/pioneers/stibitz.htm |title=Pioneers – The people and ideas that made a difference – George Stibitz (1904–1995) |publisher=Kerry Redshaw |date=20 February 2006 |accessdate=5 July 2010 }}</ref><ref>{{cite web|url=http://ei.cs.vt.edu/~history/Stibitz.html |title=George Robert Stibitz – Obituary |publisher=Computer History Association of California |date=6 February 1995 |accessdate=5 July 2010}}</ref>\n\nThe [[Z1 (computer)|Z1 computer]], which was designed and built by [[Konrad Zuse]] between 1935 and 1938, used Boolean logic and binary [[floating point numbers]].<ref name=zuse>{{cite journal |url=http://ed-thelen.org/comp-hist/Zuse_Z1_and_Z3.pdf  |title=Konrad Zuse’s Legacy: The Architecture of the Z1 and Z3 |journal=IEEE Annals of the History of Computing |volume=19 |number=2 |year=1997 |pages=5–15 |doi=10.1109/85.586067}}</ref>\n\n==Representation==\nAny number can be represented by a sequence of [[bit]]s (binary digits), which in turn may be represented by any mechanism capable of being in two mutually exclusive states. Any of the following rows of symbols can  be interpreted as the binary numeric value of 667:\n\n{| style=\"text-align:center;\"\n| 1 || 0 || 1 || 0 || 0 || 1 || 1 || 0 || 1 || 1\n|-\n| <nowiki>|</nowiki> || ― || <nowiki>|</nowiki> || ― || ― || <nowiki>|</nowiki> || <nowiki>|</nowiki> || ― || <nowiki>|</nowiki> || <nowiki>|</nowiki>\n|-\n| ☒ || ☐ || ☒ || ☐ || ☐ || ☒ || ☒ || ☐ || ☒ || ☒\n|-\n| y || n || y || n || n || y || y || n || y || y\n|}\n\n[[Image:Binary clock.svg|250px|thumbnail|right|A [[binary clock]] might use [[Light-emitting diode|LEDs]] to express binary values. In this clock, each column of LEDs shows a [[binary-coded decimal]] numeral of the traditional [[sexagesimal]] time.]]\n\nThe numeric value represented in each case is dependent upon the value assigned to each symbol. In the earlier days of computing, switches, punched holes and punched paper tapes were used to represent binary values.<ref>{{Cite web|url=https://www.bbc.com/bitesize/guides/zwsbwmn/revision/1|website=www.bbc.com|access-date=2019-06-26}}</ref> In a modern computer, the numeric values may be represented by two different [[voltage]]s; on a [[Magnetic field|magnetic]] [[Disk storage|disk]], [[Magnetic polarity|magnetic polarities]] may be used. A \"positive\", \"[[yes and no|yes]]\", or \"on\" state is not necessarily equivalent to the numerical value of one; it depends on the architecture in use.\n\nIn keeping with customary representation of numerals using [[Arabic numerals]], binary numbers are commonly written using the symbols '''0''' and '''1'''. When written, binary numerals are often subscripted, prefixed or suffixed in order to indicate their base, or radix. The following notations are equivalent:\n* 100101 binary (explicit statement of format)\n* 100101b (a suffix indicating binary format; also known as [[Intel convention]]<ref name=\"Kueveler-Schwoch_1996\">{{cite book|title=Arbeitsbuch Informatik - eine praxisorientierte Einführung in die Datenverarbeitung mit Projektaufgabe|language=German|first1=Gerd|last1=Küveler|first2=Dietrich|last2=Schwoch|date=2013|orig-year=1996|publisher=Vieweg-Verlag, reprint: Springer-Verlag|isbn=978-3-528-04952-2|id=9783322929075|doi=10.1007/978-3-322-92907-5|url=https://books.google.com/books?id=b8-dBgAAQBAJ|accessdate=2015-08-05}}</ref><ref name=\"Kueveler-Schwoch_2007\">{{cite book|title=Informatik für Ingenieure und Naturwissenschaftler: PC- und Mikrocomputertechnik, Rechnernetze|language=German|first1=Gerd|last1=Küveler|first2=Dietrich|last2=Schwoch|date=2007-10-04|publisher=Vieweg, reprint: Springer-Verlag|edition=5|volume=2|isbn=3834891916|id=9783834891914|url=https://books.google.com/books?id=xQbvPYxceY0C|accessdate=2015-08-05}}</ref>)\n* 100101B (a suffix indicating binary format)\n* bin 100101 (a prefix indicating binary format)\n* 100101<sub>2</sub> (a subscript indicating base-2 (binary) notation)\n* %100101 (a prefix indicating binary format; also known as [[Motorola convention]]<ref name=\"Kueveler-Schwoch_1996\"/><ref name=\"Kueveler-Schwoch_2007\"/>)\n* 0b100101 (a prefix indicating binary format, common in programming languages)\n* 6b100101 (a prefix indicating number of bits in binary format, common in programming languages)\n\nWhen spoken, binary numerals are usually read digit-by-digit, in order to distinguish them from decimal numerals. For example, the binary numeral 100 is pronounced ''one zero zero'', rather than ''one hundred'', to make its binary nature explicit, and for purposes of correctness. Since the binary numeral 100 represents the value four, it would be confusing to refer to the numeral as ''one hundred'' (a word that represents a completely different value, or amount). Alternatively, the binary numeral 100 can be read out as \"four\" (the correct ''value''), but this does not make its binary nature explicit.\n\n==Counting in binary==\n{{Aligned table |class=wikitable |cols=2\n|style=float:right; |rowstyle=text-align:right;\n|row1header=y\n| Decimal<br>number | Binary<br>number\n|  0 |    0\n|  1 |    1\n|  2 |   10\n|  3 |   11\n|  4 |  100\n|  5 |  101\n|  6 |  110\n|  7 |  111\n|  8 | 1000\n|  9 | 1001\n| 10 | 1010\n| 11 | 1011\n| 12 | 1100\n| 13 | 1101\n| 14 | 1110\n| 15 | 1111\n}}\nCounting in binary is similar to counting in any other number system. Beginning with a single digit, counting proceeds through each symbol, in increasing order. Before examining binary counting, it is useful to briefly discuss the more familiar [[decimal]] counting system as a frame of reference.\n\n===Decimal counting===\n[[Decimal]] counting uses the ten symbols ''0'' through ''9''.  Counting begins with the incremental substitution of the least significant digit (rightmost digit) which is often called the ''first digit''. When the available symbols for this position are exhausted, the least significant digit is reset to ''0'', and the next digit of higher significance (one position to the left) is incremented (''overflow''), and incremental substitution of the low-order digit resumes. This method of reset and overflow is repeated for each digit of significance. Counting progresses as follows:\n\n:000, 001, 002, ... 007, 008, 009, (rightmost digit is reset to zero, and the digit to its left is incremented)\n:0'''1'''0, 011, 012, ...\n:&nbsp;&nbsp;&nbsp;...\n:090, 091, 092, ... 097, 098, 099, (rightmost two digits are reset to zeroes, and next digit is incremented)\n:'''1'''00, 101, 102, ...\n\n===Binary counting===\n[[File:Binary counter.gif|thumb|This counter shows how to count in binary from numbers zero through thirty-one.]]\nBinary counting follows the same procedure, except that only the two symbols ''0'' and ''1'' are available. Thus, after a digit reaches 1 in binary, an increment resets it to 0 but also causes an increment of the next digit to the left:\n\n:0000,\n:000'''1''', (rightmost digit starts over, and next digit is incremented)\n:00'''1'''0, 0011, (rightmost two digits start over, and next digit is incremented)\n:0'''1'''00, 0101, 0110, 0111, (rightmost three digits start over, and the next digit is incremented)\n:'''1'''000, 1001, 1010, 1011, 1100, 1101, 1110, 1111 ...\n\nIn the binary system, each digit represents an increasing power of 2, with the rightmost digit representing 2<sup>0</sup>, the next representing 2<sup>1</sup>, then 2<sup>2</sup>, and so on. The equivalent decimal representation of a binary number is the sum of the powers of 2 which each digit represents. For example, the binary number 100101 is converted to decimal form as follows:\n\n:100101<sub>2</sub> = [ ( '''1''' ) × 2<sup>5</sup> ] + [ ( '''0''' ) × 2<sup>4</sup> ] + [ ( '''0''' ) × 2<sup>3</sup> ] + [ ( '''1''' ) × 2<sup>2</sup> ] + [ ( '''0''' ) × 2<sup>1</sup> ] + [ ( '''1''' ) × 2<sup>0</sup> ]\n\n:100101<sub>2</sub> = [ '''1''' × 32 ] + [ '''0''' × 16 ] + [ '''0''' × 8 ] + [ '''1''' × 4 ] + [ '''0''' × 2 ] + [ '''1''' × 1 ]\n\n:'''100101<sub>2</sub> = 37<sub>10</sub>'''\n\n==Fractions==\n\nFractions in binary arithmetic terminate only if [[2 (number)|2]]  is the only [[prime factor]] in the [[denominator]]. As a result, 1/10 does not have a finite binary representation ('''10''' has prime factors '''2''' and '''5'''). This causes 10 × 0.1 not to precisely equal 1 in [[floating-point arithmetic]]. As an example, to interpret the binary expression for 1/3 = .010101..., this means: 1/3 = 0 × '''2<sup>−1</sup>''' + 1 × '''2<sup>−2</sup>''' + 0 × '''2<sup>−3</sup>''' +  1 × '''2<sup>−4</sup>''' + ... = 0.3125 + ... An exact value cannot be found with a sum of a finite number of inverse powers of two, the zeros and ones in the binary representation of 1/3 alternate forever.\n\n{| class=\"wikitable\"\n|-\n! Fraction\n! [[Base 10|Decimal]]\n! Binary\n! Fractional approximation\n|-\n| 1/1\n| 1{{pad|0.25em}}or{{pad|0.25em}}0.999...\n| 1{{pad|0.25em}}or{{pad|0.25em}}0.111...\n| 1/2 + 1/4 + 1/8...\n|-\n| 1/2\n| 0.5{{pad|0.25em}}or{{pad|0.25em}}0.4999...\n| 0.1{{pad|0.25em}}or{{pad|0.25em}}0.0111...\n| 1/4 + 1/8 + 1/16 . . .\n|-\n| 1/3\n| 0.333...\n| 0.010101...\n| 1/4 + 1/16 + 1/64 . . .\n|-\n| 1/4\n| 0.25{{pad|0.25em}}or{{pad|0.25em}}0.24999...\n| 0.01{{pad|0.25em}}or{{pad|0.25em}}0.00111...\n| 1/8 + 1/16 + 1/32 . . .\n|-\n| 1/5\n| 0.2{{pad|0.25em}}or{{pad|0.25em}}0.1999...\n| 0.00110011...\n| 1/8 + 1/16 + 1/128 . . .\n|-\n| 1/6\n| 0.1666...\n| 0.0010101...\n| 1/8 + 1/32 + 1/128 . . .\n|-\n| 1/7\n| 0.142857142857...\n| 0.001001...\n| 1/8 + 1/64 + 1/512 . . .\n|-\n| 1/8\n| 0.125{{pad|0.25em}}or{{pad|0.25em}}0.124999...\n| 0.001{{pad|0.25em}}or{{pad|0.25em}}0.000111...\n| 1/16 + 1/32 + 1/64 . . .\n|-\n| 1/9\n| 0.111...\n| 0.000111000111...\n| 1/16 + 1/32 + 1/64 . . .\n|-\n| 1/10\n| 0.1{{pad|0.25em}}or{{pad|0.25em}}0.0999...\n| 0.000110011...\n| 1/16 + 1/32 + 1/256 . . .\n|-\n| 1/11\n| 0.090909...\n| 0.00010111010001011101...\n| 1/16 + 1/64 + 1/128 . . .\n|-\n| 1/12\n| 0.08333...\n| 0.00010101...\n| 1/16 + 1/64 + 1/256 . . .\n|-\n| 1/13\n| 0.076923076923...\n| 0.000100111011000100111011...\n| 1/16 + 1/128 + 1/256 . . .\n|-\n| 1/14\n| 0.0714285714285...\n| 0.0001001001...\n| 1/16 + 1/128 + 1/1024 . . .\n|-\n| 1/15\n| 0.0666...\n| 0.00010001...\n| 1/16 + 1/256 . . .\n|-\n| 1/16\n| 0.0625{{pad|0.25em}}or{{pad|0.25em}}0.0624999...\n| 0.0001{{pad|0.25em}}or{{pad|0.25em}}0.0000111...\n| 1/32 + 1/64 + 1/128 . . .\n|}\n\n==Binary arithmetic==\n[[Arithmetic]] in binary is much like arithmetic in other numeral systems. Addition, subtraction, multiplication, and division can be performed on binary numerals.\n\n===Addition===\n{{main | Adder (electronics)}}\n[[Image:Half Adder.svg|thumbnail|200px|right|The [[circuit diagram]] for a binary [[Adder (electronics)|half adder]], which adds two bits together, producing sum and carry bits]]\n\nThe simplest arithmetic operation in binary is addition. Adding two single-digit binary numbers is relatively simple, using a form of carrying:\n\n:0 + 0 → 0\n:0 + 1 → 1\n:1 + 0 → 1\n:1 + 1 → 0, carry 1 (since 1 + 1 = 2 = 0 + (1 × 2<sup>1</sup>)  )\nAdding two \"1\" digits produces a digit \"0\", while 1 will have to be added to the next column. This is similar to what happens in decimal when certain single-digit numbers are added together; if the result equals or exceeds the value of the radix (10), the digit to the left is incremented:\n\n:5 + 5 → 0, carry 1 (since 5 + 5 = 10 = 0 + (1 × 10<sup>1</sup>)  )\n:7 + 9 → 6, carry 1 (since 7 + 9 = 16 = 6 + (1 × 10<sup>1</sup>)  )\n\nThis is known as ''carrying''. When the result of an addition exceeds the value of a digit, the procedure is to \"carry\" the excess amount divided by the radix (that is, 10/10) to the left, adding it to the next positional value. This is correct since the next position has a weight that is higher by a factor equal to the radix. Carrying works the same way in binary:\n\n   {{brown|1 1 1 1 1    (carried digits)}}\n     0 1 1 0 1\n +   1 0 1 1 1\n -------------\n = 1 0 0 1 0 0 = 36\n\nIn this example, two numerals are being added together: 01101<sub>2</sub> (13<sub>10</sub>) and 10111<sub>2</sub> (23<sub>10</sub>). The top row shows the carry bits used. Starting in the rightmost column, 1 + 1 = 10<sub>2</sub>. The 1 is carried to the left, and the 0 is written at the bottom of the rightmost column. The second column from the right is added: 1 + 0 + 1 = 10<sub>2</sub> again; the 1 is carried, and 0 is written at the bottom. The third column: 1 + 1 + 1 = 11<sub>2</sub>. This time, a 1 is carried, and a 1 is written in the bottom row. Proceeding like this gives the final answer 100100<sub>2</sub> (36 decimal).\n\nWhen computers must add two numbers, the rule that:\nx [[Exclusive or|xor]] y = (x + y) [[Modulo operation|mod]] 2\nfor any two bits x and y allows for very fast calculation, as well.\n\n====Long carry method====\nA simplification for many binary addition problems is the [[Long Carry Method]] or [[Brookhouse Method of Binary Addition]]. This method is generally useful in any binary addition in which one of the numbers contains a long \"string\" of ones. It is based on the simple premise that under the binary system, when given a \"string\" of digits composed entirely of {{varserif|n}} ones (''where:'' {{varserif|n}} is any integer length), adding 1 will result in the number 1 followed by a string of {{var|n}} zeros. That concept follows, logically, just as in the decimal system, where adding 1 to a string of {{varserif|n}} 9s will result in the number 1 followed by a string of {{var|n}} 0s:\n\n      Binary                        Decimal\n     1 1 1 1 1     likewise        9 9 9 9 9\n  +          1                  +          1\n   ———————————                   ———————————\n   1 0 0 0 0 0                   1 0 0 0 0 0\n\nSuch long strings are quite common in the binary system.  From that one finds that large binary numbers can be added using two simple steps, without excessive carry operations. In the following example, two numerals are being added together: 1 1 1 0 1 1 1 1 1 0<sub>2</sub> (958<sub>10</sub>) and 1 0 1 0 1 1 0 0 1 1<sub>2</sub> (691<sub>10</sub>), using the traditional carry method on the left, and the long carry method on the right:\n\n Traditional Carry Method                       Long Carry Method\n                                 vs.\n   {{brown|1 1 1   1 1 1 1 1      (carried digits)   1 ←     1 ←}}            carry the 1 until it is one digit past the \"string\" below\n     1 1 1 0 1 1 1 1 1 0                       <s>1 1 1</s> 0 <s>1 1 1 1 1</s> 0  cross out the \"string\",\n +   1 0 1 0 1 1 0 0 1 1                   +   1 0 <s>1</s> 0 1 1 0 0 <s>1</s> 1  and cross out the digit that was added to it\n ———————————————————————                    ——————————————————————\n = 1 1 0 0 1 1 1 0 0 0 1                     1 1 0 0 1 1 1 0 0 0 1\n\nThe top row shows the carry bits used. Instead of the standard carry from one column to the next, the lowest-ordered \"1\" with a \"1\" in the corresponding place value beneath it may be added and a \"1\" may be carried to one digit past the end of the series.  The \"used\" numbers must be crossed off, since they are already added. Other long strings may likewise be cancelled using the same technique. Then, simply add together any remaining digits normally. Proceeding in this manner gives the final answer of 1 1 0 0 1 1 1 0 0 0 1<sub>2</sub> (1649<sub>10</sub>).  In our simple example using small numbers, the traditional carry method required eight carry operations, yet the long carry method required only two, representing a substantial reduction of effort.\n\n====Addition table====\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! style=\"width:1.5em\" |\n! style=\"width:1.5em\" | 0\n! style=\"width:1.5em\" | 1\n|-\n! 0\n| 0\n| 1\n|-\n! 1\n| 1\n| 10\n|}\n\nThe binary addition table is similar, but not the same, as the [[Logical disjunction#Truth table|truth table]] of the [[logical disjunction]] operation <math>\\lor</math>. The difference is that <math>1</math><math>\\lor </math><math>1=1</math>, while <math>1+1=10</math>.\n\n=== Subtraction ===\n{{further|signed number representations|two's complement}}\n\n[[Subtraction]] works in much the same way:\n\n:0 − 0 → 0\n:0 − 1 → 1, borrow 1\n:1 − 0 → 1\n:1 − 1 → 0\nSubtracting a \"1\" digit from a \"0\" digit produces the digit \"1\", while 1 will have to be subtracted from the next column. This is known as ''borrowing''. The principle is the same as for carrying. When the result of a subtraction is less than 0, the least possible value of a digit, the procedure is to \"borrow\" the deficit divided by the radix (that is, 10/10) from the left, subtracting it from the next positional value.\n\n     *   * * *   (starred columns are borrowed from)\n   1 1 0 1 1 1 0\n −     1 0 1 1 1\n ----------------\n = 1 0 1 0 1 1 1\n\n   *             (starred columns are borrowed from)\n   1 0 1 1 1 1 1\n -   1 0 1 0 1 1\n ----------------\n = 0 1 1 0 1 0 0\n\nSubtracting a positive number is equivalent to ''adding'' a [[negative number]] of equal [[absolute value]]. Computers use [[signed number representations]] to handle negative numbers—most commonly the [[two's complement]] notation. Such representations eliminate the need for a separate \"subtract\" operation. Using two's complement notation subtraction can be summarized by the following formula:\n\n'''A − B = A + not B + 1'''\n\n===Multiplication===<!-- This section is linked from [[Binary-coded decimal]] -->\n[[Multiplication]] in binary is similar to its decimal counterpart. Two numbers {{varserif|A}} and {{varserif|B}} can be multiplied by partial products: for each digit in {{varserif|B}}, the product of that digit in {{varserif|A}} is calculated and written on a new line, shifted leftward so that its rightmost digit lines up with the digit in {{varserif|B}} that was used. The sum of all these partial products gives the final result.\n\nSince there are only two digits in binary, there are only two possible outcomes of each partial multiplication:\n* If the digit in {{varserif|B}} is 0, the partial product is also 0\n* If the digit in {{varserif|B}} is 1, the partial product is equal to {{varserif|A}}\n\nFor example, the binary numbers 1011 and 1010 are multiplied as follows:\n\n            1 0 1 1   ({{varserif|A}})\n          × 1 0 1 0   ({{varserif|B}})\n          ---------\n            0 0 0 0   ← Corresponds to the rightmost 'zero' in {{varserif|B}}\n    +     1 0 1 1     ← Corresponds to the next 'one' in {{varserif|B}}\n    +   0 0 0 0\n    + 1 0 1 1\n    ---------------\n    = 1 1 0 1 1 1 0\n\nBinary numbers can also be multiplied with bits after a [[binary point]]:\n\n                1 0 1 . 1 0 1     {{varserif|A}} (5.625 in decimal)\n              × 1 1 0 . 0 1       {{varserif|B}} (6.25  in decimal)\n              -------------------\n                    1 . 0 1 1 0 1   ← Corresponds to a 'one' in {{varserif|B}}\n      +           0 0 . 0 0 0 0     ← Corresponds to a 'zero' in {{varserif|B}}\n      +         0 0 0 . 0 0 0\n      +       1 0 1 1 . 0 1\n      +     1 0 1 1 0 . 1\n      ---------------------------\n      =   1 0 0 0 1 1 . 0 0 1 0 1  (35.15625 in decimal)\n\nSee also [[Booth's multiplication algorithm]].\n\n====Multiplication table====\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! style=\"width:1.5em\" |\n! style=\"width:1.5em\" | 0\n! style=\"width:1.5em\" | 1\n|-\n! 0\n| 0\n| 0\n|-\n! 1\n| 0\n| 1\n|}\n\nThe binary multiplication table is the same as the [[Logical conjunction#Truth table|truth table]] of the [[logical conjunction]] operation <math>\\land</math>.\n\n===Division===\n:{{See also|Division algorithm}}\n[[Long division]] in binary is again similar to its decimal counterpart.\n\nIn the example below, the [[divisor]] is 101<sub>2</sub>, or 5 in decimal, while the [[Division (mathematics)|dividend]] is 11011<sub>2</sub>, or 27 in decimal. The procedure is the same as that of decimal [[long division]]; here, the divisor 101<sub>2</sub> goes into the first three digits 110<sub>2</sub> of the dividend one time, so a \"1\" is written on the top line. This result is multiplied by the divisor, and subtracted from the first three digits of the dividend; the next digit (a \"1\") is included to obtain a new three-digit sequence:\n\n               1\n         ___________\n 1 0 1   ) 1 1 0 1 1\n         − 1 0 1\n           -----\n           0 0 1\n\nThe procedure is then repeated with the new sequence, continuing until the digits in the dividend have been exhausted:\n\n              1 0 1\n        ___________\n 1 0 1  ) 1 1 0 1 1\n        − 1 0 1\n          -----\n              1 1 1\n          −   1 0 1\n              -----\n                1 0\n\nThus, the [[quotient]] of 11011<sub>2</sub> divided by 101<sub>2</sub> is 101<sub>2</sub>, as shown on the top line, while the remainder, shown on the bottom line, is 10<sub>2</sub>. In decimal, this corresponds to the fact that 27 divided by 5 is 5, with a remainder of 2.\n\nAside from long division, one can also devise the procedure so as to allow for over-subtracting from the partial remainder at each iteration, thereby leading to alternative methods which are less systematic, but more flexible as a result.<ref>{{Cite web|url=https://mathvault.ca/long-division/|title=The Definitive Higher Math Guide to Long Division and Its Variants — for Integers|date=2019-02-24|website=Math Vault|language=en-US|access-date=2019-06-26}}</ref>\n\n===Square root===\nThe process of taking a binary square root digit by digit is the same as for a decimal square root, and is explained [[Methods of computing square roots#Binary numeral system (base 2)|here]]. An example is:\n\n              1 0 0 1\n             ---------\n            √ 1010001\n              1\n             ---------\n       101     01 \n                0\n              --------\n       1001     100\n                  0\n              --------\n       10001    10001\n                10001\n               -------\n                    0\n\n==Bitwise operations==\n{{Main|bitwise operation}}\nThough not directly related to the numerical interpretation of binary symbols, sequences of bits may be manipulated using [[logical connective|Boolean logical operators]]. When a string of binary symbols is manipulated in this way, it is called a [[bitwise operation]]; the logical operators [[Logical conjunction|AND]], [[Logical disjunction|OR]], and [[Exclusive disjunction|XOR]] may be performed on corresponding bits in two binary numerals provided as input. The logical [[Negation|NOT]] operation may be performed on individual bits in a single binary numeral provided as input. Sometimes, such operations may be used as arithmetic short-cuts, and may have other computational benefits as well.  For example, an [[arithmetic shift]] left of a binary number is the equivalent of multiplication by a (positive, integral) power of 2.\n\n==Conversion to and from other numeral systems==\n\n===Decimal===\n[[File:Decimal to Binary Conversion.gif|alt=|frame|Conversion of (357)<sub>10</sub> to binary notation results in (101100101)]]\nTo convert from a base-10 [[Integer (computer science)|integer]] to its base-2 (binary) equivalent, the number is [[division by two|divided by two]]. The remainder is the [[least-significant bit]]. The quotient is again divided by two; its remainder becomes the next least significant bit. This process repeats until a quotient of one is reached. The sequence of remainders (including the final quotient of one) forms the binary value, as each remainder must be either zero or one when dividing by two. For example, (357)<sub>10</sub> is expressed as (101100101)<sub>2.</sub><ref>{{Cite web|url=https://www.chalkstreet.com/aptipedia/knowledgebase/base-system/|title=Base System|last=|first=|date=|website=|publisher=|access-date=31 August 2016}}</ref>\n\nConversion from base-2 to base-10 simply inverts the preceding algorithm. The bits of the binary number are used one by one, starting with the most significant (leftmost) bit. Beginning with the value 0, the prior value is doubled, and the next bit is then added to produce the next value. This can be organized in a multi-column table. For example, to convert 10010101101<sub>2</sub> to decimal:\n\n:{| class=\"wikitable\"\n!Prior value\n!× 2 +\n!Next bit\n!Next value\n|-\n|align=\"right\"|0 ||× 2 +|| '''1''' || = 1\n|-\n|align=\"right\"|1 ||× 2 +|| '''0''' || = 2\n|-\n|align=\"right\"|2 ||× 2 +|| '''0''' || = 4\n|-\n|align=\"right\"|4 ||× 2 +|| '''1''' || = 9\n|-\n|align=\"right\"|9 ||× 2 +|| '''0''' || = 18\n|-\n|align=\"right\"|18 ||× 2 +|| '''1''' || = 37\n|-\n|align=\"right\"|37 ||× 2 +|| '''0''' || = 74\n|-\n|align=\"right\"|74 ||× 2 +|| '''1''' || = 149\n|-\n|align=\"right\"|149 ||× 2 +|| '''1''' || = 299\n|-\n|align=\"right\"|299 ||× 2 +|| '''0''' || = 598\n|-\n|align=\"right\"|598 ||× 2 +|| '''1''' || = '''1197'''\n|}\n\nThe result is 1197<sub>10</sub>. Note that the first Prior Value of 0 is simply an initial decimal value. This method is an application of the [[Horner scheme]].\n\n{|\n! Binary&nbsp;\n| 1 || 0 || 0 || 1 || 0 || 1 || 0 || 1 || 1 || 0 || 1 ||\n|-\n! Decimal&nbsp;\n| 1×2<sup>10</sup> + || 0×2<sup>9</sup> + || 0×2<sup>8</sup> + || 1×2<sup>7</sup> + || 0×2<sup>6</sup> + || 1×2<sup>5</sup> + || 0×2<sup>4</sup> + || 1×2<sup>3</sup> + || 1×2<sup>2</sup> + || 0×2<sup>1</sup> + || 1×2<sup>0</sup> = || 1197\n|}\n\nThe fractional parts of a number are converted with similar methods. They are again based on the equivalence of shifting with doubling or halving.\n\nIn a fractional binary number such as 0.11010110101<sub>2</sub>, the first digit is <math>\\begin{matrix} \\frac{1}{2} \\end{matrix}</math>, the second <math>\\begin{matrix} (\\frac{1}{2})^2 = \\frac{1}{4} \\end{matrix}</math>, etc. So if there is a 1 in the first place after the decimal, then the number is at least <math>\\begin{matrix} \\frac{1}{2} \\end{matrix}</math>, and vice versa. Double that number is at least 1. This suggests the algorithm: Repeatedly double the number to be converted, record if the result is at least 1, and then throw away the integer part.\n\nFor example, <math>\\begin{matrix} (\\frac{1}{3}) \\end{matrix}</math><sub>10</sub>, in binary, is:\n\n:{| class=\"wikitable\"\n!Converting!!Result\n|-\n|<math>\\begin{matrix} \\frac{1}{3} \\end{matrix}</math> || 0.\n|-\n|<math>\\begin{matrix} \\frac{1}{3} \\times 2 = \\frac{2}{3} < 1 \\end{matrix}</math> || 0.0\n|-\n|<math>\\begin{matrix} \\frac{2}{3} \\times 2 = 1\\frac{1}{3} \\ge 1 \\end{matrix}</math> || 0.01\n|-\n|<math>\\begin{matrix} \\frac{1}{3} \\times 2 = \\frac{2}{3} < 1 \\end{matrix}</math> || 0.010\n|-\n|<math>\\begin{matrix} \\frac{2}{3} \\times 2 = 1\\frac{1}{3} \\ge 1 \\end{matrix}</math> || 0.0101\n|}\n\nThus the repeating decimal fraction 0.{{overline|3}}... is equivalent to the repeating binary fraction 0.{{overline|01}}... .\n\nOr for example, 0.1<sub>10</sub>, in binary, is:\n\n:{| class=\"wikitable\"\n! Converting                !! Result\n|-\n|          '''0.1'''        || 0.\n|-\n|0.1 × 2 = '''0.2''' < 1    || 0.0\n|-\n|0.2 × 2 = '''0.4''' < 1    || 0.00\n|-\n|0.4 × 2 = '''0.8''' < 1    || 0.000\n|-\n|0.8 × 2 = '''1.6''' ≥ 1 || 0.0001\n|-\n|0.6 × 2 = '''1.2''' ≥ 1 || 0.00011\n|-\n|0.2 × 2 = '''0.4''' < 1    || 0.000110\n|-\n|0.4 × 2 = '''0.8''' < 1    || 0.0001100\n|-\n|0.8 × 2 = '''1.6''' ≥ 1 || 0.00011001\n|-\n|0.6 × 2 = '''1.2''' ≥ 1 || 0.000110011\n|-\n|0.2 × 2 = '''0.4''' < 1    || 0.0001100110\n|}\n\nThis is also a repeating binary fraction 0.0{{overline|0011}}... .  It may come as a surprise that terminating decimal fractions can have repeating expansions in binary. It is for this reason that many are surprised to discover that 0.1 + ... + 0.1, (10 additions) differs from 1 in [[floating point arithmetic]]. In fact, the only binary fractions with terminating expansions are of the form of an integer divided by a power of 2, which 1/10 is not.\n\nThe final conversion is from binary to decimal fractions. The only difficulty arises with repeating fractions, but otherwise the method is to shift the fraction to an integer, convert it as above, and then divide by the appropriate power of two in the decimal base. For example:\n\n: <math>\n\\begin{align}\nx & = & 1100&.1\\overline{01110}\\ldots \\\\\nx\\times 2^6 & = & 1100101110&.\\overline{01110}\\ldots \\\\\nx\\times 2 & = & 11001&.\\overline{01110}\\ldots \\\\\nx\\times(2^6-2) & = & 1100010101 \\\\\nx & = & 1100010101/111110 \\\\\nx & = & (789/62)_{10}\n\\end{align}\n</math>\n\nAnother way of converting from binary to decimal, often quicker for a person familiar with [[hexadecimal]], is to do so indirectly—first converting (<math>x</math> in binary) into (<math>x</math> in hexadecimal) and then converting (<math>x</math> in hexadecimal) into (<math>x</math> in decimal).\n\nFor very large numbers, these simple methods are inefficient because they perform a large number of multiplications or divisions where one operand is very large. A simple divide-and-conquer algorithm is more effective asymptotically: given a binary number, it is divided by 10<sup>''k''</sup>, where ''k'' is chosen so that the quotient roughly equals the remainder; then each of these pieces is converted to decimal and the two are [[Concatenation|concatenated]]. Given a decimal number, it can be split into two pieces of about the same size, each of which is converted to binary, whereupon the first converted piece is multiplied by 10<sup>''k''</sup> and added to the second converted piece, where ''k'' is the number of decimal digits in the second, least-significant piece before conversion.\n\n===Hexadecimal===\n{{Main|Hexadecimal}}\n{{Hexadecimal table}}\nBinary may be converted to and from hexadecimal more easily. This is because the [[radix]] of the hexadecimal system (16) is a power of the radix of the binary system (2). More specifically, 16 = 2<sup>4</sup>, so it takes four digits of binary to represent one digit of hexadecimal, as shown in the adjacent table.\n\nTo convert a hexadecimal number into its binary equivalent, simply substitute the corresponding binary digits:\n\n:3A<sub>16</sub> = 0011 1010<sub>2</sub>\n:E7<sub>16</sub> = 1110 0111<sub>2</sub>\n\nTo convert a binary number into its hexadecimal equivalent, divide it into groups of four bits. If the number of bits isn't a multiple of four, simply insert extra '''0''' bits at the left (called [[Padding (cryptography)#Bit padding|padding]]). For example:\n\n:1010010<sub>2</sub> = 0101 0010 grouped with padding = 52<sub>16</sub>\n:11011101<sub>2</sub> = 1101 1101 grouped = DD<sub>16</sub>\n\nTo convert a hexadecimal number into its decimal equivalent, multiply the decimal equivalent of each hexadecimal digit by the corresponding power of 16 and add the resulting values:\n\n:C0E7<sub>16</sub> = (12 × 16<sup>3</sup>) + (0 × 16<sup>2</sup>) + (14 × 16<sup>1</sup>) + (7 × 16<sup>0</sup>) = (12 × 4096) + (0 × 256) + (14 × 16) + (7 × 1) = 49,383<sub>10</sub>\n\n===Octal===\n{{Main|Octal}}\nBinary is also easily converted to the [[octal]] numeral system, since octal uses a radix of 8, which is a [[power of two]] (namely, 2<sup>3</sup>, so it takes exactly three binary digits to represent an octal digit). The correspondence between octal and binary numerals is the same as for the first eight digits of [[hexadecimal]] in the table above. Binary 000 is equivalent to the octal digit 0, binary 111 is equivalent to octal 7, and so forth.\n\n:{| class=\"wikitable\"  style=\"text-align:center\"\n!Octal!!Binary\n|-\n| 0 || 000\n|-\n| 1 || 001\n|-\n| 2 || 010\n|-\n| 3 || 011\n|-\n| 4 || 100\n|-\n| 5 || 101\n|-\n| 6 || 110\n|-\n| 7 || 111\n|}\n\nConverting from octal to binary proceeds in the same fashion as it does for [[hexadecimal]]:\n\n:65<sub>8</sub> = 110 101<sub>2</sub>\n:17<sub>8</sub> = 001 111<sub>2</sub>\n\nAnd from binary to octal:\n\n:101100<sub>2</sub> = 101 100<sub>2</sub> grouped = 54<sub>8</sub>\n:10011<sub>2</sub> = 010 011<sub>2</sub> grouped with padding = 23<sub>8</sub>\n\nAnd from octal to decimal:\n\n:65<sub>8</sub> = (6 × 8<sup>1</sup>) + (5 × 8<sup>0</sup>) = (6 × 8) + (5 × 1) = 53<sub>10</sub>\n:127<sub>8</sub> = (1 × 8<sup>2</sup>) + (2 × 8<sup>1</sup>) + (7 × 8<sup>0</sup>) = (1 × 64) + (2 × 8) + (7 × 1) = 87<sub>10</sub>\n\n==Representing real numbers==<!-- This section is linked from [[Chaitin's constant]] -->\nNon-integers can be represented by using negative powers, which are set off from the other digits by means of a [[radix point]] (called a [[decimal point]] in the decimal system). For example, the binary number 11.01<sub>2</sub> thus means:\n\n:{|\n|'''1''' × 2<sup>1</sup>  || (1 × 2 = '''2''')           || plus\n|-\n|'''1''' × 2<sup>0</sup>  || (1 × 1 = '''1''')           || plus\n|-\n|'''0''' × 2<sup>−1</sup> || (0 × {{frac|2}} = '''0''')    || plus\n|-\n|'''1''' × 2<sup>−2</sup> || (1 × {{frac|4}} = '''0.25''')\n|}\n\nFor a total of 3.25 decimal.\n\nAll [[dyadic fraction|dyadic rational numbers]] <math>\\frac{p}{2^a}</math> have a ''terminating'' binary numeral—the binary representation has a finite number of terms after the radix point.  Other [[rational numbers]] have binary representation, but instead of terminating, they ''recur'', with a finite sequence of digits repeating indefinitely. For instance\n\n:<math>\\frac{1_{10}}{3_{10}} = \\frac{1_2}{11_2} = 0.01010101\\overline{01}\\ldots\\,_2 </math>\n\n:<math>\\frac{12_{10}}{17_{10}} = \\frac{1100_2}{10001_2} = 0.10110100 10110100\\overline{10110100}\\ldots\\,_2 </math>\n\nThe phenomenon that the binary representation of any rational is either terminating or recurring also occurs in other radix-based numeral systems.  See, for instance, the explanation in [[decimal]].  Another similarity is the existence of alternative representations for any terminating representation, relying on the fact that 0.111111... is the sum of the [[geometric series]] 2<sup>−1</sup> + 2<sup>−2</sup> + 2<sup>−3</sup> + ... which is 1.\n\nBinary numerals which neither terminate nor recur represent [[irrational number]]s.  For instance,\n* 0.10100100010000100000100... does have a pattern, but it is not a fixed-length recurring pattern, so the number is irrational\n* 1.0110101000001001111001100110011111110... is the binary representation of <math>\\sqrt{2}</math>, the [[square root of 2]], another irrational.  It has no discernible pattern. See [[irrational number]].\n\n== See also ==\n\n{{Portal|Mathematics|Information technology}}\n* [[Binary code]]\n* [[Binary-coded decimal]]\n* [[Finger binary]]\n* [[Gray code]]\n* [[IEEE 754]]\n* [[Linear feedback shift register]]\n* [[Offset binary]]\n* [[Quibinary]]\n* [[Reduction of summands]]\n* [[Redundant binary representation]]\n* [[Repeating decimal]]\n* [[Two's complement]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite book |last1=Sanchez |first1=Julio |last2=Canton |first2=Maria P. |year=2007 |title=Microcontroller programming: the microchip PIC |location=Boca Raton, FL |publisher=CRC Press |page=37 |isbn=0-8493-7189-9}}\n* {{cite book|ref=harv|last1=Redmond|first1=Geoffrey|last2=Hon|first2=Tze-Ki|title=Teaching the I Ching|date=2014|publisher=Oxford University Press|isbn=0-19-976681-9}}\n\n==External links==\n<!--=========================================================\n\n  There are MANY web pages devoted to binary numbers. This section should not be a repository of such web pages.\n  Before adding another external link here, make sure it provides a unique resource beyond what is already here.\n  We do not need yet another explanation of the binary number system.\n\n======\n======\n======\n======\n========\n-->\n{{Commons category|Binary numeral system}}\n* [http://www.cut-the-knot.org/do_you_know/BinaryHistory.shtml Binary System] at [[cut-the-knot]]\n* [http://www.cut-the-knot.org/blue/frac_conv.shtml Conversion of Fractions] at [[cut-the-knot]]\n* [http://www.baconlinks.com/docs/BILITERAL.doc Sir Francis Bacon's BiLiteral Cypher system], predates binary number system.\n* Leibniz' binary numeral system, 'De progressione dyadica', 1679, online and analyzed on ''[https://www.bibnum.education.fr/calculinformatique/calcul/de-la-numeration-binaire BibNum]'' <small>[click 'à télécharger' for English analysis]</small>\n\n{{Authority control}}\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Elementary arithmetic]]\n[[Category:Positional numeral systems]]\n[[Category:Gottfried Leibniz]]"
    },
    {
      "title": "Binary-coded decimal",
      "url": "https://en.wikipedia.org/wiki/Binary-coded_decimal",
      "text": "{{redir|BCD code|BCD character sets|BCD (character encoding)||}}\n{{Use dmy dates|date=May 2019|cs1-dates=y}}\n[[File:Binary clock.svg|250px|thumbnail|right|A [[binary clock]] might use [[Light-emitting diode|LED]]s to express binary values. In this clock, each column of LEDs shows a binary-coded decimal numeral of the traditional [[sexagesimal]] time.]]\n\nIn [[computing]] and [[electronics|electronic]] systems, '''binary-coded decimal''' ('''BCD''') is a class of [[Binary numeral system|binary]] encodings of [[decimal]] numbers where each decimal [[numerical digit|digit]] is represented by a fixed number of [[bit]]s, usually four or eight. Special bit patterns are sometimes used for a [[Sign (mathematics)|sign]] or for other indications (e.g., error or overflow).\n\nIn byte-oriented systems (i.e. most modern computers), the term ''unpacked'' BCD<ref name=\"Intel_IA32\"/> usually implies a full [[byte]] for each digit (often including a sign), whereas ''packed'' BCD typically encodes two decimal digits within a single byte by taking advantage of the fact that four bits are enough to represent the range 0 to 9. The precise 4-bit encoding may vary however, for technical reasons, see [[Excess-3]] for instance. The ten states representing a BCD decimal digit are sometimes called ''tetrades'' (for the [[nibble]] typically needed to hold them also known as [[tetrade (computing)|tetrade]]) with those [[don't care]]-states unused named {{Interlanguage link multi|pseudo-tetrade|de|3=Pseudotetrade|lt=''pseudo-tetrad(e)s''}}<ref name=\"Schneider_1986\"/><ref name=\"Tafel_1971\"/><ref name=\"Steinbuch-Weber_1974\"/><ref name=\"Tietze-Schenk_2013\"/><ref name=\"Kowalski_1070\"/> or ''pseudo-decimal digit''<ref name=\"Ferretti_2013\"/><ref name=\"Speiser_1965\"/>).<ref group=\"nb\" name=\"Pseudo-tetrades\"/>\n\nBCD's main virtue is its more accurate representation and rounding of decimal quantities as well as an ease of conversion into human-readable representations, in comparison to binary [[positional system]]s. BCD's principal drawbacks are a small increase in the complexity of the circuits needed to implement basic arithmetics and a slightly less dense storage.\n\nBCD was used in many early [[decimal computer]]s, and is implemented in the instruction set of machines such as the [[IBM System/360]] series and its descendants, [[Digital Equipment Corporation]]'s [[VAX]], the [[Burroughs B1700]], and the Motorola [[68000]]-series processors. Although BCD ''per se'' is not as widely used as in the past and is no longer implemented in newer computers' instruction sets (such as [[ARM architecture|ARM]]; [[x86]] does not support [[Intel BCD opcode|its BCD instruction]]s in [[long mode]] any more), decimal [[Fixed-point arithmetic|fixed-point]] and [[floating-point]] formats are still important and continue to be used in financial, commercial, and industrial computing, where subtle conversion and [[fraction (mathematics)|fractional]] [[rounding]] [[round-off error|errors]] that are inherent in floating point binary representations cannot be tolerated.<ref name=\"Cowlishaw_GDA\"/>\n\n=={{anchor|Unpacked BCD|NBCD|8-4-2-1}}Basics==\nBCD takes advantage of the fact that any one decimal numeral can be represented by a four bit pattern. The most obvious way of encoding digits is \"natural BCD\" (NBCD), where each decimal digit is represented by its corresponding four-bit binary value, as shown in the following table. This is also called \"8421\" encoding.\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! scope=\"col\" rowspan=\"2\" | Decimal digit\n! scope=\"col\" colspan=\"4\" | BCD\n|-\n! 8 !! 4 !! 2 !! 1\n|-\n| 0 || 0 || 0 || 0 || 0\n|-\n| 1 || 0 || 0 || 0 || 1\n|-\n| 2 || 0 || 0 || 1 || 0\n|-\n| 3 || 0 || 0 || 1 || 1\n|-\n| 4 || 0 || 1 || 0 || 0\n|-\n| 5 || 0 || 1 || 0 || 1\n|-\n| 6 || 0 || 1 || 1 || 0\n|-\n| 7 || 0 || 1 || 1 || 1\n|-\n| 8 || 1 || 0 || 0 || 0\n|-\n| 9 || 1 || 0 || 0 || 1\n|}\n\nOther encodings are also used, including so-called \"4221\" and \"7421\"—named after the weighting used for the bits—and \"[[Excess-3]]\".<ref>{{cite book |author-last=Parag K. |author-first=Lala |title=Principles of Modern Digital Design |date=2007 |publisher=[[John Wiley & Sons]] |isbn=978-0-470-07296-7 |pages=20–25 |url=https://books.google.com/books?id=doNGOrHUyCoC&lpg=PA20}}</ref> For example, the BCD digit 6, '0110'b in 8421 notation, is '1100'b in 4221 (two encodings are possible), '0110'b in 7421, and '1001'b (6+3=9) in excess-3.\n\nAs most computers deal with data in 8-bit [[byte]]s, it is possible to use one of the following methods to encode a BCD number:\n* '''Unpacked''': Each number is encoded into one byte, with four bits representing the number and the remaining bits having no significance.\n* '''Packed''': Two number are encoded into a single byte, with one number in the least significant [[nibble]] (bits 0 through 3) and the other numeral in the most significant nibble (bits 4 through 7).\n\nAs an example, encoding the decimal number <tt>'''91'''</tt> using unpacked BCD results in the following binary pattern of two bytes:\n  Decimal:          9          1\n  Binary :  0000 1001  0000 0001\n\nIn packed BCD, the same number would fit into a single byte:\n  Decimal:     9    1\n  Binary :  1001 0001\n\nHence the numerical range for one unpacked BCD byte is zero through nine inclusive, whereas the range for one packed BCD is zero through ninety-nine inclusive.\n\nTo represent numbers larger than the range of a single byte any number of contiguous bytes may be used. For example, to represent the decimal number <tt>'''12345'''</tt> in packed BCD, using [[big-endian]] format, a program would encode as follows:\n  Decimal:     0    1     2    3     4    5\n  Binary :  0000 0001  0010 0011  0100 0101\n\nHere, the most significant nibble of the most significant byte has been encoded as zero, so the number is stored as <tt>'''012345'''</tt> (but formatting routines might replace or remove leading zeros). Packed BCD is more efficient in storage usage than unpacked BCD; encoding the same number (with the leading zero) in unpacked format would consume twice the storage.\n\n[[logical shift|Shifting]] and [[mask (computing)|masking]] operations are used to pack or unpack a packed BCD digit. Other [[bitwise operation]]s are used to convert a numeral to its equivalent bit pattern or reverse the process.\n\n==BCD in electronics==\n{{Refimprove section|date=January 2018}}{{Primary sources|section|date=January 2018}}\n\nBCD is very common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By employing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit. This matches much more closely the physical reality of display hardware—a designer might choose to use a series of separate identical [[seven-segment display]]s to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing to such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple, working throughout with BCD can lead to a simpler overall system than converting to and from binary. Most pocket calculators do all their calculations in BCD.\n\nThe same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, smaller code results when representing numbers internally in BCD format, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature BCD arithmetic modes, which assist when writing routines that manipulate BCD quantities.<ref>{{cite web |author=University of Alicante |title=A Cordic-based Architecture for High Performance Decimal Calculations |url=http://rua.ua.es/dspace/bitstream/10045/11826/1/VF-016519.pdf |publisher=[[IEEE]] |access-date=2015-08-15}}</ref><ref>{{cite web |title=Decimal CORDIC Rotation based on Selection by Rounding: Algorithm and Architecture |url=http://faculties.sbu.ac.ir/~jaberipur/Papers/Journals/19.pdf |publisher=[[British Computer Society]] |access-date=2015-08-14}}</ref>\n\n==Packed BCD==\n<!-- Section header used in redirects -->\nIn '''packed BCD''' (or simply '''packed decimal'''), each of the two [[nibble]]s of each byte represent a decimal digit. Packed BCD has been in use since at least the 1960s and is implemented in all IBM mainframe hardware since then. Most implementations are [[big endian]], i.e. with the more significant digit in the upper half of each byte, and with the leftmost byte (residing at the lowest memory address) containing the most significant digits of the packed decimal value. The lower nibble of the rightmost byte is usually used as the sign flag, although some unsigned representations lack a sign flag. As an example, a 4-byte value consists of 8 nibbles, wherein the upper 7 nibbles store the digits of a 7-digit decimal value and the lowest nibble indicates the sign of the decimal integer value.\n\nStandard sign values are 1100 ([[hexadecimal|hex]] C) for positive (+) and 1101 (D) for negative (−). This convention comes from the zone field for [[EBCDIC]] characters and the [[signed overpunch]] representation. Other allowed signs are 1010 (A) and 1110 (E) for positive and 1011 (B) for negative. IBM System/360 processors will use the 1010 (A) and 1011 (B) signs if the A bit is set in the PSW, for the ASCII-8 standard that never passed. Most implementations also provide unsigned BCD values with a sign nibble of 1111 (F).<ref name=\"IBM_POP\">{{citation |title=IBM System/370 Principles of Operation |chapter=Chapter 8: Decimal Instructions |publisher=[[IBM]] |date=March 1980 }}</ref><ref name=\"DEC_PDP11\">{{citation |title=PDP-11 Architecture Handbook |chapter=Chapter 3: Data Representation |publisher=[[Digital Equipment Corporation]] |year=1983}}</ref><ref name=\"DEC_VAX11\">{{citation |title=VAX-11 Architecture Handbook |publisher=[[Digital Equipment Corporation]] |year=1985}}</ref> ILE RPG uses 1111 (F) for positive and 1101 (D) for negative.<ref>{{cite web |url=http://publib.boulder.ibm.com/iseries/v5r2/ic2924/books/c0925083170.htm |title=ILE RPG Reference}}</ref> These match the EBCDIC zone for digits without a sign overpunch. In packed BCD, the number 127 is represented by 0001 0010 0111 1100 (127C) and −127 is represented by 0001 0010 0111 1101 (127D). Burroughs systems used 1101 (D) for negative, and any other value is considered a positive sign value (the processors will normalize a positive sign to 1100 (C)).\n{| border=\"1\" cellpadding=\"2\" cellspacing=\"0\"  style=\"margin:auto; width:40%;\"\n|-\n!  style=\"background:#e0e0e0; width:20%;\"|Sign<br>Digit\n!  style=\"background:#e0e0e0; width:20%;\"|BCD<br>8&nbsp;4&nbsp;2&nbsp;1\n!  style=\"background:#e0e0e0; width:20%;\"|Sign\n!  style=\"background:#e0e0e0; width:40%;\"|Notes\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|A\n| 1&nbsp;0&nbsp;1&nbsp;0\n| '''+'''\n| &nbsp;\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|B\n| 1&nbsp;0&nbsp;1&nbsp;1\n| '''−'''\n| &nbsp;\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|C\n| 1&nbsp;1&nbsp;0&nbsp;0\n| '''+'''\n| Preferred\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|D\n| 1&nbsp;1&nbsp;0&nbsp;1\n| '''−'''\n| Preferred\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|E\n| 1&nbsp;1&nbsp;1&nbsp;0\n| '''+'''\n| &nbsp;\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|F\n| 1&nbsp;1&nbsp;1&nbsp;1\n| '''+'''\n| Unsigned\n|}\n\nNo matter how many bytes wide a [[Word (data type)|word]] is, there are always an even number of nibbles because each byte has two of them. Therefore, a word of ''n'' bytes can contain up to (2''n'')−1 decimal digits, which is always an odd number of digits. A decimal number with ''d'' digits requires {{sfrac|1|2}}(''d''+1) bytes of storage space.\n\nFor example, a 4-byte (32-bit) word can hold seven decimal digits plus a sign, and can represent values ranging from ±9,999,999. Thus the number −1,234,567 is 7 digits wide and is encoded as:\n 0001 0010 0011 0100 0101 0110 0111 1101\n ''1    2    3    4    5    6    7    −''\n\nLike character strings, the first byte of the packed decimal{{snd}} that with the most significant two digits{{snd}} is usually stored in the lowest address in memory, independent of the [[endianness]] of the machine.\n\nIn contrast, a 4-byte binary [[two's complement]] integer can represent values from −2,147,483,648 to +2,147,483,647.\n\nWhile packed BCD does not make optimal use of storage (about one-sixth of the memory used is wasted), conversion to [[ASCII]], [[EBCDIC]], or the various encodings of [[Unicode]] is still trivial, as no arithmetic operations are required. The extra storage requirements are usually offset by the need for the accuracy and compatibility with calculator or hand calculation that fixed-point decimal arithmetic provides. Denser packings of [[BCD (character encoding)|BCD]] exist which avoid the storage penalty and also need no arithmetic operations for common conversions.\n\nPacked BCD is supported in the [[COBOL]] programming language as the \"COMPUTATIONAL-3\" (an IBM extension adopted by many other compiler vendors) or \"PACKED-DECIMAL\" (part of the 1985 COBOL standard) data type. It is supported in [[PL/I]] as \"FIXED DECIMAL\". Besides the IBM System/360 and later compatible mainframes, packed BCD is implemented in the native instruction set of the original [[VAX]] processors from [[Digital Equipment Corporation]] and some models of the [[SDS Sigma series]] mainframes, and is the native format for the [[Burroughs Corporation]] Medium Systems line of mainframes (descended from the 1950s [[Burroughs 205|Electrodata 200 series]]).\n\n[[Ten's complement]] representations for negative numbers offer an alternative approach to encoding the sign of packed (and other) BCD numbers. In this case, positive numbers always have a most significant digit between 0 and 4 (inclusive), while negative numbers are represented by the 10's complement of the corresponding positive number. As a result, this system allows for 32-bit packed BCD numbers to range from −50,000,000 to +49,999,999, and −1 is represented as 99999999. (As with [[two's complement]] binary numbers, the range is not symmetric about zero.)\n\n===Fixed-point packed decimal===\n[[Fixed-point arithmetic|Fixed-point]] decimal numbers are supported by some programming languages (such as [[COBOL]], [[PL/I]] and [[Ada (programming language)|Ada]]). These languages allow the programmer to specify an implicit decimal point in front of one of the digits. For example, a packed decimal value encoded with the bytes 12 34 56 7C represents the fixed-point value +1,234.567 when the implied decimal point is located between the 4th and 5th digits:\n 12 34 56 7C\n ''12 34.56 7+''\n\nThe decimal point is not actually stored in memory, as the packed BCD storage format does not provide for it. Its location is simply known to the compiler and the generated code acts accordingly for the various arithmetic operations.\n\n===Higher-density encodings===\nIf a decimal digit requires four bits, then three decimal digits require 12 bits. However, since 2<sup>10</sup> (1,024) is greater than 10<sup>3</sup> (1,000), if three decimal digits are encoded together, only 10 bits are needed. Two such encodings are ''[[Chen–Ho encoding]]'' and ''[[densely packed decimal]]'' (DPD). The latter has the advantage that subsets of the encoding encode two digits in the optimal seven bits and one digit in four bits, as in regular BCD.\n\n=={{anchor|Zoned BCD}}Zoned decimal==\nSome implementations, for example [[IBM]] mainframe systems, support '''zoned decimal''' numeric representations. Each decimal digit is stored in one byte, with the lower four bits encoding the digit in BCD form. The upper four bits, called the \"zone\" bits, are usually set to a fixed value so that the byte holds a character value corresponding to the digit. EBCDIC systems use a zone value of 1111 (hex F); this yields bytes in the range F0 to F9 (hex), which are the [[EBCDIC]] codes for the characters \"0\" through \"9\". Similarly, [[ASCII]] systems use a zone value of 0011 (hex 3), giving character codes 30 to 39 (hex).\n\nFor signed zoned decimal values, the rightmost (least significant) zone nibble holds the sign digit, which is the same set of values that are used for signed packed decimal numbers (see above). Thus a zoned decimal value encoded as the hex bytes F1 F2 D3 represents the signed decimal value −123:\n F1 F2 D3\n '' 1  2 −3''\n\n===EBCDIC zoned decimal conversion table===\n<!-- Note that this table should also include codes A0-A9, B0-B9, and E0-E9 for completeness. -->\n{|  style=\"margin:auto; width:70%;\" class=\"wikitable\"\n|-\n! style=\"background:#e0e0e0;\"|BCD Digit\n! style=\"background:#e0e0e0;\" colspan=\"4\"|Hexadecimal\n! style=\"background:#e0e0e0;\" colspan=\"4\"|EBCDIC Character\n|- style=\"text-align:center;\" \n|  style=\"width:20%; width:12%;\"|<tt>0+</tt>\n|  style=\"width:10%; width:11%;\"|<tt>C0</tt>\n|  style=\"width:10%; width:11%;\"|<tt>A0</tt>\n|  style=\"width:10%; width:11%;\"|<tt>E0</tt>\n|  style=\"width:10%; width:11%;\"|<tt>F0</tt>\n|  style=\"width:10%; width:11%;\"|<tt>{</tt> (*)\n|  style=\"width:10%; width:11%; background:#f0f0f0;\"|&nbsp;\n|  style=\"width:10%; width:11%;\"|<tt>\\</tt> (*)\n|  style=\"width:10%; width:11%;\"|<tt>0</tt>\n|- style=\"text-align:center;\" \n||<tt>1+</tt>\n||<tt>C1</tt>\n||<tt>A1</tt>\n||<tt>E1</tt>\n||<tt>F1</tt>\n||<tt>A</tt>\n||<tt>~</tt> (*)\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>1</tt>\n|- style=\"text-align:center;\" \n||<tt>2+</tt>\n||<tt>C2</tt>\n||<tt>A2</tt>\n||<tt>E2</tt>\n||<tt>F2</tt>\n||<tt>B</tt>\n||<tt>s</tt>\n||<tt>S</tt>\n||<tt>2</tt>\n|- style=\"text-align:center;\" \n||<tt>3+</tt>\n||<tt>C3</tt>\n||<tt>A3</tt>\n||<tt>E3</tt>\n||<tt>F3</tt>\n||<tt>C</tt>\n||<tt>t</tt>\n||<tt>T</tt>\n||<tt>3</tt>\n|- style=\"text-align:center;\" \n||<tt>4+</tt>\n||<tt>C4</tt>\n||<tt>A4</tt>\n||<tt>E4</tt>\n||<tt>F4</tt>\n||<tt>D</tt>\n||<tt>u</tt>\n||<tt>U</tt>\n||<tt>4</tt>\n|- style=\"text-align:center;\" \n||<tt>5+</tt>\n||<tt>C5</tt>\n||<tt>A5</tt>\n||<tt>E5</tt>\n||<tt>F5</tt>\n||<tt>E</tt>\n||<tt>v</tt>\n||<tt>V</tt>\n||<tt>5</tt>\n|- style=\"text-align:center;\" \n||<tt>6+</tt>\n||<tt>C6</tt>\n||<tt>A6</tt>\n||<tt>E6</tt>\n||<tt>F6</tt>\n||<tt>F</tt>\n||<tt>w</tt>\n||<tt>W</tt>\n||<tt>6</tt>\n|- style=\"text-align:center;\" \n||<tt>7+</tt>\n||<tt>C7</tt>\n||<tt>A7</tt>\n||<tt>E7</tt>\n||<tt>F7</tt>\n||<tt>G</tt>\n||<tt>x</tt>\n||<tt>X</tt>\n||<tt>7</tt>\n|- style=\"text-align:center;\" \n||<tt>8+</tt>\n||<tt>C8</tt>\n||<tt>A8</tt>\n||<tt>E8</tt>\n||<tt>F8</tt>\n||<tt>H</tt>\n||<tt>y</tt>\n||<tt>Y</tt>\n||<tt>8</tt>\n|- style=\"text-align:center;\" \n||<tt>9+</tt>\n||<tt>C9</tt>\n||<tt>A9</tt>\n||<tt>E9</tt>\n||<tt>F9</tt>\n||<tt>I</tt>\n||<tt>z</tt>\n||<tt>Z</tt>\n||<tt>9</tt>\n|- style=\"text-align:center;\" \n||<tt>0−</tt>\n||<tt>D0</tt>\n||<tt>B0</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>}</tt> &nbsp;(*)\n||<tt>^</tt> &nbsp;(*)\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>1−</tt>\n||<tt>D1</tt>\n||<tt>B1</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>J</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>2−</tt>\n||<tt>D2</tt>\n||<tt>B2</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>K</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>3−</tt>\n||<tt>D3</tt>\n||<tt>B3</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>L</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>4−</tt>\n||<tt>D4</tt>\n||<tt>B4</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>M</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>5−</tt>\n||<tt>D5</tt>\n||<tt>B5</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>N</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>6−</tt>\n||<tt>D6</tt>\n||<tt>B6</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>O</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>7−</tt>\n||<tt>D7</tt>\n||<tt>B7</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>P</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>8−</tt>\n||<tt>D8</tt>\n||<tt>B8</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>Q</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|- style=\"text-align:center;\" \n||<tt>9−</tt>\n||<tt>D9</tt>\n||<tt>B9</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n||<tt>R</tt>\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|  style=\"background:#f0f0f0;\"|&nbsp;\n|}\n\n(*) ''Note: These characters vary depending on the local character [[code page]] setting.''\n\n===Fixed-point zoned decimal===\nSome languages (such as [[COBOL]] and [[PL/I]]) directly support fixed-point zoned decimal values, assigning an implicit decimal point at some location between the decimal digits of a number. For example, given a six-byte signed zoned decimal value with an implied decimal point to the right of the fourth digit, the hex bytes F1 F2 F7 F9 F5 C0 represent the value +1,279.50:\n F1 F2 F7 F9 F5 C0\n '' 1  2  7  9. 5 +0''\n\n{{Anchor|IBMBCD}}<!--- keep anchor with following section, \"IBM and BCD\" --->\n\n==IBM and BCD==\n{{Main|BCDIC}}\n[[IBM]] used the terms ''[[Binary-Coded Decimal Interchange Code]]'' (BCDIC, sometimes just called BCD), for 6-bit ''[[alphanumeric]]'' codes that represented numbers, upper-case letters and special characters. Some variation of BCDIC ''alphamerics'' is used in most early IBM computers, including the [[IBM 1620]], [[IBM 1400 series]], and non-[[IBM 700/7000 series#Decimal architecture (7070/7072/7074)|Decimal Architecture]] members of the [[IBM 700/7000 series]].\n\nThe [[IBM 1400 series]] are character-addressable machines, each location being six bits labeled ''B, A, 8, 4, 2'' and ''1,'' plus an odd parity check bit (''C'') and a word mark bit (''M''). For encoding digits ''1'' through ''9'', ''B'' and ''A'' are zero and the digit value represented by standard 4-bit BCD in bits ''8'' through ''1''. For most other characters bits ''B'' and ''A'' are derived simply from the \"12\", \"11\", and \"0\" \"zone punches\" in the punched card character code, and bits ''8'' through ''1'' from the ''1'' through ''9'' punches. A \"12 zone\" punch set both ''B'' and ''A'', an \"11 zone\" set ''B'', and a \"0 zone\" (a 0 punch combined with any others) set ''A''. Thus the letter '''A''', which is ''(12,1)'' in the punched card format, is encoded ''(B,A,1)''. The currency symbol '''$''', ''(11,8,3)'' in the punched card, was encoded in memory as ''(B,8,2,1)''. This allows the circuitry to convert between the punched card format and the internal storage format to be very simple with only a few special cases. One important special case is digit ''0'', represented by a lone ''0'' punch in the card, and ''(8,2)'' in core memory.<ref>[http://ed-thelen.org/1401Project/Van1401-CodeChart.pdf IBM BM 1401/1440/1460/1410/7010 Character Code Chart in BCD Order]{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\nThe memory of the [[IBM 1620]] is organized into 6-bit addressable digits, the usual ''8, 4, 2, 1'' plus ''F'', used as a flag bit and ''C'', an odd parity check bit. BCD ''alphamerics'' are encoded using digit pairs, with the \"zone\" in the even-addressed digit and the \"digit\" in the odd-addressed digit, the \"zone\" being related to the ''12'', ''11'', and ''0'' \"zone punches\" as in the 1400 series. Input/Output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.\n\nIn the Decimal Architecture [[IBM 7070]], [[IBM 7072]], and [[IBM 7074]] ''alphamerics'' are encoded using digit pairs (using [[two-out-of-five code]] in the digits, '''not''' BCD) of the 10-digit word, with the \"zone\" in the left digit and the \"digit\" in the right digit. Input/Output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.\n\nWith the introduction of [[System/360]], IBM expanded 6-bit BCD ''alphamerics'' to 8-bit [[EBCDIC]], allowing the addition of many more characters (e.g., lowercase letters). A variable length Packed BCD ''numeric'' data type is also implemented, providing machine instructions that perform arithmetic directly on packed decimal data.\n\nOn the [[IBM 1130]] and [[IBM 1800|1800]], packed BCD is supported in software by IBM's Commercial Subroutine Package.\n\nToday, BCD data is still heavily used in IBM processors and databases, such as [[IBM DB2]], mainframes, and [[Power6]]. In these products, the BCD is usually zoned BCD (as in EBCDIC or ASCII), Packed BCD (two decimal digits per [[byte]]), or \"pure\" BCD encoding (one decimal digit stored as BCD in the low four bits of each byte). All of these are used within hardware registers and processing units, and in software. To convert packed decimals in EBCDIC table unloads to readable numbers, you can use the OUTREC FIELDS mask of the JCL utility DFSORT.<ref>http://publib.boulder.ibm.com/infocenter/zos/v1r12/index.jsp?topic=%2Fcom.ibm.zos.r12.iceg200%2Fenf.htm{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\n==Other computers and BCD==\nThe [[Digital Equipment Corporation]] [[VAX|VAX-11]] series includes [[Instruction set|instructions]] that can perform arithmetic directly on packed BCD data and convert between packed BCD data and other integer representations.<ref name=\"DEC_VAX11\"/> The VAX's packed BCD format is compatible with that on IBM System/360 and IBM's later compatible processors. The MicroVAX and later VAX implementations dropped this ability from the CPU but retained code compatibility with earlier machines by implementing the missing instructions in an operating system-supplied software library. This is invoked automatically via exception handling when the no longer implemented instructions are encountered, so that programs using them can execute without modification on the newer machines.\n\nThe [[Intel]] [[x86]] architecture supports a [[Intel BCD opcode|unique 18-digit (ten-byte) BCD format]] that can be loaded into and stored from the floating point registers, and computations can be performed there.<ref>{{cite book|title=Intel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 1: Basic Architecture|pages=4–9|date=December 2016|publisher=Intel Corporation|url=http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html}}</ref>\n\nThe [[Motorola 68000 series]] had BCD instructions.<ref name=\"m68k\">url=http://www.tigernt.com/onlineDoc/68000.pdf</ref>\n\nIn more recent computers such capabilities are almost always implemented in software rather than the CPU's instruction set, but BCD numeric data is still extremely common in commercial and financial applications. There are tricks for implementing packed BCD and zoned decimal add or subtract operations using short but difficult to understand sequences of word-parallel logic and binary arithmetic operations.<ref name=\"Jones_AT\">{{cite web |title=BCD Arithmetic, a tutorial |work=Arithmetic Tutorials |author-first=Douglas W. |author-last=Jones |author-link=Douglas W. Jones |publisher=The University of Iowa, Department of Computer Science |orig-year=1999 |date=2015-11-25 |location=Iowa City, Iowa, USA |url=http://homepage.cs.uiowa.edu/~jones/bcd/bcd.html |access-date=2016-01-03}}</ref> For example, the following code (written in [[C (programming language)|C]]) computes an unsigned 8-digit packed BCD add using 32-bit binary operations:\n<source lang=c>\nuint32_t BCDadd(uint32_t a,uint32_t b)\n{\n    uint32_t  t1, t2;    // unsigned 32-bit intermediate values\n\n    t1 = a + 0x06666666;\n    t2 = t1 ^ b;                   // sum without carry propagation\n    t1 = t1 + b;                   // provisional sum\n    t2 = t1 ^ t2;                  // all the binary carry bits\n    t2 = ~t2 & 0x11111110;         // just the BCD carry bits\n    t2 = (t2 >> 2) | (t2 >> 3);    // correction\n    return t1 - t2;                // corrected BCD sum\n}\n</source>\n\n==Addition with BCD==\nIt is possible to perform [[addition]] in BCD by first adding in binary, and then converting to BCD afterwards. Conversion of the simple sum of two digits can be done by adding 6 (that is, 16 – 10) when the five-bit result of adding a pair of digits has a value greater than 9. For example:\n 1001 + 1000 = 10001\n    9 +    8 =    17\n\n10001 is the binary, not decimal, representation of the desired result, but the most-significant 1 (the \"carry\") cannot fit in a 4-bit binary number. In BCD as in decimal, there cannot exist a value greater than 9 (1001) per digit. To correct this, 6 (0110) is added to the total and then the result is treated as two [[nibble]]s:\n\n 10001 + 0110 = 00010111 => 0001 0111\n    17 +    6 =       23       1    7\n\nThe two nibbles of the result, 0001 and 0111, correspond to the digits \"1\" and \"7\". This yields \"17\" in BCD, which is the correct result.\n\nThis technique can be extended to adding multiple digits by adding in groups from right to left, propagating the second digit as a carry, always comparing the 5-bit result of each digit-pair sum to 9. Some CPUs provide a [[half-carry flag]] to facilitate BCD arithmetic adjustments following binary addition and subtraction operations.\n\n==Subtraction with BCD==\nSubtraction is done by adding the [[ten's complement]] of the [[subtrahend]]. To represent the sign of a number in BCD, the number 0000 is used to represent a [[positive number]], and 1001 is used to represent a [[negative number]]. The remaining 14 combinations are invalid signs. To illustrate signed BCD subtraction, consider the following problem: 357 − 432.\n\nIn signed BCD, 357 is 0000 0011 0101 0111. The [[ten's complement]] of 432 can be obtained by taking the [[nine's complement]] of 432, and then adding one. So, 999 − 432 = 567, and 567 + 1 = 568. By preceding 568 in BCD by the negative sign code, the number −432 can be represented. So, −432 in signed BCD is 1001 0101 0110 1000.\n\nNow that both numbers are represented in signed BCD, they can be added together:\n   0000 0011 0101 0111\n      0    3    5    7\n + 1001 0101 0110 1000\n <u>     9    5    6    8</u>\n = 1001 1000 1011 1111\n      9    8   11   15\n\nSince BCD is a form of decimal representation, several of the digit sums above are invalid. In the event that an invalid entry (any BCD digit greater than 1001) exists, 6 is added to generate a carry bit and cause the sum to become a valid entry. The reason for adding 6 is that there are 16 possible 4-bit BCD values (since 2<sup>4</sup> = 16), but only 10 values are valid (0000 through 1001). So adding 6 to the invalid entries results in the following:\n   1001 1000 1011 1111\n      9    8   11   15\n + 0000 0000 0110 0110\n <u>     0    0    6    6</u>\n = 1001 1001 0010 0101\n      9    9    2    5\n\nThus the result of the subtraction is 1001 1001 0010 0101 (−925). To confirm the result, note that the first digit is 9, which means negative. This seems to be correct, since 357 − 432 should result in a negative number. The remaining nibbles are BCD, so  1001 0010 0101 is 925. The [[ten's complement]] of 925 is 1000 − 925 = 75, so the calculated answer is −75.\n\nIf there are a different number of nibbles being added together (such as 1053 − 2), the number with the fewer digits must first be prefixed with zeros before taking the [[ten's complement]] or subtracting. So, with 1053 − 2, 2 would have to first be represented as 0002 in BCD, and the [[ten's complement]] of 0002 would have to be calculated.\n\n==Background==\nThe binary-coded decimal scheme described in this article is the most common encoding, but there are many others. The method here can be referred to as ''Simple Binary-Coded Decimal'' (''SBCD'') or ''BCD&nbsp;8421''.\n<!-- It is also sometimes named ''NBCD'' (''Naturally Binary-Coded Decimal''). (where?) -->\nThe following table represents [[decimal]] digits from 0 to 9 in various BCD systems. \nIn the headers to the table, the '{{code|8 4 2 1}}', indicates the weight of each bit shown; in the fifth column, \"BCD&nbsp;8&nbsp;4&nbsp;−2&nbsp;−1\", two of the weights are negative. Both ASCII and EBCDIC character codes for the digits are examples of zoned BCD, and are also shown in the table.\n\n:\n{| border=\"1\" cellpadding=\"2\" cellspacing=\"0\" style=\"margin:auto;\"\n|-\n! style=\"background:#e0e0e0;\"| &nbsp;<br />Digit\n! style=\"background:#e0e0e0;\"| BCD<br />8&nbsp;4&nbsp;2&nbsp;1\n! style=\"background:#e0e0e0;\"| [[George Stibitz|Stibitz]]&nbsp;Code or [[Excess-3]]\n! style=\"background:#e0e0e0;\"| [[Aiken-Code]] or BCD<br />2&nbsp;4&nbsp;2&nbsp;1\n! style=\"background:#e0e0e0;\"| BCD<br />8&nbsp;4&nbsp;−2&nbsp;−1\n! style=\"background:#e0e0e0;\"| {{nowrap| [[IBM 702]], }} {{nowrap| [[IBM 705]], }} {{nowrap| [[IBM 7080]], }} {{nowrap| [[IBM 1401]] }} 8&nbsp;4&nbsp;2&nbsp;1\n! style=\"background:#e0e0e0;\"| [[ASCII]] 0000&nbsp;8421\n! style=\"background:#e0e0e0;\"| [[EBCDIC]] 0000&nbsp;8421\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|0\n| 0000\n| 0011\n| 0000\n| 0000\n| 1010\n| 0011&nbsp;0000\n| 1111&nbsp;0000\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|1\n| 0001\n| 0100\n| 0001\n| 0111\n| 0001\n| 0011&nbsp;0001\n| 1111&nbsp;0001\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|2\n| 0010\n| 0101\n| 0010\n| 0110\n| 0010\n| 0011&nbsp;0010\n| 1111&nbsp;0010\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|3\n| 0011\n| 0110\n| 0011\n| 0101\n| 0011\n| 0011&nbsp;0011\n| 1111&nbsp;0011\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|4\n| 0100\n| 0111\n| 0100\n| 0100\n| 0100\n| 0011&nbsp;0100\n| 1111&nbsp;0100\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|5\n| 0101\n| 1000\n| 1011\n| 1011\n| 0101\n| 0011&nbsp;0101\n| 1111&nbsp;0101\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|6\n| 0110\n| 1001\n| 1100\n| 1010\n| 0110\n| 0011 0110\n| 1111 0110\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|7\n| 0111\n| 1010\n| 1101\n| 1001\n| 0111\n| 0011&nbsp;0111\n| 1111&nbsp;0111\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|8\n| 1000\n| 1011\n| 1110\n| 1000\n| 1000\n| 0011&nbsp;1000\n| 1111&nbsp;1000\n|- style=\"text-align:center;\"\n! style=\"background:#f0f0f0;\"|9\n| 1001\n| 1100\n| 1111\n| 1111\n| 1001\n| 0011&nbsp;1001\n| 1111&nbsp;1001\n|}\n\n==Legal history==\nIn the 1972 case ''[[Gottschalk v. Benson]]'', the U.S. Supreme Court overturned a lower court decision which had allowed a patent for converting BCD encoded numbers to binary on a computer. This was an important case in determining the patentability of software and algorithms.\n\n==Comparison with pure binary==\n<!-- not at all clear why this needs to be here at all, shouldn't the article just describe BCD? (mfc)-->\n<!--as an alternative number system for computing and electronics imo a comparison to the standard one is justified (plugwash)-->\n\n===Advantages===\n* Many non-integral values, such as decimal 0.2, have an infinite place-value representation in binary (.001100110011...) but have a finite place-value in binary-coded decimal (0.0010). Consequently, a system based on binary-coded decimal representations of decimal fractions avoids errors representing and calculating such values. This is useful in financial calculations.\n* Scaling by a power of 10 is simple.\n* [[Rounding]] at a decimal digit boundary is simpler. Addition and subtraction in decimal does not require rounding.\n* Alignment of two decimal numbers (for example 1.3 + 27.08) is a simple, exact, shift.\n* Conversion to a character form or for display (e.g., to a text-based format such as [[XML]], or to drive signals for a [[seven-segment display]]) is a simple per-digit mapping, and can be done in linear ([[Big-O notation|O]](''n'')) time. Conversion from pure [[binary numeral system|binary]] involves relatively complex logic that spans digits, and for large numbers no linear-time conversion algorithm is known (see {{see section|Binary numeral system|Conversion to and from other numeral systems}}).\n\n===Disadvantages===\n* Some operations are more complex to implement. [[Adder (electronics)|Adder]]s require extra logic to cause them to wrap and generate a carry early. 15 to 20 percent more circuitry is needed for BCD add compared to pure binary.{{Citation needed|date=May 2011}} Multiplication requires the use of algorithms that are somewhat more complex than shift-mask-add (a [[Binary numeral system#Multiplication|binary multiplication]], requiring binary shifts and adds or the equivalent, per-digit or group of digits is required)\n* Standard BCD requires four bits per digit, roughly 20 percent more space than a binary encoding (the ratio of 4 bits to log<sub>2</sub>10 bits is 1.204). When packed so that three digits are encoded in ten bits, the storage overhead is greatly reduced, at the expense of an encoding that is unaligned with the 8-bit byte boundaries common on existing hardware, resulting in slower implementations on these systems.<!-- Could add: encoding or decoding is trivial in software using a table lookup, and fast using direct logic otherwise. In hardware it requires no more than three gate delays. -->\n* Practical existing implementations of BCD are typically slower than operations on binary representations, especially on embedded systems,{{Citation needed|date=May 2011}} due to limited processor support for native BCD operations.\n\n==Application==\nThe [[BIOS]] in many [[personal computer]]s stores the date and time in BCD because the [[MC6818]] real-time clock chip used in the original [[IBM PC AT]] motherboard provided the time encoded in BCD. This form is easily converted into ASCII for display.<ref>http://www.se.ecu.edu.au/units/ens1242/lectures/ens_Notes_08.pdf{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref><!-- also see MC6818 datasheet -->\n\nThe [[Atari 8-bit family]] of computers used BCD to implement floating-point algorithms. The [[MOS Technology 6502|MOS 6502]] processor has a BCD mode that affects the addition and subtraction instructions. The [[Psion Organiser|Psion Organiser 1]] handheld computer’s manufacturer-supplied software also used entirely BCD to implement floating point; later Psion models used binary exclusively.\n\nEarly models of the [[PlayStation 3]] store the date and time in BCD. This led to a worldwide outage of the console on 1 March 2010. The last two digits of the year stored as BCD [[time formatting and storage bugs|were misinterpreted]] as 16 causing an error in the unit's date, rendering most functions inoperable. This has been referred to as the [[Year 2010 problem|Year 2010 Problem]].\n\n==Representational variations==\nVarious BCD implementations exist that employ other representations for numbers. [[Programmable calculator]]s manufactured by [[Texas Instruments]], [[Hewlett-Packard]], and others typically employ a [[floating-point]] BCD format, typically with two or three digits for the (decimal) exponent. The extra bits of the sign digit may be used to indicate special numeric values, such as [[infinity]], [[arithmetic underflow|underflow]]/[[arithmetic overflow|overflow]], and [[Defined and undefined|error]] (a blinking display).\n\n===Signed variations===\nSigned decimal values may be represented in several ways. The [[COBOL]] programming language, for example, supports a total of five zoned decimal formats, each one encoding the numeric sign in a different way:\n{| class=\"wikitable\" style=\"width:95%\"\n|-\n! style=\"background:#D0E0FF; width:25%\"| Type\n! style=\"background:#D0E0FF; width:55%\"| Description\n! style=\"background:#D0E0FF; width:20%\"| Example\n|-\n| Unsigned\n| No sign nibble\n| <code>F1 F2 <u>F</u>3</code>\n|-\n| Signed trailing ''(canonical format)''\n| Sign nibble in the last (least significant) byte\n| <code>F1 F2 <u>C</u>3</code>\n|-\n| Signed leading ''(overpunch)''\n| Sign nibble in the first (most significant) byte\n| <code><u>C</u>1 F2 F3</code>\n|-\n| Signed trailing separate\n| Separate sign character byte (<code>'+'</code> or <code>'−'</code>) following the digit bytes\n| <code>F1 F2 F3 <u>2B</u></code>\n|-\n| Signed leading separate\n| Separate sign character byte (<code>'+'</code> or <code>'−'</code>) preceding the digit bytes\n| <code><u>2B</u> F1 F2 F3</code>\n|}\n\n==={{anchor|TBCD}}Telephony Binary Coded Decimal (TBCD)===\n[[3GPP]] developed '''TBCD''',<ref>{{cite techreport |title=3GPP TS 29.002: Mobile Application Part (MAP) specification |at=sec. 17.7.8 Common data types |year=2013 |url=http://www.3gpp.org/ftp/Specs/html-info/29002.htm}}</ref> an expansion to BCD where the remaining (unused) bit combinations are used to add specific [[telephony]] characters,<ref>{{cite web |url=http://www.etsi.org/deliver/etsi_etr/001_099/060/02_60/etr_060e02p.pdf |title=Signalling Protocols and Switching (SPS) Guidelines for using Abstract Syntax Notation One (ASN.1) in telecommunication application protocols |page=15}}</ref><ref>{{cite web |url=http://www.openss7.org/specs/xmap.pdf |title=XOM Mobile Application Part (XMAP) Specification |page=93}}</ref> with digits similar to those found in [[Dual-tone multi-frequency signaling|telephone keypads]] original design.\n{| class=\"wikitable\" style=\"width:30%; text-align:center\"\n|-\n! style=\"background:#E0E0E0; width:50%\" |Decimal<br/>Digit\n! style=\"background:#E0E0E0; width:50%\" |TBCD<br/>8 4 2 1\n|-\n! style=\"background:#F0F0F0\" |*\n| 1 0 1 0\n|-\n! style=\"background:#F0F0F0\" |#\n| 1 0 1 1\n|-\n! style=\"background:#F0F0F0\" |a\n| 1 1 0 0\n|-\n! style=\"background:#F0F0F0\" |b\n| 1 1 0 1\n|-\n! style=\"background:#F0F0F0\" |c\n| 1 1 1 0\n|-\n! style=\"background:#F0F0F0\" |Used as filler when there is an odd number of digits\n| 1 1 1 1\n|}\n\nThe mentioned 3GPP document defines '''TBCD-STRING''' with swapped nibbles in each byte. Bits, octets and digits indexed from 1, bits from the right, digits and octets from the left.\n<blockquote>\nbits 8765 of octet n encoding digit 2n\n\nbits 4321 of octet n encoding digit 2(n-1) +1\n</blockquote>\n\nMeaning number <code>1234</code>, would become <code>21 43</code> in TBCD.\n\n==Alternative encodings==\nIf errors in representation and computation are more important than the speed of conversion to and from display, a scaled binary representation may be used, which stores a decimal number as a binary-encoded integer and a binary-encoded signed decimal exponent. For example, 0.2 can be represented as 2{{e|-1}}.\n\nThis representation allows rapid multiplication and division, but may require shifting by a power of 10 during addition and subtraction to align the decimal points. It is appropriate for applications with a fixed number of decimal places that do not then require this adjustment—particularly financial applications where 2 or 4 digits after the decimal point are usually enough. Indeed, this is almost a form of [[fixed point arithmetic]] since the position of the [[radix point]] is implied.\n\n[[Chen–Ho encoding]] provides a boolean transformation for converting groups of three BCD-encoded digits to and from 10-bit values that can be efficiently encoded in hardware with only 2 or 3 gate delays. [[Densely packed decimal]] (DPD) is a similar scheme that is used for most of the [[significand]], except the lead digit, for one of the two alternative decimal encodings specified in the [[IEEE 754-2008]] standard.\n\n==See also==\n* [[Bi-quinary coded decimal]]\n* [[Binary integer decimal]] (BID)\n* [[Chen–Ho encoding]]\n* [[Densely packed decimal]] (DPD)\n* [[Double dabble]], an algorithm for converting binary numbers to BCD\n* [[Gray code]]\n* [[Year 2000 problem]]\n* [[Decimal computer]]\n* [[Binary-coded ternary]] (BCT)\n\n==Notes==\n{{reflist|group=\"nb\"|refs=\n<ref group=\"nb\" name=\"Pseudo-tetrades\">That is, in a standard packed 4-bit representation, there are 16 states (four bits for each digit) with 10 tetrades and 6 pseudo-tetrades, whereas in more densely packed schemes such as [[Chen–Ho encoding|Chen–Ho]] or [[densely packed decimal|DPD]] coding there are less, f.e. only 24 pseudo-tetrades in 1024 states (10 bits for three digits).</ref>\n}}\n\n==References==\n{{Reflist|refs=\n<ref name=\"Intel_IA32\">{{cite web |author=Intel |title=ia32 architecture manual |url=http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developers-manual.pdf |publisher=[[Intel]] |access-date=2015-07-01}}</ref>\n<ref name=\"Schneider_1986\">{{cite book |title=Lexikon der Informatik und Datenverarbeitung |language=German |author-first=Hans-Jochen |author-last=Schneider |date=1986 |edition=2 |publisher=R. Oldenbourg Verlag München Wien |isbn=3-486-22662-2}}</ref>\n<ref name=\"Tafel_1971\">{{cite book |title=Einführung in die digitale Datenverarbeitung |language=German |trans-title=Introduction to digital information processing |author-first=Hans Jörg |author-last=Tafel |publisher=[[Carl Hanser Verlag]] |date=1971 |location=[[RWTH]], Aachen, Germany |publication-place=Munich, Germany |isbn=3-446-10569-7}}</ref>\n<ref name=\"Steinbuch-Weber_1974\">{{cite book |title=Taschenbuch der Informatik - Band II - Struktur und Programmierung von EDV-Systemen |language=German |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Wolfgang |editor-last2=Weber |editor-first3=Traute |editor-last3=Heinemann |date=1974 |orig-year=1967 |edition=3 |volume=2 |work=Taschenbuch der Nachrichtenverarbeitung |publisher=[[Springer-Verlag]] |location=Berlin, Germany |isbn=3-540-06241-6 |lccn=73-80607}}</ref>\n<ref name=\"Tietze-Schenk_2013\">{{cite book |title=Advanced Electronic Circuits |author-first1=Ulrich |author-last1=Tietze |author-first2=Christoph |author-last2=Schenk |date=2012-12-06 |publisher=[[Springer Science & Business Media]] |isbn=3642812414 |id=9783642812415 |url=https://books.google.com/books?id=dYruCAAAQBAJ |access-date=2015-08-05}}</ref>\n<ref name=\"Kowalski_1070\">{{cite book |title=Nuclear Electronics |author-first=Emil |author-last=Kowalski |date=2013-03-08 |orig-year=1970 |publisher=[[Springer-Verlag]] |isbn=3642876633 |id=9783642876639, 978-3-642-87664-6 |doi=10.1007/978-3-642-87663-9 |url=https://books.google.com/books?id=Xr-IBwAAQBAJ |access-date=2015-08-05}}</ref>\n<ref name=\"Ferretti_2013\">{{cite book |title=Wörterbuch der Elektronik, Datentechnik und Telekommunikation / Dictionary of Electronics, Computing and Telecommunications: Teil 1: Deutsch-Englisch / Part 1: German-English |author-first=Vittorio |author-last=Ferretti |edition=2 |volume=1 |publisher=Springer-Verlag |date=2013-03-13 \n|isbn=3642980880 |id=9783642980886 |url=https://books.google.com/books?id=gtHzBQAAQBAJ |access-date=2015-08-05}}</ref>\n<ref name=\"Speiser_1965\">{{cite book |author-first=Ambrosius Paul |author-last=Speiser |author-link=Ambrosius Paul Speiser |location=[[Eidgenössische Technische Hochschule Zürich|ETH Zürich]], Zürich, Switzerland |title=Digitale Rechenanlagen - Grundlagen / Schaltungstechnik / Arbeitsweise / Betriebssicherheit |language=German |trans-title=Digital computers - Basics / Circuits / Operation / Reliability |edition=2 |date=1965 |orig-year=1961 |publisher=[[Springer-Verlag]] / [[IBM]] |lccn=65-14624 |id=0978 |page=209}}</ref>\n<ref name=\"Cowlishaw_GDA\">{{cite web |author-first=Mike F. |author-last=Cowlishaw |author-link=Mike F. Cowlishaw |title=General Decimal Arithmetic |orig-year=1981,2008 |date=2015 |url=http://speleotrove.com/decimal/<!-- http://www2.hursley.ibm.com/decimal/ --> |access-date=2016-01-02}}</ref>\n}}\n\n==Further reading==\n*{{cite book |title=Coded Character Sets, History and Development |work=The Systems Programming Series |author-last=Mackenzie |author-first=Charles E. |year=1980 |edition=1 |publisher=[[Addison-Wesley Publishing Company, Inc.]] |isbn=0-201-14460-3 |lccn=77-90165 |page=xii |id={{ISBN|978-0-201-14460-4}} |url=https://books.google.com/books?id=6-tQAAAAMAAJ |access-date=2016-05-22}} [https://web.archive.org/web/20160526172151/https://textfiles.meulie.net/bitsaved/Books/Mackenzie_CodedCharSets.pdf]\n*<!-- <ref name=\"Richards_1955\"> -->{{cite book |author-first=Richard Kohler |author-last=Richards |title=Arithmetic Operations in Digital Computers |publisher=[[van Nostrand (publisher)|van Nostrand]] |location=New York, USA |date=1955 |pages=397-}}<!-- </ref> -->\n*{{cite book |title=Decimal Computation |first=Hermann |last=Schmid<!-- General Electric Company, Binghamton, New York, USA --> |author-link=Hermann Schmid (computer scientist) |date=1974 |edition=1 |publisher=[[John Wiley & Sons]] |location=Binghamton, New York, USA |isbn=0-471-76180-X}} and {{cite book |title=Decimal Computation |first=Hermann |last=Schmid<!-- General Electric Company, Binghamton, New York, USA --> |author-link=Hermann Schmid (computer scientist) |orig-year=1974 |date=1983 |edition=1 (reprint) |publisher=Robert E. Krieger Publishing Company |location=Malabar, Florida, USA |isbn=0-89874-318-4}} (NB. At least some batches of the Krieger reprint edition were [[misprint]]s with defective pages 115–146.)\n*<!-- <ref name=\"Massalin_1987_Superoptimizer\"> -->{{cite journal |author-first=Henry |author-last=Massalin |author-link=Henry Massalin |editor-first=Randy |editor-last=Katz |editor-link=Randy Katz |title=Superoptimizer: A Look at the Smallest Program |journal=Proceedings of the Second International Conference on Architectural support for Programming Languages and Operating Systems [[Association for Computing Machinery|ACM]] [[SIGOPS]] Operating Systems Review |pages=122–126 |lay-url=http://hpux.connect.org.uk/hppd/hpux/Gnu/superopt-2.5/readme.html |lay-date=1995-06-14 |doi=10.1145/36206.36194 |date=October 1987 |volume=21 |issue=4 |isbn=0-8186-0805-6 |url=http://www.stanford.edu/class/cs343/resources/superoptimizer.pdf |access-date=2012-04-25 |dead-url=no |archive-url=https://web.archive.org/web/20170704123738/https://web.stanford.edu/class/cs343/resources/superoptimizer.pdf |archive-date=2017-07-04}} (Also: ACM SIGPLAN Notices, Vol. 22 #10, IEEE Computer Society Press #87CH2440-6, October 1987)<!-- </ref> -->\n* ''VLSI designs for redundant binary-coded decimal addition'', Behrooz Shirazi, David Y. Y. Yun, and Chang N. Zhang, IEEE Seventh Annual International Phoenix Conference on Computers and Communications, 1988, pp52–56, IEEE, March 1988\n* ''Fundamentals of Digital Logic'' by Brown and Vranesic, 2003\n* ''Modified Carry Look Ahead BCD Adder With CMOS and Reversible Logic Implementation'', Himanshu Thapliyal and Hamid R. Arabnia, Proceedings of the 2006 International Conference on Computer Design (CDES'06), {{ISBN|1-60132-009-4}}, pp64–69, CSREA Press, November 2006\n* ''Reversible Implementation of {{Sic|hide=y|Densely|-}}Packed-Decimal Converter to and from Binary-Coded-Decimal Format Using in IEEE-754R'', A. Kaivani, A. Zaker Alhosseini, S. Gorgin, and M. Fazlali, 9th International Conference on Information Technology (ICIT'06), pp273–276, IEEE, December 2006.\n* {{cite web |author-first=Mike F. |author-last=Cowlishaw |author-link=Mike F. Cowlishaw |title=Bibliography of material on Decimal Arithmetic – by category |work=General Decimal Arithmetic |orig-year=2002,2008 |publisher=IBM |date=2009 |url=http://speleotrove.com/decimal/decbibindex.html<!-- http://www2.hursley.ibm.com/decimal/decbibindex.html --> |access-date=2016-01-02}}\n\n==External links==\n* {{cite web |author-first=Mike F. |author-last=Cowlishaw |author-link=Mike F. Cowlishaw |title=A Summary of Chen-Ho Decimal Data encoding |work=General Decimal Arithmetic |orig-year=2000 |publisher=[[IBM]] |date=2014 |url=http://speleotrove.com/decimal/chen-ho.html<!-- http://www2.hursley.ibm.com/decimal/chen-ho.html --> |access-date=2016-01-02}}\n* {{cite web |author-first=Mike F. |author-last=Cowlishaw |author-link=Mike F. Cowlishaw |title=A Summary of Densely Packed Decimal encoding |work=General Decimal Arithmetic |orig-year=2000 |publisher=[[IBM]] |date=2007 |url=http://speleotrove.com/decimal/DPDecimal.html<!-- http://www2.hursley.ibm.com/decimal/DPDecimal.html --> |access-date=2016-01-02}}\n* [http://www.unitjuggler.com/convert-numbersystems-from-decimal-to-bcd.html Convert BCD to decimal, binary and hexadecimal and vice versa]\n* [https://github.com/c-rack/bcd4j BCD for Java]\n\n{{DEFAULTSORT:Binary-Coded Decimal}}\n[[Category:Computer arithmetic]]\n[[Category:Numeral systems]]\n[[Category:Non-standard positional numeral systems]]\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Bit",
      "url": "https://en.wikipedia.org/wiki/Bit",
      "text": "{{About|the unit of information}}\n{{Use dmy dates|date=July 2012}}\n{{Fundamental info units}}\n\nThe '''bit''' is a [[Units of information|basic unit]] of [[information]] in [[information theory]], [[computing]], and digital [[communication]]s. The name is a [[portmanteau]] of '''binary digit'''.<ref name=\"Mackenzie_1980\">{{cite book |title=Coded Character Sets, History and Development |work=The Systems Programming Series |author-last=Mackenzie |author-first=Charles E. |year=1980 |edition=1 |publisher=[[Addison-Wesley Publishing Company, Inc.]] |isbn=978-0-201-14460-4 |lccn=77-90165 |page=x |url=https://books.google.com/books?id=6-tQAAAAMAAJ |accessdate=2016-05-22 |deadurl=no |archiveurl=https://web.archive.org/web/20161118230039/https://books.google.com/books?id=6-tQAAAAMAAJ |archivedate=18 November 2016 |df=dmy-all }} [https://web.archive.org/web/20160526172151/https://textfiles.meulie.net/bitsaved/Books/Mackenzie_CodedCharSets.pdf]</ref>\n\nIn [[information theory]], one bit is typically defined as the [[information entropy]] of a binary random variable that is 0 or 1 with equal probability,<ref>John B. Anderson, Rolf Johnnesson (2006) ''Understanding Information Transmission''.</ref> or the information that is gained when the value of such a variable becomes known.<ref>Simon Haykin (2006), ''Digital Communications''</ref><ref>[[IEEE Std 260.1-2004]]</ref> As a [[unit of information]], the bit has also been called a ''[[shannon (unit)|shannon]]'',<ref>{{cite web|url=https://www.unc.edu/~rowlett/units/dictB.html#bit|title=Units: B|deadurl=no|archiveurl=https://web.archive.org/web/20160504055432/http://www.unc.edu/~rowlett/units/dictB.html#bit|archivedate=4 May 2016|df=dmy-all}}</ref> named after [[Claude Shannon]].\n\nAs a [[Binary number|binary]] digit, the bit represents a [[truth value|logical value]], having only one of two [[value (computer science)|values]]. It may be physically implemented with a two-state device. These state values are most commonly represented as either {{gaps|0|or|1}}, but other representations such as ''true/false'', ''yes/no'', ''+/−'', or ''on/off'' are possible. The correspondence between these values and the physical states of the underlying [[Data storage device|storage]] or [[computing device|device]] is a matter of convention, and different assignments may be used even within the same device or [[computer program|program]].\n\nThe symbol for the binary digit is either simply ''bit'' per recommendation by the [[IEC 80000-13]]:2008 standard, or the lowercase character ''b'', as recommended by the [[IEEE 1541-2002]] and [[IEEE Std 260.1-2004]] standards. A group of eight binary digits is commonly called one&nbsp;[[byte]], but historically the size of the byte is not strictly defined.\n\n==History==\nThe encoding of data by discrete bits was used in the [[punched card]]s invented by [[Basile Bouchon]] and Jean-Baptiste Falcon (1732), developed by [[Joseph Marie Jacquard]] (1804), and later adopted by [[Semyon Korsakov]], [[Charles Babbage]], [[Hermann Hollerith]], and early computer manufacturers like [[IBM]]. Another variant of that idea was the perforated [[paper tape]]. In all those systems, the medium (card or tape) conceptually carried an array of hole positions; each position could be either punched through or not, thus carrying one&nbsp;bit of information. The encoding of text by bits was also used in [[Morse code]] (1844) and early digital communications machines such as [[teletype]]s and [[stock ticker machine]]s (1870).\n\n[[Ralph Hartley]] suggested the use of a logarithmic measure of information in 1928.<ref name=\"abramson\">Norman Abramson (1963), ''Information theory and coding''. McGraw-Hill.</ref> [[Claude E. Shannon]] first used the word '''''bit''''' in his seminal 1948 paper ''[[A Mathematical Theory of Communication]]''.<ref>{{cite journal|last=Shannon |first=Claude |title=A Mathematical Theory of Communication |journal=Bell Labs Technical Journal |url=http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf |archiveurl=https://web.archive.org/web/19980715013250/http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf |dead-url=yes |archivedate=1998-07-15 }}</ref>\nHe attributed its origin to [[John W. Tukey]], who had written a Bell Labs memo on 9 January 1947 in which he contracted \"binary information digit\" to simply \"bit\". [[Vannevar Bush]] had written in 1936 of \"bits of information\" that could be stored on the [[punched card]]s used in the mechanical computers of that time.<ref>{{cite journal|last=Bush|first=Vannevar|title=Instrumental analysis|journal=Bulletin of the American Mathematical Society|year=1936|volume=42|issue=10|pages=649–669|url=http://projecteuclid.org/euclid.bams/1183499313|doi=10.1090/S0002-9904-1936-06390-1|deadurl=no|archiveurl=https://web.archive.org/web/20141006153002/http://projecteuclid.org/euclid.bams/1183499313|archivedate=6 October 2014|df=dmy-all}}</ref> The first programmable computer, built by [[Konrad Zuse]], used binary notation for numbers.\n\n==Physical representation{{anchor|representation}}==<!-- Warning: this heading is the target of a link in [[Flip-flop (electronics)]] -->\nA bit can be stored by a digital device or other physical system that exists in either of two possible distinct [[state (computer science)|states]]. These may be the two stable states of a [[Flip-flop (electronics)|flip-flop]], two positions of an [[Switch|electrical switch]], two distinct [[voltage]] or [[electric current|current]] levels allowed by a [[electrical circuit|circuit]], two distinct levels of [[Irradiance|light intensity]], two directions of [[magnetism|magnetization]] or [[electrical polarity|polarization]], the orientation of reversible double stranded [[DNA]], etc.\n\nBits can be implemented in several forms. In most modern computing devices, a bit is usually represented by an [[Electricity|electrical]] [[voltage]] or [[Electric current|current]] pulse, or by the electrical state of a [[flip-flop (electronics)|flip-flop circuit]].\n\nFor devices using [[positive logic]], a digit value of 1 (or a logical value of true) is represented by a more positive voltage relative to the representation of 0. The specific voltages are different for different logic families and variations are permitted to allow for component aging and noise immunity. For example, in [[transistor–transistor logic]] (TTL) and compatible circuits, digit values 0 and 1 at the output of a device are represented by no higher than 0.4 volts and no lower than 2.6 volts, respectively; while TTL inputs are specified to recognize 0.8 volts or below as 0 and 2.2 volts or above as 1.\n\n===Transmission and processing===\nBits are transmitted one at a time in [[serial transmission]], and by a multiple number of bits in [[parallel transmission]]. A [[bitwise operation]] optionally processes bits one at a time. Data transfer rates are usually measured in decimal SI multiples of the unit [[bit per second]] (bit/s), such as kbit/s.\n\n===Storage===\nIn the earliest non-electronic information processing devices, such as Jacquard's loom or Babbage's [[Analytical Engine]], a bit was often stored as the position of a mechanical lever or gear, or the presence or absence of a hole at a specific point of a [[Punched card|paper card]] or [[Punched tape|tape]]. The first electrical devices for discrete logic (such as [[elevator]] and [[traffic light]] control circuits, [[telephone switches]], and Konrad Zuse's computer) represented bits as the states of [[electrical relay]]s which could be either \"open\" or \"closed\". When relays were replaced by [[vacuum tube]]s, starting in the 1940s, computer builders experimented with a variety of storage methods, such as pressure pulses traveling down a [[mercury delay line]], charges stored on the inside surface of a [[cathode-ray tube]], or opaque spots printed on [[optical disc|glass discs]] by [[photolithographic]] techniques.\n\nIn the 1950s and 1960s, these methods were largely supplanted by [[magnetic storage]] devices such as [[magnetic core memory]], [[magnetic tape]]s, [[magnetic drum|drums]], and [[Disk storage|disks]], where a bit was represented by the polarity of [[magnetism|magnetization]] of a certain area of a [[ferromagnetic]] film, or by a change in polarity from one direction to the other. The same principle was later used in the [[magnetic bubble memory]] developed in the 1980s, and is still found in various [[magnetic strip]] items such as [[Rapid transit|metro]] tickets and some [[credit card]]s.\n\nIn modern [[semiconductor memory]], such as [[dynamic random-access memory]], the two values of a bit may be represented by two levels of [[electric charge]] stored in a [[capacitor]]. In certain types of [[programmable logic array]]s and [[read-only memory]], a bit may be represented by the presence or absence of a conducting path at a certain point of a circuit. In [[optical disc]]s, a bit is encoded as the presence or absence of a microscopic pit on a reflective surface. In one-dimensional [[bar code]]s, bits are encoded as the thickness of alternating black and white lines.\n\n==Unit and symbol==\nThe bit is not defined in the [[International System of Units]] (SI). However, the [[International Electrotechnical Commission]] issued standard [[IEC 60027]], which specifies that the symbol for binary digit should be ''bit'', and this should be used in all multiples, such as ''kbit'', for kilobit.<ref name=\"nist\">National Institute of Standards and Technology (2008), ''Guide for the Use of the International System of Units''. [http://physics.nist.gov/cuu/pdf/sp811.pdf Online version.] {{webarchive|url=https://web.archive.org/web/20160603203340/http://physics.nist.gov/cuu/pdf/sp811.pdf |date=3 June 2016 }}</ref> However, the lower-case letter b is widely used as well and was recommended by the [[IEEE 1541-2002|IEEE 1541 Standard (2002)]]. In contrast, the upper case letter B is the standard and customary symbol for byte.{{Quantities of bits}}\n\n===Multiple bits===\nMultiple bits may be expressed and represented in several ways. For convenience of representing commonly reoccurring groups of bits in information technology, several [[units of information]] have traditionally been used. The most common is the unit [[byte]], coined by [[Werner Buchholz]] in June 1956, which historically was used to represent the group of bits used to encode a single [[character (computing)|character]] of text (until [[UTF-8]] multibyte encoding took over) in a computer<ref name=\"Bemer_2000\"/><ref name=\"Buchholz_1956\"/><ref name=\"Buchholz_1977\"/><ref name=\"Buchholz_1962\"/><ref name=\"Bemer_1959\"/> and for this reason it was used as the basic [[address space|addressable]] element in many [[computer architecture]]s. The trend in hardware design converged on the most common implementation of using eight&nbsp;bits per byte, as it is widely used today. However, because of the ambiguity of relying on the underlying hardware design, the unit [[Octet (computing)|octet]] was defined to explicitly denote a sequence of eight&nbsp;bits.\n\nComputers usually manipulate bits in groups of a fixed size, conventionally named \"[[Word (computer architecture)|words]]\". Like the byte, the number of bits in a word also varies with the hardware design, and is typically between 8 and 80&nbsp;bits, or even more in some specialized computers. In the 21st century, retail personal or server computers have a word size of 32 or 64&nbsp;bits.\n\nThe [[International System of Units]] defines a series of decimal prefixes for multiples of standardized units which are commonly also used with the bit and the byte. The prefixes [[kilo-|kilo]] (10<sup>3</sup>) through [[yotta-|yotta]] (10<sup>24</sup>) increment by multiples of 1000, and the corresponding units are the [[kilobit]] (kbit) through the [[yottabit]] (Ybit).\n\n==Information capacity and information compression==\n{{Update|type=section|date=October 2018|reason=it cites a fact about global information content in computers from 2007}}\nWhen the information capacity of a storage system or a communication channel is presented in ''bits'' or ''bits per second'', this often refers to binary digits, which is a [[computer hardware]] capacity to store binary data (0 or 1, up or down, current or not, etc.)<ref name=\"Information in small bits\" />. Information capacity of a storage system is only an upper bound to the quantity of information stored therein. If the two possible values of one&nbsp;bit of storage are not equally likely, that bit of storage contains less than one&nbsp;bit of information. Indeed, if the value is completely predictable, then the reading of that value provides no information at all (zero entropic bits, because no resolution of uncertainty occurs and therefore no information is available). If a computer file that uses ''n''&nbsp;bits of storage contains only ''m'' < ''n''&nbsp;bits of information, then that information can in principle be encoded in about ''m''&nbsp;bits, at least on the average. This principle is the basis of [[lossless data compression|data compression]] technology. Using an analogy, the hardware binary digits refer to the amount of storage space available (like the number of buckets available to store things), and the information content the filling, which comes in different levels of granularity (fine or coarse, that is, compressed or uncompressed information). When the granularity is finer—when information is more compressed—the same bucket can hold more.\n\nFor example, it is estimated that the combined technological capacity of the world to store information provides 1,300 [[exabytes]] of hardware digits in 2007. However, when this storage space is filled and the corresponding content is optimally compressed, this only represents 295 [[exabytes]] of information.<ref name=\"HilbertLopez2011\">[http://www.sciencemag.org/content/332/6025/60 \"The World's Technological Capacity to Store, Communicate, and Compute Information\"] {{webarchive|url=https://web.archive.org/web/20130727161911/http://www.sciencemag.org/content/332/6025/60 |date=27 July 2013 }}, especially [http://www.sciencemag.org/content/suppl/2011/02/08/science.1200970.DC1/Hilbert-SOM.pdf Supporting online material] {{webarchive|url=https://web.archive.org/web/20110531133712/http://www.sciencemag.org/content/suppl/2011/02/08/science.1200970.DC1/Hilbert-SOM.pdf |date=31 May 2011 }}, Martin Hilbert and Priscila López (2011), [[Science (journal)|Science]], 332(6025), 60-65; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html</ref> When optimally compressed, the resulting carrying capacity approaches [[Shannon information]] or [[information entropy]].<ref name=\"Information in small bits\" />\n\n==Bit-based computing==\nCertain [[bitwise operation|bitwise]] computer [[central processing unit|processor]] instructions (such as ''bit set'') operate at the level of manipulating bits rather than manipulating data interpreted as an aggregate of bits.\n\nIn the 1980s, when [[bitmap]]ped computer displays became popular, some computers provided specialized [[bitblt|bit block transfer]] (\"bitblt\" or \"blit\") instructions to set or copy the bits that corresponded to a given rectangular area on the screen.\n\nIn most computers and programming languages, when a bit within a group of bits, such as a byte or word, is referred to, it is usually specified by a number from 0 upwards corresponding to its position within the byte or word. However, 0 can refer to either the [[most significant bit|most]] or [[least significant bit]] depending on the context.\n\n==Other information units==\n{{Main article|Units of information}}\nSimilar to [[angular momentum]] and [[energy]] in physics; [[Information#Information theory approach|information-theoretic information]] and data storage size have the same [[Dimensional analysis|dimensionality]] of [[Unit of measurement|units of measurement]], but there is in general no meaning to adding, subtracting or otherwise combining the units mathematically.\n\nOther units of information, sometimes used in information theory, include the ''[[natural digit]]'' also called a ''[[nat (unit)|nat]]'' or ''[[nit (unit of information)|nit]]'' and defined as [[logarithm|log]]<sub>2</sub>&nbsp;''e'' (≈ 1.443)&nbsp;bits, where ''e'' is the [[e (mathematical constant)|base of the natural logarithms]]; and the ''[[dit (unit)|dit]]'', ''[[ban (unit)|ban]]'', or ''[[hartley (unit)|hartley]]'', defined as log<sub>2</sub>&nbsp;10 (≈ 3.322)&nbsp;bits.<ref name=\"abramson\" /> This value, slightly less than 10/3, may be understood because 10<sup>3</sup> = 1000 ≈ 1024 = 2<sup>10</sup>: three decimal digits are slightly less information than ten binary digits, so one decimal digit is slightly less than 10/3 binary digits. Conversely, one&nbsp;bit of information corresponds to about [[natural logarithm|ln]]&nbsp;2 (≈ 0.693) nats, or log<sub>10</sub>&nbsp;2 (≈ 0.301) hartleys. As with the inverse ratio, this value, approximately 3/10, but slightly more, corresponds to the fact that 2<sup>10</sup> = 1024 ~ 1000 = 10<sup>3</sup>: ten binary digits are slightly more information than three decimal digits, so one binary digit is slightly more than 3/10 decimal digits. Some authors also define a '''binit''' as an arbitrary information unit equivalent to some fixed but unspecified number of bits.<ref>{{cite book |author-first=Amitabha |author-last=Bhattacharya |title=Digital Communication |publisher=[[Tata McGraw-Hill Education]] |date=2005 |isbn=978-0070591172 |url=https://books.google.com/books?id=0CI8bd0upS4C&pg=PR20&lpg=PR20 |deadurl=no |archiveurl=https://web.archive.org/web/20170327011019/https://books.google.com/books?id=0CI8bd0upS4C&pg=PR20&lpg=PR20 |archivedate=27 March 2017 |df=dmy-all }}</ref>\n\n==See also==\n* [[Integer (computer science)]]\n* [[Primitive data type]]\n* [[Trit (computing)|Trit]] (Trinary digit)\n* [[Qubit]] (quantum bit)\n* [[Bitstream]]\n* [[Entropy (information theory)]]\n* [[Baud|Baud rate]] (bits per second)\n* [[Binary numeral system]]\n* [[Ternary numeral system]]\n* [[Shannon (unit)]]\n\n==References==\n{{Reflist|refs=\n<ref name=\"Information in small bits\">[https://informationinsmallbits.com/ Information in small bits] Information in Small Bits is a book produced as part of a non-profit outreach project of the IEEE Information Theory Society.\nThe book introduces Claude Shannon and basic concepts of Information Theory to children 8+ using relatable cartoon stories and problem-solving activities.</ref>\n<ref name=\"Bemer_2000\">{{cite web |title=Why is a byte 8 bits? Or is it? |author-first=Robert William |author-last=Bemer |author-link=Robert William Bemer |date=2000-08-08 |work=Computer History Vignettes |url=http://www.bobbemer.com/BYTE.HTM |access-date=2017-04-03 |dead-url=yes |archive-url=https://web.archive.org/web/20170403130829/http://www.bobbemer.com/BYTE.HTM |archive-date=3 April 2017 |quote=[…] With [[IBM]]'s [[IBM STRETCH|STRETCH]] computer as background, handling 64-character words divisible into groups of 8 (I designed the character set for it, under the guidance of Dr. [[Werner Buchholz]], the man who DID coin the term \"[[byte]]\" for an 8-bit grouping). […] The [[IBM System 360|IBM 360]] used 8-bit characters, although not ASCII directly. Thus Buchholz's \"byte\" caught on everywhere. I myself did not like the name for many reasons. […] }}</ref>\n<ref name=\"Buchholz_1956\">{{cite book |title=The Link System |chapter=7. The Shift Matrix |author-first=Werner |author-last=Buchholz |author-link=Werner Buchholz |date=1956-06-11 |id=[[IBM Stretch|Stretch]] Memo No. 39G |publisher=[[IBM]] |pages=5–6 |chapter-url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-07/102632284.pdf |access-date=2016-04-04 |dead-url=no |archive-url=https://web.archive.org/web/20170404152534/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-07/102632284.pdf |archive-date=2017-04-04 |quote=[…] Most important, from the point of view of editing, will be the ability to handle any characters or digits, from 1 to 6 bits long […] the Shift Matrix to be used to convert a 60-bit [[word (computer architecture)|word]], coming from Memory in parallel, into [[character (computing)|characters]], or \"[[byte]]s\" as we have called them, to be sent to the [[serial adder|Adder]] serially. The 60 bits are dumped into [[magnetic core]]s on six different levels. Thus, if a 1 comes out of position 9, it appears in all six cores underneath. […] The Adder may accept all or only some of the bits. […] Assume that it is desired to operate on 4 bit [[decimal digit]]s, starting at the right. The 0-diagonal is pulsed first, sending out the six bits 0 to 5, of which the Adder accepts only the first four (0-3). Bits 4 and 5 are ignored. Next, the 4 diagonal is pulsed. This sends out bits 4 to 9, of which the last two are again ignored, and so on. […] It is just as easy to use all six bits in [[alphanumeric]] work, or to handle bytes of only one bit for logical analysis, or to offset the bytes by any number of bits. […]}}</ref>\n<ref name=\"Buchholz_1977\">{{cite journal |author-last=Buchholz |author-first=Werner |author-link=Werner Buchholz |title=The Word \"Byte\" Comes of Age... |journal=[[Byte Magazine]] |date=February 1977 |volume=2 |issue=2 |page=144 |url=https://archive.org/stream/byte-magazine-1977-02/1977_02_BYTE_02-02_Usable_Systems#page/n145/mode/2up |quote=[…] The first reference found in the files was contained in an internal memo written in June 1956 during the early days of developing [[IBM Stretch|Stretch]]. A [[byte]] was described as consisting of any number of parallel bits from one to six. Thus a byte was assumed to have a length appropriate for the occasion. Its first use was in the context of the input-output equipment of the 1950s, which handled six bits at a time. The possibility of going to 8 bit bytes was considered in August 1956 and incorporated in the design of Stretch shortly thereafter. The first published reference to the term occurred in 1959 in a paper \"Processing Data in Bits and Pieces\" by [[Gerrit Anne Blaauw|G&nbsp;A&nbsp;Blaauw]], [[Frederick Phillips Brooks, Jr.|F&nbsp;P&nbsp;Brooks&nbsp;Jr]] and [[Werner Buchholz|W&nbsp;Buchholz]] in the ''[[IRE Transactions on Electronic Computers]]'', June 1959, page 121. The notions of that paper were elaborated in Chapter 4 of ''[[#Buchholz-1962|Planning a Computer System (Project Stretch)]]'', edited by W&nbsp;Buchholz, [[McGraw-Hill Book Company]] (1962). The rationale for coining the term was explained there on page 40 as follows:<br />Byte ''denotes a group of bits used to encode a character, or the number of bits transmitted in parallel to and from input-output units. A term other than ''character'' is used here because a given character may be represented in different applications by more than one code, and different codes may use different numbers of bits (ie, different byte sizes). In input-output transmission the grouping of bits may be completely arbitrary and have no relation to actual characters. (The term is coined from ''[[bite]]'', but respelled to avoid accidental mutation to ''bit''.)''<br />[[System/360]] took over many of the Stretch concepts, including the basic byte and word sizes, which are powers of 2. For economy, however, the byte size was fixed at the 8 bit maximum, and addressing at the bit level was replaced by byte addressing. […]}}</ref>\n<ref name=\"Buchholz_1962\">{{anchor|Buchholz-1962}}{{cite |title=Planning a Computer System – Project Stretch |author-first1=Gerrit Anne |author-last1=Blaauw |author-link1=Gerrit Anne Blaauw |author-first2=Frederick Phillips |author-last2=Brooks, Jr. |author-link2=Frederick Phillips Brooks, Jr. |author-first3=Werner |author-last3=Buchholz |author-link3=Werner Buchholz |editor-first=Werner |editor-last=Buchholz |editor-link=Werner Buchholz |publisher=[[McGraw-Hill Book Company, Inc.]] / The Maple Press Company, York, PA. |lccn=61-10466 |year=1962 |chapter=4: Natural Data Units |format=PDF |pages=39–40 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/Buchholz_102636426.pdf |access-date=2017-04-03 |dead-url=yes |archive-url=https://web.archive.org/web/20170403014651/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/Buchholz_102636426.pdf |archive-date=3 April 2017 }}</ref>\n<ref name=\"Bemer_1959\">{{cite |author-first=Robert William |author-last=Bemer |author-link=Robert William Bemer |title=A proposal for a generalized card code of 256 characters |journal=[[Communications of the ACM]] |volume=2 |number=9 |pages=19–23 |year=1959 |doi=10.1145/368424.368435}}</ref>\n}}\n\n==External links==\n{{wiktionary}}\n* [http://www.bit-calculator.com/ Bit Calculator] – a tool providing conversions between bit, byte, kilobit, kilobyte, megabit, megabyte, gigabit, gigabyte\n* [http://nxu.biz/tools/BitXByteConverter/ BitXByteConverter] – a tool for computing file sizes, storage capacity, and digital information in various units\n\n{{Information units}}\n{{Data types}}\n\n[[Category:Binary arithmetic]]\n[[Category:Primitive types]]\n[[Category:Data types]]\n[[Category:Units of information]]"
    },
    {
      "title": "Bit manipulation",
      "url": "https://en.wikipedia.org/wiki/Bit_manipulation",
      "text": "{{Short description|algorithmically modifying data below the word level}}\n{{Use American English|date = March 2019}}\n{{More citations needed|date=April 2017}}\n\n'''Bit manipulation''' is the act of [[algorithm]]ically manipulating [[bit]]s or other pieces of [[Data (computing)|data]] shorter than a [[Word (data type)|word]]. [[Computer programming]] tasks that require bit manipulation include low-level device control, [[error detection]] and [[error correction|correction]] algorithms, [[data compression]], [[encryption]] algorithms, and [[Optimization (computer science)|optimization]]. For most other tasks, modern [[programming language]]s allow the [[programmer]] to work directly with [[Abstraction (computer science)|abstractions]] instead of bits that represent those abstractions. [[Source code]] that does bit manipulation makes use of the [[bitwise operation]]s: AND, OR, XOR, NOT, and [[bitwise operation#Bit shifts|bit shifts]].\n\nBit manipulation, in some cases, can obviate or reduce the need to loop over a data structure and can give many-fold speed ups, as bit manipulations are processed in parallel, but the code can become more difficult to write and maintain.\n\n== Terminology ==\n\n''Bit twiddling'' and ''bit bashing'' are often used interchangeably with bit manipulation, but sometimes exclusively refer to clever or non-obvious ways or uses of bit manipulation, or tedious or challenging [[low-level device control]] data manipulation tasks.\n\nThe term ''bit twiddling'' dates from [[History of computing hardware|early computing hardware]], where computer operators would make adjustments by tweaking or ''twiddling'' computer controls. As computer programming languages evolved, programmers adopted the term to mean any handling of data that involved bit-level [[computation]].\n\n== Bitwise operation ==\n{{main|bitwise operation}}\nA bitwise operation operates on one or more [[bit pattern]]s or [[Binary numeral system|binary numerals]] at the level of their individual [[bit]]s. It is a fast, primitive action directly supported by the [[central processing unit]] (CPU), and is used to manipulate values for comparisons and calculations.\nOn simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition. While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer [[instruction pipeline]]s and other [[Computer architecture|architectural]] design choices, bitwise operations do commonly use less power because of the reduced use of resources.\n\n== Masks ==\n{{main|mask (computing)}}\nA mask is data that is used for [[bitwise operation]]s, particularly in a [[bit field]].\n\nUsing a mask, multiple bits in a [[Byte]], [[nibble]], [[Word (computer architecture)|word]] (etc.) can be set either on, off or inverted from on to off (or vice versa) in a single bitwise operation.\n\n== Example of bit manipulation ==\n\nThe following two code samples, written in the programming language [[C++]], both determine if the given unsigned integer '''x''' is a [[power of two]].\n<source lang=\"cpp\">\n // The obvious method\n unsigned int x = ...;\n bool isPowerOfTwo;\n if (x > 0) {\n     /* Divide by two as long as the next division is an integer,\n     and if it isn't, check if the number is 1 (meaning the number is\n     some power of two) */\n     while ((x % 2) == 0) {\n         x = x / 2;\n     }\n     isPowerOfTwo = (x==1);\n }\n else { // zero is never a power of two\n     isPowerOfTwo = false;\n }\n</source>\n<source lang=\"cpp\">\n // A method using bit manipulation\n bool isPowerOfTwo = x && !(x & (x - 1));\n</source>\n\nThe second method uses the fact that powers of two have one and only one bit set in their binary representation:\n x         == 0...0<span style=\"color:red\">1</span>0...0\n x-1       == 0...001...1\n x & (x-1) == 0...000...0\n\nIf the number is neither zero nor a power of two, it will have '1' in more than one place:\n x         == 0...<span style=\"color:red\">1</span>...0<span style=\"color:red\">1</span>0...0\n x-1       == 0...<span style=\"color:red\">1</span>...001...1\n x & (x-1) == 0...<span style=\"color:red\">1</span>...000...0\n\nIf inline [[assembly language]] code is used, then an instruction that counts the number of 1's or 0's might be available; for example, the POPCNT instruction from the x86 [[instruction set]]. Some compilers provide predefined function for that, e.g. [[GNU Compiler Collection]] has __builtin_popcount(). Such instructions may have greater latency, however, than the bit-twiddling solution.\n\n== Bit manipulation in the C programming language ==\n\nThe programming language [[C (programming language)|C]] has direct support for bitwise operations that can be used for bit manipulation. In the following examples, <code>n</code> is the index of the bit to be manipulated within the variable <code>bit_fld</code>, which is an <code>unsigned char</code> being used as a [[bit field]]. Bit indexing begins at 0, not 1. Bit 0 is the least significant bit.\n\n;Set a bit:\n<source lang=\"c\">\nbit_fld |= (1 << n)\n</source>\n\n;Clear a bit:\n<source lang=\"c\">\nbit_fld &= ~(1 << n)\n</source>\n\n;Toggle a bit:\n<source lang=\"c\">\nbit_fld ^= (1 << n)\n</source>\n\n;Test a bit:\n<source lang=\"c\">\nbit_fld & (1 << n)\n</source>\n\nWhen using an array of bytes to represent a set of bits, i.e., a [[bit array]] or [[bitset]], the index of the byte in the array associated with a bit <code>n</code> can be calculated using division:\n\n<source lang=\"c\">\nn / CHAR_BIT\n</source>\n\nwhere <code>n</code> is the index of the given bit and <code>CHAR_BIT</code> gives the number of bits in a C <code>char</code>.\n\nThe index of the bit within the byte indexed by the above can be calculated via a [[modulo operation]]:\n<source lang=\"c\">\nn % CHAR_BIT\n</source>\n\nNote: <code>unsigned char</code> is typically used in C to represent a byte and <code>CHAR_BIT</code> is most often 8 on modern processors. <code>%</code> is the C modulo operator.\n\n== See also ==\n* [[Bit twiddler (disambiguation)]]\n* [[Bit specification (disambiguation)]]\n* [[Find first set]]\n* [[Flag (computing)]] — a bit representing a boolean value\n* [[Nibble]] — unit of data consisting of 4 bits, or half a byte\n* [[Mask (computing)]]\n* [[bit-banging]]\n* [[Bit array]]\n* [[BIT predicate]]\n* [[Bit Manipulation Instruction Sets]] bit manipulation extensions for the [[x86]] instruction set.\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |last=Warren |first=Henry S. |date=2012 |title=Hacker's Delight |edition=2nd |publisher=Addison–Wesley Professional |page=512 |isbn=978-0321842688}}\n* {{cite book |last=Knuth |first=Donald E. |date=2009 |title=[[The Art of Computer Programming]] Volume 4, Fascicle 1: Bitwise tricks & techniques; Binary Decision Diagrams |edition=1st |publisher=Addison–Wesley Professional |page=272 |isbn=978-0321580504 }} ([http://www-cs-faculty.stanford.edu/~knuth/fasc1a.ps.gz Draft of Fascicle 1a] available for download)\n\n== External links ==\n* [https://graphics.stanford.edu/~seander/bithacks.html Bit Twiddling Hacks], the \"gold\" standard for bit hacks\n* [http://bits.stephan-brumme.com Bit Manipulation Tricks] with full explanations and source code\n* [https://software.intel.com/sites/landingpage/IntrinsicsGuide/ Intel Intrinsics Guide]\n* [http://xchg.xorpd.net xchg rax,rax]: x86_64 riddles and hacks\n* [http://aggregate.org/MAGIC/ The Aggregate Magic Algorithms] from University of Kentucky\n\n{{DEFAULTSORT:Bit Manipulation}}\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]"
    },
    {
      "title": "Bit numbering",
      "url": "https://en.wikipedia.org/wiki/Bit_numbering",
      "text": "{{Cleanup|reason=The article is full of inaccuracies and mixes endianness with bit numbering. The article also implies that bit numbering is a property of the CPU; it is not. Mixes least significant byte with least significant bit, which have the same TLA.|date=May 2019}}\n\nIn [[computing]], '''bit numbering''' (or sometimes '''bit endianness''') is the convention used to identify the [[bit]] positions in a [[binary numeral system|binary number]] or a container of such a value. The bit number starts with zero and is incremented by one for each subsequent bit position.\n\n==Least significant bit==\n[[Image:Least significant bit.svg|thumb|280px|right|The [[binary numeral system|binary representation]] of decimal 149, with the LSB highlighted.  The MSB in an 8-bit binary number represents a value of 128 decimal.  The LSB represents a value of 1.]]\n\nIn [[computing]], the '''least significant bit''' ('''LSB''') is the [[bit]] position in a [[Binary numeral system|binary]] [[integer]] giving the units value, that is, determining whether the number is even or odd.  The LSB is sometimes referred to as the ''low-order bit'' or ''right-most bit'', due to the convention in [[positional notation]] of writing less significant digits further to the right. It is analogous to the least significant [[Numerical digit|digit]] of a [[decimal]] integer, which is the digit in the ''ones'' (right-most) position.<ref>{{cite web\n| url=https://support.microsoft.com/en-us/kb/130861\n| title=IBM SNA Formats Bit Ordering is Opposite of Intel Convention\n| date=2014-02-23\n| publisher=[[Microsoft]]}}</ref>\n\nIt is common to assign each bit a position number, ranging from zero to N-1, where N is the number of bits in the binary representation used. Normally, this is simply the exponent for the corresponding bit weight in base-2 (such as in <code>2<sup>31</sup>..2<sup>0</sup></code>). Although a few CPU manufacturers assign bit numbers the opposite way (which is not the same as different [[endianness]]), the term least significant bit itself remains unambiguous as an alias for the unit bit.\n\nBy extension, the least significant bits (plural) are the bits of the number closest to, and including, the LSB.\n\nThe least significant bits have the useful property of changing rapidly if the number changes even slightly.  For example, if 1 (binary 00000001) is added to 3 (binary 00000011), the result will be 4 (binary 00000100) and three of the least significant bits will change (011 to 100).  By contrast, the three [[most significant bit]]s (MSBs) stay unchanged (000 to 000).\n\nLeast significant bits are frequently employed in [[pseudorandom number generator]]s, [[steganographic]] tools, [[hash function]]s and [[checksum]]s.\n\n=== Least significant bit in digital steganography ===\n[[File:LeastSignificantBitDemonstration.jpg|right|300px|A diagram showing how manipulating the least significant bits of a color can have a very subtle and generally unnoticeable affect on the color. In this diagram, green is represented by its [[RGB]] value, both in decimal and in binary. The red box surrounding the last two bits illustrates the least significant bits changed in the binary representation. ]]\n\nIn digital [[steganography]], sensitive messages may be concealed by manipulating and storing information in the least significant bits of an image or a sound file. In the context of an image, if a user were to manipulate the last two bits of a color in a pixel, the value of the color would change at most +/- 3 value places, which is likely to be indistinguishable by the human eye. The user may later recover this information by extracting the least significant bits of the manipulated pixels to recover the original message.\n\nThis allows for the storage or transfer of digital information to be kept concealed.\n\n=== Least significant byte ===\n''LSB'' can also stand for '''least significant ''byte'''''.<ref>{{cite web\n| url=http://www.buczynski.com/Proteus/msblsb.html\n| title=MSB/LSB Tutorial\n| first=Don\n| last=Buczynski\n| date=2002-09-05}}</ref>  The meaning is parallel to the above: it is the [[byte]] (or [[Octet (computing)|octet]]) in that position of a multi-byte number which has the least potential value. If the abbreviation's meaning ''least significant byte'' isn't obvious from context, it should be stated explicitly to avoid confusion with ''least significant bit''.\n\nTo avoid this ambiguity, the less abbreviated terms \"lsbit\" or \"lsbyte\" may be used.\n\n==Most significant bit==\nIn [[computing]], the '''most significant bit''' ('''MSB''', also called the '''high-order bit''') is the [[bit]] position in a [[Binary numeral system|binary number]] having the greatest value. The MSB is sometimes referred to as the '''high-order bit''' or '''left-most bit''' due to the convention in [[positional notation]] of writing more significant digits further to the left.\n\nThe MSB can also correspond to the [[sign bit]] of a [[Signed number representations|signed binary number]] in [[One's complement|one's]] or [[two's complement]] notation, \"1\" signifies a negative and \"0\" signifies a positive binary numbers.\n\nIt is common to assign each bit a position number ranging from zero to N-1 where N is the number of bits in the binary representation used. Normally, this is simply the exponent for the corresponding bit weight in base-2 (such as in <code>2<sup>31</sup>..2<sup>0</sup></code>). Although a few CPU manufacturers assign bit numbers the opposite way (which is not the same as different [[endianness]]), the ''MSB'' unambiguously remains the ''most'' significant bit. This may be one of the reasons why the term ''MSB'' is often used instead of a bit number, although the primary reason is probably that different number representations use different numbers of bits.\n\nBy extension, the '''most significant bits''' (plural) are the bits closest to, and including, the MSB.\n[[Image:Most significant bit.svg|thumb|280px|right|The unsigned [[binary numeral system|binary]] representation of decimal 149, with the MSB highlighted.  The MSB in an 8-bit binary number represents a value of 128 decimal.  The LSB represents a value of 1.]]\n\n=== Most significant byte ===\n'''MSB''' can also stand for \"'''most significant ''byte'''''\".<ref>{{cite web\n| url=http://whatis.techtarget.com/definition/most-significant-bit-or-byte\n| title=most significant bit or byte\n| first=Margaret\n| last=Rouse\n| publisher=[[TechTarget]]\n| date=September 2005}}</ref> The meaning is parallel to the above: it is the byte (or [[Octet (computing)|octet]]) in that position of a multi-byte number which has the greatest potential value.\n\nTo avoid this ambiguity, the less abbreviated terms \"'''MSbit'''\" or \"'''MSbyte'''\" are often used.<ref>{{Cite book|url=https://books.google.com/books?id=Gb6w54X7Kw0C&pg=PA370&lpg=PA370&dq=%22msbit%22++%22msbyte%22&source=bl&ots=G95JfHZlCy&sig=0Qigjbw0BK_-NSA6qKVN6GAJrLQ&hl=en&sa=X&ved=0ahUKEwj0qIyvlrXZAhXNwVkKHRoHDKIQ6AEISDAE#v=onepage&q=%22msbit%22%20%20%22msbyte%22&f=false|title=RFID For Dummies|last=II|first=Patrick J. Sweeney|date=2010-03-11|publisher=John Wiley & Sons|isbn=9781118054475|language=en}}</ref><ref>{{Cite web|url=http://web.mit.edu/6.115/www/amulet/Bitmap.htm|title=Amulet Bitmap Format|website=web.mit.edu|access-date=2018-02-25}}</ref><ref>{{Cite web|url=https://www.cs.umd.edu/~meesh/cmsc411/website/projects/outer/memory/memory.htm|title=Memory MAYHEM! Memory, Byte Ordering and Alignment|website=www.cs.umd.edu|access-date=2018-02-25}}</ref>\n\n== Unsigned integer example ==\nThis table illustrates an example of decimal value of 149 and the location of LSB. In this particular example, the position of unit value (decimal 1 or 0) is located in bit position 0 ( n=0). MSB stands for Most Significant Bit, while LSB stands for Least Significant Bit. \n{| class=\"wikitable\"\n|Binary (Decimal: 149)\n|1\n|0\n|0\n|1\n|0\n|1\n|0\n|'''1'''\n|-\n|Bit weight for given bit position n ( 2<sup>n</sup> )\n|2<sup>7</sup>      \n|2<sup>6</sup>      \n|2<sup>5</sup>      \n|2<sup>4</sup>      \n|2<sup>3</sup>      \n|2<sup>2</sup>      \n|2<sup>1</sup>      \n|'''2<sup>0</sup>'''      \n|-\n|Bit position label\n|MSB\n|___\n|___\n|___\n|___\n|___\n|___\n|'''LSB'''\n|}\nPosition of LSB is independent of how the bit position is transmitted (Some system transmit MSB first, others transmit LSB first), which is a question more of a topic of [[Endianness]].\n\n== Most Significant Bit First vs Least Significant Bit First ==\nThe expressions ''Most Significant Bit First'' and ''Least Significant Bit First'' are indications on the ordering of the sequence of the bits in the bytes sent over a wire in a transmission protocol or in a stream (e.g. an audio stream).\n\n''Most Significant Bit First'' means that the most significant bit will arrive first: hence e.g. the hexadecimal number <code>0x12</code>, <code>00010010</code> in binary representation, will arrive as the sequence <code>0 0 0 1 0 0 1 0</code> .\n\n''Least Significant Bit First'' means that the [[least significant bit]] will arrive first: hence e.g. the same hexadecimal number <code>0x12</code>, again <code>00010010</code> in binary representation, will arrive as the (reversed) sequence <code>0 1 0 0 1 0 0 0</code>.\n\n=={{anchor|LSB 0}}LSB 0 bit numbering==\n[[Image:Lsb0.svg|thumb|300px|right|LSB 0: A container for 8-bit binary number with the highlighted [[least significant bit]] assigned the bit number 0]]\nWhen the bit numbering starts at zero for the [[least significant bit]] (LSB) the numbering scheme is called \"LSB&nbsp;0\".<ref>{{cite book \n |    author = Langdon, Glen G. \n |     title = Computer Design \n | publisher = Computeach Press Inc\n |      year = 1982 \n |      isbn = 0-9607864-0-6 \n |      page = 52 \n }}</ref> This bit numbering method has the advantage that for any [[unsigned number]] the value of the number can be calculated by using [[Positional notation#Exponentiation|exponentiation]] with the bit number and a [[Radix|base]] of&nbsp;2. The value of an unsigned binary [[integer (computer science)|integer]] is therefore\n:<math> \\sum_{i=0}^{N-1} b_i \\cdot 2^i </math>\nwhere ''b<sub>i</sub>'' denotes the value of the bit with number ''i'', and ''N'' denotes the number of bits in total.\n\n=={{anchor|MSB 0}}MSB 0 bit numbering==\n[[Image:Msb0.svg|thumb|300px|right|MSB 0: A container for 8-bit binary number with the highlighted [[most significant bit]] assigned the bit number 0]]\nSimilarly, when the bit numbering starts at zero for the [[most significant bit]] (MSB) the numbering scheme is called \"MSB&nbsp;0\".\n\nThe value of an unsigned binary integer is therefore\n:<math> \\sum_{i=0}^{N-1} b_i \\cdot 2^{N-1-i} </math>\n\n== {{anchor|MSB 1|LSB 1}}Other ==\n[[ALGOL 68]]'s '''elem''' operator is effectively \"MSB&nbsp;1 bit numbering\" as the bits are numbered from left to right, with the first bit (bits '''elem''' 1) being the \"most significant bit\", and the expression (bits '''elem''' bits width) giving the \"least significant bit\".  Similarly, when '''bits''' are coerced (typecast) to an array of [[Boolean data type|Boolean]] ([&nbsp;]'''bool''' bits), the first element of this array (bits['''lwb''' bits]) is again the \"most significant bit\".\n\nFor MSB 1 numbering, the value of an unsigned binary integer is\n:<math> \\sum_{i=1}^{N} b_i \\cdot 2^{N-i} </math>\n\n[[PL/I]] numbers {{var|BIT}} strings starting with 1 for the leftmost bit.\n\nThe [[Fortran]] {{var|BTEST}} function uses LSB&nbsp;0 numbering.\n\n==Usage==\n[[Little-endian]] CPUs usually employ \"LSB&nbsp;0\" bit numbering, however both bit numbering conventions can be seen in [[big-endian]] machines. Some architectures like [[SPARC]] and [[Motorola 68000]] use \"LSB&nbsp;0\" bit numbering, while [[S/390]], PowerPC and [[PA-RISC]] use \"MSB&nbsp;0\".<ref>{{cite journal\n |     author = David V. James\n |      title = Multiplexed buses: the endian wars continue\n |    journal = [[IEEE Micro]]\n |date=June 1990\n |     volume = 10\n |      issue = 3\n |      pages = 9&ndash;21\n |        doi = 10.1109/40.56322\n |       issn = 0272-1732\n |        url = http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=56322\n | accessdate = 2008-12-20\n |      quote = their first bits and [[nibble]]s came from [[arithmetic logic unit|ALU]] chips, which map zero (0) to the least significant bit. (...) some (otherwise) big-endian designers insist on using the little-endian notation to describe bits and the big-endian notation to describe bytes. (...) Note that IBM (on the [[IBM System/360|S/360]] and [[IBM System/370|370]]) and Hewlett-Packard (on the [[PA-RISC]] processor) consistently map zero to the MSB}}</ref>\n\nThe recommended style for [[Request for Comments]] (RfC) documents is \"MSB&nbsp;0\" bit numbering.<ref>\n{{cite web\n| url         = http://tools.ietf.org/html/rfc2360\n| title       = RFC 2360 - Guide for Internet Standards Writers\n| first       = Gregor\n| last        = Scott\n| date        = June 1998\n| publisher   = Internet Engineering Task Force (IETF)\n| page        = 11\n| accessdate  = 2010-02-14\n| quote       = The preferred form for packet diagrams is a sequence of long words in network byte order, with each word horizontal on the page and bit numbering at the top}}\n</ref><ref>\n{{cite web\n| url         = http://tools.ietf.org/html/rfc1166\n| title       = RFC 1166 - INTERNET NUMBERS\n| date        = July 1990\n| publisher   = Internet Engineering Task Force (IETF)\n| page        = 1\n| accessdate  = 2014-06-11\n| quote       = Whenever an octet represents a numeric quantity the left most bit in the diagram is the high order or most significant bit}}\n</ref>\n\nBit numbering is usually transparent to the [[software]], but some programming languages like [[Ada (programming language)|Ada]] and hardware description languages like [[VHDL]] and [[verilog]] allow specifying the appropriate bit order for data type representation.<ref>{{cite journal\n |     author = Norman H. Cohen\n |      title = Endian-independent record representation clauses\n |    journal = Ada Letters\n |       date = January 1994\n |     volume = XIV\n |      issue = 1\n |      pages = 27&ndash;29  \n |       issn = 1094-3641\n |  publisher = ACM SIGAda\n |        url = http://researchweb.watson.ibm.com/people/n/ncohen/EndianIndepRecRepClauses.pdf\n | accessdate = 2008-12-20\n |        doi = 10.1145/181492.181493\n}}</ref>\n\n==See also==\n*[[ARINC 429]]\n*[[Binary numeral system]]\n*[[Signed number representations]]\n*[[Two's complement]]\n*[[Endianness]]\n*[[Binary logarithm]]\n*[[Unit in the last place]] (ULP)\n*[[Find first set]]\n*[[MAC address#Bit-reversed notation|MAC address: Bit-reversed notation]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.xcprod.com/titan/XCSB-DOC/bit_numbers.html Bit Numbers]\n* Bit numbering for different CPUs:\n** [https://web.archive.org/web/20041029235810/http://developer.apple.com/documentation/mac/OSUtilities/OSUtilities-39.html Motorola 68000] (\"Bit manipulation\" and \"Reversed Bit-Numbering\" sections)\n** [http://publib.boulder.ibm.com/infocenter/systems/scope/syssw/index.jsp?topic=/eiccj/tutorial/cbet_1byteorder.html IBM Cell Broadband Processors] (\"Byte ordering and bit numbering\")\n\n[[Category:Binary arithmetic]]\n[[Category:Assembly languages]]"
    },
    {
      "title": "BIT predicate",
      "url": "https://en.wikipedia.org/wiki/BIT_predicate",
      "text": "In [[mathematics]] and [[computer science]], the '''BIT predicate''' or '''Ackermann coding''', sometimes written BIT(''i'',&nbsp;''j''), is a [[Predicate (mathematical logic)|predicate]] which tests whether the ''j''th [[bit]] of the number ''i'' is 1, when ''i'' is written in binary.\n\n==History==\nThe BIT predicate was first introduced as the encoding of [[hereditarily finite set]]s as [[natural number]]s by [[Wilhelm Ackermann]] in his 1937 paper<ref name=ackermann>{{cite journal|\nlast=Ackermann|first=Wilhelm| title=Die Widerspruchsfreiheit der allgemeinen Mengenlehre|\njournal=[[Mathematische Annalen]]|\nyear=1937|volume=114|pages=305–315|\nurl=http://www.digizeitschriften.de/dms/img/?PPN=PPN235181684_0114&DMDID=dmdlog23|\naccessdate=2012-01-09|doi=10.1007/bf01594179\n}}</ref><ref>{{cite journal|\nlast=Kirby|first=Laurence|\ntitle=Finitary Set Theory|\njournal=Notre Dame Journal of Formal Logic|\nyear=2009|volume=50|issue=3|pages=227–244|\nurl=https://projecteuclid.org/euclid.ndjfl/1257862036|\naccessdate=27 January 2017|doi=10.1215/00294527-2009-009\n}}</ref>\n(''The Consistency of General Set Theory'').\n\nEach natural number encodes a finite set and\neach finite set is represented by a natural number.\nThis mapping uses the [[binary numeral system]].\nIf the number ''n'' encodes a finite set ''A'' and the ''i''th binary digit of ''n'' is 1, then the set encoded by ''i'' is an [[Element (mathematics)|element]] of ''A''.\n\nThe Ackermann coding is a [[primitive recursive function]].<ref>{{cite book|page=261|last=Rautenberg|first=Wolfgang|authorlink=Wolfgang Rautenberg|doi=10.1007/978-1-4419-1221-3|title=A Concise Introduction to Mathematical Logic|url=http://www.springerlink.com/content/978-1-4419-1220-6/|publisher=[[Springer Science+Business Media]]|location=[[New York City|New York]]|edition=3rd|isbn=978-1-4419-1220-6|year=2010}}</ref>\n\n==Implementation==\nIn programming languages such as [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]], or [[Python (programming language)|Python]] that provide a [[Bitwise operation#Bit shifts|right shift operator]] <code>&gt;&gt;</code> and a [[Bitwise operation|bitwise Boolean and operator]] <code>&amp;</code>, the BIT predicate BIT(''i'',&nbsp;''j'') can be implemented by the expression\n<code>(i>>j)&1</code>. Here the bits of ''i'' are numbered from the low order bits to high order bits in the [[binary representation]] of ''i'', with the ones bit being numbered as bit&nbsp;0.<ref>{{cite book|title=Mastering C++|first=K. R.|last=Venugopal|publisher=Muhammadali Shaduli|year=1997|isbn=9780074634547|page=123|url=https://books.google.com/books?id=2soaY85jGQIC&pg=PA123}}.</ref>\n\n==Private information retrieval==\nIn the mathematical study of computer security, the [[private information retrieval]] problem can be modeled as one in which a client, communicating with a collection of servers that store a binary number ''i'', wishes to determine the result of a BIT predicate BIT(''i'',&nbsp;''j'') without divulging the value of ''j'' to the servers. {{harvtxt|Chor|Kushilevitz|Goldreich|Sudan|1998}} describe a method for replicating ''i'' across two servers in such a way that the client can solve the private information retrieval problem using a substantially smaller amount of communication than would be necessary to recover the complete value of&nbsp;''i''.<ref>{{cite journal\n | last1 = Chor | first1 = Benny\n | last2 = Kushilevitz | first2 = Eyal\n | last3 = Goldreich | first3 = Oded\n | last4 = Sudan | first4 = Madhu\n | date = 1998\n | doi = 10.1145/293347.293350\n | issue = 6\n | journal = [[Journal of the ACM]]\n | pages = 965–981\n | title = Private information retrieval\n | volume = 45\n | ref = harv}}.</ref>\n\n==Mathematical logic==\nThe BIT predicate is often examined in the context of [[first-order logic]], where we can examine the system resulting from adding the BIT predicate to first-order logic.  In [[descriptive complexity]], the complexity class FO&nbsp;+&nbsp;BIT resulting from adding the BIT predicate to [[FO (complexity)|FO]] results in a more robust complexity class.<ref name=\"Immerman book\">{{cite book | last = Immerman | first = Neil | authorlink = Neil Immerman | title = Descriptive Complexity | year = 1999 | publisher = Springer-Verlag | location = New York | isbn = 0-387-98600-6}}</ref>  The class FO&nbsp;+&nbsp;BIT, of first-order logic with the BIT predicate, is the same as the class FO&nbsp;+&nbsp;PLUS&nbsp;+&nbsp;TIMES, of first-order logic with addition and multiplication predicates.<ref name=\"Immerman BIT predicate\">{{cite book | last = Immerman | first = Neil | authorlink = Neil Immerman | title = Descriptive Complexity | year = 1999 | publisher = Springer-Verlag | location = New York | isbn = 0-387-98600-6 | pages = 14–16}}</ref>\n\n==Construction of the Rado graph==\n[[File:Rado graph.svg|thumb|316x316px|The Rado graph: for instance there is an edge from 0 to 3 because the 0th bit of 3 is non zero.]]\nAckermann in 1937 and [[Richard Rado]] in 1964 used this predicate to construct the infinite [[Rado graph]]. In their construction, the vertices of this graph correspond to the non-negative integers, written in binary, and there is an undirected edge from vertex ''i'' to vertex ''j'', for ''i''&nbsp;<&nbsp;''j'', when BIT(''j'',''i'') is nonzero.<ref>{{cite journal|first=Richard|last=Rado|authorlink=Richard Rado|url=http://matwbn.icm.edu.pl/ksiazki/aa/aa9/aa9133.pdf|title=Universal graphs and universal functions|journal=Acta Arith.|year=1964|volume=9|pages=331–340}}.</ref>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Bit Predicate}}\n[[Category:Binary arithmetic]]\n[[Category:Descriptive complexity]]\n[[Category:Circuit complexity]]\n[[Category:Set theory]]"
    },
    {
      "title": "Bit-length",
      "url": "https://en.wikipedia.org/wiki/Bit-length",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Number of bits needed to represent an integer datum or message in computing and communications}}\n{{Refimprove|date=May 2011}}\n\n'''Bit-length''' or '''bit width''' is the number of binary digits, called [[bit]]s, necessary to represent an [[Integer (computer science)|integer]]<ref>{{cite web |url=http://reference.wolfram.com/mathematica/ref/BitLength.html |title=Wolfram Mathematica 8 Documentation |accessdate=10 Jan 2012 }}</ref> in the [[binary number system]]. Formally the number of bits of zero is 1 and any other  natural number <math>n>0</math> is a function, ''bitLength''(''n''), of the [[binary logarithm]] of ''n'':\n\n: <math>bitLength(n)= \\lfloor log_2(n) + 1 \\rfloor = \\lceil log_2(n+1) \\rceil</math>\n\nAt their most fundamental level, digital computers and telecommunications devices (as opposed to [[analog signal|analog]] devices) can process only data that has been expressed in [[binary code|binary]] format.  The binary format expresses data as an arbitrary length series of values with one of two choices:  Yes/No, 1/0, True/False, etc., all of which can be expressed electronically as On/Off.  For information technology applications, the amount of information being processed is an important design consideration.  The term bit-length is technical shorthand for this measure.\n\nFor example, computer [[processors]] are often designed to process data grouped into [[data type|word]]s of a given length of bits (8 bit, 16 bit, 32 bit, 64 bit, etc.). The bit-length of each [[word (data type)|word]] defines, for one thing, how many memory locations can be independently addressed by the processor. In [[public-key cryptography]], [[key (cryptography)|key]]s are defined by their length expressed in binary digits - their bit length.\n\n==See also==\n*[[Bit width]]<!-- red link with possibilities -->\n\n==References==\n{{Reflist}}\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]"
    },
    {
      "title": "Bitwise operation",
      "url": "https://en.wikipedia.org/wiki/Bitwise_operation",
      "text": "{{Use dmy dates|date=August 2018}}\n{{more citations needed|date=August 2018}}\n\nIn [[digital computer]] [[computer programming|programming]], a '''bitwise operation''' operates on one or more [[bit pattern]]s or [[Binary numeral system|binary numerals]] at the level of their individual [[bit]]s. It is a fast and simple action, directly supported by the [[central processing unit|processor]], and is used to manipulate values for comparisons and calculations.\n\nOn simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition.{{clarify|reason= what is the difference between \"substantially\",\"signigicantly\", and \"several times\"?|date=November 2018}} While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer [[instruction pipeline]]s and other [[computer architecture|architectural]] design choices, bitwise operations do commonly use less power because of the reduced use of resources.<ref>{{cite web|url=http://cmicrotek.com/wordpress_159256135/|title=CMicrotek Low-power Design Blog|accessdate=12 August 2015|publisher=CMicrotek}}</ref>\n\n==Bitwise operators==\nIn the explanations below, any indication of a bit's position is counted from the right (least significant) side, advancing left. For example, the binary value 0001 (decimal 1) has zeroes at every position but the first one.\n\n===NOT===\n<!-- linked from redirects [[Bitwise complement]], [[Bit complement]], [[Bitwise NOT]] -->\nThe '''bitwise NOT''', or '''complement''', is a [[unary operation]] that performs [[negation|logical negation]] on each bit, forming the [[ones' complement]] of the given binary value. Bits that are 0 become 1, and those that are 1 become 0. For example:\n\n NOT '''0'''111  (decimal 7)\n   = '''1'''000  (decimal 8)\n\n NOT 10101011  (decimal 171)\n   = 01010100  (decimal 84)\n\nThe bitwise complement is equal to the [[two's complement]] of the value minus one. If two's complement arithmetic is used, then <code>NOT x = -x − 1</code>.\n\nFor unsigned [[Integer (computer science)|integers]], the bitwise complement of a number is the \"mirror reflection\" of the number across the half-way point of the unsigned integer's range. For example, for 8-bit unsigned integers, <code>NOT x = 255 - x</code>, which can be visualized on a graph as a downward line that effectively \"flips\" an increasing range from 0 to 255, to a decreasing range from 255 to 0. A simple but illustrative example use is to invert a grayscale image where each pixel is stored as an unsigned integer.\n\n===AND===\nA '''bitwise AND''' takes '''two equal-length binary representations''' and performs the [[Logical conjunction|logical AND]] operation on each pair of the corresponding bits, which is equivalent to multiplying them. Thus, if both bits in the compared position are 1, the bit in the resulting binary representation is 1 (1&nbsp;× 1&nbsp;= 1); otherwise, the result is 0 (1&nbsp;× 0&nbsp;= 0 and 0&nbsp;× 0&nbsp;= 0). For example:\n\n     010'''1''' (decimal 5)\n AND 001'''1''' (decimal 3)\n   = 000'''1''' (decimal 1)\n\nThe operation may be used to determine whether a particular bit is ''set'' (1) or ''clear'' (0). For example, given a bit pattern 0011 (decimal 3), to determine whether the second bit is set we use a bitwise AND with a bit pattern containing 1 only in the second bit:\n\n     00'''1'''1 (decimal 3)\n AND 00'''1'''0 (decimal 2)\n   = 00'''1'''0 (decimal 2)\n\nBecause the result 0010 is non-zero, we know the second bit in the original pattern was set. This is often called ''bit masking''. (By analogy, the use of [[masking tape]] covers, or ''masks'', portions that should not be altered or portions that are not of interest. In this case, the 0 values mask the bits that are not of interest.)\n\nThe bitwise AND may be used to clear selected bits (or [[Flag word|flags]]) of a register in which each bit represents an individual [[Boolean data type|Boolean]] state. This technique is an efficient way to store a number of Boolean values using as little memory as possible.\n\nFor example, 0110 (decimal 6) can be considered a set of four flags, where the first and fourth flags are clear (0), and the second and third flags are set (1). The third flag may be cleared by using a bitwise AND with the pattern that has a zero only in the third bit:\n\n     0'''1'''10 (decimal 6)\n AND 1'''0'''11 (decimal 11)\n   = 0'''0'''10 (decimal 2)\n\nBecause of this property, it becomes easy to check the [[Parity (mathematics)|parity]] of a binary number by checking the value of the lowest valued bit. Using the example above:\n\n     0110 (decimal 6)\n AND 0001 (decimal 1)\n   = 0000 (decimal 0)\n\nBecause 6 AND 1 is zero, 6 is divisible by two and therefore even.\n\n===OR===\nA '''bitwise OR''' takes two bit patterns of equal length and performs the [[Logical disjunction|logical inclusive OR]] operation on each pair of corresponding bits. The result in each position is 0 if both bits are 0, while otherwise the result is 1. For example:\n\n    0'''101''' (decimal 5)\n OR 0'''011''' (decimal 3)\n  = 0'''111''' (decimal 7)\n\nThe bitwise OR may be used to set to 1 the selected bits of the register described above. For example, the fourth bit of 0010 (decimal 2) may be set by performing a bitwise OR with the pattern with only the fourth bit set:\n\n    '''0'''0'''1'''0 (decimal 2)\n OR '''1'''0'''0'''0 (decimal 8)\n  = '''1'''0'''1'''0 (decimal 10)\n\n===XOR===\nA '''bitwise XOR''' takes two bit patterns of equal length and performs the [[Exclusive disjunction|logical exclusive OR]] operation on each pair of corresponding bits. The result in each position is 1 if only the first bit is 1 ''or'' only the second bit is 1, but will be 0 if both are 0 or both are 1. In this we perform the comparison of two bits, being 1 if the two bits are different, and 0 if they are the same. For example:\n\n     0'''10'''1 (decimal 5)\n XOR 0'''01'''1 (decimal 3)\n   = 0'''11'''0 (decimal 6)\n\nThe bitwise XOR may be used to invert selected bits in a register (also called toggle or flip). Any bit may be toggled by XORing it with 1. For example, given the bit pattern 0010 (decimal 2) the second and fourth bits may be toggled by a bitwise XOR with a bit pattern containing 1 in the second and fourth positions:\n\n     '''0'''0'''1'''0 (decimal 2)\n XOR '''1'''0'''1'''0 (decimal 10)\n   = '''1'''0'''0'''0 (decimal 8)\n\nThis technique may be used to manipulate bit patterns representing sets of Boolean states.\n\n[[Assembly language]] programmers and optimizing [[compiler]]s sometimes use XOR as a short-cut to setting the value of a [[Processor register|register]] to zero. Performing XOR on a value against itself always yields zero, and on many architectures this operation requires fewer clock cycles and memory than loading a zero value and saving it to the register.\n\n===Mathematical equivalents===\n\nAssuming {{tmath|x \\geq y}}, for the non-negative integers, the bitwise operations can be written as follows:\n\n:<math>\\begin{align}\n   \\operatorname{NOT}x &= \\sum_{n=0}^{\\lfloor\\log_2(x)\\rfloor} 2^n\\left[\\left(\\left\\lfloor\\frac{x}{2^n}\\right\\rfloor \\bmod 2 + 1\\right) \\bmod 2\\right] = 2^{\\left\\lfloor\\log_2(x)\\right\\rfloor + 1} - 1 - x\n\\\\\n  x\\operatorname{AND}y &= \\sum_{n=0}^{\\lfloor\\log_2(x)\\rfloor} 2^n\\left(\\left\\lfloor\\frac{x}{2^n}\\right\\rfloor \\bmod 2\\right)\\left(\\left\\lfloor\\frac{y}{2^n}\\right\\rfloor \\bmod 2\\right)\n\\\\\n   x\\operatorname{OR}y &= \\sum_{n=0}^{\\lfloor\\log_2(x)\\rfloor} 2^n\\left(\\left[\\left(\\left\\lfloor\\frac{x}{2^n}\\right\\rfloor \\bmod 2\\right) + \\left(\\left\\lfloor\\frac{y}{2^n}\\right\\rfloor \\bmod 2\\right) + \\left(\\left\\lfloor\\frac{x}{2^n}\\right\\rfloor \\bmod 2\\right)\\left(\\left\\lfloor\\frac{y}{2^n}\\right\\rfloor \\bmod 2\\right)\\right]\\bmod 2\\right)\n\\\\\n  x\\operatorname{XOR}y &=\n    \\sum_{n=0}^{\\lfloor\\log_2(x)\\rfloor} 2^n\\left(\\left[\\left(\\left\\lfloor\\frac{x}{2^n}\\right\\rfloor \\bmod 2\\right) + \\left(\\left\\lfloor\\frac{y}{2^n}\\right\\rfloor \\bmod 2\\right)\\right]\\bmod 2\\right) =\n    \\sum_{n=0}^{\\lfloor\\log_2(x)\\rfloor} 2^n\\left[\\left(\\left\\lfloor\\frac{x}{2^n}\\right\\rfloor + \\left\\lfloor\\frac{y}{2^n}\\right\\rfloor\\right) \\bmod 2\\right]\n\\end{align}</math>\n\n=== Truth table for all binary logical operators ===\nThere are 16 possible [[Truth function|truth functions]] of two [[Binary variable|binary variables]], this defines a [[truth table]].\n\nHere is the bitwise equivalent operations of two bits P and Q:\n\n{| class=\"wikitable\" style=\"margin:1em auto 1em auto; text-align:center;\"\n|-\n! ''p'' !! ''q''\n| rowspan=6 |\n! [[Contradiction|F]]<sup>0</sup>\n! [[Logical NOR|NOR]]<sup>1</sup>\n! [[Converse nonimplication|Xq]]<sup>2</sup>\n! [[Negation|'''¬p''']]<sup>3</sup>\n! [[Material nonimplication|↛]]<sup>4</sup>\n! [[Negation|'''¬q''']]<sup>5</sup>\n! [[Exclusive disjunction|XOR]]<sup>6</sup>\n! [[Logical NAND|NAND]]<sup>7</sup>\n| rowspan=6 |\n! [[Logical conjunction|AND]]<sup>8</sup>\n! [[Logical biconditional|XNOR]]<sup>9</sup>\n! [[Projection function|q]]<sup>10</sup>\n! [[Material conditional|If/then]]<sup>11</sup>\n! [[Projection function|p]]<sup>12</sup>\n! [[Converse implication|Then/if]]<sup>13</sup>\n! [[Logical disjunction|OR]]<sup>14</sup>\n! [[Tautology (logic)|T]]<sup>15</sup>\n|-\n! 1 !! 1\n| 0 || 0 || 0 || 0 || 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1 || 1 || 1 || 1 || 1\n|-\n! 1 !! 0\n| 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1 || 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1\n|-\n! 0 !! 1\n| 0 || 0 || 1 || 1 || 0 || 0 || 1 || 1 || 0 || 0 || 1 || 1 || 0 || 0 || 1 || 1\n|-\n! 0 !! 0\n| 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1\n|-\n! colspan=\"2\" | Bitwise <br/>equivalents\n! <small>0</small>\n! <small>NOT <br/>(p OR q)</small>\n! <small>(NOT p) <br/>AND q</small>\n! <small>NOT <br/>p</small>\n! <small>p AND <br/>(NOT q)</small>\n! <small>NOT <br/>q</small>\n! <small>p XOR q</small>\n! <small>NOT <br/>(p AND q)</small>\n! <small>p AND q</small>\n! <small>NOT <br/>(p XOR q)</small>\n! <small>q</small>\n! <small>(NOT p) <br/>OR q</small>\n! <small>p</small>\n! <small>p OR <br/>(NOT q)</small>\n! <small>p OR q</small>\n! <small>1</small>\n|}\n\n==Bit shifts==\n<!-- Courtesy note per [[WP:RSECT]]: [[Bit shift]], [[Bit-shift]] and variants redirect here -->\nThe '''bit shifts''' are sometimes considered bitwise operations, because they treat a value as a series of bits rather than as a numerical quantity. In these operations the digits are moved, or ''shifted'', to the left or right. [[Processor register|Register]]s in a computer processor have a fixed width, so some bits will be \"shifted out\" of the register at one end, while the same number of bits are \"shifted in\" from the other end; the differences between bit shift operators lie in how they determine the values of the shifted-in bits.\n\n===Arithmetic shift===\n{{main|Arithmetic shift}}\n[[File:Rotate left logically.svg|thumb|150px|Left arithmetic shift]]\n[[File:Rotate right arithmetically.svg|thumb|150px|Right arithmetic shift]]\nIn an ''arithmetic shift'', the bits that are shifted out of either end are discarded. In a left arithmetic shift, zeros are shifted in on the right; in a right arithmetic shift, the [[sign bit]] (the MSB in two's complement) is shifted in on the left, thus preserving the sign of the operand.\n\nThis example uses an 8-bit register:\n\n    00010111 (decimal +23) LEFT-SHIFT\n =  0010111'''0''' (decimal +46)\n\n    10010111 (decimal −105) RIGHT-SHIFT\n =  '''1'''1001011 (decimal −53)\n\nIn the first case, the leftmost digit was shifted past the end of the register, and a new 0 was shifted into the rightmost position. In the second case, the rightmost 1 was shifted out (perhaps into the [[carry flag]]), and a new 1 was copied into the leftmost position, preserving the sign of the number. Multiple shifts are sometimes shortened to a single shift by some number of digits. For example:\n\n    00010111 (decimal +23) LEFT-SHIFT-BY-TWO\n =  010111'''00''' (decimal +92)\n\nA left arithmetic shift by ''n'' is equivalent to multiplying by 2<sup>''n''</sup> (provided the value does not [[arithmetic overflow|overflow]]), while a right arithmetic shift by ''n'' of a [[two's complement]] value is equivalent to dividing by 2<sup>''n''</sup> and [[rounding#Round half down|rounding toward negative infinity]]. If the binary number is treated as [[ones' complement]], then the same right-shift operation results in division by 2<sup>''n''</sup> and [[rounding#Round half towards zero|rounding toward zero]].\n\n===Logical shift===\n{{main|Logical shift}}\n<!--images placed next to each other as text is currently so short-->\n{| style=\"float:right;\"  border=\"0\" cellpadding=\"0\" cellspacing=\"0\"\n|-\n| [[File:Rotate left logically.svg|thumb|150px|Left logical shift]]\n| [[File:Rotate right logically.svg|thumb|150px|Right logical shift]]\n|}\nIn a ''logical shift'', zeros are shifted in to replace the discarded bits. Therefore, the logical and arithmetic left-shifts are exactly the same.\n\nHowever, as the logical right-shift inserts value 0 bits into the most significant bit, instead of copying the sign bit, it is ideal for unsigned binary numbers, while the arithmetic right-shift is ideal for signed [[two's complement]] binary numbers.\n{{Clear}}\n\n===Circular shift===\n{{further|Circular shift}}\n<!--images placed next to each other as text is currently so short-->\n{| style=\"float:right;\"  border=\"0\" cellpadding=\"0\" cellspacing=\"0\"\n|-\n| [[File:Rotate left.svg|thumb|150px|Left circular shift or rotate]]\n| [[File:Rotate right.svg|thumb|150px|Right circular shift or rotate]]\n|}\n{{anchor|bit rotation}} <!-- Courtesy note per [[WP:RSECT]]: [[Bit rotation]] and [[Bitwise rotation]] link here -->\nAnother form of shift is the ''circular shift'', ''bitwise rotation'' or ''bit rotation''. \n\n====Rotate====\nIn this operation, sometimes called ''rotate no carry'', the bits are \"rotated\" as if the left and right ends of the register were joined. The value that is shifted into the right during a left-shift is whatever value was shifted out on the left, and vice versa for a right-shift operation. This is useful if it is necessary to retain all the existing bits, and is frequently used in digital [[cryptography]].\n{{Clear}}\n\n====Rotate through carry====\n<!--images placed next to each other as text is currently so short-->\n{| style=\"float:right;\"  border=\"0\" cellpadding=\"0\" cellspacing=\"0\"\n|-\n|[[File:Rotate left through carry.svg|thumb|150px|Left rotate through carry]]\n|[[File:Rotate right through carry.svg|thumb|150px|Right rotate through carry]]\n|}\n''Rotate through carry'' is a variant of the rotate operation, where the bit that is shifted in (on either end) is the old value of the carry flag, and the bit that is shifted out (on the other end) becomes the new value of the carry flag.\n\nA single ''rotate through carry'' can simulate a logical or arithmetic shift of one position by setting up the carry flag beforehand. For example, if the carry flag contains 0, then <code>x RIGHT-ROTATE-THROUGH-CARRY-BY-ONE</code> is a logical right-shift, and if the carry flag contains a copy of the sign bit, then <code>x RIGHT-ROTATE-THROUGH-CARRY-BY-ONE</code> is an arithmetic right-shift. For this reason, some microcontrollers such as low end [[PIC microcontroller|PIC]]s just have ''rotate'' and ''rotate through carry'', and don't bother with arithmetic or logical shift instructions.\n\nRotate through carry is especially useful when performing shifts on numbers larger than the processor's native [[word size]], because if a large number is stored in two registers, the bit that is shifted off one end of the first register must come in at the other end of the second. With rotate-through-carry, that bit is \"saved\" in the carry flag during the first shift, ready to shift in during the second shift without any extra preparation.\n{{Clear}}\n\n=== In high-level languages  ===\n{{further|Circular shift#Implementing circular shifts}}\n\n==== C-family ====\n{{anchor|Shifts in C-family languages}}\nIn [[C-family]] languages, the logical shift operators are \"<code>&lt;&lt;</code>\" for left shift and \"<code>&gt;&gt;</code>\" for right shift. The number of places to shift is given as the second argument to the operator. For example,\n\n<source lang=\"c\">x = y << 2;</source>\n\nassigns <code>x</code> the result of shifting <code>y</code> to the left by two bits, which is equivalent to a multiplication by four.\n\nShifts can result in implementation-defined behavior or [[undefined behavior]], so care must be taken when using them. The result of shifting by a bit count greater than or equal to the word's size is undefined behavior in C and C++.<ref name=\":0\" /><ref>{{Cite web|url=http://en.cppreference.com/w/cpp/language/operator_arithmetic#Bitwise_shift_operators|title=Arithmetic operators - cppreference.com|website=en.cppreference.com|access-date=2016-07-06}}</ref> Right-shifting a negative value is implementation-defined and not recommended by good coding practice;<ref>{{cite web|title=INT13-C. Use bitwise operators only on unsigned operands|url=https://www.securecoding.cert.org/confluence/display/c/INT13-C.+Use+bitwise+operators+only+on+unsigned+operands|website=CERT: Secure Coding Standards|publisher=Software Engineering Institute, Carnegie Mellon University|accessdate=7 September 2015}}</ref> the result of left-shifting a signed value is undefined if the result cannot be represented in the result type.<ref name=\":0\">[http://std.dkuug.dk/JTC1/SC22/WG14/www/docs/n843.htm JTC1/SC22/WG14 N843 \"C programming language\"], section 6.5.7</ref> \n\nIn C#, the right-shift is an arithmetic shift when the first operand is an int or long. If the first operand is of type uint or ulong, the right-shift is a logical shift.<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/xt18et0d%28v=vs.110%29.aspx|title=Operator (C# Reference)|accessdate=14 July 2013|publisher=Microsoft}}</ref>\n\n=====Circular shifts=====\n{{anchor|Circular shifts in C-family languages}}\nThe C-family of languages lack a rotate operator, but one can be synthesized from the shift operators. Care must be taken to ensure the statement is well formed to avoid [[undefined behavior]] and [[timing attack]]s in software with security requirements.<ref name=\"StackOverflow\">{{cite web|url=https://stackoverflow.com/q/31387778|title=Near constant time rotate that does not violate the standards?|accessdate=12 August 2015|publisher=Stack Exchange Network}}</ref> For example, a naive implementation that left rotates a 32-bit unsigned value <code>x</code> by <code>n</code> positions is simply:\n\n<source lang=\"c\">unsigned int x = ..., n = ...;\nunsigned int y = (x << n) | (x >> (32 - n));</source>\n\nHowever, a shift by <code>0</code> bits results in undefined behavior in the right hand expression <code>(x >> (32 - n))</code> because <code>32 - 0</code> is <code>32</code>, and <code>32</code> is outside the range <code>[0 - 31]</code> inclusive. A second try might result in:\n\n<source lang=\"c\">unsigned int x = ..., n = ...;\nunsigned int y = n ? (x << n) | (x >> (32 - n)) : x;</source>\n\nwhere the shift amount is tested to ensure it does not introduce undefined behavior. However, the branch adds an additional code path and presents an opportunity for timing analysis and attack, which is often not acceptable in high integrity software.<ref name=\"StackOverflow\" /> In addition, the code compiles to multiple machine instructions, which is often less efficient than the processor's native instruction.\n\nTo avoid the undefined behavior and branches under GCC and Clang, the following is recommended. The pattern is recognized by many compilers, and the compiler will emit a single rotate instruction:<ref>{{cite web|url=https://gcc.gnu.org/bugzilla/show_bug.cgi?id=57157|title=Poor optimization of portable rotate idiom|accessdate=11 August 2015|publisher=GNU GCC Project}}</ref><ref>{{cite web|url=https://software.intel.com/en-us/forums/topic/580884|title=Circular rotate that does not violate C/C++ standard?|accessdate=12 August 2015|publisher=Intel Developer Forums}}</ref><ref name=LLVM>{{cite web|url=https://llvm.org/bugs/show_bug.cgi?id=24226|title=Constant not propagated into inline assembly, results in &quot;constraint 'I' expects an integer constant expression&quot;|accessdate=11 August 2015|publisher=LLVM Project}}</ref>\n\n<source lang=\"c\">unsigned int x = ..., n = ...;\nunsigned int y = (x << n) | (x >> (-n & 31));</source>\n\nThere are also compiler-specific [[Intrinsic function|intrinsics]] implementing [[circular shift]]s, like [http://msdn.microsoft.com/en-us/library/t5e2f3sc(VS.80).aspx _rotl8, _rotl16], [http://msdn.microsoft.com/en-us/library/yy0728bz(VS.80).aspx _rotr8, _rotr16] in Microsoft [[Visual C++]]. Clang provides some rotate intrinsics for Microsoft compatibility that suffers the problems above.<ref name=LLVM /> GCC does not offer rotate intrinsics. Intel also provides x86 [https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=rot&techs=Other Intrinsics].\n\n==== Java ====\n{{anchor|Shifts in Java}}\nIn [[Java (programming language)|Java]], all integer types are signed, so the \"<code>&lt;&lt;</code>\" and \"<code>&gt;&gt;</code>\" operators perform arithmetic shifts. Java adds the operator \"<code>&gt;&gt;&gt;</code>\" to perform logical right shifts, but since the logical and arithmetic left-shift operations are identical for signed integer, there is no \"<code>&lt;&lt;&lt;</code>\" operator in Java.\n\nMore details of Java shift operators:<ref>The Java Language Specification, section [http://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.19 15.19. Shift Operators]</ref>\n* The operators <code><<</code> (left shift), <code>>></code> (signed right shift), and <code>>>></code> (unsigned right shift) are called the ''shift operators''.\n* The type of the shift expression is the promoted type of the left-hand operand. For example, <code>aByte >>> 2</code> is equivalent to {{code|2=java|((int) aByte) >>> 2}}.\n* If the promoted type of the left-hand operand is int, only the five lowest-order bits of the right-hand operand are used as the shift distance. It is as if the right-hand operand were subjected to a bitwise logical AND operator & with the mask value 0x1f (0b11111).<ref name=\"jls15.22.1\">{{cite web|url=http://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.22.1|title=Chapter 15. Expressions|website=oracle.com}}</ref> The shift distance actually used is therefore always in the range 0 to 31, inclusive.\n* If the promoted type of the left-hand operand is long, then only the six lowest-order bits of the right-hand operand are used as the shift distance. It is as if the right-hand operand were subjected to a bitwise logical AND operator & with the mask value 0x3f (0b111111).<ref name=\"jls15.22.1\"/> The shift distance actually used is therefore always in the range 0 to 63, inclusive.\n* The value of {{code|n >>> s}} is ''n'' right-shifted ''s'' bit positions with zero-extension.\n* In bit and shift operations, the type <code>''byte''</code> is implicitly converted to <code>''int''</code>. If the byte value is negative, the highest bit is one, then ones are used to fill up the extra bytes in the int. So {{code|1=byte b1 = -5; int i = b1 {{!}} 0x0200;|2=java}} will give {{code|1=i == -5}} as result.\n\n==== JavaScript ====\n{{anchor|Shifts in JavaScript}}\n[[JavaScript]] uses bitwise operations to evaluate each of two or more [[units place]] to 1 or 0.<ref>[https://www.w3schools.com/js/js_bitwise.asp \"JavaScript Bitwise\"]. ''W3Schools.com''.</ref>\n\n==== Pascal ====\n{{anchor|Shifts in Pascal}}\nIn Pascal, as well as in all its dialects (such as [[Object Pascal]] and [[GNU Pascal|Standard Pascal]]), the left and right shift operators are \"<code>shl</code>\" and \"<code>shr</code>\", respectively. The number of places to shift is given as the second argument. For example, the following assigns ''x'' the result of shifting ''y'' to the left by two bits:\n\n <source lang=\"pascal\">x := y shl 2;</source>\n\n== Other ==\n* [[popcount]], used in cryptography\n* [[count leading zeros]]\n\n==Applications==\nBitwise operations are necessary particularly in lower-level programming such as device drivers, low-level graphics, communications protocol packet assembly, and decoding.\n\nAlthough machines often have efficient built-in instructions for performing arithmetic and logical operations, all these operations can be performed by combining the bitwise operators and zero-testing in various ways.<ref>{{cite web|url=http://bisqwit.iki.fi/story/howto/bitmath/ |title=Synthesizing arithmetic operations using bit-shifting tricks |publisher=Bisqwit.iki.fi |date=15 February 2014 |accessdate=8 March 2014}}</ref>  For example, here is a [[pseudocode]] implementation of [[ancient Egyptian multiplication]] showing how to multiply two arbitrary integers <code>a</code> and <code>b</code> (<code>a</code> greater than <code>b</code>) using only bitshifts and addition:\n\n<syntaxhighlight lang=\"c\">\nc ← 0\nwhile b ≠ 0\n    if (b and 1) ≠ 0\n        c ← c + a\n    left shift a by 1\n    right shift b by 1\nreturn c\n</syntaxhighlight>\n\nAnother example is a pseudocode implementation of addition, showing how to calculate a sum of two integers <code>a</code> and <code>b</code> using bitwise operators and zero-testing:\n\n<syntaxhighlight lang=\"c\">\nwhile a ≠ 0\n    c ← b and a\n    b ← b xor a\n    left shift c by 1\n    a ← c\nreturn b\n</syntaxhighlight>\n\n==See also==\n{{Div col|colwidth=22em}}\n* [[Arithmetic logic unit]]\n* [[Bit manipulation]]\n* [[Bitboard]]\n* [[Bitwise operations in C]]\n* [[Boolean algebra (logic)]]\n* [[Double dabble]]\n* [[Find first set]]\n* [[Karnaugh map]]\n* [[Logic gate]]\n* [[Logical operator]]\n* [[Primitive data type]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.miniwebtool.com/bitwise-calculator/ Online Bitwise Calculator] supports Bitwise AND, OR and XOR\n*[http://www.cs.uiowa.edu/~jones/bcd/divide.html Division using bitshifts]\n* \"[http://demonstrations.wolfram.com/BitwiseOperationsModN/ Bitwise Operations Mod N]\" by Enrique Zeleny, [[Wolfram Demonstrations Project]].\n* \"[http://demonstrations.wolfram.com/PlotsOfCompositionsOfBitwiseOperations/ Plots Of Compositions Of Bitwise Operations]\" by Enrique Zeleny, The Wolfram Demonstrations Project.\n\n{{DEFAULTSORT:Bitwise Operation}}\n[[Category:Binary arithmetic]]\n[[Category:Operators (programming)]]\n[[Category:Articles with example pseudocode]]\n[[Category:Boolean algebra]]"
    },
    {
      "title": "Bitwise operations in C",
      "url": "https://en.wikipedia.org/wiki/Bitwise_operations_in_C",
      "text": "{{Short description|operations transforming individual bits of integral data types}}\n{{Use American English|date = March 2019}}\n{{manual|date=March 2015}}\nIn the [[C (programming language)|C programming language]], operations can be performed on a [[bit|bit level]] using [[bitwise operation|bitwise operators]].\n\nBitwise operations are contrasted by [[byte|byte-level]] operations which characterize the bitwise operators' logical counterparts, the AND, OR and NOT operators. Instead of performing on individual bits, byte-level operators perform on strings of eight bits (known as bytes) at a time. The reason for this is that a byte is normally the smallest unit of addressable memory (i.e. data with a unique [[memory address]].)\n\nThis applies to bitwise operators as well, which means that even though they operate on only one bit at a time they cannot accept anything smaller than a byte as their input.\n\nAll of these operators are also available in [[C++]].\n\n==Bitwise operators==\nC provides six [[Operators in C and C++|operators]] for [[bit manipulation]].<ref name=\"k&r2e\">{{cite book | last = Kernighan | author1-link = Brian Kernighan | author2 = Dennis M. Ritchie | authorlink2 = Dennis Ritchie | title = The C Programming Language | edition = 2nd | publisher = [[Prentice Hall]] | date = March 1988 | location = [[Englewood Cliffs, NJ]] | url = http://cm.bell-labs.com/cm/cs/cbook/ | isbn = 0-13-110362-8 | deadurl = yes | archiveurl = https://web.archive.org/web/20081106175456/http://cm.bell-labs.com/cm/cs/cbook/ | archivedate = 2008-11-06 | df =  }} Regarded by many to be the authoritative reference on C.</ref>\n\n{| class=\"wikitable\"\n! Symbol || Operator \n|-\n| <code>&</code> || bitwise AND\n|-\n| <code>&#124;</code> || bitwise inclusive OR\n|-\n| <code>^</code> || bitwise XOR (eXclusive OR)\n|-\n| <code><<</code> || left shift\n|-\n| <code>>></code> || right shift\n|-\n| <code>~</code> || bitwise NOT (one's complement) (unary)\n|}\n\n===Bitwise AND <code>&</code> ===\n{| class=\"wikitable\"\n|-\n! bit a !! bit b !! <code>a & b</code> (a AND b)\n|-\n| 0|| 0 || 0\n|-\n| 0 || 1 || 0\n|-\n| 1 || 0 || 0\n|-\n| 1 || 1 ||1\n|}\n\nThe bitwise AND operator is a single ampersand: <code>&</code>. It is just a representation of AND which does its work on the bits of the operands rather than the truth value of the operands.  Bitwise binary AND does the logical '''AND''' (as shown in the table above) of the bits in each position of a number in its binary form.\n\nFor instance, working with a byte (the char type):\n      11001000  \n    & 10111000 \n      -------- \n    = 10001000\n\nThe [[most significant bit]] of the first number is 1 and that of the second number is also 1 so the most significant [[bit]] of the result is 1; in the second most significant bit, the bit of second number is zero, so we have the  result as 0.  \n<ref name=\"cprogramming.com\">{{cite web|url=http://www.cprogramming.com/tutorial/bitwise_operators.html|title=Tutorials - Bitwise Operators and Bit Manipulations in C and C++|website=cprogramming.com}}</ref>\n\n===Bitwise OR <code>|</code>===\n{| class=\"wikitable\"\n|-\n! bit a !! bit b !! a &#124; b (a OR b)\n|-\n| 0        || 0        || 0\n|-\n| 0        || 1           || 1\n|-\n| 1        || 0           || 1\n|-\n| 1        || 1           || 1\n|}\nSimilar to bitwise AND, bitwise OR only operates at the bit level. Its result is a 1 if one of the either bits is 1 and zero only when both bits are 0. Its symbol is <code>|</code> which can be called a pipe.\n<syntaxhighlight lang = \"text\">\n      11001000  \n    | 10111000 \n      -------- \n    = 11111000\n</syntaxhighlight>\n \n<ref name=\"cprogramming.com\"/>\n\n===Bitwise XOR <code>^</code>===\n{| class=\"wikitable\"\n|-\n! bit a !! bit b !! <code>a ^ b</code> (a XOR b)\n|-\n|  0|| 0 || 0\n|-\n|  0|| 1 || 1\n|-\n| 1 || 0 || 1\n|-\n| 1 || 1 || 0\n|}\nThe bitwise XOR (exclusive or) performs a logical XOR function, which is equivalent to adding two bits and discarding the carry. The result is zero only when we have two zeroes or two ones.<ref>{{cite web|url=http://www.electronics-tutorials.ws/logic/logic_7.html|title=Exclusive-OR Gate Tutorial|work=Basic Electronics Tutorials}}</ref>  XOR can be used to toggle the bits between 1 and 0. Thus <code>i = i ^ 1</code> when used in a loop toggles its values between 1 and 0.<ref>{{cite web|url=http://www.fredosaurus.com/notes-cpp/expressions/bitops.html|title=C++ Notes: Bitwise Operators|website=fredosaurus.com}}</ref>\n<syntaxhighlight lang = \"text\">\n      11001000  \n    ^ 10111000 \n      -------- \n    = 01110000\n</syntaxhighlight>\n \n===Bitwise NOT <code>~</code> / ones' complement (unary)===\n{| class=\"wikitable\"\n|-\n! bit a !! <code>~a</code> (complement of a)\n|-\n| 0 || 1\n|-\n| 1 || 0\n|}\nThe ones' complement (<code>~</code>) or the bitwise complement gets us the complement of a given number. Thus we get the bits inverted, for every bit <code>1</code> the result is bit <code>0</code> and conversely for every bit <code>0</code> we have a bit <code>1</code>.  This operation should not be confused with [[negation|logical negation]] [[C operators#Logical operators|<code>!</code>]].\n<syntaxhighlight lang = \"text\">\n    ~ 11001000   \n      -------- \n    = 00110111\n</syntaxhighlight>\n \n==Shift operators==\nThere are two bitwise shift operators. They are\n*Right shift (<code>>></code>)\n*Left shift (<code><<</code>)\n\n===Right shift <code>>></code>===\nThe symbol of right shift operator is <code>>></code>. For its operation, it requires two [[operand]]s. It shifts each bit in its left operand to the right.\nThe number following the operator decides the number of places the bits are shifted (i.e. the right operand).\nThus by doing <code>ch >> 3</code> all the bits will be shifted to the right by three places and so on.\n\nExample:\n\n:If the variable <code>ch</code> contains the bit pattern <code>11100101</code>, then <code>ch >> 1</code> will produce the result <code>01110010</code>, and <code>ch >> 2</code> will produce <code>00111001</code>.\n\nHere blank spaces are generated simultaneously on the left when the bits are shifted to the right. When performed on an unsigned type, the operation performed is a [[logical shift]], causing the blanks to be filled by <code>0</code>s (zeros).  When performed on a signed type, the result is technically undefined and compiler dependent,<ref>{{cite web|url=http://www.learncpp.com/cpp-tutorial/38-bitwise-operators/|title=3.8 — Bitwise operators|work=Learn C++}}</ref> however most compilers will perform an [[arithmetic shift]], causing the blank to be filled with the sign bit of the left operand.\n\nRight shift can be used to divide a bit pattern by 2 as shown:\n<syntaxhighlight lang = \"c\">\ni = 14; // Bit pattern 00001110\nj = i >> 1; // here we have the bit pattern shifted by 1 thus we get 00000111 = 7 which is 14/2 \n</syntaxhighlight>\n\n====Right shift operator usage====\nTypical usage of a right shift operator in C can be seen from the following code.\n\nExample:\n<syntaxhighlight lang=\"c\">\n#include <stdio.h>\n    \nvoid showbits(unsigned int x) {\n    for(int i = (sizeof(int) * 8) - 1; i >= 0; i--) {\n       putchar(x & (1u << i) ? '1' : '0');\n    }\n    printf(\"\\n\");\n}\n\nint main() {\n    int j = 5225;\n    printf(\"%d in binary \\t\\t \", j);\n    /* assume we have a function that prints a binary string when given \n       a decimal integer \n     */\n    showbits(j);\n\n    /* the loop for right shift operation */\n    for (int m = 0; m <= 5; m++) {\n        int n = j >> m;\n        printf(\"%d right shift %d gives \", j, m);\n        showbits(n);\n    }\n    return 0;\n}\n</syntaxhighlight>\n\nThe output of the above program will be\n<syntaxhighlight lang = \"text\">\n5225 in binary           00000000000000000001010001101001\n5225 right shift 0 gives 00000000000000000001010001101001\n5225 right shift 1 gives 00000000000000000000101000110100\n5225 right shift 2 gives 00000000000000000000010100011010\n5225 right shift 3 gives 00000000000000000000001010001101\n5225 right shift 4 gives 00000000000000000000000101000110\n5225 right shift 5 gives 00000000000000000000000010100011\n</syntaxhighlight>\n\n===Left shift <code><<</code>===\nThe symbol of left shift operator is  <code><<</code>. It shifts each bit in its left-hand operand to the left by the number of positions indicated by the right-hand operand. It works opposite to that of right shift operator. Thus by doing <code>ch << 1</code> in the above example we have <code>11001010</code>.\nBlank spaces generated are filled up by zeroes as above.\n\nLeft shift can be used to multiply an integer by powers of 2 as in\n<syntaxhighlight lang = \"c\">\nint i = 4; /* bit pattern equivalent is binary 100 */\nint j = i << 2; /* makes it binary 10000, which multiplies the original number by 4 i.e. 16 */\n</syntaxhighlight>\n\n==A simple addition program Example==\nThe following program adds two operands using AND, XOR and left shift (<<).\n<syntaxhighlight lang = \"c\">\n#include <stdio.h>\n\nint main()\n{\n    unsigned int x = 3, y = 1, sum, carry;\n    sum = x ^ y; // x XOR y\n    carry = x & y; // x AND y\n    while (carry != 0) {\n        carry = carry << 1; // left shift the carry\n        x = sum; // initialize x as sum\n        y = carry; // initialize y as carry\n        sum = x ^ y; // sum is calculated\n        carry = x & y; /* carry is calculated, the loop condition is \n                          evaluated and the process is repeated until \n                          carry is equal to 0.\n                        */\n    }\n    printf(\"%u\\n\", sum); // the program will print 4\n    return 0;\n}\n</syntaxhighlight>\n\n==Bitwise assignment operators==\nC provides a compound assignment operator for each [[binary operation|binary]] arithmetic and bitwise operation (i.e. each operation which accepts two operands).  Each of the compound bitwise assignment operators perform the appropriate binary operation and store the result in the left operand.<ref>{{cite web|url=https://publib.boulder.ibm.com/infocenter/comphelp/v8v101/index.jsp?topic=%2Fcom.ibm.xlcpp8a.doc%2Flanguage%2Fref%2Fcomasse.htm|title=C/C++ Compound assignment operators|work=XL C/C++ V8.0 for AIX|publisher=IBM|accessdate=11 November 2013}}</ref>\n\nThe bitwise assignment operators are as follows:\n\n{| class=\"wikitable sortable\"\n! Symbol || Operator\n|-\n| <code>&=</code> || bitwise AND assignment\n|-\n| <code>&#124;=</code> || bitwise inclusive OR assignment\n|-\n| <code>^=</code> || bitwise exclusive OR assignment\n|-\n| <code><<=</code> || left shift assignment\n|-\n| <code>>>=</code> || right shift assignment\n|}\n\n== Logical equivalents ==\nFour of the bitwise operators have equivalent logical operators.  They are equivalent in that they have the same truth tables.  However, logical operators treat each operand as having only one value, either true or false, rather than treating each bit of an operand as an independent value.   Logical operators consider zero false and any nonzero value true.  Another difference is that logical operators perform [[short-circuit evaluation]].\n\nThe table below matches equivalent operators and shows a and b as operands of the operators.\n\n{| class=\"wikitable\"\n! Bitwise || Logical \n|-\n| <code>a & b</code> || <code>a && b</code>\n|-\n| <code>a &#124; b</code> || <code>a &#124;&#124; b</code>\n|-\n| <code>a ^ b</code> || <code>a != b</code>\n|-\n| <code>~a</code> || <code>!a</code>\n|-\n|}\n\n<code>!=</code> has the same truth table as <code>^</code> but unlike the true logical operators, by itself <code>!=</code> is not strictly speaking a logical operator.  This is because a logical operator must treat any nonzero value the same.  To be used as a logical operator <code>!=</code> requires that operands be normalized first. A logical not applied to both operands won’t change the truth table that results but will ensure all nonzero values are converted to the same value before comparison. This works because <code>!</code> on a zero always results in a one and <code>!</code> on any nonzero value always results in a zero.\n\nExample:\n<syntaxhighlight lang = \"c\">\n/* Equivalent bitwise and logical operator tests */\n#include <stdio.h>\n\nvoid testOperator(char* name, unsigned char was, unsigned char expected);\n\nint main()\n{\n\t// -- Bitwise operators -- //\n\n\t//Truth tables packed in bits\n\tconst unsigned char operand1    = 0x0A; //0000 1010\n\tconst unsigned char operand2    = 0x0C; //0000 1100\n\tconst unsigned char expectedAnd = 0x08; //0000 1000\n\tconst unsigned char expectedOr  = 0x0E; //0000 1110\n\tconst unsigned char expectedXor = 0x06; //0000 0110\n\t\n\tconst unsigned char operand3    = 0x01; //0000 0001\n\tconst unsigned char expectedNot = 0xFE; //1111 1110\n\n\ttestOperator(\"Bitwise AND\", operand1 & operand2, expectedAnd);\n\ttestOperator(\"Bitwise  OR\", operand1 | operand2, expectedOr);\n\ttestOperator(\"Bitwise XOR\", operand1 ^ operand2, expectedXor);\n\ttestOperator(\"Bitwise NOT\", ~operand3, expectedNot);\t\n\tprintf(\"\\n\");\n\n\t// -- Logical operators -- //\n\n\tconst unsigned char F = 0x00; //Zero\n\tconst unsigned char T = 0x01; //Any nonzero value\n\n\t//Truth tables packed in arrays\n\tconst unsigned char operandArray1[4]    = {T, F, T, F};\n\tconst unsigned char operandArray2[4]    = {T, T, F, F};\n\tconst unsigned char expectedArrayAnd[4] = {T, F, F, F};\n\tconst unsigned char expectedArrayOr[4]  = {T, T, T, F};\n\tconst unsigned char expectedArrayXor[4] = {F, T, T, F};\n\t\n\tconst unsigned char operandArray3[2]    = {F, T};\n\tconst unsigned char expectedArrayNot[2] = {T, F};\n\n\tint i;\n\tfor (i = 0; i < 4; i++) {\n\t\ttestOperator(\"Logical AND\", operandArray1[i] && operandArray2[i], expectedArrayAnd[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tfor (i = 0; i < 4; i++) {\n\t\ttestOperator(\"Logical  OR\", operandArray1[i] || operandArray2[i], expectedArrayOr[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tfor (i = 0; i < 4; i++) {\n\t\t//Needs ! on operand's in case nonzero values are different\n\t\ttestOperator(\"Logical XOR\", !operandArray1[i] != !operandArray2[i], expectedArrayXor[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tfor (i = 0; i < 2; i++) {\n\t\ttestOperator(\"Logical NOT\", !operandArray3[i], expectedArrayNot[i]);\n\t}\n\tprintf(\"\\n\");\n\treturn 0;\n}\n\nvoid testOperator(char* name, unsigned char was, unsigned char expected) {\n    char* result = (was == expected) ? \"passed\" : \"failed\";\n\tprintf(\"%s %s test, was: %X expected: %X \\n\", name, result, was, expected);    \n}\n</syntaxhighlight>\n\nThe output of the above program will be\n<syntaxhighlight lang = \"text\">\n Bitwise AND passed, was: 8 expected: 8\n Bitwise  OR passed, was: E expected: E\n Bitwise XOR passed, was: 6 expected: 6\n Bitwise NOT passed, was: FE expected: FE\n \n Logical AND passed, was: 1 expected: 1\n Logical AND passed, was: 0 expected: 0\n Logical AND passed, was: 0 expected: 0\n Logical AND passed, was: 0 expected: 0\n \n Logical  OR passed, was: 1 expected: 1\n Logical  OR passed, was: 1 expected: 1\n Logical  OR passed, was: 1 expected: 1\n Logical  OR passed, was: 0 expected: 0\n \n Logical XOR passed, was: 0 expected: 0\n Logical XOR passed, was: 1 expected: 1\n Logical XOR passed, was: 1 expected: 1\n Logical XOR passed, was: 0 expected: 0\n \n Logical NOT passed, was: 1 expected: 1\n Logical NOT passed, was: 0 expected: 0\n</syntaxhighlight>\n\n==See also==\n*[[Bit manipulation]]\n*[[Bitwise operation]]\n*[[Find first set]]\n*[[Operators in C and C++]]\n*[[Bitboard]]\n*[[Boolean algebra (logic)]]\n*[[XOR swap algorithm]]\n*[[XOR linked list]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.cprogramming.com/tutorial/bitwise_operators.html Bitwise Operators]\n\n[[Category:Binary arithmetic]]\n[[Category:C (programming language)]]"
    },
    {
      "title": "Booth's multiplication algorithm",
      "url": "https://en.wikipedia.org/wiki/Booth%27s_multiplication_algorithm",
      "text": "'''Booth's multiplication algorithm''' is a [[multiplication algorithm]] that multiplies two signed [[base 2|binary]] numbers in [[two's complement|two's complement notation]]. The [[algorithm]] was invented by [[Andrew Donald Booth]] in 1950 while doing research on [[crystallography]] at [[Birkbeck, University of London|Birkbeck College]] in [[Bloomsbury]], [[London]].<ref name=\"Booth_1951\"/> Booth's algorithm is of interest in the study of [[computer architecture]].\n\n==The algorithm==\nBooth's algorithm examines adjacent pairs of [[bit]]s of the 'N'-bit multiplier ''Y'' in signed [[two's complement]] representation, including an implicit bit below the [[least significant bit]], ''y''<sub>−1</sub> = 0.  For each bit ''y''<sub>''i''</sub>, for ''i'' running from 0 to ''N'' − 1, the bits ''y''<sub>''i''</sub> and ''y''<sub>''i''−1</sub> are considered.  Where these two bits are equal, the product accumulator ''P'' is left unchanged.  Where ''y''<sub>''i''</sub> = 0 and  ''y''<sub>''i''−1</sub> = 1, the multiplicand times 2<sup>''i''</sup> is added to ''P''; and where ''y''<sub>i</sub> = 1 and ''y''<sub>i−1</sub> = 0, the multiplicand times 2<sup>''i''</sup> is subtracted from ''P''.  The final value of ''P'' is the signed product.\n\nThe representations of the multiplicand and product are not specified; typically, these are both also in two's complement representation, like the multiplier, but any number system that supports addition and subtraction will work as well.  As stated here, the order of the steps is not determined.  Typically, it proceeds from [[Least significant bit|LSB]] to [[Most significant bit|MSB]], starting at ''i'' = 0; the multiplication by 2<sup>''i''</sup> is then typically replaced by incremental shifting of the ''P'' accumulator to the right between steps; low bits can be shifted out, and subsequent additions and subtractions can then be done just on the highest ''N'' bits of ''P''.<ref name=\"Chen_1992\"/> There are many variations and optimizations on these details.....\n\nThe algorithm is often described as converting strings of 1s in the multiplier to a high-order +1 and a low-order −1 at the ends of the string.  When a string runs through the MSB, there is no high-order +1, and the net effect is interpretation as a negative of the appropriate value.\n\n==A typical implementation==\n[[Image:Calculator walther hg.jpg|thumb|right|A Walther WSR160 [[Odhner Arithmometer|arithmometer]] from 1960. Each turn of the crank handle adds ''(up)'' or subtracts ''(down)'' the operand set to the top register from the value in the accumulator register at the bottom. [[Arithmetic shift|Shifting]] the adder left or right multiplies the effect by ten.]]\nBooth's algorithm can be implemented by repeatedly adding (with ordinary unsigned binary addition) one of two predetermined values ''A'' and ''S'' to a product ''P'', then performing a rightward [[arithmetic shift]] on ''P''.  Let '''m''' and '''r''' be the [[multiplicand]] and [[Multiplication#Terminology|multiplier]], respectively; and let ''x'' and ''y'' represent the number of bits in '''m''' and '''r'''.\n\n# Determine the values of ''A'' and ''S'', and the initial value of ''P''.  All of these numbers should have a length equal to (''x''&nbsp;+&nbsp;''y''&nbsp;+&nbsp;1).\n## A: Fill the most significant (leftmost) bits with the value of '''m'''.  Fill the remaining (''y''&nbsp;+&nbsp;1) bits with zeros.\n## S: Fill the most significant bits with the value of (&minus;'''m''') in two's complement notation.  Fill the remaining (''y''&nbsp;+&nbsp;1) bits with zeros.\n## P: Fill the most significant ''x'' bits with zeros.  To the right of this, append the value of '''r'''.  Fill the least significant (rightmost) bit with a zero.\n# Determine the two least significant (rightmost) bits of ''P''.\n## If they are 01, find the value of ''P''&nbsp;+&nbsp;''A''.  Ignore any overflow.\n## If they are 10, find the value of ''P''&nbsp;+&nbsp;''S''.  Ignore any overflow.\n## If they are 00, do nothing.  Use ''P'' directly in the next step.\n## If they are 11, do nothing.  Use ''P'' directly in the next step.\n# [[arithmetic shift|Arithmetically shift]] the value obtained in the 2nd step by a single place to the right.  Let ''P'' now equal this new value.\n# Repeat steps 2 and 3 until they have been done ''y'' times.\n# Drop the least significant (rightmost) bit from ''P''.  This is the product of '''m''' and '''r'''.\n\n==Example==\nFind 3 &times; (&minus;4), with '''m''' = 3 and '''r''' = &minus;4, and ''x'' = 4 and ''y'' = 4:\n\n* m = 0011, -m = 1101, r = 1100\n* A = 0011 0000 0\n* S = 1101 0000 0\n* P = 0000 1100 0\n* Perform the loop four times:\n*# P = 0000 110'''0 0'''.  The last two bits are 00.\n*#* P = 0000 0110 0.  Arithmetic right shift.\n*# P = 0000 011'''0 0'''. The last two bits are 00.\n*#* P = 0000 0011 0.  Arithmetic right shift.\n*# P = 0000 001'''1 0'''.  The last two bits are 10.\n*#* P = 1101 0011 0.  P = P + S.\n*#* P = 1110 1001 1.  Arithmetic right shift.\n*# P = 1110 100'''1 1'''.  The last two bits are 11.\n*#* P = 1111 0100 1.  Arithmetic right shift.\n* The product is 1111 0100, which is &minus;12.\n\nThe above-mentioned technique is inadequate when the multiplicand is the [[two's complement#Most negative number|most negative number]] that can be represented (e.g. if the multiplicand has 4 bits then this value is &minus;8). One possible correction to this problem is to add one more bit to the left of A, S and P. This then follows the implementation described above, with modifications in determining the bits of A and S; e.g., the value of '''m''', originally  assigned to the first ''x'' bits of A, will be assigned to the first ''x''+1 bits of A. Below, the improved technique is demonstrated by multiplying &minus;8 by 2 using 4 bits for the multiplicand and the multiplier:\n* A = 1 1000 0000 0\n* S = 0 1000 0000 0\n* P = 0 0000 0010 0\n* Perform the loop four times:\n*# P = 0 0000 001'''0 0'''. The last two bits are 00.\n*#* P = 0 0000 0001 0. Right shift.\n*# P = 0 0000 000'''1 0'''. The last two bits are 10.\n*#* P = 0 1000 0001 0. P = P + S.\n*#* P = 0 0100 0000 1. Right shift.\n*# P = 0 0100 000'''0 1'''. The last two bits are 01.\n*#* P = 1 1100 0000 1. P = P + A.\n*#* P = 1 1110 0000 0. Right shift.\n*# P = 1 1110 000'''0 0'''. The last two bits are 00.\n*#* P = 1 1111 0000 0. Right shift.\n* The product is 11110000 (after discarding the first and the last bit) which is &minus;16.\n\n==How it works==\nConsider a positive multiplier consisting of a block of 1s surrounded by 0s. For example, 00111110. The product is given by:\n: <math> M \\times \\,^{\\prime\\prime} 0 \\; 0 \\; 1 \\; 1 \\; 1 \\; 1 \\; 1 \\; 0 \\,^{\\prime\\prime} = M \\times (2^5 + 2^4 + 2^3 + 2^2 + 2^1) =  M \\times 62 </math>\nwhere M is the multiplicand. The number of operations can be reduced to two by rewriting the same as \n: <math> M \\times \\,^{\\prime\\prime} 0 \\; 1 \\; 0 \\;0 \\;0 \\; 0 \\mbox{-1} \\; 0 \\,^{\\prime\\prime} = M \\times (2^6 - 2^1) = M \\times 62. </math>\n\nIn fact, it can be shown that any sequence of 1s in a binary number can be broken into the difference of two binary numbers:\n\n: <math> (\\ldots 0 \\overbrace{1 \\ldots 1}^{n} 0 \\ldots)_{2} \\equiv (\\ldots 1 \\overbrace{0 \\ldots 0}^{n} 0 \\ldots)_{2} - (\\ldots 0 \\overbrace{0 \\ldots 1}^{n} 0 \\ldots)_2. </math>\n\nHence, the multiplication can actually be replaced by the string of ones in the original number by simpler operations, adding the multiplier, shifting the partial product thus formed by appropriate places, and then finally subtracting the multiplier. It is making use of the fact that it is not necessary to do anything but shift while  dealing with 0s in a binary multiplier, and is similar to using the mathematical property that 99&nbsp;=&nbsp;100&nbsp;&minus;&nbsp;1 while multiplying by 99.\n\nThis scheme can be extended to any number of blocks of 1s in a multiplier (including the case of a single 1 in a block). Thus,\n\n: <math> M \\times \\,^{\\prime\\prime} 0 \\; 0 \\; 1 \\; 1 \\; 1 \\; 0 \\; 1 \\; 0 \\,^{\\prime\\prime} = M \\times (2^5 + 2^4 + 2^3 + 2^1) = M \\times 58 </math>\n: <math> M \\times \\,^{\\prime\\prime} 0 \\; 1 \\; 0 \\; 0 \\mbox{-1} \\; 1 \\mbox{-1} \\; 0 \\,^{\\prime\\prime} = M \\times (2^6 - 2^3 + 2^2 - 2^1) = M \\times 58. </math>\n\nBooth's algorithm follows this old scheme by performing an addition when it encounters the first digit of a block of ones (0 1) and a subtraction when it encounters the end of the block (1 0).  This works for a negative multiplier as well.  When the ones in a multiplier are grouped into long blocks, Booth's algorithm performs fewer additions and subtractions than the normal multiplication algorithm.\n\n== See also ==\n* [[Binary multiplier]]\n* [[Non-adjacent form]]\n* [[Redundant binary representation]]\n* [[Wallace tree]]\n\n==References==\n{{reflist|refs=\n<ref name=\"Booth_1951\">{{cite journal |title=A Signed Binary Multiplication Technique |author-first=Andrew Donald |author-last=Booth |author-link=Andrew Donald Booth |journal=The Quarterly Journal of Mechanics and Applied Mathematics<!-- Q.J. Mech. Appl. Math. --> |volume=IV |issue=2 |date=1951 |orig-year=1950-08-01 |pages=236--240 |url=http://bwrc.eecs.berkeley.edu/Classes/icdesign/ee241_s00/PAPERS/archive/booth51.pdf |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180716222422/http://bwrcs.eecs.berkeley.edu/Classes/icdesign/ee241_s00/PAPERS/archive/booth51.pdf |archive-date=2018-07-16}} Reprinted in {{cite book |title=A Signed Binary Multiplication Technique |author-first=Andrew Donald |author-last=Booth |author-link=Andrew Donald Booth |publisher=[[Oxford University Press]] |pages=100-104}}</ref>\n<ref name=\"Chen_1992\">{{cite book |title=Signal processing handbook |author-first=Chi-hau |author-last=Chen |publisher=[[CRC Press]] |date=1992 |isbn=978-0-8247-7956-6 |page=234 |url=https://books.google.com/books?id=10Pi0MRbaOYC&pg=PA234}}</ref>\n}}\n\n==Further reading==\n* {{cite journal |author-last=Collin |author-first=Andrew |title=Andrew Booth's Computers at Birkbeck College |journal=Resurrection |issue=5 |date=Spring 1993 |location=London |publisher=[[Computer Conservation Society]] |url=http://www.cs.man.ac.uk./CCS/res/res05.htm#e}}\n* {{cite book |author-last1=Patterson |author-first1=David Andrew |author-link1=David Patterson (computer scientist) |author-first2=John Leroy |author-last2=Hennessy |author-link2=John L. Hennessy |title=Computer Organization and Design: The Hardware/Software Interface |edition=Second |isbn=1-55860-428-6 |location=San Francisco, California, USA |publisher=[[Morgan Kaufmann Publishers]] |date=1998}}\n* {{cite book |author-last=Stallings |author-first=William |author-link=William Stallings |title=Computer Organization and Architecture: Designing for performance |edition=Fifth |isbn=0-13-081294-3 |location=New Jersey |publisher=[[Prentice-Hall, Inc.]] |date=2000}}\n* {{cite web |title=Advanced Arithmetic Techniques |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2006 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0202.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703001722/http://www.quadibloc.com/comp/cp0202.htm |archive-date=2018-07-03}}\n\n==External links==\n* [http://www.geoffknagge.com/fyp/booth.shtml Radix-4 Booth Encoding]\n* [http://www.russinoff.com/libman/text/node65.html Radix-8 Booth Encoding] in [https://web.archive.org/web/20070927194831/http://www.russinoff.com/libman/ A Formal Theory of RTL and Computer Arithmetic]\n* [http://www.ecs.umass.edu/ece/koren/arith/simulator/Booth/ Booth's Algorithm JavaScript Simulator]\n* [http://philosophyforprogrammers.blogspot.com/2011/05/booths-multiplication-algorithm-in.html Implementation in Python]\n\n[[Category:1950 introductions]]\n[[Category:1950 in London]]\n[[Category:1950 in science]]\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic algorithms]]\n[[Category:Multiplication]]\n[[Category:Birkbeck, University of London]]"
    },
    {
      "title": "Byte",
      "url": "https://en.wikipedia.org/wiki/Byte",
      "text": "{{hatnote|This article is about the unit of information. For other uses, see [[Byte (disambiguation)]].}}\n{{Infobox unit\n| name         = byte\n| standard     = [[Units of information#Units derived from bit|units derived from bit]]\n| quantity     = digital information, data size\n| symbol       = B\n| symbol2      = (when referring to exactly 8&nbsp;bits) [[octet (computing)|o]]\n}}\n{{anchor|4 bit|6 bit|8 bit|9 bit}}The '''byte''' is a [[units of information|unit of digital information]] that most commonly consists of eight [[bit]]s, representing a [[binary number]]. Historically, the byte was the number of bits used to encode a single [[character (computing)|character]] of text in a computer<ref name=\"Buchholz_1962\"/><ref name=\"Bemer_1959\"/> and for this reason it is the smallest [[address space|addressable]] unit of [[Computer memory|memory]] in many [[computer architecture]]s.\n\nThe size of the byte has historically been hardware dependent and no definitive standards existed that mandated the size – byte-sizes from 1<ref name=\"Buchholz_1956_1\"/> to 48 bits<ref name=\"CDC_1965_3600\"/> are known to have been used in the past.<ref name=\"Rao_1989\"/><ref name=\"Tafel_1971\"/> Early character encoding systems often used [[Six-bit character code|six bits]], and machines using six-bit and nine-bit bytes were common into the 1960s. These machines most commonly had [[Word (computer architecture)|memory words]] of 12, 24, 36, 48 or 60 bits, corresponding to two, four, six, eight or 10 six-bit bytes. In this era, bytes in the instruction stream were often referred to as ''[[Syllable (computing)|syllables]]'', before the term byte became common.\n\nThe modern [[de facto standard|''de facto'' standard]] of eight bits, as documented in ISO/IEC 2382-1:1993, is a convenient [[power of two]] permitting the values 0 through 255 for one byte (2 to the power 8 is 256, where zero signifies a number as well).<ref name=\"ISO_IEC_2382-1_1993\"/> The international standard [[IEC 80000-13]] codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers optimize for this common usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the eight-bit size.<ref name=\"CHM_1964\"/> Modern architectures typically use 32- or 64-bit words, built of four or eight bytes.\n\nThe unit symbol for the byte was designated as the upper-case letter ''B'' by the [[International Electrotechnical Commission]] (IEC) and [[Institute of Electrical and Electronics Engineers]] (IEEE)<ref name=\"MIXF\"/> in contrast to the bit, whose IEEE symbol is a lower-case ''b''. Internationally, the unit ''[[Octet (computing)|octet]]'', symbol ''o'', explicitly denotes a sequence of eight bits, eliminating the ambiguity of the byte.<ref name=\"TCPIP\"/><ref name=\"ISO_2382-4\"/>\n\n==History==\nThe term ''byte'' was coined by [[Werner Buchholz]] in June 1956,<ref name=\"Buchholz_1956_1\"/><ref name=\"Buchholz_1977\"/><ref name=\"Timeline_1956\"/>{{efn|{{anchor|Note-Dates}}Many sources erroneously indicate a birthday of the term ''byte'' in July 1956, but [[Werner Buchholz]] claimed that the term would have been coined in [[#Buchholz-1977|June 1956]]. In fact, the [[#Buchholz-1956-1|earliest document]] supporting this dates from 1956-06-11. Buchholz stated that the transition to 8-bit bytes was conceived in [[#Buchholz-1977|August 1956]], but the earliest document found using this notion dates from [[#Buchholz-1956-3|September 1956]].}} during the early design phase for the [[IBM 7030|IBM Stretch]]<ref name=\"Buchholz_1956_2\"/><ref name=\"Buchholz_1956_3\"/><ref name=\"Buchholz_1962\"/><ref name=\"Buchholz_1977\"/><ref name=\"Timeline_1956\"/><ref name=\"ESR\"/><ref name=\"Bemer_2000\"/> computer, which had addressing to the bit and [[variable field length]] (VFL) instructions with a byte size encoded in the instruction.<ref name=\"Buchholz_1977\"/>\nIt is a deliberate respelling of ''[[bite]]'' to avoid accidental mutation to ''bit''.<ref name=\"Buchholz_1962\"/><ref name=\"Buchholz_1977\"/><ref name=\"Blaauw_1959\"/>\n\nAnother origin of ''byte'' for bit groups smaller than a machine's word size (and in particular groups of [[Nibble (computing)|four bits]]) is on record by Louis G. Dooley, who claimed he coined the term while working with [[Jules Schwartz]] and Dick Beeler on an air defense system called [[Experimental SAGE Subsector|SAGE]] at [[MIT Lincoln Laboratory]] in ca. 1956/1957, which was jointly developed by [[Rand Corporation|Rand]], MIT, and IBM.<ref name=\"Dooley_1995_Byte\"/><ref name=\"Ram_Byte\"/> Later on, Schwartz's language [[JOVIAL]] actually used the term, but he recalled vaguely that it was derived from [[AN/FSQ-31]].<ref name=\"Schwartz_Brooks_ACM\"/><ref name=\"Ram_Byte\"/>\n\nEarly computers used a variety of four-bit [[binary-coded decimal]] (BCD) representations and the [[Sixbit|six-bit]] codes for printable graphic patterns common in the [[U.S. Army]] ([[FIELDATA]]) and [[United States Navy|Navy]]. These representations included alphanumeric characters and special graphical symbols. These sets were expanded in 1963 to seven bits of coding, called the [[American Standard Code for Information Interchange]] (ASCII) as the [[Federal Information Processing Standard]], which replaced the incompatible teleprinter codes in use by different branches of the U.S. government and universities during the 1960s. ASCII included the distinction of upper- and lowercase alphabets and a set of [[control character]]s to facilitate the transmission of written language as well as printing device functions, such as page advance and line feed, and the physical or logical control of data flow over the transmission media.<ref name=\"Bemer_2000\"/> During the early 1960s, while also active in ASCII standardization, IBM simultaneously introduced in its product line of [[System/360]] the eight-bit [[Extended Binary Coded Decimal Interchange Code]] (EBCDIC), an expansion of their [[six-bit binary-coded decimal]] (BCDIC) representations{{efn|There was more than one BCD code page.}} used in earlier card punches.<ref name=\"ibmebcdic\"/>\nThe prominence of the System/360 led to the ubiquitous adoption of the eight-bit storage size,<ref name=\"Bemer_2000\"/><ref name=\"Buchholz_1956_3\"/><ref name=\"Buchholz_1977\"/> while in detail the EBCDIC and ASCII encoding schemes are different.\n\nIn the early 1960s, [[AT&T]] introduced [[digital telephony]] first on long-distance [[trunk line]]s. These used the eight-bit [[µ-law algorithm|µ-law encoding]]. This large investment promised to reduce transmission costs for eight-bit data.\n\nThe development of [[eight-bit]] [[microprocessor]]s in the 1970s popularized this storage size. Microprocessors such as the [[Intel 8008]], the direct predecessor of the [[Intel 8080|8080]] and the [[Intel 8086|8086]], used in early personal computers, could also perform a small number of operations on the [[4bit|four-bit]] pairs in a byte, such as the decimal-add-adjust (DAA) instruction. A four-bit quantity is often called a [[nibble]], also ''nybble'', which is conveniently represented by a single [[hexadecimal]] digit.\n\nThe term ''[[Octet (computing)|octet]]'' is used to unambiguously specify a size of eight bits.<ref name=\"Bemer_2000\"/><ref name=\"ISO_2382-4\"/> It is used extensively in [[Protocol (computing)|protocol]] definitions.\n\nHistorically, the term ''octad'' or ''octade'' was used to denote eight bits as well at least in Western Europe;<ref name=\"Williams_1969\"/><ref name=\"Philips_1971\"/> however, this usage is no longer common. The exact origin of the term is unclear, but it can be found in British, Dutch, and German sources of the 1960s and 1970s, and throughout the documentation of [[Philips]] mainframe computers.\n\n==Unit symbol==\n{{Bit and byte prefixes}}\nThe unit symbol for the byte is specified in [[IEC 80000-13]], [[IEEE 1541]] and the Metric Interchange Format<ref name=\"MIXF\"/> as the upper-case character ''B''. In contrast, IEEE 1541 specifies the lower case character ''b'' as the symbol for the [[bit]], but IEC 80000-13 and Metric-Interchange-Format specify the symbol as ''bit'', providing disambiguation from B for byte.\n\nIn the [[International System of Quantities]] (ISQ), B is the symbol of the ''[[bel (acoustics)|bel]]'', a unit of logarithmic power ratios named after [[Alexander Graham Bell]], creating a conflict with the IEC specification. However, little danger of confusion exists, because the bel is a rarely used unit. It is used primarily in its decadic fraction, the [[decibel]] (dB), for [[signal strength]] and [[sound pressure level]] measurements, while a unit for one tenth of a byte, the decibyte, and other fractions, are only used in derived units, such as transmission rates.\n\nThe lowercase letter o for [[Octet (computing)|octet]] is defined as the symbol for octet in IEC 80000-13 and is commonly used in languages such as [[French language|French]]<ref name=\"IEC_Binary\"/> and [[Romanian language|Romanian]], and is also combined with metric prefixes for multiples, for example ko and Mo.\n\nThe usage of the term ''octad(e)'' for eight bits is no longer common.<ref name=\"Williams_1969\"/><ref name=\"Philips_1971\"/>\n\n==Unit multiples==\n[[File:Binaryvdecimal.svg|thumb|right|275px|Percentage difference between decimal and binary interpretations of the unit prefixes grows with increasing storage size]]\n\nDespite standardization efforts, ambiguity still exists in the meanings of the [[SI prefix|SI (or metric) prefixes]] used with the unit byte, especially concerning the prefixes ''kilo'' (k or K), ''mega'' (M), and ''giga'' (G). Computer memory has a binary architecture in which multiples are expressed in [[power of two|powers of 2]]. In some fields of the software and computer hardware industries a [[binary prefix]] is used for bytes and bits, while producers of computer storage devices practice adherence to decimal SI multiples. For example, a computer disk drive capacity of 100&nbsp;gigabytes is specified when the disk contains 100&nbsp;billion bytes (93&nbsp;gibibytes) of storage space.\n\nWhile the numerical difference between the decimal and binary interpretations is relatively small for the prefixes [[Kilo-|kilo]] and [[Mega-|mega]], it grows to over 20% for prefix [[yotta]]. The linear–log graph illustrates the difference versus storage size up to an [[exa]]byte.\n\n==Common uses==\nMany [[programming language]]s defined the [[data type]] ''byte''.\n\nThe [[C (programming language)|C]] and [[C++]] programming languages define ''byte'' as an \"''addressable unit of data storage large enough to hold any member of the basic character set of the execution environment''\" (clause 3.6 of the C standard). The C standard requires that the integral data type ''unsigned char'' must hold at least 256 different values, and is represented by at least eight bits (clause 5.2.4.2.1). Various implementations of C and C++ reserve 8, 9, 16, 32, or 36 bits for the storage of a byte.<ref name=\"Cline_Bytes\"/><ref name=\"Klein_2008\"/>{{efn|The actual number of bits in a particular implementation is documented as <code>CHAR_BIT</code> as implemented in the file [[limits.h]].}} In addition, the C and C++ standards require that there are no \"gaps\" between two bytes. This means every bit in memory is part of a byte.<ref name=\"Cline_FAQ\"/>\n\n[[Java (programming language)|Java's]] primitive <code>byte</code> data type is always defined as consisting of 8 bits and being a signed data type, holding values from −128 to 127.\n\n.NET programming languages, such as C#, define both an unsigned <code>byte</code> and a signed <code>sbyte</code>, holding values from 0 to 255, and −128 to 127, respectively.\n\nIn data transmission systems, the byte is defined as a contiguous sequence of bits in a serial data stream representing the smallest distinguished unit of data. A transmission unit might include start bits, stop bits, or [[parity bit]]s, and thus could vary from 7 to 12 bits to contain a single 7-bit [[ASCII]] code.<ref name=\"NWU\"/>\n\n==See also==\n* [[Data]]\n* [[Data hierarchy]]\n* [[JBOB]], Just a Bunch Of Bytes\n* [[Nibble]]\n* [[Primitive data type]]\n* [[Tryte]]\n* [[Qubyte]] (quantum byte)\n* [[Word (computer architecture)]]\n* [[Octet (computing)]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist|refs=\n<ref name=\"Buchholz_1956_1\">{{anchor|Buchholz-1956-1}}{{cite book |title=The Link System |chapter=7. The Shift Matrix |author-first=Werner |author-last=Buchholz |author-link=Werner Buchholz |date=1956-06-11 |id=[[IBM Stretch|Stretch]] Memo No. 39G |publisher=[[IBM]] |pages=5–6 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-07/102632284.pdf |access-date=2016-04-04 |dead-url=yes |archive-url=https://web.archive.org/web/20170404152534/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-07/102632284.pdf |archive-date=2017-04-04 |quote=[…] Most important, from the point of view of editing, will be the ability to handle any characters or digits, from 1 to 6 bits long.<br />Figure 2 shows the Shift Matrix to be used to convert a 60-bit [[word (computer architecture)|word]], coming from Memory in parallel, into [[character (computing)|characters]], or 'bytes' as we have called them, to be sent to the [[serial adder|Adder]] serially. The 60 bits are dumped into [[magnetic core]]s on six different levels. Thus, if a 1 comes out of position 9, it appears in all six cores underneath. Pulsing any diagonal line will send the six bits stored along that line to the Adder. The Adder may accept all or only some of the bits.<br />Assume that it is desired to operate on 4 bit [[decimal digit]]s, starting at the right. The 0-diagonal is pulsed first, sending out the six bits 0 to 5, of which the Adder accepts only the first four (0–3). Bits 4 and 5 are ignored. Next, the 4 diagonal is pulsed. This sends out bits 4 to 9, of which the last two are again ignored, and so on.<br />It is just as easy to use all six bits in [[alphanumeric]] work, or to handle bytes of only one bit for logical analysis, or to offset the bytes by any number of bits. All this can be done by pulling the appropriate shift diagonals. An analogous matrix arrangement is used to change from serial to parallel operation at the output of the adder. […] }}</ref>\n<ref name=\"Buchholz_1956_2\">{{cite book |title=Memory Word Length |chapter=5. Input-Output |author-first=Werner |author-last=Buchholz |author-link=Werner Buchholz |date=1956-07-31 |id=[[IBM Stretch|Stretch]] Memo No. 40 |publisher=[[IBM]] |page=2 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632289.pdf |access-date=2016-04-04 |dead-url=yes |archive-url=https://web.archive.org/web/20170404160423/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632289.pdf |archive-date=2017-04-04 |quote=[…] 60 is a multiple of 1, 2, 3, 4, 5, and 6. Hence bytes of length from 1 to 6 bits can be packed efficiently into a 60-bit [[word (computer architecture)|word]] without having to split a byte between one word and the next. If longer bytes were needed, 60 bits would, of course, no longer be ideal. With present applications, 1, 4, and 6 bits are the really important cases.<br />With 64-bit words, it would often be necessary to make some compromises, such as leaving 4 bits unused in a word when dealing with 6-bit bytes at the input and output. However, the LINK Computer can be equipped to edit out these gaps and to permit handling of bytes which are split between words. […] }}</ref>\n<ref name=\"Buchholz_1956_3\">{{anchor|Buchholz-1956-3}}{{cite book |title=Memory Word Length and Indexing |chapter=2. Input-Output Byte Size |author-first=Werner |author-last=Buchholz |author-link=Werner Buchholz |date=1956-09-19 |id=[[IBM Stretch|Stretch]] Memo No. 45 |publisher=[[IBM]] |page=1 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632292.pdf |access-date=2016-04-04 |dead-url=yes |archive-url=https://web.archive.org/web/20170404161611/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632292.pdf |archive-date=2017-04-04 |quote=[…] The maximum input-output byte size for serial operation will now be 8 bits, not counting any error detection and correction bits. Thus, the Exchange will operate on an 8-bit byte basis, and any input-output units with less than 8 bits per byte will leave the remaining bits blank. The resultant gaps can be edited out later by programming […] }}</ref>\n<ref name=\"Buchholz_1962\">{{anchor|Buchholz-1962}}{{cite |title=Planning a Computer System – Project Stretch |author-first1=Gerrit Anne |author-last1=Blaauw |author-link1=Gerrit Anne Blaauw |author-first2=Frederick Phillips |author-last2=Brooks, Jr. |author-link2=Frederick Phillips Brooks, Jr. |author-first3=Werner |author-last3=Buchholz |author-link3=Werner Buchholz |editor-first=Werner |editor-last=Buchholz |editor-link=Werner Buchholz |publisher=[[McGraw-Hill Book Company, Inc.]] / The Maple Press Company, York, PA. |lccn=61-10466 |year=1962 |chapter=4: Natural Data Units |format=PDF |pages=39–40 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/Buchholz_102636426.pdf |access-date=2017-04-03 |dead-url=yes |archive-url=https://web.archive.org/web/20170403014651/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/Buchholz_102636426.pdf |archive-date=2017-04-03 |quote=Terms used here to describe the structure imposed by the machine design, in addition to ''[[bit]]'', are listed below.<br />''Byte'' denotes a group of bits used to encode a character, or the number of bits transmitted in parallel to and from input-output units. A term other than ''[[character (computing)|character]]'' is used here because a given character may be represented in different applications by more than one code, and different codes may use different numbers of bits (i.e., different byte sizes). In input-output transmission the grouping of bits may be completely arbitrary and have no relation to actual characters. (The term is coined from ''[[bite]]'', but respelled to avoid accidental mutation to ''bit''.)<br />A ''[[Word (unit)|word]]'' consists of the number of data bits transmitted in parallel from or to memory in one memory cycle. [[Word size]] is thus defined as a structural property of the memory. (The term ''[[catena (unit)|catena]]'' was coined for this purpose by the designers of the [[Groupe Bull|Bull]] {{ill|Bull Gamma 60{{!}}GAMMA 60|fr|Gamma 60}} computer.)<br />''[[Block (data storage)|Block]]'' refers to the number of words transmitted to or from an input-output unit in response to a single input-output instruction. Block size is a structural property of an input-output unit; it may have been fixed by the design or left to be varied by the program. }}</ref>\n<ref name=\"Bemer_1959\">{{cite |author-first=Robert William |author-last=Bemer |author-link=Robert William Bemer |title=A proposal for a generalized card code of 256 characters |journal=[[Communications of the ACM]] |volume=2 |number=9 |pages=19–23 |year=1959 |doi=10.1145/368424.368435}}</ref>\n<ref name=\"CHM_1964\">{{cite web |title=Computer History Museum – Exhibits – Internet History – 1964: Internet History 1962 to 1992 |publisher=[[Computer History Museum]] |date=2017 |orig-year=2015 |url=http://www.computerhistory.org/internet_history/#1964 |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403115211/http://www.computerhistory.org/internethistory/ |archive-date=2017-04-03}}</ref>\n<ref name=\"MIXF\">{{cite web |title=Metric-Interchange-Format |author-first=Aubrey |author-last=Jaffer |author-link=Aubrey Jaffer |date=2011 |orig-year=2008 |url=http://people.csail.mit.edu/jaffer/MIXF |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403121705/https://people.csail.mit.edu/jaffer/MIXF/ |archive-date=2017-04-03}}</ref>\n<ref name=\"TCPIP\">{{cite web |title=The TCP/IP Guide – Binary Information and Representation: Bits, Bytes, Nibbles, Octets and Characters – Byte versus Octet |version=3.0 |date=2005-09-20 |orig-year=2001 |author-first=Charles M. |author-last=Kozierok |url=http://www.tcpipguide.com/free/t_BinaryInformationandRepresentationBitsBytesNibbles-3.htm |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403122042/http://www.tcpipguide.com/free/t_BinaryInformationandRepresentationBitsBytesNibbles-3.htm |archive-date=2017-04-03}}</ref>\n<ref name=\"Timeline_1956\">{{cite web |title=Timeline of the IBM Stretch/Harvest era (1956–1961) |publisher=[[Computer History Museum]] |date=June 1956 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/102636400.txt |access-date=2017-04-03 |dead-url=yes |archive-url=https://web.archive.org/web/20160429212717/http://archive.computerhistory.org/resources/text/IBM/Stretch/102636400.txt |archive-date=2016-04-29 |quote=1956 Summer: [[Gerrit Blaauw]], [[Fred Brooks]], [[Werner Buchholz]], [[John Cocke]] and Jim Pomerene join the [[IBM Stretch|Stretch]] team. Lloyd Hunter provides [[transistor]] leadership.<br />{{sic|1956 July|expected=1956 June}}: In a report Werner Buchholz lists the advantages of a 64-bit word length for Stretch. It also supports [[NSA]]'s requirement for 8-bit bytes. Werner's term \"Byte\" first popularized in this memo. }} (NB. This timeline erroneously specifies the birth date of the term \"byte\" as ''[[#Note-Dates|July 1956]]'', while Buchholz actually used the term as early as ''[[#Buchholz-1956-1|June 1956]]''.)</ref>\n<ref name=\"ESR\">{{cite web |title=byte definition |author-first=Eric Steven |author-last=Raymond |author-link=Eric Steven Raymond |date=2017 |orig-year=2003 |url=http://catb.org/~esr/jargon/html/B/byte.html |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403120304/http://catb.org/~esr/jargon/html/B/byte.html |archive-date=2017-04-03}}</ref>\n<ref name=\"Blaauw_1959\">{{anchor|Blaauw-1959}}{{cite journal |title=Processing Data in Bits and Pieces |author-first1=Gerrit Anne |author-last1=Blaauw |author-link1=Gerrit Anne Blaauw |author-first2=Frederick Phillips |author-last2=Brooks, Jr. |author-link2=Frederick Phillips Brooks, Jr. |author-first3=Werner |author-last3=Buchholz |author-link3=Werner Buchholz |journal=[[IRE Transactions on Electronic Computers]] |date=June 1959 |page=121}}</ref>\n<ref name=\"Buchholz_1977\">{{anchor|Buchholz-1977}}{{cite journal |author-last=Buchholz |author-first=Werner |author-link=Werner Buchholz |title=The Word 'Byte' Comes of Age... |journal=[[Byte Magazine]] |date=February 1977 |volume=2 |issue=2 |page=144 |url=https://archive.org/stream/byte-magazine-1977-02/1977_02_BYTE_02-02_Usable_Systems#page/n145/mode/2up |quote=<!-- The Word \"Byte\" Comes of Age...<br />''We received the following from W Buchholz, one of the individuals who was working on IBM's Project Stretch in the mid 1950s. His letter tells the story.''<br />Not being a regular reader of your magazine, I heard about the question in the November 1976 issue regarding the origin of the term \"byte\" from a colleague who knew that I had prepetrated this piece of jargon ''[see page 77 of November 1976 BYTE, \"Olde Englishe\"]''. I searched my files and could not locate a birth certificate. But I am sure that \"byte\" is coming of age in 1977 with its 21st birthday. Many have assumed that byte, meaning 8 bits, originated with the IBM System/360, which spread such bytes far and wide in the mid-1960s. The editor is correct in pointing out that the term goes back to the earlier Stretch computer (but incorrect in that Stretch was the first, not the last, of IBM's second-generation transistorized computers to be developed). -->[…] The first reference found in the files was contained in an internal memo written in June 1956 during the early days of developing [[IBM Stretch|Stretch]]. A byte was described as consisting of any number of parallel bits from one to six. Thus a byte was assumed to have a length appropriate for the occasion. Its first use was in the context of the input-output equipment of the 1950s, which handled six bits at a time. The possibility of going to 8 bit bytes was considered in [[#Note-Dates|August 1956]] and incorporated in the design of Stretch [[#Buchholz-1956-3|shortly thereafter]]. The first published reference to the term occurred in 1959 in a paper '[[#Blaauw-1959|Processing Data in Bits and Pieces]]' by [[Gerrit Anne Blaauw|G&nbsp;A&nbsp;Blaauw]], [[Frederick Phillips Brooks, Jr.|F&nbsp;P&nbsp;Brooks&nbsp;Jr]] and [[Werner Buchholz|W&nbsp;Buchholz]] in the ''[[IRE Transactions on Electronic Computers]]'', June 1959, page 121. The notions of that paper were elaborated in Chapter 4 of ''[[#Buchholz-1962|Planning a Computer System (Project Stretch)]]'', edited by W&nbsp;Buchholz, [[McGraw-Hill Book Company]] (1962). The rationale for coining the term was explained there on page 40 as follows:<br />Byte ''denotes a group of bits used to encode a character, or the number of bits transmitted in parallel to and from input-output units. A term other than ''character'' is used here because a given character may be represented in different applications by more than one code, and different codes may use different numbers of bits (ie, different byte sizes). In input-output transmission the grouping of bits may be completely arbitrary and have no relation to actual characters. (The term is coined from ''[[bite]]'', but respelled to avoid accidental mutation to ''bit''.)''<br />[[System/360]] took over many of the Stretch concepts, including the basic byte and word sizes, which are powers of 2. For economy, however, the byte size was fixed at the 8 bit maximum, and addressing at the bit level was replaced by byte addressing. […]<!-- Since then the term byte has generally meant 8 bits, and it has thus passed into the general vocabulary. Are there any other terms coined especially for the computer field which have found their way into general dictionaries of English language?<br />W. Buchholz<br />24 Edge Hill Rd<br />Wappingers Fall NY 12590 -->}}</ref>\n<ref name=\"Bemer_2000\">{{cite web |title=Why is a byte 8 bits? Or is it? |author-first=Robert William |author-last=Bemer |author-link=Robert William Bemer |date=2000-08-08 |work=Computer History Vignettes |url=http://www.bobbemer.com/BYTE.HTM |access-date=2017-04-03 |dead-url=yes |archive-url=https://web.archive.org/web/20170403130829/http://www.bobbemer.com/BYTE.HTM# |archive-date=2017-04-03 |quote=[…] I came to work for [[IBM]], and saw all the confusion caused by the 64-character limitation. Especially when we started to think about word processing, which would require both upper and lower case. […]<!-- Add 26 lower case letters to 47 existing, and one got 73 -- 9 more than 6 bits could represent. --> I even made a proposal (in view of [[IBM Stretch|STRETCH]], the very first computer I know of with an 8-bit byte) that would extend the number of [[punch card]] character codes to 256 […].<!-- [1]. Some folks took it seriously. I thought of it as a spoof. --> So some folks started thinking about 7-bit characters, but this was ridiculous. With IBM's STRETCH computer as background, handling 64-character words divisible into groups of 8 (I designed the character set for it, under the guidance of Dr. [[Werner Buchholz]], the man who DID coin the term 'byte' for an 8-bit grouping). […]<!-- [2] --> It seemed reasonable to make a universal 8-bit character set, handling up to 256. In those days my mantra was 'powers of 2 are magic'. And so the group I headed developed and justified such a proposal […]<!-- [3]. That was a little too much progress when presented to the standards group that was to formalize ASCII, so they stopped short for the moment with a 7-bit set, or else an 8-bit set with the upper half left for future work. --> The [[IBM System 360|IBM 360]] used 8-bit characters, although not ASCII directly. Thus Buchholz's 'byte' caught on everywhere. I myself did not like the name for many reasons. The design had 8 bits moving around in parallel. But then came a new IBM part, with 9 bits for self-checking, both inside the CPU and in the [[tape drive]]s. I exposed this 9-bit byte to the press in 1973. But long before that, when I headed software operations for [[Cie. Bull]] in France in 1965–66, I insisted that 'byte' be deprecated in favor of '[[octet (computing)|octet]]'. […]<!-- You can notice that my preference then is now the preferred term. --> It is justified by new communications methods that can carry 16, 32, 64, and even 128 bits in parallel. But some foolish people now refer to a '16-bit byte' because of this parallel transfer, which is visible in the [[UNICODE]] set. I'm not sure, but maybe this should be called a '[[hextet]]'. […]<!-- But you will notice that I am still correct. Powers of 2 are still magic! --> |df= }}</ref>\n<ref name=\"ibmebcdic\">{{cite web |title=IBM confirms the use of EBCDIC in their mainframes as a default practice |year=2008 |publisher=[[IBM]] |url=http://publib.boulder.ibm.com/infocenter/zos/v1r9/index.jsp?topic=/com.ibm.zos.r9.adms700/adms7a05158.htm |access-date=2008-06-16 }} {{dead link |date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes}}</ref>\n<ref name=\"IEC_Binary\">{{cite web |url=http://www.iec.ch/si/binary.htm |title=When is a kilobyte a kibibyte? And an MB an MiB? |work=The International System of Units and the IEC |publisher=[[International Electrotechnical Commission]] |access-date=2010-08-30}})</ref>\n<ref name=\"Williams_1969\">{{cite web |title=British Commercial Computer Digest: Pergamon Computer Data Series |author-first=R. H. |author-last=Williams |publisher=[[Pergamon Press]] |date=1969-01-01 |isbn=1483122107|id=978-1483122106 |url=https://www.amazon.de/British-Commercial-Computer-Digest-Pergamon/dp/1483122107 |access-date=2015-08-03}}</ref>\n<ref name=\"Philips_1971\">{{cite web |title=Philips – Philips Data Systems' product range – April 1971 |publisher=[[Philips]] |date=April 1971 |url=http://www.intact-reunies.nl/pdf/product1971.pdf |access-date=2015-08-03 |dead-url=yes |archive-url=https://web.archive.org/web/20160304072023/http://www.intact-reunies.nl/pdf/product1971.pdf |archive-date=2016-03-04}}</ref>\n<ref name=\"Cline_Bytes\">{{cite web |author-first=Marshall |author-last=Cline |url=https://isocpp.org/wiki/faq/intrinsic-types#very-large-bytes |title=I could imagine a machine with 9-bit bytes. But surely not 16-bit bytes or 32-bit bytes, right?}}</ref>\n<ref name=\"Klein_2008\">{{Citation |author-last=Klein |author-first=Jack |year=2008 |title=Integer Types in C and C++ |url=http://home.att.net/~jackklein/c/inttypes.html#char |archive-url=https://web.archive.org/web/20100327225121/http://home.att.net/~jackklein/c/inttypes.html#char |archive-date=2010-03-27 |access-date=2015-06-18}}</ref>\n<ref name=\"Cline_FAQ\">{{cite web |author-first=Marshall |author-last=Cline |url=https://isocpp.org/wiki/faq/intrinsic-types#bytes-review |title=C++ FAQ: the rules about bytes, chars, and characters}}</ref>\n<ref name=\"NWU\">{{cite web |publisher=Northwestern University |url=http://www.ece.northwestern.edu/local-apps/matlabhelp/techdoc/matlab_external/ch_seri8.html |title=External Interfaces/API}}</ref>\n<ref name=\"CDC_1965_3600\">{{cite book |title=3600 Computer System – Reference Manual |date=1966-10-11 |orig-year=1965 |version=K |publisher=[[Control Data Corporation]] (CDC) |location=St. Paul, Minnesota, USA |id=60021300 |url=http://bitsavers.org/pdf/cdc/3x00/48bit/60021300K_3600_SysRef_Oct66.pdf |access-date=2017-04-05 |dead-url=yes |archive-url=https://web.archive.org/web/20170405154001/http://bitsavers.informatik.uni-stuttgart.de/pdf/cdc/3x00/48bit/60021300K_3600_SysRef_Oct66.pdf# |archive-date=2017-04-05 |quote=Byte – A partition of a computer word. |df= }} (NB. Discusses 12-bit, 24-bit and 48-bit bytes.)</ref>\n<ref name=\"Dooley_1995_Byte\">{{cite journal |title=Byte: The Word |author-first=Louis G. |author-last=Dooley |date=February 1995 |journal=[[BYTE]] |location=Ocala, FL, USA |url=http://www.byte.com/art/9502/sec2/art12.htm |dead-url=yes |archive-url=https://web.archive.org/web/19961220122258/http://www.byte.com/art/9502/sec2/art12.htm |archive-date=1996-12-20 |quote=<!-- I would like to get the following on record: -->[…] The word byte was coined around 1956 to 1957 at [[MIT Lincoln Laboratory|MIT Lincoln Laboratories]] within a project called [[Experimental SAGE Subsector|SAGE]] (the North American Air Defense System), which was jointly developed by [[Rand Corporation|Rand]], Lincoln Labs, and [[IBM]]. In that era, computer memory structure was already defined in terms of [[word size (computing)|word size]]. A word consisted of x number of [[bit]]s; a bit represented a binary notational position in a word. Operations typically operated on all the bits in the full word.<br />We coined the word byte to refer to a logical set of bits less than a full word size. At that time, it was not defined specifically as x bits but typically referred to as a set of [[Nibble (computing)|4 bits]], as that was the size of most of our coded data items. Shortly afterward, I went on to other responsibilities that removed me from SAGE. After having spent many years in Asia, I returned to the U.S. and was bemused to find out that the word byte was being used in the new microcomputer technology to refer to the basic addressable memory unit.}} (NB. According to his son, Dooley wrote to him:<!-- See: https://en.wikipedia.org/wiki/Talk:Byte#Not_.22to_bite.22_but_.22a_bite.22 --> \"On good days, we would have the [[IBM XD-1|XD-1]] up and running and all the programs doing the right thing, and we then had some time to just sit and talk idly, as we waited for the computer to finish doing its thing. On one such occasion, I coined the word \"byte\", they ([[Jules Schwartz]] and Dick Beeler) liked it, and we began using it amongst ourselves. The origin of the word was a need for referencing only a part of the word length of the computer, but a part larger than just one bit...Many programs had to access just a specific [[Nibble (computing)|4-bit]] segment of the full word...I wanted a name for this smaller segment of the fuller word. The word \"[[bit]]\" lead to \"[[bite]]\" (meaningfully less than the whole), but for a unique spelling, \"i\" could be \"y\", and thus the word \"byte\" was born.\")</ref>\n<ref name=\"Schwartz_Brooks_ACM\">{{cite |title=Origin of the term \"byte\", 1956 |url=http://www.xent.com/FoRK-archive/july99/0646.html |access-date=2017-04-10 |dead-url=no |archive-url=https://web.archive.org/web/20170410125522/http://www.xent.com/FoRK-archive/july99/0646.html |archive-date=2017-04-10 |quote=A question-and-answer session at an [[ACM conference]] on the history of programming languages included this exchange:<br />[[John Goodenough|JOHN GOODENOUGH]]: You mentioned that the term \"byte\" is used in [[JOVIAL]]. Where did the term come from?<br />[[Jules Schwartz|JULES SCHWARTZ]] (inventor of JOVIAL): As I recall, the [[AN/FSQ-31]], a totally different computer than the [[IBM 709|709]], was byte oriented. I don't recall for sure, but I'm reasonably certain the description of that computer included the word \"byte,\" and we used it.<br />[[Fred Brooks|FRED BROOKS]]: May I speak to that? [[Werner Buchholz]] coined the word as part of the definition of [[IBM STRETCH|STRETCH]], and the AN/FSQ-31 picked it up from STRETCH, but Werner is very definitely the author of that word.<br />SCHWARTZ: That's right. Thank you.}}</ref>\n<ref name=\"Ram_Byte\">{{cite web |author-first=Stefan |author-last=Ram |title=Erklärung des Wortes \"Byte\" im Rahmen der Lehre binärer Codes |language=German |publisher=[[Freie Universität Berlin]] |location=Berlin, Germany |url=http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/code_byte_de |access-date=2017-04-10}}</ref>\n<ref name=\"ISO_IEC_2382-1_1993\">{{cite book |title=ISO/IEC 2382-1: 1993, Information technology – Vocabulary – Part 1: Fundamental terms |date=1993 |quote=byte<br />A string that consists of a number of bits, treated as a unit, and usually representing a character or a part of a character.<br />NOTES<br />1 The number of bits in a byte is fixed for a given data processing system.<br />2 The number of bits in a byte is usually 8.}}</ref>\n<ref name=\"ISO_2382-4\">{{cite book |title=ISO 2382-4, Organization of data |edition=2 |quote=byte, octet, 8-bit byte: A string that consists of eight bits.}}</ref>\n<ref name=\"Rao_1989\">{{cite book |title=Error-Control Coding for Computer Systems |author-first1=Thammavaram R. N. |author-last1=Rao |author-first2=Eiji |author-last2=Fujiwara |editor-first=Edward J. |editor-last=McCluskey |series=Prentice Hall Series in Computer Engineering |date=1989 |lccn=88-17892 |edition=1 |isbn=0-13-283953-9 |publisher=[[Prentice Hall]] |location=Englewood Cliffs, NJ, USA}} (NB. Example of the usage of a code for \"4-bit bytes\".)</ref>\n<ref name=\"Tafel_1971\">{{cite book |title=Einführung in die digitale Datenverarbeitung |language=German |trans-title=Introduction to digital information processing |author-first=Hans Jörg |author-last=Tafel |publisher=[[Carl Hanser Verlag]] |date=1971 |location=[[RWTH]], Aachen, Germany |publication-place=Munich, Germany |isbn=3-446-10569-7 |page=300 |quote=Byte = zusammengehörige Folge von i.a. neun Bits; davon sind acht Datenbits, das neunte ein Prüfbit}} (NB. Defines a byte as a group of typically 9 bits; 8 data bits plus 1 parity bit.)</ref>\n}}\n\n==Further reading==\n* {{cite book |title=Programming with the PDP-10 Instruction Set |series=PDP-10 System Reference Manual |date=August 1969 |volume=1 |publisher=[[Digital Equipment Corporation]] (DEC) |url=http://bitsavers.org/pdf/dec/pdp10/1970_PDP-10_Ref/1970PDP10Ref_Part1.pdf |access-date=2017-04-05 |dead-url=no |archive-url=https://web.archive.org/web/20170405154620/http://bitsavers.informatik.uni-stuttgart.de/pdf/dec/pdp10/1970_PDP-10_Ref/1970PDP10Ref_Part1.pdf |archive-date=2017-04-05}}\n\n{{Computer Storage Volumes}}\n{{Data types}}\n\n[[Category:Data types]]\n[[Category:Units of information]]\n[[Category:Binary arithmetic]]\n[[Category:Computer memory]]\n[[Category:Data unit]]\n[[Category:Primitive types]]\n[[Category:Words coined in the 1950s]]\n[[Category:8 (number)]]"
    },
    {
      "title": "Carry-less product",
      "url": "https://en.wikipedia.org/wiki/Carry-less_product",
      "text": "{{refimprove|date=April 2017}}\n\nThe '''carry-less product''' of two [[binary number]]s\nis the result of '''carry-less multiplication''' of these numbers.\nThis operation conceptually works like [[long multiplication]]\nexcept for the fact that the [[carry (arithmetic)|carry]]\nis discarded instead of applied to the more significant position.\nIt can be used to model operations over [[finite field]]s,\nin particular multiplication of polynomials from GF(2)[''X''],\nthe [[polynomial ring]] over [[GF(2)]].\n\n== Definition ==\n\nGiven two numbers <math>\\textstyle a=\\sum_i a_i2^i</math> and <math>\\textstyle b=\\sum_i b_i2^i</math>,\nwith <math>a_i,b_i\\in\\{0,1\\}</math> denoting the bits of these numbers.\nThen the carry-less product of these two numbers is defined to be\n<math>\\textstyle c=\\sum_i c_i2^i</math>, with each bit <math>c_i</math> computed\nas the [[exclusive or]] of products of bits from the input numbers as follows:<ref>{{cite web|url=http://software.intel.com/en-us/articles/intel-carry-less-multiplication-instruction-and-its-usage-for-computing-the-gcm-mode/|title=Intel Carry-Less Multiplication Instruction and its Usage for Computing the GCM Mode - Rev 2|publisher=[[Intel]]|author=Shay Gueron|date=2011-04-13}}</ref>\n\n: <math>c_i=\\bigoplus_{j=0}^i a_jb_{i-j}</math>\n\n== Example ==\n\nConsider ''a'' = 10100010<sub>2</sub> and ''b'' = 10010110<sub>2</sub>,\nwith all numbers given in binary.\nThen the carry-less multiplication of these is essentially what one would get\nfrom performing a long multiplication but ignoring the carries.\n\n                   1 0 1 0 0 0 1 0 = a\n    ---------------|---|-------|--\n    1 0 0 1 0 1 1 0|0 0 0 0 0 0 0\n        1 0 0 1 0 1 1 0|0 0 0 0 0\n                1 0 0 1 0 1 1 0|0\n    ------------------------------\n    1 0 1 1 0 0 0 1 1 1 0 1 1 0 0\n              ^ ^\n\nSo the carry-less product of ''a'' and ''b'' would be ''c'' = 101100011101100<sub>2</sub>.\nFor every bit set in the number ''a'', the number ''b'' is shifted to the left\nas many bits as indicated by the position of the bit in ''a''.\nAll these shifted versions are then combined using an exclusive or,\ninstead of the regular addition which would be used for regular long multiplication.\nThis can be seen in the columns indicated by <code>^</code>, where regular addition\nwould cause a carry to the column to the left, which does not happen here.\n\n== Multiplication of polynomials ==\n\nThe carry-less product can also be seen as multiplication of polynomials\nover the field [[GF(2)]].\nThis is because the exclusive or corresponds to the addition in this field.\n\nIn the example above, the numbers ''a'' and ''b'' corresponds to polynomials\n\n: <math>A=\\sum_i a_i X^i=X^7+X^5+X^1\\qquad B=\\sum_i b_i X^i=X^7+X^4+X^2+X^1</math>\n\nand the product of these is\n\n: <math>C=A\\cdot B=\\sum_i c_i X^i=X^{14}+X^{12}+X^{11}+X^7+X^6+X^5+X^3+X^2</math>\n\nwhich is what the number ''c'' computed above encodes.\nNotice how <math>(X^7\\cdot X^1)+(X^1\\cdot X^7)\\equiv0</math> and\n<math>(X^7\\cdot X^2)+(X^5\\cdot X^4)\\equiv0</math> thanks to the\narithmetic in GF(2).\nThis corresponds to the columns marked <code>^</code> in the example.\n\n== Applications ==\n\nThe elements of GF(2<sup>''n''</sup>), i.e. a [[finite field]] whose order is a [[power of two]],\nare usually represented as polynomials in GF(2)[''X''].\n[[Finite field arithmetic#Multiplication|Multiplication]] of two such field elements\nconsists of multiplication of the corresponding polynomials,\nfollowed by a reduction with respect to some irreducible polynomial\nwhich is taken from the construction of the field.\nIf the polynomials are encoded as binary numbers,\ncarry-less multiplication can be used to perform the first step of this computation.\n\nSuch fields have applications in [[cryptography]] and for some [[checksum]] algorithms.\n\n== Implementations ==\n\nSome processors support the [[CLMUL instruction set]] and thus\nprovide a hardware instruction to perform this operation.\n\nFor other targets it is possible to implement the computation above as a software algorithm,\nand many cryptography libraries will contain an implementation\nas part of their finite field arithmetic operations.\n\n== Other bases ==\n\nThe definition of a carry-less product as the result of a long multiplication discarding carry\nwould readily apply to [[Radix|bases]] other than 2.\nBut the result depends on the basis, which is therefore an essential part of the operation.\nAs this operation is typically being used on computers operating in binary,\nthe binary form discussed above is the one employed in practice.\n\nPolynomials over other finite fields of prime order do have applications,\nbut treating the coefficients of such a polynomial as the digits of a single number is rather uncommon,\nso the multiplication of such polynomials would not be seen as a carry-less multiplication of numbers.\n\n== See also ==\n\n* [[CLMUL instruction set]]\n* [[Finite field arithmetic]]\n* [[Galois/Counter Mode]]\n\n== References ==\n\n{{reflist}}\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Binary operations]]\n[[Category:Multiplication]]"
    },
    {
      "title": "Carry-save adder",
      "url": "https://en.wikipedia.org/wiki/Carry-save_adder",
      "text": "{{ALUSidebar|expand=Components|expand-components=Adder}}\nA '''carry-save adder'''<ref name=\"Earle_1965_1\"/><ref name=\"Earle_1965_2\"/><ref group=\"nb\" name=\"NB_CSA\"/> is a type of [[adder (electronics)|digital adder]], used in computer microarchitecture to compute the sum of three or more ''n''-bit numbers in [[binary numeral system|binary]]. It differs from other digital adders in that it outputs two numbers of the same dimensions as the inputs, one which is a sequence of partial sum bits and another which is a sequence of [[carry (arithmetic)|carry]] bits.\n\n==Motivation==\n\nConsider the sum:\n    12345678\n +  87654322\n = 100000000\n\nUsing basic arithmetic, we calculate right to left, \"8 + 2 = 0, carry 1\", \"7 + 2 + 1 = 0, carry 1\", \"6 + 3 + 1 = 0, carry 1\", and so on to the end of the sum. Although we know the last digit of the result at once, we cannot know the first digit until we have gone through every digit in the calculation, passing the carry from each digit to the one on its left. Thus adding two ''n''-digit numbers has to take a time proportional to ''n'', even if the machinery we are using would otherwise be capable of performing many calculations simultaneously.\n\nIn electronic terms, using bits (binary digits), this means that even if we have ''n'' one-bit adders at our disposal, we still have to allow a time proportional to ''n'' to allow a possible carry to propagate from one end of the number to the other. Until we have done this, \n# We do not know the result of the addition.\n# We do not know whether the result of the addition is larger or smaller than a given number (for instance, we do not know whether it is positive or negative).\n\nA [[carry look-ahead adder]] can reduce the delay. In principle the delay can be reduced so that it is proportional to log ''n'', but for large numbers this is no longer the case, because even when carry look-ahead is implemented, the distances that signals have to travel on the chip increase in proportion to ''n'', and propagation delays increase at the same rate. Once we get to the 512-bit to 2048-bit number sizes that are required in [[public-key cryptography]], carry look-ahead is not of much help.\n\n==The basic concept==\nThe idea of delaying carry resolution until the end, or saving carries, is due to [[John von Neumann]].<ref name=\"Neumann\"/>\n\nHere is an example of a binary sum:\n   10111010101011011111000000001101\n + 11011110101011011011111011101111\n\nCarry-save arithmetic works by abandoning the binary notation while still working to base 2. It computes the sum digit by digit, as\n   10111010101011011111000000001101\n + 11011110101011011011111011101111\n = 21122120202022022122111011102212\n\nThe notation is unconventional, but the result is still unambiguous. Moreover, given ''n'' adders (here ''n'' = 32 full adders), the result can be calculated after propagating the inputs through a single adder, since each digit result does not depend on any of the others.\n\nIf the adder is required to add two numbers and produce a result, carry-save addition is useless, since the result still has to be converted back into binary, and this still means that carries have to propagate from right to left. But in large-integer arithmetic, addition is a very rare operation, and adders are mostly used to accumulate partial sums in a multiplication.\n\n==Carry-save accumulators==\n\nSupposing that we have two bits of storage per digit, we can use a [[redundant binary representation]], storing the values 0, 1, 2, or 3 in each digit position. It is therefore obvious that one more binary number can be added to our carry-save result without overflowing our storage capacity: but then what?\n\nThe key to success is that at the moment of each partial addition we add three bits:\n* 0 or 1, from the number we are adding.\n* 0 if the digit in our store is 0 or 2, or 1 if it is 1 or 3.\n* 0 if the digit to its right is 0 or 1, or 1 if it is 2 or 3.\nTo put it another way, we are taking a carry digit from the position on our right, and passing a carry digit to the left, just as in conventional addition; but the carry digit we pass to the left is the result of the ''previous'' calculation and not the current one. In each clock cycle, carries only have to move one step along, and not ''n'' steps as in conventional addition.\n\nBecause signals don't have to move as far, the clock can tick much faster. ..\n\nThere is still a need to convert the result to binary at the end of a calculation, which effectively just means letting the carries travel all the way through the number just as in a conventional adder. But if we have done 512 additions in the process of performing a 512-bit multiplication, the cost of that final conversion is effectively split across those 512 additions, so each addition bears 1/512 of the cost of that final \"conventional\" addition.\n\n==Drawbacks==\n\nAt each stage of a carry-save addition,\n# We know the result of the addition at once.\n# We ''still do not know'' whether the result of the addition is larger or smaller than a given number (for instance, we do not know whether it is positive or negative).\n\nThis latter point is a drawback when using carry-save adders to implement modular multiplication (multiplication followed by division, keeping the remainder only). If we cannot know whether the intermediate result is greater or less than the modulus, how can we know whether to subtract the modulus?\n\n[[Montgomery multiplication]], which depends on the rightmost digit of the result, is one solution; though rather like carry-save addition itself, it carries a fixed overhead, so that a sequence of Montgomery multiplications saves time but a single one does not. Fortunately exponentiation, which is effectively a sequence of multiplications, is the most common operation in public-key cryptography.\n\nCareful error analysis<ref name=\"encrypt\"/> allows a choice to be made about subtracting the modulus even though we don't know for certain whether the result of the addition is big enough to warrant the subtraction. For this to work, it is necessary for the circuit design to be able to add −2, −1, 0, +1 or +2 times the modulus. The advantage over Montgomery multiplication is that there is no fixed overhead attached to each sequence of multiplications.\n\n==Technical details==\n\nThe carry-save unit consists of ''n'' [[adder (electronics)#Full adder|full adders]], each of which computes a single sum and carry bit based solely on the corresponding bits of the three input numbers.  Given the three ''n''-bit numbers '''a''', '''b''', and '''c''', it produces a partial sum '''ps''' and a shift-carry '''sc''':\n\n:<math>ps_i = a_i \\oplus b_i \\oplus c_i,</math>\n:<math>sc_i = (a_i \\wedge b_i) \\vee (a_i \\wedge c_i) \\vee (b_i \\wedge c_i).</math>\n\nThe entire sum can then be computed by:\n# [[Logical shift|Shifting]] the carry sequence '''sc''' left by one place.\n# Appending a 0 to the front ([[most significant bit]]) of the partial sum sequence '''ps'''.\n# Using a [[Adder (electronics)#Ripple-carry adder|ripple carry adder]] to add these two together and produce the resulting (''n'' + 1)-bit value.\n\n==See also==\n* [[Wallace tree]]\n\n==Notes==\n{{reflist|group=\"nb\"|refs=\n<ref group=\"nb\" name=\"NB_CSA\">''Carry-save adder'' is often abbreviated as CSA, however, this can be confused with the [[carry-skip adder]].</ref>\n}}\n\n==References==\n{{Reflist|refs=\n<ref name=\"Earle_1965_1\">{{cite |author-last=Earle |author-first=John G. |id={{US patent|3340388}} |title=Latched Carry Save Adder Circuit for Multipliers |date=1965-07-12}}</ref>\n<ref name=\"Earle_1965_2\">{{Citation |author-last= Earle |author-first=John G. |title=Latched Carry-Save Adder |journal=IBM Technical Disclosure Bulletin |volume=7 |issue=10 |date=March 1965 |pages=909–910}}</ref>\n<ref name=\"Neumann\">{{cite book |author-first=John |author-last=von Neumann |author-link=John von Neumann |title=Collected Works}}</ref>\n<ref name=\"encrypt\">{{cite web |title=A New Method of Serial Modular Multiplication |author-first=Martin |author-last=Kochanski |date=2003-08-19 |url=http://www.nugae.com/encryption/bin/design.pdf |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180716202844/http://www.nugae.com/encryption/bin/design.pdf |archive-date=2018-07-16}}</ref> \n}}\n\n==Further reading==\n* {{cite web |title=Advanced Arithmetic Techniques |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2006 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0202.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703001722/http://www.quadibloc.com/comp/cp0202.htm |archive-date=2018-07-03}}\n\n{{CPU technologies|state=collapsed}}\n\n{{DEFAULTSORT:Carry-Save Adder}}\n[[Category:Binary arithmetic]]\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Chen–Ho encoding",
      "url": "https://en.wikipedia.org/wiki/Chen%E2%80%93Ho_encoding",
      "text": "{{short description|an efficient alternate system of binary encoding for decimal digits}}\n{{Use dmy dates|date=May 2019|cs1-dates=y}}\n'''Chen–Ho encoding''' is a memory-efficient alternate system of [[Binary numeral system|binary]] encoding for [[decimal]] digits.\n\nThe traditional system of binary encoding for decimal digits, known as [[binary-coded decimal]] (BCD), uses four bits to encode each digit, resulting in significant wastage of binary data bandwidth (since four bits can store 16 states and are being used to store only 10).<ref name=\"Muller_2010\"/>\n\nThe encoding reduces the storage requirements of two decimal digits (100 states) from 8 to 7 bits, and those of three decimal digits (1000 states) from 12 to 10 bits using only simple [[Boolean algebra (logic)|Boolean]] transformations avoiding any complex arithmetic operations like a [[base conversion]].\n\n==History==\nIn what appears to have been a [[multiple discovery]], some of the concepts behind what later became known as Chen–Ho encoding were independently developed by [[Theodore M. Hertz]] in 1969<ref name=\"Hertz_1969\"/> and by [[Tien Chi Chen]] in 1971.<ref name=\"Chen_2015\"/>\n\nHertz of [[North American Rockwell Corporation|Rockwell]] filed a patent for his encoding in 1969, which was granted in 1971.<ref name=\"Hertz_1969\"/>\n\nChen first discussed his ideas with [[Irving Tze Ho]]<ref name=\"Tseng_1988\"/> in 1971. Chen and Ho were both working for [[IBM]] at the time, although in different locations.<ref name=\"Chen_1971_1\"/><ref name=\"Chen_1971_2\"/> Chen also consulted with [[Frank C. Tung]]<ref name=\"Tung_2016\"/> to verify the results of his theories independently.<ref name=\"Chen_1971_2\"/> IBM filed a patent in their name in 1973, which was granted in 1974.<ref name=\"Chen_1973_US3842414A\"/> At least by 1973 Hertz's earlier work must have been known to them, as the patent cites his patent as [[prior art]].<ref name=\"Chen_1973_US3842414A\"/>\n\nThe final version of the Chen–Ho encoding was circulated inside IBM in 1974<ref name=\"Chen_Ho_1974\"/> and published in 1975 in the journal ''[[Communications of the Association for Computing Machinery]] (CACM)''.<ref name=\"Chen_Ho_1975\"/><ref name=\"Cowlishaw_2000_CH\"/> This version included several refinements, primarily related to the application of the encoding system. It constitutes a [[Huffmann code|Huffman]]-like [[prefix code]].\n\nThe encoding became known as ''Chen–Ho encoding'' or ''Chen–Ho algorithm'' only since 2000.<ref name=\"Cowlishaw_2000_CH\"/> After having filed a patent in 2001,<ref name=\"Cowlishaw_2001_US6525679B1\"/> [[Michael F. Cowlishaw]] published a further refinement of Chen–Ho encoding known as [[Densely Packed Decimal]] (DPD) encoding in ''IEE Proceedings – Computers and Digital Techniques'' in 2002.<ref name=\"Cowlishaw_2002\"/><ref name=\"Cowlishaw_2007\"/> Densely Packed Decimal has subsequently been adopted as the ''decimal encoding'' used in the [[IEEE 754-2008]] and [[ISO/IEC/IEEE 60559:2011]] [[floating-point]] standards.\n\n==Application==\nChen noted that the digits zero through seven were simply encoded using three binary digits of the corresponding [[octal]] group. He also postulated that one could use a [[flag (computing)|flag]] to identify a different encoding for the digits eight and nine, which would be encoded using a single bit.\n\nIn practice, a series of [[Boolean algebra (logic)|Boolean]] transformations are applied to the stream of input bits, compressing BCD encoded digits from 12 bits per three digits to 10 bits per three digits. Reversed transformations are used to decode the resulting coded stream to BCD. Equivalent results can also be achieved by the use of a [[look-up table]].\n\nChen–Ho encoding is limited to encoding sets of three decimal digits into groups of 10 bits (so called ''[[declet (computing)|declet]]s'').<ref name=\"Muller_2010\"/> Of the 1024 states possible by using 10 bits, it leaves only 24<!-- 3 states (from 2 don't-care bits minus one implicitly used state) for 3 higher value digits each = 3*8 states = 24 --> states unused<ref name=\"Muller_2010\"/> (with [[don't care]] bits typically set to 0 on write and ignored on read). With only 0.34% wastage it gives a 20% more efficient encoding than BCD with one digit in 4 bits.<ref name=\"Chen_1971_2\"/><ref name=\"Cowlishaw_2000_CH\"/>\n\nBoth Hertz and Chen also proposed similar, but less efficient, encoding schemes to compress sets of two decimal digits (requiring 8 bits in BCD) into groups of 7 bits.<ref name=\"Hertz_1969\"/><ref name=\"Chen_1971_2\"/>\n\nLarger sets of decimal digits could by divided into three- and two-digit groups.<ref name=\"Hertz_1969\"/>\n\nThe patents also discuss the possibility to adapt the scheme to digits encoded in any other decimal codes than BCD, like f.e. [[Excess-3]].<ref name=\"Hertz_1969\"/> The same principles could also be applied to other bases.\n\nIn 1973, some form of Chen–Ho encoding appears to have been utilized in the address conversion hardware of the optional [[IBM 7070]]/[[IBM 7074|7074]] emulation feature for the [[IBM System/370 Model 165]] and [[IBM System/370 Model 168|370 Model 168]] computers.<ref name=\"Savard_2007_CH\"/><ref name=\"IBM_1973_CF\"/>\n\nOne prominent application uses a 128-bit register to store 33 decimal digits with a three digit exponent, effectively not less than what could be achieved using binary encoding (whereas BCD encoding would need 144 bits to store the same number of digits).\n\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Storage efficiency\n|-\n! colspan=\"4\"|BCD\n! colspan=\"4\"|Necessary bits\n! colspan=\"2\"|Bit difference\n|-\n!Digits\n!States\n!Bits\n!Binary code space\n!Binary encoding [A]\n!2-digit encoding [B]\n!3-digit encoding [C]\n!Mixed encoding\n!Mixed vs. Binary\n!Mixed vs. BCD\n|-\n| 1 || {{val|10}} || 4 || {{val|16}} || 4 || (7) || (10) || 4 [1×A] || 0 || 0\n|-\n| 2 || {{val|100}} || 8 || {{val|128}} || 7 || 7 || (10) || 7 [1×B] || 0 || −1\n|-\n| 3 || {{val|1000}} || 12 || {{val|1024}} || 10 || (14) || 10 || 10 [1×C] || 0 || −2\n|-\n| 4 || {{val|10000}} || 16 || {{val|16384}} || 14 || 14 || (20) || 14 [2×B] || 0 || −2\n|-\n| 5 || {{val|100000}} || 20 || {{val|131072}} || 17 || (21) || (20) || 17 [1×C+1×B] || 0 || −3\n|-\n| 6 || {{val|1000000}} || 24 || {{val|1048576}} || 20 || 21 || 20 || 20 [2×C] || 0 || −4\n|-\n| 7 || {{val|10000000}} || 28 || {{val|16777216}} || 24 || (28) || (30) || 24 [2×C+1×A] || 0 || −4\n|-\n| 8 || {{val|100000000}} || 32 || {{val|134217728}} || 27 || 28 || (30) || 27 [2×C+1×B] || 0 || −5\n|-\n| 9 || {{val|1000000000}} || 36 || {{val|1073741824}} || 30 || (35) || 30 || 30 [3×C] || 0 || −6\n|-\n| 10 || {{val|10000000000}} || 40 || {{val|17179869184}} || 34 || 35 || (40) || 34 [3×C+1×A] || 0 || −6\n|-\n| 11 || {{val|100000000000}} || 44 || {{val|137438953472}} || 37 || (42) || (40) || 37 [3×C+1×B] || 0 || −7\n|-\n| 12 || {{val|1000000000000}} || 48 || {{val|1099511627776}} || 40 || 42 || 40 || 40 [4×C] || 0 || −8\n|-\n| 13 || {{val|10000000000000}} || 52 || {{val|17592186044416}} || 44 || (49) || (50) || 44 [4×C+1×A] || 0 || −8\n|-\n| 14 || {{val|100000000000000}} || 56 || {{val|140737488355328}} || 47 || 49 || (50) || 47 [4×C+1×B] || 0 || −9\n|-\n| 15 || {{val|1000000000000000}} || 60 || {{val|1125899906842624}} || 50 || (56) || 50 || 50 [5×C] || 0 || −10\n|-\n| 16 || {{val|10000000000000000}} || 64 || {{val|18014398509481984}} || 54 || 56 || (60) || 54 [5×C+1×A] || 0 || −10\n|-\n| 17 || {{val|100000000000000000}} || 68 || {{val|144115188075855872}} || 57 || (63) || (60) || 57 [5×C+1×B] || 0 || −11\n|-\n| 18 || {{val|1000000000000000000}} || 72 || {{val|1152921504606846976}} || 60 || 63 || 60 || 60 [6×C] || 0 || −12\n|-\n| 19 || {{val|10000000000000000000}} || 76 || {{val|18446744073709551616}} || 64 || (70) || (70) || 64 [6×C+1×A] || 0 || −12\n|-\n| 20 || …<!-- {{val|100000000000000000000}} --> || 80 || …<!-- {{val|147573952589676412928}} --> || 67 || 70 || (70) || 67 [6×C+1×B] || 0 || −13\n|-\n| 21 || …<!-- {{val|1000000000000000000000}} --> || 84 || …<!-- {{val|1180591620717411303424}} --> || 70 || (77) || 70 || 70 [7×C] || 0 || −14\n|-\n| 22 || …<!-- {{val|10000000000000000000000}} --> || 88 || …<!-- {{val|18889465931478580854784}} --> || 74 || 77 || (80) || 74 [7×C+1×A] || 0 || −14\n|-\n| 23 || …<!-- {{val|100000000000000000000000}} --> || 92 || …<!-- {{val|151115727451828646838272}} --> || 77 || (84) || (80) || 77 [7×C+1×B] || 0 || −15\n|-\n| 24 || …<!-- {{val|1000000000000000000000000}} --> || 96 || …<!-- {{val|1208925819614629174706176}} --> || 80 || 84 || 80 || 80 [8×C] || 0 || −16\n|-\n| 25 || …<!-- {{val|10000000000000000000000000}} --> || 100 || …<!-- {{val|19342813113834066795298816}} --> || 84 || (91) || (90) || 84 [8×C+1×A] || 0 || −16\n|-\n| 26 || …<!-- {{val|100000000000000000000000000}} --> || 104 || …<!-- {{val|154742504910672534362390528}} --> || 87 || 91 || (90) || 87 [8×C+1×B] || 0 || −17\n|-\n| 27 || …<!-- {{val|1000000000000000000000000000}} --> || 108 || …<!-- {{val|1237940039285380274899124224}} --> || 90 || (98) || 90 || 90 [9×C] || 0 || −18\n|-\n| 28 || …<!-- {{val|10000000000000000000000000000}} --> || 112 || …<!-- {{val|19807040628566084398385987584}} --> || 94 || 98 || (100) || 94 [9×C+1×A] || 0 || −18\n|-\n| 29 || …<!-- {{val|100000000000000000000000000000}} --> || 116 || …<!-- {{val|158456325028528675187087900672}} --> || 97 || (105) || (100) || 97 [9×C+1×B] || 0 || −19\n|-\n| 30 || …<!-- {{val|1000000000000000000000000000000}} --> || 120 || …<!-- {{val|1267650600228229401496703205376}} --> || 100 || 105 || 100 || 100 [10×C] || 0 || −20\n|-\n| 31 || …<!-- {{val|10000000000000000000000000000000}} --> || 124 || …<!-- {{val|10141204801825835211973625643008}} --> || 103 || (112) || (110) || 104 [10×C+1×A] || +1 || −20\n|-\n| 32 || …<!-- {{val|100000000000000000000000000000000}} --> || 128 || …<!-- {{val|1.6225927682921336339157801028813e+32}} --> || 107 || 112 || (110) || 107 [10×C+1×B] || 0 || −21\n|-\n| 33 || …<!-- {{val|1000000000000000000000000000000000}} --> || 132 || …<!-- {{val|1.298074214633706907132624082305e+33}} --> || 110 || (119) || 110 || 110 [11×C] || 0 || −22\n|-\n| 34 || …<!-- {{val|10000000000000000000000000000000000}} --> || 136 || …<!-- {{val|1.038459371706965525706099265844e+34}} --> || 113 || 119 || (120) || 114 [11×C+1×A] || +1 || −22\n|-\n| 35 || …<!-- {{val|100000000000000000000000000000000000}} --> || 140 || …<!-- {{val|1.6615349947311448411297588253504e+35}} --> || 117 || (126) || (120) || 117 [11×C+1×B] || 0 || −23\n|-\n| 36 || …<!-- {{val|1000000000000000000000000000000000000}} --> || 144 || …<!-- {{val|1.3292279957849158729038070602803e+36}} --> || 120 || 126 || 120 || 120 [12×C] || 0 || −24\n|-\n| 37 || …<!-- {{val|10000000000000000000000000000000000000}} --> || 148 || …<!-- {{val|1.0633823966279326983230456482243e+37}} --> || 123 || (133) || (130) || 124 [12×C+1×A] || +1 || −24\n|-\n| 38 || …<!-- {{val|100000000000000000000000000000000000000}} --> || 152 || …<!-- {{val|1.7014118346046923173168730371588e+38}} --> || 127 || 133 || (130) || 127 [12×C+1×B] || 0 || −25\n|-\n| … || … || … || … || … || … || … || … || … || …\n|}\n\n==Encodings for three decimal digits==\n===Hertz encoding===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Hertz decimal data encoding for a single declet (1969 form)<ref name=\"Hertz_1969\"/>\n|-\n!scope=\"col\" colspan=\"11\"| Binary encoding\n|rowspan=\"10\"|\n!scope=\"col\" colspan=\"6\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (1024&nbsp;states<!-- 0..1023 -->) !!scope=\"col\"| b9 !!scope=\"col\"| b8 !!scope=\"col\"| b7 !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!scope=\"col\"| d2 !!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (1000&nbsp;states<!-- 000..999 -->)\n|-\n|50.0% (512&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (0–7) (0–7) || Three lower digits || 51.2% (512&nbsp;states)\n|- bgcolor=#f2f2f2\n|rowspan=3|37.5% (384&nbsp;states)||'''1'''||'''0'''||'''0'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (0–7) (0–7) ||rowspan=3| Two lower digits,<br/>one higher digit ||rowspan=3|38.4% (384&nbsp;states)\n|- bgcolor=#f2f2f2\n|'''1'''||'''0'''||'''1'''||bgcolor=#cedff2|f||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|'''1'''||'''1'''||'''0'''||bgcolor=#ddcef2|i||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c\n|bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(0–7) (0–7) (8–9)\n|-\n|rowspan=3|9.375% (96&nbsp;states)||'''1'''||'''1'''||'''1'''||bgcolor=#cedff2|f||'''0'''||'''0'''||bgcolor=#ddcef2|i||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(0–7) (8–9) (8–9) ||rowspan=3| One lower digit,<br/>two higher digits ||rowspan=3|9.6% (96&nbsp;states)\n|-\n|'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||'''0'''||'''1'''||bgcolor=#ddcef2|i||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(8–9) (0–7) (8–9)\n|-\n|'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||'''1'''||'''0'''||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i||bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|3.125% (32&nbsp;states, 8&nbsp;used)||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||'''1'''||'''1'''||bgcolor=#cedff2|f||('''0''')||('''0''')||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(8–9) (8–9) (8–9) || Three higher digits, bits b2 and b1 are [[don't care]] || 0.8% (8&nbsp;states)\n|}\n\n===Early Chen–Ho encoding===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Decimal data encoding for a single declet (early 1971 form)<ref name=\"Chen_1971_2\"/>\n|-\n!scope=\"col\" colspan=\"11\"| Binary encoding\n|rowspan=\"10\"|\n!scope=\"col\" colspan=\"6\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (1024&nbsp;states<!-- 0..1023 -->) !!scope=\"col\"| b9 !!scope=\"col\"| b8 !!scope=\"col\"| b7 !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!scope=\"col\"| d2 !!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (1000&nbsp;states<!-- 000..999 -->)\n|-\n|50.0% (512&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (0–7) (0–7) || Three lower digits || 51.2% (512&nbsp;states)\n|- bgcolor=#f2f2f2\n|rowspan=3|37.5% (384&nbsp;states)||'''1'''||'''0'''||'''0'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (0–7) (0–7) ||rowspan=3| Two lower digits,<br/>one higher digit ||rowspan=3|38.4% (384&nbsp;states)\n|- bgcolor=#f2f2f2\n|'''1'''||'''0'''||'''1'''||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|'''1'''||'''1'''||'''0'''||bgcolor=#ddcef2|i||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(0–7) (0–7) (8–9)\n|-\n|rowspan=3|9.375% (96&nbsp;states)||'''1'''||'''1'''||'''1'''||'''0'''||'''0'''||bgcolor=#cedff2|f||bgcolor=#ddcef2|i||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(0–7) (8–9) (8–9) ||rowspan=3| One lower digit,<br/>two higher digits ||rowspan=3|9.6% (96&nbsp;states)\n|-\n|'''1'''||'''1'''||'''1'''||'''0'''||'''1'''||bgcolor=#ddcef2|i||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(8–9) (0–7) (8–9)\n|-\n|'''1'''||'''1'''||'''1'''||'''1'''||'''0'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i||bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|3.125% (32&nbsp;states, 8&nbsp;used)||'''1'''||'''1'''||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i||('''0''')||('''0''')\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(8–9) (8–9) (8–9) || Three higher digits, bits b2 and b1 are [[don't care]] || 0.8% (8&nbsp;states)\n|}\n\n===Patented Chen–Ho encoding===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Decimal data encoding for a single declet (patented 1973 form)<ref name=\"Chen_1973_US3842414A\"/>\n|-\n!scope=\"col\" colspan=\"11\"| Binary encoding\n|rowspan=\"10\"|\n!scope=\"col\" colspan=\"6\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (1024&nbsp;states<!-- 0..1023 -->) !!scope=\"col\"| b9 !!scope=\"col\"| b8 !!scope=\"col\"| b7 !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!scope=\"col\"| d2 !!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (1000&nbsp;states<!-- 000..999 -->)\n|-\n|50.0% (512&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (0–7) (0–7) || Three lower digits || 51.2% (512&nbsp;states)\n|- bgcolor=#f2f2f2\n|rowspan=3|37.5% (384&nbsp;states)||'''1'''||'''0'''||'''0'''||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (0–7) (0–7) ||rowspan=3| Two lower digits,<br/>one higher digit ||rowspan=3|38.4% (384&nbsp;states)\n|- bgcolor=#f2f2f2\n|'''1'''||'''0'''||'''1'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|'''1'''||'''1'''||'''0'''||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(0–7) (0–7) (8–9)\n|-\n|rowspan=3|9.375% (96&nbsp;states)||'''1'''||'''1'''||'''1'''||'''1'''||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(0–7) (8–9) (8–9) ||rowspan=3| One lower digit,<br/>two higher digits ||rowspan=3|9.6% (96&nbsp;states)\n|-\n|'''1'''||'''1'''||'''1'''||'''0'''||'''1'''||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(8–9) (0–7) (8–9)\n|-\n|'''1'''||'''1'''||'''1'''||'''0'''||'''0'''||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|3.125% (32&nbsp;states, 8&nbsp;used)||'''1'''||'''1'''||'''1'''||'''1'''||'''1'''||('''0''')||('''0''')||bgcolor=#cef2e0|c||bgcolor=#cedff2|f||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(8–9) (8–9) (8–9) || Three higher digits, bits b2 and b1 are [[don't care]] || 0.8% (8&nbsp;states)\n|}\n\n===Final Chen–Ho encoding===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Chen-Ho decimal data encoding for a single declet (final 1975 form)<ref name=\"Chen_Ho_1975\"/><ref name=\"Cowlishaw_2000_CH\"/>\n|-\n!scope=\"col\" colspan=\"11\"| Binary encoding\n|rowspan=\"10\"|\n!scope=\"col\" colspan=\"6\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (1024&nbsp;states<!-- 0..1023 -->) !!scope=\"col\"| b9 !!scope=\"col\"| b8 !!scope=\"col\"| b7 !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!scope=\"col\"| d2 !!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (1000&nbsp;states<!-- 000..999 -->)\n|-\n|50.0% (512&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (0–7) (0–7) || Three lower digits || 51.2% (512&nbsp;states)\n|- bgcolor=#f2f2f2\n|rowspan=3|37.5% (384&nbsp;states)||'''1'''||'''0'''||'''0'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (0–7) (0–7) ||rowspan=3| Two lower digits,<br/>one higher digit ||rowspan=3|38.4% (384&nbsp;states)\n|- bgcolor=#f2f2f2\n|'''1'''||'''0'''||'''1'''||bgcolor=#cef2e0|c||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(0–7) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|'''1'''||'''1'''||'''0'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(0–7) (0–7) (8–9)\n|-\n|rowspan=3|9.375% (96&nbsp;states)||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||'''0'''||'''0'''||bgcolor=#cedff2|f||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(0–7) (8–9) (8–9) ||rowspan=3| One lower digit,<br/>two higher digits ||rowspan=3|9.6% (96&nbsp;states)\n|-\n|'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||'''0'''||'''1'''||bgcolor=#cedff2|f||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''||bgcolor=#ddcef2|100'''i'''||(8–9) (0–7) (8–9)\n|-\n|'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||'''1'''||'''0'''||bgcolor=#cedff2|f||bgcolor=#ddcef2|g||bgcolor=#ddcef2|h||bgcolor=#ddcef2|i||bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|0'''ghi'''||(8–9) (8–9) (0–7)\n|- bgcolor=#f2f2f2\n|3.125% (32&nbsp;states, 8&nbsp;used)||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||'''1'''||'''1'''||bgcolor=#cedff2|f||('''0''')||('''0''')||bgcolor=#ddcef2|i\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''||bgcolor=#ddcef2|100'''i'''||(8–9) (8–9) (8–9) || Three higher digits, bits b2 and b1 are [[don't care]] || 0.8% (8&nbsp;states)\n|}\n\n==Encodings for two decimal digits==\n\n===Hertz encoding===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Hertz decimal data encoding for a single [[heptad (computing)|heptad]] (1969 form)<ref name=\"Hertz_1969\"/>\n|-\n!scope=\"col\" colspan=\"8\"| Binary encoding\n|rowspan=\"6\"|\n!scope=\"col\" colspan=\"5\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (128&nbsp;states<!-- 0..127 -->) !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (100&nbsp;states<!-- 00..99 -->)\n|-\n|50.0% (64&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''|| (0–7) (0–7) || Two lower digits || 64.0% (64&nbsp;states)\n<!-- unfinished |- bgcolor=#f2f2f2\n|12.5% (16&nbsp;states, 0&nbsp;used)||'''1'''||'''0'''||'''0'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''|| (8–9) (0–7) || One lower digit,<br/>one higher digit || 0.0% (0&nbsp;states) -->\n|- bgcolor=#f2f2f2\n|12.5% (16&nbsp;states)||'''1'''||'''1'''||'''0'''||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''|| (8–9) (0–7) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|-\n|12.5% (16&nbsp;states)||'''1'''||'''0'''||'''1'''||bgcolor=#cedff2|f||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''|| (0–7) (8–9) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|- bgcolor=#f2f2f2\n|12.5% (16&nbsp;states, 4&nbsp;used)||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||x||x||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''|| (8–9) (8–9) || Two higher digits || 4.0% (4&nbsp;states)\n|}\n\n===Early Chen–Ho encoding, method A===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Decimal data encoding for a single [[heptad (computing)|heptad]] (early 1971 form, method A)<ref name=\"Chen_1971_2\"/>\n|-\n!scope=\"col\" colspan=\"8\"| Binary encoding\n|rowspan=\"6\"|\n!scope=\"col\" colspan=\"5\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (128&nbsp;states<!-- 0..127 -->) !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (100&nbsp;states<!-- 00..99 -->)\n|-\n|50.0% (64&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''|| (0–7) (0–7) || Two lower digits || 64.0% (64&nbsp;states)\n|- bgcolor=#f2f2f2\n|25.0% (32&nbsp;states, 16&nbsp;used)||'''1'''||'''0'''||x||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''|| (8–9) (0–7) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|-\n|12.5% (16&nbsp;states)||'''1'''||'''1'''||'''0'''||bgcolor=#cedff2|f||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''|| (0–7) (8–9) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|- bgcolor=#f2f2f2\n|12.5% (16&nbsp;states, 4&nbsp;used)||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||x||x||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''|| (8–9) (8–9) || Two higher digits || 4.0% (4&nbsp;states)\n|}\n\n<!-- unfinished\n===Early Chen–Ho encoding, method B===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Decimal data encoding for a single heptad (early 1971 form, method B)<ref name=\"Chen_1971_2\"/>\n|-\n!scope=\"col\" colspan=\"8\"| Binary encoding\n|rowspan=\"6\"|\n!scope=\"col\" colspan=\"5\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (128&nbsp;states) !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (100&nbsp;states)\n|-\n|50.0% (64&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''|| (0–7) (0–7) || Two lower digits || 64.0% (64&nbsp;states)\n|- bgcolor=#f2f2f2\n|25.0% (32&nbsp;states, 16&nbsp;used)||'''1'''||'''0'''||bgcolor=#cef2e0|c||x||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''|| (8–9) (0–7) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|-\n|25.0% (32&nbsp;states, 16 used)||'''1'''||'''1'''||bgcolor=#cedff2|f||x||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''|| (0–7) (8–9) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|- bgcolor=#f2f2f2\n|12.5% (16&nbsp;states, 4&nbsp;used)||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||x||x||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''|| (8–9) (8–9) || Two higher digits || 4.0% (4&nbsp;states)\n|}\n-->\n\n===Patented Chen–Ho encoding===\n{| class=\"wikitable\" border=\"1\" style=\"text-align:center\"\n|+Decimal data encoding for a single heptad (patented 1973 form)<ref name=\"Chen_1973_US3842414A\"/>\n|-\n!scope=\"col\" colspan=\"8\"| Binary encoding\n|rowspan=\"6\"|\n!scope=\"col\" colspan=\"5\"| Decimal digits\n|-\n!scope=\"col\"| Code&nbsp;space (128&nbsp;states<!-- 0..127 -->) !!scope=\"col\"| b6 !!scope=\"col\"| b5\n!scope=\"col\"| b4 !!scope=\"col\"| b3 !!scope=\"col\"| b2 !!scope=\"col\"| b1 !!scope=\"col\"| b0\n!!scope=\"col\"| d1 !!scope=\"col\"| d0\n!scope=\"col\"| Values encoded\n!scope=\"col\"| Description\n!scope=\"col\"| Possibilities (100&nbsp;states<!-- 00..99 -->)\n|-\n|50.0% (64&nbsp;states)||'''0'''||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|0'''def'''|| (0–7) (0–7) || Two lower digits || 64.0% (64&nbsp;states)\n|- bgcolor=#f2f2f2\n|25.0% (32&nbsp;states, 16&nbsp;used)||'''1'''||'''0'''||x||bgcolor=#cef2e0|c||bgcolor=#cedff2|d||bgcolor=#cedff2|e||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|0'''def'''|| (8–9) (0–7) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|-\n|12.5% (16&nbsp;states)||'''1'''||'''1'''||'''1'''||bgcolor=#cef2e0|c||bgcolor=#cef2e0|a||bgcolor=#cef2e0|b||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|0'''abc'''||bgcolor=#cedff2|100'''f'''|| (0–7) (8–9) || One lower digit,<br/>one higher digit || 16.0% (16&nbsp;states)\n|- bgcolor=#f2f2f2\n|12.5% (16&nbsp;states, 4&nbsp;used)||'''1'''||'''1'''||'''0'''||bgcolor=#cef2e0|c||x||x||bgcolor=#cedff2|f\n|bgcolor=#cef2e0|100'''c'''||bgcolor=#cedff2|100'''f'''|| (8–9) (8–9) || Two higher digits || 4.0% (4&nbsp;states)\n|}\n\n==See also==\n* [[Binary-coded decimal]] (BCD)\n* [[Densely packed decimal]] (DPD)\n* [[Unicode transformation format]] (UTF) (similar encoding scheme)\n* [[Length-limited Huffman code]]\n\n==References==\n{{Reflist|refs=\n<ref name=\"Muller_2010\">{{cite book |author-last1=Muller |author-first1=Jean-Michel |author-last2=Brisebarre |author-first2=Nicolas |author-last3=de Dinechin |author-first3=Florent |author-last4=Jeannerod |author-first4=Claude-Pierre |author-last5=Lefèvre |author-first5=Vincent |author-last6=Melquiond |author-first6=Guillaume |author-last7=Revol |author-first7=Nathalie |author-last8=Stehlé |author-first8=Damien |author-last9=Torres |author-first9=Serge |title=Handbook of Floating-Point Arithmetic |date=2010 |publisher=[[Birkhäuser]] |edition=1 |isbn=978-0-8176-4704-9<!-- print --> |doi=10.1007/978-0-8176-4705-6 |lccn=2009939668<!-- |id={{isbn|978-0-8176-4705-6}} (online), {{isbn|0-8176-4704-X}} (print) -->|url=http://cds.cern.ch/record/1315760 }}</ref>\n<ref name=\"Chen_2015\">{{cite web |title=CHEN Tien Chi |url=http://www.cse.cuhk.edu.hk/v7/en/people/tcchen.html |access-date=2016-02-07 |dead-url=no |archive-url=https://web.archive.org/web/20151023061814/http://www.cse.cuhk.edu.hk/v7/en/people/tcchen.html |archive-date=2015-10-23}}</ref>\n<ref name=\"Tseng_1988\">{{cite web |title=High-Tech Leadership: Irving T. Ho |author-last=Tseng |author-first=Li-Ling |date=1988-04-01 |publisher=[[Taiwan Info]]<!-- 2015 ministère des Affaires étrangères, République de Chine (Taiwan) --> |url=http://taiwaninfo.nat.gov.tw/ct.asp?xItem=105983&ctNode=124 |access-date=2016-02-08 |dead-url=no |archive-url=https://web.archive.org/web/20160208172221/http://taiwaninfo.nat.gov.tw/ct.asp?xItem=105983&ctNode=124 |archive-date=2016-01-01}}</ref>\n<ref name=\"Tung_2016\">{{cite web |title=IBM资深专家Frank Tung博士8月4日来我校演讲 |url=http://www2.scut.edu.cn/software/news/news13.htm |access-date=2016-02-06 |dead-url=no |archive-url=https://web.archive.org/web/20041208004402/http://www2.scut.edu.cn/software/news/news13.htm |archive-date=2004-12-08}}</ref>\n<ref name=\"Chen_1971_1\">{{cite paper |author-first=Tien Chi |author-last=Chen |title=Decimal-binary integer conversion scheme |date=1971-03-12 |type=Internal memo to Irving Tze Ho |location=San Jose Research Laboratory |publisher=[[IBM]]}}</ref>\n<ref name=\"Chen_1971_2\">{{cite paper |author-first=Tien Chi |author-last=Chen |title=Decimal Number Compression |date=1971-03-29 |type=Internal memo to Irving Tze Ho |pages=1–4 |location=San Jose Research Laboratory |publisher=[[IBM]] |url=http://speleotrove.com/decimal/chen1971-memo-to-Ho.pdf |access-date=2016-02-07 |dead-url=no |archive-url=https://web.archive.org/web/20121017134315/http://speleotrove.com/decimal/chen1971-memo-to-Ho.pdf |archive-date=2012-10-17}}</ref>\n<ref name=\"Chen_Ho_1974\">{{cite journal |title=Storage-Efficient Representation of Decimal Data |author-first1=Tien Chi |author-last1=Chen |author-first2=Irving Tze |author-last2=Ho |date=1974-06-25 |journal=Research Report RJ 1420 |type=Technical report |location=IBM Research Lab, San Jose, USA |publisher=[[IBM]]}}</ref>\n<ref name=\"Chen_Ho_1975\">{{cite journal |title=Storage-Efficient Representation of Decimal Data |author-first1=Tien Chi |author-last1=Chen |author-first2=Irving Tze |author-last2=Ho |date=January 1975 |journal=[[Communications of the Association for Computing Machinery]]|volume=18 |issue=1 |pages=49–52 |doi=10.1145/360569.360660 }}</ref>\n<ref name=\"Cowlishaw_2000_CH\">{{cite web |author-first=Michael Frederic |author-last=Cowlishaw |author-link=Michael Frederic Cowlishaw |title=A Summary of Chen-Ho Decimal Data encoding |orig-year=2000<!-- June 2000 --> |date=2014 |publisher=[[IBM]] |url=http://speleotrove.com/decimal/chen-ho.html |access-date=2016-02-07 |dead-url=no |archive-url=https://web.archive.org/web/20150924145419/http://speleotrove.com/decimal/chen-ho.html |archive-date=2015-09-24}}</ref>\n<ref name=\"Cowlishaw_2002\">{{cite journal |title=Densely Packed Decimal Encoding |author-first=Michael Frederic |author-last=Cowlishaw |author-link=Michael Frederic Cowlishaw |journal=IEE Proceedings – Computers and Digital Techniques |volume=149 |issue=3 |pages=102–104 |publisher=[[Institution of Electrical Engineers]] (IEE) |date=May 2002 |issn=1350-2387 |doi=10.1049/ip-cdt:20020407 |location=London |url=http://ieeexplore.ieee.org/xpl/login.jsp?reload=true&arnumber=1008829 |access-date=2016-02-07}}</ref>\n<ref name=\"Chen_1973_US3842414A\">{{cite web |title=Binary coded decimal conversion apparatus |author-first1=Tien Chi |author-last1=Chen |author-first2=Irving Tze |author-last2=Ho |publisher=[[International Business Machines Corporation]] (IBM) |location=San Jose, CA, USA & Poughkeepsie, NY, USA |publication-place=Armonk, NY, USA |date=1974-10-15 |orig-year=1973-06-18 |id=US3842414A |type=US Patent |url=https://patents.google.com/patent/US3842414A |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/98/0e/62/0169530be1048f/US3842414.pdf<!-- https://web.archive.org/web/20180718234732/https://patentimages.storage.googleapis.com/98/0e/62/0169530be1048f/US3842414.pdf -->] [http://www.freepatentsonline.com/3842414.html] (NB. This patent is about the Chen–Ho algorithm.)</ref>\n<ref name=\"Cowlishaw_2007\">{{cite web |author-first=Michael Frederic |author-last=Cowlishaw |author-link=Michael Frederic Cowlishaw |publisher=[[IBM]] |title=A Summary of Densely Packed Decimal encoding |orig-year=2000<!-- (2000-10-03) --> |date=2007-02-13 |url=http://speleotrove.com/decimal/DPDecimal.html |access-date=2016-02-07 |dead-url=no |archive-url=https://web.archive.org/web/20150924145411/http://speleotrove.com/decimal/DPDecimal.html |archive-date=2015-09-24}}</ref>\n<ref name=\"Cowlishaw_2001_US6525679B1\">{{cite web |title=Decimal to binary coder/decoder |author-first=Michael Frederic |author-last=Cowlishaw |author-link=Michael Frederic Cowlishaw |publisher=[[International Business Machines Corporation]] (IBM) |location=Coventry, UK |publication-place=Armonk, NY, USA |date=2003-02-25 |orig-year=2002-05-20, 2001-01-27 |id=US6525679B1 |type=US Patent |url=https://patents.google.com/patent/US6525679B1 |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/b4/27/a6/f4ef138a335192/US6525679.pdf<!-- https://web.archive.org/web/20180719215823/https://patentimages.storage.googleapis.com/b4/27/a6/f4ef138a335192/US6525679.pdf -->] and {{cite web |title=Decimal to binary coder/decoder |author-first=Michael Frederic |author-last=Cowlishaw |author-link=Michael Frederic Cowlishaw |publisher=[[International Business Machines Corporation]] (IBM) |location=Winchester, Hampshire, UK |publication-place=Armonk, NY, USA |date=2007-11-07 |orig-year=2004-01-14, 2002-08-14, 2001-09-24, 2001-01-27 |id=EP1231716A2 |type=European Patent |url=https://patents.google.com/patent/EP1231716A2/en |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/e1/0c/a1/69981125812742/EP1231716A2.pdf<!-- https://web.archive.org/web/20180719001335/https://patentimages.storage.googleapis.com/e1/0c/a1/69981125812742/EP1231716A2.pdf -->] [https://patentimages.storage.googleapis.com/ba/25/b6/b0ad365c086f5f/EP1231716B1.pdf<!-- https://web.archive.org/web/20180719054809/https://patentimages.storage.googleapis.com/ba/25/b6/b0ad365c086f5f/EP1231716B1.pdf -->] [http://www.freepatentsonline.com/6525679.html] (NB. This patent about [[Densely Packed Decimal|DPD]] also discusses the Chen–Ho algorithm.)</ref>\n<ref name=\"Hertz_1969\">{{cite web |title=System for the compact storage of decimal numbers |author-first=Theodore M. |author-last=Hertz |location=Whittier, CA, USA |publisher=[[North American Rockwell Corporation]] |date=1971-11-02 |orig-year=1969-12-15 |id=US3618047A |type=US Patent |url=https://patents.google.com/patent/US3618047A |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/34/cb/46/fd134e38804af5/US3618047.pdf<!-- https://web.archive.org/web/20180719211146/https://patentimages.storage.googleapis.com/34/cb/46/fd134e38804af5/US3618047.pdf -->] [http://www.freepatentsonline.com/3618047.html] (NB. A coding system very similar to Chen-Ho, also cited as [[prior art]] in the Chen–Ho patent.)</ref>\n<ref name=\"Savard_2007_CH\">{{cite web |title=Chen-Ho Encoding and Densely Packed Decimal |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2007 |work=quadibloc |url=http://www.quadibloc.com/comp/cp020301.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703002320/http://www.quadibloc.com/comp/cp020301.htm |archive-date=2018-07-16}}</ref>\n<ref name=\"IBM_1973_CF\">{{cite book |title=7070/7074 Compatibility Feature for IBM System/370 Models 165, 165 II, and 168 |publisher=[[IBM]] |id=GA22-6958-1 (File No. 5/370-13) |edition=2 |date=June 1973 |orig-year=1970 |url=http://www.bitsavers.org/pdf/ibm/370/compatibility_feature/GA22-6958-1_707x_Compatibility_Feature_for_IBM-370_165_168.pdf |access-date=2018-07-21 |dead-url=no |archive-url=https://web.archive.org/web/20180722120344/http://www.bitsavers.org/pdf/ibm/370/compatibility_feature/GA22-6958-1_707x_Compatibility_Feature_for_IBM-370_165_168.pdf |archive-date=2018-07-22}}</ref>\n}}\n\n==Further reading==\n* {{cite journal |title=Comments on a paper by T. C. Chen and I. T. Ho |author-first=Alan Jay |author-last=Smith |author-link=Alan Jay Smith |date=August 1975 |journal=[[Communications of the Association for Computing Machinery]] |volume=18 |issue=8 |page=463 |doi=10.1145/360933.360986 |id={{CODEN|CACMA2}} |issn=0001-0782<!-- eissn=1557-7317 --> }} (NB. A publication also discussing Chen-Ho alternatives and variations.)\n* {{cite web |title=Packed Decimal Encoding IEEE-754-2008 |author-first=Jo H. M. |author-last=Bonten |orig-year=2006-10-05 |date=2009-10-06 |url=http://home.kpn.nl/jhm.bonten/computers/bitsandbytes/wordsizes/ibmpde.htm#dense4 |access-date=2018-07-11 |dead-url=no |archive-url=https://web.archive.org/web/20180711143550/http://home.kpn.nl/jhm.bonten/computers/bitsandbytes/wordsizes/ibmpde.htm#dense4 |archive-date=2018-07-11}}\n* {{cite web |title=Base-26 Armor |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2001 |work=quadibloc |url=http://www.quadibloc.com/crypto/mi060301.htm |access-date=2018-07-21 |dead-url=no |archive-url=https://web.archive.org/web/20180721041840/http://www.quadibloc.com/crypto/mi060301.htm |archive-date=2018-07-21}}\n* {{cite web |title=Data compression/expansion and compressed data processing |author-first1=Russell G. |author-last1=Rinaldi |author-first2=Brian B. |author-last2=Moore |location=Poughkeepsie, NY, USA & New Paltz, NY, USA |publication-place=New York, USA |publisher=[[International Business Machines Corporation]] (IBM) |date=1967-03-21 |orig-year=1964-06-30 |id=US3310786A |type=US Patent |url=https://patents.google.com/patent/US3310786A |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/98/65/5e/63a98aac4b5184/US3310786.pdf<!-- https://web.archive.org/web/20180719213000/https://patentimages.storage.googleapis.com/98/65/5e/63a98aac4b5184/US3310786.pdf -->], {{cite web |title=Serial digital adder employing a compressed data format |author-first1=Russell G. |author-last1=Rinaldi |author-first2=Brian B. |author-last2=Moore |location=Poughkeepsie, NY, USA & New Paltz, NY, USA |publication-place=New York, USA |publisher=[[International Business Machines Corporation]] (IBM) |date=1969-05-20 |orig-year=1967-01-19, 1964-06-30 |id=US3445641A |type=US Patent |url=https://patents.google.com/patent/US3445641A |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/f8/84/2b/227f22291c1846/US3445641.pdf<!-- https://web.archive.org/web/20180719214333/https://patentimages.storage.googleapis.com/f8/84/2b/227f22291c1846/US3445641.pdf -->] and {{cite web |title=Data compression/expansion and compressed data processing |author-first1=Russell G. |author-last1=Rinaldi |author-first2=Brian B. |author-last2=Moore |location=Poughkeepsie, NY, USA & New Paltz, NY, USA |publication-place=New York, USA |publisher=[[International Business Machines Corporation]] (IBM) |date=1969-03-11 |orig-year=1967-01-19, 1964-06-30 |id=US3432811A |type=US Patent |url=https://patents.google.com/patent/US3432811A |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/6c/e8/d7/c1d354d8c9b947/US3432811.pdf<!-- https://web.archive.org/web/20180719213355/https://patentimages.storage.googleapis.com/6c/e8/d7/c1d354d8c9b947/US3432811.pdf -->] (NB. A patent cited in both, the Hertz and Chen–Ho patents.)\n<!-- * Bender, R. R.; Galage, D. J. \"Packing Mode Control\". IBM Technical Disclosure Bulletin; Vol. 4, No. 3, August 1961; pp. 61-63.\n* Tilem, J. Y: \"Data Packing and Unpacking Means\". IBM Technical Disclosure Bulletin, Vol. 5, No. 7, December 1962; pp. 48-49.\n* Lengyel, E. J.; McMahon, R. F. \"Direct Decimal to Binary Address Generator For Small Memories\". IBM Technical Disclosure Bulletin; Vol. 9, No. 10, March 1967; p. 1347. --><!-- https://priorart.ip.com/IPCOM/000092875#text -->\n\n{{DEFAULTSORT:Chen-Ho encoding}}\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Binary clock",
      "url": "https://en.wikipedia.org/wiki/Binary_clock",
      "text": "{{inline citations|date=February 2015}}\nA '''binary clock''' is a [[clock]] that displays the time of day in a [[binary numeral system|binary]] format.  Originally, such clocks showed ''each decimal digit'' of sexagesimal time as a binary value, but presently binary clocks also exist which display hours, minutes, and seconds as binary numbers. Most binary clocks are [[Digital data|digital]], although [[analog signal|analog]] varieties exist. True binary clocks also exist, which indicate the time by successively halving the day, instead of using hours, minutes, or seconds. Similar clocks, based on [[Gray code]]d binary, also exist. \n\n== Binary-coded decimal clocks ==\n[[File:Binary clock.svg|thumbnail|right|Reading a [[binary-coded decimal|BCD]] [[clock]]: Add the values of each column of [[Light-emitting diode|LEDs]] to get six decimal digits. There are two columns each for [[hour]]s, [[minute]]s and [[second]]s.]]\n[[File:Digital-BCD-clock.jpg|thumbnail|right|Both clocks read 12:15:45.]]\n\nMost common binary clocks  use six columns of [[LED]]s to represent [[0 (number)|zeros]] and [[1 (number)|ones]]. Each column represents a single decimal digit, a format known as [[binary-coded decimal]] (BCD). The bottom row in each column represents 1 (or 2<sup>0</sup>), with each row above representing higher powers of two, up to 2<sup>3</sup> (or 8). \n\nTo read each individual digit in the time, the user adds the values that each illuminated [[light-emitting diode|LED]] represents, then reads these from left to right. The first two columns represent the [[hour]], the next two represent the [[minute]] and the last two represent the [[second]]. Since zero digits are not illuminated, the positions of each digit must be memorized if the clock is to be usable in the dark.\n\n== Binary-coded sexagesimal clocks ==\n\n[[File:Binary clock samui moon.jpg|thumbnail|right|Time Technology's Samui Moon binary-coded sexagesimal wristwatch. This clock reads 3:25.]]\n\nBinary clocks that display time in binary-coded [[sexagesimal]] also exist. Instead of representing each digit of traditional sexagesimal time with one binary number, each component of traditional sexagesimal time is represented with one binary number, that is, using up to 6 bits instead of only 4.\n\nFor 24-hour binary-coded sexagesimal clocks, there are 11 or 17 LED lights to show us the time.  There are 5 LEDs to show the hours, there are 6 LEDs to show the minutes, and there are 6 LEDs to show the seconds. 6 LEDs to show the seconds are not needed in 24-hour binary-coded sexagesimal clocks with 11 LED lights.\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|-\n| \n! Hours\n! Minutes\n! Seconds\n|-\n! 32\n|  \n| 1\n| 1\n|-\n! 16\n| 0\n| 0\n| 1\n|-\n! 8\n| 1\n| 0\n| 0\n|-\n! 4\n| 0\n| 1\n| 0\n|-\n! 2\n| 1\n| 0\n| 0\n|-\n! 1\n| 0\n| 1\n| 1\n|-\n| \n! 10\n! 37\n! 49\n|}\n\n[[File:Binary clock Swiss railway station.jpg|thumbnail|right|Binary large-scale electronic clock to indicate the time of day on 3 lines in hours, minutes, seconds on the face of the main railway station in St. Gallen, Switzerland. Time indicated is 9 o'clock 25 minutes 46 seconds.]]\nA format exists also where hours, minutes and seconds are shown on three lines instead of columns as binary numbers.<ref>[https://www.tagblatt.ch/ostschweiz/stgallen-gossau-rorschach/stgallen-verpassen-sie-den-zug-oder-koennen-sie-eine-binaer-uhr-lesen-ld.1013362 ''Verpassen Sie den Zug oder koennen Sie eine Binaer-Uhr lesen?''] (in German), St. Galler Tagblatt, 4 April 2018. Retrieved 20 November 2018.</ref>\n\n== See also ==\n* [[Hexadecimal time]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://www.glassgiant.com/geek/binaryclock/binary_clock_flash.swf Binary clock made in Flash]\n* [http://public.tembolab.pl/?dir=applications/binary-clocks Binary clocks as applications] (for Windows)\n* [http://www.abulsme.com/binarytime/ \"True\" binary clock] — portrays time of day as a sequence of 16 bits\n* [http://scratch.mit.edu/projects/23808257/ Colorful binary clock] - A colorful binary clock on Scratch\n* [https://flyinghyrax.deviantart.com/art/Binary-Clock-266686125] - Binary Clock for Rainmeter by FlyingHyrax\n[[Category:Clock designs]]\n[[Category:Time measurement systems]]\n[[Category:Binary arithmetic|Clock]]"
    },
    {
      "title": "Cyclic redundancy check",
      "url": "https://en.wikipedia.org/wiki/Cyclic_redundancy_check",
      "text": "{{Use dmy dates|date=July 2013}}\n\nA '''cyclic redundancy check''' ('''CRC''') is an [[Error detection and correction|error-detecting code]] commonly used in digital [[Telecommunications network|networks]] and storage devices to detect accidental changes to raw data.  Blocks of data entering these systems get a short ''check value'' attached, based on the remainder of a [[Polynomial long division|polynomial division]] of their contents. On retrieval, the calculation is repeated and, in the event the check values do not match, corrective action can be taken against data corruption. CRCs can be used for [[Error correcting code|error correction]] (see [[Mathematics of cyclic redundancy checks#Bitfilters|bitfilters]]).<ref>{{cite web|url=http://www.drdobbs.com/an-algorithm-for-error-correcting-cyclic/184401662|title=An Algorithm for Error Correcting Cyclic Redundance Checks|author=|date=|website=drdobbs.com}}</ref>\n\nCRCs are so called because the ''check'' (data verification) value is a ''redundancy'' (it expands the message without adding [[Entropy (information theory)|information]]) and the [[algorithm]] is based on [[Cyclic code|''cyclic'' codes]]. CRCs are popular because they are simple to implement in binary [[Computer hardware|hardware]], easy to analyze mathematically, and particularly good at detecting common errors caused by [[Noise (electronics)|noise]] in transmission channels.  Because the check value has a fixed length, the [[Function (mathematics)|function]] that generates it is occasionally used as a [[hash function]].\n\nThe CRC was invented by [[W. Wesley Peterson]] in 1961; the 32-bit CRC function, used in Ethernet and many other standards, is the work of several researchers and was published in 1975.\n\n== Introduction ==\nCRCs are based on the theory of [[cyclic code|cyclic]] [[error-correcting code]]s. The use of [[systematic code|systematic]] cyclic codes, which encode messages by adding a fixed-length check value, for the purpose of error detection in communication networks, was first proposed by [[W. Wesley Peterson]] in 1961.<ref name=\"PetersonBrown1961\">{{Cite journal\n|last = Peterson |first=W. W. |last2 =Brown |first2=D. T.\n|date=January 1961\n| title = Cyclic Codes for Error Detection\n| journal = Proceedings of the IRE\n| doi = 10.1109/JRPROC.1961.287814\n| volume = 49\n| issue = 1\n| pages = 228–235\n}}</ref>\nCyclic codes are not only simple to implement but have the benefit of being particularly well suited for the detection of [[burst error]]s: contiguous sequences of erroneous data symbols in messages. This is important because burst errors are common transmission errors in many [[communication channel]]s, including magnetic and optical storage devices. Typically an ''n''-bit CRC applied to a data block of arbitrary length will detect any single error burst not longer than ''n'' bits and the fraction of all longer error bursts that it will detect is {{nowrap|(1 − 2<sup>−''n''</sup>)}}.\n\nSpecification of a CRC code requires definition of a so-called [[generator polynomial]]. This polynomial becomes the [[divisor]] in a [[polynomial long division]], which takes the message as the [[division (mathematics)|dividend]] and in which the [[quotient]] is discarded and the [[remainder]] becomes the result.  The important caveat is that the polynomial [[coefficient]]s are calculated according to the arithmetic of a [[finite field]], so the addition operation can always be performed bitwise-parallel (there is no carry between digits).\n\nIn practice, all commonly used CRCs employ the [[Galois field]] of two elements, [[GF(2)]]. The two elements are usually called 0 and 1, comfortably matching computer architecture.\n\nA CRC is called an ''n''-bit CRC when its check value is ''n'' bits long. For a given ''n'', multiple CRCs are possible, each with a different polynomial. Such a polynomial has highest degree ''n'', which means it has {{nowrap|''n'' + 1}} terms. In other words, the polynomial has a length of {{nowrap|''n'' + 1}}; its encoding requires {{nowrap|''n'' + 1}} bits. Note that most polynomial specifications either drop the [[Most significant bit|MSB]] or [[Least significant bit|LSB]], since they are always 1. The CRC and associated polynomial typically have a name of the form CRC-''n''-XXX as in the [[#table|table]] below.\n\nThe simplest error-detection system, the [[parity bit]], is in fact a 1-bit CRC: it uses the generator polynomial&nbsp;{{nowrap|''x'' + 1}} (two terms), and has the name CRC-1.\n\n== Application ==\nA CRC-enabled device calculates a short, fixed-length binary sequence, known as the ''check value'' or ''CRC'', for each block of data to be sent or stored and appends it to the data, forming a ''codeword''.\n\nWhen a codeword is received or read, the device either compares its check value with one freshly calculated from the data block, or equivalently, performs a CRC on the whole codeword and compares the resulting check value with an expected ''residue'' constant.\n\nIf the CRC values do not match, then the block contains a data error.\n\nThe device may take corrective action, such as rereading the block or requesting that it be sent again. Otherwise, the data is assumed to be error-free (though, with some small probability, it may contain undetected errors; this is inherent in the nature of error-checking).<ref name=\"ritter-1986\">{{Cite journal|first=Terry|last=Ritter|title=The Great CRC Mystery|url=http://www.ciphersbyritter.com/ARTS/CRCMYST.HTM|accessdate=21 May 2009|journal=[[Dr. Dobb's Journal]]|date=February 1986|volume=11|issue=2|pages=26–34, 76–83}}</ref>\n\n== Data integrity ==\nCRCs are specifically designed to protect against common types of errors on communication channels, where they can provide quick and reasonable assurance of the [[data integrity|integrity]] of messages delivered. However, they are not suitable for protecting against intentional alteration of data.\n\nFirstly, as there is no authentication, an attacker can edit a message and recompute the CRC without the substitution being detected. When stored alongside the data, CRCs and cryptographic hash functions by themselves do not protect against ''intentional'' modification of data. Any application that requires protection against such attacks must use cryptographic authentication mechanisms, such as [[message authentication code]]s or [[digital signatures]] (which are commonly based on [[cryptographic hash]] functions).\n\nSecondly, unlike cryptographic hash functions, CRC is an easily reversible function, which makes it unsuitable for use in digital signatures.<ref name=\"stigge-reversecrc\">{{Cite journal|last=Stigge|first=Martin|last2=Plötz|first2=Henryk|last3=Müller|first3=Wolf|last4=Redlich|first4=Jens-Peter|title=Reversing CRC – Theory and Practice|date=May 2006|page=17|location=Berlin|publisher=Humboldt University Berlin|url=http://sar.informatik.hu-berlin.de/research/publications/SAR-PR-2006-05/SAR-PR-2006-05_.pdf|accessdate=4 February 2011|quote=The presented methods offer a very easy and efficient way to modify your data so that it will compute to a CRC you want or at least know in advance.|deadurl=yes|archiveurl=https://web.archive.org/web/20110719042902/http://sar.informatik.hu-berlin.de/research/publications/SAR-PR-2006-05/SAR-PR-2006-05_.pdf|archivedate=19 July 2011|df=dmy-all}}</ref>\n\nThirdly, CRC is a [[linear function]] with a property that<ref>{{cite web |title=algorithm design - Why is CRC said to be linear? |url=https://crypto.stackexchange.com/a/34013 |website=Cryptography Stack Exchange |accessdate=5 May 2019}}</ref>\n\n: <math>\\operatorname{crc}(x \\oplus y \\oplus z) = \\operatorname{crc}(x) \\oplus \\operatorname{crc}(y) \\oplus \\operatorname{crc}(z);</math>\n\nas a result, even if the CRC is encrypted with a [[stream cipher]] that uses [[Exclusive or|XOR]] as its combining operation (or [[Block cipher modes of operation|mode]] of [[block cipher]] which effectively turns it into a stream cipher, such as OFB or CFB), both the message and the associated CRC can be manipulated without knowledge of the encryption key; this was one of the well-known design flaws of the [[Wired Equivalent Privacy]] (WEP) protocol.<ref name=\"wep\">{{Cite journal| last1=Cam-Winget | first1=Nancy | last2=Housley | first2=Russ | last3=Wagner | first3=David | last4=Walker | first4=Jesse | title=Security Flaws in 802.11 Data Link Protocols | journal=Communications of the ACM | volume=46 | issue=5 | pages=35–39 |date = May 2003| doi=10.1145/769800.769823 | url=http://www.cs.berkeley.edu/~daw/papers/wireless-cacm.pdf | citeseerx=10.1.1.14.8775 }}</ref>\n\n== CRC-32 algorithm ==\nThis is the algorithm for the CRC-32 variant of CRC.<ref>{{cite web|url=https://msdn.microsoft.com/en-us/library/dd905031.aspx|title=[MS-ABS]: 32-Bit CRC Algorithm|author=|date=|website=msdn.microsoft.com}}</ref> The CRCTable is a [[memoization]] of a calculation that would have to be repeated for each byte of the message.\n\n <span style=\"color:blue;\">'''Function'''</span> CRC32\n    <span style=\"color:blue;\">'''Input:'''</span>\n       data:  Bytes     <span style=\"color:green;\">'''//Array of bytes'''</span>\n    <span style=\"color:blue;\">'''Output:'''</span>\n       crc32: UInt32    <span style=\"color:green;\">'''//32-bit unsigned crc-32 value'''</span><br>\n <span style=\"color:green;\">'''//Initialize crc-32 to starting value'''</span>\n crc32 &larr; 0xFFFFFFFF<br>\n <span style=\"color:blue;\">'''for each'''</span> byte <span style=\"color:blue;\">'''in'''</span> data <span style=\"color:blue;\">'''do'''</span>\n    nLookupIndex &larr; (crc32 xor byte) and 0xFF;\n    crc32 &larr; (crc32 shr 8) xor CRCTable[nLookupIndex] <span style=\"color:green;\">'''//CRCTable is an array of 256 32-bit constants'''</span><br>\n <span style=\"color:green;\">'''//Finalize the CRC-32 value by inverting all the bits'''</span>\n crc32 &larr; crc32 xor 0xFFFFFFFF\n <span style=\"color:blue;\">'''return'''</span> crc32\n\n== Computation ==\n{{unreferenced section|date=July 2016}}\n{{Main|Computation of cyclic redundancy checks}}\n\nTo compute an ''n''-bit binary CRC, line the bits representing the input in a row, and position the ({{nowrap|''n'' + 1}})-bit pattern representing the CRC's divisor (called a \"[[polynomial]]\") underneath the left-hand end of the row.\n\nIn this example, we shall encode 14 bits of message with a 3-bit CRC, with a polynomial {{nowrap|''x''<sup>3</sup> + ''x'' + 1}}. The polynomial is written in binary as the coefficients; a 3rd-degree polynomial has 4 coefficients ({{nowrap|1''x''<sup>3</sup> + 0''x''<sup>2</sup> + 1''x'' + 1}}). In this case, the coefficients are 1, 0, 1 and 1.  The result of the calculation is 3 bits long.\n\nStart with the message to be encoded:\n\n<pre>\n11010011101100\n</pre>\n\nThis is first padded with zeros corresponding to the bit length ''n'' of the CRC. Here is the first calculation for computing a 3-bit CRC:\n\n<pre>\n11010011101100 000 <--- input right padded by 3 bits\n1011               <--- divisor (4 bits) = x³ + x + 1\n------------------\n01100011101100 000 <--- result\n</pre>\n\nThe algorithm acts on the bits directly above the divisor in each step.  The result for that iteration is the bitwise XOR of the polynomial divisor with the bits above it.  The bits not above the divisor are simply copied directly below for that step.  The divisor is then shifted one bit to the right, and the process is repeated until the divisor reaches the right-hand end of the input row. Here is the entire calculation:\n\n<pre>\n11010011101100 000 <--- input right padded by 3 bits\n1011               <--- divisor\n01100011101100 000 <--- result (note the first four bits are the XOR with the divisor beneath, the rest of the bits are unchanged)\n 1011              <--- divisor ...\n00111011101100 000\n  1011\n00010111101100 000\n   1011\n00000001101100 000 <--- note that the divisor moves over to align with the next 1 in the dividend (since quotient for that step was zero)\n       1011             (in other words, it doesn't necessarily move one bit per iteration)\n00000000110100 000\n        1011\n00000000011000 000\n         1011\n00000000001110 000\n          1011\n00000000000101 000\n           101 1\n-----------------\n00000000000000 100 <--- remainder (3 bits).  Division algorithm stops here as dividend is equal to zero.\n</pre>\n\nSince the leftmost divisor bit zeroed every input bit it touched, when this process ends the only bits in the input row that can be nonzero are the n bits at the right-hand end of the row. These ''n'' bits are the remainder of the division step, and will also be the value of the CRC function (unless the chosen CRC specification calls for some postprocessing).\n\nThe validity of a received message can easily be verified by performing the above calculation again, this time with the check value added instead of zeroes. The remainder should equal zero if there are no detectable errors.\n\n<pre>\n11010011101100 100 <--- input with check value\n1011               <--- divisor\n01100011101100 100 <--- result\n 1011              <--- divisor ...\n00111011101100 100\n\n......\n\n00000000001110 100\n          1011\n00000000000101 100\n           101 1\n------------------\n00000000000000 000 <--- remainder\n</pre>\n\nThe following [[Python (programming language)|Python]] code outlines a function which will return the initial CRC remainder for a chosen input and polynomial, with either 1 or 0 as the initial padding. Note that this code works with string inputs rather than raw numbers:\n<syntaxhighlight lang=\"python\">\ndef crc_remainder(input_bitstring, polynomial_bitstring, initial_filler):\n    '''\n    Calculates the CRC remainder of a string of bits using a chosen polynomial.\n    initial_filler should be '1' or '0'.\n    '''\n    polynomial_bitstring = polynomial_bitstring.lstrip('0')\n    len_input = len(input_bitstring)\n    initial_padding = initial_filler * (len(polynomial_bitstring) - 1)\n    input_padded_array = list(input_bitstring + initial_padding)\n    while '1' in input_padded_array[:len_input]:\n        cur_shift = input_padded_array.index('1')\n        for i in range(len(polynomial_bitstring)):\n            input_padded_array[cur_shift + i] = str(int(polynomial_bitstring[i] != input_padded_array[cur_shift + i]))\n    return ''.join(input_padded_array)[len_input:]\n\ndef crc_check(input_bitstring, polynomial_bitstring, check_value):\n    '''\n    Calculates the CRC check of a string of bits using a chosen polynomial.\n    '''\n    polynomial_bitstring = polynomial_bitstring.lstrip('0')\n    len_input = len(input_bitstring)\n    initial_padding = check_value\n    input_padded_array = list(input_bitstring + initial_padding)\n    while '1' in input_padded_array[:len_input]:\n        cur_shift = input_padded_array.index('1')\n        for i in range(len(polynomial_bitstring)):\n            input_padded_array[cur_shift + i] = str(int(polynomial_bitstring[i] != input_padded_array[cur_shift + i]))\n    return ('1' not in ''.join(input_padded_array)[len_input:])\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"pycon\">\n>>> crc_check('11010011101100','1011','100')\nTrue\n>>> crc_remainder('11010011101100','1011','0')\n'100'\n</syntaxhighlight>\n\n== Mathematics ==\n{{unreferenced section|date=July 2016}}\n{{Main|Mathematics of cyclic redundancy checks}}\nMathematical analysis of this division-like process reveals how to select a divisor that guarantees good error-detection properties. In this analysis, the digits of the bit strings are taken as the coefficients of a polynomial in some variable ''x''—coefficients that are elements of the finite field [[GF(2)]], instead of more familiar numbers. The set of binary polynomials is a mathematical [[ring (mathematics)|ring]].\n\n=== Designing polynomials ===\nThe selection of the generator polynomial is the most important part of implementing the CRC algorithm. The polynomial must be chosen to maximize the error-detecting capabilities while minimizing overall collision probabilities.\n\nThe most important attribute of the polynomial is its length (largest degree(exponent) +1 of any one term in the polynomial), because of its direct influence on the length of the computed check value.\n\nThe most commonly used polynomial lengths are:\n*9 bits (CRC-8)\n*17 bits (CRC-16)\n*33 bits (CRC-32)\n*65 bits (CRC-64)\n\nA CRC is called an ''n''-bit CRC when its check value is ''n''-bits. For a given ''n'', multiple CRCs are possible, each with a different polynomial. Such a polynomial has highest degree ''n'', and hence {{nowrap|''n'' + 1}} terms (the polynomial has a length of {{nowrap|''n'' + 1}}). The remainder has length ''n''. The CRC has a name of the form CRC-''n''-XXX.\n\nThe design of the CRC polynomial depends on the maximum total length of the block to be protected (data + CRC bits), the desired error protection features, and the type of resources for implementing the CRC, as well as the desired performance. A common misconception is that the \"best\" CRC polynomials are derived from either [[irreducible polynomial]]s or irreducible polynomials times the factor&nbsp;{{nowrap|1 + ''x''}}, which adds to the code the ability to detect all errors affecting an odd number of bits.<ref name=\"williams93\" /> In reality, all the factors described above should enter into the selection of the polynomial and may lead to a reducible polynomial. However, choosing a reducible polynomial will result in a certain proportion of missed errors, due to the quotient ring having [[zero divisor]]s.\n\nThe advantage of choosing a [[Primitive polynomial (field theory)|primitive polynomial]] as the generator for a CRC code is that the resulting code has maximal total block length in the sense that all 1-bit errors within that block length have different remainders (also called [[Syndrome decoding|syndromes]]) and therefore, since the remainder is a linear function of the block, the code can detect all 2-bit errors within that block length. If <math>r</math> is the degree of the primitive generator polynomial, then the maximal total block length is <math>2 ^ {r} - 1 </math>, and the associated code is able to detect any single-bit or double-bit errors.<ref>{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 22.4 Cyclic Redundancy and Other Checksums | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=1168}}</ref> We can improve this situation. If we use the generator polynomial <math>g(x) = p(x)(1 + x)</math>, where <math>p(x)</math> is a primitive polynomial of degree <math>r - 1</math>, then the maximal total block length is <math>2^{r - 1} - 1</math>, and the code is able to detect single, double, triple and any odd number of errors.\n\nA polynomial <math>g(x)</math> that admits other factorizations may be chosen then so as to balance the maximal total blocklength with a desired error detection power. The [[BCH code]]s are a powerful class of such polynomials. They subsume the two examples above. Regardless of the reducibility properties of a generator polynomial of degree&nbsp;''r'', if it includes the \"+1\" term, the code will be able to detect error patterns that are confined to a window of ''r'' contiguous bits. These patterns are called \"error bursts\".\n\n== Specification ==\nThe concept of the CRC as an error-detecting code gets complicated when an implementer or standards committee uses it to design a practical system. Here are some of the complications:\n*Sometimes an implementation '''prefixes a fixed bit pattern''' to the bitstream to be checked. This is useful when clocking errors might insert 0-bits in front of a message, an alteration that would otherwise leave the check value unchanged.\n*Usually, but not always, an implementation '''appends ''n'' 0-bits''' (''n'' being the size of the CRC) to the bitstream to be checked before the polynomial division occurs. Such appending is explicitly demonstrated in the [[Computation of cyclic redundancy checks|Computation of CRC]] article. This has the convenience that the remainder of the original bitstream with the check value appended is exactly zero, so the CRC can be checked simply by performing the polynomial division on the received bitstream and comparing the remainder with zero. Due to the associative and commutative properties of the exclusive-or operation, practical table driven implementations can obtain a result numerically equivalent to zero-appending without explicitly appending any zeroes, by using an equivalent,<ref name=\"williams93\" /> faster algorithm that combines the message bitstream with the stream being shifted out of the CRC register.\n*Sometimes an implementation '''exclusive-ORs a fixed bit pattern''' into the remainder of the polynomial division.\n*'''Bit order:''' Some schemes view the low-order bit of each byte as \"first\", which then during polynomial division means \"leftmost\", which is contrary to our customary understanding of \"low-order\". This convention makes sense when [[serial port|serial-port]] transmissions are CRC-checked in hardware, because some widespread serial-port transmission conventions transmit bytes least-significant bit first.\n*'''[[Byte order]]''': With multi-byte CRCs, there can be confusion over whether the byte transmitted first (or stored in the lowest-addressed byte of memory) is the least-significant byte (LSB) or the most-significant byte (MSB). For example, some 16-bit CRC schemes swap the bytes of the check value.\n*'''Omission of the high-order bit''' of the divisor polynomial: Since the high-order bit is always 1, and since an ''n''-bit CRC must be defined by an ({{nowrap|''n'' + 1}})-bit divisor which [[Arithmetic overflow|overflows]] an ''n''-bit [[processor register|register]], some writers assume that it is unnecessary to mention the divisor's high-order bit.\n*'''Omission of the low-order bit''' of the divisor polynomial: Since the low-order bit is always 1, authors such as Philip Koopman represent polynomials with their high-order bit intact, but without the low-order bit (the <math>x^0</math> or 1 term).  This convention encodes the polynomial complete with its degree in one integer.\n\nThese complications mean that there are three common ways to express a polynomial as an integer: the first two, which are mirror images in binary, are the constants found in code; the third is the number found in Koopman's papers.  ''In each case, one term is omitted.'' So the polynomial <math>x^4 + x + 1</math> may be transcribed as:\n*0x3 = 0b0011, representing <math>x^4 + (0x^3 + 0x^2 + 1x^1 + 1x^0)</math> (MSB-first code)\n*0xC = 0b1100, representing <math>(1x^0 + 1x^1 + 0x^2 + 0x^3) + x^4</math> (LSB-first code)\n*0x9 = 0b1001, representing <math>(1x^4 + 0x^3 + 0x^2 + 1x^1) + x^0</math> (Koopman notation)\nIn the table below they are shown as:\n{|class=\"wikitable\"\n|+ Examples of CRC representations\n|-\n! Name\n! Normal\n! Reversed\n! Reversed reciprocal\n|-\n| CRC-4\n| 0x3\n| 0xC\n| 0x9\n|}\n\n== Obfuscation ==\nCRCs in proprietary protocols might be [[Obfuscation|obfuscated]] by using a non-trivial initial value and a final XOR, but these techniques do not add cryptographic strength to the algorithm and can be [[Reverse engineering|reverse engineered]] using straightforward methods.<ref name=\"ewing-rev-eng\">{{Cite web|last=Ewing|first=Gregory C.|date=March 2010|title=Reverse-Engineering a CRC Algorithm|location=Christchurch|publisher=University of Canterbury|url=http://www.cosc.canterbury.ac.nz/greg.ewing/essays/CRC-Reverse-Engineering.html|accessdate=26 July 2011}}</ref>\n\n== Standards and common use ==\nNumerous varieties of cyclic redundancy checks have been incorporated into [[technical standard]]s.  By no means does one algorithm, or one of each degree, suit every purpose; Koopman and Chakravarty recommend selecting a polynomial according to the application requirements and the expected distribution of message lengths.<ref name=koop04 />  The number of distinct CRCs in use has confused developers, a situation which authors have sought to address.<ref name=\"williams93\">{{Cite web | url= http://www.wolfgang-ehrhardt.de/crc_v3.html | title= A Painless Guide to CRC Error Detection Algorithms V3.0 | accessdate= 23 May 2019 | last= Williams | first= Ross N. | date= 24 September 1996 }}</ref>  There are three polynomials reported for CRC-12,<ref name=koop04 /> twenty-two conflicting definitions of CRC-16, and seven of CRC-32.<ref name=\"cook-catalogue\">{{Cite web | last = Cook | first = Greg | url = http://reveng.sourceforge.net/crc-catalogue/all.htm | title = Catalogue of parametrised CRC algorithms | accessdate = 23 May 2019 | date = 6 May 2019 }}</ref>\n\nThe polynomials commonly applied are not the most efficient ones possible. Since 1993, Koopman, Castagnoli and others have surveyed the space of polynomials between 3 and 64 bits in size,<ref name=koop04>{{Cite book |  last = Koopman | first = Philip |  last2 = Chakravarty | first2 = Tridib |  title = Cyclic Redundancy Code (CRC) Polynomial Selection For Embedded Networks |  journal = The International Conference on Dependable Systems and Networks |date = June 2004 <!-- date from http://www.ece.cmu.edu/~koopman/pubs.html --> |  url = http://www.ece.cmu.edu/~koopman/roses/dsn04/koopman04_crc_poly_embedded.pdf |  accessdate = 14 January 2011 |  pages = 145–154 |  doi = 10.1109/DSN.2004.1311885 |  isbn = 978-0-7695-2052-0 | citeseerx = 10.1.1.648.9080 }}</ref><ref name=\"cast93\">{{Cite journal |  last = Castagnoli | first =  G. |  last2 = Bräuer | first2 = S. |  last3 = Herrmann | first3 = M. |date=June 1993 |  title = Optimization of Cyclic Redundancy-Check Codes with 24 and 32 Parity Bits |  journal =  IEEE Transactions on Communications |  volume = 41 | issue = 6| doi = 10.1109/26.231911 | pages = 883–892 }}</ref><ref name=\"koop02\">{{cite book |  last = Koopman | first = Philip |  title = 32-Bit Cyclic Redundancy Codes for Internet Applications |  journal = The International Conference on Dependable Systems and Networks |date=July 2002 <!-- date from http://www.ece.cmu.edu/~koopman/pubs.html --> |  url = http://www.ece.cmu.edu/~koopman/networks/dsn02/dsn02_koopman.pdf |  accessdate = 14 January 2011 |  pages = 459–468 |  doi = 10.1109/DSN.2002.1028931 |  isbn = 978-0-7695-1597-7 | citeseerx = 10.1.1.11.8323 }}</ref><ref name=\"koopman-best-crc-polys\">{{Cite web|last=Koopman|first=Philip|date=21 January 2016|title=Best CRC Polynomials|institution=Carnegie Mellon University|location=Pittsburgh|url=https://users.ece.cmu.edu/~koopman/crc/|accessdate=26 January 2016}}</ref> finding examples that have much better performance (in terms of [[Hamming distance]] for a given message size) than the polynomials of earlier protocols, and publishing the best of these with the aim of improving the error detection capacity of future standards.<ref name=\"koop02\" /> In particular, [[iSCSI]] and [[SCTP]] have adopted one of the findings of this research, the CRC-32C (Castagnoli) polynomial.\n\nThe design of the 32-bit polynomial most commonly used by standards bodies, CRC-32-IEEE, was the result of a joint effort for the [[Rome Laboratory]] and the Air Force Electronic Systems Division by Joseph Hammond, James Brown and Shyan-Shiang Liu of the [[Georgia Institute of Technology]] and Kenneth Brayer of the [[Mitre Corporation]]. The earliest known appearances of the 32-bit polynomial were in their 1975 publications: Technical Report 2956 by Brayer for Mitre, published in January and released for public dissemination through [[DTIC]] in August,<ref name=\"Brayer1975\">{{cite journal | last = Brayer | first = Kenneth | date = August 1975 | title = Evaluation of 32 Degree Polynomials in Error Detection on the SATIN IV Autovon Error Patterns | publisher = [[National Technical Information Service]] | page = 74 | url = http://www.dtic.mil/srch/doc?collection=t3&id=ADA014825 <!-- http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA014825 --> | accessdate = 3 February 2011 }}</ref> and Hammond, Brown and Liu's report for the Rome Laboratory, published in May.<ref name=\"Hammond1975\">{{cite journal | last = Hammond | first = Joseph L., Jr. | last2 = Brown | first2 = James E. | last3 = Liu | first3 = Shyan-Shiang | publication-date = May 1975 | title = Development of a Transmission Error Model and an Error Control Model | journal = NASA Sti/recon Technical Report N | url = http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA013939&Location=U2&doc=GetTRDoc.pdf | accessdate = 7 July 2012 | page = 74 | doi =  | bibcode = 1975STIN...7615344H | volume = 76 | year = 1975 }}</ref> Both reports contained contributions from the other team. During December 1975, Brayer and Hammond presented their work in a paper at the IEEE National Telecommunications Conference: the IEEE CRC-32 polynomial is the generating polynomial of a [[Hamming code]] and was selected for its error detection performance.<ref name=\"BrayerHammond1975\">{{cite conference | last = Brayer | first = Kenneth | last2 = Hammond | first2 = Joseph L., Jr. |date=December 1975 | title = Evaluation of error detection polynomial performance on the AUTOVON channel | conference = IEEE National Telecommunications Conference, New Orleans, La | booktitle = Conference Record | volume = 1 | publisher = Institute of Electrical and Electronics Engineers | location = New York | pages = 8–21 to 8–25 | bibcode = 1975ntc.....1....8B }}</ref>  Even so, the Castagnoli CRC-32C polynomial used in iSCSI or SCTP matches its performance on messages from 58 bits to 131 kbits, and outperforms it in several size ranges including the two most common sizes of Internet packet.<ref name=\"koop02\" /> The [[ITU-T]] [[G.hn]] standard also uses CRC-32C to detect errors in the payload (although it uses CRC-16-CCITT for [[Physical layer|PHY headers]]).\n\nCRC32 computation is implemented in hardware as an operation of [[SSE4#SSE4.2|SSE4.2]] instruction set, first introduced in [[Intel]] processors' [[Nehalem (microarchitecture)|Nehalem]] microarchitecture.\n\n== Polynomial representations of cyclic redundancy checks ==\nThe table below lists only the polynomials of the various algorithms in use. Variations of a particular protocol can impose pre-inversion, post-inversion and reversed bit ordering as described above. For example, the CRC32 used in Gzip and Bzip2 use the same polynomial, but Gzip employs reversed bit ordering, while Bzip2 does not.<ref name=\"cook-catalogue\" />\n\n{{anchor|table}}\n{| class=\"wikitable\"\n! rowspan=\"2\" | Name\n! rowspan=\"2\" | Uses\n! colspan=\"4\" | [[Mathematics of cyclic redundancy checks#Polynomial representations|Polynomial representations]]\n! rowspan=\"2\" | [[Parity (telecommunication)|Parity]]<ref>CRCs with even parity detect any odd number of bit errors, at the expense of lower hamming distance for long payloads. Note that parity is computed over the entire generator polynomial, including implied 1 at the beginning or the end. For example, the full representation of CRC-1 is 0x3, which has two 1 bits. Thus, its parity is even.</ref>\n! rowspan=\"2\" | Primitive<ref name=\"users.ece.cmu.edu\">{{cite web|url=https://users.ece.cmu.edu/~koopman/crc/crc32.html|title=32 Bit CRC Zoo|author=|date=|website=users.ece.cmu.edu}}</ref>\n! colspan=\"15\" | Maximum bits of payload by [[Hamming distance]]<ref>Payload means length exclusive of CRC field. A Hamming distance of ''d'' means that ''d''&nbsp;−&nbsp;1 bit errors can be detected and ⌊(''d''&nbsp;−&nbsp;1)/2⌋ bit errors can be corrected</ref><ref name=\"koop02\" /><ref name=\"users.ece.cmu.edu\"/>\n|-\n! Normal\n! [[Mathematics of cyclic redundancy checks#Reversed representations and reciprocal polynomials|Reversed]]\n! [[Mathematics of cyclic redundancy checks#Reciprocal polynomials|Reciprocal]]\n! Reversed reciprocal\n! ≥&nbsp;16\n! 15\n! 14\n! 13\n! 12\n! 11\n! 10\n! 9\n! 8\n! 7\n! 6\n! 5\n! 4\n! 3\n! 2<ref>is always achieved for arbitrarily long messages</ref>\n|-\n| rowspan=\"2\" | CRC-1\n| rowspan=\"2\" | most hardware; also known as ''[[parity bit]]''\n| 0x1\n| 0x1\n| 0x1\n| 0x1\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x + 1</math>\n|-\n| rowspan=\"2\" | CRC-3-[[GSM]]\n| rowspan=\"2\" | mobile networks<ref name=\"ts-100-909\">{{Cite book|title=ETSI TS 100 909|version=V8.9.0|date=January 2005|publisher=European Telecommunications Standards Institute|location=Sophia Antipolis, France|url=https://www.etsi.org/deliver/etsi_ts/100900_100999/100909/08.09.00_60/ts_100909v080900p.pdf|accessdate=21 October 2016}}</ref>\n| 0x3\n| 0x6\n| 0x5\n| 0x5\n| rowspan=\"2\" {{Bad|odd}}\n| rowspan=\"2\" {{Good|yes}} <ref name=\"koop_crc3\">{{cite web|url=https://users.ece.cmu.edu/~koopman/crc/crc3.html|title=3 Bit CRC Zoo|author=|date=|website=users.ece.cmu.edu}}</ref>\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 4\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^3 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-4-ITU\n| rowspan=\"2\" | [[ITU-T]] [http://www.itu.int/rec/T-REC-G.704-199810-I/en G.704], p.&nbsp;12\n| 0x3\n| 0xC\n| 0x9\n| 0x9\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^4 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-5-EPC\n| rowspan=\"2\" | [[Radio-frequency identification|Gen 2 RFID]]<ref name=\"gen-2-spec\">{{Cite book|title=Class-1 Generation-2 UHF RFID Protocol|version=1.2.0|publisher=[[EPCglobal]]|url=http://www.gs1.org/gsmp/kc/epcglobal/uhfc1g2/uhfc1g2_1_2_0-standard-20080511.pdf|date=23 October 2008|accessdate=4 July 2012|page=35}} (Table 6.12)</ref>\n| 0x09\n| 0x12\n| 0x05\n| 0x14\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^5 + x^3 + 1</math>\n|-\n| rowspan=\"2\" | CRC-5-ITU\n| rowspan=\"2\" | ITU-T [http://www.itu.int/rec/T-REC-G.704-199810-I/en G.704], p.&nbsp;9\n| 0x15\n| 0x15\n| 0x0B\n| 0x1A\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^5 + x^4 + x^2 + 1</math>\n|-\n| rowspan=\"2\" | CRC-5-USB\n| rowspan=\"2\" | [[Universal Serial Bus|USB]] token packets\n| 0x05\n| 0x14\n| 0x09\n| 0x12\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^5 + x^2 + 1</math>\n|-\n| CRC-6-[[CDMA2000]]-A\n| mobile networks<ref name=\"cdma2000-spec\">{{Cite book|publisher=3rd Generation Partnership Project 2| date=October 2005 |title=Physical layer standard for cdma2000 spread spectrum systems|version=Revision D version 2.0|pages=2–89–2–92|url=http://www.3gpp2.org/public_html/specs/C.S0002-D_v2.0_051006.pdf|accessdate=14 October 2013}}</ref>\n| 0x27\n| 0x39\n| 0x33\n| 0x33\n| {{Bad|odd}}\n|-\n| CRC-6-[[CDMA2000]]-B\n| mobile networks<ref name=\"cdma2000-spec\" />\n| 0x07\n| 0x38\n| 0x31\n| 0x23\n| {{Good|even}}\n|-\n| CRC-6-DARC\n| [[Data Radio Channel]]<ref name=\"en-300-751\">{{Cite book|title=ETSI EN 300 751|version=V1.2.1|date=January 2003|publisher=European Telecommunications Standards Institute|location=Sophia Antipolis, France|url=http://www.etsi.org/deliver/etsi_en/300700_300799/300751/01.02.01_60/en_300751v010201p.pdf|section=11. Error correction strategy|pages=67–8|accessdate=26 January 2016}}</ref>\n| 0x19\n| 0x26\n| 0x0D\n| 0x2C\n| {{Good|even}}\n|-\n| rowspan=\"2\" | CRC-6-[[GSM]]\n| rowspan=\"2\" | mobile networks<ref name=\"ts-100-909\" />\n| 0x2F\n| 0x3D\n| 0x3B\n| 0x37\n| rowspan=\"2\" {{Good|even}}\n| rowspan=\"2\" {{Good|yes}} <ref name=\"koop_crc6\">{{cite web|url=https://users.ece.cmu.edu/~koopman/crc/crc6.html|title=6 Bit CRC Zoo|author=|date=|website=users.ece.cmu.edu}}</ref>\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 1\n| rowspan=\"2\" | 1\n| rowspan=\"2\" | 25\n| rowspan=\"2\" | 25\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^6 + x^5 + x^3 + x^2 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-6-ITU\n| rowspan=\"2\" | ITU-T [http://www.itu.int/rec/T-REC-G.704-199810-I/en G.704], p.&nbsp;3\n| 0x03\n| 0x30\n| 0x21\n| 0x21\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^6 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-7\n| rowspan=\"2\" | telecom systems, ITU-T [http://www.itu.int/rec/T-REC-G.707/en G.707], ITU-T [http://www.itu.int/rec/T-REC-G.832/en G.832], [[MultiMediaCard|MMC]], [[Secure Digital card|SD]]\n| 0x09\n| 0x48\n| 0x11\n| 0x44\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^7 + x^3 + 1</math>\n|-\n| CRC-7-MVB\n| [[Train Communication Network]], [[IEC 60870-5]]<ref name=\"chakravarty-thesis\">{{Cite thesis|last=Chakravarty|first=Tridib|others=Philip Koopman, advisor|date=December 2001|title=Performance of Cyclic Redundancy Codes for Embedded Networks|publisher=Carnegie Mellon University|location=Pittsburgh|url=http://www.ece.cmu.edu/~koopman/thesis/chakravarty.pdf|accessdate=8 July 2013|pages=5,18}}</ref>\n| 0x65\n| 0x53\n| 0x27\n| 0x72\n| {{Bad|odd}}\n|-\n| rowspan=\"2\" | CRC-8\n| rowspan=\"2\" | [[DVB-S2]]<ref name=\"en-302-307\">{{Cite book|title=EN 302 307|version=V1.3.1|date=March 2013|publisher=European Telecommunications Standards Institute|location=Sophia Antipolis, France|url=https://www.etsi.org/deliver/etsi_en/302300_302399/302307/01.03.01_60/en_302307v010301p.pdf|section=5.1.4 CRC-8 encoder (for packetized streams only)|page=17|accessdate=29 July 2016}}</ref>\n| 0xD5\n| 0xAB\n| 0x57\n| 0xEA<ref name=\"koop04\" />\n| rowspan=\"2\" {{Good|even}}\n| rowspan=\"2\" {{Bad|no}} <ref name=\"koop_crc8\">{{cite web|url=https://users.ece.cmu.edu/~koopman/crc/crc8.html|title=8 Bit CRC Zoo|author=|date=|website=users.ece.cmu.edu}}</ref>\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 2\n| rowspan=\"2\" | 2\n| rowspan=\"2\" | 85\n| rowspan=\"2\" | 85\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^8 + x^7 + x^6 + x^4 + x^2 + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-[[AUTOSAR]]\n| rowspan=\"2\" | automotive integration,<ref name=\"autosar-crc\">{{Cite book|title=Specification of CRC Routines|version=4.2.2|date=22 July 2015|publisher=AUTOSAR|location=Munich|url=https://www.autosar.org/fileadmin/files/releases/4-2/software-architecture/safety-and-security/standard/AUTOSAR_SWS_CRCLibrary.pdf|section=7.2.1.2 8-bit 0x2F polynomial CRC Calculation|page=24|accessdate=24 July 2016}}</ref> [[OpenSafety]]<ref name=\"opensafety-profile\">{{Cite book|title=openSAFETY Safety Profile Specification: EPSG Working Draft Proposal 304|version=1.4.0|date=13 March 2013|publisher=Ethernet POWERLINK Standardisation Group|location=Berlin|url=http://www.ethernet-powerlink.org/en/downloads/technical-documents/action/open-download/download/epsg-wdp-304-v-1-4-0/?no_cache=1|section=5.1.1.8 Cyclic Redundancy Check field (CRC-8 / CRC-16)|page=42|accessdate=22 July 2016}}</ref>\n| 0x2F\n| 0xF4\n| 0xE9\n| 0x97<ref name=\"koop04\" />\n| rowspan=\"2\" {{Good|even}}\n| rowspan=\"2\" {{Good|yes}} <ref name=\"koop_crc8\" />\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 3\n| rowspan=\"2\" | 3\n| rowspan=\"2\" | 119\n| rowspan=\"2\" | 119\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^8 + x^5 + x^3 + x^2 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-[[Bluetooth]]\n| rowspan=\"2\" | wireless connectivity<ref name=\"core-4.2\">{{Cite book|title=Specification of the Bluetooth System|volume=2|section=B.7.1.1 HEC generation|pages=144–5|publisher=Bluetooth SIG|date=2 December 2014|url=https://www.bluetooth.org/DocMan/handlers/DownloadDoc.ashx?doc_id=286439|accessdate=20 October 2014}}</ref>\n| 0xA7\n| 0xE5\n| 0xCB\n| 0xD3\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^8 + x^7 + x^5 + x^2 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-[[CCITT]]\n| rowspan=\"2\" | ITU-T [http://www.itu.int/rec/T-REC-I.432.1-199902-I/en I.432.1 (02/99)]; [[Asynchronous Transfer Mode|ATM]] [[Header Error Correction|HEC]], [[ISDN]] HEC and cell delineation\n| 0x07\n| 0xE0\n| 0xC1\n| 0x83\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^8 + x^2 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-[[Dallas Semiconductor|Dallas]]/[[Maxim Integrated Products|Maxim]]\n| rowspan=\"2\" | [[1-Wire]] [[Bus (computing)|bus]]<ref name=\"Whitfield01\">{{Cite web|author1=Harry Whitfield|url=http://homepages.cs.ncl.ac.uk/harry.whitfield/home.formal/CRCs.html|title=XFCNs for Cyclic Redundancy Check Calculations|archiveurl=https://web.archive.org/web/20050525224339/http://homepages.cs.ncl.ac.uk/harry.whitfield/home.formal/CRCs.html|archivedate=May 25, 2005|date=April 24, 2001}}</ref>\n| 0x31\n| 0x8C\n| 0x19\n| 0x98\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^8 + x^5 + x^4 + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-DARC\n| rowspan=\"2\" | [[Data Radio Channel]]<ref name=\"en-300-751\" />\n| 0x39\n| 0x9C\n| 0x39\n| 0x9C\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^8 + x^5 + x^4 + x^3 + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-[[GSM]]-B\n| rowspan=\"2\" | mobile networks<ref name=\"ts-100-909\" />\n| 0x49\n| 0x92\n| 0x25\n| 0xA4\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^8 + x^6 + x^3 + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-[[SAE J1850]]\n| rowspan=\"2\" | [[AES3]]; [[On-board_diagnostics|OBD]]\n| 0x1D\n| 0xB8\n| 0x71\n| 0x8E\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^8 + x^4 + x^3 + x^2 + 1</math>\n|-\n| rowspan=\"2\" | CRC-8-[[W-CDMA (UMTS)|WCDMA]]\n| rowspan=\"2\" | mobile networks<ref name=\"cdma2000-spec\" /><ref name=\"richardson-wcdma\">{{Cite book|last=Richardson|first=Andrew|title=WCDMA Handbook|location=Cambridge, UK|publisher=Cambridge University Press|date=17 March 2005|isbn=978-0-521-82815-4|page=223|url=https://books.google.com/books?id=yN5lve5L4vwC&lpg=PA223&dq=&pg=PA223#v=onepage&q&f=false}}</ref>\n| 0x9B\n| 0xD9\n| 0xB3\n| 0xCD<ref name=\"koop04\" />\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^8 + x^7 + x^4 + x^3 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-10\n| rowspan=\"2\" | ATM; ITU-T [http://www.itu.int/rec/T-REC-I.610/en I.610]\n| 0x233\n| 0x331\n| 0x263\n| 0x319\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{10} + x^9 + x^5 + x^4 + x + 1</math>\n|-\n| CRC-10-[[CDMA2000]]\n| mobile networks<ref name=\"cdma2000-spec\" />\n| 0x3D9\n| 0x26F\n| 0x0DF\n| 0x3EC\n| {{Good|even}}\n|-\n| CRC-10-[[GSM]]\n| mobile networks<ref name=\"ts-100-909\" />\n| 0x175\n| 0x2BA\n| 0x175\n| 0x2BA\n| {{Bad|odd}}\n|-\n| rowspan=\"2\" | CRC-11\n| rowspan=\"2\" | [[FlexRay]]<ref name=\"flexray-spec\">{{Cite book|title=FlexRay Protocol Specification|version=3.0.1|publisher=Flexray Consortium|date=October 2010|page=114}} (4.2.8 Header CRC (11 bits))</ref>\n| 0x385\n| 0x50E\n| 0x21D\n| 0x5C2\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{11} + x^9 + x^8 + x^7 + x^2 + 1</math>\n|-\n| rowspan=\"2\" | CRC-12\n| rowspan=\"2\" | telecom systems<ref>{{Cite journal | last = Perez | first = A. | title =  Byte-Wise CRC Calculations | journal = IEEE Micro | volume = 3 | issue = 3 | pages = 40–50 | year = 1983 | doi = 10.1109/MM.1983.291120}}</ref><ref>{{Cite journal | last = Ramabadran | first =  T.V. | last2 = Gaitonde | first2 = S.S. | title = A tutorial on CRC computations | journal = IEEE Micro | volume = 8 | issue = 4 | year = 1988 | pages = 62–75 | doi = 10.1109/40.7773 }}</ref>\n| 0x80F\n| 0xF01\n| 0xE03\n| 0xC07<ref name=\"koop04\" />\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{12} + x^{11} + x^3 + x^2 + x + 1</math>\n|-\n| CRC-12-[[CDMA2000]]\n| mobile networks<ref name=\"cdma2000-spec\" />\n| 0xF13\n| 0xC8F\n| 0x91F\n| 0xF89\n| {{Good|even}}\n|-\n| CRC-12-[[GSM]]\n| mobile networks<ref name=\"ts-100-909\" />\n| 0xD31\n| 0x8CB\n| 0x197\n| 0xE98\n| {{Bad|odd}}\n|-\n| rowspan=\"2\" | CRC-13-BBC\n| rowspan=\"2\" | Time signal, [[Radio teleswitch]]<ref>http://www.freescale.com/files/microcontrollers/doc/app_note/AN1597.pdf</ref><ref>{{Cite book|last=Ely|first=S.R.|last2=Wright|first2=D.T.|title=L.F. Radio-Data: specification of BBC experimental transmissions 1982|publisher=Research Department, Engineering Division, The British Broadcasting Corporation| date=March 1982 |page=9|url=http://downloads.bbc.co.uk/rd/pubs/reports/1982-02.pdf|accessdate=11 October 2013}}</ref>\n| 0x1CF5\n| 0x15E7\n| 0x0BCF\n| 0x1E7A\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{13} + x^{12} + x^{11} + x^{10} + x^7 + x^6 + x^5 + x^4 + x^2 + 1</math>\n|-\n| CRC-14-DARC\n| [[Data Radio Channel]]<ref name=\"en-300-751\" />\n| 0x0805\n| 0x2804\n| 0x1009\n| 0x2402\n| {{Good|even}}\n|-\n| CRC-14-[[GSM]]\n| mobile networks<ref name=\"ts-100-909\" />\n| 0x202D\n| 0x2D01\n| 0x1A03\n| 0x3016\n| {{Good|even}}\n|- id=\"CRC-15-CAN\"\n| rowspan=\"2\" | CRC-15-[[Controller Area Network|CAN]]\n| rowspan=\"2\" |\n| 0x4599<ref name=\"cypress-psoc\">{{Cite book|date=20 February 2013|institution=Cypress Semiconductor|title=Cyclic Redundancy Check (CRC): PSoC Creator™ Component Datasheet|page=4|url=http://www.cypress.com/file/128066/download|accessdate=26 January 2016}}</ref><ref name=\"cia-can-crc\">{{Cite web|url = http://www.can-cia.org/can-knowledge/can/crc/|title = Cyclic redundancy check (CRC) in CAN frames|access-date = 26 January 2016|website = CAN in Automation}}</ref>\n| 0x4CD1\n| 0x19A3\n| 0x62CC\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{15} + x^{14} + x^{10} + x^8 + x^7 + x^4 + x^3 + 1 </math>\n|-\n| CRC-15-[[MPT1327]]\n| <ref name=\"mpt1327\">{{Cite book | date=June 1997 |title=A signalling standard for trunked private land mobile radio systems (MPT 1327) |edition=3rd |publisher=[[Ofcom]] |section=3.2.3 Encoding and error checking |page=3 |url=http://www.ofcom.org.uk/static/archive/ra/publication/mpt/mpt_pdf/mpt1327.pdf |accessdate=16 July 2012}}</ref>\n| 0x6815\n| 0x540B\n| 0x2817\n| 0x740A\n| {{Bad|odd}}\n|-\n| CRC-16-Chakravarty\n| Optimal for payloads ≤64 bits<ref name=\"chakravarty-thesis\" />\n| 0x2F15\n| 0xA8F4\n| 0x51E9\n| 0x978A\n| {{Bad|odd}}\n|-\n| CRC-16-[[ARINC]]\n| [[Aircraft Communications Addressing and Reporting System|ACARS]] applications<ref name=\"rehmann-acars\">{{Cite journal | url=http://ntl.bts.gov/lib/1000/1200/1290/tn95_66.pdf | title=Air Ground Data Link VHF Airline Communications and Reporting System (ACARS) Preliminary Test Report |date=February 1995 | page=5 | last1=Rehmann | first1=Albert | last2=Mestre | first2=José D. | publisher=Federal Aviation Authority Technical Center | accessdate=7 July 2012}}</ref>\n| 0xA02B\n| 0xD405\n| 0xA80B\n| 0xD015\n| {{Bad|odd}}\n|-\n| rowspan=\"2\" | CRC-16-CCITT\n| rowspan=\"2\" | [[X.25]], [[V.41]], [[HDLC]] ''FCS'', [[XMODEM]], [[Bluetooth]], [[PACTOR]], [[Secure Digital card|SD]], [[DigRF]], many others; known as ''CRC-CCITT''\n| 0x1021\n| 0x8408\n| 0x811\n| 0x8810<ref name=\"koop04\" />\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{16} + x^{12} + x^5 + 1</math>\n|-\n| CRC-16-[[CDMA2000]]\n| mobile networks<ref name=\"cdma2000-spec\" />\n| 0xC867\n| 0xE613\n| 0xCC27\n| 0xE433\n| {{Bad|odd}}\n|-\n| rowspan=\"2\" | CRC-16-[[Digital Enhanced Cordless Telecommunications|DECT]]\n| rowspan=\"2\" | cordless telephones<ref name=\"en-300-175-3\">{{Cite book|title=ETSI EN 300 175-3|version=V2.5.1|date=August 2013|publisher=European Telecommunications Standards Institute|location=Sophia Antipolis, France|url=http://www.etsi.org/deliver/etsi_en/300100_300199/30017503/02.05.01_60/en_30017503v020501p.pdf|section=6.2.5 Error control|pages=99,101|accessdate=26 January 2016}}</ref>\n| 0x0589\n| 0x91A0\n| 0x2341\n| 0x82C4\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{16} + x^{10} + x^8 + x^7 + x^3 + 1</math>\n|-\n| rowspan=\"2\" | CRC-16-[[International Committee for Information Technology Standards|T10]]-[[Data Integrity Field|DIF]]\n| rowspan=\"2\" | [[SCSI]] DIF\n| 0x8BB7<ref name=\"thaler-t10-selection\">{{Cite journal|last=Thaler|first=Pat|title=16-bit CRC polynomial selection|publisher=INCITS T10|date=28 August 2003|url=http://www.t10.org/ftp/t10/document.03/03-290r0.pdf|accessdate=11 August 2009}}</ref>\n| 0xEDD1\n| 0xDBA3\n| 0xC5DB\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^{16} + x^{15} + x^{11} + x^{9} + x^8 + x^7 + x^5 + x^4 + x^2 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-16-[[DNP3|DNP]]\n| rowspan=\"2\" | DNP, [[IEC 60870-5|IEC 870]], [[Meter-Bus|M-Bus]]\n| 0x3D65\n| 0xA6BC\n| 0x4D79\n| 0x9EB2\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{16} + x^{13} + x^{12} + x^{11} + x^{10} + x^8 + x^6 + x^5 + x^2 + 1</math>\n|-\n| rowspan=\"2\" | CRC-16-[[IBM]]\n| rowspan=\"2\" | [[Binary Synchronous Communications|Bisync]], [[Modbus]], [[Universal Serial Bus|USB]], [[ANSI]] [https://web.archive.org/web/20091001172850/http://www.incits.org/press/1997/pr97020.htm X3.28], SIA DC-07, many others; also known as ''CRC-16'' and ''CRC-16-ANSI''\n| 0x8005\n| 0xA001\n| 0x4003\n| 0xC002\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{16} + x^{15} + x^2 + 1</math>\n|-\n| CRC-16-[[OpenSafety]]-A\n| safety fieldbus<ref name=\"opensafety-profile\" />\n| 0x5935\n| 0xAC9A\n| 0x5935\n| 0xAC9A<ref name=\"koop04\" />\n| {{Bad|odd}}\n|-\n| CRC-16-[[OpenSafety]]-B\n| safety fieldbus<ref name=\"opensafety-profile\" />\n| 0x755B\n| 0xDAAE\n| 0xB55D\n| 0xBAAD<ref name=\"koop04\" />\n| {{Bad|odd}}\n|-\n| CRC-16-[[Profibus]]\n| fieldbus networks<ref name=\"profibus-spec\">{{Cite book|title=PROFIBUS Specification Normative Parts|version=1.0|publisher=Profibus International|date=March 1998|volume=9|section=8.8.4 Check Octet (FCS)|page=906|url=https://www.kuebler.com/PDFs/Feldbus_Multiturn/specification_DP.pdf|accessdate=9 July 2016}}</ref>\n| 0x1DCF\n| 0xF3B8\n| 0xE771\n| 0x8EE7\n| {{Bad|odd}}\n|-\n| Fletcher-16\n| Used in [[Adler-32]] A & B Checksums\n| colspan=\"4\" | Often confused to be a CRC, but actually a checksum; see [[Fletcher's checksum]]\n|-\n| CRC-17-CAN\n| CAN FD<ref name=\"can-fd-spec\">{{Cite book|title=CAN with Flexible Data-Rate Specification|version=1.0|publisher=Robert Bosch GmbH|date=April 17, 2012|page=13|url=http://www.bosch-semiconductors.de/media/pdf_1/canliteratur/can_fd_spec.pdf|deadurl=yes|archiveurl=https://web.archive.org/web/20130822124728/http://www.bosch-semiconductors.de/media/pdf_1/canliteratur/can_fd_spec.pdf|archivedate=22 August 2013|df=dmy-all}} (3.2.1 DATA FRAME)</ref>\n| 0x1685B\n| 0x1B42D\n| 0x1685B\n| 0x1B42D\n| {{Good|even}}\n|-\n| CRC-21-CAN\n| CAN FD<ref name=\"can-fd-spec\" />\n| 0x102899\n| 0x132281\n| 0x064503\n| 0x18144C\n| {{Good|even}}\n|-\n| rowspan=\"2\" | CRC-24\n| rowspan=\"2\" | [[FlexRay]]<ref name=\"flexray-spec\" />\n| 0x5D6DCB\n| 0xD3B6BA\n| 0xA76D75\n| 0xAEB6E5\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{24} + x^{22} + x^{20} + x^{19} + x^{18} + x^{16} + x^{14} + x^{13} + x^{11} + x^{10} + x^8 + x^7 + x^6 + x^3 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-24-[[Radix-64]]\n| rowspan=\"2\" | [[Pretty Good Privacy#OpenPGP|OpenPGP]], [[RTCM]]104v3\n| 0x864CFB\n| 0xDF3261\n| 0xBE64C3\n| 0xC3267D\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math> x^{24} + x^{23} + x^{18} + x^{17} + x^{14} + x^{11} + x^{10} + x^7 + x^6 + x^5 + x^4 + x^3 + x + 1 </math>\n|-\n| rowspan=\"2\" | CRC-24-[[WCDMA]]\n| rowspan=\"2\" | Used in [[OS-9|OS-9 RTOS]]. Residue = 0x800FE3.<ref>{{cite web|url=http://www.roug.org/soren/6809/os9sysprog.html#f.crc|title=OS-9 Operating System System Programmer's Manual|author=|date=|website=www.roug.org}}</ref>\n| 0x800063\n| 0xC60001\n| 0x8C0003\n| 0xC00031\n| rowspan=\"2\" {{Good|even}}\n| rowspan=\"2\" {{Good|yes}}<ref name=\"koop_crc24\">{{cite web|url=https://users.ece.cmu.edu/~koopman/crc/crc8.html|title=24 Bit CRC Zoo|author=Philip P. Koopman|date=20 May 2018|website=users.ece.cmu.edu}}</ref>\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 4\n| rowspan=\"2\" | 4\n| rowspan=\"2\" | 8388583\n| rowspan=\"2\" | 8388583\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^{24} + x^{23} + x^6 + x^5 + x + 1 </math>\n|-\n| rowspan=\"2\" | CRC-30\n| rowspan=\"2\" | [[CDMA]]\n| 0x2030B9C7\n| 0x38E74301\n| 0x31CE8603\n| 0x30185CE3\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{30} + x^{29} + x^{21} + x^{20} + x^{15} + x^{13} + x^{12} + x^{11} + x^{8} + x^{7} + x^{6} + x^{2} + x + 1 </math>\n|-\n| rowspan=\"2\" | CRC-32\n| rowspan=\"2\" | [[ISO]] 3309 ([[High-Level Data Link Control|HDLC]]), [[ANSI]] X3.66 ([[ADCCP]]), [[Federal Information Processing Standard|FIPS]] PUB 71, FED-STD-1003, ITU-T [[V.42]], ISO/IEC/IEEE 802-3 ([[Ethernet]]), [[SATA]], [[MPEG-2]], [[PKZIP]], [[Gzip]], [[Bzip2]], [[POSIX]] [[cksum]],<ref>{{cite web|url=http://pubs.opengroup.org/onlinepubs/9699919799/utilities/cksum.html|title=cksum|author=|date=|website=pubs.opengroup.org}}</ref> [[Portable Network Graphics|PNG]],<ref>{{Cite web | last = Boutell | first = Thomas | last2 = Randers-Pehrson | first2 = Glenn  | date = 14 July 1998 | title = PNG (Portable Network Graphics) Specification, Version 1.2 | url = http://www.libpng.org/pub/png/spec/1.2/PNG-Structure.html |publisher=Libpng.org | accessdate = 3 February 2011 |display-authors=etal}}</ref> [[ZMODEM]], many others\n| 0x04C11DB7\n| 0xEDB88320\n| 0xDB710641\n| 0x82608EDB<ref name=\"koop02\" />\n| rowspan=\"2\" {{Bad|odd}}\n| rowspan=\"2\" {{Good|yes}}\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 10\n| rowspan=\"2\" | –\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 12\n| rowspan=\"2\" | 21\n| rowspan=\"2\" | 34\n| rowspan=\"2\" | 57\n| rowspan=\"2\" | 91\n| rowspan=\"2\" | 171\n| rowspan=\"2\" | 268\n| rowspan=\"2\" | 2974\n| rowspan=\"2\" | 91607\n| rowspan=\"2\" | 4294967263\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^{32} + x^{26} + x^{23} + x^{22} + x^{16} + x^{12} + x^{11} + x^{10} + x^8 + x^7 + x^5 + x^4 + x^2 + x + 1</math>\n|-\n| rowspan=\"2\" | {{visible anchor|CRC-32C}} (Castagnoli)\n| rowspan=\"2\" | [[iSCSI]], [[SCTP]], [[G.hn]] payload, [[SSE4#SSE4.2|SSE4.2]], [[Btrfs]], [[ext4]], [[Ceph (software)|Ceph]]\n| 0x1EDC6F41\n| 0x82F63B78\n| 0x05EC76F1\n| 0x8F6E37A0<ref name=\"koop02\" />\n| rowspan=\"2\" {{Good|even}}\n| rowspan=\"2\" {{Good|yes}}\n| rowspan=\"2\" | 6\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 8\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 20\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 47\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 177\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 5243\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 2147483615\n| rowspan=\"2\" | –\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^{32} + x^{28} + x^{27} + x^{26} + x^{25} + x^{23} + x^{22} + x^{20} + x^{19} + x^{18} + x^{14} + x^{13} + x^{11} + x^{10} + x^9 + x^8 + x^6 + 1</math>\n|-\n| rowspan=\"2\" | {{visible anchor|CRC-32K}} (Koopman {1,3,28})\n| rowspan=\"2\" | Excellent at Ethernet frame length, poor performance with long files\n| 0x741B8CD7\n| 0xEB31D82E\n| 0xD663B05D\n| 0xBA0DC66B<ref name=\"koop02\" />\n| rowspan=\"2\" {{Good|even}}\n| rowspan=\"2\" {{Bad|no}}\n| rowspan=\"2\" | 2\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 4\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 16\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 18\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 152\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 16360\n| rowspan=\"2\" | –\n| rowspan=\"2\" | 114663\n| rowspan=\"2\" | –\n| rowspan=\"2\" | ∞\n|-\n| colspan=\"4\" | <math>x^{32} + x^{30} + x^{29} + x^{28} + x^{26} + x^{20} + x^{19} + x^{17} + x^{16} + x^{15} + x^{11} + x^{10} + x^{7} + x^{6} + x^{4} + x^{2} + x + 1</math>\n|-\n|CRC-32K<sub>2</sub> (Koopman {1,1,30})\n| Excellent at Ethernet frame length, poor performance with long files\n| 0x32583499\n| 0x992C1A4C\n| 0x32583499\n| 0x992C1A4C<ref name=\"koop02\" />\n| {{Good|even}}\n| {{Bad|no}}\n| –\n| –\n| 3\n| –\n| 16\n| –\n| 26\n| –\n| 134\n| –\n| 32738\n| –\n| 65506\n| –\n| ∞\n|-\n| rowspan=\"2\" | CRC-32Q\n| rowspan=\"2\" | aviation; [[AIXM]]<ref name=\"aixm-primer\">{{Cite book|title=AIXM Primer|url=http://www.eurocontrol.int/sites/default/files/service/content/documents/information-management/20060320-aixm-primer.pdf|version=4.5|publisher=[[European Organisation for the Safety of Air Navigation]]|date=20 March 2006|accessdate=3 Feb 2019}}</ref>\n| 0x814141AB\n| 0xD5828281\n| 0xAB050503\n| 0xC0A0A0D5\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{32} + x^{31} + x^{24} + x^{22} + x^{16} + x^{14} + x^{8} + x^{7} + x^{5} + x^{3} + x + 1</math>\n|-\n| Adler-32\n|\n| colspan=\"4\" | Often confused to be a CRC, but actually a checksum; see [[Adler-32]]\n|-\n| rowspan=\"2\" | CRC-40-[[GSM]]\n| rowspan=\"2\" | GSM control channel<ref>[http://www.etsi.org/deliver/etsi_ts/100900_100999/100909/08.09.00_60/ts_100909v080900p.pdf ETSI TS 100 909] version 8.9.0 (January 2005), Section 4.1.2 a</ref><ref name=\"gammel\">{{Cite book |last=Gammel|first=Berndt M.|date=31 October 2005|title=Matpack documentation: Crypto – Codes <!-- |unusedurl=http://users.physik.tu-muenchen.de/gammel/matpack/html/LibDoc/Crypto/MpCRC.html long-time home, gone--> |url=http://www.matpack.de/index.html#DOWNLOAD |publisher=Matpack.de |accessdate=21 April 2013}} (Note: MpCRC.html is included with the Matpack compressed software source code, under /html/LibDoc/Crypto)</ref><ref name=\"geremia\">{{Cite journal|last=Geremia|first=Patrick|date=April 1999|title=Cyclic redundancy check computation: an implementation using the TMS320C54x|issue=SPRA530|publisher=Texas Instruments|page=5|url=http://www.ti.com/lit/an/spra530/spra530.pdf|accessdate=4 July 2012}}</ref>\n| 0x0004820009\n| 0x9000412000\n| 0x2000824001\n| 0x8002410004\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{40} + x^{26} + x^{23} + x^{17} + x^3 + 1 = (x^{23} + 1) (x^{17} + x^3 + 1)</math>\n|-\n| rowspan=\"2\" | CRC-64-[[Ecma International|ECMA]]\n| rowspan=\"2\" | [http://www.ecma-international.org/publications/standards/Ecma-182.htm ECMA-182]  p.&nbsp;51, [[XZ Utils]]\n| 0x42F0E1EBA9EA3693\n| 0xC96C5795D7870F42\n| 0x92D8AF2BAF0E1E85\n| 0xA17870F5D4F51B49\n| rowspan=\"2\" {{Good|even}}\n|-\n| colspan=\"4\" | <math>x^{64} + x^{62} + x^{57} + x^{55} + x^{54} + x^{53} + x^{52} + x^{47} + x^{46} + x^{45} + x^{40} + x^{39} + x^{38} + x^{37} + x^{35} + x^{33} +</math> <math>x^{32} + x^{31} + x^{29} + x^{27} + x^{24} + x^{23} + x^{22} + x^{21} + x^{19} + x^{17} + x^{13} + x^{12} + x^{10} + x^9 + x^7 + x^4 + x + 1</math>\n|-\n| rowspan=\"2\" | CRC-64-ISO\n| rowspan=\"2\" | ISO 3309 ([[High-Level Data Link Control|HDLC]]), [[Swiss-Prot]]/[[TrEMBL]]; considered weak for hashing<ref name=\"jones-improved64\">{{Cite journal|last=Jones|first=David T.|title=An Improved 64-bit Cyclic Redundancy Check for Protein Sequences|publisher=University College London|url=http://www.cs.ucl.ac.uk/staff/d.jones/crcnote.pdf|accessdate=15 December 2009}}<!-- date of PDF=1 Dec 2009; date of referenced C file=2 Mar 2006; date in C file comments=28 Sep 2002 --></ref>\n| 0x000000000000001B\n| 0xD800000000000000\n| 0xB000000000000001\n| 0x800000000000000D\n| rowspan=\"2\" {{Bad|odd}}\n|-\n| colspan=\"4\" | <math>x^{64} + x^4 + x^3 + x + 1</math>\n|}\n\n=== Implementations ===\n*[https://web.archive.org/web/20130715065157/http://gnuradio.org/redmine/projects/gnuradio/repository/revisions/1cb52da49230c64c3719b4ab944ba1cf5a9abb92/entry/gr-digital/lib/digital_crc32.cc Implementation of CRC32 in Gnuradio];\n*[http://sourceforge.net/projects/crccalculator/files/CRC/ C class code for CRC checksum calculation with many different CRCs to choose from]\n*[http://networkdls.com/Software/View/CRC32/ A very simple C++ CRC32 implementation used by PKZip]\n\n===CRC catalogues===\n* [http://reveng.sourceforge.net/crc-catalogue/all.htm Catalogue of parametrised CRC algorithms]\n* [http://users.ece.cmu.edu/~koopman/crc/crc32.html CRC Polynomial Zoo]\n\n== See also ==\n*[[Mathematics of cyclic redundancy checks]]\n*[[Computation of cyclic redundancy checks]]\n*[[List of hash functions]]\n*[[List of checksum algorithms]]\n*[[Information security]]\n*[[Simple file verification]]\n*[[Longitudinal redundancy check|LRC]]\n\n== References ==\n{{Reflist|30em}}\n== Further reading ==\n* {{Cite book |title=Hacker's Delight |first=Henry S. |last=Warren Jr. |date=2013 |edition=2 |publisher=[[Addison Wesley]] – [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8|title-link=Hacker's Delight }}\n\n== External links ==\n{{refbegin}}\n*{{cite journal |first=Jubin |last=Mitra |first2=Tapan |last2=Nayak |title=Reconfigurable very high throughput low latency VLSI (FPGA) design architecture of CRC 32 |journal=Integration, the VLSI Journal |volume=56 |issue= |pages=1–14 |date=January 2017 |doi=10.1016/j.vlsi.2016.09.005 }}\n* [http://www.mathpages.com/home/kmath458.htm Cyclic Redundancy Checks], MathPages, overview of error-detection of different polynomials\n*{{cite web |authorlink=Ross Williams |first=Ross |last=Williams |title=A Painless Guide to CRC Error Detection Algorithms |date=1993 |url=http://www.ross.net/crc/crcpaper.html}}\n*{{cite web |first=Richard |last=Black |title=Fast CRC32 in Software |date=1994 |work=The Blue Book |publisher=Systems Research Group, Computer Laboratory, University of Cambridge |url=http://www.cl.cam.ac.uk/Research/SRG/bluebook/21/crc/crc.html}} Algorithm 4 was used in Linux and Bzip2.\n*{{cite web |last=Kounavis |first=M. |last2 = Berry |first2=F. |title=A Systematic Approach to Building High Performance, Software-based, CRC generators |year=2005 |publisher=Intel |url=http://www.intel.com/technology/comms/perfnet/download/CRC_generators.pdf}}, Slicing-by-4 and slicing-by-8 algorithms\n*{{cite web |first=W. |last=Kowalk |title=CRC Cyclic Redundancy Check Analysing and Correcting Errors |date=August 2006 |publisher=Universität Oldenburg |url=http://einstein.informatik.uni-oldenburg.de/papers/CRC-BitfilterEng.pdf }} — Bitfilters\n*{{cite book |first=Henry S., Jr. |last=Warren |chapter=Cyclic Redundancy Check |title=Hacker's Delight |publisher= |chapterurl=http://www.hackersdelight.org/crc.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20150503014404/http://www.hackersdelight.org/crc.pdf |archivedate=3 May 2015 |df=dmy-all |title-link=Hacker's Delight }} — theory, practice, hardware, and software with emphasis on CRC-32.\n*[http://www.cosc.canterbury.ac.nz/greg.ewing/essays/CRC-Reverse-Engineering.html Reverse-Engineering a CRC Algorithm]\n*{{cite web |first=Greg |last=Cook  |title=Catalogue of parameterised CRC algorithms |date= |work=CRC RevEng |url=http://reveng.sourceforge.net/crc-catalogue/all.htm}}\n*{{cite web |last=Koopman |first=Phil |title=Blog: Checksum and CRC Central |url=http://checksumcrc.blogspot.com/}} — includes links to PDFs giving 16 and 32-bit CRC [[Hamming distance]]s\n*{{cite web |first=Philip |last=Koopman |first2=Kevin |last2=Driscoll |first3=Brendan |last3=Hall |title=Cyclic Redundancy Code and Checksum Algorithms to Ensure Critical Data Integrity |date=March 2015 |id=DOT/FAA/TC-14/49 |publisher=Federal Aviation Administration |url=http://www.tc.faa.gov/its/worldpac/techrpt/tc14-49.pdf }}\n{{refend}}\n* [https://www.iso.org/standard/37010.html ISO/IEC 13239:2002: Information technology -- Telecommunications and information exchange between systems -- High-level data link control (HDLC) procedures]\n* [https://github.com/spotify/linux/blob/master/crypto/crc32c.c CRC32-Castagnoli Linux Library]\n\n{{Ecma International Standards}}\n\n[[Category:Binary arithmetic]]\n[[Category:Cyclic redundancy checks| ]]\n[[Category:Finite fields]]\n[[Category:Polynomials]]\n[[Category:Wikipedia articles with ASCII art]]"
    },
    {
      "title": "Densely packed decimal",
      "url": "https://en.wikipedia.org/wiki/Densely_packed_decimal",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\n'''Densely packed decimal''' (DPD) is an efficient method for [[binary coding|binary]] encoding [[decimal]] digits.\n\nThe traditional system of binary encoding for decimal digits, known as [[binary-coded decimal]] (BCD), uses four bits to encode each digit, resulting in significant wastage of binary data bandwidth (since four bits can store 16 states and are being used to store only 10). Densely packed decimal is a more efficient code that packs three digits into ten bits using a scheme that allows compression from, or expansion to, BCD with only two or three gate delays in hardware.<ref name=\"Cowlishaw_2002\"/>\n\nThe densely packed decimal encoding is a refinement of [[Chen–Ho encoding]]; it gives the same compression and speed advantages, but the particular arrangement of bits used confers additional advantages:\n* Compression of one or two digits (into the optimal four or seven bits respectively) is achieved as a subset of the three-digit encoding. This means that arbitrary numbers of decimal digits (not just multiples of three digits) can be encoded efficiently. For example, 38=12×3+2 decimal digits can be encoded in 12×10+7=127 bits – that is, 12 sets of three decimal digits can be encoded using 12 sets of ten binary bits and the remaining two decimal digits can be encoded using a further seven binary bits.\n* The subset encoding mentioned above is simply the rightmost bits of the standard three-digit encoding; the encoded value can be widened simply by adding leading 0 bits.\n* All seven-bit BCD numbers (0 through 79) are encoded identically by DPD. This makes conversions of common small numbers trivial. (This must break down at 80, because that requires eight bits for BCD, but the above property requires that the DPD encoding must fit into seven bits.)\n* The low-order bit of each digit is copied unmodified. Thus, the non-trivial portion of the encoding can be considered a conversion from three base-5 digits to seven binary bits. Further, digit-wise [[Truth value|logical values]] (in which each digit is either 0 or 1) can be manipulated directly without any encoding or decoding being necessary.\n\n== History ==\nIn 1971, [[Tien Chi Chen]]<ref name=\"CSE\"/> and [[Irving Tze Ho]]<ref name=\"Tseng_1988\"/> devised a lossless [[prefix code]] (known as [[Chen–Ho encoding]] since 2000<ref name=\"Cowlishaw_2000_CH\"/>) which packed three decimal digits into ten binary bits using a scheme which allowed compression from or expansion to BCD with only two or three gate delays in hardware. Densely packed decimal is a refinement of this, devised by [[Mike F. Cowlishaw]] in 2002,<ref name=\"Cowlishaw_2002\"/> which was incorporated into the [[IEEE 754-2008]]<ref name=\"IEEE-754_2008\"/> and [[ISO/IEC/IEEE 60559:2011]]<ref name=\"ISO-60559_2011\"/> standards for decimal [[floating-point]].\n\n== Encoding ==\nLike Chen–Ho encoding, DPD encoding classifies each decimal digit into one of two ranges, depending on the most significant bit of the binary form: \"small\" digits have values 0 through 7 (binary 0000–0111), and \"large\" digits, 8 through 9 (binary 1000–1001). Once it is known or has been indicated that a digit is small, three more bits are still required to specify the value. If a large value has been indicated, only one bit is required to distinguish between the values 8 or 9.\n\nWhen encoding, the most significant bit of each of the three digits to be encoded select one of eight coding patterns for the remaining bits, according to the following table. The table shows how, on decoding, the ten bits of the coded form in columns ''b9'' through ''b0'' are copied into the three digits ''d2'' through ''d0'', and the remaining bits are filled in with constant zeros or ones.\n\n{{Densely packed decimal}}\n\nBits b7, b4 and b0 (<code>c</code>, <code>f</code> and <code>i</code>) are passed through the encoding unchanged, and do not affect the meaning of the other bits. The remaining seven bits can be considered a seven-bit encoding for three base-5 digits.\n\nBits b8 and b9 are not needed and ignored when decoding DPD groups with three large digits (marked as \"x\" in the last row of the table above), but are filled with zeros when encoding.\n\nThe eight decimal values whose digits are all 8s or 9s have four codings each.\nThe bits marked x in the table above are ignored on input, but will always be 0 in computed results.\n(The 8×3 = 24 non-standard encodings fill in the gap between 10<sup>3</sup>=1000 and 2<sup>10</sup>=1024.)\n\n== Examples ==\nThis table shows some representative decimal numbers and their encodings in BCD, Chen–Ho, and densely packed decimal (DPD):\n\n{|class=wikitable style=\"text-align:center\"\n|- valign=top align=center\n! Decimal !! BCD !! Chen–Ho !! DPD\n|-\n!005\n|0000 0000 0101\n|000 000 0101\n|000 000 0101\n|-\n!009\n|0000 0000 1001\n|110 000 0001\n|000 000 1001\n|-\n!055\n|0000 0101 0101\n|000 010 1101\n|000 101 0101\n|-\n!079\n|0000 0111 1001\n|110 011 1001\n|000 111 1001\n|-\n!080\n| 0000 1000 0000\n| 101 000 0000\n| 000 000 1010\n|-\n!099\n|0000 1001 1001\n|111 000 1001\n|000 101 1111\n|-\n!555\n|0101 0101 0101\n|010 110 1101\n|101 101 0101\n|-\n!999\n|1001 1001 1001\n|111 111 1001\n|001 111 1111\n|}\n\n== See also ==\n* [[decimal32 floating-point format]]\n* [[decimal64 floating-point format]]\n* [[decimal128 floating-point format]]\n* [[Binary integer decimal]] (BID)\n\n== References ==\n{{Reflist|refs=\n<ref name=\"Cowlishaw_2002\">{{cite journal |title=Densely Packed Decimal Encoding |author-first=Mike F. |author-last=Cowlishaw |author-link=Mike F. Cowlishaw |journal=IEE Proceedings – Computers and Digital Techniques |volume=149 |issue=3 |pages=102–104 |publisher=[[Institution of Electrical Engineers]] (IEE) |date=May 2002 |issn=1350-2387 |doi=10.1049/ip-cdt:20020407 |location=London |url=http://ieeexplore.ieee.org/xpl/login.jsp?reload=true&arnumber=1008829 |access-date=2016-02-07}}</ref>\n<ref name=\"CSE\">{{cite web |title=CHEN Tien Chi |url=http://www.cse.cuhk.edu.hk/v7/en/people/tcchen.html |access-date=2016-02-07 |dead-url=no |archive-url=https://web.archive.org/web/20151023061814/http://www.cse.cuhk.edu.hk/v7/en/people/tcchen.html |archive-date=2015-10-23}}</ref>\n<ref name=\"Tseng_1988\">{{cite web |title=High-Tech Leadership: Irving T. Ho |author-last=Tseng |author-first=Li-Ling |date=1988-04-01 |publisher=[[Taiwan Info]]<!-- 2015 ministère des Affaires étrangères, République de Chine (Taiwan) --> |url=http://taiwaninfo.nat.gov.tw/ct.asp?xItem=105983&ctNode=124 |access-date=2016-02-08 |dead-url=no |archive-url=https://web.archive.org/web/20160208172221/http://taiwaninfo.nat.gov.tw/ct.asp?xItem=105983&ctNode=124 |archive-date=2016-01-01}}</ref>\n<ref name=\"IEEE-754_2008\">{{cite paper |title=IEEE Standard for Floating-Point Arithmetic |author=IEEE Computer Society |date=2008-08-29 |publisher=[[IEEE]] |id=IEEE Std 754-2008 |doi=10.1109/IEEESTD.2008.4610935 |ref=CITEREFIEEE_7542008 |isbn=978-0-7381-5753-5 |url=http://ieeexplore.ieee.org/servlet/opac?punumber=4610933 |access-date=2016-02-08}}</ref>\n<ref name=\"ISO-60559_2011\">{{cite paper |title=ISO/IEC/IEEE 60559:2011 |date=2011 |url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=57469 |access-date=2016-02-08}}</ref>\n<ref name=\"Cowlishaw_2000_CH\">{{cite web |author-first=Mike F. |author-last=Cowlishaw |author-link=Mike F. Cowlishaw |title=A Summary of Chen-Ho Decimal Data encoding |orig-year=2000<!-- June 2000 --> |date=2014 |publisher=[[IBM]] |url=http://speleotrove.com/decimal/chen-ho.html |access-date=2016-02-07 |dead-url=no |archive-url=https://web.archive.org/web/20150924145419/http://speleotrove.com/decimal/chen-ho.html |archive-date=2015-09-24}}</ref>\n}}\n<!-- This reference is included in the template\n<ref name=\"Cowlishaw_2000\">{{cite web |author-first=Mike F. |author-last=Cowlishaw |author-link=Mike F. Cowlishaw |publisher=[[IBM]] |title=A Summary of Densely Packed Decimal encoding |orig-year=2000 // (2000-10-03)// |date=2007-02-13 |url=http://speleotrove.com/decimal/DPDecimal.html |access-date=2016-02-07 |dead-url=no |archive-url=https://web.archive.org/web/20150924145411/http://speleotrove.com/decimal/DPDecimal.html |archive-date=2015-09-24}}</ref> -->\n\n==Further reading==\n* {{cite web |title=Decimal to binary coder/decoder |author-first=Michael Frederic |author-last=Cowlishaw |author-link=Michael Frederic Cowlishaw |publisher=[[International Business Machines Corporation]] (IBM) |location=Coventry, UK |publication-place=Armonk, NY, USA |date=2003-02-25 |orig-year=2002-05-20, 2001-01-27 |id=US6525679B1 |type=US Patent |url=https://patents.google.com/patent/US6525679B1 |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/b4/27/a6/f4ef138a335192/US6525679.pdf<!-- https://web.archive.org/web/20180719215823/https://patentimages.storage.googleapis.com/b4/27/a6/f4ef138a335192/US6525679.pdf -->] and {{cite web |title=Decimal to binary coder/decoder |author-first=Michael Frederic |author-last=Cowlishaw |author-link=Michael Frederic Cowlishaw |publisher=[[International Business Machines Corporation]] (IBM) |location=Winchester, Hampshire, UK |publication-place=Armonk, NY, USA |date=2007-11-07 |orig-year=2004-01-14, 2002-08-14, 2001-09-24, 2001-01-27 |id=EP1231716A2 |type=European Patent |url=https://patents.google.com/patent/EP1231716A2/en |access-date=2018-07-18}} [https://patentimages.storage.googleapis.com/e1/0c/a1/69981125812742/EP1231716A2.pdf<!-- https://web.archive.org/web/20180719001335/https://patentimages.storage.googleapis.com/e1/0c/a1/69981125812742/EP1231716A2.pdf -->] [https://patentimages.storage.googleapis.com/ba/25/b6/b0ad365c086f5f/EP1231716B1.pdf<!-- https://web.archive.org/web/20180719054809/https://patentimages.storage.googleapis.com/ba/25/b6/b0ad365c086f5f/EP1231716B1.pdf -->] [http://www.freepatentsonline.com/6525679.html] (NB. This patent is about DPD.)\n* {{cite web |title=Packed Decimal Encoding IEEE-754-2008 |author-first=Jo H. M. |author-last=Bonten |orig-year=2006<!-- 2006-10-05 --> |date=2009-10-06 |url=http://home.kpn.nl/jhm.bonten/computers/bitsandbytes/wordsizes/ibmpde.htm#dense |access-date=2018-07-11 |dead-url=no |archive-url=https://web.archive.org/web/20180711143550/http://home.kpn.nl/jhm.bonten/computers/bitsandbytes/wordsizes/ibmpde.htm#dense |archive-date=2018-07-11}} (NB. An older version can be found here: [https://web.archive.org/web/20070824053303/http://home.hetnet.nl/mr_1/81/jhm.bonten/computers/bitsandbytes/wordsizes/ibmpde.htm#dense Packed Decimal Encoding IEEE-754r].)\n* {{cite web |title=Chen-Ho Encoding and Densely Packed Decimal |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2007 |work=quadibloc |url=http://www.quadibloc.com/comp/cp020301.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703002320/http://www.quadibloc.com/comp/cp020301.htm |archive-date=2018-07-16}}\n\n[[Category:Binary arithmetic]]\n[[Category:2002 introductions]]\n[[Category:2002 in science]]"
    },
    {
      "title": "Division algorithm",
      "url": "https://en.wikipedia.org/wiki/Division_algorithm",
      "text": "{{anchor|AEGP}}{{about|algorithms for division|the theorem proving the existence of a unique quotient and remainder|Euclidean division}}\nA '''division algorithm''' is an [[algorithm]] which, given two integers N and D, computes their [[quotient]] and/or [[remainder]], the result of [[Euclidean division]]. Some are applied by hand, while others are employed by digital circuit designs and software.\n\nDivision algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include [[#Restoring division|restoring]], non-performing restoring, [[#Non-restoring division|non-restoring]], and [[#SRT division|SRT]] division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. [[#Newton–Raphson division|Newton–Raphson]] and [[#Goldschmidt division|Goldschmidt]] algorithms fall into this category.\n\nVariants of these algorithms allow using fast [[multiplication algorithm]]s. It results that, for large integers, the [[computational complexity|computer time]] needed for a division is the same, up to a constant factor, as the time needed for a multiplication, whichever multiplication algorithm is used.\n\nDiscussion will refer to the form <math>N/D = (Q, R)</math>, where\n* ''N'' = Numerator (dividend)\n* ''D'' = Denominator (divisor)\nis the input, and\n* ''Q'' = Quotient\n* ''R'' = Remainder\n\nis the output.\n\n==Division by repeated subtraction==\nThe simplest division algorithm, historically incorporated into a [[greatest common divisor]] algorithm presented in [[Euclid's Elements|Euclid's ''Elements'']], Book VII, Proposition 1, finds the remainder given two positive integers using only subtractions and comparisons:\n\n<syntaxhighlight lang=\"lua\">\nwhile N ≥ D do\n  N := N − D\nend\nreturn N\n</syntaxhighlight>\n\nThe proof that the quotient and remainder exist and are unique (described at [[Euclidean division]]) gives rise to a complete division algorithm using additions, subtractions, and comparisons:\n\n<syntaxhighlight lang=\"lua\">\nfunction divide(N, D)\n  if D = 0 then error(DivisionByZero) end\n  if D < 0 then (Q, R) := divide(N, −D); return (−Q, R) end\n  if N < 0 then\n    (Q,R) := divide(−N, D)\n    if R = 0 then return (−Q, 0)\n    else return (−Q − 1, D − R) end\n  end\n  -- At this point, N ≥ 0 and D > 0\n  return divide_unsigned(N, D)\nend  \nfunction divide_unsigned(N, D)\n  Q := 0; R := N\n  while R ≥ D do\n    Q := Q + 1\n    R := R − D\n  end\n  return (Q, R)\nend\n</syntaxhighlight>\n\nThis procedure always produces R ≥ 0. Although very simple, it takes &Omega;(Q) steps, and so is exponentially slower than even slow division algorithms like long division. It is useful if Q is known to be small (being an [[output-sensitive algorithm]]), and can serve as an executable specification.\n\n==Long division==\n{{main|Long division}}\nLong division is the standard algorithm used for pen-and-paper division of multi-digit numbers expressed in decimal notation. It shifts gradually from the left to the right end of the dividend, subtracting the largest possible multiple of the divisor (at the digit level) at each stage; the multiples then become the digits of the quotient, and the final difference is then the remainder. \n\nWhen used with a binary radix, this method forms the basis for the (unsigned) integer division with remainder algorithm below. [[Short division]] is an abbreviated form of long division suitable for one-digit divisors. [[Chunking (division)|Chunking]] — also known as the partial quotients method or the hangman method — is a less-efficient form of long division which may be easier to understand. By allowing one to subtract more multiples than what one currently has at each stage, a more freeform variant of long division can be developed as well<ref>{{Cite web|url=https://mathvault.ca/long-division/|title=The Definitive Higher Math Guide to Long Division and Its Variants — for Integers|date=2019-02-24|website=Math Vault|language=en-US|access-date=2019-06-24}}</ref>\n\n==Integer division (unsigned) with remainder==\nThe following algorithm, the binary version of the famous [[long division]], will divide ''N'' by ''D'', placing the quotient in ''Q'' and the remainder in ''R''.  In the following code, all values are treated as unsigned integers.\n\n<syntaxhighlight lang=\"lua\">\nif D = 0 then error(DivisionByZeroException) end\nQ := 0                  -- Initialize quotient and remainder to zero\nR := 0                     \nfor i := n − 1 .. 0 do  -- Where n is number of bits in N\n  R := R << 1           -- Left-shift R by 1 bit\n  R(0) := N(i)          -- Set the least-significant bit of R equal to bit i of the numerator\n  if R ≥ D then\n    R := R − D\n    Q(i) := 1\n  end\nend\n</syntaxhighlight>\n\n===Example===\nIf we take N=1100<sub>2</sub> (12<sub>10</sub>) and D=100<sub>2</sub> (4<sub>10</sub>)\n\n''Step 1'':  Set R=0 and Q=0 <br />\n''Step 2'':  Take i=3 (one less than the number of bits in N) <br />\n''Step 3'':  R=00 (left shifted by 1) <br />\n''Step 4'':  R=01 (setting R(0) to N(i))  <br />\n''Step 5'':  R<D, so skip statement\n\n''Step 2'':  Set i=2  <br />\n''Step 3'':  R=010  <br />\n''Step 4'':  R=011  <br />\n''Step 5'':  R<D, statement skipped\n\n''Step 2'':  Set i=1  <br />\n''Step 3'':  R=0110  <br />\n''Step 4'':  R=0110  <br />\n''Step 5'':  R>=D, statement entered  <br />\n''Step 5b'':  R=10 (R−D)  <br />\n''Step 5c'':  Q=10 (setting Q(i) to 1)\n\n''Step 2'':  Set i=0  <br />\n''Step 3'':  R=100  <br />\n''Step 4'':  R=100  <br />\n''Step 5'':  R>=D, statement entered  <br />\n''Step 5b'':  R=0 (R−D)  <br />\n''Step 5c'':  Q=11 (setting Q(i) to 1)\n\n'''end'''  <br />\nQ=11<sub>2</sub> (3<sub>10</sub>) and R=0.\n\n==Slow division methods==\nSlow division methods are all based on a standard recurrence equation\n:<math>R_{j+1} = B \\times R_j - q_{n-(j+1)}\\times D\\,</math>,\nwhere:\n* ''R''<sub>''j''</sub> is the ''j''-th partial remainder of the division\n* ''B'' is the [[radix]] (base, usually 2 internally in computers and calculators)\n* ''q''<sub> ''n'' &minus; (''j'' + 1)</sub> is the digit of the quotient in position ''n−(j+1)'', where the digit positions are numbered from least-significant 0 to most significant ''n''−1\n* ''n'' is number of digits in the quotient\n* ''D'' is the divisor\n\n===Restoring division===\nRestoring division operates on [[fixed point arithmetic|fixed-point]] fractional numbers and depends on the assumption 0 < ''D'' < ''N''. {{citation needed|date=February 2012}} <!-- see \"Integer division (unsigned) with remainder\" on talk -->\n\nThe quotient digits ''q'' are formed from the digit set {0,1}.\n\nThe basic algorithm for binary (radix 2) restoring division is:\n\n<syntaxhighlight lang=\"lua\">\nR := N\nD := D << n            -- R and D need twice the word width of N and Q\nfor i := n − 1 .. 0 do  -- For example 31..0 for 32 bits\n  R := 2 * R − D          -- Trial subtraction from shifted value (multiplication by 2 is a shift in binary representation)\n  if R ≥ 0 then\n    q(i) := 1          -- Result-bit 1\n  else\n    q(i) := 0          -- Result-bit 0\n    R := R + D         -- New partial remainder is (restored) shifted value\n  end\nend\n\n-- Where: N = Numerator, D = Denominator, n = #bits, R = Partial remainder, q(i) = bit #i of quotient\n</syntaxhighlight>\n\nThe above restoring division algorithm can avoid the restoring step by saving the shifted value 2''R'' before the subtraction in an additional register ''T'' (i.e., ''T'' =&nbsp;''R''&nbsp;<<&nbsp;1) and copying register ''T'' to ''R'' when the result of the subtraction 2''R''&nbsp;&minus;&nbsp;''D'' is negative.\n\nNon-performing restoring division is similar to restoring division except that the value of 2R is saved, so ''D'' does not need to be added back in for the case of R &lt; 0.\n\n===Non-restoring division===\n\nNon-restoring division uses the digit set {&minus;1, 1} for the quotient digits instead of {0, 1}.  The algorithm is more complex, but has the advantage when implemented in hardware that there is only one decision and addition/subtraction per quotient bit; there is no restoring step after the subtraction, which potentially cuts down the numbers of operations by up to half and lets it be executed faster.<ref>{{Cite web|url=https://web.stanford.edu/class/ee486/doc/chap5.pdf|title=Stanford EE486 (Advanced Computer Arithmetic Division) — Chapter 5 Handout (Division)|last=Flynn|first=|date=|website=Stanford University|archive-url=|archive-date=|dead-url=|access-date=}}</ref>  The basic algorithm for binary (radix 2) non-restoring division of non-negative numbers is:\n\n<syntaxhighlight lang=\"lua\">\nR := N\nD := D << n            -- R and D need twice the word width of N and Q\nfor i = n − 1 .. 0 do  -- for example 31..0 for 32 bits\n  if R >= 0 then\n    q[i] := +1\n    R := 2 * R − D\n  else\n    q[i] := −1\n    R := 2 * R + D\n  end if\nend\n \n-- Note: N=Numerator, D=Denominator, n=#bits, R=Partial remainder, q(i)=bit #i of quotient.\n</syntaxhighlight>\n\nFollowing this algorithm, the quotient is in a non-standard form consisting of digits of &minus;1 and +1. This form needs to be converted to binary to form the final quotient.  Example:\n{| border=\"0\" cellpadding=\"0\"\n|colspan=2|Convert the following quotient to the digit set {0,1}:\n|-\n|Start: || <math>Q = 111\\bar{1}1\\bar{1}1\\bar{1}</math>\n|-\n|1. Form the positive term:       ||<math>P = 11101010\\,</math>\n|-\n|2. Mask the negative term*:      ||<math>M = 00010101\\,</math>\n|-\n|3. Subtract: <math>P - M</math>: ||<math>Q = 11010101\\,</math>\n|-\n|colspan=2|*.( Signed binary notation with [[One's complement]] without [[Two's Complement]] ) \n|}\n\nIf the −1 digits of <math>Q</math> are stored as zeros (0) as is common, then <math>P</math> '''is''' <math>Q</math> and computing <math>M</math> is trivial: perform a one's complement (bit by bit complement) on the original <math>Q</math>.\n<syntaxhighlight lang=\"lua\">\nQ := Q − bit.bnot(Q)      * Appropriate if −1 Digits in Q are Represented as zeros as is common.\n</syntaxhighlight>\n\nFinally, quotients computed by this algorithm are always odd, and the remainder in R is in the range −D &le; R &lt; D.  For example, 5 / 2 = 3 R −1.  To convert to a positive remainder, do a single restoring step ''after'' Q is converted from non-standard form to standard form:\n<syntaxhighlight lang=\"lua\">\nif R < 0 then\n   Q := Q − 1\n   R := R + D  -- Needed only if the Remainder is of interest.\nend if\n</syntaxhighlight>\n\nThe actual remainder is R >> n.  (As with restoring division, the low-order bits of R are used up at the same rate as bits of the quotient Q are produced, and it is common to use a single shift register for both.)\n\n==={{anchor|SRT}}SRT division===\nNamed for its creators (Sweeney, Robertson, and Tocher), SRT division is a popular method for division in many [[microprocessor]] implementations.<ref>{{cite techreport |url=http://pages.hmc.edu/harris/research/srtlong.pdf |title=SRT Division: Architectures, Models, and Implementations |first1=David L. |last1=Harris |first2=Stuart F. |last2=Oberman |first3=Mark A. |last3=Horowitz |publisher=Stanford University |date=9 September 1998}}</ref><ref>{{cite journal |url=http://scholarship.claremont.edu/cgi/viewcontent.cgi?article=1094&context=hmc_fac_pub |title=SRT Division Algorithms as Dynamical Systems |first1=Mark |last1=McCann |first2=Nicholas |last2=Pippenger |journal=SIAM Journal on Computing |volume=34 |issue=6 |pages=1279–1301 |year=2005 |doi=10.1137/S009753970444106X |citeseerx=10.1.1.72.6993}}</ref> SRT division is similar to non-restoring division, but it uses a [[lookup table]] based on the dividend and the divisor to determine each quotient digit.\n\nThe most significant difference is that a ''redundant representation'' is used for the quotient.  For example, when implementing radix-4 SRT division, each quotient digit is chosen from ''five'' possibilities: { −2, −1, 0, +1, +2 }.  Because of this, the choice of a quotient digit need not be perfect; later quotient digits can correct for slight errors.  (For example, the quotient digit pairs (0, +2) and (1, −2) are equivalent, since 0×4+2 = 1×4−2.)  This tolerance allows quotient digits to be selected using only a few most-significant bits of the dividend and divisor, rather than requiring a full-width subtraction.  This simplification in turn allows a radix higher than 2 to be used.\n\nLike non-restoring division, the final steps are a final full-width subtraction to resolve the last quotient bit, and conversion of the quotient to standard binary form.\n\nThe [[Original Intel Pentium (P5 microarchitecture)|Intel Pentium]] processor's infamous [[Pentium FDIV bug|floating-point division bug]] was caused by an incorrectly coded lookup table. Five of the 1066 entries had been mistakenly omitted.<ref>{{cite web |url=http://www.intel.com/support/processors/pentium/sb/cs-012997.htm |title=Statistical Analysis of Floating Point Flaw |publisher=Intel Corporation |year=1994 |accessdate=22 October 2013 }}</ref><ref>{{cite techreport |url=http://i.stanford.edu/pub/cstr/reports/csl/tr/95/675/CSL-TR-95-675.pdf |title=An Analysis of Division Algorithms and Implementations|first1=Stuart F. |last1=Oberman |first2=Michael J. |last2=Flynn |id=CSL-TR-95-675 |date=July 1995 |publisher=Stanford University}}</ref>\n\n==Fast division methods==\n\n=== Newton–Raphson division ===\nNewton–Raphson uses [[Newton's method]] to find the [[Multiplicative inverse|reciprocal]] of <math>D</math> and multiply that reciprocal by <math>N</math> to find the {{nowrap|final quotient <math>Q</math>.}}\n\nThe steps of Newton–Raphson division are:\n# Calculate an estimate <math>X_0</math> for the reciprocal <math>1/D</math> of the divisor <math>D</math>.\n# Compute successively more accurate estimates <math>X_1,X_2,\\ldots,X_S</math> of the reciprocal. This is where one employs the Newton–Raphson method as such.\n# Compute the quotient by multiplying the dividend by the reciprocal of the divisor: <math>Q = N X_S</math>.\n\nIn order to apply Newton's method to find the reciprocal of <math>D</math>, it is necessary to find a function <math>f(X)</math> that has a zero at <math>X=1/D</math>. The obvious such function is <math>f(X)=DX-1</math>, but the Newton–Raphson iteration for this is unhelpful, since it cannot be computed without already knowing the reciprocal of <math>D</math> (moreover it attempts to compute the exact reciprocal in one step, rather than allow for iterative improvements). A function that does work is <math>f(X)=(1/X)-D</math>, for which the Newton–Raphson iteration gives\n: <math>X_{i+1} = X_i - {f(X_i)\\over f'(X_i)} = X_i - {1/X_i - D\\over -1/X_i^2} = X_i + X_i(1-DX_i) = X_i(2-DX_i),</math>\nwhich can be calculated from <math>X_i</math> using only multiplication and subtraction, or using two [[fused multiply–add]]s.\n\nFrom a computation point of view, the expressions <math>X_{i+1} = X_i + X_i(1-DX_i)</math> and <math>X_{i+1} = X_i(2-DX_i)</math> are not equivalent. To obtain a result with a precision of 2''n'' bits while making use of the second expression, one must compute the product between <math>X_i</math> and <math>(2-DX_i)</math> with double the given precision of <math>X_i</math>(''n'' bits).{{citation needed|date=February 2014}} In contrast, the product between <math>X_i</math> and <math>(1-DX_i)</math> need only be computed with a precision of ''n'' bits, because the leading ''n'' bits (after the binary point) of <math>(1-DX_i)</math> are zeros.\n\nIf the error is defined as <math>\\varepsilon_i = 1 - D X_i</math>, then:\n:<math>\\begin{align}\n\\varepsilon_{i+1} &= 1 - D X_{i+1} \\\\\n               &= 1 - D (X_i(2-DX_i)) \\\\\n               &= 1 - 2DX_i + D^2X_i^2 \\\\\n               &= (1 - DX_i)^2 \\\\\n               &= {\\varepsilon_i}^2. \\\\\n\\end{align}</math>\nThis squaring of the error at each iteration step — the so-called [[Newton's method#Practical considerations|quadratic convergence]] of Newton–Raphson's method — has the effect that the number of correct digits in the result roughly ''doubles for every iteration'', a property that becomes extremely valuable when the numbers involved have many digits (e.g. in the large integer domain). But it also means that the initial convergence of the method can be comparatively slow, especially if the initial estimate <math>X_0</math> is poorly chosen.\n\nFor the subproblem of choosing an initial estimate <math>X_0</math>, it is convenient to apply a bit-shift to the divisor ''D'' to scale it so that 0.5&nbsp;≤&nbsp;''D''&nbsp;≤&nbsp;1; by applying the same bit-shift to the numerator ''N'', one ensures the quotient does not change. Then one could use a linear [[approximation]] in the form\n:<math>X_0 = T_1 + T_2 D \\approx \\frac{1}{D} \\,</math>\n\nto initialize Newton–Raphson. To minimize the maximum of the absolute value of the error of this approximation on interval <math>[0.5,1]</math>, one should use\n:<math>X_0 = {48 \\over 17} - {32 \\over 17} D. \\,</math>\nThe coefficients of the linear approximation are determined as follows. The absolute value of the error is <math>|\\varepsilon_0| = |1 - D(T_1 + T_2 D)|</math>. The minimum of the maximum absolute value of the error is determined by the [[Equioscillation theorem|Chebyshev equioscillation theorem]] applied to <math>F(D) = 1 - D(T_1 + T_2 D)</math>.  The local minimum of <math>F(D)</math> occurs when <math>F'(D) = 0</math>, which has solution <math>D = -T_1/(2T_2)</math>. The function at that minimum must be of opposite sign as the function at the endpoints, namely, <math>F(1/2) = F(1) = -F(-T_1/(2T_2))</math>.  The two equations in the two unknowns have a unique solution <math>T_1 = 48/17</math> and <math>T_2 = -32/17</math>, and the maximum error is <math>F(1) = 1/17</math>.  Using this approximation, the absolute value of the error of the initial value is less than\n:<math>\\vert \\varepsilon_0 \\vert \\leq {1 \\over 17} \\approx 0.059 . \\,</math>\nIt is possible to generate a polynomial fit of degree larger than 1, computing the coefficients using the [[Remez algorithm]].  The trade-off is that the initial guess requires more computational cycles but hopefully in exchange for fewer iterations of Newton–Raphson.\n\nSince for this method the [[rate of convergence|convergence]] is exactly quadratic, it follows that\n\n:<math>S = \\left \\lceil \\log_2 \\frac{P + 1}{\\log_2 17} \\right \\rceil \\,</math>\n\nsteps are enough to calculate the value up to <math>P \\,</math> binary places. This evaluates to 3 for IEEE [[single precision]] and 4 for both [[double precision]] and [[extended precision|double extended]] formats.\n\n====Pseudocode====\n\nThe following computes the quotient of N and D with a precision of P binary places:\n\n Express D as M × 2<sup>e</sup> where 1 &le; M < 2 (standard floating point representation)\n D' := D / 2<sup>e+1</sup>   ''// scale between 0.5 and 1, can be performed with bit shift / exponent subtraction''\n N' := N / 2<sup>e+1</sup>\n X := 48/17 − 32/17 × D'   ''// precompute constants with same precision as D''\n {{nowrap|'''repeat''' <math>\\left \\lceil \\log_2 \\frac{P + 1}{\\log_2 17} \\right \\rceil \\,</math> '''times'''}}   ''// can be precomputed based on fixed P''\n     X := X + X × (1 - D' × X)\n '''end'''\n '''return''' N' × X\n\nFor example, for a double-precision floating-point division, this method uses 10 multiplies, 9 adds, and 2 shifts.\n\n==== Variant Newton–Raphson division ====\n\nThe Newton-Raphson division method can be modified to be slightly faster as follows. After shifting ''N'' and ''D'' so that ''D'' is in [0.5, 1.0], initialize with\n:<math> X := \\frac{140}{33} + D \\cdot \\left(\\frac{-64}{11} + D \\cdot \\frac{256}{99}\\right) .</math>\nThis is the best quadratic fit to 1/''D'' and gives an absolute value of the error less than or equal to 1/99. It is chosen to make the error equal to a re-scaled third order [[Chebyshev polynomial]] of the first kind. The coefficients should be pre-calculated and hard-coded.\n\nThen in the loop, use an iteration which cubes the error.\n:<math> E := 1 - D \\cdot X </math>\n:<math> Y := X \\cdot E </math>\n:<math> X := X + Y + Y \\cdot E .</math>\nThe ''Y''&middot;''E'' term is new.\n\nIf the loop is performed until X agrees with 1/''D'' on its leading ''P'' bits, then the number of iterations will be no more than\n:<math> \\left \\lceil \\log_3 \\left( \\frac{P + 1}{\\log_2 99} \\right) \\right \\rceil </math>\nwhich is the number of times 99 must be cubed to get to 2<sup>''P''+1</sup>. Then\n:<math> Q:= N \\cdot X </math>\nis the quotient to ''P'' bits.\n\nUsing higher degree polynomials in either the initialization or the iteration results in a degradation of performance because the extra multiplications required would be better spent on doing more iterations.\n\n===Goldschmidt division===\nGoldschmidt division<ref>{{cite thesis |first=Robert E. |last=Goldschmidt |title=Applications of Division by Convergence |series=M.Sc. dissertation |publisher=M.I.T. |date=1964 |oclc=34136725 |url=http://dspace.mit.edu/bitstream/handle/1721.1/11113/34136725-MIT.pdf }}</ref> (after Robert Elliott Goldschmidt<ref><!-- some bio bits for possible article -->https://web.archive.org/web/20180718114413/https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5392026</ref>) uses an iterative process of repeatedly multiplying both the dividend and divisor by a common factor ''F''<sub>''i''</sub>, chosen such that the divisor converges to 1. This causes the dividend to converge to the sought quotient ''Q'':\n\n:<math>Q = \\frac{N}{D} \\frac{F_1}{F_1} \\frac{F_2}{F_2}  \\frac{F_\\ldots}{F_\\ldots}.</math>\n\nThe steps for Goldschmidt division are:\n# Generate an estimate for the multiplication factor ''F<sub>i</sub>'' .\n# Multiply the dividend and divisor by ''F<sub>i</sub>'' .\n# If the divisor is sufficiently close to 1, return the dividend, otherwise, loop to step 1.\n\nAssuming ''N''/''D'' has been scaled so that 0&nbsp;<&nbsp;''D''&nbsp;<&nbsp;1, each ''F<sub>i</sub>'' is based on ''D'':\n\n:<math>F_{i+1} = 2 - D_i.</math>\n\nMultiplying the dividend and divisor by the factor yields:\n:<math>\\frac{N_{i+1}}{D_{i+1}} = \\frac{N_i}{D_i}\\frac{F_{i+1}}{F_{i+1}}.</math>\n\nAfter a sufficient number ''k'' of iterations <math>Q=N_k</math>.\n\nThe Goldschmidt method is used in [[AMD]] Athlon CPUs and later models.<ref>{{cite journal |first=Stuart F. |last=Oberman |title=Floating Point Division and Square Root Algorithms and Implementation in the AMD-K7 Microprocessor |journal=Proceedings of the IEEE Symposium on Computer Arithmetic |pages=106&ndash;115 |date=1999 |url=http://www.acsel-lab.com/arithmetic/arith14/papers/ARITH14_Oberman.pdf }}</ref><ref>{{cite journal |first=Peter |last=Soderquist |first2=Miriam |last2=Leeser |title=Division and Square Root: Choosing the Right Implementation |journal=IEEE Micro |volume=17 |issue=4 |pages=56&ndash;66 |date=July–August 1997 |url=https://www.researchgate.net/publication/2511700 |doi=10.1109/40.612224 }}</ref>\n\n====Binomial theorem====\nThe Goldschmidt method can be used with factors that allow simplifications by the [[binomial theorem]].\nAssume N/D has been scaled by a [[power of two]] such that <math>D\\in(\\tfrac{1}{2},1]</math>.\nWe choose <math>D = 1-x</math> and <math>F_{i} = 1+x^{2^i}</math>.\nThis yields\n\n: <math>\n\\frac{N}{1-x}\n = \\frac{N\\cdot(1+x)}{1-x^2}\n = \\frac{N\\cdot(1+x)\\cdot(1+x^2)}{1-x^4}\n = \\cdots\n = Q' = \\frac{N' = N\\cdot(1+x)\\cdot(1+x^2)\\cdot\\cdot\\cdot(1+x^{2^{(n-1)}})}{D' = 1-x^{2^n} \\approx 1}\n</math>.\n\nAfter <math>n</math> steps <math>( x\\in[0,\\tfrac{1}{2}) )</math>, the denominator <math>1-x^{2^n}</math> can be rounded to <math>1</math> with a [[relative error]]\n\n:<math>\n\\varepsilon_n = \\frac{Q' - N'}{Q'}  =  x^{2^n}\n</math>\n\nwhich is maximum at <math>2^{-2^n}</math> when <math>x = {1 \\over 2}</math>, thus providing a minimum precision of <math>2^n</math> binary digits.\n\n== Large-integer methods ==\nMethods designed for hardware implementation generally do not scale to integers with thousands or millions of decimal digits; these frequently occur, for example, in [[Modular arithmetic|modular]] reductions in [[cryptography]]. For these large integers, more efficient division algorithms transform the problem to use a small number of multiplications, which can then be done using an asymptotically efficient [[multiplication algorithm]] such as the [[Karatsuba algorithm]], [[Toom–Cook multiplication]] or the [[Schönhage–Strassen algorithm]]. The result is that the [[computational complexity]] of the division is of the same order (up to a multiplicative constant) as that of the multiplication. Examples include reduction to multiplication by [[Newton's method]] as [[#Newton–Raphson division|described above]],<ref>{{Cite thesis |degree=M.Sc. in Computer Science |title=Fast Division of Large Integers: A Comparison of Algorithms |url=https://treskal.com/s/masters-thesis.pdf |last=Hasselström |first=Karl |year=2003 |publisher=Royal Institute of Technology |accessdate=2017-07-08 |archivedate=8 July 2017 |archiveurl=https://web.archive.org/web/20170708221722/https://static1.squarespace.com/static/5692a9ad7086d724272eb00a/t/5692dbe6b204d50df79e577f/1452465127528/masters-thesis.pdf}}</ref> as well as the slightly faster [[Barrett reduction]] and [[Montgomery reduction]] algorithms.<ref>{{cite conference |url=http://portal.acm.org/citation.cfm?id=36688 |title=Implementing the Rivest Shamir and Adleman public key encryption algorithm on a standard digital signal processor |first=Paul |last=Barrett |year=1987 |publisher=Springer-Verlag |booktitle=Proceedings on Advances in cryptology---CRYPTO '86 |pages=311&ndash;323 |location=London, UK |isbn=0-387-18047-8 }}</ref>{{verification needed|date=June 2015|reason=Barrett reduction is usually understood to be the algorithm for computing the remainder that one gets from having precomputed the inverse of the denominator. Rather than providing a solution to the problem of division, it requires that a separate solution is already available!}} Newton's method is particularly efficient in scenarios where one must divide by the same divisor many times, since after the initial Newton inversion only one (truncated) multiplication is needed for each division.\n\n== Division by a constant ==\n\nThe division by a constant ''D'' is equivalent to the multiplication by its [[Multiplicative inverse|reciprocal]]. \nSince the denominator is constant, so is its reciprocal (1/''D''). Thus it is possible to compute the value of (1/''D'') once at compile time, and at run time perform the multiplication ''N''·(1/''D'') rather than the division ''N/D''. In [[floating-point]] arithmetic the use of (1/''D'') presents little problem, but in [[Integer (computer science)|integer]] arithmetic the reciprocal will always evaluate to zero (assuming |''D''| > 1).\n\nIt is not necessary to use specifically (1/''D''); any value (''X''/''Y'') that reduces to (1/''D'') may be used.  For example, for division by 3, the factors 1/3, 2/6, 3/9, or 194/582 could be used. Consequently, if ''Y'' were a power of two the division step would reduce to a fast right bit shift. The effect of calculating ''N''/''D'' as (''N''·''X'')/''Y'' replaces a division with a multiply and a shift. Note that the parentheses are important, as ''N''·(''X''/''Y'') will evaluate to zero.\n\nHowever, unless ''D'' itself is a power of two, there is no ''X'' and ''Y'' that satisfies the conditions above. Fortunately, (''N''·''X'')/''Y'' gives exactly the same result as ''N''/''D'' in integer arithmetic even when (''X''/''Y'') is not exactly equal to 1/''D'', but \"close enough\" that the error introduced by the approximation is in the bits that are discarded by the shift operation.<ref>{{cite journal |title=Division by Invariant Integers using Multiplication |first=Torbjörn |last=Granlund |first2=Peter L. |last2=Montgomery |journal=SIGPLAN Notices |volume=29 |issue=6 |date=June 1994 |pages=61–72 |doi=10.1145/773473.178249 |url=http://gmplib.org/~tege/divcnst-pldi94.pdf |citeseerx=10.1.1.1.2556 }}</ref><ref>{{cite journal |title=Improved Division by Invariant Integers |first1=Niels |last1=Möller |first2=Torbjörn |last2=Granlund |journal=IEEE Transactions on Computers |date=February 2011 |volume=60 |issue=2 |pages=165–175 |doi=10.1109/TC.2010.143 |url=http://gmplib.org/~tege/division-paper.pdf }}</ref><ref>\nridiculous_fish.\n[http://ridiculousfish.com/files/faster_unsigned_division_by_constants.pdf \"Labor of Division (Episode III): Faster Unsigned Division by Constants\"].\n2011.\n</ref>\n\nAs a concrete [[fixed-point arithmetic]] example, for 32-bit unsigned integers, division by 3 can be replaced with a multiply by {{sfrac|2863311531|2<sup>33</sup>}}, a multiplication by 2863311531 ([[hexadecimal]] 0xAAAAAAAB) followed by a 33 right bit shift.  The value of 2863311531 is calculated as {{sfrac|2<sup>33</sup>|3}}, then rounded up.\n\nLikewise, division by 10 can be expressed as a multiplication by 3435973837 (0xCCCCCCCD) followed by division by 2<sup>35</sup> (or 35 right bit shift).\n\nIn some cases, division by a constant can be accomplished in even less time by converting the \"multiply by a constant\" into a [[Multiplication algorithm#Shift and add|series of shifts and adds or subtracts]].<ref>LaBudde, Robert A.; Golovchenko, Nikolai; Newton, James; and Parker, David; [http://techref.massmind.org/techref/method/math/divconst.htm ''Massmind: \"Binary Division by a Constant\"'']</ref> Of particular interest is division by 10, for which the exact quotient is obtained, with remainder if required.<ref>{{cite journal |first=R. A. |last=Vowels |title=Division by 10 |journal=Australian Computer Journal |volume=24 |issue=3 |year=1992 |pages=81–85 }}</ref>\n\n== Rounding error ==\n{{expand section|date=September 2012}}\n[[Round-off error]] can be introduced by division operations due to limited [[Precision (computer science)|precision]].\n\n{{further | Floating point}}\n\n== See also ==\n*[[Multiplication algorithm]]\n*[[Pentium FDIV bug]]\n\n== References==\n{{reflist|30em}}\n\n==Further reading==\n* {{Cite book |title=Hacker's Delight |first=Henry S. |last=Warren Jr. |date=2013 |edition=2 |publisher=[[Addison Wesley]] - [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8|title-link=Hacker's Delight }}\n* {{cite web |title=Advanced Arithmetic Techniques |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2006 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0202.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703001722/http://www.quadibloc.com/comp/cp0202.htm |archive-date=2018-07-03}}\n\n== External links ==\n* [http://www.ecs.umass.edu/ece/koren/arith/simulator/ Computer Arithmetic Algorithms JavaScript Simulator] – contains simulators for many different division algorithms\n* {{cite web <!-- name guessed from corydoras@ridiculousfish.com -->|first=Cory |last=Doras |title=Labor of Division (Episode III): Faster Unsigned Division by Constants |url=http://ridiculousfish.com/files/faster_unsigned_division_by_constants.pdf |publisher=ridiculous_fish |date=19 October 2011}} (Extends division by constants.)\n* http://www.dauniv.ac.in/downloads/CArch_PPTs/CompArchCh03L07IntegerDivision.pdf\n* http://www.seas.ucla.edu/~ingrid/ee213a/lectures/division_presentV2.pdf\n* http://www.m1c4a1.wz.cz/docs/goldschmidt.pdf\n\n{{DEFAULTSORT:Division (Digital)}}\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Division (mathematics)|Digital]]\n[[Category:Articles with example pseudocode]]\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "Double dabble",
      "url": "https://en.wikipedia.org/wiki/Double_dabble",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\nIn [[computer science]], the '''double dabble''' [[algorithm]] is used to convert [[binary numbers]] into [[binary-coded decimal]] (BCD) notation.<ref name=\"Gao_2012_1\"/><ref name=\"Gao_2012_2\"/> It is also known as the '''[[shift-and-add algorithm|shift-and-add]]-3 algorithm''', and can be implemented using a small number of gates in computer hardware, but at the expense of high [[Latency (engineering)|latency]].<ref name=\"Véstias_2010\"/>\n\n==Algorithm==\nThe algorithm operates as follows:\n\nSuppose the original number to be converted is stored in a [[processor register|register]] that is ''n''&nbsp;bits wide. Reserve a scratch space wide enough to hold both the original number and its BCD representation; {{math|''n'' + 4×''ceil''(''n''/3)}} bits will be enough. It takes a maximum of 4&nbsp;bits in binary to store each decimal digit.\n\nThen partition the scratch space into BCD digits (on the left) and the original register (on the right). For example, if the original number to be converted is eight bits wide, the scratch space would be partitioned as follows:\n\n 100s Tens Ones   Original\n 0010 0100 0011   11110011\n\nThe diagram above shows the binary representation of 243<sub>10</sub> in the original register, and the BCD representation of 243 on the left.\n\nThe scratch space is initialized to all zeros, and then the value to be converted is copied into the \"original register\" space on the right.\n\n 0000 0000 0000   11110011\n\nThe algorithm then iterates ''n'' times. On each iteration, any BCD digit which is greater than 4 is incremented by 3; then the entire scratch space is left-shifted one&nbsp;bit. The increment ensures that a value of 5, incremented and left-shifted, becomes 16, thus correctly \"carrying\" into the next BCD digit.\n\nEssentially, the algorithm operates by doubling the BCD value on the left each iteration and adding either one or zero according to the original bit pattern. Shifting left accomplishes both tasks simultaneously. If any digit is five or above, three is added to ensure the value \"carries\" in base 10.\n\nThe double-dabble algorithm, performed on the value 243<sub>10</sub>, looks like this:\n\n 0000 0000 0000   11110011   Initialization\n 0000 0000 0001   11100110   Shift\n 0000 0000 0011   11001100   Shift\n 0000 0000 0111   10011000   Shift\n 0000 0000 1010   10011000   Add 3 to ONES, since it was 7\n 0000 0001 0101   00110000   Shift\n 0000 0001 1000   00110000   Add 3 to ONES, since it was 5\n 0000 0011 0000   01100000   Shift\n 0000 0110 0000   11000000   Shift\n 0000 1001 0000   11000000   Add 3 to TENS, since it was 6\n 0001 0010 0001   10000000   Shift\n 0010 0100 0011   00000000   Shift\n    2    4    3\n        BCD\n\nNow eight shifts have been performed, so the algorithm terminates. The BCD digits to the left of the \"original register\" space display the BCD encoding of the original value 243.\n\nAnother example for the double dabble algorithm{{snd}} value 65244<sub>10</sub>.\n\n  10<sup>4</sup>  10<sup>3</sup>  10<sup>2</sup>   10<sup>1</sup>  10<sup>0</sup>    Original binary\n 0000 0000 0000 0000 0000   1111111011011100   Initialization\n 0000 0000 0000 0000 0001   1111110110111000   Shift left (1st)\n 0000 0000 0000 0000 0011   1111101101110000   Shift left (2nd)\n 0000 0000 0000 0000 0111   1111011011100000   Shift left (3rd)\n 0000 0000 0000 0000 1010   1111011011100000   Add 3 to 10<sup>0</sup>, since it was 7\n 0000 0000 0000 0001 0101   1110110111000000   Shift left (4th)\n 0000 0000 0000 0001 1000   1110110111000000   Add 3 to 10<sup>0</sup>, since it was 5\n 0000 0000 0000 0011 0001   1101101110000000   Shift left (5th)\n 0000 0000 0000 0110 0011   1011011100000000   Shift left (6th)\n 0000 0000 0000 1001 0011   1011011100000000   Add 3 to 10<sup>1</sup>, since it was 6\n 0000 0000 0001 0010 0111   0110111000000000   Shift left (7th)\n 0000 0000 0001 0010 1010   0110111000000000   Add 3 to 10<sup>0</sup>, since it was 7\n 0000 0000 0010 0101 0100   1101110000000000   Shift left (8th)\n 0000 0000 0010 1000 0100   1101110000000000   Add 3 to 10<sup>1</sup>, since it was 5\n 0000 0000 0101 0000 1001   1011100000000000   Shift left (9th)\n 0000 0000 1000 0000 1001   1011100000000000   Add 3 to 10<sup>2</sup>, since it was 5\n 0000 0000 1000 0000 1100   1011100000000000   Add 3 to 10<sup>0</sup>, since it was 9\n 0000 0001 0000 0001 1001   0111000000000000   Shift left (10th)\n 0000 0001 0000 0001 1100   0111000000000000   Add 3 to 10<sup>0</sup>, since it was 9\n 0000 0010 0000 0011 1000   1110000000000000   Shift left (11th)\n 0000 0010 0000 0011 1011   1110000000000000   Add 3 to 10<sup>0</sup>, since it was 8\n 0000 0100 0000 0111 0111   1100000000000000   Shift left (12th)\n 0000 0100 0000 1010 0111   1100000000000000   Add 3 to 10<sup>1</sup>, since it was 7\n 0000 0100 0000 1010 1010   1100000000000000   Add 3 to 10<sup>0</sup>, since it was 7\n 0000 1000 0001 0101 0101   1000000000000000   Shift left (13th)\n 0000 1011 0001 0101 0101   1000000000000000   Add 3 to 10<sup>3</sup>, since it was 8\n 0000 1011 0001 1000 0101   1000000000000000   Add 3 to 10<sup>1</sup>, since it was 5\n 0000 1011 0001 1000 1000   1000000000000000   Add 3 to 10<sup>0</sup>, since it was 5\n 0001 0110 0011 0001 0001   0000000000000000   Shift left (14th)\n 0001 1001 0011 0001 0001   0000000000000000   Add 3 to 10<sup>3</sup>, since it was 6\n 0011 0010 0110 0010 0010   0000000000000000   Shift left (15th)\n 0011 0010 1001 0010 0010   0000000000000000   Add 3 to 10<sup>2</sup>, since it was 6\n 0110 0101 0010 0100 0100   0000000000000000   Shift left (16th)\n    6    5    2    4    4\n             BCD\n\nSixteen shifts have been performed, so the algorithm terminates. The BCD digits is: 6*10<sup>4</sup> + 5*10<sup>3</sup> + 2*10<sup>2</sup> + 4*10<sup>1</sup> + 4*10<sup>0</sup> = 65244.\n\n==C implementation==\nThe double dabble algorithm might look like this when implemented in [[C (programming language)|C]]. Notice that this implementation is designed to convert an \"input register\" of any width, by taking an array as its parameter and returning a [[malloc|dynamically allocated]] string. Also notice that this implementation does not store an explicit copy of the input register in its scratch space, as the description of the algorithm did; copying the input register into the scratch space was just a [[pedagogical]] device.\n\n<!-- EDITORS: If you think this code should be changed, please TEST your changes before committing! -->\n<source lang=\"c\">\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n/*\n   This function takes an array of n unsigned integers,\n   each holding a value in the range [0, 65535],\n   representing a number in the range [0, 2**(16n)-1].\n   arr[0] is the most significant \"digit\".\n   This function returns a new array containing the given\n   number as a string of decimal digits.\n\n   For the sake of brevity, this example assumes that\n   calloc and realloc will never fail.\n*/\nvoid double_dabble(int n, const unsigned int *arr, char **result)\n{\n    int nbits = 16*n;         /* length of arr in bits */\n    int nscratch = nbits/3;   /* length of scratch in bytes */\n    char *scratch = calloc(1 + nscratch, sizeof *scratch);\n    int i, j, k;\n    int smin = nscratch-2;    /* speed optimization */\n\n    for (i=0; i < n; ++i) {\n        for (j=0; j < 16; ++j) {\n            /* This bit will be shifted in on the right. */\n            int shifted_in = (arr[i] & (1 << (15-j)))? 1: 0;\n\n            /* Add 3 everywhere that scratch[k] >= 5. */\n            for (k=smin; k < nscratch; ++k)\n              scratch[k] += (scratch[k] >= 5)? 3: 0;\n\n            /* Shift scratch to the left by one position. */\n            if (scratch[smin] >= 8)\n              smin -= 1;\n            for (k=smin; k < nscratch-1; ++k) {\n                scratch[k] <<= 1;\n                scratch[k] &= 0xF;\n                scratch[k] |= (scratch[k+1] >= 8);\n            }\n\n            /* Shift in the new bit from arr. */\n            scratch[nscratch-1] <<= 1;\n            scratch[nscratch-1] &= 0xF;\n            scratch[nscratch-1] |= shifted_in;\n        }\n    }\n\n    /* Remove leading zeros from the scratch space. */\n    for (k=0; k < nscratch-1; ++k)\n      if (scratch[k] != 0) break;\n    nscratch -= k;\n    memmove(scratch, scratch+k, nscratch+1);\n\n    /* Convert the scratch space from BCD digits to ASCII. */\n    for (k=0; k < nscratch; ++k)\n      scratch[k] += '0';\n\n    /* Resize and return the resulting string. */\n    *result = realloc(scratch, nscratch+1);\n    return;\n}\n\n/*\n   This test driver should print the following decimal values:\n   246\n   16170604\n   1059756703745\n*/\nint main(void)\n{\n    unsigned int arr[] = { 246, 48748, 1 };\n    char *text = NULL;\n    int i;\n    for (i=0; i < 3; ++i) {\n        double_dabble(i+1, arr, &text);\n        printf(\"%s\\n\", text);\n        free(text);\n    }\n    return 0;\n}\n</source>\n\n==VHDL implementation==\n<source lang=\"vhdl\">\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\nuse IEEE.numeric_std.all;\n\n\nentity bin2bcd_12bit is\n    Port ( binIN : in  STD_LOGIC_VECTOR (11 downto 0);\n           ones : out  STD_LOGIC_VECTOR (3 downto 0);\n           tens : out  STD_LOGIC_VECTOR (3 downto 0);\n           hundreds : out  STD_LOGIC_VECTOR (3 downto 0);\n           thousands : out  STD_LOGIC_VECTOR (3 downto 0)\n          );\nend bin2bcd_12bit;\n\narchitecture Behavioral of bin2bcd_12bit is\n\nbegin\n\nbcd1: process(binIN)\n\n  -- temporary variable\n  variable temp : STD_LOGIC_VECTOR (11 downto 0);\n  \n  -- variable to store the output BCD number\n  -- organized as follows\n  -- thousands = bcd(15 downto 12)\n  -- hundreds = bcd(11 downto 8)\n  -- tens = bcd(7 downto 4)\n  -- units = bcd(3 downto 0)\n  variable bcd : UNSIGNED (15 downto 0) := (others => '0');\n\n  -- by\n  -- https://en.wikipedia.org/wiki/Double_dabble\n  \n  begin\n    -- zero the bcd variable\n    bcd := (others => '0');\n    \n    -- read input into temp variable\n    temp(11 downto 0) := binIN;\n    \n    -- cycle 12 times as we have 12 input bits\n    -- this could be optimized, we do not need to check and add 3 for the \n    -- first 3 iterations as the number can never be >4\n    for i in 0 to 11 loop\n    \n      if bcd(3 downto 0) > 4 then \n        bcd(3 downto 0) := bcd(3 downto 0) + 3;\n      end if;\n      \n      if bcd(7 downto 4) > 4 then \n        bcd(7 downto 4) := bcd(7 downto 4) + 3;\n      end if;\n    \n      if bcd(11 downto 8) > 4 then  \n        bcd(11 downto 8) := bcd(11 downto 8) + 3;\n      end if;\n    \n      -- thousands can't be >4 for a 12-bit input number\n      -- so don't need to do anything to upper 4 bits of bcd\n    \n      -- shift bcd left by 1 bit, copy MSB of temp into LSB of bcd\n      bcd := bcd(14 downto 0) & temp(11);\n    \n      -- shift temp left by 1 bit\n      temp := temp(10 downto 0) & '0';\n    \n    end loop;\n \n    -- set outputs\n    ones <= STD_LOGIC_VECTOR(bcd(3 downto 0));\n    tens <= STD_LOGIC_VECTOR(bcd(7 downto 4));\n    hundreds <= STD_LOGIC_VECTOR(bcd(11 downto 8));\n    thousands <= STD_LOGIC_VECTOR(bcd(15 downto 12));\n  \n  end process bcd1;            \n  \nend Behavioral;\n\n</source>\n\n===VHDL testbench===\n<source lang=\"vhdl\">\nLIBRARY ieee;\nUSE ieee.std_logic_1164.ALL;\n \nENTITY bin2bcd_12bit_test_file IS\nEND bin2bcd_12bit_test_file;\n \nARCHITECTURE behavior OF bin2bcd_12bit_test_file IS \n \n    -- Component Declaration for the Unit Under Test (UUT)\n \n    COMPONENT bin2bcd_12bit\n    PORT(\n         binIN : IN  std_logic_vector(11 downto 0);\n         ones : OUT  std_logic_vector(3 downto 0);\n         tens : OUT  std_logic_vector(3 downto 0);\n         hundreds : OUT  std_logic_vector(3 downto 0);\n\t thousands : OUT  std_logic_vector(3 downto 0)\n        );\n    END COMPONENT;\n    \n  -- WARNING: Please, notice that there is no need for a clock signal in the testbench, since the design is strictly\n  --    combinational (or concurrent, in contrast to the C implementation which is sequential).\n  -- This clock is here just for simulation; you can omit all clock references and process, and use \"wait for ... ns\"\n  --    statements instead.\n\n   --Inputs\n   signal binIN : std_logic_vector(11 downto 0) := (others => '0');\n   signal clk : std_logic := '0';  -- can be omitted\n\n \t--Outputs\n   signal ones : std_logic_vector(3 downto 0);\n   signal tenths : std_logic_vector(3 downto 0);\n   signal hunderths : std_logic_vector(3 downto 0);\n   signal thousands : std_logic_vector(3 downto 0);\n\n   -- Clock period definitions\n   constant clk_period : time := 10 ns;  -- can be omitted\n\n   -- Miscellaneous\n   signal full_number : std_logic_vector(15 downto 0);\n\nBEGIN\n \n\t-- Instantiate the Unit Under Test (UUT)\n   uut: bin2bcd_12bit PORT MAP (\n          binIN => binIN,\n          ones => ones,\n          tens => tenths,\n          hundreds => hunderths,\n\t  thousands => thousands\n        );\n\n   -- Clock process definitions  -- the whole process can be omitted\n   clk_process :process\n   begin\n\t\tclk <= '0';\n\t\twait for clk_period/2;\n\t\tclk <= '1';\n\t\twait for clk_period/2;\n   end process;\n \n   -- Combine signals for full number\n   full_number <= thousands & hunderths & tenths & ones;\n\n   -- Stimulus process\n   stim_proc: process\n   begin\t\t\n      -- hold reset state for 100 ns.\n      wait for 100 ns;\t\n\n      wait for clk_period*10;\n\n      -- insert stimulus here \n\t\t-- should return 4095\n\t\tbinIN <= X\"FFF\";\n\t\twait for clk_period*10;  assert full_number = x\"4095\" severity error;  -- use \"wait for ... ns;\"\n\n\t\t-- should return 0\n\t\tbinIN <= X\"000\";\n\t\twait for clk_period*10;  assert full_number = x\"0000\" severity error;\n\n\t\t-- should return 2748\n\t\tbinIN <= X\"ABC\";\n\t\twait for clk_period*10;  assert full_number = x\"2748\" severity error;\n\t\t\n\t\t\n      wait;\n   end process;\n\nEND;\n\n</source>\n\n==Optimized snippet Bin2BCD for SBA (VHDL)==\n<ref name=\"SBA\"/><source lang=\"vhdl\">\n-- /SBA: Program Details --===================================================--\n-- Snippet: 16 bit Binary to BCD converter\n-- Author: Miguel A. Risco-Castillo\n-- Description: 16 bit to BCD converter using \"Double Dabble\" algorithm.\n-- Before call you must fill \"bin_in\" with the appropriate value, after called,\n-- the snippet put into the variable \"bcd_out\" the BCD result of the conversion.\n-- Put the snippet in the routines section of the user program.\n-- /SBA: End Program Details ---------------------------------------------------\n\n-- /SBA: User Registers and Constants --======================================--\n  variable bin_in  : unsigned(15 downto 0);      -- 16 bit input register\n  variable bcd_out : unsigned(19 downto 0);      -- 20 bit output register\n-- /SBA: End User Registers and Constants --------------------------------------\n\n-- /SBA: User Program --======================================================--\n-- /L:Bin2BCD\n=> bcd_out := (others=>'0');\n   if bin_in=0 then SBARet; end if; -- if zero then return\n=> bcd_out(2 downto 0) := bin_in(15 downto 13); -- shl 3\n   bin_in := bin_in(12 downto 0) & \"000\";\n=> for j in 0 to 12 loop\n     for i in 0 to 3 loop -- for nibble 0 to 3, last nibble do not need adjust\n       if bcd_out(3+4*i downto 4*i)>4 then -- is nibble > 4?\n         bcd_out(3+4*i downto 4*i):=bcd_out(3+4*i downto 4*i)+3; -- add 3 to nibble\n       end if;\n     end loop;\n     bcd_out := bcd_out(18 downto 0) & bin_in(15); --shl\n     bin_in := bin_in(14 downto 0) & '0';\n   end loop;\n   SBARet; -- return to main program\n-- /SBA: End User Program ------------------------------------------------------\n</source>\n\n==Historical==\nIn the 1960s, the term '''double dabble''' was also used for a different mental algorithm, used by programmers to convert a binary number to decimal. It is performed by reading the binary number from left to right, doubling if the next bit is zero, and doubling and adding one if the next bit is one.<ref name=\"Godse\"/> In the example above, 11110011, the thought process would be: \"one, three, seven, fifteen, thirty, sixty, one hundred twenty-one, two hundred forty-three\", the same result as that obtained above.\n\n==See also==\n* [[Lookup table]]{{snd}} an alternate approach to perform conversion\n\n==References==\n{{Reflist|refs=\n<ref name=\"Gao_2012_1\">{{citation |author-last1=Gao |author-first1=Shuli |author-last2=Al-Khalili |author-first2=D. |author-last3=Chabini |author-first3=N. |contribution=An improved BCD adder using 6-LUT FPGAs |date=June 2012 |doi=10.1109/NEWCAS.2012.6328944 |pages=13–16 |title=IEEE 10th International New Circuits and Systems Conference (NEWCAS 2012)}}</ref>\n<ref name=\"Gao_2012_2\">{{cite web |title=Binary-to-BCD Converter: \"Double-Dabble Binary-to-BCD Conversion Algorithm\" |url=http://edda.csie.dyu.edu.tw/course/fpga/Binary2BCD.pdf |archive-url=https://web.archive.org/web/20120131075956/http://edda.csie.dyu.edu.tw/course/fpga/Binary2BCD.pdf |archive-date=2012-01-31}}</ref>\n<ref name=\"Véstias_2010\">{{citation |author-last1=Véstias |author-first1=Mario P. |author-last2=Neto |author-first2=Horatio C. |contribution=Parallel decimal multipliers using binary multipliers |date=March 2010 |doi=10.1109/SPL.2010.5483001 |pages=73–78 |title=VI Southern Programmable Logic Conference (SPL 2010)}}</ref>\n<ref name=\"SBA\">{{cite web |title=SBA |quote=The Simple Bus Architecture |website=sba.accesus.com |url=http://sba.accesus.com |access-date=2016-12-31}}</ref>\n<ref name=\"Godse\">{{cite book |title=Digital Techniques |author-first1=Deepali A. |author-last1=Godse |author-first2=Atul P. |author-last2=Godse |publisher=Technical Publications |date=2008 |edition= |isbn=978-8-18431401-4 |page=4 |location=Pune, India |url=https://books.google.com/books?id=F6Vor58f9FYC&pg=PA4}}</ref>\n}}\n\n==Further reading==\n*{{cite web |title=An Explanation of the Double-Dabble Bin-BCD Conversion Algorithm |author-first=Charles \"Chuck\" B. |author-last=Falconer<!-- *1931-09-13 Switzerland to +2012-06-04 Damariscotta, Maine --> |date=2004-04-16 |url=http://cbfalconer.home.att.net/download/dubldabl.txt |dead-url=yes |archive-url=https://web.archive.org/web/20090325002523/http://cbfalconer.home.att.net/download/dubldabl.txt |archive-date=2009-03-25}}\n\n{{DEFAULTSORT:Double Dabble}}\n\n[[Category:Shift-and-add algorithms]]\n[[Category:Articles with example C code]]\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Double-precision floating-point format",
      "url": "https://en.wikipedia.org/wiki/Double-precision_floating-point_format",
      "text": "'''Double-precision floating-point format''' is a [[computer number format]], usually occupying 64 bits in computer memory; it represents a wide [[dynamic range]] of numeric values by using a floating radix point.\n\nFloating point is used to represent fractional values, or when a wider range is needed than is provided by [[fixed-point arithmetic|fixed point]] (of the same bit width), even if at the cost of precision. Double precision may be chosen when the range or precision of [[single-precision floating-point format|single precision]] would be insufficient.\n\nIn the [[IEEE 754-2008]] [[standardization|standard]], the 64-bit base-2 format is officially referred to as '''binary64'''; it was called '''double''' in [[IEEE 754-1985]]. IEEE 754 specifies additional floating-point formats, including 32-bit base-2 ''single precision'' and, more recently, base-10 representations.\n\nOne of the first [[programming language]]s to provide single- and double-precision floating-point data types was [[Fortran]]. Before the widespread adoption of IEEE 754-1985, the representation and properties of floating-point data types depended on the [[computer manufacturer]] and computer model, and upon decisions made by programming-language implementers. E.g., [[GW-BASIC]]'s double-precision data type was the [[64-bit MBF]] floating-point format.\n{{Floating-point}}\n\n==IEEE 754 double-precision binary floating-point format: binary64==\nDouble-precision binary floating-point is a commonly used format on PCs, due to its wider range over single-precision floating point, in spite of its performance and bandwidth cost. As with [[Single-precision floating-point format|single-precision]] floating-point format, it lacks precision on integer numbers when compared with an integer format of the same size. It is commonly known simply as ''double''. The IEEE 754 standard specifies a '''binary64''' as having:\n* [[Sign bit]]: 1 bit\n* [[Exponent]]: 11 bits\n* [[Significand]] [[precision (arithmetic)|precision]]: 53 bits (52 explicitly stored)\nThe sign bit determines the sign of the number (including when this number is zero, which is [[signed zero|signed]]).\n\nThe exponent field can be interpreted as either an 11-bit signed integer from −1024 to 1023 ([[2's complement]]) or an 11-bit unsigned integer from 0 to 2047, which is the accepted biased form in the IEEE 754 binary64 definition. If the unsigned integer format is used, the exponent value used in the arithmetic is the exponent shifted by a bias – for the IEEE 754 binary64 case, an exponent value of 1023 represents the actual zero (i.e. for 2<sup>e − 1023</sup> to be one, e must be 1023). Exponents range from −1022 to +1023 because exponents of −1023 (all 0s) and +1024 (all 1s) are reserved for special numbers.\n\nThe 53-bit significand precision gives from 15 to 17 [[Significant figures|significant decimal digits]] precision (2<sup>−53</sup>&nbsp;≈&nbsp;1.11&nbsp;×&nbsp;10<sup>−16</sup>). If a decimal string with at most 15 significant digits is converted to IEEE 754 double-precision representation, and then converted back to a decimal string with the same number of digits, the final result should match the original string. If an IEEE 754 double-precision number is converted to a decimal string with at least 17 significant digits, and then converted back to double-precision representation, the final result must match the original number.<ref name=\"whyieee\">{{cite web|url=http://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF|title=Lecture Notes on the Status of IEEE Standard 754 for Binary Floating-Point Arithmetic|author=William Kahan|date=1 October 1997|deadurl=no|archiveurl=https://web.archive.org/web/20120208075518/http://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF|archivedate=8 February 2012|df=}}</ref>\n\nThe format is written with the [[significand]] having an implicit integer bit of value 1 (except for special data, see the exponent encoding below).  With the 52 bits of the fraction significand appearing in the memory format, the total precision is therefore 53 bits (approximately 16 decimal digits, 53 log<sub>10</sub>(2) ≈ 15.955).  The bits are laid out as follows:\n\n[[File:IEEE 754 Double Floating Point Format.svg]]\n\nThe real value assumed by a given 64-bit double-precision datum with a given [[Exponent bias|biased exponent]] <math>e</math> \nand a 52-bit fraction is\n: <math> (-1)^{\\text{sign}}(1.b_{51}b_{50}...b_{0})_2 \\times 2^{e-1023} </math>\nor\n: <math> (-1)^{\\text{sign}}\\left(1 + \\sum_{i=1}^{52} b_{52-i} 2^{-i} \\right)\\times 2^{e-1023} </math>\n\nBetween 2<sup>52</sup>=4,503,599,627,370,496 and 2<sup>53</sup>=9,007,199,254,740,992 the representable numbers are exactly the integers. For the next range, from 2<sup>53</sup> to 2<sup>54</sup>, everything is multiplied by 2, so the representable numbers are the even ones, etc. Conversely, for the previous range from 2<sup>51</sup> to 2<sup>52</sup>, the spacing is 0.5, etc.\n\nThe spacing as a fraction of the numbers in the range from 2<sup>''n''</sup> to 2<sup>''n''+1</sup> is 2<sup>''n''−52</sup>.\nThe maximum relative rounding error when rounding a number to the nearest representable one (the [[machine epsilon]]) is therefore 2<sup>−53</sup>.\n\nThe 11 bit width of the exponent allows the representation of numbers between 10<sup>−308</sup> and 10<sup>308</sup>, with full 15–17 decimal digits precision. By compromising precision, the subnormal representation allows even smaller values up to about 5&nbsp;×&nbsp;10<sup>−324</sup>.\n\n===Exponent encoding===\nThe double-precision binary floating-point exponent is encoded using an [[offset-binary]] representation, with the zero offset being 1023; also known as exponent bias in the IEEE 754 standard. Examples of such representations would be:\n{|\n|-\n|''e'' =<code>00000000001<sub>2</sub></code>=<code>001<sub>16</sub></code>=1:\n|style=\"width: 0.4em\"|\n| <math>2^{1-1023}=2^{-1022}</math>\n|(smallest exponent for [[Normal number (computing)|normal numbers]])\n|-\n|''e'' =<code>01111111111<sub>2</sub></code>=<code>3ff<sub>16</sub></code>=1023:\n|\n|<math>2^{1023-1023}=2^0</math>\n|(zero offset)\n|-\n|''e'' =<code>10000000101<sub>2</sub></code>=<code>405<sub>16</sub></code>=1029:\n|\n|<math>2^{1029-1023}=2^6</math>\n|\n|-\n|''e'' =<code>11111111110<sub>2</sub></code>=<code>7fe<sub>16</sub></code>=2046:\n|\n|<math>2^{2046-1023}=2^{1023}</math>\n|(highest exponent)\n|}\n\nThe exponents <code>000<sub>16</sub></code> and <code>7ff<sub>16</sub></code> have a special meaning:\n* <code>00000000000<sub>2</sub></code>=<code>000<sub>16</sub></code> is used to represent a [[signed zero]] (if ''F'' = 0) and [[Denormal number|subnormals]] (if ''F'' ≠ 0); and\n* <code>11111111111<sub>2</sub></code>=<code>7ff<sub>16</sub></code> is used to represent [[infinity|∞]] (if ''F'' = 0) and [[NaN]]s (if ''F'' ≠ 0),\nwhere ''F'' is the fractional part of the [[significand]]. All bit patterns are valid encoding.\n\nExcept for the above exceptions, the entire double-precision number is described by:\n\n: <math>(-1)^{\\text{sign}} \\times 2^{e - 1023} \\times 1.\\text{fraction}</math>\n\nIn the case of [[Denormal number|subnormals]] (''e'' = 0) the double-precision number is described by:\n\n: <math>(-1)^{\\text{sign}} \\times 2^{1-1023} \\times 0.\\text{fraction} = (-1)^{\\text{sign}} \\times 2^{-1022} \\times 0.\\text{fraction}</math>\n\n===Endianness===\n{{transcluded section|source=Endianness}}\n{{trim|{{#section:Endianness|Floating-point}}}}\n\n===Double-precision examples===\n{| style=\"background-color: #f8f9fa; margin: 1em 0; border: 1px solid #eaecf0;\"\n|-\n| <code>0 01111111111 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>0</sup> × 1 = 1\n|-\n| <code>0 01111111111 0000000000000000000000000000000000000000000000000001<sub>2</sub></code> ≙ +2<sup>0</sup> × (1 + 2<sup>−52</sup>) ≈ 1.0000000000000002, the smallest number > 1\n|-\n| <code>0 01111111111 0000000000000000000000000000000000000000000000000010<sub>2</sub></code> ≙ +2<sup>0</sup> × (1 + 2<sup>−51</sup>) ≈ 1.0000000000000004\n|-\n| <code>0 10000000000 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>1</sup> × 1 = 2\n|-\n| <code>1 10000000000 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ −2<sup>1</sup> × 1 = −2\n|}\n\n{| style=\"background-color: #f8f9fa; margin: 1em 0; border: 1px solid #eaecf0;\"\n|-\n| <code>0 10000000000 1000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>1</sup> × 1.1<sub>2</sub>\n| = 11<sub>2</sub> = 3\n|-\n| <code>0 10000000001 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>2</sup> × 1 \n| = 100<sub>2</sub> = 4\n|-\n| <code>0 10000000001 0100000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>2</sup> × 1.01<sub>2</sub> \n| = 101<sub>2</sub> = 5\n|-\n| <code>0 10000000001 1000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>2</sup> × 1.1<sub>2</sub> \n| = 110<sub>2</sub> = 6\n|-\n| <code>0 10000000011 0111000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>4</sup> × 1.0111<sub>2</sub> \n| = 10111<sub>2</sub> = 23\n|-\n| <code>0 01111111000 1000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +2<sup>−7</sup> × 1.1<sub>2</sub> \n| = 0.00000011<sub>2</sub> = 0.01171875 (3/256)\n|}\n{| style=\"background-color: #f8f9fa; margin: 1em 0; border: 1px solid #eaecf0;\"\n|- style=\"vertical-align: top;\"\n| <code>0 00000000000 0000000000000000000000000000000000000000000000000001<sub>2</sub></code>\n| ≙ +2<sup>−1022</sup> × 2<sup>−52</sup> = 2<sup>−1074</sup> <br/> ≈ 4.9406564584124654 × 10<sup>−324</sup>\n| (Min. subnormal positive double)\n|- style=\"vertical-align: top;\"\n| <code>0 00000000000 1111111111111111111111111111111111111111111111111111<sub>2</sub></code>\n| ≙ +2<sup>−1022</sup> × (1 − 2<sup>−52</sup>) <br/> ≈ 2.2250738585072009 × 10<sup>−308</sup>\n| (Max. subnormal double)\n|- style=\"vertical-align: top;\"\n| <code>0 00000000001 0000000000000000000000000000000000000000000000000000<sub>2</sub></code>\n| ≙ +2<sup>−1022</sup> × 1 <br/> ≈ 2.2250738585072014 × 10<sup>−308</sup>\n| (Min. normal positive double)\n|- style=\"vertical-align: top;\"\n| <code>0 11111111110 1111111111111111111111111111111111111111111111111111<sub>2</sub></code>\n| ≙ +2<sup>1023</sup> × (1 + (1 − 2<sup>−52</sup>)) <br/> ≈ 1.7976931348623157 × 10<sup>308</sup>\n| (Max. Double)\n|}\n\n{| style=\"background-color: #f8f9fa; margin: 1em 0; border: 1px solid #eaecf0;\"\n|-\n| <code>0 00000000000 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +0\n|\n|-\n| <code>1 00000000000 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ −0\n|\n|-\n| <code>0 11111111111 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ +∞\n| (positive infinity)\n|-\n| <code>1 11111111111 0000000000000000000000000000000000000000000000000000<sub>2</sub></code> ≙ −∞\n| (negative infinity)\n|-\n| <code>0 11111111111 0000000000000000000000000000000000000000000000000001<sub>2</sub></code> ≙ NaN\n| (sNaN on most processors, such as x86 and ARM)\n|\n|-\n| <code>0 11111111111 1000000000000000000000000000000000000000000000000001<sub>2</sub></code> ≙ NaN\n| (qNaN on most processors, such as x86 and ARM)\n|\n|-\n| <code>0 11111111111 1111111111111111111111111111111111111111111111111111<sub>2</sub></code> ≙ NaN\n| (an alternative encoding)\n|}\n\n{| style=\"background-color: #f8f9fa; margin: 1em 0; border: 1px solid #eaecf0;\"\n|-\n| <code>0 01111111101 0101010101010101010101010101010101010101010101010101<sub>2</sub></code> <br/> = <code>3fd5 5555 5555 5555<sub>16</sub></code>\n| ≙ +2<sup>−2</sup> × (1 + 2<sup>−2</sup> + 2<sup>−4</sup> + ... + 2<sup>−52</sup>) <br/> ≈ <sup>1</sup>/<sub>3</sub>\n|}\n\n{| style=\"background-color: #f8f9fa; margin: 1em 0; border: 1px solid #eaecf0;\"\n|-\n| <code>0 10000000000 1001001000011111101101010100010001000010110100011000<sub>2</sub></code> <br/> = <code>4009 21fb 5444 2d18<sub>16</sub></code>\n| ≈ pi\n|}\n\n[[NaN#Encoding|Encodings of qNaN and sNaN]] are not completely specified in [[IEEE floating point|IEEE 754]] and depend on the processor. Most processors, such as the [[x86]] family and the [[ARM architecture|ARM]] family processors, use the most significant bit of the significand field to indicate a quiet NaN; this is what is recommended by IEEE 754. The [[PA-RISC]] processors use the bit to indicate a signaling NaN.\n\nBy default, <sup>1</sup>/<sub>3</sub> rounds down, instead of up like [[single precision]], because of the odd number of bits in the significand.\n\nIn more detail:\n Given the hexadecimal representation 3FD5 5555 5555 5555<sub>16</sub>,\n   Sign = 0\n   Exponent = 3FD<sub>16</sub> = 1021\n   Exponent Bias = 1023 (constant value; see above)\n   Fraction = 5 5555 5555 5555<sub>16</sub>\n   Value = 2<sup>(Exponent − Exponent Bias)</sup> × 1.Fraction – Note that Fraction must not be converted to decimal here\n         = 2<sup>−2</sup> × (15 5555 5555 5555<sub>16</sub> × 2<sup>−52</sup>)\n         = 2<sup>−54</sup> × 15 5555 5555 5555<sub>16</sub>\n         = 0.333333333333333314829616256247390992939472198486328125\n         ≈ 1/3\n\n===Execution speed with double-precision arithmetic===\nUsing double-precision floating-point variables and mathematical functions (e.g., sin, cos, atan2, log, exp and sqrt) are slower than working with their single precision counterparts. One area of computing where this is a particular issue is for parallel code running on GPUs. For example, when using [[Nvidia|NVIDIA]]'s [[CUDA]] platform, calculations with double precision take approximately 2 times longer to complete compared to those done using [[Single-precision floating-point format|single precision]].<ref>{{Cite news|url=https://www.tomshardware.com/news/nvidia-titan-v-110-teraflops,36085.html|title=Nvidia’s New Titan V Pushes 110 Teraflops From A Single Chip|date=2017-12-08|work=Tom's Hardware|access-date=2018-11-05|language=en}}</ref>\n\n==Implementations==\nDoubles are implemented in many programming languages in different ways such as the following. On processors with only dynamic precision, such as [[x86]] without [[SSE2]] (or when SSE2 is not used, for compatibility purpose) and with extended precision used by default, software may have difficulties to fulfill some requirements.\n\n===C and C++===\nC and C++ offer a wide variety of [[C data types#Basic types|arithmetic types]]. Double precision is not required by the standards (except by the optional annex F of [[C99]], covering IEEE 754 arithmetic), but on most systems, the <code>double</code> type corresponds to double precision. However, on 32-bit x86 with extended precision by default, some compilers may not conform to the C standard and/or the arithmetic may suffer from [[Rounding#Double rounding|double rounding]].<ref>{{cite web|url=https://gcc.gnu.org/bugzilla/show_bug.cgi?id=323|title=Bug 323 – optimized code gives strange floating point results|author=|date=|website=gcc.gnu.org|accessdate=30 April 2018|deadurl=no|archiveurl=https://web.archive.org/web/20180430012629/https://gcc.gnu.org/bugzilla/show_bug.cgi?id=323|archivedate=30 April 2018|df=}}</ref>\n\n===Common Lisp===\n[[Common Lisp]] provides the types SHORT-FLOAT, SINGLE-FLOAT, DOUBLE-FLOAT and LONG-FLOAT.  Most implementations provide SINGLE-FLOATs and DOUBLE-FLOATs with the other types appropriate synonyms. [[Common Lisp]] provides exceptions for catching floating-point underflows and overflows, and the inexact floating-point exception, as per IEEE 754.  No infinities and NaNs are described in the ANSI standard, however, several implementations do provide these as extensions.\n\n===Java===\n\nOn [[Java (programming language)|Java]] before version 1.2, every implementation had to be IEEE 754 compliant. Version 1.2 allowed implementations to bring extra precision in intermediate computations for platforms like [[x87]]. Thus a modifier [[strictfp]] was introduced to enforce strict IEEE 754 computations.\n\n===JavaScript===\nAs specified by the [[ECMAScript]] standard, all arithmetic in [[JavaScript (programming language)|JavaScript]] shall be done using double-precision floating-point arithmetic.<ref>{{cite book | title=ECMA-262 ECMAScript Language Specification | url=http://www.ecma-international.org/publications/files/ECMA-ST-ARCH/ECMA-262%205th%20edition%20December%202009.pdf | edition=5th | publisher=Ecma International | at=p. 29, §8.5 ''The Number Type'' | deadurl=no | archiveurl=https://web.archive.org/web/20120313145717/http://www.ecma-international.org/publications/files/ECMA-ST-ARCH/ECMA-262%205th%20edition%20December%202009.pdf | archivedate=2012-03-13 | df= }}</ref>\n<!-- \"shall be\" instead of \"is\" because this may not be the case in practice on processors with only dynamic precision. For instance, Mozilla's Javascript engine had such a problem in the past: https://bugzilla.mozilla.org/show_bug.cgi?id=264912 -->\n\n==See also==\n* [[IEEE 754]], IEEE standard for floating-point arithmetic\n\n==Notes and references==\n{{Reflist}}\n\n{{data types}}\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Floating point types]]"
    },
    {
      "title": "Excess-3",
      "url": "https://en.wikipedia.org/wiki/Excess-3",
      "text": "{{Redir|XS-3|the experimental aircraft|Douglas XS-3 Stiletto}}\n{{Use dmy dates|date=May 2019|cs1-dates=y}}\n{{Infobox code\n|name=Stibitz code\n|digits=4<ref name=\"Steinbuch_1962\"/>\n|tracks=4<ref name=\"Steinbuch_1962\"/>\n|digit_values=8&nbsp;&nbsp;4&nbsp;-{{overline|2}}&nbsp;-{{overline|1}}\n|weight=1..3<ref name=\"Steinbuch_1962\"/>\n|continuity=no<ref name=\"Steinbuch_1962\"/>\n|cyclic=no<ref name=\"Steinbuch_1962\"/>\n|minimum_distance=1<ref name=\"Steinbuch_1962\"/>\n|maximum_distance=4\n<!-- |hamming_distance=1 -->\n|redundancy=0.7\n|lexicography=1<ref name=\"Steinbuch_1962\"/>\n|complement=9<ref name=\"Steinbuch_1962\"/>\n}}\n\n'''Excess-3''', '''3-excess'''<ref name=\"Steinbuch_1962\"/><ref name=\"Steinbuch-Weber_1974\"/><ref name=\"Richards_1955\"/> or '''10-excess-3''' [[binary number|binary]] code (often abbreviated as '''XS-3''', '''3XS'''<ref name=\"Steinbuch_1962\"/> or '''X3'''<ref name=\"Schmid_1974\"/><ref name=\"Schmid_1983\"/>) or '''Stibitz code'''<ref name=\"Steinbuch_1962\"/><ref name=\"Steinbuch-Weber_1974\"/> (after [[George Stibitz]], who built a relay-based adding machine in 1937<ref name=\"Mietke_2017\"/><ref name=\"Ritchie_1986\"/>) is a self-complementary [[binary-coded decimal]] (BCD) code and [[numeral system]]. It is a [[offset binary|biased representation]]. Excess-3 code was used on some older computers as well as in cash registers and hand-held portable electronic calculators of the 1970s, among other uses.\n\n==Representation==\nBiased codes are a way to represent values with a balanced number of positive and negative numbers using a pre-specified number ''N'' as a biasing value. Biased codes (and [[Gray code]]s) are non-weighted codes. In excess-3 code, numbers are represented as decimal digits, and each digit is represented by four [[bit]]s as the digit value plus 3 (the \"excess\" amount):\n*The smallest binary number represents the smallest value ({{nobr|0 − excess}}).\n*The greatest binary number represents the largest value ({{nobr|2<sup>''N''+1</sup> − excess − 1}}).\n\n{| class=\"wikitable\"\n|+ Excess-3 / Stibitz code\n|-\n! Decimal\n! Excess-3\n! Stibitz\n! [[BCD 8-4-2-1]]\n! Binary\n! 3-of-6 [[CCITT]] extension<ref name=\"CCITT_1959\"/><ref name=\"Steinbuch_1962\"/>\n! 4-of-8 [[Hamming code|Hamming]] extension<ref name=\"Steinbuch_1962\"/>\n|- align=\"center\" style=\"background:white\"\n| &minus;3\n| style=\"background:#F000FF; color: #fff;\" | '''0000'''\n| [[pseudo-tetrade]]\n| N/A\n| N/A\n| N/A\n| N/A\n|- align=\"center\" style=\"background:white\"\n| &minus;2\n| style=\"background:#8000FF; color: #FFF;\" | '''0001'''\n| pseudo-tetrade\n| N/A\n| N/A\n| N/A\n| N/A\n|- align=\"center\" style=\"background:white\"\n| &minus;1\n| style=\"background:#0000FF; color: #FFF;\" | '''0010'''\n| pseudo-tetrade\n| N/A\n| N/A\n| N/A\n| N/A\n|- align=\"center\"\n| '''0'''\n| style=\"background:#008888; color: #FFF;\" | '''0011'''\n| '''0011'''\n| 0000\n| 0000\n| …'''10'''\n| …'''0011'''\n|- align=\"center\"\n| '''1'''\n| style=\"background:#00AA00; color: #FFF;\" | '''0100'''\n| '''0100'''\n| 0001\n| 0001\n| …'''11'''\n| …'''1011'''\n|- align=\"center\"\n| '''2'''\n| style=\"background:#808000; color: #FFF;\" | '''0101'''\n| '''0101'''\n| 0010\n| 0010\n| …'''10'''\n| …'''0101'''\n|- align=\"center\"\n| '''3'''\n| style=\"background:#AA5500; color: #FFF;\" | '''0110'''\n| '''0110'''\n| 0011\n| 0011\n| …'''10'''\n| …'''0110'''\n|- align=\"center\"\n| '''4'''\n| style=\"background:#F00; color: #FFF;\" | '''0111'''\n| '''0111'''\n| 0100\n| 0100\n| …'''00'''\n| …'''1000'''\n|- align=\"center\"\n| '''5'''\n| style=\"background:#F00; color: #FFF;\" | '''1000'''\n| '''1000'''\n| 0101\n| 0101\n| …'''11'''\n| …'''0111'''\n|- align=\"center\"\n| '''6'''\n| style=\"background:#AA5500; color: #FFF;\" | '''1001'''\n| '''1001'''\n| 0110\n| 0110\n| …'''10'''\n| …'''1001'''\n|- align=\"center\"\n| '''7'''\n| style=\"background:#808000; color: #FFF;\" | '''1010'''\n| '''1010'''\n| 0111\n| 0111\n| …'''10'''\n| …'''1010'''\n|- align=\"center\"\n| '''8'''\n| style=\"background:#00AA00; color: #FFF;\" | '''1011'''\n| '''1011'''\n| 1000\n| 1000\n| …'''00'''\n| …'''0100'''\n|- align=\"center\"\n| '''9'''\n| style=\"background:#008888; color: #FFF;\" | '''1100'''\n| '''1100'''\n| 1001\n| 1001\n| …'''10'''\n| …'''1100'''\n|- align=\"center\"\n| 10\n| style=\"background:#0000FF; color: #FFF;\" | '''1101'''\n| pseudo-tetrade\n| pseudo-tetrade\n| 1010\n| N/A\n| N/A\n|- align=\"center\"\n| 11\n| style=\"background:#8000FF; color: #FFF;\" | '''1110'''\n| pseudo-tetrade\n| pseudo-tetrade\n| 1011\n| N/A\n| N/A\n|- align=\"center\"\n| 12\n| style=\"background:#F000FF; color: #FFF\" | '''1111'''\n| pseudo-tetrade\n| pseudo-tetrade\n| 1100\n| N/A\n| N/A\n|- align=\"center\" style=\"background:white\"\n| 13\n| N/A\n| N/A\n| pseudo-tetrade\n| 1101\n| N/A\n| N/A\n|- align=\"center\" style=\"background:white\"\n| 14\n| N/A\n| N/A\n| pseudo-tetrade\n| 1110\n| N/A\n| N/A\n|- align=\"center\" style=\"background:white\"\n| 15\n| N/A\n| N/A\n| pseudo-tetrade\n| 1111\n| N/A\n| N/A\n|}\n\nTo encode a number such as 127, one simply encodes each of the decimal digits as above, giving (0100, 0101, 1010).\n\nExcess-3 arithmetic uses different [[algorithm]]s than normal non-biased BCD or binary [[positional system]] numbers. After adding two excess-3 digits, the raw sum is excess-6. For instance, after adding 1 (0100 in excess-3) and 2 (0101 in excess-3), the sum looks like 6 (1001 in excess-3) instead of 3 (0110 in excess-3). In order to correct this problem, after adding two digits, it is necessary to remove the extra bias by subtracting binary 0011 (decimal 3 in unbiased binary) if the resulting digit is less than decimal 10, or subtracting binary 1101 (decimal 13 in unbiased binary) if an [[integer overflow|overflow]] (carry) has occurred. (In 4-bit binary, subtracting binary 1101 is equivalent to adding 0011 and vice versa.)\n\n==Motivation==\nThe primary advantage of excess-3 coding over non-biased coding is that a decimal number can be [[method of complements|nines' complement]]ed<ref name=\"Steinbuch_1962\"/> (for subtraction) as easily as a binary number can be [[ones' complement]]ed: just by inverting all bits.<ref name=\"Steinbuch_1962\"/> Also, when the sum of two excess-3 digits is greater than 9, the carry bit of a 4-bit adder will be set high. This works because, after adding two digits, an \"excess\" value of 6 results in the sum. Because a 4-bit integer can only hold values 0 to 15, an excess of 6 means that any sum over 9 will overflow (produce a carry out).\n\nAnother advantage is that the codes 0000 and 1111 are not used for any digit. A fault in a memory or basic transmission line may result in these codes. It is also more difficult to write the zero pattern to magnetic media.<ref name=\"Steinbuch_1962\"/><ref name=\"Bashe_1956\"/><ref name=\"Mietke_2017\"/>\n\n==Example==\n[[BCD 8-4-2-1]] to excess-3 converter example in [[VHDL]]:\n<source lang=\"VHDL\">\nentity bcd8421xs3 is\n  port (\n    a   : in    std_logic;\n    b   : in    std_logic;\n    c   : in    std_logic;\n    d   : in    std_logic;\n\n    an  : buffer std_logic;\n    bn  : buffer std_logic;\n    cn  : buffer std_logic;\n    dn  : buffer std_logic;\n\n    w   : out   std_logic;\n    x   : out   std_logic;\n    y   : out   std_logic;\n    z   : out   std_logic\n  );\nend entity bcd8421xs3;\n\narchitecture dataflow of bcd8421xs3 is\nbegin\n    an  <=  not a;\n    bn  <=  not b;\n    cn  <=  not c;\n    dn  <=  not d;\n\n    w   <=  (an and b  and d ) or (a  and bn and cn)\n         or (an and b  and c  and dn);\n    x   <=  (an and bn and d ) or (an and bn and c  and dn)\n         or (an and b  and cn and dn) or (a  and bn and cn and d);\n    y   <=  (an and cn and dn) or (an and c  and d )\n         or (a  and bn and cn and dn);\n    z   <=  (an and dn) or (a  and bn and cn and dn);\n\nend architecture dataflow; -- of bcd8421xs3\n</source>\n\n==Extensions==\n{{Infobox code\n|name=3-of-6 extension\n|digits=6<ref name=\"Steinbuch_1962\"/>\n|tracks=6<ref name=\"Steinbuch_1962\"/>\n<!-- |digit_values=N/A -->\n|weight=3<ref name=\"Steinbuch_1962\"/>\n|continuity=no<ref name=\"Steinbuch_1962\"/>\n|cyclic=no<ref name=\"Steinbuch_1962\"/>\n|minimum_distance=2<ref name=\"Steinbuch_1962\"/>\n|maximum_distance=6\n<!-- |hamming_distance= -->\n<!-- |redundancy= -->\n|lexicography=1<ref name=\"Steinbuch_1962\"/>\n|complement=(9)<ref name=\"Steinbuch_1962\"/>\n}}\n{{Infobox code\n|name=4-of-8 extension\n|digits=8<ref name=\"Steinbuch_1962\"/>\n|tracks=8<ref name=\"Steinbuch_1962\"/>\n<!-- |digit_values=N/A -->\n|weight=4<ref name=\"Steinbuch_1962\"/>\n|continuity=no<ref name=\"Steinbuch_1962\"/>\n|cyclic=no<ref name=\"Steinbuch_1962\"/>\n|minimum_distance=4<ref name=\"Steinbuch_1962\"/>\n|maximum_distance=8\n<!-- |hamming_distance= -->\n<!-- |redundancy= -->\n|lexicography=1<ref name=\"Steinbuch_1962\"/>\n|complement=9<ref name=\"Steinbuch_1962\"/>\n}}\n* 3-of-6 code extension: The excess-3 code is sometimes also used for data transfer, then often expanded to a 6-bit code per [[CCITT]] GT&nbsp;43&nbsp;No.&nbsp;1, where 3 out of 6 bits are set.<ref name=\"CCITT_1959\"/><ref name=\"Steinbuch_1962\"/>\n* 4-of-8 code extension: As an alternative to the [[IBM]] [[transceiver code]]<ref name=\"IBM_Transceiver\"/> (which is a 4-of-8 code with a [[Hamming distance]] of 2),<ref name=\"Steinbuch_1962\"/> it is also possible to define a 4-of-8 excess-3 code extension achieving a Hamming distance of 4, if only denary digits are to be transferred.<ref name=\"Steinbuch_1962\"/>\n\n==See also==\n* [[Offset binary]], excess-''N'', biased representation\n* [[Excess-128]]\n* [[Excess-Gray code]]\n* [[Shifted Gray code]]\n* [[Gray code]]\n* [[m-of-n code]]\n* [[Aiken code]]\n\n==References==\n{{Reflist|refs=\n<ref name=\"Steinbuch_1962\">{{cite book |title=Taschenbuch der Nachrichtenverarbeitung |language=German |editor-first=Karl W. |editor-last=Steinbuch |editor-link=Karl W. Steinbuch |date=1962 |edition=1 |publisher=[[Springer-Verlag OHG]] |location=Karlsruhe, Germany  |publication-place=Berlin / Göttingen / New York |lccn=62-14511 |pages=71–73, 1081–1082}}</ref>\n<ref name=\"Steinbuch-Weber_1974\">{{cite book |title=Taschenbuch der Informatik – Band II – Struktur und Programmierung von EDV-Systemen |language=German |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Wolfgang |editor-last2=Weber |editor-first3=Traute |editor-last3=Heinemann |date=1974 |orig-year=1967 |edition=3 |volume=2 |work=Taschenbuch der Nachrichtenverarbeitung |publisher=[[Springer Verlag]] |location=Berlin, Germany |isbn=3-540-06241-6 |lccn=73-80607 |pages=98–100}}</ref>\n<ref name=\"Schmid_1974\">{{cite book |title=Decimal Computation |author-first=Hermann |author-last=Schmid<!-- General Electric Company, Binghamton, New York, USA --> |author-link=Hermann Schmid (computer scientist) |date=1974 |edition=1 |publisher=[[John Wiley & Sons, Inc.]] |location=Binghamton, New York, USA |isbn=0-471-76180-X |page=11 |url=https://books.google.com/books/about/Decimal_computation.html?id=UuAmAAAAMAAJ |access-date=2016-01-03}}</ref>\n<ref name=\"Schmid_1983\">{{cite book |title=Decimal Computation |author-first=Hermann |author-last=Schmid<!-- General Electric Company, Binghamton, New York, USA --> |author-link=Hermann Schmid (computer scientist) |orig-year=1974 |date=1983 |edition=1 (reprint) |publisher=Robert E. Krieger Publishing Company |location=Malabar, Florida, USA |isbn=0-89874-318-4 |page=11 |url=https://books.google.com/books/about/Decimal_computation.html?id=uEYZAQAAIAAJ |access-date=2016-01-03}} (NB. At least some batches of this reprint edition were [[misprint]]s with defective pages 115–146.<!-- they contain the contents of another book -->)</ref>\n<ref name=\"Richards_1955\">{{cite book |author-first=Richard Kohler |author-last=Richards |title=Arithmetic Operations in Digital Computers |publisher=[[van Nostrand (publisher)|van Nostrand]] |location=New York, USA |date=1955 |page=182}}</ref>\n<ref name=\"CCITT_1959\">{{cite book |author=[[Comité Consultatif International Téléphonique et Télégraphique]] (CCITT), Groupe de Travail 43 |title=Contribution No. 1 |date=1959-06-03 |id=CCITT,&nbsp;GT&nbsp;43&nbsp;No.&nbsp;1}}</ref>\n<ref name=\"Bashe_1956\">{{cite journal |author-first1=C. J. |author-last1=Bashe |author-first2=P. W. |author-last2=Jackson |author-first3=H. A. |author-last3=Mussell |author-first4=W. D. |author-last4=Winger |title=The Design of the IBM Type 702 System |journal=[[Transactions of the American Institute of Electrical Engineers]] (AIEE), Part 1: Communication and Electronics |volume=74 |id=Paper No. 55-719 |issue=6 |date=January 1956 |pages=695–704 |url=http://ieeexplore.ieee.org/document/6372444/?reload=true |doi=10.1109/TCE.1956.6372444}}</ref>\n<ref name=\"IBM_Transceiver\">{{cite book |author=[[IBM]] |title=65 Data Transceiver / 66 Printing Data Receiver |date=July 1957}}</ref>\n<ref name=\"Mietke_2017\">{{cite web |title=Binäre Codices |at=Exzeß-3-Code mit Additions- und Subtraktionsverfahren |work=Informations- und Kommunikationstechnik |language=German |author-first=Detlef |author-last=Mietke |location=Berlin, Germany |date=2017 |orig-year=2015 |url=http://elektroniktutor.de/digitaltechnik/codices.html |access-date=2017-04-25 |dead-url=no |archive-url=https://web.archive.org/web/20170425181921/http://elektroniktutor.de/digitaltechnik/codices.html |archive-date=2017-04-25}}</ref>\n<ref name=\"Ritchie_1986\">{{cite book |author-last=Ritchie |author-first=David |date=1986 |title=The Computer Pioneers |page=35 |location=New York, USA |publisher=[[Simon and Schuster]] |isbn=067152397X}}</ref>\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-6",
      "url": "https://en.wikipedia.org/wiki/Excess-6",
      "text": "#redirect [[Offset binary#Excess-6]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-11",
      "url": "https://en.wikipedia.org/wiki/Excess-11",
      "text": "#redirect [[Offset binary#Excess-11]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-15",
      "url": "https://en.wikipedia.org/wiki/Excess-15",
      "text": "#REDIRECT [[Offset binary#Excess-15]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-25",
      "url": "https://en.wikipedia.org/wiki/Excess-25",
      "text": "#redirect [[Offset binary#Excess-25]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-32",
      "url": "https://en.wikipedia.org/wiki/Excess-32",
      "text": "#REDIRECT [[Offset binary#Excess-32]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-64",
      "url": "https://en.wikipedia.org/wiki/Excess-64",
      "text": "#REDIRECT [[Offset binary#Excess-64]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-123",
      "url": "https://en.wikipedia.org/wiki/Excess-123",
      "text": "#redirect [[Offset binary#Excess-123]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-127",
      "url": "https://en.wikipedia.org/wiki/Excess-127",
      "text": "#REDIRECT [[Offset binary#Excess-127]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-128",
      "url": "https://en.wikipedia.org/wiki/Excess-128",
      "text": "#REDIRECT [[Signed number representations#Excess-128]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-129",
      "url": "https://en.wikipedia.org/wiki/Excess-129",
      "text": "#REDIRECT [[Offset binary#Excess-129]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-250",
      "url": "https://en.wikipedia.org/wiki/Excess-250",
      "text": "#redirect [[Offset binary#Excess-250]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-256",
      "url": "https://en.wikipedia.org/wiki/Excess-256",
      "text": "#REDIRECT [[Offset binary#Excess-256]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-500",
      "url": "https://en.wikipedia.org/wiki/Excess-500",
      "text": "#redirect [[Offset binary#Excess-500]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-512",
      "url": "https://en.wikipedia.org/wiki/Excess-512",
      "text": "#redirect [[Offset binary#Excess-512]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-976",
      "url": "https://en.wikipedia.org/wiki/Excess-976",
      "text": "#REDIRECT [[Offset binary#Excess-976]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-1023",
      "url": "https://en.wikipedia.org/wiki/Excess-1023",
      "text": "#REDIRECT [[Offset binary#Excess-1023]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-1024",
      "url": "https://en.wikipedia.org/wiki/Excess-1024",
      "text": "#redirect [[Offset binary#Excess-1024]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-2048",
      "url": "https://en.wikipedia.org/wiki/Excess-2048",
      "text": "#REDIRECT [[Offset binary#Excess-2048]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-16383",
      "url": "https://en.wikipedia.org/wiki/Excess-16383",
      "text": "#REDIRECT [[Offset binary#Excess-16383]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-16384",
      "url": "https://en.wikipedia.org/wiki/Excess-16384",
      "text": "#REDIRECT [[Offset binary#Excess-16384]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    },
    {
      "title": "Excess-weighted code",
      "url": "https://en.wikipedia.org/wiki/Excess-weighted_code",
      "text": "#redirect [[Offset binary#Excess-weighted code]] {{R to related topic}}\n<!-- term \"Excess-weighted code\" is used in \"Decimal computation\" by Hermann Schmid, 1974 -->\n\n[[Category:Numeral systems]]\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Find first set",
      "url": "https://en.wikipedia.org/wiki/Find_first_set",
      "text": "In [[software]], '''find first set''' ('''ffs''') or '''find first one''' is a [[bit operation]] that, given an unsigned [[Word (computer architecture)|machine word]], identifies the least significant index or position of the bit set to one in the word. A nearly equivalent operation is '''count trailing zeros''' ('''ctz''') or '''number of trailing zeros''' ('''ntz'''), which counts the number of zero bits following the least significant one bit. The complementary operation that finds the index or position of the most significant set bit is ''log base 2'', so called because it computes the [[binary logarithm]] <math>\\lfloor \\log_2 x\\rfloor</math>.<ref>Anderson, [http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious Find the log base 2 of an integer with the MSB N set in O(N) operations (the obvious way)]</ref> This is [[#Properties and relations|closely related]] to '''count leading zeros''' ('''clz''') or '''number of leading zeros''' ('''nlz'''), which counts the number of zero bits preceding the most significant one bit. These four operations also have negated versions:\n* '''find first zero''' ('''ffz'''), which identifies the index of the least significant zero bit;\n* '''count trailing ones''', which counts the number of one bits following the least significant zero bit.\n* '''count leading ones''', which counts the number of one bits preceding the most significant zero bit;\n* The operation that finds the index of the most significant zero bit, which is a rounded version of the [[binary logarithm]].\nThere are two common variants of find first set, the POSIX definition which starts indexing of bits at 1,<ref name=\"ffsmanpage\">{{cite web|title=FFS(3)|url=https://www.kernel.org/doc/man-pages/online/pages/man3/ffs.3.html|work=Linux Programmer's Manual|publisher=The Linux Kernel Archives|accessdate=2 January 2012}}</ref> herein labelled ffs, and the variant which starts indexing of bits at zero, which is equivalent to ctz and so will be called by that name.\n\n== Examples ==\nGiven the following 32-bit word:\n 00000000000000001000000000001000\nThe count trailing zeros operation would return 3, while the count leading zeros operation returns 16. The count leading zeros operation depends on the word size: if this 32-bit word were truncated to a 16-bit word, count leading zeros would return zero. The find first set operation would return 4, indicating the 4th position from the right. The log base 2 is 15.\n\nSimilarly, given the following 32-bit word, the bitwise negation of the above word:\n 11111111111111110111111111110111\nThe count trailing ones operation would return 3, the count leading ones operation would return 16, and the find first zero operation ffz would return 4.\n\nIf the word is zero (no bits set), count leading zeros and count trailing zeros both return the number of bits in the word, while ffs returns zero. Both log base 2 and zero-based implementations of find first set generally return an undefined result for the zero word.\n\n== Hardware support ==\nMany architectures include [[instruction set|instructions]] to rapidly perform find first set and/or related operations, listed below. The most common operation is count leading zeros (clz), likely because all other operations can be implemented efficiently in terms of it (see [[#Properties and relations|Properties and relations]]).\n\n{|class=\"wikitable\"\n! Platform !! Mnemonic !! Name !! Operand widths !! Description !! Result of operating on 0\n|-\n| [[ARM architecture|ARM]] ([[List of ARM microarchitectures|ARMv5T architecture and later]])<br>except [[ARM Cortex-M|Cortex-M0/M0+/M1/M23]] || clz<ref>{{cite web|title=ARM Instruction Reference > ARM general data processing instructions > CLZ|url=http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0068b/CIHJGJED.html|work=ARM Developer Suite Assembler Guide|publisher=ARM|accessdate=3 January 2012}}</ref> || Count Leading Zeros || 32 || clz || 32\n|-\n| [[ARM architecture|ARM]] ([[List of ARM microarchitectures|ARMv8-A architecture]]) || clz || Count Leading Zeros || 32, 64 || clz || Operand width\n|-\n| [[AVR32]] || clz<ref>{{cite web | url=http://www.atmel.com/dyn/resources/prod_documents/doc32000.pdf | title=AVR32 Architecture Document | publisher=[[Atmel]] | accessdate=2016-10-22}}</ref> || Count Leading Zeros || 32 || clz || 32\n|-\n| rowspan=2 | [[DEC Alpha]]\n| ctlz<ref name=alpha>{{cite book|title=Alpha Architecture Reference Manual|year=2002|publisher=Compaq|pages=4–32, 4–34|url=http://download.majix.org/dec/alpha_arch_ref.pdf}}</ref> || Count Leading Zeros || 64 || clz || 64\n|-\n| cttz<ref name=alpha/> || Count Trailing Zeros || 64 || ctz || 64\n|-\n| rowspan=2 | [[Intel 80386]] and later\n| bsf<ref name=\"intel dev manual\">{{cite book|title=Intel 64 and IA-32 Architectures Software Developer Manual|publisher=Intel|location=Volume 2A|pages=3–92&ndash;3–97|url=http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html}} Order number 325383.</ref> || Bit Scan Forward || 16, 32, 64 || ctz || Undefined; sets zero flag \n|-\n| bsr<ref name=\"intel dev manual\"/> || Bit Scan Reverse || 16, 32, 64 || Log base 2 || Undefined; sets zero flag\n|-\n| [[x86]] supporting [[Bit Manipulation Instruction Sets#ABM|ABM]] || lzcnt<ref>{{cite book|title=AMD64 Architecture Programmer's Manual Volume 3: General Purpose and System Instructions3|year=2011|publisher=AMD|pages=204&ndash;5|url=http://support.amd.com/us/Processor_TechDocs/APM_V3_24594.pdf}}</ref> || Count Leading Zeros || 16, 32, 64 || clz || Operand width; sets carry flag\n|-\n| [[x86]] supporting [[Bit Manipulation Instruction Sets#BMI1|BMI1]] || tzcnt<ref>{{cite web|url=http://support.amd.com/TechDocs/24594.pdf|title=AMD64 Architecture Programmer's Manual, Volume 3: General-Purpose and System Instructions|date=October 2013 |accessdate=2014-01-02 |publisher=[[AMD]] |work=amd.com}}</ref>|| Count Trailing Zeros || 16, 32, 64 || ctz || Operand width; sets carry flag\n|-\n| [[Itanium]] || clz<ref>{{cite book|title=Intel Itanium Architecture Software Developer's Manual. Volume 3: Intel Itanium Instruction Set|year=2010|publisher=Intel|pages=3:38|url=http://www.intel.com/content/www/us/en/processors/itanium/itanium-architecture-vol-3-manual.html}}</ref> || Count Leading Zeros || 64 || clz || 64\n|-\n| rowspan=2 | [[MIPS architecture|MIPS]]\n| clz<ref name=\"mips32\">{{cite book|title=MIPS Architecture For Programmers. Volume II-A: The MIPS32 Instruction Set|year=2011|publisher=MIPS Technologies|edition=Revision 3.02|pages=101–102|url=http://www.mips.com/products/architectures/mips32/}}</ref><ref name=\"mips64\">{{cite book|title=MIPS Architecture For Programmers. Volume II-A: The MIPS64 Instruction Set|year=2011|publisher=MIPS Technologies|edition=Revision 3.02|pages=105, 107, 122, 123|url=http://www.mips.com/products/architectures/mips64/}}</ref>  || Count Leading Zeros in Word || 32, 64 || clz || Operand width\n|-\n| clo<ref name=\"mips32\"/><ref name=\"mips64\"/> || Count Leading Ones in Word || 32, 64 || clo || Operand width\n|-\n| [[Motorola 68020]] and later || bfffo<ref>{{cite book|title=M68000 Family Programmer's Reference Manual|year=1992|publisher=Motorola|pages=4–43&ndash;4–45|url=http://www.freescale.com/files/archives/doc/ref_manual/M68000PRM.pdf}}</ref> || Find First One in Bit Field || Arbitrary || Log base 2 || Field offset + field width\n|-\n| [[PDP-10]] || jffo || Jump if Find First One  || 36 || ctz || 0; no operation\n|-\n| [[IBM POWER Instruction Set Architecture|POWER]]/[[PowerPC]]/[[Power ISA]] || cntlz/cntlzw/cntlzd<ref>{{cite book|last=Frey|first=Brad|title=PowerPC Architecture Book|publisher=IBM|location=3.3.11 Fixed-Point Logical Instructions |pages=70|edition=Version 2.02|url=http://www.ibm.com/developerworks/systems/library/es-archguide-v2.html}}</ref> || Count Leading Zeros || 32, 64 || clz || Operand width\n|-\n| [[Power ISA#Power ISA v.3.0|Power ISA 3.0 and later]] || cnttzw/cnttzd<ref>{{cite book|title=Power ISA Version 3.0B|publisher=IBM|location=3.3.13 Fixed-Point Logical Instructions, 3.3.13.1 64-bit Fixed-Point Logical Instructions |pages=95, 98|url=https://openpowerfoundation.org/?resource_lib=power-isa-version-3-0}}</ref> || Count Trailing Zeros || 32, 64 || ctz || Operand width\n|-\n| rowspan=2 | [[RISC-V]] (\"B\" Extension) (draft)\n| clz<ref name=\"RISC-V-B\">{{cite web |url=https://github.com/riscv/riscv-bitmanip/blob/master/bitmanip-draft.pdf |title=RISC-V \"B\" Bit Manipulation Extension for RISC-V, Draft v0.37 |date=March 22, 2019 |last=Wolf |first=Clifford |website=Github |publisher=Clifford Wolf}}</ref> || Count Leading Zeros || TBD || clz || Operand width\n|-\n| ctz<ref name=\"RISC-V-B\"/> || Count Trailing Zeros || TBD || ctz || Operand width\n|-\n| [[SPARC|SPARC Oracle Architecture 2011 and later]] || lzcnt (synonym: lzd) <ref>{{cite book|title=Oracle SPARC Architecture 2011|publisher=Oracle|url=http://www.oracle.com/technetwork/server-storage/sun-sparc-enterprise/documentation/index.html}}</ref> || Leading Zero Count || 64 || clz || 64\n|-\n| [[VAX]] || ffs<ref>{{cite book|url=http://www.bitsavers.org/pdf/dec/vax/archSpec/EY-3459E-DP_VAX_Architecture_Reference_Manual_1987.pdf|title=VAX Architecture Reference Manual|year=1987|publisher=DEC|pages=70–71}}</ref> || Find First Set  || 0–32 || ctz || Operand width; sets zero flag\n|-\n| rowspan=2 | [[z/Architecture]]\n| vclz<ref>{{cite book|url=http://publibfp.dhe.ibm.com/epubs/pdf/dz9zr010.pdf|title=IBM z/Architecture Principles of Operation|date=March 2015|edition=Eleventh|publisher=IBM|location=Chapter 22. Vector Integer Instructions|at=p. 22-10}}</ref> || Vector Count Leading Zeroes  || 8, 16, 32, 64 || clz || Operand width\n|-\n| vctz<ref>{{cite book|url=http://publibfp.dhe.ibm.com/epubs/pdf/dz9zr010.pdf|title=IBM z/Architecture Principles of Operation|date=March 2015|edition=Eleventh|publisher=IBM|location=Chapter 22. Vector Integer Instructions|at=p. 22-10}}</ref> || Vector Count Trailing Zeroes  || 8, 16, 32, 64 || ctz || Operand width\n|}\n\nOn some Alpha platforms CTLZ and CTTZ are emulated in software.\n\n== Tool and library support ==\nA number of compiler and library vendors supply compiler intrinsics or library functions to perform find first set and/or related operations, which are frequently implemented in terms of the hardware instructions above:\n{|class=\"wikitable\"\n! Tool/library !! Name !! Type !! Input type(s) !! Notes !! Result for zero input\n|-\n| [[POSIX]].1 compliant libc<br/>[[4.3BSD]] libc<br/>OS X 10.3 libc<ref name=\"ffsmanpage\"/><ref>{{cite web|title=FFS(3)|url=https://developer.apple.com/library/mac/#documentation/darwin/reference/manpages/10.3/man3/ffs.3.html|work=Mac OS X Developer Library|publisher=Apple, Inc.|date=1994-04-19|accessdate=4 January 2012}}</ref> || <code>ffs</code> || Library function || int || Includes [[glibc]].<br/>POSIX does not supply the complementary log base 2 / clz. || 0\n|-\n| [[FreeBSD]] 5.3 libc<br/>OS X 10.4 libc<ref>{{cite web|title=FFS(3)|url=https://developer.apple.com/library/mac/#documentation/darwin/reference/manpages/man3/ffs.3.html|work=Mac OS X Developer Library|publisher=Apple|date=2004-01-13|accessdate=4 January 2012}}</ref> || <code>ffsl</code><br/><code>fls</code><br/><code>flsl</code> || Library function || int,<br/>long || fls (\"find last set\") computes (log base 2) + 1. || 0\n|-\n| [[FreeBSD]] 7.1 libc<ref>{{cite web|title=FFS(3)|url=http://www.freebsd.org/cgi/man.cgi?query=ffs&apropos=0&sektion=0&manpath=FreeBSD+8.2-RELEASE&arch=default&format=html|work=FreeBSD Library Functions Manual|publisher=The FreeBSD Project|accessdate=4 January 2012}}</ref> || <code>ffsll</code><br/><code>flsll</code> || Library function || long long || || 0\n|-\n| [[GNU Compiler Collection|GCC]] || <code>__builtin_ffs[l,ll,imax]</code> || rowspan=\"2\" | Built-in functions || rowspan=\"2\" | unsigned int,<br/>unsigned long,<br/>unsigned long long,<br/>uintmax_t || rowspan=\"2\" | || 0\n|-\n| [[GNU Compiler Collection|GCC]] 3.4.0<ref>{{cite web|title=Other built-in functions provided by GCC|url=https://gcc.gnu.org/onlinedocs/gcc-3.4.0/gcc/Other-Builtins.html|work=Using the GNU Compiler Collection (GCC)|publisher=Free Software Foundation, Inc.|accessdate=14 November 2015}}</ref><ref>{{cite web|title=GCC 3.4.0 ChangeLog|url=https://gcc.gnu.org/git/?p=gcc.git;a=blob;f=gcc/ChangeLog.9;h=8eed245136d1c2a1d198826fc692260ec077e14b;hb=3b3ea0678785edcb024c8fb6c2a870a1260bd407#l17526|work=GCC 3.4.0|publisher=Free Software Foundation, Inc.|accessdate=14 November 2015}}</ref> \n[[Clang]] 5.x <ref>{{cite web|title=Clang Language Extensions, chapter Builtin Functions|url=http://clang.llvm.org/docs/LanguageExtensions.html#builtin-functions|work=Clang supports a number of builtin library functions with the same syntax as GCC|publisher=The Clang Team|accessdate=9 April 2017}}</ref><ref>{{cite web|title=Source code of Clang|url=https://github.com/llvm-mirror/clang/blob/7a351322b4f455f6485649124423f347cb559c3b/include/clang/Basic/Builtins.def#L390|publisher=LLVM Team, University of Illinois at Urbana-Champaign|accessdate=9 April 2017}}</ref>\n|| <code>__builtin_clz[l,ll,imax]</code><br/><code>__builtin_ctz[l,ll,imax]</code><br/>|| Undefined\n|-\n|[[Visual Studio]] 2005 || <code>_BitScanForward</code><ref>{{cite web|title=_BitScanForward, _BitScanForward64|url=http://msdn.microsoft.com/en-us/library/wfd9z0bb(v=VS.90).aspx|work=Visual Studio 2008: Visual C++: Compiler Intrinsics|publisher=Microsoft|accessdate=21 May 2018}}</ref><br/><code>_BitScanReverse</code><ref>{{cite web|title=_BitScanReverse, _BitScanReverse64|url=https://msdn.microsoft.com/en-us/library/fbxyd7zd(v=VS.90).aspx|work=Visual Studio 2008: Visual C++: Compiler Intrinsics|publisher=Microsoft|accessdate=21 May 2018}}</ref> ||  Compiler intrinsics || unsigned long,<br/>unsigned __int64 || Separate return value to indicate zero input || 0\n|-\n|[[Visual Studio]] 2008 || <code>__lzcnt</code><ref>{{cite web|title=__lzcnt16, __lzcnt, __lzcnt64|url=http://msdn.microsoft.com/en-us/library/bb384809(v=VS.90).aspx|work=Visual Studio 2008: Visual C++: Compiler Intrinsics|publisher=Microsoft|accessdate=3 January 2012}}</ref> || Compiler intrinsic || unsigned short,<br/>unsigned int,<br/>unsigned __int64 || Relies on x64-only lzcnt instruction || Input size in bits\n|-\n|[[Intel C++ Compiler]] || <code>_bit_scan_forward</code><br/><code>_bit_scan_reverse</code><ref>{{cite book|title=Intel C++ Compiler for Linux Intrinsics Reference|year=2006|publisher=Intel|pages=21|url=http://software.intel.com/file/6373}}</ref> || Compiler intrinsics || int || || Undefined\n|-\n| rowspan=\"2\" | NVIDIA [[CUDA]]<ref>{{cite book|title=NVIDIA CUDA Programming Guide|year=2010|publisher=NVIDIA|pages=92|edition=Version 3.0|url=http://developer.download.nvidia.com/compute/cuda/3_0/toolkit/docs/NVIDIA_CUDA_ProgrammingGuide.pdf}}</ref> || <code>__clz</code>\n| rowspan=\"2\" | Functions\n| rowspan=\"2\" | 32-bit, 64-bit\n| rowspan=\"2\" | Compiles to fewer instructions on the [[GeForce 400 Series]] || 32\n|-\n| <code>__ffs</code> || 0\n|-\n|[[LLVM]] || <code>llvm.ctlz.*</code><br/><code>llvm.cttz.*</code><ref>{{cite web|title='llvm.ctlz.*' Intrinsic, 'llvm.cttz.*' Intrinsic|url=http://llvm.org/docs/LangRef.html#llvm-ctlz-intrinsic|work=LLVM Language Reference Manual|publisher=The LLVM Compiler Infrastructure|accessdate=23 February 2016}}</ref>  || Intrinsic || 8, 16, 32, 64, 256 || LLVM assembly language || Input size if arg 2<br/>is 0, else undefined\n|-\n|[[Glasgow Haskell Compiler|GHC]] 7.10 (base 4.8), in <code>Data.Bits</code>  || <code>countLeadingZeros</code><br/><code>countTrailingZeros</code> || Library function || <code>FiniteBits b => b</code> || Haskell programming language || Input size in bits\n|}\n\n== Properties and relations ==\nIf bits are labeled starting at 1 (which is the convention used in this article), then count trailing zeros and find first set operations are related by {{math|1=ctz(''x'') = ffs(''x'') − 1}} (except when the input is zero).  If bits are labeled starting at 0, then count trailing zeros and find first set are exactly equivalent operations.\n\nGiven w bits per word, the log base 2 is easily computed from the clz and vice versa by {{math|1=lg(''x'') = ''w'' − 1 − clz(''x'')}}.\n\nAs demonstrated in the example above, the find first zero, count leading ones, and count trailing ones operations can be implemented by negating the input and using find first set, count leading zeros, and count trailing zeros. The reverse is also true.\n\nOn platforms with an efficient log base 2 operation such as M68000, ctz can be computed by:\n:ctz(x) = lg(x & (−x))\nwhere \"&\" denotes bitwise AND and \"−x\" denotes the [[two's complement]] of x. The expression x & (−x) clears all but the least-significant 1 bit, so that the most- and least-significant 1 bit are the same.\n\nOn platforms with an efficient count leading zeros operation such as ARM and PowerPC, ffs can be computed by:\n:ffs(x) = w − clz(x & (−x)).\n\nConversely, clz can be computed using ctz by first rounding up to the nearest power of two using shifts and bitwise ORs,<ref>Anderson, [http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2 Round up to the next highest power of 2].</ref> as in this 32-bit example (which depends on ctz returning 32 for the zero input):\n\n '''function''' clz(x):\n     '''for each''' y '''in''' {1, 2, 4, 8, 16}: x ← x | (x >> y)\n     '''return''' 32 − ctz(x + 1)\n\nOn platforms with an efficient [[Hamming weight]] (population count) operation such as [[SPARC]]'s <code>POPC</code><ref name=\"SPARC_1992\"/><ref name=\"Warren_2013\"/> or [[Blackfin]]'s <code>ONES</code>,<ref name=\"AD_2001\"/> ctz can be computed using the identity:<ref>{{cite web|last=Dietz|first=Henry Gordon|title=The Aggregate Magic Algorithms|url=http://aggregate.org/MAGIC/#Trailing%20Zero%20Count|publisher=University of Kentucky}}</ref><ref>{{cite web|author-first=Gerd|author-last=Isenberg|title=BitScanProtected|url=http://chessprogramming.wikispaces.com/BitScan#Bitscan forward-Index of LS1B by Popcount|work=Chess Programming Wiki|accessdate=3 January 2012}}</ref> \n:ctz(x) = pop((x & (−x)) − 1),\nffs can be computed using:<ref>{{cite book|last=SPARC International, Inc.|title=The SPARC architecture manual : version 8|year=1992|publisher=Prentice Hall|location=Englewood Cliffs, N.J.|isbn=978-0-13-825001-0|pages=231|url=http://www.sparc.org/standards/SPARCV9.pdf|edition=Version 8.|deadurl=yes|archiveurl=https://web.archive.org/web/20120118213535/http://www.sparc.org/standards/SPARCV9.pdf|archivedate=2012-01-18|df=}} A.41: Population Count. Programming Note.</ref>\n:ffs(x) = pop(x ^ (~(−x)))\nwhere \"^\" denotes bitwise exclusive-or, and clz can be computed by:\n '''function''' clz(x):\n     '''for each''' y '''in''' {1, 2, 4, 8, 16}: x ← x | (x >> y)\n     '''return''' 32 − pop(x)\n\nThe inverse problem (given i, produce an x such that ctz(x) = i) can be computed with a left-shift (1 << i).\n\nFind first set and related operations can be extended to arbitrarily large [[bit array]]s in a straightforward manner by starting at one end and proceeding until a word that is not all-zero (for ffs/ctz/clz) or not all-one (for ffz/clo/cto) is encountered. A tree data structure that recursively uses bitmaps to track which words are nonzero can accelerate this.\n\n==Algorithms==\n\n===FFS===\nFind first set can also be implemented in software. A simple loop implementation:\n\n '''function''' ffs (x)\n     '''if''' x = 0 '''return''' 0\n     t ← 1\n     r ← 1\n     '''while''' (x & t) = 0\n         t ← t << 1\n         r ← r + 1\n     '''return''' r\n\nwhere \"<<\" denotes left-shift. Similar loops can be used to implement all the related operations. On modern architectures this loop is inefficient due to a large number of conditional branches. A lookup table can eliminate most of these:\n\n table[0..2<sup>n</sup>-1] = ffs(i) '''for''' i '''in''' 0..2<sup>n</sup>-1\n '''function''' ffs_table (x)\n     '''if''' x = 0 '''return''' 0\n     r ← 0\n     '''loop'''\n         '''if''' (x & (2<sup>n</sup>-1)) ≠ 0\n             '''return''' r + table[x & (2<sup>n</sup>-1)]\n         x ← x >> n\n         r ← r + n\n\nThe parameter ''n'' is fixed (typically 8) and represents a [[time–space tradeoff]]. The loop may also be fully [[loop unrolling|unrolled]].\n\n===CTZ===\nCount Trailing Zeros (ctz) counts the number of zero bits succeeding the least significant one bit.  For example, the ctz of 0x00000F00 is 8, and the ctz of 0x80000000 is 31.\n\nAn algorithm for 32-bit ctz by Leiserson, Prokop, and Randall uses [[de Bruijn sequence]]s to construct a [[minimal perfect hash function]] that eliminates all branches.<ref>{{Citation\n  | last1 = Leiserson  | first1 = Charles E.\n  | author1-link = Charles E. Leiserson\n  | last2 = Prokop | first2 = Harald\n  | author2-link = Harald Prokop\n  | last3 = Randall | first3 = Keith H.\n  | title = Using de Bruijn Sequences to Index a 1 in a Computer Word\n  | year = 1998\n  | url = http://supertech.csail.mit.edu/papers/debruijn.pdf\n}}</ref>\n<ref>{{Citation\n  | last1 = Busch  | first1 = Philip\n  | author1-link = Philip Busch\n  | title = Computing trailing Zeros HOWTO\n  | year = 2009\n  | url = http://7ooo.mooo.com/text/ComputingTrailingZerosHOWTO.pdf\n}}</ref> \nThis algorithm assumes that the result of the multiplication is truncated to 32 bit.\n\n table[0..31] initialized by: '''for''' i '''from''' 0 '''to''' 31: table[ ( 0x077CB531 * ( 1 << i ) ) >> 27 ] ← i\n '''function''' ctz_debruijn (x)\n     '''return''' table[((x & (-x)) * 0x077CB531) >> 27]\n\nThe expression (x & (-x)) again isolates the least-significant 1 bit. There are then only 32 possible words, which the unsigned multiplication and shift hash to the correct position in the table. (This algorithm does not handle the zero input.) A similar algorithm works for log base 2, but rather than isolate the most-significant bit, it rounds up to the nearest integer of the form 2<sup>''n''</sup>&minus;1 using shifts and bitwise ORs:<ref>Anderson, [http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogDeBruijn Find the log base 2 of an N-bit integer in O(lg(N)) operations with multiply and lookup]</ref>\n table[0..31] = {0, 9, 1, 10, 13, 21, 2, 29, 11, 14, 16, 18, 22, 25, 3, 30,\n                 8, 12, 20, 28, 15, 17, 24, 7, 19, 27, 23, 6, 26, 5, 4, 31}\n '''function''' lg_debruijn (x)\n     '''for each''' y '''in''' {1, 2, 4, 8, 16}: x ← x | (x >> y)\n     '''return''' table[(x * 0x07C4ACDD) >> 27]\n\nA [[binary search]] implementation which takes a logarithmic number of operations and branches, as in these 32-bit versions:<ref name=\"hackersdelight-clz\">Warren, Section 5-3: Counting Leading 0's.</ref><ref name=\"hackersdelight-ctz\">Warren, Section 5-4: Counting Trailing 0's.</ref>  This algorithm can be assisted by a table as well, replacing the bottom three \"if\" statements with a 256 entry lookup table using the final byte as an index.\n\n '''function''' ctz (x)\n     '''if''' x = 0 '''return''' 32\n     n ← 0\n     '''if''' (x & 0x0000FFFF) = 0: n ← n + 16, x ← x >> 16\n     '''if''' (x & 0x000000FF) = 0: n ← n +  8, x ← x >>  8\n     '''if''' (x & 0x0000000F) = 0: n ← n +  4, x ← x >>  4\n     '''if''' (x & 0x00000003) = 0: n ← n +  2, x ← x >>  2\n     '''if''' (x & 0x00000001) = 0: n ← n +  1\n     '''return''' n\n\n===CLZ===\nCount Leading Zeros (clz) counts the number of zero bits preceding the most significant one bit.  For example, a 32-bit clz of 0x00000F00 is 20, and the clz of 0x00000001 is 31.\n\nJust as count leading zeros is useful for software [[floating point]] implementations, conversely, on platforms that provide hardware conversion of integers to floating point, the exponent field can be extracted and subtracted from a constant to compute the count of leading zeros. Corrections are needed to account for rounding errors.<ref name=\"hackersdelight-clz\"/><ref>Anderson, [http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogIEEE64Float Find the integer log base 2 of an integer with a 64-bit IEEE float].</ref>\n\nThe non-optimized approach examines one bit at a time until a non-zero bit is found, as shown in this [[C (programming language)|C language]] example, and slowest with an input value of 1 because of the many loops it has to perform to find it.\n\n<source lang=\"c\">\n#include <stdint.h> /* This header exists in C99 compatible C/C++ compilers. It defines uint32_t, uint16_t, uint8_t in the following examples */\nunsigned clz32a( uint32_t x ) /* 32-bit clz */\n{\n  unsigned n;\n  if (x == 0)\n     n = 32;\n  else \n     for (n = 0; ((x & 0x80000000) == 0); n++, x <<= 1);\n  return n;\n}\nunsigned clz16a( uint16_t x ) /* 16-bit clz */\n{\n  unsigned n;\n  if (x == 0)\n     n = 16;\n  else \n     for (n = 0; ((x & 0x8000) == 0); n++, x <<= 1);\n  return n;\n}\nunsigned clz8a( uint8_t x ) /* 8-bit clz */\n{\n  unsigned n;\n  if (x == 0)\n     n = 8;\n  else \n     for (n = 0; ((x & 0x80) == 0); n++, x <<= 1);\n  return n;\n}\n</source><!-- NOTE TO OTHER EDITORS: clz16a() is required to fill clz_table_16bit[] further below, so please don't delete it -->\n\nAn evolution of the previous looping approach examines four bits at a time then using a lookup table for the final four bits, which is shown here with a 16 (2<sup>4</sup>) entry lookup table.  A faster looping approach would examine eight bits at a time with a 256 (2<sup>8</sup>) entry table.\n\n<source lang=\"c\">\n#include <stdint.h>\nstatic const uint8_t clz_table_4bit[16] = { 4, 3, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0 };\nunsigned clz32b( uint32_t x ) /* 32-bit clz */\n{\n  unsigned n;\n  if (x == 0)\n    n = 32;\n  else\n  {\n    for (n = 0; ((x & 0xF0000000) == 0); n += 4, x <<= 4);\n    n += (unsigned)clz_table_4bit[x >> (32-4)];\n  }\n  return n;\n}\n</source>\n\nFaster than the looping method is a [[binary search]] implementation which takes a logarithmic number of operations and branches, as in these 32-bit versions:<ref name=\"hackersdelight-clz\"/><ref name=\"hackersdelight-ctz\"/>  The initial check for zero was removed because this method doesn't have a loop.\n\n<source lang=\"c\">\n#include <stdint.h>\nunsigned clz32c( uint32_t x ) /* 32-bit clz */\n{\n  unsigned n = 0;\n  if ((x & 0xFFFF0000) == 0) {n  = 16; x <<= 16;}\n  if ((x & 0xFF000000) == 0) {n +=  8; x <<=  8;}\n  if ((x & 0xF0000000) == 0) {n +=  4; x <<=  4;}\n  if ((x & 0xC0000000) == 0) {n +=  2; x <<=  2;}\n  if ((x & 0x80000000) == 0) {n +=  1;}\n  return n;\n}\n</source>\n\nThe binary search algorithm can be assisted by a table as well, replacing the bottom two \"if\" statements with a 16 (2<sup>4</sup>) entry lookup table using the final 4-bits as an index, which is shown here.  An alternate approach replaces the bottom three \"if\" statements with a 256 (2<sup>8</sup>) entry table using the final 8 bits as an index.\n\n<source lang=\"c\">\n#include <stdint.h>\nstatic const uint8_t clz_table_4bit[16] = { 4, 3, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0 };\nunsigned clz32d( uint32_t x ) /* 32-bit clz */\n{\n  unsigned n = 0;\n  if ((x & 0xFFFF0000) == 0) {n  = 16; x <<= 16;}\n  if ((x & 0xFF000000) == 0) {n +=  8; x <<=  8;}\n  if ((x & 0xF0000000) == 0) {n +=  4; x <<=  4;}\n  n += (unsigned)clz_table_4bit[x >> (32-4)];\n  return n;\n}\n</source>\n\nAs with many algorithms, the fastest approach to simulate clz would be a precomputed lookup table.  In two previous examples, <code>clz_table_4bit[16]</code> was a lookup table for all 16 (2<sup>4</sup>) values of a 4-bit clz operation.  For common byte-width increments, an 8-bit clz requires 256 (2<sup>8</sup>) values, a 16-bit clz requires 64K (2<sup>16</sup>) values, a 32-bit clz requires 4G (2<sup>32</sup>) values.  Though a 32-bit clz table is possible on some computers, wasting 4GB of memory for a table is not practical.  The next step down requires 64KB, which may be a reasonable amount for some computers.\n\n<source lang=\"c\">\n#include <stdint.h>\nstatic uint8_t clz_table_16bit[65536]; /* This table MUST be computed, using clz16a() above, before calling the following functions */\nunsigned clz32e( uint32_t x ) /* 32-bit clz */\n{\n  if ((x & 0xFFFF0000) == 0)\n    return (unsigned)clz_table_16bit[x] + 16;\n  else\n    return (unsigned)clz_table_16bit[x >> 16];\n}\nunsigned clz16e( uint16_t x ) /* 16-bit clz */\n{\n  return (unsigned)clz_table_16bit[x];\n}\nunsigned clz8e( uint8_t x ) /* 8-bit clz */\n{\n  return (unsigned)clz_table_16bit[x] - 8;\n}\n</source>\n\n==Applications==\nThe count leading zeros (clz) operation can be used to efficiently implement ''normalization'', which encodes an integer as ''m''&nbsp;&times;&nbsp;2<sup>''e''</sup>, where ''m'' has its most significant bit in a known position (such as the highest position). This can in turn be used to implement [[Newton-Raphson division]], perform integer to [[floating point]] conversion in software, and other applications.<ref name=\"hackersdelight-clz\"/><ref>{{cite book|last=Sloss|first=Andrew N.|last2=Symes|first2=Dominic|last3=Wright|first3=Chris|title=ARM system developer's guide designing and optimizing system software|year=2004|publisher=[[Morgan Kaufmann]]|location=San Francisco, CA|isbn=978-1-55860-874-0|pages=212–213|edition=1st}}</ref>\n\nCount leading zeros (clz) can be used to compute the 32-bit predicate \"x = y\" (zero if true, one if false) via the identity clz(x − y) >> 5, where \">>\" is unsigned right shift.<ref>Warren, Section 2-11: Comparison Predicates</ref> It can be used to perform more sophisticated bit operations like finding the first string of ''n'' 1 bits.<ref>Warren, Section 6-2. Find First String of 1-Bits of a Given Length.</ref> The expression 1 << (16&nbsp;−&nbsp;clz(x&nbsp;−&nbsp;1)/2) is an effective initial guess for computing the square root of a 32-bit integer using [[Newton's method]].<ref>Warren, 11-1: Integer Square Root.</ref> CLZ can efficiently implement ''null suppression'', a fast [[data compression]] technique that encodes an integer as the number of leading zero bytes together with the nonzero bytes.<ref>{{cite book|last=Schlegel|first=Benjamin|author2=Rainer Gemulla |author3=Wolfgang Lehner |title=Fast integer compression using SIMD instructions|journal=Proceedings of the Sixth International Workshop on Data Management on New Hardware (DaMoN 2010)|date=June 2010|pages=34–40|doi=10.1145/1869389.1869394|isbn=9781450301893|citeseerx=10.1.1.230.6379}}<!--|accessdate=4 January 2012--></ref> It can also efficiently generate [[exponential distribution|exponentially distributed]] integers by taking the clz of [[uniform distribution (discrete)|uniformly random]] integers.<ref name=\"hackersdelight-clz\"/>\n\nThe log base 2 can be used to anticipate whether a multiplication will overflow, since <math>\\lceil\\log_2 xy\\rceil \\leq \\lceil\\log_2 x\\rceil + \\lceil\\log_2 y\\rceil</math>.<ref>Warren, Section 2-12. Overflow Detection.</ref>\n\nCount leading zeros and count trailing zeros can be used together to implement [[Gosper's loop-detection algorithm]],<ref>{{cite journal|last=Gosper|first=Bill|title=Loop detector|journal=HAKMEM|year=1972|issue=239|pages=Item 132|url=http://www.inwap.com/pdp10/hbaker/hakmem/flows.html#item132}}</ref> which can find the period of a function of finite range using limited resources.<ref name=\"hackersdelight-ctz\"/>\n\nThe [[binary GCD algorithm]] spends many cycles removing trailing zeros; this can be replaced by a count trailing zeros (ctz) followed by a shift. A similar loop appears in computations of the [[hailstone sequence]].\n\nA [[bit array]] can be used to implement a [[priority queue]]. In this context, find first set (ffs) is useful in implementing the \"pop\" or \"pull highest priority element\" operation efficiently. The [[Linux kernel]] real-time scheduler internally uses <code>sched_find_first_bit()</code> for this purpose.<ref>{{cite book|last=Aas|first=Josh|title=Understanding the Linux 2.6.8.1 CPU Scheduler|year=2005|publisher=Silicon Graphics, Inc.|pages=19|url=http://www.inf.ed.ac.uk/teaching/courses/os/prac/lcpusched-fullpage-2x1.pdf}}</ref>\n\nThe count trailing zeros operation gives a simple optimal solution to the [[Tower of Hanoi]] problem: the disks are numbered from zero, and at move ''k'', disk number ctz(''k'') is moved the minimum possible distance to the right (circling back around to the left as needed). It can also generate a [[Gray code]] by taking an arbitrary word and flipping bit ctz(''k'') at step ''k''.<ref name=\"hackersdelight-ctz\"/>\n\n==See also==\n* [[Bit Manipulation Instruction Sets]] for Intel and AMD x86-based processors\n\n==References==\n{{Reflist|40em|refs=\n<ref name=\"SPARC_1992\">{{cite book |author=SPARC International, Inc. |title=The SPARC architecture manual: version 8 |date=1992 |publisher=[[Prentice Hall]] |location=Englewood Cliffs, New Jersey, USA |isbn=978-0-13-825001-0 |pages=231 |chapter-url=http://www.sparc.org/standards/SPARCV9.pdf |edition=Version 8 |chapter=A.41: Population Count. Programming Note |deadurl=yes |archiveurl=https://web.archive.org/web/20120118213535/http://www.sparc.org/standards/SPARCV9.pdf |archivedate=2012-01-18 |df= }}</ref>\n<ref name=\"Warren_2013\">{{Cite book |title=Hacker's Delight |title-link=Hacker's Delight |author-first=Henry S. |author-last=Warren, Jr. |date=2013 |orig-year=2002 |edition=2 |publisher=[[Addison Wesley]] - [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8 |id=0-321-84268-5}}</ref>\n<ref name=\"AD_2001\">{{cite book |title=Blackfin Instruction Set Reference |year=2001 |publisher=[[Analog Devices]] |edition=Preliminary |pages=8–24 |id=Part Number 82-000410-14}}</ref>\n}}\n\n==Further reading==\n* {{Cite book |title=Hacker's Delight |first=Henry S. |last=Warren Jr. |date=2013 |edition=2 |publisher=[[Addison Wesley]] - [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8|title-link=Hacker's Delight }}\n* {{cite web|last=Anderson|first=Sean Eron|title=Bit Twiddling Hacks|url=http://graphics.stanford.edu/~seander/bithacks.html|work=Sean Eron Anderson student homepage|publisher=Stanford University|accessdate=3 January 2012}} (NB. Lists several efficient public domain C implementations for [http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightLinear count trailing zeros] and [http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious log base 2].)\n\n==External links==\n* [https://software.intel.com/sites/landingpage/IntrinsicsGuide/ Intel Intrinsics Guide]\n* [http://chessprogramming.wikispaces.com/BitScan Chess Programming Wiki: BitScan]: A detailed explanation of a number of implementation methods for ffs (called LS1B) and log base 2 (called MS1B).\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]"
    },
    {
      "title": "Finger binary",
      "url": "https://en.wikipedia.org/wiki/Finger_binary",
      "text": "{{No footnotes|date=January 2009}}\n[[File:I love you in Sign Language or the number 19 in Finger Binary.jpg|thumb|19 in finger binary: the pinkie finger is 16, added to the 2 of the index finger and the 1 of the thumb]]\n'''Finger binary''' is a system for [[Finger counting|counting]] and displaying [[Binary numeral system|binary numbers]] on the [[finger]]s of one or more [[hand]]s. It is possible to count from 0 to 31 (2<sup>5</sup> &minus; 1) using the fingers of a single hand, from 0 through 1023 (2<sup>10</sup> &minus; 1) if both hands are used, or from 0 to 1,048,575 (2<sup>20</sup> &minus; 1) if the toes on both feet are used as well.\n\n== Mechanics ==\n{{see|Binary numeral system}}\n\nIn the binary number system, each [[numerical digit]] has two possible states (0 or 1) and each successive digit represents an increasing [[power of two]].\n\nNote: What follows is but one of several possible schemes for assigning the values 1, 2, 4, 8, 16, etc. to fingers, not necessarily the best. (see below the illustrations.): The rightmost digit represents two to the [[zeroth power]] (i.e., it is the \"ones digit\"); the digit to its left represents two to the first power (the \"twos digit\"); the next digit to the left represents two to the second power (the \"fours digit\"); and so on.  (The [[decimal|decimal number system]] is essentially the same, only that powers of ten are used: \"ones digit\", \"tens digit\" \"hundreds digit\", etc.)\n\nIt is possible to use [[digit (anatomy)|anatomical digits]] to represent [[numerical digit]]s by using a raised finger to represent a binary digit in the \"1\" state and a lowered finger to represent it in the \"0\" state. Each successive finger represents a higher power of two.\n\nWith palms oriented toward the counter's face, the values for when only the right hand is used are:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! !![[little finger|Pinky]] !![[ring finger|Ring]] !![[middle finger|Middle]] !![[index finger|Index]] !![[Thumb]] \n|-\n!Power of two\n|2<sup>4</sup> || 2<sup>3</sup> || 2<sup>2</sup> || 2<sup>1</sup> || 2<sup>0</sup>\n|-\n!Value\n| [[16 (number)|16]] || [[8 (number)|8]] || [[4 (number)|4]] || [[2 (number)|2]] || [[1 (number)|1]]\n|}\n\nWhen only the left hand is used:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! !![[Thumb]] !![[index finger|Index]] !![[middle finger|Middle]] !![[ring finger|Ring]] !![[little finger|Pinky]] \n|-\n!Power of two\n|2<sup>4</sup> || 2<sup>3</sup> || 2<sup>2</sup> || 2<sup>1</sup> || 2<sup>0</sup>\n|-\n!Value\n| [[16 (number)|16]] || [[8 (number)|8]] || [[4 (number)|4]] || [[2 (number)|2]] || [[1 (number)|1]]\n|}\n\nWhen both hands are used:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n!rowspan=2|\n!colspan=5| Left hand\n!colspan=5| Right hand\n|-\n!Thumb !!Index !!Middle !!Ring !!Pinky\n!Pinky !!Ring !!Middle !!Index !!Thumb\n|-\n!Power of two\n|2<sup>9</sup> || 2<sup>8</sup> || 2<sup>7</sup> || 2<sup>6</sup> || 2<sup>5</sup> || 2<sup>4</sup> || 2<sup>3</sup> || 2<sup>2</sup> || 2<sup>1</sup> || 2<sup>0</sup>\n|-\n!Value\n| 512 || 256 || 128 || 64 || 32 || 16 || 8 || 4 || 2 || 1\n|}\n\nAnd, alternately, with the palms oriented away from the counter:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n!rowspan=2|\n!colspan=5| Left hand\n!colspan=5| Right hand\n|-\n!Pinky !!Ring !!Middle !!Index !!Thumb\n!Thumb !!Index !!Middle !!Ring !!Pinky\n|-\n!Power of two\n|2<sup>9</sup> || 2<sup>8</sup> || 2<sup>7</sup> || 2<sup>6</sup> || 2<sup>5</sup> || 2<sup>4</sup> || 2<sup>3</sup> || 2<sup>2</sup> || 2<sup>1</sup> || 2<sup>0</sup>\n|-\n!Value\n| 512 || 256 || 128 || 64 || 32 || 16 || 8 || 4 || 2 || 1\n|}\nThe values of each raised finger are added together to arrive at a total number. In the one-handed version, all fingers raised is thus '''31''' (16 + 8 + 4 + 2 + 1), and all fingers lowered (a fist) is 0. In the two-handed system, all fingers raised is '''1,023''' (512 + 256 + 128 + 64 + 32 + 16 + 8 + 4 + 2 + 1) and two fists (no fingers raised) represents 0.\n\nIt is also possible to have each hand represent an independent number between 0 and 31; this can be used to represent various types of paired numbers, such as [[month]] and [[day]], X-Y [[coordinate]]s, or sports scores (such as for [[table tennis]] or [[baseball]]).\n\n=== Examples ===\n==== Right hand ====\n<gallery>\nFile:LSQ a.jpg|'''0''' = [[empty sum]]\nFile:Thumbs up.JPG|'''1''' = 1\nFile:D@InForward.jpg|'''2''' = 2\nMiddle_finger_3_(mirrored).JPG|'''4''' = 4 \nFile:LSQ v.jpg|'''6''' = 4 + 2\nFile:Tri prsta.jpg|'''7''' = 4 + 2 + 1\nFile:LSQ 6.jpg|'''14''' = 8 + 4 + 2\nFile:LSQ i.jpg|'''16''' = 16\nFile:I love you in Sign Language or the number 19 in Finger Binary.jpg|'''19''' = 16 + 2 + 1\nFile:LSQ 8.jpg|'''26''' = 16 + 8 + 2\nFile:LSQ 9.jpg|'''28''' = 16 + 8 + 4\nFile:LSQ 4.jpg|'''30''' = 16 + 8 + 4 + 2\nFile:LSQ 5.jpg|'''31''' = 16 + 8 + 4 + 2 + 1\n</gallery>\n\n==== Left hand ====\nWhen used in addition to the right.\n<gallery>\nImage:Thumbs up.jpg|'''512''' = 512\nImage:Chinesische.Zahl.Eins.jpg|'''256''' = 256\nImage:Chinesische.Zahl.Acht.jpg|'''768''' = 512 + 256\nImage:Chinesische.Zahl.Drei.jpg|'''448''' = 256 + 128 + 64\nImage:Chinesische.Zahl.Sechs.jpg|'''544''' = 512 + 32\nImage:Chinesische.Zahl.Vier.jpg|'''480''' = 256 + 128 + 64 + 32\nImage:Chinesische.Zahl.Fuenf.jpg|'''992''' = 512 + 256 + 128 + 64 + 32\n</gallery>\n\n== Negative numbers and non-integers ==\n\n{{more|Binary numeral system#Representing real numbers}}\n\nJust as fractional and negative numbers can be represented in binary, they can be represented in finger binary.  \n\n=== Negative numbers ===\n\nRepresenting negative numbers is extremely simple, by using the leftmost finger as a [[sign bit]]: raised means the number is negative, in a [[sign-magnitude]] system. Anywhere between -511 and +511 can be represented this way, using two hands. Note that, in this system, both a positive and a negative zero may be represented.\n\nIf a convention were reached on palm up/palm down or fingers pointing up/down representing positive/negative, you could maintain 2<sup>10</sup> - 1 in both positive and negative numbers (-1023 to +1023, with positive and negative zero still represented).\n\n=== Fractions ===\n\nThere are multiple ways of representing fractions in finger binary.\n\n==== Dyadic fractions ====\n\nFractions can be stored natively in a binary format by having each finger represent a fractional power of two: <math>\\tfrac{1}{2^x}</math>.  (These are known as [[dyadic fraction]]s.)\n\nUsing the left hand only:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n!\n!Pinky !! Ring !! Middle !! Index !! Thumb\n|-\n!Value\n|1/2 || 1/4 || 1/8 || 1/16 || 1/32\n|}\n\nUsing two hands:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n!colspan=5| <big>Left hand</big>\n!colspan=5| <big>Right hand</big>\n|-\n!Pinky !!Ring !!Middle !!Index !!Thumb\n!Thumb !!Index !!Middle !!Ring !!Pinky\n|-\n|1/2 || 1/4 || 1/8 || 1/16 || 1/32 || 1/64 || 1/128 || 1/256 || 1/512 || 1/1024\n|}{{clear}}\n\n[[File:Chinesische.Zahl.Acht.jpg|thumb|right|3/4, in fractional finger binary]]\nThe total is calculated by adding all the values in the same way as regular (non-fractional) finger binary, then dividing by the largest fractional power being used (32 for one-handed fractional binary, 1024 for two-handed), and [[Equivalent fractions|simplifying the fraction]] as necessary.\n\nFor example, with thumb and index finger raised on the left hand and no fingers raised on the right hand, this is (512 + 256)/1024 = 768/1024 = 3/4. If using only one hand (left or right), it would be (16 + 8)/32 = 24/32 = 3/4 also.\n\nThe simplification process can itself be greatly simplified by performing a [[bit shift]] operation: all digits to the right of the rightmost raised finger (i.e., all trailing zeros) are discarded and the rightmost raised finger is treated as the ones digit. The digits are added together using their now-shifted values to determine the [[numerator]] and the rightmost finger's original value is used to determine the [[denominator]].\n\nFor instance, if the thumb and index finger on the left hand are the only raised digits, the rightmost raised finger (the index finger) becomes \"1\". The thumb, to its immediate left, is now the 2s digit; added together, they equal 3. The index finger's original value (1/4) determines the denominator: the result is 3/4.\n\n==== Rational numbers ====\n\nCombined [[integer]] and fractional values (i.e., [[rational number]]s) can be represented by setting a [[radix point]] somewhere between two fingers (for instance, between the left and right pinkies). All digits to the left of the radix point are integers; those to the right are fractional.\n\n=== Decimal fractions and vulgar fractions ===\n\n[[Dyadic fraction]]s, explained above, unfortunately have limited use in a society based around decimal figures. A simple non-dyadic fraction such as 1/3 can be approximated as 341/1024 (0.3330078125), but the conversion between dyadic and [[decimal fraction|decimal]] (0.333) or [[vulgar fraction|vulgar]] (1/3) forms is complicated.  \n\nInstead, either decimal or vulgar fractions can be represented natively in finger binary. Decimal fractions can be represented by using regular integer binary methods and dividing the result by 10, 100, 1000, or some other power of ten. Numbers between 0 and 102.3, 10.23, 1.023, etc. can be represented this way, in increments of 0.1, 0.01, 0.001, etc.\n\n[[Vulgar fraction]]s can be represented by using one hand to represent the [[numerator]] and one hand to represent the [[denominator]]; a spectrum of rational numbers can be represented this way, ranging from 1/31 to 31/1 (as well as 0).\n\n== Finger ternary ==\n\nIn theory, it is possible to use other positions of the fingers to represent more than two states (0 and 1); for instance, a [[ternary numeral system]] ([[number base|base]] 3) could be used by having a fully raised finger represent 2, fully lowered represent 0, and \"curled\" (half-lowered) represent 1. This would make it possible to count up to 59,048 (3<sup>10</sup>&minus;1) on two hands. In practice, however, many people will find it difficult to hold all fingers independently (especially the middle and ring fingers) in more than two distinct positions.\n\n==See also==\n*[[Chisanbop]]\n*[[Senary#Finger counting]]\n\n==References==\n* {{cite book|last=Pohl|first=Frederik|title=Chasing Science|publisher=Macmillan|date=2003|edition=reprint, illustrated|page=304|isbn=978-0-7653-0829-0|url=https://books.google.com/books?id=XsLXJMagfmUC&pg=PA187&dq=fingers+binary+1023#PPA187,M1}}\n* {{cite book|last=Pohl|first=Frederik|title=The Best of Frederik Pohl|publisher=Sidgwick & Jackson|date=1976|page=363|url=https://books.google.com/books?id=fDxbAAAAMAAJ&q=fingers+binary+1023&dq=fingers+binary+1023&pgis=1}}\n* {{cite book|last=Fahnestock|first=James D.|title=Computers and how They Work|publisher=Ziff-Davis Pub. Co.|date=1959|page=228|url=https://books.google.com/books?id=j_0mAAAAMAAJ&q=fingers+binary+1023&dq=fingers+binary+1023&pgis=1}}\n\n==External links==\n<!-- * [http://www.intuitor.com/counting/ How to count to 1,023 on your fingers] -->\n* [http://www.instructables.com/id/Binary-Counting/ Binary Counting]\n\n{{Gestures}}\n\n[[Category:Finger-counting|Binary]]\n[[Category:Elementary arithmetic]]\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Gray code",
      "url": "https://en.wikipedia.org/wiki/Gray_code",
      "text": "{{short description|ordering of binary strings such that subsequent strings differ only in one bit}}\n{{Use dmy dates|date = May 2019|cs1-dates = y}}\n\nThe '''reflected binary code''' ('''RBC'''), also known just as '''reflected binary''' ('''RB''') or '''Gray code''' after [[Frank Gray (researcher)|Frank Gray]], is an ordering of the [[binary numeral system]] such that two successive values differ in only one [[bit]] (binary digit).\nThe reflected binary code was originally designed to prevent spurious output from [[electromechanical]] [[switch]]es. Today, Gray codes are widely used to facilitate [[error correction]] in digital communications such as [[digital terrestrial television]] and some [[DOCSIS|cable TV]] systems.\n{{Gray code by bit width}}\n\n== Name ==\n\n[[File:Reflected binary Gray 2632058.png|thumb|Gray's patent introduces the term \"reflected binary code\"]] [[Bell Labs]] researcher [[Frank Gray (researcher)|Frank Gray]] introduced the term ''reflected binary code'' in his 1947 patent application, remarking that the code had \"as yet no recognized name\".<ref name=\"patent\">{{citation |author-first=Frank |author-last=Gray |author-link=Frank Gray (researcher) |title=Pulse code communication |date=1953-03-17}} (NB. {{US patent|2632058}} filed November 1947.)</ref> He derived the name from the fact that it \"may be built up from the conventional binary code by a sort of reflection process\".\n\nThe code was later named after Gray by others who used it. Two different 1953 patent applications use \"Gray code\" as an alternative name for the \"reflected binary code\";<ref name=\"pat1\">{{citation |author-first=Jack |author-last=Breckman |title=Encoding Circuit |date=1956-01-31}} (NB. {{US patent|2733432}} filed December 1953.)</ref><ref name=\"pat2\">{{citation |author-first1=Earl Albert |author-last1=Ragland |author-first2=Harry B. |author-last2=Schultheis, Jr. |title=Direction-Sensitive Binary Code Position Control System |date=1958-02-11}} (NB. {{US patent|2823345}} filed October 1953).)</ref> one of those also lists \"minimum error code\" and \"cyclic permutation code\" among the names.<ref name=\"pat2\" /> A 1954 patent application refers to \"the Bell Telephone Gray code\".<ref name=\"pat3\">{{citation |author-first1=Sol |author-last1=Domeshek |author-first2=Stewart |author-last2=Reiner |title=Automatic Rectification System |date=1958-06-24}} (NB. {{US patent|2839974}} filed January 1954.)</ref> Other names include \"cyclic binary code\" and \"cyclic progression code\".<ref name=\"Petherick_1953\" /><ref name=\"Winder_1959\">{{cite journal |title=Shaft Angle Encoders Afford High Accuracy |author-first=C. Farrell |author-last=Winder |journal=[[Electronic Industries (journal)|Electronic Industries]] |publisher=[[Chilton Company]] |volume=18 |number=10 |date=October 1959 |pages=76–80 |url=http://www.americanradiohistory.com/Archive-Electronic-Industries/50s/Electronic-Industries-1959-10.pdf |access-date=2018-01-14 |quote=[…] The type of code wheel most popular in [[optical encoder]]s contains a cyclic binary code pattern designed to give a cyclic sequence of \"on-off\" outputs. The cyclic binary code is also known as the cyclic progression code, the reflected binary code, and the Gray code. This code was originated by [[G. R. Stibitz]], of [[Bell Telephone Laboratories]], and was first proposed for [[pulse code modulation]] systems by [[Frank Gray (researcher)|Frank Gray]], also of BTL. Thus the name Gray code.It is also named as \"Unit Distance Code\" as any two adjacent codes is differ by one (1). The Gray or cyclic code is used mainly to eliminate the possibility of errors at code transition which could result in gross ambiguities. […]}}</ref>\n\n== Motivation ==\n\nMany devices indicate position by closing and opening switches. If that device uses [[natural binary code]]s, positions 3 and 4 are next to each other but all three bits of the binary representation differ:\n\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! Decimal !! Binary\n|-\n| ... || ...\n|-\n| 3 || 011\n|-\n| 4 || 100\n|-\n| ... || ...\n|}\n\nThe problem with natural binary codes is that physical switches are not ideal: it is very unlikely that physical switches will change states exactly in synchrony. In the transition between the two states shown above, all three switches change state. In the brief period while all are changing, the switches will read some spurious position. Even without [[keybounce]], the transition might look like 011 — 001 — 101 — 100. When the switches appear to be in position 001, the observer cannot tell if that is the \"real\" position 001, or a transitional state between two other positions. If the output feeds into a [[sequential logic|sequential]] system, possibly via [[combinational logic]], then the sequential system may store a false value.\n\nThe reflected binary code solves this problem by changing only one switch at a time, so there is never any ambiguity of position:\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! Decimal !! Binary !! Gray\n|-\n| 0 || 0000 || 0000\n|-\n| 1 || 0001 || 0001\n|-\n| 2 || 0010 || 0011\n|-\n| 3 || 0011 || 0010\n|-\n| 4 || 0100 || 0110\n|-\n| 5 || 0101 || 0111\n|-\n| 6 || 0110 || 0101\n|-\n| 7 || 0111 || 0100\n|-\n|8\n|1000\n|1100\n|-\n|9\n|1001\n|1101\n|-\n|10\n|1010\n|1111\n|-\n|11\n|1011\n|1110\n|-\n|12\n|1100\n|1010\n|-\n|13\n|1101\n|1011\n|-\n|14\n|1110\n|1001\n|-\n|15\n|1111\n|1000\n|}\n\nThe Gray code for decimal 15 rolls over to decimal 0 with only one switch change. This is called the \"cyclic\" property of a Gray code. In the standard Gray coding the least significant bit follows a repetitive pattern of 2 on, 2 off {{nowrap|( … 11001100 … );}} the next digit a pattern of 4 on, 4 off; and so forth.\n\nMore formally, a '''Gray code''' is a code assigning to each of a contiguous set of [[integer]]s, or to each member of a circular list, a word of symbols such that no two code words are identical and each two adjacent code words differ by exactly one symbol. These codes are also known as ''single-distance codes'', in reference to the [[Hamming distance]] of 1 between adjacent codes. In principle, there can be more than one such code for a given word length, but the term Gray code was first applied to a particular [[binary numeral system|binary]] code for non-negative integers, the ''binary reflected Gray code'', or '''BRGC''', the four-bit version of which is shown above.\n\n== History and practical application ==\n\nReflected binary codes were applied to mathematical puzzles before they became known to engineers. [[Martin Gardner]] wrote a popular account of the Gray code in his August 1972 [[Mathematical Games column]] in Scientific American. The French engineer [[Émile Baudot]] used Gray codes in [[telegraphy]] in 1878.<ref>{{cite book |url=https://books.google.com/books?id=JrslMKTgSZwC&pg=PA392 |title=The Math Book: From Pythagoras to the 57th Dimension, 250 Milestones in the History of Mathematics |page=392 |author-first1=Clifford A. |author-last1=Pickover |publisher=[[Sterling Publishing Company]] |date=2009 |isbn=978-1-4027-5796-9}}</ref>  He received the French [[Légion d'honneur|Legion of Honor]] medal for his work. The Gray code is sometimes attributed, incorrectly,<ref name=\"Knuth\">{{cite book |author-last=Knuth |author-first=Donald Ervin |author-link=Donald Ervin Knuth |chapter=Generating all ''n''-tuples |title=The Art of Computer Programming, Volume 4A: Enumeration and Backtracking |volume=pre-fascicle 2a |date=2004-10-15 |chapter-url=http://www-cs-faculty.stanford.edu/~knuth/fasc2a.ps.gz}}</ref> to [[Elisha Gray]].<ref>{{cite book |author-first=Kenneth W. |author-last=Cattermole |title=Principles of Pulse Code Modulation |publisher=[[American Elsevier]] |date=1969 |location=New York, USA |isbn=0-444-19747-8 }}</ref><ref name=\"Edwards_2004\">{{cite book |title=Cogwheels of the Mind: The Story of Venn Diagrams |author-first=Anthony William Fairbank |author-last=Edwards |author-link=Anthony William Fairbank Edwards<!-- |contribution=Foreword |contributor-first=Ian |contributor-last=Stewart --> |publisher=[[Johns Hopkins University Press]] |date=2004 |isbn=0-8018-7434-3 |location=Baltimore, Maryland, USA |pages=48, 50 |url=https://books.google.com/books?id=7_0Thy4V3JIC&pg=PA65}}</ref>\n\n[[Frank Gray (researcher)|Frank Gray]], who became famous for inventing the signaling method that came to be used for compatible color television, invented a method to convert analog signals to reflected binary code groups using [[vacuum tube]]-based apparatus. The method and apparatus were patented in 1953 and the name of Gray stuck to the codes. The \"[[Pulse code modulation#History|PCM tube]]\" apparatus that Gray patented was made by Raymond W. Sears of Bell Labs, working with Gray and William M. Goodall, who credited Gray for the idea of the reflected binary code.<ref name=\"Goodall_1951\">{{cite journal |author-first=William M. |author-last=Goodall |title=Television by Pulse Code Modulation |journal=[[Bell System Technical Journal]] |volume=30 |issue=1 |pages=33–49 |date=January 1951 |doi=10.1002/j.1538-7305.1951.tb01365.x}} (NB. Presented orally before the I.R.E. National Convention, New York City, March 1949.)</ref>\n[[File:US02632058 Gray.png|thumb|500px|center|Part of front page of Gray's patent, showing PCM tube (10) with reflected binary code in plate (15)]]\n\nGray was most interested in using the codes to minimize errors in converting analog signals to digital; his codes are still used today for this purpose.\n\n=== Position encoders ===\n\n[[File:Encoder Disc (3-Bit).svg|thumb|[[Rotary encoder]] for angle-measuring devices marked in 3-bit binary reflected Gray code (BRGC)]]\n[[File:Gray code rotary encoder 13-track opened.jpg|thumb|A Gray code absolute rotary encoder with 13 tracks. At the top can be seen the housing, interrupter disk, and light source; at the bottom can be seen the sensing element and support components.]]\n\nGray codes are used in position encoders ([[linear encoder]]s and [[rotary encoder]]s), in preference to straightforward binary encoding. This avoids the possibility that, when several bits change in the binary representation of an angle, a misread will result from some of the bits changing before others. Originally, the code pattern was electrically conductive, supported (in a rotary encoder) by an insulating disk. Each track had its own stationary metal spring contact; one more contact made the connection to the pattern. That common contact was connected by the pattern to whichever of the track contacts were resting on the conductive pattern. However, sliding contacts wear out and need maintenance, so non-contact detectors, such as optical or magnetic sensors, are often used instead.\n\nRegardless of the care in aligning the contacts, and accuracy of the pattern, a natural-binary code would have errors at specific disk positions, because it is impossible to make all bits change at exactly the same time as the disk rotates. The same is true of an optical encoder; transitions between opaque and transparent cannot be made to happen simultaneously for certain exact positions.  Rotary encoders benefit from the cyclic nature of Gray codes, because consecutive positions of the sequence differ by only one bit. This means that, for a transition from state A to state B, timing mismatches can only affect when the A&nbsp;→&nbsp;B transition occurs, rather than inserting one or more (up to ''N''&nbsp;−&nbsp;1 for an ''N''-bit codeword) false intermediate states, as would occur if a standard binary code were used.\n\n=== Mathematical puzzles ===\n\nThe binary reflected Gray code can serve as a solution guide for the [[Tower of Hanoi|Towers of Hanoi problem]], as well as the classical [[Chinese rings puzzle]], a sequential mechanical puzzle mechanism.<ref name=\"Knuth\" /> It also forms a [[Hamiltonian cycle]] on a [[hypercube]], where each bit is seen as one dimension.\n\n=== Genetic algorithms ===\n\nDue to the [[Hamming distance]] properties of Gray codes, they are sometimes used in [[genetic algorithm]]s. They are very useful in this field, since mutations in the code allow for mostly incremental changes, but occasionally a single bit-change can cause a big leap and lead to new properties.\n\n=== Boolean circuit minimization ===\n\nGray codes are also used in labelling the axes of [[Karnaugh map]]s<ref>{{cite book |title=Digital Design: Principles & Practices |author-last=Wakerly |author-first=John F. |date=1994 |publisher=[[Prentice Hall]] |location=New Jersey, USA |isbn=0-13-211459-3 |pages=222, 48–49}} (NB. The two page sections taken together say that [[K-map]]s are labeled with Gray code. The first section says that they are labeled with a code that changes only one bit between entries and the second section says that such a code is called Gray code.)</ref><ref>{{cite book |title=Boolean Reasoning – The Logic of Boolean Equations |author-first=Frank Markham |author-last=Brown |edition=<!-- 2012 -->reissue of 2nd |publisher=[[Dover Publications, Inc.]] |location=Mineola, New York |date=2012 |orig-year=2003, 1990 |isbn=978-0-486-42785-0 |page=49}} [<!-- 1st edition -->http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf<!-- https://web.archive.org/web/20170416231752/http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf -->]</ref> as well as in [[Händler circle graph]]s,<ref>{{cite book |title=Ein Minimisierungsverfahren zur Synthese von Schaltkreisen: Minimisierungsgraphen |language=de |author-first=Wolfgang |author-last=Händler |author-link=Wolfgang Händler |publisher=[[Technische Hochschule Darmstadt]] |date=1958 |id=D&nbsp;17 |type=Dissertation |url=https://books.google.com/books?id=D58TAQAAIAAJ}} (NB. Although written by a German, the title contains an [[anglicism]]; the correct German term would be \"Minimierung\" instead of \"Minimisierung\".)</ref><ref>{{cite book |title=Taschenbuch der Nachrichtenverarbeitung |language=de |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Siegfried W. |editor-last2=Wagner |author-first1=Erich R. |author-last1=Berger |author-first2=Wolfgang |author-last2=Händler |author-link2=Wolfgang Händler |date=1967 |orig-year=1962 |edition=2 |publisher=[[Springer-Verlag OHG]] |location=Berlin, Germany |id=Title No. 1036 |lccn=67-21079 |pages=64, 1034–1035, 1036, 1038 |quote=[…] Übersichtlich ist die Darstellung nach ''[[Wolfgang Händler|Händler]]'', die sämtliche Punkte, numeriert nach dem ''Gray-Code'' […], auf dem Umfeld eines Kreises anordnet. Sie erfordert allerdings sehr viel Platz. […] [''Händler's'' diagram, where all points, numbered according to the ''Gray code'', are arranged on the circumference of a circle, is easily comprehensible. It needs, however, a lot of space.]}}</ref><ref>{{cite web |title=Informatik Sammlung Erlangen (ISER) |date=2012-03-13 |publisher=[[Friedrich-Alexander Universität]] |location=Erlangen, Germany |language=de |url=https://www.rrze.fau.de/wir-ueber-uns/kooperationen/iser.shtml |access-date=2017-04-12 |dead-url=yes |archive-url=https://web.archive.org/web/20170516154655/https://www.rrze.fau.de/wir-ueber-uns/kooperationen/iser.shtml |archive-date=2017-05-16}} (NB. Shows a picture of a [[Wolfgang Händler|Händler]] circle graph.)</ref><ref>{{cite web |title=Informatik Sammlung Erlangen (ISER) – Impressum |date=2012-03-13 |publisher=[[Friedrich-Alexander Universität]] |location=Erlangen, Germany |language=de |url=http://www.iser.uni-erlangen.de:80/index.php?ort_id=327&tree=0 |access-date=2017-04-15 |dead-url=no |archive-url=https://web.archive.org/web/20120226004316/http://www.iser.uni-erlangen.de/index.php?ort_id=327&tree=0 |archive-date=2012-02-26}} (NB. Shows a picture of a [[Wolfgang Händler|Händler]] circle graph.)</ref> both graphical methods for [[logic circuit minimization]].\n\n=== Error correction ===\n\nIn modern [[digital communications]], Gray codes play an important role in [[error correction]]. For example, in a [[digital modulation]] scheme such as [[quadrature amplitude modulation|QAM]] where data is typically transmitted in [[symbol rate|symbols]] of 4 bits or more, the signal's [[constellation diagram]] is arranged so that the bit patterns conveyed by adjacent constellation points differ by only one bit. By combining this with [[forward error correction]] capable of correcting single-bit errors, it is possible for a [[Receiver (radio)|receiver]] to correct any transmission errors that cause a constellation point to deviate into the area of an adjacent point. This makes the transmission system less susceptible to [[noise]].\n\n=== Communication between clock domains ===\n{{Main|Clock domain crossing}}\n\nDigital logic designers use Gray codes extensively for passing multi-bit count information between synchronous logic that operates at different clock frequencies. The logic is considered operating in different \"clock domains\". It is fundamental to the design of large chips that operate with many different clocking frequencies.\n\n=== Cycling through states with minimal effort ===\n\nIf a system has to cycle through all possible combinations of on-off states of some set of controls, and the changes of the controls require non-trivial expense (e.g. time, wear, human work), a Gray code minimizes the number of setting changes to just one change for each combination of states. An example would be testing a piping system for all combinations of settings of its manually operated valves.\n\n==== Gray code counters and arithmetic ====\n\nA typical use of Gray code counters is building a [[FIFO (computing and electronics)|FIFO]] (first-in, first-out) data buffer that has read and write ports that exist in different clock domains. The input and output counters inside such a dual-port FIFO are often stored using Gray code to prevent invalid transient states from being captured when the count crosses clock domains.<ref name=\"Donohue_2003\">{{cite web |author-first=Ryan |author-last=Donohue |title=Synchronization in Digital Logic Circuits |date=2003 |url=http://www.stanford.edu/class/ee183/handouts_spr2003/synchronization_pres.pdf |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115012747/https://web.stanford.edu/class/ee183/handouts_spr2003/synchronization_pres.pdf |archive-date=2018-01-15}}</ref>\nThe updated read and write pointers need to be passed between clock domains when they change, to be able to track FIFO empty and full status in each domain. Each bit of the pointers is sampled non-deterministically for this clock domain transfer. So for each bit, either the old value or the new value is propagated. Therefore, if more than one bit in the multi-bit pointer is changing at the sampling point, a \"wrong\" binary value (neither new nor old) can be propagated. By guaranteeing only one bit can be changing, Gray codes guarantee that the only possible sampled values are the new or old multi-bit value. Typically Gray codes of power-of-two length are used.\n\nSometimes digital buses in electronic systems are used to convey quantities that can only increase or decrease by one at a time, for example the output of an event counter which is being passed between clock domains or to a digital-to-analog converter. The advantage of Gray codes in these applications is that differences in the propagation delays of the many wires that represent the bits of the code cannot cause the received value to go through states that are out of the Gray code sequence.  This is similar to the advantage of Gray codes in the construction of mechanical encoders, however the source of the Gray code is an electronic counter in this case. The counter itself must count in Gray code, or if the counter runs in binary then the output value from the counter must be reclocked after it has been converted to Gray code, because when a value is converted from binary to Gray code, it is possible that differences in the arrival times of the binary data bits into the binary-to-Gray conversion circuit will mean that the code could go briefly through states that are wildly out of sequence. Adding a clocked register after the circuit that converts the count value to Gray code may introduce a clock cycle of latency, so counting directly in Gray code may be advantageous. A Gray code counter was patented in 1962 {{US patent|3020481|US3020481}}, and there have been many others since. In recent times a Gray code counter can be implemented as a state machine in [[Verilog]]. In order to produce the next count value, it is necessary to have some combinational logic that will increment the current count value that is stored in Gray code.  Probably the most obvious way to increment a Gray code number is to convert it into ordinary binary code, add one to it with a standard binary adder, and then convert the result back to Gray code. This approach was discussed in a paper in 1996<ref>{{cite book |author-last1=Mehta |author-first1=Huzefa |author-last2=Owens |author-first2=Robert Michael |author-last3=Irwin |author-first3=Mary Jane \"Janie\" |date=1996-03-22 |issn=1066-1395 |doi=10.1109/GLSV.1996.497616 |title=Some issues in Gray code addressing |journal=Proceedings of the 6th Great Lakes Symposium on VLSI (GLSVLSI 96) |publisher=[[IEEE Computer Society]] |isbn=978-0-8186-7502-7 |pages=178–181}}</ref> and then subsequently patented by someone else in 1998 {{US patent|5754614|US5754614}}. Other methods of counting in Gray code are discussed in a report by Robert W. Doran, including taking the output from the first latches of the master-slave flip flops in a binary ripple counter.<ref>{{cite web |author-first=Robert W. |author-last=Doran |title=The Gray Code |series=CDMTCS Research Report Series |publisher=[[University of Auckland]], New Zealand |date=March 2007 |id=CDMTCS-304 |url=http://www.cs.auckland.ac.nz/CDMTCS//researchreports/304bob.pdf |access-date=2017-10-29 |dead-url=no |archive-url=https://web.archive.org/web/20171029190028/https://www.cs.auckland.ac.nz/research/groups/CDMTCS//researchreports/304bob.pdf |archive-date=2017-10-29}}</ref>\n\nPerhaps the most common electronic counter with the \"only one bit changes at a time\" property is the [[Johnson counter]].\n\n==== {{anchor|Shifted Gray encoding}}Gray code addressing ====\nAs the execution of [[program code]] typically causes an instruction memory access pattern of locally consecutive addresses, [[bus encoding]]s using Gray code addressing instead of binary addressing can reduce the number of state changes of the address bits significantly, thereby reducing the [[CPU power consumption]] in some low-power designs.<ref>{{cite report |author-first1=Ching-Long |author-last1=Su |author-first2=Chi-Ying |author-last2=Tsui |author-first3=Alvin M. |author-last3=Despain |url=http://www.scarpaz.com/2100-papers/Power%20Estimation/su94-low%20power%20architecture%20and%20compilation.pdf |title=Low Power Architecture Design and Compilation Techniques for High-Performance Processors |date=1994 |publisher=Advanced Computer Architecture Laboratory |id=ACAL-TR-94-01}}</ref><ref name=\"Shifted_Gray\">{{cite journal |author-first1=Hui |author-last1=Guo |author-first2=Sri |author-last2=Parameswaran |doi=10.1016/j.sysarc.2010.03.003 |volume=56 |issue=4–6 |date=April–June 2010 |title=Shifted Gray encoding to reduce instruction memory address bus switching for low-power embedded systems |journal=Journal of Systems Architecture |pages=180–190}}</ref>\n\n== Constructing an ''n''-bit Gray code ==\n\n[[File:Binary-reflected Gray code construction.svg|frame|right|The first few steps of the reflect-and-prefix method.]]\n[[File:Gray code permutation matrix 16.svg|thumb|right|250px|4-bit Gray code permutation]]\n\nThe binary reflected Gray code list for ''n'' bits can be generated [[recursion|recursively]] from the list for ''n''&nbsp;−&nbsp;1 bits by reflecting the list (i.e. listing the entries in reverse order), prefixing the entries in the original list with a binary 0, prefixing the entries in the reflected list with a binary&nbsp;1, and then concatenating the original list with the reversed list.<ref name=\"Knuth\" />  For example, generating the ''n''&nbsp;=&nbsp;3 list from the ''n''&nbsp;=&nbsp;2 list:\n\n{| cellpadding=\"5\" border=\"0\" style=\"margin: 1em;\"\n|-\n| 2-bit list:\n| 00, 01, 11, 10\n| &nbsp;\n|-\n| Reflected:\n| &nbsp;\n| 10, 11, 01, 00\n|-\n| Prefix old entries with ''0'':\n| 000, 001, 011, 010,\n| &nbsp;\n|-\n| Prefix new entries with ''1'':\n| &nbsp;\n| 110, 111, 101, 100\n|-\n| Concatenated:\n| 000, 001, 011, 010,\n| 110, 111, 101, 100\n|}\n\nThe one-bit Gray code is ''G''<sub>1</sub>&nbsp;=&nbsp;(0,&nbsp;1). This can be thought of as built recursively as above from a zero-bit Gray code ''G''<sub>0</sub>&nbsp;=&nbsp;(&nbsp;[[Empty string|Λ]]&nbsp;) consisting of a single entry of zero length. This iterative process of generating ''G''<sub>''n''+1</sub> from ''G''<sub>''n''</sub> makes the following properties of the standard reflecting code clear:\n\n* ''G''<sub>''n''</sub> is a [[permutation]] of the numbers 0, ..., 2<sup>''n''</sup>&nbsp;−&nbsp;1.  (Each number appears exactly once in the list.)\n* ''G''<sub>''n''</sub> is embedded as the first half of ''G''<sub>''n''+1</sub>.\n* Therefore, the coding is ''stable'', in the sense that once a binary number appears in ''G''<sub>''n''</sub> it appears in the same position in all longer lists; so it makes sense to talk about ''the'' reflective Gray code value of a number: ''G''(''m'') = the ''m''-th reflecting Gray code, counting from 0.\n* Each entry in ''G''<sub>''n''</sub> differs by only one bit from the previous entry. (The Hamming distance is 1.)\n* The last entry in ''G''<sub>''n''</sub> differs by only one bit from the first entry. (The code is cyclic.)\n\nThese characteristics suggest{{Elucidate|date=February 2017}} a simple and fast method of translating a binary value into the corresponding Gray code. Each bit is inverted if the next higher bit of the input value is set to one. This can be performed in parallel by a bit-shift and exclusive-or operation if they are available: the ''n''th Gray code is obtained by computing <math>n \\oplus \\lfloor n/2 \\rfloor</math>\n\nA similar method can be used to perform the reverse translation, but the computation of each bit depends on the computed value of the next higher bit so it cannot be performed in parallel. Assuming <math>g_i</math> is the <math>i</math>th Gray-coded bit (<math>g_0</math> being the most significant bit), and <math>b_i</math> is the <math>i</math>th binary-coded bit (<math>b_0</math> being the most-significant bit), the reverse translation can be given recursively: <math>b_0 = g_0</math>, and <math>b_i=g_i \\oplus b_{i-1}</math>. Alternatively, decoding a Gray code into a binary number can be described as a [[prefix sum]] of the bits in the Gray code, where each individual summation operation in the prefix sum is performed modulo two.\n\nTo construct the binary reflected Gray code iteratively, at step 0 start with the <math>\\mathrm{code}_0 = 0</math>, and at step <math>i > 0</math> find the bit position of the least significant 1 in the binary representation of <math>i</math> and flip the bit at that position in the previous code <math>\\mathrm{code}_{i-1}</math> to get the next code <math>\\mathrm{code}_i</math>. The bit positions start 0, 1, 0, 2, 0, 1, 0, 3, ... {{OEIS|id=A007814}}. See [[find first set]] for efficient algorithms to compute these values.\n\nThere is an equivalent and conceptually much simpler definition of the binary reflected Gray code via the following greedy algorithm<ref name=\"Williams_2013\">{{cite conference |title=The greedy Gray code algorithm |last=Williams |first=Aaron |date=2013 |publisher= |book-title=Proceedings of the 13th International Symposium on Algorithms and Data Structures (WADS) |pages=525–536 |location=London (Ontario, Canada) |doi=10.1007/978-3-642-40104-6_46}}</ref>:\nWe initialize the algorithm with the all-0 string of length <math>n</math>.\nNow we repeatedly flip the rightmost (least significant) bit, such that in each step, a new binary string is created that has not been encountered in the list of strings before.\nFor example, in the case <math>n=3</math> we start with 000, then we flip the rightmost bit and get 001.\nWe then flip the middle bit, as flipping the rightmost bit would yield again 000, which we have seen before, yielding 011, etc.\nUnlike the previous descriptions, the greedy rule does not directly translate into an efficient algorithm, as it requires storing an exponentially long list of strings and therefore potentially expensive lookup operations.\n\n== Converting to and from Gray code ==\n\nThe following functions in [[C (programming language)|C]] convert between binary numbers and their associated Gray codes. While it may seem that Gray-to-binary conversion requires each bit to be handled one at a time, faster algorithms exist.<ref>{{cite web |author-first=Henry Gordon |author-last=Dietz |title=The Aggregate Magic Algorithms: Gray Code Conversion |url=http://aggregate.org/MAGIC/#Gray%20Code%20Conversion}}</ref>\n\n<syntaxhighlight lang=\"C\">\n/*\n * This function converts an unsigned binary\n * number to reflected binary Gray code.\n *\n * The operator >> is shift right. The operator ^ is exclusive or.\n */\nunsigned int BinaryToGray(unsigned int num)\n{\n    return num ^ (num >> 1);\n}\n\n/*\n * This function converts a reflected binary\n * Gray code number to a binary number.\n * Each Gray code bit is exclusive-ored with all\n * more significant bits.\n */\nunsigned int GrayToBinary(unsigned int num)\n{\n    unsigned int mask = num >> 1;\n    while (mask != 0)\n    {\n        num = num ^ mask;\n        mask = mask >> 1;\n    }\n    return num;\n}\n\n/*\n * A more efficient version for Gray codes 32 bits or fewer\n * through the use of SWAR (SIMD within a register) techniques.\n * It implements a parallel prefix XOR function.  The assignment\n * statements can be in any order.\n *\n * This function can be adapted for longer Gray codes by adding steps.\n * A 4-bit variant changes a binary number (abcd)2 to (abcd)2 ^ (00ab)2,\n * then to (abcd)2 ^ (00ab)2 ^ (0abc)2 ^ (000a)2.\n */\nunsigned int GrayToBinary32(unsigned int num)\n{\n    num = num ^ (num >> 16);\n    num = num ^ (num >> 8);\n    num = num ^ (num >> 4);\n    num = num ^ (num >> 2);\n    num = num ^ (num >> 1);\n    return num;\n}\n</syntaxhighlight>\n\n== Restricting the weight range ==\n\n[[File:BRGC weight range restrictions.svg|thumb|Restricting the BRGC of length <math>n=5</math> to four different weight intervals, indicated at the top. 0-bits are drawn as white squares, 1-bits as black squares.]]\nWe define the ''weight'' of a binary string as the number of 1s in the string.\nThe <math>n</math>-bit BRGC has the surprising property that if we consider the subsequence of strings whose weight is in any fixed interval <math>[k,l]</math>, where <math>0\\leq k\\leq l\\leq n</math>, then any two consecutive strings in this subsequence differ in either 1 bit or 2 bits (in the case of distance 2, the difference is an exchange of a 0 with a 1)<ref name=\"Tang_Liu_1973\">{{cite journal |author-first1=Donald T.|author-last1=Tang|author-first2=C. N.|author-last2=Liu|title=Distance-2 cyclic chaining of constant-weight codes|journal=[[IEEE Transactions on Computers]]|volume=C-22|issue=2|pages=176–180|date=1973|doi=10.1109/T-C.1973.223681}}</ref>.\nThis distance property even holds for the difference between the first and last string in the subsequence.\nIn particular, by restricting the weight range to a single number, i.e., by choosing <math>k=l</math>, we get a cyclic distance-2 Gray code for <math>k</math>-[[combination]]s of an <math>n</math>-element set.\nThis is illustrated in the figure on the right for the case <math>n=5</math> and four different weight intervals <math>[k,l]</math>.\n{{clear}}\n\n== Special types of Gray codes ==\n\nIn practice, \"Gray code\" almost always refers to a binary reflected Gray code (BRGC).\nHowever, mathematicians have discovered other kinds of Gray codes.\nLike BRGCs, each consists of a lists of words, where each word differs from the next in only one digit (each word has a [[Hamming distance]] of 1 from the next word).\n\n=== ''n''-ary Gray code ===\n\n{| border=\"0\" cellpadding=\"10\" align=\"right\"\n|-\n| <!-- Second table to provide spacing around the inner table, can't get it otherwise… -->\n{| width=\"150\" align=\"right\" cellpadding=\"5\" border=\"1\" style=\"border-collapse: collapse;\"\n|-\n| ''Ternary number → ternary Gray code''\n   0 → 000\n   1 → 001\n   2 → 002\n  10 → 012\n  11 → 011\n  12 → 010\n  20 → 020\n  21 → 021\n  22 → 022\n 100 → 122\n 101 → 121\n 102 → 120\n 110 → 110\n 111 → 111\n 112 → 112\n 120 → 102\n 121 → 101\n 122 → 100\n 200 → 200\n 201 → 201\n 202 → 202\n 210 → 212\n 211 → 211\n 212 → 210\n 220 → 220\n 221 → 221\n 222 → 222\n|}\n|}\n\nThere are many specialized types of Gray codes other than the binary reflected Gray code. One such type of Gray code is the '''''n''-ary Gray code''', also known as a '''non-Boolean Gray code'''. As the name implies, this type of Gray code uses non-[[Boolean data type|Boolean]] values in its encodings.\n\nFor example, a 3-ary ([[ternary numeral system|ternary]]) Gray code would use the values {0, 1, 2}. The (''n'',&nbsp;''k'')-''Gray code'' is the ''n''-ary Gray code with ''k'' digits.<ref name=\"guan\">{{cite journal |title=Generalized Gray Codes with Applications |author-last=Guan |author-first=Dah-Jyh |journal=Proceedings of the National Scientific Council, Republic of China, Part A |volume=22 |date=1998 |pages=841–848 |citeseerx=10.1.1.119.1344}}</ref>\nThe sequence of elements in the (3,&nbsp;2)-Gray code is: {00, 01, 02, 12, 11, 10, 20, 21, 22}. The (''n'',&nbsp;''k'')-Gray code may be constructed recursively, as the BRGC, or may be constructed [[iteration|iteratively]]. An [[algorithm]] to iteratively generate the (''N'',&nbsp;''k'')-Gray code is presented (in [[C (programming language)|C]]):\n\n<syntaxhighlight lang=\"C\" enclose=\"div\">\n// inputs: base, digits, value\n// output: Gray\n// Convert a value to a Gray code with the given base and digits.\n// Iterating through a sequence of values would result in a sequence\n// of Gray codes in which only one digit changes at a time.\nvoid toGray(unsigned base, unsigned digits, unsigned value, unsigned gray[digits])\n{\n\tunsigned baseN[digits];\t// Stores the ordinary base-N number, one digit per entry\n\tunsigned i;\t\t// The loop variable\n\n\t// Put the normal baseN number into the baseN array. For base 10, 109\n\t// would be stored as [9,0,1]\n\tfor (i = 0; i < digits; i++) {\n\t\tbaseN[i] = value % base;\n\t\tvalue    = value / base;\n\t}\n\n\t// Convert the normal baseN number into the Gray code equivalent. Note that\n\t// the loop starts at the most significant digit and goes down.\n\tunsigned shift = 0;\n\twhile (i--) {\n\t\t// The Gray digit gets shifted down by the sum of the higher\n\t\t// digits.\n\t\tgray[i] = (baseN[i] + shift) % base;\n\t\tshift = shift + base - gray[i];\t// Subtract from base so shift is positive\n\t}\n}\n// EXAMPLES\n// input: value = 1899, base = 10, digits = 4\n// output: baseN[] = [9,9,8,1], gray[] = [0,1,7,1]\n// input: value = 1900, base = 10, digits = 4\n// output: baseN[] = [0,0,9,1], gray[] = [0,1,8,1]\n</syntaxhighlight>\n\nThere are other Gray code algorithms for (''n'',''k'')-Gray codes. The (''n'',''k'')-Gray code produced by the above algorithm is always cyclical; some algorithms, such as that by Guan,<ref name=\"guan\" /> lack this property when k is odd. On the other hand, while only one digit at a time changes with this method, it can change by wrapping (looping from ''n''&nbsp;−&nbsp;1 to 0). In Guan's algorithm, the count alternately rises and falls, so that the numeric difference between two Gray code digits is always one.\n\nGray codes are not uniquely defined, because a permutation of the columns of such a code is a Gray code too. The above procedure produces a code in which the lower the significance of a digit, the more often it changes, making it similar to normal counting methods.\n\nSee also [[Skew binary number system]], a variant ternary number system where at most 2 digits change on each increment, as each increment can be done with at most one digit [[Carry (arithmetic)|carry]] operation.\n\n=== Balanced Gray code ===\n\nAlthough the binary reflected Gray code is useful in many scenarios, it is not optimal in certain cases because of a lack of \"uniformity\".<ref name=\"balanced\">{{cite journal |author-first1=Girish S. |author-last1=Bhat |author-first2=Carla Diane |author-last2=Savage |author-link2=Carla Diane Savage |title=Balanced Gray codes |journal=[[Electronic Journal of Combinatorics]] |date=1996 |volume=3 |issue=1 |pages=R25 |url=http://www.combinatorics.org/Volume_3/Abstracts/v3i1r25.html}}</ref> In '''balanced Gray codes''', the number of changes in different coordinate positions are as close as possible. To make this more precise, let ''G'' be an ''R''-ary complete Gray cycle having transition sequence <math>(\\delta_k)</math>; the ''transition counts'' (''spectrum'') of ''G'' are the collection of integers defined by\n\n:<math>\\lambda_k = |\\{ j \\in \\mathbb{Z}_{R^n} : \\delta_j = k \\}| \\, , \\text { for } k \\in \\mathbb{Z}_n</math>\n\nA Gray code is ''uniform'' or ''uniformly balanced'' if its transition counts are all equal, in which case we have <math>\\lambda_k = R^n / n</math>\nfor all ''k''. Clearly, when <math>R = 2</math>, such codes exist only if ''n'' is a power of 2. Otherwise, if ''n'' does not divide <math>R^n</math> evenly, it is possible to construct ''well-balanced'' codes where every transition count is either <math>\\lfloor R^n / n \\rfloor</math> or <math>\\lceil R^n / n \\rceil</math>. Gray codes can also be ''exponentially balanced'' if all of their transition counts are adjacent powers of two, and such codes exist for every power of two.<ref name=\"Suparta-EJoC\">{{cite journal |author-first=I. Nengah |author-last=Suparta |title=A simple proof for the existence of exponentially balanced Gray codes |journal=[[Electronic Journal of Combinatorics]] |date=2005 |volume=12}}</ref>\n\nFor example, a balanced 4-bit Gray code has 16 transitions, which can be evenly distributed among all four positions (four transitions per position), making it uniformly balanced:<ref name=\"balanced\" />\n 0 {{fontcolor|red|1}} 1 1 1 1 1 {{fontcolor|red|0}} 0 0 0 0 0 {{fontcolor|red|1}} 1 {{fontcolor|red|0}}\n 0 0 {{fontcolor|red|1}} 1 1 1 {{fontcolor|red|0}} 0 {{fontcolor|red|1}} 1 1 1 {{fontcolor|red|0}} 0 0 0\n 0 0 0 0 {{fontcolor|red|1}} 1 1 1 1 {{fontcolor|red|0}} 0 {{fontcolor|red|1}} 1 1 {{fontcolor|red|0}} 0\n {{fontcolor|red|0}} 0 0 {{fontcolor|red|1}} 1 {{fontcolor|red|0}} 0 0 0 0 {{fontcolor|red|1}} 1 1 1 1 1\n\nwhereas a balanced 5-bit Gray code has a total of 32 transitions, which cannot be evenly distributed among the positions. In this example, four positions have six transitions each, and one has eight:<ref name=\"balanced\" />\n {{fontcolor|red|1}} 1 1 1 1 {{fontcolor|red|0}} 0 0 0 {{fontcolor|red|1}} 1 1 1 1 1 {{fontcolor|red|0}} 0 {{fontcolor|red|1}} 1 1 1 1 {{fontcolor|red|0}} 0 0 0 0 0 0 0 0 0\n 0 0 0 {{fontcolor|red|1}} 1 1 1 1 1 1 1 {{fontcolor|red|0}} 0 0 0 0 0 0 {{fontcolor|red|1}} 1 1 1 1 1 {{fontcolor|red|0}} 0 0 {{fontcolor|red|1}} 1 {{fontcolor|red|0}} 0 0\n 1 1 {{fontcolor|red|0}} 0 {{fontcolor|red|1}} 1 1 {{fontcolor|red|0}} 0 0 0 0 0 {{fontcolor|red|1}} 1 1 {{fontcolor|red|0}} 0 0 {{fontcolor|red|1}} 1 1 1 1 1 {{fontcolor|red|0}} 0 0 0 0 {{fontcolor|red|1}} 1\n 1 {{fontcolor|red|0}} 0 0 0 0 0 0 {{fontcolor|red|1}} 1 1 1 1 1 {{fontcolor|red|0}} 0 0 0 0 0 {{fontcolor|red|1}} 1 1 1 1 1 1 1 {{fontcolor|red|0}} 0 0 {{fontcolor|red|1}}\n 1 1 1 1 1 1 {{fontcolor|red|0}} 0 0 0 {{fontcolor|red|1}} 1 {{fontcolor|red|0}} 0 0 0 0 0 0 0 0 {{fontcolor|red|1}} 1 {{fontcolor|red|0}} 0 0 {{fontcolor|red|1}} 1 1 1 1 1\n\nWe will now show a construction<ref name=\"balancing\">{{cite journal |author-first1=Mary Elizabeth |author-last1=Flahive |author-link1=Mary Elizabeth Flahive |author-first2=Bella |author-last2=Bose |title=Balancing cyclic R-ary Gray codes |journal=[[Electronic Journal of Combinatorics]] |date=2007 |volume=14}}</ref> and implementation<ref name=\"ariadne\">{{cite journal |author-first1=Raoul |author-last1=Strackx |author-first2=Frank |author-last2=Piessens |title=Ariadne: A Minimal Approach to State Continuity  |journal = Usenix Security |date=2016 |volume=25 |url=https://distrinet.cs.kuleuven.be/software/sce/ariadne.html}}</ref> for well-balanced binary Gray codes which allows us to generate an ''n''-digit balanced Gray code for every ''n''. The main principle is to inductively construct an (''n''&nbsp;+&nbsp;2)-digit Gray code <math>G'</math> given an ''n''-digit Gray code ''G'' in such a way that the balanced property is preserved. To do this, we consider partitions of <math>G = g_0, \\ldots, g_{2^n-1}</math> into an even number ''L'' of non-empty blocks of the form\n\n: <math>\\{g_0\\}, \\{g_1, \\ldots, g_{k_2}\\}, \\{g_{k_2+1}, \\ldots, g_{k_3}\\}, \\ldots, \\{g_{k_{L-2}+1}, \\ldots, g_{-2}\\}, \\{g_{-1}\\}</math>\n\nwhere <math>k_1 = 0, k_{L-1} = -2</math>, and <math>k_{L} = -1 \\pmod{2^n}</math>). This partition induces an <math>(n+2)</math>-digit Gray code given by\n\n:<math>00g_0,</math>\n:<math>00g_1, \\ldots, 00g_{k_2}, 01g_{k_2}, \\ldots, 01g_1, 11g_1, \\ldots, 11g_{k_2}, </math>\n:<math>11g_{k_2+1}, \\ldots, 11g_{k_3}, 01g_{k_3}, \\ldots, 01g_{k_2+1}, 00g_{k_2+1}, \\ldots, 00g_{k_3}, \\ldots,</math>\n:<math>00g_{-2}, 00g_{-1}, 10g_{-1}, 10g_{-2}, \\ldots, 10g_0, 11g_0, 11g_{-1}, 01g_{-1}, 01g_0</math>\n\nIf we define the ''transition multiplicities'' <math>m_i = |\\{ j : \\delta_{k_j} = i, 1 \\leq j \\leq L \\}|</math> to be the number of times the digit in position ''i'' changes between consecutive blocks in a partition, then for the (''n''&nbsp;+&nbsp;2)-digit Gray code induced by this partition the transition spectrum <math>\\lambda'_i</math> is\n\n: <math>\n\\lambda'_i = \\begin{cases}\n4 \\lambda_i - 2 m_i, & \\text{if } 0 \\leq i < n \\\\\nL, & \\text{ otherwise }\n\\end{cases}\n</math>\n\nThe delicate part of this construction is to find an adequate partitioning of a balanced ''n''-digit Gray code such that the code induced by it remains balanced, but for this only the transition multiplicities matter; joining two consecutive blocks over a digit <math>i</math> transition and splitting another block at another digit <math>i</math> transition produces a different Gray code with exactly the same transition spectrum <math>\\lambda'_i</math>, so one may for example<ref name=\"Suparta-EJoC\" /> designate the first <math>m_i</math> transitions at digit <math>i</math> as those that fall between two blocks. Uniform codes can be found when <math>R \\equiv 0 \\pmod 4</math> and <math>R^n \\equiv 0 \\pmod n</math>, and this construction can be extended to the ''R''-ary case as well.<ref name=\"balancing\" />\n\n=== Monotonic Gray codes ===\n\nMonotonic codes are useful in the theory of interconnection networks, especially for minimizing dilation for linear arrays of processors.<ref name=\"monotone\">{{cite journal |author-first1=Carla Diane |author-last1=Savage |author-link1=Carla Diane Savage |author-first2=Peter |author-last2=Winkler |author-link2=Peter Winkler |title=Monotone Gray codes and the middle levels problem |journal=[[Journal of Combinatorial Theory, Series A]] |date=1995 |volume=70 |issn=0097-3165 |pages=230–248 |issue=2 |doi=10.1016/0097-3165(95)90091-8}}</ref>\nClearly, we cannot have a Gray code flipping a single bit in each step with strictly increasing weight (recall that the weight is the number of 1s in the string), so we may want to approximate this by having the code run through two adjacent weights before reaching the next one.\n\nWe can formalize the concept of monotone Gray codes as follows: consider the partition of the hypercube <math>Q_n = (V_n, E_n)</math> into ''levels'' of vertices that have equal weight, i.e.\n\n: <math>V_n(i) = \\{ v \\in V_n : v \\text{ has weight } i \\}</math>\n\nfor <math>0 \\leq i \\leq n</math>. These levels satisfy <math>|V_n(i)| = \\binom{n}{i}</math>. Let <math>Q_n(i)</math> be the subgraph of <math>Q_n</math> induced by <math>V_n(i) \\cup V_n(i+1)</math>, and let <math>E_n(i)</math> be the edges in <math>Q_n(i)</math>. A monotonic Gray code is then a [[Hamiltonian path]] in <math>Q_n</math> such that whenever <math>\\delta_1 \\in E_n(i)</math> comes before <math>\\delta_2 \\in E_n(j)</math> in the path, then <math>i \\leq j</math>.\n\nAn elegant construction of monotonic ''n''-digit Gray codes for any ''n'' is based on the idea of recursively building subpaths <math>P_{n,j}</math> of length <math>2 \\binom{n}{j}</math> having edges in <math>E_n(j)</math>.<ref name=\"monotone\" /> We define <math>P_{1,0} = (0, 1)</math>, <math>P_{n,j} = \\emptyset</math> whenever <math>j < 0</math> or <math>j \\geq n</math>, and\n\n: <math>\nP_{n+1,j} = 1P^{\\pi_n}_{n,j-1}, 0P_{n,j}\n</math>\n\notherwise. Here, <math>\\pi_n</math> is a suitably defined permutation and <math>P^{\\pi}</math> refers to the path ''P'' with its coordinates permuted by <math>\\pi</math>. These paths give rise to two monotonic ''n''-digit Gray codes <math>G_n^{(1)}</math> and <math>G_n^{(2)}</math> given by\n\n: <math>\nG_n^{(1)} = P_{n,0} P_{n,1}^R P_{n,2} P_{n,3}^R \\cdots \\text{ and } G_n^{(2)} = P_{n,0}^R P_{n,1} P_{n,2}^R P_{n,3} \\cdots\n</math>\n\nThe choice of <math>\\pi_n</math> which ensures that these codes are indeed Gray codes turns out to be <math>\\pi_n = E^{-1}(\\pi_{n-1}^2)</math>. The first few values of <math>P_{n,j}</math> are shown in the table below.\n\n{| class=\"wikitable infobox\" style=\"text-align: center; width: 400px; height: 200px;\"\n|+ Subpaths in the Savage–Winkler algorithm\n|-\n! scope=\"col\" style=\"width:3em;\"| <math>P_{n,j}</math>\n! scope=\"col\" | ''j'' = 0\n! scope=\"col\" | ''j'' = 1\n! scope=\"col\" | ''j'' = 2\n! scope=\"col\" | ''j'' = 3\n|-\n! scope=\"row\" | ''n'' = 1\n| 0, 1 || || ||\n|-\n! scope=\"row\" | ''n'' = 2\n| 00, 01 || 10, 11 || ||\n|-\n! scope=\"row\" | ''n'' = 3\n| 000, 001 || 100, 110, 010, 011 || 101, 111 ||\n|-\n! scope=\"row\" | ''n'' = 4\n| 0000, 0001 || 1000, 1100, 0100, 0110, 0010, 0011 || 1010, 1011, 1001, 1101, 0101, 0111 || 1110, 1111\n|}\n\nThese monotonic Gray codes can be efficiently implemented in such a way that each subsequent element can be generated in <math>O(n)</math> time. The algorithm is most easily described using [[coroutine]]s.\n\n[[File:Length 7 middle-levels Gray code wheel.svg|thumb|Wheel representation of a middle-levels Gray code for <math>n=3</math> (binary strings of length 7 with weight 3 or 4).]]\nMonotonic codes have an interesting connection to a [[Lovász conjecture|conjecture of Lovász]], which asserts that every connected [[vertex-transitive graph]] contains a Hamiltonian path. The \"middle-level\" subgraph <math>Q_{2n+1}(n)</math> is vertex-transitive (that is, its automorphism group is transitive, so that each vertex has the same \"local environment\" and cannot be differentiated from the others, since we can relabel the coordinates as well as the binary digits to obtain an [[automorphism]]) and the problem of finding a Hamiltonian path in this subgraph is called the \"middle-levels problem\", which can provide insights into the more general conjecture.\nThe preceding construction for monotonic codes ensures a Hamiltonian path of length at least <math>0.839N</math> where <math>\\textstyle N=2\\binom{2n+1}{n}</math> is the number of vertices in the middle-level subgraph (see also <ref>{{cite journal |author-first=Carla Diane|author-last=Savage|author-link=Carla Diane Savage|title=Long cycles in the middle two levels of the Boolean lattice|journal=[[Ars Combinatoria (journal)|Ars Combinatoria]]|volume=35|pages=97–108|date=1993}}</ref>).\nThis construction was later improved by Johnson<ref name=\"Johnson_2004\">{{cite journal |author-first=J. Robert |author-last=Johnson |title=Long cycles in the middle two layers of the discrete cube |journal=[[Journal of Combinatorial Theory]] Series B|volume=105 |number=2 |pages=255–271|date=2004|doi=10.1016/j.jcta.2003.11.004}}</ref> to cycles of length <math>(1-o(1))N,</math>. The figure on the right shows a solution for <math>n=3</math>.\n\n=== Beckett–Gray code ===\n\nAnother type of Gray code, the '''Beckett–Gray code''', is named for Irish playwright [[Samuel Beckett]], who was interested in [[symmetry]]. His play \"[[Quad (play)|Quad]]\" features four actors and is divided into sixteen time periods. Each period ends with one of the four actors entering or leaving the stage. The play begins with an empty stage, and Beckett wanted each subset of actors to appear on stage exactly once.<ref name=\"Goddyn_1999\">{{cite web |title=MATH 343 Applied Discrete Math Supplementary Materials |author-last=Goddyn |author-first=Luis |date=1999 |publisher=Department of Mathematics, [[Simon Fraser University]] |url=http://www.math.sfu.ca/~goddyn/Courses/343/supMaterials.pdf |dead-url=yes |archive-url=https://web.archive.org/web/20150217160033/http://people.math.sfu.ca/~goddyn/Courses/343/supMaterials.pdf |archive-date=2015-02-17}}</ref> Clearly the set of actors currently on stage can be represented by a 4-bit binary Gray code. Beckett, however, placed an additional restriction on the script: he wished the actors to enter and exit so that the actor who had been on stage the longest would always be the one to exit. The actors could then be represented by a [[FIFO (computing and electronics)|first in, first out]] [[Queue (data structure)|queue]], so that (of the actors onstage) the actor being dequeued is always the one who was enqueued first.<ref name=\"Goddyn_1999\" /> Beckett was unable to find a Beckett–Gray code for his play, and indeed, an exhaustive listing of all possible sequences reveals that no such code exists for ''n'' = 4. It is known today that such codes do exist for ''n'' = 2, 5, 6, 7, and 8, and do not exist for ''n'' = 3 or 4. An example of an 8-bit Beckett–Gray code can be found in [[Donald Knuth]]'s ''Art of Computer Programming''.<ref name=\"Knuth\" /> According to Sawada and Wong, the search space for ''n'' = 6 can be explored in 15 hours, and more than 9,500 solutions for the case ''n'' = 7 have been found.<ref>{{cite journal |author-first1=Joseph \"Joe\" |author-last1=Sawada |author-first2=Dennis Chi-Him |author-last2=Wong |title=A Fast Algorithm to generate Beckett–Gray codes |journal=Electronic Notes in Discrete Mathematics |volume=29 |pages=571–577 |date=2007 |doi=10.1016/j.endm.2007.07.091}}</ref>\n\n=== Snake-in-the-box codes ===\n\n[[Snake-in-the-box]] codes, or ''snakes'', are the sequences of nodes of [[induced path]]s in an ''n''-dimensional [[hypercube graph]], and coil-in-the-box codes, or ''coils'', are the sequences of nodes of induced [[cycle (graph theory)|cycles]] in a hypercube. Viewed as Gray codes, these sequences have the property of being able to detect any single-bit coding error. Codes of this type were first described by [[William H. Kautz]] in the late 1950s;<ref>{{cite journal |author-last=Kautz |author-first=William H. |author-link=William H. Kautz |title=Unit-distance error-checking codes |journal=[[IRE Transactions on Electronic Computers]] |volume=7 |pages=177–180 |date=1958}}</ref> since then, there has been much research on finding the code with the largest possible number of codewords for a given hypercube dimension.\n\n=== {{anchor|STGC}}Single-track Gray code ===\n\nYet another kind of Gray code is the '''single-track Gray code''' (STGC) developed by Norman B. Spedding<ref name=\"Spedding_1994\">{{cite patent |inventor-last=Spedding |inventor-first=Norman Bruce<!-- Industrial Research Limited --> |pubdate=1994-10-28 |title=A position encoder |country=NZ |number=264738}}</ref>{{failed verification|date=July 2015}}<ref name=\"Spedding_2\">{{cite web |title=The following is a copy of the provisional patent filed on behalf of Industrial Research Limited on 28 October 1994 – NZ Patent 264738 |author-first=Norman Bruce |author-last=Spedding |url=http://www.winzurf.co.nz/Single_Track_Grey_Code_Patent/Single_track_Grey_code_encoder_patent.pdf |access-date=2018-01-14 |dead-url=no |archive-url=https://web.archive.org/web/20171029205005/http://www.winzurf.co.nz/Single_Track_Grey_Code_Patent/Single_track_Grey_code_encoder_patent.pdf |archive-date=2017-10-29}}</ref> and refined by Hiltgen, Paterson and Brandestini in \"Single-track Gray codes\" (1996).<ref name=\"HiltgenPatersonBrandestini_1996\">{{cite journal |title=Single-Track Gray Codes |author-last1=Hiltgen |author-first1=Alain P. |author-first2=Kenneth G. |author-last2=Paterson |author-first3=Marco |author-last3=Brandestini |journal=[[IEEE Transactions on Information Theory]] |volume=42 |issue=5 |date=September 1996 |pages=1555–1561 |doi=10.1109/18.532900 |zbl=857.94007 }}</ref><ref name=\"HiltgenPaterson_2001\">{{cite journal |title=Single-Track Circuit Codes |author-last1=Hiltgen |author-first1=Alain P. |author-first2=Kenneth G. |author-last2=Paterson |journal=[[IEEE Transactions on Information Theory]] |volume=47 |issue=6 |date=September 2001 |pages=2587–2595 |doi=10.1109/18.945274 |citeseerx=10.1.1.10.8218 |url=http://www.hpl.hp.com/techreports/2000/HPL-2000-81.pdf |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115013155/http://www.hpl.hp.com/techreports/2000/HPL-2000-81.pdf |archive-date=2018-01-15}} (NB. No mention of Spedding.)</ref> The STGC is a cyclical list of ''P'' unique binary encodings of length n such that two consecutive words differ in exactly one position, and when the list is examined as a ''P''&nbsp;×&nbsp;''n'' [[Matrix (mathematics)|matrix]], each column is a cyclic shift of the first column.<ref name=\"Etzion_1999\">{{cite journal |title=The Structure of Single-Track Gray Codes |author-last1=Etzion |author-first1=Tuvi |author-first2=Moshe |author-last2=Schwartz |journal=[[IEEE Transactions on Information Theory]] |volume=IT-45 |issue=7 |date=November 1999 |orig-year=1998-05-17 |pages=2383–2396 |doi=10.1109/18.796379 |citeseerx=10.1.1.14.8333 |url=http://etzion.net.technion.ac.il/files/2016/02/P54.pdf |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115022531/http://etzion.net.technion.ac.il/files/2016/02/P54.pdf |archive-date=2018-01-15}} [http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-info.cgi?1998/CS/CS0937<!-- Technical Report CS0937 https://web.archive.org/web/20180115023816/http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-info.cgi?1998/CS/CS0937 -->] [http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-info.cgi?1998/CS/CS0937<!-- https://web.archive.org/web/20180115023312/http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/1998/CS/CS0937.pdf -->]</ref>\n\n[[File:Enkelspoors-Graycode.svg|thumb|Single-track Gray code with 5 sensors.]]\n[[File:Animated Graycode.gif|thumb|Animated and color-coded version of the STGC rotor.]]\nThe name comes from their use with [[rotary encoder]]s, where a number of tracks are being sensed by contacts, resulting for each in an output of 0 or 1. To reduce noise due to different contacts not switching at exactly the same moment in time, one preferably sets up the tracks so that the data output by the contacts are in Gray code. To get high angular accuracy, one needs lots of contacts; in order to achieve at least 1 degree accuracy, one needs at least 360 distinct positions per revolution, which requires a minimum of 9 bits of data, and thus the same number of contacts.\n\nIf all contacts are placed at the same angular position, then 9 tracks are needed to get a standard BRGC with at least 1 degree accuracy. However, if the manufacturer moves a contact to a different angular position (but at the same distance from the center shaft), then the corresponding \"ring pattern\" needs to be rotated the same angle to give the same output. If the most significant bit (the inner ring in Figure 1) is rotated enough, it exactly matches the next ring out. Since both rings are then identical, the inner ring can be cut out, and the sensor for that ring moved to the remaining, identical ring (but offset at that angle from the other sensor on that ring). Those two sensors on a single ring make a quadrature encoder. That reduces the number of tracks for a \"1 degree resolution\" angular encoder to 8 tracks. Reducing the number of tracks still further can't be done with BRGC.\n\nFor many years, Torsten Sillke<ref>{{cite web |author-first=Torsten |author-last=Sillke |date=1997 |orig-year= 1993-03-01 |title=Gray-Codes with few tracks (a question of Marco Brandestini) |url=http://www.mathematik.uni-bielefeld.de/~sillke/PROBLEMS/gray |access-date=2017-10-29 |dead-url=no |archive-url=https://web.archive.org/web/20171029202303/https://www.math.uni-bielefeld.de/~sillke/PROBLEMS/gray |archive-date=2017-10-29}}</ref> and other mathematicians believed that it was impossible to encode position on a single track such that consecutive positions differed at only a single sensor, except for the 2-sensor, 1-track quadrature encoder. So for applications where 8 tracks were too bulky, people used single-track incremental encoders (quadrature encoders) or 2-track \"quadrature encoder + reference notch\" encoders.\n\nNorman B. Spedding, however, registered a patent in 1994 with several examples showing that it was possible.<ref name=\"Spedding_1994\" /> Although it is not possible to distinguish 2<sup>''n''</sup> positions with ''n'' sensors on a single track, it ''is'' possible to distinguish close to that many.  Etzion and Paterson conjecture that when ''n'' is itself a power of 2, ''n'' sensors can distinguish at most 2<sup>''n''</sup>&nbsp;−&nbsp;2''n'' positions and that for prime ''n'' the limit is 2<sup>''n''</sup>&nbsp;−&nbsp;2 positions.<ref name=\"EtzionPaterson_1996\">{{cite journal |title=Near Optimal Single-Track Gray Codes |author-first1=Tuvi |author-last1=Etzion |author-first2=Kenneth G. |author-last2=Paterson |journal=[[IEEE Transactions on Information Theory]] |volume=IT-42 |issue=3 |pages=779–789 |date=May 1996 |doi=10.1109/18.490544 |citeseerx=10.1.1.14.1527 |url=http://etzion.net.technion.ac.il/files/2016/02/P36.pdf |access-date=2018-04-08 |dead-url=no |archive-url=https://web.archive.org/web/20161030214251/http://etzion.net.technion.ac.il/files/2016/02/P36.pdf |archive-date=2016-10-30}}</ref> The authors went on to generate a 504 position single track code of length 9 which they believe is optimal. Since this number is larger than 2<sup>8</sup> = 256, more than 8 sensors are required by any code, although a BRGC could distinguish 512 positions with 9 sensors.\n\nAn STGC for ''P''&nbsp;=&nbsp;30 and ''n''&nbsp;=&nbsp;5 is reproduced here:\n{|class=\"wikitable\" style=\"text-align:center;\"\n|+ Single-track Gray code for 30 positions\n! Angle || Code\n|rowspan=\"7\"|\n! Angle || Code\n|rowspan=\"7\"|\n! Angle || Code\n|rowspan=\"7\"|\n! Angle || Code\n|rowspan=\"7\"|\n! Angle || Code\n|-\n|   0° || 10000 ||  72° || 01000 || 144° || 00100 || 216° || 00010 || 288° || 00001\n|-\n|  12° || 10100 ||  84° || 01010 || 156° || 00101 || 228° || 10010 || 300° || 01001\n|-\n|  24° || 11100 ||  96° || 01110 || 168° || 00111 || 240° || 10011 || 312° || 11001\n|-\n|  36° || 11110 || 108° || 01111 || 180° || 10111 || 252° || 11011 || 324° || 11101\n|-\n|  48° || 11010 || 120° || 01101 || 192° || 10110 || 264° || 01011 || 336° || 10101\n|-\n|  60° || 11000 || 132° || 01100 || 204° || 00110 || 276° || 00011 || 348° || 10001\n|}\n<!--\nBut even better would be an actual illustration of the single track, on a rotary encoder, with the 5 pick-up sensors.\nThis is done – does that mean that the 1s and 0s can go now? They take up a whole lot of visual space.\n-->\nEach column is a cyclic shift of the first column, and from any row to the next row only one bit changes.<ref>{{cite journal |title=A Survey of Venn Diagrams: Symmetric Diagrams |url=http://www.combinatorics.org/Surveys/ds5/VennSymmEJC.html |journal=[[Electronic Journal of Combinatorics]] |date=2005-06-18 |author-first1=Frank |author-last1=Ruskey |author-link1=Frank Ruskey |author-first2=Mark |author-last2=Weston}}</ref>\nThe single-track nature (like a code chain) is useful in the fabrication of these wheels (compared to BRGC), as only one track is needed, thus reducing their cost and size.\nThe Gray code nature is useful (compared to [[chain code]]s, also called [[De Bruijn sequence]]s), as only one sensor will change at any one time, so the uncertainty during a transition between two discrete states will only be plus or minus one unit of angular measurement the device is capable of resolving.<ref>{{cite book |author-last1=Alciatore |author-first1=David G. |author-first2=Michael B. |author-last2=Histand |title=Mechatronics |date=1999 |publisher=[[McGraw–Hill Education]] – Europe |isbn=978-0-07-131444-2 |url=http://mechatronics.colostate.edu/}}</ref>\n\n=== Two-dimensional Gray code ===\n\n[[File:16QAM Gray Coded.svg|200px|thumb|right|A Gray-coded constellation diagram for rectangular 16-[[Quadrature amplitude modulation|QAM]].]]\nTwo-dimensional Gray codes are used in communication to minimize the number of bit errors in [[quadrature amplitude modulation]] adjacent points in the [[Constellation diagram|constellation]]. In a typical encoding the horizontal and vertical adjacent constellation points differ by a single bit, and diagonal adjacent points differ by 2 bits.<ref>{{cite web |author=Krishna |title=Gray code for QAM |date=2008-05-11 |url=http://www.dsprelated.com/showthread/comp.dsp/96917-1.php |access-date=2017-10-29 |dead-url=no |archive-url=https://web.archive.org/web/20171029192539/https://www.dsprelated.com/showthread/comp.dsp/96917-1.php |archive-date=2017-10-29}}</ref>\n\n== Gray isometry ==\n\nThe bijective mapping { 0 ↔ 00, 1 ↔ 01, 2 ↔ 11, 3  ↔ 10 } establishes an [[isometry]] between the [[metric space]] over the [[finite field]] <math>\\mathbb{Z}_2^2</math> with the metric given by the [[Hamming distance]] and the metric space over the [[finite ring]] <math>\\mathbb{Z}_4</math> (the usual [[modulo arithmetic]]) with the metric given by the [[Lee distance]]. The mapping is suitably extended to an isometry of the [[Hamming space]]s <math>\\mathbb{Z}_2^{2m}</math> and <math>\\mathbb{Z}_4^m</math>. Its importance lies in establishing a correspondence between various \"good\" but not necessarily [[linear code]]s as Gray-map images in <math>\\mathbb{Z}_2^2</math> of [[linear code#Generalization|ring-linear codes]] from <math>\\mathbb{Z}_4</math>.<ref name=\"Greferath2009\">{{cite book |editor-first1=Massimiliano |editor-last1=Sala |editor-first2=Teo |editor-last2=Mora |editor-first3=Ludovic |editor-last3=Perret |editor-first4=Shojiro |editor-last4=Sakata |editor-first5=Carlo |editor-last5=Traverso |title=Gröbner Bases, Coding, and Cryptography |date=2009 |publisher=[[Springer Science & Business Media]] |isbn=978-3-540-93806-4 |chapter=An Introduction to Ring-Linear Coding Theory |author-first=Marcus |author-last=Greferath |page=220}}</ref><ref>{{cite book |title=Kerdock and Preparata codes |author-first=Patrick |author-last=Solé |work=[[Encyclopedia of Mathematics]] |editor-first=Michiel |editor-last=Hazewinkel |editor-link=Michiel Hazewinkel |publisher=[[Springer Science+Business Media]] |date=2016-04-17 |isbn=978-1-4020-0609-8 |url=https://www.encyclopediaofmath.org/index.php/Kerdock_and_Preparata_codes |access-date=2017-10-29 |dead-url=no |archive-url=https://web.archive.org/web/20171029191032/https://www.encyclopediaofmath.org/index.php/Kerdock_and_Preparata_codes |archive-date=2017-10-29}}</ref>\n\n== {{anchor|MRB|Lucal|Datex|Varec|Gillham|Hoklas|Gray-Excess|Glixon|O'Brien|Petherick|Tompkins|Kautz}}Related codes ==\n\nThere are a number of binary codes similar to Gray codes, including:<!-- to be expanded at a later stage -->\n* Lucal code<ref name=\"Sellers-Hsiao-Bearnson_1968\">{{cite book |author-first1=Frederick F. |author-last1=Sellers, Jr. |author-first2=Mu-Yue |author-last2=Hsiao |author-first3=Leroy W. |author-last3=Bearnson |title=Error Detecting Logic for Digital Computers |publisher=[[McGraw-Hill Book Company]] |location=New York, USA |edition=1st |oclc=439460 |lccn=68-16491 |pages=152–164 |date=November 1968}}</ref><ref name=\"Richards_1955\">{{cite book |author-first=Richard Kohler |author-last=Richards |title=Arithmetic Operations in Digital Computers |publisher=[[D. Van Nostrand Co., Inc.]] |location=New York, USA |edition=5 |date=1955 |url=https://books.google.com/books?id=BI5QAAAAMAAJ}}</ref><ref name=\"Lucal_1959\">{{cite journal |author-first=Harold M. |author-last=Lucal |title=Arithmetic Operations for Digital Computers Using a Modified Reflected Binary |journal=[[IEEE Transactions on Electronic Computers]]<!-- IRE Transactions on Electronic Computers? --> |volume=EC-8 |number=4 |pages=444–458 |date=December 1959 |issn=0367-9950 |doi=10.1109/TEC.1959.5222057 }}</ref> aka ''Modified reflected binary code'' (MRB)<ref name=\"Lucal_1959\" /><ref name=\"Sellers-Hsiao-Bearnson_1968\" />\n* Varec code\n* Datex code (aka Giannini code)<ref name=\"Datex_1965\">{{cite web |title=US Patent: Digital coding and translating system |author-first=Carl P. |author-last=Spaulding |publisher=Datex Corp |date=1965-01-12 |orig-year=1954-03-09 |id=Patent US3165731A |url=https://www.google.com/patents/US3165731 |access-date=2018-01-21 |dead-url=no |archive-url=https://web.archive.org/web/20180121204752/https://www.google.com/patents/US3165731 |archive-date=2018-01-21}}</ref>\n* [[Gillham code]]\n* Hoklas code<ref name=\"Hoklas_1989\">{{cite web |title=DDR-Wirtschaftspatent DD 271 603 A1: Abtastvorrichtung zur digitalen Weg- oder Winkelmessung |language=de |author-first=Archibald |author-last=Hoklas |location={{ill|VEB Schiffselektronik Johannes Warnke|de}}, GDR |publisher={{ill|DEPATIS|de}} |date=1989-09-06<!-- published --> |orig-year=1988-04-29<!-- filed --> |id=WP H 03 M / 315 194 8 |url=https://depatisnet.dpma.de/DepatisNet/depatisnet/DD000000271603A1_all_pages.pdf?window=1&space=menu&content=download_doc_verify&action=download_doc&docid=DD000000271603A1&so=asc&sf=vn&firstdoc=0&struct=&Cl=2&Bi=1&Ab=1&De=2&Dr=5&Pts=&Pa=&We=&Sr=&Eam=&Cor=&Aa=&NrFaxPages=5&pdfpage=2&pdfmatrix=1x1&origin=pdf_window&verify_str=null |access-date=2018-01-18 |dead-url=no |archive-url=https://web.archive.org/web/20180118015239/https://depatisnet.dpma.de/DepatisNet/depatisnet/DD000000271603A1_all_pages.pdf?window=1&space=menu&content=download_doc_verify&action=download_doc&docid=DD000000271603A1&so=asc&sf=vn&firstdoc=0&struct=&Cl=2&Bi=1&Ab=1&De=2&Dr=5&Pts=&Pa=&We=&Sr=&Eam=&Cor=&Aa=&NrFaxPages=5&pdfpage=2&pdfmatrix=1x1&origin=pdf_window&verify_str=null |archive-date=2018-01-18}} [https://depatisnet.dpma.de/DepatisNet/depatisnet?window=1&space=menu&content=treffer&action=bibdat&docid=DD000000271603A1] [https://depatisnet.dpma.de/DepatisNet/depatisnet?window=1&space=menu&content=treffer&action=pdf&docid=DD000000271603A1&Cl=2&Bi=1&Ab=1&De=2&Dr=5&Pts=&Pa=&We=&Sr=&Eam=&Cor=&Aa=&so=asc&sf=vn&firstdoc=0&NrFaxPages=5&pdfpage=2&xxxfull=1]</ref><ref name=\"Hoklas_2005_EN\">{{cite web |title=Gray code – Unit distance code |author-first=Archibald |author-last=Hoklas |date=2005 |url=http://www.ahok.de/en/hoklas-code.html |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115012854/http://www.ahok.de/en/hoklas-code.html |archive-date=2018-01-15}}</ref><ref name=\"Hoklas_2005_DE\">{{cite web |title=Gray-Kode – Einschrittiger Abtastkode |language=de |author-first=Archibald |author-last=Hoklas |date=2005 |url=http://www.ahok.de/dt/hoklas-code.html |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115012827/http://www.ahok.de/dt/hoklas-code.html |archive-date=2018-01-15}}</ref>\n\nThe following [[binary-coded decimal]] (BCD) codes are Gray code variants as well:\n* Gray-Excess code (aka Gray-[[Excess-3]] code, Gray-3-Excess code, Reflex-Excess-3 code, Excess-Gray code,<ref name=\"Hoklas_2005_EN\" /> 10-Excess-3-Gray code or Gray-Stibitz code)\n* Glixon code<ref name=\"Glixon_1957\">{{cite journal|date=March 1957|title=Can You Take Advantage of the Cyclic Binary-Decimal Code?|journal=Control Engineering|volume=4|pages=87–91|author-first=Harry Robert|author-last=Glixon|number=3}}<!-- https://web.archive.org/web/20180115014809/https://donmooreswartales.com/2010/05/12/harry-glixon/ --></ref><ref name=\"Steinbuch_1962\">{{cite book |title=Taschenbuch der Nachrichtenverarbeitung |language=de |editor-first=Karl W. |editor-last=Steinbuch |editor-link=Karl W. Steinbuch |date=1962 |edition=1 |publisher=[[Springer-Verlag OHG]] |location=Karlsruhe, Germany |publication-place=Berlin / Göttingen / New York |lccn=62-14511 |pages=71–74, 97, 761–764, 770, 1080–1081}}</ref><ref name=\"Steinbuch-Weber_1974\">{{cite book |title=Taschenbuch der Informatik – Band II – Struktur und Programmierung von EDV-Systemen |language=de |editor-first1=Karl W. |editor-last1=Steinbuch |editor-link1=Karl W. Steinbuch |editor-first2=Wolfgang |editor-last2=Weber |editor-first3=Traute |editor-last3=Heinemann |date=1974 |orig-year=1967 |edition=3 |volume=2 |work=Taschenbuch der Nachrichtenverarbeitung |publisher=[[Springer Verlag]] |location=Berlin, Germany |isbn=3-540-06241-6 |lccn=73-80607 |pages=98–100}}</ref><ref name=\"Hoklas_2005_EN\" />\n* O'Brien codes<!-- two types --><ref name=\"O'Brien_1956\">{{cite journal |author-first=Joseph A. |author-last=O'Brien |title=Cyclic Decimal Codes for Analogue to Digital Converters |journal=[[Transactions of the American Institute of Electrical Engineers, Part I: Communication and Electronics]] |volume=75 |issue=2 |date=May 1956 |pages=120–122 |issn=0097-2452 |doi=10.1109/TCE.1956.6372498 }}</ref><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" /><ref name=\"Hoklas_2005_EN\" />\n* Petherick code<ref name=\"Petherick_1953\">{{cite journal |author-first=Edward J. |author-last=Petherick |title=A Cyclic Progressive Binary-coded-decimal System of Representing Numbers |date=1953 |type=Technical Note MS15 |publisher=[[Royal Aircraft Establishment]] (RAE) |location=Farnborough, UK}}</ref><ref name=\"Charnley_1965\">{{cite journal |title=The Design of a Pneumatic Position Encoder |author-first1=C. J. |author-last1=Charnley |author-first2=R. E. |author-last2=Bidgood |author-first3=G. E. T. |author-last3=Boardman |journal=IFAC Proceedings Volumes |publisher=The College of Aeronautics, Cranfield, Bedford, England |volume=2 |issue=3 |date=October 1965 |pages=75–88 |id=Chapter 1.5. |doi=10.1016/S1474-6670(17)68955-9 }}</ref><ref name=\"Hoklas_2005_EN\" />\n* Tompkins codes<ref name=\"Tompkins_1956\">{{cite journal |author-first=Howard E. |author-last=Tompkins |title=Unit-Distance Binary-Decimal Codes for Two-Track Commutation |date=September 1956 |journal=[[IRE Transactions on Electronic Computers]] |issn=0367-9950 |volume=EC-5 |issue=3 |page=139 |doi=10.1109/TEC.1956.5219934 }}</ref><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" /><ref name=\"Hoklas_2005_EN\" />\n* Kautz code<ref name=\"Kautz_1954\">{{cite journal |author-first=William H. |author-last=Kautz |author-link=William H. Kautz |title=Optimized Data Encoding for Digital Computers |date=1954 |journal=Convention Record IRE |issue=part 4 |pages=47–57}}</ref><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />\n{|\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ Decimal\n! &nbsp;\n|-\n| 0\n|-\n| 1\n|-\n| 2\n|-\n| 3\n|-\n| 4\n|-\n| 5\n|-\n| 6\n|-\n| 7\n|-\n| 8\n|-\n| 9\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ Gray\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || 0 || 0 || 0\n|-\n| 0 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ Glixon\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || 0 || 0 || 0\n|-\n| 0 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || 0\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ O'Brien&nbsp;I\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || 0 || 0 || 0\n|-\n| 0 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || 0\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ O'Brien&nbsp;II\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || style=\"background:#0FF\"|1\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ Petherick\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ Tompkins&nbsp;I\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || 0 || 0 || 0\n|-\n| 0 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || 0\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ Tompkins&nbsp;II\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || 0\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ {{nowrap|Gray-Excess}}\n! 4 !! 3 !! 2 !! 1\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || 0\n|}\n<!--\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|+ Kautz\n! 4 !! 3 !! 2 !! 1\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || 0 || 0\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| 0 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || 0\n|-\n| style=\"background:#0FF\"|1 || 0 || style=\"background:#0FF\"|1 || style=\"background:#0FF\"|1\n|-\n| style=\"background:#0FF\"|1 || 0 || 0 || 0\n|-\n| 0 || 0 || 0 || style=\"background:#0FF\"|1\n|}\n-->\n|-\n| colspan=\"9\" style=\"text-align:left\" | References for charts: {{nowrap|Gray,<ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />}} {{nowrap|Glixon,<ref name=\"Glixon_1957\" /><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />}} {{nowrap|O'Brien&nbsp;I,<ref name=\"O'Brien_1956\" /><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />}} {{nowrap|O'Brien&nbsp;II,<ref name=\"O'Brien_1956\" /><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />}} {{nowrap|Petherick,<ref name=\"Petherick_1953\" /><ref name=\"Charnley_1965\" />}} {{nowrap|Tompkins I,<ref name=\"Tompkins_1956\" /><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />}} {{nowrap|Tompkins II,<ref name=\"Tompkins_1956\" /><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />}} {{nowrap|Gray-Excess<ref name=\"Hoklas_2005_EN\" />}}<!-- {{nowrap|Kautz<ref name=\"Kautz_1954\" /><ref name=\"Steinbuch_1962\" /><ref name=\"Steinbuch-Weber_1974\" />}}-->\n|}\n\n== Generalizations ==\n\nThe term ''combinatorial Gray code'' refers more generally to any listing of combinatorial objects with the property that any two consecutive objects differ in a specified, usually small, way.<ref name=\"Savage_1997\">{{cite journal |author-last=Savage |author-first=Carla Diane |author-link=Carla Diane Savage |title=A Survey of Combinatorial Gray Codes |journal=[[SIAM Review]] |publisher=[[Society for Industrial and Applied Mathematics]] (SIAM) |volume=39 |issue=4 |pages=605–629 |date=1997 |doi=10.1137/S0036144595295272 |url=http://www.csc.ncsu.edu/faculty/savage/AVAILABLE_FOR_MAILING/survey.ps |jstor=2132693 |citeseerx=10.1.1.39.1924|bibcode=1997SIAMR..39..605S }}</ref>\nFor instance, the classical reflected binary code described before is a listing of binary strings of a certain length such that any two consecutive strings differ only in a single bit.\nThe [[Steinhaus-Johnson-Trotter algorithm]] generates a listing of all permutations of a certain length such that any two consecutive permutations differ only in an adjacent transposition.\nAnother classical combinatorial Gray code is a listing of all binary trees on a certain number of nodes such that any two consecutive trees differ only in a tree rotation.<ref name=\"Lucas_et_al_1993\">{{cite journal |author-last1=Lucas |author-first1=Joan M.|author-last2=Roelants van Baronaigien|author-first2=D.|author-last3=Ruskey|author-first3=Frank|author-link3=Frank Ruskey|title=On rotations and the generation of binary trees |journal=Journal of Algorithms |publisher=[[Elsevier]] |volume=15 |number=3 |pages=343–366 |date=1993 |doi=10.1006/jagm.1993.1045 |citeseerx=10.1.1.51.8866}}</ref>\nAs a last example, it is possible to list of [[spanning tree]]s of a graph such that any two consecutive trees differ only in exchanging a single edge.<ref name=\"Holzmann_Harary_1972\">{{cite journal |author-last1=Holzmann |author-first1=Carlos A.|author-last2=Harary|author-first2=Frank|author-link2=Frank Harary|title=On the tree graph of a matroid |journal=[[SIAM Journal on Applied Mathematics]] |publisher=[[Society for Industrial and Applied Mathematics]] (SIAM) |volume=22 |issue=2|pages=187–193 |date=1972 |doi=10.1137/0122021 |jstor=2099712}}</ref>\nAll of these algorithms are described in Knuth's book<ref name=\"Knuth_TAOCP4A_2011\">{{cite book |author-last=Knuth |author-first=Donald E.|author-link=Donald Knuth|title=The Art of Computer Programming. Vol. 4A. Combinatorial algorithms. Part 1| publisher=[[Addison-Wesley]]|title-link=The Art of Computer Programming}}</ref>.\n\nAny such combinatorial Gray code problem can be rephrased as a [[Hamiltonian path]] problem in a suitably defined graph, by taking the combinatorial objects as vertices, and connecting any two that differ in the specified way by an edge. The resulting graph is called a [[flip graph]]. Clearly, the question for a Gray code listing of the objects is equivalent to asking for a Hamiltonian path in the flip graph. For the first three combinatorial Gray codes mentioned before, these graphs are the [[hypercube graph]] (binary strings), the [[permutohedron]] (permutations), and the [[associahedron]] (binary trees).\n\n== See also ==\n\n* [[Linear feedback shift register]]\n* [[De Bruijn sequence]]\n* [[Steinhaus–Johnson–Trotter algorithm]], an algorithm that generates Gray codes for the [[factorial number system]]\n\n== References ==\n\n{{Reflist}}\n\n== Further reading ==\n\n* {{cite web |author-last=Black |author-first=Paul E. |title=Gray code |date=2004-02-25 |publisher=[[NIST]] |url=https://xlinux.nist.gov/dads/HTML/graycode.html}}\n* {{cite book |author-last1=Press |author-first1=William H. |author-last2=Teukolsky |author-first2=Saul A. |author-last3=Vetterling |author-first3=William T. |author-last4=Flannery |author-first4=Brian P. |date=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3rd |publisher=[[Cambridge University Press]] |location=New York, USA |isbn=978-0-521-88068-8 |chapter=Section 22.3. Gray Codes |chapter-url=http://apps.nrbook.com/empanel/index.html#pg=1166}}\n* {{cite book |author-last=Wilf |author-first=Herbert Saul |author-link=Herbert Saul Wilf |chapter=Chapters 1–3 |title=Combinatorial algorithms: An update |publisher=[[Society for Industrial and Applied Mathematics]] (SIAM) |date=1989 |isbn=0-89871-231-9 }}\n* {{cite book |author-first1=Megan |author-last1=Dewar |author-first2=Brett |author-last2=Stevens |title=Ordering Block Designs – Gray Codes, Universal Cycles and Configuration |publisher=[[Springer Science+Business Media]] |location=New York, USA |edition=1 |date=2012-08-29 |isbn=978-1-4614-4324-7 |series=CMS Books in Mathematics |issn=1613-5237 |doi=10.1007/978-1-4614-4325-4}}\n* {{cite web |title=Gray Code Fundamentals |work=Design How-To |at=Part 1 |author-first=Clive \"Max\" |author-last=Maxfield |date=2012-10-01 |orig-year=2011-05-28 |publisher=[[EETimes]] |url=https://www.eetimes.com/document.asp?doc_id=1278809 |access-date=2017-10-30 |dead-url=no |archive-url=https://web.archive.org/web/20171030135842/https://www.eetimes.com/document.asp?doc_id=1278809 |archive-date=2017-10-30}} [https://www.eetimes.com/document.asp?doc_id=1278827<!-- https://web.archive.org/web/20171030140209/https://www.eetimes.com/document.asp?doc_id=1278827 --> Part 2] [https://www.eetimes.com/document.asp?doc_id=1278853<!-- https://web.archive.org/web/20171030140323/https://www.eetimes.com/document.asp?doc_id=1278853 --> Part 3]\n* {{Cite book |title=Hacker's Delight |title-link=Hacker's Delight |author-first=Henry S. |author-last=Warren Jr. |date=2013 |edition=2 |publisher=[[Addison Wesley]] – [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8 |pages=311–317}}\n* {{cite journal |title=Computing Binary Combinatorial Gray Codes Via Exhaustive Search With SAT Solvers |author-last1=Zinovik |author-first1=Igor |author-last2=Kroening |author-first2=Daniel |author-last3=Chebiryak |author-first3=Yury |journal=[[IEEE Transactions on Information Theory]] |publisher=[[IEEE]] |volume=54 |issue=4 |date=2008-03-21 |pages=1819–1823 |doi=10.1109/TIT.2008.917695 |hdl=20.500.11850/11304 }}\n\n== External links ==\n\n* [http://demonstrations.wolfram.com/BinaryGrayCode/ \"Gray Code\" demonstration] by Michael Schreiber, [[Wolfram Demonstrations Project]] (with Mathematica implementation). 2007.\n* [https://xlinux.nist.gov/dads/HTML/graycode.html NIST Dictionary of Algorithms and Data Structures: Gray code].\n* [http://www.aip.de/~ast/EvolCompFAQ/Q21.htm Hitch Hiker's Guide to Evolutionary Computation, Q21: What are Gray codes, and why are they used?], including [[C (programming language)|C]] code to convert between binary and BRGC.\n* Dragos A. Harabor uses [https://web.archive.org/web/20151122120754/http://www.ugcs.caltech.edu/~dragos/3DP/coord.html Gray codes in a 3D digitizer].\n* Single-track gray codes, binary [[chain code]]s ([http://tinaja.com/text/chain01.html Lancaster 1994]), and [[linear feedback shift register]]s are all useful in finding one's absolute position on a single-track rotary encoder (or other position sensor).\n* [http://www.ams.org/featurecolumn/archive/gray.html AMS Column: Gray codes]\n* [http://www.bushytails.net/~randyg/encoder/encoderwheel.html Optical Encoder Wheel Generator]\n* [https://web.archive.org/web/20110724021700/http://prototalk.net/forums/showthread.php?t=78 ProtoTalk.net – Understanding Quadrature Encoding] – Covers quadrature encoding in more detail with a focus on robotic applications\n\n[[Category:Data transmission]]\n[[Category:Numeral systems]]\n[[Category:Binary arithmetic]]\n[[Category:Non-standard positional numeral systems]]"
    },
    {
      "title": "Half-carry flag",
      "url": "https://en.wikipedia.org/wiki/Half-carry_flag",
      "text": "{{primary sources|date=December 2018}}\nA '''half-carry flag''' (also known as an '''auxiliary flag''' or '''decimal adjust flag''') is a condition flag bit in the [[status register]] of many [[CPU]] families, such as the [[Intel 8080]], [[Zilog Z80]], the [[x86]],<ref name=\"intel\">{{cite web |url=http://download.intel.com/design/PentiumII/manuals/24319102.PDF |format=PDF |title=Intel Architecture Software Developer's Manual, Volume 2: Instruction Set Reference Manual |accessdate=2013-05-29}}</ref> and the [[Atmel AVR]] series, among others. It indicates when a [[carry (arithmetic)|carry]] or borrow has been generated out of the least significant four bits of the [[accumulator (computing)|accumulator]] register following the execution of an [[arithmetic logic unit|arithmetic]] instruction. It is primarily used in decimal ([[Binary-coded decimal|BCD]]) arithmetic instructions.\n\n== Usage ==\n\nNormally, a processor that utilizes [[binary arithmetic]] (which includes almost all modern CPUs) will add two 8-bit byte values according to the rules of simple binary addition. For example, adding 25{{sub|16}} and 48{{sub|16}} produces 6D{{sub|16}}. However, for [[binary-coded decimal]] (BCD) values, where each 4-bit nibble represents a decimal digit, addition is more complicated. For example, adding the decimal value 25 and 48, which are encoded as the BCD values 25{{sub|16}} and 48{{sub|16}}, the binary addition of the two values produces 6D{{sub|16}}. Since the lower nibble of this value is a non-decimal digit (D), it must be adjusted by adding 06{{sub|16}} to produce the correct BCD result of 73{{sub|16}}, which represents the decimal value 73.\n\n   0010 0101   25\n + 0100 1000   48\n -----------\n '''  0110 1101   6D''', ''intermediate result''\n +      0110   06, ''adjustment''\n -----------\n '''  0111 0011   73''', ''adjusted result''\n\nLikewise, adding the BCD values 39{{sub|16}} and 48{{sub|16}} produces 81{{sub|16}}. This result does not have a non-decimal low nibble, but it does cause a carry out of the least significant digit (lower four bits) into the most significant digit (upper four bits). This is indicated by the CPU setting the half-carry flag. This value must also be corrected, by adding 06{{sub|16}} to 81{{sub|16}} to produce a corrected BCD result of 87{{sub|16}}.\n\n   0011 1001   39\n + 0100 1000   48\n -----------\n '''  1000 0001   81''', ''intermediate result''\n +      0110   06, ''adjustment''\n -----------\n '''  1000 0111   87''', ''adjusted result''\n\nFinally, if an addition results in a non-decimal high digit, then 60{{sub|16}} must be added to the value to produce the correct BCD result. For example, adding 72{{sub|16}} and 73{{sub|16}} produces E5{{sub|16}}. Since the most significant digit of this sum is non-decimal (E), adding 60{{sub|16}} to it produces a corrected BCD result of 145{{sub|16}}. (Note that the leading 1 digit is actually a [[carry flag|carry bit]].)\n\n   0111 0010   72\n + 0111 0011   73\n -----------\n '''  1110 0101   E5''', ''intermediate result''\n + 0110        60, ''adjustment''\n -----------\n '''1 0100 0101  145''', ''adjusted result''\n\nSummarizing, if the result of a binary addition contains a non-decimal low digit or causes the half-carry flag to be set, the result must be corrected by adding 06{{sub|16}} to it; if the result contains a non-decimal high digit, the result must be further corrected by adding 60{{sub|16}} to produce the correct final BCD value.\n\n== See also ==\n* [[Carry flag]]\n\n== References ==\n\n{{reflist}}\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]"
    },
    {
      "title": "Half-precision floating-point format",
      "url": "https://en.wikipedia.org/wiki/Half-precision_floating-point_format",
      "text": "In [[computing]], '''half precision''' is a [[binary (computing)|binary]] [[floating-point]] [[computer number format]] that occupies [[16 bit]]s (two bytes in modern computers) in [[computer memory]].\n\nIn the [[IEEE 754-2008]] standard, the 16-bit [[radix|base-2]] format is referred to as '''binary16'''. It is intended for storage of floating-point values in applications where higher precision is not essential for performing arithmetic computations.\n\nAlthough implementations of the IEEE Half-precision floating point are relatively new, several earlier 16-bit floating point formats have existed including that of Hitachi's HD61810 DSP<ref>{{cite web|url=https://archive.org/details/bitsavers_hitachidatlSignalProcessorUsersManual_4735688 |title=hitachi :: dataBooks :: HD61810 Digital Signal Processor Users Manual |website=Archive.org |date= |accessdate=2017-07-14}}</ref> of 1982, Scott's WIF<ref>{{cite journal|last1=Scott|first1=Thomas J.|title=Mathematics and Computer Science at Odds over Real Numbers|journal=SIGCSE '91 Proceedings of the twenty-second SIGCSE technical symposium on Computer science education|date=March 1991|volume=23|issue=1|pages=130–139|url=https://dl.acm.org/citation.cfm?id=107029}}</ref> and the [[3dfx Interactive|3dfx Voodoo Graphics processor]].<ref>{{cite web|url=http://www.gamers.org/dEngine/xf3D/glide/glidepgm.htm |title=/home/usr/bk/glide/docs2.3.1/GLIDEPGM.DOC |website=Gamers.org |date= |accessdate=2017-07-14}}</ref>\n\n[[Nvidia]] and [[Microsoft]] defined the '''half''' [[datatype]] in the [[Cg (programming language)|Cg language]], released in early 2002, and implemented it in silicon in the [[GeForce FX]], released in late 2002.<ref>{{cite web|title=vs_2_sw|url=https://developer.download.nvidia.com/cg/vs_2_sw.html|website=Cg 3.1 Toolkit Documentation|publisher=Nvidia|accessdate=17 August 2016}}</ref> [[Industrial Light & Magic|ILM]] was searching for an image format that could handle a wide [[dynamic range]], but without the hard drive and memory cost of floating-point representations that are commonly used for floating-point computation (single and double precision).<ref name=\"exr\">{{cite web|url=http://www.openexr.com/about.html |title=OpenEXR |publisher=OpenEXR |date= |accessdate=2017-07-14}}</ref> The hardware-accelerated programmable shading group led by John Airey at [[Silicon Graphics|SGI (Silicon Graphics)]] invented the s10e5 data type in 1997 as part of the 'bali' design effort. This is described in a [[SIGGRAPH]] 2000 paper<ref name=\"sgi\">{{cite web|url=https://people.csail.mit.edu/ericchan/bib/pdf/p425-peercy.pdf |format=PDF |title=Interactive Multi-Pass Programmable Shading |author1=Mark S. Peercy |author2=Marc Olano |author3=John Airey |author4=P. Jeffrey Ungar |website=People.csail.mit.edu |accessdate=2017-07-14}}</ref> (see section 4.3) and further documented in US patent 7518615.<ref name=\"patent\">{{cite web|url=https://www.google.com/patents/US7518615 |title=Patent US7518615 - Display system having floating point rasterization and floating point ... - Google Patents |website=Google.com |date= |accessdate=2017-07-14}}</ref>\n\nThis format is used in several [[computer graphics]] environments including [[OpenEXR]], [[JPEG XR]], [[GIMP]], [[OpenGL]], [[Cg (programming language)|Cg]], [[Direct3D]], and [[D3DX]].  The advantage over 8-bit or 16-bit binary integers is that the increased [[dynamic range]] allows for more detail to be preserved in highlights and [[shadow]]s for images.  The advantage over 32-bit [[single-precision]] binary formats is that it requires half the storage and [[bandwidth (computers)|bandwidth]] (at the expense of precision and range).<ref name=\"exr\"/>\n\nThe [[F16C]] extension allows x86 processors to convert half-precision floats to and from [[Single-precision floating-point format|single-precision floats]].\n\n{{Floating-point}}\n\n== IEEE 754 half-precision binary floating-point format: binary16 ==\n<!-- \"significand\", with a d at the end, is a technical term, please do not confuse with \"significant\" -->\n\nThe IEEE 754 standard specifies a '''binary16''' as having the following format:\n* [[Sign bit]]: 1 bit\n* [[Exponent]] width: 5 bits\n* [[Significand]] [[precision (arithmetic)|precision]]: 11 bits (10 explicitly stored)\n\nThe format is laid out as follows:\n\n[[File:IEEE 754r Half Floating Point Format.svg]]\n\nThe format is assumed to have an implicit lead bit with value 1 unless the exponent field is stored with all zeros. Thus only 10 bits of the [[significand]] appear in the memory format but the total precision is 11 bits. In IEEE 754 parlance, there are 10 bits of significand, but there are 11 bits of significand precision (log<sub>10</sub>(2<sup>11</sup>) ≈ 3.311 decimal digits, or 4 digits ± slightly less than 5 [[unit in the last place|units in the last place]]).\n\n=== Exponent encoding ===\nThe half-precision binary floating-point exponent is encoded using an [[offset-binary]] representation, with the zero offset being 15; also known as exponent bias in the IEEE 754 standard.\n\n* E<sub>min</sub> = 00001<sub>2</sub> − 01111<sub>2</sub> = −14\n* E<sub>max</sub> = 11110<sub>2</sub> − 01111<sub>2</sub> = 15\n* [[Exponent bias]] = 01111<sub>2</sub> = 15\n\nThus, as defined by the offset binary representation, in order to get the true exponent the offset of 15 has to be subtracted from the stored exponent.\n\nThe stored exponents 00000<sub>2</sub> and 11111<sub>2</sub> are interpreted specially.\n\n{|class=\"wikitable\" style=\"text-align:center\"\n|-\n! Exponent !! Significand = zero !! Significand ≠ zero !! Equation\n|-\n| 00000<sub>2</sub> || [[0 (number)|zero]], [[−0]] || [[subnormal numbers]] || (−1)<sup>signbit</sup> × 2<sup>−14</sup> × 0.significantbits<sub>2</sub>\n|-\n| 00001<sub>2</sub>, ..., 11110<sub>2</sub> ||colspan=2| normalized value || (−1)<sup>signbit</sup> × 2<sup>exponent−15</sup> × 1.significantbits<sub>2</sub>\n|-\n| 11111<sub>2</sub> || ±[[infinity]] || [[NaN]] (quiet, signalling) ||\n|}\n\nThe minimum strictly positive (subnormal) value is\n2<sup>−24</sup> ≈ 5.96 × 10<sup>−8</sup>.\nThe minimum positive normal value is 2<sup>−14</sup> ≈ 6.10 × 10<sup>−5</sup>.\nThe maximum representable value is (2−2<sup>−10</sup>) × 2<sup>15</sup> = 65504.\n\n=== Half precision examples ===\nThese examples are given in bit representation\nof the floating-point value. This includes the sign bit, (biased) exponent, and significand.\n\n 0 00000 0000000001<sub>2</sub> = 0001<sub>16</sub> = 2<sup>−14</sup> × 2<sup>−10</sup> = 2<sup>−24</sup> ≈ 0.000000059605\n                               (smallest positive subnormal number)\n\n 0 00000 1111111111<sub>2</sub> = 03ff<sub>16</sub> = 2<sup>−14</sup> × (1 − 2<sup>−10</sup>) ≈ 0.000060976\n                               (largest subnormal number)\n\n 0 00001 0000000000<sub>2</sub> = 0400<sub>16</sub> = 2<sup>−14</sup> ≈ 0.000061035\n                               (smallest positive normal number)\n\n 0 11110 1111111111<sub>2</sub> = 7bff<sub>16</sub> = 2<sup>15</sup> × (1 + (1 − 2<sup>−10</sup>)) = 65504\n                               (largest normal number)\n\n 0 01110 1111111111<sub>2</sub> = 3bff<sub>16</sub> = 1 − 2<sup>−11</sup> ≈ 0.99951\n                               (largest number less than one)\n\n 0 01111 0000000000<sub>2</sub> = 3c00<sub>16</sub> = 1 (one)\n\n 0 01111 0000000001<sub>2</sub> = 3c01<sub>16</sub> = 1 + 2<sup>−10</sup> ≈ 1.001\n                               (smallest number larger than one)\n\n 1 10000 0000000000<sub>2</sub> = c000<sub>16</sub> = −2\n \n 0 00000 0000000000<sub>2</sub> = 0000<sub>16</sub> = 0\n 1 00000 0000000000<sub>2</sub> = 8000<sub>16</sub> = −0\n \n 0 11111 0000000000<sub>2</sub> = 7c00<sub>16</sub> = infinity\n 1 11111 0000000000<sub>2</sub> = fc00<sub>16</sub> = −infinity\n \n 0 01101 0101010101<sub>2</sub> = 3555<sub>16</sub> = 0.333251953125 ≈ 1/3\n\nBy default, 1/3 rounds down like for [[double precision]], because of the odd number of bits in the significand. So the bits beyond the rounding point are <code>0101...</code> which is less than 1/2 of a [[unit in the last place]].\n\n=== Precision limitations on decimal values in [0, 1] ===\n* Decimals between 2<sup>−24</sup> (minimum positive subnormal) and 2<sup>−14</sup> (maximum subnormal): fixed interval 2<sup>−24</sup>\n* Decimals between 2<sup>−14</sup> (minimum positive normal) and 2<sup>−13</sup>: fixed interval 2<sup>−24</sup>\n* Decimals between 2<sup>−13</sup> and 2<sup>−12</sup>: fixed interval 2<sup>−23</sup>\n* Decimals between 2<sup>−12</sup> and 2<sup>−11</sup>: fixed interval 2<sup>−22</sup>\n* Decimals between 2<sup>−11</sup> and 2<sup>−10</sup>: fixed interval 2<sup>−21</sup>\n* Decimals between 2<sup>−10</sup> and 2<sup>−9</sup>: fixed interval 2<sup>−20</sup>\n* Decimals between 2<sup>−9</sup> and 2<sup>−8</sup>: fixed interval 2<sup>−19</sup>\n* Decimals between 2<sup>−8</sup> and 2<sup>−7</sup>: fixed interval 2<sup>−18</sup>\n* Decimals between 2<sup>−7</sup> and 2<sup>−6</sup>: fixed interval 2<sup>−17</sup>\n* Decimals between 2<sup>−6</sup> and 2<sup>−5</sup>: fixed interval 2<sup>−16</sup>\n* Decimals between 2<sup>−5</sup> and 2<sup>−4</sup>: fixed interval 2<sup>−15</sup>\n* Decimals between 2<sup>−4</sup> and 2<sup>−3</sup>: fixed interval 2<sup>−14</sup>\n* Decimals between 2<sup>−3</sup> and 2<sup>−2</sup>: fixed interval 2<sup>−13</sup>\n* Decimals between 2<sup>−2</sup> and 2<sup>−1</sup>: fixed interval 2<sup>−12</sup>\n* Decimals between 2<sup>−1</sup> and 2<sup>−0</sup>: fixed interval 2<sup>−11</sup>\n\n=== Precision limitations on decimal values in [1, 2048] ===\n* Decimals between 1 and 2: fixed interval 2<sup>−10</sup> (1+2<sup>−10</sup> is the next largest float after 1)\n* Decimals between 2 and 4: fixed interval 2<sup>−9</sup>\n* Decimals between 4 and 8: fixed interval 2<sup>−8</sup>\n* Decimals between 8 and 16: fixed interval 2<sup>−7</sup>\n* Decimals between 16 and 32: fixed interval 2<sup>−6</sup>\n* Decimals between 32 and 64: fixed interval 2<sup>−5</sup>\n* Decimals between 64 and 128: fixed interval 2<sup>−4</sup>\n* Decimals between 128 and 256: fixed interval 2<sup>−3</sup>\n* Decimals between 256 and 512: fixed interval 2<sup>−2</sup>\n* Decimals between 512 and 1024: fixed interval 2<sup>−1</sup>\n* Decimals between 1024 and 2048: fixed interval 2<sup>0</sup>\n\n=== Precision limitations on integer values ===\n* Integers between 0 and 2048 can be exactly represented (and also between −2048 and 0)\n* Integers between 2048 and 4096 round to a multiple of 2 (even number)\n* Integers between 4096 and 8192 round to a multiple of 4\n* Integers between 8192 and 16384 round to a multiple of 8\n* Integers between 16384 and 32768 round to a multiple of 16\n* Integers between 32768 and 65519 round to a multiple of 32<ref>{{cite web|title=Mediump float calculator|trans-title=|periodical=|publisher=|url=https://oletus.github.io/float16-simulator.js/|format=|accessdate=2016-07-26|last=|date=|year=|month=|day=|language=|pages=|quote=}}&#32;Half precision floating point calculator</ref>\n* Integers equal to or above 65520 are rounded to \"infinity\" (if using round-to-even, or equal to or above 65536 if using round-to-zero, or strictly above 65504 if using round-to-infinity).\n\n== ARM alternative half-precision ==\nARM processors support (via a floating point [[control register]] bit) an \"alternative half-precision\" format, which does away with the special case for an exponent value of 31 (11111<sub>2</sub>).<ref>{{cite book |chapter-url=http://infocenter.arm.com/help/topic/com.arm.doc.dui0205j/CIHGAECI.html |title=RealView Compilation Tools Compiler User Guide |chapter= Half-precision floating-point number support |date=10 December 2010 |accessdate=2015-05-05}}</ref>  It is almost identical to the IEEE format, but there is no encoding for infinity or NaNs; instead, an exponent of 31 encodes normalized numbers in the range 65536 to 131008.\n\n== See also ==\n* [[bfloat16 floating-point format]]: Alternative 16-bit floating-point format with 8 bits of exponent and 7 bits of mantissa\n* [[IEEE 754]]:  IEEE standard for floating-point arithmetic (IEEE 754)\n* [[ISO/IEC 10967]], Language Independent Arithmetic\n* [[Primitive data type]]\n* [[RGBE image format]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* [https://www.khronos.org/registry/DataFormat/specs/1.2/dataformat.1.2.html#16bitfp Khronos Vulkan signed 16-bit floating point format]\n\n== External links ==\n{{External links|date=July 2017}}\n* [https://www.mrob.com/pub/math/floatformats.html#minifloat Minifloats] (in ''Survey of Floating-Point Formats'')\n* [http://www.openexr.org/ OpenEXR site]\n* [https://technet.microsoft.com/en-us/library/bb147247(v=vs.85).aspx Half precision constants] from [[D3DX]]\n* [https://oss.sgi.com/projects/ogl-sample/registry/ARB/half_float_pixel.txt OpenGL treatment of half precision]\n* [ftp://ftp.fox-toolkit.org/pub/fasthalffloatconversion.pdf Fast Half Float Conversions]\n* [http://www.analog.com/static/imported-files/processor_manuals/ADSP_2136x_PGR_rev1-1.pdf Analog Devices variant]{{dead link|date=October 2017 |bot=InternetArchiveBot |fix-attempted=yes }} (four-bit exponent)\n* [https://www.mathworks.com/matlabcentral/fileexchange/23173 C source code to convert between IEEE double, single, and half precision can be found here]\n* [https://stackoverflow.com/a/6162687/237321 Java source code for half-precision floating-point conversion]\n* [https://gcc.gnu.org/onlinedocs/gcc/Half-Precision.html  Half precision floating point for one of the extended GCC features]\n\n{{data types}}\n\n{{DEFAULTSORT:Half-Precision Floating-Point Format}}\n[[Category:Binary arithmetic]]\n[[Category:Floating point types]]"
    },
    {
      "title": "Hexadecimal",
      "url": "https://en.wikipedia.org/wiki/Hexadecimal",
      "text": "\n{{redirect|Sexadecimal|base 60|Sexagesimal}}\n{{redirect|hex digit|binary coded hexadecimals|nybble}}\n{{Numeral systems}}\nIn [[mathematics]] and [[computing]], '''hexadecimal''' (also '''base 16''', or '''hex''') is a [[positional notation|positional]] [[numeral system]] with a [[radix]], or base, of 16. It uses sixteen distinct symbols, most often the symbols \"0\"–\"9\" to represent values [[zero]] to [[9|nine]], and \"A\"–\"F\" (or alternatively \"a\"–\"f\") to represent values [[10|ten]] to [[15 (number)|fifteen]].<!--YES, NOT SIXTEEN. Fifteen represents the largest digit in base sixteen, just as nine represents the largest digit in base ten.-->\n\nHexadecimal numerals are widely used by computer system designers and programmers, as they provide a more human-friendly representation of [[binary code|binary-coded]] values. Each hexadecimal digit represents four [[bit|binary digits]], also known as a [[nibble]], which is half a [[byte]]. For example, a single byte can have values ranging from 0000 0000 to 1111 1111 in binary form, which can be more conveniently represented as 00 to FF in hexadecimal.\n\nIn mathematics, a subscript is typically used to specify the [[radix]].  For example, the decimal value {{val|10995|fmt=commas}} would be expressed in hexadecimal as {{hexadecimal|10995}}.  In programming, a number of notations are used to support hexadecimal representation, usually involving a prefix or suffix.  The prefix <code>0x</code> is used in [[C (programming language)|C]] and related languages, which would denote this value by <code>0x{{hexadecimal|10995|no}}</code>.\n\nHexadecimal is used in the transfer encoding '''Base16''', in which each byte of the plaintext is broken into two 4-bit values and represented by two hexadecimal digits.\n\n==Representation==\n\n===Written representation===\n\n====Using 0–9 and A–F====\n{{Hexadecimal table}}\nIn contexts where the [[radix|base]] is not clear, hexadecimal numbers can be ambiguous and confused with numbers expressed in other bases. There are several conventions for expressing values unambiguously. A numerical subscript (itself written in decimal) can give the base explicitly: 159<sub>10</sub> is decimal 159; 159<sub>16</sub> is hexadecimal 159, which is equal to 345<sub>10</sub>. Some authors prefer a text subscript, such as 159<sub>decimal</sub> and 159<sub>hex</sub>, or 159<sub>d</sub> and 159<sub>h</sub>.\n\nIn linear text systems, such as those used in most computer programming environments, a variety of methods have arisen:<!--\n * * * These are ordered from most likely to be encountered by lay people\n * * * to least likely to be encountered by lay people\n * * * -->\n* In [[URI]]s (including [[URL]]s), [[character encoding|character codes]] are written as hexadecimal pairs prefixed with <code>%</code>: <code><nowiki>http://www.example.com/name%20with%20spaces</nowiki></code> where <code>%20</code> is the [[Space (punctuation)#Space characters and digital typography|space (blank)]] character, [[ASCII]] code point 20 in hex, 32 in decimal.\n* In [[XML]] and [[XHTML]], characters can be expressed as hexadecimal [[numeric character reference]]s using the notation <code>&amp;#x''code'';</code>, where the ''x'' denotes that ''code'' is a hex code point (of 1- to 6-digits) assigned to the character in the [[Unicode]] standard. Thus <code>&amp;#x2019;</code> represents the right single quotation mark (’), Unicode code point number 2019 in hex, 8217 (thus <code>&amp;#8217;</code> in decimal).<ref>{{cite web|url=https://www.unicode.org/charts/PDF/U2000.pdf|title=The Unicode Standard, Version 7|last=|first=|date=|website=Unicode|archive-url=|archive-date=|dead-url=|access-date=October 28, 2018}}</ref>\n* In the [[Unicode]] standard, a character value is represented with <code>U+</code> followed by the hex value, e.g. <code>U+20AC</code> is the [[Euro sign]] (€).\n* [[Web colors|Color references]] in HTML, [[Cascading Style Sheets|CSS]] and [[X window system|X Window]] can be expressed with six hexadecimal digits (two each for the red, green and blue components, in that order) prefixed with <code>#</code>: white, for example, is represented as <code>#FFFFFF</code>.<ref>{{cite web |url=http://www.web-colors-explained.com/hex.php |title=Hexadecimal web colors explained}}</ref> CSS also allows 3-hexdigit abbreviations with one hexdigit per component: #FA3 abbreviates #FFAA33 (a golden orange: {{color box|#FA3}}).\n* {{anchor|_nix}}[[*nix|Unix]] (and related) shells, [[AT&T Corporation|AT&T]] assembly language and likewise the [[C (programming language)|C programming language]] (and its syntactic descendants such as [[C++]], [[C Sharp (programming language)|C#]], [[D (programming language)|D]], [[Java (programming language)|Java]], [[JavaScript]], [[Python (programming language)|Python]] and [[Windows PowerShell]]) use the prefix <code>0x</code> for numeric constants represented in hex: <code>0x5A3</code>. Character and string constants may express character codes in hexadecimal with the prefix <code>\\x</code> followed by two hex digits: <code>'\\x1B'</code> represents the [[Escape character|Esc]] control character; <code>\"\\x1B[0m\\x1B[25;1H\"</code> is a string containing 11 characters (plus a trailing NUL to mark the end of the string) with two embedded Esc characters.<ref>The string <code>\"\\x1B[0m\\x1B[25;1H\"</code> specifies the character sequence <tt>Esc [ 0 m Esc [ 2 5 ; 1 H Nul</tt>. These are the escape sequences used on an [[ANSI escape code|ANSI terminal]] that reset the character set and color, and then move the cursor to line 25.</ref> To output an integer as hexadecimal with the [[printf]] function family, the format conversion code <code>%X</code> or <code>%x</code> is used.\n* In [[MIME]] (e-mail extensions) [[quoted-printable]] encoding, characters that cannot be represented as literal [[ASCII]] characters are represented by their codes as two hexadecimal digits (in ASCII) prefixed by an ''equal to'' sign <code>=</code>, as in <code>Espa=F1a</code> to send \"España\" (Spain). (Hexadecimal F1, equal to decimal 241, is the code number for the lower case n with tilde in the ISO/IEC 8859-1 character set.<ref>{{Cite web|url=https://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html|title=ISO-8859-1 (ISO Latin 1) Character Encoding|website=www.ic.unicamp.br|access-date=2019-06-26}}</ref>)\n* In Intel-derived [[assembly language]]s and Modula-2,<ref>{{cite web |title=Modula-2 - Vocabulary and representation |url=http://modula2.org/reference/vocabulary.php |website=Modula -2 |access-date=1 November 2015}}</ref> hexadecimal is denoted with a suffixed <tt>H</tt> or <tt>h</tt>: <code>FFh</code> or <code>05A3H</code>. Some implementations require a leading zero when the first hexadecimal digit character is not a decimal digit, so one would write <code>0FFh</code> instead of <code>FFh</code>\n* Other assembly languages ([[MOS Technology 6502|6502]], [[Motorola]]), [[Pascal (programming language)|Pascal]], [[Object Pascal|Delphi]], some versions of [[BASIC]] ([[Commodore BASIC|Commodore]]), [[GameMaker Studio|GameMaker Language]], [[Godot (game engine)|Godot]] and [[Forth (programming language)|Forth]] use <code>$</code> as a prefix: <code>$5A3</code>.\n* Some assembly languages (Microchip) use the notation <code>H'ABCD'</code> (for ABCD<sub>16</sub>). Similarly, [[Fortran 95 language features|Fortran 95]] uses Z'ABCD'.\n* [[Ada (programming language)|Ada]] and [[VHDL]] enclose hexadecimal numerals in based \"numeric quotes\": <code>16#5A3#</code>. For bit vector constants [[VHDL]] uses the notation <code>x\"5A3\"</code>.<ref>The [http://www.eng.auburn.edu/department/ee/mgc/vhdl.html#numbers VHDL MINI-REFERENCE: VHDL IDENTIFIERS, NUMBERS, STRINGS, AND EXPRESSIONS]</ref>\n* [[Verilog]] represents hexadecimal constants in the form <code>8'hFF</code>, where 8 is the number of bits in the value and FF is the hexadecimal constant.\n* The [[Smalltalk]] language uses the prefix <code>16r</code>: <code>16r5A3</code>\n* [[PostScript]] and the [[Bourne shell]] and its derivatives denote hex with prefix <code>16#</code>: <code>16#5A3</code>. For PostScript, binary data (such as image [[pixel]]s) can be expressed as unprefixed consecutive hexadecimal pairs: <code>AA213FD51B3801043FBC</code>...\n* [[Common Lisp]] uses the prefixes <code>#x</code> and <code>#16r</code>. Setting the variables *read-base*<ref>{{cite web |title=*read-base* variable in Common Lisp |url=http://www.lispworks.com/documentation/HyperSpec/Body/v_rd_bas.htm}}</ref> and *print-base*<ref>{{cite web |title=*print-base* variable in Common Lisp |url=http://www.lispworks.com/documentation/HyperSpec/Body/v_pr_bas.htm#STprint-baseST}}</ref> to 16 can also be used to switch the reader and printer of a Common Lisp system to Hexadecimal number representation for reading and printing numbers. Thus Hexadecimal numbers can be represented without the #x or #16r prefix code, when the input or output base has been changed to 16.\n* [[MSX BASIC]],<ref>[http://www.atarimagazines.com/compute/issue56/107_1_MSX_IS_COMING.php MSX is Coming — Part 2: Inside MSX] [[Compute!]], issue 56, January 1985, p. 52</ref> [[QuickBASIC]], [[FreeBASIC]] and [[Visual Basic]] prefix hexadecimal numbers with <code>&amp;H</code>: <code>&amp;H5A3</code>\n* [[BBC BASIC]] and [[Locomotive BASIC]] use <code>&amp;</code> for hex.<ref>BBC BASIC programs are not fully portable to [[Microsoft BASIC]] (without modification) since the latter takes <code>&amp;</code> to prefix [[octal]] values. (Microsoft BASIC primarily uses <code>&amp;O</code> to prefix octal, and it uses <code>&amp;H</code> to prefix hexadecimal, but the ampersand alone yields a default interpretation as an octal prefix.</ref>\n* [[TI-89]] and 92 series uses a <code>0h</code> prefix: <code>0h5A3</code>\n* [[ALGOL 68]] uses the prefix <code>16r</code> to denote hexadecimal numbers: <code>16r5a3</code>.  Binary, quaternary (base-4) and octal numbers can be specified similarly.\n* The most common format for hexadecimal on IBM mainframes ([[zSeries]]) and midrange computers ([[IBM System i]]) running the traditional OS's ([[z/OS|zOS]], [[VSE (operating system)|zVSE]], [[z/VM|zVM]], [[Transaction Processing Facility|TPF]], [[IBM i]]) is <code>X'5A3'</code>, and is used in Assembler, [[PL/I]], [[COBOL]], [[Job Control Language|JCL]], scripts, commands and other places. This format was common on other (and now obsolete) IBM systems as well.  Occasionally quotation marks were used instead of apostrophes.\n* [[Donald Knuth]] introduced the use of a particular typeface to represent a particular radix in his book ''The TeXbook''.<ref>Donald E. Knuth. ''The TeXbook'' ([[Computers and Typesetting]], Volume A). Reading, Massachusetts: Addison–Wesley, 1984. {{isbn|0-201-13448-9}}. The [http://www.ctan.org/tex-archive/systems/knuth/tex/texbook.tex source code of the book in TeX] {{webarchive|url=https://web.archive.org/web/20070927224129/http://www.ctan.org/tex-archive/systems/knuth/tex/texbook.tex |date=2007-09-27 }} (and a required set of macros [ftp://tug.ctan.org/pub/tex-archive/systems/knuth/lib/manmac.tex CTAN.org]) is available online on [[CTAN]].</ref> Hexadecimal representations are written there in a typewriter typeface: <tt>5A3</tt>\n* Any [[IPv6 address]] can be written as eight groups of four hexadecimal digits (sometimes called [[hextet (computing)|hextet]]s), where each group is separated by a colon (<code>:</code>). This, for example, is a valid IPv6 address: 2001:0db8:85a3:0000:0000:8a2e:0370:7334; this can be abbreviated as 2001:db8:85a3::8a2e:370:7334. By contrast, [[IPv4 address]]es are usually written in decimal.\n* [[Globally unique identifier]]s are written as thirty-two hexadecimal digits, often in unequal hyphen-separated groupings, for example <code>{3F2504E0-4F89-41D3-9A0C-0305E82C3301}</code>.\n\nThere is no universal convention to use lowercase or uppercase for the letter digits, and each is prevalent or preferred in particular environments by community standards or convention.\n\n===History of written representations===\n\n[[Image:Bruce Martin hexadecimal notation proposal.png|thumb|Bruce Alan Martin's hexadecimal notation proposal<ref name=\"Martin_1968\"/>]]\nThe use of the letters ''A'' through ''F'' to represent the digits above 9 was not universal in the early history of computers.\n* During the 1950s, some installations{{which|date=August 2017}} favored using the digits 0 through 5 with an [[overline]] to denote the values 10–15 as {{overline|0}}, {{overline|1}}, {{overline|2}}, {{overline|3}}, {{overline|4}} and {{overline|5}}.\n* The [[SWAC (computer)|SWAC]] (1950)<ref name=\"Savard_2018_CA\"/> and [[Bendix G-15]] (1956)<ref name=\"Bendix\"/><ref name=\"Savard_2018_CA\"/> computers used the lowercase letters ''u'', ''v'', ''w'', ''x'', ''y'' and ''z'' for the values 10 to 15.\n* The [[ILLIAC I]] (1952) computer used the uppercase letters ''K'', ''S'', ''N'', ''J'', ''F'' and ''L'' for the values 10 to 15.<ref name=\"Illiac-I\"/><ref name=\"Savard_2018_CA\"/>\n* The Librascope [[LGP-30]] (1956) used the letters ''F'', ''G'', ''J'', ''K'', ''Q'' and ''W'' for the values 10 to 15.<ref name=\"RP_1957_LGP-30\"/><ref name=\"Savard_2018_CA\"/>\n* The [[Honeywell]] [[Datamatic D-1000]] (1957) used the lowercase letters ''b'', ''c'', ''d'', ''e'', ''f'', and ''g'' whereas the [[Elbit]]&nbsp;100 (1967) used the uppercase letters ''B'', ''C'', ''D'', ''E'', ''F'' and ''G'' for the values 10 to 15.<ref name=\"Savard_2018_CA\"/>\n* The [[Monrobot XI]] (1960) used the letters ''S'', ''T'', ''U'', ''V'', ''W'' and ''X'' for the values 10 to 15.<ref name=\"Savard_2018_CA\"/>\n* The [[NEC]] [[parametron]] computer {{ill|NEAC 1103|ja|NEAC}} (1960) used the letters ''D'', ''G'', ''H'', ''J'', ''K'' (and possibly ''V'') for values 10–15.<ref name=\"NEC_1960_NEAC-1103\">{{cite book |title=NEC Parametron Digital Computer Type NEAC-1103 |publisher=[[Nippon Electric Company Ltd.]] |location=Tokyo, Japan |id=Cat. No. 3405-C |date=1960 |url=http://archive.computerhistory.org/resources/text/NEC/NEC.1103.1958102646285.pdf |access-date=2017-05-31 |dead-url=no |archive-url=https://web.archive.org/web/20170531112850/http://archive.computerhistory.org/resources/text/NEC/NEC.1103.1958102646285.pdf |archive-date=2017-05-31}}</ref>\n* The Pacific Data Systems&nbsp;1020 (1964) used the letters ''L'', ''C'', ''A'', ''S'', ''M'' and ''D'' for the values 10 to 15.<ref name=\"Savard_2018_CA\"/>\n* New numeric symbols and names were introduced in the [[Bibi-binary]] notation by [[Boby Lapointe]] in 1968. This notation did not become very popular. \n* Bruce Alan Martin of [[Brookhaven National Laboratory]] considered the choice of A–F \"ridiculous\". In a 1968 letter to the editor of the [[Communications of the ACM|CACM]], he proposed an entirely new set of symbols based on the bit locations, which did not gain much acceptance.<ref name=\"Martin_1968\">{{cite journal | title=Letters to the editor: On binary notation | first=Bruce Alan | last=Martin | publisher=[[Associated Universities Inc.]] | work=[[Communications of the ACM]] | volume=11 | issue=10 | date=October 1968 | page=658 | doi=10.1145/364096.364107}}</ref>\n* Soviet [[programmable calculator]]s [[Elektronika B3-34|Б3-34]] (1980) and similar used the symbols \"−\", \"L\", \"C\", \"Г\", \"E\", \"&ensp;\" (space) for the values 10 to 15 on their displays.{{cn|date=August 2017}}\n* [[Seven-segment display]] decoder chips used various schemes for outputting values above nine. The [[7400 series|Texas Instruments 7446/7447/7448/7449 and 74246/74247/74248/74249]] use truncated versions of \"2\", \"3\", \"4\", \"5\" and \"6\" for the values 10 to 14. Value 15 (1111 binary) was blank.<ref name=\"TI_BCD-7SEGDEC\">{{cite |title=BCD-to-Seven-Segment Decoders/Drivers: SN54246/SN54247/SN54LS247, SN54LS248 SN74246/SN74247/SN74LS247/SN74LS248 |date=March 1988 |orig-year=March 1974 |id=SDLS083 |publisher=[[Texas Instruments]] |url=http://www.ralphselectronics.com/ProductImages/SEMI-SN74247N.PDF |access-date=2017-03-30 |dead-url=no |archive-url=https://web.archive.org/web/20170329223343/http://www.ralphselectronics.com/ProductImages/SEMI-SN74247N.PDF |archive-date=2017-03-29 |quote=[…] They can be used interchangeable in present or future designs to offer designers a choice between two indicator fonts. The '46A, '47A, 'LS47, and 'LS48 compose the 6 and the 9 without tails and the '246, '247, 'LS247, and 'LS248 compose the 6 and the 0 with tails. Composition of all other characters, including display patterns for BCD inputs above nine, is identical. […] Display patterns for BCD input counts above 9 are unique symbols to authenticate input conditions. […]}}</ref>\n\n===Verbal and digital representations===\nThere are no traditional numerals to represent the quantities from ten to fifteen – letters are used as a substitute – and most [[Europe]]an languages lack non-decimal names for the numerals above ten. Even though English has names for several non-decimal powers (''[[2 (number)|pair]]'' for the first [[binary numeral system|binary]] power, ''[[20 (number)|score]]'' for the first [[vigesimal]] power, ''[[dozen]]'', ''[[Gross (unit)|gross]]'' and ''[[great gross]]'' for the first three [[duodecimal]] powers), no English name describes the hexadecimal powers (decimal 16, 256, 4096, 65536, ...&nbsp;). Some people read hexadecimal numbers digit by digit like a phone number, or using the [[ICAO spelling alphabet|NATO phonetic alphabet]], the [[Joint Army/Navy Phonetic Alphabet]], or a similar ad hoc system.\n\n[[File:Hexadecimal-counting.jpg|right|thumb|Hexadecimal finger-counting scheme]]\nSystems of counting on [[Digit (anatomy)|digits]] have been devised for both binary and hexadecimal.\n[[Arthur C. Clarke]] suggested using each finger as an on/off bit, allowing finger counting from zero to 1023<sub>10</sub> on ten fingers.<ref>{{cite book |last1=Clarke |first1=Arthur |last2=Pohl |first2=Frederik |title=The Last Theorem |date=2008 |publisher=Ballantine |isbn=978-0007289981 |page=91}}</ref> Another system for counting up to FF<sub>16</sub> (255<sub>10</sub>) is illustrated on the right.\n\n===Signs===\nThe hexadecimal system can express negative numbers the same way as in decimal: −2A to represent −42<sub>10</sub> and so on.\n\nHexadecimal can also be used to express the exact bit patterns used in the [[central processing unit|processor]], so a sequence of hexadecimal digits may represent a [[signedness|signed]] or even a [[floating point]] value. This way, the negative number −42<sub>10</sub> can be written as FFFF&nbsp;FFD6 in a 32-bit [[Processor register|CPU register]] (in [[two's-complement]]), as C228&nbsp;0000 in a 32-bit [[Floating point unit|FPU]] register or C045&nbsp;0000&nbsp;0000&nbsp;0000 in a 64-bit FPU register (in the [[IEEE floating-point standard]]).\n\n===Hexadecimal exponential notation===\nJust as decimal numbers can be represented in [[exponential notation]], so too can hexadecimal numbers.  By convention, the letter ''P'' (or ''p'', for \"power\") represents ''times two raised to the power of'', whereas ''E'' (or ''e'') serves a similar purpose in decimal as part of the [[E notation]].  The number after the ''P'' is ''decimal'' and represents the ''binary'' exponent.\n\nUsually the number is normalised so that the leading hexadecimal digit is 1 (unless the value is exactly 0).\n\nExample: 1.3DEp42 represents {{math|1.3DE<sub>16</sub> × 2<sup>42</sup>}}.\n\nHexadecimal exponential notation is required by the [[IEEE 754-2008]] binary floating-point standard.\nThis notation can be used for floating-point literals in the [[C99]] edition of the [[C (programming_language)|C programming language]].<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=29237 |title=ISO/IEC 9899:1999 - Programming languages - C |publisher=Iso.org |date=2011-12-08 |accessdate=2014-04-08}}</ref>\nUsing the ''%a'' or ''%A'' conversion specifiers, this notation can be produced by implementations of the ''[[printf]]'' family of functions following the C99 specification<ref name=\"Rationale_2003_C\">{{cite web |title=Rationale for International Standard - Programming Languages - C |version=5.10 |date=April 2003 |pages=52, 153–154, 159 |url=http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf |access-date=2010-10-17 |dead-url=no |archive-url=https://web.archive.org/web/20160606072228/http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf |archive-date=2016-06-06}}</ref> and\n[[Single UNIX Specification|Single Unix Specification]] (IEEE Std 1003.1) [[POSIX]] standard.<ref name=\"printf_2013\">{{cite web |title=dprintf, fprintf, printf, snprintf, sprintf - print formatted output |work=The Open Group Base Specifications |edition=Issue 7, IEEE Std 1003.1, 2013 |date=2013 |orig-year=2001 |author=The IEEE and The Open Group |url=http://pubs.opengroup.org/onlinepubs/9699919799/functions/printf.html |access-date=2016-06-21 |dead-url=no |archive-url=https://web.archive.org/web/20160621211105/http://pubs.opengroup.org/onlinepubs/9699919799/functions/printf.html |archive-date=2016-06-21}}</ref>\n\n==Conversion==\n\n===Binary conversion===\nMost computers manipulate binary data, but it is difficult for humans to work with the large number of digits for even a relatively small binary number. Although most humans are familiar with the base 10 system, it is much easier to map binary to hexadecimal than to decimal because each hexadecimal digit maps to a whole number of bits (4<sub>10</sub>).\nThis example converts 1111<sub>2</sub> to base ten. Since each [[Positional notation|position]] in a binary numeral can contain either a 1 or a 0, its value may be easily determined by its position from the right:\n* 0001<sub>2</sub> = 1<sub>10</sub>\n* 0010<sub>2</sub> = 2<sub>10</sub>\n* 0100<sub>2</sub> = 4<sub>10</sub>\n* 1000<sub>2</sub> = 8<sub>10</sub>\nTherefore:\n{|\n|-\n| 1111<sub>2</sub>|| = 8<sub>10</sub> + 4<sub>10</sub> + 2<sub>10</sub> + 1<sub>10</sub>\n|-\n| &nbsp;|| = 15<sub>10</sub>\n|}\nWith little practice, mapping 1111<sub>2</sub> to F<sub>16</sub> in one step becomes easy: see table in [[#Written representation|written representation]]. The advantage of using hexadecimal rather than decimal increases rapidly with the size of the number. When the number becomes large, conversion to decimal is very tedious. However, when mapping to hexadecimal, it is trivial to regard the binary string as 4-digit groups and map each to a single hexadecimal digit.\n\nThis example shows the conversion of a binary number to decimal, mapping each digit to the decimal value, and adding the results.\n{|\n| (01011110101101010010)<sub>2</sub>|| = 262144<sub>10</sub> + 65536<sub>10</sub> + 32768<sub>10</sub> + 16384<sub>10</sub> + 8192<sub>10</sub> + 2048<sub>10</sub> + 512<sub>10</sub> + 256<sub>10</sub> + 64<sub>10</sub> + 16<sub>10</sub> + 2<sub>10</sub>\n|-\n| &nbsp;|| = 387922<sub>10</sub>\n|}\nCompare this to the conversion to hexadecimal, where each group of four digits can be considered independently, and converted directly:\n{|\n|-\n| (01011110101101010010)<sub>2</sub>|| = ||0101<sub>&nbsp;</sub>||1110<sub>&nbsp;</sub>||1011<sub>&nbsp;</sub>||0101<sub>&nbsp;</sub>||0010<sub>2</sub>\n|-\n| &nbsp;|| = || align=\"center\" |5|| align=\"center\" |E|| align=\"center\" |B|| align=\"center\" |5|| align=\"center\" |2<sub>16</sub>\n|-\n| &nbsp;|| = || colspan=\"5\" |5EB52<sub>16</sub>\n|}\nThe conversion from hexadecimal to binary is equally direct.\n\n===Other simple conversions===\n\nAlthough [[Quaternary numeral system|quaternary]] (base 4) is little used, it can easily be converted to and from hexadecimal or binary. Each hexadecimal digit corresponds to a pair of quaternary digits and each quaternary digit corresponds to a pair of binary digits. In the above example 5&nbsp;E&nbsp;B&nbsp;5&nbsp;2<sub>16</sub> = 11&nbsp;32&nbsp;23&nbsp;11&nbsp;02<sub>4</sub>.\n\nThe [[octal]] (base 8) system can also be converted with relative ease, although not quite as trivially as with bases 2 and 4. Each octal digit corresponds to three binary digits, rather than four. Therefore we can convert between octal and hexadecimal via an intermediate conversion to binary followed by regrouping the binary digits in groups of either three or four.\n\n===Division-remainder in source base===\nAs with all bases there is a simple [[algorithm]] for converting a representation of a number to hexadecimal by doing integer division and remainder operations in the source base. In theory, this is possible from any base, but for most humans only decimal and for most computers only binary (which can be converted by far more efficient methods) can be easily handled with this method.\n\nLet d be the number to represent in hexadecimal, and the series h<sub>i</sub>h<sub>i−1</sub>...h<sub>2</sub>h<sub>1</sub> be the hexadecimal digits representing the number.\n\n# i ← 1\n# h<sub>i</sub> ← d mod 16\n# d ← (d − h<sub>i</sub>) / 16\n# If d = 0 (return series h<sub>i</sub>) else increment i and go to step 2\n\n\"16\" may be replaced with any other base that may be desired.\n\nThe following is a [[JavaScript]] implementation of the above algorithm for converting any number to a hexadecimal in String representation. Its purpose is to illustrate the above algorithm. To work with data seriously, however, it is much more advisable to work with [[bitwise operators]].\n\n<source lang=\"javascript\">\nfunction toHex(d) {\n  var r = d % 16;\n  if (d - r == 0) {\n    return toChar(r);\n  }\n  return toHex( (d - r)/16 ) + toChar(r);\n}\n\nfunction toChar(n) {\n  const alpha = \"0123456789ABCDEF\";\n  return alpha.charAt(n);\n}\n</source>\n\n===Conversion through addition and multiplication===\n[[Image:Hexadecimal multiplication table.svg|right|thumb|A hexadecimal [[multiplication table]]]]\nIt is also possible to make the conversion by assigning each place in the source base the hexadecimal representation of its place value — before carrying out multiplication and addition to get the final representation.\nFor example, to convert the number B3AD to decimal, one can split the hexadecimal number into its digits: B (11<sub>10</sub>), 3 (3<sub>10</sub>), A (10<sub>10</sub>) and D (13<sub>10</sub>), and then get the final result by multiplying each decimal representation by 16<sup>''p''</sup> (''p'' being the corresponding hex digit position, counting from right to left, beginning with 0). In this case, we have that:\n\n{{math|B3AD {{=}} (11 × 16<sup>3</sup>) + (3 × 16<sup>2</sup>) + (10 × 16<sup>1</sup>) + (13 × 16<sup>0</sup>)}}\n\nwhich is 45997 in base 10.\n\n===Tools for conversion===\nMost modern computer systems with [[graphical user interface]]s provide a built-in calculator utility capable of performing conversions between the various radices, and in most cases would include the hexadecimal as well.\n\nIn [[Microsoft Windows]], the [[Calculator (Windows)|Calculator]] utility can be set to Scientific mode (called Programmer mode in some versions), which allows conversions between radix 16 (hexadecimal), 10 (decimal), 8 ([[octal]]) and 2 ([[Binary numeral system|binary]]), the bases most commonly used by programmers. In Scientific Mode, the on-screen [[numeric keypad]] includes the hexadecimal digits A through F, which are active when \"Hex\" is selected. In hex mode, however, the Windows Calculator supports only integers.\n\n== Elementary arithmetic ==\nElementary operations such additions, subtractions, multiplications and divisions can be carried out indirectly through conversion to an alternate [[numeral system]], such as the decimal system, since it's the most commonly adopted system, or the binary system, since each hex digit corresponds to four binary digits, \n\nAlternatively, one can also perform elementary operations directly within the hex system itself — by relying on its addition/multiplication tables and its corresponding standard algorithms such as [[long division]] and the traditional subtraction algorithm.<ref>{{Cite web|url=https://mathvault.ca/long-division/|title=The Definitive Higher Math Guide to Long Division and Its Variants — for Integers|date=2019-02-24|website=Math Vault|language=en-US|access-date=2019-06-26}}</ref>\n\n==Real numbers==\n\n=== Rational numbers ===\nAs with other numeral systems, the hexadecimal system can be used to represent [[rational number]]s, although [[repeating decimal|repeating expansions]] are common since sixteen (10<sub>16</sub>) has only a single prime factor; two.\n\nFor any base, 0.1 (or \"1/10\") is always equivalent to one divided by the representation of that base value in its own number system. Thus, whether dividing one by two for [[binary numeral system|binary]] or dividing one by sixteen for hexadecimal, both of these fractions are written as <code>0.1</code>. Because the radix 16 is a [[square number|perfect square]] (4<sup>2</sup>), fractions expressed in hexadecimal have an odd period much more often than decimal ones, and there are no [[cyclic number]]s (other than trivial single digits). Recurring digits are exhibited when the denominator in lowest terms has a [[prime factor]] not found in the radix; thus, when using hexadecimal notation, all fractions with denominators that are not a [[power of two]] result in an infinite string of recurring digits (such as thirds and fifths). This makes hexadecimal (and binary) less convenient than [[decimal]] for representing rational numbers since a larger proportion lie outside its range of finite representation.\n\nAll rational numbers finitely representable in hexadecimal are also finitely representable in decimal, [[duodecimal]] and [[sexagesimal]]: that is, any hexadecimal number with a finite number of digits also has a finite number of digits when expressed in those other bases. Conversely, only a fraction of those finitely representable in the latter bases are finitely representable in hexadecimal. For example, decimal 0.1 corresponds to the infinite recurring representation 0.1{{overline|9}} in hexadecimal. However, hexadecimal is more efficient than duodecimal and sexagesimal for representing fractions with powers of two in the denominator. For example, 0.0625<sub>10</sub> (one sixteenth) is equivalent to 0.1<sub>16</sub>, 0.09<sub>12</sub>, and 0;3,45<sub>60</sub>.\n\n{|class=\"wikitable\"\n! rowspan=2 style=\"vertical-align:bottom;\" | n\n! colspan=3 | Decimal<br />Prime factors of base, b = 10: {{color|#920000|2}}, {{color|#920000|5}}; b − 1 = 9: {{color|#000092|3}}; b + 1 = 11: {{color|#004900|11}}\n! colspan=3 | Hexadecimal<br/>Prime factors of base, b = 16{{sub|10}} = 10: {{color|#920000|2}}; b − 1 = 15{{sub|10}} = F: {{color|#000092|3, 5}}; b + 1 = 17{{sub|10}} = 11: {{color|#004900|11}}\n|-\n! Fraction\n! Prime factors\n! Positional representation\n! Positional representation\n! Prime factors\n! Fraction(1/n)\n|-\n| 2\n| align=\"center\" | 1/2\n| align=\"center\" | {{color|#920000|'''2'''}}\n| '''0.5'''\n| '''0.8'''\n| align=\"center\" | {{color|#920000|'''2'''}}\n| align=\"center\" | 1/2\n|-\n| 3\n| align=\"center\" | 1/3\n| align=\"center\" | {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.'''3333... = '''0.'''{{overline|3}}\n| bgcolor=#e4e4e4 | '''0.'''5555... = '''0.'''{{overline|5}}\n| align=\"center\" | {{color|#000092|'''3'''}}\n| align=\"center\" | 1/3\n|-\n| 4\n| align=\"center\" | 1/4\n| align=\"center\" | {{color|#920000|'''2'''}}\n| '''0.25'''\n| '''0.4'''\n| align=\"center\" | {{color|#920000|'''2'''}}\n| align=\"center\" | 1/4\n|-\n| 5\n| align=\"center\" | 1/5\n| align=\"center\" | {{color|#920000|'''5'''}}\n| '''0.2'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|3}}\n| align=\"center\" | {{color|#000092|'''5'''}}\n| align=\"center\" | 1/5\n|-\n| 6\n| align=\"center\" | 1/6\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.1'''{{overline|6}}\n| bgcolor=#e4e4e4 | '''0.2'''{{overline|A}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| align=\"center\" | 1/6\n|-\n| 7\n| align=\"center\" | 1/7\n| align=\"center\" | '''7'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|142857}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|249}}\n| align=\"center\" | '''7'''\n| align=\"center\" | 1/7\n|-\n| 8\n| align=\"center\" | 1/8\n| align=\"center\" | {{color|#920000|'''2'''}}\n| '''0.125'''\n| '''0.2'''\n| align=\"center\" | {{color|#920000|'''2'''}}\n| align=\"center\" | 1/8\n|-\n| 9\n| align=\"center\" | 1/9\n| align=\"center\" | {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|1}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|1C7}}\n| align=\"center\" | {{color|#000092|'''3'''}}\n| align=\"center\" | 1/9\n|-\n| 10\n| align=\"center\" | 1/10\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#920000|'''5'''}}\n| '''0.1'''\n| bgcolor=#e4e4e4 | '''0.1'''{{overline|9}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''5'''}}\n| align=\"center\" | 1/A\n|-\n| 11\n| align=\"center\" | 1/11\n| align=\"center\" | {{color|#004900|'''11'''}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|09}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|1745D}}\n| align=\"center\" | '''B'''\n| align=\"center\" | 1/B\n|-\n| 12\n| align=\"center\" | 1/12\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.08'''{{overline|3}}\n| bgcolor=#e4e4e4 | '''0.1'''{{overline|5}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| align=\"center\" | 1/C\n|-\n| 13\n| align=\"center\" | 1/13\n| align=\"center\" | '''13'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|076923}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|13B}}\n| align=\"center\" | '''D'''\n| align=\"center\" | 1/D\n|-\n| 14\n| align=\"center\" | 1/14\n| align=\"center\" | {{color|#920000|'''2'''}}, '''7'''\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|714285}}\n| bgcolor=#e4e4e4 | '''0.1'''{{overline|249}}\n| align=\"center\" | {{color|#920000|'''2'''}}, '''7'''\n| align=\"center\" | 1/E\n|-\n| 15\n| align=\"center\" | 1/15\n| align=\"center\" | {{color|#000092|'''3'''}}, {{color|#920000|'''5'''}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|6}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|1}}\n| align=\"center\" | {{color|#000092|'''3'''}}, {{color|#000092|'''5'''}}\n| align=\"center\" | 1/F\n|-\n| 16\n| align=\"center\" | 1/16\n| align=\"center\" | {{color|#920000|'''2'''}}\n| '''0.0625'''\n| '''0.1'''\n| align=\"center\" | {{color|#920000|'''2'''}}\n| align=\"center\" | 1/10\n|-\n| 17\n| align=\"center\" | 1/17\n| align=\"center\" | '''17'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0588235294117647}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0F}}\n| align=\"center\" | {{color|#004900|'''11'''}}\n| align=\"center\" | 1/11\n|-\n| 18\n| align=\"center\" | 1/18\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|5}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|E38}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| align=\"center\" | 1/12\n|-\n| 19\n| align=\"center\" | 1/19\n| align=\"center\" | '''19'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|052631578947368421}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0D79435E5}}\n| align=\"center\" | '''13'''\n| align=\"center\" | 1/13\n|-\n| 20\n| align=\"center\" | 1/20\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#920000|'''5'''}}\n| '''0.05'''\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|C}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''5'''}}\n| align=\"center\" | 1/14\n|-\n| 21\n| align=\"center\" | 1/21\n| align=\"center\" | {{color|#000092|'''3'''}}, '''7'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|047619}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0C3}}\n| align=\"center\" | {{color|#000092|'''3'''}}, '''7'''\n| align=\"center\" | 1/15\n|-\n| 22\n| align=\"center\" | 1/22\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#004900|'''11'''}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|45}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|BA2E8}}\n| align=\"center\" | {{color|#920000|'''2'''}}, '''B'''\n| align=\"center\" | 1/16\n|-\n| 23\n| align=\"center\" | 1/23\n| align=\"center\" | '''23'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0434782608695652173913}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0B21642C859}}\n| align=\"center\" | '''17'''\n| align=\"center\" | 1/17\n|-\n| 24\n| align=\"center\" | 1/24\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.041'''{{overline|6}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|A}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| align=\"center\" | 1/18\n|-\n| 25\n| align=\"center\" | 1/25\n| align=\"center\" | {{color|#920000|'''5'''}}\n| '''0.04'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0A3D7}}\n| align=\"center\" | {{color|#000092|'''5'''}}\n| align=\"center\" | 1/19\n|-\n| 26\n| align=\"center\" | 1/26\n| align=\"center\" | {{color|#920000|'''2'''}}, '''13'''\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|384615}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|9D8}}\n| align=\"center\" | {{color|#920000|'''2'''}}, '''D'''\n| align=\"center\" | 1/1A\n|-\n| 27\n| align=\"center\" | 1/27\n| align=\"center\" | {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|037}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|097B425ED}}\n| align=\"center\" | {{color|#000092|'''3'''}}\n| align=\"center\" | 1/1B\n|-\n| 28\n| align=\"center\" | 1/28\n| align=\"center\" | {{color|#920000|'''2'''}}, '''7'''\n| bgcolor=#e4e4e4 | '''0.03'''{{overline|571428}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|924}}\n| align=\"center\" | {{color|#920000|'''2'''}}, '''7'''\n| align=\"center\" | 1/1C\n|-\n| 29\n| align=\"center\" | 1/29\n| align=\"center\" | '''29'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|0344827586206896551724137931}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|08D3DCB}}\n| align=\"center\" | '''1D'''\n| align=\"center\" | 1/1D\n|-\n| 30\n| align=\"center\" | 1/30\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}, {{color|#920000|'''5'''}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|3}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|8}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}, {{color|#000092|'''5'''}}\n| align=\"center\" | 1/1E\n|-\n| 31\n| align=\"center\" | 1/31\n| align=\"center\" | '''31'''\n| bgcolor=#e4e4e4 | '''0.'''{{overline|032258064516129}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|08421}}\n| align=\"center\" | '''1F'''\n| align=\"center\" | 1/1F\n|-\n| 32\n| align=\"center\" | 1/32\n| align=\"center\" | {{color|#920000|'''2'''}}\n| '''0.03125'''\n| '''0.08'''\n| align=\"center\" | {{color|#920000|'''2'''}}\n| align=\"center\" | 1/20\n|-\n| 33\n| align=\"center\" | 1/33\n| align=\"center\" | {{color|#000092|'''3'''}}, {{color|#004900|'''11'''}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|03}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|07C1F}}\n| align=\"center\" | {{color|#000092|'''3'''}}, '''B'''\n| align=\"center\" | 1/21\n|-\n| 34\n| align=\"center\" | 1/34\n| align=\"center\" | {{color|#920000|'''2'''}}, '''17'''\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|2941176470588235}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|78}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#004900|'''11'''}}\n| align=\"center\" | 1/22\n|-\n| 35\n| align=\"center\" | 1/35\n| align=\"center\" | {{color|#920000|'''5'''}}, '''7'''\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|285714}}\n| bgcolor=#e4e4e4 | '''0.'''{{overline|075}}\n| align=\"center\" | {{color|#000092|'''5'''}}, '''7'''\n| align=\"center\" | 1/23\n|-\n| 36\n| align=\"center\" | 1/36\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| bgcolor=#e4e4e4 | '''0.02'''{{overline|7}}\n| bgcolor=#e4e4e4 | '''0.0'''{{overline|71C}}\n| align=\"center\" | {{color|#920000|'''2'''}}, {{color|#000092|'''3'''}}\n| align=\"center\" | 1/24\n|}\n\n===Irrational numbers===\nThe table below gives the expansions of some common [[irrational number]]s in decimal and hexadecimal.\n{| class=\"wikitable\"\n! rowspan=2 | Number\n! colspan=2 | Positional representation\n|-\n! Decimal\n! Hexadecimal\n|-\n| [[Square root of 2|{{sqrt|2}}]] {{small|(the length of the [[diagonal]] of a unit [[Square (geometry)|square]])}}\n| {{val|1.414213562373095048}}...\n| 1.6A09E667F3BCD...\n|-\n| [[Square root of 3|{{sqrt|3}}]] {{small|(the length of the diagonal of a unit [[cube]])}}\n| {{val|1.732050807568877293}}...\n| 1.BB67AE8584CAA...\n|-\n| [[Square root of 5|{{sqrt|5}}]] {{small|(the length of the [[diagonal]] of a 1×2 [[rectangle]])}}\n| {{val|2.236067977499789696}}...\n| 2.3C6EF372FE95...\n|-\n| [[Golden ratio|{{mvar|φ}}]] {{small|1=(phi, the [[golden ratio]] = {{math|(1+{{radical|5}})/2}})}}\n| {{val|1.618033988749894848}}...\n| 1.9E3779B97F4A...\n|-\n| [[Pi|{{mvar|π}}]] {{small|(pi, the ratio of [[circumference]] to [[diameter]] of a circle)}}\n| {{val|3.141592653589793238462643}}<br>{{val|383279502884197169399375105}}...\n| 3.243F6A8885A308D313198A2E0<br>3707344A4093822299F31D008...\n|-\n| [[E (mathematical constant)|{{mvar|e}}]] {{small|(the base of the [[natural logarithm]])}}\n| {{val|2.718281828459045235}}...\n| 2.B7E151628AED2A6B...\n|-\n| [[Thue–Morse constant|{{mvar|τ}}]] {{small|(the [[Thue–Morse constant]])}}\n| {{val|0.412454033640107597}}...\n| 0.6996 9669 9669 6996...\n|-\n| [[Euler-Mascheroni constant|{{mvar|γ}}]] {{small|(the limiting difference between the <br/>[[harmonic series (mathematics)|harmonic series]] and the natural logarithm)}}\n| {{val|0.577215664901532860}}...\n| 0.93C467E37DB0C7A4D1B...\n|}\n\n===Powers===\nPowers of two have very simple expansions in hexadecimal. The first sixteen powers of two are shown below.\n{| class=\"wikitable\"\n! 2<sup>''x''</sup> !! Value !! Value (Decimal)\n|-\n| 2<sup>0</sup> || 1 || 1\n|-\n| 2<sup>1</sup> || 2 || 2\n|-\n| 2<sup>2</sup> || 4 || 4\n|-\n| 2<sup>3</sup> || 8 || 8\n|-\n| 2<sup>4</sup> || 10<sub>hex</sub> || 16<sub>dec</sub>\n|-\n| 2<sup>5</sup> || 20<sub>hex</sub> || 32<sub>dec</sub>\n|-\n| 2<sup>6</sup> || 40<sub>hex</sub> || 64<sub>dec</sub>\n|-\n| 2<sup>7</sup> || 80<sub>hex</sub> || 128<sub>dec</sub>\n|-\n| 2<sup>8</sup> || 100<sub>hex</sub> || 256<sub>dec</sub>\n|-\n| 2<sup>9</sup> || 200<sub>hex</sub> || 512<sub>dec</sub>\n|-\n| 2<sup>A</sup> (2{{sup|10{{sub|dec}}}}) || 400<sub>hex</sub> || 1024<sub>dec</sub>\n|-\n| 2<sup>B</sup> (2{{sup|11{{sub|dec}}}}) || 800<sub>hex</sub> || 2048<sub>dec</sub>\n|-\n| 2<sup>C</sup> (2{{sup|12{{sub|dec}}}}) || 1000<sub>hex</sub> || 4096<sub>dec</sub>\n|-\n| 2<sup>D</sup> (2{{sup|13{{sub|dec}}}}) || 2000<sub>hex</sub> || 8192<sub>dec</sub>\n|-\n| 2<sup>E</sup> (2{{sup|14{{sub|dec}}}}) || 4000<sub>hex</sub> || 16,384<sub>dec</sub>\n|-\n| 2<sup>F</sup> (2{{sup|15{{sub|dec}}}}) || 8000<sub>hex</sub> || 32,768<sub>dec</sub>\n|-\n| 2<sup>10</sup> (2{{sup|16{{sub|dec}}}}) || 10000<sub>hex</sub> || 65,536<sub>dec</sub>\n|}\n\n==Cultural==\n\n===Etymology===\nThe word ''hexadecimal'' is composed of ''hexa-'', derived from the [[Greek language|Greek]] ἕξ (hex) for ''six'', and ''-decimal'', derived from the [[Latin]] for ''tenth''. Webster's Third New International online derives ''hexadecimal'' as an alteration of the all-Latin ''sexadecimal'' (which appears in the earlier Bendix documentation). The earliest date attested for ''hexadecimal'' in Merriam-Webster Collegiate online is 1954, placing it safely in the category of [[international scientific vocabulary]] (ISV). It is common in ISV to mix Greek and Latin [[combining form]]s freely. The word ''[[sexagesimal]]'' (for base 60) retains the Latin prefix. [[Donald Knuth]] has pointed out that the etymologically correct term is ''senidenary'' (or possibly, ''sedenary''), from the Latin term for ''grouped by 16''. (The terms ''binary'', ''ternary'' and ''quaternary'' are from the same Latin construction, and the etymologically correct terms for ''decimal'' and ''octal'' arithmetic are ''denary'' and ''octonary'', respectively.)<ref>Knuth, Donald. (1969). ''[[The Art of Computer Programming]], Volume 2''. {{isbn|0-201-03802-1}}. (Chapter 17.)</ref> Alfred B. Taylor used ''senidenary'' in his mid-1800s work on alternative number bases, although he rejected base 16 because of its \"incommodious number of digits\".<ref>Alfred B. Taylor, [https://books.google.com/books?id=X7wLAAAAYAAJ&pg=PP5 Report on Weights and Measures], Pharmaceutical Association, 8th Annual Session, Boston, Sept. 15, 1859.  See pages and 33 and 41.</ref><ref>Alfred B. Taylor, \"Octonary numeration and its application to a system of weights and measures\", [https://books.google.com/books?id=KsAUAAAAYAAJ&pg=PA296 ''Proc Amer. Phil. Soc.'' Vol XXIV], Philadelphia, 1887; pages 296-366.  See pages 317 and 322.</ref> Schwartzman notes that the expected form from usual Latin phrasing would be ''sexadecimal'', but computer hackers would be tempted to shorten that word to ''sex''.<ref>Schwartzman, S. (1994). ''The Words of Mathematics: an etymological dictionary of mathematical terms used in English''. {{isbn|0-88385-511-9}}.</ref> The [[Etymology|etymologically]] proper [[Greek language|Greek]] term would be ''hexadecadic'' / ''ἑξαδεκαδικός'' / ''hexadekadikós'' (although in [[Modern Greek]], ''decahexadic'' / ''δεκαεξαδικός'' / ''dekaexadikos'' is more commonly used). <!--please, do not add anything on \"drome numbers\", nobody outside MathWorld refers to them and it may be a deliberate fake entry-->\n\n===Use in Chinese culture===\nThe traditional [[Chinese units of measurement]] were base-16. For example, one jīn&nbsp;(斤) in the old system equals sixteen [[tael]]s. The [[suanpan]] (Chinese [[abacus]]) can be used to perform hexadecimal calculations such as additions and subtractions<ref>{{Cite web|url=http://totton.idirect.com/soroban/Hex_as/|title=算盤 Hexadecimal Addition & Subtraction on an Chinese Abacus|website=totton.idirect.com|access-date=2019-06-26}}</ref>.\n\n===Primary numeral system===\nAs with the [[duodecimal]] system, there have been occasional attempts to promote hexadecimal as the preferred numeral system. These attempts often propose specific pronunciation and symbols for the individual numerals.<ref>{{cite web\n | url = http://www.hauptmech.com/base42\n | title = Base 4^2 Hexadecimal Symbol Proposal\n}}</ref> Some proposals unify standard measures so that they are multiples of 16.<ref>{{cite web|url=http://www.intuitor.com/hex/|title=Intuitor Hex Headquarters|last=|first=|date=|website=Intuitor|archive-url=|archive-date=|dead-url=|access-date=October 28, 2018}}</ref><ref>{{cite web|url=http://std.dkuug.dk/jtc1/sc2/wg2/docs/n2677|title=A proposal for addition of the six Hexadecimal digits (A-F) to Unicode|last=Niemietz|first=Ricardo Cancho|date=October 21, 2003|website=|archive-url=|archive-date=|dead-url=|access-date=October 28, 2018}}</ref><ref name=\"nystrom\">{{cite book|url=https://books.google.com/books?id=aNYGAAAAYAAJ|title=Project of a New System of Arithmetic, Weight, Measure and Coins: Proposed to be called the Tonal System, with Sixteen to the Base|last=Nystrom|first=John William|publisher=Lippincott|year=1862|isbn=|location=Philadelphia|pages=}}</ref>\n\nAn example of unified standard measures is [[hexadecimal time]], which subdivides a day by 16 so that there are 16 \"hexhours\" in a day.<ref name=\"nystrom\" />\n\n==Base16 (Transfer encoding)==\n'''Base16''' (as a proper name without a space) can also refer to a [[binary to text encoding]] belonging to the same family as [[Base32]], [[Base58]], and [[Base64]].\n\nIn this case, data is broken into 4-bit sequences, and each value (between 0 and 15 inclusively) is encoded using 16 symbols from the [[ASCII]] character set. Although any 16 symbols from the ASCII character set can be used, in practice the ASCII digits '0'-'9' and the letters 'A'-'F' (or the lowercase 'a'-'f') are always chosen in order to align with standard written notation for hexadecimal numbers.\n\nThere are several advantages of Base16 encoding:\n* Most programming languages already have facilities to parse ASCII-encoded hexadecimal\n* Being exactly half a byte, 4-bits is easier to process than the 5 or 6 bits of Base32 and Base64 respectively\n* The symbols 0-9 and A-F are universal in hexadecimal notation, so it is easily understood at a glance without needing to rely on a symbol lookup table\n* Many CPU architectures have dedicated instructions that allow access to a half-byte (otherwise known as a \"[[Nibble]]\"), making it more efficient in hardware than Base32 and Base64\n\nThe main disadvantages of Base16 encoding are:\n* Space efficiency is only 50%, since each 4-bit value from the original data will be encoded as an 8-bit byte. In contrast, Base32 and Base64 encodings have a space efficiency of 63% and 75% respectively.\n* Possible added complexity of having to accept both uppercase and lowercase letters\n\nSupport for Base16 encoding is ubiquitous in modern computing. It is the basis for the [[World Wide Web Consortium|W3C]] standard for [[Percent-encoding|URL Percent Encoding]], where a character is replaced with a percent sign \"%\" and its Base16-encoded form. Most modern programming languages directly include support for formatting and parsing Base16-encoded numbers.\n\n==See also==\n* [[Base32]], [[Base64]] (content encoding schemes)\n*[[Hexadecimal time]]\n*[[IBM hexadecimal floating point]]\n* [[Hex editor]]\n* [[Hex dump]]\n* [[Bailey–Borwein–Plouffe formula]] (BBP)\n* [[Hexspeak]]\n\n==References==\n{{reflist|refs=\n<ref name=\"Savard_2018_CA\">{{cite web |title=Computer Arithmetic |at=The Early Days of Hexadecimal |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2005 |work=quadibloc |url=http://www.quadibloc.com/comp/cp02.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180716102439/http://www.quadibloc.com/comp/cp02.htm |archive-date=2018-07-16}}</ref>\n<ref name=\"Bendix\">{{cite book |title=G15D Programmer's Reference Manual |chapter=2.1.3 Sexadecimal notation |date= |publisher=[[Bendix Computer]], Division of [[Bendix Aviation Corporation]] |location=Los Angeles, CA, USA |page=4 |url=http://bitsavers.trailing-edge.com/pdf/bendix/g-15/G15D_Programmers_Ref_Man.pdf |access-date=2017-06-01 |dead-url=no |archive-url=https://web.archive.org/web/20170601222212/http://bitsavers.trailing-edge.com/pdf/bendix/g-15/G15D_Programmers_Ref_Man.pdf |archive-date=2017-06-01 |quote=This base is used because a group of four bits can represent any one of sixteen different numbers (zero to fifteen). By assigning a symbol to each of these combinations we arrive at a notation called sexadecimal (usually hex in conversation because nobody wants to abbreviate sex). The symbols in the sexadecimal language are the ten decimal digits and, on the G-15 typewriter, the letters u, v, w, x, y and z. These are arbitrary markings; other computers may use different alphabet characters for these last six digits.}}</ref>\n<ref name=\"Illiac-I\">{{cite web |title=ILLIAC Programming - A Guide to the Preparation of Problems For Solution by the University of Illinois Digital Computer |author-first1=S. |author-last1=Gill |author-first2=R. E. |author-last2=Neagher |author-first3=D. E. |author-last3=Muller |author-first4=J. P. |author-last4=Nash |author-first5=J. E. |author-last5=Robertson |author-first6=T. |author-last6=Shapin |author-first7=D. J. |author-last7=Whesler |editor-first=J. P. |editor-last=Nash |edition=Fourth printing. Revised and corrected |date=1956-09-01 |publisher=Digital Computer Laboratory, Graduate College, [[University of Illinois]] |location=Urbana, Illinois, USA |pages=3-2 |url=http://www.textfiles.com/bitsavers/pdf/illiac/ILLIAC/ILLIAC_programming_Sep56.pdf |access-date=2014-12-18 |dead-url=no |archive-url=https://web.archive.org/web/20170531153804/http://www.textfiles.com/bitsavers/pdf/illiac/ILLIAC/ILLIAC_programming_Sep56.pdf |archive-date=2017-05-31}}</ref>\n<ref name=\"RP_1957_LGP-30\">{{cite book |title=ROYAL PRECISION Electronic Computer LGP - 30 PROGRAMMING MANUAL |publisher=[[Royal McBee Corporation]] |location=Port Chester, New York |date=April 1957 |url=http://ed-thelen.org/comp-hist/lgp-30-man.html#R4.13 |access-date=2017-05-31 |dead-url=no |archive-url=https://web.archive.org/web/20170531153004/http://ed-thelen.org/comp-hist/lgp-30-man.html |archive-date=2017-05-31}} (NB. This somewhat odd sequence was from the next six sequential numeric keyboard codes in the [[LGP-30]]'s 6-bit character code.)</ref>\n}}\n\n<!--\n==External links==\n If you're here, you're probably thinking about adding an external link to an online calculator or some such. Some points to keep in mind (from the policy at [[WP:EL]], http://en.wikipedia.org/w/index.php?title=Wikipedia:External_links):\n\n* The \"External links\" section should be kept to a minimum. A lack of external links ... is not a reason to add them.\n* Links to be avoided:\n** Any site that does not provide a unique resource beyond what the article might contain...\n** Links mainly intended to promote a website\n** Sites that are only indirectly related to the article's subject\n\nSince the article is about hexadecimal representation and mentions standard tools for conversion only as a minor example, there is little any external link to an online calculator or converter could possibly add to the reader's knowledge.\n-->\n\n[[Category:Binary arithmetic]]\n[[Category:Hexadecimal numeral system]]\n[[Category:Positional numeral systems]]"
    },
    {
      "title": "IEEE 754",
      "url": "https://en.wikipedia.org/wiki/IEEE_754",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\n\nThe '''IEEE Standard for Floating-Point Arithmetic''' ('''IEEE 754''') is a [[technical standard]]  for [[floating-point arithmetic]] established in 1985 by the [[Institute of Electrical and Electronics Engineers]] (IEEE). The standard [[Floating-point arithmetic#IEEE 754 design rationale|addressed many problems]] found in the diverse floating-point implementations that made them difficult to use reliably and [[Software portability|portably]]. Many hardware [[floating-point unit]]s use the IEEE 754 standard.\n\nThe standard defines:\n* ''arithmetic formats:'' sets of [[Binary code|binary]] and [[decimal]] floating-point data, which consist of finite numbers (including [[signed zero]]s and [[subnormal number]]s), [[infinity|infinities]], and special \"not a number\" values ([[NaN]]s)\n* ''interchange formats:'' encodings (bit strings) that may be used to exchange floating-point data in an efficient and compact form\n* ''rounding rules:'' properties to be satisfied when rounding numbers during arithmetic and conversions\n* ''operations:'' arithmetic and other operations (such as [[trigonometric functions]]) on arithmetic formats\n* ''exception handling:'' indications of exceptional conditions (such as [[division by zero]], overflow, ''etc.'')\n\nThe current version, [[IEEE 754-2008 revision]] published in August 2008, includes nearly all of the original [[IEEE 754-1985]] standard plus [[IEEE 854-1987|IEEE 854-1987 Standard for Radix-Independent Floating-Point Arithmetic]].\n\n==Standard development==\nThe current version, IEEE 754-2008 published in August 2008, is derived from and replaces [[IEEE 754-1985]], the previous version, following a seven-year [[IEEE 754 revision|revision process]], chaired by Dan Zuras and edited by [[Mike Cowlishaw]].\n\nThe international standard '''ISO/IEC/IEEE 60559:2011''' (with content identical to IEEE 754-2008) has been approved for adoption through [[ISO/IEC JTC1|JTC1]]/SC 25 under the ISO/IEEE PSDO Agreement<ref>{{cite web|url=http://grouper.ieee.org/groups/754/email/msg04167.html|title=FW: ISO/IEC/IEEE 60559 (IEEE Std 754-2008)|website=grouper.ieee.org|accessdate=4 April 2018|archive-url=https://web.archive.org/web/20171027190846/http://grouper.ieee.org/groups/754/email/msg04167.html|archive-date=2017-10-27}}</ref> and published.<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=57469|title=ISO/IEC/IEEE 60559:2011 - Information technology -- Microprocessor Systems -- Floating-Point arithmetic|website=www.iso.org|accessdate=4 April 2018}}</ref>\n\nThe binary formats in the original standard are included in the new standard along with three new basic formats, one binary and two decimal.  To conform to the current standard, an implementation must implement at least one of the basic formats as both an arithmetic format and an interchange format.\n\nAs of September 2015, the standard is being revised to incorporate clarifications and errata.<ref>{{cite web|url=http://speleotrove.com/misc/IEEE754-errata.html|title=IEEE 754-2008 errata|website=speleotrove.com|accessdate=4 April 2018}}</ref><ref>{{cite web|url=http://754r.ucbtest.org/|title=Revising ANSI/IEEE Std 754-2008 |website=ucbtest.org|accessdate=4 April 2018}}</ref>\n\n== Formats ==\nAn IEEE 754 ''format'' is a \"set of representations of numerical values and symbols\". A format may also include how the set is encoded.<ref>{{Harvnb|IEEE 754|2008|loc=§2.1.27}}</ref> \n\nA floating-point format is specified by:\n* a base (also called ''radix'') ''b'', which is either 2 (binary) or 10 (decimal) in IEEE 754;\n* a precision ''p'';\n* an exponent range from ''emin'' to ''emax'', with ''emin'' = 1 − ''emax'' for all IEEE 754 formats.\n\nA format comprises:\n* Finite numbers, which can be described by three integers: ''s''&nbsp;= a ''sign'' (zero or one), ''c''&nbsp;= a ''[[significand]]'' (or ''coefficient'') having no more than ''p'' digits when written in base ''b'' (i.e., an integer in the range through 0 to ''b''<sup>''p''</sup>−1), and ''q''&nbsp;= an ''exponent'' such that ''emin'' ≤ ''q''+''p''−1 ≤ ''emax''. The numerical value of such a finite number is {{nowrap|(−1)<sup>''s''</sup> × ''c'' × ''b''<sup>''q''</sup>}}.<ref group=\"lower-alpha\">For example, if the base is 10, the sign is 1 (indicating negative), the significand is 12345, and the exponent is −3, then the value of the number is {{nowrap|(−1)<sup>1</sup> × 12345 × 10<sup>−3</sup>}} = {{nowrap|−1 × 12345 × .001}} = −12.345.</ref> Moreover, there are two zero values, called [[signed zero]]s: the sign bit specifies whether a zero is +0 (positive zero) or −0 (negative zero).\n* Two infinities: +∞ and −∞.\n* Two kinds of [[NaN]] (not-a-number): a quiet NaN (qNaN) and a signaling NaN (sNaN).\n\nFor example, if ''b'' = 10, ''p'' = 7 and ''emax'' = 96, then ''emin'' = −95, the significand satisfies 0 ≤ ''c'' ≤ {{val|9999999|fmt=commas}}, and the exponent satisfies −101 ≤ ''q'' ≤ 90. Consequently, the smallest non-zero positive number that can be represented is 1×10<sup>−101</sup>, and the largest is 9999999×10<sup>90</sup> (9.999999×10<sup>96</sup>), so the full range of numbers is −9.999999×10<sup>96</sup> through 9.999999×10<sup>96</sup>.  The numbers −''b''<sup>1−''emax''</sup> and ''b''<sup>1−''emax''</sup> (here, −1×10<sup>−95</sup> and 1×10<sup>−95</sup>) are the smallest (in magnitude) ''normal numbers''; non-zero numbers between these smallest numbers are called [[subnormal number]]s.\n\n=== Representation and encoding in memory ===\nSome numbers may have several possible exponential format representations. For instance, if ''b''=10 and ''p''=7, −12.345 can be represented by −12345×10<sup>−3</sup>, −123450×10<sup>−4</sup>, and −1234500×10<sup>−5</sup>. However, for most operations, such as arithmetic operations, the result (value) does not depend on the representation of the inputs.\n\nFor the decimal formats, any representation is valid, and the set of these representations is called a ''cohort''. When a result can have several representations, the standard specifies which member of the cohort is chosen.\n\nFor the binary formats, the representation is made unique by choosing the smallest representable exponent allowing the value to be represented exactly. Further, the exponent is not represented directly, but a bias is added so that the smallest representable exponent is represented as 1, with 0 used for subnormal numbers. For numbers with an exponent in the normal range (the exponent field being not all ones or all zeros), the leading bit of the significand will always be 1. Consequently, a leading 1 can be implied rather than explicitly present in the memory encoding, and under the standard the explicitly represented part of the significand will lie between 0 and 1. This rule is called ''leading bit convention'', ''implicit bit convention'', or ''hidden bit convention''.  This rule allows the binary format to have an extra bit of precision. The leading bit convention cannot be used for the subnormal numbers as they have an exponent outside the normal exponent range and scale by the smallest represented exponent as used for the smallest normal numbers.\n\nDue to the possibility of multiple encodings (at least in formats called ''interchange formats''), a NaN may carry other information: a sign bit (which has no meaning, but may be used by some operations) and a ''payload'', which is intended for diagnostic information indicating the source of the NaN (but the payload may have other uses, such as ''NaN-boxing''<ref>{{Cite web |url=https://developer.mozilla.org/en-US/docs/Mozilla/Projects/SpiderMonkey/Internals |title=SpiderMonkey Internals |website=developer.mozilla.org |accessdate=11 March 2018}}</ref><ref>{{Cite book |last1=Klemens |first1=Ben |title=21st Century C: C Tips from the New School |date=September 2014 |publisher=O'Reilly Media, Incorporated |page=160 |url=https://books.google.fr/books?id=ASuiBAAAQBAJ |accessdate=11 March 2018|isbn=9781491904442 }}</ref><ref>{{Cite web |url=https://github.com/zuiderkwast/nanbox |title=zuiderkwast/nanbox: NaN-boxing in C |website=[[GitHub]] |accessdate=11 March 2018}}</ref>).\n\n=== Basic and interchange formats ===\n\nThe standard defines five basic formats that are named for their numeric base and the number of bits used in their interchange encoding. There are three binary floating-point basic formats (encoded with 32, 64 or 128 bits) and two decimal floating-point basic formats (encoded with 64 or 128 bits).  The [[binary32]] and [[binary64]] formats are the ''single'' and ''double'' formats of [[IEEE 754-1985]] respectively. A conforming implementation must fully implement at least one of the basic formats.\n\nThe standard also defines ''[[#Interchange formats|interchange formats]]'', which generalize these basic formats.<ref>{{Harvnb|IEEE 754|2008|loc=§3.6}}</ref> For the binary formats, the leading bit convention is required. The following table summarizes the smallest interchange formats (including the basic ones).\n\n{|class=\"wikitable\"\n! Name                                                   !! Common name        !! Base           !! Significand bits<ref group=\"lower-alpha\">including the implicit bit (which always equals 1 for normal numbers, and 0 for subnormal numbers. This implicit bit is not stored in memory), but not the sign bit.</ref> or digits           !! Decimal digits !! Exponent bits    !! Decimal E max !! Exponent bias<ref name=DAE>{{cite web|last1=Cowlishaw|first1=Mike|title=Decimal Arithmetic Encodings|url=http://speleotrove.com/decimal/decbits.pdf|publisher=IBM|accessdate=6 August 2015}}</ref> !! E min !! E max !! Notes\n|-\n|[[Half-precision floating-point format|binary16]]       ||Half precision      ||align=\"right\"| 2||align=\"right\"| 11||align=\"right\"| 3.31||align=\"right\"| 5   ||align=\"right\"|   4.51||2<sup>4</sup>−1 = 15    ||align=\"right\"|   −14||align=\"right\"|   +15||not basic\n|-\n|[[Single-precision floating-point format|binary32]]     ||Single precision    ||align=\"right\"| 2||align=\"right\"| 24||align=\"right\"| 7.22||align=\"right\"| 8   ||align=\"right\"|  38.23||2<sup>7</sup>−1 = 127   ||align=\"right\"|  −126||align=\"right\"|  +127|| \n|-\n|[[Double-precision floating-point format|binary64]]     ||Double precision    ||align=\"right\"| 2||align=\"right\"| 53||align=\"right\"|15.95||align=\"right\"|11   ||align=\"right\"| 307.95||2<sup>10</sup>−1 = 1023 ||align=\"right\"| −1022||align=\"right\"| +1023|| \n|-\n|[[Quadruple-precision floating-point format|binary128]] ||Quadruple precision ||align=\"right\"| 2||align=\"right\"|113||align=\"right\"|34.02||align=\"right\"|15   ||align=\"right\"|4931.77||2<sup>14</sup>−1 = 16383||align=\"right\"|−16382||align=\"right\"|+16383|| \n|-\n|[[Octuple-precision floating-point format|binary256]] ||Octuple precision ||align=\"right\"| 2||align=\"right\"|237||align=\"right\"|71.34||align=\"right\"|19   ||align=\"right\"|78913.2||2<sup>18</sup>−1 = 262143||align=\"right\"|−262142||align=\"right\"|+262143||not basic \n|-\n|[[Decimal32 floating-point format|decimal32]]           ||                    ||align=\"right\"|10||align=\"right\"|  7||align=\"right\"| 7   ||align=\"right\"| 7.58||align=\"right\"|  96   ||                   101||align=\"right\"|   −95||align=\"right\"|   +96||not basic\n|-\n|[[Decimal64 floating-point format|decimal64]]           ||                    ||align=\"right\"|10||align=\"right\"| 16||align=\"right\"|16   ||align=\"right\"| 9.58||align=\"right\"| 384   ||                   398||align=\"right\"|  −383||align=\"right\"|  +384|| \n|-\n|[[Decimal128 floating-point format|decimal128]]         ||                    ||align=\"right\"|10||align=\"right\"| 34||align=\"right\"|34   ||align=\"right\"|13.58||align=\"right\"|6144   ||                  6176||align=\"right\"| −6143||align=\"right\"| +6144|| \n|-\n|}\n\nNote that in the table above, the minimum exponents listed are for normal numbers; the special [[subnormal number]] representation allows even smaller numbers to be represented (with some loss of precision). For example, the smallest positive number that can be represented in binary64 is 2<sup>−1074</sup>; contributions to the -1074 figure include the -1022 E min and all but one of the 53 significand bits (2<sup>−1022&nbsp;−&nbsp;(53&nbsp;−&nbsp;1)</sup>&nbsp;=&nbsp;2<sup>−1074</sup>).\n\nDecimal digits is ''digits'' × log<sub>10</sub> ''base'', this gives an approximate precision in number of decimal digits.\n\nDecimal E max is ''Emax'' × log<sub>10</sub> ''base'', this gives the maximum decimal exponent.<!--[[User:Kvng/RTH]]-->\n\nAs stated previously, the [[binary32]] and [[binary64]] formats are identical to the [[IEEE_754-1985#Single_precision|single]] and [[IEEE_754-1985#Double_precision|double]] formats respectively of [[IEEE 754-1985]] and are two of the most common formats used today. The figure below shows the absolute precision for both the [[binary32]] and [[binary64]] formats in the range of 10<sup>−12</sup> to 10<sup>+12</sup>. Such a figure can be used to select an appropriate format given the expected value of a number and the required precision.\n\n[[File:IEEE754.png|thumb|none|frame|550px|Precision of binary32 and binary64 in the range 10<sup>−12</sup> to 10<sup>12</sup>.]]\n\n=== Extended and extendable precision formats ===\nThe standard specifies extended and extendable precision formats, which are recommended for allowing a greater precision than that provided by the basic formats.<ref>{{Harvnb|IEEE 754|2008|loc=§3.7}}</ref> An extended precision format extends a basic format by using more precision and more exponent range. An extendable precision format allows the user to specify the precision and exponent range. An implementation may use whatever internal representation it chooses for such formats; all that needs to be defined are its parameters (''b'', ''p'', and ''emax'').  These parameters uniquely describe the set of finite numbers (combinations of sign, significand, and exponent for the given radix) that it can represent.\n\nThe standard does not require an implementation to support extended or extendable precision formats.\n\nThe standard recommends that languages provide a method of specifying ''p'' and ''emax'' for each supported base ''b''.<ref>{{Harvnb|IEEE 754|2008|loc=§3.7}} states, \"Language standards should define mechanisms supporting extendable precision for each supported radix.\"</ref><!-- extendable recommendation does not apply to \"implementations\". -->\n\nThe standard recommends that languages and implementations support an extended format which has a greater precision than the largest basic format supported for each radix ''b''.<ref>{{Harvnb|IEEE 754|2008|loc=§3.7}} states, \"Language standards or implementations should support an extended precision format that extends the widest basic format that is supported in that radix.\"</ref>\n\nFor an extended format with a precision between two basic formats the exponent range must be as great as that of the next wider basic format. So for instance a 64-bit extended precision binary number must have an 'emax' of at least 16383. The [[x87]] [[Extended precision|80-bit extended format]] meets this requirement.\n\n=== Interchange formats ===\nInterchange formats are intended for the exchange of floating-point data using a fixed-length bit-string for a given format.\n\nFor the exchange of binary floating-point numbers, interchange formats of length 16 bits, 32 bits, 64 bits, and any multiple of 32 bits ≥128 are defined.  The 16-bit format is intended for the exchange or storage of small numbers (''e.g.'', for graphics).\n\nThe encoding scheme for these binary interchange formats is the same as that of IEEE 754-1985: a sign bit, followed by ''w'' exponent bits that describe the exponent offset by a ''[[Exponent bias|bias]]'', and ''p''−1 bits that describe the significand.  The width of the exponent field for a ''k''-bit format is computed as ''w''&nbsp;=&nbsp;round(4&nbsp;log<sub>2</sub>(''k''))−13.  The existing 64- and 128-bit formats follow this rule, but the 16- and 32-bit formats have more exponent bits (5 and 8) than this formula would provide (3 and 7, respectively).\n\nAs with IEEE 754-1985, the biased-exponent field is filled with all 1 bits to indicate either infinity (trailing significand field = 0) or a NaN (trailing significand field ≠ 0).  For NaNs, quiet NaNs and signaling NaNs are distinguished by using the most significant bit of the trailing significand field exclusively (the standard recommends 0 for signaling NaNs, 1 for quiet NaNs, so that a signaling NaNs can be quieted by changing only this bit to 1, while the reverse could yield the encoding of an infinity), and the payload is carried in the remaining bits.\n\nFor the exchange of decimal floating-point numbers, interchange formats of any multiple of 32 bits are defined.\n\nThe encoding scheme for the decimal interchange formats similarly encodes the sign, exponent, and significand, but two different bit-level representations are defined.  Interchange is complicated by the fact that some external indicator of the representation in use is required.  The two options allow the significand to be encoded as a compressed sequence of decimal digits (using [[densely packed decimal]]) or alternatively as a binary integer.  The former is more convenient for direct hardware implementation of the standard, while the latter is more suited to software emulation on a binary computer.  In either case the set of numbers (combinations of sign, significand, and exponent) that may be encoded is identical, and [[Floating point#Special values|special values]] (±zero, ±infinity, quiet NaNs, and signaling NaNs) have identical binary representations.\n\n== Rounding rules ==\nThe standard defines five rounding rules.  The first two rules round to a nearest value; the others are called ''[[directed rounding]]s'':\n\n=== Roundings to nearest ===\n* '''[[Rounding#Round half to even|Round to nearest, ties to even]]'''&nbsp;– rounds to the nearest value; if the number falls midway, it is rounded to the nearest value with an even least significant digit; this is the default for binary floating point and the recommended default for decimal.\n* '''[[Rounding#Round half away from zero|Round to nearest, ties away from zero]]'''&nbsp;– rounds to the nearest value; if the number falls midway, it is rounded to the nearest value above (for positive numbers) or below (for negative numbers); this is intended as an option for decimal floating point.\n\n=== Directed roundings ===\n* '''Round toward 0'''&nbsp;– directed rounding towards zero (also known as ''truncation'').\n* '''Round toward +∞'''&nbsp;– directed rounding towards positive infinity (also known as ''rounding up'' or ''ceiling'').\n* '''Round toward −∞'''&nbsp;– directed rounding towards negative infinity (also known as ''rounding down'' or ''floor'').\n\n{| class=\"wikitable\"\n|+ Example of rounding to integers using the IEEE 754 rules\n! Mode    /     Example Value\n! +11.5\n! +12.5\n! −11.5\n! −12.5\n|-\n| to nearest, ties to even\n| +12.0\n| +12.0\n| −12.0\n| −12.0\n|-\n| to nearest, ties away from zero\n| +12.0\n| +13.0\n| −12.0\n| −13.0\n|-\n| toward 0\n| +11.0\n| +12.0\n| −11.0\n| −12.0\n|-\n| toward +∞\n| +12.0\n| +13.0\n| −11.0\n| −12.0\n|-\n| toward −∞\n| +11.0\n| +12.0\n| −12.0\n| −13.0\n|}\n\n== Required operations ==\n\nRequired operations for a supported arithmetic format (including the basic formats) include:\n\n* Arithmetic operations (add, subtract, multiply, divide, square root, [[Multiply–accumulate operation|fused multiply–add]], remainder)<ref>{{Harvnb|IEEE 754|2008|loc=§5.3.1}}</ref><ref>{{Harvnb|IEEE 754|2008|loc=§5.4.1}}</ref>\n* Conversions (between formats, to and from strings, ''etc.'')<ref>{{Harvnb|IEEE 754|2008|loc=§5.4.2}}</ref><ref>{{Harvnb|IEEE 754|2008|loc=§5.4.3}}</ref>\n* Scaling and (for decimal) quantizing<ref>{{Harvnb|IEEE 754|2008|loc=§5.3.2}}</ref><ref>{{Harvnb|IEEE 754|2008|loc=§5.3.3}}</ref>\n* Copying and manipulating the sign (abs, negate, ''etc.'')<ref>{{Harvnb|IEEE 754|2008|loc=§5.5.1}}</ref>\n* Comparisons and total ordering<ref name=total-ordering>{{Harvnb|IEEE 754|2008|loc=§5.10}}</ref><ref>{{Harvnb|IEEE 754|2008|loc=§5.11}}</ref>\n* Classification and testing for NaNs, ''etc.''<ref>{{Harvnb|IEEE 754|2008|loc=§5.7.2}}</ref>\n* Testing and setting flags<ref>{{Harvnb|IEEE 754|2008|loc=§5.7.4}}</ref>\n* Miscellaneous operations.\n\n=== Total-ordering predicate ===\n\nThe standard provides a predicate ''totalOrder'' which defines a [[total order]]ing for all floating-point data for each format. The predicate agrees with the normal comparison operations when one floating-point number is less than the other one. The normal comparison operations, however, treat NaNs as unordered and compare −0 and +0 as equal. The ''totalOrder'' predicate orders all floating-point data strictly and totally. When comparing two floating-point numbers, it acts as the {{math|&le;}} operation, except that {{math|totalOrder(−0, +0) ∧ ¬ totalOrder(+0, −0)}}, and different representations of the same floating-point number are ordered by their exponent multiplied by the sign bit. The ordering is then extended to the NaNs by ordering {{math|&minus;qNaN < &minus;sNaN < numbers < +sNaN < +qNaN}}, with ordering between two NaNs in the same class being based on the integer payload, multiplied by the sign bit, of those data.<ref name=total-ordering/>\n\n== Exception handling ==\nThe standard defines five exceptions, each of which returns a default value and has a corresponding status flag that (except in certain cases of underflow) is raised when the exception occurs.  No other exception handling is required, but additional non-default alternatives are recommended (see below).\n\nThe five possible exceptions are:\n* Invalid operation: mathematically undefined, ''e.g.'', the square root of a negative number. By default, returns qNaN.\n* Division by zero: an operation on finite operands gives an exact infinite result, ''e.g.'', 1/0 or log(0). By default, returns ±infinity.\n* Overflow: a result is too large to be represented correctly (''i.e.'', its exponent with an unbounded exponent range would be larger than ''emax''). By default, returns ±infinity for the round-to-nearest mode (and follows the rounding rules for the directed rounding modes).\n* Underflow: a result is very small (outside the normal range) and is inexact. By default, returns a [[Denormal number|subnormal]] or zero (following the rounding rules).\n* Inexact: the exact (''i.e.'', unrounded) result is not representable exactly. By default, returns the correctly rounded result.\n\nThese are the same five exceptions as were defined in IEEE 754-1985, but the ''division by zero'' exception has been extended to operations other than the division.\n\nFor decimal floating point, there are additional exceptions along with the above:<ref>{{cite web|url=https://docs.python.org/library/decimal.html#signals|title=9.4. decimal — Decimal fixed point and floating point arithmetic — Python 3.6.5 documentation|website=docs.python.org|accessdate=4 April 2018}}</ref><ref>{{cite web|url=http://speleotrove.com/decimal/daexcep.html|title=Decimal Arithmetic - Exceptional conditions|website=speleotrove.com|accessdate=4 April 2018}}</ref>\n* Clamped: a result's exponent is too large for the destination format. By default, trailing zeros will be added to the coefficient to reduce the exponent to the largest usable value. If this is not possible (because this would cause the number of digits needed to be more than the destination format) then overflow occurs.\n* Rounded: a result's coefficient requires more digits than the destination format provides. The inexact is signaled if any non-zero digits are discarded.\n\nAdditionally, operations like quantize when either operand is infinite, or when the result does not fit the destination format, will also signal invalid operation exception.<ref>{{Harvnb|IEEE 754|2008|loc=&sect;7.2(h)}}</ref>\n\n== Recommendations ==\n\n=== Alternate exception handling ===\nThe standard recommends optional exception handling in various forms, including presubstitution of user-defined default values, and traps (exceptions that change the flow of control in some way) and other exception handling models which interrupt the flow, such as try/catch.  The traps and other exception mechanisms remain optional, as they were in IEEE 754-1985.\n\n=== Recommended operations ===\nClause 9 in the standard recommends fifty operations, that language standards should define.<ref>{{Harvnb|IEEE 754|2008|loc=Clause 9}}</ref>  These are all optional (not required in order to conform to the standard).\n\nRecommended arithmetic operations, which must round correctly:<ref>{{Harvnb|IEEE 754|2008|loc=§9.2}}</ref>\n\n* [[Exponential function|<math>e^x</math>]], <math>2^x</math>, <math>10^x</math>\n* [[Exponential function#exp and exmp1|<math>e^x - 1</math>]], <math>2^x - 1</math>, <math>10^x - 1</math>\n* [[Natural logarithm|<math>\\ln x</math>]], [[Binary logarithm|<math>\\log_{2} x</math>]], [[Common logarithm|<math>\\log_{10} x</math>]]\n* [[Natural logarithm#lnp1|<math>\\ln (1 + x)</math>]], <math>\\log_{2} (1 + x)</math>, <math>\\log_{10} (1 + x)</math>\n* [[Hypot|<math>\\sqrt{x^2 + y^2}</math>]]\n* <math>\\sqrt{x}</math>\n* <math>(1 + x)^n</math>\n* <math>x^{\\frac{1}{n}}</math>\n* <math>x^n</math>, <math>x^y</math>\n* [[Sine|<math>\\sin x</math>]], [[Cosine|<math>\\cos x</math>]], [[Tangent|<math>\\tan x</math>]]\n* [[Inverse trigonometric functions|<math>\\arcsin x</math>]], <math>\\arccos x</math>, <math>\\arctan x</math>, [[atan2|<math>\\operatorname{atan2}(y, x)</math>]]\n* <math>\\operatorname{sinPi} x = \\sin \\pi x</math>, <math>\\operatorname{cosPi} x = \\cos \\pi x</math>\n* <math>\\operatorname{atanPi} x = \\frac{\\arctan x}{\\pi}</math>, <math>\\operatorname{atan2Pi} (y, x) = \\frac{\\operatorname{atan2}(y, x)}{\\pi}</math>\n* [[Hyperbolic function|<math>\\sinh x</math>]], <math>\\cosh x</math>, <math>\\tanh x</math>\n* [[Inverse hyperbolic functions|<math>\\operatorname{arsinh} x</math>]], <math>\\operatorname{arcosh} x</math>, <math>\\operatorname{artanh} x</math>\n\nThe {{math|asinPi}}, {{math|acosPi}} and {{math|tanPi}} functions are not part of the standard because the feeling was that they were less necessary.<ref>{{cite web|url=http://grouper.ieee.org/groups/754/email/msg03842.html|deadurl=yes|archive-url=https://web.archive.org/web/20170706053605/http://grouper.ieee.org/groups/754/email/msg03842.html|archive-date=2017-07-06|title=Re: Missing functions tanPi, asinPi and acosPi|website=grouper.ieee.org|accessdate=4 April 2018}}</ref> The first two are mentioned in a paragraph, but this is regarded as an error.<ref>{{citation\n|url=http://speleotrove.com/misc/IEEE754-errata.html\n|title=IEEE 754-2008 errata 2013.12.19\n|first=Mike|last=Cowlishaw\n}}</ref>\n\nThe operations also include setting and accessing dynamic mode rounding direction,<ref>{{Harvnb|IEEE 754|2008|loc=§9.3}}</ref> and implementation-defined vector reduction operations such as sum, scaled product, and [[dot product]], whose accuracy is unspecified by the standard.<ref>{{Harvnb|IEEE 754|2008|loc=§9.4}}</ref>\n\n=== Expression evaluation ===\nThe standard recommends how language standards should specify the semantics of sequences of operations, and points out the subtleties of literal meanings and optimizations that change the value of a result. By contrast, the previous [[IEEE 754-1985|1985]] version of the standard left aspects of the language interface unspecified, which led to inconsistent behavior between compilers, or different optimization levels in a single compiler.\n\nProgramming languages should allow a user to specify a minimum precision for intermediate calculations of expressions for each radix. This is referred to as \"preferredWidth\" in the standard, and it should be possible to set this on a per block basis. Intermediate calculations within expressions should be calculated, and any temporaries saved, using the maximum of the width of the operands and the preferred width, if set. Thus, for instance, a compiler targeting [[x87]] floating-point hardware should have a means of specifying that intermediate calculations must use the [[Extended precision#IEEE 754 extended precision formats|double-extended format]]. The stored value of a variable must always be used when evaluating subsequent expressions, rather than any precursor from before rounding and assigning to the variable.\n\n=== Reproducibility ===\nThe IEEE 754-1985 allowed many variations in implementations (such as the encoding of some values and the detection of certain exceptions).  IEEE 754-2008 has strengthened up many of these, but a few variations still remain (especially for binary formats).  The reproducibility clause recommends that language standards should provide a means to write reproducible programs (''i.e.'', programs that will produce the same result in all implementations of a language), and describes what needs to be done to achieve reproducible results.\n\n== Character representation ==\nThe standard requires operations to convert between basic formats and ''external character sequence'' formats.<ref>{{Harvnb|IEEE 754|2008|loc=§5.12}}</ref> Conversions to and from a decimal character format are required for all formats. Conversion to an external character sequence must be such that conversion back using round to even will recover the original number. There is no requirement to preserve the payload of a NaN or signaling NaN, and conversion from the external character sequence may turn a signaling NaN into a quiet NaN.\n\nThe original binary value will be preserved by converting to decimal and back again using:<ref>{{Harvnb|IEEE 754|2008|loc=§5.12.2}}</ref>\n* 5 decimal digits for binary16\n* 9 decimal digits for binary32\n* 17 decimal digits for binary64\n* 36 decimal digits for binary128\n\nFor other binary formats, the required number of decimal digits is\n\n:<math>1 +\\lceil p\\log_{10}(2)\\rceil</math>\n\nwhere ''p'' is the number of significant bits in the binary format, e.g. 237 bits for binary256.\n\n(Note: as an implementation limit, correct rounding is only guaranteed for the number of decimal digits above plus 3 for the largest supported binary format. For instance, if binary32 is the largest supported binary format, then a conversion from a decimal external sequence with 12 decimal digits is guaranteed to be correctly rounded when converted to binary32; but conversion of a sequence of 13 decimal digits is not; however the standard recommends that implementations impose no such limit.)\n\nWhen using a decimal floating-point format, the decimal representation will be preserved using:\n*7 decimal digits for decimal32\n*16 decimal digits for decimal64\n*34 decimal digits for decimal128\n\nAlgorithms, with code, for correctly rounded conversion from binary to decimal and decimal to binary are discussed in <ref>{{Cite journal |first=David M. |last=Gay |title=Correctly rounded binary-decimal and decimal-binary conversions |series=Numerical Analysis Manuscript |id=90-10 |publisher=AT&T Laboratories |location=Murry Hill, NJ, USA |date=November 30, 1990 |url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.4049 |doi= |ref=harv |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}} }}</ref> and for testing in.<ref>{{Cite journal |title=A Program for Testing IEEE Decimal–Binary Conversion |publisher= |series=Manuscript |first1=Vern |last1=Paxson |first2=William |last2=Kahan |author2-link=William Kahan |date=May 22, 1991 |doi= |ref=harv |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}} |citeseerx = 10.1.1.144.5889}}</ref>\n\n== See also ==\n* [[Coprocessor]]\n* [[C99#IEEE.C2.A0754 floating point support|C99]] for code examples demonstrating  access and use of IEEE 754 features.\n* [[Floating point#IEEE 754: floating point in modern computers|Floating point]], for history, design rationale and example usage of IEEE 754 features.\n* [[Fixed point arithmetic]], for an alternative approach at computation with rational numbers (especially beneficial when the mantissa range is known, fixed, or bound at compile time).\n* [[Half-precision floating-point format|Half precision]]&nbsp;– [[Single precision]]&nbsp;– [[Double precision]]&nbsp;– [[Quadruple precision]]&nbsp;– [[Extended precision]].\n* [[IBM System z9]], the first CPU to implement IEEE 754-2008 decimal arithmetic (using hardware microcode)\n* [[IBM z10 (microprocessor)|IBM z10]], [[IBM z196]], [[IBM zEC12 (microprocessor)|IBM zEC12]], and [[IBM z13 (microprocessor)|IBM z13]], CPUs that implement IEEE 754-2008  decimal arithmetic fully in hardware\n* [[ISO/IEC 10967]] Language independent arithmetic (LIA)\n* [[Minifloat]], low-precision binary floating-point formats following IEEE 754 principles\n* [[POWER6]], [[POWER7]], and [[POWER8]] CPUs that implement IEEE 754-2008  decimal arithmetic fully in hardware\n* [[strictfp]], a keyword in the [[Java (programming language)|Java programming language]] that restricts arithmetic to IEEE 754 single and double precision to ensure reproducibility across common hardware platforms.\n* [[Rounding#Table-maker's dilemma|The table-maker's dilemma]] for more about the correct rounding of functions.\n* [[Tapered floating point]]\n\n== Notes ==\n{{Reflist|group=\"lower-alpha\"}}\n\n== References ==\n{{Reflist|30em}}\n\n=== Standard ===\n* {{Cite book |title=IEEE Standard for Floating-Point Arithmetic |journal=IEEE Std 754-2008 |pages=1–70 |author=IEEE Computer Society |date=August 29, 2008 |publisher=IEEE |id=IEEE Std 754-2008 |url=https://ieeexplore.ieee.org/document/4610935 |doi=10.1109/IEEESTD.2008.4610935 |ref=CITEREFIEEE_7542008 |isbn=978-0-7381-5753-5 |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}} }}\n* [https://www.iso.org/standard/57469.html ISO/IEC/IEEE 60559:2011]\n\n=== Secondary references ===\n* [http://speleotrove.com/decimal Decimal floating-point] arithmetic, FAQs, bibliography, and links\n* [http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm Comparing binary floats]\n* [http://babbage.cs.qc.cuny.edu/IEEE-754.old/References.xhtml IEEE 754 Reference Material]\n* [http://speleotrove.com/decimal/854mins.html IEEE 854-1987]&nbsp;– History and minutes\n* [https://web.archive.org/web/20171230124220/http://grouper.ieee.org/groups/754/reading.html Supplementary readings for IEEE 754]. Includes historical perspectives.\n\n== Further reading ==\n* {{cite journal |first=David |last=Goldberg |author-link=David Goldberg (PARC) |title=What Every Computer Scientist Should Know About Floating-Point Arithmetic |journal=[[ACM Computing Surveys]] |date=March 1991 |volume=23 |issue=1 |pages=5–48 |doi=10.1145/103162.103163 |url=https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html |access-date=2019-03-08}}\n* {{cite journal\n | first = Chris\n | last = Hecker\n |     authorlink = Chris Hecker\n |      title = Let's Get To The (Floating) Point\n |    journal = Game Developer Magazine\n |date=February 1996\n |      pages = 19–24\n |       issn = 1073-922X\n |        url = http://chrishecker.com/images/f/fb/Gdmfp.pdf\n | accessdate =\n |      quote =\n  }}\n*{{cite journal\n |     first = Charles\n | last = Severance\n |      title = IEEE 754: An Interview with William Kahan\n |    journal = [[IEEE Computer]]\n |date=March 1998\n |     volume = 31\n |      issue = 3\n |      pages = 114–115\n |        doi = 10.1109/MC.1998.660194\n |        url = http://www.dr-chuck.com/dr-chuck/papers/columns/r3114.pdf\n | accessdate = 8 March 2019\n |      quote =\n }}\n* {{cite book\n | first = Mike\n | last = Cowlishaw\n |     authorlink = Mike Cowlishaw\n |      title = Decimal Floating-Point: Algorism for Computers\n |    journal = Proceedings 16th IEEE Symposium on Computer Arithmetic\n |date=June 2003\n |      pages = 104–111\n |       isbn = 978-0-7695-1894-7\n |        url = http://speleotrove.com/decimal/IEEE-cowlishaw-arith16.pdf\n | accessdate = 14 November 2014\n |      quote =\n |     publisher = IEEE Computer Society\n |     location = Los Alamitos, Calif.\n }}.  (Note: ''Algorism'' is not a misspelling of the title; see also [[algorism]].)\n* {{cite journal\n |     first = David\n |  last = Monniaux\n |      title = The pitfalls of verifying floating-point computations\n |    journal = [[ACM Transactions on Programming Languages and Systems]]\n |date=May 2008\n |      pages = 1–41\n |     volume = 30\n |      issue = 3\n |        doi = 10.1145/1353445.1353446\n |       issn = 0164-0925\n |        url = http://hal.archives-ouvertes.fr/hal-00128124/en/\n |arxiv = cs/0701192\n }}: A compendium of non-intuitive behaviours of floating-point on popular architectures, with implications for program verification and testing.\n* {{cite book |author-last1=Muller |author-first1=Jean-Michel |author-last2=Brunie |author-first2=Nicolas |author-last3=de Dinechin |author-first3=Florent |author-last4=Jeannerod |author-first4=Claude-Pierre |author-first5=Mioara |author-last5=Joldes |author-last6=Lefèvre |author-first6=Vincent |author-last7=Melquiond |author-first7=Guillaume |author-last8=Revol |author-first8=Nathalie |author-last9=Torres |author-first9=Serge |title=Handbook of Floating-Point Arithmetic |date=2018 |orig-year=2010 |publisher=[[Birkhäuser]] |edition=2 |isbn=978-3-319-76525-9 |doi=10.1007/978-3-319-76526-6}}\n* {{cite book |first=Michael L. |last=Overton |title=Numerical Computing with IEEE Floating Point Arithmetic |year=2001 |location=[[Courant Institute of Mathematical Sciences]], [[New York University]], New York, USA |doi=10.1137/1.9780898718072 |edition=1 |publisher=[[Society for Industrial and Applied Mathematics|SIAM]] |publication-place=Philadelphia, USA |isbn=978-0-89871-482-1\n|id=978-0-89871-571-2, 0-89871-571-7 }}\n* [http://blogs.mathworks.com/cleve/2014/07/07/floating-point-numbers/ Cleve Moler on Floating Point numbers]\n* {{cite book |author-first=Nelson H. F. |author-last=Beebe |title=The Mathematical-Function Computation Handbook - Programming Using the MathCW Portable Software Library |date=2017-08-22 |location=Salt Lake City, UT, USA |publisher=[[Springer International Publishing AG]] |edition=1 |lccn=2017947446 |isbn=978-3-319-64109-6 |doi=10.1007/978-3-319-64110-2 }}\n\n== External links ==\n{{wikibooks\n| Floating Point | Special Numbers\n| special numbers specified in the IEEE 754 standard\n}}\n{{Commons category|IEEE 754}}\n*IEEE pages: [https://ieeexplore.ieee.org/document/30711 754-1985 - IEEE Standard for Binary Floating-Point Arithmetic], [https://ieeexplore.ieee.org/document/4610935 754-2008 - IEEE Standard for Floating-Point Arithmetic]\n*[https://babbage.cs.qc.cuny.edu/IEEE-754/ Online IEEE 754 binary calculators]\n{{IEEE standards}}\n{{List of IEC standards}}\n{{use dmy dates|date=January 2012}}\n[[Category:Computer arithmetic]]\n[[Category:IEEE standards]]\n[[Category:Floating point types]]\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Leading-one detector",
      "url": "https://en.wikipedia.org/wiki/Leading-one_detector",
      "text": "{{unreferenced|date=December 2017}}\nA '''leading-one detector''' is an [[electronic circuit]] commonly found in [[central processing unit]]s and especially their [[arithmetic logic unit]]s (ALUs). It is used to detect whether the leading bit in a [[computer word]] is 1 or 0, which is used to perform basic binary tests like <code>IF A>0</code>.\n\n{{compu-stub}}\n\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Least significant bit",
      "url": "https://en.wikipedia.org/wiki/Least_significant_bit",
      "text": "#REDIRECT [[Bit numbering#Least significant bit]] {{R from merge}} {{R to section}}\n\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Linear-feedback shift register",
      "url": "https://en.wikipedia.org/wiki/Linear-feedback_shift_register",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Type of shift register in computing}}\n{{Redirect|LFSR|the airport using that ICAO code|Reims – Champagne Air Base}}\n{{Multiple issues|\n{{Refimprove|date=March 2009}}\n{{More footnotes|date=April 2009}}\n}}\n\nIn [[computing]], a '''linear-feedback shift register''' ('''LFSR''') is a [[shift register]] whose input bit is a [[Linear#Boolean functions|linear function]] of its previous state.\n\nThe most commonly used linear function of single bits is [[exclusive-or]] (XOR). Thus, an LFSR is most often a shift register whose input bit is driven by the XOR of some bits of the overall shift register value.\n\nThe initial value of the LFSR is called the seed, and because the operation of the register is deterministic, the stream of values produced by the register is completely determined by its current (or previous) state. Likewise, because the register has a finite number of possible states, it must eventually enter a repeating cycle. However, an LFSR with a [[Primitive polynomial (field theory)|well-chosen feedback function]] can produce a sequence of bits that appears random and has a [[Maximal length sequence|very long cycle]].\n\nApplications of LFSRs include generating [[Pseudorandomness|pseudo-random numbers]], [[Pseudorandom noise|pseudo-noise sequences]], fast digital counters, and [[whitening sequences]]. Both hardware and software implementations of LFSRs are common.\n\nThe mathematics of a [[cyclic redundancy check]], used to provide a quick check against transmission errors, are closely related to those of an LFSR.<ref>{{Cite web|url=https://www.ti.com/lit/an/spra530/spra530.pdf|title=Cyclic Redundancy Check Computation: An Implementation Using the TMS320C54x|last=Geremia|first=Patrick|date=|website=|publisher=Texas Instruments|page=6|access-date=October 16, 2016}}</ref>\n\n== Fibonacci LFSRs ==\n[[File:LFSR-F16.svg|thumb|right|351 px|A 16-bit [[Fibonacci]] LFSR. The feedback tap numbers shown correspond to a primitive polynomial in the table, so the register cycles through the maximum number of 65535 states excluding the all-zeroes state. The state shown, 0xACE1 ([[hexadecimal]]) will be followed by 0x5670. ]]\nThe bit positions that affect the next state are called the taps. In the diagram the taps are [16,14,13,11]. The rightmost bit of the LFSR is called the output bit. The taps are XOR'd sequentially with the output bit and then fed back into the leftmost bit. The sequence of bits in the rightmost position is called the output stream.\n* The bits in the LFSR state that influence the input are called ''taps''.\n* A maximum-length LFSR produces an [[maximum length sequence|m-sequence]] (i.e., it cycles through all possible 2<sup>''m''</sup>&nbsp;−&nbsp;1 states within the shift register except the state where all bits are zero), unless it contains all zeros, in which case it will never change.\n* As an alternative to the XOR-based feedback in an LFSR, one can also use [[XNOR]].<ref>[http://www.xilinx.com/support/documentation/application_notes/xapp210.pdf Linear Feedback Shift Registers in Virtex Devices]</ref> This function is an [[affine transformation|affine map]], not strictly a [[linear map]], but it results in an equivalent polynomial counter whose state is the complement of the state of an LFSR. A state with all ones is illegal when using an XNOR feedback, in the same way as a state with all zeroes is illegal when using XOR. This state is considered illegal because the counter would remain \"locked-up\" in this state.\n\nThe sequence of numbers generated by an LFSR or its XNOR counterpart can be considered a [[binary numeral system]] just as valid as [[Gray code]] or the natural binary code.\n<!-- perhaps this statement should be moved to the [[binary numeral system]] article ? -->\n\nThe arrangement of taps for feedback in an LFSR can be expressed in [[finite field arithmetic]] as a [[polynomial]] [[modular arithmetic|mod]] 2. This means that the coefficients of the polynomial must be 1s or 0s. This is called the feedback polynomial or reciprocal characteristic polynomial. For example, if the taps are at the 16th, 14th, 13th and 11th bits (as shown), the feedback polynomial is\n\n:<math>x^{16} + x^{14} + x^{13} + x^{11} + 1.</math>\n\nThe \"one\" in the polynomial does not correspond to a tap – it corresponds to the input to the first bit (i.e. ''x''<sup>0</sup>, which is equivalent to 1). The powers of the terms represent the tapped bits, counting from the left. The first and last bits are always connected as an input and output tap respectively.\n[[File:31 bit Fibonacci linear feedback shift register.webm|thumb|400x400px|A Fibonacci 31 bit linear feedback shift register with taps at positions 28 and 31, giving it a maximum cycle and period at this speed of nearly 6.7 years. The circuit uses 4x74HC565N for the shift registers, a 74HC86N for the XOR and an inverter, and an LMC555 timer for clock pulses.]]\nThe LFSR is maximal-length if and only if the corresponding feedback polynomial is [[primitive polynomial (field theory)|primitive]]. This means that the following conditions are necessary (but not sufficient):\n* The number of taps is [[Even and odd numbers|even]].\n* The set of taps is [[coprime integers#Coprimality in sets|setwise co-prime]]; i.e., there must be no divisor other than 1 common to all taps.\n\nTables of primitive polynomials from which maximum-length LFSRs can be constructed are given below and in the references.\n\nThere can be more than one maximum-length tap sequence for a given LFSR length. Also, once one maximum-length tap sequence has been found, another automatically follows. If the tap sequence in an ''n''-bit LFSR is {{nobr|[''n'', ''A'', ''B'', ''C'', 0]}}, where the 0 corresponds to the ''x''<sup>0</sup>&nbsp;=&nbsp;1 term, then the corresponding \"mirror\" sequence is {{nobr|[''n'', ''n'' − ''C'', ''n'' − ''B'', ''n'' − ''A'', 0]}}. So the tap sequence {{nobr|[32, 22, 2, 1, 0]}} has as its counterpart {{nobr|[32, 31, 30, 10, 0]}}. Both give a maximum-length sequence.\n\nAn example in [[C_(programming_language)|C]] is below:\n\n<source lang=\"c\">\n# include <stdint.h>\nunsigned lfsr1(void)\n{\n    uint16_t start_state = 0xACE1u;  /* Any nonzero start state will work. */\n    uint16_t lfsr = start_state;\n    uint16_t bit;                    /* Must be 16-bit to allow bit<<15 later in the code */\n    unsigned period = 0;\n\n    do\n    {   /* taps: 16 14 13 11; feedback polynomial: x^16 + x^14 + x^13 + x^11 + 1 */\n        bit = ((lfsr >> 0) ^ (lfsr >> 2) ^ (lfsr >> 3) ^ (lfsr >> 5)) /* & 1u */;\n        lfsr = (lfsr >> 1) | (bit << 15);\n        ++period;\n    }\n    while (lfsr != start_state);\n\n    return period;\n}\n</source>\n\nIf a fast [[parity function|parity]] or [[popcount]] operation is available, the feedback bit can be computed more efficiently as the [[dot product]] of the register with the characteristic polynomial:\n<source lang=\"c\">bit = parity(lfsr & 0x002Du);</source>\nor\n<source lang=\"c\">bit = popcnt(lfsr & 0x002Du) /* & 1u */;</source>\n\n\nIf a rotation operation is available, the new state can be computed more efficiently as\n<source lang=\"c\">lfsr = rotateright((lfsr & ~1u) | (bit & 1u), 1);</source>\nor the equivalent\n<source lang=\"c\">bit ^= lfsr, bit &= 1u, bit ^= lfsr, lfsr = rotateright(bit, 1);</source>\n\nThis LFSR configuration is also known as '''standard''', '''many-to-one''' or '''external XOR gates'''. The alternative Galois configuration is described in the next section.\n\n== Galois LFSRs ==\n[[File:LFSR-G16.svg|thumb|right|393 px|A 16-bit Galois LFSR. The register numbers above correspond to the same primitive polynomial as the Fibonacci example but are counted in reverse to the shifting direction. This register also cycles through the maximal number of 65535 states excluding the all-zeroes state. The state ACE1 hex shown will be followed by E270 hex.]]\n\nNamed after the French mathematician [[Évariste Galois]], an LFSR in Galois configuration, which is also known as '''modular''', '''internal XORs''', or '''one-to-many LFSR''', is an alternate structure that can generate the same output stream as a conventional LFSR (but offset in time).<ref>\n{{cite book\n  |last1 = Press\n  |first1 = William\n  |last2 = Teukolsky\n  |first2 = Saul\n  |last3 = Vetterling\n  |first3 = William\n  |last4 = Flannery\n  |first4 = Brian\n  |title = Numerical Recipes: The Art of Scientific Computing, Third Edition\n  |publisher = [[Cambridge University Press]]\n  |year = 2007\n  |page = 386\n  |isbn = 978-0-521-88407-5\n}}\n</ref> In the Galois configuration, when the system is clocked, bits that are not taps are shifted one position to the right unchanged. The taps, on the other hand, are XORed with the output bit before they are stored in the next position. The new output bit is the next input bit. The effect of this is that when the output bit is zero, all the bits in the register shift to the right unchanged, and the input bit becomes zero. When the output bit is one, the bits in the tap positions all flip (if they are 0, they become 1, and if they are 1, they become 0), and then the entire register is shifted to the right and the input bit becomes 1.\n\nTo generate the same output stream, the order of the taps is the ''counterpart'' (see above) of the order for the conventional LFSR, otherwise the stream will be in reverse. Note that the internal state of the LFSR is not necessarily the same. The Galois register shown has the same output stream as the Fibonacci register in the first section. A time offset exists between the streams, so a different startpoint will be needed to get the same output each cycle.\n* Galois LFSRs do not concatenate every tap to produce the new input (the XORing is done within the LFSR, and no XOR gates are run in serial, therefore the propagation times are reduced to that of one XOR rather than a whole chain), thus it is possible for each tap to be computed in parallel, increasing the speed of execution.\n* In a software implementation of an LFSR, the Galois form is more efficient, as the XOR operations can be implemented a word at a time: only the output bit must be examined individually.\n\nBelow is a [[C (programming language)|C]] code example for the 16-bit maximal-period Galois LFSR example in the figure:\n<source lang=\"c\">\n# include <stdint.h>\nunsigned lfsr2(void)\n{\n    uint16_t start_state = 0xACE1u;  /* Any nonzero start state will work. */\n    uint16_t lfsr = start_state;\n    unsigned period = 0;\n\n    do\n    {\n#ifndef LEFT\n        unsigned lsb = lfsr & 1u;  /* Get LSB (i.e., the output bit). */\n        lfsr >>= 1;                /* Shift register */\n        if (lsb)                   /* If the output bit is 1, */\n            lfsr ^= 0xB400u;       /*  apply toggle mask. */\n#else\n        unsigned msb = (int16_t) lfsr < 0;   /* Get MSB (i.e., the output bit). */\n        lfsr <<= 1;                          /* Shift register */\n        if (msb)                             /* If the output bit is 1, */\n            lfsr ^= 0x002Du;                 /*  apply toggle mask. */\n#endif\n        ++period;\n    }\n    while (lfsr != start_state);\n\n    return period;\n}\n</source>\n\nNote that\n<source lang=\"c\">\n        if (lsb)\n            lfsr ^= 0xB400u;\n</source>\ncan also be written as\n<source lang=\"c\">\n        lfsr ^= (-lsb) & 0xB400u;\n</source>\nwhich may produce more efficient code on some compilers; the left-shifting variant may produce even better code: the [[most significant bit|msb]] is the [[Carry flag|carry]] from the addition of <code>lfsr</code> to itself.\n<!-- NOTE: The C standard guarantees that arithmetic operations on unsigned types are computed modulo 2^bitsize (i.e., as if in two's complement arithmetic). Thus, the \"-lsb\" is fully portable and gives the intended result even if the target environment uses natively a different integer representation. -->\n\n=== Non-binary Galois LFSR ===\nBinary Galois LFSRs like the ones shown above can be generalized to any ''q''-ary alphabet {0, 1, ..., ''q''&nbsp;−&nbsp;1} (e.g., for binary, ''q'' = 2, and the alphabet is simply {0, 1}). In this case, the exclusive-or component is generalized to addition [[Modular arithmetic|modulo]]-''q'' (note that XOR is addition modulo 2), and the feedback bit (output bit) is multiplied (modulo-''q'') by a ''q''-ary value, which is constant for each specific tap point. Note that this is also a generalization of the binary case, where the feedback is multiplied by either 0 (no feedback, i.e., no tap) or 1 (feedback is present). Given an appropriate tap configuration, such LFSRs can be used to generate [[Finite field|Galois fields]] for arbitrary prime values of ''q''.\n\n== [[Xorshift]] LFSRs ==\n{{ main | Xorshift }}\nAs shown by [[George Marsaglia]]<ref name=\"marsaglia\">{{cite journal\n| first=George | last=Marsaglia | authorlink=George Marsaglia\n| title=Xorshift RNGs\n| journal=[[Journal of Statistical Software]]\n| volume=8 | issue=14\n| date=July 2003\n| url=https://www.jstatsoft.org/v08/i14/paper | doi=10.18637/jss.v008.i14}}</ref> and further analysed by [[Richard P. Brent]]<ref name=\"brent\">{{cite journal\n| first=Richard P. | last=Brent | authorlink=Richard P. Brent\n| title=Note on Marsaglia’s Xorshift Random Number Generators\n| journal=[[Journal of Statistical Software]]\n| volume=11 | issue=5\n| date=August 2004\n| url=https://www.jstatsoft.org/v11/i05/paper | doi=10.18637/jss.v011.i05}}</ref>, linear feedback shift registers can be implemented efficiently using XOR and Shift operations.\n\nBelow is a [[C (programming language)|C]] code example for a 16-bit maximal-period Xorshift LFSR:\n<source lang=\"c\">\n# include <stdint.h>\nunsigned lfsr3(void)\n{\n    uint16_t start_state = 0xACE1u;  /* Any nonzero start state will work. */\n    uint16_t lfsr = start_state;\n    unsigned period = 0;\n\n    do\n    {\n        lfsr ^= lfsr >> 7;\n        lfsr ^= lfsr << 9;\n        lfsr ^= lfsr >> 13;\n        ++period;\n    }\n    while (lfsr != start_state);\n\n    return period;\n}\n</source>\n\n== Matrix forms ==\nBinary LFSRs of both Fibonacci and Galois configurations can be expressed as linear functions using matrices in <math>\\mathbb{F}_2</math>.<ref>{{Cite book|title=Stream Ciphers|chapter=Linear Feedback Shift Registers|last=Klein|first=A.|year=2013|pages=17–18|publisher=Springer|location=London|doi=10.1007/978-1-4471-5079-4_2|isbn=978-1-4471-5079-4|chapter-url=https://pdfs.semanticscholar.org/0488/8883afd08ffbc4b6e8eb4d3c2f9d2182adae.pdf}}</ref> Using the [[companion matrix]] of the characteristic polynomial of the LFSR and denoting the seed as a column vector <math>(a_0, a_1, \\dots, a_{n-1})^\\mathrm{T}</math>, the state of the register in Fibonacci configuration after <math>k</math> steps is given by\n\n:<math>\\begin{pmatrix} a_{k} \\\\ a_{k+1} \\\\ a_{k+2} \\\\ \\vdots \\\\ a_{k+n-1} \\end{pmatrix} =\n\\begin{pmatrix} 0 & 1 & 0 & \\cdots & 0 \\\\ 0 & 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{0} & c_{1} & c_{2} & \\cdots & c_{n-1} \\end{pmatrix}\n\\begin{pmatrix} a_{k-1} \\\\ a_{k} \\\\ a_{k+1} \\\\ \\vdots \\\\ a_{k+n-2} \\end{pmatrix} =\n\\begin{pmatrix} 0 & 1 & 0 & \\cdots & 0 \\\\ 0 & 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{0} & c_{1} & c_{2} & \\cdots & c_{n-1} \\end{pmatrix}^k\n\\begin{pmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_{n-1} \\end{pmatrix}</math>\n\nFor the Galois form, we have\n\n:<math>\\begin{pmatrix} a_{k} \\\\ a_{k+1} \\\\ a_{k+2} \\\\ \\vdots \\\\ a_{k+n-1} \\end{pmatrix} =\n\\begin{pmatrix} c_{0} & 1 & 0 & \\cdots & 0 \\\\ c_{1} & 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{n-1} & 0 & 0 & \\cdots & 0 \\end{pmatrix}\n\\begin{pmatrix} a_{k-1} \\\\ a_{k} \\\\ a_{k+1} \\\\ \\vdots \\\\ a_{k+n-2} \\end{pmatrix} =\n\\begin{pmatrix} c_{0} & 1 & 0 & \\cdots & 0 \\\\ c_{1} & 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{n-1} & 0 & 0 & \\cdots & 0 \\end{pmatrix}^k\n\\begin{pmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_{n-1} \\end{pmatrix}</math>\n\nThese forms generalize naturally to arbitrary fields.\n\n== Some polynomials for maximal LFSRs ==\nThe following table lists maximal-length polynomials for shift-register lengths up to 24.  Note that more than one maximal-length polynomial may exist for any given shift-register length. A list of alternative maximal-length polynomials for shift-register lengths 4–32 (beyond which it becomes unfeasible to store or transfer them) can be found here: http://www.ece.cmu.edu/~koopman/lfsr/index.html.\n<!-- Since alternative polynomials are possible I have verified independently those tabulated here. ~~~~ -->\n{|class=\"wikitable\" style=\"text-align:right\"\n|-\n!Bits (n)\n!Feedback polynomial\n!Period (<math>2^n - 1</math>)\n|-\n! 2\n|<math>x^2 + x + 1</math>\n| 3\n|-\n! 3\n|<math>x^3 + x^2 + 1</math>\n| 7\n|-\n! 4\n|<math>x^4 + x^3 + 1</math>\n| 15\n|-\n! 5\n|<math>x^{ 5 }+x^{ 3 }+1</math>\n| 31\n|-\n! 6\n|<math>x^{ 6 }+x^{ 5 }+1</math>\n| 63\n|-\n! 7\n|<math>x^{ 7 }+x^{ 6 }+1</math>\n| 127\n|-\n! 8\n|<math>x^{ 8 }+x^{ 6 }+x^{ 5 }+x^{ 4 }+1</math>\n| 255\n|-\n! 9\n|<math>x^{ 9 }+x^{ 5 }+1</math>\n| 511\n|-\n! 10\n|<math>x^{ 10 }+x^{ 7 }+1</math>\n| 1,023\n|-\n! 11\n|<math>x^{ 11 }+x^{ 9 }+1</math>\n| 2,047\n|-\n! 12\n|<math>x^{ 12 }+x^{ 11 }+x^{ 10 }+x^{ 4 }+1</math>\n| 4,095\n|-\n! 13\n|<math>x^{ 13 }+x^{ 12 }+x^{ 11 }+x^{ 8 }+1</math>\n| 8,191\n|-\n! 14\n|<math>x^{ 14 }+x^{ 13 }+x^{ 12 }+x^{ 2 }+1</math>\n| 16,383\n|-\n! 15\n|<math>x^{ 15 }+x^{ 14 }+1</math>\n| 32,767\n|-\n! 16\n|<math>x^{ 16 }+x^{ 15 }+x^{ 13 }+x^{ 4 }+1</math>\n| 65,535\n|-\n! 17\n|<math>x^{ 17 }+x^{ 14 }+1</math>\n| 131,071\n|-\n! 18\n|<math>x^{ 18 }+x^{ 11 }+1</math>\n| 262,143\n\n|-\n! 19\n|<math>x^{ 19 }+x^{ 18 }+x^{ 17 }+x^{ 14 }+1</math>\n| 524,287\n|-\n! 20\n|<math>x^{ 20 }+x^{ 17 }+1</math>\n| 1,048,575\n|-\n! 21\n|<math>x^{ 21 }+x^{ 19 }+1</math>\n| 2,097,151\n|-\n! 22\n|<math>x^{ 22 }+x^{ 21 }+1</math>\n| 4,194,303\n|-\n! 23\n|<math>x^{ 23 }+x^{ 18 }+1</math>\n| 8,388,607\n|-\n! 24\n|<math>x^{ 24 }+x^{ 23 }+x^{ 22 }+x^{ 17 }+1</math>\n| 16,777,215\n|-\n!3–168\n|[http://www.xilinx.com/support/documentation/application_notes/xapp052.pdf]\n|\n|-\n!2–786,<br>1024,<br>2048,<br>4096\n|[https://web.archive.org/web/20161007061934/http://courses.cse.tamu.edu/csce680/walker/lfsr_table.pdf]\n|\n|}\n\n== Output-stream properties ==\n* Ones and zeroes occur in \"runs\". The output stream 1110010, for example, consists of four runs of lengths 3, 2, 1, 1, in order. In one period of a maximal LFSR, 2<sup>''n''−1</sup> runs occur (in the example above, the 3-bit LFSR has 4 runs). Exactly half of these runs are one bit long, a quarter are two bits long, up to a single run of zeroes ''n''&nbsp;−&nbsp;1 bits long, and a single run of ones ''n'' bits long. This distribution almost equals the statistical [[Expected value|expectation value]] for a truly random sequence. However, the probability of finding exactly this distribution in a sample of a truly random sequence is rather low{{vague|date=April 2013}}.\n* LFSR output streams are [[deterministic]]. If the present state and the positions of the XOR gates in the LFSR are known, the next state can be predicted.<ref name=\"xilinx.com\">http://www.xilinx.com/support/documentation/application_notes/xapp052.pdf</ref> This is not possible with truly random events. With maximal-length LFSRs, it is much easier to compute the next state, as there are only an easily limited number of them for each length.\n* The output stream is reversible; an LFSR with mirrored taps will cycle through the output sequence in reverse order.\n* The value consisting of all zeros cannot appear. Thus an LFSR of length ''n'' cannot be used to generate all 2<sup>''n''</sup> values.\n\n== Applications ==\nLFSRs can be implemented in hardware, and this makes them useful in applications that require very fast generation of a pseudo-random sequence, such as [[direct-sequence spread spectrum]] radio. LFSRs have also been used for generating an approximation of [[white noise]] in various [[programmable sound generator]]s.\n\n=== Uses as counters ===\nThe repeating sequence of states of an LFSR allows it to be used as a [[clock divider]] or as a counter when a non-binary sequence is acceptable, as is often the case where computer index or framing locations need to be machine-readable.<ref name=\"xilinx.com\"/> LFSR [[Counter (digital)|counter]]s have simpler feedback logic than natural binary counters or [[Gray-code counter]]s, and therefore can operate at higher clock rates. However, it is necessary to ensure that the LFSR never enters an all-zeros state, for example by presetting it at start-up to any other state in the sequence.\nThe table of primitive polynomials shows how LFSRs can be arranged in Fibonacci or Galois form to give maximal periods. One can obtain any other period by adding to an LFSR that has a longer period some logic that shortens the sequence by skipping some states.\n\n=== Uses in cryptography ===\nLFSRs have long been used as [[pseudo-random number generator]]s for use in [[stream cipher]]s (especially in [[military]] [[cryptography]]), due to the ease of construction from simple [[electromechanical]] or [[electronic circuits]], long [[periodic function|periods]], and very uniformly [[probability distribution|distributed]] output streams. However, an LFSR is a linear system, leading to fairly easy [[cryptanalysis]]. For example, given a stretch of [[known-plaintext attack|known plaintext and corresponding ciphertext]], an attacker can intercept and recover a stretch of LFSR output stream used in the system described, and from that stretch of the output stream can construct an LFSR of minimal size that simulates the intended receiver by using the [[Berlekamp-Massey algorithm]]. This LFSR can then be fed the intercepted stretch of output stream to recover the remaining plaintext.\n\nThree general methods are employed to reduce this problem in LFSR-based stream ciphers:\n* [[Non-linear]] combination of several [[bit]]s from the LFSR [[state (computer science)|state]];\n* Non-linear combination of the output bits of two or more LFSRs (see also: [[shrinking generator]]); or using [[Evolutionary algorithm]] to introduce non-linearity.<ref>A. Poorghanad, A. Sadr, A. Kashanipour\" Generating High Quality Pseudo Random Number Using Evolutionary Methods\", IEEE Congress on Computational Intelligence and Security, vol. 9, pp. 331-335 , May,2008 [http://www.computer.org/csdl/proceedings/cis/2008/3508/01/3508a331.pdf]</ref>\n* Irregular clocking of the LFSR, as in the [[alternating step generator]].\n\nImportant LFSR-based stream ciphers include [[A5/1]] and [[A5/2]], used in [[GSM]] cell phones, [[E0 (cipher)|E0]], used in [[Bluetooth]], and the [[shrinking generator]]. The A5/2 cipher has been broken and both A5/1 and E0 have serious weaknesses.<ref>{{Citation\n | last = Barkam\n | first = Elad\n | last2 = Biham\n | first2 = Eli\n | last3 = Keller\n | first3 = Nathan\n | title= Instant Ciphertext-Only Cryptanalysis of GSM Encrypted Communication\n | journal=Journal of Cryptology\n | volume=21\n | issue=3\n | year=2008\n | pages=392–429\n | url=http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2006/CS/CS-2006-07.pdf\n | doi=10.1007/s00145-007-9001-y\n }}</ref><ref>{{cite book\n | first = Yi | last = Lu |author2=Willi Meier |author3=Serge Vaudenay\n | title =  The Conditional Correlation Attack: A Practical Attack on Bluetooth Encryption\n | journal =  Crypto 2005 | year = 2005 | location = Santa Barbara, California, USA\n | url = http://www.terminodes.org/micsPublicationsDetail.php?pubno=1216 | volume = 3621 | pages = 97–117 | doi=10.1007/11535218_7\n| series = Lecture Notes in Computer Science | isbn = 978-3-540-28114-6 | citeseerx = 10.1.1.323.9416 }}</ref>\n\nThe linear feedback shift register has a strong relationship to [[linear congruential generator]]s.<ref>\nRFC 4086\nsection 6.1.3 \"Traditional Pseudo-random Sequences\"\n</ref>\n\n=== Uses in circuit testing ===\n{{unsourced section|date=May 2016}}\nLFSRs are used in circuit testing for test-pattern generation (for exhaustive testing, pseudo-random testing or pseudo-exhaustive testing) and for signature analysis.\n\n==== Test-pattern generation ====\nComplete LFSR are commonly used as pattern generators for exhaustive testing, since they cover all possible inputs for an ''n''-input circuit. Maximal-length LFSRs and weighted LFSRs are widely used as pseudo-random test-pattern generators for pseudo-random test applications.\n\n==== Signature analysis ====\nIn [[built-in self-test]] (BIST) techniques, storing all the circuit outputs on chip is not possible, but the circuit output can be compressed to form a signature that will later be compared to the golden signature (of the good circuit) to detect faults. Since this compression is lossy, there is always a possibility that a faulty output also generates the same signature as the golden signature and the faults cannot be detected. This condition is called error masking or aliasing. BIST is accomplished with a multiple-input signature register (MISR or MSR), which is a type of LFSR. A standard LFSR has a single XOR or XNOR gate, where the input of the gate is connected to several \"taps\" and the output is connected to the input of the first flip-flop. A MISR has the same structure, but the input to every flip-flop is fed through an XOR/XNOR gate. For example, a 4-bit MISR has a 4-bit parallel output and a 4-bit parallel input. The input of the first flip-flop is XOR/XNORd with parallel input bit zero and the \"taps\". Every other flip-flop input is XOR/XNORd with the preceding flip-flop output and the corresponding parallel input bit. Consequently, the next state of the MISR depends on the last several states opposed to just the current state. Therefore, a MISR will always generate the same golden signature given that the input sequence is the same every time.\n\n=== Uses in digital broadcasting and communications ===\n\n==== Scrambling ====\n{{ main | Scrambler }}\nTo prevent short repeating sequences (e.g., runs of 0s or 1s) from forming spectral lines that may complicate symbol tracking at the\nreceiver or interfere with other transmissions, the data bit sequence is combined with the output of a linear-feedback register before modulation and transmission. This scrambling is removed at the receiver after demodulation. When the LFSR runs at the same [[bit rate]] as the transmitted symbol stream, this technique is referred to as [[scrambler|scrambling]]. When the LFSR runs considerably faster than the symbol stream, the LFSR-generated bit sequence is called ''chipping code''. The chipping code is combined with the data using [[exclusive or]] before transmitting using [[binary phase-shift keying]] or a similar modulation method. The resulting signal has a higher bandwidth than the data, and therefore this is a method of [[spread-spectrum]] communication. When used only for the spread-spectrum property, this technique is called [[direct-sequence spread spectrum]]; when used to distinguish several signals transmitted in the same channel at the same time and frequency, it is called [[code division multiple access]].\n\nNeither scheme should be confused with [[encryption]] or [[encipherment]]; scrambling and spreading with LFSRs do ''not'' protect the information from eavesdropping. They are instead used to produce equivalent streams that possess convenient engineering properties to allow robust and efficient modulation and demodulation.\n\nDigital broadcasting systems that use linear-feedback registers:\n* [[ATSC Standards]] (digital TV transmission system – North America)\n* [[Digital Audio Broadcasting|DAB]] ([[Digital Audio Broadcasting]] system – for radio)\n* [[DVB-T]] (digital TV transmission system – Europe, Australia, parts of Asia)\n* [[NICAM]] (digital audio system for television)\n\nOther digital communications systems using LFSRs:\n* INTELSAT business service (IBS)\n* Intermediate data rate (IDR)\n* [[Serial digital interface|SDI]] (Serial Digital Interface transmission)\n* Data transfer over [[PSTN]] (according to the [[ITU-T]] V-series recommendations)\n* [[CDMA]] (Code Division Multiple Access) cellular telephony\n* [[Fast Ethernet#100BASE-T2|100BASE-T2 \"fast\" Ethernet]] scrambles bits using an LFSR\n* [[Gigabit Ethernet#1000BASE-T|1000BASE-T Ethernet]], the most common form of Gigabit Ethernet, scrambles bits using an LFSR\n* [[PCI Express]] 3.0\n* [[SATA]]<ref>Section 9.5 of the SATA Specification, revision 2.6</ref>\n* [[Serial attached SCSI]] (SAS/SPL)\n* [[USB 3.0]]\n* [[IEEE 802.11a]] scrambles bits using an LFSR\n* [[Bluetooth Low Energy]] Link Layer is making use of LFSR (referred to as whitening)\n* [[Satellite navigation system]]s such as [[GPS]] and [[GLONASS]]. All current systems use LFSR outputs to generate some or all of their ranging codes (as the chipping code for CDMA or DSSS) or to modulate the carrier without data (like GPS&nbsp;L2&nbsp;CL ranging code). GLONASS also uses [[frequency-division multiple access]] combined with DSSS.\n\n==== Other uses ====\nLFSRs are also used in [[radio jamming]] systems to generate pseudo-random noise to raise the noise floor of a target communication system.\n\nThe German time signal [[DCF77]], in addition to amplitude keying, employs [[phase-shift keying]] driven by a 9-stage LFSR to increase the accuracy of received time and the robustness of the data stream in the presence of noise.<ref name=\"phasemod\">{{cite conference |url=https://www.ptb.de/cms/fileadmin/internet/fachabteilungen/abteilung_4/4.4_zeit_und_frequenz/pdf/5_1988_Hetzel_-_Proc_EFTF_88.pdf |title=Time dissemination via the LF transmitter DCF77 using a pseudo-random phase-shift keying of the carrier |first=P. |last=Hetzel |date=16 March 1988 |conference=2nd European Frequency and Time Forum |location=Neuchâtel |pages=351–364 |accessdate=11 October 2011}}</ref>\n\n== See also ==\n* [[Pinwheel (cryptography)|Pinwheel]]\n* [[Mersenne twister]]\n* [[Maximum length sequence]]\n* [[Analog feedback shift register]]\n* [[NLFSR]], Non-Linear Feedback Shift Register\n* [[Ring counter]]\n* [[Pseudo-random binary sequence]]\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n{{Div col|colwidth=30em}}\n* [http://www.newwaveinstruments.com/resources/articles/m_sequence_linear_feedback_shift_register_lfsr.htm LFSR Reference] LFSR theory and implementation, maximal length sequences, and comprehensive feedback tables for lengths from 7 to 16,777,215 (3 to 24 stages), and partial tables for lengths up to 4,294,967,295 (25 to 32 stages).\n* [http://www.itu.int/rec/T-REC-O.151-199210-I/en International Telecommunications Union Recommendation O.151] (August 1992)\n* [http://spreadsheets.google.com/ccc?key=0AvYtZsho-JTldFRYZnJLRFFaSWtUcVNXc1Y3M2VWd1E&hl=en Maximal Length LFSR table] with length from 2 to 67.\n* [http://www.maxim-ic.com/appnotes.cfm?appnote_number=1743&CMP=WP-9 Pseudo-Random Number Generation Routine]\n* http://www.ece.ualberta.ca/~elliott/ee552/studentAppNotes/1999f/Drivers_Ed/lfsr.html\n* http://www.quadibloc.com/crypto/co040801.htm\n* [https://web.archive.org/web/20060315203220/http://www.yikes.com/~ptolemy/lfsr_web/index.htm Simple explanation of LFSRs for Engineers]\n* [http://www.ece.cmu.edu/~koopman/lfsr/index.html Feedback terms]\n* [https://web.archive.org/web/20060111183721/http://homepage.mac.com/afj/lfsr.html General LFSR Theory]\n* [http://opencores.org/project,lfsr_randgen An implementation of LFSR in VHDL.]\n* [http://emmanuel.pouly.free.fr Simple VHDL coding for Galois and Fibonacci LFSR.]\n* [https://bitbucket.org/gallen/mlpolygen mlpolygen: A Maximal Length polynomial generator]\n{{div col end}}\n\n{{Cryptography stream}}\n\n[[Category:Binary arithmetic]]\n[[Category:Digital registers]]\n[[Category:Cryptographic algorithms]]\n[[Category:Pseudorandom number generators]]\n[[Category:Articles with example C code]]"
    },
    {
      "title": "Logical shift",
      "url": "https://en.wikipedia.org/wiki/Logical_shift",
      "text": ":''See also: '''[[Shift left testing]]''' about enabling testing early (left) in the development chain of software''\n\n{{refimprove|date=April 2016}}\n<!-- Table arranged in rough alphabetic order of shifts. -->\n{| class=\"wikitable\" style=\"float:right; clear:right;\"\n|+ Logical shift operators in various programming languages and processors\n! Language or processor !! Left !! Right\n|-\n| [[C (programming language)|C]], [[C++]], [[Go (programming language)|Go]], [[Swift (programming language)|Swift]] (unsigned types only);<br /> [[Standard ML]], [[Verilog]], [[PHP]], [[Python (programming language)|Python]]<ref>{{Cite web|url=https://wiki.python.org/moin/BitwiseOperators|title=BitwiseOperators - Python Wiki|website=wiki.python.org|access-date=2018-01-24}}</ref> || <tt> << </tt> || <tt> >> </tt>\n|-\n| [[D (programming language)|D]], [[Java (programming language)|Java]], [[JavaScript]], [[Julia (programming language)|Julia]] || <tt> << </tt> || <tt> >>> </tt>\n|-\n| [[F Sharp (programming language)|F#]] (unsigned types only) || <tt><<<</tt> || <tt>>>></tt>\n|-\n| [[Fortran]] || <tt>LSHIFT</tt> || <tt>RSHIFT</tt>\n|-\n| [[OCaml]] || <tt>lsl</tt> || <tt>lsr</tt>\n|-\n| [[Object Pascal]], [[Delphi (programming language)|Delphi]], [[x86 assembly language|x86 assembly]], [[Kotlin (programming language)|Kotlin]] || <tt>shl</tt> || <tt>shr</tt>\n|-\n| VHSIC Hardware Description Language ([[VHDL]]), [[MIPS architecture|MIPS]] || <tt>sll</tt> || <tt>srl</tt>\n|-\n| [[PowerPC]] || <tt>slw</tt> || <tt>srw</tt>\n|}\n\n\nIn [[computer science]], a '''logical shift''' is a [[bitwise operation]] that shifts all the bits of its operand. The two base variants are the '''logical left shift''' and the '''logical right shift'''. This is further modulated by the number of bit positions a given value shall be shifted, such as ''shift left by 1'' or ''shift right by n''. Unlike an [[arithmetic shift]], a logical shift does not preserve a number's sign bit or distinguish a number's [[exponent]] from its [[significand]] (mantissa); every bit in the operand is simply moved a given number of bit positions, and the vacant bit-positions are filled, usually with zeros, and possibly ones (contrast with a [[circular shift]]).\n\nA logical shift is often used when its operand is being treated as a [[sequence]] of bits instead of as a number.\n\nLogical shifts can be useful as efficient ways to perform multiplication or division of unsigned [[integer]]s by powers of two. Shifting left by ''n'' bits on a signed or unsigned binary number has the effect of multiplying it by 2<sup>''n''</sup>. Shifting right by ''n'' bits on an ''unsigned'' binary number has the effect of dividing it by 2<sup>''n''</sup> (rounding towards 0).\n\nLogical right shift differs from arithmetic right shift. Thus, many languages have different [[operator (programming)|operators]] for them. For example, in [[Java (programming language)|Java]] and [[JavaScript]], the logical right shift operator is <tt>>>></tt>, but the arithmetic right shift operator is <tt>>></tt>. (Java has only one left shift operator (<tt><<</tt>), because left shift via logic and arithmetic have the same effect.)\n\nThe [[programming language]]s [[C (programming language)|C]], [[C++]], and [[Go (programming language)|Go]], however, have only one right shift operator, <tt>>></tt>. Most C and C++ implementations, and Go, choose which right shift to perform depending on the type of integer being shifted: signed integers are shifted using the arithmetic shift, and unsigned integers are shifted using the logical shift.\n\nAll currently relevant C standards (ISO/IEC 9899:1999 to 2011) leave a definition gap for cases where the number of shifts is equal to or bigger than the number of bits in the operands in a way that the result is undefined. This helps allow C compilers to emit efficient code for various platforms by allowing direct use of the native shift instructions which have differing behavior. For example, shift-left-word in [[PowerPC]] chooses the more-intuitive behavior where shifting by the bit width or above gives zero,<ref>{{cite web |title=PowerPC Instruction Set: slw |url=http://pds.twi.tudelft.nl/vakken/in101/labcourse/instruction-set/slw.html |website=pds.twi.tudelft.nl |accessdate=9 April 2016}}</ref> whereas SHL in [[x86]] chooses to mask the shift amount to the lower bits ''to reduce the maximum execution time of the instructions'', and as such a shift by the bit width doesn't change the value.<ref>{{cite web|title=x86 Instruction Set Reference|url=http://x86.renejeschke.de/html/file_module_x86_id_285.html|website=x86.renejeschke.de|accessdate=9 April 2016}}</ref>\n\nSome languages, such as the [[.NET Framework]] and [[LLVM]], also leave shifting by the bit width and above ''unspecified'' (.NET)<ref>{{cite web|title=Opcodes.Shl Field|url=http://msdn.microsoft.com/en-us/library/system.reflection.emit.opcodes.shl.aspx|website=msdn.microsoft.com|publisher=Microsoft|accessdate=9 April 2016}}</ref> or ''undefined'' (LLVM).<ref>{{cite web|title=LLVM Language Reference Manual - shl Instruction|url=http://llvm.org/docs/LangRef.html#shl-instruction|website=llvm.org|publisher=LLVM Project|accessdate=9 April 2016}}</ref>  Others choose to specify the behavior of their most common target platforms, such as [[C Sharp (programming language)|C#]] which specifies the x86 behavior.<ref>{{cite web|title=<< Operator (C# Reference)|url=http://msdn.microsoft.com/en-us/library/a1sway8w.aspx|website=msdn.microsoft.com|publisher=Microsoft|accessdate=9 April 2016}}</ref>\n\n==Example==\nIf the bit sequence 0001 0111 (decimal 23) is logically shifted by one bit position, then:\n\n{|\n| Shift left yields: 0010 1110 (decimal 46)\n[[File:Rotate left logically.svg|thumb|left|300px|Logical left shift one bit]]\n| Shift right yields: 0000 1011 (decimal 11)\n[[File:Rotate right logically.svg|thumb|left|300px|Logical right shift one bit]]\n|}\n\n{{Clear}}\n\nNote: MSB = Most Significant Bit, \nLSB = Least Significant Bit\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Logical shift}}\n[[Category:Binary arithmetic]]\n[[Category:Operators (programming)]]\n\n[[zh:位操作#逻辑移位]]"
    },
    {
      "title": "Mask (computing)",
      "url": "https://en.wikipedia.org/wiki/Mask_%28computing%29",
      "text": "{{Redirect|Signal masking||Masking (disambiguation)}}\n\nIn [[computer science]], a '''mask''' or '''bitmask''' is data that is used for [[bitwise operation]]s, particularly in a [[bit field]]. Using a mask, multiple bits in a [[byte]], [[nibble]], [[Word (computer architecture)|word]] etc. can be set either on, off or inverted from on to off (or vice versa) in a single bitwise operation.\n\n==Common bitmask functions==\n\n===Masking bits to <code>1</code>===\nTo turn certain bits on, the [[Logical disjunction|bitwise <code>OR</code>]] operation can be used, following [[Logical disjunction#Bitwise operation|the principle]] that <code>Y OR 1 = 1</code> and <code>Y OR 0 = Y</code>. Therefore, to make sure a bit is on, <code>OR</code> can be used with a <code>1</code>. To leave a bit unchanged, <code>OR</code> is used with a <code>0</code>.\n\nExample: Masking ''on'' the higher [[nibble]] (bits 4, 5, 6, 7) the lower nibble (bits 0, 1, 2, 3) unchanged. \n     '''1001'''0101   '''1010'''0101\n  OR '''1111'''0000   '''1111'''0000\n   = '''1111'''0101   '''1111'''0101\n\n===Masking bits to <code>0</code>===\nMore often in practice bits are \"masked ''off''\" (or masked to <code>0</code>) than \"masked ''on''\" (or masked to <code>1</code>). When a bit is <code>AND</code>ed with a 0, the result is always 0, i.e. <code>Y AND 0 = 0</code>. To leave the other bits as they were originally, they can be <code>AND</code>ed with <code>1</code>, since <code>Y AND 1 = Y</code>.\n\nExample: Masking ''off'' the higher [[nibble]] (bits 4, 5, 6, 7) the lower nibble (bits 0, 1, 2, 3) unchanged. \n     '''1001'''0101   '''1010'''0101\n AND '''0000'''1111   '''0000'''1111\n   = '''0000'''0101   '''0000'''0101\n\n===Querying the status of a bit===\nIt is possible to use bitmasks to easily check the state of individual bits regardless of the other bits. To do this, turning off all the other bits using the bitwise <code>AND</code> is done as discussed [[#Masking bits to 0|above]] and the value is compared with <code>1</code>. If it is equal to <code>0</code>, then the bit was off, but if the value is any other value, then the bit was on. What makes this convenient is that it is not necessary to figure out what the value actually is, just that it is not <code>0</code>.\n\nExample: Querying the status of the 4th bit\n     1001'''1'''101   1001'''0'''101\n AND 0000'''1'''000   0000'''1'''000\n   = 0000'''1'''000   0000'''0'''000\n\n===Toggling bit values===\nSo far the article has covered how to turn bits on and turn bits off, but not both at once. Sometimes it does not really matter what the value is, but it must be made the opposite of what it currently is. This can be achieved using the [[Exclusive or|<code>XOR</code> (exclusive or)]] operation. <code>XOR</code> returns <code>1</code> [[if and only if]] an [[odd number]] of bits are <code>1</code>. Therefore, if two corresponding bits are <code>1</code>, the result will be a <code>0</code>, but if only one of them is <code>1</code>, the result will be <code>1</code>. Therefore inversion of the values of bits is done by <code>XOR</code>ing them with a <code>1</code>. If the original bit was <code>1</code>, it returns <code>1 XOR 1 = 0</code>. If the original bit was <code>0</code> it returns <code>0 XOR 1 = 1</code>. Also note that <code>XOR</code> masking is bit-safe, meaning that it will not affect unmasked bits because <code>Y XOR 0 = Y</code>, just like an <code>OR</code>.\n\nExample: Toggling bit values\n     10011101   10010101\n XOR '''00001111   11111111'''\n   = 10010010   01101010\n\nTo write arbitrary 1s and 0s to a subset of bits, first write 0s to that subset, then set the high bits:\n\n   register = (register & ~bitmask) | value;\n\n==Uses of bitmasks==\n\n=== Arguments to functions ===\nIn programming languages such as [[C (language)|C]], bit fields are a useful way to pass a set of named boolean arguments to a function. For example, in the graphics API [[OpenGL]], there is a command, <code>glClear()</code> which clears the screen or other buffers. It can clear up to four buffers (the color, depth, accumulation, and stencil buffers), so the API authors could have had it take four arguments. But then a call to it would look like\n<source lang=\"c\"> glClear(1,1,0,0); // This is not how glClear actually works and would make for unstable code.</source>\nwhich is not very descriptive. Instead there are four defined field bits, <code>GL_COLOR_BUFFER_BIT</code>, <code>GL_DEPTH_BUFFER_BIT</code>, <code>GL_ACCUM_BUFFER_BIT</code>, and <code>GL_STENCIL_BUFFER_BIT</code> and <code>glClear()</code> is declared as\n<source lang=\"c\"> void glClear(GLbitfield bits);</source>\nThen a call to the function looks like this\n<source lang=\"c\"> glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</source>\nInternally, a function taking a bitfield like this can use binary <code>and</code> to extract the individual bits. For example, an implementation of <code>glClear()</code> might look like:\n<source lang=\"c\">\nvoid glClear(GLbitfield bits) {\n  if ((bits & GL_COLOR_BUFFER_BIT) != 0) {\n    // Clear color buffer.\n  }\n  if ((bits & GL_DEPTH_BUFFER_BIT) != 0) {\n    // Clear depth buffer.\n  }\n  if ((bits & GL_ACCUM_BUFFER_BIT) != 0) {\n    // Clear accumulation buffer.\n  }\n  if ((bits & GL_STENCIL_BUFFER_BIT) != 0) {\n    // Clear stencil buffer.\n  }\n}</source>\nThe advantage to this approach is that function argument overhead is decreased. Since the minimum datum size is one byte, separating the options into separate arguments would be wasting seven bits per argument and would occupy more stack space. Instead, functions typically accept one or more 32-bit integers, with up to 32 option bits in each. While elegant, in the simplest implementation this solution is not [[type safety|type-safe]]. A <code>GLbitfield</code> is simply defined to be an <code>unsigned int</code>, so the compiler would allow a meaningless call to <code>glClear(42)</code> or even <code>glClear(GL_POINTS)</code>. In [[C++]] an alternative would be to create a class to encapsulate the set of arguments that glClear could accept and could be cleanly encapsulated in a library (see the external links for an example).\n\n===Inverse masks===\nMasks are used with IP addresses in IP ACLs (Access Control Lists) to specify what should be permitted and denied. To configure IP addresses on interfaces, masks start with 255 and have the large values on the left side: for example, IP address 209.165.202.129 with a 255.255.255.224 mask. Masks for IP ACLs are the reverse: for example, mask 0.0.0.255. This is sometimes called an inverse mask or a [[wildcard mask]]. When the value of the mask is broken down into binary (0s and 1s), the results determine which address bits are to be considered in processing the traffic. A 0 indicates that the address bits must be considered (exact match); a 1 in the mask is a \"don't care\". This table further explains the concept.\n\nMask example:\n\nnetwork address (traffic that is to be processed) \t\n10.1.1.0\n\nmask\t\n0.0.0.255\n\nnetwork address (binary)\t\n00001010.00000001.00000001.00000000\n\nmask (binary)\t\n00000000.00000000.00000000.11111111\n\nBased on the binary mask, it can be seen that the first three sets ([[Octet (computing)|octets]]) must match the given binary network address exactly (00001010.00000001.00000001). The last set of numbers is made of \"don't cares\" (.11111111). Therefore, all traffic that begins with 10.1.1. matches since the last octet is \"don't care\". Therefore, with this mask, network addresses 10.1.1.1 through 10.1.1.255 (10.1.1.x) are processed.\n\nSubtract the normal mask from 255.255.255.255 in order to determine the ACL inverse mask. In this example, the inverse mask is determined for network address 172.16.1.0 with a normal mask of 255.255.255.0.\n\n255.255.255.255 - 255.255.255.0 (normal mask) = 0.0.0.255 (inverse mask)\n\nACL equivalents\n\nThe source/source-wildcard of 0.0.0.0/255.255.255.255 means \"any\".\n\nThe source/wildcard of 10.1.1.2/0.0.0.0 is the same as \"host 10.1.1.2\"\n\n===Image masks===\n{{see also|Bit blit|Clipping path}}\n[[Image:Blit dot.gif|thumb|[[Raster graphics|Raster graphic]] [[Sprite (computer graphics)|sprite]]s (left) and masks (right)]]\nIn [[computer graphics]], when a given image is intended to be placed over a background, the transparent areas can be specified through a binary mask. This way, for each intended image there are actually two [[bitmap]]s: the actual image, in which the  unused areas are given a [[pixel]] value with all [[bit]]s set to 0s, and an additional ''mask'', in which the correspondent image areas are given a pixel value of all bits set to 0s and the surrounding areas a value of all bits set to 1s. In the sample at right, black pixels have the all-zero bits and white pixels have the all-one bits.\n\nAt [[Run time (program lifecycle phase)|run time]], to put the image on the screen over the background, the program first masks the screen pixel's bits with the image mask at the desired coordinates using the [[bitwise AND]] operation. This preserves the background pixels of the transparent areas while resets with zeros the bits of the pixels which will be obscured by the overlapped image.\n\nThen, the program renders the image pixel's bits by combining them with the background pixel's bits using the [[Logical disjunction|bitwise OR]] operation. This way, the image pixels are appropriately placed while keeping the background surrounding pixels preserved. The result is a perfect compound of the image over the background.\n\n[[Image:Sprite rendering by binary image mask.png|center]]\n\nThis technique is used for painting pointing device cursors, in typical 2-D videogames for characters, bullets and so on (the [[Sprite (computer graphics)|sprite]]s), for [[GUI]] [[Icon (computing)|icon]]s, and for video titling and other image mixing applications.\n\nAlthough related (due to being used for the same purposes), [[Palette (computing)#Transparent color in palettes|transparent color]]s and [[alpha channel]]s are techniques which do not involve the image pixel mixage by binary masking.\n\n===Hash tables===\nTo create a hashing function for a [[hash table]], often a function is used that has a large domain. To create an index from the output of the function, a modulo can be taken to reduce the size of the domain to match the size of the array; however, it is often faster on many processors to restrict the size of the hash table to powers of two sizes and use a bitmask instead.\n\n==See also==\n*[[Affinity mask]]\n*[[Bit field]]\n*[[Bit manipulation]]\n*[[Bitwise operation]]\n*[[Subnetwork]]\n*[[Tagged pointer]]\n*[[umask]]\n\n== External links ==\n* [https://bitbucket.org/fudepan/mili/wiki/BitwiseEnums bit_enum: a type-safe C++ library for bitwise operations]\n\n[[Category:Binary arithmetic]]\n\n{{DEFAULTSORT:Mask (Computing)}}"
    },
    {
      "title": "Microsoft Binary Format",
      "url": "https://en.wikipedia.org/wiki/Microsoft_Binary_Format",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\nIn [[computing]], '''Microsoft Binary Format''' (MBF) was a format for [[floating-point]] numbers used in [[Microsoft]]'s [[BASIC]] language products, including [[MBASIC]], [[GW-BASIC]] and [[QuickBASIC]] prior to version 4.00.<ref name=\"kb\">{{cite web |title=IEEE vs. Microsoft Binary Format; Rounding Issues (Complete) |publisher=Microsoft Support |date=2006-11-21 |url=http://support.microsoft.com/kb/35826 |access-date=2010-02-24}}</ref><ref>{{cite web |url=https://support.microsoft.com/en-us/kb/42980 |title=(Complete) Tutorial to Understand IEEE Floating-Point Errors |website=support.microsoft.com |access-date=2016-06-02}}</ref><ref name=\"masm50\">{{cite web |url=https://stackoverflow.com/questions/2686298/convert-pre-ieee-754-c-floating-point-numbers-to-from-c-sharp |quote=Read with care. The second reference could be mistaken to say that QB 4.0 could use MBF internally, but it only uses IEEE. It just has a few conversion functions to convert IEEE floating point numbers to strings containing MBF data, e.g. MKDMBF$ in addition to MKD$ which just copies the bytes of the IEEE value to a string. |title=Article: Floating Point Numbers |website=stackoverflow.com |access-date=2016-06-02}}</ref><ref>{{cite web |title=The MASM 6.1 documentation notes that 5.1 was the last MASM version to support MBF |url=http://people.sju.edu/~ggrevera/arch/references/MASM61PROGUIDE.pdf |website=people.sju.edu |access-date=2016-06-02}}</ref><ref>GW-BASIC User's Manual, Appendix D.3 USR Function Calls</ref><ref>BASIC Second edition (May 1982), IBM: Appendix C-15 (This is the BASICA manual.)</ref><ref>{{cite web |url=http://www.trs-80.com/trs80-zaps-internals.htm#rommath |title=ROM Routes (Integer Math) |website=Trs-80.com |access-date=2016-06-02}}</ref>\n\nThere are two main versions of the format. The original version was designed for memory-constrained systems and stored numbers in 32-bits (4 bytes), with a 23-bit [[Significand|mantissa]], 1-bit sign, and an 8-bit [[exponent]]. This was updated when computers began to ship with more memory, expanding the mantissa another 8-bits to produce a 40-bit format (5 bytes). The 40-bit format was widely used in most [[home computer]]s of the 1970s and 80s. These two versions are sometimes known as \"6-digit\" and \"9-digit\", respectively.<ref>{{cite web |url=https://www.pagetable.com/?p=46 |title=Create your own Version of Microsoft BASIC for 6502 |date=20 October 2008 |first=Michael |last=Steil}}</ref>\n\nThe format was expanded once again for [[QuickBASIC]], prior to version 4, using a 55-bit mantissa in a 64-bit (8 bytes) double-precision format. MBF was abandoned during the move to QuickBASIC 4, which instead used the standard [[IEEE 754]] format, introduced a few years earlier.\n\n==History==\nIn 1975, [[Bill Gates]] and [[Paul Allen]] were working on [[Altair BASIC]], which they were developing at [[Harvard University]] on a [[PDP-10]] running their Altair [[emulator]].<ref name=\"dawn\">{{cite web |author-last=Ireland |author-first=Corydon |url=http://news.harvard.edu/gazette/story/2013/09/dawn-of-a-revolution |title=Dawn of a revolution &#124; Harvard Gazette |publisher=news.harvard.edu |access-date=2016-05-30}}</ref> One thing still missing was code to handle floating-point numbers, needed to support calculations with very big and very small numbers,<ref name=\"dawn\"/> which would be particularly useful for science and engineering.<ref>{{cite journal |url=http://www.sciencedirect.com/science/article/pii/0898122187901817 |doi=10.1016/0898-1221(87)90181-7 |volume=14 |title=An introduction to the scientific computing language Pascal-SC |journal=Computers |pages=53–69 |date=1987 |author-last=Rall |author-first=L. B.}}</ref><ref>{{cite web |url=http://cis.poly.edu/~mleung/CS3734/s05/ch01/floatingPoints2.pdf |title=Floating-Point Numbers in Digital Computers |author-first=K. Ming |author-last=Leung |website=cis.poly.edu |access-date=2016-06-02}}</ref> One of the proposed uses of the Altair was as a scientific calculator.<ref>{{cite book |url=https://books.google.com/books?id=klV_BAAAQBAJ&pg=PA16 |title=Bill Gates: A Biography |date=2014-08-26 |access-date=2016-05-30 |isbn=978-1-44083014-3 |author-last=Becraft |author-first=Michael B.}}</ref>\n\n[[File:Altair 8800 at the Computer History Museum, cropped.jpg|thumb|[[Altair 8800]] front panel]]\n\nAt a dinner at [[Currier House (Harvard College)|Currier House]], an undergraduate residential house at Harvard, Gates and Allen complained to their dinner companions about having to write this code.<ref name=\"dawn\"/> One of them, [[Monte Davidoff]], told them that he had written floating-point routines before and convinced Gates and Allen that he was capable of writing the Altair BASIC floating-point code.<ref name=\"dawn\"/> At the time there was no standard for floating-point numbers, so Davidoff had to come up with his own. He decided that 32 bits would allow enough range and precision.<ref>{{cite web |url=http://altairbasic.org/math_ex.htm |title=Altair BASIC 3.2 (4K Edition) |publisher=altairbasic.org |access-date=2016-05-30}}</ref> When Allen had to demonstrate it to [[Micro Instrumentation and Telemetry Systems|MITS]], it was the first time it ran on an actual Altair.<ref>{{cite web |url=https://www.theregister.co.uk/2001/05/11/microsoft_altair_basic_legend_talks/ |title=Microsoft Altair BASIC legend talks about Linux, CPRM and that very frightening photo |website=theregister.co.uk |access-date=2016-05-30}}</ref> But it worked, and when he entered ‘PRINT 2+2’, Davidoff's adding routine gave the correct answer.<ref name=\"dawn\"/>\n\nThe source code for Altair BASIC was thought to have been lost to history, but resurfaced in 2000. It had been sitting behind Gates's former tutor and dean [[Harry R. Lewis|Harry Lewis]]'s file cabinet, who rediscovered it.<ref name=\"raiders\">{{cite web |url=https://www.theregister.co.uk/2001/05/13/raiders_of_the_lost_altair/ |title=Raiders of the Lost Altair BASIC Source Code |website=theregister.co.uk |access-date=2016-05-30}}</ref><ref name=\"quest\">{{cite web |url=http://www.rjh.org.uk/altair/ian.htm |title=Quest for the Holy Source - Ian's trip to Harvard |access-date=2016-05-30 |dead-url=unfit |archive-url=https://web.archive.org/web/20020102173701/http://www.rjh.org.uk/altair/ian.htm |archive-date=2002-01-02}}</ref> A comment in the source credits Davidoff as the writer of Altair BASIC's math package.<ref name=\"raiders\"/><ref name=\"quest\"/>\n\n[[File:Radio Shack Tandy TRS-80 Model I System.JPG|thumb|left|Radio Shack Tandy TRS-80 Model I System]]\n\nAltair BASIC took off, and soon most early home computers ran some form of Microsoft BASIC.<ref>{{cite web |url=http://www.oldcomputers.net/important-computer-people.html |title=Important computer inventors |publisher=Oldcomputers.net |access-date=2016-05-30}}</ref><ref>{{cite web |url=http://comp.lang.basic.powerbasic.narkive.com/erBpXdwr/basic-7-0-for-windows#post4 |title=Basic 7.0 for Windows |publisher=comp.lang.basic.powerbasic.narkive.com |access-date=2016-05-30}}</ref> The BASIC port for the [[MOS Technology 6502|6502]] CPU, such as used in the [[Commodore PET]], took up more space due to the lower code density of the 6502. Because of this it would likely not fit in a single ROM chip together with the machine-specific input and output code. Since an extra chip was necessary, extra space was available, and this was used in part to extend the floating-point format from 32 to 40 bits.<ref name=\"Steil_2008\">{{cite web |title=Create your own Version of Microsoft BASIC for 6502 |author-first=Michael |author-last=Steil |publisher=pagetable.com |date=2008-10-20 |url=http://www.pagetable.com/?p=46 |access-date=2016-05-30 |dead-url=no |archive-url=https://web.archive.org/web/20160530092603/http://www.pagetable.com/?p=46 |archive-date=2016-05-30}}</ref> This extended format was not only provided by [[Commodore BASIC]] 1 & 2, but was also supported by [[Applesoft BASIC]] I & II since version 1.1 (1977), [[KIM-1]] BASIC since version 1.1a (1977), and [[Tangerine Microtan 65|MicroTAN]] BASIC since version 2b (1980).<ref name=\"Steil_2008\"/> Not long afterwards the [[Zilog Z80|Z80]] ports, such as [[TRS-80 Level II BASIC|Level II BASIC]] for the [[TRS-80]] (1978), introduced the 64-bit, double-precision format as a separate data type from 32-bit, single-precision.<ref>{{cite web |url=https://archive.org/details/Level_II_BASIC_Reference_Manual_1st_Ed._1978_Radio_Shack |title=Radio Shack Hardware Manual: Level II BASIC Reference Manual 1st Ed. (1978)(Radio Shack): Free Download & Streaming: Internet Archive |access-date=2016-05-30}}</ref><ref>{{cite web |url=http://akhara.com/trs-80/docs/model1/Level%20II%20BASIC%20Reference%20Manual%20(1979)(Radio%20Shack)(pdf).zip |format=PDF |title=Level II Basic |website=akhara.com |access-date=2016-06-02}}</ref><ref>{{cite web |url=https://archive.org/details/BASIC-80_MBASIC_Reference_Manual |title=BASIC-80 (MBASIC) Reference Manual |format=PDF |access-date=2016-05-30}}</ref> Microsoft used the same floating-point formats in their implementation of [[Fortran]]<ref>{{cite web |url=http://www.textfiles.com/bitsavers/pdf/microsoft/cpm/Microsoft_FORTRAN-80_Ver3.4_Users_Manual_Nov80.pdf |title=(page 45 and 55) |website=textfiles.com |access-date=2016-05-30}}</ref> and for their macro assembler [[MASM]],<ref>{{cite web |url=ftp://ftp.gwdg.de/pub/mpsf/pc_doc/dosbesch.ps |title=Archived copy |access-date=2015-10-07 |dead-url=yes |archive-url=https://web.archive.org/web/20050220040002/http://ftp.gwdg.de/pub/mpsf/pc_doc/dosbesch.ps |archive-date=2005-02-20}}</ref> although their spreadsheet [[Multiplan]]<ref>{{cite web |url=http://www.classiccmp.org/dunfield/kyocera/t200mpln.pdf |title=Tandy 200 Multiplan Manual |website=classiccmp.org  |access-date=2016-06-02}}</ref><ref>Microsoft C [[Microsoft P-Code|Pcode]] Specifications, page 13; Multiplan wasn't compiled to machine code, but to a kind of byte-code which was run by an interpreter, in order to make Multiplan portable across the widely varying hardware of the time. This byte-code distinguished between the machine-specific floating point format to calculate on, and an external (standard) format, which was [[binary coded decimal]] (BCD). The PACK and UNPACK instructions converted between the two.</ref> and their [[COBOL]] implementation used [[binary-coded decimal]] (BCD) floating point.<ref>{{cite web |url=http://www.textfiles.com/bitsavers/pdf/microsoft/cpm/Microsoft_COBOL-80_1978.pdf |title=(page 26 = 32 in the PDF) |website=textfiles.com |access-date=2016-05-30}}</ref> Even so, for a while MBF became the de facto floating-point format on home computers, to the point where people still occasionally encounter legacy files and file formats using it.<ref>{{cite web |url=http://www.textfiles.com/programming/qwk.txt |format=TXT |title=QWK Mail Packet File Layout |author-first=Patrick Y. |author-last=Lee |website=textfiles.com |access-date=2016-06-02}}</ref><ref>{{cite web |url=http://www.csidata.com/patch/csim.txt |format=TXT |title=This document describes the abandoned CompuTrac data format, which until recently was actively used by Equis' MetaStock charting software |website=csidata.com |access-date=2016-06-02 |dead-url=yes |archive-url=https://web.archive.org/web/20160305170347/http://www.csidata.com/patch/csim.txt |archive-date=2016-03-05}}</ref><ref>{{cite web |url=https://www.7api.com/converting-microsoft-binary-format-to-ieee-format-using-vb-6-beatfx.html |title=Converting Microsoft Binary Format to IEEE format Using VB 6 |publisher=7api.com |date=2016-05-04 |access-date=2016-05-30}}{{dead link|date=January 2018 |bot=InternetArchiveBot |fix-attempted=yes}}</ref><ref>{{cite web |url=http://www.tek-tips.com/viewthread.cfm?qid=592713 |title=Help !Anybody know how to convert old M/S MBF value from Qbasic to VB6 - Visual Basic(Microsoft): Version 5 & 6 |publisher=Tek-Tips |access-date=2016-05-30}}</ref><ref>{{cite web |author=GL88 |url=https://social.msdn.microsoft.com/Forums/vstudio/en-US/64a201b5-852d-41df-9abe-2ad2645cc937/reading-binary-format-qbasic-with-c?forum=netfxbcl |title=Reading Binary Format (QBasic) with C# |publisher=Social.msdn.microsoft.com |access-date=2016-05-30}}</ref><ref>{{cite web |url=http://r.789695.n4.nabble.com/Reading-MetaStock-data-format-in-R-td4677240.html |title=Rmetrics - Reading MetaStock data format in R |publisher=R.789695.n4.nabble.com |date=2013-09-30 |access-date=2016-05-30}}</ref>\n[[Image:VAX 11-780 intero.jpg|thumb|[[VAX-11/780]] minicomputer]]\n\n===IEEE 754===\n{{Main|IEEE 754}}\nAs early as in 1976, [[Intel]] was starting the development of a floating-point [[coprocessor]].<ref name=\"case\">{{cite web |url=http://www.intel.com/content/dam/www/public/us/en/documents/case-studies/floating-point-case-study.pdf |title=Archived copy |access-date=2016-05-30 |dead-url=yes |archive-url=https://web.archive.org/web/20160304114859/http://www.intel.com/content/dam/www/public/us/en/documents/case-studies/floating-point-case-study.pdf |archive-date=2016-03-04}}</ref><ref name=\"story\">{{cite web |url=https://www.cs.berkeley.edu/~wkahan/ieee754status/754story.html |title=An Interview with the Old Man of Floating-Point |publisher=cs.berkeley.edu |date=1998-02-20 |access-date=2016-05-30}}</ref> Intel hoped to be able to sell a chip containing good implementations of all the operations found in the widely varying maths software libraries.<ref name=\"case\"/><ref name=\"conv\">{{cite web |author-last=Woehr |author-first=Jack |url=http://www.drdobbs.com/architecture-and-design/a-conversation-with-william-kahan/184410314 |title=A Conversation with William Kahan &#124; Dr. Dobb's |publisher=drdobbs.com |date=1997-11-01 |access-date=2016-05-30}}</ref>\n\nJohn Palmer, who managed the project, contacted [[William Kahan]] of the [[University of California]], who suggested that Intel use the floating point of [[Digital Equipment Corporation]]'s (DEC) VAX. The first VAX, the [[VAX-11/780]] had just come out in late 1977, and its floating point was highly regarded. However, seeking to market their chip to the broadest possible market, Intel wanted the best floating point possible, and Kahan went on to draw up specifications.<ref name=\"case\"/>\n\nWhen rumours of Intel's new chip reached its competitors, they started a standardization effort, called [[IEEE 754]], to prevent Intel from gaining too much ground. Kahan got Palmer's permission to participate; he was allowed to explain Intel's design decisions and their underlying reasoning, but not anything related to Intel's implementation architecture.<ref name=\"case\"/><ref name=\"story\"/><ref name=\"conv\"/><ref name=\"interview\">{{cite web |url=http://www.dr-chuck.com/dr-chuck/papers/columns/r3114.pdf |title=IEEE 754: An Interview with William Kahan |website=dr-chuck.com |access-date=2016-06-02}}</ref> VAX's floating-point formats differed from MBF only in that it had the sign in the most significant bit.<ref>{{cite web |url=http://nssdc.gsfc.nasa.gov/nssdc/formats/VAXFloatingPoint.htm |title=The VAX-11/780 did not implement the \"G\" format yet. Although this is not directly apparent from the tables because the structures have been cut up in two-byte words, the byte order is actually the same as on modern CPUs. There isn't enough room in the exponent range for NaNs, Infinity, infinities or denormals.<!-- You can also check the results of ‘?CVS(STRING$(3,0)+CHR$(1)),CVS(STRING$(2,255)+CHR$(127)+CHR$(255))’ in e.g. GW-BASIC against the stated limits for the VAX.--> |website=nssdc.gsfc.nasa.gov |access-date=2016-06-02}}</ref><ref>{{cite web |url=http://www.ece.cmu.edu/~ece447/s15/lib/exe/fetch.php?media=vax_hwhbk_1979.pdf |title=VAX11 780 |website=Ece.cmu.edu |access-date=2016-06-02}}</ref>\n\n[[File:Intel C8087.jpg|thumb|left|[[Intel 8087]] floating-point coprocessor]]\n\nIt turned out that an 8-bit exponent was not wide enough for some operations desired for double-precision numbers, e.g. to store the product of two 32-bit numbers.<ref name=\"kb\"/> Both Kahan's proposal and a counter-proposal by DEC therefore used 11 bits, like the time-tested [[CDC 6600#60-bit floating point|60-bit floating-point format]] of the [[CDC 6600]] from 1965.<ref name=\"story\"/><ref name=\"interview\"/><ref>{{cite web |url=http://ygdes.com/CDC/DesignOfAComputer_CDC6600.pdf |title=Design of a Computer: The Control Data 6600 |author-first=J. E. |author-last=Thornton |website=ygdes.com |access-date=2016-06-02}}</ref> Kahan's proposal also provided for infinities, which are useful when dealing with division-by-zero conditions; not-a-number values, which are useful when dealing with invalid operations; [[denormal number]]s, which help mitigate problems caused by underflow;<ref name=interview/><ref>{{cite web |url=https://www.cs.berkeley.edu/~wkahan/ieee754status/why-ieee.pdf |title=Why do we need a floating-point arithmetic standard? |author-first=William Morton |author-last=Kahan |website=cs.berkeley.edu |access-date=2016-06-02}}</ref><ref>{{cite web |url=https://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf |title=How Java's Floating-Point Hurts Everyone Everywhere |author-first1=William Morton |author-last1=Kahan |author-first2=Joseph D. |author-last2=Darcy |website=cs.berkeley.edu |access-date=2016-06-02}}</ref> and a better balanced [[exponent bias]], which can help avoid overflow and underflow when taking the reciprocal of a number.<ref>{{cite book |url=https://books.google.com/books?id=sUD8CAAAQBAJ&pg=PA171 |title=Numerical Analysis and Parallel Processing: Lectures given at The Lancaster ... |date=2013-12-21 |access-date=2016-05-30 |isbn=978-3-66239812-8 |author-last=Turner |author-first=Peter R.}}</ref><ref>{{cite web |url=https://www.cs.berkeley.edu/~wkahan/ieee754status/Names.pdf |title=Names for Standardized Floating-Point Formats |website=cs.berkeley.edu |access-date=2016-06-02}}</ref>\n\nIn 1980 the [[Intel 8087]] chip was already released,<ref>{{cite web |url=http://micro.magnet.fsu.edu/optics/olympusmicd/galleries/chips/intel8087.html |title=Molecular Expressions: Science, Optics & You - Olympus MIC-D: Integrated Circuit Gallery - Intel 8087 Math Coprocessor |publisher=micro.magnet.fsu.edu |access-date=2016-05-30}}</ref> but DEC remained opposed, to denormal numbers in particular, because of performance concerns and since it would give DEC a competitive advantage to standardise on DEC's format. The next year DEC had a study done in order to demonstrate that gradual underflow was a bad idea, but the study concluded the opposite, and DEC gave in. In 1985 the standard was ratified, but it had already become the de facto standard a year earlier, implemented by many manufacturers.<ref name=\"story\"/><ref name=\"interview\"/><ref>{{cite web |url=https://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF |title=IEEE Standard 754 for Binary Floating-Point Arithmetic |author-first=William Morton |author-last=Kahan |website=cs.berkeley.edu |access-date=2016-06-02}}</ref>\n\nBy the time [[QuickBASIC 4.00]] was released, the IEEE 754 standard had become widely adopted&mdash;for example, it was incorporated into Intel's [[Intel 387|387]] coprocessor and every [[x86]] processor from the [[Intel 80486|486]] on.  QuickBASIC versions 4.0 and 4.5 use IEEE 754 floating-point variables by default, but (at least in version 4.5) there is a command-line option <tt>/MBF</tt> for the IDE and the compiler that switches from IEEE to MBF floating-point numbers, to support earlier-written programs that rely on details of the MBF data formats.  [[Visual Basic]] also uses the IEEE 754 format instead of MBF.\n\n=={{anchor|32-bit MBF|40-bit MBF|64-bit MBF}}Technical details==\nMBF numbers consist of an 8-bit base-2 [[exponent]] with a bias of 128, so that exponents −127…−1 are represented by ''x'' = 1…127 (01h…7Fh), exponents 0…127 are represented by ''x'' = 128…255 (80h…FFh), with a special case for ''x'' = 0 (00h) representing the whole number being zero, a sign [[bit]] (positive mantissa: ''s'' = 0; negative mantissa: ''s'' = 1) and a 23-,<ref name=\"Steil_2008\"/> 31-<ref name=\"Steil_2008\"/> or 55-bit mantissa of the [[significand]].  There is always a 1-bit implied to the left of the explicit mantissa, and the [[radix point]] is located before this [[assumed bit]]<!-- whereas IEEE locates it afterwards -->.\n\nThe MBF double-precision format provides less scale than the [[IEEE 754]] format, and although the format itself provides almost one extra decimal digit of precision, in practice the stored values are less accurate because IEEE calculations use 80-bit intermediate results, and MBF doesn't.<ref name=\"kb\"/><ref name=\"masm50\"/><ref name=\"Borland_1994\">{{cite web |title=Converting between Microsoft Binary and IEEE formats |date=1998-07-02 |orig-year=1994-03-10 |id=1400 |author=Borland staff |publisher=[[Embarcadero USA]] (originally: [[Borland]]) |work=Technical Information Database |type=TI1431C.txt |url=https://community.embarcadero.com/index.php/article/technical-articles/162-programming/14799-converting-between-microsoft-binary-and-ieee-forma |access-date=2016-05-30 |dead-url=no |archive-url=https://web.archive.org/web/20190220230417/https://community.embarcadero.com/index.php/article/technical-articles/162-programming/14799-converting-between-microsoft-binary-and-ieee-forma |archive-date=2019-02-20}}</ref><ref>{{cite web |url=https://groups.google.com/forum/#!topic/comp.os.msdos.programmer/mDyRI1wolAc |title=Google Groups |website=Groups.google.com |access-date=2016-06-02}}</ref> Unlike IEEE floating point, MBF doesn't support [[denormal number]]s, [[Infinity|infinities]] or [[NaN]]s.<ref>{{cite web |url=http://www.boyet.com/Articles/MBFSinglePrecision.html |title=julian m bucknall >> Understanding single precision MBF |publisher=boyet.com |access-date=2016-05-30}}</ref><!-- By the way, you can easily check this yourself using GW-BASIC. Evaluating ‘?CVS(STRING$(2,0)+CHR$(64)+CHR$(129))’ (for single precision) or ‘?CVD(STRING$(6,0)+CHR$(64)+CHR$(129))’ (for double precision) will yield 1.5 as expected. You can vary the numbers and see what the result is. As you increase the exponent you just get bigger numbers as it reaches 255, instead of infinities and NaNs as in IEEE. If the exponent is zero, the result will equal 0, according to GW-BASIC. -->\n\nMBF single-precision format (32 bits, \"6-digit BASIC\"):\n{| class=\"wikitable\"\n|-\n! Exponent !! Sign !! Significand\n|-\n! 8&nbsp;bits,<br/>bit&nbsp;31–24 !! 1&nbsp;bit,<br/>bit&nbsp;23 !! 23&nbsp;bits,<br/>bit&nbsp;22–0 \n|-\n| xxxxxxxx || s || mmmmmmmmmmmmmmmmmmmmmmm\n|}\n\nMBF extended-precision format (40 bits, \"9?<!-- is it really 9 digits, not 8? -->-digit BASIC\"):\n{| class=\"wikitable\"\n|-\n! Exponent !! Sign !! Significand\n|-\n! 8&nbsp;bits,<br/>bit&nbsp;39–32 !! 1&nbsp;bit,<br/>bit&nbsp;31 !! 31&nbsp;bits,<br/>bit&nbsp;30–0 \n|-\n| xxxxxxxx || s || mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n|}\n\nMBF double-precision format (64 bits):\n{| class=\"wikitable\"\n|-\n! Exponent !! Sign !! Significand\n|-\n! 8&nbsp;bits,<br/>bit&nbsp;63–56 !! 1&nbsp;bit,<br/>bit&nbsp;55 !! 55&nbsp;bits,<br/>bit&nbsp;54–0\n|-\n| xxxxxxxx || s || mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm \n|}\n\n==Examples==\n\n* \"10\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 84h, 20h, 00h, 00h\n: 40-bit format: 84h, 20h, 00h, 00h, 00h\n\n* \"1\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 81h, 00h, 00h, 00h\n: 40-bit format: 81h, 00h, 00h, 00h, 00h\n\n* \"0\":\n: 32-bit format: 00h, 00h, 00h, 00h (or 00h, xxh, xxh, xxh)\n: 40-bit format: 00h, 00h, 00h, 00h, 00h (or 00h, xxh, xxh, xxh, xxh)\n\n* \"0.5\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 80h, 00h, 00h, 00h\n: 40-bit format: 80h, 00h, 00h, 00h, 00h\n\n* \"0.25\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 7Fh, 00h, 00h, 00h\n: 40-bit format: 7Fh, 00h, 00h, 00h, 00h\n\n* \"-0.5\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 80h, 80h, 00h, 00h\n: 40-bit format: 80h, 80h, 00h, 00h, 00h\n\n* \"sqrt(0.5)\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 80h, 35h, 04h, F3h\n: 40-bit format: 80h, 35h, 04h, F3h, 34h\n\n* \"sqrt(2)\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 81h, 35h, 04h, F3h\n: 40-bit format: 81h, 35h, 04h, F3h, 34h\n\n* \"ln(2)\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 80h, 31h, 72h, 18h\n: 40-bit format: 80h, 31h, 72h, 17h, F8h\n\n* \"log<sub>2</sub>(e)\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 81h, 38h, AAh, 3Bh\n: 40-bit format: 81h, 38h, AAh, 3Bh, 29h\n\n* \"pi/2\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 81h, 49h, 0Fh, DBh\n: 40-bit format: 81h, 49h, 0Fh, DAh, A2h\n\n* \"2*pi\":<ref name=\"Steil_2008\"/>\n: 32-bit format: 83h, 49h, 0Fh, DBh\n: 40-bit format: 83h, 49h, 0Fh, DAh, A2h\n<!--\n; these values are /1000 of what the labels say\nCON_99999999_9:\n:  32-bit format: 91h, 43h, 4Fh, F8h\n:  40-bit format: 9Bh, 3Eh, BCh, 1Fh, FDh\n\nCON_999999999:\n: 32-bit format: 94h, 74h, 23h, F7h\n: 40-bit format: 9Eh, 6Eh, 6Bh, 27h, FEh ; alternative values \n: 40-bit format: 9Eh, 6Eh, 6Bh, 27h, FDh ; alternative values\n\nCON_BILLION:\n: 32-bit format: 94h, 74h, 24h, 00h\n: 40-bit format: 9Eh, 6Eh, 6Bh, 28h, 00h\n-->\n\n==See also==\n{{Portal|Computer Science}}\n* [[Floating point]]\n* [[IEEE 754]] — Standard for Binary Floating-Point Arithmetic\n* [[IBM hexadecimal floating point|IBM Floating Point Architecture]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* Microsoft [https://web.archive.org/web/20101026203154/http://support.microsoft.com/kb/140520 provides] a dynamic link library for 16-bit Visual Basic containing functions to convert between MBF data and IEEE 754.\n**This library wraps the MBF conversion functions in the 16-bit Visual C(++) CRT.\n**These conversion functions will round an IEEE double precision number like ¾ ⋅ 2<sup>−128</sup> to zero rather than to 2<sup>−128</sup>.\n**They don't support denormals at all: the IEEE or MBF single precision number 2<sup>−128</sup> will be converted to zero even though it is representable in either format.\n**This library is only intended for use with Visual Basic; C(++) programs are expected to call the CRT functions directly.\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Floating point]]"
    },
    {
      "title": "Most significant bit",
      "url": "https://en.wikipedia.org/wiki/Most_significant_bit",
      "text": "#REDIRECT [[Bit numbering#Most significant bit]] {{R from merge}} {{R to section}}\n\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Negative and non-negative in binary",
      "url": "https://en.wikipedia.org/wiki/Negative_and_non-negative_in_binary",
      "text": "#REDIRECT [[Signed number representations]]\n\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Octal",
      "url": "https://en.wikipedia.org/wiki/Octal",
      "text": "{{Table Numeral Systems}}\nThe '''octal''' [[numeral system]], or '''oct''' for short, is the [[radix|base]]-8 number system, and uses the digits 0 to 7. Octal numerals can be made from [[Binary numeral system|binary]] numerals by grouping consecutive binary digits into groups of three (starting from the right). For example, the binary representation for decimal 74 is 1001010.  Two zeroes can be added at the left:  {{nowrap|(00)1 001 010}}, corresponding the octal digits {{nowrap|1 1 2}}, yielding the octal representation 112.\n\nIn the decimal system each decimal place is a power of ten. For example:\n: <math>\\mathbf{74}_{10} = \\mathbf{7} \\times 10^1 + \\mathbf{4} \\times  10^0</math>\nIn the octal system each place is a power of eight. For example:\n: <math>\\mathbf{112}_8 = \\mathbf{1} \\times  8^2 + \\mathbf{1} \\times  8^1 + \\mathbf{2} \\times  8^0 </math>\nBy performing the calculation above in the familiar decimal system we see why 112 in octal is equal to 64+8+2 = 74 in decimal.\n\n{| class=\"wikitable\" style=\"float:right; text-align:center\"\n|+ The octal [[multiplication table]]\n|-\n| × || '''1''' || '''2''' || '''3''' || '''4''' || '''5''' || '''6''' || '''7''' || '''10''' \n|-\n| '''1''' || 1 || 2 || 3 || 4 || 5 || 6 || 7 || 10\n|-\n| '''2''' || 2 || 4 || 6 || 10 || 12 || 14 || 16 || 20\n|-\n| '''3''' || 3 || 6 || 11 || 14 || 17 || 22 || 25 || 30\n|-\n| '''4''' || 4 || 10 || 14 || 20 || 24 || 30 || 34 || 40\n|-\n| '''5''' || 5 || 12 || 17 || 24 || 31 || 36 || 43 || 50\n|-\n| '''6''' || 6 || 14 || 22 || 30 || 36 || 44 || 52 || 60\n|-\n| '''7''' || 7 || 16 || 25 || 34 || 43 || 52 || 61 || 70\n|-\n| '''10''' || 10 || 20 || 30 || 40 || 50 || 60 || 70 || 100\n|}\n\n==Usage==\n\n===By Native Americans===\nThe [[Yuki language]] in [[California]] and the [[Pame language|Pamean languages]]<ref>{{Cite journal\n | last=Avelino\n | first=Heriberto\n | title=The typology of Pame number systems and the limits of Mesoamerica as a linguistic area\n | journal=Linguistic Typology\n | year=2006\n | volume=10\n | issue=1\n | pages=41–60\n | url=http://linguistics.berkeley.edu/~avelino/Avelino_2006.pdf\n | doi=10.1515/LINGTY.2006.002\n | postscript=<!--None-->\n}}</ref> in [[Mexico]] have octal systems because the speakers count using the spaces between their fingers rather than the fingers themselves.<ref>{{cite journal |jstor=2686959 |title=Ethnomathematics: A Multicultural View of Mathematical Ideas |author-first=Marcia |author-last=Ascher |publisher=The College Mathematics Journal}}</ref>\n\n===By Europeans===\n\n* It has been suggested that the reconstructed [[Proto-Indo-European]] word for \"nine\" might be related to the PIE word for \"new\".  Based on this, some have speculated that proto-Indo-Europeans used an octal number system, though the evidence supporting this is slim.<ref>{{cite book \n |last=Winter\n |first=Werner\n |chapter=Some thoughts about Indo-European numerals\n |title=Indo-European numerals\n |series=Trends in Linguistics\n |volume=57\n |editor1-last=Gvozdanović\n |editor1-first=Jadranka\n |date=1991\n |publisher=Mouton de Gruyter\n |location=Berlin\n |isbn=3-11-011322-8\n |pages=13–14\n |url=https://books.google.com/books?id=S-hmNOLuDGsC&lpg=PA170&pg=PA13\n |access-date=2013-06-09\n}}</ref>\n* In 1668 [[John Wilkins]] in ''[[An Essay towards a Real Character, and a Philosophical Language]]'' proposed use of base 8 instead of 10 \"because the way of Dichotomy or Bipartition being the most natural and easie kind of Division, that Number is capable of this down to an Unite\".<ref>{{cite book \n |last=Wilkins\n |first=John\n |title=An Essay Towards a Real Character and a Philosophical Language\n |date=1668\n |publisher= \n |location=London\n |pages=190\n |url=https://books.google.com/books?id=BCCtZjBtiEYC&pg=PA190&cad=3\n |access-date=2015-02-08\n}}</ref>\n* In 1716 King [[Charles XII of Sweden]] asked [[Emanuel Swedenborg]] to elaborate a number system based on 64 instead of 10. Swedenborg however argued that for people with less intelligence than the king such a big base would be too difficult and instead proposed 8 as the base. In 1718 Swedenborg wrote (but did not publish) a manuscript: \"En ny rekenkonst som om vexlas wid Thalet 8 i stelle then wanliga wid Thalet 10\" (\"A new arithmetic (or art of counting) which changes at the Number 8 instead of the usual at the Number 10\"). The numbers 1-7 are there denoted by the consonants l, s, n, m, t, f, u (v) and zero by the vowel o. Thus 8 = \"lo\", 16 = \"so\", 24 = \"no\", 64 = \"loo\", 512 = \"looo\" etc. Numbers with consecutive consonants are pronounced with vowel sounds between in accordance with a special rule.<ref>[[Donald Knuth]], ''[[The Art of Computer Programming]]''</ref>\n*Writing under the pseudonym \"Hirossa Ap-Iccim\" in ''[[The Gentleman's Magazine]]'', (London) July 1745, [[Hugh Jones (reverend)|Hugh Jones]] proposed an octal system for British coins, weights and measures. \"Whereas reason and convenience indicate to us an uniform standard for all quantities; which I shall call the ''Georgian standard''; and that is only to divide every integer in each ''species'' into eight equal parts, and every part again into 8 real or imaginary particles, as far as is necessary. For tho' all nations count universally by ''tens'' (originally occasioned by the number of digits on both hands) yet 8 is a far more complete and commodious number; since it is divisible into halves, quarters, and half quarters (or units) without a fraction, of which subdivision ''ten'' is uncapable....\" In a later treatise on [[Hugh Jones (reverend)#Publications|Octave computation]] (1753) Jones concluded: \"Arithmetic by ''Octaves'' seems most agreeable to the Nature of Things, and therefore may be called Natural Arithmetic in Opposition to that now in Use, by Decades; which may be esteemed Artificial Arithmetic.\"<ref>See H. R. Phalen, \"Hugh Jones and Octave Computation,\" ''The American Mathematical Monthly'' 56 (August–September 1949): 461-465.</ref> \n*In 1801, [[James Anderson of Hermiston|James Anderson]] criticized the French for basing the Metric system on decimal arithmetic.  He suggested base 8, for which he coined the term ''octal''. His work was intended as recreational mathematics, but he suggested a purely octal system of weights and measures and observed that the existing system of [[English units]] was already, to a remarkable extent, an octal system.<ref>James Anderson, On Octal Arithmetic [title appears only in page headers], [https://books.google.com/books?id=olhHAAAAYAAJ&pg=PA437 Recreations in Agriculture, Natural-History, Arts, and Miscellaneous Literature], Vol. IV, No. 6 (February 1801), T. Bensley, London; pages 437-448.</ref>\n*In the mid 19th century, Alfred B. Taylor concluded that \"Our octonary [base 8] radix is, therefore, beyond all comparison the \"''best possible one''\" for an arithmetical system.\" The proposal included a graphical notation for the digits and new names for the numbers, suggesting that we should count \"''un'', ''du'', ''the'', ''fo'', ''pa'', ''se'', ''ki'', ''unty'', ''unty-un'', ''unty-du''\" and so on, with successive multiples of eight named \"''unty'', ''duty'', ''thety'', ''foty'', ''paty'', ''sety'', ''kity'' and ''under''.\" So, for example, the number 65 (101 in octal) would be spoken in octonary as ''under-un''.<ref>A.B. Taylor, [https://books.google.com/books?id=X7wLAAAAYAAJ&pg=PP5 Report on Weights and Measures], Pharmaceutical Association, 8th Annual Session, Boston, September 15, 1859.  See pages and 48 and 53.</ref><ref>Alfred B. Taylor, Octonary numeration and its application to a system of weights and measures, [https://books.google.com/books?id=KsAUAAAAYAAJ&pg=PA296 Proc. Amer. Phil. Soc. Vol XXIV], Philadelphia, 1887; pages 296-366. See pages 327 and 330.</ref> Taylor also republished some of Swedenborg's work on octonary as an appendix to the above-cited publications.\n\n==={{anchor|\\nnn}}In computers===\nOctal became widely used in computing when systems such as the [[UNIVAC 1050]], [[PDP-8]], [[ICT 1900 series|ICL 1900]] and [[IBM mainframe]]s employed [[Six-bit character code|6-bit]], [[12-bit]], [[24-bit]] or [[36-bit]] words. Octal was an ideal abbreviation of binary for these machines because their word size is divisible by three (each octal digit represents three binary digits). So two, four, eight or twelve digits could concisely display an entire [[Word (computer architecture)|machine word]]. It also cut costs by allowing [[Nixie tube]]s, [[seven-segment display]]s, and [[calculator]]s to be used for the operator consoles, where binary displays were too complex to use, decimal displays needed complex hardware to convert radices, and [[hexadecimal]] displays needed to display more numerals.\n\nAll modern computing platforms, however, use [[word (computer architecture)|16-, 32-, or 64-bit words]], further divided into [[octet (computing)|eight-bit bytes]]. On such systems three octal digits per byte would be required, with the most significant octal digit representing two binary digits (plus one bit of the next significant byte, if any). Octal representation of a 16-bit word requires 6&nbsp;digits, but the most significant octal digit represents (quite inelegantly) only one bit (0 or 1). This representation offers no way to easily read the most significant byte, because it's smeared over four octal digits. Therefore, hexadecimal is more commonly used in programming languages today, since two hexadecimal digits exactly specify one byte.  Some platforms with a power-of-two word size still have instruction subwords that are more easily understood if displayed in octal; this includes the [[PDP-11]] and [[Motorola 68000 family]].  The modern-day ubiquitous [[x86 architecture]] belongs to this category as well, but octal is rarely used on this platform, although certain properties of the binary encoding of opcodes become more readily apparent when displayed in octal, e.g. the ModRM byte, which is divided into fields of 2, 3, and 3 bits, so octal can be useful in describing these encodings.\n\nOctal is sometimes used in computing instead of hexadecimal, perhaps most often in modern times in conjunction with [[file permissions]] under [[Unix]] systems (see [[chmod]]). It has the advantage of not requiring any extra symbols as digits (the hexadecimal system is base-16 and therefore needs six additional symbols beyond 0–9). It is also used for digital displays.\n\nIn programming languages, octal [[Literal (computer programming)|literals]] are typically identified with a variety of [[prefix]]es, including the digit <tt>0</tt>, the letters <tt>o</tt> or <tt>q</tt>, the digit–letter combination <tt>0o</tt>, or the symbol <tt>&</tt><ref>{{cite book |chapter=Constants, Variables, Expressions and Operators |author=Microsoft Corporation |year=1987 |access-date=2015-12-12 |chapter-url=http://www.antonis.de/qbebooks/gwbasman/chapter%206.html |title=GW-BASIC User's Manual |url=http://www.antonis.de/qbebooks/gwbasman/}}</ref> or <tt>$</tt>. In ''Motorola convention'', octal numbers are prefixed with <tt>@</tt>, whereas a small letter <tt>o</tt> is added as a [[suffix|postfix]] following the ''Intel convention''.<ref name=\"Kueveler-Schwoch_1996\">{{cite book |title=Arbeitsbuch Informatik - eine praxisorientierte Einführung in die Datenverarbeitung mit Projektaufgabe |language=de |first1=Gerd |last1=Küveler |first2=Dietrich |last2=Schwoch |date=2013 |orig-year=1996 |publisher=Vieweg-Verlag, reprint: Springer-Verlag |isbn=978-3-528-04952-2 |id=978-3-32292907-5 |doi=10.1007/978-3-322-92907-5 |url=https://books.google.com/books?id=b8-dBgAAQBAJ |access-date=2015-08-05}}</ref><ref name=\"Kueveler-Schwoch_2007\">{{cite book |title=Informatik für Ingenieure und Naturwissenschaftler: PC- und Mikrocomputertechnik, Rechnernetze |language=de |first1=Gerd |last1=Küveler |first2=Dietrich |last2=Schwoch |date=2007-10-04 |publisher=Vieweg, reprint: Springer-Verlag |edition=5 |volume=2 |isbn=3-83489191-6 |id=978-3-83489191-4 |url=https://books.google.com/books?id=xQbvPYxceY0C |access-date=2015-08-05}}</ref> In [[DR-DOS]] and [[Multiuser DOS]] various [[environment variable]]s like [[%$CLS%|$CLS]], [[%$ON%|$ON]], [[%$OFF%|$OFF]], [[%$HEADER%|$HEADER]] or [[%$FOOTER%|$FOOTER]] support an <tt>\\nnn</tt> octal number notation,<ref name=\"Paul_1997_NWDOSTIP\"/><ref name=\"Paul_2002_CLS\"/><ref name=\"CCI_1997_HELP\"/> and DR-DOS [[DEBUG]] utilizes <tt>\\</tt> to prefix octal numbers as well.\n\nFor example, the literal 73 (base 8) might be represented as <tt>073</tt>, <tt>o73</tt>, <tt>q73</tt>, <tt>0o73</tt>, <tt>\\73</tt>, <tt>@73</tt>, <tt>&73</tt>, <tt>$73</tt> or <tt>73o</tt> in various languages.\n\nNewer languages have been abandoning the prefix <tt>0</tt>, as decimal numbers are often represented with leading zeroes.  The prefix <tt>q</tt> was introduced to avoid the prefix <tt>o</tt> being mistaken for a zero, while the prefix <tt>0o</tt> was introduced to avoid starting a numerical literal with an alphabetic character (like <tt>o</tt> or <tt>q</tt>), since these might cause the literal to be confused with a variable name.  The prefix <tt>0o</tt> also follows the model set by the prefix <tt>0x</tt> used for hexadecimal literals in the [[C (programming language)|C language]]; it is supported by [[Haskell (programming language)|Haskell]],<ref>Haskell: http://www.haskell.org/onlinereport/lexemes.html#sect2.5</ref> [[OCaml]],<ref>OCaml: http://caml.inria.fr/pub/docs/manual-ocaml/lex.html</ref> [[Perl 6]],<ref>Perl 6: http://perlcabal.org/syn/S02.html#Radix_markers</ref> [[Python (programming language)|Python]] as of version 3.0,<ref>Python 3: https://docs.python.org/3.1/reference/lexical_analysis.html#integer-literals</ref> [[Ruby (programming language)|Ruby]],<ref>RubySpec: https://github.com/kostya/rubyspec/blob/master/core/string/to_i_spec.rb</ref> [[Tcl]] as of version 9,<ref>Tcl: http://wiki.tcl.tk/498</ref> and it is intended to be supported by [[ECMAScript]] 6<ref>ECMAScript 6th Edition draft: https://people.mozilla.org/~jorendorff/es6-draft.html#sec-literals-numeric-literals</ref> (the prefix <tt>0</tt> has been discouraged in ECMAScript 3 and dropped in ECMAScript 5<ref>Mozilla Developer Network: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/parseInt</ref>).\n\nOctal numbers that are used in some programming languages (C, [[Perl]], [[PostScript]]…) for textual/graphical representations of byte strings when some byte values (unrepresented in a code page, non-graphical, having special meaning in current context or otherwise undesired) have to be to [[escape character|escaped]] as <tt>\\nnn</tt>. Octal representation may be particularly handy with non-ASCII bytes of [[UTF-8]], which encodes groups of 6 bits, and where any start byte has octal value <tt>\\3nn</tt> and any continuation byte has octal value <tt>\\2nn</tt>.\n\n=== In aviation ===\n[[Transponder (aeronautics)|Transponders]] in aircraft transmit a [[Transponder (aeronautics)#Transponder codes|code]], expressed as a four-octal-digit number, when interrogated by ground radar. This code is used to distinguish different aircraft on the radar screen.\n\n==Conversion between bases==\n\n===Decimal to octal conversion===\n\n====Method of successive Euclidean division by 8====\nTo convert integer decimals to octal, [[Euclidean division|divide]] the original number by the largest possible power of 8 and divide the remainders by successively smaller powers of 8 until the power is 1. The octal representation is formed by the quotients, written in the order generated by the algorithm.\nFor example, to convert 125<sub>10</sub> to octal:\n:125 = 8<sup>2</sup> × '''1''' + 61\n:61 = 8<sup>1</sup> × '''7''' + 5\n:5 = 8<sup>0</sup> × '''5''' + 0\nTherefore, 125<sub>10</sub> = 175<sub>8</sub>.\n\nAnother example:\n:900 = 8<sup>3</sup> × '''1''' + 388\n:388 = 8<sup>2</sup> × '''6''' + 4\n:4 = 8<sup>1</sup> × '''0''' + 4\n:4 = 8<sup>0</sup> × '''4''' + 0\nTherefore, 900<sub>10</sub> = 1604<sub>8</sub>.\n\n====Method of successive multiplication by 8====\nTo convert a decimal fraction to octal, multiply by 8; the integer part of the result is the first digit of the octal fraction. Repeat the process with the fractional part of the result, until it is null or within acceptable error bounds.\n\nExample: Convert 0.1640625 to octal:\n:0.1640625 × 8 = 1.3125 = '''1''' + 0.3125\n:0.3125 × 8 = 2.5 = '''2''' + 0.5\n:0.5 × 8 = 4.0 = '''4''' + 0\nTherefore, 0.1640625<sub>10</sub> = 0.124<sub>8</sub>.\n\nThese two methods can be combined to handle decimal numbers with both integer and fractional parts, using the first on the integer part and the second on the fractional part.\n\n====Method of successive duplication====\nTo convert integer decimals to octal, prefix the number with \"0.\". Perform the following steps for as long as digits remain on the right side of the radix:\nDouble the value to the left side of the radix, using ''octal'' rules, move the radix point one digit rightward, and then place the doubled value underneath the current value so that the radix points align. If the moved radix point crosses over a digit that is 8 or 9, convert it to 0 or 1 and add the carry to the next leftward digit of the current value. ''Add'' ''octally'' those digits to the left of the radix and simply drop down those digits to the right, without modification.\n\nExample:\n<pre>\n 0.4 9 1 8 decimal value\n  +0\n ---------\n   4.9 1 8\n  +1 0\n  --------\n   6 1.1 8\n  +1 4 2\n  --------\n   7 5 3.8\n  +1 7 2 6\n  --------\n 1 1 4 6 6. octal value\n</pre>\n\n===Octal to decimal conversion===\nTo convert a number {{mvar|k}} to decimal, use the formula that defines its base-8 representation:\n:<math>k = \\sum_{i=0}^n \\left( a_i\\times 8^i \\right)</math>\n\nIn this formula, {{math|''a''<sub>''i''</sub>}} is an individual octal digit being converted, where {{mvar|i}} is the position of the digit (counting from 0 for the right-most digit).\n\nExample: Convert 764<sub>8</sub> to decimal:\n \n:764<sub>8</sub> = 7 × 8<sup>2</sup> + 6 × 8<sup>1</sup> + 4 × 8<sup>0</sup>  = 448 + 48 + 4 = 500<sub>10</sub>\n\nFor double-digit octal numbers this method amounts to multiplying the lead digit by 8 and adding the second digit to get the total.\n\nExample: 65<sub>8</sub> = 6 × 8 + 5 = 53<sub>10</sub>\n\n====Method of successive duplication====\nTo convert octals to decimals, prefix the number with \"0.\". Perform the following steps for as long as digits remain on the right side of the radix: Double the value to the left side of the radix, using ''decimal'' rules, move the radix point one digit rightward, and then place the doubled value underneath the current value so that the radix points align. ''Subtract'' ''decimally'' those digits to the left of the radix and simply drop down those digits to the right, without modification.\n\nExample:\n<pre>\n 0.1 1 4 6 6  octal value\n  -0\n -----------\n   1.1 4 6 6\n  -  2\n  ----------\n     9.4 6 6\n  -  1 8\n  ----------\n     7 6.6 6\n  -  1 5 2\n  ----------\n     6 1 4.6\n  -  1 2 2 8\n  ----------\n     4 9 1 8. decimal value\n</pre>\n\n===Octal to binary conversion===\nTo convert octal to binary, replace each octal digit by its binary representation.\n\t\nExample: Convert 51<sub>8</sub> to binary:\n:5<sub>8</sub> = 101<sub>2</sub>\n:1<sub>8</sub> = 001<sub>2</sub>\nTherefore, 51<sub>8</sub> = 101 001<sub>2</sub>.\n\n===Binary to octal conversion===\nThe process is the reverse of the previous algorithm. The binary digits are grouped by threes, starting from the least significant bit and proceeding to the left and to the right.  Add leading zeroes (or trailing zeroes to the right of decimal point) to fill out the last group of three if necessary.  Then replace each trio with the equivalent octal digit.\n\nFor instance, convert binary 1010111100 to octal:\n\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"4\"\n|- align=\"center\"\n| 001 || 010 || 111 || 100\n|- align=\"center\"\n| 1 || 2 || 7 || 4\n|}\n\nTherefore, 1010111100<sub>2</sub> = 1274<sub>8</sub>.\n\nConvert binary 11100.01001 to octal:\n\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"4\"\n|- align=\"center\"\n|  011 || 100|| &nbsp;.&nbsp; || 010 || 010\n|- align=\"center\"\n| 3 || 4 || &nbsp;.&nbsp; || 2 || 2 \n|}\n\nTherefore, 11100.01001<sub>2</sub> = 34.22<sub>8</sub>.\n\n===Octal to hexadecimal conversion===\nThe conversion is made in two steps using binary as an intermediate base. Octal is converted to binary and then binary to hexadecimal, grouping digits by fours, which correspond each to a hexadecimal digit.\n\nFor instance, convert octal 1057 to hexadecimal:\n\n:To binary:\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"4\"\n|- align=\"center\"\n| 1 || 0 || 5 || 7\n|- align=\"center\"\n| 001 || 000 || 101 || 111\n|}\n\n:then to hexadecimal:\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"4\"\n|- align=\"center\"\n| 0010 || 0010 || 1111\n|- align=\"center\"\n| 2 || 2 || F\n|}\n\nTherefore, 1057<sub>8</sub> = 22F<sub>16</sub>.\n\n===Hexadecimal to octal conversion===\nHexadecimal to octal conversion proceeds by first converting the hexadecimal digits to 4-bit binary values, then regrouping the binary bits into 3-bit octal digits.\n\nFor example, to convert 3FA5<sub>16</sub>:\n\n:To binary:\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"4\"\n|- align=\"center\"\n| 3 || F || A || 5\n|- align=\"center\"\n| 0011 || 1111 || 1010 || 0101\n|}\n\n:then to octal:\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"4\"\n|- align=\"center\"\n| 0 || 011 || 111 || 110 || 100 || 101\n|- align=\"center\"\n| 0 || 3 || 7 || 6 || 4 || 5\n|}\n\nTherefore, 3FA5<sub>16</sub> = 37645<sub>8</sub>.\n\n==Real numbers==\n===Fractions===\nDue to having only factors of two, many octal fractions have repeating digits, although these tend to be fairly simple:\n\n{|class=\"wikitable\"\n\n| colspan=\"3\" align=\"center\" | Decimal base<br><SMALL>Prime factors of the base: <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Green\">'''5'''</span></SMALL><br><SMALL>Prime factors of one below the base: <span style=\"color:Blue\">'''3'''</span></SMALL><br><SMALL>Prime factors of one above the base: <span style=\"color:Magenta\">'''11'''</span></SMALL><br><SMALL>Other Prime factors: <span style=\"color:Red\">'''7 13 17 19 23 29 31'''</span></SMALL>\n| colspan=\"3\" align=\"center\" | '''Octal base'''<br><SMALL>Prime factors of the base: <span style=\"color:Green\">'''2'''</span></SMALL><br><SMALL>Prime factors of one below the base: <span style=\"color:Blue\">'''7'''</span></SMALL><br><SMALL>Prime factors of one above the base: <span style=\"color:Magenta\">'''3'''</span></SMALL><br><SMALL>Other Prime factors: <span style=\"color:Red\">'''5 13 15 21 23 27 35 37'''</span></SMALL>\n|-\n| align=\"center\" | Fraction\n| align=\"center\" | <SMALL>Prime factors<br>of the denominator</SMALL>\n| align=\"center\" | Positional representation\n| align=\"center\" | Positional representation\n| align=\"center\" | <SMALL>Prime factors<br>of the denominator</SMALL>\n| align=\"center\" | Fraction\n|-\n| align=\"center\" | 1/2\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| '''0.5'''\n| '''0.4'''\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| align=\"center\" | 1/2\n|-\n| align=\"center\" | 1/3\n| align=\"center\" | <span style=\"color:Blue\">'''3'''</span>\n| bgcolor=#c0c0c0 | '''0.'''3333... = '''0.'''{{overline|3}}\n| bgcolor=#c0c0c0 | '''0.'''2525... = '''0.'''{{overline|25}}\n| align=\"center\" | <span style=\"color:Magenta\">'''3'''</span>\n| align=\"center\" | 1/3\n|-\n| align=\"center\" | 1/4\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| '''0.25'''\n| '''0.2'''\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| align=\"center\" | 1/4\n|-\n| align=\"center\" | 1/5\n| align=\"center\" | <span style=\"color:Green\">'''5'''</span>\n| '''0.2'''\n| bgcolor=#c0c0c0 | '''0.'''{{overline|1463}}\n| align=\"center\" | <span style=\"color:Red\">'''5'''</span>\n| align=\"center\" | 1/5\n|-\n| align=\"center\" | 1/6\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Blue\">'''3'''</span>\n| bgcolor=#c0c0c0 | '''0.1'''{{overline|6}}\n| bgcolor=#c0c0c0 | '''0.1'''{{overline|25}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Magenta\">'''3'''</span>\n| align=\"center\" | 1/6\n|-\n| align=\"center\" | 1/7\n| align=\"center\" | <span style=\"color:Red\">'''7'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|142857}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|1}}\n| align=\"center\" | <span style=\"color:Blue\">'''7'''</span>\n| align=\"center\" | 1/7\n|-\n| align=\"center\" | 1/8\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| '''0.125'''\n| '''0.1'''\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| align=\"center\" | 1/10\n|-\n| align=\"center\" | 1/9\n| align=\"center\" | <span style=\"color:Blue\">'''3'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|1}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|07}}\n| align=\"center\" | <span style=\"color:Magenta\">'''3'''</span>\n| align=\"center\" | 1/11\n|-\n| align=\"center\" | 1/10\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Green\">'''5'''</span>\n| '''0.1'''\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|6314}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Red\">'''5'''</span>\n| align=\"center\" | 1/12\n|-\n| align=\"center\" | 1/11\n| align=\"center\" | <span style=\"color:Magenta\">'''11'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|09}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|0564272135}}\n| align=\"center\" | <span style=\"color:Red\">'''13'''</span>\n| align=\"center\" | 1/13\n|-\n| align=\"center\" | 1/12\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Blue\">'''3'''</span>\n| bgcolor=#c0c0c0 | '''0.08'''{{overline|3}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|52}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Magenta\">'''3'''</span>\n| align=\"center\" | 1/14\n|-\n| align=\"center\" | 1/13\n| align=\"center\" | <span style=\"color:Red\">'''13'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|076923}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|0473}}\n| align=\"center\" | <span style=\"color:Red\">'''15'''</span>\n| align=\"center\" | 1/15\n|-\n| align=\"center\" | 1/14\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Red\">'''7'''</span>\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|714285}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|4}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Blue\">'''7'''</span>\n| align=\"center\" | 1/16\n|-\n| align=\"center\" | 1/15\n| align=\"center\" | <span style=\"color:Blue\">'''3'''</span>, <span style=\"color:Green\">'''5'''</span>\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|6}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|0421}}\n| align=\"center\" | <span style=\"color:Magenta\">'''3'''</span>, <span style=\"color:Red\">'''5'''</span>\n| align=\"center\" | 1/17\n|-\n| align=\"center\" | 1/16\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| '''0.0625'''\n| '''0.04'''\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| align=\"center\" | 1/20\n|-\n| align=\"center\" | 1/17\n| align=\"center\" | <span style=\"color:Red\">'''17'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|0588235294117647}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|03607417}}\n| align=\"center\" | <span style=\"color:Red\">'''21'''</span>\n| align=\"center\" | 1/21\n|-\n| align=\"center\" | 1/18\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Blue\">'''3'''</span>\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|5}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|34}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Magenta\">'''3'''</span>\n| align=\"center\" | 1/22\n|-\n| align=\"center\" | 1/19\n| align=\"center\" | <span style=\"color:Red\">'''19'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|052631578947368421}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|032745}}\n| align=\"center\" | <span style=\"color:Red\">'''23'''</span>\n| align=\"center\" | 1/23\n|-\n| align=\"center\" | 1/20\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Green\">'''5'''</span>\n| '''0.05'''\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|3146}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Red\">'''5'''</span>\n| align=\"center\" | 1/24\n|-\n| align=\"center\" | 1/21\n| align=\"center\" | <span style=\"color:Blue\">'''3'''</span>, <span style=\"color:Red\">'''7'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|047619}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|03}}\n| align=\"center\" | <span style=\"color:Magenta\">'''3'''</span>, <span style=\"color:Blue\">'''7'''</span>\n| align=\"center\" | 1/25\n|-\n| align=\"center\" | 1/22\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Magenta\">'''11'''</span>\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|45}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|2721350564}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Red\">'''13'''</span>\n| align=\"center\" | 1/26\n|-\n| align=\"center\" | 1/23\n| align=\"center\" | <span style=\"color:Red\">'''23'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|0434782608695652173913}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|02620544131}}\n| align=\"center\" | <span style=\"color:Red\">'''27'''</span>\n| align=\"center\" | 1/27\n|-\n| align=\"center\" | 1/24\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Blue\">'''3'''</span>\n| bgcolor=#c0c0c0 | '''0.041'''{{overline|6}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|25}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Magenta\">'''3'''</span>\n| align=\"center\" | 1/30\n|-\n| align=\"center\" | 1/25\n| align=\"center\" | <span style=\"color:Green\">'''5'''</span>\n| '''0.04'''\n| bgcolor=#c0c0c0 | '''0.'''{{overline|02436560507534121727}}\n| align=\"center\" | <span style=\"color:Red\">'''5'''</span>\n| align=\"center\" | 1/31\n|-\n| align=\"center\" | 1/26\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Red\">'''13'''</span>\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|384615}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|2354}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Red\">'''15'''</span>\n| align=\"center\" | 1/32\n|-\n| align=\"center\" | 1/27\n| align=\"center\" | <span style=\"color:Blue\">'''3'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|037}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|022755}}\n| align=\"center\" | <span style=\"color:Magenta\">'''3'''</span>\n| align=\"center\" | 1/33\n|-\n| align=\"center\" | 1/28\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Red\">'''7'''</span>\n| bgcolor=#c0c0c0 | '''0.03'''{{overline|571428}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|2}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Blue\">'''7'''</span>\n| align=\"center\" | 1/34\n|-\n| align=\"center\" | 1/29\n| align=\"center\" | <span style=\"color:Red\">'''29'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|0344827586206896551724137931}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|0215173454106475626043236713}}\n| align=\"center\" | <span style=\"color:Red\">'''35'''</span>\n| align=\"center\" | 1/35\n|-\n| align=\"center\" | 1/30\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Blue\">'''3'''</span>, <span style=\"color:Green\">'''5'''</span>\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|3}}\n| bgcolor=#c0c0c0 | '''0.0'''{{overline|2104}}\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>, <span style=\"color:Magenta\">'''3'''</span>, <span style=\"color:Red\">'''5'''</span>\n| align=\"center\" | 1/36\n|-\n| align=\"center\" | 1/31\n| align=\"center\" | <span style=\"color:Red\">'''31'''</span>\n| bgcolor=#c0c0c0 | '''0.'''{{overline|032258064516129}}\n| bgcolor=#c0c0c0 | '''0.'''{{overline|02041}}\n| align=\"center\" | <span style=\"color:Red\">'''37'''</span>\n| align=\"center\" | 1/37\n|-\n| align=\"center\" | 1/32\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| '''0.03125'''\n| '''0.02'''\n| align=\"center\" | <span style=\"color:Green\">'''2'''</span>\n| align=\"center\" | 1/40\n|}\n\n===Irrational numbers===\nThe table below gives the expansions of some common [[irrational number]]s in decimal and octal.\n{| class=\"wikitable\"\n! rowspan=2 | Number\n! colspan=2 | Positional representation\n|-\n! Decimal\n! Octal\n|-\n| [[Square root of 2|{{sqrt|2}}]] {{small|(the length of the [[diagonal]] of a unit [[Square (geometry)|square]])}}\n| {{val|1.414213562373095048}}...\n| 1.3240 4746 3177 1674...\n|-\n| [[Square root of 3|{{sqrt|3}}]] {{small|(the length of the diagonal of a unit [[cube]])}}\n| {{val|1.732050807568877293}}...\n| 1.5666 3656 4130 2312...\n|-\n| [[Square root of 5|{{sqrt|5}}]] {{small|(the length of the [[diagonal]] of a 1×2 [[rectangle]])}}\n| {{val|2.236067977499789696}}...\n| 2.1706 7363 3457 7224...\n|-\n| [[Golden ratio|{{mvar|φ}}]] {{small|1=(phi, the [[golden ratio]] = {{math|(1+{{radical|5}})/2}})}}\n| {{val|1.618033988749894848}}...\n| 1.4743 3571 5627 7512...\n|-\n| [[Pi|{{mvar|π}}]] {{small|(pi, the ratio of [[circumference]] to [[diameter]] of a circle)}}\n| {{val|3.141592653589793238462643}}<br>{{val|383279502884197169399375105}}...\n| 3.1103 7552 4210 2643...\n|-\n| [[E (mathematical constant)|{{mvar|e}}]] {{small|(the base of the [[natural logarithm]])}}\n| {{val|2.718281828459045235}}...\n| 2.5576 0521 3050 5355...\n|}\n\n==See also==\n* [[Computer numbering formats]]\n* [[Octal games]], a game numbering system used in [[combinatorial game theory]]\n* [[Syllabic octal]], a specific octal representation of 8-bit syllables\n* [[Squawk code]], an octal representation of [[Gillham code]]\n\n==References==\n{{reflist|refs=\n<ref name=\"Paul_1997_NWDOSTIP\">{{cite book |title=NWDOS-TIPs&nbsp;— Tips &amp; Tricks rund um Novell DOS 7, mit Blick auf undokumentierte Details, Bugs und Workarounds |work=MPDOSTIP |author-first=Matthias |author-last=Paul |date=1997-07-30 |edition=3 |version=Release 157 |language=de |url=http://www.antonis.de/dos/dos-tuts/mpdostip/html/nwdostip.htm |access-date=2014-08-06 |dead-url=no |archive-url=https://web.archive.org/web/20161104235829/http://www.antonis.de/dos/dos-tuts/mpdostip/html/nwdostip.htm |archive-date=2016-11-04}} (NB. NWDOSTIP.TXT is a comprehensive work on [[Novell DOS 7]] and [[OpenDOS 7.01]], including the description of many undocumented features and internals. It is part of the author's yet larger <code>MPDOSTIP.ZIP</code> collection maintained up to 2001 and distributed on many sites at the time. The provided link points to a HTML-converted older version of the <code>NWDOSTIP.TXT</code> file.)</ref>\n<ref name=\"Paul_2002_CLS\">{{cite web |title=Updated CLS posted |author-first=Matthias |author-last=Paul |date=2002-03-26 |url=http://marc.info/?l=freedos-dev&m=101717593306186&w=2 |publisher=freedos-dev mailing list |access-date=2014-08-06 |dead-url=no |archive-url=http://archive.is/qE1wl |archive-date=2019-04-27}}</ref>\n<ref name=\"CCI_1997_HELP\">{{cite book |title=CCI Multiuser DOS 7.22 GOLD Online Documentation |id=HELP.HLP |date=1997-02-10 |publisher=[[Concurrent Controls, Inc.]] (CCI)}}</ref>\n}}\n\n==External links==\n* [http://www.octomatics.org Octomatics] is a [[numeral system]] enabling simple visual calculation in octal.\n\n[[Category:Binary arithmetic]]\n[[Category:Positional numeral systems]]"
    },
    {
      "title": "Octuple-precision floating-point format",
      "url": "https://en.wikipedia.org/wiki/Octuple-precision_floating-point_format",
      "text": "{{refimprove|date=June 2016}}\nIn [[computing]], '''octuple precision''' is a binary [[floating-point]]-based [[computer number format]] that occupies 32 [[byte]]s (256 [[bit]]s) in computer memory. This 256-[[bit]] octuple precision is for applications requiring results in higher than [[quadruple precision]]. This format is rarely (if ever) used and very few environments support it.\n{{Floating-point}}\n\n== IEEE 754 octuple-precision binary floating-point format: binary256 ==\n\nIn its 2008 revision, the [[IEEE 754]] standard specifies a '''binary256''' format among the ''interchange formats'' (it is not a basic format), as having:\n* [[Sign bit]]: 1 bit\n* [[Exponent]] width: 19 bits\n* [[Significand]] [[precision (arithmetic)|precision]]: 237 bits (236 explicitly stored)\n<!-- \"significand\", with a d at the end, is a technical term, please do not confuse with \"significant\" -->\n\nThe format is written with an implicit lead bit with value 1 unless the exponent is all zeros. Thus only 236 bits of the [[significand]] appear in the memory format, but the total precision is 237 bits (approximately 71 decimal digits: {{nowrap|log<sub>10</sub>(2<sup>237</sup>) ≈ 71.344}}).\n<!-- (Commented out since the image is incorrect; it could be re-added once corrected.)-->\nThe bits are laid out as follows:\n\n[[File:Octuple precision visual demonstration.png|1000px|Layout of octuple precision floating point format]]\n\n=== Exponent encoding ===\n\nThe octuple-precision binary floating-point exponent is encoded using an [[offset binary]] representation, with the zero offset being 262143; also known as exponent bias in the IEEE&nbsp;754 standard.\n\n* E<sub>min</sub> = −262142\n* E<sub>max</sub> = 262143\n* [[Exponent bias]] = 3FFFF<sub>16</sub> = 262143\n\nThus, as defined by the offset binary representation, in order to get the true exponent the offset of 262143 has to be subtracted from the stored exponent.\n\nThe stored exponents 00000<sub>16</sub> and 7FFFF<sub>16</sub> are interpreted specially.\n\n{|class=\"wikitable\" style=\"text-align:center\"\n! Exponent !! Significand zero !! Significand non-zero !! Equation\n|-\n| 00000<sub>16</sub> || [[0 (number)|0]], [[−0]] || [[subnormal numbers]] || (-1)<sup>signbit</sup> × 2<sup>−262142</sup> × 0.significandbits<sub>2</sub>\n|-\n| 00001<sub>16</sub>, ..., 7FFFE<sub>16</sub> ||colspan=2| normalized value || (-1)<sup>signbit</sup> × 2<sup>exponent bits<sub>2</sub></sup> × 1.significandbits<sub>2</sub>\n|-\n| 7FFFF<sub>16</sub> || ±[[infinity|∞]] || [[NaN]] (quiet, signalling)\n|}\n\nThe minimum strictly positive (subnormal) value is {{nowrap|2<sup>−262378</sup> ≈ 10<sup>−78984</sup>}} and has a precision of only one bit.\nThe minimum positive normal value is 2<sup>−262142</sup> ≈ 2.4824 × 10<sup>−78913</sup>.\nThe maximum representable value is 2<sup>262144</sup> − 2<sup>261907</sup> ≈ 1.6113 × 10<sup>78913</sup>.\n\n=== Octuple-precision examples ===\n\nThese examples are given in bit ''representation'', in [[hexadecimal]],\nof the floating-point value. This includes the sign, (biased) exponent, and significand.\n\n 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = +0\n 8000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = −0\n\n 7fff f000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = +infinity\n ffff f000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = −infinity\n\n 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0001<sub>16</sub>\n = 2<sup>−262142</sup> × 2<sup>−236</sup> = 2<sup>−262378</sup>\n ≈ 2.24800708647703657297018614776265182597360918266100276294348974547709294462 × 10<sup>−78984</sup>\n   (smallest positive subnormal number)\n\n 0000 0fff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff<sub>16</sub>\n = 2<sup>−262142</sup> × (1 − 2<sup>−236</sup>)\n ≈ 2.4824279514643497882993282229138717236776877060796468692709532979137875392 × 10<sup>−78913</sup>\n   (largest subnormal number)\n\n 0000 1000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub>\n = 2<sup>−262142</sup>\n ≈ 2.48242795146434978829932822291387172367768770607964686927095329791378756168 × 10<sup>−78913</sup>\n   (smallest positive normal number)\n\n 7fff efff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff<sub>16</sub>\n = 2<sup>262143</sup> × (2 − 2<sup>−236</sup>)\n ≈ 1.61132571748576047361957211845200501064402387454966951747637125049607182699 × 10<sup>78913</sup>\n   (largest normal number)\n\n 3fff efff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff ffff<sub>16</sub>\n = 1 − 2<sup>−237</sup>\n ≈ 0.999999999999999999999999999999999999999999999999999999999999999999999995472\n   (largest number less than one)\n\n 3fff f000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub>\n = 1 (one)\n\n 3fff f000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0001<sub>16</sub>\n = 1 + 2<sup>−236</sup>\n ≈ 1.00000000000000000000000000000000000000000000000000000000000000000000000906\n   (smallest number larger than one)\n\nBy default, 1/3 rounds down like [[double precision]], because of the odd number of bits in the significand.\nSo the bits beyond the rounding point are <code>0101...</code> which is less than 1/2 of a [[unit in the last place]].\n\n==Implementations==\nOctuple precision is rarely implemented since usage of it is extremely rare. [[Apple Inc.]] had an implementation of addition, subtraction and multiplication of octuple-precision numbers with a 224-bit [[two's complement]] significand and a 32-bit exponent.<ref>{{cite web|url=http://images.apple.com/ca/acg/pdf/oct3a.pdf |title=Octuple-precision floating point on Apple G4 (archived copy on web.archive.org) |author1=R. Crandall |author2=J. Papadopoulos |date=8 May 2002 |deadurl=unfit |archiveurl=https://web.archive.org/web/20060728140052/http://images.apple.com/ca/acg/pdf/oct3a.pdf |archivedate=July 28, 2006 }}</ref>  One can use general [[arbitrary-precision arithmetic]] libraries to obtain octuple (or higher) precision, but specialized octuple-precision implementations may achieve higher performance.\n\n=== Hardware support ===\nThere is no known hardware implementation of octuple precision.\n\n== See also ==\n* [[IEEE 754-2008|IEEE Standard for Floating-Point Arithmetic (IEEE 754)]]\n* [[ISO/IEC 10967]], Language-independent arithmetic\n* [[Primitive data type]]\n\n== References ==\n{{reflist}}\n\n==Further reading==\n* {{cite book |author-first=Nelson H. F. |author-last=Beebe |title=The Mathematical-Function Computation Handbook - Programming Using the MathCW Portable Software Library |date=2017-08-22 |location=Salt Lake City, UT, USA |publisher=[[Springer International Publishing AG]] |edition=1 |lccn=2017947446 |isbn=978-3-319-64109-6 |doi=10.1007/978-3-319-64110-2 }}\n\n{{data types}}\n\n[[Category:Binary arithmetic]]\n[[Category:Floating point types]]"
    },
    {
      "title": "Offset binary",
      "url": "https://en.wikipedia.org/wiki/Offset_binary",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\n'''Offset binary''',<ref name=\"Patrice_2006\"/> also referred to as '''excess-K''',<ref name=\"Patrice_2006\"/> '''excess-''N''''', '''excess code''' or '''biased representation''', is a digital coding scheme where all-zero corresponds to the minimal negative value and all-one to the [[Integer overflow|maximal positive value]]. There is no standard for offset binary, but most often the offset ''K'' for an ''n''-bit binary word is ''K''&nbsp;=&nbsp;2<sup>''n''−1</sup>. This has the consequence that the \"zero\" value is represented by a 1 in the most significant bit and zero in all other bits, and in general the effect is conveniently the same as using [[two's complement]] except that the most significant bit is inverted. It also has the consequence that in a logical comparison operation, one gets the same result as with a true form numerical comparison operation, whereas, in two's complement notation a logical comparison will agree with true form numerical comparison operation if and only if the numbers being compared have the same sign. Otherwise the sense of the comparison will be inverted, with all negative values being taken as being larger than all positive values. \n\n{{anchor|Excess-64}}One historically prominent example of offset-64 (''excess-64'') notation was in the [[floating point]] (exponential) notation in the IBM System/360 and System/370 generations of computers. The \"characteristic\" (exponent) took the form of a seven-bit excess-64 number (The high-order bit of the same byte contained the sign of the [[significand]]).<ref name=\"IBM_360\"/>\n\n{{anchor|Excess-129}}The 8-bit exponent in [[Microsoft Binary Format]], a floating point format used in various programming languages (in particular [[BASIC]]) in the 1970s and 1980s, was encoded using an offset-129 notation (''excess-129'').\n\n{{anchor|Excess-15|Excess-127|Excess-1023|Excess-16383}}The [[IEEE 754-2008|IEEE Standard for Floating-Point Arithmetic (IEEE 754)]] uses various sizes of exponent, but also uses offset notation for the format of each precision. Unusually however, instead of using \"excess 2<sup>''n''−1</sup>\" it uses \"excess 2<sup>''n''−1</sup>&nbsp;−&nbsp;1\" (i.e. ''excess-15'', ''excess-127'', ''excess-1023'', ''excess-16383'') which means that inverting the leading (high-order) bit of the exponent will not convert the exponent to correct two's complement notation. \n\nOffset binary is often used in [[digital signal processing]] (DSP). Most [[analog to digital converter|analog to digital]] (A/D) and [[digital to analog converter|digital to analog]] (D/A) chips are unipolar, which means that they cannot handle [[bipolar signal]]s (signals with both positive and negative values). A simple solution to this is to bias the analog signals with a DC offset equal to half of the A/D and D/A converter's range. The resulting digital data then ends up being in offset binary format.<ref name=\"Chen_1988\"/>\n\nMost standard computer CPU chips cannot handle the offset binary format directly. CPU chips typically can only handle signed and unsigned integers, and floating point value formats. Offset binary values can be handled in several ways by these CPU chips. The data may just be treated as unsigned integers, requiring the programmer to deal with the zero offset in software. The data may also be converted to signed integer format (which the CPU can handle natively) by simply subtracting the zero offset. As a consequence of the most common offset for an ''n''-bit word being 2<sup>''n''−1</sup>, which implies that the first bit is inverted relative to two's complement, there is no need for a separate subtraction step, but one simply can invert the first bit. This sometimes is a useful simplification in hardware, and can be convenient in software as well.\n\nTable of offset binary for four bits, with two's complement for comparison<ref name=\"Intersil_1997\"/>\n{| class=\"wikitable\" border=\"1\"\n|-\n! Offset binary code, ''K'' = 8\n! Decimal code\n! Two's complement binary\n|-\n| 1111\n| 7\n| 0111\n|-\n| 1110\n| 6\n| 0110\n|-\n| 1101\n| 5\n| 0101\n|-\n| 1100\n| 4\n| 0100\n|-\n| 1011\n| 3\n| 0011\n|-\n| 1010\n| 2\n| 0010\n|-\n| 1001\n| 1\n| 0001\n|-\n| 1000\n| 0\n| 0000\n|-\n| 0111\n| &minus;1\n| 1111\n|-\n| 0110\n| &minus;2\n| 1110\n|-\n| 0101\n| &minus;3\n| 1101\n|-\n| 0100\n| &minus;4\n| 1100\n|-\n| 0011\n| &minus;5\n| 1011\n|-\n| 0010\n| &minus;6\n| 1010\n|-\n| 0001\n| &minus;7\n| 1001\n|-\n| 0000\n| &minus;8\n| 1000\n|}\n\nOffset binary may be converted into two's complement by inverting the most significant bit.  For example, with 8-bit values, the offset binary value may be XORed with 0x80 in order to convert to two's complement. In specialised hardware it may be simpler to accept the bit as it stands, but to apply its value in inverted significance.\n\n== See also ==\n*[[Signed number representations]]\n*[[Binary numeral system]]\n*[[Excess-3]]\n*[[Excess-128]]\n*[[Exponent bias]]\n*[[Excess-Gray code]]\n*[[Ones' complement]]\n\n== References ==\n{{Reflist|refs=\n<ref name=\"Patrice_2006\">{{cite book |author-first1=Angela |author-last1=Chang |author-first2=Yen |author-last2=Chen |author-first3=Patrice |author-last3=Delmas |title=COMPSCI 210S1T 2006 |chapter=2.5.2: Data Representation: Offset binary representation (Excess-K) |date=2006-03-07 |publisher=Department of Computer Science, [[The University of Auckland]], NZ |url=http://www.cs.auckland.ac.nz/~patrice/210-2006/210%20LN04_2.pdf |page=18 |access-date=2016-02-04}}</ref>\n<ref name=\"Chen_1988\">{{cite book |title=Signal Processing Handbook |editor-first=Chi-hau |editor-last=Chen |publisher=[[Marcel Dekker, Inc.]]/[[CRC Press]] |author=((Electrical and Computer Science Department, [[Southeastern Massachusetts University]], North Dartmouth, MA, USA)) |publication-place=New York, USA |date=1988 |isbn=0-8247-7956-8 |url=https://books.google.com/books?id=10Pi0MRbaOYC |access-date=2016-02-04}}</ref>\n<ref name=\"IBM_360\">IBM System/360 Principles of Operation Form A22-6821. Various editions available on the WWW.</ref>\n<ref name=\"Intersil_1997\">{{cite web |title=Data Conversion Binary Code Formats |publisher=[[Intersil Corporation]] |publication-date=2000 |date=May 1997 |id=AN9657.1 |url=http://www.intersil.com/data/an/an9657.pdf |access-date=2016-02-04}}</ref>\n}}\n\n==Further reading==\n* {{anchor|Excess-6|Excess-11|Excess-123}} {{cite web |title=Decimal Representations |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2006 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0203.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180716101321/http://www.quadibloc.com/comp/cp0203.htm |archive-date=2018-07-16}} (NB. Mentions Excess-3, Excess-6, Excess-11, Excess-123.)\n* {{anchor|Excess-25|Excess-250}} {{cite web |title=Chen-Ho Encoding and Densely Packed Decimal |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2007 |work=quadibloc |url=http://www.quadibloc.com/comp/cp020301.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703002320/http://www.quadibloc.com/comp/cp020301.htm |archive-date=2018-07-16}} (NB. Mentions Excess-25, Excess-250.)\n* {{anchor|Excess-32|Excess-256|Excess-976|Excess-2048|Excess-16384}} {{cite web |title=Floating-Point Formats |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2005 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0201.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703001709/http://www.quadibloc.com/comp/cp0201.htm |archive-date=2018-07-16}} (NB. Mentions Excess-32, Excess-64, Excess-128, Excess-256, Excess-976, Excess-1023, Excess-1024, Excess-2048, Excess-16384.)\n* {{anchor|Excess-500|Excess-512|Excess-1024}} {{cite web |title=Computer Arithmetic |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2005 |work=quadibloc |url=http://www.quadibloc.com/comp/cp02.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180716102439/http://www.quadibloc.com/comp/cp02.htm |archive-date=2018-07-16}} (NB. Mentions Excess-64, Excess-500, Excess-512, Excess-1024.)\n\n[[Category:Numeral systems]]\n[[Category:Binary arithmetic]]"
    },
    {
      "title": "Ones' complement",
      "url": "https://en.wikipedia.org/wiki/Ones%27_complement",
      "text": "{{No footnotes|date=January 2014}}\n\n{|class=\"wikitable sortable floatright\" style=\"margin-left: 1.5em;\"\n|+8-bit ones'-complement integers\n! Bits\n! Unsigned<br />value\n! Ones'<br />complement<br />value\n|- \n| 0111 1111\n|align=\"right\"| 127 \n|align=\"right\"| 127 \n|- \n| 0111 1110 \n|align=\"right\"| 126 \n|align=\"right\"| 126 \n|- \n| 0000 0010 \n|align=\"right\"| 2 \n|align=\"right\"| 2 \n|- \n| 0000 0001 \n|align=\"right\"| 1 \n|align=\"right\"| 1 \n|- \n| 0000&nbsp;0000 \n|align=\"right\"| 0 \n|align=\"right\"| 0 \n|- \n| 1111 1111\n|align=\"right\"| 255 \n|align=\"right\"| −0 \n|- \n| 1111 1110\n|align=\"right\"| 254 \n|align=\"right\"| −1 \n|- \n| 1111 1101\n|align=\"right\"| 253 \n|align=\"right\"| −2 \n|- \n| 1000 0001\n|align=\"right\"| 129 \n|align=\"right\"| −126 \n|- \n| 1000 0000\n|align=\"right\"| 128 \n|align=\"right\"| −127 \n|- \n|}\n\nThe '''ones' complement''' of a [[binary number]] is defined as the value obtained by inverting all the bits in the binary representation of the number (swapping 0s for 1s and vice versa). The ones' complement of the number then behaves like the negative of the original number in some arithmetic operations. To within a constant (of −1), the ones' complement behaves like the negative of the original number with [[binary addition]]. However, unlike [[two's complement]], these numbers have not seen widespread use because of issues such as the offset of −1, that negating zero results in a distinct [[negative zero]] bit pattern, less simplicity with arithmetic [[Carry (arithmetic)|borrowing]], etc.\n\nA '''ones' complement system''' or '''ones' complement arithmetic''' is a system in which negative numbers are represented by the inverse of the binary representations of their corresponding positive numbers. In such a system, a number is negated (converted from positive to negative or vice versa) by computing its ones' complement. An N-bit ones' complement numeral system can only represent integers in the range −(2<sup>N−1</sup>−1) to 2<sup>N−1</sup>−1 while [[two's complement]] can express −2<sup>N−1</sup> to 2<sup>N−1</sup>−1.\n\nThe '''ones' complement binary''' [[numeral system]] is characterized by the [[bit complement]] of any integer value being the arithmetic negative of the value.  That is, inverting all of the bits of a number (the logical complement) produces the same result as subtracting the value from 0.\n\nMany early computers, including the [[CDC 6600]], the [[LINC]], the [[PDP-1]], and the [[UNIVAC 1100/2200 series#1107|UNIVAC 1107]], used ones' complement notation.  Successors of the CDC 6600 continued to use ones' complement until the late 1980s, and the descendants of the UNIVAC 1107 (the [[UNIVAC 1100/2200 series]]) still do, but the majority of modern computers use [[two's complement]].\n\n==Number representation==\n\nPositive numbers are the same simple, binary system used by two's complement and sign-magnitude.  Negative values are the bit complement of the corresponding positive value. The largest positive value is characterized by the sign (high-order) bit being off (0) and all other bits being on (1).  The lowest negative value is characterized by the sign bit being 1, and all other bits being 0.  The table below shows all possible values in a 4-bit system, from −7 to +7.\n\n      +      −\n  0   0000   1111   — Note that both +0 and −0 return TRUE when tested for zero\n  1   0001   1110   — and FALSE when tested for non-zero. \n  2   0010   1101\n  3   0011   1100\n  4   0100   1011\n  5   0101   1010\n  6   0110   1001\n  7   0111   1000\n\n==Basics==\n\nAdding two values is straightforward.  Simply align the values on the least significant bit and add, propagating any carry to the bit one position left. If the carry extends past the end of the word it is said to have \"wrapped around\", a condition called an \"[[end-around carry]]\". When this occurs, the bit must be added back in at the right-most bit.  This phenomenon does not occur in two's complement arithmetic.\n\n   0001 0110     22\n + 0000 0011      3\n ===========   ====\n   0001 1001     25\n\nSubtraction is similar, except that borrows, rather than carries, are propagated to the left.  If the borrow extends past the end of the word it is said to have \"wrapped around\", a condition called an \"'''end-around borrow'''\".  When this occurs, the bit must be subtracted from the right-most bit.  This phenomenon does not occur in two's complement arithmetic.\n\n   0000 0110      6\n − 0001 0011     19\n ===========   ====\n 1 1111 0011    −12    —An [[end-around borrow]] is produced, and the sign bit of the intermediate result is 1.\n − 0000 0001      1    —Subtract the end-around borrow from the result.\n ===========   ====\n   1111 0010    −13    —The correct result (6 − 19 = -13)\n\nIt is easy to demonstrate that the bit complement of a positive value is the negative magnitude of the positive value.  The computation of 19&nbsp;+&nbsp;3 produces the same result as&nbsp;19&nbsp;−&nbsp;(−3).\n\nAdd 3 to 19.\n\n   0001 0011     19\n + 0000 0011      3\n ===========   ====\n   0001 0110     22\n\nSubtract −3 from 19.\n\n   0001 0011     19\n − 1111 1100     −3\n ===========   ====\n 1 0001 0111     23    —An [[end-around borrow]] is produced.\n − 0000 0001      1    —Subtract the end-around borrow from the result.\n ===========   ====\n   0001 0110     22    —The correct result (19 − (−3) = 22).\n\n==Negative zero==\n{{main article|Signed zero}}\n\nNegative zero is the condition where all bits in a signed word are 1.  This follows the ones' complement rules that a value is negative when the left-most bit is 1, and that a negative number is the bit complement of the number's magnitude.  The value also behaves as zero when computing.  Adding or subtracting negative zero to/from another value produces the original value.\n\nAdding negative zero:\n\n   0001 0110     22\n + 1111 1111     −0\n ===========   ====\n 1 0001 0101     21    An [[end-around carry]] is produced.\n + 0000 0001      1\n ===========   ====\n   0001 0110     22    The correct result (22 + (−0) = 22)\n\nSubtracting negative zero:\n\n   0001 0110     22\n − 1111 1111     −0\n ===========   ====\n 1 0001 0111     23    An [[end-around borrow]] is produced.\n − 0000 0001      1\n ===========   ====\n   0001 0110     22    The correct result (22 − (−0) = 22)\n\nNegative zero is easily produced in a 1's complement adder.  Simply add the positive and negative of the same magnitude.\n\n   0001 0110     22\n + 1110 1001    −22\n ===========   ====\n   1111 1111     −0    Negative zero.\n\nAlthough the math always produces the correct results, a side effect of negative zero is that software must test for negative zero.\n\n==Avoiding negative zero==\n\nThe generation of negative zero becomes a non-issue if addition is achieved with a complementing subtractor.  The first operand is passed to the subtract unmodified, the second operand is complemented, and the subtraction generates the correct result, avoiding negative zero. The previous example added 22 and −22 and produced −0.\n\n   0001 0110     22         0001 0110     22                  1110 1001   −22         1110 1001   −22\n + 1110 1001    −22       − 0001 0110     22                + 0001 0110    22       − 1110 1001   −22\n ===========   ====  but  ===========   ====   likewise,    ===========   ===  but  ===========   ===\n   1111 1111     −0         0000 0000      0                  1111 1111    −0         0000 0000     0\n\n\"Corner cases\" arise when one or both operands are zero and/or negative zero.\n\n   0001 0010     18         0001 0010     18\n − 0000 0000      0       − 1111 1111     −0\n ===========   ====       ===========   ====\n   0001 0010     18       1 0001 0011     19\n                          − 0000 0001      1\n                          ===========   ====\n                            0001 0010     18\n\nSubtracting +0 is trivial (as shown above).  If the second operand is negative zero it is inverted and the original value of the first operand is the result.  Subtracting −0 is also trivial.  The result can be only 1 of two cases.  In case 1, operand 1 is −0 so the result is produced simply by subtracting 1 from 1 at every bit position.  In case 2, the subtraction will generate a value that is 1 larger than operand 1 and an [[end-around borrow]].  Completing the borrow generates the same value as operand 1.\n\nThe next example shows what happens when both operands are plus or minus zero:\n\n   0000 0000      0         0000 0000      0         1111 1111     −0         1111 1111     −0\n + 0000 0000      0       + 1111 1111     −0       + 0000 0000      0       + 1111 1111     −0\n ===========   ====       ===========   ====       ===========   ====       ===========   ====\n   0000 0000      0         1111 1111     −0         1111 1111     −0       1 1111 1110     −1\n                                                                            + 0000 0001      1\n                                                                            ==================\n                                                                              1111 1111     −0\n\n   0000 0000      0         0000 0000      0         1111 1111     −0         1111 1111     −0\n − 1111 1111     −0       − 0000 0000      0       − 1111 1111     −0       − 0000 0000      0\n ===========   ====       ===========   ====       ===========   ====       ===========   ====\n 1 0000 0001      1         0000 0000      0         0000 0000      0         1111 1111     −0\n − 0000 0001      1\n ===========   ====\n   0000 0000      0\n\nThis example shows that of the 4 possible conditions when adding only ±0, an adder will produce −0 in three of them.  A complementing subtractor will produce −0 only when both operands are −0.\n\n== See also ==\n{{Portal|Computer programming}}\n\n* [[Signed number representations]]\n* [[Two's complement]]\n* [[IEEE floating point]]\n\n== References ==\n* [[Donald Knuth]]: ''[[The Art of Computer Programming]]'', Volume 2: Seminumerical Algorithms, chapter 4.1\n\n<!-- interwiki kept as it links to a section -->\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]\n[[Category:Unary operations]]\n\n[[th:การแทนจำนวนที่มีเครื่องหมาย#1’s Complement System]]"
    },
    {
      "title": "Parity bit",
      "url": "https://en.wikipedia.org/wiki/Parity_bit",
      "text": "{{ref improve|date=January 2013}}\n{{No footnotes|date=January 2017}}\n\n{| class=\"wikitable\" style=\"float: right; text-align: center; margin: 1em;\"\n|-\n! rowspan=\"2\" | 7 bits of data<br/>\n! rowspan=\"2\" | (count of 1-bits)\n! colspan=\"2\" | 8 bits including parity\n|- \n! 0\n\n! odd\n|-\n| 0000000 \n|0\n| '''0'''0000000\n| '''1'''0000000\n|- \n| 1010001\n| 3\n| '''1'''1010001\n| '''0'''1010001\n|- \n| 1101001\n|4\n| '''0'''1101001\n| '''1'''1101001\n|-\n| 1111111 \n|7\n| '''1'''1111111\n| '''0'''1111111\n|}\n\nA '''parity bit''', or '''check bit''', is a [[bit]] added to a string of [[binary code]] to ensure that the total number of 1-bits in the string is [[even number|even]] or [[odd number|odd]]<ref>{{Cite book |title=Principles of communication : systems, modulation, and noise |last=Ziemer |first=RodgerE. |last2=Tranter |first2=William H. |isbn=9781118078914 |edition=Seventh |location=Hoboken, New Jersey |oclc=856647730}}</ref>. Parity bits are used as the simplest form of [[Error detection and correction|error detecting code]].\n\nThere are two variants of parity bits: '''even parity bit''' and '''odd parity bit'''. \n\nIn the case of even parity, for a given set of bits, the occurrences of bits whose value is 1 is counted. If that count is odd, the parity bit value is set to 1, making the total count of occurrences of 1s in the whole set (including the parity bit) an even number. If the count of 1s in a given set of bits is already even, the parity bit's value is 0. \n\nIn the case of odd parity, the coding is reversed. For a given set of bits, if the count of bits with a value of 1 is even, the parity bit value is set to 1 making the total count of 1s in the whole set (including the parity bit) an odd number.  If the count of bits with a value of 1 is odd, the count is already odd so the parity bit's value is 0.\n\nEven parity is a special case of a [[cyclic redundancy check]] (CRC), where the 1-bit CRC is generated by the [[polynomial]] ''x''+1.\n\nIf a bit is present at a point otherwise dedicated to a parity bit, but is not used for parity, it may be referred to as a '''mark parity bit''' if the parity bit is always 1, or a '''space parity bit''' if the bit is always 0. In such cases where the value of the bit is constant, it may be called a '''stick parity bit''' even though its function has nothing to do with parity.<ref>What is the difference between using mark or space parity and parity-none [https://stackoverflow.com/questions/13953095/what-is-the-difference-between-using-mark-space-parity-and-parity-none/44612613#44612613]</ref> The function of such bits varies with the system design, but examples of functions for such bits include timing management, or identification of a packet as being of data or address significance.<ref>What is the purpose of the Stick Parity? [http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.faqs/ka13893.html]</ref> If its actual bit value is irrelevant to its function, the bit amounts to a [[don't-care term]].<ref>Serial Communications - Sat-Digest[http://www.sat-digest.com/SatXpress/SmartCard/SerialPort.htm]</ref>\n\nParity bits are generally applied to the smallest units of a communication protocol, typically 8-bit [[octet (computing)|octets]] (bytes), although they can also be applied separately to an entire message string of bits.\n\n==Parity==\nIn mathematics, [[parity (mathematics)|parity]] refers to the evenness or oddness of an integer, which for a [[binary numeral system|binary number]] is determined only by the [[least significant bit]]. In telecommunications and computing, parity refers to the evenness or oddness of the number of bits with value one within a given set of bits, and is thus determined by the value of all the bits. It can be calculated via a [[XOR]] sum of the bits, yielding 0 for even parity and 1 for odd parity. This property of being dependent upon all the bits and changing value if any one bit changes allows for its use in [[Error detection and correction#Parity bits|error detection]] schemes.\n\n==Error detection==\nIf an odd number of bits (including the parity bit) are [[Transmission (telecommunications)|transmitted]] incorrectly, the parity bit will be incorrect, thus indicating that a '''parity error''' occurred in the transmission. The parity bit is only suitable for detecting errors; it cannot [[error detection and correction|correct]] any errors, as there is no way to determine which particular bit is corrupted. The data must be discarded entirely, and [[automatic repeat-request|re-transmitted from scratch]]. On a noisy transmission medium, successful transmission can therefore take a long time, or even never occur. However, parity has the advantage that it uses only a single bit and requires only a number of [[XOR gate]]s to generate. See [[Hamming code]] for an example of an error-correcting code.\n\nParity bit checking is used occasionally for transmitting [[ASCII]] characters, which have 7 bits, leaving the 8th bit as a parity bit.\n\nFor example, the parity bit can be computed as follows. Assume [[Alice and Bob]] are communicating and Alice wants to send Bob the simple 4-bit message 1001.\n{| class=\"wikitable\"\n|-\n! Type of bit parity !! Successful transmission scenario\n|-\n| Even parity || \nAlice wants to transmit:          1001\n\nAlice computes parity bit value:  1+0+0+1 (mod 2) = 0 \n\nAlice adds parity bit and sends:  10010\n\nBob receives:                   10010\n\nBob computes parity:            1+0+0+1+0 (mod 2) = 0\n\nBob reports correct transmission after observing expected even result.\n|-\n| Odd parity ||\nAlice wants to transmit:          1001\n\nAlice computes parity bit value:  1+0+0+1 (mod 2) = 0\n\nAlice adds parity bit and sends:  1001'''1'''\n\nBob receives:                   10011\n\nBob computes overall parity:    1+0+0+1+1 (mod 2) = 1\n\nBob reports correct transmission after observing expected odd result.\n|}\n\nThis mechanism enables the detection of single bit errors, because if one bit gets flipped due to line noise, there will be an incorrect number of ones in the received data. In the two examples above, Bob's calculated parity value matches the parity bit in its received value, indicating there are no single bit errors. Consider the following example with a transmission error in the second bit using XOR:\n\n{| class=\"wikitable\"\n|-\n! Type of bit parity error !! Failed transmission scenario\n|-\n| Even parity\nError in the second bit \n|| Alice wants to transmit:          1001\n\nAlice computes parity bit value:  1^0^0^1 = 0\n\nAlice adds parity bit and sends:  10010\n\n'''...TRANSMISSION ERROR...'''\n\nBob receives:                   1'''1'''010\n\nBob computes overall parity:    1^1^0^1^0 = 1\n\nBob reports incorrect transmission after observing unexpected odd result.\n|-\n| Even parity\nError in the parity bit \n||Alice wants to transmit:          1001\n\nAlice computes even parity value: 1^0^0^1 = 0\n\nAlice sends:                      10010\n\n'''...TRANSMISSION ERROR...'''\n\nBob receives:                   1001'''1'''\n\nBob computes overall parity:    1^0^0^1^1 = 1\n\nBob reports incorrect transmission after observing unexpected odd result.\n|}\n\nThere is a limitation to parity schemes. A parity bit is only guaranteed to detect an odd number of bit errors. If an even number of bits have errors, the parity bit records the correct number of ones, even though the data is corrupt. (See also [[error detection and correction]].) Consider the same example as before with an even number of corrupted bits:\n\n{| class=\"wikitable\"\n|-\n! Type of bit parity error !! Failed transmission scenario\n|-\n| Even parity\nTwo corrupted bits \n|| Alice wants to transmit:          1001\n\nAlice computes even parity value: 1^0^0^1 = 0\n\nAlice sends:                      10010\n\n'''...TRANSMISSION ERROR...'''\n\nBob receives:                   1'''1'''01'''1'''\n\nBob computes overall parity:    1^1^0^1^1 = 0\n\nBob reports correct transmission though actually incorrect.\n|}\nBob observes even parity, as expected, thereby failing to catch the two bit errors.\n\n==Usage==\nBecause of its simplicity, parity is used in many [[computer hardware|hardware]] applications where an operation can be repeated in case of difficulty, or where simply detecting the error is helpful. For example, the [[SCSI]]  and [[PCI bus]]es use parity to detect transmission errors, and many [[microprocessor]] instruction [[CPU cache|caches]] include parity protection. Because the [[Instruction cache|I-cache]] data is just a copy of [[main memory]], it can be disregarded and re-fetched if it is found to be corrupted.\n\nIn [[serial communications|serial]] [[data transmission]], a common format is 7 data bits, an even parity bit, and one or two [[stop bit]]s. This format neatly accommodates all the 7-bit [[ASCII]] characters in a convenient 8-bit byte. Other formats are possible; 8 bits of data plus a parity bit can convey all 8-bit byte values. \n\nIn serial communication contexts, parity is usually generated and checked by interface hardware (e.g., a [[Universal asynchronous receiver-transmitter|UART]]) and, on reception, the result made available to a [[Processor (computing)|processor]] such as the CPU (and so too, for instance, the [[operating system]]) via a status bit in a [[hardware register]] in the [[Interface (computing)|interface]] hardware. Recovery from the error is usually done by retransmitting the data, the details of which are usually handled by software (e.g., the operating system I/O routines).\n\nWhen the total number of transmitted bits, including the parity bit, is even, odd parity has the advantage that the all-zeros and all-ones patterns are both detected as errors.  If the total number of bits is odd, only one of the patterns is detected as an error, and the choice can be made based on which is expected to be the more common error.\n\n===Redundant array of independent disks===\nParity data is used by some [[RAID|redundant array of independent disks]] (RAID) levels to achieve [[Redundancy (engineering)|redundancy]]. If a drive in the array fails, remaining data on the other drives can be combined with the parity data (using the Boolean [[XOR]] function) to reconstruct the missing data.\n\nFor example, suppose two drives in a three-drive [[RAID 5]] array contained the following data:\n\nDrive 1: '''01101101'''<br />Drive 2: '''11010100'''\n\nTo calculate parity data for the two drives, an XOR is performed on their data:\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'''01101101'''<br />XOR '''11010100'''<br />   _____________<br />&nbsp;  &nbsp; &nbsp; &nbsp; '''10111001'''\n\nThe resulting parity data, '''10111001''', is then stored on Drive 3.\n\nShould any of the three drives fail, the contents of the failed drive can be reconstructed on a replacement drive by subjecting the data from the remaining drives to the same XOR operation. If Drive 2 were to fail, its data could be rebuilt using the XOR results of the contents of the two remaining drives, Drive 1 and Drive 3:\n\nDrive 1: '''01101101'''<br />Drive 3: '''10111001'''\n\nas follows:\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'''10111001'''<br />XOR '''01101101'''<br />  _____________<br />&nbsp; &nbsp; &nbsp; &nbsp; '''11010100'''\n\nThe result of that XOR calculation yields Drive 2's contents. '''11010100''' is then stored on Drive 2, fully repairing the array. This same XOR concept applies similarly to larger arrays, using any number of disks. In the case of a RAID 3 array of 12 drives, 11 drives participate in the XOR calculation shown above and yield a value that is then stored on the dedicated parity drive.\n\n== History ==\nA \"parity track\" was present on the first [[magnetic tape data storage]] in 1951. Parity in this form, applied across multiple parallel signals, is known as a [[transverse redundancy check]]. This can be combined with parity computed over multiple bits sent on a single signal, a [[longitudinal redundancy check]]. In a parallel bus, there is one longitudinal redundancy check bit per parallel signal.\n\nParity was also used on at least some paper-tape ([[punched tape]]) data entry systems (which preceded magnetic tape systems). On the systems sold by British company ICL (formerly ICT) the {{convert|1|in|mm|adj=mid|-wide}} paper tape had 8 hole positions running across it, with the 8th being for parity. 7 positions were used for the data, e.g., 7-bit ASCII. The 8th position had a hole punched in it depending on the number of data holes punched.\n\n== See also ==\n* [[BIP-8]]\n* [[Parity function]]\n* [[Single event upset]]\n* [[8-N-1]]\n* [[Check digit]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://graphics.stanford.edu/~seander/bithacks.html Different methods of generating the parity bit, among other bit operations]\n\n{{DEFAULTSORT:Parity Bit}}\n[[Category:Binary arithmetic]]\n[[Category:Data transmission]]\n[[Category:Error detection and correction]]\n[[Category:Parity (mathematics)]]\n[[Category:RAID]]\n\n[[fr:Somme de contrôle#Exemple : bit de parité]]"
    },
    {
      "title": "Quadruple-precision floating-point format",
      "url": "https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format",
      "text": "In [[computing]], '''quadruple precision''' (or '''quad precision''') is a binary [[floating point]]–based [[computer number format]] that occupies 16 bytes (128 bits) with precision more than twice the 53-bit [[Double-precision floating-point format|double precision]].\n\nThis 128-bit quadruple precision is designed not only for applications requiring results in higher than double precision,<ref>{{cite web|url=http://crd.lbl.gov/~dhbailey/dhbpapers/dhb-jmb-acat08.pdf|title=High-Precision Computation and Mathematical Physics|author1=David H. Bailey  |author2=Jonathan M. Borwein |lastauthoramp=yes |date=July 6, 2009}}</ref> but also, as a primary function, to allow the computation of double precision results more reliably and accurately by minimising overflow and [[round-off error]]s in intermediate calculations and scratch variables. [[William Kahan]], primary architect of the original IEEE-754 floating point standard noted, \"For now the [[extended precision#x86 Architecture Extended Precision Format|10-byte Extended format]] is a tolerable compromise between the value of extra-precise arithmetic and the price of implementing it to run fast; very soon two more bytes of precision will become tolerable, and ultimately a 16-byte format ... That kind of gradual evolution towards wider precision was already in view when [[IEEE 754|IEEE Standard 754 for Floating-Point Arithmetic]] was framed.\"<ref>{{cite book|first=Nicholas | last=Higham |title=\"Designing stable algorithms\" in Accuracy and Stability of Numerical Algorithms (2 ed)| publisher=SIAM|year=2002 | pages=43 }}</ref>\n\nIn [[IEEE 754-2008]] the 128-bit base-2 format is officially referred to as '''binary128'''.\n\n{{Floating-point}}\n\n== IEEE 754 quadruple-precision binary floating-point format: binary128 ==\nThe IEEE 754 standard specifies a '''binary128''' as having:\n\n* [[Sign bit]]: 1 bit\n* [[Exponent]] width: 15 bits\n* [[Significand]] [[precision (arithmetic)|precision]]: 113 bits (112 explicitly stored)\n<!-- \"significand\", with a d at the end, is a technical term, please do not confuse with \"significant\" -->\n\nThis gives from 33 to 36 significant decimal digits precision. If a decimal string with at most 33 significant digits is converted to IEEE 754 quadruple-precision representation, and then converted back to a decimal string with the same number of digits, the final result should match the original string. If an IEEE 754 quadruple-precision number is converted to a decimal string with at least 36 significant digits, and then converted back to quadruple-precision representation, the final result must match the original number.<ref name=whyieee>{{cite web|url=http://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF|title=Lecture Notes on the Status of IEEE Standard 754 for Binary Floating-Point Arithmetic| author=William Kahan |date=1 October 1987}}</ref>\n\nThe format is written with an implicit lead bit with value 1 unless the  exponent is stored with all zeros. Thus only 112 bits of the [[significand]] appear in the memory format, but the total precision is 113 bits (approximately 34 decimal digits: {{nowrap|log<sub>10</sub>(2<sup>113</sup>) ≈ 34.016}}). The bits are laid out as:\n\n[[File:IEEE 754 Quadruple Floating Point Format.svg|800px|A sign bit, a 15-bit exponent, and a 112-bit significand]]\n\nA '''binary256''' would have a significand precision of 237 bits (approximately 71 decimal digits) and exponent bias 262143.\n\n=== Exponent encoding ===\nThe quadruple-precision binary floating-point exponent is encoded using an [[offset binary]] representation, with the zero offset being 16383; this is also known as exponent bias in the IEEE 754 standard.\n\n* E<sub>min</sub> = 0001<sub>16</sub> − 3FFF<sub>16</sub> = −16382\n* E<sub>max</sub> = 7FFE<sub>16</sub> − 3FFF<sub>16</sub> = 16383\n* [[Exponent bias]] = 3FFF<sub>16</sub> = 16383\n\nThus, as defined by the offset binary representation, in order to get the true exponent, the offset of 16383 has to be subtracted from the stored exponent.\n\nThe stored exponents 0000<sub>16</sub> and 7FFF<sub>16</sub> are interpreted specially.\n\n{|class=\"wikitable\" style=\"text-align:center\"\n! Exponent !! Significand zero !! Significand non-zero !! Equation\n|-\n| 0000<sub>16</sub> || [[0 (number)|0]], [[−0]] || [[subnormal numbers]] || (−1)<sup>signbit</sup> × 2<sup>−16382</sup> × 0.significandbits<sub>2</sub>\n|-\n| 0001<sub>16</sub>, ..., 7FFE<sub>16</sub> ||colspan=2| normalized value || (−1)<sup>signbit</sup> × 2<sup>exponentbits<sub>2</sub> − 16383</sup> × 1.significandbits<sub>2</sub>\n|-\n| 7FFF<sub>16</sub> || ±[[infinity|∞]] || [[NaN]] (quiet, signalling)\n|}\n\nThe minimum strictly positive (subnormal) value is 2<sup>−16494</sup> ≈ 10<sup>−4965</sup> and has a precision of only one bit.\nThe minimum positive normal value is 2<sup>−16382</sup> ≈ {{nowrap|3.3621 × 10<sup>−4932</sup>}} and has a precision of 113&nbsp;bits, i.e. ±2<sup>−16494</sup> as well. The maximum representable value is {{nowrap|2<sup>16384</sup> − 2<sup>16271</sup>}} ≈ {{nowrap|1.1897 × 10<sup>4932</sup>}}.\n\n=== Quadruple precision examples ===\nThese examples are given in bit ''representation'', in [[hexadecimal]],\nof the floating-point value. This includes the sign, (biased) exponent, and significand.\n\n 0000 0000 0000 0000 0000 0000 0000 0001<sub>16</sub> = 2<sup>−16382</sup> × 2<sup>−112</sup> = 2<sup>−16494</sup>\n                                           ≈ 6.4751751194380251109244389582276465525 × 10<sup>−4966</sup>\n                                             (smallest positive subnormal number)\n\n 0000 ffff ffff ffff ffff ffff ffff ffff<sub>16</sub> = 2<sup>−16382</sup> × (1 − 2<sup>−112</sup>)\n                                           ≈ 3.3621031431120935062626778173217519551 × 10<sup>−4932</sup>\n                                             (largest subnormal number)\n\n 0001 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = 2<sup>−16382</sup>\n                                           ≈ 3.3621031431120935062626778173217526026 × 10<sup>−4932</sup>\n                                             (smallest positive normal number)\n\n 7ffe ffff ffff ffff ffff ffff ffff ffff<sub>16</sub> = 2<sup>16383</sup> × (2 − 2<sup>−112</sup>)\n                                           ≈ 1.1897314953572317650857593266280070162 × 10<sup>4932</sup>\n                                             (largest normal number)\n\n 3ffe ffff ffff ffff ffff ffff ffff ffff<sub>16</sub> = 1 − 2<sup>−113</sup>\n                                           ≈ 0.9999999999999999999999999999999999037\n                                             (largest number less than one)\n\n 3fff 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = 1 (one)\n\n 3fff 0000 0000 0000 0000 0000 0000 0001<sub>16</sub> = 1 + 2<sup>−112</sup>\n                                           ≈ 1.0000000000000000000000000000000001926\n                                             (smallest number larger than one)\n\n c000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = −2\n\n 0000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = 0\n 8000 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = −0\n\n 7fff 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = infinity\n ffff 0000 0000 0000 0000 0000 0000 0000<sub>16</sub> = −infinity\n\n 4000 921f b544 42d1 8469 898c c517 01b8<sub>16</sub> ≈ π\n\n 3ffd 5555 5555 5555 5555 5555 5555 5555<sub>16</sub> ≈ 1/3\n\nBy default, 1/3 rounds down like [[double precision]], because of the odd number of bits in the significand.\nSo the bits beyond the rounding point are <code>0101...</code> which is less than 1/2 of a [[unit in the last place]].\n\n== Double-double arithmetic ==\nA common software technique to implement nearly quadruple precision using ''pairs'' of [[double-precision]] values is sometimes called '''double-double arithmetic'''.<ref name=Hida>Yozo Hida, X. Li, and D. H. Bailey, [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.5769 Quad-Double Arithmetic: Algorithms, Implementation, and Application], Lawrence Berkeley National Laboratory Technical Report LBNL-46996 (2000). Also Y. Hida et al., [http://web.mit.edu/tabbott/Public/quaddouble-debian/qd-2.3.4-old/docs/qd.pdf Library for double-double and quad-double arithmetic] (2007).</ref><ref name=Shewchuk>J. R. Shewchuk, [http://www.cs.cmu.edu/~quake/robust.html Adaptive Precision Floating-Point Arithmetic and Fast Robust Geometric Predicates], Discrete & Computational Geometry 18:305-363, 1997.</ref><ref name=\"Knuth-4.2.3-pr9\">{{cite book |last=Knuth |first=D. E. |title=The Art of Computer Programming |edition=2nd |at=chapter 4.2.3. problem 9. }}</ref>  Using pairs of IEEE double-precision values with 53-bit significands, double-double arithmetic can represent operations with at least<ref name=Hida/> a 2&times;53=106-bit significand (actually 107 bits<ref>Robert Munafo [http://mrob.com/pub/math/f161.html F107 and F161 High-Precision Floating-Point Data Types] (2011).</ref> except for some of the largest values, due to the limited exponent range), only slightly less precise than the 113-bit significand of IEEE binary128 quadruple precision. The range of a double-double remains essentially the same as the double-precision format because the exponent has still 11 bits,<ref name=Hida /> significantly lower than the 15-bit exponent of IEEE quadruple precision (a range of {{nowrap|1.8 × 10<sup>308</sup>}} for double-double versus {{nowrap|1.2 × 10<sup>4932</sup>}} for binary128).\n\nIn particular, a double-double/quadruple-precision value ''q'' in the double-double technique is represented implicitly as a sum {{nowrap|1=''q'' = ''x'' + ''y''}} of two double-precision values ''x'' and ''y'', each of which supplies half of ''q''<nowiki/>'s significand.<ref name=Shewchuk/>  That is, the pair {{nowrap|(''x'', ''y'')}} is stored in place of ''q'', and operations on ''q'' values {{nowrap|(+, −, ×, ...)}} are transformed into equivalent (but more complicated) operations on the ''x'' and ''y'' values.  Thus, arithmetic in this technique reduces to a sequence of double-precision operations; since double-precision arithmetic is commonly implemented in hardware, double-double arithmetic is typically substantially faster than more general [[arbitrary-precision arithmetic]] techniques.<ref name=Hida/><ref name=Shewchuk/>\n\nNote that double-double arithmetic has the following special characteristics:<ref>[http://pic.dhe.ibm.com/infocenter/aix/v7r1/index.jsp?topic=%2Fcom.ibm.aix.genprogc%2Fdoc%2Fgenprogc%2F128bit_long_double_floating-point_datatype.htm 128-Bit Long Double Floating-Point Data Type]</ref>\n\n* As the magnitude of the value decreases, the amount of extra precision also decreases. Therefore, the smallest number in the normalized range is narrower than double precision. The smallest number with full precision is {{nowrap|1000...0<sub>2</sub> (106 zeros) × 2<sup>−1074</sup>}}, or {{nowrap|1.000...0<sub>2</sub> (106 zeros) × 2<sup>−968</sup>}}. Numbers whose magnitude is smaller than 2<sup>−1021</sup> will not have additional precision compared with double precision.\n* The actual number of bits of precision can vary. In general, the magnitude of the low-order part of the number is no greater than half ULP of the high-order part. If the low-order part is less than half ULP of the high-order part, significant bits (either all 0's or all 1's) are implied between the significant of the high-order and low-order numbers. Certain algorithms that rely on having a fixed number of bits in the significand can fail when using 128-bit long double numbers.\n* Because of the reason above, it is possible to represent values like {{nowrap|1 + 2<sup>−1074</sup>}}, which is the smallest representable number greater than 1.\n\nIn addition to the double-double arithmetic, it is also possible to generate triple-double or quad-double arithmetic if higher precision is required without any higher precision floating-point library. They are represented as a sum of three (or four) double-precision values respectively. They can represent operations with at least 159/161 and 212/215 bits respectively.\n\nA similar technique can be used to produce a '''double-quad arithmetic''', which is represented as a sum of two quadruple-precision values. They can represent operations with at least 226 (or 227) bits.<ref>sourceware.org [http://sourceware.org/ml/libc-alpha/2012-03/msg01024.html Re: The state of glibc libm]</ref>\n\n==Implementations==\nQuadruple precision is often implemented in software by a variety of techniques (such as the double-double technique above, although that technique does not implement IEEE quadruple precision), since direct hardware support for quadruple precision is, as of 2016, less common (see \"[[#Hardware support|Hardware support]]\" below).  One can use general [[arbitrary-precision arithmetic]] libraries to obtain quadruple (or higher) precision, but specialized quadruple-precision implementations may achieve higher performance.\n\n===Computer-language support===\nA separate question is the extent to which quadruple-precision types are directly incorporated into computer [[programming language]]s.\n\nQuadruple precision is specified in [[Fortran]] by the <code>real(real128)</code> (module <code>iso_fortran_env</code> from Fortran 2008 must be used, the constant <code>real128</code> is equal to 16 on most processors),  or as <code>real(selected_real_kind(33, 4931))</code>, or in a non-standard way as <code>REAL*16</code>.  (Quadruple-precision <code>REAL*16</code> is supported by the [[Intel Fortran Compiler]]<ref>{{cite web|title=Intel Fortran Compiler Product Brief (archived copy on web.archive.org) |url=http://h21007.www2.hp.com/portal/download/files/unprot/intel/product_brief_Fortran_Linux.pdf |work= |publisher=Su |date= |accessdate=2010-01-23 |deadurl=unfit |archiveurl=https://web.archive.org/web/20081025174427/http://h21007.www2.hp.com/portal/download/files/unprot/intel/product_brief_Fortran_Linux.pdf |archivedate=October 25, 2008 }}</ref> and by the [[GNU Fortran]] compiler<ref>{{cite web|title= GCC 4.6 Release Series - Changes, New Features, and Fixes |url=https://gcc.gnu.org/gcc-4.6/changes.html|work=|publisher=|date=|accessdate=2010-02-06}}</ref> on [[x86]], [[x86-64]], and [[Itanium]] architectures, for example.)\n\nIn [[C (programming language)|C]]/[[C++]] with a few systems and compilers, quadruple precision may be specified by the [[long double]] type, but this is not required by the language (which only requires <code>long double</code> to be at least as precise as <code>double</code>), nor is it common.  On x86 and x86-64, the most common C/C++ compilers implement <code>long double</code> as either 80-bit [[extended precision]] (e.g. the [[GNU C Compiler]] gcc<ref>[https://web.archive.org/web/20080713131713/https://gcc.gnu.org/onlinedocs/gcc/i386-and-x86_002d64-Options.html i386 and x86-64 Options (archived copy on web.archive.org)], ''Using the GNU Compiler Collection''.</ref> and the [[Intel C++ compiler]] with a <code>/Qlong&#8209;double</code> switch<ref>[http://software.intel.com/en-us/articles/size-of-long-integer-type-on-different-architecture-and-os/ Intel Developer Site]</ref>) or simply as being synonymous with double precision (e.g. [[Microsoft Visual C++]]<ref>[http://msdn.microsoft.com/en-us/library/9cx8xs15.aspx MSDN homepage, about Visual C++ compiler]</ref>), rather than as quadruple precision.  On a few other architectures, some C/C++ compilers implement <code>long double</code> as quadruple precision, e.g. gcc on [[PowerPC]] (as double-double<ref>[https://gcc.gnu.org/onlinedocs/gcc/RS_002f6000-and-PowerPC-Options.html RS/6000 and PowerPC Options], ''Using the GNU Compiler Collection''.</ref><ref>[https://developer.apple.com/legacy/mac/library/documentation/Performance/Conceptual/Mac_OSX_Numerics/Mac_OSX_Numerics.pdf Inside Macintosh - PowerPC Numerics] {{webarchive |url=https://web.archive.org/web/20121009191824/http://developer.apple.com/legacy/mac/library/documentation/Performance/Conceptual/Mac_OSX_Numerics/Mac_OSX_Numerics.pdf |date=October 9, 2012 }}</ref><ref>[https://opensource.apple.com/source/gcc/gcc-5646/gcc/config/rs6000/darwin-ldouble.c 128-bit long double support routines for Darwin]</ref>) and [[SPARC]],<ref>[https://gcc.gnu.org/onlinedocs/gcc/SPARC-Options.html SPARC Options], ''Using the GNU Compiler Collection''.</ref> or the [[Sun Studio (software)|Sun Studio compilers]] on SPARC.<ref>[http://docs.oracle.com/cd/E19422-01/819-3693/ncg_lib.html The Math Libraries], Sun Studio 11 ''Numerical Computation Guide'' (2005).</ref>  Even if <code>long double</code> is not quadruple precision, however, some C/C++ compilers provide a nonstandard quadruple-precision type as an extension.  For example, gcc provides a quadruple-precision type called <code>__float128</code> for x86, x86-64 and [[Itanium]] CPUs,<ref>[https://gcc.gnu.org/onlinedocs/gcc/Floating-Types.html Additional Floating Types], ''Using the GNU Compiler Collection''</ref> and on [[PowerPC]] as IEEE 128-bit floating-point using the -mfloat128-hardware or -mfloat128 options;<ref name=gcc6changes>{{cite web|title=GCC 6 Release Series - Changes, New Features, and Fixes|url=https://gcc.gnu.org/gcc-6/changes.html|accessdate=2016-09-13}}</ref> and some versions of Intel's C/C++ compiler for x86 and x86-64 supply a nonstandard quadruple-precision type called <code>_Quad</code>.<ref>[http://software.intel.com/en-us/forums/showthread.php?t=56359 Intel C++ Forums] (2007).</ref>\n\n===Libraries and toolboxes===\n* The [[GNU Compiler Collection|GCC]] quad-precision math library, [https://gcc.gnu.org/onlinedocs/libquadmath libquadmath], provides <code>__float128</code> and <code>__complex128</code> operations.\n* The [[Boost (C++ libraries)|Boost]] multiprecision library Boost.Multiprecision provides unified cross-platform C++ interface for <code>__float128</code> and <code>_Quad</code> types, and includes a custom implementation of the standard math library.<ref>{{cite web|url=http://www.boost.org/doc/libs/1_58_0/libs/multiprecision/doc/html/boost_multiprecision/tut/floats/float128.html|title=Boost.Multiprecision - float128|accessdate=2015-06-22}}</ref>\n* The Multiprecision Computing Toolbox for MATLAB allows quadruple-precision computations in [[MATLAB]]. It includes basic arithmetic functionality as well as numerical methods, dense and sparse linear algebra.<ref>{{cite web|url=http://www.advanpix.com/2013/01/20/fast-quadruple-precision-computations/|title=Fast Quadruple Precision Computations in MATLAB|author=Pavel Holoborodko|date=2013-01-20|accessdate=2015-06-22}}</ref>\n* The DoubleDouble<ref>{{cite web|title=DoubleDouble.jl|url=https://github.com/simonbyrne/DoubleDouble.jl}}</ref> package provides support for double-double computations for the Julia programming language.\n* The doubledouble.py<ref>{{cite web|title=doubledouble.py|url=https://github.com/sukop/doubledouble}}</ref> library enables double-double computations in Python.\n\n=== Hardware support ===\nThe IBM [[POWER9]] CPU ([[Power ISA#Power ISA v.3.0|Power ISA 3.0]]) has native 128-bit hardware support.<ref name=gcc6changes/>\n\nNative support of IEEE 128-bit floats is defined in [[PA-RISC]] 1.0,<ref>[http://grouper.ieee.org/groups//754/email/msg04128.html Implementor support for the binary interchange formats]</ref> and in [[SPARC]] V8<ref>{{cite book\n |title       = The SPARC Architecture Manual: Version 8 (archived copy on web.archive.org)\n |year        = 1992\n |publisher   = SPARC International, Inc\n |url         = http://www.sparc.org/standards/V8.pdf\n |accessdate  = 2011-09-24\n |quote       = SPARC is an instruction set architecture (ISA) with 32-bit integer and 32-, 64-, and 128-bit IEEE Standard 754 floating-point as its principal data types.\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20050204100221/http://www.sparc.org/standards/V8.pdf\n |archivedate = 2005-02-04\n |df          = \n}}</ref> and V9<ref>{{cite book |title=The SPARC Architecture Manual: Version 9 (archived copy on web.archive.org) |year=1994 |editor1=David L. Weaver |editor2=Tom Germond |publisher=SPARC International, Inc |url=http://www.sparc.org/standards/SPARCV9.pdf |accessdate=2011-09-24 |quote=Floating-point: The architecture provides an IEEE 754-compatible floating-point instruction set, operating on a separate register file that provides 32 single-precision (32-bit), 32 double-precision (64-bit), 16 quad-precision (128-bit) registers, or a mixture thereof. |deadurl=yes |archiveurl=https://web.archive.org/web/20120118213535/http://www.sparc.org/standards/SPARCV9.pdf |archivedate=2012-01-18 |df= }}</ref> architectures (e.g. there are 16 quad-precision registers %q0, %q4, ...), but no SPARC CPU implements quad-precision operations in hardware {{as of|2004|lc=on}}.<ref>{{cite book\n | title      = Numerical Computation Guide &mdash; Sun Studio 10\n | chapter    = SPARC Behavior and Implementation\n | year       = 2004\n | publisher  = Sun Microsystems, Inc\n | url        = http://docs.oracle.com/cd/E19059-01/stud.10/819-0499/ncg_sparc.html\n | accessdate = 2011-09-24\n | quote      = There are four situations, however, when the hardware will not successfully complete a floating-point instruction: ... The instruction is not implemented by the hardware (such as ... quad-precision instructions on any SPARC FPU).\n}}</ref>\n\n[[IBM Floating Point Architecture#Extended-precision 128-bit|Non-IEEE extended-precision]] (128 bit of storage, 1 sign bit, 7 exponent bit, 112 fraction bit, 8 bits unused) was added to the [[IBM System/370]] series (1970s–1980s) and was available on some S/360 models in the 1960s (S/360-85,<ref>{{cite journal | doi = 10.1147/sj.71.0022 | volume=7 | title=Structural aspects of the System/360 Model 85, III: Extensions to floating-point architecture | year=1968 | journal=IBM Systems Journal | pages=22–29 | author=Padegs A}}</ref> -195, and others by special request or simulated by OS software). IEEE quadruple precision was added to the [[S/390]] G5 in 1998,<ref>[http://domino.research.ibm.com/tchjr/journalindex.nsf/c469af92ea9eceac85256bd50048567c/6c80b870ebb2c30585256bfa0067f9e9!OpenDocument \"The S/390 G5 floating-point unit\", Schwarz, E. M. and Krygowsk, C. A., ''IBM Journal of Research and Development''], Vol:43 No: 5/6 (1999), p.707</ref> and is supported in hardware in subsequent [[z/Architecture]] processors.<ref>{{cite news|title=The IBM eServer z990 floating-point unit. IBM J. Res. Dev. 48; pp. 311-322|author=Gerwig, G. and Wetter, H. and Schwarz, E. M. and Haess, J. and Krygowski, C. A. and Fleischer, B. M. and Kroener, M.|date=May 2004}}</ref><ref>{{cite web|url=http://arith22.gforge.inria.fr/slides/s1-schwarz.pdf|author=Eric Schwarz|title=The IBM z13 SIMD Accelerators for Integer, String, and Floating-Point|date=June 22, 2015|accessdate=July 13, 2015}}</ref>\n\nThe [[VAX]] processor implemented non-IEEE quadruple-precision floating point as its \"H Floating-point\" format. It had one sign bit, a 15-bit exponent and 112-fraction bits, however the layout in memory was significantly different from IEEE quadruple precision and the exponent bias also differed.  Only a few of the earliest VAX processors implemented H Floating-point instructions in hardware, all the others emulated H Floating-point in software.\n\nQuadruple-precision (128-bit) hardware implementation should not be confused with \"128-bit FPUs\" that implement [[SIMD]] instructions, such as [[Streaming SIMD Extensions]] or [[AltiVec]], which refers to 128-bit [[Vector processor|vectors]] of four 32-bit single-precision or two 64-bit double-precision values that are operated on simultaneously.\n\n== See also ==\n* [[IEEE 754]], IEEE standard for floating-point arithmetic\n* [[ISO/IEC 10967]], Language Independent Arithmetic\n* [[Primitive data type]]\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [http://crd.lbl.gov/~dhbailey/mpdist/ High-Precision Software Directory]\n* [https://github.com/coder0xff/QPFloat QPFloat], a [[free software]] ([[GPL]]) software library for quadruple-precision arithmetic\n* [http://www.nongnu.org/hpalib/ HPAlib], a free software ([[LGPL]]) software library for quad-precision arithmetic\n* [https://gcc.gnu.org/onlinedocs/libquadmath libquadmath], the [[GNU Compiler Collection|GCC]] quad-precision math library\n* [http://babbage.cs.qc.cuny.edu/IEEE-754 IEEE-754 Analysis], Interactive web page for examining Binary32, Binary64, and Binary128 floating-point values\n\n{{data types}}\n\n[[Category:Binary arithmetic]]\n[[Category:Floating point types]]"
    },
    {
      "title": "Redundant binary representation",
      "url": "https://en.wikipedia.org/wiki/Redundant_binary_representation",
      "text": "A '''redundant binary representation (RBR)''' is a [[numeral system]] that uses more bits than needed to represent a single binary [[Numerical digit|digit]] so that most numbers have several representations. An RBR is unlike usual [[binary numeral system]]s, including [[two's complement]], which use a single bit for each digit. Many of an RBR's properties differ from those of regular binary representation systems. Most importantly, an RBR allows addition without using a typical carry.<ref>{{Cite journal |first1=Dhananjay S. |last1=Phatak |first2=Israel |last2=Koren |title=Hybrid Signed-Digit Number Systems: A Unified Framework for Redundant Number Representations with Bounded Carry Propagation Chains |date=August 1994 |journal=IEEE Transactions on Computers |volume=43 |issue=8 |pages=880&ndash;891 |doi=10.1109/12.295850 |url=http://euler.ecs.umass.edu/research/PhKo-IEEETC-1994.pdf|citeseerx=10.1.1.352.6407 }}</ref> When compared to non-redundant representation, an RBR makes [[bitwise operation|bitwise logical operation]] slower, but [[arithmetic operation#Arithmetic operations|arithmetic operations]] are faster when a greater bit width is used.<ref>{{cite web |first=Louis Philippe |last=Lessard |title=Fast Arithmetic on FPGA Using Redundant Binary Apparatus |year=2008 |url=http://www.louislessard.com/rbin/ |accessdate=2015-09-12}}</ref>  Usually, each digit has its own sign that is not necessarily the same as the sign of the number represented. When digits have signs, that RBR is also a [[signed-digit representation]].\n\n==Conversion from RBR==\nAn RBR is a [[positional notation|place-value notation system]]. In an RBR, [[Numerical digit|digit]]s are ''pairs'' of bits, that is, for every place, an RBR uses a pair of bits. The value represented by a redundant digit can be found using a translation table. This table indicates the mathematical value of each possible pair of bits.\n\nAs in conventional binary representation, the [[integer]] value of a given representation is a weighted sum of the values of the digits. The weight starts at 1 for the rightmost position and goes up by a factor of 2 for each next position. Usually, an RBR allows negative values. There is no single sign bit that tells if a redundantly represented number is positive or negative. Most integers have several possible representations in an RBR.\n\nOften one of the several possible representations of an integer is chosen as the \"canonical\" form, so each integer has only one possible \"canonical\" representation; [[non-adjacent form]] and two's complement are popular choices for that canonical form.\n\nAn [[integer]] value can be converted back from an RBR using the following formula, where ''n'' is the number of digit and ''d<sub>k</sub>'' is the interpreted value of the ''k''-th digit, where ''k'' starts at 0 at the rightmost position:\n:<math>\n\\sum_{k=0}^{n-1} d_k 2^k\n</math>\n\nThe conversion from an RBR to ''n''-bit two's complement can be done in O(log(''n'')) time using a [[prefix adder]].<ref>{{Cite conference |first1=Sreehari |last1=Veeramachaneni |first2=M. Kirthi |last2=Krishna |first3=Lingamneni |last3=Avinash |first4=Sreekanth |last4=Reddy P. |first5=M.B. |last5=Srinivas |title=Novel High-Speed Redundant Binary to Binary converter using Prefix Networks  |conference=IEEE International Symposium on Circuits and Systems (ISCAS 2007) |date=May 2007 |location=New Orleans |doi=10.1109/ISCAS.2007.378170 |url=http://euler.ecs.umass.edu/research/PhKo-IEEETC-1994.pdf}}</ref>\n\n==Example of redundant binary representation==\n{| class=\"wikitable\" align=\"right\" style=\"margin-left: 3em; text-align:center;\"\n|+ Example of translation table for a digit\n! Digit !! Interpreted value\n|-\n| 00 || −1\n|-\n| 01 || &nbsp;0\n|-\n| 10 || &nbsp;0\n|-\n| 11 || &nbsp;1\n|}\nNot all redundant representations have the same properties. For example, using the translation table on the right, the number 1 can be represented in this RBR in many ways: \"01·01·01·11\" (0+0+0+1), \"01·01·10·11\" (0+0+0+1), \"01·01·11·00\" (0+0+2−1), or \"11·00·00·00\" (8−4−2−1).  Also, for this translation table, flipping all bits ([[NOT gate]]) corresponds to finding the [[additive inverse]] ([[multiplication]] by [[−1]]) of the integer represented.<ref>{{Cite journal |first1=Marcel |last1=Lapointe |first2=Huu Tue |last2=Huynh |first3=Paul |last3=Fortier |title=Systematic Design of Pipelined Recursive Filters |journal=IEEE Transactions on Computers |volume=42 |issue=4 |date=April 1993 |pages=413&ndash;426 |doi=10.1109/12.214688}}</ref>\n\nIn this case: <math>d_k \\isin \\{ -1, 0, 1 \\}</math>\n\n==Arithmetic operations==\nRedundant representations are commonly used inside high-speed [[arithmetic logic unit]]s.\n\nIn particular, a [[carry-save adder]] uses a redundant representation.{{Cn|date=September 2015}}\n\n===Addition===\n[[Image:Redundant binary adder.png|thumb|right|Schematic of an adder unit using [[full adder]] block (z = x + y)]]\nThe addition operation in all RBRs is carry-free, which means that the carry does not have to propagate through the full width of the addition unit. In effect, the addition in all RBRs is a constant-time operation. The addition will always take the same amount of time independently of the bit-width of the [[operand]]s. This does not imply that the addition is always faster in an RBR than its [[two's complement]] equivalent, but that the addition will eventually be faster in an RBR with increasing bit width because the two's complement addition unit's delay is proportional to log(''n'') (where ''n'' is the bit width).<ref>{{Cite conference |author1=Yu-Ting Pai |author2=Yu-Kumg Chen |title=The fastest carry lookahead adder |conference=Second IEEE International Workshop on Electronic Design, Test and Applications (DELTA '04) |location=Perth |date=January 2004 |doi=10.1109/DELTA.2004.10071 |url=http://galia.fc.uaslp.mx/~rmariela/digital/FastCLA.pdf}}</ref>  Addition in an RBR takes a constant time because each digit of the result can be calculated independently of one another, implying that each digit of the result can be calculated in parallel.<ref>{{Cite conference |first1=Bijoy |last1=Jose |first2=Damu |last2=Radhakrishnan |title=Delay Optimized Redundant Binary Adders |conference=13th IEEE International Conference on Electronics, Circuits and Systems, 2006. (ICECS '06) |date=December 2006 |location=Nice |doi=10.1109/ICECS.2006.379838}}</ref>\n\n===Subtraction===\nSubtraction is the same as the addition except that the additive inverse of the second operand needs to be computed first. For common representations, this can be done on a digit-by-digit basis.\n\n==Logical operations==\nBitwise logical operations, such as [[AND gate|AND]], [[logical disjunction|OR]] and [[XOR]], are not possible in redundant representations.  While it is possible to do [[bitwise operation]]s directly on the underlying bits inside an RBR, it is not clear that this is a meaningful operation; there are many ways to represent a value in an RBR, and the value of the result would depend on the representation used.\n\nTo get the expected results, it is necessary to convert the two operands first to non-redundant representations. Consequently, logical operations are slower in an RBR. More precisely, they take a time proportional to log(''n'') (where ''n'' is the number of digit) compared to a constant-time in [[two's complement]].\n\nIt is, however, possible to ''partially'' convert only the least-significant portion of a redundantly represented number to non-redundant form.  This allows operations such as masking off the low ''k'' bits can be done in log(''k'') time.\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Redundant Binary Representation}}\n{{Processor technologies|state=collapsed}}\n[[Category:Binary arithmetic]]\n[[Category:Non-standard positional numeral systems]]"
    },
    {
      "title": "Binary scaling",
      "url": "https://en.wikipedia.org/wiki/Binary_scaling",
      "text": "{{Refimprove|date=December 2009}}\n'''Binary scaling''' is a [[computer programming]] technique used typically in embedded [[C (programming language)|C]], [[Digital signal processing|DSP]] and [[assembly language|assembler]] programs to implement pseudo [[floating point]] operations by using the native [[integer]] arithmetic of the processor. \n\n==Overview==\nA representation of a floating point value using binary scaling is more precise than a floating point representation occupying the same number of bits, but typically represents values of a more limited range, therefore more easily leading to [[arithmetic overflow]] during computation.  Implementation of operations using integer arithmetic instructions is often (but not always) faster than the corresponding floating point instructions.\n\nA position for the 'binary point' is chosen for each variable to be represented, and binary shifts associated with arithmetic operations are adjusted accordingly. The Binary scaling corresponds in [[Q (number format)]] to the first digit, i.e. Q1.15 is a 16 bit integer\nscaled with one bit as integer and fifteen as fractional. A Bscal 1 or Q1.15 number would represent approx 1.999 to -2.0 as [[floating point]].\n\nTo give an example, a common way to use [[Arbitrary-precision arithmetic|integer arithmetic]] to simulate floating point, using 32 bit numbers, is to multiply the coefficients by 65536.\n\nUsing [[binary scientific notation]], this will place the binary point at B16.  That is to say, the most significant 16 bits represent the integer part the remainder are represent the fractional part.  This means, as a signed two's complement integer B16 number can hold a highest value of <math> \\approx 32767.9999847 </math> and a lowest value of −32768.0. Put another way, the B number, is the number of integer bits used to represent the number which defines its value range. Remaining low  bits (i.e. the non-integer bits) are used to store fractional quantities and supply more accuracy.\n\nFor instance, to represent 1.2 and 5.6 as B16 one multiplies them by 2<sup>16</sup>, giving 78643 and 367001.\n\nMultiplying these together gives\n\n 28862059643\n\nTo convert it back to B16, divide it by 2<sup>16</sup>.\n\nThis gives 440400B16, which when converted back to a floating point number (by dividing again by 2<sup>16</sup>, but holding the result as floating point) gives 6.71999.  The correct floating point result is 6.72.\n\n==Re-scaling after multiplication==\nThe example above for a B16 multiplication is a simplified example. Re-scaling depends on both the B scale value and the word size. B16 is often used in 32 bit systems because it works simply by multiplying and dividing by 65536 (or shifting 16 bits).\n\nConsider the Binary Point in a signed 32 bit word thus:\n\n 0 1 2 3 4 5 6 7 8 9\n  S X X X X X X X   X X X X X X X X   X X X X X X X X   X X X X X X X X\n\nwhere S is the sign bit and X are the other bits.\n\nPlacing the binary point at\n* 0 gives a range of −1.0 to 0.999999.\n* 1 gives a range of −2.0 to 1.999999\n* 2 gives a range of −4.0 to 3.999999 and so on.\n\nWhen using different B scalings and/or word sizes the complete B scaling conversion formula must be used.\n\nConsider a 32 bit word size, and two variables, one with a B scaling of 2 and the other with a scaling of 4.\n\n 1.4 @ B2 is 1.4 * (2 ^ (wordsize-2-1)) == 1.4 * 2 ^ 29 == 0x2CCCCCCD\n\nNote that here the 1.4 values is very well represented with 30 fraction bits. A 32 bit [[IEEE floating-point standard|floating-point number]] has 23 bits to store the fraction in. This is why B scaling is always more accurate than floating point of the same word size.\nThis is especially useful in [[integrator]]s or repeated summing of small quantities where [[rounding error]] can be a subtle but very dangerous problem when using floating point.\n\nNow a larger number 15.2 at B4.\n\n 15.2 @ B4 is 15.2 * (2 ^ (wordsize-4-1)) == 15.2 * 2 ^ 27 == 0x7999999A\n\nThe number of bits to store the fraction is 28 bits.\nMultiplying these 32 bit numbers give the 64 bit result {{mono|0x1547AE14A51EB852}}\n\nThis result is in B7 in a 64 bit word. Shifting it down by 32 bits gives the result in B7 in 32 bits.\n\n 0x1547AE14\n\nTo convert back to floating point, divide this by {{code|1=(2^(wordsize-7-1)) == 21.2800000099}}\n\nVarious scalings may be used. B0 for instance can be used to represent any number between -1 and 0.999999999.\n\n=={{anchor|BAM}}Binary angles==\n<!-- Binary angle (computing), Binary Angular Measurement System, and Binary radian redirect here -->\n[[Image:Binary angles.svg|360px|thumb|Binary scaling (B0) Representation of angles. <span style=\"color:black\">Black</span> is traditional degrees representation, <span style=\"color:green\">green</span> is floating point representation and <span style=\"color:red\">red</span> is [[hexadecimal]] 32-bit representation.]]\n\nBinary angles are mapped using B0, with 0 as 0 degrees, 0.5 as 90° (or <math>\\frac{\\pi}{2}</math>), &minus;1.0 or 0.9999999 as 180° (or π) and &minus;0.5 as 270° (or <math>\\frac{3\\pi}{2}</math>). When these binary angles are added using normal [[two's complement]] mathematics, the rotation of the angles is correct, even when crossing the sign boundary (this of course does away with checks for angle ≥ 360° when handling normal degrees<ref>[http://blogs.msdn.com/shawnhar/archive/2010/01/04/angles-integers-and-modulo-arithmetic.aspx Angles, integers, and modulo arithmetic] Shawn Hargreaves, ''blogs.msdn.com''</ref>).\n\nThe terms '''binary angular measurement''' ('''BAM''')<ref name=\"ship\">{{cite web |url=http://www.tpub.com/content/fc/14100/css/14100_314.htm |title=Binary angular measurement |archive-url=https://web.archive.org/web/20091221160257/http://www.tpub.com/content/fc/14100/css/14100_314.htm |archive-date=2009-12-21}}</ref> and '''binary angular measurement system''' ('''BAMS''')<ref>{{cite web |url=http://acronyms.thefreedictionary.com/Binary+Angular+Measurement+System |title=Binary Angular Measurement System |work=acronyms.thefreedictionary}}</ref> as well as '''brads''' ('''binary radians''' or '''binary degree''') refer to implementations of binary angles. They find use in robotics, navigation,<ref>[http://www.globalspec.com/reference/14722/160210/Chapter-7-5-3-Binary-Angular-Measure  Real-Time Systems Design and Analysis] Chapter 7.5.3, Binary Angular Measure , Phillip A. LaPlante, page via ''www.globalspec.com''</ref> computer games,<ref>[http://fabiensanglard.net/doomIphone/doomClassicRenderer.php Doom 1993 code review] Fabien Sanglard, section \"Walls\", 13/1/2010, ''fabiensanglard.net''</ref> and digital sensors.<ref>[http://www.hobbyengineering.com/specs/PX-29123.pdf Hitachi HM55B Compass Module (#29123)] {{webarchive |url=https://web.archive.org/web/20110711172521/http://www.hobbyengineering.com/specs/PX-29123.pdf |date=July 11, 2011 }} pdf via ''www.parallax.com'' via ''www.hobbyengineering.com''</ref>\n\nNo matter what bit-pattern is stored in a binary angle, when it is multiplied by 180° (or π) using standard signed [[fixed-point arithmetic]], the result is always a valid angle in the range of −180° [[degree (angle)|degree]]s (−π [[radian]]s) to +180° degrees (+π radians).\nIn some cases, it is convenient to use unsigned multiplication (rather than signed multiplication) on a binary angle, which gives the correct angle in the range of 0 to +360° degrees (+2π radians or +1 [[turn (geometry)|turn]]).\nCompared to storing angles in a binary angle format, storing angles in any other format inevitably results in some bit patterns giving \"angles\" outside that range, requiring extra steps to [[trigonometric functions#Computation|range-reduce]] the value to the desired range, or results in some bit patterns that are not valid angles at all ([[NaN]]), or both.\n\n==Application of binary scaling techniques==\nBinary scaling techniques were used in the 1970s and 1980s for real-time computing that was mathematically intensive, such as [[flight simulation]] and in [[Nuclear Power Plant]] control algorithms since the late 1960's. The code was often commented with the binary scalings of the intermediate results of equations.\n\nBinary scaling is still used in many [[digital signal processing|DSP]] applications and custom made microprocessors are usually based on binary scaling techniques.\n\nBinary scaling is currently used in the [[Discrete cosine transform|DCT]] used to compress [[JPEG]] images in utilities such as [[GIMP]].\n\nAlthough floating point has taken over to a large degree, where speed and extra accuracy are required, binary scaling works on simpler hardware and is more accurate when the range of values is known in advance.{{Clarify|date=March 2016|post-text=→[[Talk:Binary_scaling#Comparison_with_Floating_Point |talk]]}}\n\n==See also==\n{{Portal|Computer Science}}\n* [[Libfixmath]] – a library written in C for fixed-point math\n* [[Q (number format)]]\n* [[Minifloat]]\n* [[Block floating-point scaling]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Binary Scaling}}\n[[Category:Binary arithmetic|Scaling]]"
    },
    {
      "title": "Serial binary adder",
      "url": "https://en.wikipedia.org/wiki/Serial_binary_adder",
      "text": "The '''serial binary adder''' or '''bit-serial adder''' is a [[digital circuit]] that performs [[binary numeral system|binary]] [[binary adder|addition]] bit by bit.  The serial [[full adder]] has three single-bit inputs for the numbers to be added and the carry in.  There are two single-bit outputs for the sum and carry out.  The carry-in signal is the previously calculated carry-out signal.  The addition is performed by adding each bit, lowest to highest, one per clock cycle.\n\n==Serial binary addition==\n\nSerial binary addition is done by a [[Flip-flop (electronics)|flip-flop]] and a [[full adder]].  The flip-flop takes the carry-out signal on each clock cycle and provides its value as the carry-in signal on the next clock cycle.  After all of the bits of the input operands have arrived, all of the bits of the sum have come out of the sum output.\n\n==Serial binary subtracter==\n\nThe serial binary [[subtracter (electronics)|subtracter]] operates the same as the serial binary adder, except the subtracted number is converted to its [[two's complement]] before being added.  Alternatively, the number to be subtracted is converted to its [[ones' complement]], by inverting its bits, and the carry flip-flop is initialized to a 1 instead of to 0 as in addition.  The ones' complement plus the 1 is the two's complement.\n\n==Example of operation==\n\n;Decimal: 5+9=14\n:*X=5, Y=9, Sum=14\n;Binary: 0101+1001=1110\n\n;Addition of each step\n{|border=1\n!colspan=3 |Inputs\n!colspan=2 |Outputs\n|-\n!width=30|Cin\n!width=30|X\n!width=30|Y\n!width=30|Sum\n!width=30|Cout\n|-\n|0\n|1\n|1\n|0\n|1\n|-\n|1\n|0\n|0\n|1\n|0\n|-\n|0\n|1\n|0\n|1\n|0\n|-\n|0\n|0\n|1\n|1\n|0\n|}\n''*addition starts from lowest''\n;Result=1110 or 14\n\n==See also==\n* [[Parallel binary adder]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* http://www.quinapalus.com/wires8.html\n* http://www.asic-world.com/digital/arithmetic3.html\n\n==External links==\n* [http://teahlab.com/finite_state_machine_Serial_Adder/ Interactive Serial Adder], Provides the visual logic of the Serial Adder circuit built with Teahlab's Simulator.\n\n[[Category:Binary arithmetic]]\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Sign bit",
      "url": "https://en.wikipedia.org/wiki/Sign_bit",
      "text": "{{Refimprove|date=December 2009}}\nIn [[computer science]], the '''sign bit''' is a [[bit]] in a [[signed number representation]] that indicates the [[sign (mathematics)|sign]] of a number.  Although only [[signedness|signed]] numeric data types have a sign bit, it is invariably located in the [[most significant bit]] position, so the term may be used interchangeably with \"most significant bit\" in some contexts.\n\nAlmost always, if the sign bit is 0, the number is non-negative (positive or zero).  If the sign bit is 1 then the number is negative, although formats other than [[two's complement]] integers allow a [[signed zero]]: distinct \"positive zero\" and \"negative zero\" representations, the latter of which does not correspond to the mathematical concept of a [[negative number]].\n\nIn the two's complement representation, the sign bit has the [[positional notation|weight]] {{math|−2<sup>''w''−1</sup>}} where {{mvar|w}} is the number of bits.  In the ones' complement representation, the most negative value is {{math|1 − 2<sup>''w''−1</sup>}}, but there are [[signed zero|two representations of zero]], one for each value of the sign bit.  In a [[sign-and-magnitude]] representation of numbers, the value of the sign bit determines whether the numerical value is positive or negative.<ref name=Bryant/>{{Rp|52–54}}\n\n[[Floating point]] numbers, such as [[IEEE floating point|IEEE format]], [[IBM Floating Point Architecture|IBM format]], [[VAX]] format, and even the format used by the Zuse [[Z1 (computer)|Z1]] and [[Z3 (computer)|Z3]] use a sign-and-magnitude representation.\n\nWhen using a complement representation, to convert a signed number to a wider format the additional bits must be filled with copies of the sign bit in order to preserve its numerical value,<ref name=Bryant/>{{Rp|61–62}} a process called ''[[sign extension]]'' or ''sign propagation''.<ref>{{Cite web |url=http://www.adrc.com/glossary/s1.html |title=Data Dictionary (Glossary and Algorithms) |website=Adroit Data Recovery Centre Pte Ltd}}</ref>\n\n==References==\n{{reflist|refs=\n<ref name=Bryant>{{cite book |last1=Bryant |first1=Randal E. |last2=O'Hallaron |first2=David R. |title=Computer Systems: a Programmer's Perspective |year=2003 |publisher= Prentice Hall |location=Upper Saddle River, New Jersey |isbn=0-13-034074-X |chapter=Chapter 2: Representing and Manipulating Information}}</ref>\n}}\n\n{{DEFAULTSORT:Sign Bit}}\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n\n\n{{Comp-sci-stub}}"
    },
    {
      "title": "Sign extension",
      "url": "https://en.wikipedia.org/wiki/Sign_extension",
      "text": "'''Sign extension''' is the operation, in [[computer]] [[arithmetic]], of increasing the number of [[bit]]s of a [[binary number]] while preserving the number's [[sign (mathematics)|sign (positive/negative)]] and value. This is done by appending digits to the [[most significant bit|most significant]] side of the number, following a procedure dependent on the particular [[signed number representation]] used.\n\nFor example, if six bits are used to represent the number \"<code>00 1010</code>\" (decimal positive 10) and the sign extend operation increases the [[Word (data type)|word length]] to 16 bits, then the new representation is simply \"<code>0000 0000 0000 1010</code>\".  Thus, both the value and the fact that the value was positive are maintained.\n\nIf ten bits are used to represent the value \"<code>11 1111 0001</code>\" (decimal negative 15) using [[two's complement]], and this is sign extended to 16 bits, the new representation is \"<code>1111 1111 1111 0001</code>\".  Thus, by padding the left side with ones, the negative sign and the value of the original number are maintained.\n\nIn the [[Intel]] [[x86 instruction listings|x86 instruction set]], for example, there are two ways of doing sign extension:\n* using the instructions <tt>cbw</tt>, <tt>cwd</tt>, <tt>cwde</tt>, and <tt>cdq</tt>: convert byte to word, word to doubleword, word to extended doubleword, and doubleword to quadword, respectively (in the x86 context a byte has 8 bits, a word 16 bits, a doubleword and extended doubleword 32 bits, and a quadword 64 bits);\n* using one of the sign extended moves, accomplished by the <tt>movsx</tt> (\"move with sign extension\") family of instructions.\n\n==Zero extension==\nA similar concept is zero extension. In a move or convert operation, zero extension refers to setting the high bits of the destination to zero, rather than setting them to a copy of the most significant bit of the source. If the source of the operation is an unsigned number, then zero extension is usually the correct way to move it to a larger field while preserving its numeric value, while sign extension is correct for signed numbers. \n\nIn the x86 and x64 instruction sets, the <code>movzx</code> instruction (\"move with zero extension\") performs this function. For example, <code>movzx ebx, al</code> copies a byte from the <code>al</code> register to the low-order byte of <code>ebx</code> and then fills the remaining bytes of <code>ebx</code> with zeroes.   \n\nOn x64, most instructions that write to the lower 32 bits of any of the general-purpose registers will zero the upper half of the destination register. For example, the instruction <code>mov eax, 1234</code> will clear the upper 32 bits of the <code>rax</code> register.\n\n==References==\n* Mano, Morris M.; Kime, Charles R. (2004). ''Logic and Computer Design Fundamentals'' (3rd ed.), pp 453. Pearson Prentice Hall. {{ISBN|0-13-140539-X}}.\n\n[[Category:Binary arithmetic]]\n\n[[de:Zweierkomplement#Vorzeichenerweiterung]]\n[[ru:Дополнительный_код_(представление_числа)#Расширение_знака]]"
    },
    {
      "title": "Single-precision floating-point format",
      "url": "https://en.wikipedia.org/wiki/Single-precision_floating-point_format",
      "text": "'''Single-precision floating-point format ''' is a [[computer number format]], usually occupying [[32 bits]] in [[computer memory]]; it represents a wide [[dynamic range]] of numeric values by using a [[floating point|floating radix point]].\n\nA floating-point variable can represent a wider range of numbers than a [[fixed-point arithmetic|fixed-point]] variable of the same bit width at the cost of precision. A [[signedness|signed]] 32-bit [[integer]] variable has a maximum value of 2<sup>31</sup> − 1 = 2,147,483,647, whereas an IEEE 754 32-bit base-2 floating-point variable has a maximum value of (2 − 2<sup>−23</sup>) × 2<sup>127</sup> ≈ 3.4028235 × 10<sup>38</sup>. All integers with 6 or fewer [[significant figures|significant decimal digits]], and any number that can be written as 2<sup>n</sup> such that n is a whole number from -126 to 127, can be converted into an IEEE 754 floating-point value without loss of precision.\n\nIn the [[IEEE 754-2008]] [[standardization|standard]], the 32-bit base-2 format is officially referred to as '''binary32'''; it was called '''single''' in [[IEEE 754-1985]]. IEEE 754 specifies additional floating-point types, such as 64-bit base-2 ''[[double-precision floating-point format|double precision]]'' and, more recently, base-10 representations.\n\nOne of the first [[programming language]]s to provide single- and double-precision floating-point data types was [[Fortran]]. Before the widespread adoption of IEEE 754-1985, the representation and properties of floating-point data types depended on the [[computer manufacturer]] and computer model, and upon decisions made by programming-language designers. E.g., [[GW-BASIC]]'s single-precision data type was the [[32-bit MBF]] floating-point format.\n\nSingle precision is termed ''REAL'' in [[Fortran]],<ref>{{cite web|url=http://scc.ustc.edu.cn/zlsc/sugon/intel/compiler_f/main_for/lref_for/source_files/rfreals.htm|title=REAL Statement|website=scc.ustc.edu.cn}}</ref> ''SINGLE-FLOAT'' in [[Common Lisp]],<ref>{{cite web|url=http://www.lispworks.com/documentation/HyperSpec/Body/t_short_.htm|title=CLHS: Type SHORT-FLOAT, SINGLE-FLOAT, DOUBLE-FLOAT...}}</ref> ''float'' in [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]],<ref>{{cite web|url=http://java.sun.com/docs/books/tutorial/java/nutsandbolts/datatypes.html|title=Primitive Data Types|website=Java Documentation}}</ref> ''Float'' in [[Haskell (programming language)|Haskell]],<ref>{{cite web|url=https://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1350006.4|title=6 Predefined Types and Classes|date=20 July 2010|website=haskell.org}}</ref> and ''Single'' in [[Object Pascal]] ([[Delphi (programming language)|Delphi]]), [[Visual Basic]], and [[MATLAB]]. However, ''float'' in [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], [[PHP]], and [[OCaml]] and ''single'' in versions of [[GNU Octave|Octave]] before 3.2 refer to [[double-precision floating-point format|double-precision]] numbers. In most implementations of [[PostScript]], and some [[embedded systems]], the only supported precision is single.\n{{Floating-point}}\n\n==IEEE 754 single-precision binary floating-point format: binary32==\nThe IEEE 754 standard specifies a ''binary32'' as having:\n*[[Sign bit]]: 1 bit\n*[[Exponent]] width: 8 bits\n*[[Significand]] [[precision (arithmetic)|precision]]: 24 bits (23 explicitly stored)\n<!-- \"significand\", with a d at the end, is a technical term; please do not confuse with \"significant\" -->\n\nThis gives from 6 to 9 [[significant figures|significant decimal digits]] precision. If a decimal string with at most 6 significant digits is converted to IEEE 754 single-precision representation, and then converted back to a decimal string with the same number of digits, the final result should match the original string. If an IEEE 754 single-precision number is converted to a decimal string with at least 9 significant digits, and then converted back to single-precision representation, the final result must match the original number.<ref name=whyieee>{{cite web|url=http://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF|title=Lecture Notes on the Status of IEEE Standard 754 for Binary Floating-Point Arithmetic| author=William Kahan |date=1 October 1997|page=4}}</ref>\n\nSign bit determines the sign of the number, which is the sign of the significand as well. Exponent is either an 8-bit signed integer from −128 to 127 ([[2's complement]]) or an 8-bit unsigned integer from 0 to 255, which is the accepted biased form in IEEE 754 binary32 definition. If the unsigned integer format is used, the exponent value used in the arithmetic is the exponent shifted by a bias – for the IEEE 754 binary32 case, an exponent value of 127 represents the actual zero (i.e. for 2{{sup|e − 127}} to be one, e must be 127). Exponents range from −126 to +127 because exponents of −127 (all 0s) and +128 (all 1s) are reserved for special numbers.\n\nThe true significand includes 23 fraction bits to the right of the binary point and an ''implicit leading bit'' (to the left of the binary point) with value 1, unless the exponent is stored with all zeros. Thus only 23 fraction bits of the [[significand]] appear in the memory format, but the total precision is 24 bits (equivalent to log<sub>10</sub>(2<sup>24</sup>) ≈ 7.225 decimal digits). The bits are laid out as follows:\n\n[[Image:Float example.svg]]\n\nThe real value assumed by a given 32-bit ''binary32'' data with a given biased ''sign'', exponent ''e'' (the 8-bit unsigned integer), and a ''23-bit fraction'' is\n:<math>(-1)^{b_{31}} \\times 2^{(b_{30}b_{29} \\dots b_{23})_2 - 127} \\times (1.b_{22}b_{21} \\dots b_0)_2,</math>\nwhich yields\n:<math>\\text{value} = (-1)^\\text{sign} \\times 2^{(e-127)} \\times \\left(1 + \\sum_{i=1}^{23} b_{23-i} 2^{-i} \\right).</math>\n\nIn this example:\n*<math>\\text{sign} = b_{31} = 0</math>,\n*<math>(-1)^\\text{sign} = (-1)^{0} = +1 \\in \\{-1, +1\\}</math>,\n*<math>e = b_{30}b_{29} \\dots b_{23} = \\sum_{i=0}^{7} b_{23+i} 2^{+i} = 124 \\in \\{1, \\ldots, (2^8 - 1) - 1\\} = \\{1, \\ldots, 254 \\}</math>,\n*<math>2^{(e-127)} = 2^{124-127} = 2^{-3} \\in \\{2^{-126}, \\ldots, 2^{127}\\} </math>,\n*<math>1.b_{22}b_{21}...b_{0} = 1 + \\sum_{i=1}^{23} b_{23-i} 2^{-i} = 1 + 1\\cdot 2^{-2} = 1.25\n\\in \\{1, 1+2^{-23}, \\ldots, 2-2^{-23}\\}\n  \\subset [1; 2 - 2^{-23}]\n  \\subset [1; 2)</math>.\nthus:\n*<math>\\text{value} = (+1) \\times 1.25 \\times 2^{-3} = +0.15625</math>.\n\nNote:\n*<math>1+2^{-23} \\approx 1.000\\,000\\,119</math>,\n*<math>2-2^{-23} \\approx 1.999\\,999\\,881</math>,\n*<math>2^{-126}  \\approx 1.175\\,494\\,35 \\times 10^{-38}</math>,\n*<math>2^{+127}  \\approx 1.701\\,411\\,83 \\times 10^{+38}</math>.\n\n===Exponent encoding===\nThe single-precision binary floating-point exponent is encoded using an [[offset binary|offset-binary]] representation, with the zero offset being 127; also known as exponent bias in the IEEE 754 standard.\n\n*E<sub>min</sub> = 01<sub>H</sub>−7F<sub>H</sub> = −126\n*E<sub>max</sub> = FE<sub>H</sub>−7F<sub>H</sub> = 127\n*[[Exponent bias]] = 7F<sub>H</sub> = 127\n\nThus, in order to get the true exponent as defined by the offset-binary representation, the offset of 127 has to be subtracted from the stored exponent.\n\nThe stored exponents 00<sub>H</sub> and FF<sub>H</sub> are interpreted specially.\n\n{|class=\"wikitable\" style=\"text-align:center\"\n! Exponent !! Significand zero !! Significand non-zero !! Equation\n|-\n| 00<sub>H</sub> || [[0 (number)|zero]], [[−0]] || [[denormal numbers]] || (−1)<sup>signbit</sup>×2<sup>−126</sup>× 0.significandbits\n|-\n| 01<sub>H</sub>, ..., FE<sub>H</sub> ||colspan=2| normalized value || (−1)<sup>signbit</sup>×2<sup>exponentbits−127</sup>× 1.significandbits\n|-\n| FF<sub>H</sub> || ±[[infinity]] || [[NaN]] (quiet, signalling) ||\n|}\n\nThe minimum positive normal value is 2<sup>−126</sup> ≈ 1.18 × 10<sup>−38</sup> and the minimum positive (denormal) value is 2<sup>−149</sup> ≈ 1.4 × 10<sup>−45</sup>.\n\n===Converting from decimal representation to binary32 format===\nIn general, refer to the IEEE 754 standard itself for the strict conversion (including the rounding behaviour) of a real number into its equivalent binary32 format.\n\nHere we can show how to convert a base 10 real number into an IEEE 754 binary32 format using the following outline:\n*consider a real number with an integer and a fraction part such as 12.375\n*convert and [[normalized number|normalize]] the integer part into [[binary numeral system|binary]]\n*convert the fraction part using the following technique as shown here\n*add the two results and adjust them to produce a proper final conversion\n\n''Conversion of the fractional part:''\nconsider 0.375, the fractional part of 12.375. To convert it into a binary fraction, multiply the fraction by 2, take the integer part and re-multiply new fraction by 2 until a fraction of zero is found or until the precision limit is reached which is 23 fraction digits for IEEE 754 binary32 format.\n\n0.375 x 2 = 0.750 = 0 + 0.750 => b<sub>−1</sub> = 0, the integer part represents the binary fraction digit. Re-multiply 0.750 by 2 to proceed\n\n0.750 x 2 = 1.500 = 1 + 0.500 => b<sub>−2</sub> = 1\n\n0.500 x 2 = 1.000 = 1 + 0.000 => b<sub>−3</sub> = 1, fraction = 0.000, terminate\n\nWe see that (0.375)<sub>10</sub> can be exactly represented in binary as (0.011)<sub>2</sub>. Not all decimal fractions can be represented in a finite digit binary fraction. For example, decimal 0.1 cannot be represented in binary exactly. So it is only approximated.\n\nTherefore, (12.375)<sub>10</sub> = (12)<sub>10</sub> + (0.375)<sub>10</sub> = (1100)<sub>2</sub> + (0.011)<sub>2</sub> = (1100.011)<sub>2</sub>\n\nSince IEEE 754 binary32 format requires real values to be represented in <math>(1.x_1x_2...x_{23})_2 \\times 2^{e}</math> format (see [[Normalized number]], [[Denormalized number]]), 1100.011 is shifted to the right by 3 digits to become <math>(1.100011)_2 \\times 2^{3}</math>\n\nFinally we can see that: <math>(12.375)_{10} =(1.100011)_2 \\times 2^{3}</math>\n\nFrom which we deduce:\n*The exponent is 3 (and in the biased form it is therefore 130 = 1000 0010)\n*The fraction is 100011 (looking to the right of the binary point)\n\nFrom these we can form the resulting 32 bit IEEE 754 binary32 format representation of\n12.375 as: 0-10000010-10001100000000000000000 = 41460000<sub>H</sub>\n\nNote: consider converting 68.123 into IEEE 754 binary32 format: Using the above procedure you expect to get 42883EF9<sub>H</sub> with the last 4 bits being 1001. However, due to the default rounding behaviour of IEEE 754 format, what you get is 42883EFA<sub>H</sub>, whose last 4 bits are 1010.\n\n''Ex 1:''\nConsider decimal 1. We can see that: <math>(1)_{10} =(1.0)_2 \\times 2^{0}</math>\n\nFrom which we deduce:\n*The exponent is 0 (and in the biased form it is therefore 127 = 0111 1111 )\n*The fraction is 0 (looking to the right of the binary point in 1.0 is all 0 = 000...0)\n\nFrom these we can form the resulting 32 bit IEEE 754 binary32 format representation of real number 1 as: 0-01111111-00000000000000000000000 = 3f800000<sub>H</sub>\n\n''Ex 2:''\nConsider a value 0.25. We can see that: <math>(0.25)_{10} =(1.0)_2 \\times 2^{-2}</math>\n\nFrom which we deduce:\n*The exponent is −2 (and in the biased form it is 127+(−2)= 125 = 0111 1101 )\n*The fraction is 0 (looking to the right of binary point in 1.0 is all zeros)\n\nFrom these we can form the resulting 32 bit IEEE 754 binary32 format representation of real number 0.25 as: 0-01111101-00000000000000000000000 = 3e800000<sub>H</sub>\n\n''Ex 3:''\nConsider a value of 0.375. We saw that <math>0.375 = {(1.1)_2}\\times 2^{-2}</math>\n\nHence after determining a representation of 0.375 as <math>{(1.1)_2}\\times 2^{-2}</math> we can proceed as above:\n\n*The exponent is −2 (and in the biased form it is 127+(−2)= 125 = 0111 1101 )\n*The fraction is 1 (looking to the right of binary point in 1.1 is a single 1 = x<sub>1</sub>)\n\nFrom these we can form the resulting 32 bit IEEE 754 binary32 format representation of real number 0.375 as: 0-01111101-10000000000000000000000 = 3ec00000<sub>H</sub>\n\n===Single-precision examples===\nThese examples are given in bit ''representation'', in [[hexadecimal]] and [[Binary number|binary]], of the floating-point value. This includes the sign, (biased) exponent, and significand.\n\n 0 00000000 00000000000000000000001<sub>2</sub> = 0000 0001<sub>16</sub> = 2<sup>−126</sup> × 2<sup>−23</sup> = 2<sup>−149</sup> ≈ 1.4012984643 × 10<sup>−45</sup>\n                                                    (smallest positive subnormal number)\n\n 0 00000000 11111111111111111111111<sub>2</sub> = 007f ffff<sub>16</sub> = 2<sup>−126</sup> × (1 − 2<sup>−23</sup>) ≈ 1.1754942107 ×10<sup>−38</sup>\n                                                    (largest subnormal number)\n\n 0 00000001 00000000000000000000000<sub>2</sub> = 0080 0000<sub>16</sub> = 2<sup>−126</sup> ≈ 1.1754943508 × 10<sup>−38</sup>\n                                                    (smallest positive normal number)\n\n 0 11111110 11111111111111111111111<sub>2</sub> = 7f7f ffff<sub>16</sub> = 2<sup>127</sup> × (2 − 2<sup>−23</sup>) ≈ 3.4028234664 × 10<sup>38</sup>\n                                                    (largest normal number)\n\n 0 01111110 11111111111111111111111<sub>2</sub> = 3f7f ffff<sub>16</sub> = 1 − 2<sup>−24</sup> ≈ 0.9999999404\n                                                    (largest number less than one)\n\n 0 01111111 00000000000000000000000<sub>2</sub> = 3f80 0000<sub>16</sub> = 1 (one)\n\n 0 01111111 00000000000000000000001<sub>2</sub> = 3f80 0001<sub>16</sub> = 1 + 2<sup>−23</sup> ≈ 1.0000001192\n                                                    (smallest number larger than one)\n\n 1 10000000 00000000000000000000000<sub>2</sub> = c000 0000<sub>16</sub> = −2\n 0 00000000 00000000000000000000000<sub>2</sub> = 0000 0000<sub>16</sub> = 0\n 1 00000000 00000000000000000000000<sub>2</sub> = 8000 0000<sub>16</sub> = −0\n                                     \n 0 11111111 00000000000000000000000<sub>2</sub> = 7f80 0000<sub>16</sub> = infinity\n 1 11111111 00000000000000000000000<sub>2</sub> = ff80 0000<sub>16</sub> = −infinity\n                                     \n 0 10000000 10010010000111111011011<sub>2</sub> = 4049 0fdb<sub>16</sub> ≈ 3.14159274101 ≈ π ( pi )\n 0 01111101 01010101010101010101011<sub>2</sub> = 3eaa aaab<sub>16</sub> ≈ 0.333333343267 ≈ 1/3\n                                     \n x 11111111 10000000000000000000001<sub>2</sub> = ffc0 0001<sub>16</sub> = qNaN (on x86 and ARM processors)\n x 11111111 00000000000000000000001<sub>2</sub> = ff80 0001<sub>16</sub> = sNaN (on x86 and ARM processors)\n\nBy default, 1/3 rounds up, instead of down like [[double precision]], because of the even number of bits in the significand. The bits of 1/3 beyond the rounding point are <code>1010...</code> which is more than 1/2 of a [[unit in the last place]].\n\nEncodings of qNaN and sNaN are not specified in [[IEEE floating point|IEEE 754]] and implemented differently on different processors. The [[x86]] family and the [[ARM architecture|ARM]] family processors use the most significant bit of the significand field to indicate a quiet NaN. The [[PA-RISC]] processors use the bit to indicate a signalling NaN.\n\n===Converting from single-precision binary to decimal===\nWe start with the hexadecimal representation of the value, 41c80000, in this example, and convert it to binary:\n\n: 41c8 0000<sub>16</sub> = 0100 0001 1100 1000 0000 0000 0000 0000<sub>2</sub>\n\nthen we break it down into three parts: sign bit, exponent, and significand.\n\n* Sign bit: 0\n* Exponent: 1000 0011<sub>2</sub> = 83<sub>16</sub> = 131\n* Significand: 100 1000 0000 0000 0000 0000<sub>2</sub> = 480000<sub>16</sub>\n\nWe then add the implicit 24th bit to the significand:\n\n* Significand: '''1'''100 1000 0000 0000 0000 0000<sub>2</sub> = C80000<sub>16</sub>\n\nand decode the exponent value by subtracting 127:\n\n* Raw exponent: 83<sub>16</sub> = 131\n* Decoded exponent: 131 − 127 = ''4''\n\nEach of the 24 bits of the significand (including the implicit 24th bit), bit 23 to bit 0, represents a value, starting at 1 and halves for each bit, as follows:\n\n bit 23 = 1\n bit 22 = 0.5\n bit 21 = 0.25\n bit 20 = 0.125\n bit 19 = 0.0625\n bit 18 = 0.03125\n .\n .\n bit 0 = 0.00000011920928955078125\n\nThe significand in this example has three bits set: bit 23, bit 22, and bit 19. We can now decode the significand by adding the values represented by these bits.\n\n* Decoded significand: 1 + 0.5 + 0.0625 = 1.5625 = C80000/2<sup>23</sup>\n\nThen we need to multiply with the base, 2, to the power of the exponent, to get the final result:\n\n: 1.5625 × 2<sup>4</sup> = ''25''\n\nThus\n: 41c8 0000  = 25\n\nThis is equivalent to:\n: <math>n = (-1)^s \\times\n           (1+m*2^{-23})\\times\n           2^{x - 127}</math>\nwhere {{mvar|s}} is the sign bit, {{mvar|x}} is the exponent, and {{mvar|m}} is the significand.\n\n===Precision limits on integer values===\n\n* Integers in <math>[-16777216,16777216]</math> can be exactly represented\n* Integers in <math>[-33554432,-16777217]</math> or in <math>[16777217,33554432]</math> round to a multiple of 2\n* Integers in <math>[-2^{26},-2^{25}-1]</math> or in <math>[2^{25}+1,2^{26}]</math> round to a multiple of 4\n* ....\n* Integers in <math>[-2^{127},-2^{126}-1]</math> or in <math>[2^{126}+1,2^{127}]</math> round to a multiple of <math>2^{103}</math>\n* Integers in <math>[-2^{128}+2^{104},-2^{127}-1]</math> or in <math>[2^{127}+1,2^{128}-2^{104}]</math> round to a multiple of <math>2^{127-23}</math>\n* Integers larger than or equal to <math>2^{128}</math> or smaller than or equal to <math>-2^{128}</math> are rounded to \"infinity\".\n\n=== Optimizations ===\n\nThe design of floating-point format allows various optimisations, resulting from the easy generation of a [[base-2 logarithm]] approximation from an integer view of the raw bit pattern. Integer arithmetic and bit-shifting can yield an approximation to [[reciprocal square root]] ([[fast inverse square root]]), commonly required in [[computer graphics]].\n\n==See also==\n*[[IEEE 754-2008|IEEE Standard for Floating-Point Arithmetic (IEEE 754)]]\n*[[ISO/IEC 10967]], language independent arithmetic\n*[[Primitive data type]]\n*[[Numerical stability]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[https://evanw.github.io/float-toy/ Live floating-point bit pattern editor]\n*[http://www.h-schmidt.net/FloatConverter/IEEE754.html Online calculator]\n*[http://www.binaryconvert.com/convert_float.html Online converter for IEEE 754 numbers with single precision]\n*[https://web.archive.org/web/20091031135212/http://www.mathworks.com/matlabcentral/fileexchange/23173 C source code to convert between IEEE double, single, and half precision]\n\n{{data types}}\n\n[[Category:Binary arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Floating point types]]"
    },
    {
      "title": "Stibitz code",
      "url": "https://en.wikipedia.org/wiki/Stibitz_code",
      "text": "#redirect [[Excess-3]] {{R from alternative name}}\n\n[[Category:Binary arithmetic]]\n[[Category:Numeral systems]]"
    }
  ]
}