{
  "pages": [
    {
      "title": "Comparison of topologies",
      "url": "https://en.wikipedia.org/wiki/Comparison_of_topologies",
      "text": "In [[topology]] and related areas of [[mathematics]], the set of all possible topologies on a given set forms a [[partially ordered set]]. This [[order relation]] can be used for '''comparison of the topologies'''.\n\n== Definition ==\nA topology on a set may be defined as the collection of [[subset]]s which are considered to be \"open\". An alternative definition is that it is the collection of subsets which are considered \"closed\". These two ways of defining the topology are essentially equivalent because the [[Complement (set theory)|complement]] of an open set is closed and vice versa. In the following, it doesn't matter which definition is used.\n\nLet τ<sub>1</sub> and τ<sub>2</sub> be two topologies on a set ''X'' such that τ<sub>1</sub> is [[subset|contained in]] τ<sub>2</sub>:\n:<math>\\tau_1 \\subseteq \\tau_2</math>.\nThat is, every element of τ<sub>1</sub> is also an element of τ<sub>2</sub>. Then the topology τ<sub>1</sub> is said to be a '''coarser''' ('''weaker''' or '''smaller''') '''topology''' than τ<sub>2</sub>, and  τ<sub>2</sub> is said to be a '''finer''' ('''stronger''' or '''larger''') '''topology''' than τ<sub>1</sub>.\n<ref group=\"nb\">There are some authors, especially [[mathematical analysis|analyst]]s, who use the terms ''weak'' and ''strong'' with opposite meaning (Munkres, p. 78).</ref>\n\nIf additionally \n:<math>\\tau_1 \\neq \\tau_2</math>\nwe say τ<sub>1</sub> is '''strictly coarser''' than  τ<sub>2</sub> and τ<sub>2</sub> is '''strictly finer''' than τ<sub>1</sub>.<ref name=\"Munkres\"/>\n\nThe [[binary relation]] ⊆ defines a [[partial ordering relation]] on the set of all possible topologies on ''X''.\n\n== Examples ==\n\nThe finest topology on ''X'' is the [[discrete topology]]; this topology makes all subsets open.  The coarsest topology on ''X'' is the [[trivial topology]]; this topology only admits the empty set\nand the whole space as open sets.\n\nIn [[function space]]s and spaces of [[Measure (mathematics)|measures]] there are often a number of possible topologies. See [[topologies on the set of operators on a Hilbert space]] for some intricate relationships.\n\nAll possible [[polar topology|polar topologies]] on a [[dual pair]] are finer than the [[weak topology (polar topology)|weak topology]] and coarser than the [[strong topology (polar topology)|strong topology]].\n\n== Properties ==\n\nLet τ<sub>1</sub> and τ<sub>2</sub> be two topologies on a set ''X''. Then the following statements are equivalent:\n* τ<sub>1</sub> ⊆ τ<sub>2</sub>\n* the [[identity function|identity map]] id<sub>X</sub> : (''X'', τ<sub>2</sub>) → (''X'', τ<sub>1</sub>) is a [[continuous map (topology)|continuous map]].\n* the identity map id<sub>X</sub> : (''X'', τ<sub>1</sub>) → (''X'', τ<sub>2</sub>) is an [[open map]]\n\nTwo immediate corollaries of this statement are\n*A continuous map ''f'' : ''X'' → ''Y'' remains continuous if the topology on ''Y'' becomes ''coarser'' or the topology on ''X'' ''finer''.\n*An open (resp. closed) map ''f'' : ''X'' → ''Y'' remains open (resp. closed) if the topology on ''Y'' becomes ''finer'' or the topology on ''X'' ''coarser''.\n\nOne can also compare topologies using [[neighborhood base]]s. Let τ<sub>1</sub> and τ<sub>2</sub> be two topologies on a set ''X'' and let ''B''<sub>''i''</sub>(''x'') be a local base for the topology τ<sub>''i''</sub> at ''x'' ∈ ''X'' for ''i'' = 1,2. Then τ<sub>1</sub> ⊆ τ<sub>2</sub> if and only if for all ''x'' ∈ ''X'', each open set ''U''<sub>1</sub> in ''B''<sub>1</sub>(''x'') contains some open set ''U''<sub>2</sub> in ''B''<sub>2</sub>(''x''). Intuitively, this makes sense: a finer topology should have smaller neighborhoods.\n\n==Lattice of topologies==\n\nThe set of all topologies on a set ''X'' together with the partial ordering relation ⊆ forms a [[complete lattice]] that is also closed under arbitrary intersections.  That is, any collection of topologies on ''X'' have a ''meet'' (or [[infimum]]) and a ''join'' (or [[supremum]]). The meet of a collection of topologies is the [[intersection (set theory)|intersection]] of those topologies. The join, however, is not generally the [[union (set theory)|union]] of those topologies (the union of two topologies need not be a topology) but rather the topology [[subbase|generated by]] the union.\n\nEvery complete lattice is also a [[bounded lattice]], which is to say that it has a [[greatest element|greatest]] and [[least element]]. In the case of topologies, the greatest element is the [[discrete topology]] and the least element is the [[trivial topology]].\n\n== Notes ==\n{{reflist|group=nb}}\n\n== See also ==\n\n* [[Initial topology]], the coarsest topology on a set to make a family of mappings from that set continuous\n* [[Final topology]], the finest topology on a set to make a family of mappings into that set continuous\n\n==References==\n{{Reflist|refs=\n<ref name=\"Munkres\">\n{{cite book | last = Munkres | first = James R. | authorlink = James Munkres\n  | title = Topology | edition = 2nd\n  | publisher = [[Prentice Hall]] | location = Saddle River, NJ | year = 2000\n  | isbn = 0-13-181629-2\n  | pages = 77–78 }}\n</ref>\n}}\n\n[[Category:General topology]]\n[[Category:Comparison (mathematical)]]"
    },
    {
      "title": "Ordinal data",
      "url": "https://en.wikipedia.org/wiki/Ordinal_data",
      "text": "{{distinguish|Ordinal data (programming)}}\n{{redirect|Ordinal scale|the ''Sword Art Online'' movie|Sword Art Online The Movie: Ordinal Scale}}\n\n'''Ordinal data''' is a categorical, [[statistical data type]] where the variables have natural, ordered categories and the distances between the categories is not known.<ref name=\"agresti\">{{cite book|last1=Agresti|first1=Alan|title=Categorical Data Analysis|date=2013|publisher=John Wiley & Sons|location=Hoboken, New Jersey|isbn=978-0-470-46363-5|edition=3|pages=}}</ref>{{rp|2}} These data exist on an '''ordinal scale''', one of four [[Level of measurement|levels of measurement]] described by [[Stanley Smith Stevens|S. S. Stevens]] in 1946. The ordinal scale is distinguished from the nominal scale by having a [[ranking]]. It also differs from interval and ratio scales by not having category widths that represent equal increments of the underlying attribute.<ref name=\"stevens\">{{Cite journal|last=Stevens|first=S. S.|year=1946|title=On the Theory of Scales of Measurement|url=|journal=Science|series=New Series|volume=103|issue=2684|pages=677–680|via=}}</ref>\n\n==Examples of ordinal data==\n\nA well-known example of ordinal data is the [[Likert scale]]. An example of a Likert scale is:<ref name=\"cohenetal\">{{Cite book|title=Psychological Testing and Assessment: An Introduction to Tests and Measurement|last=Cohen|first=Ronald Jay|last2=Swerdik|first2=Mark E.|last3=Phillips|first3=Suzanne M.|publisher=Mayfield|year=1996|isbn=1-55934-427-X|edition=3rd|location=Mountain View, CA|pages=685|quote=|via=}}</ref>{{rp|685}}\n{| class=\"wikitable\" style=\"text-align: center;\"\n!Like\n!Like Somewhat\n!Neutral\n!Dislike Somewhat\n!Dislike\n|-\n|1\n|2\n|3\n|4\n|5\n|}Examples of ordinal data are often found in questionnaires: for example, the survey question \"Is your general health poor, reasonable, good, or excellent?\" may have those answers coded respectively as 1, 2, 3, and 4. Sometimes data on an [[interval scale]] or [[ratio scale]] are grouped onto an ordinal scale: for example, individuals whose income is known might be grouped into the income categories $0-$19,999, $20,000-$39,999, $40,000-$59,999, ..., which then might be coded as 1, 2, 3, 4, .... Other examples of ordinal data include socioeconomic status, military ranks, and letter grades for coursework.<ref name=\"s&c\">{{Cite book|title=Nonparametric Statistics for the Behavioral Sciences|last=Siegel|first=Sidney|last2=Castellan|first2=N. John Jr.|publisher=McGraw-Hill|year=1988|isbn=0-07-057357-3|edition=2nd|location=Boston|pages=25–26|quote=|via=}}</ref>\n\n==Ways to analyze ordinal data==\n\nOrdinal data analysis requires a different set of analyses than other qualitative variables. These methods incorporate the natural ordering of the variables in order to avoid loss of power.<ref name=\"agresti\" />{{rp|88}} Finding a mean or standard deviation for ordinal data is often discouraged, but other methods such as median or mode should instead be used.<ref>{{cite journal|last1=Jamieson|first1=Susan|title=Likert scales: how to (ab)use them|journal=Medical Education|date=December 2004|volume=38|issue=12|pages=1212–1218|doi=10.1111/j.1365-2929.2004.02012.x}}</ref>\n\n===General===\n\nStevens (1946) argued that, because the assumption of equal distance between categories does not hold for ordinal data, the use of means and standard deviations for description of ordinal distributions and of inferential statistics based on means and standard deviations was not appropriate. Instead, positional measures like the median and percentiles, in addition to descriptive statistics appropriate for nominal data (number of cases, mode, contingency correlation), should be used.<ref name=\"stevens\" />{{rp|678}} [[Nonparametric statistics|Nonparametric methods]] have been proposed as the most appropriate procedures for inferential statistics involving ordinal data, especially those developed for the analysis of ranked measurements.<ref name=\"s&c\" />{{rp|25–28}} However, use of parametric statistics for ordinal data may be permissible with certain caveats to take advantage of the greater range of available statistical procedures.<ref>{{Cite web|url=ftp://ftp.sas.com/pub/neural/measurement.html|title=Measurement theory: Frequently asked questions|last=Sarle|first=Warren S.|date=Sep 14, 1997|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref><ref>{{Cite book|title=Statistical Rules of Thumb|last=van Belle|first=Gerald|publisher=John Wiley & Sons|year=2002|isbn=0-471-40227-3|location=New York|pages=23–24|quote=|via=}}</ref><ref name=\"cohenetal\" />{{rp|90}}\n\n===Univariate statistics===\nIn place of means and standard deviations, univariate statistics appropriate for ordinal data include the median,<ref name=\"blalock\">{{Cite book|title=Social Statistics|last=Blalock|first=Hubert M. Jr.|publisher=McGraw-Hill|year=1979|isbn=0-07-005752-4|edition=Rev. 2nd|location=New York|pages=|quote=|via=}}</ref>{{rp|59–61}} other percentiles (such as quartiles and deciles),<ref name=\"blalock\" />{{rp|71}} and the quartile deviation.<ref name=\"blalock\" />{{rp|77}} One-sample tests for ordinal data include the [[Kolmogorov–Smirnov test|Kolmogorov-Smirnov one-sample test]],<ref name=\"s&c\" />{{rp|51–55}} the [[Wald–Wolfowitz runs test|one-sample runs test]],<ref name=\"s&c\" />{{rp|58–64}} and the change-point test.<ref name=\"s&c\" />{{rp|64–71}}\n\n===Bivariate statistics===\n\nIn lieu of testing differences in means with [[Student's t-test|''t''-tests]], differences in distributions of ordinal data from two independent samples can be tested with [[Mann–Whitney U test|Mann-Whitney]],<ref name=\"blalock\" />{{rp|259–264}} [[Wald–Wolfowitz runs test|runs]],<ref name=\"blalock\" />{{rp|253–259}} [[Kolmogorov–Smirnov test|Smirnov]],<ref name=\"blalock\" />{{rp|266–269}} and [[Wilcoxon signed-rank test|signed-ranks]]<ref name=\"blalock\" />{{rp|269–273}} tests. Test for two related or matched samples include the [[sign test]]<ref name=\"s&c\" />{{rp|80–87}} and the [[Wilcoxon signed-rank test|Wilcoxon signed ranks test]].<ref name=\"s&c\" />{{rp|87–95}} [[Kruskal–Wallis one-way analysis of variance|Analysis of variance with ranks]]<ref name=\"blalock\" />{{rp|367–369}} and the [[Jonckheere's trend test|Jonckheere test for ordered alternatives]]<ref name=\"s&c\" />{{rp|216–222}} can be conducted with ordinal data in place of independent samples [[Analysis of variance|ANOVA]]. Tests for more than two related samples include the [[Friedman test|Friedman two-way analysis of variance by ranks]]<ref name=\"s&c\" />{{rp|174–183}} and the [[Page's trend test|Page test for ordered alternatives]].<ref name=\"s&c\" />{{rp|184–188}} Correlation measures appropriate for two ordinal-scaled variables include [[Kendall rank correlation coefficient|Kendall's tau]],<ref name=\"blalock\" />{{rp|436–439}} [[Goodman and Kruskal's gamma|gamma]],<ref name=\"blalock\" />{{rp|442–443}} ''[[Spearman's rank correlation coefficient|r<sub>s</sub>]]'',<ref name=\"blalock\" />{{rp|434–436}} and ''[[Somers' D|d<sub>yx</sub>/d<sub>xy</sub>]]''.<ref name=\"blalock\" />{{rp|443}}\n\n===Regression applications===\nOrdinal data can be considered as a quantitative variable. In [[logistic regression]], the equation\n: <math>\nlogit[P(Y=1)] = \\alpha + \\beta_1 c + \\beta_2 x \n</math>\nis the model and c takes on the assigned levels of the categorical scale.<ref name=\"agresti\" />{{rp|189}} In [[regression analysis]], outcomes ([[dependent variable]]s) that are ordinal variables can be predicted using a variant of [[ordinal regression]], such as [[ordered logit]] or [[ordered probit]].\n\nIn multiple regression/correlation analysis, ordinal data can be accommodated using power polynomials and through normalization of scores and ranks.<ref>{{Cite book|title=Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences|last=Cohen|first=Jacob|last2=Cohen|first2=Patricia|publisher=Lawrence Erlbaum Associates|year=1983|isbn=0-89859-268-2|edition=2nd|location=Hillsdale, New Jersey|pages=273}}</ref>\n\n===Linear trends===\nLinear trends are also used to find associations between ordinal data and other categorical variables, normally in a [[contingency table]]s. A correlation ''r'' is found between the variables where ''r'' lies between -1 and 1. To test the trend, a test statistic:\n: <math>\nM^2 = (n-1)r^2\n</math>\nis used where ''n'' is the sample size.<ref name=\"agresti\" />{{rp|87}}\n\n''R'' can be found by letting <math> u_1 \\leq u_2 \\leq ... \\leq u_I </math> be the row scores and <math> v_1 \\leq v_2 \\leq ... \\leq v_I </math> be the column scores. Let <math> \\bar u \\ = \\sum_i u_i p_{i+} </math> be the mean of the row scores while <math> \\bar v \\ = \\sum_j v_j p_{j+}. </math>. Then <math> p_{i+} </math> is the marginal row probability and <math> p_{+j} </math> is the marginal column probability. ''R'' is calculated by:\n: <math>\nr = \\frac{ \\sum_{i,j} \\left (u_i - \\bar u\\ \\right ) \\left (v_j - \\bar v\\ \\right )p_{ij}} {\\sqrt{ \\left \\lbrack \\sum_i ( u_i - \\bar u\\ \\right )^2p_{i+} \\rbrack  \\lbrack \\sum_j ( v_j - \\bar v\\ )^2p_{+j} \\rbrack }}\n</math>\n\n===Classification methods===\nClassification methods have also been developed for ordinal data. The data are divided into different categories such that each observations are similar to each other. Dispersion is measured and minimized in each group to maximize classification results. The dispersion function is used in [[information theory]].<ref>{{cite journal|last1=Laird|first1=Nan M.|title=A Note on Classifying Ordinal-Scale Data|journal=Sociological Methodology|date=1979|volume=10|pages=303–310|doi=10.2307/270775}}</ref>\n\n\n==Statistical models for ordinal data==\nThere are several different models that can be used to describe the structure of ordinal data<ref>{{cite book |last=Agresti |first=Alan |chapter= |title=Analysis of Ordinal Categorical Data |location=Hoboken, New Jersey |publisher=Wiley |edition=2nd |year=2010 |isbn=978-0470082898 |pages= |chapterurl= }}</ref>. Four major classes of model are described below, each defined for a random variable <math>Y</math>, with levels indexed by <math>k = 1, 2, \\dots, q</math>. \n\nNote that in the model definitions below, the values of <math>\\mu_k</math> and <math>\\mathbf{\\beta}</math> will not be the same for all the models for the same set of data, but the notation is used to compare the structure of the different models.\n\n===Proportional odds model===\nThe most commonly-used model for ordinal data is the proportional odds model, defined by\n<math>\n\\log\\left[\\frac{\\Pr(Y \\leq k)}{Pr(Y > k)}\\right] = \\log\\left[\\frac{\\Pr(Y \\leq k)}{1-\\Pr(Y \\leq k)}\\right] = \\mu_k + \\mathbf{\\beta}^T\\mathbf{x}\n</math>\nwhere the parameters <math>\\mu_k</math> describe the base distribution of the ordinal data, <math>\\mathbf{x}</math> are the covariates and <math>\\mathbf{\\beta}</math> are the coefficients describing the effects of the covariates.\n\nThis model can be generalized by defining the model using <math>\\mu_k + \\mathbf{\\beta}_k^T\\mathbf{x}</math> instead of <math>\\mu_k + \\mathbf{\\beta}^T\\mathbf{x}</math>, and this would make the model suitable for nominal data (in which the categories have no natural ordering) as well as ordinal data. However, this generalization can make it much more difficult to fit the model to the data.\n\n===Baseline category logit model===\nThe baseline category model is defined by\n<math>\n\\log\\left[\\frac{\\Pr(Y = k)}{\\Pr(Y = 1)}\\right] = \\mu_k + \\mathbf{\\beta}_k^T\\mathbf{x}\n</math>\n\nThis model does not impose an ordering on the categories and so can be applied to nominal data as well as ordinal data.\n\n===Ordered stereotype model===\nThe ordered stereotype model is defined by\n<math>\n\\log\\left[\\frac{\\Pr(Y = k)}{\\Pr(Y = 1)}\\right] = \\mu_k + \\phi_k\\mathbf{\\beta}^T\\mathbf{x}\n</math>\nwhere the score parameters are constrained such that <math>0=\\phi_1 \\leq \\phi_2 \\leq \\dots \\leq \\phi_q=1</math>.\n\nThis is a more parsimonious, and more specialised, model than the baseline category logit model: <math>\\phi_k\\mathbf{\\beta}</math> can be thought of as similar to <math>\\mathbf{\\beta}_k</math>.\n\nThe non-ordered stereotype model has the same form as the ordered stereotype model, but without the ordering imposed on <math>\\phi_k</math>. This model can be applied to nominal data.\n\nNote that the fitted scores, <math>\\hat{\\phi}_k</math>, indicate how easy it is to distinguish between the different levels of <math>Y</math>. If <math>\\hat{\\phi}_k \\approx \\hat{\\phi}_{k-1}</math> then that indicates that the current set of data for the covariates <math>\\mathbf{x}</math> do not provide much information to distinguish between levels <math>k</math> and <math>k-1</math>, but that does '''not''' necessarily imply that the actual values <math>k</math> and <math>k-1</math> are far apart. And if the values of the covariates change, then for that new data the fitted scores <math>\\hat{\\phi}_k</math> and <math>\\hat{\\phi}_{k-1}</math> might then be far apart.\n\n===Adjacent categories logit model===\nThe adjacent categories model is defined by\n<math>\n\\log\\left[\\frac{\\Pr(Y = k)}{\\Pr(Y = k+1)}\\right] = \\mu_k + \\mathbf{\\beta}_k^T\\mathbf{x}\n</math>\nalthough the most common form, referred to in Agresti (2010)<ref>{{cite book |last=Agresti |first=Alan |chapter= |title=Analysis of Ordinal Categorical Data |location=Hoboken, New Jersey |publisher=Wiley |edition=2nd |year=2010 |isbn=978-0470082898 |pages= |chapterurl= }}</ref> as the \"proportional odds form\" is defined by\n<math>\n\\log\\left[\\frac{\\Pr(Y = k)}{\\Pr(Y = k+1)}\\right] = \\mu_k + \\mathbf{\\beta}^T\\mathbf{x}\n</math>\n\nThis model can only be applied to ordinal data, since modelling the probabilities of shifts from one category to the next category implies that an ordering of those categories exists.\n\nThe adjacent categories logit model can be thought of as a special case of the baseline category logit model, where <math>\\mathbf{\\beta}_k = \\mathbf{\\beta}(k-1)</math>. The adjacent categories logit model can also be thought of as a special case of the ordered stereotype model, where <math>\\phi_k \\propto k-1</math>, i.e. the distances between the <math>\\phi_k</math> are defined in advance, rather than being estimated based on the data.\n\n===Comparisons between the models===\nThe proportional odds model has a very different structure to the other three models, and also a different underlying meaning. Note that the size of the reference category in the proportional odds model varies with <math>k</math>, since <math>Y \\leq k</math> is compared to <math>Y > k</math>, whereas in the other models the size of the reference category remains fixed, as <math>Y=k</math> is compared to <math>Y=1</math> or <math>Y=k+1</math>.\n\n===Different link functions===\nThere are variants of all the models that use different link functions, such as the probit link or the complementary log-log link.\n\n==Visualization and display==\nOrdinal data can be visualized in several different ways. Common visualizations are the [[bar chart]] or a [[pie chart]]. [[Table (information)|Tables]] can also be useful for displaying ordinal data and frequencies. [[Mosaic plot]]s can be used to show the relationship between an ordinal variable and a nominal or ordinal variable.<ref>{{Cite web|url=http://www-stat.wharton.upenn.edu/~buja/mba/plotting-techniques.html|title=Plotting Techniques|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref> A bump chart—a line chart that shows the relative ranking of items from one time point to the next—is also appropriate for ordinal data.<ref>{{Cite book|title=Good Charts: The HBR Guide to Making Smarter, More Persuasive Data Visualizations|last=Berinato|first=Scott|publisher=Harvard Business Review Press|year=2016|isbn=978-1633690707|location=Boston|pages=228}}</ref>\n\nColor or [[grayscale]] gradation can be used to represent the ordered nature of the data. A single-direction scale, such as income ranges, can be represented with a bar chart where increasing (or decreasing) saturation or lightness of a single color indicates higher (or lower) income. The ordinal distribution of a variable measured on a dual-direction scale, such as a Likert scale, could also be illustrated with color in a stacked bar chart. A neutral color (white or gray) might be used for the middle (zero or neutral) point with contrasting colors used in the opposing directions from the midpoint, where increasing saturation or darkness of the colors could indicate categories at increasing distance from the midpoint.<ref>{{Cite book|title=Data Visualisation: A Handbook for Data Driven Design|last=Kirk|first=Andy|publisher=SAGE|year=2016|isbn=978-1473912144|edition=1st|location=London|pages=269}}</ref> [[Choropleth map]]s also use color or grayscale shading to display ordinal data.<ref>{{Cite book|title=The Truthful Art: Data, Charts, and Maps for Communication|last=Cairo|first=Alberto|publisher=New Riders|year=2016|isbn=978-0321934079|edition=1st|location=San Francisco|pages=280}}</ref>\n\n{|style=\"margin: 0 auto;\"\n| [[File:Bar plot defense spending example (self-made).jpg|thumb|Example bar plot of opinion on defense spending.]]\n| [[File:Bump plot defense spending.jpg|thumb|Example bump plot of opinion on defense spending by political party.]]\n| [[File:Mosaic plot defense spending example.jpg|thumb|Example mosaic plot of opinion on defense spending by political party.]]\n| [[File:Stacked bar plot defense spending.jpg|thumb|Example stacked bar plot of opinion on defense spending by political party.]]\n|}\n\n==Applications==\n\nThe use of ordinal data can be found in most areas of research where categorical data are generated. Settings where ordinal data are often collected include the social and behavioral sciences and governmental and business settings where measurements are collected from persons by observation, testing, or [[questionnaire]]s. Some common contexts for the collection of ordinal data include [[Survey (human research)|survey research]];<ref>{{Cite book|title=Assessing the Reliability and Validity of Survey Measures|last=Alwin|first=Duane F.|work=Handbook of Survey Research|publisher=Emerald House|year=2010|isbn=978-1-84855-224-1|editor-last=Marsden|editor-first=Peter V.|location=Howard House, Wagon Lane, Bingley BD16 1WA, UK|pages=420|quote=|editor-last2=Wright|editor-first2=James D.|via=}}</ref><ref>{{Cite book|title=Improving Survey Questions: Design and Evaluation|last=Fowler|first=Floyd J. Jr.|publisher=SAGE|year=1995|isbn=0-8039-4583-3|location=Thousand Oaks, CA|pages=156–165|quote=|via=}}</ref> and [[Intelligence quotient|intelligence]], [[Test (assessment)|aptitude]], and [[Personality test|personality]] testing.<ref name=\"cohenetal\" />{{rp|89–90}}\n\n==See also==\n{{Portal|Statistics}}\n* [[List of analyses of categorical data]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book |last=Agresti |first=Alan |chapter= |title=Analysis of Ordinal Categorical Data |location=Hoboken, New Jersey |publisher=Wiley |edition=2nd |year=2010 |isbn=978-0470082898 |pages= |chapterurl= }}\n\n[[Category:Statistical data types]]\n[[Category:Comparison (mathematical)]]"
    },
    {
      "title": "Ranking",
      "url": "https://en.wikipedia.org/wiki/Ranking",
      "text": "{{short description|Relationship between items in a set}}\n{{distinguish|text=[[standings]], which are listings of entities that employ ranking}}\n{{Redirects|World ranking|various rankings among countries and regions|List of international rankings}}\n{{other uses|Rank (disambiguation)}}\n{{Multiple issues|\n{{Refimprove|article|date=November 2008}}\n{{Cleanup|date=January 2010}}\n{{Original research|date=July 2008}}\n}}\n\nA '''ranking''' is a relationship between a set of items such that, for any two items, the first is either 'ranked higher than', 'ranked lower than' or 'ranked equal to' the second.<ref>http://www.merriam-webster.com/dictionary/ranking</ref>\nIn [[order theory|mathematics]], this is known as a [[Strict weak ordering#Total preorders|weak order or total preorder]] of objects. It is not necessarily a [[total order]] of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by [[Hardness (materials science)|hardness]], while degrees of hardness are totally ordered. If two items are the same in rank it is considered a tie.\n\nBy reducing detailed measures to a sequence of [[ordinal numbers]], rankings make it possible to evaluate complex information according to certain criteria.<ref>{{Cite journal|last=Malara|first=Zbigniew|last2=Miśko|first2=Rafał|last3=Sulich|first3=Adam|year=|title=Wroclaw University of Technology graduates' career paths|url=https://www.academia.edu/31334733|journal=|language=en|volume=|pages=|via=}}</ref> Thus, for example, an Internet search engine may rank the pages it finds according to an estimation of their [[relevance]], making it possible for the user quickly to select the pages they are likely to want to see.\n\nAnalysis of data obtained by ranking commonly requires [[non-parametric statistics]].\n\n== Strategies for assigning rankings ==\nIt is not always possible to assign rankings uniquely. For example, in a race or competition two (or more) entrants might tie for a place in the ranking.<ref>{{Cite web|url=https://www.academia.edu/25508981|title=The young people's labour market and crisis of integration in European Union|last=Sulich|first=Adam|date=|website= |archiveurl=https://www.academia.edu/25508981/The_young_people_s_labour_market_and_crisis_of_integration_in_European_Union |archivedate=2017-03-04 |deadurl=no |accessdate=2017-03-04}}</ref> When computing an [[ordinal measurement]], two (or more) of the quantities being ranked might measure equal. In these cases, one of the strategies shown below for assigning the rankings may be adopted. \nA common shorthand way to distinguish these ranking strategies is by the ranking numbers that would be produced for four items, with the first item ranked ahead of the second and third (which compare equal) which are both ranked ahead of the fourth. These names are also shown below.\n\n=== Standard competition ranking (\"1224\" ranking) ===\nIn competition ranking, items that compare equal receive the same ranking number, and then a gap is left in the ranking numbers. The number of ranking numbers that are left out in this gap is one less than the number of items that compared equal. Equivalently, each item's ranking number is 1 plus the number of items ranked above it. This ranking strategy is frequently adopted for competitions, as it means that if two (or more) competitors tie for a position in the ranking, the position of all those ranked below them is unaffected (i.e., a competitor only comes second if exactly one person scores better than them, third if exactly two people score better than them, fourth if exactly three people score better than them, etc.).\n\nThus if A ranks ahead of B and C (which compare equal) which are both ranked ahead of D, then A gets ranking number 1 (\"first\"), B gets ranking number 2 (\"joint second\"), C also gets ranking number 2 (\"joint second\") and D gets ranking number 4 (\"fourth\").\n\n=== Modified competition ranking (\"1334\" ranking) ===\nSometimes, competition ranking is done by leaving the gaps in the ranking numbers ''before'' the sets of equal-ranking items (rather than after them as in standard competition ranking).{{where|date=July 2016}} The number of ranking numbers that are left out in this gap remains one less than the number of items that compared equal. Equivalently, each item's ranking number is equal to the number of items ranked equal to it or above it. This ranking ensures that a competitor only comes second if they score higher than all but one of their opponents, third if they score higher than all but two of their opponents, etc.\n\nThus if A ranks ahead of B and C (which compare equal) which are both ranked head of D, then A gets ranking number 1 (\"first\"), B gets ranking number 3 (\"joint third\"), C also gets ranking number 3 (\"joint third\") and D gets ranking number 4 (\"fourth\"). In this case, nobody would get ranking number 2 (\"second\") and that would be left as a gap.\n\n=== Dense ranking (\"1223\" ranking) ===\nIn dense ranking, items that compare equally receive the same ranking number, and the next item(s) receive the immediately following ranking number. Equivalently, each item's ranking number is 1 plus the number of items ranked above it that are distinct with respect to the ranking order.\n\nThus if A ranks ahead of B and C (which compare equal) which are both ranked ahead of D, then A gets ranking number 1 (\"first\"), B gets ranking number 2 (\"joint second\"), C also gets ranking number 2 (\"joint second\") and D gets ranking number 3 (\"Third\").\n\n=== Ordinal ranking (\"1234\" ranking) ===\nIn ordinal ranking, all items receive distinct ordinal numbers, including items that compare equal. The assignment of distinct ordinal numbers to items that compare equal can be done at random, or arbitrarily, but it is generally preferable to use a system that is arbitrary but consistent, as this gives stable results if the ranking is done multiple times. An example of an arbitrary but consistent system would be to incorporate other attributes into the ranking order (such as alphabetical ordering of the competitor's name) to ensure that no two items exactly match.\n\nWith this strategy, if A ranks ahead of B and C (which compare equal) which are both ranked ahead of D, then A gets ranking number 1 (\"first\") and D gets ranking number 4 (\"fourth\"), and '''either''' B gets ranking number 2 (\"second\") and C gets ranking number 3 (\"third\") '''or''' C gets ranking number 2 (\"second\") and B gets ranking number 3 (\"third\").\n\nIn computer data processing, ordinal ranking is also referred to as \"row numbering\".\n\n=== Fractional ranking (\"1 2.5 2.5 4\" ranking) ===\nItems that compare equal receive the same ranking number, which is the [[mean]] of what they would have under ordinal rankings. Equivalently, the ranking number of 1 plus the number of items ranked above it plus half the number of items equal to it. This strategy has the property that the sum of the ranking numbers is the same as under ordinal ranking. For this reason, it is used in computing [[Borda count]]s and in statistical tests (see below).\n\nThus if A ranks ahead of B and C (which compare equal) which are both ranked ahead of D, then A gets ranking number 1 (\"first\"), B and C each get ranking number 2.5 (average of \"joint second/third\") and D gets ranking number 4 (\"fourth\").\n\nHere is an example: \nSuppose you have the data set 1.0, 1.0, 2.0, 3.0, 3.0, 4.0, 5.0, 5.0, 5.0.\n\nThe ordinal ranks are 1, 2, 3, 4, 5, 6, 7, 8, 9.\n\nFor v = 1.0, the fractional rank is the average of the ordinal ranks: (1 + 2) / 2 = 1.5.\nIn a similar manner, for v = 5.0, the fractional rank is (7 + 8 + 9) / 3 = 8.0.\n\nThus the fractional ranks are: 1.5, 1.5, 3.0, 4.5, 4.5, 6.0, 8.0, 8.0, 8.0\n\n== Ranking in statistics ==\nIn [[statistics]], \"ranking\" refers to the [[data transformation (statistics)|data transformation]] in which [[number|numerical]] or [[Ordinal scale|ordinal]] values are replaced by their rank when the data are sorted. For example, the numerical data 3.4, 5.1, 2.6, 7.3 are observed, the ranks of these data items would be 2, 3, 1 and 4 respectively. For example, the ordinal data hot, cold, warm would be replaced by 3, 1, 2. In these examples, the ranks are assigned to values in ascending order. (In some other cases, descending ranks are used.) Ranks are related to the indexed list of [[order statistics]], which consists of the original dataset rearranged into ascending order.\n\nSome kinds of [[statistical test]]s employ calculations based on ranks. Examples include:\n\n* [[Friedman test]]\n* [[Kruskal–Wallis one-way analysis of variance|Kruskal–Wallis test]]\n* [[Rank product]]s\n* [[Spearman's rank correlation coefficient]]\n* [[Wilcoxon rank-sum test]]\n* [[Wilcoxon signed-rank test]]\n\nThe distribution of values in decreasing order of rank is often of interest when values vary widely in scale; this is the [[rank-size distribution]] (or rank-frequency distribution), for example for city sizes or word frequencies. These often follow a [[power law]].\n\nSome ranks can have non-integer values for tied data values. For example, when there is an even number of copies of the same data value, the above described [[#Fractional ranking (\"1 2.5 2.5 4\" ranking)|fractional statistical rank]] of the tied data ends in ½.\n\n=== Rank function in Excel ===\n[[Microsoft Excel]] provides two ranking functions, the '''Rank.EQ''' function which assigns competition ranks (\"1224\") and the '''Rank.AVG''' function which assigns fractional ranks (\"1 2.5 2.5 4\") as described above.\n\n=== Comparison of rankings ===\nA [[rank correlation]] can be used to compare two rankings for the same set of objects. For example, [[Spearman's rank correlation coefficient]] is useful to measure the statistical dependence between the rankings of athletes in two tournaments. Another example is the \"Rank–rank hypergeometric overlap\" approach<ref>{{cite journal|last1=Plaisier|first1=Seema B.|last2=Taschereau|first2=Richard|last3=Wong|first3=Justin A.|last4=Graeber|first4=Thomas G.|title=Rank–rank hypergeometric overlap: identification of statistically significant overlap between gene-expression signatures|journal=Nucleic Acids Research|date=September 2010|volume=38|issue=17|pages=e169|doi=10.1093/nar/gkq636|pmid=20660011|pmc=2943622}}</ref>, which is designed to compare ranking of the genes that are at the \"top\" of two ordered lists of differentially expressed genes.\n\n== Examples of ranking ==\n{{Unreferenced section|date=September 2011}}\n* In [[politics]], rankings focus on the comparison of economic, social, environmental and governance performance of countries, see [[List of international rankings]].\n* In many [[sport]]s, individuals or teams are given rankings, generally by the sport's [[Sports governing body|governing body]].\n** In [[association football]] (soccer), national teams are ranked in the [[FIFA World Rankings]], the [[FIFA Women's World Rankings|Women's World Rankings]] and, unofficially, in the [[World Football Elo Ratings]].\n** In the [[Olympic Games]], each member country ([[National Olympic Committee|NOC]]) is ranked based upon gold, silver and bronze medal counts in the [[Olympic medal table|Olympic medal rankings]].\n** In [[basketball]], national teams are ranked in the [[FIBA World Rankings]] and the [[FIBA Women's World Ranking|Women's World Rankings]].\n** In [[baseball]] and [[softball]], national teams are ranked in the [[WBSC World Rankings]].\n** In [[ice hockey]], national teams are ranked in the [[IIHF World Ranking]].\n** In [[golf]], the top male golfers are ranked using the [[Official World Golf Rankings]], and the top female golfers are ranked using the [[Women's World Golf Rankings]].\n** In [[snooker]], players are ranked using the [[Snooker world rankings]].\n** In [[tennis]], male and female players are ranked using the [[ATP Rankings]] and [[WTA Rankings]] respectively, whilst the [[ITF Rankings]] are used for national [[Davis Cup]] and [[Fed Cup]] teams.\n** In [[road bicycle racing]], male cyclists have been ranked using the [[UCI World Ranking]] from 2016, having previously been ranked using the [[UCI Road World Rankings]] from 1984 to 2004. Female cyclists have been ranked using the [[UCI Women's Road World Rankings]] since 1994.\n** In [[track cycling]] riders and nations are ranked using the [[UCI Track Cycling World Ranking]]\n** In [[chess]], players are ranked using the [[FIDE World Rankings]].\n** In [[sailing]], boats are scored directly using the sum of the ranking.\n** In [[Bridge (game)|bridge]], matchpoint scoring uses fractional ranking to assign the score.\n* In relation to [[Credit (finance)|credit]] standing, the ranking of a security refers to where that particular security would stand in a [[Liquidation|wind up]] of the issuing company, i.e., its [[seniority (finance)|seniority]] in the company's [[capital structure]]. For instance, [[capital note]]s are subordinated securities; they would rank behind senior debt in a wind up. In other words, the holders of [[senior debt]] would be paid out before [[subordinated debt]] holders received any funds.\n* [[Search engine]]s rank web pages by their expected [[relevance]] to a user's query using a combination of query-dependent and query-independent methods. Query-independent methods attempt to measure the estimated importance of a page, independent of any consideration of how well it matches the specific query. Query-independent ranking is usually based on link analysis; examples include the [[HITS algorithm]], [[PageRank]] and [[TrustRank]]. Query-dependent methods attempt to measure the degree to which a page matches a specific query, independent of the importance of the page. Query-dependent ranking is usually based on [[heuristic]]s that consider the number and locations of matches of the various query words on the page itself, in the [[URL]] or in any [[anchor text]] referring to the page.\n* In [[Webometrics]] it is possible to rank institutions according to their presence in the web (number of webpages) and the impact of these contents (external inlinks=site citations), such as the [[Webometrics Ranking of World Universities]]\n* In [[video gaming]], players may be given a ranking. To \"[[rank up]]\" is to achieve a higher ranking relative to other players, especially with strategies that do not depend on the player's skill.\n* The [[TrueSkill]] ranking system is a skill based ranking system for Xbox Live developed at Microsoft Research\n* A [[bibliogram]] ranks common noun phrases in a piece of text.\n* In language, the status of an item (usually through what is known as \"downranking\" or \"rank-shifting\") in relation to the uppermost rank in a clause; for example, in the sentence \"I want to eat the cake you made today\", \"eat\" is on the uppermost rank, but \"made\" is downranked as part of the nominal group \"the cake you made today\"; this nominal group behaves as though it were a single noun (i.e., I want to eat ''it''), and thus the verb within it (\"made\") is ranked differently from \"eat\".\n* [[Academic journal#Ranking|Academic journal]]s are sometimes ranked according to [[impact factor]]; the number of later articles that cite articles in a given journal.\n\n==See also==\n* [[League table]]\n* [[Ordinal data]]\n* [[Rating (disambiguation)]]\n\n== References ==\n{{reflist}}\n\n==External links==\n{{Wiktionary|ranking}}\n*[https://uk.mathworks.com/matlabcentral/fileexchange/70301-ranknum RANKNUM, a Matlab function to compute the 5 type of ranks]\n*[http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=19496 Matlab Toolbox with functions to compute ranks]\n* [http://research.microsoft.com/en-us/projects/trueskill/default.aspx TrueSkill Ranking System]\n* [https://github.com/quidproquo/ranker Ranking Library written in Ruby]\n* [http://www.dataworldwide.org/websites/data_indexes.htm List of Global Development Indexes and Rankings]\n\n{{statistics|inference|collapsed}}\n\n[[Category:Nonparametric statistics]]\n[[Category:Rankings| ]]<!-- most of these are not 'mathematical' -->\n[[Category:Comparison (mathematical)]]"
    },
    {
      "title": ".EQ.",
      "url": "https://en.wikipedia.org/wiki/.EQ.",
      "text": "#REDIRECT [[Relational operator]]\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "=",
      "url": "https://en.wikipedia.org/wiki/%3D",
      "text": "#REDIRECT [[Equals sign]]\n\n{{Rcat shell|\n{{R from Unicode}}\n{{R symbol}}\n{{R from mathematical symbol or equation}}\n}}\n\n[[Category:Definition]]<!-- let a=1.23 -->\n[[Category:Assignment operations]]<!-- a=3.21 -->\n[[Category:Equivalence (mathematics)]]<!-- if a=1.23 then -->"
    },
    {
      "title": "==",
      "url": "https://en.wikipedia.org/wiki/%3D%3D",
      "text": "#Redirect [[Relational operator#Equality]]\n\n{{Redirect category shell|1=\n{{Redirect to section}}\n{{Redirect from merge}}\n{{Redirect printworthy}}\n}}\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "===",
      "url": "https://en.wikipedia.org/wiki/%3D%3D%3D",
      "text": "#REDIRECT [[Relational operator#Comparing values of different types]]\n\n{{R to section}}\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "∽",
      "url": "https://en.wikipedia.org/wiki/%E2%88%BD",
      "text": "#REDIRECT [[Approximation#Unicode]]\n\n{{Redirect shell |\n  {{R from symbol}}\n  {{R from Unicode}}\n  {{R to section}}\n}}\n\n[[Category:Equivalence (mathematics)|=∽]]"
    },
    {
      "title": "≅",
      "url": "https://en.wikipedia.org/wiki/%E2%89%85",
      "text": "{{wiktionary|≅|congruence}}\nThe symbol '''≅''' is officially defined as {{unichar|2245|approximately equal to}}. It may refer to:\n* [[Approximate symbol|Approximate equality]]\n* [[Congruence (geometry)]]\n* [[Congruence relation]]\n* [[Isomorphism]]\n\n== See also ==\n* [[Congruence (disambiguation)]]\n\n{{disambiguation}}\n\n[[Category:Equivalence (mathematics)|=≅]]<!-- for all ambiguities -->"
    },
    {
      "title": "≈",
      "url": "https://en.wikipedia.org/wiki/%E2%89%88",
      "text": "#REDIRECT [[Approximation#Unicode]]\n\n{{redirect category shell|{{R from symbol}}{{R from Unicode}}{{R to section}}}}\n\n[[Category:Equivalence (mathematics)|=≈]]"
    },
    {
      "title": "≊",
      "url": "https://en.wikipedia.org/wiki/%E2%89%8A",
      "text": "#REDIRECT [[Approximation#Unicode]]\n\n{{Redirect shell |\n  {{R from symbol}}\n  {{R from Unicode}}\n  {{R to section}}\n}}\n\n[[Category:Equivalence (mathematics)|=≊]]"
    },
    {
      "title": "≏",
      "url": "https://en.wikipedia.org/wiki/%E2%89%8F",
      "text": "#REDIRECT [[Approximation#Unicode]]\n\n{{Redirect shell|\n{{Wikidata redirect}}\n{{R from Unicode}}\n{{R to section}}\n}}\n[[Category:Equivalence (mathematics)|=≏]]"
    },
    {
      "title": "≐",
      "url": "https://en.wikipedia.org/wiki/%E2%89%90",
      "text": "#Redirect [[Approximation#Unicode]]\n\n{{Redirect category shell|1=\n{{Redirect from symbol}}\n{{Redirect from Unicode}}\n{{Redirect to section}}\n}}\n\n[[Category:Equivalence (mathematics)|=≐]]"
    },
    {
      "title": "≒",
      "url": "https://en.wikipedia.org/wiki/%E2%89%92",
      "text": "#REDIRECT [[Approximation#Unicode]]\n\n{{Redirect shell |\n  {{R from symbol}}\n  {{R from Unicode}}\n  {{R to section}}\n}}\n\n[[Category:Equivalence (mathematics)|=≒]]"
    },
    {
      "title": "≓",
      "url": "https://en.wikipedia.org/wiki/%E2%89%93",
      "text": "#REDIRECT [[Approximation#Unicode]]\n\n{{Redirect shell |\n  {{R from symbol}}\n  {{R from Unicode}}\n  {{R to section}}\n}}\n\n[[Category:Equivalence (mathematics)|=≓]]"
    },
    {
      "title": "A-equivalence",
      "url": "https://en.wikipedia.org/wiki/A-equivalence",
      "text": "In [[mathematics]], '''<math>\\mathcal{A}</math>-equivalence''', sometimes called '''right-left equivalence''', is an [[equivalence relation]] between [[germ (mathematics)|map germs]].\n\nLet <math>M</math>  and <math>N</math> be two [[manifold]]s, and let <math>f, g : (M,x) \\to (N,y)</math> be two smooth [[germ (mathematics)|map germs]]. We say that <math>f</math> and <math>g</math> are <math>\\mathcal{A}</math>-equivalent if there exist [[diffeomorphism]] germs <math>\\phi : (M,x) \\to (M,x)</math> and <math>\\psi : (N,y) \\to (N,y)</math> such that <math>\\psi \\circ f = g \\circ \\phi.</math>\n\nIn other words, two [[germ (mathematics)|map germs]] are <math>\\mathcal{A}</math>-equivalent if one can be taken onto the other by a [[diffeomorphism|diffeomorphic]] change of co-ordinates in the source (i.e. <math>M</math>) and the target (i.e. <math>N</math>).\n\nLet <math>\\Omega(M_x,N_y)</math> denote the space of smooth [[germ (mathematics)|map germs]] <math>(M,x) \\to (N,y).</math> Let <math>\\mbox{diff}(M_x)</math> be the [[Group (mathematics)|group]] of [[diffeomorphism]] germs <math>(M,x) \\to (M,x)</math> and \n<math>\\mbox{diff}(N_y)</math> be the group of [[diffeomorphism]] germs <math>(N,y) \\to (N,y).</math> The group <math> G := \\mbox{diff}(M_x) \\times \\mbox{diff}(N_y)</math> acts on <math>\\Omega(M_x,N_y)</math> in the natural way: <math> (\\phi,\\psi) \\cdot f = \\psi^{-1} \\circ f \\circ \\phi.</math> Under this action we see that the [[germ (mathematics)|map germs]] <math>f, g : (M,x) \\to (N,y)</math> are <math>\\mathcal{A}</math>-equivalent if, and only if, <math>g</math> lies in the [[orbit (group theory)|orbit]] of <math>f</math>, i.e. <math> g \\in \\mbox{orb}_G(f)</math> (or vice versa).\n\nA map germ is called stable if its [[orbit (group theory)|orbit]] under the [[Group action (mathematics)|action]] of <math> G := \\mbox{diff}(M_x) \\times \\mbox{diff}(N_y)</math> is [[open set|open]] relative to the [[Whitney topology]]. Since <math>\\Omega(M_x,N_y)</math> is an infinite dimensional space [[metric topology]] is no longer trivial. [[Whitney topology]] compares the differences in successive derivatives and gives a notion of proximity within the infinite dimensional space. A base for the [[open set]]s of the [[topology]] in question is given by taking <math>k</math>-jets for every <math>k</math> and taking [[open set|open]] [[Neighbourhood (mathematics)|neighbourhood]]s in the ordinary Euclidean sense. [[Open set]]s in the [[topology]] are then [[Union (set theory)|union]]s of\nthese base sets.\n\nConsider the [[orbit (group theory)|orbit]] of some [[germ (mathematics)|map germ]] <math>orb_G(f).</math> The [[germ (mathematics)|map germ]] <math>f</math> is called simple if there are only finitely many other [[orbit (group theory)|orbit]]s in a [[neighbourhood]] of each of its points. [[Vladimir Arnold]] has shown that the only simple [[Mathematical singularity|singular]] [[germ (mathematics)|map germs]] <math>(\\mathbb{R}^n,0) \\to (\\mathbb{R},0)</math> for <math>1 \\le n \\le 3</math> are the infinite sequence <math>A_k</math> (<math>k \\in \\mathbb{N}</math>), the infinite sequence <math>D_{4+k}</math> (<math>k \\in \\mathbb{N}</math>), <math>E_6,</math> <math>E_7,</math> and <math>E_8.</math>\n\n==See also==\n* [[K-equivalence]] (contact equivalence)\n\n==References==\n* [[Marty Golubitsky|M. Golubitsky]] and V. Guillemin, ''Stable Mappings and Their Singularities''. Graduate Texts in Mathematics, Springer.\n\n[[Category:Functions and mappings]]\n[[Category:Singularity theory]]\n[[Category:Equivalence (mathematics)]]\n\n\n{{mathematics-stub}}"
    },
    {
      "title": "Adequate equivalence relation",
      "url": "https://en.wikipedia.org/wiki/Adequate_equivalence_relation",
      "text": "In [[algebraic geometry]], a branch of [[mathematics]], an '''adequate equivalence relation''' is an [[equivalence relation]] on [[algebraic cycles]] of smooth [[projective varieties]] used to obtain a well-working theory of such cycles, and in particular, well-defined [[intersection theory|intersection products]]. [[Pierre Samuel]] formalized the concept of an adequate equivalence relation in 1958.<ref>{{citation | last=Samuel | first=Pierre | title=Relations d'équivalence en géométrie algébrique | journal=Proc. ICM | year=1958 | publisher=Cambridge Univ. Press | pages=470–487 | url=http://www.mathunion.org/ICM/ICM1958/Main/icm1958.0470.0487.ocr.pdf | author-link=Pierre Samuel | access-date=2015-07-22 | archive-url=https://web.archive.org/web/20170722145343/http://www.mathunion.org/ICM/ICM1958/Main/icm1958.0470.0487.ocr.pdf | archive-date=2017-07-22 | dead-url=yes | df= }}</ref> Since then it has become central to theory of motives. For every adequate equivalence relation, one may define the category of [[motive (algebraic geometry)|pure motives]] with respect to that relation.\n\nPossible (and useful) adequate equivalence relations include ''rational'', ''algebraic'', ''homological'' and ''numerical equivalence''. They are called \"adequate\" because dividing out by the equivalence relation is functorial, i.e. push-forward (with change of co-dimension) and pull-back of cycles is well-defined. Codimension one cycles modulo rational equivalence form the classical group of [[Divisor (algebraic geometry)|divisor]]s. All cycles modulo rational equivalence form the [[Chow ring]].\n\n== Definition ==\nLet ''Z<sup>*</sup>(X)'' := '''Z'''[''X''] be the free abelian group on the algebraic cycles of ''X''. Then an adequate equivalence relation is a family of [[equivalence relation]]s, ''∼<sub>X</sub>'' on ''Z<sup>*</sup>(X)'', one for each smooth projective variety ''X'', satisfying the following three conditions:\n# (Linearity) The equivalence relation is compatible with addition of cycles.\n# ([[Chow's moving lemma|Moving lemma]]) If <math>\\alpha, \\beta \\in Z^{*}(X)</math> are cycles on ''X'', then there exists a cycle <math>\\alpha' \\in Z^{*}(X)</math> such that <math>\\alpha</math> ''~<sub>X</sub>'' <math>\\alpha'</math> and <math>\\alpha'</math> intersects <math>\\beta</math> properly.\n# (Push-forwards) Let <math>\\alpha \\in Z^{*}(X)</math> and <math>\\beta \\in Z^{*}(X \\times Y)</math> be cycles such that <math>\\beta</math> intersects <math>\\alpha \\times Y</math> properly. If <math>\\alpha</math> ''~<sub>X</sub> 0'', then <math>(\\pi_Y)_{*}(\\beta \\cdot (\\alpha \\times Y))</math> ''~<sub>Y</sub> 0'', where <math>\\pi_Y : X \\times Y \\to Y</math> is the projection.\n\nThe push-forward cycle in the last axiom is often denoted\n:<math>\\beta(\\alpha) := (\\pi_Y)_{*}(\\beta \\cdot (\\alpha \\times Y))</math>\nIf <math>\\beta</math> is the graph of a function, then this reduces to the push-forward of the function. The generalizations of functions from ''X'' to ''Y'' to cycles on ''X × Y'' are known as [[correspondence (algebraic geometry)|correspondences]]. The last axiom allows us to push forward cycles by a correspondence.\n\n== Examples of equivalence relations ==\n\nThe most common equivalence relations, listed from strongest to weakest, are gathered below in a table.\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! !! definition !! remarks\n|-\n! rational equivalence\n| ''Z ∼<sub>rat</sub> Z' '' if there is a cycle ''V'' on ''X × ''[[projective line|'''P'''<sup>1</sup>]] [[Flat morphism|flat]] over '''P'''<sup>1</sup>, such that [''V ∩ X × {0}''] - [''V ∩ X × {∞}''] = [''Z''] - [''Z' ''].\n|| the finest adequate equivalence relation (Lemma 3.2.2.1 in Yves André's book<ref>{{Citation | last1=André | first1=Yves | title=Une introduction aux motifs (motifs purs, motifs mixtes, périodes) | publisher=Société Mathématique de France | location=Paris | series=Panoramas et Synthèses | isbn=978-2-85629-164-1 |mr=2115000 | year=2004 | volume=17}}</ref>) \"∩\" denotes intersection in the cycle-theoretic sense (i.e. with multiplicities) and [''.''] denotes the cycle associated to a subscheme. see also [[Chow ring]]\n|-\n! algebraic equivalence\n| ''Z ∼<sub>alg</sub> Z' '' if there is a [[curve]] ''C'' and a cycle ''V'' on ''X × C'' flat over ''C'', such that [''V ∩ X × {c}''] - [''V ∩ X × {d}''] = [''Z''] - [''Z' ''] for two points ''c'' and ''d'' on the curve.\n|| Strictly stronger than homological equivalence, as measured by the [[Griffiths group]]. See also [[Néron–Severi group]].\n|-\n! smash-nilpotence equivalence\n| ''Z ∼<sub>sn</sub> Z' '' if ''Z - Z' '' is smash-nilpotent on ''X'', that is, if <math>(Z-Z')^{\\otimes n}</math> ''∼<sub>rat</sub> 0'' on ''X''<sup>n</sup> for ''n >> 0''.\n|| introduced by Voevodsky in 1995.<ref>{{citation | first=V. | last=Voevodsky | title=A nilpotence theorem for cycles algebraically equivalent to 0 | journal=Int. Math. Res. Notices | volume=4 | year=1995 | pages=1–12}}</ref>\n|-\n! homological equivalence\n| for a given [[Weil cohomology theory|Weil cohomology]] ''H'', ''Z ∼<sub>hom</sub> Z' '' if the image of the cycles under the cycle class map agrees\n|| depends a priori of the choice of ''H'', not assuming the [[standard conjectures on algebraic cycles|standard conjecture]] ''D'' \n|-\n! numerical equivalence\n| ''Z ∼<sub>num</sub> Z' '' if ''deg(Z ∩ T) = deg(Z' ∩ T)'', where ''T'' is any cycle such that ''dim T = codim Z'' (The intersection is a linear combination of points and we add the intersection multiplicities at each point to get the degree.)\n|| the coarsest equivalence relation (Exercise 3.2.7.2 in Yves André's book<ref>{{Citation | last1=André | first1=Yves | title=Une introduction aux motifs (motifs purs, motifs mixtes, périodes) | publisher=Société Mathématique de France | location=Paris | series=Panoramas et Synthèses | isbn=978-2-85629-164-1 |mr=2115000 | year=2004 | volume=17}}</ref>)\n|}\n\n== Notes ==\n<references />\n\n== References==\n* {{Citation | last1=Kleiman | first1=Steven L. | editor1-last=Oort | editor1-first=F. | title=Algebraic geometry, Oslo 1970 (Proc. Fifth Nordic Summer-School in Math., Oslo, 1970) | publisher=Wolters-Noordhoff | location=Groningen | mr=0382267 | year=1972 | chapter=Motives | pages=53–82}}\n* {{Citation | last=Jannsen | first=U. | title=Equivalence relations on algebraic cycles | journal=The Arithmetic and Geometry of Algebraic Cycles, NATO, 200 | publisher=Kluwer Ac. Publ. Co. | year=2000 | pages=225–260}}\n\n{{DEFAULTSORT:Adequate Equivalence Relation}}\n[[Category:Algebraic geometry]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Characterization (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Characterization_%28mathematics%29",
      "text": "In [[mathematics]], the statement that \"Property ''P'' '''characterizes''' object ''X''\" means that not only does ''X'' have [[property (philosophy)|property]] ''P'', but that ''X'' is the ''only'' thing that has property ''P''. In other words, ''P'' is a defining property of ''X''. Similarly a set of properties ''P'' is said to characterize ''X'' when these properties distinguish ''X'' from all other objects. Common mathematical expressions for a characterization of ''X'' in terms of ''P'' include \"P is [[necessary and sufficient]] for X\", and \"X holds [[if and only if]] P\".\n\nIt is also common to find statements such as \"Property ''Q'' characterises ''Y'' [[up to]] [[isomorphism]]\". The first type of statement says in different words that the [[extension (semantics)|extension]] of ''P'' is a [[singleton (mathematics)|singleton]] set. The second says that the extension of ''Q'' is a single [[equivalence class]] (for isomorphism, in the given example &mdash; depending on how ''up to'' is being used, some other [[equivalence relation]] might be involved).\n\nA reference on mathematical terminology notes ''characteristic'' is from Greek ''kharax'', \"a pointed stake\". \"From Greek ''kharax'' came ''kharakhter'', an instrument used to mark or engrave an object. Once an object was marked, it became distinctive, so the character of something came to mean its distinctive nature. The Late Greek suffix ''-istikos'' converted the noun ''character'' into the adjective ''characteristic'', which, in addition to maintaining its adjectival meaning, later became a noun as well.\"<ref>Steven Schwartzmann (1994) ''The Words of Mathematics: An etymological dictionary of mathematical terms used in English'', page 43, [[The Mathematical Association of America]] {{ISBN|0-88385-511-9}}</ref>\n\nJust as in chemistry, the [[characteristic property]] of a material will serve to identify a sample, or in the study of materials, structures and properties determine [[characterization (materials science)]], so in mathematics there is a continual effort to express properties that will distinguish a desired feature in a theory or system. Characterization is not unique to mathematics, but since the science is abstract, much of the activity can be described as \"characterization\". For instance, in [[Mathematical Reviews]], as of 2018, more than 24,000 articles contain the word in the article title, and 93,600 somewhere in the review.\n\nIn an arbitrary context of objects and features, characterizations have been expressed via the [[heterogeneous relation]] ''aRb'' meaning that object ''a'' has feature ''b''. For example, ''b'' may mean [[abstract and concrete|abstract or concrete]]. The objects can be considered the [[extension (semantics)|extension]]s of the world, while the features are expression of the [[intension]]s. A continuing program of characterization of various objects leads to their [[categorization]].\n\n==Examples==\n* A [[parallelogram]] is a [[quadrilateral]] with opposite sides parallel. One of its characterizations is that the diagonals bisect each other. This means that the diagonals in all parallelograms bisect each other, and conversely, that any quadrilateral where the diagonals bisect each other must be a parallelogram. The latter statement is only true if inclusive definitions of quadrilaterals are used (so that, for example, [[rectangle]]s count as parallelograms), which is the dominant way of defining objects in mathematics nowadays.\n* \"Among [[probability distribution]]s on the interval from 0 to ∞ on the real line, [[memorylessness]] characterizes the [[exponential distribution]]s.\"  This statement means that the exponential distributions are the only such probability distributions that are memoryless. (See also [[Characterization of probability distributions]].)\n* \"According to [[Bohr–Mollerup theorem]], among all functions ''f'' such that ''f''(1) = 1 and ''x f''(''x'') = ''f''(''x'' + 1) for ''x'' > 0, log-convexity characterizes the [[gamma function]].\"  This means that among all such functions, the gamma function is the ''only'' one that is log-convex.  (A function ''f'' is ''log-convex'' [[iff]] log(''f'') is a [[convex function]].  The base of the logarithm does not matter as long as it is more than 1, but conventionally mathematicians take \"log\" with no subscript to mean the [[natural logarithm]], whose base is ''e''.)\n* The circle is characterized as a [[manifold]] by being one-dimensional, [[compact space|compact]] and [[connected space|connected]]; here the characterization, as a smooth manifold, is [[up to]] [[diffeomorphism]].\n\n== See also ==\n\n* [[Characterization of probability distributions]]\n* [[Characterizations of the category of topological spaces]]\n* [[Characterizations of the exponential function]]\n* [[Characteristic (algebra)]]\n* [[Characteristic (exponent notation)]]\n* [[Euler characteristic]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Characterization (Mathematics)}}\n[[Category:Mathematical terminology]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Congruence (geometry)",
      "url": "https://en.wikipedia.org/wiki/Congruence_%28geometry%29",
      "text": "{{pp-semi|small=yes}}\n[[File:Congruent_non-congruent_triangles.svg|thumb|333px|An example of congruence. The two triangles on the left are congruent, while the third is [[Similarity (geometry)|similar]] to them. The last triangle is neither similar nor congruent to any of the others. Congruence permits alteration of some properties, such as location and orientation, but leaves others unchanged, like [[distance]] and [[angle]]s. The unchanged properties are called [[invariant (mathematics)|invariant]]s.]]\n\nIn [[geometry]], two figures or objects are '''congruent''' if they have the same [[shape]] and size, or if one has the same shape and size as the [[mirror image]] of the other.<ref>{{cite web|url=http://web.cortland.edu/matresearch/OxfordDictionaryMathematics.pdf|title=Oxford Concise Dictionary of Mathematics, Congruent Figures|first1=C.|last1=Clapham|first2=J.|last2=Nicholson|publisher=Addison-Wesley|year=2009|page=167|accessdate=2 June 2017|deadurl=bot: unknown|archiveurl=https://web.archive.org/web/20131029203826/http://web.cortland.edu/matresearch/OxfordDictionaryMathematics.pdf|archivedate=29 October 2013|df=}}</ref> \n\nMore formally, two sets of  [[point (geometry)|points]] are called '''congruent''' if, and only if, one can be transformed into the other by an [[isometry]], i.e., a combination of '''rigid motions''', namely a [[translation (geometry)|translation]], a [[rotation]], and a [[reflection (mathematics)|reflection]]. This means that either object can be repositioned and reflected (but not resized) so as to coincide precisely with the other object. So two distinct plane figures on a piece of paper are congruent if we can cut them out and then match them up completely. Turning the paper over is permitted.\n\n[[File:Angle-angle-side_triangle_congruence.svg|thumb|333px|This diagram illustrates the geometric principle of angle-angle-side triangle congruence: Given triangle ABC and triangle A'B'C', triangle ABC is congruent with triangle A'B'C' if and only if angle CAB is congruent with C'A'B' and angle ABC is congruent with A'B'C' and BC is congruent with B'C']]\n\nIn elementary geometry the word ''congruent'' is often used as follows.<ref>{{cite web|url=http://mathopenref.com/congruent.html|title=Congruence|publisher=Math Open Reference|year=2009|accessdate=2 June 2017}}</ref>  The word ''equal'' is often used in place of ''congruent'' for these objects.\n*Two [[line segment]]s are congruent if they have the same length.  \n*Two [[angle]]s are congruent if they have the same measure.\n*Two [[circle]]s are congruent if they have the same diameter.\n\nIn this sense, ''two plane figures are congruent'' implies that their corresponding characteristics are \"congruent\" or \"equal\" including not just their corresponding sides and angles, but also their corresponding diagonals, perimeters and areas.\n\nThe related concept of [[Similarity (geometry)|similarity]] applies if the objects have the same shape but do not necessarily have the same size. (Most definitions consider congruence to be a form of similarity, although a minority require that the objects have different sizes in order to qualify as similar.)\n\n==Determining congruence of polygons==\n[[File:quadrilateral_congruence.png|thumb|333px|The orange and green quadrilaterals are congruent; the blue is not congruent to them. All three have the same [[perimeter]] and [[area]]. (The ordering of the sides of the blue quadrilateral is \"mixed\" which results in two of the interior angles and one of the diagonals not being congruent.)]]\n\nFor two polygons to be congruent, they must have an equal number of sides (and hence an equal number&mdash;the same number&mdash;of vertices). Two polygons with ''n'' sides are congruent if and only if they each have numerically identical sequences (even if clockwise for one polygon and counterclockwise for the other) side-angle-side-angle-... for ''n'' sides and ''n'' angles.\n\nCongruence of polygons can be established graphically as follows:\n\n*First, match and label the corresponding vertices of the two figures.\n*Second, draw a vector from one of the vertices of the one of the figures to the corresponding vertex of the other figure. ''Translate'' the first figure by this vector so that these two vertices match.\n*Third, ''rotate'' the translated figure about the matched vertex until one pair of [[corresponding sides]] matches.\n*Fourth, ''reflect'' the rotated figure about this matched side until the figures match. \n\nIf at any time the step cannot be completed, the polygons are not congruent.\n\n==Congruence of triangles==<!-- This section is linked from [[SAS]] and from [[congruence of triangles]], a redirect page. -->\n{{see also|Solution of triangles}}\n\nTwo [[triangle]]s are congruent if their corresponding [[Edge (geometry)|sides]] are equal in length, and their corresponding [[angle | angles]] are equal in measure.\n\nIf triangle ABC is congruent to triangle DEF, the relationship can be written mathematically as:\n\n:<math>\\triangle \\mathrm{ABC} \\cong \\triangle \\mathrm{DEF}.</math>\nIn many cases it is sufficient to establish the equality of three corresponding parts and use one of the following results to deduce the congruence of the two triangles.\n\n[[Image:Congruent triangles.svg|thumb|333px|right|The shape of a triangle is determined up to congruence by specifying two sides and the angle between them (SAS), two angles and the side between them (ASA) or two angles and a corresponding adjacent side (AAS). Specifying two sides and an adjacent angle (SSA), however, can yield two distinct possible triangles.]]\n\n===Determining congruence===\nSufficient evidence for congruence between two triangles in [[Euclidean space]] can be shown through the following comparisons:\n\n*'''SAS''' (Side-Angle-Side): If two pairs of sides of two triangles are equal in length, and the included angles are equal in measurement, then the triangles are congruent.\n*'''SSS''' (Side-Side-Side): If three pairs of sides of two triangles are equal in length, then the triangles are congruent.\n*'''ASA''' (Angle-Side-Angle): If two pairs of angles of two triangles are equal in measurement, and the included sides are equal in length, then the triangles are congruent.  \n\nThe ASA Postulate was contributed by [[Thales of Miletus]] (Greek).  In most systems of axioms, the three criteria – SAS, SSS and ASA – are established as [[theorem]]s.  In the [[School Mathematics Study Group]] system '''SAS''' is taken as one (#15) of 22 postulates.\n\n*'''AAS''' (Angle-Angle-Side): If two pairs of angles of two triangles are equal in measurement, and a pair of corresponding non-included sides are equal in length, then the triangles are congruent. AAS is equivalent to an ASA condition, by the fact that if any two angles are given, so is the third angle, since their sum should be 180°. ASA and AAS are sometimes combined into a single condition, '''AAcorrS''' – any two angles and a corresponding side.<ref>{{cite book\n  | last = Parr\n  | first = H. E.\n  | title = Revision Course in School mathematics\n  | publisher = G Bell and Sons Ltd.\n  | series = Mathematics Textbooks Second Edition\n  | year = 1970\n  | isbn = 0-7135-1717-4}}</ref>\n*'''RHS''' (Right-angle-Hypotenuse-Side), also known as  '''HL''' (Hypotenuse-Leg): If two right-angled triangles have their hypotenuses equal in length, and a pair of shorter sides are equal in length, then the triangles are congruent.  \n\n====Side-side-angle====\nThe SSA condition (Side-Side-Angle) which specifies two sides and a non-included angle (also known as ASS, or Angle-Side-Side) does not by itself prove congruence. In order to show congruence, additional information is required such as the measure of the corresponding angles and in some cases the lengths of the two pairs of corresponding sides. There are a few possible cases:\n\nIf two triangles satisfy the SSA condition and the length of the side opposite the angle is greater than or equal to the length of the adjacent side (SSA, or long side-short side-angle), then the two triangles are congruent. The opposite side is sometimes longer when the corresponding angles are acute, but it is ''always'' longer when the corresponding angles are right or obtuse. Where the angle is a right angle, also known as the Hypotenuse-Leg (HL) postulate or the Right-angle-Hypotenuse-Side (RHS) condition, the third side can be calculated using the [[Pythagorean Theorem]] thus allowing the SSS postulate to be applied.\n\nIf two triangles satisfy the SSA condition and the corresponding angles are acute and the length of the side opposite the angle is equal to the length of the adjacent side multiplied by the sine of the angle, then the two triangles are congruent.\n\nIf two triangles satisfy the SSA condition and the corresponding angles are acute and the length of the side opposite the angle is greater than the length of the adjacent side multiplied by the sine of the angle (but less than the length of the adjacent side), then the two triangles cannot be shown to be congruent. This is the [[ambiguous case]] and two different triangles can be formed from the given information, but further information distinguishing them can lead to a proof of congruence.\n\n====Angle-angle-angle====\nIn Euclidean geometry, AAA (Angle-Angle-Angle) (or just AA, since in Euclidean geometry the angles of a triangle add up to 180°) does not provide information regarding the size of the two triangles and hence proves only [[similarity (geometry)|similarity]] and not congruence in Euclidean space.\n\nHowever, in [[spherical geometry]] and [[hyperbolic geometry]] (where the sum of the angles of a triangle varies with size) AAA is sufficient for congruence on a given curvature of surface.<ref>{{cite book\n  | last = Cornel\n  | first = Antonio\n  | authorlink = Antonio Coronel\n  | title = Geometry for Secondary Schools\n  | publisher = Bookmark Inc.\n  | series = Mathematics Textbooks Second Edition\n  | year = 2002\n  | doi = \n  | isbn = 971-569-441-1}}</ref>\n\n=== {{anchor|CPCTC}} CPCTC===\nThis [[acronym]] stands for ''Corresponding Parts of Congruent Triangles are Congruent'' an abbreviated version of the definition of congruent triangles.<ref>{{citation|first=Harold R.|last=Jacobs|title=Geometry|page=160|year=1974|publisher=W.H. Freeman|isbn=0-7167-0456-0}} Jacobs uses a slight variation of the phrase</ref><ref>{{cite web|url=https://www.cliffsnotes.com/study-guides/geometry/triangles/congruent-triangles|title=Congruent Triangles|publisher=Cliff's Notes|accessdate=2014-02-04}}</ref>\n \nIn more detail, it is a succinct way to say that if triangles {{mvar|ABC}} and {{mvar|DEF}} are congruent, that is,\n\n:<math>\\triangle ABC \\cong \\triangle DEF,</math>\n\nwith corresponding pairs of angles at vertices {{mvar|A}} and {{mvar|D}}; {{mvar|B}} and {{mvar|E}}; and {{mvar|C}} and {{mvar|F}}, and with corresponding pairs of sides {{mvar|AB}} and {{mvar|DE}}; {{mvar|BC}} and {{mvar|EF}}; and {{mvar|CA}} and {{mvar|FD}}, then the following statements are true:\n\n:<math>\\overline{AB} \\cong \\overline{DE}</math>\n:<math>\\overline{BC} \\cong \\overline{EF}</math>\n:<math>\\overline{AC} \\cong \\overline{DF}</math>\n:<math>\\angle BAC \\cong \\angle EDF</math>\n:<math>\\angle ABC \\cong \\angle DEF</math>\n:<math>\\angle BCA \\cong \\angle EFD.</math>\n\nThe statement is often used as a justification in elementary geometry proofs when a conclusion of the congruence of parts of two triangles is needed after the congruence of the triangles has been established. For example, if two triangles have been shown to be congruent by the ''SSS'' criteria and a statement that corresponding angles are congruent is needed in a proof, then CPCTC may be used as a justification of this statement.\n\nA related theorem is '''CPCFC''', in which \"triangles\" is replaced with \"figures\" so that the theorem applies to any pair of [[polygon]]s or [[polyhedron]]s that are congruent.\n\n==Definition of congruence in analytic geometry==\nIn a [[Euclidean geometry|Euclidean system]], congruence is fundamental; it is the counterpart of equality for numbers.  In [[analytic geometry]], congruence may be defined intuitively thus: two mappings of figures onto one Cartesian coordinate system are congruent if and only if, for ''any'' two points in the first mapping, the [[Euclidean distance]] between them is equal to the Euclidean distance between the corresponding points in the second mapping.\n\nA more formal definition states that two [[subset]]s ''A'' and ''B'' of [[Euclidean space]] '''R'''<sup>''n''</sup> are called congruent if there exists an [[isometry]] ''f'' : '''R'''<sup>''n''</sup>  → '''R'''<sup>''n''</sup> (an element of the [[Euclidean group]] ''E''(''n'')) with ''f''(''A'') = ''B''. Congruence is an [[equivalence relation]].\n\n==Congruent conic sections==\n\nTwo conic sections are congruent if their [[Eccentricity (mathematics)|eccentricities]] and one other distinct parameter characterizing them are equal. Their eccentricities establish their shapes, equality of which is sufficient to establish similarity, and the second parameter then establishes size. Since two [[circle]]s, [[parabola]]s, or [[rectangular hyperbola]]s always have the same eccentricity (specifically 0 in the case of circles, 1 in the case of parabolas, and <math>\\sqrt{2}</math> in the case of rectangular hyperbolas), two circles, parabolas, or rectangular hyperbolas need to have only one other common parameter value, establishing their size, for them to be congruent.\n\n==Congruent polyhedra==\n\nFor two [[polyhedra]] with the same number ''E'' of edges, the same number of [[face (geometry)|faces]], and the same number of sides on corresponding faces, there exists a set of at most ''E'' measurements that can establish whether or not the polyhedra are congruent.<ref>{{cite journal |first=Alexander |last=Borisov |first2=Mark |last2=Dickinson |first3=Stuart |last3=Hastings |title=A Congruence Problem for Polyhedra |journal=[[American Mathematical Monthly]] |volume=117 |date=March 2010 |pages=232–249 |doi=10.4169/000298910X480081 }}</ref><ref>{{cite web |first=Alexa |last=Creech |title=A Congruence Problem |url=http://146.163.152.131/teaching/projects/creech_final.pdf |archive-url=https://web.archive.org/web/20131111162553/http://146.163.152.131/teaching/projects/creech_final.pdf |archive-date=November 11, 2013 }}</ref>  For [[cube]]s, which have 12 edges, only 9 measurements are necessary.\n\n==Congruent triangles on a sphere==\n{{Main|Solving triangles#Solving spherical triangles|Spherical trigonometry#Solution of triangles}}\n\nAs with plane triangles, on a sphere two triangles sharing the same sequence of angle-side-angle (ASA) are necessarily congruent (that is, they have three identical sides and three identical angles).<ref name=Bolin>{{cite web |first=Michael |last=Bolin |title=Exploration of Spherical Geometry |date=September 9, 2003 |pages=6–7 |url=http://math.iit.edu/~mccomic/420/notes/Bolin_spherical.pdf#page=6 }}</ref> This can be seen as follows: One can situate one of the vertices with a given angle at the south pole and run the side with given length up the prime meridian. Knowing both angles at either end of the segment of fixed length ensures that the other two sides emanate with a uniquely determined trajectory, and thus will meet each other at a uniquely determined point; thus ASA is valid. \n\nThe congruence theorems side-angle-side (SAS) and side-side-side (SSS) also hold on a sphere; in addition, if two spherical triangles have an identical angle-angle-angle (AAA) sequence, they are congruent (unlike for plane triangles).<ref name=Bolin/>\n\nThe plane-triangle congruence theorem angle-angle-side (AAS) does not hold for spherical triangles.<ref>{{cite web |last=Hollyer |first=L. |url=http://www.uh.edu/~hollyer/Module6/m6ppt/sld089.htm |title=Slide 89 of 112 |work= }}</ref> As in plane geometry, side-side-angle (SSA) does not imply congruence.\n\n== Notation ==\nA symbol commonly used for congruence is an equals symbol with a [[tilde]] above it, '''≅''', corresponding to the [[Unicode]] character 'approximately equal to' (U+2245).  In the UK, the three-bar equal sign ''≡'' (U+2261) is sometimes used.\n\n==See also==\n*[[CPCTC]] (Corresponding parts of congruent triangles are congruent)\n*[[Euclidean plane isometry]]\n*[[Isometry]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{Commons category|Congruence}}\n* [http://www.cut-the-knot.org/pythagoras/SSS.shtml The SSS] at [[Cut-the-Knot]]\n* [http://www.cut-the-knot.org/pythagoras/SSA.shtml The SSA] at [[Cut-the-Knot]]\n* Interactive animations demonstrating [http://www.mathopenref.com/congruentpolygons.html Congruent polygons], [http://www.mathopenref.com/congruentangles.html Congruent angles], [http://www.mathopenref.com/congruentlines.html Congruent line segments], [http://www.mathopenref.com/congruenttriangles.html Congruent triangles] at Math Open Reference\n\n[[Category:Euclidean geometry]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Congruence of squares",
      "url": "https://en.wikipedia.org/wiki/Congruence_of_squares",
      "text": "{{Unreferenced|date=December 2009}}\nIn [[number theory]], a '''congruence of squares''' is a [[congruence relation|congruence]] commonly used in [[integer factorization]] algorithms.\n\n==Derivation==\nGiven a positive [[integer]] ''n'', [[Fermat's factorization method]] relies on finding numbers ''x'', ''y'' satisfying the [[equation|equality]]\n\n:<math>x^2 - y^2 = n\\,\\!</math>\n\nWe can then factor ''n'' = ''x''<sup>2</sup> - ''y''<sup>2</sup> = (''x'' + ''y'')(''x'' - ''y''). This algorithm is slow in practice because we need to search many such numbers, and only a few satisfy the strict equation. However, ''n'' may also be factored if we can satisfy the weaker '''congruence of squares''' condition:\n\n:<math>x^2 \\equiv y^2 \\pmod{n}</math>\n:<math>x \\not\\equiv \\pm y \\pmod{n}</math>\n\n\nFrom here we easily deduce\n:<math>x^2 - y^2 \\equiv 0 \\pmod{n}</math>\n:<math>(x + y)(x - y) \\equiv 0 \\pmod{n}</math>\n\nThis means that ''n'' divides the product (''x'' + ''y'')&nbsp;(''x'' - ''y''). Thus (''x'' + ''y'') and (''x'' − ''y'') each contain factors of ''n'', but those factors can be trivial. In this case we need to find another ''x'' and ''y''. Computing the [[greatest common divisor]]s of (''x'' + ''y'', ''n'') and of (''x'' - ''y'', ''n'') will give us these factors; this can be done quickly using the [[Euclidean algorithm]].\n\nCongruences of squares are extremely useful in integer factorization algorithms and are extensively used in, for example, the [[quadratic sieve]], [[general number field sieve]], [[continued fraction factorization]], and [[Dixon's factorization method|Dixon's factorization]]. Conversely, because finding square roots modulo a composite number turns out to be probabilistic polynomial-time equivalent to factoring that number, any integer factorization algorithm can be used efficiently to identify a congruence of squares.\n\n===Further generalizations===\nIt is also possible to use [[factor base]]s to help find congruences of squares more quickly. Instead of looking for <math>\\textstyle x^2 \\equiv y^2 \\pmod{n}</math> from the outset, we find many <math>\\textstyle x^2 \\equiv y \\pmod{n}</math> where the ''y'' have small prime factors, and try to multiply a few of these together to get a square on the right-hand side.\n\n==Examples==\n===Factorize 35===\nWe take '''''n'' = 35''' and find that \n\n:<math>\\textstyle 6^2 = 36 \\equiv 1 \\equiv 1^2 \\pmod{35}</math>. \n\nWe thus factor as\n\n:<math> \\gcd( 6-1, 35 ) \\cdot \\gcd( 6+1, 35 ) = 5 \\cdot 7 = 35</math>\n\n===Factorize 1649===\nUsing '''''n'' = 1649''', as an example of finding a congruence of squares built up from the products of non-squares (see [[Dixon's factorization method]]), first we obtain several congruences\n\n:<math> 41^2 \\equiv 32 \\pmod{1649}</math>\n:<math> 42^2 \\equiv 115 \\pmod{1649}</math>\n:<math> 43^2 \\equiv 200 \\pmod{1649}</math>\n\nof these, two have only small primes as factors\n\n:<math> 32 = 2^5</math>\n:<math> 200 = 2^3 \\cdot 5^2</math>\n\nand a combination of these has an even power of each small prime, and is therefore a square\n\n:<math> 32 \\cdot 200 = 2^{5+3} \\cdot 5^2 = 2^8 \\cdot 5^2 = ( 2^4 \\cdot 5 )^2 = 80^2</math>\n\nyielding the congruence of squares\n\n:<math> 32 \\cdot 200 = 80^2 \\equiv 41^2 \\cdot 43^2 \\equiv 114^2 \\pmod{1649}</math>\n\nSo using the values of 80 and 114 as our ''x'' and ''y'' gives factors\n\n:<math> \\gcd( 114-80, 1649 ) \\cdot \\gcd( 114+80, 1649 ) = 17 \\cdot 97 = 1649.</math>\n\n== See also ==\n*[[Congruence relation]]\n\n<!--== External links ==-->\n\n{{DEFAULTSORT:Congruence Of Squares}}\n[[Category:Equivalence (mathematics)]]\n[[Category:Integer factorization algorithms]]\n[[Category:Modular arithmetic]]\n[[Category:Squares in number theory]]"
    },
    {
      "title": "Elementary equivalence",
      "url": "https://en.wikipedia.org/wiki/Elementary_equivalence",
      "text": "In [[model theory]], a branch of [[mathematical logic]], two [[Structure (mathematical logic)|structure]]s ''M'' and ''N'' of the same [[signature (mathematical logic)|signature]] σ are called '''elementarily equivalent''' if they satisfy the same [[first-order logic|first-order]] [[σ-sentence]]s.\n\nIf ''N'' is a [[substructure (mathematics)|substructure]] of ''M'', one often needs a stronger condition. In this case ''N'' is called an '''elementary substructure''' of ''M'' if every first-order σ-formula φ(''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>''n''</sub>) with parameters ''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>''n''</sub> from ''N'' is true in ''N'' if and only if it is true in&nbsp;''M''.\nIf ''N'' is an elementary substructure of ''M'', ''M'' is called an '''elementary extension''' of&nbsp;''N''. An [[embedding#Universal algebra and model theory|embedding]] ''h'':&nbsp;''N''&nbsp;→&nbsp;''M'' is called an '''elementary embedding''' of ''N'' into ''M'' if ''h''(''N'') is an elementary substructure of&nbsp;''M''.\n\nA substructure ''N'' of ''M'' is elementary if and only if it passes the '''Tarski–Vaught test''': every first-order formula φ(''x'',&nbsp;''b''<sub>1</sub>,&nbsp;…,&nbsp;''b''<sub>''n''</sub>) with parameters in ''N'' that has a solution in ''M'' also has a solution in&nbsp;''N'' when evaluated in&nbsp;''M''. One can prove that two structures are elementary equivalent with the [[Ehrenfeucht–Fraïssé games]].\n\n==Elementarily equivalent structures==\nTwo structures ''M'' and ''N'' of the same signature&nbsp;σ are '''elementarily equivalent''' if every first-order sentence (formula without free variables) over&nbsp;σ is true in ''M'' if and only if it is true in ''N'', i.e. if ''M'' and ''N'' have the same [[complete theory|complete]] first-order theory.\nIf ''M'' and ''N'' are elementarily equivalent, one writes ''M''&nbsp;≡&nbsp;''N''.\n\nA first-order [[theory (mathematical logic)|theory]] is complete if and only if any two of its models are elementarily equivalent.\n\nFor example, consider the language with one binary relation symbol '<'.  The model '''R''' of [[real numbers]] with its usual order and the model '''Q''' of [[rational numbers]] with its usual order are elementarily equivalent, since they both interpret '<' as an unbounded dense [[linear ordering]]. This is sufficient to ensure elementary equivalence, because the theory of unbounded dense linear orderings is complete, as can be shown by [[Vaught's test]].\n\nMore generally, any first-order theory with an infinite model has non-isomorphic, elementary equivalent models, which can be obtained via the [[Löwenheim–Skolem theorem]]. Thus, for example, there are [[Non-standard model of arithmetic|non-standard models]] of [[Peano arithmetic]], which contain other objects than just the numbers 0, 1, 2, etc., and yet are elementarily equivalent to the standard model.\n\n==Elementary substructures and elementary extensions==\n''N'' is an '''elementary substructure''' of ''M'' if ''N'' and ''M'' are structures of the same [[signature (mathematical logic)|signature]]&nbsp;σ such that for all first-order σ-formulas φ(''x''<sub>1</sub>,&nbsp;…,&nbsp;''x''<sub>''n''</sub>) with free variables ''x''<sub>1</sub>,&nbsp;…,&nbsp;''x''<sub>''n''</sub>, and all elements ''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>n</sub> of&nbsp;''N'', φ(''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>n</sub>) holds in ''N'' if and only if it holds in ''M'':\n:''N'' <math>\\models</math> φ(''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>''n''</sub>) iff ''M'' <math>\\models</math> φ(''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>''n''</sub>).\nIt follows that ''N'' is a substructure of ''M''.\n\nIf ''N'' is a substructure of ''M'', then both ''N'' and ''M'' can be interpreted as structures in the signature σ<sub>''N''</sub> consisting of σ together with a new constant symbol for every element of&nbsp;''N''. ''N'' is an elementary substructure of ''M'' if and only if ''N'' is a substructure of ''M'' and ''N'' and ''M'' are elementarily equivalent as σ<sub>''N''</sub>-structures.\n\nIf ''N'' is an elementary substructure of ''M'', one writes ''N'' <math>\\preceq</math> ''M'' and says that ''M'' is an '''elementary extension''' of ''N'': ''M'' <math>\\succeq</math> ''N''.\n\nThe downward [[Löwenheim–Skolem theorem]] gives a countable elementary substructure for any infinite first-order structure in at most countable signature; the upward Löwenheim–Skolem theorem gives elementary extensions of any infinite first-order structure of arbitrarily large cardinality.\n\n==Tarski–Vaught test==\nThe '''Tarski–Vaught test''' (or '''Tarski–Vaught criterion''') is a necessary and sufficient condition for a substructure ''N'' of a structure ''M'' to be an elementary substructure. It can be useful for constructing an elementary substructure of a large structure.\n\nLet ''M'' be a structure of signature σ and ''N'' a substructure of ''M''. ''N'' is an elementary substructure of ''M'' if and only if for every first-order formula φ(''x'',&nbsp;''y''<sub>1</sub>,&nbsp;…,&nbsp;''y''<sub>''n''</sub>) over σ and all elements ''b''<sub>1</sub>,&nbsp;…,&nbsp;''b''<sub>''n''</sub> from ''N'', if ''M'' <math>\\models</math> {{exist}}''x''&nbsp;φ(''x'',&nbsp;''b''<sub>1</sub>,&nbsp;…,&nbsp;''b''<sub>''n''</sub>), then there is an element ''a'' in ''N'' such that ''M'' <math>\\models</math>φ(''a'',&nbsp;''b''<sub>1</sub>,&nbsp;…,&nbsp;''b''<sub>''n''</sub>).\n\n==Elementary embeddings==\nAn '''elementary embedding''' of a structure ''N'' into a structure ''M'' of the same signature σ is a map ''h'':&nbsp;''N''&nbsp;→&nbsp;''M'' such that for every first-order σ-formula φ(''x''<sub>1</sub>,&nbsp;…,&nbsp;''x''<sub>''n''</sub>) and all elements ''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>n</sub> of&nbsp;''N'', \n:''N'' <math>\\models</math> φ(''a''<sub>1</sub>,&nbsp;…,&nbsp;''a''<sub>''n''</sub>) if and only if ''M'' <math>\\models</math> φ(''h''(''a''<sub>1</sub>),&nbsp;…,&nbsp;''h''(''a''<sub>''n''</sub>)).\nEvery elementary embedding is a [[Structure (mathematical logic)#Homomorphisms|strong homomorphism]], and its image is an elementary substructure.\n\nElementary embeddings are the most important maps in model theory.  In [[set theory]], elementary embeddings whose domain is ''V'' (the universe of set theory) play an important role in the theory of [[large cardinals]] (see also [[critical point (set theory)|critical point]]).\n\n== References ==\n* {{Citation | last1=Chang | first1=Chen Chung | last2=Keisler | first2=H. Jerome | author1-link=Chen Chung Chang | author2-link=Howard Jerome Keisler | title=Model Theory | origyear=1973 | publisher=Elsevier | edition=3rd | series=Studies in Logic and the Foundations of Mathematics | isbn=978-0-444-88054-3 | year=1990}}.\n* {{Citation | last1=Hodges | first1=Wilfrid | author1-link=Wilfrid Hodges | title=A shorter model theory | publisher= [[Cambridge University Press]]| location=Cambridge | isbn=978-0-521-58713-6 | year=1997}}.\n* {{Citation |last=Monk |first=J. Donald |title=Mathematical Logic |series=Graduate Texts in Mathematics |publisher=Springer Verlag |location=New York • Heidelberg • Berlin |year=1976 |isbn=0-387-90170-1}}\n\n{{DEFAULTSORT:Elementary Equivalence}}\n[[Category:Model theory]]\n[[Category:Equivalence (mathematics)]]\n\n[[zh:基本子结构]]"
    },
    {
      "title": "Equality operator",
      "url": "https://en.wikipedia.org/wiki/Equality_operator",
      "text": "#REDIRECT [[Relational operator#Equality]]\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Equals sign",
      "url": "https://en.wikipedia.org/wiki/Equals_sign",
      "text": "{{Redirect2|&#061;|＝|double hyphens|Double hyphen}}\n{{Other uses2|Equal}}\n{{Technical reasons|:<nowiki>=</nowiki>|the computer programming assignment operator|Assignment (computer science)|the definition symbol|List of mathematical symbols#Symbols based on equality}}\n{{Punctuation marks|{{=}}|variant1=≠|caption1=Not equals sign|variant2=≈|caption2=Almost equals sign}}\n\n[[Image:2+2.svg|thumb|right|200px|A well-known [[Equality (mathematics)|equality]] featuring the equals sign]] The '''equals sign''' or '''equality sign''' ('''{{Big|1==}}''') is a [[mathematical symbol]] used to indicate [[equality (mathematics)|equality]]. It was invented in 1557 by [[Robert Recorde]]. In an [[equation]], the equals sign is placed between two (or more) expressions that have the same value. In [[Unicode]] and [[ASCII]], it is {{unichar|003d|equals sign|html=}}.\n\n==History==\nThe etymology of the word \"equal\" is from the Latin word \"''æqualis\"'' as meaning \"uniform\", \"identical\", or \"equal\", from ''aequus'' (\"level\", \"even\", or \"just\").\n\n[[Image:First Equation Ever.png|thumb|The first use of an equals sign, equivalent to 14''x''+15=71 in modern notation. From ''[[The Whetstone of Witte]]'' (1557) by [[Robert Recorde]].]]\n[[Image:Recorde - The Whetstone of Witte - equals.jpg|thumb|Recorde's introduction of \"=\"]]\n\nThe \"=\" symbol that is now universally accepted in mathematics for equality was first recorded by Welsh mathematician [[Robert Recorde]] in ''[[The Whetstone of Witte]]'' (1557). The original form of the symbol was much wider than the present form. In his book Recorde explains his design of the \"Gemowe lines\" (meaning ''twin'' lines, from the [[Latin]] ''[[wiktionary:gemellus|gemellus]]''<ref name=\"gemellus\">See also [[wiktionary:geminus|geminus]] and [[Gemini (constellation)|Gemini]].</ref>):<ref>Recorde, Robert, ''The Whetstone of Witte'' … (London, England: Jhon Kyngstone, 1557), [https://archive.org/stream/TheWhetstoneOfWitte#page/n237/mode/2up the third page of the chapter \"The rule of equation, commonly called Algebers Rule.\"]</ref>  \n<blockquote>\n''And to auoide the tediouſe repetition of theſe woordes : is equalle to : I will ſette as I doe often in woorke vſe, a paire of paralleles, or Gemowe lines of one lengthe, thus: =, bicauſe noe .2. thynges, can be moare equalle.'' \n</blockquote>\n<blockquote>\nAnd to avoid the tedious repetition of these words: is equal to: I will set as I do often in work use, a pair of parallels, or Gemowe lines of one length, thus: =, because no 2 things, can be more equal.\n</blockquote>\n\nAccording to Scotland's [[University of St Andrews]] History of Mathematics website:<ref>{{cite web | url = http://www-history.mcs.st-and.ac.uk/Mathematicians/Recorde.html | website = [[MacTutor History of Mathematics archive]] | title = Robert Recorde | accessdate = 19 October 2013}}</ref>\n<blockquote>The symbol '=' was not immediately popular. The symbol || was used by some and ''æ'' (or ''œ''), from the Latin word ''aequalis'' meaning equal, was widely used into the 1700s.</blockquote>\n\n==Usage in mathematics and computer programming==\nIn mathematics, the equals sign can be used as a simple statement of fact in a specific case (x&nbsp;=&nbsp;2), or to create definitions (let&nbsp;x&nbsp;=&nbsp;2), conditional statements (if&nbsp;x&nbsp;=&nbsp;2, then …), or to express a universal equivalence <code>(x&nbsp;+&nbsp;1)<sup>2</sup>&nbsp;=&nbsp;x<sup>2</sup>&nbsp;+&nbsp;2x&nbsp;+&nbsp;1</code>.\n\nThe first important [[computer programming language]] to use the equals sign was the original version of [[Fortran]], FORTRAN&nbsp;I, designed in 1954 and implemented in 1957. In Fortran, \"=\" serves as an [[assignment (computer science)|assignment]] operator: <code>X&nbsp;=&nbsp;2</code> sets the value of <code>X</code> to 2. This somewhat resembles the use of \"=\" in a mathematical definition, but with different semantics: the expression following \"=\" is evaluated first and may refer to a previous value of <code>X</code>. For example, the assignment <code>X&nbsp;=&nbsp;X&nbsp;+&nbsp;2</code> increases the value of <code>X</code> by 2.\n\nA rival programming-language usage was pioneered by the original version of [[ALGOL]], which was designed in 1958 and implemented in 1960. ALGOL included a [[relational operator]] that tested for equality, allowing constructions like <code>if&nbsp;x&nbsp;=&nbsp;2</code> with essentially the same meaning of \"=\" as the conditional usage in mathematics. The equals sign was reserved for this usage.\n\nBoth usages have remained common in different programming languages into the early 21st century. As well as Fortran, \"=\" is used for assignment in such languages as [[C (programming language)|C]], [[Perl]], [[Python (programming language)|Python]], [[awk]], and their descendants. But \"=\" is used for equality and not assignment in the [[Pascal (programming language)|Pascal]] family, [[Ada (programming language)|Ada]], [[Eiffel (programming language)|Eiffel]], [[APL (programming language)|APL]], and other languages.\n\nA few languages, such as [[BASIC]] and [[PL/I]], have used the equals sign to mean both assignment and equality, distinguished by context. However, in most languages where \"=\" has one of these meanings, a different character or, more often, a sequence of characters is used for the other meaning. Following ALGOL, most languages that use \"=\" for equality use \":=\" for assignment, although APL, with its special character set, uses a left-pointing arrow.\n\nFortran did not have an equality operator (it was only possible to compare an expression to zero, using the [[arithmetic IF]] statement) until FORTRAN&nbsp;IV was released in 1962, since when it has used the four characters \".EQ.\" to test for equality. The language [[B (programming language)|B]] introduced the use of \"==\" with this meaning, which has been copied by its descendant C and most later languages where \"=\" means assignment.\n\nThe equals sign is also used in defining [[attribute–value pair]]s, in which an [[Attribute (computing)|attribute]] is assigned a [[Value (computer science)|value]].{{ctn|date=June 2013}}\n\n===Usage of several equals signs===\nIn [[PHP]], the [[===|triple equals sign (<code>===</code>)]] denotes value and [[Data type|type]] equality,<ref>\n{{cite web\n| url        = http://www.php.net/manual/en/language.operators.comparison.php\n| title      = Comparison Operators\n| website    = PHP.net\n| accessdate = 19 October 2013}}</ref> meaning that not only do the two expressions evaluate to equal values, they are also of the same data type. For instance, the expression <code>0&nbsp;==&nbsp;false</code> is true, but <code>0&nbsp;===&nbsp;false</code> is not, because the number 0 is an integer value whereas false is a Boolean value.\n\n[[JavaScript]] has the same semantics for <code>===</code>, referred to as \"equality without type coercion\". However, in JavaScript the behavior of <code>==</code> cannot be described by any simple consistent rules. The expression <code>0&nbsp;==&nbsp;false</code> is true, but <code>0&nbsp;==&nbsp;undefined</code> is false, even though both sides of the <code>==</code> act the same in Boolean context. For this reason it is sometimes recommended to avoid the <code>==</code> operator in JavaScript in favor of <code>===</code>.<ref>\n{{cite web\n| last       = Crockford\n| first      = Doug\n| title      = JavaScript: The Good Parts\n| website    = [[YouTube]]\n| url        = https://www.youtube.com/watch?v=hQVTIJBZook\n| accessdate = 19 October 2013}}</ref>\n\nIn Ruby, equality under <code>==</code> requires both operands to be of identical type, e.g. <code>0&nbsp;==&nbsp;false</code> is false. The <code>===</code> operator is flexible and may be defined arbitrarily for any given type. For example, a value of type <code>Range</code> is a range of integers, such as <code>1800..1899</code>. <code>(1800..1899)&nbsp;==&nbsp;1844</code> is false, since the types are different (Range vs. Integer); however <code>(1800..1899)&nbsp;===&nbsp;1844</code> is true, since <code>===</code> on <code>Range</code> values means \"inclusion in the range\".<ref>\n{{cite book\n| title       = [[why's (poignant) Guide to Ruby]]\n| chapter     = 5.1 This One’s For the Disenfranchised\n| author      = [[why the lucky stiff]]\n| chapter-url = http://mislav.uniqpath.com/poignant-guide/book/chapter-5.html#section1\n| accessdate  = 19 October 2013}}</ref> Note that under these semantics, <code>===</code> is [[Symmetric relation|non-symmetric]]; e.g. <code>1844&nbsp;===&nbsp;(1800..1899)</code> is false, since it is interpreted to mean <code>Integer#===</code> rather than <code>Range#===</code>.<ref>\n{{cite web\n| last       = Rasmussen\n| first      = Brett\n| title      = Don't Call it Case Equality\n| date       = 30 July 2009\n| url        = http://www.pmamediagroup.com/2009/07/dont-call-it-case-equality/\n| website    = pmamediagroup.com\n| accessdate = 19 October 2013}}</ref>\n\n==Other uses==\nThe equals sign is sometimes used in Japanese as a separator between names.\n\n===Spelling===\n====Tone letter====\nThe equals sign is also used as a grammatical [[tone letter]] in the orthographies of [[Budu language|Budu]] in the [[Congo-Kinshasa]], in [[Krumen language|Krumen]], [[Mwan language|Mwan]] and [[Dan language|Dan]] in the [[Ivory Coast]].<ref>{{cite book | author = Peter G. Constable | author2 = Lorna A. Priest | url = https://www.unicode.org/L2/L2006/06259r-mod-letters.pdf | title = Proposal to Encode Additional Orthographic and Modifier Characters | date = 31 July 2006 | accessdate = 19 October 2013}}</ref><ref>{{cite book | editor = Hartell, Rhonda L. | year = 1993 | title = The Alphabets of Africa | location = Dakar | publisher = [[UNESCO]] and SIL | url = https://archive.org/details/rosettaproject_pbi_ortho-1 | accessdate = 19 October 2013}}</ref> The Unicode character used for the tone letter (U+A78A)<ref>{{cite web | title = Unicode Latin Extended-D code chart | website = Unicode.org | url = https://www.unicode.org/charts/PDF/UA720.pdf | accessdate = 19 October 2013}}</ref> is different from the mathematical symbol (U+003D).\n\n====Personal names====\n[[File:Assinatura do Santos Dumont 2.png|thumb|The signature of Santos-Dumont, showing a hyphen that looks like an equal sign.]]\n\nA possibly unique case of the equals sign of European usage in a person's name, specifically in a [[double-barreled name]], was by pioneer aviator [[Alberto Santos-Dumont]], as he is also known not only to have often used an equals sign (=) between his [[double-barreled name|two surnames]] in place of a hyphen, but also seems to have personally preferred that practice, to display equal respect for his father's French ethnicity and the Brazilian ethnicity of his mother.<ref>{{cite journal |last=Gray |first=Carroll F.|title=The 1906 Santos=Dumont No. 14bis |journal=World War I Aeroplanes |volume=No. 194 |date=November 2006 |page=4}}</ref>\n\n=== Linguistics === \nIn linguistic [[interlinear gloss]]es, an equals sign is conventionally used to mark clitic boundaries: the equals sign is placed between the [[clitic]] and the word that the clitic is attached to.<ref>{{Cite web|title = Conventions for interlinear morpheme-by-morpheme glosses| url = https://www.eva.mpg.de/lingua/resources/glossing-rules.php| access-date = 2017-11-20 }}</ref>\n\n===Chemistry===\nIn [[chemical formula]]s, the two parallel lines denoting a [[double bond]] are commonly rendered using an equals sign.\n\n===LGBT symbol===\n{{Expand section|date=July 2018}}\nIn recent years, the equals sign has been used to [[LGBT symbols|symbolize]] [[LGBT rights]]. The symbol has been used since 1995 by the [[Human Rights Campaign]], which lobbies for [[marriage equality]], and subsequently by the [[United Nations Free & Equal]], which promotes [[LGBT rights at the United Nations]].<ref>[http://www.hrc.org/hrc-story/about-our-logo \"HRC Story: Our Logo.\"] \n The Human Rights Campaign.  ''HRC.org'', Retrieved 4 December 2018.</ref>\n\n==Related symbols==\n{{See also|Unicode mathematical operators}}\n\n===Approximately equal===\n{{Main|Approximation#Unicode}}\nSymbols used to denote items that are [[approximation|approximately equal]] include the following:<ref name = \"The Unicode Consortium\">{{cite web | title = Mathematical Operators | website = Unicode.org | url = https://www.unicode.org/charts/PDF/U2200.pdf | accessdate = 19 October 2013}}</ref>\n* <span style=\"font-size: 150%;line-height:50%;\">≈</span> ([[Unicode|U]]+2248, [[LaTeX]] ''\\approx'')\n* <span style=\"font-size: 150%;line-height:50%\">≃</span> (U+2243, LaTeX ''\\simeq''), a combination of ≈ and =, also used to indicate [[Asymptotic analysis|asymptotic equality]]\n* <span style=\"font-size: 150%;line-height:50%\">≅</span> (U+2245, LaTeX ''\\cong''), another combination of ≈ and =, which is also sometimes used to indicate [[isomorphism]] or [[Congruence relation|congruence]]\n* <span style=\"font-size: 150%;line-height:50%\">∼</span> (U+223C, LaTeX ''\\sim''), which is also sometimes used to indicate [[proportionality (mathematics)|proportionality]] or [[similarity (geometry)|similarity]], being related by an [[equivalence relation]], or to indicate that a [[random variable]] is distributed according to a specific [[probability distribution]] (see also [[tilde]])\n* <span style=\"font-size: 150%;line-height:50%\">∽</span> (U+223D, LaTex ''\\backsim''), which is also used to indicate [[proportionality (mathematics)|proportionality]]\n* <span style=\"font-size: 150%;line-height:50%\">≐</span> (U+2250, LaTeX ''\\doteq''), which can also be used to represent the approach of a variable to a [[Limit (mathematics)|limit]]\n* <span style=\"font-size: 150%;line-height:50%\">≒</span> (U+2252, LaTeX ''\\fallingdotseq''), commonly used in [[Japan]], [[Taiwan]] and [[Korea]].\n* <span style=\"font-size: 150%;line-height:50%\">≓</span> (U+2253, LaTex ''\\risingdotseq'')\n\n===Not equal===\nThe symbol used to denote [[inequation]] (when items are not equal) is a [[Slash (punctuation)|slashed]] equals sign \"≠\" (U+2260; 2260,Alt+X in [[Microsoft Windows]]). In [[LaTeX]], this is done with the \"\\neq\" command.\n\nMost programming languages, limiting themselves to the [[ASCII|7-bit ASCII]] [[character set]] and [[QWERTY|typeable characters]], use <code>~=</code>, <code>'=</code>, <code>!=</code>, <code>/=</code>, <code>=/=</code>, or <code><></code> to represent their [[Boolean logic|Boolean]] [[inequality operator]].\n\n===Identity===\nThe [[triple bar]] symbol \"≡\" (U+2261, LaTeX ''\\equiv'') is often used to indicate an [[identity (mathematics)|identity]], a [[definition]] (which can also be represented by U+225D \"[[≝]]\" or U+2254 \"[[≔]]\"), or a [[congruence relation]] in [[modular arithmetic]].\n\n===Isomorphism===\nThe symbol \"≅\" is often used to indicate [[isomorphic]] algebraic structures or [[Congruence (geometry)|congruent]] geometric figures.\n\n===In logic===\nEquality of [[truth value]]s, i.e. [[bi-implication]] or [[logical equivalence]], may be denoted by various symbols including =, ~, and ⇔.\n\n===Other related symbols===\nAdditional symbols in Unicode related to the equals sign include:<ref name=\"The Unicode Consortium\"/>\n* <span style=\"font-size: 150%;line-height:50%;\">[[≌]]</span> ({{unichar|224C|ALL EQUAL TO}})\n* <span style=\"font-size: 150%;line-height:50%;\">[[≔]]</span> ({{unichar|2254|COLON EQUALS}}) (see also [[assignment (computer science)]])\n* <span style=\"font-size: 150%;line-height:50%;\">[[≕]]</span> ({{unichar|2255|EQUALS COLON}})\n* <span style=\"font-size: 150%;line-height:50%;\">[[≖]]</span> ({{unichar|2256|RING IN EQUAL TO}})\n* <span style=\"font-size: 150%;line-height:50%;\">[[≗]]</span> ({{unichar|2257|RING EQUAL TO}})\n<!-- U+2258 is mentioned already in the section \"Identity\" above. -->\n* <span style=\"font-size: 150%;line-height:50%;\">[[≙]]</span> ({{unichar|2259|ESTIMATES}})\n* <span style=\"font-size: 150%;line-height:50%;\">[[wikt:equiangular|≚]]</span> ({{unichar|225A|EQUIANGULAR TO}})\n* <span style=\"font-size: 150%;line-height:50%;\">[[≛]]</span> ({{unichar|225B|STAR EQUALS}})\n* <span style=\"font-size: 150%;line-height:50%;\">[[≜]]</span> ({{unichar|225C|DELTA EQUAL TO}})\n<!-- U+225D is mentioned already in the section \"Identity\" above. -->\n* <span style=\"font-size: 150%;line-height:50%;\">[[≞]]</span> ({{unichar|225E|MEASURED BY}})\n* <span style=\"font-size: 150%;line-height:50%;\">[[≟]]</span> ({{unichar|225F|QUESTIONED EQUAL TO}}).\n\n==Incorrect usage==\nThe equals sign is sometimes used incorrectly within a mathematical argument to connect math steps in a non-standard way, rather than to show equality (especially by early mathematics students).\n\nFor example, if one were finding the sum, step by step, of the numbers 1, 2, 3, 4, and 5, one might incorrectly write\n:1 + 2 = 3 + 3 = 6 + 4 = 10 + 5 = 15.\nStructurally, this is shorthand for\n:([(1 + 2 = 3) + 3 = 6] + 4 = 10) + 5 = 15,\nbut the notation is incorrect, because each part of the equality has a different value. If interpreted strictly as it says, it implies\n:3 = 6 = 10 = 15 = 15.\nA correct version of the argument would be\n:1 + 2 = 3, 3 + 3 = 6, 6 + 4 = 10, 10 + 5 = 15.\n\nThis difficulty results from subtly different uses of the sign in education. In early, arithmetic-focused grades, the equals sign may be ''operational''; like the equals button on an electronic calculator, it demands the result of a calculation. Starting in algebra courses, the sign takes on a ''relational'' meaning of equality between two calculations. Confusion between the two uses of the sign sometimes persists at the university level.<ref>{{cite journal | url = https://tamu.academia.edu/SencerCorlu/Papers/522225/Capraro_R._M._Capraro_M._M._Yetkiner_Z._E._Corlu_M._S._Ozel_S._Ye_S._and_Kim_H._G._2011_._An_international_perspective_between_problem_types_in_textbooks_and_students_understanding_of_relational_equality._Mediterranean_Journal_for_Research_in_Mathematics_Education_An_International_Journal_10_187-213 | title = An International Perspective between Problem Types in Textbooks and Students' understanding of relational equality | last = Capraro | first = Robert M. | last2 = Capraro | first2 = Mary Margaret | last3 = Yetkiner | first3 = Ebrar Z. | last4 = Corlu | first4 = Sencer M. | last5 = Ozel | first5 = Serkan | last6 = Ye | first6 = Sun | last7 = Kim | first7 = Hae Gyu | journal = Mediterranean Journal for Research in Mathematics Education | volume = 10 | number = 1–2 | pages = 187–213 | year = 2011 | accessdate = 19 October 2013}}</ref>\n\n==Encodings==\n* {{unichar|003d|equals sign|html=}}\nRelated:\n* {{unichar|2260|not equal to|html=}}\n\n==See also==\n* [[2 + 2 = 5]]\n* [[Double hyphen]]\n* [[Equality (mathematics)]]\n* [[Logical equality]]\n* [[Plus and minus signs]]\n\n==Notes==\n{{Reflist|33em}}\n\n==References==\n*{{Cite book | author=Cajori, Florian| authorlink = Florian Cajori| title=A History of Mathematical Notations | location=New York | publisher=Dover (reprint) | year=1993 | isbn=0-486-67766-4}}\n* Boyer, C. B.: ''A History of Mathematics'', 2nd ed. rev. by [[Uta Merzbach|Uta C. Merzbach]]. New York: Wiley, 1989 {{ISBN|0-471-09763-2}} (1991 pbk ed. {{ISBN|0-471-54397-7}})\n\n==External links==\n*[http://jeff560.tripod.com/relation.html Earliest Uses of Symbols of Relation]\n*[http://jeff560.tripod.com/witte.jpg Image of the page of ''The Whetstone of Witte'' on which the equals sign is introduced]\n*[http://www.numericana.com/answer/symbol.htm#equal Scientific Symbols, Icons, Mathematical Symbols]\n*[http://blog.plover.com/math/recorde.html Robert Recorde invents the equals sign]\n\n{{DEFAULTSORT:Equals Sign}}\n[[Category:Mathematical symbols]]\n[[Category:Welsh inventions]]\n[[Category:1557 introductions]]\n[[Category:Definition]]<!-- let a=1.23 -->\n[[Category:Assignment operations]]<!-- a=3.21 -->\n[[Category:Equivalence (mathematics)]]<!-- if a=1.23 then -->"
    },
    {
      "title": "Equivalence (measure theory)",
      "url": "https://en.wikipedia.org/wiki/Equivalence_%28measure_theory%29",
      "text": "In [[mathematics]], and specifically in [[measure theory]], '''equivalence''' is a notion of two [[Measure (mathematics)|measures]] being qualitatively similar. Specifically, the two measures agree on which events have measure zero.\n\n==Definition==\nLet <math> \\mu </math> and <math> \\nu </math> be two [[measure (mathematics)|measures]] on the measurable space <math> (X, \\mathcal A) </math>, and let \n:<math> \\mathcal N_\\mu := \\{ A \\in \\mathcal A \\mid \\mu(A)=0 \\}</math>\nbe the set of all <math> \\mu</math>-[[null set]]s; <math> \\mathcal N_\\nu </math> is similarly defined. Then the measure <math> \\nu </math> is said to be [[Absolutely continuous measure|absolutely continuous]] in reference to <math> \\mu </math> iff <math> \\mathcal N_\\nu \\supset \\mathcal N_\\mu </math>. This is denoted as <math> \\nu \\ll \\mu </math>.\n\nThe two measures are called equivalent iff <math> \\mu \\ll \\nu </math> and <math> \\nu \\ll \\mu </math>,<ref>{{cite book |last1=Klenke |first1=Achim |year=2008  |title=Probability Theory |location=Berlin |publisher=Springer |doi=10.1007/978-1-84800-048-3 |isbn=978-1-84800-047-6|page=156 }}</ref> which is denoted as <math>\\mu \\sim \\nu</math>. An equivalent definition is that two measures are equivalent if they satisfy <math> \\mathcal N_\\mu = \\mathcal N_\\nu </math>.\n\n==Examples==\n=== On the real line ===\nDefine the two measures on the real line as\n:<math> \\mu(A)= \\int_A \\mathbf 1_{[0,1]}(x) \\mathrm dx </math>\n:<math> \\nu(A)= \\int_A  x^2 \\mathbf 1_{[0,1]}(x) \\mathrm dx </math>\nfor all [[Borel set]]s <math> A </math>. Then <math> \\mu </math> and <math> \\nu </math> are equivalent, since all sets outside of <math> [0,1] </math> have <math> \\mu/\\nu</math> measure zero, and a set inside <math> [0,1] </math> is a <math> \\mu/\\nu</math> null set in respect to the [[Lebesgue measure]].\n\n=== Abstract measure space ===\nLook at some measurable space <math> (X, \\mathcal A) </math> and let <math> \\mu </math> be the [[counting measure]], so\n:<math> \\mu(A)= |A| </math>,\n\nwhere <math> |A| </math> is the [[cardinality]] of the set a. So the counting measure has only one null set, which is the [[empty set]]. Therefore,\n:<math> \\mathcal N_\\mu= \\{ \\emptyset\\} </math>.\n\nSo by the second definition, any other measure <math> \\nu </math> is equivalent to the counting measure iff it also has just the empty set as the only null set.\n\n== Supporting measures ==\nA measure <math> \\mu </math> is called a '''supporting measure''' of a measure <math> \\nu </math> if <math> \\mu </math> is [[sigma-finite|<math> \\sigma</math>-finite]] and <math> \\nu </math> is equivalent to <math> \\mu </math>.<ref>{{cite book |last1=Kallenberg |first1=Olav |author-link1=Olav Kallenberg |year=2017  |title=Random Measures, Theory and Applications|location= Switzerland |publisher=Springer |doi= 10.1007/978-3-319-41598-7|isbn=978-3-319-41596-3|page=21}}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Measure theory]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Equivalence of categories",
      "url": "https://en.wikipedia.org/wiki/Equivalence_of_categories",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Categories with invertible functors to each other, whose compositions are naturally isomorphic to each category's identity}}\n{{More footnotes|date=June 2015}}\nIn [[category theory]], an abstract branch of [[mathematics]], an '''equivalence of categories''' is a relation between two [[Category (mathematics)|categories]] that establishes that these categories are \"essentially the same\". There are numerous examples of categorical equivalences from many areas of mathematics. Establishing an equivalence involves demonstrating strong similarities between the mathematical structures concerned. In some cases, these structures may appear to be unrelated at a superficial or intuitive level, making the notion fairly powerful: it creates the opportunity to \"translate\" theorems between different kinds of mathematical structures, knowing that the essential meaning of those theorems is preserved under the translation.\n\nIf a category is equivalent to the [[dual (category theory)|opposite (or dual)]] of another category then one speaks of\na '''duality of categories''', and says that the two categories are '''dually equivalent'''.\n\nAn equivalence of categories consists of a [[functor]] between the involved categories, which is required to have an \"inverse\" functor. However, in contrast to the situation common for [[isomorphism]]s in an algebraic setting, the composition of the functor and its \"inverse\" is not necessarily the identity mapping. Instead it is sufficient that each object be ''[[natural transformation|naturally isomorphic]]'' to its image under this composition. Thus one may describe the functors as being \"inverse up to isomorphism\". There is indeed a concept of [[isomorphism of categories]] where a strict form of inverse functor is required, but this is of much less practical use than the ''equivalence'' concept.\n\n==Definition==\nFormally, given two categories ''C'' and ''D'', an ''equivalence of categories'' consists of a functor ''F'' : ''C'' → ''D'', a functor ''G'' : ''D'' → ''C'', and two natural isomorphisms ε: ''FG''→'''I'''<sub>''D''</sub> and η : '''I'''<sub>''C''</sub>→''GF''. Here ''FG'': ''D''→''D'' and ''GF'': ''C''→''C'', denote the respective compositions of ''F'' and ''G'', and '''I'''<sub>''C''</sub>: ''C''→''C'' and '''I'''<sub>''D''</sub>: ''D''→''D'' denote the ''identity functors'' on ''C'' and ''D'', assigning each object and morphism to itself. If ''F'' and ''G'' are contravariant functors one speaks of a ''duality of categories'' instead.\n\nOne often does not specify all the above data. For instance, we say that the categories ''C'' and ''D'' are ''equivalent'' (respectively ''dually equivalent'') if there exists an equivalence (respectively duality) between them. Furthermore, we say that ''F'' \"is\" an equivalence of categories if an inverse functor ''G'' and natural isomorphisms as above exist. Note however that knowledge of ''F'' is usually not enough to reconstruct ''G'' and the natural isomorphisms: there may be many choices (see example below).\n\n==Equivalent characterizations==\nA functor ''F'' : ''C'' → ''D'' yields an equivalence of categories if and only if it is simultaneously:\n* [[full functor|full]], i.e. for any two objects ''c''<sub>1</sub> and ''c''<sub>2</sub> of ''C'', the map Hom<sub>''C''</sub>(''c''<sub>1</sub>,''c''<sub>2</sub>) → Hom<sub>''D''</sub>(''Fc''<sub>1</sub>,''Fc''<sub>2</sub>) induced by ''F'' is [[surjective]];\n* [[faithful functor|faithful]], i.e. for any two objects ''c''<sub>1</sub> and ''c''<sub>2</sub> of ''C'', the map Hom<sub>''C''</sub>(''c''<sub>1</sub>,''c''<sub>2</sub>) → Hom<sub>''D''</sub>(''Fc''<sub>1</sub>,''Fc''<sub>2</sub>) induced by ''F'' is [[injective]]; and\n* [[essentially surjective functor|essentially surjective (dense)]], i.e. each object ''d'' in ''D'' is isomorphic to an object of the form ''Fc'', for ''c'' in ''C''.\nThis is a quite useful and commonly applied criterion, because one does not have to explicitly construct the \"inverse\" ''G'' and the natural isomorphisms between ''FG'', ''GF'' and the identity functors. On the other hand, though the above properties guarantee the ''existence'' of a categorical equivalence (given a sufficiently strong version of the [[axiom of choice]] in the underlying set theory), the missing data is not completely specified, and often there are many choices. It is a good idea to specify the missing constructions explicitly whenever possible.\nDue to this circumstance, a functor with these properties is sometimes called a '''weak equivalence of categories'''. (Unfortunately this conflicts with terminology from [[homotopy type theory]].)\n\nThere is also a close relation to the concept of [[adjoint functors]]. The following statements are equivalent for functors ''F'' : ''C'' → ''D'' and ''G'' : ''D'' → ''C'':\n* There are natural isomorphisms from ''FG'' to '''I'''<sub>''D''</sub> and '''I'''<sub>''C''</sub> to ''GF''.\n* ''F'' is a left adjoint of ''G'' and both functors are full and faithful.\n* ''G'' is a right adjoint of ''F'' and both functors are full and faithful.\nOne may therefore view an adjointness relation between two functors as a \"very weak form of equivalence\". Assuming that the natural transformations for the adjunctions are given, all of these formulations allow for an explicit construction of the necessary data, and no choice principles are needed. The key property that one has to prove here is that the ''counit'' of an adjunction is an isomorphism if and only if the right adjoint is a full and faithful functor.\n\n==Examples==\n* Consider the category <math>C</math> having a single object <math>c</math> and a single morphism <math>1_{c}</math>, and the category <math>D</math> with two objects <math>d_{1}</math>, <math>d_{2}</math> and four morphisms: two identity morphisms <math>1_{d_{1}}</math>, <math>1_{d_{2}}</math> and two isomorphisms <math>\\alpha \\colon d_{1} \\to d_{2}</math> and <math>\\beta \\colon d_{2} \\to d_{1}</math>.  The categories <math>C</math> and <math>D</math> are equivalent; we can (for example) have <math>F</math> map <math>c</math> to <math>d_{1}</math> and <math>G</math> map both objects of <math>D</math> to <math>c</math> and all morphisms to <math>1_{c}</math>.\n* By contrast, the category <math>C</math> with a single object and a single morphism is ''not'' equivalent to the category <math>E</math> with two objects and only two identity morphisms as the two objects therein are ''not'' isomorphic.\n* Consider a category <math>C</math> with one object <math>c</math>, and two morphisms <math>1_{c}, f \\colon c \\to c</math>.  Let <math>1_{c}</math> be the identity morphism on <math>c</math> and set <math>f \\circ f = 1</math>.  Of course, <math>C</math> is equivalent to itself, which can be shown by taking <math>1_{c}</math> in place of the required natural isomorphisms between the functor <math>\\mathbf{I}_{C}</math> and itself.  However, it is also true that <math>f</math> yields a natural isomorphism from <math>\\mathbf{I}_{C}</math> to itself.  Hence, given the information that the identity functors form an equivalence of categories, in this example one still can choose between two natural isomorphisms for each direction.\n* The category of sets and [[partial function]]s is equivalent to but not isomorphic with the category of [[pointed set]]s and point-preserving maps.<ref name=\"KoslowskiMelton2001\">{{cite book|editor=Jürgen Koslowski and Austin Melton|title=Categorical Perspectives|year=2001|publisher=Springer Science & Business Media|isbn=978-0-8176-4186-3|page=10|author=Lutz Schröder|chapter=Categories: a free tour}}</ref>\n* Consider the category <math>C</math> of finite-[[dimension of a vector space|dimensional]] [[real number|real]] [[vector space]]s, and the category <math>D = \\mathrm{Mat}(\\mathbb{R})</math> of all real [[matrix (mathematics)|matrices]] (the latter category is explained in the article on [[additive category|additive categories]]).  Then <math>C</math> and <math>D</math> are equivalent: The functor <math>G \\colon D \\to C</math> which maps the object <math>A_{n}</math> of <math>D</math> to the vector space <math>\\mathbb{R}^{n}</math> and the matrices in <math>D</math> to the corresponding linear maps is full, faithful and essentially surjective.\n* One of the central themes of [[algebraic geometry]] is the duality of the category of [[affine scheme]]s and the category of [[commutative ring]]s.  The functor <math>G</math> associates to every commutative ring its [[spectrum of a ring|spectrum]], the scheme defined by the [[prime ideal]]s of the ring.  Its adjoint <math>F</math> associates to every affine scheme its ring of global sections.\n* In [[functional analysis]] the category of commutative [[C*-algebra]]s with identity is contravariantly equivalent to the category of [[compact space|compact]] [[Hausdorff space]]s.  Under this duality, every compact Hausdorff space <math>X</math> is associated with the algebra of continuous complex-valued functions on <math>X</math>, and every commutative C*-algebra is associated with the space of its [[maximal ideal]]s.  This is the [[Gelfand representation]].\n* In [[lattice theory]], there are a number of dualities, based on representation theorems that connect certain classes of lattices to classes of [[topology|topological spaces]].  Probably the most well-known theorem of this kind is ''[[Stone's representation theorem for Boolean algebras]]'', which is a special instance within the general scheme of ''[[Stone duality]]''.  Each [[Boolean algebra (structure)|Boolean algebra]] <math>B</math> is mapped to a specific topology on the set of [[lattice theory|ultrafilters]] of <math>B</math>.  Conversely, for any topology the clopen (i.e. closed and open) subsets yield a Boolean algebra.  One obtains a duality between the category of Boolean algebras (with their homomorphisms) and [[Stone space]]s (with continuous mappings). Another case of Stone duality is [[Birkhoff's representation theorem]] stating a duality between finite partial orders and finite distributive lattices.\n* In [[pointless topology]] the category of spatial locales is known to be equivalent to the dual of the category of sober spaces.\n* For two [[Ring (mathematics)|rings]] ''R'' and ''S'', the [[product category]] ''R''-'''Mod'''×''S''-'''Mod''' is equivalent to (''R''×''S'')-'''Mod'''.{{Citation needed|date=May 2015}}\n* Any category is equivalent to its [[skeleton (category theory)|skeleton]].\n\n==Properties==\nAs a rule of thumb, an equivalence of categories preserves all \"categorical\" concepts and properties. If ''F'' : ''C'' → ''D'' is an equivalence, then the following statements are all true:\n* the object ''c'' of ''C'' is an [[initial object]] (or [[terminal object]], or [[zero object]]), [[if and only if]] ''Fc'' is an [[initial object]] (or [[terminal object]], or [[zero object]]) of ''D''\n* the morphism α in ''C'' is a [[monomorphism]] (or [[epimorphism]], or [[isomorphism]]), if and only if ''Fα'' is a monomorphism (or epimorphism, or isomorphism) in ''D''.\n* the functor ''H'' : ''I'' → ''C'' has [[limit (category theory)|limit]] (or colimit) ''l'' if and only if the functor ''FH'' : ''I'' → ''D'' has limit (or colimit) ''Fl''. This can be applied to [[equaliser (mathematics)|equalizers]], [[product (category theory)|product]]s and [[coproduct]]s among others. Applying it to [[kernel (category theory)|kernel]]s and [[cokernel]]s, we see that the equivalence ''F'' is an [[Regular category#Exact sequences and regular functors|exact functor]].\n* ''C'' is a [[cartesian closed category]] (or a [[topos]]) if and only if ''D'' is cartesian closed (or a topos).\n\nDualities \"turn all concepts around\": they turn initial objects into terminal objects, monomorphisms into epimorphisms, kernels into cokernels, limits into colimits etc.\n\nIf ''F'' : ''C'' → ''D'' is an equivalence of categories, and ''G''<sub>1</sub> and ''G''<sub>2</sub> are two inverses of ''F'', then ''G''<sub>1</sub> and ''G''<sub>2</sub> are naturally isomorphic.\n\nIf ''F'' : ''C'' → ''D'' is an equivalence of categories, and if ''C'' is a [[preadditive category]] (or [[additive category]], or [[abelian category]]), then ''D'' may be turned into a preadditive category (or additive category, or abelian category) in such a way that ''F'' becomes an [[additive functor]]. On the other hand, any equivalence between additive categories is necessarily additive. (Note that the latter statement is not true for equivalences between preadditive categories.)\n\nAn '''auto-equivalence''' of a category ''C'' is an equivalence ''F'' : ''C'' → ''C''. The auto-equivalences of ''C'' form a [[group (mathematics)|group]] under composition if we consider two auto-equivalences that are naturally isomorphic to be identical. This group captures the essential \"symmetries\" of ''C''. (One caveat: if ''C'' is not a small category, then the auto-equivalences of ''C'' may form a proper [[class (set theory)|class]] rather than a [[Set (mathematics)|set]].)\n\n== See also ==\n* [[Equivalent definitions of mathematical structures]]\n\n==References==\n{{Reflist}}\n* {{nlab|id=equivalence+of+categories|title=equivalence of categories}}\n*{{Springer|id=Equivalence_of_categories|title=Equivalence of categories}}\n*{{cite book|last=Mac Lane|first=Saunders|title=Categories for the working mathematician|year=1998|publisher=Springer|location=New York|isbn=0-387-98403-8|pages=xii+314}}\n\n{{DEFAULTSORT:Equivalence Of Categories}}\n[[Category:Adjoint functors]]\n[[Category:Category theory]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Equivalence of metrics",
      "url": "https://en.wikipedia.org/wiki/Equivalence_of_metrics",
      "text": "{{Orphan|date=April 2016}}\n\nIn the study of [[metric spaces]] in [[mathematics]], there are various notions of two [[metric (mathematics)|metrics]] on the same underlying space being \"the same\", or '''equivalent'''.\n\nIn the following, <math>X</math> will denote a non-[[empty set]] and <math>d_1</math> and <math>d_2</math> will denote two metrics on <math>X</math>.\n\n==Topological equivalence==\nThe two metrics <math>d_1</math> and <math>d_2</math> are said to be '''topologically equivalent''' if they generate the same [[topology]] on <math>X</math>. The adjective \"topological\" is often dropped.<ref>Bishop and Goldberg, p. 10.</ref> There are multiple ways of expressing this condition:\n* a subset <math>A \\subseteq X</math> is <math>d_1</math>-[[open set|open]] [[if and only if]] it is <math>d_2</math>-open;\n* the [[open ball]]s \"nest\": for any point <math>x \\in X</math> and any radius <math>r > 0</math>, there exist radii <math>r', r'' > 0</math> such that\n:: <math>B_{r'} (x; d_1) \\subseteq B_r (x; d_2) \\text{ and } B_{r''} (x; d_2) \\subseteq B_r (x; d_1).</math>\n* the [[identity function]] <math>I : X \\to X</math> is both <math>(d_1, d_2)</math>-[[continuous function|continuous]] and <math>(d_2, d_1)</math>-continuous.\n\nThe following are sufficient but not necessary conditions for topological equivalence:\n* there exists a strictly increasing, continuous, and [[subadditive]] <math>f:\\mathbb{R}_{+} \\to \\mathbb{R}</math> such that <math>d_2 = f \\circ d_1 </math>.<ref>Ok, p. 127, footnote 12.</ref>\n* for each <math>x \\in X</math>, there exist positive constants <math>\\alpha</math> and <math>\\beta</math> such that, for every point <math>y \\in X</math>,\n:: <math>\\alpha d_1 (x, y) \\leq d_2 (x, y) \\leq \\beta d_1 (x, y).</math>\n\n==Strong equivalence==\nTwo metrics <math>d_1</math> and <math>d_2</math> are '''strongly equivalent''' if and only if there exist positive constants <math>\\alpha</math> and <math>\\beta</math> such that, for every <math>x,y\\in X</math>,\n:<math>\\alpha d_1(x,y) \\leq d_2(x,y) \\leq \\beta d_1 (x, y).</math>\nIn contrast to the sufficient condition for topological equivalence listed above, strong equivalence requires that there is a single set of constants that holds for every pair of points in <math>X</math>, rather than potentially different constants associated with each point of <math>X</math>.\n\nStrong equivalence of two metrics implies topological equivalence, but not vice versa. An intuitive reason why topological equivalence does not imply strong equivalence is that [[Bounded set#Metric space|bounded sets]] under one metric are also bounded under a strongly equivalent metric, but not necessarily under a topologically equivalent metric.\n\nWhen the two metrics <math>d_1,d_2</math> are those induced by norms <math>\\|\\cdot \\|_A, \\|\\cdot\\|_B</math> respectively, then strong equivalence is equivalent to the condition that, for all <math>x \\in X</math>,\n\n: <math>\\alpha\\|x\\|_A \\leq \\|x\\|_B \\leq \\beta\\|x\\|_A</math>\n\nIn finite dimensional spaces, all metrics induced by the [[p-norm]], including the [[euclidean metric]], the [[taxicab metric]], and the [[Chebyshev distance]], are strongly equivalent.<ref>Ok, p. 138.</ref>\n\nEven if two metrics are strongly equivalent, not all properties of the respective metric spaces are preserved. For instance, a function from the space to itself might be a [[contraction mapping]] under one metric, but not necessarily under a strongly equivalent one.<ref>Ok, p. 175.</ref>\n\n==Properties preserved by equivalence==\n* The [[continuous function|continuity]] of a function is preserved if either the domain or range is remetrized by an equivalent metric, but [[uniform continuity]] is preserved only by strongly equivalent metrics.<ref>Ok, p. 209.</ref>\n* The [[differentiability]] of a function <math>f:U\\to V</math>, for <math>V</math> a normed space and <math>U</math> a subset of a normed space, is preserved if either the domain or range is renormed by a strongly equivalent norm.<ref>Cartan, p. 27.</ref>\n* A metric that is strongly equivalent to a [[complete metric]] is also complete; the same is not true of equivalent metrics because homeomorphisms do not preserve completeness. For example, since <math>(0,1)</math> and <math>\\mathbb R</math> are homeomorphic, the homeomorphism induces a metric on <math>(0,1)</math> which is complete because <math>\\mathbb R</math> is, and generates the same topology as the usual one, yet <math>(0,1)</math> with the usual metric is not complete, because the sequence <math>(2^{-n})_{n\\in\\mathbb N}</math> is Cauchy but not convergent. (It is not Cauchy in the induced metric.)\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin}}\n* {{cite book\n  | author = Richard L. Bishop\n  | author-link = Richard L. Bishop\n  \n  | author2 = Samuel I. Goldberg\n  | title = Tensor analysis on manifolds\n  | year = 1980\n  | publisher = Dover Publications\n  | url = https://books.google.com/books?id=LAuN5-og4jwC\n  }}\n* {{cite book\n  | author = Efe Ok\n  | title = Real analysis with economics applications\n  | year = 2007\n  | publisher = Princeton University Press\n  | isbn = 0-691-11768-3\n  }}\n* {{cite book\n  | author = Henri Cartan\n  | author-link = Henri Cartan\n  | title = Differential Calculus\n  | year = 1971\n  | publisher = Kershaw Publishing Company LTD.\n  | isbn = 0-395-12033-0\n  }}\n{{refend}}\n\n[[Category:Metric geometry]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Equivalence test",
      "url": "https://en.wikipedia.org/wiki/Equivalence_test",
      "text": "Equivalence tests are a variation of [[hypothesis test]]s used to draw statistical inferences from observed data. In equivalence tests, the [[null hypothesis]] is defined as an effect large enough to be deemed interesting, specified by an equivalence bound. The alternative hypothesis is any effect that is less extreme than said equivalence bound. The observed data is statistically compared against the equivalence bounds. If the statistical test indicates the observed data is surprising, assuming that true effects at least as extreme as the equivalence bounds, a [[Neyman–Pearson lemma|Neyman-Pearson]] approach to statistical inferences can be used to reject effect sizes larger than the equivalence bounds with a pre-specified [[Type 1 error]] rate.\n\nEquivalence testing originates from the field of [[pharmacokinetics]].<ref>{{Cite journal|last=Hauck|first=Walter W.|last2=Anderson|first2=Sharon|date=1984-02-01|title=A new statistical procedure for testing equivalence in two-group comparative bioavailability trials|journal=Journal of Pharmacokinetics and Biopharmaceutics|language=en|volume=12|issue=1|pages=83–91|doi=10.1007/BF01063612|pmid=6747820|issn=0090-466X}}</ref> One application is to show that a new drug that is cheaper than available alternatives works just as well as an existing drug.  In essence, equivalence tests consist of calculating a [[confidence interval]] around an observed [[effect size]], and rejecting effects more extreme than the equivalence bound when the confidence interval does not overlap with the equivalence bound. In two-sided tests an upper and lower equivalence bound is specified. In non-inferiority trials, where the goal is to test the hypothesis that a new treatment is not worse than existing treatments, only a lower equivalence bound is pre-specified.  [[File:Equivalence Test.png|thumb|Mean differences (black squares) and 90% confidence intervals (horizontal lines) with equivalence bounds ΔL = -0.5 and ΔU= 0.5 for four combinations of test results that are statistically equivalent or not and statistically different from zero or not. Pattern A is statistically equivalent, pattern B is statistically different from 0, pattern C is practically insignificant, and pattern D is inconclusive (neither statistically different from 0 nor equivalent).]]Equivalence tests can be performed in addition to null-hypothesis significance tests.<ref>{{Cite journal|last=Rogers|first=James L.|last2=Howard|first2=Kenneth I.|last3=Vessey|first3=John T.|title=Using significance tests to evaluate equivalence between two experimental groups.|journal=Psychological Bulletin|volume=113|issue=3|pages=553–565|doi=10.1037/0033-2909.113.3.553|year=1993}}</ref> This might prevent common misinterpretations of p-values larger than the alpha level as support for the absence of a true effect. Furthermore, equivalence tests can identify effects that are statistically significant but practically insignificant, whenever effects are statistically different from zero, but also statistically smaller than any effect size deemed worthwhile (see first Figure).<ref>{{Cite journal|last=Lakens|first=Daniël|date=2017-05-05|title=Equivalence Tests|journal=Social Psychological and Personality Science|volume=8|issue=4|pages=355–362|language=en|doi=10.1177/1948550617697177|pmid=28736600}}</ref>\n\n== TOST procedure ==\n\nA very simple equivalence testing approach is the ‘two-one-sided t-tests’ (TOST) procedure.<ref>{{Cite journal|last=Schuirmann|first=Donald J.|date=1987-12-01|title=A comparison of the Two One-Sided Tests Procedure and the Power Approach for assessing the equivalence of average bioavailability|journal=Journal of Pharmacokinetics and Biopharmaceutics|language=en|volume=15|issue=6|pages=657–680|doi=10.1007/BF01068419|issn=0090-466X}}</ref> In the TOST procedure an upper (Δ<sub>U</sub>) and lower (–Δ<sub>L</sub>) equivalence bound is specified based on the smallest effect size of interest (e.g., a positive or negative difference of d = 0.3). Two composite null hypotheses are tested: H<sub>01</sub>: Δ ≤ –Δ<sub>L</sub> and H<sub>02</sub>: Δ ≥ Δ<sub>U</sub>. When both these one-sided tests can be statistically rejected, we can conclude that –Δ<sub>L</sub> < Δ < Δ<sub>U</sub>, or that the observed effect falls within the equivalence bounds and is statistically smaller than any effect deemed worthwhile, and considered practically equivalent.<ref>{{Cite journal|last=Seaman|first=Michael A.|last2=Serlin|first2=Ronald C.|title=Equivalence confidence intervals for two-group comparisons of means.|journal=Psychological Methods|volume=3|issue=4|pages=403–411|doi=10.1037/1082-989x.3.4.403|year=1998}}</ref> Alternatives to the TOST procedure have been developed as well.<ref>{{Cite book|title=Testing statistical hypotheses of equivalence and noninferiority|last=Wellek|first=Stefan|publisher=Chapman and Hall/CRC|year=2010|isbn=978-1439808184|location=|pages=}}</ref> A recent modification to TOST makes the approach feasible in cases of repeated measures and assessing multiple variables. <ref>{{Cite journal|last=Rose|first=Evangeline M.|last2=Mathew|first2=Thomas|last3=Coss|first3=Derek A.|last4=Lohr|first4=Bernard|last5=Omland|first5=Kevin E.|date=2018|title=A new statistical method to test equivalence: an application in male and female eastern bluebird song|journal=Animal Behaviour|volume=145|pages=77–85|doi=10.1016/j.anbehav.2018.09.004|issn=0003-3472}}</ref>\n\n== Comparison between t-test and equivalence test ==\nThe equivalence test can, for comparison purposes, be ''induced'' from the [[Student's t-test|t-test]].<ref name=\"siebert2019\">{{Cite journal|last=Siebert|first=Michael|last2=Ellenberger|first2=David|date=2019-04-10|title=Validation of automatic passenger counting: introducing the t-test-induced equivalence test|journal=Transportation|language=en|doi=10.1007/s11116-019-09991-9|issn=0049-4488}}</ref> Considering a t-test at the significance level α<sub>t-test</sub> achieving a [[Power (statistics)|power]] of 1-β<sub>t-test</sub> for a relevant effect size d<sub>r</sub>, both tests lead to the same inference whenever parameters Δ=d<sub>r</sub> as well as α<sub>equiv.-test</sub>=β<sub>t-test</sub> and β<sub>equiv.-test</sub>=α<sub>t-test</sub> coincide, i.e. the error types (type I and type II) are interchanged between the t-test and the equivalence test. To achieve this for the t-test, either the sample size calculation needs to be carried out correctly, or by adjusting the t-test significance level α<sub>t-test</sub>, referred to as the so-called ''revised t-test''.<ref name=\"siebert2019\" /> Both approaches have difficulties in practice, since sample size planning relies on unverifiable assumptions of the standard deviation <math display=\"inline\">\\sigma</math>, and the revised t-test yields numerical problems.<ref name=\"siebert2019\" /> Preserving the test behaviour, those limitations can be removed by using an equivalence test.\n\nThe second Figure allows a visual comparison of the equivalence test and the t-test when the sample size calculation is affected by differences between the a priori standard deviation <math display=\"inline\">\\sigma</math> and the sample's standard deviation <math display=\"inline\">\\widehat{\\sigma}</math>, which is a common problem. Using an equivalence test instead of a t-test additionally ensures that α<sub>equiv.-test</sub> is bounded, which the t-test does not do in case that <math display=\"inline\">\\widehat{\\sigma} > \\sigma</math> with the type II error growing arbitrary large. On the other hand, having <math display=\"inline\">\\widehat{\\sigma} < \\sigma</math> results in the t-test being stricter than the d<sub>r</sub> specified in the planning, which may randomly penalize the sample source (e.g. a device manufacturer). This makes the equivalence test safer to use.\n\n[[File:T-test vs equivalence test.png|thumb|Chances to pass the t-test (a) resp. the equivalence test (b) depending on the actual error 𝜇. For more details, see<ref name=\"siebert2019\" />]]\n\n==Further reading==\n* {{cite journal|last=Walker|first=Esteban|last2=Nowacki|first2=Amy S.|title=Understanding Equivalence and Noninferiority Testing|journal=Journal of  General Internal Medicine|volume=26|pages=192–6|number=2|date=February 2011|pmc=3019319|pmid=20857339|doi=10.1007/s11606-010-1513-8|accessdate=}}\n\n==References==\n<references />\n\n[[Category:Statistical hypothesis testing]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Equivalent definitions of mathematical structures",
      "url": "https://en.wikipedia.org/wiki/Equivalent_definitions_of_mathematical_structures",
      "text": "In mathematics, '''equivalent definitions''' are used in two somewhat different ways. First, within a particular mathematical theory (for example, [[Euclidean geometry]]), a notion (for example, [[Ellipse#Definition|ellipse]] or [[Minimal surface#Definitions|minimal surface]]) may have more than one definition. These definitions are equivalent in the context of a given [[mathematical structure]] ([[Euclidean space]], in this case). Second, a mathematical structure may have more than one definition (for example, [[topological space]] has at least [[Characterizations of the category of topological spaces|seven definitions]]; [[ordered field]] has at least [[Ordered field#Definition|two definitions]]).\n\nIn the former case, equivalence of two definitions means that a mathematical object (for example, geometric body) satisfies one definition [[if and only if]] it satisfies the other definition.\n\nIn the latter case, the meaning of equivalence (between two definitions of a structure) is more complicated, since a structure is more abstract than an object. Many different objects may implement the same structure.\n\n==Isomorphic implementations==\n\n[[Natural number#Formal definitions|Natural numbers]] may be implemented as 0 = {{mset|&nbsp;}}, 1 = {{mset|0}} = {{mset|{{mset|&nbsp;}}}}, 2 = {{mset|0, 1}} = {{mset|{{mset|&nbsp;}}, {{mset|{{mset|&nbsp;}}}}}}, 3 = {{mset|0, 1, 2}} = {{mset|{{mset|&nbsp;}}, {{mset|{{mset|&nbsp;}}}}, {{mset|{{mset|&nbsp;}}, {{mset|{{mset|&nbsp;}}}}}}}} and so on; or alternatively as 0 = {{mset|&nbsp;}}, 1 = {{mset|0}} ={{mset|{{mset|&nbsp;}}}}, 2 = {{mset|1}} = {{mset|{{mset|{{mset|&nbsp;}}}}}} and so on. These are two different but [[Isomorphism|isomorphic]] implementations of natural numbers in set theory.\nThey are isomorphic as models of [[Peano axioms#Models|Peano axioms]], that is, triples (''N'',0,''S'') where ''N'' is a set, 0 an element of ''N'', and ''S'' (called the [[successor function]]) a map of ''N'' to itself (satisfying appropriate conditions). In the first implementation ''S''(''n'') = ''n'' ∪ {{mset|''n''}}; in the second implementation ''S''(''n'') = {{mset|''n''}}. As emphasized in [[Benacerraf's identification problem]], the two implementations differ in their answer to the question whether 0 ∈ 2; however, this is not a legitimate question about natural numbers (since the relation ∈ is not stipulated by the relevant signature(s), see the next section).<ref group=\"details\">Technically, \"0 ∈ 2\" is an example of a non-transportable relation, see {{harvnb|Bourbaki|1968|loc=Sect.IV.1.3}}, {{harvnb|Marshall|Chuaqui|1991}}.</ref> Similarly, different but isomorphic implementations are used for [[Complex number#Formal construction|complex numbers]].\n\n==Deduced structures and cryptomorphisms==\n\nThe successor function ''S'' on natural numbers leads to [[Peano axioms#Arithmetic|arithmetic operations]], addition and multiplication, and the total order, thus endowing ''N'' with an [[Peano axioms#Equivalent axiomatizations|ordered semiring]] structure. This is an example of a deduced structure. The ordered semiring structure (''N'', +, ·, ≤) is deduced from the Peano structure (''N'', 0, ''S'') by the following procedure:\n''n'' + 0 = ''n'', &nbsp; ''m'' + S (''n'') = S (''m'' + ''n''), &nbsp; ''m'' · 0 = 0, &nbsp; ''m'' · S (''n'') = ''m'' + (''m'' · ''n''), and ''m'' ≤ ''n'' if and only if there exists ''k'' ∈ ''N'' such that ''m'' + ''k'' = ''n''. And conversely, the Peano structure is deduced from the ordered semiring structure as follows: ''S'' (''n'') = ''n'' + 1, and 0 is defined by 0 + 0 = 0. It means that the two structures on ''N'' are equivalent by means of the two procedures.\n\nThe two isomorphic implementations of natural numbers, mentioned in the previous section, are isomorphic as triples (''N'',0,''S''), that is, structures of the same [[Signature (logic)|signature]] (0,''S'') consisting of a constant symbol 0 and a unary function ''S''. An ordered semiring structure (''N'', +, ·, ≤) has another signature (+, ·, ≤) consisting of two binary functions and one binary relation. The notion of isomorphism does not apply to structures of different signatures. In particular, a Peano structure cannot be isomorphic to an ordered semiring. However, an ordered semiring deduced from a Peano structure may be isomorphic to another ordered semiring. Such relation between structures of different signatures is sometimes called a [[cryptomorphism]].\n\n==Ambient frameworks==\n\nA structure may be implemented within a set theory [[Zermelo–Fraenkel set theory|ZFC]], or another set theory such as [[Von Neumann–Bernays–Gödel set theory|NBG]], [[New Foundations|NFU]], [[Elementary theory of the category of sets|ETCS]].<ref>About ETCS see [[Type theory#Mathematical foundations]]</ref> Alternatively, a structure may be treated in the framework of [[first-order logic]], [[second-order logic]], [[higher-order logic]], a [[type theory]], [[homotopy type theory]] etc.<ref group=\"details\">A reasonable choice of an ambient framework should not alter basic properties of a structure, but can alter provability of finer properties. For example, some theorems about the natural numbers are provable in set theory (and some other strong systems) but not provable in first-order logic; see [[Paris–Harrington theorem]] and [[Goodstein's theorem]]. The same applies to definability; see for example [[Tarski's undefinability theorem]].</ref>\n\n==Structures according to Bourbaki==\n\n:\"Mathematics [...] cannot be explained completely by a single concept such as the mathematical structure. Nevertheless, Bourbaki's structuralist approach is the best that we have.\" ({{harvnb|Pudlák|2013|loc=page 3}})\n\n:\"Evident as the notion of mathematical structure may seem these days, it was at least not made explicit until the middle of the 20th century. Then it was the influence of the Bourbaki-project and then later the development of category theory which made the notion explicit\" ([http://ncatlab.org/nlab/show/structure#related_entries nLab]).\n\nAccording to [[Nicolas Bourbaki|Bourbaki]], the scale of sets on a given set ''X'' consists of all sets arising from ''X'' by taking [[Cartesian product]]s and [[power set]]s, in any combination, a finite number of times. Examples: ''X''; ''X'' × ''X''; ''P''(''X''); ''P''(''P''(''X'' × ''X'') × ''X'' × ''P''(''P''(''X''))) × ''X''. (Here ''A'' × ''B'' is the product of ''A'' and ''B'', and ''P''(''A'') is the powerset of ''A''.) In particular, a pair (0,''S'') consisting of an element 0 ∈ ''N'' and a unary function ''S'' : ''N'' → ''N'' belongs to ''N'' × ''P''(''N'' × ''N'') (since [[Function (mathematics)#Definition|a function is a subset of the Cartesian product]]). A triple (+, ·, ≤) consisting of two binary functions ''N'' × ''N'' → ''N'' and one binary relation on ''N'' belongs to ''P''(''N'' × ''N'' × ''N'')  × ''P''(''N'' × ''N'' × ''N'')  × ''P''(''N'' × ''N''). Similarly, every algebraic structure on a set belongs to the corresponding set in the scale of sets on ''X''.\n\nNon-algebraic structures on a set ''X'' often involve sets of subsets of ''X'' (that is, subsets of ''P''(''X''), in other words, elements of ''P''(''P''(''X''))). For example, the structure of a [[topological space]], called a topology on ''X'', treated as [[Topological space#Open sets definition|the set of \"open\" sets]]; or the structure of a measurable space, treated as the [[Sigma-algebra|σ-algebra]] of \"measurable\" sets; both are elements of ''P''(''P''(''X'')). These are second-order structures.<ref>{{harvnb|Pudlák|2013|loc=pages 10–11}}</ref>\n\nMore complicated non-algebraic structures combine an algebraic component and a non-algebraic component. For example, the structure of a [[topological group]] consists of a topology and the structure of a group. Thus it belongs to the product of ''P''(''P''(''X'')) and another (\"algebraic\") set in the scale; this product is again a set in the scale.\n\n===Transport of structures; isomorphism===\n\n{{main article|Transport of structure}}\n\nGiven two sets ''X'', ''Y'' and a [[bijection]] ''f'' : ''X'' → ''Y'', one constructs the corresponding bijections between scale sets. Namely, the bijection ''X'' × ''X'' → ''Y'' × ''Y'' sends (''x''<sub>1</sub>,''x''<sub>2</sub>) to (''f''(''x''<sub>1</sub>),''f''(''x''<sub>2</sub>)); the bijection ''P''(''X'') → ''P''(''Y'') sends a subset ''A'' of ''X'' into its [[Image (mathematics)|image]] ''f''(''A'') in ''Y''; and so on, recursively: a scale set being either product of scale sets or power set of a scale set, one of the two constructions applies.\n\nLet (''X'',''U'') and (''Y'',''V'') be two structures of the same signature. Then ''U'' belongs to a scale set ''S''<sub>''X''</sub>, and ''V'' belongs to the corresponding scale set ''S''<sub>''Y''</sub>. Using the bijection ''F'' : ''S''<sub>''X''</sub> → ''S''<sub>''Y''</sub> constructed from a bijection ''f'' : ''X'' → ''Y'', one defines:\n: ''f'' is an ''isomorphism'' between (''X'',''U'') and (''Y'',''V'') if ''F''(''U'') = ''V''.\nThis general notion of isomorphism generalizes many less general notions listed below.\n* For algebraic structures: [[Homomorphism#Specific kinds of homomorphisms|isomorphism is a bijective homomorphism]].\n* In particular, for [[vector space]]s: [[Linear operator#Algebraic classifications of linear transformations|linear bijection]].\n* For [[partially ordered set]]s: [[order isomorphism]].\n* For [[Graph (discrete mathematics)|graphs]]: [[graph isomorphism]].\n* More generally, for sets endowed with a binary relation: [[Isomorphism#A relation-preserving isomorphism|relation-preserving isomorphism]].\n* For topological spaces: [[Homeomorphism|homeomorphism or topological isomorphism or bi continuous function]].\n* For [[uniform space]]s: [[uniform isomorphism]].\n* For [[metric space]]s: [[Metric space#Notions of metric space equivalence|bijective isometry]].\n* For topological groups: [[Topological group#Homomorphisms|group isomorphism which is also a homeomorphism of the underlying topological spaces]].\n* For [[topological vector space]]s: isomorphism of vector spaces which is also a homeomorphism of the underlying topological spaces.\n* For [[Banach space]]s: [[Banach space#Linear operators, isomorphisms|bijective linear isometry]].\n* For [[Hilbert space]]s: [[unitary transformation]].\n* For Lie groups: [[Lie group#Homomorphisms and isomorphisms|a bijective smooth group homomorphism whose inverse is also a smooth group homomorphism]].\n* For [[Differentiable manifold|smooth manifolds]]: [[diffeomorphism]].\n* For [[symplectic manifold]]s: [[symplectomorphism]].\n* For [[Riemannian manifold]]s: [[Riemannian manifold#Isometries|isometric diffeomorphism]].\n* For [[Conformal geometry|conformal spaces]]: [[Conformal transformation#Riemannian geometry|conformal diffeomorphism]].\n* For [[probability space]]s: [[Standard probability space#Isomorphism|a bijective measurable and measure preserving map whose inverse is also measurable and measure preserving]].\n* For [[affine space]]s: bijective [[affine transformation]].\n* For [[projective space]]s: [[homography]].\nIn fact, Bourbaki stipulates two additional features. First, several sets ''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub> (so-called principal base sets) may be used, rather than a single set ''X''. However, this feature is of little use. All the items listed above use a single principal base set. Second, so-called auxiliary base sets ''E''<sub>1</sub>, ..., ''E''<sub>''m''</sub> may be used. This feature is widely used. Indeed, the structure of a vector space stipulates not only addition ''X'' × ''X'' → ''X'' but also scalar multiplication '''R''' × ''X'' → ''X'' (if '''R''' is the field of scalars). Thus, '''R''' is an auxiliary base set (called also \"external\"<ref>{{harvnb|Pudlák|2013|loc=page 12}}</ref>). The scale of sets consists of all sets arising from all base sets (both principal and auxiliary) by taking Cartesian products and power sets. Still, the map ''f'' (possibly an isomorphism) acts on ''X'' only; auxiliary sets are endowed by identity maps. (However, the case of ''n'' principal sets leads to ''n'' maps.)\n\n===Functoriality===\n\nSeveral statements formulated by Bourbaki without mentioning categories can be reformulated readily in the language of [[category theory]]. First, some terminology. \n* The scale of sets is indexed by \"echelon construction schemes\",<ref name=\"B-IV.1.1\">{{harvnb|Bourbaki|1968|loc=Sect.IV.1.1}}</ref> called also \"types\".<ref>{{harvnb|Pudlák|2013|loc=page 10}}</ref><ref>{{harvnb|Marshall|Chuaqui|1991|loc=§2}}</ref> One may think of, say, the set ''P''(''P''(''X'' × ''X'') × ''X'' × ''P''(''P''(''X''))) × ''X'' as a set ''X'' substituted into the formula \"''P''(''P''(''a'' × ''a'') × ''a'' × ''P''(''P''(''a''))) × ''a''\" for the variable ''a''; this formula is the corresponding echelon construction scheme.<ref group=\"details\">In order to be more formal, Bourbaki encodes such formulas with sequences of ordered pairs of natural numbers.</ref> (This notion, defined for all structures, may be thought of as a generalization of the signature defined only for algebraic structures.)<ref group=\"details\">On one hand, it is possible to exclude the Cartesian products, treating a pair (''x'',''y'') as just the set {{''x''},{''x'',''y''}}. On the other hand, it is possible to include the set operation ''X'',''Y''->''Y''<sup>''X''</sup> (all functions from ''X'' to ''Y''). \"It is possible to simplify the matter by considering operations and functions as a special kind of relations (for example, a binary operation is a ternary relation). However, quite often, it is an advantage to have operations as a primitive concept.\"  {{harvnb|Pudlák|2013|loc=page 17}}</ref>\n* Let '''Set*''' denote the [[groupoid]] of sets and bijections. That is, the category whose objects are (all) sets, and morphisms are (all) bijections.\n\n''Proposition.'' <ref name=\"B-IV.1.2\">{{harvnb|Bourbaki|1968|loc=Sect.IV.1.2}}</ref> Each echelon construction scheme leads to a functor from '''Set*''' to itself.\n\nIn particular, the [[permutation group]] of a set ''X'' [[Group action (mathematics)|acts]] on every scale set ''S''<sub>''X''</sub>.\n\nIn order to formulate one more proposition, the notion \"species of structures\" is needed, since echelon construction scheme gives only preliminary information on a structure. For example, commutative groups and (arbitrary) groups are two different species of the same echelon construction scheme. Another example: topological spaces and measurable spaces. They differ in the so-called axiom of the species. This axiom is the conjunction of all required properties, such as \"multiplication is associative\" for groups, or \"the union of open sets is an open set\" for topological spaces.\n* A species of structures consists of an echelon construction scheme and an axiom of the species.\n\n''Proposition.'' <ref name=\"B-IV.1.5\">{{harvnb|Bourbaki|1968|loc=Sect.IV.1.5}}</ref> Each species of structures leads to a functor from '''Set*''' to itself.\n\nExample. For the species of groups, the functor ''F'' maps a set ''X'' to the set ''F''(''X'') of all group structures on ''X''. For the species of topological spaces, the functor ''F'' maps a set ''X'' to the set ''F''(''X'') of all topologies on ''X''. The morphism ''F''(''f'') : ''F''(''X'') → ''F''(''Y'') corresponding to a bijection ''f'' : ''X'' → ''Y'' is the transport of structures. Topologies on ''Y'' correspond bijectively to topologies on ''X''. The same holds for group structures, etc.\n\nIn particular, the set of all structures of a given species on a given set is invariant under the action of the permutation group on the corresponding scale set ''S''<sub>''X''</sub>, and is a [[Fixed point (mathematics)|fixed point]] of the action of the group on another scale set ''P''(''S''<sub>''X''</sub>). However, not all fixed points of this action correspond to species of structures.<ref group=\"details\">The set of all possible axioms of species is [[Countable set|countable]], while the set of all fixed points of the considered action may be uncountable. Tarski's \"[[Alfred Tarski#What are logical notions?|logical notions of higher order]]\" are closer to the fixed points than to species of structures, see {{harvnb|Feferman|2010}} and references therefrom.</ref>\n\nGiven two species, Bourbaki defines the notion \"procedure of deduction\" (of a structure of the second species from a structure of the first species).<ref name=\"B-IV.1.6\">{{harvnb|Bourbaki|1968|loc=Sect.IV.1.6}}</ref> A pair of mutually inverse procedures of deduction leads to the notion \"equivalent species\".<ref name=\"B-IV.1.7\">{{harvnb|Bourbaki|1968|loc=Sect.IV.1.7}}</ref>\n\nExample. The structure of a topological space may be defined as an [[Topological space#Open sets definition|open set topology]] or alternatively, a [[Topological space#Closed sets definition|closed set topology]]. The two corresponding procedures of deduction coincide; each one replaces all given subsets of ''X'' with their complements. In this sense, these are two equivalent species.\n\nIn the general definition of Bourbaki, deduction procedure may include a change of the principal base set(s), but this case is not treated here. In the language of category theory one have the following result.\n\n''Proposition.'' <ref name=\"B-IV.1.7\" /> Equivalence between two species of structures leads to a [[Natural transformation#Definition|natural isomorphism]] between the corresponding functors.\n\nHowever, in general, not all natural isomorphisms between these functors correspond to equivalences between the species.<ref group=\"details\">The set of all possible deduction procedures is countable, while the set of all natural isomorphisms between the considered functors may be uncountable (see an example in Section [[#Canonical, not just natural]]).</ref>\n\n==Mathematical practice==\n\n:\"We often do not distinguish structures that are isomorphic and often say that {{'}}''two structures are the same, up to isomorphism''{{'}}.\"<ref>{{harvnb|Pudlák|2013|loc=page 13}}</ref>\n:\"When studying structures we are interested only in their form, but when we prove their existence we need to construct them.\"<ref>{{harvnb|Pudlák|2013|loc=page 22}}</ref>\n\n:'Mathematicians are of course used to identifying isomorphic structures in practice, but they generally do so by \"abuse of notation\", or some other informal device, knowing that the objects involved are not \"really\" identical.'<ref name=\"HoTT\">{{harvnb|The Univalent Foundations Program|2013|loc=Subsection \"Univalent foundations\" of Introduction}}</ref> (A radically better approach is expected; but for now, Summer 2014, the definitive book quoted above does not elaborate on structures.)\n\nIn practice, one makes no distinction between equivalent species of structures.<ref name=\"B-IV.1.7\" />\n\nUsually, a text based on natural numbers (for example, the article \"[[prime number]]\") does not specify the used definition of natural numbers. Likewise, a text based on topological spaces (for example, the article \"[[homotopy]]\", or \"[[inductive dimension]]\") does not specify the used definition of a topological space. Thus, it is possible (and rather probable) that the reader and the author interpret the text differently, according to different definitions. Nevertheless, the communication is successful, which means that such different definitions may be thought of as equivalent.\n\nA person acquainted with topological spaces knows basic relations between neighborhoods, convergence, continuity, boundary, closure, interior, open sets, closed sets, and does not need to know that some of these notions are \"primary\", stipulated in the definition of a topological space, while others are \"secondary\", characterized in terms of \"primary\" notions. Moreover, knowing that subsets of a topological space are themselves topological spaces, as well as products of topological spaces, the person is able to construct some new topological spaces irrespective of the definition.\n\nThus, in practice a topology on a set is treated like an [[abstract data type]] that provides all needed notions (and [[Constructor (object-oriented programming)|constructors]]) but hides the distinction between \"primary\" and \"secondary\" notions. The same applies to other kinds of mathematical structures. \"Interestingly, the formalization of structures in set theory is a similar task as the formalization of structures for computers.\"<ref>{{harvnb|Pudlák|2013|loc=page 34}}</ref>\n\n==Canonical, not just natural==\n\nAs was mentioned, equivalence between two species of structures leads to a natural isomorphism between the corresponding functors. However, \"[[Natural transformation|natural]]\" does not mean \"[[Canonical map|canonical]]\". A natural transformation is generally non-unique.\n\nExample. Consider again the two equivalent structures for natural numbers. One is the \"Peano structure\" (0,''S''), the other is the structure (+, ·, ≤) of ordered semiring. If a set ''X'' is endowed by both structures then, on one hand, ''X'' = {{mset| ''a''<sub>0</sub>, ''a''<sub>1</sub>, ''a''<sub>2</sub>, ... }} where ''S''(''a''<sub>''n''</sub>) = ''a''<sub>''n''+1</sub> for all ''n'' and 0 = ''a''<sub>0</sub>; and on the other hand, ''X'' = {{mset| ''b''<sub>0</sub>, ''b''<sub>1</sub>, ''b''<sub>2</sub>, ... }} where ''b''<sub>''m''+''n''</sub> = ''b''<sub>''m''</sub> + ''b''<sub>''n''</sub>, ''b''<sub>''m''·''n''</sub> = ''b''<sub>''m''</sub> · ''b''<sub>''n''</sub>, and ''b''<sub>''m''</sub>  ≤''b''<sub>''n''</sub> if and only if ''m'' ≤ ''n''. Requiring that ''a''<sub>''n''</sub> = ''b''<sub>''n''</sub> for all ''n'' one gets the canonical equivalence between the two structures. However, one may also require ''a''<sub>0</sub> = ''b''<sub>1</sub>, ''a''<sub>1</sub> = ''b''<sub>0</sub>, and ''a''<sub>''n''</sub> = ''b''<sub>''n''</sub> for all ''n'' > 1, thus getting another, non-canonical, natural isomorphism. Moreover, every [[permutation]] of the index set {{mset| 0, 1, 2, ... }} leads to a natural isomorphism; they are uncountably many!\n\nAnother example. A structure of a (simple) graph on a set ''V'' = {{mset| 1, 2, ..., n }} of vertices may be described by means of its [[adjacency matrix]], a (0,1)-matrix of size ''n''×''n'' (with zeros on the diagonal). More generally, for arbitrary ''V'' an adjacency function on ''V'' × ''V'' may be used. The canonical equivalence is given by the rule: \"1\" means \"connected\" (with an edge), \"0\" means \"not connected\". However, another rule, \"0\" means \"connected\", \"1\" means \"not\", may be used, and leads to another, natural but not canonical, equivalence. In this example, canonicity is rather a matter of convention. But here is a worse case. Instead of \"0\" and \"1\" one may use, say, the two possible orientations of the plane '''R'''<sup>2</sup> (\"clockwise\" and \"counterclockwise\"). It is difficult to choose a canonical rule in this case!\n\n\"Natural\" is a well-defined mathematical notion, but it does not ensure uniqueness. \"Canonical\" does, but generally is more or less conventional. A consistent choice of canonical equivalences is an inevitable component of equivalent definitions of mathematical structures.\n\n==See also==\n{{div col|colwidth=22em}}\n* [[Structure (category theory)]]\n* [[Concrete category]]\n* [[Abuse of terminology#Equality vs. isomorphism]]\n* [[Equivalence of categories]]\n{{div col end}}\n\n==Notes==\n<references group=\"details\" />\n\n==Footnotes==\n\n{{Reflist|2}}\n\n==References==\n*{{Citation\n | first = Pavel\n | last = Pudlák \n | title = Logical Foundations of Mathematics and Computational Complexity. A Gentle Introduction\n | year = 2013\n | publisher = Springer\n}}.\n*{{Citation\n | last = Bourbaki\n | first = Nicolas\n | author-link=Nicolas Bourbaki\n | title = Elements of mathematics: Theory of sets\n | year = 1968\n | publisher = Hermann (original), Addison-Wesley (translation)\n}}.\n\n==Further reading==\n*{{citation|last=Feferman|first=S.|author-link=Solomon Feferman|year=2010|title=Set-theoretical invariance criteria for logicality|journal=Notre Dame Journal of Formal Logic|volume=51|pages=3&ndash;20|doi=10.1215/00294527-2010-002}}.\n*{{citation|last1=Marshall|first1=M.V.|last2=Chuaqui|first2=R.|author2-link= Rolando Chuaqui|year=1991|title=Sentences of type theory: the only sentences preserved under isomorphisms|journal=The Journal of Symbolic Logic|volume=56|number=3|pages=932&ndash;948|doi=10.2178/jsl/1183743741}}.\n*{{Citation\n | last = The Univalent Foundations Program\n | title = Homotopy Type Theory: Univalent Foundations of Mathematics\n | year = 2013\n | place = Institute for Advanced Study\n | publisher =\n | url = http://homotopytypetheory.org/book\n}}.\n\n==External links==\n* [http://ncatlab.org/nlab/show/structured+set nLab:structured set] \"Almost everything in contemporary mathematics is an example of a structured set.\" (quoted from Section \"Examples\") \n* [http://ncatlab.org/nlab/show/structure+in+model+theory nLab: structure in model theory]\n* [http://ncatlab.org/nlab/show/stuff%2C+structure%2C+property nLab: stuff, structure, property]\n* [http://mathoverflow.net/questions/19644/what-is-the-definition-of-canonical MathOverflow: What is the definition of “canonical”?] \"a rule of thumb: there is a canonical isomorphism between X and Y if and only if you would feel comfortable writing X = Y\" (Reid Barton)\n* [http://www.abstractmath.org/MM/MMMathStructure.htm Abstract Math:Mathematical structures] \"When you think of a structure it is best to think of it as containing all that information, not just the stuff in the definition\" (Charles Wells)\n* [http://math.stackexchange.com/questions/303751/a-pedantic-question-about-defining-new-structures-in-a-path-independent-way MathStackExchange: A pedantic question about defining new structures in a path-independent way] `We would continue making statements like, \"A topological space is determined by its open sets,\" but would never make a statement like, \"A topological space is an ordered pair <math>(X,\\mathcal O)</math> such that...\"'\n* [http://math.stackexchange.com/questions/290507/does-there-exist-another-way-of-obtaining-a-topological-space-from-a-metric-spac?rq=1 MathStackExchange: Does there exist another way of obtaining a topological space from a metric space equally deserving of the term “canonical”?]\n\n[[Category:Mathematical structures]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Equivalent infinitesimal",
      "url": "https://en.wikipedia.org/wiki/Equivalent_infinitesimal",
      "text": "#REDIRECT [[Indeterminate form#Equivalent infinitesimal]] {{R to section}}\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Equivalent latitude",
      "url": "https://en.wikipedia.org/wiki/Equivalent_latitude",
      "text": "{{Orphan|date=September 2011}}\n\nIn [[differential geometry]], the '''equivalent latitude''' is a [[Lagrangian coordinates|Lagrangian coordinate]] .\nIt is often used in [[atmospheric science]],\nparticularly in the study of [[stratosphere|stratospheric]] dynamics.\nEach isoline in a map of equivalent latitude follows the [[flow velocity]] and encloses the same area as the latitude line of equivalent value, hence \"equivalent latitude.\"  \nEquivalent latitude is calculated from [[potential vorticity]],  from passive tracer simulations  and from actual measurements of atmospheric tracers such as [[ozone]].\n\n==Calculation of equivalent latitude==\n\nThe calculation of equivalent latitude involves creating a [[monotonic function|\nmonotonic mapping]] between the values of equivalent latitude and\nthe tracer it is based upon: higher values of the tracer map to higher\nvalues of equivalent latitude.\nA precise method is to assign a\nrepresentative area to each of the tracer measurements, filling the entire globe.\nThus, for a tracer field regularly gridded in longitude and latitude, \ngrid points closer to the pole will take up a smaller area,\nin proportion to the cosine of the latitude.\nNow, [[sorting|rank]] all the tracer values then form the cumulative sum.\nThe equivalent latitude from the area is given as:\n\n:<math>\n\\phi = \\sin^{-1} \\left ( \\frac{A}{2 \\pi R^2} -1 \\right )\n</math>\n\nwhere ''A'' is the area enclosed to the South (''A'' = 0 corresponds to the equivalent South Pole) and ''R'' is the radius of the Earth.\nThis method generates a mapping that is as continuous as the data allows\nas opposed to binning which produces a coarse-grained mapping.\n\n==References==\n\n* {{cite journal\n|author1=Douglas R. Allen\n|author2=Noboru Nakamura\n|title=Tracer Equivalent Latitude: A Diagnostic Tool for Isentropic Transport Studies\n|journal=Journal of the Atmospheric Sciences\n|year=2003\n|volume=60\n|pages=287–304\n|doi=10.1175/1520-0469(2003)060<0287:teladt>2.0.co;2}}\n* {{cite journal\n|author1=Neal Butchart\n|author2=Ellis E. Remsberg\n|title=The area of the stratospheric polar vortex as a diagnostic for tracer transport on an isentropic surface\n|journal=Journal of the Atmospheric Sciences\n|year=1986\n|volume=43\n|pages=1319–1339\n|doi=10.1175/1520-0469(1986)043<1319:taotsp>2.0.co;2}}\n\n[[Category:Atmospheric sciences]]\n[[Category:Climatology]]\n[[Category:Differential geometry]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Exponentially equivalent measures",
      "url": "https://en.wikipedia.org/wiki/Exponentially_equivalent_measures",
      "text": "{{short description|equivalence relation on mathematical measures}}\nIn [[mathematics]], '''exponential equivalence of measures''' is how two sequences or families of [[probability measure]]s are “the same” from the point of view of [[large deviations theory]].\n\n==Definition==\nLet <math>(M,d)</math> be a [[metric space]] and consider two one-[[parameter]] families of probability measures on <math>M</math>, say <math>(\\mu_\\varepsilon)_{\\varepsilon >0}</math> and <math>(\\nu_\\varepsilon)_{\\varepsilon >0}</math>. These two families are said to be '''exponentially equivalent''' if there exist\n* a one-parameter family of probability spaces <math>(\\Omega,\\Sigma_\\varepsilon,P_\\varepsilon)_{\\varepsilon >0}</math>,\n* two families of <math>M</math>-valued random variables <math>(Y_\\varepsilon)_{\\varepsilon >0}</math> and <math>(Z_\\varepsilon)_{\\varepsilon >0}</math>,\nsuch that\n* for each <math>\\varepsilon >0</math>, the <math>P_\\varepsilon</math>-law (i.e. the [[push-forward measure]]) of <math>Y_\\varepsilon</math> is <math>\\mu_\\varepsilon</math>, and the <math>P_\\varepsilon</math>-law of <math>Z_\\varepsilon</math> is <math>\\nu_\\varepsilon</math>,\n* for each <math>\\delta >0</math>, &ldquo;<math>Y_\\varepsilon</math> and <math>Z_\\varepsilon</math> are further than <math>\\delta</math> apart&rdquo; is a <math>\\Sigma_\\varepsilon</math>-[[measurable set|measurable event]], i.e.\n::<math>\\big\\{ \\omega \\in \\Omega \\big| d(Y_{\\varepsilon}(\\omega), Z_{\\varepsilon}(\\omega)) > \\delta \\big\\} \\in \\Sigma_{\\varepsilon},</math>\n* for each <math>\\delta >0</math>,\n::<math>\\limsup_{\\varepsilon \\downarrow 0}\\, \\varepsilon \\log P_\\varepsilon \\big( d(Y_\\varepsilon, Z_\\varepsilon) > \\delta \\big) = - \\infty.</math>\n\nThe two families of random variables <math>(Y_\\varepsilon)_{\\varepsilon >0}</math> and <math>(Z_\\varepsilon)_{\\varepsilon >0}</math> are also said to be '''exponentially equivalent'''.\n\n==Properties==\nThe main use of exponential equivalence is that as far as large deviations principles are concerned, exponentially equivalent families of measures are indistinguishable.  More precisely, if a large deviations principle holds for <math>(\\mu_\\varepsilon)_{\\varepsilon >0}</math> with good [[rate function]] <math>I</math>, and <math>(\\mu_\\varepsilon)_{\\varepsilon >0}</math> and <math>(\\nu_\\varepsilon)_{\\varepsilon >0}</math> are exponentially equivalent, then the same large deviations principle holds for <math>(\\nu_\\varepsilon)_{\\varepsilon >0}</math> with the same good rate function <math>I</math>.\n\n==References==\n\n* {{cite book\n| last= Dembo\n| first = Amir\n|author2=Zeitouni, Ofer\n| title = Large deviations techniques and applications\n| series = Applications of Mathematics (New York) 38\n| edition = Second\n| publisher = Springer-Verlag\n| location = New York\n| year = 1998\n| pages = xvi+396\n| isbn = 0-387-98406-2\n| mr = 1619036\n}} (See section 4.2.2)\n\n[[Category:Asymptotic analysis]]\n[[Category:Probability theory]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Extensionality",
      "url": "https://en.wikipedia.org/wiki/Extensionality",
      "text": "In [[logic]], '''extensionality''', or '''extensional equality''', refers to principles that judge objects to be [[equality (mathematics)|equal]] if they have the same external properties. It stands in contrast to the concept of [[intensionality]], which is concerned with whether the internal definitions of objects are the same.\n\n==Example==\n\nConsider the two functions ''f'' and ''g'' mapping from and to [[natural number]]s, defined as follows:\n* To find ''f''(''n''), first add 5 to ''n'', then multiply by 2.\n* To find ''g''(''n''), first multiply ''n'' by  2, then add 10. \n\nThese functions are extensionally equal; given the same input, both functions always produce the same value. But the definitions of the functions are not equal, and in that intensional sense the functions are not the same. \n\nSimilarly, in natural language there are many predicates (relations) that are intensionally different but are extensionally identical. For example, suppose that a town has one person named Joe, who is also the oldest person in the town. Then, the two argument predicates \"has one person named\", \"is the oldest person in\" are intensionally distinct, but extensionally equal for \"Joe\" in that \"town\" now.\n\n==In mathematics==\nThe extensional definition of function equality, discussed above, is commonly used in mathematics. Sometimes additional information is attached to a function, such as an explicit [[codomain]], in which case two functions must not only agree on all values, but must also have the same codomain, in order to be equal. \n\nA similar extensional definition is usually employed for relations: two relations are said to be equal if they have the same [[Extension (predicate logic)|extensions]].\n\nIn set theory, the [[axiom of extensionality]] states that two sets are equal if and only if they contain the same elements.  In mathematics formalized in set theory, it is common to identify relations&mdash;and, most importantly, [[function (mathematics)|functions]]&mdash;with their extension as stated above, so that it is impossible for two relations or functions with the same extension to be distinguished.\n\nOther mathematical objects are also constructed in such a way that the intuitive notion of \"equality\" agrees with set-level extensional equality; thus, equal [[ordered pair]]s have equal elements, and elements of a set which are related by an [[equivalence relation]] belong to the same [[equivalence class]].\n\n[[Type theory|Type-theoretical]] foundations of mathematics are generally ''not'' extensional in this sense, and [[setoid]]s are commonly used to maintain a difference between intensional equality and a more general equivalence relation (which generally has poor [[constructivism (mathematics)|constructibility]] or [[Decidability (logic)|decidability]] properties).\n\n==See also==\n*[[Duck typing]]\n*[[Identity of indiscernibles]]\n*[[Structural typing]]\n*[[Univalence axiom]]\n\n==References==\n{{reflist}}\n* [https://plato.stanford.edu/entries/logic-intensional/ Intensional Logic (Stanford Encyclopedia of Philosophy)]\n* [https://ncatlab.org/nlab/show/equality equality] in [[nLab]]\n\n{{Mathematical logic}}\n\n[[Category:Set theory]]\n[[Category:Concepts in logic]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Homomorphic equivalence",
      "url": "https://en.wikipedia.org/wiki/Homomorphic_equivalence",
      "text": "{{refimprove|date=August 2016}}\nIn the [[mathematics]] of [[graph theory]], two graphs, ''G'' and ''H'', are called '''homomorphically equivalent''' if there exists a graph homomorphism <math>f\\colon G\\to H</math> and a graph homomorphism <math>g\\colon H\\to G</math>. An example usage of this notion is that any two [[Core (graph theory)#Properties|cores]] of a graph are homomorphically equivalent.\n\nHomomorphic equivalence also comes up in the theory of [[databases]]. Given a [[database schema]], two [[instance (database)|instances]] I and J on it are called homomorphically equivalent if there exists an instance homomorphism <math>f\\colon I\\to J</math> and an instance homomorphism <math>g\\colon J\\to I</math>.\n\nIn fact for any [[Category (mathematics)|category]] ''C'', one can define homomorphic equivalence. It is used in the theory of [[accessible categories]], where \"weak universality\" is the best one can hope for in terms of injectivity classes; see <ref>Adamek and Rosicky, \"Locally Presentable and Accessible Categories\".</ref>\n\n==References==\n{{Reflist}}\n\n\n\n[[Category:Graph theory]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Identity (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Identity_%28mathematics%29",
      "text": "{{Distinguish|Identity element|Identity function|identity matrix}}\n\n[[File:Trig functions on unit circle.PNG|thumb|Visual proof of the [[Pythagorean identity]]. For any angle θ, The point (cos(θ),sin(θ)) lies on the [[unit circle]], which satisfies the equation ''x''<sup>2</sup>+''y''<sup>2</sup>=1. Thus, cos<sup>2</sup>(θ)+sin<sup>2</sup>(θ)=1.]]\n\nIn [[mathematics]] an '''identity''' is an [[equality (mathematics)|equality]] relation ''A''&nbsp;=&nbsp;''B'', such that ''A'' and ''B'' contain some [[variable (mathematics)|variables]] and ''A'' and ''B'' produce the same value as each other regardless of what values (usually numbers) are substituted for the variables. In other words, ''A''&nbsp;=&nbsp;''B'' is an identity if ''A'' and ''B'' define the same [[function (mathematics)|functions]]. This means that an ''identity'' is an ''equality'' between functions that are differently defined. For example, (''a''&nbsp;+&nbsp;''b'')<sup>2</sup> &nbsp;=&nbsp; ''a''<sup>2</sup>&nbsp;+&nbsp;2''ab''&nbsp;+&nbsp;''b''<sup>2</sup> and {{nowrap|1=cos<sup>2</sup>(''x'')&nbsp;+&nbsp;sin<sup>2</sup>(''x'')&nbsp;=&nbsp;1}} are identities. Identities are sometimes indicated by the [triple bar] symbol {{math|≡}} instead of {{math|1==}}, the [[equals sign]].\n\n== Common identities ==\n\n=== Trigonometric identities ===\n{{Main|List of trigonometric identities}}\n\nGeometrically, these are identities involving certain functions of one or more [[angle]]s.  They are distinct from [[Trigonometry#Triangle identities|triangle identities]], which are identities involving both angles and side lengths of a [[triangle]]. Only the former are covered in this article.\n\nThese identities are useful whenever expressions involving trigonometric functions need to be simplified. An important application is the [[integral|integration]] of non-trigonometric functions: a common technique involves first using the [[Trigonometric substitution|substitution rule with a trigonometric function]], and then simplifying the resulting integral with a trigonometric identity.\n\nOne example is <math> \\sin ^2 \\theta +  \\cos ^2 \\theta = 1, </math>\nwhich is true for all [[Complex number|complex]] values of <math>\\theta</math> (since the complex numbers <math>\\mathbb{C}</math> are the domain of sin and cos), as opposed to\n:<math>\\cos \\theta = 1,</math>\nwhich is true only for some values of <math>\\theta</math>, not all.  For example, the latter equation is true when <math> \\theta = 0,</math> false when <math>\\theta = 2</math>.\n\n=== Exponential identities ===\n{{Main|Exponentiation}}\n\nThe following identities hold for all integer exponents, provided that the base is non-zero:\n:<math>\\begin{align}\n      b^{m + n} &= b^m \\cdot b^n \\\\\n        (b^m)^n &= b^{m\\cdot n} \\\\\n  (b \\cdot c)^n &= b^n \\cdot c^n\n\\end{align}</math>\n\nExponentiation is not [[commutative]]. This contrasts with addition and multiplication, which are. For example, {{nowrap|1=2 + 3 = 3 + 2 = 5}} and {{nowrap|1=2 · 3 = 3 · 2 = 6}}, but {{nowrap|1=2<sup>3</sup> = 8}}, whereas {{nowrap|1=3<sup>2</sup> = 9}}.\n\nExponentiation is not [[associative]] either. Addition and multiplication are. For example,\n{{nowrap|1=(2 + 3) + 4 = 2 + (3 + 4) = 9}} and {{nowrap|1=(2 · 3) · 4 = 2 · (3 · 4) = 24}}, but 2<sup>3</sup> to the 4 is 8<sup>4</sup> or 4,096, whereas 2 to the 3<sup>4</sup> is 2<sup>81</sup> or 2,417,851,639,229,258,349,412,352. Without parentheses to modify the order of calculation, by convention the order is top-down, not bottom-up:\n:<math>b^{p^q} = b^{(p^q)} \\ne (b^p)^q = b^{(p \\cdot q)} = b^{p \\cdot q} .</math>\n\n=== Logarithmic identities ===\n{{Main|Logarithmic identities}}\nSeveral important formulas, sometimes called ''logarithmic identities'' or ''log laws'', relate logarithms to one another.<ref>All statements in this section can be found in {{Harvard citations|last1=Shirali|first1=Shailesh|year=2002|loc=section 4|nb=yes}}, {{Harvard citations|last1=Downing| first1=Douglas |year=2003|loc=p. 275}}, or {{Harvard citations|last1=Kate|last2=Bhapkar|year=2009|loc=p. 1-1|nb=yes}}, for example.</ref>\n\n==== Product, quotient, power and root ====\nThe logarithm of a product is the sum of the logarithms of the numbers being multiplied; the logarithm of the ratio of two numbers is the difference of the logarithms. The logarithm of the {{nowrap|''p''-th}} power of a number is ''p'' times the logarithm of the number itself; the logarithm of a {{nowrap|''p''-th}} root is the logarithm of the number divided by ''p''. The following table lists these identities with examples. Each of the identities can be derived after substitution of the logarithm definitions {{nowrap begin}}x = b<big><sup>log<sub>b</sub>(x)</sup></big>{{nowrap end}}, and/or {{nowrap begin}}y = b<big><sup>log<sub>b</sub>(y)</sup></big>{{nowrap end}}, in the left hand sides.\n\n<center>\n{| class=\"wikitable\"\n|-\n!  !! Formula !! Example\n|-\n| product || <cite id=labegarithmProducts><math> \\log_b(x y) = \\log_b (x) + \\log_b (y) </math></cite>|| <math> \\log_3 (243) = \\log_3(9 \\cdot 27) = \\log_3 (9) + \\log_3 (27) =  2 + 3 = 5 </math>\n|-\n| quotient || <math>\\log_b \\!\\left(\\frac x y \\right) = \\log_b (x) - \\log_b (y) </math>|| <math> \\log_2 (16) = \\log_2 \\!\\left ( \\frac{64}{4} \\right ) = \\log_2 (64) - \\log_2 (4) = 6 - 2 = 4</math>\n|-\n| power || <cite id=labelLogarithmPowers><math>\\log_b(x^p) = p \\log_b (x) </math></cite>|| <math> \\log_2 (64) = \\log_2 (2^6) = 6 \\log_2 (2) = 6 </math>\n|-\n| root || <math>\\log_b \\sqrt[p]{x} = \\frac {\\log_b (x)} p </math>|| <math> \\log_{10} \\sqrt{1000} = \\frac{1}{2}\\log_{10} 1000 = \\frac{3}{2} = 1.5 </math>\n|}\n</center>\n\n==== Change of base ====\nThe logarithm log<sub>''b''</sub>(''x'') can be computed from the logarithms of ''x'' and ''b'' with respect to an arbitrary base ''k'' using the following formula:\n: <cite id=labelLogarithmBaseChange><math> \\log_b(x) = \\frac{\\log_k(x)}{\\log_k(b)}.</math></cite>\nTypical [[scientific calculators]] calculate the logarithms to bases 10 and [[e (mathematical constant)|''e'']].<ref>{{Citation | last1=Bernstein | first1=Stephen | last2=Bernstein | first2=Ruth | title=Schaum's outline of theory and problems of elements of statistics. I, Descriptive statistics and probability| publisher=[[McGraw-Hill]] | location=New York | series=Schaum's outline series | isbn=978-0-07-005023-5 | year=1999}}, p. 21</ref> Logarithms with respect to any base ''b'' can be determined using either of these two logarithms by the previous formula:\n:<math> \\log_b (x) = \\frac{\\log_{10} (x)}{\\log_{10} (b)} = \\frac{\\log_{e} (x)}{\\log_{e} (b)}. </math>\nGiven a number ''x'' and its logarithm log<sub>''b''</sub>(''x'') to an unknown base ''b'', the base is given by:\n: <math> b = x^\\frac{1}{\\log_b(x)}.</math>\n\n=== Hyperbolic function identities ===\n{{Main|Hyperbolic function}}\nThe hyperbolic functions satisfy many identities, all of them similar in form to the [[trigonometric identity|trigonometric identities]]. In fact, '''Osborn's rule'''<ref>{{cite journal|jstor=3602492|title=109. Mnemonic for Hyperbolic Formulae|journal=The Mathematical Gazette|first=G.|last=Osborn|date=1 January 1902|volume=2|issue=34|pages=189|doi=10.2307/3602492|url=https://zenodo.org/record/1449741}}</ref> states that one can convert any trigonometric identity into a hyperbolic identity by expanding it completely in terms of integral powers of sines and cosines, changing sine to sinh and cosine to cosh, and switching the sign of every term which contains a product of 2, 6, 10, 14, ... sinhs.<ref>{{cite book\n |title=Technical mathematics with calculus\n |edition=3rd\n |first1=John Charles\n |last1=Peterson\n |publisher=Cengage Learning\n |year=2003\n |isbn=0-7668-6189-9\n |page=1155\n |url=https://books.google.com/books?id=PGuSDjHvircC}}, [https://books.google.com/books?id=PGuSDjHvircC&pg=PA1155 Chapter 26, page 1155]</ref>\n\nThe [[Gudermannian function]] gives a direct relationship between the circular functions and the hyperbolic ones that does not involve complex numbers.\n\n== See also ==\n* [[Accounting identity]]\n* [[List of mathematical identities]]\n\n== References ==\n{{reflist}}\n\n==External links==\n{{Commonscat}}\n*[http://encyclopedia-of-equation.webnode.jp/ Encyclopedia of Equation]   Online  encyclopedia of mathematical identities\n*[http://sites.google.com/site/tpiezas/Home/ A Collection of Algebraic Identities]\n\n[[Category:Elementary algebra]]\n[[Category:Mathematical identities| ]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Isometry",
      "url": "https://en.wikipedia.org/wiki/Isometry",
      "text": "{{About|distance-preserving functions|other mathematical uses|isometry (disambiguation)|non-mathematical uses|Isometric (disambiguation){{!}}Isometric}}\n{{Distinguish|Isometric projection}}\n{{refimprove|date=June 2016}}\n\nIn [[mathematics]], an '''isometry''' (or '''[[congruence (geometry)|congruence]]''', or '''congruent transformation''') is a [[distance]]-preserving transformation between [[metric spaces]], usually assumed to be [[Bijection|bijective]].<ref name=CoxeterIsometryDef>{{harvnb|Coxeter|1969|page=29}} <p>\"We shall find it convenient to use the word ''transformation'' in the special sense of a one-to-one correspondence <math> P \\to P'</math> among all points in the plane (or in space), that is, a rule for associating pairs of points, with the understanding that each pair has a first member {{mvar|P}} and a second member {{mvar|P'}} and that every point occurs as the first member of just one pair and also as the second member of just one pair...<p>In particular, an ''isometry'' (or \"congruent transformation,\" or \"congruence\") is a transformation which preserves length...\"</ref> \n[[File:Academ Reflections with parallel axis on wallpaper.svg|thumb|upright=1.4|A [[Function composition|composition]] of two [[Euclidean group#Direct and indirect isometries|opposite]] isometries is a direct isometry.  [[Reflection (mathematics)|A reflection]] in a line is an opposite isometry, like  {{math|''R''<sub> 1</sub>}}  or {{math|''R''<sub> 2</sub>}}  on the image. [[Translation (geometry)|Translation]] {{math|''T''}} is a direct isometry: [[Rigid body|a rigid motion]].<ref>{{harvnb|Coxeter|1969|p=46}} <p>'''3.51''' ''Any direct isometry is either a translation or a rotation. Any opposite isometry is either a reflection or a glide reflection.''</ref>]]\n\n==Introduction==\nGiven a metric space (loosely, a set and a scheme for assigning distances between elements of the set), an isometry is a [[Transformation (geometry)|transformation]] which maps elements to the same or another metric space such that the distance between the image elements in the new metric space is equal to the distance between the elements in the original metric space. In a two-dimensional or three-dimensional [[Euclidean space]], two geometric figures are [[Congruence (geometry)|congruent]] if they are related by an isometry;<ref>{{harvnb|Coxeter|1969|page=39}}<p>'''3.11''' ''Any two congruent triangles are related by a unique isometry.''</ref> the isometry that relates them is either a rigid motion (translation or rotation), or a [[Function composition|composition]] of a rigid motion and a [[Reflection (mathematics)|reflection]].<!--commentary: i presume \"they\" here means the geometric figures. still commenting out because it doesn't seem to help.--> <!--They are equal, up to an action of a rigid motion, if related by a [[Euclidean group#Direct and indirect isometries|direct isometry]] (orientation preserving).-->\n\nIsometries are often used in constructions where one space is [[Embedding|embedded]] in another space. For instance, the [[Complete space#Completion|completion]] of a metric space ''M'' involves an isometry from ''M'' into ''M''', a [[quotient set]] of the space of [[Cauchy sequence]]s on ''M''. The original space ''M'' is thus isometrically [[isomorphism|isomorphic]] to a subspace of a [[complete metric space]], and it is usually identified with this subspace. Other embedding constructions show that every metric space is isometrically isomorphic to a [[closed set|closed subset]] of some [[normed vector space]] and that every complete metric space is isometrically isomorphic to a closed subset of some [[Banach space]].\n\nAn isometric surjective linear operator on a [[Hilbert space]] is called a [[unitary operator]].\n\n==Formal definitions==\n\nLet ''X'' and ''Y'' be [[metric space]]s with metrics ''d''<sub>''X''</sub> and ''d''<sub>''Y''</sub>. A [[function (mathematics)|map]] ''f'' : ''X'' → ''Y'' is called an '''isometry''' or '''distance preserving''' if for any ''a'',''b'' ∈ ''X'' one has\n\n:<math>d_Y\\left(f(a),f(b)\\right)=d_X(a,b).</math><ref>{{cite journal\n | last1 = Beckman | first1 = F. S.\n | last2 = Quarles | first2 = D. A., Jr.\n | journal = [[Proceedings of the American Mathematical Society]]\n | mr = 0058193\n | pages = 810–815\n | title = On isometries of Euclidean spaces\n | volume = 4\n | issue = 5\n | year = 1953\n | doi=10.2307/2032415\n | url=http://www.ams.org/journals/proc/1953-004-05/S0002-9939-1953-0058193-5/S0002-9939-1953-0058193-5.pdf\n | quote=<br>Let {{mvar|T}} be a transformation (possibly many-valued) of <math>E^n</math> (<math>2\\leq n < \\infty</math>) into itself.<br>Let <math>d(p,q)</math> be the distance between points {{mvar|p}} and {{mvar|q}} of <math>E^n</math>, and let {{mvar|Tp}}, {{mvar|Tq}} be any images of {{mvar|p}} and {{mvar|q}}, respectively.<br>If there is a length {{mvar|a}} > 0 such that <math>d(Tp,Tq)=a</math> whenever <math>d(p,q)=a</math>, then {{mvar|T}} is a Euclidean transformation of <math>E^n</math> onto itself.| jstor = 2032415\n }}</ref>\n\nAn isometry is automatically [[Injective function|injective]];<ref name=CoxeterIsometryDef/> otherwise two distinct points, ''a'' and ''b'', could be mapped to the same point, thereby contradicting the coincidence axiom of the metric ''d''. This proof is similar to the proof that an [[order embedding]] between [[partially ordered set]]s is injective. Clearly, every isometry between metric spaces is a topological embedding.\n\nA '''global isometry''', '''isometric isomorphism''' or '''congruence mapping''' is a [[bijective]] isometry. Like any other bijection, a global isometry has a [[function inverse]].  The inverse of a global isometry is also a global isometry.\n\nTwo metric spaces ''X'' and ''Y'' are called '''isometric''' if there is a bijective isometry from ''X'' to ''Y''. The [[Set (mathematics)|set]] of bijective isometries from a metric space to itself forms a [[group (mathematics)|group]] with respect to [[function composition]], called the '''[[isometry group]]'''.\n\nThere is also the weaker notion of ''path isometry'' or ''arcwise isometry'':\n\nA '''path isometry''' or '''arcwise isometry''' is a map which preserves the [[Arc length#Definition|lengths of curves]]; such a map is not necessarily an isometry in the distance preserving sense, and it need not necessarily be bijective, or even injective. This term is often abridged to simply ''isometry'', so one should take care to determine from context which type is intended.\n\n==Examples==\n* Any [[reflection (mathematics)|reflection]], [[translation (geometry)|translation]] and [[rotation]] is a global isometry on Euclidean spaces. See also [[Euclidean group#Overview of isometries in up to three dimensions|Euclidean group]].\n*The map <math> x\\mapsto |x|</math> in <math>{\\mathbb R}</math> is a path isometry but not an isometry.  Note that unlike an isometry, it is not injective.\n*The isometric [[linear map]]s from '''C'''<sup>''n''</sup> to itself are given by the [[unitary matrix|unitary matrices]].<ref>{{Cite journal | last1 = Roweis | first1 = S. T. | last2 = Saul | first2 = L. K. | title = Nonlinear Dimensionality Reduction by Locally Linear Embedding | doi = 10.1126/science.290.5500.2323 | journal = [[Science (journal)|Science]]| volume = 290 | issue = 5500 | pages = 2323–2326 | year = 2000 | pmid =  11125150| pmc = | citeseerx = 10.1.1.111.3313 }}</ref><ref>{{cite journal|last1=Saul |first1=Lawrence K. |last2=Roweis |first2=Sam T. | title= Think globally, fit locally: Unsupervised learning of nonlinear manifolds |journal=[[Journal of Machine Learning Research]]|volume=4|issue=June|pages=119–155|year=2003|quote=Quadratic optimisation of <math>\\mathbf M =(I-W)^\\top(I-W)</math> (page 135) such that <math>\\mathbf M\\equiv YY^\\top</math> }}</ref><ref>{{Cite journal |last=Zhang |first=Zhenyue |last2=Zha|first2=Hongyuan  |title=Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment |journal=SIAM Journal on Scientific Computing |volume=26 |issue=1 |year=2004 |pages=313–338 |doi=10.1137/s1064827502419154}}</ref><ref>{{cite journal|last1=Zhang|first1=Zhenyue|last2=Wang|first2=Jing|title=MLLE: Modified Locally Linear Embedding Using Multiple Weights|journal=[[Advances in Neural Information Processing Systems]]|year=2006|volume=19|url=https://papers.nips.cc/paper/3132-mlle-modified-locally-linear-embedding-using-multiple-weights|quote=It can retrieve the ideal embedding if MLLE is applied on data points sampled from an isometric manifold.}}</ref>\n\n==Linear isometry==\n\nGiven two [[normed vector space]]s ''V'' and ''W'', a '''linear isometry''' is a [[linear map]] ''f'' : ''V'' → ''W'' that preserves the norms:\n:<math>\\|f(v)\\| = \\|v\\|</math>\nfor all ''v'' in ''V''.<ref name=\"Thomsen 2017 p125\">{{cite book |last=Thomsen |first=Jesper Funch |date=2017 |title=Lineær algebra |trans-title=Linear algebra |page=125 |language=Danish |location=Århus |publisher=Department of Mathematics, Aarhus University}}</ref> Linear isometries are distance-preserving maps in the above sense. They are global isometries if and only if they are [[surjective]].\n\nBy the [[Mazur–Ulam theorem]], any isometry of normed vector spaces over '''R''' is [[Affine transformation|affine]].\n\nIn an inner product space, the fact that any linear isometry is an orthogonal transformation can be shown by using [[Polarization identity|polarization identities]] to\nprove ''<Ax, Ay> = <x, y>''<ref name=\"Thomsen 2017 p125\"/> and then applying the [[Riesz representation theorem]].\n\n==Manifolds==\nAn isometry of a [[manifold]] is any (smooth) mapping of that manifold into itself, or into another manifold that preserves the notion of distance between points. The definition of an isometry requires the notion of a [[metric (mathematics)|metric]] on the manifold; a manifold with a (positive-definite) metric is a [[Riemannian manifold]], one with an indefinite metric is a [[pseudo-Riemannian manifold]]. Thus, isometries are studied in [[Riemannian geometry]].\n\nA '''local isometry''' from one ([[Pseudo-Riemannian manifold|pseudo]]-)[[Riemannian manifold]] to another is a map which [[pullback (differential geometry)|pulls back]] the [[metric tensor]] on the second manifold to the metric tensor on the first. When such a map is also a [[diffeomorphism]], such a map is called an '''isometry''' (or '''isometric isomorphism'''), and provides a notion of [[isomorphism]] (\"sameness\") in the [[category theory|category]] '''Rm''' of Riemannian manifolds.\n\n===Definition===\n\nLet <math>R = (M, g)</math> and <math>R' = (M', g')</math> be two (pseudo-)Riemannian manifolds, and let <math>f : R \\to R'</math> be a diffeomorphism. Then <math>f</math> is called an '''isometry''' (or '''isometric isomorphism''') if\n\n:<math>g = f^{*} g', \\, </math>\n\nwhere <math>f^{*} g'</math> denotes the [[pullback (differential geometry)|pullback]] of the rank (0, 2) metric tensor <math>g'</math> by <math>f</math>. Equivalently, in terms of the [[pushforward (differential)|pushforward]] <math>f_{*}</math>, we have that for any two vector fields <math>v, w</math> on <math>M</math> (i.e. sections of the [[tangent bundle]] <math>\\mathrm{T} M</math>),\n\n:<math>g(v, w) = g' \\left( f_{*} v, f_{*} w \\right). \\, </math>\n\nIf <math>f</math> is a [[local diffeomorphism]] such that <math>g = f^{*} g'</math>, then <math>f</math> is called a '''local isometry'''.\n\n==Generalizations==\n* Given a positive real number ε, an '''ε-isometry''' or '''almost isometry''' (also called a '''[[Felix Hausdorff|Hausdorff]] approximation''') is a map <math>f:X\\to Y</math> between metric spaces such that\n*# for ''x'',''x''&prime; ∈ ''X'' one has |''d''<sub>''Y''</sub>(ƒ(''x''),ƒ(''x''&prime;))&minus;''d''<sub>''X''</sub>(''x'',''x''&prime;)| < ε, and\n*# for any point ''y'' ∈ ''Y'' there exists a point ''x'' ∈ ''X'' with ''d''<sub>''Y''</sub>(''y'',ƒ(''x'')) < ε\n\n:That is, an ε-isometry preserves distances to within ε and leaves no element of the codomain further than ε away from the image of an element of the domain.  Note that ε-isometries are not assumed to be [[continuous function|continuous]].\n\n*The '''[[restricted isometry property]]''' characterizes nearly isometric matrices for sparse vectors.\n*'''[[Quasi-isometry]]''' is yet another useful generalization.\n* One may also define an element in an abstract unital C*-algebra to be an isometry: \n*:<math> a\\in\\mathfrak{A}</math> is an isometry if and only if <math> a^* \\cdot a = 1 </math>.\n:Note that as mentioned in the introduction this is not necessarily a unitary element because one does not in general have that left inverse is a right inverse.\n\n*On a [[pseudo-Euclidean space]], the term ''isometry'' means a linear bijection preserving magnitude. See also [[Quadratic form#Quadratic spaces|Quadratic spaces]].\n\n==See also==\n{{Div col|colwidth=20em}}\n*[[Motion (geometry)]]\n*[[Beckman&ndash;Quarles theorem]]\n*[[Semidefinite embedding]]\n*[[Flat (geometry)]]\n*[[Euclidean plane isometry]]\n*[[Orthogonal group#3D isometries that leave the origin fixed|3D isometries that leave the origin fixed]]\n*[[Space group]]\n*[[Involution (mathematics)|Involution]]\n*[[Symmetry in mathematics]]\n*[[Homeomorphism group]]\n*[[Partial isometry]]\n*[[Dual norm#The second dual of a Banach space|The second dual of a Banach space as an isometric isomorphism]]\n*[[Isometry group]]\n*[[Myers–Steenrod theorem]]\n{{Div col end}}\n\n==References==\n{{reflist}}\n\n==Bibliography==\n* {{cite book|last=Coxeter|first=H. S. M.|author-link1=Harold Scott MacDonald Coxeter|title=Introduction to Geometry, Second edition|year=1969|publisher=[[John Wiley & Sons|Wiley]]|isbn=9780471504580|ref=harv}}\n* {{cite book | author=Lee, Jeffrey M. | title=Manifolds and Differential Geometry |location=Providence, RI |publisher=American Mathematical Society | year=2009 |isbn=978-0-8218-4815-9 |url=https://books.google.com/books?id=QqHdHy9WsEoC }}\n\n[[Category:Functions and mappings]]\n[[Category:Metric geometry]]\n[[Category:Symmetry]]\n[[Category:Equivalence (mathematics)]]\n[[Category:Riemannian geometry]]"
    },
    {
      "title": "Isomorphism",
      "url": "https://en.wikipedia.org/wiki/Isomorphism",
      "text": "{{short description|In mathematics, invertible homomorphism}}\n{{About|mathematics}}\n{{multiple image\n| footer    = The [[Group (mathematics)|group]] of fifth [[roots of unity]] under multiplication is isomorphic to the group of rotations of the regular pentagon under composition.\n| width     = 200\n| image1    = One5Root.svg\n| alt1      = Fifth roots of unity\n| image2    = Regular polygon 5 annotated.svg\n| alt2      = Rotations of a pentagon\n}}\nIn [[mathematics]], an '''isomorphism''' (from the [[Ancient Greek]]: [[wikt:ἴσος|ἴσος]] ''isos'' \"equal\", and [[wikt:μορφή|μορφή]] ''morphe'' \"form\" or \"shape\") is a [[homomorphism]] or [[morphism]] (i.e. a [[Map (mathematics)|mathematical mapping]]) that can be reversed by an [[inverse function|inverse]] morphism. Two [[mathematical object]]s are '''isomorphic''' if an isomorphism exists between them.  An ''[[automorphism]]'' is an isomorphism whose source and target coincide.  The interest of isomorphisms lies in the fact that two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences.\n\nFor most [[algebraic structure]]s, including [[group (mathematics)|group]]s and [[ring (mathematics)|ring]]s, a homomorphism is an isomorphism if and only if it is [[bijective]].\n\nIn [[topology]], where the morphisms are [[continuous function]]s, isomorphisms are also called ''[[homeomorphism]]s'' or ''bicontinuous functions''. In [[mathematical analysis]], where the morphisms are [[differentiable function]]s, isomorphisms are also called ''[[diffeomorphism]]s''.\n\nA '''canonical isomorphism''' is a [[canonical map]] that is an isomorphism. Two objects are said to be '''canonically isomorphic''' if there is a canonical isomorphism between them. For example, the canonical map from a finite-dimensional vector space ''V'' to its second dual space is a canonical isomorphism; on the other hand, ''V'' is isomorphic to its dual space but not canonically in general.\n\nIsomorphisms are formalized using [[category theory]].  A morphism {{nowrap|''f'' : ''X'' → ''Y''}} in a category is an isomorphism if it admits a two-sided inverse, meaning that there is another morphism {{nowrap|''g'' : ''Y'' → ''X''}} in that category such that {{nowrap|''gf'' {{=}} 1<sub>''X''</sub>}} and {{nowrap|''fg'' {{=}} 1<sub>''Y''</sub>}}, where 1<sub>''X''</sub> and 1<sub>''Y''</sub> are the identity morphisms of ''X'' and ''Y'', respectively.<ref>{{cite book|author=Awodey, Steve|chapter=Isomorphisms|title=Category theory|publisher=Oxford University Press|year=2006|isbn=9780198568612|page=11|url=https://books.google.com/books?id=IK_sIDI2TCwC&pg=PA11}}</ref>\n\n==Examples==<!-- This section is linked from [[List of small groups]] -->\n\n===Logarithm and exponential===\nLet <math>\\mathbb{R}^+</math> be the [[multiplicative group]] of [[positive real numbers]], and let <math>\\mathbb{R}</math> be the additive group of real numbers.\n\nThe [[logarithm function]] <math>\\log \\colon \\mathbb{R}^+ \\to \\mathbb{R}</math> satisfies <math>\\log(xy) = \\log x + \\log y</math> for all <math>x,y \\in \\mathbb{R}^+</math>, so it is a [[group homomorphism]].  The [[exponential function]] <math>\\exp \\colon \\mathbb{R} \\to \\mathbb{R}^+</math> satisfies <math>\\exp(x+y) =  (\\exp x)(\\exp y)</math> for all <math>x,y \\in \\mathbb{R}</math>, so it too is a homomorphism.\n\nThe identities <math>\\log \\exp x = x</math> and <math>\\exp \\log y = y</math> show that <math>\\log</math> and <math>\\exp</math> are [[inverse function|inverses]] of each other.  Since <math>\\log</math> is a homomorphism that has an inverse that is also a homomorphism, <math>\\log</math> is an isomorphism of groups.\n\nBecause <math>\\log</math> is an isomorphism, it translates multiplication of positive real numbers into addition of real numbers.  This facility makes it possible to multiply real numbers using a [[ruler]] and a [[table of logarithms]], or using a [[slide rule]] with a logarithmic scale.\n\n===Integers modulo 6===\nConsider the group <math>(\\mathbb{Z}_6, +)</math>, the integers from 0 to 5 with addition [[modular arithmetic|modulo]]&nbsp;6.  Also consider the group <math>(\\mathbb{Z}_2 \\times \\mathbb{Z}_3, +)</math>, the ordered pairs where the ''x'' coordinates can be 0 or 1, and the y coordinates can be 0, 1, or 2, where addition in the ''x''-coordinate is modulo 2 and addition in the ''y''-coordinate is modulo 3.\n\nThese structures are isomorphic under addition, under the following scheme:\n\n:(0,0) ↦ 0\n:(1,1) ↦ 1\n:(0,2) ↦ 2\n:(1,0) ↦ 3\n:(0,1) ↦ 4\n:(1,2) ↦ 5\n\nor in general {{nowrap|(''a'',''b'') ↦ (3''a'' + 4''b'')}} mod 6.\n\nFor example, {{nowrap|1=(1,1) + (1,0) = (0,1)}}, which translates in the other system as {{nowrap|1=1 + 3 = 4}}.\n\nEven though these two groups \"look\" different in that the sets contain different elements, they are indeed '''isomorphic''': their structures are exactly the same. More generally, the [[direct product of groups|direct product]] of two [[cyclic group]]s <math>\\mathbb{Z}_m</math> and <math>\\mathbb{Z}_n</math> is isomorphic to <math>(\\mathbb{Z}_{mn}, +)</math> if and only if ''m'' and ''n'' are [[coprime]], per the [[Chinese remainder theorem]].\n\n===Relation-preserving isomorphism===\nIf one object consists of a set ''X'' with a [[binary relation]] R and the other object consists of a set ''Y'' with a binary relation S then an isomorphism from ''X'' to ''Y'' is a bijective function {{nowrap|1=ƒ: ''X'' → ''Y''}} such that:<ref>{{Cite book|author=Vinberg, Ėrnest Borisovich|title=A Course in Algebra|publisher=American Mathematical Society|year=2003|isbn=9780821834138|page=3|url=https://books.google.com/books?id=kd24d3mwaecC&pg=PA3}}</ref>\n:<math> \\operatorname{S}(f(u),f(v)) \\iff \\operatorname{R}(u,v) </math>\n\nS is [[reflexive relation|reflexive]], [[irreflexive relation|irreflexive]], [[symmetric relation|symmetric]], [[antisymmetric relation|antisymmetric]], [[asymmetric relation|asymmetric]], [[transitive relation|transitive]], [[total relation|total]], [[Binary relation#Relations over a set|trichotomous]], a [[partial order]], [[total order]], [[well-order]], [[strict weak order]], [[Strict weak order#Total preorders|total preorder]] (weak order), an [[equivalence relation]], or a relation with any other special properties, if and only if R is.\n\nFor example, R is an [[Order theory|ordering]] ≤ and S an ordering <math>\\scriptstyle \\sqsubseteq</math>, then an isomorphism from ''X'' to ''Y'' is a bijective function {{nowrap|1=ƒ: ''X'' → ''Y''}} such that\n:<math>f(u) \\sqsubseteq f(v) \\iff u \\le v . </math>\nSuch an isomorphism is called an ''[[order isomorphism]]'' or (less commonly) an ''isotone isomorphism''.\n\nIf {{nowrap|1=''X'' = ''Y''}}, then this is a relation-preserving [[automorphism]].\n\n==Isomorphism vs. bijective morphism==\nIn a [[concrete category]] (that is, roughly speaking, a category whose objects are sets and morphisms are mappings between sets), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets.  In algebraic categories (specifically, categories of [[variety (universal algebra)|varieties in the sense of universal algebra]]), an isomorphism is the same as a homomorphism which is bijective on underlying sets.  However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces), and there are categories in which each object admits an underlying set but in which isomorphisms need not be bijective (such as the homotopy category of CW-complexes).\n\n==Applications==\nIn [[abstract algebra]], two basic isomorphisms are defined:\n* [[Group isomorphism]], an isomorphism between [[group (mathematics)|groups]]\n* [[Ring isomorphism]], an isomorphism between [[ring (mathematics)|rings]]. (Isomorphisms between [[field (mathematics)|fields]] are actually ring isomorphisms)\n\nJust as the [[automorphism]]s of an [[algebraic structure]] form a [[group (mathematics)|group]], the isomorphisms between two algebras sharing a common structure form a [[heap (mathematics)|heap]]. Letting a particular isomorphism identify the two structures turns this heap into a group.\n\nIn [[mathematical analysis]], the [[Laplace transform]] is an isomorphism mapping hard [[differential equations]] into easier [[algebra]]ic equations.\n\nIn [[category theory]], let the [[category (mathematics)|category]] ''C'' consist of two [[class (set theory)|classes]], one of ''objects'' and the other of [[morphisms]]. Then a general definition of isomorphism that covers the previous and many other cases is: an isomorphism is a morphism {{nowrap|1=ƒ: ''a'' → ''b''}} that has an inverse, i.e. there exists a morphism {{nowrap|1=''g'': ''b'' → ''a''}} with {{nowrap|1=''ƒg'' = 1<sub>''b''</sub>}} and {{nowrap|1=''gƒ'' = 1<sub>''a''</sub>}}. For example, a bijective [[linear map]] is an isomorphism between [[vector space]]s, and a bijective [[continuous function]] whose inverse is also continuous is an isomorphism between [[topological space]]s, called a [[homeomorphism]].\n\nIn [[graph theory]], an isomorphism between two graphs ''G'' and ''H'' is a [[bijective]] map ''f'' from the vertices of ''G'' to the vertices of ''H'' that preserves the \"edge structure\" in the sense that there is an edge from [[vertex (graph theory)|vertex]] ''u'' to vertex ''v'' in ''G'' if and only if there is an edge from ƒ(''u'') to ƒ(''v'') in ''H''. See [[graph isomorphism]].\n\nIn mathematical analysis, an isomorphism between two [[Hilbert spaces]] is a bijection preserving addition, scalar multiplication, and inner product.\n\nIn early theories of [[logical atomism]], the formal relationship between facts and true propositions was theorized by [[Bertrand Russell]] and [[Ludwig Wittgenstein]] to be isomorphic. An example of this line of thinking can be found in Russell's ''[[Introduction to Mathematical Philosophy]]''.\n\nIn [[cybernetics]], the [[good regulator]] or Conant–Ashby theorem is stated \"Every good regulator of a system must be a model of that system\". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.\n\n==Relation with equality==\n{{See also|Equality (mathematics)}}\n\nIn certain areas of mathematics, notably [[category theory]], it is valuable to distinguish between ''[[Equality (mathematics)|equality]]'' on the one hand and ''isomorphism'' on the other.<ref>{{Harvnb|Mazur|2007}}</ref> Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the sets\n:<math>A = \\{ x \\in \\mathbb{Z} \\mid x^2 < 2\\}</math> and <math>B = \\{-1, 0, 1\\} \\,</math>\nare ''equal''; they are merely different representations—the first an [[intensional definition|intensional]] one (in [[set builder notation]]), and the second [[extensional definition|extensional]] (by explicit enumeration)—of the same subset of the integers. By contrast, the sets {''A'',''B'',''C''} and {1,2,3} are not ''equal''—the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their [[cardinality]] (number of elements) and these both have three elements, but there are many choices of isomorphism—one isomorphism is\n:<math>\\text{A} \\mapsto 1, \\text{B} \\mapsto 2, \\text{C} \\mapsto 3,</math> while another is <math>\\text{A} \\mapsto 3, \\text{B} \\mapsto 2, \\text{C} \\mapsto 1,</math>\nand no one isomorphism is intrinsically better than any other.<ref group=\"note\">''A'', ''B'', ''C'' have a conventional order, namely alphabetical order, and similarly 1, 2, 3 have the order from the integers, and thus one particular isomorphism is \"natural\", namely\n:<math>\\text{A} \\mapsto 1, \\text{B} \\mapsto 2, \\text{C} \\mapsto 3</math>.\nMore formally, as ''sets'' these are isomorphic, but not naturally isomorphic (there are multiple choices of isomorphism), while as ''ordered sets'' they are naturally isomorphic (there is a unique isomorphism, given above), since [[finite total order]]s are uniquely determined up to unique isomorphism by [[cardinality]].\n\nThis intuition can be formalized by saying that any two finite [[totally ordered set]]s of the same cardinality have a natural isomorphism, the one that sends the [[least element]] of the first to the least element of the second, the least element of what remains in the first to the least element of what remains in the second, and so forth, but in general, pairs of sets of a given finite cardinality are not naturally isomorphic because there is more than one choice of map—except if the cardinality is 0 or 1, where there is a unique choice.</ref><ref group=\"note\">In fact, there are precisely <math>3! = 6</math> different isomorphisms between two sets with three elements. This is equal to the number of [[automorphism]]s of a given three-element set (which in turn is equal to the order of the [[symmetric group]] on three letters), and more generally one has that the set of isomorphisms between two objects, denoted <math>\\operatorname{Iso}(A,B),</math> is a [[torsor]] for the automorphism group of ''A,'' <math>\\operatorname{Aut}(A)</math> and also a torsor for the automorphism group of ''B.'' In fact, automorphisms of an object are a key reason to be concerned with the distinction between isomorphism and equality, as demonstrated in the effect of change of basis on the identification of a vector space with its dual or with its double dual, as elaborated in the sequel.</ref> On this view and in this sense, these two sets are not equal because one cannot consider them ''identical'': one can choose an isomorphism between them, but that is a weaker claim than identity—and valid only in the context of the chosen isomorphism.\n\nSometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the [[genealogy|genealogical]] relationships among [[Joseph Kennedy|Joe]], [[John F. Kennedy|John]], and [[Robert F. Kennedy|Bobby]] Kennedy are, in a real sense, the same as those among the [[American football]] [[quarterbacks]] in the Manning family: [[Archie Manning|Archie]], [[Peyton Manning|Peyton]], and [[Eli Manning|Eli]]. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word ''isomorphism'' (Greek ''iso''-, \"same,\" and -''morph'', \"form\" or \"shape\"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.\n\nAnother example is more formal and more directly illustrates the motivation for distinguishing equality from isomorphism: the distinction between a [[finite-dimensional vector space]] ''V'' and its [[dual space]] {{nowrap|1=''V''* = { φ: V → '''K''' }}} of linear maps from ''V'' to its field of scalars '''K'''.\nThese spaces have the same dimension, and thus are isomorphic as abstract vector spaces (since algebraically, vector spaces are classified by dimension, just as sets are classified by cardinality), but there is no \"natural\" choice of isomorphism <math>\\scriptstyle V \\, \\overset{\\sim}{\\to} \\, V^*</math>.\nIf one chooses a basis for ''V'', then this yields an isomorphism: For all {{nowrap|1=''u''. ''v'' ∈ ''V''}},\n:<math>v \\ \\overset{\\sim}{\\mapsto} \\ \\phi_v \\in V^* \\quad \\text{such that} \\quad \\phi_v(u) = v^\\mathrm{T} u</math>.\n\nThis corresponds to transforming a [[column vector]] (element of ''V'') to a [[row vector]] (element of ''V''*) by [[transpose]], but a different choice of basis gives a different isomorphism: the isomorphism \"depends on the choice of basis\".\nMore subtly, there ''is'' a map from a vector space ''V'' to its [[double dual]] {{nowrap|1= ''V''** = { ''x'': ''V''* → '''K''' }}} that does not depend on the choice of basis: For all {{nowrap|1=''v'' ∈ ''V'' and φ ∈ ''V''*,}}\n:<math>v \\ \\overset{\\sim}{\\mapsto} \\ x_v \\in V^{**} \\quad \\text{such that} \\quad x_v(\\phi) = \\phi(v)</math>.\n\nThis leads to a third notion, that of a [[natural isomorphism]]: while ''V'' and ''V''** are different sets, there is a \"natural\" choice of isomorphism between them.\nThis intuitive notion of \"an isomorphism that does not depend on an arbitrary choice\" is formalized in the notion of a [[natural transformation]]; briefly, that one may ''consistently'' identify, or more generally map from, a finite-dimensional vector space to its double dual, <math>\\scriptstyle V \\, \\overset{\\sim}{\\to} \\, V^{**}</math>, for ''any'' vector space in a consistent way.\nFormalizing this intuition is a motivation for the development of category theory.\n\nHowever, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a [[universal property]]. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of [[real number]]s, which may be defined through infinite decimal expansion, infinite binary expansion, [[Cauchy sequence]]s, [[Dedekind cut]]s and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of \"''the'' set of the real numbers\". The same occurs with [[quotient space (topology)|quotient space]]s: they are commonly constructed as sets of [[equivalence class]]es. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called \"points\", and a surjective map onto this set.\n\nIf one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write {{math|≈}} for an [[unnatural isomorphism]] and {{math|<big>≅</big>}} for a natural isomorphism, as in {{math|1=''V'' ≈  ''V''*}} and {{math|1=''V'' <big>≅</big> ''V''**.}}\nThis convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.\n\nGenerally, saying that two objects are ''equal'' is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional space\n:<math>S^2 := \\{ (x,y,z) \\in \\mathbb{R}^3 \\mid x^2 + y^2 + z^2 = 1\\}</math> and the [[Riemann sphere]] <math>\\widehat{\\mathbb{C}}</math>\n\nwhich can be presented as the [[one-point compactification]] of the complex plane {{math|1='''C''' ∪ {∞}}} ''or'' as the complex [[projective line]] (a quotient space)\n:<math>\\mathbf{P}_{\\mathbb{C}}^1 := (\\mathbb{C}^2\\setminus \\{(0,0)\\}) / (\\mathbb{C}^*)</math>\n\nare three different descriptions for a mathematical object, all of which are isomorphic, but not ''equal'' because they are not all subsets of a single space: the first is a subset of '''R'''<sup>3</sup>, the second is {{math|1='''C''' ≅ '''R'''}}<sup>2</sup><ref group=\"note\">Being precise, the identification of the complex numbers with the real plane,\n:<math>\\mathbf{C} \\cong \\mathbf{R}\\cdot 1 \\oplus \\mathbf{R} \\cdot i = \\mathbf{R}^2</math>\n\ndepends on a choice of <math>i;</math> one can just as easily choose <math>(-i),</math>, which yields a different identification—formally, [[complex conjugation]] is an automorphism—but in practice one often assumes that one has made such an identification.</ref> plus an additional point, and the third is a [[subquotient]] of '''C'''<sup>2</sup>\n\nIn the context of category theory, objects are usually at most isomorphic—indeed, a motivation for the development of category theory was showing that different constructions in [[homology theory]] yielded equivalent (isomorphic) groups. Given maps between two objects ''X'' and ''Y'', however, one asks if they are equal or not (they are both elements of the set Hom(''X'',&nbsp;''Y''), hence equality is the proper relationship), particularly in [[commutative diagram]]s.\n\n==See also==\n{{Portal|Mathematics}}\n*[[Bisimulation]]\n*[[Heap (mathematics)]]\n*[[Isometry]]\n*[[Isomorphism class]]\n*[[Isomorphism theorem]]\n*[[Universal property]]\n\n==Notes==\n{{Reflist|group=note}}\n\n==References==\n{{refimprove|date=September 2010}}\n{{Reflist}}\n\n==Further reading==\n* {{Citation | first = Barry | last = Mazur | authorlink = Barry Mazur | title = When is one thing equal to some other thing? | date = 12 June 2007 | url = http://www.math.harvard.edu/~mazur/preprints/when_is_one.pdf | ref = harv}}\n\n==External links==\n{{Wiktionary|isomorphism}}\n*{{springer|title=Isomorphism|id=p/i052840}}\n*{{planetmath reference|id=1936|title=Isomorphism}}\n*{{MathWorld | urlname=Isomorphism | title = Isomorphism}}\n\n[[Category:Morphisms]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Isomorphism of categories",
      "url": "https://en.wikipedia.org/wiki/Isomorphism_of_categories",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Relation of categories in category theory}}\nIn [[category theory]], two categories ''C'' and ''D'' are '''isomorphic''' if there exist [[functor]]s ''F'' : ''C'' → ''D'' and ''G'' : ''D'' → ''C'' which are mutually inverse to each other, i.e. ''FG'' = 1<sub>''D''</sub> (the identity functor on ''D'') and ''GF'' = 1<sub>''C''</sub>.<ref name=\"catswork\">{{cite book |last=Mac Lane |first=Saunders |title=[[Categories for the Working Mathematician]] |publisher=Springer-Verlag |year=1998 |edition=2nd |series=Graduate Texts in Mathematics | volume=5 |authorlink=Saunders Mac Lane |isbn=0-387-98403-8 |ref=harv | mr=1712872 | page=14}}</ref> This means that both the [[object (category theory)|object]]s and the [[morphism]]s of ''C'' and ''D'' stand in a one-to-one correspondence to each other. Two isomorphic categories share all properties that are defined solely in terms of category theory; for all practical purposes, they are identical and differ only in the notation of their objects and morphisms.\n\nIsomorphism of categories is a very strong condition and rarely satisfied in practice. Much more important is the notion of [[equivalence of categories]]; roughly speaking, for an equivalence of categories we don't require that <math>FG</math> be ''equal'' to <math>1_D</math>, but only ''[[natural transformation|naturally isomorphic]]'' to <math>1_D</math>, and likewise that <math>GF</math> be naturally isomorphic to <math>1_C</math>.\n\n==Properties==\nAs is true for any notion of [[isomorphism]], we have the following general properties formally similar to an [[equivalence relation]]:\n* any category ''C'' is isomorphic to itself\n* if ''C'' is isomorphic to ''D'', then ''D'' is isomorphic to ''C''\n* if ''C'' is isomorphic to ''D'' and ''D'' is isomorphic to ''E'', then ''C'' is isomorphic to ''E''.\n\nA functor ''F'' : ''C'' → ''D'' yields an isomorphism of categories if and only if it is [[bijective]] on objects and on [[Hom set|morphism sets]].<ref name=\"catswork\"/> This criterion can be convenient as it avoids the need to construct the inverse functor ''G''. (We use \"bijection\" informally here because, if a category is not [[Concrete category|concrete]], we don't have such a notion.)\n\n==Examples==\n* {{anchor|Category of representations}} Consider a finite [[group (mathematics)|group]] ''G'', a [[field (mathematics)|field]] ''k'' and the [[Group ring#Group algebra over a finite group|group algebra]] ''kG''. The category of ''k''-linear [[group representation]]s of ''G'' is isomorphic to the category of [[module (mathematics)|left module]]s over ''kG''. The isomorphism can be described as follows: given a group representation ρ : ''G'' → GL(''V''), where ''V'' is a [[vector space]] over ''k'', GL(''V'') is the group of its ''k''-linear [[automorphism]]s, and ρ is a [[group homomorphism]], we turn ''V'' into a left ''kG'' module by defining\n:<math display=\"block\">\\left(\\sum_{g\\in G} a_g g\\right) v = \\sum_{g\\in G} a_g \\rho(g)(v)</math>\nfor every ''v'' in ''V'' and every element Σ ''a<sub>g</sub>'' ''g'' in ''kG''.\nConversely, given a left ''kG'' module ''M'', then ''M'' is a ''k'' vector space, and multiplication with an element ''g'' of ''G'' yields a ''k''-linear automorphism of ''M'' (since ''g'' is invertible in ''kG''), which describes a group homomorphism ''G'' → GL(''M''). (There are still several things to check: both these assignments are functors, i.e. they can be applied to maps between group representations resp. ''kG'' modules, and they are inverse to each other, both on objects and on morphisms). See also [[Representation theory of finite groups#Representations, modules and the convolution algebra]].\n* Every [[ring (mathematics)|ring]] can be viewed as a [[preadditive category]] with a single object. The [[functor category]] of all [[additive functor]]s from this category to the [[category of abelian groups]] is isomorphic to the category of left modules over the ring.\n* Another isomorphism of categories arises in the theory of [[Boolean algebra (structure)|Boolean algebra]]s: the category of Boolean algebras is isomorphic to the category of [[Boolean ring]]s. Given a Boolean algebra ''B'', we turn ''B'' into a Boolean ring by using the [[symmetric difference]] as addition and the meet operation <math>\\land</math> as multiplication. Conversely, given a Boolean ring ''R'', we define the join operation by ''a''<math>\\lor</math>''b'' =  ''a'' + ''b'' + ''ab'', and the meet operation as multiplication. Again, both of these assignments can be extended to morphisms to yield functors, and these functors are inverse to each other.\n* If ''C'' is a category with an initial object s, then the [[slice category]] (''s''↓''C'') is isomorphic to ''C''. [[Dual (category theory)|Dually]], if ''t'' is a terminal object in ''C'', the functor category (''C''↓''t'') is isomorphic to ''C''. Similarly, if '''1''' is the category with one object and only its identity morphism (in fact, '''1''' is the [[terminal object|terminal category]]), and ''C'' is any category, then the functor category ''C''<sup>'''1'''</sup>, with objects functors ''c'': '''1''' → ''C'', selecting an object ''c''∈Ob(''C''), and arrows natural transformations ''f'': ''c'' → ''d'' between these functors, selecting a morphism ''f'': ''c'' → ''d'' in ''C'', is again isomorphic to ''C''. \n\n== See also ==\n\n* [[Equivalence of categories]]\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Isomorphism Of Categories}}\n[[Category:Adjoint functors]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "K-equivalence",
      "url": "https://en.wikipedia.org/wiki/K-equivalence",
      "text": "In [[mathematics]], '''<math>\\mathcal{K}</math>-equivalence''', or '''contact equivalence''', is an [[equivalence relation]] between [[germ (mathematics)|map germs]].  It was introduced by [[John Mather (mathematician)|John Mather]] in his seminal work in [[Singularity theory]] in the 1970s as a technical tool for studying stable maps.  Since then it has proved important in its own right.  Roughly speaking, two map germs ''&fnof;'',&nbsp;''g'' are <math>\\scriptstyle\\mathcal{K}</math>-equivalent if ''&fnof;''<sup>&minus;1</sup>(0) and ''g''<sup>&minus;1</sup>(0) are [[diffeomorphic]].\n\n==Definition==\nTwo map germs <math>f,g:X \\to (Y,0)</math> are <math>\\scriptstyle\\mathcal{K}</math>-equivalent if there is a [[diffeomorphism]] \n:<math>\\Psi: X \\times Y \\to X\\times Y</math>\nof the form &Psi;(x,y) = (&phi;(x),&psi;(x,y)), satisfying,\n:<math>\\Psi(x,0) = (\\varphi(x), 0)</math>, and \n:<math>\\Psi(x,f(x)) = (\\varphi(x), g(\\varphi(x)))</math>.\nIn other words, &Psi; maps the graph of ''f'' to the graph of ''g'', as well as the graph of the zero map to itself.  In particular, the diffeomorphism &phi; maps ''f''<sup>&minus;1</sup>(0) to ''g''<sup>&minus;1</sup>(0). The name ''contact'' is explained by the fact that this equivalence is measuring the contact between the graph of ''f'' and the graph of the zero map.\n\nContact equivalence is the appropriate equivalence relation for studying the sets of solution of equations, and finds many applications in [[dynamical systems]] and [[bifurcation theory]], for example.\n\nIt is easy to see that this equivalence relation is ''weaker'' than [[A-equivalence]], in that any pair of <math>\\scriptstyle\\mathcal{A}</math>-equivalent map germs are necessarily <math>\\scriptstyle\\mathcal{K}</math>-equivalent.\n\n==K<sub>V</sub>-equivalence==\nThis modification of <math>\\scriptstyle\\mathcal{K}</math>-equivalence was introduced by [[James Damon]] in the 1980s. Here ''V'' is a subset (or subvariety) of ''Y'', and the diffeomorphism &Psi; above is required to preserve not <math>X\\times\\{0\\}</math> but <math>X\\times V</math> (that is, <math>y\\in V \\Rightarrow \\psi(x,y)\\in V</math>). In particular, &Psi; maps ''f''<sup>&minus;1</sup>(V) to ''g''<sup>&minus;1</sup>(V).\n\n==See also==\n* [[A-equivalence]]\n\n==References==\n* J. Martinet, ''Singularities of Smooth Functions and Maps'', Volume 58 of LMS Lecture Note Series. Cambridge University Press, 1982.\n* J. Damon, ''The Unfolding and Determinacy Theorems for Subgroups of <math>\\scriptstyle{\\mathcal{A}}</math> and <math>\\scriptstyle\\mathcal{K}</math>''.  Memoirs Amer. Math. Soc.  '''50''', no. 306 (1984).\n\n[[Category:Functions and mappings]]\n[[Category:Singularity theory]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Logical equivalence",
      "url": "https://en.wikipedia.org/wiki/Logical_equivalence",
      "text": "In [[logic]], statements <math>p</math> and <math>q</math> are '''logically equivalent''' if they have the same logical content. That is, if they have the same [[truth value]] in every [[model (logic)|model]] (Mendelson 1979:56). The logical equivalence of <math>p</math> and <math>q</math> is sometimes expressed as <math>p \\equiv q</math>, <math>\\textsf{E}pq</math>, or <math>p \\iff q</math>.\nHowever, these symbols are also used for [[material equivalence]]. Proper interpretation depends on the context. Logical equivalence is different from material equivalence, although the two concepts are closely related.\n\n==Logical equivalences==\n{| class=\"wikitable\"\n|-\n! ''Equivalence'' !! ''Name''\n|-\n| <math>p \\wedge \\top \\equiv p</math><br /><math>p \\vee \\bot \\equiv p</math> || Identity laws\n|-\n| <math>p \\vee \\top \\equiv \\top</math><br /><math>p \\wedge \\bot \\equiv \\bot</math> || Domination laws\n|-\n| <math>p \\vee p \\equiv p</math><br /><math>p \\wedge p \\equiv p</math> || Idempotent laws\n|-\n| <math>\\neg (\\neg p) \\equiv p</math> || Double negation law\n|-\n| <math>p \\vee q \\equiv q \\vee p</math><br /><math>p \\wedge q \\equiv q \\wedge p</math> || Commutative laws\n|-\n| <math>(p \\vee q) \\vee r \\equiv p \\vee (q \\vee r)</math><br /><math>(p \\wedge q) \\wedge r \\equiv p \\wedge (q \\wedge r) </math>|| Associative laws\n|-\n| <math>p \\vee (q \\wedge r) \\equiv (p \\vee q) \\wedge (p \\vee r)</math><br /><math>p \\wedge (q \\vee r) \\equiv (p \\wedge q) \\vee (p \\wedge r)</math> || Distributive laws\n|-\n| <math>\\neg (p \\wedge q) \\equiv  \\neg p \\vee \\neg q</math><br /><math>\\neg (p \\vee q) \\equiv  \\neg p \\wedge \\neg q</math> || De Morgan's laws\n|-\n| <math>p \\vee (p \\wedge q) \\equiv p</math><br /><math>p \\wedge (p \\vee q) \\equiv p</math> || Absorption laws\n|-\n| <math>p \\vee \\neg p \\equiv \\top</math><br /><math>p \\wedge \\neg p \\equiv \\bot</math> || Negation laws\n|}\n\nLogical equivalences involving conditional statements：<br />\n:#<math>p \\implies q \\equiv \\neg p \\vee q</math>\n:#<math>p \\implies q \\equiv \\neg q \\implies \\neg p</math>\n:#<math>p \\vee q \\equiv \\neg p \\implies q</math>\n:#<math>p \\wedge q \\equiv \\neg (p \\implies \\neg q)</math>\n:#<math>\\neg (p \\implies q) \\equiv p \\wedge \\neg q</math>\n:#<math>(p \\implies q) \\wedge (p \\implies r) \\equiv p \\implies (q \\wedge r)</math>\n:#<math>(p \\implies q) \\vee (p \\implies r) \\equiv p \\implies (q \\vee r)</math>\n:#<math>(p \\implies r) \\wedge (q \\implies r) \\equiv (p \\vee q) \\implies r</math>\n:#<math>(p \\implies r) \\vee (q \\implies r) \\equiv (p \\wedge q) \\implies r</math>\n\nLogical equivalences involving biconditionals：<br />\n:#<math>p \\iff q \\equiv (p \\implies q) \\wedge (q \\implies p)</math>\n:#<math>p \\iff q \\equiv \\neg p \\iff \\neg q</math>\n:#<math>p \\iff q \\equiv (p \\wedge q) \\vee (\\neg p \\wedge \\neg q)</math>\n:#<math>\\neg (p \\iff q) \\equiv p \\iff \\neg q</math>\n\n==Example==\nThe following statements are logically equivalent:\n\n#If Lisa is in [[Denmark]], then she is in [[Europe]].  (In symbols, <math>d \\implies e</math>.)\n#If Lisa is not in Europe, then she is not in Denmark.  (In symbols, <math>\\neg e \\implies \\neg d</math>.)\n\nSyntactically, (1) and (2) are derivable from each other via the rules of [[contraposition]] and [[double negation]].  Semantically, (1) and (2) are true in exactly the same models (interpretations, valuations); namely, those in which either ''Lisa is in Denmark'' is false or ''Lisa is in Europe'' is true.\n\n(Note that in this example [[classical logic]] is assumed.  Some [[non-classical logic]]s do not deem (1) and (2) logically equivalent.)\n\n==Relation to material equivalence==\n\nLogical equivalence is different from material equivalence. Formulas <math>p</math> and <math>q</math> are logically equivalent if and only if the statement of their material equivalence (<math>p \\iff q</math>)  is a tautology (Copi et at. 2014:348).{{full citation needed|date=February 2019}} \n\nThe material equivalence of <math>p</math> and <math>q</math> (often written <math>p \\iff q</math>) is itself another statement in the same [[formal system|object language]] as <math>p</math> and <math>q</math>.  This statement expresses the idea \"'<math>p</math> if and only if <math>q</math>'\". In particular, the truth value of <math>p \\iff q</math> can change from one model to another.\n\nThe claim that two formulas are logically equivalent is a statement in the [[metalanguage]], expressing a relationship between two statements <math>p</math> and <math>q</math>.   The statements are logically equivalent if, in every model, they have the same truth value.\n\n==See also==\n{{Portal|Thinking}}\n* [[Logical consequence|Entailment]]\n* [[Equisatisfiability]]\n* [[If and only if]]\n* [[Logical biconditional]]\n* [[Logical equality]]\n\n== References ==\n\n* Irving M. Copi, Carl Cohen, and Kenneth McMahon, [https://books.google.com/books?id=1iUDDQAAQBAJ&printsec=frontcover#v=onepage&q=%22logical%20equivalence%22&f=false Introduction to Logic], 14th edition, Pearson New International Edition, 2014. \n* Elliot Mendelson, ''Introduction to Mathematical Logic'', second edition, 1979.\n\n\n{{DEFAULTSORT:Logical Equivalence}}\n[[Category:Mathematical logic]]\n[[Category:Metalogic]]\n[[Category:Logical consequence]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Matrix congruence",
      "url": "https://en.wikipedia.org/wiki/Matrix_congruence",
      "text": "In [[mathematics]], two [[Square matrix|square matrices]]  '''''A''''' and '''''B''''' over a [[Field (mathematics)|field]] are called '''congruent''' if there exists an [[invertible matrix]] '''''P'''''  over the same field such that \n\n:'''''P'''''<sup>T</sup>'''''AP''''' = '''''B'''''\n\nwhere \"T\" denotes the [[matrix transpose]].  Matrix congruence is an [[equivalence relation]].\n\nMatrix congruence arises when considering the effect of [[change of basis]] on the [[Gram matrix]] attached to a [[bilinear form]] or [[quadratic form]] on a [[finite-dimensional]] [[vector space]]: two matrices are congruent if and only if they represent the same bilinear form with respect to different [[Basis (linear algebra)|bases]].\n\nNote that [[Paul Halmos|Halmos]] defines congruence in terms of [[conjugate transpose]] (with respect to a complex [[inner product space]]) rather than transpose,<ref>{{cite book|last=Halmos | first=Paul R. | authorlink=Paul Halmos | title=Finite dimensional vector spaces | publisher=[[van Nostrand]] | year=1958 | page=134 }}</ref> but this definition has not been adopted by most other authors.\n\n==Congruence over the reals==\n\n[[Sylvester's law of inertia]] states that two congruent [[symmetric matrix|symmetric]] matrices with [[real number|real]] entries have the same numbers of positive, negative, and zero [[eigenvalue]]s.  That is, the number of eigenvalues of each sign is an invariant of the associated quadratic form.<ref name=\"sylvester\">{{cite journal|author=Sylvester, J J | title=A demonstration of the theorem that every homogeneous quadratic polynomial is reducible by real orthogonal substitutions to the form of a sum of positive and negative squares | journal=Philosophical Magazine | volume=IV | pages=138–142 | year=1852 |  url=http://www.maths.ed.ac.uk/~aar/sylv/inertia.pdf | accessdate=2007-12-30}}</ref>\n\n==See also==\n*[[Congruence relation]]\n*[[Matrix similarity]]\n*[[Matrix equivalence]]\n\n==References==\n<references/>\n* {{cite book|author=Gruenberg, K.W. |author2=Weir, A.J. | title=Linear geometry | publisher=van Nostrand | year=1967 | page=80 }}\n* {{cite book|author=Hadley, G. | title=Linear algebra | publisher=[[Addison-Wesley]] | year=1961 | page=253 }}\n* {{cite book|author=Herstein, I.N. | authorlink=Israel Nathan Herstein| title=Topics in algebra | publisher=[[John Wiley & Sons|Wiley]] | year=1975 |  isbn=0-471-02371-X | page=352 }}\n* {{cite book|author=Mirsky, L. | authorlink=Leon Mirsky|title=An introduction to linear algebra | publisher=[[Dover Publications]] | year=1990 |  isbn=0-486-66434-1 | page=182 }}\n* {{cite book|author=Marcus, Marvin |author2=Minc, Henryk | title=A survey of matrix theory and matrix inequalities | publisher=Dover Publications | year=1992 |  isbn=0-486-67102-X | page=81 }}\n* {{cite book|author=Norman, C.W. | title=Undergraduate algebra | publisher=[[Oxford University Press]] | year=1986 |  isbn=0-19-853248-2 | page=354 }}\n\n[[Category:Linear algebra]]\n[[Category:Matrices]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Matrix equivalence",
      "url": "https://en.wikipedia.org/wiki/Matrix_equivalence",
      "text": "In [[linear algebra]], two rectangular ''m''-by-''n'' [[matrix (mathematics)|matrices]] ''A'' and ''B'' are called '''equivalent''' if\n:<math>\\! B = Q^{-1} A P</math>\nfor some [[invertible matrix|invertible]] ''n''-by-''n'' matrix ''P'' and some invertible ''m''-by-''m'' matrix ''Q''.  Equivalent matrices represent the same [[linear map|linear transformation]] ''V''&nbsp;→&nbsp;''W'' under two different choices of a pair of [[Basis (linear algebra)|bases]] of ''V'' and ''W'', with ''P'' and ''Q'' being the [[change of basis]] matrices in ''V'' and ''W'' respectively.\n\nThe notion of equivalence should not be confused with that of [[Similar matrix|similarity]], which is only defined for square matrices, and is much more restrictive (similar matrices are certainly equivalent, but equivalent square matrices need not be similar). That notion corresponds to matrices representing the same [[endomorphism]] ''V''&nbsp;→&nbsp;''V'' under two different choices of a ''single'' basis of ''V'', used both for initial vectors and their images.\n\n== Properties ==\nMatrix equivalence is an [[equivalence relation]] on the space of rectangular matrices.\n\nFor two rectangular matrices of the same size, their equivalence can also be characterized by the following conditions\n* The matrices can be transformed into one another by a combination of [[elementary row operation|elementary row and column operations]].\n* Two matrices are equivalent if and only if they have the same [[rank of a matrix|rank]].\n\n==Canonical form==\n\nThe [[rank of a matrix|rank]] property yields an intuitive [[canonical form]] for matrices of the equivalence class of rank <math>k</math> as\n\n<math>\n\\begin{pmatrix}\n1 & 0 & 0 & & \\cdots & & 0 \\\\\n0 & 1 & 0 & & \\cdots & & 0 \\\\\n0 & 0 & \\ddots & & & & 0\\\\\n\\vdots & & & 1 & & & \\vdots \\\\\n & & & & 0 & & \\\\\n & & & & & \\ddots &  \\\\\n0 & & & \\cdots & & & 0\n\\end{pmatrix}\n</math>, \n\nwhere the number of <math>1</math>s on the diagonal is equal to <math>k</math>. This is a special case of the [[Smith normal form]], which generalizes this concept on vector spaces to [[free module|free modules]] over [[principal ideal domain|principal ideal domains]].\n\n==See also==\n*[[Matrix similarity]]\n*[[Row equivalence]]\n*[[Matrix congruence]]\n\n[[Category:Matrices]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Matrix similarity",
      "url": "https://en.wikipedia.org/wiki/Matrix_similarity",
      "text": "{{other uses|Similarity (geometry)|Similarity transformation (disambiguation)}}\n{{Distinguish|similarity matrix}}\nIn [[linear algebra]], two ''n''-by-''n'' [[matrix (mathematics)|matrices]] {{mvar|A}} and {{mvar|B}} are called '''similar''' if there exists an [[invertible matrix|invertible]] ''n''-by-''n'' matrix {{mvar|P}} such that\n:<math>B = P^{-1} A P .</math>\nSimilar matrices represent the same [[linear operator]] under two (possibly) different [[Basis (linear algebra)|bases]], with {{mvar|P}} being the [[change of basis]] matrix.<ref>{{harvtxt|Beauregard|Fraleigh|1973|pp=240–243}}</ref><ref>{{harvtxt|Bronson|1970|pp=176–178}}</ref>\n\nA transformation {{math|''A'' ↦ ''P''<sup>−1</sup>''AP''}} is called a '''similarity transformation''' or '''conjugation''' of the matrix {{mvar|A}}.  In the [[general linear group]], similarity is therefore the same as '''[[conjugacy class|conjugacy]]''', and similar matrices are also called '''conjugate'''; however in a given subgroup {{mvar|H}} of the general linear group, the notion of conjugacy may be more restrictive than similarity, since it requires that {{mvar|P}} be chosen to lie in {{mvar|H}}.\n\n== Motivating example ==\n\nWhen defining a linear transformation, it can be the case that a change of basis can result in a simpler form of the same transformation. For example, the matrix representing a rotation in {{math|ℝ<sup>3</sup>}} when the [[axis–angle representation|axis of rotation]] is not aligned with the coordinate axis can be complicated to compute. If the axis of rotation were aligned with the positive {{mvar|z}}-axis, then it would simply be\n:<math>S = \\begin{bmatrix}\n  \\cos\\theta & -\\sin\\theta & 0 \\\\\n  \\sin\\theta &  \\cos\\theta & 0 \\\\\n           0 &           0 & 1\n\\end{bmatrix}</math>,\n\nwhere <math>\\theta</math> is the angle of rotation. In the new coordinate system, the transformation would be written as \n:<math>y' = Sx'</math>,\n\nwhere {{mvar|x'}} and {{mvar|y'}} are the original and transformed vectors in a new basis containing a vector parallel to the axis of rotation. In the original basis, the transform would be written as\n:<math>y = Tx</math>,\n\nwhere vectors {{mvar|x}} and {{mvar|y}} and the unknown transform matrix {{mvar|T}} are in the original basis. To write {{mvar|T}} in terms of the simpler matrix, we use the change-of-basis matrix {{mvar|P}} that transforms {{mvar|x}} and {{mvar|y}} as <math>x' = Px</math> and <math>y' = Py</math>:\n:<math>\\begin{align}\n  &            &y' &= Sx' \\\\\n  &\\Rightarrow &Py &= SPx \\\\\n  &\\Rightarrow & y &= \\left(P^{-1}SP\\right)x = Tx\n\\end{align}</math>\n\nThus, the matrix in the original basis is given by <math>T = P^{-1}SP</math>. The transform in the original basis is found to be the product of three easy-to-derive matrices. In effect, the similarity transform operates in three steps: change to a new basis ({{mvar|P}}), perform the simple transformation ({{mvar|S}}), and change back to the old basis ({{math|P<sup>−1</sup>}}).\n\n== Properties ==\nSimilarity is an [[equivalence relation]] on the space of square matrices.\n\nBecause matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator:\n\n*[[Rank (linear algebra)|Rank]]\n*[[Characteristic polynomial]], and attributes that can be derived from it:\n**[[Determinant]]\n**[[Trace (linear algebra)|Trace]]\n**[[Eigenvalues and eigenvectors|Eigenvalues]], and their [[Algebraic multiplicity|algebraic multiplicities]]\n*[[Geometric multiplicity|Geometric multiplicities]] of eigenvalues (but not the eigenspaces, which are transformed according to the base change matrix ''P'' used).\n*[[Minimal polynomial (linear algebra)|Minimal polynomial]]\n*[[Frobenius normal form]]\n*[[Jordan normal form]], up to a permutation of the Jordan blocks\n*[[Nilpotent matrix|Index of nilpotence]] \n*[[Elementary divisors]], which form a complete set of invariants for similarity of matrices over a [[principal ideal domain]]\n\nBecause of this, for a given matrix ''A'', one is interested in finding a simple \"normal form\" ''B'' which is similar to ''A''—the study of ''A'' then reduces to the study of the simpler matrix ''B''. For example, ''A'' is called [[diagonalizable matrix|diagonalizable]] if it is similar to a [[diagonal matrix]]. Not all matrices are diagonalizable, but at least over the [[complex number]]s (or any [[algebraically closed field]]), every matrix is similar to a matrix in [[Jordan form]]. Neither of these forms is unique (diagonal entries or Jordan blocks may be permuted) so they are not really normal forms; moreover their determination depends on being able to factor the minimal or characteristic polynomial of ''A'' (equivalently to find its eigenvalues). The [[rational canonical form]] does not have these drawbacks: it exists over any field, is truly unique, and it can be computed using only arithmetic operations in the field; ''A'' and ''B'' are similar if and only if they have the same rational canonical form. The rational canonical form is determined by the elementary divisors of ''A''; these can be immediately read off from a matrix in Jordan form, but they can also be determined directly for any matrix by computing the [[Smith normal form]], over the ring of polynomials, of the matrix (with polynomial entries) {{math|''XI''<sub>''n''</sub> − ''A''}} (the same one whose determinant defines the characteristic polynomial). Note that this Smith normal form is not a normal form of ''A'' itself; moreover it is not similar to {{math|''XI''<sub>''n''</sub> − ''A''}} either, but obtained from the latter by left and right multiplications by different invertible matrices (with polynomial entries).\n\nSimilarity of matrices does not depend on the base field: if ''L'' is a field containing ''K'' as a [[Field extension|subfield]], and ''A'' and ''B'' are two matrices over ''K'', then ''A'' and ''B'' are similar as matrices over ''K'' [[if and only if]] they are similar as matrices over ''L''. This is so because the rational canonical form over ''K'' is also the rational canonical form over ''L''. This means that one may use Jordan forms that only exist over a larger field to determine whether the given matrices are similar.\n\nIn the definition of similarity, if the matrix ''P'' can be chosen to be a [[permutation matrix]] then ''A'' and ''B'' are '''permutation-similar;''' if ''P'' can be chosen to be a [[unitary matrix]] then ''A'' and ''B'' are '''unitarily equivalent.'''  The [[spectral theorem]] says that every [[normal matrix]] is unitarily equivalent to some diagonal matrix. [[Specht's theorem]] states that two matrices are unitarily equivalent if and only if they satisfy certain trace equalities.\n\n==See also==\n*[[Canonical form#Linear algebra|Canonical forms]]\n*[[Matrix congruence]]\n*[[Matrix equivalence]]\n\n== Notes ==\n{{reflist}}\n\n==References==\n* {{ citation | first1 = Raymond A. | last1 = Beauregard | first2 = John B. | last2 = Fraleigh | year = 1973 | isbn = 0-395-14017-X | title = A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields | publisher = [[Houghton Mifflin Co.]] | location = Boston }}\n* {{ citation | first1 = Richard | last1 = Bronson | year = 1970 | lccn = 70097490 | title = Matrix Methods:  An Introduction | publisher = [[Academic Press]] | location = New York }}\n* Horn and Johnson, ''Matrix Analysis,'' Cambridge University Press, 1985. {{isbn|0-521-38632-2}}. (Similarity is discussed many places, starting at page 44.)\n\n[[Category:Matrices]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Morita equivalence",
      "url": "https://en.wikipedia.org/wiki/Morita_equivalence",
      "text": "In [[abstract algebra]], '''Morita equivalence''' is a relationship defined between [[ring (mathematics)|rings]] that preserves many ring-theoretic properties. It is named after Japanese mathematician [[Kiiti Morita]] who defined equivalence and a similar notion of duality in 1958.\n\n== Motivation ==\n[[ring (mathematics)|Rings]] are commonly studied in terms of their [[module (mathematics)|modules]], as modules can be viewed as [[representation theory|representations]] of rings. Every ring ''R'' has a natural ''R''-module structure on itself where the module action is defined as the multiplication in the ring, so the approach via modules is more general and gives useful information. Because of this, one often studies a ring by studying the [[category (mathematics)|category]] of modules over that ring. Morita equivalence takes this viewpoint to a natural conclusion by defining rings to be Morita equivalent if their module categories are [[Equivalence of categories|equivalent]]. This notion is of interest only when dealing with [[noncommutative ring]]s, since it can be shown that two [[commutative ring]]s are Morita equivalent if and only if they are [[Glossary of ring theory#Homomorphisms and ideals|isomorphic]].\n\n== Definition ==\nTwo rings ''R'' and ''S'' (associative, with 1) are said to be ('''Morita''') '''equivalent''' if there is an equivalence of the category of (left) modules over ''R'', ''R-Mod'', and the category of (left) modules over ''S'', ''S-Mod''. It can be shown that the left module categories ''R-Mod'' and ''S-Mod'' are equivalent if and only if the right module categories ''Mod-R'' and ''Mod-S'' are equivalent. Further it can be shown that any functor from ''R-Mod'' to ''S-Mod'' that yields an equivalence is automatically [[Additive functor|additive]].\n\n== Examples ==\nAny two isomorphic rings are Morita equivalent.\n\nThe ring of ''n''-by-''n'' [[matrix (mathematics)|matrices]] with elements in ''R'', denoted M<sub>''n''</sub>(''R''), is Morita-equivalent to ''R'' for any ''n > 0''. Notice that this generalizes the classification of simple artinian rings given by [[Artin–Wedderburn theorem|Artin–Wedderburn theory]]. To see the equivalence, notice that if  ''X'' is a left ''R''-module then ''X<sup>n</sup>'' is an M<sub>''n''</sub>(''R'')-module where the module structure is given by matrix multiplication on the left of column vectors from ''X''. This allows the definition of a functor from the category of left ''R''-modules to the category of left M<sub>''n''</sub>(''R'')-modules. The inverse functor is defined by realizing that for any M<sub>''n''</sub>(''R'')-module there is a left ''R''-module ''X'' such that the M<sub>''n''</sub>(''R'')-module is obtained from ''X'' as described above.\n\n== Criteria for equivalence ==\nEquivalences can be characterized as follows: if ''F'':''R-Mod'' <math>\\to</math> ''S-Mod'' and ''G'':''S-Mod''<math>\\to</math> ''R-Mod'' are additive (covariant) [[functors]], then ''F'' and ''G'' are an equivalence if and only if there is a balanced (''S'',''R'')-[[bimodule]] ''P'' such that  <sub>S</sub>''P''  and  ''P''<sub>R</sub>  are [[finitely generated module|finitely generated]] [[projective module|projective]] [[generator (category theory)|generators]] and there are [[natural transformation|natural isomorphisms]] of the functors <math> \\operatorname{F}(-) \\cong P \\otimes_R - </math>, and of the functors <math>\\operatorname{G}(-) \\cong \\operatorname{Hom}(_{S}P,-).</math> Finitely generated projective generators are also sometimes called '''progenerators''' for their module category.<ref name=Sep6>DeMeyer & Ingraham (1971) p.6</ref>\n\nFor every [[exact functor|right-exact]] functor ''F'' from the category of left-''R'' modules to the category of left-''S'' modules that commutes with [[direct sum]]s, a theorem of [[homological algebra]] shows that there is a ''(S,R)''-bimodule ''E'' such that the functor <math>\\operatorname{F}(-)</math> is naturally isomorphic to the functor <math>E \\otimes_R -</math>. Since equivalences are by necessity exact and commute with direct sums, this implies that ''R'' and ''S'' are Morita equivalent if and only if there are bimodules ''<sub>R</sub>M<sub>S</sub>'' and ''<sub>S</sub>N<sub>R</sub>'' such that <math>M \\otimes_{S} N \\cong R</math> as ''(R,R)'' bimodules and <math>N \\otimes_{R} M \\cong S</math> as ''(S,S)'' bimodules.  Moreover, ''N'' and ''M'' are related via an ''(S,R)'' bimodule isomorphism: <math>N \\cong \\operatorname{Hom}(M_S,S_S)</math>.\n\nMore concretely, two rings ''R'' and ''S'' are Morita equivalent if and only if <math>S\\cong \\operatorname{End}(P_R)</math> for a [[progenerator]] module ''P<sub>R</sub>'',<ref name=Sep16>DeMeyer & Ingraham (1971) p.16</ref> which is the case if and only if \n:<math>S\\cong e\\mathbb{M}_{n}(R)e</math> \n(isomorphism of rings) for some positive integer ''n'' and [[Idempotent (ring theory)#Types of ring idempotents|full idempotent]] ''e'' in the matrix ring M<sub>n</sub>(''R'').\n\nIt is known that if ''R'' is Morita equivalent to ''S'', then the ring C(''R'') is isomorphic to the ring C(''S''), where C(-) denotes the [[center of a ring|center of the ring]], and furthermore ''R''/''J''(''R'') is Morita equivalent to ''S''/''J''(''S''), where ''J''(-) denotes the [[Jacobson radical]].\n\nWhile isomorphic rings are Morita equivalent, Morita equivalent rings can be nonisomorphic.  An easy example is that a [[division ring]] ''D'' is Morita equivalent to all of its matrix rings ''M''<sub>''n''</sub>(''D''), but cannot be isomorphic when ''n''&nbsp;>&nbsp;1.  In the special case of commutative rings, Morita equivalent rings are actually isomorphic.  This follows immediately from the comment above, for if ''R'' is Morita equivalent to ''S'', <math>R=\\operatorname{C}(R)\\cong \\operatorname{C}(S)=S</math>. In fact, if ''R'' and ''S'' are isomorphic commutative rings, every equivalence between ''R''-Mod and ''S''-Mod arises up to natural isomorphism from an isomorphism between ''R'' and ''S''.\n\n== Properties preserved by equivalence ==\nMany properties are preserved by the equivalence functor for the objects in the module category.  Generally speaking, any property of modules defined purely in terms of modules and their homomorphisms (and not to their underlying elements or ring) is a '''categorical property''' which will be preserved by the equivalence functor.  For example, if ''F''(-) is the equivalence functor from ''R-Mod'' to ''S-Mod'', then the ''R'' module ''M'' has any of the following properties if and only if the ''S'' module ''F''(''M'') does: [[injective module|injective]], [[projective module|projective]], [[flat module|flat]], [[faithful module|faithful]], [[simple module|simple]], [[semisimple module|semisimple]], [[finitely generated module|finitely generated]], [[Finitely-generated module#Finitely presented.2C finitely related.2C and coherent modules|finitely presented]], [[artinian module|Artinian]], and [[noetherian module|Noetherian]].  Examples of properties not necessarily preserved include being [[free module|free]], and being [[cyclic module|cyclic]].\n\nMany ring theoretic properties are stated in terms of their modules, and so these properties are preserved between Morita equivalent rings.  Properties shared between equivalent rings are called '''Morita invariant''' properties.  For example, a ring ''R'' is [[Semisimple module#Semisimple rings|semisimple]] if and only if all of its modules are semisimple, and since semisimple modules are preserved under Morita equivalence, an equivalent ring ''S'' must also have all of its modules semisimple, and therefore be a semisimple ring itself.\n\nSometimes it is not immediately obvious why a property should be preserved.  For example, using one standard definition of [[von Neumann regular ring]] (for all ''a'' in ''R'', there exists ''x'' in ''R'' such that ''a''&nbsp;=&nbsp;''axa'') it is not clear that an equivalent ring should also be von Neumann regular.  However another formulation is:  a ring is von Neumann regular if and only if all of its modules are flat.  Since flatness is preserved across Morita equivalence, it is now clear that von Neumann regularity is Morita invariant.\n\nThe following properties are Morita invariant:\n*[[Simple ring|simple]], [[Semisimple ring|semisimple]]\n*[[von Neumann regular ring|von Neumann regular]]\n*right (or left) [[Noetherian ring|Noetherian]], right (or left) [[Artinian ring|Artinian]]\n*right (or left) [[Injective module#Self-injective rings|self-injective]]\n*[[quasi-Frobenius ring|quasi-Frobenius]]\n*[[prime ring|prime]], right (or left) [[Primitive ring|primitive]], [[semiprime ring|semiprime]], [[semiprimitive ring|semiprimitive]]\n*right (or left) [[hereditary ring|(semi-)hereditary]]\n*right (or left) [[nonsingular ring|nonsingular]]\n*right (or left) [[coherent ring|coherent]]\n*[[semiprimary ring|semiprimary]], right (or left) [[perfect ring|perfect]], [[semiperfect ring|semiperfect]] \n*[[semilocal ring|semilocal]]\n\nExamples of properties which are ''not'' Morita invariant include [[commutative ring|commutative]], [[local ring|local]], [[reduced ring|reduced]], [[domain (ring theory)|domain]], right (or left) [[Goldie ring|Goldie]], [[Frobenius ring|Frobenius]], [[invariant basis number]], and [[Dedekind finite ring|Dedekind finite]].\n\nThere are at least two other tests for determining whether or not a ring property <math>\\mathcal{P}</math> is Morita invariant.  An element ''e'' in a ring ''R'' is a '''[[Idempotent (ring theory)#Types of ring idempotents|full idempotent]]''' when ''e''<sup>2</sup>&nbsp;=&nbsp;''e'' and ''ReR''&nbsp;=&nbsp;''R''.\n\n*<math>\\mathcal{P}</math> is Morita invariant if and only if whenever a ring ''R'' satisfies <math>\\mathcal{P}</math>, then so does ''eRe'' for every full idempotent ''e'' and so does every matrix ring M<sub>n</sub>(''R'') for every positive integer ''n'';\nor\n*<math>\\mathcal{P}</math> is Morita invariant if and only if: for any ring ''R'' and full idempotent ''e'' in ''R'', ''R'' satisfies <math>\\mathcal{P}</math> if and only if the ring ''eRe'' satisfies <math>\\mathcal{P}</math>.\n\n== Further directions ==\nDual to the theory of equivalences is the theory of [[Opposite category|dualities]] between the module categories, where the functors used are [[Functor#Covariance and contravariance|contravariant]] rather than covariant. This theory, though similar in form, has significant differences because there is no duality between the categories of modules for any rings, although dualities may exist for subcategories. In other words, because infinite dimensional modules{{clarify|what's an infinite-dimensional module?|date=May 2015}} are not generally [[reflexive module|reflexive]], the theory of dualities applies more easily to finitely generated algebras over noetherian rings. Perhaps not surprisingly, the criterion above has an analogue for dualities, where the natural isomorphism is given in terms of the hom functor rather than the tensor functor.\n\nMorita equivalence can also be defined in more structured situations, such as for symplectic groupoids and [[C*-algebra]]s. In the case of C*-algebras, a stronger type equivalence, called '''strong Morita equivalence''', is needed to obtain results useful in applications, because of the additional structure of C*-algebras (coming from the involutive *-operation) and also because C*-algebras do not necessarily have an identity element.\n\n== Significance in K-theory ==\nIf two rings are Morita equivalent, there is an induced equivalence of the respective categories of projective modules since the Morita equivalences will preserve exact sequences (and hence projective modules). Since the [[algebraic K-theory]] of a ring is defined (in [[Q-construction|Quillen's approach]]) in terms of the [[homotopy group]]s of (roughly) the [[classifying space]] of the [[nerve (category theory)|nerve]] of the (small) category of finitely generated projective modules over the ring, Morita equivalent rings must have isomorphic K-groups.\n\n==References==\n{{reflist}}\n* {{cite journal | last=Morita | first=Kiiti | authorlink=Kiiti Morita | title=Duality for modules and its applications to the theory of rings with minimum condition | journal=Science reports of the Tokyo Kyoiku Daigaku. Section A | year=1958 | volume=6 | issue=150 | pages=83–142 | zbl=0080.25702 | issn=0371-3539 }}\n* {{cite book | last1=DeMeyer | first1=F. | last2=Ingraham | first2=E. | title=Separable algebras over commutative rings | series=Lecture Notes in Mathematics | volume=181 | location=Berlin-Heidelberg-New York |publisher=[[Springer-Verlag]] | year=1971 | isbn=978-3-540-05371-2 | zbl=0215.36602 }}\n* {{cite book | first1=F.W. | last1=Anderson | first2=K.R. | last2=Fuller | title=Rings and Categories of Modules | series=[[Graduate Texts in Mathematics]] | volume=13 | edition=2nd | publisher=[[Springer-Verlag]] | location=New York | year=1992 | isbn=0-387-97845-3 | zbl=0765.16001  }}\n* {{cite book | last=Lam | first=T.Y. | authorlink=T. Y. Lam | title=Lectures on Modules and Rings | series=[[Graduate Texts in Mathematics]] | volume=189 | location=New York, NY | publisher=[[Springer-Verlag]] | year=1999 | isbn=978-1-4612-6802-4 | at=Chapters 17-18-19 | zbl=0911.16001 }}\n* {{cite web | last=Meyer | first=Ralf | title=Morita Equivalence In Algebra And Geometry | url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.3449}}\n\n==Further reading==\n* {{cite book | last=Reiner | first=I. | authorlink=Irving Reiner | title=Maximal Orders | series=London Mathematical Society Monographs.  New Series | volume=28 | publisher=[[Oxford University Press]] | year=2003 | isbn=0-19-852673-3 | zbl=1024.16008 | pages=154–169 }}\n\n[[Category:Module theory|* Module]]\n[[Category:Ring theory]]\n[[Category:Adjoint functors]]\n[[Category:Duality theories]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Naimark equivalence",
      "url": "https://en.wikipedia.org/wiki/Naimark_equivalence",
      "text": "In mathematical [[representation theory]], two [[group representation|representations]] of a [[group (mathematics)|group]] on [[topological vector space]]s are called '''Naimark equivalent''' (named after [[Mark Naimark]]) if there is a closed bijective linear map between [[dense subspace]]s preserving the [[Group action (mathematics)|group action]].\n\n==References==\n\n*{{Citation | last1=Warner | first1=Garth | title=Harmonic analysis on semi-simple Lie groups. I | publisher=[[Springer-Verlag]] | location=Berlin, New York |mr=0498999 | year=1972}}\n\n[[Category:Representation theory]]\n[[Category:Equivalence (mathematics)]]\n\n{{abstract-algebra-stub}}"
    },
    {
      "title": "Normalization (statistics)",
      "url": "https://en.wikipedia.org/wiki/Normalization_%28statistics%29",
      "text": "{{other uses|Normalizing constant}}\n{{refimprove|date=July 2012}}\n\nIn [[statistics]] and applications of statistics, '''normalization''' can have a range of meanings.<ref name=Dodge/> In the simplest cases, '''normalization of ratings''' means adjusting values measured on different scales to a notionally common scale, often prior to averaging. In more complicated cases, normalization may refer to more sophisticated adjustments where the intention is to bring the entire [[probability distribution]]s of adjusted values into alignment. In the case of '''normalization of scores''' in educational assessment, there may be an intention to align distributions to a [[normal distribution]]. A different approach to normalization of probability distributions is [[quantile normalization]], where the [[quantile]]s of the different measures are brought into alignment.\n\nIn another usage in statistics, normalization refers to the creation of shifted and scaled versions of statistics, where the intention is that these '''normalized values''' allow the comparison of corresponding normalized values for different datasets in a way that eliminates the effects of certain gross influences, as in an [[anomaly time series]]. Some types of normalization involve only a rescaling, to arrive at values relative to some size variable. In terms of [[levels of measurement]], such ratios only make sense for ''ratio'' measurements (where ratios of measurements are meaningful), not ''interval'' measurements (where only distances are meaningful, but not ratios).\n\nIn theoretical statistics, parametric normalization can often lead to [[pivotal quantity|pivotal quantities]] – functions whose [[sampling distribution]] does not depend on the parameters – and to [[ancillary statistic]]s – pivotal quantities that can be computed from observations, without knowing parameters.\n\n== Examples ==\nThere are different types of normalizations in statistics – nondimensional ratios of errors, residuals, means and [[standard deviation|standard deviations]], which are hence [[scale invariant]] – some of which may be summarized as follows. Note that in terms of [[levels of measurement]], these ratios only make sense for ''ratio'' measurements (where ratios of measurements are meaningful), not ''interval'' measurements (where only distances are meaningful, but not ratios). See also [[:Category:Statistical ratios]].\n{| class=\"wikitable\"\n! Name !!   Formula !! Use\n|-\n| [[Standard score]] || <math>\\frac{X - \\mu}{\\sigma}</math> || Normalizing errors when population parameters are known. Works well for populations that are [[normal distribution|normally distributed]]{{citation needed|date=May 2018}}\n|-\n| [[Student's t-statistic]] || <math>\\frac{X - \\overline{X}}{s}</math> || Normalizing [[Residual (statistics)|residuals]] when population parameters are unknown (estimated).\n|-\n| [[Studentized residual]] || <math>\\frac{\\hat \\epsilon_i}{\\hat \\sigma_i} = \\frac{X_i - \\hat \\mu_i}{\\hat \\sigma_i}</math> || Normalizing residuals when parameters are estimated, particularly across different data points in [[regression analysis]].\n|-\n| [[Standardized moment]] || <math>\\frac{\\mu_k}{\\sigma^k}</math> || Normalizing moments, using the standard deviation <math>\\sigma</math> as a measure of scale.\n|-\n| [[Coefficient of variation|Coefficient of<br> variation]] || <math>\\frac{\\sigma}{\\mu}</math>  || Normalizing dispersion, using the mean <math>\\mu</math> as a measure of scale, particularly for positive distribution such as the [[exponential distribution]] and [[Poisson distribution]].\n|- \n| [[Feature scaling|Min-Max Feature scaling]]  ||<math>X' = \\frac{X - X_{\\min}}{X_{\\max}-X_{\\min}}</math>  || [[Feature scaling]] is used to bring all values into the range [0,1]. This is also called unity-based normalization. This can be generalized to restrict the range of values in the dataset between any arbitrary points <math> a </math> and <math> b </math>, using for example <math> X' = a + \\frac{\\left(X-X_{\\min}\\right)\\left(b-a\\right)}{X_{\\max} - X_{\\min}} </math>.\n|-\n|}\n\nNote that some other ratios, such as the [[variance-to-mean ratio]] <math>\\left(\\frac{\\sigma^2}{\\mu}\\right)</math>, are also done for normalization, but are not nondimensional: the units do not cancel, and thus the ratio has units, and is not scale-invariant.\n\n==Other types==\nOther non-dimensional normalizations that can be used with no assumptions on the distribution include:\n* Assignment of [[percentiles]].  This is common on standardized tests. See also [[quantile normalization]].\n* Normalization by adding and/or multiplying by constants so values fall between 0 and 1.  This used for [[probability density function|probability density functions]], with applications in fields such as physical chemistry in assigning probabilities to {{math|{{abs|''ψ''}}<sup>2</sup>}}.\n\n==See also==\n\n*[[Normal score]]\n\n==References==\n{{reflist|refs=\n\n<ref name=Dodge> [[Yadolah Dodge|Dodge, Y]] (2003) ''The Oxford Dictionary of Statistical Terms'', OUP. {{ISBN|0-19-920613-9}} (entry for normalization of scores)</ref>\n\n}}\n\n\n[[Category:Statistical ratios]]\n[[Category:Statistical data transformation]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Numerical equivalence",
      "url": "https://en.wikipedia.org/wiki/Numerical_equivalence",
      "text": "#REDIRECT [[Adequate equivalence relation]]\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Quantile normalization",
      "url": "https://en.wikipedia.org/wiki/Quantile_normalization",
      "text": "In statistics, '''quantile normalization''' is a technique for making two [[probability distribution|distributions]] identical in statistical properties.  To quantile-normalize a test distribution to a reference distribution of the same length, sort the test distribution and sort the reference distribution. The highest entry in the test distribution then takes the value of the highest entry in the reference distribution, the next highest entry in the reference distribution, and so on, until the test distribution is a perturbation of the reference distribution.\n\nTo [[quantile]] normalize two or more distributions to each other, without a reference distribution, sort as before, then set to the average (usually, [[arithmetic mean]]) of the distributions. So the highest value in all cases becomes the mean of the highest values, the second highest value becomes the mean of the second highest values, and so on.\n \nGenerally a reference distribution will be one of the standard statistical distributions such as the [[Gaussian distribution]] or the [[Poisson distribution]]. The reference distribution can be generated randomly or from taking regular samples from the [[cumulative distribution function]] of the distribution. However, any reference distribution can be used.\n\nQuantile normalization is frequently used in [[microarray]] data analysis. It was introduced as '''quantile standardization'''<ref name=Amaratunga2001>{{Cite journal | last1 = Amaratunga | first1 = D. | last2 = Cabrera | first2 = J. | doi = 10.1198/016214501753381814 | title = Analysis of Data from Viral DNA Microchips | journal = Journal of the American Statistical Association | volume = 96 | issue = 456 | pages = 1161 | year = 2001 | pmid =  | pmc = }}</ref> and then renamed as '''quantile normalization'''.<ref name='boldstad2003'>{{Cite journal | last1 = Bolstad | first1 = B. M. | last2 = Irizarry | first2 = R. A. | last3 = Astrand | first3 = M. | last4 = Speed | first4 = T. P. | title = A comparison of normalization methods for high density oligonucleotide array data based on variance and bias | doi = 10.1093/bioinformatics/19.2.185 | journal = Bioinformatics | volume = 19 | issue = 2 | pages = 185–193 | year = 2003 | pmid =  12538238| pmc = }}</ref>\n\n==Example==\n\nA quick illustration of such normalizing on a very small dataset:\n\nArrays 1 to 3, genes A to D\n\n A    5    4    3\n B    2    1    4\n C    3    4    6\n D    4    2    8\n\nFor each column determine a rank from lowest to highest and assign number i-iv\n\n A    iv    iii   i\n B    i     i     ii\n C    ii    iii   iii\n D    iii   ii    iv\n\nThese rank values are set aside to use later.\nGo back to the first set of data. Rearrange that first set of column values so each column is in order going lowest to highest value. (First column consists of 5,2,3,4. This is rearranged to 2,3,4,5. Second Column 4,1,4,2 is rearranged to 1,2,4,4, and column 3 consisting of 3,4,6,8 stays the same because it is already in order from lowest to highest value.) The result is:\n\n A    5    4    3    becomes A 2 1 3\n B    2    1    4    becomes B 3 2 4\n C    3    4    6    becomes C 4 4 6\n D    4    2    8    becomes D 5 4 8\n\nNow find the mean for each row to determine the ranks\n\n A (2 1 3)/3 = 2.00 = rank i\n B (3 2 4)/3 = 3.00 = rank ii\n C (4 4 6)/3 = 4.67 = rank iii\n D (5 4 8)/3 = 5.67 = rank iv\n\nNow take the ranking order and substitute in new values\n\n A    iv    iii   i\n B    i     i     ii\n C    ii    iii   iii\n D    iii   ii    iv\n\nbecomes:\n\n A    5.67    4.67    2.00\n B    2.00    2.00    3.00\n C    3.00    4.67    4.67\n D    4.67    3.00    5.67\n\nThese are the new normalized values. The new values have the same distribution and can now be easily compared.\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.bea.ki.se/staff/reimers/Web.Pages/Affymetrix.Normalization.htm  Normalization of Affymetrix Chips]\n*[http://www.rci.rutgers.edu/~cabrera/DNAMR/  Quantile Standardization for Microarray data]\n\n[[Category:Statistical data transformation]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Quasi-isometry",
      "url": "https://en.wikipedia.org/wiki/Quasi-isometry",
      "text": "In [[mathematics]], '''quasi-isometry''' is an [[equivalence relation]] on [[metric space]]s that ignores their small-scale details in favor of their [[coarse structure]]. The concept is especially important in [[geometric group theory]] following the work of [[Mikhail Leonidovich Gromov|Gromov]].<ref>{{citation|first=Martin R.|last=Bridson|authorlink=Martin Bridson|contribution=Geometric and combinatorial group theory|pages=431–448|title=The Princeton Companion to Mathematics|editor1-first=Timothy|editor1-last=Gowers|editor1-link=Timothy Gowers|editor2-first=June|editor2-last=Barrow-Green|editor3-first=Imre|editor3-last=Leader|editor3-link=Imre Leader|year=2008|publisher=Princeton University Press|isbn=978-0-691-11880-2|title-link=The Princeton Companion to Mathematics}}\n</ref>\n\n[[File:Equilateral_Triangle_Lattice.svg|thumb|350px|This [[lattice (group)|lattice]] is quasi-isometric to the plane.]]\n\n==Definition==\nSuppose that <math>f</math> is a (not necessarily continuous) function from one metric space <math>(M_1,d_1)</math> to a second metric space <math>(M_2,d_2)</math>. Then <math>f</math> is called a ''quasi-isometry'' from <math>(M_1,d_1)</math> to <math>(M_2,d_2)</math> if there exist constants <math>A\\ge 1</math>, <math>B\\ge 0</math>, and <math>C\\ge 0</math> such that the following two properties both hold:<ref name= \"Topics\" >P. de la Harpe, ''Topics in geometric group theory''. Chicago Lectures in Mathematics. University of Chicago Press, Chicago, IL, 2000. {{isbn|0-226-31719-6}}</ref>\n#For every two points <math>x</math> and <math>y</math> in <math>M_1</math>, the distance between their images is (up to the additive constant <math>B</math>) within a factor of <math>A</math> of their original distance. More formally:\n#:<math>\\forall x,y\\in M_1: \\frac{1}{A}\\; d_1(x,y)-B\\leq d_2(f(x),f(y))\\leq A\\; d_1(x,y)+B.</math>\n#Every point of <math>M_2</math> is within the constant distance <math>C</math> of an image point. More formally:\n#:<math>\\forall z\\in M_2:\\exists x\\in M_1: d_2(z,f(x))\\le C.</math>\n\nThe two metric spaces <math>(M_1,d_1)</math> and <math>(M_2,d_2)</math> are called '''quasi-isometric''' if there exists a quasi-isometry <math>f</math> from <math>(M_1,d_1)</math> to <math>(M_2,d_2)</math>.\n\nA map is called a '''quasi-isometric embedding''' if it satisfies the first condition but not necessarily the second (i.e. it is coarsely [[Lipschitz continuity|Lipschitz]] but may fail to be coarsely surjective).\n\n==Examples==\nThe map between the [[Euclidean plane]] and the plane with the [[Manhattan distance]] that sends every point to itself is a quasi-isometry: in it, distances are multiplied by a factor of at most <math>\\sqrt 2</math>.\n\nThe map <math>f:\\mathbb{Z}^n\\mapsto\\mathbb{R}^n</math> (both with the [[Euclidean metric]]) that sends every <math>n</math>-tuple of integers to itself is a quasi-isometry: distances are preserved exactly, and every real tuple is within distance <math>\\sqrt{n/4}</math> of an integer tuple. In the other direction, the discontinuous function that [[Rounding|rounds]] every tuple of real numbers to the nearest integer tuple is also a quasi-isometry: each point is taken by this map to a point within distance <math>\\sqrt{n/4}</math> of it, so rounding changes the distance between pairs of points by adding or subtracting at most <math>2\\sqrt{n/4}</math>.\n\nEvery pair of finite or bounded metric spaces is quasi-isometric. In this case, every function from one space to the other is a quasi-isometry.\n\n==Equivalence relation==\nIf <math>f:M_1\\mapsto M_2</math> is a quasi-isometry, then there exists a quasi-isometry <math>g:M_2\\mapsto M_1</math>. Indeed, <math>g(x)</math> may be defined by letting <math>y</math> be any point in the image of <math>f</math> that is within distance <math>C</math> of <math>x</math>, and letting <math>g(x)</math> be any point in <math>f^{-1}(y)</math>.\n\nSince the [[identity function|identity map]] is a quasi-isometry, and the [[functional composition|composition]] of two quasi-isometries is a quasi-isometry, it follows that the relation of being quasi-isometric is an [[equivalence relation]] on the class of metric spaces.\n\n==Use in geometric group theory==\nGiven a finite [[generating set of a group|generating set]] ''S'' of a finitely generated [[group (mathematics)|group]] ''G'', we can form the corresponding [[Cayley graph]] of ''S'' and ''G''. This graph becomes a metric space if we declare the length of each edge to be 1. Taking a different finite generating set ''T'' results in a different graph and a different metric space, however the two spaces are quasi-isometric.<ref>R.B. Sher and R.J. Daverman (2002), ''Handbook of Geometric Topology'', North-Holland. {{isbn|0-444-82432-4}}.</ref> This quasi-isometry class is thus an [[Invariant (mathematics)|invariant]] of the group ''G''. Any property of metric spaces that only depends on a space's quasi-isometry class immediately yields another invariant of groups, opening the field of group theory to geometric methods.\n\nMore generally, the '''[[Švarc–Milnor lemma]]''' states that if a group ''G'' acts [[Properly discontinuous action|properly discontinuously]] with compact quotient on a proper geodesic space ''X'' then ''G'' is quasi-isometric to ''X'' (meaning that any Cayley graph for ''G'' is). This gives new examples of groups quasi-isometric to each other: \n* If ''G' '' is a subgroup of finite [[Index of a subgroup|index]] in ''G'' then ''G' '' is quasi-isometric to ''G''; \n* If ''G'' and ''H'' are the fundamental groups of two compact [[hyperbolic manifold]]s of the same dimension ''d'' then they are both quasi-isometric to the hyperbolic space '''H'''<sup>''d''</sup> and hence to each other; on the other hand there are infinitely many quasi-isometry classes of fundamental groups of finite-volume.<ref>{{cite journal | ref=harv | last=Schwartz | first=Richard | title=The Quasi-Isometry Classification of Rank One Lattices | journal=I.H.E.S. Publications Mathematiques | date=1995 | volume=82 | pages=133&ndash;168| doi=10.1007/BF02698639 }}</ref>\n\n== Quasigeodesics and the Morse lemma ==\n\nA ''quasi-geodesic'' in a metric space <math>(X, d)</math> is a quasi-isometric embedding of <math>\\mathbb R</math> into <math>X</math>. More precisely a map <math>\\phi: \\mathbb R \\to X</math> such that there exists <math>C,K > 0</math> so that \n:<math>\\forall s, t \\in \\mathbb R : C^{-1} |s - t| - K \\le d(\\phi(t), \\phi(s)) \\le C|s - t| + K</math>\nis called a <math>(C,K)</math>-quasi-geodesic. Obviously geodesics (parametrised by arclength) are quasi-geodesics. The fact that in some spaces the converse is coarsely true, i.e. that every quasi-geodesic stays within bounded distance of a true geodesic, is called the ''Morse Lemma'' (not to be confused with the perhaps more widely known [[Morse lemma]] in differential topology). Formally the statement is:\n\n:''Let <math>\\delta, C, K > 0</math> and <math>X</math> a proper [[δ-hyperbolic space]]. There exists <math>M</math> such that for any <math>(C, K)</math>-quasi-geodesic there exists a geodesic <math>L</math> in <math>X</math> such that <math>d(\\phi(t), L) \\le M</math> for all <math>t \\in \\mathbb R</math>. ''\n\nIt is an important tool in geometric group theory. An immediate application is that any quasi-isometry between proper hyperbolic spaces induces a homeomorphism between their boundaries. This result is the first step in the proof of the [[Mostow rigidity theorem]].\n\n==Examples of quasi-isometry invariants of groups==\nThe following are some examples of properties of group Cayley graphs that are invariant under quasi-isometry:<ref name = \"Topics\" />\n\n===Hyperbolicity===\n{{main article|Hyperbolic group}}\nA group is called ''hyperbolic'' if one of its Cayley graphs is a δ-hyperbolic space for some δ. When translating between different definitions of hyperbolicity, the particular value of δ may change, but the resulting notions of a hyperbolic group turn out to be equivalent.\n\nHyperbolic groups have a solvable [[word problem for groups|word problem]]. They are [[biautomatic group|biautomatic]] and [[automatic group|automatic]].:<ref name=charney>{{citation | last=Charney | first=Ruth | title=Artin groups of finite type are biautomatic | journal=Mathematische Annalen | volume= 292 | year=1992 | doi=10.1007/BF01444642 | pages=671–683}}</ref> indeed, they are [[automatic group|strongly geodesically automatic]], that is, there is an automatic structure on the group, where the language accepted by the word acceptor is the set of all geodesic words.\n\n===Growth===\n{{main article|Growth rate (group theory)}}\nThe '''growth rate''' of a [[group (mathematics)|group]] with respect to a symmetric [[generating set of a group|generating set]] describes the size of balls in the group.  Every element in the group can be written as a product of generators, and the growth rate counts the number of elements that can be written as a product of length ''n''.\n\nAccording to [[Gromov's theorem on groups of polynomial growth|Gromov's theorem]], a group of polynomial growth is [[virtually nilpotent]], i.e. it has a [[nilpotent group|nilpotent]] [[subgroup]] of finite [[Index of a subgroup|index]].  In particular, the order of polynomial growth <math>k_0</math> has to be a [[natural numbers|natural number]] and in fact <math>\\#(n)\\sim n^{k_0}</math>.\n\nIf <math>\\#(n)</math> grows more slowly than any exponential function, ''G'' has a '''subexponential growth rate'''. Any such group is [[amenable group|amenable]].\n\n===Ends===\n{{main article|End (topology)}}\nThe '''ends''' of a [[topological space]] are, roughly speaking, the [[connected component (topology)|connected components]] of the “ideal boundary” of the space.  That is, each end represents a topologically distinct way to move to [[infinity]] within the space.  Adding a point at each end yields a [[Compactification (mathematics)|compactification]] of the original space, known as the '''end compactification'''.\n\nThe ends of a [[finitely generated group]] are defined to be the ends of the corresponding [[Cayley graph]]; this definition is insensitive to the choice of generating set.  Every finitely-generated infinite group has either 1, 2, or infinitely many ends, and [[Stallings theorem about ends of groups]] provides a decomposition for groups with more than one end.\n\n===Amenability===\n{{main article|Amenable group}}\nAn '''amenable group''' is a [[locally compact]] [[topological group]] ''G'' carrying a kind of averaging operation on bounded functions that is [[Invariant (mathematics)|invariant]] under translation by group elements. The original definition, in terms of a finitely additive invariant measure (or mean) on subsets of ''G'', was introduced by [[John von Neumann]] in 1929 under the [[German language|German]] name \"messbar\" (\"measurable\" in English) in response to the [[Banach–Tarski paradox]]. In 1949 Mahlon M. Day introduced the English translation \"amenable\", apparently as a pun.<ref>Day's first published use of the word is in his abstract for an AMS summer meeting in 1949, [http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle=euclid.bams/1183514222 ''Means on semigroups and groups'', Bull. A.M.S. 55 (1949) 1054–1055]. Many text books on amenabilty, such as Volker Runde's, suggest that Day chose the word as a pun.</ref>\n\nIn [[discrete group theory]], where ''G'' has the [[discrete topology]], a simpler definition is used.  In this setting, a group is amenable if one can say what proportion of ''G'' any given subset takes up.\n\nIf a group has a [[Følner sequence]] then it is automatically amenable.\n\n===Asymptotic cone===\n{{main article|Ultralimit#Asymptotic_cones}}\nAn '''ultralimit''' is a geometric construction that assigns to a sequence of [[metric space]]s ''X<sub>n</sub>'' a limiting metric space. An important class of ultralimits are the so-called ''asymptotic cones'' of metric spaces. Let (''X'',''d'') be a metric space, let ''&omega;'' be a non-principal ultrafilter on <math>\\mathbb N </math> and let ''p<sub>n</sub>''&nbsp;∈&nbsp;''X'' be a sequence of base-points. Then the ''&omega;''&ndash;ultralimit of the sequence <math>(X, \\frac{d}{n}, p_n)</math> is called the asymptotic cone of ''X'' with respect to ''&omega;'' and <math>(p_n)_n\\,</math> and is denoted <math>Cone_\\omega(X,d, (p_n)_n)\\,</math>. One often takes the base-point sequence to be constant, ''p<sub>n</sub>'' = ''p'' for some ''p &isin; X''; in this case the asymptotic cone does not depend on the choice of ''p &isin; X'' and is denoted by  <math>Cone_\\omega(X,d)\\,</math> or just <math>Cone_\\omega(X)\\,</math>.\n\nThe notion of an asymptotic cone plays an important role in [[geometric group theory]] since asymptotic cones (or, more precisely, their [[homeomorphism|topological types]] and [[Lipschitz continuity|bi-Lipschitz types]]) provide quasi-isometry invariants of metric spaces in general and of finitely generated groups in particular.<ref name=\"Roe\">John Roe. ''Lectures on Coarse Geometry.'' [[American Mathematical Society]], 2003. {{isbn|978-0-8218-3332-2}}</ref> Asymptotic cones also turn out to be a useful tool in the study of [[relatively hyperbolic group]]s and their generalizations.<ref>[[Cornelia Druţu]] and Mark Sapir (with an Appendix by [[Denis Osin]] and [[Mark Sapir]]), ''Tree-graded spaces and asymptotic cones of groups.'' [[Topology (journal)|Topology]], Volume 44 (2005), no. 5, pp. 959&ndash;1058.</ref>\n\n==See also==\n*[[Isometry]]\n\n==References==\n{{reflist}}\n\n[[Category:Geometric group theory]]\n[[Category:Metric geometry]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Quasi-isomorphism",
      "url": "https://en.wikipedia.org/wiki/Quasi-isomorphism",
      "text": "In [[homological algebra]], a branch of [[mathematics]], a '''quasi-isomorphism''' or '''quism''' is a morphism ''A'' → ''B'' of [[chain complex]]es (respectively, cochain complexes) such that the induced morphisms  \n\n:<math>H_n(A_\\bullet) \\to H_n(B_\\bullet)\\ (\\text{respectively, } H^n(A^\\bullet) \\to H^n(B^\\bullet))</math>\n\nof [[homology (mathematics)|homology]] groups (respectively, of cohomology groups) are isomorphisms for all ''n''.\n\nIn the theory of [[model category|model categories]], quasi-isomorphisms are sometimes used as the class of [[weak equivalence (homotopy theory)|weak equivalence]]s when the objects of the category are chain or cochain complexes. This results in a homology-local theory, in the sense of [[Bousfield localization]] in [[homotopy theory]].\n\n==See also==\n* [[Derived category]]\n\n==References==\n*Gelfand, Manin. ''Methods of Homological Algebra'', 2nd ed. Springer, 2000.\n\n[[Category:Algebraic topology]]\n[[Category:Homological algebra]]\n[[Category:Equivalence (mathematics)]]\n{{topology-stub}}"
    },
    {
      "title": "Ramanujan's congruences",
      "url": "https://en.wikipedia.org/wiki/Ramanujan%27s_congruences",
      "text": "In [[mathematics]], '''Ramanujan's congruences''' are some remarkable congruences for the [[Partition (number theory)|partition function]] ''p''(''n''). The mathematician [[Srinivasa Ramanujan]] discovered the congruences\n\n:<math>\n\\begin{align}\np(5k+4) & \\equiv 0 \\pmod 5, \\\\\np(7k+5) & \\equiv 0 \\pmod 7, \\\\\np(11k+6) & \\equiv 0 \\pmod {11}.\n\\end{align}\n</math>\n\nThis means that:\n\n* If a number is 4 more than a multiple of 5, i.e. it is in the sequence\n:: 4, 9, 14, 19, 24, 29, . . .\n: then the number of its partitions is a multiple of 5.\n\n* If a number is 5 more than a multiple of 7, i.e. it is in the sequence\n:: 5, 12, 19, 26, 33, 40, . . .\n: then the number of its partitions is a multiple of 7.\n\n* If a number is 6 more than a multiple of 11, i.e. it is in the sequence\n:: 6, 17, 28, 39, 50, 61, . . .\n: then the number of its partitions is a multiple of 11.\n\n== Background ==\n\nIn his 1919 paper,<ref>{{cite journal |last=Ramanujan |first=S. |title=Congruence properties of partitions |journal=[[Mathematische Zeitschrift]] |volume=9 |issue=1–2 |year=1921 |pages=147–153 |doi=10.1007/bf01378341}}</ref> he proved the first two congruences using the following identities (using [[q-Pochhammer symbol]] notation):\n\n:<math>\n\\begin{align}\n& \\sum_{k=0}^\\infty p(5k+4)q^k=5\\frac{(q^5)_\\infty^5}{(q)_\\infty^6}, \\\\[4pt]\n& \\sum_{k=0}^\\infty p(7k+5)q^k=7\\frac{(q^7)_\\infty^3}{(q)_\\infty^4}+49q\\frac{(q^7)_\\infty^7}{(q)_\\infty^8}.\n\\end{align}\n</math>\n\nHe then stated that \"It appears there are no equally simple properties for any moduli involving primes other than these\".\n\nAfter Ramanujan died in 1920, [[G. H. Hardy]] extracted proofs of all three congruences from an unpublished manuscript of Ramanujan on ''p''(''n'') (Ramanujan, 1921). The proof in this manuscript employs the [[Eisenstein series]].\n\nIn 1944, [[Freeman Dyson]] defined the rank function and conjectured the existence of a [[crank (mathematics)|crank]] function for partitions that would provide a [[combinatorial proof]] of Ramanujan's congruences modulo 11. Forty years later, [[George Andrews (mathematician)|George Andrews]] and [[Frank Garvan]] successfully found such a function, and proved the celebrated result that the crank simultaneously \"explains\" the three Ramanujan congruences modulo 5, 7 and 11.\n\nIn the 1960s, [[A. O. L. Atkin]] of the [[University of Illinois at Chicago]] discovered additional congruences for small prime moduli. For example:\n:<math>p(11^3 \\cdot 13k + 237)\\equiv 0 \\pmod {13}.</math>\n\nExtending the results of A. Atkin, [[Ken Ono]] in 2000 proved that there are such Ramanujan congruences modulo every integer coprime to 6. For example, his results give\n:<math>p(107^4\\cdot 31k + 30064597)\\equiv 0\\pmod{31}.</math>\nLater [[Ken Ono]] conjectured that the elusive crank also satisfies exactly the same types of general congruences. This was proved by his Ph.D. student [[Karl Mahlburg]] in his 2005 paper ''Partition Congruences and the Andrews–Garvan–Dyson Crank'', linked below. This paper won the first [[Proceedings of the National Academy of Sciences]] Paper of the Year prize.<ref>{{cite web | url=http://www.pnas.org/site/misc/cozzarelliprize.xhtml | title=Cozzarelli Prize | publisher=[[National Academy of Sciences]] | date=June 2014 | accessdate=2014-08-06 }}</ref>\n\nA conceptual explanation for Ramanujan's observation was finally discovered in January 2011 <ref>{{cite journal |doi=10.1016/j.aim.2011.11.013|title=ℓ-Adic properties of the partition function|journal=[[Advances in Mathematics]]|volume=229|issue=3|pages=1586|year=2012|last1=Folsom|first1=Amanda|author1-link=Amanda Folsom|last2=Kent|first2=Zachary A.|last3=Ono|first3=Ken|author3-link=Ken Ono}}</ref> by considering the [[Hausdorff dimension]] of the following <math>P</math> function in the [[P-adic number|l-adic]] topology:\n:<math>P_\\ell(b;z) := \\sum_{n=0}^\\infty p\\left(\\frac{\\ell^bn+1}{24}\\right)q^{n/24}.</math>\n\nIt is seen to have dimension 0 only in the cases where ''ℓ''&nbsp;= 5,&nbsp;7&nbsp;or&nbsp;11 and since the partition function can be written as a linear combination of these functions<ref>{{cite journal |last=Bruinier  |first=J. H. |last2=Ono |first2=K. |year= 2011|title=Algebraic Formulas for the Coefficients of Half-Integral Weight Harmonic Weak Maas Forms |url=http://www.aimath.org/news/partition/brunier-ono.pdf |journal= |volume= |issue= |pages= |arxiv=1104.1182|bibcode=2011arXiv1104.1182H}}</ref> this can be considered a formalization and proof of Ramanujan's observation.\n\nIn 2001, R.L. Weaver gave an effective algorithm for finding congruences of the partition function, and tabulated 76,065 congruences.<ref>{{cite journal |doi=10.1023/A:1011493128408|year=2001|last1=Weaver|first1=Rhiannon L.|title=New congruences for the partition function|journal=[[The Ramanujan Journal]]|volume=5|pages=53–63}}</ref> This was extended in 2012 by F. Johansson to 22,474,608,014 congruences,<ref>{{cite journal |doi=10.1112/S1461157012001088|title=Efficient implementation of the Hardy–Ramanujan–Rademacher formula|journal=[[LMS Journal of Computation and Mathematics]]|volume=15|pages=341–359|year=2012|last1=Johansson|first1=Fredrik|arxiv=1205.5991}}</ref> one large example being\n:<math>p(999959^4\\cdot29k+ 28995221336976431135321047) \\equiv 0 \\pmod{29}.</math>\n\n== See also ==\n* [[Ramanujan tau function|Tau-function]], for which there are other so-called ''Ramanujan congruences''\n* [[Rank of a partition]]\n* [[Crank of a partition]]\n\n== References ==\n{{Reflist}}\n* {{cite journal | last=Ono | first=Ken | authorlink=Ken Ono | title=Distribution of the partition function modulo m | zbl=0984.11050 | journal=[[Annals of Mathematics]] |series=Second Series | volume=151 | number=1 | pages=293–307 | year=2000 | doi=10.2307/121118 | url=http://www.math.princeton.edu/~annals/issues/2000/151_1.html | jstor=121118 | arxiv=math/0008140 }}\n* {{cite book | last=Ono | first=Ken | authorlink=Ken Ono | title=The web of modularity: arithmetic of the coefficients of modular forms and q-series | zbl=1119.11026 | series=CBMS Regional Conference Series in Mathematics | volume=102 | location=Providence, RI | publisher=[[American Mathematical Society]] | isbn=978-0-8218-3368-1 | year=2004 }}\n* {{cite journal | last=Ramanujan | first=S. | title=Some properties of p(n), the number of partitions of n | journal=[[Proceedings of the Cambridge Philosophical Society]] | volume=19 | year=1919 | pages=207–210 | jfm=47.0885.01 }}\n\n==External links==\n*{{cite journal |last1=Mahlburg |first1=K.|url=http://math.mit.edu/~mahlburg/preprints/mahlburg-CrankCong.pdf|title= Partition Congruences and the Andrews–Garvan–Dyson Crank|journal= [[Proceedings of the National Academy of Sciences]]|volume=102|issue=43|pages= 15373–76|year=2005|doi= 10.1073/pnas.0506702102|pmid= 16217020|pmc=1266116|bibcode=2005PNAS..10215373M}}\n* [http://www.math.ucla.edu/~pak/papers/dyson.htm Dyson's rank, crank and adjoint]. A list of references.\n\n[[Category:Theorems in number theory]]\n[[Category:Srinivasa Ramanujan]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Rational equivalence",
      "url": "https://en.wikipedia.org/wiki/Rational_equivalence",
      "text": "#REDIRECT [[Adequate equivalence relation]]\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Row equivalence",
      "url": "https://en.wikipedia.org/wiki/Row_equivalence",
      "text": "{{more footnotes|date=September 2012}}\nIn [[linear algebra]], two [[matrix (mathematics)|matrices]] are '''row equivalent''' if one can be changed to the other by a sequence of [[elementary row operations]].  Alternatively, two ''m''&nbsp;&times;&nbsp;''n'' matrices are row equivalent if and only if they have the same [[row space]].  The concept is most commonly applied to matrices that represent [[systems of linear equations]], in which case two matrices of the same size are row equivalent if and only if the corresponding [[systems of linear equations#Homogeneous systems|homogeneous]] systems have the same set of solutions, or equivalently the matrices have the same [[null space]].\n\nBecause elementary row operations are reversible, row equivalence is an [[equivalence relation]].  It is commonly denoted by a [[tilde]] (~).\n\nThere is a similar notion of '''column equivalence''', defined by elementary column operations; two matrices are column equivalent if and only if their transpose matrices are row equivalent. Two rectangular matrices that can be converted into one another allowing both elementary row and column operations are called simply [[Matrix equivalence|equivalent]].\n\n==Elementary row operations==\nAn [[elementary row operations|elementary row operation]] is any one of the following moves:\n# '''Swap:''' Swap two rows of a matrix.\n# '''Scale:''' Multiply a row of a matrix by a nonzero constant.\n# '''Pivot:''' Add a multiple of one row of a matrix to another row.\nTwo matrices ''A'' and ''B'' are '''row equivalent''' if it is possible to transform ''A'' into ''B'' by a sequence of elementary row operations.\n\n==Row space==\n{{main|Row space}}\nThe row space of a matrix is the set of all possible [[linear combination]]s of its row vectors.    If the rows of the matrix represent a [[system of linear equations]], then the row space consists of all linear equations that can be deduced algebraically from those in the system. Two ''m''&nbsp;&times;&nbsp;''n'' matrices are row equivalent if and only if they have the same row space.\n\nFor example, the matrices\n:<math>\\begin{pmatrix}1 & 0 & 0 \\\\ 0 & 1 & 1\\end{pmatrix}\n\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;\n\\begin{pmatrix}1 & 0 & 0 \\\\ 1 & 1 & 1\\end{pmatrix}</math>\nare row equivalent, the row space being all vectors of the form <math>\\begin{pmatrix}a & b & b\\end{pmatrix}</math>.  The corresponding systems of homogeneous equations convey the same information:\n:<math>\\begin{matrix}x = 0 \\\\ y+z=0\\end{matrix}\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;\\begin{matrix} x=0 \\\\ x+y+z=0.\\end{matrix}</math>\nIn particular, both of these systems imply every equation of the form <math>ax+by+bz=0.\\,</math>\n\n==Equivalence of the definitions==\nThe fact that two matrices are row equivalent if and only if they have the same row space is an important theorem in linear algebra.  The proof is based on the following observations:\n# Elementary row operations do not affect the row space of a matrix.  In particular, any two row equivalent matrices have the same row space.\n# Any matrix can be [[row reduction|reduced]] by elementary row operations to a matrix in [[reduced row echelon form]].\n# Two matrices in reduced row echelon form have the same row space if and only if they are equal.\nThis line of reasoning also proves that every matrix is row equivalent to a unique matrix with reduced row echelon form.\n\n==Additional properties==\n* Because the [[null space]] of a matrix is the [[orthogonal complement]] of the [[row space]], two matrices are row equivalent if and only if they have the same null space.\n* The [[rank (linear algebra)|rank]] of a matrix is equal to the [[dimension]] of the row space, so row equivalent matrices must have the same rank.  This is equal to the number of [[pivot element|pivots]] in the reduced row echelon form.\n* A matrix is [[invertible matrix|invertible]] if and only if it is row equivalent to the [[identity matrix]].\n\n==See also==\n*[[Elementary row operations]]\n*[[Row space]]\n*[[Basis (linear algebra)]]\n*[[Row reduction]]\n*[[Row echelon form|(Reduced) row echelon form]]\n\n==References==\n* {{Citation\n | last = Axler\n | first = Sheldon Jay\n | date = 1997\n | title = Linear Algebra Done Right\n | publisher = Springer-Verlag\n | edition = 2nd\n | isbn = 0-387-98259-0\n}}\n* {{Citation\n | last = Lay\n | first = David C.\n | date = August 22, 2005\n | title = Linear Algebra and Its Applications\n | publisher = Addison Wesley\n | edition = 3rd\n | isbn = 978-0-321-28713-7\n}}\n* {{Citation\n |last        = Meyer\n |first       = Carl D.\n |date        = February 15, 2001\n |title       = Matrix Analysis and Applied Linear Algebra\n |publisher   = Society for Industrial and Applied Mathematics (SIAM)\n |isbn        = 978-0-89871-454-8\n |url         = http://www.matrixanalysis.com/DownloadChapters.html\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20010301161440/http://matrixanalysis.com/DownloadChapters.html\n |archivedate = March 1, 2001\n |df          = \n}}\n* {{Citation\n | last = Poole\n | first = David\n | date = 2006\n | title = Linear Algebra: A Modern Introduction\n | publisher = Brooks/Cole\n | edition = 2nd\n | isbn = 0-534-99845-3\n}}\n* {{Citation\n | last = Anton\n | first = Howard\n | date = 2005\n | title = Elementary Linear Algebra (Applications Version)\n | publisher = Wiley International\n | edition = 9th\n}}\n* {{Citation\n | last = Leon\n | first = Steven J.\n | date = 2006\n | title = Linear Algebra With Applications\n | publisher = Pearson Prentice Hall\n | edition = 7th\n}}\n\n==External links==\n{{Wikibooks|Linear Algebra|Row Equivalence|Row Equivalence}}\n[[Category:Linear algebra]]\n[[Category:Matrices]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "S-equivalence",
      "url": "https://en.wikipedia.org/wiki/S-equivalence",
      "text": "{{multiple issues|\n{{Orphan|date=October 2013}}\n{{unreferenced|date=March 2013}}\n}}\n\n'''S-equivalence''' is an [[equivalence relation]] on the families of [[Stable vector bundle|semistable]] [[vector bundle]]s on an [[algebraic curve]].\n\n== Definition ==\nLet ''X'' be a [[projective curve]] over an [[algebraically closed field]] ''k''. A vector bundle on ''X'' can be considered as a locally free [[sheaf (mathematics)|sheaf]]. Every semistable locally free ''E'' on ''X'' admits a Jordan-Hölder [[filtration (mathematics)|filtration]] with stable [[subquotient]]s, i.e.\n:<math> 0 = E_0 \\subseteq E_1 \\subseteq \\ldots \\subseteq E_n = E </math>\nwhere <math>E_i</math> are locally free sheaves on ''X'' and <math>E_i/E_{i-1}</math> are stable. Although the Jordan-Hölder filtration is not unique, the subquotients are, which means that <math>gr E = \\bigoplus_i E_i/E_{i-1}</math> is unique up to isomorphism.\n\nTwo semistable locally free sheaves ''E'' and ''F'' on ''X'' are ''S''-equivalent if ''gr E'' ≅ ''gr F''.\n\n[[Category:Algebraic curves]]\n[[Category:Vector bundles]]\n[[Category:Equivalence (mathematics)]]\n\n{{algebraic-geometry-stub}}"
    },
    {
      "title": "Similarity (geometry)",
      "url": "https://en.wikipedia.org/wiki/Similarity_%28geometry%29",
      "text": "{{short description|Idea in geometry}}\n{{other uses|Similarity (disambiguation)|Similarity transformation (disambiguation)}}\n\n[[Image:Similar-geometric-shapes.svg|thumb|300px|Figures shown in the same color are similar]]\nTwo [[geometry|geometrical]] objects are called '''similar''' if they both have the same [[shape]], or one has the same shape as the mirror image of the other.  More precisely, one can be obtained from the other by uniformly [[Scaling (geometry)|scaling]] (enlarging or reducing), possibly with additional [[Translation (geometry)|translation]], [[Rotation (mathematics)|rotation]] and [[Reflection (mathematics)|reflection]]. This means that either object can be rescaled, repositioned, and reflected, so as to coincide precisely with the other object. If two objects are similar, each is [[congruence (geometry)|congruent]] to the result of a particular uniform scaling of the other.  A modern and novel perspective of similarity is to consider geometrical objects similar if one appears congruent to the other when zoomed in or out at some level.\n\nFor example, all [[circle]]s are similar to each other, all [[square]]s are similar to each other, and all [[equilateral triangle]]s are similar to each other.  On the other hand, [[ellipse]]s are not all similar to each other, [[rectangle]]s  are not all similar to each other, and [[isosceles triangle]]s are not all similar to each other.\n\nIf two angles of a triangle have measures equal to the measures of two angles of another triangle, then the triangles are similar. Corresponding sides of similar polygons are in proportion, and corresponding angles of similar polygons have the same measure.\n\nThis article assumes that a scaling can have a scale factor of 1, so that all congruent shapes are also similar, but some school textbooks specifically exclude congruent triangles from their definition of similar triangles by insisting that the sizes must be different if the triangles are to qualify as similar.\n\n==Similar triangles==\nIn [[geometry]] two triangles, {{math|△''ABC''}} and {{math|△''A′B′C′''}}, are similar if and only if corresponding angles have the same measure : this implies that they are similar if and only if the lengths of [[corresponding sides]] are [[Proportionality (mathematics)|proportional]].<ref>{{harvnb|Sibley|1998|loc=p. 35}}</ref> It can be shown that two triangles having congruent angles (''equiangular triangles'') are similar, that is, the corresponding sides can be proved to be proportional. This is known as the AAA similarity theorem.<ref>{{harvnb|Stahl|2003|loc=p. 127}}. This is also proved in [[Euclid's Elements]], Book VI, Proposition 4.</ref> Note that the \"AAA\" is a mnemonic: each one of the three A's refers to an \"angle\". Due to this theorem, several authors simplify the definition of similar triangles to only require that the corresponding three angles are congruent.<ref>For instance, {{harvnb|Venema|2006|loc=p. 122}} and {{harvnb|Henderson|Taimiṇa|2005|loc=p. 123}}</ref>\n\nThere are several statements each of which is necessary and sufficient for two triangles to be similar:\n\n*The triangles have two congruent angles,<ref>[[Euclid's elements]] Book VI Proposition 4.</ref> which in Euclidean geometry implies that all their angles are congruent.<ref>This statement is not true in [[Non-euclidean geometry]] where the triangle angle sum is not 180 degrees.</ref> That is:\n\n::If {{math|∠''BAC''}} is equal in measure to {{math|∠''B′A′C′''}}, and {{math|∠''ABC''}} is equal in measure to {{math|∠''A′B′C′''}}, then this implies that {{math|∠''ACB''}} is equal in measure to {{math|∠''A′C′B′''}} and the triangles are similar.\n\n*All the corresponding sides have lengths in the same ratio:<ref>[[Euclid's elements]] Book VI Proposition 5</ref>\n\n:: {{math|{{sfrac|''AB''|''A′B′''}} {{=}} {{sfrac|''BC''|''B′C′''}} {{=}} {{sfrac|''AC''|''A′C′''}}}}. This is equivalent to saying that one triangle (or its mirror image) is an [[Homothetic transformation|enlargement]] of the other.\n\n*Two sides have lengths in the same ratio, and the angles included between these sides have the same measure.<ref>[[Euclid's elements]] Book VI Proposition 6</ref> For instance:\n:: {{math|{{sfrac|''AB''|''A′B′''}} {{=}} {{sfrac|''BC''|''B′C′''}}}} and {{math|∠''ABC''}} is equal in measure to {{math|∠''A′B′C′''}}.\nThis is known as the SAS similarity criterion.<ref name=\"Venema 2006 loc=p. 143\">{{harvnb|Venema|2006|loc=p. 143}}</ref> The \"SAS\" is a mnemonic: each one of the two S's refers to a \"side\"; the A refers to an \"angle\" between the two sides.\n\nWhen two triangles {{math|△''ABC''}} and {{math|△''A′B′C′''}} are similar, one writes<ref name=PL>[[Alfred Posamentier|Posamentier, Alfred S.]] and Lehmann, Ingmar. ''The Secrets of Triangles'', Prometheus Books, 2012.</ref>{{rp|p. 22}}\n\n:{{math|△''ABC'' ∼ △''A′B′C′''}}.\n\nThere are several elementary results concerning similar triangles in Euclidean geometry:<ref>{{harvnb|Jacobs|1974|loc=pp. 384 - 393}}</ref>\n* Any two [[equilateral triangle]]s are similar.\n* Two triangles, both similar to a third triangle, are similar to each other ([[Transitive relation|transitivity]] of similarity of triangles).\n* Corresponding [[altitude (triangle)|altitudes]] of similar triangles have the same ratio as the corresponding sides.\n* Two [[right triangle]]s are similar if the [[hypotenuse]] and one other side have lengths in the same ratio.<ref>{{citation|title=Lessons in Geometry, Vol. I: Plane Geometry|first=Jacques|last=Hadamard|authorlink=Jacques Hadamard|publisher=American Mathematical Society|year=2008|isbn=9780821843673|at=Theorem&nbsp;120, p.&nbsp;125|url=https://books.google.com/books?id=SaZwAAAAQBAJ&pg=PA125}}.</ref>\n\nGiven a triangle {{math|△''ABC''}} and a line segment {{math|{{overline|''DE''}}}} one can, with [[Compass-and-straightedge construction|ruler and compass]], find a point {{math|''F''}} such that {{math|△''ABC'' ∼ △''DEF''}}. The statement that the point {{math|''F''}} satisfying this condition exists is Wallis's postulate<ref>Named for [[John Wallis]] (1616–1703)</ref> and is logically equivalent to [[Euclid's Fifth Axiom|Euclid's parallel postulate]].<ref>{{harvnb|Venema|2006|loc=p. 122}}</ref> In [[hyperbolic geometry]] (where Wallis's postulate is false) similar triangles are congruent.\n\nIn the axiomatic treatment of Euclidean geometry given by [[George David Birkhoff|G.D. Birkhoff]] (see [[Birkhoff's axioms]]) the SAS similarity criterion given above was used to replace both Euclid's Parallel Postulate and the SAS axiom which enabled the dramatic shortening of [[Hilbert's axioms]].<ref name=\"Venema 2006 loc=p. 143\"/>\n\nSimilar triangles provide the basis for many [[Synthetic geometry|synthetic]] (without the use of coordinates) proofs in Euclidean geometry. Among the elementary results that can be proved this way are: the [[angle bisector theorem]], the [[geometric mean theorem]], [[Ceva's theorem]], [[Menelaus's theorem]] and the [[Pythagorean theorem]]. Similar triangles also provide the foundations for [[Trigonometry|right triangle trigonometry]].<ref>{{harvnb|Venema|2006|loc=p. 145}}</ref>\n\n==Other similar polygons==\n\nThe concept of similarity extends to [[polygon]]s with more than three sides. Given any two similar polygons, corresponding sides taken in the same sequence (even if clockwise for one polygon and counterclockwise for the other) are [[Proportionality (mathematics)|proportional]] and corresponding angles taken in the same sequence are equal in measure. However, proportionality of corresponding sides is not by itself sufficient to prove similarity for polygons beyond triangles (otherwise, for example, all [[rhombus|rhombi]] would be similar). Likewise, equality of all angles in sequence is not sufficient to guarantee similarity (otherwise all [[rectangle]]s would be similar).  A sufficient condition for similarity of polygons is that corresponding sides and diagonals are proportional.\n\nFor given ''n'', all [[regular polygon|regular ''n''-gons]] are similar.\n\n==Similar curves==\n\nSeveral types of curves have the property that all examples of that type are similar to each other. These include:\n*[[Circle]]s\n*[[Parabola]]s <ref>[https://www.academia.edu/5601461/Similarity_of_Parabolas_-_A_Geometrical_Perspective a proof from academia.edu]</ref>\n*[[Hyperbola]]s of a specific [[eccentricity (mathematics)|eccentricity]]<ref name=\"uluc\">[http://www.geom.uiuc.edu/docs/reference/CRC-formulas/node27.html The shape of an ellipse or hyperbola depends only on the ratio b/a]</ref>\n*[[Ellipse]]s of a specific eccentricity<ref name=\"uluc\" />\n*[[Catenary|Catenaries]]<ref>{{cite web|url=http://xahlee.org/SpecialPlaneCurves_dir/Catenary_dir/catenary.html |title=Catenary |publisher=Xahlee.org |date=2003-05-28 |accessdate=2010-11-17}}</ref>\n*Graphs of the [[logarithm]] function for different bases\n*Graphs of the [[exponential function]] for different bases\n*[[Logarithmic spiral]]s are self-similar\n\n==In Euclidean space==\n\nA '''similarity''' (also called a '''similarity transformation''' or '''similitude''') of a [[Euclidean space]] is a [[bijection]] {{math|''f''}} from the space onto itself that multiplies all distances by the same positive [[real number]] {{math|''r''}}, so that for any two points {{math|''x''}} and {{math|''y''}} we have\n\n:<math>d(f(x),f(y)) = r d(x,y), \\, </math>\n\nwhere \"{{math|''d''(''x'',''y'')}}\" is the [[Euclidean distance]] from {{math|''x''}} to {{math|''y''}}.<ref>{{harvnb|Smart|1998|loc=p. 92}}</ref>  The [[Scalar (mathematics)|scalar]] {{math|''r''}} has many names in the literature including; the ''ratio of similarity'', the ''stretching factor'' and the ''similarity coefficient''. When {{math|''r''}} = 1 a similarity is called an [[Euclidean plane isometry|isometry]] (rigid motion). Two sets are called '''similar''' if one is the image of the other under a similarity.\n\nAs a map {{math|''f'' : ℝ{{sup|''n''}} → ℝ{{sup|''n''}}}}, a similarity of ratio {{math|''r''}} takes the form\n\n:<math>f(x) = rAx + t,</math>\n\nwhere {{math|''A'' ∈ ''O''{{sub|''n''}}(ℝ)}} is an {{math|''n'' × ''n''}} [[orthogonal matrix]] and {{math|''t'' ∈ ℝ{{sup|''n''}}}} is a translation vector.\n\nSimilarities preserve planes, lines, perpendicularity, parallelism, midpoints, inequalities between distances and line segments.<ref>{{harvnb|Yale|1968|loc=p. 47 Theorem 2.1}}</ref> Similarities preserve angles but do not necessarily preserve orientation, ''direct similitudes'' preserve orientation and ''opposite similitudes'' change it.<ref>{{harvnb|Pedoe|1988|loc=pp. 179-181}}</ref>\n\nThe similarities of Euclidean space form a [[Group (mathematics)|group]] under the operation of composition called the ''similarities group ''{{math|''S''}}.<ref>{{harvnb|Yale|1968|loc=p. 46}}</ref> The direct similitudes form a [[normal subgroup]] of {{math|''S''}} and the [[Euclidean group]] {{math|''E''(''n'')}} of isometries also forms a normal subgroup.<ref>{{harvnb|Pedoe|1988|loc=p. 182}}</ref> The similarities group {{math|''S''}} is itself a subgroup of the [[affine group]], so every similarity is an [[affine transformation]].\n<!-- FOR LATER INCLUSION\nA special case is a [[homothetic transformation]] or central similarity: it neither involves rotation nor taking the mirror image. A similarity is a composition of a homothety and an orthogonal transformation. -->\n\nOne can view the Euclidean plane as the [[complex plane]],<ref>This traditional term, as explained in its article, is a misnomer. This is actually the 1-dimensional complex line.</ref> that is, as a 2-dimensional space over the [[real number|reals]]. The 2D similarity transformations can then be expressed in terms of complex arithmetic and are given by {{math|''f''(''z'') {{=}} ''az'' + ''b''}} (direct similitudes) and {{math|''f''(''z'') {{=}} ''a{{overline|z}}'' + ''b''}} (opposite similitudes), where {{math|''a''}} and {{math|''b''}} are complex numbers, {{math|''a'' ≠ 0}}. When {{math|{{!}}''a''{{!}} {{=}} 1}}, these similarities are isometries.\n\n==Ratios of sides, of areas, and of volumes==\n{{Main|Square-cube law}}\nThe ratio between the [[Area (geometry)|areas]] of similar figures is equal to the square of the ratio of corresponding lengths of those figures (for example, when the side of a square or the radius of a circle is multiplied by three, its area is multiplied by nine — i.e. by three squared). The altitudes of similar triangles are in the same ratio as corresponding sides. If a triangle has a side of length {{math|''b''}} and an altitude drawn to that side of length {{math|''h''}} then a similar triangle with corresponding side of length {{math|''kb''}} will have an altitude drawn to that side of length {{math|''kh''}}. The area of the first triangle is, {{math|''A'' {{=}} {{sfrac|1|2}}''bh''}}, while the area of the similar triangle will be {{math|''A′'' {{=}} {{sfrac|1|2}}(''kb'')(''kh'') {{=}} ''k''<sup>2</sup>''A''}}. Similar figures which can be decomposed into similar triangles will have areas related in the same way. The relationship holds for figures that are not rectifiable as well.\n\nThe ratio between the [[volume]]s of similar figures is equal to the cube of the ratio of corresponding lengths of those figures (for example, when the edge of a cube or the radius of a sphere is multiplied by three, its volume is multiplied by 27 — i.e. by three cubed).\n\nGalileo's square–cube law concerns similar solids. If the ratio of similitude (ratio of corresponding sides) between the solids is {{math|''k''}}, then the ratio of surface areas of the solids will be {{math|''k''<sup>2</sup>}}, while the ratio of volumes will be {{math|''k''<sup>3</sup>}}.\n\n==In general metric spaces==\n\n[[Image:Sierpinski triangle (blue).jpg|thumb|300px|[[Sierpiński triangle]]. A space having self-similarity dimension {{math|{{sfrac|log 3|log 2}} {{=}} log<sub>2</sub>3}}, which is approximately 1.58. (From [[Hausdorff dimension]].)]]\n\nIn a general [[metric space]] {{math|(''X'', ''d'')}}, an exact '''similitude''' is a [[function (mathematics)|function]] {{math|''f''}} from the metric space {{math|''X''}} into itself that multiplies all distances by the same positive [[scalar (mathematics)|scalar]] {{math|''r''}}, called {{math|''f''}} 's contraction factor, so that for any two points {{math|''x''}} and {{math|''y''}} we have\n\n:<math>d(f(x),f(y)) = r d(x,y).\\, \\,</math>\n\nWeaker versions of similarity would for instance have {{math|''f''}} be a bi-[[Lipschitz continuity|Lipschitz]] function and the scalar {{math|''r''}} a limit\n\n:<math>\\lim \\frac{d(f(x),f(y))}{d(x,y)} = r. </math>\n\nThis weaker version applies when the metric is an effective resistance on a topologically self-similar set.\n\nA self-similar subset of a metric space {{math|(''X'', ''d'')}} is a set {{math|''K''}} for which there exists a finite set of similitudes {{math|&#123; ''f{{sub|s}}'' &#125;{{sub|''s''∈''S''}}}} with contraction factors {{math|0 ≤ ''r{{sub|s}}'' < 1}} such that {{math|''K''}} is the unique compact subset of {{math|''X''}} for which\n\n:[[File:Epi17.png|thumb|A self-similar set constructed with two similitudes z'=0.1[(4+i)z+4] and z'=0.1[(4+7i)z*+5-2i]]]<math>\\bigcup_{s\\in S} f_s(K)=K. \\,</math>\n\nThese self-similar sets have a self-similar [[Measure (mathematics)|measure]] {{math|''μ{{sup|D}}''}} with dimension {{math|''D''}} given by the formula\n\n:<math>\\sum_{s\\in S} (r_s)^D=1 \\, </math>\n\nwhich is often (but not always) equal to the set's [[Hausdorff dimension]] and [[packing dimension]]. If the overlaps between the {{math|''f{{sub|s}}''(''K'')}} are \"small\", we have the following simple formula for the measure:\n\n:<math>\\mu^D(f_{s_1}\\circ f_{s_2} \\circ \\cdots \\circ f_{s_n}(K))=(r_{s_1}\\cdot r_{s_2}\\cdots r_{s_n})^D.\\,</math>\n\n==Topology==\n{{refimprove section|date=August 2018}}\nIn [[topology]], a [[metric space]] can be constructed by defining a '''similarity''' instead of a [[distance]]. The similarity is a function such that its value is greater when two points are closer (contrary to the distance, which is a measure of '''dissimilarity:''' the closer the points, the lesser the distance).\n\nThe definition of the similarity can vary among authors, depending on which properties are desired. The basic common properties are\n# Positive defined:\n#:<math>\\forall (a,b), S(a,b)\\geq 0</math>\n# Majored by the similarity of one element on itself ('''auto-similarity'''):\n#:<math>S (a,b) \\leq S (a,a) \\quad \\text{and} \\quad \\forall (a,b), S (a,b) = S (a,a) \\Leftrightarrow a=b</math>\n\nMore properties can be invoked, such as '''reflectivity''' (<math>\\forall (a,b)\\ S (a,b) = S (b,a)</math>) or '''finiteness''' (<math>\\forall (a,b)\\ S(a,b) < \\infty</math>). The upper value is often set at 1 (creating a possibility for a probabilistic interpretation of the similitude).\n\nNote that, in the topological sense used here, a similarity is a kind of [[measure (mathematics)|measure]].  This usage is '''not''' the same as the ''similarity transformation'' of the {{alink|In Euclidean space}} and {{alink|In general metric spaces}} sections of this article.\n\n==Self-similarity==\n[[Self-similarity]] means that a pattern is '''non-trivially similar''' to itself, e.g., the set {{math|&#123;…, 0.5, 0.75, 1, 1.5, 2, 3, 4, 6, 8, 12, …&#125;}} of numbers of the form {{math|&#123;2{{sup|''i''}}, 3·2{{sup|''i''}}&#125;}} where {{math|''i''}} ranges over all integers. When this set is plotted on a [[logarithmic scale]] it has one-dimensional [[translational symmetry]]: adding or subtracting the logarithm of two to the logarithm of one of these numbers produces the logarithm of another of these numbers. In the given set of numbers themselves, this corresponds to a similarity transformation in which the numbers are multiplied or divided by two.\n\n==Psychology==\nThe intuition for the notion of geometric similarity already appears in human children, as can be seen in their drawings.<ref>{{Cite thesis|type=Ph.D.|url=https://books.google.com/books?id=-wpbd9YaIa4C&printsec=frontcover&hl=pt-BR#v=onepage&q&f=false|title=Understanding Similarity: Bridging Geometric and Numeric Contexts for Proportional Reasoning|last=Cox|first=Dana Christine|date=2008|publisher=ProQuest|isbn=9780549756576|language=en|deadurl=yes|archiveurl=https://web.archive.org/web/20160601145518/https://books.google.com/books?id=-wpbd9YaIa4C&printsec=frontcover&hl=pt-BR#v=onepage&q&f=false|archivedate=2016-06-01|df=}}</ref>\n\n==See also==\n* [[Congruence (geometry)]]\n* [[Hamming distance]] (string or sequence similarity)\n* [[Inversive geometry#Dilations|Inversive geometry]]\n* [[Jaccard index]]\n* [[Proportionality (mathematics)|Proportionality]]\n* [[Basic proportionality theorem]]\n* [[Semantic similarity]]\n* [[Nearest neighbor search|Similarity search]]\n* [[Similarity space]] on [[Numerical taxonomy]]\n* [[Homoeoid]] (shell of concentric, similar ellipsoids)\n* [[Solution of triangles]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{citation|first1=David W.|last1=Henderson|first2=Daina|last2=Taimina|title=Experiencing Geometry/Euclidean and Non-Euclidean with History|edition=3rd|publisher=Pearson Prentice-Hall|year=2005|isbn=978-0-13-143748-7}}\n* {{citation|first=Harold R.|last=Jacobs|title=Geometry|year=1974|publisher=W.H. Freeman and Co.|isbn=0-7167-0456-0}}\n* {{citation|first=Dan|last=Pedoe|title=Geometry/A Comprehensive Course|year=1988|origyear=1970|publisher=Dover|isbn=0-486-65812-0}}\n* {{citation|first=Thomas Q.|last=Sibley|title=The Geometric Viewpoint/A Survey of Geometries|year=1998|publisher=Addison-Wesley|isbn=978-0-201-87450-1}}\n* {{citation|first=James R.|last=Smart|title=Modern Geometries|edition=5th|publisher=Brooks/Cole|year=1998|isbn=0-534-35188-3}}\n* {{citation|first=Saul|last=Stahl|title=Geometry/From Euclid to Knots|year=2003|publisher=Prentice-Hall|isbn=978-0-13-032927-1}}\n* {{citation|first=Gerard A.|last=Venema|title=Foundations of Geometry|year=2006|publisher=Pearson Prentice-Hall|isbn=978-0-13-143700-5}}\n* {{citation|first=Paul B.|last=Yale|title=Geometry and Symmetry|year=1968|publisher=Holden-Day}}\n\n==Further reading==\n* Judith N. Cederberg (1989, 2001) ''A Course in Modern Geometries'', Chapter 3.12 Similarity Transformations, pp.&nbsp;183&ndash;9, Springer {{isbn|0-387-98972-2}} .\n* [[H.S.M. Coxeter]] (1961,9) ''Introduction to Geometry'', §5 Similarity in the Euclidean Plane, pp.&nbsp;67&ndash;76, §7 Isometry and Similarity in Euclidean Space, pp 96&ndash;104, [[John Wiley & Sons]].\n* Günter Ewald (1971) ''Geometry: An Introduction'', pp 106, 181, [[Wadsworth Publishing]].\n* George E. Martin (1982) ''Transformation Geometry: An Introduction to Symmetry'', Chapter 13 Similarities in the Plane, pp.&nbsp;136&ndash;46, Springer {{isbn|0-387-90636-3}} .\n\n==External links==\n{{commons category|Similarity (geometry)}}\n*[http://www.mathopenref.com/similartriangles.html Animated demonstration of similar triangles]\n\n[[Category:Euclidean geometry]]\n[[Category:Triangle geometry]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Similarity (network science)",
      "url": "https://en.wikipedia.org/wiki/Similarity_%28network_science%29",
      "text": "{{orphan|date=May 2014}}\n\n{{refimprove|date=May 2014}}\n\n{{Network Science}}\n\n'''Similarity''' in network analysis occurs when two nodes (or other more elaborate structures) fall in the same equivalence class. \n\nThere are three fundamental approaches to constructing measures of network similarity: structural equivalence, automorphic equivalence, and regular equivalence.<ref name=\"NewmanNetworks\">Newman, M.E.J. 2010. ''Networks: An Introduction.'' Oxford, UK: Oxford University Press.</ref> There is a hierarchy of the three equivalence concepts: any set of structural equivalences are also automorphic and regular equivalences. Any set of automorphic equivalences are also regular equivalences. Not all regular equivalences are necessarily automorphic or structural; and not all automorphic equivalences are necessarily structural.<ref name=\"Hanneman Riddle 2005\" />\n\n== Visualizing similarity and distance ==\n\n=== Clustering tools ===\n\nAgglomerative [[Hierarchical clustering]] of nodes on the basis of the similarity of their profiles of ties to other nodes provides a joining tree or [[Dendrogram]] that visualizes the degree of similarity among cases - and can be used to find approximate equivalence classes.<ref name=\"Hanneman Riddle 2005\">Hanneman, Robert A. and Mark Riddle.  2005.  Introduction to social network methods.  Riverside, CA:  University of California, Riverside ( published in digital form at http://faculty.ucr.edu/~hanneman/ )</ref>\n\n=== Multi-dimensional scaling tools ===\n{{main article|Multidimensional scaling}}\n\nUsually our goal in equivalence analysis is to identify and visualize \"classes\" or clusters of cases.  In using cluster analysis, we are implicitly assuming that the similarity or distance among cases reflects as single underlying dimension.  It is possible, however, that there are multiple \"aspects\" or \"dimensions\" underlying the observed similarities of cases.  Factor or components analysis could be applied to correlations or covariances among cases.  Alternatively, multi-dimensional scaling could be used (non-metric for data that are inherently nominal or ordinal; metric for valued).<ref name=\"Hanneman Riddle 2005\" />\n\nMDS represents the patterns of similarity or dissimilarity in the tie profiles among the actors (when applied to adjacency or distances) as a \"map\" in multi-dimensional space. This map lets us see how \"close\" actors are, whether they \"cluster\" in multi-dimensional space, and how much variation there is along each dimension.<ref name=\"Hanneman Riddle 2005\" />\n\n== Structural equivalence ==\nTwo vertices of a network are structurally equivalent if they share many of the same neighbors. \n\n[[File:Structural_Equivalence.jpg|thumb|Structural equivalence]]\n\nThere is no actor who has exactly the same set of ties as actor A, so actor A is in a class by itself. The same is true for actors B, C, D and G.  Each of these nodes has a unique set of edges to other nodes. E and F, however, fall in the same structural equivalence class. Each has only one edge; and that tie is to B.  Since E and F have exactly the same pattern of edges with all the vertices, they are structurally equivalent. The same is true in the case of H and I.<ref name=\"Hanneman Riddle 2005\" />\n\nStructural equivalence is the strongest form of similarity. In many real networks exact equivalence may be rare, and it could be useful to ease the criteria and measure approximate equivalence. \n\nA closely related concept is ''institutional equivalence'': two actors (e.g., firms) are institutionally equivalent if they operate in the same set of institutional fields.<ref name=\":0\">{{Cite journal|last=Marquis|first=Christopher|last2=Tilcsik|first2=András|date=2016-10-01|title=Institutional Equivalence: How Industry and Community Peers Influence Corporate Philanthropy|url=http://pubsonline.informs.org/doi/10.1287/orsc.2016.1083|journal=Organization Science|volume=27|issue=5|pages=1325–1341|doi=10.1287/orsc.2016.1083|issn=1047-7039}}</ref> While structurally equivalent actors have identical relational patterns or network positions, institutional equivalence captures the similarity of institutional influences that actors experience from being in the same fields, regardless of how similar their network  positions are. For example, two banks in Chicago might have very different patterns of ties (e.g., one may be a central node, and the other may be in a peripheral position) such that they are not  structural equivalents, but because they both operate in the field of finance and banking and in the same geographically defined field (Chicago), they will be subject to some of the same institutional influences.<ref name=\":0\" />\n\n=== Measures for structural equivalence ===\n==== Cosine similarity ====\n{{main article|Cosine similarity}}\n\nA simple count of common neighbors for two vertices is not on its own a very good measure. One should know the degree of the vertices or how many common neighbors other pairs of vertices has. [[Cosine similarity]] takes into account these regards and also allow for the varying degrees of vertices. Salton proposed that we regard the i-th and j-th rows/columns of the adjacency matrix as two vectors and use the cosine of the angle between them as a [[similarity measure]]. The cosine similarity of i and j is the number of common neighbors divided by the geometric mean of their degrees. <ref>Salton G., Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer, Addison-Wesley, Reading, MA (1989)</ref>\n\nIts value lies in the range from 0 to 1. The value of 1 indicates that the two vertices have exactly the same neighbors while the value of zero means that they do not have any common neighbors. Cosine similarity is technically undefined if one or both of the nodes has zero degree, but according to the convention we say that cosine similarity is 0 in these cases.<ref name=\"NewmanNetworks\">Newman, M.E.J. 2010. ''Networks: An Introduction.'' Oxford, UK: Oxford University Press.</ref>\n\n==== Pearson coefficient ====\n{{main article|Pearson product-moment correlation coefficient}}\n\n[[Pearson product-moment correlation coefficient]] is an alternative method to normalize the count of common neighbors. This method compares the number of common neighbors with the expected value that count would take in a network where vertices are connected randomly. This quantity lies strictly in the range from -1 to 1.<ref name=\"NewmanNetworks\">Newman, M.E.J. 2010. ''Networks: An Introduction.'' Oxford, UK: Oxford University Press.</ref>\n\n==== Euclidean distance ====\n{{main article|Euclidean distance}}\n\n[[Euclidean distance]] is equal to the number of neighbors that differ between two vertices. It is rather a dissimilarity measure, since it is larger for vertices which differ more. It could be normalized by dividing by its maximum value. The maximum means that there are no common neighbors, in which case the distance is equal to the sum of the degrees of the vertices.<ref name=\"NewmanNetworks\">Newman, M.E.J. 2010. ''Networks: An Introduction.'' Oxford, UK: Oxford University Press.</ref>\n\n== Automorphic equivalence == \n\nFormally \"Two vertices are automorphically equivalent if all the vertices can be re-labeled to form an isomorphic graph with the labels of u and v interchanged. Two automorphically equivalent vertices share exactly the same label-independent properties.\"<ref name=\"Borgatti Everett\">Borgatti, Steven, Martin Everett, and Linton Freeman. 1992. UCINET IV Version 1.0 User's Guide. Columbia, SC: Analytic Technologies.</ref>\n\nMore intuitively, actors are automorphically equivalent if we can permute the graph in such a way that exchanging the two actors has no effect on the distances among all actors in the graph.\n\n[[File:Automorphic equivalence.jpg|thumb|Automorphic equivalence]]\n\nSuppose the graph describes the organizational structure of a company.  Actor A is the central headquarter, actors B, C, and D are managers.  Actors E, F and H, I are workers at smaller stores; G is the lone worker at another store.\n\nEven though actor B and actor D are not structurally equivalent (they do have the same boss, but not the same workers), they do seem to be \"equivalent\" in a different sense.  Both manager B and D has a boss (in this case, the same boss), and each has two workers. If we swapped them, and also swapped the four workers, all of the distances among all the actors in the network would be exactly identical. \n\nThere are actually five automorphic equivalence classes: {A}, {B, D}, {C}, {E, F, H, I}, and {G}. Note that the less strict definition of \"equivalence\" has reduced the number of classes.<ref name=\"Hanneman Riddle 2005\" />\n\n== Regular equivalence == \n\nFormally, \"Two actors are regularly equivalent if they are equally related to equivalent others.\" In other words, regularly equivalent vertices are vertices that, while they do not necessarily share neighbors, have neighbors who are themselves similar.<ref name=\"Borgatti Everett\" />\n\nTwo mothers, for example, are equivalent, because each has a similar pattern of connections with a husband, children, etc. The two mothers do not have ties to the same husband or the same children, so they are not structurally equivalent. Because different mothers may have different numbers of husbands and children, they will not be automorphically equivalent.  But they are similar because they have the same relationships with some member or members of another set of actors (who are themselves regarded as equivalent because of the similarity of their ties to a member of the set \"mother\").<ref name=\"Hanneman Riddle 2005\" />\n\n[[File:Regular equivalence.jpg|thumb|Regular equivalence]]\n\nIn the graph there are three regular equivalence classes.  The first is actor A; the second is composed of the three actors B, C, and D; the third consists of the remaining five actors E, F, G, H, and I.\n\nThe easiest class to see is the five actors across the bottom of the diagram (E, F, G, H, and I). These actors are regularly equivalent to one another because:\n\n# they have no tie with any actor in the first class (that is, with actor A) and  \n# each has a tie with an actor in the second class (either B or C or D). \n\nEach of the five actors, then, has an identical pattern of ties with actors in the other classes.\n\nActors B, C, and D form a class similarly. B and D actually have ties with two members of the third class, whereas actor C has a tie to only one member of the third class, but this doesn't matter, as there is a tie to some member of the third class.\n\nActor A is in a class by itself, defined by:\n\n# a tie to at least one member of class two and \n# no tie to any member of class three.<ref name=\"Hanneman Riddle 2005\" />\n\n==References==\n<references />\n\n[[Category:Networks]]\n[[Category:Network theory]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Simple-homotopy equivalence",
      "url": "https://en.wikipedia.org/wiki/Simple-homotopy_equivalence",
      "text": "In [[mathematics]], particularly the area of [[topology]], a '''simple-homotopy equivalence''' is a refinement of the concept of [[homotopy equivalence]]. Two [[CW-complex]]es are simple-homotopy equivalent if they are related by a sequence of [[collapse (topology)|collapses]] and expansions (inverses of collapses), and a homotopy equivalence is a simple homotopy equivalence if it is homotopic to such a map.\n\nThe obstruction to a homotopy equivalence being a simple homotopy equivalence is the [[Whitehead torsion]], <math>\\tau(f).</math>\n\n== See also ==\n* [[Discrete Morse theory]]\n\n==References==\n*{{Citation | last1=Cohen | first1=Marshall M. | title=A course in simple-homotopy theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-90055-9 |mr=0362320 | year=1973}}\n\n[[Category:Homotopy theory]]\n[[Category:Equivalence (mathematics)]]\n\n{{topology-stub}}"
    },
    {
      "title": "System equivalence",
      "url": "https://en.wikipedia.org/wiki/System_equivalence",
      "text": "In the [[systems sciences]] '''system equivalence''' is the behavior of a [[parameter]] or component of a [[system]] in a way similar to a parameter or component of a different system.  Similarity means that mathematically the parameters and components will be indistinguishable from each other. Equivalence can be very useful in understanding how [[complex system]]s work.\n\n== Overview ==\nExamples of equivalent systems are first- and second-[[order (differential equation)|order]] (in the [[independent variable]]) [[translational]], [[electrical]], [[Torque|torsional]], [[Fluid mechanics|fluidic]], and [[caloric]] systems.\n\nEquivalent systems can be used to change large and expensive mechanical, thermal, and fluid systems into a simple, cheaper electrical system.  Then the electrical system can be analyzed to validate that the [[system dynamics]] will work as designed.  This is a preliminary inexpensive way for engineers to test that their complex system performs the way they are expecting.\n\nThis testing is necessary when designing new complex systems that have many components.  Businesses do not want to spend millions of dollars on a system that does not perform the way that they were expecting. Using the equivalent system technique, engineers can verify and prove to the business that the system will work.  This lowers the risk factor that the business is taking on the project.\n\nChart of equivalent variables for the different types of systems {{citation needed|date=November 2012}}\n\n:{| class=\"wikitable\"\n|-\n! '''System type\n! Flow variable\n! Effort variable\n! Compliance\n! Inductance\n! Resistance'''\n|-\n| '''Mechanical'''\n| ''dx''/''dt''\n| ''F'' = force\n| spring (''k'')\n| mass (''m'')\n| damper (''c'')\n|-\n| '''Electrical'''\t\t\n| ''i'' = current\n| ''V'' = voltage\n| capacitance (''C'')\n| inductance (''L'')\n| resistance (''R'')\n|-\n| '''Thermal'''\n| ''qh'' = heat flow rate\n| ∆''T'' = change in temperature\n| object (''C'')\n| [[thermal inductance|inductance]] (''L'')<ref>{{cite journal|title=Thermal Mutual Inductance|journal=Nature|first=R.C.L.|last=Bosworth|volume=161|pages=166-167|date=31 January 1948|doi=10.1038/161166a0}}</ref>\n| conduction and convection (''R'')\n|-\n| '''Fluid'''\n| ''qm'' = mass flow rate,\n''qv'' = volume flow rate\n| ''p' = pressure, ''h'' = height\n| tank (''C'')\n| mass (''m'')\n| valve or orifice (''R'')\n|}\n\n: Flow variable: moves through the system\n: Effort variable: puts the system into action\n: Compliance: stores energy as potential\n: Inductance: stores energy as kinetic\n: Resistance: dissipates or uses energy\n\nThe equivalents shown in the chart are not the only way to form mathematical analogies.  In fact there are any number of ways to do this.  A common requirement for analysis is that the analogy correctly models energy storage and flow across energy domains.  To do this, the equivalences must be compatible.  A pair of variables whose product is power (or energy) in one domain must be equivalent to a pair of variables in the other domain whose product is also power (or energy).  These are called power conjugate variables.  The thermal variables shown in the chart are not power conjugates and thus do not meet this criterion.  See [[mechanical-electrical analogies]] for more detailed information on this.  Even specifying power conjugate variables does not result in a unique analogy and there are at least three analogies of this sort in use.  At least one more criterion is needed to uniquely specify the analogy, such as the requirement that [[electrical impedance|impedance]] is equivalent in all domains as is done in the [[impedance analogy]].\n\n==Examples==\n; Mechanical systems\n:Force <math>F = -kx = c\\frac{dx}{dt} = m\\frac{d^2x}{dt^2}</math>\n\n; Electrical systems\n:Voltage <math>V = \\frac{Q}{C} = R\\frac{dQ}{dt} = L\\frac{d^2Q}{dt^2}</math>\n\nAll the fundamental [[Variable (mathematics)|variables]] of these systems have the same functional form.\n\n==Discussion==\nThe system equivalence method may be used to describe systems of two types: \"vibrational\" systems (which are thus described - approximately - by harmonic oscillation) and \"translational\" systems (which deal with \"flows\").  These are not mutually exclusive; a system may have features of both. Similarities also exist; the two systems can often be analysed by the methods of Euler, Lagrange and Hamilton, so that in both cases the energy is quadratic in the relevant degree(s) of freedom, provided they are linear.\n\nVibrational systems are often described by some sort of wave (partial differential) equation, or oscillator (ordinary differential) equation. Furthermore, these sorts of systems follow the capacitor or spring analogy, in the sense that the dominant degree of freedom in the energy is the generalized position. In more physical language, these systems are predominantly characterised by their potential energy. This often works for solids, or (linearized) undulatory systems near equilibrium.\n\nOn the other hand, flow systems may be easier described by the hydraulic analogy or the diffusion equation. For example, Ohm's law was said to be inspired by Fourier's law (as well as the work of C.-L. Navier).<ref>\n{{cite book\n |language    = German\n |author      = G. S. Ohm\n |title       = Die galvanische Kette, mathematisch bearbeitet\n |trans-title = The galvanic circuit investigated mathematically\n |year        = 1827\n |publisher   = Berlin: T. H. Riemann\n |url         = http://www.ohm-hochschule.de/bib/textarchiv/Ohm.Die_galvanische_Kette.pdf\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20090326094110/http://www.ohm-hochschule.de/bib/textarchiv/Ohm.Die_galvanische_Kette.pdf\n |archivedate = 2009-03-26\n |df          = \n}}</ref><ref>B. Pourprix, \"G.-S. Ohm théoricien de l'action contiguë,\" ''Archives internationales d'histoire des sciences'' '''45'''(134) (1995), 30-56</ref><ref>T Archibald, \"Tension and potential from Ohm to Kirchhoff,\" ''Centaurus'' '''31''' (2) (1988), 141-163</ref> Other laws include Fick's laws of diffusion and generalized transport problems. The most important idea is the flux, or rate of transfer of some important physical quantity considered (like electric or magnetic fluxes). In these sorts of systems, the energy is dominated by  the derivative of the generalized position (generalized velocity). In physics parlance, these systems tend to be kinetic energy-dominated. Field theories, in particular electromagnetism, draw heavily from the hydraulic analogy.\n\n== See also ==\n* [[Capacitor analogy]]\n* [[Hydraulic analogy]]\n*  [[Analogical models]]\n* For [[harmonic oscillators]], see [[Harmonic oscillator#Universal oscillator equation|Universal oscillator equation]] and [[Harmonic oscillator#Equivalent systems|Equivalent systems]]\n* [[Linear time-invariant system]]\n* [[Resonance]]\n* [[Q-factor]]\n* [[Impedance (disambiguation)|Impedance]]\n* [[Thermal inductance]]\n\n== References ==\n\n{{reflist}}\n\n== Further reading ==\n* Panos J. Antsaklis, Anthony N. Michel (2006), ''Linear Systems'', 670 pp. \n* [[M.F. Kaashoek]] & [[Jan H. van Schuppen|J.H. Van Schuppen]] (1990), ''Realization and Modelling in System Theory''. \n* Katsuhiko Ogata (2003), ''System dynamics'', Prentice Hall; 4 edition (July 30, 2003), 784 pp.\n\n== External links ==\n* [http://vam.anest.ufl.edu/demos/firstordersystem.html A simulation using a hydraulic analog as a mental model for the dynamics of a first order system]\n* [http://www.dartmouth.edu/~sullivan/22files/System_analogy_all.pdf System Analogies], Engs 22 — Systems Course, [[Dartmouth College]].\n\n[[Category:Applied mathematics]]\n[[Category:Dynamical systems|Equivalence]]\n[[Category:Systems engineering|Equivalence]]\n[[Category:Systems theory|Equivalence]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Univalence axiom",
      "url": "https://en.wikipedia.org/wiki/Univalence_axiom",
      "text": "#REDIRECT [[Homotopy type theory#The univalence axiom]]\n\n{{Redirect category shell|1=\n{{R from merge}}\n{{R to section}}\n}}\n\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Weak equivalence (homotopy theory)",
      "url": "https://en.wikipedia.org/wiki/Weak_equivalence_%28homotopy_theory%29",
      "text": "In [[mathematics]], a '''weak equivalence''' is a notion from [[homotopy theory]] which in some sense identifies objects that have the same \"shape\". This notion is formalized in the [[axiom]]atic definition of a [[model category]].\n\nA model category is a [[category (mathematics)|category]] with classes of [[morphism]]s called weak equivalences, [[fibration]]s, and [[cofibration]]s, satisfying several axioms. The associated [[homotopy category]] of a model category has the same objects, but the morphisms are changed in order to make the weak equivalences into [[isomorphism]]s. It is a useful observation that the associated homotopy category depends only on the weak equivalences, not on the fibrations and cofibrations.\n\n==Topological spaces==\nModel categories were defined by [[Daniel Quillen|Quillen]] as an axiomatization of homotopy theory that applies to [[topological space]]s, but also to many other categories in algebra and geometry. The example that started the subject is the category of topological spaces with [[Serre fibration]]s as fibrations and '''weak homotopy equivalences''' as weak equivalences (the cofibrations for this model structure can be described as the [[retract]]s of relative cell complexes ''X'' ⊂ ''Y''<ref>Hovey (1999), Definition 2.4.3.</ref>). By definition, a [[continuous mapping]] ''f'': ''X'' → ''Y'' of spaces is called a weak homotopy equivalence if the induced function on sets of [[path component]]s\n:<math>f_*\\colon \\pi_0(X) \\to \\pi_0(Y)</math>\nis [[bijective]], and for every point ''x'' in ''X'' and every ''n'' ≥ 1, the induced [[group homomorphism|homomorphism]]\n:<math>f_*\\colon \\pi_n(X,x) \\to \\pi_n(Y,f(x))</math>\non [[homotopy group]]s is bijective. (For ''X'' and ''Y'' [[path-connected]], the first condition is automatic, and it suffices to state the second condition for a single point ''x'' in ''X''.)\n\nFor [[simply connected]] topological spaces ''X'' and ''Y'', a map ''f'': ''X'' → ''Y'' is a weak homotopy equivalence if and only if the induced homomorphism ''f''<sub>*</sub>: ''H''<sub>''n''</sub>(''X'','''Z''') → ''H''<sub>''n''</sub>(''Y'','''Z''') on [[singular homology]] groups is bijective for all ''n''.<ref>Hatcher (2002), Theorem 4.32.</ref> Likewise, for simply connected spaces ''X'' and ''Y'', a map ''f'': ''X'' → ''Y'' is a weak homotopy equivalence if and only if the pullback homomorphism ''f''*: ''H''<sup>''n''</sup>(''Y'','''Z''') → ''H''<sup>''n''</sup>(''X'','''Z''') on [[singular cohomology]] is bijective for all ''n''.<ref>http://mathoverflow.net/questions/57783/is-there-the-whitehead-theorem-for-cohomology-theory</ref>\n\nExample: Let ''X'' be the set of natural numbers {0, 1, 2, ...} and let ''Y'' be the set {0} ∪ {1, 1/2, 1/3, ...}, both with the [[subspace topology]] from the [[real line]]. Define ''f'': ''X'' → ''Y'' by mapping 0 to 0 and ''n'' to 1/''n'' for positive integers ''n''. Then ''f'' is continuous, and in fact a weak homotopy equivalence, but it is not a [[homotopy equivalence]].\n\nThe homotopy category of topological spaces (obtained by inverting the weak homotopy equivalences) greatly simplifies the category of topological spaces. Indeed, this homotopy category is [[equivalence of categories|equivalent]] to the category of [[CW complex]]es with morphisms being [[homotopy class]]es of continuous maps.\n\nMany other model structures on the category of topological spaces have also been considered. For example, in the Strøm model structure on topological spaces, the fibrations are the [[Hurewicz fibration]]s and the weak equivalences are the homotopy equivalences.<ref>Strøm (1972).</ref>\n\n==Chain complexes==\nSome other important model categories involve [[chain complex]]es. Let ''A'' be a [[Grothendieck category|Grothendieck abelian category]], for example the category of [[module (mathematics)|module]]s over a [[ring (mathematics)|ring]] or the category of [[sheaf (mathematics)|sheaves]] of [[abelian group]]s on a topological space. Define a category ''C''(''A'') with objects the complexes ''X'' of objects in ''A'',\n:<math>\\cdots\\to X_1\\to X_0\\to X_{-1}\\to\\cdots,</math>\nand morphisms the [[chain map]]s. (It is equivalent to consider \"cochain complexes\" of objects of ''A'', where the numbering is written as\n:<math>\\cdots\\to X^{-1}\\to X^0\\to X^1\\to\\cdots,</math>\nsimply by defining ''X''<sup>''i''</sup> = ''X''<sub>−''i''</sub>.)\n\nThe category ''C''(''A'') has a model structure in which the cofibrations are the [[monomorphism]]s and the weak equivalences are the '''[[quasi-isomorphism]]s'''.<ref>Beke (2000), Proposition 3.13.</ref> By definition, a chain map ''f'': ''X'' → ''Y'' is a quasi-isomorphism if the induced homomorphism\n:<math>f_*\\colon H_n(X) \\to H_n(Y)</math>\non [[homology (mathematics)|homology]] is an isomorphism for all integers ''n''. (Here ''H''<sub>''n''</sub>(''X'') is the object of ''A'' defined as the [[kernel (category theory)|kernel]] of ''X''<sub>''n''</sub> → ''X''<sub>''n''−1</sub> modulo the [[image (mathematics)|image]] of ''X''<sub>''n''+1</sub> → ''X''<sub>''n''</sub>.) The resulting homotopy category is called the [[derived category]] ''D''(''A'').\n\n==Trivial fibrations and trivial cofibrations==\nIn any model category, a fibration which is also a weak equivalence is called a '''trivial''' (or '''acyclic''') '''fibration'''.  A cofibration which is also a weak equivalence is called a '''trivial''' (or '''acyclic''') '''cofibration'''.\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{Citation | author1-first=Tibor | author1-last=Beke | title=Sheafifiable homotopy model categories | journal=[[Mathematical Proceedings of the Cambridge Philosophical Society]] | volume=129 | pages=447–473 | year=2000 | mr=1780498 | doi=10.1017/S0305004100004722| arxiv=math/0102087 | bibcode=2000MPCPS.129..447B }}\n*{{Citation | author1-first=Allen | author1-last=Hatcher | author1-link=Allen Hatcher | title=Algebraic Topology | publisher=[[Cambridge University Press]] | year=2002 | url=http://www.math.cornell.edu/~hatcher/AT/ATpage.html | isbn=0-521-79540-0 | mr =1867354}}\n*{{Citation | author1-first=Mark | author1-last=Hovey | title=Model Categories | publisher=[[American Mathematical Society]] | year=1999 | mr=1650134 | isbn=0-8218-1359-5 | url=https://web.math.rochester.edu/people/faculty/doug/otherpapers/hovey-model-cats.pdf}}\n*{{Citation | author1-first=Arne | author1-last=Strøm | title=The homotopy category is a homotopy category | journal=Archiv der Mathematik | volume=23 | pages=435–441 | year=1972 | mr=0321082 | doi=10.1007/BF01304912}}\n\n{{DEFAULTSORT:Weak Equivalence}}\n[[Category:Homotopy theory]]\n[[Category:Homological algebra]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Wilf equivalence",
      "url": "https://en.wikipedia.org/wiki/Wilf_equivalence",
      "text": "In the study of [[permutation]]s and [[permutation pattern]]s, '''Wilf equivalence''' is an [[equivalence relation]] on [[permutation class]]es.\nTwo permutation classes are Wilf equivalent when they have the same numbers of permutations of each possible length, or equivalently if they have the same [[generating function]]s.{{r|bevan}} The equivalence classes for Wilf equivalence are called '''Wilf classes''';{{r|open}} they are the [[combinatorial class]]es of permutation classes. The counting functions and Wilf equivalences among many [[Enumerations of specific permutation classes|specific permutation classes]] are known.\n\nWilf equivalence may also be described for individual permutations rather than permutation classes. In this context, two permutations are said to be Wilf equivalent if the principal permutation classes formed by forbidding them are Wilf equivalent.{{r|bevan}}\n\n==References==\n{{reflist|refs=\n\n<ref name=bevan>{{citation\n | last = Bevan | first = David\n | arxiv = 1506.06673\n | title = Permutation patterns: basic definitions and notation\n | year = 2015| bibcode = 2015arXiv150606673B}}</ref>\n\n<ref name=open>{{citation\n | last = Steingrímsson | first = Einar\n | contribution = Some open problems on permutation patterns\n | mr = 3156932\n | pages = 239–263\n | publisher = Cambridge Univ. Press, Cambridge\n | series = London Math. Soc. Lecture Note Ser.\n | title = Surveys in combinatorics 2013\n | volume = 409\n | year = 2013}}</ref>\n\n}}\n\n[[Category:Enumerative combinatorics]]\n[[Category:Permutation patterns]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Approximate computing",
      "url": "https://en.wikipedia.org/wiki/Approximate_computing",
      "text": "'''Approximate computing''' is a computation technique which returns a possibly inaccurate result rather than a guaranteed accurate result, and can be used for applications where an approximate result is sufficient for its purpose.<ref name=\"surveyACT\">{{cite journal|last1=Mittal|first1=Sparsh|title=A Survey of Techniques for Approximate Computing|journal=ACM Comput. Surv.|date=May 2016|volume=48|issue=4|pages=62:1–62:33|doi=10.1145/2893356|publisher=ACM|language=en}}</ref><ref>A. Sampson, et al. \"[http://dl.acm.org/citation.cfm?id=1993518 EnerJ: Approximate data types for safe and general low-power computation]\", In ACM SIGPLAN Notices, vol. 46, no. 6, 2011.</ref> One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some [[Frame (video)|frames]] in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing [[Approximation theory|bounded approximation]] can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.{{clarify|date=January 2016}}  For example, in [[k-means clustering|''k''-means clustering]] algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.<ref name=\"surveyACT\"/>\n\nThe key requirement in approximate computing is that approximation can be introduced only in non-critical data, since approximating critical data (e.g., control operations) can lead to disastrous consequences, such as [[program crash]] or erroneous output.\n\n==Strategies==\nSeveral strategies can be used for performing approximate computing.\n\n; Approximate circuits\n: Approximate [[Adder (electronics)|adders]],<ref>J. Echavarria, et al. \"FAU: Fast and Error-Optimized Approximate Adder Units on LUT-Based FPGAs\", FPT, 2016.</ref><ref>J. Miao, et al. \"Modeling and synthesis of quality-energy optimal approximate adders\", ICCAD, 2012</ref> [[Binary multiplier|multipliers]]<ref>{{Cite journal|last=Rehman|first=Semeen|last2=El-Harouni|first2=Walaa|last3=Shafique|first3=Muhammad|last4=Kumar|first4=Akash|last5=Henkel|first5=Jörg|date=2016-11-07|title=Architectural-space exploration of approximate multipliers|url=http://dl.acm.org/citation.cfm?id=2966986.2967005|publisher=ACM|pages=80|doi=10.1145/2966986.2967005|isbn=9781450344661}}</ref> and other [[logical circuit]]s can reduce hardware overhead.<ref>S. Venkataramani, et al. \"SALSA: systematic logic synthesis of approximate circuits\", DAC, 2012.</ref><ref>J. Miao, et al. \"Approximate logic synthesis under general error magnitude and frequency constraints\", ICCAD, 2013</ref><ref>R. Hegde et al. \"Energy-efficient signal processing via algorithmic noise-tolerance\", ISLPED, 1999.</ref> For example, an approximate multi-bit adder can ignore the [[carry chain]] and thus, allow all its sub-adders to perform addition operation in parallel.\n; Approximate storage\n: Instead of [[Computer data storage|storing data]] values exactly, they can be stored approximately, e.g., by [[Data truncation|truncating]] the lower-bits in [[floating point]] data. Another method is accept less reliable memory. For this, in [[DRAM]]<ref>{{Cite journal|last=Raha|first=A.|last2=Sutar|first2=S.|last3=Jayakumar|first3=H.|last4=Raghunathan|first4=V.|date=July 2017|title=Quality Configurable Approximate DRAM|url=http://ieeexplore.ieee.org/document/7784741/|journal=IEEE Transactions on Computers|volume=66|issue=7|pages=1172–1187|doi=10.1109/TC.2016.2640296|issn=0018-9340}}</ref> and [[eDRAM]], [[refresh rate]] can be lowered and in [[Static random-access memory|SRAM]], supply voltage can be lowered. In general, any [[error detection and correction]] mechanisms should be disabled.\n; Software-level approximation\n: There are several ways to approximate at software level. [[Memoization]] can be applied. Some [[iteration]]s of [[Loop (computing)|loops]] can be skipped (termed as [[loop perforation]]) to achieve a result faster. Some tasks can also be skipped, for example when a run-time condition suggests that those tasks are not going to be useful ([[task skipping]]). [[Monte Carlo algorithm]]s and [[Randomized algorithm]]s trade correctness for execution time guarantees<ref>C.Alippi, Intelligence for Embedded Systems: a Methodological approach,  Springer, 2014, pp. 283</ref>. The computation can be reformulated according to paradigms that allow easily the acceleration on specialized hardware, e.g. a neural processing unit.<ref>H. Esmaeilzadeh, et al. \"Neural acceleration for general-purpose approximate programs\", MICRO, 2012</ref>\n; Approximate system \n: In an approximate system<ref>{{Cite journal|last=Raha|first=Arnab|last2=Raghunathan|first2=Vijay|date=2017|title=Towards Full-System Energy-Accuracy Tradeoffs: A Case Study of An Approximate Smart Camera System|url=http://doi.acm.org/10.1145/3061639.3062333|journal=Proceedings of the 54th Annual Design Automation Conference 2017|series=DAC '17|location=New York, NY, USA|publisher=ACM|pages=74:1–74:6|doi=10.1145/3061639.3062333|isbn=9781450349277}}</ref>, different subsystems of the system such as the processor, memory, sensor, and communication modules are synergistically approximated to obtain a much better system-level Q-E trade-off curve compared to individual approximations to each of the subsystems.\n\n==Application areas==\nApproximate computing has been used in a variety of domains where the applications are error-tolerant, such as [[multimedia]] processing, [[machine learning]], [[signal processing]], [[Computational science|scientific computing]], etc. Google is using this approach in their [[Tensor processing unit]]s (TPU, a custom ASIC).\n\n==Derived paradigms==\nThe main issue in approximate computing is the identification of the section of the application that can be approximated. In the case of large scale applications, it is very common to find people holding the expertise on approximate computing techniques not having enough expertise on the application domain (and vice versa). In order to solve this problem, [[programming paradigm]]s<ref>{{cite journal|last1=Nguyen|first1=Donald|last2=Lenharth|first2=Andrew|last3=Pingali|first3=Keshav|title=A lightweight infrastructure for graph analytics|journal=Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles|date=2013|pages=456–471|publisher=ACM|language=en}}</ref><ref>{{cite journal|last1=Silvano|first1=Cristina|last2=Agosta|first2=Giovanni|last3=Cherubin|first3=Stefano|last4=Gadioli|first4=Davide|last5=Palermo|first5=Gianluca|last6=Bartolini|first6=Andrea|last7=Benini|first7=Luca|last8=Martinovič|first8=Jan|last9=Palkovič|first9=Martin|last10=Slaninová|first10=Kateřina|last11=Bispo|first11=João|last12=Cardoso|first12=João M. P.|last13=Rui|first13=Abreu|last14=Pinto|first14=Pedro|last15=Cavazzoni|first15=Carlo|last16=Sanna|first16=Nico|last17=Beccari|first17=Andrea R.|last18=Cmar|first18=Radim|last19=Rohou|first19=Erven|title=The ANTAREX approach to autotuning and adaptivity for energy efficient HPC systems|journal=Proceedings of the ACM International Conference on Computing Frontiers|date=2016|pages=288–293|publisher=ACM|language=en}}</ref> have been proposed. They all have in common the clear role separation between application [[programmer]] and application [[domain expert]]. These approaches allow the spread of the most common [[Program optimization|optimizations]] and approximate computing techniques.\n\n==See also==\n* [[Artificial neural network]]\n* [[PCMOS]]\n\n==References==\n{{Reflist|30em}}\n\n<!---->\n\n[[Category:Software optimization]]\n[[Category:Computer architecture]]\n[[Category:Approximations]]"
    },
    {
      "title": "Back-of-the-envelope calculation",
      "url": "https://en.wikipedia.org/wiki/Back-of-the-envelope_calculation",
      "text": "A '''back-of-the-envelope calculation''' is a rough calculation, typically jotted down on any available scrap of paper such as an [[envelope]]. It is more than a [[guess]] but less than an accurate [[calculation]] or [[mathematical proof]]. The defining characteristic of back-of-the-envelope calculations is the use of simplified assumptions. A similar phrase in the U.S. is \"back of a [[napkin]]\", also used in the business world to describe sketching out a quick, rough idea of a business or product.<ref>[http://www.networkworld.com/community/node/76615 NetworkWorld.com: Ethernet, Compaq, Facebook and napkins]</ref> In British English, a similar idiom is \"back of a [[cigarette pack|fag packet]]\".\n\n== History ==\nIn the natural sciences, ''back-of-the-envelope calculation'' is often associated with physicist [[Enrico Fermi]],<ref>[http://www.encyclopedia.com/doc/1G1-78334537.html Where Fermi stood. - Bulletin of the Atomic Scientists | Encyclopedia.com<!-- Bot generated title -->]</ref> who was well known for emphasizing ways that complex scientific equations could be approximated within an [[order of magnitude]] using simple calculations. He went on to develop a series of sample calculations, which are called \"Fermi Questions\" or \"Back-of-the-Envelope Calculations\" and used to solve [[Fermi problem]]s.<ref>[http://serc.carleton.edu/quantskills/teaching_methods/boe/index.html  Back of the Envelope Calculations<!-- Bot generated title -->]</ref><ref>[http://www.nap.edu/html/hs_math/be.html High School Mathematics at Work: Essays and Examples for the Education of All Students<!-- Bot generated title -->]</ref>\n\nFermi was known for getting quick and accurate answers to problems that would stump other people. The most famous instance came during the [[first atomic bomb]] test in [[New Mexico]] on 16 July 1945. As the blast wave reached him, Fermi dropped bits of paper. By measuring the distance they were blown, he could compare to a previously computed table and thus estimate the bomb energy yield. He estimated 10 kilotons of TNT; the measured result was 18.6.<ref>[http://www.lanl.gov/science/weapons_journal/wj_pubs/11nwj2-05.pdf Nuclear Weapons Journal, Los Alamos National Laboratory, Issue 2 2005.]</ref>\n\nPerhaps the most influential example of such a calculation was carried out over a period of a few hours by [[Arnold Wilkins]] after being asked to consider a problem by [[Robert Watson Watt]]. Watt had learned that the Germans claimed to have invented a radio-based death ray, but Wilkins' one-page calculations demonstrated that such a thing was almost certainly impossible. When Watt asked what role radio might play, Wilkins replied that it might be useful for detection at long range, a suggestion that led to the rapid development of [[radar]] and the [[Chain Home]] system.<ref>{{cite journal |first=B.A. |last=Austin |title=Precursors To Radar — The Watson-Watt Memorandum And The Daventry Experiment |url=http://www.bawdseyradar.org.uk/wp-content/uploads/2012/12/Wilkins-Calculations.pdf |journal=International Journal of Electrical Engineering Education |volume=36 |year=1999 |pages=365–372 |access-date=2016-07-08 |archive-url=https://web.archive.org/web/20150525040134/http://www.bawdseyradar.org.uk/wp-content/uploads/2012/12/Wilkins-Calculations.pdf |archive-date=2015-05-25 |dead-url=yes |df= }}</ref>\n\nAnother example is [[Victor Weisskopf]]'s pamphlet ''Modern Physics from an Elementary Point of View''.<ref>[http://cdsweb.cern.ch/record/274976/ Lectures given in the 1969 Summer Lecture Programme, CERN (European Organization for  Nuclear Research), CERN 70-8, 17 March 1970.]</ref> In these notes Weisskopf used back-of-the-envelope calculations to calculate the size of a hydrogen atom, a star, and a mountain, all using elementary physics.\n\n== Examples ==\nNobel laureate [[Charles Townes]] describes in a video interview for the [[University of California, Berkeley]] on the 50th anniversary of the laser, how he pulled an envelope from his pocket while sitting in a park and wrote down calculations during his initial insight into lasers.<ref>[http://laserfest.org/lasers/video-history.cfm Video of interview with Charles Townes; envelope mention comes about halfway in]</ref>\n\nAn important Internet protocol, the [[Border Gateway Protocol]], was sketched out in 1989 by engineers on the back of \"three ketchup-stained napkins\", and is still known as the three-napkin protocol.<ref>[https://www.washingtonpost.com/sf/business/2015/05/31/net-of-insecurity-part-2/ Washington Post: \"Net of Insecurity\"]</ref>\n\n[[UTF-8]], the dominant character encoding for the [[World Wide Web]], <ref>{{Cite web|url=https://w3techs.com/technologies/cross/character_encoding/ranking|title=Usage Survey of Character Encodings broken down by Ranking|website=w3techs.com|language=en|access-date=2018-11-01}}</ref> was designed by [[Ken Thompson]] and [[Rob Pike]] on a placemat.<ref name=\":0\">[https://www.cl.cam.ac.uk/~mgk25/ucs/utf-8-history.txt Email Subject: UTF-8 history], From: \"Rob 'Commander' Pike\", Date: Wed, 30 Apr 2003..., ''...UTF-8 was designed, in front of my eyes, on a placemat in a New Jersey diner one night in September or so 1992...So that night Ken wrote packing and unpacking code and I started tearing into the C and graphics libraries.  The next day all the code was done...''</ref>\n\nThe [[Bailey bridge]] is a type of portable, pre-fabricated, truss bridge and was extensively used by British, Canadian and US military engineering units. [[Donald Bailey (civil engineer)|Donald Bailey]] drew the original design for the bridge on the back of an envelope.<ref name=\"bailey\">{{Cite news|url=http://articles.latimes.com/1985-05-07/news/mn-11177_1_sir-donald-bailey|title=Sir Donald Bailey, WW II Engineer, Dies|last=Services|first=From Times Wire|date=1985-05-07|work=Los Angeles Times|access-date=2018-11-01|language=en-US|issn=0458-3035|quote=\"He sketched the original design for the Bailey Bridge on the back of an envelope as he was being driven to a meeting of Royal Engineers to debate the failure of existing portable bridges\"}}</ref>\n\nThe [[Laffer Curve]], purporting to show the relationship between tax cuts and government income, was drawn by [[Arthur Laffer]] in 1974 on a bar napkin to show an aide to President [[Gerald R. Ford]] why the federal government should cut taxes. <ref>[https://www.nytimes.com/2017/10/13/us/politics/arthur-laffer-napkin-tax-curve.html?module=WatchingPortal&region=c-column-middle-span-region&pgType=Homepage&action=click&mediaId=thumb_square&state=standard&contentPlacement=2&version=internal&contentCollection=www.nytimes.com&contentId=https%3A%2F%2Fwww.nytimes.com%2F2017%2F10%2F13%2Fus%2Fpolitics%2Farthur-laffer-napkin-tax-curve.html&eventName=Watching-article-click&_r=0 \"This is not Arthur Laffer's famous napin\" NY Times 13 Oct. 2017]</ref>\n\n== See also ==\n* [[Buckingham pi theorem]], a technique often used in [[fluid mechanics]] to obtain order-of-magnitude estimates\n* [[Guesstimate]]\n* [[Scientific Wild-Ass Guess]]\n* [[Heuristic]]\n* [[Order-of-magnitude analysis]]\n* [[Rule of thumb]]\n* [[Sanity testing]]\n\n== Notes and references ==\n{{Reflist}}\n\n== External links ==\n{{Wiktionary|back-of-the-envelope|back-of-an-envelope}}\n* [http://www-cse.ucsd.edu/classes/sp97/cse141/botec.html Syllabus at UCSD]\n\n{{Orders of magnitude}}\n\n[[Category:Approximations]]\n[[Category:Informal estimation]]\n[[Category:Metaphors referring to objects]]"
    },
    {
      "title": "Born–Huang approximation",
      "url": "https://en.wikipedia.org/wiki/Born%E2%80%93Huang_approximation",
      "text": "The '''Born&ndash;Huang approximation'''<ref name=BHCrystalBook>{{cite book|last=Born|first=Max|title=Dynamical Theory of Crystal Lattices|year=1954|publisher=Oxford University Press|location=Oxford|author2=Kun, Huang}}</ref> (named after [[Max Born]] and [[Huang Kun]]) is an approximation closely related to the [[Born&ndash;Oppenheimer approximation]]. It takes into account diagonal nonadiabatic effects in the electronic [[Hamiltonian (quantum mechanics)|Hamiltonian]] better than the Born&ndash;Oppenheimer approximation.<ref>[http://www.chemistry.mcmaster.ca/courses/3bb3/z_HW/Set%201.pdf Mathematical Methods and the Born-Oppenheimer Approximation] {{webarchive |url=https://web.archive.org/web/20140303004455/http://www.chemistry.mcmaster.ca/courses/3bb3/z_HW/Set%201.pdf |date=March 3, 2014 }}</ref>  Despite the addition of correction terms, the [[electronic state]]s remain uncoupled under the Born–Huang approximation, making it an [[adiabatic approximation]].\n\n==Mathematical formula==\nThe Born–Huang approximation asserts that the representation matrix of nuclear kinetic energy operator in the basis of Born–Oppenheimer electronic wavefunctions is diagonal:\n:<math>\n\\langle\\chi_{k'}(\\mathbf{r}; \\mathbf{R}) | T_\\mathrm{n} | \\chi_k(\\mathbf{r}; \\mathbf{R})\\rangle_{(\\mathbf{r})} =\n \\mathcal{T}_\\mathrm{k}(\\mathbf{R})\\delta_{k'k}.\n</math>\n\n==Consequences==\nThe Born–Huang approximation loosens the Born–Oppenheimer approximation by including some electronic matrix elements, while at the same time maintains its diagonal structure in the nuclear equations of motion.  As a result, the nuclei still move on isolated surfaces, obtained by the addition of a small correction to the Born–Oppenheimer [[potential energy surface]].\n\nUnder the Born–Huang approximation, the Schrödinger equation of the molecular system simplifies to\n:<math>\n\\left[ T_\\mathrm{n} +E_k(\\mathbf{R})+\\mathcal{T}_\\mathrm{k}(\\mathbf{R})\\right] \\phi_k(\\mathbf{R}) =\n E \\phi_k(\\mathbf{R})\n \\quad\\text{for}\\quad k=1, \\ldots, K.\n</math>\n\nThe quantity <math>\\left[E_k(\\mathbf{R})+\\mathcal{T}_\\mathrm{k}(\\mathbf{R})\\right]</math> serves as the corrected potential energy surface.\n\n==Upper-bound property==\nThe value of Born–Huang approximation is that it provides the upper bound for the ground-state energy.<ref name=BHCrystalBook/>  The Born–Oppenheimer approximation, on the other hand, provides the lower bound for this value.<ref>{{cite journal|last=Epstein|first=Saul T.|title=Ground-State Energy of a Molecule in the Adiabatic Approximation|journal=The Journal of Chemical Physics|date=1 January 1966|volume=44|issue=2|pages=836–837|doi=10.1063/1.1726771|bibcode = 1966JChPh..44..836E |hdl=2060/19660026030}}</ref>\n\n==See also==\n* [[Vibronic coupling]]\n* [[Born–Oppenheimer approximation]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Born-Huang Approximation}}\n[[Category:Quantum chemistry]]\n[[Category:Approximations]]"
    },
    {
      "title": "Born–Oppenheimer approximation",
      "url": "https://en.wikipedia.org/wiki/Born%E2%80%93Oppenheimer_approximation",
      "text": "{{distinguish|text=the [[Born approximation]]}}\n{{multiple issues|\n{{more footnotes|date=March 2013}}\n{{Technical|date=September 2010}}\n}}\n\nIn [[quantum chemistry]] and [[molecular physics]], the '''Born&ndash;Oppenheimer''' ('''BO''') '''approximation''' is the assumption that the motion of [[Atomic nucleus|atomic nuclei]] and [[electron]]s in a molecule can be separated. The approach is named after [[Max Born]], and [[J. Robert Oppenheimer]]. In mathematical terms, it allows the wavefunction of a molecule to be broken into its electronic and nuclear (vibrational, rotational) components.\n\n: <math> \\Psi_\\mathrm{total} = \\psi_\\mathrm{electronic} \\otimes \\psi_\\mathrm{nuclear} </math>\n\nComputation of the energy and the [[wavefunction]] of an average-size [[molecule]] is simplified by the approximation.  For example, the [[benzene]] molecule consists of 12 nuclei and 42 electrons. The [[Schrödinger equation|time independent Schrödinger equation]], which must be solved to obtain the energy and wavefunction of this molecule, is a [[partial differential equation|partial differential eigenvalue equation]] in 162 variables&mdash;the spatial coordinates of ''the electrons and the nuclei''. The BO approximation makes it possible to compute the wavefunction in two less complicated consecutive steps. This approximation was proposed in 1927, in the early period of quantum mechanics, by Born and Oppenheimer and is still indispensable in quantum chemistry.\n\nIn the first step of the BO approximation the ''electronic'' Schrödinger equation is solved, yielding the wavefunction <math>\\psi_{\\text{electronic}}</math> depending on electrons only. For benzene this wavefunction depends on 126 electronic coordinates. During this solution the nuclei are fixed in a certain configuration, very often the equilibrium configuration. If the effects of the quantum mechanical nuclear motion are to be studied, for instance because a [[molecular vibration|vibrational spectrum]] is required, this electronic computation must be in nuclear coordinates. In the second step of the BO approximation this function serves as a potential in a Schrödinger equation ''containing only the nuclei''&mdash;for benzene an equation in 36 variables.\n\nThe success of the BO approximation is due to the difference between nuclear and electronic masses. The approximation is an important tool of quantum chemistry: all computations of molecular wavefunctions for large molecules make use of it, and without it only the lightest molecule, H<sub>2</sub>, can be handled. Even in the cases where the BO approximation breaks down, it is used as a point of departure for the computations.\n\nThe electronic energies consist of kinetic energies, interelectronic repulsions, internuclear repulsions, and electron&ndash;nuclear attractions. In accord with the [[Hellmann-Feynman theorem]], the nuclear potential is taken to be an average over electron configurations of the sum of the electron&ndash;nuclear and internuclear electric potentials.\n\nIn molecular [[spectroscopy]], because the ratios of the periods of the electronic, vibrational and rotational energies are each related to each other on scales in the order of a thousand, the Born–Oppenheimer name has also been attached to the approximation where the energy components are treated separately.\n\n:: <math> E_\\mathrm{total} = E_\\mathrm{electronic} + E_\\mathrm{vibrational} + E_\\mathrm{rotational}+ E_\\mathrm{nuclear}</math>\n\nThe nuclear spin energy is so small that it is normally omitted.\n\n==Short description==\nThe [[Max Born|Born]]&ndash;[[Robert Oppenheimer|Oppenheimer]] (BO) [[approximation]] is ubiquitous in [[Quantum chemistry|quantum chemical]] calculations of molecular [[wavefunction]]s. It consists of two steps.\n\nIn the first step the nuclear [[kinetic energy]] is neglected,<ref>This step is often justified by stating that \"the heavy nuclei move more slowly than the light [[electron]]s\". Classically this statement makes sense only if the [[momentum]] ''p'' of electrons and nuclei is of the same order of magnitude. In that case ''m''<sub>n</sub> ≫ ''m''<sub>e</sub> implies ''p''<sup>2</sup>/(2''m''<sub>n</sub>) ≪ ''p''<sup>2</sup>/(2''m''<sub>e</sub>).  It is easy to show that for two bodies in circular orbits around their center of mass (regardless of individual masses), the momenta of the two bodies are equal and opposite, and that for any collection of particles in the center-of-mass frame, the net momentum is zero.  Given that the center-of-mass frame is the lab frame (where the molecule is stationary), the momentum of the nuclei must be equal and opposite to that of the electrons.  A hand-waving justification can be derived from quantum mechanics as well.  Recall that the corresponding operators do not contain mass and think of the molecule as a box containing the electrons and nuclei and see [[particle in a box]]. Since the kinetic energy is ''p''<sup>2</sup>/(2''m''), it follows that, indeed, the kinetic energy of the nuclei in a molecule is usually much smaller than the kinetic energy of the electrons, the mass ratio being on the order of 10<sup>4</sup>).{{Citation needed|date=January 2015}}</ref> that is, the corresponding operator ''T''<sub>n</sub> is subtracted from the total [[molecular Hamiltonian]]. In the remaining electronic Hamiltonian ''H''<sub>e</sub> the nuclear positions enter as parameters. The electron&ndash;nucleus interactions are ''not'' removed, and the electrons still \"feel\" the [[Coulomb potential]] of the nuclei clamped at certain positions in space. (This first step of the BO approximation is therefore often referred to as the ''clamped-nuclei'' approximation.)\n\nThe electronic [[Schrödinger equation]]\n\n:<math> H_\\text{e}(\\mathbf r, \\mathbf R) \\chi(\\mathbf r, \\mathbf R) = E_\\text{e} \\chi(\\mathbf r, \\mathbf R) </math>\n\nis solved (out of necessity, approximately). The quantity '''r''' stands for all electronic coordinates and '''R''' for all nuclear coordinates. The electronic energy [[eigenvalue]] ''E''<sub>e</sub> depends on the chosen positions '''R''' of the nuclei. Varying these positions '''R''' in small steps and repeatedly solving the electronic [[Schrödinger equation]], one obtains ''E''<sub>e</sub> as a function of '''R'''. This is the [[potential energy surface]] (PES): ''E''<sub>e</sub>('''R''') . Because this procedure of recomputing the electronic wave functions as a function of an infinitesimally changing nuclear geometry is reminiscent of the conditions for the [[adiabatic theorem]], this manner of obtaining a PES is often referred to as the ''adiabatic approximation'' and the PES itself is called an ''adiabatic surface''.<ref>It is assumed, in accordance with the [[adiabatic theorem]], that the same electronic state (for instance, the electronic ground state) is obtained upon small changes of the nuclear geometry. The method would give a discontinuity (jump) in the PES if electronic state switching would occur.{{Citation needed|date=January 2015}}</ref>\n\nIn the second step of the BO approximation the nuclear kinetic energy ''T''<sub>n</sub> (containing partial derivatives with respect to the components of '''R''') is reintroduced, and the Schrödinger equation for the nuclear motion<ref>This equation is time-independent, and stationary wavefunctions for the nuclei are obtained; nevertheless, it is traditional to use the word \"motion\" in this context, although classically motion implies time dependence.{{Citation needed|date=January 2015}}\n</ref>\n\n:<math> [T_\\text{n} + E_\\text{e}(\\mathbf R)] \\phi(\\mathbf R) = E \\phi(\\mathbf R) </math>\n\nis solved. This second step of the BO approximation involves separation of vibrational, translational, and rotational motions. This can be achieved by application of the [[Eckart conditions]]. The eigenvalue ''E'' is the total energy of the molecule, including contributions from electrons, nuclear vibrations, and overall rotation and translation of the molecule.\n\n==Derivation==\nIt will be discussed how the BO approximation may be derived and under which conditions it is applicable. At the same time we will show how the BO approximation may be improved by including [[vibronic coupling]]. To that end the second step of the BO approximation is generalized to a set of coupled eigenvalue equations depending on nuclear coordinates only. Off-diagonal elements in these equations are shown to be nuclear kinetic energy terms.\n\nIt will be shown that the BO approximation can be trusted whenever the PESs, obtained from the solution of the electronic Schrödinger equation, are well separated:\n\n:<math> E_0(\\mathbf{R}) \\ll E_1(\\mathbf{R}) \\ll E_2(\\mathbf{R}) \\ll \\cdots \\text{ for all }\\mathbf{R}</math>.\n\nWe start from the ''exact'' non-relativistic, time-independent molecular Hamiltonian:\n\n:<math>\nH = H_\\text{e} + T_\\text{n}\n</math>\nwith\n:<math>\nH_\\text{e} =\n-\\sum_{i}{\\frac{1}{2}\\nabla_i^2} -\n\\sum_{i,A}{\\frac{Z_A}{r_{iA}}} + \\sum_{i>j}{\\frac{1}{r_{ij}}} + \\sum_{B > A}{\\frac{Z_A Z_B}{R_{AB}}}\n\\quad\\text{and}\\quad T_\\text{n} = -\\sum_{A}{\\frac{1}{2M_A}\\nabla_A^2}.\n</math>\n\nThe position vectors <math>\\mathbf{r} \\equiv \\{\\mathbf{r}_i\\}</math> of the electrons and the position vectors <math>\\mathbf{R} \\equiv \\{\\mathbf{R}_A = (R_{Axy}, R_{Ayz}, R_{Azx})\\}</math> of the nuclei are with respect to a Cartesian [[inertial frame]]. Distances between particles are written as <math>r_{iA} \\equiv |\\mathbf{r}_i - \\mathbf{R}_A|</math> (distance between electron ''i'' and nucleus ''A'') and similar definitions hold for <math>r_{ij}</math> and <math> R_{AB}</math>.\n\nWe assume that the molecule is in a homogeneous (no external force) and isotropic (no external torque) space. The only interactions are the two-body Coulomb interactions among the electrons and nuclei. The Hamiltonian is expressed in [[atomic units]], so that we do not see Planck's constant, the dielectric constant of the vacuum, electronic charge, or electronic mass in this formula. The only constants explicitly entering the formula are ''Z<sub>A</sub>'' and ''M<sub>A</sub>'' – the atomic number and mass of nucleus ''A''.\n\nIt is useful to introduce the total nuclear momentum and to rewrite the nuclear kinetic energy operator as follows:\n\n:<math> T_\\text{n} = \\sum_{A} \\sum_{\\alpha=x,y,z} \\frac{P_{A\\alpha} P_{A\\alpha}}{2M_A}\n\\quad\\text{with}\\quad\nP_{A\\alpha} = -i \\frac{\\partial}{\\partial R_{A\\alpha}}. </math>\n\nSuppose we have ''K'' electronic eigenfunctions <math>\\chi_k (\\mathbf{r}; \\mathbf{R})</math> of <math>H_\\text{e}</math>, that is, we have solved\n\n:<math>\nH_\\text{e} \\chi_k(\\mathbf{r}; \\mathbf{R}) = E_k(\\mathbf{R}) \\chi_k(\\mathbf{r}; \\mathbf{R}) \\quad\\text{for}\\quad k = 1, \\ldots, K.\n</math>\n\nThe electronic wave functions <math>\\chi_k</math> will be taken to be real, which is possible when there are no magnetic or spin interactions. The ''parametric dependence'' of the functions <math>\\chi_k</math> on the nuclear coordinates is indicated by the symbol after the semicolon. This indicates that, although <math>\\chi_k</math> is a real-valued function of <math>\\mathbf{r}</math>, its functional form depends on <math>\\mathbf{R}</math>.\n\nFor example, in the molecular-orbital-linear-combination-of-atomic-orbitals [[Molecular orbital#Qualitative discussion|(LCAO-MO)]] approximation, <math>\\chi_k</math> is a molecular orbital (MO) given as a linear expansion of atomic orbitals (AOs). An AO depends visibly on the coordinates of an electron, but the nuclear coordinates are not explicit in the MO. However, upon change of geometry, i.e., change of <math>\\mathbf{R}</math>, the LCAO coefficients obtain different values and we see corresponding changes in the functional form of the MO <math>\\chi_k</math>.\n\nWe will assume that the parametric dependence is continuous and differentiable, so that it is meaningful to consider\n\n:<math>\nP_{A\\alpha}\\chi_k(\\mathbf{r}; \\mathbf{R}) = -i \\frac{\\partial\\chi_k(\\mathbf{r}; \\mathbf{R})}{\\partial R_{A\\alpha}}\n\\quad \\text{for}\\quad\n\\alpha = x,y,z,\n</math>\nwhich in general will not be zero.\n\nThe total wave function <math>\\Psi(\\mathbf{R}, \\mathbf{r})</math> is expanded in terms of <math>\\chi_k(\\mathbf{r}; \\mathbf{R})</math>:\n:<math>\n\\Psi(\\mathbf{R}, \\mathbf{r}) = \\sum_{k=1}^K \\chi_k(\\mathbf{r}; \\mathbf{R}) \\phi_k(\\mathbf{R}),\n</math>\nwith\n:<math>\n\\langle \\chi_{k'}(\\mathbf{r}; \\mathbf{R}) | \\chi_k(\\mathbf{r}; \\mathbf{R}) \\rangle_{(\\mathbf{r})} = \\delta_{k' k},\n</math>\nand where the subscript <math>(\\mathbf{r})</math> indicates that the integration, implied by the [[bra–ket notation]], is over electronic coordinates only. By definition, the matrix with general element\n:<math> \\big(\\mathbb{H}_\\text{e}(\\mathbf{R})\\big)_{k'k} \\equiv \\langle \\chi_{k'}(\\mathbf{r}; \\mathbf{R})\n        | H_\\text{e} |\n        \\chi_k(\\mathbf{r}; \\mathbf{R}) \\rangle_{(\\mathbf{r})} = \\delta_{k'k} E_k(\\mathbf{R})\n</math>\nis diagonal. After multiplication by the real function <math>\\chi_{k'}(\\mathbf{r}; \\mathbf{R})</math> from the left and integration over the electronic coordinates <math>\\mathbf{r}</math> the total Schrödinger equation\n:<math>\nH \\Psi(\\mathbf{R}, \\mathbf{r}) = E \\Psi(\\mathbf{R}, \\mathbf{r})\n</math>\nis turned into a set of ''K'' coupled eigenvalue equations depending on nuclear coordinates only\n\n:<math> [\\mathbb{H}_\\text{n}(\\mathbf{R}) + \\mathbb{H}_\\text{e}(\\mathbf{R})] \\boldsymbol{\\phi}(\\mathbf{R}) =\n E \\boldsymbol{\\phi}(\\mathbf{R}).\n</math>\n\nThe column vector <math>\\boldsymbol{\\phi}(\\mathbf{R})</math> has elements <math>\\phi_k(\\mathbf{R}),\\ k = 1, \\ldots, K</math>. The matrix <math>\\mathbb{H}_\\text{e}(\\mathbf{R})</math> is diagonal, and the nuclear Hamilton matrix is non-diagonal; its off-diagonal (''vibronic coupling'') terms <math> \\big(\\mathbb{H}_\\text{n}(\\mathbf{R})\\big)_{k'k}</math> are further discussed below. The vibronic coupling in this approach is through nuclear kinetic energy terms.\n\nSolution of these coupled equations gives an approximation for energy and wavefunction that goes beyond the Born–Oppenheimer approximation.\nUnfortunately, the off-diagonal kinetic energy terms are usually difficult to handle. This is why often a [[diabatic]] transformation is applied, which retains part of the nuclear kinetic energy terms on the diagonal, removes the kinetic energy terms from the off-diagonal and creates coupling terms between the adiabatic PESs on the off-diagonal.\n\nIf we can neglect the off-diagonal elements the equations will uncouple and simplify drastically. In order to show when this neglect is justified, we suppress the coordinates in the notation and write, by applying the [[Leibniz rule (generalized product rule)|Leibniz rule]] for differentiation, the matrix elements of <math>T_\\text{n}</math> as\n:<math>\nH_\\text{n}(\\mathbf{R})_{k'k} \\equiv\n \\big(\\mathbb{H}_\\text{n}(\\mathbf{R})\\big)_{k'k}\n = \\delta_{k'k} T_\\text{n}\n - \\sum_{A,\\alpha}\\frac{1}{M_A} \\langle\\chi_{k'}|P_{A\\alpha}|\\chi_k\\rangle_{(\\mathbf{r})} P_{A\\alpha} + \\langle\\chi_{k'}|T_\\text{n}|\\chi_k\\rangle_{(\\mathbf{r})}.\n</math>\n\nThe diagonal (<math>k' = k</math>) matrix elements <math>\\langle\\chi_{k}|P_{A\\alpha}|\\chi_k\\rangle_{(\\mathbf{r})}</math> of the operator <math>P_{A\\alpha}</math> vanish, because we assume time-reversal invariant, so <math>\\chi_k</math> can be chosen to be always real. The off-diagonal matrix elements satisfy\n\n:<math>\n\\langle\\chi_{k'}|P_{A\\alpha}|\\chi_k\\rangle_{(\\mathbf{r})} =\n \\frac{\\langle\\chi_{k'}| [P_{A\\alpha}, H_\\text{e}] |\\chi_k\\rangle_{(\\mathbf{r})}}\n      {E_{k}(\\mathbf{R}) - E_{k'}(\\mathbf{R})}.\n</math>\nThe matrix element in the numerator is\n:<math>\n\\langle\\chi_{k'}| [P_{A\\alpha}, H_\\mathrm{e}] |\\chi_k\\rangle_{(\\mathbf{r})} =\niZ_A\\sum_i \\left\\langle\\chi_{k'}\\left|\\frac{(\\mathbf{r}_{iA})_\\alpha}{r_{iA}^3}\\right|\\chi_k\\right\\rangle_{(\\mathbf{r})}\n\\quad\\text{with}\\quad\n\\mathbf{r}_{iA} \\equiv \\mathbf{r}_i - \\mathbf{R}_A.\n</math>\nThe matrix element of the one-electron operator appearing on the right side is finite.\n\nWhen the two surfaces come close, <math>E_{k}(\\mathbf{R}) \\approx E_{k'}(\\mathbf{R})</math>, the nuclear momentum coupling term becomes large and is no longer negligible. This is the case where the BO approximation breaks down, and a coupled set of nuclear motion equations must be considered instead of the one equation appearing in the second step of the BO approximation.\n\nConversely, if all surfaces are well separated, all off-diagonal terms can be neglected, and hence the whole matrix of <math>P^A_\\alpha</math> is effectively zero. The third term on the right side of the expression for the matrix element of ''T''<sub>n</sub> (the ''Born–Oppenheimer diagonal correction'') can approximately be written as the matrix of <math>P^A_\\alpha</math> squared and, accordingly, is then negligible also. Only the first (diagonal) kinetic energy term in this equation survives in the case of well separated surfaces, and a diagonal, uncoupled, set of nuclear motion equations results:\n:<math>\n[T_\\text{n} + E_k(\\mathbf{R})] \\phi_k(\\mathbf{R}) = E \\phi_k(\\mathbf{R})\n\\quad\\text{for}\\quad\nk = 1, \\ldots, K,\n</math>\nwhich are the normal second step of the BO equations discussed above.\n\nWe reiterate that when two or more potential energy surfaces approach each other, or even cross, the Born–Oppenheimer approximation breaks down, and one must fall back on the coupled equations. Usually one invokes then the [[diabatic]] approximation.\n\n==The Born–Oppenheimer approximation with the correct symmetry==\nTo include the correct symmetry within the Born–Oppenheimer (BO) approximation,<ref name=BornOppie>{{cite journal|author1=Max Born |author2=J. Robert Oppenheimer|title=Zur Quantentheorie der Molekeln|journal=Annalen der Physik|year=1927|volume=389|issue=20|pages=457–484|doi=10.1002/andp.19273892002|trans-title=On the Quantum Theory of Molecules|language=German|bibcode = 1927AnP...389..457B }}</ref><ref>M. Born and K. Huang, Dynamical Theory of Crystal Lattices, 1954 (Oxford University Press, New York), Chapter IV.</ref> a molecular system presented in terms of (mass-dependent) nuclear coordinates <math>\\mathbf{q}</math> and formed by the two lowest BO adiabatic potential energy surfaces (PES) <math>u_1(\\mathbf{q})</math> and <math>u_2 (\\mathbf{q})</math> is considered. To ensure the validity of the BO approximation, the energy ''E'' of the system is assumed to be low enough so that <math>u_2 (\\mathbf{q})</math> becomes a closed PES in the region of interest, with the exception of sporadic infinitesimal sites surrounding degeneracy points formed by <math>u_1(\\mathbf{q})</math> and <math>u_2(\\mathbf{q})</math> (designated as (1, 2) degeneracy points).\n\nThe starting point is the nuclear adiabatic BO (matrix) equation written in the form<ref>M. Baer, Beyond Born–Oppenheimer: Electronic non-Adiabatic Coupling Terms and Conical Intersections, 2006 (Wiley and Sons, Inc., Hoboken, N.J.), Chapter 2.</ref>\n\n: <math>\\frac{\\hbar^2}{2m} (\\nabla + \\tau)^2 \\Psi + (\\mathbf{u} - E)\\Psi = 0, </math>\n\nwhere <math>\\Psi(\\mathbf{q}) </math> is a column vector containing the unknown nuclear wave functions <math>\\psi_k(\\mathbf{q})</math>, <math>\\mathbf{u}(\\mathbf{q})</math> is a diagonal matrix containing the corresponding adiabatic potential energy surfaces <math>u_k(\\mathbf{q})</math>, ''m'' is the reduced mass of the nuclei, ''E'' is the total energy of the system, <math>\\nabla</math> is the [[gradient]] operator with respect to the nuclear coordinates <math>\\mathbf{q}</math>, and <math>\\mathbf{\\tau}(\\mathbf{q})</math> is a matrix containing the vectorial non-adiabatic coupling terms (NACT):\n\n:<math>\\mathbf{\\tau}_{jk} = \\langle \\zeta_j | \\nabla\\zeta_k \\rangle.</math>\n\nHere <math>|\\zeta_n\\rangle</math> are eigenfunctions of the [[electronic Hamiltonian]] assumed to form a complete [[Hilbert space]] in the given region in [[Configuration space (physics)|configuration space]].\n\nTo study the scattering process taking place on the two lowest surfaces, one extracts from the above BO equation the two corresponding equations:\n\n:<math>-\\frac{\\hbar^2}{2m} \\nabla^2\\psi_1 + (\\tilde{u}_1 - E)\\psi_1 - \\frac{\\hbar^2}{2m} [2\\mathbf{\\tau}_{12}\\nabla + \\nabla\\mathbf{\\tau}_{12}]\\psi_2 = 0,</math>\n\n:<math>-\\frac{\\hbar^2}{2m} \\nabla^2\\psi_2 + (\\tilde{u}_2 - E)\\psi_2 + \\frac{\\hbar^2}{2m} [2\\mathbf{\\tau}_{12}\\nabla + \\nabla\\mathbf{\\tau}_{12}]\\psi_1 = 0,</math>\n\nwhere <math>\\tilde{u}_k(\\mathbf{q}) = u_k(\\mathbf{q}) + (\\hbar^{2}/2m)\\tau_{12}^2</math> (''k'' = 1, 2), and <math>\\mathbf\\tau_{12} = \\mathbf\\tau_{12}(\\mathbf{q})</math> is the (vectorial) NACT responsible for the coupling between <math>u_1(\\mathbf{q})</math> and <math>u_2(\\mathbf{q})</math>.\n\nNext a new function is introduced:<ref>M. Baer and R. Englman, Chem. Phys. Lett. 265, 105 (1997).</ref>\n\n:<math> \\chi = \\psi_1 + i\\psi_2, </math>\n\nand the corresponding rearrangements are made:\n\n1. Multiplying the second equation by ''i'' and combining it with the first equation yields the (complex) equation\n\n:<math>-\\frac{\\hbar^2}{2m} \\nabla^{2}\\chi + (\\tilde{u}_1 - E)\\chi + i\\frac{\\hbar^2}{2m}[2\\mathbf{\\tau}_{12}\\nabla + \\nabla\\mathbf{\\tau}_{12}]\\chi + i(u_1 - u_2)\\psi_2 = 0.</math>\n\n2. The last term in this equation can be deleted for the following reasons:  At those points where <math>u_2(\\mathbf{q})</math> is classically closed, <math>\\psi_{2}(\\mathbf{q}) \\sim 0</math> by definition, and at those points where <math>u_2(\\mathbf{q})</math> becomes classically allowed (which happens at the vicinity of the (1, 2) degeneracy points) this implies that: <math>u_1(\\mathbf{q}) \\sim u_2(\\mathbf{q})</math>, or <math>u_1(\\mathbf{q}) - u_2(\\mathbf{q}) \\sim 0</math>. Consequently, the last term is, indeed, negligibly small at every point in the region of interest, and the equation simplifies to become\n\n:<math>-\\frac{\\hbar^2}{2m} \\nabla^{2}\\chi + (\\tilde{u}_1 - E)\\chi + i\\frac{\\hbar^2}{2m}[2\\mathbf{\\tau}_{12}\\nabla + \\nabla\\mathbf{\\tau}_{12}]\\chi = 0.</math>\n\nIn order for this equation to yield a solution with the correct symmetry, it is suggested to apply a perturbation approach based on an elastic potential <math>u_0(\\mathbf{q})</math>, which coincides with <math>u_1(\\mathbf{q})</math> at the asymptotic region.\n\nThe equation with an elastic potential can be solved, in a straightforward manner, by substitution. Thus, if <math>\\chi_0</math> is the solution of this equation, it is presented as\n\n:<math>\\chi_0(\\mathbf{q}|\\Gamma) = \\xi_{0}(\\mathbf{q}) \\exp\\left[-i \\int_\\Gamma d\\mathbf{q}' \\cdot \\mathbf{\\tau}(\\mathbf{q}'|\\Gamma)\\right],</math>\n\nwhere <math>\\Gamma</math> is an arbitrary contour, and the exponential function contains the relevant symmetry as created while moving along <math>\\Gamma</math>.\n\nThe function <math>\\xi_0(\\mathbf{q})</math> can be shown to be a solution of the (unperturbed/elastic) equation\n\n:<math>-\\frac{\\hbar^2}{2m} \\nabla^{2}\\xi_0 + (u_0 - E) \\xi_0 = 0.</math>\n\nHaving <math>\\chi_0(\\mathbf{q}|\\Gamma)</math>, the full solution of the above decoupled equation takes the form\n\n:<math>\\chi(\\mathbf{q}|\\Gamma) = \\chi_0(\\mathbf{q}|\\Gamma) + \\eta(\\mathbf{q}|\\Gamma),</math>\n\nwhere <math>\\eta(\\mathbf{q}|\\Gamma)</math> satisfies the resulting inhomogeneous equation:\n\n:<math>-\\frac{\\hbar^2}{2m} \\nabla^{2}\\eta + (\\tilde{u}_1 - E)\\eta + i\\frac{\\hbar^2}{2m}[2\\mathbf{\\tau}_{12}\\nabla + \\nabla\\mathbf{\\tau}_{12}]\\eta = (u_1 - u_0)\\chi_0.</math>\n\nIn this equation the inhomogeneity ensures the symmetry for the perturbed part of the solution along any contour and therefore for the solution in the required region in configuration space.\n\nThe relevance of the present approach was demonstrated while studying a two-arrangement-channel model (containing one inelastic channel and one reactive channel) for which the two adiabatic states were coupled by a [[Jahn–Teller effect|Jahn–Teller]] [[conical intersection]].<ref>(a) R. Baer, D. M. Charutz, R. Kosloff and M. Baer, J. Chem. Phys. 111, 9141 (1996); (b) S. Adhikari and G. D. Billing, J. Chem. Phys. 111, 40 (1999).</ref><ref>D. M. Charutz, R. Baer and M. Baer, Chem. Phys. Lett. 265, 629 (1996).</ref>  A nice fit between the symmetry-preserved single-state treatment and the corresponding two-state treatment was obtained. This applies in particular to the reactive state-to-state probabilities (see Table III in Ref. 5a and Table III in Ref. 5b), for which the ordinary BO approximation led to erroneous results, whereas the symmetry-preserving BO approximation produced the accurate results, as they followed from solving the two coupled equations.\n\n==See also==\n* [[Adiabatic ionization]]\n* [[Adiabatic process (quantum mechanics)]]\n* [[Avoided crossing]]\n* [[Born–Huang approximation]]\n* [[Franck–Condon principle]]\n\n==References ==\n{{Reflist|300em}}\n\n==External links==\nResources related to the Born&ndash;Oppenheimer approximation:\n* [http://gallica.bnf.fr/ark:/12148/bpt6k15386r The original article] (in German)\n* [http://elib.bsu.by/bitstream/123456789/154381/1/1927-084%20AP%20Born%20%26%20Oppenheimer%20-%20On%20the%20Quantum%20Theory%20of%20Molecules.pdf Translation by S. M. Blinder]\n* [http://www.tcm.phy.cam.ac.uk/%7Epdh1001/thesis/node13.html The Born&ndash;Oppenheimer approximation], a section from Peter Haynes' doctoral thesis\n\n{{Use dmy dates|date=March 2017}}\n\n{{DEFAULTSORT:Born-Oppenheimer approximation}}\n[[Category:Quantum chemistry]]\n[[Category:Approximations]]"
    },
    {
      "title": "Hartman–Grobman theorem",
      "url": "https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem",
      "text": "In [[mathematics]], in the study of [[dynamical systems]], the '''Hartman–Grobman theorem''' or '''linearization theorem''' is a theorem about the local behavior of dynamical systems in the [[neighbourhood (mathematics)|neighbourhood]] of a [[hyperbolic equilibrium point]].  It asserts that [[linearization]]—a natural simplification of the system—is effective in predicting qualitative patterns of behavior.\n\nThe theorem states that the behavior of a dynamical system in a domain near a hyperbolic equilibrium point is qualitatively the same as the behavior of its [[linearization]] near this equilibrium point, where hyperbolicity means that no eigenvalue of the linearization has real part equal to zero. Therefore, when dealing with such dynamical systems one can use the simpler linearization of the system to analyze its behavior around equilibria.<ref>{{cite book |first=D. K. |last=Arrowsmith |first2=C. M. |last2=Place |title=Dynamical Systems: Differential Equations, Maps, and Chaotic Behaviour |chapter=The Linearization Theorem |publisher=Chapman & Hall |location=London |year=1992 |isbn=978-0-412-39080-7 |pages=77–81 |chapterurl=https://books.google.com/books?id=8qCcP7KNaZ0C&pg=PA77 }}</ref>\n\n== Main theorem ==\nConsider a system evolving in time with state <math>u(t)\\in\\mathbb R^n</math> that satisfies the differential equation <math>du/dt=f(u)</math> for some [[smooth map]] <math>f: \\mathbb{R}^n \\to \\mathbb{R}^n</math>.  Suppose the map has a hyperbolic equilibrium state <math>u^*\\in\\mathbb R^n</math>: that is, <math>f(u^*)=0</math> and the [[Jacobian matrix]] <math>A=[\\partial f_i/\\partial x_j]</math> of <math>f</math> at state <math>u^*</math> has no [[eigenvalue]] with real part equal to zero. Then there exists a neighborhood <math>N</math> of the equilibrium <math>u^*</math> and a [[homeomorphism]] <math>h : N \\to \\mathbb{R}^n</math>,\nsuch that  <math>h(u^*)=0</math> and such that in the neighbourhood <math>N</math> the [[flow (mathematics)|flow]] of <math>du/dt=f(u)</math> is [[topologically conjugate]] by the continuous map <math>U=h(u)</math> to the flow of its linearization <math>dU/dt=AU</math>.<ref>{{cite journal|last = Grobman|first = D. M.|title=О гомеоморфизме систем дифференциальных уравнений|trans-title= Homeomorphisms of systems of differential equations|journal = [[Doklady Akademii Nauk SSSR]]|volume = 128|pages = 880–881|year = 1959}}</ref><ref>{{cite journal|last = Hartman|first = Philip|authorlink=Philip Hartman|title = A lemma in the theory of structural stability of differential equations|journal = Proc. A.M.S.|volume = 11|issue = 4|pages = 610–620|doi = 10.2307/2034720|date=August 1960|jstor=2034720}}<!--|accessdate = 2010-05-28--></ref><ref>{{cite journal|last = Hartman|first = Philip|title = On local homeomorphisms of Euclidean spaces|journal = Bol. Soc. Math. Mexicana|volume = 5|pages = 220–241|year = 1960}}</ref><ref>{{cite book |first=C. |last=Chicone |title=Ordinary Differential Equations with Applications |volume=34 |series=Texts in Applied Mathematics |location= |publisher=Springer |edition=2nd |year=2006 |isbn=978-0-387-30769-5 }}</ref>\n\nEven for infinitely differentiable maps <math>f</math>, the homeomorphism <math>h</math> need not to be smooth, nor even locally Lipschitz. However, it turns out to be [[Hölder continuous]], with an exponent depending on the constant of hyperbolicity of <math>A</math>.<ref>{{cite paper |first=Genrich |last=Belitskii |first2=Victoria |last2=Rayskin |year=2011 |title=On the Grobman–Hartman theorem in α-Hölder class for Banach spaces |work=[[Working paper]] |url=http://www.ma.utexas.edu/mp_arc/c/11/11-134.pdf }}</ref>\n\nThe Hartman–Grobman theorem has been extended to infinite-dimensional Banach spaces, non-autonomous systems <math>du/dt=f(u,t)</math> (potentially stochastic), and to cater for the topological differences that occur when there are eigenvalues with zero or near-zero real-part.<ref>{{cite book |first=B.  |last=Aulbach |first2=T. |last2=Wanner |chapter=Integral manifolds for Caratheodory type differential equations in Banach spaces |editor-first=B. |editor-last=Aulbach |editor2-first=F. |editor2-last=Colonius |title=Six Lectures on Dynamical Systems |pages=45–119 |publisher=World Scientific |location=Singapore |year=1996 |isbn=978-981-02-2548-3 }}</ref><ref>{{cite book |first=B. |last=Aulbach |first2=T. |last2=Wanner |chapter=Invariant Foliations for Carathéodory Type Differential Equations in Banach Spaces |editor-first=V. |editor-last=Lakshmikantham |editor2-first=A. A. |editor2-last=Martynyuk |title=Advances in Stability Theory at the End of the 20th Century |publisher=Gordon & Breach |year=1999 |isbn=978-0-415-26962-9 |citeseerx=10.1.1.45.5229 }}</ref><ref>{{cite journal | last1 = Aulbach | first1 = B. | last2 = Wanner | first2 = T. | year = 2000 | title = The Hartman–Grobman theorem for Caratheodory-type differential equations in Banach spaces | url = | journal = Non-linear Analysis | volume = 40 | issue = 1–8| pages = 91–104 | doi = 10.1016/S0362-546X(00)85006-3 }}</ref><ref>{{cite journal | last1 = Roberts | first1 = A. J. | year = 2008 | title = Normal form transforms separate slow and fast modes in stochastic dynamical systems | doi = 10.1016/j.physa.2007.08.023 | journal = Physica A | volume = 387 | issue = 1| pages = 12–38 | arxiv = math/0701623 | bibcode = 2008PhyA..387...12R }}</ref>\n\n==Example==\n\nThe algebra necessary for this example is easily carried out by a web service that computes [[Normal_form_(dynamical_systems)|normal form]] coordinate transforms of systems of differential equations, autonomous or non-autonomous, deterministic or [[Stochastic differential equation|stochastic]].<ref>{{cite web |first=A. J. |last=Roberts |title=Normal form of stochastic or deterministic multiscale differential equations |url=http://www.maths.adelaide.edu.au/anthony.roberts/sdenf.php |year=2007 |archiveurl=https://web.archive.org/web/20131109164316/http://www.maths.adelaide.edu.au/anthony.roberts/sdenf.php |archivedate=November 9, 2013 }}</ref>\n\nConsider the 2D system in variables <math>u=(y,z)</math> evolving according to the pair of coupled differential equations\n\n: <math> \\frac{dy}{dt} = -3y+yz\\quad\\text{and}\\quad \\frac{dz}{dt} = z+y^2.</math>\n\nBy direct computation it can be seen that the only equilibrium of this system lies at the origin, that is <math>u^*=0</math>.  The coordinate transform, <math>u=h^{-1}(U)</math> where <math>U=(Y,Z)</math>, given by\n\n: <math>\n\\begin{align}\ny & \\approx Y+YZ+\\dfrac1{42}Y^3+\\dfrac1 2Y Z^2 \\\\[5pt]\nz & \\approx Z-\\dfrac1 7Y^2-\\dfrac1 3Y^2 Z\n\\end{align}\n</math>\n\nis a smooth map between the original <math>u=(y,z)</math> and new <math>U=(Y,Z)</math> coordinates, at least near the equilibrium at the origin.  In the new coordinates the dynamical system  transforms to its linearisation\n\n: <math> \\frac{dY}{dt}=-3Y\\quad\\text{and}\\quad \\frac{dZ}{dt} = Z.</math>\n\nThat is, a distorted version of the linearization gives the original dynamics in some finite neighbourhood.\n\n==See also==\n*[[Stable manifold theorem]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite book |first=Lawrence |last=Perko |title=Differential Equations and Dynamical Systems |location=New York |publisher=Springer |edition=Third |year=2001 |isbn=0-387-95116-4 |pages=119–127 }}\n* {{cite book |first=Clark |last=Robinson |title=Dynamical Systems : Stability, Symbolic Dynamics, and Chaos |location=Boca Raton |publisher=CRC Press |year=1995 |isbn=0-8493-8493-1 |pages=156–165 }}\n\n==External links==\n*{{cite journal|last = Coayla-Teran|first = E. |author2=Mohammed, S. |author3=Ruffino, P.|title = Hartman–Grobman Theorems along Hyperbolic Stationary Trajectories|journal = Discrete and Continuous Dynamical Systems|volume = 17|issue = 2|pages = 281–292|date=February 2007|url = http://sfde.math.siu.edu/Hartmangrobman.pdf|accessdate = 2007-03-09|doi = 10.3934/dcds.2007.17.281}}\n* {{cite book| last = Teschl| given = Gerald|authorlink=Gerald Teschl| title = Ordinary Differential Equations and Dynamical Systems| publisher=[[American Mathematical Society]]| place = [[Providence, Rhode Island|Providence]]| year = 2012| isbn= 978-0-8218-8328-0| url = http://www.mat.univie.ac.at/~gerald/ftp/book-ode/}}\n* {{cite web |title=The Most Addictive Theorem in Applied Mathematics |url=https://blogs.scientificamerican.com/roots-of-unity/the-most-addictive-theorem-in-applied-mathematics/ |website=Scientific American}}\n\n{{DEFAULTSORT:Hartman-Grobman Theorem}}\n[[Category:Theorems in analysis]]\n[[Category:Theorems in dynamical systems]]\n[[Category:Approximations]]"
    },
    {
      "title": "Linearization",
      "url": "https://en.wikipedia.org/wiki/Linearization",
      "text": "{{for|the linearization of a partial order|Linear extension}}\n{{for|the linearization in concurrent computing|Linearizability}}\nIn [[mathematics]], '''linearization''' is finding the [[linear approximation]] to a [[function (mathematics)|function]] at a given point. The linear approximation of a function is the first order [[Taylor expansion]] around the point of interest. In the study of [[dynamical system]]s, linearization is a method for assessing the local [[stability theory|stability]] of an [[equilibrium point]] of a [[system]] of [[nonlinear]] [[differential equation]]s or discrete [[dynamical system]]s.<ref>[http://www.scholarpedia.org/article/Siegel_disks/Linearization The linearization problem in complex dimension one dynamical systems at Scholarpedia]</ref>  This method is used in fields such as [[engineering]], [[physics]], [[economics]], and [[ecology]].\n\n==Linearization of a function==\nLinearizations of a [[function (mathematics)|function]] are [[linear function|lines]]—usually lines that can be used for purposes of calculation. Linearization is an effective method for approximating the output of a function <math>y = f(x)</math> at any <math>x = a</math> based on the value and [[slope]] of the function at <math>x = b</math>, given that <math>f(x)</math> is differentiable on <math>[a, b]</math> (or <math>[b, a]</math>) and that <math>a</math> is close to <math>b</math>. In short, linearization approximates the output of a function near <math>x = a</math>.\n\nFor example, <math>\\sqrt{4} = 2</math>. However, what would be a good approximation of <math>\\sqrt{4.001} = \\sqrt{4 + .001}</math>?\n\nFor any given function <math>y = f(x)</math>, <math>f(x)</math> can be approximated if it is near a known differentiable point. The most basic requisite is that <math>L_a(a) = f(a)</math>, where <math>L_a(x)</math> is the linearization of <math>f(x)</math> at <math>x = a</math>. The [[Linear equation#Point–slope form|point-slope form]] of an equation forms an equation of a line, given a point <math>(H, K)</math> and slope <math>M</math>. The general form of this equation is: <math>y - K = M(x - H)</math>.\n\nUsing the point <math>(a, f(a))</math>, <math>L_a(x)</math> becomes <math>y = f(a) + M(x - a)</math>. Because differentiable functions are [[Local linearity|locally linear]], the best slope to substitute in would be the slope of the line [[tangent]] to <math>f(x)</math> at <math>x = a</math>.\n\nWhile the concept of local linearity applies the most to points [[Limit of a function#Limit of a function at a point|arbitrarily close]] to <math>x = a</math>, those relatively close work relatively well for linear approximations. The slope <math>M</math> should be, most accurately, the slope of the tangent line at <math>x = a</math>.\n\n[[Image:Tangent-calculus.svg|thumb|300px|An approximation of f(x)=x^2 at (''x'', ''f''(''x''))]]\nVisually, the accompanying diagram shows the tangent line of <math>f(x)</math> at <math>x</math>. At <math>f(x+h)</math>, where <math>h</math> is any small positive or negative value, <math>f(x+h)</math> is very nearly the value of the tangent line at the point <math>(x+h, L(x+h))</math>.\n\nThe final equation for the linearization of a function at <math>x = a</math> is:\n\n<math>y = (f(a) + f'(a)(x - a))</math>\n\nFor <math>x = a</math>, <math>f(a) = f(x)</math>. The [[derivative]] of <math>f(x)</math> is <math>f'(x)</math>, and the slope of <math>f(x)</math> at <math>a</math> is <math>f'(a)</math>.\n\n==Example==\nTo find <math>\\sqrt{4.001}</math>, we can use the fact that <math>\\sqrt{4} = 2</math>. The linearization of  <math>f(x) = \\sqrt{x}</math> at <math>x = a</math> is <math>y = \\sqrt{a} + \\frac{1}{2 \\sqrt{a}}(x - a)</math>, because the function <math>f'(x) = \\frac{1}{2 \\sqrt{x}}</math> defines the slope of the function <math>f(x) = \\sqrt{x}</math> at <math>x</math>. Substituting in <math>a = 4</math>, the linearization at 4 is <math>y = 2 + \\frac{x-4}{4}</math>. In this case <math>x = 4.001</math>, so <math>\\sqrt{4.001}</math> is approximately <math>2 + \\frac{4.001-4}{4} = 2.00025</math>. The true value is close to 2.00024998, so the linearization approximation has a relative error of less than 1 millionth of a percent.\n\n==Linearization of a multivariable function==\nThe equation for the linearization of a function <math>f(x,y)</math> at a point <math>p(a,b)</math> is:\n\n<math> f(x,y) \\approx f(a,b) + \\left. {\\frac{{\\partial f(x,y)}}{{\\partial x}}} \\right|_{a,b} (x - a) + \\left. {\\frac{{\\partial f(x,y)}}{{\\partial y}}} \\right|_{a,b} (y - b)</math>\n\nThe general equation for the linearization of a multivariable function <math>f(\\mathbf{x})</math> at a point <math>\\mathbf{p}</math> is:\n\n<math>f({\\mathbf{x}}) \\approx f({\\mathbf{p}}) + \\left. {\\nabla f} \\right|_{\\mathbf{p}}  \\cdot ({\\mathbf{x}} - {\\mathbf{p}})</math>\n\nwhere <math>\\mathbf{x}</math> is the vector of variables, and <math>\\mathbf{p}</math> is the linearization point of interest\n.<ref>[http://www.ece.jhu.edu/~pi/Courses/454/linear.pdf Linearization. The Johns Hopkins University. Department of Electrical and Computer Engineering] {{webarchive|url=https://web.archive.org/web/20100607120539/http://www.ece.jhu.edu/~pi/Courses/454/linear.pdf |date=2010-06-07 }}</ref>\n\n==Uses of linearization==\n\nLinearization makes it possible to use tools for studying [[linear system]]s to analyze the behavior of a nonlinear function near a given point.  The linearization of a function is the first order term of its [[Taylor expansion]] around the point of interest.  For a system defined by the equation\n\n:<math>\\frac{d\\mathbf{x}}{dt} = \\mathbf{F}(\\mathbf{x},t)</math>,\n\nthe linearized system can be written as\n\n:<math>\\frac{d\\mathbf{x}}{dt} \\approx \\mathbf{F}(\\mathbf{x_0},t) + D\\mathbf{F}(\\mathbf{x_0},t)  \\cdot (\\mathbf{x} - \\mathbf{x_0})</math>\n\nwhere <math>\\mathbf{x_0}</math> is the point of interest and <math>D\\mathbf{F}(\\mathbf{x_0})</math> is the [[Jacobian matrix and determinant|Jacobian]] of <math>\\mathbf{F}(\\mathbf{x})</math> evaluated at <math>\\mathbf{x_0}</math>.\n\n===Stability analysis===\nIn [[stability theory|stability]] analysis of [[Autonomous_system_(mathematics)|autonomous systems]], one can use the [[eigenvalue]]s of the [[Jacobian matrix and determinant|Jacobian matrix]] evaluated at a [[hyperbolic equilibrium point]] to determine the nature of that equilibrium. This is the content of [[linearization theorem]]. For time-varying systems, the linearization requires additional justification.<ref>{{cite journal |first=G. A. |last=Leonov |first2=N. V. |last2=Kuznetsov |title=Time-Varying Linearization and the Perron effects |journal=[[International Journal of Bifurcation and Chaos]] |volume=17 |issue=4 |year=2007 |pages=1079–1107 |doi=10.1142/S0218127407017732 }}</ref>\n\n===Microeconomics===\nIn [[microeconomics]], [[decision rule]]s may be approximated under the state-space approach to linearization.<ref name=\"statespace\">Moffatt, Mike. (2008) [[About.com]] ''[http://economics.about.com/od/economicsglossary/g/statespace.htm State-Space Approach]'' Economics Glossary; Terms Beginning with S. Accessed June 19, 2008.</ref> Under this approach, the [[Euler_equations_(fluid_dynamics)#Linearized_form|Euler equations]] of the [[utility maximization problem]] are linearized around the stationary steady state.<ref name=\"statespace\"/> A unique solution to the resulting system of dynamic equations then is found.<ref name=\"statespace\"/>\n\n===Optimization===\nIn [[mathematical optimization]], cost functions and non-linear components within can be linearized in order to apply a linear solving method such as the [[Simplex algorithm]]. The optimized result is reached much more efficiently and is deterministic as a [[global optimum]].\n\n===Multiphysics===\nIn [[multiphysics]] systems—systems involving multiple physical fields that interact with one another—linearization with respect to each of the physical fields may be performed. This linearization of the system with respect to each of the fields results in a linearized monolithic equation system that can be solved using monolithic iterative solution procedures such as the [[Newton-Raphson]] method. Examples of this include [[MRI scanner]] systems which results in a system of electromagnetic, mechanical and acoustic fields.<ref>{{cite journal |first=S. |last=Bagwell |first2=P. D. |last2=Ledger |first3=A. J. |last3=Gil |first4=M. |last4=Mallett |first5=M. |last5=Kruip |year=2017 |title=A linearised ''hp''–finite element framework for acousto-magneto-mechanical coupling in axisymmetric MRI scanners |journal=International Journal for Numerical Methods in Engineering |volume=112 |issue=10 |pages=1323–1352 |doi=10.1002/nme.5559 }}</ref>\n\n==See also==\n* [[Linear stability]]\n* [[Tangent stiffness matrix]]\n* [[Stability derivatives]]\n* [[Linearization theorem]]\n* [[Taylor approximation]]\n* [[Functional equation (L-function)]]\n\n==References==\n{{reflist}}\n\n==External links==\n\n===Linearization tutorials===\n* [http://www.mathworks.com/discovery/linearization.html Linearization for Model Analysis and Control Design]\n\n[[Category:Differential calculus]]\n[[Category:Dynamical systems]]\n[[Category:Approximations]]"
    },
    {
      "title": "Precision (computer science)",
      "url": "https://en.wikipedia.org/wiki/Precision_%28computer_science%29",
      "text": "{{More citations needed|date=March 2007}}\n\nIn [[computer science]], the '''precision''' of a numerical quantity is a measure of the detail in which the quantity is expressed.  This is usually measured in bits, but sometimes in decimal digits. It is related to [[precision (arithmetic)|precision in mathematics]], which describes the number of digits that are used to express a value.\n\nSome of the standardized precision formats are \n* [[Half-precision floating-point format]]\n* [[Single-precision floating-point format]]\n* [[Double-precision floating-point format]]\n* [[Quadruple-precision floating-point format]]\n* [[Octuple-precision floating-point format]]\n\nOf these, octuple-precision format is rarely used. The single- and double-precision formats are most widely used and supported on nearly all platforms. The use of half-precision format has been increasing especially in the field of [[machine learning]] since many machine learning algorithms are inherently error-tolerant.<ref name=\"surveyACT\">{{cite journal|last1=Mittal|first1=Sparsh|title=A Survey of Techniques for Approximate Computing|journal=ACM Comput. Surv.|date=May 2016|volume=48|issue=4|pages=62:1–62:33|doi=10.1145/2893356|publisher=ACM|language=en|url=https://zenodo.org/record/1236172}}</ref>\n\n==Rounding error==\n{{further|Floating point}}\n\nPrecision is often the source of [[rounding error]]s in [[computation]]. The number of bits used to store a number will often cause some loss of accuracy. An example would be to store \"sin(0.1)\" in IEEE single precision floating point standard. The error is then often magnified as subsequent computations are made using the data (although it can also be reduced).\n\n== See also ==\n* [[Arbitrary-precision arithmetic]]\n* [[Extended precision]]\n* [[IEEE754]] (IEEE floating point standard)\n* [[Integer (computer science)]]\n* [[Significant figures]]\n* [[Truncation]]\n* [[Approximate computing]]\n\n==References==\n{{Reflist|60em}}\n\n[[Category:Computer data]]\n[[Category:Approximations]]"
    },
    {
      "title": "Relaxation (approximation)",
      "url": "https://en.wikipedia.org/wiki/Relaxation_%28approximation%29",
      "text": "In [[mathematical optimization]] and related fields, '''relaxation''' is a [[mathematical model|modeling strategy]].  A relaxation is an [[approximation theory|approximation]] of a difficult problem by a nearby problem that is easier to solve. A solution of the relaxed problem provides information about the original problem.\n\nFor example, a [[linear programming]] relaxation of an [[integer programming]] problem  removes the integrality constraint and so allows non-integer rational solutions. A [[Lagrangian relaxation]] of a complicated problem in combinatorial optimization penalizes violations of some constraints, allowing an easier relaxed problem to be solved. Relaxation techniques complement or supplement [[branch and bound]] algorithms of combinatorial optimization; linear programming and Lagrangian relaxations are used to obtain bounds in branch-and-bound algorithms for integer programming.<ref name=\"Geoff\">{{harvtxt|Geoffrion|1971}}</ref>\n\nThe modeling strategy of relaxation should not be confused with [[iterative method]]s of [[relaxation method|relaxation]],  such as [[successive over-relaxation]] (SOR); iterative methods of relaxation are used in solving problems in [[partial differential equation|differential equation]]s, [[linear least squares (mathematics)|linear least-squares]], and [[linear programming]].<ref name=\"Murty\">{{cite book|last=Murty|first=Katta&nbsp;G.|authorlink=Katta G. Murty|chapter=16 Iterative methods for linear inequalities and linear programs (especially 16.2 Relaxation methods, and 16.4 Sparsity-preserving iterative SOR algorithms for linear programming)| title=Linear programming|publisher=John Wiley & Sons, Inc.|location=New York|year=1983|pages=453–464|isbn=978-0-471-09725-9|mr=720547|ref=harv}}</ref><ref>{{cite journal|last=Goffin|first=J.-L.|title=The relaxation method for solving systems of linear inequalities|journal=Math. Oper. Res.|volume=5|year=1980|number=3|pages=388–414|jstor=3689446|doi=10.1287/moor.5.3.388|mr=594854|ref=harv}}</ref><ref name=\"Minoux\">{{cite book|last=Minoux|first=M.|authorlink=Michel Minoux|title=Mathematical programming: Theory and algorithms|others=Egon Balas (foreword)|edition=Translated by Steven Vajda from the (1983 Paris: Dunod) French|publisher=A Wiley-Interscience Publication. John Wiley & Sons, Ltd.|location=Chichester|year=1986|pages=xxviii+489|isbn=978-0-471-90170-9|mr=868279|ref=harv|id=(2008 Second ed., in French: ''Programmation mathématique: Théorie et algorithmes''. Editions Tec & Doc, Paris, 2008. xxx+711 pp. }}. {{MR|2571910}})</ref> However, iterative methods of relaxation have been used to solve Lagrangian relaxations.<ref>Relaxation methods for finding feasible solutions to linear inequality systems arise in linear programming and in Lagrangian relaxation. {{harvtxt|Goffin|1980}} and {{harvtxt|Minoux|1986}}|loc=Section 4.3.7, pp. 120–123  cite [[Shmuel Agmon]] (1954), and [[Theodore Motzkin]] and [[Isaac Schoenberg]] (1954), and L. T. Gubin, [[Boris T. Polyak]], and E. V. Raik (1969).</ref>\n\n== Definition ==\nA ''relaxation'' of the minimization problem\n\n: <math>z = \\min \\{c(x) : x \\in X \\subseteq \\mathbf{R}^{n}\\}</math>\n\nis another minimization problem of the form\n\n:<math>z_R = \\min \\{c_R(x) : x \\in X_R \\subseteq \\mathbf{R}^{n}\\}</math>\n\nwith these two properties\n\n# <math>X_R \\supseteq X</math>\n# <math>c_R(x) \\leq c(x)</math> for all <math>x \\in X</math>.\n\nThe first property states that the original problem's feasible domain is a subset of the relaxed problem's feasible domain. The second property states that the original problem's objective-function is greater than or equal to the relaxed problem's objective-function.<ref name=\"Geoff\"/>\n\n=== Properties===\nIf <math>x^*</math> is an optimal solution of the original problem, then <math>x^* \\in X \\subseteq X_R</math> and <math>z = c(x^*) \\geq c_R(x^*)\\geq z_R</math>. Therefore, <math>x^* \\in X_R</math> provides an upper bound on <math>z_R</math>.\n\nIf in addition to the previous assumptions, <math>c_R(x)=c(x)</math>, <math>\\forall x\\in X</math>, the following holds: If an optimal solution for the relaxed problem is feasible for the original problem, then it is optimal for the original problem.<ref name=\"Geoff\"/>\n\n==Some relaxation techniques==\n*[[Linear programming relaxation]]\n*[[Lagrangian relaxation]]\n<!-- NO this is an ITERATIVE METHOD, not a relaxation strategy,  *[[Successive over-relaxation]] -->\n*[[Semidefinite relaxation]]\n* [[Surrogate relaxation]] and [[surrogate duality|duality]]\n\n==Notes==\n<references/>\n\n==References==\n\n* {{cite book|author=G.Buttazzo\n|title = Semicontinuity, Relaxation and Integral Representation in the Calculus of Variations \n|series=Pitman Res. Notes in Math. 207|publisher=Longmann|location=Harlow|year=1989}}\n\n* {{cite news |last1=Geoffrion |first1=A. M. |title=Duality in Nonlinear Programming: A Simplified Applications-Oriented Development|journal=SIAM Review |volume=13 |year=1971 |number=1 |pages=1–37|jstor=2028848|ref=harv}}.\n* {{cite journal|last=Goffin|first=J.-L.|title=The relaxation method for solving systems of linear inequalities|journal=Math. Oper. Res.|volume=5|year=1980|number=3|pages=388–414|jstor=3689446|doi=10.1287/moor.5.3.388|mr=594854|ref=harv}}\n* {{cite book|last=Minoux|first=M.|authorlink=Michel Minoux|title=Mathematical programming: Theory and algorithms \n|edition=(With a foreword by Egon Balas) Translated by Steven Vajda from the (1983 Paris: Dunod) French|publisher=A Wiley-Interscience Publication. John Wiley & Sons, Ltd.|location=Chichester|year=1986|pages=xxviii+489|isbn=978-0-471-90170-9|mr=868279|ref=harv|id=(2008 Second ed., in French: ''Programmation mathématique: Théorie et algorithmes''. Editions Tec & Doc, Paris, 2008. xxx+711 pp. }}. {{MR|2571910}})|\n\n* {{cite book|title=Optimization|editor1-first=G. L.|editor1-last=Nemhauser|editor1-link=George L. Nemhauser|editor2-first=A. H. G.|editor2-last=Rinnooy Kan|editor3-first=M. J.|editor3-last=Todd|editor3-link=Michael J. Todd (mathematician)|series=Handbooks in Operations Research and Management Science|volume=1|publisher=North-Holland Publishing Co.|location=Amsterdam|year=1989|pages=xiv+709|isbn=978-0-444-87284-5|mr=1105099|ref=harv}}\n<!-- ** [[J. E. Dennis, Jr.]] and [[Robert B. Schnabel]], A view of unconstrained optimization (pp. 1–72);\n** [[Donald Goldfarb]] and [[Michael J. Todd (mathematician)|Michael J. Todd]], Linear programming (pp. 73–170);\n** Philip E. Gill, Walter Murray, Michael A. Saunders, and [[Margaret H. Wright]], Constrained nonlinear programming (pp. 171–210);\n** [[Ravindra K. Ahuja]], [[Thomas L. Magnanti]], and [[James B. Orlin]], Network flows (pp. 211–369); -->\n** [[W. R. Pulleyblank]], Polyhedral combinatorics (pp.&nbsp;371–446);\n** George L. Nemhauser and Laurence A. Wolsey, Integer programming (pp.&nbsp;447–527);\n** [[Claude Lemaréchal]], Nondifferentiable optimization (pp.&nbsp;529–572);\n<!-- ** [[Roger J-B Wets]], Stochastic programming (pp. 573–629);\n** A. H. G. Rinnooy Kan and G. T. Timmer, Global optimization (pp. 631–662);\n** P. L. Yu, Multiple criteria decision making: five basic concepts (pp. 663--699). -->\n* {{cite book|title=Optimization in operations research|first=Ronald L.|last=Rardin|publisher=Prentice Hall|date=1998|isbn=978-0-02-398415-0}}\n* {{cite book|title=Relaxation in Optimization Theory and Variational Calculus|last=Roubíček|first=T.|publisher=Walter de Gruyter|location=Berlin|year=1997|isbn=978-3-11-014542-7}}\n\n{{DEFAULTSORT:Relaxation (Approximation)}}\n[[Category:Mathematical optimization]]\n[[Category:Relaxation (approximation)| ]]\n[[Category:Approximations]]"
    },
    {
      "title": "Rough set",
      "url": "https://en.wikipedia.org/wiki/Rough_set",
      "text": "In [[computer science]], a '''rough set''', first described by [[Poles|Polish]] computer scientist [[Zdzislaw Pawlak|Zdzisław I. Pawlak]], is a formal approximation of a [[crisp set]] (i.e., conventional set) in terms of a pair of sets which give the ''lower'' and the ''upper'' approximation of the original set. In the standard version of rough set theory (Pawlak 1991), the lower- and upper-approximation sets are crisp sets, but in other variations, the approximating sets may be [[fuzzy sets]].\n\n==Definitions==\nThe following section contains an overview of the basic framework of rough set theory, as originally proposed by [[Zdzislaw Pawlak|Zdzisław I. Pawlak]], along with some of the key definitions. More formal properties and boundaries of rough sets can be found in Pawlak (1991) and cited references. The initial and basic theory of rough sets is sometimes referred to as ''\"Pawlak Rough Sets\"'' or ''\"classical rough sets\"'', as a means to distinguish from more recent extensions and generalizations.\n\n===Information system framework===\nLet <math>I = (\\mathbb{U},\\mathbb{A})</math> be an information system ([[attribute-value system]]), where <math> \\mathbb{U}</math> is a non-empty, finite set of objects (the universe) and <math> \\mathbb{A}</math> is a non-empty, finite set of attributes such that <math>a:\\mathbb{U} \\rightarrow V_a</math> for every <math>a \\in \\mathbb{A}</math>. <math>V_a</math> is the set of values that attribute <math>a</math> may take. The information table assigns a value <math>a(x)</math> from <math>V_a</math> to each attribute <math>a</math> and object <math>x</math> in the universe <math>\\mathbb{U}</math>.\n\nWith any <math>P \\subseteq \\mathbb{A}</math> there is an associated [[equivalence relation]] <math>\\mathrm{IND}(P)</math>:\n\n:<math>\n  \\mathrm{IND}(P) = \\left\\{(x,y) \\in \\mathbb{U}^2 \\mid \\forall a \\in P, a(x)=a(y)\\right\\}\n</math>\n\nThe relation <math>\\mathrm{IND}(P)</math> is called a <math>P</math>''-indiscernibility relation''. The partition of <math>\\mathbb{U}</math> is a family of all equivalence classes of <math>\\mathrm{IND}(P)</math> and is denoted  by <math>\\mathbb{U}/\\mathrm{IND}(P)</math> (or <math>\\mathbb{U}/P</math>).\n\nIf <math>(x,y)\\in \\mathrm{IND}(P)</math>, then <math>x</math> and <math>y</math> are ''indiscernible'' (or indistinguishable) by attributes from <math>P</math> .\n\nThe equivalence classes of the <math>P</math>-indiscernibility relation are denoted <math>[x]_P</math>.\n\n===Example: equivalence-class structure===\nFor example, consider the following information table:\n\n:{| class=\"wikitable\" style=\"text-align:center; width:30%\" border=\"1\"\n|+ Sample Information System\n! Object !! <math>P_{1}</math> !! <math>P_{2}</math> !! <math>P_{3}</math> !! <math>P_{4}</math> !! <math>P_{5}</math>\n|-\n! <math>O_{1}</math>\n| 1 || 2 || 0 || 1 || 1\n|-\n! <math>O_{2}</math>\n| 1 || 2 || 0 || 1 || 1\n|-\n! <math>O_{3}</math>\n| 2 || 0 || 0 || 1 || 0\n|-\n! <math>O_{4}</math>\n| 0 || 0 || 1 || 2 || 1\n|-\n! <math>O_{5}</math>\n| 2 || 1 || 0 || 2 || 1\n|-\n! <math>O_{6}</math>\n| 0 || 0 || 1 || 2 || 2\n|-\n! <math>O_{7}</math>\n| 2 || 0 || 0 || 1 || 0\n|-\n! <math>O_{8}</math>\n| 0 || 1 || 2 || 2 || 1\n|-\n! <math>O_{9}</math>\n| 2 || 1 || 0 || 2 || 2\n|-\n! <math>O_{10}</math>\n| 2 || 0 || 0 || 1 || 0\n|}\n\nWhen the full set of attributes <math>P = \\{P_{1},P_{2},P_{3},P_{4},P_{5}\\}</math> is considered, we see that we have the following seven equivalence classes:\n\n:<math>\n\\begin{cases} \n\\{O_{1},O_{2}\\} \\\\ \n\\{O_{3},O_{7},O_{10}\\} \\\\ \n\\{O_{4}\\} \\\\ \n\\{O_{5}\\} \\\\\n\\{O_{6}\\} \\\\\n\\{O_{8}\\} \\\\\n\\{O_{9}\\} \\end{cases}\n</math>\n\nThus, the two objects within the first equivalence class, <math>\\{O_{1},O_{2}\\}</math>, cannot be distinguished from each other based on the available attributes, and the three objects within the second equivalence class, <math>\\{O_{3},O_{7},O_{10}\\}</math>, cannot be distinguished from one another based on the available attributes. The remaining five objects are each discernible from all other objects.\n\nIt is apparent that different attribute subset selections will in general lead to different indiscernibility classes. For example, if attribute <math>P =\\{ P_{1}\\}</math> alone is selected, we obtain the following, much coarser, equivalence-class structure:\n\n:<math>\n\\begin{cases}\n\\{O_{1},O_{2}\\} \\\\ \n\\{O_{3},O_{5},O_{7},O_{9},O_{10}\\} \\\\ \n\\{O_{4},O_{6},O_{8}\\} \\end{cases}\n</math>\n\n===Definition of a ''rough set''===\nLet <math>X \\subseteq \\mathbb{U}</math> be a target set that we wish to represent using attribute subset <math>P</math>; that is, we are told that an arbitrary set of objects <math>X</math> comprises a single class, and we wish to express this class (i.e., this subset) using the equivalence classes induced by attribute subset <math>P</math>. In general, <math>X</math> cannot be expressed exactly, because the set may include and exclude objects which are indistinguishable on the basis of attributes <math>P</math>.\n\nFor example, consider the target set <math>X = \\{O_{1},O_{2},O_{3},O_{4}\\}</math>, and let attribute subset <math>P = \\{P_{1}, P_{2}, P_{3}, P_{4}, P_{5}\\}</math>, the full available set of features.  It will be noted that the set <math>X</math> cannot be expressed exactly, because in <math>[x]_P,</math>, objects <math>\\{O_{3}, O_{7}, O_{10}\\}</math> are indiscernible. Thus, there is no way to represent any set <math>X</math> which ''includes'' <math>O_{3}</math> but ''excludes'' objects <math>O_{7}</math> and <math>O_{10}</math>.\n\nHowever, the target set <math>X</math> can be ''approximated'' using only the information contained within <math>P</math> by constructing the <math>P</math>-lower and <math>P</math>-upper approximations of <math>X</math>:\n\n:<math>\n  {\\underline P}X= \\{x \\mid [x]_P \\subseteq X\\}\n</math>\n\n:<math>\n  {\\overline P}X = \\{x \\mid [x]_P \\cap X \\neq \\emptyset \\}\n</math>\n\n====Lower approximation and positive region====\nThe <math>P</math>''-lower approximation'', or ''positive region'', is the union of all equivalence classes in <math>[x]_P</math> which are contained by (i.e., are subsets of) the target set – in the example, <math>{\\underline P}X = \\{O_{1}, O_{2}\\} \\cup \\{O_{4}\\}</math>, the union of the two equivalence classes in <math>[x]_P</math> which are contained in the target set. The lower approximation is the complete set of objects in <math>\\mathbb{U}/P</math> that can be ''positively'' (i.e., unambiguously) classified as belonging to target set <math>X</math>.\n\n====Upper approximation and negative region====\nThe <math>P</math>''-upper approximation'' is the union of all equivalence classes in <math>[x]_P</math> which have non-empty intersection with the target set – in the example, <math>{\\overline P}X = \\{O_{1}, O_{2}\\} \\cup \\{O_{4}\\} \\cup \\{O_{3}, O_{7}, O_{10}\\}</math>, the union of the three equivalence classes in <math>[x]_P</math> that have non-empty intersection with the target set. The upper approximation is the complete set of objects that in <math>\\mathbb{U}/P</math> that ''cannot'' be positively (i.e., unambiguously) classified as belonging to the ''complement'' (<math>\\overline X</math>) of the target set <math>X</math>. In other words, the upper approximation is the complete set of objects that are ''possibly'' members of the  target set <math>X</math>.\n\nThe set <math>\\mathbb{U}-{\\overline P}X</math> therefore represents the ''negative region'', containing the set of objects that can be definitely ruled out as members of the target set.\n\n====Boundary region====\nThe ''boundary region'', given by set difference <math>{\\overline P}X - {\\underline P}X</math>, consists of those objects that can neither be ruled in nor ruled out as members of the target set <math>X</math>.\n\nIn summary, the lower approximation of a target set is a ''conservative'' approximation consisting of only those objects which can positively be identified as members of the set. (These objects have no indiscernible \"clones\" which are excluded by the target set.) The upper approximation is a ''liberal'' approximation which includes all objects that might be members of target set. (Some objects in the upper approximation may not be members of the target set.) From the perspective of <math>\\mathbb{U}/P</math>, the lower approximation contains objects that are members of the target set with certainty (probability = 1), while the upper approximation contains objects that are members of the target set with non-zero probability (probability > 0).\n\n====The rough set====\nThe tuple <math>\\langle{\\underline P}X,{\\overline P}X\\rangle</math> composed of the lower and upper approximation is called a ''rough set''; thus, a rough set is composed of two crisp sets, one representing a ''lower boundary'' of the target set <math>X</math>, and the other representing an ''upper boundary'' of the target set <math>X</math>.\n\nThe ''accuracy'' of the rough-set representation of the set <math>X</math> can be given (Pawlak 1991) by the following:\n\n:<math>\n\\alpha_{P}(X) = \\frac{\\left | {\\underline P}X \\right |} {\\left | {\\overline P}X \\right |} \n</math>\n\nThat is, the accuracy of the rough set representation of <math>X</math>, <math>\\alpha_{P}(X)</math>, <math>0 \\leq \\alpha_{P}(X) \\leq 1</math>, is the ratio of the number of objects which can ''positively'' be placed in <math>X</math> to the number of objects that can ''possibly'' be placed in <math>X</math> – this provides a measure of how closely the rough set is approximating the target set. Clearly, when the upper and lower approximations are equal (i.e., boundary region empty), then <math>\\alpha_{P}(X) = 1</math>, and the approximation is perfect; at the other extreme, whenever the lower approximation is empty, the accuracy is zero (regardless of the size of the upper approximation).\n\n====Objective analysis====\nRough set theory is one of many methods that can be employed to analyse uncertain (including vague) systems, although less common than more traditional methods of [[probability]], [[statistics]], [[Entropy (information theory)|entropy]] and [[Dempster–Shafer theory]]. However a key difference, and a unique strength, of using classical rough set theory is that it provides an objective form of analysis (Pawlak et al. 1995). Unlike other methods, as those given above, classical rough set analysis requires no additional information, external parameters, models, functions, grades or subjective interpretations to determine set membership – instead it only uses the information presented within the given data (Düntsch and Gediga 1995).  More recent adaptations of rough set theory, such as dominance-based, decision-theoretic and fuzzy rough sets, have introduced more subjectivity to the analysis.\n\n===Definability===\nIn general, the upper and lower approximations are not equal; in such cases, we say that target set <math>X</math> is ''undefinable'' or ''roughly definable'' on attribute set <math>P</math>. When the upper and lower approximations are equal (i.e., the boundary is empty),  <math>{\\overline P}X = {\\underline P}X</math>, then the target set <math>X</math> is ''definable'' on attribute set <math>P</math>. We can distinguish the following special cases of undefinability:\n\n* Set <math>X</math> is ''internally'' ''undefinable'' if <math>{\\underline P}X = \\emptyset</math> and <math>{\\overline P}X \\neq \\mathbb{U}</math>.  This means that on attribute set <math>P</math>, there are ''no'' objects which we can be certain belong to target set <math>X</math>, but there ''are'' objects which we can definitively exclude from set <math>X</math>.\n* Set <math>X</math> is ''externally undefinable'' if <math>{\\underline P}X \\neq \\emptyset</math> and <math>{\\overline P}X = \\mathbb{U}</math>.  This means that on attribute set <math>P</math>, there ''are'' objects which we can be certain belong to target set <math>X</math>, but there are ''no'' objects which we can definitively exclude from set <math>X</math>.\n* Set <math>X</math> is ''totally undefinable'' if <math>{\\underline P}X = \\emptyset</math> and <math>{\\overline P}X = \\mathbb{U}</math>.  This means that on attribute set <math>P</math>, there are ''no'' objects which we can be certain belong to target set <math>X</math>, and there are ''no'' objects which we can definitively exclude from set <math>X</math>.  Thus, on attribute set <math>P</math>, we cannot decide whether any object is, or is not, a member of <math>X</math>.\n\n===Reduct and core===\nAn interesting question is whether there are attributes in the information system (attribute-value table) which are more important to the knowledge represented in the equivalence class structure than other attributes.  Often, we wonder whether there is a subset of attributes which can, by itself, fully characterize the knowledge in the database; such an attribute set is called a ''reduct''.\n\nFormally, a reduct is a subset of attributes <math>\\mathrm{RED} \\subseteq P</math> such that\n\n* <math>[x]_{\\mathrm{RED}}</math> = <math>[x]_P</math>, that is, the equivalence classes induced by the reduced attribute set <math>\\mathrm{RED}</math> are the same as the equivalence class structure induced by the full attribute set <math>P</math>.\n* the attribute set <math>\\mathrm{RED}</math> is ''minimal'', in the sense that <math>[x]_{(\\mathrm{RED}-\\{a\\})} \\neq [x]_P</math> for any attribute <math>a \\in \\mathrm{RED}</math>; in other words, no attribute can be removed from set <math>\\mathrm{RED}</math> without changing the equivalence classes <math>[x]_P</math>.\n\nA reduct can be thought of as a ''sufficient'' set of features – sufficient, that is, to represent the category structure. In the example table above, attribute set <math>\\{P_3,P_4,P_5\\}</math> is a reduct – the information system projected on just these attributes possesses the same equivalence class structure as that expressed by the full attribute set:\n\n:<math>\n\\begin{cases} \n\\{O_{1},O_{2}\\} \\\\ \n\\{O_{3},O_{7},O_{10}\\} \\\\ \n\\{O_{4}\\} \\\\ \n\\{O_{5}\\} \\\\\n\\{O_{6}\\} \\\\\n\\{O_{8}\\} \\\\\n\\{O_{9}\\} \\end{cases}\n</math>\n\nAttribute set <math>\\{P_3,P_4,P_5\\}</math> is a reduct because eliminating any of these attributes causes a collapse of the equivalence-class structure, with the result that <math>[x]_{\\mathrm{RED}} \\neq [x]_P</math>.\n\nThe reduct of an information system is ''not unique'': there may be many subsets of attributes which preserve the equivalence-class structure (i.e., the knowledge) expressed in the information system.  In the example information system above, another reduct is <math>\\{P_1,P_2,P_5\\}</math>, producing the same equivalence-class structure as <math>[x]_P</math>.\n\nThe set of attributes which is common to all reducts is called the ''core'': the core is the set of attributes which is possessed by ''every'' reduct, and therefore consists of attributes which cannot be removed from the information system without causing collapse of the equivalence-class structure.  The core may be thought of as the set of ''necessary'' attributes – necessary, that is, for the category structure to be represented. In the example, the only such attribute is <math>\\{P_5\\}</math>; any one of the other attributes can be removed singly without damaging the equivalence-class structure, and hence these are all ''dispensable''.  However, removing <math>\\{P_5\\}</math> by itself ''does'' change the equivalence-class structure, and thus <math>\\{P_5\\}</math> is the ''indispensable'' attribute of this information system, and hence the core.\n\nIt is possible for the core to be empty, which means that there is no indispensable attribute: any single attribute in such an information system can be deleted without altering the equivalence-class  structure. In such cases, there is no ''essential'' or necessary attribute which is required for the class structure to be represented.\n\n===Attribute dependency===\nOne of the most important aspects of database analysis or data acquisition is the discovery of attribute dependencies; that is, we wish to discover which variables are strongly related to which other variables. Generally, it is these strong relationships that will warrant further investigation, and that will ultimately be of use in predictive modeling.\n\nIn rough set theory, the notion of dependency is defined very simply. Let us take two (disjoint) sets of attributes, set <math>P</math> and set <math>Q</math>, and inquire what degree of dependency obtains between them.  Each attribute set induces an (indiscernibility) equivalence class structure, the equivalence classes induced by <math>P</math> given by <math>[x]_P</math>, and the equivalence classes induced by <math>Q</math> given by <math>[x]_Q</math>.\n\nLet <math>[x]_Q = \\{Q_1, Q_2, Q_3, \\dots, Q_N \\}</math>, where <math>Q_i</math> is a given equivalence class from the equivalence-class structure induced by attribute set <math>Q</math>.  Then, the ''dependency'' of attribute set <math>Q</math> on attribute set <math>P</math>, <math>\\gamma_{P}(Q)</math>, is given by\n\n:<math>\n\\gamma_{P}(Q) =  \\frac{\\sum_{i=1}^N \\left | {\\underline P}Q_i \\right |} {\\left | \\mathbb{U} \\right |} \\leq 1\n</math>\n\nThat is, for each equivalence class <math>Q_i</math> in <math>[x]_Q</math>, we add up the size of its lower approximation by the attributes in <math>P</math>, i.e., <math>{\\underline P}Q_i</math>.  This approximation (as above, for arbitrary set <math>X</math>) is the number of objects which on attribute set <math>P</math> can be positively identified as belonging to target set <math>Q_i</math>. Added across all equivalence classes in <math>[x]_Q</math>, the numerator above represents the total number of objects which – based on attribute set <math>P</math> – can be positively categorized according to the classification induced by attributes <math>Q</math>. The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects.  The dependency <math>\\gamma_{P}(Q)</math> \"can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in <math>P</math> to determine the values of attributes in <math>Q</math>\".\n\nAnother, intuitive, way to consider dependency is to take the partition induced by Q as the target class C, and consider P as the attribute set we wish to use in order to \"re-construct\" the target class C. If P can completely reconstruct C, then Q depends totally upon P; if P results in a poor and perhaps a random reconstruction of C, then Q does not depend upon P at all.\n\nThus, this measure of dependency expresses the degree of ''functional'' (i.e., deterministic) dependency of attribute set <math>Q</math> on attribute set <math>P</math>; it is ''not'' symmetric.  The relationship of this notion of attribute dependency to more traditional information-theoretic (i.e., entropic) notions of attribute dependence has been discussed in a number of sources (e.g., Pawlak, Wong, & Ziarko 1988; Yao & Yao 2002; Wong, Ziarko, & Ye 1986, Quafafou & Boussouf 2000).\n\n==Rule extraction==\nThe category representations discussed above are all ''extensional'' in nature; that is, a category or complex class is simply the sum of all its members. To represent a category is, then, just to be able to list or identify all the objects belonging to that category.  However, extensional category representations have very limited practical use, because they provide no insight for deciding whether novel (never-before-seen) objects are members of the category.\n\nWhat is generally desired is an ''intentional'' description of the category, a representation of the category based on a set of ''rules'' that describe the scope of the category.  The choice of such rules is not unique, and therein lies the issue of [[inductive bias]]. See [[Version space]] and [[Model selection]] for more about this issue.\n\nThere are a few rule-extraction methods.  We will start from a rule-extraction procedure based on Ziarko & Shan (1995).\n\n===Decision matrices===\n\nLet us say that we wish to find the minimal set of consistent rules ([[logical implication]]s) that characterize our sample system.  For a set of ''condition'' attributes <math>\\mathcal{P} = \\{P_1, P_2, P_3, \\dots , P_n\\}</math> and a decision attribute <math>Q, Q \\notin \\mathcal{P}</math>, these rules should have the form <math>P_i^a P_j^b \\dots P_k^c \\to Q^d</math>, or, spelled out,\n\n:<math>(P_i=a) \\land (P_j=b) \\land \\dots \\land (P_k=c) \\to (Q=d)</math>\n\nwhere <math>\\{a, b, c, \\dots\\}</math> are legitimate values from the domains of their respective attributes.  This is a form typical of [[association rules]], and the number of items in <math>\\mathbb{U}</math> which match the condition/antecedent is called the ''support'' for the rule.  The method for extracting such rules given in {{Harvtxt|Ziarko|Shan|1995}} is to form a ''decision matrix'' corresponding to each individual value <math>d</math> of decision attribute <math>Q</math>.  Informally, the decision matrix for value <math>d</math> of decision attribute <math>Q</math> lists all attribute–value pairs that ''differ'' between objects having <math>Q = d </math> and <math>Q \\ne d</math>.\n\nThis is best explained by example (which also avoids a lot of notation).  Consider the table above, and let <math>P_{4}</math> be the decision variable (i.e., the variable on the right side of the implications) and let <math>\\{P_1,P_2,P_3\\}</math> be the condition variables (on the left side of the implication). We note that the decision variable <math>P_{4}</math> takes on two different values, namely <math>\\{1, 2\\}</math>.  We treat each case separately.\n\nFirst, we look at the case <math>P_{4}=1</math>, and we divide up <math>\\mathbb{U}</math> into objects that have <math>P_{4}=1</math> and those that have <math>P_{4} \\ne 1</math>. (Note that objects with <math>P_{4} \\ne 1</math> in this case are simply the objects that have <math>P_{4}=2</math>, but in general, <math>P_{4} \\ne 1</math> would include all objects having any value for <math>P_{4}</math> ''other than'' <math>P_{4}=1</math>, and there may be several such classes of objects (for example, those having <math>P_{4}=2,3,4,etc.</math>).)  In this case, the objects having <math>P_{4}=1</math> are <math>\\{O_1,O_2,O_3,O_7,O_{10}\\}</math> while the objects which have <math>P_{4} \\ne 1</math> are <math>\\{O_4,O_5,O_6,O_8,O_9\\}</math>.  The decision matrix for <math>P_{4}=1</math> lists all the differences between the objects having <math>P_{4}=1</math> and those having <math>P_{4} \\ne 1</math>;  that is, the decision matrix lists all the differences between <math>\\{O_1,O_2,O_3,O_7,O_{10}\\}</math> and <math>\\{O_4,O_5,O_6,O_8,O_9\\}</math>.  We put the \"positive\" objects (<math>P_{4}=1</math>) as the rows, and the \"negative\" objects <math>P_{4} \\ne 1</math> as the columns.\n\n:{| class=\"wikitable\" style=\"text-align:center; width:30%\" border=\"1\"\n|+ Decision matrix for <math>P_{4}=1</math>\n! Object !! <math>O_{4}</math> !! <math>O_{5}</math> !! <math>O_{6}</math> !! <math>O_{8}</math> !! <math>O_{9}</math>\n|-\n! <math>O_{1}</math>\n| <math>P_1^1,P_2^2,P_3^0</math>|| <math>P_1^1,P_2^2</math> || <math>P_1^1,P_2^2,P_3^0</math> || <math>P_1^1,P_2^2,P_3^0</math> || <math>P_1^1,P_2^2</math>  \n|-\n! <math>O_{2}</math>\n| <math>P_1^1,P_2^2,P_3^0</math> || <math>P_1^1,P_2^2</math> || <math>P_1^1,P_2^2,P_3^0</math> || <math>P_1^1,P_2^2,P_3^0</math> || <math>P_1^1,P_2^2</math>\n|-\n! <math>O_{3}</math>\n| <math>P_1^2,P_3^0</math> || <math>P_2^0</math> || <math>P_1^2,P_3^0</math> || <math>P_1^2,P_2^0,P_3^0</math> || <math>P_2^0</math>\n|-\n! <math>O_{7}</math>\n| <math>P_1^2,P_3^0</math> || <math>P_2^0</math> || <math>P_1^2,P_3^0</math> || <math>P_1^2,P_2^0,P_3^0</math> || <math>P_2^0</math>\n|-\n! <math>O_{10}</math>\n| <math>P_1^2,P_3^0</math> || <math>P_2^0</math> || <math>P_1^2,P_3^0</math> || <math>P_1^2,P_2^0,P_3^0</math> || <math>P_2^0</math>\n|}\n\nTo read this decision matrix, look, for example, at the intersection of row <math>O_{3}</math> and column <math>O_{6}</math>, showing <math>P_1^2,P_3^0</math> in the cell.  This means that ''with regard to'' decision value <math>P_{4}=1</math>, object <math>O_{3}</math> differs from object <math>O_{6}</math> on attributes <math>P_1</math> and <math>P_3</math>, and the particular values on these attributes for the positive object <math>O_{3}</math> are <math>P_1=2</math> and <math>P_3=0</math>. This tells us that the correct classification of <math>O_{3}</math> as belonging to decision class <math>P_{4}=1</math> rests on attributes <math>P_1</math> and <math>P_3</math>;  although one or the other might be dispensable, we know that ''at least one'' of these attributes is ''in''dispensable.\n\nNext, from each decision matrix we form a set of [[Boolean logic|Boolean]] expressions, one expression for each row of the matrix. The items within each cell are aggregated disjunctively, and the individuals cells are then aggregated conjunctively. Thus, for the above table we have the following five Boolean expressions:\n\n:<math>\n\\begin{cases}\n(P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2) \\land (P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2) \\\\ \n(P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2) \\land (P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2) \\\\\n(P_1^2 \\lor P_3^0) \\land (P_2^0) \\land (P_1^2 \\lor P_3^0) \\land (P_1^2 \\lor P_2^0 \\lor P_3^0) \\land (P_2^0) \\\\\n(P_1^2 \\lor P_3^0) \\land (P_2^0) \\land (P_1^2 \\lor P_3^0) \\land (P_1^2 \\lor P_2^0 \\lor P_3^0) \\land (P_2^0) \\\\\n(P_1^2 \\lor P_3^0) \\land (P_2^0) \\land (P_1^2 \\lor P_3^0) \\land (P_1^2 \\lor P_2^0 \\lor P_3^0) \\land (P_2^0)\n\\end{cases}\n</math>\n\nEach statement here is essentially a highly specific (probably ''too'' specific) rule governing the membership in class <math>P_{4}=1</math>  of the corresponding object. For example, the last statement, corresponding to object <math>O_{10}</math>, states that all the following must be satisfied: \n# Either <math>P_1</math> must have value 2, or  <math>P_3</math> must have value 0, or both.\n# <math>P_2</math> must have value 0.\n# Either <math>P_1</math> must have value 2, or  <math>P_3</math> must have value 0, or both.\n# Either <math>P_1</math> must have value 2, or <math>P_2</math> must have value 0, or <math>P_3</math> must have value 0, or any combination thereof.\n# <math>P_2</math> must have value 0.\n\nIt is clear that there is a large amount of redundancy here, and the next step is to simplify using traditional [[Boolean algebra (logic)|Boolean algebra]].  The statement <math>(P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2) \\land (P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2 \\lor P_3^0) \\land (P_1^1 \\lor P_2^2)</math>  corresponding to objects <math>\\{O_{1},O_{2}\\}</math> simplifies to <math>P_1^1  \\lor P_2^2</math>, which yields the implication\n\n:<math>(P_1=1)  \\lor (P_2=2) \\to (P_{4}=1)</math>\n\nLikewise, the statement <math>(P_1^2 \\lor P_3^0) \\land (P_2^0) \\land (P_1^2 \\lor P_3^0) \\land (P_1^2 \\lor P_2^0 \\lor P_3^0) \\land (P_2^0)</math> corresponding to objects <math>\\{O_{3},O_{7},O_{10}\\}</math> simplifies to <math>P_1^2 P_2^0 \\lor P_3^0 P_2^0</math>.  This gives us the implication\n\n:<math>(P_1=2 \\land P_2=0) \\lor (P_3=0 \\land P_2=0) \\to (P_{4}=1)</math>\n\nThe above implications can also be written as the following rule set:\n\n:<math>\n\\begin{cases}\n(P_1=1) \\to (P_{4}=1) \\\\\n(P_2=2) \\to (P_{4}=1) \\\\\n(P_1=2) \\land (P_2=0) \\to (P_{4}=1) \\\\\n(P_3=0) \\land (P_2=0) \\to (P_{4}=1) \n\\end{cases}\n</math>\n\nIt can be noted that each of the first two rules has a ''support'' of 1 (i.e., the antecedent matches two objects), while each of the last two rules has a support of 2.  To finish writing the rule set for this knowledge system, the same procedure as above (starting with writing a new decision matrix) should be followed for the case of <math>P_{4}=2</math>, thus yielding a new set of implications for that decision value (i.e., a set of implications with <math>P_{4}=2</math> as the consequent). In general, the procedure will be repeated for each possible value of the decision variable.\n\n===LERS rule induction system===\n\nThe data system LERS (Learning from Examples based on Rough Sets) Grzymala-Busse (1997) may induce rules from inconsistent data, i.e., data with conflicting objects.  Two objects are conflicting when they are characterized by the same values of all attributes, but they belong to different concepts (classes). LERS uses rough set theory to compute lower and upper approximations for concepts involved in conflicts with other concepts.\n\nRules induced from the lower approximation of the concept ''certainly'' describe the concept, hence such rules are called ''certain''.  On the other hand, rules induced from the upper approximation of the concept describe the concept ''possibly'', so these rules are called ''possible''.  For rule induction LERS uses three algorithms: LEM1, LEM2, and IRIM.\n\nThe LEM2 algorithm of LERS is frequently used for rule induction and is used not only in LERS but also in other systems, e.g., in RSES (Bazan et al. (2004).  LEM2 explores the search space of [[attribute-value pair]]s.  Its input data set is a lower or upper approximation of a concept, so its input data set is always consistent.  In general, LEM2 computes a local covering and then converts it into a rule set.  We will quote a few definitions to describe the LEM2 algorithm.\n\nThe LEM2 algorithm is based on an idea of an attribute-value pair block.  Let <math>X</math> be a nonempty lower or upper approximation of a concept represented by a decision-value pair <math>(d, w)</math>.  Set <math>X</math> ''depends'' on a set <math>T</math> of attribute-value pairs <math>t = (a, v)</math>  if and only if\n\n: <math>\\emptyset \\neq [T] = \\bigcap_{t \\in T} [t] \\subseteq X.</math>\n\nSet <math>T</math> is a ''minimal complex'' of <math>X</math> if and only if <math>X</math> depends on <math>T</math> and no proper subset <math>S</math> of <math>T</math> exists such that <math>X</math> depends on <math>S</math>.  Let <math>\\mathbb{T}</math> be a nonempty collection of nonempty sets of attribute-value pairs.  Then <math>\\mathbb{T}</math> is a ''local covering'' of <math>X</math> if and only if the following three conditions are satisfied:\n\neach member <math>T</math> of <math>\\mathbb{T}</math> is a minimal complex of <math>X</math>,\n\n: <math>\n\\bigcup_{t \\in \\mathbb{T}} [T]  = X, </math>\n\n: <math>\\mathbb{T}</math> is minimal, i.e., <math>\\mathbb{T}</math> has the smallest possible number of members.\n\nFor our sample information system, LEM2 will induce the following rules:\n\n:<math>\n\\begin{cases}\n(P_1, 1) \\to (P_4, 1) \\\\\n(P_5, 0) \\to (P_4, 1) \\\\\n(P_1, 0) \\to (P_4, 2) \\\\\n(P_2, 1) \\to (P_4, 2)\n\\end{cases}\n</math>\n\nOther rule-learning methods can be found, e.g., in Pawlak (1991), Stefanowski (1998), Bazan et al. (2004), etc.\n\n==Incomplete data==\n\nRough set theory is useful for rule induction from incomplete data sets. Using this approach we can distinguish between three types of missing attribute values: ''lost values'' (the values that were recorded but currently are unavailable), ''attribute-concept values'' (these missing attribute values may be replaced by any attribute value limited to the same concept), and ''\"do not care\" conditions''  (the original values were irrelevant).  A  ''concept'' (''class'') is a set of all objects classified (or diagnosed) the same way.\n\nTwo special data sets with missing attribute values were extensively studied: in the first case, all missing attribute values were lost (Stefanowski and Tsoukias, 2001), in the second case, all missing attribute values were \"do not care\" conditions (Kryszkiewicz, 1999).\n\nIn attribute-concept values interpretation of a missing attribute value, the missing attribute value may be replaced by any value of the attribute domain restricted to the concept to which the object with a missing attribute value belongs (Grzymala-Busse and Grzymala-Busse, 2007).  For example, if for a patient the value of an attribute Temperature is missing, this patient is sick with flu, and all remaining patients sick with flu have values high or very-high for  Temperature when using the interpretation of the missing attribute value as the  attribute-concept value, we will replace the missing attribute value with  high and very-high.  Additionally, the ''characteristic relation'', (see, e.g., Grzymala-Busse and Grzymala-Busse, 2007) enables to process data sets with all three kind of missing attribute values at the same time: lost, \"do not care\" conditions, and attribute-concept values.\n\n==Applications==\n{{Unreferenced section|date=July 2017}}\n\nRough set methods can be applied as a component of hybrid solutions in [[machine learning]] and [[data mining]]. They have been found to be particularly useful for [[rule induction]] and [[feature selection]] (semantics-preserving dimensionality reduction). Rough set-based data analysis methods have been successfully applied in bioinformatics, economics and finance, medicine, multimedia, web and text mining, signal and image processing, software engineering, robotics, and engineering (e.g. power systems and control engineering). Recently the three regions of rough sets are interpreted as regions of acceptance, rejection and deferment. This leads to three-way decision making approach with the model which can potentially lead to interesting future applications.\n\n==History==\n\nThe idea of rough set was proposed by [[Zdzisław Pawlak|Pawlak]] (1981) as a new mathematical tool to deal with vague concepts. Comer, Grzymala-Busse, Iwinski, Nieminen, Novotny, Pawlak, Obtulowicz, and Pomykala have studied algebraic properties of rough sets. Different algebraic semantics have been developed by P. Pagliani, I. Duntsch, M. K. Chakraborty, M. Banerjee and A. Mani; these have been extended to more generalized rough sets by D. Cattaneo and A. Mani, in particular. Rough sets can be used to represent [[ambiguity]], [[vagueness]] and general [[uncertainty]].\n\n==Extensions and generalizations==\n\nSince the development of rough sets, extensions and generalizations have continued to evolve. Initial developments focused on the relationship - both similarities and difference - with [[fuzzy sets]]. While some literature contends these concepts are different, other literature considers that rough sets are a generalization of fuzzy sets - as represented through either fuzzy rough sets or rough fuzzy sets.  Pawlak (1995) considered that fuzzy and rough sets should be treated as being complementary to each other, addressing different aspects of uncertainty and vagueness.\n\nThree notable extensions of classical rough sets are:\n* [[Dominance-based rough set approach]] (DRSA) is an extension of rough set theory for [[multi-criteria decision analysis]] (MCDA), introduced by Greco, Matarazzo and Słowiński (2001). The main change in this extension of classical rough sets is the substitution of the indiscernibility relation by a ''dominance'' relation, which permits the formalism to deal with inconsistencies typical in consideration of criteria  and preference-ordered decision classes.\n* [[Decision-theoretic rough sets]] (DTRS) is a probabilistic extension of rough set theory introduced by Yao, Wong, and Lingras (1990).  It utilizes a Bayesian decision procedure for minimum risk decision making.  Elements are included into the lower and upper approximations based on whether their conditional probability is above thresholds <math>\\textstyle \\alpha</math> and <math>\\textstyle \\beta</math>.  These upper and lower thresholds determine region inclusion for elements.  This model is unique and powerful since the thresholds themselves are calculated from a set of six loss functions representing classification risks.\n* [[Game-theoretic rough sets]] (GTRS) is a game theory-based extension of rough set that was introduced by Herbert and Yao (2011). It utilizes a game-theoretic environment to optimize certain criteria of rough sets based classification or decision making in order to obtain effective region sizes.\n\n===Rough membership===\n\nRough sets can be also defined, as a generalisation, by employing a rough membership function instead of objective approximation. The rough membership function expresses a conditional probability that <math>x</math> belongs to <math>X</math> given <math>\\textstyle \\R</math>. This can be interpreted as a degree that <math>x</math> belongs to <math>X</math> in terms of information about <math>x</math> expressed by <math>\\textstyle \\R</math>.\n\nRough membership primarily differs from the fuzzy membership in that the membership of union and intersection of sets cannot, in general, be computed from their constituent membership as is the case of fuzzy sets. In this, rough membership is a generalization of fuzzy membership. Furthermore, the rough membership function is grounded more in probability than the conventionally held concepts of the fuzzy membership function.\n\n===Other generalizations===\nSeveral generalizations of rough sets have been introduced, studied and applied to solving problems. Here are some of these generalizations:\n\n*rough multisets (Grzymala-Busse, 1987)\n*fuzzy rough sets extend the rough set concept through the use of fuzzy equivalence classes(Nakamura, 1988)\n*Alpha rough set theory (α-RST) - a generalization of rough set theory that allows approximation using of fuzzy concepts (Quafafou, 2000)\n*intuitionistic fuzzy rough sets (Cornelis, De Cock and Kerre, 2003)\n*generalized rough fuzzy sets (Feng, 2010)\n*rough intuitionistic fuzzy sets (Thomas and Nair, 2011)\n*soft rough fuzzy sets and soft fuzzy rough sets (Meng, Zhang and Qin, 2011)\n*composite rough sets (Zhang, Li and Chen, 2014)\n\n==See also==\n* [[Algebraic semantics (mathematical logic)|Algebraic semantics]]\n* [[Alternative set theory]]\n* [[Analog computer]]\n* [[Description logic]]\n* [[Fuzzy logic]]\n* [[Fuzzy set theory]]\n* [[Granular computing]]\n* [[Near sets]]\n* [[Rough fuzzy hybridization]]\n* [[Soft computing]]\n* [[Type-2 fuzzy sets and systems]]\n* [[Decision-theoretic rough sets]]* [[Version space]]\n* [[Dominance-based rough set approach]]\n\n==References==\n\n*{{cite journal\n  | last = Pawlak\n  | first = Zdzisław \n  | title = Rough sets\n  | journal = International Journal of Parallel Programming\n  | volume = 11\n  | issue = 5\n  | pages = 341–356\n  | year = 1982\n  | doi = 10.1007/BF01001956}}\n*{{cite book\n  | last1 = Bazan  | first1 = Jan \n  | last2 = Szczuka |first2= Marcin\n  | last3 = Wojna |first3= Arkadiusz\n  | last4 = Wojnarski |first4= Marcin\n  | title = On the evolution of rough set exploration system\n  | journal = Proceedings of the RSCTC 2004\n  | volume = 3066 \n  | pages = 592–601\n  | year = 2004\n  | doi = 10.1007/978-3-540-25929-9_73| series = Lecture Notes in Computer Science \n  | isbn = 978-3-540-22117-3 \n  | citeseerx = 10.1.1.60.3957 \n  }}\n*{{cite journal\n  | last1 = Dubois  | first1 = D.\n  | last2=Prade |first2= H.\n  | title = Rough fuzzy sets and fuzzy rough sets\n  | journal = International Journal of General Systems\n  | volume = 17\n  | pages = 191–209\n  | year = 1990\n  | doi = 10.1080/03081079008935107\n  | issue = 2–3}}\n* {{cite journal\n  | last1 = Herbert | first1 = J. P.\n  | last2=Yao |first2= J. T.\n  | title = Game-theoretic Rough Sets\n  | journal = Fundamenta Informaticae\n  | volume = 108\n  | pages = 267–286\n  | year = 2011\n  | doi = 10.3233/FI-2011-423\n  | issue = 3–4}}\n*{{cite journal\n  | last1 = Greco  | first1 = Salvatore\n  | last2 = Matarazzo |first2= Benedetto\n  | last3 = Słowiński |first3= Roman\n  | title = Rough sets theory for multicriteria decision analysis\n  | journal = European Journal of Operational Research\n  | volume = 129\n  | issue = 1\n  | year = 2001\n  | pages = 1–47\n  | doi = 10.1016/S0377-2217(00)00167-3}}\n*{{cite journal\n  | last = Grzymala-Busse\n  | first = Jerzy\n  | title = A new version of the rule induction system LERS\n  | journal = Fundamenta Informaticae\n  | volume = 31\n  | year = 1997\n  | pages = 27–39}}\n*{{cite book\n  | last1 = Grzymala-Busse |first1= Jerzy\n  | last2 = Grzymala-Busse |first2= Witold\n  | title = An experimental comparison of three rough set approaches to missing attribute values\n  | journal = Transactions on Rough Sets\n  | volume = 6\n  | year = 2007\n  | pages = 31–50\n  | doi=10.1007/978-3-540-71200-8_3|series= Lecture Notes in Computer Science\n  |isbn= 978-3-540-71198-8\n  }}\n*{{cite journal\n  | last = Kryszkiewicz\n  | first = Marzena\n  | title = Rules in incomplete systems\n  | journal = Information Sciences\n  | volume = 113\n  | year = 1999\n  | pages = 271–292\n  | doi = 10.1016/S0020-0255(98)10065-8\n  | issue = 3–4}}\n* Pawlak, Zdzisław ''Rough Sets'' Research Report PAS 431, Institute of Computer Science, Polish Academy of Sciences (1981)\n*{{cite journal\n  | last1 = Pawlak | first1 = Zdzisław \n  | last2 = Wong |first2= S. K. M.\n  | last3 = Ziarko |first3= Wojciech\n  | title = Rough sets: Probabilistic versus deterministic approach\n  | journal = International Journal of Man-Machine Studies\n  | volume = 29\n  | pages = 81–95\n  | year = 1988\n  | doi = 10.1016/S0020-7373(88)80032-4}}\n*{{cite book\n  | last = Pawlak\n  | first = Zdzisław\n  | title = Rough Sets: Theoretical Aspects of Reasoning About Data\n  | publisher = Kluwer Academic Publishing\n  | year = 1991\n  | location = Dordrecht\n  | isbn =  978-0-7923-1472-1}}\n*{{cite journal\n  | last1 = Slezak  | first1 = Dominik\n  | last2 = Wroblewski |first2= Jakub\n  | last3 = Eastwood |first3= Victoria\n  | last4 = Synak |first4= Piotr\n  | title = Brighthouse: an analytic data warehouse for ad-hoc queries\n  | journal = Proceedings of the VLDB Endowment |volume=1 |issue=2 | pages = 1337–1345\n  | year = 2008\n  | url=http://www.vldb.org/pvldb/1/1454174.pdf | doi=10.14778/1454159.1454174}}\n*{{cite conference\n  | first = Jerzy\n  | last = Stefanowski\n  | title = On rough set based approaches to induction of decision rules\n  | booktitle = Rough Sets in Knowledge Discovery 1: Methodology and Applications\n  | pages = 500–529\n  | publisher = Physica-Verlag\n  | editor    = Polkowski, Lech |editor2=Skowron, Andrzej\n  | year = 1998\n  | location = Heidelberg}}\n*{{cite conference\n  | last1 = Stefanowski  | first1 = Jerzy\n  | last2=Tsoukias |first2=Alexis\n  | title = Incomplete information tables and rough classification\n  | journal = Computational Intelligence\n  | volume = 17\n  | pages = 545–566\n  | year = 2001\n  | doi=10.1111/0824-7935.00162}}\n*{{cite journal\n  | last1 = Wong  | first1 = S. K. M.\n  | last2 = Ziarko |first2= Wojciech\n  | last3 = Ye |first3= R. Li\n  | title = Comparison of rough-set and statistical methods in inductive learning\n  | journal = International Journal of Man-Machine Studies\n  | volume = 24\n  | pages = 53–72\n  | year = 1986| doi = 10.1016/S0020-7373(86)80033-5\n  }}\n*{{cite conference\n  | last1 = Yao  | first1 = J. T.\n  | last2 = Yao |first2= Y. Y.\n  | title = Induction of classification rules by granular computing\n  | booktitle = Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing (TSCTC'02)\n  | pages = 331–338\n  | publisher = Springer-Verlag\n  | year = 2002\n  | location = London, UK}}\n*{{cite conference\n  | first = Wojciech\n  | last = Ziarko\n  | title = Rough sets as a methodology for data mining\n  | booktitle = Rough Sets in Knowledge Discovery 1: Methodology and Applications\n  | pages = 554–576\n  | publisher = Physica-Verlag\n  | year = 1998\n  | location = Heidelberg}}\n*{{cite conference\n  | last1 = Ziarko  | first1 = Wojciech \n  | last2=Shan |first2= Ning\n  | title = Discovering attribute relationships, dependencies and rules by using rough sets\n  | booktitle = Proceedings of the 28th Annual Hawaii International Conference on System Sciences (HICSS'95)\n  | pages = 293–299\n  | year = 1995\n  | location = Hawaii}}\n*{{cite journal\n  | last = Pawlak\n  | first = Zdzisław \n  | title = Decision rules, Bayes' rule and rough sets\n  | journal = New Direction in Rough Sets, Data Mining, and Granular-soft Computing\n  | pages = 1–9\n  | year = 1999}}\n*{{cite journal\n  | last = Pawlak\n  | first = Zdzisław \n  | title = Rough relations, reports\n  | journal =Institute of Computer Science\n  | volume = 435}}\n*{{cite journal\n  | last = Orlowska\n  | first = E.\n  | title = Reasoning about vague concepts\n  | journal = Bulletin of the Polish Academy of Sciences\n  | volume = 35\n  | pages = 643–652\n  | year = 1987}}\n*{{cite journal\n  | last = Polkowski\n  | first = L.\n  | title = Rough sets: Mathematical foundations\n  | journal = Advances in Soft Computing\n   | year = 2002}}\n*{{cite journal\n  | last = Skowron\n  | first = A.\n  | title = Rough sets and vague concepts\n  | journal = Fundamenta Informaticae\n  | pages = 417–431\n  | year = 1996}}\n*Burgin M. (1990). Theory of Named Sets as a Foundational Basis for Mathematics, In Structures in mathematical theories: Reports of the San Sebastian international symposium, September 25–29, 1990 (http://www.blogg.org/blog-30140-date-2005-10-26.html)\n*Burgin, M. (2004). Unified Foundations of Mathematics, Preprint Mathematics LO/0403186, p39.  (electronic edition: https://arxiv.org/ftp/math/papers/0403/0403186.pdf) \n*Burgin, M. (2011), Theory of Named Sets, Mathematics Research Developments, Nova Science Pub Inc, {{isbn|978-1-61122-788-8}}\n*Cornelis, C., De Cock, M. and Kerre, E. (2003) Intuitionistic fuzzy rough sets: at the crossroads of imperfect knowledge, Expert Systems, 20:5, pp260–270\n*Düntsch, I. and Gediga, G. (1995) Rough Set Dependency Analysis in Evaluation Studies – An Application in the Study of Repeated Heart Attacks. University of Ulster, Informatics Research Reports No. 10\n*Feng F. (2010). Generalized Rough Fuzzy Sets Based on Soft Sets, Soft Computing, 14:9, pp 899–911\n*Grzymala-Busse, J. (1987). Learning from examples based on rough multisets, in Proceedings of the 2nd International Symposium on Methodologies for Intelligent Systems, pp.&nbsp;325–332. Charlotte, NC, USA, \n*Meng, D., Zhang, X. and Qin, K. (2011). Soft rough fuzzy sets and soft fuzzy rough sets, Computers & Mathematics with Applications, 62:12, pp4635–4645\n*Quafafou M. (2000). α-RST: a generalization of rough set theory, Information Sciences, 124:1–4, pp301–316.\n*Quafafou M. and Boussouf M. (2000). Generalized rough sets based feature selection. Journal Intelligent Data Analysis, 4:1 pp3 – 17\n*Nakamura, A. (1988) Fuzzy rough sets, ‘Notes on Multiple-valued Logic in Japan’, 9:1, pp1–8\n*Pawlak, Z., Grzymala-Busse, J., Slowinski, R. Ziarko, W. (1995). Rough Sets. Communications of the ACM, 38:11, pp88–95\n*Thomas, K. and Nair, L. (2011). Rough intuitionistic fuzzy sets in a lattice, International Mathematical Forum, 6:27, pp1327–1335\n*Zhang J., Li T., Chen H. (2014). Composite rough sets for dynamic data mining, Information Sciences, 257, pp81–100\n*Zhang J., Wong J-S, Pan Y, Li T. (2015). A parallel matrix-based method for computing approximations in incomplete information systems, IEEE Transactions on Knowledge and Data Engineering, 27(2): 326-339\n*Chen H., Li T., Luo C., Horng S-J., Wang G. (2015). A decision-theoretic rough set approach for dynamic data mining. IEEE Transactions on Fuzzy Systems, 23(6): 1958-1970\n*Chen H., Li T., Luo C., Horng S-J., Wang G. (2014). A rough set-based method for updating decision rules on attribute values' coarsening and refining, IEEE Transactions on Knowledge and Data Engineering, 26(12): 2886-2899\n*Chen H., Li T.,  Ruan D., Lin J., Hu C, (2013) A rough-set based incremental approach for updating approximations under dynamic maintenance environments. IEEE Transactions on Knowledge and Data Engineering, 25(2): 274-284\n\n== Further reading ==\n* Gianpiero Cattaneo and Davide Ciucci, \"Heyting Wajsberg Algebras as an Abstract Environment Linking Fuzzy and Rough Sets\" in J.J. Alpigini et al. (Eds.): RSCTC 2002, LNAI 2475, pp.&nbsp;77–84, 2002. {{doi|10.1007/3-540-45813-1_10}}\n\n==External links==\n* [http://www.roughsets.org The International Rough Set Society]\n* [http://eecs.ceas.uc.edu/~mazlack/dbm.w2011/Komorowski.RoughSets.tutor.pdf Rough set tutorial]\n* [https://web.archive.org/web/20100328002310/http://www.ghastlyfop.com/blog/2006/01/rough-sets-quick-tutorial.html Rough Sets: A Quick Tutorial]\n* [http://logic.mimuw.edu.pl/~rses/ Rough Set Exploration System]\n* [http://rsctc2008.cs.uakron.edu/Invited%20Speakers/Presentations/Slezak%20Revised%20RSCTC%20Presentation.ppt Rough Sets in Data Warehousing]\n\n[[Category:Systems of set theory]]\n[[Category:Theoretical computer science]]\n[[Category:Approximations]]"
    },
    {
      "title": "Successive approximation ADC",
      "url": "https://en.wikipedia.org/wiki/Successive_approximation_ADC",
      "text": "{{dablink|For behaviorist psychologist B. F. Skinner's method of guiding learned behavior, see [[Shaping (psychology)]].  For successive approximation in general, see [[Successive approximation (disambiguation)|Successive approximation]].}}\n\nA '''successive approximation ADC''' is a type of [[analog-to-digital converter]] that converts a continuous [[Analog signal|analog]] waveform into a discrete [[Digital data|digital]] representation via a [[binary search]] through all possible [[Quantization (signal processing)|quantization]] levels before finally converging upon a digital output for each conversion.\n\n==Block diagram==\n[[File:SA ADC block diagram.png|right|thumb|300px|Successive approximation ADC block diagram]]\n'''Key'''\n* DAC = digital-to-analog converter\n* EOC = end of conversion\n* SAR = successive approximation register\n* S/H = sample and hold circuit\n* ''V''<sub>in</sub> = input voltage\n* ''V''<sub>ref</sub> = reference voltage\n\n== Algorithm ==\nThe successive approximation [[analog-to-digital converter]] circuit typically consists of four chief subcircuits:\n\n:# A [[sample and hold]] circuit to acquire the input [[voltage]] (''V''<sub>in</sub>).\n:# An analog voltage comparator that compares ''V''<sub>in</sub> to the output of the internal [[Digital-to-analog converter|DAC]] and outputs the result of the comparison to the successive approximation [[Processor register|register]] (SAR).\n:# A successive approximation register subcircuit designed to supply an approximate digital code of ''V''<sub>in</sub> to the internal DAC.\n:# An internal reference DAC that, for comparison with ''V''<sub>REF</sub>, supplies the [[comparator]] with an analog voltage equal to the digital code output of the SAR<sub>in</sub>.\n\nThe successive approximation register is initialized so that the [[most significant bit]] (MSB) is equal to a [[Digital data|digital]] 1. This code is fed into the DAC, which then supplies the analog equivalent of this digital code (''V''<sub>ref</sub>/2) into the comparator circuit for comparison with the sampled input voltage.  If this analog voltage exceeds ''V''<sub>in</sub> the comparator causes the SAR to reset this bit; otherwise, the bit is left as 1.  Then the next bit is set to 1 and the same test is done, continuing this [[Binary search algorithm|binary search]] until every bit in the SAR has been tested.  The resulting code is the digital approximation of the sampled input voltage and is finally output by the SAR at the end of the conversion (EOC).\n\nMathematically, let ''V''<sub>in</sub> = ''xV''<sub>ref</sub>, so ''x'' in [−1,&nbsp;1] is the normalized input voltage. The objective is to approximately digitize ''x'' to an accuracy of 1/2<sup>''n''</sup>. The algorithm proceeds as follows:\n:# Initial approximation ''x''<sub>0</sub> = 0.\n:# ''i''th approximation ''x''<sub>''i''</sub> = ''x''<sub>''i''−1</sub> − ''s''(''x''<sub>''i''−1</sub> − ''x'')/2<sup>''i''</sup>.\nwhere, ''s''(''x'') is the signum-function (sgn(''x'')) (+1 for ''x'' ≥ 0, −1 for ''x'' < 0). It follows using mathematical induction that |''x''<sub>''n''</sub> − ''x''| ≤ 1/2<sup>''n''</sup>.\n\nAs shown in the above algorithm, a SAR ADC requires:\n:# An input voltage source ''V''<sub>in</sub>.\n:# A reference voltage source ''V''<sub>ref</sub> to normalize the input.\n:# A DAC to convert the ''i''th approximation ''x''<sub>''i''</sub> to a voltage.\n:# A comparator to perform the function ''s''(''x''<sub>''i''</sub> − ''x'') by comparing the DAC's voltage with the input voltage.\n:# A register to store the output of the comparator and apply ''x''<sub>''i''−1</sub> − ''s''(''x''<sub>''i''−1</sub> − ''x'')/2<sup>''i''</sup>.\n\n[[File:ADC animation 20.gif|thumb|alt=successive approximation|ADC using successive approximation|right]]\n'''Example:''' The ten steps to converting an analog input to 10 bit digital, using successive approximation, are shown here for all voltages from 5 V to 0 V in 0.1 V iterations. Since the reference voltage is 5 V, when the input voltage is also 5 V all bits are set. As the voltage is decreased to 4.9 V, only some of the least significant bits are cleared. The MSB will remain set until the input is one half the reference voltage, 2.5 V.\n\nThe binary weights assigned to each bit, starting with the MSB, are 2.5, 1.25, 0.625, 0.3125, 0.15625, 0.078125, 0.0390625, 0.01953125, 0.009765625, 0.0048828125. All of these add up to 4.9951171875, meaning binary 1111111111, or one LSB less than 5. \n\nWhen the analog input is being compared to the internal DAC output, it effectively is being compared to each of these binary weights, starting with the 2.5 V and either keeping it or clearing it as a result. Then by adding the next weight to the previous result, comparing again, and repeating until all the bits and their weights have been compared to the input, the end result, a binary number representing the analog input, is found.\n\n==Charge-redistribution successive approximation ADC==\n[[File:ChargeScalingDAC.png|right|thumb|300px|Charge-scaling DAC]]\nOne of the most common implementations of the successive approximation ADC, the ''charge-redistribution'' successive approximation ADC, uses a charge scaling [[Digital-to-analog converter|DAC]].  The charge scaling DAC simply consists of an array of individually switched binary-weighted capacitors.  The amount of charge upon each capacitor in the array is used to perform the aforementioned binary search in conjunction with a comparator internal to the DAC and the successive approximation register.\n\n:# First,  the capacitor array is completely discharged to the offset voltage of the comparator, ''V''<sub>OS</sub>.  This step provides automatic offset cancellation(i.e. The offset voltage represents nothing but dead charge which can't be juggled by the capacitors).\n:# Next, all of the capacitors within the array are switched to the input signal, ''v''<sub>IN</sub>.  The capacitors now have a charge equal to their respective capacitance times the input voltage minus the offset voltage upon each of them.\n:# In the third step, the capacitors are then switched so that this charge is applied across the comparator's input, creating a comparator input voltage equal to −''v''<sub>IN</sub>.\n:# Finally, the actual conversion process proceeds.  First, the MSB capacitor is switched to ''V''<sub>REF</sub>, which corresponds to the full-scale range of the ADC.  Due to the binary-weighting of the array the MSB capacitor forms a 1:1 charge divider with the rest of the array.  Thus, the input voltage to the comparator is now −''v''<sub>IN</sub> plus ''V''<sub>REF</sub>/2.  Subsequently, if ''v''<sub>IN</sub> is greater than ''V''<sub>REF</sub>/2 then the comparator outputs a digital 1 as the MSB, otherwise it outputs a digital 0 as the MSB.  Each capacitor is tested in the same manner until the comparator input voltage converges to the offset voltage, or at least as close as possible given the resolution of the DAC.\n[[File:CAPadc.png|thumb|3 bits simulation of a capacitive ADC]]\n\n=== Use with non-ideal analog circuits ===\nWhen implemented as an analog circuit – where the value of each successive bit is not perfectly 2<sup>''N''</sup> (e.g. 1.1, 2.12, 4.05, 8.01, etc.) – a successive approximation approach might not output the ideal value because the binary search algorithm incorrectly removes what it believes to be half of the values the unknown input cannot be.  Depending on the difference between actual and ideal performance, the maximum error can easily exceed several LSBs, especially as the error between the actual and ideal 2<sup>''N'' </sup>becomes large for one or more bits.  Since we don't know the actual unknown input, it is therefore very important that accuracy of the analog circuit used to implement a SAR ADC be very close to the ideal 2<sup>''N''</sup> values; otherwise, we cannot guarantee a best match search.\n\n== See also ==\n* [[Quantization noise]]\n* [[Digital-to-analog converter]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* ''CMOS Circuit Design, Layout, and Simulation, 3rd Edition''; R. J. Baker; Wiley-IEEE; 1208 pages; 2010; {{ISBN|978-0-470-88132-3}}\n* ''Data Conversion Handbook''; Analog Devices; Newnes; 976 pages; 2004; {{ISBN|978-0750678414}}\n\n== External links ==\n* [https://pdfserv.maximintegrated.com/en/an/AN1080.pdf Understanding SAR ADCs: Their Architecture and Comparison with Other ADCs] - Maxim\n* [https://www.ti.com/europe/downloads/Choose%20the%20right%20data%20converter%20for%20your%20application.pdf Choose the right A/D converter for your application] - TI\n\n{{DEFAULTSORT:Successive Approximation Adc}}\n[[Category:Electronic circuits]]\n[[Category:Digital signal processing]]\n[[Category:Analog circuits]]\n[[Category:Approximations]]"
    },
    {
      "title": "Taylor's theorem",
      "url": "https://en.wikipedia.org/wiki/Taylor%27s_theorem",
      "text": "[[File:Taylorspolynomialexbig.svg|thumb|right|300px|The exponential function ''y''&nbsp;=&nbsp;''e''<sup>''x''</sup> (red) and the corresponding Taylor polynomial of degree four (dashed green) around the origin.]]\n{{Calculus |Differential}}\n\nIn [[calculus]], '''Taylor's theorem''' gives an approximation of a ''k''-times [[Differentiable function|differentiable]] [[function (mathematics)|function]] around a given point by a ''k''-th order '''Taylor [[polynomial]]'''. For [[analytic functions]] the Taylor polynomials at a given point are finite-order truncations of its [[Taylor series]], which completely determines the function in some [[Neighbourhood (mathematics)|neighborhood]] of the point. It can be thought of as the extension of [[linear approximation]] to higher order polynomials, and in the case of ''k'' equals 2 is often referred to as a ''quadratic approximation''.<ref> (2013). [http://www.math.ubc.ca/~sujatha/2013/103/week10-12/Linearapp.pdf\"Linear and quadratic approximation\"] Retrieved December 6, 2018</ref> The exact content of \"Taylor's theorem\" is not universally agreed upon. Indeed, there are several versions of it applicable in different situations, and some of them contain explicit estimates on the approximation error of the function by its Taylor polynomial.\n\nTaylor's theorem is named after the mathematician [[Brook Taylor]], who stated a version of it in [[1712 in science|1712]]. Yet an explicit expression of the error was not provided until much later on by [[Joseph-Louis Lagrange]]. An earlier version of the result was already mentioned in [[1671 in science|1671]] by [[James Gregory (astronomer and mathematician)|James Gregory]].<ref>{{harvnb|Kline|1972|p=442, 464}}.</ref>\n\nTaylor's theorem is taught in introductory-level calculus courses and is one of the central elementary tools in [[mathematical analysis]]. Within [[pure mathematics]] it is the starting point of more advanced [[asymptotic analysis]] and is commonly used in more applied fields of numerics, as well as in [[mathematical physics]]. Taylor's theorem also generalizes to [[multivariate function|multivariate]] and [[Euclidean vector|vector valued]] functions <math>f \\colon \\mathbb R^n \\to \\mathbb R^m</math> on any [[dimension]]s ''n'' and ''m''. This generalization of Taylor's theorem is the basis for the definition of so-called [[Jet (mathematics)|jets]], which appear in [[differential geometry]] and [[partial differential equations]].\n\n== Motivation ==\n[[File:E^x with linear approximation.png|thumb|right|Graph of {{nowrap|''f''(''x'') {{=}} ''e<sup>x</sup>''}} (blue) with its [[linear approximation]] {{nowrap|''P''<sub>1</sub>(''x'') {{=}} 1 + ''x''}} (red) at ''a''&nbsp;=&nbsp;0.]]\nIf a real-valued [[function (mathematics)|function]] ''f'' is [[derivative|differentiable]] at the point ''a'' then it has a [[linear approximation]] at the point ''a''.  This means that there exists a function ''h''<sub>1</sub> such that\n\n:<math> f(x) = f(a) + f'(a)(x - a) + h_1(x)(x - a), \\quad \\lim_{x \\to a} h_1(x) = 0.</math>\n\nHere\n\n:<math>P_1(x) = f(a) + f'(a)(x - a)</math>\n\nis the linear approximation of ''f'' at the point ''a''.  The graph of {{nowrap|''y'' {{=}} ''P''<sub>1</sub>(''x'')}} is the [[tangent line]] to the graph of ''f'' at {{nowrap|''x'' {{=}} ''a''}}.  The error in the approximation is\n:<math>R_1(x) = f(x) - P_1(x) = h_1(x)(x - a).</math>\nNote that this goes to zero a little bit faster than {{nowrap|''x'' &minus; ''a''}} as ''x'' tends to&nbsp;''a'', given the limiting behavior of ''h''<sub>1</sub>.\n\n[[File:E^x with quadratic approximation corrected.png|thumb|right|Graph of {{nowrap|''f''(''x'') {{=}} ''e<sup>x</sup>''}} (blue) with its quadratic approximation {{nowrap|''P''<sub>2</sub>(''x'') {{=}} 1 + ''x'' + ''x''<sup>2</sup>/2}} (red) at ''a''&nbsp;=&nbsp;0. Note the improvement in the approximation.]]\nIf we wanted a better approximation to ''f'', we might instead try a [[quadratic polynomial]] instead of a linear function.  Instead of just matching one derivative of ''f'' at ''a'', we can match two derivatives, thus producing a polynomial that has the same slope and concavity as ''f'' at ''a''.  The quadratic polynomial in question is\n\n:<math>P_2(x) = f(a) + f'(a)(x - a) + \\frac{f''(a)}{2}(x - a)^2.</math>\n\nTaylor's theorem ensures that the ''quadratic approximation'' is, in a sufficiently small neighborhood of the point ''a'', a better approximation than the linear approximation.  Specifically,\n\n:<math>f(x) = P_2(x) + h_2(x)(x - a)^2, \\quad \\lim_{x \\to a} h_2(x) = 0.</math>\n\nHere the error in the approximation is\n\n:<math>R_2(x) = f(x) - P_2(x) = h_2(x)(x - a)^2,</math>\n\nwhich, given the limiting behavior of <math>h_2</math>, goes to zero faster than <math>(x - a)^2</math> as ''x'' tends to&nbsp;''a''.\n\n[[File:Tayloranimation.gif|thumb|360px|right|Approximation of ''f''(''x'')&nbsp;=&nbsp;1/(1&nbsp;+&nbsp;''x''<sup>2</sup>) (blue) by its Taylor polynomials ''P<sub>k</sub>'' of order ''k''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;16 centered at ''x''&nbsp;=&nbsp;0 (red) and ''x''&nbsp;=&nbsp;1 (green). The approximations do not improve at all outside (−1,&nbsp;1) and (1&nbsp;−&nbsp;√2,&nbsp;1&nbsp;+&nbsp;√2) respectively.]] Similarly, we might get still better approximations to ''f'' if we use [[polynomial]]s of higher degree, since then we can match even more derivatives with ''f'' at the selected base point.\n\nIn general, the error in approximating a function by a polynomial of degree ''k'' will go to zero a little bit faster than {{nowrap|(''x'' &minus; ''a'')<sup>''k''</sup>}} as ''x'' tends to&nbsp;''a''. But this might not always be the case: it is also possible that increasing the degree of the approximating [[polynomial]] does not increase the quality of approximation at all even if the function ''f'' to be approximated is infinitely many times differentiable. An example of this behavior is given [[Taylor's theorem#Taylor's theorem and convergence of Taylor series|below]], and it is related to the fact that unlike [[analytic functions]], more general functions are not (locally) determined by the values of their derivatives at a single point.\n\nTaylor's theorem is of asymptotic nature: it only tells us that the error ''R<sub>k</sub>'' in an [[approximation]] by a ''k''-th order Taylor polynomial ''P<sub>k</sub>'' tends to zero faster than any nonzero ''k''-th degree [[polynomial]] as ''x''&nbsp;→&nbsp;''a''. It does not tell us how large the error is in any concrete [[neighborhood (mathematics)|neighborhood]] of the center of expansion, but for this purpose there are explicit formulae for the remainder term (given below) which are valid under some additional regularity assumptions on ''f''. These enhanced versions of Taylor's theorem typically lead to [[uniform convergence|uniform estimates]] for the approximation error in a small neighborhood of the center of expansion, but the estimates do not necessarily hold for neighborhoods which are too large, even if the function ''f'' is [[analytic function|analytic]]. In that situation one may have to select several Taylor polynomials with different centers of expansion to have reliable Taylor-approximations of the original function (see animation on the right.)\n\nThere are several things we might do with the remainder term:\n# Estimate the error in using a polynomial ''P''<sub>''k''</sub>(''x'') of degree ''k'' to estimate ''f''(''x'') on a given interval (''a'' – ''r'', ''a'' + ''r''). (The interval and the degree ''k'' are fixed; we want to find the error.)\n# Find the smallest degree ''k'' for which the polynomial ''P''<sub>''k''</sub>(''x'') approximates ''f''(''x'') to within a given error (or tolerance) on a given interval (''a'' − ''r'', ''a'' + ''r'') . (The interval and the error are fixed; we want to find the degree.)\n# Find the largest interval (''a'' − ''r'', ''a'' + ''r'') on which ''P''<sub>''k''</sub>(''x'') approximates ''f''(''x'') to within a given error (\"tolerance\"). (The degree and the error are fixed; we want to find the interval.)\n\n== Taylor's theorem in one real variable ==\n\n=== Statement of the theorem ===\n\nThe precise statement of the most basic version of Taylor's theorem is as follows:\n\n{{quotation|'''Taylor's theorem.'''<ref>{{ citation|first1=Angelo|last1=Genocchi|first2= Giuseppe|last2=Peano|title=Calcolo differenziale e principii di calcolo integrale|location=(N. 67, pp.&nbsp;XVII–XIX)|publisher=Fratelli Bocca ed.|year=1884}}</ref><ref>{{Citation | last1=Spivak | first1=Michael | author1-link=Michael Spivak | title=Calculus  | publisher=Publish or Perish | location=Houston, TX | edition=3rd | isbn=978-0-914098-89-8 | year=1994| page=383}}</ref><ref>{{springer|title=Taylor formula|id=p/t092300}}</ref> Let ''k''&nbsp;&ge;&nbsp;1 be an [[integer]] and let the [[Function (mathematics)|function]] {{nowrap|''f'' : '''R''' &rarr; '''R'''}} be ''k'' times [[Differentiable function|differentiable]] at the point {{nowrap|''a'' &isin; '''R'''}}. Then there exists a function  {{nowrap|''h<sub>k</sub>'' : '''R''' &rarr; '''R'''}} such that\n\n:<math> f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(k)}(a)}{k!}(x-a)^k + h_k(x)(x-a)^k,</math>\n<math>\\mbox{and}\\quad\\lim_{x\\to a}h_k(x)=0</math>. This is called the '''[[Peano]] form of the remainder'''.}}\n\nThe polynomial appearing in Taylor's theorem is the '''''k''-th order Taylor polynomial'''\n\n:<math>P_k(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(k)}(a)}{k!}(x-a)^k </math>\n\nof the function ''f'' at the point ''a''. The Taylor polynomial is the unique \"asymptotic best fit\" polynomial in the sense that if there exists a function  {{nowrap|''h<sub>k</sub>'' : '''R''' &rarr; '''R'''}} and a ''k''-th order polynomial ''p'' such that\n\n:<math> f(x) = p(x) + h_k(x)(x-a)^k, \\quad \\lim_{x\\to a}h_k(x)=0,</math>\n\nthen ''p''&nbsp;=&nbsp;''P<sub>k</sub>''. Taylor's theorem describes the asymptotic behavior of the '''remainder term'''\n\n:<math> \\ R_k(x) = f(x) - P_k(x),</math>\n\nwhich is the [[approximation error]] when approximating ''f'' with its Taylor polynomial. Using the [[little-o notation]], the statement in Taylor's theorem reads as\n\n:<math>R_k(x) = o(|x-a|^k), \\quad x\\to a.</math>\n\n=== Explicit formulas for the remainder ===\n\nUnder stronger regularity assumptions on ''f'' there are several precise formulae for the remainder term ''R<sub>k</sub>'' of the Taylor polynomial, the most common ones being the following.\n\n{{quotation|'''Mean-value forms of the remainder.''' Let {{nowrap|''f'' : '''R''' &rarr; '''R'''}}  be ''k''&nbsp;+&nbsp;1 times [[Differentiable function|differentiable]] on the [[open interval]] with ''f''<sup>(''k'')</sup> [[continuous function|continuous]] on the [[closed interval]] between ''a'' and ''x''.<ref>The hypothesis of ''f''<sup>(''k'')</sup> being [[continuous function|continuous]] on the [[closed interval|''closed'' interval]] between ''a'' and ''x'' is ''not'' redundant. Although ''f'' being ''k''&nbsp;+&nbsp;1 times [[Differentiable function|differentiable]] on the [[open interval]] between ''a'' and ''x'' does imply that ''f''<sup>(''k'')</sup> is [[continuous function|continuous]] on the [[open interval|''open'' interval]] between ''a'' and ''x'', it does ''not'' imply that ''f''<sup>(''k'')</sup> is [[continuous function|continuous]] on the [[closed interval|''closed'' interval]] between ''a'' and ''x'', i.e. it does not imply that ''f''<sup>(''k'')</sup> is [[continuous function|continuous]] at the ''endpoints'' of that interval. Consider, for example, the [[Function (mathematics)|function]] {{nowrap|''f'' : ''[0,1]'' &rarr; '''R'''}} defined to equal <math> \\sin(1/x)</math> on <math>(0,1]</math> and with <math>f(0)=0</math>. This is not [[continuous function|continuous]] at ''0'', but is [[continuous function|continuous]] on <math>(0,1)</math>. Moreover, one can show that this [[Function (mathematics)|function]] has an [[Antiderivative|antiderivative]]. Therefore that [[Antiderivative|antiderivative]] is [[Differentiable function|differentiable]] on <math>(0,1)</math>, its [[Derivative|derivative]] (the function ''f'') is [[continuous function|continuous]] on the [[open interval|''open'' interval]] <math>(0,1)</math>, but its [[Derivative|derivative]] ''f'' is ''not'' [[continuous function|continuous]] on the [[closed interval|''closed'' interval]] <math>[0,1]</math>. So the theorem would not apply in this case.</ref> Then\n\n:<math> R_k(x) = \\frac{f^{(k+1)}(\\xi_L)}{(k+1)!} (x-a)^{k+1} </math>\n\nfor some real number ''&xi;<sub>L</sub>'' between ''a'' and ''x''. This is the '''[[Joseph Louis Lagrange|Lagrange]] form'''<ref>{{harvnb|Kline|1998|loc=§20.3}}; {{harvnb|Apostol|1967|loc=§7.7}}.</ref> of the remainder. Similarly,\n\n:<math> R_k(x) = \\frac{f^{(k+1)}(\\xi_C)}{k!}(x-\\xi_C)^k(x-a) </math>\n\nfor some real number ''&xi;<sub>C</sub>'' between ''a'' and ''x''. This is the '''[[Augustin Louis Cauchy|Cauchy]] form'''<ref>{{harvnb|Apostol|1967|loc=§7.7}}.</ref> of the remainder.\n}}\n\nThese refinements of Taylor's theorem are usually proved using the [[mean value theorem]], whence the name. Also other similar expressions can be found. For example, if ''G''(''t'') is continuous on the closed interval and differentiable with a non-vanishing derivative on the open interval between ''a'' and ''x'', then\n\n:<math> R_k(x) = \\frac{f^{(k+1)}(\\xi)}{k!}(x-\\xi)^k \\frac{G(x)-G(a)}{G'(\\xi)} </math>\n\nfor some number ''&xi;'' between ''a'' and ''x''. This version covers the Lagrange and Cauchy forms of the remainder as special cases, and is proved below using [[mean value theorem#Cauchy's mean value theorem|Cauchy's mean value theorem]].\n\nThe statement for the integral form of the remainder is more advanced than the previous ones, and requires understanding of [[Lebesgue integral|Lebesgue integration theory]] for the full generality. However, it holds also in the sense of [[Riemann integral]] provided the (''k''&nbsp;+&nbsp;1)th derivative of ''f'' is continuous on the closed interval [''a'',''x''].\n\n{{quotation|'''Integral form of the remainder.'''<ref>{{harvnb|Apostol|1967|loc=§7.5}}.</ref> Let ''f''<sup>(''k'')</sup> be [[absolutely continuous]] on the [[closed interval]] between ''a'' and ''x''. Then\n:<math> R_k(x) = \\int_a^x \\frac{f^{(k+1)} (t)}{k!} (x - t)^k \\, dt. </math>}}\n\nDue to [[absolutely continuous|absolute continuity]] of ''f''<sup>(''k'')</sup> on the [[closed interval]] between ''a'' and ''x'', its derivative ''f''<sup>(''k''+1)</sup> exists as an ''L''<sup>1</sup>-function, and the result can be proven by a formal calculation using [[fundamental theorem of calculus]] and [[integration by parts]].\n\n=== Estimates for the remainder ===\n\nIt is often useful in practice to be able to estimate the remainder term appearing in the Taylor approximation, rather than having an exact formula for it.  Suppose that ''f'' is {{nowrap|(''k'' + 1)}}-times continuously differentiable in an interval ''I'' containing ''a''. Suppose that there are real constants ''q'' and ''Q'' such that\n:<math>q\\le f^{(k+1)}(x)\\le Q</math>\nthroughout ''I''. Then the remainder term satisfies the inequality<ref>{{harvnb|Apostol|1967|loc=§7.6}}</ref>\n\n:<math>q\\frac{(x-a)^{k+1}}{(k+1)!}\\le R_k(x)\\le Q\\frac{(x-a)^{k+1}}{(k+1)!},</math>\n\nif {{nowrap|''x'' > ''a''}}, and a similar estimate if {{nowrap|''x'' < ''a''}}. This is a simple consequence of the Lagrange form of the remainder. In particular, if\n:<math>|f^{(k+1)}(x)|\\le M</math>\non an interval {{nowrap|''I'' {{=}} (''a'' &minus; ''r'',''a'' + ''r'')}} with some <math>r>0</math> , then\n\n:<math>|R_k(x)|\\le M\\frac{|x-a|^{k+1}}{(k+1)!}\\le M\\frac{r^{k+1}}{(k+1)!}</math>\n\nfor all {{nowrap|''x''&isin;(''a'' − ''r'',''a'' + ''r'').}} The second inequality is called a [[uniform convergence|uniform estimate]], because it holds uniformly for all ''x'' on the interval {{nowrap|(''a'' − ''r'',''a'' + ''r'').}}\n\n=== Example ===\n\n[[File:Expanimation.gif|thumb|400px|right|Approximation of ''e''<sup>''x''</sup> (blue) by its Taylor polynomials ''P<sub>k</sub>'' of order ''k''&nbsp;=&nbsp;1,...,7 centered at ''x''&nbsp;=&nbsp;0 (red).]] Suppose that we wish to [[approximation|approximate]] the function {{nowrap|''f''(''x'') {{=}} ''e''<sup>''x''</sup>}} on the interval {{nowrap|[&minus;1,1]}} while ensuring that the error in the approximation is no more than 10<sup>&minus;5</sup>. In this example we pretend that we only know the following properties of the exponential function:\n\n:<math>(*) \\qquad e^0=1, \\qquad \\frac{d}{dx} e^x = e^x, \\qquad e^x>0, \\qquad x\\in\\mathbb{R}.</math>\n\nFrom these properties it follows that {{nowrap|''f''<sup>(''k'')</sup>(''x'') {{=}} ''e''<sup>''x''</sup>}} for all ''k'', and in particular, {{nowrap|''f''<sup>(''k'')</sup>(0) {{=}} 1}}. Hence the ''k''-th order Taylor polynomial of ''f'' at 0 and its remainder term in the Lagrange form are given by\n\n:<math> P_k(x) = 1+x+\\frac{x^2}{2!}+\\cdots+\\frac{x^k}{k!}, \\qquad R_k(x)=\\frac{e^\\xi}{(k+1)!}x^{k+1},</math>\n\nwhere ''&xi;'' is some number between 0 and ''x''. Since ''e''<sup>''x''</sup> is increasing by (*), we can simply use ''e<sup>x</sup>''&nbsp;≤&nbsp;1 for ''x''&nbsp;∈&nbsp;[&minus;1,&nbsp;0] to estimate the remainder on the subinterval [&minus;1,&nbsp;0]. To obtain an upper bound for the remainder on [0,1], we use the property {{nowrap|''e<sup>&xi;</sup>''<''e<sup>x</sup>''}} for 0<''&xi;<x'' to estimate\n\n:<math> e^x = 1 + x + \\frac{e^\\xi}{2}x^2 < 1 + x + \\frac{e^x}{2}x^2, \\qquad 0 < x\\leq 1 </math>\n\nusing the second order Taylor expansion. Then we solve for ''e<sup>x</sup>'' to deduce that\n\n:<math> e^x \\leq \\frac{1+x}{1-\\frac{x^2}{2}} = 2\\frac{1+x}{2-x^2} \\leq 4, \\qquad 0 \\leq x\\leq 1 </math>\n\nsimply by maximizing the [[numerator]] and minimizing the [[denominator]]. Combining these estimates for ''e<sup>x</sup>'' we see that\n\n:<math> |R_k(x)| \\leq \\frac{4|x|^{k+1}}{(k+1)!} \\leq \\frac{4}{(k+1)!}, \\qquad -1\\leq x \\leq 1, </math>\n\nso the required precision is certainly reached, when\n\n:<math> \\frac{4}{(k+1)!} < 10^{-5} \\quad \\Longleftrightarrow \\quad 4\\cdot 10^5 < (k+1)!  \\quad \\Longleftrightarrow \\quad k \\geq 9. </math>\n\n(See [[factorial]] or compute by hand the values 9!=362 880 and 10!=3 628 800.) As a conclusion, Taylor's theorem leads to the approximation\n\n:<math> e^x = 1+x+\\frac{x^2}{2!} + \\cdots + \\frac{x^9}{9!} + R_9(x), \\qquad |R_9(x)| < 10^{-5}, \\qquad -1\\leq x \\leq 1. </math>\n\nFor instance, this approximation provides a [[decimal representation|decimal expression]] ''e''&nbsp;≈&nbsp;2.71828, correct up to five decimal places.\n\n== Relationship to analyticity ==\n\n=== Taylor expansions of real analytic functions ===\n\nLet ''I'' ⊂ '''R''' be an [[open interval]]. By definition, a function ''f'' : ''I'' → '''R''' is [[analytic function|real analytic]] if it is locally defined by a convergent [[power series]]. This means that for every ''a''&nbsp;∈&nbsp;''I'' there exists some ''r''&nbsp;>&nbsp;0 and a sequence of coefficients ''c<sub>k</sub>''&nbsp;∈&nbsp;'''R''' such that {{nowrap|(''a'' − ''r'', ''a'' + ''r'') ⊂ ''I''}} and\n\n:<math> f(x) = \\sum_{k=0}^\\infty c_k(x-a)^k = c_0 + c_1(x-a) + c_2(x-a)^2 + \\cdots, \\qquad |x-a|<r. </math>\n\nIn general, the [[power series#Radius of convergence|radius of convergence]] of a power series can be computed from the [[Cauchy–Hadamard theorem|Cauchy–Hadamard formula]]\n\n:<math> \\frac{1}{R} = \\limsup_{k\\to\\infty}|c_k|^\\frac{1}{k}. </math>\n\nThis result is based on comparison with a [[geometric series]], and the same method shows that if the power series based on ''a'' converges for some ''b'' ∈ '''R''', it must converge [[uniform convergence|uniformly]] on the [[closed interval]] {{nowrap|[''a'' − ''r<sub>b</sub>'', ''a'' + ''r''<sub>''b''</sub>]}}, where ''r<sub>b</sub>''&nbsp;=&nbsp;|''b''&nbsp;−&nbsp;''a''|. Here only the convergence of the power series is considered, and it might well be that {{nowrap|(''a'' − ''R'',''a'' + ''R'')}} extends beyond the domain ''I'' of ''f''.\n\nThe Taylor polynomials of the real analytic function ''f'' at ''a'' are simply the finite truncations\n\n:<math> P_k(x) = \\sum_{j=0}^k c_j(x-a)^j, \\qquad c_j = \\frac{f^{(j)}(a)}{j!}</math>\n\nof its locally defining power series, and the corresponding remainder terms are locally given by the analytic functions\n\n:<math> R_k(x) = \\sum_{j=k+1}^\\infty c_j(x-a)^j = (x-a)^k h_k(x), \\qquad |x-a|<r. </math>\n\nHere the functions\n\n:<math>\\begin{cases} h_k:(a-r,a+r)\\to \\mathbf R \\\\  h_k(x) = (x-a)\\sum_{j=0}^\\infty c_{k+1+j}(x-a)^j\\end{cases}</math>\n\nare also analytic, since their defining power series have the same radius of convergence as the original series. Assuming that {{nowrap|[''a'' − ''r'', ''a'' + ''r'']}} ⊂ ''I'' and ''r''&nbsp;<&nbsp;''R'', all these series converge uniformly on {{nowrap|(''a'' − ''r'', ''a'' + ''r'')}}. Naturally, in the case of analytic functions one can estimate the remainder term ''R<sub>k</sub>''(''x'') by the tail of the sequence of the derivatives ''f&prime;''(''a'') at the center of the expansion, but using [[complex analysis]] also another possibility arises, which is described [[Taylor's theorem#Relationship to analyticity##Taylor's theorem in complex analysis|below]].\n\n=== Taylor's theorem and convergence of Taylor series ===\n\nThe Taylor series of ''f'' will converge in some interval, given that all its derivatives are bounded over it and do not grow too fast as ''k'' goes to infinity. (However, it is ''not always the case'' that the Taylor series of ''f'', if it converges, will in fact converge to ''f'', as explained below; ''f'' is then said to be non-[[analytic function|analytic]].) \n\nOne might think of the Taylor series\n\n:<math> f(x) \\approx \\sum_{k=0}^\\infty c_k(x-a)^k = c_0 + c_1(x-a) + c_2(x-a)^2 + \\cdots </math>\n\nof an infinitely many times differentiable function ''f'' : '''R''' → '''R''' as its \"infinite order Taylor polynomial\" at ''a''. Now the [[Taylor's theorem#Estimates for the remainder|estimates for the remainder]] imply that if, for any ''r'', the derivatives of ''f'' are known to be bounded over (''a''&nbsp;−&nbsp;''r'',''a''&nbsp;+&nbsp;''r''), then for any order ''k'' and for any ''r''&nbsp;>&nbsp;0 there exists a constant {{nowrap|''M<sub>k,r</sub>'' > 0}} such that\n\n:<math>(*) \\quad |R_k(x)|\\leqslant M_{k,r}\\frac{|x-a|^{k+1}}{(k+1)!} </math>\n\nfor every ''x''&nbsp;∈&nbsp;(''a''&nbsp;−&nbsp;''r'',''a''&nbsp;+&nbsp;''r''). Sometimes the constants {{nowrap|''M<sub>k,r</sub>''}} can be chosen in such way that {{nowrap|''M<sub>k,r</sub>''}} is bounded above, for fixed ''r'' and all ''k''. Then the Taylor series of ''f'' [[uniform convergence|converges uniformly]] to some analytic function\n\n:<math>\\begin{cases} T_f:(a-r,a+r)\\to\\mathbf R \\\\ T_f(x) = \\sum_{k=0}^\\infty \\frac{f^{(k)}(a)}{k!}(x-a)^k\\end{cases}</math>\n\n(One also gets convergence even if {{nowrap|''M<sub>k,r</sub>''}} is not bounded above as long as it grows slowly enough.)\n\nHowever, even though {{nowrap|''T<sub>f</sub>''}} is always analytic, the case may be that ''f'' is not. That is to say, it may well be that an infinitely many times differentiable function ''f'' has a Taylor series at ''a'' which converges on some open neighborhood of ''a'', but the limit function ''T<sub>f</sub>'' is different from ''f''. An important example of this phenomenon is provided by the [[non-analytic smooth function]] known as a [[flat function]]:\n\n:<math>\\begin{cases} f:\\mathbf R \\to \\mathbf R \\\\ f(x) = \\begin{cases} e^{-\\frac{1}{x^2}} & x>0  \\\\ 0 & x\\leqslant 0\\end{cases} \\end{cases}.</math>\n\nUsing the [[chain rule]] one can show by [[mathematical induction]] that for any order&nbsp;''k'',\n\n:<math> f^{(k)}(x) = \\begin{cases} \\frac{p_k(x)}{x^{3k}}\\cdot e^{-\\frac{1}{x^2}}  & x>0 \\\\ 0 & x\\leqslant 0\\end{cases}</math>\n\nfor some polynomial ''p<sub>k</sub>'' of degree 2(''k'' − 1). The function <math>e^{-\\frac{1}{x^2}}</math> tends to zero faster than any polynomial as {{nowrap|''x'' → 0}}, so ''f'' is infinitely many times differentiable and {{nowrap|''f''<sup>(''k'')</sup>(0) {{=}} 0}} for every positive integer ''k''. Now the estimates for the remainder for the Taylor polynomials show that the Taylor series of ''f'' converges uniformly to the zero function on the whole real axis. Nothing is wrong in here:\n\n* The Taylor series of ''f'' converges uniformly to the zero function ''T<sub>f</sub>''(''x'')&nbsp;=&nbsp;0.\n* The zero function is analytic and every coefficient in its Taylor series is zero.\n* The function ''f'' is infinitely many times differentiable, but not analytic.\n* For any ''k''&nbsp;∈&nbsp;''N'' and ''r''&nbsp;>&nbsp;0 there exists ''M<sub>k,r</sub>''&nbsp;>&nbsp;0 such that the remainder term for the ''k''-th order Taylor polynomial of ''f'' satisfies&nbsp;(*), and is bounded above, for all ''k'' and fixed ''r''.\n\n=== Taylor's theorem in complex analysis ===\n\nTaylor's theorem generalizes to functions ''f'' : '''C''' → '''C''' which are [[complex differentiable]] in an open subset ''U''&nbsp;⊂&nbsp;'''C''' of the [[complex plane]]. However, its usefulness is dwarfed by other general theorems in [[complex analysis]]. Namely, stronger versions of related results can be deduced for [[complex differentiable]] functions ''f''&nbsp;:&nbsp;''U''&nbsp;→&nbsp;'''C''' using [[Cauchy's integral formula]] as follows.\n\nLet ''r''&nbsp;>&nbsp;0 such that the [[closed disk]] ''B''(''z'',&nbsp;''r'')&nbsp;∪&nbsp;''S''(''z'',&nbsp;''r'') is contained in ''U''. Then Cauchy's integral formula with a positive parametrization {{nowrap|''&gamma;''(''t''){{=}}''z'' + ''re<sup>it</sup>''}} of the circle ''S''(''z'', ''r'') with {{nowrap|''t'' ∈ [0, 2{{pi}}]}} gives\n\n:<math>f(z) = \\frac{1}{2\\pi i}\\int_\\gamma \\frac{f(w)}{w-z}\\,dw, \\quad f'(z) = \\frac{1}{2\\pi i}\\int_\\gamma \\frac{f(w)}{(w-z)^2} \\, dw, \\quad \\ldots, \\quad f^{(k)}(z) = \\frac{k!}{2\\pi i}\\int_\\gamma \\frac{f(w)}{(w-z)^{k+1}} \\, dw.</math>\n\nHere all the integrands are continuous on the [[circle]] ''S''(''z'',&nbsp;''r''), which justifies differentiation under the integral sign. In particular, if ''f'' is once [[complex differentiable]]  on the open set ''U'', then it is actually infinitely many times [[complex differentiable]] on ''U''. One also obtains the Cauchy's estimates<ref>{{harvnb|Rudin|1987|loc=§10.26}}</ref>\n\n:<math>  |f^{(k)}(z)| \\leqslant \\frac{k!}{2\\pi}\\int_\\gamma \\frac{M_r}{|w-z|^{k+1}} \\, dw = \\frac{k!M_r}{r^k}, \\quad M_r = \\max_{|w-c|=r}|f(w)| </math>\n\nfor any ''z''&nbsp;∈&nbsp;''U'' and ''r''&nbsp;>&nbsp;0 such that ''B''(''z'',&nbsp;''r'')&nbsp;∪&nbsp;''S''(''c'',&nbsp;''r'')&nbsp;⊂&nbsp;''U''. These estimates imply that the [[complex number|complex]] [[Taylor series]]\n\n:<math> T_f(z) =  \\sum_{k=0}^\\infty \\frac{f^{(k)}(c)}{k!}(z-c)^k </math>\n\nof ''f'' converges uniformly on any [[open disk]] ''B''(''c'',&nbsp;''r'')&nbsp;⊂&nbsp;''U'' with ''S''(''c'',&nbsp;''r'')&nbsp;⊂&nbsp;''U'' into some function ''T<sub>f</sub>''. Furthermore, using the contour integral formulae for the derivatives ''f''<sup>(''k'')</sup>(''c''),\n\n:<math>\\begin{align} \nT_f(z) &= \\sum_{k=0}^\\infty  \\frac{(z-c)^k}{2\\pi i}\\int_\\gamma \\frac{f(w)}{(w-c)^{k+1}} \\, dw \\\\\n&=  \\frac{1}{2\\pi i} \\int_\\gamma \\frac{f(w)}{w-c} \\sum_{k=0}^\\infty  \\left(\\frac{z-c}{w-c}\\right)^k \\, dw \\\\\n&= \\frac{1}{2\\pi i} \\int_\\gamma \\frac{f(w)}{w-c}\\left( \\frac{1}{1-\\frac{z-c}{w-c}} \\right) \\, dw \\\\\n&= \\frac{1}{2\\pi i} \\int_\\gamma \\frac{f(w)}{w-z} \\, dw = f(z),\n\\end{align}</math>\n\nso any [[complex derivative|complex differentiable]] function ''f'' in an open set ''U''&nbsp;⊂&nbsp;'''C''' is in fact [[complex analytic]]. All that is said for real analytic functions [[Taylor's theorem#Relationship to analyticity##Taylor expansions of analytic functions|here]] holds also for complex analytic functions with the open interval ''I'' replaced by an open subset ''U''&nbsp;∈&nbsp;'''C''' and ''a''-centered intervals (''a''&nbsp;−&nbsp;''r'',&nbsp;''a''&nbsp;+&nbsp;''r'') replaced by ''c''-centered disks ''B''(''c'',&nbsp;''r''). In particular, the Taylor expansion holds in the form\n\n:<math> f(z) = P_k(z) + R_k(z), \\quad P_k(z) = \\sum_{j=0}^k \\frac{f^{(j)}(c)}{j!}(z-c)^j, </math>\n\nwhere the remainder term ''R<sub>k</sub>'' is complex analytic. Methods of complex analysis provide some powerful results regarding Taylor expansions. For example, using Cauchy's integral formula for any positively oriented [[Jordan curve]] ''&gamma;'' which parametrizes the boundary ∂''W''&nbsp;⊂&nbsp;''U'' of a region ''W''&nbsp;⊂&nbsp;''U'', one obtains expressions for the derivatives {{nowrap|''f''<sup>(''j'')</sup>(''c'')}} as above, and modifying slightly the computation for {{nowrap|''T<sub>f</sub>''(''z'') {{=}} ''f''(''z'')}}, one arrives at the exact formula\n\n:<math> R_k(z) = \\sum_{j=k+1}^\\infty  \\frac{(z-c)^j}{2\\pi i} \\int_\\gamma \\frac{f(w)}{(w-c)^{j+1}} \\, dw = \\frac{(z-c)^{k+1}}{2\\pi i} \\int_\\gamma \\frac{f(w) \\, dw}{(w-c)^{k+1}(w-z)} , \\qquad z\\in W. </math>\n\nThe important feature here is that the quality of the approximation by a Taylor polynomial on the region ''W''&nbsp;⊂&nbsp;''U'' is dominated by the values of the function ''f'' itself on the boundary ∂''W''&nbsp;⊂&nbsp;''U''. Similarly, applying Cauchy's estimates to the series expression for the remainder, one obtains the uniform estimates\n\n:<math> |R_k(z)| \\leqslant \\sum_{j=k+1}^\\infty  \\frac{M_r |z-c|^j}{r^j} = \\frac{M_r}{r^{k+1}} \\frac{|z-c|^{k+1}}{1-\\frac{|z-c|}{r}} \\leqslant \\frac{M_r \\beta^{k+1}}{1-\\beta}, \\qquad \\frac{|z-c|}{r}\\leqslant \\beta < 1. </math>\n\n=== Example ===\n\n[[File:Function with two poles.png|thumb|right|Complex plot of ''f''(''z'')&nbsp;=&nbsp;1/(1&nbsp;+&nbsp;''z''<sup>2</sup>). Modulus is shown by elevation and argument by coloring: cyan=0, blue&nbsp;=&nbsp;{{pi}}/3, violet&nbsp;=&nbsp;2{{pi}}/3, red&nbsp;=&nbsp;{{pi}}, yellow=4{{pi}}/3, green=5{{pi}}/3.]]\nThe function\n\n:<math>\\begin{cases} f : \\mathbf{R} \\to \\mathbf{R} \\\\ f(x) = \\frac{1}{1+x^2} \\end{cases}</math>\n\nis [[analytic function|real analytic]], that is, locally determined by its Taylor series. This function was plotted [[Taylor's theorem#Motivation|above]] to illustrate the fact that some elementary functions cannot be approximated by Taylor polynomials in neighborhoods of the center of expansion which are too large. This kind of behavior is easily understood in the framework of complex analysis. Namely, the function ''f'' extends into a [[meromorphic function]]\n\n:<math>\\begin{cases} f:\\mathbf C\\cup\\{\\infty\\} \\to \\mathbf C\\cup\\{\\infty\\} \\\\ f(z) = \\frac{1}{1+z^2} \\end{cases}</math>\n\non the compactified complex plane. It has simple poles at ''z''&nbsp;=&nbsp;''i'' and ''z''&nbsp;= −''i'', and it is analytic elsewhere. Now its Taylor series centered at ''z''<sub>0</sub> converges on any disc ''B''(''z''<sub>0</sub>, ''r'') with ''r'' < |''z''&nbsp;−&nbsp;''z''<sub>0</sub>|, where the same Taylor series converges at ''z''&nbsp;∈&nbsp;'''C'''. Therefore, Taylor series of ''f'' centered at 0 converges on ''B''(0, 1) and it does not converge for any ''z'' ∈ '''C''' with |''z''|&nbsp;>&nbsp;1 due to the poles at ''i'' and −''i''. For the same reason the Taylor series of ''f'' centered at 1 converges on ''B''(1, √2) and does not converge for any ''z''&nbsp;∈&nbsp;'''C''' with |''z''&nbsp;−&nbsp;1| > √2.\n\n== Generalizations of Taylor's theorem ==\n\n=== Higher-order differentiability ===\n\nA function ''f'': '''R'''<sup>''n''</sup>&nbsp;→&nbsp;'''R''' is [[derivative|differentiable]] at '''''a'''''&nbsp;∈&nbsp;'''R'''<sup>''n''</sup> [[if and only if]] there exists a [[linear functional]] ''L''&nbsp;:&nbsp;'''R'''<sup>''n''</sup>&nbsp;→&nbsp;'''R''' and a function ''h''&nbsp;:&nbsp;'''R'''<sup>''n''</sup>&nbsp;→&nbsp;'''R''' such that\n\n:<math>f(\\boldsymbol{x}) = f(\\boldsymbol{a}) + L(\\boldsymbol{x}-\\boldsymbol{a}) + h(\\boldsymbol{x})|\\boldsymbol{x}-\\boldsymbol{a}|,\n\\qquad \\lim_{\\boldsymbol{x}\\to\\boldsymbol{a}}h(\\boldsymbol{x})=0. </math>\n\nIf this is the case, then ''L''&nbsp;=&nbsp;''df''('''''a''''') is the (uniquely defined) [[differential of a function|differential]] of ''f'' at the point '''''a'''''. Furthermore, then the [[partial derivatives]] of ''f'' exist at '''''a''''' and the differential of ''f'' at '''''a''''' is given by\n\n:<math> df( \\boldsymbol{a} )( \\boldsymbol{v} ) = \\frac{\\partial f}{\\partial x_1}(\\boldsymbol{a})v_1 + \\cdots + \\frac{\\partial f}{\\partial x_n}(\\boldsymbol{a})v_n. </math>\n\nIntroduce the [[multi-index notation]]\n\n:<math> |\\alpha| = \\alpha_1+\\cdots+\\alpha_n, \\quad \\alpha!=\\alpha_1!\\cdots\\alpha_n!, \\quad \\boldsymbol{x}^\\alpha=x_1^{\\alpha_1}\\cdots x_n^{\\alpha_n} </math>\n\nfor ''&alpha;''&nbsp;∈&nbsp;'''N'''<sup>''n''</sup> and '''''x'''''&nbsp;∈&nbsp;'''R'''<sup>''n''</sup>.  If all the ''k''-th order [[partial derivatives]] of {{nowrap|''f'' : '''R'''<sup>''n''</sup> &rarr; '''R'''}} are continuous at {{nowrap|'''''a''''' &isin; '''R'''<sup>''n''</sup>}}, then by [[symmetry of second derivatives|Clairaut's theorem]], one can change the order of mixed derivatives at '''''a''''', so the notation\n\n:<math> D^\\alpha f = \\frac{\\partial^{|\\alpha|}f}{\\partial x_1^{\\alpha_1}\\cdots \\partial x_n^{\\alpha_n}}, \\qquad |\\alpha|\\leq k </math>\n\nfor the higher order [[partial derivatives]] is justified in this situation. The same is true if all the (''k''&nbsp;&minus;&nbsp;1)-th order partial derivatives of ''f'' exist in some neighborhood of '''''a''''' and are differentiable at '''''a'''''.<ref>This follows from iterated application of the theorem that if the partial derivatives of a function ''f'' exist in a neighborhood of '''''a''''' and are continuous at '''''a''''', then the function is differentiable at '''''a'''''.  See, for instance, {{harvnb|Apostol|1974|loc=Theorem 12.11}}.</ref> Then we say that ''f'' is ''k'' '''times differentiable at the point&nbsp;''a'' '''.\n\n=== Taylor's theorem for multivariate functions ===\n\n{{quotation|'''Multivariate version of Taylor's theorem.'''<ref>Königsberger Analysis 2, p. 64 ff.</ref> Let {{nowrap|''f'' : '''R'''<sup>''n''</sup> &rarr; '''R'''}} be a ''k'' times differentiable function at the point {{nowrap|'''''a'''''&isin;'''R'''<sup>''n''</sup>}}. Then there exists  {{nowrap|''h''<sub>''&alpha;''</sub> : '''R'''<sup>n</sup>&rarr;'''R'''}} such that \n\n:<math>\\begin{align}& f(\\boldsymbol{x}) = \\sum_{|\\alpha|\\leq k} \\frac{D^\\alpha f(\\boldsymbol{a})}{\\alpha!} (\\boldsymbol{x}-\\boldsymbol{a})^\\alpha  + \\sum_{|\\alpha|=k} h_\\alpha(\\boldsymbol{x})(\\boldsymbol{x}-\\boldsymbol{a})^\\alpha, \\\\& \\mbox{and}\\quad \\lim_{\\boldsymbol{x}\\to \\boldsymbol{a}}h_\\alpha(\\boldsymbol{x})=0.\\end{align}</math>}}\n\nIf the function {{nowrap|''f'' : '''R'''<sup>''n''</sup> &rarr; '''R'''}} is ''k''&nbsp;+&nbsp;1 times [[continuously differentiable]] in the [[closed ball]] ''B'', then one can derive an exact formula for the remainder in terms of {{nowrap|(''k''+1)-th}} order [[partial derivatives]] of ''f'' in this neighborhood. Namely,\n\n:<math> \\begin{align}& f( \\boldsymbol{x} ) = \\sum_{|\\alpha|\\leq k} \\frac{D^\\alpha f(\\boldsymbol{a})}{\\alpha!} (\\boldsymbol{x}-\\boldsymbol{a})^\\alpha  + \\sum_{|\\beta|=k+1} R_\\beta(\\boldsymbol{x})(\\boldsymbol{x}-\\boldsymbol{a})^\\beta, \\\\&\nR_\\beta( \\boldsymbol{x} ) = \\frac{|\\beta|}{\\beta!} \\int_0^1 (1-t)^{|\\beta|-1}D^\\beta f \\big(\\boldsymbol{a}+t( \\boldsymbol{x}-\\boldsymbol{a} )\\big) \\, dt. \\end{align}\n</math>\n\nIn this case, due to the [[continuous function|continuity]] of (''k''+1)-th order [[partial derivative]]s in the [[compact set]] ''B'', one immediately obtains the uniform estimates\n\n:<math>\\left|R_\\beta(\\boldsymbol{x})\\right| \\leq \\frac{1}{\\beta!} \\max_{|\\alpha|=|\\beta|} \\max_{\\boldsymbol{y}\\in B} |D^\\alpha f(\\boldsymbol{y})|, \\qquad \\boldsymbol{x}\\in B. </math>\n\n=== Example in two dimensions ===\n\nFor example, the third-order Taylor polynomial of a smooth function ''f'': '''R'''<sup>''2''</sup>&nbsp;→&nbsp;'''R''' is, denoting '''''x''''' &minus; '''''a''''' = '''''v''''',\n:<math>\\begin{align}\nP_3(\\boldsymbol{x}) = f ( \\boldsymbol{a} ) + {} &\\frac{\\partial f}{\\partial x_1}( \\boldsymbol{a} ) v_1 + \\frac{\\partial f}{\\partial x_2}( \\boldsymbol{a} ) v_2 + \\frac{\\partial^2 f}{\\partial x_1^2}( \\boldsymbol{a} ) \\frac {v_1^2}{2!} +  \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}( \\boldsymbol{a} ) v_1 v_2 + \\frac{\\partial^2 f}{\\partial x_2^2}( \\boldsymbol{a} ) \\frac{v_2^2}{2!}  \\\\\n& + \\frac{\\partial^3 f}{\\partial x_1^3}( \\boldsymbol{a} ) \\frac{v_1^3}{3!} + \\frac{\\partial^3 f}{\\partial x_1^2 \\partial x_2}( \\boldsymbol{a} ) \\frac{v_1^2 v_2}{2!} + \\frac{\\partial^3 f}{\\partial x_1 \\partial x_2^2}( \\boldsymbol{a} ) \\frac{v_1 v_2^2}{2!} + \\frac{\\partial^3 f}{\\partial x_2^3}( \\boldsymbol{a} ) \\frac{v_2^3}{3!}\n\\end{align}</math>\n\n== Proofs ==\n\n=== Proof for Taylor's theorem in one real variable ===\n\nLet<ref>{{harvnb|Stromberg|1981}}</ref>\n\n:<math>h_k(x) = \\begin{cases}\n\\frac{f(x) - P(x)}{(x-a)^k} & x\\not=a\\\\\n0&x=a\n\\end{cases}\n</math>\n\nwhere, as in the statement of Taylor's theorem,\n\n:<math>P(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(k)}(a)}{k!}(x-a)^k.</math>\n\nIt is sufficient to show that\n\n:<math>\\lim_{x\\to a} h_k(x) =0. </math>\n\nThe proof here is based on repeated application of [[L'Hôpital's rule]].  Note that, for each {{nowrap|''j'' {{=}} 0,1,...,''k''&minus;1}}, <math>f^{(j)}(a)=P^{(j)}(a)</math>.  Hence each of the first ''k''&minus;1 derivatives of the numerator in <math>h_k(x)</math> vanishes at <math>x=a</math>, and the same is true of the denominator.  Also, since the condition that the function ''f'' be ''k'' times differentiable at a point requires differentiability up to order ''k''−1 in a neighborhood of said point (this is true, because differentiability requires a function to be defined in a whole neighborhood of a point), the numerator and its ''k''&nbsp;−&nbsp;2 derivatives are differentiable in a neighborhood of ''a''. Clearly, the denominator also satisfies said condition, and additionally, doesn't vanish unless ''x''=''a'', therefore all conditions necessary for L'Hopital's rule are fulfilled, and its use is justified. So\n\n:<math>\\begin{align}\n\\lim_{x\\to a} \\frac{f(x) - P(x)}{(x-a)^k} &= \\lim_{x\\to a} \\frac{\\frac{d}{dx}(f(x) - P(x))}{\\frac{d}{dx}(x-a)^k} = \\cdots = \\lim_{x\\to a} \\frac{\\frac{d^{k-1}}{dx^{k-1}}(f(x) - P(x))}{\\frac{d^{k-1}}{dx^{k-1}}(x-a)^k}\\\\\n&=\\frac{1}{k!}\\lim_{x\\to a} \\frac{f^{(k-1)}(x) - P^{(k-1)}(x)}{x-a}\\\\\n&=\\frac{1}{k!}(f^{(k)}(a) - f^{(k)}(a)) = 0\n\\end{align}</math>\n\nwhere the second to last equality follows by the definition of the derivative at&nbsp;''x''&nbsp;=&nbsp;''a''.\n\n=== Derivation for the mean value forms of the remainder ===\n\nLet ''G'' be any real-valued function, continuous on the closed interval between ''a'' and ''x'' and differentiable with a non-vanishing derivative on the open interval between ''a'' and ''x'', and define\n\n:<math>\n  F(t) = f(t) + f'(t)(x-t) + \\frac{f''(t)}{2!}(x-t)^2 + \\cdots + \\frac{f^{(k)}(t)}{k!}(x-t)^k.\n</math>\n\nFor <math> t \\in [a,x] </math>. Then, by [[mean value theorem#Cauchy's mean value theorem|Cauchy's mean value theorem]],\n\n:<math>\n(*) \\quad  \\frac{F'(\\xi)}{G'(\\xi)} = \\frac{F(x) - F(a)}{G(x) - G(a)}\n</math>\n\nfor some ξ on the open interval between ''a'' and ''x''. Note that here the numerator {{nowrap|''F''(''x'') − ''F''(''a'') {{=}} ''R<sub>k</sub>''(''x'')}} is exactly the remainder of the Taylor polynomial for ''f''(''x''). Compute\n\n:<math>\\begin{align}\nF'(t) = {} & f'(t) + \\big(f''(t)(x-t) - f'(t)\\big) + \\left(\\frac{f^{(3)}(t)}{2!}(x-t)^2 - \\frac{f^{(2)}(t)}{1!}(x-t)\\right)  +  \\cdots \\\\\n& \\cdots + \\left( \\frac{f^{(k+1)}(t)}{k!}(x-t)^k - \\frac{f^{(k)}(t)}{(k-1)!}(x-t)^{k-1}\\right)\n= \\frac{f^{(k+1)}(t)}{k!}(x-t)^k,\n\\end{align}</math>\n\nplug it into (*) and rearrange terms to find that\n\n:<math>\n  R_k(x) = \\frac{f^{(k+1)}(\\xi)}{k!}(x-\\xi)^k \\frac{G(x)-G(a)}{G'(\\xi)}.\n</math>\n\nThis is the form of the remainder term mentioned after the actual statement of Taylor's theorem with remainder in the mean value form.\nThe Lagrange form of the remainder is found by choosing <math> \\ G(t)=(t-x)^{k+1} </math> and the Cauchy form by choosing <math> \\ G(t) = t-a</math>.\n\n'''Remark.''' Using this method one can also recover the integral form of the remainder by choosing\n\n:<math>\n G(t) = \\int_a^t \\frac{f^{(k+1)}(s)}{k!} (x-s)^k \\, ds,\n</math>\n\nbut the requirements for ''f'' needed for the use of mean value theorem are too strong, if one aims to prove the claim in the case that ''f''<sup>(''k'')</sup> is only [[absolutely continuous]]. However, if one uses [[Riemann integral]] instead of [[Lebesgue integral]], the assumptions cannot be weakened.\n\n=== Derivation for the integral form of the remainder ===\n\nDue to [[absolutely continuous|absolute continuity]] of ''f''<sup>(''k'')</sup> on the [[closed interval]] between ''a'' and ''x'' its derivative ''f''<sup>(''k''+1)</sup> exists as an ''L''<sup>1</sup>-function, and we can use [[fundamental theorem of calculus]] and [[integration by parts]]. This same proof applies for the [[Riemann integral]] assuming that ''f''<sup>(''k'')</sup> is [[continuous function|continuous]] on the closed interval and [[Differentiable function|differentiable]] on the [[open interval]] between ''a'' and ''x'', and this leads to the same result than using the mean value theorem.\n\nThe [[fundamental theorem of calculus]] states that\n\n:<math>f(x)=f(a)+ \\int_a^x \\, f'(t) \\, dt.</math>\n\nNow we can [[Integration by parts|integrate by parts]] and use the fundamental theorem of calculus again to see that\n\n:<math> \\begin{align}\nf(x) &= f(a)+\\Big(xf'(x)-af'(a)\\Big)-\\int_a^x tf''(t) \\, dt \\\\\n&= f(a) + x\\left(f'(a) + \\int_a^x f''(t) \\,dt \\right) -af'(a)-\\int_a^x  tf''(t) \\, dt \\\\\n&= f(a)+(x-a)f'(a)+\\int_a^x \\, (x-t)f''(t) \\, dt,\n\\end{align} </math>\n\nwhich is exactly Taylor's theorem with remainder in the integral form in the case ''k=1''.\nThe general statement is proved using [[mathematical induction|induction]]. Suppose that\n:<math> (*) \\quad f(x) = f(a) + \\frac{f'(a)}{1!}(x - a) + \\cdots + \\frac{f^{(k)}(a)}{k!}(x - a)^k + \\int_a^x \\frac{f^{(k+1)} (t)}{k!} (x - t)^k \\, dt. </math>\n\nIntegrating the remainder term by parts we arrive at\n\n:<math>\\begin{align}\n\\int_a^x \\frac{f^{(k+1)} (t)}{k!} (x - t)^k \\, dt = & - \\left[ \\frac{f^{(k+1)} (t)}{(k+1)k!} (x - t)^{k+1} \\right]_a^x + \\int_a^x \\frac{f^{(k+2)} (t)}{(k+1)k!} (x - t)^{k+1} \\, dt \\\\\n= & \\ \\frac{f^{(k+1)} (a)}{(k+1)!} (x - a)^{k+1} + \\int_a^x \\frac{f^{(k+2)} (t)}{(k+1)!} (x - t)^{k+1} \\, dt.\n\\end{align}</math>\n\nSubstituting this into the formula {{nowrap|in (*)}} shows that if it holds for the value ''k'', it must also hold for the value ''k''&nbsp;+&nbsp;1.\nTherefore, since it holds for ''k''&nbsp;=&nbsp;1, it must hold for every positive integer&nbsp;''k''.\n\n=== Derivation for the Cauchy form of the remainder ===\n\nTo the integral form of the remainder, we can apply the mean value theorem for integral.\n:<math>\\begin{align}\\int_a^x\\frac {f^{(k+1)}(t)}{k!}(x-t)^k\\mathrm dt\n&=\\frac{f^{(k+1)}(\\xi)}{k!}(x-\\xi)^k\\int_a^x\\mathrm dt\\\\\n&=\\frac{f^{(k+1)}(\\xi)}{k!}(x-\\xi)^k(x-a)\n\\end{align}</math>\n\n,where <math>\\xi\\in\\{a+\\theta(x-a)\\colon 0<\\theta<1\\}</math>\n\nSo, The Cauchy form of the remainder is hold.\n\n=== Derivation for the remainder of multivariate Taylor polynomials ===\n\nWe prove the special case, where ''f'' : '''R'''<sup>''n''</sup> → '''R''' has continuous partial derivatives up to the order ''k''+1 in some closed ball ''B'' with center '''''a'''''.  The strategy of the proof is to apply the one-variable case of Taylor's theorem to the restriction of ''f'' to the line segment adjoining '''''x''''' and '''''a'''''.<ref>{{harvnb|Hörmander|1976|pp=12&ndash;13}}</ref>  Parametrize the line segment between '''''a''''' and '''''x''''' by '''''u'''''(''t'') = {{nowrap|'''''a''''' + ''t''('''''x''''' − '''''a''''').}} We apply the one-variable version of Taylor's theorem to the function {{nowrap|''g''(''t'') {{=}} ''f''('''''u'''''(''t''))}}:\n\n:<math>f(\\mathbf{x})=g(1)=g(0)+\\sum_{j=1}^k\\frac{1}{j!}g^{(j)}(0)\\ +\\ \\int_0^1 \\frac{(1-t)^k }{k!} g^{(k+1)}(t)\\, dt.</math>\n\nApplying the [[chain rule]] for several variables gives\n\n:<math>\\begin{align}\ng^{(j)}(t)&=\\frac{d^j}{dt^j}f(u(t)) = \\frac{d^j}{dt^j} f(\\mathbf{a}+t(\\mathbf{x}-\\mathbf{a})) \\\\\n&= \\sum_{|\\alpha|=j} \\left(\\begin{matrix} j \\\\ \\alpha\\end{matrix} \\right) (D^\\alpha f) (\\mathbf{a}+t(\\mathbf{x}-\\mathbf{a})) (\\mathbf{x}-\\mathbf{a})^\\alpha\n\\end{align}</math>\n\nwhere <math>\\left(\\begin{matrix}j \\\\ \\alpha\\end{matrix}\\right)</math> is the [[multinomial coefficient]]. Since <math>\\frac{1}{j!}\\left(\\begin{matrix}j\\\\ \\alpha\\end{matrix}\\right)=\\frac{1}{\\alpha!}</math>, we get\n\n:<math>f(\\mathbf x)= f(\\mathbf a) + \\sum_{1 \\leq |\\alpha| \\leq k}\\frac{1}{\\alpha!} (D^\\alpha f) (\\mathbf a)(\\mathbf x-\\mathbf a)^\\alpha+\\sum_{|\\alpha|=k+1}\\frac{k+1}{\\alpha!}\n(\\mathbf x-\\mathbf a)^\\alpha \\int_0^1 (1-t)^k (D^\\alpha f)(\\mathbf a+t(\\mathbf x-\\mathbf a))\\,dt.</math>\n\n== See also ==\n* [[Laurent series]]\n* [[Padé approximant]]\n* [[Newton series]]\n\n== Footnotes ==\n{{Reflist}}\n\n== References ==\n*{{citation\n | title=Calculus\n | authorlink=Tom Apostol\n | first=Tom\n | last=Apostol\n | publisher=Wiley\n | year=1967\n | isbn=0-471-00005-1\n}}.\n*{{citation\n | title=Mathematical analysis\n | first=Tom\n | last=Apostol\n | publisher=Addison–Wesley\n | year=1974\n}}.\n*{{citation\n | first1=Robert G.\n | last1=Bartle\n | first2=Donald R.\n | last2=Sherbert\n | title=Introduction to Real Analysis\n | edition=4th\n | publisher=Wiley\n | year=2011\n | isbn=978-0-471-43331-6\n}}.\n*{{citation\n | first=L.\n | last=Hörmander\n | authorlink=Lars Hörmander\n | title=Linear Partial Differential Operators, Volume 1\n | publisher=Springer\n | year=1976\n | isbn=978-3-540-00662-6\n}}.\n*{{citation\n | title = Mathematical thought from ancient to modern times, Volume 2\n | authorlink=Morris Kline\n | first=Morris\n | last=Kline\n | publisher=Oxford University Press\n | year=1972\n}}.\n*{{citation\n | title=Calculus: An Intuitive and Physical Approach\n | first=Morris\n | last=Kline\n | publisher=Dover\n | year=1998\n | isbn=0-486-40453-6\n}}.\n*{{citation\n | last=Pedrick\n | first=George\n | year=1994\n | title=A First Course in Analysis\n | publisher=Springer\n | isbn=0-387-94108-8\n}}.\n*{{citation\n | last=Stromberg\n | first=Karl\n | title=Introduction to classical real analysis\n | publisher=Wadsworth\n | year=1981\n | isbn=978-0-534-98012-2\n}}.\n*{{citation\n | last=Rudin\n | first=Walter\n | title=Real and complex analysis\n | edition=3rd\n | publisher=McGraw-Hill\n | year=1987\n | isbn=0-07-054234-1\n}}.\n*{{citation\n | last=Tao\n | first=Terence\n | title=Analysis, Volume I\n | edition=3rd\n | publisher=Hindustan Book Agency\n | year=2014\n | isbn=978-93-80250-64-9\n}}.\n\n== External links ==\n{{ProofWiki|id=Taylor's_Theorem/One_Variable|title=Proofs for a few forms of the remainder in one-variable case}}\n* [http://www.cut-the-knot.org/Curriculum/Calculus/TaylorSeries.shtml Taylor Series Approximation to Cosine] at [[cut-the-knot]]\n* [http://cinderella.de/files/HTMLDemos/2C02_Taylor.html Trigonometric Taylor Expansion] interactive demonstrative applet\n* [http://numericalmethods.eng.usf.edu/topics/taylor_series.html Taylor Series Revisited] at [http://numericalmethods.eng.usf.edu Holistic Numerical Methods Institute]\n\n[[Category:Articles containing proofs]]\n[[Category:Theorems in calculus]]\n[[Category:Theorems in real analysis]]\n[[Category:Approximations]]"
    },
    {
      "title": "Variational method (quantum mechanics)",
      "url": "https://en.wikipedia.org/wiki/Variational_method_%28quantum_mechanics%29",
      "text": "In [[quantum mechanics]], the '''variational method''' is one way of finding [[approximation]]s to the lowest energy eigenstate or [[ground state]], and some excited states. This allows calculating approximate wavefunctions such as [[molecular orbital]]s.<ref>''Lorentz Trial Function for the Hydrogen Atom: A Simple, Elegant Exercise'' Thomas Sommerfeld Journal of Chemical Education 2011 88 (11), 1521–1524 {{DOI|10.1021/ed200040e}}</ref> The basis for this method is the [[variational principle]].<ref>{{cite book |last=Griffiths |first=D. J. |authorlink=David Griffiths (physicist) |title=Introduction to Quantum Mechanics |year=1995 |publisher=[[Prentice Hall]] |location=Upper Saddle River, New Jersey |isbn=978-0-13-124405-4 |ref=Griffiths1995 }}\n</ref><ref>{{cite book |last=Sakurai |first=J. J. |authorlink=J. J. Sakurai |editor-first=San Fu |editor-last=Tuan |title=Modern Quantum Mechanics |edition=Revised |year=1994 |origyear= |publisher=[[Addison–Wesley]] |isbn=978-0-201-53929-5 |ref=Sakurai1994}}\n</ref>\n\nThe method consists of choosing a \"trial [[wavefunction]]\" depending on one or more [[parameter]]s, and finding the values of these parameters for which the [[expectation value (quantum mechanics)|expectation value]] of the energy is the lowest possible. The wavefunction obtained by fixing the parameters to such values is then an approximation to the ground state wavefunction, and the expectation value of the energy in that state is an [[upper bound]] to the ground state energy.  The [[Hartree–Fock method]], [[Density matrix renormalization group]], and [[Ritz method]] apply the variational method.\n\n== Description ==\nSuppose we are given a [[Hilbert space]] and a [[Hermitian operator]] over it called the [[Hamiltonian (quantum mechanics)|Hamiltonian]] '''''H'''''. Ignoring complications about [[continuous spectrum|continuous spectra]], we look at the [[spectrum (functional analysis)|discrete spectrum]] of '''''H''''' and the corresponding [[eigenspace]]s of each [[eigenvalue]] λ (see [[Hermitian operator#Spectral theorem|spectral theorem for Hermitian operator]]s for the mathematical background):\n\n:<math>\\lang\\psi_{\\lambda_1}\\mid \\psi_{\\lambda_2}\\rang=\\delta_{\\lambda_1\\lambda_2}</math>\n\nwhere <math>\\delta_{i,j}</math> is the [[Kronecker delta]]\n\n:<math>\\delta_{ij} = \\begin{cases}\n0 &\\text{if } i \\neq j,   \\\\\n1 &\\text{if } i=j.   \\end{cases}</math>\n\nand the Hamiltonian is related to λ through the typical eigenvalue relation\n\n:<math>\\hat{H} \\left| \\psi_\\lambda\\right\\rangle = \\lambda\\left|\\psi_\\lambda \\right\\rangle. </math>\n\nPhysical states are normalized, meaning that their norm is equal to&nbsp;1. Once again ignoring complications involved with a continuous spectrum of '''''H''''', suppose it is bounded from below and that its [[greatest lower bound]] is ''E''<sub>0</sub>. Suppose also that we know the corresponding state |ψ⟩. The [[expectation value]] of '''''H''''' is then\n\n:<math>\n\\begin{align}\n\\left\\langle\\psi\\mid H\\mid \\psi\\right\\rangle & = \\sum_{\\lambda_1,\\lambda_2 \\in \\mathrm{Spec}(H)} \\left\\langle\\psi|\\psi_{\\lambda_1}\\right\\rangle \\left\\langle\\psi_{\\lambda_1}|H|\\psi_{\\lambda_2}\\right\\rangle \\left\\langle\\psi_{\\lambda_2}|\\psi\\right\\rangle \\\\\n& =\\sum_{\\lambda\\in \\mathrm{Spec}(H)}\\lambda \\left|\\left\\langle\\psi_\\lambda\\mid \\psi\\right\\rangle\\right|^2\\ge\\sum_{\\lambda \\in \\mathrm{Spec}(H)}E_0 \\left|\\left\\langle\\psi_\\lambda\\mid \\psi\\right\\rangle\\right|^2=E_0\n\\end{align}\n</math>\n\nObviously, if we were to vary over all possible states with norm 1 trying to minimize the expectation value of '''''H''''', the lowest value would be ''E''<sub>0</sub> and the corresponding state would be an eigenstate of ''E''<sub>0</sub>. Varying over the entire Hilbert space is usually too complicated for physical calculations, and a subspace of the entire Hilbert space is chosen, parametrized by some (real) differentiable parameters ''α''<sub>''i''</sub> (''i''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;...,&nbsp;''N''). The choice of the subspace is called the [[ansatz]]. Some choices of ansatzes lead to better approximations than others, therefore the choice of ansatz is important.\n\nLet's assume there is some overlap between the ansatz and the [[ground state]] (otherwise, it's a bad ansatz). We still wish to normalize the ansatz, so we have the constraints\n\n:<math> \\left\\langle \\psi(\\mathbf{\\alpha}) \\mid \\psi(\\mathbf{\\alpha}) \\right\\rangle = 1</math>\n\nand we wish to minimize\n\n:<math> \\varepsilon(\\mathbf{\\alpha}) =  \\left\\langle \\psi(\\mathbf{\\alpha})|H|\\psi(\\mathbf{\\alpha}) \\right\\rangle. </math>\n\nThis, in general, is not an easy task, since we are looking for a [[global minimum]] and finding the zeroes of the partial derivatives of ''ε'' over all ''α''<sub>''i''</sub> is not sufficient. If ''ψ'' ('''''α''''') is expressed as a linear combination of other functions (''α''<sub>''i''</sub> being the coefficients), as in the [[Ritz method]], there is only one minimum and the problem is straightforward. There are other, non-linear methods, however, such as the [[Hartree–Fock method]], that are also not characterized by a multitude of minima and are therefore comfortable in calculations.\n\nThere is an additional complication in the calculations described. As ε tends toward E<sub>0</sub> in minimization calculations, there is no guarantee that the corresponding trial wavefunctions will tend to the actual wavefunction. This has been demonstrated by calculations using a modified harmonic oscillator as a model system, in which an exactly solvable system is approached using the variational method. A wavefunction different from the exact one is obtained by use of the method described above.{{citation needed|date=April 2016}}\n\nAlthough usually limited to calculations of the ground state energy, this method can be applied in certain cases to calculations of excited states as well. If the ground state wavefunction is known, either by the method of variation or by direct calculation, a subset of the Hilbert space can be chosen which is orthogonal to the ground state wavefunction. \n\n:<math>\\left| \\psi \\right\\rangle = \\left|\\psi_{\\text{test}}\\right\\rangle - \\left\\langle\\psi_{\\mathrm{gr}} \\mid  \\psi_{\\text{test}}\\right\\rangle \\left|\\psi_{\\text{gr}}\\right\\rangle </math>\n\nThe resulting minimum is usually not as accurate as for the ground state, as any difference between the true ground state and <math>\\psi_{\\text{gr}}</math>  results in a lower excited energy. This defect is worsened with each higher excited state.\n\nIn another formulation:\n\n:<math>E_\\text{ground} \\le \\left\\langle\\phi|H|\\phi\\right\\rangle. </math>\n\nThis holds for any trial φ since, by definition, the ground state wavefunction has the lowest energy, and any trial wavefunction will have energy greater than or equal to it.\n\nProof:\nφ can be expanded as a linear combination of the actual eigenfunctions of the Hamiltonian (which we assume to be normalized and orthogonal):\n:<math>\\phi = \\sum_n c_n \\psi_n. \\,</math>\n\nThen, to find the expectation value of the Hamiltonian:\n\n:<math>\n\\begin{align}\n& \\left\\langle\\phi|H|\\phi\\right\\rangle \\\\\n= {} & \\left\\langle\\sum_n c_n \\psi_n |H|\\sum_m c_m\\psi_m\\right\\rangle \\\\\n= {} & \\sum_n\\sum_m \\left\\langle c_n^* \\psi_{n}|E_m|c_m\\psi_m\\right\\rangle \\\\\n= {} & \\sum_n\\sum_m c_n^*c_m E_m\\left\\langle\\psi_n\\mid\\psi_m\\right\\rangle \\\\\n= {} & \\sum_{n} |c_n|^2 E_n.\n\\end{align}\n</math>\n\nNow, the ground state energy is the lowest energy possible, i.e. <math>E_{n} \\ge E_{g}</math>. Therefore, if the guessed wave function φ is normalized:\n:<math>\\left\\langle\\phi|H|\\phi\\right\\rangle \\ge E_g \\sum_n |c_n|^2 = E_g. \\,</math>\n\n===In general===\n\nFor a hamiltonian '''''H''''' that describes the studied system and ''any'' normalizable function '''''Ψ''''' with arguments appropriate for the unknown wave function of the system, we define the [[functional (mathematics)|functional]]\n\n: <math> \\varepsilon\\left[\\Psi\\right] = \\frac{\\left\\langle\\Psi|\\hat{H}|\\Psi\\right\\rangle}{\\left\\langle\\Psi \\mid \\Psi\\right\\rangle}.</math>\n\nThe variational principle states that\n* <math>\\varepsilon \\geq E_0</math>, where <math>E_0</math> is the lowest energy eigenstate (ground state) of the hamiltonian\n* <math>\\varepsilon = E_0</math> if and only if <math>\\Psi</math> is exactly equal to the wave function of the ground state of the studied system.\n\nThe variational principle formulated above is the basis of the variational method used in [[quantum mechanics]] and [[quantum chemistry]] to find approximations to the [[ground state]].\n\nAnother facet in variational principles in quantum mechanics is that since <math>\\Psi</math> and <math>\\Psi^\\dagger</math> can be varied separately (a fact arising due to the complex nature of the wave function), the quantities can be varied in principle just one at a time.<ref>see Landau, Quantum Mechanics , pg. 58 for some elaboration.</ref>\n\n== Helium atom ground state ==\nThe [[helium atom]] consists of two [[electron]]s with mass ''m'' and electric charge&nbsp;−''e'', around an essentially fixed [[atomic nucleus|nucleus]] of mass ''M'' ≫ ''m'' and charge +2''e''. The Hamiltonian for it, neglecting the [[fine structure]], is:\n:<math>H = -\\frac{\\hbar^2}{2m} (\\nabla_1^2 + \\nabla_2^2) - \\frac{e^2}{4\\pi\\epsilon_0} \\left(\\frac{2}{r_1} + \\frac{2}{r_2} - \\frac{1}{|\\mathbf{r}_1 - \\mathbf{r}_2|}\\right)</math>\nwhere ''ħ'' is the [[reduced Planck constant]], ''ε''<sub>0</sub> is the [[vacuum permittivity]], ''r<sub>i</sub>'' (for ''i'' = 1, 2) is the distance of the ''i''-th electron from the nucleus, and |'''r'''<sub>1</sub>&nbsp;−&nbsp;'''r'''<sub>2</sub>| is the distance between the two electrons.\n\nIf the term ''V<sub>ee</sub>'' = ''e''<sup>2</sup>/(4π''ε''<sub>0</sub>|'''r'''<sub>1</sub>&nbsp;−&nbsp;'''r'''<sub>2</sub>|), representing the repulsion between the two electrons, were excluded, the Hamiltonian would become the sum of two [[hydrogen-like atom]] Hamiltonians  with nuclear charge +2''e''. The ground state energy would then be 8''E''<sub>1</sub> = −109&nbsp;eV, where ''E''<sub>1</sub> is the [[Rydberg constant]], and its ground state wavefunction would be the product of two wavefunctions for the ground state of hydrogen-like atoms:<ref name=Griffiths1995>Griffiths (1995), p. 262.</ref>\n:<math> \\psi(\\mathbf{r}_1,\\mathbf{r}_2) = \\frac{Z^3}{\\pi a_0^3} e^{-Z(r_1+r_2)/a_0}.</math>\nwhere ''a''<sub>0</sub> is the [[Bohr radius]] and Z = 2, helium's nuclear charge.  The expectation value of the total Hamiltonian ''H'' (including the term ''V<sub>ee</sub>'') in the state described by ''ψ''<sub>0</sub> will be an upper bound for its ground state energy. <''V<sub>ee</sub>''> is −5''E''<sub>1</sub>/2 = 34&nbsp;eV, so <H> is 8''E''<sub>1</sub>&nbsp;−&nbsp;5''E''<sub>1</sub>/2 = −75&nbsp;eV.\n\nA tighter upper bound can be found by using a better trial wavefunction with 'tunable' parameters. Each electron can be thought to see the nuclear charge partially \"shielded\" by the other electron, so we can use a trial wavefunction equal with an \"effective\" nuclear charge ''Z''&nbsp;<&nbsp;2: The expectation value of ''H'' in this state is:\n\n: <math> \\langle H \\rangle = \\left[-2Z^2 + \\frac{27}{4}Z\\right] E_1 </math>\n\nThis is minimal for Z = 27/16 implying shielding reduces the effective charge to ~1.69. Substituting this value of ''Z'' into the expression for ''H'' yields 729''E''<sub>1</sub>/128 = −77.5&nbsp;eV,\nwithin 2% of the experimental value, −78.975&nbsp;eV.<ref>G.W.F. Drake and Zong-Chao Van (1994). \"Variational eigenvalues for the S states of helium\", ''Chem. Phys. Lett.'' '''229''' 486–490.  [https://www.sciencedirect.com/science/article/abs/pii/0009261494010854]</ref> \n\nEven closer estimations of this energy have been found using more complicated trial wave functions with more parameters.  This is done in physical chemistry via [[variational Monte Carlo]].\n\n==References==\n{{Reflist|2}}\n\n[[Category:Quantum chemistry]]\n[[Category:Theoretical chemistry]]\n[[Category:Computational chemistry]]\n[[Category:Computational physics]]\n[[Category:Approximations]]"
    },
    {
      "title": "WKB approximation",
      "url": "https://en.wikipedia.org/wiki/WKB_approximation",
      "text": "{{Other uses|WKB (disambiguation)}}\nIn [[mathematical physics]], the '''WKB approximation''' or '''WKB method''' is a method for finding approximate solutions to linear differential equations with spatially varying coefficients.  It is typically used for a semiclassical calculation in [[quantum mechanics]] in which the wavefunction is recast as an exponential function, semiclassically expanded, and then either the amplitude or the phase is taken to be changing slowly.\n\nThe name is an initialism for '''Wentzel–Kramers–Brillouin'''.  It is also known as the '''LG''' or '''Liouville–Green method'''.  Other often-used letter combinations include '''JWKB''' and '''WKBJ''', where the \"J\" stands for Jeffreys.\n\n== Brief history ==\nThis method is named after physicists [[Gregor Wentzel]], [[Hendrik Anthony Kramers]], and [[Léon Brillouin]], who all developed it in 1926. In 1923, mathematician [[Harold Jeffreys]] had developed a general method of approximating solutions to linear, second-order differential equations, a class that includes the [[Schrödinger equation]].  The Schrödinger equation itself was not developed until two years later, and Wentzel, Kramers, and Brillouin were apparently unaware of this earlier work, so Jeffreys is often neglected credit. Early texts in quantum mechanics contain any number of combinations of their initials, including WBK, BWK, WKBJ, JWKB and BWKJ. An authoritative discussion and critical survey has been given by Robert B. Dingle.<ref>Robert Balson Dingle, Asymptotic Expansions: Their Derivation and Interpretation (Academic Press, 1973).</ref>\n\nEarlier appearances of essentially equivalent methods are: [[Francesco Carlini]] in 1817, [[Joseph Liouville]] in 1837, [[George Green (mathematician)|George Green]] in 1837, [[Lord Rayleigh]] in 1912 and [[Richard Gans]] in 1915. Liouville and Green may be said to have founded the method in 1837, and it is also commonly referred to as the Liouville–Green or LG method.<ref>{{cite book\n | title = Atmosphere-ocean dynamics\n | author = Adrian E. Gill\n | publisher = Academic Press\n | year = 1982\n | isbn = 978-0-12-283522-3\n | page = 297\n | url = https://books.google.com/?id=1WLNX_lfRp8C&pg=PA297&dq=Liouville-Green+WKBJ+WKB\n }}</ref><ref>\n{{cite book\n | chapter = A Survey on the Liouville–Green (WKB) approximation for linear difference equations of the second order\n |author1=Renato Spigler  |author2=Marco Vianello\n  |lastauthoramp=yes | title = Advances in difference equations: proceedings of the Second International Conference on Difference Equations : Veszprém, Hungary, August 7–11, 1995     \n |editor1=Saber Elaydi |editor2=I. Győri |editor3=G. E. Ladas | publisher = CRC Press\n | year = 1998\n | isbn = 978-90-5699-521-8\n | page = 567\n | chapter-url = https://books.google.com/?id=a36iXw5_VzcC&pg=PA567&dq=Liouville-Green+WKBJ+WKB+LG\n }}</ref>\n\nThe important contribution of Jeffreys, Wentzel, Kramers, and Brillouin to the method was the inclusion of the treatment of [[stationary point|turning points]], connecting the [[evanescent wave|evanescent]] and [[oscillation|oscillatory]] solutions at either side of the turning point. For example, this may occur in the Schrödinger equation, due to a [[potential energy]] hill.\n\n==WKB method==\n\nGenerally, WKB theory is a method for approximating the solution of a differential equation whose ''highest derivative is multiplied by a small parameter'' {{mvar|ε}}. The method of approximation is as follows.\n\nFor a differential equation\n:<math> \\varepsilon \\frac{d^ny}{dx^n} + a(x)\\frac{d^{n-1}y}{dx^{n-1}} + \\cdots + k(x)\\frac{dy}{dx} + m(x)y= 0,</math>\nassume a solution of the form of an [[asymptotic series]] expansion\n:<math> y(x) \\sim \\exp\\left[\\frac{1}{\\delta}\\sum_{n=0}^{\\infty}\\delta^nS_n(x)\\right]</math>\nin the limit {{math| ''δ'' → 0}}. The asymptotic scaling of {{mvar|δ}} in terms of {{mvar|ε}} will be determined by the equation – see the example below.\n\nSubstituting the above [[ansatz]] into the differential equation and cancelling out the exponential terms allows one to solve for an arbitrary number of terms {{math|''S<sub>n</sub>(x)''}} in the expansion.\n\nWKB theory is a special case of [[multiple scale analysis]].<ref>{{cite book\n | title = Acoustics: basic physics, theory and methods\n | first = Paul \n | last = Filippi\n | publisher = Academic Press\n | year = 1999\n | isbn = 978-0-12-256190-0\n | page = 171\n | url = https://books.google.com/?id=xHWiOMp63WsC&pg=PA171&dq=wkb+multi-scale&q=wkb%20multi-scale\n }}</ref><ref>\n{{Cite book \n | author1=Kevorkian, J. \n | author2=Cole, J. D. \n | title=Multiple scale and singular perturbation methods \n | year=1996 \n | publisher=Springer \n | isbn=0-387-94202-5 \n }}</ref><ref name=\":0\">{{cite book\n | first1=Carl M.\n | last1=Bender\n | authorlink1=Carl M. Bender\n | first2=Steven A.\n | last2=Orszag\n | authorlink2=Steven Orszag\n | title=Advanced mathematical methods for scientists and engineers\n | publisher=Springer\n | year=1999\n | isbn=0-387-98931-5\n | pages=549–568\n }}</ref>\n\n==An example==\nThis example comes from the text of [[Carl M. Bender]] and [[Steven Orszag]].<ref name=\":0\" /> Consider the second-order homogeneous linear differential equation\n:<math> \\epsilon^2 \\frac{d^2 y}{dx^2} = Q(x) y, </math>\nwhere <math>Q(x) \\neq 0</math>. Substituting\n:<math>y(x) = \\exp\\left[\\frac{1}{\\delta}\\sum_{n=0}^\\infty \\delta^nS_n(x)\\right]</math>\nresults in the equation\n:<math>\\epsilon^2\\left[\\frac{1}{\\delta^2}\\left(\\sum_{n=0}^\\infty \\delta^nS_n'\\right)^2 + \\frac{1}{\\delta}\\sum_{n=0}^{\\infty}\\delta^nS_n''\\right] = Q(x).</math>\n\nTo [[leading-order|leading order]] (assuming, for the moment, the series will be asymptotically consistent), the above can be approximated as\n:<math>\\frac{\\epsilon^2}{\\delta^2}S_0'^2 + \\frac{2\\epsilon^2}{\\delta}S_0'S_1' + \\frac{\\epsilon^2}{\\delta}S_0'' = Q(x).</math>\n\nIn the limit {{math|''δ'' → 0}}, the [[Method of dominant balance|dominant balance]] is given by\n:<math>\\frac{\\epsilon^2}{\\delta^2}S_0'^2 \\sim Q(x).</math>\n\nSo {{mvar|δ}} is proportional to ''ε''. Setting them equal and comparing powers yields\n:<math>\\epsilon^0: \\quad S_0'^2 = Q(x),</math>\nwhich can be recognized as the [[Eikonal equation]], with solution\n:<math>S_0(x) = \\pm \\int_{x_0}^x \\sqrt{Q(t)}\\,dt.</math>\n\nConsidering first-order powers of  {{mvar|ε}} fixes\n:<math>\\epsilon^1: \\quad 2S_0'S_1' + S_0'' = 0.</math>\nThis is the unidimensional [[transport equation]], having the solution\n:<math>S_1(x) = -\\frac{1}{4}\\ln Q(x) + k_1,</math>\nwhere {{math|''k''<sub>1</sub>}} is an arbitrary constant.\n\nWe now have a pair of approximations to the system (a pair, because   {{math|''S''<sub>0</sub>}} can take two signs); the first-order WKB-approximation will be a linear combination of the two:\n:<math>y(x) \\approx c_1Q^{-\\frac{1}{4}}(x)\\exp\\left[\\frac{1}{\\epsilon}\\int_{x_0}^x\\sqrt{Q(t)}\\,dt\\right] + c_2Q^{-\\frac{1}{4}}(x)\\exp\\left[-\\frac{1}{\\epsilon}\\int_{x_0}^x\\sqrt{Q(t)}\\,dt\\right].</math>\n\nHigher-order terms can be obtained by looking at equations for higher powers of {{mvar|δ}}.   Explicitly,\n:<math> 2S_0'S_n' + S''_{n-1} + \\sum_{j=1}^{n-1}S'_jS'_{n-j} = 0</math>\nfor {{mvar|n}} ≥ 2.\n\n=== Precision of the asymptotic series ===\nThe asymptotic series for {{math|''y(x)''}} is usually a [[divergent series]], whose general term {{math|''δ<sup>n</sup> S<sub>n</sub>(x)''}} starts to increase after a certain value {{math|''n''{{=}}''n''<sub>max</sub>}}. Therefore, the smallest error achieved by the WKB method is at best of the order of the last included term.\n\nFor the equation\n:<math> \\epsilon^2 \\frac{d^2 y}{dx^2} = Q(x) y, </math>\nwith {{math|''Q(x)''}} <0 an analytic function, the value <math>n_\\max</math> and the magnitude of the last term can be estimated as follows:<ref>{{cite journal| last=Winitzki |first=S. |year=2005 |arxiv=gr-qc/0510001 |title=Cosmological particle production and the precision of the WKB approximation |journal=Phys. Rev. D |volume=72 |issue=10 |pages=104011, 14&nbsp;pp |nopp=yes |doi=10.1103/PhysRevD.72.104011 |bibcode = 2005PhRvD..72j4011W }}</ref>\n:<math>n_\\max \\approx 2\\epsilon^{-1} \\left|  \\int_{x_0}^{x_{\\ast}} \\sqrt{-Q(z)}\\,dz \\right| , </math>\n:<math>\\delta^{n_\\max}S_{n_\\max}(x_0) \\approx \\sqrt{\\frac{2\\pi}{n_\\max}} \\exp[-n_\\max], </math>\nwhere <math>x_0</math> is the point at which <math>y(x_0)</math> needs to be evaluated and <math>x_{\\ast}</math> is the (complex) turning point where <math>Q(x_{\\ast})=0</math>, closest to <math>x=x_0</math>.\n\nThe number {{math|''n''<sub>max</sub>}}  can be interpreted as the number of oscillations between <math>x_0</math> and the closest turning point.\n\nIf <math>\\epsilon^{-1}Q(x)</math> is a slowly changing function,\n:<math>\\epsilon\\left| \\frac{dQ}{dx} \\right| \\ll Q^2 ,</math>\nthe number {{math|''n''<sub>max</sub>}}  will be large, and the minimum error of the asymptotic series will be exponentially small.\n\n==Application to the Schrödinger equation==\n[[File:WKB_approximation_example.svg|thumb|WKB approximation to the indicated potential. Vertical lines show the turning points]]\n[[File:WKB_approximation_to_probability_density.svg|thumb|Probability density for the approximate wave function. Vertical lines show the turning points]]\nThe above example may be applied specifically to the one-dimensional, time-independent [[Schrödinger equation]],\n:<math>-\\frac{\\hbar^2}{2m} \\frac{d^2}{dx^2} \\Psi(x) + V(x) \\Psi(x) = E \\Psi(x),</math>\nwhich can be rewritten as\n:<math>\\frac{d^2}{dx^2} \\Psi(x) = \\frac{2m}{\\hbar^2} \\left( V(x) - E \\right) \\Psi(x).</math>\n\n===Approximation away from the turning points===\nThe wavefunction can be rewritten as the exponential of another function {{mvar|Φ}} (which is closely related to the [[Action (physics)|action]]), which could be complex,\n:<math>\\Psi(x) = e^{\\Phi(x)},</math>\nso that\n:<math>\\Phi''(x) + \\left[\\Phi'(x)\\right]^2 = \\frac{2m}{\\hbar^2} \\left( V(x) - E \\right),</math>\nwhere  {{mvar|Φ}} ' indicates the derivative of  {{mvar|Φ}} with respect to ''x''. This derivative  {{mvar|Φ}} ' can be separated into real and imaginary parts by introducing the real functions ''A'' and ''B'',\n:<math>\\Phi'(x) = A(x) + i B(x).</math>\n\nThe amplitude of the wavefunction is then \n:<math>\\exp\\left[\\int_{x_0}^x A(x')\\,dx'\\right],</math>\nwhile the phase is\n:<math>\\int_{x_0}^x B(x')\\,dx'.</math>\n\nThe real and imaginary parts of the Schrödinger equation then become\n:<math>A'(x) + A(x)^2 - B(x)^2 = \\frac{2m}{\\hbar^2} \\left( V(x) - E \\right),</math>\n:<math>B'(x) + 2 A(x) B(x) = 0.</math>\n\nNext, the semiclassical approximation is used. This means that each function is expanded as a power series in {{mvar|ħ}}. From the above equations, it can be seen that the power series must start with at least an order of 1/{{mvar|ħ}} to satisfy the real part of the equation. In order to achieve a good classical limit, it is necessary to start with as high a power of Planck's constant {{mvar|ħ}} as possible:\n:<math>A(x) = \\frac{1}{\\hbar} \\sum_{n=0}^\\infty \\hbar^n A_n(x),</math>\n:<math>B(x) = \\frac{1}{\\hbar} \\sum_{n=0}^\\infty \\hbar^n B_n(x).</math>\n\nTo the zeroth order in this expansion, the conditions on ''A'' and ''B'' can be written,\n:<math>A_0(x)^2 - B_0(x)^2 = 2m \\left( V(x) - E \\right),</math>\n:<math>A_0(x) B_0(x) = 0 \\;.</math>\n\nThe first derivatives {{math|''A'(x)''}} and {{math|'' B'(x)''}} were discarded, because they include factors of order 1/{{mvar|ħ}}, higher than  the dominant {{mvar|ħ}}<sup>−2</sup>.\n\nThen, if the amplitude varies sufficiently slowly as compared to the phase (<math>A_0(x) = 0</math>), it follows that\n:<math>B_0(x) = \\pm \\sqrt{ 2m \\left( E - V(x) \\right) },</math>\nwhich is only valid when the total energy is greater than the potential energy, as is always the case in [[Classical mechanics|classical motion]].\n\nAfter the same procedure on the next order of the expansion, it follows that\n<div id=\"mass_in_exponent\">\n:<math>\\Psi(x) \\approx C_0 \\frac{ e^{\\theta + i \\int \\hbar^{-1}\\sqrt{2m \\left( E - V(x) \\right)}\\,dx} }{\\hbar^{-1/2}\\sqrt[4]{2m \\left( E - V(x) \\right)}}.</math>\n</div>\n\nOn the other hand, if it is the phase that varies slowly (as compared to the amplitude), (<math>B_0(x) = 0</math>) then\n:<math>A_0(x) = \\pm \\sqrt{ 2m \\left( V(x) - E \\right) },</math>\nwhich is only valid when the potential energy is greater than the total energy (the regime in which [[quantum tunneling]] occurs).\n\nFinding the next order of the expansion yields, as in the example of the previous section,<ref>{{harvnb|Hall|2013}} Section 15.4</ref>\n{{Equation box 1\n|indent =:\n|equation =  <math>\\Psi(x) \\approx \\frac{ C_{+} e^{\\int \\hbar^{-1}\\sqrt{2m \\left( V(x) - E \\right)}\\,dx} \n+ C_{-} e^{-\\int \\hbar^{-1}\\sqrt{2m \\left( V(x) - E \\right)}\\,dx}}{\\hbar^{-1/2}\\sqrt[4]{2m \\left( V(x) - E \\right)}}.</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|bgcolor=#F9FFF7}}\n\nIn the classically allowed region, namely the region where <math>V(x)<E</math> the integrand in the exponent is imaginary and the approximate wave function is oscillatory. In the classically forbidden region <math>V(x)>E</math>, the solutions are growing or decaying. It is evident in the denominator that both of these approximate solutions become singular near the classical '''turning points''', where {{math|''E'' {{=}} ''V(x)''}}, and cannot be valid. (The turning points are the points where the classical particle changes direction.)\n\n===Behavior near the turning points===\nWe now consider the behavior of the wave function near the turning points. For this, we need a different method. Near the first turning points, {{mvar|x}}<sub>1</sub>, the term <math>\\frac{2m}{\\hbar^2}\\left(V(x)-E\\right)</math> can be expanded in a power series,\n:<math>\\frac{2m}{\\hbar^2}\\left(V(x)-E\\right) = U_1 \\cdot (x - x_1) + U_2 \\cdot (x - x_1)^2 + \\cdots\\;.</math>\n\nTo first order, one finds\n:<math>\\frac{d^2}{dx^2} \\Psi(x) = U_1 \\cdot (x - x_1) \\cdot \\Psi(x).</math>\nThis differential equation is known as the [[Airy equation]], and the solution may be written in terms of [[Airy function]]s,<ref>{{harvnb|Hall|2013}} Section 15.5</ref>\n:<math>\\Psi(x) = C_A \\textrm{Ai}\\left( \\sqrt[3]{U_1} \\cdot (x - x_1) \\right) + C_B \\textrm{Bi}\\left( \\sqrt[3]{U_1} \\cdot (x - x_1) \\right).</math>\n\nAlthough for any fixed value of <math>\\hbar</math>, the wave function is bounded near the turning points, the wave function will be peaked there, as can be seen in the images above. As <math>\\hbar</math> gets smaller, the height of the wave function at the turning points grows.\n\n===The matching conditions===\nIt now remains to construct a global (approximate) solution to the Schrödinger equation. For the wave function to be square-integrable, we must take only the exponentially decaying solution in the two classically forbidden regions. These must then \"connect\" properly through the turning points to the classically allowed region. For most values of ''E'', this matching procedure will not work: The function obtained by connecting the solution near <math>+\\infty</math> to the classically allowed region will not agree with the function obtained by connecting the solution near <math>-\\infty</math> to the classically allowed region. The requirement that the two functions agree imposes a condition on the energy ''E'', which will give an approximation to the exact quantum energy levels.\n\nGiven the two coefficients on one side of the classical turning point, the 2 coefficients on the other side of the classical turning point can be determined by using the Airy function to connect them. Thus, a relationship between <math>C_0,\\theta</math> and <math>C_{+},C_{-}</math> can be found. This relationship is obtained using known asymptotic of the Airy function. The relationship can be found to be as follows (often referred to as the \"connection formulas\"):<ref>{{harvnb|Hall|2013}} Claim 15.7</ref>\n:<math>\n    C_{+} = + \\frac{1}{2} C_0 \\cos{\\left(\\theta - \\frac{\\pi}{4}\\right)},\n</math>\n:<math>\n    C_{-} = - \\frac{1}{2} C_0 \\sin{\\left(\\theta - \\frac{\\pi}{4}\\right)}.\n</math>\nNow the global (approximate) solutions can be constructed. The same can be done at the other turning points; assume there is just another one,  {{mvar|x}}<sub>2</sub>. The expression there, however,  will appear different than the one determined above at  {{mvar|x}}<sub>1</sub> by a difference in the argument of these trigonometric functions.\n\nThe matching condition, needed to get a single-valued, square-integrable approximate solution, takes the following form:\n::<math>\\int_{x_1}^{x_2} \\sqrt{2m \\left( E-V(x)\\right)}\\,dx = (n+1/2)\\pi \\hbar ,</math>\nwhere <math>x_1,x_2</math> are the turning points of the potential discussed, where the integrand vanishes. Here ''n'' is a non-negative integer. This condition can also be rewritten as saying that\n::The area enclosed by the classical energy curve is <math>2\\pi\\hbar(n+1/2)</math>.\nEither way, the condition on the energy is a version of the [[Bohr-Sommerfeld quantization]] condition, with a \"[[Lagrangian Grassmannian#Maslov index|Maslov correction]]\" equal to 1/2.<ref>{{harvnb|Hall|2013}} Section 15.2</ref>\n\nIt is possible to show that after piecing together the approximations in the various regions, one obtains a good approximation to the actual eigenfunction. In particular, the Maslov-corrected Bohr-Sommerfeld energies are good approximations to the actual eigenvalues of the Schrödinger operator.<ref>{{harvnb|Hall|2013}} Theorem 15.8</ref> Specifically, the error in the energies is small compared to the typical spacing of the quantum energy levels. Thus, although the \"old quantum theory\" of Bohr and Sommerfeld was ultimately replaced by the Schrödinger equation, some vestige of that theory remains, as an approximation to the eigenvalues of the appropriate Schrödinger operator.\n\n===The probability density===\nOne can then compute the probability density associated to the approximate wave function. The probability that the quantum particle will be found in the classically forbidden region is small. In the classically allowed region, meanwhile, the  probability the quantum particle will be found in a given interval is approximately the ''fraction of time the classical particle spends in that interval'' over one period of motion.<ref>{{harvnb|Hall|2013}} Conclusion 15.5</ref> Since the classical particle's velocity goes to zero at the turning points, it spends more time near the turning points than in other classically allowed regions. This observation accounts for the peak in the wave function (and its probability density) near the turning points.\n\nApplications of the WKB method to Schrödinger equations with a large variety of potentials and comparison with perturbation methods and path integrals are treated in  Müller-Kirsten.<ref>Harald J.W. Müller-Kirsten, Introduction to Quantum Mechanics: Schrödinger Equation and Path Integral, 2nd ed. (World Scientific, 2012).</ref>\n\n==See also==\n{{colbegin}}\n* [[Instanton]]\n* [[Airy function]]\n* [[Field electron emission]]\n* [[Langer correction]]\n* [[Maslov index]]\n* [[Method of steepest descent]] / Laplace Method\n* [[Method of matched asymptotic expansions]]\n* [[Old quantum theory]]\n* [[Perturbation methods]]\n* [[Quantum tunneling]]\n* [[Slowly varying envelope approximation]]\n* [[Supersymmetric WKB approximation]]\n{{colend}}\n\n==References==\n{{Reflist}}\n\n===Modern references===\n*{{cite book | author=[[Carl M. Bender|Bender, Carl]]; [[Steven A. Orszag|Orszag, Steven]] | title=Advanced Mathematical Methods for Scientists and Engineers | publisher=McGraw-Hill | year=1978 | isbn=0-07-004452-X}}\n*{{cite book | author=Child, M. S. | title=Semiclassical mechanics with molecular applications | year=1991 | publisher=Clarendon Press | location=Oxford | isbn=0-19-855654-3}}\n*{{cite book | author=Griffiths, David J. | title=Introduction to Quantum Mechanics (2nd ed.) | publisher=Prentice Hall |year=2004 |isbn=0-13-111892-7}}\n*{{citation|first=Brian C.|last=Hall|title=Quantum Theory for Mathematicians|series=Graduate Texts in Mathematics|volume=267 |publisher=Springer|year=2013| isbn=978-1461471158}}\n*{{cite book | author=Liboff, Richard L. | title=Introductory Quantum Mechanics (4th ed.) | publisher=Addison-Wesley |year=2003 |isbn=0-8053-8714-5| author-link=Liboff, Richard L }}\n*{{cite book | author=Olver, Frank William John |author-link=Frank William John Olver | title=Asymptotics and Special Functions | publisher=Academic Press | year=1974 | isbn=0-12-525850-X}}\n*{{cite book | author=Razavy, Mohsen | title=Quantum Theory of Tunneling | publisher=World Scientific | year=2003 | isbn=981-238-019-1}}\n*{{cite book | author=Sakurai, J. J. | title=Modern Quantum Mechanics | publisher=Addison-Wesley |year=1993 |isbn=0-201-53929-2}}\n\n===Historical references===\n*{{cite book | author=Carlini, Francesco | year=1817 | title=Ricerche sulla convergenza della serie che serva alla soluzione del problema di Keplero | publisher=Milano | author-link=Francesco Carlini }}\n*{{cite journal | author=Liouville, Joseph | year=1837 | title=Sur le développement des fonctions et séries..| journal=Journal de Mathématiques Pures et Appliquées | volume=1 | pages=16–35 | author-link=Joseph Liouville }}\n*{{cite journal | author=Green, George | year=1837 | title=On the motion of waves in a variable canal of small depth and width | journal=Transactions of the Cambridge Philosophical Society | volume=6 | pages=457–462 | author-link=George Green (mathematician) }}\n*{{cite journal | author=Rayleigh, Lord (John William Strutt) | year=1912 | title=On the propagation of waves through a stratified medium, with special reference to the question of reflection | journal=[[Proceedings of the Royal Society A]] | volume=86 | pages=207–226 | doi=10.1098/rspa.1912.0014 |bibcode = 1912RSPSA..86..207R | issue=586 | author-link=Lord Rayleigh }}\n*{{cite journal | author=Gans, Richard | year=1915 | title=Fortplantzung des Lichts durch ein inhomogenes Medium | journal=Annalen der Physik | volume=47 | issue=14 | pages=709–736 | doi = 10.1002/andp.19153521402 |bibcode = 1915AnP...352..709G | author-link=Richard Gans }}\n*{{cite journal | author=Jeffreys, Harold | year=1924 | title=On certain approximate solutions of linear differential equations of the second order | journal=Proceedings of the London Mathematical Society | volume=23 | pages=428–436 | doi=10.1112/plms/s2-23.1.428 | author-link=Harold Jeffreys }}\n*{{cite journal | author=Brillouin, Léon | year=1926 | title=La mécanique ondulatoire de Schrödinger: une méthode générale de resolution par approximations successives | journal=Comptes Rendus de l'Académie des Sciences | volume=183 | pages=24–26 | author-link=Léon Brillouin }}\n*{{cite journal | author=Kramers, Hendrik A. | year=1926 | title=Wellenmechanik und halbzahlige Quantisierung | journal=Zeitschrift für Physik | volume=39 |pages=828–840 | doi=10.1007/BF01451751 |bibcode = 1926ZPhy...39..828K | issue=10–11 | author-link=Hendrik Anthony Kramers }}\n*{{cite journal | author=Wentzel, Gregor | year=1926 | title=Eine Verallgemeinerung der Quantenbedingungen für die Zwecke der Wellenmechanik | journal=Zeitschrift für Physik | volume=38 | pages=518–529 | doi=10.1007/BF01397171 |bibcode = 1926ZPhy...38..518W | issue=6–7 | author-link=Gregor Wentzel }}\n\n==External links==\n* {{cite web| first=Richard |last=Fitzpatrick|url=http://farside.ph.utexas.edu/teaching/jk1/lectures/node70.html|title= The W.K.B. Approximation|year=2002}} (An application of the WKB approximation to the scattering of radio waves from the ionosphere.)\n\n[[Category:Approximations]]\n[[Category:Theoretical physics]]\n[[Category:Asymptotic analysis]]\n[[Category:Mathematical physics]]"
    },
    {
      "title": "Approximation algorithm",
      "url": "https://en.wikipedia.org/wiki/Approximation_algorithm",
      "text": "In [[computer science]] and [[operations research]], '''approximation algorithms''' are [[Time complexity#Polynomial time|efficient]] [[algorithm]]s that find approximate solutions to [[NP-hardness|NP-hard]] [[optimization problem]]s with '''provable guarantees''' on the distance of the returned solution to the optimal one.<ref name=\"Bernard. 2011\">{{Cite book|title=The design of approximation algorithms|last=Bernard.|first=Shmoys, David|date=2011|publisher=Cambridge University Press|isbn=9780521195270|oclc=671709856}}</ref> Approximation algorithms naturally arise in the field of [[theoretical computer science]] as a consequence of the widely believed [[P versus NP problem|P ≠ NP]] conjecture. Under this conjecture, a wide class of optimization problems cannot be solved exactly in [[Time complexity|polynomial time]]. The field of approximation algorithms, therefore, tries to understand how closely it is possible to approximate optimal solutions to such problems in polynomial time. In an overwhelming majority of the cases, the guarantee of such algorithms is a multiplicative one expressed as an approximation ratio or approximation factor i.e., the optimal solution is always guaranteed to be within a (predetermined) multiplicative factor of the returned solution. However, there are also many approximation algorithms that provide an additive guarantee on the quality of the returned solution. A notable example of an approximation algorithm that provides ''both'' is the classic approximation algorithm of [[Jan Karel Lenstra|Lenstra]], [[David Shmoys|Shmoys]] and [[Éva Tardos|Tardos]]<ref>{{Cite journal|last=Lenstra|first=Jan Karel|last2=Shmoys|first2=David B.|last3=Tardos|first3=Éva|date=1990-01-01|title=Approximation algorithms for scheduling unrelated parallel machines|journal=Mathematical Programming|language=en|volume=46|issue=1–3|pages=259–271|doi=10.1007/BF01585745|issn=0025-5610|citeseerx=10.1.1.115.708}}</ref> for [[Scheduling on Unrelated Parallel Machines]].\n\nThe design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case.<ref name=\"Bernard. 2011\"/> This distinguishes them from [[heuristic (computer science)|heuristics]] such as [[Simulated annealing|annealing]] or [[genetic algorithm]]s, which find reasonably good solutions on some inputs, but provide no clear indication at the outset on when they may succeed or fail.\n\nThere is widespread interest in theoretical computer science to better understand the limits to which we can approximate certain famous optimization problems. For example, one of the long-standing open questions in computer science is to determine whether there is an algorithm that outperforms the [[Christofides algorithm|1.5 approximation algorithm]] of Christofides to the [[Travelling salesman problem|Metric Traveling Salesman Problem]]. The desire to understand hard optimization problems from the perspective of approximability is motivated by the discovery of surprising mathematical connections and broadly applicable techniques to design algorithms for hard optimization problems. One well-known example of the former is the [[Goemans-Williamson algorithm]] for [[Maximum cut|Maximum Cut]] which solves a graph theoretic problem using high dimensional geometry.<ref>{{Cite journal|last=Goemans|first=Michel X.|last2=Williamson|first2=David P.|date=November 1995|title=Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming|journal=J. ACM|volume=42|issue=6|pages=1115–1145|doi=10.1145/227683.227684|issn=0004-5411|citeseerx=10.1.1.34.8500}}</ref>\n\n== Introduction ==\nA simple example of an approximation algorithm is one for the [[Minimum vertex cover|Minimum Vertex Cover]] problem, where the goal is to choose the smallest set of vertices such that every edge in the input graph contains at least one chosen vertex. One way to find a vertex cover is to repeat the following process: find an uncovered edge, add both its endpoints to the cover, and remove all edges incident to either vertex from the graph. As any vertex cover of the input graph must use a distinct vertex to cover each edge that was considered in the process (since it forms a [[Matching (graph theory)|matching]]), the vertex cover produced, therefore, is at most twice as large as the optimal one. In other words, this is a [[constant factor approximation algorithm]] with an approximation factor of 2. Under the recent [[Unique games conjecture|Unique Games Conjecture]], this factor is even the best possible one.<ref>{{Cite journal|last=Khot|first=Subhash|last2=Regev|first2=Oded|date=2008-05-01|title=Vertex cover might be hard to approximate to within 2−ε|url=http://www.sciencedirect.com/science/article/pii/S0022000007000864|journal=Journal of Computer and System Sciences|series=Computational Complexity 2003|volume=74|issue=3|pages=335–349|doi=10.1016/j.jcss.2007.06.019}}</ref>\n\nNP-hard problems vary greatly in their approximability; some, such as the [[Knapsack Problem]], can be approximated within a multiplicative factor <math>1 + \\epsilon</math>, for any fixed <math>\\epsilon > 0</math>, and therefore produce solutions arbitrarily close to the optimum (such a family of approximation algorithms is called a [[polynomial time approximation scheme]] or PTAS). Others are impossible to approximate within any constant, or even polynomial, factor unless [[P = NP]], as in the case of the [[maximum clique problem|Maximum Clique Problem]]. Therefore, an important benefit of studying approximation algorithms is a fine-grained classification of the difficulty of various NP-hard problems beyond the one afforded by the [[NP-completeness|theory of NP-completeness]]. In other words, although NP-complete problems may be equivalent (under polynomial time reductions) to each other from the perspective of exact solutions, the corresponding optimization problems behave very differently from the perspective of approximate solutions.\n\n==Algorithm design techniques==\nBy now there are several established techniques to design approximation algorithms. These include the following ones.\n# [[Greedy algorithm]]\n# [[Local search (optimization)|Local search]]\n# Enumeration and [[dynamic programming]]\n# Solving a [[convex programming]] relaxation to get a fractional solution. Then converting this fractional solution into a feasible solution by some appropriate rounding. The popular relaxations include the following.\n#* [[Linear programming]] relaxations\n#* [[Semidefinite programming]] relaxations\n# Primal-Dual Methods\n# Dual Fitting\n# Embedding the problem in some metric and then solving the problem on the metric. This is also known as metric embedding.\n# Random sampling and the use of randomness in general in conjunction with the methods above.\n\n== A Posteriori Guarantees ==\nWhile approximation algorithms always provide an a priori worst case guarantee (be it additive or multiplicative), in some cases they also provide an a posteriori guarantee that is often much better. This is often the case for algorithms that work by solving a [[Convex programming|convex relaxation]] of the optimization problem on the given input. For example, there is a different approximation algorithm for Minimum Vertex Cover that solves a [[linear programming relaxation]] to find a vertex cover that is at most twice the value of the relaxation. Since the value of the relaxation is never larger than the size of the optimal vertex cover, this yields another 2-approximation algorithm. While this is similar to the a priori guarantee of the previous approximation algorithm, the guarantee of the latter can be much better (indeed when the value of the LP relaxation is far from the size of the optimal vertex cover).\n\n== Hardness of Approximation ==\nApproximation algorithms as a research area is closely related to and informed by [[Hardness of approximation|inapproximability theory]] where the non-existence of efficient algorithms with certain approximation ratios is proved (conditioned on widely believed hypotheses such as the P ≠ NP conjecture) by means of [[Reduction (complexity)|reductions]]. In the case of the Metric Traveling Salesman Problem, the best known inapproximability result rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196 unless P = NP.<ref>{{Cite journal|last=Karpinski|first=Marek|last2=Lampis|first2=Michael|last3=Schmied|first3=Richard|date=2015-12-01|title=New inapproximability bounds for TSP|url=http://www.sciencedirect.com/science/article/pii/S0022000015000641|journal=Journal of Computer and System Sciences|volume=81|issue=8|pages=1665–1677|doi=10.1016/j.jcss.2015.06.003|arxiv=1303.6437}}</ref> Coupled with the knowledge of the existence of Christofides' 1.5 approximation algorithm, this tells us that the [[threshold of approximability]] for Metric Traveling Salesman (if it exists) is somewhere between 123/122 and 1.5.\n\nWhile inapproximability results have been proved since the 1970s, such results were obtained by ad-hoc means and no systematic understanding was available at the time. It is only since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of [[Independent set (graph theory)|Independent Set]]<ref>{{Cite journal|last=Feige|first=Uriel|last2=Goldwasser|first2=Shafi|last3=Lovász|first3=Laszlo|last4=Safra|first4=Shmuel|last5=Szegedy|first5=Mario|date=March 1996|title=Interactive Proofs and the Hardness of Approximating Cliques|journal=J. ACM|volume=43|issue=2|pages=268–292|doi=10.1145/226643.226652|issn=0004-5411}}</ref> and the famous [[PCP theorem]],<ref>{{Cite journal|last=Arora|first=Sanjeev|last2=Safra|first2=Shmuel|date=January 1998|title=Probabilistic Checking of Proofs: A New Characterization of NP|journal=J. ACM|volume=45|issue=1|pages=70–122|doi=10.1145/273865.273901|issn=0004-5411}}</ref> that modern tools for proving inapproximability results were uncovered. The PCP theorem, for example, shows that [[David S. Johnson|Johnson's]] 1974 approximation algorithms for [[Maximum satisfiability problem|Max SAT]], [[Set cover problem|Set Cover]], [[Independent set (graph theory)|Independent Set]] and [[Graph coloring|Coloring]] all achieve the optimal approximation ratio, assuming P ≠ NP.<ref>{{Cite journal|last=Johnson|first=David S.|date=1974-12-01|title=Approximation algorithms for combinatorial problems|url=http://www.sciencedirect.com/science/article/pii/S0022000074800449|journal=Journal of Computer and System Sciences|volume=9|issue=3|pages=256–278|doi=10.1016/S0022-0000(74)80044-9}}</ref>\n\n== Practicality ==\nNot all approximation algorithms are suitable for direct practical applications. Some involve solving non-trivial [[Linear programming relaxation|linear programming]]/[[Semidefinite programming|semidefinite]] relaxations (which may themselves invoke the [[Ellipsoid method|ellipsoid algorithm]]), complex data structures, or sophisticated algorithmic techniques, leading to difficult implementation issues or improved running time performance (over exact algorithms) only on impractically large inputs. Implementation and running time issues aside, the guarantees provided by approximation algorithms may themselves not be strong enough to justify their consideration in practice. Despite their inability to be used \"out of the box\" in practical applications, the ideas and insights behind the design of such algorithms can often be incorporated in other ways in practical algorithms. In this way, the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights.\n\nIn other cases, even if the initial results are of purely theoretical interest, over time, with an improved understanding, the algorithms may be refined to become more practical. One such example is the initial PTAS for [[Euclidean traveling salesman problem|Euclidean TSP]] by [[Sanjeev Arora]] (and independently by [[Joseph S. B. Mitchell|Joseph Mitchell]]) which had a prohibitive running time of <math>n^{O(1/\\epsilon)}</math> for a <math>1+\\epsilon</math> approximation.<ref>{{Cite book|last=Arora|first=S.|date=October 1996|title=Polynomial time approximation schemes for Euclidean TSP and other geometric problems|url=http://ieeexplore.ieee.org:80/document/548458/?reload=true|journal=Proceedings of 37th Conference on Foundations of Computer Science|pages=2–11|doi=10.1109/SFCS.1996.548458|isbn=978-0-8186-7594-2|citeseerx=10.1.1.32.3376}}</ref> Yet, within a year these ideas were incorporated into a near-linear time <math>O(n\\log n)</math> algorithm for any constant <math>\\epsilon > 0</math>.<ref>{{Cite book|last=Arora|first=S.|date=October 1997|title=Nearly linear time approximation schemes for Euclidean TSP and other geometric problems|url=http://ieeexplore.ieee.org:80/document/646145/?reload=true|journal=Proceedings 38th Annual Symposium on Foundations of Computer Science|pages=554–563|doi=10.1109/SFCS.1997.646145|isbn=978-0-8186-8197-4}}</ref>\n\n== Performance guarantees ==\nFor some approximation algorithms it is possible to prove certain properties about the approximation of the optimum result. For example, a '''''ρ''-approximation algorithm''' ''A'' is defined to be an algorithm for which it has been proven that the value/cost, ''f''(''x''), of the approximate solution ''A''(''x'') to an instance ''x'' will not be more (or less, depending on the situation) than a factor ''ρ'' times the value, OPT, of an optimum solution.\n\n:<math>\\begin{cases}\\mathrm{OPT} \\leq f(x) \\leq \\rho \\mathrm{OPT},\\qquad\\mbox{if } \\rho > 1; \\\\ \\rho \\mathrm{OPT} \\leq f(x) \\leq \\mathrm{OPT},\\qquad\\mbox{if } \\rho < 1.\\end{cases}</math>\n\nThe factor ''ρ'' is called the ''relative performance guarantee''. An approximation algorithm has an ''absolute performance guarantee'' or ''bounded error'' ''c'', if it has been proven for every instance ''x'' that\n\n:<math> (\\mathrm{OPT} - c) \\leq f(x) \\leq (\\mathrm{OPT} + c).</math>\n\nSimilarly, the ''performance guarantee'', ''R''(''x,y''), of a solution ''y'' to an instance ''x'' is defined as\n\n:<math>R(x,y) =  \\max \\left ( \\frac{OPT}{f(y)}, \\frac{f(y)}{OPT} \\right ),</math>\n\nwhere ''f''(''y'') is the value/cost of the solution ''y'' for the instance ''x''. Clearly, the performance guarantee is greater than or equal to 1 and equal to 1 if and only if ''y'' is an optimal solution. If an algorithm ''A'' guarantees to return solutions with a performance guarantee of at most ''r''(''n''), then ''A'' is said to be an ''r''(''n'')-approximation algorithm and has an ''approximation ratio'' of ''r''(''n''). Likewise, a problem with an ''r''(''n'')-approximation algorithm is said to be r''(''n'')''-''approximable'' or have an approximation ratio of ''r''(''n'').<ref name=ausiello99complexity>{{cite book|title=Complexity and Approximation: Combinatorial Optimization Problems and their Approximability Properties|year=1999|author1=G. Ausiello |author2=P. Crescenzi |author3=G. Gambosi |author4=V. Kann |author5=A. Marchetti-Spaccamela |author6=M. Protasi }}</ref><ref name=\"kann92onthe\">{{cite book|title=On the Approximability of NP-complete Optimization Problems|author=Viggo Kann|year=1992|url=http://www.csc.kth.se/~viggo/papers/phdthesis.pdf}}</ref>\n\nOne may note that for minimization problems, the two different guarantees provide the same result and that for maximization problems, a relative performance guarantee of ρ is equivalent to a performance guarantee of <math>r = \\rho^{-1}</math>. In the literature, both definitions are common but it is clear which definition is used since, for maximization problems, as ρ ≤ 1 while r ≥ 1.\n\nThe ''absolute performance guarantee'' <math>\\Rho_A</math> of some approximation algorithm ''A'', where ''x'' refers to an instance of a problem, and where <math>R_A(x)</math> is the performance guarantee of ''A'' on ''x'' (i.e. ρ for problem instance ''x'') is:\n\n:<math> \\Rho_A = \\inf \\{ r \\geq 1 \\mid R_A(x) \\leq r, \\forall x \\}.</math>\n\nThat is to say that <math>\\Rho_A</math> is the largest bound on the approximation ratio, ''r'', that one sees over all possible instances of the problem. Likewise, the ''asymptotic performance ratio'' <math>R_A^\\infty</math> is:\n\n:<math> R_A^\\infty = \\inf \\{ r \\geq 1 \\mid \\exists n \\in \\mathbb{Z}^+, R_A(x) \\leq r, \\forall x, |x| \\geq n\\}. </math>\n\nThat is to say that it is the same as the ''absolute performance ratio'', with a lower bound ''n'' on the size of problem instances. These two types of ratios are used because there exist algorithms where the difference between these two is significant.\n\n{| class=\"wikitable\"\n|+Performance guarantees\n|-\n!  !! ''r''-approx<ref name=\"ausiello99complexity\"/><ref name=\"kann92onthe\"/> !! ''ρ''-approx !! rel. error<ref name=\"kann92onthe\"/> !! rel. error<ref name=\"ausiello99complexity\"/> !! norm. rel. error<ref name=\"ausiello99complexity\"/><ref name=\"kann92onthe\"/> !! abs. error<ref name=\"ausiello99complexity\"/><ref name=\"kann92onthe\"/>\n|-\n! max: <math>f(x) \\geq</math>\n| <math>r^{-1} \\mathrm{OPT}</math> || <math>\\rho \\mathrm{OPT}</math> || <math>(1-c)\\mathrm{OPT}</math> || <math>(1-c)\\mathrm{OPT}</math> || <math>(1-c)\\mathrm{OPT} + c\\mathrm{WORST}</math> || <math>\\mathrm{OPT} - c</math>\n|-\n! min: <math>f(x) \\leq</math>\n| <math>r \\mathrm{OPT}</math> || <math>\\rho \\mathrm{OPT}</math> || <math>(1+c)\\mathrm{OPT}</math> || <math>(1-c)^{-1}\\mathrm{OPT}</math> || <math>(1-c)^{-1} \\mathrm{OPT} + c\\mathrm{WORST}</math> || <math>\\mathrm{OPT} + c</math>\n|-\n|}\n\n== Epsilon terms ==\nIn the literature, an approximation ratio for a maximization (minimization) problem of ''c'' - ϵ (min: ''c'' + ϵ) means that the algorithm has an approximation ratio of ''c'' ∓ ϵ  for arbitrary ϵ > 0 but that the ratio has not (or cannot) be shown for ϵ = 0. An example of this is the optimal inapproximability — inexistence of approximation — ratio of 7 / 8 + ϵ for satisfiable [[MAX-3SAT]] instances due to [[Johan Håstad]].<ref name=\"hastad99someoptimal\">{{cite journal|title=Some Optimal Inapproximability Results|journal=Journal of the ACM|volume=48|issue=4|pages=798–859|year=1999|url=http://www.nada.kth.se/~johanh/optimalinap.ps|author=[[Johan Håstad]]|doi=10.1145/502090.502098|citeseerx=10.1.1.638.2808}}</ref> As mentioned previously, when ''c'' = 1, the problem is said to have a [[polynomial-time approximation scheme]].\n\nAn ϵ-term may appear when an approximation algorithm introduces a multiplicative error and a constant error while the minimum optimum of instances of size ''n'' goes to infinity as ''n'' does. In this case, the approximation ratio is ''c'' ∓ ''k'' / OPT = ''c'' ∓ o(1) for some constants ''c'' and ''k''. Given arbitrary ϵ > 0, one can choose a large enough ''N'' such that the term ''k'' / OPT < ϵ for every ''n ≥ N''. For every fixed ϵ, instances of size ''n < N'' can be solved by brute force , thereby showing an approximation ratio — existence of approximation algorithms with a guarantee — of ''c'' ∓ ϵ for every ϵ > 0.\n\n== See also ==\n* [[Domination analysis]] considers guarantees in terms of the rank of the computed solution.\n* [[Polynomial-time approximation scheme|PTAS]] - a type of approximation algorithm that takes the approximation ratio as a parameter\n* [[APX]] is the class of problems with some constant-factor approximation algorithm\n* [[Approximation-preserving reduction]]\n* [[Exact algorithm]]\n\n==Citations==\n{{More footnotes|date=April 2009}}\n{{reflist}}\n\n==References==\n* {{cite book\n  | last = Vazirani\n  | first = Vijay V.\n  | authorlink = Vijay Vazirani\n  | title = Approximation Algorithms\n  | publisher = Springer\n  | year = 2003\n  | location = Berlin\n  | isbn = 978-3-540-65367-7 }}\n* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{ISBN|0-262-03293-7}}. Chapter 35: Approximation Algorithms, pp.&nbsp;1022&ndash;1056.\n* [[Dorit S. Hochbaum]], ed. ''[[Approximation Algorithms for NP-Hard problems]]'', PWS Publishing Company, 1997. {{ISBN|0-534-94968-1}}. Chapter 9: Various Notions of Approximations: Good, Better, Best, and More\n*{{Citation|last1=Williamson|first1=David P.|author1-link=David P. Williamson|last2=Shmoys|first2=David B.|authorlink2=David Shmoys|date=April 26, 2011|title=The Design of Approximation Algorithms|location=|publisher=[[Cambridge University Press]]|isbn=978-0521195270}}\n\n==External links==\n*Pierluigi Crescenzi, Viggo Kann, Magnús Halldórsson, [[Marek Karpinski]] and [[Gerhard J. Woeginger|Gerhard Woeginger]], [http://www.nada.kth.se/~viggo/wwwcompendium/ ''A compendium of NP optimization problems''].\n\n{{optimization algorithms|combinatorial|state=expanded}}\n\n{{Authority control}}\n\n[[Category:Computational complexity theory]]\n[[Category:Approximation algorithms| ]]"
    },
    {
      "title": "Submodular set function",
      "url": "https://en.wikipedia.org/wiki/Submodular_set_function",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Function on subsets that has a diminishing returns property}}\nIn mathematics, a '''submodular set function''' (also known as a '''submodular function''') is a [[set function]] whose value, informally, has the property that the difference in the incremental value of the function that a single element makes when added to an input set decreases as the size of the input set increases. Submodular functions have a natural [[diminishing returns]] property which makes them suitable for many applications, including [[approximation algorithms]], [[game theory]] (as functions modeling user preferences) and [[electrical network]]s. Recently, submodular functions have also found immense utility in several real world problems in [[machine learning]] and [[artificial intelligence]], including [[automatic summarization]], [[multi-document summarization]], [[feature selection]], [[Active learning (machine learning)|active learning]], sensor placement, image collection summarization and many other domains.<ref name=\"LB\" /><ref name=\"TIWB\" /><ref name=\"KG1\" /><ref name=\"KG\" />\n\n== Definition ==\nIf <math>\\Omega</math> is a finite [[set (mathematics)|set]], a submodular function is a set function <math>f:2^{\\Omega}\\rightarrow \\mathbb{R}</math>, where <math>2^\\Omega</math> denotes the [[Power set#Representing subsets as functions|power set]] of <math>\\Omega</math>, which satisfies one of the following equivalent conditions.<ref>{{Harvard citations|last = Schrijver|year = 2003|loc = §44, p. 766|nb = }}</ref>\n# For every <math>X, Y \\subseteq \\Omega</math> with <math> X \\subseteq Y</math> and every <math>x \\in \\Omega \\setminus Y</math> we have that <math>f(X\\cup \\{x\\})-f(X)\\geq f(Y\\cup \\{x\\})-f(Y)</math>.\n# For every <math>S, T \\subseteq \\Omega</math> we have that <math>f(S)+f(T)\\geq f(S\\cup T)+f(S\\cap T)</math>.\n# For every <math>X\\subseteq \\Omega</math> and <math>x_1,x_2\\in \\Omega\\backslash X</math> such that <math>x_1\\neq x_2</math> we have that <math>f(X\\cup \\{x_1\\})+f(X\\cup \\{x_2\\})\\geq f(X\\cup \\{x_1,x_2\\})+f(X)</math>.\n\nA nonnegative submodular function is also a [[Subadditive set function|subadditive]] function, but a subadditive function need not be submodular.\nIf <math>\\Omega</math> is not assumed finite, then the above conditions are not equivalent.  In particular a function \n<math>f</math> defined by <math>f(S) = 1</math> if <math>S</math> is finite and <math>f(S) = 0</math> if <math>S</math> is infinite \nsatisfies the first condition above, but the second condition fails when <math>S</math> and <math>T</math> are infinite sets with finite intersection.\n\n== Types of submodular functions ==\n\n=== Monotone ===\nA submodular function <math>f</math> is ''monotone'' if for every <math>T\\subseteq S</math> we have that <math>f(T)\\leq f(S)</math>. Examples of monotone submodular functions include:\n; Linear (Modular) functions : Any function of the form <math>f(S)=\\sum_{i\\in S}w_i</math> is called a linear function. Additionally if <math>\\forall i,w_i\\geq 0</math> then f is monotone.\n; Budget-additive functions : Any function of the form <math>f(S)=\\min\\left\\{B,~\\sum_{i\\in S}w_i\\right\\}</math> for each <math>w_i\\geq 0</math> and <math>B\\geq 0</math> is called budget additive.{{citation needed|date=August 2014}}\n; Coverage functions : Let <math>\\Omega=\\{E_1,E_2,\\ldots,E_n\\}</math> be a collection of subsets of some [[matroid|ground set]] <math>\\Omega'</math>. The function <math>f(S)=\\left|\\bigcup_{E_i\\in S}E_i\\right|</math> for <math>S\\subseteq \\Omega</math> is called a coverage function. This can be generalized by adding non-negative weights to the elements.\n; [[Entropy (information theory)|Entropy]] : Let <math>\\Omega=\\{X_1,X_2,\\ldots,X_n\\}</math> be a set of [[random variables]]. Then for any <math>S\\subseteq \\Omega</math> we have that <math>H(S)</math> is a submodular function, where <math>H(S)</math> is the entropy of the set of random variables <math>S</math>, a fact known as [[Entropic_vector#Shannon-type_inequalities_and_Γn|Shannon's inequality]].<ref>{{Cite web|url = http://www.cs.cmu.edu/~aarti/Class/10704_Spring15/lecs/lec3.pdf|title = Information Processing and Learning|date = |access-date = |website = |publisher = cmu|last = |first = }}</ref> Further inequalities for the entropy function are known to hold, see [[entropic vector]].\n; [[Matroid]] [[matroid rank|rank functions]] : Let <math>\\Omega=\\{e_1,e_2,\\dots,e_n\\}</math> be the ground set on which a matroid is defined. Then the rank function of the matroid is a submodular function.<ref name=F22>Fujishige (2005) p.22</ref>\n\n=== Non-monotone ===\nA submodular function which is not monotone is called ''non-monotone''.\n\n==== Symmetric ====\nA non-monotone submodular function <math>f</math> is called ''symmetric'' if for every <math>S\\subseteq \\Omega</math> we have that <math>f(S)=f(\\Omega-S)</math>.\nExamples of symmetric non-monotone submodular functions include:\n; Graph cuts : Let <math>\\Omega=\\{v_1,v_2,\\dots,v_n\\}</math> be the vertices of a [[Graph (discrete mathematics)|graph]]. For any set of vertices <math>S\\subseteq \\Omega</math> let <math>f(S)</math> denote the number of edges <math>e=(u,v)</math> such that <math>u\\in S</math> and <math>v\\in \\Omega-S</math>. This can be generalized by adding non-negative weights to the edges.\n; [[Mutual information]] : Let <math>\\Omega=\\{X_1,X_2,\\ldots,X_n\\}</math> be a set of [[random variable]]s. Then for any <math>S\\subseteq \\Omega</math> we have that <math>f(S)=I(S;\\Omega-S)</math> is a submodular function, where <math>I(S;\\Omega-S)</math> is the mutual information.\n\n==== Asymmetric ====\nA non-monotone submodular function which is not symmetric is called asymmetric.\n; Directed cuts : Let <math>\\Omega=\\{v_1,v_2,\\dots,v_n\\}</math> be the vertices of a [[directed graph]]. For any set of vertices <math>S\\subseteq \\Omega</math> let <math>f(S)</math> denote the number of edges <math>e=(u,v)</math> such that <math>u\\in S</math> and <math>v\\in \\Omega-S</math>. This can be generalized by adding non-negative weights to the directed edges.\n\n== Continuous extensions ==\n\n=== Lovász extension ===\nThis extension is named after mathematician [[László Lovász]]. Consider any vector <math>\\mathbf{x}=\\{x_1,x_2,\\dots,x_n\\}</math> such that each <math>0\\leq x_i\\leq 1</math>. Then the Lovász extension is defined as <math>f^L(\\mathbf{x})=\\mathbb{E}(f(\\{i|x_i\\geq \\lambda\\}))</math> where the expectation is over <math>\\lambda</math> chosen from the [[uniform distribution (continuous)|uniform distribution]] on the interval <math>[0,1]</math>. The Lovász extension is a convex function.\n\n=== Multilinear extension ===\nConsider any vector <math>\\mathbf{x}=\\{x_1,x_2,\\ldots,x_n\\}</math> such that each <math>0\\leq x_i\\leq 1</math>. Then the multilinear extension is defined as <math>F(\\mathbf{x})=\\sum_{S\\subseteq \\Omega} f(S) \\prod_{i\\in S} x_i \\prod_{i\\notin S} (1-x_i)</math>.\n\n=== Convex closure ===\nConsider any vector <math>\\mathbf{x}=\\{x_1,x_2,\\dots,x_n\\}</math> such that each <math>0\\leq x_i\\leq 1</math>. Then the convex closure is defined as <math>f^-(\\mathbf{x})=\\min\\left(\\sum_S \\alpha_S f(S):\\sum_S \\alpha_S 1_S=\\mathbf{x},\\sum_S \\alpha_S=1,\\alpha_S\\geq 0\\right)</math>. It can be shown that <math>f^L(\\mathbf{x})=f^-(\\mathbf{x})</math>.\n\n=== Concave closure ===\nConsider any vector <math>\\mathbf{x}=\\{x_1,x_2,\\dots,x_n\\}</math> such that each <math>0\\leq x_i\\leq 1</math>. Then the concave closure is defined as <math>f^+(\\mathbf{x})=\\max\\left(\\sum_S \\alpha_S f(S):\\sum_S \\alpha_S 1_S=\\mathbf{x},\\sum_S \\alpha_S=1,\\alpha_S\\geq 0\\right)</math>.\n\n== Properties ==\n# The class of submodular functions is [[closure (mathematics)|closed]] under non-negative [[linear combination]]s. Consider any submodular function <math>f_1,f_2,\\ldots,f_k</math> and non-negative numbers <math>\\alpha_1,\\alpha_2,\\ldots,\\alpha_k</math>. Then the function <math>g</math> defined by <math>g(S)=\\sum_{i=1}^k \\alpha_i f_i(S)</math> is submodular. \n#For any submodular function <math>f</math>, the function defined by <math>g(S)=f(\\Omega \\setminus S)</math> is submodular. \n#The function <math>g(S)=\\min(f(S),c)</math>, where <math>c</math> is a real number, is submodular whenever <math>f</math> is monotone submodular. More generally, <math>g(S)=h(f(S))</math> is submodular, for any non decreasing concave function <math>h</math>. \n# Consider a random process where a set <math>T</math> is chosen with each element in <math>\\Omega</math> being included in <math>T</math> independently with probability <math>p</math>. Then the following inequality is true <math>\\mathbb{E}[f(T)]\\geq p f(\\Omega)+(1-p) f(\\varnothing)</math> where <math>\\varnothing</math> is the empty set. More generally consider the following random process where a set <math>S</math> is constructed as follows. For each of <math>1\\leq i\\leq l, A_i\\subseteq \\Omega</math> construct <math>S_i</math> by including each element in <math>A_i</math> independently into <math>S_i</math> with probability <math>p_i</math>. Furthermore let <math>S=\\cup_{i=1}^l S_i</math>. Then the following inequality is true <math>\\mathbb{E}[f(S)]\\geq \\sum_{R\\subseteq [l]} \\Pi_{i\\in R}p_i \\Pi_{i\\notin R}(1-p_i)f(\\cup_{i\\in R}A_i)</math>.{{Citation needed|date=November 2013}}\n\n== Optimization problems ==\nSubmodular functions have properties which are very similar to [[convex function|convex]] and [[concave function]]s. For this reason, an [[optimization problem]] which concerns optimizing a convex or concave function can also be described as the problem of maximizing or minimizing a submodular function subject to some constraints.\n\n=== Submodular minimization===\nThe simplest minimization problem is to find a set <math>S\\subseteq \\Omega</math> which minimizes a submodular function subject to no constraints. This problem is computable in (strongly)<ref name=\"IFF\" /><ref name=\"Schrijver\" /> [[polynomial time]].<ref name=\"GLS\" /><ref name=\"Cunningham\" /> Computing the [[minimum cut]] in a graph is a special case of this general minimization problem. However, even simple constraints like cardinality lower bound constraints make this problem [[NP hard]], with polynomial lower bound approximation factors.<ref name=\"SF\" /><ref name=\"IJB\" />\n\n=== Submodular maximization===\nUnlike minimization, maximization of submodular functions is usually [[NP-hard]]. Many problems, such as [[max cut]] and the [[maximum coverage problem]], can be cast as special cases of this general maximization problem under suitable constraints. Typically, the approximation algorithms for these problems are based on either [[greedy algorithm]]s or [[local search (optimization)|local search algorithm]]s. The problem of maximizing a symmetric non-monotone submodular function subject to no constraints admits a 1/2 approximation algorithm.<ref name=\"FMV\" /> Computing the [[maximum cut]] of a graph is a special case of this problem. The more general problem of maximizing an arbitrary non-monotone submodular function subject to no constraints also admits a 1/2 approximation algorithm.<ref name=\"BFNS\" /> The problem of maximizing a monotone submodular function subject to a cardinality constraint admits a <math>1 - 1/e</math> approximation algorithm.<ref name=\"NVF\" /> The [[maximum coverage problem]] is a special case of this problem. The more general problem of maximizing a monotone submodular function subject to a [[matroid]] constraint also admits a <math>1 - 1/e</math> approximation algorithm.<ref name=\"CCPV\" /><ref name=\"FNS\" /><ref name=\"FW\" /> Many of these algorithms can be unified within a semi-differential based framework of algorithms.<ref name=\"IJB\" />\n\n===Related optimization problems===\nApart from submodular minimization and maximization, another natural problem is Difference of Submodular Optimization.<ref name=\"NB\" /><ref name=\"IBUAI\" /> Unfortunately, this problem is not only NP hard, but also inapproximable.<ref name=\"IBUAI\" /> A related optimization problem is minimize or maximize a submodular function, subject to a submodular level set constraint (also called submodular optimization subject to submodular cover or submodular knapsack constraint). This problem admits bounded approximation guarantees.<ref name=\"IB\" /> Another optimization problem involves partitioning data based on a submodular function, so as to maximize the average welfare. This problem is called the submodular welfare problem.<ref name=\"JV\" />\n\n== Applications ==\nSubmodular functions naturally occur in several real world applications, in [[economics]], [[game theory]], [[machine learning]] and [[computer vision]]. Owing the diminishing returns property, submodular functions naturally model costs of items, since there is often a larger discount, with an increase in the items one buys. Submodular functions model notions of complexity, similarity and cooperation when they appear in minimization problems. In maximization problems, on the other hand, they model notions of diversity, information and coverage. For more information on applications of submodularity, particularly in machine learning, see <ref name=\"KG\" /><ref name=\"ST\" /><ref name=\"JB\" />\n\n== See also ==\n* [[Supermodular function]]\n* [[Matroid]], [[Polymatroid]]\n* [[Utility functions on indivisible goods]]\n\n== Citations ==\n{{reflist|30em|\nrefs=\n<ref name=\"GLS\">{{cite journal |authorlink=Martin Grötschel |first=M. |last=Grötschel |authorlink2=László Lovász |first2=L. |last2=Lovasz |authorlink3=Alexander Schrijver |first3=A. |last3=Schrijver |title=The ellipsoid method and its consequences in combinatorial optimization |journal=Combinatorica |volume=1 |issue=2 |year=1981 |pages=169–197 |doi=10.1007/BF02579273 }}</ref>\n<ref name=\"Cunningham\">{{cite journal |first=W. H. |last=Cunningham |title=On submodular function minimization |journal=Combinatorica |volume=5 |issue=3 |year=1985 |pages=185–192 |doi=10.1007/BF02579361 }}</ref>\n<ref name=\"IFF\">{{cite journal |first=S. |last=Iwata |first2=L. |last2=Fleischer |first3=S. |last3=Fujishige |title=A combinatorial strongly polynomial algorithm for minimizing submodular functions |journal=J. ACM |volume=48 |year=2001 |issue=4 |pages=761–777 |doi=10.1145/502090.502096 }}</ref>\n<ref name=\"Schrijver\">{{cite journal |authorlink=Alexander Schrijver |first=A. |last=Schrijver |title=A combinatorial algorithm minimizing submodular functions in strongly polynomial time |journal=J. Combin. Theory Ser. B |volume=80 |year=2000 |issue=2 |pages=346–355 |doi=10.1006/jctb.2000.1989 }}</ref>\n<ref name=\"IJB\">R. Iyer, S. Jegelka and J. Bilmes, Fast Semidifferential based submodular function optimization, Proc. ICML (2013).</ref>\n<ref name=\"IB\">R. Iyer and J. Bilmes, Submodular Optimization Subject to Submodular Cover and Submodular Knapsack Constraints, In Advances of NIPS (2013).</ref>\n<ref name=\"IBUAI\">R. Iyer and J. Bilmes, Algorithms for Approximate Minimization of the Difference between Submodular Functions, In Proc. UAI (2012).</ref>\n<ref name=\"NB\">M. Narasimhan and J. Bilmes, A submodular-supermodular procedure with applications to discriminative structure learning, In Proc. UAI (2005).</ref>\n<ref name=\"FMV\">[[Uriel Feige|U. Feige]], V. Mirrokni and J. Vondrák, Maximizing non-monotone submodular functions, Proc. of 48th FOCS (2007), pp. 461–471.</ref>\n<ref name=\"NVF\">[[George Nemhauser|G. L. Nemhauser]], L. A. Wolsey and M. L. Fisher, An analysis of approximations for maximizing submodular set functions I, Mathematical Programming 14 (1978), 265–294.</ref>\n<ref name=\"CCPV\">G. Calinescu, C. Chekuri, M. Pál and J. Vondrák, Maximizing a submodular set function subject to a matroid constraint, SIAM J. Comp. 40:6 (2011), 1740-1766.</ref>\n<ref name=\"BFNS\">N. Buchbinder, M. Feldman, J. Naor and R. Schwartz, A tight linear time (1/2)-approximation for unconstrained submodular maximization, Proc. of 53rd FOCS (2012), pp. 649-658.</ref>\n<ref name=\"FW\">Y. Filmus, J. Ward, A tight combinatorial algorithm for submodular maximization subject to a matroid constraint, Proc. of 53rd FOCS (2012), pp. 659-668.</ref>\n<ref name=\"SF\">Z. Svitkina and L. Fleischer, Submodular approximation: Sampling-based algorithms and lower bounds, SIAM Journal on Computing (2011).</ref>\n<ref name=\"JV\">J. Vondrák, Optimal approximation for the submodular welfare problem in the value oracle model, Proc. of STOC (2008), pp. 461–471.</ref>\n<ref name=\"ST\">http://submodularity.org/.</ref>\n<ref name=\"KG\">A. Krause and C. Guestrin, Beyond Convexity: Submodularity in Machine Learning, Tutorial at ICML-2008</ref>\n<ref name=\"JB\">J. Bilmes, Submodularity in Machine Learning Applications, Tutorial at AAAI-2015.</ref>\n<ref name=\"LB\">H. Lin and J. Bilmes, A Class of Submodular Functions for Document Summarization, ACL-2011.</ref>\n<ref name=\"TIWB\">S. Tschiatschek, R. Iyer, H. Wei and J. Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, NIPS-2014.</ref>\n<ref name=\"KG1\">A. Krause and C. Guestrin, Near-optimal nonmyopic value of information in graphical models, UAI-2005.</ref>\n<ref name=\"FNS\">M. Feldman, J. Naor and R. Schwartz, A unified continuous greedy algorithm for submodular maximization, Proc. of 52nd FOCS (2011).</ref>\n}}\n\n== References ==\n\n*{{Citation|last=Schrijver|first=Alexander|authorlink=Alexander Schrijver|year=2003|title=Combinatorial Optimization|location=|publisher=[[Springer Publishing|Springer]]|isbn=3-540-44389-4}}\n*{{Citation|last=Lee|first=Jon|authorlink=Jon Lee (mathematician)|year= 2004 |title=A First Course in Combinatorial Optimization |location=|publisher=[[Cambridge University Press]]|isbn= 0-521-01012-8}}\n*{{Citation|last=Fujishige|first=Satoru|year=2005|title=Submodular Functions and Optimization|location=|publisher=[[Elsevier]]|isbn=0-444-52086-4}}\n*{{Citation|last=Narayanan|first=H.|year= 1997 |title=Submodular Functions and Electrical Networks|location=|publisher=|isbn= 0-444-82523-1}}\n*{{citation | last=Oxley | first=James G. | title=Matroid theory | series=Oxford Science Publications | location=Oxford | publisher=[[Oxford University Press]] | year=1992 | isbn=0-19-853563-5 | zbl=0784.05002 }}\n\n==External links==\n* http://www.cs.berkeley.edu/~stefje/references.html has a longer bibliography\n\n<!--- Categories --->\n[[Category:Combinatorial optimization| ]]\n[[Category:Approximation algorithms| ]]\n[[Category:Matroid theory|Matroid theory]]"
    },
    {
      "title": "(1+ε)-approximate nearest neighbor search",
      "url": "https://en.wikipedia.org/wiki/%281%2B%CE%B5%29-approximate_nearest_neighbor_search",
      "text": "\n'''(1+ε)-approximate nearest neighbor search''' is a special case of the [[nearest neighbor search]] problem. The solution to the (1+ε)-approximate nearest neighbor search is a point or multiple points within distance (1+ε) R from a query point, where R is the distance between the query point and its true nearest neighbor.<ref>{{Cite journal|last=Arya|first=Sunil|last2=Mount|first2=David M.|date=1993|title=Approximate Nearest Neighbor Queries in Fixed Dimensions|url=https://cgis.cs.umd.edu/~mount/Papers/soda93-ann.pdf|journal=SODA|volume=93|pages=|via=}}</ref>\n\nReasons to approximate nearest neighbor search include the space and time costs of exact solutions in high-dimensional spaces (see [[curse of dimensionality]]) and that in some domains, finding an approximate nearest neighbor is an acceptable solution.\n\nApproaches for solving '''(1+ε)-approximate nearest neighbor search''' include [[kd-tree]]s,<ref>{{Cite journal|last=Arya|first=Sunil|last2=Mount|first2=David M.|last3=Netanyahu|first3=Nathan|last4=Silverman|first4=Ruth|last5=Wu|first5=Angela Y.|date=1994|title=An optimal algorithm for approximate nearest neighbor searching in fixed dimensions|url=ftp://138.96.0.43/abs/fcazals/classes/centrale-exam-papers/mount-ANN.pdf|journal=Proc. 5th ACM-SIAM Sympos. Discrete Algorithms|volume=|pages=|via=}}</ref> [[Locality Sensitive Hashing]] and [[brute force search]].\n\n\n==References==\n<references/>\n\n{{DEFAULTSORT:(1 + epsilon)-approximate nearest neighbor search}}\n[[Category:Approximation algorithms]]\n[[Category:Classification algorithms]]\n[[Category:Search algorithms]]\n\n{{algorithm-stub}}"
    },
    {
      "title": "Alpha max plus beta min algorithm",
      "url": "https://en.wikipedia.org/wiki/Alpha_max_plus_beta_min_algorithm",
      "text": "{{Distinguish|Minimax|Alpha–beta pruning}}\n\n[[File:AlphaMaxBetaMin.png|thumb|The locus of points that give the same value in the algorithm, for different values of alpha and beta.]]\n\nThe '''alpha max plus beta min algorithm''' is a high-speed approximation of the [[square root]] of the sum of two squares. The square root of the sum of two squares, also known as [[Pythagorean addition]], is a useful function, because it finds the [[hypotenuse]] of a right triangle given the two side lengths, the [[norm (mathematics)|norm]] of a 2-D [[vector (geometric)|vector]], or the [[magnitude (mathematics)|magnitude]] <math> |z| = \\sqrt{a^2 + b^2 }</math> of a [[complex number]] ''z'' = ''a'' + ''b''i given the [[real number|real]] and [[imaginary number|imaginary]] parts.\n\nThe algorithm avoids performing the square and square-root operations, instead using simple operations such as comparison, multiplication, and addition. Some choices of the α and β parameters of the algorithm allow the multiplication operation to be reduced to a simple shift of binary digits that is particularly well suited to implementation in high-speed digital circuitry.\n\nThe approximation is expressed as\n\n:<math> |z| = \\alpha\\,\\! \\mathbf{Max} + \\beta\\,\\! \\mathbf{Min}</math>,\n\nwhere <math>\\mathbf{Max}</math> is the maximum absolute value of ''a'' and ''b'' and <math>\\mathbf{Min}</math> is the minimum absolute value of ''a'' and ''b''.\n\nFor the closest approximation, the optimum values for <math>\\alpha\\,\\!</math> and <math>\\beta\\,\\!</math> are <math>\\alpha_0 = \\frac{2 \\cos \\frac{\\pi}{8}}{1 + \\cos \\frac{\\pi}{8}} = 0.960433870103...</math> and <math>\\beta_0 = \\frac{2 \\sin \\frac{\\pi}{8}}{1 + \\cos \\frac{\\pi}{8}} = 0.397824734759...</math>, giving a maximum error of 3.96%.\n\n{| class=\"wikitable\"\n|-\n! <math>\\alpha\\,\\!</math> || <math>\\beta\\,\\!</math> || Largest error (%) || Mean error (%)<br />\n|-\n| align=\"right\" | 1/1 || align=\"right\" | 1/2 || align=\"right\" | 11.80 || align=\"right\" | 8.68\n|-\n| align=\"right\" | 1/1 || align=\"right\" | 1/4 || align=\"right\" | 11.61 || align=\"right\" | 3.20\n|-\n| align=\"right\" | 1/1 || align=\"right\" | 3/8 || align=\"right\" | 6.80 || align=\"right\" | 4.25\n|-\n| align=\"right\" | 7/8 || align=\"right\" | 7/16 || align=\"right\" | 12.50 || align=\"right\" | 4.91\n|-\n| align=\"right\" | 15/16 || align=\"right\" | 15/32 || align=\"right\" | 6.25 || align=\"right\" | 3.08\n|-\n| align=\"right\" | <math>\\alpha_0</math> || align=\"right\" | <math>\\beta_0</math> || align=\"right\" | 3.96 || align=\"right\" | 2.41\n|-\n|}\n[[File:Alpha Max Beta Min approximation.png|800px|centre]]\n\n==Improvements==\nWhen <math> \\alpha\\ < 1 </math>, <math>|z|</math> becomes smaller than <math>\\mathbf{Max}</math> (which is geometrically impossible) near the axes where <math>\\mathbf{Min}</math> is near 0.\nThis can be remedied by replacing the result with <math>\\mathbf{Max}</math> whenever that is greater, essentially splitting the line into two different segments.\n\n:<math> |z| = MaxOf(\\mathbf{Max} , \\alpha\\ \\mathbf{Max} + \\beta\\ \\mathbf{Min})</math>\n\nDepending on the hardware, this improvement can be almost free.\n\nUsing this improvement changes which parameter values are optimal, because they no longer need a close match for the entire interval. A lower <math> \\alpha\\ </math> and higher <math> \\beta\\ </math> can therefore increase precision further.\n\n<em>Increasing precision:</em> When splitting the line in two like this one could improve precision even more by replacing the first segment by a better estimate than <math>\\mathbf{Max}</math>, and adjust <math> \\alpha\\ </math> and <math> \\beta\\ </math> accordingly. \n\n:<math> |z| = MaxOf(|z_0|, |z_1|)</math>\n:<math> |z_0|= \\alpha\\ _0 \\mathbf{Max} + \\beta\\ _0 \\mathbf{Min}</math>\n:<math> |z_1|= \\alpha\\ _1 \\mathbf{Max} + \\beta\\ _1 \\mathbf{Min}</math>\n\n{| class=\"wikitable\"\n|-\n! <math>\\alpha\\ _0 </math> || <math>\\beta\\ _0 </math> || <math>\\alpha\\ _1 </math> || <math>\\beta\\ _1 </math> || Largest error (%) <br />\n|-\n| align=\"right\" | 1 || align=\"right\" | 0 || align=\"right\" | 7/8 || align=\"right\" | 17/32 || align=\"right\" | -2.65% \n|-\n| align=\"right\" | 1 || align=\"right\" | 0 || align=\"right\" | 29/32 || align=\"right\" | 61/128 || align=\"right\" | +2.4%\n|-\n| align=\"right\" | 1 || align=\"right\" | 1/8 || align=\"right\" | 7/8 || align=\"right\" | 33/64 || align=\"right\" | -1.7%\n|-\n| align=\"right\" | 1 || align=\"right\" | 5/32 || align=\"right\" | 27/32 || align=\"right\" | 71/128 || align=\"right\" | 1.22%\n|-\n| align=\"right\" | 127/128 || align=\"right\" | 3/16 || align=\"right\" | 27/32 || align=\"right\" | 71/128 || align=\"right\" | -1.13%  \n|-\n|}\n\nBeware however, that a non-zero <math> \\beta\\ _0 </math> would require at least one extra addition and some bit-shifts (or a multiplication), probably nearly doubling the cost and, depending on the hardware, possibly defeat the purpose of using an approximation in the first place.\n\n==See also==\n*[[Hypot]], a precise function or algorithm that is also safe against overflow and underflow\n\n==References==\n*[[Richard G. Lyons|Lyons, Richard G]]. ''Understanding Digital Signal Processing, section 13.2.'' Prentice Hall, 2004 {{ISBN|0-13-108989-7}}.\n*Griffin, Grant. [http://www.dspguru.com/dsp/tricks/magnitude-estimator DSP Trick: Magnitude Estimator].\n\n==External links==\n*{{cite web |title=Extension to three dimensions |date=May 14, 2015 |work=[[Stack Exchange]] |url=https://math.stackexchange.com/q/1282435 }}\n\n[[Category:Approximation algorithms]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Approximation-preserving reduction",
      "url": "https://en.wikipedia.org/wiki/Approximation-preserving_reduction",
      "text": "In [[computability theory]] and [[computational complexity theory]], especially the study of [[Approximation_algorithm|approximation algorithms]], an '''approximation-preserving reduction''' is an [[algorithm]] for transforming one [[computational problem|optimization problem]] into another problem, such that the distance of solutions from optimal is preserved to some degree.  Approximation-preserving reductions are a subset of more general [[Reduction_(complexity)|reductions]] in complexity theory; the difference is that approximation-preserving reductions usually make statements on [[Approximation_algorithm|approximation problems]] or [[Optimization_problem|optimization problems]], as opposed to [[Decision_problem|decision problems]].\n\nIntuitively, problem A is reducible to problem B via an approximation-preserving reduction if, given an instance of problem A and a (possibly approximate) solver for problem B, one can convert the instance of problem A into an instance of problem B, apply the solver for problem B, and recover a solution for problem A that also has some guarantee of approximation.\n\n==Definition==\nUnlike reductions on decision problems, an approximation-preserving reduction must preserve more than the truth of the problem instances when reducing from one problem to another.  It must also maintain some guarantee on the relationship between the cost of the solution to the cost of the optimum in both problems.  To formalize:\n\nLet <math>A</math> and <math>B</math> be optimization problems.  \n\nLet <math>x</math> be an instance of problem <math>A</math>, with optimal solution <math>OPT(x)</math>.  Let <math>c_A(x, y)</math> denote the cost of a solution <math>y</math> to an instance <math>x</math> of problem <math>A</math>.  This is also the metric used to determine which solution is considered optimal.\n\nAn '''approximation-preserving reduction''' is a pair of functions <math>(f, g)</math> (which often must be computable in polynomial time), such that:\n* <math>f</math> maps an ''instance'' <math>x</math> of <math>A</math> to an ''instance'' <math>x'</math> of <math>B</math>.\n* <math>g</math> maps a ''solution'' <math>y'</math> of <math>B</math> to a ''solution'' <math>y</math> of <math>A</math>.\n* <math>g</math> preserves some guarantee of the solution's ''[[Approximation_algorithm#Performance_guarantees|performance]],'' or ''approximation ratio'', defined as <math>R_A(x, y) = \\max \\left ( \\frac{c_A(x, OPT(x))}{c_A(x, y)}, \\frac{c_A(x, y)}{c_A(x, OPT(x))} \\right )</math>.\n\n== Types ==\nThere are many different types of approximation-preserving reductions, all of which have a different guarantee (the third point in the definition above).  However, unlike with other reductions, approximation-preserving reductions often overlap in what properties they demonstrate on optimization problems (e.g. complexity class membership or completeness, or inapproximability).  The different types of reductions are used instead as varying reduction techniques, in that the applicable reduction which is most easily adapted to the problem is used.  \n\nNot all types of approximation-preserving reductions can be used to show membership in all approximability complexity classes, the most notable of which are [[Polynomial-time approximation scheme|PTAS]] and [[APX]].  A reduction below '''preserves membership''' in a complexity class C if, given a problem A that reduces to problem B via the reduction scheme, and B is in C, then A is in C as well.  Some reductions shown below only preserve membership in APX or PTAS, but not the other.  Because of this, careful choice must be made when selecting an approximation-preserving reductions, especially for the purpose of proving [[Complete_(complexity)|completeness]] of a problem within a complexity class.  \n\nCrescenzi suggests that the three most ideal styles of reduction, for both ease of use and proving power, are PTAS reduction, AP reduction, and L-reduction.<ref name=crescenzi>{{cite journal|last1=Crescenzi|first1=Pierluigi|title=A Short Guide To Approximation Preserving Reductions|journal=Proceedings of the 12th Annual IEEE Conference on Computational Complexity|date=1997|pages=262–|url=http://dl.acm.org/citation.cfm?id=792302|publisher=IEEE Computer Society|location=Washington, D.C.}}</ref>  The reduction descriptions that follow are from Crescenzi's survey of approximation-preserving reductions.\n\n=== Strict reduction ===\n\n'''Strict reduction''' is the simplest type of approximation-preserving reduction.  In a strict reduction, the approximation ratio of a solution y' to an instance x' of a problem B must be at most as good as the approximation ratio of the corresponding solution y to instance x of problem A.  In other words:\n: <math>R_A(x, y) \\le R_B(x', y')</math> for <math>x' = f(x), y = g(y')</math>.\n\nStrict reduction is the most straightforward: if a strict reduction from problem A to problem B exists, then problem A can always be approximated to at least as good a ratio as problem B.  Strict reduction preserves membership in both PTAS and APX.\n\nThere exists a similar concept of an '''S-reduction,''' for which <math>c_A(x, y) = c_B(x', y')</math>, and the optima of the two corresponding instances must have the same cost as well.  S-reduction is a very special case of strict reduction, and is even more constraining.  In effect, the two problems A and B must be in near-perfect correspondence with each other.  The existence of an S-reduction implies not only the existence of a strict reduction but every other reduction listed here.\n\n=== L-reduction ===\n{{Main|L-reduction}}\n\nL-reductions preserve membership in PTAS as well as APX (but ''only for minimization problems in the case of the latter'').  As a result, they cannot be used in general to prove completeness results about APX, Log-APX, or Poly-APX, but nevertheless they are valued for their natural formulation and ease of use in proofs.<ref name=crescenzi></ref>\n\n=== PTAS-reduction ===\n{{Main|PTAS reduction}}\n\nPTAS-reduction is another commonly used reduction scheme.  Though it preserves membership in PTAS, it does not do so for APX.  Nevertheless, APX-completeness is defined in terms of PTAS reductions.\n\nPTAS-reductions are a generalization of P-reductions, shown below, with the only difference being that the function <math>g</math> is allowed to depend on the approximation ratio <math>r</math>.\n\n=== A-reduction and P-reduction ===\n\nA-reduction and P-reduction are similar reduction schemes that can be used to show membership in APX and PTAS respectively.  Both introduce a new function <math>c</math>, defined on numbers greater than 1, which must be computable.\n\nIn an A-reduction, we have that\n:<math>R_B(x', y') \\le r \\rightarrow R_A(x, y) \\le c(r)</math>.\n\nIn a P-reduction, we have that\n:<math>R_B(x', y') \\le c(r) \\rightarrow R_A(x, y) \\le r</math>.\n\nThe existence of a P-reduction implies the existence of a PTAS-reduction.\n\n=== E-reduction ===\n\nE-reduction, which is a generalization of strict reduction but implies both A-reduction and P-reduction, is an example of a less restrictive reduction style that preserves membership not only in PTAS and APX, but also the larger classes [[Log-APX]] and [[Poly-APX]]. E-reduction introduces two new parameters, a polynomial <math>p</math> and a constant <math>\\beta</math>.  Its definition is as follows.\n\nIn an E-reduction, we have that for some polynomial <math>p</math> and constant <math>\\beta</math>,\n* <math>c_B(OPT_B(x')) \\le p(|x|) c_A(OPT_A(x))</math>, where <math>|x|</math> denotes the size of the problem instance's description.\n* For any solution <math>y'</math> to <math>B</math>, we have <math>R_A(x, y) \\le 1 + \\beta \\cdot (R_B(x', y') - 1)</math>.\n\nTo obtain an A-reduction from an E-reduction, let <math>c(r) = 1+\\beta \\cdot (r-1)</math>, and to obtain a P-reduction from an E-reduction, let <math>c(r) = 1 + (r-1)/\\beta</math>.\n\n=== AP-reduction ===\n\nAP-reductions are used to define completeness in the classes [[Log-APX]] and [[Poly-APX]].  They are a special case of PTAS reduction, meeting the following restrictions.\n\nIn an AP-reduction, we have that for some constant <math>\\alpha</math>,\n: <math>R_B(x', y') \\le r \\rightarrow R_A(x, y) \\le 1 + \\alpha \\cdot (r-1)</math>\nwith the additional generalization that the function <math>g</math> is allowed to depend on the approximation ratio <math>r</math>, as in PTAS-reduction.\n\nAP-reduction is also a generalization of E-reduction.  An additional restriction actually needs to be imposed for AP-reduction to preserve Log-APX and Poly-APX membership, as E-reduction does: for fixed problem size, the computation time of <math>f, g</math> must be non-increasing as the approximation ratio increases.\n\n=== Gap reduction ===\n{{Main|Gap reduction}}\n\nA gap reduction is a type of reduction that, while useful in proving some inapproximability results, does not resemble the other reductions shown here.  Gap reductions deal with optimization problems within a decision problem container, generated by changing the problem goal to distinguishing between the optimal solution and solutions some multiplicative factor worse than the optimum.\n\n== See also ==\n* [[Reduction (complexity)]]\n* [[PTAS reduction]]\n* [[L-reduction]]\n* [[Approximation algorithm]]\n\n== References ==\n{{reflist}} \n\n[[Category:Approximation algorithms]]\n[[Category:Reduction (complexity)]]"
    },
    {
      "title": "APX",
      "url": "https://en.wikipedia.org/wiki/APX",
      "text": "{{other uses}}\nIn [[computational complexity theory|complexity theory]] the class '''APX''' (an abbreviation of \"approximable\") is the set of '''[[NP (complexity)|NP]]''' [[optimization problem]]s that allow [[polynomial-time]] [[approximation algorithm]]s with approximation ratio bounded by a constant (or '''constant-factor approximation algorithms''' for short). In simple terms, problems in this class have efficient [[algorithm]]s that can find an answer within some fixed multiplicative factor of the optimal answer.\n\nAn approximation algorithm is called an <math>f(n)</math>-approximation algorithm for input size <math>n</math> if it can be proven that the solution that the algorithm finds is at most a multiplicative factor of <math>f(n)</math> times worse than the optimal solution. Here, <math>f(n)</math> is called the ''approximation ratio''. Problems in APX are those with algorithms for which the approximation ratio <math>f(n)</math> is a constant <math>c</math>. The approximation ratio is conventionally stated greater than 1. In the case of minimization problems, <math>f(n)</math> is the found solution's score divided by the optimum solution's score, while for maximization problems the reverse is the case. For maximization problems, where an inferior solution has a smaller score, <math>f(n)</math> is sometimes stated as less than 1; in such cases, the reciprocal of <math>f(n)</math> is the ratio of the score of the found solution to the score of the optimum solution.\n\nIf there is a polynomial-time algorithm to solve a problem to within ''every'' multiplicative factor of the optimum other than 1, then the problem is said to have a [[Polynomial-time approximation scheme|polynomial-time approximation scheme ('''PTAS''')]]. Unless [[P = NP problem|P = NP]] there exist problems that are in APX but without a PTAS, so the class of problems with a PTAS is strictly contained in APX. One such problem is the [[bin packing problem]].\n\n== APX-hardness and APX-completeness ==\n\nA problem is said to be '''APX-hard''' if there is a [[PTAS reduction]] from every problem in APX to that problem, and to be '''APX-complete''' if the problem is APX-hard and also in APX. As a consequence of P ≠ NP ⇒ PTAS ≠ APX, if P ≠ NP is assumed, no APX-hard problem has a PTAS. In practice, reducing one problem to another to demonstrate APX-completeness is often done using other reduction schemes, such as [[L-reduction]]s, which imply PTAS reductions.\n\n=== Examples ===\n\nOne of the simplest APX-complete problems is [[MAX-3SAT|MAX-3SAT-3]], a variation of the [[boolean satisfiability problem]]. In this problem, we have a boolean formula in [[conjunctive normal form]] where each variable appears at most 3 times, and we wish to know the maximum number of clauses that can be simultaneously satisfied by a single assignment of true/false values to the variables.\n\nOther APX-complete problems include:\n\n* [[Independent set (graph theory)#Approximation algorithms|Max independent set]] in bounded-degree graphs (here, the approximation ratio depends on the maximum degree of the graph, but is constant if the max degree is fixed).\n* [[Vertex cover#Approximate evaluation|Min vertex cover]]. The complement of any maximal independent set must be a vertex cover.\n* [[Dominating set#Algorithms and computational complexity|Min dominating set]] in bounded-degree graphs.\n* The [[Travelling salesman problem#Complexity of approximation|travelling salesman problem]] when the distances in the graph satisfy the conditions of a [[Metric (mathematics)|metric]]. TSP is [[Optimization problem#NP optimization problem|NPO-complete]] in the general case.\n* The [[token reconfiguration]] problem, via [[L-reduction]] from set cover.\n\n== Related complexity classes ==\n\n=== PTAS ===\n{{Main|Polynomial-time approximation scheme}}\n\nPTAS (''polynomial time approximation scheme'') consists of problems that can be approximated to within any constant factor besides 1 in time that is polynomial to the input size, but the polynomial depends on such factor. This class is a subset of APX.\n\n=== APX-intermediate ===\n\nUnless [[P = NP problem|P = NP]], there exist problems in APX that are neither in PTAS nor APX-complete. Such problems can be thought of as having a hardness between PTAS problems and APX-complete problems, and may be called '''APX-intermediate'''. The [[bin packing problem]] is thought to be APX-intermediate. Despite not having a known PTAS, the bin packing problem has several \"asymptotic PTAS\" algorithms, which behave like a PTAS when the optimum solution is large, so intuitively it may be easier than problems that are APX-hard.\n\nOne other example of a potentially APX-intermediate problem is [[Edge coloring#Algorithms that use more than the optimal number of colors|min edge coloring]].\n\n=== f(n)-APX ===\n\nOne can also define a family of complexity classes <math>f(n)</math>-APX, where <math>f(n)</math>-APX contains problems with a polynomial time approximation algorithm with a <math>O(f(n))</math> approximation ratio. One can analogously define <math>f(n)</math>-APX-complete classes; some such classes contain well-known optimization problems. Log-APX-completeness and poly-APX-completeness are defined in terms of [[Approximation-preserving reduction#AP-reduction|AP-reductions]] rather than PTAS-reductions; this is because PTAS-reductions are not strong enough to preserve membership in Log-APX and Poly-APX, even though they suffice for APX.\n\nLog-APX-complete, consisting of the hardest problems that can be approximated efficiently to within a factor logarithmic in the input size, includes [[Dominating set#Algorithms and computational complexity|min dominating set]] when degree is unbounded.\n\nPoly-APX-complete, consisting of the hardest problems that can be approximated efficiently to within a factor polynomial in the input size, includes [[Independent set (graph theory)#Approximation algorithms|max independent set]] in the general case.\n\nThere also exist problems that are exp-APX-complete, where the approximation ratio is exponential in the input size. This may occur when the approximation is dependent on the value of numbers within the problem instance; these numbers may be expressed in space logarithmic in their value, hence the exponential factor.\n\n== See also ==\n* [[Approximation-preserving reduction]]\n* [[Complexity class]]\n* [[Approximation algorithm]]\n* [[Max/min CSP/Ones classification theorems]] - a set of theorems that enable mechanical classification of problems about boolean relations into approximability complexity classes\n* [[MaxSNP]] - a closely related subclass\n\n== References ==\n\n* {{CZoo|APX|A#apx}}\n* C. Papadimitriou and M. Yannakakis. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.3995&rep=rep1&type=pdf Optimization, approximation and complexity classes]. Journal of Computer and System Sciences, 43:425–440, 1991.\n* Pierluigi Crescenzi, Viggo Kann, Magnús Halldórsson, [[Marek Karpinski]] and [[Gerhard J. Woeginger|Gerhard Woeginger]]. [http://www.nada.kth.se/~viggo/wwwcompendium/node225.html Maximum Satisfiability]. [http://www.nada.kth.se/%7Eviggo/wwwcompendium/ ''A compendium of NP optimization problems'']. Last updated March 20, 2000.\n\n{{ComplexityClasses}}\n\n{{DEFAULTSORT:APX}}\n[[Category:Complexity classes]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Baker's technique",
      "url": "https://en.wikipedia.org/wiki/Baker%27s_technique",
      "text": "In [[theoretical computer science]], '''Baker's technique''' is a method for designing [[polynomial-time approximation scheme]]s (PTASs) for problems on [[planar graph]]s. It is named after [[Brenda Baker]], who announced it in a 1983 conference and published it in the ''[[Journal of the ACM]]'' in 1994.\n\nThe idea for Baker's technique is to break the graph into layers, such that the problem can be solved optimally on each layer, then combine the solutions from each layer in a reasonable way that will result in a feasible solution.  This technique has given PTASs for the following problems: [[subgraph isomorphism]], [[maximum independent set]], [[minimum vertex cover]], [[minimum dominating set]], minimum [[edge dominating set]], maximum triangle matching, and many others.\n\nThe [[bidimensionality|bidimensionality theory]] of  [[Erik Demaine]], Fedor Fomin, [[Mohammad Hajiaghayi|Hajiaghayi]], and Dimitrios Thilikos and its offshoot ''simplifying decompositions'' ({{harvtxt|Demaine|Hajiaghayi|Kawarabayashi|2005}},{{harvtxt|Demaine|Hajiaghayi|Kawarabayashi|2011}}) generalizes and greatly expands the applicability of Baker's technique\nfor a vast set of problems on [[planar graph]]s and more generally  [[graph minor|graphs excluding a fixed minor]], such as bounded genus graphs, as well as to other classes of graphs not closed under taking minors such as the [[1-planar graph]]s.\n\n==Example of technique==\nThe example that we will use to demonstrate Baker's technique is the maximum weight [[Independent set (graph theory)|independent set]] problem.\n\n===Algorithm===\n INDEPENDENT-SET(<math>G</math>,<math>w</math>,<math>\\epsilon</math>)\n Choose an arbitrary vertex <math> r </math>\n <math>k = 1/\\epsilon</math>\n find the breadth-first search levels for <math> G </math> rooted at <math> r </math> <math>\\pmod k</math>: <math>\\{V_0,V_1, \\ldots, V_{k-1} \\}</math>\n for <math>\\ell = 0, \\ldots, k-1</math>\n find the components <math>G^\\ell_1, G^\\ell_2, \\ldots,</math> of <math>G</math> after deleting <math>V_\\ell</math>\n for <math>i = 1,2, \\ldots </math>\n compute <math>S_i^\\ell</math>, the maximum-weight independent set of <math>G_i^\\ell</math>\n <math>S^\\ell = \\cup_i S_i^\\ell</math>\n let <math>S^{\\ell^*}</math> be the solution of maximum weight among <math>\\{S^0,S^1, \\ldots, S^{k-1} \\}</math>\n return <math>S^{\\ell^*}</math>\n\nNotice that the above algorithm is feasible because each <math>S^\\ell </math> is the union of disjoint independent sets.\n\n===Dynamic programming===\n[[Dynamic programming]] is used when we compute the maximum-weight independent set for each <math>G_i^\\ell</math>. This dynamic program works because each <math>G_i^\\ell</math> is a [[K-outerplanar graph|<math>k</math>-outerplanar graph]]. Many NP-complete problems can be solved with dynamic programming on <math>k</math>-outerplanar graphs. Baker's technique can be interpreted as covering the given planar graphs with subgraphs of this type, finding the solution to each subgraph using dynamic programming, and gluing the solutions together.\n\n==References==\n{{refbegin|colwidth=30em}}\n* {{citation\n | last = Baker| first = B.  | authorlink = Brenda Baker\n | journal = [[Journal of the ACM]]\n | title = Approximation algorithms for NP-complete problems on planar graphs\n | volume = 41\n | issue = 1\n | year = 1994\n | pages = 153–180\n | doi = 10.1145/174644.174650}}.\n* {{citation\n  | last = Baker| first = B.  | authorlink = Brenda Baker\n| journal = FOCS\n| title = Approximation algorithms for NP-complete problems on planar graphs\n | volume = 24\n | year = 1983}}.\n* {{citation\n | last = Bodlaender| first = H.  | authorlink = Hans L. Bodlaender\n | journal = ICALP\n | title = Dynamic programming on graphs with bounded treewidth\n | year = 1988\n | doi = 10.1007/3-540-19488-6_110}}.\n* {{citation\n | last1 = Demaine|first1 = E.|authorlink= Erik Demaine|last2 = Hajiaghayi|first2 = M.|last3 = Kawarabayashi|first3 = K.  | author3-link = Ken-ichi Kawarabayashi\n | journal = FOCS\n | title = Algorithmic graph minor theory: Decomposition, approximation, and coloring\n | volume = 46\n | year = 2005\n | doi = 10.1109/SFCS.2005.14}}.\n* {{citation\n | last1 = Demaine|first1 = E.|authorlink= Erik Demaine|last2 = Hajiaghayi|first2 = M.|last3 = Kawarabayashi|first3 = K.  | author3-link = Ken-ichi Kawarabayashi\n | journal = STOC\n | title = Contraction decomposition in H-minor-free graphs and algorithmic applications\n | volume = 43\n | year = 2011\n | doi = 10.1145/1993636.1993696}}.\n* {{citation\n | last1 = Demaine|first1 = E.|authorlink= Erik Demaine|last2 = Hajiaghayi| first2 = M.|last3 = Nishimura|first3 = N.|last4 = Ragde|first4 = P.|last5 = Thilikos|first5 = D. \n | journal = J. Comput. Syst. Sci.\n | title = Approximation algorithms for classes of graphs excluding single-crossing graphs as minors.\n | volume = 69\n | year = 2004\n | doi = 10.1016/j.jcss.2003.12.001}}.\n* {{citation\n | last = Eppstein| first = D. \n | authorlink=David Eppstein\n | journal = Algorithmica\n | title = Diameter and treewidth in minor-closed graph families.\n | volume = 27\n | doi=10.1007/s004530010020\n | arxiv = math/9907126v1\n | year = 2000}}.\n* {{citation\n | last = Eppstein| first = D. \n | authorlink=David Eppstein\n | journal = SODA\n | title = Subgraph isomorphism in planar graphs and related problems.\n | volume = 6\n | year = 1995}}.\n*{{citation\n | last1 = Grigoriev | first1 = Alexander\n | last2 = Bodlaender | first2 = Hans L. | author2-link = Hans L. Bodlaender\n | doi = 10.1007/s00453-007-0010-x\n | issue = 1\n | journal = Algorithmica\n | mr = 2344391\n | pages = 1–11\n | title = Algorithms for graphs embeddable with few crossings per edge\n | volume = 49\n | year = 2007}}.\n\n<!--- Categories --->\n\n\n\n\n[[Category:1983 in computer science]]\n[[Category:Planar graphs]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Bidimensionality",
      "url": "https://en.wikipedia.org/wiki/Bidimensionality",
      "text": "'''Bidimensionality'''  theory characterizes a broad range of graph problems ('''bidimensional''') that admit efficient approximate,  fixed-parameter or kernel solutions in a broad range of graphs. These graph classes include [[planar graph]]s, map graphs, [[Graph embedding|bounded-genus]] graphs and graphs excluding any fixed minor. In particular, bidimensionality theory builds on the [[graph minor]] theory  of [[Neil Robertson (mathematician)|Robertson]] and [[Paul Seymour (mathematician)|Seymour]] by extending the mathematical results and building new algorithmic tools. The theory was introduced   in the work of [[Erik Demaine|Demaine]], [[Fedor_Fomin|Fomin]], [[Mohammad Hajiaghayi| Hajiaghayi]], and Thilikos,<ref name=\"dfht05\">{{harvtxt|Demaine|Fomin|Hajiaghayi|Thilikos|2005}}</ref> for which the authors received the [[Nerode Prize]] in 2015.\n\n==Definition==\nA [[Parameterized complexity|parameterized problem]] <math> \\Pi </math> is a subset of <math>\\Gamma^{*}\\times \\mathbb{N}</math> for some finite alphabet <math>\\Gamma</math>. An instance of a parameterized problem consists of ''(x,k)'', where ''k'' is called the parameter.\n\nA parameterized problem <math>\\Pi</math> is ''minor-bidimensional'' if\n# For any pair of graphs <math>H,G</math>, such that  <math>H</math> is a minor of <math>G</math> and integer <math>k</math>, <math>(G,k)\\in \\Pi</math> yields that  <math>(H,k)\\in \\Pi </math>. In other words, contracting or deleting an edge in a graph <math>G</math> cannot increase the parameter; and\n#  there is  <math>\\delta > 0</math> such that for every   <math>(r \\times r)</math>-grid <math>R</math>, <math>(R, k)\\not \\in \\Pi</math> for every   <math>k\\leq \\delta r^2</math>. In other words, the value of the solution on  <math>R</math> should be at least   <math>\\delta r^2</math>.\n\nExamples of minor-bidimensional problems are the parameterized versions of [[vertex cover]],  [[feedback vertex set]], minimum maximal matching, and [[longest path]].\n\n[[Image:Gamma graph.jpg|thumb|180px|right| Graph <math>\\Gamma_6</math>]]\n\nLet <math>\\Gamma_{r}</math>    be the graph obtained from the  <math>(r\\times r)</math>-grid by\ntriangulating internal faces such that all internal vertices become  of degree  6,\nand then one corner of degree two   joined by edges with all vertices\nof the external face.\nA parameterized problem <math>\\Pi</math> is ''contraction-bidimensional'' if\n# For any pair of graphs <math>H,G</math>, such that  <math>H</math> is a contraction of <math>G</math> and integer <math>k</math>, <math>(G,k)\\in \\Pi</math> yields that  <math>(H,k)\\in \\Pi </math>. In other words, contracting  an edge in a graph <math>G</math> cannot increase the parameter; and\n# there is <math>\\delta > 0</math> such that <math>(\\Gamma_r, k)\\not \\in \\Pi</math> for every  <math>k\\leq \\delta r^2</math>.\n\nExamples of contraction-bidimensional problems are [[dominating set]], [[connected dominating set]], max-leaf spanning tree, and [[edge dominating set]].\n\n==Excluded grid theorems==\nAll algorithmic applications of bidimensionality are based on the following combinatorial property: either the [[treewidth]] of a graph is small, or the graph contains a large grid as a minor or contraction. More precisely, \n\n# There is a function ''f'' such that every graph ''G'' excluding a fixed ''h''-vertex graph  as a [[Minor (graph theory)|minor]] and of treewidth   at least ''f(h)r''  contains '' (r x r)''-grid as  a minor.<ref name=\"dh06\">{{harvtxt|Demaine|Hajiaghayi|2008}}</ref>\n# There is a function ''g'' such that every graph ''G'' excluding a fixed  ''h''-vertex [[apex graph]] as a minor and of treewidth   at least ''g(h) r''  can be edge-contracted to <math>\\Gamma_r</math>.<ref name=\"fgt09\">{{harvtxt|Fomin|Golovach|Thilikos|2009}}</ref>\n\n[[Halin's grid theorem]] is an analogous excluded grid theorem for infinite graphs.<ref>{{harvtxt|Diestel|2004}}</ref>\n\n==Subexponential parameterized algorithms==\n\nLet <math> \\Pi </math> be a minor-bidimensional problem such that for any graph ''G'' excluding some fixed graph as a minor and of treewidth at most ''t'', deciding whether <math> (G,k) \\in \\Pi </math> can be done in time <math> 2^{O(t)}\\cdot |G|^{O(1)}</math>. Then  for every graph ''G'' excluding some fixed graph as a minor, deciding whether <math> (G,k) \\in \\Pi </math> can be done in time <math> 2^{O(\\sqrt{k})}\\cdot |G|^{O(1)}</math>. Similarly, for contraction-bidimensional problems,    for  graph ''G'' excluding some fixed [[apex graph]] as a minor, inclusion  <math> (G,k) \\in \\Pi </math> can be decided in time <math> 2^{O(\\sqrt{k})}\\cdot |G|^{O(1)}</math>.\n\nThus many bidimensional problems like Vertex Cover, Dominating Set, k-Path, are solvable in time  <math> 2^{O(\\sqrt{k})}\\cdot |G|^{O(1)}</math> on graphs excluding some fixed graph as a minor.\n\n==Polynomial time approximation schemes==\n\nBidimensionality theory has been used to obtain [[polynomial-time approximation scheme]]s for many bidimensional problems.\nIf a minor (contraction) bidimensional problem has several  additional properties <ref name=\"flrs10\">{{harvtxt|Fomin|Lokshtanov|Raman|Saurabh|2011}}</ref><ref name=\"dh05\">{{harvtxt|Demaine|Hajiaghayi|2005}}</ref>  then the problem poses efficient polynomial-time approximation schemes on (apex) minor-free graphs.\n\nIn particular,  by making use of bidimensionality, it was shown that [[feedback vertex set]],  [[vertex cover]], connected vertex cover, cycle packing,  diamond hitting set,  maximum induced forest, maximum induced bipartite subgraph and maximum induced planar subgraph admit an EPTAS on H-minor-free graphs. Edge dominating set, [[dominating set]], r-dominating set, connected dominating set,  r-scattered set, minimum maximal matching, [[independent set (graph theory)|independent set]],  maximum full-degree spanning tree, maximum induced at most d-degree subgraph,  [[maximum internal spanning tree]], [[induced matching]],  triangle packing, partial r-dominating set  and partial vertex cover admit an  EPTAS on apex-minor-free graphs.\n\n==Kernelization==\n{{main|Kernelization}}\nA parameterized problem with a parameter ''k'' is said to admit a linear vertex kernel if there is a polynomial time reduction, called a [[kernelization]] algorithm, that maps the input instance to an equivalent instance with at most ''O(k)'' vertices.\n\nEvery minor-bidimensional problem  <math>\\Pi</math> with additional properties, namely, with the separation property and with finite integer index,  has a linear vertex kernel on graphs excluding some fixed graph as a minor. Similarly, every contraction-bidimensional problem <math>\\Pi</math> with the separation property and with finite integer index has a linear vertex kernel on graphs excluding some fixed [[apex graph]] as a minor.<ref name=\"flst10\">{{harvtxt|Fomin|Lokshtanov|Saurabh|Thilikos|2010}}</ref>\n\n\n\n==Notes==\n{{reflist|colwidth=30em}}\n\n==References==\n{{refbegin|colwidth=30em}}\n*{{citation\n | last1 =  Demaine| first1 = Erik D.  \n | last2 = Fomin| first2 = Fedor V. \n | last3 = Hajiaghayi| first3 = MohammadTaghi \n | last4 = Thilikos| first4 = Dimitrios M. \n | doi = 10.1145/1101821.1101823\n | issue = 6\n | journal = [[J. ACM]]\n | pages = 866–893\n | title = Subexponential parameterized algorithms on bounded-genus graphs and ''H''-minor-free graphs\n | volume = 52\n | year = 2005| arxiv = 1104.2230}}.\n*{{citation\n | last1 = Demaine| first1 = Erik D. \n | last2 = Fomin| first2 = Fedor V. \n | last3 = Hajiaghayi| first3 = MohammadTaghi \n | last4 = Thilikos| first4 = Dimitrios M. \n | doi = 10.1137/S0895480103433410\n | issue = 3\n | journal =  [[SIAM Journal on Discrete Mathematics]]\n | pages = 501–511\n | title = Bidimensional parameters and local treewidth\n | volume = 18\n | year = 2004| citeseerx = 10.1.1.81.9021}}.\n*{{citation\n | last1 = Demaine| first1 = Erik D. \n | last2 = Hajiaghayi| first2 = MohammadTaghi \n |title =  16th ACM-SIAM Symposium on Discrete Algorithms (SODA 2005)\n | pages = 590–601\n | contribution = Bidimensionality: new connections between FPT algorithms and PTASs\n | year = 2005}}.\n*{{citation\n | last1 = Demaine| first1 = Erik D. \n | last2 = Hajiaghayi| first2 = MohammadTaghi \n | doi = 10.1007/s00493-008-2140-4\n | issue = 1\n | journal = [[Combinatorica]]\n | pages = 19–36\n | title = Linearity of grid minors in treewidth with applications through bidimensionality\n | volume = 28\n | year = 2008}}.\n*{{citation\n | last1 = Demaine| first1 = Erik D. \n | last2 = Hajiaghayi| first2 = MohammadTaghi \n | doi = 10.1093/comjnl/bxm033 \n | issue = 3\n | journal = The Computer Journal\n | pages = 332–337\n | title = The bidimensionality theory and its algorithmic applications\n | volume = 51\n | year = 2008}}.\n*{{citation\n | last = Diestel | first = R.\n | doi = 10.1007/BF02941538\n | journal = Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg\n | mr = 2112834\n | pages = 237–242\n | title = A short proof of Halin's grid theorem\n | volume = 74\n | year = 2004}}.\n*{{citation\n | last1 = Fomin| first1 = Fedor V. \n | last2 = Golovach| first2 = Petr A. \n | last3 = Thilikos| first3 = Dimitrios M. \n | doi = 10.1007/978-3-642-04128-0_63\n | series = Lecture Notes in Computer Science\n | title =  17th Annual European Symposium on Algorithms  (ESA 2009)\n | volume = 5757\n | pages = 706–717\n | contribution = Contraction Bidimensionality: The Accurate Picture\n | year = 2009}}.\n*{{citation\n | last1 = Fomin| first1 = Fedor V. \n | last2 = Lokshtanov| first2 = Daniel \n | last3 = Raman| first3 = Venkatesh\n | last4 = Saurabh| first4 = Saket\n | arxiv = 1005.5449 | contribution = Bidimensionality and EPTAS\n | year = 2011\n | title = Proc. 22nd ACM/SIAM Symposium on Discrete Algorithms (SODA 2011)\n | pages = 748-759| bibcode = 2010arXiv1005.5449F}}.\n*{{citation\n | last1 = Fomin| first1 = Fedor V. \n | last2 = Lokshtanov| first2 = Daniel \n | last3 = Saurabh| first3 = Saket\n | last4 = Thilikos| first4 = Dimitrios M.\n |title =  21st ACM-SIAM Symposium on Discrete Algorithms (SODA 2010)\n | pages = 503–510\n | contribution = Bidimensionality and Kernels\n | year = 2010}}.\n{{refend}}\n\n==Further reading==\n*{{citation\n|last1=Cygan|first1=Marek\n|last2=Fomin|first2=Fedor V.\n|last3=Kowalik|first3=Lukasz\n|last4=Lokshtanov|first4=Daniel\n|last5=Marx|first5=Daniel\n|last6=Pilipczuk|first6=Marcin\n|last7=Pilipczuk|first7=Michal\n|last8=Saurabh|first8=Saket\n|year=2015\n|chapter=Chapter 7\n|title=Parameterized Algorithms\n|publisher=Springer\n|isbn=978-3-319-21274-6\n|page=612}}\n*{{citation\n|last1=Fomin|first1=Fedor V.\n|last2=Lokshtanov|first2=Daniel\n|last3=Saurabh|first3=Saket\n|last4=Zehavi|first4=Meirav\n|year=2019\n|chapter=Chapter 15\n|title=Kernelization: Theory of Parameterized Preprocessing\n|publisher=Cambridge University Press\n|isbn=1107057760\n|doi=10.1017/9781107415157\n|page=528}}\n\n[[Category:Analysis of algorithms]]\n[[Category:Approximation algorithms]]\n[[Category:Graph minor theory]]\n[[Category:Parameterized complexity]]"
    },
    {
      "title": "Christofides algorithm",
      "url": "https://en.wikipedia.org/wiki/Christofides_algorithm",
      "text": "The '''Christofides algorithm''' is an [[algorithm]] for finding approximate solutions to the [[travelling salesman problem]], on instances where the distances form a [[metric space]] (they are symmetric and obey the [[triangle inequality]]).<ref name=\"gt\">{{citation|title=Algorithm Design and Applications|first1=Michael T.|last1=Goodrich|author1-link=Michael T. Goodrich|first2=Roberto|last2=Tamassia|author2-link=Roberto Tamassia|publisher=Wiley|year=2015|pages=513–514|chapter=18.1.2 The Christofides Approximation Algorithm}}.</ref>\nIt is an [[approximation algorithm]] that guarantees that its solutions will be within a factor of 3/2 of the optimal solution length, and is named after Nicos Christofides, who published it in 1976.<ref>Nicos Christofides, Worst-case analysis of a new heuristic for the travelling salesman problem, Report 388, Graduate School of Industrial Administration, CMU, 1976.</ref> {{as of|2017}}, this is the best [[approximation ratio]] that has been proven for the traveling salesman problem on general metric spaces, although better approximations are known for some special cases.\n\n== Algorithm ==\nLet {{math|1=''G'' = (''V'',''w'')}} be an instance of the travelling salesman problem. That is, {{mvar|G}} is a complete graph on the set {{mvar|V}} of vertices, and the function {{mvar|w}} assigns a nonnegative real weight to every edge of {{mvar|G}}.\nAccording to the triangle inequality, for every three vertices {{mvar|u}}, {{mvar|v}}, and {{mvar|x}}, it should be the case that {{math|''w''(''uv'') + ''w''(''vx'') ≥ ''w''(''ux'')}}.\n\nThen the algorithm can be described in [[pseudocode]] as follows.<ref name=\"gt\"/>\n# Create a [[minimum spanning tree]] {{mvar|T}} of {{mvar|G}}.\n# Let {{mvar|O}} be the set of vertices with odd [[degree (graph theory)|degree]] in {{mvar|T}}. By the [[handshaking lemma]], {{mvar|O}} has an even number of vertices.\n# Find a minimum-weight [[perfect matching]] {{mvar|M}} in the [[induced subgraph]] given by the vertices from {{mvar|O}}.\n# Combine the edges of {{mvar|M}} and {{mvar|T}} to form a connected [[multigraph]] {{mvar|H}} in which each vertex has even degree.\n# Form an [[Eulerian circuit]] in {{mvar|H}}.\n# Make the circuit found in previous step into a [[Hamiltonian circuit]] by skipping repeated vertices (''shortcutting'').\n\n== Approximation ratio ==\n\nThe cost of the solution produced by the algorithm is within 3/2 of the optimum.\nTo prove this, let {{mvar|C}} be the optimal traveling salesman tour. Removing an edge from {{mvar|C}} produces a spanning tree, which must have weight at least that of the minimum spanning tree, implying that {{math|''w''(''T'') ≤ ''w''(''C'')}}.\nNext, number the vertices of {{mvar|O}} in cyclic order around {{mvar|C}}, and partition {{mvar|C}} into two sets of paths: the ones in which the first path vertex in cyclic order has an odd number and the ones in which the first path vertex has an even number.\nEach set of paths corresponds to a perfect matching of {{mvar|O}} that matches the two endpoints of each path, and the weight of this matching is at most equal to the weight of the paths.\nSince these two sets of paths partition the edges of {{mvar|C}}, one of the two sets has at most half of the weight of {{mvar|C}}, and thanks to the triangle inequality its corresponding matching has weight that is also at most half the weight of {{mvar|C}}.\nThe minimum-weight perfect matching can have no larger weight, so {{math|''w''(''M'') ≤ ''w''(''C'')/2}}.\nAdding the weights of {{mvar|T}} and {{mvar|M}} gives the weight of the Euler tour, at most {{math|3''w''(''C'')/2}}. Thanks to the triangle inequality, shortcutting does not increase the weight,\nso the weight of the output is also at most {{math|3''w''(''C'')/2}}.<ref name=\"gt\"/>\n\n==Lower bounds==\nThere exist inputs to the travelling salesman problem that cause the Christofides algorithm to find a solution whose approximation ratio is arbitrarily close to 3/2. One such class of \ninputs are formed by a [[path (graph theory)|path]] of {{mvar|n}} vertices, with the path edges having weight {{math|1}}, together with a set of edges connecting vertices two steps apart in the path with weight {{math|1 + ''&epsilon;''}}\nfor a number {{math|''&epsilon;''}} chosen close to zero but positive. All remaining edges of the complete graph have distances given by the [[Shortest path problem|shortest path]]s in this subgraph.\nThen the minimum spanning tree will be given by the path, of length {{math|''n'' &minus; 1}}, and the only two odd vertices will be the path endpoints, whose perfect matching consists of a single edge with weight approximately {{math|''n''/2}}.\nThe union of the tree and the matching is a cycle, with no possible shortcuts, and with weight approximately {{math|3''n''/2}}. However, the optimal solution uses the edges of weight {{math|1 + ''&epsilon;''}} together with two weight-{{math|1}} edges incident to the endpoints of the path,\nand has total weight {{math|(1 + ''&epsilon;'')(''n'' &minus; 2) + 2}}, close to {{mvar|n}} for small values of {{math|''&epsilon;''}}. Hence we obtain an approximation ratio of 3/2.<ref>{{citation|title=Encyclopedia of Algorithms}|editor-first=Ming-Yang|editor-last=Kao|publisher=Springer-Verlag|year=2008|isbn=9780387307701|url=https://books.google.com/books?id=i3S9_GnHZwYC&pg=PA517|pages=517–519|chapter=Metric TSP|first=Markus|last=Bläser}}.</ref>\n\n== Example ==\n{| class=\"wikitable\"\n|-\n|[[File:Metrischer Graph mit 5 Knoten.svg|200px]]|| Given: complete graph whose edge weights obey the triangle inequality\n|-\n|[[File:Christofides MST.svg|200px]] || Calculate [[minimum spanning tree]] {{mvar|T}}\n|-\n|[[File:V'.svg|200px]] || Calculate the set of vertices {{mvar|O}} with odd degree in {{mvar|T}}\n|-\n|[[File:G V'.svg|200px]] || Form the subgraph of {{mvar|G}} using only the vertices of {{mvar|O}}\n|-\n|[[File:Christofides Matching.svg|200px]] || Construct a minimum-weight perfect matching {{mvar|M}} in this subgraph\n|-\n|[[File:TuM.svg|200px]] || Unite matching and spanning tree {{math|''T'' &cup; ''M''}} to form an Eulerian multigraph\n|-\n|[[File:Eulertour.svg|200px]] || Calculate Euler tour\n|-\n|[[File:Eulertour bereinigt.svg|200px]] || Remove repeated vertices, giving the algorithm's output\n|}\n\n== References ==\n{{reflist|30em}}\n\n==External links==\n* [https://xlinux.nist.gov/dads/HTML/christofides.html NIST Christofides Algorithm Definition]\n\n[[Category:1976 in computer science]]\n[[Category:Travelling salesman problem]]\n[[Category:Graph algorithms]]\n[[Category:Spanning tree]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Convex volume approximation",
      "url": "https://en.wikipedia.org/wiki/Convex_volume_approximation",
      "text": "In the [[analysis of algorithms]], several authors have studied the computation of the [[volume]] of high-dimensional [[convex body|convex bodies]], a problem that can also be used to model many other problems in [[combinatorial enumeration]].\nOften these works use a black box model of computation in which the input is given by a subroutine for testing whether a point is inside or outside of the convex body, rather than by an explicit listing of the vertices or faces of a [[convex polytope]].\nIt is known that, in this model, no [[deterministic algorithm]] can achieve an accurate approximation,{{R|E|BF}} and even for an explicit listing of faces or vertices the problem is [[Sharp-P-complete|#P-hard]].{{r|DF}}\nHowever, a joint work by [[Martin Dyer]], [[Alan M. Frieze]] and [[Ravindran Kannan]] provided a randomized [[polynomial time approximation scheme]] for the problem,\nproviding a sharp contrast between the capabilities of randomized and deterministic algorithms.{{r|DFK}}\n\nThe main result of the paper is a randomized algorithm for finding an <math>\\varepsilon</math> approximation to the volume of a convex body <math>K</math> in <math>n</math>-dimensional Euclidean space by assuming the existence of a membership oracle. The algorithm takes time bounded by a polynomial in <math>n</math>, the dimension of <math>K</math> and <math>1/\\varepsilon</math>.\nThe algorithm combines two ideas:\n*By using a [[Markov chain Monte Carlo]] (MCMC) method, it is possible to generate points that are nearly uniformly randomly distributed within a given convex body. The basic scheme of the algorithm is a nearly uniform sampling from within <math>K</math> by placing a grid consisting of <math>n</math>-dimensional cubes and doing a [[random walk]] over these cubes. By using the theory of [[Markov chain mixing time|rapidly mixing Markov chains]], they show that it takes a polynomial time for the random walk to settle down to being a nearly uniform distribution.{{r|DFK}}\n*By using [[rejection sampling]], it is possible to compare the volumes of two convex bodies, one nested within another, when their volumes are within a small factor of each other. The basic idea is to generate random points within the outer of the two bodies, and to count how often those points are also within the inner body.\nThe given convex body can be approximated by a sequence of nested bodies, eventually reaching one of known volume (a hypersphere), with this approach used to estimate the factor by which the volume changes at each step of this sequence. Multiplying these factors gives the approximate volume of the original body.\n\nThis work earned its authors the 1991 [[Fulkerson Prize]].<ref>[http://www.ams.org/profession/prizes-awards/pabrowse?purl=fulkerson-prize Fulkerson Prize winners], [[American Mathematical Society]], retrieved 2017-08-03.</ref>\nAlthough the time for this algorithm is polynomial, it has a high exponent.\nSubsequent authors improved the running time of this method by providing more quickly mixing Markov chains for the same problem.{{r|AK|KLS|LS|LV}}\n\n==References==\n{{reflist|refs=\n\n<ref name=AK>{{citation\n | last1 = Applegate | first1 = David | author1-link = David Applegate\n | last2 = Kannan | first2 = Ravi | author2-link = Ravindran Kannan\n | contribution = Sampling and Integration of Near Log-concave Functions\n | doi = 10.1145/103418.103439\n | isbn = 978-0-89791-397-3\n | location = New York, NY, USA\n | pages = 156–163\n | publisher = ACM\n | title = Proceedings of the Twenty-third Annual ACM Symposium on Theory of Computing (STOC '91)\n | year = 1991}}</ref>\n\n<ref name=BF>{{citation\n | last1 = Bárány | first1 = Imre | author1-link = Imre Bárány\n | last2 = Füredi | first2 = Zoltán | author2-link = Zoltán Füredi\n | doi = 10.1007/BF02187886\n | issue = 4\n | journal = [[Discrete and Computational Geometry]]\n | mr = 911186\n | pages = 319–326\n | title = Computing the volume is difficult\n | volume = 2\n | year = 1987}}</ref>\n\n<ref name=DF>{{citation\n | last1 = Dyer | first1 = Martin | author1-link = Martin Dyer\n | last2 = Frieze | first2 = Alan | author2-link = Alan M. Frieze\n | doi = 10.1137/0217060\n | issue = 5\n | journal = [[SIAM Journal on Computing]]\n | mr = 961051\n | pages = 967–974\n | title = On the complexity of computing the volume of a polyhedron\n | volume = 17\n | year = 1988}}</ref>\n\n<ref name=DFK>{{citation\n | last1 = Dyer | first1 = Martin | author1-link = Martin Dyer\n | last2 = Frieze | first2 = Alan | author2-link = Alan M. Frieze\n | last3 = Kannan | first3 = Ravi | author3-link = Ravindran Kannan\n | doi = 10.1145/102782.102783\n | issue = 1\n | journal = [[Journal of the ACM]]\n | mr = 1095916\n | pages = 1–17\n | title = A random polynomial-time algorithm for approximating the volume of convex bodies\n | volume = 38\n | year = 1991}}</ref>\n\n<ref name=E>{{citation\n | last = Elekes | first = G. | authorlink = György Elekes\n | doi = 10.1007/BF02187701\n | issue = 4\n | journal = [[Discrete and Computational Geometry]]\n | mr = 866364\n | pages = 289–292\n | title = A geometric inequality and the complexity of computing volume\n | volume = 1\n | year = 1986}}</ref>\n\n<ref name=KLS>{{citation\n | last1 = Kannan | first1 = Ravi | author1-link = Ravindran Kannan\n | last2 = Lovász | first2 = László | author2-link = László Lovász\n | last3 = Simonovits | first3 = Miklós | author3-link = Miklós Simonovits\n | doi = 10.1002/(SICI)1098-2418(199708)11:1<1::AID-RSA1>3.0.CO;2-X\n | issue = 1\n | journal = Random Structures & Algorithms\n | mr = 1608200\n | pages = 1–50\n | title = Random walks and an <math>O^*(n^5)</math> volume algorithm for convex bodies\n | volume = 11\n | year = 1997}}</ref>\n\n<ref name=LS>{{citation\n | last1 = Lovász | first1 = L. | author1-link = László Lovász\n | last2 = Simonovits | first2 = M. | author2-link = Miklós Simonovits\n | doi = 10.1002/rsa.3240040402\n | issue = 4\n | journal = Random Structures & Algorithms\n | mr = 1238906\n | pages = 359–412\n | title = Random walks in a convex body and an improved volume algorithm\n | volume = 4\n | year = 1993}}</ref>\n\n<ref name=LV>{{citation\n | last1 = Lovász | first1 = L. | author1-link = László Lovász\n | last2 = Vempala | first2 = Santosh | author2-link = Santosh Vempala\n | doi = 10.1016/j.jcss.2005.08.004\n | issue = 2\n | journal = [[Journal of Computer and System Sciences]]\n | mr = 2205290\n | pages = 392–417\n | title = Simulated annealing in convex bodies and an <math>O^*(n^4)</math> volume algorithm\n | volume = 72\n | year = 2006}}</ref>\n}}\n\n[[Category:Computational geometry]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Domination analysis",
      "url": "https://en.wikipedia.org/wiki/Domination_analysis",
      "text": "'''Domination analysis''' of an [[approximation algorithm]] is a way to estimate its performance, introduced by Glover and Punnen in 1997. Unlike the classical [[approximation ratio]] analysis, which compares the numerical quality of a calculated solution with that of an optimal solution, domination analysis involves examining the rank of the calculated solution in the sorted order of all possible solutions.  In this style of analysis, an algorithm is said to have '''dominance number''' or '''domination number''' ''K'', if there exists a subset of ''K'' different solutions to the problem among which the algorithm's output is the best. Domination analysis can also be expressed using a '''domination ratio''', which is the fraction of the solution space that is no better than the given solution; this number always lies within the interval [0,1], with larger numbers indicating better solutions. Domination analysis is most commonly applied to problems for which the total number of possible solutions is known and for which exact solution is difficult.\n\nFor instance, in the [[Traveling salesman problem]], there are (''n''-1)! possible solutions for a problem instance with ''n'' cities. If an algorithm can be shown to have dominance number close to (''n''-1)!, or equivalently to have domination ratio close to 1, then it can be taken as preferable to an algorithm with lower dominance number.\n\nIf it is possible to efficiently find [[random sample]]s of a problem's solution space, as it is in the Traveling salesman problem, then it is straightforward for a [[randomized algorithm]] to find a solution that with high probability has high domination ratio: simply construct a set of samples and select the best solution from among them. (See, e.g., Orlin and Sharma.)\n\nThe dominance number described here should not be confused with the domination number of a graph, which refers to the number of vertices in the smallest [[dominating set]] of the graph.\n\nRecently, a growing number of articles in which domination analysis has been applied to assess the performance of heuristics has appeared. This kind of analysis may be seen as competing with the classical approximation ratio analysis tradition.  The two measures may also be viewed as complementary.\n\n== Known Results ==\nThis section contains a technical survey of known results.\n\n=== Vertex Cover ===\n  '''Inapproximability.''' Let ε > 0. Unless [[P = NP problem|P=NP]], there is no polynomial algorithm for Vertex Cover\n  such that its domination number is greater than 3^((n-n^ε)/3).\n\n=== Knapsack ===\n  '''Inapproximability.''' Let ε > 0. Unless P=NP, there is no polynomial algorithm for Knapsack\n  such that its domination number is greater than 2^(n-n^ε).\n\n=== Max Satisfiability ===\n\n=== TSP ===\n\n== References ==\n* {{cite journal\n  | author = Glover, F. and Punnen, A. P.\n  | title = The traveling salesman problem: new solvable cases and linkages with the development of approximation algorithms\n  | journal = J. Oper. Res. Soc.\n  | volume = 48\n  | year = 1997\n  | pages = 502–510\n  | doi = 10.1057/palgrave.jors.2600392\n  | issue = 5}}\n* {{cite web\n  | author = Gutin, Gregory and Yeo, Anders\n  | title = Introduction to Domination Analysis\n  | year = 2004\n  | url = http://www.optimization-online.org/DB_FILE/2004/02/818.pdf\n  | publisher = [http://www.optimization-online.org/index.html Optimization Online]}}\n* {{cite web\n  | author = [[James B. Orlin|Orlin, James B.]] and Sharma, Dushyant\n  | title = The Extended Neighborhood: Definition and Characterization\n  | year = 2002\n  | url = http://web.mit.edu/sloan-msa/Papers/3.4.pdf}}\n\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Farthest-first traversal",
      "url": "https://en.wikipedia.org/wiki/Farthest-first_traversal",
      "text": "[[File:Farthest-first traversal.svg|thumb|upright=1.3|The first five steps of the farthest-first traversal of a planar point set. The first point is chosen arbitrarily and each successive point is as far as possible from all previously chosen points.]]\nIn [[computational geometry]], the '''farthest-first traversal''' of a bounded [[metric space]] is a sequence of points in the space, where the first point is selected arbitrarily and each successive point is as far as possible from the set of previously-selected points. The same concept can also be applied to a [[finite set]] of geometric points, by restricting the selected points to belong to the set or equivalently by considering the finite metric space generated by these points. For a finite metric space or finite set of geometric points, the resulting sequence forms a [[permutation]] of the points, known as the '''greedy permutation'''.\n\nFarthest-point traversals have many applications, including the approximation of the [[traveling salesman problem]] and the [[metric k-center|metric ''k''-center]] problem. They may be constructed in [[polynomial time]], or (for low-dimensional [[Euclidean space]]s) approximated in near-[[linear time]].\n\n==Properties==\nFix a number ''k'', and consider the subset formed by the first ''k'' points of the farthest-first traversal of any metric space. Let ''r'' be the distance between the final point of the prefix and the set of previously-selected points. Then this subset has the following two properties:\n*All pairs of the selected points are at distance at least ''r'' from each other, and\n*All points of the whole metric space are at distance at most ''r'' from the subset.\nIn other words, each [[prefix]] of the farthest-first traversal forms a [[Delone set]].<ref name=\"gonzalez\">{{citation\n | last = Gonzalez | first = T. F. | authorlink = Teofilo F. Gonzalez\n | doi = 10.1016/0304-3975(85)90224-5\n | issue = 2-3\n | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]\n | mr = 807927\n | pages = 293–306\n | title = Clustering to minimize the maximum intercluster distance\n | volume = 38\n | year = 1985}}.</ref>\n\n==Applications==\nThe first use of the farthest-first traversal was by {{harvtxt|Rosenkrantz|Stearns|Lewis|1977}} in connection with [[heuristic (computer science)|heuristics]] for the [[travelling salesman problem]]. In the farthest-insertion heuristic, discussed by Rosenkrantz et al., a tour is built up incrementally, by adding one point at a time in the ordering given by a farthest-first traversal. To add each point to the traveling salesman tour of the previous points, this heuristic considers all possible ways of breaking one edge of the tour and replacing it by two edges through the new point, and chooses the cheapest of these replacements. Although Rosenkrantz et al. prove only a logarithmic [[approximation ratio]] for this method, they show that in practice it often works better than other insertion methods with better provable approximation ratios.<ref>{{citation\n | last1 = Rosenkrantz | first1 = D. J.\n | last2 = Stearns | first2 = R. E.\n | last3 = Lewis | first3 = P. M., II\n | doi = 10.1137/0206041\n | issue = 3\n | journal = [[SIAM Journal on Computing]]\n | mr = 0459617\n | pages = 563–581\n | title = An analysis of several heuristics for the traveling salesman problem\n | volume = 6\n | year = 1977}}.</ref>\n\nLater, the same sequence of points was popularized by {{harvtxt|Gonzalez|1985}}, who used it as part of a [[greedy algorithm|greedy]] [[approximation algorithm]] for the problem of finding ''k'' clusters that minimize the maximum [[diameter]] of a cluster. The same algorithm applies also, with the same approximation quality, to the [[metric k-center|metric ''k''-center]] problem. This problem is one of several formulations of [[cluster analysis]] and [[facility location]], in which the goal is to partition a given set of points into ''k'' different clusters, each with a chosen center point, such that the maximum distance from any point to the center of its cluster is minimized. For instance, this problem can be used to model the placement of fire stations within a city, in order to ensure that every address within the city can be reached quickly by a fire truck. Gonzalez described a clustering heuristic that selects as centers the first ''k'' points of a farthest-first traversal, and then assigns each of the input points to its nearest center. If ''r'' is the distance from the set of ''k'' selected centers to the next point at position ''k''&nbsp;+&nbsp;1 in the traversal, then this\nclustering achieves a distance of ''r''. However, the subset of ''k'' centers together with the next point are all at distance at least ''r'' from each other, and any ''k''-clustering would put two of these points into one cluster, so there is no clustering with distance better than ''r''/2. Thus, Gonzalez's heuristic gives an [[approximation ratio]] of&nbsp;2 for this problem.<ref name=\"gonzalez\"/> This heuristic, and the name \"farthest-first traversal\", are often incorrectly attributed to a different paper from the same time, {{harvtxt|Hochbaum|Shmoys|1985}}. However, Hochbaum and Shmoys used graph-theoretic techniques, not the farthest-first traversal, to obtain a different approximation algorithm for the metric ''k''-center with the same approximation ratio as Gonzalez's heuristic.<ref name=\"hs85\">{{citation\n | last1 = Hochbaum | first1 = Dorit S. | author1-link = Dorit S. Hochbaum\n | last2 = Shmoys | first2 = David B. | author2-link = David Shmoys\n | doi = 10.1287/moor.10.2.180\n | issue = 2\n | journal = [[Mathematics of Operations Research]]\n | mr = 793876\n | pages = 180–184\n | title = A best possible heuristic for the ''k''-center problem\n | volume = 10\n | year = 1985}}.</ref> For both the min-max diameter clustering problem and the metric ''k''-center problem, these approximations are optimal: the existence of a polynomial-time heuristic with any constant approximation ratio less than&nbsp;2 would imply that [[P = NP]].<ref name=\"gonzalez\"/><ref name=\"hs85\"/>\n\nAs well as for clustering, the farthest-first traversal can also be used in another type of facility location problem, the max-min facility dispersion problem, in which the goal is to choose the locations of ''k'' different facilities so that they are as far apart from each other as possible. More precisely, the goal in this problem is to choose ''k'' points from a given metric space or a given set of candidate points, in such a way as to maximize the minimum pairwise distance between the selected points. Again, this can be approximated by choosing the first ''k'' points of a farthest-first traversal. If ''r'' denotes the distance of the ''k''th point from all previous points, then every point of the metric space or the candidate set is within distance ''r'' of the first ''k''&nbsp;&minus;&nbsp;1 points. By the [[pigeonhole principle]], some two points of the optimal solution (whatever it is) must both be within distance ''r'' of the same point among these first ''k''&nbsp;&minus;&nbsp;1 chosen points, and (by the [[triangle inequality]]) within distance 2''r'' of each other. Therefore, the heuristic solution given by the farthest-first traversal is within a factor of two of optimal.<ref>{{citation\n | last = White | first = Douglas J.\n | doi = 10.1093/imaman/3.2.131\n | issue = 2\n | journal = IMA Journal of Mathematics Applied in Business and Industry\n | mr = 1154657\n | pages = 131–140 (1992)\n | title = The maximal-dispersion problem\n | volume = 3\n | year = 1991}}. White credits the use of the farthest-first traversal as a heuristic for this problem to {{citation|first=R. E.|last=Steuer|title=Multiple-Criteria Optimization: Theory, Computation, and Applications|publisher=Wiley|location=New York|year=1986}}.</ref><ref>{{citation\n | last = Tamir | first = Arie\n | doi = 10.1137/0404048\n | issue = 4\n | journal = SIAM Journal on Discrete Mathematics\n | mr = 1129392\n | pages = 550–567\n | title = Obnoxious facility location on graphs\n | volume = 4\n | year = 1991}}.</ref><ref>{{citation\n | last1 = Ravi | first1 = S. S.\n | last2 = Rosenkrantz | first2 = D. J.\n | last3 = Tayi | first3 = G. K.\n | doi = 10.1287/opre.42.2.299\n | issue = 2\n | journal = [[Operations Research (journal)|Operations Research]]\n | jstor = 171673\n | pages = 299–310\n | title = Heuristic and special case algorithms for dispersion problems\n | volume = 42\n | year = 1994}}.</ref>\n\nOther applications of the farthest-first traversal include [[color quantization]] (clustering the colors in an image to a smaller set of representative colors),<ref>{{citation\n | last = Xiang | first = Z.\n | doi = 10.1145/256157.256159\n | issue = 3\n | journal = [[ACM Transactions on Graphics]]\n | pages = 260–276\n | title = Color image quantization by minimizing the maximum intercluster distance\n | volume = 16\n | year = 1997}}.</ref>\n[[progressive scan]]ning of images (choosing an order to display the [[pixel]]s of an image so that prefixes of the ordering produce good lower-resolution versions of the whole image rather than filling in the image from top to bottom),<ref name=\"elpz\">{{citation\n | last1 = Eldar | first1 = Y.\n | last2 = Lindenbaum | first2 = M.\n | last3 = Porat | first3 = M.\n | last4 = Zeevi | first4 = Y. Y.\n | doi = 10.1109/83.623193\n | issue = 9\n | journal = [[IEEE Signal Processing Society|IEEE Transactions on Image Processing]]\n | pages = 1305–1315\n | title = The farthest point strategy for progressive image sampling\n | volume = 6\n | year = 1997| bibcode = 1997ITIP....6.1305E}}.</ref>\npoint selection in the [[probabilistic roadmap]] method for [[motion planning]],<ref>{{citation\n | last1 = Mazer | first1 = E.\n | last2 = Ahuactzin | first2 = J. M.\n | last3 = Bessiere | first3 = P.\n | doi = 10.1613/jair.468\n | journal = [[Journal of Artificial Intelligence Research]]\n | pages = 295–316\n | title = The Ariadne's clew algorithm\n | volume = 9\n | year = 1998}}.</ref>\nsimplification of [[point cloud]]s,<ref>{{citation\n | last1 = Moenning | first1 = C.\n | last2 = Dodgson | first2 = N. A.\n | contribution = A new point cloud simplification algorithm\n | title = 3rd IASTED International Conference on Visualization, Imaging, and Image Processing\n | year = 2003}}.</ref>\ngenerating masks for [[halftone]] images,<ref>{{citation\n | last1 = Gotsman | first1 = Craig\n | last2 = Allebach | first2 = Jan P.\n | editor1-last = Rogowitz | editor1-first = Bernice E.\n | editor2-last = Allebach | editor2-first = Jan P.\n | contribution = Bounds and algorithms for dither screens\n | contribution-url = http://www.cs.technion.ac.il/~gotsman/AmendedPubl/BoundsAndAlgorithms/BoundsAndAlg-scanned.pdf\n | doi = 10.1117/12.238746\n | pages = 483–492\n | series = Proc. SPIE\n | title = Human Vision and Electronic Imaging\n | volume = 2657\n | year = 1996}}.</ref><ref>{{citation\n | last1 = Shahidi | first1 = R.\n | last2 = Moloney | first2 = C.\n | last3 = Ramponi | first3 = G.\n | doi = 10.1155/S1110865704403217\n | journal = EURASIP Journal on Applied Signal Processing\n | pages = 1886–1898\n | title = Design of farthest-point masks for image halftoning\n | volume = 12\n | year = 2004| bibcode = 2004EJASP2004...45S}}.</ref>\n[[hierarchical clustering]],<ref>{{citation\n | last1 = Dasgupta | first1 = S.\n | last2 = Long | first2 = P. M.\n | doi = 10.1016/j.jcss.2004.10.006\n | issue = 4\n | journal = [[Journal of Computer and System Sciences]]\n | mr = 2136964\n | pages = 555–569\n | title = Performance guarantees for hierarchical clustering\n | volume = 70\n | year = 2005}}.</ref>\nfinding the similarities between [[polygon mesh]]es of similar surfaces,<ref>{{citation\n | last1 = Lipman | first1 = Y.\n | last2 = Funkhouser | first2 = T.\n | contribution = Möbius voting for surface correspondence\n | doi = 10.1145/1576246.1531378\n | pages = 72:1–72:12\n | title = Proc. ACM SIGGRAPH\n | year = 2009}}.</ref>\nunderwater robot task planning,<ref>{{citation\n | last1 = Girdhar | first1 = Y.\n | last2 = Giguère | first2 = P.\n | last3 = Dudek | first3 = G.\n | contribution = Autonomous adaptive underwater exploration using online topic modelling\n | contribution-url = http://cim.mcgill.ca/~yogesh/publications/iser2012.pdf\n | title = Proc. Int. Symp. Experimental Robotics\n | year = 2012}}.</ref>\n[[fault detection]] in [[sensor network]]s,<ref>{{citation\n | last1 = Altinisik | first1 = U.\n | last2 = Yildirim | first2 = M.\n | last3 = Erkan | first3 = K.\n | doi = 10.1021/ie201850k\n | issue = 32\n | journal = Ind. Eng. Chem. Res.\n | pages = 10641–10648\n | title = Isolating non-predefined sensor faults by using farthest first traversal algorithm\n | volume = 51\n | year = 2012}}.</ref>\nmodeling [[phylogenetic diversity]],<ref>{{citation\n | last1 = Bordewich | first1 = Magnus\n | last2 = Rodrigo | first2 = Allen\n | last3 = Semple | first3 = Charles\n | doi = 10.1080/10635150802552831\n | issue = 6\n | journal = Systematic Biology\n | pages = 825–834\n | title = Selecting taxa to save or sequence: Desirable criteria and a greedy solution\n | volume = 57\n | year = 2008}}.</ref>\nmatching vehicles in a heterogenous fleet to customer delivery requests,<ref>{{citation\n | last1 = Fisher | first1 = Marshall L.\n | last2 = Jaikumar | first2 = Ramchandran|authorlink2=Ramchandran Jaikumar\n | doi = 10.1002/net.3230110205\n | issue = 2\n | journal = Networks\n | mr = 618209\n | pages = 109–124\n | title = A generalized assignment heuristic for vehicle routing\n | volume = 11\n | year = 1981}}. As cited by {{citation\n | last1 = Gheysens | first1 = Filip\n | last2 = Golden | first2 = Bruce\n | last3 = Assad | first3 = Arjang\n | editor1-last = Gallo | editor1-first = Giorgio\n | editor2-last = Sandi | editor2-first = Claudio\n | contribution = A new heuristic for determining fleet size and composition\n | doi = 10.1007/bfb0121103\n | pages = 233–236\n | publisher = Springer\n | series = Mathematical Programming Studies\n | title = Netflow at Pisa\n | volume = 26\n | year = 1986}}.</ref>\nuniform distribution of [[geodesy|geodetic observatories]] on the Earth's surface<ref>{{citation\n | last = Hase | first = Hayo\n | editor1-last = Rummel | editor1-first = Reinhard\n | editor2-last = Drewes | editor2-first = Hermann\n | editor3-last = Bosch | editor3-first = Wolfgang\n |display-editors = 3 | editor4-last = Hornik | editor4-first = Helmut\n | contribution = New method for the selection of additional sites for the homogenisation of an inhomogeneous cospherical point distribution\n | doi = 10.1007/978-3-642-59745-9_35\n | pages = 180–183\n | publisher = Springer\n | series = International Association of Geodesy Symposia\n | title = Towards an Integrated Global Geodetic Observing System (IGGOS): IAG Section II Symposium Munich, October 5-9, 1998, Posters – Session B\n | year = 2000}}.</ref>\nor of other types of sensor network,<ref>{{citation\n | last1 = Vieira | first1 = Luiz Filipe M.\n | last2 = Vieira | first2 = Marcos Augusto M.\n | last3 = Ruiz | first3 = Linnyer Beatrys\n | last4 = Loureiro | first4 = Antonio A. F.\n | last5 = Silva | first5 = Diógenes Cecílio\n | last6 = Fernandes | first6 = Antônio Otávio\n | contribution = Efficient Incremental Sensor Network Deployment Algorithm\n | contribution-url = http://www.lbd.dcc.ufmg.br/colecoes/sbrc/2004/001.pdf\n | pages = 3–14\n | title = Proc. Brazilian Symp. Computer Networks\n | year = 2004}}.</ref>\ngeneration of virtual point lights in the instant radiosity [[Rendering (computer graphics)|computer graphics rendering]] method,<ref>{{citation\n | last1 = Laine | first1 = Samuli\n | last2 = Saransaari | first2 = Hannu\n | last3 = Kontkanen | first3 = Janne\n | last4 = Lehtinen | first4 = Jaakko\n | last5 = Aila | first5 = Timo\n | contribution = Incremental instant radiosity for real-time indirect illumination\n | doi = 10.2312/EGWR/EGSR07/277-286\n | isbn = 978-3-905673-52-4\n | location = Aire-la-Ville, Switzerland, Switzerland\n | pages = 277–286\n | publisher = Eurographics Association\n | title = Proceedings of the 18th Eurographics Conference on Rendering Techniques (EGSR'07)\n | year = 2007}}.</ref>\nand geometric [[range searching]] [[data structure]]s.<ref>{{citation\n | last1 = Abbar | first1 = S.\n | last2 = Amer-Yahia | first2 = S.\n | last3 = Indyk | first3 = P. | author3-link = Piotr Indyk\n | last4 = Mahabadi | first4 = S.\n | last5 = Varadarajan | first5 = K. R.\n | contribution = Diverse near neighbor problem\n | doi = 10.1145/2462356.2462401\n | pages = 207–214\n | title = Proceedings of the 29th Annual [[Symposium on Computational Geometry]]\n | year = 2013}}.</ref>\n\n==Algorithms==\nThe farthest-first traversal of a finite point set may be computed by a [[greedy algorithm]] that maintains the distance of each point from the previously selected points, performing the following steps:\n*Initialize the sequence of selected points to the empty sequence, and the distances of each point to the selected points to infinity.\n*While not all points have been selected, repeat the following steps:\n**Scan the list of not-yet-selected points to find a point ''p'' that has the maximum distance from the selected points.\n**Remove ''p'' from the not-yet-selected points and add it to the end of the sequence of selected points.\n**For each remaining not-yet-selected point ''q'', replace the distance stored for ''q'' by the minimum of its old value and the distance from ''p'' to ''q''.\nFor a set of ''n'' points, this algorithm takes ''O''(''n''<sup>2</sup>) steps and ''O''(''n''<sup>2</sup>) distance computations. A faster [[approximation algorithm]], given by {{harvtxt|Har-Peled|Mendel|2006}}, applies to any subset of points in a metric space with bounded [[doubling space|doubling dimension]], a class of spaces that include the [[Euclidean space]]s of bounded dimension. Their algorithm finds a sequence of points in which each successive point has distance within a 1&nbsp;&minus;&nbsp;''&epsilon;'' factor of the farthest distance from the previously-selected point, where ''&epsilon;'' can be chosen to be any positive number. It takes time ''O''(''n''&nbsp;log&nbsp;''n'').<ref>{{citation\n | last1 = Har-Peled | first1 = S. | author1-link = Sariel Har-Peled\n | last2 = Mendel | first2 = M.\n | doi = 10.1137/S0097539704446281\n | issue = 5\n | journal = [[SIAM Journal on Computing]]\n | mr = 2217141\n | pages = 1148–1184\n | title = Fast construction of nets in low-dimensional metrics, and their applications\n | volume = 35\n | year = 2006| arxiv = cs/0409057\n }}.</ref>\n\nFor selecting points from a continuous space such as the [[Euclidean plane]], rather than a finite set of candidate points, these methods will not work directly, because there would be an infinite number of distances to maintain. Instead, each new point should be selected as the center of the [[largest empty sphere|largest empty circle]] defined by the previously-selected point set.<ref name=\"elpz\"/> This center will always lie on a vertex of the [[Voronoi diagram]] of the already selected points, or at a point where an edge of the Voronoi diagram crosses the domain boundary. In this formulation the method for constructing farthest-first traversals has also been called '''incremental Voronoi insertion'''.<ref>{{citation|title=Inserting points uniformly at every instance|journal=IEICE Transactions on Information and Systems|volume=E89-D|issue=8|pages=2348–2356|first1=Sachio|last1=Teramoto|first2=Tetsuo|last2=Asano|author2-link=Tetsuo Asano|first3=Naoki|last3=Katoh|first4=Benjamin|last4=Doerr|url=http://search.ieice.org/bin/summary.php?id=e89-d_8_2348|doi=10.1093/ietisy/e89-d.8.2348|bibcode=2006IEITI..89.2348T}}.</ref> It is similar to [[Ruppert's algorithm]] for [[Finite element method|finite element]] [[mesh generation]], but differs in the choice of which Voronoi vertex to insert at each step.<ref>{{citation | doi=10.1006/jagm.1995.1021 | first=Jim | last=Ruppert |  title=A Delaunay refinement algorithm for quality 2-dimensional mesh generation | journal=Journal of Algorithms | year=1995 | issue=3 | pages= 548–585 | volume=18}}.</ref>\n\n==See also==\n*[[Lloyd's algorithm]], a different method for generating evenly spaced points in geometric spaces\n\n==References==\n{{reflist|30em}}\n\n[[Category:Computational geometry]]\n[[Category:Approximation algorithms]]\n[[Category:Cluster analysis]]"
    },
    {
      "title": "Gap reduction",
      "url": "https://en.wikipedia.org/wiki/Gap_reduction",
      "text": "{{technical|date=December 2014}}\n\nIn [[computational complexity theory]], a '''gap reduction''' is a [[reduction (complexity)|reduction]] to a particular type of decision problem, known as a ''c-gap problem''. Such reductions provide information about the hardness of [[approximation algorithm|approximating]] solutions to [[optimization problem]]s.  In short, a gap problem refers to one wherein the objective is to distinguish between cases where the best solution is above one threshold from cases where the best solution is below another threshold, such that the two thresholds have a gap in between.  Gap reductions can be used to demonstrate inapproximability results, as if a problem may be approximated to a better factor than the size of gap, then the approximation algorithm can be used to solve the corresponding gap problem.\n\n==c-gap problem==\n\nWe define a '''c-gap problem''' as follows:<ref name=\"d\">{{cite web|first1=Erik|last1=Demaine|date=Fall 2014|title=Algorithmic Lower Bounds: Fun with Hardness Proofs Lecture 12 Notes|url=http://courses.csail.mit.edu/6.890/fall14/scribe/lec12.pdf}}</ref> given an optimization (maximization or minimization) problem P, the equivalent c-gap problem distinguishes between two cases, for an input k and an instance x of problem P:\n\n: <math>OPT_P(x) \\le k</math>. Here, the best solution to instance x of problem P has a cost, or score, below k.\n: <math>OPT_P(x) > c\\cdot k</math>. Here, the best solution to instance x of problem P has a cost above c⋅k.  The gap between the two thresholds is thus c.\n\nNote that whenever OPT falls between the thresholds, there is no requirement on what the output should be.  A valid algorithm for the c-gap problem may answer anything if OPT is in the middle of the gap.  The value c does not need to be constant; it can depend on the size of the instance of P. Note that [[Approximation algorithm|c-approximating]] the solution to an instance of P is at least as hard as solving the c-gap version of P.\n\nOne can define an '''(a,b)-gap problem''' similarly.  The difference is that the thresholds do not depend on the input; instead, the lower threshold is a and the upper threshold is b.\n\n==Gap-producing reduction==\n\nA gap-producing reduction is a [[reduction (complexity)|reduction]] from an optimization problem to a c-gap problem, so that solving the c-gap problem quickly would enable solving the optimization problem quickly.  The term gap-producing arises from the nature of the reduction: the optimal solution in the optimization problem maps to the opposite side of the gap from every other solution via reduction.  Thus, a gap is produced between the optimal solution and every other solution.\n\nA simple example of a gap-producing reduction is the nonmetric [[Traveling Salesman problem]] (i.e. where the graph's edge costs need not satisfy the conditions of a [[metric (mathematics)|metric]]). We can reduce from the [[Hamiltonian path]] problem on a given graph G = (V, E) to this problem as follows: we construct a complete graph G' = (V, E'), for the traveling salesman problem. For each edge e ∈ G', we let the cost of traversing it be 1 if e is in the original graph G and ∞ otherwise. A Hamiltonian path in the original graph G exists if and only if there exists a traveling salesman solution with weight (|V|-1).  However, if no such Hamiltonian path exists, then the best traveling salesman tour must have weight at least |V|.  Thus, Hamiltonian Path reduces to |V|/(|V|-1)-gap nonmetric traveling salesman.\n\n==Gap-preserving reduction==\n\nA gap-preserving reduction is a [[reduction (complexity)|reduction]] from a c-gap problem to a c'-gap problem. More specifically, we are given an instance x of a problem A with |x| = n and want to reduce it to an instance x' of a problem B with |x'| = n'. A gap-preserving reduction from A to B is a set of functions (k(n), k'(n'), c(n), c'(n')) such that\n\nFor minimization problems:\n: OPT<sub>A</sub>(x) ≤ k ⇒ OPT<sub>B</sub>(x') ≤ k', and\n: OPT<sub>A</sub>(x) ≥ c⋅k ⇒ OPT<sub>B</sub>(x') ≥ c'⋅k'\n\nFor maximization problems:\n: OPT<sub>A</sub>(x) ≥ k ⇒ OPT<sub>B</sub>(x') ≥ k', and\n: OPT<sub>A</sub>(x) ≤ k/c ⇒ OPT<sub>B</sub>(x') ≤ k'/c'\n\nIf c' > c, then this is a ''gap-amplifying reduction''.\n\n==Examples==\n\n===Max E3SAT===\n\nThis problem is a form of the [[Boolean satisfiability problem]] (SAT), where each clause contains three distinct literals and we want to maximize the number of clauses satisfied.<ref name=\"d\" />\n\nHåstad showed that the (1/2+ε, 1-ε)-gap version of a similar problem, MAX E3-X(N)OR-SAT, is NP-hard.<ref>{{cite web|first1=Hastad|last1=Johan|date=July 2001|title=Some Optimal Inapproximability Results|journal=J. ACM|publisher=ACM|volume=48|number=4|url=http://doi.acm.org/10.1145/502090.502098}}</ref> The MAX E3-X(N)OR-SAT problem is a form of SAT where each clause is the XOR of three distinct literals, exactly one of which is negated. We can reduce from MAX E3-X(N)OR-SAT to MAX E3SAT as follows:\n\n: A clause x<sub>i</sub> ⊕ x<sub>j</sub> ⊕ x<sub>k</sub> = 1 is converted to (x<sub>i</sub> ∨ x<sub>j</sub> ∨ x<sub>k</sub>) ∧ (¬x<sub>i</sub> ∨ ¬x<sub>j</sub> ∨ x<sub>k</sub>) ∧ (¬x<sub>i</sub> ∨ x<sub>j</sub> ∨ ¬x<sub>k</sub>) ∧ (x<sub>i</sub> ∨ ¬x<sub>j</sub> ∨ ¬x<sub>k</sub>)\n: A clause x<sub>i</sub> ⊕ x<sub>j</sub> ⊕ x<sub>k</sub> = 0 is converted to (¬x<sub>i</sub> ∨ ¬x<sub>j</sub> ∨ ¬x<sub>k</sub>) ∧ (¬x<sub>i</sub> ∨ x<sub>j</sub> ∨ x<sub>k</sub>) ∧ (x<sub>i</sub> ∨ ¬x<sub>j</sub> ∨ x<sub>k</sub>) ∧ (x<sub>i</sub> ∨ x<sub>j</sub> ∨ ¬x<sub>k</sub>)\n\nIf a clause is not satisfied in the original instance of MAX E3-X(N)OR-SAT, then at most three of the four corresponding clauses in our MAX E3SAT instance can be satisfied. Using a gap argument, it follows that a YES instance of the problem has at least a (1-ε) fraction of the clauses satisfied, while a NO instance of the problem has at most a (1/2+ε)(1) + (1/2-ε)(3/4) = (7/8 + ε/4)-fraction of the clauses satisfied. Thus, it follows that (7/8 + ε, 1 - ε)-gap MAX E3SAT is NP-hard. Note that this bound is tight, as a random assignment of variables gives an expected 7/8 fraction of satisfied clauses.\n\n===Label Cover===\nThe '''label cover''' problem is defined as follows: given a bipartite graph G = (A∪B, E), with\n\n: A = A<sub>1</sub> ∪ A<sub>2</sub> ∪ ... ∪ A<sub>k</sub>, |A| = n, and |A<sub>i</sub>| = n/k\n: B = B<sub>1</sub> ∪ B<sub>2</sub> ∪ ... ∪ B<sub>k</sub>, |B| = n, and |B<sub>i</sub>| = n/k\n\nWe define a \"superedge\" between A<sub>i</sub> and B<sub>j</sub> if at least one edge exists from A<sub>i</sub> to B<sub>j</sub> in G, and define the superedge to be covered if at least one edge from A<sub>i</sub> to B<sub>j</sub> is covered.\n\nIn the ''max-rep'' version of the problem, we are allowed to choose one vertex from each A<sub>i</sub> and each B<sub>i</sub>, and we aim to maximize the number of covered superedges. In the ''min-rep'' version, we are required to cover every superedge in the graph, and want to minimize the number of vertices we choose. Manurangsi and Moshkovitz show that the (O(n<sup>1/4</sup>), 1)-gap version of both problems is solvable in polynomial time.<ref>{{cite web|first1=Pasin|last1=Manurangsi|first2=Dana|last2=Moshkovitz|date=2013|title=Improved Approximation Algorithms for Projection Games|journal=ESA 2013|publisher=ESA|volume=8125|pages=683–694}}</ref>\n\n== See also ==\n* [[Approximation-preserving reduction]]\n* [[Optimization problem]]\n* [[Approximation algorithm]]\n* [[PTAS reduction]]\n\n==References==\n{{reflist}}\n\n[[Category:Approximation algorithms]]\n[[Category:Computational problems]]"
    },
    {
      "title": "GNRS conjecture",
      "url": "https://en.wikipedia.org/wiki/GNRS_conjecture",
      "text": "{{unsolved|mathematics|Do minor-closed graph families have <math>\\ell_1</math> embeddings with bounded distortion?}}\nIn [[theoretical computer science]] and [[metric geometry]], the '''GNRS conjecture''' connects the theory of [[graph minor]]s, the [[stretch factor]] of [[embedding]]s, and the [[approximation ratio]] of [[multi-commodity flow problem]]s. It is named after Anupam Gupta, Ilan Newman, Yuri Rabinovich, and [[Alistair Sinclair]], who formulated it in 2004.{{r|gnrs}}\n\n==Formulation==\nOne formulation of the conjecture involves embeddings of the [[shortest path problem|shortest path distances]] of weighted [[undirected graph]]s into [[Lp space|<math>\\ell_1</math> spaces]], real [[vector space]]s in which the distance between two vectors is the sum of their coordinate differences. If an embedding maps all pairs of vertices with distance <math>d</math> to pairs of vectors with distance in the range <math>[cd,Cd]</math> then its [[stretch factor]] or distortion is the ratio <math>C/c</math>; an [[isometry]] has stretch factor one, and all other embeddings have greater stretch factor.{{r|gnrs}}\n\nThe graphs that have an embedding with at most a given distortion are closed under [[graph minor]] operations, operations that delete vertices or edges from a graph or contract some of its edges. The GNRS conjecture states that, conversely, every nontrivial minor-closed family of graphs can be embedded into an <math>\\ell_1</math> space with bounded distortion. That is, the distortion of graphs in the family is bounded by a constant that depends on the family but not on the individual graphs. For instance, the [[planar graph]]s are closed under minors. Therefore, it would follow from the GNRS conjecture that the planar graphs have bounded distortion.{{r|gnrs}}\n\nAn alternative formulation involves analogues of the [[max-flow min-cut theorem]] for undirected [[multi-commodity flow problem]]s. The ratio of the maximum flow to the minimum cut, in such problems, is known as the ''flow-cut gap''. The largest flow-cut gap that a flow problem can have on a given graph equals the distortion of the optimal <math>\\ell_1</math> embedding of the graph.{{r|ad|llr}} Therefore, the GNRS conjecture can be rephrased as stating that the minor-closed families of graphs have bounded flow-cut gap.{{r|gnrs}}\n\n==Related results==\nArbitrary <math>n</math>-vertex graphs (indeed, arbitrary <math>n</math>-point [[metric space]]s) have <math>\\ell_1</math> embeddings with distortion <math>O(\\log n)</math>.{{r|b}} Some graphs have logarithmic flow-cut gap, and in particular this is true for a multicommodity flow with every pair of vertices having equal demand on a bounded-degree [[expander graph]].{{r|lr}} Therefore, this logarithmic bound on the distortion of arbitrary graphs is tight. [[Planar graph]]s can be embedded with smaller distortion, <math>O(\\sqrt{\\log n})</math>.{{r|r}}\n\nAlthough the GNRS conjecture remains unsolved, it has been proven for some minor-closed graph families that bounded-distortion embeddings exist.\nThese include the [[series-parallel graph]]s and the graphs of bounded [[circuit rank]],{{r|gnrs}} the graphs of bounded [[pathwidth]],{{r|ls}} the [[Clique-sum|2-clique-sums]] of graphs of bounded size,{{r|lp}} and the [[k-outerplanar graph|<math>k</math>-outerplanar graphs]].{{r|cgnrs}}\n\nIn contrast to the behavior of metric embeddings into <math>\\ell_1</math> spaces, every finite metric space has embeddings into <math>\\ell_2</math> with stretch arbitrarily close to one by the [[Johnson–Lindenstrauss lemma]], and into <math>\\ell_\\infty</math> spaces with stretch exactly one by the [[tight span]] construction.\n\n==See also==\n*[[Partial cube]], a class of graphs with distortion-free unweighted <math>\\ell_1</math>-embeddings\n\n==References==\n{{reflist|refs=\n\n<ref name=ad>{{citation\n | last1 = Avis | first1 = David | author1-link = David Avis\n | last2 = Deza | first2 = Michel | author2-link = Michel Deza\n | doi = 10.1002/net.3230210602\n | issue = 6\n | journal = Networks\n | mr = 1128272\n | pages = 595–617\n | title = The cut cone, <math>L^1</math> embeddability, complexity, and multicommodity flows\n | volume = 21\n | year = 1991}}</ref>\n\n<ref name=b>{{citation\n | last = Bourgain | first = J. | authorlink = Jean Bourgain\n | doi = 10.1007/BF02776078\n | issue = 1-2\n | journal = Israel Journal of Mathematics\n | mr = 815600\n | pages = 46–52\n | title = On Lipschitz embedding of finite metric spaces in Hilbert space\n | volume = 52\n | year = 1985}}</ref>\n\n<ref name=cgnrs>{{citation\n | last1 = Chekuri | first1 = Chandra\n | last2 = Gupta | first2 = Anupam\n | last3 = Newman | first3 = Ilan\n | last4 = Rabinovich | first4 = Yuri\n | last5 = Sinclair | first5 = Alistair | author5-link = Alistair Sinclair\n | doi = 10.1137/S0895480102417379\n | issue = 1\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 2257250\n | pages = 119–136\n | title = Embedding <math>k</math>-outerplanar graphs into <math>\\ell_1</math>\n | volume = 20\n | year = 2006}}</ref>\n\n<ref name=gnrs>{{citation\n | last1 = Gupta | first1 = Anupam\n | last2 = Newman | first2 = Ilan\n | last3 = Rabinovich | first3 = Yuri\n | last4 = Sinclair | first4 = Alistair | author4-link = Alistair Sinclair\n | doi = 10.1007/s00493-004-0015-x\n | issue = 2\n | journal = [[Combinatorica]]\n | mr = 2071334\n | pages = 233–269\n | title = Cuts, trees and <math>\\ell_1</math>-embeddings of graphs\n | volume = 24\n | year = 2004}}</ref>\n\n<ref name=llr>{{citation\n | last1 = Linial | first1 = Nathan | author1-link = Nati Linial \n | last2 = London | first2 = Eran\n | last3 = Rabinovich | first3 = Yuri\n | doi = 10.1007/BF01200757\n | issue = 2\n | journal = [[Combinatorica]]\n | mr = 1337355\n | pages = 215–245\n | title = The geometry of graphs and some of its algorithmic applications\n | volume = 15\n | year = 1995}}</ref>\n\n<ref name=lp>{{citation\n | last1 = Lee | first1 = James R.\n | last2 = Poore | first2 = Daniel E.\n | contribution = On the 2-sum embedding conjecture\n | doi = 10.1145/2462356.2492436\n | mr = 3208212\n | pages = 197–206\n | publisher = ACM | location = New York\n | title = Proceedings of the Twenty-Ninth Annual Symposium on Computational Geometry (SoCG '13)\n | year = 2013}}</ref>\n\n<ref name=lr>{{citation\n | last1 = Leighton | first1 = Tom | author1-link = F. Thomson Leighton\n | last2 = Rao | first2 = Satish\n | doi = 10.1145/331524.331526\n | issue = 6\n | journal = [[Journal of the ACM]]\n | mr = 1753034\n | pages = 787–832\n | title = Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms\n | volume = 46\n | year = 1999}}</ref>\n\n<ref name=ls>{{citation\n | last1 = Lee | first1 = James R.\n | last2 = Sidiropoulos | first2 = Anastasios\n | doi = 10.1007/s00493-013-2685-8\n | issue = 3\n | journal = [[Combinatorica]]\n | mr = 3144806\n | pages = 349–374\n | title = Pathwidth, trees, and random embeddings\n | volume = 33\n | year = 2013}}</ref>\n\n<ref name=r>{{citation\n | last = Rao | first = Satish\n | contribution = Small distortion and volume preserving embeddings for planar and Euclidean metrics\n | doi = 10.1145/304893.304983\n | mr = 1802217\n | pages = 300–306\n | publisher = ACM | location = New York\n | title = Proceedings of the Fifteenth Annual Symposium on Computational Geometry (SoCG '99)\n | year = 1999}}</ref>\n\n}}\n\n[[Category:Approximation algorithms]]\n[[Category:Conjectures]]\n[[Category:Graph minor theory]]\n[[Category:Metric geometry]]"
    },
    {
      "title": "Hardness of approximation",
      "url": "https://en.wikipedia.org/wiki/Hardness_of_approximation",
      "text": "In [[computer science]], '''hardness of approximation''' is a field  that studies the [[Computational complexity theory|algorithmic complexity]] of finding near-optimal solutions to [[optimization problem]]s.\n\n==Scope==\nHardness of approximation complements the study of [[approximation algorithm]]s by proving, for certain problems, a limit on the factors with which their solution can be efficiently approximated. Typically such limits show a factor of approximation beyond which a problem becomes [[NP-hard]], implying that finding a [[polynomial time]] approximation for the problem is impossible unless [[NP=P]]. Some hardness of approximation results, however, are based on other hypotheses, a notable one among which is the [[unique games conjecture]].\n\n==History==\nSince the early 1970s it was known that many optimization problems could not be solved in polynomial time unless [[P = NP]], but in many of these problems the optimal solution could be efficiently approximated to a certain degree. In the 1970s, [[Teofilo F. Gonzalez]] and [[Sartaj Sahni]] began the study of hardness of approximation, by showing that certain optimization problems were NP-hard even to approximate to within a given [[approximation ratio]]. That is, for these problems, there is a threshold such that any polynomial-time approximation with approximation ratio beyond this threshold could be used to solve NP-complete problems in polynomial time.<ref>{{citation\n | last1 = Sahni | first1 = Sartaj | author1-link = Sartaj Sahni\n | last2 = Gonzalez | first2 = Teofilo\n | issue = 3\n | journal = [[Journal of the ACM]]\n | mr = 0408313\n | pages = 555–565\n | title = ''P''-complete approximation problems\n | volume = 23\n | year = 1976 | doi=10.1145/321958.321975}}.</ref>  In the early 1990s, with the development of [[PCP (complexity)|PCP]] theory, it became clear that many more approximation problems were hard to approximate, and that (unless P = NP) many known approximation algorithms achieved the best possible approximation ratio.\n\nHardness of approximation theory deals with studying the approximation threshold of such problems.\n\n==Examples==\nFor an example of an NP-hard optimization problem that is hard to approximate, see [[set cover]].\n\n==See also==\n* [[PCP theorem]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{citation|url=http://cs.stanford.edu/people/trevisan/pubs/inapprox.pdf|first=Luca|last=Trevisan|authorlink=Luca Trevisan|title=Inapproximability of Combinatorial Optimization Problems|date=July 27, 2004}}\n\n==External links==\n* [http://www.cs.washington.edu/education/courses/533/05au/ CSE 533: The PCP Theorem and Hardness of Approximation, Autumn 2005], syllabus from the [[University of Washington]], [[Venkatesan Guruswami]] and Ryan O'Donnell\n\n[[Category:Approximation algorithms]]\n[[Category:Computational complexity theory]]\n[[Category:Relaxation (approximation)]]"
    },
    {
      "title": "K-approximation of k-hitting set",
      "url": "https://en.wikipedia.org/wiki/K-approximation_of_k-hitting_set",
      "text": "{{DISPLAYTITLE:k-approximation of k-hitting set}}\nIn [[computer science]], '''k-approximation of k-hitting set''' is an [[approximation algorithm]] for weighted [[Set_cover_problem#Hitting_set_formulation|hitting set]]. The input is a [[Collection (computing)|collection]] ''S'' of [[subset]]s of some universe ''T'' and a [[Map (mathematics)|mapping]] ''W'' from ''T'' to non-negative numbers called the weights of the elements of ''T''. In '''k-hitting set''' the size of the sets in ''S'' cannot be larger than ''k''. That is, <math>\\forall i \\in S: |i| \\leq k</math>. The problem is now to pick some subset ''T''<nowiki>'</nowiki> of ''T'' such that every set in ''S'' contains some element of ''T''<nowiki>'</nowiki>, and such that the total weight of all elements in ''T''<nowiki>'</nowiki> is as small as possible.\n\n==The algorithm==\nFor each set <math>j</math> in ''S'' is maintained a ''price'', <math>p_j</math>, which is initially 0. For an element ''a'' in ''T'', let ''S''(''a'') be the collection of sets from ''S'' containing ''a''. During the algorithm the following invariant is kept\n\n:<math>\\forall a \\in T: \\sum_{j \\in S(a)} p_j \\leq W(a).\\,</math>\n\nWe say that an element, ''a'', from ''T'' is ''tight'' if <math>\\Sigma_{j \\in S(a)} p_j = W(a)</math>. The main part of the algorithm consists of a loop: As long as there is a set in ''S'' that contains no element from ''T'' which is tight, the price of this set is increased as much as possible without violating the invariant above. When this loop exits, all sets contain some tight element. Pick all the tight elements to be the hitting set.\n\n==Correctness==\nThe algorithm always terminates because in each iteration of the loop the price of some set in ''S'' is increased enough to make one more element from ''T'' tight. If it cannot increase any price, it exits. It runs in polynomial time because the loop will not make more iterations than the number of elements in the union of all the sets of ''S''. It returns a hitting set, because when the loop exits, all sets in ''S'' contain a tight element from ''T'', and the set of these tight elements are returned.\n\nNote that for any hitting set ''T*'' and any prices <math>p_1, \\ldots, p_{|S|}</math> where the invariant from the algorithm is true, the total weight of the hitting set is an upper limit to the sum over all members of ''T*'' of the sum of the prices of sets containing this element, that is: <math>\\Sigma_{a \\in T^*} \\Sigma_{j \\in S(a)} p_j \\leq \\Sigma_{a \\in T^*} W(a)</math>. This follows from the invariant property. Further, since the price of every set must occur at least once on the left hand side, we get <math>\\Sigma_{j \\in S} p_j \\leq \\Sigma_{a \\in T^*} W(a)</math>. Especially, this property is true for the optimal hitting set.\n\nFurther, for the hitting set ''H'' returned from the algorithm above, we have <math>\\Sigma_{a \\in H} \\Sigma_{j \\in S(a)} p_j = \\Sigma_{a \\in H} W(a)</math>. Since any price <math>p_j</math> can appear at most ''k'' times in the left-hand side (since subsets of ''S'' can contain no more than ''k'' element from ''T'') we get <math>\\Sigma_{a \\in H} W(a) \\leq k \\cdot \\Sigma_{j \\in S} p_j</math> Combined with the previous paragraph we get <math>\\Sigma_{a \\in H} W(a) \\leq k \\cdot \\Sigma_{a \\in T^*} W(a)</math>, where ''T*'' is the optimal hitting set. This is exactly the guarantee that it is a k-approximation algorithm.\n\n==Relation to linear programming==\nThis algorithm is an instance of the [[primal-dual method]], also called '''the pricing method'''. The intuition is that it is [[duality (mathematics)|dual]] to a [[linear programming]] algorithm. For an explanation see http://algo.inria.fr/seminars/sem00-01/vazirani.html.\n\n==References==\n* {{cite book|first1=J.|last1=Kleinberg|author1-link=Jon Kleinberg|author2-link=Éva Tardos|first2=E.|last2=Tardos|title=Algorithm Design|year=2006|isbn=0-321-29535-8}}.\n* {{cite journal\n  | author    = S. Even\n  | authorlink= Shimon Even\n  |author2=R. Bar-Yehuda\n  | title     = A Linear-Time Approximation Algorithm for the Weighted Vertex Cover Problem\n  | journal   = J. Algorithms\n  | volume    = 2\n  | date      = 1981\n  | publisher = \n  | pages     = 198–203\n  | doi     = 10.1016/0196-6774(81)90020-1\n  | issue    = 2\n}}\n*{{cite book|first1=M. X.|last1=Goemans|author1-link=Michel Goemans|first2=D. P.|last2=Williamson|author2-link=David P. Williamson|contribution=The primal-dual method for approximation algorithms and its application to network design problems|title=Approximation Algorithms for NP-Hard Problems|editor-first=Dorit S.|editor-last=Hochbaum|editor-link=Dorit S. Hochbaum|publisher=PWS Publishing Company|year=1997|isbn=0-534-94968-1}}.\n\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Karloff–Zwick algorithm",
      "url": "https://en.wikipedia.org/wiki/Karloff%E2%80%93Zwick_algorithm",
      "text": "The '''Karloff–Zwick algorithm''', in [[computational complexity theory]], is a [[randomized algorithm|randomised]] [[approximation algorithm]] taking an instance of [[MAX-3SAT]] [[Boolean satisfiability problem]] as input.  If the instance is satisfiable, then the expected weight of the assignment found is at least 7/8 of optimal.  There is strong evidence (but not a [[mathematical proof]]) that the algorithm performs equally well on arbitrary MAX-3SAT instances.  [[Howard Karloff]] and [[Uri Zwick]] presented the algorithm in 1997.<ref name=\"Karloff\">{{citation|last1=Karloff|first1= H.|title= Proceedings 38th Annual Symposium on Foundations of Computer Science|last2= Zwick|first2= U. |chapter=A 7/8-approximation algorithm for MAX 3SAT?|title-link=Symposium on Foundations of Computer Science|year=1997|pages=406–415|doi=10.1109/SFCS.1997.646129|isbn= 978-0-8186-8197-4|citeseerx= 10.1.1.51.1351}}.</ref>\n\n==Comparison to random assignment==\nFor the related MAX-E3SAT problem, in which all clauses in the input 3SAT formula are guaranteed to have exactly three literals, the simple [[randomized algorithm|randomized]] [[approximation algorithm]] which assigns a truth value to each variable independently and uniformly at random satisfies 7/8 of all clauses in expectation, irrespective of whether the original formula is satisfiable.   Further, this simple algorithm can also be easily [[Randomized_algorithm#Derandomization|derandomized]] using the [[Method_of_conditional_probabilities#The_method_of_conditional_probabilities_with_conditional_expectations|method of conditional expectations]].  The Karloff–Zwick algorithm, however, does not require the restriction that the input formula should have three literals in every clause.<ref name=\"Karloff\"/>\n\n==Optimality==\nBuilding upon previous work on the [[PCP theorem]], [[Johan Håstad]] showed that, assuming P ≠ NP, no polynomial-time algorithm for MAX 3SAT can achieve a performance ratio exceeding 7/8, even when restricted to satisfiable instances of the problem in which each clause contains exactly three literals. Both the Karloff–Zwick algorithm and the above simple algorithm are therefore optimal in this sense.<ref>{{citation|first=J.|last=Hastad|title=Some optimal inapproximability results|journal=[[Journal of the ACM]]|volume=48|issue=4|year=2001|doi=10.1145/502090.502098|pages=798–859|citeseerx=10.1.1.638.2808}}.</ref>\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Karloff-Zwick algorithm}}\n[[Category:Approximation algorithms]]\n[[Category:Randomized algorithms]]"
    },
    {
      "title": "L-reduction",
      "url": "https://en.wikipedia.org/wiki/L-reduction",
      "text": "In [[computer science]], particularly the study of [[approximation algorithms]], an \n'''L-reduction''' (\"''linear reduction''\") is a transformation of [[optimization problem]]s which linearly preserves approximability features; it is one type of [[approximation-preserving reduction]]. L-reductions in studies of approximability of [[optimization problem]]s play a similar role to that of [[Polynomial-time reduction|polynomial reductions]] in the studies of [[Computational complexity theory|computational complexity]] of [[decision problem]]s.\n\nThe term ''L reduction'' is sometimes used to refer to [[log-space reduction]]s, by analogy with the complexity class [[L (complexity)|L]], but this is a different concept.\n\n==Definition==\nLet A and B be [[optimization problem]]s and c<sub>A</sub> and c<sub>B</sub> their respective cost functions. A pair of functions ''f'' and ''g'' is an L-reduction if all of the following conditions are met:\n* functions ''f'' and ''g'' are computable in [[polynomial time]],\n* if ''x'' is an instance of problem A, then ''f''(''x'') is an instance of problem B,\n* if ''y' '' is a solution to ''f''(''x''), then ''g''(''y' '') is a solution to ''x'',\n* there exists a positive constant α such that\n:<math>\\mathrm{OPT_B}(f(x)) \\le \\alpha \\mathrm{OPT_A}(x)</math>,\n* there exists a positive constant β such that for every solution ''y' '' to ''f''(''x'')\n:<math>|\\mathrm{OPT_A}(x)-c_A(g(y'))| \\le \\beta |\\mathrm{OPT_B}(f(x))-c_B(y')|</math>.\n\n==Properties==\n\n=== Implication of PTAS reduction ===\nAn L-reduction from problem A to problem B implies an [[Approximation-preserving reduction#AP-reduction|AP-reduction]] when A and B are minimization problems and a [[PTAS reduction]] when A and B are maximization problems.  In both cases, when B has a PTAS and there is a L-reduction from A to B, then A also has a PTAS.<ref name=Kann92>{{cite book\n | last1 = Kann | first1 = Viggo\n | year = 1992\n | title = On the Approximability of NP-complete \\mathrm{OPT}imization Problems\n | publisher = Royal Institute of Technology, Sweden\n | isbn = 978-91-7170-082-7\n}}</ref><ref name=Papadimitriou88>{{cite conference\n | author= Christos H. Papadimitriou\n |author2=Mihalis Yannakakis\n  | booktitle = STOC '88: Proceedings of the twentieth annual ACM Symposium on Theory of Computing\n | title = \\mathrm{OPT}imization, Approximation, and Complexity Classes\n | year = 1988\n | doi = 10.1145/62212.62233\n}}</ref>  This enables the use of L-reduction as a replacement for showing the existence of a PTAS-reduction; Crescenzi has suggested that the more natural formulation of L-reduction is actually more useful in many cases due to ease of usage.<ref name=crescenzi>{{cite journal|last1=Crescenzi|first1=Pierluigi|title=A Short Guide To Approximation Preserving Reductions|journal=Proceedings of the 12th Annual IEEE Conference on Computational Complexity|date=1997|pages=262–|url=http://dl.acm.org/citation.cfm?id=792302|publisher=IEEE Computer Society|location=Washington, D.C.}}</ref>\n\n==== Proof (minimization case) ====\nLet the approximation ratio of B be <math>1 + \\delta</math>.\nBegin with the approximation ratio of A, <math>\\frac{c_A(y)}{\\mathrm{OPT}_A(x)}</math>.  \nWe can remove absolute values around the third condition of the L-reduction definition since we know A and B are minimization problems.  Substitute that condition to obtain\n: <math>\\frac{c_A(y)}{\\mathrm{OPT}_A(x)} \\le \\frac{\\mathrm{OPT}_A(x) + \\beta(c_B(y') - \\mathrm{OPT}_B(x'))}{\\mathrm{OPT}_A(x)}</math>\nSimplifying, and substituting the first condition, we have\n: <math>\\frac{c_A(y)}{\\mathrm{OPT}_A(x)} \\le 1 + \\alpha \\beta \\left( \\frac{c_B(y')-\\mathrm{OPT}_B(x')}{\\mathrm{OPT}_B(x')} \\right)</math>\nBut the term in parentheses on the right-hand side actually equals <math>\\delta</math>.  Thus, the approximation ratio of A is <math>1 + \\alpha\\beta\\delta</math>.\n\nThis meets the conditions for AP-reduction.\n\n==== Proof (maximization case) ====\n\nLet the approximation ratio of B be <math>\\frac{1}{1 - \\delta'}</math>.\nBegin with the approximation ratio of A, <math>\\frac{c_A(y)}{\\mathrm{OPT}_A(x)}</math>.  \nWe can remove absolute values around the third condition of the L-reduction definition since we know A and B are maximization problems.  Substitute that condition to obtain\n: <math>\\frac{c_A(y)}{\\mathrm{OPT}_A(x)} \\ge \\frac{\\mathrm{OPT}_A(x) - \\beta(c_B(y') - \\mathrm{OPT}_B(x'))}{\\mathrm{OPT}_A(x)}</math>\nSimplifying, and substituting the first condition, we have\n: <math>\\frac{c_A(y)}{\\mathrm{OPT}_A(x)} \\ge 1 - \\alpha \\beta \\left( \\frac{c_B(y')-\\mathrm{OPT}_B(x')}{\\mathrm{OPT}_B(x')} \\right)</math>\nBut the term in parentheses on the right-hand side actually equals <math>\\delta'</math>.  Thus, the approximation ratio of A is <math>\\frac{1}{1 - \\alpha\\beta\\delta'}</math>.\n\nIf <math>\\frac{1}{1 - \\alpha\\beta\\delta'} = 1+\\epsilon</math>, then <math>\\frac{1}{1 - \\delta'} = 1 + \\frac{\\epsilon}{\\alpha\\beta(1+\\epsilon) - \\epsilon}</math>, which meets the requirements for PTAS reduction but not AP-reduction.\n\n=== Other properties ===\n\nL-reductions also imply [[Approximation-preserving reduction#A-reduction and P-reduction|P-reduction]].<ref name=crescenzi></ref>  One may deduce that L-reductions imply PTAS reductions from this fact and the fact that P-reductions imply PTAS reductions.\n\nL-reductions preserve membership in APX for the minimizing case only, as a result of implying AP-reductions.\n\n== Examples ==\n\n* [[Dominating set#L-reductions|Dominating set]]: an example with α&nbsp;=&nbsp;β&nbsp;=&nbsp;1\n* [[Token reconfiguration#Approximation|Token reconfiguration]]: an example with α&nbsp;=&nbsp;1/5, β&nbsp;=&nbsp;2\n\n==See also==\n* [[MAXSNP]]\n* [[Approximation-preserving reduction]]\n* [[PTAS reduction]]\n\n==References==\n{{reflist}}\n* G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann, A. Marchetti-Spaccamela, M. Protasi. Complexity and Approximation. Combinatorial optimization problems and their approximability properties. 1999, Springer. {{isbn|3-540-65431-3}}\n\n{{comp-sci-theory-stub}}\n\n[[Category:Reduction (complexity)]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Max/min CSP/Ones classification theorems",
      "url": "https://en.wikipedia.org/wiki/Max%2Fmin_CSP%2FOnes_classification_theorems",
      "text": "In [[computational complexity theory]], a branch of [[computer science]], the '''Max/min CSP/Ones classification theorems''' state necessary and sufficient conditions that determine the complexity classes of problems about satisfying a subset ''S'' of boolean relations.<ref name=kstw>{{cite journal|first1=Sanjeev|last1=Khanna|first2=Madhu|last2=Sudan|first3=Luca|last3=Trevisan|first4=David|last4=Williamson|date=Mar 2000|title=The Approximability of Constraint Satisfaction Problems|journal=SIAM J. Comput.|publisher=SIAM|volume=30 | number=6 |pages=1863–1920| url=http://people.csail.mit.edu/madhu/papers/2001/kstw.pdf}}</ref> They are similar to [[Schaefer's dichotomy theorem]], which classifies the complexity of satisfying finite sets of relations; however, the Max/min CSP/Ones classification theorems give information about the complexity of [[approximation algorithm|approximating]] an optimal solution to a problem defined by ''S''.\n\nGiven a set ''S'' of clauses, the ''Max constraint satisfaction problem (CSP)'' is to find the maximum number of satisfiable (possibly weighted) clauses in ''S''. Similarly, the ''Min CSP problem'' is to minimize this number of clauses. The ''Max Ones problem'' is to maximize the number of boolean variables in ''S'' that are set to 1 when all clauses are satisfied, and the ''Min Ones problem'' is to minimize this number.<ref>{{cite web|first1=Erik|last1=Demaine|date=Fall 2014|title=Algorithmic Lower Bounds: Fun with Hardness Proofs Lecture 11 Notes|url=http://courses.csail.mit.edu/6.890/fall14/scribe/lec11.pdf}}</ref>\n\nWhen using the classifications below, '''the problem's complexity class is determined by the topmost classification that it satisfies'''.\n\n== Definitions ==\n\nWe define for brevity some terms here, which are used in the classifications below.\n\n* '''PO''' stands for [[Time complexity|Polynomial time]] optimizable; problems for which finding the optimum can be done in polynomial time, so that approximation to arbitrary precision can also clearly be done in polynomial time.\n* [[Conjunctive normal form]] is abbreviated '''CNF''' below.\n* '''X(N)OR-SAT''' stands for a satisfiability problem which is the AND of several boolean linear equations that can be written as XOR clauses. Exactly one literal in each XOR clause must be negated (e.g. <math>x_1 \\oplus \\lnot x_2 \\oplus x_3 = 1</math>).  See [[XOR-SAT#XOR-Satisfiability|XOR-SAT]].\n* '''Min UnCut-complete''' refers to a complexity class historically defined in terms of a problem named Min UnCut.  Such problems are [[APX|APX-hard]] but with an <math>O(\\sqrt{\\log n})</math> factor approximation.<ref name=acmm>{{cite conference  |url=http://dl.acm.org/citation.cfm?id=1060675 |title=<math>O(\\sqrt{\\log n})</math> approximation algorithms for min UnCut, min 2CNF deletion, and directed cut problems |last1=Agarwal |first1=Amit |last2=Charikar |first2=Moses |last3=Makarychev |first3=Konstantin |last4=Makarychev |first4=Yury |date=2005 |publisher=ACM |book-title=Proceedings of the Thirty-seventh Annual ACM Symposium on Theory of Computing |series=STOC '05 |pages= 573–581 |location=New York, NY, USA}}</ref>\n* '''Min 2CNF-Deletion-complete''' is another complexity class historically defined via a problem.  Such problems are APX-hard but with an <math>O(\\sqrt{\\log n})</math> approximation.<ref name=acmm />\n* '''Nearest Codeword-complete''' is yet another such complexity class.  Such problems are inapproximable to within a <math>2^{\\log^{1-\\epsilon}(n)}</math> factor for some <math>\\epsilon</math>.\n* '''Min Horn-Deletion-complete''' is yet another such complexity class.  Such problems are inapproximable to within a <math>2^{\\log^{1-\\epsilon}(n)}</math> factor for some <math>\\epsilon</math>, but are in Poly-APX, so they have some polynomial factor approximation.\n\n== Classification theorems ==\n\n=== Max CSP ===\nThe following conditions comprise the classification theorem for Max CSP problems.<ref name=kstw />\n\n# If setting all variables true or all variables false satisfies all clauses, it is in PO.\n# If all clauses, when converted to [[disjunctive normal form]], have two terms, one consisting of all positive (unnegated) variables and the other all negated variables, it is in PO.\n# Otherwise, the problem is [[APX|APX-complete]].\n\n=== Max Ones ===\nThe following conditions comprise the classification theorem for Max Ones problems.<ref name=kstw />\n\n# If setting all variables true satisfies all clauses, it is in PO.\n# If each clause can be written as the CNF of [[dual-Horn clause|Dual-Horn subclauses]], it is in PO.\n# If it is an instance of 2-X(N)OR-SAT, which is X(N)OR-SAT with two variables per linear equation, it is in PO.\n# If it is an instance of X(N)OR-SAT but not 2-X(N)OR-SAT, it is APX-complete.\n# If each clause can be written as the CNF of [[Horn clause|Horn subclauses]], it is [[Poly-APX-complete]].\n# If it is an instance of [[2-CNF-SAT]], it is Poly-APX-complete.\n# If setting all or all but one variable false satisfies each clause, it is Poly-APX-complete.\n# It is [[NP-hard]] to distinguish between an answer of 0 and a nonzero answer if setting all variables false satisfies all clauses.\n# Otherwise, it is NP-hard to find even a feasible solution.\n\n=== Min CSP ===\nThe following conditions comprise the classification theorem for Min CSP problems.<ref name=kstw />\n\n# If setting all variables false or all variables true satisfies all clauses, it is in PO.\n# If all clauses, when converted to [[disjunctive normal form]], have two terms, one consisting of all positive (unnegated) variables and the other all negated variables, it is in PO.\n# If all clauses are the OR of O(1) variables, it is APX-complete.\n# If it is an instance of 2-X(N)OR-SAT, it is Min UnCut-complete.\n# If it is an instance of X(N)OR-SAT but not 2-X(N)OR-SAT, it is Nearest Codeword-complete.\n# If it is an instance of [[2-CNF-SAT]], it is Min 2CNF-Deletion-complete.\n# If all clauses are [[Horn clause|Horn]] or [[dual-Horn clause|Dual-Horn]], it is Min Horn Deletion-complete.\n# Otherwise, distinguishing between an answer of 0 and a nonzero answer is NP-complete.\n\n=== Min Ones ===\nThe following conditions comprise the classification theorem for Min Ones problems.<ref name=kstw />\n\n# If setting all variables false satisfies all clauses, it is in PO.\n# If each clause can be written as a CNF of [[Horn clause|Horn subclauses]], it is in PO.\n# If it is an instance of 2-X(N)OR-SAT, it is in PO.\n# If it is an instance of [[2-CNF-SAT]], it is APX-complete.\n# If all clauses are the OR of O(1) variables, it is APX-complete.\n# If it is an instance of X(N)OR-SAT but not 2-X(N)OR-SAT, it is Nearest Codeword-complete.\n# If each clause can be written as a CNF of [[dual-Horn clause|Dual-Horn subclauses]], it is Min Horn Deletion-complete.\n# If setting all variables true satisfies all clauses, it is Poly-APX-complete.\n# Otherwise, it is NP-Hard to even find a feasible solution.\n\n== See also ==\n* [[Boolean satisfiability problem]]\n* [[APX]]\n* [[MaxSNP]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Max min CSP Ones classification theorems}}\n[[Category:Approximation algorithms]]\n[[Category:Constraint programming]]\n[[Category:Theorems in computational complexity theory]]"
    },
    {
      "title": "Method of conditional probabilities",
      "url": "https://en.wikipedia.org/wiki/Method_of_conditional_probabilities",
      "text": "In [[mathematics]] and [[computer science]], the [[probabilistic method]] is used to prove the existence of mathematical objects with desired combinatorial properties. The proofs are probabilistic — they work by showing that a random object, chosen from some probability distribution, has the desired properties with positive probability. Consequently, they are [[nonconstructive proof|nonconstructive]] — they don't explicitly describe an efficient method for computing the desired objects.\n\nThe '''method of conditional probabilities''' {{harv|Spencer|1987}}, {{harv|Raghavan|1988}} converts such a proof, in a \"very precise sense\", into an efficient [[deterministic algorithm]], one that is guaranteed to compute an object with the desired properties. That is, the method [[derandomization|derandomizes]] the proof. The basic idea is to replace each random choice in a random experiment by a deterministic choice, so as to keep the conditional probability of failure, given the choices so far, below 1.\n\nThe method is particularly relevant in the context of [[randomized rounding]] (which uses the probabilistic method to design [[approximation algorithm]]s).\n\nWhen applying the method of conditional probabilities, the technical term '''pessimistic estimator''' refers to a quantity used in place of the true conditional probability (or conditional expectation) underlying the proof.\n\n== Overview ==\n{{harv|Raghavan|1988}}  gives this description:\n\n: ''We first show the existence of a provably good approximate solution using the [[probabilistic method]]... [We then] show that the probabilistic existence proof can be converted, in a very precise sense, into a deterministic approximation algorithm.''\n\n(Raghavan is discussing the method in the context of [[randomized rounding]], but it works with the probabilistic method in general.)\n\n[[File:Method of conditional probabilities.png|thumb|450px|border|right|The method of conditional probabilities]]\n\nTo apply the method to a probabilistic proof, the randomly chosen object in the proof must be choosable by a random experiment that consists of a sequence of \"small\" random choices.\n\nHere is a trivial example to illustrate the principle.\n\n: '''Lemma:''' ''It is possible to flip three coins so that the number of tails is at least 2.''\n: ''Probabilistic proof.'' If the three coins are flipped randomly, the expected number of tails is 1.5.  Thus, there must be some outcome (way of flipping the coins) so that the number of tails is at least 1.5.  Since the number of tails is an integer, in such an outcome there are at least 2 tails.  ''QED''\n\nIn this example the random experiment consists of flipping three fair coins. The experiment is illustrated by the rooted tree in the adjacent diagram. There are eight outcomes, each corresponding to a leaf in the tree. A trial of the random experiment corresponds to taking a random walk from the root (the top node in the tree, where no coins have been flipped) to a leaf. The successful outcomes are those in which at least two coins came up tails. The interior nodes in the tree correspond to partially determined outcomes, where only 0, 1, or 2 of the coins have been flipped so far.\n\nTo apply the method of conditional probabilities, one focuses on the ''conditional probability of failure, given the choices so far'' as the experiment proceeds step by step.\nIn the diagram, each node is labeled with this conditional probability. (For example, if only the first coin has been flipped, and it comes up tails, that corresponds to the second child of the root. Conditioned on that partial state, the probability of failure is 0.25.)\n\nThe method of conditional probabilities replaces the random root-to-leaf walk  in the random experiment by a deterministic root-to-leaf walk, where each step is chosen to inductively maintain the following invariant:\n\n:: ''the conditional probability of failure, given the current state, is less than 1.''\n\nIn this way, it is guaranteed to arrive at a leaf with label 0, that is, a successful outcome.\n\nThe invariant holds initially (at the root), because the original proof showed that the (unconditioned) probability of failure is less than 1. The conditional probability at any interior node is the average of the conditional probabilities of its children. The latter property is important because it implies that ''any interior node whose conditional probability is less than 1 has at least one child whose conditional probability is less than 1.'' Thus, from any interior node, one can always choose some child to walk to so as to maintain the invariant. Since the invariant holds at the end, when the walk arrives at a leaf and all choices have been determined, the outcome reached in this way must be a successful one.\n\n== Efficiency ==\n\nIn a typical application of the method, the goal is to be able to implement the resulting deterministic process by a reasonably efficient algorithm (the word \"efficient\" usually means an algorithm that runs in  [[polynomial time]]), even though typically the number of possible outcomes is huge (exponentially large). For example, consider the task with coin flipping, but extended to ''n'' flips for large ''n''.\n\nIn the ideal case, given a partial state (a node in the tree), the conditional probability of failure (the label on the node) can be efficiently and exactly computed. (The example above is like this.) If this is so, then the algorithm can select the next node to go to by computing the conditional probabilities at each of the children of the current node, then moving to any child whose conditional probability is less than 1. As discussed above, there is guaranteed to be such a node.\n\nUnfortunately, in most applications, the conditional probability of failure is not easy to compute efficiently. There are two standard and related techniques for dealing with this:\n\n* '''Using a conditional expectation:''' Many probabilistic proofs work as follows: they implicitly define a random variable ''Q'', show that (i) the expectation of ''Q'' is at most (or at least) some threshold value, and (ii) in any outcome where ''Q'' is at most (at least) this threshold, the outcome is a success.  Then (i) implies that there exists an outcome where ''Q'' is at most the threshold, and this and (ii) imply that there is a successful outcome. (In the example above, ''Q'' is the number of tails, which should be at least the threshold 1.5.  In many applications, ''Q'' is the number of \"bad\" events (not necessarily disjoint) that occur in a given outcome, where each bad event corresponds to one way the experiment can fail, and the expected number of bad events that occur is less than 1.)\n\nIn this case, to keep the conditional probability of failure below 1, it suffices to keep the conditional expectation of ''Q'' below (or above) the threshold.  To do this, instead of computing the conditional probability of failure, the algorithm computes the conditional expectation of ''Q'' and proceeds accordingly: at each interior node, there is some child whose conditional expectation is at most (at least) the node's conditional expectation; the algorithm moves from the current node to such a child, thus keeping the conditional expectation below (above) the threshold.\n\n* '''Using a pessimistic estimator:''' In some cases, as a proxy for the exact conditional expectation of the quantity ''Q'', one uses an appropriately tight bound called a ''pessimistic estimator''. The pessimistic estimator is a function of the current state. It should be an upper (or lower) bound for the conditional expectation of ''Q'' given the current state, and it should be non-increasing (or non-decreasing) in expectation with each random step of the experiment. Typically, a good pessimistic estimator can be computed by precisely deconstructing the logic of the original proof.\n\n== Example using conditional expectations ==\n\nThis example demonstrates the method of conditional probabilities using a conditional expectation.\n\n=== Max-Cut Lemma ===\n\nGiven any undirected [[Graph (discrete mathematics)|graph]] ''G'' = (''V'', ''E''), the [[Max cut]] problem is to color each vertex of the graph with one of two colors (say black or white) so as to maximize the number of edges whose endpoints have different colors. (Say such an edge is ''cut''.)\n\n'''Max-Cut Lemma:''' In any graph ''G'' = (''V'', ''E''), at least |''E''|/2 edges can be cut.\n\n<blockquote>'''Probabilistic proof.''' Color each vertex black or white by flipping a fair coin. By calculation, for any edge e in ''E'', the probability that it is cut is 1/2. Thus, by [[Expected value#Linearity|linearity of expectation]], the expected number of edges cut is |''E''|/2. Thus, there exists a coloring that cuts at least |''E''|/2 edges. ''QED''</blockquote>\n\n=== The method of conditional probabilities with conditional expectations ===\n\nTo apply the method of conditional probabilities, first model the random experiment as a sequence of small random steps. In this case it is natural to consider each step to be the choice of color for a particular vertex (so there are |''V''| steps).\n\nNext, replace the random choice at each step by a deterministic choice, so as to keep the conditional probability of failure, given the vertices colored so far, below 1.  (Here ''failure'' means that finally fewer than |''E''|/2 edges are cut.)\n\nIn this case, the conditional probability of failure is not easy to calculate. Indeed, the original proof did not calculate the probability of failure directly; instead, the proof worked by showing that the expected number of cut edges was at least |''E''|/2.\n\nLet random variable ''Q'' be the number of edges cut. To keep the conditional probability of failure below 1, it suffices to keep the conditional expectation of ''Q''  at or above the threshold |''E''|/2. (This is because as long as the conditional expectation of ''Q'' is at least |''E''|/2, there must be some still-reachable outcome where ''Q'' is at least |''E''|/2, so the conditional probability of reaching such an outcome is positive.) To keep the conditional expectation of ''Q'' at |''E''|/2 or above, the algorithm will, at each step, color the vertex under consideration so as to ''maximize'' the resulting conditional expectation of ''Q''. This suffices, because there must be some child whose conditional expectation is at least the current state's conditional expectation  (and thus at least |''E''|/2).\n\nGiven that some of the vertices are colored already, what is this conditional expectation? Following the logic of the original proof, the conditional expectation of the number of cut edges is\n\n:: ''the number of edges whose endpoints are colored differently so far''\n:: + (1/2)*(''the number of edges with at least one endpoint not yet colored'').\n\n=== Algorithm ===\n\nThe algorithm colors each vertex to maximize the resulting value of the above conditional expectation. This is guaranteed to keep the conditional expectation at |''E''|/2 or above, and so is guaranteed to keep the conditional probability of failure below 1, which in turn guarantees a successful outcome. By calculation, the algorithm simplifies to the following:\n\n  1. For each vertex ''u'' in ''V'' (in any order):\n  2.   Consider the already-colored neighboring vertices of ''u''.\n  3.       Among these vertices, if more are black than white, then color ''u'' white.\n  4.   Otherwise, color ''u'' black.\n\nBecause of its derivation, this deterministic algorithm is guaranteed to cut at least half the edges of the given graph. (This makes it a [[Maximum cut#Approximation algorithms|0.5-approximation algorithm for Max-cut]].)\n\n== Example using pessimistic estimators ==\n\nThe next example demonstrates the use of pessimistic estimators.\n\n=== Turán's theorem <!-- linked to from [[Randomized rounding#Comparison to other applications of the probabilistic method]] and from [[Turán's theorem#See also]] --> ===\n\nOne way of stating [[Turán's theorem]] is the following:\n\n: Any graph ''G'' = (''V'', ''E'') contains an [[Independent set (graph theory)|independent set]] of size at least |''V''|/(''D''+1), where ''D'' = 2|''E''|/|''V''| is the average degree of the graph.\n\n=== Probabilistic proof of Turán's theorem ===\n\nConsider the following random process for constructing an independent set ''S'':\n  1. Initialize ''S'' to be the empty set.\n  2. For each vertex ''u'' in ''V'' in random order:\n  3.    If no neighbors of ''u'' are in ''S'', add ''u'' to ''S''\n  4. Return ''S''.\nClearly the process computes an independent set. Any vertex ''u'' that is considered before all of its neighbors will be added to ''S''. Thus, letting ''d''(''u'') denote the degree of ''u'', the probability that ''u'' is added to ''S'' is at least 1/(''d''(''u'')+1). By [[Expected value#Linearity|linearity of expectation]], the expected size of ''S'' is at least\n\n: <math>\\sum_{u\\in V} \\frac{1}{d(u)+1} ~\\ge~\\frac{|V|}{D+1}.</math>\n\n(The inequality above follows because 1/(''x''+1) is [[Convex function|convex]] in ''x'', so the left-hand side is minimized, subject to the sum of the degrees being fixed at 2|''E''|, when each ''d''(''u'') = ''D'' = 2|''E''|/|''V''|.) ''QED''\n\n=== The method of conditional probabilities using pessimistic estimators ===\n\nIn this case, the random process has |''V''| steps. Each step considers some not-yet considered vertex ''u'' and adds ''u'' to ''S'' if none of its neighbors have yet been added. Let random variable ''Q'' be the number of vertices added to ''S''. The proof shows that ''E''[''Q''] ≥ |''V''|/(''D''+1).\n\nWe will replace each random step by a deterministic step that keeps the conditional expectation of ''Q'' at or above |''V''|/(''D''+1). This will ensure a successful outcome, that is, one in which the independent set ''S'' has size at least |''V''|/(''D''+1), realizing the bound in Turán's theorem.\n\nGiven that the first t steps have been taken, let ''S''<sup>(''t'')</sup> denote the vertices added so far. Let ''R''<sup>(''t'')</sup> denote those vertices that have not yet been considered, and that have no neighbors in ''S''<sup>(''t'')</sup>. Given the first t steps, following the reasoning in the original proof, any given vertex ''w'' in ''R''<sup>(''t'')</sup> has conditional probability at least 1/(''d''(''w'')+1) of being added to ''S'', so the conditional expectation of ''Q'' is ''at least''\n\n: <math>|S^{(t)}| ~+~ \\sum_{w\\in R^{(t)}} \\frac{1}{d(w)+1}. </math>\n\nLet ''Q''<sup>(''t'')</sup> denote the above quantity, which is called a '''pessimistic estimator''' for the conditional expectation.\n\nThe proof showed that the pessimistic estimator is initially at least |''V''|/(''D''+1). (That is, ''Q''<sup>(0)</sup> ≥ |''V''|/(''D''+1).) The algorithm will make each choice to keep the pessimistic estimator from decreasing, that is, so that ''Q''<sup>(''t''+1)</sup> ≥ ''Q''<sup>(''t'')</sup> for each ''t''. Since the pessimistic estimator is a lower bound on the conditional expectation, this will ensure that the conditional expectation stays above |''V''|/(''D''+1), which in turn will ensure that the conditional probability of failure stays below 1.\n\nLet ''u'' be the vertex considered by the algorithm in the next ((''t''+1)-st) step.\n\nIf ''u'' already has a neighbor in ''S'', then ''u'' is not added to ''S'' and (by inspection of ''Q''<sup>(''t'')</sup>), the pessimistic estimator is unchanged. If ''u'' does ''not'' have a neighbor in ''S'',  then ''u'' is added to ''S''.\n\nBy calculation, if ''u'' is chosen randomly from the remaining vertices, the expected increase in the pessimistic estimator is non-negative. ['''The calculation.''' Conditioned on choosing a vertex in ''R''<sup>(''t'')</sup>, the probability that a given term 1/(''d''(''w'')+1) is dropped from the sum in the pessimistic estimator is at most (''d''(''w'')+1)/|''R''<sup>(''t'')</sup>|, so the expected decrease in each term in the sum is at most 1/|''R''<sup>(''t'')</sup>|. There are ''R''<sup>(''t'')</sup> terms in the sum. Thus, the expected decrease in the sum is at most 1. Meanwhile, the size of ''S'' increases by 1.]\n\nThus, there must exist some choice of ''u'' that keeps the pessimistic estimator from decreasing.\n\n=== Algorithm maximizing the pessimistic estimator ===\n\nThe algorithm below chooses each vertex ''u'' to maximize the resulting pessimistic estimator. By the previous considerations, this keeps the pessimistic estimator from decreasing and guarantees a successful outcome.\n\nBelow, ''N''<sup>(''t'')</sup>(''u'') denotes the neighbors of ''u'' in ''R''<sup>(''t'')</sup> (that is, neighbors of ''u'' that are neither in ''S'' nor have a neighbor in ''S'').\n 1. Initialize ''S'' to be the empty set.\n 2. While there exists a not-yet-considered vertex ''u'' with no neighbor in ''S'':\n 3.    Add such a vertex ''u'' to ''S'' where ''u'' minimizes <math>\\sum_{w\\in N^{(t)}(u)\\cup\\{u\\}} \\frac{1}{d(w)+1}</math>.\n 4. Return ''S''.\n\n=== Algorithms that don't maximize the pessimistic estimator ===\n\nFor the method of conditional probabilities to work, it suffices if the algorithm keeps the pessimistic estimator from decreasing (or increasing, as appropriate). The algorithm does not necessarily have to maximize (or minimize) the pessimistic estimator. This gives some flexibility in deriving the algorithm. The next two algorithms illustrate this.\n\n 1. Initialize ''S'' to be the empty set.\n 2. While there exists a vertex ''u'' in the graph with no neighbor in ''S'':\n 3.    Add such a vertex ''u'' to ''S'', where ''u'' minimizes ''d''(''u'') (the initial degree of ''u'').\n 4. Return ''S''.\n\n 1. Initialize ''S'' to be the empty set.\n 2. While the remaining graph is not empty:\n 3.    Add a vertex ''u'' to ''S'', where ''u'' has minimum degree in the ''remaining'' graph.\n 4.    Delete ''u'' and all of its neighbors from the graph.\n 5. Return ''S''.\n\nEach algorithm is analyzed with the same pessimistic estimator as before. With each step of either algorithm, the net increase in the pessimistic estimator is\n\n: <math>1 - \\sum_{w\\in N^{(t)}(u)\\cup\\{u\\}} \\frac{1}{d(w)+1},</math>\n\nwhere ''N''<sup>(''t'')</sup>(''u'') denotes the neighbors of ''u'' in the remaining graph (that is, in ''R''<sup>(''t'')</sup>).\n\nFor the first algorithm, the net increase is non-negative because, by the choice of ''u'',\n\n: <math>\\sum_{w\\in N^{(t)}(u)\\cup\\{u\\}} \\frac{1}{d(w)+1} \\le (d(u)+1) \\frac{1}{d(u)+1} = 1 </math>,\n\nwhere ''d''(''u'') is the degree of ''u'' in the original graph.\n\nFor the second algorithm, the net increase is non-negative because, by the choice of ''u'',\n\n: <math>\\sum_{w\\in N^{(t)}(u)\\cup\\{u\\}} \\frac{1}{d(w)+1} \\le (d'(u)+1) \\frac{1}{d'(u)+1} = 1 </math>,\n\nwhere ''d′''(''u'') is the degree of ''u'' in the remaining graph.\n\n== See also ==\n\n* [[Probabilistic method]]\n* [[Derandomization]]\n* [[Randomized rounding]]\n\n{{no footnotes|date=June 2012}}\n\n== References ==\n\n* {{Citation\n\n| title=Ten lectures on the probabilistic method\n| last=Spencer|first=Joel H.|authorlink=Joel Spencer\n| url=https://books.google.com/books?id=Kz0B0KkwfVAC\n| year=1987\n| publisher=SIAM\n| isbn=978-0-89871-325-1}}\n\n* {{Citation\n\n| title= Probabilistic construction of deterministic algorithms: approximating packing integer programs\n| first = Prabhakar | last = Raghavan | authorlink=Prabhakar Raghavan\n| journal=[[Journal of Computer and System Sciences]]\n| volume=37\n| issue=2\n| pages=130–143\n| year = 1988\n| doi = 10.1016/0022-0000(88)90003-7}}.\n\n== Further reading ==\n\n* {{Cite book |first1=Noga |last1= Alon |authorlink1=Noga Alon\n\n| first2=Joel |last2=Spencer |authorlink2=Joel Spencer\n| series=Wiley-Interscience Series in Discrete Mathematics and Optimization\n| title=The probabilistic method\n| url=https://books.google.com/books?id=q3lUjheWiMoC&q=%22method+of+conditional+probabilities%22#v=snippet&q=%22method%20of%20conditional%20probabilities%22&f=false\n| year=2008\n| edition=Third\n| publisher=John Wiley and Sons\n| location=Hoboken, NJ\n| isbn=978-0-470-17020-5\n| pages=250 et seq. (in 2nd edition, {{ISBN|9780471653981}})\n| mr=2437651 }}\n\n* {{Cite book\n\n| first1=Rajeev |last1=Motwani |authorlink1=Rajeev Motwani\n| first2=Prabhakar |last2=Raghavan |authorlink2=Prabhakar Raghavan\n| title=Randomized algorithms\n| url=https://books.google.com/books?id=QKVY4mDivBEC&q=%22method+of+conditional+probabilities%22#v=snippet&q=%22method%20of%20conditional%20probabilities%22&f=false\n| publisher=[[Cambridge University Press]]\n| pages=120-\n| isbn=978-0-521-47465-8}}\n\n* {{Citation\n\n| first=Vijay |last=Vazirani\n| authorlink=Vijay Vazirani\n| title=Approximation algorithms\n| url=https://books.google.com/books?id=EILqAmzKgYIC&q=%22method+of+conditional%22#v=snippet&q=%22method%20of%20conditional%22&f=false\n| publisher=[[Springer Verlag]]\n| pages=130-\n| isbn=978-3-540-65367-7}}\n<!-- |url=https://books.google.com/books?id=EILqAmzKgYIC -->\n<!-- book references generated by http://reftag.appspot.com -->\n\n== External links ==\n\n* [http://algnotes.info/on/background/probabilistic-method/method-of-conditional-probabilities/ The probabilistic method — method of conditional probabilities], blog entry by Neal E. Young, accessed 19/04/2012.\n\n{{DEFAULTSORT:Method Of Conditional Probabilities}}\n[[Category:Approximation algorithms]]\n[[Category:Probabilistic arguments]]"
    },
    {
      "title": "Methods of successive approximation",
      "url": "https://en.wikipedia.org/wiki/Methods_of_successive_approximation",
      "text": "Mathematical '''methods''' relating to '''successive approximation''' include the following:\n\n* [[Babylonian method]], for finding square roots of numbers\n* [[Fixed-point iteration]]\n*  Means of finding zeros of functions:\n** [[Halley's method]]\n** [[Newton's method]]\n* Differential-equation matters:\n** [[Picard–Lindelöf theorem]], on existence of solutions of differential equations\n** [[Runge–Kutta method]], for numerical solution of differential equations\n\n\n\n\n[[Category:Approximation algorithms]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Metric k-center",
      "url": "https://en.wikipedia.org/wiki/Metric_k-center",
      "text": "In [[graph theory]], the '''metric ''k''-center''' or '''metric facility location''' problem  is a [[combinatorial optimization]] problem studied in [[theoretical computer science]]. Given ''n'' cities with specified distances, one wants to build ''k'' warehouses in different cities and minimize the maximum distance of a city to a warehouse. In graph theory this means finding a set of ''k'' vertices for which the largest distance of any point to its closest vertex in the ''k''-set is minimum. The vertices must be in a metric space, providing a [[complete graph]] that satisfies the [[triangle inequality]].\n\n==Formal definition==\nLet <math>(X,d)</math> be a [[metric space]] where <math>X</math> is a set and <math>d</math> is a [[Metric (mathematics)|metric]]<br />\nA set <math>\\mathbf{V}\\subseteq\\mathcal{X}</math>, is provided together with a parameter <math>k</math>. The goal is to find a subset <math>\\mathcal{C}\\subseteq \\mathbf{V}</math> with <math>|\\mathcal{C}|=k</math> such that the maximum distance of a point in <math>\\mathbf{V}</math> to the closest point in <math>\\mathcal{C}</math> is minimized. The problem can be formally defined as follows: <br />\nFor a metric space (<math>\\mathcal{X}</math>,d),\n\t\n* Input: a set <math>\\mathbf{V}\\subseteq\\mathcal{X}</math>,and a parameter <math>k</math>.\n* Output: a set <math>\\mathcal{C}</math> of <math>k</math> points.\n* Goal: Minimize the cost <math>r^\\mathcal{C}(\\mathbf{V}) = \\underset{v\\in V}{max}</math> d(v,<math>\\mathcal{C}</math>)\n\nThat is, Every point in a cluster is in distance at most <math>r^\\mathcal{C}(V)</math> from its respective center. \n<ref name=\"Har-peled:2011:GAA:2031416\">\n{{ cite book \n |author = Har-peled, Sariel | authorlink = Sariel Har-Peled\n |title = Geometric Approximation Algorithms\n |year = 2011\n |isbn = 0821849115\n |publisher = American Mathematical Society\n |location = Boston, MA, USA\n}}\n</ref>\n\nThe k-Center Clustering problem can also be defined on a complete undirected graph ''G''&nbsp;=&nbsp;(''V'',&nbsp;''E'') as follows:<br />\nGiven a complete undirected graph ''G''&nbsp;=&nbsp;(''V'',&nbsp;''E'') with distances ''d''(''v''<sub>''i''</sub>,&nbsp;''v''<sub>''j''</sub>)&nbsp;&isin;&nbsp;''N'' satisfying the triangle inequality, find a subset ''C''&nbsp;&sube;&nbsp;''V'' with |''C''|&nbsp;=&nbsp;''k'' while minimizing:\n\n: <math>\\max_{v \\in V} \\min_{c \\in C} d(v,c)</math>\n\n==Computational complexity==\nIn a complete undirected graph ''G''&nbsp;=&nbsp;(''V'',&nbsp;''E''), if we sort the edges in nondecreasing order of the distances: ''d''(''e''<sub>1</sub>)&nbsp;&le;&nbsp;''d''(''e''<sub>2</sub>)&nbsp;&le;&nbsp;&hellip;&nbsp;&le;&nbsp;''d''(''e''<sub>m</sub>) and let ''G''<sub>i</sub>&nbsp;=&nbsp;(V,&nbsp;''E''<sub>i</sub>), where ''E''<sub>i</sub>&nbsp;=&nbsp;{''e''<sub>1</sub>,&nbsp;''e''<sub>2</sub>,&nbsp;&hellip;,&nbsp;''e''<sub>i</sub>}. The ''k''-center problem is equivalent to finding the smallest index ''i'' such that ''G''<sub>i</sub> has a [[dominating set]] of size at most ''k''.\n<ref>\n{{citation\n  | last = Vazirani\n  | first = Vijay V.\n  | authorlink = Vijay Vazirani\n  | title = Approximation Algorithms\n  | publisher = Springer\n  | year = 2003\n  | location = Berlin\n  | isbn = 3-540-65367-8 \n  | pages = 47&ndash;48\n}}\n</ref>\n\nAlthough Dominating Set is [[NP-complete]], the ''k''-center problem remains [[NP-hard]]. This is clear, since the optimality of a given feasible solution for the ''k''-center problem can be determined through the Dominating Set reduction only if we know in first place the size of the optimal solution (i.e. the smallest index ''i'' such that ''G''<sub>i</sub> has a [[dominating set]] of size at most ''k'') , which is precisely the difficult core of the [[NP-Hard]] problems.\n\n==Approximations==\n=== A simple greedy algorithm ===\nA simple [[greedy algorithm|greedy]] [[approximation algorithm]] that achieves an approximation factor of 2 builds <math>\\mathcal{C}</math> using a [[farthest-first traversal]] in ''k'' iterations. \nThis algorithm simply chooses the point farthest away from the current set of centers in each iteration as the new center. It can be described as follows:\n\n* Pick an arbitrary point <math>\\bar{c}_1</math> into <math>C_1</math>\n* For every point <math>v\\in \\mathbf{V}</math> compute <math>d_1[v]</math> from <math>\\bar{c}_1</math>\n* Pick the point <math>\\bar{c}_2</math> with highest distance from <math>\\bar{c}_1</math>.\n* Add it to the set of centers and denote this expanded set of centers as <math>C_2</math>. Continue this till ''k'' centers are found\n\n==== Running time ====\n* The i<sup>th</sup> iteration of choosing the i<sup>th</sup> center takes <math>\\mathcal{O}(n)</math> time.\n* There are ''k'' such iterations.\n* Thus, overall the algorithm takes <math>\\mathcal{O}(nk)</math> time.<ref>{{citation\n  | last = Gonzalez\n  | first = Teofilo F. | authorlink = Teofilo F. Gonzalez\n  | contribution = Clustering to minimize the maximum intercluster distance\n  | title = Theoretical Computer Science\n  | publisher = Elsevier Science B.V.\n  | year = 1985\n  | volume = 38\n  | pages=293&ndash;306\n  | doi = 10.1016/0304-3975(85)90224-5}}\n</ref>\n\n==== Proving the approximation factor ====\nThe solution obtained using the simple greedy algorithm is a 2-approximation to the optimal solution. This section focuses on proving this approximation factor.\n\n\tGiven a set of ''n'' points <math>\\mathbf{V}\\subseteq\\mathcal{X}</math>,belonging to a metric space (<math>\\mathcal{X}</math>,d), the greedy ''K''-center algorithm computes a set '''K''' of ''k'' centers, such that '''K''' is a 2-approximation to the optimal ''k''-center clustering of '''V'''.\n\n''i.e.''  <math>r^{\\mathbf{K}}(\\mathbf{V})\\leq 2r^{opt}(\\mathbf{V},\\textit{k})</math>\n<ref name=\"Har-peled:2011:GAA:2031416\"/>\n\nThis theorem can be proven using two cases as follows,\n\nCase 1: Every cluster of <math>\\mathcal{C}_{opt}</math> contains exactly one point of <math>\\mathbf{K}</math>\n\n* Consider a point <math>v\\in \\mathbf{V}</math>\n* Let <math>\\bar{c}</math> be the center it belongs to in <math>\\mathcal{C}_{opt}</math>\n* Let <math>\\bar{k}</math> be the center of <math>\\mathbf{K}</math> that is in <math>\\Pi(\\mathcal{C}_{opt},\\bar{c})</math>\n* <math>d(v,\\bar{c})=d(v,\\mathcal{C}_{opt})\\leq r^{opt}(\\mathbf{V},k)</math>\n* Similarly, <math>d(\\bar{k},\\bar{c})=d(\\bar{k},\\mathcal{C}_{opt})\\leq r^{opt}</math>\n* By the triangle inequality: <math>d(v,\\bar{k})\\leq d(v,\\bar{c})+d(\\bar{c},\\bar{k})\\leq 2r^{opt}</math>\n<br />\nCase 2: There are two centers <math>\\bar{k}</math> and <math>\\bar{u}</math> of <math>\\mathbf{K}</math> that are both in <math>\\Pi(\\mathcal{C}_{opt},\\bar{c})</math>, for some <math>\\bar{c}\\in \\mathcal{C}_{opt}</math> (By pigeon hole principle, this is the only other possibility)\n* Assume, without loss of generality, that <math>\\bar{u}</math> was added later to the center set <math>\\mathbf{K}</math> by the greedy algorithm, say in i<sup>th</sup> iteration.\n* But since the greedy algorithm always chooses the point furthest away from the current set of centers, we have that <math>\\bar{c}\\in\\mathcal{C}_{i-1}</math>and,\n<math>\n\t\t\\begin{align}\n\t\tr^\\mathbf{K}(\\mathbf{V})\\leq r^{\\mathcal{C}_{i-1}}(\\mathbf{V})&=d(\\bar{u},\\mathcal{C}_{i-1})\\\\\n\t\t&\\leq d(\\bar{u},\\bar{k})\\\\\n\t\t&\\leq d(\\bar{u},\\bar{c})+d(\\bar{c},\\bar{k})\\\\\n\t\t&\\leq 2r^{opt}\n\t\t\\end{align}\n</math>\n<ref name=\"Har-peled:2011:GAA:2031416\"/>\n\n=== Another 2-factor approximation algorithm ===\nAnother algorithm with the same approximation factor takes advantage of the fact that the ''k''-center problem is equivalent to finding the smallest index ''i'' such that ''G''<sub>i</sub> has a dominating set of size at most ''k'' and computes a maximal [[Independent set (graph theory)|independent set]] of ''G''<sub>i</sub>, looking for the smallest index ''i'' that has a maximal independent set with a size of at least ''k''.\n<ref>{{citation\n  | last1 = Hochbaum\n  | first1 = Dorit S.  | author1-link = Dorit S. Hochbaum\n  | last2 = Shmoys\n  | first2 = David B. | author2-link = David Shmoys\n  | contribution = A unified approach to approximation algorithms for bottleneck problems\n  | title = Journal of the ACM\n  | volume = 33\n  | issue = 3\n  | year = 1986\n  | issn = 0004-5411 \n  | pages=533&ndash;550}}\n</ref>\nIt is not possible to find an approximation algorithm with an approximation factor of 2&nbsp;&minus;&nbsp;&epsilon; for any &epsilon;&nbsp;>&nbsp;0, unless P = NP.\n<ref>\n{{citation\n  | last = Hochbaum\n  | first = Dorit S.\n  | title = Approximation Algorithms for NP-Hard problems\n  | publisher = PWS Publishing Company\n  | year = 1997\n  | location = Boston\n  | isbn = 0-534-94968-1\n  | pages=346&ndash;398}}\n</ref>\nFurthermore, the distances of all edges in G must satisfy the triangle inequality if the ''k''-center problem is to be approximated within any constant factor, unless P = NP.\n<ref>\n{{citation\n | last1=Crescenzi | first1=Pierluigi\n | last2=Kann | first2=Viggo\n | last3=Halldórsson | first3=Magnús\n | last4=Karpinski | first4=Marek | authorlink4=Marek Karpinski\n | last5=Woeginger | first5=Gerhard | authorlink5 = Gerhard J. Woeginger\n | title=A Compendium of NP Optimization Problems\n | contribution=Minimum k-center\n | url=http://www.csc.kth.se/~viggo/wwwcompendium/node128.html\n | year=2000 }}\n</ref>\n\n==See also==\n* [[Traveling salesman problem]]\n* [[Minimum k-cut]]\n* [[Dominating set]]\n* [[Independent set (graph theory)]]\n* [[Facility location problem]]\n\n==References==\n<references />\n\n==Further reading==\n\n* {{citation\n  | last1 = Hochbaum\n  | first1 = Dorit S.  | author1-link = Dorit S. Hochbaum\n  | last2 = Shmoys\n  | first2 = David B. | author2-link = David Shmoys\n  | contribution = A Best Possible Heuristic for the k-Center Problem\n  | title = Mathematics of Operations Research\n  | year = 1985\n  | volume = 10\n  | issue = 2\n  | pages = 180&ndash;184}}\n\n[[Category:Combinatorial optimization]]\n[[Category:Computational problems in graph theory]]\n[[Category:Approximation algorithms]]\n[[Category:NP-hard problems]]"
    },
    {
      "title": "Minimum k-cut",
      "url": "https://en.wikipedia.org/wiki/Minimum_k-cut",
      "text": "In mathematics, the '''minimum ''k''-cut''', is a [[combinatorial optimization]] problem that requires finding a set of edges whose removal would partition the graph to at least ''k'' connected components. These edges are referred to as '''''k''-cut'''. The goal is to find the minimum-weight ''k''-cut. This partitioning can have applications in [[VLSI]] design, [[data-mining]], [[finite elements]] and communication in [[parallel computing]].\n\n==Formal definition==\nGiven an undirected graph ''G''&nbsp;=&nbsp;(''V'',&nbsp;''E'') with an assignment of weights to the edges ''w'':&nbsp;''E''&nbsp;&rarr;&nbsp;''N'' and an integer ''k''&nbsp;&isin;&nbsp;{2,&nbsp;3,&nbsp;&hellip;,&nbsp;|''V''|}, partition ''V'' into ''k'' disjoint sets ''F''&nbsp;=&nbsp;{''C''<sub>1</sub>,&nbsp;''C''<sub>2</sub>,&nbsp;&hellip;,&nbsp;''C''<sub>''k''</sub>} while minimizing\n\n: <math>\\sum_{i=1}^{k-1}\\sum_{j=i+1}^k\\sum_{\\begin{smallmatrix} v_1 \\in C_i \\\\ v_2 \\in C_j \\end{smallmatrix}} w ( \\left \\{ v_1, v_2 \\right \\} )</math>\n\nFor a fixed ''k'', the problem is [[polynomial time]] solvable in ''O''(|''V''|<sup>''k''<sup>2</sup></sup>).<ref>{{harvnb|Goldschmidt|Hochbaum|1988}}.</ref> However, the problem is [[NP-complete]] if ''k'' is part of the input.<ref>{{harvnb|Garey|Johnson|1979}}</ref> It is also NP-complete if we specify <math>k</math> vertices and ask for the minimum <math>k</math>-cut which separates these vertices among each of the sets.<ref>[https://www.jstor.org/stable/3690374?seq=13], which cites [http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.6534]</ref>\n\n==Approximations==\nSeveral [[approximation algorithms]] exist with an approximation of 2&nbsp;&minus;&nbsp;2/''k''. A simple [[greedy algorithm]] that achieves this approximation factor computes a [[minimum cut]] in each of the connected components and removes the heaviest one. This algorithm requires a total of ''n''&nbsp;&minus;&nbsp;1 [[max flow]] computations. Another algorithm achieving the same guarantee uses the [[Gomory–Hu tree]] representation of minimum cuts. Constructing the Gomory&ndash;Hu tree requires ''n''&nbsp;&minus;&nbsp;1 max flow computations, but the algorithm requires an overall ''O''(''kn'') max flow computations. Yet, it is easier to analyze the approximation factor of the second algorithm.<ref>{{harvnb|Saran|Vazirani|1991}}.</ref><ref>{{harvnb|Vazirani|2003|pp=40&ndash;44}}.</ref> Moreover, under the Small Set Expansion Hypothesis (a conjecture closely related to the [[Unique Games Conjecture]]), the problem is NP-hard to approximate to within <math>(2 - \\epsilon)</math> factor for every constant <math>\\epsilon > 0</math>,<ref>{{harvnb|Manurangsi|2017}}</ref> meaning that the aforementioned approximation algorithms are essentially tight for large <math>k</math>.\n\nA variant of the problem asks for a minimum weight ''k''-cut where the output partitions have pre-specified sizes. This problem variant is approximable to within a factor of 3 for any fixed ''k'' if one restricts the graph to a metric space, meaning a [[complete graph]] that satisfies the [[triangle inequality]].<ref>{{harvnb|Guttmann-Beck|Hassin|1999|pp=198&ndash;207}}.</ref> More recently, [[polynomial time approximation scheme]]s (PTAS) were discovered for those problems.<ref>{{harvnb|Fernandez de la Vega|Karpinski|Kenyon|2004}}\n</ref>\n\n==See also==\n* [[Maximum cut]]\n* [[Minimum cut]]\n\n==Notes==\n<references />\n\n==References==\n* {{citation\n  | first1=O.\n  | last1=Goldschmidt\n  | first2=D. S.\n  | last2=Hochbaum | author2-link = Dorit S. Hochbaum\n  | title=Proc. 29th Ann. IEEE Symp. on Foundations of Comput. Sci.\n  | publisher=IEEE Computer Society\n  | year=1988\n  | pages=444&ndash;451 }}\n* {{citation\n  | first1=M. R.\n  | last1=Garey \n  | first2=D. S.\n  | last2=Johnson\n  | title=Computers and Intractability: A Guide to the Theory of NP-Completeness\n  | publisher = W.H. Freeman\n  | year=1979\n  | isbn = 978-0-7167-1044-8 }}\n* {{citation\n  | first1=H.\n  | last1=Saran\n  | first2=V.\n  | last2=Vazirani\n  | contribution=Finding ''k''-cuts within twice the optimal\n  | contribution-url=http://www.cc.gatech.edu/~vazirani/k-cut.ps\n  | title=Proc. 32nd Ann. IEEE Symp. on Foundations of Comput. Sci\n  | publisher=IEEE Computer Society\n  | year=1991\n  | pages=743&ndash;751 }}\n* {{citation\n  | last = Vazirani\n  | first = Vijay V.\n  | authorlink = Vijay Vazirani\n  | title = Approximation Algorithms\n  | publisher = Springer\n  | year = 2003\n  | location = Berlin\n  | isbn = 978-3-540-65367-7 }}\n* {{citation\n  | last1 = Guttmann-Beck\n  | first1 = N.\n  | last2 = Hassin\n  | first2 = R.\n  | contribution = Approximation algorithms for minimum ''k''-cut\n  | contribution-url = http://www.math.tau.ac.il/~hassin/k_cut_00.pdf\n  | title = Algorithmica\n  | year = 1999\n  | pages = 198&ndash;207 }}\n* {{citation\n |last1       = Comellas\n |first1      = Francesc\n |last2       = Sapena\n |first2      = Emili\n |title       = A multiagent algorithm for graph partitioning. Lecture Notes in Comput. Sci.\n |year        = 2006\n |volume      = 3907\n |pages       = 279&ndash;285\n |issn        = 0302-9743\n |doi         = 10.1007/s004530010013\n |journal     = Algorithmica\n |issue       = 2\n |url         = http://www-ma4.upc.es/~comellas/evocomnet06/CoSa06-EvoCOMNET06.html\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20091212080856/http://www-ma4.upc.es/~comellas/evocomnet06/CoSa06-EvoCOMNET06.html\n |archivedate = 2009-12-12\n |df          = \n|citeseerx      = 10.1.1.55.5697\n }}\n* {{citation\n | last1=Crescenzi | first1=Pierluigi\n | last2=Kann | first2=Viggo\n | last3=Halldórsson | first3=Magnús\n | last4=Karpinski | first4=Marek | authorlink4=Marek Karpinski\n | last5=Woeginger | first5=Gerhard | authorlink5 = Gerhard J. Woeginger\n | title=A Compendium of NP Optimization Problems\n | contribution=Minimum k-cut\n | url=http://www.csc.kth.se/~viggo/wwwcompendium/node90.html\n | year=2000 }}\n* {{Cite conference\n | title=Approximation schemes for Metric Bisection and partitioning\n | last1=Fernandez de la Vega | first1=W.\n | last2=Karpinski | first2 = M. \n | last3=Kenyon | first3=C. | author3-link = Claire Mathieu\n | booktitle=Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete Algorithms\n | pages=506–515,\n | year=2004\n | url=http://dl.acm.org/citation.cfm?id=982792.982864\n | ref=harv\n}}\n* {{Cite conference\n | title=Inapproximability of Maximum Edge Biclique, Maximum Balanced Biclique and Minimum k-Cut from the Small Set Expansion Hypothesis\n | last=Manurangsi | first=P.\n | booktitle=44th International Colloquium on Automata, Languages, and Programming, ICALP 2017\n | pages=79:1–79:14,\n | year=2017\n | ref=harv\n| doi=10.4230/LIPIcs.ICALP.2017.79 }}\n\n\n[[Category:NP-complete problems]]\n[[Category:Combinatorial optimization]]\n[[Category:Computational problems in graph theory]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Minimum relevant variables in linear system",
      "url": "https://en.wikipedia.org/wiki/Minimum_relevant_variables_in_linear_system",
      "text": "'''MINimum Relevant Variables in Linear System''' ('''Min-RVLS''') is a problem in [[mathematical optimization]]. Given a [[linear program]], it is required to find a feasible solution in which the number of non-zero variables is as small as possible.\n\nThe problem is known to be [[NP-hardness|NP-hard]] and even hard to approximate.\n\n== Definition ==\nA Min-RVLS problem is defined by:<ref name=\":1\">{{Cite journal|date=1998-12-06|title=On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems|journal=Theoretical Computer Science|language=en|volume=209|issue=1–2|pages=237–260|doi=10.1016/S0304-3975(97)00115-1|issn=0304-3975|last1=Amaldi|first1=Edoardo|last2=Kann|first2=Viggo}}</ref>\n\n* A binary relation ''R'', which is one of {=, ≥, >, ≠};\n* An ''m''-by-''n'' matrix ''A'' (where ''m'' is the number of constraints and ''n'' the number of variables);\n* An ''m''-by-1 vector ''b''.\n\nThe linear system is given by: ''A x'' ''R'' ''b.'' It is assumed to be feasible (i.e., satisfied by at least one ''x''). Depending on R, there are four different variants of this system: ''A x = b, A x ≥ b, A x > b, A x ≠ b''.\n\nThe goal is to find an ''n''-by-1 vector ''x'' that satisfies the system ''A x'' ''R'' ''b'', and subject to that, contains as few as possible nonzero elements.\n\n== Special case ==\nThe problem Min-RVLS[=] was presented by Garey and Johnson,<ref>{{Cite web|url=https://www.semanticscholar.org/paper/Computers-and-Intractability:-A-Guide-to-the-Theory-Garey-Johnson/1e3b61f29e5317ef59d367e1a53ba407912d240e|title=Computers and Intractability: A Guide to the Theory of NP-Completeness|last=Johnson|first=David S.|last2=Garey|first2=M. R.|date=1979|website=www.semanticscholar.org|language=en|access-date=2019-01-07}}</ref> who called it \"minimum weight solution to linear equations\". They proved it was NP-hard, but did not consider approximations.\n\n== Applications ==\nThe Min-RVLS problem is important in [[machine learning]] and [[linear discriminant analysis]]. Given a set of positive and negative examples, it is required to minimize the number of features that are required to correctly classify them.<ref>{{Cite journal|last=Koehler|first=Gary J.|date=1991-11-01|title=Linear Discriminant Functions Determined by Genetic Search|journal=ORSA Journal on Computing|volume=3|issue=4|pages=345–357|doi=10.1287/ijoc.3.4.345|issn=0899-1499}}</ref> The problem is known as the [[minimum feature set problem]]. An algorithm that approximates Min-RVLS within a factor of <math>O(\\log(m))</math>could substantially reduce the number of training samples required to attain a given accuracy level. <ref>{{Cite journal|last=Van Horn|first=Kevin S.|last2=Martinez|first2=Tony R.|date=1994-03-01|title=The Minimum Feature Set Problem|journal=Neural Netw.|volume=7|issue=3|pages=491–494|doi=10.1016/0893-6080(94)90082-5|issn=0893-6080}}</ref>\n\nThe [[shortest codeword problem]] in [[coding theory]] is the same problem as Min-RVLS[=] when the coefficients are in GF(2).\n\n== Related problems ==\nIn '''MINimum Unsatisfied Linear Relations''' ('''Min-ULR'''), we are given a binary relation ''R'' and a linear system ''A x'' ''R'' ''b'', which is now assumed to be  ''infeasible''.  The goal is to find a vector ''x'' that violates as few relations as possible, while satisfying all the others.\n\nMin-ULR[≠] is trivially solvable, since any system with real variables and a finite number of inequality constraints is feasible. As for the other three variants:\n\n* Min-ULR[=,>,≥] are NP-hard even with homogeneous systems and bipolar coefficients (coefficients in {1,-1}). <ref name=\":0\">{{Cite journal|date=1995-08-07|title=The complexity and approximability of finding maximum feasible subsystems of linear relations|url=https://www.sciencedirect.com/science/article/pii/030439759400254G|journal=Theoretical Computer Science|language=en|volume=147|issue=1–2|pages=181–210|doi=10.1016/0304-3975(94)00254-G|issn=0304-3975|last1=Amaldi|first1=Edoardo|last2=Kann|first2=Viggo}}</ref>\n* The NP-complete problem [[Minimum feedback arc set]] reduces to Min-ULR[≥], with exactly one 1 and one -1 in each constraint, and all right-hand sides equal to 1. <ref name=\":2\">{{Cite journal|date=1993-02-01|title=A note on resolving infeasibility in linear programs by constraint relaxation|url=https://www.sciencedirect.com/science/article/pii/016763779390079V|journal=Operations Research Letters|language=en|volume=13|issue=1|pages=19–20|doi=10.1016/0167-6377(93)90079-V|issn=0167-6377|last1=Sankaran|first1=Jayaram K.}}</ref>\n* Min-ULR[=,>,≥] are polynomial if the number of variables ''n'' is constant: they can be solved polynomially using an algorithm of Greer<ref>{{Cite book|url=https://books.google.com/?id=DRxEqwzbb2UC&pg=PP1&dq=Trees+and+hills:+Methodology+for+maximizing+functions+of+systems+of+linear+relations,#v=onepage&q=Trees%20and%20hills:%20Methodology%20for%20maximizing%20functions%20of%20systems%20of%20linear%20relations,&f=false|title=Trees and Hills: Methodology for Maximizing Functions of Systems of Linear Relations|last=Greer|first=R.|date=2011-08-18|publisher=Elsevier|isbn=9780080872070|language=en}}</ref> in time  <math>O(n\\cdot m^n / 2^{n-1})</math>.\n* Min-ULR[=,>,≥] are linear if the number of constraints ''m'' is constant, since all subsystems can be checked in time ''O''(''n'').\n*Min-ULR[≥] is polynomial in some special case.<ref name=\":2\" />\n*Min-ULR[=,>,≥] can be approximated within ''n''&nbsp;+&nbsp;1 in polynomial time.<ref name=\":1\" />\n*Min-ULR[>,≥] are [[Minimum dominating set|minimum-dominating-set]]-hard, even with homogeneous systems and ternary coefficients (in {−1,0,1}).\n*Min-ULR[=] cannot be approximated within a factor of <math>2^{\\log^{1-\\varepsilon}n}</math>, for any <math>\\varepsilon>0</math>, unless NP is contained in [[DTIME]](<math>n^{\\operatorname{polylog}(n)}</math>). <ref>{{Cite journal|date=1997-04-01|title=The Hardness of Approximate Optima in Lattices, Codes, and Systems of Linear Equations|journal=Journal of Computer and System Sciences|language=en|volume=54|issue=2|pages=317–331|doi=10.1006/jcss.1997.1472|issn=0022-0000|last1=Arora|first1=Sanjeev|last2=Babai|first2=László|last3=Stern|first3=Jacques|last4=Sweedyk|first4=Z.}}</ref>\n\nIn the complementary problem '''MAXimum Feasible Linear Subystem''' ('''Max-FLS'''), the goal is to find a maximum subset of the constraints that can be satisfied simultaneously.<ref name=\":0\" />  \n\n* Max-FLS[≠] can be solved in polynomial time.\n* Max-FLS[=] is NP-hard even with homogeneous systems and bipolar coefficients.\n\n* . With integer coefficients, it is hard to approximate within <math>m^{\\varepsilon}</math>. With coefficients over GF[q], it is ''q''-approximable.\n* Max-FLS[>] and Max-FLS[≥] are NP-hard even with homogeneous systems and bipolar coefficients. They are 2-approximable, but they cannot be approximated within any smaller constant factor.\n\n== Hardness of approximation ==\nAll four variants of Min-RVLS are hard to approximate. In particular all four variants cannot be approximated within a factor of <math>2^{\\log^{1-\\varepsilon}n}</math>, for any <math>\\varepsilon>0</math>, unless NP is contained in [[DTIME]](<math>n^{\\operatorname{polylog}(n)}</math>).<ref name=\":1\" />{{Rp|247-250}} The hardness is proved by reductions:\n\n* There is a reduction from Min-ULR[=] to Min-RVLS[=]. It also applies to Min-RVLS[≥] and Min-RVLS[>], since each equation can be replaced by two complementary inequalities.\n* There is a reduction from [[Minimum dominating set|minimum-dominating-set]] to Min-RVLS[≠].\n\nOn the other hand, there is a reduction fro Min-RVLS[=] to Min-ULR[=]. It also applies to Min-ULR[≥] and Min-ULR[>], since each equation can be replaced by two complementary inequalities.\n\nTherefore, when R is in {=,>,≥}, Min-ULR and Min-RVLS are equivalent in terms of approximation hardness.\n\n== References ==\n{{Reflist}}\n[[Category:Linear programming]]\n[[Category:NP-hard problems]]\n[[Category:Approximation algorithms]]\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Multi-fragment algorithm",
      "url": "https://en.wikipedia.org/wiki/Multi-fragment_algorithm",
      "text": "{{Orphan|date=January 2019}}\n\n{{merge to|Travelling salesman problem|discuss=Talk:Travelling salesman problem#Proposed merge with Multi-fragment algorithm|date=November 2018}}\n{{Infobox Algorithm\n|class=[[Approximation algorithm]]\n|image=\n|caption=\n|data=[[Graph (data structure)|Graph]]\n|time=<math>\\Theta(N^2 \\log N)</math>\n|space=\n|optimal=No\n|complete=\n}}\n\nThe '''multi-fragment (MF) algorithm''' is a [[heuristic]] or [[approximation algorithm|approximation]] algorithm for the [[travelling salesman problem]] (TSP) (and related problems). This algorithm is also sometimes called the \"greedy algorithm\" for the TSP.\n\nThe algorithm builds a tour for the traveling salesman one edge at a time and thus maintains multiple tour fragments, each of which is a simple path in the complete graph of cities. At each stage, the algorithm selects the edge of minimal cost that either creates a new fragment, extends one of the existing paths or creates a cycle of length equal to the number of cities.<ref name=\"johnson1997\">{{cite journal |last1=Johnson |first1=David |last2=A. McGeoch |first2=Lyle |title=The Traveling Salesman Problem: A Case Study in Local Optimization |journal=Local Search in Combinatorial Optimization |date=1997 |volume=1 |url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.1635&rep=rep1&type=pdf}}</ref>\n\n==References==\n<references />\n\n[[Category:Travelling salesman problem]]\n[[Category:Approximation algorithms]]\n\n\n{{algorithm-stub}}"
    },
    {
      "title": "Nearest neighbor search",
      "url": "https://en.wikipedia.org/wiki/Nearest_neighbor_search",
      "text": "'''Nearest neighbor search''' ('''NNS'''), as a form of proximity search,  is the [[optimization problem]] of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set ''S'' of points in a space ''M'' and a query point ''q''&nbsp;∈&nbsp;''M'', find the closest point in ''S'' to ''q''. [[Donald Knuth]] in vol. 3 of ''[[The Art of Computer Programming]]'' (1973) called it the '''post-office problem''', referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a ''k''-NN search, where we need to find the ''k'' closest points.\n\nMost commonly ''M'' is a  [[metric space]] and dissimilarity is expressed as a [[distance metric]], which is symmetric and satisfies the [[triangle inequality]]. Even more common, ''M'' is taken to be the ''d''-dimensional [[vector space]] where dissimilarity is measured using the [[Euclidean distance]], [[Taxicab geometry|Manhattan distance]] or other [[Statistical distance|distance metric]]. However, the dissimilarity function can be arbitrary. One example is asymmetric [[Bregman divergence]], for which the triangle inequality does not hold.<ref name=Cayton2008>{{Cite journal\n | last1 = Cayton | first1 = Lawerence\n | year = 2008\n | title =  Fast nearest neighbor retrieval for bregman divergences\n | journal = Proceedings of the 25th International Conference on Machine Learning\n | pages = 112–119\n | doi = 10.1145/1390156.1390171\n}}</ref>\n\n==Applications==\nThe nearest neighbor search problem arises in numerous fields of application, including:\n* [[Pattern recognition]] – in particular for [[optical character recognition]]\n* [[Statistical classification]] – see [[k-nearest neighbor algorithm]]\n* [[Computer vision]]\n* [[Computational geometry]] – see [[Closest pair of points problem]]\n* [[Database]]s – e.g. [[content-based image retrieval]]\n* [[Coding theory]] – see [[Decoding methods|maximum likelihood decoding]]\n* [[Data compression]] – see [[MPEG-2]] standard\n* [[Robotic]] sensing<ref name=panSearch>{{cite conference|last1=Bewley|first1=A.|last2=Upcroft|first2=B.|date=2013|title=Advantages of Exploiting Projection Structure for Segmenting Dense 3D Point Clouds|conference=Australian Conference on Robotics and Automation |url=http://www.araa.asn.au/acra/acra2013/papers/pap148s1-file1.pdf}}</ref>\n* [[Recommender system|Recommendation systems]], e.g. see [[Collaborative filtering]]\n* [[Internet marketing]] – see [[contextual advertising]] and [[behavioral targeting]]\n* [[DNA sequencing]]\n* [[Spell checking]] – suggesting correct spelling\n* [[Plagiarism detection]]\n* [[Contact searching algorithms in FEA]]\n* [[Similarity score]]s for predicting career paths of professional athletes.\n* [[Cluster analysis]] – assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on [[Euclidean distance]]\n* [[Chemical similarity]]\n* [[Motion planning#Sampling-based algorithms|Sampling-based motion planning]]\n* [[Intermodal freight transport]]<ref>{{cite journal|last1=Munim|first1=Ziaul Haque|last2=Haralambides|first2= Hercules |title= Competition and cooperation for intermodal container transhipment: A network optimization approach|journal=Research in Transportation Business & Management|date=2018|pages=87–99|doi= 10.1016/j.rtbm.2018.03.004|url= https://www.sciencedirect.com/science/article/pii/S2210539517301001|volume=26}}</ref>\n\n==Methods==\n\nVarious solutions to the NNS problem have been proposed.  The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the [[curse of dimensionality]] states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.\n\n=== Exact methods ===\n\n====Linear search====\nThe simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the \"best so far\".  This algorithm, sometimes referred to as the naive approach, has a [[running time]] of ''O''(''dN''), where ''N'' is the [[cardinality]] of ''S'' and ''d'' is the dimensionality of ''M''.  There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.<ref>{{cite journal|title=A quantitative analysis and performance study for similarity search methods in high dimensional spaces|last1=Weber|first1=Roger|last2=Schek|first2=Hans-J.|last3=Blott|first3=Stephen | journal=VLDB '98 Proceedings of the 24rd International Conference on Very Large Data Bases | pages=194-205 | year=1998 | url=http://www.vldb.org/conf/1998/p194.pdf}}</ref>\n\n====Space partitioning====\nSince the 1970s, the [[branch and bound]] methodology has been applied to the problem. In the case of Euclidean space this approach encompasses [[spatial index]] or spatial access methods. Several [[Space partitioning|space-partitioning]] methods have been developed for solving the NNS problem.  Perhaps the simplest is the [[k-d tree]], which iteratively bisects the search space into two regions containing half of the points of the parent region.  Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is ''O''(log&nbsp;''N'') <ref>{{cite web|title=An introductory tutorial on KD trees|author=Andrew Moore | url=http://www.autonlab.com/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&language=en}}</ref> in the case of randomly distributed points, worst case complexity is ''O''(''kN''^(1-1/''k''))<ref name=Lee1977>{{Cite journal\n | last1 = Lee | first1 = D. T. | author1-link = Der-Tsai Lee\n | last2 = Wong | first2 = C. K.\n | year = 1977\n | title = Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees\n | journal = [[Acta Informatica]]\n | volume = 9\n | issue = 1\n | pages = 23–29\n | doi = 10.1007/BF00263763\n | postscript = .\n}}</ref>\nAlternatively the [[R-tree]] data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the [[R* tree]].<ref>{{Cite conference | doi = 10.1145/223784.223794| chapter = Nearest neighbor queries| title = Proceedings of the 1995 ACM SIGMOD international conference on Management of data  – SIGMOD '95| pages = 71| year = 1995| last1 = Roussopoulos | first1 = N. | last2 = Kelley | first2 = S. | last3 = Vincent | first3 = F. D. R. | isbn = 0897917316}}</ref> R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.\n\nIn the case of general metric space, the branch-and-bound approach is known as the [[metric tree]] approach. Particular examples include [[vp-tree]] and [[BK-tree]] methods.\n\nUsing a set of points taken from a 3-dimensional space and put into a [[Binary space partitioning|BSP tree]], and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm.  (Strictly speaking, no such point may exist, because it may not be unique.  But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.)  The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point.  This may not be the case, but it is a good heuristic.  After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane.  This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched.  If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space.  If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result.  The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.\n\n=== Approximation methods ===\nAn approximate nearest neighbor search algorithm is allowed to return points, whose distance from the query is at most <math>c</math> times the distance from the query to its nearest points. The appeal of this approach is that, in many cases, an approximate nearest neighbor is almost as good as the exact one. In particular, if the distance measure accurately captures the notion of user quality, then small differences in the distance should not matter.<ref>{{Cite book|last=Andoni|first=A.|last2=Indyk|first2=P.|date=2006-10-01|title=Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions|url=http://ieeexplore.ieee.org/document/4031381/|journal=2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)|pages=459–468|doi=10.1109/FOCS.2006.49|isbn=978-0-7695-2720-8|citeseerx=10.1.1.142.3471}}</ref>\n\n====Greedy search in proximity neighborhood graphs====\nProximity graph methods (such as HNSW<ref name=\":0\">{{cite arxiv|last=Malkov|first=Yury|last2=Yashunin|first2=Dmitry|date=2016|title=Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs|eprint=1603.09320|class=cs.DS}}</ref>) are considered the current state-of-the-art for the approximate nearest neighbors search.<ref name=\":0\" /><ref>{{Cite web|url=https://erikbern.com/2018/06/17/new-approximate-nearest-neighbor-benchmarks.html|title=New approximate nearest neighbor benchmarks|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref><ref>{{Cite web|url=https://www.benfrederickson.com/approximate-nearest-neighbours-for-recommender-systems/|title=Approximate Nearest Neighbours for Recommender Systems|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\nThe methods are based on greedy traversing in proximity neighborhood graphs <math>G(V,E)</math>in which every point <math>x_i \\in S </math> is uniquely associated with vertex <math>v_i \\in V </math>. The search for the nearest neighbors to a query ''q'' in the set ''S'' takes the form of searching for the vertex in the graph <math>G(V,E)</math>.\nThe basic algorithm – greedy search – works as follows: search starts from an enter-point vertex <math>v_i \\in V </math> by computing the distances from the query q to each vertex of its the neighborhood <math>\\{v_j:(v_i,v_j) \\in E\\}</math>, and then finds a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new enter-point. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.\n\nThe idea of proximity neighborhood graphs was exploited in multiple publications, including the seminal paper by Arya and Mount,<ref>{{cite journal|last1=Arya|first1=Sunil|last2=Mount|first2=David|date=1993|title=Approximate Nearest Neighbor Queries in Fixed Dimensions|journal=Proceedings of the Fourth Annual {ACM/SIGACT-SIAM} Symposium on Discrete Algorithms, 25–27 January 1993, Austin, Texas.|pages=271–280}}</ref> in the VoroNet system for the plane,<ref name=\"voroNet\">{{Cite journal|last1=Olivier|first1=Beaumont|last2=Kermarrec|first2=Anne-Marie|last3=Marchal|first3=Loris|last4=Rivière|first4=Etienne|year=2006|title=VoroNet: A scalable object network based on Voronoi tessellations|journal=INRIA|volume=RR-5833|issue=1|pages=23–29|doi=10.1109/IPDPS.2007.370210|url=https://hal.inria.fr/inria-00071210/PDF/RR-5833.pdf|postscript=.}}</ref> in the RayNet system for the <math>\\mathbb{E}^n</math>,<ref name=\"rayNet\">{{Cite book|last1=Olivier|first1=Beaumont|last2=Kermarrec|first2=Anne-Marie|last3=Rivière|first3=Etienne|year=2007|title=Peer to Peer Multidimensional Overlays: Approximating Complex Structures|journal=Principles of Distributed Systems|volume=4878|issue=|pages=315–328|doi=10.1007/978-3-540-77096-1_23|isbn=978-3-540-77095-4|postscript=.|citeseerx=10.1.1.626.2980}}</ref> and in the Metrized Small World<ref name=\"msw2014\">{{Cite journal|last1=Malkov|first1=Yury|last2=Ponomarenko|first2=Alexander|last3=Krylov|first3=Vladimir|last4=Logvinov|first4=Andrey|year=2014|title=Approximate nearest neighbor algorithm based on navigable small world graphs|journal=Information Systems|volume=45|pages=61–68|doi=10.1016/j.is.2013.10.006|postscript=.}}</ref>  and HNSW<ref name=\":0\" /> algorithms for the general case of spaces with a distance function. These works were preceded by a pioneering paper by Toussaint, in which he introduced the concept of a ''relative neighborhood'' graph.<ref>{{cite journal|last1=Toussaint|first1=Godfried|date=1980|title=The relative neighbourhood graph of a finite planar set|journal=Pattern Recognition|volume=12|issue=4|pages=261–268|doi=10.1016/0031-3203(80)90066-7}}</ref>\n\n====Locality sensitive hashing====\n\n[[Locality sensitive hashing]] (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.<ref>{{cite web|author1=A. Rajaraman  |author2=J. Ullman |lastauthoramp=yes | url=http://infolab.stanford.edu/~ullman/mmds.html |title=Mining of Massive Datasets, Ch. 3 |year=2010}}</ref>\n\n====Nearest neighbor search in spaces with small intrinsic dimension====\n\nThe [[cover tree]] has a theoretical bound that is based on the dataset's [[doubling constant]]. The bound on search time is ''O''(''c''<sup>12</sup>&nbsp;log&nbsp;''n'') where ''c''  is the [[Expansivity constant|expansion constant]] of the dataset.\n\n====Projected radial search====\n\nIn the special case where the data is a dense 3D map of geometric points, the projection geometry of the sensing technique can be used to dramatically simplify the search problem.\nThis approach requires that the 3D data is organized by a projection to a two dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries.\nThese assumptions are valid when dealing with 3D sensor data in applications such as surveying, robotics and stereo vision but may not hold for unorganized data in general.\nIn practice this technique has an average search time of ''O''(''1'')  or ''O''(''K'')  for the ''k''-nearest neighbor problem when applied to real world stereo vision data.\n<ref name=panSearch/>\n\n====Vector approximation files====\n\nIn high-dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.<ref>{{cite journal|title=An Approximation-Based Data Structure for Similarity Search|last1=Weber|first1=Roger|last2=Blott|first2=Stephen|url=https://pdfs.semanticscholar.org/83e4/e3281411ffef40654a4b5d29dae48130aefb.pdf}}</ref>\n\n====Compression/clustering based search====\nThe VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is [[Vector Quantization]] (VQ), implemented through clustering. The database is clustered and the most \"promising\" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed.<ref>{{cite journal|title=Adaptive cluster-distance bounding for similarity search in image databases|last1=Ramaswamy|first1=Sharadh|last2=Rose|first2=Kenneth|journal=ICIP|date=2007}}</ref><ref>{{cite journal|title=Adaptive cluster-distance bounding for high-dimensional indexing|last1=Ramaswamy|first1=Sharadh|last2=Rose|first2=Kenneth|journal=TKDE|date=2010}}</ref> Also note the parallels between clustering and LSH.\n\n==Variants==\n\nThere are numerous variants of the NNS problem and the two most well-known are the [[K-nearest neighbor algorithm|''k''-nearest neighbor search]] and the [[ε-approximate nearest neighbor search]].\n\n===<span id=\"K-nearest neighbor\"> ''k''-nearest neighbors </span>===\n\n[[K-nearest neighbor algorithm|''k''-nearest neighbor search]] identifies the top ''k'' nearest neighbors to the query.  This technique is commonly used in [[predictive analytics]] to estimate or classify a point based on the consensus of its neighbors. ''k''-nearest neighbor graphs are graphs in which every point is connected to its ''k'' nearest neighbors.\n\n===Approximate nearest neighbor===\nIn some applications it may be acceptable to retrieve a \"good guess\" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.\n\nAlgorithms that support the approximate nearest neighbor search include [[Locality-sensitive hashing#LSH algorithm for nearest neighbor search|locality-sensitive hashing]], [[best bin first]] and [[balanced box-decomposition tree]] based search.<ref>{{cite journal|first1=S.|last1=Arya|author2-link=David Mount|first2=D. M.|last2=Mount|author3-link=Nathan Netanyahu|first3=N. S.|last3=Netanyahu|first4=R.|last4=Silverman|first5=A.|last5=Wu|title=An optimal algorithm for approximate nearest neighbor searching|journal= Journal of the ACM|volume=45|number=6|pages=891–923|date=1998|url=http://www.cse.ust.hk/faculty/arya/pub/JACM.pdf|doi=10.1145/293347.293348|citeseerx=10.1.1.15.3125}}</ref>\n\n===Nearest neighbor distance ratio===\n\n[[Nearest neighbor distance ratio]] does not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in [[Content-based image retrieval|CBIR]] to retrieve pictures through a \"query by example\" using the similarity between local features. More generally it is involved in several [[Pattern matching|matching]] problems.\n\n===Fixed-radius near neighbors===\n\n[[Fixed-radius near neighbors]] is the problem where one wants to efficiently find all points given in [[Euclidean space]] within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.\n\n===All nearest neighbors===\n\nFor some applications (e.g. [[entropy estimation]]), we may have ''N'' data-points and wish to know which is the nearest neighbor ''for every one of those N points''. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these ''N'' queries to produce a more efficient search. As a simple example: when we find the distance from point ''X'' to point ''Y'', that also tells us the distance from point ''Y'' to point ''X'', so the same calculation can be reused in two different queries.\n\nGiven a fixed dimension, a semi-definite positive norm (thereby including every  [[lp space|L<sup>p</sup> norm]]), and ''n'' points in this space, the nearest neighbour of every point can be found in ''O''(''n''&nbsp;log&nbsp;''n'') time and the ''m'' nearest neighbours of every point can be found in ''O''(''mn''&nbsp;log&nbsp;''n'') time.<ref>{{citation\n | last = Clarkson | first = Kenneth L. | author-link = Kenneth L. Clarkson\n | contribution = Fast algorithms for the all nearest neighbors problem\n | doi = 10.1109/SFCS.1983.16\n | pages = 226–232\n | title = 24th IEEE Symp. Foundations of Computer Science, (FOCS '83)\n | year = 1983| isbn = 978-0-8186-0508-6 }}.</ref><ref name=Vaidya>{{Cite journal\n | doi = 10.1007/BF02187718\n | last1 = Vaidya | first1 = P. M.\n | year = 1989\n | title = An ''O''(''n''&nbsp;log&nbsp;''n'') Algorithm for the All-Nearest-Neighbors Problem\n | journal = [[Discrete and Computational Geometry]]\n | volume = 4\n | issue = 1\n | pages = 101–115\n | url = http://www.springerlink.com/content/p4mk2608787r7281/?p=09da9252d36e4a1b8396833710ef08cc&pi=8\n | postscript = .\n}}</ref>\n\n==See also==\n{{div col|colwidth=20em}}\n* [[Ball tree]]\n* [[Closest pair of points problem]]\n* [[Cluster analysis]]\n* [[Content-based image retrieval]]\n* [[Curse of dimensionality]]\n* [[Digital signal processing]]\n* [[Dimension reduction]]\n* [[Fixed-radius near neighbors]]\n* [[Fourier analysis]]\n* [[Instance-based learning]]\n* [[k-nearest neighbor algorithm|''k''-nearest neighbor algorithm]]\n* [[Linear least squares (mathematics)|Linear least squares]]\n* [[Locality sensitive hashing]]\n* [[MinHash]]\n* [[Multidimensional analysis]]\n* [[Nearest-neighbor interpolation]]\n* [[Neighbor joining]]\n* [[Principal component analysis]]\n* [[Range search]]\n* [[Set cover problem]]\n* [[Similarity learning]]\n* [[Singular value decomposition]]\n* [[Sparse distributed memory]]\n* [[Statistical distance]]\n* [[Time series]]\n* [[Voronoi diagram]]\n* [[Wavelet]]\n\n{{div col end}}\n\n== Open source implementations ==\n* [[Accord.NET]] has C# implementation of KNN classifier\n* [[ALGLIB]] has C# and C++ implementations of nearest neighbor and approximate nearest neighbor algorithms\n* [https://www.cs.umd.edu/~mount/ANN/ ANN] library (LGPL license) implements exact and approximate NN search in C++\n* [https://github.com/spotify/annoy Annoy] (Apache license) is an implementation in C++ with Python binding from [[Spotify]]\n* [https://falconn-lib.org/ FALCONN] (MIT license) is an LSH-based implementation in C++ with a Python wrapper\n* [https://www.cs.ubc.ca/research/flann/ FLANN] library (BSD license) implements special fast approximate NN algorithms for high-dimensional problems\n* [http://www.cgal.org/Pkg/SpatialSearchingD dD Spatial Searching] in [[CGAL]] – the Computational Geometry Algorithms Library\n* [https://github.com/searchivarius/NonMetricSpaceLib Non-Metric Space Library] – An open source similarity search library containing realisations of various Nearest neighbor search methods.\n* [https://github.com/yahoojapan/NGT NGT] (Apache License 2.0) implements graph-based approximate NN search methods in C++, with Python, C, and [[Go (programming language)|Go]] wrappers.\n\n== References ==\n=== Citations ===\n{{Reflist}}\n\n=== Sources ===\n* {{cite journal |last= Andrews |first=L. |title= A template for the nearest neighbor problem |journal = C/C++ Users Journal |volume=19 |number=11 |date= November 2001 |pages=40–49 |issn = 1075-2838 |url = http://www.ddj.com/architect/184401449}}\n* {{cite journal |last1=Arya|first1=S. |first2=D.M. |last2=Mount|author2-link=David Mount |first3=N. S. |last3=Netanyahu|author3-link=Nathan Netanyahu |first4=R. |last4=Silverman|author4-link=Ruth Silverman |first5=A. Y. |last5=Wu|author5-link=Angela Y. Wu |title = An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions |journal= Journal of the ACM |volume = 45 |number=6 |pages= 891–923 |doi=10.1145/293347.293348|year=1998 |citeseerx=10.1.1.15.3125 }}\n* {{cite journal |last1=Beyer |first1=K. |last2= Goldstein |first2=J. |last3= Ramakrishnan |first3=R. |last4=Shaft |first4=U. |year = 1999 |title=When is nearest neighbor meaningful? |journal=Proceedings of the 7th ICDT }}\n* {{cite journal |first1=Chung-Min |last1= Chen |first2=Yibei |last2=Ling|title=A Sampling-Based Estimator for Top-k Query |journal=ICDE |date=2002 |pages = 617–627 }}\n* {{cite book |last=Samet |first=H.|authorlink=Hanan Samet |year = 2006 |title= Foundations of Multidimensional and Metric Data Structures |publisher= Morgan Kaufmann |isbn = 978-0-12-369446-1 }}\n* {{cite book |last1=Zezula |first1=P. |last2= Amato |first2=G. |last3=Dohnal |first3=V. |last4=Batko |first4=M. |title= Similarity Search – The Metric Space Approach|publisher=Springer |year=2006 |isbn = 978-0-387-29146-8 }}\n\n==Further reading==\n* {{cite book | last = Shasha | first = Dennis | title = High Performance Discovery in Time Series | publisher = Springer | location = Berlin | year = 2004 | isbn = 978-0-387-00857-8 }}\n\n==External links==\n{{commons category|Nearest neighbours search}}\n* [http://simsearch.yury.name/tutorial.html Nearest Neighbors and Similarity Search] – a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits\n* [http://sswiki.tierra-aoi.net Similarity Search Wiki] – a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours\n\n{{DEFAULTSORT:Nearest Neighbor Search}}\n[[Category:Approximation algorithms]]\n[[Category:Classification algorithms]]\n[[Category:Data mining]]\n[[Category:Discrete geometry]]\n[[Category:Geometric algorithms]]\n[[Category:Machine learning]]\n[[Category:Mathematical optimization]]\n[[Category:Search algorithms]]"
    },
    {
      "title": "Nearest neighbour algorithm",
      "url": "https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm",
      "text": "{{About|an approximation algorithm to solve the [[travelling salesman problem]]||Nearest neighbor (disambiguation){{!}}Nearest neighbor}}\n\n{{Infobox Algorithm\n|class=[[Approximation algorithm]]\n|image=\n|caption=\n|data=[[Graph (data structure)|Graph]]\n|time=<math>\\Theta(N^2)</math>\n|space=<math>\\Theta(N)</math>\n|optimal=No\n|complete=\n}}{{Merge|Nearest neighbor search|date=December 2018}}\nThe '''nearest neighbour algorithm''' was one of the first [[algorithm]]s used to solve the [[travelling salesman problem]]. In it, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. It quickly yields a short tour, but usually not the optimal one.\n\n==Algorithm ==\n\nThese are the steps of the algorithm:\n# Initialize all vertices as unvisited.\n# Select an arbitrary vertex, set it as the current vertex '''u'''. Mark '''u''' as visited.\n# Find out the shortest edge connecting the current vertex '''u''' and an unvisited vertex '''v'''.\n# Set '''v''' as the current vertex '''u'''. Mark '''v''' as visited.\n# If all the vertices in the domain are visited, then terminate. Else, go to step 3.\n\nThe sequence of the visited vertices is the output of the algorithm.\n\nThe nearest neighbour algorithm is easy to implement and executes quickly, but it can sometimes miss shorter routes which are easily noticed with human insight, due to its \"greedy\" nature. As a general guide, if the last few stages of the tour are comparable in length to the first stages, then the tour is reasonable; if they are much greater, then it is likely that much better tours exist. Another check is to use an algorithm such as the [[Upper and lower bounds|lower bound]] algorithm to estimate if this tour is good enough.\n\nIn the worst case, the algorithm results in a tour that is much longer than the optimal tour. To be precise, for every constant '''r''' there is an instance of the traveling salesman problem such that the length of the tour computed by the nearest neighbour algorithm is greater than '''r''' times the length of the optimal tour. Moreover, for each number of cities there is an assignment of distances between the cities for which the nearest neighbor heuristic produces the unique worst possible tour. (If the algorithm is applied on every vertex as the starting vertex, the best path found will be better than at least N/2-1 other tours, where N is the number of vertexes)<ref>G. Gutin, A. Yeo and A. Zverovich, 2002</ref>\n\nThe nearest neighbour algorithm may not find a feasible tour at all, even when one exists.\n\n==Notes==\n<references />\n\n==References==\n\n* G. Gutin, A. Yeo and A. Zverovich, [http://www.sciencedirect.com/science/article/pii/S0166218X01001950 Traveling salesman should not be greedy: domination analysis of greedy-type heuristics for the TSP]. Discrete Applied Mathematics 117 (2002), 81-86. \n* J. Bang-Jensen, G. Gutin and A. Yeo, [http://www.sciencedirect.com/science/article/pii/S1572528604000222 When the greedy algorithm fails]. Discrete Optimization 1 (2004), 121-127. \n* G. Bendall and F. Margot, [http://www.sciencedirect.com/science/article/pii/S1572528606000430 Greedy Type Resistance of Combinatorial Problems], Discrete Optimization 3 (2006), 288-298. \n{{DEFAULTSORT:Nearest Neighbour Algorithm}}\n[[Category:Travelling salesman problem]]\n[[Category:Approximation algorithms]]\n[[Category:Heuristic algorithms]]\n[[Category:Graph algorithms]]"
    }
  ]
}