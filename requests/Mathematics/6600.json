{
  "pages": [
    {
      "title": "Particle swarm optimization",
      "url": "https://en.wikipedia.org/wiki/Particle_swarm_optimization",
      "text": "{{Use American English|date=January 2019}}\n{{Short description|Iterative simulation method}}\n[[File:ParticleSwarmArrowsAnimation.gif|thumb|A particle swarm searching for the global minimum of a function]]\n{{Evolutionary algorithms}}\nIn [[computational science]], '''particle swarm optimization''' ('''PSO''')<ref name=\"GOLBO4\"/> is a computational method that [[Mathematical optimization|optimizes]] a problem by [[iterative method|iteratively]] trying to improve a [[candidate solution]] with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed [[Point particle|particle]]s, and moving these particles around in the [[Optimization (mathematics)#Concepts and notation|search-space]] according to simple [[formula|mathematical formulae]] over the particle's [[Position (vector)|position]] and [[velocity]]. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.\n\nPSO is originally attributed to [[James Kennedy (social psychologist)|Kennedy]], [[Russell C. Eberhart|Eberhart]] and Shi<ref name=kennedy95particle/><ref name=shi98modified/> and was first intended for [[computer simulation|simulating]] [[social behaviour]],<ref name=kennedy97particle/> as a stylized representation of the movement of organisms in a bird [[Flocking (behavior)|flock]] or [[fish school]]. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart<ref name=kennedy01swarm/> describes many philosophical aspects of PSO and [[swarm intelligence]]. An extensive  survey of PSO applications is made by [[Riccardo Poli|Poli]].<ref name=poli07analysis/><ref name=poli08analysis/> Recently, a comprehensive review on theoretical and experimental works on PSO has been published by Bonyadi and Michalewicz.<ref name=bonyadi16survey/>\n\nPSO is a [[metaheuristic]] as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. Also, PSO does not use the [[gradient]] of the problem being optimized, which means PSO does not require that the optimization problem be [[Differentiable function|differentiable]] as is required by classic optimization methods such as [[gradient descent]] and [[quasi-newton methods]].\n\n== Algorithm ==\n\nA basic variant  of the PSO algorithm works by having a population (called a swarm) of [[candidate solution]]s (called particles). These particles are moved around in the search-space according to a few simple formulae.<ref>{{cite journal|last1=Zhang|first1=Y.|title=A Comprehensive Survey on Particle Swarm Optimization Algorithm and Its Applications|journal=Mathematical Problems in Engineering|date=2015|volume=2015|page=931256|url=http://www.hindawi.com/journals/mpe/2015/931256}}</ref> The movements of the particles are guided by their own best known position in the search-space as well as the entire swarm's best known position. When improved positions are being discovered these will then come to guide the movements of the swarm. The process is repeated and by doing so it is hoped, but not guaranteed, that a satisfactory solution will eventually be discovered.\n\nFormally, let ''f'':&nbsp;ℝ<sup>''n''</sup>&nbsp;→ ℝ be the cost function which must be minimized. The function takes a candidate solution as an argument in the form of a [[Row vector|vector]] of [[real number]]s and produces a real number as output which indicates the objective function value of the given candidate solution. The [[gradient]] of ''f'' is not known. The goal is to find a solution '''a''' for which ''f''('''a''')&nbsp;≤&nbsp;''f''('''b''') for all '''b''' in the search-space, which would mean '''a''' is the global minimum.\n\nLet ''S'' be the number of particles in the swarm, each having a position '''x'''<sub>i</sub>&nbsp;∈ ℝ<sup>''n''</sup> in the search-space and a velocity '''v'''<sub>i</sub>&nbsp;∈ ℝ<sup>''n''</sup>. Let '''p'''<sub>i</sub> be the best known position of particle ''i'' and let '''g''' be the best known position of the entire swarm. A basic PSO algorithm is then:<ref name=clerc12spso/>\n \n<!-- Please see discussion page why this particular PSO variant was chosen. --> \n  '''for''' each particle ''i''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;''S'' '''do'''\n    Initialize the particle's position with a [[Uniform distribution (continuous)|uniformly distributed]] random vector: '''x'''<sub>i</sub>&nbsp;~&nbsp;''U''('''b<sub>lo</sub>''',&nbsp;'''b<sub>up</sub>''')\n    Initialize the particle's best known position to its initial position: '''p'''<sub>i</sub>&nbsp;←&nbsp;'''x'''<sub>i</sub>\n    '''if''' ''f''('''p'''<sub>i</sub>) < ''f''('''g''') '''then'''\n        update the swarm's best known  position: '''g'''&nbsp;←&nbsp;'''p'''<sub>i</sub>\n    Initialize the particle's velocity: '''v'''<sub>i</sub>&nbsp;~&nbsp;''U''(-|'''b<sub>up</sub>'''-'''b<sub>lo</sub>'''|,&nbsp;|'''b<sub>up</sub>'''-'''b<sub>lo</sub>'''|)\n '''while''' a termination criterion is not met '''do''':\n    '''for''' each particle ''i''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;''S'' '''do'''\n       '''for''' each dimension ''d''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;''n'' '''do'''\n          Pick random numbers: ''r''<sub>p</sub>, ''r''<sub>g</sub> ~ ''U''(0,1)\n          Update the particle's velocity: '''v'''<sub>i,d</sub>&nbsp;←&nbsp;ω '''v'''<sub>i,d</sub> + φ<sub>p</sub> ''r''<sub>p</sub> ('''p'''<sub>i,d</sub>-'''x'''<sub>i,d</sub>) + φ<sub>g</sub> ''r''<sub>g</sub> ('''g'''<sub>d</sub>-'''x'''<sub>i,d</sub>)\n       Update the particle's position: '''x'''<sub>i</sub>&nbsp;←&nbsp;'''x'''<sub>i</sub> + '''v'''<sub>i</sub>\n       '''if''' ''f''('''x'''<sub>i</sub>) < ''f''('''p'''<sub>i</sub>) '''then'''\n          Update the particle's best known position: '''p'''<sub>i</sub>&nbsp;←&nbsp;'''x'''<sub>i</sub>\n          '''if''' ''f''('''p'''<sub>i</sub>) < ''f''('''g''') '''then'''\n             Update the swarm's best known position: '''g'''&nbsp;←&nbsp;'''p'''<sub>i</sub>\nThe values '''b<sub>lo</sub>''' and '''b<sub>up</sub>''' represents the lower and upper boundaries of the search-space. The termination criterion can be the number of iterations performed, or a solution where the adequate objective function value is found.<ref name=bratton2007/> The parameters ω, φ<sub>p</sub>, and φ<sub>g</sub> are selected by the practitioner and control the behaviour and efficacy of the PSO method, see [[#Parameter selection|below]].\n\n== Parameter selection ==\n[[File:PSO Meta-Fitness Landscape (12 benchmark problems).JPG|thumb|Performance landscape showing how a simple PSO variant performs in aggregate on several benchmark problems when varying two PSO parameters.]]\n\nThe choice of PSO parameters can have a large impact on optimization performance. Selecting PSO parameters that yield good performance has therefore been the subject of much research.<ref name=\"GOLBO4\"/><ref name=taherkhani2016inertia/><ref name=shi98parameter/><ref name=eberhart00comparing/><ref name=carlisle01offtheshelf/><ref name=bergh01thesis/><ref name=clerc02explosion/><ref name=trelea03particle/><ref name=bratton08simplified/><ref name=evers09thesis/>\n\nThe PSO parameters can also be tuned by using another overlaying optimizer, a concept known as [[meta-optimization]],<ref name=meissner06optimized/><ref name=pedersen08thesis/><ref name=pedersen08simplifying/><ref name=mason2017meta/> or even fine-tuned during the optimization, e.g., by means of fuzzy logic.<ref name=\"nobile2017\"/><ref name=\"nobile2015\"/>\n\nParameters have also been tuned for various optimization scenarios.<ref name=cazzaniga2015/><ref name=pedersen10good-pso/>\n\n==Neighbourhoods and topologies==\nThe topology of the swarm defines the subset of particles with which each particle can exchange information.<ref name=kennedy2002population/> The basic version of the algorithm uses the global topology as the swarm communication structure.<ref name=bratton2007/> This topology allows all particles to communicate with all the other particles, thus the whole swarm share the same best position '''g''' from a single particle. However, this approach might lead the swarm to be trapped into a local minimum,<ref>Mendes, R. (2004). [https://pdfs.semanticscholar.org/d224/80b09d1f0759fb20e0fb0bd2de205457c8bc.pdf Population Topologies and Their Influence in Particle Swarm Performance] (PhD thesis). Universidade do Minho.</ref> thus different topologies have been used to control the flow of information among particles. For instance, in local topologies, particles only share information with a subset of particles.<ref name=bratton2007/> This subset can be a geometrical one<ref>Suganthan, Ponnuthurai N. \"[https://ieeexplore.ieee.org/abstract/document/785514/ Particle swarm optimiser with neighbourhood operator].\" Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress on. Vol. 3. IEEE, 1999.</ref> – for example \"the ''m'' nearest particles\" – or, more often, a social one, i.e. a set of particles that is not depending on any distance. In such cases, the PSO variant is said to be local best (vs global best for the basic PSO).\n\nA commonly used swarm topology is the ring, in which each particle has just two neighbours, but there are many others.<ref name=bratton2007/> The topology is not necessarily static. In fact, since the topology is related to the diversity of communication of the particles,<ref name=oliveira2016communication/> some efforts have been done to create adaptive topologies (SPSO,<ref>SPSO [http://www.particleswarm.info Particle Swarm Central]</ref> APSO,<ref> Almasi, O. N. and Khooban, M. H. (2017). A parsimonious SVM model selection criterion for classification of real-world data sets via an adaptive population-based algorithm. Neural Computing and Applications, 1-9. [https://link.springer.com/article/10.1007/s00521-017-2930-y https://doi.org/10.1007/s00521-017-2930-y]</ref> stochastic star,<ref>Miranda, V., Keko, H. and Duque, Á. J. (2008). [https://repositorio.inesctec.pt/bitstream/123456789/1561/1/PS-05818.pdf Stochastic Star Communication Topology in Evolutionary Particle Swarms (EPSO)]. International Journal of Computational Intelligence Research (IJCIR), Volume 4, Number 2, pp. 105-116</ref> TRIBES,<ref>Clerc, M. (2006). Particle Swarm Optimization. ISTE (International Scientific and Technical Encyclopedia), 2006</ref> Cyber Swarm,<ref>Yin, P., Glover, F., Laguna, M., & Zhu, J. (2011). [http://leeds-faculty.colorado.edu/glover/fred%20pubs/428%20-%20A_complementary_cyber_swarm_algorithm_pub%20version%20w%20pen%20et%20al.pdf A Complementary Cyber Swarm Algorithm]. International Journal of Swarm Intelligence Research (IJSIR), 2(2), 22-41</ref> and C-PSO<ref name=elshamy07sis/>).\n\n== Inner workings ==\nThere are several [[schools of thought]] as to why and how the PSO algorithm can perform optimization.\n\nA common belief amongst researchers is that the swarm behaviour varies between exploratory behaviour, that is, searching a broader region of the search-space, and exploitative behaviour, that is, a locally oriented search so as to get closer to a (possibly local) optimum. This school of thought has been prevalent since the inception of PSO.<ref name=shi98modified/><ref name=kennedy97particle/><ref name=shi98parameter/><ref name=clerc02explosion/> This school of thought contends that the PSO algorithm and its parameters must be chosen so as to properly balance between exploration and exploitation to avoid [[premature convergence]] to a [[local optimum]] yet still ensure a good rate of [[Convergent sequence|convergence]] to the optimum. This belief is the precursor of many PSO variants, see [[#Variants|below]].\n\nAnother school of thought is that the behaviour of a PSO swarm is not well understood in terms of how it affects actual optimization performance, especially for higher-dimensional search-spaces and optimization problems that may be discontinuous, noisy, and time-varying. This school of thought merely tries to find PSO algorithms and parameters that cause good performance regardless of how the swarm behaviour can be interpreted in relation to e.g. exploration and exploitation. Such studies have led to the simplification of the PSO algorithm, see [[#Simplifications|below]].\n\n=== Convergence ===\nIn relation to PSO the word ''convergence'' typically refers to two different definitions:\n\n* Convergence of the sequence of solutions (aka, stability analysis, [[convergent sequence|converging]]) in which all particles have converged to a point in the search-space, which may or may not be the optimum,\n* Convergence to a local optimum where all personal bests '''p''' or, alternatively, the swarm's best known position '''g''', approaches a local optimum of the problem, regardless of how the swarm behaves.\n\nConvergence of the sequence of solutions has been investigated for PSO.<ref name=bergh01thesis/><ref name=clerc02explosion/><ref name=trelea03particle/> These analyses have resulted in guidelines for selecting PSO parameters that are believed to cause convergence to a point and prevent divergence of the swarm's particles (particles do not move unboundedly and will converge to somewhere). However, the analyses were criticized by Pedersen<ref name=pedersen08simplifying/> for being oversimplified as they assume the swarm has only one particle, that it does not use stochastic variables and that the points of attraction, that is, the particle's best known position '''p''' and the swarm's best known position '''g''', remain constant throughout the optimization process. However, it was shown<ref>{{cite journal|last1=Cleghorn|first1=Christopher W|title=Particle Swarm Convergence: Standardized Analysis and Topological Influence|journal=Swarm Intelligence Conference|date=2014|url=https://link.springer.com/chapter/10.1007/978-3-319-09952-1_12}}</ref> that these simplifications do not affect the boundaries found by these studies for parameter where the swarm is convergent. Considerable effort has been made in recent years to weaken the modelling assumption utilized during the stability analysis of PSO <ref name=Liu2015/>, with the most recent generalized result applying to numerous PSO variants and utilized what was shown to be the minimal necessary modeling assumptions <ref name=Cleghorn2018/>.\n\nConvergence to a local optimum has been analyzed for PSO in<ref>{{cite journal|last1=Van den Bergh|first1=F|title=A convergence proof for the particle swarm optimiser|journal=Fundamenta Informaticae|url=https://repository.up.ac.za/bitstream/handle/2263/17262/VanDenBergh_Convergence(2010).pdf?sequence=1}}</ref> and.<ref name=Bonyadi2014/> It has been proven that PSO need some modification to guarantee to find a local optimum.\n\nThis means that determining convergence capabilities of different PSO algorithms and parameters therefore still depends on [[empirical]] results. One attempt at addressing this issue is the development of an \"orthogonal learning\" strategy for an improved use of the information already existing in the relationship between '''p''' and '''g''', so as to form a leading converging exemplar and to be effective with any PSO topology. The aims are to improve the performance of PSO overall, including faster global convergence, higher solution quality, and stronger robustness.<ref name=zhan10OLPSO/> However, such studies do not provide theoretical evidence to actually prove their claims.\n\n=== Adaptive mechanisms ===\nWithout the need for a trade-off between convergence ('exploitation') and divergence ('exploration'), an adaptive mechanism can be introduced. Adaptive particle swarm optimization (APSO) <ref name=zhan09adaptive/> features better search efficiency than standard PSO. APSO can perform global search over the entire search space with a higher convergence speed. It enables automatic control of the inertia weight, acceleration coefficients, and other algorithmic parameters at the run time, thereby improving the search effectiveness and efficiency at the same time. Also, APSO can act on the globally best particle to jump out of the likely local optima. However, APSO will introduce new algorithm parameters, it does not introduce additional design or implementation complexity nonetheless.\n\n== Variants ==\n<!-- Please provide complete references to journal / conference papers, tech reports, msc/phd theses, etc. when adding PSO variants. Please only include major research contributions and keep the descriptions short - there are probably hundreds of PSO variants in existence and Wikipedia is not the proper place to list them all. -->\n\nNumerous variants of even a basic PSO algorithm are possible. For example, there are different ways to initialize the particles and velocities (e.g. start with zero velocities instead), how to dampen the velocity, only update '''p'''<sub>i</sub> and '''g''' after the entire swarm has been updated, etc. Some of these choices and their possible performance impact have been discussed in the literature.<ref name=carlisle01offtheshelf/>\n\nA series of standard implementations have been created by leading researchers, \"intended for use both as a baseline for performance testing of improvements to the technique, as well as to represent PSO to the wider optimization community. Having a well-known, strictly-defined standard algorithm provides a valuable point of comparison which can be used throughout the field of research to better test new advances.\"<ref name=bratton2007/> The latest is Standard PSO 2011 (SPSO-2011).<ref name=Zambrano-Bigiarini2013/>\n\n=== Hybridization ===\n\nNew and more sophisticated PSO variants are also continually being introduced in an attempt to improve optimization performance. There are certain trends in that research; one is to make a hybrid optimization method using PSO combined with other optimizers,<ref name=lovbjerg02lifecycle/><ref name=niknam10efficient/><ref name=zx03depso/> e.g., combined PSO with biogeography-based optimization,<ref>{{cite journal|last1=Zhang|first1=Y.|last2=Wang|first2=S.|title=Pathological Brain Detection in Magnetic Resonance Imaging Scanning by Wavelet Entropy and Hybridization of Biogeography-based Optimization and Particle Swarm Optimization|journal=Progress in Electromagnetics Research – Pier|date=2015|volume=152|pages=41–58|doi=10.2528/pier15040602}}</ref> and the incorporation of an effective learning method.<ref name=zhan10OLPSO/>\n\n=== Alleviate premature convergence===\nAnother research trend is to try and alleviate premature convergence (that is, optimization stagnation), e.g. by reversing or perturbing the movement of the PSO particles,<ref name=evers09thesis/><ref name=lovbjerg02extending/><ref name=xinchao10perturbed/><ref name=xzy02dpso/> another approach to deal with premature convergence is the use of multiple swarms<ref>Cheung, N. J., Ding, X.-M., & Shen, H.-B. (2013). OptiFel: A Convergent Heterogeneous Particle Sarm Optimization Algorithm for Takagi-Sugeno Fuzzy Modeling, IEEE Transactions on Fuzzy Systems, {{DOI|10.1109/TFUZZ.2013.2278972}}</ref> ([[multi-swarm optimization]]). The multi-swarm approach can also be used to implement multi-objective optimization.<ref name=nobile2012 /> Finally, there are  developments in adapting the behavioural parameters of PSO during optimization.<ref name=zhan09adaptive/><ref name=nobile2017/>\n\n=== Simplifications ===\nAnother school of thought is that PSO should be simplified as much as possible without impairing its performance; a general concept often referred to as [[Occam's razor]]. Simplifying PSO was originally suggested by Kennedy<ref name=kennedy97particle/> and has been studied more extensively,<ref name=bratton08simplified/><ref name=pedersen08thesis/><ref name=pedersen08simplifying/><ref name=yang08nature/> where it appeared that optimization performance was improved, and the parameters were easier to tune and they performed more consistently across different optimization problems.\n\nAnother argument in favour of simplifying PSO is that [[metaheuristic]]s can only have their efficacy demonstrated [[empirical]]ly by doing computational experiments on a finite number of optimization problems. This means a metaheuristic such as PSO cannot be [[Program correctness|proven correct]] and this increases the risk of making errors in its description and implementation. A good example of this<ref name=tu04robust/> presented a promising variant of a [[genetic algorithm]] (another popular metaheuristic) but it was later found to be defective as it was strongly biased in its optimization search towards similar values for different dimensions in the search space, which happened to be the optimum of the benchmark problems considered. This bias was because of a programming error, and has now been fixed.<ref name=tu04corrections/>\n\nInitialization of velocities may require extra inputs. The Bare Bones PSO variant<ref>{{Cite journal|last=Kennedy|first=James|date=2003|title=Bare Bones Particle Swarms|url=|journal=Proceedings of the 2003 IEEE Swarm Intelligence Symposium|volume=|issue=|doi=|pmid=|access-date=|via=}}</ref> has been proposed in 2003 by James Kennedy, and does not need to use velocity at all.\n\nAnother simpler variant is the accelerated particle swarm optimization (APSO),<ref>X. S. Yang, S. Deb and S. Fong, [https://arxiv.org/pdf/1203.6577 Accelerated particle swarm optimization and support vector machine for business optimization and applications], NDT 2011, Springer CCIS 136, pp. 53-66 (2011).</ref> which also does not need to use velocity and can speed up the convergence in many applications. A simple demo code of APSO is available.<ref>{{Cite web | url=http://www.mathworks.com/matlabcentral/fileexchange/?term=APSO | title=Search Results: APSO - File Exchange - MATLAB Central}}</ref>\n\n===Multi-objective optimization===\nPSO has also been applied to [[multi-objective optimization|multi-objective problems]],<ref name=parsopoulos02particle/><ref name=coellocoello02MOPSO/><ref name=MASON2017188/> in which the objective function comparison takes [[pareto efficiency|pareto dominance]] into account when moving the PSO particles and non-dominated solutions are stored so as to approximate the pareto front.\n\n===Binary, discrete, and combinatorial===\nAs the PSO equations given above work on real numbers, a commonly used method to solve discrete problems is to map the discrete search space to a continuous domain, to apply a classical PSO, and then to demap the result.  Such a mapping can be very simple (for example by just using rounded values) or more sophisticated.<ref>Roy, R., Dehuri, S., & Cho, S. B. (2012). [http://sclab.yonsei.ac.kr/publications/Papers/IJ/A%20Novel%20Particle%20Swarm%20Optimization%20Algorithm%20for%20Multi-Objective%20Combinatorial%20Optimization%20Problem.pdf A Novel Particle Swarm Optimization Algorithm for Multi-Objective Combinatorial Optimization Problem]. 'International Journal of Applied Metaheuristic Computing (IJAMC)', 2(4), 41-57</ref>\n\nHowever, it can be noted that the equations of movement make use of operators that perform four actions:\n*computing the difference of two positions. The result is a velocity (more precisely a displacement)\n*multiplying a velocity by a numerical coefficient\n*adding two velocities\n*applying a velocity to a position\n\nUsually a position and a velocity are represented by ''n'' real numbers, and  these operators are simply -, *, +, and again +. But all these mathematical objects can be defined in a completely different way, in order to cope with binary problems (or more generally discrete ones), or even combinatorial ones.<ref>Kennedy, J. & Eberhart, R. C. (1997). [http://ahmetcevahircinar.com.tr/wp-content/uploads/2017/02/A_discrete_binary_version_of_the_particle_swarm_algorithm.pdf A discrete binary version of the particle swarm algorithm], Conference on Systems, Man, and Cybernetics, Piscataway, NJ: IEEE Service Center, pp. 4104-4109</ref><ref>Clerc, M. (2004). [https://link.springer.com/chapter/10.1007/978-3-540-39930-8_8 Discrete Particle Swarm Optimization, illustrated by the Traveling Salesman Problem], New Optimization Techniques in Engineering, Springer, pp. 219-239</ref><ref>Clerc, M. (2005). Binary Particle Swarm Optimisers: toolbox, derivations, and mathematical insights, [http://hal.archives-ouvertes.fr/hal-00122809/en/ Open Archive HAL]</ref><ref>Jarboui, B., Damak, N., Siarry, P., and Rebai, A.R. (2008).  [https://www.sciencedirect.com/science/article/pii/S009630030700553X A combinatorial particle swarm optimization for solving multi-mode resource-constrained project scheduling problems].  In Proceedings of Applied Mathematics and Computation, pp. 299-308.</ref> One approach is to redefine the operators based on sets.<ref name=Chen10SPSO />\n\n== See also ==\n<!-- Please only add optimizers that are conceptually related to PSO. The navbox at the bottom contains the most popular optimizers in this field, and the categories lists all optimizers. Spam will be removed. -->\n* [[Bees algorithm]] / [[Artificial bee colony algorithm]]\n* [[Derivative-free optimization]]\n* [[Multi-swarm optimization]]\n* [[Particle filter]]\n* [[Swarm intelligence]]\n* [[Fish School Search]]\n* [[Dispersive Flies Optimisation]]\n\n== References ==\n<!-- Please provide complete references to journal / conference papers, tech reports, msc/phd theses, etc. and make sure the reference format is correct. Only include major research contributions - there are hundreds of PSO variants in existence and Wikipedia is not the proper place to list them all. Only add a reference when you use it in a concise description in the main article. -->\n{{Reflist|refs=\n\n<ref name=\"GOLBO4\">{{Cite journal | url =http://www.jpier.org/PIERM/pier.php?paper=18011016| title=PATTERN SYNTHESIS FOR THE CYLINDRICAL POLARIMETRIC PHASED ARRAY RADAR (CPPAR)|date= 2018| last1=Golbon-Haghighi|first1=M.H.|last2= H. Saeidi-manesh|last3= G. Zhang|last4= Y. Zhang| journal= Progress in Electromagnetics Research M|volume=66| pages=87–98}}</ref>\n\n<ref name=\"lovbjerg02lifecycle\">{{cite conference | first1=M. | last1=Lovbjerg | title=The LifeCycle Model: combining particle swarm optimisation, genetic algorithms and hillclimbers | last2=Krink | first2=T. | booktitle=Proceedings of Parallel Problem Solving from Nature VII (PPSN) | year=2002 | pages=621–630|url=http://www.lovbjerghome.dk/Morten/EvaLife/TK_PPSN2002_LifeCycle_PSO_HC_GA.pdf}}</ref>\n\n<ref name=\"bonyadi16survey\">{{cite journal | first1=M. R. | last1=Bonyadi | title=Particle swarm optimization for single objective continuous space problems: a review | last2=Michalewicz | first2=Z. | journal=Evolutionary Computation | year=2017 | volume=25 | issue=1 | pages=1–54 | doi=10.1162/EVCO_r_00180| pmid=26953883 }}</ref>\n\n<ref name=\"niknam10efficient\">{{cite journal | first1=T. | last1=Niknam | title=An efficient hybrid approach based on PSO, ACO and k-means for cluster analysis | last2=Amiri | first2=B. | journal=Applied Soft Computing | year=2010 | volume=10 | issue=1 | pages=183–197 | doi=10.1016/j.asoc.2009.07.001}}</ref>\n\n<ref name=\"lovbjerg02extending\">{{cite conference | first1=M. | last1=Lovbjerg | title=Extending Particle Swarm Optimisers with Self-Organized Criticality | last2=Krink | first2=T. | booktitle=Proceedings of the Fourth Congress on Evolutionary Computation (CEC) | year=2002 | volume=2 | pages=1588–1593|url=http://www.lovbjerghome.dk/Morten/EvaLife/ML_CEC2002_SOCPSO.pdf}}</ref>\n\n<ref name=\"xinchao10perturbed\">{{cite journal | title=A perturbed particle swarm algorithm for numerical optimization | last=Xinchao | first=Z. | journal=Applied Soft Computing | year=2010 | volume=10 | issue=1 | pages=119–124 | doi=10.1016/j.asoc.2009.06.010}}</ref>\n\n<ref name=\"zhan09adaptive\">{{cite journal | first1=Z-H. | last1=Zhan | url=https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4812104 | title=Adaptive Particle Swarm Optimization | last2=Zhang | first2=J. | last3=Li | first3=Y | last4=Chung | first4=H.S-H. | journal=IEEE Transactions on Systems, Man, and Cybernetics | year=2009 | volume=39 | issue=6 | pages=1362–1381 | doi=10.1109/TSMCB.2009.2015956 | pmid=19362911 }}</ref>\n\n<ref name=\"zhan10OLPSO\">{{cite journal | first1=Z-H. | last1=Zhan | url=https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5560790 | title=Orthogonal Learning Particle Swarm Optimization | last2=Zhang | first2=J. | last3=Li | first3=Y | last4=Shi | first4=Y-H. | journal=IEEE Transactions on Evolutionary Computation | year=2011 | volume=15 | issue=6 | pages=832–847| doi=10.1109/TEVC.2010.2052054 }}</ref>\n\n<ref name=\"Bonyadi2014\">{{cite journal | first1=Mohammad reza. | last1=Bonyadi | title=A locally convergent rotationally invariant particle swarm optimization algorithm | last2=Michalewicz | first2=Z. | journal=Swarm Intelligence | year=2014 | volume=8 | issue=3 | pages=159–198 | doi=10.1007/s11721-014-0095-1}}</ref>\n\n<!-- not used\n<ref name=\"Bonyadi2014Analysis\">{{cite journal | first1=Mohammad reza. | last1=Bonyadi | title=An analysis of the velocity updating rule of the particle swarm optimization algorithm | last2=Michalewicz | first2=Z. | journal=Journal of Heuristics | year=2014 | volume=20 | issue=4 | pages=417–452 | doi=10.1007/s10732-014-9245-2}}</ref> -->\n\n<ref name=\"Cleghorn2018\">{{cite journal | first1=Christopher W. | last1=Cleghorn | title=Particle Swarm Stability: A Theoretical Extension using the Non-Stagnate Distribution Assumption | last2=Engelbrecht | first2=Andries. | journal=Swarm Intelligence | year=2018 | volume=12 | issue=1 | pages=1–22 | doi=10.1007/s11721-017-0141-x}}</ref>\n\n<ref name=\"Liu2015\">{{cite journal | first1=Q | last1=Liu | title=Order-2 stability analysis of particle swarm optimization | journal=Evolutionary Computation | year=2015 | volume=23 | issue=2 | pages=187–216 | doi=10.1162/EVCO_a_00129| pmid=24738856 }}</ref>\n\n<ref name=\"kennedy95particle\">{{cite conference | first1=J. | last1=Kennedy | url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=488968 | title=Particle Swarm Optimization | last2=Eberhart | first2=R. | booktitle=Proceedings of IEEE International Conference on Neural Networks | year=1995 | volume=IV | pages=1942–1948 | doi=10.1109/ICNN.1995.488968}}</ref>\n\n<ref name=\"kennedy97particle\">{{cite conference | first1=J. | last1=Kennedy | title=The particle swarm: social adaptation of knowledge | booktitle=Proceedings of IEEE International Conference on Evolutionary Computation | year=1997 | pages=303–308|url=https://ieeexplore.ieee.org/abstract/document/592326/}}</ref>\n\n<ref name=\"shi98modified\">{{cite conference | first1=Y. | last1=Shi | url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=699146 | title=A modified particle swarm optimizer | last2=Eberhart | first2=R.C. | booktitle=Proceedings of IEEE International Conference on Evolutionary Computation | year=1998 | pages=69–73}}</ref>\n\n<ref name=\"shi98parameter\">{{cite conference | first1=Y. | last1=Shi | title=Parameter selection in particle swarm optimization | last2=Eberhart | first2=R.C. | booktitle=Proceedings of Evolutionary Programming VII (EP98) | year=1998 | pages=591–600}}</ref>\n\n<ref name=\"eberhart00comparing\">{{cite conference | first1=R.C. | last1=Eberhart | title=Comparing inertia weights and constriction factors in particle swarm optimization | last2=Shi | first2=Y. | booktitle=Proceedings of the Congress on Evolutionary Computation | year=2000 | volume=1 | pages=84–88|url=https://www.researchgate.net/profile/Yuhui_Shi2/publication/3865142_Comparing_inertial_weights_and_Constriction_factor_in_particle_swarm_optimization/links/54d9a96f0cf24647581f009a.pdf}}</ref>\n\n<ref name=\"carlisle01offtheshelf\">{{cite conference | archive-date=2003-05-03 | archive-url=https://web.archive.org/web/20030503203304/http://antho.huntingdon.edu/publications/Off-The-Shelf_PSO.pdf | first1=A. | last1=Carlisle | url=http://antho.huntingdon.edu/publications/Off-The-Shelf_PSO.pdf | title=An Off-The-Shelf PSO | last2=Dozier | first2=G. | booktitle=Proceedings of the Particle Swarm Optimization Workshop | dead-url=yes | year=2001 | pages=1–6}}</ref>\n\n<ref name=\"zx03depso\">Zhang, Wen-Jun; Xie, Xiao-Feng (2003). [http://www.wiomax.com/team/xie/paper/SMCC03.pdf DEPSO: hybrid particle swarm with differential evolution operator]. ''IEEE International Conference on Systems, Man, and Cybernetics'' (SMCC), Washington, DC, USA: 3816-3821.</ref>\n\n<ref name=\"bergh01thesis\">{{cite book | title=An Analysis of Particle Swarm Optimizers | publisher=University of Pretoria, Faculty of Natural and Agricultural Science | last=van den Bergh | first=F. | year=2001 | type=PhD thesis|url=http://bee22.com/resources/Bergh%202006.pdf}}</ref>\n\n<ref name=\"trelea03particle\">{{cite journal | title=The Particle Swarm Optimization Algorithm: convergence analysis and parameter selection | last=Trelea | first=I.C. | journal=Information Processing Letters | year=2003 | volume=85 | issue=6 | pages=317–325 | doi=10.1016/S0020-0190(02)00447-7}}</ref>\n\n<ref name=\"clerc02explosion\">{{cite journal | first1=M. | last1=Clerc | title=The particle swarm - explosion, stability, and convergence in a multidimensional complex space | last2=Kennedy | first2=J. | journal=IEEE Transactions on Evolutionary Computation | year=2002 | volume=6 | issue=1 | pages=58–73 | doi=10.1109/4235.985692| citeseerx=10.1.1.460.6608 }}</ref>\n\n<ref name=\"xzy02dpso\">Xie, Xiao-Feng; Zhang, Wen-Jun; Yang, Zhi-Lian (2002). [http://www.wiomax.com/team/xie/paper/CEC02.pdf A dissipative particle swarm optimization]. ''Congress on Evolutionary Computation'' (CEC), Honolulu, HI, USA: 1456-1461.</ref>\n\n<ref name=\"bratton08simplified\">{{cite journal | first1=D. | last1=Bratton | url=http://downloads.hindawi.com/archive/2008/654184.pdf | title=A Simplified Recombinant PSO | last2=Blackwell | first2=T. | journal=Journal of Artificial Evolution and Applications | volume=2008 | pages=1–10 | year=2008| doi=10.1155/2008/654184 }}</ref>\n\n<ref name=\"kennedy01swarm\">{{cite book | first1=J. | last1=Kennedy | title=Swarm Intelligence | publisher=Morgan Kaufmann | last2=Eberhart | first2=R.C. | year=2001 | isbn=978-1-55860-595-4}}</ref>\n\n<ref name=\"pedersen08thesis\">{{cite book | url=http://www.hvass-labs.org/people/magnus/thesis/pedersen08thesis.pdf | title=Tuning & Simplifying Heuristical Optimization | publisher=University of Southampton, School of Engineering Sciences, Computational Engineering and Design Group | last=Pedersen | first=M.E.H. | year=2010 | format=PhD thesis}}</ref>\n\n<ref name=\"pedersen08simplifying\">{{cite journal | url=http://www.hvass-labs.org/people/magnus/publications/pedersen08simplifying.pdf | title=Simplifying particle swarm optimization | last=Pedersen | first=M.E.H. | author2=Chipperfield, A.J. | journal=Applied Soft Computing | year=2010 | volume=10 | issue=2 | pages=618–628 | doi=10.1016/j.asoc.2009.08.029| citeseerx=10.1.1.149.8300 }}</ref>\n\n<ref name=\"mason2017meta\">{{cite journal | title=A Meta Optimisation Analysis of Particle Swarm Optimisation Velocity Update Equations for Watershed Management Learning | last=Mason | first=Karl | author2=Duggan, Jim | author3=Howley, Enda | journal=Applied Soft Computing | year=2018 | volume=62 | pages=148–161 | doi=10.1016/j.asoc.2017.10.018}}</ref>\n\n<ref name=\"pedersen10good-pso\">{{cite journal | url=http://www.hvass-labs.org/people/magnus/publications/pedersen10good-pso.pdf | title=Good parameters for particle swarm optimization | last=Pedersen | first=M.E.H. | journal=Technical Report HL1001 | year=2010}}</ref>\n\n<ref name=\"poli07analysis\">{{cite journal | url=http://cswww.essex.ac.uk/technical-reports/2007/tr-csm469.pdf | title=An analysis of publications on particle swarm optimisation applications | last=Poli | first=R. | journal=Technical Report CSM-469 | year=2007}}</ref>\n\n<ref name=\"poli08analysis\">{{cite journal | url=http://downloads.hindawi.com/archive/2008/685175.pdf | title=Analysis of the publications on the applications of particle swarm optimisation | last=Poli | first=R. | journal=Journal of Artificial Evolution and Applications | year=2008 | volume=2008 | pages=1–10 | doi=10.1155/2008/685175}}</ref>\n\n<ref name=\"evers09thesis\">{{cite book | url=http://www.georgeevers.org/publications.htm | title=An Automatic Regrouping Mechanism to Deal with Stagnation in Particle Swarm Optimization | publisher=The University of Texas - Pan American, Department of Electrical Engineering | last=Evers | first=G. | year=2009 | format=Master's thesis}}</ref>\n\n<ref name=\"tu04robust\">{{cite journal | first1=Z. | last1=Tu | title=A robust stochastic genetic algorithm (StGA) for global numerical optimization | last2=Lu | first2=Y. | journal=IEEE Transactions on Evolutionary Computation | year=2004 | volume=8 | issue=5 | pages=456–470 | doi=10.1109/TEVC.2004.831258}}</ref>\n\n<ref name=\"tu04corrections\">{{cite journal | first1=Z. | last1=Tu | title=Corrections to \"A Robust Stochastic Genetic Algorithm (StGA) for Global Numerical Optimization'' | last2=Lu | first2=Y. | journal=IEEE Transactions on Evolutionary Computation | year=2008 | volume=12 | issue=6 | pages=781 | doi=10.1109/TEVC.2008.926734}}</ref>\n\n<ref name=\"meissner06optimized\">{{cite journal | first1=M. | last1=Meissner | title=Optimized Particle Swarm Optimization (OPSO) and its application to artificial neural network training | last2=Schmuker | first2=M. | last3=Schneider | first3=G. | journal=BMC Bioinformatics | pmc=1464136 | year=2006 | volume=7 | issue=1 | pages=125 | doi=10.1186/1471-2105-7-125 | pmid=16529661}}</ref>\n\n<ref name=\"yang08nature\">{{cite book | first1=X.S. | last1=Yang | title=Nature-Inspired Metaheuristic Algorithms | publisher=Luniver Press | year=2008 | isbn=978-1-905986-10-1}}</ref>\n\n<ref name=\"parsopoulos02particle\">{{cite conference | first1=K. | last1=Parsopoulos | title=Particle swarm optimization method in multiobjective problems | last2=Vrahatis | first2=M. | booktitle=Proceedings of the ACM Symposium on Applied Computing (SAC) | year=2002 | pages=603–607| doi=10.1145/508791.508907 }}</ref>\n\n<ref name=\"MASON2017188\">{{cite journal | title=Multi-objective dynamic economic emission dispatch using particle swarm optimisation variants | last=Mason | first=Karl | author2=Duggan, Jim | author3=Howley, Enda | journal=Neurocomputing | year=2017 | volume=270 | pages=188–197 | doi=10.1016/j.neucom.2017.03.086}}</ref>\n\n<!-- not used \n<ref name=\"BonyadiSPSO2014\">{{cite journal | first1=Mohammad reza | last1=Bonyadi | title=SPSO 2011 analysis of stability; local convergence; and rotation sensitivity. | last2=Michalewicz | first2=Z. | journal=GECCO2014 (the best paper award in the track ACSI) | year=2014 | pages=9–16}}</ref>-->f\n\n<ref name=\"coellocoello02MOPSO\">{{cite conference | first1=C. | last1=Coello Coello | url=http://portal.acm.org/citation.cfm?id=1252327 | title=MOPSO: A Proposal for Multiple Objective Particle Swarm Optimization | last2=Salazar Lechuga | first2=M. | booktitle=Congress on Evolutionary Computation (CEC'2002) | year=2002 | pages=1051–1056}}</ref>\n\n<ref name=\"Chen10SPSO\">{{cite journal | first1=Wei-neng | last1=Chen | title=A novel set-based particle swarm optimization method for discrete optimization problem | last2=Zhang | first2=Jun | journal=IEEE Transactions on Evolutionary Computation | year=2010 | volume=14 | issue=2 | pages=278–300 | doi=10.1109/tevc.2009.2030331| citeseerx=10.1.1.224.5378 }}</ref>\n\n<ref name=\"elshamy07sis\">{{cite conference | first1=W. | last1=Elshamy | url=http://people.cis.ksu.edu/~welshamy/pubs/ieee_sis07.pdf | title=Clubs-based Particle Swarm Optimization | last2=Rashad | first2=H. | last3=Bahgat | first3=A. | booktitle=IEEE Swarm Intelligence Symposium 2007 (SIS2007) | year=2007 | location=Honolulu, HI | pages=289–296}}</ref>\n\n<ref name=\"nobile2012\">{{cite conference | first1=M. | last1=Nobile | title=A GPU-Based Multi-Swarm PSO Method for Parameter Estimation in Stochastic Biological Systems Exploiting Discrete-Time Target Series | last2=Besozzi | first2=D. | last3=Cazzaniga | first3=P. | last4=Mauri | first4=G. | last5=Pescini | first5=D. | booktitle=Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics. Lecture Notes in Computer Science. | year=2012 | volume=7264 | pages=74–85| doi=10.1007/978-3-642-29066-4_7 }}</ref>\n\n<ref name=\"clerc12spso\">{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/76/49/96/PDF/SPSO_descriptions.pdf | title=Standard Particle Swarm Optimisation | last=Clerc | first=M. | journal=HAL Open Access Archive | year=2012}}</ref>\n\n<ref name=\"taherkhani2016inertia\">{{cite journal | first1=M. | last1=Taherkhani | title=A novel stability-based adaptive inertia weight for particle swarm optimization | last2=Safabakhsh | first2=R. | journal=Applied Soft Computing | year=2016 | volume=38 | pages=281–295 | doi=10.1016/j.asoc.2015.10.004}}</ref>\n\n<ref name=\"bratton2007\">{{cite book | first1=Daniel | last1=Bratton | url=http://www.cil.pku.edu.cn/resources/pso_paper/src/2007SPSO.pdf | title=Defining a Standard for Particle Swarm Optimization | last2=Kennedy | first2=James | journal=Proceedings of the 2007 IEEE Swarm Intelligence Symposium (SIS 2007) | pages=120–127 | year=2007| doi=10.1109/SIS.2007.368035 | isbn=978-1-4244-0708-8 }}</ref>\n\n<ref name=\"Zambrano-Bigiarini2013\">{{cite book | first1=M. | last1=Zambrano-Bigiarini | title=Standard Particle Swarm Optimisation 2011 at CEC-2013: A baseline for future PSO improvements | last2=Clerc | first2=M. | last3=Rojas | first3=R. | journal=Evolutionary Computation (CEC), 2013 IEEE Congress on | pages=2337–2344 | year=2013| doi=10.1109/CEC.2013.6557848 | isbn=978-1-4799-0454-9 }}</ref>\n\n<ref name=\"kennedy2002population\">{{cite book | first1=J. | last1=Kennedy | title=Population structure and particle swarm performance | last2=Mendes | first2=R. | journal=Evolutionary Computation, 2002. CEC'02. Proceedings of the 2002 Congress on | volume=2 | pages=1671–1676 vol.2 | year=2002 | doi=10.1109/CEC.2002.1004493| isbn=978-0-7803-7282-5 | citeseerx=10.1.1.114.7988 }}</ref>\n\n<ref name=\"oliveira2016communication\">{{cite book | first1=M. | last1=Oliveira | title=Communication Diversity in Particle Swarm Optimizers | last2=Pinheiro | first2=D. | last3=Andrade | first3=B. | last4=Bastos-Filho | first4=C. | last5=Menezes | first5=R. | journal=International Conference on Swarm Intelligence | volume=9882 | year=2016 | pages=77–88 | doi=10.1007/978-3-319-44427-7_7| series=Lecture Notes in Computer Science | isbn=978-3-319-44426-0 }}</ref>\n\n<ref name=\"cazzaniga2015\">{{cite conference | first1=P. | last1=Cazzaniga | url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7300288&tag=1 | title=The impact of particles initialization in PSO: parameter estimation as a case in point, (Canada) | last2=Nobile | first2=M.S. | last3=Besozzi | first3=D. | booktitle=Proceedings of IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology, | year=2015}}</ref>\n\n<ref name=\"nobile2017\">{{cite journal | first1=M.S | last1=Nobile | title=Fuzzy Self-Tuning PSO: a settings-free algorithm for global optimization | last2=Cazzaniga | first2=P. | last3=Besozzi | first3=D. | last4=Colombo | first4=R. | last5=Mauri | first5=G. | last6=Pasi | first6=G. | journal=Swarm and Evolutionary Computation | volume=39 | pages=70–85 | year=2017 | doi=10.1016/j.swevo.2017.09.001}}</ref>\n\n<ref name=\"nobile2015\">{{cite conference | first1=M.S: | last1=Nobile | url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7337957&filter=AND%28p_Publication_Number:7329077%29 | title=Proactive particles in swarm optimization: a self-tuning algorithm based on fuzzy logic | last2=Pasi | first2=G. | last3=Cazzaniga | first3=P. | last4=Besozzi | first4=D. | last5=Colombo | first5=R. | last6=Mauri | first6=G. | booktitle=Proceedings of the 2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2015), Istanbul (Turkey) | year=2015 | pages=1–8,}}</ref>\n}}\n\n==External links==\n<!-- Please see discussion page before adding / removing links. -->\n*[http://www.particleswarm.info Particle Swarm Central] is a repository for information on PSO. Several source codes are freely available.\n*[http://vimeo.com/17407010 A brief video] of particle swarms optimizing three benchmark functions.\n*[http://www.mathworks.com/matlabcentral/fileexchange/11559-particle-swarm-optimization-simulation Simulation of PSO convergence in a two-dimensional space (Matlab).]\n*[http://www.vocal.com/particle-swarm-optimization/ Applications] of PSO.\n*[http://www.sciencedirect.com/science/article/pii/S0957417408007823 Automatic Calibration of a Rainfall-Runoff Model Using a Fast and  Elitist Multi-objective Particle Swarm  Algorithm]\n*[http://emlab.utep.edu/ee5390cem.htm Particle Swarm Optimization (see and listen to Lecture 27)]\n*[http://www.adaptivebox.net/research/bookmark/psocodes_link.html Links to PSO source code]\n\n{{Major subfields of optimization}}\n{{swarming}}\n\n{{DEFAULTSORT:Particle swarm optimization}}\n[[Category:Metaheuristics]]\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Promoter based genetic algorithm",
      "url": "https://en.wikipedia.org/wiki/Promoter_based_genetic_algorithm",
      "text": "{{Evolutionary algorithms}}\n'''The promoter based genetic algorithm''' ('''PBGA''') is a [[genetic algorithm]] for neuroevolution developed by F. Bellas and R.J. Duro in the [[Integrated Group for Engineering Research]] (GII) at the University of Coruña, in Spain. It evolves variable size feedforward [[artificial neural network]]s (ANN) that are encoded into sequences of genes for constructing a basic ANN unit. Each of these blocks is preceded by a gene promoter acting as an on/off switch that determines if that particular unit will be expressed or not.\n\n==PBGA basics==\n\nThe basic unit in the PBGA is a [[neuron]] with all of its inbound connections as represented in the following figure:\n\n[[File:pbga2.jpg]]\n\nThe [[genotype]] of a basic unit is a set of real valued weights followed by the parameters of the [[neuron]] and proceeded by an integer valued field that determines the promoter gene value and, consequently, the expression of the unit. By concatenating units of this type we can construct the whole network.\n\nWith this encoding it is imposed that the information that is not expressed is still carried by the genotype in evolution but it is shielded from direct selective pressure, maintaining this way the diversity in the population, which has been a design premise for this algorithm. Therefore, a clear difference is established between the search space and the solution space, permitting information learned and encoded into the genotypic representation to be preserved by disabling promoter genes.\n\n==Results ==\n\nThe PBGA was originally presented<ref name=\"bellasIASTED\">F. Bellas, R. J. Duro, (2002) Statistically neutral promoter based GA for evolution with dynamic fitness functions, Proc. of IASTED International Conference Artificial Intelligence and Applications</ref><ref name=\"bellasICONIP\">F. Bellas, R. J. Duro, (2002) Modelling the world with statiscally neutral PBGAs. Enhancement and real applications, Proc. 9th Internacional Conference on Neural Information Processing</ref> within the field of autonomous robotics, in particular in the real time learning of environment models of the robot.\n\nIt has been used inside the [[Multilevel Darwinist Brain]] (MDB) cognitive mechanism  developed in the GII for real robots on-line learning. In another paper<ref name=\"bellas06\">F. Bellas, A. Faiña, A. Prieto, and R.J. Duro (2006), Adaptive Learning Application of the MDB Evolutionary Cognitive Architecture in Physical Agents, Lecture notes on artificial intelligence, vol 4095, 434-445</ref> it is shown how the application of the PBGA together with an external memory that stores the successful obtained world models, is an optimal strategy for adaptation in dynamic environments. \n\nRecently, the PBGA has provided results that outperform other neuroevolutionary algorithms in non-stationary problems, where the fitness function varies in time.<ref name=\"bellas09\">F. Bellas, J.A. Becerra, R. J. Duro, (2009), Using Promoters and Functional Introns in Genetic Algorithms for Neuroevolutionary Learning in Non-Stationary Problems, Neurocomputing 72, 2134-2145</ref>\n\n==References==\n\n<references/>\n\n==External links==\n*[http://www.gii.udc.es Grupo Integrado de Ingeniería]\n*[https://archive.is/20130106140625/http://www.gii.udc.es/francisco_bellas Francisco Bellas’ website]\n*[https://archive.is/20130106133654/http://www.gii.udc.es/richard_duro Richard J. Duro’s website]\n\n[[Category:Artificial neural networks]]\n[[Category:Evolutionary algorithms]]\n[[Category:Evolutionary computation]]\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Reward-based selection",
      "url": "https://en.wikipedia.org/wiki/Reward-based_selection",
      "text": "'''Reward-based selection''' is a technique used in [[evolutionary algorithm]]s for selecting potentially useful solutions for recombination. \nThe probability of being selected for an individual is proportional to the cumulative reward, obtained by the individual. The cumulative reward can be computed as a sum of the individual reward and the reward, inherited from parents.\n\n==Description==\nReward-based selection can be used within [[Multi-armed bandit]] framework for [[Multi-objective optimization]] to obtain a better approximation of the [[Pareto efficiency|Pareto front]].\n<ref>{{cite conference\n |first       = I.\n |last        = Loshchilov\n |author2     = M. Schoenauer\n |author3     = M. Sebag\n |title       = Not all parents are equal for MO-CMA-ES\n |booktitle   = Evolutionary Multi-Criterion Optimization 2011 (EMO 2011)\n |pages       = 31–45\n |publisher   = Springer Verlag, LNCS 6576\n |date        = 2011\n |location    = \n |url         = http://www.lri.fr/~ilya/publications/EMO2011_MOCMAselection.pdf\n |accessdate  = \n |id          = \n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20120604215318/http://www.lri.fr/~ilya/publications/EMO2011_MOCMAselection.pdf\n |archivedate = 2012-06-04\n |df          = \n}}</ref>\n \nThe newborn <math>a'^{(g+1)}</math> and its parents receive a reward <math>r^{(g)}</math>, if <math>a'^{(g+1)}</math> was selected for new population <math>Q^{(g+1)}</math>, otherwise the reward is zero. \nSeveral reward definitions are possible:\n\n*1. <math> r^{(g)}=1</math>, if the newborn individual <math>a'^{(g+1)}</math> was selected for new population <math>Q^{(g+1)}</math>.\n*2. <math> r^{(g)} = 1 - \\frac{rank(a'^{(g+1)})}{\\mu} \\mbox{ if } a'^{(g+1)} \\in Q^{(g+1)} </math>, where <math>rank(a'^{(g+1)})</math> is the rank of newly inserted individual in the population of <math>\\mu</math> individuals. Rank can be computed using a well-known [[non-dominated sorting]] procedure.<ref>{{cite journal|last1=Deb|first1=K.|last2=Pratap|first2=A.|last3=Agarwal|first3=S.|last4=Meyarivan|first4=T.|title=A fast and elitist multi-objective genetic algorithm: NSGA-II|journal=IEEE Transactions on Evolutionary Computation|volume=6|issue=2|pages=182–197|year=2002|doi=10.1109/4235.996017|citeseerx=10.1.1.17.7771}}</ref>\n*3. <math> r^{(g)} = \\sum_{a\\in Q^{(g+1)}} \\Delta{H}(a,Q^{(g+1)}) - \\sum_{a\\in Q^{(g)}} \\Delta{H}(a,Q^{(g)})</math>, where <math>\\Delta{H}(a,Q^{(g)})</math> is the [[hypervolume indicator]] contribution of the individual <math>a</math> to the population <math>Q^{(g)}</math>. The reward <math>r^{(g)}>0</math> if the newly inserted individual improves the quality of the population, which is measured as its hypervolume contribution in the objective space.\n*4. A relaxation of the above reward, involving a rank-based penalization for points for <math>k</math>-th dominated Pareto front: <math> r^{(g)} =  \\frac{1}{2^{k-1}} \\left( \\sum_{ndom_k(Q^{(g+1)})} \\Delta{H}(a,ndom_k(Q^{(g+1)})) - \\sum_{ndom_k(Q^{(g)})}  \\Delta{H}(a,ndom_k(Q^{(g)})) \\right)</math>\n\nReward-based selection can quickly identify the most fruitful directions of search by maximizing the cumulative reward of individuals.\n\n==See also==\n*[[Fitness proportionate selection]]\n*[[Selection (genetic algorithm)]]\n*[[Stochastic universal sampling]]\n*[[Tournament selection]]\n\n== References ==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n{{DEFAULTSORT:Rewardbased Selection}}\n<!--- Categories --->\n[[Category:Evolutionary algorithms]]\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "List of genetic algorithm applications",
      "url": "https://en.wikipedia.org/wiki/List_of_genetic_algorithm_applications",
      "text": "This is a list of '''[[genetic algorithm]] (GA) applications'''.\n\n==Natural Sciences, Mathematics and Computer Science==\n* Bayesian inference links to particle methods in Bayesian statistics and hidden Markov chain models<ref>{{cite web|url=http://www.math.u-bordeaux1.fr/~delmoral/simu-statistics.html|title=Del Moral - Bayesian Statistics|work=u-bordeaux1.fr|access-date=2011-12-29|archive-url=https://web.archive.org/web/20120501080015/http://www.math.u-bordeaux1.fr/~delmoral/simu-statistics.html#|archive-date=2012-05-01|dead-url=yes|df=}}</ref><ref name=\"hal.inria.fr\">[http://hal.inria.fr/docs/00/60/79/65/PDF/RR-7677.pdf a tutorial on genetic particle models]</ref>\n* [[Computational creativity|Artificial creativity]]\n* Chemical kinetics ([https://archive.is/20121223015305/http://www.personal.leeds.ac.uk/~fuensm/project.html gas] and [http://repositories.cdlib.org/postprints/1154 solid] phases)\n* Calculation of [[bound state]]s and [[local-density approximation]]s\n* [[Code-breaking]], using the GA to search large solution spaces of [[cipher]]s for the one correct decryption.<ref>Joachim De Zutter</ref>\n* Computer architecture: using GA to find out weak links in [[approximate computing]] such as [[Combinatorial search#Lookahead|lookahead]].\n* Configuration applications, particularly physics applications of optimal molecule configurations for particular systems like C<sub>60</sub> ([[Fullerene|buckyballs]])\n* Construction of [[facial composite]]s of suspects by [[Witness|eyewitnesses]] in forensic science.<ref>{{cite journal | title=A (r)evolution in Crime-fighting. | author=Craig Aaen Stockdale | date=June 1, 2008 | url=http://www.forensicmag.com/article/revolution-crime-fighting | journal=Forensic Magazine}}</ref>\n* Data Center/Server Farm.<ref>[http://dssg.cs.umb.edu/wiki/index.php/SymbioticSphere SymbioticSphere – Distributed Software Systems Group, University of Massachusetts, Boston<!-- Bot generated title -->] {{webarchive|url=https://web.archive.org/web/20090329225051/http://dssg.cs.umb.edu/wiki/index.php/SymbioticSphere |date=2009-03-29 }}</ref>\n* [[Distributed computer network]] [[topologies]]\n* Electronic circuit design, known as [[evolvable hardware]]\n* [[Feature selection]] for [[Machine learning|Machine Learning]]<ref>{{Cite web|url=https://www.kdnuggets.com/2017/11/rapidminer-evolutionary-algorithms-feature-selection.html|title=Evolutionary Algorithms for Feature Selection|website=www.kdnuggets.com|language=en-US|access-date=2018-02-19}}</ref>\n* Feynman-Kac models <ref>{{cite web|url=http://www.math.u-bordeaux1.fr/~delmoral/simulinks.html|title=Website for Feynman-Kac particle models|work=u-bordeaux1.fr|deadurl=yes|archiveurl=https://web.archive.org/web/20120501080314/http://www.math.u-bordeaux1.fr/~delmoral/simulinks.html|archivedate=2012-05-01|df=}}</ref><ref>{{Cite web |url=http://www.math.u-bordeaux1.fr/~delmoral/seminaire.ps# |title=a review article on genetic particle models |access-date=2011-12-29 |archive-url=https://web.archive.org/web/20120501080538/http://www.math.u-bordeaux1.fr/~delmoral/seminaire.ps# |archive-date=2012-05-01 |dead-url=yes |df= }}</ref><ref>{{cite web|url=http://www.math.u-bordeaux1.fr/~delmoral/gips.html|title=Feynman-Kac Formulae|work=u-bordeaux1.fr|access-date=2011-12-29|archive-url=https://web.archive.org/web/20120501080605/http://www.math.u-bordeaux1.fr/~delmoral/gips.html#|archive-date=2012-05-01|dead-url=yes|df=}}</ref>\n* File allocation for a [[distributed system]]\n* Filtering and signal processing <ref>{{Cite web |url=http://www.math.u-bordeaux1.fr/~delmoral/simu-filtering.html# |title=links to particle filters |access-date=2011-12-29 |archive-url=https://web.archive.org/web/20120501080727/http://www.math.u-bordeaux1.fr/~delmoral/simu-filtering.html# |archive-date=2012-05-01 |dead-url=yes |df= }}</ref><ref>[http://hal.inria.fr/docs/00/40/39/17/PDF/RR-6991.pdf a tutorial on genetic particle models]</ref>\n* Finding hardware bugs.<ref>Hitoshi Iba, Sumitaka Akiba, Tetsuya Higuchi, Taisuke Sato: BUGS: A Bug-Based Search Strategy using Genetic Algorithms. PPSN 1992:</ref><ref>Ibrahim, W. and Amer, H.: An Adaptive Genetic Algorithm for VLSI Test Vector Selection</ref>\n* [[Game theory]] equilibrium resolution\n* [[Genetic Algorithm for Rule Set Production]]\n* [[Genetic algorithm scheduling|Scheduling applications]], including [[Job Shop Scheduling|job-shop scheduling]] and scheduling in [[printed circuit board]] assembly.<ref name=\"PCB\">{{cite journal | last1 = Maimon | first1 = Oded | last2 = Braha | first2 = Dan | year = 1998 | title = A genetic algorithm approach to scheduling PCBs on a single machine | url = http://necsi.edu/affiliates/braha/IJPR_GA.pdf | journal = International Journal of Production Research | volume = 36 | issue = 3| page = 3 | doi = 10.1080/002075498193688 | citeseerx = 10.1.1.129.9504 }}</ref> The objective being to schedule jobs in a [[sequence-dependent setup|sequence-dependent]] or non-sequence-dependent setup environment in order to maximize the volume of production while minimizing penalties such as tardiness. Satellite communication scheduling for the NASA Deep Space Network was shown to benefit from genetic algorithms.<ref>{{cite journal |title=Deep space network scheduling using evolutionary computational methods |year=2007 |last1= Guillaume |first1=A. |journal=Aerospace Conference, 2007 IEEE |pages=1–6 }}</ref>\n* Learning [[robot]] behavior using genetic algorithms\n* Image processing: Dense pixel matching<ref>A. dos Santos-Paulino, J.-C. Nebel and F.Florez-Revuelta (2014) Evolutionary algorithm for dense pixel matching in presence of distortions, EvoStar Conference, Granada, Spain, 23–25 April 2014</ref>\n* Learning fuzzy rule base using genetic algorithms\n* Molecular structure optimization (chemistry)\n* Optimisation of data compression systems, for example using [[wavelet]]s.\n* [[Power electronics]] design.<ref>{{Cite web |url=http://www.cs.sysu.edu.cn/~jzhang/papers/SMCC.pdf# |title=Zhang, J., Lo, W.L., and Chung, H., \"Pseudocoevolutionary Genetic Algorithms for Power Electronic Circuits Optimization\", IEEE Trans Systems, Man, and Cybernetics, Part C., Vol.36, No.4, July 2006, pp. 590–598. |access-date=2010-08-09 |archive-url=https://web.archive.org/web/20110707025618/http://www.cs.sysu.edu.cn/~jzhang/papers/SMCC.pdf# |archive-date=2011-07-07 |dead-url=yes |df= }}</ref>\n* [[Software engineering]] {{Citation needed|date=November 2008}}\n* [[Traveling salesman problem]] and its applications<ref name=\"PCB\"/>\n\n==Earth Sciences==\n* [[Climatology]]: Estimation of [[heat flux]] between the atmosphere and sea ice<ref>{{cite journal | title=Genetic Programming for Estimation of Heat Flux between the Atmosphere and Sea Ice in Polar Regions. |author1=Karolina Stanislawska |author2=Krzysztof Krawiec |author3=Timo Vihma | date=July 15, 2015 | url=http://dl.acm.org/citation.cfm?id=2754675}}</ref>\n* [[Climatology]]: Modelling [[Temperature record|global temperature]] changes<ref>{{cite journal | title=Modelling global temperature changes with genetic programming. | journal=Computers and Mathematics with Applications |author1=Karolina Stanislawska |author2=Krzysztof Krawiec |author3=Zbigniew W. Kundzewicz | date=April 2012 | url=http://dl.acm.org/citation.cfm?id=2401077}}</ref>\n* Design of [[water resource]] systems <ref name = \"Zhang & Babovic 2012\">{{cite journal |last=Zhang |first=S.X. |last2=Babovic |first2=V. |year=2012 |title=A real options approach to the design and architecture of water supply systems using innovative water technologies under uncertainty |journal=Journal of Hydroinformatics  |volume=14 |issue=1 |pages=13–29 |doi= 10.2166/hydro.2011.078|ssrn= |url=https://www.researchgate.net/publication/249643295}}</ref>\n* Groundwater monitoring networks<ref>[https://purl.fdlp.gov/GPO/gpo41529 Optimization of Water-level Monitoring Networks in the Eastern Snake River Plain Aquifer Using a Kriging-based Genetic Algorithm Method] [[United States Geological Survey]]</ref>\n\n==Finance, Economics and Social Sciences==\n* Financial mathematics<ref name=\"hal.inria.fr\"/><ref>{{cite web|url=http://www.math.u-bordeaux1.fr/~delmoral/simu-finance.html|title=Del Moral - Financial Mathematics|work=u-bordeaux1.fr|access-date=2011-12-29|archive-url=https://archive.is/20121211142015/http://www.math.u-bordeaux1.fr/~delmoral/simu-finance.html#|archive-date=2012-12-11|dead-url=yes|df=}}</ref>\n* [[Genetic algorithm in economics|Economics]]\n*Design of [[anti-terrorism]] systems <ref name = \"Buurman, Zhang & Babovic\">{{cite journal |last=Buurman |first=J. |last2=Zhang |first2=S.X. |last3=Babovic |first3=V. |year=2009 |title=Reducing risk through real options in systems design: the case of architecting a maritime domain protection system |journal=Risk Analysis  |volume=29 |issue=3 |pages=366–379 |doi= 10.1111/j.1539-6924.2008.01160.x|pmid=19076327 |ssrn= |url=https://www.researchgate.net/publication/23657202}}</ref>\n* Linguistic analysis, including [[grammar induction]] and other aspects of [[Natural language processing]] (NLP) such as word sense disambiguation.\n* Automated design of sophisticated trading systems in the financial sector\n* Representing rational agents in economic models such as the [[cobweb model]]\n* [[Real options valuation]] <ref name = \"Zhang & Babovic\">{{cite journal |last=Zhang |first=S.X. |last2=Babovic |first2=V. |year=2011 |title=An evolutionary real options framework for the design and management of projects and systems with complex real options and exercising conditions |journal=Decision Support Systems |volume=51 |issue=1 |pages=119–129 |doi= 10.1016/j.dss.2010.12.001|ssrn= |url=https://www.researchgate.net/publication/220197192}}</ref>\n\n==Industry, Management and Engineering==\n* [[Audio watermark]] insertion/detection\n* Airlines revenue management<ref>Aloysius George, B. R. Rajakumar, D. Binu, (2012) [http://dl.acm.org/citation.cfm?id=2345426 \"Genetic algorithm based airlines booking terminal open/close decision system\"]</ref>\n* Automated design of [[mechatronics|mechatronic]] systems using [[bond graphs]] and [[genetic programming]] (NSF)\n* Automated design = [[computer-automated design]]\n* Automated design of industrial equipment using catalogs of exemplar lever patterns\n* [[Automated]] design, including research on [[composite material]] design and [[multi-objective]] design of automotive components for [[crashworthiness]], weight savings, and other characteristics\n* Automated planning of structural inspection<ref name=\"EllefsenLepikson2017\">{{cite journal|last1=Ellefsen|first1=K.O.|last2=Lepikson|first2=H.A.|last3=Albiez|first3=J.C.|title=Multiobjective coverage path planning: Enabling automated inspection of complex, real-world structures|journal=Applied Soft Computing|volume=61|year=2017|pages=264–282|issn=1568-4946|doi=10.1016/j.asoc.2017.07.051|url=https://www.researchgate.net/publication/318893583}}</ref>\n* Container loading optimization\n* [[Control engineering]],<ref>{{cite web|url=http://citeseerx.ist.psu.edu/showciting;jsessionid=B4A9784CCCB282ECE0FD1622F12FB9FD?cid=2669976|title=CiteSeerX — Citation Query Switching Control Systems and Their Design Automation via Genetic Algorithms|work=psu.edu}}</ref><ref>{{cite journal | last1 = Li | first1 = Y. | year = 1996 | title = Genetic algorithm automated approach to design of sliding mode control systems | journal = Int J Control | volume = 63 | issue = 4 | pages = 721–739 | citeseerx = 10.1.1.43.1654 | doi=10.1080/00207179608921865|display-authors=etal}}</ref><ref>{{cite document|title=Loughborough University Institutional Repository|work=handle.net|hdl = 2134/5806}}</ref><ref name=Patrascu2015>{{cite journal|last=Patrascu|first=M.|year=2015|title=Genetically enhanced modal controller design for seismic vibration in nonlinear multi-damper configuration|journal=Proceedings of the Institution of Mechanical Engineers, Part I|volume=229|issue=2|pages=158–168|url=http://pii.sagepub.com/content/229/2/158|doi=10.1177/0959651814550540}}</ref>\n* [[Marketing mix]] analysis\n* [[Mechanical engineering]]<ref name=\"iit kanpur\" >{{cite web\n  |title=Genetic Algorithms for Engineering Optimization\n  |url=http://www.iitk.ac.in/kangal/course/gaann06.pdf\n}}</ref><ref>{{cite web\n  |title=Applications of evolutionary algorithms in mechanical engineering.\n  |url=http://digitool.fcla.edu/dtl_publish/34/12514.html\n}}</ref>\n* Mobile communications infrastructure [[Optimization (mathematics)|optimization]].\n* [[Plant floor layout]]\n* [[Pop music]] record production<ref>{{cite news| url=http://news.bbc.co.uk/2/hi/entertainment/123983.stm | work=BBC News | title=To the beat of the byte | date=1998-07-01 | accessdate=2010-05-03}}</ref>\n* [[Quality control and genetic algorithms|Quality control]]\n*[[Sorting network]]\n* Timetabling problems, such as designing a non-conflicting class timetable for a large university\n* [[Vehicle routing problem]]  <ref>{{Cite journal|vauthors=Vidal T, Crainic TG, Gendreau M, Lahrichi N, Rei W|title=A hybrid genetic algorithm for multidepot and periodic vehicle routing problems|journal=Operations Research|volume=60|issue=3|pages=611–624|doi=10.1287/opre.1120.1048|year=2012}}</ref>\n* Optimal bearing placement <ref>{{Cite journal|last=Liu|first=Shibing|last2=Yang|first2=Bingen|title=Optimal placement of water-lubricated rubber bearings for vibration reduction of flexible multistage rotor systems|journal=Journal of Sound and Vibration|volume=407|pages=332–349|doi=10.1016/j.jsv.2017.07.004|year=2017}}</ref>\n* [[Computer-automated design]] <ref>{{cite journal | last1 = Li | first1 = Y. | display-authors = etal   | year = 2004 | title = CAutoCSD – Evolutionary search and optimisation enabled computer automated control system design | url = http://eprints.gla.ac.uk/3818/ | journal = International Journal of Automation and Computing | volume = 1 | issue = 1| pages = 76–88 | doi=10.1007/s11633-004-0076-8}}</ref>\n\n==Biological Sciences and Bioinformatics==\n* [[Bioinformatics]] [[Multiple Sequence Alignment]]<ref name=\"Gondro\">{{cite journal|vauthors=Gondro C, Kinghorn BP | title = A simple genetic algorithm for multiple sequence alignment | journal = Genetics and Molecular Research | year = 2007 | volume = 6 | pages = 964–982 |issue= 4|pmid= 18058716 }}</ref><ref name=\"Notredame\">{{cite journal|vauthors=Notredame C, Higgins DG | title = SAGA a Genetic Algorithm for Multiple Sequence Alignment | journal = Nucleic Acids Research | year = 1995 | volume = 24 | pages = 1515–24 | pmid = 8628686|issue= 8|pmc= 145823 | doi=10.1093/nar/24.8.1515}}</ref><ref>{{cite web|url=http://www.tcoffee.org/homepage.html|title=Notredame Lab Home Page - Comparative Bioinformatics|work=tcoffee.org}}</ref>\n* [[Bioinformatics]]: [[RNA]] structure prediction<ref name=\"Batenburg\">{{cite journal|vauthors=van Batenburg FH, Gultyaev AP, Pleij CW | title = An APL-programmed genetic algorithm for the prediction of RNA secondary structure | journal = Journal of Theoretical Biology | year = 1995 | volume = 174 | pages = 269–280 | pmid = 7545258 | doi = 10.1006/jtbi.1995.0098|issue= 3 }}</ref>\n* [[Bioinformatics]]: [[Motif Discovery]]<ref>{{cite journal|title=Generalizing and learning protein-DNA binding sequence representations by an evolutionary algorithm | doi=10.1007/s00500-011-0692-5 | volume=15|issue=8 |journal=Soft Computing|pages=1631–1642|year=2011 |last1=Wong |first1=Ka-Chun |last2=Peng |first2=Chengbin |last3=Wong |first3=Man-Hon |last4=Leung |first4=Kwong-Sak }}</ref>\n* Biology and computational chemistry<ref>{{cite web|url=http://www.math.u-bordeaux1.fr/~delmoral/simu-biology.html|title=Del Moral - Biology & Chemistry|work=u-bordeaux1.fr|access-date=2011-12-29|archive-url=https://web.archive.org/web/20120501080114/http://www.math.u-bordeaux1.fr/~delmoral/simu-biology.html#|archive-date=2012-05-01|dead-url=yes|df=}}</ref><ref>{{Cite web |url=http://www.math.u-bordeaux1.fr/~delmoral/ihp.ps# |title=an article on genetic particle models |access-date=2011-12-29 |archive-url=https://web.archive.org/web/20120501080256/http://www.math.u-bordeaux1.fr/~delmoral/ihp.ps# |archive-date=2012-05-01 |dead-url=yes |df= }}</ref>\n* Building [[phylogenetic tree]]s.<ref name=\"Hill\">{{cite journal|vauthors=Hill T, Lundgren A, Fredriksson R, Schiöth HB | title = Genetic algorithm for large-scale maximum parsimony phylogenetic analysis of proteins | journal = Biochimica et Biophysica Acta | year = 2005 | volume = 1725 | pages = 19–29 | pmid = 15990235|issue= 1|doi= 10.1016/j.bbagen.2005.04.027 }}</ref>\n* [[Expression profiling|Gene expression profiling]] analysis.<ref name=\"To\">{{cite journal|vauthors=To CC, Vohradsky J | title = A parallel genetic algorithm for single class pattern classification and its application for gene expression profiling in Streptomyces coelicolor | journal = BMC Genomics | year = 2007 | volume = 8 | pages = 49 | pmid = 17298664  | doi = 10.1186/1471-2164-8-49|pmc= 1804277}}</ref>\n* [[Medicine]]: [[Clinical decision support]] in ophthalmology<ref>{{cite journal | title=Genetic Programming with Alternative Search Drivers for Detection of Retinal Blood Vessels |author1=Krzysztof Krawiec |author2=Mikołaj Pawlak | date=April 10, 2015 | url=https://www.researchgate.net/publication/272017132}}</ref> and oncology<ref>{{cite journal | title=An Integrated Approach to Stage 1 Breast Cancer Detection. | author=Fitzgerald, Jeannie, Ryan, Conor, Medernach, David and Krawiec, Krzysztof | date = July 15, 2015 | url=http://dl.acm.org/citation.cfm?id=2754761}}</ref>\n* [[Computational Neuroscience]]: finding values for the maximal conductances of ion channels in biophysically detailed neuron models<ref>{{cite journal |last1=Van Geit |first1=Werner |last2=Gevaert |first2=Michael |last3=Chindemi |first3=Giuseppe |last4=Rössert |first4=Christian |last5=Courcol |first5=Jean-Denis |last6=Muller |first6=Eilif B. |last7=Schürmann |first7=Felix |last8=Segev |first8=Idan |last9=Markram |first9=Henry |title=BluePyOpt: Leveraging Open Source Software and Cloud Infrastructure to Optimise Model Parameters in Neuroscience |journal=Frontiers in Neuroinformatics |date=7 June 2016 |volume=10 |pages=17 |doi=10.3389/fninf.2016.00017|pmid=27375471 |pmc=4896051 }}</ref>\n* [[Protein folding]] and protein/[[ligand docking]]<ref name=\"Willet\">{{cite journal|author= Willett P | title = Genetic algorithms in molecular recognition and design | journal = Trends in Biotechnology | year = 1995 | volume = 13 | pages = 516–521 | pmid = 8595137  | doi = 10.1016/S0167-7799(00)89015-0|issue= 12}}</ref><ref>{{cite web|url=http://portal.acm.org/citation.cfm?id=1830483.1830513|title=Protein structure prediction on a lattice model via multimodal optimization techniques|work=acm.org}}</ref>\n* Selection of optimal mathematical model to describe biological systems\n* [[Operon]] prediction.<ref name=\"Wang\">{{cite journal|vauthors=Wang S, Wang Y, Du W, Sun F, Wang X, Zhou C, Liang Y | title = A multi-approaches-guided genetic algorithm with application to operon prediction | journal = Artificial Intelligence in Medicine | year = 2007 | volume = 41 | pages = 151–159 | pmid = 17869072  | doi = 10.1016/j.artmed.2007.07.010|issue= 2}}</ref>\n\n==General Applications==\n* [[Neural network|Neural Network]]s; particularly [[recurrent neural networks]]<ref>{{cite web|url=http://arimaa.com/arimaa/about/Thesis/|title=Applying Genetic Algorithms to Recurrent Neural Networks for Learning Network Parameters and Architecture|work=arimaa.com}}</ref>\n* Training [[artificial neural networks]] when pre-classified training examples are not readily obtainable ([[neuroevolution]])\n\n==Other Applications==\n* Clustering, using genetic algorithms to optimize a wide range of different fit-functions.{{dead link|date=December 2014}}<ref>Auffarth, B. (2010). Clustering by a Genetic Algorithm with Biased Mutation Operator. WCCI CEC. IEEE, July 18–23, 2010. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.170.869{{Dead link|date=November 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n* [[Multidimensional systems]]\n* Multimodal Optimization <ref>{{cite book|title=Effect of Spatial Locality on an Evolutionary Algorithm for Multimodal Optimization | doi=10.1007/978-3-642-12239-2_50 | journal=Lecture Notes in Computer Science|volume=6024 |pages=481–490|year=2010 |last1=Wong |first1=Ka-Chun |last2=Leung |first2=Kwong-Sak |last3=Wong |first3=Man-Hon |isbn=978-3-642-12238-5 |citeseerx = 10.1.1.655.5490}}</ref><ref>{{cite web|url=http://portal.acm.org/citation.cfm?id=1570027|title=An evolutionary algorithm with species-specific explosion for multimodal optimization|work=acm.org}}</ref><ref>{{cite journal|title=Evolutionary multimodal optimization using the principle of locality | doi=10.1016/j.ins.2011.12.016 | volume=194|journal=Information Sciences|pages=138–170|year=2012 |last1=Wong |first1=Ka-Chun |last2=Wu |first2=Chun-Ho |last3=Mok |first3=Ricky K.P. |last4=Peng |first4=Chengbin |last5=Zhang |first5=Zhaolei }}</ref>\n* Multiple criteria production scheduling<ref name=\"Bagchi\">{{cite book|author= Bagchi Tapan P | title = Multiobjective Scheduling by Genetic Algorithms  | year = 1999 | publisher = [[Kluwer Academic]] | isbn = 978-0-7923-8561-5 }}</ref>\n* Multiple population [[topologies]] and interchange [[methodologies]]\n* [[Mutation testing]]\n* [[Parallelization]] of GAs/GPs including use of [[hierarchical decomposition]] of [[problem domains]] and design spaces [[Nesting problem (geometric optimization)|nesting of irregular shapes]] using [[feature matching]] and GAs.\n* Rare event analysis <ref>{{cite web|url=http://www.math.u-bordeaux1.fr/~delmoral/simu-rare-events.html|title=Del Moral - Rare events|work=u-bordeaux1.fr|access-date=2011-12-29|archive-url=https://web.archive.org/web/20120423152151/http://www.math.u-bordeaux1.fr/~delmoral/simu-rare-events.html#|archive-date=2012-04-23|dead-url=yes|df=}}</ref><ref>[http://www-math.unice.fr/publis/delmoral_lezaud.ps a review article]</ref>\n* Solving the machine-component grouping problem required for [[cellular manufacturing]] systems\n* Stochastic optimization <ref>{{cite web|url=http://www.math.u-bordeaux1.fr/~delmoral/simu-optim.html|title=Del Moral - Optimal Control|work=u-bordeaux1.fr|access-date=2011-12-29|archive-url=https://web.archive.org/web/20120508011256/http://www.math.u-bordeaux1.fr/~delmoral/simu-optim.html#|archive-date=2012-05-08|dead-url=yes|df=}}</ref>\n* [[Tactical asset]] allocation and [[international equity]] strategies\n* Wireless sensor/ad-hoc networks.<ref>[http://dssg.cs.umb.edu/wiki/index.php/BiSNET/e BiSNET/e – Distributed Software Systems Group, University of Massachusetts, Boston<!-- Bot generated title -->] {{webarchive|url=https://web.archive.org/web/20090622110049/http://dssg.cs.umb.edu/wiki/index.php/BiSNET/e |date=2009-06-22 }}</ref>\n\n==References==\n{{Reflist|30em}}\n\n{{DEFAULTSORT:Genetic algorithm applications}}\n[[Category:Mathematics-related lists]]\n[[Category:Genetic algorithms|Applications]]"
    },
    {
      "title": "Chromosome (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Chromosome_%28genetic_algorithm%29",
      "text": "{{Evolutionary algorithms}}\n{{For|information about chromosomes in biology|chromosome}}\nIn [[genetic algorithm]]s, a '''chromosome''' (also sometimes called a '''genotype''') is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the ''population''.<ref name=ga-description>{{cite web|title=Introduction to genetic algorithms: IV. Genetic Algorithm|url=http://www.obitko.com/tutorials/genetic-algorithms/ga-basic-description.php|accessdate=12 August 2015}}</ref> The chromosome is often represented as a binary [[string (computer science)|string]], although a wide variety of other [[data structure]]s are also used.\n\n==Chromosome design==\nThe design of the chromosome and its parameters is by necessity specific to the problem to be solved. Traditionally, chromosomes are represented in binary as strings of 0s and 1s, however other encodings are also possible;<ref name=ga-tutorial>{{cite journal|last1=Whitley|first1=Darrell|title=A genetic algorithm tutorial|journal=Statistics and Computing|date=June 1994|volume=4|issue=2|doi=10.1007/BF00175354|citeseerx=10.1.1.184.3999}}<!--|accessdate=12 August 2015--></ref> almost any representation which allows the solution to be represented as a finite-length string can be used.<ref name=what-are-gas>{{cite web|title=What are Genetic Algorithms?|url=http://www.optiwater.com/optiga/ga.html|accessdate=12 August 2015}}</ref> Finding a suitable representation of the problem domain for a chromosome is an important consideration, as a good representation will make the search easier by limiting the search space; similarly, a poorer representation will allow a larger search space.<ref name=ga-notes>{{cite web|title=Genetic algorithms|url=http://www.cse.unsw.edu.au/~billw/cs9414/notes/ml/05ga/05ga.html|accessdate=12 August 2015}}</ref> The [[mutation (genetic algorithm)|mutation]] [[genetic operator|operator]] and [[crossover (genetic algorithm)|crossover]] operator employed by the genetic algorithm must also take into account the chromosome's design.\n\n===Example 1: binary representation===\nSuppose the problem is to find the integer value of <math>x</math> between 0 and 255 that provides the maximal result for <math>f(x) = x^2</math>. The possible solutions for this problem are the integers from 0 to 255, which can all be represented as 8-digit binary strings. Thus, we might use an 8-digit binary string as our chromosome. If a given chromosome in the population represents the value 155, its chromosome would be <code>10011011</code>.\n\nNote that this is not the type of problem that is normally solved by a genetic algorithm, since it can be trivially solved using numeric methods; it is only used to serve as a simple example.\n\n===Example 2: string representation===\nA more realistic problem we might wish to solve is the [[travelling salesman problem]]. In this problem, we seek an ordered list of cities that results in the shortest trip for the salesman to travel. Suppose there are six cities, which we'll call A, B, C, D, E, and F. A good design for our chromosome might be the ordered list we want to try. An example chromosome we might encounter in the population might be <code>DFABEC</code>.\n\n==Selection, crossover and mutation==\n{{see also|Selection (genetic algorithm)|Crossover (genetic algorithm)|Mutation (genetic algorithm)}}\nIn each generation of the genetic algorithm, two parent chromosomes are selected based on their fitness values; these chromosomes are used by the mutation and crossover operators to produce two offspring chromosomes for the new population.<ref name=what-are-gas />\n\n==References==\n{{reflist}}\n\n\n{{DEFAULTSORT:Chromosome (Genetic Algorithm)}}\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Clonal selection algorithm",
      "url": "https://en.wikipedia.org/wiki/Clonal_selection_algorithm",
      "text": "{{Evolutionary algorithms}}\nIn [[artificial immune systems]], '''clonal selection algorithms''' are a class of algorithms inspired by the [[clonal selection]] theory of [[acquired immunity]] that explains how B and T [[lymphocyte]]s improve their response to [[antigens]] over time called [[affinity maturation]]. These algorithms focus on the [[Universal Darwinism|Darwinian]] attributes of the theory where selection is inspired by the affinity of [[antigen-antibody interaction|antigen-antibody interactions]], reproduction is inspired by [[cell division]], and variation is inspired by [[somatic hypermutation]]. Clonal selection algorithms are most commonly applied to [[Optimization (mathematics)|optimization]] and [[pattern recognition]] domains, some of which resemble parallel [[hill climbing]] and the [[genetic algorithm]] without the recombination operator<ref>{{Cite web|url=http://www.cleveralgorithms.com/nature-inspired/immune/clonal_selection_algorithm.html|title=Clonal Selection Algorithm|last=Brownlee|first=Jason|date=|website=Clonal Selection Algorithm|archive-url=|archive-date=|dead-url=|access-date=}}</ref>.\n\n== Techniques ==\n*'''CLONALG''': The '''CLON'''al selection '''ALG'''orithm<ref>\n{{cite journal\n | last = de Castro\n | first = L. N.\n |author2=Von Zuben, F. J.\n | title = Learning and Optimization Using the Clonal Selection Principle\n | journal = IEEE Transactions on Evolutionary Computation \n | volume = 6\n | issue = 3\n | year = 2002\n | pages = 239–251\n | url = ftp://ftp.dca.fee.unicamp.br/pub/docs/vonzuben/lnunes/ieee_tec01.pdf\n | doi=10.1109/tevc.2002.1011539}}\n</ref>\n*'''AIRS''': The Artificial Immune Recognition System<ref>\n{{cite journal\n | last = Watkins\n | first = Andrew |author2=Timmis, Jon |author3=Boggess, Lois\n | title = Artificial Immune Recognition System (AIRS): An Immune-Inspired Supervised Learning Algorithm\n | journal = Genetic Programming and Evolvable Machines\n | volume = 5\n | issue = 3\n | year = 2004\n | pages = 291–317\n | url = http://www.cse.msstate.edu/%7Eandrew/research/publications/airs.pdf\n | doi = 10.1023/B:GENP.0000030197.83685.94 | citeseerx = 10.1.1.58.1410 }}\n</ref>\n*'''BCA''': The B-Cell Algorithm<ref>\n{{cite conference\n  | last = Kelsey\n  | first = Johnny\n  |author2=Timmis, Jon\n  | title = Immune Inspired Somatic Contiguous Hypermutation for Function Optimisation \n  | booktitle = Genetic and Evolutionary Computation (GECCO 2003)\n  | year = 2003\n  | pages = 202\n  | url = http://www.springerlink.com/content/jmdkyhnrthmec78v/ }}\n</ref>\n\n== See also ==\n*[[Artificial immune system]]\n*[[Biologically inspired computing]]\n*[[Computational immunology]]\n*[[Computational intelligence]]\n*[[Evolutionary computation]]\n*[[Immunocomputing]]\n*[[Natural computation]]\n*[[Swarm intelligence]]\n\n== Notes ==\n{{reflist}}\n\n== External links ==\n*[http://www.artificial-immune-systems.org/algorithms.shtml#clonal-alg Clonal Selection Pseudo code] on AISWeb\n*[https://web.archive.org/web/20090711163953/http://www.dca.fee.unicamp.br/~lnunes/manual.html CLONALG in Matlab] developed by Leandro de Castro and Fernando Von Zuben\n*[http://optalgtoolkit.sourceforge.net Optimization Algorithm Toolkit] in Java developed by Jason Brownlee which includes the following clonal selection algorithms: Adaptive Clonal Selection (ACS), Optimization Immune Algorithm (opt-IMMALG), Optimization Immune Algorithm (opt-IA), Clonal Selection Algorithm (CLONALG, CLONALG1, CLONALG2), B-Cell Algorithm (BCA), Cloning, Information Gain, Aging (CLIGA), Immunological Algorithm (IA)\n*[http://www.artificial-immune-systems.org/code/airs/airs_andrew.tar.gz AIRS in C++]{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }} developed by Andrew Watkins\n*[http://www.artificial-immune-systems.org/code/bca/BCA.tar.gz BCA in C++]{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }} developed by Johnny Kelsey\n\n[[Category:Genetic algorithms]]\n[[Category:Artificial immune systems]]"
    },
    {
      "title": "Crossover (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Crossover_%28genetic_algorithm%29",
      "text": "{{Evolutionary algorithms}}\nIn [[genetic algorithm|genetic algorithms]] and [[evolutionary computation]], '''crossover''', also called recombination, is a [[genetic operator]] used to combine the [[chromosome (genetic algorithm)|genetic information]] of two parents to generate new offspring. It is one way to [[stochastic|stochastically]] generate new [[candidate solution|solutions]] from an existing population, and analogous to the [[chromosomal crossover|crossover]] that happens during [[sexual reproduction]] in [[biology]]. Solutions can also be generated by [[cloning]] an existing solution, which is analogous to [[asexual reproduction]]. Newly generated solutions are typically [[mutation (genetic algorithm)|mutated]] before being added to the population.\n\nDifferent algorithms in evolutionary computation may use different data structures to store genetic information, and each [[genetic representation]] can be recombined with different crossover operators. Typical [[data structure|data structures]] that can be recombined with crossover are [[bit array|bit arrays]], vectors of real numbers, or [[tree (data structure)|trees]].\n\n==Examples==\nTraditional genetic algorithms store genetic information in a chromosome represented by a [[bit array]]. Crossover methods for bit arrays are popular and an illustrative example of genetic recombination.\n\n===Single-point crossover===\nA point on both parents' chromosomes is picked randomly, and designated a 'crossover point'. Bits to the right of that point are swapped between the two parent chromosomes. This results in two offspring, each carrying some genetic information from both parents.\n\n[[File:OnePointCrossover.svg]]\n\n===Two-point and k-point crossover===\nIn two-point crossover, two crossover points are picked randomly from the parent chromosomes. The bits in between the two points are swapped between the parent organisms. \n\n[[File:TwoPointCrossover.svg|TwoPointCrossover.svg]]\n\nTwo-point crossover is equivalent to performing two single-point crossovers with different crossover points. This strategy can be generalized to k-point crossover for any positive integer k, picking k crossover points.\n\n=== Uniform crossover===\nIn uniform crossover, typically, each bit is chosen from either parent with equal probability. Other mixing ratios are sometimes used, resulting in offspring which inherit more genetic information from one parent than the other.\n\n=== Crossover for ordered lists ===\nIn some genetic algorithms, not all possible chromosomes represent valid solutions. In some cases, it is possible to use specialized crossover and mutation operators that are designed to avoid violating the constraints of the problem.\n\nFor example, a genetic algorithm solving the [[travelling salesman problem]] may use an ordered list of cities to represent a solution path. Such a chromosome only represents a valid solution if the list contains all the cities that the salesman must visit. Using the above crossovers will often result in chromosomes that violate that constraint. Genetic algorithms optimizing the ordering of a given list thus require different crossover operators that will avoid generating invalid solutions. Many such crossovers have been published:<ref>Pedro Larrañaga et al., \"Learning Bayesian Network Structures by searching for the best ordering with genetic algorithms\", IEEE Transactions on systems, man and cybernetics, Vol 26, No. 4, 1996</ref>\n# partially matched crossover (PMX)\n# cycle crossover (CX)\n# order crossover operator (OX1)\n# order-based crossover operator (OX2)\n# position-based crossover operator (POS)\n# voting recombination crossover operator (VR)\n# alternating-position crossover operator (AP)\n# sequential constructive crossover operator (SCX)<ref>Ahmed, Zakir H. \"Genetic Algorithm for the Traveling Salesman Problem Using Sequential Constructive Crossover Operator.\" International Journal of Biometric and Bioinformatics 3.6 (2010). Computer Science Journals. Web. <https://pdfs.semanticscholar.org/a1e6/50daed4ed9c6a403b08e5d50b3ea9f3b5de4.pdf>.</ref>\nOther possible methods include the [[edge recombination operator]].\n\n== See also ==\n\n* [[Evolutionary computation]]\n* [[Genetic algorithm]]\n* [[Chromosome (genetic algorithm)]]\n* [[Mutation (genetic algorithm)]]\n* [[Fitness approximation]]\n* [[Fitness function]]\n* [[Selection (genetic algorithm)]]\n\n== References ==\n* John Holland, ''Adaptation in Natural and Artificial Systems'', [[University of Michigan Press]], Ann Arbor, Michigan. 1975. {{ISBN|0-262-58111-6}}.\n* Larry J. Eshelman, ''The CHC Adaptive Search Algorithm: How to Have Safe Search When Engaging in Nontraditional Genetic Recombination'', in Gregory J. E. Rawlins editor, Proceedings of the First Workshop on Foundations of Genetic Algorithms. pages 265-283. Morgan Kaufmann, 1991. {{ISBN|1-55860-170-8}}.\n* Tomasz D. Gwiazda, ''Genetic Algorithms Reference Vol.1 Crossover for single-objective numerical optimization problems'', Tomasz Gwiazda, Lomianki, 2006. {{ISBN|83-923958-3-2}}.\n{{Reflist}}\n\n== External links ==\n* [http://www.faqs.org/faqs/ai-faq/genetic/part2/ Newsgroup: comp.ai.genetic FAQ] - see section on crossover (also known as recombination).\n\n{{DEFAULTSORT:Crossover (Genetic Algorithm)}}\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Defining length",
      "url": "https://en.wikipedia.org/wiki/Defining_length",
      "text": "{{Technical|date=August 2011}}\nIn [[genetic algorithms]] and [[genetic programming]] '''defining length''' L(H) is the maximum distance between two defining symbols (that is symbols that have a fixed value as opposed to symbols that can take any value, commonly denoted as # or *) in [[schema (genetic algorithms)|schema]] H. In tree GP schemata, L(H) is the number of links in the minimum tree fragment including all the non-= symbols within a schema H.<ref name=\"UCL1\">{{cite web|title=Foundations of Genetic Programming|url=http://www.cs.ucl.ac.uk/staff/W.Langdon/FOGP/|publisher=UCL UK|accessdate=13 July 2010}}</ref>\n\n== Example ==\n\nSchemata \"00##0\", \"1###1\", \"01###\", and \"##0##\" have defining lengths of 4, 4, 1, and 0, respectively. Lengths are computed by determining the last fixed position and subtracting from it the first fixed position.\n\nIn [[genetic algorithms]] as the defining length of a solution increases so does the susceptibility of the solution to disruption due to [[mutation]] or [[Crossover (genetic algorithm)|cross-over]].\n\n== References ==\n{{reflist}}\n\n{{comp-sci-stub}}\n{{Use dmy dates|date=August 2011}}\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Edge recombination operator",
      "url": "https://en.wikipedia.org/wiki/Edge_recombination_operator",
      "text": "{{Multiple issues|\n{{expert needed|date=June 2011}}\n{{refimprove|date=June 2011}}\n}}\n\nThe '''edge recombination operator''' ('''ERO''') is an operator that creates a [[path (graph theory)|path]] that is similar to a set of existing paths (parents) by looking at the edges rather than the vertices. The main application of this is for [[crossover (genetic algorithm)|crossover]] in [[genetic algorithms]] when a genotype with non-repeating gene sequences is needed such as for the [[travelling salesman problem]]. It was described by [[Darrell Whitley]] and others in 1989.<ref>{{cite conference\n        | first = Darrell\n        | last = Whitley\n        |author2=Timothy Starkweather |author3=D'Ann Fuquay\n        | title = Scheduling problems and traveling salesman: The genetic edge recombination operator\n        | booktitle = International Conference on Genetic Algorithms\n        | pages = 133–140\n        | year = 1989 \n        | isbn = 1-55860-066-3}}</ref>\n\n==Algorithm==\n\nERO is based on an [[adjacency matrix]], which lists the neighbors of each node in any parent.\n\n[[Image:Genetic ero crossover.svg|thumb|right|ERO crossover]]\nFor example, in a travelling salesman problem such as the one depicted, the node map for the parents CABDEF and ABCEFD (see illustration) is generated by taking the first parent, say, 'ABCEFD' and recording its immediate neighbors, including those that roll around the end of the string.\n\nTherefore;\n\n ... -> [A] <-> [B] <-> [C] <-> [E] <-> [F] <-> [D] <- ...\n\n...is converted into the following [[adjacency matrix]] by taking each node in turn, and listing its connected neighbors;\n\n A: B D\n B: A C\n C: B E\n D: F A\n E: C F\n F: E D\n\nWith the same operation performed on the second parent (CABDEF), the following is produced:\n\n A: C B\n B: A D\n C: F A\n D: B E\n E: D F\n F: E C\n\nFollowed by making a [[Union (set theory)|union]] of these two lists, and ignoring any duplicates.  This is as simple as taking the elements of each list and appending them to generate a list of unique link end points. In our example, generating this;\n\n A: B C D         = {B,D} ∪ {C,B}\n B: A C D         = {A,C} ∪ {A,D}\n C: A B E F       = {B,E} ∪ {F,A}\n D: A B E F       = {F,A} ∪ {B,E}\n E: C D F         = {C,F} ∪ {D,F}\n F: C D E         = {E,D} ∪ {E,C}\n\nThe result is another [[adjacency matrix]], which stores the links for a network described by all the links in the parents. Note that more than two parents can be employed here to give more diverse links.  However, this approach may result in sub-optimal paths.\n\nThen, to create a path K, the following algorithm is employed:<ref>Darrell Whitley, Timothy Starkweather and Daniel Shaner: ''The Travelling Salesman and Sequence Scheduling: Quality Solutions using Genetic Edge Recombination'' in L. Davis (ed.): ''Handbook of Genetic Algorithms''. Van Nostrand Reinhold, New York 1991</ref>\n\n Let K be the empty list\n Let N be the first node of a random parent.\n \n While Length(K) < Length(Parent):\n     K := K, N   (append N to K)\n     Remove N from all neighbor lists\n \n     If N's neighbor list is non-empty\n        then let N* be the neighbor of N with the fewest neighbors in its list (or a random one, should there be multiple)\n        else let N* be a randomly chosen node that is not in K\n \n     N := N*\n\nTo step through the example, we randomly select a node from the parent starting points, {A, C}.\n\n* () -> A. We remove A from all the neighbor sets, and find that the smallest of B, C and D is B={C,D}.\n* AB. The smallest sets of C and D are C={E,F} and D={E,F}. We randomly select D.\n* ABD. Smallest are E={C,F}, F={C,E}. We pick F.\n* ABDF. C={E}, E={C}. We pick C.\n* ABDFC. The smallest set is E={}.\n* ABDFCE. The length of the child is now the same as the parent, so we are done.\n\nNote that the only edge introduced in ABDFCE is AE.\n\n== Comparison with other operators ==\nEdge recombination is generally considered a good option for problems like the travelling salesman problem. In a 1999 study at the [[University of the Basque Country]], edge recombination provided better results than all the other crossover operators including [[partially mapped crossover]] and [[cycle crossover]].<ref>P. Larrañaga et al: ''Genetic Algorithms for the Travelling Salesman Problem: A Review of Representations and Operators''. Artificial Intelligence Review, Volume 13, Number 2, April 1999, p.&nbsp;129−170</ref>\n\n==References==\n{{Reflist}}\n\n==Implementations==\n*[https://github.com/raunak/Travelling-Salesman-Problem/blob/master/edge_recombination.py \"Edge Recombination Operator\"] (Python)\n\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Evolver (software)",
      "url": "https://en.wikipedia.org/wiki/Evolver_%28software%29",
      "text": "'''Evolver''' is a [[Software package (installation)|software package]] that allows users to solve a wide variety of [[optimization problem]]s using a [[genetic algorithm]]. Launched in 1989, it was the first commercially available [[genetic algorithm]] package for personal computers. The program was originally developed by [[Axcelis, Inc.]] and is now owned by [[Palisade Corporation]].\n\n== External links ==\n* [http://www.palisade.com/evolver/ Evolver official page]\n* [https://www.nytimes.com/1990/08/29/business/business-technology-what-s-the-best-answer-it-s-survival-of-the-fittest.html?scp=1&sq=axcelis%20evolver&st=cse/ Axcelis, Inc. New York Times Article]\n\n[[Category:Genetic algorithms]]\n\n{{software-eng-stub}}"
    },
    {
      "title": "Fitness function",
      "url": "https://en.wikipedia.org/wiki/Fitness_function",
      "text": "{{no footnotes|date=May 2015}}\nA '''fitness function''' is a particular type of [[objective function]] that is used to summarise, as a single [[figure of merit]], how close a given design solution is to achieving the set aims. Fitness functions are used in [[genetic programming]] and [[genetic algorithm]]s to guide simulations towards optimal design solutions.\n\n==Genetic programming and algorithms==\n\nIn particular, in the fields of [[genetic programming]] and [[genetic algorithm]]s, each design solution is commonly represented as a string of numbers (referred to as a [[chromosome (genetic algorithm)|chromosome]]). After each round of testing, or simulation, the idea is to delete the ''n'' worst design solutions, and to [[crossover (genetic algorithm)|breed]] ''n'' new ones from the best design solutions. Each design solution, therefore, needs to be awarded a figure of merit, to indicate how close it came to meeting the overall specification, and this is generated by applying the fitness function to the test, or simulation, results obtained from that solution.\n\nThe reason that genetic algorithms cannot be considered to be a lazy way of performing design work is precisely because of the effort involved in designing a workable fitness function. Even though it is no longer the human designer, but the computer which comes up with the final design, it is still the human designer who has to design the fitness function. If this is designed badly, the algorithm will either converge on an inappropriate solution, or will have difficulty converging at all.\n\nThe fitness function must not only correlate closely with the designer's goal, it must also be computed quickly. Speed of execution is very important, as a typical genetic algorithm must be iterated many times in order to produce a usable result for a non-trivial problem.\n\n[[Fitness approximation]] may be appropriate, especially in the following cases: \n* Fitness computation time of a single solution is extremely high \n* Precise model for fitness computation is missing\n* The fitness function is uncertain or noisy.\n\nTwo main classes of fitness functions exist: one where the fitness function does not change, as in optimizing a fixed function or testing with a fixed set of test cases; and one where the fitness function is mutable, as in [[niche differentiation]] or [[co-evolution|co-evolving]] the set of test cases.\n\nAnother way of looking at fitness functions is in terms of a [[fitness landscape]], which shows the fitness for each possible chromosome.\n\nDefinition of the fitness function is not straightforward in many cases and often is performed iteratively if the fittest solutions produced by genetic algorithms are not what is desired. [[Interactive genetic algorithms]] address this difficulty by outsourcing evaluation to external agents (normally humans).\n\n== See also ==\n*[[Evolutionary computation]]\n*[[Inferential programming]]\n*[[Test functions for optimization]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n*[http://profsite.um.ac.ir/~davarynej/Resources/CEC'07-Draft.pdf A Nice Introduction to Adaptive Fuzzy Fitness Granulation (AFFG)] ([[PDF]]), A promising approach to accelerate the convergence rate of EAs.\n*[http://www.davarynejad.com/Mohsen/index.php?n=Main.AFFG The cyber shack of Adaptive Fuzzy Fitness Granulation (AFFG)] That is designed to accelerate the convergence rate of EAs.\n*[http://www.nelsonrobotics.org/paper_archive_nelson/nelson-jras-2009.pdf Fitness functions in evolutionary robotics: A survey and analysis (AFFG)] ([[PDF]]), A review of fitness functions used in [[evolutionary robotics]].\n\n\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Fitness proportionate selection",
      "url": "https://en.wikipedia.org/wiki/Fitness_proportionate_selection",
      "text": "[[Image:Fitness proportionate selection example.png|thumb|270px|Example of the selection of a single individual]]\n'''Fitness proportionate selection''', also known as '''roulette wheel selection''', is a [[genetic operator]] used in [[genetic algorithm]]s for selecting potentially useful solutions for recombination.\n\nIn fitness proportionate selection, as in all selection methods, the [[fitness function]] assigns a fitness to possible solutions or [[chromosome]]s.  This fitness level is used to associate a [[probability]] of selection with each individual chromosome. If <math>f_i</math> is the fitness of individual <math>i</math> in the population, its probability of being selected is \n: <math>p_i = \\frac{f_i}{\\Sigma_{j=1}^{N} f_j},</math> \nwhere <math>N</math> is the number of individuals in the population.\n\nThis could be imagined similar to a Roulette wheel in a casino. Usually a proportion of the wheel is assigned to each of the possible selections based on their fitness value. This could be achieved by dividing the fitness of a selection by the total fitness of all the selections, thereby normalizing them to 1. Then a random selection is made similar to how the roulette wheel is rotated.\n\nWhile candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may be eliminated because their probability of selection is less than 1 (or 100%).  Contrast this with a less sophisticated selection algorithm, such as [[truncation selection]], which will eliminate a fixed percentage of the weakest candidates. With fitness proportionate selection there is a chance some weaker solutions may survive the selection process. This is because even though the probability that the weaker solutions will survive is low, it is not zero which means it is still possible they will survive; this is an advantage, because there is a chance that even weak solutions may have some features or characteristics which could prove useful following the recombination process.\n\nThe analogy to a roulette wheel can be envisaged by imagining a roulette wheel in which each candidate solution represents a pocket on the wheel; the size of the pockets are proportionate to the probability of selection of the solution.{{fact|date=January 2017}}  Selecting N chromosomes from the population is equivalent to playing N games on the roulette wheel, as each candidate is drawn independently.\n\nOther selection techniques, such as [[stochastic universal sampling]]<ref>Bäck, Thomas, ''Evolutionary Algorithms in Theory and Practice'' (1996), p. 120, Oxford Univ. Press</ref> or [[tournament selection]], are often used in practice. This is because they have less stochastic noise, or are fast, easy to implement and have a constant selection pressure.<ref>{{Cite journal|last=Blickle|first=Tobias|last2=Thiele|first2=Lothar|date=1996|title=A Comparison of Selection Schemes Used in Evolutionary Algorithms|url=http://www.mitpressjournals.org/doi/10.1162/evco.1996.4.4.361|journal=Evolutionary Computation|language=en|volume=4|issue=4|pages=361–394|doi=10.1162/evco.1996.4.4.361|issn=1063-6560|via=}}</ref>\n\nThe naive implementation is carried out by first generating the [[Cumulative probability distribution function|cumulative probability distribution]] (CDF) over the list of individuals using a probability proportional to the fitness of the individual. A [[uniform distribution (continuous)|uniform random]] number from the range [0,1) is chosen and the inverse of the CDF for that number gives an individual. This corresponds to the roulette ball falling in the bin of an individual with a probability proportional to its width. The \"bin\" corresponding to the inverse of the uniform random number can be found most quickly by using a [[Binary search algorithm|binary search]] over the elements of the CDF. It takes in the [[Big O notation|O(log n)]] time to choose an individual. A faster alternative that generates individuals in O(1) time will be to use the [[alias method]].\n\nRecently, a very simple algorithm was introduced that is based on \"stochastic acceptance\".<ref>A. Lipowski, Roulette-wheel selection via stochastic acceptance (arXiv:1109.3627)[https://arxiv.org/abs/1109.3627]</ref> The algorithm randomly selects an individual (say <math>i</math>) and accepts the selection with probability <math>f_i/f_M</math>, where <math>f_M</math> is the maximum fitness in the population. Certain analysis indicates that the stochastic acceptance version has a considerably better performance than versions based on linear or binary search, especially in applications where fitness values might change during the run.<ref>[https://jbn.github.io/fast_proportional_selection/ Fast Proportional Selection]</ref> While the behavior of this algorithm is typically fast, some fitness distributions (such as exponential distributions) may require <math>O(n)</math> iterations in the worst case. This algorithm also requires more random numbers than binary search.\n\n==Pseudocode==\nFor example, if you have a population with fitnesses [1, 2, 3, 4], then the sum is (1 + 2 + 3 + 4 = 10).  Therefore, you would want the probabilities or chances to be [1/10, 2/10, 3/10, 4/10] or [0.1, 0.2, 0.3, 0.4].  If you were to visually normalize this between 0.0 and 1.0, it would be grouped like below with [red = 1/10, green = 2/10, blue = 3/10, black = 4/10]:\n\n <span style=\"color:red\">\n 0.1 ]</span>\n <span style=\"color:green\">\n 0.2 \\\n 0.3 /</span>\n <span style=\"color:blue\">\n 0.4 \\\n 0.5 |\n 0.6 /</span>\n <span style=\"color:black\">\n 0.7 \\\n 0.8 |\n 0.9 |\n 1.0 /</span>\n\nUsing the above example numbers, this is how to determine the probabilities:\n\n sum_of_fitness = 10\n previous_probability = 0.0\n \n [1] = previous_probability + (fitness / sum_of_fitness) = 0.0 + (1 / 10) = 0.1\n previous_probability = 0.1\n \n [2] = previous_probability + (fitness / sum_of_fitness) = 0.1 + (2 / 10) = 0.3\n previous_probability = 0.3\n \n [3] = previous_probability + (fitness / sum_of_fitness) = 0.3 + (3 / 10) = 0.6\n previous_probability = 0.6\n \n [4] = previous_probability + (fitness / sum_of_fitness) = 0.6 + (4 / 10) = 1.0\n\nThe last index should always be 1.0 or close to it.  Then this is how to randomly select an individual:\n\n random_number # Between 0.0 and 1.0\n \n if random_number < 0.1\n   select <span style=\"color:red\">1</span>\n else if random_number < 0.3 # 0.3 - 0.1 = 0.2 probability\n   select <span style=\"color:green\">2</span>\n else if random_number < 0.6 # 0.6 - 0.3 = 0.3 probability\n   select <span style=\"color:blue\">3</span>\n else if random_number < 1.0 # 1.0 - 0.6 = 0.4 probability\n   select <span style=\"color:black\">4</span>\n en\n\n==See also==\n*[[Reward-based selection]]\n*[[Stochastic universal sampling]]\n*[[Tournament selection]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.cs.ucl.ac.uk/staff/W.Langdon/ftp/gp-code/GProc-1.8b.tar.gz C implementation] (.tar.gz; see selector.cxx) WBL\n*[http://www.edc.ncl.ac.uk/highlight/rhjanuary2007g02.php/ Example on Roulette wheel selection]\n*[http://lipowski.home.amu.edu.pl/homepage/roulette.html An outline of implementation of the O(1) version]\n{{DEFAULTSORT:Fitness Proportionate Selection}}\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "GATTO",
      "url": "https://en.wikipedia.org/wiki/GATTO",
      "text": "#REDIRECT [[Genetic algorithm]]\n\n[[Category:Genetic algorithms]]\n[[Category:Integrated circuits]]"
    },
    {
      "title": "Genetic algorithm scheduling",
      "url": "https://en.wikipedia.org/wiki/Genetic_algorithm_scheduling",
      "text": "The [[genetic algorithm]] is an [[operational research]] method that may be used to solve [[Scheduling (production processes)|scheduling]] problems in [[production planning]].\n\n==Importance of production scheduling==\nTo be competitive, corporations must minimize inefficiencies and maximize productivity. In manufacturing, productivity is inherently linked to how well the firm can optimize the available resources, reduce waste and increase efficiency. Finding the best way to maximize efficiency in a manufacturing process can be extremely complex. Even on simple projects, there are multiple inputs, multiple steps, many constraints and limited resources. In general a resource constrained scheduling problem consists of:\n* A set of jobs that must be executed\n* A [[finite set]] of resources that can be used to complete each job\n* A set of constraints that must be satisfied\n** Temporal Constraints–the time window to complete the task\n** Procedural Constraints–the order each task must be completed\n** Resource Constraints - is the resource available\n* A set of objectives to evaluate the scheduling performance\n\nA typical factory floor setting is a good example of this, where it is necessary to schedule which jobs need to be completed on which machines, by which employees, in what order and at what time. \n\n==Use of algorithms in scheduling==\nIn very complex problems such as scheduling there is no known way to get to a final answer, so we resort to searching for it trying to find a “good” answer. Scheduling problems most often use heuristic algorithms to search for the optimal solution. Heuristic search methods suffer as the inputs become more complex and varied. This type of problem is known in [[computer science]] as an [[NP-hard|NP-Hard]] problem. This means that there are no known algorithms for finding an optimal solution in polynomial time.\n\n[[Image:Precedence.jpg|frame|Fig. 1. Precedence in scheduling]]\n[[Genetic algorithm]]s are well suited to solving [[Scheduling (production processes)|production scheduling]] problems, because unlike heuristic methods genetic algorithms operate on a population of solutions rather than a single solution. In production scheduling this population of solutions consists of many answers that may have different sometimes conflicting objectives. For example, in one solution we may be optimizing a production process to be completed in a minimal amount of time. In another solution we may be optimizing for a minimal amount of defects. By cranking up the speed at which we produce we may run into an increase in defects in our final product.\n\nAs we increase the number of objectives we are trying to achieve we also increase the number of constraints on the problem and similarly increase the complexity. Genetic algorithms are ideal for these types of problems where the search space is large and the number of feasible solutions is small.\n\n==Application of a genetic algorithm==\n[[Image:SchedulingGenome1.jpg|frame|Fig. 2 A. Example Schedule genome]]\n<!-- Image with unknown copyright status removed: [[Image:SchedulingGenome2.jpg|frame|Fig. 2 B. Example Schedule genome]] -->\nTo apply a genetic algorithm to a scheduling problem we must first represent it as a genome. One way to represent a scheduling genome is to define a sequence of tasks and the start times of those tasks relative to one another. Each task and its corresponding start time represents a gene.\n\nA specific sequence of tasks and start times (genes) represents one genome in our population. To make sure that our genome is a [[Candidate solution|feasible solution]] we must take care that it obeys our precedence constraints. We generate an initial population using random start times within the precedence constraints. With genetic algorithms we then take this initial population and cross it, combining genomes along with a small amount of randomness (mutation). The offspring of this combination is selected based on a [[fitness function]] that includes one or many of our constraints, such as minimizing time and minimizing defects. We let this process continue either for a pre-allotted time or until we find a solution that fits our minimum criteria. Overall each successive generation will have a greater average fitness, i.e. taking less time with higher quality than the preceding generations. In scheduling problems, as with other genetic algorithm solutions, we must make sure that we do not select offspring that are infeasible, such as offspring that violate our precedence constraint. We of course may have to add further fitness values such as minimizing costs; however, each constraint that we add greatly increases the search space and lowers the number of solutions that are good matches.\n\n==Bibliography==\n* {{Citation\n | last = Wall | first = M. | title = A Genetic Algorithm for Resource-Constrained Scheduling  | url = http://lancet.mit.edu/mwall/phd/thesis/thesis.pdf }}\n* {{Citation\n | last1 = Lim | first1 = C.\n | last2 = Sim | first2 = E.\n | title = Production Planning in Manufacturing/Remanufacturing Environment using Genetic Algorithm }}\n\n==See also==\n* [[Job Shop Scheduling]]\n* [[Quality control and genetic algorithms]]\n* [[Genetic algorithm in economics]]\n\n==External links==\n*[http://www.dna-evolutions.com/dnaappletsample.html Demo applet of a genetic algorithm solving TSPs and VRPTW problems]\n\n{{DEFAULTSORT:Genetic Algorithm Scheduling}}\n[[Category:Production planning]]\n[[Category:Genetic algorithms]]\n[[Category:Mathematical optimization in business]]"
    },
    {
      "title": "Genetic fuzzy systems",
      "url": "https://en.wikipedia.org/wiki/Genetic_fuzzy_systems",
      "text": "{{Evolutionary algorithms}}\n{{Context|date=October 2009}}\n'''Genetic fuzzy systems''' are [[fuzzy system]]s constructed by using [[genetic algorithms]] or genetic programming, which mimic the process of natural evolution, to identify its structure and parameter.\n\nWhen it comes to automatically identifying and building a fuzzy system, given the high degree of nonlinearity of the output, traditional linear optimization tools have several limitations. Therefore, in the framework of soft computing, genetic algorithms (GAs) and genetic programming (GP) methods have been used successfully to identify structure and parameters of fuzzy systems.\n\n==Fuzzy systems==\nFuzzy systems are fundamental methodologies to represent and process [[Linguistics|linguistic]] information, with mechanisms to deal with uncertainty and imprecision. For instance, the task of modeling a driver parking a car involves greater difficulty in writing down a concise mathematical model as the description becomes more detailed.  However, the level of difficulty is not so much using simple linguistic rules, which are themselves fuzzy.  With such remarkable attributes, fuzzy systems have been widely and successfully applied to control, classification and modeling problems ([[Ebrahim Mamdani|Mamdani]], 1974) (Klir and Yuan, 1995) (Pedrycz and Gomide, 1998).\n\nAlthough simplistic in its design, the identification of a fuzzy system is a rather complex task that comprises the identification\nof (a) the input and output variables, (b) the rule base (knowledge base), (c) the membership functions and (d) the mapping parameters.\n\nUsually the rule base consists of several IF-THEN rules, linking input(s) and output(s).\nA simple rule of a fuzzy controller could be:\n\nIF (TEMPERATURE = HOT) THEN (COOLING = HIGH)\n\nThe numerical impact/meaning of this rule depends on how the membership functions of HOT and HIGH are shaped and defined.\n\nThe construction and identification of a fuzzy system can be divided into (a) the structure and (b) the parameter identification of a fuzzy system.\n\nThe structure of a fuzzy system is expressed by the input and output variables and the rule base, while the parameters of a fuzzy system are the rule parameters (defining the membership functions, the aggregation operator and the implication function) and the mapping parameters related to the mapping of a crisp set to a fuzzy set, and vice versa. (Bastian, 2000).\n\nMuch work has been done to develop or adapt methodologies that are capable of automatically identifying a fuzzy system from numerical data. Particularly in the framework of soft computing, significant methodologies have been proposed with the objective of building fuzzy systems by means of genetic algorithms (GAs) or genetic programming (GP).\n\n==Genetic algorithms for fuzzy system identification==\nGiven the high degree of nonlinearity of the output of a fuzzy system, traditional linear optimization tools do have their limitations. \nGenetic algorithms have demonstrated to be a robust and very powerful tool to perform tasks such as the generation of fuzzy rule base, optimization of fuzzy rule bases, generation of membership functions, and tuning of membership functions (Cordón et al., 2001a). All these tasks can be considered as optimization or search processes within large solution spaces (Bastian and Hayashi, 1995) (Yuan and Zhuang, 1996) (Cordón et al., 2001b).\n\n== Genetic programming for fuzzy system identification==\nWhile genetic algorithms are very powerful tools to identify the fuzzy membership functions of a pre-defined rule base, they have their limitation especially when it also comes to identify the input and output variables of a fuzzy system from a given set of data. Genetic programming has been used to identify the input variables, the rule base as well as the involved membership functions of a fuzzy model (Bastian, 2000)\n\n== Multiobjective Genetic Fuzzy Systems==\nIn the last decade multi-objective optimization of fuzzy rule based systems has attracted wide interest within the research community and practitioners. It is based on the use of stochastic algorithms for [[Multi-objective optimization]] to search for the [[Pareto efficiency]] in a multiple objectives scenario. For instance, the objectives to simultaneously optimize can be accuracy and complexity, or accuracy and interpretability. A recent review of the field is provided in the work of Fazzolari et al. (2013). In addition, [1] provides an up-to-date and continuously growing list of references on the subject.\n\n== References ==\n* 1974, E.H. Mamdani, Applications of fuzzy algorithms for control of simple dynamic plant, Proc. IEE 121 1584 - 1588.\n* 1995, A. Bastian, I. Hayashi: \"An Anticipating Hybrid Genetic Algorithm for Fuzzy Modeling\", Journal of Japan Society for Fuzzy Theory and Systems, Vol.10, pp.&nbsp;801–810\n* 1995, [[George Klir|Klir, G.]] B. Yuan, ''Fuzzy sets and Fuzzy Logic - Theory and Applications'', Prentice-Hall.\n* 1996, Y. Yuan and H. Zhuang, \"A genetic algorithm for generating fuzzy classification rules\", Fuzzy Sets and Systems, V. 84, N. 4, pp.&nbsp;1–19.\n* 1998, W. Pedrycz and F. Gomide, ''An Introduction to Fuzzy Sets: Analysis and Design'', MIT Press.\n* 2000, A. Bastian: ”Identifying Fuzzy Models utilizing Genetic Programming”, Fuzzy Sets and Systems 113, 333–350.\n* 2001, O. Cordón,  F. Herrera, F. Gomide,  F. Hoffmann and L. Magdalena, ''Ten years of genetic-fuzzy systems: a current framework and new trends'',  Proceedings of Joint 9th IFSA World Congress and 20th NAFIPS International Conference, pp.&nbsp;1241–1246, Vancouver - Canada, 2001.\n* 2001, O. Cordon, F. Herrera, F. Hoffmann and L. Magdalena, ''Genetic Fuzzy Systems. Evolutionary tuning and learning of fuzzy knowledge bases'', Advances in Fuzzy Systems: Applications and Theory, World Scientific.\n* 1997, H. Ishibuchi, T. Murata, IB. Türkşen, ''Single-objective and two-objective genetic algorithms for selecting linguistic rules for pattern classification problems'', Fuzzy Sets and Systems, V. 89, N. 2, pp.&nbsp;135–150\n* 2007, M. Cococcioni, B. Lazzerini, F. Marcelloni, ''A Pareto-based multi-objective evolutionary approach to the identification of Mamdani fuzzy systems'', Soft Computing, V.11, N.11, pp.&nbsp;1013–1031\n* 2011, M. Cococcioni, B. Lazzerini, F. Marcelloni, ''On reducing computational overhead in multi-objective genetic Takagi-Sugeno fuzzy systems'', Applied Soft Computing V. 11, N. 1, pp.&nbsp;675–688\n* 2013, M. Fazzolari, R. Alcalá, Y. Nojima, H. Ishibuchi, F. Herrera, ''A Review of the Application of Multiobjective Evolutionary Fuzzy Systems: Current Status and Further Directions'', IEEE T. Fuzzy Systems, V. 21, N. 1, pp.&nbsp;45–65\n* [http://www.iet.unipi.it/m.cococcioni/emofrbss.html] The Evolutionary Multiobjective Optimization of Fuzzy Rule-Based Systems Bibliography Page\n\n[[Category:Computational linguistics]]\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Genetic memory (computer science)",
      "url": "https://en.wikipedia.org/wiki/Genetic_memory_%28computer_science%29",
      "text": "{{Evolutionary algorithms}}\nIn [[computer science]], '''genetic memory''' refers to an [[artificial neural network]] combination of [[genetic algorithm]] and the mathematical model of [[sparse distributed memory]]. It can be used to predict weather patterns.<ref name=\"isbn1-55860-100-7\">{{cite book |author=Rogers, David |editor=Touretzky, David S. |title=Advances in neural information processing systems: Weather prediction using a genetic memory |publisher=M. Kaufmann Publishers |location=Los Altos, Calif |year=1989 |pages=455–464 |isbn=978-1-55860-100-0 |oclc= |doi=}}</ref> Genetic memory and genetic algorithms have also gained an interest in the creation of artificial life.<ref name=\"Rocha\">{{cite journal |vauthors=Rocha LM, Hordijk W |title=Material representations: From the genetic code to the evolution of cellular automata |journal=Artificial Life |volume=11 |issue= 1–2|pages=189–214 |year=2005 |pmid= 15811227|doi=10.1162/1064546053278964 |citeseerx=10.1.1.115.6605 }}</ref>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Genetic Memory (Computer Science)}}\n[[Category:Genetic algorithms]]\n\n\n{{Comp-sci-stub}}"
    },
    {
      "title": "Genetic operator",
      "url": "https://en.wikipedia.org/wiki/Genetic_operator",
      "text": "A '''genetic operator''' is an [[Operator (programming)|operator]] used in [[genetic algorithms]] to guide the algorithm towards a solution to a given problem. There are three main types of operators ([[Mutation (genetic algorithm) |mutation]], [[Crossover (genetic algorithm)|crossover]] and [[selection (genetic algorithm)|selection]]), which must work in conjunction with one another in order for the algorithm to be successful. Genetic operators are used to create and maintain [[genetic diversity]] (mutation operator), combine existing solutions (also known as [[chromosome (genetic algorithm)|chromosome]]s) into new solutions (crossover) and select between solutions (selection).<ref name=ga-intro>{{cite web|title=Introduction to Genetic Algorithms|url=http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol1/hmw/article1.html|accessdate=20 August 2015}}</ref> In his book discussing the use of [[genetic programming]] for the optimization of complex problems, computer scientist [[John Koza]] has also identified an 'inversion' or 'permutation' operator; however, the effectiveness of this operator has never been conclusively demonstrated and this operator is rarely discussed.<ref name=koza>{{cite book|last1=Koza|first1=John R.|title=Genetic programming : on the programming of computers by means of natural selection|date=1996|publisher=MIT Press|location=Cambridge, Mass.|isbn=0-262-11170-5|edition=6. print}}</ref><ref name=gp-operators>{{cite web|title=Genetic programming operators|url=ftp://ftp.cis.upenn.edu/pub/hollick/public_html/genetic/node7.html#SECTION00023000000000000000|accessdate=20 August 2015}}</ref>\n\nMutation (or mutation-like) operators are said to be ''[[Unary operation|unary]]'' operators, as they only operate on one chromosome at a time. In contrast, crossover operators are said to be ''[[Binary operation|binary]]'' operators, as they operate on two chromosomes at a time, combining two existing chromosomes into one new chromosome.<ref name=ga-operators>{{cite web|title=Genetic operators|url=http://kal-el.ugr.es/GAGS/gags-tutorial/node3.html|accessdate=20 August 2015}}</ref>\n\n==Operators==\nGenetic variation is a necessity for the process of [[evolution]]. Genetic operators used in genetic algorithms are analogous to those in the natural world: [[survival of the fittest]], or [[selection (genetic algorithm)|selection]]; reproduction ([[crossover (genetic algorithm)|crossover]], also called recombination); and [[mutation (genetic algorithm)|mutation]].\n\n===Selection===\n{{main|Selection (genetic algorithm)}}\nSelection operators give preference to better solutions (chromosomes), allowing them to pass on their 'genes' to the next generation of the algorithm. The best solutions are determined using some form of [[objective function]] (also known as a '[[fitness function]]' in genetic algorithms), before being passed to the crossover operator. Different methods for choosing the best solutions exist, for example, [[fitness proportionate selection]] and [[tournament selection]]; different methods may choose different solutions as being 'best'. The selection operator may also simply pass the best solutions from the current generation directly to the next generation without being mutated; this is known as ''elitism'' or ''elitist selection''.<ref name=ga-intro /><ref name=ga-intro2>{{cite web|title=Introduction to Genetic Algorithm|url=http://www.rennard.org/alife/english/gavintrgb.html|accessdate=20 August 2015}}</ref>\n\n===Crossover===\n{{main|Crossover (genetic algorithm)}}\nCrossover is the process of taking more than one parent solutions (chromosomes) and producing a child solution from them. By recombining portions of good solutions, the genetic algorithm is more likely to create a better solution.<ref name=ga-intro /> As with selection, there are a number of different methods for combining the parent solutions, including the ''edge recombination operator'' (ERO) and the 'cut and splice crossover' and 'uniform crossover' methods. The crossover method is often chosen to closely match the chromosome's representation of the solution; this may become particularly important when variables are grouped together as [[Genetic algorithm#The building block hypothesis|building blocks]], which might be disrupted by a non-respectful crossover operator. Similarly, crossover methods may be particularly suited to certain problems; the ERO is generally considered a good option for solving the [[travelling salesman problem]].<ref name=ero>{{cite book|last1=Schaffer|first1=George Mason University, June, 4 - 7, 1989. Ed.: J. David|title=Proceedings of the Third International Conference on Genetic Algorithms|date=1991|publisher=Kaufmann|location=San Mateo, Calif.|isbn=1558600663|edition=2. [Dr.]}}</ref>\n\n===Mutation===\n{{main|Mutation (genetic algorithm)}}\nThe mutation operator encourages genetic diversity amongst solutions and attempts to prevent the genetic algorithm converging to a [[local minimum]] by stopping the solutions becoming too close to one another. In mutating the current pool of solutions, a given solution may change entirely from the previous solution. By mutating the solutions, a genetic algorithm can reach an improved solution solely through the mutation operator.<ref name=ga-intro /> Again, different methods of mutation may be used; these range from a simple ''bit mutation'' (flipping random bits in a binary string chromosome with some low probability) to more complex mutation methods, which may replace genes in the solution with random values chosen from the [[uniform distribution (continuous)|uniform distribution]] or the [[Gaussian distribution]]. As with the crossover operator, the mutation method is usually chosen to match the representation of the solution within the chromosome.\n\n==Combining operators==\nWhile each operator acts to improve the solutions produced by the genetic algorithm working individually, the operators must work in conjunction with each other for the algorithm to be successful in finding a good solution. Using the selection operator on its own will tend to fill the solution population with copies of the best solution from the population. If the selection and crossover operators are used without the mutation operator, the algorithm will tend to converge to a [[local minimum]], that is, a good but sub-optimal solution to the problem. Using the mutation operator on its own leads to a [[random walk]] through the search space. Only by using all three operators together can the genetic algorithm become a noise-tolerant hill-climbing algorithm, yielding good solutions to the problem.<ref name=ga-intro />\n\n==References==\n{{reflist|2}}\n\n\n{{DEFAULTSORT:Genetic Operator}}\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Genetic programming",
      "url": "https://en.wikipedia.org/wiki/Genetic_programming",
      "text": "{{distinguish|Generic programming|Genetic engineering}}\n{{Evolutionary algorithms}}\n\nIn artificial intelligence, '''genetic programming''' ('''GP''') is a technique of evolving programs, starting from a population of unfit (usually random) programs, fit for a particular task by applying operations analogous to natural genetic processes to the population of programs.  It is essentially a heuristic search technique often described as 'hill climbing', i.e. searching for an optimal or at least suitable program among the space of all programs.\n\nThe operations are: selection of the fittest programs for reproduction (crossover) and mutation according to a predefined fitness measure, usually proficiency at the desired task.  The crossover operation involves swapping random parts of selected pairs (parents) to produce new and different offspring that become part of the new generation of programs.  Mutation involves substitution of some random part of a program with some other random part of a program. - Some programs not selected for reproduction are copied from the current generation to the new generation. Then the selection and other operations are recursively applied to the new generation of programs.\n\nTypically, members of each new generation are on average more fit than the members of the previous generation, and the best-of-generation program is often better than the best-of-generation programs from previous generations.  Termination of the recursion is when some individual program reaches a predefined proficiency or fitness level.\n\nIt may and often does happen that a particular run of the algorithm results in premature convergence to some local maximum which\nis not a globally optimal or even good solution.  Multiple runs (dozens to hundreds) are usually necessary to produce a very good result.  It may also be necessary to increase the starting population size and variability of the individuals to avoid pathologies.\n\nThe technique, embodied in a system called the 'invention machine' was patented by [[Stanford University]] computer scientist [[John Koza]] in 1988.\n\n==History==\nThe first record of the proposal to evolve programs is probably that of Alan Turing in 1950.<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/oai_cogprints_soton_ac_uk_499.html|title=Computing Machinery and Intelligence|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref> There was a gap of 25 years before the publication of John Holland's 'Adaptation in Natural and Artificial Systems' laid out the theoretical and empirical foundations of the science. In 1981, Richard Forsyth demonstrated the successful evolution of small programs, represented as trees, to perform classification of crime scene evidence for the UK Home Office.<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/kybernetes_forsyth.html|title=BEAGLE A Darwinian Approach to Pattern Recognition|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref> \n\nAlthough the idea of evolving programs, initially in the computer language [[Lisp (programming language)|Lisp]], was current amongst John Holland’s students<ref>A personal communication with [http://www.dcs.bbk.ac.uk/~tom/ Tom Westerdale]</ref>, it was not until they organised the first Genetic Algorithms conference in Pittsburgh that Nichael Cramer<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/icga85_cramer.html|title=A representation for the Adaptive Generation of Simple Sequential Programs|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref> published evolved programs in two specially designed languages. In 1988 John Koza (also a PhD student of John Holland) patented his invention of a GA for program evolution<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/Koza_1990_pat-GAsp.html|title=Non-Linear Genetic Algorithms for Solving Problems|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref>. This was followed by publication in the International Joint Conference on Artificial Intelligence IJCAI-89<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/Koza89.html|title=Hierarchical genetic algorithms operating on populations of computer programs|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref>.\n\nKoza followed this with 205 publications on “Genetic Programming” (GP), name coined by David Goldberg, also a PhD student of John Holland<ref>Goldberg. D.E. (1983), Computer-aided gas pipeline operation using genetic algorithms and rule learning. Dissertation presented to the University of Michigan at Ann Arbor, Michigan, in partial fulfillment of the requirements for Ph.D.</ref>. However, it is the series of 4 books by Koza, starting in 1992<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/koza_book.html|title=Genetic Programming: On the Programming of Computers by Means of Natural Selection|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref> with accompanying videos<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/koza_video.html|title=Genetic Programming:The Movie|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref>, that really established GP. Subsequently, there was an enormous expansion of the number of publications with the Genetic Programming Bibliography, surpassing 10,000 entries<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/Hu_2014_Alife.html|title=The effects of recombination on phenotypic exploration and robustness in evolution|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref>. In 2010, Koza<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/Koza_2010_GPEM.html|title=Human-competitive results produced by genetic programming|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref> listed 77 results where Genetic Programming was human competitive.\n\nIn 1996 Koza started the annual Genetic Programming conference<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/koza_gp96.html|title=Genetic Programming 1996: Proceedings of the First Annual Conference|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref> which was followed in 1998 by the annual EuroGP conference<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/banzhaf_1998_GP.html|title=Genetic Programming|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-19}}</ref>, and the first book<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/langdon_book.html|title=Genetic Programming and Data Structures: Genetic Programming + Data Structures = Automatic Programming!|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref> in a GP series edited by Koza. 1998 also saw the first GP textbook<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/banzhaf_1997_book.html|title=Genetic Programming -- An Introduction; On the Automatic Evolution of Computer Programs and its Applications|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>. GP continued to flourish, leading to the first specialist GP journal<ref>{{Cite journal|last=Banzhaf|first=Wolfgang|date=2000-04-01|title=Editorial Introduction|journal=Genetic Programming and Evolvable Machines|language=en|volume=1|issue=1–2|pages=5–6|doi=10.1023/A:1010026829303|issn=1389-2576}}</ref> and three years later (2003) the annual Genetic Programming Theory and Practice (GPTP) workshop was established by Rick Riolo<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/RioloWorzel_2003.html|title=Genetic Programming Theory and Practice|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref><ref name=\"field guide\">{{Cite web|url=http://www.gp-field-guide.org.uk/|title=A Field Guide to Genetic Programming|website=www.gp-field-guide.org.uk|access-date=2018-05-20}}</ref>. Genetic Programming papers continue to be published at a diversity of conferences and associated journals. Today there are nineteen GP books including several for students<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/banzhaf_1997_book.html|title=Genetic Programming -- An Introduction; On the Automatic Evolution of Computer Programs and its Applications|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>.\n\n== Foundational Work in GP ==\nEarly work that set the stage for current genetic programming research topics and applications is diverse, and includes software synthesis and repair, predictive modeling, data mining<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/freitas_2002_book.html|title=Data Mining and Knowledge Discovery with Evolutionary Algorithms|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>, financial modeling<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/tsang_1998_eddie.html|title=EDDIE beats the bookies|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>, soft sensors<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/Kordon_book.html|title=Applying Computational Intelligence How to Create Value|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>, design<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/DBLP_journals_aiedam_Koza08.html|title=Human-competitive machine invention by means of genetic programming|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>, and image processing<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/lam_doh_gecco2004.html|title=Discovery of Human-Competitive Image Texture Feature Extraction Programs Using Genetic Programming|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>. Applications in some areas, such as design, often make use of intermediate representations<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/bentley_1999_TWGDACEEDP.html|title=Three Ways to Grow Designs: A Comparison of Embryogenies for an Evolutionary Design Problem|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>, such as Fred Gruau’s cellular encoding<ref>{{Cite web|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=243137&url=http://ieeexplore.ieee.org/iel3/1415/6248/00243137.pdf?arnumber=243137|title=Cellular encoding as a graph grammar - IET Conference Publication|pages=17/1–1710|website=ieeexplore.ieee.org|language=en-US|access-date=2018-05-20|date=April 1993}}</ref>. Industrial uptake has been significant in several areas including finance, the chemical industry, bioinformatics<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/taylor_1998_gadiirsab.html|title=Genetic Algorithm Decoding for the Interpretation of Infra-red Spectra in Analytical Biotechnology|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref><ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/langdon_2004_GPEM.html|title=Genetic Programming for Mining DNA Chip data from Cancer Patients|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref> and the steel industry<ref>{{Cite web|url=https://www.cs.bham.ac.uk/~wbl/biblio/gp-html/Kovacic_2009_MMP2.html|title=Genetic Programming and Jominy Test Modeling|website=www.cs.bham.ac.uk|language=en|access-date=2018-05-20}}</ref>.\n\n==Methods==\n===Program representation===\n[[Image:Genetic Program Tree.png|frame|A function represented as a [[tree structure]].]]\n{{main article | genetic representation }}\n\nGP evolves computer programs, traditionally represented in memory as [[tree structure]]s.<ref name=\"sover1985\">Nichael L. Cramer [http://www.sover.net/~nichael/nlc-publications/icga85/index.html \"A Representation for the Adaptive Generation of Simple Sequential Programs\"].</ref> Trees can be easily evaluated in a recursive manner. Every tree node has an operator function and every terminal node has an operand, making mathematical expressions easy to evolve and evaluate. Thus traditionally GP favors the use of [[programming language]]s that naturally embody tree structures (for example, [[Lisp (programming language)|Lisp]]; other [[Functional programming|functional programming languages]] are also suitable).\n\nNon-tree representations have been suggested and successfully implemented, such as [[linear genetic programming]] which suits the more traditional [[imperative languages]] [see, for example, Banzhaf ''et al.'' (1998)].<ref>Garnett Wilson and Wolfgang Banzhaf. [http://www.cs.mun.ca/~banzhaf/papers/eurogp08_clgp.pdf \"A Comparison of Cartesian Genetic Programming and Linear Genetic Programming\"].</ref> The commercial GP software ''Discipulus'' uses automatic induction of binary machine code (\"AIM\")<ref>([[Peter Nordin]], 1997, Banzhaf et al., 1998, Section 11.6.2-11.6.3)</ref> to achieve better performance. ''µGP''<ref>{{cite web|url=http://ugp3.sourceforge.net/|title=µGP (MicroGP)|author=Giovanni Squillero|publisher=}}</ref> uses [[directed multigraph]]s to generate programs that fully exploit the syntax of a given [[assembly language]]. Other program representations on which significant research and development have been conducted include programs for stack-based virtual machines<ref>{{Cite book|last=Perkis|first=T.|title=Stack-based genetic programming|url=http://ieeexplore.ieee.org/document/350025/|journal=Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence|pages=148–153|language=en-US|publisher=IEEE|doi=10.1109/icec.1994.350025|isbn=978-0780318991|year=1994|citeseerx=10.1.1.27.7055}}</ref><ref>{{Cite journal|last=Spector|first=Lee|last2=Robinson|first2=Alan|date=2002-03-01|title=Genetic Programming and Autoconstructive Evolution with the Push Programming Language|journal=Genetic Programming and Evolvable Machines|language=en|volume=3|issue=1|pages=7–40|doi=10.1023/A:1014538503543|issn=1389-2576}}</ref><ref>{{Cite book|last=Spector|first=Lee|last2=Klein|first2=Jon|last3=Keijzer|first3=Maarten|date=2005-06-25|title=The Push3 execution stack and the evolution of control|url=http://dl.acm.org/citation.cfm?id=1068009.1068292|publisher=ACM|pages=1689–1696|doi=10.1145/1068009.1068292|isbn=978-1595930101|citeseerx=10.1.1.153.384}}</ref>, and sequences of integers that are mapped to arbitrary programming languages via grammars<ref>{{Cite book|title=Lecture Notes in Computer Science|last=Ryan|first=Conor|last2=Collins|first2=JJ|last3=Neill|first3=Michael O|date=1998|publisher=Springer Berlin Heidelberg|isbn=9783540643609|location=Berlin, Heidelberg|pages=83–96|doi = 10.1007/bfb0055930|citeseerx = 10.1.1.38.7697}}</ref><ref>{{Cite journal|last=O'Neill|first=M.|last2=Ryan|first2=C.|date=2001|title=Grammatical evolution|journal=IEEE Transactions on Evolutionary Computation|language=en-US|volume=5|issue=4|pages=349–358|doi=10.1109/4235.942529|issn=1089-778X}}</ref>. [[Cartesian genetic programming]] is another form of GP, which uses a graph representation instead of the usual tree based representation to encode computer programs.\n\nMost representations have structurally noneffective code ([[intron]]s). Such non-coding genes may seem to be useless, because they have no effect on the performance of any one individual.  However, they alter the probabilities of generating different offspring under the variation operators, and thus alter the individual's [[variational properties]].\nExperiments seem to show faster convergence when using program representations that allow such non-coding genes, compared to program representations that do not have any non-coding genes.<ref>\nJulian F. Miller.\n[https://www.springer.com/cda/content/document/cda_downloaddocument/9783642173097-c2.pdf \"Cartesian Genetic Programming\"].\np. 19.</ref><ref>\nJanet Clegg; James Alfred Walker; Julian Francis Miller.\n[http://www.cs.bham.ac.uk/~wbl/biblio/gecco2007/docs/p1580.pdf A New Crossover Technique for Cartesian Genetic Programming\"].\n2007.\n</ref>\n\n===Selection===\nSelection is a process whereby certain individuals are selected from the current generation that would serve as parents for the next generation. The individuals are selected probabilistically such that the better performing individuals have a higher chance of getting selected<ref name=\"field guide\" />. The most commonly used selection method in GP is [[tournament selection]], although other methods such as [[fitness proportionate selection]], lexicase selection<ref>{{Cite book|last=Spector|first=Lee|title=Assessment of problem modality by differential performance of lexicase selection in genetic programming: a preliminary report|url=https://dl.acm.org/citation.cfm?id=2330846|journal=Proceedings of the 14th Annual Conference Companion on Genetic and Evolutionary Computation.|pages=401–408|language=en-US|publisher=ACM|doi=10.1145/2330784.2330846|year=2012|isbn=9781450311786|series=Gecco '12}}</ref>, and others have been demonstrated to perform better for many GP problems.\n\nElitism, which involves seeding the next generation with the best individual (or best ''n'' individuals) from the current generation, is a technique sometimes employed to avoid regression.\n\n===Crossover===\nVarious genetic operators (i.e., crossover and mutation) are applied to the individuals selected in the selection step described above to breed new individuals. The rate at which these operators are applied determine the diversity in the population.\n\n===Mutation===\n\n==Applications==\nGP has been successfully used as an [[automatic programming]] tool, a machine learning tool and an automatic problem-solving engine<ref name=\"field guide\" />. GP is especially useful in the domains where the exact form of the \nsolution is not known in advance or an approximates solution is acceptable (possibly because finding the exact solution is very difficult). Some of the applications of GP are curve fitting, data modeling, [[Symbolic regression]], feature selection, classification, etc. John R. Koza mentions 76\ninstances where Genetic Programming has been able to produce results that are competitive with human-produced results (called Human-competitive results)<ref>{{Cite journal|last=Koza|first=John R|title=Human-competitive results produced by genetic programming|journal=Genetic Programming and Evolvable Machines|volume=11|issue=3–4|pages=251–284|language=en-US|doi=10.1007/s10710-010-9112-3|year=2010}}</ref>. Since 2004, the annual Genetic and Evolutionary Computation Conference (GECCO) holds Human Competitive Awards (called Humies) competition<ref>{{cite web|url=http://www.human-competitive.org/awards|title=Humn =Human-Competitive Awards|publisher=}}</ref>, where cash awards are presented to human-competitive results produced by any form of genetic and evolutionary computation. GP has won many awards in this competition over the years.\n\n==Meta-genetic programming==\nMeta-genetic programming is the proposed [[meta learning (computer science)|meta learning]] technique of evolving a genetic programming system using genetic programming itself. It suggests that chromosomes, crossover, and mutation were themselves evolved, therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer. Meta-GP was formally proposed by [[Jürgen Schmidhuber]] in 1987<ref>{{cite web|url=http://www.idsia.ch/~juergen/diploma.html|title=1987 THESIS ON LEARNING HOW TO LEARN, METALEARNING, META GENETIC PROGRAMMING,CREDIT-CONSERVING MACHINE LEARNING ECONOMY|publisher=}}</ref>. [[Douglas Lenat|Doug Lenat]]'s [[Eurisko]] is an earlier effort that may be the same technique. It is a recursive but terminating algorithm, allowing it to avoid infinite recursion. In the \"autoconstructive evolution\" approach to meta-genetic programming, the methods for the production and variation of offspring are encoded within the evolving programs themselves, and programs are executed to produce new programs to be added to the population.<ref>{{Cite journal|last=Spector|first=Lee|last2=Robinson|first2=Alan|date=2002-03-01|title=Genetic Programming and Autoconstructive Evolution with the Push Programming Language|journal=Genetic Programming and Evolvable Machines|language=en|volume=3|issue=1|pages=7–40|doi=10.1023/A:1014538503543|issn=1389-2576}}</ref><ref>{{Cite book|title=GECCO '16 Companion : proceedings of the 2016 Genetic and Evolutionary Computation Conference : July 20-24, 2016, Denver, Colorado, USA|others=Neumann, Frank (Computer scientist), Association for Computing Machinery. SIGEVO|isbn=9781450343237|location=New York, New York|oclc=987011786}}</ref>\n\nCritics of this idea often say this approach is overly broad in scope. However, it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes. This might take the form of a meta evolved GP for producing human walking algorithms which is then used to evolve human running, jumping, etc. The fitness criterion applied to the meta GP would simply be one of efficiency.\n\n==See also==\n\n* [[Bio-inspired computing]]\n* [[CMA-ES|Covariance Matrix Adaptation Evolution Strategy]] (CMA-ES)\n* [[Fitness approximation]]\n* [[Gene expression programming]]\n* [[Genetic improvement]]\n* [[Genetic representation]]\n* [[Grammatical evolution]]\n* [[Inductive programming]]\n* [[Linear genetic programming]]\n* [[Multi expression programming]]\n* [[Propagation of schema]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [https://web.archive.org/web/20070813222058/http://uk.geocities.com/markcsinclair/abstracts.html#pro00a/ Aymen S Saket & Mark C Sinclair]\n* [https://www.springer.com/computer/ai/journal/10710 ''Genetic Programming and Evolvable Machines''], a journal\n* [http://www.modulusfe.com/products/trading-system-developer-components/evo2-genetic-algorithm/ ''Evo2 for genetic programming'']\n* [http://www.cs.bham.ac.uk/~wbl/biblio/README.html GP bibliography]\n* [http://www.etsimo.uniovi.es/ftp/pub/EC/FAQ/www/ The Hitch-Hiker's Guide to Evolutionary Computation]\n* Riccardo Poli, William B. Langdon,Nicholas F. McPhee, John R. Koza, \"[http://cswww.essex.ac.uk/staff/poli/gp-field-guide/index.html A Field Guide to Genetic Programming]\" (2008)\n* [http://www.geneticprogramming.com Genetic Programming, a community maintained resource]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Genetic Programming}}\n<!-- categories -->\n[[Category:Genetic algorithms]]\n[[Category:Genetic programming| ]]"
    },
    {
      "title": "Holland's schema theorem",
      "url": "https://en.wikipedia.org/wiki/Holland%27s_schema_theorem",
      "text": "'''Holland's schema theorem''', also called the '''fundamental theorem of genetic algorithms''',<ref>{{cite conference |last1=Bridges |first1=Clayton L. |first2=David E. |last2=Goldberg |title=An analysis of reproduction and crossover in a binary-coded genetic algorithm |conference=2nd Int'l Conf. on Genetic Algorithms and their applications |year=1987|url=https://books.google.com/books?id=MYJ_AAAAQBAJ&printsec=frontcover#v=onepage&q=%22An%20analysis%20of%20reproduction%20and%20crossover%20in%20a%20binary-coded%20genetic%20algorithm%22&f=false}}</ref> is an inequality that results from coarse-graining an equation for evolutionary dynamics.  The Schema Theorem says that short, low-order schemata with above-average fitness increase exponentially in frequency in successive generations. The theorem was proposed by [[John Henry Holland|John Holland]] in the 1970s. It was initially widely taken to be the foundation for explanations of the power of [[genetic algorithm]]s. However, this interpretation of its implications has been criticized in several publications reviewed in <ref>Altenberg, L. (1995). [http://dynamics.org/Altenberg/FILES/LeeSTPT.pdf The Schema Theorem and Price’s Theorem]. Foundations of genetic algorithms, 3, 23-49.</ref>, where the Schema Theorem is shown to be a special case of the [[Price equation]] with the schema indicator function as the macroscopic measurement.  \n\nA [[schema (genetic algorithms)|schema]] is a template that identifies a [[subset]] of strings with similarities at certain string positions. Schemata are a special case of [[cylinder set]]s, and hence form a [[topological space]].\n\n== Description ==\n\nConsider binary strings of length 6. The schema <code>1*10*1</code> describes the set of all strings of length 6 with 1's at positions 1, 3 and 6 and a 0 at position 4. The * is a [[Wildcard character|wildcard]] symbol, which means that positions 2 and 5 can have a value of either 1 or 0. The ''order of a schema'' <math> o(H)</math> is defined as the number of fixed positions in the template, while the ''[[defining length]]'' <math> \\delta(H) </math> is the distance between the first and last specific positions. The order of <code>1*10*1</code> is 4 and its defining length is 5. The ''fitness of a schema'' is the average fitness of all strings matching the schema. The fitness of a string is a measure of the value of the encoded problem solution, as computed by a problem-specific evaluation function. Using the established methods and [[genetic operator]]s of [[genetic algorithms]], the schema theorem states that short, low-order schemata with above-average fitness increase exponentially in successive generations. Expressed as an equation:\n\n:<math>\\operatorname{E}(m(H,t+1)) \\geq {m(H,t) f(H) \\over a_t}[1-p].</math>\n\nHere <math>m(H,t)</math> is the number of strings belonging to schema <math>H</math> at generation <math>t</math>, <math>f(H)</math> is the ''observed'' average fitness of schema <math>H</math> and <math>a_t</math> is the ''observed'' average fitness at generation <math>t</math>. The probability of disruption <math>p</math> is the probability that crossover or mutation will destroy the schema <math>H</math>. It can be expressed as:\n\n:<math>p = {\\delta(H) \\over l-1}p_c + o(H) p_m</math>\n\nwhere <math> o(H)</math> is the order of the schema, <math>l</math> is the length of the code, <math> p_m</math> is the probability of mutation and <math> p_c </math> is the probability of crossover. So a schema with a shorter defining length <math> \\delta(H) </math> is less likely to be disrupted.<br />An often misunderstood point is why the Schema Theorem is an ''inequality'' rather than an equality. The answer is in fact simple: the Theorem neglects the small, yet non-zero, probability that a string belonging to the schema <math>H</math> will be created \"from scratch\" by mutation of a single string (or recombination of two strings) that did ''not'' belong to <math>H</math> in the previous generation.\n\n==Limitation==\n[[File:Bimodal-bivariate-small.png|thumb|right|Plot of a multimodal function in two variables.]]\nThe schema theorem holds under the assumption of a genetic algorithm that maintains an infinitely large population, but does not always carry over to (finite) practice: due to [[sampling error]] in the initial population, genetic algorithms may converge on schemata that have no selective advantage. This happens in particular in [[multimodal distribution|multimodal optimization]], where a function can have multiple peaks: the population may [[genetic drift|drift]] to prefer one of the peaks, ignoring the others.<ref>{{cite conference |first1=Goldberg |last1=David E. |first2=Jon |last2=Richardson |title=Genetic algorithms with sharing for multimodal function optimization |conference=2nd Int'l Conf. on Genetic Algorithms and their applications |year=1987|url=https://books.google.com/books?id=MYJ_AAAAQBAJ&printsec=frontcover#v=onepage&q&f=false}}</ref>\n\nThe reason that the Schema Theorem cannot explain the power of genetic algorithms is that it holds for all problem instances, and cannot distinguish between problems in which genetic algorithms perform poorly, and problems for which genetic algorithms perform well.\n\n==References==\n{{reflist}}\n\n* J. Holland, ''[https://books.google.com/books?id=5EgGaBkwvWcC&printsec=frontcover#v=onepage&q&f=false Adaptation in Natural and Artificial Systems]'', The MIT Press; Reprint edition 1992 (originally published in 1975).\n* J. Holland, ''Hidden Order: How Adaptation Builds Complexity'', Helix Books; 1996.\n\n[[Category:Genetic algorithms]]\n[[Category:Theorems in discrete mathematics]]"
    },
    {
      "title": "Inheritance (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Inheritance_%28genetic_algorithm%29",
      "text": "{{For|information about inheritance in biology|heredity}}\n\nIn [[genetic algorithm]]s, '''inheritance''' is the ability of modeled objects to [[mating|mate]], [[mutation (genetic algorithm)|mutate]] (similar to [[biology|biological]] [[mutation]]), and propagate their problem solving [[gene]]s to the next [[generation]], in order to produce an evolved solution to a particular problem. The [[selection (genetic algorithm)|selection]] of objects that will be inherited from in each successive generation is determined by a [[fitness function]], which varies depending upon the problem being addressed.<ref name=\"Stuart Norvig 1995\">Russell, Stuart J.; Norvig, Peter (1995). ''[[Artificial Intelligence: A Modern Approach]]''. Englewood Heights, NJ: Prentice-Hall.</ref>\n\nThe traits of these objects are passed on through [[chromosome]]s by a means similar to biological [[reproduction]]. These chromosomes are generally represented by a series of [[gene]]s, which in turn are usually represented using [[binary number]]s. This propagation of traits between generations is similar to the inheritance of [[phenotypic trait|traits]] between generations of biological [[organism]]s. This process can also be viewed as a form of [[reinforcement learning]], because the [[evolution]] of the objects is driven by the passing of traits from successful objects which can be viewed as a [[reinforcement|reward]] for their success, thereby promoting beneficial traits.<ref name=\"Stuart Norvig 1995\" />\n\n== Process ==\nOnce a new generation is ready to be created, all of the individuals that have been successful and have been chosen for reproduction are randomly paired together. Then the traits of these individuals are passed on through a combination of [[crossover (genetic algorithm)|crossover]] and mutation.<ref name=\"Stuart Norvig 1995\" /> This process follows these basic steps:\n\n# Pair off successful objects for mating.\n# Determine randomly a crossover point for each pair.\n# Switch the genes after the crossover point in each pair.\n# Determine randomly if any genes are mutated in the child objects.\n\nAfter following these steps, two child objects will be produced for every pair of parent objects used. Then, after determining the success of the objects in the new generation, this process can be repeated using whichever new objects were most successful. This will usually be repeated until either a desired generation is reached or an object that meets a minimum desired result from the fitness function is found.\n\nWhile crossover and mutation are the common [[genetic operator]]s used in inheritance, there are also other operators such as regrouping and colonization-extinction.<ref>Akbari, Ziarati (2010). \"A multilevel evolutionary algorithm for optimizing numerical functions\" IJIEC 2 (2011): 419&ndash;430  [http://growingscience.com/ijiec/Vol2/IJIEC_2010_11.pdf]</ref>\n\n=== Example ===\nAssume these two strings of bits represent the traits being passed on by two parent objects:\n\n* Object 1: 1100011010110001\n* Object 2: 1001100110011001\n\nNow, consider that the crossover point is randomly positioned after the fifth bit:\n\n* Object 1: 11000 | 11010110001\n* Object 2: 10011 | 00110011001\n\nDuring crossover, the two objects will swap all of the bits after the crossover point, leading to:\n\n* Object 1: 11000 | 00110011001\n* Object 2: 10011 | 11010110001\n\nFinally, mutation is simulated on the objects by there being zero or more bits flipped randomly. Assuming the tenth bit for object 1 is mutated, and the second and seventh bits are mutated for object 2, the final children produced by this inheritance would be:\n\n* Object 1: 1100000111011001\n* Object 2: 1101110010110001\n\n== See also ==\n* [[Artificial intelligence]]\n* [[Bioinformatics]]\n* [[Speciation (genetic algorithm)]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.boxcar2d.com/ BoxCar 2D] An interactive example of the use of a genetic algorithm to construct 2-dimensional cars.\n\n{{DEFAULTSORT:Inheritance (Genetic Algorithm)}}\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Linkage disequilibrium score regression",
      "url": "https://en.wikipedia.org/wiki/Linkage_disequilibrium_score_regression",
      "text": "In [[statistical genetics]], '''linkage disequilibrium score regression''' ('''LDSR'''<ref name=levinsonetal/> or '''LDSC'''<ref>{{Cite journal |last=Ni |first=Guiyan |last2=Moser |first2=Gerhard |last3=Wray |first3=Naomi R. |last4=Lee |first4=S. Hong |last5=Ripke |first5=Stephan |last6=Neale |first6=Benjamin M. |last7=Corvin |first7=Aiden |last8=Walters |first8=James T.R. |last9=Farh |first9=Kai-How |date=June 2018 |title=Estimation of Genetic Correlation via Linkage Disequilibrium Score Regression and Genomic Restricted Maximum Likelihood |journal=The American Journal of Human Genetics |volume=102 |issue=6 |pages=1185–1194 |doi=10.1016/j.ajhg.2018.03.021 |issn=0002-9297 |pmc=5993419 |pmid=29754766}}</ref>) is a technique that aims to quantify the separate contributions of [[polygenic]] effects and various [[confounding]] factors, such as [[population stratification]], based on [[summary statistics]] from [[genome-wide association studies]] (GWASs). The approach involves using [[regression analysis]] to examine the relationship between [[linkage disequilibrium]] scores and the [[test statistic]]s of the [[single-nucleotide polymorphism]]s (SNPs) from the GWAS. Here, the \"linkage disequilibrium score\" for a SNP \"is the sum of LD [[coefficient of determination|''r<sup>2</sup>'']] measured with all other SNPs\".<ref>{{Cite journal |last=Neale |first=Benjamin M. |last2=Price |first2=Alkes L. |last3=Daly |first3=Mark J. |last4=Patterson |first4=Nick |last5=Consortium |first5=Schizophrenia Working Group of the Psychiatric Genomics |last6=Yang |first6=Jian |last7=Ripke |first7=Stephan |last8=Finucane |first8=Hilary K. |last9=Loh |first9=Po-Ru |date=March 2015 |title=LD Score regression distinguishes confounding from polygenicity in genome-wide association studies |journal=Nature Genetics |language=en |volume=47 |issue=3 |pages=291–295 |doi=10.1038/ng.3211 |issn=1546-1718 |pmc=4495769 |pmid=25642630}}</ref> LDSC can be used to produce SNP-based [[heritability]] estimates, to partition this heritability into separate categories, and to calculate [[genetic correlation]]s between separate [[phenotype]]s. Because the LDSC approach relies only on summary statistics from an entire GWAS, it can be used efficiently even with very large sample sizes.<ref>{{Cite journal |last=Neale |first=Benjamin M. |last2=Evans |first2=David M. |last3=Gaunt |first3=Tom R. |last4=Paternoster |first4=Lavinia |last5=Anttila |first5=Verneri |last6=Bulik-Sullivan |first6=Brendan K. |last7=Price |first7=Alkes L. |last8=Finucane |first8=Hilary K. |last9=Warrington |first9=Nicole M. |date=2017-01-15 |title=LD Hub: a centralized database and web interface to perform LD score regression that maximizes the potential of summary level GWAS data for SNP heritability and genetic correlation analysis |journal=Bioinformatics |language=en |volume=33 |issue=2 |pages=272–279 |doi=10.1093/bioinformatics/btw613 |issn=1367-4803 |pmc=5542030 |pmid=27663502|hdl=2381/38771 }}</ref> In LDSC, genetic correlations are calculated based on the deviation between [[chi-square statistic]]s and what would be expected assuming the [[null hypothesis]].<ref name=levinsonetal>{{Cite journal |last=Levinson |first=Douglas F. |last2=Noordsy |first2=Douglas L. |last3=Hardy |first3=Kate V. |last4=Ballon |first4=Jacob S. |last5=Shen |first5=Hanyang |last6=Duncan |first6=Laramie E. |date=2018-10-17 |title=Genetic Correlation Profile of Schizophrenia Mirrors Epidemiological Results and Suggests Link Between Polygenic and Rare Variant (22q11.2) Cases of Schizophrenia |journal=Schizophrenia Bulletin |language=en |volume=44 |issue=6 |pages=1350–1361 |doi=10.1093/schbul/sbx174 |issn=0586-7614 |pmc=6192473 |pmid=29294133}}</ref>\n\n==Extensions==\nLDSC can also be applied across traits to estimate genetic correlations. This extension of LDSC, known as '''cross-trait LD score regression''', has the advantage of not being biased if used on overlapping samples.<ref>{{Cite journal |last=Neale |first=Benjamin M. |last2=Price |first2=Alkes L. |last3=Daly |first3=Mark J. |last4=Robinson |first4=Elise B. |last5=Patterson |first5=Nick |last6=Perry |first6=John R. B. |last7=Duncan |first7=Laramie |last8=Consortium 3 |first8=Genetic Consortium for Anorexia Nervosa of the Wellcome Trust Case Control |last9=Consortium |first9=Psychiatric Genomics |date=November 2015 |title=An atlas of genetic correlations across human diseases and traits |journal=Nature Genetics |language=en |volume=47 |issue=11 |pages=1236–1241 |doi=10.1038/ng.3406 |issn=1546-1718 |pmc=4797329 |pmid=26414676}}</ref> There is also another extension of LDSC, known as '''stratified LD score regression''' (abbreviated '''SLDSR'''),<ref>{{Cite journal |last=Nivard |first=Michel G. |last2=Boomsma |first2=Dorret I. |last3=Consortium |first3=UK Brain Expression |last4=Bartels |first4=Meike |last5=Abdellaoui |first5=Abdel |last6=Jansen |first6=Rick |last7=Ip |first7=Hill F. |date=2018-09-01 |title=Characterizing the Relation Between Expression QTLs and Complex Traits: Exploring the Role of Tissue Specificity |journal=Behavior Genetics |language=en |volume=48 |issue=5 |pages=374–385 |doi=10.1007/s10519-018-9914-2 |issn=1573-3297 |pmc=6097736 |pmid=30030655}}</ref> that aims to partition heritability by functional annotation by taking into account [[genetic linkage]] between [[genetic marker|marker]]s.<ref>{{Cite journal |last=Price |first=Alkes L. |last2=Neale |first2=Benjamin M. |last3=Patterson |first3=Nick |last4=Daly |first4=Mark J. |last5=Raychaudhuri |first5=Soumya |last6=Okada |first6=Yukinori |last7=Perry |first7=John R. B. |last8=Lindstrom |first8=Sara |last9=Stahl |first9=Eli |date=November 2015 |title=Partitioning heritability by functional annotation using genome-wide association summary statistics |journal=Nature Genetics |language=en |volume=47 |issue=11 |pages=1228–1235 |doi=10.1038/ng.3404 |issn=1546-1718 |pmc=4626285 |pmid=26414678}}</ref><ref>{{Cite journal |last=Smoller |first=Jordan W. |last2=Sabuncu |first2=Mert R. |last3=Neale |first3=Benjamin M. |last4=Chen |first4=Chia-Yen |last5=Ge |first5=Tian |date=2017-04-07 |title=Phenome-wide heritability analysis of the UK Biobank |journal=PLOS Genetics |language=en |volume=13 |issue=4 |pages=e1006711 |doi=10.1371/journal.pgen.1006711 |issn=1553-7404 |pmc=5400281 |pmid=28388634}}</ref>\n==References==\n{{Reflist}}\n\n\n[[Category:Regression analysis]]\n[[Category:Genetic algorithms]]\n[[Category:Statistical genetics]]\n{{Genetics-stub}}"
    },
    {
      "title": "Mating pool",
      "url": "https://en.wikipedia.org/wiki/Mating_pool",
      "text": "A '''mating pool''' is a concept used in [[evolutionary computation]]. More specifically, it is one of the four steps in [[genetic algorithm]] operations that places value on the [[Fitness function|fitness]] of individuals in a population.<ref>Regupathi, R. “Cost Optimization Of Multistoried Rc Framed Structure Using Hybrid Genetic Algorithm.”  ''International Research Journal of Engineering and Technology (IRJET)'', vol. 04, no. 07, July 2017, p. 890., www.irjet.net/archives/V4/i7/IRJET-V4I7211.pdf.</ref> \n\nTypically, the population of [[candidate solution]]s for a genetic algorithm is treated as a single entity. An alternative approach is to separate only individuals who will produce offspring from the remainder of the current population. These are placed into a ''mating pool''.<ref>{{Cite book|url=https://www.worldcat.org/oclc/47717957|title=Foundations of genetic programming|last=B.)|first=Langdon, W. B. (William|date=2002|publisher=Springer|others=Poli, Riccardo, 1961-|year=|isbn=3540424512|location=Berlin|pages=|oclc=47717957}}</ref>\n\n[[Selection_(genetic_algorithm)|Selection operators]] are applied to the entire population in order to determine the fittest individuals of the current population, which are then placed into the mating pool.<ref>{{Cite web|url=http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol1/hmw/article1.html#introduction|title=Introduction to Genetic Algorithms|website=www.doc.ic.ac.uk|access-date=2017-11-09}}</ref> [[Genetic operator|Genetic operations]], such as crossover and mutation, are then applied to the mating pool to create the next generation.<ref>{{Cite web|url=http://kal-el.ugr.es/GAGS/gags-tutorial/node3.html|title=Genetic Operators|website=kal-el.ugr.es|access-date=2017-11-09}}</ref>\n\nThere are multiple ways to create a mating pool, including selecting the best individuals based on certain criteria, known as [[truncation selection]], and choosing individuals randomly with the probability of being chosen based on fitness level, known as [[fitness proportionate selection]]. Additionally, a random sub-group can be selected from the population and then the individuals with the highest fitness are selected for the mating pool, known as [[tournament selection]].<ref>{{Cite book|url=https://www.worldcat.org/oclc/39261941|title=Rough sets and current trends in computing : first international conference, RSCTC '98, Warsaw, Poland, June 22-26, 1998 : proceedings|date=1998|publisher=Springer|others=Polkowski, Lech., Skowron, Andrzej.|isbn=3540646558|location=Berlin|oclc=39261941}}</ref>\n==References==\n<references />{{evolution-stub}}\n\n[[Category:Population genetics]]\n[[Category:Evolutionary computation]]\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Mutation (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Mutation_%28genetic_algorithm%29",
      "text": "{{Evolutionary algorithms}}\n'''Mutation''' is a [[genetic operator]] used to maintain [[genetic diversity]] from one generation of a population of [[genetic algorithm]] [[chromosome (genetic algorithm)|chromosomes]] to the next. It is analogous to biological [[mutation]]. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to a better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search.\n\nThe classic example of a mutation operator involves a probability that an arbitrary [[bit]] in a [[genome (genetic algorithm)|genetic sequence]] will be changed from its original state. A common method of implementing the mutation operator involves generating a [[random variable]] for each bit in a sequence. This random variable tells whether or not a particular bit will be modified. This mutation procedure, based on the biological [[point mutation]], is called single point mutation. Other types are inversion and floating point mutation. When the gene encoding is restrictive as in permutation problems, mutations are swaps, inversions, and scrambles.\n\nThe purpose of mutation in GAs is preserving and introducing diversity. Mutation should allow the algorithm to avoid [[local minimum|local minima]] by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping evolution. This reasoning also explains the fact that most GA systems avoid only taking the [[Fitness function|fittest]] of the population in generating the next but rather a random (or semi-random) selection with a weighting toward those that are fitter.<ref>{{cite web\n| accessdate = 2011-04-07\n| location = http://www.obitko.com/\n| publisher = Marek Obitko \n| title = XI. Crossover and Mutation\n| url = http://www.obitko.com/tutorials/genetic-algorithms/crossover-mutation.php}}</ref>\n\nFor different genome types, different mutation types are suitable:\n\n* '''Bit string mutation'''\n::The mutation of bit strings ensue through bit flips at random positions.\n\n::Example:\n::{|\n|1 || 0 || 1 || 0 || 0 || 1 || 0\n|-\n| || || || || ↓ || || \n|- \n|1 || 0 || 1 || 0 || 1 || 1 || 0\n|}\n\n::The probability of a mutation of a bit is <math>\\frac{1}{l}</math>, where <math>l</math> is the length of the binary vector. Thus, a mutation rate of <math>1</math> per mutation and individual selected for mutation is reached.\n\n* '''Flip Bit'''\nThis mutation operator takes the chosen genome and inverts the bits \n(i.e. if the genome bit is 1, it is changed to 0 and vice versa).\n\n* '''Boundary'''\nThis mutation operator replaces the genome with either lower or upper bound randomly.\nThis can be used for integer and float genes.\n\n* '''Non-Uniform'''\nThe probability that amount of mutation will go to 0 with the next generation is increased by using non-uniform mutation operator. It keeps the population from stagnating in the early stages of the evolution. It tunes solution in later stages of evolution. This mutation operator can only be used for integer and float genes.\n\n* '''Uniform'''\nThis operator replaces the value of the chosen gene with a uniform random value selected between the user-specified upper and lower bounds for that gene. This mutation operator can only be used for integer and float genes.\n\n* '''Gaussian'''\nThis operator adds a unit Gaussian distributed random value to the chosen gene. If it falls outside of the user-specified lower or upper bounds for that gene, the new gene value is clipped. This mutation operator can only be used for integer and float genes.\n\n* '''Shrink'''\nThis operator adds a random number taken from a Gaussian distribution with mean equal to the original value of each decision variable characterizing the entry parent vector.  <ref>Claudio Comis Da Ronco, Ernesto Benini, A Simplex-Crossover-Based Multi-Objective Evolutionary Algorithm, IAENG Transactions on Engineering Technologies, Volume 247 of the series Lecture Notes in Electrical Engineering pp 583-598, 2013 https://link.springer.com/chapter/10.1007%2F978-94-007-6818-5_41</ref>\n\n==See also==\n* [[Genetic algorithm]]s\n\n==References==\n{{Reflist}}\n\n==Bibliography==\n* John Holland, Adaptation in Natural and Artificial Systems, [[University of Michigan Press]], Ann Arbor, Michigan. 1975. {{ISBN|0-262-58111-6}}.\n\n{{DEFAULTSORT:Mutation (Genetic Algorithm)}}\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Population-based incremental learning",
      "url": "https://en.wikipedia.org/wiki/Population-based_incremental_learning",
      "text": "In [[computer science]] and [[machine learning]], '''population-based incremental learning''' ('''PBIL''') is an [[Optimization (mathematics)|optimization]] [[algorithm]], and an [[estimation of distribution algorithm]]. This is a type of [[genetic algorithm]] where the [[genotype]] of an entire population ([[probability]] [[Euclidean vector|vector]]) is evolved rather than individual members.<ref>\n{{Citation\n  | last1 = Karray | first1 = Fakhreddine O.\n  | last2 = de Silva | first2 = Clarence\n  | title = Soft computing and intelligent systems design\n  | year = 2004\n  | publisher = Addison Wesley\n  | isbn = 0-321-11617-8}}</ref> The algorithm is proposed by Shumeet Baluja in 1994. The algorithm is simpler than a standard genetic algorithm, and in many cases leads to better results than a standard genetic algorithm.<ref name=\"Baluja1\">{{Citation\n  | last1 = Baluja | first1 = Shumeet\n  | title = Population-Based Incremental Learning: A Method for Integrating Genetic Search Based Function Optimization and Competitive Learning\n  | year = 1994\n  | periodical = Technical Report\n  | publisher = Carnegie Mellon University\n  | place = Pittsburgh, PA\n  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.8554 \n  | issue = CMU–CS–94–163\n}}</ref><ref name=\"Baluja2\">{{Citation\n  | last1 = Baluja | first1 = Shumeet\n  | last2 = Caruana | first2 = Rich\n  | title = Removing the Genetics from the Standard Genetic Algorithm\n  | year = 1995\n  | publisher = Morgan Kaufmann Publishers\n  | pages = 38–46\n  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.5424\n}}</ref><ref name=\"Baluja3\">{{Citation\n  | last1 = Baluja | first1 = Shumeet\n  | title = An Empirical Comparison of Seven Iterative and Evolutionary Function Optimization Heuristics\n  | year = 1995\n  | publisher =\n  | pages =\n  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.1108\n}}</ref>\n\n== Algorithm ==\nIn PBIL, genes are represented as real values in the range [0,1], indicating the probability that any particular [[allele]] appears in that [[gene]].\n\nThe PBIL algorithm is as follows:\n\n# A population is generated from the probability vector.\n# The fitness of each member is evaluated and ranked.\n# Update population genotype (probability vector) based on fittest individual.\n# Mutate.\n# Repeat steps 1-4\n\n== Source code ==\nThis is a part of source code implemented in [[Java (programming language)|Java]]. In the paper, learnRate = 0.1, negLearnRate = 0.075, mutProb = 0.02, and mutShift = 0.05 is used. N = 100 and ITER_COUNT = 1000 is enough for a small problem.\n\n<source lang=\"java\">\npublic void optimize() {\n    final int totalBits = getTotalBits();\n    final double[] probVec = new double[totalBits];\n    Arrays.fill(probVec, 0.5);\n    bestCost = POSITIVE_INFINITY;\n \n    for (int i = 0; i < ITER_COUNT; i++) {\n        // Creates N genes\n        final boolean[][] genes = new [N][totalBits];\n        for (boolean[] gene : genes) {\n            for (int k = 0; k < gene.length; k++) {\n                if (rand_nextDouble() < probVec[k])\n                    gene[k] = true;\n            }\n        }\n\n        // Calculate costs\n        final double[] costs = new double[N];\n        for (int j = 0; j < N; j++) {\n            costs[j] = costFunc.cost(toRealVec(genes[j], domains));\n        }\n\n        // Find min and max cost genes\n        boolean[] minGene = null, maxGene = null;\n        double minCost = POSITIVE_INFINITY, maxCost = NEGATIVE_INFINITY;\n        for (int j = 0; j < N; j++) {\n            double cost = costs[j];\n            if (minCost > cost) {\n                minCost = cost;\n                minGene = genes[j];\n            }\n            if (maxCost < cost) {\n                maxCost = cost;\n                maxGene = genes[j];\n            }\n        }\n\n        // Compare with the best cost gene\n        if (bestCost > minCost) {\n            bestCost = minCost;\n            bestGene = minGene;\n        }\n\n        // Update the probability vector with max and min cost genes\n        for (int j = 0; j < totalBits; j++) {\n            if (minGene[j] == maxGene[j]) {\n                probVec[j] = probVec[j] * (1d - learnRate) +\n                        (minGene[j] ? 1d : 0d) * learnRate;\n            } else {\n                final double learnRate2 = learnRate + negLearnRate;\n                probVec[j] = probVec[j] * (1d - learnRate2) +\n                        (minGene[j] ? 1d : 0d) * learnRate2;\n            }\n        }\n\n        // Mutation\n        for (int j = 0; j < totalBits; j++) {\n            if (rand.nextDouble() < mutProb) {\n                probVec[j] = probVec[j] * (1d - mutShift) +\n                        (rand.nextBoolean() ? 1d : 0d) * mutShift;\n            }\n        }\n    }\n}\n</source>\n\n==See also==\n* [[Estimation of distribution algorithm]] (EDA)\n* [[Learning classifier system|Learning Classifier System]] (LCS)\n\n== References ==\n<references/>\n\n[[Category:Genetic algorithms]]\n[[Category:Articles with example Java code]]"
    },
    {
      "title": "Premature convergence",
      "url": "https://en.wikipedia.org/wiki/Premature_convergence",
      "text": "In [[genetic algorithm]]s, the term of '''premature convergence''' means that a population for an [[optimization problem]] converged too early, resulting in being [[suboptimal]]. In this context, the parental solutions, through the aid of [[genetic operator]]s, are not able to generate offspring that are superior to, or outperform, their parents. Premature convergence is a common problem found in genetic algorithms, as it leads to a loss, or convergence of, a large number of alleles, subsequently making it very difficult to search for a specific gene in which the alleles were present.<ref name=\":0\"><small>Leung, Y., ''et al.'' (1997). Degree of population diversity – A perspective on premature convergence in genetic algorithms and its markov chain, ''IEEE Transactions on Neural Networks'', vol. 8, pp. 1165 – 1176.</small></ref><ref name=\":1\"><small>Baker, J.E. & Grefenstette, J. (2014). ''Proceedings of the First International Conference on Genetic Algorithms and their Applications''. Hoboken: Taylor and Francis, pp. 101 - 105.</small></ref> An allele is considered lost if in a population a gene is present where all individuals are sharing the same value for that particular gene. An allele is, as defined by De Jong, considered to be a converged allele, when 95% of a population share the same value for a certain gene (see also [[Convergence (evolutionary computing)|convergence]]).<ref><small>De Jong, K.A. (1975). Analysis of the behaviour of a class of genetic adaptive systems, ph.D. dissertation, University of Michigan.</small></ref>\n\n==Strategies for preventing premature convergence==\nStrategies to regain genetic variation can be:\n* a mating strategy called ''incest prevention'',<ref name=zhigl1991>\n{{cite book\n  | author = Michalewicz, Zbigniew\n  | title = Genetic Algorithms + Data Structures = Evolution Programs, 3rd Edition\n  | year = 1996\n  | publisher = Springer-Verlag\n  | isbn = 3-540-60676-9\n  | page = 58\n}}</ref>\n* [[Crossover (genetic algorithm)#Uniform Crossover and Half Uniform Crossover|uniform crossover]],\n* favored replacement of similar individuals (''preselection'' or ''crowding''),\n* segmentation of individuals of similar fitness (''fitness sharing''),\n* increasing population size.\n\nThe genetic variation can also be regained by [[mutation]] though this process is highly random.\n\n== Identification of the occurrence of premature convergence ==\nIt is hard to determine when premature convergence has occurred, and it is equally hard to predict its presence in the future.<ref name=\":1\" /><ref name=\":0\" /> One measure is to use the difference between the average and maximum fitness values, as used by Patnaik & Srinivas, to then vary the crossover and mutation probabilities.<ref><small>Patnaik, L.M. & Srinivas, M. (1994). Adaptive probabilities of crossover and mutation in genetic algorithms. ''IEEE Trans. Syst. Man Cybern.'', vol. 24, pp. 656-667.</small></ref> [[Population diversity]] is another measure which has been extensively used in studies to measure premature convergence. However, although it has been widely accepted that a decrease in the population diversity directly leads to premature convergence, there have been little studies done on the analysis of population diversity. In other words, by using the term population diversity, the argument for a study in preventing premature convergence lacks robustness, unless specified what their definition of population diversity is.<ref name=\":2\"><small>Günter, R. (2001). Self-adaptation may lead to premature convergence , ''Fachbereich Informatik, LS XI,'' Universität Dortmund, pp. 1 - 13.</small></ref> \n\n== Causes for premature convergence ==\nThere are a number of presumed or hypothesized causes for the occurrence of premature convergence.\n\n=== Self-adaptive mutations ===\nRechenberg introduced the idea of self-adaptation of mutation distributions in [[Evolution strategy|evolutionary strategies]].<ref><small>Rechenberg, I. (1973). Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. ''Frommann-Holzboog Verlag'', Stuttgart.</small> </ref> According to Rechenberg, the control parameters for these mutation distributions evolved internally through self-adaptation, rather than predetermination. He called it the ''1/5-success rule of evolutionary strategies'' (1 + 1)-ES: The step size control parameter would be increased by some factor if the relative frequency of positive mutations through a determined period of time is larger than 1/5, vice versa if it is smaller than 1/5. Self-adaptive mutations may very well be one of the causes for premature convergence.<ref name=\":2\" /> Accurately locating of optima can be enhanced by self-adaptive mutation, as well as accelerating the search for this optima. This has been widely recognized, though the mechanism’s underpinnings of this have been poorly studied, as it is often unclear whether the optima is found locally or globally.<ref name=\":2\" /> Self-adaptive methods can cause global convergence to global optimum, provided that the selection methods used are using [[Genetic algorithm#Elitism|elitism]], as well as that the rule of self-adaptation doesn’t interfere with the mutation distribution, which has the property of ensuring a positive minimum probability when hitting a random subset.<ref><small>Rudolph, G. (1999). ''Global convergence and self-adaptation: A counter-example. In Proceedings of the 1999 Congress of Evolutionary Computation (CEC 1999)''. IEEE Press, New Jersey, pp. 646–651.</small></ref> This is for non-convex objective functions with sets that include bounded lower levels of non-zero measurements. A study by Günter suggests that self-adaption mechanisms among elitist evolutionary strategies do resemble the 1/5-success rule, and could very well get caught by a local optimum that include a positive probability.<ref name=\":2\" /> \n\n==References==\n{{Reflist}}\n\n==See also==\n* [[Evolutionary computation]]\n* [[Evolution]]\n\n{{DEFAULTSORT:Premature Convergence}}\n[[Category:Evolutionary biology]]\n[[Category:Genetic algorithms]]\n\n\n{{evolution-stub}}"
    },
    {
      "title": "Quality control and genetic algorithms",
      "url": "https://en.wikipedia.org/wiki/Quality_control_and_genetic_algorithms",
      "text": "The combination of '''quality control and genetic algorithms''' led to novel solutions of complex [[quality control]] design and [[Optimization (mathematics)|optimization]] problems. [[Quality control]] is a process by which entities review the quality of all factors involved in production. Quality is the degree to which a set of inherent characteristics fulfils a need or expectation that is stated, general implied or obligatory.<ref>Hoyle D. ISO 9000 quality systems handbook. Butterworth-Heineman 2001;p.654</ref> [[Genetic algorithms]] are search algorithms, based on the mechanics of natural selection and natural genetics.<ref>Goldberg DE. Genetic algorithms in search, optimization and machine learning. Addison-Wesley 1989; p.1.</ref> \n\n==Quality control==\nAlternative [[quality control]]<ref>Duncan AJ. Quality control and industrial statistics. Irwin 1986;pp.1-1123.</ref> (QC) procedures can be applied on a process to [[Statistical hypothesis testing|test]] statistically the [[null hypothesis]], that the process conforms to the quality requirements, therefore that the process is in control, against the alternative, that the process is out of control. When a true [[null hypothesis]] is rejected, a statistical type I error is committed. We have then a false rejection of a run of the process. The probability of a type I error is called probability of false rejection. When a false null hypothesis is accepted, a statistical type II error is committed. We fail then to detect a significant change in the process. The probability of rejection of a false [[null hypothesis]] equals the probability of detection of the nonconformity of the process to the quality requirements.\n\nThe QC procedure to be designed or optimized can be formulated as:\n\n''Q''<sub>1</sub>(''n''<sub>1</sub>,'''''X''<sub>1</sub>''')# ''Q''<sub>2</sub>(''n''<sub>2</sub>,'''''X''<sub>2</sub>''') #...# ''Q''<sub>''q''</sub>(''n''<sub>''q''</sub>,'''''X''<sub>''q''</sub>''')  (1)\n\nwhere ''Q''<sub>''i''</sub>(''n''<sub>''i''</sub>,'''''X''<sub>''i''</sub>''') denotes a statistical decision rule, ''n<sub>i</sub>'' denotes the size of the sample '''S'''<sub>''i''</sub>, that is the number of the samples the rule is applied upon, and '''X'''<sub>''i''</sub> denotes the vector of the rule specific parameters, including the decision limits. Each symbol ''#'' denotes either the [[Boolean logic|Boolean]] operator AND or the operator OR. Obviously, for ''#'' denoting AND, and for ''n''<sub>1</sub> < ''n''<sub>2</sub> <...< ''n''<sub>''q''</sub>, that is for '''S'''<sub>1</sub> <math>\\subset</math> '''S'''<sub>2</sub> <math>\\subset</math> ....<math>\\subset</math> '''S'''<sub>''q''</sub>, the (1) denotes a ''q''-sampling QC procedure.\n\nEach statistical decision rule is evaluated by calculating the respective statistic of a monitored variable of samples taken from the process. Then, if the statistic is out of the interval between the decision limits, the decision rule is considered to be true. Many statistics can be used, including the following: a single value of the variable of a sample, the [[range (statistics)|range]], the [[mean]], and the [[standard deviation]] of the values of the variable of the samples, the cumulative sum, the smoothed mean, and the smoothed standard deviation. Finally, the QC procedure is evaluated as a Boolean proposition. If it is true, then the [[null hypothesis]] is considered to be false, the process is considered to be out of control, and the run is rejected.\n\nA [[quality control]] procedure is considered to be optimum when it minimizes (or maximizes) a context specific objective function. The objective function depends on the probabilities of detection of the nonconformity of the process and of false rejection. These probabilities depend on the parameters of the [[quality control]] procedure (1) and on the probability density functions (see [[probability density function]]) of the monitored variables of the process.\n\n==Genetic algorithms==\n[[Genetic algorithms]]<ref>Holland, JH. Adaptation in natural and artificial systems. The University of Michigan Press 1975;pp.1-228.</ref><ref>Goldberg DE. Genetic algorithms in search, optimization and machine learning. Addison-Wesley 1989; pp.1-412.</ref><ref>Mitchell M. An Introduction to genetic algorithms. The MIT Press 1998;pp.1-221.</ref> are robust search [[algorithms]], that do not require [[knowledge]] of the objective function to be optimized and search through large spaces quickly. [[Genetic algorithms]] have been derived from the processes of the [[molecular biology]] of the [[gene]] and the [[evolution]] of life. Their operators, cross-over, [[mutation]], and [[reproduction]], are [[isomorphic]] with the synonymous biological processes. [[Genetic algorithms]] have been used to solve a variety of complex [[Optimization (mathematics)|optimization]] problems. Additionally the classifier systems and the [[genetic programming]] [[paradigm]] have shown us that [[genetic algorithms]] can be used for tasks as complex as the program induction.\n\n==Quality control and genetic algorithms==\nIn general, we can not use algebraic methods to optimize the [[quality control]] procedures. Usage of [[enumerative]] methods would be very tedious, especially with multi-rule procedures, as the number of the points of the parameter space to be searched grows exponentially with the number of the parameters to be optimized. [[Optimization (mathematics)|Optimization]] methods based on the [[genetic algorithms]] offer an appealing alternative. \n\nFurthermore, the complexity of the design process of novel [[quality control]] procedures is obviously greater than the complexity of the [[Optimization (mathematics)|optimization]] of predefined ones.  \n\nIn fact, since 1993, [[genetic algorithms]] have been used successfully to optimize and to design novel [[quality control]] procedures.<ref> Hatjimihail AT. Genetic algorithms based design and [[Optimization (mathematics)|optimization]] of statistical quality control procedures. [[Clin Chem]] 1993;39:1972-8. [http://www.clinchem.org/cgi/reprint/39/9/1972]</ref><ref>Hatjimihail AT, Hatjimihail TT. Design of statistical quality control procedures using genetic algorithms. In LJ Eshelman (ed): Proceedings of the Sixth International Conference on Genetic Algorithms. [[San Francisco]]: [[Morgan Kaufmann]] 1995;551-7.</ref><ref>He D, Grigoryan A. Joint statistical design of double sampling x and s charts. European Journal of Operational Research 2006;168:122-142.</ref>\n\n==See also==\n*[[Quality control]]\n*[[Genetic algorithm]]\n*[[Optimization (mathematics)]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.asq.org/index.html American Society for Quality (ASQ)]\n* [http://illigal.org/ Illinois Genetic Algorithms Laboratory (IlliGAL)]\n* [https://www.hcsl.com Hellenic Complex Systems Laboratory (HCSL)]\n\n[[Category:Statistical process control]]\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Santa Fe Trail problem",
      "url": "https://en.wikipedia.org/wiki/Santa_Fe_Trail_problem",
      "text": "The '''Santa Fe Trail problem''' is a [[genetic programming]] exercise in which [[artificial ants]] search for food pellets according to a programmed set of instructions.<ref>Koza, John R., ''Genetic Programming: On the Programming of Computers by Means of Natural Selection''. MIT Press, Cambridge, MA.  1992.  pp. 147-155.  Print.</ref><ref>[http://www.cs.ucl.ac.uk/staff/ucacbbl/bloat_csrp-97-29/node2.html#SECTION00020000000000000000 The Artificial Ant Problem]</ref> The layout of food pellets in the Santa Fe Trail problem has become a standard for comparing different genetic programming algorithms and solutions.\n\nOne method for programming and testing algorithms on the Santa Fe Trail problem is by using the [[NetLogo]] application.<ref>[http://ccl.northwestern.edu/netlogo/ NetLogo]</ref> There is at least one case of a student creating a Lego robotic ant to solve the problem.<ref>[http://lslwww.epfl.ch/pages/research/papers/romero/ Romero's Pilgrimage to Santa Fe: A Tale of Robot Evolution]</ref>\n\n[[File:SantaFeTrail.gif|thumb|SantaFeTrail]]\n\n==See also==\n* [[Genetic programming]]\n* [[Agent-based model]]\n* [[Java Grammatical Evolution]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://genetic-programming.org/ Genetic-programming.org]\n* [http://grammatical-evolution.org/ Grammatical-evolution.org]\n* [http://www.lalena.com/AI/Ant/ Teamwork in genetic programming]\n* [https://archive.is/20110502154819/http://pages.bangor.ac.uk/~eep201/jge/index.html java Grammatical Evolution]\n\n[[Category:Genetic algorithms]]\n[[Category:Genetic programming]]"
    },
    {
      "title": "Schema (genetic algorithms)",
      "url": "https://en.wikipedia.org/wiki/Schema_%28genetic_algorithms%29",
      "text": "{{Evolutionary algorithms}}\nA '''schema''' is a template in [[computer science]] used in the field of [[genetic algorithm]]s that identifies a [[subset]] of strings with similarities at certain string positions. Schemata are a special case of [[cylinder set]]s; and so form a [[topological space]].<ref name=\"Holland1\">{{cite book |title=Adaptation in Natural and Artificial Systems|year=1992|edition=reprint|publisher=The MIT Press|author=Holland, John Henry |isbn=9780472084609 |url=https://books.google.com/books/about/Adaptation_in_natural_and_artificial_sys.html?id=JE5RAAAAMAAJ |deadurl=no |accessdate=22 April 2014}}</ref>\n\n== Description ==\nFor example, consider binary strings of length 6. The schema 1**0*1 describes the set of all words of length 6 with 1's at the first and sixth positions and a 0 at the fourth position. The * is a [[Wildcard character|wildcard]] symbol, which means that positions 2, 3 and 5 can have a value of either 1 or 0. The ''order of a schema'' is defined as the number of fixed positions in the template, while the ''[[defining length]]'' <math> \\delta(H) </math> is the distance between the first and last specific positions. The order of 1**0*1 is 3 and its defining length is 5. The ''fitness of a schema'' is the average fitness of all strings matching the schema. The fitness of a string is a measure of the value of the encoded problem solution, as computed by a problem-specific evaluation function.\n\n===Length===\nThe length of a schema <math>H</math>, called <math>N(H)</math>, is defined as the total number of nodes in the schema. <math>N(H)</math> is also equal to the number of nodes in the programs matching <math>H</math>.<ref name=\"UCL1\">{{cite web|title=Foundations of Genetic Programming|url=http://www.cs.ucl.ac.uk/staff/W.Langdon/FOGP/|publisher=UCL UK|accessdate=13 July 2010}}</ref>\n\n===Disruption===\nIf the child of an individual that matches schema H does not ''itself'' match H, the schema is said to have been ''disrupted''.<ref name=\"UCL1\" />\n\n==Propagation of schema==\nIn [[evolutionary computing]] such as [[genetic algorithms]] and [[genetic programming]], '''propagation''' refers to the inheritance of characteristics of one generation by the next. For example, a schema is propagated if individuals in the current generation match it and so do those in the next generation. Those in the next generation may be (but don't have to be) children of parents who matched it.\n\n== The Expansion and Compression Operators ==\nRecently schema have been studied using [[order theory]].<ref name = \"Fletcher\">\n{{cite arxiv |author=Jack McKay Fletcher and Thomas Wennkers |year=2017 |title=A natural approach to studying schema processing |eprint=1705.04536|class=cs.NE }}</ref>\n\nTwo basic operators are defined for schema: expansion and compression. The expansion maps a schema onto a set of words which it represents, while the compression maps a set of words on to a schema.\n\nIn the following definitions <math> \\Sigma </math> denotes an alphabet, <math> \\Sigma^l </math> denotes all words of length <math> l </math> over the alphabet <math> \\Sigma </math>, <math> \\Sigma_* </math> denotes the alphabet <math>\\Sigma</math> with the extra symbol <math>*</math>. <math>\\Sigma_*^l</math> denotes all schema of length <math> l </math> over the alphabet <math> \\Sigma_* </math> as well as the empty schema <math> \\epsilon_* </math>.\n \nFor any schema <math>s \\in \\Sigma^l_*</math>  the following operator <math>{\\uparrow}s</math>, called the <math>expansion</math> of <math>s</math>, which maps <math>s</math> to a subset of words in <math>\\Sigma^l </math>:\n\n<math display=\"block\">{\\uparrow}s := \\{b \\in \\Sigma^l | b_i = s_i \\mbox{ or } s_i = * \\mbox{ for each }  i \\in \\{1,...,l\\}\\} </math>\n\nWhere subscript <math>i</math> denotes the character at position <math>i</math> in a word or schema.  When <math> s= \\epsilon_*</math> then <math>{\\uparrow}s  = \\emptyset</math>.  More simply put, <math>{\\uparrow}s</math> is the set of all words in <math>\\Sigma^l</math> that can be made by exchanging the <math>*</math> symbols in <math>s</math> with symbols from <math>\\Sigma</math>. For example, if <math>\\Sigma=\\{0,1\\}</math>, <math>l=3</math> and <math>s=10*</math> then <math>{\\uparrow}s=\\{100,101\\} </math>.\n\nConversely, for any <math>A \\subseteq \\Sigma^l</math> we define <math>{\\downarrow}{A}</math>, called the <math>compression</math> of <math>A</math>,  which maps <math>A</math> on to a schema <math>s\\in \\Sigma_*^l</math>:\n<math display=\"block\">{\\downarrow}A:= s</math>\nwhere <math>s</math> is a schema of length <math>l</math> such that the symbol at position <math>i</math> in <math>s</math> is determined in the following way: if <math>x_i = y_i</math> for all <math>x,y \\in A</math> then <math>s_i = x_i</math> otherwise <math>s_i = *</math>. If <math>A = \\emptyset</math> then <math>{\\downarrow}A = \\epsilon_*</math>. One can think of this operator as stacking up all the items in <math>A</math> and if all elements in a column are equivalent, the symbol at that position in <math>s</math> takes this value, otherwise there is a wild card symbol. For example, let <math>A = \\{100,000,010\\}</math> then <math>{\\downarrow}A = **0</math>.\n\nSchemata can be [[partially ordered]]. For any <math>a,b \\in \\Sigma^l_*</math> we say <math>a \\leq b</math> if and only if <math>{\\uparrow}a \\subseteq {\\uparrow}b</math>. It follows that <math>\\leq</math> is a [[partial ordering]] on a set of schemata from the [[reflexive operator algebra|reflexivity]], [[antisymmetry]] and [[transitive relation|transitivity]] of the [[subset]] relation. For example, <math>\\epsilon_* \\leq 11 \\leq 1* \\leq **</math>.\nThis is because <math>{\\uparrow}\\epsilon_* \\subseteq {\\uparrow}11 \\subseteq {\\uparrow}1* \\subseteq {\\uparrow}** = \\emptyset \\subseteq \\{11\\} \\subseteq \\{11,10\\} \\subseteq \\{11,10,01,00\\}</math>.\n\nThe compression and expansion operators form a [[Galois connection]], where <math>\\downarrow</math> is the lower adjoint and <math>\\uparrow</math> the upper adjoint.<ref name=\"Fletcher\"/>\n\n== The Schematic Completion and The Schematic Lattice ==\nFor a set <math>A \\subseteq \\Sigma^l</math>, we call the process of calculating the compression on each subset of A, that is <math>\\{{\\downarrow}X | X \\subseteq A\\}</math>, the schematic completion of <math>A</math>, denoted <math>\\mathcal{S}(A)</math>.<ref name=\"Fletcher\"/>\n\nFor example, let <math>A = \\{110, 100, 001, 000\\}</math>. The schematic completion of <math>A</math>,  results in the following set: \n<math display=\"block\">\\mathcal{S}(A) =\n\n\\{001, 100, 000, 110, 00*, *00, 1*0, **0, *0*, ***, \\epsilon_*\\}</math>\n\nThe [[poset]] <math>(\\mathcal{S}(A),\\leq)</math> always forms a [[complete lattice]] called the schematic lattice. \n[[File:Schematic Lattice.png|thumb|The Schematic lattice formed from the schematic completion on the set <math>A=\\{111, 011, 001\\}</math>. Here the schematic lattice <math>(\\mathcal{S}(A),\\leq)</math> is shown as a [[Hasse diagram]]. \n]]\n\nThe schematic lattice is similar to the concept lattice found in [[Formal concept analysis]].\n{{clear}}\n\n==See also==\n*[[Holland's schema theorem]]\n*[[Formal concept analysis]]\n\n==References==\n{{Reflist}}\n\n[[Category:Genetic algorithms]]\n[[Category:Genetic programming]]"
    },
    {
      "title": "Selection (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Selection_%28genetic_algorithm%29",
      "text": "{{Evolutionary algorithms}}\n'''Selection''' is the stage of a [[genetic algorithm]] in which individual genomes are chosen from a population for later breeding (using the [[Crossover (genetic algorithm)|crossover operator]]).\n\nA generic selection procedure may be implemented as follows:\n#The [[fitness function]] is evaluated for each individual, providing fitness values, which are then normalized. Normalization means dividing the fitness value of each individual by the sum of all fitness values, so that the sum of all resulting fitness values equals 1.\n#The population is sorted by descending fitness values.\n#Accumulated normalized fitness values are computed:  the accumulated fitness value of an individual is the sum of its own fitness value plus the fitness values of all the previous individuals; the accumulated fitness of the last individual should be 1, otherwise something went wrong in the normalization step.\n#A random number ''R'' between 0 and 1 is chosen.\n#The selected individual is the last one whose accumulated normalized value is greater than or equal to ''R''.\n\nFor a large number of individuals the above algorithm might be computationally quite demanding. A simpler and faster alternative uses the so-called stochastic acceptance.\n\nIf this procedure is repeated until there are enough selected individuals, this selection method is called [[fitness proportionate selection]] or ''roulette-wheel selection''. If instead of a single pointer spun multiple times, there are multiple, equally spaced pointers on a wheel that is spun once, it is called [[stochastic universal sampling]].\nRepeatedly selecting the best individual of a randomly chosen subset is [[tournament selection]]. Taking the best half, third or another proportion of the individuals is [[truncation selection]].\n\nThere are other selection algorithms that do not consider all individuals for selection, but only those with a fitness value that is higher than a given (arbitrary) constant. Other algorithms select from a restricted pool where only a certain percentage of the individuals are allowed, based on fitness value.\n\nRetaining the best individuals in a generation unchanged in the next generation, is called ''elitism'' or ''elitist selection''. It is a successful (slight) variant of the general process of constructing a new population.\n\n==See also==\n*[[Fitness proportionate selection]]\n*[[Tournament selection]]\n*[[Stochastic universal sampling]]\n*[[Reward-based selection]]\n*[[Truncation selection]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.rennard.org/alife/english/gavintrgb.html Introduction to Genetic Algorithms]\n*[http://lipowski.home.amu.edu.pl/homepage/roulette.html An outline of implementation of the stochastic-acceptance version]\n[[Category:Genetic algorithms]]\n\n[[de:Evolutionärer Algorithmus#Selektionsstrategien]]\n[[nl:Genetisch algoritme#Selectie]]"
    },
    {
      "title": "Stochastic universal sampling",
      "url": "https://en.wikipedia.org/wiki/Stochastic_universal_sampling",
      "text": "[[Image:Statistically Uniform.png|thumb|270px|SUS example]]\n'''Stochastic universal sampling''' ('''SUS''') is a technique used in [[genetic algorithm]]s for selecting potentially useful solutions for recombination. It was introduced by James Baker.<ref name=\"baker\">\n{{Cite journal | last = Baker | first = James E. | title = Reducing Bias and Inefficiency in the Selection Algorithm | journal = Proceedings of the Second International Conference on Genetic Algorithms and their Application | pages = 14–21 | publisher = L. Erlbaum Associates | location = Hillsdale, New Jersey | year = 1987 }}</ref>\n\nSUS is a development of [[fitness proportionate selection]] (FPS) which exhibits no bias and minimal spread. Where FPS chooses several solutions from the population by repeated random sampling, SUS uses a single random value to sample all of the solutions by choosing them at '''evenly spaced intervals'''. This gives weaker members of the population (according to their fitness) a chance to be chosen. \n\nFPS can have bad performance when a member of the population has a really large fitness in comparison with other members. Using a comb-like ruler, SUS starts from a small random number, and chooses the next candidates from the rest of population remaining, not allowing the fittest members to saturate the candidate space.\n\nDescribed as an algorithm, pseudocode for SUS looks like:\n\n SUS('''Population''', '''N''')\n     '''F''' := total fitness of '''Population'''\n     '''N''' := number of offspring to keep\n     '''P''' := distance between the pointers ('''F'''/'''N''')\n     '''Start''' := random number between 0 and '''P'''\n     '''Pointers''' := ['''Start''' + '''i'''*'''P''' | '''i''' in [0..('''N'''-1)]]\n     return RWS('''Population''','''Pointers''')\n \n RWS('''Population''', '''Points''')\n     '''Keep''' = []\n     '''for P''' in '''Points'''\n         '''i''' := 0\n         '''while''' fitness sum of '''Population['''0..'''i]''' < '''P'''\n             '''i'''++\n         add '''Population[i]''' to '''Keep'''\n     return '''Keep'''\n\nWhere '''Population[0'''..'''i]''' is the set of individuals with array-index 0 to (and including) i.\n\nHere RWS() describes the bulk of fitness proportionate selection (also known as \"roulette wheel selection\") – in true fitness proportional selection the parameter '''Points''' is always a (sorted) list of random numbers from 0 to '''F'''. The algorithm above is intended to be illustrative rather than canonical.\n\n==See also==\n*[[Fitness proportionate selection]]\n*[[Reward-based selection]]\n\n==References==\n<references />\n\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Tournament selection",
      "url": "https://en.wikipedia.org/wiki/Tournament_selection",
      "text": "'''Tournament selection''' is a method of selecting an individual from a population of individuals in a [[genetic algorithm]].<ref name=miller-goldberg>{{cite journal|last1=Miller|first1=Brad|last2=Goldberg|first2=David|title=Genetic Algorithms, Tournament Selection, and the Effects of Noise|journal=Complex Systems|date=1995|volume=9|pages=193–212}}</ref> Tournament selection involves running several \"tournaments\" among a few individuals (or \"[[chromosome (genetic algorithm)|chromosome]]s\") chosen at random from the population.  The winner of each tournament (the one with the best fitness) is selected for [[Crossover (genetic algorithm)|crossover]].  ''Selection pressure'', a probabilistic measure of a chromosome's likelihood of participation in the tournament based on the participant selection pool size, is easily adjusted by changing the tournament size{{why|date=January 2018}}. If the tournament size is larger, weak individuals have a smaller chance to be selected, because, if a weak individual is selected to be in a tournament, there is a higher probability that a stronger individual is also in that tournament.\n\nThe tournament selection method may be described in pseudo code:\n\n choose k (the tournament size) individuals from the population at random\n choose the best individual from the tournament with probability p\n choose the second best individual with probability p*(1-p)\n choose the third best individual with probability p*((1-p)^2)\n and so on\n\nDeterministic tournament selection selects the best individual (when ''p'' = 1) in any tournament. A 1-way tournament (''k'' = 1) selection is equivalent to random selection. The chosen individual can be removed from the population that the selection is made from if desired, otherwise individuals can be selected more than once for the next generation. In comparison with the (stochastic) [[fitness proportionate selection]] method, tournament selection is often implemented in practice due to its lack of stochastic noise.<ref>{{cite journal|last1=Blickle|first1=Tobias|last2=Thiele|first2=Lothar|title=A Comparison of Selection Schemes Used in Evolutionary Algorithms|journal=Evolutionary Computation|date=December 1996|volume=4|issue=4|pages=361–394|doi=10.1162/evco.1996.4.4.361|citeseerx=10.1.1.15.9584}}</ref>\n\nTournament selection has several benefits over alternative selection methods for genetic algorithms (for example, fitness proportionate selection and [[reward-based selection]]): it is efficient to code, works on parallel architectures and allows the selection pressure to be easily adjusted.<ref name=miller-goldberg />  Tournament selection has also been shown to be independent of the scaling of the genetic algorithm [[fitness function]] (or '[[Loss function|objective function]]') in some classifier systems.<ref>{{cite book|last1=Miller|first1=edited by Erick Cant-Paz, James A. Foster, Kalyanmoy Deb, Lawrence David Davis, Rajkumar Roy, Una-May OReilly, Hans-Georg Beyer, Russell Standish, Graham Kendall, Stewart Wilson, Mark Harman, Joachim Wegener, Dipankar Dasgupta, Mitch A. Potter, Alan C. Schultz, Kathryn A. Dowsland, Natasha Jonoska, Julian|title=Genetic and Evolutionary Computation GECCO 2003 00 Genetic and Evolutionary Computation Conference Chicago, IL, USA, July 1216, 2003 Proceedings, Part II|date=2003|publisher=Springer-Verlag Berlin Heidelberg|location=Berlin|isbn=978-3-540-45110-5}}</ref><ref>{{cite journal|last1=Goldberg|first1=David|last2=Deb|first2=Kalyanmoy|title=A comparative analysis of selection schemes used in genetic algorithms|journal=Foundations of Genetic Algorithms|date=1991|pages=69–93}}</ref>\n\n==See also==\n*[[Fitness proportionate selection]]\n*[[Reward-based selection]]\n\n==References==\n{{reflist}}\n\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "Truncation selection",
      "url": "https://en.wikipedia.org/wiki/Truncation_selection",
      "text": "In animal and plant breeding, '''truncation selection''' is a standard method in [[selective breeding]] in selecting animals to be bred for the next generation. Animals are ranked by their phenotypic value on some trait such as milk production, and the top percentage is reproduced. The effects of truncation selection for a continuous trait can be modeled by the standard [[breeder's equation]]  by using [[heritability]] and [[truncated normal distribution]]s; on a binary trait, it can be modeled easily using the [[liability threshold model]]. It is considered an easy and efficient method of breeding.<ref>Crow & Kimura 1979, [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC382946/ \"Efficiency of truncation selection\"]</ref>\n\n== Computer science ==\nIn computer science, truncation selection is a [[selection (genetic algorithm)|selection method]] used in [[genetic algorithm]]s to select potential candidate solutions for recombination modeled after the breeding method.\n\nIn truncation selection the candidate solutions are ordered by fitness, and some proportion, ''p'', (e.g. ''p'' = 1/2, 1/3, etc.), of the fittest individuals are selected and reproduced 1/p times.  Truncation selection is less sophisticated than many other selection methods, and is not often used in practice.  It is used in Muhlenbein's [[Breeder Genetic Algorithm]].<ref>{{cite journal|\njournal=Evolutionary Computation|\nyear=1993|\ntitle=Predictive Models for the Breeder Genetic Algorithm|\nauthor=H Muhlenbein, D Schlierkamp-Voosen|\nurl=http://citeseer.comp.nus.edu.sg/rd/0,730860,1,0.25,Download/http:qSqqSqwww.ais.fraunhofer.deqSq%257EmuehlenqSqpublicationsqSqgmd_as_ga-93_01.ps}}</ref>\n\n== References ==\n{{reflist}}\n* [http://nitro.biosci.arizona.edu/zbook/NewVolume_2/pdf/WLChapter14.pdf \"Chapter 14: Short-term Changes in the Mean: 2. Truncation and Threshold Selection\"]\n* Crow 2010, [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2871814/ \"On epistasis: why it is unimportant in polygenic directional selection\"]\n* Visscher et al. 2008, [http://medicine.tums.ac.ir:803/Users/Javad_TavakoliBazzaz/Medical%20Genetics-2/Heritability%20in%20the%20genomics%20era.pdf \"Heritability in the genomics era - concepts and misconceptions\"]\n* Visscher 2016, [http://www.genetics.org/content/202/2/377 \"Human Complex Trait Genetics in the 21st Century\"]\n* Weight & Harpending 2016, [https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S002193201600002X \"Some Uses of Models of Quantitative Genetic Selection in Social Science\"]\n* Frost & Harpending 2015, [http://evp.sagepub.com/content/13/1/147470491501300114.full.pdf+html \"Western Europe, state formation, and genetic pacification\"]\n\n[[Category:Genetic algorithms]]\n[[Category:Animal breeding]]\n\n\n{{compu-AI-stub}}\n{{comp-sci-stub}}\n{{bioinformatics-stub}}"
    },
    {
      "title": "Weasel program",
      "url": "https://en.wikipedia.org/wiki/Weasel_program",
      "text": "[[File:Mustela frenata.jpg|thumb|right|The software's name takes itself from dialogue in ''Hamlet'':<br>''Hamlet:'' Do you see yonder cloud that's almost in shape of a camel?<br>''Polonius:'' By the mass, and 'tis like a camel, indeed.<br>''Hamlet:'' Methinks it is like a [[weasel]].]]\n\nThe '''weasel program''' or '''Dawkins' weasel''' is a [[thought experiment]] and a variety of [[computer simulation]]s illustrating it.  Their aim is to demonstrate that the process that drives [[evolution]]ary systems—random [[Mutation|variation]] combined with non-random cumulative [[Selection (biology)|selection]]—is different from pure [[chance (philosophy)|chance]].\n\nThe thought experiment was formulated by [[Richard Dawkins]], and the first simulation written by him; various other implementations of the program have been written by others.\n\n== Overview ==\nIn chapter 3 of his book ''[[The Blind Watchmaker]]'', Dawkins gave the following introduction to the program, referencing the well-known [[infinite monkey theorem]]:\n\n{{quote|I don't know who it was first pointed out that, given enough time, a [[monkey]] bashing away at [[Randomness|random]] on a [[typewriter]] could produce all the works of [[Shakespeare]]. The operative phrase is, of course, given enough time. Let us limit the task facing our monkey somewhat. Suppose that he has to produce, not the complete works of Shakespeare but just the short sentence 'Methinks it is like a [[weasel]]', and we shall make it relatively easy by giving him a typewriter with a restricted keyboard, one with just the 26 (capital) letters, and a space bar. How long will he take to write this one little sentence?}}\n\nThe scenario is staged to produce a string of [[gibberish]] letters, assuming that the selection of each letter in a sequence of 28 characters will be random. The number of possible combinations in this random sequence is 27<sup>28</sup>, or about 10<sup>40</sup>, so the [[probability]] that the monkey will produce a given sequence is extremely low. Any particular sequence of 28 characters could be selected as a \"target\" phrase, all equally as improbable as Dawkins's chosen target, \"METHINKS IT IS LIKE A WEASEL\".\n\nA [[computer program]] could be written to carry out the actions of Dawkins's [[Hypothesis|hypothetical]] monkey, continuously generating combinations of 26 letters and spaces at high speed. Even at the rate of millions of combinations per second, it is unlikely, even given the entire [[Age of the universe|lifetime of the universe]] to run, that the program would ever produce the phrase \"METHINKS IT IS LIKE A WEASEL\".<ref>For a string of 28 characters, with 27 possible characters (A-Z plus space), any randomly generated string has the probability one in 27^28 of being correct; that is approximately one in 10^40. If a program generating 10 million strings per second had been running since the start of the universe (around 14 billion years, or 10^17 seconds), it would have only generated around 10^24 strings by now.</ref>\n\nDawkins intends this example to illustrate a common misunderstanding of [[evolution]]ary change, i.e. that [[DNA]] sequences or [[organic compound]]s such as [[proteins]] are the result of atoms randomly combining to form more complex structures. In these types of computations, any sequence of [[amino acids]] in a protein will be extraordinarily improbable (this is known as [[Hoyle's fallacy]]). Rather, evolution proceeds by [[hill climbing]], as in [[adaptive landscape]]s.\n\nDawkins then goes on to show that a process of ''cumulative'' selection can take far fewer steps to reach any given target. In Dawkins's words:\n\n{{quote|We again use our computer monkey, but with a crucial difference in its program. It again begins by choosing a random sequence of 28 letters, just as before ... it duplicates it repeatedly, but with a certain chance of random [[error]] &ndash; 'mutation' &ndash; in the copying. The computer examines the [[mutant]] nonsense phrases, the 'progeny' of the original phrase, and chooses the one which,'' however slightly, ''most resembles the target phrase, METHINKS IT IS LIKE A WEASEL.}}\n\nBy repeating the procedure, a randomly generated sequence of 28 letters and spaces will be gradually changed each [[generation]]. The sequences progress through each generation:\n\n:Generation&nbsp;01:<tt>&nbsp;&nbsp;&nbsp;WDLTMNLT&nbsp;DTJBKWIRZREZLMQCO&nbsp;P</tt> <ref>Note: the 4th character of line 1 is missing in Dawkins' text; however line 2 suggests it was probably a T</ref>\n:Generation&nbsp;02:<tt>&nbsp;&nbsp;&nbsp;WDLTMNLT&nbsp;DTJBSWIRZREZLMQCO&nbsp;P</tt>\n:Generation&nbsp;10:<tt>&nbsp;&nbsp;&nbsp;MDLDMNLS&nbsp;ITJISWHRZREZ&nbsp;MECS&nbsp;P</tt>\n:Generation&nbsp;20:<tt>&nbsp;&nbsp;&nbsp;MELDINLS&nbsp;IT&nbsp;ISWPRKE&nbsp;Z&nbsp;WECSEL</tt>\n:Generation&nbsp;30:<tt>&nbsp;&nbsp;&nbsp;METHINGS&nbsp;IT&nbsp;ISWLIKE&nbsp;B&nbsp;WECSEL</tt>\n:Generation&nbsp;40:<tt>&nbsp;&nbsp;&nbsp;METHINKS&nbsp;IT&nbsp;IS&nbsp;LIKE&nbsp;I&nbsp;WEASEL</tt>\n:Generation&nbsp;43:<tt>&nbsp;&nbsp;&nbsp;METHINKS&nbsp;IT&nbsp;IS&nbsp;LIKE&nbsp;A&nbsp;WEASEL</tt>\n\nDawkins continues:\n\n{{quote|The exact time taken by the computer to reach the target doesn't matter. If you want to know, it completed the whole exercise for me, the first time, while I was out to lunch. It took about half an hour. (Computer enthusiasts may think this unduly slow. The reason is that the program was written in [[BASIC programming language|BASIC]], a sort of computer baby-talk. When I rewrote it in [[Pascal programming language|Pascal]], it took 11 seconds.) Computers are a bit faster at this kind of thing than monkeys, but the difference really isn't significant. What matters is the difference between the time taken by'' cumulative ''selection, and the time which the same computer, working flat out at the same rate, would take to reach the target phrase if it were forced to use the other procedure of'' single-step selection: ''about a million million million million million years. This is more than a million million million times as long as the universe has so far existed.}}\n\n==Implications for biology==\nThe program aims to demonstrate that the preservation of small changes in an evolving string of characters (or [[gene]]s) can produce meaningful combinations in a relatively short time as long as there is some mechanism to select cumulative changes, whether it is a person identifying which traits are desirable (in the case of artificial selection) or a criterion of survival (\"fitness\") imposed by the environment (in the case of natural selection). Reproducing systems tend to preserve traits across generations, because the offspring inherit a copy of the parent's traits. It is the differences between offspring, the variations in copying, which become the basis for selection, allowing phrases closer to the target to survive, and the remaining variants to \"die.\"\n\nDawkins discusses the issue of the mechanism of selection with respect to his \"biomorphs\" program:\n\n{{quote|The human eye has an active role to play in the story. It is the selecting agent. It surveys the litter of progeny and chooses one for breeding. ...Our model, in other words, is strictly a model of artificial selection, not natural selection. The criterion for 'success' is not the direct criterion of survival, as it is in true natural selection. In true natural selection, if a body has what it takes to survive, its genes automatically survive because they are inside it. So the genes that survive tend to be, automatically, those genes that confer on bodies the qualities that assist them to survive.}}\n\nRegarding the example's applicability to biological evolution, he is careful to point out that it has its limitations:\n\n{{quote|Although the monkey/Shakespeare model is useful for explaining the distinction between single-step selection and cumulative selection, it is misleading in important ways. One of these is that, in each generation of selective 'breeding', the mutant 'progeny' phrases were judged according to the criterion of resemblance to a'' distant ideal ''target, the phrase METHINKS IT IS LIKE A WEASEL. Life isn't like that. Evolution has no long-term goal. There is no long-distance target, no final perfection to serve as a criterion for selection, although human vanity cherishes the absurd notion that our species is the final goal of evolution. In real life, the criterion for selection is always short-term, either simple survival or, more generally, reproductive success.}}\n\n[[File:Dawkins-Weasel.png|thumb | A full run of a weasel program, with 100 offspring per generation, and a 5% mutation chance per character copied. Only the \"fittest\" string of each generation is shown. Note that, in generation 8, the 25th character, which had been correct (<code>A</code>), becomes incorrect (<code>I</code>). The program does not \"lock\" correct characters, rather it measures at each iteration the closeness of the complete string to the 'target' phrase.]]\n\n==More complex models==\nIn ''The Blind Watchmaker,'' Dawkins goes on to provide a graphical model of [[gene selection]] involving  entities he calls biomorphs. These are [[Plane (mathematics)|two-dimensional]] sets of [[Line (mathematics)#Line segment|line segments]] which bear relationships to each other, drawn under the control of \"genes\" that determine the appearance of the biomorph. By selecting entities from sequential generations of biomorphs, an experimenter can guide the evolution of the figures toward given shapes, such as \"airplane\" or \"octopus\" biomorphs.\n\nAs a simulation, the biomorphs are not much closer to the actual genetic behavior of biological organisms. Like the Weasel program, their development is shaped by an external factor, in this case the decisions of the experimenter who chooses which of many possible shapes will go forward into the following generation. They do however serve to illustrate the concept of \"genetic space,\" where each possible gene is treated as a [[dimension]], and the actual genomes of living organisms make up a tiny fraction of all possible gene combinations, most of which will not produce a viable organism. As Dawkins puts it, \"however many ways there may be of being alive, it is certain that there are vastly more ways of being dead\". \n\nIn ''Climbing Mount Improbable'', Dawkins responded to the limitations of the Weasel program by describing programs, written by other parties, that modeled the evolution of the [[spider web]].  He suggested that these programs were more realistic models of the evolutionary process, since they had no predetermined goal other than coming up with a web that caught more flies through a \"trial and error\" process.  Spiderwebs were seen as good topics for evolutionary modeling because they were simple examples of biosystems that were easily visualized; the modeling programs successfully generated a range of spider webs similar to those found in nature.\n\n== Example algorithm ==\n{{or section|date=August 2016}}\nAlthough Dawkins did not provide the source code for his program, a \"Weasel\" style algorithm could run as follows.\n\n# Start with a random string of 28 characters.\n# Make 100 copies of the string (''reproduce'').\n# For each character in each of the 100 copies, with a probability of 5%, replace (''mutate'') the character with a new random character.\n#  Compare each new string with the target string \"METHINKS IT IS LIKE A WEASEL\", and give each a score (the number of letters in the string that are correct and in the correct position).\n# If any of the new strings has a perfect score (28), halt. Otherwise, take the highest scoring string, and go to step 2.\n\nFor these purposes, a \"character\" is any uppercase letter, or a space. The number of copies per generation, and the chance of mutation per letter are not specified in Dawkins's book; 100 copies and a 5% mutation rate are examples. Correct letters are not \"locked\". Each correct letter may become incorrect in subsequent generations. The terms of the program and the existence of the target phrase do however mean that such 'negative mutations' will quickly be 'corrected'.\n\n== See also ==\n*[[Genetic algorithm]]\n*[[Objections to evolution]]\n*[[Watchmaker analogy]]\n\n== References ==\n*Dawkins, R. (1986) ''[[The Blind Watchmaker]]'' Oxford University Press.\n{{Reflist}}\n\n== External links ==\n* [http://rosettacode.org/wiki/Evolutionary_algorithm Many examples of Weasel programs in various computer languages]\n* [https://web.archive.org/web/20080514161817/http://home.pacbell.net/s-max/scott/weasel.html The Weasel Applet (the \"weasel program\" written in Java)]\n* [http://vlab.infotech.monash.edu.au/simulations/evolution/richard-dawkin-weasel/ Dawkin's Weasel demo applet] (in Monash University's Virtual Lab)\n* [http://www.talkorigins.org/indexcc/CF/CF011_1.html Talk.origins claim CF011_1 Dawkins' WEASEL simulation]\n* [https://s3.amazonaws.com/files.nice/weasel2.html An open sourced, HTML/Javascript web-based version by Damian Peterson]\n* [http://bytesizebio.net/index.php/2011/04/23/shakespeares-birthday-and-evolution/ An open sourced python script by Iddo Friedberg]\n\n{{Richard Dawkins}}\n{{Portal bar|Evolutionary biology}}\n\n{{DEFAULTSORT:Weasel Program}}\n[[Category:Artificial life]]\n[[Category:Genetic algorithms]]\n[[Category:Richard Dawkins]]\n[[Category:Simulation software]]\n[[Category:Thought experiments]]"
    },
    {
      "title": "Artificial immune system",
      "url": "https://en.wikipedia.org/wiki/Artificial_immune_system",
      "text": "In [[artificial intelligence]], '''artificial immune systems''' (AIS) are a class of computationally intelligent, [[rule-based machine learning]] systems inspired by the principles and processes of the vertebrate [[immune system]]. The algorithms are typically modeled after the immune system's characteristics of [[learning]] and [[memory]] for use in [[Problem solving|problem-solving]].\n\n==Definition==\nThe field of Artificial Immune Systems (AIS) is concerned with abstracting the structure and function of the [[immune system]] to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. AIS is a sub-field of [[Biologically-inspired computing]], and [[Natural computation]], with interests in [[Machine Learning]] and belonging to the broader field of [[Artificial Intelligence]].\n\n<blockquote>Artificial Immune Systems (AIS) are adaptive systems, inspired by theoretical immunology and observed immune functions, principles and models, which are applied to problem solving.<ref>\n{{cite book\n  | last = de Castro\n  | first = Leandro N.\n  |author2=Timmis, Jonathan\n  | title = Artificial Immune Systems: A New Computational Intelligence Approach\n  | publisher = [[Springer Science+Business Media|Springer]]\n  | year = 2002\n  | pages = 57–58\n  | isbn = 978-1-85233-594-6| title-link = Artificial Immune Systems: A New Computational Intelligence Approach\n  }}\n</ref></blockquote>\n\nAIS is distinct from [[computational immunology]] and [[theoretical biology]] that are concerned with simulating immunology using computational and mathematical models towards better understanding the immune system, although such models initiated the field of AIS and continue to provide a fertile ground for inspiration. Finally, the field of AIS is not concerned with the investigation of the immune system as a substrate for computation, unlike other fields such as [[DNA computing]].\n\n==History==\n\nAIS emerged in the mid 1980s with articles authored by Farmer, Packard and Perelson (1986) and Bersini and Varela (1990) on immune networks. However, it was only in the mid 1990s that AIS became a field in its own right. Forrest ''et al.'' (on [[negative selection (immunology)|negative selection]]) and Kephart ''et al.''<ref>\n{{cite conference\n | last = Kephart\n | first = J. O.\n | title = A biologically inspired immune system for computers\n | booktitle = Proceedings of Artificial Life IV: The Fourth International Workshop on the Synthesis and Simulation of Living Systems\n | year = 1994\n | pages = 130–139\n | publisher = MIT Press }}\n</ref> published their first papers on AIS in 1994, and Dasgupta conducted extensive studies on Negative Selection Algorithms. Hunt and Cooke started the works on Immune Network models in 1995; Timmis and Neal continued this work and made some improvements. De Castro & Von Zuben's and Nicosia & Cutello's work (on [[clonal selection]]) became notable in 2002. The first book on Artificial Immune Systems was edited by Dasgupta in 1999.\n\nCurrently, new ideas along AIS lines, such as [[danger theory]] and algorithms inspired by the [[innate immune system]], are also being explored. Although some believe that these new ideas do not yet offer any truly 'new' abstract, over and above existing AIS algorithms. This, however, is hotly debated, and the debate provides one of the main driving forces for AIS development at the moment. Other recent developments involve the exploration of [[degeneracy (biology)|degeneracy]] in AIS models,<ref name=\"Andrews and Timmis\">{{cite book|author=Andrews and Timmis|year=2006|title=A Computational Model of Degeneracy in a Lymph Node|url=|journal=Lecture Notes in Computer Science|volume=4163|issue=|pages=164–177|doi=10.1007/11823940_13|id=|isbn=978-3-540-37749-8}}<!--| accessdate = 2011-03-11 --></ref><ref name=\"Mendao et al.\">{{cite journal |  author = Mendao | title =  The Immune System in Pieces: Computational Lessons from Degeneracy in the Immune System |  journal = Foundations of Computational Intelligence (FOCI) | year = 2007 | volume = | issue = | pages = 394–400 | id = |display-authors=etal}}</ref> which is motivated by its hypothesized role in open ended learning and evolution.<ref name=\"Edelman and Gally\">{{cite journal|author=Edelman and Gally|year=2001|title=Degeneracy and complexity in biological systems|url=|journal=Proceedings of the National Academy of Sciences of the United States of America|volume=98|issue=24|pages=13763–13768|doi=10.1073/pnas.231499798|pmid=11698650|pmc=61115|id=|bibcode=2001PNAS...9813763E}}<!--| accessdate = 2011-03-11 --></ref><ref name=\"Whitacre\">{{cite journal |  author = Whitacre | title =  Degeneracy: a link between evolvability, robustness and complexity in biological systems |  journal = Theoretical Biology and Medical Modelling | year = 2010 | volume = 7 | issue = 6 | pages = 6| id = | doi=10.1186/1742-4682-7-6| pmid =  20167097 | pmc =  2830971 }}</ref>\n\nOriginally AIS set out to find efficient abstractions of processes found in the [[immune system]] but, more recently, it is becoming interested in modelling the biological processes and in applying immune algorithms to bioinformatics problems.\n\nIn 2008, Dasgupta and Nino <ref>\n{{cite book|title=Immunological Computation: Theory and Applications|last=Dasgupta|first=Dipankar|author2=Nino, Fernando|publisher=CRC Press|year=2008|isbn=978-1-4200-6545-9|location=|pages=296|doi=|booktitle=}}\n</ref> published a textbook on [[Immunological Computation]] which presents a compendium of up-to-date work related to immunity-based techniques and describes a wide variety of applications.\n\n==Techniques==\nThe common techniques are inspired by specific immunological theories that explain the function and behavior of the [[mammal]]ian [[adaptive immune system]].\n\n*[[Clonal Selection Algorithm]]: A class of algorithms inspired by the [[clonal selection]] theory of acquired immunity that explains how B and T [[lymphocyte]]s improve their response to [[antigens]] over time called [[affinity maturation]]. These algorithms focus on the [[Darwinism|Darwinian]] attributes of the theory where selection is inspired by the affinity of antigen-antibody interactions, reproduction is inspired by [[cell division]], and variation is inspired by [[somatic hypermutation]]. Clonal selection algorithms are most commonly applied to [[Optimization (mathematics)|optimization]] and [[pattern recognition]] domains, some of which resemble parallel [[hill climbing]] and the [[genetic algorithm]] without the recombination operator.<ref>\n{{cite journal\n | last = de Castro\n | first = L. N.\n |author2=Von Zuben, F. J.\n | title = Learning and Optimization Using the Clonal Selection Principle\n | journal = IEEE Transactions on Evolutionary Computation \n | volume = 6\n | issue = 3\n | year = 2002\n | pages = 239–251\n | url = ftp://ftp.dca.fee.unicamp.br/pub/docs/vonzuben/lnunes/ieee_tec01.pdf\n | doi=10.1109/tevc.2002.1011539}}\n</ref>\n\n*[[Negative Selection Algorithm]]: Inspired by the positive and negative selection processes that occur during the maturation of [[T cells]] in the [[thymus]] called [[Central tolerance|T cell tolerance]]. Negative selection refers to the identification and deletion ([[apoptosis]]) of self-reacting cells, that is T cells that may select for and attack self tissues. This class of algorithms are typically used for classification and pattern recognition problem domains where the problem space is modeled in the complement of available knowledge. For example, in the case of an [[anomaly detection]] domain the algorithm prepares a set of exemplar pattern detectors trained on normal (non-anomalous) patterns that model and detect unseen or anomalous patterns.<ref>\n{{cite conference\n  | last = Forrest\n  | first = S.\n  |author2= Perelson, A.S.|author3= Allen, L.|author4= Cherukuri, R.\n  | title = Self-nonself discrimination in a computer\n  | booktitle = Proceedings of the 1994 IEEE Symposium on Research in Security and Privacy\n  | place =  Los Alamitos, CA\n  | year = 1994\n  | pages = 202–212\n  | url = http://www.cs.unm.edu/~immsec/publications/virus.pdf\n  | format = PDF }}\n</ref>\n\n*[[Immune Network Algorithms]]: Algorithms inspired by the [[idiotypic network]] theory proposed by [[Niels Kaj Jerne]] that describes the regulation of the immune system by anti-idiotypic antibodies (antibodies that select for other antibodies). This class of algorithms focus on the network graph structures involved where antibodies (or antibody producing cells) represent the nodes and the training algorithm involves growing or pruning edges between the nodes based on affinity (similarity in the problems representation space). Immune network algorithms have been used in clustering, data visualization, control, and optimization domains, and share properties with [[artificial neural networks]].<ref>\n{{cite journal\n | last = Timmis\n | first = J.\n |author2= Neal, M.|author3= Hunt, J.\n | title = An artificial immune system for data analysis\n | journal = BioSystems\n | volume = 55\n | issue = 1\n | year = 2000\n | pages = 143–150\n | doi = 10.1016/S0303-2647(99)00092-1\n| pmid=10745118}}\n</ref>\n\n*[[Dendritic Cell Algorithms]]: The Dendritic Cell Algorithm (DCA) is an example of an immune inspired algorithm developed using a multi-scale approach. This algorithm is based on an abstract model of [[dendritic cells]] (DCs). The DCA is abstracted and implemented through a process of examining and modeling various aspects of DC function, from the molecular networks present within the cell to the behaviour exhibited by a population of cells as a whole. Within the DCA information is granulated at different layers, achieved through multi-scale processing.<ref>{{cite book\n | last = Greensmith\n | first = J.\n | author2 = Aickelin, U.\n | title = Artificial Dendritic Cells: Multi-faceted Perspectives\n | journal = Human-Centric Information Processing Through Granular Modelling\n | volume = 182\n | year = 2009\n | pages = 375–395\n | url = http://ima.ac.uk/papers/greensmith2009.pdf\n | doi = 10.1007/978-3-540-92916-1_16\n | citeseerx = 10.1.1.193.1544\n | series = Studies in Computational Intelligence\n | isbn = 978-3-540-92915-4\n | access-date = 2009-06-19\n | archive-url = https://web.archive.org/web/20110809185435/http://ima.ac.uk/papers/greensmith2009.pdf\n | archive-date = 2011-08-09\n | dead-url = yes\n }}</ref>\n\n==See also==\n{{Portal|Artificial intelligence}}\n*[[Biologically inspired computing]]\n*[[Computational immunology]]\n*[[Computational intelligence]]\n*[[Evolutionary computation]]\n*[[Immunocomputing]]\n*[[Natural computation]]\n*[[Swarm intelligence]]\n*[[Learning classifier system]]\n*[[Rule-based machine learning]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*J.D. Farmer, N. Packard and A. Perelson, (1986) \"The immune system, adaptation and machine learning\", Physica D, vol. 2, pp.&nbsp;187–204\n*H. Bersini, F.J. Varela, Hints for adaptive problem solving gleaned from immune networks. Parallel Problem Solving from Nature, First Workshop PPSW 1, Dortmund, FRG, October, 1990.\n*D. Dasgupta (Editor), Artificial Immune Systems and Their Applications, Springer-Verlag, Inc. Berlin, January 1999, {{ISBN|3-540-64390-7}}\n* V. Cutello and G. Nicosia (2002) \"An Immunological Approach to Combinatorial Optimization Problems\" Lecture Notes in Computer Science, Springer vol. 2527, pp.&nbsp;361–370.\n*L. N. de Castro and F. J. Von Zuben, (1999) \"Artificial Immune Systems: Part I -Basic Theory and Applications\", School of Computing and Electrical Engineering, State University of Campinas, Brazil, No. DCA-RT 01/99.\n*S. Garrett (2005) \"How Do We Evaluate Artificial Immune Systems?\" Evolutionary Computation, vol. 13, no. 2, pp.&nbsp;145–178. http://mitpress.mit.edu/journals/pdf/EVCO_13_2_145_0.pdf\n* V. Cutello, G. Nicosia, M. Pavone, J. Timmis (2007) An Immune Algorithm for Protein Structure Prediction on Lattice Models, IEEE Transactions on Evolutionary Computation, vol. 11, no. 1, pp.&nbsp;101–117. http://www.dmi.unict.it/nicosia/papers/journals/Nicosia-IEEE-TEVC07.pdf\n\n==External links==\n*[http://www.artificial-immune-systems.org AISWeb: The Online Home of Artificial Immune Systems] Information about AIS in general and links to a variety of resources including ICARIS conference series, code, teaching material and algorithm descriptions.\n*[https://web.archive.org/web/20051013060826/http://www.elec.york.ac.uk/ARTIST/ ARTIST: Network for Artificial Immune Systems] Provides information about the UK AIS network, ARTIST. It provides technical and financial support for AIS in the UK and beyond, and aims to promote AIS projects.\n*[http://www.cs.unm.edu/~immsec/ Computer Immune Systems] Group at the University of New Mexico led by [[Stephanie Forrest]].\n*[https://web.archive.org/web/20110509044906/http://ais.cs.memphis.edu/ AIS: Artificial Immune Systems] Group at the University of Memphis led by Dipankar Dasgupta.\n*[http://www.research.ibm.com/antivirus/ IBM Antivirus Research] Early work in AIS for computer security.\n\n{{DEFAULTSORT:Artificial Immune System}}\n[[Category:Artificial immune systems| ]]"
    },
    {
      "title": "Emma Hart (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Emma_Hart_%28computer_scientist%29",
      "text": "{{Infobox scientist\n| name = Emma Hart\n| image = \n| birth_name = \n| birth_date = {{Birth year and age|1967}}\n| birth_place = [[Middlesbrough]], [[England]]\n| death_date = <!-- {{death date and age|mf=yes|2001|07|19|1925|03|05}} -->\n| death_place = \n| residence = \n| nationality = [[United Kingdom|English]]\n| field = computer science\n| known_for = [[evolutionary computation|Evolutionary algorithms]], [[Optimisation (computer science)|optimisation]]\n| work_institutions = [[Edinburgh Napier University]]\n| alma_mater = [[University of Oxford]]<br />[[University of Edinburgh]]\n| doctoral_advisor = [[Peter Ross (computer scientist)|Peter Ross]]\n| awards = 2018, Bronze Award in International Human-Competitive Awards (Humies)\n| spouse = \n| thesis_title = Immunology as a metaphor for computational information processing: Fact or fiction?\n}}\n\n{{external media | width = 210px | align = right | headerimage=  | video1 = [https://www.contactengine.com/insiders-guide-ai-evolutionary-computation/ \"An Insider's Guide to Artificial Intelligence: Evolutionary Computation\"], Laura van Beers talks to Professor Emma Hart | video2 = [https://www.youtube.com/watch?v=PtNI3DYFOVA \"Emma Hart - Full Interview\"], Sentient Technologies, Aug 16, 2018 }}\n\n'''Professor Emma Hart''' (born 1967) is an English computer scientist known for her work in [[Artificial immune system|Artificial Immune Systems (AIS)]], [[evolutionary computation|evolutionary]] [[evolutionary computation|computation]] and [[Optimisation (computer science)|optimisation]]. She is a professor of [[computational intelligence]] at [[Edinburgh Napier University]], editor-in-chief of the ''[[Evolutionary Computation (journal)|Journal of Evolutionary Computation]]'' (MIT Press), and D. Coordinator of the Future & Emerging Technologies (FET) Proactive Initiative, Fundamentals of Collective Adaptive Systems. \n\n== Early life and education ==\nHart was born in [[Middlesbrough]], England in 1967.<ref name=\"Minervascientifica\">{{cite web |title=Emma Hart: Bio-inspired computing |url=http://minervascientifica.co.uk/emma-hart/ |website=Minerva Scientifica |publisher=[[National Library of Scotland]], [[British Society for the History of Science]] |accessdate=15 February 2019 |location=Edinburg, Scotland |date=6 November 2017}}</ref> In 1990 she graduated from the [[University of Oxford]] with a first class BA(Hons) in Chemistry. She then continued her studies at the [[University of Edinburgh]], graduating with an MSc in Artificial Intelligence in 1994, followed by a PhD that explored the use of immunology as an inspiration for computing, examining a range of techniques applied to optimization and data classification problems.<ref name=\"Ishibuchi2015\">{{cite book |editor-last=Ishibuchi |editor-first=Hisao |title=Computational Intelligence |url=https://books.google.com/books?id=lZhgCwAAQBAJ&pg=PA138 |volume=II |year=2015 |publisher=[[UNESCO]], [[Encyclopedia of Life Support Systems|ELOSS Publishers Ltd.]] |location=United Kingdom |isbn=978-1-78021-021-6 |pages=138}}</ref> Her disseration was titled ''Immunology as a metaphor for computational information processing: Fact or fiction?,''<ref name=\"Dissertation\">{{Cite book|last=Hart|first=Emma|date=2002|title=Immunology as a metaphor for computational information processing: fact or fiction? (Dissertation) |website=Edinburgh Research Archive |url=https://www.era.lib.ed.ac.uk/handle/1842/23042|language=en|publisher=The University of Edinburgh|pages=|via=}}</ref> and her doctoral advisor was [[Peter Ross (computer scientist)|Peter Ross]].\n\n== Career ==\nIn 2000 Hart took a position as a lecturer at [[Edinburgh Napier University]] , and was promoted to a [[Reader (academic rank)|Reader]], [[Professor]], and in 2008 Chair in [[Natural Computation]].<ref name=\"Ishibuchi2015\" /> She is now director of the [[Centre of Algorithms, Visualisation and Evolving Systems]] (CAVES) group in the School of Computing. She continues to research in the area of developing novel bio-inspired techniques for solving a range of real-world optimisation and classification problems<ref name=\"RobotEvolutionProject\">{{cite web |title=Autonomous robot evolution cradle to grave |url=https://www.napier.ac.uk/research-and-innovation/research-search/projects/autonomous-robot-evolution-cradle-to-grave |website=Edinburgh Napier University |language=English}}</ref>, as well as exploring  the fundamental properties of immune-inspired computing through modelling and simulation.<ref name=\"Ishibuchi2015\" /> She is also involved in editorial activity and currently occupies the position of Editor-in-Chief of the ''[[Evolutionary Computation (journal)|Journal of Evolutionary Computation]]'' (MIT Press).<ref name=\"MIT2017\">{{cite news |title=Welcome Emma Hart |url=https://mitpress.mit.edu/blog/welcome-emma-hart |accessdate=21 February 2019 |work=The MIT Press |date=13 January 2017}}</ref><ref>{{cite news |first1=Emma|last1= Hart |first2=Barry|last2= Gardiner |title=Storm Damage to Forests Costs Billions – Here's How AI Can Help |url=http://www.brinknews.com/storm-damage-to-forests-costs-billions-heres-how-ai-can-help/ |accessdate=15 February 2019 |agency=Marsh & McLennan Insights |newspaper=Brink News |date=29 May 2018 |location=Washington, D.C.}}</ref>\n\nHer interests lie in the area of [[bio-inspired computing]]''''',''''' in particular [[Artificial immune system|Artificial Immune Systems (AIS)]]''.'' She also undertakes research in three main areas:'' ''[[Optimisation (computer science)|optimisation]]'','' [[Self-organization|self-organising/self-adaptive systems]]'', ''and [[artificial intelligence]]''.''\n\nHart is D. Coordinator of Fundamentals of Collective Adaptive Systems (FoCAS), a Future and Emerging Technologies Proactive Initiative funded by the European Commission under FP7.<ref name=\"AboutFoCAS\">{{cite web |title=About FoCAS |url=http://www.focas.eu/about-focas/ |website=Fundamentals of Collective Adaptive Systems}}</ref>\n\n== Selected works ==\n\n=== Conference talks ===\n*{{cite web |last1=Hart |first1=Emma |title=Lifelong learning in optimization (video) |url=https://euro2016.euro-online.org/index.html%3Fp=781.html |website=28th European Conference on Operational Research |publisher=The Association of European Operational Research Societies}}\n\n=== Journal articles ===\n* \"An immune system approach to scheduling in changing environments\". E.Hart, P.Ross. 1999. ''Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation'' (2), 1559-1566.\n* \"Exploiting the analogy between immunology and sparse distributed memories: A system for clustering non-stationary data\". E.Hart, P.Ross. 2002. ''1st International Conference on Artificial Immune Systems''.  \n* \"Evolutionary scheduling: A review\". E Hart, P Ross, D Corne. 2005. ''Genetic Programming and Evolvable Machines'' 6(2), 191-220. DOI: https://doi.org/10.1007/s10710-005-7580-7 \n* \"Application areas of AIS: The past, the present and the future\". E.Hart, J.Timmis. 2008. ''Applied soft computing'' 8(1), 191-201. DOI: https://doi.org/10.1016/j.asoc.2006.12.004  \n* \"Structure versus function: a topological perspective on immune networks\". E.Hart, H.Bersini, F.Santos. 2010. ''Natural computing'' 9(3), 603-624. DOI: https://doi.org/10.1007/s11047-009-9138-8 \n* \"On the life-long learning capabilities of a nelli*: A hyper-heuristic optimisation system\". E.Hart, K.Sim. 2014. ''International Conference on Parallel Problem Solving from Nature'', 282-291. DOI: https://doi.org/10.1007/978-3-319-10762-2_28 \n* \"A hyper-heuristic ensemble method for static job-shop scheduling\". E.Hart, K.Sim. 2016. ''Evolutionary computation 24(4), 609-635. DOI:'' https://dx.doi.org/10.1162/EVCO_a_00183 \n\n== Awards and recognition ==\n* 2016, Featured article on Lifelong Learning in Optimisation, IFORS newsletter<ref name=\"IFORSFeatureArticle\">{{cite web |title=IFORS newsletter features article on Prof. Harts work on Lifelong Learning in Optimisation |url=https://www.napier.ac.uk/research-and-innovation/research-search/news/ifors-newsletter-features-article-on-prof-harts-work-on-lifelong-learning-in |website=Edinburgh Napier University |language=English |date=December 5, 2016}}</ref>\n* 2016, \"A Combined Generative and Selective Hyper-heuristic for the Vehicle Routing Problem\" presented at GECCO 2016 (Denver, USA), ACM<ref name=\"Gecco2016Paper\">{{cite web |title=Conference Success for members of Bio-Inspired Special Interest group |url=https://www.napier.ac.uk/research-and-innovation/research-search/news/conference-success-for-members-of-bioinspired-special-interest-group |website=Napier |language=English |date=July 18, 2016}}</ref>\n* 2016, \"A Hybrid Parameter Control Approach Applied to a Diversity-based Multi-objective Memetic Algorithm for Frequency Assignment Problems\" presented at WCCI 2016 (Vancouver, Canada), IEEE<ref name=\"Segredo\">{{cite journal |last1=Segredo |first1=E. |last2=Paechter |first2=B. |last3=Hart |first3=E. |last4=González-Vila |first4=C. I. |title=Hybrid parameter control approach applied to a diversity-based multi-objective memetic algorithm for frequency assignment problems |journal=2016 IEEE Congress on Evolutionary Computation (CEC), Vancouver, BC |date=2016 |pages=1517–1524 |doi=10.1109/CEC.2016.7743969 |url=https://ieeexplore.ieee.org/document/7743969/authors#authors |accessdate=22 February 2019|isbn=978-1-5090-0623-6 }}</ref>\n* 2017, Keynote Speaker, 2017 International Joint Conference on Computational Intelligence<ref name=\"IJCCI2017Keynote\">{{cite web |title=Prof. Emma Hart invited as a keynote speaker at IJCCI in Funchal, Madeira, November 2017 |url=https://www.napier.ac.uk/research-and-innovation/research-search/news/prof-emma-hart-invited-as-a-keynote-speaker-at-ijcci-in-funchal-madeira-november-2017 |website=Edinburgh Napier University |language=English |date=November 1, 2017}}</ref>\n* 2018, Bronze Award in International Human-Competitive Awards (Humies),  International Conference on Genetic and Evolutionary Computation, Kyoto Japan<ref name=\"Hummies2018\">{{cite web |title=Prof. Emma Hart and Dr Kevin Sim win Bronze Award in International Humies competition for work on predicting wind damage in Forestry |url=https://www.napier.ac.uk/research-and-innovation/research-search/news/prof-emma-hart-and-dr-kevin-sim-win-bronze-award-in-international-humies-competition-for |website=Edinburgh Napier University |date=July 19, 2018}}</ref><ref name=\"Langdon`\">{{cite journal |last1=Langdon |first1=W. B. |title=Human-Competitive awards 2018 |journal=ACM SIGEVOlution |date=2 January 2019 |volume=11 |issue=4 |pages=3–8 |doi=10.1145/3302542.3302543 }}</ref>\n* 2018, Nomination for best paper award, GECCO 18, Kyoto, Japan<ref name=\"Gecco18BestPaper\">{{cite web |title= Evolutionary Robotics Research Nominated for Best Paper Award |url=https://www.napier.ac.uk/research-and-innovation/research-search/news/evolutionary-robotics-research-nominated-for-best-paper-award |website=Edinburgh Napier University |language=English |date=July 15, 2018}}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Google scholar id}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Hart, Emma}}\n[[Category:Artificial immune systems]]\n[[Category:Artificial intelligence researchers]]\n[[Category:Evolutionary computation]]\n[[Category:Living people]]\n[[Category:People from Middlesbrough]]\n[[Category:English computer scientists]]\n[[Category:English women educators]]\n[[Category:20th-century English educators]]\n[[Category:English women scientists]]\n[[Category:21st-century English educators]]\n[[Category:English academics]]\n[[Category:British women academics]]\n[[Category:20th-century English scientists]]\n[[Category:21st-century English scientists]]\n[[Category:20th-century women scientists]]\n[[Category:21st-century women scientists]]\n[[Category:20th-century English women]]\n[[Category:21st-century English women]]\n[[Category:1967 births]]\n[[Category:British women computer scientists]]\n[[Category:Alumni of the University of Oxford]]\n[[Category:Alumni of the University of Edinburgh]]"
    },
    {
      "title": "Immunocomputing",
      "url": "https://en.wikipedia.org/wiki/Immunocomputing",
      "text": "{{multiple issues|\n{{essay-like|date=February 2016}}\n{{original research|date=February 2016}}\n{{unreferenced|date=February 2016}}\n{{update|date=February 2016}}\n{{weasel|date=February 2016}}\n}}\n'''Immunocomputing''' explores the principles of [[information processing]] that '''[[proteins]]''' and '''immune networks''' utilize in order to solve specific complex problems while protected from viruses, noise, errors and intrusions.\n\nIt intends to establish:\n\n* A proper mathematical framework\n* A new kind of [[computing]]\n* A new kind of hardware\n\nThe main difference with other kinds of computing lay on the function of its ''basic element'', the '''formal protein''', defined according with its biological prototype and its [[mathematical model]]. \n\nThe main [[biophysics|biophysical issues]] considered in immunocomputing are:\n\n* Free '''[[Protein folding|folding]]''' to a stable state (inspiration for the Formal Protein)\n* Free '''binding''' with other elements dependent on their reciprocal states (inspiration for the [[Immune network theory|Formal Immune Networks]])\n\n'''Formal immune networks''' (FINs) have as closest model the ''idiotypic network'' of ''[[Niels Kaj Jerne|N. Jerne]]'' but they consider specific mechanisms of [[Protein-protein interaction|interactions between proteins]]. FINs are able to learn, recognize and solve problems.\n\n[[Category:Artificial immune systems]]\n[[Category:Biophysics|*]]\n[[Category:Cognitive science]]"
    },
    {
      "title": "MIMIC (immunology)",
      "url": "https://en.wikipedia.org/wiki/MIMIC_%28immunology%29",
      "text": "{{about|the vaccine development tool||Mimic (disambiguation)}}\n'''MIMIC''', or '''modular immune in vitro construct''', is an artificial system imitating the human immune system.  It has applications in vaccine development.\n\nWhite blood cells, specifically peripheral blood mononuclear cells including [[T cells]] and [[B cells]], from human donors are placed in standard tubes containing specially designed tissue constructs made out of [[collagen]], where they develop into small but functioning immune systems. Up to ninety-six individual tubes can be carried on a plate the size of a deck of cards, allowing scientists to use cells from almost a hundred different donors at once.<ref name=\"TIME\">{{Citation\n  | last = Guthrie\n  | first = Catharine\n  | author-link =\n  | last2 =\n  | first2 =\n  | author2-link =\n  | title = Putting Immunity in a Test Tube\n  | newspaper = TIME\n  | pages =\n  | date = March 27, 2008\n  | url = http://www.time.com/time/health/article/0,8599,1725904,00.html\n  | accessdate = 2009-12-22\n}}</ref>\n\nThe MIMIC system replaces some steps in the vaccine development process that would otherwise be [[animal testing|performed on animals]] and offers scientists better speed and flexibility than traditional methods.  However, critics are concerned that MIMIC may be too simple for use as widespread as its developers hope.<ref name=\"TIME\"/>\n\nThe MIMIC system was developed by [[VaxDesign]] and became available for use in 2008.\n\n==References==\n{{Reflist}}\n\n[[Category:Artificial immune systems]]\n[[Category:Alternatives to animal testing]]"
    },
    {
      "title": "Cartesian genetic programming",
      "url": "https://en.wikipedia.org/wiki/Cartesian_genetic_programming",
      "text": "{{Underlinked|date=April 2019}}\n{{Evolutionary algorithms}}\n'''Cartesian genetic programming''' is a form of [[genetic programming]], which uses a graph representation to encode computer programs. It grew from a method of evolving digital circuits developed by Miller et al. in 1997.<ref>Miller, J.F., Thomson, P., Fogarty, T.C.: Designing Electronic Circuits Using Evolutionary Algorithms: Arithmetic Circuits: A Case Study. In: D. Quagliarella, J. Periaux, C. Poloni, G. Winter (eds.) Genetic Algorithms and Evolution Strategies in Engineering and Computer Science: Recent Advancements and Industrial Applications, pp. 105–131. Wiley (1998)</ref> However the term ‘Cartesian genetic programming’ first appeared in 1999<ref>Miller, J.F.: An Empirical Study of the Efficiency of Learning Boolean Functions using a Cartesian Genetic Programming Approach. In: Proc. Genetic and Evolutionary Computation Conference, pp. 1135–1142. Morgan Kaufmann (1999)</ref> and was proposed as a general form of genetic programming in 2000.<ref>Miller, J.F., Thomson, P.: Cartesian Genetic Programming. In: Proc. European Conference on Genetic Programming, LNCS, vol. 1802, pp. 121–132. Springer (2000)</ref> It is called ‘[[Cartesian coordinate system|Cartesian]]’ because it represents a program using a two-dimensional grid of nodes.\n\nJulian F. Miller (the inventor of CGP) has a web site<ref>{{Cite web|url=http://www.cartesiangp.com|title=CGP home|website=www.cartesiangp.com|access-date=2018-08-02}}</ref> which explains how CGP works. You can download many publications about CGP from this site. He also edited a book titled 'Cartesian Genetic Programming,<ref>{{Cite book|date=2011|editor-last=Miller|editor-first=Julian F.|title=Cartesian Genetic Programming|journal=Natural Computing Series|language=en-gb|doi=10.1007/978-3-642-17310-3|issn=1619-7127|isbn=978-3-642-17309-7|citeseerx=10.1.1.8.3777}}</ref> published in 2011 by Springer.\n\n== References ==\n{{reflist}}\n\n[[Category:Genetic programming]]\n\n\n{{Compu-prog-stub}}"
    },
    {
      "title": "Eurisko",
      "url": "https://en.wikipedia.org/wiki/Eurisko",
      "text": "{{Infobox software\n| name                   = Eurisko\n| title                  = \n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo size              = \n| logo alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot size        = \n| screenshot alt         = \n| collapsible            = \n| author                 = [[Douglas Lenat]]\n| developer              = \n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = \n| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| status                 = \n| programming language   = [[Representation Language Language|RLL-1]]\n| operating system       = \n| platform               = \n| size                   = \n| language               = \n| language count         = <!-- Number only -->\n| language footnote      = \n| genre                  = learning system\n| license                = \n| alexa                  = \n| website                = \n| standard               = \n| AsOf                   = \n}}{{Evolutionary algorithms}}\n\n'''Eurisko''' ([[greek language|Gr.]], ''I discover'') is a [[discovery system]] written by [[Douglas Lenat]] in [[Representation Language Language|RLL-1]], a representation language itself written in the [[Lisp programming language]]. A sequel to [[Automated Mathematician]], it consists of [[heuristics]], i.e. rules of thumb, including heuristics describing how to use and change its own heuristics.<ref>{{cite journal |last=Lenat |first=Douglas |authorlink=Douglas Lenat |year=1983 |title=EURISKO: A program that learns new heuristics and domain concepts |journal=Artificial Intelligence |volume=21 |issue=1–2 |pages=61–98 |doi=10.1016/s0004-3702(83)80005-8}}</ref><ref>{{cite book |title=Engines of Creation |last=Drexler |first=K. Eric |authorlink=K. Eric Drexler |publisher=[[Doubleday (publisher)|Doubleday]] |isbn=978-0-385-19973-5 |chapter=Thinking Machines (Chapter 5) |chapterurl=http://www.e-drexler.com/d/06/00/EOC/EOC_Chapter_5.html |quote=EURISKO ... is guided by heuristics ... in effect, various rules of thumb. |year=1986|title-link=Engines of Creation }}</ref> Lenat was frustrated by Automated Mathematician's constraint to a single domain and so developed Eurisko; his frustration with the effort of encoding domain knowledge for Eurisko led to Lenat's subsequent (and, {{As of|2014|lc=on}}, continuing) development of [[Cyc]]. Lenat envisions ultimately coupling the Cyc knowledgebase with the Eurisko discovery engine.\n\n== History ==\n\nDevelopment commenced at [[Carnegie Mellon]] in 1976 and continued at [[Stanford University]] in 1978 when Lenat returned to teach. \"For the first five years, nothing good came out of it\", Lenat said. But when the implementation was changed to a [[frame language]] based representation he called RLL ([[Representation Language Language]]), heuristic creation and modification became much simpler. Eurisko was then applied to a number of [[Field of study|domains]] with surprising success, including [[Very Large-Scale Integration|VLSI]] chip design.\n\nLenat and Eurisko gained notoriety by submitting the winning fleet (a large number of stationary, lightly-armored ships with many small weapons)<ref name=newyorker/> to the United States [[Traveller (role-playing game)#Rules system and gameplay|Traveller TCS]] national championship in 1981, forcing extensive changes to the game's rules. However, Eurisko won again in 1982 when the program discovered that the rules permitted the program to destroy its own ships, permitting it to continue to use much the same strategy.<ref name=newyorker>{{cite web|url=http://www.newyorker.com/reporting/2009/05/11/090511fa_fact_gladwell?currentPage=all|title=How underdogs can win|publisher=The New Yorker|author=Malcolm Gladwell|accessdate=2010-01-11|date=2009-05-11}}</ref> Tournament officials announced that if Eurisko won another championship the competition would be abolished; Lenat retired Eurisko from the game.<ref>{{cite journal |title=Eurisko, The Computer With A Mind Of Its Own |last=Johnson |first=George |year=1984 |journal=[http://aliciapatterson.org/stories/eurisko-computer-mind-its-own the APF Reporter] |volume=7 |issue=4 |publisher=The Alicia Patterson Foundation |location=Washington, D.C.}}</ref> The Traveller TCS wins brought Lenat to the attention of [[DARPA]],<ref>{{cite book |title = Understanding Computers: Artificial Intelligence |publisher = [[Time-Life Books]] |year= 1986 |location = Amsterdam |page=84 |isbn = 978-0-7054-0915-5 }}</ref> which has funded much of his subsequent work.\n\n==In popular culture==\nIn the first-season ''[[The X-Files]]'' episode \"[[Ghost in the Machine (The X-Files)|Ghost in the Machine]]\", Eurisko is the name of a fictional software company responsible for the episode's \"[[Villain of the week|monster of the week]]\", facilities management software known as \"Central Operating System\", or \"COS\". COS (described in the episode as an \"adaptive network\") is shown to be capable of learning when its designer arrives at Eurisko headquarters and is surprised to find that COS has given itself the ability to speak. The designer is forced to create a virus to destroy COS after COS commits a series of murders in an apparent effort to prevent its own destruction.\n\nLenat is also mentioned and Eurisko is discussed at the end of [[Richard Feynman]]'s Computer Heuristics Lecture as part of the Idiosyncratic Thinking Workshop Series.<ref>https://youtube.com/EKWGGDXe5MA?t=1h8m38s</ref>\n\n==Notes==\n{{reflist|colwidth=40em}}\n\n==References==\n*{{cite book |title = Understanding Computers: Artificial Intelligence |publisher = [[Time-Life Books]] |year= 1986 |location = Amsterdam |pages = 81–84 |isbn = 978-0-7054-0915-5 }}\n* {{cite journal |doi=10.1016/0004-3702(84)90016-X |last=Lenat |first=Douglas |authorlink=Douglas Lenat |author2=Brown, J.S. |year=1984 |title=Why AM and EURISKO appear to work |journal=Artificial Intelligence |volume=23 |issue=3 |pages=269–294 |url=http://www.aaai.org/Papers/AAAI/1983/AAAI83-059.pdf|citeseerx=10.1.1.565.8830 }}\n* {{cite paper |url=http://dspace.mit.edu/bitstream/handle/1721.1/14257/22713693.pdf?sequence=1 |format=PDF |title=Invention and exploration in discovery |last=Haase |first=Kenneth W |date=February 1990 |publisher=[[Massachusetts Institute of Technology]] |accessdate=2008-12-13 |archiveurl=https://web.archive.org/web/20050122170922/http://web.media.mit.edu/~haase/thesis/|archivedate=2005-01-22}}\n\n[[Category:Heuristics]]\n[[Category:Artificial intelligence applications]]\n[[Category:Genetic programming]]"
    },
    {
      "title": "Grammar induction",
      "url": "https://en.wikipedia.org/wiki/Grammar_induction",
      "text": "{{Machine learning bar}}\n'''Grammar induction''' (or '''grammatical inference'''<ref name=\"Grammatical Inference\">{{cite book|last=de la Higuera|first=Colin|title=Grammatical Inference: Learning Automata and Grammars|date=2010|publisher=Cambridge University Press|location=Cambridge|url=http://bootcamp.lif.univ-mrs.fr/de-la-higuera.pdf}}</ref>) is the process in [[machine learning]] of learning a [[formal grammar]] (usually as a collection of ''re-write rules'' or ''[[productions (computer science)|productions]]'' or alternatively as a [[finite state machine]] or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\n\n==Grammar classes==\n\nGrammatical inference has often been very focused on the problem of learning finite state machines of various types (see the article [[Induction of regular languages]] for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.\n\nSince the beginning of the century, these approaches have been extended to the problem of inference of [[context-free grammars]] and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.\nOther classes of grammars for which grammatical inference has been studied are contextual grammars and pattern languages.\n\n==Learning models==\n\nThe simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question: the aim is to learn the language from examples of it (and, rarely, from counter-examples, that is, example that do not belong to the language).\nHowever, other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin<ref>{{cite journal|author=Dana Angluin |title=Learning Regular Sets from Queries and Counter-Examples |journal=[[Information and Control]] |year=1987 |volume=75 |pages=87–106 |url=http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf |doi=10.1016/0890-5401(87)90052-6 |deadurl=yes |archiveurl=https://web.archive.org/web/20131202232143/http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf |archivedate=2013-12-02 |df= }}</ref>.\n\n==Methodologies==\nThere is a wide variety of methods for grammatical inference.  Two of the classic sources are {{Harvtxt|Fu|1977}} and {{Harvtxt|Fu|1982}}. {{Harvtxt|Duda|Hart|Stork|2001}} also devote a brief section to the problem, and cite a number of references.  The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of [[regular languages]] in particular, see ''[[Induction of regular languages]]''. A more recent textbook is de la Higuera (2010),<ref name = \"Grammatical Inference\"/> which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni<ref>D’Ulizia, A., Ferri, F., Grifoni, P. (2011) \"[https://www.academia.edu/download/41900378/A_survey_of_grammatical_inference_method20160202-5760-79hwcu.pdf A Survey of Grammatical Inference Methods for Natural Language Learning]{{Dead link|date=September 2018 |bot=InternetArchiveBot |fix-attempted=yes }}\", ''Artificial Intelligence Review'', Vol. 36, No. 1, pp. 1–27.</ref> provide a survey that explores grammatical inference methods for natural languages.\n\n===Grammatical inference by trial-and-error===\nThe method proposed in Section 8.7 of {{Harvtxt|Duda|Hart|Stork|2001}} suggests successively guessing grammar rules (productions) and testing them against positive and negative observations.  The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded.  This particular approach can be characterized as \"hypothesis testing\" and bears some similarity to Mitchel's [[version space]] algorithm. The {{Harvtxt|Duda|Hart|Stork|2001}} text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.\n\n=== Grammatical inference by genetic algorithms ===\nGrammatical induction using [[evolutionary algorithm]]s is the process of evolving a representation of the grammar of a target language through some evolutionary process. [[Formal grammar]]s can easily be represented as [[tree (data structure)|tree structures]] of production rules that can be subjected to evolutionary operators. [[Algorithm]]s of this sort stem from the [[genetic programming]] paradigm pioneered by [[John Koza]].{{Citation needed|date=August 2007}} Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the [[Extended Backus–Naur form|EBNF]] language made trees a more flexible approach.\n\nKoza represented [[Lisp (programming language)|Lisp]] programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the [[grammatical function|functions]] of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.\n\nIn the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a [[terminal symbol]] of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a [[noun phrase]] or a [[verb phrase]]) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.\n\n===Grammatical inference by greedy algorithms===\nLike all [[greedy algorithm]]s, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage.\nThe decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules.\nBecause there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.\n\nThese [[context-free grammar]] generating algorithms make the decision after every read symbol:\n* [[LZW|Lempel-Ziv-Welch algorithm]] creates a context-free grammar in a deterministic way such that it is necessary to store only the start rule of the generated grammar.\n* [[Sequitur algorithm|Sequitur]] and its modifications.\n\nThese context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:\n* [[Byte pair encoding]] and its optimizations.\n\n===Distributional learning===\nA more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning [[context-free grammars]] and [[mildly context-sensitive language]]s and have been proven to be correct and efficient for large subclasses of these grammars.<ref>Clark and Eyraud (2007) ''Journal of Machine Learning Research''; Ryo Yoshinaka (2011) ''Theoretical Computer Science''</ref>\n\n===Learning of [[Pattern language (formal languages)|pattern languages]]===\n\nAngluin defines a ''pattern'' to be \"a string of constant symbols from Σ and '''variable symbols''' from a disjoint set\".\nThe language of such a pattern is the set of all its nonempty ground instances  i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.<ref group=note>The language of a pattern with at least two occurrences of the same variable is not regular due to the [[Pumping lemma for regular languages|pumping lemma]].</ref>\nA pattern is called '''descriptive''' for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.\n\nAngluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable ''x''.<ref group=note>''x'' may occur several times, but no other variable ''y'' may occur</ref>\nTo this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on ''x'' being the only variable, the state count can be drastically reduced.<ref>{{cite journal| author=Dana Angluin| title=Finding Patterns Common to a Set of Strings| journal=Journal of Computer and System Sciences| year=1980| volume=21| pages=46–62| url=http://www.sciencedirect.com/science/article/pii/0022000080900410/pdf?md5=c3534f6c086df22fbf814b12984fab5e&pid=1-s2.0-0022000080900410-main.pdf| doi=10.1016/0022-0000(80)90041-0}}</ref>\n\nErlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.<ref>{{cite book|author1=T. Erlebach |author2=P. Rossmanith |author3=H. Stadtherr |author4=A. Steger |author4-link=Angelika Steger|author5=T. Zeugmann | chapter=Learning One-Variable Pattern Languages Very Efficiently on Average, in Parallel, and by Asking Queries| title=Proc. 8th International Workshop on Algorithmic Learning Theory — ALT'97| year=1997| volume=1316| pages=260–276| publisher=Springer|editor1=M. Li |editor2=A. Maruoka | series=LNAI}}</ref>\n\nArimura et al. show that a language class  obtained from limited unions of patterns can be learned in polynomial time.<ref>{{cite book|author1=Hiroki Arimura |author2=Takeshi Shinohara |author3=Setsuko Otsuki | chapter=Finding Minimal Generalizations for Unions of Pattern Languages and Its Application to Inductive Inference from Positive Data| title=Proc. STACS 11| year=1994| volume=775| pages=649–660| publisher=Springer| series=LNCS|url=http://ai2-s2-pdfs.s3.amazonaws.com/6a4c/0482e0030b0e5791cf75b0edd9f55fdfc10e.pdf}}{{dead link|date=February 2018}}</ref>\n\n===Pattern theory===\n[[Pattern theory]], formulated by [[Ulf Grenander]],<ref>Grenander, Ulf, and Michael I. Miller. ''[http://www.ulb.tu-darmstadt.de/tocs/185410162.pdf Pattern theory: from representation to inference]''.{{dead link|date=March 2018}} Vol. 1. Oxford: Oxford university press, 2007.</ref> is a mathematical [[Formalism (mathematics)|formalism]] to describe knowledge of the world as patterns. It differs from other approaches to [[artificial intelligence]] in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.\n\nIn addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:\n* Identify the [[Latent variable|hidden variables]] of a data set using real world data rather than artificial stimuli, which was commonplace at the time.\n* Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.\n* Study the randomness and variability of these graphs.\n* Create the basic classes of stochastic models applied by listing the deformations of the patterns.\n* Synthesize (sample) from the models, not just analyze signals with it.\nBroad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.\n\n== Applications ==\nThe principle of grammar induction has been applied to other aspects of [[natural language processing]], and has been applied (among many other problems) to [[semantic parsing]],<ref>Kwiatkowski, Tom, et al. \"[https://aclanthology.info/pdf/D/D11/D11-1140.pdf Lexical generalization in CCG grammar induction for semantic parsing].\" Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2011.</ref> [[natural language understanding]],<ref>Miller, Scott, et al. \"[http://www.aclweb.org/anthology/P94-1004 Hidden understanding models of natural language].\" Proceedings of the 32nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1994.</ref> [[example-based translation]],<ref>Brown, Ralf D. \"[https://pdfs.semanticscholar.org/c537/ac8bac9d83651e0ce6b37333034a5f572e39.pdf#page=5 Transfer-rule induction for example-based translation].\" Proceedings of the MT Summit VIII Workshop on Example-Based Machine Translation. 2001.</ref> [[morpheme]] analysis, and place name derivations.{{citation needed|date=February 2018}} Grammar induction has also been used for [[lossless data compression]]<ref>Cherniavsky, Neva, and Richard Ladner. \"[https://pdfs.semanticscholar.org/1be9/0a2f40d10acd17d5910eb21fb3b4a117d08b.pdf Grammar-based compression of DNA sequences].\" DIMACS Working Group on The Burrows-Wheeler Transform 21 (2004).</ref> and [[statistical inference]] via [[minimum message length]] (MML) and [[minimum description length]] (MDL) principles.{{citation needed|date=August 2017}} Grammar induction has also been used in some [[probabilistic models of language acquisition]].<ref>Chater, Nick, and Christopher D. Manning. \"[https://www.stanford.edu/class/linguist1/Rdgs/chater.pdf Probabilistic models of language processing and acquisition].\" Trends in cognitive sciences 10.7 (2006): 335-344.</ref>\n\n==See also==\n* [[Artificial grammar learning#Artificial intelligence]]\n* [[Example-based machine translation]]\n* [[Inductive programming]]\n* [[Kolmogorov complexity]]\n* [[Language identification in the limit]]\n* [[Straight-line grammar]]\n* [[Syntactic pattern recognition]]\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{Reflist}}\n\n==Sources==\n* {{Citation\n  | last=Duda | first=Richard O.| last2=Hart| first2=Peter E.\n  | last3=Stork| first3=David G.\n  | title=Pattern Classification | publisher=John Wiley & Sons\n  | place=[[New York City|New York]] | year=2001| edition=2\n  | url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html}}\n* {{Citation\n  | last=Fu | first=King Sun\n  | title=Syntactic Pattern Recognition and Applications\n  | publisher=Prentice-Hall | place=[[Englewood Cliffs, NJ]]\n  | year=1982}}\n* {{Citation\n  | last=Fu | first=King Sun\n  | title=Syntactic Pattern Recognition, Applications\n  | publisher=Springer-Verlag | place=[[Berlin]] | year=1977}}\n* {{Citation\n  | last=Horning | first=James Jay\n  | title=A Study of Grammatical Inference\n  | publisher=Stanford University Computer Science Department\n  | place=[[Stanford]] | year=1969 | edition=Ph.D. Thesis\n  | url=http://proquest.umi.com/pqdlink?Ver=1&Exp=05-16-2013&FMT=7&DID=757518381&RQT=309&attempt=1&cfc=1}}\n* {{Citation\n |last         = Gold\n |first        = E. Mark\n |title        = Language Identification in the Limit\n |url          = http://groups.lis.illinois.edu/amag/langev/paper/gold67limit.html\n |year         = 1967\n |volume       = 10\n |pages        = 447–474\n |publisher    = [[Information and Control]]\n |access-date  = 2016-09-04\n |archive-url  = https://web.archive.org/web/20160828171937/http://groups.lis.illinois.edu/amag/langev/paper/gold67limit.html\n |archive-date = 2016-08-28\n |dead-url     = yes\n |df           = \n}}\n* {{Citation\n  | last=Gold | first=E. Mark\n  | title=Language Identification in the Limit\n  | volume=10\n  | pages=447–474\n  | url=http://web.mit.edu/~6.863/www/spring2009/readings/gold67limit.pdf\n  | publisher=[[Information and Control]] | year=1967}}\n\n[[Category:Genetic programming]]\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Grammar]]\n[[Category:Inference]]\n[[Category:Machine learning]]"
    },
    {
      "title": "Linear genetic programming",
      "url": "https://en.wikipedia.org/wiki/Linear_genetic_programming",
      "text": "{{Evolutionary algorithms}}\n:''\"Linear genetic programming\" is unrelated to \"[[linear programming]]\".''\n\n'''Linear genetic programming''' (LGP) is a particular subset of [[genetic programming]] wherein [[computer programs]] in a population are represented as a sequence of [[Instruction (computer science)|instruction]]s from [[Imperative programming|imperative programming language]] or [[Machine code|machine language]]. The graph-based data flow that results from a multiple usage of [[Processor register|register]] contents and the existence of structurally noneffective code ([[introns]]) are two main differences of this [[genetic representation]] from the more common tree-based [[genetic programming]] (TGP) variant.<ref name=Brameier>Brameier, M.: \"[https://eldorado.uni-dortmund.de/handle/2003/20098 On linear genetic programming] {{webarchive|url=https://web.archive.org/web/20070629120053/https://eldorado.uni-dortmund.de/handle/2003/20098 |date=2007-06-29 }}\", Dortmund, 2003</ref><ref name=Introduction>W. Banzhaf, P. Nordin, R. Keller, F. Francone, \"Genetic Programming – An\nIntroduction. On the Automatic Evolution of Computer Programs and its Application\", Morgan Kaufmann, Heidelberg/San Francisco, 1998</ref><ref>{{cite book | author1=Poli, R.|author2= Langdon, W. B.|author3= McPhee, N. F. |year=2008 |title=A Field Guide to Genetic Programming | publisher=Lulu.com, freely available from the internet | isbn = 978-1-4092-0073-4}}</ref>\n\nIn [[genetic programming]] (GP) a '''linear tree''' is a program composed of a variable number of unary functions and a single [[leaf node|terminal]]. Note linear tree GP differs from bit string [[genetic algorithms]] since a population may contain programs of different lengths and there may be more than two types of functions or more than two types of terminals.<ref>\n[http://www.cs.ucl.ac.uk/staff/W.Langdon/FOGP/ Foundations of Genetic Programming].\n</ref>\n\n==Examples of LGP programs==\n\nBecause LGP programs are basically represented by a linear sequence of instructions, they are simpler to read and to operate on than their tree-based counterparts. For example, a simple program written in the LGP language [https://github.com/arturadib/slash-a Slash/A] looks like a series of instructions separated by a slash:\n<source lang=\"bash\">\ninput/   # gets an input from user and saves it to register F\n0/       # sets register I = 0\nsave/    # saves content of F into data vector D[I] (i.e. D[0] := F)\ninput/   # gets another input, saves to F\nadd/     # adds to F current data pointed to by I (i.e. F := F + D[0])\noutput/. # outputs result from F\n</source>\nBy representing such code in [[bytecode]] format, i.e. as an array of bytes each representing a different instruction, one can make [[Mutation (genetic algorithm)|mutation]] operations simply by changing an element of such an array.\n\n== See also ==\n* [[Multi expression programming]]\n* [[Cartesian genetic programming]]\n* [[Grammatical evolution]]\n* [[Genetic programming]]\n\n== Notes ==\n<references/>\n\n==External links==\n*[https://github.com/arturadib/slash-a Slash/A] A programming language and C++ library specifically designed for linear GP\n*[http://www.digitalbiology.net/ DigitalBiology.NET] Vertical search engine for GA/GP resources\n*[https://web.archive.org/web/20060816011453/http://www.aimlearning.com/ Discipulus] Genetic-Programming Software\n*[http://ugp3.sourceforge.net/ MicroGP] Genetic-Programming Software (open source)\n*[http://www.genetic-programming.org ]\n\n[[Category:Genetic programming]]"
    },
    {
      "title": "Joseph Nechvatal",
      "url": "https://en.wikipedia.org/wiki/Joseph_Nechvatal",
      "text": "{{Infobox artist\n| name          = Joseph Nechvatal\n| image         = Portrait_of_Joseph_Nechvatal,_2015.jpg\n| caption       = \n| birth_name     = \n| birth_date     = {{birth-date and age|15 January 1951}} \n| birth_place      = [[Chicago]], [[Illinois]]\n| death_date     = \n| death_place    = \n| nationality   =  [[Americans|American]]\n| field         = [[post-conceptual art]], [[digital art]], [[sound art]], [[art theory]], [[art criticism]]\n| training      = \n| movement      = \n| works         = \n| patrons       = \n| influenced by =\n| influenced    =\n| awards        = \n}}\n\n'''Joseph James Nechvatal''' (born 15 January 1951)<ref>{{cite web|url=http://the-artists.org/artist/joseph-nechvatal|title=Joseph Nechvatal - the artists}}</ref>  is a [[post-conceptual]] [[digital artist]] and [[Aesthetics|art theoretician]] who creates computer-assisted paintings and [[computer animation]]s, often using custom-created [[computer virus]]es.\n\n==Life and work==\n[[File:BOtv2002.jpg|thumb|260px|Joseph Nechvatal ''birth Of the viractual'' 2001 computer-robotic assisted acrylic on canvas]]\n\nJoseph Nechvatal was born in [[Chicago]]. He studied fine art and [[philosophy]] at [[Southern Illinois University Carbondale]], [[Cornell University]] and [[Columbia University]], where he studied with [[Arthur Danto]] while serving as the [[archivist]] to the [[Minimalism|minimalist]] composer [[La Monte Young]].<ref>{{cite web|url=http://www.brooklynrail.org/2012/03/books/flawed-composition|title=BIOGRAPHY Flawed Composition - The Brooklyn Rail|author=Joseph Nechvatal|publisher=}}</ref> From 1979, he exhibited his work in [[New York City]], primarily at [[Galerie Richard]], Brooke Alexander Gallery and [[Universal Concepts Unlimited]]. He has also solo exhibited in [[Berlin]],<ref>[http://www.art-in-berlin.de/ausstellungs-text.php?id=11027 ART LABORATORY BERLIN ''bOdy pandemOnium. Immersion into Noise'']</ref> [[Paris]], [[Chicago]], [[Cologne]], [[Atlanta]], [[Los Angeles]], [[Aalst, Belgium]], [[Youngstown]], [[Senouillac]], [[Lund]], [[Toulouse]], [[Turin]], [[Arles]] and [[Munich]].<ref name=\"Joseph Nechvatal\">{{cite web|url=http://www.artnet.com/artists/joseph-nechvatal/biography-links|title=Joseph Nechvatal|publisher=}}</ref>\n\nHis work in the early 1980s chiefly consisted of [[postminimalist]] gray graphite drawings that were often photomechanically enlarged.<ref>Milazzo & Collins 1990, pp. 3-7.</ref> During that period he was associated with the artist group [[Colab]] and helped establish the non-profit cultural space [[ABC No Rio]].<ref>Alan Moore and Marc Miller, eds. ''ABC No Rio Dinero: The Story of a Lower East Side Art Gallery'' New York: ABC No Rio with Collaborative Projects, 1985 with Joseph Nechvatal drawing on cover.</ref><ref>[http://98bowery.com/returntothebowery/abcnorio-the-book.php  ''ABC No Rio Dinero: The Story of a Lower East Side Art Gallery'' with Joseph Nechvatal drawing on cover]</ref> In 1983 he co-founded the [[avant-garde]] [[electronic art music]] audio project [[Tellus Audio Cassette Magazine]].<ref>McCormick 2005</ref> In 1984, Nechvatal began work on an opera called ''[[XS: The Opera Opus]]'' (1984-6)<ref>[[Rhys Chatham]], ''Die Donnergötter'' (LP, CD), Table of the Elements/Radium 2006, CD Book, p. 14</ref> with the [[no wave]] musical composer [[Rhys Chatham]].<ref>Sharp 1984, pp. 52-55.</ref>\n\nHe began using computers to make \"paintings\" in 1986 <ref>Joseph Nechvatal, ''Selected Writings''. Paris: Editions Antoine Candau, 1990</ref> and later, in his signature work, began to employ [[computer viruses]]. These \"collaborations\" with viral systems positioned his work as an early contribution to what is increasingly referred to as a [[post-human]] aesthetic.<ref>Popper 2007, pp. 120-123.</ref><ref>Lieser, Wolf. ''Digital Art''. Langenscheidt: h.f. ullmann. 2009 p. 87</ref>\n\nFrom 1991–1993 he was artist-in-residence at the [[Louis Pasteur]] [[wikt:Atelier|Atelier]] in [[Arbois]], France and at the [[La Saline Royale|Saline Royale]]/[[Claude Nicholas Ledoux|Ledoux]] Foundation's computer lab.There he worked on ''The Computer Virus Project'', which was an artistic experiment with [[computer viruses]] and computer [[animation]].<ref>Morgan 2006, pp. 75-76.</ref> He exhibited at [[Documenta]] 8 in 1987.<ref>Documenta GmbH., Museum Fridericianum Veranstaltungs GmbH, ''Volume 3 de Documenta 8: Kassel'' 1987</ref><ref>Nechvatal, J. 1987. ''Theoretical Statement Concerning Computer Robotic Paintings'', Documenta 8 Catalogue, Vol. 3</ref>\n\nIn 1999 Nechvatal obtained his [[Ph.D.]] in the philosophy of art and new technology concerning [[immersive virtual reality]] at [[Roy Ascott]]'s Centre for Advanced Inquiry in the Interactive Arts (CAiiA), University of Wales College, Newport, UK (now the [[Planetary Collegium]] at the [[University of Plymouth]]). There he developed his concept of '''viractualism''', a [[conceptual art]] idea that strives \"to create an interface between the biological and the technological.\"<ref name=\"Paul 2006, pp. 57-58\">Paul 2006, pp. 57-58.</ref> According to Nechvatal, this is a new topological space.<ref>See Joseph Nechvatal, ''Towards an Immersive Intelligence: Essays on the Work of Art in the Age of Computer Technology and Virtual Reality  (1993-2006)''. Edgewise Press. New York, N.Y. 2009</ref>\n\nIn 2002 he extended his experimentation into viral [[artificial life]] through a collaboration with the programmer Stephane Sikora of music2eye in a work called the ''Computer Virus Project II'',<ref>Liu 2004, pp. 331-336 & 485-486.</ref> inspired by the [[a-life]] work of [[John Horton Conway]] (particularly [[Conway's Game of Life]]), by the general [[cellular automata]] work of [[John von Neumann]], by the [[genetic programming]] [[algorithm]]s of [[John Koza]] and the [[auto-destructive art]] of [[Gustav Metzger]].<ref>{{cite web|url=http://www.artpool.hu/Fluxus/Metzger.html|title=Gustav Metzger (1926-)|publisher=}}</ref>\n\nIn 2005 he exhibited ''Computer Virus Project II'' works ([[digital painting]]s, [[digital print]]s, a [[digital audio]] installation and two ''live'' electronic virus-attack [[art installation]]s)<ref>{{YouTube|up29Rc-ksfM|Video on Joseph Nechvatal's Computer Virus Project 2.0}}</ref> in a solo show called ''cOntaminatiOns'' at [[Château de Linardié]] in [[Senouillac]], France. In 2006 Nechvatal received a retrospective exhibition entitled ''Contaminations'' at the [[Butler Institute of American Art]]'s Beecher Center for Arts and Technology.<ref name=\"Joseph Nechvatal\"/>\n\nDr. Nechvatal has also contributed to [[digital audio]] work with his [[noise music]] ''[[viral symphOny]]'', a collaborative sound symphony created by using his computer virus software at the Institute for Electronic Arts at [[Alfred University]].<ref>{{cite web|url=http://blogs.alfred.edu/iea/?page_id=159|title=Artist in Residence Archive|publisher=}}</ref><ref>{{cite web|url=http://sonhors.free.fr/kronik/Joseph_Nechvatal_viral_symphony.htm|title=Joseph Nechvatal :: Viral symphony :: IEA :: 2007|publisher=}}</ref> ''viral symphOny'' was presented as a part of ''nOise anusmOs'' in New York in 2012.<ref>{{cite journal|last=Morgan|first=Robert|title=Joseph Nechvatal: nOise anusmOs|journal=The Brooklyn Rail|date=June 2012|url=http://brooklynrail.org/2012/06/artseen/joseph-nechvatal-noise-anusmos}}</ref> In 2016, a limited edition CD recording of his sex farce poetry book ''Destroyer of Naivetés'' was released on Entr'acte label under the name of Cave Bacchus. Cave Bacchus is Nechvatal, [[Black Sifichi]] and [[Rhys Chatham]].<ref>[http://entracte.co.uk/projects/cave-bacchus-e207-/ Cave Bacchus: Joseph Nechvatal, Black Sifichi, Rhys Chatham ''Destroyer of Naivetés'' Entr’acte CD (E207)]</ref>\n\nIn 2013, Nechvatal showed work in ''Noise'', an official collateral show of the ''55th [[Venice Biennale]] of Art'', that was based on his book ''Immersion Into Noise''.<ref>[http://www.dearteassociazione.org/www.dearteassociazione.org/Testo_Catalogo_NOISE.html De Arte Association press statement]</ref>\n\nFrom 1999 to 2013, Nechvatal taught art theories of [[immersive virtual reality]] and the viractual at the [[School of Visual Arts]] in New York City (SVA). A book of his collected essays entitled ''Towards an Immersive Intelligence: Essays on the Work of Art in the Age of Computer Technology and Virtual Reality  (1993–2006)'' was published by Edgewise Press in 2009. Also in 2009, his book ''Immersive Ideals / Critical Distances'' was published.<ref>Nechvatal. J. ''Immersive Ideals / Critical Distances'', [[LAP Lambert Academic Publishing]] (July 7, 2009) {{ISBN|3-8383-0445-4}} / {{ISBN|978-3-8383-0445-8}}</ref> In 2011, his book ''Immersion Into Noise'' was published by [[Open Humanities Press]] in conjunction with the [[University of Michigan]] Library's Scholarly Publishing Office.<ref>[http://quod.lib.umich.edu/cgi/t/text/text-idx?c=ohp;idno=9618970.0001.001''Immersion Into Noise''] published by Open Humanities Press in conjunction with the [[University of Michigan]] Library's Scholarly Publishing Office. Ann Arbor. 2011.</ref> In 2014 he published (as editor) a book and CD/cassette tape with Punctum Books and Punctum Records on the [[noise music]] artist [[Minóy]] and in 2015 he published with Punctum Books a collection of his farcical erotic poetry entitled ''Destroyer of Naivetés''.<ref>[http://punctumbooks.com/category/titles/joseph-nechvatal/] Nechvatal's author page at Punctum Books</ref> Since 2013, Nechvatal has regularly been publishing his [[art criticism]] as the Paris correspondent for [[Hyperallergic]] blogazine.<ref>[http://hyperallergic.com/about/ [[Hyperallergic]] Regular Contributors: Joseph Nechvatal (Paris correspondent)]</ref>\n\n[[Joe Lewis (artist)|Joe Lewis]] wrote:\n\n{{quote|in the artist/theorist tradition of [[Robert Smithson]], Joseph Nechvatal is a pioneer in the field of digital image making who challenges our perceptions of nature by altering conventional notions of space and time, gender, and self. ... Nechvatal successfully plunged into the depths where art, technology and theory meet.<ref>Lewis 2003, pp.123-124.</ref>}}\n\n===Viractualism===\n'''Viractualism''' is an [[art theory]] concept developed by Nechvatal in 1999<ref>Christiane Paul, in her seminal book ''Digital Art'', discusses Nechvatal's concept of viractualism on page 58. One of the images she chooses to illustrate that section of the book is Nechvatal's painting entitled ''the birth Of the viractual'' (2001). Joe Lewis, in the March 2003 issue of [[Art in America]] (pp.123-124), discusses the viractual in his review ''Joseph Nechvatal at Universal Concepts Unlimited''.  John Reed in [[Artforum]] ''Web 3-2004 Critic's Picks'' discusses the concept in his piece ''#1 Joseph Nechvatal''. [[Frank Popper]] also writes about the viractual concept in his book ''From Technological to Virtual Art'' on page 122.</ref><ref name=\"scan.net.au\">[http://scan.net.au/scan/magazine/display.php?journal_id=56] ''Our Digital Noology: [[Catherine Perret]] in conversation with Joseph Nechvatal''</ref><ref>Viractualism is a [[conceptual art]] concept that indicates and initiates communions of the protoplasmic mass to [[Virtuality|virtual]] spatial conditions. [[Roy Ascott]], in his essay \"The Architecture of Cyberception\"*, has said, \"... to inhabit both the real and virtual worlds at one and the same time, and to be both here and potentially everywhere else at the same time is giving us a new sense of self, new ways of thinking and perceiving which extend what we have believed to be our natural, genetic capabilities.\"  Ascott, R. 1994. \"The Architecture of Cyberception\" In Leonardo Electronic Almanac, Vol. 2, No. 8, [[MIT Press]] Journals, August 1994</ref><ref name=\"ctheory.net\">{{cite web|url=http://www.ctheory.net/articles.aspx?id=330|title=CTheory.net|publisher=}}</ref>) from the Ph.D. research <ref>The title of the Ph.D. dissertation is \"Immersive Ideals / Critical Distances : A Study of the Affinity Between Artistic Ideologies Based in Virtual Reality and Previous Immersive Idioms\". A url introduction to the thesis, entitled \"Frame and Excess\", can be read on-line and the entire thesis downloaded in PDF at: [http://www.eyewithwings.net/nechvatal/ideals.htm]</ref> Nechvatal conducted in the philosophy of art and new technology concerning [[immersive virtual reality]] at [[Roy Ascott]]'s Centre for Advanced Inquiry in the Interactive Arts (CAiiA), University of Wales College, Newport, UK (now the [[Planetary Collegium]] at the [[University of Plymouth]]). There he developed his concept of the '''viractual''', which strives to create an interface between the biological and the virtual.<ref name=\"Paul 2006, pp. 57-58\"/> It is central to Nechvatal's work as an artist.<ref>{{YouTube|NSyN_wlvA0g|Joseph Nechvatal 2009 Video Interview}}</ref><ref>http://thefishpond.in/himanshudamle/2009/viral-art/ Viral Art: consciousness in concurrency with mutation</ref>\n\nNechvatal suggests that viractualism may be an entrainment/égréore conception helpful in defining our now third-fused inter-spatiality which is forged from the meeting of the virtual and the actual.<ref name=\"ctheory.net\"/> - a concept close to the military's [[augmented reality]], which is the use of transparent displays worn as see-through glasses on which computer data is projected and layered.<ref>[http://www.eyewithwings.net/nechvatal/ideals.htm Nechvatal's Ph.D. dissertation ''Immersive Ideals / Critical Distances: A Study of the Affinity Between Artistic Ideologies Based in Virtual Reality  and Previous Immersive Idioms'']</ref><ref>Concerning the viractual span of liminality in viractualism, Nechvatal refers to two very different, yet complimentary, concepts: entrainment and égréore.[[Entrainment (physics)|Entrainment]], in electro-physics, is the coupling of two or more [[oscillator]]s as they lock into a commonly sensed interacting frequency. In [[alchemical]] terms an égréore (an old form of the word agréger) is a third concept or phenomenon which is established from conjoining two different elements together.</ref>\n\nThe basis of the viractual conception is that virtual producing [[computer technology]] has become a noteworthy means for making and understanding [[contemporary art]] and that this brings artists to a place where one finds the emerging of the computed (the virtual) with the uncomputed corporeal (the actual).<ref>Joseph Nechvatal, \"Towards an Immersive Intelligence: Essays on the Work of Art in the Age of Computer Technology and Virtual Reality (1993-2006)\". Edgewise Press. 2009. pp. 53-58</ref> This amalgamate - which tends to contradict some central techno clichés of our time - is what Nechvatal calls '''the viractual'''.<ref name=\"ctheory.net\"/>\n[[Digitization]] is a key [[metaphor]] for viractuality in the sense that it is the elementary translating procedure today. Nechvatal thinks that in every era the attempt must be made anew to wrest the art practice away from conformisms that are about to overcome it.<ref>[http://www.eyewithwings.net/nechvatal/oberlin.html] Paper read at [[Oberlin College]] in application for The Henry Luce Professorship in the Emerging Arts Position (2000) titled ''The Emerging Arts Lecture at Oberlin''</ref><ref>[http://beehive.temporalimage.com/content_apps52/app_b.html] ''Viractualism defined at BeeHive'' Volume 5 : Issue 2  (12.2002)</ref>\n\n==Footnotes==\n{{reflist}}\n\n== Further reading ==\n* John Johnston, ''The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI'', MIT Press, 2008, cover\n* [[Donald Kuspit]], [http://www.artnet.com/magazineus/features/kuspit/kuspit8-5-05.asp ''The Matrix of Sensations'']  ''VI: Digital Artists and the New Creative Renaissance''\n* Joline Blais and [[Jon Ippolito]], ''The Edge of Art'', Thames & Hudson Ltd, p.&nbsp;213\n* [[Frank Popper]], ''From Technological to Virtual Art'', MIT Press, pp.&nbsp;120–123\n* [[Johanna Drucker]], [http://www.eyewithwings.net/nechvatal/drucker.html] ''Joseph Nechvatal : Critical Pleasure''\n* [[Robert C. Morgan]],  ''Voluptuary: An algorithic hermaphornology'', Tema Celeste Magazine, volume #93, p.&nbsp;94\n* Bruce Wands, ''Art of the Digital Age'', London: Thames & Hudson, p.&nbsp;65\n* [[Robert C. Morgan]], ''Laminations of the Soul'', Editions Antoine Candau, 1990, pp.&nbsp;23–30\n* [[Margot Lovejoy]], ''Digital Currents: Art in the Electronic Age'' Routledge 2004\n* Joseph Nechvatal, ''Immersive Excess in the Apse of [[Lascaux]]'', Technonoetic Arts 3, no3. 2005\n* Joseph Nechvatal. [http://quod.lib.umich.edu/cgi/t/text/text-idx?c=ohp;idno=9618970.0001.001''Immersion Into Noise'']. Open Humanities Press in conjunction with the [[University of Michigan]] Library's Scholarly Publishing Office. Ann Arbor. 2011\n* [[Johanna Drucker]], ''Joseph Nechvatal : Critical Pleasure'', Redaktion Frank Berndt, 1996, pp.&nbsp;10–13\n* Mario Costa, ''Phenomenology of New Tech Arts'', Artmedia, Salerno, 2005, p.&nbsp;6 & pp.&nbsp;36 – 38\n* [[Dominique Moulon]], ''L'art numerique: spectateur-acteuret vie artificielle'', ''Les images numeriques'' #47-48, 2004, pp.&nbsp;124–125\n* [[Christine Buci-Glucksmann]], ''L'art à l'époque virtuel'', in ''Frontières esthétiques de l'art'', Arts 8, Paris: L'Harmattan, 2004\n* Brandon Taylor, ''[[Collage]]'', Thames & Hudson Ltd, 2006, p.&nbsp;221\n* [[Dominique Moulon]], [http://www.moulon.net/conf3.htm] ''Conférence Report : Media Art in France'', Un Point d'Actu, L'Art Numerique, pp.&nbsp;124–125\n* [[Edmond Couchot]], ''Des Images, du temps et des machines'', édité Actes Sud, 2007, pp.&nbsp;263–264\n* [[Fred Forest]], ''Art et Internet'', Editions Cercle D'Art / Imaginaire Mode d'Emploi, pp.&nbsp;48 –51\n* Wayne Enstice & Melody Peters, ''Drawing: Space, Form, & Expression'', New Jersey: Prentice Hall, pp.&nbsp;312–313\n* Ellen K. Levy, ''Synthetic Lighting: Complex Simulations of Nature'', Photography Quarterly (#88) 2004, pp.&nbsp;7–9\n* Marie-Paule Nègre, ''Des artistes en leur monde'', volume 2, la Gazette de l'Hotel Drout, 2008, pp.&nbsp;82–83\n* Corrado Levi, ''È andata così: Cronaca e critica dell'arte 1970-2008'', ''Joseph Nechvatal intervistato nel suo studio a New York (1985–86)'', pp.&nbsp;130–135\n* [[Donald Kuspit]], ''Del Atre Analogico al Arte Digital'' in ''Arte Digital Y Videoarte'', Kuspit, D. ed., Consorcio del Circulo de Bellas Artes, Madrid, pp.&nbsp;33–34 & pp.&nbsp;210 – 212\n* [[Robert C. Morgan]], ''Nechvatal's Visionary Computer Virus'', in Gruson, L. ed. 1993. ''Joseph Nechvatal: Computer Virus Project'', [[Royal Saltworks at Arc-et-Senans]]: Fondation [[Claude-Nicolas Ledoux]], pp.&nbsp;8–15\n* Sarah J. Rogers (ed), ''Body Mécanique: Artistic Explorations of Digital Realms'', Columbus, Ohio, Wexner Center for the Arts, The Ohio State University\n*[[Edward A. Shanken]], ''Art and Electronic Media''. London: Phaidon, 2009. {{ISBN|978-0-7148-4782-5}}, pp.&nbsp;42, 285, 160\n\n== External links ==\n\n<!--==========================({{NoMoreLinks}})============================\n    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA  |\n    | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |\n    |                                                                     |\n    |           Excessive or inappropriate links WILL BE DELETED.         |\n    | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details.  |\n    |                                                                     |\n    | If there are already plentiful links, please propose additions or   |\n    | replacements on this article's discussion page, or submit your link |\n    | to the relevant category at the Open Directory Project (dmoz.org)   |\n    | and link back to that category using the {{dmoz}} template.         |\n    =========================({{NoMoreLinks}})=============================-->\n{{Wikiquote}}\n* [http://www.nechvatal.net Joseph Nechvatal's website]\n* [http://www.galerierichard.com Joseph Nechvatal's Paris and New York City Gallery]\n* [http://www.ubu.com/sound/nechvatal.html Examples of Joseph Nechvatal's noise music] at [[UbuWeb]]\n* [http://www.on-verge.org/conversations/interview-with-joseph-nechvatal-part-1/ Taney Roniger 2012 interview with Joseph Nechvatal on Verge part I]\n* [http://www.on-verge.org/conversations/interview-with-joseph-nechvatal-part-2/ Taney Roniger 2012 interview with Joseph Nechvatal on Verge part II]\n* [http://www.ctheory.net/articles.aspx?id=330 ''Viractualism'' defined at CTheory]\n* [http://www.computerfinearts.com/collection/nechvatal/redattack/ Example of ''red viral attack'' in the computer fine arts collection]\n* [http://www.biota.org/people/josephnechvatal/ Interview with Joseph Nechvatal]\n* [http://www.eyewithwings.net/nechvatal/ideals.htm Nechvatal's Ph.D. dissertation ''Immersive Ideals / Critical Distances' : A Study of the Affinity Between Artistic Ideologies Based in Virtual Reality  and Previous Immersive Idioms'']\n* {{YouTube|user=Nechvatal|title=Nechvatal}}\n* [http://www.ubu.com/sound/tellus.html ''Tellus Audio Cassette Magazine'' audio archive] at [[UbuWeb]]\n* [http://scan.net.au/scan/magazine/display.php?journal_id=56 ''Our Digital Noology]: [[Catherine Perret]] in conversation with Joseph Nechvatal''\n\n{{Authority control}}\n\n{{DEFAULTSORT:Nechvatal, Joseph}}\n[[Category:1951 births]]\n[[Category:Living people]]\n[[Category:20th-century American painters]]\n[[Category:American male painters]]\n[[Category:21st-century American painters]]\n[[Category:American academics]]\n[[Category:American conceptual artists]]\n[[Category:American digital artists]]\n[[Category:American expatriates in France]]\n[[Category:American experimental musicians]]\n[[Category:Artists from Chicago]]\n[[Category:Artists from New York (state)]]\n[[Category:Cellular automatists]]\n[[Category:Experimental composers]]\n[[Category:Genetic programming]]\n[[Category:American installation artists]]\n[[Category:Media theorists]]\n[[Category:New media artists]]\n[[Category:Noise musicians]]\n[[Category:Postmodern artists]]\n[[Category:Robotic art]]\n[[Category:Sound artists]]\n[[Category:Male classical composers]]\n[[Category:20th-century American printmakers]]\n[[Category:20th-century American composers]]"
    },
    {
      "title": "Parity benchmark",
      "url": "https://en.wikipedia.org/wiki/Parity_benchmark",
      "text": "{{Evolutionary algorithms}}\n{{multiple issues|\n{{notability|Neologisms|date=March 2014}}\n{{no footnotes|date=March 2014}}\n}}\n\nParity problems are widely used as [[Benchmark (computing)|benchmark]] problems in [[genetic programming]] but inherited from the [[artificial neural network]] community. Parity is calculated by summing all the binary inputs and reporting if the sum is odd or even. This is considered difficult because:\n#a very simple artificial neural network cannot solve it, and\n#all inputs need to be considered and a change to any one of them changes the answer.\n\n==References==\n* [http://www.cs.ucl.ac.uk/staff/W.Langdon/FOGP/ Foundations of Genetic Programming]\n\n{{DEFAULTSORT:Parity Benchmark}}\n[[Category:Genetic programming]]\n\n\n{{compu-AI-stub}}\n{{robotics-stub}}"
    },
    {
      "title": "Symbolic regression",
      "url": "https://en.wikipedia.org/wiki/Symbolic_regression",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Type of regression analysis}}\n'''Symbolic regression''' is a type of [[regression analysis]] that searches the space of mathematical expressions to find the model that best fits a given dataset, both in terms of accuracy and simplicity. No particular model is provided as a starting point to the algorithm. Instead, initial expressions are formed by randomly combining mathematical building blocks such as [[Operation (mathematics)|mathematical operators]], [[analytic function]]s, [[Constant (mathematics)|constants]], and [[state variable]]s. (Usually, a subset of these primitives will be specified by the person operating it, but that's not a requirement of the technique.) New equations are then formed by recombining previous equations, using [[genetic programming]].\n\nBy not requiring a specific model to be specified, symbolic regression isn't affected by human bias, or unknown gaps in [[domain knowledge]]. It attempts to uncover the intrinsic relationships of the dataset, by letting the patterns in the data itself reveal the appropriate models, rather than imposing a model structure that is deemed mathematically tractable from a human perspective. The [[fitness function]] that drives the evolution of the models takes into account not only [[Residual (numerical analysis)|error metrics]] (to ensure the models accurately predict the data), but also special complexity measures,<ref name=\"complexity\"/> thus ensuring that the resulting models reveal the data's underlying structure in a way that's understandable from a human perspective. This facilitates reasoning and favors the odds of getting insights about the data-generating system.\n\n== Difference from classical regression ==\n\nWhile conventional regression techniques seek to optimize the parameters for a pre-specified model structure, symbolic regression avoids imposing prior assumptions, and instead infers the model from the data. In other words, it attempts to discover both model structures and model parameters.\n\nThis approach has, of course, the disadvantage of having a much larger space to search — in fact, not only the search space in symbolic regression is infinite, but there are an infinite number of models which will perfectly fit a finite data set (provided that the model complexity isn't artificially limited). This means that it will possibly take a symbolic regression algorithm much longer to find an appropriate model and parametrization, than traditional regression techniques. This can be attenuated by limiting the set of building blocks provided to the algorithm, based on existing knowledge of the system that produced the data; but in the end, using symbolic regression is a decision that has to be balanced with how much is known about the underlying system.\n\nNevertheless, this characteristic of symbolic regression also has advantages: because the [[evolutionary algorithm]] requires diversity in order to effectively explore the search space, the end result is likely to be a selection of high-scoring models (and their corresponding set of parameters). Examining this collection could provide better insight into the underlying process, and allows the user to identify an approximation that better fits their needs in terms of accuracy and simplicity.\n\n== See also ==\n* [[Eureqa]], a symbolic regression engine\n* [[HeuristicLab]], a software environment for heuristic and evolutionary algorithms, including symbolic regression\n* [[Closed-form expression#Conversion from numerical forms|Closed-form expression § Conversion from numerical forms]]\n* [[Genetic programming]]\n* [[Kolmogorov complexity]]\n* [[Mathematical optimization]]\n* [[Regression analysis]]\n* [[Reverse mathematics]]\n\n== References ==\n{{reflist|refs=\n<ref name=\"complexity\">{{cite journal\n  | title     = Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming\n  | author1   = Ekaterina J. Vladislavleva\n  | author2   = Guido F. Smits\n  | author3   = Dick Den Hertog\n  | journal   = IEEE Transactions on Evolutionary Computation\n  | volume    = 13\n  | number    = 2\n  | pages     = 333–349\n  | year      = 2009\n  | url       = http://symbolicregression.com/sites/SRDocuments/NonlinearityPreprint.pdf\n  | doi=10.1109/tevc.2008.926486\n  }}</ref>\n}}\n\n== Further reading ==\n* {{cite conference\n  | title     = Genetic programming: An introduction and survey of applications\n  | author1   = Mark J. Willis\n  | author2   = Hugo G. Hiden\n  | author3   = Ben McKay\n  | author4   = Gary A. Montague\n  | author5   = Peter Marenbach\n  | booktitle = IEE Conference Publications\n  | number    = 446\n  | pages     = 314–319\n  | year      = 1997\n  | publisher = [[Institution of Electrical Engineers|IEE]]\n  | url       = http://www.cs.bham.ac.uk/~wbl/biblio/cache/cache/.hidden_13-jun_1525733794/http___www.staff.ncl.ac.uk_d.p.searson_docs_galesia97surveyofGP.pdf\n  }}\n* {{cite journal\n  | title     = Distilling free-form natural laws from experimental data\n  | author1   = Michael Schmidt\n  | author2   = Hod Lipson\n  | journal   = [[Science (journal)|Science]]\n  | volume    = 324\n  | number    = 5923\n  | pages     = 81–85\n  | year      = 2009\n  | url       = http://creativemachines.cornell.edu/sites/default/files/Science09_Schmidt.pdf\n  | doi=10.1126/science.1165893\n  | citeseerx   = 10.1.1.308.2245\n  }}\n* {{cite thesis\n  |degree    = M.Sc.\n  |author1   = Wouter Minnebo\n  |author2   = Sean Stijven\n  |year      = 2011\n  |title     = Empowering Knowledge Computing with Variable Selection\n  |chapter   = Chapter 4: Symbolic Regression\n  |publisher = [[University of Antwerp]]\n  |url       = https://community.alteryx.com/pvsmt99345/attachments/pvsmt99345/product-ideas/1300/1/ThesisWouterSean_v2.pdf\n  }}\n* {{cite conference\n  |title     = Performance improvement of machine learning via automatic discovery of facilitating functions as applied to a problem of symbolic system identification\n  |author1   = John R. Koza\n  |author2   = Martin A. Keane\n  |author3   = James P. Rice\n  |booktitle = IEEE International Conference on Neural Networks\n  |pages     = 191–198\n  |year      = 1993\n  |location  = San Francisco\n  |publisher = [[Institute of Electrical and Electronics Engineers|IEEE]]\n  |url       = http://www.genetic-programming.com/jkpdf/icnn1993impulse.pdf\n}}\n\n== External links ==\n\n* {{cite web |title = Symbolic regression — an overview |url = http://www.mafy.lut.fi/EcmiNL/older/ecmi35/node70.html |author = Ivan Zelinka |year = 2004 }}\n* {{cite web |title = Simple Symbolic Regression Using Genetic Programming |url = http://alphard.ethz.ch/gerber/approx/default.html |author = Hansueli Gerber |year = 1998 }} (Java applet) — approximates a function by evolving combinations of simple arithmetic operators, using algorithms developed by [[John Koza]].\n* {{cite web |title = Symbolic Regression: Function Discovery & More |url = http://www.symbolicregression.com |author = Katya Vladislavleva |archiveurl = https://web.archive.org/web/20141218105301/http://symbolicregression.com/ |archivedate = 2014-12-18}}\n* [https://cran.r-project.org/web/packages/rgp/index.html RGP], a Genetic Programming (GP) framework in [[R (programming language)|R]] that supports symbolic regression\n* [https://sites.google.com/site/gptips4matlab/ GPTIPS], a Genetic Programming and Symbolic Data Mining Platform for [[MATLAB]]\n* [https://github.com/darioizzo/dcgp dcgp], an open source symbolic regression toolbox.\n* [https://github.com/Ambrosys/glyph Glyph], a python 3 library based on deap providing abstraction layers for symbolic regression problems\n\n[[Category:Regression analysis]]\n[[Category:Genetic programming]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Bio-inspired computing",
      "url": "https://en.wikipedia.org/wiki/Bio-inspired_computing",
      "text": "{{distinguish|Computational biology}}\n{{cleanup|reason=This article has potential, but is currently mostly used as a coatrack for [[WP:REFSPAM]].|date=August 2016}}\n'''Bio-inspired computing''', short for '''biologically inspired computing''', is a field of study that loosely knits together subfields related to the topics of [[connectionism]], [[collective intelligence|social behaviour]] and [[emergence]]. It is often closely related to the field of [[artificial intelligence]], as many of its pursuits can be linked to [[machine learning]]. It relies heavily on the fields of [[biology]], [[computer science]] and [[mathematics]]. Briefly put, it is the use of computers to model the living phenomena, and simultaneously the study of life to improve the usage of computers. Biologically inspired computing is a major subset of [[natural computation]].\n\n== Areas of research ==\n\nSome areas of study encompassed under the canon of biologically inspired computing, and their biological counterparts:\n\n*[[genetic algorithm]]s ↔ [[evolution]]\n*[[biodegradability prediction]] ↔ [[biodegradation]]\n*[[cellular automata]] ↔ [[life]]\n*[[emergence|emergent systems]] ↔ [[ant]]s, [[termite]]s, [[bee]]s, [[wasp]]s\n*[[neural networks]] ↔ the [[brain]]\n*[[artificial life]] ↔ [[life]]\n*[[artificial immune system]]s ↔ [[immune system]]\n*[[rendering (computer graphics)]] ↔ patterning and rendering of animal skins, bird feathers, mollusk shells and bacterial colonies\n*[[Lindenmayer systems]] ↔ plant structures\n*[[communication networks]] and [[communication protocols|protocols]] ↔ epidemiology and the spread of disease\n*[[P system|membrane computers]] ↔ intra-[[Cell membrane|membrane]] [[Molecular biology|molecular]] processes in the [[Cell (biology)|living cell]]\n*[[Excitable medium|excitable media]] ↔ [[Wildfire|forest fires]], [[Audience wave|\"the wave\"]], [[Tachycardia|heart conditions]], [[axon]]s, etc.\n*[[sensor networks]] ↔ [[sensory organs]]\n*[[learning classifier system]]s ↔ [[cognition]], [[evolution]]\n\n== Artificial intelligence ==\n\nThe way in which bio-inspired computing differs from the traditional artificial intelligence (AI) is in how it takes a more evolutionary approach to learning, as opposed to what could be described as '[[creationist]]' methods used in traditional AI. In traditional AI, intelligence is often programmed from above: the programmer is the creator, and makes something and imbues it with its intelligence. Bio-inspired computing, on the other hand, takes a more [[Top-down and bottom-up design|bottom-up]], [[decentralisation|decentralised]] approach; bio-inspired techniques often involve the method of specifying a set of simple rules, a set of simple organisms which adhere to those rules, and a method of iteratively applying those rules. For example, training a virtual insect to navigate in an unknown terrain for finding food includes six simple rules. The insect is trained to \n* turn right for target-and-obstacle left; \n* turn left for target-and-obstacle right; \n* turn left for target-left-obstacle-right; \n* turn right for target-right-obstacle-left, \n* turn left for target-left without obstacle, \n* turn right for target right without obstacle. \nThe virtual insect controlled by the trained [[spiking neural network]] can find food after training in any unknown terrain.<ref name=\"Silvia_2013\">{{cite book | author = Xu Z |author2=Ziye X |author3=Craig H |author4=Silvia F | title = Spike-based indirect training of a spiking neural network-controlled virtual insect | journal = IEEE Decision and Control | pages = 6798–6805 |date=Dec 2013 | doi = 10.1109/CDC.2013.6760966 | isbn = 978-1-4673-5717-3 |citeseerx=10.1.1.671.6351 }}</ref> After several generations of rule application it is usually the case that some forms of complex behaviour arise. Complexity gets built upon complexity until the end result is something markedly complex, and quite often completely counterintuitive from what the original rules would be expected to produce (see [[complex system]]s). For this reason, in [[neural network model]]s, it is necessary to accurately model an ''in vivo'' network, by live collection of \"noise\" coefficients that can be used to refine statistical inference and extrapolation as system complexity increases.<ref>{{cite web|url=http://www.duke.edu/~jme17/Joshua_E._Mendoza-Elias/Research_Interests.html#Neuroscience_-_Neural_Plasticity_in|title=\"Smart Vaccines\" – The Shape of Things to Come|author=Joshua E. Mendoza|work=Research Interests|archiveurl=https://web.archive.org/web/20121114233853/http://people.duke.edu/~jme17/Joshua_E._Mendoza-Elias/Research_Interests.html|archivedate=November 14, 2012}}</ref>\n\nNatural evolution is a good analogy to this method–the rules of evolution ([[Selection (biology)|selection]], [[Genetic recombination|recombination]]/reproduction, [[mutation]] and more recently [[transposition (genetics)|transposition]]) are in principle simple rules, yet over millions of years have produced remarkably complex organisms. A similar technique is used in [[genetic algorithm]]s.\n\n== Brain-inspired Computing ==\n\nBrain-inspired computing refers to computational models and methods that are mainly based on the mechanism of the brain, rather than completely imitating the brain. The goal is to enable the machine to realize various cognitive abilities and coordination mechanisms of human beings in a brain-inspired manner, and finally achieve or exceed Human intelligence level.\n\n=== The research status ===\n[[Artificial intelligence]] researchers are now aware of the benefits of learning from the brain information processing mechanism. And the progress of brain science and neuroscience also provides the necessary basis for artificial intelligence to learn from the brain information processing mechanism.Brain and neuroscience researchers are also trying to apply the understanding of brain information processing to a wider range of science field. The development of the discipline benefits from the push of information technology and smart technology and in turn brain and neuroscience will also inspire the next generation of the transformation of information technology.\n\n=== The influence of brain science on Brain-inspired computing ===\nAdvances in brain and neuroscience, especially with the help of new technologies and new equipment, support researchers to obtain multi-scale, multi-type biological evidence of the brain through different experimental methods, and are trying to reveal the structure of bio-intelligence from different aspects and functional basis. From the microscopic neurons, synaptic working mechanisms and their characteristics, to the mesoscopic network connection model, to the links in the macroscopic brain interval and their synergistic characteristics, the multi-scale structure and functional mechanisms of brains derived from these experimental and mechanistic studies will provide important inspiration for building a future brain-inspired computing model.<ref>徐波，刘成林，曾毅.类脑智能研究现状与发展思考[J].中国科学院院刊,2016,31(7):793-802.</ref>\n\n=== Brain-inspired chip ===\nBroadly speaking, brain-inspired chip refers to a chip designed with reference to the structure of human brain neurons and the cognitive mode of human brain. Obviously, the \"neuromorphic chip\" is a brain-inspired chip that focuses on the design of the chip structure with reference to the human brain neuron model and its tissue structure, which represents a major direction of brain-inspired chip research. Along with the rise and development of “brain plans” in various countries, a large number of research results on neuromorphic chips have emerged, which have received extensive international attention and are well known to the academic community and the industry. For example, EU-backed SpiNNaker and BrainScaleS, Stanford's Neurogrid, IBM's TrueNorth, and Qualcomm's Zeroth.\n\nTrueNorth is a brain-inspired chip that IBM has been developing for nearly 10 years. The US DARPA program has been funding IBM to develop pulsed neural network chips for intelligent processing since 2008. In 2011, IBM first developed two cognitive silicon prototypes by simulating brain structures that could learn and process information like the brain. Each neuron of a brain-inspired chip is cross-connected with massive parallelism. In 2014, IBM released a second-generation brain-inspired chip called \"TrueNorth.\" Compared with the first generation brain-inspired chips, the performance of the TrueNorth chip has increased dramatically, and the number of neurons has increased from 256 to 1 million; the number of programmable synapses has increased from 262,144 to 256 million; Subsynaptic operation with a total power consumption of 70&nbsp;mW and a power consumption of 20&nbsp;mW per square centimeter. At the same time, TrueNorth handles a nuclear volume of only 1/15 of the first generation of brain chips. At present, IBM has developed a prototype of a neuron computer that uses 16 TrueNorth chips with real-time video processing capabilities.<ref>{{cite web|url=http://www.eepw.com.cn/article/271641.htm|title=美国类脑芯片发展历程|website=www.eepw.com.cn}}</ref> The super-high indicators and excellence of the TrueNorth chip have caused a great stir in the academic world at the beginning of its release.\n\nIn 2012, the Institute of Computing Technology of the Chinese Academy of Sciences(CAS) and the French Inria collaborated to develop the first chip in the world to support the deep neural network processor architecture chip \"Cambrian\".<ref>Chen T, Du Z, Sun N, et al. Diannao: A small-footprint high throughput accelerator for ubiquitous machine-learning//ACM Sigplan Notices. New York: ACM, 2014, 49(4): 269-284</ref> The technology has won the best international conferences in the field of computer architecture, ASPLOS and MICRO, and its design method and performance have been recognized internationally. The chip can be used as an outstanding representative of the research direction of brain-inspired chips.\n\n=== The problem Brain-inspired Computing are facing ===\n*Unclear Brain mechanism cognition\nThe human brain is a product of evolution. Although its structure and information processing mechanism are constantly optimized, compromises in the evolution process are inevitable. The cranial nervous system is a multi-scale structure. There are still several important problems in the mechanism of information processing at each scale, such as the fine connection structure of neuron scales and the mechanism of brain-scale feedback. Therefore, even a comprehensive calculation of the number of neurons and synapses is only 1/1000 of the size of the human brain, and it is still very difficult to study at the current level of scientific research.<ref>Markram Henry , Muller Eilif , Ramaswamy Srikanth Reconstruction and simulation of neocortical microcircuitry [J].Cell, 2015, Vol.163 (2), pp.456-92PubMed</ref>\n\n* Unclear Brain-inspired computational models and algorithms\nIn the future research of cognitive brain computing model, it is necessary to model the brain information processing system based on multi-scale brain neural system data analysis results, construct a brain-inspired multi-scale neural network computing model, and simulate multi-modality of brain in multi-scale. Intelligent behavioral ability such as perception, self-learning and memory, and choice.Machine learning algorithms are not flexible and require high-quality sample data that is manually labeled on a large scale. Training models require a lot of computational overhead. Brain-inspired artificial intelligence still lacks advanced cognitive ability and inferential learning ability.\n\n* Constrained  Computational architecture and capabilities\t\nMost of the existing brain-inspired chips are still based on the research of von Neumann architecture, and most of the chip manufacturing materials are still using traditional semiconductor materials. The neural chip is only borrowing the most basic unit of brain information processing. The most basic computer system, such as storage and computational fusion, pulse discharge mechanism, the connection mechanism between neurons, etc., and the mechanism between different scale information processing units has not been integrated into the study of brain-inspired computing architecture. Now an important international trend is to develop neural computing components such as brain memristors, memory containers, and sensory sensors based on new materials such as nanometers, thus supporting the construction of more complex brain-inspired computing architectures. The development of brain-inspired computers and large-scale brain computing systems based on brain-inspired chip development also requires a corresponding software environment to support its wide application.\n\n== See also ==\n{{prose|date=December 2016}}\n* [[Applications of artificial intelligence]]\n* [[Artificial life]]\n* [[Artificial neural network]]\n* [[Behavior based robotics]]\n* [[Bioinformatics]]\n* [[Bionics]]\n* [[Cognitive architecture]]\n* [[Cognitive modeling]]\n* [[Cognitive science]]\n* [[Connectionism]]\n* [[Digital morphogenesis]]\n* [[Digital organism]]\n* [[Evolutionary algorithm]]\n* [[Evolutionary computation]]\n* [[Fuzzy logic]]\n* [[Gene expression programming]]\n* [[Genetic algorithm]]\n* [[Genetic programming]]\n* [[Gerald Edelman]]\n* [[Janine Benyus]]\n* [[Learning classifier system]]\n* [[Mark A. O'Neill]]\n* [[Mathematical biology]]\n* [[Mathematical model]]\n* [[Natural computation]]\n* [[Neuroevolution]]\n* [[Olaf Sporns]]\n* [[Organic computing]]\n* [[Swarm intelligence]]\n\n; Lists\n* [[List of emerging technologies]]\n* [[Outline of artificial intelligence]]\n\n== References ==\n\n<references/>\n\n== Further reading ==\n\n''(the following are presented in ascending order of complexity and depth, with those new to the field suggested to start from the top)''\n\n* \"[http://www.cs.uvm.edu/~jbongard/papers/2009_IEEEComp_Bongard.pdf Biologically Inspired Computing]\"\n* \"[http://peterjbentley.com/ Digital Biology]\", Peter J. Bentley.\n* \"[https://web.archive.org/web/20060216011353/http://bic05.fsksm.utm.my/ First International Symposium on Biologically Inspired Computing]\"\n* ''[https://books.google.com/books?id=Au_tLkCwExQC Emergence: The Connected Lives of Ants, Brains, Cities and Software]'', Steven Johnson.\n* ''Dr. Dobb's Journal'', Apr-1991. (Issue theme: Biocomputing)\n* ''[https://books.google.com/books?id=K8P1rX8T4kYC Turtles, Termites and Traffic Jams]'', Mitchel Resnick.\n* ''Understanding Nonlinear Dynamics'', Daniel Kaplan and [[Leon Glass]].\n* {{cite journal | first1 = E. | last1= Ridge | first2 = D. | last2 = Kudenko | first3 = D. | last3 = Kazakov | first4 = E. |last4=Curry | title = Moving Nature-Inspired Algorithms to Parallel, Asynchronous and Decentralised Environments | citeseerx = 10.1.1.64.3403 | journal = Self-Organization and Autonomic Informatics (I) | year = 2005 | volume = 135 | pages = 35–49 }}\n*''Swarms and Swarm Intelligence'' by Michael G. Hinchey, Roy Sterritt, and Chris Rouff,\n* ''[https://books.google.com/books?id=2wTOBQAAQBAJ Fundamentals of Natural Computing: Basic Concepts, Algorithms, and Applications]'', L. N. de Castro, Chapman & Hall/CRC, June 2006.\n* \"[http://mitpress.mit.edu/books/FLAOH/cbnhtml/home.html The Computational Beauty of Nature]\", [http://flakenstein.net/ Gary William Flake]. MIT Press. 1998, hardcover ed.; 2000, paperback ed. An in-depth discussion of many of the topics and underlying themes of bio-inspired computing.\n* Kevin M. Passino, [https://books.google.com/books?id=7ttpWS75Uo0C Biomimicry for Optimization, Control, and Automation], Springer-Verlag, London, UK, 2005.\n* ''[https://books.google.com/books?id=s_Q5YZ2nh2kC Recent Developments in Biologically Inspired Computing]'', L. N. de Castro and F. J. Von Zuben, Idea Group Publishing, 2004.\n*Nancy Forbes, Imitation of Life: How Biology is Inspiring Computing, MIT Press, Cambridge, MA 2004.\n* M. Blowers and A. Sisti, ''Evolutionary and Bio-inspired Computation: Theory and Applications'', SPIE Press, 2007.\n* X. S. Yang, Z. H. Cui, R. B. Xiao, A. H. Gandomi, M. Karamanoglu, ''Swarm Intelligence and Bio-Inspired Computation: Theory and Applications'', Elsevier, 2013. \n* \"[http://informatics.indiana.edu/rocha/i-bic/ Biologically Inspired Computing Lecture Notes]\", [[Luis M. Rocha]]\n* ''The portable UNIX programming system (PUPS) and CANTOR: a computational envorionment for dynamical representation and analysis of complex neurobiological data'', [[Mark A. O'Neill]], and Claus-C Hilgetag, Phil Trans R Soc Lond B 356 (2001), 1259–1276\n* \"[https://arxiv.org/abs/cs/0512071 Going Back to our Roots: Second Generation Biocomputing]\", J. Timmis, M. Amos, W. Banzhaf, and A. Tyrrell, Journal of Unconventional Computing 2 (2007) 349–378.\n* {{cite book | last1=Neumann | first1=Frank | last2=Witt | first2=Carsten | title=Bioinspired computation in combinatorial optimization. Algorithms and their computational complexity | zbl=1223.68002 | series=Natural Computing Series | location=Berlin | publisher=[[Springer-Verlag]] | isbn=978-3-642-16543-6 | year=2010 }}\n* {{cite book | last1=Brabazon | first1=Anthony | last2=O’Neill | first2=Michael | title=Biologically inspired algorithms for financial modelling | zbl=1117.91030 | series=Natural Computing Series | location=Berlin | publisher=[[Springer-Verlag]] | isbn=978-3-540-26252-7 | year=2006 }}\n* C-M. Pintea, 2014, [https://www.springer.com/la/book/9783642401787 Advances in Bio-inspired Computing for Combinatorial Optimization Problem], Springer {{ISBN|978-3-642-40178-7}}\n* \"[https://arxiv.org/pdf/1709.09840.pdf PSA: A novel optimization algorithm based on survival rules of porcellio scaber]\", Y. Zhang and S. Li\n\n== External links ==\n*[http://www2.surrey.ac.uk/computing/research/nice/ Nature Inspired Computing and Engineering (NICE)] Group, University of Surrey, UK\n*[http://www.cogs.susx.ac.uk/users/ezequiel/alife-page/development.html ALife Project in Sussex]\n*[https://web.archive.org/web/20130621005509/http://neurochem-project.eu/ Biologically Inspired Computation for Chemical Sensing ''Neurochem'' Project]\n*[http://www.andcorporation.com AND Corporation]\n*[http://www.cercia.ac.uk/ Centre of Excellence for Research in Computational Intelligence and Applications] Birmingham, UK\n* [https://web.archive.org/web/20080828173733/http://dssg.cs.umb.edu/wiki/index.php/BiSNET BiSNET: Biologically-inspired architecture for Sensor NETworks]\n* [https://web.archive.org/web/20090622110049/http://dssg.cs.umb.edu/wiki/index.php/BiSNET/e BiSNET/e: A Cognitive Sensor Networking Architecture with Evolutionary Multiobjective Optimization]\n*[https://web.archive.org/web/20060621194332/http://www.neuralnetworksolutions.com/ Biologically inspired neural networks]\n*[http://ncra.ucd.ie NCRA] UCD, Dublin Ireland\n*[http://www.tumblingdice.co.uk/pupsp3 The PUPS/P3 Organic Computing Environment for Linux]\n* [https://web.archive.org/web/20090329225051/http://dssg.cs.umb.edu/wiki/index.php/SymbioticSphere SymbioticSphere: A Biologically-inspired Architecture for Scalable, Adaptive and Survivable Network Systems]\n* [http://www.sciencedirect.com/science/article/pii/S1568494615002756 The runner-root algorithm]\n* [http://www.bionet.ufpr.br Bio-inspired Wireless Networking Team (BioNet)]\n* [http://www.ai-one.com Biologically Inspired Intelligence]\n\n{{DEFAULTSORT:Bio-Inspired Computing}}\n[[Category:Theoretical computer science]]\n[[Category:Artificial intelligence]]\n[[Category:Natural computation]]\n[[Category:Nature-inspired metaheuristics| ]]\n[[Category:Bioinspiration]]"
    },
    {
      "title": "List of metaphor-based metaheuristics",
      "url": "https://en.wikipedia.org/wiki/List_of_metaphor-based_metaheuristics",
      "text": "This is a chronologically ordered list of metaphor-based [[metaheuristics]] and [[swarm intelligence]] algorithms.\n\n== Algorithms ==\n\n{{expand list|date=August 2016}}\n\n=== Simulated annealing (Kirkpatrick et al. 1983) ===\n\n{{main|Simulated annealing}}\n\nSimulated annealing (SA) is a [[probabilistic algorithm|probabilistic]] technique inspired by a heat treatment method in [[metallurgy]]. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities).  For problems where finding the precise global optimum is less important than finding an acceptable local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as [[gradient descent]].\n\nSimulated annealing interprets slow cooling as a slow decrease in the probability of accepting worse solutions as it explores the solution space. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the optimal solution.\n\n=== Ant colony optimization (Dorigo, 1992) ===\n\n{{main|Ant colony optimization algorithms}}\n\nThe ant colony optimization algorithm (ACO) is a [[probability|probabilistic]] technique for solving computational problems which can be reduced to finding good paths through [[Graph (discrete mathematics)|graph]]s. Initially proposed by [[Marco Dorigo]] in 1992 in his PhD thesis,<ref>{{cite book |first1=Alberto |last1=Colorni |first2=Marco |last2=Dorigo |first3=Vittorio |last3=Maniezzo |chapter=Distributed Optimization by Ant Colonies |pages=134–42 |chapterurl={{Google books|pWsNJkdZ4tgC|page=134|plainurl=yes}} |editor1-first=Francisco J. |editor1-last=Varela |editor2-first=Paul |editor2-last=Bourgine |year=1992 |title=Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life |isbn=978-0-262-72019-9 }}</ref><ref name=\"M. Dorigo, Optimization, Learning and Natural Algorithms\">M. Dorigo, ''Optimization, Learning and Natural Algorithms'', PhD thesis, Politecnico di Milano, Italy, 1992.{{page needed|date=January 2018}}</ref> the first algorithm was aiming to search for an optimal path in a graph, based on the behavior of [[ants]] seeking a path between their [[ant colony|colony]] and a source of food. The original idea has since diversified to solve a wider class of numerical problems, and as a result, several problems have emerged, drawing on various aspects of the behavior of ants. From a broader perspective, ACO performs a model-based search<ref>{{cite journal |doi=10.1023/B:ANOR.0000039526.52305.af |title=Model-Based Search for Combinatorial Optimization: A Critical Survey |journal=Annals of Operations Research |volume=131 |issue=1–4 |pages=373–95 |year=2004 |last1=Zlochin |first1=Mark |last2=Birattari |first2=Mauro |last3=Meuleau |first3=Nicolas |last4=Dorigo |first4=Marco |citeseerx=10.1.1.3.427 }}</ref> and shares some similarities with [[Estimation of Distribution Algorithm]]s.\n\n=== Particle swarm optimization (Kennedy & Eberhart 1995) ===\n\n{{main|Particle swarm optimization}}\n\nParticle swarm optimization (PSO) is a computational method that [[Mathematical optimization|optimizes]] a problem by [[iterative method|iteratively]] trying to improve a [[candidate solution]] with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed [[Point particle|particle]]s, and moving these particles around in the [[Optimization (mathematics)#Concepts and notation|search-space]] according to simple [[formula|mathematical formulae]] over the particle's [[Position (vector)|position]] and [[velocity]]. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.\n\nPSO is originally attributed to [[James Kennedy (social psychologist)|Kennedy]], [[Russell C. Eberhart|Eberhart]] and Shi<ref name=kennedy95particle/><ref name=shi98modified/> and was first intended for [[computer simulation|simulating]] [[social behaviour]],<ref name=kennedy97particle/> as a stylized representation of the movement of organisms in a bird [[Flocking (behavior)|flock]] or [[fish school]]. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart<ref name=kennedy01swarm/> describes many philosophical aspects of PSO and [[swarm intelligence]]. An extensive survey of PSO applications is made by [[Riccardo Poli|Poli]].<ref name=poli07analysis/><ref name=poli08analysis/> Recently, a comprehensive review on theoretical and experimental works on PSO has been published by Bonyadi and Michalewicz.<ref name=bonyadi16survey/>\n\n=== Harmony search (Geem, Kim & Loganathan 2001) ===\nHarmony search is a phenomenon-mimicking [[metaheuristic]] introduced in 2001 by Zong Woo Geem, Joong Hoon Kim, and G. V. Loganathan.<ref>{{cite journal |doi=10.1177/003754970107600201 |title=A New Heuristic Optimization Algorithm: Harmony Search |journal=Simulation |volume=76 |issue=2 |pages=60–8 |year=2016 |last1=Zong Woo Geem |last2=Joong Hoon Kim |last3=Loganathan |first3=G.V. }}</ref> Harmony search is inspired by the improvisation process of jazz musicians. Harmony search has been strongly criticized for being a special case of the well-established [[Evolution strategy|Evolution Strategies]] algorithm.<ref>{{cite journal |doi=10.1016/j.orp.2015.04.001 |title=A critical analysis of the harmony search algorithm—How not to solve sudoku |journal=Operations Research Perspectives |volume=2 |pages=97–105 |year=2015 |last1=Weyland |first1=Dennis }}</ref>\n\nThe Harmony search (HS) is a relatively simple yet very efficient evolutionary algorithm.  In HS algorithm a bunch/group of solutions is randomly generated (Called Harmony memory). A new solution is generated by using all the solutions in the Harmony memory (rather than just two as used in GA) and if this new solution is better than the Worst solution in Harmony memory, the Worst solution gets replaced by this new solution. Although HS is a relatively new meta heuristic algorithm, its effectiveness and advantages have been demonstrated in various applications like design of municipal water distribution networks,<ref>{{cite journal |doi=10.1080/03052150500467430 |title=Optimal cost design of water distribution networks using harmony search |journal=Engineering Optimization |volume=38 |issue=3 |pages=259–277 |year=2006 |last1=Geem |first1=Zong Woo }}</ref> structural design,<ref>{{cite journal |doi=10.1080/0305215X.2012.704028 |title=Shape optimization of structures for frequency constraints by sequential harmony search algorithm |journal=Engineering Optimization |volume=45 |issue=6 |pages=627 |year=2013 |last1=Gholizadeh |first1=S. |last2=Barzegar |first2=A. |bibcode=2013EnOp...45..627G }}</ref> traffic routing,<ref>{{cite journal |doi=10.3844/ajassp.2005.1552.1557 |title=Application of Harmony Search to Vehicle Routing |journal=American Journal of Applied Sciences |volume=2 |issue=12 |pages=1552 |year=2005 |last1=Geem |first1=Zong Woo |last2=Lee |first2=Kang Seok |last3=Park |first3=Yongjin }}</ref> load dispatch problem in electrical engineering,<ref>{{cite journal |doi=10.1016/j.ijepes.2012.08.021 |title=An effective differential harmony search algorithm for the solving non-convex economic load dispatch problems |journal=International Journal of Electrical Power & Energy Systems |volume=44 |pages=832–843 |year=2013 |last1=Wang |first1=Ling |last2=Li |first2=Ling-po }}</ref> multi objective optimization,<ref>{{cite journal |doi=10.1109/TSG.2012.2237420 |title=An Improved Multi-Objective Harmony Search for Optimal Placement of DGs in Distribution Systems |journal=IEEE Transactions on Smart Grid |volume=4 |pages=557–567 |year=2013 |last1=Nekooei |first1=Komail |last2=Farsangi |first2=Malihe M. |last3=Nezamabadi-Pour |first3=Hossein |last4=Lee |first4=Kwang Y. }}</ref> rostering problems,<ref>{{cite journal |doi=10.1016/j.ins.2012.12.025 |title=A harmony search algorithm for nurse rostering problems |journal=Information Sciences |volume=233 |pages=126–140 |year=2013 |last1=Hadwan |first1=Mohammed |last2=Ayob |first2=Masri |last3=Sabar |first3=Nasser R. |last4=Qu |first4=Roug |citeseerx=10.1.1.298.6805 }}</ref> clustering,<ref>{{cite journal |doi=10.1109/TII.2013.2273739 |title=Real-Time Implementation of a Harmony Search Algorithm-Based Clustering Protocol for Energy-Efficient Wireless Sensor Networks |journal=IEEE Transactions on Industrial Informatics |volume=10 |pages=774–783 |year=2014 |last1=Hoang |first1=Duc Chinh |last2=Yadav |first2=Parikshit |last3=Kumar |first3=Rajesh |last4=Panda |first4=Sanjib Kumar }}</ref> classification and feature selection<ref>{{cite journal |doi=10.1109/TSMCB.2012.2193613 |pmid=22645272 |title=Feature Selection with Harmony Search |journal=IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) |volume=42 |issue=6 |pages=1509–23 |year=2012 |last1=Ren Diao |last2=Qiang Shen }}</ref><ref>{{cite journal |doi=10.1007/s00521-014-1766-y |title=Estimation of asphaltene precipitation from titration data: A hybrid support vector regression with harmony search |journal=Neural Computing and Applications |volume=26 |issue=4 |pages=789 |year=2014 |last1=Fattahi |first1=Hadi |last2=Gholami |first2=Amin |last3=Amiribakhtiar |first3=Mohammad Sadegh |last4=Moradi |first4=Siyamak }}</ref> to name a few. A detailed survey on applications of HS can be found in<ref>{{cite journal |doi=10.1016/j.engappai.2013.05.008 |title=A survey on applications of the harmony search algorithm |journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=8 |pages=1818 |year=2013 |last1=Manjarres |first1=D. |last2=Landa-Torres |first2=I. |last3=Gil-Lopez |first3=S. |last4=Del Ser |first4=J. |last5=Bilbao |first5=M.N. |last6=Salcedo-Sanz |first6=S. |last7=Geem |first7=Z.W. }}</ref> and applications of HS in data mining can be found in.<ref>{{cite book |doi=10.1007/978-981-10-0451-3_77 |chapter=Applications of Harmony Search Algorithm in Data Mining: A Survey |title=Proceedings of Fifth International Conference on Soft Computing for Problem Solving |volume=437 |pages=863–74 |series=Advances in Intelligent Systems and Computing |year=2016 |last1=Assif Assad |last2=Deep |first2=Kusum |isbn=978-981-10-0450-6 }}</ref>\n\n===Artificial bee colony algorithm (Karaboga 2005)===\n{{main|Artificial bee colony algorithm}}\nArtificial bee colony algorithm is a meta-heuristic algorithm introduced by Karaboga in 2005,<ref>{{cite journal |doi=10.4249/scholarpedia.6915 |title=Artificial bee colony algorithm |journal=Scholarpedia |volume=5 |issue=3 |pages=6915 |year=2010 |last1=Karaboga |first1=Dervis |bibcode=2010SchpJ...5.6915K }}</ref>  and simulates the foraging behaviour of honey bees. The ABC algorithm has three phases: employed bee, onlooker bee and scout bee. In the employed bee and the onlooker bee phases, bees exploit the sources by local searches in the neighbourhood of the solutions selected based on deterministic selection in the employed bee phase and the [[Random selection|probabilistic selection]] in the onlooker bee phase. In the scout bee phase which is an analogy of abandoning exhausted food sources in the foraging process, solutions that are not beneficial anymore for search progress are abandoned, and new solutions are inserted instead of them to explore new regions in the search space. The algorithm has a well-balanced exploration and exploitation ability.\n\n=== Bees algorithm (Pham 2005) ===\n{{main|Bees algorithm}}\nThe bees algorithm in its basic formulation was created by Pham and his co-workers in 2005,<ref name=\"Pham & al, 2005\">Pham DT, Ghanbarzadeh A, Koc E, Otri S, Rahim S and Zaidi M. The Bees Algorithm. Technical Note, Manufacturing Engineering Centre, Cardiff University, UK, 2005.{{page needed|date=September 2017}}</ref> and further refined in the following years.<ref name=\"Pham & Castellani, 2009\">{{cite journal |doi=10.1243/09544062jmes1494 |title=The Bees Algorithm: Modelling foraging behaviour to solve continuous optimization problems |journal=Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science |volume=223 |issue=12 |pages=2919 |year=2009 |last1=Pham |first1=D T |last2=Castellani |first2=M }}</ref> Modelled on the foraging behaviour of [[honey bees]], the algorithm combines global explorative search with local exploitative search. A small number of artificial bees (scouts) explores randomly the solution space (environment) for solutions of high fitness (highly profitable food sources), whilst the bulk of the population search (harvest) the neighbourhood of the fittest solutions looking for the fitness optimum. A deterministics recruitment procedure which simulates the [[waggle dance]] of biological bees is used to communicate the scouts' findings to the foragers, and distribute the foragers depending on the fitness of the neighbourhoods selected for local search. Once the search in the neighbourhood of a solution stagnates, the local fitness optimum is considered to be found, and the site is abandoned. In summary, the Bees Algorithm searches concurrently the most promising regions of the solution space, whilst continuously sampling it in search of new favourable regions.\n\n=== {{anchor|Glowworm swarm}} Glowworm swarm optimization (Krishnanand & Ghose 2005) ===\nThe glowworm swarm optimization is a [[swarm intelligence]] [[Mathematical optimization|optimization]] [[algorithm]] developed based on the behaviour of [[glowworm]]s (also known as fireflies or lightning bugs). The GSO algorithm was developed and introduced by K.N. Krishnanand and [[Debasish Ghose]] in 2005 at the Guidance, Control, and Decision Systems Laboratory in the Department of Aerospace Engineering at the [[Indian Institute of Science]], [[Bangalore]], [[India]].<ref>{{cite book |doi=10.1109/SIS.2005.1501606 |chapter=Detection of multiple source locations using a glowworm metaphor with applications to collective robotics |title=Proceedings 2005 IEEE Swarm Intelligence Symposium, 2005. SIS 2005 |pages=84–91 |year=2005 |last1=Krishnanand |first1=K.N. |last2=Ghose |first2=D. |isbn=978-0-7803-8916-8 }}</ref>\n\nThe behaviour pattern of glowworms which is used for this algorithm is the apparent capability of the glowworms to change the intensity of the luciferin emission and thus appear to glow at different intensities. \n# The GSO algorithm makes the agents glow at intensities approximately proportional to the function value being optimized. It is assumed that glowworms of brighter intensities attract glowworms that have lower intensity. \n# The second significant part of the algorithm incorporates a dynamic decision range by which the effect of distant glowworms are discounted when a glowworm has sufficient number of neighbours or the range goes beyond the range of perception of the glowworms.\n\nThe part 2 of the algorithm makes it different from other [[evolutionary multimodal optimization]] algorithms. It is this step that allows glowworm [[swarm]]s to automatically subdivide into subgroups which can then converge to multiple local optima simultaneously,  This property of the algorithm allows it to be used to identify multiple peaks of a multi-modal function and makes it a part of the evolutionary multimodal optimization algorithms family.\n\n=== {{anchor|Shuffled frog leaping}} Shuffled frog leaping algorithm (Eusuff, Lansey & Pasha 2006) ===\nThe shuffled frog leaping algorithm is an optimization algorithm used in [[artificial intelligence]].<ref>{{cite journal |doi=10.1080/03052150500384759 |title=Shuffled frog-leaping algorithm: A memetic meta-heuristic for discrete optimization |journal=Engineering Optimization |volume=38 |issue=2 |pages=129 |year=2006 |last1=Eusuff |first1=Muzaffar |last2=Lansey |first2=Kevin |last3=Pasha |first3=Fayzul }}</ref> It is comparable to a [[genetic algorithm]].\n\n=== {{anchor|Cat Swarm Optimization }} Cat Swarm Optimization (Chu, Tsai, and Pan 2006) ===\nThe cat swarm optimization algorithm which solves optimization problems and is inspired by the behavior of cats to <ref>{{cite book |doi=10.1007/11801603_94 |chapter=Cat Swarm Optimization |conference=PRICAI 2006: Trends in Artificial Intelligence, 9th Pacific Rim International Conference on Artificial Intelligence | location=Guilin, China |year=2006 |last1=Chu|first1=Shu-Chuan|title=PRICAI 2006: Trends in Artificial Intelligence |volume=4099 |pages=854–858 |last2=Tsai|first2=Pei-wei|last3=Pan |first3=Jeng-Shyang|series=Lecture Notes in Computer Science |isbn=978-3-540-36667-6 }}</ref> It is similar to other swarm optimization algorithms such as the Ant Colony Optimization or Particle Swarm Optimization algorithms. Seeking and Tracing, two common behavior of cats, make up the two sub-models of the algorithm. Seeking Mode is inspired by the behavior of a cat at rest, seeking where to move next. In Seeking Mode, it selects several candidate points and then selects one to move to randomly, increasing the probability of choosing points that have a higher fitness value. Tracing Mode is inspired by a cat tracing some target. In this mode, the cat will try to move towards the position with the best fitness value. Cats will continue moving in Seeking and Tracing mode until a terminating condition is met.\n\n=== {{anchor|Imperialist competitive}} Imperialist competitive algorithm (Atashpaz-Gargari & Lucas 2007) ===\n{{main|Imperialist competitive algorithm}}\nThe imperialist competitive algorithm is a computational method that is used to solve [[optimization problem]]s of different types.<ref name=ica_en_2007_cnf_atashpaz_ica_ica>{{cite book |doi=10.1109/CEC.2007.4425083 |chapter=Imperialist competitive algorithm: An algorithm for optimization inspired by imperialistic competition |title=2007 IEEE Congress on Evolutionary Computation |pages=4661–7 |year=2007 |last1=Atashpaz-Gargari |first1=Esmaeil |last2=Lucas |first2=Caro |isbn=978-1-4244-1339-3 }}</ref><ref name=ICA_2014_Survey>{{cite journal |doi=10.1016/j.asoc.2014.08.024 |title=A survey on the Imperialist Competitive Algorithm metaheuristic: Implementation in engineering domain and directions for future research |journal=Applied Soft Computing |volume=24 |pages=1078–1094 |year=2014 |last1=Hosseini |first1=Seyedmohsen |last2=Al Khaled |first2=Abdullah }}</ref>  Like most of the methods in the area of [[evolutionary computation]], ICA does not need the gradient of the function in its optimization process. From a specific point of view, ICA can be thought of as the social counterpart of [[genetic algorithms]] (GAs). ICA is the mathematical model and the computer simulation of human [[social evolution]], while GAs are based on the [[biological evolution]] of species.\n\nThis algorithm starts by generating a set of random candidate solutions in the search space of the optimization problem. The generated random points are called the initial ''Countries''. Countries in this algorithm are the counterpart of ''Chromosome''s in GAs and ''Particle''s in [[Particle Swarm Optimization]] (PSO) and it is an array of values of a candidate solution of optimization problem. The [[Loss function|cost function]] of the optimization problem determines the power of each country. Based on their power, some of the best initial countries (the countries with the least cost function value), become ''Imperialists'' and start taking control of other countries (called ''colonies'') and form the initial ''Empires''.<ref name=ica_en_2007_cnf_atashpaz_ica_ica />\n\nTwo main operators of this algorithm are ''Assimilation'' and ''Revolution''. Assimilation makes the colonies of each empire get closer to the imperialist state in the space of socio-political characteristics (optimization search space). Revolution brings about sudden random changes in the position of some of the countries in the search space. During assimilation and revolution a colony might reach a better position and has the chance to take the control of the entire empire and replace the current imperialist state of the empire.<ref name=ica_en_2010_jnl_nazari_integrated_product_mix_outsourcing>{{cite journal |doi=10.1016/j.eswa.2010.04.081 |title=Solving the integrated product mix-outsourcing problem using the Imperialist Competitive Algorithm |journal=Expert Systems with Applications |volume=37 |issue=12 |pages=7615 |year=2010 |last1=Nazari-Shirkouhi |first1=S. |last2=Eivazy |first2=H. |last3=Ghodsi |first3=R. |last4=Rezaie |first4=K. |last5=Atashpaz-Gargari |first5=E. }}</ref>\n\n''Imperialistic Competition'' is another part of this algorithm. All the empires try to win this game and take possession of colonies of other empires. In each step of the algorithm, based on their power, all the empires have a chance to take control of one or more of the colonies of the weakest empire.<ref name=ica_en_2007_cnf_atashpaz_ica_ica />\n\nAlgorithm continues with the mentioned steps (Assimilation, Revolution, Competition) until a stop condition is satisfied.\n\nThe above steps can be summarized as the below [[pseudocode]].<ref name=ICA_2014_Survey /><ref name=ica_en_2010_jnl_nazari_integrated_product_mix_outsourcing />\n 0) Define objective function: <math>f(\\mathbf{x}), \\quad \\mathbf{x}=(x_1,x_2,\\dots,x_d); \\, </math>\n 1) Initialization of the algorithm. Generate some random solution in the search space and create initial empires.\n     2) Assimilation: Colonies move towards imperialist states in different in directions.\n     3) Revolution: Random changes occur in the characteristics of some countries.\n     4) Position exchange between a colony and Imperialist. A colony with a better position than the imperialist,\n        has the chance to take the control of empire by replacing the existing imperialist.\n     5) Imperialistic competition: All imperialists compete to take possession of colonies of each other.\n     6) Eliminate the powerless empires. Weak empires lose their power gradually and they will finally be eliminated.\n     7) If the stop condition is satisfied, stop, if not go to 2.\n 8) End\n\n==={{anchor|River formation dynamics}} River formation dynamics (Rabanal, Rodríguez & Rubio 2007) ===\nRiver formation dynamics is based on imitating how water forms rivers by eroding the ground and depositing sediments (the drops act as the swarm). After drops transform the landscape by increasing/decreasing the altitude of places, solutions are given in the form of paths of decreasing altitudes. Decreasing gradients are constructed, and these gradients are followed by subsequent drops to compose new gradients and reinforce the best ones.\nThis heuristic optimization method was first presented in 2007 by Rabanal et al.<ref>{{cite book |doi=10.1007/978-3-540-73554-0 |title=Unconventional Computation |volume=4618 |series=Lecture Notes in Computer Science |year=2007 |isbn=978-3-540-73553-3 |last1=Akl |first1=Selim G. |last2=Calude |first2=Cristian S. |last3=Dinneen |first3=Michael J. |last4=Rozenberg |first4=Grzegorz |last5=Todd Wareham |first5=H. |arxiv=0711.2964 }}</ref> The applicability of RFD to other NP-complete problems has been studied,<ref>{{cite book |doi=10.1007/978-3-642-00267-0_12 |chapter=Applying River Formation Dynamics to Solve NP-Complete Problems |title=Nature-Inspired Algorithms for Optimisation |volume=193 |pages=333–68 |series=Studies in Computational Intelligence |year=2009 |last1=Rabanal |first1=Pablo |last2=Rodríguez |first2=Ismael |last3=Rubio |first3=Fernando |isbn=978-3-642-00266-3 }}</ref> and the algorithm has been applied to fields such as routing<ref>{{cite journal |doi=10.1016/j.bjp.2013.11.015 |title=Smart data packet ad hoc routing protocol |journal=Computer Networks |volume=62 |pages=162–181 |year=2014 |last1=Amin |first1=Saman Hameed |last2=Al-Raweshidy |first2=H.S. |last3=Abbas |first3=Rafed Sabbar }}</ref> and robot navigation.<ref>{{cite journal |doi=10.4028/www.scientific.net/SSP.198.138 |title=Using River Formation Dynamics Algorithm in Mobile Robot Navigation |journal=Solid State Phenomena |volume=198 |pages=138–143 |year=2013 |last1=Redlarski |first1=Grzegorz |last2=Pałkowski |first2=Aleksander |last3=Dąbkowski |first3=Mariusz }}</ref> The main applications of RFD can be found at a detailed survey.<ref>{{cite journal |doi=10.1016/j.jocs.2017.08.002 |title=Applications of river formation dynamics |journal=Journal of Computational Science |volume=22 |pages=26–35 |year=2017 |last1=Rabanal |first1=Pablo |last2=Rodríguez |first2=Ismael |last3=Rubio |first3=Fernando }}</ref>\n\n==={{anchor|Intelligent water drops}} Intelligent water drops algorithm (Shah-Hosseini 2007) ===\nIntelligent water drops algorithm contains a few essential elements of natural water drops and actions and reactions that occur between river's bed and the water drops that flow within. The IWD was first introduced for the [[traveling salesman problem]] in 2007.<ref name=shah-hosseini2009>{{cite journal |doi=10.1504/ijbic.2009.022775 |title=The intelligent water drops algorithm: A nature-inspired swarm-based optimization algorithm |journal=International Journal of Bio-Inspired Computation |volume=1 |pages=71 |year=2009 |last1=Hosseini |first1=Hamed Shah }}</ref>\n\nAlmost every IWD algorithm is composed of two parts: a graph that plays the role of distributed memory on which soils of different edges are preserved, and the moving part of the IWD algorithm, which is a few number of Intelligent water drops. These intelligent water drops (IWDs) both compete and cooperate to find better solutions and by changing soils of the graph, the paths to better solutions become more reachable. It is mentioned that the IWD-based algorithms need at least two IWDs to work.\n\nThe IWD algorithm has two types of parameters: static and dynamic parameters. Static parameters are constant during the process of the IWD algorithm. Dynamic parameters are reinitialized after each iteration of the IWD algorithm. The pseudo-code of an IWD-based algorithm may be specified in eight steps:\n\n:'''1)''' Static parameter initialization\n:: a) ''Problem representation in the form of a graph''\n:: b) ''Setting values for static parameters'' \n:'''2)''' Dynamic parameter initialization: soil and velocity of IWDs\n:'''3)''' Distribution of IWDs on the problem’s graph\n:'''4)''' Solution construction by IWDs along with soil and velocity updating\n:: a) ''Local soil updating on the graph''\n:: b) ''Soil and velocity updating on the IWDs''\n:'''5)''' Local search over each IWD’s solution (optional)\n: '''6)''' Global soil updating\n: '''7)''' Total-best solution updating\n: '''8)''' Go to step 2 unless termination condition is satisfied\n\n=== {{anchor|Gravitational search algorithm}} Gravitational search algorithm (Rashedi, Nezamabadi-pour & Saryazdi 2009) ===\nA gravitational search algorithm is based on the [[law of gravity]] and the notion of mass interactions. The GSA algorithm uses the theory of [[Newtonian physics]] and its searcher [[Agent (artificial intelligence)|agents]] are the collection of masses. In GSA, there is an [[isolated system]] of masses. Using the gravitational force, every mass in the system can [[Action at a distance|see the situation]] of other masses. The gravitational force is therefore a way of transferring information between different masses (Rashedi, Nezamabadi-pour and Saryazdi 2009).<ref>{{cite journal |doi=10.1016/j.ins.2009.03.004 |title=GSA: A Gravitational Search Algorithm |journal=Information Sciences |volume=179 |issue=13 |pages=2232 |year=2009 |last1=Rashedi |first1=Esmat |last2=Nezamabadi-Pour |first2=Hossein |last3=Saryazdi |first3=Saeid }}</ref> In GSA, agents are considered as objects and their performance is measured by their masses. All these objects attract each other by a [[gravity]] force, and this force causes movement of all objects towards the objects with heavier masses. Heavier masses correspond to better solutions of the problem. The [[Position (vector)|position]] of the agent corresponds to a solution of the problem, and its mass is determined using a fitness function. By lapse of time, masses are attracted by the heaviest mass, which would ideally present an optimum solution in the search space. The GSA could be considered as an isolated system of masses. It is like a small artificial world of masses obeying the Newtonian laws of gravitation and motion.<ref>Rashedi, Nezamabadi-pour and Saryazdi 2009</ref> A multi-objective variant of GSA, called MOGSA, was first proposed by Hassanzadeh et al. in 2010.<ref>{{cite book |doi=10.1109/CICSyN.2010.32 |chapter=A Multi-objective Gravitational Search Algorithm |title=2010 2nd International Conference on Computational Intelligence, Communication Systems and Networks |pages=7–12 |year=2010 |last1=Hassanzadeh |first1=Hamid Reza |last2=Rouhani |first2=Modjtaba |isbn=978-1-4244-7837-8 }}</ref>\n\n=== Cuckoo search (Yang & Deb 2009) ===\n{{main|Cuckoo search}}\nIn [[operations research]], '''cuckoo search''' is an [[Optimization (mathematics)|optimization]] [[algorithm]] developed by [[Xin-she Yang]] and Suash Deb in 2009.<ref name=NaBIC>{{cite book |doi=10.1109/NABIC.2009.5393690 |chapter=Cuckoo Search via Lévy flights |title=2009 World Congress on Nature & Biologically Inspired Computing (NaBIC) |pages=210–4 |year=2009 |last1=Yang |first1=Xin-She |last2=Suash Deb |isbn=978-1-4244-5053-4 }}</ref><ref>{{cite web|author=Inderscience |url=http://www.alphagalileo.org/ViewItem.aspx?ItemId=76985&CultureCode=en |title=Cuckoo designs spring |publisher=Alphagalileo.org |date=27 May 2010 |accessdate=2010-05-27}}</ref> It was inspired by the [[Obligate parasite|obligate brood parasitism]] of some [[cuckoo]] species by laying their eggs in the nests of other host birds (of other species). Some host birds can engage direct conflict with the intruding cuckoos. For example, if a host bird discovers the eggs are not their own, it will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere. Some cuckoo species such as the [[New World]] brood-parasitic [[Tapera]] have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colors and pattern of the eggs of a few chosen host species.<ref>R. B. Payne, M. D. Sorenson, and K. Klitz, The Cuckoos, Oxford University Press, (2005).{{page needed|date=September 2017}}</ref>\n\n===Bat algorithm (Yang 2010) ===\n{{main|Bat algorithm}}\nBat algorithm is a swarm-intelligence-based algorithm, inspired by the [[Animal echolocation|echolocation]] behavior of [[microbat]]s. BA automatically balances exploration (long-range jumps around the global search space to avoid getting stuck around one local maximum) with exploitation (searching in more detail around known good solutions to find local maxima) by controlling loudness and pulse emission rates of simulated bats in the multi-dimensional search space.<ref>{{cite book |doi=10.1007/978-3-642-12538-6_6 |chapter=A New Metaheuristic Bat-Inspired Algorithm |title=Nature Inspired Cooperative Strategies for Optimization (NICSO 2010) |volume=284 |pages=65–74 |series=Studies in Computational Intelligence |year=2010 |last1=Yang |first1=Xin-She |isbn=978-3-642-12537-9 |citeseerx=10.1.1.761.2708 }}</ref>\n\n===Spiral optimization (SPO) algorithm (Tamura & Yasuda 2011,2016-2017) ===\n{{main|Spiral optimization algorithm}}\n[[File:spo_movie4.gif|thumb|Spiral optimization (SPO) algorithm]]\n\nThe [[spiral]] optimization (SPO) algorithm is an uncomplicated search concept inspired by spiral phenomena in nature. The motivation for focusing on spiral phenomena was due to the insight that the dynamics that generate logarithmic spirals share the diversification and intensification behavior. The diversification behavior can work for a global search (exploration) and the intensification behavior enables an intensive search around a current found good solution (exploitation). \nThe SPO algorithm is a multipoint search algorithm that has no objective function gradient, which uses multiple spiral models that can be described as deterministic dynamical systems. As search points follow logarithmic spiral trajectories towards the common center, defined as the current best point, better solutions can be found and the common center can be updated.<ref>{{cite journal |doi=10.9746/jcmsi.9.134 |title=Spiral Optimization Algorithm Using Periodic Descent Directions |journal=SICE Journal of Control, Measurement, and System Integration |volume=9 |issue=3 |pages=134–43 |year=2016 |last1=Tamura |first1=Kenichi |last2=Yasuda |first2=Keiichiro |bibcode=2016JCMSI...9..134T }}</ref>\n\n=== {{anchor|Flower pollination}} Flower pollination algorithm (Yang 2012) ===\nFlower pollination algorithm is a meta[[Heuristic (computer science)|heuristic]] [[algorithm]] that was developed by [[Xin-She Yang]],<ref>{{cite book |doi=10.1007/978-3-642-32894-7_27 |chapter=Flower Pollination Algorithm for Global Optimization |title=Unconventional Computation and Natural Computation |volume=7445 |pages=240–9 |series=Lecture Notes in Computer Science |year=2012 |last1=Yang |first1=Xin-She |isbn=978-3-642-32893-0 |citeseerx=10.1.1.747.9556 }}</ref> based on the [[pollination]] process of flowering [[plants]].\n\nThis algorithm has 4 rules or assumptions:\n#[[Biotic pollination|Biotic]] and [[cross-pollination]] is considered as a global pollination process with pollen carrying pollinators performing [[Lévy flight|Levy flights]].\n#[[Abiotic pollination|Abiotic]] and [[self-pollination]] are considered as local pollination.\n#[[Flower constancy]] can be considered as the reproduction probability is [[Proportionality (mathematics)|proportional]] to the similarity of two flowers involved.\n# Local and global pollination are controlled by a switch probability{{Clarify|reason=|date=October 2018}} <math>p \\in [0, 1]</math>. Due to the physical proximity and other factors such as wind, local pollination can have a significant fraction ''q'' in the overall pollination activities.\n\nThese rules can be translated into the following updating equations:\n:<math> x_i^{t+1}=x_i^t + L (x_i^t-g_*)</math>\n:<math> x_i^{t+1}=x_i^t + \\epsilon (x_i^t-x_k^t)</math>\nwhere <math>x_i^t</math> is the solution vector and <math>g_*</math> is the current best found so far during iteration. The switch probability between two equations during iterations is <math>p</math>. In addition, <math>\\epsilon</math> is a random number drawn from a uniform distribution. <math>L</math> is a step size drawn from a Lévy distribution.\n\nLévy flights using Lévy steps is a powerful random walk because both global and local search capabilities can be carried out at the same time.\nIn contrast with standard Random walks, Lévy flights have occasional long jumps, which enable the algorithm to jump out any local valleys.\nLévy steps obey the following approximation: \n:<math> L \\sim \\frac{1}{s^{1+\\beta}}, </math>\nwhere <math>\\beta</math> is the Lévy exponent.<ref>{{cite journal |doi=10.1016/j.jcp.2007.06.008 |title=Lévy flights, non-local search and simulated annealing |journal=Journal of Computational Physics |volume=226 |issue=2 |pages=1830–1844 |year=2007 |last1=Pavlyukevich |first1=Ilya |bibcode=2007JCoPh.226.1830P |arxiv=cond-mat/0701653 }}</ref> It may be challenging to draw Lévy steps properly, and a simple way of generating Lévy flights <math>s</math> is to use two normal distributions <math>u</math> and <math>v</math> by a transform<ref>X. S. Yang, Nature-Inspired Optimization Algorithms, Elsevier, (2014).{{page needed|date=September 2017}}</ref>\n:<math> s = \\frac{u}{|v|^{1+\\beta}}, </math>\nwith\n:<math> u \\sim N(0, \\sigma^2), \\quad v \\sim N(0,1), </math>\nwhere <math>\\sigma</math> is a function of <math>\\beta</math>.\n\n=== {{anchor|Cuttlefish}} Cuttlefish optimization algorithm (Eesa, Mohsin, Brifcani & Orman 2013) ===\n[[File:Cuttlefish2.png|thumb|The six cases of reflection used by the Cuttlefish]]\nThe cuttlefish optimization algorithm is a population-based [[search algorithm]] inspired by skin color changing behaviour of [[Cuttlefish]] which was developed in 2013<ref name=\"Adel & al, 2013\">{{cite journal |first1=Adel Sabry |last1=Eesa |first2=Adnan Mohsin Abdulazeez |last2=Brifcani |first3=Zeynep |last3=Orman |date=September 2013 |title=Cuttlefish Algorithm – A Novel Bio-Inspired Optimization Algorithm |journal=International Journal of Scientific & Engineering Research |volume=4 |issue=9 |pages=1978–86 |url=https://www.ijser.org/paper/Cuttlefish-Algorithm-A-Novel-Bio-Inspired-Optimization-Algorithm.html }}</ref><ref name=\"Adel & al, 2015\">{{cite journal |doi=10.1016/j.eswa.2014.11.009 |title=A novel feature-selection approach based on the cuttlefish optimization algorithm for intrusion detection systems |journal=Expert Systems with Applications |volume=42 |issue=5 |pages=2670 |year=2015 |last1=Eesa |first1=Adel Sabry |last2=Orman |first2=Zeynep |last3=Brifcani |first3=Adnan Mohsin Abdulazeez }}</ref>  It has two global search and two local search.\n\nThe algorithm considers two main processes: ''Reflection'' and ''Visibility''. Reflection process simulates the light reflection mechanism, while visibility simulates the visibility of matching patterns. These two processes are used as a search strategy to find the global optimal solution. The formulation of finding the new solution (''newP'') by using ''reflection'' and ''visibility'' is as follows:\n<math>\nnewP = Reflection + Visibility \n</math>\n\nCFA divide the population into 4 Groups (G1, G2, G3 and G4). For G1 the algorithm applying case 1 and 2 (the interaction between chromatophores and iridophores) to produce a new solutions. These two cases are used as a global search. For G2, the algorithm uses case 3 (Iridophores reflection opaerator) and case 4 (the interaction between Iridophores and chromatophores) to produces a new solutions)  as a local search. While for G3 the interaction between the leucophores and chromatophores (case 5) is used to produce solutions around the best solution (local search). Finally for G4, case 6 (reflection operator of leucophores) is used as a global search by reflecting any incoming light as it with out any modification. The main step of CFA is described as follows:\n<source lang=\"C#\">\n   1 Initialize population (P[N]) with random solutions, Assign the values of r1, r2, v1, v2.\n   2 Evaluate the population and Keep the best solution.\n   3 Divide population into four groups (G1, G2, G3 and G4).\n   4 Repeat \n        4.1 Calculate the average value of the best solution.\n        4.2 for (each element in G1)\n                     generate new solution using Case(1 and 2)\n        4.3 for (each element in G2)\n                     generate new solution using Case(3 and 4)\n        4.4 for (each element in G3)\n                     generate new solution using Case(5)\n        4.5 for (each element in G4)\n                     generate new solution using Case(6)\n        4.6 Evaluate the new solutions \n   5. Until (stopping criterion is met)\n   6. Return the best solution\n</source>\n\nEquations that are used to calculate reflection and visibility for the four Groups are described below:\n\nCase 1 and 2 for G1:\n\n<math> Reflection[j] = R*G_{1}[j].Points[j] </math>\n\n<math> Visibility[j] = V*(Best.Points[j]-G_{1}[i].Points[j])  </math>\n\nCase 3 and 4 for G2:\n\n<math> Reflection[j] = R*Best.Points[j] </math>\n\n<math> Visibility[j] = V*(Best.Points[j]-G_{2}[i].Points[j]) </math>\n\nCase 5 for G3:\n\n<math> Reflection[j] = R*Best.Points[j] </math>\n\n<math> Visibility[j] = V*(Best.Points[j]-AV_{Best} </math>\n\nCase 6 for G4:\n\n<math> P[i].Points[j] = random*(upperLimit-lowerLinit)+LowerLimit, i = 1, 2, ..., N; j=1, 2, ..., d  </math>\n\nWhere <math>G_{1}</math>, <math>G_{2}</math> are Group1 and Group2, ''i'' presents the <math>i^{th}</math> element in ''G'', ''j'' is the <math>j^{th}</math> point of <math>i^{th}</math> element in group ''G'', ''Best'' is the best solution and <math>AV_{Best}</math> presents the average value of the ''Best'' points. While ''R'' and ''V'' are two random numbers produced around zero such as between (-1, 1), ''R'' represents the degree of reflection, ''V'' represents the visibility degree of the final view of the pattern, ''upperLimit'' and ''lowerLimit'' are the upper limit and the lower limit of the problem domain.\n\n=== Heterogeneous Distributed Bees Algorithm (Tkach et al., 2013) ===\nThe Heterogeneous Distributed Bees Algorithm (HDBA) also known as the Modified Distributed Bees Algorithm (MDBA) is a multi-agent [[metaheuristic]] algorithm initially introduced by Tkach and his co-workers in 2013,<ref>{{Cite journal|last=Tkach|first=I.|last2=Edan|first2=Y.|last3=Jevtic|first3=A.|last4=Nof|first4=S. Y.|date=October 2013|title=Automatic Multi-sensor Task Allocation Using Modified Distributed Bees Algorithm|url=https://ieeexplore.ieee.org/abstract/document/6721995|journal=2013 IEEE International Conference on Systems, Man, and Cybernetics|pages=1401–1406|doi=10.1109/SMC.2013.242}}</ref><ref>{{Cite journal|last=Edan|first=Yael|last2=Nof|first2=Shimon Y.|last3=Jevtić|first3=Aleksandar|last4=Tkach|first4=Itshak|date=March 2018|title=A Modified Distributed Bees Algorithm for Multi-Sensor Task Allocation|url=https://www.mdpi.com/1424-8220/18/3/759|journal=Sensors|volume=18|issue=3|pages=759|doi=10.3390/s18030759}}</ref> developed as part of his PhD dissertation. HDBA uses probabilistic technique taking inspiration from the foraging behaviour of bees. It enables to solve [[combinatorial optimization]] problems with multiple heterogeneous agents that possess different capabilities and performances. The final decision-making mechanism uses a wheel-selection rule, where each agent has a probability with which it selects a solution. It was first applied for the case of heterogeneous sensors in target recognition problem to improve system performance by correlating sensors’ utility function with the value of their performances. Afterwards, it was successfully applied to other problems, including the problem of allocating police agents to crime incidents and producing near-optimal solutions to the travelling salesman problem.\n\n=== Artificial swarm intelligence (Rosenberg 2014) ===\nArtificial swarm intelligence refers to a real-time closed-loop system of human users connected over the internet and structured in a framework modeled after natural swarms such that it evokes the group's collective wisdom as a unified emergent intelligence.<ref>{{Cite journal|last=Rosenberg|first=Louis|date=February 12, 2016|title=Artificial Swarm Intelligence, a Human-in-the-loop approach to A.I.|url=https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12087/12302|journal=Proceedings of the 13th Annual AAAI Conference on Artificial Intelligence (AAAI-16)|doi=|pmid=|access-date=}}</ref><ref>{{Cite news|url=http://www.techrepublic.com/article/how-artificial-swarm-intelligence-uses-people-to-make-better-predictions-than-experts/|title=How 'artificial swarm intelligence' uses people to make better predictions than experts|last=Reese|first=Hope|date=Jan 22, 2016|work=|access-date=|via=}}</ref> In this way, human swarms can answer questions, make predictions, reach decisions, and solve problems by collectively exploring a diverse set of options and converging on preferred solutions in synchrony.  Invented by [[Louis B. Rosenberg|Dr. Louis Rosenberg]] in 2014, the ASI methodology has become notable for its ability to make accurate collective predictions that outperform the individual members of the swarm.<ref>{{cite book |doi=10.1109/SHBI.2015.7321685 |chapter=Human swarming, a real-time method for parallel distributed intelligence |title=2015 Swarm/Human Blended Intelligence Workshop (SHBI) |pages=1–7 |year=2015 |last1=Rosenberg |first1=Louis B. |isbn=978-1-4673-6522-2 }}</ref>  In 2016 an Artificial Swarm Intelligence from [[Unanimous A.I.]] was challenged by a reporter to predict the winners of the [[Kentucky Derby]], and successfully picked the first four horses, in order, beating 540 to 1 odds.<ref>{{Cite news|url=http://www.newsweek.com/artificial-intelligence-turns-20-11000-kentucky-derby-bet-457783|title=ARTIFICIAL INTELLIGENCE TURNS $20 INTO $11,000 IN KENTUCKY DERBY BET (Newsweek)|last=CUTHBERTSON|first=ANTHONY|date=May 10, 2016|work=|access-date=|via=}}</ref><ref>{{Cite news|url=https://www.washingtonpost.com/news/the-intersect/wp/2016/06/02/what-happened-when-an-ai-hive-mind-answered-reddits-burning-politics-questions/|title=What happened when an A.I. hive mind answered Reddit’s burning politics questions (Washington Post)|last=Ohlheiser|first=Abby|date=June 2, 2016|work=|access-date=|via=}}</ref>\n\n=== Colliding bodies optimization (Kaveh and Mahdavi 2014) ===\nThe Colliding bodies optimization (CBO) <ref>{{cite journal |last1=Kaveh |first1=Ali |last2=Mahdavi |first2=Vahid Reza |title=Colliding bodies optimization : A novel meta-heuristic method |journal=Computers and Structures |volume=139 |pages=18–27|doi=10.1016/j.compstruc.2014.04.005 |year=2014 }}</ref> algorithm was created by Kaveh and Mahdavi in 2014 based on laws of momentum and energy. This algorithm does not depend on any internal parameter and also it is extremely simple to implement and to use and used in different types of problems in engineering.<ref>{{cite journal |last1=Kaveh |first1=Ali |last2=Vazirinia |first2=Yasin |title=Optimization of tower crane location and material quantity between supply and demand points: A comparative study |journal=Periodica Polytechnica Civil Engineering |volume=62 |issue=3 |pages=732–745 |doi=10.3311/PPci.11816|year=2018 }}</ref>\n\n===  {{anchor|Intelligent water drops}}Duelist Algorithm (Biyanto 2016) ===\nDuelist algorithm refers to a gene-based optimization algorithm similar to [[Genetic algorithm|Genetic Algorithms]]. Duelist Algorithm starts with an initial set of duelists. The duel is to determine the winner and loser. The loser learns from the winner, while the winner try their new skill or technique that may improve their fighting capabilities. A few duelists with highest fighting capabilities are called as champion. The champion train a new duelist such as their capabilities. The new duelist will join the tournament as a representative of each champion. All duelist are re-evaluated, and the duelists with worst fighting capabilities is eliminated to maintain the amount of duelists.<ref>{{cite book |doi=10.1007/978-3-319-41000-5_4 |chapter=Duelist Algorithm: An Algorithm Inspired by How Duelist Improve Their Capabilities in a Duel |title=Advances in Swarm Intelligence |volume=9712 |pages=39–47 |series=Lecture Notes in Computer Science |year=2016 |last1=Biyanto |first1=Totok Ruki |last2=Fibrianto |first2=Henokh Yernias |last3=Nugroho |first3=Gunawan |last4=Hatta |first4=Agus Muhamad |last5=Listijorini |first5=Erny |last6=Budiati |first6=Titik |last7=Huda |first7=Hairul |isbn=978-3-319-40999-3 }}</ref>\n\n===  {{anchor|Harris hawks optimization}}[[Harris hawks optimization]] (Heidari et al. 2019) ===\nHarris hawks optimizer (HHO) inspires the [[hunting]] strategies of [[Harris's hawk]] and escaping patterns of [[rabbits]] in [[nature]] <ref name=\"HeidariMirjalili2019\">{{cite journal|last1=Heidari|first1=Ali Asghar|last2=Mirjalili|first2=Seyedali|last3=Faris|first3=Hossam|last4=Aljarah|first4=Ibrahim|last5=Mafarja|first5=Majdi|last6=Chen|first6=Huiling|title=Harris hawks optimization: Algorithm and applications|journal=Future Generation Computer Systems|volume=97|year=2019|pages=849–872|issn=0167739X|doi=10.1016/j.future.2019.02.028}}</ref>. \n\n===  {{anchor|Intelligent water drops}}Killer Whale Algorithm (Biyanto 2016) ===\nKiller Whale Algorithm is an algorithm Inspired by the Killer Whale Life. The philosophy of algorithm is the patterns of movement Killer Whale in prey hunting and Killer whale social structure. The novelty of this algorithm is incorporating \"'''memorize capability'''\" of Killer Whale in the algorithm.<ref>{{cite journal |doi=10.1016/j.procs.2017.12.141 |title=Killer Whale Algorithm: An Algorithm Inspired by the Life of Killer Whale |journal=Procedia Computer Science |volume=124 |pages=151–7 |year=2017 |last1=Biyanto |first1=Totok R |last2=Matradji |last3=Irawan |first3=Sonny |last4=Febrianto |first4=Henokh Y |last5=Afdanny |first5=Naindar |last6=Rahman |first6=Ahmad H |last7=Gunawan |first7=Kevin S |last8=Pratama |first8=Januar A.D |last9=Bethiana |first9=Titania N }}</ref>\n\n===  {{anchor|Intelligent water drops}}Rain Water Algorithm (Biyanto 2017) ===\n\"'''Physical movements of rain drops by utilizing Newton’s Law motion'''\" was inspired the authors to create this algorithm. Each rain drop represent as random values of optimized variables that it have vary in mass and elevation. It will fall on the ground by following \"'''the free fall movement'''\" with velocity is square root of gravity acceleration time elevation. The next movement is \"'''uniformly accelerated motion'''\" along the rain drop travel to reach the lowest place on the ground. The lowest place in the ground is an objective function of this algorithm.<ref>{{cite journal |doi=10.1088/1757-899X/267/1/012036 |title=Optimization of Energy Efficiency and Conservation in Green Building Design Using Duelist, Killer-Whale and Rain-Water Algorithms |journal=IOP Conference Series: Materials Science and Engineering |volume=267 |issue=1 |pages=012036 |year=2017 |last1=Biyanto |first1=T R |last2=Matradji |last3=Syamsi |first3=M N |last4=Fibrianto |first4=H Y |last5=Afdanny |first5=N |last6=Rahman |first6=A H |last7=Gunawan |first7=K S |last8=Pratama |first8=J A D |last9=Malwindasari |first9=A |last10=Abdillah |first10=A I |last11=Bethiana |first11=T N |last12=Putra |first12=Y A |bibcode=2017MS&E..267a2036B }}</ref>\n\n===  {{anchor|Intelligent water drops}}Mass and Energy Balances Algorithm (Biyanto 2018) ===\nMass and Energy Balances is a fundamental \"'''laws of physics'''\" states that mass can neither be produced nor destroyed. It is only conserved. Equally fundamental is the law of conservation of energy. Although energy can change in form, it can not be created or destroyed also. The beauty of this algorithm is the capability to reach the global optimum solution by simultaneously work either \"'''minimize and maximize searching method'''\".\n\n===  {{anchor|Hydrological Cycle Algorithm}}Hydrological Cycle Algorithm (Wedyan et al. 2017) ===\nA new nature-inspired optimization algorithm called the Hydrological Cycle Algorithm (HCA) is proposed based on the continuous movement of water in nature. In the HCA, a collection of water drops passes through various hydrological water cycle stages, such as flow, evaporation, condensation, and precipitation. Each stage plays an important role in generating solutions and avoiding premature convergence. The HCA shares information by direct and indirect communication among the water drops, which improves solution quality. HCA provides an alternative approach to tackling various types of optimization problems as well as an overall framework for water-based particle algorithms in general.<ref>{{cite journal |doi=10.1155/2017/3828420 |title=Hydrological Cycle Algorithm for Continuous Optimization Problems |journal=Journal of Optimization |volume=2017 |pages=1–25 |year=2017 |last1=Wedyan |first1=Ahmad |last2=Whalley |first2=Jacqueline |last3=Narayanan |first3=Ajit }}</ref>\n\n=== {{Anchor|}}Emperor Penguins Colony (Harifi et al. 2019) ===\nThis algorithm is a new metaheuristic algorithm that inspired by the behavior of emperor penguins that live in Antarctica. EPC is controlled by the body heat radiation of the penguins and their spiral-like movement in their colony. The emperor penguins in the colony seek to create the appropriate heat and regulate their body temperature, and this heat is completely coordinated and controlled by the movement of the penguins.<ref>{{cite journal |doi=10.1007/s12065-019-00212-x |title=Emperor Penguins Colony: A new metaheuristic algorithm for optimization |journal=Evolutionary Intelligence |year=2019 |last1=Harifi |first1=Sasan |last2=Khalilian |first2=Madjid |last3=Mohammadzadeh |first3=Javad |last4=Ebrahimnejad |first4=Sadoullah }}</ref>\n\n=== Momentum Balance Algorithm (MBA) (Biyanto et al. 2019) ===\n\n{{main|Momentum Balance Algorithm}}\n\nMomentum balance is one of three fundamental \"laws of physics\" that states mass, energy and momentum is only conserved. The utilizations of momentum balance have been proposed in many applications.<ref>{{cite book |doi=10.1533/9780857099174.2.75 |chapter=Melt spinning of synthetic polymeric filaments |title=Advances in Filament Yarn Spinning of Textiles and Polymers |pages=75–99 |year=2014 |last1=Rawal |first1=A. |last2=Mukhopadhyay |first2=S. |isbn=9780857094995 }}</ref><ref>{{cite book |doi=10.1016/B978-1-85617-634-7.00014-4 |chapter=A Nonlinear Geometrically Exact Shell Model |title=The Finite Element Method for Solid and Structural Mechanics |pages=519–588 |year=2014 |last1=Zienkiewicz |first1=O.C. |last2=Taylor |first2=R.L. |last3=Fox |first3=David |isbn=9781856176347 }}</ref><ref>{{cite book |doi=10.1016/B978-0-12-803848-2.00011-8 |chapter=Multiphase Fluid and Heat Flow Coupled with Geomechanics |title=Multiphase Fluid Flow in Porous and Fractured Reservoirs |pages=265–293 |year=2016 |last1=Wu |first1=Yu-Shu |isbn=9780128038482 }}</ref>\n\nIn this research momentum balance was adopted to obtain the perfectly elastic collision. In an ideal, perfectly elastic collision, there is no kinetic energy losses into other forms such as potential energy, heat and noise. The beauty of this algorithm is easy as simple as deterministic optimization algorithms, however the momentum balance algorithm has capability to reach the global optimum solution.\n\n== {{anchor|Criticism}} Criticism of the metaphor methodology ==\nWhile individual metaphor-inspired metaheuristics have produced remarkably effective solutions to specific problems,<ref name=\"brownlee\">Alexander Brownlee and John R. Woodward (2015). [http://theconversation.com/why-we-fell-out-of-love-with-algorithms-inspired-by-nature-42718 \"Why we fell out of love with algorithms inspired by nature\"]. ''[[The Conversation (website)|The Conversation]]''.</ref> metaphor-inspired metaheuristics in general have attracted criticism in the research community for hiding their lack of effectiveness or novelty behind an elaborate metaphor.<ref name=\"brownlee\" /><ref>Jerry Swan, Steven Adriaensen, Mohamed Bishr, Edmund K. Burke, John A. Clark, Patrick De Causmaecker, Juanjo Durillo, Kevin Hammond, Emma Hart, Colin G. Johnson, Zoltan A. Kocsis, Ben Kovitz, Krzysztof Krawiec, Simon Martin, J. J. Merelo, Leandro L. Minku, Ender Özcan, Gisele L. Pappa, Erwin Pesch, Pablo Garcáa-Sánchez, Andrea Schaerf, Kevin Sim, Jim E. Smith, Thomas Stützle, Stefan Voß, Stefan Wagner, Xin Yao. [http://www.cs.nott.ac.uk/~exo/docs/publications/research-agenda-metaheuristic.pdf \"A Research Agenda for Metaheuristic Standardization\"]. \"Metaphors often inspire new metaheuristics, but without mathematical rigor, it can be hard to tell if a new metaheuristic is really distinct from a familiar one. For example, mathematically, 'Harmony search' turned out to be a simple variant of '[[Evolution Strategies]]' even though the metaphors that inspired them were quite different. Formally describing state, representation, and operators allows genuine novelty to be distinguished from minor variation.\"</ref> Kenneth Sörensen noted that:<ref>{{cite journal |doi=10.1111/itor.12001 |title=Metaheuristics-the metaphor exposed |journal=International Transactions in Operational Research |volume=22 |pages=3–18 |year=2015 |last1=Sörensen |first1=Kenneth |citeseerx=10.1.1.470.3422 }}</ref>\n<blockquote>\nIn recent years, the field of [[combinatorial optimization]] has witnessed a true tsunami of \"novel\" metaheuristic methods, most of them based on a metaphor of some natural or man-made process. The behavior of virtually any species of insects, the flow of water, musicians playing together – it seems that no idea is too far-fetched to serve as inspiration to launch yet another metaheuristic. [I] will argue that this line of research is threatening to lead the area of metaheuristics away from scientific rigor.\n</blockquote>\nSörensen and [[Fred W. Glover|Glover]] stated that:<ref>{{scholarpedia|title=Metaheuristics|urlname=Metaheuristics|curator=[[Fred W. Glover|Fred Glover]] and Kenneth Sörensen}}</ref>\n<blockquote>\nA large (and increasing) number of publications focuses on the development of (supposedly) new metaheuristic frameworks based on metaphors. The list of natural or man-made processes that has been used as the basis for a metaheuristic framework now includes such diverse processes as bacterial foraging, [[#River formation dynamics|river formation]], biogeography, musicians playing together, electromagnetism, [[#Gravitational search algorithm|gravity]], [[Imperialist competitive algorithm|colonization by an empire]], mine blasts, league championships, clouds, and so forth. An important subcategory is found in metaheuristics based on animal behavior. [[Ant colony optimization algorithms|Ants]], bees, [[Bat algorithm|bats]], wolves, cats, [[Firefly algorithm|fireflies]], [[Eagle strategy|eagles]], dolphins, [[Shuffled frog leaping algorithm|frogs]], salmon, vultures, termites, flies, and many others, have all been used to inspire a \"novel\" metaheuristic. [...] As a general rule, publication of papers on metaphor-based metaheuristics has been limited to second-tier journals and conferences, but some recent exceptions to this rule can be found. Sörensen (2013) states that research in this direction is fundamentally flawed. Most importantly, the author contends that the novelty of the underlying metaphor does not automatically render the resulting framework \"novel\". On the contrary, there is increasing evidence that very few of the metaphor-based methods are new in any interesting sense.\n</blockquote>\nIn response, [[Springer Science+Business Media|Springer]]'s ''Journal of Heuristics'' has updated their editorial policy to state that:<ref>[https://www.springer.com/cda/content/document/cda_downloaddocument/Journal+of+Heuristic+Policies+on+Heuristic+Search.pdf?SGWID=0-0-45-1483502-p35487524 Journal of Heuristic Policies on Heuristic Search Research]. Springer.</ref>\n<blockquote>\nProposing new paradigms is only acceptable if they contain innovative basic ideas, such as those that are embedded in classical frameworks like [[genetic algorithm]]s, [[tabu search]], and [[simulated annealing]]. The Journal of Heuristics avoids the publication of articles that repackage and embed old ideas in methods that are claimed to be based on metaphors of natural or manmade systems and processes. These so-called \"novel\" methods employ analogies that range from [[Intelligent water drops algorithm|intelligent water drops]], musicians playing jazz, [[Imperialist competitive algorithm|imperialist societies]], [[Shuffled frog leaping algorithm|leapfrogs]], kangaroos, all types of swarms and insects and even mine blast processes (Sörensen, 2013). If a researcher uses a metaphor to stimulate his or her own ideas about a new method, the method must nevertheless be translated into metaphor-free language, so that the strategies employed can be clearly understood, and their novelty is made clearly visible. (See items 2 and 3 below.) Metaphors are cheap and easy to come by. Their use to \"window dress\" a method is not acceptable.\"\n\n[...] Implementations should be explained by employing standard optimization terminology, where a solution is called a \"solution\" and not something else related to some obscure metaphor (e.g., harmony, [[Firefly algorithm|flies]], [[bat algorithm|bats]], [[Imperialist competitive algorithm|countries]], etc.).\n\n[...] The Journal of Heuristics fully endorses Sörensen’s view that metaphor-based “novel” methods should not be published if they cannot demonstrate a contribution to their field. Renaming existing concepts does not count as a contribution. Even though these methods are often called “novel”, many present no new ideas, except for the occasional marginal variant of an already existing methodology. These methods should not take the journal space of truly innovative ideas and\nresearch. Since they do not use the standard optimization vocabulary, they are unnecessarily difficult to understand.\n</blockquote>\n\nThe policy of [[Springer Science+Business Media|Springer]]'s journal [[4OR - A Quarterly Journal of Operations Research]] states<ref>{{Cite web | url=https://www.springer.com/business+%26+management/operations+research/journal/10288 | title=4OR – incl. Option to publish open access}}</ref>\n\nThe emphasis on scientific rigor and on innovation implies, in particular, that the journal does not publish articles that simply propose disguised variants of known methods without adequate validation (e.g., metaheuristics that are claimed to be \"effective\" on the sole basis of metaphorical comparisons with natural or artificial systems and processes). New methods must be presented in metaphor-free language by establishing their relationship with classical paradigms. Their properties must be established on the basis of scientifically compelling arguments: mathematical proofs, controlled experiments, objective comparisons, etc.\n\n== See also ==\n\n*[[Swarm intelligence#Algorithms]]\n\n== Notes ==\n{{reflist|2|refs=\n<ref name=kennedy95particle>{{cite book |doi=10.1109/ICNN.1995.488968 |chapter=Particle swarm optimization |title=Proceedings of ICNN'95 - International Conference on Neural Networks |volume=4 |pages=1942–8 |year=1995 |last1=Kennedy |first1=J. |last2=Eberhart |first2=R. |isbn=978-0-7803-2768-9 |citeseerx=10.1.1.709.6654 }}</ref>\n\n<ref name=kennedy97particle>{{cite book |doi=10.1109/ICEC.1997.592326 |chapter=The particle swarm: Social adaptation of knowledge |title=Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97) |pages=303–8 |year=1997 |last1=Kennedy |first1=J. |isbn=978-0-7803-3949-1 }}</ref>\n\n<ref name=shi98modified>{{cite book |doi=10.1109/ICEC.1998.699146 |chapter=A modified particle swarm optimizer |title=1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360) |pages=69–73 |year=1998 |last1=Shi |first1=Y. |last2=Eberhart |first2=R. |isbn=978-0-7803-4869-1 }}</ref>\n\n<ref name=kennedy01swarm>\n{{cite book\n|title=Swarm Intelligence\n|last1=Kennedy\n|first1=J.\n|last2=Eberhart\n|first2=R.C.\n|year=2001\n|publisher=Morgan Kaufmann\n|isbn=978-1-55860-595-4\n}}\n</ref>\n\n<ref name=poli07analysis>\n{{cite journal\n|last=Poli\n|first=R.\n|url=http://cswww.essex.ac.uk/technical-reports/2007/tr-csm469.pdf\n|title=An analysis of publications on particle swarm optimisation applications\n|journal=Technical Report CSM-469\n|publisher=Department of Computer Science, University of Essex, UK\n|year=2007\n}}\n</ref>\n\n<ref name=poli08analysis>{{cite journal |doi=10.1155/2008/685175 |title=Analysis of the Publications on the Applications of Particle Swarm Optimisation |journal=Journal of Artificial Evolution and Applications |volume=2008 |pages=1–10 |year=2008 |last1=Poli |first1=Riccardo }}</ref>\n\n<ref name=bonyadi16survey>{{cite journal |doi=10.1162/EVCO_r_00180 |pmid=26953883 |title=Particle Swarm Optimization for Single Objective Continuous Space Problems: A Review |journal=Evolutionary Computation |volume=25 |issue=1 |pages=1–54 |year=2017 |last1=Bonyadi |first1=Mohammad Reza |last2=Michalewicz |first2=Zbigniew }}</ref>\n}}\n\n== References ==\n\n*{{cite book|first1=Kenneth|last1=Sörensen|first2=Marc|last2=Sevaux|first3=Fred|last3=Glover|editor-last1=Martí|editor-first1=Rafael|editor-last2=Panos|editor-first2=Pardalos|editor-last3=Resende|editor-first3=Mauricio|isbn=978-3-319-07123-7|title=Handbook of Heuristics|chapter=A History of Metaheuristics|publisher=Springer|chapter-url=http://leeds-faculty.colorado.edu/glover/468%20-%20A%20History%20of%20Metaheuristics%20w%20Sorensen%20%26%20Sevaux.pdf|date=2017-01-16}}\n*{{cite journal |doi=10.1111/itor.12001 |title=Metaheuristics-the metaphor exposed |journal=International Transactions in Operational Research |volume=22 |pages=3–18 |year=2015 |last1=Sörensen |first1=Kenneth |citeseerx=10.1.1.470.3422 }}\n*{{cite book |doi=10.1145/2598394.2609841 |chapter=Metaheuristics in nature-inspired algorithms |title=Proceedings of the 2014 conference companion on Genetic and evolutionary computation companion - GECCO Comp '14 |pages=1419–22 |year=2014 |last1=Lones |first1=Michael A. |isbn=9781450328814 |citeseerx=10.1.1.699.1825 }}\n*{{Cite journal |bibcode=2013arXiv1307.4186F |title=A Brief Review of Nature-Inspired Algorithms for Optimization |journal=Elektrotehniški Vestnik |volume=1307 |issue=3 |pages=arXiv:1307.4186 |author1=Fister |first1=Iztok |last2=Yang |first2=Xin-She |last3=Fister |first3=Iztok |last4=Brest |first4=Janez |last5=Fister |first5=Dušan |year=2013 |arxiv=1307.4186 }}\n\n== External links ==\n\n*[https://github.com/fcampelo/EC-Bestiary Evolutionary Computation Bestiary] &mdash; a tongue-in-cheek account of all the weird, even bizarre metaphor-based metaheuristics out there in the wide world of academic publishing\n*[https://thesciencematrix.com/Apps/metaheuristics/ The Science Matrix's List of Metaheuristic] &mdash; a complete list of metaheuristic algorithms. The list can be easily filter by Name, Author or Year, and provides the link to the main publication of each algorithm.\n\n[[Category:Nature-inspired metaheuristics| ]]"
    },
    {
      "title": "Swarm intelligence",
      "url": "https://en.wikipedia.org/wiki/Swarm_intelligence",
      "text": "'''Swarm intelligence''' ('''SI''') is the [[collective behavior]] of [[decentralization|decentralized]], [[Self-organization|self-organized]] systems, natural or artificial. The concept is employed in work on [[artificial intelligence]]. The expression was introduced by [[Gerardo Beni]] and Jing Wang in 1989, in the context of cellular robotic systems.<ref>{{cite book|author=Beni, G., Wang, J.|chapter=Swarm Intelligence in Cellular Robotic Systems|title=Proceed. NATO Advanced Workshop on Robots and Biological Systems, Tuscany, Italy, June 26–30 (1989)|pages=703–712|doi=10.1007/978-3-642-58069-7_38|year=1993|isbn=978-3-642-63461-1}}</ref>\n\nSI systems consist typically of a population of simple [[Intelligent agent|agents]] or [[boids]] interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the [[emergence]] of \"intelligent\" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include [[ant colony|ant colonies]], bird [[flocking (behavior)|flocking]], hawks [[hunting]], animal [[herding]], [[bacteria#Growth and reproduction|bacterial growth]], fish [[shoaling and schooling|schooling]] and [[microbial intelligence]].\n\nThe application of swarm principles to [[robot]]s is called [[swarm robotics]], while 'swarm intelligence' refers to the more general set of algorithms. 'Swarm prediction' has been used in the context of forecasting problems.\n\n== Models of swarm behavior ==\n{{see also|Swarm behaviour}}\n\n=== Boids (Reynolds 1987) ===\n{{main|Boids}}\nBoids is an [[artificial life]] program, developed by [[Craig Reynolds (computer graphics)|Craig Reynolds]] in 1986, which simulates the [[Flocking (behavior)|flocking]] behaviour of birds. His paper on this topic was published in 1987 in the proceedings of the [[Association for Computing Machinery|ACM]] [[SIGGRAPH]] conference.<ref>{{Cite book\n | last1=Reynolds\n | first1=Craig\n | author1-link=Craig Reynolds (computer_graphics)\n | title=Flocks, herds and schools: A distributed behavioral model.\n | year=1987\n | volume=\n | number=\n | journal=SIGGRAPH '87: Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques\n | publisher=[[Association for Computing Machinery]]\n | pages=25–34\n | isbn=978-0-89791-227-3\n | doi=10.1145/37401.37406\n| citeseerx=10.1.1.103.7187\n }}</ref>\nThe name \"boid\" corresponds to a shortened version of \"bird-oid object\", which refers to a bird-like object.<ref>{{Cite journal\n | last1=Banks\n | first1=Alec\n | last2=Vincent\n | first2=Jonathan\n | last3=Anyakoha\n | first3=Chukwudi\n | title=A review of particle swarm optimization. Part I: background and development\n |date=July 2007\n\n | volume=6\n | issue=4\n | pages=467–484\n | journal=Natural Computing\n | doi=10.1007/s11047-007-9049-5\n| citeseerx=10.1.1.605.5879\n }}</ref>\n\nAs with most artificial life simulations, Boids is an example of [[emergence|emergent]] behavior; that is, the complexity of Boids arises from the interaction of individual agents (the boids, in this case) adhering to a set of simple rules.  The rules applied in the simplest Boids world are as follows:\n\n* '''separation''': [[wikt:steer#Verb|steer]] to avoid crowding local flockmates\n* '''alignment''': steer towards the average heading of local flockmates\n* '''cohesion''': steer to move toward the average position (center of mass) of local flockmates\n\nMore complex rules can be added, such as obstacle avoidance and goal seeking.\n\n=== Self-propelled particles (Vicsek ''et al''.  1995) ===\n{{main|Self-propelled particles}}\nSelf-propelled particles (SPP), also referred to as the ''Vicsek model'', was introduced in 1995 by [[Tamás Vicsek|Vicsek]] ''et al.''<ref name=\"Vicsek1995\">{{cite journal | last1 = Vicsek | first1 = T. |authorlink1=Tamás Vicsek| last2 = Czirok | first2 = A. | last3 = Ben-Jacob | first3 = E.; | last4 = Cohen | first4 = I. | last5 = Shochet | first5 = O. | year = 1995 | arxiv = cond-mat/0611743 | title = Novel type of phase transition in a system of self-driven particles | journal = [[Physical Review Letters]] | volume = 75 | issue = 6 | pages = 1226–1229 | doi = 10.1103/PhysRevLett.75.1226 | pmid=10060237|bibcode = 1995PhRvL..75.1226V }}</ref> as a special case of the [[boids]] model introduced in 1986 by [[Craig Reynolds (computer graphics)|Reynolds]].<ref>{{cite book | last = Reynolds | first = C. W. | year = 1987 | citeseerx = 10.1.1.103.7187 | title = Flocks, herds and schools: A distributed behavioral model | journal = Computer Graphics | volume = 21 | issue = 4 | pages = 25–34 | doi = 10.1145/37401.37406 | isbn = 978-0897912273 }}</ref> A swarm is modelled in SPP by a collection of particles that move with a constant speed but respond to a random perturbation by adopting at each time increment the average direction of motion of the other particles in their local neighbourhood.<ref>{{cite journal | last1 = Czirók | first1 = A. | last2 = Vicsek | first2 = T. | year = 2006 | arxiv = cond-mat/0611742 | title = Collective behavior of interacting self-propelled particles | journal = [[Physica A]] | volume = 281 | issue = 1 | pages = 17–29 | doi = 10.1016/S0378-4371(00)00013-3 | bibcode=2000PhyA..281...17C}}</ref> SPP models predict that swarming animals share certain properties at the group level, regardless of the type of animals in the swarm.<ref name=\"Buhl et al\">{{cite journal | last1 = Buhl | first1 = J. | last2 = Sumpter | first2 = D.J.T. | last3 = Couzin | first3 = D. | last4 = Hale | first4 = J.J. | last5 = Despland | first5 = E. | last6 = Miller | first6 = E.R. | last7 = Simpson | first7 = S.J.| year = 2006 | title = From disorder to order in marching locusts | url = http://webscript.princeton.edu/~icouzin/website/wp-content/plugins/bib2html/data/papers/buhl06.pdf | journal = Science | volume = 312 | issue = 5778| pages = 1402–1406 | doi = 10.1126/science.1125142 | pmid = 16741126 |bibcode = 2006Sci...312.1402B |display-authors=etal}}</ref> Swarming systems give rise to [[emergent behaviour]]s which occur at many different scales, some of which are turning out to be both universal and robust. It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours.<ref>{{cite journal | last1 = Toner | first1 = J. | last2 = Tu | first2 = Y. | last3 = Ramaswamy | first3 = S. | year = 2005 | title = Hydrodynamics and phases of flocks | url = http://eprints.iisc.ernet.in/3397/1/A89.pdf | journal = Annals of Physics | volume = 318 | issue = 1| pages = 170–244 |bibcode = 2005AnPhy.318..170T |doi = 10.1016/j.aop.2005.04.011 }}</ref><ref name=\"Bertin et al\">{{cite journal | last1 = Bertin | first1 = E. | last2 = Droz | first2 = M. | last3 = Grégoire | first3 = G. | year = 2009 | arxiv = 0907.4688 | title = Hydrodynamic equations for self-propelled particles: microscopic derivation and stability analysis | journal = [[J. Phys. A]] | volume = 42 | issue = 44 | page = 445001 | doi = 10.1088/1751-8113/42/44/445001 |bibcode = 2009JPhA...42R5001B }}</ref><ref name=\"Li et al\">{{cite journal | last1 = Li | first1 = Y.X. | last2 = Lukeman | first2 = R. | last3 = Edelstein-Keshet | first3 = L. | year = 2007 | title = Minimal mechanisms for school formation in self-propelled particles | url = http://www.iam.ubc.ca/~lukeman/fish_school_f.pdf | journal = Physica D: Nonlinear Phenomena | volume = 237 | issue = 5 | pages = 699–720 | doi = 10.1016/j.physd.2007.10.009 | bibcode = 2008PhyD..237..699L | display-authors = etal }}{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\n== Metaheuristics ==\n{{see also|List of metaphor-based metaheuristics}}\n[[Evolutionary algorithm]]s (EA), [[particle swarm optimization]] (PSO), [[Differential Evolution]] (DE), [[ant colony optimization]] (ACO) and their variants dominate the field of nature-inspired [[metaheuristic]]s.<ref>{{cite book|first=Michael A.|last=Lones|year=2014|title=Metaheuristics in Nature-Inspired Algorithms|journal=[[Genetic and Evolutionary Computation Conference|GECCO '14]]|pages=1419–1422|url=http://www.macs.hw.ac.uk/~ml355/common/papers/lones-gecco2014-metaheuristics.pdf|doi=10.1145/2598394.2609841|isbn=9781450328814|citeseerx=10.1.1.699.1825}}</ref> This list includes algorithms published up to circa the year 2000. A large number of more recent metaphor-inspired metaheuristics have started to [[List of metaphor-inspired metaheuristics#Criticism|attract criticism in the research community]] for hiding their lack of novelty behind an elaborate metaphor. For algorithms published since that time, see [[List of metaphor-based metaheuristics]].\n\n=== Stochastic diffusion search (Bishop 1989) ===\n{{main|Stochastic diffusion search}}\nFirst published in 1989 Stochastic diffusion search (SDS)<ref>Bishop, J.M., Stochastic Searching Networks, Proc. 1st IEE Int. Conf. on Artificial Neural Networks, pp. 329-331, London, UK, (1989).</ref><ref>Nasuto, S.J. & Bishop, J.M., (2008), Stabilizing swarm intelligence search via positive feedback resource allocation, In: Krasnogor, N., Nicosia, G, Pavone, M., & Pelta, D. (eds), Nature Inspired Cooperative Strategies for Optimization, Studies in Computational Intelligence, vol 129, Springer, Berlin, Heidelberg, New York, pp. 115-123.</ref> was the first Swarm Intelligence metaheuristic. SDS is an agent-based [[Probabilistic algorithm|probabilistic]] global search and optimization technique best suited to problems where the objective function can be decomposed into multiple independent partial-functions. Each agent maintains a hypothesis which is iteratively tested by evaluating a randomly selected partial objective function parameterised by the agent's current hypothesis. In the standard version of SDS such partial function evaluations are binary, resulting in each agent becoming active or inactive. Information on hypotheses is diffused across the population via inter-agent communication. Unlike the [[Stigmergy|stigmergic]] communication used in ACO, in SDS agents communicate [[hypothesis|hypotheses]] via a one-to-one communication strategy analogous to the [[tandem running]] procedure observed in [[Leptothorax acervorum]].<ref>Moglich, M.; Maschwitz, U.; Holldobler, B., Tandem Calling: A New Kind of Signal in Ant Communication, Science, Volume 186, Issue 4168, pp. 1046-1047</ref> A positive feedback mechanism ensures that, over time, a population of agents stabilise around the global-best solution. SDS is both an efficient and robust global search and optimisation algorithm, which has been extensively mathematically described.<ref>Nasuto, S.J., Bishop, J.M. & Lauria, S., Time complexity analysis of the Stochastic Diffusion Search, Proc. Neural Computation '98, pp. 260-266, Vienna, Austria, (1998).</ref><ref>Nasuto, S.J., & Bishop, J.M., (1999), Convergence of the Stochastic Diffusion Search, Parallel Algorithms, 14:2, pp: 89-107.</ref><ref>Myatt, D.M., Bishop, J.M., Nasuto, S.J., (2004), Minimum stable convergence criteria for Stochastic Diffusion Search, Electronics Letters, 22:40, pp. 112-113.</ref> Recent work has involved merging the global search properties of SDS with other swarm intelligence algorithms.<ref>al-Rifaie, M.M., Bishop, J.M. & Blackwell, T., An investigation into the merger of stochastic diffusion search and particle swarm optimisation, Proc. 13th Conf. Genetic and Evolutionary Computation, (GECCO), pp.37-44, (2012).</ref><ref>al-Rifaie, Mohammad Majid, John Mark Bishop, and Tim Blackwell. \"Information sharing impact of stochastic diffusion search on differential evolution algorithm.\" Memetic Computing 4.4 (2012): 327-338.</ref>\n\n=== Ant colony optimization (Dorigo 1992) ===\n{{main|Ant colony optimization}}\nAnt colony optimization (ACO), introduced by Dorigo in his doctoral dissertation, is a class of [[Optimization (mathematics)|optimization]] [[algorithm]]s modeled on the actions of an [[ant colony]]. ACO is a [[Probabilistic algorithm|probabilistic technique]] useful in problems that deal with finding better paths through graphs. Artificial 'ants'—simulation agents—locate optimal solutions by moving through a [[parameter space]] representing all possible solutions. Natural ants lay down [[pheromone]]s directing each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate for better solutions.<ref>Ant Colony Optimization by Marco Dorigo and Thomas Stützle, MIT Press, 2004. {{ISBN|0-262-04219-3}}</ref>\n\n=== Particle swarm optimization (Kennedy, Eberhart & Shi 1995) ===\n{{main|Particle swarm optimization}}\nParticle swarm optimization (PSO) is a [[global optimization]] algorithm for dealing with problems in which a best solution can be represented as a point or surface in an n-dimensional space.  Hypotheses are plotted in this space and seeded with an initial [[velocity]], as well as a communication channel between the particles.<ref>{{cite journal |doi=10.1023/A:1016568309421 |title=Recent Approaches to Global Optimization Problems Through Particle Swarm Optimization |last=Parsopoulos |first=K. E. |last2=Vrahatis |first2=M. N. |journal=Natural Computing |volume=1 |issue=2–3 |pages=235–306 |year=2002 }}</ref><ref>[http://www.iste.co.uk/?searchtext=clerc&ACTION=Search&cat=&ACTION=Search Particle Swarm Optimization] by Maurice Clerc, ISTE, {{ISBN|1-905209-04-5}}, 2006.</ref>  Particles then move through the solution space, and are evaluated according to some [[fitness (biology)|fitness]] criterion after each timestep. Over time, particles are accelerated towards those particles within their communication grouping which have better fitness values. The main advantage of such an approach over other global minimization strategies such as [[simulated annealing]] is that the large number of members that make up the particle swarm make the technique impressively resilient to the problem of [[local minima]].\n\n==Applications==\nSwarm Intelligence-based techniques can be used in a number of applications.  The U.S. military is investigating swarm techniques for controlling unmanned vehicles. The [[European Space Agency]] is thinking about an orbital swarm for self-assembly and interferometry. [[NASA]] is investigating the use of swarm technology for planetary mapping.  A 1992 paper by [[M. Anthony Lewis (roboticist)|M. Anthony Lewis]] and [[George A. Bekey]] discusses the possibility of using swarm intelligence to control nanobots within the body for the purpose of killing cancer tumors.<ref>{{cite journal |last=Lewis |first=M. Anthony |last2=Bekey |first2=George A. |title=The Behavioral Self-Organization of Nanorobots Using Local Rules |journal=Proceedings of the 1992 IEEE/RSJ International Conference on Intelligent Robots and Systems }}</ref> Conversely al-Rifaie and Aber have used [[stochastic diffusion search]] to help locate tumours.<ref>{{cite journal | last1 = al-Rifaie | first1 = M.M. | last2 = Aber | first2 = A. | year = | title = Identifying metastasis in bone scans with Stochastic Diffusion Search | url = | journal = Proc. IEEE Information Technology in Medicine and Education, ITME | volume = 2012 | issue = | pages = 519–523 }}</ref><ref>al-Rifaie, Mohammad Majid, Ahmed Aber, and Ahmed Majid Oudah. \"Utilising Stochastic Diffusion Search to identify metastasis in bone scans and microcalcifications on mammographs.\" In Bioinformatics and Biomedicine Workshops (BIBMW), 2012 IEEE International Conference on, pp. 280-287. IEEE, 2012.</ref> Swarm intelligence has also been applied for [[data mining]].<ref>{{cite journal |first=D. |last=Martens |first2=B. |last2=Baesens |first3=T. |last3=Fawcett |title=Editorial Survey: Swarm Intelligence for Data Mining |journal=Machine Learning |volume=82 |issue=1 |pages=1–42 |year=2011 |doi=10.1007/s10994-010-5216-5 }}</ref>\n\n===Ant-based routing===\nThe use of swarm intelligence in [[Telecommunications network|telecommunication networks]] has also been researched, in the form of [[Ant colony optimization algorithms|ant-based routing]]. This was pioneered separately by Dorigo et al. and [[Hewlett Packard]] in the mid-1990s, with a number of variants are existed. Basically, this uses a [[Probabilistic algorithm|probabilistic]] routing table rewarding/reinforcing the route successfully traversed by each \"ant\" (a small control packet) which flood the network. Reinforcement of the route in the forwards, reverse direction and both simultaneously have been researched: backwards reinforcement requires a symmetric network and couples the two directions together; forwards reinforcement rewards a route before the outcome is known (but then one would pay for the cinema before one knows how good the film is). As the system behaves stochastically and is therefore lacking repeatability, there are large hurdles to commercial deployment. Mobile media and new technologies have the potential to change the threshold for collective action due to swarm intelligence (Rheingold: 2002, P175).\n\nThe location of transmission infrastructure for wireless communication networks is an important engineering problem involving competing objectives. A minimal selection of locations (or sites) are required subject to providing adequate area coverage for users. A very different-ant inspired swarm intelligence algorithm, stochastic diffusion search (SDS), has been successfully used to provide a general model for this problem, related to circle packing and set covering. It has been shown that the SDS can be applied to identify suitable solutions even for large problem instances.<ref>Whitaker, R.M., Hurley, S.. An agent based approach to site selection for wireless networks. Proc ACM Symposium on Applied Computing, pp. 574–577, (2002).</ref>\n\nAirlines have also used ant-based routing in assigning aircraft arrivals to airport gates. At [[Southwest Airlines]] a software program uses swarm theory, or swarm intelligence—the idea that a colony of ants works better than one alone. Each pilot acts like an ant searching for the best airport gate. \"The pilot learns from his experience what's the best for him, and it turns out that that's the best solution for the airline,\" [[Douglas A. Lawson]] explains. As a result, the \"colony\" of pilots always go to gates they can arrive at and depart from quickly. The program can even alert a pilot of plane back-ups before they happen. \"We can anticipate that it's going to happen, so we'll have a gate available,\" Lawson says.<ref>{{cite news |work=Science Daily |date=April 1, 2008 |title=Planes, Trains and Ant Hills: Computer scientists simulate activity of ants to reduce airline delays |url=https://www.sciencedaily.com/videos/2008/0406-planes_trains_and_ant_hills.htm |accessdate=December 1, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20101124132227/https://www.sciencedaily.com/videos/2008/0406-planes_trains_and_ant_hills.htm |archivedate=November 24, 2010 }}</ref>\n\n===Crowd simulation===\nArtists are using swarm technology as a means of creating complex interactive systems or [[Crowd simulation|simulating crowds]].\n\n''[[Stanley and Stella in: Breaking the Ice]]'' was the first movie to make use of swarm technology for rendering, realistically depicting the movements of groups of fish and birds using the Boids system. Tim Burton's ''[[Batman Returns]]'' also made use of swarm technology for showing the movements of a group of bats. [[The Lord of the Rings (film series)|''The Lord of the Rings'' film trilogy]] made use of similar technology, known as [[Massive (software)|Massive]], during battle scenes. Swarm technology is particularly attractive because it is cheap, robust, and simple.\n\nAirlines have used swarm theory to simulate passengers boarding a plane. Southwest Airlines researcher Douglas A. Lawson used an ant-based computer simulation employing only six interaction rules to evaluate boarding times using various boarding methods.(Miller, 2010, xii-xviii).<ref>{{cite book |last=Miller |first=Peter |year=2010 |title=The Smart Swarm: How understanding flocks, schools, and colonies can make us better at communicating, decision making, and getting things done |publisher=Avery |location=New York |isbn=978-1-58333-390-7 }}</ref>\n\n===Human swarming===\n\nEnabled by mediating software such as the SWARM platform (formally unu) from [[Unanimous A.I.]], networks of distributed users can be organized into \"human swarms\" through the implementation of real-time closed-loop control systems.<ref>{{Cite web|url=http://www.bbc.com/future/story/20161215-why-bees-could-be-the-secret-to-superhuman-intelligence|title=Why bees could be the secret to superhuman intelligence|last=Oxenham|first=Simon|access-date=2017-01-20}}</ref><ref name=\"Inc.com\">{{Cite news|url=https://www.inc.com/kevin-j-ryan/unanimous-ai-swarm-intelligence-makes-startlingly-accurate-predictions.html|title=This Startup Correctly Predicted the Oscars, World Series, and Super Bowl. Here's What It's Doing Next|date=2018-06-14|work=Inc.com|access-date=2018-09-10}}</ref><ref name=\"Rosenberg 58–62\">{{Cite book|last=Rosenberg|first=L.|last2=Pescetelli|first2=N.|last3=Willcox|first3=G.|date=October 2017|title=Artificial Swarm Intelligence amplifies accuracy when predicting financial markets|journal=2017 IEEE 8th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON)|pages=58–62|doi=10.1109/UEMCON.2017.8248984|isbn=978-1-5386-1104-3}}</ref><ref name=\"Inc.com\"/> As published by [[Louis B. Rosenberg|Rosenberg]] (2015), such real-time systems enable groups of human participants to behave as a unified [[collective intelligence]] that works as a single entity to make predictions, answer questions, and evoke opinions.<ref>http://sites.lsa.umich.edu/collectiveintelligence/wp-content/uploads/sites/176/2015/05/Rosenberg-CI-2015-Abstract.pdf</ref> Such systems, also referred to as \"Artificial Swarm Intelligence\" (or the brand name Swarm AI) have been shown to significantly amplify human intelligence, resulting in a string of high-profile predictions of extreme accuracy.<ref>{{Cite news|url=http://www.newsweek.com/artificial-intelligence-turns-20-11000-kentucky-derby-bet-457783|title=Artificial intelligence turns $20 into $11,000 in Kentucky Derby bet|date=2016-05-10|newspaper=Newsweek|access-date=2017-01-20}}</ref><ref>{{Cite news|url=https://www.forbes.com/sites/janetwburns/2017/01/19/ai-that-clinched-the-trifecta-gave-the-super-bowl-to-green-bay-in-august/#679482212e74|title=AI That Clinched The Trifecta Gave The Super Bowl To Green Bay--In August|last=Burns|first=Janet|newspaper=Forbes|access-date=2017-01-20}}</ref><ref>{{cite web|url=https://mitpress.mit.edu/sites/default/files/titles/content/ecal2015/ch117.html|title=Human Swarms, a real-time method for collective intelligence|access-date=2015-10-12|archive-url=https://web.archive.org/web/20151027132802/https://mitpress.mit.edu/sites/default/files/titles/content/ecal2015/ch117.html|archive-date=2015-10-27|dead-url=yes}}</ref><ref>{{cite web|url=http://news.discovery.com/human/life/swarms-of-humans-power-a-i-platform-150603.htm|title=Swarms of Humans Power A.I. Platform|work=DNews|date=2017-05-10}}</ref><ref name=\"Inc.com\"/><ref>{{Cite news|url=https://www.techrepublic.com/article/how-ai-systems-beat-vegas-oddsmakers-in-sports-forecasting-accuracy/|title=How AI systems beat Vegas oddsmakers in sports forecasting accuracy|work=TechRepublic|access-date=2018-09-10}}</ref> Academic testing shows that human swarms can out-predict individuals across a variety of real-world projections.<ref>{{Cite book|last=Rosenberg|first=L.|last2=Baltaxe|first2=D.|last3=Pescetelli|first3=N.|date=2016-10-01|title=Crowds vs swarms, a comparison of intelligence|journal=2016 Swarm/Human Blended Intelligence Workshop (SHBI)|pages=1–4|doi=10.1109/SHBI.2016.7780278|isbn=978-1-5090-3502-1}}</ref><ref>{{Cite book |doi = 10.1109/SHBI.2015.7321685|chapter = Human swarming, a real-time method for parallel distributed intelligence|title = 2015 Swarm/Human Blended Intelligence Workshop (SHBI)|pages = 1–7|year = 2015|last1 = Rosenberg|first1 = Louis B.|isbn = 978-1-4673-6522-2}}</ref><ref name=\"Rosenberg 58–62\"/><ref>{{Cite book|last=Rosenberg|first=L.|last2=Willcox|first2=G.|date=June 2018|title=Artificial Swarms find Social Optima : (Late Breaking Report)|journal=2018 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)|pages=174–178|doi=10.1109/COGSIMA.2018.8423987|isbn=978-1-5386-5288-6}}</ref><ref>{{Cite book|last=Rosenberg|first=L.|last2=Pescetelli|first2=N.|date=September 2017|title=Amplifying prediction accuracy using Swarm A.I.|journal=2017 Intelligent Systems Conference (IntelliSys)|pages=61–65|doi=10.1109/IntelliSys.2017.8324329|isbn=978-1-5090-6435-9}}</ref> Famously, human swarming was used to correctly predict the Kentucky Derby Superfecta, against 541 to 1 odds, in response to a challenge from reporters.<ref>{{Cite news|url=https://www.newsweek.com/artificial-intelligence-turns-20-11000-kentucky-derby-bet-457783|title=Artificial intelligence turns $20 into $11,000 in Kentucky Derby bet|date=2016-05-10|work=Newsweek|access-date=2018-09-10}}</ref>\n\n===Swarm grammars===\nSwarm grammars are swarms of [[stochastic grammar]]s that can be evolved to describe complex properties such as found in art and architecture.<ref>{{cite journal|last1=vonMammen|first1=Sebastian|last2=Jacob|first2=Christian|title=The evolution of swarm grammars -- growing trees, crafting art and bottom-up design|journal=Computational Intelligence|volume=4|issue=3|pages=10–19|date=2009|url=http://journal.frontiersin.org/article/10.3389/fncom.2015.00090/full|doi=10.1109/MCI.2009.933096|citeseerx=10.1.1.384.9486}}</ref> These grammars interact as agents behaving according to rules of swarm intelligence. Such behavior can also suggest [[deep learning]] algorithms, in particular when mapping of such swarms to neural circuits is considered.<ref>{{cite journal|last1=du Castel|first1=Bertrand|title=Pattern Activation/Recognition Theory of Mind|journal=Frontiers in Computational Neuroscience|volume=9|issue=90|pages=90|date = 15 July 2015|doi=10.3389/fncom.2015.00090|pmid=26236228|pmc=4502584|ref=neuroscience}}</ref>\n\n===Swarmic art===\nIn a series of works al-Rifaie et al.<ref name=\":1\">{{cite journal | last1 = al-Rifaie | first1 = MM | last2 = Bishop | first2 = J.M. | last3 = Caines | first3 = S. | year = 2012 | title = Creativity and Autonomy in Swarm Intelligence Systems | url = http://research.gold.ac.uk/17273/1/2012_CC_updated.pdf| journal = Cognitive Computing | volume = 4 | issue = 3| pages = 320–331 | doi=10.1007/s12559-012-9130-y}}</ref> have successfully used two swarm intelligence algorithms—one mimicking the behaviour of one species of ants (''Leptothorax acervorum'') foraging ([[stochastic diffusion search]], SDS) and the other algorithm mimicking the behaviour of birds flocking ([[particle swarm optimization]], PSO)—to describe a novel integration strategy exploiting the local search properties of the PSO with global SDS behaviour. The resulting [[hybrid algorithm]] is used to sketch novel drawings of an input image, exploiting an artistic tension between the local behaviour of the 'birds flocking'—as they seek to follow the input sketch—and the global behaviour of the \"ants foraging\"—as they seek to encourage the flock to explore novel regions of the canvas. The \"creativity\" of this hybrid swarm system has been analysed under the philosophical light of the \"rhizome\" in the context of [[Deleuze]]'s \"Orchid and Wasp\" metaphor.<ref>Deleuze G, Guattari F, Massumi B. A thousand plateaus. Minneapolis: University of Minnesota Press; 2004.</ref>\n\nIn a more recent work of al-Rifaie et al., \"Swarmic Sketches and Attention Mechanism\",<ref>{{Cite book | doi=10.1007/978-3-642-36955-1_8|chapter = Swarmic Sketches and Attention Mechanism|title = Evolutionary and Biologically Inspired Music, Sound, Art and Design| volume=7834| pages=85–96|series = Lecture Notes in Computer Science|year = 2013|last1 = Al-Rifaie|first1 = Mohammad Majid| last2=Bishop| first2=John Mark| isbn=978-3-642-36954-4| chapter-url=http://research.gold.ac.uk/17268/1/2013_EvoMUSART_sketches_April%283-5%29.pdf}}</ref> introduces a novel approach deploying the mechanism of 'attention' by adapting SDS to selectively attend to detailed areas of a digital canvas. Once the attention of the swarm is drawn to a certain line within the canvas, the capability of PSO is used to produce a 'swarmic sketch' of the attended line. The swarms move throughout the digital canvas in an attempt to satisfy their dynamic roles—attention to areas with more details—associated to them via their fitness function. Having associated the rendering process with the concepts of attention, the performance of the participating swarms creates a unique, non-identical sketch each time the 'artist' swarms embark on interpreting the input line drawings. In other works while PSO is responsible for the sketching process, SDS controls the attention of the swarm.\n\nIn a similar work, \"Swarmic Paintings and Colour Attention\",<ref>al-Rifaie, Mohammad Majid, and John Mark Bishop. \"[https://link.springer.com/chapter/10.1007%2F978-3-642-36955-1_9 Swarmic paintings and colour attention]\". Evolutionary and Biologically Inspired Music, Sound, Art and Design. Springer Berlin Heidelberg, 2013. 97-108.</ref> non-photorealistic images are produced using SDS algorithm which, in the context of this work, is responsible for colour attention.\n\nThe \"computational creativity\" of the above-mentioned systems are discussed in<ref name=\":1\" /><ref>al-Rifaie, Mohammad Majid, Mark JM Bishop, and Ahmed Aber. \"Creative or Not? Birds and Ants Draw with Muscle.\" Proceedings of AISB'11 Computing and Philosophy (2011): 23-30.</ref><ref>al-Rifaie MM, Bishop M (2013) Swarm intelligence and weak artificial creativity. In: The Association for the Advancement of Artificial Intelligence (AAAI) 2013: Spring Symposium, Stanford University, Palo Alto, California, U.S.A., pp 14–19</ref> through the two prerequisites of creativity (i.e. freedom and constraints) within the swarm intelligence's two infamous phases of exploration and exploitation.\n\nMichael Theodore and [[Nikolaus Correll]] use swarm intelligent art installation to explore what it takes to have engineered systems to appear lifelike.<ref>[http://correll.cs.colorado.edu/wp-content/uploads/correll.pdf N. Correll, N. Farrow, K. Sugawara, M. Theodore (2013): The Swarm Wall: Toward Life’s Uncanny Valley. In: K. Goldberg, H. Knight, P. Salvini (Ed.): IEEE International Conference on Robotics and Automation, Workshop on Art and Robotics: Freud's Unheimlich and the Uncanny Valley.]</ref>\n\n==Notable researchers==\n{{div col}}\n* [[Nikolaus Correll]]\n* [[Marco Dorigo]]\n* [[Russell C. Eberhart]]\n* [[Luca Maria Gambardella]]\n* [[James Kennedy (social psychologist)|James Kennedy]]\n* [[Alcherio Martinoli]]\n* [[Craig Reynolds (computer graphics)|Craig Reynolds]]\n* [[Louis B. Rosenberg|Louis Rosenberg]]\n* [[Seyedali Mirjalili]]\n* [[Magnus Egerstedt]]\n{{div col end}}\n\n==See also==\n{{Link farm|date=June 2019}}\n{{Portal|Artificial intelligence}}\n{{div col}}\n* [[Artificial immune systems]]\n* [[Collaborative intelligence]]\n* [[Collective effervescence]]\n* [[Group mind (science fiction)]]\n* [[Cellular automaton]]\n* [[Complex systems]]\n* [[Differential evolution]]\n* [[Dispersive Flies Optimisation|Dispersive flies optimisation]]\n* [[Evolutionary computation]]\n* [[Global brain]]\n* [[Harmony search]]\n* [[Harris hawks optimization]]\n* [[Multi-agent system]]\n* [[Myrmecology]]\n* [[Promise theory]]\n* [[Quorum sensing]]\n* [[Population protocol]]\n* [[Reinforcement learning]]\n* [[Rule 110]]\n* [[Self-organized criticality]]\n* [[Spiral optimization algorithm]]\n* [[Stochastic optimization]]\n* [[Swarm Development Group]]\n* [[Swarm robotic platforms]]\n* [[Swarming (military)|Swarming]]\n* [[SwisTrack]]\n* [[Symmetry breaking of escaping ants]]\n* ''[[The Wisdom of Crowds]]''\n* [[Wisdom of the crowd]]\n{{div col end}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite book |title=Swarm Intelligence: From Natural to Artificial Systems |first1=Eric |last1=Bonabeau |first2=Marco |last2=Dorigo |first3=Guy |last3=Theraulaz |year=1999 |isbn=978-0-19-513159-8}} \n* {{cite book |title=Swarm Intelligence |first1=James |last1=Kennedy |first2=Russell C. |last2=Eberhart |isbn=978-1-55860-595-4|date=2001-04-09 }}\n* {{cite book |title=Fundamentals of Computational Swarm Intelligence |first=Andries |last=Engelbrecht |publisher=Wiley & Sons |isbn=978-0-470-09191-3|date=2005-12-16 }}\n\n== External links ==\n* Marco Dorigo and Mauro Birattari (2007). [http://www.scholarpedia.org/article/Swarm_intelligence \"Swarm intelligence\"] in ''[[Scholarpedia]]''\n* Antoinette Brown.  [https://web.archive.org/web/20161130043122/https://medium.com/@antoinettebromwn/swarm-intelligence-review-eeb74beddbad#.xr4gpi4c6 Swarm Intelligence] \n{{animal cognition}}\n{{collective animal behaviour}}\n{{optimization algorithms|state=collapsed}}\n\n{{DEFAULTSORT:Swarm Intelligence}}\n[[Category:Nature-inspired metaheuristics| ]]\n[[Category:Collective intelligence]]\n[[Category:Intelligence by type]]\n[[Category:Multi-agent systems]]"
    },
    {
      "title": "Ant colony optimization algorithms",
      "url": "https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms",
      "text": "{{multiple issues|\n{{Original research|date=August 2018}}\n{{More footnotes|date=August 2018}}\n}}\n\n[[File:Safari ants.jpg|thumb|Ant behavior was the inspiration for the metaheuristic optimization technique]]\n[[File:Artificial ants.jpg|thumb|400px|When a colony of ants is confronted with the choice of reaching their food via two different routes of which one is much shorter than the other, their choice is entirely random. However, those who use the shorter route move faster and therefore go back and forth more often between the anthill and the food.<ref>{{cite book |last = Waldner  |first = Jean-Baptiste  |authorlink = Jean-Baptiste Waldner  |title = Nanocomputers and Swarm Intelligence |publisher = [[ISTE Ltd|ISTE]] [[John Wiley & Sons]] |place = London |year = 2008 |isbn = 978-1-84704-002-2  | page = 225}}</ref>]]\n\nIn [[computer science]] and [[operations research]], the '''ant colony optimization''' [[algorithm]] ('''ACO''') is a [[probability|probabilistic]] technique for solving computational problems which can be reduced to finding good paths through [[Graph (discrete mathematics)|graph]]s. '''Artificial Ants''' stand for [[multi-agent]] methods inspired by the behavior of real ants. \nThe pheromone-based communication of biological [[ant]]s is often the predominant paradigm used.<ref>{{cite book |last = Monmarché Nicolas, Guinand Frédéric and Siarry Patrick  |title = Artificial Ants |publisher = Wiley-ISTE  |year = 2010 |isbn = 978-1-84821-194-0}}</ref>   Combinations of Artificial Ants and [[local search (optimization)|local search]] algorithms have become a method of choice for numerous optimization tasks involving some sort of [[Graph (discrete mathematics)|graph]], e.g., [[vehicle routing problem|vehicle routing]] and internet [[routing]]. The burgeoning activity in this field has led to conferences dedicated solely to Artificial Ants, and to numerous commercial applications by specialized companies such as [[AntOptima]].\n\nAs an example, Ant colony optimization<ref>{{cite journal|last = Dorigo, Gambardella |first = M, L.M. |authorlink = M. Dorigo & L. M. Gambardella |title = Learning Approach to the Traveling Salesman Problem  |publisher = IEEE Transactions on Evolutionary Computation, 1 (1) |page=214 |year = 1997}}</ref>  is a class of [[optimization (computer science)|optimization]] [[algorithm]]s modeled on the actions of an [[ant colony]].  Artificial 'ants' (e.g. simulation agents) locate optimal solutions by moving through a [[parameter space]] representing all possible solutions.  Real ants lay down [[pheromone]]s directing each other to resources while exploring their environment.  The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions.<ref>Ant Colony Optimization by Marco Dorigo and Thomas Stützle, MIT Press, 2004. {{ISBN|0-262-04219-3}}</ref>  One variation on this approach is [[bees algorithm|the bees algorithm]], which is more analogous to the foraging patterns of the [[honey bee]], another social insect.\n\nThis algorithm is a member of the '''ant colony algorithms''' family, in [[swarm intelligence]] methods, and it constitutes some [[metaheuristic]] optimizations. Initially proposed by [[Marco Dorigo]] in 1992 in his PhD thesis,<ref>A. Colorni, M. Dorigo et V. Maniezzo, ''Distributed Optimization by Ant Colonies'', actes de la première conférence européenne sur la vie artificielle, Paris, France, Elsevier Publishing, 134-142, 1991.</ref><ref name=\"M. Dorigo, Optimization, Learning and Natural Algorithms\">M. Dorigo, ''Optimization, Learning and Natural Algorithms'', PhD thesis, Politecnico di Milano, Italy, 1992.</ref> the first algorithm was aiming to search for an optimal path in a graph, based on the behavior of [[ants]] seeking a path between their [[ant colony|colony]] and a source of food. The original idea has since diversified to solve a wider class of numerical problems, and as a result, several problems have emerged, drawing on various aspects of the behavior of ants. From a broader perspective, ACO performs a model-based search<ref>{{cite journal|last1=Zlochin|first1=Mark|last2=Birattari|first2=Mauro|last3=Meuleau|first3=Nicolas|last4=Dorigo|first4=Marco|title=Model-Based Search for Combinatorial Optimization: A Critical Survey|journal=Annals of Operations Research|date=1 October 2004|volume=131|issue=1–4|pages=373–395|doi=10.1023/B:ANOR.0000039526.52305.af|language=en|issn=0254-5330|citeseerx=10.1.1.3.427}}</ref> and shares some similarities with [[estimation of distribution algorithm]]s.\n\n==Overview==\n\nIn the natural world, ants of some species (initially) wander [[random]]ly,  and upon finding food return to their colony while laying down [[pheromone]] trails. If other ants find such a path, they are likely not to keep travelling at random, but instead to follow the trail, returning and reinforcing it if they eventually find food (see [[Ant#Communication|Ant communication]]).\n\nOver time, however, the pheromone trail starts to evaporate, thus reducing its attractive strength. The more time it takes for an ant to travel down the path and back again, the more time the pheromones have to evaporate. A short path, by comparison, gets marched over more frequently, and thus the pheromone density becomes higher on shorter paths than longer ones. Pheromone evaporation also has the advantage of avoiding the convergence to a locally optimal solution. If there were no evaporation at all, the paths chosen by the first ants would tend to be excessively attractive to the following ones. In that case, the exploration of the solution space would be constrained. The influence of pheromone evaporation in real ant systems is unclear, but it is very important in artificial systems.<ref>Marco Dorigo and Thomas Stültze, Ant Colony Optimization, p.12. 2004.</ref>\n\nThe overall result is that when one ant finds a good (i.e., short) path from the colony to a food source, other ants are more likely to follow that path, and [[positive feedback]] eventually leads to many ants following a single path. The idea of the ant colony algorithm is to mimic this behavior with \"simulated ants\" walking around the graph representing the problem to solve.\n\n===Ambient networks of intelligent objects===\nNew concepts are required since “intelligence” is no longer centralized but can be found throughout all minuscule objects. Anthropocentric concepts have been known to lead to the production of IT systems in which data processing, control units and calculating forces are centralized. These centralized units have continually increased their performance and can be compared to the human brain. The model of the brain has become the ultimate vision of computers. [[Ambient networks]] of intelligent objects and, sooner or later, a new generation of information systems which are even more diffused and based on nanotechnology, will profoundly change this concept. Small devices that can be compared to insects do not dispose of a high intelligence on their own. Indeed, their intelligence can be classed as fairly limited. It is, for example, impossible to integrate a high performance calculator with the power to solve any kind of mathematical problem into a biochip that is implanted into the human body or integrated in an intelligent tag which is designed to trace commercial articles. However, once those objects are interconnected they dispose of a form of intelligence that can be compared to a colony of ants or bees. In the case of certain problems, this type of intelligence can be superior to the reasoning of a centralized system similar to the brain.<ref name=\"Waldner 2008 214\">{{cite book |last = Waldner  |first = Jean-Baptiste  |authorlink = Jean-Baptiste Waldner  |title = Nanocomputers and Swarm Intelligence |publisher = [[ISTE Ltd|ISTE]] John Wiley & Sons |place = London |year = 2008 |isbn = 978-1-84704-002-2  | page = 214}}</ref>\n\nNature offers several examples of how minuscule organisms, if they all follow the same basic rule, can create a form of [[collective intelligence]] on the macroscopic level. Colonies of social insects perfectly illustrate this model which greatly differs from human societies. This model is based on the co-operation of independent units with simple and unpredictable behavior.<ref>{{cite book|last = Waldner  |first = Jean-Baptiste  |authorlink = Jean-Baptiste Waldner  |title = Inventer l'Ordinateur du XXIème Siècle |publisher = [[Hermes Science]] |place = London |year = 2007  | pages = 259–265 |isbn = 978-2-7462-1516-0}}</ref> They move through their surrounding area to carry out certain tasks and only possess a very limited amount of information to do so. A colony of ants, for example, represents numerous qualities that can also be applied to a network of ambient objects. Colonies of ants have a very high capacity to adapt themselves to changes in the environment as well as an enormous strength in dealing with situations where one individual fails to carry out a given task. This kind of flexibility would also be very useful for mobile networks of objects which are perpetually developing. Parcels of information that move from a computer to a digital object behave in the same way as ants would do. They move through the network and pass from one knot to the next with the objective of arriving at their final destination as quickly as possible.<ref>{{cite book |last = Waldner  |first = Jean-Baptiste  |authorlink = Jean-Baptiste Waldner  |title = Nanocomputers and Swarm Intelligence |publisher = ISTE John Wiley & Sons |place = London |year = 2008 |isbn = 978-1-84704-002-2  | page = 215}}</ref>\n\n===Artificial pheromone system===\nPheromone-based communication is one of the most effective ways of communication which is widely observed in nature. Pheromone is used by social insects such as\nbees, ants and termites; both for inter-agent and agent-swarm communications. Due to its feasibility, artificial pheromones have been adopted in multi-robot and swarm robotic systems. Pheromone-based communication was implemented by different means such as chemical <ref>Russell, R. Andrew. \"[https://ieeexplore.ieee.org/abstract/document/774005/ Ant trails-an example for robots to follow?].\" Robotics and Automation, 1999. Proceedings. 1999 IEEE International Conference on. Vol. 4. IEEE, 1999.</ref><ref>Fujisawa, Ryusuke, et al. \"[https://www.researchgate.net/profile/Shigeto_Dobata/publication/265053113_Designing_pheromone_communication_in_swarm_robotics_Group_foraging_behavior_mediated_by_chemical_substance/links/551500f60cf260a7cb2e39eb.pdf Designing pheromone communication in swarm robotics: Group foraging behavior mediated by chemical substance].\" Swarm Intelligence 8.3 (2014): 227-246.</ref> or physical (RFID tags,<ref>Sakakibara, Toshiki, and Daisuke Kurabayashi. \"[https://link.springer.com/article/10.1016/S1672-6529(07)60038-9 Artificial pheromone system using rfid for navigation of autonomous robots].\" Journal of Bionic Engineering 4.4 (2007): 245-253.</ref> light,<ref>Arvin, Farshad, et al. \"[http://eprints.lincoln.ac.uk/22466/7/Aggregation-Final.pdf Investigation of cue-based aggregation in static and dynamic environments with a mobile robot swarm].\" Adaptive Behavior (2016): 1-17.</ref><ref>Farshad Arvin, et al. \"[https://www.researchgate.net/profile/Masoud_Bekravi/publication/241683938_Imitation_of_Honeybee_Aggregation_with_Collective_Behavior_of_Swarm_Robots/links/546518320cf25b85d17d2587/Imitation-of-Honeybee-Aggregation-with-Collective-Behavior-of-Swarm-Robots.pdf Imitation of honeybee aggregation with collective behavior of swarm robots].\" International Journal of Computational Intelligence Systems 4.4 (2011): 739-748.</ref><ref>Schmickl, Thomas, et al. \"[http://swarmrobot.org/publications/Get_in_touch.pdf Get in touch: cooperative decision making based on robot-to-robot collisions].\" Autonomous Agents and Multi-Agent Systems 18.1 (2009): 133-155.</ref><ref>Garnier, Simon, et al. \"[http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002903 Do ants need to estimate the geometrical properties of trail bifurcations to find an efficient route? A swarm robotics test bed.]\" PLoS Comput Biol 9.3 (2013): e1002903.</ref> sound<ref>Arvin, Farshad, et al. \"[https://www.researchgate.net/profile/Farshad_Arvin/publication/273892103_Cue-based_aggregation_with_a_mobile_robot_swarm_A_novel_fuzzy-based_method/links/55e4b97a08ae6abe6e9031be/Cue-based-aggregation-with-a-mobile-robot-swarm-A-novel-fuzzy-based-method.pdf Cue-based aggregation with a mobile robot swarm: a novel fuzzy-based method].\" Adaptive Behavior 22.3 (2014): 189-206.</ref>) ways. However, those implementations were not able to replicate all the aspects of pheromones as seen in nature.\n\nUsing projected light was presented in <ref>Garnier, Simon, et al. \"[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.716.1460&rep=rep1&type=pdf Alice in pheromone land: An experimental setup for the study of ant-like robots].\" 2007 IEEE Swarm Intelligence Symposium. IEEE, 2007.</ref> is an experimental setup to study on pheromone-based communication with micro autonomous robots. Another study that proposed a novel pheromone communication method, ''COSΦ'',<ref>Farshad Arvin et al. \"[http://eprints.lincoln.ac.uk/17957/1/APH-colias.pdf COSΦ: artificial pheromone system for robotic swarms research].\" IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2015.</ref>  for a swarm robotic system based on precise and fast visual localization.<ref>Krajník, Tomáš, et al. \"[http://eprints.lincoln.ac.uk/13653/1/jint_2014_public.pdf A practical multirobot localization system].\" Journal of Intelligent & Robotic Systems 76.3-4 (2014): 539-562.</ref> \nThe system allows to simulate virtually unlimited number of different pheromones and provides the result of their interaction as a gray-scale image on a horizontal LCD screen that the robots move on. In order to demonstrate the pheromone communication method, Colias<ref>Farshad Arvin, et al. \"[http://journals.sagepub.com/doi/pdf/10.5772/58730 Colias: An autonomous micro robot for swarm robotic applications].\" International Journal of Advanced Robotic Systems 11 (2014).</ref> autonomous micro robot was deployed as the swarm robotic platform.\n\n==Common extensions==\nHere are some of the most popular variations of ACO algorithms.\n\n===Elitist ant system===\nThe global best solution deposits pheromone on every iteration along with all the other ants.\n\n===Max-min ant system (MMAS)===\nAdded maximum and minimum pheromone amounts [τ<sub>max</sub>,τ<sub>min</sub>]. Only global best or iteration best tour deposited pheromone <MAZ>. All edges are initialized to τ<sub>min</sub> and reinitialized to τ<sub>max</sub> when nearing stagnation.<ref name=\"T. Stützle et H.H. Hoos\">T. Stützle et H.H. Hoos, ''MAX MIN Ant System'', Future Generation Computer Systems, volume 16, pages 889-914, 2000</ref>\n\n===Ant colony system===\nIt has been presented above.<ref name=\"M. Dorigo et L.M. Gambardella\">M. Dorigo et L.M. Gambardella, ''[http://www.idsia.ch/~luca/acs-ec97.pdf Ant Colony System : A Cooperative Learning Approach to the Traveling Salesman Problem]'', IEEE Transactions on Evolutionary Computation, volume 1, numéro 1, pages 53-66, 1997.</ref>\n\n===Rank-based ant system (ASrank)===\nAll solutions are ranked according to their length. The amount of pheromone deposited is then weighted for each solution, such that solutions with shorter paths deposit more pheromone than the solutions with longer paths.\n\n===Continuous orthogonal ant colony (COAC)===\nThe pheromone deposit mechanism of COAC is to enable ants to search for solutions collaboratively and effectively. By using an orthogonal design method, ants in the feasible domain can explore their chosen regions rapidly and efficiently, with enhanced global search capability and accuracy.\n\nThe orthogonal design method and the adaptive radius adjustment method can also be extended to other optimization algorithms for delivering wider advantages in solving practical problems.<ref>[http://eprints.gla.ac.uk/3894/ X Hu, J Zhang, and Y Li (2008). Orthogonal methods based ant colony search for solving continuous optimization problems. ''Journal of Computer Science and Technology'', 23(1), pp.2-18.]</ref>\n\n===Recursive ant colony optimization===\nIt is a recursive form of ant system which divides the whole search domain into several sub-domains and solves the objective on these subdomains.<ref>Gupta, D.K.; Arora, Y.; Singh, U.K.; Gupta, J.P., \"Recursive Ant Colony Optimization for estimation of parameters of a function,\" Recent Advances in Information Technology (RAIT), 2012 1st International Conference on , vol., no., pp.448-454, 15–17 March 2012</ref> The results from all the subdomains are compared and the best few of them are promoted for the next level. The subdomains corresponding to the selected results are further subdivided and the process is repeated until an output of desired precision is obtained. This method has been tested on ill-posed geophysical inversion problems and works well.<ref>Gupta, D.K.; Gupta, J.P.; Arora, Y.; Shankar, U., \"[http://nsg.eage.org/publication/download/?publication=68286 Recursive ant colony optimization: a new technique for the estimation of function parameters from geophysical field data],\" Near Surface Geophysics , vol. 11, no. 3, pp.325-339</ref>\n\n==Convergence==\nFor some versions of the algorithm, it is possible to prove that it is convergent (i.e., it is able to find the global optimum in finite time). The first evidence of a convergence ant colony algorithm was made in 2000, the graph-based ant system algorithm, and then algorithms for ACS and MMAS. Like most [[metaheuristic]]s, it is very difficult to estimate the theoretical speed of convergence. In 2004, Zlochin and his colleagues<ref name=\"Zlochin model-based search\">M. Zlochin, M. Birattari, N. Meuleau, et M. Dorigo, ''Model-based search for combinatorial optimization: A critical survey'', Annals of Operations Research, vol. 131, pp. 373-395, 2004.</ref> showed that COA-type algorithms could be assimilated methods of [[stochastic gradient descent]], on the [[cross-entropy]] and [[estimation of distribution algorithm]]. They proposed these [[metaheuristic]]s as a \"[[research-based model]]\". A performance analysis of continuous ant colony algorithm based on its various parameter suggest its sensitivity of convergence on parameter tuning.<ref>V.K.Ojha, A. Abraham and V. Snasel, [https://arxiv.org/pdf/1707.01812 ACO for Continuous Function Optimization: A Performance Analysis], 14th International Conference on Intelligent Systems Design and Applications (ISDA), Japan, Page 145 - 150 978-1-4799-7938-7/14 2014 IEEE</ref>\n\n==Example pseudo-code and formula==\n<source lang=\"Java\">\n  procedure ACO_MetaHeuristic\n    while(not_termination)\n       generateSolutions()\n       daemonActions()\n       pheromoneUpdate()\n    end while\n  end procedure\n</source>\n\n===Edge selection===\nAn ant is a simple computational agent in the ant colony optimization algorithm. It iteratively constructs a solution for the problem at hand. The intermediate solutions are referred to as solution states. At each iteration of the algorithm, each ant moves from a state <math>x</math> to state <math>y</math>, corresponding to a more complete intermediate solution. Thus, each ant <math>k</math> computes a set <math>A_k(x)</math> of feasible expansions to its current state in each iteration, and moves to one of these in probability. For ant <math>k</math>, the probability <math>p_{xy}^k</math> of moving from state <math>x</math> to state <math>y</math> depends on the combination of two values, viz., the ''attractiveness'' <math>\\eta_{xy}</math> of the move, as computed by some heuristic indicating the ''a priori'' desirability of that move and the ''trail level'' <math>\\tau_{xy}</math> of the move, indicating how proficient it has been in the past to make that particular move.\n\nThe ''trail level'' represents a posteriori indication of the desirability of that move. Trails are updated usually when all ants have completed their solution, increasing or decreasing the level of trails corresponding to moves that were part of \"good\" or \"bad\" solutions, respectively.\n\nIn general, the <math>k</math>th ant moves from state <math>x</math> to state <math>y</math> with probability\n\n<math>\np_{xy}^k =\n\\frac\n{ (\\tau_{xy}^{\\alpha}) (\\eta_{xy}^{\\beta}) }\n{ \\sum_{z\\in \\mathrm{allowed}_x} (\\tau_{xz}^{\\alpha}) (\\eta_{xz}^{\\beta}) }\n</math>\n\nwhere\n\n<math>\\tau_{xy}</math> is the amount of pheromone deposited for transition from state <math>x</math> to <math>y</math>, 0 ≤ <math>\\alpha</math> is a parameter to control the influence of <math>\\tau_{xy}</math>, <math>\\eta_{xy}</math> is the desirability of state transition <math>xy</math> (''a priori'' knowledge, typically <math>1/d_{xy}</math>, where <math>d</math> is the distance) and <math>\\beta</math> ≥ 1 is a parameter to control the influence of <math>\\eta_{xy}</math>. <math>\\tau_{xz}</math> and <math>\\eta_{xz}</math> represent the attractiveness and trail level for the other possible state transitions.\n\n===Pheromone update===\nWhen all the ants have completed a solution, the trails are updated by\n<math>\n\\tau_{xy} \\leftarrow\n(1-\\rho)\\tau_{xy} + \\sum_{k}\\Delta \\tau^{k}_{xy}\n</math>\n\nwhere <math>\\tau_{xy}</math> is the amount of pheromone deposited for a state transition <math>xy</math>, <math>\\rho</math> is the ''pheromone evaporation coefficient'' and <math>\\Delta \\tau^{k}_{xy}</math> is the amount of pheromone deposited by <math>k</math>th ant, typically given for a [[Travelling salesman problem|TSP]] problem (with moves corresponding to arcs of the graph) by\n\n<math>\n\\Delta \\tau^{k}_{xy} =\n\\begin{cases}\nQ/L_k & \\mbox{if ant }k\\mbox{ uses curve }xy\\mbox{ in its tour} \\\\\n0 & \\mbox{otherwise}\n\\end{cases}\n</math>\n\nwhere <math>L_k</math> is the cost of the <math>k</math>th ant's tour (typically length) and <math>Q</math> is a constant.\n\n==Applications==\n[[File:Knapsack ants.svg|thumb|[[Knapsack problem]]: The ants prefer the smaller drop of honey over the more abundant, but less nutritious, sugar]]\nAnt colony optimization algorithms have been applied to many [[combinatorial optimization]] problems, ranging from quadratic assignment to [[protein]] folding or [[Vehicle routing problem|routing vehicles]] and a lot of derived methods have been adapted to dynamic problems in real variables, stochastic problems, multi-targets and [[parallel computing|parallel]] implementations.\nIt has also been used to produce near-optimal solutions to the [[travelling salesman problem]]. They have an advantage over [[simulated annealing]] and [[genetic algorithm]] approaches of similar problems when the graph may change dynamically; the ant colony algorithm can be run continuously and adapt to changes in real time. This is of interest in [[network routing]] and urban transportation systems.\n\nThe first ACO algorithm was called the ant system<ref name=\"Ant system\">M. Dorigo, V. Maniezzo, et A. Colorni, ''[http://www.cs.unibo.it/babaoglu/courses/cas05-06/tutorials/Ant_Colony_Optimization.pdf Ant system: optimization by a colony of cooperating agents]'', IEEE Transactions on Systems, Man, and Cybernetics--Part B , volume 26, numéro 1, pages 29-41, 1996.</ref> and it was aimed to solve the travelling salesman problem, in which the goal is to find the shortest round-trip to link a series of cities. The general algorithm is relatively simple and based on a set of ants, each making one of the possible round-trips along the cities. At each stage, the ant chooses to move from one city to another according to some rules:\n# It must visit each city exactly once;\n# A distant city has less chance of being chosen (the visibility);\n# The more intense the pheromone trail laid out on an edge between two cities, the greater the probability that that edge will be chosen;\n# Having completed its journey, the ant deposits more pheromones on all edges it traversed, if the journey is short;\n# After each iteration, trails of pheromones evaporate.\n\n[[File:Aco TSP.svg|thumb|600px|center]]\n\n===Scheduling problem===\n*[[Job-shop scheduling]] problem (JSP)<ref>D. Martens, M. De Backer, R. Haesen, J. Vanthienen, M. Snoeck, B. Baesens, ''[https://ieeexplore.ieee.org/abstract/document/4336122/ Classification with Ant Colony Optimization]'', IEEE Transactions on Evolutionary Computation, volume 11, number 5, pages 651—665, 2007.\n</ref>\n*[[Open-shop scheduling]] problem (OSP)<ref>B. Pfahring, \"Multi-agent search for open scheduling: adapting the Ant-Q formalism,\" Technical report TR-96-09, 1996.</ref><ref>C. Blem, \"[http://lia.disi.unibo.it/Courses/SistInt/articoli/beam-aco.pdf Beam-ACO, Hybridizing ant colony optimization with beam search. An application to open shop scheduling],\" Technical report TR/IRIDIA/2003-17, 2003.</ref>\n*Permutation flow shop problem (PFSP)<ref>T. Stützle, \"An ant approach to the flow shop problem,\" Technical report AIDA-97-07, 1997.</ref>\n*Single machine total tardiness problem (SMTTP)<ref>A. Bauer, B. Bullnheimer, R. F. Hartl and C. Strauss, \"Minimizing total tardiness on a single machine using ant colony optimization,\" Central European Journal for Operations Research and Economics, vol.8, no.2, pp.125-141, 2000.</ref>\n*Single machine total weighted tardiness problem (SMTWTP)<ref>M. den Besten, \"Ants for the single machine total weighted tardiness problem,\" Master's thesis, University of Amsterdam, 2000.</ref><ref>M, den Bseten, T. Stützle and M. Dorigo, \"Ant colony optimization for the total weighted tardiness problem,\" Proceedings of PPSN-VI, Sixth International Conference on Parallel Problem Solving from Nature, vol. 1917 of [[Lecture Notes in Computer Science]], pp.611-620, 2000.</ref><ref>D. Merkle and M. Middendorf, \"[http://www.ccas.ru/orsot/library/An%20Ant%20Algorithm%20with%20a%20New%20Pheromone%20Evaluation%20Rule%20for%20Total%20Tardiness%20Problems.pdf An ant algorithm with a new pheromone evaluation rule for total tardiness problems],\" Real World Applications of Evolutionary Computing, vol. 1803 of Lecture Notes in Computer Science, pp.287-296, 2000.</ref>\n*Resource-constrained project scheduling problem (RCPSP)<ref>D. Merkle, M. Middendorf and H. Schmeck, \"Ant colony optimization for resource-constrained project scheduling,\" Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2000), pp.893-900, 2000.</ref>\n*Group-shop scheduling problem (GSP)<ref>C. Blum, \"[ftp://nozdr.ru/biblio/kolxo3/Cs/CsLn/Ant%20Algorithms,%203%20conf.,%20ANTS%202002(LNCS2463,%20Springer,%202002)(ISBN%203540441468)(318s).pdf#page=28 ACO applied to group shop scheduling: a case study on intensification and diversification],\" Proceedings of ANTS 2002, vol. 2463 of Lecture Notes in Computer Science, pp.14-27, 2002.</ref>\n*Single-machine total tardiness problem with sequence dependent setup times (SMTTPDST)<ref>C. Gagné, W. L. Price and M. Gravel, \"[https://link.springer.com/article/10.1057/palgrave.jors.2601390 Comparing an ACO algorithm with other heuristics for the single machine scheduling problem with sequence-dependent setup times],\" Journal of the Operational Research Society, vol.53, pp.895-906, 2002.</ref>\n*Multistage flowshop scheduling problem (MFSP) with sequence dependent setup/changeover times<ref>A. V. Donati, V. Darley, B. Ramachandran, \"An Ant-Bidding Algorithm for Multistage Flowshop Scheduling Problem: Optimization and Phase Transitions\", book chapter in Advances in Metaheuristics for Hard Optimization, Springer, {{ISBN|978-3-540-72959-4}}, pp.111-138, 2008.</ref>\n\n===Vehicle routing problem===\n*Capacitated vehicle routing problem (CVRP)<ref>P. Toth, D. Vigo, \"[https://www.sciencedirect.com/science/article/pii/S0166218X01003511 Models, relaxations and exact approaches for the capacitated vehicle routing problem],\" Discrete Applied Mathematics, vol.123, pp.487-512, 2002.</ref><ref>J. M. Belenguer, and E. Benavent, \"A cutting plane algorithm for capacitated arc routing problem,\" Computers & Operations Research, vol.30, no.5, pp.705-728, 2003.</ref><ref>T. K. Ralphs, \"Parallel branch and cut for capacitated vehicle routing,\" Parallel Computing, vol.29, pp.607-629, 2003.</ref>\n*Multi-depot vehicle routing problem (MDVRP)<ref>S. Salhi and M. Sari, \"[https://www.sciencedirect.com/science/article/pii/S0377221796002536 A multi-level composite heuristic for the multi-depot vehicle fleet mix problem],\" European Journal for Operations Research, vol.103, no.1, pp.95-112, 1997.</ref>\n*Period vehicle routing problem (PVRP)<ref>E. Angelelli and M. G. Speranza, \"[https://www.sciencedirect.com/science/article/pii/S0377221701002065 The periodic vehicle routing problem with intermediate facilities],\" European Journal for Operations Research, vol.137, no.2, pp.233-247, 2002.</ref>\n*Split delivery vehicle routing problem (SDVRP)<ref>S. C. Ho and D. Haugland, \"[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.8.7096&rep=rep1&type=pdf A tabu search heuristic for the vehicle routing problem with time windows and split deliveries],\" Computers & Operations Research, vol.31, no.12, pp.1947-1964, 2004.</ref>\n*Stochastic vehicle routing problem (SVRP)<ref>N. Secomandi, \"[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.392.4034&rep=rep1&type=pdf Comparing neuro-dynamic programming algorithms for the vehicle routing problem with stochastic demands],\" Computers & Operations Research, vol.27, no.11, pp.1201-1225, 2000.</ref>\n*Vehicle routing problem with pick-up and delivery (VRPPD)<ref>W. P. Nanry and J. W. Barnes, \"[https://pdfs.semanticscholar.org/f5a3/fffdfb26ead53680a5f9d3334e556181317b.pdf Solving the pickup and delivery problem with time windows using reactive tabu search],\" Transportation Research Part B, vol.34, no. 2, pp.107-121, 2000.</ref><ref>R. Bent and P.V. Hentenryck, \"[https://pdfs.semanticscholar.org/3952/105ddd7477f04ab1225cf2821021fefeab50.pdf A two-stage hybrid algorithm for pickup and delivery vehicle routing problems with time windows],\" Computers & Operations Research, vol.33, no.4, pp.875-893, 2003.</ref>\n*Vehicle routing problem with time windows (VRPTW)<ref>A. Bachem, W. Hochstattler and M. Malich, \"[https://www.sciencedirect.com/science/article/pii/0166218X9500027O/pdf?md5=ef433e9be68b097a89f3dd9b0b0dc761&pid=1-s2.0-0166218X9500027O-main.pdf&_valck=1 The simulated trading heuristic for solving vehicle routing problems],\" Discrete Applied Mathematics, vol. 65, pp.47-72, 1996..</ref><ref>[57] S. C. Hong and Y. B. Park, \"[https://www.sciencedirect.com/science/article/pii/S0925527398002503 A heuristic for bi-objective vehicle routing with time window constraints],\" International Journal of Production Economics, vol.62, no.3, pp.249-258, 1999.</ref><ref>R. A. Rusell and W. C. Chiang, \"[https://www.sciencedirect.com/science/article/pii/S0377221704005570 Scatter search for the vehicle routing problem with time windows],\" European Journal for Operations Research, vol.169, no.2, pp.606-622, 2006.</ref>\n*Time dependent vehicle routing problem with time windows (TDVRPTW)<ref>A. V. Donati, R. Montemanni, N. Casagrande, A. E. Rizzoli, L. M. Gambardella, \"[ftp://ftp.idsia.ch/pub/andrea/ASP_Aprile07/EJOR2007.pdf Time Dependent Vehicle Routing Problem with a Multi Ant Colony System]\", European Journal of Operational Research, vol.185, no.3, pp.1174–1191, 2008.</ref>\n*Vehicle routing problem with time windows and multiple service workers (VRPTWMS)\n\n===Assignment problem===\n*[[Quadratic assignment problem]] (QAP)<ref>T. Stützle, \"[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.5167 MAX-MIN Ant System for quadratic assignment problems],\" Technical Report AIDA-97-4, FB Informatik, TU Darmstadt, Germany, 1997.</ref>\n*[[Generalized assignment problem]] (GAP)<ref>R. Lourenço and D. Serra \"[https://upcommons.upc.edu/bitstream/handle/2099/3627/4-ramalhinho.pdf Adaptive search heuristics for the generalized assignment problem],\" Mathware & soft computing, vol.9, no.2-3, 2002.</ref><ref>M. Yagiura, T. Ibaraki and F. Glover, \"[http://leeds-faculty.colorado.edu/glover/Publications/TS%20-%20PR%20-%20GAP%20in%20INFORMS%20JOC.pdf An ejection chain approach for the generalized assignment problem],\" INFORMS Journal on Computing, vol. 16, no. 2, pp. 133–151, 2004.</ref>\n*[[Frequency assignment problem]] (FAP)<ref>K. I. Aardal, [[S. P. M. van Hoesel]], A. M. C. A. Koster, C. Mannino and Antonio. Sassano, \"Models and solution techniques for the frequency assignment problem,\" A Quarterly Journal of Operations Research, vol.1, no.4, pp.261-317, 2001.</ref>\n*[[Redundancy allocation problem]] (RAP)<ref>Y. C. Liang and A. E. Smith, \"[http://www.eng.auburn.edu/sites/personal/aesmith/files/antcolonyIEEETrRel%20paper.pdf An ant colony optimization algorithm for the redundancy allocation problem (RAP)],\" IEEE Transactions on Reliability, vol.53, no.3, pp.417-423, 2004.</ref>\n\n===Set problem===\n*[[Set cover problem]] (SCP)<ref>G. Leguizamon and Z. Michalewicz, \"[https://cs.adelaide.edu.au/users/zbyszek/Papers/as4.pdf A new version of ant system for subset problems],\" Proceedings of the 1999 Congress on Evolutionary Computation(CEC 99), vol.2,  pp.1458-1464, 1999.</ref><ref>R. Hadji, M. Rahoual, E. Talbi and V. Bachelet \"Ant colonies for the set covering problem,\" Abstract proceedings of ANTS2000, pp.63-66, 2000.</ref>\n*[[Partition problem]] (SPP)<ref>V Maniezzo and M Milandri, \"[https://link.springer.com/chapter/10.1007/3-540-45724-0_19 An ant-based framework for very strongly constrained problems],\" Proceedings of ANTS2000, pp.222-227, 2002.</ref>\n*Weight constrained graph tree partition problem (WCGTPP)<ref>R. Cordone and F. Maffioli,\"[ftp://nozdr.ru/biblio/kolxo3/Cs/CsLn/A/Applications%20of%20Evolutionary%20Computing,%20EvoWorkshops%202001..%20EvoCOP(LNCS2037,%20Springer,%202001)(ISBN%203540419209)(529s)_CsLn_.pdf#page=74 Colored Ant System and local search to design local telecommunication networks],\" Applications of Evolutionary Computing: Proceedings of Evo Workshops, vol.2037, pp.60-69, 2001.</ref>\n*Arc-weighted l-cardinality tree problem (AWlCTP)<ref>C. Blum and M.J. Blesa, \"[https://upcommons.upc.edu/bitstream/handle/2117/97393/R03-1.ps Metaheuristics for the edge-weighted k-cardinality tree problem],\" Technical Report TR/IRIDIA/2003-02, IRIDIA, 2003.</ref>\n*Multiple knapsack problem (MKP)<ref>[http://parallel.bas.bg/~stefka/heuristic.ps S. Fidanova, \"ACO algorithm for MKP using various heuristic information\"], Numerical Methods and Applications, vol.2542, pp.438-444, 2003.</ref>\n*Maximum independent set problem (MIS)<ref>G. Leguizamon, Z. Michalewicz and Martin Schutz, \"[http://sedici.unlp.edu.ar/bitstream/handle/10915/23384/Documento_completo.pdf?sequence=1 An ant system for the maximum independent set problem],\" Proceedings of the 2001 Argentinian Congress on Computer Science, vol.2, pp.1027-1040, 2001.</ref>\n\n===Device sizing problem in nanoelectronics physical design===\n* Ant colony optimization (ACO) based optimization of 45&nbsp;nm CMOS-based sense amplifier circuit could converge to optimal solutions in very minimal time.<ref>O. Okobiah, S. P. Mohanty, and E. Kougianos, \"[http://www.cse.unt.edu/~smohanty/Publications_Conferences/2012/Mohanty_ISQED2012_Kriging-ACO.pdf Ordinary Kriging Metamodel-Assisted Ant Colony Algorithm for Fast Analog Design Optimization] {{webarchive |url=https://web.archive.org/web/20160304110324/http://www.cse.unt.edu/~smohanty/Publications_Conferences/2012/Mohanty_ISQED2012_Kriging-ACO.pdf |date=March 4, 2016 }}\", in Proceedings of the 13th IEEE International Symposium on Quality Electronic Design (ISQED), pp. 458--463, 2012.</ref>\n* Ant colony optimization (ACO) based reversible circuit synthesis could improve efficiency significantly.<ref>M. Sarkar, P. Ghosal, and S. P. Mohanty, \"[http://www.cse.unt.edu/~smohanty/Publications_Conferences/2013/Mohanty_MWSCAS2013_Reversible-Circuit.pdf Reversible Circuit Synthesis Using ACO and SA based Quinne-McCluskey Method] {{webarchive |url=https://web.archive.org/web/20140729081848/http://www.cse.unt.edu/~smohanty/Publications_Conferences/2013/Mohanty_MWSCAS2013_Reversible-Circuit.pdf |date=July 29, 2014 }}\", in Proceedings of the 56th IEEE International Midwest Symposium on Circuits & Systems (MWSCAS), 2013, pp. 416--419.</ref>\n\n===Antennas optimization and synthesis===\n[[File:ANT Antenna 1.jpg|thumb|Loopback vibrators 10×10, synthesized by means of ACO algorithm<ref name=slyusarant1>Ermolaev S.Y., Slyusar V.I. Antenna synthesis based on the ant colony optimization algorithm.// Proc. ICATT’2009, Lviv, Ukraine 6 - 9 Octobre, 2009. - Pages 298 - 300 [http://slyusar.kiev.ua/298_300_ICATT_2009.pdf]</ref>]]\n[[File:ANT antenna 2.jpg|thumb|Unloopback vibrators 10×10, synthesized by means of ACO algorithm<ref name=slyusarant1/>]]\nTo optimize the form of antennas, ant colony algorithms can be used. As example can be considered antennas RFID-tags based on ant colony algorithms (ACO).,<ref>Marcus Randall, Andrew Lewis, Amir Galehdar, David Thiel. Using Ant Colony Optimisation to Improve the Efficiency of Small Meander Line RFID Antennas.// In 3rd IEEE International e-Science and Grid Computing Conference [http://www98.griffith.edu.au/dspace/bitstream/10072/17063/1/47523_1.pdf], 2007</ref> loopback and unloopback vibrators 10×10<ref name=slyusarant1/>\n\n===Image processing===\nThe ACO algorithm is used in image processing for image edge detection and edge linking.<ref>S. Meshoul and M Batouche, \"[https://pdfs.semanticscholar.org/bdd2/61ab1f5a0c90009c6d84dbe4121a87dd4d31.pdf Ant colony system with extremal dynamics for point matching and pose estimation],\" Proceedings of the 16th International Conference on Pattern Recognition, vol.3, pp.823-826, 2002.</ref><ref>H. Nezamabadi-pour, S. Saryazdi, and E. Rashedi, \"[https://www.researchgate.net/profile/Esmat_Rashedi/publication/220176122_Edge_detection_using_ant_algorithms/links/5743d1ab08ae9ace841b4063.pdf Edge detection using ant algorithms]\", Soft Computing, vol. 10, no.7, pp. 623-628, 2006.</ref>\n* '''Edge detection:'''\nThe graph here is the 2-D image and the ants traverse from one pixel depositing pheromone.The movement of ants from one pixel to another is directed by the local variation of the image's intensity values. This movement causes the highest density of the pheromone to be deposited at the edges.\n\nThe following are the steps involved in edge detection using ACO:<ref>{{cite book|last1=Tian|first1=Jing|title=2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)|pages=751–756|last2=Yu|first2=Weiyu|last3=Xie|first3=Shengli|doi=10.1109/CEC.2008.4630880|year=2008|isbn=978-1-4244-1822-0}}</ref><ref>{{cite web|last1=Gupta|first1=Charu|last2=Gupta|first2=Sunanda|title=Edge Detection of an Image based on Ant ColonyOptimization Technique|url=https://www.academia.edu/4688002}}</ref><ref>{{Cite book|title = Edge detection using ant colony search algorithm and multiscale contrast enhancement|journal = IEEE International Conference on Systems, Man and Cybernetics, 2009. SMC 2009|pages = 2193–2198|doi = 10.1109/ICSMC.2009.5345922|first = A.|last = Jevtić|first2 = J.|last2 = Quintanilla-Dominguez|first3 = M.G.|last3 = Cortina-Januchs|first4 = D.|last4 = Andina|year = 2009|isbn = 978-1-4244-2793-2}}</ref>\n\n''Step1: Initialization:<br />''Randomly place <math>K</math> ants on the image <math>I_{M_1 M_2}</math> where <math>K= (M_1*M_2)^\\tfrac{1}{2}</math> . Pheromone matrix <math>\\tau_{(i,j)}</math> are initialized with a random value. The major challenge in the initialization process is determining the heuristic matrix.\n\nThere are various methods to determine the heuristic matrix. For the below example the heuristic matrix was calculated based on the local statistics:\nthe local statistics at the pixel position (i,j).\n\n<math>\\eta_{(i,j)}= \\tfrac{1}{Z}*Vc*I_{(i,j)}</math>\n\nWhere <math>I</math> is the image of size <math>M_1*M_2</math><br />\n<math>Z =\\sum_{i=1:M_1}  \\sum_{j=1:M_2} Vc(I_{i,j})</math>,which is a normalization factor\n\n<math>\\begin{align}Vc(I_{i,j}) = &f \\left( \\left\\vert I_{(i-2,j-1)} - I_{(i+2,j+1)} \\right\\vert + \\left\\vert I_{(i-2,j+1)} - I_{(i+2,j-1)} \\right\\vert \\right. \\\\\n& +\\left\\vert I_{(i-1,j-2)} - I_{(i+1,j+2)} \\right\\vert + \\left\\vert I_{(i-1,j-1)} - I_{(i+1,j+1)} \\right\\vert\\\\\n& +\\left\\vert I_{(i-1,j)} - I_{(i+1,j)} \\right\\vert + \\left\\vert I_{(i-1,j+1)} - I_{(i-1,j-1)} \\right\\vert\\\\\n& + \\left. \\left\\vert I_{(i-1,j+2)} - I_{(i-1,j-2)} \\right\\vert + \\left\\vert I_{(i,j-1)} - I_{(i,j+1)} \\right\\vert \\right) \\end{align}</math>\n\n<math>f(\\cdot)</math> can be calculated using the following functions:<br /><math>f(x) = \\lambda x, \\quad \\text{for x ≥ 0;  (1)} </math><br /><math>f(x) = \\lambda x^2, \\quad \\text{for x ≥ 0;  (2)} </math><br /><math>f(x) =\n\\begin{cases}\n\\sin(\\frac{\\pi x}{2 \\lambda}), & \\text{for 0 ≤ x ≤} \\lambda \\text{;  (3)} \\\\\n0, & \\text{else}\n\\end{cases}</math><br /><math>f(x) =\n\\begin{cases}\n\\pi x \\sin(\\frac{\\pi x}{2 \\lambda}), & \\text{for 0 ≤ x ≤} \\lambda \\text{;  (4)} \\\\\n0, & \\text{else}\n\\end{cases}</math><br />The parameter <math>\\lambda</math> in each of above functions adjusts the functions’ respective shapes.<br />''Step 2 Construction process:<br />''The ant's movement is based on [[4-connected neighborhood|4-connected]] [[pixel]]s or [[8-connected]] [[pixel]]s. The probability with which the ant moves is given by the probability equation <math>P_{x,y}</math><br />''Step 3 and Step 5 Update process:<br />''The pheromone matrix is updated twice. in step 3 the trail of the ant (given by <math>\\tau_{(x,y)}</math> ) is updated where as in step 5 the evaporation rate of the trail is updated which is given by the below equation.<br /><math>\n\\tau_{new} \\leftarrow\n(1-\\psi)\\tau_{old} + \\psi \\tau_{0}\n</math>, where <math>\\psi</math> is the pheromone decay coefficient <math>0< \\tau <1</math>\n\n''Step 7 Decision Process:<br />''Once the K ants have moved a fixed distance L for N iteration, the decision whether it is an edge or not is based on the threshold T on the pheromone matrixτ. Threshold for the below example is calculated based on [[Otsu's method]].\n\nImage Edge detected using ACO:<br />The images below are generated using different functions given by the equation (1) to (4).<ref>{{cite web|title=File Exchange {{ndash}} Ant Colony Optimization (ACO)|website=[[MATLAB]] Central|url=http://www.mathworks.com/matlabcentral/fileexchange/32009-ant-colony-optimization--aco-}}</ref>\n[[File:(a)Original Image (b)Image Generated using equation(1) (c)Image generated using equation(2) (d) Image generated using equation(3) (e)Image generated using equation(4).jpg|none|thumb]]\n\n* '''Edge linking:'''<ref>{{cite book|last1 = Jevtić|first1 = A.|title = 2009 35th Annual Conference of IEEE Industrial Electronics|last2 = Melgar|first2 = I.|last3 = Andina|first3 = D.|pages = 3353–3358|year = 2009|location = 35th Annual Conference of IEEE Industrial Electronics, 2009. IECON '09.|doi = 10.1109/IECON.2009.5415195|isbn = 978-1-4244-4648-3}}</ref> ACO has also been proven effective in edge linking algorithms too.\n\n=== Other applications ===\n\n* [[Bankruptcy prediction]]<ref>{{cite journal|last1=Zhang|first1=Y.|title=A Rule-Based Model for Bankruptcy Prediction Based on an Improved Genetic Ant Colony Algorithm|journal=Mathematical Problems in Engineering|date=2013|volume=2013|page=753251|url=http://www.hindawi.com/journals/mpe/2013/753251}}</ref>\n* [[Classification]]<ref name=\"D. Martens, M pages 651\">D. Martens, M. De Backer, R. Haesen, J. Vanthienen, M. Snoeck, B. Baesens, \"[https://ieeexplore.ieee.org/abstract/document/4336122/ Classification with Ant Colony Optimization]\", IEEE Transactions on Evolutionary Computation, volume 11, number 5, pages 651—665, 2007.</ref>\n* Connection-oriented [[network routing]]<ref>G. D. Caro and M. Dorigo, \"Extending AntNet for best-effort quality-of-service routing,\" Proceedings of the First International Workshop on Ant Colony Optimization (ANTS’98), 1998.</ref>\n* Connectionless network routing<ref>G.D. Caro and M. Dorigo \"[http://www.idsia.ch/~gianni/Papers/tech-rep-iridia-97-12.pdf AntNet: a mobile agents approach to adaptive routing],\" Proceedings of the Thirty-First Hawaii International Conference on System Science, vol.7, pp.74-83, 1998.</ref><ref>G. D. Caro and M. Dorigo, \"[https://www.researchgate.net/profile/Gianni_Di_Caro/publication/2328604_Two_Ant_Colony_Algorithms_For_Best-Effort_Routing_In_Datagram_Networks/links/0deec52909f32c7e6d000000/Two-Ant-Colony-Algorithms-For-Best-Effort-Routing-In-Datagram-Networks.pdf Two ant colony algorithms for best-effort routing in datagram networks],\" Proceedings of the Tenth IASTED International Conference on Parallel and Distributed Computing and Systems (PDCS’98), pp.541-546, 1998.</ref>\n* [[Data mining]]<ref name=\"D. Martens, M pages 651\"/><ref>D. Martens, B. Baesens, T. Fawcett \"[https://link.springer.com/content/pdf/10.1007/s10994-010-5216-5.pdf Editorial Survey: Swarm Intelligence for Data Mining],\" Machine Learning, volume 82, number 1, pp. 1-42, 2011</ref><ref>R. S. Parpinelli, H. S. Lopes and A. A Freitas, \"[http://neuro.bstu.by/ai/To-dom/My_research/Paper-0-again/For-courses/Ants/heuristic-dm-bk.pdf An ant colony algorithm for classification rule discovery],\" Data Mining: A heuristic Approach, pp.191-209, 2002.</ref><ref>R. S. Parpinelli, H. S. Lopes and A. A Freitas, \"[https://www.academia.edu/download/31181466/datamining070.pdf Data mining with an ant colony optimization algorithm],\" IEEE Transactions on Evolutionary Computation, vol.6, no.4, pp.321-332, 2002.</ref>\n* Discounted cash flows in project scheduling<ref>W. N. Chen, J. ZHANG and H. Chung, \"[http://webdelprofesor.ula.ve/economia/gsfran/Asignaturas/EvaluacionFinEconProyec/2%20OptimizingDiscounted.pdf Optimizing Discounted Cash Flows in Project Scheduling--An Ant Colony Optimization Approach]\", IEEE Transactions on Systems, Man, and Cybernetics--Part C: Applications and Reviews Vol.40 No.5 pp.64-77, Jan. 2010.</ref>\n* [[distributed computing|Distributed]] [[information retrieval]]<ref>D. Picard, A. Revel, M. Cord, \"An Application of Swarm Intelligence to Distributed Image Retrieval\", Information Sciences, 2010</ref><ref>D. Picard, M. Cord, A. Revel, \"[http://hal.upmc.fr/docs/00/65/63/63/PDF/manuscript.pdf Image Retrieval over Networks : Active Learning using Ant Algorithm]\", IEEE Transactions on Multimedia, vol. 10, no. 7, pp. 1356--1365 - nov 2008</ref>\n* Energy and electricity network design<ref name=\"warner-and-vogel-2008\">\n{{cite conference\n | last1 = Warner | first1 = Lars\n | last2 = Vogel | first2 = Ute\n | title = Optimization of energy supply networks using ant colony optimization\n | date = 2008\n | conference = Environmental Informatics and Industrial Ecology — 22th International Conference on Informatics for Environmental Protection\n | publisher = Shaker Verlag\n | location = Aachen, Germany\n | isbn = 978-3-8322-7313-2\n | url = http://enviroinfo.eu/sites/default/files/pdfs/vol119/0327.pdf\n | access-date = 2018-10-09\n}}\n</ref>\n* Grid workflow scheduling problem<ref>W. N. Chen and J. ZHANG \"Ant Colony Optimization Approach to Grid Workflow Scheduling Problem with Various QoS Requirements\", IEEE Transactions on Systems, Man, and Cybernetics--Part C: Applications and Reviews, Vol. 31, No. 1,pp.29-43,Jan 2009.</ref>\n* Inhibitory peptide design for [[protein protein interaction]]s<ref name=\":0\">{{Cite journal|last=Zaidman|first=Daniel|last2=Wolfson|first2=Haim J.|date=2016-08-01|title=PinaColada: peptide–inhibitor ant colony ad-hoc design algorithm|journal=Bioinformatics|volume=32|issue=15|pages=2289–2296|doi=10.1093/bioinformatics/btw133|pmid=27153578|issn=1367-4803}}</ref>\n* Intelligent testing system<ref>Xiao. M.Hu, J. ZHANG, and H. Chung, \"[https://ieeexplore.ieee.org/abstract/document/5061647/ An Intelligent Testing System Embedded with an Ant Colony Optimization Based Test Composition Method]\", IEEE Transactions on Systems, Man, and Cybernetics--Part C: Applications and Reviews, Vol. 39, No. 6, pp. 659-669, Dec 2009.</ref>\n* Power [[electronic circuit design]]<ref>J. ZHANG, H. Chung, W. L. Lo, and T. Huang, \"[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.4340&rep=rep1&type=pdf Extended Ant Colony Optimization Algorithm for Power Electronic Circuit Design]\", IEEE Transactions on Power Electronic. Vol.24,No.1, pp.147-162, Jan 2009.</ref>\n* [[Protein folding]]<ref>X. M. Hu, J. ZHANG，J. Xiao and Y. Li, \"[http://eprints.gla.ac.uk/5306/1/5306.pdf Protein Folding in Hydrophobic-Polar Lattice Model: A Flexible Ant- Colony Optimization Approach] \", Protein and Peptide Letters, Volume 15, Number 5, 2008, Pp. 469-477.</ref><ref>A. Shmygelska, R. A. Hernández and H. H. Hoos, \"[ftp://nozdr.ru/biblio/kolxo3/Cs/CsLn/Ant%20Algorithms,%203%20conf.,%20ANTS%202002(LNCS2463,%20Springer,%202002)(ISBN%203540441468)(318s).pdf#page=54 An ant colony optimization algorithm for the 2D HP protein folding problem],\" Proceedings of the 3rd International Workshop on Ant Algorithms/ANTS 2002, Lecture Notes in Computer Science, vol.2463, pp.40-52, 2002.</ref><ref>{{cite book |author1=M. Nardelli |author2=L. Tedesco |author3=A. Bechini |title= Cross-lattice behavior of general ACO folding for proteins in the HP model |journal= Proc. Of ACM SAC 2013|year=2013|pages=1320–1327 |doi= 10.1145/2480362.2480611|isbn=9781450316569 }}</ref>\n* System identification<ref>L. Wang and Q. D. Wu, \"Linear system parameters identification based on ant system algorithm,\" Proceedings of the IEEE Conference on Control Applications, pp. 401-406, 2001.</ref><ref>K. C. Abbaspour, R. Schulin, M. T. Van Genuchten, \"[https://www.ars.usda.gov/arsuserfiles/20360500/pdf_pubs/P1797.pdf Estimating unsaturated soil hydraulic parameters using ant colony optimization],\" Advances In Water Resources, vol. 24, no. 8, pp. 827-841, 2001.</ref>\n\n==Definition difficulty==\n[[File:Aco shortpath.svg|thumb]]\nWith an ACO algorithm, the shortest path in a graph, between two points A and B, is built from a combination of several paths.<ref>{{Cite journal | doi=10.1186/1471-2105-6-30| pmid=15710037| pmc=555464|year = 2005|last1 = Shmygelska|first1 = Alena| title=An ant colony optimisation algorithm for the 2D and 3D hydrophobic polar protein folding problem| journal=BMC Bioinformatics| volume=6| pages=30| last2=Hoos| first2=Holger H.}}</ref> It is not easy to give a precise definition of what algorithm is or is not an ant colony, because the definition may vary according to the authors and uses. Broadly speaking, ant colony algorithms are regarded as [[people|populated]] [[metaheuristics]] with each solution represented by an ant moving in the search space.<ref>Fred W. Glover,Gary A. Kochenberger, ''Handbook of Metaheuristics'', [https://books.google.com/books?id=P-HpBwAAQBAJ&pg=PA276&lpg=PA276&dq=aco+algorithms+with+guaranteed+convergence+to+the+optimal+solution+metaheuristics&source=bl&ots=4kyU_bZLpg&sig=zSrzu89MRED00H8QWjixBMkw11k&hl=fr&sa=X&ved=0ahUKEwjm4_2ysurTAhUHZ1AKHabGAZIQ6AEIODAE#v=onepage&q=aco%20algorithms%20with%20guaranteed%20convergence%20to%20the%20optimal%20solution%20metaheuristics&f=false], Springer (2003)</ref> Ants mark the best solutions and take account of previous markings to optimize their search. They can be seen as [[probabilistic]] [[multi-agent]] algorithms using a [[probability distribution]] to make the transition between each [[iteration]].<ref>http://www.multiagent.fr/extensions/ICAPManager/pdf/LauriCharpillet2006.pdf</ref> In their versions for combinatorial problems, they use an iterative construction of solutions.<ref>WJ Gutjahr , ''ACO algorithms with guaranteed convergence to the optimal solution'', [https://homes.di.unimi.it/cordone/courses/2016-ae/Lez07-Materiali/ACOAlgoithmsWithGuaranteedConvergenceToTheOptimalSolution.pdf], (2002)</ref> According to some authors, the thing which distinguishes ACO algorithms from other relatives (such as algorithms to estimate the distribution or particle swarm optimization) is precisely their constructive aspect. In combinatorial problems, it is possible that the best solution eventually be found, even though no ant would prove effective. Thus, in the example of the Travelling salesman problem, it is not necessary that an ant actually travels the shortest route: the shortest route can be built from the strongest segments of the best solutions. However, this definition can be problematic in the case of problems in real variables, where no structure of 'neighbours' exists. The collective behaviour of [[social insects]] remains a source of inspiration for researchers. The wide variety of algorithms (for optimization or not) seeking self-organization in biological systems has led to the concept of \"[[swarm intelligence]]\",<ref name=\"Waldner 2008 214\"/> which is a very general framework in which ant colony algorithms fit.\n\n==Stigmergy algorithms==\nThere is in practice a large number of algorithms claiming to be \"ant colonies\", without always sharing the general framework of optimization by canonical ant colonies (COA).<ref>Santpal Singh Dhillon , ''Ant Routing, Searching and Topology Estimation Algorithms for Ad Hoc Networks'', [https://books.google.com/books?id=j5fOJqhwcJoC&pg=PA33&dq=Stigmergy+algorithms&hl=fr&sa=X&ved=0ahUKEwjwjfaAtOrTAhWnLsAKHVPkCjYQ6AEIKTAB#v=onepage&q=Stigmergy%20algorithms&f=false], IOS Press, (2008)</ref> In practice, the use of an exchange of information between ants via the environment (a principle called \"[[stigmergy]]\") is deemed enough for an algorithm to belong to the class of ant colony algorithms. This principle has led some authors to create the term \"value\" to organize methods and behavior based on search of food, sorting larvae, division of labour and cooperative transportation.<ref>A. Ajith; G. Crina; R. Vitorino (éditeurs), ''Stigmergic Optimization'', Studies in Computational Intelligence , volume 31, 299 pages, 2006. {{ISBN|978-3-540-34689-0}}</ref>\n\n==Related methods==\n*[[Genetic algorithm]]s (GA) maintain a pool of solutions rather than just one. The process of finding superior solutions mimics that of evolution, with solutions being combined or mutated to alter the pool of solutions, with solutions of inferior quality being discarded.\n* An [[estimation of distribution algorithm]] (EDA) is an [[evolutionary algorithm]] that substitutes traditional reproduction operators by model-guided operators. Such models are learned from the population by employing machine learning techniques and represented as probabilistic graphical models, from which new solutions can be sampled<ref>{{cite book|last1=Pelikan|first1=Martin|last2=Goldberg|first2=David E.|last3=Cantú-Paz|first3=Erick|title=BOA: The Bayesian Optimization Algorithm|journal=Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 1|date=1 January 1999|pages=525–532|url=http://dl.acm.org/citation.cfm?id=2933973|isbn=9781558606111|series=Gecco'99}}</ref><ref>{{cite book|last1=Pelikan|first1=Martin|title=Hierarchical Bayesian optimization algorithm : toward a new generation of evolutionary algorithms|date=2005|publisher=Springer|location=Berlin [u.a.]|isbn=978-3-540-23774-7|edition=1st}}</ref> or generated from guided-crossover.<ref>{{cite book|last1=Thierens|first1=Dirk|title=The Linkage Tree Genetic Algorithm|journal=Parallel Problem Solving from Nature, PPSN XI|date=11 September 2010|pages=264–273|doi=10.1007/978-3-642-15844-5_27|language=en|isbn=978-3-642-15843-8}}</ref><ref>{{cite journal|last1=Martins|first1=Jean P.|last2=Fonseca|first2=Carlos M.|last3=Delbem|first3=Alexandre C. B.|title=On the performance of linkage-tree genetic algorithms for the multidimensional knapsack problem|journal=Neurocomputing|date=25 December 2014|volume=146|pages=17–29|doi=10.1016/j.neucom.2014.04.069}}</ref>\n*[[Simulated annealing]] (SA) is a related global optimization technique which traverses the search space by generating neighboring solutions of the current solution. A superior neighbor is always accepted. An inferior neighbor is accepted probabilistically based on the difference in quality and a temperature parameter. The temperature parameter is modified as the algorithm progresses to alter the nature of the search.\n* Reactive search optimization focuses on combining machine learning with optimization, by adding an internal feedback loop to self-tune the free parameters of an algorithm to the characteristics of the problem, of the instance, and of the local situation around the current solution.\n*[[Tabu search]] (TS) is similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest fitness of those generated. To prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.\n*[[Artificial immune system]] (AIS) algorithms are modeled on vertebrate immune systems.\n*[[Particle swarm optimization]] (PSO), a [[swarm intelligence]] method\n*[[Intelligent Water Drops|Intelligent water drops]] (IWD), a swarm-based optimization algorithm based on natural water drops flowing in rivers\n*Gravitational search algorithm (GSA), a [[swarm intelligence]] method\n*Ant colony clustering method (ACCM), a method that make use of clustering approach,extending the ACO.\n* [[Stochastic diffusion search]] (SDS), an agent-based probabilistic global search and optimization technique best suited to problems where the objective function can be decomposed into multiple independent partial-functions\n\n==History==\nThe inventors are [[Frans Moyson]] and [[Bernard Manderick]]. Pioneers of the field include [[Marco Dorigo]], [[Luca Maria Gambardella]].<ref>{{cite journal|last = Manderick, Moyson |first = Bernard, Frans |authorlink = Manderick, Bernard, and Moyson, Frans |title = The collective behavior of ants: An example of self-organization in massive parallelism.  |publisher = Proceedings of the AAAI Spring Symposium on Parallel Models of Intelligence |place = Stanford |year = 1988}}</ref>\n\n{{image frame|content=\n<timeline>\nImageSize = width:210 height:300\nPlotArea = width:170 height:280 left:40 bottom:10\n\nDateFormat = yyyy\nPeriod = from:1985 till:2005\nTimeAxis = orientation:vertical\nScaleMajor = unit:year increment:5 start:1985\n\nColors=\n   id:fond     value:white #rgb(0.95,0.95,0.98)\n   id:marque   value:rgb(1,0,0)\n   id:marque_fond value:rgb(1,0.9,0.9)\nBackgroundColors = canvas:fond\n\nDefine $dx = 7 # décalage du texte à droite de la barre\nDefine $dy = -3 # décalage vertical\nDefine $dy2 = 6 # décalage vertical pour double texte\n\nPlotData=\n  bar:Leaders color:marque_fond width:5 mark:(line,marque) align:left fontsize:S\n\n  from:1989  till:1989 shift:($dx,$dy)    text:studies of collective behavior\n  from:1991  till:1992 shift:($dx,$dy)    text:ant system (AS)\n  from:1995  till:1995 shift:($dx,$dy)    text:continuous problem (CACO)\n  from:1996  till:1996 shift:($dx,$dy)    text:ant colony system (ACS)\n  from:1996  till:1996 shift:($dx,$dy2)   text:max-min ant system (MMAS)\n  from:2000  till:2000 shift:($dx,$dy)   text:proof to convergence (GBAS)\n  from:2001  till:2001 shift:($dx,$dy)   text:multi-objective algorithm\n\n</timeline>|caption=Chronology of COA algorithms\n}}\n\nChronology of ant colony optimization algorithms.\n* 1959, [[Pierre-Paul Grassé]] invented the theory of [[stigmergy]] to explain the behavior of nest building in [[termites]];<ref>P.-P. Grassé, ''La reconstruction du nid et les coordinations inter-individuelles chez Belicositermes natalensis et Cubitermes sp. La théorie de la Stigmergie : Essai d’interprétation du comportement des termites constructeurs'', Insectes Sociaux, numéro 6, p. 41-80, 1959.</ref>\n* 1983, Deneubourg and his colleagues studied the [[collective behavior]] of [[ants]];<ref>J.L. Denebourg, J.M. Pasteels et J.C. Verhaeghe, ''[https://pdfs.semanticscholar.org/caac/71608a5e6b0907a8a99bba30d72d9a304152.pdf Probabilistic Behaviour in Ants : a Strategy of Errors?]'', Journal of Theoretical Biology, numéro 105, 1983.</ref>\n* 1988, and Moyson Manderick have an article on '''self-organization''' among ants;<ref name=\"F. Moyson, B. Manderick\">F. Moyson, B. Manderick, ''The collective behaviour of Ants : an Example of Self-Organization in Massive Parallelism'', Actes de AAAI Spring Symposium on Parallel Models of Intelligence, Stanford, Californie, 1988.</ref>\n* 1989, the work of Goss, Aron, Deneubourg and Pasteels on the '''collective behavior of Argentine ants''', which will give the idea of ant colony optimization algorithms;<ref name=\"S. Goss\">S. Goss, S. Aron, J.-L. Deneubourg et J.-M. Pasteels, ''[https://www.researchgate.net/profile/Serge_Aron/publication/301232811_Self-Organized_Shortcuts_in_the_Argentine_Ant/links/59967b5daca27283b11d9070/Self-Organized-Shortcuts-in-the-Argentine-Ant.pdf Self-organized shortcuts in the Argentine ant]'', Naturwissenschaften, volume 76, pages 579-581, 1989</ref>\n* 1989, implementation of a model of behavior for food by Ebling and his colleagues;<ref>M. Ebling, M. Di Loreto, M. Presley, F. Wieland, et D. Jefferson,''An Ant Foraging Model Implemented on the Time Warp Operating System'', Proceedings of the SCS Multiconference on Distributed Simulation, 1989</ref>\n* 1991, M. Dorigo proposed the '''ant system''' in his doctoral thesis (which was published in 1992<ref name=\"M. Dorigo, Optimization, Learning and Natural Algorithms\" />). A technical report extracted from the thesis and co-authored by V. Maniezzo and A. Colorni<ref>Dorigo M., V. Maniezzo et A. Colorni, ''Positive feedback as a search strategy'', rapport technique numéro 91-016, Dip. Elettronica, Politecnico di Milano, Italy, 1991</ref> was published five years later;<ref name=\"Ant system\" />\n* 1994, Appleby and Steward of British Telecommunications Plc published the first application to [[telecommunications]] networks<ref>Appleby, S. & Steward, S. Mobile software agents for control in telecommunications networks, BT Technol. J., 12(2):104–113, April 1994</ref>\n* 1996, publication of the article on ant system;<ref name=\"Ant system\" />\n* 1996, Hoos and Stützle invent the '''max-min ant system''';<ref name=\"T. Stützle et H.H. Hoos\" />\n* 1997, Dorigo and Gambardella publish the '''ant colony system''';<ref name=\"M. Dorigo et L.M. Gambardella\" />\n* 1997, Schoonderwoerd and his colleagues published an improved application to [[telecommunication]] networks;<ref>R. Schoonderwoerd, O. Holland, J. Bruten et L. Rothkrantz, ''[https://pdfs.semanticscholar.org/f09e/03c5d759c7ca04e443d496e23c981f1b4a5d.pdf Ant-based load balancing in telecommunication networks]'', Adaptive Behaviour, volume 5, numéro 2, pages 169-207, 1997</ref>\n* 1998, Dorigo launches first conference dedicated to the ACO algorithms;<ref>M. Dorigo, ''ANTS’ 98, From Ant Colonies to Artificial Ants : First International Workshop on Ant Colony Optimization, ANTS 98'', Bruxelles, Belgique, octobre 1998.</ref>\n* 1998, Stützle proposes initial '''parallel implementations''';<ref>T. Stützle, ''Parallelization Strategies for Ant Colony Optimization'', Proceedings of PPSN-V, Fifth International Conference on Parallel Problem Solving from Nature, Springer-Verlag, volume 1498, pages 722-731, 1998.</ref>\n* 1999, Bonabeau, Dorigo and Theraulaz publish a book dealing mainly with artificial ants<ref>É. Bonabeau, M. Dorigo et G. Theraulaz, ''Swarm intelligence'', Oxford University Press, 1999.</ref>\n* 2000, special issue of the Future Generation Computer Systems journal on ant algorithms<ref>M. Dorigo , G. Di Caro et T. Stützle, ''[https://www.academia.edu/download/30765111/FGCS-Editorial-final.pdf Special issue on \"Ant Algorithms]\"'', Future Generation Computer Systems, volume 16, numéro 8, 2000</ref>\n* 2000, first applications to the [[Scheduling algorithm|scheduling]], scheduling sequence and the [[constraint satisfaction|satisfaction of constraints]];\n* 2000, Gutjahr provides the first evidence of [[limit of a sequence|convergence]] for an algorithm of ant colonies<ref>W.J. Gutjahr, ''[http://iridia.ulb.ac.be/~mdorigo/ACO/downloads/ants5.pdf A graph-based Ant System and its convergence]'', Future Generation Computer Systems, volume 16, pages 873-888, 2000.</ref>\n* 2001, the first use of COA algorithms by companies ([http://www.eurobios.com/ Eurobios] and [http://www.antoptima.com/ AntOptima]);\n* 2001, Iredi and his colleagues published the first '''multi-objective''' algorithm<ref>S. Iredi, D. Merkle et M. Middendorf, ''[https://link.springer.com/chapter/10.1007/3-540-44719-9_25 Bi-Criterion Optimization with Multi Colony Ant Algorithms]'', Evolutionary Multi-Criterion Optimization, First International Conference (EMO’01), Zurich, Springer Verlag, pages 359-372, 2001.</ref>\n* 2002, first applications in the design of schedule, Bayesian networks;\n* 2002, Bianchi and her colleagues suggested the first algorithm for [[stochastic]] problem;<ref>L. Bianchi, L.M. Gambardella et M.Dorigo, ''[http://hcot.ir/wp-content/uploads/2015/03/An-Ant-Colony-Optimization-Approach-to-the-Probabilistic-Traveling-Salesman-Problem.pdf An ant colony optimization approach to the probabilistic traveling salesman problem]'', PPSN-VII, Seventh International Conference on Parallel Problem Solving from Nature, Lecture Notes in Computer Science, Springer Verlag, Berlin, Allemagne, 2002.</ref>\n* 2004, Dorigo and Stützle publish the Ant Colony Optimization book with MIT Press <ref>M. Dorigo and T. Stützle, ''Ant Colony Optimization'', MIT Press, 2004.</ref>\n* 2004, Zlochin and Dorigo show that some algorithms are equivalent to the [[stochastic gradient descent]], the [[cross-entropy method]] and [[algorithms to estimate distribution]]<ref name=\"Zlochin model-based search\"/>\n* 2005, first applications to [[protein folding]] problems.\n* 2012, Prabhakar and colleagues publish research relating to the operation of individual ants communicating in tandem without pheromones, mirroring the principles of computer network organization. The communication model has been compared to the [[Transmission Control Protocol]].<ref>B. Prabhakar, K. N. Dektar, D. M. Gordon, \"The regulation of ant colony foraging activity without spatial information \", PLOS Computational Biology, 2012. URL: http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002670</ref>\n* 2016, first application to peptide sequence design.<ref name=\":0\" />\n* 2017, successful integration of the multi-criteria decision-making method PROMETHEE into the ACO algorithm ([[HUMANT (HUManoid ANT) algorithm|HUMANT algorithm]]).<ref>{{cite journal|last1=Mladineo|first1=Marko|last2=Veza|first2=Ivica|last3=Gjeldum|first3=Nikola|title=Solving partner selection problem in cyber-physical production networks using the HUMANT algorithm|journal=International Journal of Production Research|date=2017|volume=55|issue=9|pages=2506–2521|doi=10.1080/00207543.2016.1234084}}</ref>\n\n==References==\n{{Reflist|30em}}\n\n==Publications (selected)==\n* [[Marco Dorigo|M. Dorigo]], 1992. ''Optimization, Learning and Natural Algorithms'', PhD thesis, Politecnico di Milano, Italy.\n* M. Dorigo, V. Maniezzo & A. Colorni, 1996. \"[http://www.cs.unibo.it/babaoglu/courses/cas05-06/tutorials/Ant_Colony_Optimization.pdf Ant System: Optimization by a Colony of Cooperating Agents]\", IEEE Transactions on Systems, Man, and Cybernetics–Part B, 26 (1): 29–41.\n* M. Dorigo & [[Luca Maria Gambardella|L. M. Gambardella]], 1997. \"[http://www.idsia.ch/~luca/acs-ec97.pdf Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem]\". IEEE Transactions on Evolutionary Computation, 1 (1): 53–66.\n* M. Dorigo, G. Di Caro & L. M. Gambardella, 1999. \"[http://people.idsia.ch/~gianni/Papers/ArtificialLife-original.pdf Ant Algorithms for Discrete Optimization]\". Artificial Life, 5 (2): 137–172.\n* E. Bonabeau, M. Dorigo et G. Theraulaz, 1999. ''Swarm Intelligence: From Natural to Artificial Systems'', Oxford University Press. {{ISBN|0-19-513159-2}}\n* M. Dorigo & T. Stützle, 2004. ''Ant Colony Optimization'', MIT Press. {{ISBN|0-262-04219-3}}\n* M. Dorigo, 2007. [http://www.scholarpedia.org/article/Ant_Colony_Optimization  \"Ant Colony Optimization\"]. Scholarpedia.\n* C. Blum, 2005 \"[http://aisii.azc.uam.mx/mcbc/Cursos/IntCompt/Lectura16.pdf Ant colony optimization: Introduction and recent trends]\". Physics of Life Reviews, 2: 353-373\n* M. Dorigo, M. Birattari & T. Stützle, 2006 ''[http://iridia.ulb.ac.be/IridiaTrSeries/IridiaTr2006-023r001.pdf Ant Colony Optimization: Artificial Ants as a Computational Intelligence Technique]''. TR/IRIDIA/2006-023\n* Mohd Murtadha Mohamad,\"Articulated Robots Motion Planning Using Foraging Ant Strategy\",Journal of Information Technology - Special Issues in Artificial Intelligence, Vol.20, No. 4 pp.&nbsp;163–181, December 2008, {{ISSN|0128-3790}}.\n* N. Monmarché, F. Guinand & P. Siarry (eds), \"Artificial Ants\", August 2010 Hardback 576 pp.&nbsp;{{ISBN|978-1-84821-194-0}}.\n* A. Kazharov, V. Kureichik, 2010. \"[https://www.researchgate.net/profile/Asker_Kazharov/publication/225549674_Ant_colony_optimization_algorithms_for_solving_transportation_problems/links/56e0268e08aec4b3333d0039.pdf Ant colony optimization algorithms for solving transportation problems]\", Journal of Computer and Systems Sciences International, Vol. 49. No. 1. pp.&nbsp;30–43.\n* C-M. Pintea, 2014, [https://www.springer.com/la/book/9783642401787 Advances in Bio-inspired Computing for Combinatorial Optimization Problem], Springer {{ISBN|978-3-642-40178-7}}\n* K. Saleem, N. Fisal, M. A. Baharudin, A. A. Ahmed, S. Hafizah and S. Kamilah, \"Ant colony inspired self-optimized routing protocol based on cross layer architecture for wireless sensor networks\", WSEAS Trans. Commun., vol. 9, no. 10, pp.&nbsp;669–678, 2010. {{ISBN|978-960-474-200-4}}\n* K. Saleem and N. Fisal, \"Enhanced Ant Colony algorithm for self-optimized data assured routing in wireless sensor networks\", Networks (ICON) 2012 18th IEEE International Conference on, pp.&nbsp;422–427. {{ISBN|978-1-4673-4523-1}}\n\n==External links==\n*[http://www.aco-metaheuristic.org/ Ant Colony Optimization Home Page]\n*[http://vk.com/ant_colony_optimization \"Ant Colony Optimization\" - Russian scientific and research community]\n*[https://web.archive.org/web/20080616044645/http://www.nightlab.ch/antsim/ AntSim - Simulation of Ant Colony Algorithms]\n*[http://www.midaco-solver.com/ MIDACO-Solver] General purpose optimization software based on ant colony optimization (Matlab, Excel, VBA, C/C++, R, C#, Java, Fortran and Python)\n* [https://web.archive.org/web/20110719105224/http://ems.eit.uni-kl.de/index.php?id=156 University of Kaiserslautern, Germany, AG Wehn: Ant Colony Optimization Applet] Visualization of Traveling Salesman solved by ant system with numerous options and parameters (Java Applet)\n*[http://webspace.webring.com/people/br/raguirre/hormigas/antfarm/ Ant Farm Simulator]\n*[http://www.djoh.net/inde/ANTColony/applet.html Ant algorithm simulation (Java Applet)]\n*[https://github.com/ugochirico/Java-Ant-Colony-System-Framework Java Ant Colony System Framework]\n\n{{collective animal behaviour}}\n\n{{DEFAULTSORT:Ant Colony Optimization}}\n[[Category:Articles which contain graphical timelines]]\n[[Category:Nature-inspired metaheuristics]]\n[[Category:Algorithms]]"
    },
    {
      "title": "Artificial ants",
      "url": "https://en.wikipedia.org/wiki/Artificial_ants",
      "text": "#REDIRECT [[Ant colony optimization algorithms]] {{R from merge}}\n\n[[Category:Multi-robot systems]]\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Artificial bee colony algorithm",
      "url": "https://en.wikipedia.org/wiki/Artificial_bee_colony_algorithm",
      "text": "{{Third-party sources|date=April 2017}}\n{{One source|date=April 2017}}\n{{confused|Bees algorithm}}\nIn [[computer science]] and [[operations research]], the '''artificial bee colony algorithm''' ('''ABC''') is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Derviş Karaboğa ([[Erciyes University]]) in 2005.<ref name=Karaboga2005>{{cite journal| last1=Karaboğa| first1=Derviş| title=An Idea Based on Honey Bee Swarm For Numerical Optimization| date=2005| url= https://pdfs.semanticscholar.org/015d/f4d97ed1f541752842c49d12e429a785460b.pdf}}</ref>\n\n== Algorithm ==\nIn the ABC model, the colony consists of three groups of bees: employed bees, onlookers and scouts. It is assumed that there is only one artificial employed bee for each food source. In other words, the number of employed bees in the colony is equal to the number of food sources around the hive. Employed bees go to their food source and come back to hive and dance on this area. The employed bee whose food source has been abandoned becomes a scout and starts to search for finding a new food source. Onlookers watch the dances of employed bees and choose food sources depending on dances. The main steps of the algorithm are given below.:<ref name=Karaboga2005/>\n\n* Initial food sources are produced for all employed bees\n* REPEAT\n** Each employed bee goes to a food source in her memory and determines a closest source, then evaluates its nectar amount and dances in the hive\n** Each onlooker watches the dance of employed bees and chooses one of their sources depending on the dances, and then goes to that source. After choosing a neighbour around that, she evaluates its nectar amount.\n** Abandoned food sources are determined and are replaced with the new food sources discovered by scouts.\n** The best food source found so far is registered.\n* UNTIL (requirements are met)\n\nIn ABC, a population based algorithm, the position of a food source represents a possible solution to the optimization problem and the nectar amount of a food source corresponds to the quality (fitness) of the associated solution. The number of the employed bees is equal to the number of solutions in the population. At the first step, a randomly distributed initial population (food source positions) is generated. After initialization, the population is subjected to repeat the cycles of the search processes of the employed, onlooker, and scout bees, respectively. An employed bee produces a modification on the source position in her memory and discovers a new food source position. Provided that the nectar amount of the new one is higher than that of the previous source, the bee memorizes the new source position and forgets the old one. Otherwise she keeps the position of the one in her memory. After all employed bees complete the search process, they share the position information of the sources with the onlookers on the dance area. Each onlooker evaluates the nectar information taken from all employed bees and then chooses a food source depending on the nectar amounts of sources. As in the case of the employed bee, she produces a modification on the source position in her memory and checks its nectar amount. Providing that its nectar is higher than that of the previous one, the bee memorizes the new position and forgets the old one. The sources abandoned are determined and new sources are randomly produced to be replaced with the abandoned ones by artificial scouts.\n\n== Artificial bee colony algorithm ==\n\nArtificial bee colony (ABC) algorithm is an optimization technique that simulates the foraging behavior of honey bees, and has been successfully applied to various practical problems. ABC belongs to the group of swarm intelligence algorithms and was proposed by Karaboga in 2005.\n\nA set of honey bees, called swarm, can successfully accomplish tasks through social cooperation. In the ABC algorithm, there are three types of bees: employed bees, onlooker bees, and scout bees. The employed bees search food around the food source in their memory; meanwhile they share the information of these food sources to the onlooker bees. The onlooker bees tend to select good food sources from those found by the employed bees. The food source that has higher quality (fitness) will have a large chance to be selected by the onlooker bees than the one of lower quality. The scout bees are translated from a few employed bees, which abandon their food sources and search new ones.\n\nIn the ABC algorithm, the first half of the swarm consists of employed bees, and the second half constitutes the onlooker bees.\n\nThe number of employed bees or the onlooker bees is equal to the number of solutions in the swarm. The ABC generates a randomly distributed initial population of SN solutions (food sources), where SN denotes the swarm size.\n\nLet <math>X_i=\\{x_{i,1},x_{i,2},\\ldots,x_{i,n}\\}</math> represent the <math>i^{th}</math> solution in the swarm, where <math>n</math> is the dimension size.\n\nEach employed bee <math>X_{i}</math> generates a new candidate solution <math>V_{i}</math> in the neighborhood of its present position as equation below:\n\n<center><math>v_{i,k} = x_{i,k}+\\Phi_{i,k}\\times (x_{i,k}-x_{j,k})</math></center>\n\nwhere <math>X_j</math> is a randomly selected candidate solution (<math>i\\neq j</math>), <math>k</math> is a random dimension index selected from the set <math>\\{1,2,\\ldots,n\\}</math>, and <math>\\Phi_{i,k}</math> is a random number within <math>[-1,1]</math>. Once the new candidate solution <math>V_i</math> is generated, a greedy selection is used. If the fitness value of <math>V_i</math> is better than that of its parent <math>X_i</math>, then update <math>X_i</math> with <math>V_i</math>; otherwise keep <math>X_i</math> unchanged. After all employed bees complete the search process; they share the information of their food sources with the onlooker bees through waggle dances. An onlooker bee evaluates the nectar information taken from all employed bees and chooses a food source with a probability related to its nectar amount. This probabilistic selection is really a roulette wheel selection mechanism which is described as equation below:\n\n<center><math>P_i=\\frac{\\mathrm {fit}_i}{\\sum_j{\\mathrm {fit}_j}}</math></center>\n\nwhere <math>\\mathrm {fit}_i</math> is the fitness value of the <math>i^{th}</math> solution in the swarm. As seen, the better the solution <math>i</math>, the higher the probability of the <math>i^{th}</math> food source selected. If a position cannot be improved over a predefined number (called limit) of cycles, then the food source is abandoned. Assume that the abandoned source is <math>X_i</math>, and then the scout bee discovers a new food source to be replaced with <math>i^{th}</math> as equation below:\n\n<center><math>x_{i,k}=lb_i+\\Phi_{i,k}\\times(ub_i-lb_i)</math></center>\n\nwhere <math>\\Phi_{i,k}=\\mathrm {rand}(0,1)</math> is a random number within<math>[0,1]</math> based on a normal{{Unreliable source?|date=May 2019}} distribution, and <math>lb_i, ub_i</math> are lower and upper boundaries of the <math>i^{th}</math> dimension, respectively.\n\n== See also ==\n* [[Evolutionary computation]]\n* [[Evolutionary multi-modal optimization]]\n* [[Particle swarm optimization]]\n* [[Swarm intelligence]]\n* [[Bees algorithm]]\n* [[Fish School Search]]\n*[[List of metaphor-based metaheuristics]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Citation |url=http://mf.erciyes.edu.tr/abc  |title=Artificial Bee Colony (ABC) Algorithm Homepage |publisher=Intelligent Systems Research Group, Department of Computer Engineering, [[Erciyes University]] |publication-place=Turkey}}\n\n{{Optimization algorithms}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Bat algorithm",
      "url": "https://en.wikipedia.org/wiki/Bat_algorithm",
      "text": "The '''Bat algorithm''' is a [[metaheuristic]] algorithm for [[global optimization]]. It was inspired by the echolocation behaviour of [[microbats]], with varying pulse rates of emission and loudness.<ref>J. D. Altringham, Bats: Biology and Behaviour, Oxford University Press, (1996).</ref><ref>P. Richardson, Bats. Natural History Museum, London, (2008)</ref> The Bat algorithm was developed by [[Xin-She Yang]] in 2010.<ref>{{cite journal | last1 = Yang | first1 = X. S. | year = 2010 | title = A New Metaheuristic Bat-Inspired Algorithm, in: Nature Inspired Cooperative Strategies for Optimization (NISCO 2010) | arxiv = 1004.4170| journal = Studies in Computational Intelligence | volume = 284 | issue = | pages = 65–74 | bibcode = 2010arXiv1004.4170Y }}</ref>\n\n== Metaphor ==\nThe idealization of the [[Animal echolocation|echolocation]] of microbats can be summarized as follows: Each virtual bat flies randomly with a velocity <math>v_i</math> at position (solution) <math>x_i</math> with a varying frequency or wavelength and loudness <math>A_i</math>. As it searches and finds its prey, it changes frequency, loudness and pulse emission rate <math>r</math>. Search is intensified by a local [[random walk]]. Selection of the best continues until certain stop criteria are met. This essentially uses a frequency-tuning technique to control the dynamic behaviour of a swarm of bats, and the balance between exploration and exploitation can be controlled by tuning algorithm-dependent parameters in bat algorithm.\n\nA detailed introduction of metaheuristic algorithms including the bat algorithm is given by Yang<ref>Yang, X. S., [https://books.google.com/books?hl=en&lr=&id=iVB_ETlh4ogC&oi=fnd&pg=PR5&dq=%22Nature-Inspired+Metaheuristic+Algorithms,+2nd+Edition%22+luniver&ots=DwgtqhEKua&sig=bcpfrzMR691SLIzIUIiA0GkJdHo#v=onepage&q=bat%20algorithm&f=false Nature-Inspired Metaheuristic Algorithms], 2nd Edition, Luniver Press, (2010).</ref> where a demo program in [[MATLAB]]/[[GNU Octave]] is available, while a comprehensive review is carried out by Parpinelli and Lopes.<ref>{{cite journal | last1 = Parpinelli | first1 = R. S. | last2 = Lopes | first2 = H. S. | year = 2011 | title = New inspirations in swarm intelligence: a survey,Int | url = | journal = J. Bio-Inspired Computation | volume = 3 | issue = | pages = 1–16 | doi=10.1504/ijbic.2011.038700}}</ref> A further improvement is the development of an evolving bat algorithm (EBA) with better efficiency.<ref>{{cite journal | last1 = Tsai | first1 = P. W. | last2 = Pan | first2 = J. S. | last3 = Liao | first3 = B. Y. | last4 = Tsai | first4 = M. J. | last5 = Istanda | first5 = V. | year = 2012 | title = Bat algorithm inspired algorithm for solving numerical optimization problems | url = | journal = Applied Mechanics and Materials | volume = 148-149 | issue = | pages = 134–137 | doi=10.4028/www.scientific.net/amm.148-149.134| bibcode = 2011AMM...148..134T }}</ref>\n\n== See also ==\n[[List of metaphor-based metaheuristics]]\n\n==References==\n{{Reflist|33em}}\n\n== Further reading ==\n*Yang, X.-S. (2014), ''Nature-Inspired Optimization Algorithms'', [[Elsevier]].\n\n{{swarming}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Bees algorithm",
      "url": "https://en.wikipedia.org/wiki/Bees_algorithm",
      "text": "{{distinguish|Artificial bee colony algorithm}}\nIn [[computer science]] and [[operations research]], the '''bees algorithm''' is a population-based [[search algorithm]] which was developed by Pham, Ghanbarzadeh et al. in 2005.<ref name=\"Pham & al, 2005\">Pham DT, Ghanbarzadeh A, Koc E, Otri S, Rahim S and Zaidi M. The Bees Algorithm. Technical Note, Manufacturing Engineering Centre, Cardiff University, UK, 2005.</ref> It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both [[combinatorial optimization]] and [[continuous optimization]]. The only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined. The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies.<ref name=\"Pham & Castellani, 2009\">Pham, D.T., Castellani, M. (2009), [http://pic.sagepub.com/content/223/12/2919.short The Bees Algorithm – Modelling Foraging Behaviour to Solve Continuous Optimisation Problems]. Proc. ImechE, Part C, 223(12), 2919-2938.</ref><ref>Pham, D.T. and Castellani, M. (2013), [https://link.springer.com/article/10.1007/s00500-013-1104-9 Benchmarking and Comparison of Nature-Inspired Population-Based Continuous Optimisation Algorithms], Soft Computing, 1-33.</ref><ref>Pham, D.T. and Castellani, M. (2015), [http://www.tandfonline.com/doi/abs/10.1080/23311916.2015.1091540 A comparative study of the bees algorithm as a tool for function optimisation], Cogent Engineering 2(1), 1091540.</ref><ref name=\"Nasrinpour & Massah & Teshnehlab 2017\">Nasrinpour, H. R., Massah Bavani, A., Teshnehlab, M., (2017), [http://www.mdpi.com/2073-431X/6/1/5 Grouped Bees Algorithm: A Grouped Version of the Bees Algorithm], Computers 2017, 6(1), 5; (doi: 10.3390/computers6010005)</ref>\n\n== Metaphor ==\nA colony of [[honey bees]] can extend itself over long distances (over 14&nbsp;km)<ref name=\"Tereshko & Loengarov, 2005\">Tereshko V., Loengarov A., (2005) [http://cis.uws.ac.uk/research/journal/V9/V9N3/bees.pdf Collective Decision-Making in Honey Bee Foraging Dynamics]. Journal of Computing and Information Systems, 9(3), 1-7.</ref> and in multiple directions simultaneously to harvest nectar or pollen from multiple food sources (flower patches). \nA small fraction of the colony constantly searches the environment looking for new flower patches. These scout bees move randomly in the area surrounding the hive, evaluating the profitability (net energy yield) of the food sources encountered.<ref name=\"Tereshko & Loengarov, 2005\"/> When they return to the hive, the scouts deposit the food harvested. Those individuals that found a highly profitable food source go to an area in the hive called the “dance floor”, and perform a ritual known as the [[waggle dance]].<ref>Von Frisch, K. (1967) The Dance Language and Orientation of Bees. Harvard University Press, Cambridge, Massachusetts.</ref> \nThrough the waggle dance a scout bee communicates the location of its discovery to idle onlookers, which join in the exploitation of the flower patch. Since the length of the dance is proportional to the scout’s rating of the food source, more foragers get recruited to harvest the best rated flower patches. After dancing, the scout returns to the food source it discovered to collect more food. \nAs long as they are evaluated as profitable, rich food sources will be advertised by the scouts when they return to the hive. Recruited foragers may waggle dance as well, increasing the recruitment for highly rewarding flower patches. Thanks to this autocatalytic process, the bee colony is able to quickly switch the focus of the foraging effort on the most profitable flower patches.<ref name=\"Tereshko & Loengarov, 2005\"/>\n\n== Algorithm ==\nThe bees algorithm<ref name=\"Pham & Castellani, 2009\"/><ref name=\"Pham & Ghanbarzadeh et. a. 2006\">Pham D.T., Ghanbarzadeh A., Koc E., Otri S., Rahim S., Zaidi M., The Bees Algorithm, A Novel Tool for Complex Optimisation Problems, Proc 2nd Int Virtual Conf on Intelligent Production Machines and Systems (IPROMS 2006), Oxford: Elsevier, pp. 454-459, 2006.</ref> mimics the foraging strategy of honey bees to look for the best solution to an optimisation problem. Each candidate solution is thought of as a food source (flower), and a population (colony) of ''n'' agents (bees) is used to search the solution space. Each time an artificial bee visits a flower (lands on a solution), it evaluates its profitability (fitness).\n\nThe bees algorithm consists of an initialisation procedure and a main search cycle which is iterated for a given number ''T'' of times, or until a solution of acceptable fitness is found. Each search cycle is composed of five procedures: recruitment, local search, neighbourhood shrinking, site abandonment, and global search.\n\n '''Pseudocode for the standard bees algorithm'''<ref name=\"Pham & Castellani, 2009\"/>\n    1 for i=1,…,ns\t\t\t\t\n        i  scout[i]=Initialise_scout()\n        ii flower_patch[i]=Initialise_flower_patch(scout[i])\n    2 do until stopping_condition=TRUE\t\t\n        i   Recruitment() \t\n        ii  for i =1,...,nb\n              1 flower_patch[i]=Local_search(flower_patch[i])\n              2 flower_patch[i]=Site_abandonment(flower_patch[i])\n              3 flower_patch[i]=Neighbourhood_shrinking(flower_patch[i])\t\t\n        iii for i = nb,...,ns\n              1 flower_patch[i]=Global_search(flower_patch[i])}\n\nIn the initialisation routine ''ns'' scout bees are randomly placed in the search space, and evaluate the fitness of the solutions where they land. For each solution, a neighbourhood (called flower patch) is delimited.\n\nIn the recruitment procedure, the scouts that visited the ''nb''≤''ns'' fittest solutions (best sites) perform the waggle dance. That is, they recruit foragers to search further the neighbourhoods of the most promising solutions. The scouts that located the very best ''ne''≤''nb'' solutions (elite sites) recruit ''nre'' foragers each, whilst the remaining ''nb''-''ne'' scouts recruit ''nrb''≤''nre'' foragers each. Thus, the number of foragers recruited depends on the profitability of the food source.\n\nIn the local search procedure, the recruited foragers are randomly scattered within the flower patches enclosing the solutions visited by the scouts (local exploitation). If any of the foragers in a flower patch lands on a solution of higher fitness than the solution visited by the scout, that forager becomes the new scout. If no forager finds a solution of higher fitness, the size of the flower patch is shrunk (neighbourhood shrinking procedure). Usually, flower patches are initially defined over a large area, and their size is gradually shrunk by the neighbourhood shrinking procedure. As a result, the scope of the local exploration is progressively focused on the area immediately close to the local fitness best. If no improvement in fitness is recorded in a given flower patch for a pre-set number of search cycles, the local maximum of fitness is considered found, the patch is abandoned (site abandonment), and a new scout is randomly generated.\n\nAs in biological bee colonies,<ref name=\"Tereshko & Loengarov, 2005\"/> a small number of scouts keeps exploring the solution space looking for new regions of high fitness (global search). The global search procedure re-initialises the last ''ns''-''nb'' flower patches with randomly generated solutions.\n\nAt the end of one search cycle, the scout population is again composed of ''ns'' scouts: ''nr'' scouts produced by the local search procedure (some of which may have been re-initialised by the site abandonment procedure), and ''ns''-''nb'' scouts generated by the global search procedure. The total artificial bee colony size is ''n''=''ne''•''nre''+(''nb''-''ne'')•''nrb''+''ns'' (elite sites foragers + remaining best sites foragers + scouts) bees.\n\n== Variants ==\nIn addition to the basic bees algorithm,<ref name=\"Pham & Ghanbarzadeh et. a. 2006\"/> there are a number of improved or hybrid versions of the BA, each of which focuses on some shortcomings of the basic BA. These variants include (but are not limited to) fuzzy or enhanced BA (EBA),<ref name=\"Pham & Darwish 008\">Pham D. T., Haj Darwish A., (2008), A. Fuzzy Selection of Local Search Sites in the Bees Algorithm.  Proceedings of Innovative Production Machines and Systems (IPROMS 2008)</ref> grouped BA (GBA),<ref name=\"Nasrinpour & Massah & Teshnehlab 2017\"/> hybrid modified BA (MBA)<ref name=\"Pham & Pham & Castellani 2011\">Pham Q. T., Pham D. T., Castellani M., A modified Bees Algorithm and a statistics-based method for tuning its parameters. Proceedings of the Institution of Mechanical Engineers (ImechE), Part I: Journal of Systems and Control Eng., 2011 (doi:10.1177/0959651811422759)</ref> and so on.\nThe pseudo-code for the '''grouped BA (GBA)''' <ref name=\"Nasrinpour & Massah & Teshnehlab 2017\"/> is as follows.\n\n<source lang=\"matlab\">\nfunction GBA\n %% Set the problem parameters\nmaxIteration = ..;\t\t\t% number of iterations (e.g. 1000-5000)\nmaxParameters = ..;\t\t\t% number of input variables\nmin = [..] ;\t\t\t\t% an array of the size maxParameters to indicate the minimum value of each input parameter \nmax = [..] ;\t\t\t\t% an array of the size maxParameters to indicate the maximum value of each input parameter \t\n\n %% Set the grouped bees algorithm (GBA) parameters\nR_ngh = ..;\t            % patch radius of the neighborhood search for bees in the first group (e.g. 0.001 - 1)\nn = ..;\t\t\t\t\t% number of scout bees (e.g. 4-30)\nnGroups = ..;\t\t\t% number of groups, excluding the random group\n\n %% GBA's automatic parameter settings\nk = 3 * n / ((nGroups+1)^3 - 1); \t% GBA's parameter to set the number of scout bees in each group\ngroups = zeros(1,nGroups);    \t\t% An array to keep the number of scout bees for each group\nrecruited_bees = zeros(1,nGroups);\t% An array to keep the number of recruited bees for each group\na = (((max - min) ./ 2) - R_ngh) ./ (nGroups^2 - 1);\t% GBA's parameter for setting neighborhood radiuses\nb = R_ngh - a;\t\t\t\t\t\t\t\t\t\t\t% GBA's parameter for setting neighborhood radiuses\nfor i=1:nGroups % For each group\n    groups(i) = floor(k*i^2);\t\t\t% determine the number of scout bees in each group\n    if groups(i) == 0\n        groups(i) = 1;\t\t\t\t\t% there has to be at least one scout bee per each group\n    end\n\trecruited_bees = (nGroups+1-i)^2;\t% set the number of recruited bees for each group\n\tngh(i) = a * i*i + b;\t\t\t\t% set the radius patch for each group\nend\ngroup_random = n - sum(groups);\t\t\t% assign the remainder bees (if any) to random search\ngroup_random = max(group_random,0);\t\t% make sure it is not a negative number\n\n %% initialize the population matrix\npopulation = zeros(n,maxParameters+1); \t% A population of n bees including all input variables and their fitness\nfor i=1:n\n    population(i,1:maxParameters)= generate_random_solution(maxParameters,min, max);\t% random initialization of maxParameters variables between max and min\n    population(i,maxParameters+1) = evalulate_fitness(population(i,:));\t\t\t\t\t% fitness evaluation of each solution and saving it at the last index of the population matrix\nend\n\nsorted_population = sortrows(population); % sort the population based on their fitnesses\n\n %% Iterations of the grouped bees algorithm\nfor i=1:maxIteration         \t% GBA's main loop\n\tbeeIndex = 0;\t\t\t\t% keep track of all bees (i.e, patches)\n\tfor g=1:nGroups \t\t\t% for each group of scout bees\t\n\t\tfor j =  1 : groups(g) \t% exploit each patch within each group\n\t\t\tbeeIndex = beeIndex + 1;\t\t% increase the counter per each patch\n\t\t\tfor i = 1 : recruited_bees(g)\t% for each recruited bees of the group\n\t\t\t\tsolution = bee_waggle_dance(sorted_population(beeIndex,1:maxParameters),ngh(g));\t\t\t% search the neighborhood around selected patch/solution within the radius of ngh\n\t\t\t\tfit = evaluate_fitness(solution);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t% evaluate the fitness of recently found solution\n\t\t\t\tif  fit < sorted_population(beeIndex,maxParameters+1) % A minimization problem: if a better location/patch/solution is found by the recuiter bee\n\t\t\t\t\tsorted_population(beeIndex,1 : maxParameters+1) = [solution(1 : maxParameters),fit];\t% copy new solution and its fitness to the sorted population matrix\n\t\t\t\tend\t\n\t\t\tend\n\t\tend\n\tend\n\n\tfor i= 1 : group_random % For the remaining random bees\n\t\tbeeIndex = beeIndex + 1;\n\t\tsolution(beeIndex,1:maxParameters)= generate_random_solution(maxParameters,min, max); \t% generate a new random solution at the index beeIndex\n\t\tsolution(beeIndex,maxParameters+1)= evaluate_fitness(solution);\t\t\t\t\t\t\t% evaluate its fitness\n\t\tsorted_population(beeIndex,:) = [solution(1 : maxParameters),fit]; \t\t\t\t\t\t% copy the new random solution and its fitness to the sorted population matrix\n\tend\n\t\n\tsorted_population=sortrows(sorted_population); \t% sort the population based on their fitnesses\n\tBest_solution_sofar=sorted_population(1,:);\n\t\n\tdisp('Best:');disp(Best_solution_sofar); % Display the best solution of current iteration\nend % end of GBA's main loop \nend % end of main function\n\n%% Function Bee Waggle Dance\nfunction new_solution=bee_waggle_dance(solution, ngh, maxParameters)\n    new_solution(1:maxParameters) = (solution-ngh)+(2*ngh.*rand(1, maxParameters));\nend\n</source>\n\n==See also==\n*[[Ant colony optimization algorithms]]\n*[[Artificial bee colony algorithm]]\n*[[Evolutionary computation]]\n*[[Lévy flight foraging hypothesis]]\n*[[Manufacturing Engineering Centre]]\n*[[Mathematical optimization]]\n*[[Metaheuristic]]\n*[[Particle swarm optimization]]\n*[[Swarm intelligence]]\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==External links==\n*[http://beesalgorithmsite.altervista.org/ The bees algorithm website]\n*[http://news.bbc.co.uk/1/hi/wales/south_east/5285106.stm  Boffins put dancing bees to work &ndash; BBC News]\n\n{{collective animal behaviour}}\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Bees algorithm}}\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Biogeography-based optimization",
      "url": "https://en.wikipedia.org/wiki/Biogeography-based_optimization",
      "text": "\n'''Biogeography-based optimization''' ('''BBO''') is an [[evolutionary algorithm]] (EA) that [[optimization|optimizes]] a [[function (mathematics)|function]] by [[stochastic]]ally and [[iterative method|iteratively]] improving [[candidate solution]]s with regard to a given measure of quality, or [[fitness function]]. BBO belongs to the class of [[metaheuristic]]s since it includes many variations, and since it does not make any assumptions about the problem and can therefore be applied to a wide class of problems.\n\nBBO is typically used to optimize multidimensional real-valued functions, but it does not use the [[gradient]] of the function, which means that it does not require the function to be [[Differentiable function|differentiable]] as required by classic optimization methods such as [[gradient descent]] and [[quasi-newton methods]]. BBO can therefore be used on dis[[continuous function]]s.\n\nBBO optimizes a problem by maintaining a population of candidate solutions, and creating new candidate solutions by combining existing ones according to a simple formula. In this way the [[loss function|objective function]] is treated as a black box that merely provides a measure of quality given a candidate solution, and the function's gradient is not needed.\n\nLike many EAs, BBO was motivated by a natural process; in particular, BBO was motivated by [[biogeography]], which is the study of the distribution of biological species through time and space.<ref name=Quammen1997/> BBO was originally introduced by [http://academic.csuohio.edu/simond/ Dan Simon] in 2008.<ref name=Simon2008/>\n\n== Underlying principles ==\n\nMathematical models of [[biogeography]] describe [[speciation]] (the evolution of new [[species]]), the [[Animal migration|migration]] of species (animals, fish, birds, or insects) between islands, and the [[extinction]] of species.<ref name=MacArthur1967/> Islands that are friendly to life are said to have a high habitat suitability index (HSI).<ref name=Wesche1987/> Features that correlate with HSI include rainfall, vegetative diversity, topographic diversity, land area, temperature, and others. The features that determine are called suitability index variables (SIVs). In terms of habitability, SIVs are the independent variables and HSI is the dependent variable.\n\nIslands with a high HSI can support many species, and islands with a low HSI can support only a few species. Islands with a high HSI have many species that [[emigration|emigrate]] to nearby habitats because of the large populations and the large numbers of species that they host. Note that emigration from an island with a high HSI does not occur because species ''want'' to leave their home; after all, their home island is an attractive place to live. Emigration occurs because of the accumulation of random effects on a large number of species with large populations. Emigration occurs as animals ride [[flotsam]], swim, fly, or ride the wind to neighboring islands. When a species emigrates from an island, it does not mean that the species completely disappears from its original island; only a few representatives emigrate, so an emigrating species remains present on its original island while at the same time migrating to a neighboring island. However, in BBO it is assumed that emigration from an island results in extinction from that island. This assumption is necessary in BBO because species represent the independent variables of a function, and each island represents a candidate solution to a function optimization problem.\n\nIslands with a high HSI not only have a high emigration rate, but they also have a low immigration rate because they already support many species. Species that migrate to such islands will tend to die in spite of the island's high HSI, because there is too much competition for resources from other species.\n\nIslands with a low HSI have a high immigration rate because of their low populations. Again, this is not because species ''want'' to immigrate to such islands; after all, these islands are undesirable places to live. The reason that immigration occurs to these islands is because there is a lot of room for additional species. Whether or not the immigrating species can survive in its new home, and for how long, is another question. However, [[species diversity]] is correlated with HSI, so when more species arrive at a low HSI island, the island's HSI will tend to increase.<ref name=Wesche1987/>\n\nThe figure on the right illustrates an island migration model.<ref name=MacArthur1967/> The immigration rate <math>\\lambda</math> and the emigration rate <math>\\mu</math> are functions of the number of species on the island. The maximum possible immigration rate <math>I</math> occurs when there are zero species on the island. As the number of species increases, the island becomes more crowded, fewer species are able to survive immigration, and the immigration rate decreases. The largest possible number of species that the habitat can support is <math>S_{\\max}</math>, at which point the immigration rate is zero. If there are no species on the island, then the emigration rate is zero. As the number of species on the island increases, it becomes more crowded, more species representatives are able to leave the island, and the emigration rate increases. When the island contains the largest number of possible species <math>S_{\\max}</math>, the emigration rate reaches its maximum possible value <math>E</math>.\n\n[[File:Species Migration Model.png|thumb|Model of immigration <math>\\lambda</math> and emigration <math>\\mu</math> probabilities. <math>S_0</math> is the equilibrium species count, and <math>S_{\\max}</math> is the maximum number of species that the island can support. <math>I</math> and <math>E</math> are the maximum immigration and emigration rates, respectively.]]\n\nIn BBO, <math>\\lambda_k</math> is the probability that a given independent variable in the <math>k</math>-th candidate solution will be replaced; that is, <math>\\lambda_k</math> is the immigration probability of <math>x_k</math>. If an independent variable is to be replaced, then the emigrating candidate solution is chosen with a probability that is proportional to the emigration probability <math>\\mu_k</math>. This is usually performed using [[fitness proportionate selection|roulette wheel selection]].\n\n:: <math>\n\\text{Prob}(x_j)\\text{ is selected for emigration} = \\frac{\\mu_j}{\\sum_{i=1}^N \\mu_i}\n</math>\nfor <math>j=1,\\cdots,N</math>, where <math>N</math> is the number of candidate solutions in the population.\n\n== Algorithm ==\n\nLike most other EAs, BBO includes [[mutation (genetic algorithm)|mutation]]. A basic BBO algorithm with a population size of <math> N </math> for optimizing an <math>n</math>-dimensional function can be described as follows.\n\n  Initialize a population of <math>N</math> candidate solutions <math>\\{ x_k \\}</math> \n  While not(termination criterion)\n     For each <math>x_k</math>, set emigration probability <math>\\mu_k \\propto</math> fitness of <math>x_k</math>,\n        with <math>\\mu_k \\in [0,1]</math>\n     For each <math>x_k</math>, set immigration probability <math>\\lambda_k = 1 - \\mu_k</math>\n     <math> \\{ z_k \\} \\leftarrow \\{ x_k \\} </math>\n     For each individual <math> z_k (k=1,\\cdots,N) </math> \n        For each independent variable index <math>s \\in [1,n] </math>\n           Use <math>\\lambda_k</math> to probabilistically decide whether to immigrate to <math>z_k</math>\n           If immigrating then\n              Use <math> \\{ \\mu_i \\} </math> to probabilistically select the emigrating individual <math> x_j </math>\n              <math> z_k(s) \\leftarrow x_j(s) </math>\n           End if\n        Next independent variable index: <math> s \\leftarrow s+1 </math>\n        Probabilistically mutate <math> z_k </math>\n     Next individual: <math> k \\leftarrow k+1 </math>\n     <math> \\{ x_k \\} \\leftarrow \\{ z_k \\} </math>\n  Next generation\n\n== Discussion of the BBO algorithm ==\n\n* The population size <math>N</math> is a tuning parameter. If <math>N</math> is too small or too large, then the optimization performance of BBO will suffer. Typical implementations of BBO use a value of <math>N</math> somewhere between 20 and 200.\n* The initial population of candidate solutions <math>\\{ x_k \\}_{k=1}^N</math> is usually generated randomly. However, it could be generated in a problem-dependent way based on some reasonable guesses or previously-known good solutions to the optimization problem.\n* The termination criterion is problem-dependent, like in any other EA. In most applications the termination criterion is a generation count limit or a function evaluation limit (that is, how often the objective function is evaluated).\n* <math> \\{ z_k \\} </math> is a temporary population so that all emigrating variables can originate from the population that is in place at the beginning of the generation, which is <math> \\{ x_k \\} </math>.\n\n== Algorithmic variations ==\n\nMany variations have been proposed to the basic BBO algorithm, among which are the following.\n* Elitism is implemented in most EAs to make sure that the best candidate solution is not lost from one generation to the next. This can be implemented in a variety of ways, but one common way is to save the best candidate solutions at the beginning of each generation in a set <math>\\mathbb E</math>; then replace the worst candidate solutions with <math>\\mathbb E</math> at the end of the generation, after migration and mutation have completed. The size of <math>\\mathbb E</math> is a tuning parameter, but <math>\\mathbb E</math> typically includes the best two individuals. Elitism was originally proposed for [[genetic algorithm]]s by DeJong.<ref name=DeJong1975/> Elitism can make a significant difference in the performance of BBO, and is highly recommended.\n* Duplicate replacement is often implemented in BBO. This is a procedure at the end of each generation that replaces duplicate individuals in the population. Scanning for duplicates can be computationally intensive because it is an <math>O(N^2)</math> operation, so it is often performed only every few generations, rather than every generation.\n* Blending can be implemented in BBO. With blending, instead of replacing <math>z_k(s)</math> in an immigrating candidate solution with <math>x_j(s)</math> from the emigrating candidate solution, <math>z_k(s)</math> is set equal to a linear combination of its original value and <math>x_j(s)</math>:\n:: <math> z_k(s) \\leftarrow \\alpha z_k(s) + (1 - \\alpha) x_j(s) </math>\n: where <math> \\alpha \\in [0, 1] </math>, and <math> \\alpha = 0 </math> corresponds to standard migration as shown in the algorithm above. Blended BBO is based on blended crossover in genetic algorithms,<ref name=Muhlenbein1993/> and has been shown to outperform standard BBO.<ref name=Ma2011b/>\n* The BBO algorithm presented above is called partial immigration-based BBO because the immigrating candidate solution is selected before the emigrating candidate solution is selected, and migration for each independent variable in the immigrating candidate solution is performed independently of all other independent variables. Other approaches for selecting the immigrating and emigrating candidate solutions have also been proposed.<ref name=Simon2013/><ref name=Kundra2010/>\n* The migration curves in the above figure are linear, but nonlinear migration curves often give better performance.<ref name=Ma2010/>\n\n== Hybridization ==\n\n* BBO has been hybridized with several other EAs, including [[particle swarm optimization]],<ref name=Kundra2010/><ref>{{cite journal|last1=Zhang|first1=Y.|title=Pathological Brain Detection in Magnetic Resonance Imaging Scanning by Wavelet Entropy and Hybridization of Biogeography-based Optimization and Particle Swarm Optimization|journal=Progress in Electromagnetics Research – Pier|date=2015|volume=152|pages=41–58|url=http://www.jpier.org/PIER/pier152/04.15040602.pdf|doi=10.2528/pier15040602}}</ref> [[differential evolution]],<ref name=Bhattacharya2010/> [[evolution strategy]],<ref name=Du2009/> [http://tizhoosh.uwaterloo.ca/Research/opposition_based_learning.htm opposition-based computing],<ref name=Ergezer2009/> [[case-based reasoning]],<ref name=Panchal2009/> [[artificial bee colony algorithm]],<ref name=Arora2012/> bacterial foraging optimization,<ref name=Lohokare2009/> [[harmony search]],<ref name=Wang2013/> and the [[simplex algorithm]].<ref name=Wang2011/>\n* BBO can be combined with local search to create a [[memetic algorithm]] that performs much better than BBO alone.<ref name=Simon2013b/>\n\n== Software ==\n\n=== MATLAB ===\n* The following MATLAB code gives a BBO implementation for minimizing the 20-dimensional [[Rosenbrock function]]. Note that the following code is very basic, although it does include elitism. A serious BBO implementation should include some of the variations discussed above, such as duplicate replacement, blending, nonlinear migration, and local optimization.\n\n<syntaxhighlight lang=\"matlab\">\nfunction BBO\n% Biogeography-based optimization (BBO) to minimize a continuous function\n% This program was tested with MATLAB R2012b\n\nGenerationLimit = 50; % generation count limit \nPopulationSize = 50; % population size\nProblemDimension = 20; % number of variables in each solution (i.e., problem dimension)\nMutationProbability = 0.04; % mutation probability per solution per independent variable\nNumberOfElites = 2; % how many of the best solutions to keep from one generation to the next\nMinDomain = -2.048; % lower bound of each element of the function domain\nMaxDomain = +2.048; % upper bound of each element of the function domain\n\n% Initialize the population\nrng(round(sum(100*clock))); % initialize the random number generator\nx = zeros(PopulationSize, ProblemDimension); % allocate memory for the population\nfor index = 1 : PopulationSize % randomly initialize the population\n    x(index, :) = MinDomain + (MaxDomain - MinDomain) * rand(1, ProblemDimension);\nend\nCost = RosenbrockCost(x); % compute the cost of each individual  \n[x, Cost] = PopulationSort(x, Cost); % sort the population from best to worst\nMinimumCost = zeros(GenerationLimit, 1); % allocate memory\nMinimumCost(1) = Cost(1); % save the best cost at each generation in the MinimumCost array\ndisp(['Generation 0 min cost = ', num2str(MinimumCost(1))]);\nz = zeros(PopulationSize, ProblemDimension); % allocate memory for the temporary population\n\n% Compute migration rates, assuming the population is sorted from most fit to least fit\nmu = (PopulationSize + 1 - (1:PopulationSize)) / (PopulationSize + 1); % emigration rate\nlambda = 1 - mu; % immigration rate\n\nfor Generation = 1 : GenerationLimit\n    % Save the best solutions and costs in the elite arrays\n    EliteSolutions = x(1 : NumberOfElites, :);\n    EliteCosts = Cost(1 : NumberOfElites);\n\n    % Use migration rates to decide how much information to share between solutions\n    for k = 1 : PopulationSize\n        % Probabilistic migration to the k-th solution\n        for j = 1 : ProblemDimension\n\n            if rand < lambda(k) % Should we immigrate?\n                % Yes - Pick a solution from which to emigrate (roulette wheel selection)\n                RandomNum = rand * sum(mu);\n                Select = mu(1);\n                SelectIndex = 1;\n                while (RandomNum > Select) && (SelectIndex < PopulationSize)\n                    SelectIndex = SelectIndex + 1;\n                    Select = Select + mu(SelectIndex);\n                end\n                z(k, j) = x(SelectIndex, j); % this is the migration step\n            else\n                z(k, j) = x(k, j); % no migration for this independent variable\n            end\n\n        end\n    end\n\n    % Mutation\n    for k = 1 : PopulationSize\n        for ParameterIndex = 1 : ProblemDimension\n            if rand < MutationProbability\n                z(k, ParameterIndex) = MinDomain + (MaxDomain - MinDomain) * rand;\n            end\n        end\n    end\n\n    x = z; % replace the solutions with their new migrated and mutated versions\n    Cost = RosenbrockCost(x); % calculate cost\n    [x, Cost] = PopulationSort(x, Cost); % sort the population and costs from best to worst\n\n    for k = 1 : NumberOfElites % replace the worst individuals with the previous generation's elites\n        x(PopulationSize-k+1, :) = EliteSolutions(k, :);\n        Cost(PopulationSize-k+1) = EliteCosts(k);\n    end\n\n    [x, Cost] = PopulationSort(x, Cost); % sort the population and costs from best to worst\n    MinimumCost(Generation+1) = Cost(1);\n    disp(['Generation ', num2str(Generation), ' min cost = ', num2str(MinimumCost(Generation+1))])\nend\n\n% Wrap it up by displaying the best solution and by plotting the results\ndisp(['Best solution found = ', num2str(x(1, :))])\nclose all\nplot(0:GenerationLimit, MinimumCost);\nxlabel('Generation')\nylabel('Minimum Cost')\nreturn\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nfunction [x, Cost] = PopulationSort(x, Cost)\n% Sort the population and costs from best to worst\n[Cost, indices] = sort(Cost, 'ascend');\nx = x(indices, :);\nreturn\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nfunction [Cost] = RosenbrockCost(x)\n% Compute the Rosenbrock function value of each element in x\nNumberOfDimensions = size(x, 2);\nCost = zeros(size(x, 1), 1); % allocate memory for the Cost array\nfor PopulationIndex = 1 : length(x)\n    Cost(PopulationIndex) = 0;\n    for i = 1 : NumberOfDimensions-1\n        Temp1 = x(PopulationIndex, i);\n        Temp2 = x(PopulationIndex, i+1);\n        Cost(PopulationIndex) = Cost(PopulationIndex) + 100 * (Temp2 - Temp1^2)^2 + (Temp1 - 1)^2;\n    end\nend\nreturn\n</syntaxhighlight>\n\n=== R ===\n* \"bbo: Biogeography-Based Optimization\" is an [[R (programming language)|R]] package for continuous BBO.<ref name=rPackage/>\n\n== Extensions ==\n\nBBO has been extended to noisy functions (that is, functions whose fitness evaluation is corrupted by noise);<ref name=Ma2013/> constrained functions;<ref name=Roy2010a/> combinatorial functions;<ref name=Song2010/> and multi-objective functions.<ref name=Roy2010b/><ref name=DiBarba2016/>\nMoreover, a micro biogeography-inspired multi-objective optimization algorithm (μBiMO) was implemented: it is suitable for solving multi-objective optimisations in the field of industrial design because it is based on a small number of islands (hence the name μBiMO), i.e. few objective function calls are required.<ref>{{cite journal|last1=Mognaschi|first1=M.E.|title=Micro biogeography-inspired multi-objective optimisation for industrial electromagnetic design|journal=Electronics Letters|date=2017|volume=53|issue=22|pages=1458–1460|doi=10.1049/el.2017.3072}}</ref>\n\n== Mathematical analyses ==\n\nBBO has been mathematically analyzed using Markov models<ref name=Simon2011b/> and dynamic system models.<ref name=Simon2011c/>\n\n== Applications ==\nScholars have applied BBO into various academic and industrial applications. They found BBO performed better than state-of-the-art global optimization methods. \n\nFor example, Wang et al. proved BBO performed equal performance with FSCABC but with simpler codes.<ref>{{cite journal|last1=Wang|first1=S.|title=Fruit Classification by Wavelet-Entropy and Feedforward Neural Network trained by Fitness-scaled Chaotic ABC and Biogeography-based Optimization|journal=Entropy|date=2015|volume=17|issue=8|pages=5711–5728|doi=10.3390/e17085711}}</ref>\n\nYang et al. showed BBO was superior to GA, PSO, and ABC.<ref>{{cite journal|last1=Yang|first1=G.|last2=Yang|first2=J.|title=Automated classification of brain images using wavelet-energy and biogeography-based optimization|journal=Multimedia Tools and Applications|volume=75|issue=23|pages=15601–15617|date=2015|doi=10.1007/s11042-015-2649-7}}</ref>\n\n==References==\n\n{{Reflist|refs=\n\n<ref name=Arora2012>\n{{cite journal\n|last1=Arora\n|first1=P.\n|last2=Kundra\n|first2=H.\n|last3=Panchal\n|first3=V.\n|title=Fusion of biogeography based optimization and artificial bee colony for identification of natural terrain features\n|journal=International Journal of Advanced Computer Science and Applications\n|year=2012 \n|volume=3\n|issue=10\n|pages=107&ndash;111\n|doi=10.14569/ijacsa.2012.031018\n|citeseerx=10.1.1.251.2681\n}}\n</ref>\n\n<ref name=Bhattacharya2010>\n{{cite journal\n|last1=Bhattacharya \n|first1=A.\n|last2=Chattopadhyay\n|first2=P.\n|title=Hybrid differential evolution with biogeography-based optimization for solution of economic load dispatch\n|journal=IEEE Transactions on Power Systems\n|year=2010\n|volume=25\n|issue=4\n|pages=1955&ndash;1964\n|doi=10.1109/tpwrs.2010.2043270\n}}\n</ref>\n\n<ref name=DeJong1975>\n{{cite thesis |type=Ph.D. |first=K. |last=De Jong|title=An Analysis of the Behaviour of a Class of Genetic Adaptive Systems |publisher=University of Michigan |year=1975}}\n</ref>\n\n<ref name=Du2009>\n{{cite conference\n|last1=Du\n|first1=D.\n|last2=Simon\n|first2=D.\n|last3=Ergezer\n|first3=M.\n|title=Biogeography-based optimization combined with evolutionary strategy and immigration refusal\n|booktitle=IEEE Conference on Systems, Man, and Cybernetics\n|place=San Antonio, Texas\n|year=2009\n|pages=1023&ndash;1028\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/mbbo.pdf\n}}\n</ref>\n\n<ref name=Ergezer2009>\n{{cite conference\n|last1=Ergezer\n|first1=M.\n|last2=Simon\n|first2=D.\n|last3=Du\n|first3=D.\n|title=Oppositional biogeography-based optimization\n|booktitle=IEEE Conference on Systems, Man, and Cybernetics\n|place=San Antonio, Texas\n|year=2009\n|pages=1035&ndash;1040\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/OBBO_SMC.pdf\n}}\n</ref>\n\n<ref name=Kundra2010>\n{{cite journal\n|last1=Kundra\n|first1=H.\n|last2=Sood\n|first2=M.\n|title=Cross-Country Path Finding using Hybrid approach of PSO and BBO\n|journal=International Journal of Computer Applications\n|year=2010\n|volume=7\n|issue=6\n|pages=15&ndash;19\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/Kundra2010.pdf\n|doi=10.5120/1167-1370\n}}\n</ref>\n\n<ref name=Lohokare2009>\n{{cite conference\n|last1=Lohokare\n|first1=M.\n|last2=Pattnaik\n|first2=S.\n|last3=Devi\n|first3=S.\n|last4=Panigrahi\n|first4=B.\n|last5=Das\n|first5=S.\n|last6=Bakwad\n|first6=K.\n|title=Intelligent biogeography-based optimization for discrete variables\n|booktitle=World Congress on Nature and Biologically Inspired Computing\n|place=Coimbatore, India\n|year=2009\n|pages=1088&ndash;1093\n|url=https://ieeexplore.ieee.org/abstract/document/5393808/\n}}\n</ref>\n\n<ref name=Ma2010>\n{{cite journal\n|last=Ma\n|first=H.\n|title=An analysis of the equilibrium of migration models for biogeography-based optimization\n|journal=Information Sciences\n|year=2010\n|volume=180\n|issue=18\n|pages=3444&ndash;3464\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/HaipingMa2.pdf\n|doi=10.1016/j.ins.2010.05.035\n}}\n</ref>\n\n<ref name=Ma2011b>\n{{cite journal\n|last1=Ma\n|first1=H.\n|last2=Simon\n|first2=D.\n|title=Blended biogeography-based optimization for constrained optimization\n|journal=Engineering Applications of Artificial Intelligence\n|year=2011\n|volume=24\n|issue=3\n|pages=517&ndash;525\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/MaSimonEAAI2011.pdf\n|doi=10.1016/j.engappai.2010.08.005\n}}\n</ref>\n\n<ref name=Ma2013>\n{{cite web\n|title=Biogeography-Based Optimization for Noisy Fitness Functions\n|url=http://academic.csuohio.edu/simond/bbo/noisy/\n|last1=Ma\n|first1=H.\n|last2=Fei\n|first2=M.\n|last3=Simon\n|first3=D.\n|last4=Yu\n|first4=M.\n|accessdate = 7 September 2013\n|archivedate = 24 October 2012\n}}\n</ref>\n\n<ref name=MacArthur1967>\n{{cite book\n|title=The Theory of Island Biogeography\n|last1=MacArthur \n|first1=R.\n|last2=Wilson\n|first2=E.\n|year=1967\n|publisher=Princeton University Press\n}}\n</ref>\n\n<ref name=Muhlenbein1993>\n{{cite journal\n|last1=Muhlenbein\n|first1=H.\n|last2=Schlierkamp-Voosen\n|first2=D.\n|title=Predictive models for the breeder genetic algorithm: I. Continuous parameter optimization\n|journal=Evolutionary Computation\n|year=1993\n|volume=1\n|issue=1\n|pages=25&ndash;49\n|doi=10.1162/evco.1993.1.1.25\n}}\n</ref>\n\n<ref name=Panchal2009>\n{{cite journal\n|last1=Kundra\n|first1=H.\n|last2=Kaur\n|first2=A.\n|last3=Panchal\n|first3=V.\n|title=An integrated approach to biogeography based optimization with case-based reasoning for exploring groundwater possibility\n|journal=The Delving: Journal of Technology and Engineering Sciences\n|year=2009 \n|volume=1\n|issue=1\n|pages=32&ndash;38\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/Kundra.pdf\n}}\n</ref>\n\n<ref name=Quammen1997>\n{{cite book\n|title=The Song of the Dodo: Island Biogeography in an Age of Extinction\n|last1=Quammen\n|first1=D.\n|year=1997\n|publisher=Scribner\n}}\n</ref>\n\n<ref name=Roy2010a>\n{{cite journal\n|last1=Roy\n|first1=P.\n|last2=Ghoshal\n|first2=S.\n|last3=Thakur\n|first3=S.\n|title=Biogeography based optimization for multi-constraint optimal power flow with emission and non-smooth cost function\n|journal=Expert Systems with Applications\n|year=2010\n|volume=37\n|issue=12\n|pages=8221&ndash;8228\n|doi=10.1016/j.eswa.2010.05.064\n}}\n</ref>\n\n<ref name=Roy2010b>\n{{cite journal\n|last1=Roy\n|first1=P.\n|last2=Ghoshal\n|first2=S.\n|last3=Thakur\n|first3=S.\n|title=Multi-objective optimal power flow using biogeography-based optimization\n|journal=Electric Power Components and Systems\n|year=2010\n|volume=38\n|issue=12\n|pages=1406&ndash;1426\n|doi=10.1080/15325001003735176\n}}\n</ref>\n\n<ref name=DiBarba2016>\n{{cite journal\n|last1=Di Barba\n|first1=P.\n|last2=Dughiero\n|first2=F.\n|last3=Mognaschi\n|first3=M.E.\n|last4=Savini\n|first4=A.\n|last5=Wiak\n|first5=S.\n|title=Biogeography-Inspired Multiobjective Optimization and MEMS Design\n|journal=IEEE Transactions on Magnetics\n|year=2016\n|volume=52\n|issue=3\n|pages=1&ndash;4\n|doi=10.1109/TMAG.2015.2488982\n}}\n</ref>\n\n<ref name=Simon2008>\n{{cite journal\n|last=Simon\n|first=D.\n|title=Biogeography-based optimization\n|journal=IEEE Transactions on Evolutionary Computation\n|year=2008\n|volume=12\n|issue=6\n|pages=702&ndash;713\n|url=http://academic.csuohio.edu/simond/bbo/BBO_Simon.pdf\n|doi=10.1109/tevc.2008.919004}}\n</ref>\n\n<ref name=Simon2011b>\n{{cite journal\n|last1=Simon\n|first1=D.\n|last2=Ergezer\n|first2=M.\n|last3=Du\n|first3=D.\n|last4=Rarick\n|first4=R.\n|title=Markov models for biogeography-based optimization\n|journal=IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics\n|year=2011 \n|volume=41\n|issue=1\n|pages=299&ndash;306\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/Markov9.pdf\n|doi=10.1109/tsmcb.2010.2051149\n}}\n</ref>\n\n<ref name=Simon2011c>\n{{cite journal\n|last=Simon\n|first=D.\n|title=A dynamic system model of biogeography-based optimization\n|journal=Applied Soft Computing\n|year=2011 \n|volume=1\n|issue=8\n|pages=5652&ndash;5661\n|url=http://embeddedlab.csuohio.edu/BBO/BBO_Papers/BBODyn.pdf\n|doi=10.1016/j.asoc.2011.03.028\n}}\n</ref>\n\n<ref name=Simon2013>\n{{cite book \n|title=Evolutionary Optimization Algorithms\n|url=http://academic.csuohio.edu/simond/EvolutionaryOptimization/\n|last=Simon\n|first=D.\n|year=2013\n|publisher=Wiley\n}}\n</ref>\n\n<ref name=Simon2013b>\n{{cite web\n|title=Linearized Biogeography-Based Optimization with Re-initialization and Local Search\n|url=http://academic.csuohio.edu/simond/bbo/linearized/\n|last1=Simon\n|first1=D.\n|last2=Omran\n|first2=M.\n|last3=Clerc\n|first3=M.\n|accessdate = 6 September 2013\n|archivedate = 13 March 2013\n}}\n</ref>\n\n<ref name=Song2010>\n{{cite conference\n|last1=Song\n|first1=Y.\n|last2=Liu\n|first2=M.\n|last3=Wang\n|first3=Z.\n|title=Biogeography-based optimization for the traveling salesman problems\n|booktitle=International Joint Conference on Computational Science and Optimization\n|place=Huangshan, Anhui, China \n|year=2010\n|pages=295&ndash;299\n}}\n</ref>\n\n<ref name=Wang2011>\n{{cite journal\n|last1=Wang\n|first1=L.\n|last2=Xu\n|first2=Y.\n|title=An effective hybrid biogeography-based optimization algorithm for parameter estimation of chaotic systems\n|journal=Expert Systems with Applications\n|year=2011 \n|volume=38\n|issue=12\n|pages=15103&ndash;15109\n|doi=10.1016/j.eswa.2011.05.011\n}}\n</ref>\n\n<ref name=Wang2013>\n{{cite journal\n|last1=Wang\n|first1=G.\n|last2=Guo\n|first2=L.\n|last3=Duan\n|first3=H.\n|last4=Wang\n|first4=H.\n|last5=Liu\n|first5=L.\n|last6=Shao\n|first6=M.\n|title=Hybridizing harmony search with biogeography based optimization for global numerical optimization \n|journal=Journal of Computational and Theoretical Nanoscience\n|year=2013 \n|volume=10\n|issue=10\n|pages=2312&ndash;2322\n|doi=10.1166/jctn.2013.3207\n}}\n</ref>\n\n<ref name=Wesche1987>\n{{cite journal\n|last1=Wesche \n|first1=T.\n|last2=Goertler \n|first2=G.\n|last3=Hubert\n|first3=W.\n|title=Modified habitat suitability index model for brown trout in southeastern Wyoming\n|journal=North American Journal of Fisheries Management\n|year=1987\n|volume=7\n|issue=2\n|pages=232&ndash;237\n|doi=10.1577/1548-8659(1987)7<232:mhsimf>2.0.co;2\n}}\n</ref>\n\n<ref name=rPackage>{{Cite web | url=https://cran.r-project.org/web/packages/bbo/ | title=Bbo: Biogeography-Based Optimization| date=2014-09-18}}</ref>\n\n}}\n\n==External links==\n* [http://embeddedlab.csuohio.edu/BBO/ BBO Home Page]\n\n{{Major subfields of optimization}}\n\n{{DEFAULTSORT:biogeography-based optimization}}\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Cuckoo search",
      "url": "https://en.wikipedia.org/wiki/Cuckoo_search",
      "text": "{{about|the search algorithm|the hashing algorithm|cuckoo hashing}}\n{{refimprove|date=June 2015}}\nIn [[operations research]], '''cuckoo search''' is an [[Optimization (mathematics)|optimization]] [[algorithm]] developed by [[Xin-she Yang]] and Suash Deb\nin 2009.<ref name=NaBIC>{{cite conference\n| title         =Cuckoo search via Lévy flights\n| author        =X.-S. Yang\n|author2=S. Deb\n|date=December 2009\n| conference    =World Congress on Nature & Biologically Inspired Computing (NaBIC 2009)\n| publisher     =IEEE Publications\n| pages         =210–214\n| arxiv         =1003.1594v1 \n}}<!--| accessdate    =March 10, 2010--></ref><ref>{{cite web|author=Inderscience |url=http://www.alphagalileo.org/ViewItem.aspx?ItemId=76985&CultureCode=en |title=Cuckoo designs spring |publisher=Alphagalileo.org |date=27 May 2010 |accessdate=2010-05-27}}</ref> It was inspired by the [[Obligate parasite|obligate brood parasitism]] of some [[cuckoo]] species by laying their eggs in the nests of other host birds (of other species). Some host birds can engage direct conflict with the intruding cuckoos. For example, if a host bird discovers the eggs are not their own, it will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere. Some cuckoo species such as the [[New World]] brood-parasitic [[Tapera]] have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colors and pattern of the eggs of a few chosen host species <ref>R. B. Payne, M. D. Sorenson, and K. Klitz, The Cuckoos, Oxford University Press, (2005).</ref> Cuckoo search idealized such breeding behavior, and thus can be applied for various optimization problems.\n\n== Metaphor ==\nCuckoo search (CS) uses the following representations:\n\nEach egg in a nest represents a solution, and a cuckoo egg represents a new solution. The aim is to use the new and potentially better solutions (cuckoos) to replace a not-so-good solution in the nests. In the simplest form, each nest has one egg. The algorithm can be extended to more complicated cases in which each nest has multiple eggs representing a set of solutions.\n\nCS is based on three idealized rules: \n# Each cuckoo lays one egg at a time, and dumps its egg in a randomly chosen nest;\n# The best nests with high quality of eggs will carry over to the next generation;\n# The number of available hosts nests is fixed, and the egg laid by a cuckoo is discovered by the host bird with a probability <math>p_a \\in (0,1)</math>. Discovering operate on some set of worst nests, and discovered solutions dumped from farther calculations.\nIn addition, Yang and Deb discovered that the random-walk style search is better performed by [[Lévy flight]]s rather than simple [[random walk]].\n\n== Algorithm ==\nThe [[pseudo-code]] can be summarized as:\n\n Objective function: <math>f(\\mathbf{x}), \\quad \\mathbf{x}=(x_1,x_2,\\dots,x_d); \\, </math>\n Generate an initial population of <math> n </math> host nests; \n While (t<MaxGeneration) or (stop criterion)\n    Get a cuckoo randomly (say, i) and replace its solution by performing Lévy flights;\n    Evaluate its quality/fitness <math>F_i </math>\n          [For maximization, <math>F_i \\propto f(\\mathbf{x}_i)</math> ];\n    Choose a nest among n (say, j) randomly;\n    if (<math>F_i>F_j </math>),\n           Replace j by the new solution;\n    end if\n    A fraction (<math>p_a</math>) of the worse nests are abandoned and new ones are built;\n    Keep the best solutions/nests;\n    Rank the solutions/nests and find the current best;\n    Pass the current best solutions to the next generation;\n end while\n\nAn important advantage of this algorithm is its simplicity. In fact, comparing with other population- or agent-based [[metaheuristic]] algorithms such as [[particle swarm optimization]] and [[harmony search]], there is essentially only a single parameter <math>p_a</math> in CS (apart from the population size <math>n</math>). Therefore, it is very easy to implement.\n\n==Random walks and the step size ==\n\nAn important issue is the applications of Lévy flights and random walks in the generic equation for generating new solutions\n\n: <math> \\mathbf{x}_{t+1}=\\mathbf{x}_t + s E_t, </math>\nwhere <math> E_t </math> is drawn from a standard normal distribution with zero mean and unity standard deviation for random walks, or drawn from Lévy distribution for Lévy flights. Obviously, the random walks can also be linked with the similarity between a cuckoo's egg and the host's egg which can be tricky in implementation. Here the step size <math>s</math> determines how far a random walker can go for a fixed number of iterations. The generation of Lévy step size is often tricky, and a comparison of three algorithms (including Mantegna's<ref>R. N. Mantegna, Fast, accurate algorithm for numerical simulation of Lévy stable stochastic processes, Physical Review E, Vol.49, 4677-4683 (1994).</ref>) was performed by Leccardi<ref>M. Leccardi, Comparison of three algorithms for Levy noise generation, Proceedings of fifth EUROMECH nonlinear dynamics conference (2005).</ref> who found an implementation of Chambers et al.'s approach<ref>{{cite journal | last1 = Chambers | first1 = J. M. | last2 = Mallows | first2 = C. L. | last3 = Stuck | first3 = B. W. | year = 1976 | title = A method for simulating stable random variables | url = | journal = Journal of the American Statistical Association | volume = 71 | issue = | pages = 340–344 | doi=10.1080/01621459.1976.10480344}}</ref> to be the most computationally efficient due to the low number of random numbers required.\n\nIf s is too large, then the new solution generated will be too far away from the old solution (or even jump outside of the bounds). Then, such a move is unlikely to be accepted. If s is too small, the change is too small to be significant, and consequently such search is not efficient. So a proper step size is important to maintain the search as efficient as possible.\n\nAs an example, for simple isotropic random walks, we know that the average distance <math> r </math> traveled in the d-dimension space is\n: <math> r^2=2 d D t, </math>\nwhere <math>D=s^2/2\\tau</math> is the effective diffusion coefficient. Here <math> s</math> is the step size or distance traveled at each jump, and <math> \\tau </math> is the time taken for each jump. The above equation implies that<ref>X.-S. Yang, Nature-Inspired Metaheuristic Algorithms, 2nd Edition, Luniver Press, (2010).</ref>\n: <math> s^2=\\frac{\\tau \\; r^2}{t \\; d}. </math>\nFor a typical length scale L of a dimension of interest,  the local search is typically limited in a region of <math>r=L/10 </math>. For <math>\\tau=1 </math> and t=100 to 1000, we have <math>s\\approx 0.01L </math> for d=1, and <math>s \\approx 0.001L </math> for d=10.  Therefore, we can use s/L=0.001 to 0.01 for most problems. Though the exact derivation may require detailed analysis of the behaviour of Lévy flights.<ref>M. Gutowski,  Lévy flights as an underlying mechanism for global optimization algorithms,  ArXiv Mathematical Physics e-Prints, June, (2001).</ref>\n\nAlgorithm and convergence analysis will be fruitful, because there are many open problems related to metaheuristics<ref>X. S. Yang, Metaheuristic optimization: algorithm analysis and open problems, in: Experimental Algorithms (SEA2011), Eds (P. M. Pardalos and S. Rebennack), LNCS 6630, pp.21-32 (2011).</ref>\n\n==Improved Cuckoo Search Algorithms ==\n\nConvergence of Cuckoo Search algorithm can be substantially improved by genetically replacing abandoned nests (instead of using the random replacements from the original method)<ref>{{Cite journal|last=de Oliveira|first=Victoria Y.M.|last2=de Oliveira|first2=Rodrigo M.S.|last3=Affonso|first3=Carolina M.|date=2018-07-31|title=Cuckoo Search approach enhanced with genetic replacement of abandoned nests applied to optimal allocation of distributed generation units|url=http://mr.crossref.org/iPage?doi=10.1049%2Fiet-gtd.2017.1992|journal=IET Generation, Transmission & Distribution|volume=12|issue=13|pages=3353–3362|doi=10.1049/iet-gtd.2017.1992|issn=1751-8687}}</ref>.\n\n== References ==\n{{Reflist}}\n\n{{collective animal behaviour}}\n{{Optimization algorithms}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Dual-phase evolution",
      "url": "https://en.wikipedia.org/wiki/Dual-phase_evolution",
      "text": "{{Multiple issues|\n{{refimprove|date=May 2015}}\n{{technical|date=May 2015}}\n}}\n\n'''Dual phase evolution''' ('''DPE''') is a process that drives [[self-organization]] within [[complex adaptive system]]s.<ref name=\"DPE2\">\n{{cite book\n| author = [[David G. Green|Green, D.G.]]\n| author2 = [[Liu Jing (programmer)|Liu, J.]] \n| author3 = [[Hussein Abbass|Abbass, H.]]\n| lastauthoramp = y\n| year = 2014\n| title = Dual Phase Evolution: from Theory to Practice\n| publisher = Springer\n| location = Berlin\n| isbn = 978-1441984227\n }}</ref> It arises in response to phase changes within the network of connections formed by a system's components. DPE occurs in a wide range of physical, biological and social systems. Its applications to technology include methods for manufacturing novel materials and algorithms to solve complex problems in computation.\n\n== Introduction ==\n\nDual phase evolution (DPE) is a process that promotes the emergence of large-scale order in [[complex systems]]. It occurs when a system repeatedly switches between various kinds of phases, and in each phase different processes act on the components or connections in the system. DPE arises because of a property of [[Graph theory|graphs]] and [[Network theory|networks]]: the connectivity avalanche that occurs in graphs as the number of edges increases.<ref name=Erdos1960 />\n\nSocial networks provide a familiar example. In a [[social network]] the nodes of the network are people and the network connections (edges) are relationships or interactions between people. For any individual, social activity alternates between a ''local phase'', in which they interact only with people they already know, and a ''global phase'' in which they can interact with a wide pool of people not previously known to them. Historically, these phases have been forced on people by constraints of time and space. People spend most of their time in a local phase and interact only with those immediately around them (family, neighbors, colleagues). However, intermittent activities such as parties, holidays, and conferences involve a shift into a global phase where they can interact with different people they do not know. Different processes dominate each phase. Essentially, people make new social links when in the global phase, and refine or break them (by ceasing contact) while in the local phase.\n\n== The DPE mechanism ==\n\nThe following features are necessary for DPE to occur.<ref name=\"DPE2\" />\n\n=== Underlying network ===\n\nDPE occurs where a system has an underlying network. That is, the system's components form a set of nodes and there are connections (edges) that join them. For example, a family tree is a network in which the nodes are people (with names) and the edges are relationships such as \"mother of\" or \"married to\". The nodes in the network can take physical form, such as atoms held together by atomic forces, or they may be dynamic states or conditions, such as positions on a chess board with moves by the players defining the edges.\n\nIn mathematical terms ([[graph theory]]), a graph <math>\\textstyle G = \\langle N,E\\rangle</math> is a set of nodes <math>\\textstyle N</math> and a set of edges <math>\\textstyle E \\subset \\{ (x,y) \\mid x,y \\in N \\}</math>. Each edge <math>\\textstyle (x,y )</math> provides a link between a pair of nodes <math>\\textstyle x</math> and <math>\\textstyle y</math>. A network is a graph in which values are assigned to the nodes and/or edges.\n\n=== Phase shifts ===\n\nGraphs and networks have two phases: disconnected (fragmented) and connected. In the connected phase every node is connected by an edge to at least one other node and for any pair of nodes, there is at least one path (sequence of edges) joining them.\n\nThe [[Erdős–Rényi model]] shows that random graphs undergo a connectivity avalanche as the density of edges in a graph increases.<ref name=\"Erdos1960\">\n{{cite journal\n | author = [[Paul Erdős|Erdős, P.]]\n | author2 = [[Alfréd Rényi|Rényi, A.]]\n | lastauthoramp = y\n | year = 1960\n | title = On the evolution of random graphs\n | journal = Publications of the Mathematical Institute of the Hungarian Academy of Sciences\n | volume =5\n | pages = 17&ndash;61\n | url = http://www.renyi.hu/~p_erdos/1960-10.pdf\n | accessdate =\n  }}</ref> This avalanche amounts to a sudden phase change in the size of the largest connected subgraph. In effect, a graph has two phases: connected (most nodes are linked by pathways of interaction) and fragmented (nodes are either isolated or form small subgraphs). These are often referred to as '''global''' and '''local''' phases, respectively.\n[[File:Figure1a.gif|thumb|Fragmented graph.]]\n[[File:Figure1b.gif|thumb|Connected graph.]]\n\nAn essential feature of DPE is that the system undergoes repeated shifts between the two phases. In many cases, one phase is the system’s normal state and it remains in that phase until shocked into the alternate phase by a disturbance, which may be external in origin.\n\n=== Selection and variation ===\n\nIn each of the two phases, the network is dominated by different processes.<ref name=\"DPE2\" /> In a local phase, the nodes behave as individuals; in the global phase, nodes are affected by interactions with other nodes. Most commonly the two processes at work can be interpreted as ''variation'' and ''selection''. ''Variation'' refers to new features, which typically appear in one of the two phases. These features may be new nodes, new edges, or new properties of the nodes or edges. ''Selection'' here refers to ways in which the features are modified, refined, selected or removed. A simple example would be new edges being added at random in the global phase and edges being selectively removed in the local phase.\n\n=== System memory ===\n\nThe effects of changes in one phase carry over into the other phase. This means that the processes acting in each phase can modify or refine patterns formed in the other phase. For instance, in a social network, if a person makes new acquaintances during a global phase, then some of these new social connections might survive into the local phase to become long-term friends. In this way, DPE can create effects that may be impossible if both processes act at the same time.\n\n== Examples ==\n\nDPE has been found to occur in many natural and artificial systems.<ref name=\"Paperin2011\">\n{{cite journal\n | author = [[Greg Paperin|Paperin, G.]]\n | author2 = [[David Green (computer scientist)|Green, D.G.]]\n | author3 = [[Suzanne Sadedin|Sadedin, S.]] \n | lastauthoramp = y\n | year = 2011\n | title = Dual Phase Evolution in Complex Adaptive Systems\n | journal = Journal of the Royal Society Interface\n | volume = 8\n | issue = 58\n | pages = 609&ndash;629\n | url = http://www.renyi.hu/~p_erdos/1960-10.pdf\n | doi = 10.1098/rsif.2010.0719\n  | pmc = 3061102\n }}</ref>\n\n=== Social networks ===\n\nDPE is capable of producing social networks with known topologies, notably [[small-world network]]s and [[scale-free network]]s.<ref name=\"Paperin2011\" />\nSmall world networks, which are common in traditional societies, are a natural consequence of alternating ''local'' and ''global'' phases: new, long-distance links are formed during the global phase and existing links are reinforced (or removed) during the local phase. The advent of social media has decreased the constraining influence that space used to impose on social communication, so time has become the chief constraint for many people.\n\nThe alternation between local and global phases in social networks occurs in many different guises. Some transitions between phases occur regularly, such as the daily cycle of people moving between home and work. This alternation can influence shifts in public opinion.<ref name=Stocker2003>\n{{cite journal\n | author = [[Robert Stocker|Stocker, R.]]\n | author2 = [[David Cornforth|Cornforth, D.]]\n | author3 = [[David G. Green|Green, D.G.]] \n | lastauthoramp = y\n | year = 2003\n | title = A simulation of the impact of media on social cohesion\n | journal = Advances in Complex Systems\n | volume = 6 |issue=3\n | pages = 349&ndash;359\n | url = http://www.worldscientific.com/doi/abs/10.1142/S0219525903000931\n | doi = 10.1142/S0219525903000931\n  }}</ref> In the absence of social interaction, the uptake of an opinion promoted by media is a [[Markov process]]. The effect of social interaction under DPE is to retard the initial uptake until the number converted reaches a critical point, after which uptake accelerates rapidly.\n\n=== Socio-economics ===\n\nDPE models of socio-economics interpret the economy as networks of economic agents.<ref name=\"Goodman2014\">\n{{cite journal\n | author = [[James R. Goodman|Goodman, J.]]\n | year = 2014\n | title = Evidence for ecological learning and domain specificity in rational asset pricing and market efficiency\n | journal = The Journal of Socio-Economics\n | volume = 48\n | pages = 27&ndash;39\n | url = http://isiarticles.com/bundles/Article/pre/pdf/13009.pdf\n | doi = 10.1016/j.socec.2013.10.002\n  }}</ref> Several studies have examined the way socioeconomics evolve when DPE acts on different parts of the network. One model<ref name=\"Xu2013\">\n{{cite journal\n | author = [[Xu, G.]] \n | author2 = [[Yang, J.]] \n | author3 = [[Li, G.]] \n | lastauthoramp = y\n | year = 2013\n | title = Simulating society transitions: standstill, collapse and growth in an evolving network model\n | journal = PLOS ONE\n | volume = 8 |issue=9\n | pages = e75433\n | url = http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0075433#pone-0075433-g008\n | doi = 10.1371/journal.pone.0075433\n  | pmid = 24086530\n | pmc = 3783390\n | bibcode = 2013PLoSO...875433X\n }}</ref> interpreted society as a network of occupations with inhabitants matched to those occupations. In this model social dynamics become a process of DPE within the network, with regular transitions between a development phase, during which the network settles into an equilibrium state, and a mutating phase, during which the network is transformed in random ways by the creation of new occupations.\n\nAnother model<ref name=\"Cavaliere2012\">\n{{cite journal\n | author = [[Cavaliere, M.]] \n | author2 = [[Sedwards, C.]]\n | author3 = [[Tarnita, C.E.]]\n | author4 = [[Nowak, M.A.]]\n | author5 = [[Csikász-Nagy, A.]] \n | lastauthoramp=y\n | year = 2012\n | title = Prosperity is associated with instability in dynamical networks\n | journal = Journal of Theoretical Biology\n | volume = 299\n | pages = 126&ndash;138\n | url = http://www.sciencedirect.com/science/article/pii/S0022519311004619/pdfft?md5=9da1d021c3426331bdb6b81661847a54&pid=1-s2.0-S0022519311004619-main.pdf\n | doi = 10.1016/j.jtbi.2011.09.005\n | pmid=21983567\n | pmc=3298632\n  | arxiv = 1102.4947\n }}</ref> interpreted growth and decline in socioeconomic activity as a conflict between cooperators and defectors. The cooperators form networks that lead to prosperity. However, the network is unstable and invasions by defectors intermittently fragment the network, reducing prosperity, until invasions of new cooperators rebuild networks again. Thus prosperity is seen as a dual phase process of alternating highly prosperous, connected phases and unprosperous, fragmented phases.\n\n=== Forest ecology ===\n\nIn a [[forest ecology|forest]], the landscape can be regarded as a network of sites where trees might grow.<ref>{{Cite journal|url = |title = Connectivity and complexity in ecological systems|last = Green|first = David G.|date = 1994|journal = Pacific Conservation Biology |volume=1 |issue=3 |pages=194–200|doi = |pmid =}}</ref> Some sites are occupied by living trees; others sites are empty. In the local phase, sites free of trees are few and they are surrounded by forest, so the network of free sites is fragmented. In competition for these free sites, local seed sources have a massive advantage, and seeds from distant trees are virtually excluded.<ref name=\"DPE2\" /> Major fires (or other disturbances) clear away large tracts of land, so the network of free sites becomes connected and the landscape enters a global phase. In the global phase, competition for free sites is reduced, so the main competitive advantage is adaptation to the environment.\n\nMost of the time a forest is in the local phase, as described above. The net effect is that established tree populations largely exclude invading species.<ref>{{Cite journal|url = |title = Fire and stability in the postglacial forests of southwest Nova Scotia|last = Green|first = David G|date = 1982|journal = Journal of Biogeography |volume=9 |issue=1|pages=29–40|doi = 10.2307/2844728|pmid = |jstor = 2844728}}</ref> Even if a few isolated trees do find free ground, their population is prevented from expanding by established populations, even if the invaders are better adapted to the local environment. A fire in such conditions leads to an explosion of the invading population, and possibly to a sudden change in the character of the entire forest.\n\nThis dual phase process in the landscape explains the consist appearance of [[pollen zone]]s in the postglacial forest history of North America, Europe, as well as the suppression of widespread [[taxa]], such as [[beech]] and [[Tsuga|hemlock]], followed by huge population explosions. Similar patterns, pollen zones truncated by fire-induced boundaries, have been recorded in most parts of the world\n\n=== Search algorithms ===\n\nDual phase evolution is a family of [[search algorithm]]s that exploit phase changes in the [[Mathematical optimization#Optimization problems|search space]] to mediate between local and global search. In this way they control the way algorithms explore a search space, so they can be regarded as a family of [[metaheuristic]] methods.\n\nProblems such as [[optimization]] can typically be interpreted as finding the tallest peak (optimum) within a search space of possibilities. The task can be approached in two ways: ''local search'' (e.g. [[hill climbing]]) involves tracing a path from point to point, and always moving \"uphill\". ''Global search'' involves sampling at wide-ranging points in the search space to find high points.\n\nMany search algorithms involve a transition between phases of global search and local search.<ref name=\"Paperin2011\" /> A simple example is the [[Great Deluge algorithm]] in which the searcher can move at random across the landscape, but cannot enter low-lying areas that are flooded. At first the searcher can wander freely, but rising water levels eventually confine the search to a local area. Many other nature-inspired algorithms adopt similar approaches. [[Simulated annealing]] achieves a transition between phases via its cooling schedule. The [[cellular genetic algorithm]] places solutions in a pseudo landscape in which they breed only with local neighbours. Intermittent disasters clear patches, flipping the system into a global phase until gaps are filled again.\n\nSome variations on the [[memetic algorithm]] involve alternating between selection at different levels. These are related to the [[Baldwin effect]], which arises when processes acting on ''[[phenotype]]s'' (e.g. learning) influence selection at the level of ''[[genotype]]s''. In this sense, the Baldwin effect alternates between global search (genotypes) and local search (phenotypes).\n\n== Related processes ==\n\nDual phase evolution is related to the well-known phenomenon of ''[[self-organized criticality]]'' (SOC). Both concern processes in which critical phase changes promote adaptation and organization within a system. However, SOC differs from DPE in several fundamental ways.<ref name=\"DPE2\" /> Under SOC, a system's natural condition is to be in a critical state; in DPE a system's natural condition is a non-critical state. In SOC the size of disturbances follows a power law; in DPE disturbances are not necessarily distributed the same way. In SOC a system is not necessarily subject to other processes; in DPE different processes (e.g. selection and variation) operate in the two phases.\n\n==References==\n{{reflist}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Firefly algorithm",
      "url": "https://en.wikipedia.org/wiki/Firefly_algorithm",
      "text": "In [[mathematical optimization]], the '''firefly algorithm''' is a [[metaheuristic]] proposed by [[Xin-She Yang]] and inspired by the flashing behavior of [[firefly|fireflies]].<ref>{{cite book |first=X. S. |last=Yang |title=Nature-Inspired Metaheuristic Algorithms |publisher=[[Luniver Press]] |year=2008 |isbn=978-1-905986-10-1 }}</ref>\n\n== Algorithm ==\nIn pseudocode the algorithm can be stated as:\n\n '''Begin'''\n    1) Objective function: {{nowrap|<math>f(\\mathbf{x}), \\quad \\mathbf{x}=(x_1,x_2,...,x_d) </math>;}}\n    2) Generate an initial population of fireflies {{nowrap|<math> \\mathbf{x}_i \\quad (i=1,2,\\dots,n)</math>;.}}\n    3) Formulate light intensity {{mvar|I}} so that it is associated with {{nowrap|<math>f(\\mathbf{x})</math>}}\n       (for example, for maximization problems, {{nowrap|<math>I \\propto f(\\mathbf{x})</math> or simply <math>I=f(\\mathbf{x})</math>;)}}\n    4) Define absorption coefficient {{mvar|&gamma;}}\n  \n    '''While''' (t < MaxGeneration)\n       '''for''' i = 1 : n (all n fireflies)\n          '''for''' j = 1 : i (n fireflies)\n             {{nowrap|'''if''' (<math>I_j>I_i </math>),}}\n                Vary attractiveness with distance r via {{nowrap|<math> \\exp(-\\gamma \\; r) </math>;}}\n                move firefly i towards j;                \n                Evaluate new solutions and update light intensity;\n             '''end if''' \n          '''end for''' j\n       '''end for''' i\n       Rank fireflies and find the current best;\n    '''end while'''\n \n    Post-processing the results and visualization;\n \n '''end'''\n\nNote that the number of objective function evaluations per loop is one evaluation per firefly, even though the above pseudocode suggests it is n*n. (Based on Yang's [[MATLAB]] code.) Thus the total number of objective function evaluations is (number of generations) * (number of fireflies).\n\nThe main update formula for any pair of two fireflies <math>\\mathbf{x}_i </math> and <math>\\mathbf{x}_j </math> is\n:: <math>\\mathbf{x}_i^{t+1}=\\mathbf{x}_i^t + \\beta \\exp[-\\gamma r_{ij}^2] (\\mathbf{x}_j^t - \\mathbf{x}_i^t) +\\alpha_t \\boldsymbol{\\epsilon}_t </math>\nwhere <math>\\alpha_t </math> is a parameter controlling the step size, while <math>\\boldsymbol{\\epsilon}_t </math> is a vector drawn from a Gaussian or other\ndistribution.\n\nIt can be shown that the limiting case <math>\\gamma \\rightarrow 0 </math> corresponds to the standard [[Particle Swarm Optimization]] (PSO). In fact, if the inner loop (for j) is removed and the brightness <math>I_j</math> is replaced by the current global best <math>g^*</math>, then FA essentially becomes the standard PSO.\n\n== Criticism ==\n\nNature-inspired [[Metaheuristic|metaheuristics]] in general have attracted [[List of metaphor-inspired metaheuristics#Criticism of the metaphor methodology|criticism in the research community]] for hiding their lack of novelty behind an elaborate metaphor.  The firefly algorithm has been criticized as differing from the well-established [[particle swarm optimization]] only in a negligible way.<ref>{{cite journal|first1=Omid N.|last1=Almasi| first2=Modjtaba|last2= Rouhani|year=2016|title=A new fuzzy membership assignment and model selection approach based on dynamic class centers for fuzzy SVM family using the firefly algorithm|journal=Turkish Journal of Electrical Engineering & Computer Sciences|volume=4| pages=1–19|doi=10.3906/elk-1310-253|quote= Practical application of FA on UCI datasets.}}</ref><ref>{{cite journal|first=Michael A.|last=Lones|year=2014|title=Metaheuristics in Nature-Inspired Algorithms|journal=[[Genetic and Evolutionary Computation Conference|GECCO '14]]|pages=1419–1422|url=http://www.macs.hw.ac.uk/~ml355/common/papers/lones-gecco2014-metaheuristics.pdf|doi=10.1145/2598394.2609841|quote=FA, on the other hand, has little to distinguish it from PSO, with the inverse-square law having a similar effect to crowding and fitness sharing in EAs, and the use of multi-swarms in PSO.|isbn=9781450328814|citeseerx=10.1.1.699.1825}}</ref><ref>{{cite journal|first=Dennis|last=Weyland|year=2015|title=A critical analysis of the harmony search algorithm—How not to solve sudoku|journal=Operations Research Perspectives|volume=2|pages=97–105|doi=10.1016/j.orp.2015.04.001|quote=For example, the differences between the particle swarm optimization metaheuristic and \"novel\" metaheuristics like the firefly algorithm, the fruit fly optimization algorithm, the fish swarm optimization algorithm or the cat swarm optimization algorithm seem negligible.}}</ref>\n\n==See also==\n* [[Swarm intelligence]]\n\n== References ==\n{{Reflist|2}}\n\n\n==External links==\n* [https://www.mathworks.com/matlabcentral/fileexchange/29693-firefly-algorithm] Files of the Matlab programs included in the book: Xin-She Yang, Nature-Inspired Metaheuristic Algorithms, Second Edition, Luniver Press, (2010).\n\n\n{{Optimization algorithms}}\n{{collective animal behaviour}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Fish School Search",
      "url": "https://en.wikipedia.org/wiki/Fish_School_Search",
      "text": "'''Fish School Search''' (FSS), proposed by Bastos Filho and Lima Neto in 2007 is, in its basic version,<ref>C. J. A. B Filho., F. B. de Lima Neto, A. J. C. C.. Lins, A. I. S. Nascimento., and M. P. Lima, \"A novel search algorithm based on fish school behavior,\" Systems, Man and Cybernetics, SMC 2008. IEEE International Conference on, 2008, pp. 2646-2651.</ref> an unimodal optimization algorithm inspired on the collective behavior of fish schools. The mechanisms of feeding and coordinated movement were used as inspiration to create the search operators. The core idea is to make the fishes “swim” toward the positive gradient in order to “eat” and “gain weight”. Collectively, the heavier fishes are more influent in the search process as a whole, what makes the barycenter of the fish school moves toward better places in the search space over the iterations.<ref>de Lima Neto, Fernando Buarque, and Marcelo Gomes Pereira de Lacerda. \"Multimodal Fish School Search Algorithms Based on Local Information for School Splitting.\" 2013 BRICS Congress on Computational Intelligence and 11th Brazilian Congress on Computational Intelligence. IEEE, 2013</ref>\n\nThe FSS uses the following principles:<ref>http://www.fbln.pro.br/fss/</ref> \n# Simple computations in all individuals (i.e. fish)\n# Various means of storing information (i.e. weights of fish and school barycenter)\n# Local computations (i.e. swimming is composed of distinct components)\n# Low communications between neighboring individuals (i.e. fish are to think local but also be socially aware)\n# Minimum centralized control (mainly for self-controlling of the school radius)\n# Some distinct diversity mechanisms (this to avoid undesirable flocking behavior)\n# Scalability (in terms of complexity of the optimization/search tasks)\n# Autonomy (i.e. ability to self-control functioning)\n\n==Algorithm==\n\nFSS is a population based search algorithm inspired in the behavior of swimming fishes that expand and contract while looking for food. Each fish <math>n</math>-dimensional location represents a possible solution for the optimization problem. The algorithm makes use of weights for all the fishes which represents cumulative account on how successful has been the search for each fish in the school. FSS is composed of the feeding and movement operators, the latter being divided into three sub-components, which are:<ref>J. B. Monteiro, I. M. C. Albuquerque, F. B. L. Neto, and F. V. S. Ferreira, “Optimizing multi-plateau functions with FSS-SAR (Stagnation Avoidance Routine),” Submitted to IEEE Symposium Series on Computational Intelligence, 2016.</ref>\n\n=== Individual component of the movement===\nEvery fish in the school performs a local search looking for promising regions in the search space. It is done as represented below:\n\n<math>\nx_{i}(t + 1) = x_{i}(t)+rand(-1,1)step_{ind},\n</math>\n\nwhere <math>x_{i}(t)</math> and <math>x_{i}(t + 1)</math> represent the position of the fish <math>i</math> before and after the individual movement operator, respectively. <math>rand(-1, 1)</math> is a uniformly distributed random number varying from -1 up to 1 and <math>step_{ind}</math> is a parameter that defines the maximum displacement for this movement. The new position <math>x_{i}(t + 1)</math> is only accepted if the fitness of the fish improves with the position change. If it is not the case, the fish remains in the same position and <math>x_{i}(t + 1) = x_{i}(t)</math>.\n\n=== Collective-instinctive component of the movement===\nAn average of the individual movements is calculated based on the following:\n\n<math>\nI=\\frac{\\sum^{N}_{i=1} \\Delta x_{i} \\Delta f_{i}}{\\sum^{N}_{i=1} \\Delta f_{i}}.\n</math>\n\nThe vector <math>I</math> represents the weighted average of the displacements of each fish. It means that the fishes that experienced a higher improvement will attract fishes into its position.\nAfter the vector <math>I</math> computation, every fish will be encouraged to move according to:\n\n<math>\nx_i(t+1)=x_{i}(t)+I.\n</math>\n\n=== Collective-volitive component of the movement===\n\nThis operator is used in order to regulate the exploration/exploitation ability of the school during the search process. First of all, the barycenter <math>B</math> of the school is calculated based on the position <math>x_{i}</math> and the weight <math>W_{i}</math> of each fish:\n\n<math>\nB(t)=\\frac{\\sum^{N}_{i=1} x_{i}(t) W_{i}(t)}{\\sum^{N}_{i=1} W_{i}(t)},\n</math>\n\nand then, if the total school weight <math>\\sum^{N}_{i=1} W_{i}</math> has increased from the last to the current iteration, the fishes are attracted to the barycenter according to equation A. If the total school weight has not improved, the fishes are spread away from the barycenter according to equation B:\n\nEq. A:\n\n<math>\nx_i(t+1)=x_{i}(t)- step_{vol} rand(0,1)\\frac{x_{i}(t) - B(t)}{distance(x_{i}(t),B(t))},\n</math>\n\nEq. B:\n\n<math>\nx_i(t+1)=x_{i}(t)+step_{vol} rand(0,1)\\frac{x_{i}(t) - B(t)}{distance(x_{i}(t),B(t))},\n</math>\n\nwhere <math>step_{vol}</math> defines the size of the maximum displacement performed with the use of this operator. <math>distance(x_{i}(t),B(t))</math> is the euclidean distance between the fish <math>i</math> position and the school barycenter. <math>rand(0, 1)</math>  is a uniformly distributed random number varying from 0 up to 1.\n \nBesides the movement operators, it was also defined a feeding operator used in order to update the weights of every fish according to:\n\n<math>\nW_{i}(t+1)=W_{i}(t)+\\frac{\\Delta f_i}{max(| \\Delta f_i |)},\n</math>\n\nwhere <math>W_{i}(t)</math> is the weight parameter for fish <math>i</math>, <math>\\Delta f_i</math> is the fitness variation between the last and the new position, and <math>max(| \\Delta f_i |)</math> represents the maximum absolute value of the fitness variation among all the fishes in the school.\n<math>W</math> is only allowed to vary from 1 up to <math>W_{scale}/2</math>, which is a user defined attribute. The weights of all fishes are initialized with the value <math>W_{scale}/2</math>.\n\n===The pseudo-code for FSS===\n\n# Initialize user parameters\n# Initialize fishes positions randomly\n# while Stopping condition is not met do\n# Calculate fitness for each fish\n# Run individual operator movement\n# Calculate fitness for each fish\n# Run feeding operator\n# Run collective-instinctive movement operator\n# Run collective-volitive movement operator\n# end while\n\nThe parameters <math>step_{ind}</math> and <math>step_{vol}</math> decay linearly according to:\n\n<math>\nstep_{ind}(t+1)=step_{ind}(t)-\\frac{step_{ind}(initial)}{It_{max}},\n</math>\n\nand similarly:\n\n<math>\nstep_{vol}(t+1)=step_{vol}(t)-\\frac{step_{vol}(initial)}{It_{max}},\n</math>\n\nwhere <math>step_{ind}(initial)</math> and <math>step_{vol}(initial)</math> are user defined initial values for <math>step_{ind}</math> and <math>step_{vol}</math>, respectively. <math>It_{max}</math> is the maximum number of iterations allowed in the search process.\n\n==Variations of FSS==\n\n===dFSS(Density based Fish School Search)===\n\nThis version excels for multimodal hyper-dimensional functions. It includes modifications in the previous operators: Feeding and Swimming, as well as new: Memory and Partition operators. The latter two were introduced to account for the partition of the main school into subgroups. Some changes were also included in the stop conditions that now also have to consider subswarms.<ref>Madeiro, S. S., de Lima-Neto, F. B., Bastos-Filho, C. J. A., & do Nascimento Figueiredo, E. M. (2011, June). Density as the segregation mechanism in fish school search for multimodal optimization problems. In International Conference in Swarm Intelligence (pp. 563-572). Springer Berlin Heidelberg.</ref>\n\n===wFSS(Weight based Fish School Search)===\n\nwFSS is a weight based niching version of FSS intended to produce multiple solutions. The niching strategy is based on a new operator called link formator. This operator is used to define leaders for the fishes in order to form sub-schools.<ref>F. Buarque De Lima Neto and M. Gomes Pereira de Lacerda, “Weight based fish school search,” in Systems, Man and Cybernetics (SMC), 2014 IEEE International Conference on. IEEE, 2014, pp. 270–277.</ref>\n\n===FSS-SAR(Stagnation Avoidance Routine Fish School Search)===\n\nIn the original version of the algorithm, the individual movement component is only allowed to move a fish if it improves the fitness. However, in a very smooth search space, there would be many moving trials with no success and the algorithm could fail to converge.\nTo solve these issues, was introduced a parameter X for which 0 <= X <= 1 in the individual component of the movement. X decays exponentially along with the iterations and measures a probability for a worsening allowance for each fish. It means that, every time a fish tries to move to a position that does not improve its fitness, a random number is chosen and if it is smaller than X the movement is allowed.<ref>J. B. Monteiro, I. M. C. Albuquerque, F. B. L. Neto, and F. V. S. Ferreira, “Optimizing multi-plateau functions with FSS-SAR (Stagnation Avoidance Routine),” Submitted to IEEE Symposium Series on Computational Intelligence, 2016.</ref>\n\n===bFSS(Binary Fish School Search)===\n\nThe bFSS intended to cope with premature convergence. Proposing the use of a binary encoding scheme for the internal mechanisms of the fish school search. It combined the FSS with fuzzy modeling in a wrapper approach for Feature Selection.<ref>Sargo, João AG, et al. \"Binary Fish School Search applied to feature selection: Application to ICU readmissions.\" 2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE). IEEE, 2014.</ref>\n\n===MOFSS(Multi-Objective Fish School Search)===\n\nIn the MOFSS the operators are adapted to solve multi-objective problems. The algorithm deploys an External Archive to store the best non-dominated solutions found during the search process. This approach has been extensively used for different bio-inspired multiobjective optimizers.<ref>Deb, K., Thiele, L., Laumanns, M., & Zitzler, E.(2002) Scalable Multi-Objective Optimization Test Problems, In: IEEE Congress on Evolutionary Computation (pp. 825–830).</ref><ref>Nebro, A. J., Durillo, J. J., Garça-Nieto, J., Coello\nCoello, C. A., Luna, F., & Alba, E. (2009) SMPSO: A new PSO-based metaheuristic for multi-objective optimization, In: IEEE Symposium on Computational Intelligence in Multicriteria Decision-Making (pp. 66–73). doi:10.1109/MCDM.2009.4938830</ref> Furthermore, the solutions within the External Archive are used to guide the fish movements in the proposal version.<ref>Bastos-Filho, Carmelo JA, and Augusto CS Guimarães. \"Multi-Objective Fish School Search.\" International Journal of Swarm Intelligence Research (IJSIR) 6.1 (2015): 23-40.</ref>\n\n==See also==\n\n* [[Ant colony optimization algorithms]]\n* [[Artificial Bee Colony Algorithm]]\n* [[Particle swarm optimization]]\n\n==External links==\n* http://www.fbln.pro.br/fss/\n\n==References==\n{{Reflist}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Harris hawks optimization",
      "url": "https://en.wikipedia.org/wiki/Harris_hawks_optimization",
      "text": "[[File:Fig. 3. Different phases of HHO.jpg|thumb|Different phases of HHO algorithm]]\n\nThe '''Harris hawks optimization''' ('''HHO''') algorithm is a new [[swarm intelligence]] [[optimization]] paradigm proposed by [[Ali Asghar Heidari]] et al. in 2019<ref name=\"HeidariMirjalili2019\">{{cite journal|last1=Heidari|first1=Ali Asghar|last2=Mirjalili|first2=Seyedali|last3=Faris|first3=Hossam|last4=Aljarah|first4=Ibrahim|last5=Mafarja|first5=Majdi|last6=Chen|first6=Huiling|title=Harris hawks optimization: Algorithm and applications|journal=Future Generation Computer Systems|volume=97|year=2019|pages=849–872|issn=0167739X|doi=10.1016/j.future.2019.02.028}}</ref>, which is inspired by the [[team]] behaviors and chasing patterns of [[Harris's hawk]] in [[nature]] called surprise pounce.<ref>{{cite journal |last1=Bednarz |first1=J. C. |title=Cooperative Hunting Harris' Hawks (Parabuteo unicinctus) |journal=Science |date=25 March 1988 |volume=239 |issue=4847 |pages=1525–1527 |doi=10.1126/science.239.4847.1525}}</ref>\n\n== Exploration phase ==\n\nThe exploration mechanism of HHO is modeled as:\n\n<math>\nX(t+1) = \\left\\{\\begin{matrix}\nX_{rand}(t)-r_{1}\\left | X_{rand}(t)-2r_{2}X(t) \\right |  &  q\\geq 0.5   \\\\ \n(X_{rabbit}(t)-X_{m}(t))-r_{3}(LB+r_{4}(UB-LB)) &  q<0.5\n\\end{matrix}\\right.</math>\n\nwhere <math>X(t+1)</math> is the location of hawks in the next iteration <math>t</math>, <math>X_{rabbit}(t)</math> is the position of rabbit, <math>X(t)</math> is the current location of hawks, <math>r_{1}</math>, <math>r_{2}</math>, <math>r_{3}</math>, <math>r_{4}</math>, and <math>q</math> are random numbers in (0,1), <math>LB</math> and <math>UB</math> show the upper and lower bounds, <math>X_{rand}(t)</math> is a random hawk from the current swarm, and <math>X_{m}</math> is the average location of the hawks.\nThe average equation is attained by: \n\n<math>\nX_{m}(t)=\\frac{1}{N}\\sum_{i=1}^{N}X_{i}(t)\n</math> \n\nwhere <math>X_{i}(t)</math> shows the location of hawks in iteration <math>t</math> and <math>N</math> is the size of swarm.\n\n== Transition from exploration to exploitation ==\n\nTransition phase in HHO is performed based on the energy of the rabbit. To model this fact, the energy of a prey is modeled as: \n\n<math>\nE=2E_{0}(1-\\frac{t}{T})\n</math> \n\nwhere <math>E</math> is the escaping energy of the rabbit, <math>T</math> is the maximum iterations, and <math>E_{0}</math> is the initial energy.\n\n== Exploitation phase ==\n=== Soft besiege ===\n\nWhen <math>r\\geq0.5</math> and <math>|E|\\geq0.5</math>, this step is modeled by:\n\n<math>X(t+1)=\\Delta X(t)-E\\left |JX_{rabbit}(t)-X(t)\\right |</math>\n\n<math>\\Delta X(t)=X_{rabbit}(t)-X(t)</math>\n\nwhere <math>\\Delta X(t)</math> is the difference vector in iteration <math>t</math>, <math>r_{5}</math> is a random value inside (0,1), and <math>J=2(1-r_{5})</math> is the ''random jump strength'' of the rabbit.\n\n=== Hard besiege ===\n\nWhen <math>r\\geq</math>0.5 and <math>|E|<</math>0.5, the current positions are updated using:\n\n<math>X(t+1)=X_{rabbit}(t)-E \\left |\\Delta X(t) \\right |</math>\n\n=== Soft besiege with progressive rapid dives ===\n\nWhen still <math>|E|\\geq</math>0.5 but <math>r<</math>0.5, the hawks will decide on their next move based on:\n\n<math>Y=X_{rabbit}(t)-E\\left |JX_{rabbit}(t)-X(t)\\right |</math>\n\nIn HHO, hawks will dive based on the [[Lévy flight]] patterns using:\n\n<math>Z=Y+S\\times LF(D)</math>\n\nwhere <math>D</math> is the dimension and <math>S</math> is a random vector that have the size of <math>1\\times D</math> and LF is a function to obtain the patterns of [[Lévy flight]] using:\n\n<math>LF(x)=0.01\\times \\frac{u\\times \\sigma }{\\left | v \\right |^{\\frac{1}{\\beta }}}, \\sigma =\\left ( \\frac{\\Gamma (1+\\beta )\\times sin(\\frac{\\pi\\beta}{2})}{\\Gamma(\\frac{1+\\beta}{2})\\times \\beta\\times2^{(\\frac{\\beta-1}{2})}}  \\right )^{\\frac{1}{\\beta}}</math>\n\nwhere <math>u</math>, <math>v</math> are random values in (0,1).\n\nHence, we have: \n\n<math> \nX(t+1)=\\left\\{\\begin{matrix}\nY & if F(Y)<F(X(t)) \\\\ \nZ  & if F(Z)<F(X(t)) \\\\\n\\end{matrix}\\right.</math> \n\nwhere <math>Y</math> and <math>Z</math> are obtained using previous rules.\n\n=== Hard besiege with progressive rapid dives ===\n\nWhen <math>|E|<</math>0.5 and <math>r<</math>0.5, the following rule is performed:\n\n<math>X(t+1)=\\left\\{\\begin{matrix}\nY & if F(Y)<F(X(t)) \\\\ \nZ  & if F(Z)<F(X(t)) \\\\\n\\end{matrix}\\right.</math>\n\nwhere <math>Y</math> and <math>Z</math> are obtained using:\n\n<math>Y=X_{rabbit}(t)-E\\left |JX_{rabbit}(t)-X_{m}(t)\\right |</math>\n\n<math>Z=Y+S\\times LF(D)</math>\n\n== [[Pseudocode]] of HHO ==\nThe [[pseudocode]] of the proposed HHO [[algorithm]] is reported here.\n<!-- Same example as in Wikipedia:WikiProject_Computer_science/Manual_of_style#Algorithms -->\n  '''Inputs''': The population size <math>N</math> and maximum number of iterations <math>T</math> \n  '''Outputs''': The location of rabbit and its fitness value \n   Initialize the random population <math>X_{i}(i=1,2,\\ldots,N)</math> \n  '''While''' ''stopping condition is not met''\n   Calculate the fitness values of hawks \n      Set '''<math>X_{rabbit}</math>''' as the location of rabbit (best location) \n    '''For''' each hawk <math>X_{i}</math>\n       Update the initial energy <math>E_{0}</math> and jump strength <math>J</math> \n       Update the <math>E</math> \n        '''If'''<math>|E|>1</math> \n             Update the location vector the related exploration rules \n        '''If'''<math>|E|<1</math>\n             Update the location vector the related exploitation rules \n  '''Return''' '''<math>X_{rabbit}</math>'''\n\n== Open source codes and metadata ==\nThe [[Source Code]] of HHO optimizer is publicly available on several websites Mirjalili website [http://alimirjalili.com/HHO.html HHO home page] and EVOML Research group [http://evo-ml.com/2019/03/02/hho/ EVOML Group]\n\n== Applications == \n\nThe HHO is a stochastic optimization algorithm developed to solve any kind of optimization problems. Until now, there are several applications for this algorithm.\n* [[Global optimization]]: HHO has been applied to solve [[problems]] in [[Global optimization]] \n* [[Engineering]] design problems: There are several engineering cases that HHO provided satisfactory results such as Multi-plate [[Disc brake]]\n[[Clutch]] and [[Rolling-element bearing]]\n* [[Satellite]] Image De-noising <ref name=\"GolilarzGao2019\">{{cite journal|last1=Golilarz|first1=Noorbakhsh Amiri|last2=Gao|first2=Hui|last3=Demirel|first3=Hasan|title=Satellite Image De-noising with Harris Hawks Meta Heuristic Optimization Algorithm and Improved Adaptive Generalized Gaussian Distribution Threshold Function|journal=IEEE Access|volume=7|year=2019|pages=57459–57468|issn=2169-3536|doi=10.1109/ACCESS.2019.2914101}}</ref>.\n* [[Satellite]] Image Segmentation<ref>{{cite journal |last1=Jia |last2=Lang |last3=Oliva |last4=Song |last5=Peng |title=Dynamic Harris Hawks Optimization with Mutation Mechanism for Satellite Image Segmentation |journal=Remote Sensing |date=14 June 2019 |volume=11 |issue=12 |pages=1421 |doi=10.3390/rs11121421}}</ref>\n\n== References ==\n<!-- Inline citations added to your article will automatically display here. See en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. -->\n{{reflist}}\n\n[[Category:Nature-inspired metaheuristics]]\n[[Category:Collective intelligence]]"
    },
    {
      "title": "HUMANT (HUManoid ANT) algorithm",
      "url": "https://en.wikipedia.org/wiki/HUMANT_%28HUManoid_ANT%29_algorithm",
      "text": "HUMANT (HUManoid ANT) algorithm<ref>{{cite journal|last1=Mladineo|first1=Marko|last2=Veza|first2=Ivica|last3=Gjeldum|first3=Nikola|title=Single-Objective and Multi-Objective Optimization using the HUMANT algorithm|journal=Croatian Operational Research Review (CRORR)|date=2015|volume=6|doi=10.17535/crorr.2015.0035}}</ref> belongs to [[Ant colony optimization algorithms]]. It is a Multi-Objective Ant Colony Optimization (MOACO) with ''a priori'' approach to [[Multi-objective optimization|Multi-Objective Optimization]] (MOO), based on Max-Min Ant System (MMAS) and [[Multi-Criteria Decision Analysis|multi-criteria decision-making]] [[Preference ranking organization method for enrichment evaluation|PROMETHEE method]].\n\nThe algorithm is based on ''a priori'' approach to Multi-Objective Optimization, which means that it integrates decision-makers preferences into optimization process.<ref>{{cite book|last1=Talbi|first1=El-Ghazali|title=Metaheuristics – From Design to Implementation|date=2009|publisher=John Wiley & Sons}}</ref> Using decision-makers preferences, it actually turns multi-objective problem into single-objective. It is a process called scalarization of a multi-objective problem.<ref>{{cite journal|last1=Eppe|first1=Stefan|title=Application of the Ant Colony Optimization Metaheuristic to multi-objective optimization problems|journal=Technical report – ULB, Bruxelles|date=2009}}</ref> The first Multi-Objective Ant Colony Optimization (MOACO) algorithm was published in 2001,<ref>{{cite journal|last1=Iredi|first1=Steffen|last2=Merkle|first2=Daniel|last3=Middendorf|first3=Martin|title=Bi-Criterion Optimization with Multi Colony Ant Algorithms|journal=Evolutionary Multi-Criterion Optimization|date=2001|volume=1993|pages=359-372}}</ref> but it was based on ''a posteriori'' approach to MOO.\n\nThe idea of using [[Preference ranking organization method for enrichment evaluation|PROMETHEE method]] to integrate decision-makers preferences into MOACO algorithm was born in 2009.<ref>{{cite journal|last1=Eppe|first1=Stefan|title=Integrating the decision maker's preferences into Multi Objective Ant Colony Optimization|journal=Proceedings of the 2nd Doctoral Symposium on|date=2009}}</ref>\nSo far, HUMANT algorithm is only known fully operational optimization algorithm that successfully integrated PROMETHEE method into ACO.\n\nHUMANT algorithm has been experimentally tested on the [[Traveling salesman problem]] and applied to the Partner selection problem (PSP) with up to four objectives (criteria).<ref>{{cite journal|last1=Mladineo|first1=Marko|last2=Veza|first2=Ivica|last3=Gjeldum|first3=Nikola|title=Solving partner selection problem in cyber-physical production networks using the HUMANT algorithm|journal=International Journal of Production Research|date=2016|pages=1–16|doi=10.1080/00207543.2016.1234084}}</ref>\n\n== References ==\n{{Reflist}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Imperialist competitive algorithm",
      "url": "https://en.wikipedia.org/wiki/Imperialist_competitive_algorithm",
      "text": "In [[computer science]], '''imperialist competitive algorithm''' is a computational method that is used to solve [[optimization problem]]s of different types.<ref name=ica_en_2007_cnf_atashpaz_ica_ica>{{cite conference\n|last1= Atashpaz-Gargari\n|first1= E.\n|last2= Lucas\n|first2= C\n|title= Imperialist Competitive Algorithm: An algorithm for optimization inspired by imperialistic competition\n|booktitle= IEEE Congress on Evolutionary Computation\n|year= 2007\n|volume= 7\n|pages= 4661–4666\n|url=http://www.academia.edu/download/3930081/imperialistic_competitive_algorithm__ica__ieee_cec_2007.pdf\n}}</ref><ref name=ICA_2014_Survey>{{cite journal\n|last1= Hosseini\n|first1=S.\n|last2=Al Khaled\n|first2=A.\n|title=A survey on the Imperialist Competitive Algorithm metaheuristic: Implementation in engineering domain and directions for future research\n|journal=Applied Soft Computing\n|year=2014\n|volume=24\n|pages=1078–1094\n|url=https://www.sciencedirect.com/science/article/pii/S1568494614003895\n}}</ref>  Like most of the methods in the area of [[evolutionary computation]], ICA does not need the gradient of the function in its optimization process. From a specific point of view, ICA can be thought of as the social counterpart of [[genetic algorithms]] (GAs). ICA is the mathematical model and the computer simulation of human [[social evolution]], while GAs are based on the [[biological evolution]] of species.\n\n== Metaphor ==\n[[File:Imperialist-competitive-algorithm-flowchart.jpg|thumb|420px|Figure 1: Flowchart of Imperialist Competitive Algorithm (ICA)]]\nFigure 1 shows the flowchart of the Imperialist Competitive Algorithm. This algorithm starts by generating a set of candidate random solutions in the search space of the optimization problem. The generated random points are called the initial ''Countries''. Countries in this algorithm are the counterpart of ''Chromosome''s in GAs and ''Particle''s in [[Particle Swarm Optimization]] (PSO) and it is an array of values of a candidate solution of optimization problem. The [[Loss function|cost function]] of the optimization problem determines the power of each country. Based on their power, some of the best initial countries (the countries with the least cost function value), become ''Imperialists'' and start taking control of other countries (called ''colonies'') and form the initial ''Empires''.<ref name=ica_en_2007_cnf_atashpaz_ica_ica />\n\nTwo main operators of this algorithm are ''Assimilation'' and ''Revolution''. Assimilation makes the colonies of each empire get closer to the imperialist state in the space of socio-political characteristics (optimization search space). Revolution brings about sudden random changes in the position of some of the countries in the search space. During assimilation and revolution a colony might reach a better position and has the chance to take the control of the entire empire and replace the current imperialist state of the empire.<ref name=ica_en_2010_jnl_nazari_integrated_product_mix_outsourcing>{{cite journal\n|last1= Nazari-Shirkouhi\n|first1= S.\n|last2= Eivazy\n|first2= H.\n|last3= Ghodsi\n|first3= R.\n|last4= Rezaie\n|first4= K.\n|last5= Atashpaz-Gargari\n|first5= E.\n|title= Solving the Integrated Product Mix-Outsourcing Problem by a Novel Meta-Heuristic Algorithm: Imperialist Competitive Algorithm\n|journal= Expert Systems with Applications\n|year= 2010\n|volume= 37\n|issue= 12\n|pages= 7615–7626\n|doi=10.1016/j.eswa.2010.04.081\n}}</ref>\n\n''Imperialistic Competition'' is another part of this algorithm. All the empires try to win this game and take possession of colonies of other empires. In each step of the algorithm, based on their power, all the empires have a chance to take control of one or more of the colonies of the weakest empire.<ref name=ica_en_2007_cnf_atashpaz_ica_ica />\n\nAlgorithm continues with the mentioned steps (Assimilation, Revolution, Competition) until a stop condition is satisfied.\n\n== Algorithm ==\nThe above steps can be summarized as the below [[pseudocode]].<ref name=ICA_2014_Survey /><ref name=ica_en_2010_jnl_nazari_integrated_product_mix_outsourcing />\n 0) Define objective function: <math>f(\\mathbf{x}), \\quad \\mathbf{x}=(x_1,x_2,\\dots,x_d); \\, </math>\n 1) Initialization of the algorithm. Generate some random solution in the search space and create initial empires.\n     2) Assimilation: Colonies move towards imperialist states in different in directions.\n     3) Revolution: Random changes occur in the characteristics of some countries.\n     4) Position exchange between a colony and Imperialist. A colony with a better position than the imperialist,\n        has the chance to take the control of empire by replacing the existing imperialist.\n     5) Imperialistic competition: All imperialists compete to take possession of colonies of each other.\n     6) Eliminate the powerless empires. Weak empires lose their power gradually and they will finally be eliminated.\n     7) If the stop condition is satisfied, stop, if not go to 2.\n 8) End\n\n== See also ==\n* [[List of metaphor-based metaheuristics]]\n\n== References ==\n{{Reflist|30em}}\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "Stochastic diffusion search",
      "url": "https://en.wikipedia.org/wiki/Stochastic_diffusion_search",
      "text": "'''Stochastic diffusion search (SDS)''' was first described in 1989 as a population-based, pattern-matching algorithm [Bishop, 1989]. It belongs to a family of [[swarm intelligence]] and naturally inspired search and [[optimisation]] algorithms which includes [[ant colony optimization]], [[particle swarm optimization]] and [[genetic algorithm]]s; as such SDS was the first Swarm Intelligence metaheuristic. Unlike stigmergetic communication employed in [[ant colony optimization]], which is based on modification of the physical properties of a simulated environment, SDS uses a form of direct (one-to-one) communication between the agents similar to the tandem calling mechanism employed by one species of ants, ''Leptothorax acervorum''.\n\nIn SDS agents perform cheap, partial evaluations of a hypothesis (a candidate solution to the search problem). They then share information about hypotheses (diffusion of information) through direct one-to-one communication. As a result of the diffusion mechanism, high-quality solutions can be identified from clusters of agents with the same hypothesis. The operation of SDS is most easily understood by means of a simple analogy &ndash; The Restaurant Game.\n\n== The restaurant game ==\nA group of delegates attends a long conference in an unfamiliar town. Every night each delegate must find somewhere to dine. There is a large choice of restaurants, each of which offers a large variety of meals. The problem the group faces is to find the best restaurant, that is the restaurant where the maximum number of delegates would enjoy dining. Even a parallel exhaustive search through the restaurant and meal combinations would take too long to accomplish. To solve the problem delegates decide to employ a stochastic diffusion search.\n\nEach delegate acts as an agent maintaining a hypothesis identifying the best restaurant in town. Each night each delegate tests his hypothesis by dining there and randomly selecting one of the meals on offer. The next morning at breakfast every delegate who did not enjoy his meal the previous night, asks one randomly selected colleague to share his dinner impressions.\nIf the experience was good, he also adopts this restaurant as his choice. Otherwise he simply selects another restaurant at random from those listed in `Yellow Pages'. Using this strategy it is found that very rapidly significant number of\ndelegates congregate around the 'best' restaurant in town.\n\n==Applications==\nSDS has been applied to diverse problems such as text search [Bishop, 1989], object recognition [Bishop, 1992], feature tracking [Grech-Cini, 1993], mobile robot self-localisation [Beattie, 1998] and site selection for wireless networks [Whitaker, 2002].\n\n==Analysis==\nUnlike many Nature Inspired Search techniques there is a comprehensive mathematical framework describing the behaviour of SDS. Analysis of SDS has investigated its global optimality and convergence [Nasuto, 1998], linear time complexity [Nasuto et al., 1999], robustness, [Myatt, 2004] and resource allocation [Nasuto, 1999] under a variety of search conditions.\n\n==References==\n*Bishop, J.M., (1989). Stochastic Searching Networks. Proc. 1st IEE Conf. on Artificial Neural Networks, pp 329–331, London. \n*Bishop, J.M. & Torr, P., (1992). The Stochastic Search Network. In R. Linggard, D.J. Myers, C. Nightingale (eds.), Neural Networks for Images, Speech and Natural Language, pp370–387, New York, Chapman & Hall.\n*Beattie, P.D. & Bishop, J.M., (1998). Self-Localisation in the 'Senario' Autonomous Wheelchair. Journal of Intelligent and Robotic Systems 22, pp 255–267, Kluwer Academic Publishers. \n*Grech-Cini, H.J. & McKee, G.T. (1993) Locating the Mouth Region in Images of Human Faces. In P.S.Schenker (Ed.), Proceedings of SPIE &ndash; The International Society for Optical Engineering, Sensor Fusion VI 2059, Massachusetts.\n*Myatt, D.R., Bishop J.M. and Nasuto, S.J., (2004). Minimum Stable Convergence Criteria for Stochastic Diffusion Search To be published in Electronics Letters.\n*Nasuto, S.J., (1999). Analysis of Resource Allocation of Stochastic Diffusion Search. PhD Thesis. University of Reading, UK.\n*Nasuto, S.J. & Bishop, J.M., (1999). Convergence Analysis of Stochastic Diffusion Search. Journal of Parallel Algorithms and Applications 14:2, pp 89–107. \n*Nasuto, S.J., Bishop, J.M. & Lauria, L., (1998). Time Complexity of Stochastic Diffusion Search. Neural Computation '98, Vienna, Austria. \n*Whitaker, R.M., Hurley, S., (2002). An agent based approach to site selection for wireless networks. Proc ACM Symposium on Applied Computing (Madrid). 574&ndash;577.\n*Jones, D. (2002). [http://www.readingconnect.net/web/FILES/sse/sds-djonesscarp2002.pdf Constrained Stochastic Diffusion Search].  SCARP 2002, University of Reading, UK.\n\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "AirSwap",
      "url": "https://en.wikipedia.org/wiki/AirSwap",
      "text": "{{notability|date=May 2019}}\n{{morerefs|date=May 2019}}\n\n{{use dmy dates|date=May 2019}}\n{{use Indian English|date=May 2019}}\n{{Infobox company\n| founders = Michael Oved and Don Mosites\n| parent = [[ConsenSys]] via [[Fluidity (company)|Fluidity]]\n| founded = 2017\n| website = {{url||https://www.airswap.io/}}\n}}\n'''AirSwap''' is a US based decentralised exchange trading protocol built on [[Ethereum]] blockchain.{{sfn|Qiu|2018|p=57}} The market capitalisation of AirSwap is around $5.69 million.<ref>{{citation |title=AirSwap Trading Up 2.3% This Week (AST) |url=https://www.fidaily.com/2019/05/07/airswap-trading-up-2-3-this-week-ast.html |work=fidaily.com |date=7 May 2019 }}</ref> It is a [[ConsenSys]] company,<ref>{{citation |url=https://technical.ly/brooklyn/2017/10/18/airswap-ico/ |title=East Williamsburg's Airswap raises $36 million in ICO - Technical.ly Brooklyn |date=18 October 2017 |work=Technical.ly Brooklyn }}</ref> developed by [[Fluidity (company)|Fluidity]] in 2017.<ref>{{citation |title=Margaret Hsu, Stuart Wagner |url=https://www.nytimes.com/2018/08/19/fashion/weddings/margaret-hsu-stuart-wagner.html |work=[[The New York Times]] |date=19 April 2018 }}</ref>\n\nThe company's [[initial coin offering]], AirSwap tokens, was released on 10 October 2017.<ref>{{citation |title=A start-up is raising funds for a cryptocurrency exchange — by selling a cryptocurrency |url=https://www.cnbc.com/2017/09/29/airswap-wants-to-build-a-cryptocurrency-exchange-by-selling-a-digital-token.html |work=[[CNBC]] |date=29 September 2017 }}</ref>{{sfn|Boreiko|2019|p=96}} Michael Oved and Don Mosites are the co-founders.<ref>{{citation |title=This 31-Year-Old Is Trying to Revolutionize Cryptocurrency Trading |url=https://www.bloomberg.com/news/articles/2017-09-28/upending-digital-currency-market-is-next-act-for-ex-virtu-trader |work=[[Bloomberg L.P.|Bloomberg]] |date=28 September 2017 }}</ref>\n\nThe company is also working with [[Brave (web browser)]] to pay users for viewing ads in cryptocurrency.<ref>{{citation |title=This New Privacy-focused Web Browser Wants To Pay Its Users For Looking At Ads |url=https://www.republicworld.com/technology-news/apps/this-new-privacy-focused-web-browser-wants-to-pay-its-users-for-looking-at-ads |work=[[Republic TV]] |date=26 April 2019 }}</ref>\n\n==References==\n===Citations===\n{{reflist}}\n===Sources===\n* {{citation |last=Boreiko |first=Dmitri |title=Blockchain-based financing with Initial Coin Offerings (ICOs): Financial Industry disruption or evolution? |url=https://books.google.co.in/books?id=ixyXDwAAQBAJ |publisher=Universitas Studiorum |year=2019 |isbn=978-88-3369-047-6 }}\n* {{citation |editor-last=Qiu |editor-first=Meikang |title=Smart Blockchain: First International Conference, SmartBlock 2018, Tokyo, Japan, December 10-12, 2018 Proceedings |url=https://books.google.co.in/books?id=Cvx9DwAAQBAJ |publisher=Springer |year=2018 |isbn=978-3-030-05764-0 }}\n\n==External links==\n* {{Official website|https://www.airswap.io/}}\n\n[[Category:Cryptocurrencies]]\n[[Category:Exchange algorithms]]\n[[Category:Blockchains]]\n\n\n{{Finance-stub}}"
    },
    {
      "title": "Bareiss algorithm",
      "url": "https://en.wikipedia.org/wiki/Bareiss_algorithm",
      "text": "In mathematics, the '''Bareiss algorithm''', named after [[Erwin Bareiss]], is an [[algorithm]] to calculate the [[determinant]] or the [[echelon form]] of a [[Matrix (mathematics)|matrix]] with [[integer]] entries using only integer arithmetic; any [[division (mathematics)|division]]s that are performed are guaranteed to be exact (there is no [[remainder]]). The method can also be used to compute the determinant of matrices with (approximated) [[real number|real]] entries, avoiding the introduction any round-off errors beyond those already present in the input.\n\n==Analysis==\nDuring the execution of Bareiss algorithm, every integer that is computed is the determinant of a submatrix of the input matrix. This allows, using the [[Hadamard inequality]], to bound the size of these integers. Otherwise, the Bareiss algorithm may be viewed as a variant of [[Gaussian elimination]] and needs roughly the same number of arithmetic operations.\n\nIt follows that, for an ''n'' × ''n'' matrix of maximum (absolute) value 2<sup>''L''</sup> for each entry, the Bareiss algorithm runs in [[Big O notation|O(''n''<sup>3</sup>)]] elementary operations with an O(''n''<sup>&nbsp;''n''/2</sup>&nbsp;2<sup>''nL''</sup>) bound on the absolute value of intermediate values needed. Its [[computational complexity]] is thus O(''n''<sup>5</sup>''L''<sup>2</sup>&nbsp;(log(''n'')<sup>2</sup>&nbsp;+&nbsp;''L''<sup>2</sup>)) when using elementary arithmetic or O(''n''<sup>4</sup>''L''&nbsp;(log(''n'')&nbsp;+&nbsp;''L'')&nbsp;log(log(''n'')&nbsp;+&nbsp;''L''))) by using [[fast multiplication]].\n\n==History==\nThe general Bareiss algorithm is distinct from the Bareiss algorithm for [[Toeplitz matrix|Toeplitz matrices]].\n\nIn some Spanish-speaking countries, this algorithm is also known as '''Bareiss-Montante''', because of [[René Mario Montante Pardo]], a professor of the [[Universidad Autónoma de Nuevo León]], [[Mexico]], that popularized the method among his students.  \n\n==References==\n*{{citation|first=Erwin H.|last=Bareiss|title= Sylvester's Identity and multistep integer-preserving Gaussian elimination|pages=565&ndash;578|url=http://www.ams.org/journals/mcom/1968-22-103/S0025-5718-1968-0226829-0/S0025-5718-1968-0226829-0.pdf|journal=[[Mathematics of Computation]]|year=1968|volume=22|issue=103|doi=10.2307/2004533|jstor=2004533}}.\n*{{citation|first=Erwin H.|last=Bareiss|title=MULTISTEP INTEGER-PRESERVING GAUSSIAN ELIMINATION|url=https://digital.library.unt.edu/ark:/67531/metadc1035277/m2/1/high_res_d/4474185.pdf|year=1966}}. ''(Contains a clearer picture of the operations sequence)''\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Bareiss Algorithm}}\n[[Category:Determinants]]\n[[Category:Numerical linear algebra]]\n[[Category:Exchange algorithms]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Gaussian elimination",
      "url": "https://en.wikipedia.org/wiki/Gaussian_elimination",
      "text": "'''Gaussian elimination''', also known as '''row reduction''', is an [[algorithm]] in [[linear algebra]] for solving a [[system of linear equations]]. It is usually understood as a sequence of operations performed on the corresponding [[matrix (mathematics)|matrix]] of coefficients. This method can also be used to find the [[Rank (linear algebra)|rank]] of a matrix, to calculate the [[determinant]] of a matrix, and to calculate the inverse of an [[invertible matrix|invertible square matrix]]. The method is named after [[Carl Friedrich Gauss]] (1777–1855), although it was known to Chinese mathematicians as early as 179&nbsp;A.D. (see [[#History|History section]]).\n\nTo perform row reduction on a matrix, one uses a sequence of [[elementary row operations]] to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: \n* Swapping two rows, \n* Multiplying a row by a nonzero number, \n* Adding a multiple of one row to another row.\nUsing these operations, a matrix can always be transformed into an [[Triangular matrix|upper triangular matrix]], and in fact one that is in [[row echelon form]]. Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in [[reduced row echelon form]]. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.\n\n:<math>\\left[\\begin{array}{rrr|r}\n1 & 3 & 1 & 9 \\\\\n1 & 1 & -1 & 1 \\\\\n3 & 11 & 5 & 35\n\\end{array}\\right]\\to\n\\left[\\begin{array}{rrr|r}\n1 & 3 & 1 & 9 \\\\\n0 & -2 & -2 & -8 \\\\\n0 & 2 & 2 & 8\n\\end{array}\\right]\\to\n\\left[\\begin{array}{rrr|r}\n1 & 3 & 1 & 9 \\\\\n0 & -2 & -2 & -8 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]\\to\n\\left[\\begin{array}{rrr|r}\n1 & 0 & -2 & -3 \\\\\n0 & 1 & 1 & 4 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] </math>\n\nUsing row operations to convert a matrix into reduced row echelon form is sometimes called '''Gauss–Jordan elimination'''. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (unreduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.\n\n== Definitions and example of algorithm ==\nThe process of row reduction makes use of [[elementary row operations]], and can be divided into two parts. The first part (sometimes called forward elimination) reduces a given system to ''row echelon form'', from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called [[Triangular matrix#Forward and back substitution|back substitution]]) continues to use row operations until the solution is found; in other words, it puts the matrix into ''reduced'' row echelon form.\n\nAnother point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a [[matrix decomposition]] of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by [[elementary matrix|elementary matrices]]. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a [[Frobenius matrix]]. Then the first part of the algorithm computes an [[LU decomposition]], while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.\n\n===Row operations===\n{{see also|Elementary matrix}}\n\nThere are three types of '''elementary row operations''' which may be performed on the rows of a matrix:\n# Swap the positions of two rows.\n# Multiply a row by a non-zero [[scalar (mathematics)|scalar]].\n# Add to one row a scalar multiple of another.\n\nIf the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.\n\n===Echelon form===\n{{main|Row echelon form}}\nFor each row in a matrix, if the row does not consist of only zeros, then the leftmost nonzero entry is called the ''[[leading coefficient]]'' (or ''pivot'') of that row. So if two leading coefficients are in the same column, then a row operation of [[#Row operations|type 3]] could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in '''row echelon form'''. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word \"echelon\" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.\n\nFor example, the following matrix is in row echelon form, and its leading coefficients are shown in red:\n\n: <math>\\begin{bmatrix}\n  0 & \\color{red}{\\mathbf{2}} &                     1   & -1 \\\\\n  0 &                     0   & \\color{red}{\\mathbf{3}} &  1 \\\\\n  0 &                     0   &                     0   &  0\n\\end{bmatrix}.</math>\n\nIt is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).\n\nA matrix is said to be in '''reduced row echelon form''' if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).\n\n===Example of the algorithm===\nSuppose the goal is to find and describe the set of solutions to the following [[system of linear equations]]:\n: <math>\n\\begin{alignat}{4}\n  2x &{}+{}& y &{}-{}&  z &{}={}&   8 & \\qquad (L_1) \\\\\n -3x &{}-{}& y &{}+{}& 2z &{}={}& -11 & \\qquad (L_2) \\\\\n -2x &{}+{}& y &{}+{}& 2z &{}={}&  -3 & \\qquad (L_3)\n\\end{alignat}\n</math>\n\nThe table below is the row reduction process applied simultaneously to the system of equations and its associated [[augmented matrix]]. In practice, one does not usually deal with the systems in terms of equations, but instead makes use of the augmented matrix, which is more suitable for computer manipulations. The row reduction procedure may be summarized as follows: eliminate {{mvar|x}} from all equations below {{math|''L''<sub>1</sub>}}, and then eliminate {{mvar|y}} from all equations below {{math|''L''<sub>2</sub>}}. This will put the system into [[triangular form]]. Then, using back-substitution, each unknown can be solved for.\n\n: {| style=\"background-color:white;\" class=\"wikitable\"\n|-\n! System of equations !! Row operations !! Augmented matrix\n|- align=\"center\"\n| <math>\n\\begin{alignat}{4}\n  2x &{}+{}& y &{}-{}&  z &{}={}&   8 & \\\\\n -3x &{}-{}& y &{}+{}& 2z &{}={}& -11 & \\\\\n -2x &{}+{}& y &{}+{}& 2z &{}={}&  -3 &\n\\end{alignat}\n</math>\n|\n| <math>\n\\left[\\begin{array}{rrr|r}\n  2 &  1 & -1 &   8 \\\\\n -3 & -1 &  2 & -11 \\\\\n -2 &  1 &  2 &  -3\n\\end{array}\\right]\n</math>\n|- align=\"center\"\n| <math>\n\\begin{alignat}{4}\n 2x &{}+{}&          y &{}-{}&          z &{}={}& 8 & \\\\\n    &     & \\tfrac12 y &{}+{}& \\tfrac12 z &{}={}& 1 & \\\\\n    &     &         2y &{}+{}&          z &{}={}& 5 &\n\\end{alignat}\n</math>\n| <math>\n\\begin{align}\n L_2 + \\tfrac32 L_1 &\\to L_2 \\\\\n L_3 +          L_1 &\\to L_3\n\\end{align}\n</math>\n| <math>\n\\left[\\begin{array}{rrr|r}\n 2 &      1  &     -1  & 8 \\\\\n 0 & \\frac12 & \\frac12 & 1 \\\\\n 0 &      2  &      1  & 5\n\\end{array}\\right]\n</math>\n|- align=\"center\"\n| <math>\n\\begin{alignat}{4}\n 2x &{}+{}&          y &{}-{}&          z &{}={}& 8 & \\\\\n    &     & \\tfrac12 y &{}+{}& \\tfrac12 z &{}={}& 1 & \\\\\n    &     &            &     &         -z &{}={}& 1 &\n\\end{alignat}\n</math>\n| <math>\n L_3 + -4 L_2 \\to L_3</math>\n| <math>\n\\left[\\begin{array}{rrr|r}\n 2 &      1  &     -1  & 8 \\\\\n 0 & \\frac12 & \\frac12 & 1 \\\\\n 0 &      0  &     -1  & 1\n\\end{array}\\right]\n</math>\n|-\n|colspan=3; align=\"center\"| The matrix is now in echelon form (also called triangular form)\n|- align=\"center\"\n| <math>\n\\begin{alignat}{4}\n 2x &{}+{}&          y &     &   &{}={}       7  & \\\\\n    &     & \\tfrac12 y &     &   &{}={} \\tfrac32 & \\\\\n    &     &            &{}-{}& z &{}={}       1  &\n\\end{alignat}\n</math>\n| <math>\n\\begin{align}\n L_2 + \\tfrac12 L_3 &\\to L_2 \\\\\n L_1 -          L_3 &\\to L_1\n\\end{align}\n</math>\n| <math>\n\\left[\\begin{array}{rrr|r}\n 2 &      1  &  0 &      7  \\\\\n 0 & \\frac12 &  0 & \\frac32 \\\\\n 0 &      0  & -1 &      1\n\\end{array}\\right]\n</math>\n|- align=\"center\"\n| <math>\n\\begin{alignat}{4}\n 2x &{}+{}& y &\\quad&   &{}={}&  7 & \\\\\n    &     & y &\\quad&   &{}={}&  3 & \\\\\n    &     &   &\\quad& z &{}={}& -1 &\n\\end{alignat}\n</math>\n| <math>\n\\begin{align}\n 2 L_2 &\\to L_2 \\\\\n  -L_3 &\\to L_3\n\\end{align}\n</math>\n| <math>\n\\left[\\begin{array}{rrr|r}\n 2 & 1 & 0 &  7 \\\\\n 0 & 1 & 0 &  3 \\\\\n 0 & 0 & 1 & -1\n\\end{array}\\right]\n</math>\n|- align=\"center\"\n| <math>\n\\begin{alignat}{4}\n x &\\quad&   &\\quad&   &{}={}&  2 & \\\\\n   &\\quad& y &\\quad&   &{}={}&  3 & \\\\\n   &\\quad&   &\\quad& z &{}={}& -1 &\n\\end{alignat}\n</math>\n| <math>\n\\begin{align}\n          L_1 - L_2 &\\to L_1 \\\\\n \\tfrac12 L_1       &\\to L_1\n\\end{align}\n</math>\n| <math>\n\\left[\\begin{array}{rrr|r}\n 1 & 0 & 0 &  2 \\\\\n 0 & 1 & 0 &  3 \\\\\n 0 & 0 & 1 & -1\n\\end{array}\\right]\n</math>\n|}\n\nThe second column describes which row operations have just been performed. So for the first step, the {{mvar|x}} is eliminated from {{math|''L''<sub>2</sub>}} by adding {{math|{{sfrac|3|2}}''L''<sub>1</sub>}} to {{math|''L''<sub>2</sub>}}. Next, {{mvar|x}} is eliminated from {{math|''L''<sub>3</sub>}} by adding {{math|''L''<sub>1</sub>}} to {{math|''L''<sub>3</sub>}}. These row operations are labelled in the table as\n\n: <math>\\begin{align}\n L_2 + \\tfrac32 L_1 &\\to L_2, \\\\\n L_3 +          L_1 &\\to L_3.\n\\end{align}</math>\n\nOnce {{mvar|y}} is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is {{math|''z'' {{=}} −1}}, {{math|''y'' {{=}} 3}}, and {{math|''x'' {{=}} 2}}. So there is a unique solution to the original system of equations.\n\nInstead of stopping once the matrix is in echelon form, one could continue until the matrix is in ''reduced'' row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as '''Gauss–Jordan elimination''', to distinguish it from stopping after reaching echelon form.\n\n== History ==\nThe method of Gaussian elimination appears in the Chinese mathematical text [[Rod calculus#System of linear equations|Chapter Eight: ''Rectangular Arrays'']] of ''[[The Nine Chapters on the Mathematical Art]]''. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179&nbsp;CE, but parts of it were written as early as approximately 150&nbsp;BCE.<ref>{{harvtxt|Calinger|1999}}, pp. 234–236</ref><ref name=\"princeton\">{{cite book|author1=Timothy Gowers|author2=June Barrow-Green|author3=Imre Leader|title=The Princeton Companion to Mathematics|date=8 September 2008|publisher=Princeton University Press|isbn=978-0-691-11880-2|page=607}}<!-- |accessdate=28 September 2012 --></ref> It was commented on by [[Liu Hui]] in the 3rd century.\n\nThe method in Europe stems from the notes of [[Isaac Newton]].<ref>{{harvtxt|Grcar|2011a}}, pp. 169-172</ref><ref>{{harvtxt|Grcar|2011b}}, pp. 783-785</ref> In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied.  Cambridge University eventually published the notes as ''Arithmetica Universalis'' in 1707 long after Newton had left academic life.  The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century.  [[Carl Friedrich Gauss]] in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional [[Human computer|hand computers]] to solve the normal equations of least-squares problems.<ref>{{harvtxt|Lauritzen|}}, p.&nbsp;3</ref>  The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.<ref>{{harvtxt|Grcar|2011b}}, p.&nbsp;789</ref>\n\nSome authors use the term ''Gaussian elimination'' to refer only to the procedure until the matrix is in echelon form, and use the term '''Gauss–Jordan elimination''' to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by [[Wilhelm Jordan (geodesist)|Wilhelm Jordan]] in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.<ref>{{Citation | last1=Althoen | first1=Steven C. | last2=McLaughlin | first2=Renate | title=Gauss–Jordan reduction: a brief history | doi=10.2307/2322413 | year=1987 | journal=[[American Mathematical Monthly|The American Mathematical Monthly]] | issn=0002-9890 | volume=94 | issue=2 | pages=130–142 | jstor=2322413 | publisher=Mathematical Association of America}}</ref>\n\n== Applications ==\nHistorically, the first application of the row reduction method is for solving [[systems of linear equations]]. Here are some other important applications of the algorithm.\n\n=== Computing determinants ===\nTo explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:\n* Swapping two rows multiplies the determinant by −1\n* Multiplying a row by a nonzero scalar multiplies the determinant by the same scalar\n* Adding to one row a scalar multiple of another does not change the determinant.\n\nIf Gaussian elimination applied to a square matrix {{mvar|A}} produces a row echelon matrix {{mvar|B}}, let {{mvar|d}} be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of {{mvar|A}} is the quotient by {{mvar|d}} of the product of the elements of the diagonal of {{mvar|B}}:\n:<math>\\det(A) = \\frac{\\prod\\operatorname{diag}(B)}{d}.</math>\n\nComputationally, for an {{math|''n'' × ''n''}} matrix, this method needs only {{math|[[O notation|O(''n''<sup>3</sup>)]]}} arithmetic operations, while solving by elementary methods requires {{math|O(2<sup>''n''</sup>)}} or {{math|O(''n''!)}} operations. Even on the fastest computers, the elementary methods are impractical for {{math|''n''}} above 20.\n\n=== Finding the inverse of a matrix ===\n{{see also|Invertible matrix}}\nA variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists.  If {{math|''A''}} is an {{math|''n'' × ''n''}} square matrix, then one can use row reduction to compute its [[invertible matrix|inverse matrix]], if it exists. First, the {{math|''n'' × ''n''}} [[identity matrix]] is augmented to the right of {{math|''A''}}, forming an {{math|''n'' × 2''n''}} [[block matrix]] {{math|[''A'' {{!}} ''I'']}}. Now through application of elementary row operations, find the reduced echelon form of this {{math|''n'' × 2''n''}} matrix. The matrix {{math|''A''}} is invertible if and only if the left block can be reduced to the identity matrix {{math|''I''}}; in this case the right block of the final matrix is {{math|''A''<sup>−1</sup>}}. If the algorithm is unable to reduce the left block to {{math|''I''}}, then {{math|''A''}} is not invertible.\n\nFor example, consider the following matrix:\n: <math>A =\n \\begin{bmatrix}\n  2 & -1 &  0 \\\\\n -1 &  2 & -1 \\\\\n  0 & -1 &  2\n \\end{bmatrix}.\n</math>\n\nTo find the inverse of this matrix, one takes the following matrix augmented by the identity and row-reduces it as a 3&nbsp;×&nbsp;6 matrix:\n: <math>[ A | I ] = \n \\left[\\begin{array}{rrr|rrr}\n   2 & -1 &  0 & 1 & 0 & 0 \\\\\n  -1 &  2 & -1 & 0 & 1 & 0 \\\\\n   0 & -1 &  2 & 0 & 0 & 1\n \\end{array}\\right].\n</math>\n\nBy performing row operations, one can check that the reduced row echelon form of this augmented matrix is\n: <math>[ I | B ] = \n \\left[\\begin{array}{rrr|rrr}\n  1 & 0 & 0 & \\frac34 & \\frac12 & \\frac14 \\\\\n  0 & 1 & 0 & \\frac12 &      1  & \\frac12 \\\\\n  0 & 0 & 1 & \\frac14 & \\frac12 & \\frac34\n \\end{array}\\right].\n</math>\n\nOne can think of each row operation as the left product by an [[elementary matrix]]. Denoting by {{math|''B''}} the product of these elementary matrices, we showed, on the left, that {{math|''BA'' {{=}} ''I''}}, and therefore, {{math|''B'' {{=}} ''A''<sup>−1</sup>}}. On the right, we kept a record of {{math|''BI'' {{=}} ''B''}}, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.\n\n=== Computing ranks and bases ===\nThe Gaussian elimination algorithm can be applied to any {{math|''m'' × ''n''}} matrix {{mvar|A}}. In this way, for example, some 6&nbsp;×&nbsp;9 matrices can be transformed to a matrix that has a row echelon form like\n:<math> T=\n\\begin{bmatrix}\na & * & * & *& * & * & * & * & * \\\\\n0 & 0 & b & * & * & * & * & * & * \\\\\n0 & 0 & 0 & c & * & * & * & * & * \\\\\n0 & 0 & 0 & 0 & 0 & 0 & d & * & * \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & e \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix},\n</math>\nwhere the stars are arbitrary entries, and {{math|''a'', ''b'', ''c'', ''d'', ''e''}} are nonzero entries. This echelon matrix {{mvar|T}} contains a wealth of information about {{mvar|A}}: the [[rank of a matrix|rank]] of {{mvar|A}} is 5, since there are 5 nonzero rows in {{mvar|T}}; the [[vector space]] spanned by the columns of {{mvar|A}} has a basis consisting of its columns 1, 3, 4, 7 and 9 (the columns with {{math|''a'', ''b'', ''c'', ''d'', ''e''}} in {{mvar|T}}), and the stars show how the other columns of {{mvar|A}} can be written as linear combinations of the basis columns. This is a consequence of the distributivity of the [[dot product]] in the expression of a linear map [[Linear map#Matrices|as a matrix]].\n\nAll of this applies also to the reduced row echelon form, which is a particular row echelon format.\n\n== Computational efficiency ==\nThe number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of {{math|''n''}} equations for {{math|''n''}} unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires {{math|''n''(''n'' + 1)/2}} divisions, {{math|(2''n''<sup>3</sup> + 3''n''<sup>2</sup> − 5''n'')/6}} multiplications, and {{math|(2''n''<sup>3</sup> + 3''n''<sup>2</sup> − 5''n'')/6}} subtractions,<ref>{{harvtxt|Farebrother|1988}}, p. 12.</ref> for a total of approximately {{math|2''n''<sup>3</sup>/3}} operations. Thus it has [[Arithmetic#Arithmetic operations|arithmetic]] complexity of {{math|O(''n''<sup>3</sup>)}}; see [[Big O notation]]. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by [[floating-point number]]s or when they belong to a [[finite field]]. If the coefficients are [[integer]]s or [[rational number]]s exactly represented, the intermediate entries can grow exponentially large, so the [[bit complexity]] is exponential.<ref>{{Cite conference\n  | first1 = Xin Gui\n  | last1 = Fang\n  | first2 = George\n  | last2 = Havas\n  | title = On the worst-case complexity of integer Gaussian elimination\n  | booktitle = Proceedings of the 1997 international symposium on Symbolic and algebraic computation\n  | conference = ISSAC '97\n  | pages = 28–31\n  | publisher = ACM\n  | year = 1997\n  | location = Kihei, Maui, Hawaii, United States\n  | url = http://itee.uq.edu.au/~havas/fh97.pdf\n  | doi = 10.1145/258726.258740\n  | isbn = 0-89791-875-4}}</ref>\nHowever, there is a variant of Gaussian elimination, called the [[Bareiss algorithm]], that avoids this exponential growth of the intermediate entries and, with the same arithmetic complexity of {{math|O(''n''<sup>3</sup>)}}, has a bit complexity of {{math|O(''n''<sup>5</sup>)}}.\n\nThis algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using [[iterative method]]s. Specific methods exist for systems whose coefficients follow a regular pattern (see [[system of linear equations]]).\n\nTo put an {{math|''n'' × ''n''}} matrix into reduced echelon form by row operations, one needs {{math|''n''<sup>3</sup>}} arithmetic operations, which is approximately 50% more computation steps.<ref>J. B. Fraleigh and R. A. Beauregard, Linear Algebra. Addison-Wesley Publishing Company, 1995, Chapter 10.</ref>\n\nOne possible problem is [[numerical stability|numerical instability]], caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row-reduce the matrix, one would need to divide by that number. This means that any error existed for the number that was close to zero would be amplified. Gaussian elimination is numerically stable for [[diagonally dominant]] or [[Positive-definite matrix|positive-definite]] matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using [[Pivot element#Partial and complete pivoting|partial pivoting]], even though there are examples of stable matrices for which it is unstable.<ref>{{harvtxt|Golub|Van Loan|1996}}, §3.4.6.</ref>\n\n=== Generalizations ===\n\nGaussian elimination can be performed over any [[field (mathematics)|field]], not just the real numbers.\n\n[[Buchberger's algorithm]] is a generalization of Gaussian elimination to [[systems of polynomial equations]]. This generalization depends heavily on the notion of a [[monomial order]]. The choice of an ordering on the variables is already implicit in Gaussian elimination, manifesting as the choice to work from left to right when selecting pivot positions.\n\nGaussian elimination does not generalize in any way{{cn|reason=no generalization is known or a generalization may not exist?|date=January 2019}} to higher-order [[tensors]] (matrices are [[Array data structure|array]] representations of order-2 tensors); even computing the rank of a tensor of order greater than 2 is [[NP-hard]].<ref>{{cite arXiv |last1=Hillar |first1=Christopher |last2=Lim |first2=Lek-Heng |date=2009-11-07 |eprint=0911.1393 |title=Most tensor problems are NP-hard |class=cs.CC}}</ref>\n\n== Pseudocode ==\n{{Unreferenced section|date=March 2018}}\nAs explained above, Gaussian elimination transforms a given {{math|''m'' × ''n''}} matrix {{mvar|A}} into a matrix in [[row-echelon form]].\n\nIn the following [[pseudocode]], <code>A[i, j]</code> denotes the entry of the matrix {{mvar|A}} in row {{mvar|i}} and column {{mvar|j}} with the indices starting from&nbsp;1. The transformation is performed ''in place'', meaning that the original matrix is lost for being eventually replaced by its row-echelon form.\n\n  h := 1 /* ''Initialization of the pivot row'' */\n  k := 1 /* ''Initialization of the pivot column'' */\n  '''while''' h ≤ m '''and''' k ≤ n\n    /* ''Find the k-th pivot:'' */\n    i_max := [[argmax]] (i = h ... m, abs(A[i, k]))\n    '''if''' A[i_max, k] = 0\n      /* ''No pivot in this column, pass to next column'' */\n      k := k+1\n    '''else'''\n       '''swap rows'''(h, i_max)\n       /* ''Do for all rows below pivot:'' */\n       '''for''' i = h + 1 ... m:\n          f := A[i, k] / A[h, k]\n          /* ''Fill with zeros the lower part of pivot column:'' */\n          A[i, k]  := 0\n          /* ''Do for all remaining elements in current row:'' */\n          '''for''' j = k + 1 ... n:\n             A[i, j] := A[i, j] - A[h, j] * f\n       /* ''Increase pivot row and column'' */\n       h := h+1 \n       k := k+1\n\nThis algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest [[absolute value]]. Such a ''partial pivoting'' may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the [[numerical stability]] of the algorithm, when [[floating point]] is used for representing numbers.\n\nUpon completion of this procedure the matrix will be in [[row echelon form]] and the corresponding system may be solved by back substitution.\n\n==See also==\n*[[Fangcheng (mathematics)]]\n\n== Notes ==\n<references/>\n\n== References ==\n{{wikibooks\n|1= Linear Algebra\n|2= Gauss' Method\n|3= Gaussian elimination}}\n\n* {{Citation | last1=Atkinson | first1=Kendall A. | title=An Introduction to Numerical Analysis | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0471624899| year=1989}}.\n* {{Citation | last1=Bolch | first1=Gunter | last2=Greiner | first2=Stefan | last3=de Meer | first3=Hermann | last4=Trivedi | first4=Kishor S. | title=Queueing Networks and Markov Chains: Modeling and Performance Evaluation with Computer Science Applications | publisher=[[Wiley-Interscience]] | edition=2nd | isbn=978-0-471-79156-0 | year=2006}}.\n* {{Citation | last1=Calinger | first1=Ronald | title=A Contextual History of Mathematics | publisher=[[Prentice Hall]] | isbn=978-0-02-318285-3 | year=1999}}.\n* {{Citation | last1=Farebrother | first1=R.W. | title=Linear Least Squares Computations | publisher=Marcel Dekker | series=STATISTICS: Textbooks and Monographs | isbn=978-0-8247-7661-9 | year=1988}}.\n* {{Citation | last1=Lauritzen | first1=Niels | title=Undergraduate Convexity: From Fourier and Motzkin to Kuhn and Tucker  }}.\n* {{Citation | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Van Loan | first2=Charles F. | author2-link=Charles F. Van Loan | title=Matrix Computations | publisher=Johns Hopkins | edition=3rd | isbn=978-0-8018-5414-9 | year=1996}}.\n* {{Citation\n  | last = Grcar\n  | first = Joseph F.\n  | title = How ordinary elimination became Gaussian elimination\n  | journal = Historia Mathematica\n  | year = 2011a\n  | pages = 163–218\n  | doi = 10.1016/j.hm.2010.06.003\n  | arxiv = 0907.2397\n  | volume = 38\n  | issue = 2 }}\n* {{Citation\n  | last = Grcar\n  | first = Joseph F.\n  | title = Mathematicians of Gaussian elimination\n  | journal = Notices of the American Mathematical Society\n  | year = 2011b\n  | pages = 782–792\n  | url = http://www.ams.org/notices/201106/rtx110600782p.pdf\n  | volume = 58\n  | issue = 6 }}\n* {{Citation | last1=Higham | first1=Nicholas | author1-link=Nicholas Higham | title=Accuracy and Stability of Numerical Algorithms | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | edition=2nd | isbn=978-0-89871-521-7 | year=2002}}.\n* {{Citation | last1=Katz | first1=Victor J. | title=A History of Mathematics, Brief Version | publisher=[[Addison-Wesley]] | isbn=978-0-321-16193-2 | year=2004}}.\n* {{Cite web | last1=Kaw | first1=Autar | last2=Kalu | first2=Egwu | year=2010 | title=Numerical Methods with Applications: Chapter 04.06 Gaussian Elimination | edition=1st | publisher=University of South Florida | url=http://mathforcollege.com/nm/mws/gen/04sle/mws_gen_sle_txt_gaussian.pdf }} \n* {{Citation | last1=Lipson | first1=Marc | last2=Lipschutz | first2=Seymour | title=Schaum's outline of theory and problems of linear algebra | publisher=[[McGraw-Hill]] | location=New York | isbn=978-0-07-136200-9 | year=2001 | pages=69–80}}.\n*{{Citation|last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 2.2|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=46}}\n\n{{linear algebra}}\n\n{{DEFAULTSORT:Gaussian Elimination}}\n[[Category:Numerical linear algebra]]\n[[Category:Articles with example pseudocode]]\n[[Category:Exchange algorithms]]"
    },
    {
      "title": "Pivot element",
      "url": "https://en.wikipedia.org/wiki/Pivot_element",
      "text": "{{short description|Non-zero element of a matrix selected by an algorithm}}\n{{hatnote|This article is about pivots in matrices. For the pivot in the quicksort algorithm, see [[quicksort]].}}\nThe '''pivot''' or '''pivot element''' is the element of a [[Matrix (mathematics)|matrix]], or an [[array data structure|array]], which is selected first by an [[algorithm]] (e.g. [[Gaussian elimination]], [[simplex algorithm]], etc.), to do certain calculations. In the case of matrix algorithms, a pivot entry is usually required to be at least distinct from zero, and often distant from it; in this case finding this element is called '''pivoting'''.  Pivoting may be followed by an interchange of rows or columns to bring the pivot to a fixed position and allow the algorithm to proceed successfully, and possibly to reduce round-off error. It is often used for verifying [[row echelon form]].\n\nPivoting might be thought of as swapping or sorting rows or columns in a matrix, and thus it can be represented as [[matrix multiplication|multiplication]] by [[permutation matrix|permutation matrices]]. However, algorithms rarely move the matrix elements because this would cost too much time; instead, they just keep track of the permutations.\n\nOverall, pivoting adds more operations to the computational cost of an algorithm.  These additional operations are sometimes necessary for the algorithm to work at all.  Other times these additional operations are worthwhile because they add [[numerical stability]] to the final result.\n\n==Examples of systems that require pivoting==\nIn the case of Gaussian elimination, the algorithm requires that pivot elements not be zero.\nInterchanging rows or columns in the case of a zero pivot element is necessary.  The system below requires the interchange of rows 2 and 3 to perform elimination.\n\n:<math>\n\\left[ \\begin{array}{ccc|c}\n1 & -1 & 2 & 8 \\\\\n0 & 0 & -1 & -11 \\\\\n0 & 2 & -1 & -3\n\\end{array} \\right]\n</math>\n\nThe system that results from pivoting is as follows and will allow the elimination algorithm and backwards substitution to output the solution to the system.\n\n:<math>\n\\left[ \\begin{array}{ccc|c}\n1 & -1 & 2 & 8 \\\\\n0 & 2 & -1 & -3 \\\\\n0 & 0 & -1 & -11\n\\end{array} \\right]\n</math>\nFurthermore, in Gaussian elimination it is generally desirable to choose a pivot element with large [[absolute value]]. This improves the [[numerical stability]]. The following system is dramatically affected by round-off error when Gaussian elimination and backwards substitution are performed.\n\n:<math>\n\\left[ \\begin{array}{cc|c}\n0.00300 & 59.14 & 59.17 \\\\\n5.291 & -6.130 & 46.78 \\\\\n\\end{array} \\right]\n</math>\nThis system has the exact solution of x<sub>1</sub> = 10.00 and x<sub>2</sub> = 1.000, but when the elimination algorithm and backwards substitution are performed using four-digit arithmetic, the small value of a<sub>11</sub> causes small round-off errors to be propagated.  The algorithm without pivoting yields the approximation of x<sub>1</sub> ≈ 9873.3 and x<sub>2</sub> ≈ 4.  In this case it is desirable that we interchange the two rows so that a<sub>21</sub> is in the pivot position\n\n:<math>\n\\left[ \\begin{array}{cc|c}\n5.291 & -6.130 & 46.78 \\\\\n0.00300 & 59.14 & 59.17 \\\\\n\\end{array} \\right].\n</math>\n\nConsidering this system, the elimination algorithm and backwards substitution using four-digit arithmetic yield the correct values x<sub>1</sub> = 10.00 and x<sub>2</sub> = 1.000.\n\n==Partial and complete pivoting==\nIn '''partial pivoting''', the algorithm selects the entry with largest absolute value from the column of the matrix that is currently being considered as the pivot element. Partial pivoting is generally sufficient to adequately reduce round-off error.  However, for certain systems and algorithms, '''complete pivoting''' (or maximal pivoting) may be required for acceptable accuracy.  Complete pivoting  interchanges both rows and columns in order to use the largest (by absolute value) element in the matrix as the pivot. Complete pivoting is usually not necessary to ensure numerical stability and, due to the additional cost of searching for the maximal element, the improvement in numerical stability that it provides is typically outweighed by its reduced efficiency for all but the smallest matrices. Hence, it is rarely used.<ref>[http://www-math.mit.edu/~edelman/publications/complete_pivoting.pdf Edelman, Alan, 1992. ''The Complete Pivoting Conjecture for Gaussian Elimination is False.'' Mathematica Journal 2, no.&nbsp;2: 58-61.]</ref>\n\n==Scaled pivoting==\nA variation of the partial pivoting strategy is scaled pivoting. In this approach, the algorithm selects as the pivot element the entry that is largest relative to the entries in its row. This strategy is desirable when entries' large differences in magnitude lead to the propagation of round-off error. Scaled pivoting should be used in a system like the one below where a row's entries vary greatly in magnitude. In the example below, it would be desirable to interchange the two rows because the current pivot element 30 is larger than 5.291 but it is relatively small compared with the other entries in its row.  Without row interchange in this case, rounding errors will be propagated as in the previous example.\n\n:<math>\n\\left[ \\begin{array}{cc|c}\n30 & 591400 & 591700 \\\\\n5.291 & -6.130 & 46.78 \\\\\n\\end{array} \\right]\n</math>\n\n== Pivot position ==\nA pivot position in a matrix, A, is a position in the matrix that corresponds to a row–leading 1 in the [[reduced row echelon form]] of A. Since the reduced row echelon form of A is unique, the pivot positions are uniquely determined and do not depend on whether or not row interchanges are performed in the reduction process. Also, the pivot of a row must appear to the right of the pivot in the above row in [[row echelon form]].\n\n==References==\n{{PlanetMath attribution|id=1243|title=Pivoting}}\n<references/>\n* R. L. Burden, J. D. Faires, ''Numerical Analysis'', 8th edition, Thomson Brooks/Cole, 2005. {{isbn|0-534-39200-8}}\n* G. H. Golub, C. F. Loan, ''Matrix Computations'', 3rd edition, Johns Hopkins, 1996. {{isbn|0-8018-5414-8}}.\n* {{cite journal|first1=Komei|last1=Fukuda|<!-- authorlink1=Komei Fukuda -->|first2=Tamás|last2=Terlaky|<!-- authorlink2=Tamás Terlaky -->|title=Criss-cross methods: A fresh view on pivot algorithms |doi=10.1007/BF02614325|journal=Mathematical Programming, Series B|volume=79\n|issue=1–3|pages=369–395|series=Papers from the&nbsp;16th International Symposium on Mathematical Programming held in Lausanne,&nbsp;1997|editors=Thomas&nbsp;M. Liebling and Dominique de&nbsp;Werra|year=1997|mr=1464775|ref=harv|id=[http://www.cas.mcmaster.ca/~terlaky/files/crisscross.ps Postscript preprint]|citeseerx=10.1.1.36.9373}}\n* {{cite journal|last1=Terlaky|first1=Tamás|<!-- authorlink1=Tamás Terlaky -->|last2=Zhang|first2=Shu&nbsp;Zhong|title=Pivot rules for linear programming: A Survey on recent theoretical developments|series=Degeneracy in optimization problems|journal=Annals of Operations Research|volume=46–47|year=1993|issue=1|pages=203–233|doi=10.1007/BF02096264|mr=1260019| citeseerx = 10.1.1.36.7658 |issn=0254-5330|ref=harv}}\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Exchange algorithms]]\n\n[[sv:Pivotelement]]"
    },
    {
      "title": "Revised simplex method",
      "url": "https://en.wikipedia.org/wiki/Revised_simplex_method",
      "text": "In [[mathematical optimization]], the '''revised simplex method''' is a variant of [[George Dantzig]]'s [[simplex method]] for [[linear programming]].\n\nThe revised simplex method is mathematically equivalent to the standard simplex method but differs in implementation. Instead of maintaining a tableau which explicitly represents the constraints adjusted to a set of basic variables, it maintains a representation of a [[Basis (linear algebra)|basis]] of the [[Matrix (mathematics)|matrix]] representing the constraints. The matrix-oriented approach allows for greater computational efficiency by enabling sparse matrix operations.{{sfn|Morgan|1997|loc=§2}}\n\n==Problem formulation==\nFor the rest of the discussion, it is assumed that a linear programming problem has been converted into the following standard form:\n:<math>\n\\begin{array}{rl}\n\\text{minimize} & \\boldsymbol{c}^{\\mathrm{T}} \\boldsymbol{x} \\\\\n\\text{subject to} & \\boldsymbol{Ax} = \\boldsymbol{b}, \\boldsymbol{x} \\ge \\boldsymbol{0}\n\\end{array}\n</math>\nwhere {{math|'''''A''''' ∈ '''R'''<sup>''m''×''n''</sup>}}. Without loss of generality, it is assumed that the constraint matrix {{math|'''''A'''''}} has full row rank and that the problem is feasible, i.e., there is at least one {{math|'''''x''''' ≥ '''0'''}} such that {{math|'''''Ax''''' {{=}} '''''b'''''}}. If {{math|'''''A'''''}} is rank-deficient, either there are redundant constraints, or the problem is infeasible. Both situations can be handled by a presolve step.\n\n==Algorithmic description==\n===Optimality conditions===\nFor linear programming, the [[Karush–Kuhn–Tucker conditions]] are both [[necessary and sufficient]] for optimality. The KKT conditions of a linear programming problem in the standard form is\n\n:<math>\n\\begin{align}\n\\boldsymbol{Ax} & = \\boldsymbol{b}, \\\\\n\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{\\lambda} + \\boldsymbol{s} & = \\boldsymbol{c}, \\\\\n\\boldsymbol{x} & \\ge \\boldsymbol{0}, \\\\\n\\boldsymbol{s} & \\ge \\boldsymbol{0}, \\\\\n\\boldsymbol{s}^{\\mathrm{T}} \\boldsymbol{x} & = 0\n\\end{align}\n</math>\n\nwhere {{math|'''''λ'''''}} and {{math|'''''s'''''}} are the [[Lagrange multiplier]]s associated with the constraints {{math|'''''Ax''''' {{=}} '''''b'''''}} and {{math|'''''x''''' ≥ '''0'''}}, respectively.{{sfn|Nocedal|Wright|2006|p=358|loc=Eq.&nbsp;13.4}} The last condition, which is equivalent to {{math|''s<sub>i</sub>x<sub>i</sub>'' {{=}} 0}} for all {{math|1 < ''i'' < ''n''}}, is called the ''complementary slackness condition''.\n\nBy what is sometimes known as the ''fundamental theorem of linear programming'',  a vertex {{math|'''''x'''''}} of the feasible polytope can be identified by being a basis {{math|'''''B'''''}} of {{math|'''''A'''''}} chosen from the latter's columns.{{efn|The same theorem also states that the feasible polytope has at least one vertex and that there is at least one vertex which is optimal.{{sfn|Nocedal|Wright|2006|p=363|loc=Theorem&nbsp;13.2}}}} Since {{math|'''''A'''''}} has full rank, {{math|'''''B'''''}} is nonsingular. Without loss of generality, assume that {{math|'''''A''''' {{=}} &#91;'''''B'''''&ensp;'''''N'''''&#93;}}. Then {{math|'''''x'''''}} is given by\n\n:<math>\n\\boldsymbol{x} =\n\\begin{bmatrix}\n\\boldsymbol{x_B} \\\\\n\\boldsymbol{x_N}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\boldsymbol{B}^{-1} \\boldsymbol{b} \\\\\n\\boldsymbol{0}\n\\end{bmatrix}\n</math>\n\nwhere {{math|'''''x<sub>B</sub>''''' ≥ '''0'''}}. Partition {{math|'''''c'''''}} and {{math|'''''s'''''}} accordingly into\n\n:<math>\n\\begin{align}\n\\boldsymbol{c} & =\n\\begin{bmatrix}\n\\boldsymbol{c_B} \\\\\n\\boldsymbol{c_N}\n\\end{bmatrix}, \\\\\n\\boldsymbol{s} & =\n\\begin{bmatrix}\n\\boldsymbol{s_B} \\\\\n\\boldsymbol{s_N}\n\\end{bmatrix}.\n\\end{align}\n</math>\n\nTo satisfy the complementary slackness condition, let {{math|'''''s<sub>B</sub>''''' {{=}} '''0'''}}. It follows that\n\n:<math>\n\\begin{align}\n\\boldsymbol{B}^{\\mathrm{T}} \\boldsymbol{\\lambda} & = \\boldsymbol{c_B}, \\\\\n\\boldsymbol{N}^{\\mathrm{T}} \\boldsymbol{\\lambda} + \\boldsymbol{s_N} & = \\boldsymbol{c_N},\n\\end{align}\n</math>\n\nwhich implies that\n\n:<math>\n\\begin{align}\n\\boldsymbol{\\lambda} & = (\\boldsymbol{B}^{\\mathrm{T}})^{-1} \\boldsymbol{c_B}, \\\\\n\\boldsymbol{s_N} & = \\boldsymbol{c_N} - \\boldsymbol{N}^{\\mathrm{T}} \\boldsymbol{\\lambda}.\n\\end{align}\n</math>\n\nIf {{math|'''''s<sub>N</sub>''''' ≥ '''0'''}} at this point, the KKT conditions are satisfied, and thus {{math|'''''x'''''}} is optimal.\n\n===Pivot operation===\nIf the KKT conditions are violated, a ''pivot operation'' consisting of introducing a column of {{math|'''''N'''''}} into the basis at the expense of an existing column in {{math|'''''B'''''}} is performed. In the absence of [[Degeneracy (mathematics)|degeneracy]], a pivot operation always results in a strict decrease in {{math|'''''c'''''<sup>T</sup>'''''x'''''}}. Therefore, if the problem is bounded, the revised simplex method must terminate at an optimal vertex after repeated pivot operations because there are only a finite number of vertices.{{sfn|Nocedal|Wright|2006|p=370|loc=Theorem&nbsp;13.4}}\n\nSelect an index {{math|''m'' < ''q'' ≤ ''n''}} such that {{math|''s<sub>q</sub>'' < 0}} as the ''entering index''. The corresponding column of {{math|'''''A'''''}}, {{math|'''''A'''<sub>q</sub>''}}, will be moved into the basis, and {{math|''x<sub>q</sub>''}} will be allowed to increase from zero. It can be shown that\n\n:<math>\\frac{\\partial (\\boldsymbol{c}^{\\mathrm{T}} \\boldsymbol{x})}{\\partial x_q} = s_q,</math>\n\ni.e., every unit increase in {{math|''x<sub>q</sub>''}} results in a decrease by {{math|−''s<sub>q</sub>''}} in {{math|'''''c'''''<sup>T</sup>'''''x'''''}}.{{sfn|Nocedal|Wright|2006|p=369|loc=Eq.&nbsp;13.24}} Since\n\n:<math>\\boldsymbol{B x_B} + \\boldsymbol{A}_q x_q = \\boldsymbol{b},</math>\n\n{{math|'''''x<sub>B</sub>'''''}} must be correspondingly decreased by {{math|Δ'''''x<sub>B</sub>''''' {{=}} '''''B'''''<sup>−1</sup>'''''A'''<sub>q</sub>x<sub>q</sub>''}} subject to {{math|'''''x<sub>B</sub>''''' − Δ'''''x<sub>B</sub>''''' ≥ '''0'''}}. Let {{math|'''''d''''' {{=}} '''''B'''''<sup>−1</sup>'''''A'''<sub>q</sub>''}}. If {{math|'''''d''''' ≤ '''0'''}}, no matter how much {{math|''x<sub>q</sub>''}} is increased, {{math|'''''x<sub>B</sub>''''' − Δ'''''x<sub>B</sub>'''''}} will stay nonnegative. Hence, {{math|'''''c'''''<sup>T</sup>'''''x'''''}} can be arbitrarily decreased, and thus the problem is unbounded. Otherwise, select an index {{math|''p'' {{=}} argmin<sub>1≤''i''≤''m''</sub> {{(}}''x<sub>i</sub>''/''d<sub>i</sub>'' {{!}} ''d<sub>i</sub>'' > 0{{)}}}} as the ''leaving index''. This choice effectively increases {{math|''x<sub>q</sub>''}} from zero until {{math|''x<sub>p</sub>''}} is reduced to zero while maintaining feasibility. The pivot operation concludes with replacing {{math|'''''A'''<sub>p</sub>''}} with {{math|'''''A'''<sub>q</sub>''}} in the basis.\n\n==Numerical example==\n{{seealso|Simplex method#Example}}\nConsider a linear program where\n\n:<math>\n\\begin{align}\n\\boldsymbol{c} & =\n\\begin{bmatrix}\n-2 & -3 & -4 & 0 & 0\n\\end{bmatrix}^{\\mathrm{T}}, \\\\\n\\boldsymbol{A} & =\n\\begin{bmatrix}\n3 & 2 & 1 & 1 & 0 \\\\\n2 & 5 & 3 & 0 & 1\n\\end{bmatrix}, \\\\\n\\boldsymbol{b} & =\n\\begin{bmatrix}\n10 \\\\\n15\n\\end{bmatrix}.\n\\end{align}\n</math>\n\nLet\n\n:<math>\n\\begin{align}\n\\boldsymbol{B} & =\n\\begin{bmatrix}\n\\boldsymbol{A}_4 & \\boldsymbol{A}_5\n\\end{bmatrix}, \\\\\n\\boldsymbol{N} & =\n\\begin{bmatrix}\n\\boldsymbol{A}_1 & \\boldsymbol{A}_2 & \\boldsymbol{A}_3\n\\end{bmatrix}\n\\end{align}\n</math>\n\ninitially, which corresponds to a feasible vertex {{math|'''''x''''' {{=}} &#91;0&ensp;0&ensp;0&ensp;10&ensp;15&#93;<sup>T</sup>}}. At this moment,\n\n:<math>\n\\begin{align}\n\\boldsymbol{\\lambda} & =\n\\begin{bmatrix}\n0 & 0\n\\end{bmatrix}^{\\mathrm{T}}, \\\\\n\\boldsymbol{s_N} & =\n\\begin{bmatrix}\n-2 & -3 & -4\n\\end{bmatrix}^{\\mathrm{T}}.\n\\end{align}\n</math>\n\nChoose {{math|''q'' {{=}} 3}} as the entering index. Then {{math|'''''d''''' {{=}} &#91;1&ensp;3&#93;<sup>T</sup>}}, which means a unit increase in {{math|''x''<sub>3</sub>}} results in {{math|''x''<sub>4</sub>}} and {{math|''x''<sub>5</sub>}} being decreased by {{math|1}} and {{math|3}}, respectively. Therefore, {{math|''x''<sub>3</sub>}} is increased to {{math|5}}, at which point {{math|''x''<sub>5</sub>}} is reduced to zero, and {{math|''p'' {{=}} 5}} becomes the leaving index.\n\nAfter the pivot operation,\n\n:<math>\n\\begin{align}\n\\boldsymbol{B} & =\n\\begin{bmatrix}\n\\boldsymbol{A}_3 & \\boldsymbol{A}_4\n\\end{bmatrix}, \\\\\n\\boldsymbol{N} & =\n\\begin{bmatrix}\n\\boldsymbol{A}_1 & \\boldsymbol{A}_2 & \\boldsymbol{A}_5\n\\end{bmatrix}.\n\\end{align}\n</math>\n\nCorrespondingly,\n\n:<math>\n\\begin{align}\n\\boldsymbol{x} & =\n\\begin{bmatrix}\n0 & 0 & 5 & 5 & 0\n\\end{bmatrix}^{\\mathrm{T}}, \\\\\n\\boldsymbol{\\lambda} & =\n\\begin{bmatrix}\n0 & -4/3\n\\end{bmatrix}^{\\mathrm{T}}, \\\\\n\\boldsymbol{s_N} & =\n\\begin{bmatrix}\n2/3 & 11/3 & 4/3\n\\end{bmatrix}^{\\mathrm{T}}.\n\\end{align}\n</math>\n\nA positive {{math|'''''s<sub>N</sub>'''''}} indicates that {{math|'''''x'''''}} is now optimal.\n\n==Practical issues==\n===Degeneracy===\n{{seealso|Simplex method#Degeneracy: stalling and cycling}}\nBecause the revised simplex method is mathematically equivalent to the simplex method, it also suffers from degeneracy, where a pivot operation does not result in a decrease in {{math|'''''c'''''<sup>T</sup>'''''x'''''}}, and a chain of pivot operations causes the basis to cycle. A perturbation or lexicographic strategy can be used to prevent cycling and guarantee termination.{{sfn|Nocedal|Wright|2006|p=381|loc=§13.5}}\n\n===Basis representation===\nTwo types of [[System of linear equations|linear systems]] involving {{math|'''''B'''''}} are present in the revised simplex method:\n\n:<math>\n\\begin{align}\n\\boldsymbol{B z} & = \\boldsymbol{y}, \\\\\n\\boldsymbol{B}^{\\mathrm{T}} \\boldsymbol{z} & = \\boldsymbol{y}.\n\\end{align}\n</math>\n\nInstead of refactorizing {{math|'''''B'''''}}, usually an [[LU factorization]] is directly updated after each pivot operation, for which purpose there exist several strategies such as the Forrest−Tomlin and Bartels−Golub methods. However, the amount of data representing the updates as well as numerical errors builds up over time and makes periodic refactorization necessary.{{sfn|Morgan|1997|loc=§2}}{{sfn|Nocedal|Wright|2006|p=372|loc=§13.4}}\n\n==Notes and references==\n===Notes ===\n{{notelist}}\n\n===References===\n{{reflist|2}}\n\n===Bibliography===\n{{refbegin}}\n* {{cite thesis\n|last=Morgan\n|first=S. S.\n|degree=MSc\n|title=A Comparison of Simplex Method Algorithms\n|publisher=[[University of Florida]]\n|year=1997\n|url=http://www.cise.ufl.edu/research/sparse/Morgan/index.htm\n|archiveurl=https://web.archive.org/web/20110807134509/http://www.cise.ufl.edu/research/sparse/Morgan/index.htm\n|archivedate=7 August 2011\n|ref=harv}}\n* {{cite book\n|last1=Nocedal\n|first1=J.\n|last2=Wright\n|first2=S. J.\n|editor1-last=Mikosch\n|editor1-first=T. V.\n|editor2-last=Resnick\n|editor2-first=S. I.\n|editor3-last=Robinson\n|editor3-first=S. M.\n|title=Numerical Optimization\n|edition=2nd\n|year=2006\n|series=Springer Series in Operations Research and Financial Engineering\n|publisher=[[Springer Science+Business Media|Springer]]\n|location=New York, NY, USA\n|isbn=978-0-387-30303-1\n|url=https://www.springer.com/mathematics/book/978-0-387-30303-1\n|ref=harv}}\n{{refend}}\n\n{{Optimization algorithms|convex}}\n{{Mathematical programming}}\n\n[[Category:Exchange algorithms]]\n[[Category:Linear programming]]"
    },
    {
      "title": "Coefficient of determination",
      "url": "https://en.wikipedia.org/wiki/Coefficient_of_determination",
      "text": "{{distinguish|Coefficient of variation|Coefficient of correlation}}\n[[File:Okuns law quarterly differences.svg|300px|thumb|[[Ordinary least squares]] regression of [[Okun's law]]. Since the regression line does not miss any of the points by very much, the ''R''<sup>2</sup> of the regression is relatively high.]]\n\n[[File:Thiel-Sen estimator.svg|thumb|Comparison of the [[Theil–Sen estimator]] (black) and [[simple linear regression]] (blue) for a set of points with [[outlier]]s. Because of the many outliers, neither of the regression lines fits the data well, as measured by the fact that neither gives a very high ''R''<sup>2</sup>.]]\n\nIn [[statistics]], the '''coefficient of determination''', denoted ''R''<sup>2</sup> or ''r''<sup>2</sup> and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\nIt is a [[statistic]] used in the context of [[statistical model]]s whose main purpose is either the [[Prediction#Statistics|prediction]] of future outcomes or the testing of [[hypotheses]], on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.<ref>{{cite book|last=Steel|first=R. G. D.|last2=Torrie|first2=J. H.|year=1960|title=Principles and Procedures of Statistics with Special Reference to the Biological Sciences|publisher=[[McGraw Hill]]}}</ref><ref>{{cite book |last=Glantz |first=Stanton A. |last2=Slinker |first2=B. K. |year=1990 |title=Primer of Applied Regression and Analysis of Variance |publisher=McGraw-Hill |isbn=978-0-07-023407-9}}</ref><ref>{{cite book |last=Draper |first=N. R. |last2=Smith |first2=H. |year=1998 |title=Applied Regression Analysis |publisher=Wiley-Interscience |isbn=978-0-471-17082-2}}</ref>\n\nThere are several definitions of ''R''<sup>2</sup> that are only sometimes equivalent. One class of such cases includes that of [[simple linear regression]] where ''r''<sup>2</sup> is used instead of ''R''<sup>2</sup>. When an [[regression intercept|intercept]] is included, then ''r''<sup>2</sup> is simply the square of the sample [[Pearson product-moment correlation coefficient|correlation coefficient]] (i.e., ''r'') between the observed outcomes and the observed predictor values.<ref name=Devore>{{cite book |last1 = Devore|first1 = Jay L.|title = Probability and Statistics for Engineering and the Sciences| edition=8th |publisher = Cengage Learning |location = Boston, MA | year = 2011 |isbn =978-0-538-73352-6 |pages=508–510}}</ref> If additional [[regressor]]s are included, ''R''<sup>2</sup> is the square of the [[coefficient of multiple correlation]]. In both such cases, the coefficient of determination normally ranges from 0 to 1.\n\nThere are cases where the computational definition of ''R''<sup>2</sup> can yield negative values, depending on the definition used. This can arise when the predictions that are being compared to the corresponding outcomes have not been derived from a model-fitting procedure using those data. Even if a model-fitting procedure has been used, ''R''<sup>2</sup> may still be negative, for example when linear regression is conducted without including an intercept,<ref>{{cite book |last=Barten |first=Anton P. |authorlink=Anton Barten |chapter=The Coeffecient of Determination for Regression without a Constant Term |editor-first=Risto |editor-last=Heijmans |editor2-first=Heinz |editor2-last=Neudecker |title=The Practice of Econometrics |location=Dordrecht |publisher=Kluwer |year=1987 |isbn=90-247-3502-5 |pages=181–189 }}\n\n</ref> or when a non-linear function is used to fit the data.<ref>{{cite journal |doi=10.1016/S0304-4076(96)01818-0 |title=An R-squared measure of goodness of fit for some common nonlinear regression models |year=1997 |last1=Colin Cameron |first1=A. |last2=Windmeijer |first2=Frank A.G. |journal=Journal of Econometrics |volume=77 |issue=2 |pages=1790–2 }}</ref> In cases where negative values arise, the mean of the data provides a better fit to the outcomes than do the fitted function values, according to this particular criterion.<ref>{{cite web|last=Imdadullah|first=Muhammad|title=Coefficient of Determination|url=http://itfeature.com/correlation-and-regression-analysis/coefficient-of-determination|website=itfeature.com}}</ref> Since the most general definition of the coefficient of determination is also known as the [[Nash–Sutcliffe model efficiency coefficient]], this last notation is preferred in many fields, because denoting a goodness-of-fit indicator that can vary from -∞ to 1 (i.e., it can yield negative values) with a squared letter is confusing.\n\nWhen evaluating the goodness-of-fit of simulated (''Y''<sub>''pred''</sub>) vs. measured (''Y''<sub>''obs''</sub>) values, it is not appropriate to base this on the ''R''<sup>2</sup> of the linear regression (i.e., ''Y''<sub>''obs''</sub>= m·''Y''<sub>''pred''</sub> + b). The ''R''<sup>2</sup> quantifies the degree of any linear correlation between ''Y''<sub>''obs''</sub> and Y<sub>''pred''</sub>, while for the goodness-of-fit evaluation only one specific linear correlation should be taken into consideration: ''Y''<sub>''obs''</sub> = 1·''Y''<sub>''pred''</sub> + 0 (i.e., the 1:1 line).<ref>{{cite journal |doi=10.1029/1998WR900018 |title= Evaluating the use of \"goodness-of-fit\" measures in hydrologic and hydroclimatic model validation |year=1999 |last1= Legates |first1=D.R. |last2= McCabe |first2=G.J. |journal= Water Resour. Res. |volume=35 |issue=1 |pages=233–241 }}</ref><ref>{{cite journal |doi=10.1016/j.jhydrol.2012.12.004|title= Performance evaluation of hydrological models: statistical significance for reducing subjectivity in goodness-of-fit assessments |year=2013 |last1= Ritter |first1=A. |last2= Muñoz-Carpena |first2=R. |journal= Journal of Hydrology |volume=480 |issue=1 |pages=33–45 }}</ref> \n\n==Definitions==\n[[File:Coefficient of Determination.svg|thumb|400px|<math>R^2 = 1 - \\frac{\\color{blue}{SS_\\text{res}}}{\\color{red}{SS_\\text{tot}}}</math><br>\nThe better the linear regression (on the right) fits the data in comparison to the simple average (on the left graph), the closer the value of <math>R^2</math> is to 1. The areas of the blue squares represent the squared residuals with respect to the linear regression. The areas of the red squares represent the squared residuals with respect to the average value.]]\n\nA data set has ''n'' values marked ''y''<sub>1</sub>,...,''y''<sub>''n''</sub> (collectively known as ''y''<sub>''i''</sub> or as a vector ''y'' = [''y''<sub>1</sub>,...,''y''<sub>''n''</sub>]<sup>''T''</sup>), each associated with a fitted (or modeled, or predicted) value ''f''<sub>1</sub>,...,''f''<sub>''n''</sub> (known as ''f''<sub>''i''</sub>, or sometimes ''ŷ''<sub>''i''</sub>, as a vector ''f'').\n\nDefine the [[Residuals (statistics)|residuals]] as ''e''<sub>''i''</sub> = ''y''<sub>''i''</sub> − ''f''<sub>''i''</sub> (forming a vector ''e'').\n\nIf <math>\\bar{y}</math> is the mean of the observed data:\n\n:<math>\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i </math>\n\nthen the variability of the data set can be measured using three [[Mean squared error|sums of squares]] formulas:\n\n* The [[total sum of squares]] (proportional to the [[variance]] of the data):\n:: <math>SS_\\text{tot}=\\sum_i (y_i-\\bar{y})^2,</math>\n* The regression sum of squares, also called the [[explained sum of squares]]:\n:: <math>SS_\\text{reg}=\\sum_i (f_i -\\bar{y})^2,</math>\n* The sum of squares of residuals, also called the [[residual sum of squares]]:\n:: <math>SS_\\text{res}=\\sum_i (y_i - f_i)^2=\\sum_i e_i^2\\,</math>\n\nThe most general definition of the coefficient of determination is\n\n:<math>R^2 \\equiv 1 - {SS_{\\rm res}\\over SS_{\\rm tot}} \\,</math>\n\n===Relation to unexplained variance===\n{{main|Fraction of variance unexplained}}\nIn a general form, ''R''<sup>2</sup> can be seen to be related to the fraction of variance unexplained (FVU), since the second term compares the unexplained variance (variance of the model's errors) with the total variance (of the data):\n:<math>\\begin{align}\nR^2 = 1 - \\text{FVU}\n\\end{align}</math>\n\n===As explained variance===\nSuppose ''R''<sup>2</sup> = 0.49. This implies that 49% of the variability of the dependent variable has been accounted for, and the remaining 51% of the variability is still unaccounted for. \nIn some cases the [[total sum of squares]] equals the sum of the two other sums of squares defined above,\n\n:<math>SS_\\text{res}+SS_\\text{reg}=SS_\\text{tot}. \\,</math>\n\nSee [[Explained sum of squares#Partitioning in the general ordinary least squares model|Partitioning in the general OLS model]] for a derivation of this result for one case where the relation holds. When this relation does hold, the above definition of ''R''<sup>2</sup> is equivalent to\n\n:<math>R^2 = \\frac{SS_\\text{reg}}{SS_\\text{tot}} = \\frac{SS_\\text{reg}/n}{SS_\\text{tot}/n}</math>\n\nwhere ''n'' is the number of observations (cases) on the variables.\n\nIn this form ''R''<sup>2</sup> is expressed as the ratio of the [[explained variation|explained variance]] (variance of the model's predictions, which is ''SS''<sub>reg</sub> / ''n'') to the total variance (sample variance of the dependent variable, which is ''SS''<sub>tot</sub> / ''n'').\n\nThis partition of the sum of squares holds for instance when the model values ''ƒ''<sub>''i''</sub> have been obtained by [[linear regression]]. A milder [[sufficient condition]] reads as follows: The model has the form\n\n:<math>f_i=\\widehat\\alpha+\\widehat\\beta q_i \\,</math>\n\nwhere the ''q''<sub>''i''</sub> are arbitrary values that may or may not depend on ''i'' or on other free parameters (the common choice ''q''<sub>''i''</sub>&nbsp;=&nbsp;''x''<sub>''i''</sub> is just one special case), and the coefficient estimates <math>\\widehat\\alpha</math> and <math>\\widehat\\beta</math> are obtained by minimizing the residual sum of squares.\n\nThis set of conditions is an important one and it has a number of implications for the properties of the fitted [[Errors and residuals in statistics|residuals]] and the modelled values. In particular, under these conditions:\n\n:<math>\\bar{f}=\\bar{y}.\\,</math>\n\n===As squared correlation coefficient===\nIn linear least squares [[multiple regression]] with an estimated intercept term, ''R''<sup>2</sup> equals the square of the [[Pearson correlation coefficient]] between the observed <math>y</math> and modeled (predicted) <math>f</math> data values of the dependent variable.\n\nIn a [[simple regression|linear least squares regression with an intercept term and a single explanator]], this is also equal to the squared Pearson correlation coefficient of the dependent variable <math>y</math> and explanatory variable <math>x.</math>\n\nIt should not be confused with the correlation coefficient between two estimates, defined as\n\t\n:<math>\\rho_{\\widehat\\alpha,\\widehat\\beta}={\\operatorname{cov}(\\widehat\\alpha,\\widehat\\beta) \\over \\sigma_{\\widehat\\alpha} \\sigma_{\\widehat\\beta}},</math>\n\t\nwhere the covariance between two coefficient estimates, as well as their [[standard deviation]]s, are obtained from the [[Ordinary least squares#Covariance matrix|covariance matrix]] of the coefficient estimates.\n\nUnder more general modeling conditions, where the predicted values might be generated from a model different from linear least squares regression, an ''R''<sup>2</sup> value can be calculated as the square of the [[Pearson product-moment correlation coefficient|correlation coefficient]] between the original <math>y</math> and modeled <math>f</math> data values. In this case, the value is not directly a measure of how good the modeled values are, but rather a measure of how good a predictor might be constructed from the modeled values (by creating a revised predictor of the form ''α''&nbsp;+&nbsp;''βƒ''<sub>''i''</sub>).{{Citation needed|reason=The citation for the next sentence does not discuss the information in this sentence.|date=March 2017}} According to Everitt (p.&nbsp;78),<ref>{{cite book |last=Everitt |first=B. S. |year=2002 |title=Cambridge Dictionary of Statistics |edition=2nd |publisher=CUP |isbn=978-0-521-81099-9}}</ref> this usage is specifically the definition of the term \"coefficient of determination\": the square of the correlation between two (general) variables.\n\n==Interpretation==\n''R''<sup>2</sup> is a statistic that will give some information about the [[goodness of fit]] of a model. In regression, the ''R''<sup>2</sup> coefficient of determination is a statistical measure of how well the regression predictions approximate the real data points. An ''R''<sup>2</sup> of 1 indicates that the regression predictions perfectly fit the data.\n\nValues of ''R''<sup>2</sup> outside the range 0 to 1 can occur when the model fits the data worse than a horizontal hyperplane. This would occur when the wrong model was chosen, or nonsensical constraints were applied by mistake. If equation 1 of Kvålseth<ref>{{Cite journal|last=Kvalseth|first=Tarald O.|date=1985|title=Cautionary Note about R2|jstor=2683704|journal=The American Statistician|volume=39|issue=4|pages=279–285|doi=10.2307/2683704}}</ref> is used (this is the equation used most often), ''R''<sup>2</sup> can be less than zero. If equation 2 of Kvålseth is used, ''R''<sup>2</sup> can be greater than one.\n\nIn all instances where ''R''<sup>2</sup> is used, the predictors are calculated by ordinary [[least-squares]] regression: that is, by minimizing ''SS''<sub>res</sub>. In this case ''R''<sup>2</sup> increases as we increase the number of variables in the model (''R''<sup>2</sup> is [[Monotonic function|monotone increasing]] with the number of variables included—i.e., it will never decrease). This illustrates a drawback to one possible use of ''R''<sup>2</sup>, where one might keep adding variables ([[Kitchen sink regression]]) to increase the ''R''<sup>2</sup> value. For example, if one is trying to predict the sales of a model of car from the car's gas mileage, price, and engine power, one can include such irrelevant factors as the first letter of the model's name or the height of the lead engineer designing the car because the ''R''<sup>2</sup> will never decrease as variables are added and will probably experience an increase due to chance alone.\n\nThis leads to the alternative approach of looking at the [[#Adjusted R2|adjusted ''R''<sup>2</sup>]]. The explanation of this statistic is almost the same as ''R''<sup>2</sup> but it penalizes the statistic as extra variables are included in the model. For cases other than fitting by ordinary least squares, the ''R''<sup>2</sup> statistic can be calculated as above and may still be a useful measure. If fitting is by [[weighted least squares]] or [[generalized least squares]], alternative versions of R<sup>2</sup> can be calculated appropriate to those statistical frameworks, while the \"raw\" ''R''<sup>2</sup> may still be useful if it is more easily interpreted. Values for ''R''<sup>2</sup> can be calculated for any type of predictive model, which need not have a statistical basis.\n\n===In a non-simple linear model===\nConsider a linear model with [[multiple regression|more than a single explanatory variable]], of the form\n\n:<math>Y_i = \\beta_0 + \\sum_{j=1}^p \\beta_j X_{i,j} + \\varepsilon_i,</math>\n\nwhere, for the ''i''th case, <math>{Y_i}</math> is the response variable, <math>X_{i,1},\\dots,X_{i,p}</math> are ''p'' regressors, and <math>\\varepsilon_i</math> is a mean zero [[errors and residuals in statistics|error]] term. The quantities <math>\\beta_0,\\dots,\\beta_p</math> are unknown coefficients, whose values are estimated by [[least squares]]. The coefficient of determination ''R''<sup>2</sup> is a measure of the global fit of the model. Specifically, ''R''<sup>2</sup> is an element of [0,&nbsp;1] and represents the proportion of variability in ''Y''<sub>''i''</sub> that may be attributed to some linear combination of the regressors ([[explanatory variable]]s) in ''X''.<ref>[https://www.mathworks.com/help/matlab/data_analysis/linear-regression.html#bswinlz Computing Adjusted R2 for Polynomial Regressions]</ref>\n\n''R''<sup>2</sup> is often interpreted as the proportion of response variation \"explained\" by the regressors in the model. Thus, ''R''<sup>2</sup>&nbsp;=&nbsp;1 indicates that the fitted model explains all variability in <math>y</math>, while ''R''<sup>2</sup>&nbsp;=&nbsp;0 indicates no 'linear' relationship (for straight line regression, this means that the straight line model is a constant line (slope&nbsp;=&nbsp;0, intercept&nbsp;=&nbsp;<math>\\bar{y}</math>) between the response variable and regressors). An interior value such as ''R''<sup>2</sup>&nbsp;=&nbsp;0.7 may be interpreted as follows: \"Seventy percent of the variance in the response variable can be explained by the explanatory variables. The remaining thirty percent can be attributed to unknown, [[lurking variable]]s or inherent variability.\"\n\nA caution that applies to ''R''<sup>2</sup>, as to other statistical descriptions of [[correlation]] and association is that \"[[correlation does not imply causation]].\" In other words, while correlations may sometimes provide valuable clues in uncovering causal relationships among variables, a non-zero estimated correlation between two variables is not, on its own, evidence that changing the value of one variable would result in changes in the values of other variables. For example, the practice of carrying matches (or a lighter) is correlated with incidence of lung cancer, but carrying matches does not cause cancer (in the standard sense of \"cause\").\n\nIn case of a single regressor, fitted by least squares, ''R''<sup>2</sup> is the square of the [[Pearson product-moment correlation coefficient]] relating the regressor and the response variable. More generally, ''R''<sup>2</sup> is the square of the correlation between the constructed predictor and the response variable. With more than one regressor, the ''R''<sup>2</sup> can be referred to as the [[coefficient of multiple determination]].\n\n===Inflation of ''R''<sup>2</sup>===\nIn [[least squares]] regression, ''R''<sup>2</sup> is weakly increasing with increases in the number of regressors in the model. Because increases in the number of regressors increase the value of ''R''<sup>2</sup>, ''R''<sup>2</sup> alone cannot be used as a meaningful comparison of models with very different numbers of independent variables. For a meaningful comparison between two models, an [[F-test]] can be performed on the [[residual sum of squares]], similar to the F-tests in [[Granger causality]], though this is not always appropriate. As a reminder of this, some authors denote ''R''<sup>2</sup> by ''R''<sub>''q''</sub><sup>2</sup>, where ''q'' is the number of columns in ''X'' (the number of explanators including the constant).\n\nTo demonstrate this property, first recall that the objective of least squares linear regression is\n\n:<math>\\min_b SS_\\text{res}(b) \\Rightarrow \\min_b \\sum_i (y_i - X_ib)^2\\,</math>\n\nwhere ''X<sub>i</sub>'' is a row vector of values of explanatory variables for case ''i'' and ''b'' is a column vector of coefficients of the respective elements of ''X<sub>i''</sub>.\n\nThe optimal value of the objective is weakly smaller as more explanatory variables are added and hence additional columns of <math>X</math> (the explanatory data matrix whose ''i''th row is ''X<sub>i</sub>'') are added, by the fact that less constrained minimization leads to an optimal cost which is weakly smaller than more constrained minimization does. Given the previous conclusion and noting that <math>SS_{tot}</math> depends only on ''y'', the non-decreasing property of ''R''<sup>2</sup> follows directly from the definition above.\n\nThe intuitive reason that using an additional explanatory variable cannot lower the ''R''<sup>2</sup> is this: Minimizing <math>SS_\\text{res}</math> is equivalent to maximizing ''R''<sup>2</sup>. When the extra variable is included, the data always have the option of giving it an estimated coefficient of zero, leaving the predicted values and the ''R''<sup>2</sup> unchanged. The only way that the optimization problem will give a non-zero coefficient is if doing so improves the ''R''<sup>2</sup>.\n\n===Caveats===\n''R''<sup>2</sup> does not indicate whether:\n* the independent variables are a cause of the changes in the [[dependent variable]];\n* [[omitted-variable bias]] exists;\n* the correct [[regression analysis|regression]] was used;\n* the most appropriate set of independent variables has been chosen;\n* there is [[Multicollinearity|collinearity]] present in the data on the explanatory variables;\n* the model might be improved by using transformed versions of the existing set of independent variables;\n* there are enough data points to make a solid conclusion.\n\n==Extensions==\n===Adjusted ''R''<sup>2</sup>===\n{{see also|Effect size#Omega-squared (ω2)}}\nThe use of an adjusted ''R''<sup>2</sup> (one common notation is <math>\\bar R^2</math>, pronounced \"R bar squared\"; another is <math>R^2_{\\text{adj}}</math>) is an attempt to take account of the phenomenon of the ''R''<sup>2</sup> automatically and spuriously increasing when extra explanatory variables are added to the model. It is a modification due to [[Henri Theil]] of ''R''<sup>2</sup> that adjusts for the number of [[explanatory variable|explanatory]] terms in a model relative to the number of data points.<ref>{{cite book | title=Economic Forecasts and Policy | publisher=North | author=Theil, Henri | year=1961 | location=Holland, Amsterdam | page=213}}</ref> The adjusted ''R''<sup>2</sup> can be negative, and its value will always be less than or equal to that of ''R''<sup>2</sup>. Unlike ''R''<sup>2</sup>, the adjusted ''R''<sup>2</sup> increases only when the increase in ''R''<sup>2</sup> (due to the inclusion of a new explanatory variable) is more than one would expect to see by chance. If a set of explanatory variables with a predetermined hierarchy of importance are introduced into a regression one at a time, with the adjusted ''R''<sup>2</sup> computed each time, the level at which adjusted ''R''<sup>2</sup> reaches a maximum, and decreases afterward, would be the regression with the ideal combination of having the best fit without excess/unnecessary terms. The adjusted ''R''<sup>2</sup> is defined as\n\n:<math>\\bar R^2 = {1-(1-R^2){n-1 \\over n-p-1}}</math>\n\nwhere ''p'' is the total number of explanatory variables in the model (not including the constant term), and ''n'' is the sample size.\n\nAdjusted ''R''<sup>2</sup> can also be written as\n\n:<math>\\bar R^2 = {1-{SS_\\text{res}/\\text{df}_e \\over SS_\\text{tot}/\\text{df}_t}}</math>\n\nwhere df<sub>''t''</sub> is the [[Degrees of freedom (statistics)|degrees of freedom]] ''n''– 1 of the estimate of the population variance of the dependent variable, and df<sub>''e''</sub> is the degrees of freedom ''n'' – ''p'' – 1 of the estimate of the underlying population error variance.\n\nThe principle behind the adjusted ''R''<sup>2</sup> statistic can be seen by rewriting the ordinary ''R''<sup>2</sup> as\n\n:<math>R^{2} = {1-{\\textit{VAR}_\\text{res} \\over \\textit{VAR}_\\text{tot}}}</math>\n\nwhere <math>\\text{VAR}_\\text{res} = SS_\\text{res}/n</math> and <math>\\text{VAR}_\\text{tot} = SS_\\text{tot}/n</math> are the sample variances of the estimated residuals and the dependent variable respectively, which can be seen as biased estimates of the population variances of the errors and of the dependent variable. These estimates are replaced by statistically [[Bias of an estimator#Sample variance|unbiased]] versions: <math>\\text{VAR}_\\text{res} = SS_\\text{res}/(n-p-1)</math> and <math>\\text{VAR}_\\text{tot} = SS_\\text{tot}/(n-1)</math>.\n\nAdjusted ''R''<sup>2</sup> can be interpreted as an unbiased (or less biased) estimator of the population ''R''<sup>2</sup>, whereas the observed sample ''R''<sup>2</sup> is a positively biased estimate of the population value.<ref name=\":0\">{{Cite journal|last=Shieh|first=Gwowen|date=2008-04-01|title=Improved shrinkage estimation of squared multiple correlation coefficient and squared cross-validity coefficient|journal=Organizational Research Methods|volume=11|issue=2|pages=387–407|doi=10.1177/1094428106292901|issn=1094-4281}}</ref> Adjusted ''R''<sup>2</sup> is more appropriate when evaluating model fit (the variance in the dependent variable accounted for by the independent variables) and in comparing alternative models in the [[feature selection]] stage of model building.<ref name=\":0\" />\n\n===Coefficient of partial determination===\n{{see also|Partial correlation}}\n\nThe coefficient of partial determination can be defined as the proportion of variation that cannot be explained in a reduced model, but can be explained by the predictors specified in a full(er) model.<ref>Richard Anderson-Sprecher, \"[http://www.tandfonline.com/doi/abs/10.1080/00031305.1994.10476036 Model Comparisons and R<sup>2</sup>]\", ''[[The American Statistician]]'', Volume 48, Issue 2, 1994, pp. 113–117.</ref><ref>(generalized to [[Maximum Likelihood]]) N. J. D. Nagelkerke, \"[http://www.cesarzamudio.com/uploads/1/7/9/1/17916581/nagelkerke_n.j.d._1991_-_a_note_on_a_general_definition_of_the_coefficient_of_determination.pdf A Note on a General Definition of the Coefficient of Determination]\", ''[[Biometrika]]'', Vol. 78, No. 3. (Sep., 1991), pp. 691–692.</ref><ref>\"[http://stats.stackexchange.com/questions/7775/r-implementation-of-coefficient-of-partial-determination R implementation of coefficient of partial determination]\"</ref> This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified regression model.\n\nThe calculation for the partial ''R''<sup>2</sup> is relatively straightforward after estimating two models and generating the [[ANOVA]] tables for them. The calculation for the partial ''R''<sup>2</sup> is\n\n:<math>\\frac{SS_\\text{ res, reduced} - SS_\\text{ res, full}}{SS_\\text{ res, reduced}},</math>\n\nwhich is analogous to the usual coefficient of determination:\n\n:<math>\\frac{SS_\\text{tot} - SS_\\text{res}}{SS_\\text{tot}}.</math>\n\n=== Generalizing and decomposing <math>R^2</math> <ref name=\"Hoornweg2018SUS\">{{cite book |last1=Hoornweg |first1=Victor |title=Science: Under Submission | chapter=Part II: On Keeping Parameters Fixed | date=2018 |publisher=Hoornweg Press |isbn=978-90-829188-0-9 |chapter-url=http://www.victorhoornweg.com}}</ref>=== \n\nAs explained above, model selection heuristics such as the Adjusted <math>R^2</math> criterion and the [[F-test]] examine whether the total <math>R^2</math> sufficiently increases to determine whether a new regressor should be added to the model. If a regressor is added to the model that is highly correlated with other regressors which have already been included, then the total <math>R^2</math> will hardly increase even if the new regressor is of relevance. As a result, the above-mentioned heuristics will ignore relevant regressors when cross-correlations are high. \n\n[[File:Geometric R squared .svg|thumb|Geometric representation of <math>r^2</math>.]]\nAlternatively, one can decompose a generalized version of <math>R^2</math> to quantify the relevance of deviating from a hypothesis.<ref name = \"Hoornweg2018SUS\" /> As Hoornweg (2018) shows, several shrinkage estimators - such as [[Bayesian linear regression]], [[ridge regression]], and the (adaptive) [[Lasso (statistics)#Lasso method|lasso]] - make use of this decomposition of <math>R^2</math> when they gradually shrink parameters from the unrestricted OLS solutions towards the hypothesized values. Let us first define the linear regression model as \n: <math>y=X\\beta+\\epsilon.</math>\nIt is assumed that the matrix <math>X</math> is standardized with Z-scores and that the column vector <math>y</math> is centered to have a mean of zero. Let the column vector <math>\\beta_0</math> refer to the hypothesized regression parameters and let the column vector <math>b</math> denote the estimated parameters. We can then define \n: <math>R^2=1-\\frac{(y-Xb)'(y-Xb)}{(y-X\\beta_0)'(y-X\\beta_0)}.</math>\nAn <math>R^2</math> of 75% means that the in-sample accuracy improves by 75% if the data-optimized <math>b</math> solutions are used instead of the hypothesized <math>\\beta_0</math> values. In the special case that <math>\\beta_0</math> is a vector of zeros, we obtain the traditional <math>R^2</math> again. \n\n\nThe individual effect on <math>R^2</math> of deviating from a hypothesis can be computed with <math>R^{\\otimes}</math> ('R-outer'). This <math>p</math> times <math>p</math> matrix is given by\n: <math>R^{\\otimes}=(X'\\tilde y_0)(X'\\tilde y_0)' (X'X)^{-1}(\\tilde y_0'\\tilde y_0)^{-1},</math>\nwhere <math>\\tilde y_0=y-X\\beta_0</math>. The diagonal elements of <math>R^{\\otimes}</math> exactly add up to <math>R^2</math>. If regressors are uncorrelated and <math>\\beta_0</math> is a vector of zeros, then the <math>j^{th}</math> diagonal element of <math>R^{\\otimes}</math> simply corresponds to the <math>r^2</math> value between <math>x_j</math> and <math>y</math>. When regressors <math>x_i</math> and <math>x_j</math> are correlated, <math>R^{\\otimes}_{ii}</math> might increase at the cost of a decrease in <math>R^{\\otimes}_{jj}</math>. As a result, the diagonal elements of <math>R^{\\otimes}</math> may be smaller than 0 and, in more exceptional cases, larger than 1. To deal with such uncertainties, several shrinkage estimators implicitly take a weighted average of the diagonal elements of <math>R^{\\otimes}</math> to quantify the relevance of deviating from a hypothesized value.<ref name = \"Hoornweg2018SUS\" /> Click on the [[Lasso (statistics)#Interpretations of lasso|lasso]] for an example.\n\n=== <math>R^2</math> in logistic regression ===\nIn the case of [[logistic regression]], usually fit by [[maximum likelihood]], there are several choices of [[Logistic regression#Pseudo-R2s|''pseudo-R''<sup>2</sup>]].\n\nOne is the generalized ''R''<sup>2</sup> originally proposed by Cox & Snell,<ref>{{cite book |last=Cox|first=D. D.|last2=Snell|first2=E. J.|year=1989|title=The Analysis of Binary Data|edition=2nd|publisher=Chapman and Hall}}</ref> and independently by Magee:<ref>{{cite news|last=Magee|first=L.|year=1990|title=R<sup>2</sup> measures based on Wald and likelihood ratio joint significance tests|journal=The American Statistician|volume=44|pages=250–3|doi=10.1080/00031305.1990.10475731}}</ref>\n: <math>R^2 = 1 - \\left({ \\mathcal{L}(0) \\over \\mathcal{L}(\\widehat{\\theta}) }\\right)^{2/n}</math>\nwhere <math>\\mathcal{L}(0)</math> is the likelihood of the model with only the intercept, <math>{\\mathcal{L}(\\widehat{\\theta})}</math> is the likelihood of the estimated model (i.e., the model with a given set of parameter estimates) and ''n'' is the sample size. It is easily rewritten to:\n: <math>R^2 = 1 - e^{\\frac{2}{n} (\\ln(\\mathcal{L}(0)) - \\ln(\\mathcal{L}(\\widehat{\\theta}))} = 1 - e^{-\\frac{D}{n}}</math>\nwhere D is the test statistic of the [[likelihood ratio test]].\n\nNagelkerke<ref>{{cite book |last=Nagelkerke |first=Nico J. D. |year=1992 |title=Maximum Likelihood Estimation of Functional Relationships, Pays-Bas |series=Lecture Notes in Statistics |volume=69 |isbn=978-0-387-97721-8}}</ref> noted that it had the following properties:\n# It is consistent with the classical coefficient of determination when both can be computed;\n# Its value is maximised by the maximum likelihood estimation of a model;\n# It is asymptotically independent of the sample size;\n# The interpretation is the proportion of the variation explained by the model;\n# The values are between 0 and 1, with 0 denoting that model does not explain any variation and 1 denoting that it perfectly explains the observed variation;\n# It does not have any unit.\n\nHowever, in the case of a logistic model, where <math>\\mathcal{L}(\\widehat{\\theta})</math> cannot be greater than 1, ''R''<sup>2</sup> is between 0 and <math> R^2_\\max = 1- (\\mathcal{L}(0))^{2/n} </math>: thus, Nagelkerke&nbsp;suggested the possibility to define a scaled ''R''<sup>2</sup> as ''R''<sup>2</sup>/''R''<sup>2</sup><sub>max</sub>.<ref>{{cite journal |doi=10.1093/biomet/78.3.691 |title=A Note on a General Definition of the Coefficient of Determination |year=1991 |last1=Nagelkerke |first1=N. J. D. |journal=Biometrika |volume=78 |issue=3 |pages=691–2 |jstor=2337038}}</ref>\n\n==Comparison with norm of residuals==\nOccasionally, the [[Norm (mathematics)#Examples|norm]] of residuals is used for indicating goodness of fit. This term is calculated as the square-root of the [[sum of squared residuals]]:\n\n:<math>\\text{norm of residuals} = \\sqrt{SS_\\text{res}} = \\| e \\|. </math>\n\nBoth ''R''<sup>2</sup> and the norm of residuals have their relative merits. For [[least squares]] analysis ''R''<sup>2</sup> varies between 0 and 1, with larger numbers indicating better fits and 1 representing a perfect fit. The norm of residuals varies from 0 to infinity with smaller numbers indicating better fits and zero indicating a perfect fit. One advantage and disadvantage of ''R''<sup>2</sup> is the <math>SS_\\text{tot}</math> term acts to [[Normalization (statistics)|normalize]] the value. If the ''y<sub>i</sub>'' values are all multiplied by a constant, the norm of residuals will also change by that constant but ''R''<sup>2</sup> will stay the same. As a basic example, for the linear least squares fit to the set of data:\n: <math>\n    x = 1,\\  2,\\  3,\\  4,\\  5\n  </math>\n: <math>\n    y = 1.9,\\  3.7,\\  5.8,\\  8.0,\\  9.6\n  </math>\n''R''<sup>2</sup> = 0.998, and norm of residuals = 0.302.\nIf all values of y are multiplied by 1000 (for example, in an [[Metric prefix|SI prefix]] change), then ''R''<sup>2</sup> remains the same, but norm of residuals = 302.\n\nOther single parameter indicators of fit include the [[standard deviation]] of the residuals, or the [[Root-mean-square deviation|RMSE]] of the residuals. These would have values of 0.151 and 0.174 respectively for the above example given that the fit was linear with an unforced intercept.<ref name=\"origin wp\">OriginLab webpage, http://www.originlab.com/doc/Origin-Help/LR-Algorithm. Retrieved February 9, 2016.</ref>\n\n==History==\n\nThe creation of the coefficient of determination has been attributed to the geneticist [[Sewall Wright]] and was first published in 1921.<ref>{{cite journal |last=Wright |first=Sewell|date=January 1921 |title=Correlation and causation |journal=Journal of Agricultural Research |volume=20 |pages=557–585}}</ref>\n\n==See also==\n* [[Fraction of variance unexplained]]\n* [[Goodness of fit]]\n* [[Nash–Sutcliffe model efficiency coefficient]] ([[Hydrology|hydrological applications]])\n* [[Pearson product-moment correlation coefficient]]\n* [[Proportional reduction in loss]]\n* [[Regression model validation]]\n* [[Root mean square deviation]]\n* [[Test statistic#Common test statistics|t-test of <math>H_0\\colon R^2=0.</math>]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n* {{cite book |last=Gujarati |first=Damodar N. |authorlink=Damodar N. Gujarati |last2=Porter |first2=Dawn C. |title=Basic Econometrics |location=New York |publisher=McGraw-Hill/Irwin |edition=Fifth |year=2009 |isbn=978-0-07-337577-9 |pages=73–78 }}\n* {{cite book |last=Hughes |first=Ann |first2=Dennis |last2=Grawoig |title=Statistics: A Foundation for Analysis |location=Reading |publisher=Addison-Wesley |year=1971 |isbn=0-201-03021-7 |pages=344–348 }} \n* {{cite book |last=Kmenta |first=Jan |authorlink=Jan Kmenta |title=Elements of Econometrics |location=New York |publisher=Macmillan |edition=Second |year=1986 |isbn=978-0-02-365070-3 |pages=240–243 }}\n\n{{DEFAULTSORT:Coefficient Of Determination}}\n[[Category:Regression diagnostics]]\n[[Category:Statistical ratios]]\n[[Category:Least squares]]"
    },
    {
      "title": "Constrained least squares",
      "url": "https://en.wikipedia.org/wiki/Constrained_least_squares",
      "text": "{{more citations needed|date=July 2018}}\n\nIn '''constrained least squares''' one solves a [[linear least squares (mathematics)|linear least squares]] problem with an additional constraint on the solution.<ref name=\"BoydVandenberghe2018\">{{cite book|author1=Stephen Boyd|author2=Lieven Vandenberghe|title=Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares|url=https://books.google.com/books?id=IApaDwAAQBAJ&printsec=frontcover#v=onepage&q=%22Constrained%20least%20squares%22&f=false|date=7 June 2018|publisher=Cambridge University Press|isbn=978-1-316-51896-0}}</ref> \nI.e., the unconstrained equation <math>\\mathbf {X} \\boldsymbol {\\beta} = \\mathbf {y}</math> must be fit as closely as possible (in the least squares sense) while ensuring that some other property of <math>\\boldsymbol {\\beta}</math> is maintained.\n\nThere are often special-purpose algorithms for solving such problems efficiently. Some examples of constraints are given below:\n* [[Constrained generalized inverse|Equality constrained]] least squares: the elements of <math>\\boldsymbol {\\beta}</math> must exactly satisfy <math>\\mathbf {L} \\boldsymbol {\\beta} = \\mathbf {d}</math> (see [[Ordinary least squares#Constrained estimation|Ordinary least squares]]).\n* [[Tikhonov regularization|Regularized]] least squares: the elements of <math>\\boldsymbol {\\beta}</math> must satisfy <math>\\| \\mathbf {L} \\boldsymbol {\\beta} - \\mathbf {y} \\| \\le \\alpha </math> (choosing <math>\\alpha</math> in proportion to the noise standard deviation of '''y''' prevents over-fitting).\n* [[Non-negative least squares]] (NNLS): The vector <math>\\boldsymbol {\\beta}</math> must satisfy the [[ordered vector space|vector inequality]] <math>\\boldsymbol {\\beta} \\geq \\boldsymbol{0}</math> defined componentwise—that is, each component must be either positive or zero.\n* Box-constrained least squares: The vector <math>\\boldsymbol {\\beta}</math> must satisfy the [[ordered vector space|vector inequalities]] <math> \\boldsymbol{lb} \\leq \\boldsymbol{\\beta} \\leq \\boldsymbol{ub}</math>, each of which is defined componentwise.\n* Integer-constrained least squares: all elements of <math>\\boldsymbol {\\beta}</math> must be [[integer]]s (instead of [[real number]]s).\n* Phase-constrained least squares: all elements of <math>\\boldsymbol {\\beta}</math> must be real numbers, all multiplied by a same complex number of unit modulus.\n\nWhen the constraint only applies to some of the variables, the mixed problem may be solved using '''separable least squares''' by letting <math>\\mathbf {X} = [\\mathbf {X_1} \\mathbf {X_2} ]</math> and <math>\\mathbf {\\beta}^{\\rm T} = [\\mathbf {\\beta_1}^{\\rm T} \\mathbf {\\beta_2}^{\\rm T}]</math> represent the unconstrained (1) and constrained (2) components. Then substituting the least-squares solution for <math>\\mathbf {\\beta_1}</math>, i.e.\n\n:<math>\\hat{\\boldsymbol {\\beta_1}} = \\mathbf {X_1}^+ (\\mathbf {y} - \\mathbf {X_2} \\boldsymbol {\\beta_2})</math>\n\n(where <sup>+</sup> indicates the [[Moore-Penrose pseudoinverse]]) back into the original expression gives (following some rearrangement) an equation that can be solved as a purely constrained problem in <math>\\mathbf {\\beta_2}</math>.\n\n:<math> \\mathbf{P} \\mathbf {X_2} \\boldsymbol {\\beta_2} = \\mathbf{P}\\mathbf {y},</math>\n\nwhere <math>\\mathbf{P}:=\\mathbf{I}-\\mathbf {X_1} \\mathbf {X_1}^+</math> is a [[projection matrix]]. Following the constrained estimation of <math>\\hat{\\boldsymbol {\\beta_2}}</math> the vector <math>\\hat{\\boldsymbol {\\beta_1}}</math> is obtained from the expression above.\n\n==See also==\n* [[Constrained optimization]]\n* [[Integer programming]]\n\n==References==\n{{Reflist}}\n\n[[Category:Least squares]]"
    },
    {
      "title": "Discrete least squares meshless method",
      "url": "https://en.wikipedia.org/wiki/Discrete_least_squares_meshless_method",
      "text": "{{multiple issues|\n{{primary sources|date=May 2012}}\n{{more footnotes|date=May 2012}}\n{{context|date=June 2012}}\n}}\nThe '''discrete least squares meshless (DLSM)''' method is a [[meshless method]] based on the [[least squares]] concept. The method is based on the minimization of a least squares [[functional (mathematics)|functional]], defined as the [[Weight function|weighted summation]] of the squared residual of the governing [[differential equation]] and its boundary conditions at [[nodal point]]s used to discretize the [[Domain (mathematical analysis)|domain]] and its boundaries. While most of the existing meshless methods need background cells for [[numerical integration]], DLSM did not require a numerical integration procedure due to the use of the [[Discrete mathematics|discrete]] least squares method to discretize the governing [[differential equation]]. A [[Moving least squares]] (MLS) approximation method is used to construct the shape function, making the approach a fully least squares-based approach.\n\nArzani and Afshar<ref>H. Arzani, M.H. Afshar, Solving Poisson’s equation by the discrete least square meshless method, WIT Transactions on Modelling and Simulation 42 (2006) 23–31.</ref> developed the DLSM method in 2006 for the solution of [[Poisson's equation]]. Firoozjaee and Afshar<ref>A.R. Firoozjaee, M.H. Afshar, Discrete least squares meshless method with sampling points for the solution of elliptic [[partial differential equation]]s. Engineering Analysis with Boundary Elements 33 (2009) 83–92.</ref> proposed the collocated discrete least squares meshless (CDLSM) method to solve [[Elliptic differential equation|elliptic]] partial differential equations, and studied the effect of the collocation points on the convergence and accuracy of the method. The method can be considered as an extension the earlier method of DLSM by the introduction of a set of [[collocation point]]s for the calculation of the least squares functional.\n\nCDLSM was later used by Naisipour et al.<ref>M. Naisipour, M. H. Afshar, B. Hassani, A.R. Firoozjaee, Collocation Discrete Least Square (CDLS) Method for Elasticity Problems. International Journal of Civil Engineering 7 (2009) 9–18.</ref> to solve [[Elasticity (mathematics)|elasticity]] problems regarding the irregular distribution of nodal points. Afshar and Lashckarbolok used the CDLSM method for the adaptive simulation of [[hyperbolic distribution|hyperbolic]] problems. A simple a posteriori error indicator based on the value of the least squares functional and a node moving strategy was used and tested on [[One-dimensional space|1-D]] hyperbolic problems. Shobeyri and Afshar simulated [[free surface]] problems using the DLSM method.\n\nThe method was then extended for adaptive simulation of [[two-dimensional]] shocked hyperbolic problems by Afshar and Firoozjaee. Also, [[Adaptive mesh refinement|adaptive]] node-moving refinement<ref>M.H.Afshar, M. Naisipour, J. Amani, Node moving adaptive refinement strategy for planar elasticity problems using discrete least squares meshless method, Finite Elements in Analysis and Design, 47, (2011) 1315–1325.</ref> and multi-stage node enrichment adaptive refinement<ref>M.H.Afshar, J. Amani, M. Naisipour, A node enrichment adaptive refinement by Discrete Least Squares Meshless method for solution of elasticity problems, Engineering Analysis with Boundary Elements, 36, (2012) 385–393.</ref> are formulated in the DLSM for the solution of elasticity problems.\n\nAmani, Afshar and Naisipour.<ref>J. Amani, M.H.Afshar, M. Naisipour, Mixed Discrete Least Squares Meshless method for planar elasticity problems using regular and irregular nodal distributions, Engineering Analysis with Boundary Elements, 36, (2012) 894–902.</ref> proposed mixed discrete least squares meshless (MDLSM) formulation for solution of planar elasticity problems. In this approach, the differential equations governing the planar elasticity problems are written in terms of the [[Stress (mechanics)|stresses]] and displacements which are approximated independently using the same shape functions. Since the resulting governing [[equation]]s are of the [[first-order approximation|first order]], both the displacement and stress boundary conditions are of the [[Dirichlet]] type, which is easily incorporated via a [[penalty method]]. Because this is a least squares based [[algorithm]] of the MDLSM method, the proposed method does not need to be satisfied by the [[Olga Aleksandrovna Ladyzhenskaya|Ladyzhenskaya]]–[[Ivo Babuška|Babuška]]–Brezzi (LBB) condition.\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n*H. Arzani, M.H. Afshar, Solving Poisson’s equations by the discrete least square meshless method, WIT Transactions on Modelling and Simulation 42 (2006) 23–31.\n*M. H. Afshar, M. Lashckarbolok, Collocated discrete least square (CDLS) meshless method: error estimate and adaptive refinement, International Journal for Numerical Methods in Fluids 56 (2008) 1909–1928.\n*M. Naisipour, M. H. Afshar, B. Hassani, A.R. Firoozjaee, Collocation Discrete Least Square (CDLS) Method for Elasticity Problems. International Journal of Civil Engineering 7 (2009) 9–18.\n*[http://www.sciencedirect.com/science/article/pii/S0955799708000349 A.R. Firoozjaee, M.H. Afshar, Discrete least squares meshless method with sampling points for the solution of elliptic partial differential equations. Engineering Analysis with Boundary Elements 33 (2009) 83–92.]\n*[http://www.sciencedirect.com/science/article/pii/S004579300900142X G. Shobeyri, M.H. Afshar, Simulating free surface problems using Discrete Least Squares Meshless method. Computers & Fluids 39 (2010) 461–470.]\n*[http://www.sciencedirect.com/science/article/pii/S0045793010001787 M.H.Afshar, and A.R. Firoozjaee, Adaptive Simulation of Two Dimensional Hyperbolic Problems by Collocated Discrete Least Squares Meshless Method, Computer and Fluids, 39, (2010) 2030–2039.]\n*[http://www.sciencedirect.com/science/article/pii/S0168874X11001338 M.H.Afshar, M. Naisipour, J. Amani, Node moving adaptive refinement strategy for planar elasticity problems using discrete least squares meshless method, Finite Elements in Analysis and Design, 47, (2011) 1315–1325.]\n*[http://www.sciencedirect.com/science/article/pii/S0955799711001901 M.H.Afshar, J. Amani, M. Naisipour, A node enrichment adaptive refinement by Discrete Least Squares Meshless method for solution of elasticity problems, Engineering Analysis with Boundary Elements, 36, (2012) 385–393.]\n*[http://www.sciencedirect.com/science/article/pii/S095579971100213X J. Amani, M.H.Afshar, M. Naisipour, Mixed Discrete Least Squares Meshless method for planar elasticity problems using regular and irregular nodal distributions, Engineering Analysis with Boundary Elements, 36, (2012) 894–902.]\n*[http://scientiairanica.sharif.edu/article_1650.html Faraji, S., M. Afshar, et al. (2014). \"Mixed discrete least square meshless method for solution of quadratic partial differential equations.\" Scientia Iranica. Transaction A, Civil Engineering 21(3): 492.]\n*[http://scientiairanica.sharif.edu/article_4189.html Faraji, S. et al. (2018) Mixed discrete least squares meshless method for solving the linear and non-linear propagation problems]\n\n[[Category:Differential equations]]\n[[Category:Least squares]]"
    },
    {
      "title": "Explained sum of squares",
      "url": "https://en.wikipedia.org/wiki/Explained_sum_of_squares",
      "text": "{{Multiple issues|\n{{Expert-subject|statistics|date=September 2009}}\n{{morefootnotes|date=December 2010}}\n}}\n\nIn [[statistics]], the '''explained sum of squares (ESS),''' alternatively known as the '''model sum of squares''' or '''sum of squares due to regression''' ('''\"SSR\"''' – not to be confused with the [[residual sum of squares]] '''RSS''' or sum of squares of errors), is a quantity used in describing how well a model, often a [[regression analysis|regression model]], represents the data being modelled. In particular, the explained sum of squares measures how much variation there is in the modelled values and this is compared to the [[total sum of squares]], which measures how much variation there is in the observed data, and to the [[residual sum of squares]], which measures the variation in the modelling errors.\n\n==Definition==\nThe '''explained sum of squares (ESS)''' is the sum of the squares of the deviations of the predicted values from the mean value of a response variable, in a standard [[regression model]] — for example, {{nowrap|1=''y''<sub>''i''</sub> = ''a'' + ''b''<sub>1</sub>''x''<sub>1''i''</sub> + ''b''<sub>2</sub>''x''<sub>2''i''</sub> + ... + ''ε''<sub>''i''</sub>}}, where ''y''<sub>''i''</sub> is the ''i'' <sup>th</sup> observation of the [[response variable]], ''x''<sub>''ji''</sub> is the ''i'' <sup>th</sup> observation of the ''j'' <sup>th</sup> [[explanatory variable]], ''a'' and ''b''<sub>''j''</sub> are [[coefficient]]s, ''i'' indexes the observations from 1 to ''n'', and ''ε''<sub>''i''</sub> is the ''i''&nbsp;<sup>th</sup> value of the [[error term]]. In general, the greater the ESS, the better the estimated model performs.\n\nIf <math>\\hat{a}</math> and <math>\\hat{b}_i</math> are the estimated [[coefficient]]s, then\n\n:<math>\\hat{y}_i=\\hat{a}+\\hat{b}_1 x_{1i} + \\hat{b}_2 x_{2i} + \\cdots \\,  </math>\n\nis the ''i''<sup>&nbsp;th</sup> predicted value of the response variable. The ESS is the sum of the squares of the differences of the predicted values and the mean value of the response variable:\n\n:<math>\\text{ESS} = \\sum_{i=1}^n \\left(\\hat{y}_i - \\bar{y}\\right)^2.</math>\n\nIn some cases (see below): [[total sum of squares]]&nbsp;=&nbsp;'''explained sum of squares'''&nbsp;+&nbsp;[[residual sum of squares]].\n\n==Partitioning in simple linear regression==\nThe following equality, stating that the total sum of squares equals the residual sum of squares plus the explained sum of squares, is generally true in simple linear regression:\n\n:<math>\\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2 = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 + \\sum_{i=1}^n \\left(\\hat{y}_i - \\bar{y}\\right)^2.</math>\n\n===Simple derivation===\n\n:<math>\n\\begin{align}\n(y_i - \\bar{y}) = (y_{i}-\\hat{y}_i)+(\\hat{y}_i - \\bar{y}).\n\\end{align}\n</math>\n\nSquare both sides and sum over all ''i'':\n\n:<math>\n\\sum_{i=1}^n (y_i-\\bar{y})^2=\\sum_{i=1}^n (y_i - \\hat{y}_i)^2+\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^n 2(\\hat{y}_i-\\bar{y})(y_i - \\hat{y}_i).\n</math>\n\nHere is how the last term above is zero from [[simple linear regression]]<ref name=Mendenhall>{{cite book |last=Mendenhall |first=William |title=Introduction to Probability and Statistics |publisher=Brooks/Cole |year=2009 |location=Belmont, CA |page=507 |edition=13th |isbn=9780495389538 }}</ref>\n\n:<math>\\hat{y_i} = \\hat{a} + \\hat{b}x_i</math>\n:<math>\\bar{y} = \\hat{a} + \\hat{b}\\bar{x}</math>\n:<math>\\hat{b} = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}</math>\n\nSo,\n\n:<math>\\hat{y_i} - \\bar{y} = \\hat{b}(x_i - \\bar{x})</math> \n:<math>y_i - \\hat{y}_i = (y_i - \\bar{y}) - (\\hat{y}_i - \\bar{y}) = (y_i - \\bar{y}) - \\hat{b}(x_i - \\bar{x})</math>\n\nTherefore,\n\n: <math>\n\\begin{align}\n& \\sum_{i=1}^n 2(\\hat{y}_i-\\bar{y})(y_i-\\hat{y}_i) = 2\\hat{b}\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\hat{y}_i) \\\\[4pt]\n= {} & 2\\hat{b}\\sum_{i=1}^n (x_i-\\bar{x})((y_i - \\bar{y}) - \\hat{b}(x_i - \\bar{x})) \\\\[4pt]\n= {} & 2\\hat{b}\\left(\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})-\\sum_{i=1}^n(x_i-\\bar{x})^2\\frac{\\sum_{j=1}^n (x_j-\\bar{x})(y_j-\\bar{y})}{\\sum_{j=1}^n (x_j-\\bar{x})^2}\\right) \\\\[4pt]\n= {} & 2\\hat{b} (0) = 0\n\\end{align}\n</math>\n\n==Partitioning in the general ordinary least squares model==\n\nThe general regression model with ''n'' observations and ''k'' explanators, the first of which is a constant unit vector whose coefficient is the regression intercept, is\n\n:<math> y = X \\beta + e</math>\n\nwhere ''y'' is an ''n'' × 1 vector of dependent variable observations, each column of the ''n'' × ''k'' matrix ''X'' is a vector of observations on one of the ''k'' explanators, <math>\\beta </math> is a ''k'' × 1 vector of true coefficients,  and ''e'' is an ''n'' × 1 vector of the true underlying errors.  The [[ordinary least squares]] estimator for <math>\\beta</math> is\n\n:<math> \\hat \\beta = (X^T X)^{-1}X^T y.</math>\n\nThe residual vector <math>\\hat e</math> is <math>y - X \\hat \\beta = y - X (X^T X)^{-1}X^T y</math>, so the residual sum of squares <math>\\hat e ^T \\hat e</math> is, after simplification,\n\n:<math>  RSS = y^T y - y^T X(X^T X)^{-1} X^T y.</math>\n\nDenote as <math>\\bar y</math> the constant vector all of whose elements are the sample mean <math>y_m</math> of the dependent variable values in the vector ''y''.  Then the total sum of squares is\n\n:<math> TSS = (y - \\bar y)^T(y - \\bar y) = y^T y - 2y^T \\bar y + \\bar y ^T \\bar y.</math>\n\nThe explained sum of squares, defined as the sum of squared deviations of the predicted values from the observed mean of ''y'', is\n\n:<math> ESS = (\\hat y - \\bar y)^T(\\hat y - \\bar y) = \\hat y^T \\hat y - 2\\hat y^T \\bar y + \\bar y ^T \\bar y.</math>\n\nUsing <math> \\hat y = X \\hat \\beta</math> in this, and simplifying to obtain <math>\\hat y^T \\hat y = y^TX(X^T X)^{-1}X^Ty </math>, gives the result that ''TSS'' = ''ESS'' + ''RSS'' if and only if <math>y^T \\bar y = \\hat y^T \\bar y</math>.  The left side of this is <math>y_m</math> times the sum of the elements of ''y'', and the right side is <math>y_m</math> times the sum of the elements of <math>\\hat y</math>, so the condition is that the sum of the elements of ''y'' equals the sum of the elements of <math>\\hat y</math>, or equivalently that the sum of the prediction errors (residuals) <math>y_i - \\hat y_i</math> is zero.  This can be seen to be true by noting the well-known OLS property that the ''k'' × 1 vector <math>X^T \\hat e = X^T [I - X(X^T X)^{-1}X^T]y= 0</math>:  since the first column of ''X'' is a vector of ones, the first element of this vector <math>X^T \\hat e</math> is the sum of the residuals and is equal to zero.  This proves that the condition holds for the result that ''TSS'' = ''ESS'' + ''RSS''.\n\nIn linear algebra terms, we have <math>RSS = \\|y - {\\hat y}\\|^2 </math>, <math> TSS = \\|y - \\bar y\\|^2</math>, <math> ESS = \\|{\\hat y} - \\bar y\\|^2 </math>.\nThe proof can be simplified by noting that <math> y^T {\\hat y} = {\\hat y}^T {\\hat y} </math>. The proof is as follows:\n:<math> {\\hat y}^T {\\hat y} = \ny^T X (X^T X)^{-1} X^T X (X^T X)^{-1} X^T y = y^T X (X^T X)^{-1} X^T y = y^T {\\hat y}, </math>\n\nThus,\n:<math> TSS = \\|y - \\bar y\\|^2 = \\|y - {\\hat y} + {\\hat y} - \\bar y\\|^2 </math>\n:<math> TSS = \\|y - {\\hat y}\\|^2 + \\|{\\hat y} - \\bar y\\|^2 + 2 <y - {\\hat y}, {\\hat y} - {\\bar y}>  </math>\n:<math> TSS = RSS + ESS + 2 y^T {\\hat y} -2 {\\hat y}^T {\\hat y} - 2 y^T {\\bar y} + 2 {\\hat y}^T{\\bar y}  </math>\n:<math> TSS = RSS + ESS  - 2 y^T {\\bar y} + 2 {\\hat y}^T{\\bar y} </math>\nwhich again gives the result that ''TSS'' = ''ESS'' + ''RSS'', since <math>(y-\\hat y)^T \\bar y = 0</math>.\n\n==See also==\n*[[Sum of squares (statistics)]]\n*[[Lack-of-fit sum of squares]]\n*[[Fraction of variance unexplained]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* S. E. Maxwell and H. D. Delaney (1990), \"Designing experiments and analyzing data: A model comparison perspective\". Wadsworth. pp.&nbsp;289–290.\n* G. A. Milliken and D. E. Johnson (1984), \"Analysis of messy data\", Vol. I: Designed experiments. Van Nostrand Reinhold. pp.&nbsp;146–151.\n* B. G. Tabachnick and L. S. Fidell (2007), \"Experimental design using ANOVA\". Duxbury. p.&nbsp;220.\n* B. G. Tabachnick and L. S. Fidell (2007), \"Using multivariate statistics\", 5th ed. Pearson Education. pp.&nbsp;217–218.\n\n{{DEFAULTSORT:Explained Sum Of Squares}}\n[[Category:Least squares]]"
    },
    {
      "title": "Fraction of variance unexplained",
      "url": "https://en.wikipedia.org/wiki/Fraction_of_variance_unexplained",
      "text": "{{Short description|Statistical noise}}{{Unreferenced|date=February 2007}}\n{{broader|Explained variation}}\nIn [[statistics]], the '''fraction of variance unexplained''' ('''FVU''') in the context of a [[Regression analysis|regression task]] is the fraction of variance of the [[regressand]] (dependent variable) ''Y'' which cannot be explained, i.e., which is not correctly predicted, by the [[explanatory variable]]s ''X''.\n\n==Formal definition==\nSuppose we are given a regression function <math>f</math> yielding for each <math>y_i</math> an estimate <math>\\widehat{y}_i = f(x_i)</math> where <math>x_i</math> is the vector of the ''i''<sup>th</sup> observations on all the explanatory variables. We define the fraction of variance unexplained (FVU) as:\n\n:<math>\\begin{align}\n\\text{FVU} & = {\\text{VAR}_\\text{err} \\over \\text{VAR}_\\text{tot}} = {\\text{SS}_\\text{err}/n \\over \\text{SS}_\\text{tot}/n} = {\\text{SS}_\\text{err} \\over \\text{SS}_\\text{tot}} \\left( = 1-{\\text{SS}_\\text{reg} \\over \\text{SS}_\\text{tot}} , \\text{ only true in some cases such as linear regression}\\right) \\\\[6pt]\n & = 1 - R^2,\n\\end{align}</math>\n\nwhere ''R''<sup>2</sup> is the [[coefficient of determination]] and ''VAR''<sub>err</sub> and ''VAR''<sub>tot</sub> are the variance of the residuals and the sample variance of the dependent variable. ''SS''<sub>''err''</sub> (the sum of squared predictions errors, equivalently the [[residual sum of squares]]), ''SS''<sub>''tot''</sub> (the [[total sum of squares]]), and ''SS''<sub>''reg''</sub> (the sum of squares of the regression, equivalently the [[explained sum of squares]]) are given by\n\n:<math>\\begin{align} \n\\text{SS}_\\text{err} & = \\sum_{i=1}^N\\;(y_i - \\widehat{y}_i)^2\\\\\n\\text{SS}_\\text{tot} & = \\sum_{i=1}^N\\;(y_i-\\bar{y})^2 \\\\\n\\text{SS}_\\text{reg} & = \\sum_{i=1}^N\\;(\\widehat{y}_i-\\bar{y})^2 \\text{ and}  \\\\\n\\bar{y} & = \\frac 1 N \\sum_{i=1}^N\\;y_i.\n\\end{align}</math>\n\nAlternatively, the fraction of variance unexplained can be defined as follows:\n\n:<math> \\text{FVU} = \\frac{\\operatorname{MSE}(f)}{\\operatorname{var}[Y]},</math>\n\nwhere MSE(''f'') is the [[mean squared error]] of the regression function&nbsp;''&fnof;''.\n\n==Explanation==\nIt is useful to consider the second definition to understand FVU. When trying to predict ''Y'', the most naïve regression function that we can think of is the constant function predicting the mean of ''Y'', i.e., <math>f(x_i)=\\bar{y}</math>. It follows that the MSE of this function equals the variance of ''Y''; that is, ''SS''<sub>err</sub> = ''SS''<sub>tot</sub>, and ''SS''<sub>reg</sub> = 0. In this case, no variation in ''Y'' can be accounted for, and the FVU then has its maximum value of 1.\n\nMore generally, the FVU will be 1 if the explanatory variables ''X'' tell us nothing about ''Y'' in the sense that the predicted values of ''Y'' do not [[covariance|covary]] with ''Y''. But as prediction gets better and the MSE can be reduced, the FVU goes down. In the case of perfect prediction where <math>\\hat{y}_i = y_i</math> for all ''i'', the MSE is 0, ''SS''<sub>err</sub> = 0, ''SS''<sub>reg</sub> = ''SS''<sub>tot</sub>, and the FVU is 0.\n\n==See also==\n* [[Coefficient of determination]]\n* [[Correlation]]\n* [[Explained sum of squares]]\n* [[Regression analysis]]\n* [[Linear regression]]\n\n{{DEFAULTSORT:Fraction Of Variance Unexplained}}\n[[Category:Parametric statistics]]\n[[Category:Statistical ratios]]\n[[Category:Least squares]]"
    },
    {
      "title": "Generalized least squares",
      "url": "https://en.wikipedia.org/wiki/Generalized_least_squares",
      "text": "{{multiple issues|\n{{Cleanup|date=May 2010}}\n{{Refimprove|date=July 2009}}\n}}\n\n{{Regression bar}}\nIn [[statistics]], '''generalized least squares''' ('''GLS''') is a technique for estimating the unknown [[parameter]]s in a [[linear regression]] model when there is a certain degree of [[correlation]] between the [[statistical residual|residuals]] in a [[regression model]]. In these cases, [[ordinary least squares]] and [[weighted least squares]] can be statistically [[efficiency (statistics)|inefficient]], or even give misleading [[statistical inference|inferences]]. GLS was first described by [[Alexander Aitken]] in 1934.<ref>{{cite journal |last=Aitken |first=A. C. |title=On Least-squares and Linear Combinations of Observations |journal=Proceedings of the Royal Society of Edinburgh |year=1934 |volume=55 |issue= |pages=42–48 }}</ref>\n\n== Method outline ==\nIn standard [[linear regression]] models we observe data <math>\\{y_i,x_{ij}\\}_{i=1, \\dots, n,j=2, \\dots, k}</math> on ''n'' [[statistical unit]]s. The response values are placed in a vector <math>\\mathbf{y} = \\left( y_{1}, \\dots, y_{n} \\right)^{\\mathtt{T}}</math>, and the predictor values are placed in the [[design matrix]] <math>\\mathbf{X} = \\left( \\mathbf{x}_{1}^{\\mathtt{T}}, \\dots, \\mathbf{x}_{n}^{\\mathtt{T}} \\right)^{\\mathtt{T}}</math>, where <math>\\mathbf{x}_{i} = \\left( 1, x_{2i}, \\dots, x_{ki} \\right)</math> is a vector of the ''k'' predictor variables (including a constant) for the ''i''th unit. The model forces the [[conditional mean]] of <math>\\mathbf{y}</math> given <math>\\mathbf{X}</math> to be a linear function of <math>\\mathbf{X}</math>, and assumes the conditional [[variance]] of the error term given <math>\\mathbf{X}</math> is a ''known'' nonsingular ''[[covariance matrix]]'' <math>\\mathbf{\\Omega}</math>. This is usually written as\n: <math>\n    \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon}, \\qquad \\operatorname{E}[\\varepsilon\\mid\\mathbf{X}]=0,\\ \\operatorname{Cov}[\\varepsilon\\mid\\mathbf{X}]= \\mathbf{\\Omega}.\n  </math>\nHere <math>\\beta \\in \\mathbb{R}^k</math> is a vector of unknown constants (known as “regression coefficients”) that must be estimated from the data.\n\nSuppose <math>\\mathbf{b}</math> is a candidate estimate for <math>\\mathbf{\\beta}</math>. Then the [[errors and residuals in statistics|residual]] vector for <math>\\mathbf{b}</math> will be <math>\\mathbf{y}- \\mathbf{X} \\mathbf{b}</math>. The generalized least squares method estimates <math>\\mathbf{\\beta}</math> by minimizing the squared [[Mahalanobis distance|Mahalanobis length]] of this residual vector:\n: <math>\n    \\mathbf{\\hat{\\beta}} = \\underset{b}\\operatorname{arg min}\\,(\\mathbf{y}- \\mathbf{X} \\mathbf{b})^{\\mathtt{T}}\\,\\mathbf{\\Omega}^{-1}(\\mathbf{y}- \\mathbf{X} \\mathbf{b}),\n  </math>\n\nSince the objective is a quadratic form in <math>\\mathbf{b}</math>, the estimator has an explicit formula:\n: <math>\n    \\mathbf{\\hat{\\beta}} = \\left( \\mathbf{X}^{\\mathtt{T}} \\mathbf{\\Omega}^{-1} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\mathtt{T}}\\mathbf{\\Omega}^{-1}\\mathbf{y}.\n  </math>\n\n=== Properties ===\nThe GLS estimator is [[Bias of an estimator|unbiased]], [[consistent estimator|consistent]], [[efficiency (statistics)|efficient]], and [[asymptotic distribution|asymptotically normal]] with <math>\\operatorname{E}[\\hat\\beta\\mid\\mathbf{X}] = \\beta</math> and <math>\\operatorname{Cov}[\\hat{\\beta}\\mid\\mathbf{X}] = (\\mathbf{X}^{\\mathtt{T}}\\Omega^{-1}\\mathbf{X})^{-1}</math>. GLS is equivalent to applying ordinary least squares to a linearly transformed version of the data.  To see this, factor <math>\\mathbf{\\Omega} = \\mathbf{C} \\mathbf{C}^{\\mathtt{T}}</math>, for instance using the [[Cholesky decomposition]]. Then if we pre-multiply both sides of the equation <math>\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon}</math> by <math>\\mathbf{C}^{-1}</math>, we get an equivalent linear model <math>\\mathbf{y}^{*} = \\mathbf{X}^{*} \\mathbf{\\beta} + \\mathbf{\\varepsilon}^{*}</math> where <math>\\mathbf{y}^{*} = \\mathbf{C}^{-1} \\mathbf{y}</math>, <math>\\mathbf{X}^{*} = \\mathbf{C}^{-1} \\mathbf{X}</math>, and <math>\\mathbf{\\varepsilon}^{*} = \\mathbf{C}^{-1} \\mathbf{\\varepsilon}</math>. In this model <math>\\operatorname{Var}[\\varepsilon^{*}\\mid\\mathbf{X}]= \\mathbf{C}^{-1} \\mathbf{\\Omega} \\left(\\mathbf{C}^{-1} \\right)^{\\mathtt{T}} = \\mathbf{I}</math>, where <math>\\mathbf{I}</math> is the [[identity matrix]]. Thus we can efficiently estimate <math>\\mathbf{\\beta}</math> by applying OLS to the transformed data, which requires minimizing\n\n: <math>\n    \\left(\\mathbf{y}^{*} - \\mathbf{X}^{*} \\mathbf{\\beta} \\right)^{\\mathtt{T}} (\\mathbf{y}^{*} - \\mathbf{X}^{*} \\mathbf{\\beta}) = (\\mathbf{y}- \\mathbf{X} \\mathbf{b})^{\\mathtt{T}}\\,\\mathbf{\\Omega}^{-1}(\\mathbf{y}- \\mathbf{X} \\mathbf{b}).\n  </math>\n\nThis has the effect of standardizing the scale of the errors and “de-correlating” them. Since OLS is applied to data with homoscedastic errors, the [[Gauss–Markov theorem]] applies, and therefore the GLS estimate is the [[Blue (statistics)|best linear unbiased estimator]] for ''β''.\n\n== Weighted least squares ==\n{{Main|Weighted least squares}}\nA special case of GLS called weighted least squares (WLS) occurs when all the off-diagonal entries of ''Ω'' are 0.  This situation arises when the variances of the observed values are unequal (i.e.&nbsp;[[heteroscedasticity]] is present), but where no correlations exist among the observed variances.  The weight for unit ''i'' is proportional to the reciprocal of the variance of the response for unit ''i''.<ref>{{cite book|author=Strutz, T.| title=Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond) |publisher=Springer Vieweg | year=2016 | isbn= 978-3-658-11455-8}}, chapter 3</ref>\n\n== Feasible generalized least squares ==\n\nIf the covariance of the errors <math>\\Omega </math> is unknown, one can get a consistent estimate of <math>\\Omega </math>, say <math>\\widehat \\Omega </math>,<ref name=\"Baltagi2008\">Baltagi, B. H. (2008). Econometrics (4th ed.). New York: Springer.</ref> using an implementable version of GLS known as the '''feasible generalized least squares'''<!--\"Feasible generalized least squares\" redirects here; this is bolded per MOS:BOLD--> ('''FGLS''') estimator. In FGLS, modeling proceeds in two stages: (1) the model is estimated by OLS or another consistent (but inefficient) estimator, and the residuals are used to build a consistent estimator of the errors covariance matrix (to do so, one often needs to examine the model adding additional constraints, for example if the errors follow a time series process, a statistician generally needs some theoretical assumptions on this process to ensure that a consistent estimator is available); and (2) using the consistent estimator of the covariance matrix of the errors, one can implement GLS ideas.\n\nWhereas GLS is more efficient than OLS under heteroscedasticity or autocorrelation, this is not true for FGLS. The feasible estimator is, provided the errors covariance matrix is consistently estimated, ''asymptotically'' more efficient, but for a small or medium size sample, it can be actually less efficient than OLS. This is why, some authors prefer to use OLS, and reformulate their inferences by simply considering an alternative estimator for the variance of the estimator robust to heteroscedasticity or serial autocorrelation.\nBut for large samples FGLS is preferred over OLS under heteroskedasticity or serial correlation.<ref name=\"Baltagi2008\" /> <ref name=\"Greene2003\">Greene, W. H. (2003). Econometric Analysis (5th ed.). Upper Saddle River, NJ: Prentice Hall.</ref><nowiki> </nowiki>A cautionary note is that the FGLS estimator is not always consistent. One case in which FGLS might be inconsistent is if there are individual specific fixed effects.<ref>{{Cite journal |last=Hansen |first=Christian B. |title=Generalized Least Squares Inference in Panel and Multilevel Models with Serial Correlation and Fixed Effects |journal=[[Journal of Econometrics]] |year=2007 |volume=140 |issue=2 |pages=670–694 |doi=10.1016/j.jeconom.2006.07.011 }}</ref>\n\nIn general this estimator has different properties than GLS. For large samples (i.e., asymptotically) all properties are (under appropriate conditions) common with respect to GLS, but for finite samples the properties of FGLS estimators are unknown: they vary dramatically with each particular model, and as a general rule their exact distributions cannot be derived analytically. For finite samples, FGLS may be even less efficient than OLS in some cases. Thus, while GLS can be made feasible, it is not always wise to apply this method when the sample is small.\nA method sometimes used to improve the accuracy of the estimators in finite samples is to iterate, i.e. taking the residuals from FGLS to update the errors covariance estimator, and then updating the FGLS estimation, applying the same idea iteratively until the estimators vary less than some tolerance. But this method does not necessarily improve the efficiency of the estimator very much if the original sample was small.\nA reasonable option when samples are not too large is to apply OLS, but throwing away the classical variance estimator\n:<math> \\sigma^2*(X'X)^{-1} </math>\n(which is inconsistent in this framework) and using a HAC (Heteroskedasticity and Autocorrelation Consistent) estimator. For example, in autocorrelation context we can use the Bartlett estimator (often known as Newey-West estimator since these authors popularized the use of this estimator among econometricians in their 1987 Econometrica article), and in heteroskedastic context we can use the Eicker–White estimator ([[Heteroscedasticity-consistent standard errors|Eicker–White]]). This approach is much safer, and it is the appropriate path to take unless the sample is large, and \"large\" is sometimes a slippery issue (e.g. if the errors distribution is asymmetric the required sample would be much larger).\n\nThe [[ordinary least squares]] (OLS) estimator is calculated as usual by\n\n:<math>\n\\widehat \\beta_\\text{OLS} = (X' X)^{-1} X' y\n</math>\n\nand estimates of the residuals <math>\\widehat{u}_j= (Y-X\\widehat\\beta_\\text{OLS})_j</math> are constructed.\n\nFor simplicity consider the model for heteroskedastic errors. Assume that the variance-covariance matrix <math> \\Omega </math> of the error vector is diagonal, or equivalently that errors from distinct observations are uncorrelated. Then each diagonal entry may be estimated by the fitted residuals <math>\\widehat{u}_j</math>  so <math>\\widehat{\\Omega}_{OLS}</math> may be constructed by\n\n:<math>\n\\widehat{\\Omega}_\\text{OLS} = \\operatorname{diag}(\\widehat{\\sigma}^2_1, \\widehat{\\sigma}^2_2, \\dots , \\widehat{\\sigma}^2_n).\n</math>\n\nIt is important to notice that the squared residuals cannot be used in the previous expression; we need an estimator of the errors variances. To do so, we can use a parametric heteroskedasticity model, or a nonparametric estimator. Once this step is fulfilled, we can proceed:\n\nEstimate <math> \\beta_{FGLS1}</math> using <math> \\widehat{\\Omega}_\\text{OLS}</math> using<ref name=\"Greene2003\" /> [[weighted least squares]]\n\n:<math>\n\\widehat \\beta_{FGLS1} = (X'\\widehat{\\Omega}^{-1}_\\text{OLS} X)^{-1} X' \\widehat{\\Omega}^{-1}_\\text{OLS} y\n</math>\n\nThe procedure can be iterated. The first iteration is given by\n:<math>\n \\widehat{u}_{FGLS1} = Y - X \\widehat \\beta_{FGLS1}\n</math>\n\n:<math>\n\\widehat{\\Omega}_{FGLS1} = \\operatorname{diag}(\\widehat{\\sigma}^2_{FGLS1,1}, \\widehat{\\sigma}^2_{FGLS1,2}, \\dots ,\\widehat{\\sigma}^2_{FGLS1,n})\n</math>\n\n:<math>\n\\widehat \\beta_{FGLS2} = (X'\\widehat{\\Omega}^{-1}_{FGLS1} X)^{-1} X' \\widehat{\\Omega}^{-1}_{FGLS1} y\n</math>\n\nThis estimation of <math>\\widehat{\\Omega}</math> can be iterated to convergence.\n\nUnder regularity conditions any of the FGLS estimator (or that of any of its iterations, if we iterate a finite number of times) is asymptotically distributed as\n\n: <math>\n    \\sqrt{n}(\\hat\\beta_{FGLS} - \\beta)\\ \\xrightarrow{d}\\ \\mathcal{N}\\!\\left(0,\\,V\\right).\n  </math>\n\nwhere n is the sample size and\n:<math>\nV = \\operatorname{p-lim}(X'\\Omega^{-1}X/T)\n</math>\nhere p-lim means limit in probability\n\n== See also ==\n* [[Confidence region]]\n* [[Degrees of freedom (statistics)#Effective degrees of freedom|Effective degrees of freedom]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |last=Amemiya |first=Takeshi |authorlink=Takeshi Amemiya |year=1985 |chapter=Generalized Least Squares Theory |title=Advanced Econometrics |publisher=Harvard University Press |isbn=0-674-00560-0 |ref=harv |chapterurl=https://books.google.com/books?id=0bzGQE14CwEC&pg=PA181 }}\n* {{cite book |last=Johnston |first=John |authorlink=John Johnston (econometrician) |chapter=Generalized Least-squares |title=Econometric Methods |location=New York |publisher=McGraw-Hill |edition=Second |year=1972 |isbn= |pages=208–242 |chapterurl=https://books.google.com/books?id=BZtvwZAGyV0C&pg=PA208 }}\n* {{cite book |last=Kmenta |first=Jan |authorlink=Jan Kmenta |chapter=Generalized Linear Regression Model and Its Applications |title=Elements of Econometrics |location=New York |publisher=Macmillan |edition=Second |year=1986 |isbn=0-472-10886-7 |pages=607–650 |chapterurl=https://books.google.com/books?id=Bxq7AAAAIAAJ&pg=PA607 }}\n\n{{DEFAULTSORT:Generalized Least Squares}}\n[[Category:Least squares]]"
    },
    {
      "title": "Generated regressor",
      "url": "https://en.wikipedia.org/wiki/Generated_regressor",
      "text": "{{technical|date=January 2019}}\nIn [[least squares]] estimation problems, sometimes one or more [[regressors]] specified in the model are not observable. One way to circumvent this issue is to estimate or generate regressors from observable data.<ref>Pagan, A., 1984, “Econometric Issues in the Analysis of Regressions with Generated Regressors”, International Economic Review, 25 (1), 221-247.</ref> This '''generated regressor''' method is also applicable to unobserved [[instrumental variables]]. Under some regularity conditions, consistency and asymptotic normality of least squares estimator is preserved, but asymptotic variance has a different form in general.\n\nSuppose the model of interest is the following:\n\n:<math>y_{i}=g(x_{1i},x_{2i},\\beta)+u_{i}</math> \n\nwhere g is a conditional mean function and its form is known up to finite-dimensional parameter β. Here <math>x_{2i}</math> is not observable, but we know that <math>x_{2i}=h(w_{i},\\gamma)</math> for some function ''h'' known up to parameter <math>\\gamma</math>, and a random sample <math>y_{i}=g(x_{1i},x_{2i},\\beta)+u_{i}</math> is available. Suppose we have a consistent estimator <math>\\hat\\gamma </math> of <math>\\gamma</math> that uses the observation <math>w_{i}</math>'s. Then, β can be estimated by (Non-Linear) Least Squares using <math>\\hat{x_{2i}}=h(w_{i},\\hat\\gamma)</math>. Some examples of the above setup include Anderson et al. (1976<ref>Anderson, G. J., I. F. Pearce and P. K. Trivedi, \"Output, Expected Demand and Unplanned Stocks,\" in I. F. Pearce et al., eds., A Model of Output, Employment, Wages and Prices in the U.K., Cambridge University Press.</ref> and Barro (1977).<ref>Barro, R. J., 1977, \"Unanticipated Money Growth and Unemployment in the United States,\" American Economic Review, 67, 101-115.</ref> \n\nThis problem falls into the framework of [[two-step M-estimator]] and thus consistency and asymptotic normality of the estimator can be verified using the general theory of two-step M-estimator.<ref name=\"Wooldridge\">Wooldridge, J.M., Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass</ref> As in general two-step M-estimator problem, asymptotic variance of a generated regressor estimator is usually different from that of the estimator with all regressors observed. Yet, in some special cases, the asymptotic variances of the two estimators are identical. To give one such example, consider the setting in which the regression function is linear in parameter and unobserved regressor is a scalar. Denoting the coefficient of unobserved regressor by <math>\\delta</math> if <math>\\delta=0</math> and <math>E[\\triangledown\\gamma h(W,\\gamma) U]=0</math> then the asymptotic variance is independent of whether observing the regressor.<ref name=\"Wooldridge\"/>\n\nWith minor modifications in the model, the above formulation is also applicable to Instrumental Variable estimation. Suppose the model of interest is linear in parameter. Error term is correlated with some of the regressors, and the model specifies some instrumental variables, which are not observable but have the representation <math>z_{i}=h(w_{i},\\gamma)</math>. If a consistent estimator of <math>\\gamma</math> of <math>\\hat\\gamma</math> is available using <math>\\hat z_{i}= h(w_{i},\\hat\\gamma)</math> as instruments, the parameter of interest can be estimated by IV. Similar to the above case, consistency and asymptotic normality follows under mild conditions, and the asymptotic variance has a different form than observed IV case. Yet, there are cases in which the two estimators have the same asymptotic variance. One such case occurs if <math>E[\\triangledown\\gamma h(W,\\gamma)]=0[4]</math>In this special case, inference on the estimated parameter can be conducted with the usual IV standard error estimator.\n\n== References ==\n{{Reflist}}\n\n[[Category:Least squares]]\n[[Category:M-estimators]]\n[[Category:Regression analysis]]\n[[Category:Simultaneous equation methods (econometrics)]]"
    },
    {
      "title": "Helmert–Wolf blocking",
      "url": "https://en.wikipedia.org/wiki/Helmert%E2%80%93Wolf_blocking",
      "text": "{{unreliable sources|date=May 2015}}\nThe '''Helmert–Wolf blocking'''<ref>{{cite web|url=http://www.ngs.noaa.gov/GRD/GPS/DOC/gpscom/mca.html|title=Making Combined Adjustments|first=Bill|last=Dillinger|date=4 March 1999|access-date=6 June 2017}}</ref> ('''HWB''') is a [[least squares]] solution method<ref name=\"wolf\">{{cite conference |url= https://archive.org/details/proceedingsofsec00inte|title=The Helmert block method—its origins and development|last1= Wolf|first1= Helmut|date= April 1978|publisher= U.S. Dept. of Commerce|book-title= Proceedings of the second International Symposium on Problems Related to the Redefinition of North American Geodetic Networks|pages= 319-326|location= Arlington, Virginia|conference=International Symposium on Problems Related to the Redefinition of North American Geodetic Networks|id= }}</ref> for a sparse canonical block-angular<ref>http://fkf.net/equations.gif</ref>{{better source|reason=this is an excerpt from an unidentified book|date=June 2017}} (CBA) system of [[linear equation]]s. [[Friedrich Robert Helmert|Helmert]] (1843–1917) reported on the use of such systems for [[geodesy]] in 1880.<ref>{{cite book|first=Friedrich Robert|last=Helmert|title=Die mathematischen und physikalischen Theorien der höheren Geodäsie, 1. Teil|location=Leipzig|date=1880}}</ref> Wolf (1910–1994)<ref name=\"wolf-web\">{{cite web|url=http://www.fkf.net/Wolf.html|title=The Wolf formulas|date=9 June 2004|accessdate=6 June 2017}}</ref> published his direct semianalytic solution<ref name=\"wolf-web\"/><ref>http://www.fkf.net/Wolf.jpg</ref>{{better source|reason=this is an excerpt from an unidentified book|date=June 2017}}<ref name=\"strang\">{{cite book|title=Linear algebra, geodesy, and GPS|first1=Gilbert|last1=Strang|first2=Kai|last2=Borre|date=1997|publisher=Wellesley-Cambridge Press|location=Wellesley|isbn=9780961408862|pages=507-508}}</ref> based on ordinary [[Gaussian elimination]] in [[matrix (mathematics)|matrix]] form <ref name=\"strang\"/> in 1978.<ref name=\"wolf\"/>\n\n== Description ==\n{{stub section|date=May 2015}}\n{{further|Invertible matrix#Blockwise inversion|Block matrix#Inversion}}\n{{see also|Fast Kalman Filter#Description}}\n\n== Limitations ==\nThe HWB solution is very fast to compute but it is optimal only if observational errors do not correlate between the data blocks. The [[generalized canonical correlation]] analysis (gCCA) is the statistical method of choice for making those harmful cross-covariances vanish. This may, however, become quite tedious depending on the nature of the problem.\n\n== Applications ==\nThe HWB method is critical to satellite geodesy and similar large problems.{{citation needed|date=May 2015}} The HWB method can be extended to [[fast Kalman filter]]ing (FKF) by augmenting its [[linear regression]] equation system to take into account information from numerical forecasts, physical constraints and other ancillary data sources that are available in realtime. Operational accuracies can then be computed reliably from the theory of minimum-norm quadratic unbiased estimation ([[Minque]]) of [[C. R. Rao]].\n\n== See also ==\n* [[Block matrix]]\n\n== Notes ==\n{{Reflist}}\n\n{{DEFAULTSORT:Helmert-Wolf blocking}}\n[[Category:Statistical algorithms]]\n[[Category:Least squares]]\n[[Category:Geodesy]]\n\n\n{{Statistics-stub}}"
    },
    {
      "title": "Iteratively reweighted least squares",
      "url": "https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares",
      "text": "{{Regression bar}}\nThe method of '''iteratively reweighted least squares''' ('''IRLS''') is used to solve certain optimization problems with [[objective function]]s of the form of a [[p-norm|''p''-norm]]:\n\n:<math>\\underset{\\boldsymbol\\beta} {\\operatorname{arg\\,min}} \\sum_{i=1}^n \\big| y_i - f_i (\\boldsymbol\\beta) \\big|^p, </math>\n\nby an [[iterative method]] in which each step involves solving a [[weighted least squares]] problem of the form:<ref name=Burrus>C. Sidney Burrus, ''[https://cnx.org/exports/92b90377-2b34-49e4-b26f-7fe572db78a1@12.pdf/iterative-reweighted-least-squares-12.pdf Iterative Reweighted Least Squares]''</ref>\n\n:<math>\\boldsymbol\\beta^{(t+1)} = \\underset{\\boldsymbol\\beta} {\\operatorname{arg\\,min}} \\sum_{i=1}^n w_i (\\boldsymbol\\beta^{(t)}) \\big| y_i - f_i (\\boldsymbol\\beta) \\big|^2. </math>\n\nIRLS is used to find the [[maximum likelihood]] estimates of a [[generalized linear model]], and in [[robust regression]] to find an [[M-estimator]], as a way of mitigating the influence of outliers in an otherwise normally-distributed data set. For example, by minimizing the [[least absolute errors]] rather than the [[least squares|least square errors]].\n\nOne of the advantages of IRLS over [[linear programming]] and [[convex programming]] is that it can be used with [[Gauss–Newton]] and [[Levenberg–Marquardt]] numerical algorithms.\n\n== Examples ==\n\n=== ''L''<sub>1</sub> minimization for sparse recovery ===\nIRLS can be used for '''[[L1 norm|''ℓ''<sub>1</sub>]]''' minimization and smoothed '''[[Lp quasi-norm|''ℓ''<sub>p</sub>]]''' minimization, ''p''&nbsp;<&nbsp;1, in [[compressed sensing]] problems. It has been proved that the algorithm has a linear rate of convergence for ''ℓ''<sub>1</sub> norm and superlinear for ''ℓ''<sub>''t''</sub> with ''t''&nbsp;<&nbsp;1, under the [[restricted isometry property]], which is generally a sufficient condition for sparse solutions.<ref>{{Cite conference\n  | last1 = Chartrand | first1 = R.\n  | last2 = Yin | first2 = W.\n  | title = Iteratively reweighted algorithms for compressive sensing\n  | booktitle = IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2008\n  | pages = 3869–3872\n  | date = March 31 – April 4, 2008\n  | url = http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4518498}}\n</ref><ref>{{Cite journal | last1 = Daubechies | first1 = I. | last2 = Devore | first2 = R. | last3 = Fornasier | first3 = M. | last4 = Güntürk | first4 = C. S. N. | title = Iteratively reweighted least squares minimization for sparse recovery | doi = 10.1002/cpa.20303 | journal = Communications on Pure and Applied Mathematics | volume = 63 | pages = 1–38 | year = 2010 | pmid =  | pmc = | arxiv = 0807.0575 }}</ref> However, in most practical situations, the restricted isometry property is not satisfied.\n\n=== ''L<sup>p</sup>'' norm linear regression ===\nTo find the parameters '''''β'''''&nbsp;=&nbsp;(''β''<sub>1</sub>, …,''β''<sub>''k''</sub>)<sup>T</sup> which minimize the [[Lp space|''L<sup>p</sup>'' norm]] for the [[linear regression]] problem,\n\n:<math>\n\\underset{\\boldsymbol \\beta}{ \\operatorname{arg\\,min} }\n    \\big\\| \\mathbf y - X \\boldsymbol \\beta \\|_p\n =\n\\underset{\\boldsymbol \\beta}{ \\operatorname{arg\\,min} }\n     \\sum_{i=1}^n  \\left| y_i - X_i \\boldsymbol\\beta \\right|^p ,\n</math>\n\nthe IRLS algorithm at step ''t''&nbsp;+&nbsp;1 involves solving the [[Linear least squares (mathematics)#Weighted linear least squares|weighted linear least squares]] problem:<ref>{{cite book\n |chapter=6.8.1 Solutions that Minimize Other Norms of the Residuals\n |title=Matrix algebra\n |last=Gentle |first=James\n |isbn=978-0-387-70872-0\n |doi=10.1007/978-0-387-70873-7\n |publisher=Springer |location=New York\n |year=2007\n|series=Springer Texts in Statistics\n }}</ref>\n\n:<math>\n\\boldsymbol\\beta^{(t+1)}\n =\n\\underset{\\boldsymbol\\beta}{ \\operatorname{arg\\,min} }\n    \\sum_{i=1}^n w_i^{(t)}  \\left| y_i - X_i \\boldsymbol\\beta \\right|^2\n =\n(X^{\\rm T} W^{(t)} X)^{-1} X^{\\rm T} W^{(t)} \\mathbf{y},\n</math>\n\nwhere ''W''<sup>(''t'')</sup> is the [[diagonal matrix]] of weights, usually with all elements set initially to:\n\n:<math>w_i^{(0)} = 1</math>\n\nand updated after each iteration to:\n\n:<math>w_i^{(t)} = \\big|y_i - X_i \\boldsymbol \\beta ^{(t)} \\big|^{p-2}.</math>\n\nIn the case ''p''&nbsp;=&nbsp;1, this corresponds to [[least absolute deviation]] regression (in this case, the problem would be better approached by use of [[linear programming]] methods,<ref name=Pfeil>William A. Pfeil,\n''[http://www.wpi.edu/Pubs/E-project/Available/E-project-050506-091720/unrestricted/IQP_Final_Report.pdf Statistical Teaching Aids]'', Bachelor of Science thesis, [[Worcester Polytechnic Institute]], 2006</ref> so the result would be exact) and the formula is:\n\n:<math>w_i^{(t)} = \\frac{1}{\\big|y_i - X_i \\boldsymbol \\beta ^{(t)} \\big|}.</math>\n\nTo avoid dividing by zero, [[Regularization (mathematics)|regularization]] must be done, so in practice the formula is:\n\n:<math>w_i^{(t)} = \\frac 1 {\\max\\left\\{\\delta, \\left|y_i - X_i \\boldsymbol \\beta ^{(t)} \\right|\\right\\} }.</math>\n\nwhere <math>\\delta</math> is some small value, like 0.0001.<ref name=Pfeil /> Note the use of <math>\\delta</math> in the weighting function is equivalent to the [[Huber loss]] function in robust estimation. <ref name=Fox_and_Weisberg> Fox, J.; Weisberg, S. (2013),''[http://users.stat.umn.edu/~sandy/courses/8053/handouts/robust.pdf Robust Regression]'', Course Notes, University of Minnesota</ref>\n\n== See also ==\n* [[Feasible generalized least squares]]\n* [[Weiszfeld's algorithm]] (for approximating the [[geometric median]]), which can be viewed as a special case of IRLS\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n* [http://www.mai.liu.se/~akbjo/LSPbook.html Numerical Methods for Least Squares Problems by Åke Björck] (Chapter 4: Generalized Least Squares Problems.)\n* [http://graphics.stanford.edu/~jplewis/lscourse/SLIDES.pdf Practical Least-Squares for Computer Graphics. SIGGRAPH Course 11]\n\n== External links ==\n\n* [https://stemblab.github.io/irls/ Solve under-determined linear systems iteratively]\n\n{{DEFAULTSORT:Iteratively Reweighted Least Squares}}\n[[Category:Least squares]]"
    },
    {
      "title": "Lack-of-fit sum of squares",
      "url": "https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares",
      "text": "In [[statistics]], a '''sum of squares due to lack of fit''', or more tersely a '''lack-of-fit sum of squares''', is one of the components of a partition of the [[Sum of squares (statistics)|sum of squares]] of residuals in an [[analysis of variance]], used in the [[numerator]] in an [[F-test]] of the [[null hypothesis]] that says that a proposed model fits well. The other component is the '''pure-error sum of squares'''.\n\nThe pure-error sum of squares is the sum of squared deviations of each value of the [[dependent variable]] from the average value over all observations sharing its [[independent variable]] value(s). These are errors that could never be avoided by any predictive equation that assigned a predicted value for the dependent variable as a function of the value(s) of the independent variable(s). The remainder of the residual sum of squares is attributed to lack of fit of the model since it would be mathematically possible to eliminate these errors entirely.\n\n== Sketch of the idea ==\n\nIn order for the lack-of-fit sum of squares to differ from the [[Residual sum of squares|sum of squares of residuals]], there must be [[replication (statistics)|more than one]] value of the [[response variable]] for at least one of the values of the set of predictor variables.  For example, consider fitting a line\n\n: <math> y = \\alpha x + \\beta \\, </math>\n\nby the method of [[least squares]].  One takes as estimates of ''α'' and ''β'' the values that minimize the sum of squares of residuals, i.e., the sum of squares of the differences between the observed ''y''-value and the fitted ''y''-value.  To have a lack-of-fit sum of squares that differs from the residual sum of squares, one must observe more than one ''y''-value for each of one or more of the ''x''-values.  One then partitions the \"sum of squares due to error\", i.e., the sum of squares of residuals, into two components:\n\n: sum of squares due to error = (sum of squares due to \"pure\" error) + (sum of squares due to lack of fit).\n\nThe sum of squares due to \"pure\" error is the sum of squares of the differences between each observed ''y''-value and the average of all ''y''-values corresponding to the same ''x''-value.\n\nThe sum of squares due to lack of fit is the ''weighted'' sum of squares of differences between each average of ''y''-values corresponding to the same ''x''-value and the corresponding fitted ''y''-value, the weight in each case being simply the number of observed ''y''-values for that ''x''-value.<ref>{{cite book |first=Richard J. |last=Brook |first2=Gregory C. |last2=Arnold |title=Applied Regression Analysis and Experimental Design |publisher=[[CRC Press]] |location= |year=1985 |pages=48–49 |isbn=0824772520 }}</ref><ref>{{cite book |first=John |last=Neter |first2=Michael H. |last2=Kutner |first3=Christopher J. |last3=Nachstheim |first4=William |last4=Wasserman |title=Applied Linear Statistical Models |edition=Fourth |publisher=Irwin |location=Chicago |year=1996 |pages=121–122 |isbn=0256117365 }}</ref> Because it is a property of least squares regression that the vector whose components are \"pure errors\" and the vector of lack-of-fit components are orthogonal to each other, the following equality holds:\n\n:<math>\n\\begin{align}\n&\\sum (\\text{observed value} - \\text{fitted value})^2  && \\text{(error)} \\\\\n&\\qquad = \\sum (\\text{observed value} - \\text{local average})^2  && \\text{(pure error)} \\\\\n&\\qquad\\qquad {} + \\sum \\text{weight}\\times (\\text{local average} - \\text{fitted value})^2  && \\text{(lack of fit)}\n\\end{align}\n</math>\n\nHence the residual sum of squares has been completely decomposed into two components.\n\n== Mathematical details ==\nConsider fitting a line with one predictor variable. Define ''i'' as an index of each of the ''n'' distinct ''x'' values, ''j'' as an index of the response variable observations for a given ''x'' value, and ''n''<sub>''i''</sub> as the number of ''y'' values associated with the ''i'' <sup>th</sup> ''x'' value.  The value of each response variable observation can be represented by\n\n: <math> Y_{ij} = \\alpha x_i + \\beta + \\varepsilon_{ij},\\qquad i = 1,\\dots, n,\\quad j = 1,\\dots,n_i.</math>\n\nLet\n\n: <math> \\widehat\\alpha, \\widehat\\beta \\,</math>\n\nbe the [[least squares]] estimates of the unobservable parameters ''α'' and ''β'' based on the observed values of ''x''<sub>&nbsp;''i''</sub> and ''Y''<sub>&nbsp;''i&nbsp;j''</sub>.\n\nLet\n\n: <math> \\widehat Y_i = \\widehat\\alpha x_i + \\widehat\\beta \\,</math>\n\nbe the fitted values of the response variable.  Then\n\n: <math> \\widehat\\varepsilon_{ij} = Y_{ij} - \\widehat Y_i \\,</math>\n\nare the [[errors and residuals in statistics|residuals]], which are observable estimates of the unobservable values of the error term&nbsp;''ε''<sub>&nbsp;''ij''</sub>.  Because of the nature of the method of least squares, the whole vector of residuals, with \n\n:<math> N = \\sum_{i=1}^n n_i </math>\n\nscalar components, necessarily satisfies the two constraints\n\n: <math> \\sum_{i=1}^n \\sum_{j=1}^{n_i} \\widehat\\varepsilon_{ij} = 0 \\,</math>\n\n: <math> \\sum_{i=1}^n \\left(x_i \\sum_{j=1}^{n_i} \\widehat\\varepsilon_{ij} \\right) = 0. \\,</math>\n\nIt is thus constrained to lie in an (''N''&nbsp;&minus;&nbsp;2)-dimensional subspace of '''R'''<sup>&nbsp;''N''</sup>, i.e. there are ''N''&nbsp;&minus;&nbsp;2 \"[[degrees of freedom (statistics)|degrees of freedom]] for error\".\n\nNow let\n\n: <math> \\overline{Y}_{i\\bullet} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} Y_{ij} </math>\n\nbe the average of all ''Y''-values associated with the ''i'' <sup>th</sup> ''x''-value.\n\nWe partition the sum of squares due to error into two components:\n\n:<math>\n\\begin{align}\n& \\sum_{i=1}^n \\sum_{j=1}^{n_i} \\widehat\\varepsilon_{ij}^{\\,2}\n= \\sum_{i=1}^n \\sum_{j=1}^{n_i} \\left( Y_{ij} - \\widehat Y_i \\right)^2 \\\\\n& = \\underbrace{ \\sum_{i=1}^n \\sum_{j=1}^{n_i} \\left(Y_{ij} - \\overline Y_{i\\bullet}\\right)^2 }_\\text{(sum of squares due to pure error)}\n+ \\underbrace{ \\sum_{i=1}^n n_i \\left( \\overline Y_{i\\bullet} - \\widehat Y_i \\right)^2. }_\\text{(sum of squares due to lack of fit)}\n\\end{align}\n</math>\n\n== Probability distributions ==\n\n=== Sums of squares ===\n\nSuppose the [[errors and residuals in statistics|error terms]] ''ε''<sub>&nbsp;''i&nbsp;j''</sub> are [[statistical independence|independent]] and [[normal distribution|normally distributed]] with [[expected value]]&nbsp;0 and [[variance]]&nbsp;''σ''<sup>2</sup>.  We treat ''x''<sub>&nbsp;''i''</sub> as constant rather than random.  Then the response variables ''Y''<sub>&nbsp;''i&nbsp;j''</sub> are random only because the errors ''ε''<sub>&nbsp;''i&nbsp;j''</sub> are random.\n\nIt can be shown to follow that if the straight-line model is correct, then the '''sum of squares due to error''' divided by the error variance,\n\n: <math> \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\sum_{j=1}^{n_i} \\widehat\\varepsilon_{ij}^{\\,2} </math>\n\nhas a [[chi-squared distribution]] with ''N''&nbsp;&minus;&nbsp;2 degrees of freedom.\n\nMoreover, given the total number of observations ''N'', the number of levels of the independent variable ''n,'' and the number of parameters in the model ''p'':\n\n* The sum of squares due to pure error, divided by the error variance ''σ''<sup>2</sup>, has a chi-squared distribution with ''N''&nbsp;&minus;&nbsp;''n'' degrees of freedom;\n* The sum of squares due to lack of fit, divided by the error variance ''σ''<sup>2</sup>, has a chi-squared distribution with ''n''&nbsp;&minus;&nbsp;''p'' degrees of freedom (here ''p''&nbsp;=&nbsp;2 as there are two parameters in the straight-line model);\n* The two sums of squares are probabilistically independent.\n\n=== The test statistic ===\n\nIt then follows that the statistic\n\n: <math>\n\\begin{align}\nF & = \\frac{ \\text{lack-of-fit sum of squares} /\\text{degrees of freedom} }{\\text{pure-error sum of squares} / \\text{degrees of freedom} } \\\\[8pt]\n& = \\frac{\\left.\\sum_{i=1}^n n_i \\left( \\overline Y_{i\\bullet} - \\widehat Y_i \\right)^2\\right/ (n-p)}{\\left.\\sum_{i=1}^n \\sum_{j=1}^{n_i} \\left(Y_{ij} - \\overline Y_{i\\bullet}\\right)^2 \\right/ (N - n)}\n\\end{align}\n</math>\n\nhas an [[F-distribution]] with the corresponding number of degrees of freedom in the numerator and the denominator, provided that the model is correct. If the model is wrong, then the probability distribution of the denominator is still as stated above, and the numerator and denominator are still independent.  But the numerator then has a [[noncentral chi-squared distribution]], and consequently the quotient as a whole has a [[non-central F-distribution]].\n\nOne uses this F-statistic to test the [[null hypothesis]] that the linear model is correct. Since the non-central F-distribution is [[stochastic order|stochastically larger]] than the (central) F-distribution, one rejects the null hypothesis if the F-statistic is larger than the critical F value.  The critical value corresponds to the [[cumulative distribution function]] of the [[F distribution]] with ''x'' equal to the desired [[confidence level]], and degrees of freedom ''d''<sub>1</sub>&nbsp;=&nbsp;(''n''&nbsp;&minus;&nbsp;''p'') and ''d''<sub>2</sub>&nbsp;=&nbsp;(''N''&nbsp;&minus;&nbsp;''n'').\n\nThe assumptions of [[normal distribution]] of errors and [[independence (probability theory)|independence]] can be shown to entail that this [[Goodness of fit|lack-of-fit test]] is the [[likelihood-ratio test]] of this null hypothesis.\n\n== See also ==\n* [[Linear regression]]\n\n== Notes ==\n{{reflist}}\n\n[[Category:Analysis of variance]]\n[[Category:Design of experiments]]\n[[Category:Least squares]]\n[[Category:Statistical hypothesis testing]]"
    },
    {
      "title": "Least absolute deviations",
      "url": "https://en.wikipedia.org/wiki/Least_absolute_deviations",
      "text": "{{Regression bar}}\n'''Least absolute deviations''' ('''LAD'''), also known as '''least absolute errors''' ('''LAE'''), '''least absolute value''' ('''LAV'''), '''least absolute residual''' ('''LAR'''), '''sum of absolute deviations''', or the [[L1 norm|''L''<sub>1</sub> norm]] condition, is a statistical [[optimality criterion]] and the statistical [[optimization (mathematics)|optimization]] technique that relies on it. Similar to the popular [[least squares]] technique, it attempts to find a [[function (mathematics)|function]] which closely approximates a set of data. In the simple case of a set of (''x'',''y'') data, the approximation function is a simple \"trend line\" in two-dimensional [[Cartesian coordinates]]. The method [[maxima and minima|minimizes]] the sum of absolute errors (SAE) (the sum of the absolute values of the vertical \"residuals\" between points generated by the function and corresponding points in the data). The least absolute deviations estimate also arises as the [[maximum likelihood]] estimate if the errors have a [[Laplace distribution]]. It was introduced in 1757 by [[Roger Joseph Boscovich]].<ref>{{cite book|chapter=Least Absolute Deviation Regression|title=The Concise Encyclopedia of Statistics|pages=299–302|doi=10.1007/978-0-387-32833-1_225|publisher=Springer|date=2008 |isbn=9780387328331}}</ref>\n\n==Formulation of the problem==\n\nSuppose that the [[data set]] consists of the points (''x''<sub>''i''</sub>, ''y''<sub>''i''</sub>) with ''i'' = 1, 2, ..., ''n''. We want to find a function ''f'' such that <math>f(x_i)\\approx y_i.</math>\n\nTo attain this goal, we suppose that the function ''f'' is of a particular form containing some parameters which need to be determined. For instance, the simplest form would be linear: ''f''(''x'') = ''bx'' + ''c'', where ''b'' and ''c'' are parameters whose values are not known but which we would like to estimate. Less simply, suppose that ''f''(''x'') is [[quadratic function|quadratic]], meaning that ''f''(''x'') = ''ax''<sup>2</sup> + ''bx'' + ''c'', where ''a'', ''b'' and ''c'' are not yet known. (More generally, there could be not just one explanator ''x'', but rather multiple explanators, all appearing as arguments of the function ''f''.)\n\nWe now seek estimated values of the unknown parameters that minimize the sum of the absolute values of the residuals:\n\n:<math> S = \\sum_{i=1}^n |y_i - f(x_i)|. </math>\n\n==Contrasting ordinary least squares with least absolute deviations==\n\nThe following is a table contrasting some properties of the method of least absolute deviations with those of the method of least squares (for non-singular problems).<ref>For a set of applets that demonstrate these differences, see the following site: http://www.math.wpi.edu/Course_Materials/SAS/lablets/7.3/73_choices.html</ref><ref>For a discussion of LAD versus OLS, see these academic papers and reports: http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf and https://www.leeds.ac.uk/educol/documents/00003759.htm</ref>\n\n{| border=\"1\" cellpadding=\"5\" cellspacing=\"0\"\n|-\n! Ordinary least squares regression || Least absolute deviations regression\n|-\n| rowspan=1 align=\"center\"| Not very robust\n| colspan=2 align=\"center\"| Robust\n|-\n| rowspan=1 align=\"center\"| Stable solution\n| colspan=2 align=\"center\"| Unstable solution\n|-\n| rowspan=1 align=\"center\"| Always one solution\n| colspan=2 align=\"center\"| Possibly multiple solutions\n|-\n|}\n\nThe method of least absolute deviations finds applications in many areas, due to its robustness compared to the least squares method. Least absolute deviations is robust in that it is resistant to outliers in the data.  LAD gives equal emphasis to all observations, in contrast to ordinary least squares (OLS) which, by squaring the residuals, gives more weight to large residuals, that is, outliers in which predicted values are far from actual observations.  This may be helpful in studies where outliers do not need to be given greater weight than other observations. If it is important to give greater weight to outliers, the method of least squares is a better choice.\n\n==Other properties==\n\nThere exist other unique properties of the least absolute deviations line. In the case of a set of (''x'',''y'') data, the least absolute deviations line will always pass through at least two of the data points, unless there are multiple solutions. If multiple solutions exist, then the region of valid least absolute deviations solutions will be bounded by at least two lines, each of which passes through at least two data points. More generally, if there are ''k'' [[Dependent and independent variables#Alternative terminology in statistics|regressors]] (including the constant), then at least one optimal regression surface will pass through ''k'' of the data points.<ref>Branham, R. L., Jr., \"Alternatives to least squares\", ''[[Astronomical Journal]]'' 87, June 1982, 928–937. [http://adsabs.harvard.edu/full/1982AJ.....87..928B] at SAO/NASA Astrophysics Data System (ADS)</ref>{{rp|p.936}}\n\nThis \"latching\" of the line to the data points can help to understand the \"instability\" property: if the line always latches to at least two points, then the line will jump between different sets of points as the data points are altered. The \"latching\" also helps to understand the \"robustness\" property: if there exists an outlier, and a least absolute deviations line must latch onto two data points, the outlier will most likely not be one of those two points because that will not minimize the sum of absolute deviations in most cases.\n\nOne known case in which multiple solutions exist is a set of points symmetric about a horizontal line, as shown in Figure A below.\n\n[[File:Least absolute deviations regression method diagram.gif|600px|thumb|center|Figure A: A set of data points with reflection symmetry and multiple least absolute deviations solutions. The “solution area” is shown in green. The vertical blue lines represent the absolute errors from the pink line to each data point. The pink line is one of infinitely many solutions within the green area.]]\n\nTo understand why there are multiple solutions in the case shown in Figure A, consider the pink line in the green region. Its sum of absolute errors is some value S. If one were to tilt the line upward slightly, while still keeping it within the green region, the sum of errors would still be S. It would not change because the distance from each point to the line grows on one side of the line, while the distance to each point on the opposite side of the line diminishes by exactly the same amount. Thus the sum of absolute errors remains the same. Also, since one can tilt the line in infinitely small increments, this also shows that if there is more than one solution, there are infinitely many solutions.\n\n==Variations, extensions, specializations==\nThe least absolute deviation problem may be extended to include multiple explanators, constraints and [[regularization (mathematics)|regularization]], e.g., a linear model with linear constraints:<ref>{{Cite journal |author1=Mingren Shi |authorlink1=Mingren Shi |last2=Mark A. |first2= Lukas |authorlink2=Mark A. Lukas  | date=March 2002 | title = An ''L<sub>1</sub>'' estimation algorithm with degeneracy and linear constraints\n | journal = [[Computational Statistics & Data Analysis]]\n | doi = 10.1016/S0167-9473(01)00049-4\n | volume = 39\n | issue = 1\n | pages = 35–55\n}}</ref>\n: minimize <math>S(\\mathbf{\\beta}, b) = \\sum_i | \\mathbf{x}'_i \\mathbf{\\beta} + b - y_i |</math>\n: subject to, e.g., <math>\\mathbf{x}'_1 \\mathbf{\\beta} + b - y_1 \\leq  k</math>\n\nwhere <math>\\mathbf{\\beta}</math> is a column vector of coefficients to be estimated, ''b'' is an intercept to be estimated, '''x'''<sub>'''i''' </sub> is a column vector of the ''i''<sup>th</sup> observations on the various explanators, ''y''<sub>''i''</sub> is the ''i''<sup>th</sup> observation on the dependent variable, and ''k'' is a known constant.\n\n[[Regularization (mathematics)|Regularization]] with [[Lasso (statistics)|LASSO]] may also be combined with LAD.<ref>{{Cite conference\n | author = Li Wang, Michael D. Gordon & Ji Zhu\n | title = Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning\n | booktitle = Proceedings of the Sixth International Conference on Data Mining\n |date=December 2006\n | pages = 690–700\n | doi = 10.1109/ICDM.2006.134\n}}</ref>\n\n==Solving methods==\n\nThough the idea of least absolute deviations regression is just as straightforward as that of least squares regression, the least absolute deviations line is not as simple to compute efficiently. Unlike least squares regression, least absolute deviations regression does not have an analytical solving method. Therefore, an iterative approach is required. The following is an enumeration of some least absolute deviations solving methods.\n\n* [[Simplex algorithm|Simplex-based methods]] (such as the Barrodale-Roberts algorithm<ref>{{Cite journal\n | author = I. Barrodale & F. D. K. Roberts\n | title = An improved algorithm for discrete L<sub>1</sub> linear approximation\n | journal = [[SIAM Journal on Numerical Analysis]]\n | volume = 10\n | year = 1973\n | pages = 839–848\n | jstor = 2156318\n | doi = 10.1137/0710069\n | issue = 5\n|bibcode = 1973SJNA...10..839B }}</ref>)\n** Because the problem is a [[linear program]], any of the many linear programming techniques (including the simplex method as well as others) can be applied.\n* [[Iteratively re-weighted least squares]]<ref>{{Cite journal\n | author = E. J. Schlossmacher\n | title = An Iterative Technique for Absolute Deviations Curve Fitting\n | journal = [[Journal of the American Statistical Association]]\n | volume = 68\n | issue = 344\n |date=December 1973\n | pages = 857–859\n | jstor = 2284512\n | doi = 10.2307/2284512\n }}</ref>\n* Wesolowsky’s direct descent method<ref>{{Cite journal\n | author = G. O. Wesolowsky\n | year = 1981\n | title = A new descent algorithm for the least absolute value regression problem\n | journal = Communications in Statistics – Simulation and Computation\n | volume = B10\n | issue = 5\n | pages = 479–491\n | doi = 10.1080/03610918108812224\n}}</ref>\n* Li-Arce’s maximum likelihood approach<ref>{{Cite journal\n |author  = Yinbo Li and Gonzalo R. Arce\n |title   = A Maximum Likelihood Approach to Least Absolute Deviation Regression\n |journal = [[EURASIP Journal on Applied Signal Processing]]\n |volume  = 2004\n |year    = 2004\n |issue   = 12\n |pages   = 1762–1769\n |doi     = 10.1155/S1110865704401139\n |url     = http://www.hindawi.com/journals/asp/2004/948982.abs.html\n|bibcode= 2004EJASP2004...61L\n }}{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n* Recursive reduction of dimensionality approach<ref>{{Cite journal\n |author  = Ana Sovic Krzic and Damir Sersic\n |title   = L1 minimization using recursive reduction of dimensionality\n |journal = Signal Processing\n |volume  = 151\n |year    = 2018\n |pages   = 119–129\n |doi     = 10.1016/j.sigpro.2018.05.002\n }}</ref>\n* Check all combinations of point-to-point lines for minimum sum of errors\n\nSimplex-based methods are the “preferred” way to solve the least absolute deviations problem.<ref name=Pfeil>William A. Pfeil,\n''[http://www.wpi.edu/Pubs/E-project/Available/E-project-050506-091720/unrestricted/IQP_Final_Report.pdf Statistical Teaching Aids]'', Bachelor of Science thesis, [[Worcester Polytechnic Institute]], 2006</ref> A Simplex method is a method for solving a problem in linear programming. The most popular algorithm is the Barrodale-Roberts modified Simplex algorithm. The algorithms for IRLS, Wesolowsky's Method, and Li's Method can be found in Appendix A of <ref name=Pfeil/>\namong other methods. Checking all combinations of lines traversing any two (x,y) data points is another method of finding the least absolute deviations line. Since it is known that at least one least absolute deviations line traverses at least two data points, this method will find a line by comparing the SAE (Smallest Absolute Error over data points) of each line, and choosing the line with the smallest SAE. In addition, if multiple lines have the same, smallest SAE, then the lines outline the region of multiple solutions. Though simple, this final method is inefficient for large sets of data.\n\n===Solving using linear programming===\n\nThe problem can be solved using any linear programming technique on the following problem specification. We wish to\n\n:<math> \\text{Minimize} \\sum_{i=1}^n |y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \\cdots - a_kx_{ik}|</math>\n\nwith respect to the choice of the values of the parameters <math>a_0,\\ldots, a_k</math>, where ''y''<sub>''i''</sub> is the value of the ''i''<sup>th</sup> observation of the dependent variable, and ''x''<sub>''ij''</sub> is the value of the ''i''<sup>th</sup> observation of the ''j''<sup>th</sup> independent variable (''j'' = 1,...,''k''). We rewrite this problem in terms of artificial variables ''u''<sub>''i''</sub> as\n\n:<math> \\text{Minimize} \\sum_{i=1}^n u_i</math>\n\n:with respect to <math>a_0,\\ldots, a_k</math> and <math>u_1,\\ldots, u_n</math>\n\n:subject to\n\n:<math> u_i \\ge y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \\cdots - a_kx_{ik} \\,\\ \\,\\ \\,\\ \\,\\ \\,\\ \\text{for } i=1,\\ldots,n</math>\n\n:<math> u_i \\ge -[y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \\cdots - a_kx_{ik}] \\,\\ \\,\\ \\text{ for } i=1,\\ldots,n.</math>\n\nThese constraints have the effect of forcing each <math>u_i</math> to equal <math>|y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \\cdots - a_kx_{ik}|</math> upon being minimized, so the objective function is equivalent to the original objective function. Since this version of the problem statement does not contain the absolute value operator, it is in a format that can be solved with any linear programming package.\n\n==See also==\n* [[Quantile regression]]\n* [[Regression analysis]]\n* [[Linear regression model]]\n* [[Absolute deviation]]\n* [[Average absolute deviation]]\n* [[Median absolute deviation]]\n* [[Ordinary least squares]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{Cite journal\n | author = Peter Bloomfield and William Steiger\n | title = Least Absolute Deviations Curve-Fitting\n | journal = [[SIAM Journal on Scientific Computing]]\n | year = 1980\n | volume = 1\n | issue = 2\n | pages =290–301\n | doi = 10.1137/0901019\n}}\n* {{Cite journal\n | author = Subhash C. Narula and John F. Wellington\n | title = The Minimum Sum of Absolute Errors Regression: A State of the Art Survey\n | journal = [[International Statistical Review]]\n | volume = 50\n | issue = 3\n | year = 1982\n | pages = 317–326\n | jstor = 1402501\n | doi = 10.2307/1402501\n }}\n* {{Cite journal\n | author = Robert F. Phillips\n | title = Least absolute deviations estimation via the EM algorithm\n | journal =  [[Statistics and Computing]]\n | volume = 12\n | issue = 3\n |date=July 2002\n | doi = 10.1023/A:1020759012226\n | pages = 281–285\n}}\n* {{Cite journal\n | author = Enno Siemsen & Kenneth A. Bollen\n | title = Least Absolute Deviation Estimation in Structural Equation Modeling\n | journal = [[Sociological Methods & Research]]\n | volume = 36\n | issue = 2\n | pages = 227–265\n | year = 2007\n | doi = 10.1177/0049124107301946\n | url = http://smr.sagepub.com/cgi/content/abstract/36/2/227\n}}\n\n{{DEFAULTSORT:Least Absolute Deviations}}\n[[Category:Least squares]]\n[[Category:Robust statistics]]\n[[Category:Robust regression]]\n[[Category:Point estimation performance]]"
    },
    {
      "title": "Least squares adjustment",
      "url": "https://en.wikipedia.org/wiki/Least_squares_adjustment",
      "text": "'''Least squares adjustment''' is a model for the solution of an [[overdetermined system]] of equations based on the principle of [[least squares]] of [[observation residuals]]. It is used extensively in the disciplines of [[surveying]], [[geodesy]], and [[photogrammetry]]—the field of [[geomatics]], collectively.\n\n==Formulation==\nThere are three forms of least squares adjustment: ''parametric'', ''conditional'', and ''combined''. In '''parametric adjustment''', one can find an observation equation ''h(X)=Y'' relating observations ''Y'' explicitly in terms of parameters ''X'' (leading to the A-model below). In '''conditional adjustment''', there exists a condition equation ''g(Y)=0'' involving only observations ''Y'' (leading to the B-model below) — with no parameters ''X'' at all. Finally, in a '''combined adjustment''', both parameters ''X'' and observations ''Y'' are involved implicitly in a mixed-model equation ''f(X,Y)=0''. Clearly, parametric and conditional adjustments correspond to the more general combined case when ''f(X,Y)=h(X)-Y'' and ''f(X,Y)=g(Y)'', respectively. Yet the special cases warrant simpler solutions, as detailed below. Often in the literature, ''Y'' may be denoted ''L''.\n\n==Solution==\nThe equalities above only hold for the estimated parameters <math>\\hat{X}</math> and observations <math>\\hat{Y}</math>, thus <math>f\\left(\\hat{X},\\hat{Y}\\right)=0</math>. In contrast, measured observations <math>\\tilde{Y}</math> and approximate parameters <math>\\tilde{X}</math> produce a nonzero ''misclosure'':\n:<math>\\tilde{w} = f\\left(\\tilde{X},\\tilde{Y}\\right).</math>\nOne can proceed to [[Taylor series expansion]] of the equations, which results in the [[Jacobian matrix and determinant|Jacobians]] or [[design matrix|design matrices]]: the first one,\n:<math>A=\\partial{f}/\\partial{X};</math>\nand the second one,\n:<math>B=\\partial{f}/\\partial{Y}.</math>\nThe linearized model then reads:\n:<math>\\tilde{w} + A \\hat{x} + B \\hat{y} = 0,</math>\nwhere <math>\\hat{x}=\\hat{X}-\\tilde{X}</math> are estimated ''parameter corrections'' to the ''a priori'' values, and <math>\\hat{y}=\\hat{Y}-\\tilde{Y}</math> are post-fit ''observation [[Errors and residuals in statistics|residuals]]''.\n\nIn the parametric adjustment, the second design matrix is an identity, ''B=-I'', and the misclosure vector can be interpreted as the pre-fit residuals, <math>\\tilde{y}=\\tilde{w}=h(\\tilde{X})-\\tilde{Y}</math>, so the system simplifies to:\n:<math>A \\hat{x} = \\hat{y} - \\tilde{y},</math>\nwhich is in the form of [[ordinary least squares]]. \nIn the conditional adjustment, the first design matrix is null, ''A=0''.\nFor the more general cases, [[Lagrange multipliers]] are introduced to relate the two Jacobian matrices and transform the [[Constraint (mathematics)|constrained]] least squares problem into an unconstrained one (albeit a larger one). In any case, their manipulation leads to the <math>\\hat{X}</math> and <math>\\hat{Y}</math> vectors as well as the respective parameters and observations ''a posteriori'' covariance matrices.\n\n===Computation===\nGiven the matrices and vectors above, their solution is found via standard least-squares methods; e.g., forming the [[normal matrix]] and applying [[Cholesky decomposition]], applying the [[QR factorization]] directly to the Jacobian matrix, [[iterative methods]] for very large systems, etc.\n\n==Worked-out examples==\n{{expand section|date=June 2014}}\n\n==Applications==\n* [[Leveling]], [[Traverse (surveying)|traverse]], and [[control networks]]\n* [[Bundle adjustment]]\n* [[Triangulation]], [[Trilateration]], [[wikt:Triangulateration|Triangulateration]]\n* [[GPS]]/[[GNSS positioning]]\n* [[Helmert transformation]]\n\n==Related concepts==\n*Parametric adjustment is similar to most of [[regression analysis]] and coincides with the [[Gauss–Markov model]]\n*Combined adjustment, also known as the [[Gauss–Helmert model]],<ref>\"Gauss-Helmert Model\" in: Samuel Kotz; N. Balakrishnan; Campbell Read Brani Vidakovic (2006), ''Encyclopedia of statistical sciences'', Wiley. doi:10.1002/0471667196.ess0854</ref><ref>J Cothren (2005), \"Reliability in Constrained Gauss–Markov Models\", Report No. 473. Department of Civil and Environmental Engineering and Geodetic Science. The Ohio State University. [https://earthsciences.osu.edu/sites/earthsciences.osu.edu/files/report-473.pdf], eq.(2.31), p.8</ref> is related to the [[errors-in-variables models]]<ref>Snow, Kyle, Topics in Total Least-Squares Adjustment within the Errors-In-Variables Model: Singular Cofactor Matrices and Prior Information [pdf], vii+90 pp, December 2012. [https://earthsciences.osu.edu/sites/earthsciences.osu.edu/files/report-502.pdf]</ref>\n\n*The use of ''a priori'' parameter covariance matrix is akin to [[Tikhonov regularization]]\n\n==Extensions==\nIf [[rank deficiency]] is encountered, it can often be rectified by the inclusion of additional equations imposing constraints on the parameters and/or observations, leading to [[constrained least squares]].\n\n==References==\n{{Reflist}}\n\n==Bibliography==\n{{more footnotes|date=June 2014}}\n;Lecture notes and technical reports:\n*Nico Sneeuw and Friedhelm Krum, [https://web.archive.org/web/20140714115230/http://www.uni-stuttgart.de/gi/education/BSC/19820_Ausgleichungsrechnung/skript_prelim.pdf \"Adjustment theory\"], Geodätisches Institut, [[Universität Stuttgart]], 2014\n*Krakiwsky, [http://www2.unb.ca/gge/Pubs/LN42.pdf \"A synthesis of recent advances in the method of least squares\"], Lecture Notes #42, Department of Geodesy and Geomatics Engineering, [[University of New Brunswick]], 1975\n*Cross, P.A. [ftp://stella.ncl.ac.uk/pub/Fugro/Working%20Paper%20No6%20-%20P%20A%20Cross.pdf \"Advanced least squares applied to position-fixing\"], [[University of East London]], School of Surveying, Working Paper No. 6, ISSN 0260-9142, January 1994. First edition April 1983, Reprinted with corrections January 1990. (Original Working Papers, [[North East London Polytechnic]], Dept. of Surveying, 205 pp., 1983.)\n*Snow, Kyle B., [https://earthsciences.osu.edu/sites/earthsciences.osu.edu/files/report-465.pdf Applications of Parameter Estimation and Hypothesis Testing to GPS Network Adjustments], Division of Geodetic Science, [[Ohio State University]], 2002\n\n;Books and chapters:\n* [[Reino Antero Hirvonen]], \"Adjustments by least squares in geodesy and photogrammetry\", Ungar, New York. 261 p., {{ISBN|0804443971}}, {{ISBN|978-0804443975}}, 1971.\n*Edward M. Mikhail, Friedrich E. Ackermann, \"Observations and least squares\", University Press of America, 1982\n*{{cite book | doi = 10.1007/978-1-4615-2067-2_16 | chapter=Survey Measurement Adjustments by Least Squares | title=The Surveying Handbook | date=1995 | pages=383–413 | first=Paul R. | last=Wolf}}\n* [[Peter Vaníček]] and E.J. Krakiwsky, \"Geodesy: The Concepts.\" Amsterdam: Elsevier. (third ed.): {{ISBN|0-444-87777-0}}, {{ISBN|978-0-444-87777-2}}; chap. 12, \"Least-squares solution of overdetermined models\", pp.&nbsp;202–213, 1986.\n* [[Gilbert Strang]] and Kai Borre, \"Linear Algebra, Geodesy, and GPS\", SIAM, 624 pages, 1997.\n*Paul Wolf and Bon DeWitt, \"Elements of Photogrammetry with Applications in GIS\", McGraw-Hill, 2000\n*Karl-Rudolf Koch, \"Parameter Estimation and Hypothesis Testing in Linear Models\", 2a ed., Springer, 2000\n*P.J.G. Teunissen, \"Adjustment theory, an introduction\", Delft Academic Press, 2000\n*Edward M. Mikhail, James S. Bethel, J. Chris McGlone, \"Introduction to Modern Photogrammetry\", Wiley, 2001\n*Harvey, Bruce R., \"Practical least squares and statistics for surveyors\", Monograph 13, Third Edition, School of Surveying and Spatial Information Systems, University of New South Wales, 2006\n*Huaan Fan, \"Theory of Errors and Least Squares Adjustment\", Royal Institute of Technology (KTH), Division of Geodesy and Geoinformatics, Stockholm, Sweden, 2010, {{ISBN|91-7170-200-8}}.\n*{{Cite book | doi = 10.1007/978-3-540-72680-7_2| chapter = Mathematics and Statistics| title = Springer Handbook of Geographic Information| pages = 7| year = 2011| last1 = Gielsdorf | first1 = F. | last2 = Hillmann | first2 = T. | isbn = 978-3-540-72678-4}}\n*Charles D. Ghilani, \"Adjustment Computations: Spatial Data Analysis\", John Wiley & Sons, 2011\n*Charles D. Ghilani and Paul R. Wolf, \"Elementary Surveying: An Introduction to Geomatics\", 13th Edition, Prentice Hall, 2011\n*Erik Grafarend and Joseph Awange, \"Applications of Linear and Nonlinear Models: Fixed Effects, Random Effects, and Total Least Squares\", Springer, 2012\n*Alfred Leick, Lev Rapoport, and Dmitry Tatarnikov, \"GPS Satellite Surveying\", 4th Edition, John Wiley & Sons, {{ISBN|9781119018612}}; Chapter 2, \"Least-Squares Adjustments\", pp.&nbsp;11–79, doi:10.1002/9781119018612.ch2\n*A. Fotiou (2018) \"A Discussion on Least Squares Adjustment with Worked Examples\" In: Fotiou A., D. Rossikopoulos, eds. (2018): “Quod erat demonstrandum. In quest for the ultimate geodetic insight.” Special issue for Professor Emeritus Athanasios Dermanis. Publication of the  School of Rural and Surveying Engineering, Aristotle Universsity of Thessaloniki, 405 pages. {{ISBN|978-960-89704-4-1}} [https://www.topo.auth.gr/main/images/pdf/TOMOS_DERMANIS/04_Fotiou.pdf]\n\n\n[[Category:Least squares]]\n[[Category:Geodesy]]\n[[Category:Surveying]]\n[[Category:Photogrammetry]]"
    },
    {
      "title": "Least-squares support-vector machine",
      "url": "https://en.wikipedia.org/wiki/Least-squares_support-vector_machine",
      "text": "{{context|date=November 2010}}\n'''Least-squares support-vector machines (LS-SVM)''' are [[least-squares]] versions of [[support-vector machine]]s (SVM), which are a set of related [[supervised learning]] methods that analyze data and recognize patterns, and which are used for [[statistical classification|classification]] and [[regression analysis]].  In this version one finds the solution by solving a set of [[linear equation]]s instead of a convex [[quadratic programming]] (QP) problem for classical SVMs. Least-squares SVM classifiers were proposed by Suykens and Vandewalle.<ref>Suykens, J. A. K.; Vandewalle, J. (1999) \"Least squares support vector machine classifiers\", ''Neural Processing Letters'', 9 (3), 293–300.</ref> LS-SVMs are a class of [[Kernel methods|kernel-based learning methods]].\n\n==From support-vector machine to least-squares support-vector machine==\nGiven a training set <math> \\{ x_i ,y_i \\}_{i = 1}^N</math> with input data <math> x_i \\in \\mathbb{R}^n</math> and corresponding binary class labels <math>y_i \\in \\{ -1, +1 \\}</math>, the [[Support-vector machine|SVM]]<ref>Vapnik, V. The nature of statistical learning theory. Springer-Verlag, New York, 1995.</ref> classifier, according to [[Vapnik]]’s original formulation, satisfies the following conditions:\n\n[[File:Data spiral.png|thumb|right|250px|The spiral data: <math>y_i = 1</math> for blue data point, <math>y_i = -1</math> for red data point]]\n\n: <math>\n\\begin{cases}\n   w^T \\phi (x_i ) + b \\ge 1, & \\text{if } \\quad y_i  =  +1, \\\\\n   w^T \\phi (x_i ) + b \\le  - 1, & \\text{if } \\quad y_i  =  -1,\n\\end{cases}</math>\n\nwhich is equivalent to\n\n: <math>y_i \\left[ {w^T \\phi (x_i ) + b} \\right] \\ge 1,\\quad i = 1, \\ldots, N,</math>\n\nwhere <math>\\phi(x)</math> is the nonlinear map from original space to the high- or infinite-dimensional space.\n\n===Inseparable data===\nIn case such a separating hyperplane does not exist, we introduce so-called slack variables <math>\\xi_i</math> such that\n\n: <math> \\begin{cases}\n   y_i \\left[ {w^T \\phi (x_i ) + b} \\right] \\ge 1 - \\xi _i , & i = 1, \\ldots, N, \\\\\n   \\xi _i  \\ge 0, & i = 1, \\ldots, N.\n\\end{cases}</math>\n\nAccording to the [[structural risk minimization]] principle, the risk bound is minimized by the following minimization problem:\n\n: <math>\\min J_1 (w,\\xi )=\\frac{1}{2}w^T w + c\\sum\\limits_{i = 1}^N \\xi_i ,</math>\n\n: <math>\\text{Subject to } \\begin{cases}\n   y_i \\left[ {w^T \\phi (x_i ) + b} \\right] \\ge 1 - \\xi _i , & i = 1, \\ldots, N, \\\\\n   \\xi _i  \\ge 0, & i = 1, \\ldots ,N ,\n\\end{cases}</math>\n[[File:Data spiral svm.png|thumb|right|250px|The result of the SVM classifier]]\n\nTo solve this problem, we could construct the [[Lagrange multipliers|Lagrangian function]]:\n\n:<math> L_1(w,b,\\xi,\\alpha,\\beta)=\\frac{1}{2}w^T w + c\\sum\\limits_{i = 1}^N {\\xi _i } - \\sum\\limits_{i=1}^N \\alpha_i \\left\\{ y_i \\left[ {w^T \\phi (x_i ) + b} \\right] - 1 + \\xi _i \\right\\} - \\sum\\limits_{i=1}^N \\beta_i \\xi_i, </math>\n\nwhere <math>\\alpha_i \\ge 0,\\ \\beta _i \\ge 0\\ (i = 1, \\ldots, N)</math> are the [[Lagrangian multipliers]]. The optimal point will be in the [[saddle point]] of the Lagrangian function, and then we obtain\n\n: <math> \\begin{cases}\n \\frac{ \\partial L_1 }{\\partial w} = 0\\quad  \\to \\quad w = \\sum\\limits_{i = 1}^N \\alpha _i y_i \\phi (x_i )  ,\\\\\n \\frac{\\partial L_1 }{\\partial b} = 0\\quad  \\to \\quad \\sum\\limits_{i = 1}^N \\alpha _i y_i = 0 ,\\\\\n \\frac{\\partial L_1 }{\\partial \\xi _i } = 0\\quad  \\to \\quad 0 \\le \\alpha _i  \\le c,\\;i = 1, \\ldots ,N .\n \\end{cases} </math>\n\nBy substituting <math>w</math> by its expression in the Lagrangian formed from the appropriate objective and constraints, we will get the following quadratic programming problem:\n\n: <math> \\max Q_1(\\alpha) = -\\frac{1}{2}\\sum\\limits_{i,j = 1}^N {\\alpha _i \\alpha _j y_i y_j K(x_i ,x_j )}  + \\sum\\limits_{i = 1}^N \\alpha_i, </math>\n\nwhere <math>K(x_i ,x_j ) = \\left\\langle \\phi (x_i ), \\phi (x_j) \\right\\rangle</math> is called the [[kernel function]]. Solving this QP problem subject to constraints in (8), we will get the [[hyperplane]] in the high-dimensional space and hence the [[Hierarchical classifier|classifier]] in the original space.\n\n===Least-squares SVM formulation===\nThe least-squares version of the SVM classifier is obtained by reformulating the minimization problem as\n\n: <math>\\min J_2(w,b,e) = \\frac{\\mu}{2} w^T w + \\frac{\\zeta}{2}\\sum\\limits_{i = 1}^N e_i^2,</math>\n\nsubject to the equality constraints\n\n: <math>y_i \\left[ {w^T \\phi (x_i ) + b} \\right] = 1 - e_{i} ,\\quad i = 1, \\ldots ,N .</math>\n\nThe least-squares SVM (LS-SVM) classifier formulation above implicitly corresponds to a [[Regression analysis|regression]] interpretation with binary targets <math>y_i = \\pm 1</math>.\n\nUsing <math>y_i^2 = 1</math>, we have\n\n: <math>\\sum\\limits_{i = 1}^N e_i^2 = \\sum\\limits_{i = 1}^N (y_i e_i)^2 = \\sum\\limits_{i = 1}^N e_i^2 = \\sum\\limits_{i = 1}^N \\left( y_i - (w^T \\phi(x_i) + b) \\right)^2,</math>\n\nwith <math> e_i = y_i - (w^T \\phi(x_i) + b).</math> Notice, that this error would also make sense for least-squares data fitting, so that the same end results holds for the regression case.\n\nHence the LS-SVM classifier formulation is equivalent to\n\n: <math>J_2(w,b,e) = \\mu E_W + \\zeta E_D</math>\n\nwith <math>E_W = \\frac{1}{2} w^T w</math> and <math>E_D = \\frac{1}{2} \\sum\\limits_{i = 1}^N e_i^2 = \\frac{1}{2} \\sum\\limits_{i = 1}^N \\left(y_i - (w^T \\phi(x_i) + b) \\right)^2.</math>\n\n[[File:Data spiral lssvm.png|thumb|right|250px|The result of the LS-SVM classifier]]\n\nBoth <math>\\mu</math> and <math>\\zeta</math> should be considered as hyperparameters to tune the amount of regularization versus the sum squared error. The solution does only depend on the ratio <math>\\gamma = \\zeta / \\mu</math>, therefore the original formulation uses only <math>\\gamma</math> as tuning parameter. We use both <math>\\mu</math> and <math>\\zeta</math> as parameters in order to provide a Bayesian interpretation to LS-SVM.\n\nThe solution of LS-SVM regressor will be obtained after we construct the [[Lagrange multipliers|Lagrangian function]]:\n\n: <math>\\begin{cases}\n L_2 (w,b,e,\\alpha )\\; = J_2 (w,e) - \\sum\\limits_{i = 1}^N \\alpha _i \\left\\{ { \\left[ {w^T \\phi (x_i ) + b} \\right] + e_i - y_i } \\right\\}  ,\\\\\n \\quad \\quad \\quad \\quad \\quad \\; = \\frac{1}{2}w^T w + \\frac{\\gamma }{2} \\sum\\limits_{i = 1}^N e_i^2 - \\sum\\limits_{i = 1}^N \\alpha _i \\left\\{ \\left[ w^T \\phi (x_i ) + b \\right] + e_i -y_i \\right\\} ,\n \\end{cases}</math>\n\nwhere <math>\\alpha_i \\in \\mathbb{R}</math> are the Lagrange multipliers. The conditions for optimality are\n\n: <math> \\begin{cases}\n \\frac{\\partial L_2 }{\\partial w} = 0\\quad  \\to \\quad w = \\sum\\limits_{i = 1}^N \\alpha _i \\phi (x_i ) , \\\\\n \\frac{\\partial L_2 }{\\partial b} = 0\\quad  \\to \\quad \\sum\\limits_{i = 1}^N \\alpha _i   = 0 ,\\\\\n \\frac{\\partial L_2 }{\\partial e_i } = 0\\quad  \\to \\quad \\alpha _i  =  \\gamma e_i ,\\;i = 1, \\ldots ,N ,\\\\\n \\frac{\\partial L_2 }{\\partial \\alpha _i } = 0\\quad  \\to \\quad y_i  = w^T \\phi (x_i ) + b + e_i ,\\,i = 1, \\ldots ,N .\n \\end{cases} </math>\n\nElimination of <math>w</math> and <math>e</math> will yield a [[linear system]] instead of a [[quadratic programming]] problem:\n\n: <math> \\left[ \\begin{matrix}\n   0 & 1_N^T  \\\\\n   1_N & \\Omega  + \\gamma ^{ - 1} I_N\n\\end{matrix} \\right] \\left[ \\begin{matrix}\n   b  \\\\\n   \\alpha\n\\end{matrix} \\right] = \\left[ \\begin{matrix}\n   0  \\\\\n   Y\n\\end{matrix} \\right] ,</math>\n\nwith <math>Y = [y_1 , \\ldots ,y_N ]^T</math>, <math>1_N  = [1, \\ldots ,1]^T</math> and <math>\\alpha  = [\\alpha _1 , \\ldots ,\\alpha _N ]^T</math>. Here, <math>I_N</math> is an <math>N \\times N</math> [[identity matrix]], and <math>\\Omega  \\in \\mathbb{R}^{N \\times N}</math> is the kernel matrix defined by <math>\\Omega _{ij}  = \\phi (x_i )^T \\phi (x_j ) = K(x_i ,x_j )</math>.\n\n===Kernel function ''K''===\nFor the kernel function ''K''(•, •) one typically has the following choices:\n* [[Linear]] kernel : <math>K(x,x_i ) = x_i^T x,</math>\n* [[Polynomial]] kernel of degree <math>d</math>: <math>K(x,x_i ) = \\left( {1 + x_i^T x/c} \\right)^d ,</math>\n* [[Radial basis function]] RBF kernel : <math>K(x,x_i ) = \\exp \\left( { - \\left\\| {x - x_i } \\right\\|^2 /\\sigma ^2 } \\right),</math>\n* MLP kernel : <math>K(x,x_i ) = \\tanh \\left( {k\\,x_i^T x + \\theta } \\right),</math>\n\nwhere <math>d</math>, <math>c</math>, <math>\\sigma</math>, <math>k</math> and <math>\\theta</math> are constants. Notice that the Mercer condition holds for all <math>c, \\sigma \\in \\mathbb{R}^+</math> and <math>d \\in N</math> values in the [[polynomial]] and RBF case, but not for all possible choices of <math>k</math> and <math>\\theta</math> in the MLP case. The scale parameters <math>c</math>, <math>\\sigma</math> and <math>k</math> determine the scaling of the inputs in the polynomial, RBF and MLP [[kernel function]]. This scaling is related to the bandwidth of the kernel in [[statistics]], where it is shown that the bandwidth is an important parameter of the generalization behavior of a kernel method.\n\n==Bayesian interpretation for LS-SVM==\nA [[Bayesian probability|Bayesian]] interpretation of the SVM has been proposed by Smola et al. They showed that the use of different kernels in SVM can be regarded as defining different [[prior probability]] distributions on the functional space, as <math>P[f] \\propto \\exp \\left( { - \\beta \\left\\| {\\hat Pf} \\right\\|^2 } \\right)</math>. Here <math>\\beta>0</math> is a constant and <math>\\hat{P}</math> is the regularization operator corresponding to the selected kernel.\n\nA general Bayesian evidence framework was developed by MacKay,<ref>MacKay, D. J. C. Bayesian Interpolation. Neural Computation, 4(3): 415–447, May 1992.</ref><ref>MacKay, D. J. C. A practical Bayesian framework for backpropagation networks. Neural Computation, 4(3): 448–472, May 1992.</ref><ref>MacKay, D. J. C. The evidence framework applied to classification networks. Neural Computation, 4(5): 720–736, Sep. 1992.</ref> and MacKay has used it to the problem of regression, forward [[neural network]] and classification network. Provided data set <math>D</math>, a model <math>\\mathbb{M}</math> with parameter vector <math>w</math> and a so-called hyperparameter or regularization parameter <math>\\lambda</math>, [[Bayesian inference]] is constructed with 3 levels of inference:\n* In level 1, for a given value of <math>\\lambda</math>, the first level of inference infers the posterior distribution of <math>w</math> by Bayesian rule\n::<math>p(w|D,\\lambda ,\\mathbb{M}) \\propto p(D|w,\\mathbb{M})p(w|\\lambda ,\\mathbb{M}).</math>\n* The second level of inference determines the value of <math>\\lambda</math>, by maximizing\n::<math>p(\\lambda |D,\\mathbb{M}) \\propto p(D|\\lambda ,\\mathbb{M})p(\\lambda |\\mathbb{M}).</math>\n* The third level of inference in the evidence framework ranks different models by examining their posterior probabilities\n::<math>p(\\mathbb{M}|D) \\propto p(D|\\mathbb{M})p(\\mathbb{M}).</math>\n\nWe can see that Bayesian evidence framework is a unified theory for [[learning]] the model and model selection.\nKwok used the Bayesian evidence framework to interpret the formulation of SVM and model selection. And he also applied Bayesian evidence framework to support vector regression.\n\nNow, given the data points <math> \\{ x_i ,y_i \\} _{i = 1}^N</math> and the hyperparameters <math>\\mu</math> and <math>\\zeta</math> of the model <math>\\mathbb{M}</math>, the model parameters <math>w</math> and <math>b</math> are estimated by maximizing the posterior <math>p(w,b|D,\\log \\mu ,\\log \\zeta ,\\mathbb{M})</math>. Applying Bayes’ rule, we obtain\n\n:<math>p(w,b|D,\\log \\mu ,\\log \\zeta ,\\mathbb{M}) = \\frac{{p(D|w,b,\\log \\mu ,\\log \\zeta ,\\mathbb{M})p(w,b|\\log \\mu ,\\log \\zeta ,\\mathbb{M})}}{{p(D|\\log \\mu ,\\log \\zeta ,\\mathbb{M})}},\n</math>\n\nwhere <math>p(D|\\log \\mu ,\\log \\zeta ,\\mathbb{M})</math> is a normalizing constant such the integral over all possible <math>w</math> and <math>b</math> is equal to 1.\nWe assume <math>w</math> and <math>b</math> are independent of the hyperparameter <math>\\zeta</math>, and are conditional independent, i.e., we assume\n:<math>p(w,b|\\log \\mu ,\\log \\zeta ,\\mathbb{M}) = p(w|\\log \\mu ,\\mathbb{M})p(b|\\log \\sigma _b ,\\mathbb{M}).\n</math>\n\nWhen <math>\\sigma _b  \\to \\infty</math>, the distribution of <math>b</math> will approximate a uniform distribution. Furthermore, we assume <math>w</math> and <math>b</math> are Gaussian distribution, so we obtain the a priori distribution of <math>w</math> and <math>b</math> with <math>\\sigma _b  \\to \\infty</math> to be\n\n: <math>\\begin{array}{l}\n p(w,b|\\log \\mu ,) = \\left( {\\frac{\\mu }{{2\\pi }}} \\right)^{\\frac{{n_f }}{2}} \\exp \\left( { - \\frac{\\mu }{2}w^T w} \\right)\\frac{1}{{\\sqrt {2\\pi \\sigma _b } }}\\exp \\left( { - \\frac{{b^2 }}{{2\\sigma _b }}} \\right) \\\\\n \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\propto \\left( {\\frac{\\mu }{{2\\pi }}} \\right)^{\\frac{{n_f }}{2}} \\exp \\left( { - \\frac{\\mu }{2}w^T w} \\right)\n \\end{array} .</math>\n\nHere <math>n_f</math> is the dimensionality of the feature space, same as the dimensionality of <math>w</math>.\n\nThe probability of <math>p(D|w,b,\\log \\mu ,\\log \\zeta ,\\mathbb{M})</math> is assumed to depend only on <math>w,b,\\zeta</math> and <math>\\mathbb{M}</math>. We assume that the data points are independently identically distributed (i.i.d.), so that:\n\n: <math>p(D|w,b,\\log \\zeta ,\\mathbb{M}) = \\prod\\limits_{i = 1}^N {p(x_i ,y_i |w,b,\\log \\zeta ,\\mathbb{M})} .</math>\n\nIn order to obtain the least square cost function, it is assumed that the probability of a data point is proportional to:\n\n: <math>p(x_i ,y_i |w,b,\\log \\zeta ,\\mathbb{M}) \\propto p(e_i |w,b,\\log \\zeta ,\\mathbb{M}) .</math>\n\nA Gaussian distribution is taken for the errors <math>e_i  = y_i  - (w^T \\phi (x_i ) + b)</math> as:\n\n: <math>p(e_i |w,b,\\log \\zeta ,\\mathbb{M}) = \\sqrt {\\frac{\\zeta }{{2\\pi }}} \\exp \\left( { - \\frac{{\\zeta e_i^2 }}{2}} \\right) .</math>\n\nIt is assumed that the <math>w</math> and <math>b</math> are determined in such a way that the class centers <math>\\hat m_ - </math> and <math>\\hat m_ +</math> are mapped onto the target -1 and +1, respectively. The projections <math>w^T \\phi (x) + b</math> of the class elements <math>\\phi(x)</math> follow a multivariate Gaussian distribution, which have variance <math>1/ \\zeta</math>.\n\nCombining the preceding expressions, and neglecting all constants, Bayes’ rule becomes\n\n: <math>p(w,b|D,\\log \\mu ,\\log \\zeta ,\\mathbb{M}) \\propto \\exp ( - \\frac{\\mu }{2}w^T w - \\frac{\\zeta }{2}\\sum\\limits_{i = 1}^N {e_i^2 } ) = \\exp ( - J_2 (w,b)) .</math>\n\nThe maximum posterior density estimates <math>w_{MP}</math> and <math>b_{MP}</math> are then be obtained by minimizing the negative logarithm of (26), so we arrive (10).\n\n==References==\n<references />\n\n==Bibliography==\n* J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, J. Vandewalle, Least Squares Support Vector Machines, World Scientific Pub. Co., Singapore, 2002. {{ISBN|981-238-151-1}}\n* Suykens J. A. K., Vandewalle J., Least squares support vector machine classifiers, ''Neural Processing Letters'', vol. 9, no. 3, Jun. 1999, pp.&nbsp;293–300.\n* Vladimir Vapnik. ''The Nature of Statistical Learning Theory''. Springer-Verlag, 1995. {{ISBN|0-387-98780-0}}\n* MacKay, D. J. C., Probable networks and plausible predictions—A review of practical Bayesian methods for supervised neural networks. ''Network: Computation in Neural Systems'', vol. 6, 1995, pp.&nbsp;469–505.\n\n==External links==\n* [http://www.esat.kuleuven.be/sista/lssvmlab/ www.esat.kuleuven.be/sista/lssvmlab/] \"Least squares support vector machine Lab (LS-SVMlab) toolbox contains Matlab/C implementations for a number of LS-SVM algorithms\".\n* [http://www.kernel-machines.org www.kernel-machines.org] \"Support Vector Machines and Kernel based methods (Smola & Schölkopf)\".\n* [http://www.gaussianprocess.org/ www.gaussianprocess.org] \"Gaussian Processes: Data modeling using Gaussian Process priors over functions for regression and classification (MacKay, Williams)\".\n* [http://www.support-vector.net www.support-vector.net] \"Support Vector Machines and kernel based methods (Cristianini)\".\n* [http://dlib.net/ml.html#krr_trainer dlib]: Contains a least-squares SVM implementation for large-scale datasets.\n\n[[Category:Support vector machines]]\n[[Category:Classification algorithms]]\n[[Category:Statistical classification]]\n[[Category:Least squares]]"
    },
    {
      "title": "Linear least squares",
      "url": "https://en.wikipedia.org/wiki/Linear_least_squares",
      "text": "{{Regression bar}}\n\n'''Linear least squares''' ('''LLS''') is the [[least squares approximation]] of [[linear functions]] to data.\nIt is a set of formulations for solving statistical problems involved in [[linear regression]], including variants for \n[[Ordinary least squares|ordinary (unweighted)]],\n[[Weighted least squares|weighted]], and \n[[Generalized least squares|generalized (correlated)]] [[residuals (statistics)|residuals]].\n[[Numerical methods for linear least squares]] include inverting the matrix of the normal equations and orthogonal decomposition methods.\n\n==Main formulations==\n\nThe three main linear least squares formulations are:\n\n{{unordered list<!--\n the reason for {{unordered list}} instead of wiki-formatting is that some items on this list are spanning several paragraphs.\n -->\n|1=  '''[[Ordinary least squares]]''' (OLS) is the most common estimator. OLS estimates are commonly used to analyze both [[experiment]]al and [[observational study|observational]] data.\n\nThe OLS method minimizes the sum of squared [[Errors and residuals in statistics|residuals]], and leads to a closed-form expression for the estimated value of the unknown parameter vector ''β'':\n: <math>\n  \\hat{\\boldsymbol\\beta} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y},\n  </math>\n\nwhere <math>\\mathbf{y}</math> is a vector whose ''i''th element is the ''i''th observation of the [[dependent variable]], and <math>\\mathbf{X}</math> is a matrix whose ''ij'' element is the ''i''th observation of the ''j''th [[independent variable]]. The estimator is [[bias of an estimator|unbiased]] and [[consistent estimator|consistent]] if the errors have finite variance and are uncorrelated with the regressors:<ref>{{cite journal | last1=Lai | first1=T.L. | last2=Robbins | first2=H. | last3=Wei | first3=C.Z. | journal=[[Proceedings of the National Academy of Sciences|PNAS]] | year=1978 | volume=75 | title=Strong consistency of least squares estimates in multiple regression | issue=7 | pages=3034–3036 | doi= 10.1073/pnas.75.7.3034 | pmid=16592540 | jstor=68164 | bibcode=1978PNAS...75.3034L | pmc=392707 }}</ref>\n: <math>\n \\operatorname{E}[\\,\\mathbf{x}_i\\varepsilon_i\\,] = 0,\n </math>\nwhere <math>\\mathbf{x}_i</math> is the transpose of row ''i'' of the matrix <math>\\mathbf{X}.</math> It is also [[efficiency (statistics)|efficient]] under the assumption that the errors have finite variance and are [[Homoscedasticity|homoscedastic]], meaning that E[''ε''<sub>''i''</sub><sup>2</sup>{{!}}'''x'''<sub>''i''</sub>] does not depend on ''i''. The condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment, but in the case of observational data, it is difficult to exclude the possibility of an omitted covariate ''z'' that is related to both the observed covariates and the response variable. The existence of such a covariate will generally lead to a correlation between the regressors and the response variable, and hence to an inconsistent estimator of '''β'''. The condition of homoscedasticity can fail with either experimental or observational data. If the goal is either inference or predictive modeling, the performance of OLS estimates can be poor if [[multicollinearity]] is present, unless the sample size is large.\n\n|2= '''[[Weighted least squares]]''' (WLS) are used when [[heteroscedasticity]] is present in the error terms of the model.\n\n|3= '''[[Generalized least squares]]''' (GLS) is an extension of the OLS method, that allows efficient estimation of ''β'' when either [[heteroscedasticity]], or correlations, or both are present among the error terms of the model, as long as the form of heteroscedasticity and correlation is known independently of the data. To handle heteroscedasticity when the error terms are uncorrelated with each other, GLS minimizes a weighted analogue to the sum of squared residuals from OLS regression, where the weight for the ''i''<sup>th</sup> case is inversely proportional to var(''ε''<sub>''i''</sub>). This special case of GLS is called \"weighted least squares\". The GLS solution to estimation problem is\n: <math>\n \\hat{\\boldsymbol\\beta} = (\\mathbf{X}^\\mathsf{T} \\boldsymbol\\Omega^{-1} \\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol\\Omega^{-1}\\mathbf{y},\n </math>\nwhere '''Ω''' is the covariance matrix of the errors. GLS can be viewed as applying a linear transformation to the data so that the assumptions of OLS are met for the transformed data. For GLS to be applied, the covariance structure of the errors must be known up to a multiplicative constant.\n}}\n\n==Alternative formulations==\nOther formulations include:\n{{unordered list<!--\n the reason for {{unordered list}} instead of wiki-formatting is that some items on this list are spanning several paragraphs.\n -->\n\n|4= '''[[Iteratively reweighted least squares]]''' (IRLS) is used when [[heteroscedasticity]], or correlations, or both are present among the error terms of the model, but where little is known about the covariance structure of the errors independently of the data.<ref>{{cite journal | title=The Unifying Role of Iterative Generalized Least Squares in Statistical Algorithms | last=del Pino | first=Guido | journal=Statistical Science | volume=4 | year=1989 | pages=394–403 | doi=10.1214/ss/1177012408 | issue=4 | jstor=2245853}}</ref> In the first iteration, OLS, or GLS with a provisional covariance structure is carried out, and the residuals are obtained from the fit. Based on the residuals, an improved estimate of the covariance structure of the errors can usually be obtained. A subsequent GLS iteration is then performed using this estimate of the error structure to define the weights. The process can be iterated to convergence, but in many cases, only one iteration is sufficient to achieve an efficient estimate of ''β''.<ref>{{cite journal | title=Adapting for Heteroscedasticity in Linear Models | last=Carroll | first=Raymond J. | journal=The Annals of Statistics | volume=10 | year=1982 | pages=1224–1233 | doi=10.1214/aos/1176345987 | issue=4 | jstor=2240725}}</ref><ref>{{cite journal | title=Robust, Smoothly Heterogeneous Variance Regression | last=Cohen | first=Michael |author2=Dalal, Siddhartha R. |author3=Tukey, John W.  | journal=Journal of the Royal Statistical Society, Series C | volume=42 | year=1993 | pages=339–353 | issue=2 | jstor=2986237}}</ref>\n\n|5=  '''[[Instrumental variables]]''' regression (IV) can be performed when the regressors are correlated with the errors. In this case, we need the existence of some auxiliary ''instrumental variables'' '''z'''<sub>''i''</sub> such that E['''z'''<sub>''i''</sub>''ε''<sub>''i''</sub>]&nbsp;=&nbsp;0. If '''Z''' is the matrix of instruments, then the estimator can be given in closed form as\n: <math>\n \\hat{\\boldsymbol\\beta} = (\\mathbf{X}^\\mathsf{T}\\mathbf{Z}(\\mathbf{Z}^\\mathsf{T}\\mathbf{Z})^{-1}\\mathbf{Z}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{Z}(\\mathbf{Z}^\\mathsf{T}\\mathbf{Z})^{-1}\\mathbf{Z}^\\mathsf{T}\\mathbf{y}.\n </math>\n'''Optimal instruments''' regression is an extension of classical IV regression to the situation where E[''ε<sub>i</sub>''&nbsp;{{!}}&nbsp;'''z'''<sub>''i''</sub>]&nbsp;=&nbsp;0.\n\n|6=  '''[[Total least squares]]''' (TLS)<ref>{{cite journal | title=Total Least Squares: State-of-the-Art Regression in Numerical Analysis | last=Nievergelt | first=Yves | journal=SIAM Review | volume=36 | year=1994 |pages=258–264 | doi=10.1137/1036055 | issue=2 | jstor=2132463}}</ref> is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in a more geometrically symmetric manner than OLS. It is one approach to handling the \"errors in variables\" problem, and is also sometimes used even when the covariates are assumed to be error-free.\n}}\n\nIn addition, '''percentage least squares''' focuses on reducing percentage errors, which is useful in the field of forecasting or time series analysis. It is also useful in situations where the dependent variable has a wide range without constant variance, as here the larger residuals at the upper end of the range would dominate if OLS were used. When the percentage or relative error is normally distributed, least squares percentage regression provides maximum likelihood estimates. Percentage regression is linked to a multiplicative error model, whereas OLS is linked to models containing an additive error term.<ref>{{cite journal | ssrn = 1406472 | title=Least Squares Percentage Regression | author = Tofallis, C | journal = Journal of Modern Applied Statistical Methods | volume=7 | year = 2009 | pages=526–534 | doi = 10.2139/ssrn.1406472 }}</ref>\n\nIn [[constrained least squares]], one is interested in solving a linear least squares problem with an additional constraint on the solution.\n\n== Objective function ==\nIn OLS (i.e., assuming unweighted observations), the [[mathematical optimization|optimal value]] of the [[objective function]] is found by substituting in the optimal expression for the coefficient vector, can be written as:\n\n:<math>S=\\mathbf y^{\\rm T} (\\mathbf{I} - \\mathbf{H})^{\\rm T} (\\mathbf{I} - \\mathbf{H}) \\mathbf y = \\mathbf y^{\\rm T} (\\mathbf{I} - \\mathbf{H}) \\mathbf y,</math>\n\nwhere <math>\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} </math>, the latter equality holding since <math>(\\mathbf{I} - \\mathbf{H})</math> is symmetric and idempotent. It can be shown from this<ref>{{cite book |title=Statistics in Physical Science |last=Hamilton |first=W. C. |authorlink= |year=1964 |publisher=Ronald Press |location=New York |isbn= |pages= |url= }}</ref> that under an appropriate assignment of weights the [[expected value]] of ''S'' is ''m'' − ''n''.  If instead unit weights are assumed, the expected value of ''S'' is <math>(m - n)\\sigma^2</math>, where <math>\\sigma^2</math> is the variance of each observation.\n\nIf it is assumed that the residuals belong to a normal distribution, the objective function, being a sum of weighted squared residuals, will belong to a [[Chi-squared distribution|chi-squared (<math>\\chi ^2</math>) distribution]] with ''m'' − ''n'' [[Degrees of freedom (statistics)|degrees of freedom]]. Some illustrative percentile values of <math>\\chi ^2</math> are given in the following table.<ref>{{cite book |title=Schaum's outline of theory and problems of probability and statistics |last=Spiegel |first=Murray R. |authorlink= |year=1975 |publisher=McGraw-Hill |location=New York |isbn=978-0-585-26739-5 |pages= |url= }}</ref>\n:<math>\n\\begin{array}{r|ccc}\nm - n &\\chi ^2 _{0.50} &\\chi ^2 _{0.95} &\\chi ^2 _{0.99}\\\\\n\\hline\n10 & 9.34 &18.3 &23.2 \\\\\n25 &24.3 &37.7 &44.3 \\\\\n100 &99.3 &124 &136\n\\end{array}\n</math>\nThese values can be used for a statistical criterion as to the [[goodness of fit]]. When unit weights are used, the numbers should be divided by the variance of an observation.\n\nFor WLS, the ordinary objective function above is replaced for a weighted average of residuals.\n\n==Discussion==\nIn [[statistics]] and [[mathematics]], '''linear least squares''' is an approach to fitting a [[mathematical model|mathematical]] or [[statistical model]] to [[data]] in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown [[parameter]]s of the model.  The resulting fitted model can be used to [[descriptive statistics|summarize]] the data, to [[prediction|predict]] unobserved values from the same system, and to understand the mechanisms that may underlie the system.\n\nMathematically, linear least squares is the problem of approximately solving an [[overdetermined system]] of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values.  The approach is called ''linear'' least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are [[Convex function|convex]] and have a [[closed-form expression|closed-form solution]] that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations.  In contrast, [[non-linear least squares]] problems generally must be solved by an [[iterative method|iterative procedure]], and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the [[Minimum mean square error|Bayesian MMSE estimator]].\n \nIn statistics, linear least squares problems correspond to a particularly important type of [[statistical model]] called [[linear regression]] which arises as a particular form of [[regression analysis]]. One basic form of such a model is an [[ordinary least squares]] model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and [[statistical inference]]s related to these being dealt with in the articles just mentioned. See [[outline of regression analysis]] for an outline of the topic.\n\n== Properties ==\n{{see also|Ordinary least squares#Properties}}\n\nIf the experimental errors, <math>\\epsilon \\,</math>, are uncorrelated, have a mean of zero and a constant variance, <math>\\sigma</math>, the [[Gauss–Markov theorem]] states that the least-squares estimator, <math>\\hat{\\boldsymbol{\\beta}}</math>, has the minimum variance of all estimators that are linear combinations of the observations. In this sense it is the best, or optimal, estimator of the parameters. Note particularly that this property is independent of the statistical [[Cumulative distribution function|distribution function]] of the errors. In other words, ''the distribution function of the errors need not be a [[normal distribution]]''. However, for some probability distributions, there is no guarantee that the least-squares solution is even possible given the observations; still, in such cases it is the best estimator that is both linear and unbiased.\n\nFor example, it is easy to show that the [[arithmetic mean]] of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the Gauss–Markov theorem apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.\n\nHowever, in the case that the experimental errors do belong to a normal distribution, the least-squares estimator is also a [[maximum likelihood]] estimator.<ref>{{cite book |title=The Mathematics of Physics and Chemistry |last=Margenau |first=Henry |authorlink= |author2=Murphy, George Moseley  |year=1956 |publisher=Van Nostrand |location=Princeton |isbn= |pages= |url= }}</ref>\n\nThese properties underpin the use of the method of least squares for all types of data fitting, even when the assumptions are not strictly valid.\n\n=== Limitations ===\nAn assumption underlying the treatment given above is that the independent variable, ''x'', is free of error. In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, [[total least squares]] or more generally [[errors-in-variables models]], or ''rigorous least squares'', should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure.<ref name=\"pg\">{{cite book |title=Data fitting in the Chemical Sciences |last=Gans |first=Peter |authorlink= |year=1992 |publisher=Wiley |location=New York |isbn=978-0-471-93412-7 |pages= |url= }}</ref><ref>{{cite book |title=Statistical adjustment of Data |last=Deming |first=W. E. |authorlink= |year=1943 |publisher=Wiley |location=New York |isbn= |pages= |url= }}</ref>\n\nIn some cases the (weighted) normal equations matrix ''X''<sup>T</sup>''X'' is [[ill-conditioned]]. When fitting polynomials the normal equations matrix is a [[Vandermonde matrix]]. Vandermonde matrices become increasingly ill-conditioned as the order of the matrix increases.{{citation needed|date=December 2010}} In these cases, the least squares estimate amplifies the measurement noise and may be grossly inaccurate.{{citation needed|date=December 2010}} Various [[regularization (mathematics)|regularization]] techniques can be applied in such cases, the most common of which is called [[Tikhonov regularization|ridge regression]]. If further information about the parameters is known, for example, a range of possible values of <math>\\mathbf{\\hat{\\boldsymbol{\\beta}}}</math>, then various techniques can be used to increase the stability of the solution. For example, see [[#Constrained_linear_least_squares|constrained least squares]].\n\nAnother drawback of the least squares estimator is the fact that the norm of the residuals, <math>\\| \\mathbf y - X\\hat{\\boldsymbol{\\beta}} \\|</math> is minimized, whereas in some cases one is truly interested in obtaining small error in the parameter <math>\\mathbf{\\hat{\\boldsymbol{\\beta}}}</math>, e.g., a small value of <math>\\|{\\boldsymbol{\\beta}}-\\hat{\\boldsymbol{\\beta}}\\|</math>.{{citation needed|date=December 2010}} However, since the true parameter <math>{\\boldsymbol{\\beta}}</math> is necessarily unknown, this quantity cannot be directly minimized. If a [[prior probability]] on <math>\\hat{\\boldsymbol{\\beta}}</math> is known, then a [[Minimum mean square error|Bayes estimator]] can be used to minimize the [[mean squared error]], <math>E \\left\\{ \\| {\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}} \\|^2 \\right\\} </math>. The least squares method is often applied when no prior is known. Surprisingly, when several parameters are being estimated jointly, better estimators can be constructed, an effect known as [[Stein's phenomenon]]. For example, if the measurement error is [[Normal distribution|Gaussian]], several estimators are known which [[dominating decision rule|dominate]], or outperform, the least squares technique; the best known of these is the [[James–Stein estimator]]. This is an example of more general [[shrinkage estimator]]s that have been applied to regression problems.\n\n==Applications==\n{{see also|Linear regression#Applications}}\n\n* [[Polynomial regression|Polynomial fitting]]: models are [[polynomial]]s in an independent variable, ''x'':\n** Straight line: <math>f(x, \\boldsymbol \\beta)=\\beta_1 +\\beta_2 x</math>.<ref>{{cite book |title=Analysis of Straight-Line Data |last=Acton |first=F. S. |authorlink= |year=1959 |publisher=Wiley |location=New York |isbn= |pages= |url= }}</ref>\n** Quadratic: <math>f(x, \\boldsymbol \\beta)=\\beta_1  + \\beta_2 x +\\beta_3 x^2</math>.\n** Cubic, quartic and higher polynomials. For [[polynomial regression|regression with high-order polynomials]], the use of [[orthogonal polynomials]] is recommended.<ref>{{cite book |title=Numerical Methods of Curve Fitting |last=Guest |first=P. G. |authorlink= |year=1961 |publisher=Cambridge University Press |location=Cambridge |isbn= |pages= |url= }}{{page needed|date=December 2010}}</ref>\n*[[Numerical smoothing and differentiation]] &mdash; this is an application of polynomial fitting.\n*Multinomials in more than one independent variable, including surface fitting\n*Curve fitting with [[B-spline]]s <ref name=pg/>\n*[[Chemometrics]], [[Calibration curve]], [[Standard addition]], [[Gran plot]], [[Beer-Lambert law#Chemical analysis|analysis of mixtures]]\n\n===Uses in data fitting===\n\nThe primary application of linear least squares is in [[data fitting]]. Given a set of ''m'' data points    <math>y_1, y_2,\\dots, y_m,</math> consisting of experimentally measured values taken at ''m'' values <math>x_1, x_2,\\dots, x_m</math> of an independent variable (<math>x_i</math> may be scalar or  vector quantities), and given a model function <math>y=f(x, \\boldsymbol \\beta),</math> with <math>\\boldsymbol \\beta = (\\beta_1, \\beta_2, \\dots, \\beta_n),</math> it is desired to find the parameters <math>\\beta_j</math> such that the model function \"best\" fits the data. In linear least squares, linearity is meant to be with respect to parameters <math>\\beta_j,</math> so\n\n:<math>f(x, \\boldsymbol \\beta) = \\sum_{j=1}^{n} \\beta_j \\phi_j(x).</math>\n\nHere, the functions <math>\\phi_j</math> may be '''nonlinear''' with respect to the  variable '''x'''.\n\nIdeally, the model function fits the data exactly, so\n\n: <math>y_i = f(x_i, \\boldsymbol \\beta)</math>\n\nfor all <math>i=1, 2, \\dots, m.</math> This is usually not possible in practice, as there are more data points than there are parameters to be determined. The approach chosen then is to find the minimal possible value of the sum of squares of the [[residual (statistics)|residual]]s\n:<math>r_i(\\boldsymbol \\beta)= y_i - f(x_i, \\boldsymbol \\beta),\\  (i=1, 2, \\dots, m) </math>\nso to minimize the function\n\n:<math>S(\\boldsymbol \\beta)=\\sum_{i=1}^{m}r_i^2(\\boldsymbol \\beta).</math>\n\nAfter substituting for <math>r_i</math> and then for <math>f</math>, this minimization problem becomes the quadratic minimization problem above with\n\n:<math>X_{ij}=\\phi_j(x_i),</math>\n\nand the best fit can be found by solving the normal equations.\n\n== Example ==\n{{see also|Ordinary least squares#Example|Simple linear regression#Example}}\n{{further|Polynomial regression}}\n[[Image:Linear least squares example2.svg|right|thumb|A plot of the data points (in red), the least squares line of best fit (in blue), and the residuals (in green).]]\n\nAs a result of an experiment, four <math>(x, y)</math> data points were obtained, <math>(1, 6),</math> <math>(2, 5),</math> <math>(3, 7),</math> and <math>(4, 10)</math> (shown in red in the  diagram on the right). We hope to find a line <math>y=\\beta_1+\\beta_2 x</math> that best fits these four points. In other words, we would like to find the numbers <math>\\beta_1</math> and <math>\\beta_2</math> that approximately solve the overdetermined linear system\n:<math>\\begin{alignat}{3}\n\\beta_1  +  1\\beta_2 &&\\; = \\;&& 6 & \\\\\n\\beta_1  +  2\\beta_2 &&\\; = \\;&& 5 & \\\\\n\\beta_1  +  3\\beta_2 &&\\; = \\;&& 7 & \\\\\n\\beta_1  +  4\\beta_2 &&\\; = \\;&& 10 & \\\\\n\\end{alignat}</math>\nof four equations in two unknowns in some \"best\" sense.\n\nThe residual, at each point, between the curve fit and the data is the difference between the right- and left-hand sides of the equations above. The [[least squares]] approach to solving this problem is to try to make the sum of the squares of these residuals as small as possible; that is, to find the [[maxima and minima|minimum]] of the function\n\n: <math>\\begin{align}S(\\beta_1, \\beta_2) =&\n \\left[6-(\\beta_1+1\\beta_2)\\right]^2\n+\\left[5-(\\beta_1+2\\beta_2)   \\right]^2 \\\\\n&+\\left[7-(\\beta_1 +  3\\beta_2)\\right]^2\n+\\left[10-(\\beta_1  +  4\\beta_2)\\right]^2 \\\\\n&= 4\\beta_1^2 + 30\\beta_2^2 + 20\\beta_1\\beta_2 - 56\\beta_1 - 154\\beta_2 + 210 .\\end{align}</math>\n\nThe minimum is determined by calculating the [[partial derivative]]s of <math>S(\\beta_1, \\beta_2)</math> with respect to <math>\\beta_1</math> and <math>\\beta_2</math> and setting them to zero\n\n:<math>\\frac{\\partial S}{\\partial \\beta_1}=0=8\\beta_1 + 20\\beta_2 -56</math>\n:<math>\\frac{\\partial S}{\\partial \\beta_2}=0=20\\beta_1 + 60\\beta_2 -154.</math>\n\nThis results in a system of two equations in two unknowns, called the normal equations, which when solved give\n\n:<math>\\beta_1=3.5</math>\n:<math>\\beta_2=1.4</math>\n\nand the equation <math>y=3.5+1.4x</math> of the line of best fit. The [[residual (statistics)|residual]]s, that is, the differences between the <math>y</math> values from the observations and the <math>y</math> predicated variables by using the line of best fit, are then found to be <math>1.1,</math> <math>-1.3,</math> <math>-0.7,</math> and <math>0.9</math> (see the diagram on the right). The minimum value of the sum of squares of the residuals is <math>S(3.5, 1.4)=1.1^2+(-1.3)^2+(-0.7)^2+0.9^2=4.2.</math>\n\nMore generally, one can have <math>n</math> regressors <math>x_j</math>, and a linear model\n:<math>y = \\beta_1 + \\sum_{j=2}^{n+1} \\beta_j x_{j-1}. </math>\n\n===Using a quadratic model===\n[[File:Linear least squares2.svg|alt=|thumb|The result of fitting a quadratic function <math>y=\\beta_1+\\beta_2x+\\beta_3x^2\\,</math> (in blue) through a set of data points <math>(x_i, y_i)</math> (in red). In linear least squares the function need not be linear in the argument <math>x,</math> but only in the parameters <math>\\beta_j</math> that are determined to give the best fit.]]\nImportantly, in \"linear least squares\", we are not restricted to using a line as the model as in the above example. For instance, we could have chosen the restricted quadratic model <math>y=\\beta_1 x^2</math>. This model is still linear in the <math>\\beta_1</math> parameter, so we can still perform the same analysis, constructing a system of equations from the data points:\n\n:<math>\\begin{alignat}{2}\n6 &&\\; = \\beta_1 (1)^2 \\\\\n5 &&\\; = \\beta_1 (2)^2 \\\\\n7 &&\\; = \\beta_1 (3)^2 \\\\\n10 &&\\; = \\beta_1 (4)^2 \\\\\n\\end{alignat}</math>\n\nThe partial derivatives with respect to the parameters (this time there is only one) are again computed and set to 0:\n\n<math>\\frac{\\partial S}{\\partial \\beta_1} = 0 = 708 \\beta_1 - 498</math>\n\nand solved\n\n<math>\\beta_1 = 0.703</math>\n\nleading to the resulting best fit model <math>y = 0.703 x^2.</math>\n\n==See also==\n* [[Line-line intersection#Nearest point to non-intersecting lines]], an application\n* [[Line fitting]]\n* [[Nonlinear least squares]]\n* [[Regularized least squares]]\n* [[Simple linear regression]]\n* [[Partial least squares regression]]\n==References==\n{{reflist}}\n\n==Further reading==\n*{{Cite book | author=Bevington, Philip R. |author2=Robinson, Keith D.  | title=Data Reduction and Error Analysis for the Physical Sciences | year=2003 | publisher=McGraw-Hill | location= | isbn=978-0-07-247227-1 | pages=}}\n\n==External links==\n*[http://mathworld.wolfram.com/LeastSquaresFitting.html Least Squares Fitting &ndash; From MathWorld]\n*[http://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html Least Squares Fitting-Polynomial &ndash; From MathWorld]\n\n{{Least Squares and Regression Analysis}}\n\n[[Category:Broad-concept articles]]\n[[Category:Least squares]]\n[[Category:Computational statistics]]"
    },
    {
      "title": "Mean squared error",
      "url": "https://en.wikipedia.org/wiki/Mean_squared_error",
      "text": "{{distinguish-redirect|Mean squared deviation|Mean squared displacement}}\nIn [[statistics]], the '''mean squared error''' ('''MSE''') or '''mean squared deviation''' ('''MSD''') of an [[estimator]] (of a procedure for estimating an unobserved quantity) measures the [[expected value|average]] of the squares of the [[Error (statistics)|errors]]—that is, the average squared difference between the estimated values and what is estimated. MSE is a [[risk function]], corresponding to the [[expected value]] of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of [[randomness]] or because the estimator [[Omitted-variable bias|does not account for information]] that could produce a more accurate estimate.<ref name=\"pointEstimation\">{{cite book\n |first1=E. L. |last1=Lehmann\n |first2=George |last2=Casella \n |title=Theory of Point Estimation\n |publisher=Springer |location=New York\n |year=1998 |edition=2nd\n |isbn=978-0-387-98502-2 |mr=1639875\n}}</ref>\n\nThe MSE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero are better.\n\nThe MSE is the second [[moment (mathematics)|moment]] (about the origin) of the error, and thus incorporates both the [[variance]] of the estimator (how widely spread the estimates are from one [[data sample]] to another) and its [[Bias of an estimator|bias]] (how far off the average estimated value is from the truth). For an [[unbiased estimator]], the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to [[standard deviation]], taking the square root of MSE yields the root-mean-square error or [[root-mean-square deviation]] (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the [[variance]], known as the [[standard error]].\n\n==Definition and basic properties==\n\nThe MSE assesses the quality of a '''predictor''' (i.e., a function mapping arbitrary inputs to a sample of values of some [[random variable]]), or an '''[[estimator]]''' (i.e., a [[mathematical function]] mapping a [[Sample (statistics)|sample]] of data to an estimate of a [[Statistical parameter|parameter]] of the [[Statistical population|population]] from which the data is sampled). The definition of an MSE differs according to whether one is describing a predictor or an estimator.\n\n===Predictor===\n\nIf  a vector of <math>n</math> predictions generated from a sample of ''n'' data points on all variables, and <math>Y</math> is the vector of observed values of the variable being predicted, then the within-sample MSE of the predictor is computed as\n\n:<math>\\operatorname{MSE}=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y_i})^2.</math>\n\nI.e., the MSE is the ''mean'' <math>\\left(\\frac{1}{n}\\sum_{i=1}^n \\right)</math> of the ''squares of the errors'' <math>(Y_i-\\hat{Y_i})^2</math>. This is an easily computable quantity for a particular sample (and hence is sample-dependent).\n\nThe MSE can also be computed on ''q ''data points that were not used in estimating the model, either because they were held back for this purpose or because these data have been newly obtained. In this process, which is known as [[cross-validation (statistics)|cross-validation]], the MSE is often called the [[mean squared prediction error]], and is computed as\n\n:<math>\\operatorname{MSPE}=\\frac{1}{q}\\sum_{i=n+1}^{n+q}(Y_i-\\hat{Y_i})^2.</math>\n\n===Estimator===\n\nThe MSE of an estimator <math>\\hat{\\theta}</math> with respect to an unknown parameter <math>\\theta</math> is defined as\n\n:<math>\\operatorname{MSE}(\\hat{\\theta})=\\operatorname{E}_{\\hat\\theta}\\left[(\\hat{\\theta}-\\theta)^2\\right].</math>\n\nThis definition depends on the unknown parameter, but the MSE is ''a priori'' a property of an estimator. The MSE could be a function of unknown parameters, in which case any ''estimator'' of the MSE based on estimates of these parameters would be a function of the data and thus a random variable. If the estimator <math>\\hat{\\theta}</math> is derived from a sample statistic and is used to estimate some population statistic, then the expectation is with respect to the sampling distribution of the sample statistic.\n\nThe MSE can be written as the sum of the [[variance]] of the estimator and the squared [[Bias_of_an_estimator|bias]] of the estimator, providing a useful way to calculate the MSE and implying that in the case of unbiased estimators, the MSE and variance are equivalent.<ref name=\"wackerly\">{{cite book\n |first1=Dennis |last1=Wackerly\n |first2=William|last2=Mendenhall\n |first3=Richard L.|last3=Scheaffer\n |title=Mathematical Statistics with Applications\n |publisher=Thomson Higher Education|location=Belmont, CA, USA\n |year=2008 |edition=7\n |isbn=978-0-495-38508-0\n}}</ref>\n\n:<math>\\operatorname{MSE}(\\hat{\\theta})=\\operatorname{Var}_{\\hat\\theta}(\\hat{\\theta})+ \\operatorname{Bias}(\\hat{\\theta},\\theta)^2.</math>\n\n====Proof of variance and bias relationship====\n\n:<math>\\begin{align}\\operatorname{MSE}(\\hat{\\theta}) &= \\operatorname{E}_{\\hat\\theta} \\left [(\\hat{\\theta}-\\theta)^2 \\right ] \\\\\n&=  \\operatorname{E}_{\\hat\\theta}\\left[\\left(\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta} [\\hat\\theta]+\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta\\right)^2\\right]\\\\ \n&= \\operatorname{E}_{\\hat\\theta}\\left[\\left(\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]\\right)^2 +2\\left (\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta] \\right ) \\left (\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta \\right )+\\left( \\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta \\right)^2\\right] \\\\ \n&= \\operatorname{E}_{\\hat\\theta}\\left[\\left(\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]\\right)^2\\right]+\\operatorname{E}_{\\hat\\theta}\\left[2 \\left (\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta] \\right ) \\left (\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta \\right ) \\right] + \\operatorname{E}_{\\hat\\theta}\\left [ \\left(\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta\\right)^2 \\right] \\\\\n&=  \\operatorname{E}_{\\hat\\theta}\\left[\\left(\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]\\right)^2\\right]+ 2 \\left(\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta\\right) \\operatorname{E}_{\\hat\\theta}\\left[\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta] \\right] +  \\left(\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta\\right)^2 && \\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta = \\text{const.} \\\\\n&=  \\operatorname{E}_{\\hat\\theta}\\left[\\left(\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]\\right)^2\\right]+ 2 \\left(\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta\\right) \\left ( \\operatorname{E}_{\\hat\\theta}[\\hat{\\theta}]-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta] \\right )+  \\left(\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta\\right)^2 && \\operatorname{E}_{\\hat\\theta}[\\hat\\theta] = \\text{const.} \\\\\n&= \\operatorname{E}_{\\hat\\theta}\\left[\\left(\\hat{\\theta}-\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]\\right)^2\\right]+\\left(\\operatorname{E}_{\\hat\\theta}[\\hat\\theta]-\\theta\\right)^2\\\\ \n&= \\operatorname{Var}_{\\hat\\theta}(\\hat\\theta)+ \\operatorname{Bias}_{\\hat\\theta}(\\hat\\theta,\\theta)^2\n\\end{align}</math>\n\n==Regression==\n{{further|Reduced chi-squared statistic}}\n\nIn regression analysis, the term ''mean squared error'' is sometimes used to refer to the unbiased estimate of error variance: the [[residual sum of squares]] divided by the number of [[Degrees of freedom (statistics)|degrees of freedom]]. This definition for a known, computed quantity differs from the above definition for the computed MSE of a predictor in that a different denominator is used. The denominator is the sample size reduced by the number of model parameters estimated from the same data, ''(n-p)'' for ''p'' [[regressor]]s or ''(n-p-1)'' if an intercept is used.<ref>Steel, R.G.D, and Torrie, J. H., ''Principles and Procedures of Statistics with Special Reference to the Biological Sciences.'', [[McGraw Hill]], 1960, page 288.</ref> For more details, see [[errors and residuals in statistics]]. Note that, although the MSE (as defined in the present article) is not an unbiased estimator of the error variance, it is [[consistency (statistics)|consistent]], given the consistency of the predictor.\n\nAlso in regression analysis, \"mean squared error\", often referred to as [[mean squared prediction error]] or \"out-of-sample mean squared error\", can refer to the mean value of the squared deviations of the predictions from the true values, over an out-of-sample test space, generated by a model estimated over a particular sample space. This also is a known, computed quantity, and it varies by sample and by out-of-sample test space.\n\n==Examples==\n\n===Mean===\nSuppose we have a random sample of size <math>n</math> from a population, <math>X_1,\\dots,X_n</math>. Suppose the sample units were chosen with replacement. That is, the <math>n</math> units are selected one at a time, and previously selected units are still eligible for selection for all <math>n</math> draws. The usual estimator for the <math>\\mu</math> is the sample average\n\n:<math>\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n X_i </math>\n\nwhich has an expected value equal to the true mean <math>\\mu</math> (so it is unbiased) and a mean square error of\n\n:<math>\\operatorname{MSE}\\left(\\overline{X}\\right)=\\operatorname{E}\\left[\\left(\\overline{X}-\\mu\\right)^2\\right]=\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)^2= \\frac{\\sigma^2}{n}</math>\n\nwhere <math>\\sigma^2</math> is the [[Sample variance#Population variance|population variance]].\n\nFor a [[Gaussian distribution]] this is the [[best unbiased estimator]] (that is, it has the lowest MSE among all unbiased estimators), but not, say, for a [[Uniform distribution (continuous)|uniform distribution]].\n\n===Variance===\n{{further|Sample variance}}\nThe usual estimator for the variance is the ''corrected [[sample variance]]:''\n\n:<math>S^2_{n-1} = \\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\overline{X} \\right)^2 =\\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2-n\\overline{X}^2\\right).</math>\n\nThis is unbiased (its expected value is <math>\\sigma^2</math>), hence also called the ''unbiased sample variance,'' and its MSE is<ref>{{cite book\n  | last = Mood |first=A.\n  | last2 = Graybill |first2=F.\n  |last3=Boes |first3=D.\n  | title = Introduction to the Theory of Statistics\n  |page=229\n  | edition = 3rd\n  | publisher = McGraw-Hill\n  | year = 1974\n}}</ref>\n\n:<math>\\operatorname{MSE}(S^2_{n-1})= \\frac{1}{n} \\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right) =\\frac{1}{n} \\left(\\gamma_2+\\frac{2n}{n-1}\\right)\\sigma^4,</math>\n\nwhere <math>\\mu_4</math> is the fourth [[central moment]] of the distribution or population and <math>\\gamma_2=\\mu_4/\\sigma^4-3</math> is the [[excess kurtosis]].\n\nHowever, one can use other estimators for <math>\\sigma^2</math> which are proportional to <math>S^2_{n-1}</math>, and an appropriate choice can always give a lower mean square error. If we define\n\n:<math>S^2_a = \\frac{n-1}{a}S^2_{n-1}= \\frac{1}{a}\\sum_{i=1}^n\\left(X_i-\\overline{X}\\,\\right)^2</math>\n\nthen we calculate:\n\n:<math>\\begin{align}\n\\operatorname{MSE}(S^2_a)&=\\operatorname{E}\\left[\\left(\\frac{n-1}{a} S^2_{n-1}-\\sigma^2\\right)^2 \\right] \\\\\n&= \\operatorname{E}\\left[ \\frac{(n-1)^2}{a^2} S^4_{n-1} -2 \\left ( \\frac{n-1}{a} S^2_{n-1} \\right ) \\sigma^2 + \\sigma^4 \\right ] \\\\\n&= \\frac{(n-1)^2}{a^2} \\operatorname{E}\\left[ S^4_{n-1} \\right ] - 2 \\left ( \\frac{n-1}{a}\\right )  \\operatorname{E}\\left[ S^2_{n-1} \\right ] \\sigma^2 + \\sigma^4 \\\\\n&= \\frac{(n-1)^2}{a^2} \\operatorname{E}\\left[ S^4_{n-1} \\right ] - 2 \\left ( \\frac{n-1}{a}\\right )  \\sigma^4 + \\sigma^4 && \\operatorname{E}\\left[ S^2_{n-1} \\right ]  = \\sigma^2 \\\\\n&= \\frac{(n-1)^2}{a^2} \\left ( \\frac{\\gamma_2}{n} + \\frac{n+1}{n-1} \\right ) \\sigma^4- 2 \\left ( \\frac{n-1}{a}\\right )  \\sigma^4+\\sigma^4 &&  \\operatorname{E}\\left[ S^4_{n-1} \\right ] = \\operatorname{MSE}(S^2_{n-1}) + \\sigma^4 \\\\\n&=\\frac{n-1}{n a^2} \\left ((n-1)\\gamma_2+n^2+n \\right ) \\sigma^4- 2 \\left ( \\frac{n-1}{a}\\right )  \\sigma^4+\\sigma^4   \n\\end{align}</math>\n\nThis is minimized when\n\n:<math>a=\\frac{(n-1)\\gamma_2+n^2+n}{n} = n+1+\\frac{n-1}{n}\\gamma_2.</math>\n\nFor a [[Gaussian distribution]], where <math>\\gamma_2=0</math>, this means the MSE is minimized when dividing the sum by <math>a=n+1</math>. The minimum excess kurtosis is <math>\\gamma_2=-2</math>,{{efn|1=This can be proved by [[Jensen's inequality]] as follows. The fourth [[central moment]] is an upper bound for the square of variance, so that the least value for their ratio is one, therefore, the least value for the [[excess kurtosis]] is −2, achieved, for instance, by a Bernoulli with ''p''=1/2.}} which is achieved by a [[Bernoulli distribution]] with ''p''&nbsp;=&nbsp;1/2 (a coin flip), and the MSE is minimized for <math>a=n-1+\\tfrac{2}{n}.</math> So no matter what the kurtosis, we get a \"better\" estimate (in the sense of having a lower MSE) by scaling down the unbiased estimator a little bit; this is a simple example of a [[shrinkage estimator]]: one \"shrinks\" the estimator towards zero (scales down the unbiased estimator).\n\nFurther, while the corrected sample variance is the [[best unbiased estimator]] (minimum mean square error among unbiased estimators) of variance for Gaussian distributions, if the distribution is not Gaussian then even among unbiased estimators, the best unbiased estimator of the variance may not be <math>S^2_{n-1}.</math>\n\n===Gaussian distribution===\nThe following table gives several estimators of the true parameters of the population, μ and σ<sup>2</sup>, for the Gaussian case.<ref>{{cite book\n  | last = DeGroot\n  | first = Morris H.\n  | authorlink = Morris H. DeGroot\n  | title = Probability and Statistics\n  | edition = 2nd\n  | publisher = Addison-Wesley\n  | year = 1980\n  | ref = degroot }}</ref>\n\n{| class=\"wikitable\"\n! True value !! Estimator !! Mean squared error\n|-\n| <math>\\theta=\\mu</math> || <math>\\hat{\\theta}</math> = the unbiased estimator of the [[population mean]], <math>\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n(X_i)</math> || <math>\\operatorname{MSE}(\\overline{X})=\\operatorname{E}((\\overline{X}-\\mu)^2)=\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)^2</math>\n|-\n| <math>\\theta=\\sigma^2</math> || <math>\\hat{\\theta}</math> = the unbiased estimator of the [[population variance]], <math>S^2_{n-1} = \\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\overline{X}\\,\\right)^2</math> || <math>\\operatorname{MSE}(S^2_{n-1})=\\operatorname{E}((S^2_{n-1}-\\sigma^2)^2)=\\frac{2}{n - 1}\\sigma^4</math>\n|-\n| <math>\\theta=\\sigma^2</math> || <math>\\hat{\\theta}</math> = the biased estimator of the [[population variance]], <math>S^2_{n} = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\overline{X}\\,\\right)^2</math> || <math>\\operatorname{MSE}(S^2_{n})=\\operatorname{E}((S^2_{n}-\\sigma^2)^2)=\\frac{2n - 1}{n^2}\\sigma^4</math>\n|-\n| <math>\\theta=\\sigma^2</math> || <math>\\hat{\\theta}</math> = the biased estimator of the [[population variance]], <math>S^2_{n+1} = \\frac{1}{n+1}\\sum_{i=1}^n\\left(X_i-\\overline{X}\\,\\right)^2</math> || <math>\\operatorname{MSE}(S^2_{n+1})=\\operatorname{E}((S^2_{n+1}-\\sigma^2)^2)=\\frac{2}{n + 1}\\sigma^4</math>\n|}\n\n==Interpretation==\n\nAn MSE of zero, meaning that the estimator <math>\\hat{\\theta}</math> predicts observations of the parameter <math>\\theta</math> with perfect accuracy, is the ideal, but is typically not possible.\n\nValues of MSE may be used for comparative purposes. Two or more [[statistical model]]s may be compared using their MSEs as a measure of how well they explain a given set of observations: An unbiased estimator (estimated from a statistical model) with the smallest variance among all unbiased estimators is the [[best unbiased estimator]] or MVUE (Minimum Variance Unbiased Estimator).\n\nBoth [[linear regression]] techniques such as [[analysis of variance]] estimate the MSE as part of the analysis and use the estimated MSE to determine the [[statistical significance]] of the factors or predictors under study. The goal of [[experimental design]] is to construct experiments in such a way that when the observations are analyzed, the MSE is close to zero relative to the magnitude of at least one of the estimated treatment effects.\n\nMSE is also used in several [[stepwise regression]] techniques as part of the determination as to how many predictors from a candidate set to include in a model for a given set of observations.\n\n==Applications==\n\n*Minimizing MSE is a key criterion in selecting estimators: see [[minimum mean-square error]]. Among unbiased estimators, minimizing the MSE is equivalent to minimizing the variance, and the estimator that does this is the [[minimum variance unbiased estimator]]. However, a biased estimator may have lower MSE; see [[estimator bias]].\n*In [[statistical modelling]] the MSE can represent the difference between the actual observations and the observation values predicted by the model. In this context, it is used to determine the extent to which the model fits the data as well as whether removing some explanatory variables is possible without significantly harming the model's predictive ability.\n\n==Loss function==\n\nSquared error loss is one of the most widely used [[loss function]]s in statistics, though its widespread use stems more from mathematical convenience than considerations of actual loss in applications. [[Carl Friedrich Gauss]], who introduced the use of mean squared error, was aware of its arbitrariness and was in agreement with objections to it on these grounds.<ref name=\"pointEstimation\" /> The mathematical benefits of mean squared error are particularly evident in its use at analyzing the performance of [[linear regression]], as it allows one to partition the variation in a dataset into variation explained by the model and variation explained by randomness.\n\n===Criticism===\nThe use of mean squared error without question has been criticized by the [[decision theory|decision theorist]] [[James Berger (statistician)|James Berger]]. Mean squared error is the negative of the expected value of one specific [[utility function]], the quadratic utility function, which may not be the appropriate utility function to use under a given set of circumstances. There are, however, some scenarios where mean squared error can serve as a good approximation to a loss function occurring naturally in an application.<ref>{{cite book\n |title=Statistical Decision Theory and Bayesian Analysis\n |first=James O. |last=Berger |authorlink=James Berger (statistician)\n |year=1985\n |edition=2nd\n |publisher=Springer-Verlag |location=New York\n |isbn=978-0-387-96098-2 |mr=0804611\n |chapter=2.4.2 Certain Standard Loss Functions |page=60\n}}</ref>\n\nLike [[variance]], mean squared error has the disadvantage of heavily weighting [[outliers]].<ref>{{cite journal | last1 = Bermejo | first1 = Sergio | last2 = Cabestany | first2 = Joan | year = 2001 | title = Oriented principal component analysis for large margin classifiers | url = http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6T08-43PS3GC-1&_user=483692&_coverDate=12%2F31%2F2001&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000022720&_version=1&_urlVersion=0&_userid=483692&md5=8586e409694e1b50da3aa3c6fce18cb8| archive-url = https://web.archive.org/web/20090123005439/http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6T08-43PS3GC-1&_user=483692&_coverDate=12%2F31%2F2001&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000022720&_version=1&_urlVersion=0&_userid=483692&md5=8586e409694e1b50da3aa3c6fce18cb8| dead-url = yes| archive-date = 2009-01-23| journal = Neural Networks | volume = 14 | issue = 10| pages = 1447–1461 | doi=10.1016/S0893-6080(01)00106-X}}</ref> This is a result of the squaring of each term, which effectively weights large errors more heavily than small ones. This property, undesirable in many applications, has led researchers to use alternatives such as the [[mean absolute error]], or those based on the [[median]].\n\n==See also==\n*[[Hodges' estimator]]\n*[[James–Stein estimator]]\n*[[Mean percentage error]]\n*[[Mean square quantization error]]\n*[[Mean square weighted deviation]]\n*[[Mean squared displacement]]\n*[[Mean squared prediction error]]\n*[[Minimum mean squared error|Minimum mean squared error estimator]]\n*[[Peak signal-to-noise ratio]]\n*[[Root mean square deviation]]\n*[[Squared deviations]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist}}\n\n[[Category:Point estimation performance]]\n[[Category:Statistical deviation and dispersion]]\n[[Category:Loss functions]]\n[[Category:Least squares]]"
    },
    {
      "title": "Moment matrix",
      "url": "https://en.wikipedia.org/wiki/Moment_matrix",
      "text": "In [[mathematics]], a '''moment matrix''' is a special symmetric square [[matrix (mathematics)|matrix]] whose rows and columns are indexed by [[monomial]]s. The entries of the matrix depend on the product of the indexing monomials only (cf. [[Hankel matrix|Hankel matrices]].)\n\nMoment matrices play an important role in [[polynomial optimization]], since [[positive semidefinite matrix|positive semidefinite]] moment matrices correspond to polynomials which are [[Polynomial SOS|sums of squares,]] and [[econometrics]].<ref>{{cite book |first=Arthur S. |last=Goldberger |authorlink=Arthur Goldberger |chapter=Classical Linear Regression |title=Econometric Theory |location=New York |publisher=John Wiley & Sons |year=1964 |isbn=0-471-31101-4 |pages=156–212 |chapterurl=https://books.google.com/books?id=KZq5AAAAIAAJ&pg=PA156 }}</ref>\n\n==Application in regression==\nA multiple [[linear regression]] model can be written as\n:<math>y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\dots \\beta_{k} x_{k} + u</math>\nwhere <math>y</math> is the explained variable, <math>x_{1}, x_{2} \\dots x_{k}</math> are the explanatory variables, <math>u</math> is the error, and <math>\\beta_{0}, \\beta_{1} \\dots \\beta_{k}</math> are unknown coefficients to be estimated. Given observations <math>\\left\\{ y_{i}, x_{1i}, x_{2i}, \\dots x_{ki} \\right\\}_{i=1}^{n}</math>, we have a system of <math>n</math> linear equations that can be expressed in matrix notation.<ref>{{cite book |first=David S. |last=Huang |title=Regression and Econometric Methods |location=New York |publisher=John Wiley & Sons |year=1970 |isbn=0-471-41754-8 |pages=52–65 |url=https://books.google.com/books?id=5IxRAAAAMAAJ&pg=PA52 }}</ref>\n:<math>\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\dots & x_{1k} \\\\ 1 & x_{21} & x_{22} & \\dots & x_{2k} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\dots & x_{nk} \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{k} \\end{bmatrix} + \\begin{bmatrix} u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n} \\end{bmatrix}</math>\nor\n:<math>\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{u}</math>\nwhere <math>\\mathbf{y}</math> and <math>\\mathbf{u}</math> are each a vector of dimension <math>n \\times 1</math>, <math>\\mathbf{X}</math> is the [[design matrix]] of order <math>N \\times (k+1)</math>, and <math>\\boldsymbol{\\beta}</math> is a vector of dimension <math>(k+1) \\times 1</math>. Under the [[Gauss–Markov theorem|Gauss–Markov assumptions]], the best linear unbiased estimator of <math>\\boldsymbol{\\beta}</math> is the linear [[least squares]] estimator <math>\\mathbf{b} = \\left( \\mathbf{X}^{\\mathsf{T}} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\mathsf{T}} \\mathbf{y}</math>, involving the two moment matrices <math>\\mathbf{X}^{\\mathsf{T}} \\mathbf{X}</math> and <math>\\mathbf{X}^{\\mathsf{T}} \\mathbf{y}</math> defined as\n:<math>\\mathbf{X}^{\\mathsf{T}} \\mathbf{X} = \\begin{bmatrix} n & \\sum x_{i1} & \\sum x_{i2} & \\dots & \\sum x_{ik} \\\\ \\sum x_{i1} & \\sum x_{i1}^{2} & \\sum x_{i1} x_{i2} & \\dots & \\sum x_{i1} x_{ik} \\\\ \\sum x_{i2} & \\sum x_{i1} x_{i2} & \\sum x_{i2}^{2} & \\dots & \\sum x_{i2} x_{ik} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sum x_{ik} & \\sum x_{i1} x_{ik} & \\sum x_{i2} x_{ik} & \\dots & \\sum x_{ik}^{2} \\end{bmatrix}</math>\nand\n:<math>\\mathbf{X}^{\\mathsf{T}} \\mathbf{y} = \\begin{bmatrix} \\sum y_{i} \\\\ \\sum x_{i1} y_{i} \\\\ \\vdots \\\\ \\sum x_{ik} y_{i} \\end{bmatrix}</math>\nwhere <math>\\mathbf{X}^{\\mathsf{T}} \\mathbf{X}</math> is a square matrix of dimension <math>(k+1) \\times (k+1)</math>, and <math>\\mathbf{X}^{\\mathsf{T}} \\mathbf{y}</math> is a vector of dimension <math>(k+1 ) \\times 1</math>.\n\n==See also==\n* [[Design matrix]]\n* [[Gramian matrix]]\n* [[Projection matrix]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{springer|title=Moment matrix|id=p/m130190}}\n\n{{Matrix classes}}\n\n[[Category:Matrices]]\n[[Category:Least squares]]\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Moving least squares",
      "url": "https://en.wikipedia.org/wiki/Moving_least_squares",
      "text": "'''Moving least squares''' is a method of reconstructing [[continuous function]]s from a [[set (mathematics)|set]] of unorganized point samples via the calculation of a [[weighted least squares]] [[measure (mathematics)|measure]] biased towards the region around the point at which the reconstructed value is requested.\n\nIn [[computer graphics]], the moving least squares method is useful for reconstructing a surface from a set of points. Often it is used to create a 3D surface from a [[point cloud]] through either [[downsampling]] or [[upsampling]].\n\n==Definition==\n[[Image:Moving_Least_Squares2.png|thumb|200px|Here is a 2D example. The circles are the samples  and the polygon is a linear interpolation. The blue curve is a smooth approximation of order 3.]]\nConsider a function <math>f: \\mathbb{R}^n \\to \\mathbb{R}</math> and a set of sample points <math>S = \\{ (x_i,f_i) | f(x_i) = f_i \\} </math>. Then, the moving least square approximation of degree <math>m</math> at the point <math>x</math> is <math>\\tilde{p}(x)</math> where <math>\\tilde{p}</math> minimizes the weighted least-square error \n:<math>\\sum_{i \\in I} (p(x_i)-f_i)^2\\theta(\\|x-x_i\\|)</math>\nover all polynomials <math>p</math> of degree <math>m</math> in <math>\\mathbb{R}^n</math>. <math>\\theta(s)</math> is the weight and it tends to zero as <math>s\\to \\infty</math>.\n\nIn the example <math>\\theta(s) = e^{-s^2}</math>.  The smooth interpolator of \"order 3\" is a quadratic interpolator.\n\n==See also==\n*[[Local regression]]\n*[[Diffuse element method]]\n*[[Moving average]]\n\n==References==\n{{Reflist}}\n*[http://dl.acm.org/citation.cfm?id=301704 The approximation power of moving least squares] David Levin, Mathematics of Computation, Volume 67, 1517-1531, 1998 [http://www.ams.org/mcom/1998-67-224/S0025-5718-98-00974-0/S0025-5718-98-00974-0.pdf ]\n*[http://www.sciencedirect.com/science/article/pii/S0045794905000726/ Moving least squares response surface approximation: Formulation and metal forming applications] Piotr Breitkopf; Hakim Naceur; Alain Rassineux; Pierre Villon, Computers and Structures, Volume 83, 17-18, 2005.\n* [http://www.springerlink.com/content/v7164702238848p1/ Generalizing the finite element method: diffuse approximation and diffuse elements], B Nayroles, G Touzot. Pierre Villon, P, Computational Mechanics Volume 10, pp 307-318, 1992\n\n==External links==\n* [http://www.nealen.net/projects/mls/asapmls.pdf An As-Short-As-Possible Introduction to the Least Squares, Weighted Least Squares and Moving Least Squares Methods for Scattered Data Approximation and Interpolation]\n\n[[Category:Least squares]]\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Non-linear least squares",
      "url": "https://en.wikipedia.org/wiki/Non-linear_least_squares",
      "text": "{{Regression bar}}\n'''Non-linear least squares''' is the form of [[least squares]] analysis used to fit a set of ''m'' observations with a model that is non-linear in ''n'' unknown parameters (''m''&nbsp;≥&nbsp;''n'').  It is used in some forms of [[nonlinear regression]].  The basis of the method is to approximate the model by a linear one and to refine the parameters by successive iterations. There are many similarities to [[linear least squares (mathematics)|linear least squares]], but also some [[least squares#Differences between linear and nonlinear least squares|significant differences]]. Examples of NLLS are (i) the Probit Regression, (ii) Threshold Regression, (iii) Smooth Regression, (iv) Logistic Link Regression, (v) Box-Cox Transformed Regressors (<math>m(x,\\theta_{i})=\\theta_{1} +\\theta_{2}x^{(\\theta_{3})}</math>),  and many others in Economic Theory.\n\n== Theory ==\nConsider a set of <math>m</math> data points, <math>(x_1, y_1), (x_2, y_2),\\dots,(x_m, y_m),</math> and a curve (model function) <math>y=f(x, \\boldsymbol \\beta),</math> that in addition to the variable <math>x</math> also depends on <math>n</math> parameters, <math>\\boldsymbol \\beta = (\\beta_1, \\beta_2, \\dots, \\beta_n),</math> with <math>m\\ge n.</math> It is desired to find the vector  <math>\\boldsymbol \\beta</math> of parameters such that the curve fits best the given data in the least squares sense, that is, the sum of squares \n:<math>S=\\sum_{i=1}^{m}r_i^2</math>\nis minimized, where the [[errors and residuals in statistics|residuals]] (in-sample prediction errors) ''r<sub>i</sub>'' are given by \n:<math>r_i= y_i - f(x_i, \\boldsymbol \\beta) </math>\n\nfor <math>i=1, 2,\\dots, m.</math>\n\nThe [[Maxima and minima|minimum]] value of ''S'' occurs when the [[gradient]] is zero. Since the model contains ''n'' parameters there are ''n'' gradient equations:\n\n:<math>\\frac{\\partial S}{\\partial \\beta_j}=2\\sum_i r_i\\frac{\\partial r_i}{\\partial \\beta_j}=0 \\quad (j=1,\\ldots,n).</math>\n\nIn a nonlinear system, the derivatives <math>\\frac{\\partial r_i}{\\partial \\beta_j}</math> are functions of both the independent variable and the parameters, so in general these gradient equations do not have a closed solution. Instead, initial values must be chosen for the parameters. Then, the parameters are refined iteratively, that is, the values are obtained by successive approximation,\n\n:<math>\\beta_j \\approx \\beta_j^{k+1} =\\beta^k_j+\\Delta \\beta_j. \\, </math>\n\nHere, ''k'' is an iteration number and the vector of increments, <math>\\Delta \\boldsymbol \\beta\\,</math> is known as the shift vector. At each iteration the model is linearized by approximation to a first-order [[Taylor series|Taylor polynomial]] expansion about <math> \\boldsymbol \\beta^k\\!</math>\n:<math>f(x_i,\\boldsymbol \\beta)\\approx f(x_i,\\boldsymbol \\beta^k) +\\sum_j \\frac{\\partial f(x_i,\\boldsymbol \\beta^k)}{\\partial \\beta_j} \\left(\\beta_j -\\beta^{k}_j \\right) = f(x_i,\\boldsymbol \\beta^k) +\\sum_j J_{ij} \\,\\Delta\\beta_j. </math>\nThe [[Jacobian matrix and determinant|Jacobian]], '''J''', is a function of constants, the independent variable ''and'' the parameters, so it changes from one iteration to the next. Thus, in terms of the linearized model, <math>\\frac{\\partial r_i}{\\partial \\beta_j}=-J_{ij}</math> and the residuals are given by\n\n:<math>\\Delta y_i=y_i- f(x_i,\\boldsymbol \\beta^k).</math>\n:<math>r_i=y_i - f(x_i, \\boldsymbol \\beta)=\\left(y_i- f(x_i,\\boldsymbol \\beta^k)\\right)+ \\left(f(x_i,\\boldsymbol \\beta^k)- f(x_i, \\boldsymbol \\beta)\\right)\\approx\\Delta y_i- \\sum_{s=1}^{n} J_{is} \\Delta \\beta_s .</math>\n\nSubstituting these expressions into the gradient equations, they become\n\n:<math>-2\\sum_{i=1}^{m}J_{ij} \\left( \\Delta y_i-\\sum_{s=1}^{n} J_{is}\\ \\Delta \\beta_s \\right)=0</math>\n\nwhich, on rearrangement, become ''n'' simultaneous linear equations, the '''normal equations'''\n\n:<math>\\sum_{i=1}^{m}\\sum_{s=1}^{n} J_{ij}J_{is}\\ \\Delta \\beta_s=\\sum_{i=1}^{m} J_{ij}\\ \\Delta y_i \\qquad (j=1,\\dots,n).\\,</math>\n\nThe normal equations are written in matrix notation as\n\n:<math>\\mathbf{\\left(J^TJ\\right)\\Delta \\boldsymbol \\beta=J^T\\ \\Delta y}.</math>\n\nWhen the observations are not equally reliable, a weighted sum of squares may be minimized,\n\n:<math>S=\\sum_{i=1}^m W_{ii}r_i^2.</math>\n\nEach element of the [[diagonal matrix|diagonal]] weight matrix '''W''' should, ideally, be equal to the reciprocal of the error [[variance]] of the measurement.<ref>This implies that the observations are uncorrelated. If the observations are [[correlated]], the expression\n\n:<math>S=\\sum_k \\sum_j r_k W_{kj} r_j\\,</math>\n\napplies. In this case the weight matrix should ideally be equal to the inverse of the error [[variance-covariance matrix]] of the observations.</ref>  \nThe normal equations are then\n\n:<math>\\mathbf{\\left(J^TWJ\\right)\\Delta \\boldsymbol \\beta=J^TW\\ \\Delta y}.</math>\n\nThese equations form the basis for the [[Gauss–Newton algorithm]] for a non-linear least squares problem.\n<!--\n=== Differences between linear and non-linear least squares ===\n*NLLSQ (Non-linear least squares) requires initial estimates of the parameters, LLSQ (linear least squares) does not.\n*NLLSQ requires that the Jacobian be calculated. Analytical expressions for the partial derivatives can be complicated. If analytical expressions are impossible to obtain the partial derivatives must be calculated by numerical approximation.\n*In NLLSQ divergence is a common phenomenon whereas in LLSQ it is quite rare. Divergence occurs when the sum of squares increases from one iteration to the next. It is caused by the inadequacy of the approximation that the Taylor series can be truncated at the first term.\n*NLLSQ is an iterative process, LLSQ is not. The iterative process has to be terminated when a convergence criterion is satisfied.\n*In LLSQ the solution is unique, but in NLLSQ there may be multiple minima in the sum of squares.\n*In NLLSQ estimates of the parameter errors are [[biased]], but in LLSQ they are not.\nThese differences must be considered whenever the solution to a non-linear least squares problem is being sought. -->\n\n== Geometrical interpretation ==\nIn linear least squares the [[Optimization (mathematics)|objective function]], ''S'', is a [[quadratic function#Bivariate quadratic function|quadratic function]] of the parameters.\n:<math>S=\\sum_i W_{ii} \\left(y_i-\\sum_jX_{ij}\\beta_j \\right)^2</math>\nWhen there is only one parameter the graph of ''S'' with respect to that parameter will be a [[parabola]]. With two or more parameters the contours of ''S'' with respect to any pair of parameters will be concentric [[ellipse]]s (assuming that the normal equations matrix <math>\\mathbf{X^TWX}</math> is [[positive-definite matrix|positive definite]]). The minimum parameter values are to be found at the centre of the ellipses. The geometry of the general objective function can be described as paraboloid elliptical. \nIn NLLSQ the objective function is quadratic with respect to the parameters only in a region close to its minimum value, where the truncated Taylor series is a good approximation to the model. \n:<math>S \\approx\\sum_i W_{ii} \\left(y_i-\\sum_j J_{ij}\\beta_j \\right)^2</math>\nThe more the parameter values differ from their optimal values, the more the contours deviate from elliptical shape. A consequence of this is that initial parameter estimates should be as close as practicable to their (unknown!) optimal values. It also explains how divergence can come about as the Gauss–Newton algorithm is convergent only when the objective function is approximately quadratic in the parameters.\n\n== Computation ==\n\n=== Initial parameter estimates ===\nSome problems of ill-conditioning and divergence can be corrected by finding initial parameter estimates that are near to the optimal values. A good way to do this is by [[computer simulation]]. Both the observed and calculated data are displayed on a screen. The parameters of the model are adjusted by hand until the agreement between observed and calculated data is reasonably good. Although this will be a subjective judgment, it is sufficient to find a good starting point for the non-linear refinement.  Initial parameter estimates can be created using transformations or linearizations.  Better still evolutionary algorithms such as the Stochastic Funnel Algorithm can lead to the convex basin of attraction that surrounds the optimal parameter estimates.  Hybrid algorithms that use randomization and elitism, followed by Newton methods have been shown to be useful and computationally efficient.\n\n=== Solution ===\nAny method among the ones described [[#Algorithms|below]] can be applied to find a solution.\n\n=== Convergence criteria ===\nThe common sense criterion for convergence is that the sum of squares does not decrease from one iteration to the next. However this criterion is often difficult to implement in practice, for various reasons. A useful convergence criterion is\n:<math>\\left|\\frac{S^k-S^{k+1}}{S^k}\\right|<0.0001.</math>\nThe value 0.0001 is somewhat arbitrary and may need to be changed. In particular it may need to be increased when experimental errors are large. An alternative criterion is\n\n:<math>\\left|\\frac{\\Delta \\beta_j}{\\beta_j}\\right|<0.001, \\qquad j=1,\\dots,n.</math>\n\nAgain, the numerical value is somewhat arbitrary; 0.001 is equivalent to specifying that each parameter should be refined to 0.1% precision. This is reasonable when it is less than the largest relative standard deviation on the parameters.\n\n===Calculation of the Jacobian by numerical approximation===\n{{main|Numerical differentiation}}\nThere are models for which it is either very difficult or even impossible to derive analytical expressions for the elements of the Jacobian. Then, the numerical approximation\n:<math>\\frac{\\partial f(x_i, \\boldsymbol \\beta)}{\\partial \\beta_j} \\approx \\frac{\\delta f(x_i, \\boldsymbol \\beta)}{\\delta \\beta_j}</math>\nis obtained by calculation of <math>f(x_i, \\boldsymbol \\beta)\\,</math> for <math>\\beta_j\\,</math> and <math>\\beta_j+\\delta \\beta_j\\,</math>. The increment,<math>\\delta \\beta_j\\,</math>, size should be chosen so the numerical derivative is not subject to approximation error by being too large, or [[round-off]] error by being too small.\n\n=== Parameter errors, confidence limits, residuals etc. ===\nSome information is given in [[linear least squares (mathematics)#Weighted linear least squares|the corresponding section]] on the [[linear least squares (mathematics)|linear least squares]] page.\n\n=== Multiple minima ===\nMultiple minima can occur in a variety of circumstances some of which are: \n*A parameter is raised to a power of two or more. For example, when fitting data to a [[Cauchy distribution|Lorentzian]] curve\n:: <math>f(x_i, \\boldsymbol \\beta)=\\frac{\\alpha}{1+\\left(\\frac{\\gamma-x_i}{\\beta} \\right)^2}</math>\nwhere <math>\\alpha</math> is the height, <math>\\gamma</math> is the position and <math>\\beta</math> is the half-width at half height, there are two solutions for the half-width, <math>\\hat \\beta</math> and <math>-\\hat \\beta</math> which give the same optimal value for the objective function.\n*Two parameters can be interchanged without changing the value of the model. A simple example is when the model contains the product of two parameters, since <math>\\alpha \\beta</math> will give the same value as <math>\\beta \\alpha</math>.\n*A parameter is in a trigonometric function, such as <math>\\sin \\beta\\,</math>, which has identical values at <math>\\hat \\beta +2n \\pi</math>. See [[Levenberg–Marquardt algorithm#Example|Levenberg&ndash;Marquardt algorithm]] for an example.\nNot all multiple minima have equal values of the objective function. False minima, also known as local minima, occur when the objective function value is greater than its value at the so-called global minimum. To be certain that the minimum found is the global minimum, the refinement should be started with widely differing initial values of the parameters. When the same minimum is found regardless of starting point, it is likely to be the global minimum.\n\nWhen multiple minima exist there is an important consequence: the objective function will have a maximum value somewhere between two minima. The normal equations matrix is not positive definite at a maximum in the objective function, as the gradient is zero and no unique direction of descent exists. Refinement from a point (a set of parameter values) close to a maximum will be ill-conditioned and should be avoided as a starting point. For example, when fitting a Lorentzian the normal equations matrix is not positive definite when the half-width of the band is zero.<ref>In the absence of [[round-off error]] and of experimental error in the independent variable the normal equations matrix would be singular</ref>\n\n=== Transformation to a linear model ===\n{{main|Nonlinear regression#Transformation}}\nA non-linear model can sometimes be transformed into a linear one. For example, when the model is a simple exponential function,\n:<math>f(x_i,\\boldsymbol \\beta)= \\alpha e^{\\beta x_i}</math>\nit can be transformed into a linear model by taking logarithms.\n:<math>\\log f(x_i,\\boldsymbol \\beta)=\\log \\alpha + \\beta x_i</math>\nGraphically this corresponds to working on a [[semi-log plot]]. The sum of squares becomes\n:<math>S=\\sum_i (\\log y_i-\\log \\alpha - \\beta x_i)^2.\\!</math>\nThis procedure should be avoided unless the errors are multiplicative and [[log normal distribution|log-normally distributed]] because it can give misleading results. This comes from the fact that whatever the experimental errors on '''y''' might be, the errors on '''log y''' are different. Therefore, when the transformed sum of squares is minimized different results will be obtained both for the parameter values and their calculated standard deviations. However, with multiplicative errors that are log-normally distributed, this procedure gives unbiased and consistent parameter estimates.\n\nAnother example is furnished by [[Michaelis&ndash;Menten kinetics#Equation optimization|Michaelis&ndash;Menten kinetics]], used to determine two parameters <math>V_{\\max}</math> and <math>K_m</math>:  \n:<math> v = \\frac{V_{\\max}[S]}{K_{m} + [S]}</math>.\nThe [[Lineweaver–Burk plot]] \n:<math> \\frac{1}{v} = \\frac{1}{V_\\max} + \\frac{K_m}{V_{\\max}[S]}</math>\nof <math>\\frac{1}{v}</math> against <math>\\frac{1}{[S]}</math> is linear in the parameters <math>\\frac{1}{V_\\max}</math> and <math>\\frac{K_m}{V_\\max}</math>, but very sensitive to data error and strongly biased toward fitting the data in a particular range of the independent variable <math>[S]</math>.\n\n== Algorithms ==\n=== Gauss–Newton method ===\n{{main|Gauss–Newton algorithm}}\nThe normal equations\n:<math>\\mathbf{\\left( J^TWJ \\right)\\Delta \\boldsymbol\\beta=\\left( J^TW \\right) \\Delta y}</math>\nmay be solved for <math>\\Delta \\boldsymbol\\beta</math> by [[Cholesky decomposition]], as described in [[linear least squares (mathematics)#Computation|linear least squares]]. The parameters are updated iteratively\n:<math>\\boldsymbol\\beta^{k+1}=\\boldsymbol\\beta^k+\\Delta \\boldsymbol\\beta</math>\nwhere ''k'' is an iteration number. While this method may be adequate for simple models, it will fail if divergence occurs. Therefore, protection against divergence is essential.\n\n==== Shift-cutting ====\nIf divergence occurs, a simple expedient is to reduce the length of the shift vector, <math>\\mathbf{\\Delta \\beta}</math>, by a fraction, ''f''\n:<math>\\boldsymbol\\beta^{k+1}=\\boldsymbol\\beta^k+f\\ \\Delta \\boldsymbol\\beta.</math>\nFor example, the length of the shift vector may be successively halved until the new value of the objective function is less than its value at the last iteration. The fraction, ''f'' could be optimized by a [[line search]].<ref name=BDS>M.J. Box, D. Davies and W.H. Swann, Non-Linear optimisation Techniques, Oliver & Boyd, 1969</ref>  As each trial value of ''f'' requires the objective function to be re-calculated it is not worth optimizing its value too stringently.\n\nWhen using shift-cutting, the direction of the shift vector remains unchanged. This limits the applicability of the method to situations where the direction of the shift vector is not very different from what it would be if the objective function were approximately quadratic in the parameters, <math>\\boldsymbol\\beta^k.</math>\n\n==== Marquardt parameter ====\n{{main|Levenberg–Marquardt algorithm}}\nIf divergence occurs and the direction of the shift vector is so far from its \"ideal\" direction that shift-cutting is not very effective, that is, the fraction, ''f'' required to avoid divergence is very small, the direction must be changed. This can be achieved by using the [[Levenberg–Marquardt algorithm|Marquardt]] parameter.<ref>This technique was proposed independently by Levenberg (1944), Girard (1958), Wynne (1959), Morrison (1960) and Marquardt (1963). Marquardt's name alone is used for it in much of the scientific literature.</ref> In this method the normal equations are modified\n:<math>\\mathbf{\\left( J^TWJ +\\lambda I \\right)\\Delta \\boldsymbol \\beta=\\left( J^TW \\right) \\Delta y}</math>\nwhere <math>\\lambda</math> is the Marquardt parameter and '''I''' is an identity matrix. Increasing the value of <math>\\lambda</math> has the effect of changing both the direction and the length of the shift vector. The shift vector is rotated towards the direction of [[steepest descent]]\n:when <math>\\lambda \\mathbf{I\\gg{}J^TWJ}, \\  \\mathbf{\\Delta \\boldsymbol \\beta} \\approx (1/\\lambda) \\mathbf{J^TW\\  \\Delta y}.</math>\n<math>\\mathbf{J^TW\\  \\Delta y}</math> is the steepest descent vector. So, when <math>\\lambda</math> becomes very large, the shift vector becomes a small fraction of the steepest descent vector.\n\nVarious strategies have been proposed for the determination of the Marquardt parameter. As with shift-cutting, it is wasteful to optimize this parameter too stringently. Rather, once a value has been found that brings about a reduction in the value of the objective function, that value of the parameter is carried to the next iteration, reduced if possible, or increased if need be. When reducing the value of the Marquardt parameter, there is a cut-off value below which it is safe to set it to zero, that is, to continue with the unmodified Gauss–Newton method. The cut-off value may be set equal to the smallest singular value of the Jacobian.<ref name=LH/> A bound for this value is given by <math>1/\\mbox{trace} \\mathbf{\\left(J^TWJ \\right)^{-1}}</math>.<ref>R. Fletcher, UKAEA Report AERE-R 6799, H.M. Stationery Office, 1971</ref>\n\n=== QR decomposition ===\nThe minimum in the sum of squares can be found by a method that does not involve forming the normal equations. The residuals with the linearized model can be written as\n:<math>\\mathbf{r=\\Delta y-J\\ \\Delta\\boldsymbol\\beta}.</math>\nThe Jacobian is subjected to an orthogonal decomposition; the [[QR decomposition]] will serve to illustrate the process.\n\n:<math>\\mathbf{J=QR}</math>\n\nwhere '''Q''' is an [[Orthogonal matrix|orthogonal]] <math>m \\times m</math> matrix and '''R''' is an <math>m \\times n</math> matrix which is [[block matrix|partitioned]] into an <math>n \\times n</math> block, <math>\\mathbf{R}_n</math>, and a <math>(m-n) \\times n</math> zero block. <math>\\mathbf{R}_n</math> is upper triangular.\n\n:<math>\\mathbf{R}= \\begin{bmatrix}\n\\mathbf{R}_n \\\\\n\\mathbf{0}\\end{bmatrix}</math>\n\nThe residual vector is left-multiplied by <math>\\mathbf Q^T</math>.\n\n:<math>\\mathbf{Q^Tr=Q^T\\ \\Delta y -R\\ \\Delta\\boldsymbol\\beta}= \\begin{bmatrix}\n\\mathbf{\\left(Q^T\\ \\Delta y -R\\ \\Delta\\boldsymbol\\beta \\right)}_n \\\\\n\\mathbf{\\left(Q^T\\ \\Delta y  \\right)}_{m-n}\\end{bmatrix}</math>\n\nThis has no effect on the sum of squares since <math>S=\\mathbf{r^T Q Q^Tr = r^Tr}</math> because '''Q''' is [[orthogonal]]\nThe minimum value of ''S'' is attained when the upper block is zero. Therefore, the shift vector is found by solving\n\n:<math>\\mathbf{R_n\\ \\Delta\\boldsymbol\\beta =\\left(Q^T\\ \\Delta y \\right)_n}. \\, </math>\n\nThese equations are easily solved as '''R''' is upper triangular.\n\n=== Singular value decomposition ===\nA variant of the method of orthogonal decomposition involves [[singular value decomposition]], in which '''R''' is diagonalized by further orthogonal transformations.\n\n:<math>\\mathbf{J=U \\boldsymbol\\Sigma V^T} \\, </math>\n\nwhere <math>\\mathbf U</math> is orthogonal, <math>\\boldsymbol\\Sigma </math> is a diagonal matrix of singular values and <math>\\mathbf V</math> is the orthogonal matrix of the eigenvectors of <math>\\mathbf {J^TJ}</math> or equivalently the right singular vectors of <math>\\mathbf{J}</math>. In this case the shift vector is given by\n\n:<math>\\mathbf{\\boldsymbol\\Delta\\beta=V \\boldsymbol\\Sigma^{-1}\\left( U^T\\   \\boldsymbol\\Delta y \\right)}_n. \\, </math>\n\nThe relative simplicity of this expression is very useful in theoretical analysis of non-linear least squares. The application of singular value decomposition is discussed in detail in Lawson and Hanson.<ref name=LH>C.L. Lawson and R.J. Hanson, Solving Least Squares Problems, Prentice–Hall, 1974</ref>\n\n=== Gradient methods ===\nThere are many examples in the scientific literature where different methods have been used for non-linear data-fitting problems.\n\n*Inclusion of second derivatives in The Taylor series expansion of the model function. This is [[Newton's method in optimization]].\n:: <math>f(x_i, \\boldsymbol \\beta)=f^k(x_i, \\boldsymbol \\beta) +\\sum_j J_{ij} \\, \\Delta \\beta_j + \\frac{1}{2}\\sum_j\\sum_k \\Delta\\beta_j \\, \\Delta\\beta_k \\,H_{jk_{(i)}},\\ H_{jk_{(i)}}=\\frac{\\partial^2 f(x_i, \\boldsymbol \\beta)}{\\partial \\beta_j \\, \\partial \\beta_k }. </math>\n: The matrix '''H''' is known as the [[Hessian matrix]]. Although this model has better convergence properties near to the minimum, it is much worse when the parameters are far from their optimal values. Calculation of the Hessian adds to the complexity of the algorithm. This method is not in general use.\n*[[Davidon–Fletcher–Powell formula|Davidon–Fletcher–Powell method]]. This method, a form of pseudo-Newton method, is similar to the one above but calculates the Hessian by successive approximation, to avoid having to use analytical expressions for the second derivatives.\n*[[Steepest descent]]. Although a reduction in the sum of squares is guaranteed when the shift vector points in the direction of steepest descent,  this method often performs poorly. When the parameter values are far from optimal the direction of the steepest descent vector, which is normal (perpendicular) to the contours of the objective function, is very different from the direction of the Gauss–Newton vector. This makes divergence much more likely, especially as the minimum along the direction of steepest descent may correspond to a small fraction of the length of the steepest descent vector. When the contours of the objective function are very eccentric, due to there being high correlation between parameters, the steepest descent iterations, with shift-cutting, follow a slow, zig-zag trajectory towards the minimum.\n*[[Conjugate gradient method|Conjugate gradient search]]. This is an improved steepest descent based method with good theoretical convergence properties, although it can fail on finite-precision digital computers even when used on quadratic problems.<ref>M. J. D. Powell, Computer Journal, (1964), '''7''', 155.</ref>\n\n=== Direct search methods ===\nDirect search methods depend on evaluations of the objective function at a variety of parameter values and do not use derivatives at all. They offer alternatives to the use of numerical derivatives in the Gauss–Newton method and gradient methods.\n* Alternating variable search.<ref name=BDS/> Each parameter is varied in turn by adding a fixed or variable increment to it and retaining the value that brings about a reduction in the sum of squares. The method is simple and effective when the parameters are not highly correlated. It has very poor convergence properties, but may be useful for finding initial parameter estimates.\n*[[Nelder–Mead method|Nelder–Mead (simplex) search]]. A [[simplex]] in this context is a [[polytope]] of ''n''&nbsp;+&nbsp;1 vertices in ''n'' dimensions; a triangle on a plane, a tetrahedron in three-dimensional space and so forth. Each vertex corresponds to a value of the objective function for a particular set of parameters. The shape and size of the simplex is adjusted by varying the parameters in such a way that the value of the objective function at the highest vertex always decreases. Although the sum of squares may initially decrease rapidly, it can converge to a nonstationary point on quasiconvex problems, by an example of M. J. D. Powell.\n\nMore detailed descriptions of these, and other, methods are available, in ''[[Numerical Recipes]]'', together with computer code in various languages.\n\n== See also ==\n* [[Least squares support vector machine]]\n* [[Curve fitting]]\n* [[Grey box model]]\n* [[Nonlinear programming]]\n* [[Nonlinear regression]]\n* [[Optimization (mathematics)]]\n* [[Levenberg&ndash;Marquardt algorithm]]\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n*C. T. Kelley, ''Iterative Methods for Optimization'', SIAM Frontiers in Applied Mathematics, no 18, 1999, {{ISBN|0-89871-433-8}}. [http://www.siam.org/books/textbooks/fr18_book.pdf  Online copy]\n* T. Strutz: ''Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond).'' 2nd edition, Springer Vieweg, 2016, {{ISBN|978-3-658-11455-8}}.\n\n{{Least Squares and Regression Analysis}}\n\n[[Category:Least squares]]"
    },
    {
      "title": "Non-negative least squares",
      "url": "https://en.wikipedia.org/wiki/Non-negative_least_squares",
      "text": "{{Regression bar}}\nIn [[mathematical optimization]], the problem of '''non-negative least squares''' ('''NNLS''') is a type of [[constrained least squares]] problem where the coefficients are not allowed to become negative. That is, given a matrix {{math|'''A'''}} and a (column) vector of [[response variable]]s {{math|'''y'''}}, the goal is to find<ref name=\"chen\"/>\n\n:<math>\\operatorname{arg\\,min}\\limits_\\mathbf{x} \\|\\mathbf{Ax} - \\mathbf{y}\\|_2</math> subject to {{math|'''x''' ≥ 0}}.\n\nHere {{math|'''x''' ≥ 0}} means that each component of the vector {{math|'''x'''}} should be non-negative, and {{math|‖·‖₂}} denotes the [[Euclidean norm]].\n\nNon-negative least squares problems turn up as subproblems in [[matrix decomposition]], e.g. in algorithms for [[CP decomposition|PARAFAC]]<ref name=\"bro\"/> and [[non-negative matrix factorization|non-negative matrix/tensor factorization]].<ref>{{Cite journal | last1 = Lin | first1 = Chih-Jen| title = Projected Gradient Methods for Nonnegative Matrix Factorization | doi = 10.1162/neco.2007.19.10.2756 | journal = [[Neural Computation]]| volume = 19 | issue = 10 | pages = 2756–2779 | year = 2007 | pmid =  17716011| pmc = | url = http://www.csie.ntu.edu.tw/~cjlin/papers/pgradnmf.pdf| citeseerx = 10.1.1.308.9135}}</ref><ref>{{cite journal |title=Random projections for the nonnegative least-squares problem |first1=Christos |last1=Boutsidis |first2=Petros |last2=Drineas |journal=Linear Algebra and its Applications |volume=431 |issue=5–7 |year=2009 |pages=760–771 |doi=10.1016/j.laa.2009.03.026}}</ref> The latter can be considered a generalization of NNLS.<ref name=\"chen\"/>\n\nAnother generalization of NNLS is '''bounded-variable least squares''' (BVLS), with simultaneous upper and lower bounds {{math|α''ᵢ'' ≤ '''x'''''ᵢ'' ≤ β''ᵢ''}}.{{r|lawson}}{{rp|291}}<ref>{{cite journal |last1=Stark |first1=Philip B. |first2=Robert L. |last2=Parker |title=Bounded-variable least-squares: an algorithm and applications |journal=Computational Statistics |volume=10 |year=1995 |pages=129 |url=http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/394.pdf}}</ref>\n\n==Quadratic programming version==\nThe NNLS problem is equivalent to a [[quadratic programming]] problem\n\n:<math>\\operatorname{arg\\,min}\\limits_\\mathbf{x \\ge 0} \\left(\\frac{1}{2} \\mathbf{x}^\\mathsf{T} \\mathbf{Q}\\mathbf{x} + \\mathbf{c}^\\mathsf{T} \\mathbf{x}\\right),</math>\n\nwhere {{math|'''Q'''}} = {{math|'''A'''ᵀ'''A'''}} and {{math|'''c'''}} = {{math|−'''A'''ᵀ '''y'''}}. This problem is [[Convex optimization|convex]], as {{math|'''Q'''}} is [[positive-semidefinite matrix|positive semidefinite]] and the non-negativity constraints form a convex feasible set.<ref name=\"sca\">{{cite book|doi=10.1007/11556121_50|title=Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem|journal=Computer Analysis of Images and Patterns|volume=3691|pages=407–414|series=Lecture Notes in Computer Science|year=2005|last1=Franc|first1=Vojtěch|last2=Hlaváč|first2=Václav|last3=Navara|first3=Mirko|isbn=978-3-540-28969-2}}</ref>\n\n==Algorithms==\nThe first widely used algorithm for solving this problem is an [[active set method]] published by Lawson and Hanson in their 1974 book ''Solving Least Squares Problems''.<ref name=\"lawson\">{{cite book |last1=Lawson |first1=Charles L. |last2=Hanson |first2=Richard J. |title=Solving Least Squares Problems |year=1995 |publisher=SIAM}}</ref>{{rp|291}} In [[pseudocode]], this algorithm looks as follows:{{r|chen}}<ref name=\"bro\">{{Cite journal|doi=10.1002/(SICI)1099-128X(199709/10)11:5<393::AID-CEM483>3.0.CO;2-L|title=A fast non-negativity-constrained least squares algorithm|journal=Journal of Chemometrics|volume=11|issue=5|pages=393|year=1997|last1=Bro|first1=Rasmus|last2=De Jong|first2=Sijmen}}</ref>\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Inputs:\n** a real-valued matrix {{mvar|A}} of dimension {{math|''m'' × ''n''}}\n** a real-valued vector {{math|'''y'''}} of dimension {{mvar|m}}\n** a real value {{mvar|ε}}, the tolerance for the stopping criterion\n* Initialize:\n** Set {{math|''P'' {{=}} ∅}}\n** Set {{math|''R'' {{=}} {1, ..., ''n''}}}\n** Set {{math|'''x'''}} to an all-zero vector of dimension {{mvar|n}}\n** Set {{math|'''w''' {{=}} ''A''ᵀ('''y''' − ''A'''''x''')}}\n* Main loop: while {{math|''R'' ≠ ∅}} and {{math|max('''w''') > ε}},\n** Let {{mvar|j}} in {{math|''R''}} be the index of {{math|max('''w''')}} in {{math|'''w'''}}\n** Add {{mvar|j}} to {{mvar|P}}\n** Remove {{mvar|j}} from {{mvar|R}}\n** Let {{mvar|A<sup>P</sup>}} be {{mvar|A}} restricted to the variables included in {{mvar|P}}\n** Let {{math|''s''}} be vector of same length as {{math|'''x'''}}. Let {{math|''s<sup>P</sup>''}} denote the sub-vector with indexes from ''P'', and let {{math|''s<sup>R</sup>''}} denote the sub-vector with indexes from ''R''.\n** Set {{math|''s<sup>P</sup>'' {{=}} ((''A<sup>P</sup>'')ᵀ ''A<sup>P</sup>'')<sup>−1</sup> (''A<sup>P</sup>'')ᵀ'''y'''}}\n** Set {{math|''s<sup>R</sup>''}} to zero\n** While {{math|min(''s<sup>P</sup>'') ≤ 0}}:\n*** Let {{math|''α'' {{=}} min({{sfrac|''x<sub>i</sub>''|''x<sub>i</sub>'' - ''s<sub>i</sub>''}}) for ''i'' in ''P'' where ''s<sub>i</sub> ≤ 0}}\n*** Set {{math|'''x'''}} to {{math|'''x''' + ''α''(''s'' - ''x'')}}\n*** Move to {{mvar|R}} all indices {{mvar|j}} in {{mvar|P}} such that {{math|''x<sub>j</sub>'' {{=}} 0}}\n*** Set {{math|''s<sup>P</sup>'' {{=}} ((''A<sup>P</sup>'')ᵀ ''A<sup>P</sup>'')<sup>−1</sup> (''A<sup>P</sup>'')ᵀ'''y'''}}\n*** Set {{math|''s<sup>R</sup>''}} to zero\n** Set {{math|'''x'''}} to {{mvar|s}}\n** Set {{math|'''w'''}} to {{math|''A''ᵀ('''y''' − ''A'''''x''')}}\n{{frame-footer}}\n</div>\n\nThis algorithm takes a finite number of steps to reach a solution and smoothly improves its candidate solution as it goes (so it can find good approximate solutions when cut off at a reasonable number of iterations), but is very slow in practice, owing largely to the computation of the [[Moore–Penrose pseudoinverse|pseudoinverse]] {{math|(('''A'''ᴾ)ᵀ '''A'''ᴾ)⁻¹}}.{{r|chen}} Variants of this algorithm are available in [[MATLAB]] as the routine {{mono|lsqnonneg}}<ref name=\"chen\">{{cite conference|authorlink2=Robert J. Plemmons |last1=Chen |first1=Donghui |first2=Robert J. |last2=Plemmons |title=Nonnegativity constraints in numerical analysis |conference=Symposium on the Birth of Numerical Analysis |date=2009 |citeseerx = 10.1.1.157.9203}}</ref> and in [[SciPy]] as {{mono|optimize.nnls}}.<ref>{{cite web |url=http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html |title=scipy.optimize.nnls |website=SciPy v0.13.0 Reference Guide |accessdate=25 January 2014}}</ref>\n<!-- TODO cite the R package nnls as well -->\n\nMany improved algorithms have been suggested since 1974.{{r|chen}} Fast NNLS (FNNLS) is an optimized version of the Lawson—Hanson algorithm.{{r|bro}} Other algorithms include variants of [[Landweber iteration|Landweber]]'s [[gradient descent]] method<ref>{{Cite journal | doi = 10.1016/j.mcm.2005.12.010| title = The application of an oblique-projected Landweber method to a model of supervised learning| journal = Mathematical and Computer Modelling| volume = 43| issue = 7–8| pages = 892| year = 2006| last1 = Johansson | first1 = B. R. | last2 = Elfving | first2 = T. | last3 = Kozlov | first3 = V. | last4 = Censor | first4 = Y. | last5 = Forssén | first5 = P. E. | last6 = Granlund | first6 = G. S. }}</ref> and [[coordinate descent|coordinate-wise optimization]] based on the quadratic programming problem above.{{r|sca}}\n\n==See also==\n* [[M-matrix]]\n* [[Perron–Frobenius theorem]]\n\n==References==\n{{reflist|30em}}\n\n[[Category:Least squares]]"
    },
    {
      "title": "Numerical methods for linear least squares",
      "url": "https://en.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares",
      "text": "'''Numerical methods for linear least squares''' entails the [[numerical analysis]] of [[linear least squares]] problems.\n\n==Introduction==\nA general approach to the least squares problem <math>\\operatorname{\\,min} \\, \\big\\|\\mathbf y - X \\boldsymbol \\beta \\big\\|^2</math> can be described as follows. Suppose that we can find an ''n''  by ''m''   matrix '''S''' \nsuch that '''XS''' is an      \n[[Linear projection|orthogonal projection]] onto the image of '''X'''. Then a solution to our  minimization problem is  given by\n\n:<math>\\boldsymbol \\beta  =  S   \\mathbf y  </math>\n\nsimply because\n\n:<math>  X \\boldsymbol \\beta  =  X ( S   \\mathbf y) = (X  S)  \\mathbf y</math>\n\nis exactly a sought for orthogonal projection of <math>  \\mathbf y </math> onto an image of '''X''' \n([[#Properties_of_the_least-squares_estimators|see the picture below]] and  note that as  explained in the\n[[#Properties_of_the_least-squares_estimators|next section]] the image of '''X''' is just a subspace generated by column vectors of  '''X'''). \nA few popular ways to find such a matrix ''S'' are described below.\n\n==Inverting the matrix of the normal equations==\nThe algebraic solution of the normal equations with a full-rank matrix ''X''<sup>T</sup>''X'' can be written as\n\n: <math> \\hat{\\boldsymbol{\\beta}} = (\\mathbf X^ {\\rm T} \\mathbf X )^{-1} \\mathbf X^ {\\rm T} \\mathbf y \n= \\mathbf X^+ \\mathbf y</math>\n\nwhere ''X''<sup>+</sup> is the [[Moore–Penrose pseudoinverse]] of ''X''. Although this equation is correct and can work in many applications, it is not computationally efficient to invert the normal-equations matrix (the [[Gramian matrix]]). An exception occurs in [[numerical smoothing and differentiation]] where an analytical expression is required.\n\nIf the matrix ''X''<sup>T</sup>''X'' is [[Condition number|well-conditioned]] and [[Positive-definite matrix|positive definite]], implying that it has full [[rank (linear algebra)|rank]], the normal equations can be solved directly by using the [[Cholesky decomposition]] ''R''<sup>T</sup>''R'', where ''R'' is an upper [[triangular matrix]], giving:\n\n: <math> R^{\\rm T} R \\hat{\\boldsymbol{\\beta}} =  X^{\\rm T} \\mathbf y. </math>\n\nThe solution is obtained in two stages, a [[forward substitution]] step, solving for '''z''':\n\n: <math> R^{\\rm T} \\mathbf z = X^{\\rm T} \\mathbf y,</math>\n\nfollowed by a backward substitution, solving for <math>\\hat{\\boldsymbol{\\beta}}</math>:\n\n: <math>R \\hat{\\boldsymbol{\\beta}}= \\mathbf z.</math>\n\nBoth substitutions are facilitated by the triangular nature of ''R''.\n\n==Orthogonal decomposition methods==\nOrthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more [[Numerical stability|numerically stable]] because they avoid forming the product ''X''<sup>T</sup>''X''.\n\nThe residuals are written in matrix notation as\n\n:<math>\\mathbf r= \\mathbf y - X \\hat{\\boldsymbol{\\beta}}.</math>\n\nThe matrix ''X'' is subjected to an orthogonal decomposition, e.g., the [[QR decomposition]] as follows. \n:<math>X=Q\n\\begin{pmatrix}\nR \\\\\n0\n\\end{pmatrix} \n\\ </math>,\nwhere ''Q'' is an ''m''×''m'' [[orthogonal matrix]] (''Q''<sup>T</sup>''Q=I'') and ''R'' is an ''n''×''n'' upper triangular matrix with <math>r_{ii}>0</math>.\n\nThe residual vector is left-multiplied by ''Q''<sup>T</sup>.\n\n:<math>Q^{\\rm T} \\mathbf r = Q^{\\rm T} \\mathbf y - \\left( Q^{\\rm T} Q \\right) \n\\begin{pmatrix}\nR \\\\\n0\n\\end{pmatrix} \n\\hat{\\boldsymbol{\\beta}}= \\begin{bmatrix}\n\\left(Q^{\\rm T} \\mathbf y \\right)_n - R \\hat{\\boldsymbol{\\beta}}  \\\\\n\\left(Q^{\\rm T} \\mathbf y \\right)_{m-n} \n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf u \\\\\n\\mathbf v\n\\end{bmatrix}\n</math>\n\nBecause ''Q'' is [[orthogonal matrix|orthogonal]], the sum of squares of the residuals, ''s'', may be written as:\n:<math>s = \\|\\mathbf r \\|^2 = \\mathbf r^{\\rm T} \\mathbf r = \\mathbf r^{\\rm T} Q Q^{\\rm T} \\mathbf r = \\mathbf u^{\\rm T} \\mathbf u + \\mathbf v^{\\rm T} \\mathbf v </math>\nSince '''v''' doesn't depend on '''''β''''', the minimum value of ''s'' is attained when the upper block, '''u''', is zero. Therefore, the parameters are found by solving:\n:<math> R \\hat{\\boldsymbol{\\beta}} =\\left(Q^{\\rm T} \\mathbf y \\right)_n.</math>\nThese equations are easily solved as ''R'' is upper triangular.\n\nAn alternative decomposition of ''X'' is the [[singular value decomposition]] (SVD)<ref>{{cite book |title=Solving Least Squares Problems |last=Lawson |first=C. L. |authorlink= |author2=Hanson, R. J.  |year=1974 |publisher=Prentice-Hall |location=Englewood Cliffs, NJ |isbn=0-13-822585-0 |pages= |url= }}</ref>\n\n:<math> X = U \\Sigma V^{\\rm T} \\ </math>,\n\nwhere ''U'' is ''m'' by ''m'' orthogonal matrix,  ''V'' is ''n'' by ''n'' orthogonal matrix and <math>\\Sigma</math> is an ''m'' by ''n'' matrix with all its elements outside of the main diagonal   equal to ''0''. The [[pseudoinverse]] of <math>\\Sigma</math> is easily obtained by inverting its non-zero diagonal elements and transposing. Hence,\n\n:<math>  \\mathbf X \\mathbf X^+ = U \\Sigma V^{\\rm T}  V \\Sigma^+  U^{\\rm T} =  U P  U^{\\rm T},</math>\n\nwhere ''P'' is obtained from <math>\\Sigma</math> by replacing its non-zero diagonal elements with ones. Since <math>(\\mathbf X \\mathbf X^+)^* = \\mathbf X \\mathbf X^+ </math> (the property of pseudoinverse), the matrix <math>U P U^{\\rm T}</math> is an orthogonal projection onto the image (column-space) of ''X''. In accordance with a general approach described in the introduction above (find '''XS''' which is an orthogonal projection),\n\n:<math> S = \\mathbf X^+ </math>,\n\nand thus,\n\n:<math> \\beta = V\\Sigma^+ U^{\\rm T} \\mathbf y </math>\n\nis a solution of a least squares problem. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, ''X''<sup>T</sup>''X'', is very ill-conditioned (i.e. if its [[condition number]] multiplied by the machine's relative [[round-off error]] is appreciably large).  In that case, including the smallest [[singular value]]s in the inversion merely adds numerical noise to the solution.  This can be cured with the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to [[factor analysis]].\n\n==Discussion==\nThe numerical methods for linear least squares are important because [[linear regression]] models are among the most important types of model, both as formal [[statistical model]]s and for exploration of data-sets. The majority of [[Comparison of statistical packages|statistical computer packages]] contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to [[round-off error]].\n\nIndividual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can be\n*Computations where a number of similar, and often [[Statistical model#Nested models|nested]], models are considered for the same data-set. That is, where models with the same [[dependent variable]] but different sets of [[independent variables]] are to be considered, for essentially the same set of data-points.\n*Computations for analyses that occur in a sequence, as the number of data-points increases.\n*Special considerations for very extensive data-sets.\n\nFitting of linear models by least squares often, but not always, arise in the context of [[statistical analysis]]. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.\n\nMatrix calculations, like any other, are affected by [[rounding error]]s. An early summary of these effects, regarding the choice of computation methods for matrix inversion, was provided by Wilkinson.<ref>Wilkinson, J.H. (1963) \"Chapter 3: Matrix Computations\", ''Rounding Errors in Algebraic Processes'', London: Her Majesty's Stationery  Office (National Physical Laboratory, Notes in Applied Science, No.32)</ref>\n\n==See also==\n*[[Numerical linear algebra]]\n*[[Numerical methods for non-linear least squares]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*Ake Bjorck, ''Numerical Methods for Least Squares Problems'', SIAM, 1996.\n*R. W. Farebrother, ''Linear Least Squares Computations'', CRC Press, 1988.\n*{{Citation\n | last=Barlow\n | first=Jesse L.\n | author-link=\n | chapter=Chapter 9: Numerical aspects of Solving Linear Least Squares Problems\n | editor-last=Rao  | editor-first=C. R.\n | title=Computational Statistics  |  series=Handbook of Statistics  | volume=9\n | publisher=North-Holland\n | publication-date=1993\n | isbn=0-444-88096-8\n }}\n*{{Cite book | last1=Björck |first1= Åke | authorlink= | title=Numerical methods for least squares problems | year=1996 | publisher=SIAM | location=Philadelphia  | isbn=0-89871-360-9 | pages=}}\n*{{Citation\n | last=Goodall\n | first=Colin R.\n | author-link=\n | chapter=Chapter 13: Computation using the QR decomposition\n | editor-last=Rao  | editor-first=C. R.\n | title=Computational Statistics  |  series=Handbook of Statistics  | volume=9\n | publisher=North-Holland\n | publication-date=1993\n | isbn=0-444-88096-8\n }}\n*{{Citation\n | last=National Physical Laboratory\n | first=\n | chapter=Chapter 1: Linear Equations and Matrices: Direct Methods\n | title=Modern Computing Methods\n | edition=2nd\n | series=Notes on Applied Science\n | volume=16\n | publisher=Her Majesty's Stationery Office\n | publication-date=1961\n }}\n*{{Citation\n | last=National Physical Laboratory\n | first=\n | chapter=Chapter 2: Linear Equations and Matrices: Direct Methods on Automatic Computers\n | title=Modern Computing Methods\n | edition=2nd\n | series=Notes on Applied Science\n | volume=16\n | publisher=Her Majesty's Stationery Office\n | publication-date=1961\n }}\n\n\n[[Category:Numerical linear algebra|Least squares]]\n[[Category:Least squares]]"
    },
    {
      "title": "Ordinary least squares",
      "url": "https://en.wikipedia.org/wiki/Ordinary_least_squares",
      "text": "{{Regression bar}}\n\nIn [[statistics]], '''ordinary least squares''' ('''OLS''') is a type of [[linear least squares]] method for estimating the unknown [[statistical parameter|parameters]] in a [[linear regression]] model. OLS chooses the parameters of a [[linear function]] of a set of [[explanatory variable]]s by the principle of [[least squares]]: minimizing the sum of the squares of the differences between the observed [[dependent variable]] (values of the variable being predicted) in the given [[dataset]] and those predicted by the linear function.\n\nGeometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface – the smaller the differences, the better the model fits the data. The resulting [[Statistical estimation|estimator]] can be expressed by a simple formula, especially in the case of a [[simple linear regression]], in which there is a single [[regressor]] on the right side of the regression equation.\n\nThe OLS estimator is [[consistent estimator|consistent]] when the [[regressors]] are [[exogenous]], and [[best linear unbiased estimator|optimal in the class of linear unbiased estimators]] when the [[statistical error|error]]s are [[homoscedastic]] and [[autocorrelation|serially uncorrelated]] {{Citation needed|date=October 2018}}. Under these conditions, the method of OLS provides [[UMVU|minimum-variance mean-unbiased]] estimation when the errors have finite [[variance]]s. Under the additional assumption that the errors are [[normal distribution|normally distributed]], OLS is the [[maximum likelihood estimator]].\n\nOLS is used in fields as diverse as [[economics]] ([[econometrics]]), [[data science]], [[political science]], [[psychology]] and [[engineering]] ([[control theory]] and [[signal processing]]).\n\n== Linear model ==\n{{main|Linear regression model}}\n[[File:Okuns law quarterly differences.svg|300px|thumb|[[Okun's law]] in [[macroeconomics]] states that in an economy the GDP growth should depend linearly on the changes in the unemployment rate. Here the ordinary least squares method is used to construct the regression line describing this law.]]\nSuppose the data consists of ''n'' [[statistical unit|observations]] <span class=\"texhtml\">{&thinsp;''y{{su|b=i}},&thinsp;x{{su|b=i}}''&thinsp;}{{su|p=''n''|b=''i''=1}}</span>. Each observation ''i'' includes a scalar response ''y<sub>i</sub>'' and a column vector ''x<sub>i</sub>'' of values of ''p'' predictors (regressors) ''x<sub>ij</sub>'' for ''j'' = 1, ..., ''p''. In a [[linear regression model]], the response variable, <math>y_i</math>, is a linear function of the regressors:\n\n:<math>y_i=\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots +\\beta_px_{ip}+\\varepsilon_i,</math>\n\nor in [[row and column vectors|vector]] form,\n: <math>\n    y_i = x_i^T  \\beta + \\varepsilon_i, \\,\n  </math>\nwhere ''β'' is a ''p×''1 vector of unknown parameters; the ''ε<sub>i</sub>'''s are unobserved scalar random variables ([[errors and residuals in statistics|errors]]) which account for influences upon the responses ''y<sub>i</sub>'' from sources other than the explanators ''x<sub>i</sub>''; and <math>x_i</math> is a column vector of the ''i''th observations of all the explanatory variables. This model can also be written in matrix notation as\n: <math>\n    y = X\\beta + \\varepsilon, \\,\n  </math>\nwhere ''y'' and ''ε'' are ''n''×1 vectors of the values of the response variable and the errors for the various observations, and ''X'' is an ''n''×''p'' matrix of regressors, also sometimes called the [[design matrix]], whose row ''i'' is ''x<sub>i</sub><sup>T</sup>'' and contains the ''i''th observations on all the explanatory variables.\n\nAs a rule, the constant term is always included in the set of regressors ''X'', say, by taking {{Math|1=''x''<sub>''i''1</sub>&nbsp;=&nbsp;1}} for all {{Math|1=''i'' = 1, …, ''n''}}. The coefficient ''β''<sub>1</sub> corresponding to this regressor is called the ''intercept''.\n\nThere may be some relationship between the regressors. For instance, the third regressor may be the square of the second regressor. In this case (assuming that the first regressor is constant) we have a quadratic model in the second regressor. But this is still considered a linear model because it is linear in the ''β''s.\n\n===Matrix/vector formulation===\nConsider an [[overdetermined system]]\n\n:<math>\\sum_{j=1}^{p} X_{ij}\\beta_j = y_i,\\ (i=1, 2, \\dots, n),</math>\n\nof ''n'' [[linear equation]]s in ''p'' unknown [[coefficients]], ''β''<sub>1</sub>,''β''<sub>2</sub>,…,''β''<sub>''p''</sub>, with ''n'' > ''p''. (Note: for a linear model as above, not all of <math>X</math> contains information on the data points. The first column is populated with ones, <math>X_{i1} = 1</math>, only the other columns contain actual data, so here ''p'' = number of regressors + 1.) This can be written in [[matrix (mathematics)|matrix]] form as\n\n:<math>\\mathbf {X} \\boldsymbol {\\beta} = \\mathbf {y},</math>\n\nwhere\n\n:<math>\\mathbf {X}=\\begin{bmatrix}\nX_{11} & X_{12} & \\cdots & X_{1p} \\\\\nX_{21} & X_{22} & \\cdots & X_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nX_{n1} & X_{n2} & \\cdots & X_{np}\n\\end{bmatrix} ,\n\\qquad \\boldsymbol \\beta = \\begin{bmatrix}\n\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} ,\n\\qquad \\mathbf y = \\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}. </math>\n\nSuch a system usually has no exact solution, so the goal is instead to find the coefficients <math>\\boldsymbol{\\beta}</math> which fit the equations \"best\", in the sense of solving the [[Quadratic form (statistics)|quadratic]] [[Mathematical optimization|minimization]] problem\n\n:<math>\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{arg\\,min}}\\,S(\\boldsymbol{\\beta}), </math>\n\nwhere the objective function ''S'' is given by\n\n:<math>S(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\bigl| y_i - \\sum_{j=1}^p X_{ij}\\beta_j\\bigr|^2 = \\bigl\\|\\mathbf y - \\mathbf X \\boldsymbol \\beta \\bigr\\|^2.</math>\n\nA justification for choosing this criterion is given in [[#Properties|Properties]] below. This minimization problem has a unique solution, provided that the ''p'' columns of the matrix <math>\\mathbf X</math> are [[linearly independent]], given by solving the [[normal equations]]\n\n:<math>(\\mathbf X^{\\rm T} \\mathbf X )\\hat{\\boldsymbol{\\beta}}= \\mathbf X^{\\rm T} \\mathbf y.</math>\n\nThe matrix <math>\\mathbf X^{\\rm T} \\mathbf X</math> is known as the [[Gramian matrix]] of <math>\\mathbf X</math>, which possesses several nice properties such as being a [[positive semi-definite matrix]], and the matrix <math>\\mathbf X^{\\rm T} \\mathbf y</math> is known as the [[moment matrix]] of regressand by regressors.<ref>{{cite book |first=Arthur S. |last=Goldberger |authorlink=Arthur Goldberger |chapter=Classical Linear Regression |title=Econometric Theory |location=New York |publisher=John Wiley & Sons |year=1964 |isbn=0-471-31101-4 |pages=156–212 [p. 158] |chapterurl=https://books.google.com/books?id=KZq5AAAAIAAJ&pg=PA156 }}</ref> Finally, <math>\\hat{\\boldsymbol{\\beta}}</math> is the coefficient vector of the least-squares [[hyperplane]], expressed as\n\n:<math>\\hat{\\boldsymbol{\\beta}}= (\\mathbf X^{\\rm T} \\mathbf X )^{-1} \\mathbf X^{\\rm T} \\mathbf y.</math>\n\n== Estimation ==\n\nSuppose ''b'' is a \"candidate\" value for the parameter vector ''β''. The quantity {{math|''y<sub>i</sub>'' − ''x<sub>i</sub>''<sup>T</sup>''b''}}, called the '''[[errors and residuals in statistics|residual]]''' for the ''i''-th observation, measures the vertical distance between the data point {{math|(''x<sub>i</sub>'', ''y<sub>i</sub>'')}} and the hyperplane {{math|1=''y'' = ''x''<sup>T</sup>''b''}}, and thus assesses the degree of fit between the actual data and the model. The '''sum of squared residuals''' ('''SSR''') (also called the '''error sum of squares''' ('''ESS''') or '''residual sum of squares''' ('''RSS'''))<ref>{{cite book |ref=harv |last=Hayashi |first=Fumio |authorlink=Fumio Hayashi  |title=Econometics |location= |publisher=Princeton University Press |year=2000 |isbn= |page=15 }}</ref> is a measure of the overall model fit:\n: <math>\n    S(b) = \\sum_{i=1}^n (y_i - x_i ^\\mathrm{T} b)^2 = (y-Xb)^\\mathrm{T}(y-Xb),\n  </math>\nwhere ''T'' denotes the matrix [[transpose]], and the rows of ''X'', denoting the values of all the independent variables associated with a particular value of the dependent variable, are ''X<sub>i</sub> = x<sub>i</sub>''<sup>T</sup>.  The value of ''b'' which minimizes this sum is called the '''OLS estimator for ''β'''''. The function ''S''(''b'') is quadratic in ''b'' with positive-definite [[Hessian matrix|Hessian]], and therefore this function possesses a unique global minimum at <math>b =\\hat\\beta</math>, which can be given by the explicit formula:<ref>{{harvtxt|Hayashi|2000|loc=page 18}}</ref><sup>[[Proofs involving ordinary least squares#Least squares estimator for .CE.B2|[proof]]]</sup>\n\n: <math>\n    \\hat\\beta = \\operatorname{argmin}_{b\\in\\mathbb{R}^p} S(b) =  (X^\\mathrm{T}X)^{-1}X^\\mathrm{T}y\\ .\n  </math>\n\n{{anchor|Normal matrix}}The product ''N''=''X''<sup>T</sup> ''X'' is a [[normal matrix]] and its inverse, ''Q''=''N''<sup>–1</sup>, is the ''cofactor matrix'' of ''β'',<ref>[https://books.google.com/books?id=hZ4mAOXVowoC&pg=PA160]</ref><ref>[https://books.google.com/books?id=Np7y43HU_m8C&pg=PA263]</ref><ref>[https://books.google.com/books?id=peYFZ69HqEsC&pg=PA134]</ref> closely related to its [[#Covariance matrix|covariance matrix]], ''C''<sub>''β''</sub>.\nThe matrix (''X''<sup>T</sup> ''X'')<sup>–1</sup> ''X''<sup>T</sup>=''Q'' ''X''<sup>T</sup> is called the [[Moore–Penrose pseudoinverse]] matrix of X. This formulation highlights the point that estimation can be carried out if, and only if, there is no perfect [[multicollinearity]] between the explanatory variables (which would cause the normal matrix to have no inverse).\n\nAfter we have estimated ''β'', the '''fitted values''' (or '''predicted values''') from the regression will be \n: <math>\n    \\hat{y} = X\\hat\\beta = Py,\n  </math>\nwhere ''P'' = ''X''(''X''<sup>T</sup>''X'')<sup>−1</sup>''X''<sup>T</sup> is the [[projection matrix]] onto the space ''V'' spanned by the columns of ''X''. This matrix ''P'' is also sometimes called the [[hat matrix]] because it \"puts a hat\" onto the variable ''y''. Another matrix, closely related to ''P'' is the ''annihilator'' matrix {{math|1=''M'' = ''I<sub>n</sub>'' − ''P''}}; this is a projection matrix onto the space orthogonal to ''V''. Both matrices ''P'' and ''M'' are [[symmetric matrix|symmetric]] and [[idempotent matrix|idempotent]] (meaning that {{math|1=''P''<sup>2</sup> = ''P''}}), and relate to the data matrix ''X'' via identities {{tmath|1=PX = X}} and {{math|1=''MX'' = 0}}.<ref name=\"Hayashi 2000 loc=page 19\">{{harvtxt|Hayashi|2000|loc=page 19}}</ref> Matrix ''M'' creates the '''residuals''' from the regression:\n: <math>\n    \\hat\\varepsilon = y - \\hat y = y - X\\hat\\beta = My = M(X\\beta+\\varepsilon) = (MX)\\beta + M\\varepsilon = M\\varepsilon.\n  </math>\n\n{{anchor|Reduced chi-squared}}Using these residuals we can estimate the value of ''σ''<sup> 2</sup>, called the '''[[reduced chi-squared]]''':\n: <math>\n    s^2 = \\frac{\\hat\\varepsilon ^\\mathrm{T} \\hat\\varepsilon}{n-p} = \\frac{(My)^\\mathrm{T} My}{n-p} = \\frac{y^\\mathrm{T} M^\\mathrm{T}My}{n-p}= \\frac{y ^\\mathrm{T} My}{n-p} = \\frac{S(\\hat\\beta)}{n-p},\\qquad\n    \\hat\\sigma^2 = \\frac{n-p}{n}\\;s^2\n  </math>\nThe numerator, ''n''−''p'', is the [[Degrees of freedom (statistics)|statistical degrees of freedom]]. The first quantity, ''s''<sup>2</sup>, is the OLS estimate for ''σ''<sup>2</sup>, whereas the second, <math style=\"vertical-align:0\">\\scriptstyle\\hat\\sigma^2</math>, is the MLE estimate for ''σ''<sup>2</sup>. The two estimators are quite similar in large samples; the first estimator is always [[estimator bias|unbiased]], while the second estimator is biased but has a smaller [[mean squared error]]. In practice ''s''<sup>2</sup> is used more often, since it is more convenient for the hypothesis testing. The square root of ''s''<sup>2</sup> is called the '''regression standard error''',<ref>[https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf Julian Faraway (2000), ''Practical Regression and Anova using R'']</ref> '''standard error of the regression''',<ref>{{cite book |last=Kenney |first=J. |last2=Keeping |first2=E. S. |year=1963 |title=Mathematics of Statistics |publisher=van Nostrand |page=187 }}</ref><ref>{{cite book |last=Zwillinger |first=D. |year=1995 |title=Standard Mathematical Tables and Formulae |publisher=Chapman&Hall/CRC |isbn=0-8493-2479-3 |page=626 }}</ref> or '''standard error of the equation'''.<ref name=\"Hayashi 2000 loc=page 19\"/>\n\nIt is common to assess the goodness-of-fit of the OLS regression by comparing how much the initial variation in the sample can be reduced by regressing onto ''X''. The '''[[coefficient of determination]] ''R''<sup>2</sup>''' is defined as a ratio of \"explained\" variance to the \"total\" variance of the dependent variable ''y'':<ref>{{harvtxt|Hayashi|2000|loc=page 20}}</ref>\n: <math>\n    R^2 = \\frac{\\sum(\\hat y_i-\\overline{y})^2}{\\sum(y_i-\\overline{y})^2} = \\frac{y ^\\mathrm{T} P ^\\mathrm{T} LPy}{y ^\\mathrm{T} Ly} = 1 - \\frac{y ^\\mathrm{T} My}{y ^\\mathrm{T} Ly} = 1 - \\frac{\\rm RSS}{\\rm TSS}\n  </math>\nwhere TSS is the '''total sum of squares''' for the dependent variable, {{math|1=''L'' = ''I<sub>n</sub>'' − '''11'''<sup>T</sup>/&thinsp;''n''}}, and '''1''' is an ''n''×1 vector of ones. (''L'' is a \"centering matrix\" which is equivalent to regression on a constant; it simply subtracts the mean from a variable.) In order for ''R''<sup>2</sup> to be meaningful, the matrix ''X'' of data on regressors must contain a column vector of ones to represent the constant whose coefficient is the regression intercept. In that case, ''R''<sup>2</sup> will always be a number between 0 and 1, with values close to 1 indicating a good degree of fit.\n\nThe variance in the prediction of the independent variable as a function of the dependent variable is given in the article [[Polynomial least squares]].\n\n=== Simple linear regression model ===\n{{main|Simple linear regression}}\nIf the data matrix ''X'' contains only two variables, a constant and a scalar regressor ''x<sub>i</sub>'', then this is called the \"simple regression model\".<ref>{{harvtxt|Hayashi|2000|loc=page 5}}</ref> This case is often considered in the beginner statistics classes, as it provides much simpler formulas even suitable for manual calculation. The parameters are commonly denoted as {{math|(''α'', ''β'')}}:\n: <math>\n    y_i = \\alpha + \\beta x_i + \\varepsilon_i.\n  </math>\nThe least squares estimates in this case are given by simple formulas\n: <math>\n  \\begin{align}\n    \\hat\\beta &= \\frac{ \\sum{x_iy_i} - \\frac{1}{n}\\sum{x_i}\\sum{y_i} }\n                     { \\sum{x_i^2} - \\frac{1}{n}(\\sum{x_i})^2 } =  \\frac{ \\operatorname{Cov}[x,y] }{ \\operatorname{Var}[x]}  \\\\\n    \\hat\\alpha &= \\overline{y} - \\hat\\beta\\,\\overline{x}\\ ,\n  \\end{align}\n  </math>\n\nwhere Var(.) and Cov(.) are sample parameters.\n\n== Alternative derivations ==\nIn the previous section the least squares estimator <math>\\hat\\beta</math> was obtained as a value that minimizes the sum of squared residuals of the model. However it is also possible to derive the same estimator from other approaches. In all cases the formula for OLS estimator remains the same: {{nowrap|1=''<sup style=\"position:relative;left:.6em;top:-.2em\">^</sup>β'' = (''X<sup>T</sup>X'')<sup>−1</sup>''X<sup>T</sup>y''}}; the only difference is in how we interpret this result.\n\n=== Projection ===\n[[File:OLS geometric interpretation.svg|thumb|250px|OLS estimation can be viewed as a projection onto the linear space spanned by the regressors. (Here each of <math>X_1</math> and <math>X_2</math> refers to a column of the data matrix.)]]\n{{cleanup merge|21=section|Linear least squares (mathematics)}}\n\nFor mathematicians, OLS is an approximate solution to an overdetermined system of linear equations {{math|''Xβ'' ≈ ''y''}}, where ''β'' is the unknown. Assuming the system cannot be solved exactly (the number of equations ''n'' is much larger than the number of unknowns ''p''), we are looking for a solution that could provide the smallest discrepancy between the right- and left- hand sides. In other words, we are looking for the solution that satisfies\n: <math>\n    \\hat\\beta = {\\rm arg}\\min_\\beta\\,\\lVert y - X\\beta \\rVert,\n  </math>\nwhere ||·|| is the standard [[Norm (mathematics)#Euclidean norm|''L''<sup>2</sup>&nbsp;norm]] in the ''n''-dimensional [[Euclidean space]] '''R'''<sup>''n''</sup>. The predicted quantity ''Xβ'' is just a certain linear combination of the vectors of regressors. Thus, the residual vector {{math|''y'' − ''Xβ''}} will have the smallest length when ''y'' is [[projection (linear algebra)|projected orthogonally]] onto the [[linear subspace]] [[linear span|spanned]] by the columns of ''X''. The OLS estimator <math style=\"vertical-align:-.3em\">\\hat\\beta</math> in this case can be interpreted as the coefficients of [[vector decomposition]] of {{math|1=<sup style=\"position:relative;left:.5em;\">^</sup>''y'' = ''Py''}} along the basis of ''X''.\n\nIn other words, the gradient equations at the minimum can be written as:\n\n:<math>(\\mathbf y - X \\hat{\\boldsymbol{\\beta}})^{\\rm T} X=0.</math>\n\nA geometrical interpretation of these equations is that the vector of residuals, <math>\\mathbf y - X \\hat{\\boldsymbol{\\beta}}</math> is orthogonal to the [[column space]] of ''X'', since the dot product <math>(\\mathbf y-X\\hat{\\boldsymbol{\\beta}})\\cdot X \\mathbf v</math> is equal to zero for ''any'' conformal vector, '''v'''. This means that <math>\\mathbf y - X \\boldsymbol{\\hat \\beta}</math> is the shortest of all possible vectors <math>\\mathbf{y}- X \\boldsymbol \\beta</math>, that is, the variance of the residuals is the minimum possible. This is illustrated at the right.\n\nIntroducing <math>\\hat{\\boldsymbol{\\gamma}}</math> and a matrix ''K'' with the assumption that a matrix <math>[X \\ K]</math> is non-singular and ''K''<sup>T</sup> ''X'' = 0 (cf. [[Linear projection#Orthogonal projections|Orthogonal projections]]), the residual vector should satisfy the following equation:\n:<math>\\hat{\\mathbf{r}} \\triangleq \\mathbf{y} - X \\hat{\\boldsymbol{\\beta}} = K \\hat{{\\boldsymbol{\\gamma}}}.</math>\nThe equation and solution of linear least squares are thus described as follows:\n:<math> \\mathbf{y} = \\begin{bmatrix}X & K\\end{bmatrix} \\begin{pmatrix} \\hat{\\boldsymbol{\\beta}} \\\\ \\hat{\\boldsymbol{\\gamma}} \\end{pmatrix} ,</math>\n:<math> \\begin{pmatrix} \\hat{\\boldsymbol{\\beta}} \\\\ \\hat{\\boldsymbol{\\gamma}} \\end{pmatrix} = \\begin{bmatrix}X & K\\end{bmatrix}^{-1} \\mathbf{y} = \\begin{bmatrix} (X^{\\rm T} X)^{-1} X^{\\rm T} \\\\ (K^{\\rm T} K)^{-1} K^{\\rm T} \\end{bmatrix} \\mathbf{y} .</math>\n\nAnother way of looking at it is to consider the regression line to be a weighted average of the lines passing through the combination of any two points in the dataset.<ref>{{cite web|last=Akbarzadeh|first=Vahab|title=Line Estimation|url=http://mlmadesimple.com/2014/05/07/line-estimation/}}</ref> Although this way of calculation is more computationally expensive, it provides a better intuition on OLS.\n\n=== Maximum likelihood ===\nThe OLS estimator is identical to the [[maximum likelihood estimator]] (MLE) under the normality assumption for the error terms.<ref>{{harvtxt|Hayashi|2000|loc=page 49}}</ref><sup>[[Proofs involving ordinary least squares#Maximum likelihood approach|[proof]]]</sup> This normality assumption has historical importance, as it provided the basis for the early work in linear regression analysis by [[Udny Yule|Yule]] and [[Karl Pearson|Pearson]].{{Citation needed|date=February 2010}} From the properties of MLE, we can infer that the OLS estimator is asymptotically efficient (in the sense of attaining the [[Cramér–Rao bound]] for variance) if the normality assumption is satisfied.<ref name=\"Hayashi 2000 loc=page 52\">{{harvtxt|Hayashi|2000|loc=page 52}}</ref>\n\n=== Generalized method of moments ===\nIn [[iid]] case the OLS estimator can also be viewed as a [[Generalized method of moments|GMM]] estimator arising from the moment conditions\n: <math>\n    \\mathrm{E}\\big[\\, x_i(y_i - x_i ^T \\beta) \\,\\big] = 0.\n  </math>\nThese moment conditions state that the regressors should be uncorrelated with the errors. Since ''x<sub>i</sub>'' is a ''p''-vector, the number of moment conditions is equal to the dimension of the parameter vector ''β'', and thus the system is exactly identified. This is the so-called classical GMM case, when the estimator does not depend on the choice of the weighting matrix.\n\nNote that the original strict exogeneity assumption {{nowrap|E[''ε<sub>i</sub>''&thinsp;{{!}}&thinsp;''x<sub>i</sub>''] {{=}} 0}} implies a far richer set of moment conditions than stated above. In particular, this assumption implies that for any vector-function ''ƒ'', the moment condition {{nowrap|E[''ƒ''(''x<sub>i</sub>'')·''ε<sub>i</sub>''] {{=}} 0}} will hold. However it can be shown using the [[Gauss–Markov theorem]] that the optimal choice of function ''ƒ'' is to take {{nowrap|''ƒ''(''x'') {{=}} ''x''}}, which results in the moment equation posted above.\n\n== Properties ==\n\n=== Assumptions ===\n{{see also|Linear regression#Assumptions}}\n\nThere are several different frameworks in which the [[linear regression model]] can be cast in order to make the OLS technique applicable. Each of these settings produces the same formulas and same results. The only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results. The choice of the applicable framework depends mostly on the nature of data in hand, and on the inference task which has to be performed.\n\nOne of the lines of difference in interpretation is whether to treat the regressors as random variables, or as predefined constants. In the first case ('''random design''') the regressors ''x<sub>i</sub>'' are random and sampled together with the ''y<sub>i</sub>''&#39;s from some [[statistical population|population]], as in an [[observational study]]. This approach allows for more natural study of the [[asymptotic theory (statistics)|asymptotic properties]] of the estimators. In the other interpretation ('''fixed design'''), the regressors ''X'' are treated as known constants set by a [[design of experiments|design]], and ''y'' is sampled conditionally on the values of ''X'' as in an [[experiment]]. For practical purposes, this distinction is often unimportant, since estimation and inference is carried out while conditioning on ''X''. All results stated in this article are within the random design framework.\n\n==== Classical linear regression model ====\nThe classical model focuses on the \"finite sample\" estimation and inference, meaning that the number of observations ''n'' is fixed. This contrasts with the other approaches, which study the [[asymptotic theory (statistics)|asymptotic behavior]] of OLS, and in which the number of observations is allowed to grow to infinity.\n\n*'''Correct specification'''. The linear functional form must coincide with the form of the actual data-generating process.\n* '''Strict exogeneity'''. The errors in the regression should have [[conditional expectation|conditional mean]] zero:<ref>{{harvtxt|Hayashi|2000|loc=page 7}}</ref>\n*: <math>\n    \\operatorname{E}[\\,\\varepsilon\\mid X\\,] = 0.\n  </math>\n:The immediate consequence of the exogeneity assumption is that the errors have mean zero: {{Math|1=E[''ε''] = 0}}, and that the regressors are uncorrelated with the errors: {{math|1=E[''X''<sup>T</sup>''ε''] = 0}}.\n\n:The exogeneity assumption is critical for the OLS theory. If it holds then the regressor variables are called ''exogenous''. If it doesn't, then those regressors that are correlated with the error term are called ''[[Endogeneity (economics)|endogenous]]'',<ref>{{harvtxt|Hayashi|2000|loc=page 187}}</ref> and then the OLS estimates become invalid. In such case the [[instrumental variable|method of instrumental variables]] may be used to carry out inference.\n\n* '''No linear dependence'''. The regressors in ''X'' must all be [[linearly independent]]. Mathematically, this means that the matrix ''X'' must have full [[column rank]] almost surely:<ref name=\"Hayashi 2000 loc=page 10\">{{harvtxt|Hayashi|2000|loc=page 10}}</ref>\n*: <math>\n    \\Pr\\!\\big[\\,\\operatorname{rank}(X) = p\\,\\big] = 1.\n  </math>\n:Usually, it is also assumed that the regressors have finite moments up to at least the second moment. Then the matrix {{math|1=''Q<sub>xx</sub>'' = E[''X''<sup>T</sup>''X''&thinsp;/&thinsp;''n'']}} is finite and positive semi-definite.\n:When this assumption is violated the regressors are called linearly dependent or [[multicollinearity|perfectly multicollinear]]. In such case the value of the regression coefficient ''β'' cannot be learned, although prediction of ''y'' values is still possible for new values of the regressors that lie in the same linearly dependent subspace.\n\n* '''Spherical errors''':<ref name=\"Hayashi 2000 loc=page 10\"/>\n*: <math>\n    \\operatorname{Var}[\\,\\varepsilon \\mid X\\,] = \\sigma^2 I_n,\n  </math>\n:where {{mvar|I<sub>n</sub>}} is the [[identity matrix]] in dimension ''n'', and ''σ''<sup>2</sup> is a parameter which determines the variance of each observation. This ''σ''<sup>2</sup> is considered a [[nuisance parameter]] in the model, although usually it is also estimated. If this assumption is violated then the OLS estimates are still valid, but no longer efficient.\n:It is customary to split this assumption into two parts:\n:* '''[[Homoscedasticity]]''': {{math|1=E[&thinsp;''ε<sub>i</sub>''<sup>2</sup>&thinsp;{{!}}&thinsp;''X''&thinsp;] = ''σ''<sup>2</sup>}}, which means that the error term has the same variance ''σ''<sup>2</sup> in each observation. When this requirement is violated this is called [[heteroscedasticity]], in such case a more efficient estimator would be [[weighted least squares]]. If the errors have infinite variance then the OLS estimates will also have infinite variance (although by the [[law of large numbers]] they will nonetheless tend toward the true values so long as the errors have zero mean). In this case, [[robust regression|robust estimation]] techniques are recommended.\n:* '''No [[autocorrelation]]''': the errors are [[correlation|uncorrelated]] between observations: {{math|1=E[&thinsp;''ε<sub>i</sub>ε<sub>j</sub>''&thinsp;{{!}}&thinsp;''X''&thinsp;] = 0}} for {{math|''i'' ≠ ''j''}}. This assumption may be violated in the context of [[time series]] data, [[panel data]], cluster samples, hierarchical data, repeated measures data, longitudinal data, and other data with dependencies. In such cases [[generalized least squares]] provides a better alternative than the OLS. Another expression for autocorrelation is ''serial correlation''.\n\n* '''Normality'''. It is sometimes additionally assumed that the errors have [[multivariate normal distribution|normal distribution]] conditional on the regressors:<ref>{{harvtxt|Hayashi|2000|loc=page 34}}</ref>\n*: <math>\n    \\varepsilon \\mid X\\sim \\mathcal{N}(0, \\sigma^2I_n).\n  </math>\n:This assumption is not needed for the validity of the OLS method, although certain additional finite-sample properties can be established in case when it does (especially in the area of hypotheses testing). Also when the errors are normal, the OLS estimator is equivalent to the [[maximum likelihood estimator]] (MLE), and therefore it is asymptotically efficient in the class of all [[regular estimator]]s. Importantly, the normality assumption applies only to the error terms; contrary to a popular misconception, the response (dependent) variable is not required to be normally distributed.<ref>{{cite journal|last1=Williams|first1=M. N|last2=Grajales|first2=C. A. G|last3=Kurkiewicz|first3=D|title=Assumptions of multiple regression: Correcting two misconceptions|journal=Practical Assessment, Research & Evaluation|date=2013|volume=18|issue=11|url=http://www.pareonline.net/getvn.asp?v=18&n=11}}</ref>\n\n==== [[Independent and identically distributed]] (iid) ====\nIn some applications, especially with [[cross-sectional data]], an additional assumption is imposed — that all observations are independent and identically distributed. This means that all observations are taken from a [[random sample]] which makes all the assumptions listed earlier simpler and easier to interpret. Also this framework allows one to state asymptotic results (as the sample size {{math|''n''&thinsp;→&thinsp;∞}}), which are understood as a theoretical possibility of fetching new independent observations from the [[data collection|data generating process]]. The list of assumptions in this case is:\n* '''iid observations''': (''x<sub>i</sub>'', ''y<sub>i</sub>'') is [[independent random variables|independent]] from, and has the same [[Probability distribution|distribution]] as, (''x<sub>j</sub>'', ''y<sub>j</sub>'') for all {{nowrap|''i ≠ j''}};\n* '''no perfect multicollinearity''': {{Math|1=''Q<sub>xx</sub>'' = E[&thinsp;''x<sub>i</sub>&thinsp;x<sub>i</sub>''<sup>T</sup>&thinsp;]}} is a [[positive-definite matrix]];\n* '''exogeneity''': {{Math|1=E[&thinsp;''ε<sub>i</sub>''&thinsp;{{!}}&thinsp;''x<sub>i</sub>''&thinsp;] = 0;}}\n* '''homoscedasticity''': {{Math|1=Var[&thinsp;''ε<sub>i</sub>''&thinsp;{{!}}&thinsp;''x<sub>i</sub>''&thinsp;] = ''σ''<sup>2</sup>}}.\n\n==== Time series model ====\n* The [[stochastic process]] {''x<sub>i</sub>'', ''y<sub>i</sub>''} is [[stationary process|stationary]] and [[ergodic process|ergodic]]; if {''x<sub>i</sub>'', ''y<sub>i</sub>''} is nonstantionary, OLS results are often spurious unless {''x<sub>i</sub>'', ''y<sub>i</sub>''} is [[Cointegration|co-integrating]].\n* The regressors are ''predetermined'': E[''x<sub>i</sub>ε<sub>i</sub>''] = 0 for all ''i'' = 1, ..., ''n'';\n* The ''p''×''p'' matrix {{math|1=''Q<sub>xx</sub>'' = E[&thinsp;''x<sub>i</sub>&thinsp;x<sub>i</sub>''<sup>T</sup>&thinsp;]}} is of full rank, and hence [[Positive-definite matrix|positive-definite]];\n* {''x<sub>i</sub>ε<sub>i</sub>''} is a [[martingale difference sequence]], with a finite matrix of second moments {{math|1=''Q''<sub>''xxε''²</sub> = E[&thinsp;''ε<sub>i</sub>''<sup>2</sup>''x<sub>i</sub>&thinsp;x<sub>i</sub>''<sup>T</sup>&thinsp;]}}.\n\n=== Finite sample properties ===\nFirst of all, under the ''strict exogeneity'' assumption the OLS estimators <math style=\"vertical-align:-.3em\">\\scriptstyle\\hat\\beta</math> and ''s''<sup>2</sup> are [[Bias of an estimator|unbiased]], meaning that their expected values coincide with the true values of the parameters:<ref>{{harvtxt|Hayashi|2000|loc=pages 27, 30}}</ref><sup>[[Proofs involving ordinary least squares#Unbiasedness of .CE.B2.CC.82|[proof]]]</sup>\n: <math>\n    \\operatorname{E}[\\, \\hat\\beta \\mid X \\,] = \\beta, \\quad \\operatorname{E}[\\,s^2 \\mid X\\,] = \\sigma^2.\n  </math>\nIf the strict exogeneity does not hold (as is the case with many [[time series]] models, where exogeneity is assumed only with respect to the past shocks but not the future ones), then these estimators will be biased in finite samples.\n\n{{anchor|Covariance matrix}}The ''[[variance-covariance matrix]]'' (or simply ''covariance matrix'') of <math style=\"vertical-align:-.3em\">\\scriptstyle\\hat\\beta</math> is equal to <ref name=\"HayashiFSP\">{{harvtxt|Hayashi|2000|loc=page 27}}</ref>\n: <math>\n    \\operatorname{Var}[\\, \\hat\\beta \\mid X \\,] = \\sigma^2(X ^T X)^{-1} = \\sigma^2 Q.\n  </math>\nIn particular, the standard error of each coefficient <math style=\"vertical-align:-.4em\">\\scriptstyle\\hat\\beta_j</math> is equal to square root of the ''j''-th diagonal element of this matrix. The estimate of this standard error is obtained by replacing the unknown quantity ''σ''<sup>2</sup> with its estimate ''s''<sup>2</sup>. Thus,\n: <math>\n    \\widehat{\\operatorname{s.\\!e.}}(\\hat{\\beta}_j) = \\sqrt{s^2 (X ^T X)^{-1}_{jj}}\n  </math>\n\nIt can also be easily shown that the estimator <math style=\"vertical-align:-.3em\">\\scriptstyle\\hat\\beta</math> is uncorrelated with the residuals from the model:<ref name=\"HayashiFSP\"/>\n: <math>\n    \\operatorname{Cov}[\\, \\hat\\beta,\\hat\\varepsilon \\mid X\\,] = 0.\n  </math>\n\nThe '''[[Gauss–Markov theorem]]''' states that under the ''spherical errors'' assumption (that is, the errors should be [[uncorrelated]] and [[homoscedastic]]) the estimator <math style=\"vertical-align:-.3em\">\\scriptstyle\\hat\\beta</math> is efficient in the class of linear unbiased estimators. This is called the '''best linear unbiased estimator (BLUE)'''. Efficiency should be understood as if we were to find some other estimator <math style=\"vertical-align:-.3em\">\\scriptstyle\\tilde\\beta</math> which would be linear in ''y'' and unbiased, then <ref name=\"HayashiFSP\"/>\n: <math>\n    \\operatorname{Var}[\\, \\tilde\\beta \\mid X \\,] - \\operatorname{Var}[\\, \\hat\\beta \\mid X \\,] \\geq 0\n  </math>\nin the sense that this is a [[nonnegative-definite matrix]]. This theorem establishes optimality only in the class of linear unbiased estimators, which is quite restrictive. Depending on the distribution of the error terms ''ε'', other, non-linear estimators may provide better results than OLS.\n\n==== Assuming normality ====\nThe properties listed so far are all valid regardless of the underlying distribution of the error terms. However, if you are willing to assume that the ''normality assumption'' holds (that is, that {{math|''ε'' ~ ''N''(0, ''σ''<sup>2</sup>''I<sub>n</sub>'')}}), then additional properties of the OLS estimators can be stated.\n\nThe estimator <math style=\"vertical-align:-.3em\">\\scriptstyle\\hat\\beta</math> is normally distributed, with mean and variance as given before:<ref>{{cite book |ref=harv |first=Takeshi |last=Amemiya |authorlink=Takeshi Amemiya |title=Advanced Econometrics |location= |publisher=Harvard University Press |year=1985 |isbn= |page=13 }}</ref>\n: <math>\n    \\hat\\beta\\ \\sim\\ \\mathcal{N}\\big(\\beta,\\ \\sigma^2(X ^\\mathrm{T} X)^{-1}\\big)\n  </math>\nwhere ''Q'' is the [[#Cofactor matrix|cofactor matrix]]. This estimator reaches the [[Cramér–Rao bound]] for the model, and thus is optimal in the class of all unbiased estimators.<ref name=\"Hayashi 2000 loc=page 52\"/> Note that unlike the [[Gauss–Markov theorem]], this result establishes optimality among both linear and non-linear estimators, but only in the case of normally distributed error terms.\n\nThe estimator ''s''<sup>2</sup> will be proportional to the [[chi-squared distribution]]:<ref>{{harvtxt|Amemiya|1985|loc=page 14}}</ref>\n: <math>\n    s^2\\ \\sim\\ \\frac{\\sigma^2}{n-p} \\cdot \\chi^2_{n-p}\n  </math>\nThe variance of this estimator is equal to {{math|2''σ''<sup>4</sup>/(''n''&thinsp;−&thinsp;''p'')}}, which does not attain the [[Cramér–Rao bound]] of {{math|2''σ''<sup>4</sup>/''n''}}. However it was shown that there are no unbiased estimators of ''σ''<sup>2</sup> with variance smaller than that of the estimator ''s''<sup>2</sup>.<ref>{{cite book |first=C. R. |last=Rao |authorlink=C. R. Rao |title=Linear Statistical Inference and its Applications |location=New York |publisher=J. Wiley & Sons |year=1973 |edition=Second |page=319 |isbn=0-471-70823-2 }}</ref> If we are willing to allow biased estimators, and consider the class of estimators that are proportional to the sum of squared residuals (SSR) of the model, then the best (in the sense of the [[mean squared error]]) estimator in this class will be {{math|1=<sup style=\"position:relative;left:.7em;top:-.1em\">~</sup>''σ''<sup>2</sup> = SSR&thinsp;''/''&thinsp;(''n''&thinsp;−&thinsp;''p''&thinsp;+&thinsp;2)}}, which even beats the Cramér–Rao bound in case when there is only one regressor ({{nowrap|1=''p'' = 1}}).<ref>{{harvtxt|Amemiya|1985|loc=page 20}}</ref>\n\nMoreover, the estimators <math style=\"vertical-align:-.3em\">\\scriptstyle\\hat\\beta</math> and ''s''<sup>2</sup> are [[independent random variables|independent]],<ref>{{harvtxt|Amemiya|1985|loc=page 27}}</ref> the fact which comes in useful when constructing the t- and F-tests for the regression.\n\n==== Influential observations ====\n{{main|Influential observation}}\n{{see also|Leverage (statistics)}}\n\nAs was mentioned before, the estimator <math>\\hat\\beta</math> is linear in ''y'', meaning that it represents a linear combination of the dependent variables ''y<sub>i</sub>''. The weights in this linear combination are functions of the regressors ''X'', and generally are unequal. The observations with high weights are called '''influential''' because they have a more pronounced effect on the value of the estimator.\n\nTo analyze which observations are influential we remove a specific ''j''-th observation and consider how much the estimated quantities are going to change (similarly to the [[jackknife method]]). It can be shown that the change in the OLS estimator for ''β'' will be equal to <ref name=\"DvdMck33\">{{cite book |ref=harv |last=Davidson |first=Russell |last2=MacKinnon |first2=James G. |title=Estimation and Inference in Econometrics |location=New York |publisher=Oxford Universiry Press |year=1993 |isbn=0-19-506011-3 |page=33 }}</ref>\n: <math>\n    \\hat\\beta^{(j)} - \\hat\\beta = - \\frac{1}{1-h_j} (X ^\\mathrm{T} X)^{-1}x_j ^\\mathrm{T} \\hat\\varepsilon_j\\,,\n  </math>\nwhere {{math|1=''h<sub>j</sub>'' = ''x<sub>j</sub>''<sup>T</sup>&thinsp;(''X''<sup>T</sup>''X'')<sup>−1</sup>''x<sub>j</sub>''}} is the ''j''-th diagonal element of the hat matrix ''P'', and ''x<sub>j</sub>'' is the vector of regressors corresponding to the ''j''-th observation. Similarly, the change in the predicted value for ''j''-th observation resulting from omitting that observation from the dataset will be equal to <ref name=\"DvdMck33\"/>\n: <math>\n    \\hat{y}_j^{(j)} - \\hat{y}_j = x_j ^\\mathrm{T} \\hat\\beta^{(j)} - x_j ^T \\hat\\beta = - \\frac{h_j}{1-h_j}\\,\\hat\\varepsilon_j\n  </math>\n\nFrom the properties of the hat matrix, {{math|0 ≤ ''h<sub>j</sub>'' ≤ 1}}, and they sum up to ''p'', so that on average {{math|''h<sub>j</sub>'' ≈ ''p/n''}}. These quantities ''h<sub>j</sub>'' are called the '''leverages''', and observations with high ''h<sub>j</sub>'' are called '''leverage points'''.<ref>{{harvtxt|Davidson|Mackinnon|1993|loc=page 36}}</ref> Usually the observations with high leverage ought to be scrutinized more carefully, in case they are erroneous, or outliers, or in some other way atypical of the rest of the dataset.\n\n==== Partitioned regression ====\nSometimes the variables and corresponding parameters in the regression can be logically split into two groups, so that the regression takes form\n: <math>\n    y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon,\n  </math>\nwhere ''X''<sub>1</sub> and ''X''<sub>2</sub> have dimensions ''n''×''p''<sub>1</sub>, ''n''×''p''<sub>2</sub>, and ''β''<sub>1</sub>, ''β''<sub>2</sub> are ''p''<sub>1</sub>×1 and ''p''<sub>2</sub>×1 vectors, with {{math|1=''p''<sub>1</sub> + ''p''<sub>2</sub> = ''p''}}.\n\nThe '''[[Frisch–Waugh–Lovell theorem]]''' states that in this regression the residuals <math style=\"vertical-align:0\">\\hat\\varepsilon</math> and the OLS estimate <math style=\"vertical-align:-.3em\">\\scriptstyle\\hat\\beta_2</math> will be numerically identical to the residuals and the OLS estimate for ''β''<sub>2</sub> in the following regression:<ref>{{harvtxt|Davidson|Mackinnon|1993|loc=page 20}}</ref>\n: <math>\n    M_1y = M_1X_2\\beta_2 + \\eta\\,,\n  </math>\nwhere ''M''<sub>1</sub> is the [[annihilator matrix]] for regressors ''X''<sub>1</sub>.\n\nThe theorem can be used to establish a number of theoretical results. For example, having a regression with a constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the de-meaned variables but without the constant term.\n\n==== Constrained estimation ====\n{{main|Ridge regression}}\n\nSuppose it is known that the coefficients in the regression satisfy a system of linear equations\n: <math>\n    A\\colon\\quad Q ^T \\beta = c, \\,\n  </math>\nwhere ''Q'' is a ''p''×''q'' matrix of full rank, and ''c'' is a ''q''×1 vector of known constants, where {{nowrap|''q&thinsp;<&thinsp;p''}}. In this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint ''A''. The '''constrained least squares (CLS)''' estimator can be given by an explicit formula:<ref>{{harvtxt|Amemiya|1985|loc=page 21}}</ref>\n: <math>\n    \\hat\\beta^c = \\hat\\beta - (X ^T X)^{-1}Q\\Big(Q ^T (X ^T X)^{-1}Q\\Big)^{-1}(Q ^T \\hat\\beta - c).\n  </math>\n\nThis expression for the constrained estimator is valid as long as the matrix ''X<sup>T</sup>X'' is invertible. It was assumed from the beginning of this article that this matrix is of full rank, and it was noted that when the rank condition fails, ''β'' will not be identifiable. However it may happen that adding the restriction ''A'' makes ''β'' identifiable, in which case one would like to find the formula for the estimator. The estimator is equal to <ref name=\"Amemiya22\">{{harvtxt|Amemiya|1985|loc=page 22}}</ref>\n: <math>\n    \\hat\\beta^c = R(R ^T X ^T XR)^{-1}R ^T X ^T y + \\Big(I_p - R(R ^T X ^T XR)^{-1}R ^T X ^T X\\Big)Q(Q ^T Q)^{-1}c,\n  </math>\nwhere ''R'' is a ''p''×(''p''&nbsp;−&nbsp;''q'') matrix such that the matrix {{nowrap|[''Q R'']}} is non-singular, and {{nowrap|1=''R<sup>T</sup>Q'' = 0}}. Such a matrix can always be found, although generally it is not unique. The second formula coincides with the first in case when ''X<sup>T</sup>X'' is invertible.<ref name=\"Amemiya22\"/>\n\n=== Large sample properties ===\nThe least squares estimators are [[point estimate]]s of the linear regression model parameters ''β''. However, generally we also want to know how close those estimates might be to the true values of parameters. In other words, we want to construct the [[interval estimate]]s.\n\nSince we haven't made any assumption about the distribution of error term ''ε<sub>i</sub>'', it is impossible to infer the distribution of the estimators <math>\\hat\\beta</math> and <math>\\hat\\sigma^2</math>. Nevertheless, we can apply the [[central limit theorem]] to derive their ''asymptotic'' properties as sample size ''n'' goes to infinity. While the sample size is necessarily finite, it is customary to assume that ''n'' is \"large enough\" so that the true distribution of the OLS estimator is close to its asymptotic limit.\n\nWe can show that under the model assumptions, the least squares estimator for ''β'' is [[consistent estimator|consistent]] (that is <math>\\hat\\beta</math> [[Convergence of random variables#Convergence in probability|converges in probability]] to ''β'') and asymptotically normal:<sup>[[Proofs involving ordinary least squares#Consistency and asymptotic normality of .CE.B2.CC.82|[proof]]]</sup>\n: <math>(\\hat\\beta - \\beta)\\ \\xrightarrow{d}\\ \\mathcal{N}\\big(0,\\;\\sigma^2Q_{xx}^{-1}\\big),</math>\nwhere <math>Q_{xx} = X ^T X.</math>\n\n==== Intervals ====\n{{main|Confidence interval|Prediction interval}}\n\nUsing this asymptotic distribution, approximate two-sided confidence intervals for the ''j''-th component of the vector <math>\\hat{\\beta}</math> can be constructed as\n: <math>\\beta_j \\in \\bigg[\\ \n    \\hat\\beta_j \\pm q^{\\mathcal{N}(0, 1)}_{1 - \\frac{\\alpha}{2}}\\!\\sqrt{\\hat{\\sigma}^2 \\left[Q_{xx}^{-1}\\right]_{jj}}\\ \n  \\bigg]\n</math> &nbsp; at the {{math|1&nbsp;−&nbsp;''α''}} confidence level,\nwhere ''q'' denotes the [[quantile function]] of standard normal distribution, and [·]<sub>''jj''</sub> is the ''j''-th diagonal element of a matrix.\n\nSimilarly, the least squares estimator for ''σ''<sup>2</sup> is also consistent and asymptotically normal (provided that the fourth moment of ''ε<sub>i</sub>'' exists) with limiting distribution\n: <math>(\\hat{\\sigma}^2 - \\sigma^2)\\ \\xrightarrow{d}\\ \\mathcal{N} \\left(0,\\;\\operatorname{E}\\left[\\varepsilon_i^4\\right] - \\sigma^4\\right). </math>\n\nThese asymptotic distributions can be used for prediction, testing hypotheses, constructing other estimators, etc.. As an example consider the problem of prediction. Suppose <math>x_0</math> is some point within the domain of distribution of the regressors, and one wants to know what the response variable would have been at that point. The [[mean response]] is the quantity <math>y_0 = x_0^\\mathrm{T} \\beta</math>, whereas the [[predicted response]] is <math>\\hat{y}_0 = x_0^\\mathrm{T} \\hat\\beta</math>. Clearly the predicted response is a random variable, its distribution can be derived from that of <math>\\hat{\\beta}</math>:\n: <math>\\left(\\hat{y}_0 - y_0\\right)\\ \\xrightarrow{d}\\ \\mathcal{N}\\left(0,\\;\\sigma^2 x_0^\\mathrm{T} Q_{xx}^{-1} x_0\\right),</math>\n\nwhich allows construct confidence intervals for mean  response <math>y_0</math> to be constructed:\n: <math>y_0 \\in \\left[\\ x_0^\\mathrm{T} \\hat{\\beta} \\pm q^{\\mathcal{N}(0, 1)}_{1 - \\frac{\\alpha}{2}}\\!\\sqrt{\\hat\\sigma^2 x_0^\\mathrm{T} Q_{xx}^{-1} x_0}\\ \\right]</math> &nbsp; at the {{math|1&nbsp;−&nbsp;''α''}} confidence level.\n\n==== Hypothesis testing ====\n{{main|Hypothesis testing}}\n{{Expand section|date=February 2017}}\n\nTwo hypothesis tests are particularly widely used. First, one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean (if not, it is said to have no explanatory power). The [[null hypothesis]] of no explanatory value of the estimated regression is tested using an [[F-test]]. If the calculated F-value is found to be large enough to exceed its critical value for the pre-chosen level of significance, the null hypothesis is rejected and the [[alternative hypothesis]], that the regression has explanatory power, is accepted. Otherwise, the null hypothesis of no explanatory power is accepted.\n\nSecond, for each explanatory variable of interest, one wants to know whether its estimated coefficient differs significantly from zero—that is, whether this particular explanatory variable in fact has explanatory power in predicting the response variable. Here the null hypothesis is that the true coefficient is zero. This hypothesis is tested by computing the coefficient's [[t-statistic]], as the ratio of the coefficient estimate to its [[standard error]]. If the t-statistic is larger than a predetermined value, the null hypothesis is rejected and the variable is found to have explanatory power, with its coefficient significantly different from zero. Otherwise, the null hypothesis of a zero value of the true coefficient is accepted.\n\nIn addition, the [[Chow test]] is used to test whether two subsamples both have the same underlying true coefficient values. The sum of squared residuals of regressions on each of the subsets and on the combined data set are compared by computing an F-statistic; if this exceeds a critical value, the null hypothesis of no difference between the two subsets is rejected; otherwise, it is accepted.\n\n== Example with real data {{anchor|Example}} ==\n{{see also|Simple linear regression#Example|Linear least squares#Example}}\n[[File:OLS example weight vs height scatterplot.svg|thumb|[[Scatterplot]] of the data, the relationship is slightly curved but close to linear]]\n\nThe following data set gives average heights and weights for American women aged 30–39 (source: ''The World Almanac and Book of Facts, 1975'').\n{{clear}}\n:{|class=\"wikitable\" style=\"text-align:right;\"\n|-\n! style=\"text-align:left;\" | Height (m)\n| 1.47 || 1.50 || 1.52 || 1.55 || 1.57 || 1.60 || 1.63 || 1.65 || 1.68 || 1.70 || 1.73 || 1.75 || 1.78 || 1.80 || 1.83\n|-\n! style=\"text-align:left;\" | Weight (kg)\n| 52.21 || 53.12 || 54.48 || 55.84 || 57.20 || 58.57 || 59.93 || 61.29 || 63.11 || 64.47 || 66.28 || 68.10 || 69.92 || 72.19 || 74.46\n|}\n\nWhen only one dependent variable is being modeled, a [[scatterplot]] will suggest the form and strength of the relationship between the dependent variable and regressors. It might also reveal outliers, heteroscedasticity, and other aspects of the data that may complicate the interpretation of a fitted regression model.  The scatterplot suggests that the relationship is strong and can be approximated as a quadratic function. OLS can handle non-linear relationships by introducing the regressor <tt>HEIGHT</tt><sup>2</sup>.  The regression model then becomes a multiple linear model:\n\n:<math>w_i = \\beta_1 + \\beta_2 h_i + \\beta_3 h_i^2 + \\varepsilon_i.</math>\n\n[[File:OLS example weight vs height fitted line.svg|thumb|right|300px|Fitted regression]]\nThe output from most popular [[List of statistical packages|statistical packages]] will look similar to this:\n:{|style=\"border:1px solid #aaa; padding:2pt 10pt;\"\n|-\n| Method             || colspan=\"4\" | Least squares\n|-\n| Dependent variable || colspan=\"4\" | WEIGHT\n|-\n| Observations       || colspan=\"4\" | 15\n|-\n| colspan=\"5\" | <hr>\n|- style=\"text-align:right;\"\n! style=\"padding-left:0.5em; text-align:left;\" | Parameter \n! style=\"padding-left:0.5em;\" | Value\n! style=\"padding-left:0.5em;\" | [[Standard error|Std error]]\n! style=\"padding-left:0.5em;\" | [[t-statistic]]\n! style=\"padding-left:0.5em;\" | [[p-value]]\n|-\n| colspan=\"5\" | <hr>\n|- style=\"text-align:right;\"\n| style=\"text-align:left;\" | <math>\\beta_1</math>\n|  128.8128 || 16.3083 ||  7.8986 || 0.0000\n|- style=\"text-align:right;\"\n| style=\"text-align:left;\" | <math>\\beta_2</math>\n| –143.1620 || 19.8332 || –7.2183 || 0.0000\n|- style=\"text-align:right;\"\n| style=\"text-align:left;\" | <math>\\beta_3</math>\n|   61.9603 ||  6.0084 || 10.3122 || 0.0000\n|-\n| colspan=\"5\" | <hr>\n|-\n| [[Coefficient of determination|R<sup>2</sup>]]    || style=\"text-align:right;\" | 0.9989\n| colspan=\"2\" | S.E. of regression                  || style=\"text-align:right;\" | 0.2516 \n|-\n| Adjusted R<sup>2</sup>                            || style=\"text-align:right;\" | 0.9987\n| colspan=\"2\" | Model sum-of-sq.                    || style=\"text-align:right;\" | 692.61\n|-\n| Log-likelihood                                    || style=\"text-align:right;\" | 1.0890\n| colspan=\"2\" | Residual sum-of-sq.                 || style=\"text-align:right;\" | 0.7595\n|-\n| [[Durbin–Watson statistic|Durbin–Watson stat.]]   || style=\"text-align:right;\" | 2.1013\n| colspan=\"2\" | Total sum-of-sq.                    || style=\"text-align:right;\" | 693.37\n|-\n| [[Akaike information criterion|Akaike criterion]] || style=\"text-align:right;\" | 0.2548\n| colspan=\"2\" | F-statistic                         || style=\"text-align:right;\" | 5471.2\n|-\n| [[Schwarz criterion]]                             || style=\"text-align:right;\" | 0.3964\n| colspan=\"2\" | p-value (F-stat)                    || style=\"text-align:right;\" | 0.0000\n|}\n\nIn this table:\n* The ''Value'' column gives the least squares estimates of parameters ''β<sub>j</sub>''\n* The ''Std error'' column shows [[standard error (statistics)|standard error]]s of each coefficient estimate: <math>\\hat\\sigma_j = \\left(\\hat{\\sigma}^2\\left[Q_{xx}^{-1}\\right]_{jj}\\right)^\\frac{1}{2}</math>\n* The ''[[t-statistic]]'' and ''p-value'' columns are testing whether any of the coefficients might be equal to zero. The ''t''-statistic is calculated simply as <math>t=\\hat\\beta_j/\\hat\\sigma_j</math>. If the errors ε follow a normal distribution, ''t'' follows a Student-t distribution.  Under weaker conditions, ''t'' is asymptotically normal. Large values of ''t'' indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero. The second column, [[p-value|''p''-value]], expresses the results of the hypothesis test as a [[statistical significance|significance level]].  Conventionally, ''p''-values smaller than 0.05 are taken as evidence that the population coefficient is nonzero.\n* ''R-squared'' is the [[coefficient of determination]] indicating goodness-of-fit of the regression. This statistic will be equal to one if fit is perfect, and to zero when regressors ''X'' have no explanatory power whatsoever. This is a biased estimate of the population ''R-squared'', and will never decrease if additional regressors are added, even if they are irrelevant.\n* ''Adjusted R-squared'' is a slightly modified version of <math>R^2</math>, designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression. This statistic is always smaller than <math>R^2</math>, can decrease as new regressors are added, and even be negative for poorly fitting models:\n:: <math>\\overline{R}^2 = 1 - \\frac{n - 1}{n - p}(1 - R^2)</math>\n* ''Log-likelihood'' is calculated under the assumption that errors follow normal distribution. Even though the assumption is not very reasonable, this statistic may still find its use in conducting LR tests.\n* ''[[Durbin–Watson statistic]]'' tests whether there is any evidence of serial correlation between the residuals. As a rule of thumb, the value smaller than 2 will be an evidence of positive correlation.\n* ''[[Akaike information criterion]]'' and ''[[Schwarz criterion]]'' are both used for model selection. Generally when comparing two alternative models, smaller values of one of these criteria will indicate a better model.<ref>\n{{Cite book\n | edition = 2nd\n | publisher = Springer\n | isbn = 0-387-95364-7\n | last = Burnham\n | first = Kenneth P.\n | author2=David Anderson\n | title = Model Selection and Multi-Model Inference\n | year = 2002\n}}</ref> \n* ''Standard error of regression'' is an estimate of ''σ'', standard error of the error term.\n* ''Total sum of squares'', ''model sum of squared'', and ''residual sum of squares'' tell us how much of the initial variation in the sample were explained by the regression.\n* ''F-statistic'' tries to test the hypothesis that all coefficients (except the intercept) are equal to zero. This statistic has ''F''(''p–1'',''n–p'') distribution under the null hypothesis and normality assumption, and its ''p-value'' indicates probability that the hypothesis is indeed true. Note that when errors are not normal this statistic becomes invalid, and other tests such as [[Wald test]] or [[likelihood ratio test|LR test]] should be used.\n\n[[File:OLS example weight vs height residuals.svg|thumb|right|300px|Residuals plot]]\nOrdinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model.  These are some of the common diagnostic plots:\n* Residuals against the explanatory variables in the model. A non-linear relation between these variables suggests that the linearity of the conditional mean function may not hold.  Different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity.\n* Residuals against explanatory variables not in the model. Any relation of the residuals to these variables would suggest considering these variables for inclusion in the model.\n* Residuals against the fitted values, <math>\\hat{y}</math>.\n* Residuals against the preceding residual.  This plot may identify serial correlations in the residuals.\n\nAn important consideration when carrying out statistical inference using regression models is how the data were sampled.  In this example, the data are averages rather than measurements on individual women.  The fit of the model is very good, but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height.\n\n===Sensitivity to rounding===\n{{main|Errors-in-variables models}}\n{{see also|Quantization error model|}}\n\nThis example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared. The heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimetre. Since the conversion factor is one inch to 2.54&nbsp;cm this is ''not'' an exact conversion. The original inches can be recovered by Round(x/0.0254) and then re-converted to metric without rounding. If this is done the results become:\n{| class=\"wikitable\"\n|-\n!\n! Const    !! Height    !! Height<sup>2</sup>\n|-\n| Converted to metric with rounding.\n| 128.8128 || −143.162  || 61.96033\n|-\n| Converted to metric without rounding.\n| 119.0205 || −131.5076 || 58.5046\n|}\n\n[[Image:HeightWeightResiduals.jpg|thumb|none|460px|Residuals to a quadratic fit for correctly and incorrectly converted data.]]\n\nUsing either of these equations to predict the weight of a 5' 6\" (1.6764m) woman gives similar values: 62.94&nbsp;kg with rounding vs. 62.98&nbsp;kg without rounding. Thus a seemingly small variation in the data has a real effect on the coefficients but a small effect on the results of the equation.\n\nWhile this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range ([[extrapolation]]).\n\nThis highlights a common error: this example is an abuse of OLS which inherently requires that the errors in the independent variable (in this case height) are zero or at least negligible. The initial rounding to nearest inch plus any actual measurement errors constitute a finite and non-negligible error. As a result, the fitted parameters are not the best estimates they are presumed to be. Though not totally spurious the error in the estimation will depend upon relative size of the ''x'' and ''y'' errors.\n\n== Another example with less real data ==\n\n=== Problem statement ===\nWe can use the least square mechanism to figure out the equation of a two body orbit in polar base co-ordinates. The equation typically used is <math>r(\\theta) = \\frac{p}{1-e\\cos(\\theta)}</math> where <math>r(\\theta)</math> is the radius of how far the object is from one of the bodies. In the equation the parameters <math>p</math> and <math>e</math> are used to determine the path of the orbit. We have measured the following data.  \n{| class=\"wikitable\"\n|<math>\\theta</math>(in degrees)\n|43\n|45\n|52\n|93\n|108\n|116\n|-\n|<math>r(\\theta)</math>\n|4.7126\n|4.5542\n|4.0419\n|2.2187\n|1.8910\n|1.7599\n|}\nWe need to find the least-squares approximation of <math>e</math> and <math>p</math> for the given data.\n\n=== Solution ===\nFirst we need to represent e and p in a linear form. So we are going to rewrite the equation <math>r(\\theta)</math> as <math>\\frac{1}{r(\\theta)} = \\frac{1}{p} - \\frac{e}{p}\\cos(\\theta)</math>. Now we can use this form to represent our observational data as:\n\n<math>A^{T}A \\binom{x}{y} = A^{T}b </math> where <math>x</math> is <math>\\frac{1}{p}</math> and <math>y</math> is <math>\\frac{e}{p}</math> and <math>A</math> is constructed by the first column being the coefficient of <math>\\frac{1}{p}</math> and the second column being the coefficient of <math>\\frac{e}{p}</math> and <math>b</math> is the values for the respective <math>\\frac{1}{r(\\theta)}</math> so   <math>A = \\begin{bmatrix} 1 & -0.731354\\\\1 & -0.707107\\\\1 & -0.615661\\\\1&\\ 0.052336\\\\1& 0.309017\\\\1&0.438371 \\end{bmatrix}</math> and <math>b = \\begin{bmatrix}  0.21220\\\\\n   0.21958\\\\\n   0.24741\\\\\n   0.45071\\\\\n   0.52883\\\\\n   0.56820\\end{bmatrix}.</math>\n\nOn solving we get <math>\\binom{x}{y} = \\binom{0.43478}{0.30435}</math>\n\nso ''<math>p=\\frac{1}{x} = 2.3000</math> and <math>e=p\\cdot y = 0.70001</math>''\n\n== See also ==\n* [[Minimum mean square error|Bayesian least squares]]\n* [[Fama–MacBeth regression]]\n* [[Non-linear least squares]]\n* [[Numerical methods for linear least squares]]\n* [[Nonlinear system identification]]\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n*{{cite book |last=Dougherty |first=Christopher  |title=Introduction to Econometrics |location=New York |publisher=Oxford University Press |edition=2nd |year=2002 |isbn=0-19-877643-8 |pages=48–113 }}\n*{{cite book |last=Gujarati |first=Damodar N. |authorlink=Damodar N. Gujarati |last2=Porter |first2=Dawn C. |title=Basic Econometics |location=Boston |publisher=McGraw-Hill Irwin |edition=Fifth |year=2009 |isbn=978-0-07-337577-9 |pages=55–96 }}\n*{{cite book |last=Hill |first=R. Carter |last2=Griffiths |first2=William E. |last3=Lim |first3=Guay C. |title=Principles of Econometrics |location=Hoboken, NJ |publisher=John Wiley & Sons |edition=3rd |year=2008 |isbn=978-0-471-72360-8 |pages=8–47 }}\n*{{cite book |last=Wooldridge |first=Jeffrey |authorlink=Jeffrey Wooldridge |chapter=The Simple Regression Model |title=Introductory Econometrics: A Modern Approach |location=Mason, OH |publisher=Cengage Learning |edition=4th |year=2008 |pages=22–67 |isbn=978-0-324-58162-1 |chapterurl=https://books.google.com/books?id=64vt5TDBNLwC&pg=PA22 }}\n\n{{Least Squares and Regression Analysis}}\n\n{{DEFAULTSORT:Ordinary Least Squares}}\n[[Category:Parametric statistics]]\n[[Category:Least squares]]"
    },
    {
      "title": "Partial least squares path modeling",
      "url": "https://en.wikipedia.org/wiki/Partial_least_squares_path_modeling",
      "text": "The '''partial least squares path modeling''' or '''partial least squares structural equation modeling''' ('''PLS-PM''', '''PLS-SEM''')<ref>{{cite book |first1=J.F. |last1=Hair |first2=G.T.M. |last2=Hult |first3=C.M. |last3=[[Christian M. Ringle|Ringle]] |first4=M. |last4=Sarstedt |title=A Primer on Partial Least Squares Structural Equation Modeling (PLS-SEM) |edition=2 |publisher=Sage |location=Thousand Oaks, CA |year=2017 |isbn=9781483377445 |url=https://uk.sagepub.com/en-gb/eur/a-primer-on-partial-least-squares-structural-equation-modeling-pls-sem/book244583}}</ref><ref>{{cite book |first1=V.E. |last1=Vinzi |first2=L. |last2=Trinchera |first3=S. |last3=Amato |title=Handbook of partial least squares |publisher=Springer Berlin Heidelberg |year=2010 }}</ref><ref>{{cite book |first1=J.F. |last1=Hair |first2=M. |last2=Sarstedt |first3=C.M. |last3=[[Christian M. Ringle|Ringle]] |first4=S.P. |last4=Gudergan |title=Advanced Issues in Partial Least Squares Structural Equation Modeling (PLS-SEM) |publisher=Sage |location=Thousand Oaks, CA |year=2018 |isbn=9781483377391 |url=https://uk.sagepub.com/en-gb/eur/advanced-issues-in-partial-least-squares-structural-equation-modeling/book243803}}</ref> is a method of [[structural equation modeling]] which allows estimating complex cause-effect relationship models with [[latent variables]].\n\n==Overview==\nPLS-PM is a component-based estimation approach that differs from the covariance-based [[structural equation modeling]]. Unlike covariance-based approaches to structural equation modeling, PLS-PM does not fit a common factor model to the data, it rather fits a composite model <ref>{{Cite journal|last=Henseler|first=Jörg|last2=Dijkstra|first2=Theo K.|last3=Sarstedt|first3=Marko|last4=Ringle|first4=Christian M.|last5=Diamantopoulos|first5=Adamantios|last6=Straub|first6=Detmar W.|last7=Ketchen|first7=David J.|last8=Hair|first8=Joseph F.|last9=Hult|first9=G. Tomas M.|date=2014-04-10|title=Common Beliefs and Reality About PLS|journal=Organizational Research Methods|language=en|volume=17|issue=2|pages=182–209|doi=10.1177/1094428114526928}}</ref><ref>{{Cite journal|title=Composites as Factors, generalized canonical variables revisited (PDF Download Available)|url=http://rgdoi.net/10.13140/RG.2.1.3426.5449|journal=ResearchGate|language=en|doi=10.13140/rg.2.1.3426.5449|year=2013|last1=Dijkstra|first1=Theo K.}}</ref><ref>{{Cite journal|title=all-inclusive and single block composites (PDF Download Available)|url=http://rgdoi.net/10.13140/RG.2.1.2917.8082|journal=ResearchGate|language=en|doi=10.13140/rg.2.1.2917.8082|year=2015|last1=Dijkstra|first1=Theo K.}}</ref>; In doing so, it maximizes the amount of variance explained (though what this means from a statistical point of view is unclear and PLS users do not agree on how this goal might be achieved). In addition, by an adjustment PLS is capable to consistently estimate common factor models as well. This new approach is called consistent PLS (PLSc)<ref>{{Cite journal|last=Dijkstra|first=Theo K.|last2=Henseler|first2=Jörg|date=2015-01-01|title=Consistent and asymptotically normal PLS estimators for linear structural equations|journal=Computational Statistics & Data Analysis|volume=81|pages=10–23|doi=10.1016/j.csda.2014.07.008}}</ref>. Furthermore, PLS-PM can be used for out-sample prediction purposes<ref>{{Cite journal|last=Shmueli|first=Galit|last2=Ray|first2=Soumya|last3=Velasquez Estrada|first3=Juan Manuel|last4=Chatla|first4=Suneel Babu|date=2016-10-01|title=The elephant in the room: Predictive performance of PLS models|journal=Journal of Business Research|volume=69|issue=10|pages=4552–4564|doi=10.1016/j.jbusres.2016.03.049}}</ref> and can be employed as an estimator in [[Confirmatory composite analysis|confirmatory composite analysis]]<ref>{{cite journal |last1=Schuberth |first1=Florian |last2=Henseler |first2=Jörg |last3=Dijkstra |first3=Theo K. |title=Confirmatory Composite Analysis |journal=Frontiers in Psychology |date=2018 |volume=9 |doi=10.3389/fpsyg.2018.02541}}</ref>. \n\nThe PLS structural equation model is composed of two sub-models: the measurement model and structural model. The measurement model represents the relationships between the observed data and the [[latent variables]]. The structural model represents the relationships between the latent variables.\n\nAn iterative algorithm solves the structural equation model by estimating the [[latent variables]] by using the measurement and structural model in alternating steps, hence the procedure's name, partial. The measurement model estimates the latent variables as a weighted sum of its manifest variables. The structural model estimates the latent variables by means of simple or multiple [[linear regression]] between the latent variables estimated by the measurement model. This algorithm repeats itself until convergence is achieved.\n\nWith the availability of software applications, PLS-SEM became particularly popular in social sciences disciplines such as accounting,<ref>{{cite journal|last1=Lee|first1=L.|last2=Petter|first2=S.|last3=Fayard|first3=D.|last4=Robinson|first4=S.|title=On the Use of Partial Least Squares Path Modeling in Accounting Research|journal=International Journal of Accounting Information Systems|date=2011|volume=12|issue=4|pages=305–328|doi=10.1016/j.accinf.2011.05.002}}</ref> family business,<ref>{{cite journal|last1=Sarstedt|first1=M.|last2=Ringle|first2=C.M.|last3=Smith|first3=D.|last4=Reams|first4=R.|last5=Hair|first5=J.F.|title=Partial Least Squares Structural Equation Modeling (PLS-SEM): A Useful Tool for Family Business Researchers|journal=Journal of Family Business Strategy|date=2014|volume=5|issue=1|pages=105–115|doi=10.1016/j.jfbs.2014.01.002}}</ref> marketing,<ref>{{cite journal|last1=Sarstedt|first1=M.|last2=Ringle|first2=C.M.|last3=Hair|first3=J.F.|last4=Mena|first4=J.A.|title=An Assessment of the Use of Partial Least Squares Structural Equation Modeling in Marketing Research|journal=Journal of the Academy of Marketing Science|date=2012|volume=40|issue=3|pages=414–433|doi=10.1007/s11747-011-0261-6}}</ref> management information systems,<ref>Schmitz, K. W., Teng, J. T., & Webb, K. J. (2016). [http://cits.tamiu.edu/warppls/pubs/Schmitz_etal_2016_MISQ_AdapStrucTheoryIndiv.pdf Capturing the complexity of malleable IT use: Adaptive structuration theory for individuals.] Management Information Systems Quarterly, 40(3), 663-686.</ref><ref>{{cite journal|last1=Ringle|first1=C.M.|last2=Sarstedt|first2=M.|last3=Straub|first3=D.W.|title=A Critical Look at the Use of PLS-SEM in MIS Quarterly|journal=MIS Quarterly|date=2012|volume=36|issue=1|page=iii-xiv|url=http://misq.org/skin/frontend/default/misq/pdf/V36I1/EdCommentsV36N1.pdf}}</ref> operations management,<ref>{{cite journal|last1=Peng|first1=D.X.|last2=Lai|first2=F.|title=Using Partial Least Squares in Operations Management Research: A Practical Guideline and Summary of Past Research|journal=Journal of Operations Management|date=2012|volume=30|issue=6|pages=467–480 |doi=10.1016/j.jom.2012.06.002}}</ref> strategic management,<ref>{{cite journal|last1=Hair|first1=J.F.|last2=Sarstedt|first2=M.|last3=Pieper|first3=T.|last4=Ringle|first4=C.M.|title=The Use of Partial Least Squares Structural Equation Modeling in Strategic Management Research: A Review of Past Practices and Recommendations for Future Applications|journal=Long Range Planning|date=2012|volume=45|issue=5–6|pages=320–340|doi=10.1016/j.lrp.2012.09.008}}</ref> and tourism.<ref>Rasoolimanesh, S.M., Jaafar, M., Kock, N. and Ahmad, A. G. (2017). [http://cits.tamiu.edu/kock/pubs/journals/2017/Rasoolimanesh_etal_2017_JST_WorldHeritageSiteSustainableTourism.pdf The effects of community factors on residents’ perceptions toward World Heritage Site inscription and sustainable tourism development.] Journal of Sustainable Tourism, 25(2), 198-216.</ref> Recently, areas such as [[engineering]], [[environmental sciences]],<ref>Brewer, T.D., Cinner, J.E., Fisher, R., Green, A., & Wilson, S.K. (2012). [http://www.scriptwarp.com/warppls/pubs/Brewer_et_al_2012_GlobalEnvironChange.pdf Market access, population density, and socioeconomic development explain diversity and functional group biomass of coral reef fish assemblages.] Global Environmental Change, 22(2), 399-406.</ref> [[medicine]],<ref>Berglund, E., Lytsy, P., & Westerling, R. (2012). [http://www.scriptwarp.com/warppls/pubs/Berglund_etal_2012_LipidLoweringTreatments.pdf Adherence to and beliefs in lipid-lowering medical treatments: A structural equation modeling approach including the necessity-concern framework.] Patient Education and Counseling, 91(1), 105-112.</ref> and [[political sciences]] more broadly use PLS-SEM to estimate complex cause-effect relationship models with [[latent variables]]. Thereby, they analyse, explore and test their established and underlying their [[conceptual models]] and [[theory]].\n\nPLS is viewed critically by several methodological researchers.<ref>{{cite journal|last1= Rönkkö |first1=M.|last2= McIntosh |first2= C.N. |last3= Antonakis |first3= J. |last4= Edwards |first4= J.R. |title= Partial least squares path modeling: Time for some serious second thoughts |journal= Journal of Operations Management |date=2016|volume=47–48| pages=9–27 |doi=10.1016/j.jom.2016.05.002}}</ref><ref>Goodhue, D. L., Lewis, W., & Thompson, R. (2012). [http://aisel.aisnet.org/misq/vol36/iss3/18/ Does PLS have advantages for small sample size or non-normal data?] MIS Quarterly, 981-1001.</ref> A major point of contention has been the claim that PLS can always be used with very small sample sizes. A recent study suggests that this claim is generally unjustified, and proposes two methods for minimum sample size estimation in PLS.<ref>Kock, N., & Hadaya, P. (2018). [http://cits.tamiu.edu/kock/pubs/journals/2018/Kock_Hadaya_2018_ISJ_SampleSizePLS.pdf Minimum sample size estimation in PLS-SEM: The inverse square root and gamma-exponential methods.] Information Systems Journal, 28(1), 227–261.</ref> Another point of contention is the ad hoc way in which PLS has been developed and the lack of analytic proofs to support its main feature: the sampling distribution of PLS weights. However, PLS-SEM is still considered preferable (over CB-SEM) when it is unknown whether the data's nature is common factor- or composite-based.<ref>{{cite journal|last1= Sarstedt |first1=M.|last2= Hair |first2= J.F. |last3= Ringle |first3= C.M. |last4= Thiele |first4= K.O. |last5= Gudergan |first5= S.P. |title= Estimation issues with PLS and CBSEM: Where the bias lies! |journal= Journal of Business Research |date=2016|volume=69|issue=10| pages=3998–4010|doi=10.1016/j.jbusres.2016.06.007}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Least squares]]\n[[Category:Graphical models]]\n[[Category:Psychometrics]]\n[[Category:Structural equation models]]"
    },
    {
      "title": "Partial least squares regression",
      "url": "https://en.wikipedia.org/wiki/Partial_least_squares_regression",
      "text": "{{Regression bar}}\n'''Partial least squares regression (PLS regression)''' is a [[statistics|statistical]] method that bears some relation to [[principal component regression|principal components regression]]; instead of finding [[hyperplane]]s of maximum [[variance]] between the response and independent variables, it finds a [[linear regression]] model by projecting the [[predicted variable]]s and the [[observable variable]]s to a new space. Because both the ''X'' and ''Y'' data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical.\n\nPLS is used to find the fundamental relations between two [[matrix (mathematics)|matrices]] (''X'' and ''Y''), i.e. a [[latent variable]] approach to modeling the [[covariance]] structures in these two spaces. A PLS model will try to find the multidimensional direction in the ''X'' space that explains the maximum multidimensional variance direction in the ''Y'' space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is [[multicollinearity]] among ''X'' values. By contrast, standard regression will fail in these cases (unless it is [[Tikhonov regularization|regularized]]).\n\nPartial least squares was introduced by the Swedish statistician [[Herman Wold|Herman O. A. Wold]], who then developed it with his son, Svante Wold. An alternative term for PLS (and more correct according to Svante Wold<ref name=\"wold_2001\">{{cite journal |last1=Wold |first1=S |last2=Sjöström |first2=M. |last3=Eriksson |first3=L. |title=PLS-regression: a basic tool of chemometrics |journal=Chemometrics and Intelligent Laboratory Systems |volume=58 |issue=2 |pages=109–130 |year=2001 |doi=10.1016/S0169-7439(01)00155-1 |url=http://www.sciencedirect.com/science/article/pii/S0169743901001551}}</ref>) is '''''projection to latent structures''''', but the term ''partial least squares'' is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in [[chemometrics]] and related areas.  It is also used in bioinformatics, sensometrics, neuroscience and anthropology.\n\n==Underlying model==\n\nThe general underlying model of multivariate PLS is\n\n:<math>X = T P^\\mathrm{T} + E</math>\n:<math>Y = U Q^\\mathrm{T} + F</math>\n\nwhere {{mvar|X}} is an <math>n \\times m</math> matrix of predictors, {{mvar|Y}} is an <math>n \\times p</math> matrix of responses; {{mvar|T}} and {{mvar|U}} are <math>n \\times l</math> matrices that are, respectively, projections of {{mvar|X}} (the ''X score'', ''component'' or ''factor'' matrix) and projections of {{mvar|Y}} (the ''Y scores''); {{mvar|P}} and {{mvar|Q}} are, respectively, <math>m \\times l</math> and <math>p \\times l</math> orthogonal ''loading'' matrices; and matrices {{mvar|E}} and {{mvar|F}} are the error terms, assumed to be independent and identically distributed random normal variables. The decompositions of {{mvar|X}} and {{mvar|Y}} are made so as to maximise the [[covariance]] between {{mvar|T}} and {{mvar|U}}.\n\n==Algorithms==\n\nA number of variants of PLS exist for estimating the factor and loading matrices {{mvar|T, U, P}} and {{mvar|Q}}.  Most of them construct estimates of the linear regression between {{mvar|X}} and {{mvar|Y}} as <math>Y = X \\tilde{B} + \\tilde{B}_0</math>. Some PLS algorithms are only appropriate for the case where {{mvar|Y}} is a column vector, while others deal with the general case of a matrix {{mvar|Y}}. Algorithms also differ on whether they estimate the factor matrix {{mvar|T}} as an orthogonal, an [[orthonormal matrix]] or not.<ref>\n{{cite journal |last1=Lindgren |first1=F |last2=Geladi |first2=P |last3=Wold |first3=S |title=The kernel algorithm for PLS |journal=J. Chemometrics |volume=7 |pages=45–59 |year=1993 |doi=10.1002/cem.1180070104 }}</ref><ref>{{cite journal |last1=de Jong |first1=S. |last2=ter Braak |first2=C.J.F. |title=Comments on the PLS kernel algorithm |journal=J. Chemometrics |volume=8 |issue=2 |pages=169–174 |year=1994 |doi=10.1002/cem.1180080208 }}</ref><ref>{{cite journal |last1=Dayal |first1=B.S. |last2=MacGregor |first2=J.F. |title=Improved PLS algorithms |journal=J. Chemometrics |volume=11 |issue=1 |pages=73–85 |year=1997 |doi=10.1002/(SICI)1099-128X(199701)11:1<73::AID-CEM435>3.0.CO;2-# }}</ref><ref>{{cite journal |last=de Jong |first=S. |title=SIMPLS: an alternative approach to partial least squares regression |journal=Chemometrics and Intelligent Laboratory Systems |volume=18 |pages=251–263 |year=1993 |doi=10.1016/0169-7439(93)85002-X |issue=3 }}</ref><ref>{{cite journal |last1=Rannar |first1=S. |last2=Lindgren |first2=F. |last3=Geladi |first3=P. |last4=Wold |first4=S. |title=A PLS Kernel Algorithm for Data Sets with Many Variables and Fewer Objects. Part 1: Theory and Algorithm |journal=J. Chemometrics |volume=8 |issue=2 |pages=111–125 |year=1994 |doi=10.1002/cem.1180080204 }}</ref><ref>{{cite journal |last=Abdi |first=H. |title=Partial least squares regression and projection on latent structure regression (PLS-Regression) |journal=Wiley Interdisciplinary Reviews: Computational Statistics |volume=2 |pages=97–106 |year=2010 |doi=10.1002/wics.51 }}</ref> \nThe final prediction will be the same for all these varieties of PLS, but the components will differ.\n\n===PLS1===\n\nPLS1 is a widely used algorithm appropriate for the vector {{mvar|Y}} case. It estimates {{math|T}} as an orthonormal matrix. In pseudocode it is expressed below (capital letters are matrices, lower case letters are vectors if they are superscripted and scalars if they are subscripted):\n\n  1 {{nowrap|'''function''' PLS1({{mvar|X, y, l}})}}\n  2   {{nowrap|<math>X^{(0)} \\gets X</math>}}\n  3   {{nowrap|<math>w^{(0)} \\gets X^\\mathrm{T} y/||X^\\mathrm{T}y||</math>}}, an initial estimate of {{mvar|w}}.\n  4   {{nowrap|'''for''' <math>k = 0</math> '''to''' <math>l-1</math>}}\n  5      {{nowrap|<math>t^{(k)} \\gets X^{(k)}w^{(k)}</math>}}\n  6      {{nowrap|<math>t_k \\gets {t^{(k)}}^\\mathrm{T} t^{(k)}</math> (note this is a scalar)}}\n  7      {{nowrap|<math>t^{(k)} \\gets t^{(k)} / t_k</math>}}\n  8      {{nowrap|<math>p^{(k)} \\gets {X^{(k)}}^\\mathrm{T} t^{(k)}</math>}}\n  9      {{nowrap|<math>q_k \\gets {y}^\\mathrm{T} t^{(k)}</math> (note this is a scalar)}}\n 10      {{nowrap|'''if''' <math>q_k = 0</math>}}\n 11          {{nowrap|<math>l \\gets k</math>, '''break''' the '''for loop'''}}\n 12      {{nowrap|'''if''' <math>k < (l-1)</math>}}\n 13          {{nowrap|<math>X^{(k+1)} \\gets X^{(k)} - t_k t^{(k)} {p^{(k)}}^\\mathrm{T}</math>}}\n 14          {{nowrap|<math>w^{(k+1)} \\gets {X^{(k+1)}}^\\mathrm{T} y </math>}}\n 15   {{nowrap|'''end''' '''for'''}}\n 16   '''define''' {{mvar|W}} to be the matrix {{nowrap|with columns <math>w^{(0)},w^{(1)},...,w^{(l-1)}</math>.}}\n      Do the same to form the {{mvar|P}} matrix and {{mvar|q}} vector.\n 17   {{nowrap|<math>B \\gets W {(P^\\mathrm{T} W)}^{-1} q</math>}}\n 18   {{nowrap|<math>B_0 \\gets q_0 - {P^{(0)}}^\\mathrm{T} B</math>}}\n 19   {{nowrap|'''return''' <math>B, B_0</math>}}\n\nThis form of the algorithm does not require centering of the input {{mvar|X}} and {{mvar|Y}}, as this is performed implicitly by the algorithm.\nThis algorithm features 'deflation' of the matrix {{mvar|X}} (subtraction of <math>t_k t^{(k)} {p^{(k)}}^\\mathrm{T}</math>), but deflation of the vector {{mvar|y}} is not performed, as it is not necessary (it can be proved that deflating {{mvar|y}} yields the same results as not deflating). The user-supplied variable {{mvar|l}} is the limit on the number of latent factors in the regression; if it equals the rank of the matrix {{mvar|X}}, the algorithm will yield the least squares regression estimates for {{mvar|B}} and <math>B_0</math>\n\n==Extensions==\nIn 2002 a new method was published called orthogonal projections to latent structures (OPLS). In OPLS, continuous variable data is separated into predictive and uncorrelated information.  This leads to improved diagnostics, as well as more easily interpreted visualization. However, these changes only improve the interpretability, not the predictivity, of the PLS models.<ref>{{Cite journal\n  | last = Trygg\n  | first = J\n  | last2 = Wold \n  | first2 = S\n  | title = Orthogonal Projections to Latent Structures\n  | journal = Journal of Chemometrics\n  | volume = 16\n  | issue = 3\n  | pages = 119–128\n  | year = 2002\n  | url = \n  | doi = 10.1002/cem.695}}\n</ref> L-PLS extends PLS regression to 3 connected data blocks.<ref>{{cite journal |last1=Sæbøa |first1=S. |last2=Almøya |first2=T. |last3=Flatbergb |first3=A. |last4=Aastveita |first4=A.H. |last5=Martens |first5=H. |title=LPLS-regression: a method for prediction and classification under the influence of background information on predictor variables |journal=Chemometrics and Intelligent Laboratory Systems |volume=91  |issue=2 |pages=121–132 |year=2008 |doi=10.1016/j.chemolab.2007.10.006 }}</ref>  Similarly, OPLS-DA (Discriminant Analysis) may be applied when working with discrete variables, as in classification and biomarker studies.\n\nIn 2015 partial least squares was related to a procedure called the three-pass regression filter (3PRF).<ref>{{Cite journal|last=Kelly|first=Bryan|last2=Pruitt|first2=Seth|date=2015-06-01|title=The three-pass regression filter: A new approach to forecasting using many predictors|url=http://www.sciencedirect.com/science/article/pii/S0304407615000354|journal=Journal of Econometrics|series=High Dimensional Problems in Econometrics|volume=186|issue=2|pages=294–316|doi=10.1016/j.jeconom.2015.02.011}}</ref> Supposing the number of observations and variables are large, the 3PRF (and hence PLS) is asymptotically normal for the \"best\" forecast implied by a linear latent factor model. In stock market data, PLS has been shown to provide accurate out-of-sample forecasts of returns and cash-flow growth.<ref>{{Cite journal|last=Kelly|first=Bryan|last2=Pruitt|first2=Seth|date=2013-10-01|title=Market Expectations in the Cross-Section of Present Values|journal=The Journal of Finance|volume=68|issue=5|pages=1721–1756|doi=10.1111/jofi.12060|issn=1540-6261|citeseerx=10.1.1.498.5973}}</ref>\n\nA PLS version based on [[Singular value decomposition|singular value decomposition (SVD)]] provides a memory efficient implementation that can be used to address high-dimensional problems, such as relating millions of genetic markers to thousands of imaging features in imaging genetics, on consumer-grade hardware.<ref>{{Cite journal|last=Lorenzi|first=Marco|last2=Altmann|first2=Andre|last3=Gutman|first3=Boris|last4=Wray|first4=Selina|last5=Arber|first5=Charles|last6=Hibar|first6=Derrek P.|last7=Jahanshad|first7=Neda|last8=Schott|first8=Jonathan M.|last9=Alexander|first9=Daniel C.|date=2018-03-20|title=Susceptibility of brain atrophy to TRIB3 in Alzheimer's disease, evidence from functional prioritization in imaging genetics|journal=Proceedings of the National Academy of Sciences|volume=115|issue=12|pages=3162–3167|doi=10.1073/pnas.1706100115|issn=0027-8424|pmc=5866534|pmid=29511103}}</ref>\n\nPLS correlation (PLSC) is another methodology related to PLS regression,<ref name=\":0\">{{Cite journal|last=Krishnan|first=Anjali|last2=Williams|first2=Lynne J.|last3=McIntosh|first3=Anthony Randal|last4=Abdi|first4=Hervé|date=May 2011|title=Partial Least Squares (PLS) methods for neuroimaging: A tutorial and review|url=https://linkinghub.elsevier.com/retrieve/pii/S1053811910010074|journal=NeuroImage|volume=56|issue=2|pages=455–475|doi=10.1016/j.neuroimage.2010.07.034}}</ref> which has been used in neuroimaging <ref name=\":0\" /><ref>{{Cite journal|last=McIntosh|first=Anthony R.|last2=Mišić|first2=Bratislav|date=2013-01-03|title=Multivariate Statistical Analyses for Neuroimaging Data|url=http://www.annualreviews.org/doi/10.1146/annurev-psych-113011-143804|journal=Annual Review of Psychology|volume=64|issue=1|pages=499–525|doi=10.1146/annurev-psych-113011-143804|issn=0066-4308}}</ref><ref>{{Cite journal|last=Beggs|first=Clive B.|last2=Magnano|first2=Christopher|last3=Belov|first3=Pavel|last4=Krawiecki|first4=Jacqueline|last5=Ramasamy|first5=Deepa P.|last6=Hagemeier|first6=Jesper|last7=Zivadinov|first7=Robert|date=2016-05-02|editor-last=de Castro|editor-first=Fernando|title=Internal Jugular Vein Cross-Sectional Area and Cerebrospinal Fluid Pulsatility in the Aqueduct of Sylvius: A Comparative Study between Healthy Subjects and Multiple Sclerosis Patients|url=https://dx.plos.org/10.1371/journal.pone.0153960|journal=PLOS ONE|volume=11|issue=5|pages=e0153960|doi=10.1371/journal.pone.0153960|issn=1932-6203|pmc=4852898|pmid=27135831}}</ref>  and more recently in sport science,<ref>{{Cite journal|last=Weaving|first=Dan|last2=Jones|first2=Ben|last3=Ireton|first3=Matt|last4=Whitehead|first4=Sarah|last5=Till|first5=Kevin|last6=Beggs|first6=Clive B.|date=2019-02-14|editor-last=Connaboy|editor-first=Chris|title=Overcoming the problem of multicollinearity in sports performance data: A novel application of partial least squares correlation analysis|url=http://dx.plos.org/10.1371/journal.pone.0211776|journal=PLOS ONE|volume=14|issue=2|pages=e0211776|doi=10.1371/journal.pone.0211776|issn=1932-6203}}</ref> to quantify the strength of the relationship between data sets. Typically, PLSC divides the data into two blocks (sub-groups) each containing one or more variables, and then uses [[Singular value decomposition|singular value decomposition (SVD)]] to establish the strength of any relationship (i.e. the amount of shared information) that might exist between the two component sub-groups.<ref name=\":1\">{{Citation|last=Abdi|first=Hervé|title=Partial Least Squares Methods: Partial Least Squares Correlation and Partial Least Square Regression|date=2013|url=http://link.springer.com/10.1007/978-1-62703-059-5_23|work=Computational Toxicology|volume=930|pages=549–579|editor-last=Reisfeld|editor-first=Brad|publisher=Humana Press|doi=10.1007/978-1-62703-059-5_23|isbn=9781627030588|access-date=2019-02-18|last2=Williams|first2=Lynne J.|editor2-last=Mayeno|editor2-first=Arthur N.}}</ref> It does this by using SVD to determine the inertia (i.e. the sum of the singular values) of the covariance matrix of the sub-groups under consideration.<ref name=\":1\" /><ref name=\":0\" />\n\n==See also==\n*[[Canonical correlation]]\n*[[Data mining]]\n*[[Deming regression]]\n*[[Feature extraction]]\n*[[Machine learning]]\n*[[Multilinear subspace learning]]\n*[[Partial least squares path modeling]]\n*[[Principal component analysis]]\n*[[Regression analysis]]\n*[[Total sum of squares]]\n\n==Further reading==\n*{{cite book |first=R. |last=Kramer |title=Chemometric Techniques for Quantitative Analysis |publisher=Marcel-Dekker |year=1998 |isbn=978-0-8247-0198-7 }}\n*{{cite journal |last1=Frank |first1=Ildiko E. |first2=Jerome H. |last2=Friedman |title=A Statistical View of Some Chemometrics Regression Tools |journal=Technometrics |volume=35 |issue=2 |pages=109–148 |year=1993 |doi=10.1080/00401706.1993.10485033 }}\n*{{cite journal |last1=Haenlein |first1=Michael |first2=Andreas M. |last2=Kaplan | title=A Beginner's Guide to Partial Least Squares Analysis |journal=Understanding Statistics |volume=3 |issue=4 |pages=283–297| year=2004 |doi=10.1207/s15328031us0304_4 }}\n*{{cite journal |last1=Henseler |first1=Joerg |first2=Georg |last2=Fassott | title=Testing Moderating Effects in PLS Path Models. An Illustration of Available Procedures| year=2005 }}\n*{{cite journal |last1=Lingjærde |first1=Ole-Christian |first2=Nils |last2=Christophersen | title=Shrinkage Structure of Partial Least Squares |journal=Scandinavian Journal of Statistics |volume=27 |issue=3 |pages=459–473 | year=2000 |doi=10.1111/1467-9469.00201 }}\n*{{cite book | last=Tenenhaus |first=Michel | title= La Régression PLS: Théorie et Pratique. Paris: Technip.| year=1998}}\n*{{cite journal | last1=Rosipal |first1=Roman |first2=Nicole |last2=Kramer | title=Overview and Recent Advances in Partial Least Squares, in Subspace, Latent Structure and Feature Selection Techniques |pages=34–51 | year=2006}}\n*{{cite journal |last=Helland |first=Inge S. |title=PLS regression and statistical models |journal=Scandinavian Journal of Statistics |volume=17 |issue=2 |pages=97–114 |year=1990 |jstor=4616159}}\n*{{cite book |authorlink=Herman Wold |last=Wold |first=Herman |chapter=Estimation of principal components and related models by iterative least squares |editor-first=P.R. |editor-last=Krishnaiaah |title=Multivariate Analysis |publisher=Academic Press |location=New York |year=1966 |pages=391–420 }}\n*{{cite book |last=Wold |first=Herman |title=The fix-point approach to interdependent systems |publisher=North Holland |location=Amsterdam |year=1981 }}\n*{{cite book |last=Wold |first=Herman |chapter=Partial least squares |editor1-first=Samuel |editor1-last=Kotz |editor2-first=Norman L. |editor2-last=Johnson |title=Encyclopedia of statistical sciences |publisher=Wiley |location=New York |year=1985 |pages=581–591 |volume=6}}\n*{{cite journal |first1=Svante |last1=Wold |first2=Axel |last2=Ruhe |first3=Herman |last3=Wold |first4=W.J. |last4=Dunn |title=The collinearity problem in linear regression. the partial least squares (PLS) approach to generalized inverses |journal=SIAM Journal on Scientific and Statistical Computing |volume=5 |pages=735–743 |year=1984 |doi=10.1137/0905052 |issue=3 }}\n*{{cite journal |last=Garthwaite |first=Paul H. |title=An Interpretation of Partial Least Squares |journal=[[Journal of the American Statistical Association]] |volume=89 |pages=122–7 |year=1994 |jstor=2291207 |doi=10.1080/01621459.1994.10476452 |issue=425}}\n*{{cite book |editor1-last=Wang |editor1-first=H. |title=Handbook of Partial Least Squares |year=2010 |isbn=978-3-540-32825-4 }}\n*{{cite journal |last1=Stone |first1=M. |last2=Brooks |first2=R.J. |title=Continuum Regression: Cross-Validated Sequentially Constructed Prediction embracing Ordinary Least Squares, Partial Least Squares and Principal Components Regression |journal=Journal of the Royal Statistical Society, Series B |volume=52 |issue=2 |pages=237–269 |year=1990 |jstor=2345437}}\n*Wan Mohamad Asyraf  Bin Wan Afthanorhan. (2013). A Comparison Of Partial Least Square Structural Equation Modeling (PLS-SEM) and Covariance Based Structural EquationModeling (CB-SEM) for Confirmatory Factor Analysis International Journal of Engineering Science and Innovative Technology (IJESIT), 2(5), 9.\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Prone to spam|date=November 2017}}\n{{Z148}}<!--     {{No more links}}\n\n       Please be cautious adding more external links.\n\nWikipedia is not a collection of links and should not be used for advertising.\n\n     Excessive or inappropriate links will be removed.\n\n See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.\n\nIf there are already suitable links, propose additions or replacements on\nthe article's talk page.\n\n-->\n*[http://www.utd.edu/~herve/Abdi-PLSR2007-pretty.pdf A short introduction to PLS regression and its history]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Partial Least Squares Regression}}\n[[Category:Latent variable models]]\n[[Category:Least squares]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Partition of sums of squares",
      "url": "https://en.wikipedia.org/wiki/Partition_of_sums_of_squares",
      "text": "{{broader|Analysis of variance}}\n{{About|the partition of sums of squares in statistics||Sum of squares (disambiguation){{!}}Sum of squares}}\n{{distinguish-redirect|Variance partitioning|Variance decomposition}}\n{{Expert-subject|Statistics|date=November 2008}}\n\nThe '''partition of sums of squares''' is a concept that permeates much of [[inferential statistics]] and [[descriptive statistics]]. More properly, it is the '''partitioning of sums of [[squared deviations]] or errors'''. Mathematically, the sum of squared deviations is an unscaled, or unadjusted measure of [[statistical dispersion|dispersion]] (also called [[statistical variability|variability]]). When scaled for the number of [[Degrees of freedom (statistics)|degrees of freedom]], it estimates the [[variance]], or spread of the observations about their mean value. Partitioning of the sum of squared deviations into various components allows the overall variability in a dataset to be ascribed to different types or sources of variability, with the relative importance of each being quantified by the size of each component of the overall sum of squares.\n\n==Background==\n\nThe distance from any point in a collection of data, to the mean of the data, is the deviation. This can be written as <math>y_i - \\overline{y}</math>, where <math>y_i</math> is the ith data point, and <math>\\overline{y}</math> is the estimate of the mean. If all such deviations are squared, then summed, as in <MATH>\\sum_{i=1}^n\\left(y_i-\\overline{y}\\,\\right)^2</MATH>, this gives the \"sum of squares\" for these data.\n\nWhen more data are added to the collection the sum of squares will increase, except in unlikely cases such as the new data being equal to the mean. So usually, the sum of squares will grow with the size of the data collection. That is a manifestation of the fact that it is unscaled.\n\nIn many cases, the number of [[degrees of freedom (statistics)|degrees of freedom]] is simply the number of data in the collection, minus one. We write this as ''n''&nbsp;&minus;&nbsp;1, where ''n'' is the number of data.\n\nScaling (also known as normalizing) means adjusting the sum of squares so that it does not grow as the size of the data collection grows. This is important when we want to compare samples of different sizes, such as a sample of 100 people compared to a sample of 20 people. If the sum of squares was not normalized, its value would always be larger for the sample of 100 people than for the sample of 20 people. To scale the sum of squares, we divide it by the degrees of freedom, i.e., calculate the sum of squares per degree of freedom, or variance. [[Standard deviation]], in turn, is the square root of the variance.\n\nThe above information is how sum of squares is used in descriptive statistics; see the article on [[total sum of squares]] for an application of this broad principle to [[inferential statistics]].\n\n==Partitioning the sum of squares in linear regression==\n\n'''Theorem.''' Given a [[linear regression model]] <math> y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i </math> ''including a constant'' <math> \\beta_0</math>, based on a sample <math> (y_i, x_{i1}, \\ldots, x_{ip}), \\, i = 1, \\ldots, n </math> containing ''n'' observations, the total sum of squares <math>\\mathrm{TSS} = \\sum_{i = 1}^n (y_i - \\bar{y})^2 </math> can be partitioned as follows into the [[explained sum of squares]] (ESS) and the [[residual sum of squares]] (RSS):\n:<math>\\mathrm{TSS} =  \\mathrm{ESS} + \\mathrm{RSS},</math>\nwhere this equation is equivalent to each of the following forms:\n\n:<math>\n\\begin{align}\n\\left\\| y - \\bar{y} \\mathbf{1} \\right\\|^2 &=  \\left\\| \\hat{y} - \\bar{y} \\mathbf{1} \\right\\|^2 + \\left\\| \\hat{\\varepsilon} \\right\\|^2, \\quad \\mathbf{1} = (1, 1, \\ldots, 1)^T ,\\\\\n\\sum_{i = 1}^n (y_i - \\bar{y})^2 &= \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i = 1}^n (y_i - \\hat{y}_i)^2 ,\\\\\n\\sum_{i = 1}^n (y_i - \\bar{y})^2 &= \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i = 1}^n \\hat{\\varepsilon}_i^2 .\\\\\n\\end{align}\n</math>\n\n===Proof===\n\n:<math>\n\\begin{align}\n\\sum_{i = 1}^n (y_i - \\overline{y})^2 &= \\sum_{i = 1}^n (y_i - \\overline{y} + \\hat{y}_i - \\hat{y}_i)^2\n= \\sum_{i = 1}^n ((\\hat{y}_i - \\bar{y}) + \\underbrace{(y_i - \\hat{y}_i)}_{\\hat{\\varepsilon}_i})^2 \\\\\n&= \\sum_{i = 1}^n ((\\hat{y}_i - \\bar{y})^2 + 2 \\hat{\\varepsilon}_i (\\hat{y}_i - \\bar{y}) + \\hat{\\varepsilon}_i^2) \\\\\n&= \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i = 1}^n \\hat{\\varepsilon}_i^2 + 2 \\sum_{i = 1}^n \\hat{\\varepsilon}_i (\\hat{y}_i - \\bar{y}) \\\\\n&= \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i = 1}^n \\hat{\\varepsilon}_i^2 + 2 \\sum_{i = 1}^n \\hat{\\varepsilon}_i(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\cdots + \\hat{\\beta}_p x_{ip} - \\overline{y}) \\\\\n&= \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i = 1}^n \\hat{\\varepsilon}_i^2 + 2 (\\hat{\\beta}_0 - \\overline{y}) \\underbrace{\\sum_{i = 1}^n \\hat{\\varepsilon}_i}_0 + 2 \\hat{\\beta}_1 \\underbrace{\\sum_{i = 1}^n \\hat{\\varepsilon}_i x_{i1}}_0 + \\cdots + 2 \\hat{\\beta}_p \\underbrace{\\sum_{i = 1}^n \\hat{\\varepsilon}_i x_{ip}}_0 \\\\\n&= \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i = 1}^n \\hat{\\varepsilon}_i^2 = \\mathrm{ESS} + \\mathrm{RSS} \\\\\n\\end{align}\n</math>\n\nThe requirement that the model includes a constant or equivalently that the design matrix contains a column of ones ensures that <math> \\sum_{i = 1}^n \\hat{\\varepsilon}_i = 0 </math>.\n\nThe proof can also be expressed in vector form, as follows:\n\n:<math>\n\\begin{align}\n  SS_\\text{total}   = \\Vert \\mathbf{y} - \\bar \\mathbf{y} \\Vert^2 & = \\Vert \\mathbf{y} - \\bar \\mathbf{y} + \\mathbf{\\hat y} - \\mathbf{\\hat y} \\Vert^2 , \\\\\n     &  = \\Vert \\left( \\mathbf{\\hat y} - \\bar \\mathbf{y} \\right) + \\left( \\mathbf{y} - \\mathbf{\\hat y} \\right) \\Vert^2, \\\\\n     &  = \\Vert {\\mathbf{\\hat y} - \\bar \\mathbf{y}} \\Vert^2  + \\Vert \\hat \\varepsilon\\Vert^2  + 2 \\hat \\varepsilon^T \\left( \\mathbf{\\hat y} - \\bar \\mathbf{y} \\right), \\\\\n     &  = SS_\\text{regression}  + SS_\\text{error}  + 2\\hat \\varepsilon^T \\left( X\\hat\\beta - \\bar \\mathbf{y} \\right) ,\\\\\n     &  = SS_\\text{regression}  + SS_\\text{error} + 2\\left( \\hat \\varepsilon^T X \\right)\\hat\\beta  - 2\\hat\\varepsilon^T \\bar{\\mathbf{y}}, \\\\ \n     &  = SS_\\text{regression} + SS_\\text{error}. \n\\end{align}\n</math>\n\nThe elimination of terms in the last line, used the fact that \n\n: <math>\n\\hat \\varepsilon ^T X = \\left( \\mathbf{y} - \\mathbf{\\hat y} \\right)^T X \n    = \\mathbf{y}^T(I - X(X^T X)^{-1} X^T)X = {\\mathbf{y}}^T(X-X)={\\mathbf{0}}.\n</math>\n\n===Further partitioning===\n\nNote that the residual sum of squares can be further partitioned as the [[lack-of-fit sum of squares]] plus the sum of squares due to pure error.\n\n==See also==\n* [[Inner-product space]]\n** [[Hilbert space]]\n*** [[Euclidean space]]\n** [[Orthogonality]]\n** [[Orthonormal basis]]\n***[[Orthogonal complement]], the closed subspace orthogonal to a set (especially a subspace)\n***[[Orthomodular lattice]] of the subspaces of an inner-product space\n***[[Orthogonal projection]]\n** [[Pythagorean theorem]] that the sum of the squared norms of orthogonal summands equals the squared norm of the sum. \n* [[Least squares]]\n* [[Mean squared error]]\n* [[Squared deviations]]\n\n==References==\n\n* {{cite book |last=Bailey|first=R. A.|authorlink=Rosemary A. Bailey|title=Design of Comparative Experiments|publisher=Cambridge University Press|year=2008 |isbn=978-0-521-68357-9|url=http://www.maths.qmul.ac.uk/~rab/DOEbook}} Pre-publication chapters are available on-line.\n* {{cite book\n|title=Plane Answers to Complex Questions: The Theory of Linear Models|last=Christensen|first=Ronald|location=New York|publisher=Springer|year=2002| edition=Third|isbn=0-387-95361-2}}\n* {{cite book|title=Prediction and Regulation|last=Whittle|first=Peter|authorlink=Peter Whittle (mathematician)|publisher=English Universities Press|year=1963|isbn=0-8166-1147-5}}\n*:Republished as: {{cite book|title=Prediction and Regulation by Linear Least-Square Methods|author=Whittle, P.|publisher=University of Minnesota Press|year=1983|isbn=0-8166-1148-3}}\n* {{cite book|title=Probability Via Expectation|edition=4th|author=Whittle, P.|publisher=Springer|date=20 April 2000|isbn=0-387-98955-2}}\n\n[[Category:Analysis of variance]]\n[[Category:Least squares]]"
    },
    {
      "title": "Polynomial least squares",
      "url": "https://en.wikipedia.org/wiki/Polynomial_least_squares",
      "text": "{{multiple issues|\n {{merge to|Polynomial regression|date=April 2018|discuss=Talk:Polynomial regression#Merge}}\n {{lead extra info|date=February 2019}}\n}}\nIn [[mathematical statistics]], '''polynomial least squares''' comprises a broad range of statistical methods for estimating an underlying polynomial that describes observations. These methods include [[polynomial regression]], [[curve fitting]], [[linear regression]], [[least squares]], [[ordinary least squares]], [[simple linear regression]], [[linear least squares (mathematics)|linear least squares]], [[approximation theory]] and [[method of moments (statistics)|method of moments]]. Polynomial least squares has applications in [[radar tracker]]s, [[estimation theory]], [[signal processing]], [[statistics]], and [[econometrics]].\n\nTwo common applications of polynomial least squares methods are generating a low-degree polynomial that approximates a complicated function and estimating an assumed underlying polynomial from corrupted (also known as \"noisy\") observations. The former is commonly used in statistics and [[econometrics]] to fit a [[scatter plot]] with a first degree polynomial (that is, a linear expression).<ref name=\"Gujarati\">{{cite book | title=Basic Econometrics | last1=Gujarati | first1=Damodar N. | last2=Porter | first2=Dawn C. | url=http://egei.vse.cz/english/wp-content/uploads/2012/08/Basic-Econometrics.pdf | edition=5 | publisher=McGraw-Hill Education | isbn=978-0073375779 | year=2008}}</ref><ref name=\"Hansen\">{{cite book | title=Econometrics | last=Hansen | first=Bruce E. | date=January 16, 2015 | url=http://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics.pdf}}</ref><ref name=\"Copeland\">{{cite book | title=Financial Theory and Corporate Policy | last1=Copeland | first1=Thomas E. | last2=Weston | first2=John Fred | last3=Shastri | first3=Kuldeep | publisher=Prentice Hall | date=January 10, 2004 | edition=4 | isbn=978-0321127211}}</ref> The latter is commonly used in target tracking in the form of [[Kalman filtering]], which is effectively a recursive implementation of polynomial least squares.<ref name=\"Kalman\">{{cite journal | doi=10.1115/1.3662552 | title=A New Approach to Linear Filtering and Prediction Problems | journal=Journal of Basic Engineering | volume=82 | page=35 | date=March 1, 1960 | last=Kálmán | first=Rudolf E. | authorlink=Rudolf Emil Kálmán}}</ref><ref name= Sorenson>Sorenson, H. W., Least-squares estimation: Gauss to Kalman, IEEE Spectrum, July, 1970.</ref><ref name=Bell1>Bell, J. W., Simple Disambiguation Of Orthogonal Projection In Kalman’s Filter Derivation, Proceedings of the International Conference on Radar Systems, Glasgow, UK. October, 2012.</ref><ref name=Bell2>Bell, J. W., A Simple Kalman Filter Alternative: The Multi-Fractional Order Estimator, IET-RSN, Vol. 7, Issue 8, October 2013.</ref> Estimating an assumed underlying deterministic polynomial can be used in econometrics as well.<ref name=\"web reference 2\" >{{cite web|url=http://ssrn.com/abstract=2573840 |title=Ordinary Least Squares Revolutionized: Establishing the Vital Missing Empirically Determined Statistical Prediction Variance by Jeff Bell |doi=10.2139/ssrn.2573840 |publisher=SSRN |date= |accessdate=2019-02-27}}</ref> In effect, both applications produce average curves as generalizations of the common [[average]] of a set of numbers, which is equivalent to zero degree [[polynomial]] least squares.<ref name=\"Gujarati\"/><ref name=\"Hansen\"/><ref name=Papoulis>Papoulis, A., Probability, RVs, and Stochastic Processes, McGraw-Hill, New York, 1965</ref>\n\nIn the above applications, the term \"approximate\" is used when no statistical measurement or observation errors are assumed, as when fitting a scatter plot. The term \"estimate\", derived from statistical estimation theory, is used when assuming that measurements or observations of a polynomial are corrupted.\n\n==Polynomial least squares estimate of a deterministic first degree polynomial corrupted with observation errors==\nAssume the deterministic first degree polynomial equation ''<math>y</math>'' with unknown coefficients '''<math>\\alpha</math>''' and  '''<math>\\beta</math>''' is written as\n\n:<math>y=\\alpha+\\beta t.</math>\n\t\t\t\t\t\nThis is corrupted with an additive [[stochastic process]] <math> \\varepsilon</math> described as an error (noise in tracking), resulting in\n\n:<math>z=y+\\varepsilon=\\alpha+\\beta t+\\varepsilon.</math>\n\nGiven observations <math>z_n</math> from a [[Sample (statistics)|sample]],  where the subscript ''<math>n</math>'' is the observation index, the problem is to apply '''polynomial least squares''' to estimate  ''<math>y(t)</math>'', and to determine its [[variance]] along with its [[expected value]].\n\n===Definitions and assumptions===\n\n(1) The term [[linearity]] in mathematics may be considered to take two forms that are sometimes confusing: a linear ''system'' or transformation (sometimes called an operator)<ref name=\"Papoulis\"/> and a linear ''equation''. The term \"function\" is often used to describe both a system and an equation, which may lead to confusion. A linear ''system'' is defined by\n\n:<math>f(ax +by)= af(x) +bf(y)</math>\n\nwhere <math>a</math> and <math>b</math> are constants, and where <math>x</math> and <math>y</math> are variables. In a linear ''system'' <math>E[f(x)]=f(E[x])</math>, where <math>E</math> is the linear expectation operator. A linear ''equation'' is a straight line as is the first degree polynomial described above.\n\n(2) The error <math>\\varepsilon  </math>  is modeled as a zero [[mean]] stochastic process, sample points of which are [[random variables]] that are uncorrelated and assumed to have identical [[probability distributions]] (specifically same mean and variance), but not necessarily [[Gaussian]], treated as inputs to polynomial least squares. Stochastic processes and  random variables are described only by probability distributions.<ref name=\"Gujarati\"/><ref name=\"Papoulis\"/><ref name=\" Hansen\"/>\n \n(3) Polynomial least squares is modeled as a linear signal processing ''system'' which processes statistical inputs deterministically, the output being the linearly processed empirically determined statistical estimate, variance, and expected value.<ref name=\"Bell1\"/><ref name=\"Bell2\"/><ref name=\"web reference 2\"/>\n\n(4) Polynomial least squares processing produces deterministic [[Method of moments (statistics)|moments]] (analogous to mechanical moments), which may be considered as moments of sample statistics, but not of statistical moments.<ref name=\"web reference 2\"/>\n\n===Polynomial least squares and the orthogonality principle===\nApproximating a function ''<math>z(t)</math>'' with a polynomial\n\n: <math>\\hat z(t)=\\sum_{j=1} ^J a_j t^{j-1} </math>\n\nwhere hat (^) denotes the estimate and (''J''&nbsp;−&nbsp;1) is the polynomial degree, can be performed by applying the [[orthogonality principle]]. The [[sum of squared residuals]] can be written as\n\n: <math> \\sum_{n=1}^N (z_n - \\hat z_n)^2.</math>\n\nAccording to the orthogonality principle,<ref name=\"Kalman\"/><ref name=\"Sorenson\"/><ref name=\"Bell1\"/><ref name=\"Bell2\"/><ref name=\"web reference 2\"/><ref name=\"Papoulis\"/><ref name= Wylie>Wylie, C. R., Jr., Advanced Engineering Mathematics, McGraw-Hill, New York, 1960.</ref><ref name= Schied >Schied, F., Numerical Analysis, Schaum's Outline Series, McGraw-Hill, New York, 1968.</ref> this is at its minimum when the residual vector (<math>z-\\hat z</math>) is orthogonal to the estimate <math>\\hat z</math>, that is\n\n: <math>\\sum_{n=1}^N (z_n - \\hat z_n)\\hat z_n=0.</math>\n\nThis can be described as the orthogonal projection of the data values {<math> z_n</math>}  onto a solution in the form of the polynomial <math>\\hat z(t)</math>.<ref name=\"Kalman\"/><ref name=\"Bell1\"/><ref name=\"Bell2\"/> For  ''N'' > ''J'', orthogonal projection yields the standard overdetermined system of equations (often called [[normal equations]]) used to compute the coefficients in the polynomial approximation.<ref name=\"Gujarati\"/><ref name=\"Wylie\"/><ref name=\"Schied \"/> The minimum sum of squared residuals is then\n\n: <math>SSR_\\min = \\sum_{n=1} ^N (z_n - \\hat z_n)z_n </math>\n\nThe advantage of using orthogonal projection is that <math>SSR_\\min</math> can be determined for use in the polynomial least squares processed statistical variance of the estimate.<ref name=\"web reference 2\"/><ref name=\"Papoulis\"/><ref name=\"Schied \"/>\n\n==The empirically determined polynomial least squares output of a first degree polynomial corrupted with observation errors==\nTo fully determine the output of '''polynomial least squares''', a weighting function describing the processing must first be structured and then the statistical moments can be computed.\n\n===The weighting function describing the linear polynomial least squares \"system\"===\nThe weighting function <math> w_n (\\tau) </math> can be formulated from polynomial least squares to estimate the unknown ''<math>y(t)</math>''  as follows:<ref name=\"web reference 2\"/>\n\n: <math>\\hat y (\\tau) = \\frac {1} {N}\\sum_{n=1} ^N z_n w_n (\\tau) =  \\frac {1} {N}\\sum_{n=1} ^N (\\alpha+\\beta t_n + \\varepsilon_n) w_n (\\tau) </math>\n\nwhere ''N'' is the number of samples, <math> z_n</math>  are random variables as samples of the stochastic <math>z</math>  (noisy signal), and the first degree polynomial data weights are\n\n: <math>w_n(\\tau)\\equiv\\frac{[\\bar{t^2}-\\bar{t}t_n+(t_n-\\bar{t})\\tau]}{(\\bar{t^2}- \\bar{t}^2)}</math>\n\nwhich represent the linear polynomial least squares \"system\" and describe its processing.<ref name=\"web reference 2\"/> The Greek letter '''<math>\\tau</math>''' is the independent variable ''<math>t</math>''  when estimating the dependent variable ''<math>y(t)</math>'' after data fitting has been performed. (The letter '''<math>\\tau</math>''' is used to avoid confusion with ''<math>t</math>''  before and sampling during polynomial least squares processing.) The overbar ( ¯ ) defines the deterministic centroid of <math>u_n</math>  as processed by polynomial least squares <ref name=\"web reference 2\"/> – i.e., it defines the deterministic first order moment, which may be considered a sample average, but does not here approximate a first order statistical moment:\n\n: <math>\\bar{u}\\overset{\\underset{\\mathrm{def}}{}}{=}\\frac {1} {N}\\sum_{n=1} ^N u_n </math>\n\n===Empirically determined statistical moments===\nApplying <math> w_n (\\tau) </math> yields\n\n: <math>\\hat y(\\tau)=\\hat\\alpha+\\hat\\beta \\tau</math>\n\nwhere\n\n: <math>\\hat\\alpha=\\frac{(\\bar{z}\\bar{t^2}-\\bar{zt}\\bar{t})}{(\\bar{t^2}-\\bar{t}^2)}=\\alpha+\\frac{(\\bar{\\varepsilon}\\bar{t^2}-\\bar{{\\varepsilon}t}\\bar{t})}{(\\bar{t^2}-\\bar{t}^2)}</math>\n\nand\n\n: <math>\\hat\\beta=\\frac{(\\bar{zt}-\\bar{z}\\bar{t})}{(\\bar{t^2}-\\bar{t}^2)}=\\beta+\\frac{(\\bar{\\varepsilon t}-\\bar{\\varepsilon}\\bar{t})}{(\\bar{t^2}-\\bar{t}^2)}</math>\n\nAs linear functions of the random variables <math>\\varepsilon_n </math>, both coefficient estimates <math>\\hat\\alpha</math> and <math>\\hat\\beta</math> are random variables.<ref name=\"web reference 2\"/> In the absence of the errors <math>\\varepsilon_n</math>, <math>\\hat\\alpha=\\alpha</math> and <math>\\hat\\beta=\\beta</math>, as they should to meet that boundary condition.\n\nBecause the statistical expectation operator E[•] is a linear function and the sampled stochastic process errors <math>\\varepsilon_n</math>  are zero mean, the expected value of the estimate <math>\\hat y</math> is the first order statistical moment as follows:<ref name=\"Gujarati\"/><ref name=\"Hansen\"/><ref name=\"Copeland\"/><ref name=\"web reference 2\"/>\n             \n: <math>E[\\hat y (\\tau)] =\\alpha+\\beta\\tau+ \\frac {1} {N}\\sum_{n=1} ^N E[\\varepsilon_n] w_n (\\tau)= \\alpha+\\beta\\tau =\\alpha+\\beta t </math>\n\nThe statistical variance in <math>\\hat y</math> is given by the second order statistical central moment as follows:<ref name=\"Gujarati\"/><ref name=\"Hansen\"/><ref name=\"Copeland\"/><ref name=\"web reference 2\"/>              \n  \t\n: <math>\\sigma_\\hat y ^2 = E[(\\hat y)-E[\\hat y])^2 ]= \\frac {1} {N}\\frac {1} {N}\\sum_{n=1} ^N \\sum_{i=1} ^N w_n (\\tau) E[\\varepsilon_n \\varepsilon_i] w_i (\\tau)</math>\n<math>=\\sigma_\\varepsilon ^2 \\frac {1} {N}\\frac {1} {N}\\sum_{n=1} ^N \\sum_{i=1} ^N w_n ^2 (\\tau)</math>\n\nbecause \n\t\n: <math> \\sum_{i=1} ^N E[\\varepsilon_n \\varepsilon_i] w_i (\\tau)=\\sigma_\\varepsilon^2 w_n (\\tau) </math>\n\nwhere <math>\\sigma_\\varepsilon^2  </math> is the statistical variance of random variables <math> \\varepsilon_n </math>; i.e.,  <math> E[\\varepsilon_n \\varepsilon_i]= \\sigma_\\varepsilon ^2  </math>  for  ''i'' = ''n''  and (because <math> \\varepsilon_n </math>  are uncorrelated) <math> \\sigma_\\varepsilon ^2=0</math>  for <math>i \\ne n </math> <ref name=\"web reference 2\"/>\n\nCarrying out the multiplications and summations in <math>\\sigma_\\hat y^2</math> yields<ref name=\"web reference 2\"/>\n\n: <math>\\sigma_\\hat y^2=\\sigma_\\varepsilon^2\\frac{(\\bar{t^2}-2\\bar{t}\\tau+\\tau^2)}{N(\\bar{t^2}- \\bar{t}^2)}.</math>\n\n===Measuring or approximating the statistical variance of the random errors===\n\nIn a hardware system, such as a tracking radar, the measurement noise variance  <math>\\sigma_\\varepsilon^2</math> can be determined from measurements when there is no target return – i.e., by just taking measurements of the noise alone.\n\nHowever, if polynomial least squares is used when the variance <math>\\sigma_\\varepsilon^2</math> is not measurable (such as in econometrics or statistics), it can be estimated with observations in <math>e_\\min</math> from orthogonal projection as follows:\n\n:<math>\\sigma_\\varepsilon^2\\approx\\hat {\\sigma_\\varepsilon^2}= (\\bar {z^2}-\\hat\\alpha\\bar{z} - \\hat \\beta \\bar{zt})</math> <ref name=\"web reference 2\"/>\n\t\t\t\t\nAs a result, to the first order approximation from the estimates <math>\\hat\\alpha </math> and <math> \\hat\\beta</math> as functions of sampled <math>z </math> and <math> t</math>\n\t\n: <math>\\sigma_\\hat y^2 \\approx \\bigg[\\frac{(\\bar{z^2}-\\bar{z}^2)}{(\\bar{t^2}-\\bar{t}^2)}- \\Biggl(\\frac{(\\bar{zt}-\\bar{z}\\bar{t})}{(\\bar{t^2}-\\bar{t})}\\Biggl)^2 \\bigg]{\\frac{(\\bar{t^2}-2\\bar{t}\\tau+\\tau^2)}N}</math>\n\nwhich goes to zero in the absence of the errors <math>\\varepsilon_n </math>, as it should to meet that boundary condition.<ref name=\"web reference 2\"/>\n\nAs a result, the samples <math> z_n</math> (noisy signal) are considered to be the input to the linear polynomial least squares \"system\" which transforms the samples into the empirically determined statistical estimate  <math> \\hat y (\\tau)</math>, the expected value  <math>E[\\hat y] </math>, and the variance <math>\\sigma_\\hat y^2 </math>.<ref name=\"web reference 2\"/>\n\n==Properties of polynomial least squares modeled as a linear \"system\"==\n\n(1) The empirical statistical variance <math>\\sigma_\\hat y^2 </math> is a function of <math>\\sigma_\\varepsilon ^2 </math>, ''N'' and  <math>\\tau</math>. Setting the derivative of  <math>\\sigma_\\hat y^2 </math>  with respect to  <math>\\tau</math> equal to zero shows the minimum to occur at  <math>\\tau=\\bar t</math>; i.e., at the centroid (sample average) of the samples <math>t_n </math>. The minimum statistical variance thus becomes  <math>\\frac{\\sigma_\\varepsilon ^2 } {N} </math>. This is equivalent to the statistical variance from polynomial least squares of a zero degree polynomial – i.e., of the centroid (sample average) of <math>\\alpha </math>.<ref name=\"Gujarati\"/><ref name=\"Hansen\"/><ref name=\"web reference 2\"/>\n<ref name=\"Papoulis\"/>\n \n(2) The empirical statistical variance  <math>\\sigma_\\hat y^2 </math> is a function of the quadratic <math>\\tau^2</math> . Moreover, the further  <math>\\tau</math> deviates from  <math>\\bar t</math> (even within the data window), the larger is the variance <math>\\sigma_\\hat y^2 </math>   due to the random variable errors <math>\\varepsilon_n </math> . The independent variable  <math>\\tau</math> can take any value on the  <math>t</math> axis. It is not limited to the data window. It can extend beyond the data window – and likely will at times depending on the application. If it is within the data window, estimation is described as interpolation. If it is outside the data window, estimation is described as extrapolation. It is both intuitive and well known that the further is extrapolation, the larger is the error.<ref name=\"web reference 2\"/>\n\n(3) The empirical statistical variance  <math>\\sigma_\\hat y^2 </math> due to the random variable errors <math>\\varepsilon_n </math>  is inversely proportional to  ''N''. As  ''N'' increases, the statistical variance decreases. This is well known and what filtering out the errors   <math> \\varepsilon_n</math> is all about.<ref name=\"Gujarati\"/><ref name=\"Hansen\"/><ref name=\"web reference 2\"/><ref name=\"web reference 3\">[[Ordinary least squares]]</ref> The underlying purpose of polynomial least squares is to filter out the errors to improve estimation accuracy by reducing the empirical statistical estimation variance. In reality, only two data points are required to estimate  <math>\\alpha </math> and <math>\\beta </math>; albeit the more data points with zero mean statistical errors included, the smaller is the empirical statistical estimation variance as established by ''N'' samples.\n \n(4) There is an additional issue to be considered when the noise variance is not measurable: Independent of the polynomial least squares estimation, any new observations would be described by the variance <math>\\sigma_\\varepsilon^2\\approx\\hat {\\sigma_\\varepsilon^2}= (\\bar {z^2}-\\hat\\alpha\\bar{z} - \\hat \\beta \\bar{zt})</math>.<ref name=\"web reference 2\"/><ref name=\"Papoulis\"/>\n \nThus, the polynomial least squares statistical estimation variance <math>\\sigma_\\hat y^2 </math> and the statistical variance of any new sample in  <math>\\sigma_\\varepsilon ^2 </math> would both contribute to the uncertainty of any future observation. Both variances are clearly determined by polynomial least squares in advance.\n\n(5) This concept also applies to higher degree polynomials. However, the weighting function  <math> w_n (\\tau) </math> is obviously more complicated. In addition, the estimation variances increase exponentially as polynomial degrees increase linearly (i.e., in unit steps). However, there are ways of dealing with this as described in.<ref name=\"Bell1\"/><ref name=\"Bell2\"/>\n\n==The synergy of integrating polynomial least squares with statistical estimation theory==\nModeling polynomial least squares as a linear signal processing \"system\" creates the synergy of integrating polynomial least squares with statistical estimation theory to deterministically process samples of an assumed polynomial corrupted with a statistically described stochastic error ε. In the absence of the error ε, statistical estimation theory is irrelevant and polynomial least squares reverts to the conventional approximation of complicated functions and scatter plots.\n\n== See also ==\n* [[Multi-fractional order estimator]]\n\n==References==\n{{reflist}}\n\n[[Category:Least squares]]"
    },
    {
      "title": "Proofs involving ordinary least squares",
      "url": "https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares",
      "text": "{{multiple issues|\n{{lead too short|date=July 2015}}\n{{Unreferenced|date=February 2010}}\n{{expert needed|1=Statistics|date=October 2017}}\n}}\nThe purpose of this page is to provide supplementary materials for the [[ordinary least squares]] article, reducing the load of the main article with mathematics and improving its accessibility, while at the same time retaining the completeness of exposition.\n\n==Derivation of the normal equations==\nDefine the <math>i</math>th '''residual''' to be\n\n:<math>r_i= y_i - \\sum_{j=1}^{n} X_{ij}\\beta_j.</math>\n\nThen <math>S</math> can be rewritten\n\n:<math>S = \\sum_{i=1}^m r_i^2.</math>\n\nGiven that ''S'' is convex, it is [[Maxima and minima|minimized]] when its gradient vector is zero (This follows by definition: if the gradient vector is not zero, there is a direction in which we can move to minimize it further – see [[maxima and minima]].) The elements of the gradient vector are the partial derivatives of ''S'' with respect to the parameters:\n\n:<math>\\frac{\\partial S}{\\partial \\beta_j}=2\\sum_{i = 1}^m r_i\\frac{\\partial r_i}{\\partial \\beta_j} \\qquad (j=1,2,\\dots, n).</math>\n\nThe derivatives are\n\n:<math>\\frac{\\partial r_i}{\\partial \\beta_j}=-X_{ij}.</math>\n\nSubstitution of the expressions for the residuals and the derivatives into the gradient equations gives\n\n:<math>\\frac{\\partial S}{\\partial \\beta_j} = 2\\sum_{i=1}^{m} \\left( y_i-\\sum_{k=1}^{n} X_{ik}\\beta_k \\right) (-X_{ij})\\qquad (j=1,2,\\dots, n).</math>\n\nThus if <math>\\widehat \\beta</math> minimizes ''S'', we have\n\n:<math>2\\sum_{i=1}^{m} \\left( y_i-\\sum_{k=1}^{n} X_{ik}\\widehat \\beta_k \\right) (-X_{ij}) = 0\\qquad (j=1,2,\\dots, n).</math>\n\nUpon rearrangement, we obtain the '''normal equations''':\n\n:<math>\\sum_{i=1}^{m}\\sum_{k=1}^{n} X_{ij}X_{ik}\\widehat \\beta_k=\\sum_{i=1}^{m} X_{ij}y_i\\qquad (j=1,2,\\dots, n).</math>\n\nThe normal equations are written in matrix notation as\n\n:<math>(\\mathbf X^\\mathrm{T} \\mathbf X) \\widehat{\\boldsymbol{\\beta}} = \\mathbf X^\\mathrm{T} \\mathbf y</math> (where ''X''<sup>T</sup> is the [[matrix transpose]] of ''X'').\n\nThe solution of the normal equations yields the vector <math>\\widehat{\\boldsymbol{\\beta}}</math> of the optimal parameter values.\n\n===Derivation directly in terms of matrices===\n\nThe normal equations can be derived directly from a matrix representation of the problem as follows. The objective is to minimize\n\n:<math>S(\\boldsymbol{\\beta}) \n= \\bigl\\|\\mathbf y - \\mathbf X \\boldsymbol \\beta \\bigr\\|^2 \n= (\\mathbf y-\\mathbf X \\boldsymbol \\beta)^{\\rm T}(\\mathbf y-\\mathbf X \\boldsymbol \\beta) \n= \\mathbf y ^{\\rm T} \\mathbf y - \\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf y - \\mathbf y ^{\\rm T} \\mathbf X \\boldsymbol \\beta + \\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf X \\boldsymbol \\beta .</math>\n\nHere <math>( \\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf y ) ^{\\rm T} = \\mathbf y ^{\\rm T} \\mathbf X \\boldsymbol \\beta</math> has the dimension 1x1 (the number of columns of <math>\\mathbf y</math>), so it is a scalar and equal to its own transpose, hence <math>\\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf y = \\mathbf y ^{\\rm T} \\mathbf X \\boldsymbol \\beta</math>\nand the quantity to minimize becomes\n\n:<math>S(\\boldsymbol{\\beta}) = \\mathbf y ^{\\rm T} \\mathbf y - 2\\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf y + \\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf X \\boldsymbol \\beta .</math>\n\n[[Matrix differentiation#Scalar-by-vector|Differentiating]] this with respect to <math>\\boldsymbol \\beta</math> and equating to zero to satisfy the first-order conditions gives\n\n:<math>- \\mathbf X^{\\rm T} \\mathbf y+ (\\mathbf X^{\\rm T} \\mathbf X ){\\boldsymbol{\\beta}} = 0,</math>\n\nwhich is equivalent to the above-given normal equations. A sufficient condition for satisfaction of the second-order conditions for a minimum is that <math>\\mathbf X</math> have full column rank, in which case <math>\\mathbf X^{\\rm T} \\mathbf X</math> is [[Positive definite matrix|positive definite]].\n\n===Derivation without calculus===\n\nWhen <math>\\mathbf X^{\\rm T} \\mathbf X</math> is positive definite, the formula for the minimizing value of <math> \\boldsymbol \\beta </math> can be derived without the use of derivatives. The quantity\n\n:<math>S(\\boldsymbol{\\beta}) = \\mathbf y ^{\\rm T} \\mathbf y - 2\\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf y + \\boldsymbol \\beta ^{\\rm T} \\mathbf X ^{\\rm T} \\mathbf X \\boldsymbol \\beta </math>\n\ncan be written as\n\n:<math> \\langle \\boldsymbol \\beta, \\boldsymbol \\beta \\rangle - 2\\langle \\boldsymbol \\beta, (\\mathbf X^{\\rm T} \\mathbf X)^{-1}\\mathbf X ^{\\rm T} \\mathbf y \\rangle + \\langle(\\mathbf X^{\\rm T} \\mathbf X)^{-1}\\mathbf X ^{\\rm T} \\mathbf y,(\\mathbf X^{\\rm T} \\mathbf X)^{-1}\\mathbf X ^{\\rm T} \\mathbf y \\rangle+ C, </math>\n\nwhere <math> C </math> depends only on <math> \\mathbf y </math> and <math> \\mathbf X </math>, and <math> \\langle \\cdot, \\cdot \\rangle </math> is the [[Dot product|inner product]] defined by\n\n:<math> \\langle x, y \\rangle = x ^{\\rm T} (\\mathbf X^{\\rm T} \\mathbf X) y. </math>\n\nIt follows that <math> S(\\boldsymbol{\\beta}) </math> is equal to\n\n:<math>\\langle \\boldsymbol \\beta - (\\mathbf X^{\\rm T} \\mathbf X)^{-1}\\mathbf X ^{\\rm T} \\mathbf y,\\boldsymbol \\beta - (\\mathbf X^{\\rm T} \\mathbf X)^{-1}\\mathbf X ^{\\rm T} \\mathbf y \\rangle+ C </math>\n\nand therefore minimized exactly when\n:<math>\\boldsymbol \\beta - (\\mathbf X^{\\rm T} \\mathbf X)^{-1}\\mathbf X ^{\\rm T} \\mathbf y = 0.</math>\n\n===Generalization for complex equations===\n\nIn general, the coefficients of the matrices <math> \\mathbf {X}, \\boldsymbol{\\beta} </math>  and <math>\\mathbf{y}</math> can be complex. By using a [[Hermitian transpose]] instead of a simple transpose, it is possible to find a vector <math>\\boldsymbol{\\widehat{\\beta}} </math> which minimizes <math>S(\\boldsymbol{\\beta})</math>, just as for the real matrix case. In order to get the normal equations we follow a similar path as in previous derivations:\n\n:<math> \\displaystyle S(\\boldsymbol{\\beta})=\\langle \\mathbf {y} -\\mathbf{X} \\boldsymbol{\\beta},\\mathbf {y} -\\mathbf {X} \\boldsymbol{\\beta} \\rangle = \\langle \\mathbf {y} ,\\mathbf {y} \\rangle - \\overline{\\langle \\mathbf{X} \\boldsymbol{\\beta},\\mathbf {y} \\rangle}-{\\overline {\\langle \\mathbf{y},\\mathbf{X} \\boldsymbol{\\beta}\\rangle}} + \\langle \\mathbf {X} \\boldsymbol{\\beta},\\mathbf {X} \\boldsymbol{\\beta} \\rangle =\\mathbf {y}^{\\rm T} \\overline{\\mathbf {y}}-\\boldsymbol{\\beta}^\\dagger \\mathbf{X}^\\dagger \\mathbf{y} -\\mathbf{y}^\\dagger \\mathbf {X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}^{\\rm T} \\mathbf {X} ^{\\rm {T}} \\overline{\\mathbf {X} } \\overline{\\boldsymbol{\\beta}}, </math>\nwhere <math> \\dagger</math> stands for Hermitian transpose.\n\nWe should now take derivatives of <math> S(\\boldsymbol{\\beta}) </math> with respect to each of the coefficients <math> \\beta_j </math>, but first we separate real and imaginary parts to deal with the conjugate factors in above expression. For the <math> \\beta_j</math> we have\n\n:<math> \\beta_j = \\beta_j^R + i\\beta_j^I </math>\n\nand the derivatives change into\n\n:<math> \\frac {\\partial S}{\\partial \\beta_j} = \\frac {\\partial S}{\\partial \\beta_j^R} \\frac {\\partial \\beta_j^R}{\\partial \\beta_j} + \\frac {\\partial S}{\\partial \\beta_j^I} \\frac {\\partial \\beta_j^I}{\\partial \\beta_j} = \\frac {\\partial S}{\\partial \\beta_j^R} - i \\frac {\\partial S}{\\partial \\beta_j^I} \\quad (j=1,2,3,\\ldots,n). </math>\n\nAfter rewriting <math> S(\\boldsymbol{\\beta}) </math> in the summation form and writing <math>\\beta_j</math> explicitly, we can calculate both partial derivatives with result:\n\n:<math>\n\\begin{align}\n\\frac {\\partial S}{\\partial \\beta_j^R} = {} & -\\sum_{i=1}^m \\Big(\\overline {X}_{ij} y_i + \\overline{y}_i X_{ij} \\Big) + 2\\sum_{i=1}^m X_{ij} \\overline{X}_{ij} \\beta_j^R + \\sum_{i=1}^m \\sum_{k\\neq j}^n \\Big( X_{ij} \\overline{X}_{ik} \\overline{\\beta}_k + \\beta_k X_{ik} \\overline{X}_{ij} \\Big), \\\\[8pt]\n& {} -i{\\frac {\\partial S}{\\partial \\beta_j^I}} = \\sum_{i=1}^m \\Big(\\overline{X}_{ij} y_i - \\overline {y}_i X_{ij}{\\Big )} - 2i\\sum_{i=1}^m X_{ij}\\overline{X}_{ij} \\beta_j^I + \\sum_{i=1}^m \\sum_{k\\neq j}^n \\Big( X_{ij} \\overline{X}_{ik} \\overline{\\beta}_k - \\beta_k X_{ik} \\overline{X}_{ij} \\Big),\n\\end{align}\n</math>\n\nwhich, after adding it together and comparing to zero (minimization condition for <math>\\boldsymbol{\\widehat{\\beta}} </math>) yields\n\n:<math> \\sum_{i=1}^m X_{ij} \\overline{y}_i = \\sum_{i=1}^m \\sum_{k=1}^n X_{ij} \\overline{X}_{ik} \\overline{\\widehat{\\beta}}_k \\qquad (j=1,2,3,\\ldots,n). </math>\n\nIn matrix form:\n\n:<math> \\textbf{X}^{\\rm {T}} \\overline{\\textbf{y}} = \\textbf{X}^{\\rm T} \\overline{\\big( \\textbf{X} \\boldsymbol{\\widehat{\\beta}} \\big)} \\quad \\text{ or }\\quad \\big (\\textbf{X}^\\dagger \\textbf{X} \\big) \\boldsymbol{\\widehat{\\beta}} = \\textbf{X}^\\dagger \\textbf{y}. </math>\n\n== Least squares estimator for ''β'' ==\n\nUsing matrix notation, the sum of squared residuals is given by\n\n: <math>S(\\beta) = (y-X\\beta)^T(y-X\\beta). </math>\n\nSince this is a quadratic expression, the vector which gives the global minimum may be found via [[Matrix calculus#Derivatives with vectors|matrix calculus]] by differentiating with respect to the vector&nbsp;<math>\\beta</math> (using denominator layout) and setting equal to zero:\n\n: <math> 0 = \\frac{dS}{d\\beta}(\\widehat\\beta) = \\frac{d}{d\\beta}\\bigg(y^Ty - \\beta^TX^Ty - y^TX\\beta + \\beta^TX^TX\\beta\\bigg)\\bigg|_{\\beta=\\widehat\\beta} = -2X^Ty + 2X^TX\\widehat\\beta</math>\n\nBy assumption matrix ''X'' has full column rank, and therefore ''X<sup>T</sup>X'' is invertible and the least squares estimator for ''β'' is given by\n\n: <math> \\widehat\\beta = (X^TX)^{-1}X^Ty </math>\n\n== Unbiasedness and variance of <math>\\widehat\\beta</math> ==\nPlug ''y''&nbsp;=&nbsp;''Xβ''&nbsp;+&nbsp;''ε'' into the formula for <math>\\widehat\\beta</math> and then use the [[law of total expectation]]:\n\n: <math>\n\\begin{align}\\operatorname{E}[\\,\\widehat\\beta] &= \\operatorname{E}\\Big[(X^TX)^{-1}X^T(X\\beta+\\varepsilon)\\Big] \\\\\n&= \\beta + \\operatorname{E}\\Big[(X^TX)^{-1}X^T\\varepsilon\\Big] \\\\\n&= \\beta + \\operatorname{E}\\Big[\\operatorname{E}\\Big[(X^TX)^{-1}X^T\\varepsilon \\mid X \\Big]\\Big] \\\\\n&= \\beta + \\operatorname{E}\\Big[(X^TX)^{-1}X^T\\operatorname{E}[\\varepsilon\\mid X]\\Big]\n&= \\beta,\n\\end{align}\n</math>\n\nwhere E[''ε''|''X'']&nbsp;=&nbsp;0 by assumptions of the model.\n\nFor the variance, let the covariance matrix of <math>\\varepsilon</math> be <math>\\operatorname{E}[\\,\\varepsilon\\varepsilon^T\\,] = \\sigma^2 I</math>\n(where <math>I</math> is the identity <math>m\\,\\times\\,m</math> matrix).\nThen,\n\n: <math>\\begin{align}\n\\operatorname{E}[\\,(\\widehat\\beta - \\beta)(\\widehat\\beta - \\beta)^T] &= \\operatorname{E}\\Big[ ((X^TX)^{-1}X^T\\varepsilon)((X^TX)^{-1}X^T\\varepsilon)^T \\Big] \\\\\n&= \\operatorname{E}\\Big[ (X^TX)^{-1}X^T\\varepsilon\\varepsilon^TX(X^TX)^{-1} \\Big] \\\\\n&= \\operatorname{E}\\Big[ (X^TX)^{-1}X^T\\sigma^2X(X^TX)^{-1} \\Big] \\\\\n&= \\operatorname{E}\\Big[ \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1} \\Big] \\\\\n&= \\sigma^2 (X^TX)^{-1},\n\\end{align}</math>\n\nwhere we used the fact that <math>\\widehat{\\beta} - \\beta </math> is just an affine transformation of <math>\\varepsilon</math> by the matrix <math>(X^TX)^{-1}X^T</math> ( see article on the [[multivariate normal distribution]] under the affine transformation section).\n\nFor a simple linear regression model, where <math>\\beta = [\\beta_0,\\beta_1]^T</math> (<math>\\beta_0</math> is the ''y''-intercept and <math>\\beta_1</math> is the slope), one obtains\n\n: <math>\\begin{align}\n \\sigma^2 (X^TX)^{-1} &=\n \\sigma^2 \\left( \\begin{pmatrix} 1&1& \\cdots \\\\x_1&x_2& \\cdots \\end{pmatrix}\\begin{pmatrix} 1& x_1\\\\1& x_2\\\\ \\vdots & \\vdots\\,\\,\\, \\end{pmatrix} \\right)^{-1}\\\\[6pt]\n&=  \\sigma^2 \\left(\\sum_{i=1}^m \\begin{pmatrix} 1& x_i\\\\x_i& x_i^2\\end{pmatrix} \\right)^{-1}\\\\[6pt]\n&=  \\sigma^2 \\begin{pmatrix} m& \\sum x_i\\\\\\sum x_i& \\sum x_i^2\\end{pmatrix}^{-1}\\\\[6pt]\n&=  \\sigma^2 \\cdot \\frac{1}{m\\sum x_i^2-(\\sum x_i)^2}\\begin{pmatrix} \\sum x_i^2& -\\sum x_i\\\\-\\sum x_i& m\\end{pmatrix}\\\\[6pt]\n&=  \\sigma^2 \\cdot \\frac{1}{m\\sum{(x_i - \\bar{x})^2}}\\begin{pmatrix} \\sum x_i^2& -\\sum x_i\\\\-\\sum x_i& m\\end{pmatrix} \\\\[8pt]\n \\operatorname{Var}(\\beta_1) &= \\frac{\\sigma^2}{\\sum_{i=1}^m (x_i - \\bar{x})^2}.\n\\end{align}\n</math>\n\n== Expected value of <math>\\widehat\\sigma^{\\,2}</math> ==\nFirst we will plug in the expression for ''y'' into the estimator, and use the fact that ''X'M''&nbsp;=&nbsp;''MX''&nbsp;=&nbsp;0 (matrix ''M'' projects onto the space orthogonal to ''X''):\n\n: <math> \\widehat\\sigma^{\\,2} = \\tfrac{1}{n}y'My = \\tfrac{1}{n} (X\\beta+\\varepsilon)'M(X\\beta+\\varepsilon) = \\tfrac{1}{n} \\varepsilon'M\\varepsilon </math>\n\nNow we can recognize ''ε''&prime;''Mε'' as a 1×1 matrix, such matrix is equal to its own [[trace (linear algebra)|trace]]. This is useful because by properties of trace operator, '''tr'''(''AB'')&nbsp;=&nbsp;'''tr'''(''BA''), and we can use this to separate disturbance ''ε'' from matrix ''M'' which is a function of regressors ''X'':\n\n: <math> \\operatorname{E}\\,\\widehat\\sigma^{\\,2}\n         = \\tfrac{1}{n}\\operatorname{E}\\big[\\operatorname{tr}(\\varepsilon'M\\varepsilon)\\big] \n         = \\tfrac{1}{n}\\operatorname{tr}\\big(\\operatorname{E}[M\\varepsilon\\varepsilon']\\big)</math>\n\nUsing the [[Law of iterated expectation]] this can be written as\n\n: <math>\\operatorname{E}\\,\\widehat\\sigma^{\\,2}\n         = \\tfrac{1}{n}\\operatorname{tr}\\Big(\\operatorname{E}\\big[M\\,\\operatorname{E}[\\varepsilon\\varepsilon'|X]\\big]\\Big)\n         = \\tfrac{1}{n}\\operatorname{tr}\\big(\\operatorname{E}[\\sigma^2MI]\\big)\n         = \\tfrac{1}{n}\\sigma^2\\operatorname{E}\\big[ \\operatorname{tr}\\,M \\big] </math>\n\nRecall that ''M''&nbsp;=&nbsp;''I''&nbsp;&minus;&nbsp;''P'' where ''P'' is the projection onto linear space spanned by columns of matrix ''X''. By properties of a [[projection matrix]], it has ''p''&nbsp;=&nbsp;rank(''X'') eigenvalues equal to 1, and all other eigenvalues are equal to 0. Trace of a matrix is equal to the sum of its characteristic values, thus tr(''P'')&nbsp;=&nbsp;''p'', and tr(''M'')&nbsp;=&nbsp;''n''&nbsp;&minus;&nbsp;''p''. Therefore,\n\n: <math>\\operatorname{E}\\,\\widehat\\sigma^{\\,2} = \\frac{n-p}{n} \\sigma^2</math>\n\nNote: in the later section [[#Maximum_likelihood_approach|“Maximum likelihood”]] we show that under the additional assumption that errors are distributed normally, the estimator <math>\\widehat\\sigma^{\\,2}</math> is proportional to a chi-squared distribution with ''n''&nbsp;–&nbsp;''p'' degrees of freedom, from which the formula for expected value would immediately follow. However the result we have shown in this section is valid regardless of the distribution of the errors, and thus has importance on its own.\n\n== Consistency and asymptotic normality of <math>\\widehat\\beta</math> ==\nEstimator <math>\\widehat\\beta</math> can be written as\n: <math>\\widehat\\beta = \\big(\\tfrac{1}{n}X'X\\big)^{-1}\\tfrac{1}{n}X'y \n                  = \\beta + \\big(\\tfrac{1}{n}X'X\\big)^{-1}\\tfrac{1}{n} X'\\varepsilon \n                  = \\beta\\; + \\;\\bigg(\\frac{1}{n}\\sum_{i=1}^n x_ix'_i\\bigg)^{\\!\\!-1} \\bigg(\\frac{1}{n}\\sum_{i=1}^n x_i\\varepsilon_i\\bigg)</math>\nWe can use the [[law of large numbers]] to establish that \n: <math>\\frac{1}{n}\\sum_{i=1}^n x_ix'_i\\ \\xrightarrow{p}\\ \\operatorname{E}[x_ix_i']=\\frac{Q_{xx}}{n}, \\qquad \n        \\frac{1}{n}\\sum_{i=1}^n x_i\\varepsilon_i\\ \\xrightarrow{p}\\ \\operatorname{E}[x_i\\varepsilon_i]=0</math>\nBy [[Slutsky's theorem]] and [[continuous mapping theorem]] these results can be combined to establish consistency of estimator <math>\\widehat\\beta</math>:\n: <math>\\widehat\\beta\\ \\xrightarrow{p}\\ \\beta + nQ_{xx}^{-1}\\cdot 0 = \\beta</math>\n\nThe [[central limit theorem]] tells us that\n: <math>\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n x_i\\varepsilon_i\\ \\xrightarrow{d}\\ \\mathcal{N}\\big(0,\\,V\\big),</math> where  <math>V = \\operatorname{Var}[x_i\\varepsilon_i] = \\operatorname{E}[\\,\\varepsilon_i^2x_ix'_i\\,] = \\operatorname{E}\\big[\\,\\operatorname{E}[\\varepsilon_i^2\\mid x_i]\\;x_ix'_i\\,\\big] = \\sigma^2 \\frac{Q_{xx}}{n}</math>\n\nApplying [[Slutsky's theorem]] again we'll have\n: <math>\\sqrt{n}(\\widehat\\beta-\\beta) = \\bigg(\\frac{1}{n}\\sum_{i=1}^n x_ix'_i\\bigg)^{\\!\\!-1} \\bigg(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n x_i\\varepsilon_i\\bigg)\\ \\xrightarrow{d}\\ Q_{xx}^{-1}n\\cdot\\mathcal{N}\\big(0, \\sigma^2\\frac{Q_{xx}}{n}\\big) = \\mathcal{N}\\big(0,\\sigma^2Q_{xx}^{-1}n\\big)</math>\n\n== Maximum likelihood approach ==\n[[Maximum likelihood estimation]] is a generic technique for estimating the unknown parameters in a statistical model by constructing a log-likelihood function corresponding to the joint distribution of the data, then maximizing this function over all possible parameter values. In order to apply this method, we have to make an assumption about the distribution of y given X so that the log-likelihood function can be constructed.  The connection of maximum likelihood estimation to OLS arises when this distribution is modeled as a [[Multivariate normal distribution|multivariate normal]].\n\nSpecifically, assume that the errors ε have multivariate normal distribution with mean 0 and variance matrix ''σ''<sup>2</sup>''I''. Then the distribution of ''y'' conditionally on ''X'' is\n: <math>y\\mid X\\ \\sim\\ \\mathcal{N}(X\\beta,\\, \\sigma^2I)</math>\nand the log-likelihood function of the data will be\n: <math>\\begin{align}\n  \\mathcal{L}(\\beta,\\sigma^2\\mid X) \n    &= \\ln\\bigg( \\frac{1}{(2\\pi)^{n/2}(\\sigma^2)^{n/2}}e^{ -\\frac{1}{2}(y-X\\beta)'(\\sigma^2I)^{-1}(y-X\\beta) } \\bigg) \\\\[6pt]\n    &= -\\frac{n}{2}\\ln 2\\pi - \\frac{n}{2}\\ln\\sigma^2 - \\frac{1}{2\\sigma^2}(y-X\\beta)'(y-X\\beta)\n  \\end{align}</math>\nDifferentiating this expression with respect to ''β'' and ''σ''<sup>2</sup> we'll find the ML estimates of these parameters:\n: <math>\\begin{align}\n  \\frac{\\partial\\mathcal{L}}{\\partial\\beta'} & = -\\frac{1}{2\\sigma^2}\\Big(-2X'y + 2X'X\\beta\\Big)=0 \\quad\\Rightarrow\\quad \\widehat\\beta = (X'X)^{-1}X'y \\\\[6pt]\n  \\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} & = -\\frac{n}{2} \\frac{1}{\\sigma^2} + \\frac{1}{2\\sigma^4}(y-X\\beta)'(y-X\\beta)=0 \\quad\\Rightarrow\\quad \\widehat\\sigma^{\\,2} = \\frac{1}{n} (y-X\\widehat\\beta)'(y-X\\widehat\\beta) = \\frac{1}{n} S(\\widehat\\beta)\n  \\end{align}</math>\nWe can check that this is indeed a maximum by looking at the [[Hessian matrix]] of the log-likelihood function.\n\n=== Finite-sample distribution ===\nSince we have assumed in this section that the distribution of error terms is known to be normal, it becomes possible to derive the explicit expressions for the distributions of estimators <math>\\widehat\\beta</math> and <math>\\widehat\\sigma^{\\,2}</math>:\n: <math>\\widehat\\beta = (X'X)^{-1}X'y = (X'X)^{-1}X'(X\\beta+\\varepsilon) = \\beta + (X'X)^{-1}X'\\mathcal{N}(0,\\sigma^2I)</math>\nso that by the [[Multivariate normal distribution#Affine transformation|affine transformation properties of multivariate normal distribution]]\n\n: <math>\\widehat\\beta\\mid X\\ \\sim\\ \\mathcal{N}(\\beta,\\, \\sigma^2(X'X)^{-1}).</math>\n\nSimilarly the distribution of <math>\\widehat\\sigma^{\\,2}</math> follows from\n\n: <math>\\begin{align}\n\\widehat\\sigma^{\\,2} &= \\tfrac{1}{n}(y-X(X'X)^{-1}X'y)'(y-X(X'X)^{-1}X'y) \\\\[5pt]\n&= \\tfrac{1}{n}(My)'My \\\\[5pt]\n&=\\tfrac{1}{n}(X\\beta+\\varepsilon)'M(X\\beta+\\varepsilon) \\\\[5pt]\n&= \\tfrac{1}{n}\\varepsilon'M\\varepsilon,\n\\end{align}</math>\n\nwhere <math>M=I-X(X'X)^{-1}X'</math> is the symmetric [[projection matrix]] onto subspace orthogonal to ''X'', and thus ''MX'' = ''X''&prime;''M'' = 0. We have argued [[#Expected_value_of_.CF.83.CC.82.C2.B2|before]] that this matrix rank ''n''&nbsp;–&nbsp;''p'', and thus by properties of [[Chi-squared distribution#Related distributions and properties|chi-squared distribution]],\n: <math>\\tfrac{n}{\\sigma^2} \\widehat\\sigma^{\\,2}\\mid X = (\\varepsilon/\\sigma)'M(\\varepsilon/\\sigma)\\ \\sim\\ \\chi^2_{n-p}</math>\n\nMoreover, the estimators <math>\\widehat\\beta</math> and <math>\\widehat\\sigma^{\\,2}</math> turn out to be [[Independent random variables|independent]] (conditional on ''X''), a fact which is fundamental for construction of the classical t- and F-tests. The independence can be easily seen from following: the estimator <math>\\widehat\\beta</math> represents coefficients of vector decomposition of <math>\\widehat{y}=X\\widehat\\beta=Py=X\\beta+P\\varepsilon</math> by the basis of columns of ''X'', as such <math>\\widehat\\beta</math> is a function of ''Pε''. At the same time, the estimator <math>\\widehat\\sigma^{\\,2}</math> is a norm of vector ''Mε'' divided by ''n'', and thus this estimator is a function of ''Mε''. Now, random variables (''Pε'', ''Mε'') are jointly normal as a linear transformation of ''ε'', and they are also uncorrelated because ''PM'' = 0. By properties of multivariate normal distribution, this means that ''Pε'' and ''Mε'' are independent, and therefore estimators <math>\\widehat\\beta</math> and <math>\\widehat\\sigma^{\\,2}</math> will be independent as well.\n\n==Derivation of simple linear regression estimators==\n{{further|Simple linear regression}}\n\nWe look for <math>\\widehat{\\alpha}</math> and <math>\\widehat{\\beta}</math> that minimize the sum of squared errors (SSE):\n:<math>\\min_{\\widehat{\\alpha}, \\widehat{\\beta}} \\,\\operatorname{SSE}\\left(\\widehat{\\alpha}, \\widehat{\\beta}\\right) \\equiv \\min_{\\widehat{\\alpha}, \\widehat{\\beta}} \\sum_{i=1}^n \\left(y_i - \\widehat{\\alpha} - \\widehat{\\beta} x_i\\right)^2</math>\n\nTo find a minimum take partial derivatives with respect to <math>\\widehat{\\alpha}</math> and <math>\\widehat{\\beta}</math>\n\n: <math>\\begin{align}\n                 &\\frac{\\partial}{\\partial\\widehat{\\alpha}} \\left (\\operatorname{SSE} \\left(\\widehat{\\alpha}, \\widehat{\\beta}\\right) \\right ) = -2\\sum_{i=1}^n \\left(y_i - \\widehat{\\alpha} - \\widehat{\\beta}x_i\\right) = 0 \\\\[4pt]\n  \\Rightarrow {} &\\sum_{i=1}^n \\left(y_i - \\widehat{\\alpha} - \\widehat{\\beta}x_i\\right) = 0 \\\\[4pt]\n  \\Rightarrow {} &\\sum_{i=1}^n y_i = \\sum_{i=1}^n \\widehat{\\alpha} + \\widehat{\\beta}\\sum_{i=1}^n x_i \\\\[4pt]\n  \\Rightarrow {} &\\sum_{i=1}^n y_i = n\\widehat{\\alpha} + \\widehat{\\beta}\\sum_{i=1}^n x_i \\\\[4pt]\n  \\Rightarrow {} &\\frac{1}{n}\\sum_{i=1}^n y_{i} = \\widehat{\\alpha} + \\frac{1}{n} \\widehat{\\beta}\\sum_{i=1}^n x_i \\\\[4pt]\n  \\Rightarrow {} &\\bar{y} = \\widehat{\\alpha} + \\widehat{\\beta}\\bar{x}\n\\end{align}</math>\n\nBefore taking partial derivative with respect to <math>\\widehat{\\beta}</math>, substitute the previous result for <math>\\widehat{\\alpha}.</math>\n\n: <math>\\min_{\\widehat{\\alpha}, \\widehat{\\beta}} \\sum_{i=1}^n \\left[y_i - \\left(\\bar{y} - \\widehat{\\beta} \\bar{x}\\right) - \\widehat{\\beta}x_{i}\\right]^2 = \\min_{\\widehat{\\alpha}, \\widehat{\\beta}} \\sum_{i=1}^n \\left[\\left(y_i - \\bar{y}\\right) - \\widehat{\\beta}\\left(x_i - \\bar{x}\\right) \\right]^2</math>\n\nNow, take the derivative with respect to <math>\\widehat{\\beta}</math>:\n\n: <math>\\begin{align}\n            &\\frac{\\partial}{\\partial\\widehat{\\beta}} \\left (\\operatorname{SSE} \\left(\\widehat{\\alpha}, \\widehat{\\beta}\\right) \\right )= -2\\sum_{i=1}^n \\left[\\left(y_{i} - \\bar{y}\\right) - \\widehat{\\beta}\\left(x_{i} - \\bar{x}\\right)\\right]\\left(x_{i}-\\bar{x}\\right) = 0 \\\\\n  \\Rightarrow {} &\\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)\\left(x_i - \\bar{x}\\right) - \\widehat{\\beta}\\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)^2 = 0 \\\\\n  \\Rightarrow {} & \\widehat{\\beta} = \\frac{\\sum_{i=1}^n \\left(y_{i} - \\bar{y}\\right)\\left(x_i - \\bar{x}\\right)}{\\sum_{i=1}^n \\left(x_{i}-\\bar{x}\\right)^2} = \\frac{\\operatorname{Cov}(x, y)}{\\operatorname{Var}(x)}\n\\end{align}</math>\n\nAnd finally substitute <math>\\widehat{\\beta}</math> to determine <math>\\widehat{\\alpha}</math>\n\n: <math>\\widehat{\\alpha} = \\bar{y} - \\widehat{\\beta}\\bar{x}</math>\n\n[[Category:Article proofs]]\n[[Category:Least squares]]"
    },
    {
      "title": "Regularized least squares",
      "url": "https://en.wikipedia.org/wiki/Regularized_least_squares",
      "text": "{{summarize|to|Least squares#Regularization}}\n{{Regression bar}}\n'''Regularized least squares''' ('''RLS''') is a family of methods for solving the [[least squares|least-squares]] problem while using [[regularization (mathematics)|regularization]] to further constrain the resulting solution.\n\nRLS is used for two main reasons. The first comes up when the number of variables in the linear system exceeds the number of observations. In such settings, the [[ordinary least squares|ordinary least-squares]] problem is [[ill-posed problem|ill-posed]] and is therefore impossible to fit because the associated optimization problem has infinitely many solutions. RLS allows the introduction of further constraints that uniquely determine the solution.\n\nThe second reason that RLS is used occurs when the number of variables does not exceed the number of observations, but the learned model suffers from poor [[Generalization error|generalization]]. RLS can be used in such cases to improve the generalizability of the model by constraining it at training time. This constraint can either force the solution to be \"sparse\" in some way or to reflect other prior knowledge about the problem such as information about correlations between features. A [[Bayesian inference|Bayesian]] understanding of this can be reached by showing that RLS methods are often equivalent to [[prior probability|priors]] on the solution to the least-squares problem.\n\n== General formulation ==\n\nConsider a learning setting given by a probabilistic space <math>(X \\times Y, \\rho(X,Y))</math>, <math>Y \\in R</math>. Let <math>S=\\{x_{i},y_{i}\\}_{i=1}^{n}</math> denote a training set of <math>n</math> pairs i.i.d. with respect to <math>\\rho</math>. Let <math>V:Y \\times R \\rightarrow [0;\\infty)</math> be a loss function. Define <math>F</math> as the space of the functions such that expected risk: \n:<math>\n\\varepsilon(f) = \\int V(y,f(x)) \\, d\\rho(x,y)\n</math>\nis well defined. \nThe main goal is to minimize the expected risk:\n:<math>\n\\inf_{f \\in F}\\varepsilon(f)\n</math>\nSince the problem cannot be solved exactly there is a need to specify how to measure the quality of a solution. A good learning algorithm should provide an estimator with a small risk.\n\nAs the joint distribution  <math>\\rho</math> is typically unknown, the empirical risk is taken. For regularized least squares the square loss function is introduced:\n:<math>\n\\varepsilon(f) = \\frac{1}{n}\\sum_{i=1}^n V(y_i,f(x_i)) = \\frac{1}{n}\\sum_{i=1}^n(y_i-f(x_i))^2\n</math>\n\nHowever, if the functions are from a relatively unconstrained space, such as the set of square-integrable functions on <math>X</math>, this approach may overfit the training data, and lead to poor generalization. Thus, it should somehow constrain or penalize the complexity of the function <math>f </math>. In RLS, this is accomplished by choosing functions from a reproducing kernel Hilbert space (RKHS) <math>\\mathcal {H} </math>, and adding a regularization term to the objective function, proportional to the norm of the function in <math>\\mathcal {H} </math>:\n:<math>\n\\inf_{f \\in F}\\varepsilon(f) + \\lambda R(f), \\lambda > 0\n</math>\n\n== Kernel formulation ==\n\n=== Definition of RKHS ===\nA RKHS can be defined by a [[symmetric function|symmetric]] [[positive-definite kernel function]] <math>K(x,z)</math> with the reproducing property:\n:<math>\n\\langle K_{x},f\\rangle_{\\mathcal{H}}=f(x),\n</math>\n\nwhere <math>K_x(z)=K(x,z)</math>. The RKHS for a kernel <math>K</math> consists of the [[complete metric space#Completion|completion]] of the space of functions spanned by <math>\\left\\{ K_x\\mid x \\in X\\right\\}</math>: <math>f(x)=\\sum_{i=1}^n \\alpha_i K_{x_i}(x),\\, f\\in\\mathcal{H}</math>, where all <math>\\alpha_i</math> are real numbers. Some commonly used kernels include the linear kernel, inducing the space of linear functions:\n\n: <math>K(x,z)=x^T z,</math>\n\nthe polynomial kernel, inducing the space of polynomial functions of order <math>d</math>:\n\n: <math>K(x,z)=(x^T z+1)^d,</math>\n\nand the Gaussian kernel:\n\n: <math>K(x,z)=e^{-\\frac{\\|x-z\\|^2}{\\sigma^2}}.</math>\n\nNote that for an arbitrary loss function <math>V</math>, this approach defines a general class of algorithms named Tikhonov regularization. For instance, using the [[hinge loss]] leads to the [[support vector machine]] algorithm, and using the [[epsilon-insensitive loss]] leads to [[support vector regression]].\n\n=== Arbitrary kernel===\nThe [[representer theorem]] guarantees that the solution can be written as:\n\n:<math>\nf(x) = \\sum_{i=1}^n c_i K(x_i,x)\n</math> for some <math>c \\in \\mathbb R^n</math>.\n\nThe minimization problem can be expressed as:\n\n:<math>\n\\min_{c \\in R^n}\\frac{1}{n}\\|Y-Kc\\|^2_{R^n} + \\lambda\\|f\\|^2_H \n</math>,\n\nwhere, with some abuse of notation, the <math>i,j</math> entry of kernel matrix <math>K</math> (as opposed to kernel function <math>K(\\cdot, \\cdot)</math>) is <math>K(x_i, x_j)</math>.\n\nFor such a function,\n:<math>\n\\begin{align}\n& \\|f\\|^2_H = \\langle f,f \\rangle_{H} =\\left\\langle \\sum_{i=1}^n c_i K(x_i,\\cdot), \\sum_{j=1}^n c_j K(x_{j},\\cdot) \\right\\rangle_H \\\\\n= {} & \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\langle K(x_i,\\cdot), K(x_j,\\cdot) \\rangle_H = \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j K(x_i,x_j) = c^T Kc,\n\\end{align}\n</math>\n\nThe following minimization problem can be obtained:\n:<math>\n\\min_{c \\in R^{n}}\\frac{1}{n}\\|Y-Kc\\|^{2}_{R^{n}} + \\lambda c^{T}Kc  \n</math>.\n\nAs the sum of convex functions is convex, the solution is unique and its minimum can be found by setting the gradient w.r.t <math>c</math> to <math>0</math>:\n:<math>\n-\\frac{1}{n}K(Y-Kc) + \\lambda Kc = 0 \\Rightarrow K(K+\\lambda n I)c = K Y \\Rightarrow c  = (K+\\lambda n I)^{-1}Y </math>,\nwhere <math> c \\in R^{n}</math>.\n\n==== Complexity ====\nThe complexity of training is basically the cost of computing the kernel matrix plus the cost of solving the linear system which is roughly <math>O(n^{3})</math>. The computation of the kernel matrix for the linear or [[Gaussian kernel]] is <math>O(n^{2}D)</math>. The complexity of testing is <math>O(n)</math>.\n\n=== Prediction ===\nThe prediction at a new test point <math>x_{*}</math> is:\n:<math>\nf(x_{*}) = \\sum_{i=1}^n c_i K(x_i,x_{*}) = K(X,X_{*})^T c\n</math>\n\n=== Linear kernel ===\nFor convenience a vector notation is introduced. Let <math>X</math> be an <math>n\\times d</math> matrix, where the rows are input vectors, and <math>Y</math> a <math>n\\times 1</math> vector where the entries are corresponding outputs. In terms of vectors, the kernel matrix can be written as <math>\\operatorname K=\\operatorname X\\operatorname X^{T}</math>. The learning function can be written as:\n\n:<math>\nf(x_{*})  = \\operatorname K_{x_{*}}c\n  =  x_{*}^T \\operatorname X^T c\n  =  x_{*}^T w\n</math>\n\nHere we define <math>w = X^T c, w \\in R^d</math>. The objective function can be rewritten as:\n:<math>\n\\begin{align}\n& \\frac{1}{n}\\|Y-\\operatorname Kc\\|^2_{R^n}+\\lambda c^{T}\\operatorname Kc \\\\[4pt]\n= {} & \\frac{1}{n}\\|y-\\operatorname X\\operatorname X^T c\\|^2_{R^n}+\\lambda c^T \\operatorname X\\operatorname X^{T} c = \\frac{1}{n}\\|y-\\operatorname Xw\\|^2_{R^n}+\\lambda \\|w\\|^2_{R^d}\n\\end{align}\n</math>\n\nThe first term is the objective function from [[ordinary least squares]] (OLS) regression, corresponding to the [[residual sum of squares]]. The second term is a regularization term, not present in OLS, which penalizes large <math>w</math> values.\nAs a smooth finite dimensional problem is considered and it is possible to apply standard calculus tools. In order to minimize the objective function,  the gradient is calculated with respect to <math>w</math> and set it to zero:\n\n: <math>\\operatorname X^T \\operatorname Xw-\\operatorname X^T y+\\lambda n w=0</math>\n\n: <math>w=(\\operatorname X^T \\operatorname X+\\lambda n \\operatorname I)^{-1}\\operatorname X^T y</math>\n\nThis solution closely resembles that of standard linear regression, with an extra term <math>\\lambda\\operatorname I</math>. If the assumptions of OLS regression hold, the solution <math>\nw=(\\operatorname X^{T}\\operatorname X)^{-1}\\operatorname X^{T}y</math>, with <math>\\lambda=0</math>, is an unbiased estimator, and is the minimum-variance linear unbiased estimator, according to the [[Gauss–Markov theorem]]. The term <math>\\lambda n \\operatorname I</math> therefore leads to a biased solution; however, it also tends to reduce variance. This is easy to see, as the [[covariance]] matrix of the <math>w</math>-values is proportional to <math>(\\operatorname X^T \\operatorname X+\\lambda n \\operatorname I)^{-1}</math>, and therefore large values of <math>\\lambda</math> will lead to lower variance. Therefore, manipulating <math>\\lambda</math> corresponds to trading-off bias and variance. For problems with high-variance <math>w</math> estimates, such as cases with relatively small <math>n</math> or with correlated regressors, the optimal prediction accuracy may be obtained by using a nonzero <math>\\lambda</math>, and thus introducing some bias to reduce variance. Furthermore, it is not uncommon in [[machine learning]] to have cases where <math>n<d</math>, in which case <math>X^T X</math> is [[rank (linear algebra)|rank]]-deficient, and a nonzero <math>\\lambda</math> is necessary to compute <math>(\\operatorname X^{T}\\operatorname X+\\lambda n \\operatorname I)^{-1}</math>.\n\n==== Complexity ====\nThe parameter <math>\\lambda</math>\ncontrols the invertibility of the matrix <math>X^{T}X + \\lambda n I </math>.\nSeveral methods can be used to solve the above linear system,\n[[Cholesky decomposition]] being probably the method of choice, since the matrix <math>X^{T}X + \\lambda n I </math> is [[symmetric]] and [[positive definite]]. The complexity of this method is <math>O(nD^{2})</math> for training and\n<math>O(D)</math> for testing. The cost <math>O(nD^{2})</math> is essentially that of computing <math>X^{T}X</math>, whereas the inverse computation (or rather the solution of the linear system) is roughly <math>O(D^{3})</math>.\n\n== Feature maps and Mercer's theorem==\nIn this section it will be shown how to extend RLS to any kind of reproducing kernel K. Instead of linear kernel a feature map is considered\n<math>\\Phi: X \\rightarrow F</math> for some Hilbert space <math>F</math>, called the feature space.   In this case the kernel is defined as: The matrix <math>X</math> is now replaced by the new data matrix  <math>\\Phi</math>, where  <math>\\Phi_{ij} = \\phi_{j}(x_{i})</math>, or the  <math>j</math>-th component of the  <math> \\phi(x_{i})</math>.\n\n:<math>\nK(x,x') = \\langle \\Phi(x), \\Phi(x') \\rangle_F. </math>\nIt means that for a given training set <math>K = \\Phi \\Phi^T</math>. Thus, the objective function can be written as:\n:<math>\n\\min_{c \\in \\mathbb R^n}\\|Y - \\Phi \\Phi^{T}\\|^2_{R^n} + \\lambda c^{T}\\Phi \\Phi^{T} c\n</math>\n\nThis approach is known as the [[kernel trick]]. This technique can significantly simplify the computational operations. If <math>F</math> is high dimensional, computing  <math>\\phi(x_{i})</math> may be rather intensive.  If the explicit form of the kernel function is known, we just need to compute and store the <math>n\\times n</math> kernel matrix <math>\\operatorname K</math>.\n\nIn fact, the [[Hilbert space]] <math>F</math> need not be isomorphic to <math>\\mathbb{R}^{m}</math>, and can be infinite dimensional. This  follows from [[Mercer's theorem]], which states that a continuous, symmetric, positive definite kernel function can be expressed as:\n\n<math>K(x,z)=\\sum_{i=1}^\\infty \\sigma_i e_i(x) e_i(z)</math>\n\nwhere <math>e_i(x)</math> form an [[orthonormal basis]] for <math>\\ell^2(X)</math>, and <math>\\sigma_i \\in\\mathbb{R}</math>. If feature maps  is defined <math>\\phi(x)</math> with components <math>\\phi_{i}(x)=\\sqrt{\\sigma_{i}}e_{i}(x)</math>, it follows that <math>K(x,z)=\\langle\\phi(x),\\phi(z)\\rangle</math>. This demonstrates that any kernel can be associated with a feature map, and that RLS generally consists of linear RLS performed in some possibly higher-dimensional feature space. While Mercer's theorem shows how one feature map that can be associated with a kernel, in fact multiple feature maps can be associated with a given reproducing kernel. For instance, the map <math>\\phi(x)=K_{x}</math> satisfies the property <math>K(x,z)=\\langle\\phi(x),\\phi(z)\\rangle</math> for an arbitrary reproducing kernel.\n\n==Bayesian interpretation==\n{{further|Bayesian linear regression|Bayesian interpretation of kernel regularization}}\n\nLeast squares can be viewed as a likelihood maximization under an assumption of normally distributed residuals. This is because the exponent of the [[Gaussian distribution]] is quadratic in the data, and so is the least-squares objective function. In this framework, the regularization terms of RLS can be understood to be encoding [[prior distribution|priors]] on <math>w</math>. For instance, Tikhonov regularization corresponds to a normally distributed prior on <math>w</math> that is centered at 0. To see this, first note that the OLS objective is proportional to the [[log-likelihood]] function when each sampled <math>y^i</math> is normally distributed around <math>w^T \\cdot x^i</math>. Then observe that a normal prior on <math>w</math> centered at 0 has a log-probability of the form\n: <math>\\log P(w) = q - \\alpha \\sum_{j=1}^d w_j^2</math>\nwhere <math>q</math> and <math>\\alpha</math> are constants that depend on the variance of the prior and are independent of <math>w</math>. Thus, minimizing the logarithm of the likelihood times the prior is equivalent to minimizing the sum of the OLS loss function and the ridge regression regularization term.\n\nThis gives a more intuitive interpretation for why [[Tikhonov regularization]] leads to a unique solution to the least-squares problem: there are infinitely many vectors <math>w</math> satisfying the constraints obtained from the data, but since we come to the problem with a prior belief that <math>w</math> is normally distributed around the origin, we will end up choosing a solution with this constraint in mind.\n\nOther regularization methods correspond to different priors. See the [[Regularized least squares#List of RLS methods|list]] below for more details.\n\n==Specific examples==\n\n===Ridge regression (or Tikhonov regularization){{anchor|Ridge regression|Tikhonov regularization}}===\n{{main|Ridge regression{{!}}Ridge regression (or Tikhonov regularization)}}\nOne particularly common choice for the penalty function  <math>R</math> is the squared [[l2 norm|<math>\\ell_2</math> norm]], i.e.,\n: <math>R(w) = \\sum_{j=1}^d w_j^2</math>\n:<math> \\frac{1}{n}\\|Y-\\operatorname Xw\\|^{2}_{2}+\\lambda  \\sum_{j=1}^d |w_j|^2 \\rightarrow  \\min_{w \\in \\mathbf{R^{d}}}</math>\nThe most common names for this are called [[Tikhonov regularization]] and [[ridge regression]]. \nIt admits a closed-form solution for <math>w</math>:\n: <math>w = (X^T X + \\alpha I)^{-1} X^T Y </math>\nThe name ridge regression alludes to the fact that the <math>\\alpha I</math> term adds positive entries along the diagonal \"ridge\" of the sample [[covariance matrix]] <math>X^T X</math>.\n\nWhen <math>\\alpha=0</math>, i.e., in the case of [[ordinary least squares]], the condition that <math>d > n</math> causes the sample [[covariance matrix]] <math>X^T X</math> to not have full rank and so it cannot be inverted to yield a unique solution. This is why there can be an infinitude of solutions to the [[ordinary least squares]] problem when <math>d > n</math>. However, when <math>\\alpha > 0</math>, i.e., when ridge regression is used, the addition of <math>\\alpha I</math> to the sample covariance matrix ensures that all of its eigenvalues will be strictly greater than 0. In other words, it becomes invertible, and the solution becomes unique.\n\nCompared to ordinary least squares, ridge regression is not unbiased. It accepts little bias to reduce variance and the [[mean square error]], and helps to improve the prediction accuracy. Thus, ridge estimator yields more stable solutions by shrinking coefficients but suffers from the lack of sensitivity to the data.\n\n===Lasso regression===\n{{main|Lasso (statistics)}}\nThe least absolute selection and shrinkage (LASSO) method is another popular choice. In [[lasso regression]], the lasso penalty function <math>R</math> is the [[l1 norm|<math>\\ell_1</math> norm]], i.e.\n: <math>R(w) = \\sum_{j=1}^d \\left| w_j \\right|</math>\n:<math> \\frac{1}{n}\\|Y-\\operatorname Xw\\|^{2}_{2}+\\lambda  \\sum_{j=1}^d |w_j| \\rightarrow  \\min_{w \\in \\mathbf{R^{d}}}</math>\n\nNote that the lasso penalty function is convex but not strictly convex. \nUnlike [[Tikhonov regularization]], this scheme does not have a convenient closed-form solution: instead, the solution is typically found using [[quadratic programming]] or more general [[convex optimization]] methods, as well as by specific algorithms such as the [[least-angle regression]] algorithm.\n\nAn important difference between lasso regression and Tikhonov regularization is that lasso regression forces more entries of <math>w</math> to actually equal 0 than would otherwise. In contrast, while Tikhonov regularization forces entries of <math>w</math> to be small, it does not force more of them to be 0 than would be otherwise. Thus, LASSO regularization is more appropriate than Tikhonov regularization in cases in which we expect the number of non-zero entries of <math>w</math> to be small, and Tikhonov regularization is more appropriate when we expect that entries of <math>w</math> will generally be small but not necessarily zero. Which of these regimes is more relevant depends on the specific data set at hand.\n\nBesides feature selection described above, LASSO has some limitations. Ridge regression provides better accuracy in the case <math> n > d </math> for highly correlated variables.<ref>{{cite journal\n | author = Tibshirani Robert \n | title = Regression shrinkage and selection via the lasso\n | journal = Journal of the Royal Statistical Society, Series B\n | year = 1996\n | volume =  58\n | pages = ''pp.'' 266&ndash;288\n | url = https://web.stanford.edu/~hastie/Papers/elasticnet.pdf\n}}</ref> In another case, <math> n < d </math>, LASSO selects at most <math>\nn</math> variables. Moreover, LASSO tends to select some arbitrary variables from group of highly correlated samples, so there is no grouping effect.\n\n===''ℓ''<sub>0</sub> Penalization===\n:<math> \\frac{1}{n}\\|Y-\\operatorname Xw\\|^2_2+\\lambda \\|w_j\\|_0 \\rightarrow  \\min_{w \\in \\mathbf{R^d}}</math>\nThe most extreme way to enforce sparsity is to say that the actual magnitude of the coefficients of <math>w</math> does not matter; rather, the only thing that determines the complexity of <math>w</math> is the number of non-zero entries. This corresponds to setting <math>R(w)</math> to be the [[l0 norm|<math>\\ell_0</math> norm]] of <math>w</math>. This regularization function, while attractive for the sparsity that it guarantees, is very difficult to solve because doing so requires optimization of a function that is not even weakly [[convex optimization|convex]]. Lasso regression is the minimal possible relaxation of <math>\\ell_0</math> penalization that yields a weakly convex optimization problem.\n\n=== Elastic net===\n{{main|Elastic net regularization}}\n\nFor any non-negative <math>\\lambda_{1}</math> and <math>\\lambda_{2}</math> the objective has the following form:\n\n:<math>\\frac{1}{n}\\|Y-\\operatorname Xw\\|^2_2+\\lambda_{1}\\sum_{j=1}^d |w_j| + \\lambda_2 \\sum_{j=1}^d |w_j|^2 \\rightarrow  \\min_{w \\in \\mathbf{R^d}}</math>\n\nLet <math>\\alpha = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}</math>, then the solution of the minimization problem is described as:\n\n:<math>\\frac{1}{n}\\|Y-\\operatorname Xw\\|^2_2 \\rightarrow  \\min_{w \\in \\mathbf{R^d}} \\text{s.t.} (1-\\alpha)\\|w\\|_1 + \\alpha \\|w\\|_2 \\leq t</math> for some <math>t</math>.\n\nConsider   <math>(1-\\alpha)\\|w\\|_{1} + \\alpha \\|w\\|_{2} \\leq t</math> as an Elastic Net penalty function.\n\nWhen <math>\\alpha = 1 </math>, elastic net becomes ridge regression, whereas <math>\\alpha = 0 </math> it becomes Lasso. <math>\\forall \\alpha \\in (0,1]</math> Elastic Net penalty function doesn't have the first derivative at 0 and it is strictly convex \n<math>\\forall \\alpha > 0</math> taking the properties both [[lasso regression]] and [[ridge regression]].\n\nOne of the main properties of the Elastic Net is that it can select groups of correlated variables. The difference between weight vectors of samples <math>x_{i}</math> and <math>x_{j}</math> is given by:\n:<math>\n|w^{*}_i(\\lambda_{1}, \\lambda_{2}) -  w^{*}_{j}(\\lambda_1, \\lambda_2)| \\leq \\frac{\\sum_{i=1}^n|y_i|}{\\lambda_2}\\sqrt{2(1-\\rho_{ij})}\n</math>, where <math>\\rho_{ij} = x_{i}^{T}x_{j}</math>.<ref>{{cite journal\n | author = [[Zou Hui|Hui, Zou]] |author2=Hastie, Trevor \n | title = Regularization and Variable Selection via the Elastic Net\n | journal = JRSSB\n | year = 2003\n | volume = 67\n | issue = 2\n | pages = ''pp.'' 301&ndash;320\n | url = https://web.stanford.edu/~hastie/Papers/elasticnet.pdf\n}}</ref>\n\nIf <math>x_{i}</math> and <math>x_{j}</math> are highly correlated ( <math>\\rho_{ij} \\rightarrow 1</math>), the weight vectors are very close. In the case of negatively correlated samples  ( <math>\\rho_{ij} \\rightarrow -1</math>) the samples <math>-x_{j}</math> can be taken. To summarize, for highly correlated variables the weight vectors tend to be equal up to a sign in the case of negative correlated variables.\n\n==Partial list of RLS methods==\nThe following is a list of possible choices of the regularization function <math>R(\\cdot)</math>, along with the name for each one, the corresponding prior if there is a simple one, and ways for computing the solution to the resulting optimization problem.\n{|class=\"wikitable sortable\"\n!Name!!Regularization function!!Corresponding prior!!Methods for solving\n|-\n|[[Tikhonov regularization]]||<math>\\| w \\|_2^2 </math>||[[Normal distribution|Normal]]||Closed form\n|-\n|[[Lasso (statistics)|Lasso regression]] || <math>\\| w \\|_1</math> || [[Laplace distribution|Laplace]] || [[Proximal gradient method|Proximal gradient descent]], [[least angle regression]]\n|-\n|<math>\\ell_0</math> penalization || <math> \\|w \\|_0 </math> || – || [[Forward selection]], [[Backward elimination]], use of priors such as [[spike and slab]]\n|-\n| [[Elastic net regularization|Elastic nets]] || <math> \\beta \\|w\\|_1 + (1-\\beta) \\|w \\|_2^2 </math> || – || [[Proximal gradient method|Proximal gradient descent]]\n|-\n| [[Total variation regularization]] || <math> \\sum_{j=1}^{d-1} | w_{j+1} - w_j | </math> || – || [[Split–Bregman method]], among others\n|}\n\n==See also==\n* [[Least squares]]\n* [[Regularization (mathematics)|Regularization]] in mathematics.\n* [[Generalization error]], one of the reasons regularization is used.\n* [[Tikhonov regularization]]\n* [[Lasso regression]]\n* [[Elastic net regularization]]\n* [[:Least-angle regression]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.stanford.edu/~hastie/TALKS/enet_talk.pdf http://www.stanford.edu/~hastie/TALKS/enet_talk.pdf Regularization and Variable Selection via the Elastic Net] (presentation)\n*[http://www.mit.edu/~9.520/fall15/slides/class06/class06_RLSSVM.pdf Regularized Least Squares and Support Vector Machines] (presentation)\n* [http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf Regularized Least Squares](presentation)\n\n[[Category:Least squares]]\n[[Category:Linear algebra]]\n[[Category:Inverse problems]]"
    },
    {
      "title": "Residual sum of squares",
      "url": "https://en.wikipedia.org/wiki/Residual_sum_of_squares",
      "text": "{{more citations needed|date=April 2013}}\nIn [[statistics]], the '''residual sum of squares (RSS)''',  also known as the '''sum of squared residuals (SSR)''' or the '''sum of squared errors of prediction (SSE)''',  is the [[summation|sum]] of the [[square (arithmetic)|squares]] of [[errors and residuals in statistics|residuals]] (deviations predicted from actual empirical values of data).  It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. It is used as an [[optimality criterion]] in parameter selection and [[model selection]].\n\nIn general, [[total sum of squares]] = [[explained sum of squares]] + '''residual sum of squares'''.  For a proof of this in the multivariate [[ordinary least squares]] (OLS) case, see [[Explained sum of squares#Partitioning in the general ordinary least squares model|partitioning in the general OLS model]].\n\n==One explanatory variable==\n\nIn a model with a single explanatory variable, RSS is given by:\n\n:<math>RSS = \\sum_{i=1}^n (y_i - f(x_i))^2 </math>\n\nwhere ''y''<sub>''i''</sub> is the ''i'' <sup>th</sup> value of the variable to be predicted, ''x''<sub>''i''</sub> is the ''i'' <sup>th</sup> value of the explanatory variable, and <math>f(x_i)</math> is the predicted value of ''y''<sub>''i''</sub> (also termed <math>\\hat{y_i}</math>).\nIn a standard linear simple [[regression model]], <math>y_i = a+bx_i+\\varepsilon_i\\,</math>, where ''a'' and ''b'' are [[coefficient]]s, ''y'' and ''x'' are the [[regressand]] and the [[regressor]], respectively, and &epsilon; is the [[errors and residuals in statistics|error term]].  The sum of squares of residuals is the sum of squares of [[estimator|estimates]] of &epsilon;<sub>''i''</sub>; that is\n\n:<math>RSS = \\sum_{i=1}^n (\\varepsilon_i)^2 = \\sum_{i=1}^n (y_i - (\\alpha + \\beta x_i))^2 </math>\n\nwhere <math>\\alpha</math> is the estimated value of the constant term <math>a</math> and <math>\\beta</math> is the estimated value of the slope coefficient ''b''.\n\n==Matrix expression for the OLS residual sum of squares==\n\nThe general regression model with 'n' observations and 'k' explanators, the first of which is a constant unit vector whose coefficient is the regression intercept, is\n\n:<math> y = X \\beta + e</math>\n\nwhere ''y'' is an ''n'' × 1 vector of dependent variable observations, each column of the ''n'' × ''k'' matrix ''X'' is a vector of observations on one of the ''k'' explanators, <math>\\beta </math> is a ''k'' × 1 vector of true coefficients,  and ''e'' is an ''n''× 1 vector of the true underlying errors.  The [[ordinary least squares]] estimator for <math>\\beta</math> is\n\n:<math> X \\hat \\beta = y \\iff</math>\n\n:<math> X^T X \\hat \\beta = X^T y \\iff</math>\n\n:<math> \\hat \\beta = (X^T X)^{-1}X^T y.</math>\n\nThe residual vector <math>\\hat e</math> = <math>y - X \\hat \\beta = y - X (X^T X)^{-1}X^T y</math>, so the residual sum of squares is:\n\n:<math>RSS = \\hat e ^T \\hat e =  \\| \\hat e \\|^2 </math>,\n\n{{anchor|Norm of residuals}}(equivalent to the square of the [[vector norm|norm]] of residuals); in full:\n\n:<math>  RSS = y^T y - y^T X(X^T X)^{-1} X^T y = y^T [I - X(X^T X)^{-1} X^T] y = y^T [I - H] y</math>,\n\nwhere H is the [[hat matrix]], or the projection matrix in linear regression.\n\n== Relation with Pearson's product-moment correlation ==\nThe [[Least squares|least-squares regression line]] is given by\n\n:<math>y=ax+b</math>,\n\nwhere <math>b=\\bar{y}-a\\bar{x}</math> and <math>a=\\frac{S_{xy}}{S_{xx}}</math>, where <math>S_{xy}=\\sum_{i=1}^n(\\bar{x}-x_i)(\\bar{y}-y_i)</math> and <math>S_{xx}=\\sum_{i=1}^n(\\bar{x}-x_i)^2.</math>\n\nTherefore,\n\n:<math>RSS = \\sum_{i=1}^n (y_i - f(x_i))^2= \\sum_{i=1}^n (y_i - (ax_i+b))^2= \\sum_{i=1}^n (y_i - ax_i-\\bar{y}+a\\bar{x})^2</math>\n::<math>= \\sum_{i=1}^n (a(\\bar{x}-x_i)-(\\bar{y}-y_i))^2=a^2S_{xx}-2aS_{xy}+S_{yy}=S_{yy}-aS_{xy}=S_{yy}(1-\\frac{S_{xy}^2}{S_{xx}S_{yy}}) </math>\n\nwhere <math>S_{yy}=\\sum_{i=1}^n(\\bar{y}-y_i)^2 .</math>\n\nThe [[Pearson correlation coefficient|Pearson product-moment correlation]] is given by <math>r=\\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}; </math> therefore, <math>RSS=S_{yy}(1-r^2). </math>\n\n==See also==\n*[[Sum of squares (statistics)]]\n*[[Squared deviations]]\n*[[Errors and residuals in statistics]]\n*[[Lack-of-fit sum of squares]]\n*[[Degrees of freedom (statistics)#Sum of squares and degrees of freedom]]\n*[[Chi-squared distribution#Applications]]\n*[[Mean squared error]]\n\n==References==\n* {{cite book\n|title = Applied Regression Analysis\n|edition = 3rd\n|last1= Draper |first1=N.R. |last2=Smith |first2=H.\n|publisher = John Wiley\n|year = 1998\n|isbn = 0-471-17082-8}}\n\n[[Category:Least squares]]\n[[Category:Errors and residuals]]"
    },
    {
      "title": "Total least squares",
      "url": "https://en.wikipedia.org/wiki/Total_least_squares",
      "text": "{{Regression bar}}\n[[Image:Total least squares.svg|right|thumb|200px| The bivariate (Deming regression) case of total least squares. The red lines show the error in both ''x'' and ''y''. This is different from the traditional least squares method which measures error parallel to the ''y'' axis. The case shown, with deviations measured perpendicularly, arises when ''x'' and ''y'' have equal variances.]]\n\nIn [[applied statistics]], '''total least squares''' is a type of [[errors-in-variables regression]], a [[least squares]] data modeling technique in which observational errors on both dependent and independent variables are taken into account. It is a generalization of [[Deming regression]] and also of [[orthogonal regression]], and can be applied to both linear and non-linear models.\n\nThe total least squares approximation of the data is generically equivalent to the best, in the [[Frobenius norm]], [[low-rank approximation]] of the data matrix.<ref>I. Markovsky and [[Sabine Van Huffel|S. Van Huffel]], ''Overview of total least squares methods.'' Signal Processing, vol. 87, pp. 2283–2302, 2007. [http://eprints.ecs.soton.ac.uk/13855/1/tls_overview.pdf preprint]</ref>\n\n== Linear model ==\n\n===Background===\n\nIn the [[least squares]] method of data modeling, the [[objective function]], ''S'',\n:<math>S=\\mathbf{r^TWr},</math>\nis minimized, where ''r'' is the vector of [[errors and residuals in statistics|residuals]] and ''W'' is a weighting matrix. In [[linear least squares (mathematics)|linear least squares]] the model contains equations which are linear in the parameters appearing in the parameter vector <math>\\boldsymbol\\beta</math>, so the residuals are given by\n:<math>\\mathbf{r=y-X\\boldsymbol\\beta}.</math>\nThere are ''m'' observations in '''y''' and ''n'' parameters in '''β''' with ''m''>''n''. '''X''' is a ''m''×''n'' matrix whose elements are either constants or functions of the independent variables, '''x'''. The weight matrix '''W''' is, ideally, the inverse of the [[variance-covariance matrix]] <math>\\mathbf M_y</math> of the observations '''y'''. The independent variables are assumed to be error-free. The parameter estimates are found by setting the gradient equations to zero, which results in the normal equations\n<ref group=\"note\">An alternative form is <math>\\mathbf{X^TWX\\boldsymbol\\Delta \\boldsymbol\\beta=X^T W \\boldsymbol\\Delta y}</math>, where <math>\\boldsymbol\\Delta \\boldsymbol\\beta</math> is the parameter shift from some starting estimate of <math>\\boldsymbol\\beta</math> and <math>\\boldsymbol\\Delta \\mathbf y</math> is the difference between '''y''' and the value calculated using the starting value of <math>\\boldsymbol\\beta</math></ref> \n:<math>\\mathbf{X^TWX\\boldsymbol\\beta=X^T Wy}.</math>\n\n===Allowing observation errors in all variables===\n\nNow, suppose that both '''x''' and '''y''' are observed subject to error, with variance-covariance matrices <math>\\mathbf M_x</math> and <math>\\mathbf M_y</math> respectively. In this case the objective function can be written as\n:<math>S=\\mathbf{r_x^TM_x^{-1}r_x+r_y^TM_y^{-1}r_y},</math>\nwhere <math>\\mathbf r_x</math> and <math>\\mathbf r_y</math> are the residuals in '''x''' and '''y''' respectively. Clearly these residuals cannot be independent of each other, but they must be constrained by some kind of relationship. Writing the model function as <math>\\mathbf{f(r_x,r_y,\\boldsymbol\\beta)}</math>, the constraints are expressed by ''m'' [[condition equations]].<ref>W.E. Deming, Statistical Adjustment of Data, Wiley, 1943</ref>\n\n:<math>\\mathbf{F=\\Delta y -\\frac{\\partial f}{\\partial r_x} r_x-\\frac{\\partial f}{\\partial r_y} r_y -X\\Delta\\boldsymbol\\beta=0}.</math>\nThus, the problem is to minimize the objective function subject to the ''m'' constraints. It is solved by the use of [[Lagrange multipliers]]. After some algebraic manipulations,<ref>{{cite book |last=Gans |first=Peter |title=Data Fitting in the Chemical Sciences |year=1992 |publisher=Wiley |isbn=9780471934127 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471934127.html |accessdate=4 December 2012}}</ref> the result is obtained.\n\n:<math>\\mathbf{X^TM^{-1}X\\Delta \\boldsymbol\\beta=X^T M^{-1} \\Delta y}, </math>\n\nor alternatively <math>\\mathbf{X^TM^{-1}X \\boldsymbol\\beta=X^T M^{-1} y},</math>\nwhere '''M''' is the variance-covariance matrix relative to both independent and dependent variables.\n:<math>\\mathbf{M=K_xM_xK_x^T+K_yM_yK_y^T;\\ K_x=-\\frac{\\partial f}{\\partial r_x},\\ K_y=-\\frac{\\partial f}{\\partial r_y}}.</math>\n\n=== Example ===\n\nWhen the data errors are uncorrelated, all matrices '''M''' and '''W''' are diagonal. Then, take the example of straight line fitting.\n:<math>f(x_i,\\beta)=\\alpha + \\beta x_i</math>\nin this case\n:<math>M_{ii}=\\sigma^2_{y,i}+\\beta^2 \\sigma^2_{x,i}</math>\nshowing how the variance at the ''i''th point is determined by the variances of both independent and dependent variables and by the model being used to fit the data. The expression may be generalized by noting that the parameter <math>\\beta</math> is the slope of the line.\n:<math>M_{ii}=\\sigma^2_{y,i}+\\left(\\frac{dy}{dx}\\right)^2_i \\sigma^2_{x,i}</math>\n\nAn expression of this type is used in fitting [[Determination of equilibrium constants#Parameter errors and correlation|pH titration data]] where a small error on ''x'' translates to a large error on y when the slope is large.\n\n=== Algebraic point of view ===\nFirst of all it is necessary to note that the TLS problem does not have a solution in general, which was already shown in 1980.<ref>G. H. Golub and C. F. Van Loan, An analysis of the total least squares problem. Numer. Anal., 17, 1980, pp. 883–893.</ref> The following considers the simple case where a unique solution exists without making any particular assumptions.\n\nThe computation of the TLS using [[singular value decomposition]] is described in standard texts.<ref>{{Cite book\n | last1=Golub |first1=Gene H. |authorlink1=Gene H. Golub\n | last2=Van Loan |first2=Charles F. |authorlink2=Charles F. Van Loan\n | title = Matrix Computations\n | edition = 3rd\n | publisher = [[The Johns Hopkins University Press]]\n | year = 1996\n}} pp 596.</ref> We can solve the equation \n:<math>XB \\approx Y</math>\nfor ''B'' where ''X'' is ''m''-by-''n'' and ''Y'' is ''m''-by-''k''. <ref group=\"note\">The notation ''XB''&nbsp;≈&nbsp;''Y'' is used here to reflect the notation used in the earlier part of the article.  In the computational literature the problem has been more commonly presented as ''AX''&nbsp;≈&nbsp;''B'', i.e. with the letter ''X'' used for the ''n''-by-''k'' matrix of unknown regression coefficients.</ref>\n\nThat is, we seek to find ''B'' that minimizes error matrices ''E'' and ''F'' for ''X'' and ''Y'' respectively. That is,\n:<math>\\mathrm{argmin}_{E,F} \\| [E\\; F] \\|_F, \\qquad (X+E) B = Y+F</math>\nwhere <math>[E\\; F]</math> is the [[augmented matrix]] with ''E'' and ''F'' side by side and <math>\\|\\cdot\\|_F</math> is the [[Frobenius norm]], the square root of the sum of the squares of all entries in a matrix and so equivalently the square root of the sum of squares of the lengths of the rows or columns of the matrix.\n\nThis can be rewritten as\n:<math>[(X+E) \\; (Y+F)] \\begin{bmatrix} B\\\\ -I_k\\end{bmatrix} = 0.</math>\nwhere <math>I_k</math> is the <math>k\\times k</math> identity matrix.\nThe goal is then to find <math>[E\\; F]</math> that reduces the rank of <math>[X\\; Y]</math> by ''k''. Define <math>[U] [\\Sigma] [V]^*</math> to be the singular value decomposition of the augmented matrix <math>[X\\; Y]</math>.\n:<math>[X\\; Y] = [U_X\\; U_Y] \\begin{bmatrix}\\Sigma_X &0 \\\\ 0 & \\Sigma_Y\\end{bmatrix}\\begin{bmatrix}V_{XX} & V_{XY} \\\\ V_{YX} & V_{YY}\\end{bmatrix}^* =  [U_X\\; U_Y] \\begin{bmatrix}\\Sigma_X &0 \\\\ 0 & \\Sigma_Y\\end{bmatrix} \\begin{bmatrix} V_{XX}^* & V_{YX}^* \\\\ V_{XY}^* & V_{YY}^*\\end{bmatrix}</math>\nwhere ''V'' is partitioned into blocks corresponding to the shape of ''X'' and ''Y''.\n\nUsing the [[Eckart–Young theorem]], the approximation minimising the norm of the error is such that matrices <math>U</math> and <math>V</math> are unchanged, while the <math>k</math>-smallest singular values are replaced with zeroes. That is, we want\n:<math>[(X+E)\\; (Y+F)] = [U_X\\; U_Y] \\begin{bmatrix}\\Sigma_X &0 \\\\ 0 & 0_{k\\times k}\\end{bmatrix}\\begin{bmatrix}V_{XX} & V_{XY} \\\\ V_{YX} & V_{YY}\\end{bmatrix}^*</math>\nso by linearity,\n:<math>[E\\; F] = -[U_X\\; U_Y] \\begin{bmatrix}0_{n\\times n} &0 \\\\ 0 & \\Sigma_Y\\end{bmatrix}\\begin{bmatrix}V_{XX} & V_{XY} \\\\ V_{YX} & V_{YY}\\end{bmatrix}^*. </math>\nWe can then remove blocks from the ''U'' and Σ matrices, simplifying to\n:<math>[E\\; F] = -U_Y\\Sigma_Y \\begin{bmatrix}V_{XY}\\\\V_{YY}\\end{bmatrix}^*= -[X\\; Y] \\begin{bmatrix}V_{XY}\\\\V_{YY}\\end{bmatrix}\\begin{bmatrix}V_{XY}\\\\ V_{YY}\\end{bmatrix}^*.</math>\nThis provides ''E'' and ''F'' so that \n:<math>[(X+E) \\; (Y+F)] \\begin{bmatrix}V_{XY}\\\\ V_{YY}\\end{bmatrix} = 0.</math>\nNow if <math>V_{YY}</math> is nonsingular, which is not always the case (note that the behavior of TLS when <math>V_{YY}</math> is singular is not well understood yet), we can then right multiply both sides by <math>-V_{YY}^{-1}</math> to bring the bottom block of the right matrix to the negative identity, giving<ref>Bjõrck, Ake (1996) ''Numerical Methods for Least Squares Problems'', Society for Industrial and Applied Mathematics. {{ISBN|978-0898713602}} {{page needed|date=June 2012}}</ref>\n: <math>[(X+E) \\; (Y+F)] \\begin{bmatrix} -V_{XY} V_{YY}^{-1} \\\\ -V_{YY} V_{YY}^{-1}\\end{bmatrix} = [(X+E) \\; (Y+F)] \\begin{bmatrix} B\\\\ -I_k\\end{bmatrix} =  0 ,</math>\nand so\n:<math>B=-V_{XY} V_{YY}^{-1}.</math>\n\nA naive [[GNU Octave]] implementation of this is:\n\n<source lang=\"matlab\">\nfunction B = tls(X,Y)\n\n[m n]   = size(X);            % n is the width of X (X is m by n)\nZ       = [X Y];              % Z is X augmented with Y.\n[U S V] = svd(Z,0);           % find the SVD of Z.\nVXY     = V(1:n,1+n:end);     % Take the block of V consisting of the first n rows and the n+1 to last column\nVYY     = V(1+n:end,1+n:end); % Take the bottom-right block of V.\nB       = -VXY/VYY;\n\nend\n</source>\n\nThe way described above of solving the problem, which requires that the matrix <math>V_{YY}</math> is nonsingular, can be slightly extended by the so-called ''classical TLS algorithm''.<ref>[[Sabine Van Huffel|S. Van Huffel]] and J. Vandewalle (1991) ''The Total Least Squares Problems: Computational Aspects and Analysis''. SIAM Publications, Philadelphia PA.</ref>\n\n=== Computation ===\n\nThe standard implementation of classical TLS algorithm is available through [http://www.netlib.org/vanhuffel/index.html Netlib], see also.<ref>[[Sabine Van Huffel|S. Van Huffel]], Documented Fortran 77 programs of the extended classical total least squares algorithm, the partial singular value decomposition algorithm and the partial total least squares algorithm, Internal Report ESAT-KUL 88/1, ESAT Lab., Dept. of Electrical Engineering, Katholieke Universiteit Leuven, 1988.</ref><ref>[[Sabine Van Huffel|S. Van Huffel]], The extended classical total least squares algorithm, J. Comput. Appl. Math., 25, pp. 111–119, 1989.</ref> All modern implementations based, for example, on solving a sequence of ordinary least squares problems, approximate the matrix <math>B</math> (denoted <math>X</math> in the literature), as introduced by [[Sabine Van Huffel|Van Huffel]] and Vandewalle. It is worth noting, that this <math>B</math> is, however, ''not the TLS solution'' in many cases.<ref>M. Plešinger, The Total Least Squares Problem and Reduction of Data in AX ≈ B. Doctoral Thesis, TU of Liberec and Institute of Computer Science, AS CR Prague, 2008. Ph.D. Thesis</ref><ref>I. Hnětynková, M. Plešinger, D. M. Sima, Z. Strakoš, and [[Sabine Van Huffel|S. Van Huffel]], The total least squares problem in AX ≈ B. A new classification with the relationship to the classical works. SIMAX vol. 32 issue 3 (2011), pp. 748–770.</ref>\n\n== Non-linear model ==\nFor [[non-linear least squares|non-linear systems]] similar reasoning shows that the normal equations for an iteration cycle can be written as\n:<math>\\mathbf{J^TM^{-1}J\\Delta \\boldsymbol\\beta=J^T M^{-1} \\Delta y}. </math>\n\n== Geometrical interpretation ==\n{{main|Curve fitting#Algebraic fit versus geometric fit for curves}}\n{{further|Orthogonal regression}}\nWhen the independent variable is error-free a residual represents the \"vertical\" distance between the observed data point and the fitted curve (or surface). In total least squares a residual represents the distance between a data point and the fitted curve measured along some direction. In fact, if both variables are measured in the same units and the errors on both variables are the same, then the residual represents the [[distance from a point to a line|shortest distance between the data point and the fitted curve]], that is, the residual vector is perpendicular to the tangent of the curve. For this reason, this type of regression is sometimes called ''two dimensional Euclidean regression'' (Stein, 1983)<ref>{{cite journal |last=Stein |first=Yaakov J. |title=Two Dimensional Euclidean Regression |url =http://www.dspcsp.com/pubs/euclreg.pdf }}</ref> or ''orthogonal regression''.\n\n== Scale invariant methods ==\nA serious difficulty arises if the variables are not measured in the same units. First consider measuring distance between a data point and the line, as in the diagram – what are the measurement units for this distance? If we consider measuring distance based on Pythagoras' Theorem then it is clear that we shall be adding quantities measured in different units, which is  meaningless. Secondly, if we rescale one of the variables e.g., measure in grams rather than kilograms, then we shall end up with different results (a different line). To avoid these problems it is sometimes suggested that we convert to dimensionless variables—this may be called normalization or standardization. However there are various ways of doing this, and these lead to fitted models which are not equivalent to each other. One approach is to normalize by known (or estimated) measurement precision thereby minimizing the [[Mahalanobis distance]] from the points to the line, providing a [[maximum-likelihood]] solution;{{Citation needed|date=July 2009}} the unknown precisions could be found via [[analysis of variance]].\n\nIn short, total least squares does not have the property of units-invariance&mdash;i.e. it is not [[scale invariance|scale invariant]]. For a meaningful model we require this property to hold. A way forward is to realise that residuals (distances) measured in different units can be combined if multiplication is used instead of addition. Consider fitting a line: for each data point the product of the vertical and horizontal residuals equals twice the area of the triangle formed by the residual lines and the fitted line. We choose the line which minimizes the sum of these areas. Nobel laureate [[Paul Samuelson]] proved in 1942 that, in two dimensions, it is the only line expressible solely in terms of the ratios of standard deviations and the correlation coefficient which (1) fits the correct equation when the observations fall on a straight line, (2) exhibits scale invariance, and (3) exhibits invariance under interchange of variables.<ref>{{cite journal |last=Samuelson |first=Paul A. |year=1942 |title=A Note on Alternative Regressions |journal=Econometrica |doi=10.2307/1907024 |jstor=1907024 |volume=10 |issue=1 |pages=80–83}}</ref> This solution has been rediscovered in different disciplines and is variously known as '''standardised major axis''' (Ricker 1975, Warton et al., 2006),<ref>{{cite journal |last=Ricker |first=W. E. |year=1975 |title=A note concerning Professor Jolicoeur's Comments |journal=Journal of the Fisheries Research Board of Canada |doi=10.1139/f75-172 |volume=32 |issue=8 |pages=1494–1498}}</ref><ref>{{cite journal |last1=Warton |first1=David I. |last2=Wright |first2=Ian J. |last3=Falster |first3=Daniel S. |last4=Westoby |first4=Mark |year=2006 |title=Bivariate line-fitting methods for allometry |journal=Biological Reviews |doi=10.1017/S1464793106007007 |volume=81 |issue=2 |pages=259–291|citeseerx=10.1.1.461.9154 }}</ref> the '''reduced major axis''', the '''geometric mean functional relationship''' (Draper and Smith, 1998),<ref>Draper, NR and Smith, H. ''Applied Regression Analysis'', 3rd edition, pp. 92–96. 1998</ref> '''least products regression''', '''diagonal regression''', '''line of organic correlation''', and the '''least areas line''' (Tofallis, 2002).<ref>{{cite book |last=Tofallis |first=Chris |editor1-last=Van Huffel |editor1-first=Sabine |editor1-link= Sabine Van Huffel |editor2-last=Lemmerling |editor2-first=P. |year=2002 |title=Total Least Squares and Errors-in-Variables Modeling: Analysis, Algorithms and Applications |chapter=Model Fitting for Multiple Variables by Minimising the Geometric Mean Deviation |publisher=Kluwer Academic Publ. |location=Dordrecht |isbn=978-1402004766 |ssrn=1077322}}</ref> Tofallis (2015)<ref>{{ cite paper |last=Tofallis|first=Chris|year=2015|title=Fitting Equations to Data with the Perfect Correlation Relationship |ssrn=2707593}}</ref> has extended this approach to deal with multiple variables.\n\n== See also ==\n* [[Deming regression]], a special case with two predictors and independent errors.\n* [[Errors-in-variables model]]\n* [[Linear regression]]\n* [[Least squares]]\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{Reflist}}\n\n===Others===\n* I. Hnětynková, M. Plešinger, D. M. Sima, Z. Strakoš, and [[Sabine Van Huffel|S. Van Huffel]], ''The total least squares problem in AX ≈ B. A new classification with the relationship to the classical works.'' SIMAX vol. 32 issue 3 (2011), pp.&nbsp;748–770. Available  as a [ftp://ftp.sam.math.ethz.ch/pub/sam-reports/reports/reports2010/2010-38.pdf preprint].\n* M. Plešinger, ''The Total Least Squares Problem and Reduction of Data in AX ≈ B.'' Doctoral Thesis, TU of Liberec and Institute of Computer Science, AS CR Prague, 2008. [https://web.archive.org/web/20120724080908/http://www.fp.tul.cz/~plesinger/my_publications/doctoral_thesis/thesis.pdf Ph.D. Thesis]\n* C. C. Paige, Z. Strakoš, ''Core problems in linear algebraic systems.'' SIAM J. Matrix Anal. Appl. 27, 2006, pp.&nbsp;861–875. {{doi|10.1137/040616991}}\n* [[Sabine Van Huffel|S. Van Huffel]] and P. Lemmerling, ''Total Least Squares and Errors-in-Variables Modeling: Analysis,   Algorithms and Applications''. Dordrecht, The Netherlands: Kluwer Academic Publishers, 2002.\n* S. Jo and S. W. Kim, ''Consistent normalized least mean square filtering with noisy data matrix.'' IEEE Trans. Signal Process., vol. 53, no. 6, pp.&nbsp;2112–2123, Jun. 2005.\n* R. D. DeGroat and E. M. Dowling, ''The data least squares problem and channel equalization.'' IEEE Trans. Signal Process., vol. 41, no. 1, pp.&nbsp;407–411, Jan. 1993.\n* [[Sabine Van Huffel|S. Van Huffel]] and J. Vandewalle, ''The Total Least Squares Problems: Computational Aspects and Analysis.'' SIAM Publications, Philadelphia PA, 1991. {{doi|10.1137/1.9781611971002}}\n* T. Abatzoglou and J. Mendel, ''Constrained total least squares'', in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP’87), Apr. 1987, vol. 12, pp.&nbsp;1485–1488.\n* P. de Groen ''An introduction to total least squares'', in Nieuw Archief voor Wiskunde, Vierde serie, deel 14, 1996, pp.&nbsp;237–253 [https://arxiv.org/pdf/math.RA/9805076/ arxiv.org].\n* G. H. Golub and C. F. Van Loan, ''An analysis of the total least squares problem.'' SIAM J. on Numer. Anal., 17, 1980, pp.&nbsp;883–893. {{doi|10.1137/0717073}}\n* [http://www.mathpages.com/home/kmath110.htm Perpendicular Regression Of A Line] at MathPages\n* A. R. Amiri-Simkooei and S. Jazaeri ''Weighted total least squares formulated by standard least squares theory'',in Journal of Geodetic Science, 2 (2): 113–124, 2012 [http://engold.ui.ac.ir/~amiri/JGS_Amiri_Jazaeri_2012.pdf].\n\n{{Least Squares and Regression Analysis}}\n\n{{DEFAULTSORT:Total Least Squares}}\n[[Category:Applied mathematics]]\n[[Category:Least squares]]"
    },
    {
      "title": "Total sum of squares",
      "url": "https://en.wikipedia.org/wiki/Total_sum_of_squares",
      "text": "In [[statistics|statistical data analysis]] the '''total sum of squares''' (TSS or SST) is a quantity that appears as part of a standard way of presenting results of such analyses. It is defined as being the sum, over all observations, of the squared differences of each observation from the overall [[mean]].<ref>Everitt, B.S. (2002) ''The Cambridge Dictionary of Statistics'', CUP, {{ISBN|0-521-81099-X}}</ref>\n\nIn [[statistics|statistical]] [[linear model]]s, (particularly in standard [[regression model]]s), the '''TSS''' is the [[summation|sum]] of the [[square (algebra)|square]]s of the difference of the dependent variable and its [[mean]]:\n\n:<math>\\mathrm{TSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^2</math>\n\nwhere <math>\\bar{y}</math> is the mean.\n\nFor wide classes of linear models, the total sum of squares equals the [[explained sum of squares]] plus the [[residual sum of squares]]. For a proof of this in the multivariate OLS case, see [[Explained sum of squares#Partitioning in the general OLS model|partitioning in the general OLS model]].\n\nIn [[analysis of variance]] (ANOVA) the total sum of squares is the sum of the so-called \"within-samples\" sum of squares and \"between-samples\" sum of squares, i.e., partitioning of the sum of squares.\nIn [[multivariate analysis of variance]] (MANOVA) the following equation applies<ref name=\"MardiaK1979Multivariate\">{{Cite book\n | author = [[K. V. Mardia]], J. T. Kent and J. M. Bibby\n | title = Multivariate Analysis\n | publisher = [[Academic Press]]\n | year = 1979\n | isbn = 0-12-471252-5\n}} Especially chapters 11 and 12.</ref>\n:<math>\\mathbf{T} = \\mathbf{W} + \\mathbf{B},</math> \nwhere '''T''' is the total sum of squares and products (SSP) [[Matrix (mathematics)|matrix]], '''W''' is the within-samples SSP matrix and '''B''' is the between-samples SSP matrix.\nSimilar terminology may also be used in [[linear discriminant analysis]], where '''W''' and '''B''' are respectively referred to as the within-groups and between-groups SSP matrices.<ref name=\"MardiaK1979Multivariate\"/>\n\n==See also==\n*[[Sum of squares (statistics)]]\n*[[Lack-of-fit sum of squares]]\n\n==References==\n{{Reflist}}\n\n[[Category:Least squares]]"
    },
    {
      "title": "Weighted least squares",
      "url": "https://en.wikipedia.org/wiki/Weighted_least_squares",
      "text": "{{cleanup split|Least squares|Linear least squares (mathematics)|date=July 2018}}\n\n'''Weighted least squares''' ('''WLS'''), also known as '''weighted linear regression''',<ref>[https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/weighted-regression/]</ref><ref>[https://blogs.sas.com/content/iml/2016/10/05/weighted-regression.html]</ref> is a generalization of [[ordinary least squares]] and [[linear regression]] in which the errors [[covariance matrix]] is allowed to be different from an [[identity matrix]].\nWLS is also a specialization of [[generalized least squares]] in which the above matrix is [[diagonal matrix|diagonal]].\n\n==Introduction==\nA special case of [[generalized least squares]] called '''weighted least squares''' occurs when all the off-diagonal entries of ''Ω'' (the correlation matrix of the residuals) are null; the [[variance]]s of the observations (along the covariance matrix diagonal) may still be unequal ([[heteroscedasticity]]).\n\nThe expressions given above are based on the implicit assumption that the errors are uncorrelated with each other and with the independent variables and have equal variance. The [[Gauss–Markov theorem]] shows that, when this is so, <math>\\hat{\\boldsymbol{\\beta}}</math> is a [[best linear unbiased estimator]] (BLUE). If, however, the measurements are uncorrelated but have different uncertainties, a modified approach might be adopted. [[Alexander Aitken|Aitken]] showed that when a weighted sum of squared residuals is minimized, <math>\\hat{\\boldsymbol{\\beta}}</math> is  the [[Best linear unbiased estimator|BLUE]] if each weight is equal to the reciprocal of the variance of the measurement\n:<math> S = \\sum_{i=1}^{n} W_{ii}{r_i}^2,\\qquad W_{ii}=\\frac{1}{{\\sigma_i}^2} </math>\nThe gradient equations for this sum of squares are\n\n: <math>-2\\sum_i W_{ii}\\frac{\\partial f(x_i,\\boldsymbol {\\beta})}{\\partial \\beta_j} r_i = 0, \\qquad j=1,\\ldots,m</math>\n\nwhich, in a linear least squares system give the modified normal equations,\n\n: <math>\\sum_{i=1}^n \\sum_{k=1}^{m} X_{ij} W_{ii}X_{ik}\\hat{\\beta}_k=\\sum_{i=1}^n X_{ij} W_{ii}y_i, \\qquad j=1,\\ldots,m\\,.</math>\n\nWhen the observational errors are uncorrelated and the weight matrix, '''W''', is diagonal, these may be written as\n\n:<math>\\mathbf{\\left(X^TWX\\right)\\hat {\\boldsymbol {\\beta}}=X^TWy}.</math>\n\nIf the errors are correlated, the resulting estimator is the BLUE if the weight matrix is equal to the inverse of the [[variance-covariance matrix]] of the observations.\n\nWhen the errors are uncorrelated, it is convenient to simplify the calculations to factor the weight matrix as <math>w_{ii}=\\sqrt{W_{ii}}</math>.\nThe normal equations can then be written\nin the same form as ordinary least squares:\n\n:<math>\\mathbf{\\left(X'^TX'\\right)\\hat{\\boldsymbol{\\beta}}=X'^Ty'}\\,</math>\n\nwhere we define the following scaled matrix and vector:\n\n:<math>\\begin{align}\n\\mathbf{X'} &= \\operatorname{diag}\\left(\\mathbf{w}\\right) \\mathbf{X},\\\\\n\\mathbf{y'} &= \\operatorname{diag}\\left(\\mathbf{w}\\right) \\mathbf{y} = \\mathbf{y} \\oslash \\mathbf{\\sigma}.\\\\\n\\end{align}</math>\n\nThis is a type of [[whitening transformation]]; the last expression involves an [[entrywise division]].\n\nFor [[non-linear least squares]] systems a similar argument shows that the normal equations should be modified as follows.\n\n:<math>\\mathbf{(J^TWJ) \\, \\boldsymbol \\Delta \\beta=J^TW \\, \\boldsymbol\\Delta y}.\\,</math>\n\nNote that for empirical tests, the appropriate '''W''' is not known for sure and must be estimated.  For this [[feasible generalized least squares]] (FGLS) techniques may be used; in this case it is specialized for a diagonal covariance matrix, thus yielding a feasible weighted least squares solution.\n\nIf the uncertainty of the observations is not known from external sources, then the weights could be estimated from the given observations. This can be useful, for example, to identify outliers. After the outliers have been removed from the data set, the weights should be reset to one.<ref name=strutz>{{cite book|author=Strutz, T.| title=Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond) |publisher=Springer Vieweg | year=2016 | isbn= 978-3-658-11455-8}}, chapter 3</ref>\n\n==Motivation==\nIn some cases the observations may be weighted—for example, they may not be equally reliable. In this case, one can minimize the weighted sum of squares:\n\n:<math>\\underset{\\boldsymbol \\beta}{ \\operatorname{arg\\,min} }\\, \\sum_{i=1}^{m} w_i \\left|y_i - \\sum_{j=1}^{n} X_{ij}\\beta_j\\right|^2 = \\underset{\\boldsymbol \\beta}{ \\operatorname{arg\\,min} } \\, \\big\\|W^{1/2} (\\mathbf y - X \\boldsymbol \\beta) \\big\\|^2.</math>\n\nwhere ''w''<sub>''i''</sub> > 0 is the weight of the ''i''th observation, and ''W'' is the [[diagonal matrix]] of such weights.\n\nThe weights should, ideally, be equal to the [[multiplicative inverse|reciprocal]] of the [[variance]] of the measurement. (This implies that the observations are uncorrelated. If the observations are [[correlated]], the expression <math>\\textstyle S=\\sum_k \\sum_j r_k W_{kj} r_j\\,</math> applies. In this case the weight matrix should ideally be equal to the inverse of the [[variance-covariance matrix]] of the observations).<ref name=strutz/>\nThe normal equations are then:\n\n:<math>\\left(X^{\\rm T} W X \\right)\\hat{\\boldsymbol{\\beta}} = X^{\\rm T} W \\mathbf y.</math>\n\nThis method is used in [[iteratively reweighted least squares]].\n\n===Parameter errors and correlation{{anchor|Weighted parameter errors and correlation}}===\nThe estimated parameter values are linear combinations of the observed values\n\n:<math>\\hat{\\boldsymbol{\\beta}} = (X^{\\rm T} W X)^{-1} X^{\\rm T} W \\mathbf y. </math>\n\nTherefore, an expression for the estimated [[variance-covariance matrix]] of the parameter estimates can be obtained by [[error propagation]] from the errors in the observations. Let the variance-covariance matrix for the observations be denoted by ''M'' and that of the estimated parameters by ''M<sup>β</sup>''. Then\n\n:<math>M^\\beta = (X^{\\rm T} W X)^{-1} X^{\\rm T} W M W^{\\rm T} X (X^{\\rm T} W^{\\rm T} X)^{-1}.</math>\n<!-- Commented out: W is a diagonal matrix. so it is equal to its transpose {{Citation needed|date=August 2009|reason=Shouldn't that last inverted (X'*W*X) be transposed as well?}} -->\n\nWhen ''W'' = ''M''<sup>&minus;1</sup>, this simplifies to\n\n:<math>M^\\beta = (X^{\\rm T} W X)^{-1}.</math>\n\nWhen unit weights are used (''W'' = ''I'', the [[identity matrix]]), it is implied that the experimental errors are uncorrelated and all equal: ''M'' = ''σ''<sup>2</sup>''I'', where ''σ''<sup>2</sup> is the ''a priori'' variance of an observation.\nIn any case, ''σ''<sup>2</sup> is approximated by the [[reduced chi-squared]] <math>\\chi^2_\\nu</math>:\n:<math>M^\\beta = \\chi^2_\\nu(X^{\\rm T} X)^{-1},</math>\n:<math>\\chi^2_\\nu = S/\\nu,</math> \nwhere ''S'' is the minimum value of the (weighted) [[#Objective function|objective function]]:\n:<math>S = r^{\\rm T} W r.</math>\nThe denominator, <math>\\nu = n - m</math>, is the number of [[Degrees of freedom (statistics)|degrees of freedom]]; see [[Degrees of freedom (statistics)#Effective degrees of freedom|effective degrees of freedom]] for generalizations for the case of correlated observations.\n\nIn all cases, the [[variance]] of the parameter estimate <math>\\hat\\beta_i</math> is given by <math>M^\\beta_{ii}</math> and the [[covariance]] between the parameter estimates <math>\\hat\\beta_i</math> and <math>\\hat\\beta_j</math> is given by <math>M^\\beta_{ij}</math>. The [[standard deviation]] is the square root of variance, <math>\\sigma_i = \\sqrt{M^\\beta_{ii}}</math>, and the correlation coefficient is given by <math>\\rho_{ij} = M^\\beta_{ij}/(\\sigma_i \\sigma_j)</math>. These error estimates reflect only [[random errors]] in the measurements. The true uncertainty in the parameters is larger due to the presence of [[systematic errors]], which, by definition, cannot be quantified.\nNote that even though the observations may be uncorrelated, the parameters are typically [[Pearson product-moment correlation coefficient|correlated]].\n\n===Parameter confidence limits===\n{{Main article|Confidence interval}}\nIt is often ''assumed'', for want of any concrete evidence but often appealing to the [[central limit theorem]]—see [[Normal distribution#Occurrence]]—that the error on each observation belongs to a [[normal distribution]] with a mean of zero and standard deviation <math>\\sigma</math>. Under that assumption the following probabilities can be derived for a single scalar parameter estimate in terms of its estimated standard error <math>se_{\\beta}</math> (given [[Ordinary least squares#Large sample properties|here]]):\n:68% that the interval <math>\\hat \\beta \\pm se_{\\beta}</math> encompasses the true coefficient value\n:95% that the interval <math>\\hat \\beta \\pm 2se_{\\beta}</math> encompasses the true coefficient value\n:99% that the interval <math>\\hat \\beta \\pm 2.5se_{\\beta}</math> encompasses the true coefficient value\nThe assumption is not unreasonable when ''m''&nbsp;>>&nbsp;''n''. If the experimental errors are normally distributed the parameters will belong to a [[Student's t-distribution]] with ''m''&nbsp;&minus;&nbsp;''n'' [[Degrees of freedom (statistics)|degrees of freedom]]. When ''m''&nbsp;>>&nbsp;''n'' Student's t-distribution approximates a normal distribution. Note, however, that these confidence limits cannot take systematic error into account. Also, parameter errors should be quoted to one significant figure only, as they are subject to [[sampling error]].<ref>{{cite book |title=The Statistical Analysis of Experimental Data |last=Mandel |first=John |authorlink= |year=1964 |publisher=Interscience |location=New York |isbn= |pages= |url= }}</ref>\n\nWhen the number of observations is relatively small, [[Chebychev's inequality]] can be used for an upper bound on probabilities, regardless of any assumptions about the distribution of experimental errors: the maximum probabilities that a parameter will be more than 1, 2 or 3 standard deviations away from its expectation value are 100%, 25% and 11% respectively.\n\n=== Residual values and correlation ===\n\nThe [[errors and residuals in statistics|residuals]]  are related to the observations by\n\n:<math>\\mathbf{\\hat r} = \\mathbf y- X \\hat{\\boldsymbol{\\beta}} = \\mathbf y- H \\mathbf y =  (I - H) \\mathbf y,</math>\n\nwhere ''H'' is the [[idempotent matrix]] known as the [[hat matrix]]:\n\n:<math>H = X \\left(X^{\\rm T} W X \\right)^{-1} X^{\\rm T} W, </math>\n\nand ''I'' is the [[identity matrix]]. The variance-covariance matrix of the residuals, ''M'' <sup>'''r'''</sup> is given by\n\n:<math>M^\\mathbf{r} = (I - H) M (I - H)^{\\rm T}.</math>\n\nThus the residuals are correlated, even if the observations are not.\n\nWhen <math>W = M^{-1}</math>,\n\n:<math>M^\\mathbf{r} = (I - H) M.</math>\n\nThe sum of residual values is equal to zero whenever the model function contains a constant term. Left-multiply the expression for the residuals by ''X''<sup>T</sup>:\n\n:<math>X^{\\rm T} \\hat{\\mathbf r} = X^{\\rm T} \\mathbf y - X^{\\rm T} X \\hat{\\boldsymbol{\\beta}} = X^{\\rm T} \\mathbf y - (X^{\\rm T} X) (X^{\\rm T} X)^{-1} X^{\\rm T} \\mathbf y = \\mathbf 0.</math>\n\nSay, for example, that the first term of the model is a constant, so that <math>X_{i1} = 1</math> for all ''i''. In that case it follows that\n\n:<math>\\sum_i^m X_{i1} \\hat r_i = \\sum_i^m \\hat r_i = 0.</math>\n\nThus, in the motivational example, above, the fact that the sum of residual values is equal to zero is not accidental, but is a consequence of the presence of the constant term, α, in the model.\n\nIf experimental error follows a [[normal distribution]], then, because of the linear relationship between residuals and observations, so should residuals,<ref>{{cite book |title=Multivariate analysis |last=Mardia |first=K. V. |authorlink= |author2=Kent, J. T. |author3=Bibby, J. M.  |year=1979 |publisher=Academic Press |location=New York |isbn=0-12-471250-9 |pages= |url= }}</ref> but since the observations are only a sample of the population of all possible observations, the residuals should belong to a [[Student's t-distribution]]. [[Studentized residual]]s are useful in making a statistical test for an [[outlier]] when a particular residual appears to be excessively large.\n\n==See also==\n*[[Iteratively reweighted least squares]]\n*[[Heteroscedasticity-consistent standard errors]]\n*[[Weighted mean]]\n\n==References==\n{{reflist}}\n\n[[Category:Least squares]]"
    },
    {
      "title": "Linear programming",
      "url": "https://en.wikipedia.org/wiki/Linear_programming",
      "text": "[[File:Linear optimization in a 2-dimensional polytope.svg|thumb|A pictorial representation of a simple linear program with two variables and six inequalities. The set of feasible solutions is depicted in yellow and forms a [[polygon]], a 2-dimensional [[polytope]]. The linear cost function is represented by the red line and the arrow: The red line is a [[level set]] of the cost function, and the arrow indicates the direction in which we are optimizing.]]\n[[File:3dpoly.svg|thumb|right|A closed feasible region of a problem with three variables is a convex [[polyhedron]]. The surfaces giving a fixed value of the objective function are [[Plane (geometry)|planes]] (not shown). The linear programming problem is to find a point on the polyhedron that is on the plane with the highest possible value.]]\n\n'''Linear programming''' ('''LP''', also called '''linear optimization''') is a method to achieve the best outcome (such as maximum profit or lowest cost) in a [[mathematical model]] whose requirements are represented by [[linear function#As a polynomial function|linear relationships]]. Linear programming is a special case of mathematical programming (also known as [[mathematical optimization]]).\n\nMore formally, linear programming is a technique for the [[mathematical optimization|optimization]] of a [[linear]] [[objective function]], subject to [[linear equality]] and [[linear inequality]] [[Constraint (mathematics)|constraints]]. Its [[feasible region]] is a [[convex polytope]], which is a set defined as the [[intersection (mathematics)|intersection]] of finitely many [[Half-space (geometry)|half spaces]], each of which is defined by a linear inequality<!-- ;  alternatively, a convex polytope is the [[Minkowski sum]] of a [[convex polytope]] and a convex [[polyhedral cone]] -->. Its objective function is a [[real number|real]]-valued [[affine function|affine (linear) function]] defined on this polyhedron. A linear programming [[algorithm]] finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.\n\nLinear programs are problems that can be expressed in [[canonical form]] as\n:<math> \\begin{align}\n& \\text{Maximize}   && \\mathbf{c}^\\mathrm{T} \\mathbf{x}\\\\\n& \\text{subject to} && A \\mathbf{x} \\leq \\mathbf{b} \\\\\n& \\text{and} && \\mathbf{x} \\ge \\mathbf{0}\n\\end{align} </math>\nwhere '''x''' represents the vector of variables (to be determined), '''c''' and '''b''' are [[vector space|vectors]] of (known) coefficients, ''A'' is a (known) [[Matrix (mathematics)|matrix]] of coefficients, and <math>(\\cdot)^\\mathrm{T}</math> is the [[matrix transpose]]. The expression to be maximized or minimized is called the [[objective function]] ('''c'''<sup>T</sup>'''x''' in this case). The inequalities ''A'''''x'''&nbsp;≤&nbsp;'''b''' and '''x''' ≥ '''0''' are the constraints which specify a [[convex polytope]] over which the objective function is to be optimized. In this context, two vectors are [[Comparability|comparable]] when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector.\n\nLinear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, [[economics]], and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in [[automated planning and scheduling|planning]], [[routing]], [[scheduling (production processes)|scheduling]], [[assignment problem|assignment]], and design.\n\n== History ==\n[[File:Leonid Kantorovich 1975.jpg|thumb|[[Leonid Kantorovich]]]]\n[[File:JohnvonNeumann-LosAlamos.gif|thumb|[[John von Neumann]]]]\n\nThe problem of solving a system of linear inequalities dates back at least as far as [[Joseph Fourier|Fourier]], who in 1827 published a method for solving them,<ref name=\"SierksmaZwols2015\">{{cite book|author1=Gerard Sierksma|author2=Yori Zwols|title=Linear and Integer Optimization: Theory and Practice, Third Edition|year=2015|publisher=CRC Press|isbn=9781498710169|page=1}}</ref> and after whom the method of [[Fourier–Motzkin elimination]] is named.\n\nIn 1939 a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the [[Soviet Union|Soviet]] [[economist]] [[Leonid Kantorovich]], who also proposed a method for solving it.<ref name=\"Schrijver1998\">{{cite book|author=Alexander Schrijver|title=Theory of Linear and Integer Programming|year=1998|publisher=John Wiley & Sons|isbn=978-0-471-98232-6|pages=221–222}}</ref> It is a way he developed, during [[World War II]], to plan expenditures and returns in order to reduce costs of the army and to increase losses imposed on the enemy.{{Citation needed|date=August 2017}} Kantorovich's work was initially neglected in the [[USSR]].<ref name=\"dantzig1982\">{{cite journal|url = http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&doc=GetTRDoc.pdf&AD=ADA112060|title = Reminiscences about the origins of linear programming|author=George B. Dantzig|date = April 1982|journal = Operations Research Letters|volume = 1|issue = 2|pages = 43–48|doi = 10.1016/0167-6377(82)90043-8}}</ref> About the same time as Kantorovich, the Dutch-American economist [[Tjalling Koopmans|T. C. Koopmans]] formulated classical economic problems as linear programs. Kantorovich and Koopmans later shared the 1975 [[Nobel prize in economics]].<ref name=\"SierksmaZwols2015\" /> In 1941, [[Frank Lauren Hitchcock]] also formulated transportation  problems as linear programs and gave a solution very similar to the later [[simplex method]].<ref name=\"Schrijver1998\" /> Hitchcock had died in 1957 and the Nobel prize is not awarded posthumously.\n\nDuring 1946–1947, [[George Dantzig|George B. Dantzig]] independently developed general linear programming formulation to use for planning problems in US Air Force<ref name=\":0\">{{Cite book|url=https://www.worldcat.org/oclc/35318475|title=Linear programming|last=Dantzig, George B. (George Bernard), 1914-2005.|first=|date=©1997-2003|publisher=Springer|others=Thapa, Mukund Narain.|year=|isbn=0387948333|location=New York|pages=xxvii|oclc=35318475}}</ref>. In 1947, Dantzig also invented the [[Simplex algorithm|simplex method]] that for the first time efficiently tackled the linear programming problem in most cases<ref name=\":0\" />. When Dantzig arranged a meeting with [[John von Neumann]] to discuss his simplex method, Neumann immediately conjectured the theory of [[#Duality|duality]] by realizing that the problem he had been working in [[game theory]] was equivalent<ref name=\":0\" />. Dantzig provided formal proof in an unpublished report \"A Theorem on Linear Inequalities\" on January 5, 1948.<ref name=\"dantzig1982\"/> In the post-war years, many industries applied it in their daily planning.\n\nDantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the [[Abundance of the chemical elements|number of particles]] in the [[observable universe]]. However, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the [[simplex algorithm]]. The theory behind linear programming drastically reduces the number of possible solutions that must be checked.\n\nThe linear programming problem was first shown to be solvable in polynomial time by [[Leonid Khachiyan]] in 1979,<ref>{{cite journal|title = A Polynomial Algorithm for Linear Programming|author = Leonid Khachiyan|date = 1979|journal = Doklady Akademii Nauk SSSR|volume=224|issue=5|pages=1093–1096}}</ref> but a larger theoretical and practical breakthrough in the field came in 1984 when [[Narendra Karmarkar]] introduced a new [[interior-point method]] for solving linear-programming problems.<ref>{{cite journal|title = A New Polynomial-Time Algorithm for Linear Programming|author = Narendra Karmarkar|date = 1984|journal = Combinatorica|volume=4|issue = 4|pages=373–395|doi = 10.1007/BF02579150}}</ref>\n\n== Uses ==\nLinear programming is a widely used field of optimization for several reasons. Many practical problems in [[operations research]] can be expressed as linear programming problems.<ref name=\"dantzig1982\"/> Certain special cases of linear programming, such as ''[[network flow problem|network flow]]'' problems and [[multi-commodity flow problem|''multicommodity flow'' problems]] are considered important enough to have generated much research on specialized algorithms for their solution. A number of algorithms for other types of optimization problems work by solving LP problems as sub-problems. Historically, ideas from linear programming have inspired many of the central concepts of optimization theory, such as ''duality,'' ''decomposition,'' and the importance of ''convexity'' and its generalizations. Likewise, linear programming was heavily used in the early formation of [[microeconomics]] and it is currently utilized in company management, such as planning, production, transportation, technology and other issues. Although the modern management issues are ever-changing, most companies would like to [[profit maximization|maximize profits]] and minimize costs with limited resources. Therefore, many issues can be characterized as linear programming problems.\n\n== Standard form ==\n''Standard form'' is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:\n* A '''linear function to be maximized'''\n: e.g. <math> f(x_{1},x_{2}) = c_1 x_1 + c_2 x_2</math>\n* '''Problem constraints''' of the following form\n: e.g.\n:: <math>\\begin{matrix}\n  a_{11} x_1 + a_{12} x_2 &\\leq b_1 \\\\\n  a_{21} x_1 + a_{22} x_2 &\\leq b_2 \\\\\n  a_{31} x_1 + a_{32} x_2 &\\leq b_3 \\\\\n\\end{matrix}</math>\n* '''Non-negative variables'''\n: e.g.\n:: <math>\\begin{matrix}\n x_1 \\geq 0 \\\\\n x_2 \\geq 0\n\\end{matrix}</math>\n\nThe problem is usually expressed in ''[[Matrix (mathematics)|matrix]] form'', and then becomes:\n: <math>\\max \\{ \\mathbf{c}^\\mathrm{T} \\mathbf{x} \\;|\\; A \\mathbf{x} \\leq \\mathbf{b} \\land \\mathbf{x} \\geq 0 \\}</math>\n\nOther forms, such as minimization problems, problems with constraints on alternative forms, as well as problems involving negative [[variable (programming)|variables]] can always be rewritten into an equivalent problem in standard form.\n\n=== Example ===\nSuppose that a farmer has a piece of farm land, say ''L'' km<sup>2</sup>, to be planted with either wheat or barley or some combination of the two. The farmer has a limited amount of fertilizer, ''F'' kilograms, and pesticide, ''P'' kilograms. Every square kilometer of wheat requires ''F''<sub>1</sub> kilograms of fertilizer and ''P''<sub>1</sub> kilograms of pesticide, while every square kilometer of barley requires ''F''<sub>2</sub> kilograms of fertilizer and ''P''<sub>2</sub> kilograms of pesticide. Let S<sub>1</sub> be the selling price of wheat per square kilometer, and S<sub>2</sub> be the selling price of barley. If we denote the area of land planted with wheat and barley by ''x''<sub>1</sub> and ''x''<sub>2</sub> respectively, then profit can be maximized by choosing optimal values for ''x''<sub>1</sub> and ''x''<sub>2</sub>. This problem can be expressed with the following linear programming problem in the standard form:\n{|\n|-\n| colspan=\"2\" | Maximize: <math>S_1\\cdot x_1+S_2\\cdot x_2</math>\n| (maximize the revenue—revenue is the \"objective function\")\n|-\n| Subject to:\n| <math>x_1 + x_2\\leq L</math>\n| (limit on total area)\n|-\n|\n| <math>F_1\\cdot x_1+F_2\\cdot x_2\\leq F</math>\n| (limit on fertilizer)\n|-\n|\n| <math>P_1\\cdot x_1 + P_2\\cdot x_2\\leq P</math>\n| (limit on pesticide)\n|-\n|\n| <math>x_1\\geq 0, x_2\\geq 0</math>\n| (cannot plant a negative area).\n|}\n\nIn matrix form this becomes:\n: maximize <math>\\begin{bmatrix} S_1 & S_2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} </math>\n: subject to <math>\\begin{bmatrix} 1 & 1 \\\\ F_1 & F_2 \\\\ P_1 & P_2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} L \\\\ F \\\\ P \\end{bmatrix}, \\, \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\ge \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}. </math>\n\n== Augmented form (slack form) ==\nLinear programming problems can be converted into an ''augmented form'' in order to apply the common form of the [[simplex algorithm]]. This form introduces non-negative ''[[slack variable]]s'' to replace inequalities with equalities in the constraints. The problems can then be written in the following [[block matrix]] form:\n: Maximize <math>z</math>:\n: <math>\n  \\begin{bmatrix}\n    1 & -\\mathbf{c}^T & 0 \\\\\n    0 & \\mathbf{A} & \\mathbf{I}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    z \\\\ \\mathbf{x} \\\\ \\mathbf{s}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\ \\mathbf{b}\n  \\end{bmatrix}\n</math>\n:<math>\\mathbf{x} \\ge  0, \\mathbf{s} \\ge 0</math>\nwhere <math>\\mathbf{s}</math> are the newly introduced slack variables, <math>\\mathbf{x}</math> are the decision variables, and <math>z</math> is the variable to be maximized.\n\n=== Example ===\nThe example above is converted into the following augmented form:\n:{|\n|-\n| colspan=\"2\" | Maximize: <math>S_1\\cdot x_1+S_2\\cdot x_2</math>\n| (objective function)\n|-\n| subject to:\n| <math>x_1 + x_2 + x_3 = L</math>\n| (augmented constraint)\n|-\n|\n| <math>F_1\\cdot x_1+F_2\\cdot x_2 + x_4 = F</math>\n| (augmented constraint)\n|-\n|\n| <math>P_1\\cdot x_1 + P_2\\cdot x_2 + x_5 = P</math>\n| (augmented constraint)\n|-\n|\n| <math>x_1,x_2,x_3,x_4,x_5 \\ge 0.</math>\n|}\nwhere <math>x_3, x_4, x_5</math> are (non-negative) slack variables, representing in this example the unused area, the amount of unused fertilizer, and the amount of unused pesticide.\n\nIn matrix form this becomes:\n: Maximize <math>z</math>:\n: <math>\n  \\begin{bmatrix}\n    1 & -S_1 & -S_2 & 0 & 0 & 0 \\\\\n    0 &   1    &   1    & 1 & 0 & 0 \\\\\n    0 &  F_1  &  F_2  & 0 & 1 & 0 \\\\\n    0 &  P_1    & P_2 & 0 & 0 & 1 \\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    z \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\ L \\\\ F \\\\ P\n  \\end{bmatrix}, \\,\n  \\begin{bmatrix}\n    x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5\n  \\end{bmatrix} \\ge 0.\n</math>\n\n== Duality ==\n{{Main|Dual linear program}}\nEvery linear programming problem, referred to as a ''primal'' problem, can be converted into a [[dual problem]], which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the ''primal'' problem as:\n\n: Maximize '''c'''<sup>T</sup>'''x'''  subject to ''A'''''x''' ≤ '''b''', '''x''' ≥ 0;\n:: with the corresponding '''symmetric''' dual problem,\n: Minimize  '''b'''<sup>T</sup>'''y'''  subject to ''A''<sup>T</sup>'''y''' ≥ '''c''', '''y''' ≥ 0.\n\nAn alternative primal formulation is:\n\n: Maximize '''c'''<sup>T</sup>'''x''' subject to ''A'''''x''' ≤ '''b''';\n:: with the corresponding '''asymmetric''' dual problem,\n: Minimize  '''b'''<sup>T</sup>'''y''' subject to ''A''<sup>T</sup>'''y''' = '''c''', '''y''' ≥ 0.\n\nThere are two ideas fundamental to duality theory. One is the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program. Additionally, every feasible solution for a linear program gives a bound on the optimal value of the objective function of its dual.  The [[weak duality]] theorem states that the objective function value of the dual at any feasible solution is always greater than or equal to the objective function value of the primal at any feasible solution. The [[strong duality]] theorem states that if the primal has an optimal solution, '''x'''<sup>*</sup>, then the dual also has an optimal solution, '''y'''<sup>*</sup>, and '''c'''<sup>T</sup>'''x'''<sup>*</sup>='''b'''<sup>T</sup>'''y'''<sup>*</sup>.\n\nA linear program can also be unbounded or infeasible. Duality theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem. Likewise, if the dual is unbounded, then the primal must be infeasible. However, it is possible for both the dual and the primal to be infeasible.  See [[dual linear program]] for details and several more examples.\n\n== Variations ==\n\n=== Covering/packing dualities ===\n<!--Linked from [[Template:Covering/packing-problem pairs]]-->\n{{Covering/packing-problem pairs}}\n\nA [[Covering problem|covering LP]] is a linear program of the form:\n: Minimize:  <big>'''b'''<sup>T</sup>'''y'''</big>,\n: subject to: <big>''A''<sup>T</sup>'''y''' ≥ '''c''', '''y''' ≥ 0</big>,\nsuch that the matrix ''A'' and the vectors '''b''' and '''c''' are non-negative.\n\nThe dual of a covering LP is a [[Packing problem|packing LP]], a linear program of the form:\n: Maximize: <big>'''c'''<sup>T</sup>'''x'''</big>,\n: subject to: <big>''A'''''x''' ≤ '''b''', '''x''' ≥ 0</big>,\nsuch that the matrix ''A'' and the vectors '''b''' and '''c''' are non-negative.\n\n==== Examples ====\nCovering and packing LPs commonly arise as a [[linear programming relaxation]] of a combinatorial problem and are important in the study of [[approximation algorithms]].<ref>{{harvtxt|Vazirani|2001|p=112}}</ref> For example, the LP relaxations of the [[Set packing|set packing problem]], the [[independent set problem]], and the [[Matching (graph theory)|matching problem]] are packing LPs. The LP relaxations of the [[set cover problem]], the [[vertex cover problem]], and the [[dominating set problem]] are also covering LPs.\n\nFinding a [[fractional coloring]] of a [[Graph (discrete mathematics)|graph]] is another example of a covering LP. In this case, there is one constraint for each vertex of the graph and one variable for each [[Independent set (graph theory)|independent set]] of the graph.\n\n== Complementary slackness ==\nIt is possible to obtain an optimal solution to the dual when only an optimal solution to the primal is known using the complementary slackness theorem. The theorem states:\n\nSuppose that '''x'''&nbsp;=&nbsp;('''x'''<sub>1</sub>,&nbsp;'''x'''<sub>2</sub>,&nbsp;...&nbsp;,&nbsp;'''x'''<sub>''n''</sub>) is primal feasible and that '''y'''&nbsp;=&nbsp;('''y'''<sub>1</sub>,&nbsp;'''y'''<sub>2</sub>,&nbsp;...&nbsp;,&nbsp;'''y'''<sub>''m''</sub>) is dual feasible. Let ('''w'''<sub>1</sub>,&nbsp;'''w'''<sub>2</sub>,&nbsp;...,&nbsp;'''w'''<sub>''m''</sub>) denote the corresponding primal slack variables, and let ('''z'''<sub>1</sub>,&nbsp;'''z'''<sub>2</sub>,&nbsp;...&nbsp;,&nbsp;'''z'''<sub>''n''</sub>) denote the corresponding dual slack variables. Then '''x''' and '''y''' are optimal for their respective problems if and only if\n* '''x'''<sub>''j''</sub> '''z'''<sub>''j''</sub>&nbsp;=&nbsp;0, for ''j''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;...&nbsp;,&nbsp;''n'', and\n* '''w'''<sub>''i''</sub> '''y'''<sub>''i''</sub>&nbsp;=&nbsp;0, for ''i''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;...&nbsp;,&nbsp;''m''.\n\nSo if the ''i''-th slack variable of the primal is not zero, then the ''i''-th variable of the dual is equal to zero. Likewise, if the ''j''-th slack variable of the dual is not zero, then the ''j''-th variable of the primal is equal to zero.\n\nThis necessary condition for optimality conveys a fairly simple economic principle.  In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are \"leftovers\"), then additional quantities of that resource must have no value.  Likewise, if there is slack in the dual (shadow) price non-negativity constraint requirement, i.e., the price is not zero, then there must be scarce supplies (no \"leftovers\").\n\n== Theory ==\n\n=== Existence of optimal solutions ===\nGeometrically, the linear constraints define the [[feasible region]], which is a [[convex set|convex]] [[polyhedron]]. A [[linear functional|linear function]] is a [[convex function]], which implies that every [[local minimum]] is a [[global minimum]]; similarly, a linear function is a [[concave function]], which implies that every [[local maximum]] is a [[global maximum]].\n\nAn optimal solution need not exist, for two reasons. First, if two constraints are inconsistent, then no feasible solution exists: For instance, the constraints '''x'''&nbsp;≥&nbsp;2 and '''x'''&nbsp;≤&nbsp;1 cannot be satisfied jointly; in this case, we say that the LP is ''infeasible''. Second, when the [[polytope]] is unbounded in the direction of the gradient of the objective function (where the gradient of the objective function is the vector of the coefficients of the objective function), then no optimal value is attained because it is always possible to do better than any finite value of the objective function.\n\n=== Optimal vertices (and rays) of polyhedra ===\nOtherwise, if a feasible solution exists and if the constraint set is bounded, then the optimum value is always attained on the boundary of the constraint set, by the ''[[maximum principle]]'' for ''[[convex function]]s'' (alternatively, by the ''minimum'' principle for ''[[concave function]]s'') since linear functions are both convex and concave. However, some problems have distinct optimal solutions; for example, the problem of finding a feasible solution to a system of linear inequalities is a linear programming problem in which the objective function is the zero function (that is, the constant function taking the value zero everywhere). For this feasibility problem with the zero-function for its objective-function, if there are two distinct solutions, then every convex combination of the solutions is a solution.\n\nThe vertices of the polytope are also called ''basic feasible solutions''. The reason for this choice of name is as follows. Let ''d'' denote the number of variables. Then the fundamental theorem of linear inequalities implies (for feasible problems) that for every vertex '''x'''<sup>*</sup> of the LP feasible region, there exists a set of ''d'' (or fewer) inequality constraints from the LP such that, when we treat those ''d'' constraints as equalities, the unique solution is '''x'''<sup>*</sup>. Thereby we can study these vertices by means of looking at certain subsets of the set of all constraints (a discrete set), rather than the continuum of LP solutions. This principle underlies the [[simplex algorithm]] for solving linear programs.\n\n== Algorithms ==\n{{See also|List of numerical analysis topics#Linear programming}}\n\n[[File:Linear Programming Feasible Region.svg|frame|In a linear programming problem, a series of linear constraints produces a [[Convex set|convex]] [[feasible region]] of possible values for those variables. In the two-variable case this region is in the shape of a convex [[simple polygon]].]]\n\n=== Basis exchange algorithms ===\n\n==== Simplex algorithm of Dantzig ====\n\nThe [[simplex algorithm]], developed by [[George Dantzig]] in 1947, solves LP problems by constructing a feasible solution at a vertex of the [[polytope]] and then walking along a path on the edges of the polytope to vertices with non-decreasing values of the objective function until an optimum is reached for sure. In many practical problems, \"[[Simplex algorithm#Degeneracy: stalling and cycling|stalling]]\" occurs: many pivots are made with no increase in the objective function.<ref name=\"DT03\">{{harvtxt|Dantzig|Thapa|2003}}</ref><ref name=\"Padberg\">{{harvtxt|Padberg|1999}}</ref> In rare practical problems, the usual versions of the simplex algorithm may actually \"cycle\".<ref name=\"Padberg\" /> To avoid cycles, researchers developed new pivoting rules.<ref name=\"Bland\">{{harvtxt|Bland|1977}}</ref><ref name=\"Murty\">{{harvtxt|Murty|1983}}</ref><ref name=\" DT03\" /><ref name=\" Padberg\" /><ref name=\"PS\">{{harvtxt|Papadimitriou|Steiglitz|}}</ref><ref name=\"FukudaTerlaky\" />\n\nIn practice, the simplex [[algorithm]] is quite efficient and can be guaranteed to find the global optimum if certain precautions against ''cycling'' are taken. The simplex algorithm has been proved to solve \"random\" problems efficiently, i.e. in a cubic number of steps,<ref>{{harvtxt|Borgwardt|1987}}</ref> which is similar to its behavior on practical problems.<ref name=\"DT03\" /><ref name=\"Todd\">{{harvtxt|Todd|2002}}</ref>\n\nHowever, the simplex algorithm has poor worst-case behavior: Klee and Minty constructed a family of linear programming problems for which the simplex method takes a number of steps exponential in the problem size.<ref name=\"DT03\" /><ref name=\"Murty\" /><ref name=\"PS \" /> In fact, for some time it was not known whether the linear programming problem was solvable in [[polynomial time]], i.e. of [[P (complexity)|complexity class P]].\n\n==== Criss-cross algorithm ====\nLike the simplex algorithm of Dantzig, the [[criss-cross algorithm]] is a basis-exchange algorithm that pivots between bases. However, the criss-cross algorithm need not maintain feasibility, but can pivot rather from a feasible basis to an infeasible basis. The criss-cross algorithm does not have [[time complexity|polynomial time-complexity]] for linear programming. Both algorithms visit all&nbsp;2<sup>''D''</sup>&nbsp;corners of a (perturbed) [[unit cube|cube]] in dimension&nbsp;''D'', the [[Klee–Minty cube]], in the [[worst-case complexity|worst case]].<ref name=\"FukudaTerlaky\">{{harvtxt|Fukuda|Terlaky|1997}}: {{cite journal|first1=Komei|last1=Fukuda|first2=Tamás|last2=Terlaky|title=Criss-cross methods: A fresh view on pivot algorithms |journal=Mathematical Programming, Series B|volume=79|number=1—3|pages=369–395|editors=Thomas&nbsp;M. Liebling and Dominique de&nbsp;Werra|year=1997|doi=10.1007/BF02614325|mr=1464775|citeseerx=10.1.1.36.9373}}</ref><ref name=\"Roos\">{{harvtxt|Roos|1990}}: {{cite journal|last=Roos|first=C.|title=An exponential example for Terlaky's pivoting rule for the criss-cross simplex method|journal=Mathematical Programming|volume=46|year=1990|series=Series&nbsp;A|doi=10.1007/BF01585729|mr=1045573|ref=harv|issue=1|pages=79–84}}</ref>\n\n=== Interior point ===\nIn contrast to the simplex algorithm, which finds an optimal solution by traversing the edges between vertices on a polyhedral set, interior-point methods move through the interior of the feasible region.\n\n==== Ellipsoid algorithm, following Khachiyan ====\nThis is the first [[worst-case complexity|worst-case]] [[polynomial-time]] algorithm ever found for linear programming.  To solve a problem which has ''n'' variables and can be encoded in ''L'' input bits, this algorithm uses ''O(n<sup>4</sup>L)'' pseudo-arithmetic operations on numbers with ''O(L)'' digits. [[Leonid Khachiyan]] solved this long-standing complexity issue in 1979 with the introduction of the [[ellipsoid method]]. The convergence analysis has (real-number) predecessors, notably the [[iterative method]]s developed by [[Naum Z. Shor]] and the [[approximation algorithm]]s by Arkadi Nemirovski and D. Yudin.\n\n==== Projective algorithm of Karmarkar ====\n{{main|Karmarkar's algorithm}}\nKhachiyan's algorithm was of landmark importance for establishing the polynomial-time solvability of linear programs.  The algorithm was not a computational break-through, as the simplex method is more efficient for all but specially constructed families of linear programs.\n\nHowever, Khachiyan's algorithm inspired new lines of research in linear programming. In 1984, [[Narendra Karmarkar|N. Karmarkar]] proposed a<!-- n interior-point --> [[projective method]] for linear programming.  Karmarkar's algorithm improved on Khachiyan's worst-case polynomial bound (giving <math>O(n^{3.5}L)</math>). Karmarkar claimed that his algorithm was much faster in practical LP than the simplex method, a claim that created great interest in interior-point methods.<ref name=\"Strang\">{{cite journal|last=Strang|first=Gilbert|authorlink=Gilbert Strang|title=Karmarkar's algorithm and its place in applied mathematics|journal=[[The Mathematical Intelligencer]]|date=1 June 1987|issn=0343-6993|pages=4–10|volume=9|doi=10.1007/BF03025891|mr=883185|ref=harv|issue=2}}</ref> Since Karmarkar's discovery, many interior-point methods have been proposed and analyzed.\n\n==== Affine scaling ====\n{{main|Affine scaling}}\n\nAffine scaling is one of the oldest interior point methods to be developed. It was developed in the Soviet Union in the mid-1960s, but didn't receive much attention until the discovery of Karmarkar's algorithm, after which affine scaling was [[multiple discovery|reinvented multiple times]] and presented as a simplified version of Karmarkar's. Affine scaling amounts to doing [[gradient descent]] steps within the feasible region, while rescaling the problem to make sure the steps move toward the optimum faster.<ref>{{harvtxt|Vanderbei|2001|pages=333–347}}</ref>\n\n==== Vaidya's algorithm ====\nIn 1989, Vaidya developed an algorithm that runs in <math>O(n^{2.5})</math> time.<ref>{{cite conference|title= Speeding-up linear programming using fast matrix multiplication | conference = 30th Annual Symposium on Foun- dations of Computer Science (FOCS'89) |url=https://ieeexplore.ieee.org/document/63499|last1=Vaidya|first1=Pravin M. |year=1989}}</ref> Formally speaking, the algorithm takes <math>O( (n+d)^{1.5} n L)</math> arithmetic operations in the worst case, where <math>d</math> is the number of constraints, <math> n </math> is the number of variables, and <math>L</math> is the number of bits.\n\n==== Path-following algorithms ====\nFor both theoretical and practical purposes, [[barrier function]] or [[path-following]] methods have been the most popular interior point methods since the 1990s.<ref name=\"GondzioTerlaky\">{{harvtxt|Gondzio|Terlaky|1996}}</ref> In 2015, Lee and Sidford showed that, it can be solved in <math>O((nnz(A) + n^2)\\sqrt{n})</math> time,<ref>{{cite conference|title= Efficient inverse maintenance and faster algorithms for linear programming | conference = FOCS '15 Foundations of Computer Science |last1=Lee|first1=Yin-Tat|last2=Sidford|first2=Aaron |year=2015| arxiv = 1503.01752 }}</ref> and it remains taking <math>O(n^{2.5})</math> in the worst case. In 2018, Cohen, Lee and Song improved the running time to <math>O(n^{\\omega})</math> time, <math> \\omega </math> is the exponent of [[matrix multiplication]].<ref>{{cite conference|title= Solving Linear Programs in the Current Matrix Multiplication Time | conference =  |last1=Cohen|first1=Michael B.|last2=Lee|first2=Yin-Tat|last3=Song|first3=Zhao |year=2018| arxiv =  1810.07896}}</ref>\n\n=== Comparison of interior-point methods and simplex algorithms ===\n\nThe current opinion is that the efficiencies of good implementations of simplex-based methods and interior point methods are similar for routine applications of linear programming.<ref name=\"GondzioTerlaky\" />  However, for specific types of LP problems, it may be that one type of solver is better than another (sometimes much better), and that the structure of the solutions generated by interior point methods versus simplex-based methods are significantly different with the support set of active variables being typically smaller for the later one.<ref>{{cite journal|doi=10.1016/S0377-2217(02)00061-9|title=Pivot versus interior point methods: Pros and cons|journal=European Journal of Operational Research|volume=140|issue=2|pages=170|year=2002|last1=Illés|first1=Tibor|last2=Terlaky|first2=Tamás|url=https://strathprints.strath.ac.uk/9200/|citeseerx=10.1.1.646.3539}}</ref>\n\n=== Approximate algorithms for covering/packing LPs ===\nCovering and packing LPs can be solved [[Approximation algorithm|approximately]] in nearly-linear time. That is, if matrix {{mvar|A}} is of dimension {{math|''n''×''m''}} and has {{mvar|N}} non-zero entries, then there exist algorithms that run in time {{math|''O''(''N''·(log ''N'')<sup>''O''(1)</sup>/''ε''<sup>''O''(1)</sup>)}} and produce {{math|''O''(1±''ε'')}} approximate solutions to given covering and packing LPs. The best known sequential algorithm of this kind runs in time {{math|''O''(''N'' + (log ''N'')·(''n''+''m'')/''ε''<sup>2</sup>)}},<ref>{{cite journal |author1=Christos Koufogiannakis |author2=Neal E. Young  |title=A Nearly Linear-Time PTAS for Explicit Fractional Packing and Covering Linear Programs |journal=Algorithmica |year=2013 |arxiv=0801.1987|doi=10.1007/s00453-013-9771-6 |volume=70 |issue=4  |pages=648–674}}</ref> and the best known parallel algorithm of this kind runs in {{math|''O''((log ''N'')<sup>2</sup>/''ε''<sup>3</sup>)}} iterations, each requiring only a matrix-vector multiplication which is highly parallelizable.<ref>{{cite conference |author1=Zeyuan Allen-Zhu |author2=Lorenzo Orecchia  |title=Using Optimization to Break the Epsilon Barrier: A Faster and Simpler Width-Independent Algorithm for Solving Positive Linear Programs in Parallel |conference=ACM-SIAM Symposium on Discrete Algorithms |year=2015 |arxiv=1407.1925|bibcode=2014arXiv1407.1925A}}</ref>\n\n== Open problems and recent work ==\n{{unsolved|computer science|Does linear programming admit a strongly polynomial-time algorithm?}}\nThere are several open problems in the theory of linear programming, the solution of which would represent fundamental breakthroughs in mathematics and potentially major advances in our ability to solve large-scale linear programs.\n* Does LP admit a [[Time complexity#Strongly and weakly polynomial time|strongly polynomial]]-time algorithm?\n* Does LP admit a strongly polynomial-time algorithm to find a strictly complementary solution?\n* Does LP admit a polynomial-time algorithm in the real number (unit cost) model of computation?\n\nThis closely related set of problems has been cited by [[Stephen Smale]] as among the [[Smale's problems|18 greatest unsolved problems]] of the 21st century.  In Smale's words, the third version of the problem \"is the main unsolved problem of linear programming theory.\"  While algorithms exist to solve linear programming in weakly polynomial time, such as the [[ellipsoid method]]s and [[interior point method|interior-point techniques]], no algorithms have yet been found that allow strongly polynomial-time performance in the number of constraints and the number of variables.  The development of such algorithms would be of great theoretical interest, and perhaps allow practical gains in solving large LPs as well.\n\nAlthough the [[Hirsch conjecture]] was recently disproved for higher dimensions, it still leaves the following questions open.\n* Are there pivot rules which lead to polynomial-time simplex variants?\n* Do all polytopal graphs have polynomially bounded diameter?\n\nThese questions relate to the performance analysis and development of simplex-like methods.  The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time.  It would be of great practical and theoretical significance to know whether any such variants exist, particularly as an approach to deciding if LP can be solved in strongly polynomial time.\n\nThe simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope.  This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope.  As a result, we are interested in knowing the maximum [[Graph diameter|graph-theoretical diameter]] of polytopal [[Graph (discrete mathematics)|graphs]].  It has been proved that all polytopes have subexponential diameter. The recent disproof of the Hirsch conjecture is the first step to prove whether any polytope has superpolynomial diameter. If any such polytopes exist, then no edge-following variant can run in polynomial time. Questions about polytope diameter are of independent mathematical interest.\n\nSimplex pivot methods preserve primal (or dual) feasibility.  On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility—they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order.  Pivot methods of this type have been studied since the 1970s.{{citation needed|date=February 2019}}  Essentially, these methods attempt to find the shortest pivot path on the [[arrangement polytope]] under the linear programming problem.  In contrast to polytopal graphs, graphs of arrangement polytopes are known to have small diameter, allowing the possibility of strongly polynomial-time criss-cross pivot algorithm without resolving questions about the diameter of general polytopes.<ref name=\"FukudaTerlaky\" />\n\n== Integer unknowns ==\n\nIf all of the unknown variables are required to be integers, then the problem is called an [[integer programming]] (IP) or '''integer linear programming''' (ILP) problem.  In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) [[NP-hard]]. '''0–1 integer programming''' or '''binary integer programming''' (BIP) is the special case of integer programming where variables are required to be 0 or 1 (rather than arbitrary integers). This problem is also classified as NP-hard, and in fact the decision version was one of [[Karp's 21 NP-complete problems]].\n\nIf only some of the unknown variables are required to be integers, then the problem is called a '''mixed integer programming''' (MIP) problem.  These are generally also NP-hard because they are even more general than ILP programs.\n\nThere are however some important subclasses of IP and MIP problems that are efficiently solvable, most notably problems where the constraint matrix is [[totally unimodular]] and the right-hand sides of the constraints are integers or – more general – where the system has the [[total dual integrality]] (TDI) property.\n\nAdvanced algorithms for solving integer linear programs include:\n* [[cutting-plane method]]\n* [[Branch and bound]]\n* [[Branch and cut]]\n* [[Branch and price]]\n* if the problem has some extra structure, it may be possible to apply [[delayed column generation]].\nSuch integer-programming algorithms are discussed by Padberg and in Beasley.\n\n== Integral linear programs ==\n\nA linear program in real variables is said to be '''integral''' if it has at least one optimal solution which is integral. Likewise, a polyhedron <math>P = \\{x \\mid Ax \\ge 0\\}</math> is said to be '''integral''' if for all bounded feasible objective functions ''c'', the linear program <math>\\{\\max cx \\mid x \\in P\\}</math> has an optimum <math>x^*</math> with integer coordinates. As observed by Edmonds and Giles in 1977, one can equivalently say that the polyhedron <math>P</math> is integral if for every bounded feasible integral objective function ''c'', the optimal ''value'' of the linear program <math>\\{\\max cx \\mid x \\in P\\}</math> is an integer.\n\nIntegral linear programs are of central importance in the polyhedral aspect of [[combinatorial optimization]] since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a [[linear programming relaxation]] is integral, then it is the desired description of the convex hull of feasible (integral) solutions.\n\nNote that terminology is not consistent throughout the literature, so one should be careful to distinguish the following two concepts,\n* in an ''integer linear program,'' described in the previous section, variables are forcibly constrained to be integers, and this problem is NP-hard in general,\n* in an ''integral linear program,'' described in this section, variables are not constrained to be integers but rather one has proven somehow that the continuous problem always has an integral optimal value (assuming ''c'' is integral), and this optimal value may be found efficiently since all polynomial-size linear programs can be solved in polynomial time.\n\nOne common way of proving that a polyhedron is integral is to show that it is [[Totally unimodular matrix|totally unimodular]]. There are other general methods including the [[integer decomposition property]] and [[total dual integrality]]. Other specific well-known integral LPs include the matching polytope, lattice polyhedra, [[submodular]] flow polyhedra, and the intersection of 2 generalized polymatroids/''g''-polymatroids – e.g. see Schrijver 2003.\n\nA bounded integral polyhedron is sometimes called a [[convex lattice polytope]], particularly in two dimensions.\n\n== Solvers and scripting (programming) languages ==\n\n'''[[Permissive free software licence|Permissive]] licenses:'''\n{| class=\"wikitable\"\n|-\n!Name\n!License\n!Brief info\n|-\n| [[Pyomo]]||[[BSD licenses|BSD]]||An open-source modeling language for large-scale linear, mixed integer and nonlinear optimization\n|}\n\n'''[[Copyleft|Copyleft (reciprocal)]] licenses:'''\n{| class=\"wikitable\"\n|-\n!Name\n!License\n!Brief info\n|-\n|[[Cassowary constraint solver]]||LGPL||an incremental constraint solving toolkit that efficiently solves systems of linear equalities and inequalities\n|-\n|[[COIN-OR CLP|CLP]]||CPL|| an LP solver from COIN-OR\n|-\n|[[GNU Linear Programming Kit|glpk]]||GPL|| GNU Linear Programming Kit, an LP/MILP solver with a native C [[API]] and numerous (15) third-party wrappers for other languages.  Specialist support for [[flow network]]s.  Bundles the [[AMPL]]-like [[GNU MathProg]] modelling language and translator.\n|-\n|[[Qoca]]||GPL||a library for incrementally solving systems of linear equations with various goal functions\n|-\n|[[R-Project]]||GPL||a programming language and software environment for statistical computing and graphics\n|}\n\n[[MINTO]] (Mixed Integer Optimizer, an [[integer programming]] solver which uses branch and bound algorithm) has publicly available source code<ref>{{cite web|url=http://coral.ie.lehigh.edu/~minto/download.html|title=COR@L – Computational Optimization Research At Lehigh|work=lehigh.edu}}</ref> but is not open source.\n\n'''[[Proprietary software|Proprietary]] licenses:'''\n{| class=\"wikitable\"\n|-\n!Name\n!Brief info\n|-\n|[[AIMMS]]||\n|-\n|[[AMPL]]|| A popular modeling language for large-scale linear, mixed integer and nonlinear optimisation with a free student limited version available (500 variables and 500 constraints).\n|-\n|[[APMonitor]]|| API to MATLAB and Python. Solve example [http://apmonitor.com/me575/index.php/Main/LinearProgramming Linear Programming (LP) problems] through MATLAB, Python, or a web-interface.\n|-\n|[[CPLEX]]|| Popular solver with an API for several programming languages, and also has a modelling language and works with AIMMS, AMPL, [[General Algebraic Modeling System|GAMS]], MPL, OpenOpt, OPL Development Studio, and [[TOMLAB]]. Free for academic use.\n|-\n|[[Microsoft Excel|Excel]] Solver Function|| A nonlinear solver adjusted to spreadsheets in which function evaluations are based on the recalculating cells. Basic version available as a standard add-on for Excel.\n|-\n|[[FortMP]]||\n|-\n|[[General Algebraic Modeling System|GAMS]]||\n|-\n|[[Gurobi]]|| Solver with parallel algorithms for large-scale linear programs, quadratic programs and mixed-integer programs. Free for academic use.\n|-\n|[[IMSL Numerical Libraries]]|| Collections of math and statistical algorithms available in C/C++, Fortran, Java and C#/.NET. Optimization routines in the IMSL Libraries include unconstrained, linearly and nonlinearly constrained minimizations, and linear programming algorithms.\n|-\n|[[LINDO]]|| Solver with an API for large scale optimization of linear, integer, quadratic, conic and general nonlinear programs with stochastic programming extensions. It offers a global optimization procedure for finding guaranteed globally optimal solution to general nonlinear programs with continuous and discrete variables. It also has a statistical sampling API to integrate Monte-Carlo simulations into an optimization framework. It has an algebraic modeling language ([[Lingo (programming language)|LINGO]]) and allows modeling within a spreadsheet ([[What'sBest]]).\n|-\n|[[Maple (software)|Maple]]|| A general-purpose programming-language for symbolic and numerical computing.\n|-\n|[[MATLAB]]|| A general-purpose and matrix-oriented programming-language for numerical computing.  Linear programming in MATLAB requires the [[Optimization Toolbox]] in addition to the base MATLAB product; available routines include INTLINPROG and LINPROG\n|-\n|[[Mathcad]]|| A WYSIWYG math editor. It has functions for solving both linear and nonlinear optimization problems.\n|-\n|[[Mathematica]]|| A general-purpose programming-language for mathematics, including symbolic and numerical capabilities.\n|-\n|[[MOSEK]]|| A solver for large scale optimization with API for several languages (C++,java,.net, Matlab and python).\n|-\n|[[NAG Numerical Library]]|| A collection of mathematical and statistical routines developed by the [[Numerical Algorithms Group]] for multiple programming languages (C, C++, Fortran, Visual Basic, Java and C#) and packages (MATLAB, Excel, R, LabVIEW). The Optimization chapter of the NAG Library includes routines for linear programming problems with both sparse and non-sparse linear constraint matrices, together with routines for the optimization of quadratic, nonlinear, sums of squares of linear or nonlinear functions with nonlinear, bounded or no constraints.  The NAG Library has routines for both local and global optimization, and for continuous or integer problems.\n|-\n|[[NMath Stats]]|| A general-purpose [[.NET Framework|.NET]] statistical library containing a simplex solver.<ref>{{cite web|url=http://www.centerspace.net/landing.php?id=lp|title=C# Linear Programming|work=centerspace.net}}{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n|-\n|[[OptimJ]]|| A Java-based modeling language for optimization with a free version available.<ref>http://www.in-ter-trans.eu/resources/Zesch_Hellingrath_2010_Integrated+Production-Distribution+Planning.pdf OptimJ used in an optimization model for mixed-model assembly lines, University of Münster</ref><ref>http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1769/2076 OptimJ used in an Approximate Subgame-Perfect Equilibrium Computation Technique for Repeated Games</ref>\n|-\n|[[SAS System|SAS]]/OR|| A suite of solvers for Linear, Integer, Nonlinear, Derivative-Free, Network, Combinatorial and Constraint Optimization; the [[Algebraic modeling language]] [http://support.sas.com/documentation/cdl/en/ormpug/63975/HTML/default/ormpug_optmodel_sect005.htm OPTMODEL]; and a variety of vertical solutions aimed at specific problems/markets, all of which are fully integrated with the [[SAS System]].\n|-\n|[[SCIP (optimization software)|SCIP]]||A general-purpose constraint integer programming solver with an emphasis on MIP. Compatible with [http://zimpl.zib.de/ Zimpl] modelling language. Free for academic use and available in source code.\n|-\n|[[FICO Xpress|XPRESS]]||Solver for large-scale linear programs, quadratic programs, general nonlinear and mixed-integer programs. Has API for several programming languages, also has a modelling language Mosel and works with AMPL, [[General Algebraic Modeling System|GAMS]]. Free for academic use.\n|-\n|[[VisSim]]|| A visual [[block diagram]] language for simulation of [[dynamical system]]s.\n|}\n\n== See also ==\n{{colbegin|colwidth=22em}}\n* [[Convex programming]]\n* [[Dynamic programming]]\n* [[Input–output model]]\n* [[Job shop scheduling]]\n* [[Linear-fractional programming (LFP)]]\n* [[LP-type problem]]\n* [[Mathematical programming]]\n* [[Nonlinear programming]]\n* [[Oriented matroid]]\n* [[Quadratic programming]], a superset of linear programming\n* [[Semidefinite programming]]\n* [[Shadow price]]\n* [[Simplex algorithm]], used to solve LP problems\n{{colend}}\n\n== Notes ==\n{{Reflist|30em}}\n\n== References ==\n* {{cite journal |first=L. V. |last=Kantorovich |title=Об одном эффективном методе решения некоторых классов экстремальных проблем |trans-title=A new method of solving some classes of extremal problems |journal=[[Proceedings of the USSR Academy of Sciences|Doklady Akad Sci SSSR]] |volume=28 |year=1940 |issue= |pages=211–214 }}\n* F. L. Hitchcock: ''[https://onlinelibrary.wiley.com/doi/abs/10.1002/sapm1941201224 The distribution of a product from several sources to numerous localities]'', Journal of Mathematics and Physics, 20, 1941, 224–230.\n* G.B Dantzig: ''[https://books.google.com/books?hl=en&lr=&id=ZpYca36h464C&oi=fnd&pg=PA24&dq=%22Maximization+of+a+linear+function+of+variables+subject+to+linear+inequalities%22&ots=0viWRKQVGk&sig=25NCv3tDYjTLYxCxn9deMWBn8VE Maximization of a linear function of variables subject to linear inequalities]'', 1947. Published pp.&nbsp;339–347 in T.C. Koopmans (ed.):''Activity Analysis of Production and Allocation'', New York-London 1951 (Wiley & Chapman-Hall)\n* J. E. Beasley, editor. ''Advances in Linear and Integer Programming''. Oxford Science, 1996. (Collection of surveys)\n* {{cite journal|pages= 103–107|jstor=3689647|doi=10.1287/moor.2.2.103|title=New Finite Pivoting Rules for the Simplex Method|journal=Mathematics of Operations Research|volume=2|issue=2|year=1977|last1=Bland|first1=Robert G.}}\n* Karl-Heinz Borgwardt, ''The Simplex Algorithm: A Probabilistic Analysis'', Algorithms and Combinatorics, Volume 1, Springer-Verlag, 1987. (Average behavior on random problems)\n* Richard W. Cottle, ed. ''The Basic George B. Dantzig''. Stanford Business Books, Stanford University Press, Stanford, California, 2003. (Selected papers by [[George B. Dantzig]])\n* George B. Dantzig and Mukund N. Thapa. 1997. ''Linear programming 1: Introduction''. Springer-Verlag.\n* George B. Dantzig and Mukund N. Thapa. 2003. ''Linear Programming 2: Theory and Extensions''. Springer-Verlag. (Comprehensive, covering e.g. [[simplex algorithm|pivoting]] and interior-point algorithms, large-scale problems, [[Dantzig–Wolfe decomposition|decomposition following Dantzig–Wolfe]] and [[Benders' decomposition|Benders]], and introducing [[stochastic programming]].)\n* {{cite book |doi=10.1016/S0167-5060(08)70734-9|pages=185–204|chapter=A Min-Max Relation for Submodular Functions on Graphs|title=Studies in Integer Programming|volume=1|series=Annals of Discrete Mathematics|year=1977|last1=Edmonds|first1=Jack|last2=Giles|first2=Rick|isbn=978-0-7204-0765-5}}\n* {{cite journal|first1=Komei|last1=Fukuda|first2=Tamás|last2=Terlaky|title=Criss-cross methods: A fresh view on pivot algorithms |journal=Mathematical Programming, Series B|volume=79|number=1—3|pages=369–395|editors=Thomas&nbsp;M. Liebling and Dominique de&nbsp;Werra|year=1997|doi=10.1007/BF02614325|mr=1464775|citeseerx=10.1.1.36.9373}}\n* {{cite book|last1=Gondzio|first1=Jacek|last2=Terlaky|first2=Tamás|chapter=3 A computational view of interior point methods|mr=1438311|title=Advances in linear and integer programming|pages=103–144|editor=J.&nbsp;E. Beasley|location=New York|publisher=Oxford University Press|year=1996|series=Oxford Lecture Series in Mathematics and its Applications|volume=4|chapter-url=http://www.maths.ed.ac.uk/~gondzio/CV/oxford.ps|ref=harv|id=[http://www.maths.ed.ac.uk/~gondzio/CV/oxford.ps Postscript file at website of Gondzio] and [http://www.cas.mcmaster.ca/~terlaky/files/dut-twi-94-73.ps.gz at McMaster University website of Terlaky]}}\n* {{cite book|last=Murty|first=Katta&nbsp;G.|authorlink=Katta G. Murty|title=Linear programming|publisher=John Wiley & Sons, Inc.|location=New York|year=1983|pages=xix+482|isbn=978-0-471-09725-9|mr=720547|ref=harv|id=(comprehensive reference to classical approaches)}}\n* Evar D. Nering and [[Albert W. Tucker]], 1993, ''Linear Programs and Related Problems'', Academic Press. (elementary<!-- but profound -->)\n* M. Padberg, ''Linear Optimization and Extensions'', Second Edition, Springer-Verlag, 1999. (carefully written account of primal and dual simplex algorithms and projective algorithms, with an introduction to integer linear programming – featuring the [[traveling salesman problem]] for [[Odysseus]].)\n* [[Christos H. Papadimitriou]] and Kenneth Steiglitz, ''Combinatorial Optimization: Algorithms and Complexity'', Corrected republication with a new preface, Dover. (computer science)\n* {{cite journal|author=Michael J. Todd |date=February 2002 | title = The many facets of linear programming | journal = Mathematical Programming | volume = 91 | issue = 3 | doi = 10.1007/s101070100261 | pages=417–436}} (Invited survey, from the International Symposium on Mathematical Programming.)\n* {{cite book |first=Robert J. |last=Vanderbei |title=Linear Programming: Foundations and Extensions |year=2001 |publisher=Springer Verlag |ref=harv}}\n* {{cite book | last=Vazirani | first=Vijay V. | authorlink=Vijay Vazirani | title=Approximation Algorithms | year=2001 | publisher=Springer-Verlag | isbn=978-3-540-65367-7 | pages=}} (Computer science)\n\n== Further reading ==\n{{Library resources box |others=no}}\n\nA reader may consider beginning with Nering and Tucker, with the first volume of Dantzig and Thapa, or with Williams.\n* Dmitris Alevras and Manfred W. Padberg, ''[https://books.google.com/books?id=RAUyB8NDHJwC&printsec=frontcover#v=onepage&q&f=false Linear Optimization and Extensions: Problems and Solutions]'', Universitext, Springer-Verlag, 2001. (Problems from Padberg with solutions.)\n<!-- * A. Bachem and W. Kern. ''Linear Programming Duality: An Introduction to Oriented Matroids''. Universitext. Springer-Verlag, 1992. ([[Oriented matroid|Combinatorial]]) -->\n* {{cite book|author = Mark de Berg, Marc van Kreveld, [[Mark Overmars]], and Otfried Schwarzkopf | year = 2000 | title = Computational Geometry | publisher = [[Springer-Verlag]] | edition = 2nd revised | isbn = 978-3-540-65620-3}} Chapter 4: Linear Programming: pp.&nbsp;63–94. Describes a randomized half-plane intersection algorithm for linear programming.\n* {{cite book|author = [[Michael R. Garey]] and [[David S. Johnson]] | year = 1979 | title = Computers and Intractability: A Guide to the Theory of NP-Completeness | publisher = W.H. Freeman | isbn = 978-0-7167-1045-5| title-link = Computers and Intractability: A Guide to the Theory of NP-Completeness }} A6: MP1: INTEGER PROGRAMMING, pg.245. (computer science, complexity theory)\n* {{Cite Gartner Matousek 2006}} (elementary introduction for mathematicians and computer scientists)\n* Cornelis Roos, Tamás Terlaky, Jean-Philippe Vial, ''Interior Point Methods for Linear Optimization'', Second Edition, Springer-Verlag, 2006. (Graduate level)\n* {{cite book|author = Alexander Schrijver | year = 2003 | title = Combinatorial optimization: polyhedra and efficiency | publisher = Springer}}\n* Alexander Schrijver, ''Theory of Linear and Integer Programming''. John Wiley & sons, 1998, {{isbn|0-471-98232-6}} (mathematical)\n* {{cite book|author1=Gerard Sierksma|author2=Yori Zwols|title=Linear and Integer Optimization: Theory and Practice|year=2015|publisher=CRC Press|isbn=978-1-498-71016-9}}\n* {{cite book|author1=Gerard Sierksma|author2=Diptesh Ghosh|title=Networks in Action; Text and Computer Exercises in Network Optimization|year=2010|publisher=Springer|isbn=978-1-4419-5512-8}} (linear optimization modeling)\n* H. P. Williams, ''[https://books.google.com/books?id=YJRh0tOes7UC&printsec=frontcover#v=onepage&q&f=false Model Building in Mathematical Programming]'', Fifth Edition, 2013. (Modeling)\n* Stephen J. Wright, 1997, ''[https://books.google.com/books?id=oQdBzXhZeUkC&printsec=frontcover#v=onepage&q&f=false Primal-Dual Interior-Point Methods]'', SIAM. (Graduate level)\n* [[Yinyu Ye]], 1997, ''Interior Point Algorithms: Theory and Analysis'', Wiley. (Advanced graduate-level)\n* [[Günter M. Ziegler|Ziegler, Günter M.]], Chapters 1–3 and 6–7 in ''Lectures on Polytopes'', Springer-Verlag, New York, 1994. (Geometry)\n\n==External links==\n{{Commonscat}}\n*[http://people.brunel.ac.uk/~mastjjb/jeb/or/lp.html Guidance On Formulating LP Problems]\n*[http://glossary.computing.society.informs.org/ Mathematical Programming Glossary]\n*[http://lpsolve.sourceforge.net/4.0/LinearProgrammingFAQ.htm The Linear Programming FAQ]\n*[http://plato.asu.edu/bench.html Benchmarks For Optimisation Software]\n\n{{optimization algorithms}}\n{{Mathematical programming}}\n{{Authority control}}\n\n[[Category:Linear programming| ]]\n[[Category:Convex optimization]]\n[[Category:Geometric algorithms]]\n[[Category:P-complete problems]]"
    },
    {
      "title": "Assignment problem",
      "url": "https://en.wikipedia.org/wiki/Assignment_problem",
      "text": "The '''assignment problem''' is a fundamental [[combinatorial optimization]] problem. It consists of finding, in a [[weighted graph|weighted]] [[bipartite graph]], a [[Matching (graph theory)|matching]] in which the sum of weights of the edges is as large as possible. A common variant consists of finding a ''minimum''-weight [[Matching (graph theory)|perfect matching]]. \n\nIt is a specialization of the [[Maximum weight matching|maximum weight matching]] problem for bipartite graphs.\n\nIn its most general form, the problem is as follows:\n:The problem instance has a number of ''agents'' and a number of ''tasks''. Any agent can be assigned to perform any task, incurring some ''cost'' that may vary depending on the agent-task assignment. It is required to perform all tasks by assigning exactly one agent to each task and exactly one task to each agent in such a way that the ''total cost'' of the assignment is minimized.\n\nIf the numbers of agents and tasks are equal, and the total cost of the assignment for all tasks is equal to the sum of the costs for each agent (or the sum of the costs for each task, which is the same thing in this case), then the problem is called the ''linear assignment problem''. Commonly, when speaking of the ''assignment problem'' without any additional qualification, then the ''linear assignment problem'' is meant.\n\n== Algorithms and generalizations ==\nA naive solution for the assignment problem is to check all the assignments and calculate the cost of each one. This may be very inefficient since, with ''n'' agents and ''n'' tasks, there are ''n''! (factorial of ''n'') different assignments.\n\nMany algorithms have been developed for solving the assignment problem in time bounded by a polynomial of ''n.'' One of the first such algorithms was the [[Hungarian algorithm]], developed by Munkres.<ref>{{cite journal|last1=Munkres|first1=James|date=1957|title=Algorithms for the Assignment and Transportation Problems|journal=Journal of the Society for Industrial and Applied Mathematics|volume=5|issue=1|pages=32–38|jstor=2098689|doi=10.1137/0105003|citeseerx=10.1.1.228.3911}}</ref> Other algorithms include adaptations of the primal [[simplex algorithm]], and the [[auction algorithm]].\n\nThe assignment problem is a special case of the [[transportation problem]], which is a special case of the [[minimum cost flow problem]], which in turn is a special case of a [[linear program]].  While it is possible to solve any of these problems using the [[simplex algorithm]], each specialization has more efficient algorithms designed to take advantage of its special structure.\n\nWhen a number of agents and tasks is very large, a [[parallel algorithm]] with randomization can be applied. \n\nThe problem of finding minimum weight maximum matching can be converted to finding a minimum weight perfect matching. A [[bipartite graph]] can be extended to a complete bipartite graph by adding artificial edges with large weights. These weights should exceed the weights of all existing matchings to prevent appearance of artificial edges in the possible solution. As shown by Mulmuley, Vazirani and Varizani,<ref>{{Cite journal|last1=Mulmuley|first1=Ketan|last2=Vazirani|first2=Umesh|authorlink2=Umesh Vazirani|last3=Vazirani|first3=Vijay|authorlink3=Vijay Vazirani|year=1987|title=Matching is as easy as matrix inversion|url=http://www.springerlink.com/content/r4rw2x4l46476708/|journal=Combinatorica|volume=7|issue=1|pages=105–113|doi=10.1007/BF02579206|ref=harv|authorlink1=Ketan Mulmuley}}</ref> the problem of minimum weight perfect matching is converted to finding minors in the [[adjacency matrix]] of a graph. Using the [[isolation lemma]], a minimum weight perfect matching in a graph can be found with probability at least ½. For a graph with n vertices, it requires <math> O(\\log^2(n)) </math> time.\n\n==Example==\nSuppose that a taxi firm has three taxis (the agents) available, and three customers (the tasks) wishing to be picked up as soon as possible. The firm prides itself on speedy pickups, so for each taxi the \"cost\" of picking up a particular customer will depend on the time taken for the taxi to reach the pickup point. The solution to the assignment problem will be whichever combination of taxis and customers results in the least total cost.\n\nHowever, the assignment problem can be made rather more flexible than it first appears. In the above example, suppose that there are four taxis available, but still only three customers. Then a fourth dummy task can be invented, perhaps called \"sitting still doing nothing\", with a cost of 0 for the taxi assigned to it. The assignment problem can then be solved in the usual way and still give the best solution to the problem.\n\nSimilar adjustments can be done in order to allow more tasks than agents, tasks to which multiple agents must be assigned (for instance, a group of more customers than will fit in one taxi), or maximizing profit rather than minimizing cost.\n\n==Formal definition==\n\nThe formal definition of the '''assignment problem''' (or '''linear assignment problem''') is\n\n:Given two sets, ''A'' and ''T'', of equal size, together with a [[weight function]] ''C'' : ''A'' &times; ''T'' &rarr; '''[[real number|R]]'''. Find a [[bijection]] ''f'' : ''A'' &rarr; ''T'' such that the [[Loss function|cost function]]:\n::<math>\\sum_{a\\in A}C(a,f(a))</math>\n\nis minimized.\n\nUsually the weight function is viewed as a square real-valued [[matrix (mathematics)|matrix]] ''C'', so that the cost function is written down as:\n\n:<math>\\sum_{a\\in A}C_{a,f(a)}</math>\n\nThe problem is \"linear\" because the cost function to be optimized as well as all the constraints contain only linear terms.\n\n== Solution by linear programming ==\nThe assignment problem can be solved by presenting it as a [[linear program]]. For convenience we will present the maximization problem. Each edge (''i'',''j''), where ''i'' is in A and ''j'' is in T, has a weight ''<math>w_{ij}</math>''. For each edge ''(i,j)'' we have a variable ''<math>x_{ij}</math>''<sub>.</sub> The variable is 1 if the edge is contained in the matching and 0 otherwise, so we set the domain constraints:  <math>0\\le x_{ij}\\le 1\\text{ for }i,j\\in A,T, \\, </math> <math>x_{ij}\\in \\mathbb{Z}\\text{ for }i,j\\in A,T. </math>\n\nThe total weight of the matching is: <math>\\sum_{(i,j)\\in A\\times T} w_{ij}x_{ij}</math>. The goal is to find a maximum-weight perfect matching. \n\nTo guarantee that the variables indeed represent a perfect matching, we add constraints saying that each vertex is adjacent to exactly one edge in the matching, i.e,  <math>\\sum_{j\\in T}x_{ij}=1\\text{ for }i\\in A, \\,\n~~~\n\\sum_{i\\in A}x_{ij}=1\\text{ for }j\\in T, \\, </math>.\n\nAll in all we have the following LP:\n\n<math display=\"block\">\\text{maximize}~~\\sum_{(i,j)\\in A\\times T} w_{ij}x_{ij}\n</math><math display=\"block\">\\text{subject to}~~\\sum_{j\\in T}x_{ij}=1\\text{ for }i\\in A, \\,\n~~~\n\\sum_{i\\in A}x_{ij}=1\\text{ for }j\\in T </math><math display=\"block\">0\\le x_{ij}\\le 1\\text{ for }i,j\\in A,T, \\, </math><math display=\"block\">x_{ij}\\in \\mathbb{Z}\\text{ for }i,j\\in A,T. </math>This is an integer linear program.  However, we can solve it without the integrality constraints (i.e., drop the last constraint), using standard methods for solving continuous linear programs. While this formulation allows also fractional variable values, in this special case, the LP always has an optimal solution where the variables take integer values.  This is because the constraint matrix of the fractional LP is [[Unimodular matrix#Total unimodularity|totally unimodular]] - it satisfies the four conditions of Hoffman and Gale.  \n\nThis can also be proved directly.<ref>{{Cite Gartner Matousek 2006}}</ref>{{Rp|31-37}} Let ''x'' be an optimal solution of the fractional LP, ''w(x)'' be its total weight, and ''k(x)'' be the number of non-integral variables. If ''k(x)''=0 we are done. Otherwise, there is a fractional variable, say ''<math>x_{i1,j2}</math>''. Because the sum of variables adjacent to ''j2'' is 1, which in an integer, there must be another variable adjacent to ''j''2 with a fractional value, say  ''<math>x_{i3,j2}</math>''. By similar considerations on ''i''3, there must be another variable adjacent to ''i''3 with a fractional value, say  ''<math>x_{i3,j4}</math>''. By similar considerations we move from one vertex to another, collecting edges with fractional values. Since the graph is finite, at some point we must have a cycle. Without loss of generality we can assume that the cycle ends at vertex ''i''1, so the last fractional variable in the cycle is ''<math>x_{i1,j_{2m}}</math>''. So the number of edges in the cycle is 2''m'' - it must be even since the graph is bipartite.  \n\nSuppose we add a certain constant ''e'' to all even variables in the cycle, and remove the same constant ''e'' from all odd variables in the cycle. For any such ''e'', the sum of variables near each vertex remains the same (1), so the vertex constraints are still satisfied. Moreover, if ''e'' is sufficiently small, all variables remain between 0 and 1, so the domain constraints are still satisfied too. It is easy to find a largest ''e'' that maintains the domain constraints: it is either the smallest difference between an odd variable and 0, or the smallest difference between an even variable and 1. Now, we have one less fractional variable, so ''k''(''x'') decreases by 1. The objective value remains the same, since otherwise we could increase it by selecting ''e'' to be positive or negative, in contradiction to the assumption that it is maximal.   \n\nBy repeating the cycle-removal process we arrive, after at most ''n'' steps, at a solution in which all variables are integral.  \n\n==See also==\n*[[Auction algorithm]]\n*[[Generalized assignment problem]]\n*[[Linear bottleneck assignment problem]]\n*[[Monge-Kantorovich transportation problem]], a more general formulation\n*[[National Resident Matching Program]]\n*[[Quadratic assignment problem]]\n*[[Secretary problem]]\n*[[Stable marriage problem]]\n*[[Stable roommates problem]]\n*[[Weapon target assignment problem]]\n\n== References and further reading ==\n{{Reflist}}\n\n*\n*{{cite book | last=Brualdi | first=Richard A. | title=Combinatorial matrix classes | series=Encyclopedia of Mathematics and Its Applications | volume=108 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=978-0-521-86565-4 | zbl=1106.05001 }}\n* {{cite book | authorlink = Rainer Burkard | first = Rainer | last = Burkard |author2=M. Dell'Amico|author3=S. Martello | year = 2012 | title = Assignment Problems (Revised reprint) | publisher = SIAM | isbn = 978-1-61197-222-1 }}\n* {{cite book | authorlink = Dimitri Bertsekas | first = Dimitri | last = Bertsekas | year = 1998 | title = Network Optimization: Continuous and Discrete Models | publisher = Athena Scientific | isbn = 978-1-886529-02-1 }}\n\n[[Category:Combinatorial optimization]]\n[[Category:Matching]]\n[[Category:Polynomial-time problems]]\n[[Category:Linear programming]]"
    },
    {
      "title": "Basic feasible solution",
      "url": "https://en.wikipedia.org/wiki/Basic_feasible_solution",
      "text": "{{one source|date=December 2018}}\nIn the theory of [[linear programming]], a '''basic feasible solution''' ('''BFS''') is, intuitively, a solution with a minimal number of non-zero variables. Geometrically, each BFS corresponds to a corner of the [[polyhedron]] of feasible solutions. If there exists an optimal solution, then there exists an optimal BFS. Hence, to find an optimal solution, it is sufficient to consider the BFS-s. This fact is used by the [[simplex algorithm]], which essentially travels from some BFS to another until an optimal one is found.<ref name=gm06>{{Cite Gartner Matousek 2006}}{{rp|44-48}}</ref>\n\n== Definition ==\nTo define a BFS, we first present the linear program in the so-called ''equational form'':\n:maximize <math display=\"inline\">\\mathbf{c^T} \\cdot \\mathbf{x}</math>\n:subject to <math>A\\mathbf{x} = \\mathbf{b}</math> and <math>\\mathbf{x} \\ge 0</math>\nwhere:\n* <math>\\mathbf{c^T}</math> and <math>\\mathbf{x}</math> are vectors of size ''n'' (the number of variables);\n* <math>\\mathbf{b}</math> is a vector of size ''m'' (the number of constraints);\n* <math>A</math> is an ''m''-by-''n'' matrix; \n* <math>\\mathbf{x} \\ge 0</math> means that all variables are non-negative.\n\nAny linear program can be converted into an equational form by adding [[slack variable]]s.\n\nAs a preliminary clean-up step, we verify that:\n* The system <math>A\\mathbf{x} = \\mathbf{b}</math> has at least one solution (otherwise the whole LP has no solution and there is nothing more to do);\n* All ''m'' rows of the matrix <math>A</math> are linearly independent, i.e., its rank is ''m'' (otherwise we can just delete redundant rows without changing the LP).\n\nTypically ''m'' < ''n'', so the system <math>A\\mathbf{x} = \\mathbf{b}</math> has many solutions; each such solution is called a '''feasible solution''' of the LP.\n\nLet ''B'' be a subset of ''m'' indices from {1,...,''n''}. Denote by <math>A_B</math> the square ''m''-by-''m'' matrix made of the ''m'' columns of <math>A</math> indexed by ''B''. If <math>A_B</math> is [[nonsingular]], the columns indexed by ''B'' are a [[Basis (linear algebra)|basis]] of the [[column space]] of <math>A</math>. In this case, we call ''B'' a ''basis'' of the LP.\nSince the rank of <math>A</math> is ''m'', it has at least one basis; \nsince <math>A</math> has ''n'' columns, it has at most <math>\\binom{n}{m}</math> bases.\n\nGiven a basis ''B'', we say that a feasible solution <math>\\mathbf{x}</math> is a '''basic feasible solution with basis B''' if all its non-zero variables are indexed by ''B'', i.e., for all <math>j\\not\\in B: ~~ x_j = 0</math>.\n\n== Properties ==\n1. A BFS is determined only by the constraints of the LP (the matrix <math>A</math> and the vector <math>\\mathbf{b}</math>); it does not depend on the optimization objective.\n\n2. By definition, a BFS has at most ''m'' non-zero variables and at least ''n''-''m'' zero variables. A BFS can have less than ''m'' non-zero variables; in that case, it can have many different bases, all of which contain the indices of its non-zero variables. \n\n3. A feasible solution <math>\\mathbf{x}</math> is basic if-and-only-if the columns of the matrix <math>A_K</math> are linearly independent, where ''K'' is the set of indices of the non-zero elements of <math>\\mathbf{x}</math>. <ref name=\"gm06\" />{{rp|45}}\n\n4. A BFS is uniquely determined by the basis ''B'': for each basis ''B'' of ''m'' indices, there is at most one BFS  <math>\\mathbf{x_B}</math> with basis ''B''. This is because <math>\\mathbf{x_B}</math> must satisfy the constraint <math>A_B \\mathbf{x_B} = b</math>, and by definition of basis the matrix <math>A_B</math> is non-singular, so the constraint has a unique solution. The opposite is not true: each BFS can come from many different bases. \n\nIf the unique solution of <math>A_B \\mathbf{x_B} = b</math> satisfies the non-negativity constraints, then the basis is called a '''feasible basis'''.\n\n5. If a linear program has an optimal solution (i.e., it has a feasible solution, and the set of feasible solutions is bounded), then it has an optimal BFS. This is a consequence of the [[Bauer maximum principle]]: the objective of a linear program is convex; the set of feasible solutions is convex (it is an intersection of hyperspaces); therefore the objective attains its maximum in an extreme point of the set of feasible solutions. \n\nSince the number of BFS-s is finite and bounded by <math>\\binom{n}{m}</math>, an optimal solution to any LP can be found in finite time by just evaluating the objective function in all <math>\\binom{n}{m}</math>BFS-s. This is not the most efficient way to solve an LP; the [[simplex algorithm]] examines the BFS-s in a much more efficient way. \n\n== Examples ==\nConsider a linear program with the following constraints:\n\n<math>\\begin{align} \nx_1 + 5 x_2 + 3 x_3 + 4 x_4 + 6 x_5 &= 14\n\\\\\nx_2 + 3 x_3 + 5 x_4 + 6 x_5 &= 7\n\\\\\n\\forall i\\in\\{1,\\ldots,5\\}: x_i&\\geq 0\n\\end{align}</math>\n\nThe matrix ''A'' is:\n\n<math>A = \n\\begin{pmatrix} \n1 & 5 & 3 & 4 & 6\n\\\\ \n0 & 1 & 3 & 5 & 6\n\\end{pmatrix}\n~~~~~\n\\mathbf{b} = (14~~7)</math>\n\nHere, ''m''=2 and there are 10 subsets of 2 indices, however, not all of them are bases: the set {3,5} is not a basis since columns 3 and 5 are linearly dependent.\n\nThe set ''B''={2,4} is a basis, since the matrix  <math>A_B = \n\\begin{pmatrix} \n5 & 4\n\\\\ \n1 & 5 \n\\end{pmatrix}\n</math> is non-singular.\n\nThe unique BFS corresponding to this basis is <math>x_B = (0~~2~~0~~1~~0)\n</math>.\n\n== Geometric interpretation ==\n[[File:Elongated pentagonal orthocupolarotunda.png|thumb|100x100px]]\nThe set of all feasible solutions is an intersection of [[dimension|hyperspaces]]<nowiki/>. Therefore, it is a [[Convex polyhedra|convex polyhedron]]. If it is bounded, then it is a [[convex polytope]].\n\nEach BFS corresponds to a vertex of this polytope. <ref name=\"gm06\" />{{rp|53-56}}\n\n== Application in the Simplex algorithm ==\n{{Main|simplex algorithm}}\nThe  [[simplex algorithm]] keeps, at each point of its execution, a \"current basis\" ''B'' (a subset of ''m'' out of ''n'' variables), a \"current BFS\", and a \"current tableau\". The tableau is a representation of the linear program where the basic variables are expressed in terms of the non-basic ones: <ref name=\"gm06\" />{{rp|65}}\n\n<math display=\"block\">\\begin{align} \nx_B &= p + Q x_N\n\\\\\nz &= z_0 + r^T x_N\n\\end{align}</math>where <math>x_B</math>is the vector of ''m'' basic variables,  <math>x_N</math>is the vector of ''n'' non-basic variables, and <math>z</math>is the maximization objective.  Since non-basic variables equal 0, the current BFS is <math>p</math>, and the current maximization objective is <math>z_0</math>.\n\nIf all coefficients in <math>r</math>are negative, then <math>z_0</math>is an optimal solution, since all variables (including all non-basic variables) must be at least 0, so the second line implies <math>z\\leq z_0</math>.\n\nIf some coefficients in <math>r</math>are positive, then it may be possible to increase the maximization target. For example, if <math>x_5</math>is non-basic and its coefficient in  <math>r</math>is positive, then increasing it above 0 may make <math>z</math>larger. If it is possible to do so without violating other constraints, then the increased variable becomes basic (it \"enters the base\"), while another non-basic variable is decreased to 0 to keep the equality constraints and thus becomes non-basic (it \"exits the base\"). \n\nIf this process is done carefully, then it is possible to guarantee that <math>z</math>increases until it reachces the optimal BFS. \n\n== References ==\n{{reflist}}\n\n[[Category:linear programming]]"
    }
  ]
}