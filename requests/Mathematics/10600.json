{
  "pages": [
    {
      "title": "König's theorem (set theory)",
      "url": "https://en.wikipedia.org/wiki/K%C3%B6nig%27s_theorem_%28set_theory%29",
      "text": "{{other uses|König's theorem (disambiguation){{!}}König's theorem}}\n\nIn [[set theory]], '''König's theorem'''  states that if the [[axiom of choice]] holds, ''I'' is a [[Set (mathematics)|set]], <math>\\kappa_i</math> and <math>\\lambda_i</math> are [[cardinal number]]s for every ''i'' in ''I'', and <math>\\kappa_i < \\lambda_i</math> for every ''i'' in ''I'', then \n:<math>\\sum_{i \\in I}\\kappa_i < \\prod_{i \\in I}\\lambda_i.</math>\n\nThe ''sum'' here is the cardinality of the [[disjoint union]] of the sets ''m<sub>i</sub>'', and the product is the cardinality of the [[Cartesian product]]. However, without the use of the axiom of choice, the sum and the product cannot be defined as cardinal numbers, and the meaning of the inequality sign would need to be clarified.\n\nKönig's theorem was introduced by {{harvs|txt|last=König|authorlink=Gyula Kőnig|year=1904}} in the slightly weaker form that the sum of a strictly increasing sequence of nonzero cardinal numbers is less than their product.\n\n== Details ==\n\nThe precise statement of the result:  if ''I'' is a [[Set (mathematics)|set]], ''A<sub>i</sub>'' and ''B<sub>i</sub>'' are sets for every ''i'' in ''I'', and <math>A_i<B_i</math> for every ''i'' in ''I'', then \n:<math>\\sum_{i \\in I}A_i < \\prod_{i \\in I}B_i,</math>\nwhere '''<''' means ''strictly less than in [[cardinality]]'', i.e. there is an [[injective]] [[function (mathematics)|function]] from ''A<sub>i</sub>'' to ''B<sub>i</sub>'', but not one going the other way. The union involved need not be disjoint (a non-disjoint union can't be any bigger than the disjoint version, also assuming the [[axiom of choice]]).  In this formulation, '''König's theorem''' is equivalent to the [[axiom of choice]].<ref name=\"Rubin 1985\">{{cite book|last=Rubin|first=H.|author2=Rubin, J. E.|author2-link= Jean E. Rubin |title=Equivalents of the Axiom of Choice, II|publisher=[[North-Holland Publishing Company|North Holland]]|place=New York, NY|year=1985|pages=185|isbn=0-444-87708-8}}</ref>\n\n(Of course, König's theorem is trivial if the cardinal numbers ''m<sub>i</sub>'' and ''n<sub>i</sub>'' are [[finite set|finite]] and the index set ''I'' is finite. If ''I'' is [[empty set|empty]], then the left sum is the empty sum and therefore 0, while the right product is the [[empty product]] and therefore 1).\n\nKönig's theorem is remarkable because of the strict inequality in the conclusion.  There are many easy rules for the arithmetic of infinite sums and products of cardinals in which one can only conclude a weak inequality ≤, for example:  if <math>m_i < n_i</math> for all ''i'' in ''I'', then one can only conclude\n:<math>\\sum_{i \\in I} m_i \\le \\sum_{i \\in I} n_i,</math>\nsince, for example, setting <math>m_i = 1</math> and <math>n_i = 2</math>, where the index set ''I'' is the natural numbers, yields the sum <math>\\aleph_0</math> for both sides, and we have an equality.\n\n==Corollaries of König's theorem==\n* If <math>\\kappa</math> is a cardinal, then <math>\\kappa < 2^\\kappa</math>.\nIf we take ''m<sub>i</sub>'' = 1, and ''n<sub>i</sub>'' = 2 for each ''i'' in κ, then the left side of the above inequality is just κ, while the right side is 2<sup>κ</sup>, the cardinality of functions from κ to {0, 1}, that is, the cardinality of the power set of κ. Thus, König's theorem gives us an alternate proof of [[Cantor's theorem]]. (Historically of course Cantor's theorem was proved much earlier.)\n\n===Axiom of choice===\nOne way of stating the axiom of choice is \"an arbitrary Cartesian product of non-empty sets is non-empty\". Let ''B<sub>i</sub>'' be a non-empty set for each ''i'' in ''I''. Let ''A<sub>i</sub>'' = {} for each ''i'' in ''I''. Thus by König's theorem, we have:\n* If <math>\\forall i \\in I(\\{\\} < B_i)</math>, then <math>\\{\\} < \\prod_{i \\in I}B_i</math>.\nThat is, the Cartesian product of the given non-empty sets ''B<sub>i</sub>'' has a larger cardinality than the sum of empty sets. Thus it is non-empty, which is just what the axiom of choice states. Since the axiom of choice follows from König's theorem, we will use the axiom of choice freely and implicitly when discussing consequences of the theorem.\n\n===König's theorem and cofinality===\nKönig's theorem has also important consequences for [[cofinality]] of cardinal numbers.\n\n* If <math>\\kappa \\ge \\aleph_0</math>, then <math>\\kappa < \\kappa^{\\operatorname{cf}(\\kappa)}</math>.\n\nChoose a strictly increasing cf(κ)-sequence of ordinals approaching κ. Each of them is less than κ, so their sum, which is κ, is less than the product of cf(κ) copies of κ.\n\nAccording to [[Easton's theorem]], the next consequence of König's theorem is the only nontrivial constraint on the continuum function for [[regular cardinal]]s.\n* If <math>\\kappa \\geq \\aleph_0</math> and <math>\\lambda \\geq 2</math>, then <math>\\kappa < \\operatorname{cf}(\\lambda^\\kappa)</math>.\nLet <math>\\mu = \\lambda^\\kappa</math>. Suppose that, contrary to this corollary, <math>\\kappa \\ge \\operatorname{cf}(\\mu)</math>. Then using the previous corollary, <math>\\mu < \\mu^{\\operatorname{cf}(\\mu)} \\le \\mu^\\kappa = (\\lambda^\\kappa)^\\kappa = \\lambda^{\\kappa \\cdot \\kappa} = \\lambda^\\kappa = \\mu</math>, a contradiction. Thus the supposition must be false, and this corollary must be true.\n\n==A proof of König's theorem==\nAssuming [[Zermelo–Fraenkel set theory]], including especially the [[axiom of choice]], we can prove the theorem. Remember that we are given <math>\\forall i\\in I\\quad A_i<B_i</math>, and we want to show :<math>\\sum_{i\\in I}A_i<\\prod_{i\\in I}B_i.</math>\n\nThe axiom of choice implies that the condition ''A'' < ''B'' is equivalent to the condition that there is no function from ''A'' onto ''B'' and ''B'' is nonempty.\nSo we are given that there is no function from ''A''<sub>''i''</sub> onto ''B''<sub>''i''</sub>≠{}, and we have to show that any function ''f'' from the disjoint union of the ''A''s to the product of the ''B''s is not surjective and that the product is nonempty. That the product is nonempty follows immediately from the axiom of choice and the fact that the factors are nonempty. For each ''i'' choose a ''b''<sub>''i''</sub> in ''B''<sub>''i''</sub> not in the image of ''A''<sub>''i''</sub> under the composition of ''f'' with the projection to ''B''<sub>''i''</sub>. Then the product of the elements ''b''<sub>''i''</sub> is not in the image of ''f'', so ''f'' does not map the disjoint union of the ''A''s onto the product of the ''B''s.\n\n==Notes==\n<references/>\n\n==References==\n* {{cite book | author=M. Holz, K. Steffens and E. Weitz | title=Introduction to Cardinal Arithmetic | publisher=Birkhäuser | year=1999 | isbn=3-7643-6124-7}}\n* {{citation\n|year=1904|pages= 144–147\n|title=Verhandlungen des dritten Internationalen Mathematiker-Kongresses in Heidelberg vom 8. bis 13. August 1904\n|chapter=Zum Kontinuum-Problem\n|first=J.|last= König|url=http://ada00.math.uni-bielefeld.de/ICM/ICM1904/\n|editor-first=Adolf|editor-last=Krazer}}, reprinted as {{citation|journal=Mathematische Annalen\n|year=1905|volume= 60|issue =2|pages= 177–180\n|title=Zum Kontinuum-Problem\n|first=J.|last= König|doi=10.1007/BF01677263|url=http://archiv.ub.uni-heidelberg.de/volltextserver/12583/}}\n\n{{DEFAULTSORT:Konigs theorem}}\n[[Category:Axiom of choice]]\n[[Category:Theorems in the foundations of mathematics]]\n[[Category:Cardinal numbers]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Limit cardinal",
      "url": "https://en.wikipedia.org/wiki/Limit_cardinal",
      "text": "In [[mathematics]], '''limit cardinals''' are certain [[cardinal number]]s. A cardinal number λ is a '''weak limit cardinal''' if  λ is neither a [[successor cardinal]] nor zero. This means that one cannot \"reach\" λ from another cardinal by repeated successor operations. These cardinals are sometimes called simply \"limit cardinals\" when the context is clear.\n\nA cardinal λ is a '''strong limit cardinal''' if λ cannot be reached by repeated [[Power set|powerset]] operations. This means that λ is nonzero and, for all κ < λ, 2<sup>κ</sup> < λ. Every strong limit cardinal is also a weak limit cardinal, because κ<sup>+</sup> ≤ 2<sup>κ</sup> for every cardinal κ, where κ<sup>+</sup> denotes the successor cardinal of κ.\n\nThe first infinite cardinal, <math>\\aleph_0</math> ([[Aleph number#Aleph-naught|aleph-naught]]), is a strong limit cardinal, and hence also a weak limit cardinal.\n\n== Constructions ==\n\nOne way to construct limit cardinals is via the union operation: <math>\\aleph_{\\omega}</math> is a weak limit cardinal, defined as the union of all the alephs before it; and in general <math>\\aleph_{\\lambda}</math> for any [[limit ordinal]] λ is a weak limit cardinal.\n\nThe [[beth number|ב operation]] can be used to obtain strong limit cardinals.  This operation is a map from ordinals to cardinals defined as\n:<math>\\beth_{0} = \\aleph_0,</math>\n:<math>\\beth_{\\alpha+1} = 2^{\\beth_{\\alpha}},</math> (the smallest ordinal [[equinumerous]] with the powerset)\n:If &lambda; is a limit ordinal, <math>\\beth_{\\lambda} = \\bigcup \\{ \\beth_{\\alpha} : \\alpha < \\lambda\\}.</math>\nThe cardinal\n:<math>\\beth_{\\omega} = \\bigcup \\{ \\beth_{0}, \\beth_{1}, \\beth_{2}, \\ldots \\} = \\bigcup_{n < \\omega}  \\beth_{n} </math>\nis a strong limit cardinal of [[cofinality]] &omega;. More generally, given any ordinal &alpha;, the cardinal\n:<math>\\beth_{\\alpha+\\omega} = \\bigcup_{n < \\omega} \\beth_{\\alpha+n} </math>\nis a strong limit cardinal. Thus there are arbitrarily large strong limit cardinals.\n\n== Relationship with ordinal subscripts ==\n\nIf the [[axiom of choice]] holds, every cardinal number has an [[initial ordinal]]. If that initial ordinal is <math>\\omega_{\\lambda} \\,,</math> then the cardinal number is of the form <math>\\aleph_\\lambda</math> for the same ordinal subscript &lambda;. The ordinal &lambda; determines whether <math>\\aleph_\\lambda</math> is a weak limit cardinal. Because <math>\\aleph_{\\alpha^+} = (\\aleph_\\alpha)^+ \\,,</math> if &lambda; is a successor ordinal then <math>\\aleph_\\lambda</math> is not a weak limit. Conversely, if a cardinal &kappa; is a successor cardinal, say <math>\\kappa = (\\aleph_{\\alpha})^+ \\,,</math> then <math>\\kappa = \\aleph_{\\alpha^+} \\,.</math> Thus, in general, <math>\\aleph_\\lambda</math> is a weak limit cardinal if and only if &lambda; is zero or a limit ordinal.\n\nAlthough the ordinal subscript tells whether a cardinal is a weak limit, it does not tell whether a cardinal is a strong limit. For example, [[Zermelo–Fraenkel set theory|ZFC]] proves that <math>\\aleph_\\omega</math> is a weak limit cardinal, but neither proves nor disproves that <math>\\aleph_\\omega</math> is a strong limit cardinal (Hrbacek and Jech 1999:168). The [[generalized continuum hypothesis]] states that <math>\\kappa^+ = 2^{\\kappa} \\,</math> for every infinite cardinal &kappa;. Under this hypothesis, the notions of weak and strong limit cardinals coincide.\n\n== The notion of inaccessibility and large cardinals ==\n\nThe preceding defines a notion of \"inaccessibility\": we are dealing with cases where it is no longer enough to do finitely many iterations of the successor and powerset operations; hence the phrase \"cannot be reached\" in both of the intuitive definitions above. But the \"union operation\" always provides another way of \"accessing\" these cardinals (and indeed, such is the case of limit ordinals as well).  Stronger notions of inaccessibility can be defined using [[cofinality]]. For a weak (resp. strong) limit cardinal κ the requirement that cf(κ) = κ (i.e. κ be [[regular cardinal|regular]]) so that κ cannot be expressed as a sum (union) of fewer than κ smaller cardinals. Such a cardinal is called a [[inaccessible cardinal|weakly (resp. strongly) inaccessible cardinal]]. The preceding examples both are singular cardinals of cofinality ω and hence they are not inaccessible.\n\n<math>\\aleph_0</math> would be an inaccessible cardinal of both \"strengths\" except that the definition of inaccessible requires that they be uncountable.  Standard Zermelo-Fraenkel set theory with the Axiom of Choice (ZFC) cannot even prove the consistency of the existence of an inaccessible cardinal of either kind above <math>\\aleph_0</math>, due to [[Gödel's incompleteness theorems|Gödel's Incompleteness Theorem]].  More specifically, if <math>\\kappa</math> is weakly inaccessible then <math>L_{\\kappa} \\models ZFC</math>.  These form the first in a hierarchy of [[large cardinal]]s.\n\n== See also ==\n\n* [[Cardinal number]]\n\n== References ==\n* {{citation \n  | last1= Hrbacek | first1 = Karel \n  | last2=Jech | first2=Thomas\n  | title = Introduction to Set Theory\n  | edition = 3\n  | year = 1999 | isbn = 0-8247-7915-0\n  }}\n* {{Citation | last1=Jech | first1=Thomas | author1-link=Thomas Jech | title=Set Theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=third millennium | series=Springer Monographs in Mathematics | isbn=978-3-540-44085-7 | doi=10.1007/3-540-44761-X | year=2003}}\n* {{Citation | last1=Kunen | first1=Kenneth | author1-link=Kenneth Kunen | title=Set theory: An introduction to independence proofs | publisher=[[Elsevier]] | isbn=978-0-444-86839-8 | year=1980}}\n\n==External links==\n* http://www.ii.com/math/cardinals/ Infinite ink on cardinals\n\n[[Category:Set theory]]\n[[Category:Cardinal numbers]]"
    },
    {
      "title": "Schröder–Bernstein theorem",
      "url": "https://en.wikipedia.org/wiki/Schr%C3%B6der%E2%80%93Bernstein_theorem",
      "text": "In [[set theory]], the '''Schröder–Bernstein theorem''' states that, if there exist [[injective function]]s {{math|''f'' : ''A'' → ''B''}} and {{math|''g'' : ''B'' → ''A''}} between the [[Set (mathematics)|sets]] {{math|''A''}} and {{math|''B''}}, then there exists a [[bijection|bijective]] function {{math|''h'' : ''A'' → ''B''}}. In terms of the [[cardinality]] of the two sets, this means that if {{math|{{!}}''A''{{!}}&nbsp;≤&nbsp;{{!}}''B''{{!}}}} and {{math|{{!}}''B''{{!}}&nbsp;≤&nbsp;{{!}}''A''{{!}}}}, then {{math|1={{!}}''A''{{!}}&nbsp;=&nbsp;{{!}}''B''{{!}}}}; that is, {{math|''A''}} and {{math|''B''}} are [[equipotent]]. This is a useful feature in the ordering of [[cardinal number]]s.\n\nThis theorem does not rely on the [[axiom of choice]]. However, its various proofs are [[Constructive proof|non-constructive]], as they depend on the [[law of excluded middle]], and are therefore rejected by [[intuitionist]]s.<ref>{{cite book |title=Mathematics and Logic in History and in Contemporary Thought |author=Ettore Carruccio |publisher=Transaction Publishers |year=2006 |page=354 |isbn=978-0-202-30850-0}}</ref>\n\nThe theorem is named after [[Felix Bernstein (mathematician)|Felix Bernstein]] and [[Ernst Schröder]].  It is also known as '''Cantor–Bernstein theorem''', or '''Cantor–Schröder–Bernstein''', after [[Georg Cantor]] who first published it without proof.\n\n==Proof==\n[[Image:Cantor-Bernstein.png|thumb|400px|König's definition of a bijection {{color|#00c000|''h''}}:''A''&nbsp;→&nbsp;''B'' from given example injections {{color|#c00000|''f''}}:''A''&nbsp;→&nbsp;''B'' and {{color|#0000c0|''g''}}:''B''&nbsp;→&nbsp;''A''. An element in ''A'' and ''B'' is denoted by a number and a letter, respectively. The sequence 3&nbsp;→&nbsp;e&nbsp;→&nbsp;6&nbsp;→&nbsp;... is an ''A''-stopper, leading to the definitions {{color|#00c000|''h''}}(3)&nbsp;=&nbsp;{{color|#c00000|''f''}}(3)&nbsp;=&nbsp;''e'', {{color|#00c000|''h''}}(6)&nbsp;=&nbsp;{{color|#c00000|''f''}}(6), .... The sequence ''d''&nbsp;→&nbsp;5&nbsp;→&nbsp;''f''&nbsp;→&nbsp;... is a ''B''-stopper, leading to {{color|#00c000|''h''}}(5)&nbsp;=&nbsp;{{color|#0000c0|''g''}}<sup>−1</sup>(5)&nbsp;=&nbsp;''d'', .... The sequence ...&nbsp;→&nbsp;''a''&nbsp;→&nbsp;1&nbsp;→&nbsp;''c''&nbsp;→&nbsp;4&nbsp;→&nbsp;... is doubly infinite, leading to {{color|#00c000|''h''}}(1)&nbsp;=&nbsp;{{color|#0000c0|''g''}}<sup>−1</sup>(1)&nbsp;=&nbsp;''a'', {{color|#00c000|''h''}}(4)&nbsp;=&nbsp;{{color|#0000c0|''g''}}<sup>−1</sup>(4)&nbsp;=&nbsp;''c'',&nbsp;.... The sequence ''b''&nbsp;→&nbsp;2&nbsp;→&nbsp;''b'' is cyclic, leading to {{color|#00c000|''h''}}(2)&nbsp;=&nbsp;{{color|#0000c0|''g''}}<sup>−1</sup>(2)&nbsp;=&nbsp;''b''.]]\nThe following proof is attributed to [[Julius König]].<ref>{{cite journal| author=J. König| title=Sur la théorie des ensembles| journal=Comptes Rendus Hebdomadaires des Séances de l'Académie des Sciences| volume=143| pages=110–112| year=1906| url=http://gallica.bnf.fr/ark:/12148/bpt6k30977.image.f110.langEN}}</ref>\n\nAssume without loss of generality that ''A'' and ''B'' are [[disjoint set|disjoint]]. For any ''a'' in ''A'' or ''b'' in ''B'' we can form a unique two-sided sequence of elements that are alternately in ''A'' and ''B'', by repeatedly applying <math>f</math> and <math>g^{-1}</math> to go from ''A'' to ''B'' and <math>g</math> and <math>f^{-1}</math> to go from ''B'' to ''A'' (where defined).\n\n:''<math> \\cdots \\rightarrow  f^{-1}(g^{-1}(a)) \\rightarrow g^{-1}(a) \\rightarrow   a  \\rightarrow  f(a) \\rightarrow  g(f(a)) \\rightarrow \\cdots </math>''\n\nFor any particular ''a'', this sequence may terminate to the left or not, at a point where <math>f^{-1}</math> or <math>g^{-1}</math> is not defined.\n\nBy the fact that <math>f</math> and <math>g</math> are injective functions, each ''a'' in ''A'' and ''b'' in ''B'' is in exactly one such sequence to within identity: if an element occurs in two sequences, all elements to the left and to the right must be the same in both, by the definition of the sequences. Therefore, the sequences form a [[Partition of a set|partition]] of the (disjoint) union of ''A'' and ''B''. Hence it suffices to produce a bijection between the elements of ''A'' and ''B'' in each of the sequences separately, as follows:\n\nCall a sequence an ''A-stopper'' if it stops at an element of ''A'', or a ''B-stopper'' if it stops at an element of ''B''. Otherwise, call it ''[[doubly infinite]]'' if all the elements are distinct or ''cyclic'' if it repeats. See the picture for examples.\n\n* For an ''A-stopper'', the function ''<math>f</math>'' is a bijection between its elements in ''A'' and its elements in ''B''.\n* For a ''B-stopper'', the function ''<math>g</math>'' is a bijection between its elements in ''B'' and its elements in ''A''.\n* For a ''doubly infinite'' sequence or a ''cyclic'' sequence, either ''<math>f</math>'' or ''<math>g</math>'' will do (<math>g</math> is used in the picture).\n{{Clear}}\n\n==Original proof==\nAn earlier proof by [[Georg Cantor|Cantor]] relied, in effect, on the [[axiom of choice]] by inferring the result as a [[corollary]] of the [[well-ordering theorem]].<ref>{{cite journal |author=Georg Cantor |title=Beiträge zur Begründung der transfiniten Mengenlehre (1) |url=http://gdz.sub.uni-goettingen.de/index.php?id=img&no_cache=1&IDDOC=36218&IDDOC=36218&branch=&L=1|journal=Mathematische Annalen |volume=46 |issue=4 |pages=481–512 |year=1895 |doi=10.1007/bf02124929}}<BR>\n{{cite journal |author=Georg Cantor |title=Beiträge zur Begründung der transfiniten Mengenlehre (2) |url=http://gdz.sub.uni-goettingen.de/index.php?id=11&PPN=PPN235181684_0049&DMDID=DMDLOG_0024&L=1 |journal=Mathematische Annalen |volume=49 |issue=2 |pages=207–246 |year= 1897|doi=10.1007/bf01444205 }}</ref> The argument given above shows that the result can be proved without using the axiom of choice. Note however that the principle of excluded middle is used to do the analysis into cases, so this proof does not work in non-classical logic. \n\nThere is also a proof which uses [[Knaster–Tarski theorem|Tarski's fixed point theorem]].<ref>R. Uhl, \"[http://mathworld.wolfram.com/TarskisFixedPointTheorem.html Tarski's Fixed Point Theorem]\", from ''MathWorld''–a Wolfram Web Resource, created by Eric W. Weisstein. (Example 3)</ref>\n\n== History ==\n\nThe traditional name \"Schröder–Bernstein\" is based on two proofs published independently in 1898.\nCantor is often added because he first stated the theorem in 1887,\nwhile Schröder's name is often omitted because his proof turned out to be flawed\nwhile the name of [[Richard Dedekind]], who first proved it, is not connected with the theorem.\nAccording to Bernstein, Cantor had suggested the name ''equivalence theorem'' (Äquivalenzsatz).<ref name=\"Brieskorn.Chatterji.2002\">{{citation | author=[[Felix Hausdorff]] | editor=[[Egbert Brieskorn]] |editor2=Srishti D. Chatterji| title=Grundzüge der Mengenlehre | edition=1. | publisher=Springer | location=Berlin/Heidelberg | year=2002 | pages=587 | isbn=978-3-540-42224-2| url=https://books.google.com/books?id=3nth_p-6DpcC|display-editors=etal}} – [https://jscholarship.library.jhu.edu/handle/1774.2/34091 Original edition (1914)]</ref>\n\n[[File:CantorEquivalenceTheorem1887b.gif|thumb|right|Cantor's first statement of the theorem (1887)<ref name=\"Cantor.1932\"/>]]\n\n* '''1887''' '''Cantor''' publishes the theorem, however without proof.<ref name=\"Cantor.1932\">{{citation | author=Georg Cantor | title=Mitteilungen zur Lehre vom Transfiniten |journal=[[Zeitschrift für Philosophie und philosophische Kritik]]| volume=91 |pages=81–125 |year=1887 }}<BR>Reprinted in: {{citation | author=Georg Cantor |editor1=Adolf Fraenkel (Lebenslauf) |editor2=Ernst Zermelo | title=Gesammelte Abhandlungen mathematischen und philosophischen Inhalts| publisher= Springer | location=Berlin | year=1932 | pages=378–439 | url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN237853094&DMDID=DMDLOG_0060}} Here: p.413 bottom</ref><ref name=\"Brieskorn.Chatterji.2002\"/>\n* '''1887''' On July 11, '''Dedekind''' proves the theorem (not relying on the [[axiom of choice]])<ref>{{citation | author=Richard Dedekind | editor=[[Robert Fricke]] |editor2=Emmy Noether |editor3=Øystein Ore | title=Gesammelte mathematische Werke | volume=3 | publisher=Friedr. Vieweg & Sohn | location=Braunschweig | year=1932 | pages=447–449 (Ch.62) | url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN23569441X}}</ref> but neither publishes his proof nor tells Cantor about it. [[Ernst Zermelo]] discovered Dedekind's proof and in 1908<ref>{{citation | author=Ernst Zermelo |editor1=Felix Klein |editor2=[[Walther von Dyck]] |editor3=David Hilbert |editor4=[[Otto Blumenthal]] | title=Untersuchungen über die Grundlagen der Mengenlehre I | journal=[[Mathematische Annalen]] | volume=65 | number=2 | year=1908 | pages=261–281; here: p.271–272 | issn=0025-5831 | url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN235181684_0065&DMDID=DMDLOG_0018 | doi=10.1007/bf01449999}}</ref> he publishes his own proof based on the ''chain theory'' from Dedekind's paper ''Was sind und was sollen die Zahlen?''<ref name=\"Brieskorn.Chatterji.2002\"/><ref>{{citation | author=Richard Dedekind | title=Was sind und was sollen die Zahlen? | publisher=Friedr. Vieweg & Sohn | location=Braunschweig | year=1888 | url=http://echo.mpiwg-berlin.mpg.de/MPIWG:01MGQHHN |edition=2., unchanged (1893)}}</ref>\n* '''1895''' '''Cantor''' states the theorem in his first paper on set theory and transfinite numbers. He obtains it as an easy consequence of the linear order of cardinal numbers.<ref name=\"Cantor.1895\">{{citation | author=Georg Cantor |editor1=Adolf Fraenkel (Lebenslauf) |editor2=Ernst Zermelo | title=Gesammelte Abhandlungen mathematischen und philosophischen Inhalts | publisher= Springer | location=Berlin | year=1932 | pages=285 (\"Satz B\")| url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN237853094}}</ref><ref>{{cite journal |author=Georg Cantor |title=Beiträge zur Begründung der transfiniten Mengenlehre (1) |url=http://gdz.sub.uni-goettingen.de/index.php?id=img&no_cache=1&IDDOC=36218&IDDOC=36218&branch=&L=1|journal=[[Mathematische Annalen]] |volume=46 |issue=4 |pages=481–512 (Theorem see \"Satz B\", p.484) |year=1895 |doi=10.1007/bf02124929}}<BR>({{cite journal |author=Georg Cantor |title=Beiträge zur Begründung der transfiniten Mengenlehre (2) |url=http://gdz.sub.uni-goettingen.de/index.php?id=11&PPN=PPN235181684_0049&DMDID=DMDLOG_0024&L=1 |journal=[[Mathematische Annalen]] |volume=49 |issue=2 |pages=207–246 |year= 1897|doi=10.1007/bf01444205 }})</ref> However, he couldn't prove the latter theorem, which is shown in 1915 to be equivalent to the [[axiom of choice]] by [[Friedrich Moritz Hartogs]].<ref name=\"Brieskorn.Chatterji.2002\"/><ref>{{citation | author=Friedrich M. Hartogs |editor1=Felix Klein |editor2=Walther von Dyck |editor3=David Hilbert |editor4=Otto Blumenthal | title=Über das Problem der Wohlordnung | journal=[[Mathematische Annalen]] | volume=76 | number=4 | year=1915 | pages=438–443 | issn=0025-5831 |url=http://gdz.sub.uni-goettingen.de/index.php?id=11&PPN=PPN235181684_0076&DMDID=DMDLOG_0037&L=1 | doi=10.1007/bf01458215}}</ref>\n* '''1896''' '''Schröder''' announces a proof (as a corollary of a theorem by [[William Stanley Jevons|Jevons]]).<!---taken from 31 Jan 2014 version of [[:en:Cantor–Bernstein–Schroeder theorem#History]], without further sources, and not mentioning Jevons--- ---probably, following source was meant:---><ref>{{cite journal | author=Ernst Schröder | title=Über G. Cantorsche Sätze | journal=[[Jahresbericht der Deutschen Mathematiker-Vereinigung]] | volume=5 | pages=81–82 | url=http://gdz.sub.uni-goettingen.de/en/dms/loader/img/?PID=GDZPPN002115506 | year=1896 }}</ref>\n* '''1897''' '''Bernstein''', a 19 years old student in Cantor's Seminar, presents his proof.<ref name=\"Deiser.2010\">{{citation | author=Oliver Deiser | title=Einführung in die Mengenlehre – Die Mengenlehre Georg Cantors und ihre Axiomatisierung durch Ernst Zermelo | edition=3rd, corrected | publisher=Springer | location=Berlin/Heidelberg | year=2010 | pages=71, 501 | isbn=978-3-642-01444-4 | doi=10.1007/978-3-642-01445-1| series=Springer-Lehrbuch }}</ref><ref name=\"Suppes.1972\">{{citation | author=[[Patrick Suppes]] | title=Axiomatic Set Theory | edition=1. | publisher=Dover Publications | location=New York | year=1972 | pages=95&nbsp;f | isbn=978-0-486-61630-8 }}</ref>\n* '''1897''' Almost simultaneously, but independently, '''Schröder''' finds a proof.<ref name=\"Deiser.2010\"/><ref name=\"Suppes.1972\"/>\n* '''1897''' After a visit by Bernstein, '''Dedekind''' independently proves the theorem a second time.<!---taken from 31 Jan 2014 version of [[:en:Cantor–Bernstein–Schroeder theorem#History]], without further sources--->\n* '''1898''' '''Bernstein''''s proof (not relying on the axiom of choice) is published by [[Émile Borel]] in his book on functions.<ref>{{citation | author=Émile Borel | title=Leçons sur la théorie des fonctions | publisher=Gauthier-Villars et fils | location=Paris | year=1898 | pages=103&nbsp;ff | url=https://archive.org/stream/leconstheoriefon00borerich#page/n115/mode/2up}}</ref> (Communicated by Cantor at the 1897 [[International Congress of Mathematicians]] in Zürich.) In the same year, the proof also appears in '''Bernstein''''s dissertation.<ref>{{citation | author=Felix Bernstein | title=Untersuchungen aus der Mengenlehre | publisher=Buchdruckerei des Waisenhauses | location=Halle a.&nbsp;S. | year=1901 | url=https://archive.org/details/untersuchungena00berngoog}}<BR>Reprinted in: {{citation | author=Felix Bernstein |editor1=Felix Klein |editor2=Walther von Dyck |editor3=David Hilbert | title=Untersuchungen aus der Mengenlehre | journal=[[Mathematische Annalen]] | volume=61 | number=1 | year=1905 | pages=117–155, (Theorem see \"Satz 1\" on p.121) | issn=0025-5831  | url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN235181684_0061&DMDID=DMDLOG_0015 | doi=10.1007/bf01457734}}</ref><ref name=\"Brieskorn.Chatterji.2002\"/>\n* '''1898''' '''Schröder''' publishes his proof<ref>{{citation | author=Ernst Schröder | editor=Kaiserliche Leopoldino-Carolinische Deutsche Akademie der Naturforscher | title=Ueber zwei Definitionen der Endlichkeit und G. Cantor'sche Sätze | journal=Nova Acta | volume=71 | number=6 | year=1898 | pages=303–376 (proof: p.336–344) | url=https://www.biodiversitylibrary.org/item/45265#page/331/mode/1up}}</ref> which, however, is shown to be faulty by [[Alwin Reinhold Korselt]] in 1902 (just before Schröder's death),<ref>{{citation | author=Alwin R. Korselt |editor1=Felix Klein |editor2=Walther von Dyck |editor3=David Hilbert |editor4=Otto Blumenthal | title=Über einen Beweis des Äquivalenzsatzes | journal=[[Mathematische Annalen]] | volume=70 | number=2 | year=1911 | pages=294–296 | issn=0025-5831 | url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN235181684_0070&DMDID=DMDLOG_0029 | doi=10.1007/bf01461161}}</ref> (confirmed by Schröder),<ref name=\"Brieskorn.Chatterji.2002\"/><ref>Korselt (1911), p.295</ref>, but Korselt's paper is published only in 1911.\n\nBoth proofs of Dedekind are based on his famous memoir ''Was sind und was sollen die Zahlen?'' and derive it as a corollary of a proposition equivalent to statement C in Cantor's paper,<ref name=\"Cantor.1895\"/> which reads ''A''&nbsp;⊆&nbsp;''B''&nbsp;⊆&nbsp;''C'' and |''A''|&nbsp;=&nbsp;|''C''| implies |''A''|&nbsp;=&nbsp;|''B''|&nbsp;=&nbsp;|''C''|. Cantor observed this property as early as 1882/83 during his studies in set theory and transfinite numbers and was therefore (implicitly) relying on the [[Axiom of Choice]].<!---taken from 31 Jan 2014 version of [[:en:Cantor–Bernstein–Schroeder theorem#History]], without further sources--->\n\n== Statement C and a transparent proof ==\nStatement C  is the special case of the Schröder–Bernstein theorem where the second function ''g'' is the identity (and, hence, the second set ''B'' is a subset of ''A''). \n\nIt is easy to see that statement C implies the general form of the theorem (with ''g'' and ''B'' arbitrary):\n\nAssume that ''ƒ'' injects ''A'' into ''B'' and ''g'' injects ''B'' into ''A''. Then their composition ''gf'' injects ''A'' into ''g''[''B'']. However, ''g''[''B''] is a subset of ''A''. Thus, from statement ''C'', we obtain that ''A'' and ''g''[''B''] are equipotent. Obviously, ''B'' and ''g''[''B''] are equipotent as well. It follows, that ''A'' and ''B'' are equipotent.\n\nA proof of statement ''C'' can be obtained by translating the König proof for the general case above to the present situation. The result becomes much more transparent than the original, and looks as follows.\n\nAssume that ''ƒ'' injects ''A'' into its subset ''C''. Consider the subset ''D'' of ''A'' (the set of ''A''-stoppers, in the König terminology above) which is the union of the infinitely many sets ''A''&nbsp;−&nbsp;''C'', ''ƒ''[''A''&nbsp;−&nbsp;''C''], ''ƒƒ''[''A''&nbsp;−&nbsp;''C'']],&nbsp;...\n\nConsider the function from ''A'' to ''C'' that (i) maps elements ''a'' of ''D'' to their ''ƒ''-image ''ƒ''(''a''), whereas (ii) on ''A''&nbsp;−&nbsp;''D'' it acts as the identity, mapping elements to themselves.\n\nWe claim that this function is a bijection from ''A'' onto ''C''. For this, it should be verified that every element of ''C'' has exactly one original. However, if it belongs to ''D'', it has one original which also is an ''f''-original, and if it doesn't belong to ''D'' the only original is the element itself.\n\n== See also ==\n* [[Myhill isomorphism theorem]]\n* [[Schröder–Bernstein theorem for measurable spaces]]\n* [[Schröder–Bernstein theorems for operator algebras]]\n* [[Schröder–Bernstein property]]\n\n== Notes ==\n{{Reflist|30em}}\n\n== References ==\n* [[Martin Aigner]] & [[Gunter M. Ziegler]] (1998) ''Proofs from THE BOOK'', § 3 Analysis: Sets and functions, [[Springer books]] {{mr|id=1723092}}, fifth edition 2014 {{mr|id=3288091}}, sixth edition 2018 {{mr|id=3823190}}\n* {{citation|mr=3026479|last=Hinkis|first= Arie\n|title=Proofs of the Cantor-Bernstein theorem. A mathematical excursion|series= Science Networks. Historical Studies\n|volume= 45|publisher= Birkhäuser/Springer|place= Heidelberg|year= 2013\n|isbn= 978-3-0348-0223-9|doi=10.1007/978-3-0348-0224-6 }}\n* Míchaél Ó Searcóid (2013) \"On the history and mathematics of the equivalence theorem\", [[Mathematical Proceedings of the Royal Irish Academy]] 113A: 151–68 {{doi|10.3311/PRIA.2013.113.14}} [https://www.jstor.org/stable/42912521 Jstor link]\n\n==External links==\n*{{MathWorld|title=Schröder-Bernstein Theorem|urlname=Schroeder-BernsteinTheorem}}\n*{{nlab|id=Cantor-Schroeder-Bernstein+theorem|title=Cantor-Schroeder-Bernstein theorem}}\n* [https://link.springer.com/content/pdf/10.1007%2Fs00283-011-9242-3.pdf Cantor-Bernstein’s Theorem in a Semiring] by Marcel Crabbé.\n*{{Citizendium|title=Schröder-Bernstein_theorem}}\n\n{{DEFAULTSORT:Schroeder-Bernstein theorem}}\n[[Category:Theorems in the foundations of mathematics]]\n[[Category:Cardinal numbers]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Singular cardinals hypothesis",
      "url": "https://en.wikipedia.org/wiki/Singular_cardinals_hypothesis",
      "text": "{{short description|set theory concept}}\nIn [[set theory]], the '''singular cardinals hypothesis (SCH)''' arose from the question of whether the least [[cardinal number]] for which the [[Continuum hypothesis#The generalized continuum hypothesis|generalized continuum hypothesis]] (GCH) might fail could be a [[singular cardinal]].\n\nAccording to Mitchell (1992), the singular cardinals hypothesis is:\n:If &kappa; is any singular [[limit cardinal|strong limit cardinal]], then 2<sup>&kappa;</sup> = &kappa;<sup>+</sup>.\nHere, &kappa;<sup>+</sup> denotes the [[successor cardinal]] of &kappa;.\n\nSince SCH is a consequence of GCH which is known to be [[consistent]] with [[Zermelo–Fraenkel set theory|ZFC]], SCH is consistent with ZFC.  The negation of SCH has also been shown to be consistent with ZFC, if one assumes the existence of a sufficiently large cardinal number.  In fact, by results of [[Moti Gitik]], ZFC + the negation of SCH is equiconsistent with ZFC + the existence of a measurable cardinal &kappa; of [[Mitchell order]] &kappa;<sup>++</sup>.\n\nAnother form of the SCH is the following statement:\n:2<sup>cf(&kappa;)</sup> < &kappa; implies &kappa;<sup>cf(&kappa;)</sup> = &kappa;<sup>+</sup>,\nwhere cf denotes the [[cofinality]] function.  Note that &kappa;<sup>cf(&kappa;)</sup>= 2<sup>&kappa;</sup>  for all singular strong limit cardinals &kappa;. The second formulation of SCH is strictly stronger than the first version, since the first one only mentions strong limits; from a model in which the first version of SCH fails at &alefsym;<sub>&omega;</sub> and GCH holds above &alefsym;<sub>&omega;+2</sub>, we can construct a model in which the first version of SCH holds but the second version of SCH fails, by adding &alefsym;<sub>&omega;</sub> Cohen subsets to &alefsym;<sub>n</sub> for some n.\n\n[[Jack Silver|Silver]] proved that if &kappa; is singular with uncountable cofinality and 2<sup>&lambda;</sup> = &lambda;<sup>+</sup> for all infinite cardinals &lambda; < &kappa;, then 2<sup>&kappa;</sup> = &kappa;<sup>+</sup>.  Silver's original proof used [[generic ultrapowers]].  The following important fact follows from Silver's theorem:  if the singular cardinals hypothesis holds for all singular cardinals of countable cofinality, then it holds for all singular cardinals.  In particular, then, if <math> \\kappa </math> is the least counterexample to the singular cardinals hypothesis, then <math> cf(\\kappa) = \\omega </math>.\n\nThe negation of the singular cardinals hypothesis is intimately related to violating the GCH at a measurable cardinal.  A well-known result of [[Dana Scott]] is that if the GCH holds below a measurable cardinal <math> \\kappa </math> on a set of measure one—i.e., there is normal <math> \\kappa </math> -complete ultrafilter D on <math> \\mathcal{P}(\\kappa) </math> such that <math> \\{\\alpha < \\kappa: 2^{\\alpha} = \\alpha^+\\}\\in D </math>, then <math> 2^\\kappa = \\kappa^+ </math>.  Starting with <math> \\kappa </math> a [[supercompact cardinal]], Silver was able to produce a model of set theory in which <math> \\kappa </math> is measurable and in which <math> 2^\\kappa > \\kappa^+ </math>.  Then, by applying [[Prikry forcing]] to the measurable <math> \\kappa </math>, one gets a model of set theory in which <math> \\kappa </math> is a strong limit cardinal of countable cofinality and in which <math> 2^\\kappa > \\kappa^+ </math>—a violation of the SCH.  [[Moti Gitik|Gitik]], building on work of [[W. Hugh Woodin|Woodin]], was able to replace the supercompact in Silver's proof with a measurable of Mitchell order <math> \\kappa^{++} </math>.  That established an upper bound for the consistency strength of the failure of the SCH.  Gitik again, using results of [[Inner model theory]], was able to show that a measurable of Mitchell order <math> \\kappa^{++} </math> is also the lower bound for the consistency strength of the failure of SCH.\n\nA wide variety of propositions imply SCH.  As was noted above, GCH implies SCH.  On the other hand, the [[proper forcing axiom]] which implies <math> 2^{\\aleph_0} = \\aleph_2 </math> and hence is incompatible with GCH also implies SCH.  [[Robert M. Solovay|Solovay]] showed that large cardinals almost imply SCH—in particular, if <math> \\kappa </math> is [[strongly compact cardinal]], then the SCH holds above <math> \\kappa </math>.  On the other hand, the non-existence of (inner models for) various large cardinals (below a measurable of Mitchell order <math> \\kappa^{++} </math>) also imply SCH.\n\n==References==\n* [[Thomas Jech|T. Jech]]: [http://matwbn.icm.edu.pl/ksiazki/fm/fm81/fm8116.pdf Properties of the gimel function and a classification of singular cardinals], Fundamenta Mathematicae 81 (1974): 57-64. \n* William J. Mitchell, \"On the singular cardinal hypothesis,\" ''Trans. Amer. Math. Soc.'', volume 329 (2): pp. 507&ndash;530, 1992.\n* Jason Aubrey, ''The Singular Cardinals Problem'' ([https://web.archive.org/web/20060717043824/http://www.math.lsa.umich.edu/vigre/Expositions/Aubrey.pdf PDF]), VIGRE expository report, Department of Mathematics, University of Michigan.\n\n[[Category:Cardinal numbers]]"
    },
    {
      "title": "Strong partition cardinal",
      "url": "https://en.wikipedia.org/wiki/Strong_partition_cardinal",
      "text": "In [[Zermelo–Fraenkel set theory]] without the [[axiom of choice]] a '''strong partition cardinal''' is an [[uncountable]] [[well-ordered]] [[cardinality|cardinal]] <math>k</math> such that every [[partition (set theory)|partition]] of the set <math>[k]^k</math>of size <math>k</math> subsets of <math>k</math> into less than <math>k</math> pieces has a [[Homogeneous (large cardinal property)| homogeneous set]] of size <math>k</math>.\n\nThe existence of strong partition cardinals contradicts the axiom of choice. The [[Axiom of determinacy]] implies that ℵ<sub>1</sub> is a strong partition cardinal.\n\n==References==\n*{{citation|first=James M.|last= Henle|first2= Eugene M.|last2= Kleinberg|first3= Ronald J. |last3=Watro|title=On the ultrafilters and ultrapowers of strong partition cardinals|journal=[[Journal of Symbolic Logic]]|volume= 49|issue= 4 |year=1984|pages= 1268–1272.|doi=10.2307/2274277|jstor=2274277}} \n*{{citation|first1=Arthur W.|last1=Apter|first2=James M.|last2=Henle|first3=Stephen C.|last3=Jackson|title=The calculus of partition sequences, changing cofinalities, and a question of Woodin|journal=[[Transactions of the American Mathematical Society]]|volume=352|issue=3|year=1999|pages=969–1003|jstor=118097|mr=1695015|doi=10.1090/S0002-9947-99-02554-4 }}.\n\n[[Category:Cardinal numbers]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Successor cardinal",
      "url": "https://en.wikipedia.org/wiki/Successor_cardinal",
      "text": "In the theory of [[Cardinal number|cardinal numbers]], we can define a '''successor''' operation similar to that in the [[ordinal number]]s. This coincides with the ordinal successor operation for finite cardinals, but in the infinite case they diverge because every infinite ordinal and its successor have the same [[cardinality]] (a [[bijection]] can be set up between the two by simply sending the last element of the successor to 0, 0 to 1, etc., and fixing ω and all the elements above; in the style of Hilbert's [[Hilbert's paradox of the Grand Hotel|Hotel Infinity]]). Using the [[von Neumann cardinal assignment]] and the [[axiom of choice]] (AC), this successor operation is easy to define: for a cardinal number κ we have\n\n:<math>\\kappa^+ = |\\inf \\{ \\lambda \\in ON \\ |\\ \\kappa < |\\lambda| \\}|</math> ,\n\nwhere ON is the class of ordinals.  That is, the successor cardinal is the cardinality of the least ordinal into which a set of the given cardinality can be mapped one-to-one, but which cannot be mapped one-to-one back into that set.\n\nThat the set above is nonempty follows from  [[Hartogs number|Hartogs' theorem]], which says that for any [[well-order]]able cardinal, a larger such cardinal is constructible. The minimum actually exists because the ordinals are well-ordered. It is therefore immediate that there is no cardinal number in between κ and κ<sup>+</sup>. A '''successor cardinal''' is a cardinal which is κ<sup>+</sup> for some cardinal κ. In the infinite case, the successor operation skips over many ordinal numbers; in fact, every infinite cardinal is a [[limit ordinal]]. Therefore, the successor operation on cardinals gains a lot of power in the infinite case (relative the ordinal successorship operation), and consequently the cardinal numbers are a very \"sparse\" subclass of the ordinals. We define the sequence of [[aleph number|alephs]] (via the [[axiom of replacement]]) via this operation, through all the ordinal numbers as follows:\n\n:<math>\\aleph_0 = \\omega</math>\n:<math>\\aleph_{\\alpha+1} = \\aleph_{\\alpha}^+</math>\n\nand for λ an infinite limit ordinal,\n\n:<math>\\aleph_{\\lambda} = \\bigcup_{\\beta < \\lambda} \\aleph_\\beta</math>\n\nIf β is a [[successor ordinal]], then <math>\\aleph_{\\beta}</math> is  a successor cardinal. Cardinals which are not successor cardinals are called [[limit cardinal]]s; and by the above definition, if λ is a limit ordinal, then <math>\\aleph_{\\lambda}</math> is a limit cardinal.\n\nThe standard definition above is restricted to the case when the cardinal can be well-ordered, i.e. is finite or an aleph.  Without the axiom of choice, there are cardinals which cannot be well-ordered.  Some mathematicians have defined the successor of such a cardinal as the cardinality of the least ordinal which cannot be mapped one-to-one into a set of the given cardinality.  That is:\n\n:<math>\\kappa^+ = |\\inf \\{ \\lambda \\in ON \\ |\\ |\\lambda| \\nleq \\kappa \\}|</math> .\n\n==See also==\n\n*[[Cardinal assignment]]\n\n== References ==\n*[[Paul Halmos]], ''Naive set theory''. Princeton, NJ: D. Van Nostrand Company, 1960. Reprinted by Springer-Verlag, New York, 1974. {{isbn|0-387-90092-6}} (Springer-Verlag edition).\n*[[Thomas Jech|Jech, Thomas]], 2003. ''Set Theory: The Third Millennium Edition, Revised and Expanded''.  Springer.  {{isbn|3-540-44085-2}}.\n*[[Kenneth Kunen|Kunen, Kenneth]], 1980. ''Set Theory: An Introduction to Independence Proofs''. Elsevier.  {{isbn|0-444-86839-9}}.\n\n[[Category:Set theory]]\n[[Category:Cardinal numbers]]"
    },
    {
      "title": "Suslin cardinal",
      "url": "https://en.wikipedia.org/wiki/Suslin_cardinal",
      "text": "In [[mathematics]], a [[cardinal number|cardinal]] λ < [[Θ (set theory)|Θ]] is a '''Suslin cardinal''' if there exists a set P ⊂ 2<sup>ω</sup> such that P is [[λ-Suslin]] but P is not λ'-Suslin for any λ' < λ. It is named after the [[Russia]]n [[mathematician]]\n[[Mikhail Yakovlevich Suslin]] (1894–1919).\n\n==See also==\n*[[Suslin representation]]\n*[[Suslin line]]\n*[[AD+]]\n\n==References==\n* Howard Becker, ''The restriction of a Borel equivalence relation to a sparse set'', Arch. Math. Logic 42, 335–347 (2003), {{DOI|10.1007/s001530200142}}\n\n[[Category:Cardinal numbers]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Tarski's theorem about choice",
      "url": "https://en.wikipedia.org/wiki/Tarski%27s_theorem_about_choice",
      "text": "In [[mathematics]], '''Tarski's theorem''', proved by {{harvs|txt|first=Alfred|last= Tarski|authorlink=Alfred Tarski|year=1924}}, states that in [[Zermelo–Fraenkel set theory|ZF]] the theorem \"For every infinite set <math>A</math>, there is a [[bijective map]] between the sets  <math>A</math> and <math>A\\times A</math>\" implies the [[axiom of choice]]. The opposite direction was already known, thus the theorem and axiom of choice are equivalent.\n\nTarski told {{harvs|first=Jan|last=Mycielski|authorlink=Jan Mycielski|year=2006|txt}} that when he tried to publish the theorem in [[Comptes Rendus de l'Académie des Sciences]] Paris, [[Maurice René Fréchet|Fréchet]] and [[Henri Lebesgue|Lebesgue]] refused to present it. Fréchet wrote that an implication between two well known propositions is not a new result. Lebesgue wrote that an implication between two false propositions is of no interest.\n\n== Proof ==\nOur goal is to prove that the axiom of choice is implied by the statement \"For every infinite set <math>A</math>: <math>|A|=|A\\times A|</math>\".\nIt is known that the [[well-ordering theorem]] is equivalent to the axiom of choice, thus it is enough to show that the statement implies that for every set <math>B</math> there exist a [[well-order]].\n\nFor finite sets it is trivial, thus we will assume that <math>B</math> is infinite.\n\nSince the collection of all [[Ordinal number|ordinals]] such that there exist a [[surjective function]] from <math>B</math> to the ordinal is a set, there exist a minimal non-zero ordinal, <math>\\beta</math>, such that there is no [[surjective function]] from <math>B</math> to <math>\\beta</math>.\nWe assume [[without loss of generality]] that the sets <math>B</math> and <math>\\beta</math> are [[Disjoint sets|disjoint]].\nBy our initial assumption, <math>|B \\cup \\beta|=|(B \\cup \\beta) \\times (B \\cup \\beta)|</math>, thus there exists a [[bijection]] <math>f: B \\cup \\beta \\to (B \\cup \\beta) \\times (B \\cup \\beta)</math>.\n\nFor every <math> x \\in B</math>, it is impossible that <math> \\beta \\times \\{x\\} \\subseteq f[B]</math>, because otherwise we could define a surjective function from <math>B</math> to <math>\\beta</math>.\nTherefore, there exists at least one ordinal <math>\\gamma \\in \\beta</math>, such that <math> f(\\gamma) \\in \\beta \\times \\{x\\}</math>, thus the set <math> S_x=\\{\\gamma | f(\\gamma) \\in \\beta \\times \\{x\\}\\}</math> is not empty.\n\nWith this fact in our mind we can define a new function: <math> g(x)=\\min S_x</math>.\nThis function is well defined since <math>S_x</math> is a non-empty set of ordinals, hence it has a minimum.\nRecall that for every <math>x,y \\in B, x \\neq y</math> the sets <math>S_x</math> and <math> S_y</math> are disjoint.\nTherefore, we can define a well order on <math>B</math>, for every <math>x, y \\in B</math> we shall define <math>x \\leq y \\iff g(x) \\leq g(y)</math>, since the image of <math>g</math>, i.e. <math>g[B]</math>, is a set of ordinals and therefore well ordered.\n\n==References==\n*{{citation|first=Herman |last=Rubin|first2= Jean E.|last2= Rubin|author2-link= Jean E. Rubin |title= Equivalents of the Axiom of Choice II|publisher= North Holland/Elsevier|year= 1985|isbn=0-444-87708-8}}\n*{{citation|title=A system of axioms of set theory for the rationalists|journal=[[Notices of the American Mathematical Society]]|volume=53|issue=2|pages=209|year=2006|first=Jan|last=Mycielski|authorlink=Jan Mycielski|url=http://www.ams.org/notices/200602/fea-mycielski.pdf}}\n*{{citation|last=Tarski|first= A.|title= Sur quelques theorems qui equivalent a l'axiome du choix|journal=[[Fundamenta Mathematicae]] |volume= 5 |year=1924|pages= 147-154 |url=http://pldml.icm.edu.pl/pldml/element/bwmeta1.element.bwnjournal-article-fmv5i1p18bwm}}\n\n[[Category:Set theory]]\n[[Category:Theorems in the foundations of mathematics]]\n[[Category:Axiom of choice]]\n[[Category:Cardinal numbers]]\n\n[[fr:Ordinal de Hartogs#Produit cardinal]]"
    },
    {
      "title": "Tav (number)",
      "url": "https://en.wikipedia.org/wiki/Tav_%28number%29",
      "text": "In his work on [[set theory]], [[Georg Cantor]] denoted the collection of all [[cardinal number]]s by the last letter of the [[Hebrew alphabet|Hebrew]] [[alphabet]], '''{{lang|he|ת}}''' (transliterated as '''Tav''', '''Taw''', or '''Sav'''.)  As Cantor realized, this collection could not itself have a cardinality, as this would lead to a paradox of the [[Burali-Forti paradox|Burali-Forti]] type.  Cantor instead said that it was an \"inconsistent\" collection which was [[Absolute Infinite|absolutely infinite]].{{#tag:ref|''Gesammelte Abhandlungen'',<ref>''Gesammelte Abhandlungen mathematischen und philosophischen Inhalts'', Georg Cantor, ed. Ernst Zermelo, with biography by Adolf Fraenkel; orig. pub. Berlin: Verlag von [[Julius Springer]], 1932; reprinted Hildesheim: Georg Olms, 1962, and Berlin: [[Springer-Verlag]], 1980, {{isbn|3-540-09849-6}}.</ref> Georg Cantor, ed. Ernst Zermelo, Hildesheim: Georg Olms Verlagsbuchhandlung, 1962, pp. 443&ndash;447; translated into English in ''From Frege to Gödel: A Source Book in Mathematical Logic, 1879-1931'', ed. Jean van Heijenoort, Cambridge, Massachusetts: Harvard University Press, 1967, pp. 113&ndash;117.  These references both purport to be a letter from Cantor to [[Richard Dedekind|Dedekind]], dated July 28, 1899.  However, as [[Ivor Grattan-Guinness]] has discovered,<ref>[http://www.digizeitschriften.de/home/services/pdfterms/?ID=516899 The Rediscovery of the Cantor-Dedekind Correspondence], I. Grattan-Guinness, ''Jahresbericht der Deutschen Mathematiker-Vereinigung'' '''76''' (1974/75), pp. 104&ndash;139, at p. 126 ff.</ref> this is in fact an amalgamation by Cantor's editor, [[Ernst Zermelo]], of two letters from Cantor to Dedekind, the first dated July 28 and the second dated August 3.|name=GesammelteAbhandlungen}}<ref>[http://www.digizeitschriften.de/home/services/pdfterms/?ID=516934 The Correspondence between Georg Cantor and Philip Jourdain], I. Grattan-Guinness, ''Jahresbericht der Deutschen Mathematiker-Vereinigung'' '''73''' (1971/72), pp. 111&ndash;130, at pp. 116&ndash;117.</ref>\n\n== See also ==\n* [[Taw (letter)]]\n* [[Aleph number]]\n* [[Absolute Infinite]]\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Tav (Number)}}\n[[Category:Cardinal numbers]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Θ (set theory)",
      "url": "https://en.wikipedia.org/wiki/%CE%98_%28set_theory%29",
      "text": "{{unreferenced|date=March 2014}}\nIn [[set theory]], '''Θ''' (pronounced like the letter [[theta]]) is the least nonzero [[ordinal number|ordinal]] α such that there is no [[surjection]] from the reals onto α. \n\nIf the [[axiom of choice]] (AC) holds (or even if the reals can be [[wellordered]]), then Θ is simply <math>(2^{\\aleph_0})^+</math>, the cardinal successor of the [[cardinality of the continuum]]. However, Θ is often studied in contexts where the axiom of choice fails, such as [[model theory|models]] of the [[axiom of determinacy]]. \n\nΘ is also the [[supremum]] of the lengths of all [[prewellordering]]s of the reals. {{Citation needed|date=March 2014}}\n\n==Proof of existence==\nIt may not be obvious that it can be proven, without using AC, that there even exists a nonzero ordinal onto which there is no surjection from the reals (if there is such an ordinal, then there must be a least one because the ordinals are wellordered). However, suppose there were no such ordinal. Then to every ordinal α we could associate the set of all prewellorderings of the reals having length α. This would give an [[injective function|injection]] from the [[class (set theory)|class]] of all ordinals into the set of all sets of orderings on the reals (which can to be seen to be a set via repeated application of the [[axiom of power set|powerset axiom]]). Now the [[axiom of replacement]] shows that the class of all ordinals is in fact a set. But that is impossible, by the [[Burali-Forti paradox]].{{Citation needed|date=March 2014}}\n\n{{DEFAULTSORT:Theta (Set Theory)}}\n[[Category:Cardinal numbers]]\n[[Category:Descriptive set theory]]\n[[Category:Determinacy]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Uncountable set",
      "url": "https://en.wikipedia.org/wiki/Uncountable_set",
      "text": "{{short description|infinite set too large to be countable}}\n{{Redirect|Uncountable|the linguistic concept|Uncountable noun}}\n\nIn [[mathematics]], an '''uncountable set''' (or '''uncountably infinite set''')<ref>[http://mathworld.wolfram.com/UncountablyInfinite.html Uncountably Infinite — from Wolfram MathWorld]</ref> is an infinite [[Set (mathematics)|set]] that contains too many [[Element (mathematics)|elements]] to be [[countable set|countable]]. The uncountability of a set is closely related to its [[cardinal number]]: a set is uncountable if its cardinal number is larger than that of the set of all [[natural number]]s.\n\n==Characterizations==\n\nThere are many equivalent characterizations of uncountability.  A set ''X'' is uncountable if and only if any of the following conditions hold:\n* There is no [[injective function]] from ''X'' to the set of natural numbers.\n* ''X'' is nonempty and for every ω-[[sequence]] of elements of ''X'', there exist at least one element of X not included in it. That is, ''X'' is nonempty and there is no [[surjective function]] from the natural numbers to ''X''.\n* The [[cardinality]] of ''X'' is neither finite nor equal to <math>\\aleph_0</math> ([[aleph number|aleph-null]], the cardinality of the [[natural number]]s).  \n* The set ''X'' has cardinality strictly greater than <math>\\aleph_0</math>.\n\nThe first three of these characterizations can be proven equivalent in [[Zermelo–Fraenkel set theory]] without the [[axiom of choice]], but the equivalence of the third and fourth cannot be proved without additional choice principles.\n\n==Properties==\n* If an uncountable set ''X'' is a subset of set ''Y'', then ''Y'' is uncountable.\n\n== Examples ==\n\nThe best known example of an uncountable set is the set '''R''' of all [[real number]]s; [[Cantor's diagonal argument]] shows that this set is uncountable. The diagonalization proof technique can also be used to show that several other sets are uncountable, such as the set of all infinite [[sequence]]s of [[natural number]]s  and the set of all [[subset]]s of the set of natural numbers. The cardinality of '''R''' is often called the [[cardinality of the continuum]] and denoted by <math>\\mathfrak{c} </math>, or <math>2^{\\aleph_0}</math>, or <math>\\beth_1</math> ([[cardinality of the continuum|beth-one]]).\n\nThe [[Cantor set]] is an uncountable subset of '''R'''.  The Cantor set is a [[fractal]] and has [[Hausdorff dimension]] greater than zero but less than one ('''R''' has dimension one). This is an example of the following fact: any subset of '''R''' of Hausdorff dimension strictly greater than zero must be uncountable.\n\nAnother example of an uncountable set is the set of all [[Function (mathematics)|function]]s from '''R''' to '''R'''. This set is even \"more uncountable\" than '''R''' in the sense that the cardinality of this set is <math>\\beth_2</math> ([[beth two|beth-two]]), which is larger than <math>\\beth_1</math>.\n\nA more abstract example of an uncountable set is the set of all countable [[ordinal number]]s, denoted by Ω or ω<sub>1</sub>. The cardinality of Ω is denoted <math>\\aleph_1</math> ([[aleph number|aleph-one]]). It can be shown, using the [[axiom of choice]], that <math>\\aleph_1</math> is the ''smallest'' uncountable cardinal number.  Thus either <math>\\beth_1</math>, the cardinality of the reals, is equal to <math>\\aleph_1</math> or it is strictly larger. [[Georg Cantor]] was the first to propose the question of whether <math>\\beth_1</math> is equal to <math>\\aleph_1</math>.  In 1900, [[David Hilbert]] posed this question as the first of his [[Hilbert's problems|23 problems]]. The statement that <math>\\aleph_1 = \\beth_1</math> is now called the [[continuum hypothesis]] and is known to be independent of the [[Zermelo–Fraenkel axioms]] for [[set theory]] (including the axiom of choice).\n\n==Without the axiom of choice==\n{{main|Dedekind-infinite set}}\nWithout the [[axiom of choice]], there might exist cardinalities [[Comparability|incomparable]] to <math>\\aleph_0</math> (namely, the cardinalities of [[Dedekind-finite]] infinite sets). Sets of these cardinalities satisfy the first three characterizations above but not the fourth characterization. Because these sets are not larger than the natural numbers in the sense of cardinality, some may not want to call them uncountable.\n\nIf the axiom of choice holds, the following conditions on a cardinal <math>\\kappa</math> are equivalent:\n*<math>\\kappa \\nleq \\aleph_0;</math>\n*<math>\\kappa > \\aleph_0;</math> and\n*<math>\\kappa \\geq \\aleph_1</math>, where <math>\\aleph_1 = |\\omega_1 |</math> and <math>\\omega_1</math> is least [[initial ordinal]] greater than <math>\\omega.</math>\n\nHowever, these may all be different if the axiom of choice fails. So it is not obvious which one is the appropriate generalization of \"uncountability\" when the axiom fails. It may be best to avoid using the word in this case and specify which of these one means.\n\n==See also==\n*[[Aleph number]]\n*[[Beth number]]\n*[[Injective function]]\n*[[Natural number]]\n\n== References ==\n{{Reflist}}\n*[[Paul Halmos|Halmos, Paul]], ''[[Naive Set Theory (book)|Naive Set Theory]]''. Princeton, NJ: D. Van Nostrand Company, 1960. Reprinted by Springer-Verlag, New York, 1974. {{isbn|0-387-90092-6}} (Springer-Verlag edition). Reprinted by Martino Fine Books, 2011. {{isbn|978-1-61427-131-4}} (Paperback edition).\n*{{Citation|last=Jech|first=Thomas|authorlink=Thomas Jech|year=2002|title=Set Theory|edition=3rd millennium|series=Springer Monographs in Mathematics|publisher=Springer|isbn=3-540-44085-2}}\n\n==External links==\n*[http://www.apronus.com/math/uncountable.htm Proof that '''R''' is uncountable]\n\n{{Mathematical logic}}\n{{Set theory}}\n\n[[Category:Basic concepts in infinite set theory]]\n[[Category:Infinity]]\n[[Category:Cardinal numbers]]"
    },
    {
      "title": "Large cardinal",
      "url": "https://en.wikipedia.org/wiki/Large_cardinal",
      "text": "{{short description|set theory concept}}\nIn the mathematical field of [[set theory]], a '''large cardinal property''' is a certain kind of property of [[Transfinite number|transfinite]] [[cardinal number]]s. Cardinals with such properties are, as the name suggests, generally very \"large\" (for example, bigger than the least α such that α=ω<sub>α</sub>). The proposition that such cardinals exist cannot be proved in the most common [[axiomatization]] of set theory, namely [[ZFC]], and such propositions can be viewed as ways of measuring how \"much\", beyond ZFC, one needs to assume to be able to prove certain desired results. In other words, they can be seen, in [[Dana Scott]]'s phrase, as quantifying the fact \"that if you want more you have to assume more\".<ref>{{cite book|last=Bell|first=J.L.|title=Boolean-Valued Models and Independence Proofs in Set Theory|pages=viii|publisher=Oxford University Press|year=1985|isbn=0-19-853241-5|nopp=true}}</ref>\n\nThere is a rough convention that results provable from ZFC alone may be stated without hypotheses, but that if the proof requires other assumptions (such as the existence of large cardinals), these should be stated. Whether this is simply a linguistic convention, or something more, is a controversial point among distinct philosophical schools (see [[#Motivations and epistemic status|Motivations and epistemic status]] below).\n\nA '''{{vanchor|large cardinal axiom}}''' is an axiom stating that there exists a cardinal (or perhaps many of them) with some specified large cardinal property.\n\nMost working set theorists believe that the large cardinal axioms that are currently being considered are [[consistent]] with ZFC. These axioms are strong enough to imply the consistency of ZFC. This has the consequence (via [[Gödel's incompleteness theorem|Gödel's second incompleteness theorem]]) that their consistency with ZFC cannot be proven in ZFC (assuming ZFC is consistent).\n\nThere is no generally agreed precise definition of what a large cardinal property is, though essentially everyone agrees that those in the [[list of large cardinal properties]] are large cardinal properties.\n\n==Partial definition==\nA necessary condition for a property of cardinal numbers to be a ''large cardinal property'' is that the existence of such a cardinal is not known to be inconsistent with [[ZFC]] and it has been proven that if ZFC is [[consistent]], then ZFC + \"no such cardinal exists\" is consistent.\n\n==Hierarchy of consistency strength==\nA remarkable observation about large cardinal axioms is that they appear to occur in strict [[linear order]] by [[consistency strength]]. That is, no exception is known to the following: Given two large cardinal axioms A1 and A2, exactly one of three things happens:\n#ZFC proves \"ZFC+A1 is consistent if and only if ZFC+A2 is consistent\";\n#ZFC+A1 proves that ZFC+A2 is consistent; or\n#ZFC+A2 proves that ZFC+A1 is consistent.\nThese are mutually exclusive, unless one of the theories in question is actually inconsistent.\n\nIn case 1 we say that A1 and A2 are [[Equiconsistency|equiconsistent]]. In case 2, we say that A1 is consistency-wise stronger than A2 (vice versa for case 3). If A2 is stronger than A1, then ZFC+A1 cannot prove ZFC+A2 is consistent, even with the additional hypothesis that ZFC+A1 is itself consistent (provided of course that it really is). This follows from [[Gödel's second incompleteness theorem]].\n\nThe observation that large cardinal axioms are linearly ordered by consistency strength is just that, an observation, not a theorem. (Without an accepted definition of large cardinal property, it is not subject to proof in the ordinary sense). Also, it is not known in every case which of the three cases holds.  [[Saharon Shelah]] has asked, \"[i]s there some theorem explaining this, or is our vision just more uniform than we realize?\" [[W. Hugh Woodin|Woodin]], however, deduces this from the [[&Omega;-conjecture]], the main unsolved problem of his [[Ω-logic]].  It is also noteworthy that many combinatorial statements are exactly equiconsistent with some large cardinal rather than, say, being intermediate between them.\n\nIt should also be noted that the order of consistency strength is not necessarily the same as the order of the size of the smallest witness to a large cardinal axiom. For example, the existence of a [[huge cardinal]] is much stronger, in terms of consistency strength, than the existence of a [[supercompact cardinal]], but assuming both exist, the first huge is smaller than the first supercompact.\n\n==Motivations and epistemic status==\nLarge cardinals are understood in the context of the [[von Neumann universe]] V, which is built up by [[transfinite induction|transfinitely iterating]] the [[powerset]] operation, which collects together all [[subset]]s of a given set. Typically, [[Model theory|models]] in which large cardinal axioms ''fail'' can be seen in some natural way as submodels of those in which the axioms hold. For example, if there is an [[inaccessible cardinal]], then \"cutting the universe off\" at the height of the first such cardinal yields a [[universe (mathematics)|universe]] in which there is no inaccessible cardinal. Or if there is a [[measurable cardinal]], then iterating the ''definable'' powerset operation rather than the full one yields [[Gödel's constructible universe]], L, which does not satisfy the statement \"there is a measurable cardinal\" (even though it contains the measurable cardinal as an ordinal).\n\nThus, from a certain point of view held by many set theorists (especially those inspired by the tradition of the [[Cabal (set theory)|Cabal]]), large cardinal axioms \"say\" that we are considering all the sets we're \"supposed\" to be considering, whereas their negations are \"restrictive\" and say that we're considering only some of those sets. Moreover the consequences of large cardinal axioms seem to fall into natural patterns (see Maddy, \"Believing the Axioms, II\"). For these reasons, such set theorists tend to consider large cardinal axioms to have a preferred status among extensions of ZFC, one not shared by axioms of less clear motivation (such as [[Martin's axiom]]) or others that they consider intuitively unlikely (such as [[V=L|V&nbsp;=&nbsp;L]]). The hardcore [[Philosophy of mathematics#Mathematical realism|realists]] in this group would state, more simply, that large cardinal axioms are ''true''.\n\nThis point of view is by no means universal among set theorists. Some [[Philosophy of mathematics#Formalism|formalists]] would assert that standard set theory is by definition the study of the consequences of ZFC, and while they might not be opposed in principle to studying the consequences of other systems, they see no reason to single out large cardinals as preferred. There are also realists who deny that [[ontological maximalism]] is a proper motivation, and even believe that large cardinal axioms are false. And finally, there are some who deny that the negations of large cardinal axioms ''are'' restrictive, pointing out that (for example) there can be a [[transitive set|transitive]] set model in L that believes there exists a measurable cardinal, even though L itself does not satisfy that proposition.\n\n==See also==\n* [[List of large cardinal properties]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n* {{cite book|author=Jech, Thomas|title=Set theory, third millennium edition (revised and expanded)|publisher=Springer|year=2002|isbn=3-540-44085-2|authorlink=Thomas Jech}}\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|authorlink=Akihiro Kanamori|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n*{{citation|last=Kanamori|first=Akihiro|first2=M. |last2=Magidor\n|chapter=The evolution of large cardinal axioms in set theory\n|series=Lecture Notes in Mathematics\n|publisher =Springer Berlin / Heidelberg\n|volume =669 ([http://math.bu.edu/people/aki/e.pdf typescript])\n|title=Higher Set Theory\n|year=1978\n|isbn =978-3-540-08926-1\n|doi =10.1007/BFb0103104\n|pages=99–275\n}}\n* {{cite journal|last=Maddy|first=Penelope|title=Believing the Axioms, I|journal=Journal of Symbolic Logic|volume=53|issue=2|year=1988|pages=481–511|authorlink=Penelope Maddy|doi=10.2307/2274520|jstor=2274520}}\n* {{cite journal|last=Maddy|first=Penelope|title=Believing the Axioms, II|journal=Journal of Symbolic Logic|volume=53|issue=3|year=1988|pages=736–764|doi=10.2307/2274569|jstor=2274569}}\n* {{cite arXiv|last=Shelah|first=Saharon|title=The Future of Set Theory|year=2002|eprint=math/0211397|authorlink=Saharon Shelah}}\n* {{cite journal|last=Solovay|first=Robert M. |author2=William N. Reinhardt |author3=[[Akihiro Kanamori]]|year=1978|title=Strong axioms of infinity and elementary embeddings|journal=Annals of Mathematical Logic|volume=13|issue=1|pages=73–116|authorlink=Robert M. Solovay|url=http://math.bu.edu/people/aki/d.pdf|doi=10.1016/0003-4843(78)90031-1}}\n* {{cite journal|last=Woodin|first=W. Hugh|title=The continuum hypothesis, part II|journal=Notices of the American Mathematical Society|volume=48|issue=7|year=2001|pages=681–690}}\n\n==External links==\n* [http://plato.stanford.edu/entries/large-cardinals-determinacy/ \"Large Cardinals and Determinacy\"] at the [[Stanford Encyclopedia of Philosophy]]\n\n{{Set theory}}\n\n[[Category:Large cardinals| ]]\n[[Category:Axioms of set theory]]"
    },
    {
      "title": "List of large cardinal properties",
      "url": "https://en.wikipedia.org/wiki/List_of_large_cardinal_properties",
      "text": "{{short description|List of cardinal numbers with large cardinal properties}}\n{{main|Large cardinal}}\n\nThis page includes a list of cardinals with [[large cardinal]] properties. It is arranged roughly in order of the consistency strength of the axiom asserting the existence of cardinals with the given property. Existence of a cardinal number κ of a given type implies the existence of cardinals of most of the types listed above that type, and for most listed cardinal descriptions φ of lesser consistency strength, [[Von Neumann universe|''V''<sub>κ</sub>]] satisfies \"there is an unbounded class of cardinals satisfying φ\".\n\nThe following table usually arranges cardinals in order of [[equiconsistency#Consistency strength|consistency strength]], with size of the cardinal used as a tiebreaker. In a few cases (such as strongly compact cardinals) the exact consistency strength is not known and the table uses the current best guess.\n\n* \"Small\" cardinals: 0, 1, 2, ..., <math>\\aleph_0, \\aleph_1</math>,..., <math>\\kappa  = \\aleph_{\\kappa}</math>, ... (see [[Aleph number]])\n* [[worldly cardinal]]s\n* weakly and strongly [[inaccessible cardinal|inaccessible]], α-[[inaccessible cardinal|inaccessible]], and hyper inaccessible cardinals\n* weakly and strongly [[Mahlo cardinal|Mahlo]], α-[[Mahlo cardinal|Mahlo]], and hyper Mahlo cardinals.\n* [[reflecting cardinal|reflecting]] cardinals\n* [[weakly compact cardinal|weakly compact]] (= Π{{su|p=1|b=1}}-indescribable), [[indescribable cardinal|Π{{su|p=''m''|b=''n''}}-indescribable]], [[indescribable cardinal|totally indescribable]] cardinals\n* [[unfoldable cardinal|λ-unfoldable]], [[unfoldable cardinal|unfoldable]] cardinals, [[indescribable cardinal|&nu;-indescribable]] cardinals and [[shrewd cardinal|λ-shrewd]], [[shrewd cardinal|shrewd]] cardinals (not clear how these relate to each other).\n* [[subtle cardinal|ethereal cardinals]], [[subtle cardinal]]s\n* [[almost ineffable cardinal|almost ineffable]], [[ineffable cardinal|ineffable]], [[ineffable cardinal|''n''-ineffable]], [[ineffable cardinal|totally ineffable]] cardinals\n* [[remarkable cardinal]]s\n* [[Erdős cardinal|α-Erdős cardinal]]s (for [[countable]] α), [[zero sharp|0<sup>#</sup>]] (not a cardinal), [[iterable cardinal|γ-iterable]], [[Erdős cardinal|γ-Erdős cardinal]]s (for [[uncountable set|uncountable]] γ)\n* [[almost Ramsey cardinal|almost Ramsey]], [[Jónsson cardinal|Jónsson]], [[Rowbottom cardinal|Rowbottom]], [[Ramsey cardinal|Ramsey]], [[ineffably Ramsey cardinal|ineffably Ramsey]], completely Ramsey, strongly Ramsey, super Ramsey cardinals\n* [[measurable cardinal]]s,  [[zero dagger|0<sup>†</sup>]]\n* [[strong cardinal|λ-strong]],  [[strong cardinal|strong]] cardinals, [[tall cardinal|tall]] cardinals\n* [[Woodin cardinal|Woodin]], [[weakly hyper-Woodin cardinal|weakly hyper-Woodin]], [[Shelah cardinal|Shelah]], [[hyper-Woodin cardinal|hyper-Woodin]] cardinals\n* [[superstrong cardinal]]s (=1-superstrong; for ''n''-superstrong for ''n''&ge;2 see further down.)\n* [[subcompact cardinal|subcompact]], [[strongly compact cardinal|strongly compact]] (Woodin< strongly compact&le;supercompact), [[supercompact cardinal|supercompact]], [[hypercompact cardinal|hypercompact]] cardinals\n* [[extendible cardinal|η-extendible]], [[extendible cardinal|extendible]] cardinals\n* [[Vopěnka cardinal]]s, Shelah for supercompactness, [[high jump cardinal]]s\n* ''n''-[[superstrong cardinal|superstrong (''n''&ge;2)]], ''n''-[[huge cardinal|almost huge]], ''n''-[[huge cardinal|super almost huge]], ''n''-[[huge cardinal|huge]], ''n''-[[huge cardinal|superhuge]] cardinals  (1-huge=huge, etc.)\n* [[Wholeness axiom]], [[rank-into-rank]] (Axioms I3, I2, I1, and I0)\nThe following even stronger large cardinal properties are not consistent with the axiom of choice, but their existence has not yet been refuted in ZF alone (that is, without use of the [[axiom of choice]]). \n*[[Reinhardt cardinal]], [[Berkeley cardinal]]  <!--\n\nThese do not really make sense, and it is not clear which should be first or last.\n* [[contradiction|0=1]] is (somewhat jokingly) listed as the ultimate large cardinal axiom by some authors.\n* The [[Consistency_strength#Consistency_strength|Strong]] [[Reflection principle|Reflection Principle]] of the [[Absolute Infinite]] [[Tav_(number)|Tav]] -->\n\n==References==\n<references/>\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|authorlink=Akihiro Kanamori|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n*{{Cite book|last=Kanamori|first=Akihiro|first2=M. |last2=Magidor\n|chapter=The evolution of large cardinal axioms in set theory \n|series=Lecture Notes in Mathematics\n|publisher\t=Springer Berlin / Heidelberg \n|volume =669 ([http://math.bu.edu/people/aki/e.pdf typescript])\n|title=Higher Set Theory\n|year=1978\n|isbn\t=978-3-540-08926-1\n|doi\t=10.1007/BFb0103104\n|pages=99–275|postscript=<!--None-->}}\n* {{Cite journal|last=Solovay|first=Robert M.|first2=William N. |last2=Reinhardt|first3= Akihiro |last3=Kanamori|year=1978|title=Strong axioms of infinity and elementary embeddings|journal=Annals of Mathematical Logic|volume=13|issue=1|pages=73–116|authorlink=Robert M. Solovay|url=http://math.bu.edu/people/aki/d.pdf|doi=10.1016/0003-4843(78)90031-1|postscript=<!--None--> }}\n\n==External links==\n*[http://cantorsattic.info/Upper_attic Cantor's attic]\n*[http://mathoverflow.net/questions/194486/is-there-a-compendium-of-the-consistency-strength-between-the-most-important-for some diagrams of large cardinal properties]\n\n[[Category:Large cardinals|*]]\n[[Category:Mathematics-related lists|Large cardinals]]\n\n[[cs:Velké kardinály]]"
    },
    {
      "title": "Axiom of determinacy",
      "url": "https://en.wikipedia.org/wiki/Axiom_of_determinacy",
      "text": "In mathematics, the '''axiom of determinacy''' (abbreviated as '''AD''') is a possible [[axiom]] for [[set theory]] introduced by [[Jan Mycielski]] and [[Hugo Steinhaus]] in 1962.  It refers to certain two-person [[topological game]]s of length [[ω (ordinal number)|ω]]. AD states that every game of a [[Axiom of determinacy#Types of game that are determined | certain type]] is [[determined game|determined]]; that is, one of the two players has a [[winning strategy]].\n\nThey motivated AD by its interesting consequences, and suggested that AD could be true in the least natural model L(R) of a set theory which accepts only a weak form of the [[axiom of choice]] (AC), but contains all real and all ordinal numbers. Some consequences of AD followed from theorems proved earlier by [[Stefan Banach]] and [[Stanisław Mazur]], and [[Morton Davis]]. [[Mycielski]] and [[Stanisław Świerczkowski]] contributed another one: AD implies that all sets of [[real number]]s are [[Lebesgue measurable]]. Later [[Donald A. Martin]] and others proved more important consequences, especially in descriptive set theory. In 1988, [[John R. Steel]] and [[W. Hugh Woodin]] concluded a long line of research. Assuming the existence of some uncountable cardinal numbers analogous to <math>\\alef_0</math>, they proved the original conjecture of Mycielski and Steinhaus that AD is true in L(R).\n\n==Types of game that are determined==\n\nThe axiom of determinacy refers to games of the following specific form:\nConsider a subset ''A'' of the [[Baire space (set theory) | Baire space]] ''ω<sup>ω</sup>'' of all infinite sequences of natural numbers. Two players, '''I''' and '''II''', alternately pick natural numbers\n:''n''<sub>0</sub>, ''n''<sub>1</sub>, ''n''<sub>2</sub>, ''n''<sub>3</sub>, ...\nAfter infinitely many moves, a sequence <math>(n_i)_{i \\in \\omega}</math> is generated. Player '''I''' wins the game if and only if the sequence generated is an element of ''A''. The axiom of determinacy is the statement that all such games are determined.\n\nNot all games require the axiom of determinacy to prove them determined.  If the set ''A'' is [[Clopen set | clopen]], the game is essentially a finite game, and is therefore determined. Similarly, if ''A'' is a [[closed set]], then the game is determined. It was shown in 1975 by [[Donald A. Martin]] that games whose winning set is a [[Borel set]] are determined.  It follows from the existence of sufficiently [[large cardinal]]s that all games with winning set a [[projective set]] are determined (see [[Projective determinacy]]), and that AD holds in [[L(R)]].\n\nThe axiom of determinacy implies that for every subspace ''X'' of the [[Real line#As a topological space | real numbers]], the [[Banach–Mazur game]] ''BM''(''X'') is determined (and therefore that every set of reals has the [[property of Baire]]).\n\n== Incompatibility of the axiom of determinacy with the axiom of choice ==\n\nThe set S1 of all first player strategies in an ω-game ''G'' has the same [[cardinality]] as the [[cardinality of the continuum|continuum]]. The same is true of the set S2 of all second player strategies. We note that the cardinality of the set SG of all sequences possible in ''G'' is also the continuum. Let A be the subset of SG of all sequences which make the first player win. With the axiom of choice we can [[well order]] the continuum; furthermore, we can do so in such a way that any proper initial portion does not have the cardinality of the continuum.  We create a counterexample by [[transfinite induction]] on the set of strategies under this well ordering:\n\nWe start with the set A undefined. Let T be the \"time\" whose axis has length continuum. We need to consider all strategies {s1(T)} of the first player and all strategies {s2(T)} of the second player to make sure that for every strategy there is a strategy of the other player that wins against it. For every strategy of the player considered we will generate a sequence which gives the other player a win. Let t be the time whose axis has length ℵ<sub>0</sub> and which is used during each game sequence.\n\n# Consider the current strategy {s1(T)} of the first player.\n#Go through the entire game, generating (together with the first player's strategy s1(T)) a sequence {a(1), b(2), a(3), b(4),...,a(t), b(t+1),...}.\n#Decide that this sequence does not belong to A, i.e. s1(T) lost.\n#Consider the strategy {s2(T)} of the second player.\n#Go through the next entire game, generating (together with the second player's strategy s2(T)) a sequence {c(1), d(2), c(3), d(4),...,c(t), d(t+1),...}, making sure that this sequence is different from {a(1), b(2), a(3), b(4),...,a(t), b(t+1),...}.\n#Decide that this sequence belongs to A, i.e. s2(T) lost.\n#Keep repeating with further strategies if there are any, making sure that sequences already considered do not become generated again. (We start from the set of all sequences and each time we generate a sequence and refute a strategy we project the generated sequence onto first player moves and onto second player moves, and we take away the two resulting sequences from our set of sequences.)\n#For all sequences that did not come up in the above consideration arbitrarily decide whether they belong to A, or to the complement of A.\n\nOnce this has been done we have a game ''G''. If you give me a strategy s1 then we considered that strategy at some time T = T(s1).  At time ''T'', we decided an outcome of s1 that would be a loss of s1. Hence this strategy fails.  But this is true for an arbitrary strategy; hence the axiom of determinacy and the axiom of choice are incompatible.\n\n== Infinite logic and the axiom of determinacy ==\n\nMany different versions of [[infinitary logic]] were proposed in the late 20th century.  One reason that has been given for believing in the axiom of determinacy is that it can be written as follows (in a version of infinite logic):\n\n<math>\\forall G \\subseteq Seq(S):</math>\n\n<math>\\forall a \\in S :\\exists a' \\in S :\\forall b \\in S :\\exists b' \\in S :\\forall c \\in S :\\exists c' \\in S ... : (a,a',b,b',c,c'...) \\in G </math> OR\n\n<math>\\exists a \\in S :\\forall a' \\in S :\\exists b \\in S :\\forall b' \\in S :\\exists c \\in S :\\forall c' \\in S ... :(a,a',b,b',c,c'...) \\notin G </math>\n\nNote: Seq(''S'') is the set of all <math>\\omega</math>-sequences of ''S''.  The sentences here are infinitely long with a countably infinite list of [[Quantifiers (logic)|quantifiers]] where the ellipses appear.\n\nIn an infinitary logic, this principle is therefore a natural generalization of the usual (de Morgan) rule for quantifiers that \nare true for finite formulas, such as  <math>\\forall a:\\exists b : \\forall c: \\exists d : R(a,b,c,d) </math> OR <math> \\exists a: \\forall b: \n\\exists c: \\forall d: \\lnot R(a,b,c,d)</math>.\n\n== Large cardinals and the axiom of determinacy ==\n\nThe consistency of the axiom of determinacy is closely related to the question of the consistency of [[large cardinal]] axioms. By a theorem of [[W. Hugh Woodin|Woodin]], the consistency of Zermelo–Fraenkel set theory without choice (ZF) together with the axiom of determinacy is equivalent to the consistency of Zermelo–Fraenkel set theory with choice (ZFC) together with the existence of infinitely many [[Woodin cardinal]]s. Since Woodin cardinals are [[inaccessible cardinal|strongly inaccessible]], if AD is consistent, then so are an infinity of inaccessible cardinals.\n\nMoreover, if to the hypothesis of an infinite set of Woodin cardinals is added the existence of a [[measurable cardinal]] larger than all of them, a very strong theory of [[Lebesgue measurable]] sets of reals emerges, as it is then provable that the axiom of determinacy is true in [[L(R)]], and therefore that ''every'' set of real numbers in L(R) is determined.\n\n== See also ==\n\n* [[Axiom of real determinacy]] (AD<sub>R</sub>)\n* [[Borel determinacy theorem]]\n* [[Martin measure]]\n* [[Topological game]]\n\n==References==\n* {{Cite journal | last1=Mycielski | first1=Jan | author1-link = Jan Mycielski | last2=Steinhaus | first2=Hugo | author2-link = Hugo Steinhaus | title=A mathematical axiom contradicting the axiom of choice | mr=0140430 | year=1962 | journal=Bulletin de l'Académie Polonaise des Sciences. Série des Sciences Mathématiques, Astronomiques et Physiques | issn=0001-4117 | volume=10 | pages=1–3 | postscript=<!--None-->}}\n* {{cite journal|last1=Mycielski | first1=Jan | author1-link = Jan Mycielski | last2=Świerczkowski | first2=Stanisław | author2-link = Stanisław Świerczkowski|title=\nOn the Lebesgue measurability and the axiom of determinateness|journal=Fund. Math.|volume=54|\nyear=1964|pages=67–71}}\n* {{cite journal|author=Woodin, W. Hugh | authorlink = W. Hugh Woodin |journal=[[Proceedings of the National Academy of Sciences of the United States of America]]|year=1988|title=Supercompact cardinals, sets of reals, and weakly homogeneous trees|volume=85|issue=18|pages=6587–6591|doi=10.1073/pnas.85.18.6587|pmid=16593979|pmc=282022}}\n* {{cite journal|last1=Martin |first1=Donald A. |author1-link=Donald A. Martin |first2=John R. |last2=Steel |author2-link=John R. Steel |date=Jan 1989|title=A Proof of Projective Determinacy |journal=[[Journal of the American Mathematical Society]] |volume=2 |issue=1 |pages=71–125 |doi=10.2307/1990913 |url=http://www.ams.org/journals/jams/1989-02-01/S0894-0347-1989-0955605-X/S0894-0347-1989-0955605-X.pdf |dead-url=yes |archiveurl=https://web.archive.org/web/20160430183734/http://www.ams.org/journals/jams/1989-02-01/S0894-0347-1989-0955605-X/S0894-0347-1989-0955605-X.pdf |archivedate=April 30, 2016 |jstor=1990913 }}\n* {{cite book|last=Jech | first = Thomas | authorlink = Thomas Jech | title=Set theory, third millennium edition (revised and expanded)|publisher=Springer|year=2002|isbn=978-3-540-44085-7}}\n* {{cite book|last=Kanamori | first = Akihiro|authorlink=Akihiro Kanamori|title=The Higher Infinite|edition=2nd|year=2008|publisher=Springer Science & Business Media|isbn=978-3-540-88866-6}}\n* {{cite book|last1=Moschovakis |first1=Yiannis N. |authorlink1=Yiannis N. Moschovakis |title=Descriptive set theory |date=2009 |publisher=American Mathematical Society |location=Providence, R.I. |isbn=978-0-8218-4813-5 |edition=2nd |url=http://www.math.ucla.edu/~ynm/lectures/dst2009/dst2009.pdf |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20141112111558/http://www.math.ucla.edu/~ynm/lectures/dst2009/dst2009.pdf |archivedate=2014-11-12 |df= }}\n\n== Further reading ==\n\n* Philipp Rohde, ''On Extensions of the Axiom of Determinacy'', Thesis, Department of Mathematics, University of Bonn, Germany, 2001\n* [[Rastislav J. Telgársky|Telgársky, R.J.]] [http://telgarska.com/1987-RMJM-Telgarsky-Topological-Games.pdf ''Topological Games: On the 50th Anniversary of the Banach-Mazur Game''], Rocky Mountain J. Math. '''17''' (1987), pp.&nbsp;227–276. (3.19 MB)\n* [http://plato.stanford.edu/entries/large-cardinals-determinacy/ \"Large Cardinals and Determinacy\"] at the [[Stanford Encyclopedia of Philosophy]]\n\n{{Set theory}}\n\n[[Category:Axioms of set theory]]\n[[Category:Determinacy]]\n[[Category:Large cardinals]]"
    },
    {
      "title": "Axiom of projective determinacy",
      "url": "https://en.wikipedia.org/wiki/Axiom_of_projective_determinacy",
      "text": "In [[mathematical logic]], '''projective determinacy''' is the special case of the [[axiom of determinacy]] applying only to [[projective set]]s.\n\nThe '''axiom of projective determinacy''', abbreviated '''PD''', states that for any two-player infinite game of [[perfect information]] of length [[Ω (ordinal number)|ω]] in which the players play [[natural number]]s, if the victory set (for either player, since the projective sets are closed under complementation) is projective, then one player or the other has a [[winning strategy]].\n\nThe axiom is not a theorem of [[ZFC]] (assuming ZFC is consistent), but unlike the full axiom of determinacy (AD), which contradicts the [[axiom of choice]], it is not known to be inconsistent with ZFC. PD follows from certain [[large cardinal]] axioms, such as the existence of infinitely many [[Woodin cardinal]]s.\n\nPD implies that all projective sets are [[Lebesgue measurable]] (in fact, [[universally measurable]]) and have the [[perfect set property]] and the [[property of Baire]].  It also implies that every projective [[binary relation]] may be [[Uniformization (set theory)|uniformized]] by a projective set.\n\n==References==\n* {{cite journal|last1=Martin |first1=Donald A. |author1-link=Donald A. Martin |first2=John R. |last2=Steel |author2-link=John R. Steel |date=Jan 1989 |title=A Proof of Projective Determinacy |journal=[[Journal of the American Mathematical Society]] |volume=2 |issue=1 |pages=71–125 |doi=10.2307/1990913 |url=http://www.ams.org/journals/jams/1989-02-01/S0894-0347-1989-0955605-X/S0894-0347-1989-0955605-X.pdf |dead-url=yes |archiveurl=https://web.archive.org/web/20160430183734/http://www.ams.org/journals/jams/1989-02-01/S0894-0347-1989-0955605-X/S0894-0347-1989-0955605-X.pdf |archivedate=April 30, 2016 }}\n* {{cite book|last1=Moschovakis |first1=Yiannis N. |authorlink1=Yiannis N. Moschovakis |title=Descriptive set theory |date=2009 |publisher=American Mathematical Society |location=Providence, R.I. |isbn=0-8218-4813-5 |edition=2nd |url=http://www.math.ucla.edu/~ynm/lectures/dst2009/dst2009.pdf |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20141112111558/http://www.math.ucla.edu/~ynm/lectures/dst2009/dst2009.pdf |archivedate=2014-11-12 |df= }}\n\n[[Category:Game theory]]\n[[Category:Axioms of set theory]]\n[[Category:Descriptive set theory]]\n[[Category:Determinacy]]\n[[Category:Large cardinals]]\n\n{{settheory-stub}}"
    },
    {
      "title": "Berkeley cardinal",
      "url": "https://en.wikipedia.org/wiki/Berkeley_cardinal",
      "text": "In [[set theory]], '''Berkeley cardinals''' are certain [[large cardinal]]s suggested by [[Hugh Woodin]] in a seminar at the [[University of California, Berkeley]] in about 1992.\n\nA Berkeley cardinal is a cardinal ''κ'' in a model of ZF with the property that for every [[transitive set]] ''M'' that includes ''κ'', there is a nontrivial [[Elementary equivalence|elementary embedding]] of ''M'' into ''M'' with critical point below&nbsp;''κ''. Berkeley cardinals are a strictly stronger cardinal axiom than [[Reinhardt cardinal]]s, implying that they are not compatible with the [[axiom of choice]].\n\nA weakening of being a Berkeley cardinal is that for every binary relation ''R'' on ''V''<sub>''κ''</sub>, there is a nontrivial elementary embedding of (''V''<sub>''κ''</sub>,&nbsp;''R'') into itself.  This implies that we have elementary\n\n: ''j''<sub>1</sub>, ''j''<sub>2</sub>, ''j''<sub>3</sub>, ...\n: ''j''<sub>1</sub>: (''V''<sub>''κ''</sub>, ∈) → (''V''<sub>''κ''</sub>, ∈),\n: ''j''<sub>2</sub>: (''V''<sub>''κ''</sub>, ∈, ''j''<sub>1</sub>) → (''V''<sub>''κ''</sub>, ∈, ''j''<sub>1</sub>),\n: ''j''<sub>3</sub>: (''V''<sub>''κ''</sub>, ∈, ''j''<sub>1</sub>, ''j''<sub>2</sub>) → (''V''<sub>''κ''</sub>, ∈, ''j''<sub>1</sub>, ''j''<sub>2</sub>),\n\nand so on.  This can be continued any finite number of times, and to the extent that the model has dependent choice, transfinitely.  Thus, plausibly, this notion can be strengthened simply by asserting more dependent choice.\n\nWhile all these notions are incompatible with [[Zermelo–Fraenkel set theory]] (ZFC), their <math>\\Pi^V_2</math> consequences do not appear to be false.  There is no known inconsistency with ZFC in asserting that, for example:<br>\nFor every ordinal λ, there is a transitive model of ZF + Berkeley cardinal that is closed under λ sequences.\n\n==References==\n\n*{{citation|url=http://www.mit.edu/~evanchen/notes/Harvard-145b.pdf|title=Math 145b Lecture Notes|year=2015|last=Chen|first=Evan|first2=Peter |last2=Koellner |author2-link=Peter Koellner}}\n*{{citation|url=http://logic.harvard.edu/blog/wp-content/uploads/2014/11/Deep_Inconsistency.pdf|first=Peter |last=Koellner|title=The Search for Deep Inconsistency|year=2014}}\n\n==External links==\n\n*{{web cite|url=http://cantorsattic.info/Berkeley|title= Berkeley cardinals|publisher= Cantor's attic}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Core model",
      "url": "https://en.wikipedia.org/wiki/Core_model",
      "text": "In [[set theory]], the '''core model''' is a definable [[inner model]] of the [[von Neumann universe|universe]] of all [[Set (mathematics)|sets]]. Even though set theorists refer to \"the core model\", it is not a uniquely identified mathematical object. Rather, it is a class of inner models that under the right set-theoretic assumptions have very special properties, most notably [[covering lemma|covering properties]]. Intuitively, the core model is \"the largest canonical inner model there is\" (Ernest Schimmerling and [[John R. Steel]]) and is typically associated with a [[large cardinal]] notion. If Φ is a large cardinal notion, then the phrase \"core model below Φ\" refers to the definable inner model that exhibits the special properties under the assumption that there does ''not'' exist a cardinal satisfying Φ. The '''core model program''' seeks to analyze large cardinal axioms by determining the core models below them.\n\n==History==\nThe first core model was [[Kurt Gödel]]'s [[constructible universe]] '''L'''. [[Ronald Jensen]] proved the [[covering lemma]] for '''L''' in the 1970s under the assumption of the non-existence of [[zero sharp]], establishing that '''L''' is the \"core model below zero sharp\". The work of [[Solovay]] isolated another core model '''L'''[''U''], for ''U'' an [[ultrafilter]] on a [[measurable cardinal]] (and its associated \"sharp\", [[zero dagger]]). Together with Tony Dodd, Jensen constructed the [[Dodd–Jensen core model]] (\"the core model below a measurable cardinal\") and proved the covering lemma for it and a generalized covering lemma for '''L'''[''U''].\n\nMitchell used coherent sequences of measures to develop core models containing multiple or higher-order measurables. Still later, the Steel core model used [[extender (set theory)|extender]]s and iteration trees to construct a core model below a [[Woodin cardinal]].\n\n==Construction of core models==\nCore models are constructed by [[transfinite recursion]] from small fragments of the core model called [[mouse (Set Theory)|mice]]. An important ingredient of the construction is the comparison lemma that allows giving a [[wellordering]] of the relevant mice.\n\nAt the level of [[strong cardinal]]s and above, one constructs an intermediate countably certified core model K<sup>c</sup>, and then, if possible, extracts K from K<sup>c</sup>.\n\n==Properties of core models==\nK<sub>c</sub> (and hence K) is a fine-structural countably iterable extender model below long extenders. (It is not currently known how to deal with long extenders, which establish that a cardinal is [[superstrong cardinal|superstrong]].) Here countable iterability means ω<sub>1</sub>+1 iterability for all countable elementary substructures of initial segments, and it suffices to develop basic theory, including certain condensation properties.  The theory of such models is canonical and well understood.  They satisfy [[Generalized Continuum Hypothesis|GCH]], the [[diamond principle]] for all [[stationary subset]]s of regular cardinals, the [[square principle]] (except at [[subcompact cardinal]]s), and other principles holding in L.\n\nK<sup>c</sup> is maximal in several senses.  K<sup>c</sup> computes the successors of measurable and many singular cardinals correctly.  Also, it is expected that under an appropriate weakening of countable certifiability, K<sup>c</sup> would correctly compute the successors of all [[weakly compact cardinal|weakly compact]] and singular [[strong limit cardinal]]s correctly.  If V is closed under a mouse operator (an inner model operator), then so is K<sup>c</sup>.  K<sup>c</sup> has no sharp:  There is no natural non-trivial [[elementary embedding]] of K<sup>c</sup> into itself.  (However, unlike K, K<sup>c</sup> may be elementarily self-embeddable.)\n\nIf in addition there are also no Woodin cardinals in this model (except in certain specific cases, it is not known how the core model should be defined if K<sub>c</sub> has Woodin cardinals), we can extract the actual core model K. K is also its own core model.  K is locally definable and generically absolute:  For every generic extension of V, for every cardinal κ>ω<sub>1</sub> in V[G], K as constructed in H(κ) of V[G] equals K∩H(κ).  (This would not be possible had K contained Woodin cardinals).  K is maximal, universal, and fully iterable. This implies that for every iterable extender model M (called a mouse), there is an elementary embedding M→N and of an initial segment of K into N, and if M is universal, the embedding is of K into M.\n\nIt is conjectured that if K exists and V is closed under a sharp operator M, then K is  Σ<sup>1</sup><sub>1</sub> correct allowing real numbers in K as parameters and M as a predicate.  That amounts to Σ<sup>1</sup><sub>3</sub> correctness (in the usual sense) if M is x→x<sup>#</sup>.\n\nThe core model can also be defined above a particular set of ordinals X:  X belongs to K(X), but K(X) satisfies the usual properties of K above X.  If there is no iterable inner model with ω Woodin cardinals, then for some X, K(X) exists.  The above discussion of K and K<sup>c</sup> generalizes to K(X) and K<sup>c</sup>(X).\n\n==Construction of core models==\n'''Conjecture:'''  \n*If there is no  ω<sub>1</sub>+1 iterable model with long extenders (and hence models with superstrong cardinals), then K<sup>c</sup> exists.\n*If K<sup>c</sup> exists and as constructed in every generic extension of V (equivalently, under some generic collapse Coll(ω, <κ) for a sufficiently large ordinal κ) satisfies \"there are no Woodin cardinals\", then the Core Model K exists.\n\nPartial results for the conjecture are that:\n#If there is no inner model with a Woodin cardinal, then K exists.\n#If (boldface) Σ<sup>1</sup><sub>n</sub> determinacy (n is finite) holds in every generic extension of V, but there is no iterable inner model with n Woodin cardinals, then K exists.\n#If there is a measurable cardinal κ, then either K<sup>c</sup> below κ exists, or there is an ω<sub>1</sub>+1 iterable model with measurable limit λ of both Woodin cardinals and cardinals strong up to λ.\n\nIf V has Woodin cardinals but not cardinals strong past a Woodin one, then under appropriate circumstances (a candidate for) K can be constructed by constructing K below each Woodin cardinal (and below the class of all ordinals) κ but above that K as constructed below the supremum of Woodin cardinals below κ.  The candidate core model is not fully iterable (iterability fails at Woodin cardinals) or generically absolute, but otherwise behaves like K.\n\n==References==\n* [[W. Hugh Woodin]] (June/July 2001). [http://www.ams.org/notices/200106/fea-woodin.pdf].  Notices of the AMS.\n* William Mitchell. \"Beginning Inner Model Theory\" (being Chapter 17 in Volume 3 of \"Handbook of Set Theory\") at [https://web.archive.org/web/20110617031749/http://www.math.ufl.edu/~wjm/papers/].\n* [[Matthew Foreman]] and [[Akihiro Kanamori]] (Editors). \"Handbook of Set Theory\", Springer Verlag, 2010, {{isbn|978-1402048432}}.\n* Ronald Jensen and John R. Steel. \"K without the measurable\". Journal of Symbolic Logic Volume 78, Issue 3 (2013), 708-734.\n\n[[Category:Inner model theory]]\n[[Category:Large cardinals]]"
    },
    {
      "title": "Critical point (set theory)",
      "url": "https://en.wikipedia.org/wiki/Critical_point_%28set_theory%29",
      "text": "In [[set theory]], the '''critical point''' of an [[elementary embedding]] of a [[transitive class]] into another transitive class is the smallest [[ordinal number|ordinal]] which is not mapped to itself.<ref>{{cite book | last = Jech | first = Thomas | \n author-link = Thomas Jech | title = Set Theory | publisher = Springer-Verlag | location = Berlin | year = 2002 | isbn = 3-540-44085-2 }} p. 323</ref>\n\nSuppose that j : ''N'' → ''M'' is an elementary embedding where ''N'' and ''M'' are transitive classes and j is definable in ''N'' by a formula of set theory with parameters from ''N''.  Then j must take ordinals to ordinals and j must be strictly increasing.  Also j(ω)=ω.  If j(α)=α for all α<κ and j(κ)>κ, then κ is said to be the critical point of j.\n\nIf ''N'' is ''[[Von Neumann universe|V]]'', then κ (the critical point of j) is always a [[measurable cardinal]], i.e. an uncountable [[cardinal number]] κ such that there exists a <κ-complete, non-principal [[ultrafilter]] over κ. Specifically, one may take the filter to be <math> \\{A \\vert A \\subseteq \\kappa \\land \\kappa \\in j (A) \\} \\,.</math> Generally, there will be many other <κ-complete, non-principal ultrafilters over κ. However, j might be different from the ultrapower(s) arising from such filter(s).\n\nIf ''N'' and ''M'' are the same and j is the identity function on ''N'', then j is called \"trivial\".  If transitive class ''N'' is an [[inner model]] of ZFC and j has no critical point, i.e. every ordinal maps to itself, then j is trivial.\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Critical Point (Set Theory)}}\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Equiconsistency",
      "url": "https://en.wikipedia.org/wiki/Equiconsistency",
      "text": "{{Short description|Being equally consistent}}\nIn [[mathematical logic]], two [[theory (mathematical logic)|theories]] are '''equiconsistent''' if the consistency of one theory implies the consistency of the other theory, and [[Vice-versa|vice versa]].  In this case, they are, roughly speaking, \"as consistent as each other\".\n\nIn general, it is not possible to prove the absolute [[consistency]] of a theory ''T''.  Instead we usually take a theory ''S'', believed to be consistent, and try to prove the weaker statement that if ''S'' is consistent then ''T'' must also be consistent—if we can do this we say that ''T'' is ''consistent relative to S''.  If ''S'' is also consistent relative to ''T'' then we say that ''S'' and ''T'' are '''equiconsistent'''.\n\n== Consistency ==\n\nIn mathematical logic, formal theories are studied as [[mathematical object]]s. Since some theories are powerful enough to model different mathematical objects, it is natural to wonder about their own [[consistency]].\n\n[[David Hilbert|Hilbert]] proposed a [[Hilbert's program|program]] at the beginning of the 20th century whose ultimate goal was to show, using mathematical methods, the consistency of mathematics. Since most mathematical disciplines can be reduced to [[arithmetic]], the program quickly became the establishment of the consistency of arithmetic by methods formalizable within arithmetic itself. \n\n[[Kurt Gödel|Gödel]]'s [[Gödel's incompleteness theorem|incompleteness theorems]] show that Hilbert's program cannot be realized: if a consistent [[recursively enumerable set|recursively enumerable]] theory is strong enough to formalize its own [[metamathematics]] (whether something is a proof or not), i.e. strong enough to model a weak fragment of arithmetic ([[Robinson arithmetic]] suffices), then the theory cannot prove its own consistency. There are some technical caveats as to what requirements the formal statement representing the metamathematical statement \"The theory is consistent\" needs to satisfy, but the outcome is that if a (sufficiently strong) theory can prove its own consistency then either there is no computable way of identifying whether a statement is even an axiom of the theory or not, or else the theory itself is inconsistent (in which case it can prove anything, including false statements such as its own consistency).\n\nGiven this, instead of outright consistency, one usually considers relative consistency: Let ''S'' and ''T'' be formal theories. Assume that ''S'' is a consistent theory. Does it follow that ''T'' is consistent? If so, then ''T is consistent relative to S''. Two theories are equiconsistent if each one is consistent relative to the other.\n\n== Consistency strength ==\n\nIf ''T'' is consistent relative to ''S'', but ''S'' is not known to be consistent relative to ''T'', then we say that ''S'' has greater '''consistency strength''' than ''T''.  When discussing these issues of consistency strength the metatheory in which the discussion takes places needs to be carefully addressed. For theories at the level of [[second-order arithmetic]], the [[reverse mathematics]] program has much to say. Consistency strength issues are a usual part of [[set theory]], since this is a recursive theory that can certainly model most of mathematics. The most widely used set of axioms of set theory is called [[ZFC]]. When a set-theoretic statement {{var|A}} is said to be equiconsistent to another {{var|B}}, what is being claimed is that in the metatheory ([[Peano Arithmetic]] in this case) it can be proven that the theories ZFC+{{var|A}} and ZFC+{{var|B}} are equiconsistent. Usually, [[primitive recursive arithmetic]] can be adopted as the metatheory in question, but even if the metatheory is ZFC or an extension of it, the notion is meaningful. The method of [[forcing (mathematics)|forcing]] allows one to show that the theories ZFC, ZFC+CH and ZFC+¬CH are all equiconsistent (where CH denotes the [[continuum hypothesis]]).\n\nWhen discussing fragments of ZFC or their extensions (for example, ZF, set theory without the axiom of choice, or ZF+AD, set theory with the [[axiom of determinacy]]), the notions described above are adapted accordingly. Thus, ZF is equiconsistent with ZFC, as shown by Gödel.\n\nThe consistency strength of numerous combinatorial statements can be calibrated by [[large cardinal]]s.  For example, the negation of [[Kurepa tree|Kurepa's hypothesis]] is equiconsistent with an [[inaccessible cardinal]], the non-existence of special <math>\\omega_2</math>-[[Aronszajn tree]]s is equiconsistent with a [[Mahlo cardinal]], and the non-existence of <math>\\omega_2</math>-[[Aronszajn tree]]s is equiconsistent with a [[weakly compact cardinal]].<ref>*{{citation | last=Kunen | first=Kenneth | authorlink=Kenneth Kunen | title=Set theory | zbl=1262.03001 | series=Studies in Logic | volume=34 | location=London | publisher=College Publications | isbn=978-1-84890-050-9 | year=2011 | page=225 }}</ref>\n\n==See also==\n*[[Large cardinal property]]\n\n==References==\n{{reflist}}\n* [[Akihiro Kanamori]] (2003). ''The Higher Infinite''. Springer. {{ISBN|3-540-00384-3}}\n\n{{Metalogic}}\n\n[[Category:Mathematical logic]]\n[[Category:Large cardinals]]"
    },
    {
      "title": "Erdős cardinal",
      "url": "https://en.wikipedia.org/wiki/Erd%C5%91s_cardinal",
      "text": "In [[mathematics]], an '''Erdős cardinal''', also called a '''partition cardinal''' is a certain kind of [[large cardinal]] number introduced by {{harvs|txt|authorlink=Paul Erdős|last=Erdős|first=Paul|first2=András |last2=Hajnal|author2-link=András Hajnal|year=1958}}.\n\nThe Erdős cardinal {{math|''κ''(''α'')}} is defined to be the least cardinal such that for every function {{math|&nbsp;''f'' : ''κ''<sup>< ''ω''</sup> → {0, 1},}} there is a set of [[order type]] {{mvar|α}} that is [[Homogeneous (large cardinal property)|homogeneous]] for {{math|&thinsp;''f''&thinsp;}} (if such a cardinal exists). In the notation of the [[partition calculus]], the Erdős cardinal {{math|''κ''(''α'')}} is the smallest cardinal such that\n\n:{{math|''κ''(''α'') → (''α'')<sup>< ''ω''</sup>}}\n\nExistence of [[zero sharp]] implies that the [[constructible universe]] {{mvar|L}} satisfies \"for every [[countable ordinal]] {{mvar|α}}, there is an {{mvar|α}}-Erdős cardinal\". In fact, for every [[indiscernible]] {{mvar|κ, L<sub>κ</sub>}} satisfies \"for every ordinal {{mvar|α}}, there is an {{mvar|α}}-Erdős cardinal in {{math|Coll(''ω'', ''α'')}} (the [[Levy collapse]] to make {{mvar|α}} countable)\".\n\nHowever, existence of an {{math|''ω''<sub>1</sub>}}-Erdős cardinal implies existence of [[zero sharp]]. If {{math|&thinsp;''f''&thinsp;}} is the [[satisfaction relation]] for {{mvar|L}} (using ordinal parameters), then existence of zero sharp is equivalent to there being an {{math|''ω''<sub>1</sub>}}-Erdős ordinal with respect to {{math|&thinsp;''f''&thinsp;}}. And this in turn, the zero sharp implies the falsity of [[axiom of constructibility]], of [[Kurt Gödel]].\n\nIf κ is {{mvar|α}}-Erdős, then it is {{mvar|α}}-Erdős in every [[transitive model]] satisfying \"{{mvar|α}} is countable\".\n\n==References==\n*{{Cite journal| last1=Baumgartner | first1=James E. | author1-link=James E. Baumgartner | last2=Galvin | first2=Fred | author2-link=Fred Galvin | title=Generalized Erdős cardinals and 0<sup>#</sup> | doi=10.1016/0003-4843(78)90012-8 | mr=528659 | year=1978 | journal=Annals of Mathematical Logic | issn=0003-4843 | volume=15 | issue=3 | pages=289–313 | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n*{{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n*{{Cite journal| last1=Erdős | first1=Paul | last2=Hajnal | first2=András | title=On the structure of set-mappings | doi=10.1007/BF02023868 | mr=0095124 | year=1958 | journal=Acta Mathematica Academiae Scientiarum Hungaricae | issn=0001-5954 | volume=9 | pages=111–131 | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|authorlink=Akihiro Kanamori|edition=2nd|isbn=3-540-00384-3}}\n\n{{DEFAULTSORT:Erdos cardinal}}\n[[Category:Large cardinals]]\n[[Category:Paul Erdős|Cardinal]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Ethereal cardinal",
      "url": "https://en.wikipedia.org/wiki/Ethereal_cardinal",
      "text": "#redirect[[Subtle cardinal]]\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Extender (set theory)",
      "url": "https://en.wikipedia.org/wiki/Extender_%28set_theory%29",
      "text": "In [[set theory]], an '''extender''' is a system of [[ultrafilter]]s which represents an [[elementary embedding]] witnessing [[large cardinal]] properties. A nonprincipal ultrafilter is the most basic case of an extender.\n\nA (κ, λ)-extender can be defined as an elementary embedding of some model ''M'' of ZFC<sup>−</sup> (ZFC minus the [[power set axiom]]) having critical point κ ε ''M'', and which maps κ to an ordinal at least equal to λ. It can also be defined as a collection of ultrafilters, one for each ''n''-[[tuple]] drawn from λ.\n\n==Formal definition of an extender==\nLet &kappa; and &lambda; be cardinals with &kappa;&le;&lambda;. Then, a set <math>E=\\{E_a|a\\in [\\lambda]^{<\\omega}\\}</math> is called a (κ,λ)-extender if the following properties are satisfied:\n# each ''E<sub>a</sub>'' is a &kappa;-complete nonprincipal ultrafilter on [&kappa;]<sup>&lt;&omega;</sup> and furthermore\n## at least one ''E<sub>a</sub>'' is not &kappa;<sup>+</sup>-complete,\n## for each <math>\\alpha\\in\\kappa</math>, at least one ''E<sub>a</sub>'' contains the set <math>\\{s\\in[\\kappa]^{|a|}:\\alpha\\in s\\}</math>.\n# (Coherence) The ''E<sub>a</sub>'' are coherent (so that the [[ultraproduct|ultrapowers]] Ult(''V'',''E<sub>a</sub>'') form a directed system).\n# (Normality) If ''f'' is such that <math>\\{s\\in[\\kappa]^{|a|}: f(s)\\in\\max s\\}\\in E_a</math>, then for some <math>b\\supseteq a,\\ \\{t\\in\\kappa^{|b|}:(f\\circ \\pi_{ba})(t)\\in t\\}\\in E_b</math>.\n# (Wellfoundedness) The limit ultrapower Ult(''V'',''E'') is [[axiom of regularity|wellfounded]] (where Ult(''V'',''E'') is the [[direct limit]] of the ultrapowers Ult(''V'',''E<sub>a</sub>'')).\n\nBy coherence, one means that if ''a'' and ''b'' are finite subsets of &lambda; such that ''b'' is a superset of ''a'', then if ''X'' is an element of the ultrafilter ''E<sub>b</sub>'' and one chooses the right way to project ''X'' down to a set of sequences of length |''a''|, then ''X'' is an element of ''E<sub>a</sub>''. More formally, for <math>b=\\{\\alpha_1,\\dots,\\alpha_n\\}</math>, where <math>\\alpha_1<\\dots<\\alpha_n<\\lambda</math>, and <math>a=\\{\\alpha_{i_1},\\dots,\\alpha_{i_m}\\}</math>, where ''m''&le;''n'' and  for ''j''&le;''m'' the ''i<sub>j</sub>'' are pairwise distinct and at most ''n'', we define the projection <math>\\pi_{ba}:\\{\\xi_1,\\dots,\\xi_n\\}\\mapsto\\{\\xi_{i_1},\\dots,\\xi_{i_m}\\}\\ (\\xi_1<\\dots<\\xi_n)</math>.\n\nThen ''E''<sub>''a''</sub> and ''E<sub>b</sub>'' cohere if\n:<math> X\\in E_a\\Leftrightarrow \\{s: \\pi_{ba}(s)\\in X\\}\\in E_b</math>.\n\n==Defining an extender from an elementary embedding==\nGiven an elementary embedding j:V→M, which maps the set-theoretic universe ''V'' into a [[Transitive set#Transitive models of set theory|transitive]] [[inner model]] ''M'', with [[critical point (set theory)|critical point]] κ, and a cardinal λ, κ≤λ≤''j''(κ),  one defines <math>E=\\{E_a|a\\in [\\lambda]^{<\\omega}\\}</math> as follows:\n:<math>\\text{for }a\\in[\\lambda]^{<\\omega}, X\\subseteq [\\kappa]^{<\\omega}:\\quad X\\in E_a\\Leftrightarrow a\\in j(X).</math>\nOne can then show that ''E'' has all the properties stated above in the definition and therefore is a (&kappa;,&lambda;)-extender.\n\n==References==\n\n* {{cite book|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n* {{cite book|last=Jech|first=Thomas|authorlink=Thomas Jech|year=2002|publisher=Springer|title=Set Theory|edition=3rd|isbn=3-540-44085-2}}\n\n[[Category:Large cardinals]]\n[[Category:Inner model theory]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Extendible cardinal",
      "url": "https://en.wikipedia.org/wiki/Extendible_cardinal",
      "text": "In [[mathematics]], '''extendible cardinals''' are [[large cardinal]]s introduced by {{harvtxt|Reinhardt|1974}}, who was partly motivated by [[reflection principle]]s.  Intuitively, such a cardinal represents a point beyond which initial pieces of the [[Von Neumann universe|universe of sets]] start to look similar, in the sense that each is [[elementary embedding|elementarily embeddable]] into a later one.\n\n==Definition==\n\nFor every [[ordinal number|ordinal]] ''η'', a [[cardinal number|cardinal]] κ is called '''η-extendible''' if  for some ordinal ''λ'' there is a nontrivial [[elementary embedding]] ''j'' of ''V''<sub>κ+η</sub> into ''V''<sub>λ</sub>, where ''κ'' is the [[critical point (set theory)|critical point]] of ''j'', and as usual ''V<sub>α</sub>'' denotes the ''α''th level of the [[Von Neumann universe|von Neumann hierarchy]].  A cardinal ''κ'' is called an '''extendible cardinal''' if it is ''η''-extendible for every nonzero ordinal ''η'' (Kanamori 2003).\n\n==Variants and relation to other cardinals==\n\nA cardinal ''κ'' is called ''η-C<sup>(n)</sup>''-extendible if there is an elementary embedding ''j'' witnessing that ''κ'' is ''η''-extendible (that is, ''j'' is elementary from ''V<sub>κ+η</sub>'' to some ''V<sub>λ</sub>'' with critical point ''κ'') such that furthermore, ''V<sub>j(κ)</sub>'' is ''Σ<sub>n</sub>''-correct in ''V''.  That is, for every [[Lévy hierarchy#Definitions|''Σ<sub>n</sub>'']] formula ''φ'', ''φ'' holds in ''V<sub>j(κ)</sub>'' if and only if ''φ'' holds in ''V''.  A cardinal ''κ'' is said to be '''C<sup>(n)</sup>-extendible''' if it is ''η-C<sup>(n)</sup>''-extendible for every ordinal ''η''.  Every extendible cardinal is ''C<sup>(1)</sup>''-extendible, but for ''n≥1'', the least ''C<sup>(n)</sup>''-extendible cardinal is never ''C<sup>(n+1)</sup>''-extendible (Bagaria 2011).\n\n[[Vopěnka's principle]] implies the existence of extendible cardinals; in fact, Vopěnka's principle (for definable classes) is equivalent to the existence of ''C<sup>(n)</sup>''-extendible cardinals for all ''n'' (Bagaria 2011).  All extendible cardinals are [[supercompact cardinal]]s (Kanamori 2003).\n\n==See also==\n*[[List of large cardinal properties]]\n*[[Reinhardt cardinal]]\n\n==References==\n\n*{{cite journal|last1=Bagaria|first1=Joan|title=''C<sup>(n)</sup>''-cardinals|journal=Archive for Mathematical Logic|date=23 December 2011|volume=51|issue=3-4|pages=213–240|doi=10.1007/s00153-011-0261-8}}\n*{{cite web|last1=Friedman|first1=Harvey|authorlink=Harvey Friedman|title=Restrictions and Extensions|url=http://u.osu.edu/friedman.8/files/2014/01/ResExt021703-1t4vsx4.pdf}}\n*{{cite book|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n*{{citation|mr=0401475\n|last=Reinhardt|first= W. N.\n|chapter=Remarks on reflection principles, large cardinals, and elementary embeddings. |title=Axiomatic set theory |series=Proc. Sympos. Pure Math.|volume= XIII, Part II|pages= 189–205|publisher= Amer. Math. Soc.|publication-place= Providence, R. I.|year= 1974}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Grothendieck universe",
      "url": "https://en.wikipedia.org/wiki/Grothendieck_universe",
      "text": "In [[mathematics]], a '''Grothendieck universe''' is a set ''U'' with the following properties:\n\n# If ''x'' is an element of ''U'' and if ''y'' is an element of ''x'', then ''y'' is also an element of ''U''. (''U'' is a [[transitive set]].)\n# If ''x'' and ''y'' are both elements of ''U'', then <math>\\{x,y\\}</math> is an element of ''U''.\n# If ''x'' is an element of ''U'', then ''P''(''x''), the [[power set]] of ''x'', is also an element of ''U''.\n# If <math>\\{x_\\alpha\\}_{\\alpha\\in I}</math> is a family of elements of ''U'', and if ''I'' is an element of ''U'', then the union <math>\\bigcup_{\\alpha\\in I} x_\\alpha</math> is an element of ''U''.\n\nA Grothendieck universe is meant to provide a set in which all of mathematics can be performed.  (In fact, uncountable Grothendieck universes provide [[model theory|models]] of set theory with the natural ∈-relation, natural powerset operation etc.). Elements of a Grothendieck universe are sometimes called '''small sets'''. The idea of universes is due to [[Alexander Grothendieck]], who used them as a way of avoiding [[proper class]]es in [[algebraic geometry]].\n\nThe existence of a nontrivial Grothendieck universe goes beyond the usual axioms of [[Zermelo–Fraenkel set theory]]; in particular it would imply the existence of [[Inaccessible cardinal|strongly inaccessible cardinals]].\n[[Tarski–Grothendieck set theory]] is an axiomatic treatment of set theory, used in some automatic proof systems, in which every set belongs to a Grothendieck universe.\nThe concept of a Grothendieck universe can also be defined in a [[topos]].<ref>\n{{cite conference\n | first = Thomas\n | last = Streicher\n | authorlink = Thomas Streicher\n |authors=\n | title = Universes in Toposes\n | booktitle = From Sets and Types to Topology and Analysis: Towards Practicable Foundations for Constructive Mathematics\n | pages = 78–90\n | publisher = Clarendon Press\n | date = 2006\n | location =\n | url = http://www.mathematik.tu-darmstadt.de/~streicher/NOTES/UniTop.pdf\n | doi =\n | id =\n | isbn = 9780198566519 \n | accessdate = }}</ref>\n\n==Properties==\nAs an example, we will prove an easy proposition.\n\n:'''Proposition'''. If <math>x \\in U</math> and <math>y \\subseteq x</math>, then <math>y \\in U</math>.\n:Proof. <math>y \\in P(x)</math>  because <math>y \\subseteq x</math>. <math>P(x) \\in U</math> because <math>x \\in U</math>, so <math>y \\in U</math>.\n\nIt is similarly easy to prove that any Grothendieck universe ''U'' contains:\n* All [[singleton (mathematics)|singletons]] of each of its elements,\n* All products of all families of elements of ''U'' indexed by an element of ''U'',\n* All disjoint unions of all families of elements of ''U'' indexed by an element of ''U'',\n* All intersections of all families of elements of ''U'' indexed by an element of ''U'',\n* All functions between any two elements of ''U'', and\n* All subsets of ''U'' whose cardinal is an element of ''U''.\n\nIn particular, it follows from the last axiom that if ''U'' is non-empty, it must contain all of its finite subsets and a subset of each finite cardinality.  One can also prove immediately from the definitions that the intersection of any class of universes is a universe.\n\n== Grothendieck universes and inaccessible cardinals ==\nThere are two simple examples of Grothendieck universes:\n* The empty set, and\n* The set of all [[hereditarily finite set]]s <math>V_\\omega</math>.\nOther examples are more difficult to construct.  Loosely speaking, this is because Grothendieck universes are equivalent to [[Inaccessible cardinal|strongly inaccessible cardinal]]s.  More formally, the following two axioms are equivalent:\n\n: (U) For each set ''x'', there exists a Grothendieck universe ''U'' such that ''x'' ∈ ''U''.\n: (C) For each cardinal κ, there is a strongly inaccessible cardinal λ that is strictly larger than κ.\n\nTo prove this fact, we introduce the function '''c'''(''U'').  Define:\n:<math>\\mathbf{c}(U) = \\sup_{x \\in U} |x|</math>\nwhere by |''x''| we mean the cardinality of ''x''.  Then for any universe ''U'', '''c'''(''U'') is either zero or strongly inaccessible.  Assuming it is non-zero, it is a strong limit cardinal because the power set of any element of ''U'' is an element of ''U'' and every element of ''U'' is a subset of ''U''.  To see that it is regular, suppose that ''c''<sub>''λ''</sub> is a collection of cardinals indexed by ''I'', where the cardinality of ''I'' and of each ''c<sub>λ</sub>'' is less than '''c'''(''U'').  Then, by the definition of '''c'''(''U''), ''I'' and each ''c''<sub>''λ''</sub> can be replaced by an element of ''U''.  The union of elements of ''U'' indexed by an element of ''U'' is an element of ''U'', so the sum of the ''c''<sub>''λ''</sub> has the cardinality of an element of ''U'', hence is less than '''c'''(''U'').  By invoking the axiom of foundation, that no set is contained in itself, it can be shown that '''c'''(''U'') equals |''U''|; when the axiom of foundation is not assumed, there are counterexamples (we may take for example U to be the set of all finite sets of finite sets etc. of the sets x<sub>&alpha;</sub> where the index &alpha; is any real number, and ''x''<sub>''&alpha;''</sub> = {''x''<sub>''&alpha;''</sub>} for each ''&alpha;''. Then ''U'' has the cardinality of the continuum, but all of its members have finite cardinality and so <math>\\mathbf{c}(U) = \\aleph_0 </math> ; see Bourbaki's article for more details).\n\nLet κ be a strongly inaccessible cardinal.  Say that a set ''S'' is strictly of type κ if for any sequence ''s''<sub>''n''</sub> ∈ ... ∈ ''s''<sub>0</sub> ∈ ''S'', |''s''<sub>''n''</sub>| < ''κ''.  (''S'' itself corresponds to the empty sequence.)  Then the set ''u''(''κ'') of all sets strictly of type κ is a Grothendieck universe of cardinality κ.  The proof of this fact is long, so for details, we again refer to Bourbaki's article, listed in the references.\n\nTo show that the large cardinal axiom (C) implies the universe axiom (U), choose a set ''x''.  Let ''x<sub>0</sub>'' = ''x'', and for each ''n'', let ''x''<sub>''n''+1</sub> = <math>\\bigcup</math> ''x''<sub>''n''</sub> be the union of the elements of ''x<sub>n</sub>''.  Let ''y'' = <math>\\bigcup_n</math>''x<sub>n</sub>''.  By (C), there is a strongly inaccessible cardinal κ such that |y| < κ.  Let ''u''(''κ'') be the universe of the previous paragraph.  ''x'' is strictly of type κ, so ''x'' ∈ ''u''(''κ'').  To show that the universe axiom (U) implies the large cardinal axiom (C), choose a cardinal κ.  κ is a set, so it is an element of a Grothendieck universe ''U''.  The cardinality of ''U'' is strongly inaccessible and strictly larger than that of κ.\n\nIn fact, any Grothendieck universe is of the form ''u''(''κ'') for some κ.  This gives another form of the equivalence between Grothendieck universes and strongly inaccessible cardinals:\n\n:For any Grothendieck universe ''U'', |''U''| is either zero, <math>\\aleph_0</math>, or a strongly inaccessible cardinal. And if κ is zero, <math>\\aleph_0</math>, or a strongly inaccessible cardinal, then there is a Grothendieck universe u(κ).  Furthermore, ''u''(|''U''|) = ''U'', and |''u''(''κ'')| = ''κ''.\n\nSince the existence of strongly inaccessible cardinals cannot be proved from the axioms of [[Zermelo–Fraenkel set theory]] (ZFC), the existence of universes other than the empty set and <math>V_\\omega</math> cannot be proved from ZFC either.   However, strongly inaccessible cardinals are on the lower end of the [[List of large cardinal properties|list of large cardinals]]; thus, most set theories that use large cardinals (such as \"ZFC plus there is a [[measurable cardinal]]\", \"ZFC plus there are infinitely many [[Woodin cardinal]]s\") will prove that Grothendieck universes exist.\n\n== See also ==\n*[[Constructible universe]]\n*[[Universe (mathematics)]]\n*[[Von Neumann universe]]\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n* {{cite conference\n | first = Nicolas\n | last = Bourbaki\n | authorlink = Nicolas Bourbaki\n | year = 1972\n | title = Univers\n | booktitle = Séminaire de Géométrie Algébrique du Bois Marie – 1963–64 – Théorie des topos et cohomologie étale des schémas – (SGA 4) – vol. 1 (Lecture Notes in Mathematics '''269''')\n | editor = [[Michael Artin]] |editor2=[[Alexandre Grothendieck]] |editor3=[[Jean-Louis Verdier]]\n | publisher = [[Springer Science+Business Media|Springer-Verlag]]\n | location = Berlin; New York\n | language = French\n | pages = 185&ndash;217\n | url = http://library.msri.org/books/sga/sga/4-1/4-1t_185.html\n}}\n\n[[Category:Set-theoretic universes]]\n[[Category:Category theory]]\n[[Category:Large cardinals]]"
    },
    {
      "title": "Hereditarily countable set",
      "url": "https://en.wikipedia.org/wiki/Hereditarily_countable_set",
      "text": "In [[set theory]], a set is called '''hereditarily countable''' if it is a [[countable set]] of [[hereditary property|hereditarily]] countable sets. This [[inductive definition]] is in fact [[well-founded]] and can be expressed in the language of [[first-order logic|first-order]] set theory. A set is hereditarily countable if and only if it is countable, and every element of its [[transitive set|transitive closure]] is countable. If the [[axiom of countable choice]] holds, then a set is hereditarily countable if and only if its transitive closure is countable. \n\nThe [[class (set theory)|class]] of all hereditarily countable sets can be proven to be a set from the axioms of [[Zermelo–Fraenkel set theory]] (ZF) without any form of the [[axiom of choice]], and this set is designated <math>H_{\\aleph_1}</math>. The hereditarily countable sets form a model of [[Kripke–Platek set theory]] with the [[axiom of infinity]] (KPI), if the axiom of countable choice is assumed in the [[metatheory]].\n\nIf <math>x \\in H_{\\aleph_1}</math>, then <math>L_{\\omega_1}(x) \\subset H_{\\aleph_1}</math>.\n\nMore generally, a set is '''hereditarily of cardinality less than κ''' if and only if it is of [[cardinality]] less than κ, and all its elements  are hereditarily of cardinality less than κ; the class of all such sets can also be proven to be a set from the axioms of ZF, and is designated <math>H_\\kappa \\!</math>. If the axiom of choice holds and the cardinal κ is regular, then a set is hereditarily of cardinality less than κ if and only if its transitive closure is of cardinality less than κ.  \n\n==See also==\n*[[Hereditarily finite set]]\n*[[Constructible universe]]\n\n==External links==\n*[https://www.jstor.org/pss/2273380 \"On Hereditarily Countable Sets\"] by [[Thomas Jech]]\n\n[[Category:Set theory]]\n[[Category:Large cardinals]]\n\n{{settheory-stub}}"
    },
    {
      "title": "Homogeneous (large cardinal property)",
      "url": "https://en.wikipedia.org/wiki/Homogeneous_%28large_cardinal_property%29",
      "text": "In set theory and in the context of a [[large cardinal property]], a subset, ''S'', of ''D'' is '''homogeneous''' for a function ''f'' if for some natural number ''n'', <math>\\mathcal{P}_{=n}(D)</math> (see [[Powerset#Subsets of limited cardinality]]) is the domain of ''f'' and for some element ''r'' of the range of ''f'', every member of <math>\\mathcal{P}_{=n}(S)</math> is mapped to ''r''. That is, ''f'' is constant on the unordered ''n''-tuples of elements of ''S''.\n\n==See also==\n*[[Ramsey's theorem]]\n\n{{settheory-stub}}\n[[Category:Large cardinals]]"
    },
    {
      "title": "Huge cardinal",
      "url": "https://en.wikipedia.org/wiki/Huge_cardinal",
      "text": "In [[mathematics]], a [[cardinal number]] κ is called '''huge''' if [[there exists]] an [[elementary embedding]] ''j'' : ''V'' → ''M'' from ''V'' into a transitive [[inner model]] ''M'' with [[critical point (set theory)|critical point]] κ and\n\n:<math>{}^{j(\\kappa)}M \\subset M.\\!</math>\n\nHere, ''<sup>&alpha;</sup>M'' is the class of all [[sequence]]s of length α whose elements are in M.\n\nHuge cardinals were introduced by {{harvs|txt|authorlink=Kenneth Kunen|first=Kenneth |last=Kunen|year=1978}}.\n\n== Variants ==\nIn what follows, j<sup>''n''</sup> refers to the ''n''-th iterate of the elementary embedding j, that is, j [[function composition|composed]] with itself ''n'' times, for a finite ordinal ''n''.  Also, ''<sup>&lt;&alpha;</sup>M'' is the class of all sequences of length less than α whose elements are in M.  Notice that for the \"super\" versions, γ should be less than j(κ), not <math>{j^n(\\kappa)}</math>.\n\nκ is '''almost n-huge''' if and only if there is ''j'' : ''V'' → ''M'' with critical point κ and\n\n:<math>{}^{<j^n(\\kappa)}M \\subset M.\\!</math>\n\nκ is '''super almost n-huge''' if and only if for every ordinal γ there is ''j'' : ''V'' → ''M'' with critical point κ, γ&lt;j(κ), and\n\n:<math>{}^{<j^n(\\kappa)}M \\subset M.\\!</math>\n\nκ is '''n-huge''' if and only if there is ''j'' : ''V'' → ''M'' with critical point κ and\n\n:<math>{}^{j^n(\\kappa)}M \\subset M.\\!</math>\n\nκ is '''super n-huge''' if and only if for every ordinal γ there is ''j'' : ''V'' → ''M'' with critical point κ, γ&lt;j(κ), and\n\n:<math>{}^{j^n(\\kappa)}M \\subset M.\\!</math>\n\nNotice that 0-huge is the same as [[measurable cardinal]]; and 1-huge is the same as huge. A cardinal satisfying one of the [[rank into rank]] axioms is ''n''-huge for all finite ''n''.\n\nThe existence of an almost huge cardinal implies that [[Vopěnka's principle]] is consistent; more precisely any almost huge cardinal is also a [[Vopěnka cardinal]].\n\n== Consistency strength ==\nThe cardinals are arranged in order of increasing consistency strength as follows:\n*almost ''n''-huge\n*super almost ''n''-huge\n*''n''-huge\n*super ''n''-huge\n*almost ''n''+1-huge\nThe consistency of a huge cardinal implies the consistency of a [[supercompact cardinal]], nevertheless, the least huge cardinal is smaller than the least supercompact cardinal (assuming both exist).\n\n==ω-huge cardinals==\nOne can try defining an ω-huge cardinal κ as one such that an elementary embedding j : V → M from V into a transitive inner model M with critical point κ and <sup>λ</sup>''M''⊆''M'', where λ is the supremum of ''j''<sup>''n''</sup>(κ) for positive integers ''n''. However [[Kunen's inconsistency theorem]]  shows that such cardinals are inconsistent in ZFC, though it is still open whether they are consistent in ZF. Instead an ω-huge cardinal κ is defined as the critical point of an elementary embedding from some rank ''V''<sub>λ+1</sub> to itself. This is closely related to the [[rank-into-rank]] axiom I<sub>1</sub>.\n\n== See also ==\n\n*[[List of large cardinal properties]]\n*The [[Dehornoy order]] on a braid group was motivated by properties of huge cardinals.\n\n== References ==\n*{{Citation|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}.\n*{{Citation | last=Kunen | first=Kenneth | authorlink=Kenneth Kunen | title=Saturated ideals | doi=10.2307/2271949 | jstor=2271949 | mr=495118 | year=1978 | journal=The Journal of Symbolic Logic | issn=0022-4812 | volume=43 | issue=1 | pages=65–76}}.\n*{{Citation|last=Maddy|first=Penelope|authorlink=Penelope Maddy|journal=The Journal of Symbolic Logic|title=Believing the Axioms. II|year=1988|volume=53|issue=3|pages=736-764 (esp. 754-756)|doi=10.2307/2274569|jstor=2274569}}. A copy of parts I and II of this article with corrections is available at the [http://faculty.sites.uci.edu/pjmaddy/bibliography/ author's web page].\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Inaccessible cardinal",
      "url": "https://en.wikipedia.org/wiki/Inaccessible_cardinal",
      "text": "{{short description|cardinal unobtainible from smaller cardinals via usual cardinal arithmetic}}\nIn [[set theory]], an [[uncountable set|uncountable]] [[cardinal number|cardinal]] is '''inaccessible''' if it cannot be obtained from smaller cardinals by the usual operations of [[cardinal arithmetic]].  More precisely, a cardinal κ is '''strongly inaccessible''' if it is uncountable, it is not a sum of fewer than κ cardinals that are less than κ, and <math>\\alpha < \\kappa</math> implies <math>2^{\\alpha} < \\kappa</math>.\n\nThe term \"inaccessible cardinal\" is ambiguous. Until about 1950 it meant \"weakly inaccessible cardinal\", but since then it usually means \"strongly inaccessible cardinal\".  An uncountable cardinal is '''weakly inaccessible''' if it is a [[regular cardinal|regular]] weak [[limit cardinal]].  It is '''strongly inaccessible''', or just '''inaccessible''', if it is a regular strong limit cardinal (this is equivalent to the definition given above).  Some authors do not require weakly and strongly inaccessible cardinals to be uncountable (in which case <math>\\aleph_0</math> is strongly inaccessible). Weakly inaccessible cardinals were introduced by {{harvtxt|Hausdorff|1908}}, and strongly inaccessible ones by {{harvtxt|Sierpiński|Tarski|1930}} and {{harvtxt|Zermelo|1930}}.\n\nEvery strongly inaccessible cardinal is also weakly inaccessible, as every strong limit cardinal is also a weak limit cardinal. If the [[Continuum hypothesis#The generalized continuum hypothesis|generalized continuum hypothesis]] holds, then a cardinal is strongly inaccessible if and only if it is weakly inaccessible.\n\n<math>\\aleph_0</math> ([[aleph number|aleph-null]]) is a regular strong limit cardinal. Assuming the [[axiom of choice]], every other infinite cardinal number is regular or a (weak) limit. However, only a rather large cardinal number can be both and thus weakly inaccessible.\n\nAn [[ordinal number|ordinal]] is a weakly inaccessible cardinal if and only if it is a regular ordinal and it is a limit of regular ordinals. (Zero, one, and <math>\\aleph_0</math> are regular ordinals, but not limits of regular ordinals.) A cardinal which is weakly inaccessible and also a strong limit cardinal is strongly inaccessible.\n\nThe assumption of the existence of a strongly inaccessible cardinal is sometimes applied in the form of the assumption that one can work inside a [[Grothendieck universe]], the two ideas being intimately connected.\n\n== Models and consistency ==\n\n[[Zermelo–Fraenkel set theory]] (ZFC) implies that the [[Von Neumann universe|''V''<sub>κ</sub>]] is a [[model theory|model]] of ZFC whenever κ is strongly inaccessible. And ZF implies that the [[Gödel's constructible universe|Gödel universe]] ''L''<sub>κ</sub> is a model of ZFC whenever κ is weakly inaccessible. Thus ZF together with \"there exists a weakly inaccessible cardinal\" implies that ZFC is consistent. Therefore, inaccessible cardinals are a type of [[large cardinal]].\n\nIf ''V'' is a standard model of ZFC and κ is an inaccessible in ''V'', then: ''V''<sub>κ</sub> is one of the intended models of [[Zermelo–Fraenkel set theory]]; and Def(''V''<sub>κ</sub>) is one of the intended models of Mendelson's version of [[Von Neumann–Bernays–Gödel set theory]] which excludes global choice, replacing limitation of size by replacement and ordinary choice; and ''V''<sub>κ+1</sub> is one of the intended models of [[Morse–Kelley set theory]]. Here Def (''X'') is the Δ<sub>0</sub> definable subsets of ''X'' (see [[constructible universe]]). However, κ does not need to be inaccessible, or even a cardinal number, in order for ''V''<sub>κ</sub> to be a standard model of ZF (see [[Inaccessible cardinal#Two model-theoretic characterisations of inaccessibility|below]]).\n\nSuppose V is a model of ZFC. Either V contains no strong inaccessible or, taking κ to be the smallest strong inaccessible in V, ''V''<sub>κ</sub> is a standard model of ZFC which contains no strong inaccessibles. Thus, the consistency of ZFC implies consistency of ZFC+\"there are no strong inaccessibles\". Similarly, either V contains no weak inaccessible or, taking κ to be the smallest ordinal which is weakly inaccessible relative to any standard sub-model of V, then ''L''<sub>κ</sub> is a standard model of ZFC which contains no weak inaccessibles. So consistency of ZFC implies consistency of ZFC+\"there are no weak inaccessibles\". This shows that ZFC cannot prove the existence of an inaccessible cardinal, so ZFC is consistent with the non-existence of any inaccessible cardinals.\n\nThe issue whether ZFC is consistent with the existence of an inaccessible cardinal is more subtle. The proof sketched in the previous paragraph that the consistency of ZFC implies the consistency of ZFC + \"there is not an inaccessible cardinal\" can be formalized in ZFC. However, assuming that ZFC is consistent, no proof that the consistency of ZFC implies the consistency of ZFC + \"there is an inaccessible cardinal\" can be formalized in ZFC. This follows from [[Gödel's second incompleteness theorem]], which shows that if ZFC + \"there is an inaccessible cardinal\" is consistent, then it cannot prove its own consistency. Because ZFC + \"there is an inaccessible cardinal\" does prove the consistency of ZFC, if ZFC proved that its own consistency implies the consistency of ZFC + \"there is an inaccessible cardinal\" then this latter theory would be able to prove its own consistency, which is impossible if it is consistent.\n\nThere are arguments for the existence of inaccessible cardinals that cannot be formalized in ZFC. One such argument, presented by {{harvtxt|Hrbacek|Jech|1999|p=279}}, is that the class of all ordinals of a particular model ''M'' of set theory would itself be an inaccessible cardinal if there was a larger model of set theory extending ''M'' and preserving powerset of elements of ''M''.\n\n==Existence of a proper class of inaccessibles==\nThere are many important axioms in set theory which assert the existence of a proper class of cardinals which satisfy a predicate of interest. In the case of inaccessibility, the corresponding axiom is the assertion that for every cardinal μ, there is an inaccessible cardinal κ which is strictly larger, μ < κ. Thus this axiom guarantees the existence of an infinite tower of inaccessible cardinals (and may occasionally be referred to as the inaccessible cardinal axiom). As is the case for the existence of any inaccessible cardinal, the inaccessible cardinal axiom is unprovable from the axioms of ZFC. Assuming ZFC, the inaccessible cardinal axiom is equivalent to the '''universe axiom''' of [[Grothendieck]] and [[Jean-Louis Verdier|Verdier]]: every set is contained in a [[Grothendieck universe]]. The axioms of ZFC along with the universe axiom (or equivalently the inaccessible cardinal axiom) are denoted ZFCU (which could be confused with ZFC with [[urelement]]s). This axiomatic system is useful to prove for example that every [[category (mathematics)|category]] has an appropriate [[Yoneda embedding]].\n\nThis is a relatively weak large cardinal axiom since it amounts to saying that ∞ is 1-inaccessible in the language of the next section, where ∞ denotes the least ordinal not in V, i.e. the class of all ordinals in your model.\n\n== α-inaccessible cardinals and hyper-inaccessible cardinals ==\n\nThe term \"α-inaccessible cardinal\" is ambiguous and different authors use inequivalent definitions. One definition is that \na cardinal κ is called '''α-inaccessible''', for α any ordinal, if κ is inaccessible and for every ordinal β < α, the set of β-inaccessibles less than κ is unbounded in κ (and thus of cardinality κ, since κ is regular). In this case the 0-inaccessible cardinals are the same as strongly inaccessible cardinals. Another possible definition is that a cardinal κ is called '''α-weakly inaccessible'''  if κ is regular and for every ordinal β < α, the set of β-weakly inaccessibles less than κ is unbounded in κ. In this case the 0-weakly inaccessible cardinals are the regular cardinals and the 1-weakly inaccessible cardinals are the weakly inaccessible cardinals. \n\nThe α-inaccessible cardinals can also be described as fixed points of functions which count the lower inaccessibles. For example, denote by ψ<sub>0</sub>(λ) the λ<sup>th</sup> inaccessible cardinal, then the fixed points of ψ<sub>0</sub> are the 1-inaccessible cardinals. Then letting ψ<sub>β</sub>(λ) be the λ<sup>th</sup> β-inaccessible cardinal, the fixed points of ψ<sub>β</sub> are the (β+1)-inaccessible cardinals (the values ψ<sub>β+1</sub>(λ)). If α is a limit ordinal, an α-inaccessible is a fixed point of every ψ<sub>β</sub> for β < α (the value ψ<sub>α</sub>(λ) is the λ<sup>th</sup> such cardinal).   This process of taking fixed points of functions generating successively larger cardinals is commonly encountered in the study of [[List of large cardinal properties|large cardinal numbers]].\n\nThe term '''hyper-inaccessible''' is ambiguous and has at least three incompatible meanings. Many authors use it to mean a regular limit of strongly inaccessible cardinals (1-inaccessible). Other authors use it to mean that κ is κ-inaccessible. (It can never be κ+1-inaccessible.) It is occasionally used to mean [[Mahlo cardinal]].\n\nThe term '''α-hyper-inaccessible''' is also ambiguous. Some authors use it to mean α-inaccessible. Other authors use the definition that\nfor any ordinal α, a cardinal κ is '''α-hyper-inaccessible''' if and only if κ is hyper-inaccessible and for every ordinal β < α, the set of β-hyper-inaccessibles less than κ is unbounded in κ.\n\nHyper-hyper-inaccessible cardinals and so on can be defined in similar ways, and as usual this term is ambiguous.\n\nUsing \"weakly inaccessible\" instead of \"inaccessible\", similar definitions can be made for \"weakly α-inaccessible\", \"weakly hyper-inaccessible\", and \"weakly α-hyper-inaccessible\".\n\n[[Mahlo cardinal]]s are inaccessible, hyper-inaccessible, hyper-hyper-inaccessible, ... and so on.\n\n== Two model-theoretic characterisations of inaccessibility ==\n\nFirstly, a cardinal κ is inaccessible if and only if κ has the following [[Reflection principle|reflection]] property: for all subsets U ⊂ V<sub>κ</sub>, there exists α < κ such that <math>(V_\\alpha,\\in,U\\cap V_\\alpha)</math> is an [[elementary substructure]] of <math>(V_\\kappa,\\in,U)</math>. (In fact, the set of such α is [[Club set|closed unbounded]] in κ.) Equivalently, κ is <math>\\Pi_n^0</math>-[[Totally indescribable cardinal|indescribable]] for all n ≥ 0.\n\nIt is provable in ZF that ∞ satisfies a somewhat weaker reflection property, where the substructure (V<sub>α</sub>, ∈, U ∩ V<sub>α</sub>) is only required to be 'elementary' with respect to a finite set of formulas. Ultimately, the reason for this weakening is that whereas the model-theoretic satisfaction relation <math>\\models</math> can be defined, truth itself cannot, due to [[Tarski's undefinability theorem|Tarski's theorem]].\n\nSecondly, under ZFC it can be shown that κ is inaccessible if and only if (V<sub>κ</sub>, ∈) is a model of [[Second order logic|second order]] ZFC.\n\nIn this case, by the reflection property above, there exists α < κ such that (V<sub>α</sub>, ∈) is a standard model of ([[First order logic|first order]]) ZFC. Hence, the existence of an inaccessible cardinal is a stronger hypothesis than the existence of a standard model of ZFC.\n\n==See also==\n*[[Mahlo cardinal]]\n*[[Club set]]\n*[[Inner model]]\n*[[Von Neumann universe]]\n*[[Constructible universe]]\n\n== References ==\n* {{citation\n | author=Drake, F. R.\n | title=Set Theory: An Introduction to Large Cardinals\n | series = Studies in Logic and the Foundations of Mathematics\n | volume=76\n | publisher=Elsevier Science Ltd\n | year=1974\n | isbn=0-444-10535-2\n}}\n*{{Citation\n | last1=Hausdorff\n | first1=Felix\n | author1-link=Felix Hausdorff \n | title=Grundzüge einer Theorie der geordneten Mengen\n | doi=10.1007/BF01451165\n | url=https://gdz.sub.uni-goettingen.de/id/PPN235181684_0065?tify={%22pages%22:[453]}\n | year=1908\n | journal=[[Mathematische Annalen]]\n | issn=0025-5831\n | volume=65\n | issue=4\n | pages=435–505\n}}\n* {{citation\n | last1=Hrbáček\n | first1=Karel\n | author1-link=Karel Hrbáček\n | last2=Jech\n | first2=Thomas\n | author2-link=Thomas Jech\n | title=Introduction to set theory\n | publisher=Dekker\n | location=New York\n | edition=3rd\n | isbn=978-0-8247-7915-3\n | year=1999\n}}\n* {{citation\n | last=Kanamori\n | first=Akihiro\n | year=2003\n | publisher=Springer\n | authorlink=Akihiro Kanamori\n | title=The Higher Infinite: Large Cardinals in Set Theory from Their Beginnings\n | edition=2nd\n | isbn=3-540-00384-3\n}}\n*{{Citation\n | last1=Sierpiński\n | first1=Wacław\n | author1-link=Wacław Sierpiński\n | last2=Tarski\n | first2=Alfred\n | author2-link=Alfred Tarski\n | title=Sur une propriété caractéristique des nombres inaccessibles\n | url=http://matwbn.icm.edu.pl/ksiazki/fm/fm15/fm15129.pdf\n | year=1930\n | journal=[[Fundamenta Mathematicae]]\n | issn=0016-2736\n | volume=15\n | pages=292–300\n}}\n*{{Citation\n | last1=Zermelo\n | first1=Ernst\n | author1-link=Ernst Zermelo\n | title= Über Grenzzahlen und Mengenbereiche: neue Untersuchungen über die Grundlagen der Mengenlehre\n | url=http://matwbn.icm.edu.pl/ksiazki/fm/fm16/fm1615.pdf \n | year=1930\n | journal=[[Fundamenta Mathematicae]]\n | issn=0016-2736\n | volume=16\n | pages=29–47\n}}. English translation: {{Citation | last = Ewald | first = William B. (ed.) | title = From Immanuel Kant to David Hilbert: A Source Book in the Foundations of Mathematics | chapter = On boundary numbers and domains of sets: new investigations in the foundations of set theory | pages = 1208&ndash;1233 | publisher = Oxford University Press | year = 1996 | isbn = 978-0-19-853271-2}}.\n\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Indescribable cardinal",
      "url": "https://en.wikipedia.org/wiki/Indescribable_cardinal",
      "text": "In [[mathematics]], a '''Q-indescribable cardinal''' is a certain kind of [[large cardinal]] number that is hard to describe in some language ''Q''. There are many different types of indescribable cardinals corresponding to different choices of languages ''Q''. They were introduced by {{harvtxt|Hanf|Scott|1961}}.\n\nA cardinal number κ is called '''Π{{su|p=n|b=m}}-indescribable''' if for every Π<sub>m</sub> proposition φ, and set A ⊆ V<sub>κ</sub> with (V<sub>κ+n</sub>, ∈, A) ⊧ φ there exists an α &lt; κ with (V<sub>α+n</sub>, ∈, A ∩ V<sub>α</sub>) ⊧ φ.\nHere one looks at formulas with m-1 alternations of quantifiers with the outermost quantifier being universal. \n'''Σ{{su|p=n|b=m}}-indescribable''' cardinals are defined in a similar way. The idea is that κ cannot be distinguished (looking from below) from smaller cardinals by any formula of n+1-th order logic with m-1 alternations of quantifiers even with the advantage of an extra unary predicate symbol (for A).  This implies that it is large because it means that there must be many smaller cardinals with similar properties.\n\nThe cardinal number κ is called '''totally indescribable''' if it is Π{{su|p=n|b=m}}-indescribable for all positive integers ''m'' and ''n''.\n\nIf α is an ordinal, the cardinal number κ is called '''α-indescribable''' if for every formula φ and every subset ''U'' of ''V''<sub>κ</sub> \nsuch that φ(''U'') holds in ''V''<sub>κ+α</sub> there is a some λ<κ such that φ(''U'' ∩ ''V''<sub>λ</sub>) holds in ''V''<sub>λ+α</sub>. If α is infinite then α-indescribable ordinals are totally indescribable, and if α is finite they are the same as Π{{su|p=&alpha;|b=ω}}-indescribable ordinals.  α-indescribability implies that α<κ, but there is an alternative notion of [[shrewd cardinal]]s that makes sense when α≥κ: there is λ<κ and β such that φ(''U'' ∩ ''V''<sub>λ</sub>) holds in ''V''<sub>λ+β</sub>.\n\nΠ{{su|p=1|b=1}}-indescribable cardinals are the same as [[weakly compact cardinal]]s.\n\nA cardinal is inaccessible if and only if it is Π{{su|p=0|b=n}}-indescribable for all positive integers ''n'', equivalently iff it is Π{{su|p=0|b=2}}-indescribable, equivalently iff it is Σ{{su|p=1|b=1}}-indescribable.  A cardinal is Σ{{su|p=1|b=n+1}}-indescribable iff it is Π{{su|p=1|b=n}}-indescribable.  The property of being Π{{su|p=1|b=n}}-indescribable is Π{{su|p=1|b=n+1}}.  For m>1, the property of being Π{{su|p=m|b=n}}-indescribable is Σ{{su|p=m|b=n}} and the property of being Σ{{su|p=m|b=n}}-indescribable is Π{{su|p=m|b=n}}.  Thus, for m>1, every cardinal that is either Π{{su|p=m|b=n+1}}-indescribable or Σ{{su|p=m|b=n+1}}-indescribable is both Π{{su|p=m|b=n}}-indescribable and Σ{{su|p=m|b=n}}-indescribable and the set of such cardinals below it is stationary.  The consistency strength is Σ{{su|p=m|b=n}}-indescribable cardinals is below that of Π{{su|p=m|b=n}}-indescribable, but for m>1 it is consistent with ZFC that the least Σ{{su|p=m|b=n}}-indescribable exists and is above the least Π{{su|p=m|b=n}}-indescribable cardinal (this is proved from consistency of ZFC with Π{{su|p=m|b=n}}-indescribable cardinal and a Σ{{su|p=m|b=n}}-indescribable cardinal above it).\n\nMeasurable cardinals are Π{{su|p=2|b=1}}-indescribable, but the smallest measurable cardinal is not Σ{{su|p=2|b=1}}-indescribable. However there are many totally indescribable cardinals below any measurable cardinal.\n\nTotally indescribable cardinals remain totally indescribable in the [[constructible universe]] and in other canonical inner models, and similarly for Π{{su|p=m|b=n}} and Σ{{su|p=m|b=n}} indescribability.\n\n== References ==\n\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n*{{Citation | last1=Hanf | first1=W. P. | last2=Scott | first2=D. S. | author2-link=Dana Scott | title=Classifying inaccessible cardinals | year=1961 | journal=[[Notices of the American Mathematical Society]] | issn=0002-9920 | volume=8 | pages=445}}\n* {{cite book|last=Kanamori|first=Akihiro|author-link=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3|doi=10.1007/978-3-540-88867-3_2}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Indestructibility",
      "url": "https://en.wikipedia.org/wiki/Indestructibility",
      "text": "#REDIRECT [[Richard Laver#Research contributions]]\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Ineffable cardinal",
      "url": "https://en.wikipedia.org/wiki/Ineffable_cardinal",
      "text": "In the [[mathematics]] of [[transfinite number]]s, an '''ineffable cardinal''' is a certain kind of [[large cardinal]] number, introduced by {{harvtxt|Jensen|Kunen|1969}}.\n\nA [[cardinal number]] <math>\\kappa</math> is called '''almost ineffable''' if for every <math>f: \\kappa \\to \\mathcal{P}(\\kappa)</math> (where <math>\\mathcal{P}(\\kappa)</math> is the [[powerset]] of <math>\\kappa</math>) with the property that <math>f(\\delta)</math> is a subset of <math>\\delta</math> for all ordinals <math>\\delta < \\kappa</math>, there is a subset <math>S</math> of <math>\\kappa</math> having cardinal <math>\\kappa</math> and [[Homogeneous (large cardinal property)|homogeneous]] for <math>f</math>, in the sense that for any <math>\\delta_1 < \\delta_2</math> in <math>S</math>, <math>f(\\delta_1) = f(\\delta_2) \\cap \\delta_1</math>.\n\nA [[cardinal number]] <math>\\kappa</math> is called '''ineffable''' if for every binary-valued function <math>f : [\\kappa]^2\\to \\{0,1\\}</math>, there is a [[stationary subset]] of <math>\\kappa</math> on which <math>f</math> is [[Homogeneous (large cardinal property)|homogeneous]]: that is, either <math>f</math> maps all unordered pairs of elements drawn from that subset to zero, or it maps all such unordered pairs to one.\n\nMore generally, <math>\\kappa</math> is called '''<math>n</math>-ineffable''' (for a positive integer <math>n</math>) if  for every <math>f : [\\kappa]^n\\to \\{0,1\\}</math> there is a stationary subset of <math>\\kappa</math> on which <math>f</math> is '''<math>n</math>-[[Homogeneous (large cardinal property)|homogeneous]]''' (takes the same value for all unordered <math>n</math>-tuples drawn from the subset). Thus, it is ineffable if and only if it is 2-ineffable.\n\nA '''totally ineffable''' cardinal is a cardinal that is <math>n</math>-ineffable for every <math>2 \\leq n < \\aleph_0</math>. If <math>\\kappa</math> is <math>(n+1)</math>-ineffable, then the set of <math>n</math>-ineffable cardinals below <math>\\kappa</math> is a stationary subset of <math>\\kappa</math>.\n\nEvery ''n''-ineffable cardinal is ''n''-almost ineffable (with set of ''n''-almost ineffable below it stationary), and every ''n''-almost ineffable is ''n''-subtle (with set of ''n''-subtle below it stationary).  The least ''n''-subtle cardinal is not even [[weakly compact cardinal|weakly compact]] (and unlike ineffable cardinals, the least ''n''-almost ineffable is <math>\\Pi^1_2</math>-describable), but ''n''-1-ineffable cardinals are stationary below every ''n''-subtle cardinal.\n\nA cardinal κ is '''completely ineffable''' iff there is a non-empty <math>R \\subset \\mathcal{P}(\\kappa)</math> such that<br/>\n- every <math>A \\in R</math> is stationary<br/>\n- for every <math>A \\in R</math> and <math>f : [\\kappa]^2\\to \\{0,1\\}</math>, there is <math>B \\subset A</math> homogeneous for ''f'' with <math>B \\in R</math>.\n\nUsing any finite ''n''>1 in place of 2 would lead to the same definition, so completely ineffable cardinals are totally ineffable (and have greater consistency strength).  Completely ineffable cardinals are <math>\\Pi^1_n</math>-indescribable for every ''n'', but the property of being completely ineffable is <math>\\Delta^2_1</math>. \n\nThe consistency strength of completely ineffable is below that of 1-iterable cardinals, which in turn is below [[remarkable cardinal]]s, which in turn is below [[Erdős cardinal|ω-Erdős]] cardinals. A list of large cardinal axioms by consistency strength is available [[List_of_large_cardinal_properties | here]].\n\n==References==\n\n*{{citation|doi=10.1016/S0168-0072(00)00019-1|first=Harvey|last=Friedman|authorlink=Harvey Friedman|title=Subtle cardinals and linear orderings|journal=Annals of Pure and Applied Logic|year=2001|volume=107|issue=1–3|pages=1–34}}. \n*{{citation|title=Some Combinatorial Properties of L and V |url=http://www.mathematik.hu-berlin.de/~raesch/org/jensen.html |first=Ronald|last=Jensen|authorlink=Ronald Jensen |first2=Kenneth|last2=Kunen|author2-link=Kenneth Kunen |publisher=Unpublished manuscript|year=1969}}\n[[Category:Large cardinals]]\n\n{{settheory-stub}}"
    },
    {
      "title": "Iterable cardinal",
      "url": "https://en.wikipedia.org/wiki/Iterable_cardinal",
      "text": "In [[mathematics]], an '''iterable cardinal''' is a type of [[large cardinal]] introduced by {{harvs|txt|last=Gitman|year=2011}}, and {{harvs|txt|last=Sharpe and Welch|year=2011}}, and further studied by {{harvs|txt|last=Gitman|last2=Welch|year=2011}}. Sharpe and Welch defined a cardinal ''κ'' to be ''iterable'' if every subset of ''κ'' is contained in a weak ''κ''-model ''M'' for which there exists an ''M''-[[ultrafilter]] on ''κ'' which allows for wellfounded iterations by ultrapowers of arbitrary length.\nGitman gave a finer notion, where a cardinal ''κ'' is defined to be ''α''-iterable\nif  ultrapower iterations only of length ''α'' are required to wellfounded. (By standard arguments iterability is equivalent to ''&#969;<sub>1</sub>''-iterability.)\n\n==References==\n\n*{{citation|mr=2830435 \n|last=Gitman|first= Victoria\n|title=Ramsey-like cardinals I\n|journal=Journal of Symbolic Logic|volume= 76 |year=2011|issue= 2|pages= 519–540|doi=10.2178/jsl/1305810762 |arxiv=0801.4723}}\n*{{citation|mr=2830435 \n|last=Gitman|first= Victoria|last2= Welch|first2= P. D.\n|title=Ramsey-like cardinals II\n|journal=Journal of Symbolic Logic|volume= 76 |year=2011|issue= 2|pages= 541–560|doi=10.2178/jsl/1305810763 |arxiv=1104.4448}}\n*{{citation|mr=MR2817562 \n|last=Sharpe|first= Ian|last2= Welch|first2= P. D.\n|title=Greatly Erd&#337;s Cardinals with some generalizations to the Chang and Ramsey properties|journal=Annals of Pure and Applied Logic|volume=  |year=2011|issue= 2|pages= 863-902|doi=10.1016/j.apal.2011.04.002}}\n\n\n==External links==\n*[http://boolesrings.org/victoriagitman/files/2013/06/ramseydiagram.jpg Diagram of iterable cardinals]\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Jónsson cardinal",
      "url": "https://en.wikipedia.org/wiki/J%C3%B3nsson_cardinal",
      "text": "In [[set theory]], a '''Jónsson cardinal''' (named after [[Bjarni Jónsson]]) is a certain kind of [[large cardinal]] number.\n\nAn [[uncountable set|uncountable]] [[cardinal number]] κ is said to be '''''Jónsson''''' if for every function ''f'': [κ]<sup><ω</sup> → κ there is a set ''H'' of order type κ such that for each ''n'', ''f'' restricted to ''n''-element subsets of ''H'' omits at least one value in κ.\n\nEvery [[Rowbottom cardinal]] is Jónsson. By a theorem of Eugene M. Kleinberg, the theories ZFC + “there is a [[Rowbottom cardinal]]” and ZFC + “there is a Jónsson cardinal” are equiconsistent. [[William Mitchell (mathematician)|William Mitchell]] proved, with the help of the Dodd-Jensen [[core model]] that the consistency of the existence of a Jónsson cardinal implies the consistency of the existence of a [[Ramsey cardinal]], so that the existence of Jónsson cardinals and the existence of Ramsey cardinals are equiconsistent.<ref>Mitchell, William J.: \"Jonsson Cardinals, Erdos Cardinals and the Core Model\", Journal of Symbolic Logic 64(3):1065-1086, 1999.</ref>\n\nIn general, Jónsson cardinals need not be large cardinals in the usual sense: they can be [[singular cardinal|singular]]. But the existence of a singular Jónsson cardinal is equiconsistent to the existence of a [[measurable cardinal]]. Using the [[axiom of choice]], a lot of small cardinals (the <math>\\aleph_n</math>, for instance) can be proved to be not Jónsson. Results like this need the axiom of choice, however: The [[axiom of determinacy]] does imply that for every positive natural number ''n'', the cardinal <math>\\aleph_n</math> is Jónsson.\n\nA '''Jónsson algebra''' is an algebra with no proper subalgebras of the same cardinality. (They are unrelated to [[Jónsson–Tarski algebra]]s). Here an algebra means\na model for a language with a countable number of function symbols, in other words a set with a countable number of functions from finite products of the set to itself. A cardinal is a Jónsson cardinal if and only if there are no Jónsson algebras of that cardinality. The existence of [[Jónsson function]]s shows that if algebras are allowed to have infinitary operations, then there are no analogues of Jónsson cardinals.\n\n==References==\n{{Reflist}}\n\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|authorlink=Akihiro Kanamori|publisher=Springer|title=The Higher Infinite: Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n*{{Citation | last1=Jónsson | first1=Bjarni | title=Topics in universal algebra | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Mathematics | doi=10.1007/BFb0058648 |mr=0345895 | year=1972 | volume=250}}\n\n{{DEFAULTSORT:Jonsson Cardinal}}\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Kunen's inconsistency theorem",
      "url": "https://en.wikipedia.org/wiki/Kunen%27s_inconsistency_theorem",
      "text": "In [[set theory]], a branch of mathematics, '''Kunen's inconsistency theorem''', proved by {{harvs|txt|first=Kenneth|authorlink=Kenneth Kunen|last=Kunen|year=1971}}, shows that several plausible [[large cardinal]] axioms are [[Consistency|inconsistent]] with the [[axiom of choice]].\n\nSome consequences of Kunen's theorem (or its proof) are:\n*There is no non-trivial [[Elementary equivalence|elementary embedding]] of the universe ''V'' into itself. In other words, there is no [[Reinhardt cardinal]].\n*If ''j'' is an elementary embedding of the universe ''V'' into an inner model ''M'', and &lambda; is the smallest fixed point of ''j'' above the [[Critical point (set theory)|critical point]] &kappa; of ''j'', then ''M'' does not contain the set ''j''&nbsp;\"&lambda; (the image of ''j'' restricted  to &lambda;).\n*There is no [[ω-huge cardinal]].\n*There is no non-trivial elementary embedding of ''V''<sub>&lambda;+2</sub> into itself.\n\nIt is not known if Kunen's theorem still holds in ZF (ZFC without the axiom of choice), though {{harvtxt|Suzuki|1999}} showed that there is no definable elementary embedding from ''V'' into ''V''.  That is there is no formula ''J'' in the language of set theory such that for some parameter ''p''&isin;''V'' for all sets ''x''&isin;''V'' and ''y''&isin;''V'': <math>j(x)=y \\leftrightarrow J(x,y,p) \\,.</math>\n\nKunen used [[Morse–Kelley set theory]] in his proof. If the proof is re-written to use ZFC, then one must add the assumption that replacement holds for formulas involving ''j''.  Otherwise one could not even show that ''j''&nbsp;\"&lambda; exists as a set. The forbidden set ''j''&nbsp;\"&lambda; is crucial to the proof. The proof first shows that it cannot be in ''M''. The other parts of the theorem are derived from that.\n\nIt is possible to have models of set theory that have elementary embeddings into themselves, at least if one assumes some mild large cardinal axioms. For example, if [[zero sharp|0#]] exists then there is an elementary embedding from the [[constructible universe]] ''L'' into itself. This does not contradict Kunen's theorem because if 0# exists then ''L'' cannot be the whole universe of sets.\n\n==See also==\n*[[Rank-into-rank]]\n\n==References==\n*{{Citation | last=Kanamori | first=Akihiro | author-link=Akihiro Kanamori | title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-3-540-00384-7 | doi=10.1007/978-3-540-88867-3 | year=2003}}\n*{{citation\n|last=Kunen|first= Kenneth\n|title=Elementary embeddings and infinitary combinatorics\n|journal=[[Journal of Symbolic Logic]] |volume=36 |year=1971|pages= 407–413\n|doi=10.2307/2269948|jstor=2269948\n|mr=0311478\n|issue=3}} \n*{{Citation | last1=Suzuki | first1=Akira | title=No elementary embedding from V into V is definable from parameters |mr=1780073 | year=1999 | journal=[[Journal of Symbolic Logic]] | issn=0022-4812 | volume=64 | issue=4 | pages=1591–1594 | doi=10.2307/2586799}}\n*{{Citation | last1=Zapletal | first1=Jindřich | title=A new proof of Kunen's inconsistency |mr=1317054 | year=1996 | journal=[[Proceedings of the American Mathematical Society]] | issn=0002-9939 | volume=124 | issue=7 | pages=2203–2204 | doi=10.1090/S0002-9939-96-03281-9}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Laver function",
      "url": "https://en.wikipedia.org/wiki/Laver_function",
      "text": "In [[set theory]], a '''Laver function''' (or '''Laver diamond''', named after its inventor, [[Richard Laver]]) is a function connected with [[supercompact cardinal]]s.\n\n==Definition==\nIf κ is a supercompact cardinal, a Laver function is a function ''ƒ'':κ&nbsp;→&nbsp;''V''<sub>κ</sub> such that for every set ''x'' and every cardinal λ&nbsp;≥&nbsp;|TC(''x'')|&nbsp;+&nbsp;κ there is a supercompact measure ''U'' on [λ]<sup><κ</sup> such that if ''j''<sub>&nbsp;''U''</sub> is the associated elementary embedding then ''j''<sub>&nbsp;''U''</sub>(''ƒ'')(κ) = ''x''. (Here ''V''<sub>κ</sub> denotes the κ-th level of the [[cumulative hierarchy]], TC(''x'') is the [[transitive set|transitive closure]] of ''x'')\n\n==Applications==\nThe original application of Laver functions was the following theorem of Laver.   \nIf κ is supercompact, there is a κ-c.c. [[forcing (mathematics)|forcing]] notion (''P'',&nbsp;≤) such after forcing with (''P'',&nbsp;≤) the following holds: κ is supercompact and remains supercompact after forcing with any κ-directed closed forcing.\n\nThere are many other applications, for example the proof of the consistency of the [[proper forcing axiom]].\n\n==References==\n*{{cite journal | zbl=0381.03039 | first=Richard | last=Laver | authorlink=Richard Laver | title=Making the supercompactness of κ indestructible under κ-directed closed forcing | journal=[[Israel Journal of Mathematics]] | volume=29 | year=1978 | pages=385–388 | doi=10.1007/bf02761175}}\n\n[[Category:Set theory]]\n[[Category:Large cardinals]]\n[[Category:Functions and mappings]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Mahlo cardinal",
      "url": "https://en.wikipedia.org/wiki/Mahlo_cardinal",
      "text": "In [[mathematics]], a '''Mahlo cardinal''' is a certain kind of [[large cardinal]] number.  Mahlo cardinals were first described by {{harvs|txt|authorlink= Paul Mahlo|first=Paul|last=Mahlo|year1=1911|year2=1912|year3=1913}}.  As with all large cardinals,  none of these varieties of Mahlo cardinals can be proved to exist by [[ZFC]] (assuming ZFC is consistent).\n\nA [[cardinal number]] κ is called ''strongly Mahlo'' if κ is [[inaccessible cardinal|strongly inaccessible]] and the [[Set (mathematics)|set]] U = {λ &lt; κ: λ is strongly inaccessible} is [[stationary set#Classical notion|stationary]] in κ.\n\nA cardinal κ is called ''weakly Mahlo'' if  κ is weakly inaccessible and the set of weakly inaccessible cardinals less than κ is stationary in κ.\n\nThe term \"Mahlo cardinal\" now usually means \"strongly Mahlo cardinal\", though the cardinals originally considered by Mahlo were weakly Mahlo cardinals.\n\n== Minimal condition sufficient for a Mahlo cardinal ==\n\n* If κ is a limit ''ordinal'' and the set of regular ordinals less than κ is stationary in κ, then κ is weakly Mahlo.\n\nThe main difficulty in proving this is to show that κ is regular.  We will suppose that it is not regular and construct a [[club set]] which gives us a μ such that:  \n:μ = cf(μ) < cf(κ) < μ < κ which is a contradiction.\nIf κ were not regular, then cf(κ) < κ.  We could choose a strictly increasing and continuous cf(κ)-sequence which begins with cf(κ)+1 and has κ as its limit.  The limits of that sequence would be club in κ.  So there must be a regular μ among those limits.  So μ is a limit of an initial subsequence of the cf(κ)-sequence.  Thus its cofinality is less than the cofinality of κ and greater than it at the same time; which is a contradiction.  Thus the assumption that κ is not regular must be false, i.e. κ is regular.\n\nNo stationary set can exist below <math>\\aleph_0</math> with the required property because {2,3,4,...} is club in ω but contains no regular ordinals; so κ is uncountable.  And it is a regular limit of regular cardinals; so it is weakly inaccessible.  Then one uses the set of uncountable limit cardinals below κ as a club set to show that the stationary set may be assumed to consist of weak inaccessibles.\n\n*If κ is weakly Mahlo and also a strong limit, then κ is Mahlo.\n\nκ is weakly inaccessible and a strong limit, so it is strongly inaccessible.\n\nWe show that the set of uncountable strong limit cardinals below κ is club in κ.  Let μ<sub>0</sub> be the larger of the threshold and ω<sub>1</sub>.  For each finite n, let μ<sub>n+1</sub> = 2<sup>μ<sub>n</sub></sup> which is less than κ because it is a strong limit cardinal.  Then their limit is a strong limit cardinal and is less than κ by its regularity.  The limits of uncountable strong limit cardinals are also uncountable strong limit cardinals.  So the set of them is club in κ.  Intersect that club set with the stationary set of weakly inaccessible cardinals less than κ to get a stationary set of strongly inaccessible cardinals less than κ.\n\n== Example: showing that Mahlo cardinals κ are κ-inaccessible (hyper-inaccessible)==\n\nThe term \"hyper-inaccessible\" is ambiguous. In this section, a cardinal κ is called hyper-inaccessible if it is κ-inaccessible (as opposed to the more common meaning of 1-inaccessible).\n\nSuppose κ is Mahlo.  We proceed by transfinite induction on α to show that κ is α-inaccessible for any α ≤ κ.  Since κ is Mahlo, κ is inaccessible; and thus 0-inaccessible, which is the same thing.\n\nIf κ is α-inaccessible, then there are β-inaccessibles (for β < α) arbitrarily close to κ.  Consider the set of simultaneous limits of such β-inaccessibles larger than some threshold but less than κ.  It is unbounded in κ (imagine rotating through β-inaccessibles for β < α ω-times choosing a larger cardinal each time, then take the limit which is less than κ by regularity (this is what fails if α ≥ κ)).  It is closed, so it is club in κ.  So, by κ's Mahlo-ness, it contains an inaccessible.  That inaccessible is actually an α-inaccessible.  So κ is α+1-inaccessible.\n\nIf λ ≤ κ is a limit ordinal and κ is α-inaccessible for all α < λ, then every β < λ is also less than α for some α < λ.  So this case is trivial.  In particular, κ is κ-inaccessible and thus [[inaccessible cardinal|hyper-inaccessible]].\n\nTo show that κ is a limit of hyper-inaccessibles and thus 1-hyper-inaccessible, we need to show that the diagonal set of cardinals μ < κ which are α-inaccessible for every α < μ is club in κ.  Choose a 0-inaccessible above the threshold, call it α<sub>0</sub>.  Then pick an α<sub>0</sub>-inaccessible, call it α<sub>1</sub>.  Keep repeating this and taking limits at limits until you reach a fixed point, call it μ.  Then μ has the required property (being a simultaneous limit of α-inaccessibles for all α < μ) and is less than κ by regularity.  Limits of such cardinals also have the property, so the set of them is club in κ.  By Mahlo-ness of κ, there is an inaccessible in this set and it is hyper-inaccessible.  So κ is 1-hyper-inaccessible.  We can intersect this same club set with the stationary set less than κ to get a stationary set of hyper-inaccessibles less than κ.\n\nThe rest of the proof that κ is α-hyper-inaccessible mimics the proof that it is α-inaccessible.  So κ is hyper-hyper-inaccessible, etc..\n\n== α-Mahlo, hyper-Mahlo and greatly Mahlo cardinals ==\n\nThe term α-Mahlo is ambiguous and different authors give inequivalent definitions. One definition is that \na cardinal κ is called α-Mahlo for some ordinal α if κ is strongly inaccessible and for every ordinal β<α, the set of β-Mahlo cardinals below κ is stationary in κ. However the condition  \"κ is strongly inaccessible\" is sometimes replaced by other conditions, such as \"κ is regular\" or \"κ is weakly inaccessible\" or \"κ is Mahlo\".  We can define \"hyper-Mahlo\", \"α-hyper-Mahlo\", \"hyper-hyper-Mahlo\", \"weakly α-Mahlo\", \"weakly hyper-Mahlo\", \"weakly α-hyper-Mahlo\", and so on, by analogy with the definitions for inaccessibles, so for example a cardinal κ is called hyper-Mahlo if it is κ-Mahlo.\n\nA cardinal κ is '''greatly Mahlo''' or '''κ<sup>+</sup>-Mahlo''' if and only if it is inaccessible and there is a normal (i.e. nontrivial and closed under [[diagonal intersection]]s) κ-complete filter on the power set of κ that is closed under the Mahlo operation, which maps the set of ordinals ''S'' to {α<math>\\in</math>''S'': α has uncountable cofinality and S∩α is stationary in α}\n\nThe properties of being inaccessible, Mahlo, weakly Mahlo, α-Mahlo, greatly Mahlo, etc. are preserved if we replace the universe by an [[inner model]].\n\nEvery [[reflecting cardinal]] has strictly more consistency strength than a greatly Mahlo, but inaccessible reflecting cardinals aren't in general Mahlo -- see https://mathoverflow.net/q/212597\n\n==The Mahlo operation==\n\nIf ''X'' is a class of ordinals, them we can form a new class of ordinals ''M''(''X'') consisting of the ordinals α of uncountable cofinality such that α∩''X'' is stationary in α. This operation ''M'' is called the '''Mahlo operation'''. It can be used to define Mahlo cardinals: for example, if ''X'' is the class of regular cardinals, then ''M''(''X'') is the class of weakly Mahlo cardinals. The condition that α has uncountable cofinality ensures that the closed unbounded subsets of α are closed under intersection and so form a filter; in practice the elements of ''X'' often already have uncountable cofinality in which case this condition is redundant. Some authors add the condition that α is in ''X'', which in practice usually makes little difference as it is often automatically satisfied.\n\nFor a fixed regular uncountable cardinal κ, the Mahlo operation induces an operation on the Boolean algebra of all subsets of κ modulo the non-stationary ideal.\n\nThe Mahlo operation can be iterated transfinitely as follows:\n*''M''<sup>0</sup>(''X'') = ''X''\n*''M''<sup>α+1</sup>(''X'') = ''M''(''M''<sup>α</sup>(''X''))\n*If α is a limit ordinal then ''M''<sup>α</sup>(''X'') is the intersection of ''M''<sup>β</sup>(''X'') for β<α\n\nThese iterated Mahlo operations produce the classes of α-Mahlo cardinals starting with the class of strongly inaccessible cardinals.\n\nIt is also possible to diagonalize this process by defining\n*''M''<sup>Δ</sup>(''X'') is the set of ordinals α that are in ''M''<sup>β</sup>(''X'') for β<α.\nAnd of course this diagonalization process can be iterated too. The diagonalized Mahlo operation produces the hyper-Mahlo cardinals, and so on.\n\n==Mahlo cardinals and reflection principles==\n\nAxiom F is the statement that every normal function on the ordinals has a regular fixed point. (This is not a first-order axiom as it quantifies over all normal functions, so it can be considered either as a second-order axiom or as an axiom scheme.)\nA cardinal is called Mahlo if every normal function on it has a regular fixed point, so axiom F is in some sense saying that the class of all ordinals is Mahlo. A cardinal κ is Mahlo if and only if a second-order form of axiom F holds in ''V''<sub>κ</sub>. Axiom F is in turn equivalent to the statement that for any formula φ with parameters there are arbitrarily large inaccessible ordinals α such that ''V''<sub>α</sub> reflects φ (in other words φ holds in ''V''<sub>α</sub> if and only if it holds in the whole universe)  {{harv|Drake|1974|loc=chapter 4}}.\n\n==See also==\n*[[Inaccessible cardinal]]\n*[[Stationary set]]\n*[[Inner model]]\n\n== References ==\n* {{cite book| last=Drake | first=Frank R. |title=Set Theory: An Introduction to Large Cardinals | series=Studies in Logic and the Foundations of Mathematics | volume=76|publisher=Elsevier Science Ltd | year=1974 | isbn=0-444-10535-2 | zbl=0294.02034 }}\n* {{cite book | last=Kanamori | first=Akihiro | authorlink=Akihiro Kanamori | year=2003 | publisher=[[Springer-Verlag]] | title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings | series=Springer Monographs in Mathematics | edition=2nd | isbn=3-540-00384-3 | zbl=1022.03033  }}\n*{{Citation | last1=Mahlo | first1=Paul | authorlink=Paul Mahlo | title=Über lineare transfinite Mengen | year=1911 | journal=Berichte über die Verhandlungen der Königlich Sächsischen Gesellschaft der Wissenschaften zu Leipzig. Mathematisch-Physische Klasse | volume=63 | pages=187–225 | jfm=42.0090.02}}\n*{{Citation | last1=Mahlo | first1=Paul | authorlink=Paul Mahlo | title=Zur Theorie und Anwendung der ρ<sub>0</sub>-Zahlen | year=1912 | journal=Berichte über die Verhandlungen der Königlich Sächsischen Gesellschaft der Wissenschaften zu Leipzig. Mathematisch-Physische Klasse | volume=64 | pages=108–112 | jfm=43.0113.01 }}\n*{{Citation | last1=Mahlo | first1=Paul | authorlink=Paul Mahlo | title=Zur Theorie und Anwendung der ρ<sub>0</sub>-Zahlen II| year=1913 | journal=Berichte über die Verhandlungen der Königlich Sächsischen Gesellschaft der Wissenschaften zu Leipzig. Mathematisch-Physische Klasse | volume=65 | pages=268–282 | JFM =44.0092.02 }}\n\n{{NoBracketBot}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Measurable cardinal",
      "url": "https://en.wikipedia.org/wiki/Measurable_cardinal",
      "text": "{{short description|set theory concept}}\nIn [[mathematics]], a '''measurable cardinal''' is a certain kind of [[large cardinal]] number. In order to define the concept, one introduces a two-valued [[measure (mathematics)|measure]] on a cardinal {{mvar|κ}}, or more generally on any set. For a cardinal {{mvar|κ}}, it can be described as a subdivision of all of its subsets into large and small sets such that {{mvar|κ}} itself is large, {{mvar|∅}} and all singletons {{math|<nowiki>{</nowiki>''α''<nowiki>}</nowiki>, ''α'' ∈ ''κ''}} are small, complements of small sets are large and vice versa. The intersection of fewer than {{math|''κ''}} large sets is again large.<ref>{{harvnb|Maddy|1988}}</ref>\n\nIt turns out that uncountable cardinals endowed with a two-valued measure are large cardinals whose existence cannot be proved from [[ZFC]].<ref>{{harvnb|Jech|2002}}</ref>\n\nThe concept of a measurable cardinal was introduced by [[Stanislaw Ulam]] in 1930.<ref>{{harvnb|Ulam|1930}}</ref>\n\n== Definition ==\nFormally, a measurable cardinal is an uncountable [[cardinal number]] &kappa; such that there exists a κ-additive, non-trivial, 0-1-valued [[measure (mathematics)|measure]] on the [[power set]] of&nbsp;''κ''. (Here the term ''&kappa;-additive'' means that, for any sequence ''A''<sub>''&alpha;''</sub>, &alpha;<&lambda; of cardinality ''&lambda;''&nbsp;<&nbsp;''&kappa;'', ''A''<sub>''&alpha;''</sub> being pairwise disjoint sets of ordinals less than &kappa;, the measure of the union of the ''A''<sub>''&alpha;''</sub> equals the sum of the measures of the individual ''A''<sub>''&alpha;''</sub>.)\n\nEquivalently, ''κ'' is measurable means that it is the [[critical point (set theory)|critical point]] of a non-trivial [[elementary embedding]] of the [[universe (set theory)|universe]] ''V'' into a [[transitive class]] ''M''. This equivalence is due to [[Jerome Keisler]] and [[Dana Scott]], and uses the [[ultraproduct|ultrapower]] construction from [[model theory]]. Since ''V'' is a [[proper class]], a technical problem that is not usually present when considering ultrapowers needs to be addressed, by what is now called [[Scott's trick]].\n\nEquivalently, ''κ'' is a measurable cardinal if and only if it is an uncountable cardinal with a κ-complete, non-principal [[ultrafilter]].  Again, this means that the intersection of any ''strictly less than'' ''&kappa;''-many sets in the ultrafilter, is also in the ultrafilter.\n\n== Properties ==\nAlthough it follows from [[ZFC]] that every measurable cardinal is [[inaccessible cardinal|inaccessible]] (and is [[Ineffable cardinal|ineffable]], [[Ramsey cardinal|Ramsey]], etc.), it is consistent with [[Zermelo–Fraenkel set theory|ZF]] that a measurable cardinal can be a [[successor cardinal]].  It follows from ZF + [[axiom of determinacy]] that ω<sub>1</sub> is measurable, and that every subset of ω<sub>1</sub> contains or is disjoint from a closed and unbounded subset.\n\nUlam showed that the smallest cardinal κ that admits a non-trivial countably-additive two-valued measure must in fact admit a κ-additive measure.  (If there were some collection of fewer than κ measure-0 subsets whose union was κ, then the induced measure on this collection would be a counterexample to the minimality of κ.)  From there, one can prove (with the Axiom of Choice) that the least such cardinal must be inaccessible.\n\nIt is trivial to note that if κ admits a non-trivial κ-additive measure, then κ must be regular.  (By non-triviality and κ-additivity, any subset of cardinality less than κ must have measure 0, and then by κ-additivity again, this means that the entire set must not be a union of fewer than κ sets of cardinality less than κ.)  Finally, if λ < κ, then it can't be the case that κ ≤ 2<sup>λ</sup>.  If this were the case, then we could identify ''κ'' with some collection of 0-1 sequences of length ''λ''.  For each position in the sequence, either the subset of sequences with 1 in that position or the subset with 0 in that position would have to have measure&nbsp;1.  The intersection of these ''λ''-many measure 1 subsets would thus also have to have measure&nbsp;1, but it would contain exactly one sequence, which would contradict the non-triviality of the measure.  Thus, assuming the Axiom of Choice, we can infer that ''κ'' is a strong limit cardinal, which completes the proof of its inaccessibility.\n\nIf &kappa; is measurable and ''p''&isin;''V''<sub>''&kappa;''</sub> and ''M'' (the ultrapower of ''V'') satisfies &psi;(&kappa;,''p''), then the set of ''&alpha;''&nbsp;<&nbsp;''&kappa;'' such that ''V'' satisfies ''&psi;''(''&alpha;'',''p'') is stationary in &kappa; (actually a set of measure 1). In particular if ''&psi;'' is a &Pi;<sub>1</sub> formula and ''V'' satisfies &psi;(&kappa;,''p''), then ''M'' satisfies it and thus ''V'' satisfies ''&psi;''(''&alpha;'',''p'') for a stationary set of ''&alpha;''&nbsp;<&nbsp;''&kappa;''. This property can be used to show that ''&kappa;'' is a limit of most types of large cardinals which are weaker than measurable. Notice that the ultrafilter or measure which witnesses that ''&kappa;'' is measurable cannot be in ''M'' since the smallest such measurable cardinal would have to have another such below it which is impossible.\n\nIf one starts with an elementary embedding ''j''<sub>1</sub> of ''V'' into ''M''<sub>1</sub> with [[critical point (set theory)|critical point]] &kappa;, then one can define an ultrafilter ''U'' on &kappa; as { ''S''&sub;&kappa; : &kappa;&isin;''j''<sub>1</sub>(''S'') }. Then taking an ultrapower of ''V'' over ''U'' we can get another elementary embedding ''j''<sub>2</sub> of ''V'' into ''M''<sub>2</sub>. However, it is important to remember that ''j''<sub>2</sub> ≠ ''j''<sub>1</sub>. Thus other types of large cardinals such as [[strong cardinal]]s may also be measurable, but not using the same embedding. It can be shown that a strong cardinal &kappa; is measurable and also has &kappa;-many measurable cardinals below it.\n\nEvery measurable cardinal &kappa; is a 0-[[huge cardinal]] because <sup>''&kappa;''</sup>''M''&sub;''M'', that is, every function from &kappa; to ''M'' is in ''M''. Consequently, ''V''<sub>''&kappa;''+1</sub>&sub;''M''.\n\n== Real-valued measurable ==\nA cardinal κ is called '''real-valued measurable''' if there is a κ-additive probability measure on the power set of κ which vanishes on singletons.  Real-valued measurable cardinals were introduced by {{harvs|txt|authorlink=Stefan Banach|first=Stefan|last=Banach|year=1930}}. {{harvtxt|Banach|Kuratowski|1929}} showed that  the [[continuum hypothesis]] implies that <math>{\\mathfrak c} </math> is not real-valued measurable. {{harvs|txt|authorlink=Stanislaw Ulam|first=Stanislaw|last= Ulam|year=1930}} showed (see below for parts of Ulam's proof) that real valued measurable cardinals are weakly inaccessible (they are in fact [[weakly Mahlo]]).  All measurable cardinals are real-valued measurable, and a real-valued measurable cardinal κ is measurable if and only if κ is greater than <math>{\\mathfrak c} </math>. Thus a cardinal is measurable if and only if it is real-valued measurable and strongly inaccessible. A real valued measurable cardinal less than or equal to <math>{\\mathfrak c} </math> exists if and only if there is a countably additive extension of the [[Lebesgue measure]] to all sets of real numbers if and only if there is an [[atom (measure theory)|atomless]] probability measure on the power set of some non-empty set.\n\n{{harvtxt|Solovay|1971}} showed that existence of measurable cardinals in ZFC, real valued measurable cardinals in ZFC, and measurable cardinals in ZF, are [[equiconsistency|equiconsistent]].\n\n=== Weak inaccessibility of real-valued measurable cardinals ===\nSay that a cardinal number <math>\\alpha</math> is an ''Ulam number'' if<ref>{{harvnb|Federer|1996|loc=Section 2.1.6}}</ref><ref group=nb>The notion in the article [[Ulam number]] is different.</ref><p>\nwhenever\n# <math>\\mu</math> is an [[outer measure]] on a set <math>X,</math>\n# <math>\\mu(X) < \\infty,</math>\n# <math>\\mu(\\{x\\}) = 0, x \\in  X,</math>\n# all <math>A \\subset X</math> are [[Carathéodory-measurable set|{{math|''μ''}}-measurable]],\nthen\n::<math>\\operatorname{card} X \\le \\alpha\\Rightarrow\\mu(X) = 0.</math>\n\nEquivalently, a cardinal number <math>\\alpha</math> is an Ulam number if<p>\nwhenever\n# <math>\\nu</math> is an outer measure on a set <math>Y,</math> and <math>F</math> a disjoint family of subsets of <math>Y</math>,\n# <math>\\nu\\left(\\bigcup F\\right) < \\infty,</math>\n# <math>\\nu(A) = 0</math> for <math>A \\in F,</math>\n# <math>\\bigcup G</math> is {{math|''ν''}}-measurable for every <math>G \\subset F</math>\nthen\n::<math>\\operatorname{card} F \\le \\alpha\\Rightarrow\\nu\\left(\\bigcup F\\right) = 0.</math>\n\nThe smallest infinite cardinal <math>\\aleph_0</math> is an Ulam number. The class of Ulam numbers is closed under the [[successor cardinal|cardinal successor]] operation.<ref>{{harvnb|Federer|1996|loc=Second part of theorem in section 2.1.6.}}</ref> If an infinite cardinal <math>\\beta</math> has an immediate predecessor <math>\\alpha</math> that is an Ulam number, assume <math>\\mu</math> satisfies properties {{math|(1)&ndash;(4)}} with <math>X = \\beta</math>. In the [[Ordinal number#Von Neumann definition of ordinals|von Neumann model]] of ordinals and cardinals, choose [[injective function]]s\n\n:<math>f_x:x \\rightarrow \\alpha, \\quad \\forall x \\in \\beta,</math>\n\nand define the sets\n\n:<math>U(b, a) = \\{x \\in \\beta: f_x(b) = a\\}, \\quad a \\in \\alpha, b \\in \\beta.</math>\n\nSince the <math>f_x</math> are one-to-one, the sets\n\n:<math>\\left\\{U(b, a), b\\in \\beta\\right\\} \\text{(} a \\text{ fixed)},</math>\n:<math>\\left\\{U(b, a), a\\in \\alpha\\right\\} \\text{(} b \\text{ fixed)}</math>\n\nare disjoint. By property (2) of <math>\\mu</math>, the set\n\n:<math>\\left\\{b \\in \\beta: \\mu(U(b, a)) > 0\\right\\}</math>\n\nis [[countable set|countable]], and hence\n\n:<math>\\operatorname{card}\\left\\{(b, a) \\in \\beta \\times \\alpha |\\mu(U(b, a)) > 0\\right\\} \\le \\aleph_0 \\cdot \\alpha = \\alpha.</math>\n\nThus there is a <math>b_0</math> such that\n\n:<math>\\mu(U(b_0, a)) = 0 \\quad \\forall a \\in \\alpha</math>\n\nimplying, since <math>\\alpha</math> is an Ulam number and using the second definition (with <math>\\nu = \\mu</math> and conditions {{math|(1)&ndash;(4)}} fulfilled),\n\n:<math>\\mu\\left(\\bigcup_{a \\in \\alpha}U(b_0, a)\\right) = 0.</math>\n\nIf <math>b_0 < x < \\beta,</math> then <math>f_x(b_0) = a_x \\Rightarrow x\\in U(b_0, a_x).</math> Thus\n\n:<math>\\beta = b_0 \\cup \\{b_0\\} \\cup \\bigcup_{a \\in \\alpha}U(b_0, a),</math>\n\nBy property {{math|(2)}}, <math>\\mu\\{b_0\\} = 0,</math> and since <math>\\operatorname{card} b_0 \\le \\alpha</math>, by {{math|(4), (2)}} and {{math|(3)}}, <math>\\mu(b_0) = 0.</math> It follows that  <math>\\mu(\\beta) = 0.</math> The conclusion is that <math>\\beta</math> is an Ulam number. \n\nThere is a similar proof<ref>{{harvnb|Federer|1996|loc=First part of theorem in section 2.1.6.}}</ref> that the supremum of a set <math>S</math> of Ulam numbers with <math>\\operatorname{card} S</math> an Ulam number is again a Ulam number. Together with the previous result, this imples that a cardinal that is not an Ulam number is [[inaccessible cardinal|weakly inaccessible]].\n\n== See also ==\n* [[Normal measure]]\n* [[Mitchell order]]\n\n== Remarks ==\n{{reflist|group=nb}}\n\n== Notes ==\n{{reflist|2}}\n\n== References ==\n*{{Citation | last1=Banach | first1=Stefan | author1-link=Stefan Banach | title=Über additive Maßfunktionen in abstrakten Mengen | url=https://eudml.org/doc/212335 | year=1930 | journal=[[Fundamenta Mathematicae]] | issn=0016-2736 | volume=15 | pages=97–101}}.\n*{{Citation | last1=Banach | first1=Stefan | author1-link=Stefan Banach | last2=Kuratowski | first2=Kazimierz | author2-link=Kazimierz Kuratowski | title=Sur une généralisation du probleme de la mesure | url=https://eudml.org/doc/212126 | year=1929 | journal=[[Fundamenta Mathematicae]] | issn=0016-2736 | volume=14 | pages=127–131}}.\n*{{Citation|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn= 978-0-7204-2279-5}}.\n*{{Citation|ref=harv|title=Geometric Measure Theory|series=Classics in Mathematics|edition=1st ed reprint|location=Berlin, Heidelberg, New York|orig-year=1969|year=1996|isbn=978-3540606567|publisher=[[Springer Verlag]]|last=Federer|first=H.|authorlink=Herbert Federer}}.\n*{{Citation|author=Jech, Thomas|title=Set theory, third millennium edition (revised and expanded)|publisher=Springer|year=2002|isbn=3-540-44085-2|authorlink=Thomas Jech}}.\n*{{Citation|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}.\n*{{Citation|last=Maddy|first=Penelope|authorlink=Penelope Maddy|journal=The Journal of Symbolic Logic|title=Believing the Axioms. II|year=1988|volume=53|issue=3|pages=736-764|doi=10.2307/2274569|jstor=2274569}}. A copy of parts I and II of this article with corrections is available at the [http://faculty.sites.uci.edu/pjmaddy/bibliography/ author's web page].\n*{{Citation | last1=Solovay | first1=Robert M. | author1-link=Robert M. Solovay | title=Axiomatic set theory (Proc. Sympos. Pure Math., Vol. XIII, Part I, Univ. California, Los Angeles, Calif., 1967) | publisher=Amer. Math. Soc. | location=Providence, R.I. |mr=0290961 | year=1971 | chapter=Real-valued measurable cardinals | pages=397–428}}.\n*{{Citation | last1=Ulam | first1=Stanislaw | authorlink = Stanislaw Ulam| title=Zur Masstheorie in der allgemeinen Mengenlehre | url=https://eudml.org/doc/212487 | year=1930 | journal=[[Fundamenta Mathematicae]] | issn=0016-2736 | volume=16 | pages=140–150}}.\n\n[[Category:Large cardinals]]\n[[Category:Determinacy]]\n[[Category:Measures (set theory)| ]]"
    },
    {
      "title": "Moschovakis coding lemma",
      "url": "https://en.wikipedia.org/wiki/Moschovakis_coding_lemma",
      "text": "The '''Moschovakis coding lemma''' is a [[Lemma (mathematics)|lemma]] from descriptive [[set theory]] involving sets of [[real number]]s in the [[axiom of determinacy]] (the principle that every two-player integer game is determined). The lemma was developed and named after the mathematician, [[Yiannis N. Moschovakis]].\n\nThe lemma may be expressed generally as follows:\n\nLet Γ be a non-selfdual point-class closed under ∃ ω ω and ∧ , and ≺ a Γ well-founded relation on ω ω of rank θ ∈ ON. Let R ⊆ dom( ≺ ) × ω ω be such that ∀ x ∈ dom( ≺ ) ∃ y R ( x,y ) . Then there is a Γ set A ⊆ dom( ≺ ) × ω ω which is a choice set for R , that is: 1. ∀ α < θ ∃ x ∈ dom( ≺ ) ∃ y [ | x | ≺ = α ∧ A ( x,y )] . 2. ∀ x ∀ y A ( x,y ) → [ x ∈ dom( ≺ ) ∧ R ( x,y )] . Proof. We may assume θ is minimal so that the theorem fails, and fix ≺ , R , and a good universal set U ⊆ ( ω ω ) 3 for the Γ subsets of ( ω ω ) 2 . Easily, θ is a limit ordinal. For δ < θ , say u ∈ ω ω codes a δ -choice set provided (1) holds for α ≤ δ using A = U u , and (2) holds for A = U u where we replace x ∈ dom( ≺ ) with x ∈ dom( ≺ ) ∧| x | ≺ ≤ δ . By minimality of θ , for all δ < θ there are δ -choice sets. Play the game where I, II play out u,v ∈ ω ω , and II wins provided that if u codes a δ 1 -choice set for some δ 1 < θ , then v codes a δ 2 -choice set for some δ 2 > δ 1 . If I has a winning strategy, we get a Σ 1 1 set B of reals coding δ -choice sets for arbitrarily large δ < θ . Define then A ( x,y ) ↔∃ w ∈ B U ( w,x,y ), which easily works. Suppose now τ is a winning strategy for II. From the s - m - n theorem, let s : ( ω ω ) 2 → ω ω be continuous such that for all {{lang|el|ϵ}},x,t,w, U ( s ({{lang|el|ϵ}},x ) ,t,w ) ↔ ∃ y ∃ z [ y ≺ x ∧ U ({{lang|el|ϵ}},y,z ) ∧ U ( z,t,w )]. By the recursion theorem, let {{lang|el|ϵ}} 0 be such that U ({{lang|el|ϵ}}0 ,x,z ) ↔ z = τ ( s ({{lang|el|ϵ}}0 ,x )). A straightforward induction on | x | ≺ for x ∈ dom( ≺ ) shows that ∀ x ∈ dom( ≺ ) ∃ ! z U ( {{lang|el|ϵ}}0 ,x,z ), and ∀ x ∈ dom( ≺ ) ∀ z [ U ( {{lang|el|ϵ}}0 ,x,z ) → z codes a ≥| x | ≺ -choice set]. Let A ( x,y ) ↔∃ z ∈ dom( ≺ ) ∃ w [ U ( {{lang|el|ϵ}}0 ,z,w ) ∧ U ( w,x,y )].<ref>{{cite book\n | last =Babinkostova\n | first =Liljana\n | title =Set Theory and Its Applications\n | publisher =American Mathematical Society\n | date =2011\n | language =English\n | isbn =978-0821848128\n}}</ref>, \n<ref>{{cite book\n | last1 =Foreman\n | first1 =Matthew\n | author1-link =Matthew Foreman\n | last2 =Kanamori\n | first2 =Akihiro\n | author2-link =Akihiro Kanamori\n | title =Handbook of Set Theory\n | publisher =Springer;\n | date =October 27, 2005\n | pages =2230\n | url =http://www.math.unt.edu/~sjackson/papers/strad_27.pdf\n | isbn =978-1402048432\n}}</ref>, <ref>{{cite book\n | last =Moschovakis\n | first =Yiannis\n | title =Ordinal games and playful models\n | journal =Lecture Notes in Mathematics\n | volume =839\n | pages =169–201\n | publisher =Springer\n | location =Berlin\n | date =October 4, 2006\n | isbn =978-3-540-38422-9\n | doi = 10.1007/BFb0090241\n }}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.math.ucla.edu/~ynm/ | Yiannis N. Moschovakis homepage]\n\n<!--- Categories --->\n\n\n{{Set theory}}\n\n[[Category:Axioms of set theory]]\n[[Category:Determinacy]]\n[[Category:Large cardinals]]\n[[Category:Lemmas]]\n{{math-stub}}"
    },
    {
      "title": "Normal measure",
      "url": "https://en.wikipedia.org/wiki/Normal_measure",
      "text": "In [[set theory]], a '''normal measure''' is a measure on a [[measurable cardinal]] κ such that the equivalence class of the identity function on κ maps to κ itself in the [[ultraproduct|ultrapower]] construction. Equivalently, if f:κ→κ is such that f(α)<α for most α<κ, then there is a β<κ such that f(α)=β for most α<κ. (Here, \"most\" means that the set of elements of κ where the property holds is a member of the ultrafilter, i.e. has measure 1.) Also equivalent, the ultrafilter (set of sets of measure 1) is closed under [[diagonal intersection]].\n\nFor a normal measure, any closed unbounded (club) subset of κ contains most ordinals less than κ. And any subset containing most ordinals less than κ is stationary in κ.\n\nIf an uncountable cardinal κ has a measure on it, then it has a normal measure on it.\n\n== See also ==\n* [[Measurable cardinal]]\n* [[Club set]]\n\n== References ==\n* {{cite book|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer |title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=1st |isbn=3-540-57071-3}} pp 52–53\n\n[[Category:Large cardinals]]\n[[Category:Measures (set theory)]]"
    },
    {
      "title": "Quasicompact cardinal",
      "url": "https://en.wikipedia.org/wiki/Quasicompact_cardinal",
      "text": "#REDIRECT [[Subcompact cardinal]]\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Ramsey cardinal",
      "url": "https://en.wikipedia.org/wiki/Ramsey_cardinal",
      "text": "In [[mathematics]], a '''Ramsey cardinal'''  is a certain kind of [[large cardinal]] number introduced by {{harvtxt|Erdős|Hajnal|1962}} and named after [[Frank P. Ramsey]].\n\nLet [κ]<sup>&lt;ω</sup> denote the set of all finite subsets of κ. Then a [[cardinal number]] κ  is called Ramsey if, for every function \n\n:''f'': [κ]<sup>&lt;ω</sup> → {0, 1}\n\nthere is a set ''A'' of cardinality κ that is [[Homogeneous (large cardinal property)|homogeneous]] for ''f''. That is, for every ''n'',  ''f'' is constant on the subsets of cardinality ''n'' from ''A''.  A cardinal κ is called '''ineffably Ramsey''' if A can be chosen to be [[stationary set|stationary]] subset of κ.  A cardinal κ is called '''virtually Ramsey''' if for every function \n\n:''f'': [κ]<sup><ω</sup> → {0, 1}\n\nthere is C, a closed and unbounded subset of κ, so that for every λ in C of uncountable cofinality, there is an unbounded subset of λ, which is homogenous for ''f''; slightly weaker is the notion of '''almost Ramsey''' where homogenous sets for ''f'' are required  of order type λ, for every λ < κ.  \n\nThe existence of any of these kinds of Ramsey cardinal is sufficient to prove the existence of [[Zero sharp|0<sup>#</sup>]], or indeed that every set with [[rank (set theory)|rank]] less than κ has a [[sharp (set theory)|sharp]].\n\nEvery [[measurable cardinal]] is a Ramsey cardinal, and every Ramsey cardinal  is a [[Rowbottom cardinal]].\n\nA property intermediate in strength between Ramseyness and [[Measurable cardinal|measurability]] is existence of a κ-complete normal non-principal [[Ideal (set theory)|ideal]] ''I'' on κ such that for every {{nowrap|''A'' ∉ ''I''}} and for every function \n\n:''f'': [κ]<sup>&lt;ω</sup> → {0, 1}\n\nthere is a set ''B'' ⊂ ''A'' not in ''I'' that is homogeneous for ''f''.   This is strictly stronger than κ being ineffably Ramsey.\n\nThe existence of a Ramsey cardinal implies the existence of the zero sharp cardinal and this in turn implies the falsity of the [[Axiom of constructibility|Axiom of Constructibility]] of [[Kurt Gödel]].\n\n== References ==\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n*{{Citation | last1=Erdős | first1=Paul | author1-link=Paul Erdős| last2=Hajnal | first2=András | |author2-link=András Hajnal | title=Some remarks concerning our paper \"On the structure of set-mappings''. Non-existence of a two-valued σ-measure for the first uncountable inaccessible cardinal | doi=10.1007/BF02033641 |mr=0141603 | year=1962 | journal=Acta Mathematica Academiae Scientiarum Hungaricae | issn=0001-5954 | volume=13 | pages=223–226}}\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|author-link=Akihiro Kanamori|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Rank-into-rank",
      "url": "https://en.wikipedia.org/wiki/Rank-into-rank",
      "text": "{{more footnotes|date=November 2013}} \nIn [[set theory]], a branch of [[mathematics]], a '''rank-into-rank''' embedding is a [[large cardinal property]] defined by one of the following four [[axiom]]s given in order of increasing consistency strength. (A set of rank λ is one of the sets in V<sub>λ</sub> of the [[von Neumann hierarchy]].)\n\n*'''Axiom I3:'''  There is a nontrivial [[elementary embedding]] of V<sub>λ</sub> into itself.\n*'''Axiom I2:''' There is a nontrivial elementary embedding of V into a transitive class M that includes V<sub>λ</sub> where λ is the first fixed point above the [[critical point (set theory)|critical point]].\n*'''Axiom I1:'''  There is a nontrivial elementary embedding of V<sub>λ+1</sub> into itself.\n*'''Axiom I0:'''  There is a nontrivial elementary embedding of L(V<sub>λ+1</sub>) into itself with critical point below λ.\n\nThese are essentially the strongest known large cardinal axioms not known to be inconsistent in [[ZFC]]; the axiom for [[Reinhardt cardinal]]s is stronger, but is not consistent with the [[axiom of choice]].\n\nIf j is the elementary embedding mentioned in one of these axioms and κ is its [[critical point (set theory)|critical point]], then λ is the limit of <math>j^n(\\kappa)</math> as n goes to ω.  More generally, if the [[axiom of choice]] holds, it is provable that if there is a nontrivial elementary embedding of V<sub>α</sub> into itself then α is either a [[limit ordinal]] of [[cofinality]] ω or the successor of such an ordinal.\n\nThe axioms I0, I1, I2, and I3 were at first suspected to be inconsistent (in ZFC) as it was thought possible that [[Kunen's inconsistency theorem]] that [[Reinhardt cardinal]]s are inconsistent with the axiom of choice could be extended to them, but this has not yet happened and they are now usually believed to be consistent.\n\nEvery I0 cardinal κ (speaking here of the critical point of ''j'') is an I1 cardinal.\n\nEvery I1 cardinal κ (sometimes called ω-huge cardinals) is an I2 cardinal and has a stationary set of I2 cardinals below it.\n\nEvery I2 cardinal κ is an I3 cardinal and has a stationary set of I3 cardinals below it.\n\nEvery I3 cardinal κ has another I3 cardinal ''above'' it and is an ''n''-[[huge cardinal]] for every ''n''<ω.\n\nAxiom I1 implies that V<sub>λ+1</sub> (equivalently, H(λ<sup>+</sup>)) does not satisfy V=HOD.  There is no set S⊂λ definable in V<sub>λ+1</sub> (even from parameters V<sub>λ</sub> and ordinals &lt;λ<sup>+</sup>) with S cofinal in λ and |S|<λ, that is, no such S witnesses that λ is singular. And similarly for Axiom I0 and ordinal definability in L(V<sub>λ+1</sub>) (even from parameters in V<sub>λ</sub>). However globally, and even in V<sub>λ</sub>,<ref>Consistency of V = HOD With the Wholeness Axiom, Paul Corazza, Archive for Mathematical Logic, No. 39, 2000.</ref> V=HOD is relatively consistent with Axiom I1.\n\nNotice that I0 is sometimes strengthened further by adding an \"Icarus set\", so that it would be\n*'''Axiom Icarus set:'''  There is a nontrivial elementary embedding of L(V<sub>λ+1</sub>, Icarus) into itself with the critical point below λ.\nThe Icarus set should be in V<sub>λ+2</sub> &minus; L(V<sub>λ+1</sub>) but chosen to avoid creating an inconsistency. So for example, it cannot encode a well-ordering of V<sub>λ+1</sub>. See section 10 of Dimonte for more details.\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite arxiv\n |last=Dimonte |first= Vincenzo \n |date=2017\n |title=I0 and rank-into-rank axioms\n |eprint=1707.02613\n |mode=cs2\n|class= math.LO \n }}.\n*{{citation|mr=0376347\n|last=Gaifman|first= Haim|authorlink=Haim Gaifman\n|chapter=Elementary embeddings of models of set-theory and certain subtheories|title=Axiomatic set theory |series=Proc. Sympos. Pure Math.|volume= XIII, Part II|pages= 33–101|publisher= Amer. Math. Soc.|publication-place=Providence R.I.|year= 1974}}\n* {{Citation|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}.\n*{{citation|mr=1489305 \n|last=Laver|first= Richard|authorlink=Richard Laver\n|title=Implications between strong large cardinal axioms\n|journal=Ann. Pure Appl. Logic|volume= 90 |year=1997|issue= 1–3|pages= 79–90|doi=10.1016/S0168-0072(97)00031-6}}. \n* {{citation|last=Solovay|first=Robert M.|first2=William N. |last2=Reinhardt|first3= Akihiro |last3=Kanamori|year=1978|title=Strong axioms of infinity and elementary embeddings|journal=Annals of Mathematical Logic|volume=13|issue=1|pages=73–116|authorlink=Robert M. Solovay|doi=10.1016/0003-4843(78)90031-1}}.\n\n[[Category:Large cardinals]]\n[[Category:Determinacy]]"
    },
    {
      "title": "Reflecting cardinal",
      "url": "https://en.wikipedia.org/wiki/Reflecting_cardinal",
      "text": "In [[set theory]], a mathematical discipline, a '''reflecting cardinal''' is  a cardinal number κ for which there is a normal ideal ''I'' on κ such that for every ''X''∈''I''<sup>+</sup>, the set of α∈κ for which ''X'' reflects at α is in ''I''<sup>+</sup>. (A [[stationary set|stationary subset]] ''S''  of κ is said to '''reflect''' at α<κ if ''S''∩α is stationary in α.)\nReflecting cardinals were introduced by {{harv|Mekler|Shelah|1989}}.\n\nEvery [[weakly compact cardinal]] is a reflecting cardinal, and is also a limit of reflecting cardinals.\nThe consistency strength of an inaccessible reflecting cardinal is strictly greater than a greatly Mahlo cardinal, where a cardinal κ is called '''greatly Mahlo''' if it is [[Mahlo cardinal|&kappa;<sup>+</sup>-Mahlo]] {{harv|Mekler|Shelah|1989}}. An inaccessible reflecting cardinal is not in general Mahlo however, see https://mathoverflow.net/q/212597.\n\n==References==\n*{{Citation | last1=Jech | first1=Thomas | author1-link=Thomas Jech | title=Set Theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=third millennium | series=Springer Monographs in Mathematics | isbn=978-3-540-44085-7 | year=2003|page=697}} \n*{{Citation | last1=Mekler | first1=Alan H. | last2=Shelah | first2=Saharon | author2-link=Saharon Shelah | title=The consistency strength of ``every stationary set reflects'' | mr= 1029909| year=1989 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=67 | issue=3 | pages=353–366 | doi=10.1007/BF02764953}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Reinhardt cardinal",
      "url": "https://en.wikipedia.org/wiki/Reinhardt_cardinal",
      "text": "In mathematical [[set theory]], a '''Reinhardt cardinal''' is a  [[large cardinal]] κ in a model of ZF, Zermelo–Fraenkel set theory without the axiom of choice (Reinhardt cardinals are not compatible with the axiom of choice in ZF). They were suggested by {{harvs|txt=yes|first=William Nelson|last=Reinhardt|year=1967|year2=1974}}.\n\n==Definition==\n\nA Reinhardt cardinal is the [[critical point (set theory)|critical point]] of a non-trivial [[elementary embedding]] ''j'' of ''[[Von Neumann universe|V]]'' into itself.\n\nA minor technical problem is that this property cannot be formulated in the usual set theory [[ZFC]]: the embedding ''j'' is a class, which in ZFC means something of the form <math>\\{x|\\phi(x,a)\\}</math> for some set ''a'' and formula φ, but in the language of set theory it is not possible to quantify over all classes or define the truth of formulas. There are several ways to get round this.  One way is to add a new function symbol ''j'' to the language of ZFC, together with axioms stating that ''j'' is an elementary embedding of ''V'' (and of course adding separation and replacement axioms for formulas involving ''j''). Another way is to use a [[Class (set theory)|class theory]] such as  [[Von Neumann–Bernays–Gödel set theory|NBG]] or [[Morse–Kelley set theory|KM]]. A third way would be to treat Kunen's  [[Kunen's inconsistency theorem|theorem]]  as a countable  infinite collection of theorems, one for each formula φ, but that would trivialize the theorem. (It is possible to have nontrivial elementary embeddings of transitive models of ZFC into themselves assuming a mild large cardinal hypothesis, but these elementary embeddings are not classes of the model.)\n\n==Kunen's theorem==\n\n{{harvs|txt=yes|authorlink=Kenneth Kunen|last=Kunen|year=1971}} proved [[Kunen's inconsistency theorem]] showing  that the existence of such an embedding contradicts [[Von Neumann–Bernays–Gödel set theory|NBG]] with the [[axiom of choice]] (and ZFC extended by ''j''), but it is consistent with weaker [[Class (set theory)|class theories]]. His proof uses the axiom of choice, and it  is still an open question as to whether such an embedding is consistent with NBG without the axiom of choice (or with ZF plus the extra symbol ''j'' and its attendant axioms).\n\n==Stronger cardinal axioms==\n\nThere are some variations of Reinhardt cardinals. \nIn ZF, there is a hierarchy of hypotheses asserting existence of elementary embeddings V→V<br/>\nJ3:  There is a nontrivial elementary embedding j: V→V<br/>\nJ2:  There is a nontrivial elementary embedding j: V→V, and DC<sub>λ</sub> holds, where λ is the least fixed-point above the critical point.<br/>\nJ1:  There is a cardinal κ such that for every α, there is an elementary embedding j : V→V with j(κ)>α and cp(j) = κ.\n\nJ2 implies J3, and J1 implies J3 and also implies consistency of J2.  By adding a generic well-ordering of V to a model of J1, one gets ZFC plus a nontrivial elementary embedding of [[Hereditarily ordinal definable|HOD]] into itself.\n\n[[Berkeley cardinal]]s are a stronger large cardinals suggested by Woodin.\n\n== References ==\n*{{citation|title= Inner Models and Large Cardinals\n|first=        Ronald |last=Jensen |author-link=Ronald Jensen \n|journal=        The Bulletin of Symbolic Logic|volume= 1|issue= 4|year= 1995|pages= 393–407. \n|doi= 10.2307/421129|publisher= The Bulletin of Symbolic Logic, Vol. 1, No. 4|jstor=421129|citeseerx=10.1.1.28.1790}}\n* {{citation|last=Kanamori|first=Akihiro|author-link=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn =3-540-00384-3}}\n*{{citation|mr=0311478\n|last=Kunen|first= Kenneth\n|title=Elementary embeddings and infinitary combinatorics\n|journal=Journal of Symbolic Logic |volume=36 |year=1971|pages= 407–413\n|doi=10.2307/2269948|issue=3|publisher=The Journal of Symbolic Logic, Vol. 36, No. 3|jstor=2269948}} \n*{{citation|last=Reinhardt|first= W. N.\n|title=Topics in the metamathematics of set theory|series= Doctoral dissertation|publisher=University of California, Berkeley|year=1967}}\n*{{citation|mr=0401475\n|last=Reinhardt|first= W. N.\n|chapter=Remarks on reflection principles, large cardinals, and elementary embeddings. |title=Axiomatic set theory |series=Proc. Sympos. Pure Math.|volume= XIII, Part II|pages= 189–205|publisher= Amer. Math. Soc.|publication-place= Providence, R. I.|year= 1974}}\n\n==External links==\n*{{citation|url=http://logic.harvard.edu/blog/wp-content/uploads/2014/11/Deep_Inconsistency.pdf|first=Peter |last=Koellner|author-link=Peter Koellner|title=The Search for Deep Inconsistency|year=2014}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Remarkable cardinal",
      "url": "https://en.wikipedia.org/wiki/Remarkable_cardinal",
      "text": "In [[mathematics]], a '''remarkable cardinal''' is a certain kind of [[large cardinal]] number.\n\nA [[cardinal number|cardinal]] κ is called remarkable if for all [[regular cardinal]]s θ > κ, there exist π, ''M'', λ, σ, ''N'' and ρ such that\n\n# π : ''M'' → '''H'''<sub>θ</sub> is an [[elementary embedding]]\n# ''M'' is [[countable]] and [[transitive set|transitive]]\n# π(λ) = κ\n# σ : ''M'' → ''N'' is an elementary embedding with [[critical point (set theory)|critical point]] λ\n# ''N'' is countable and transitive\n# ρ = ''M'' ∩ '''[[Ordinal number|Ord]]''' is a [[regular cardinal]] in ''N''\n# σ(λ) &gt; ρ\n# ''M'' = ''H''<sub>ρ</sub><sup>''N''</sup>, i.e., ''M'' ∈ ''N'' and ''N'' ⊨ \"''M is the set of all sets that are hereditarily smaller than &rho;''\"\n\nEquivalently, <math>\\kappa</math> is remarkable if and only if for every <math>\\lambda>\\kappa</math> there is <math>\\bar\\lambda<\\kappa</math> such that in some [[Forcing (set theory)|forcing]] extension <math>V[G]</math>, there is an elementary embedding <math>j:V_{\\bar\\lambda}^V\\rightarrow V_\\lambda^V</math> satisfying <math>j(\\operatorname{crit}(j))=\\kappa</math>. Note that, although the definition is similar to one of the definitions of [[supercompact cardinal]]s, the elementary embedding here only has to exist in <math>V[G]</math>, not in <math>V</math>.\n\n==See also==\n*[[Hereditarily countable set]]\n\n==References==\n*{{Citation | last1=Schindler | first1=Ralf | title=Proper forcing and remarkable cardinals | url=http://www.math.ucla.edu/~asl/bsl/0602/0602-003.ps | doi=10.2307/421205 | mr=1765054  | year=2000 | journal=The Bulletin of Symbolic Logic | issn=1079-8986 | volume=6 | issue=2 | pages=176–184| citeseerx=10.1.1.297.9314 }}\n*{{Citation | last1=Gitman | first1=Victoria | title=Virtual large cardinals | url=http://nylogic.org/wp-content/uploads/virtualLargeCardinals.pdf  | year=2016 }}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Rowbottom cardinal",
      "url": "https://en.wikipedia.org/wiki/Rowbottom_cardinal",
      "text": "In [[set theory]], a '''Rowbottom cardinal''', introduced by  {{harvs|txt|authorlink=Frederick Rowbottom|last=Rowbottom|year=1971}}, is a certain kind of [[large cardinal]] number.\n\nAn [[uncountable set|uncountable]] [[cardinal number]] &kappa; is said to be '''''Rowbottom''''' if for every function ''f'': [&kappa;]<sup><&omega;</sup> &rarr; &lambda; (where &lambda; < &kappa;) there is a set ''H'' of order type &kappa; that is quasi-[[Homogeneous (large cardinal property)|homogeneous]] for ''f'', i.e., for every ''n'', the ''f''-image of the set of ''n''-element subsets of ''H'' has [[countable|countably]] many elements.\n\nEvery [[Ramsey cardinal]] is Rowbottom, and every Rowbottom cardinal is [[Jónsson cardinal|Jónsson]]. By a theorem of Kleinberg, the theories ZFC + “there is a Rowbottom cardinal” and ZFC + “there is a Jónsson cardinal” are equiconsistent.\n\nIn general, Rowbottom cardinals need not be [[large cardinal]]s in the usual sense: Rowbottom cardinals could be [[singular cardinal|singular]]. It is an open question whether ZFC + “<math>\\aleph_{\\omega}</math> is Rowbottom” is consistent. If it is, it has much higher consistency strength than the existence of a Rowbottom cardinal. The [[axiom of determinacy]] does imply that <math>\\aleph_{\\omega}</math> is Rowbottom (but contradicts the [[axiom of choice]]).\n\n== References ==\n\n* {{cite book|last=Kanamori|first=Akihiro|author-link=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite: Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n*{{Citation | last=Rowbottom | first=Frederick | author-link=Frederick Rowbottom | title=Some strong axioms of infinity incompatible with the axiom of constructibility | origyear=1964 | doi=10.1016/0003-4843(71)90009-X    |mr=0323572 | year=1971 | journal=Annals of Pure and Applied Logic | issn=0168-0072 | volume=3 | issue=1 | pages=1–44}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Shelah cardinal",
      "url": "https://en.wikipedia.org/wiki/Shelah_cardinal",
      "text": "In [[axiomatic set theory]], '''Shelah cardinals''' are a kind of [[large cardinal]]s. A [[cardinal number|cardinal]] <math>\\kappa</math> is called Shelah [[iff]] for every <math>f:\\kappa\\rightarrow\\kappa</math>, there exists a [[transitive class]] <math>N</math> and an [[elementary embedding]] <math>j:V\\rightarrow N</math> with [[critical point (set theory)|critical point]] <math>\\kappa</math>; and <math>V_{j(f)(\\kappa )}\\subset N</math>.\n\nA Shelah cardinal has a normal [[ultrafilter]] containing the set of [[weakly hyper-Woodin cardinal]]s below it.\n\n== References ==\n\n* Ernest Schimmerling, ''Woodin cardinals, Shelah cardinals and the Mitchell-Steel core model'', Proceedings of the American Mathematical Society 130/11, pp. 3385-3391, 2002, [https://web.archive.org/web/20050129103712/http://www.math.cmu.edu/~eschimme/papers/hyperwoodin.pdf online]\n\n{{settheory-stub}}\n[[Category:Large cardinals]]"
    },
    {
      "title": "Shrewd cardinal",
      "url": "https://en.wikipedia.org/wiki/Shrewd_cardinal",
      "text": "In [[mathematics]], a '''shrewd cardinal''' is a certain kind of [[large cardinal]] number introduced by {{harv|Rathjen|1995}}., extending the definition of [[indescribable cardinal]]s.\n\nA [[cardinal number]] κ is called λ-shrewd if for every [[Proposition (mathematics)|proposition]] φ, and set A ⊆ V<sub>κ</sub> with (V<sub>κ+λ</sub>, ∈, A) ⊧ φ there exists an α, λ' < κ with (V<sub>α+λ'</sub>, ∈, A ∩ V<sub>α</sub>) ⊧ φ. It is called shrewd if it is λ-shrewd for every λ (including λ > κ).\n\nThis definition extends the concept of [[totally indescribable cardinal|indescribability]] to transfinite levels. A λ-shrewd cardinal is also μ-shrewd for any ordinal μ < λ. Shrewdness was developed by [[Michael Rathjen]] as part of his [[ordinal analysis]] of [[second-order arithmetic|&Pi;<sup>1</sup><sub>2</sub>-comprehension]]. It is essentially the nonrecursive analog to the [[stable ordinal|stability]] property for [[admissible ordinal]]s.\n\nMore generally, a cardinal number κ is called λ-Π<sub>m</sub>-shrewd if for every Π<sub>m</sub> proposition φ, and set A ⊆ V<sub>κ</sub> with (V<sub>κ+λ</sub>, ∈, A) ⊧ φ there exists an α, λ' < κ with (V<sub>α+λ'</sub>, ∈, A ∩ V<sub>α</sub>) ⊧ φ.\n\nHere one looks at formulas with m-1 alternations of quantifiers with the outermost quantifier being universal.\n\nFor finite ''n'', an ''n''-Π<sub>m</sub>-shrewd cardinals is the same thing as a Π<sub>m</sub><sup>n</sup>-indescribable cardinal.\n\nIf κ is a [[subtle cardinal]], then the set of κ-shrewd cardinals is [[stationary set|stationary]] in κ. Rathjen does not state how shrewd cardinals compare to [[unfoldable cardinal]]s, however.\n\nλ-shrewdness is an improved version of λ-indescribability, as defined in Drake; this cardinal property differs in that the reflected substructure must be (V<sub>α+λ</sub>, ∈, A ∩ V<sub>α</sub>), making it impossible for a cardinal κ to be κ-indescribable. Also, the monotonicity property is lost: a λ-indescribable cardinal may fail to be α-indescribable for some ordinal α < λ.\n\n== References ==\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n* {{cite web|url=http://www.icm2006.org/proceedings/Vol_II/contents/ICM_Vol_2_03.pdf|last=Rathjen|first=Michael|year=2006|title=The Art of Ordinal Analysis}}\n*{{Citation | last1=Rathjen | first1=Michael | title=Recent advances in ordinal analysis: Π<sup>1</sup><sub>2</sub>-CA and related systems | doi=10.2307/421132 | mr=1369172 | year=1995 | journal=The Bulletin of Symbolic Logic | issn=1079-8986 | volume=1 | issue=4 | pages=468–485}}\n{{Reflist}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Solovay model",
      "url": "https://en.wikipedia.org/wiki/Solovay_model",
      "text": "In the mathematical field of [[set theory]], the '''Solovay model''' is a [[model theory|model]] constructed by {{harvs|txt|first=Robert M. |last=Solovay|authorlink=Robert M. Solovay|year=1970}} in which all of the axioms of [[Zermelo–Fraenkel set theory]] (ZF) hold, exclusive of the [[axiom of choice]], but in which all [[set (mathematics)|sets]] of [[real number]]s are [[Lebesgue measurable]].  The construction relies on the existence of an [[inaccessible cardinal]].\n\nIn this way Solovay showed that the axiom of choice is essential to the proof of the existence of a [[non-measurable set]], at least granted that the existence of an inaccessible cardinal is consistent with [[ZFC]], the axioms of Zermelo–Fraenkel set theory including the axiom of choice.\n\n==Statement==\nZF stands for Zermelo–Fraenkel set theory, and DC for the [[axiom of dependent choice]].\n\nSolovay's theorem is as follows. \nAssuming the existence of an inaccessible cardinal, there is an [[inner model]] of ZF + DC of a suitable [[forcing extension]] ''V''[''G''] such that every set of reals is Lebesgue measurable, has the [[perfect set property]], and has the [[Baire property]].\n\n==Construction==\nSolovay constructed his model in two steps, starting with a model ''M'' of ZFC containing an inaccessible cardinal κ.\n\nThe first step is to take a [[Levy collapse]] ''M''[''G''] of ''M'' by adding a generic set ''G'' for the notion of forcing that collapses all cardinals less than κ to ω. Then ''M''[''G''] is a model of ZFC with the property that every set of reals that is definable over a countable sequence of ordinals is Lebesgue measurable, and has the Baire and perfect set properties. (This includes all definable and [[projective set]]s of reals; however for reasons related to [[Tarski's undefinability theorem]] the notion of a definable set of reals cannot be defined in the language of set theory, while the notion of a set of reals definable over a countable sequence of ordinals can be.)\n\nThe second step is to construct Solovay's model ''N'' as the class of all sets in ''M''[''G''] that are hereditarily definable over a countable sequence of ordinals. The model ''N'' is an inner model of ''M''[''G''] satisfying ZF + DC such that every set of reals is Lebesgue measurable, has the perfect set property, and has the Baire property. The proof of this uses the fact that every real in ''M''[''G''] is definable over a countable sequence of ordinals, and hence ''N'' and ''M''[''G''] have the same reals.\n\nInstead of using Solovay's model ''N'', one can also use the smaller inner model [[L(R)|''L''('''R''')]] of ''M''[''G''], consisting of the constructible closure of the real numbers, which has similar properties.\n\n==Complements==\n\nSolovay suggested in his paper that the use of an inaccessible cardinal might not be necessary. Several authors proved weaker versions of Solovay's result without assuming the existence of an inaccessible cardinal. In particular {{harvtxt|Krivine|1969}} showed there was a model of ZFC in which every ordinal-definable set of reals is measurable, Solovay showed there is a model of ZF + DC in which there is some translation-invariant extension of Lebesgue measure to all subsets of the reals, and {{harvtxt|Shelah|1984}} showed that there is a model in which all sets of reals have the Baire property (so that the inaccessible cardinal is indeed unnecessary in this case).\n\nThe case of the perfect set property was solved by {{harvtxt|Specker|1957}}, who showed (in ZF) that if every set of reals has the perfect set property and the first uncountable cardinal ℵ<sub>1</sub> is regular then ℵ<sub>1</sub> is inaccessible in the [[constructible universe]]. Combined with Solovay's result, this shows that the statements \"There is an inaccessible cardinal\" and \"Every set of reals has the perfect set property\" are equiconsistent over ZF.\n\nFinally, {{harvtxt|Shelah|1984}} showed that consistency of an inaccessible cardinal  is also necessary for constructing a model in which all sets of reals are Lebesgue measurable. More precisely he showed that if every [[Projective hierarchy#Table|'''Σ'''{{su|p=1|b=3}}]] set of reals is measurable then the first uncountable cardinal ℵ<sub>1</sub> is inaccessible in the constructible universe, so that the condition about an inaccessible cardinal cannot be dropped from Solovay's theorem. Shelah also showed that the '''Σ'''{{su|p=1|b=3}} condition is close to the best possible by constructing a model (without using an inaccessible cardinal) in which all '''Δ'''{{su|p=1|b=3}} sets of reals are measurable. See {{harvtxt|Raisonnier|1984}} and {{harvtxt|Stern|1985}} and {{harvtxt|Miller|1989}} for expositions of Shelah's result.\n\n{{harvtxt|Shelah|Woodin|1990}} showed that if [[supercompact cardinal]]s exist then every set of reals in ''L''('''R'''), the constructible sets generated by the reals, is Lebesgue measurable and has the Baire property; this includes every \"reasonably definable\" set of reals.\n\n==References==\n*{{Citation | last1=Krivine | first1=Jean-Louis | title=Modèles de ZF + AC dans lesquels tout ensemble de réels définissable en termes d'ordinaux est mesurable-Lebesgue | mr=0253894 | year=1969 | journal=Comptes Rendus de l'Académie des Sciences, Série A et B | issn=0151-0509 | volume=269 | pages=A549--A552}}\n*{{Citation | last1=Krivine | first1=Jean-Louis | title=Séminaire Bourbaki vol. 1968/69 Exposés 347-363 | series=Lecture Notes in Mathematics | isbn=978-3-540-05356-9 | doi=10.1007/BFb0058812 | year=1971 | volume=179 | chapter= Théorèmes de consistance en théorie de la mesure de R. Solovay | pages=187–197}}\n*{{Citation | last1=Miller | first1=Arnold W. | title=Review of \"Can You Take Solovay's Inaccessible Away? by Saharon Shelah\" | doi=10.2307/2274892 | jstor=2274892 | publisher=Association for Symbolic Logic | year=1989 | journal=The Journal of Symbolic Logic | issn=0022-4812 | volume=54 | issue=2 | pages=633–635}}\n*{{Citation | last1=Raisonnier | first1=Jean | title=A mathematical proof of S. Shelah's theorem on the measure problem and related results.  | doi=10.1007/BF02760523 | mr=0768265 | year=1984 | journal= Israel J. Math. | volume=48 | pages=48–56}}\n*{{Citation | last1=Shelah | first1=Saharon | author1-link=Saharon Shelah | title=Can you take Solovay's inaccessible away? | doi=10.1007/BF02760522 | mr=768264 | year=1984 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=48 | issue=1 | pages=1–47}}\n*{{Citation | last1=Shelah | first1=Saharon | author1-link=Saharon Shelah | last2=Woodin | first2=Hugh | author2-link=Hugh Woodin | title=Large cardinals imply that every reasonably definable set of reals is Lebesgue measurable | doi=10.1007/BF02801471 | mr=1074499 | year=1990 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=70 | issue=3 | pages=381–394}}\n*{{Citation | last1=Solovay | first1=Robert M. | author1-link=Robert M. Solovay | title=A model of set-theory in which every set of reals is Lebesgue measurable | jstor=1970696 | mr=0265151 | year=1970 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=92 | pages=1–56 | doi=10.2307/1970696}}\n*{{Citation | last1=Specker | first1=Ernst | author-link=Ernst Specker | title=Zur Axiomatik der Mengenlehre (Fundierungs- und Auswahlaxiom) | doi=10.1002/malq.19570031302 | mr=0099297 | year=1957 | journal=Zeitschrift für Mathematische Logik und Grundlagen der Mathematik | issn=0044-3050 | volume=3 | pages=173–210}}\n*{{Citation | last1=Stern | first1=Jacques | title=Le problème de la mesure | mr=768968 | year=1985 | journal=Astérisque | issn=0303-1179 | issue=121 | pages=325–346}}\n\n[[Category:Set theory]]\n[[Category:Measure theory]]\n[[Category:Large cardinals]]"
    },
    {
      "title": "Strong cardinal",
      "url": "https://en.wikipedia.org/wiki/Strong_cardinal",
      "text": "In [[set theory]], a '''strong cardinal''' is a type of [[large cardinal]]. It is a weakening of the notion of a [[supercompact cardinal]].\n\n== Formal definition ==\n\nIf &lambda; is any [[ordinal number|ordinal]], &kappa; is '''&lambda;-strong''' means that &kappa; is a [[cardinal number]] and there exists an [[elementary embedding]] ''j'' from the universe ''V'' into a transitive [[inner model]] ''M'' with [[critical point (set theory)|critical point]] &kappa; and\n\n:<math>V_\\lambda\\subseteq M</math>\n\nThat is, ''M'' agrees with ''V'' on an initial segment. Then &kappa; is '''strong''' means that it is &lambda;-strong for all ordinals &lambda;.\n\n== Relationship with other large cardinals ==\n\nBy definitions, strong cardinals lie below [[supercompact cardinal]]s and above [[measurable cardinal]]s in the consistency strength hierarchy.\n\n&kappa; is &kappa;-strong if and only if it is measurable. If &kappa; is strong or &lambda;-strong for &lambda; ≥ &kappa;+2, then the ultrafilter ''U'' witnessing that &kappa; is measurable will be in ''V''<sub>&kappa;+2</sub> and thus in ''M''. So for any &alpha; < &kappa;, we have that there exist an ultrafilter ''U'' in ''j''(''V''<sub>&kappa;</sub>) &minus; ''j''(''V''<sub>&alpha;</sub>), remembering that ''j''(&alpha;) = &alpha;. Using the elementary embedding backwards, we get that there is an ultrafilter in ''V''<sub>&kappa;</sub> &minus; ''V''<sub>&alpha;</sub>. So there are arbitrarily large measurable cardinals below &kappa; which is regular, and thus &kappa; is a limit of &kappa;-many measurable cardinals.\n\nStrong cardinals also lie below [[superstrong cardinal]]s and [[Woodin cardinal]]s in consistency strength.  However, the least strong cardinal is larger than the least superstrong cardinal.\n\nEvery strong cardinal is [[unfoldable cardinal|strongly unfoldable]] and therefore [[indescribable cardinal|totally indescribable]].\n\n== References ==\n\n* {{cite book|last=Kanamori|first=Akihiro|author-link=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Strongly compact cardinal",
      "url": "https://en.wikipedia.org/wiki/Strongly_compact_cardinal",
      "text": "In [[mathematics|mathematical]] [[set theory]], a '''strongly compact cardinal''' is a certain kind of [[large cardinal|large]] [[cardinal number]].\n\nA cardinal κ is strongly compact if and only if every κ-complete filter can be extended to a κ complete ultrafilter.\n\nStrongly compact cardinals were originally defined in terms of [[infinitary logic]], where [[logical operator]]s are allowed to take infinitely many operands. The logic on a [[regular cardinal]] κ is defined by requiring the number of operands for each operator to be less than κ; then κ is strongly compact if its logic satisfies an analog of the [[compactness theorem|compactness]] property of finitary logic.\nSpecifically, a statement which follows from some other collection of statements should also follow from some subcollection having cardinality less than κ.\n\nThe property of strong compactness may be weakened by only requiring this compactness property to hold when the original collection of statements has cardinality below a certain cardinal λ; we may then refer to λ-compactness. A cardinal is [[weakly compact cardinal|weakly compact]] if and only if it is κ-compact; this was the original definition of that concept.\n\nStrong compactness implies [[measurable cardinal|measurability]], and is implied by [[supercompact cardinal|supercompactness]]. Given that the relevant cardinals exist, it is consistent with ZFC either that the first measurable cardinal is strongly compact, or that the first strongly compact cardinal is supercompact; these cannot both be true, however. A measurable limit of strongly compact cardinals is strongly compact, but the least such limit is not supercompact.\n\nThe consistency strength of strong compactness is strictly above that of a [[Woodin cardinal]].  Some set theorists conjecture that existence of a strongly compact cardinal is equiconsistent with that of a supercompact cardinal.  However, a proof is unlikely until a canonical inner model theory for supercompact cardinals is developed.\n\n[[Extendible cardinal|Extendibility]] is a second-order analog of strong compactness.\n\n== References ==\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Subcompact cardinal",
      "url": "https://en.wikipedia.org/wiki/Subcompact_cardinal",
      "text": "In [[mathematics]], a '''subcompact cardinal''' is a certain kind of [[large cardinal]] number.\n\nA cardinal number ''κ'' is subcompact if and only if for every ''A''&nbsp;⊂&nbsp;''H''(''κ''<sup>+</sup>) there is a non-trivial elementary embedding j:(''H''(''μ''<sup>+</sup>), ''B'') → (''H''(''κ''<sup>+</sup>), ''A'') with [[critical point (set theory)|critical point]] ''μ'' and&nbsp;''j''(''μ'')&nbsp;=&nbsp;''κ''.\n\nAnalogously, ''κ'' is a '''quasicompact cardinal''' if and only if for every ''A''&nbsp;⊂&nbsp;''H''(''κ''<sup>+</sup>) there is a non-trivial elementary embedding ''j'':(''H''(''κ''<sup>+</sup>), ''A'') → (''H''(''μ''<sup>+</sup>), ''B'') with critical point ''κ'' and&nbsp;''j''(''κ'')&nbsp;=&nbsp;''μ''.\n\n''H''(''λ'') consists of all sets whose transitive closure has cardinality less than&nbsp;''λ''.\n\nEvery quasicompact cardinal is subcompact.  Quasicompactness is a strengthening of subcompactness in that it projects large cardinal properties upwards.  The relationship is analogous to that of [[extendible cardinal|extendible]] versus [[supercompact cardinal]]s.  Quasicompactness may be viewed as a strengthened or \"boldface\" version of 1-extendibility.  Existence of subcompact cardinals implies existence of many 1-extendible cardinals, and hence many [[superstrong cardinal]]s.  Existence of a 2<sup>''κ''</sup>-supercompact cardinal ''κ'' implies existence of many quasicompact cardinals.\n\nSubcompact cardinals are noteworthy as the least large cardinals implying a failure of the [[square principle]].  If κ is subcompact, then the square principle fails at κ. Canonical inner models at the level of subcompact cardinals satisfy the square principle at all but subcompact cardinals. (Existence of such models has not yet been proved, but in any case the square principle can be forced for weaker cardinals.)\n\nQuasicompactness is one of the strongest large cardinal properties that can be witnessed by current inner models that do not use long extenders. For current inner models, the elementary embeddings included are determined by their effect on ''P''(''κ'') (as computed at the stage the embedding is included), where κ is the critical point. This prevents them from witnessing even a ''κ''<sup>+</sup> [[strongly compact cardinal]]&nbsp;''κ''.\n\nSubcompact and quasicompact cardinals were defined by [[Ronald Jensen]].\n\n==See also==\n*[[Hereditarily countable set]]\n\n==References==\n*\"Square in Core Models\" in the September 2001 issue of the Bulletin of Symbolic Logic\n\n[[Category:Large cardinals]]\n{{settheory-stub}}"
    },
    {
      "title": "Subtle cardinal",
      "url": "https://en.wikipedia.org/wiki/Subtle_cardinal",
      "text": "In [[mathematics]], '''subtle cardinals''' and '''ethereal cardinals''' are closely related kinds of [[large cardinal]] number.\n\nA cardinal κ is called subtle if for every closed and unbounded ''C''&nbsp;⊂&nbsp;''κ'' and for every sequence ''A'' of length ''κ'' for which element number ''δ'' (for an arbitrary&nbsp;''δ''), ''A''<sub>''δ''</sub>&nbsp;⊂&nbsp;''δ'' there are ''α'',&nbsp;''β'', belonging to ''C'', with ''α''&nbsp;<&nbsp;''β'', such that ''A''<sub>''α''</sub>&nbsp;=&nbsp;''A''<sub>''β''</sub>&nbsp;∩&nbsp;''α''. A cardinal ''κ'' is called ethereal if for every closed and unbounded ''C''&nbsp;⊂&nbsp;''κ'' and for every sequence ''A'' of length ''κ'' for which element number ''δ'' (for an arbitrary&nbsp;''δ''), ''A''<sub>''δ''</sub>&nbsp;⊂&nbsp;''δ'' and ''A''<sub>''δ''</sub> has the same cardinal as&nbsp;''δ'', there are ''α'',&nbsp;''β'', belonging to ''C'', with ''α''&nbsp;<&nbsp;''β'', such that card(''α'')&nbsp;=&nbsp;card(''A''<sub>''β''</sub>&nbsp;∩&nbsp;''A''<sub>''α''</sub>).\n\nSubtle cardinals were introduced by {{harvtxt|Jensen|Kunen|1969}}. Ethereal cardinals were introduced by {{harvtxt|Ketonen|1974}}. Any subtle cardinal is ethereal, and any strongly inaccessible ethereal cardinal is subtle.\n\n== Theorem ==\n\nThere is a subtle cardinal ≤&nbsp;''κ'' if and only if every transitive set ''S'' of cardinality ''κ'' contains ''x'' and ''y'' such that ''x'' is a proper subset of ''y'' and ''x''&nbsp;≠&nbsp;Ø and ''x''&nbsp;≠&nbsp;{Ø}. An infinite ordinal ''κ'' is subtle if and only if for every ''λ''&nbsp;<&nbsp;''κ'', every transitive set ''S'' of cardinality ''κ'' includes a chain (under inclusion) of order type&nbsp;''λ''.\n\n== References ==\n\n*{{citation|first=Harvey |last=Friedman|authorlink=Harvey Friedman|title=Subtle Cardinals and Linear Orderings|journal= Annals of Pure and Applied Logic |year= 2001|volume=107|issue=1–3|pages=1–34 \n|doi=10.1016/S0168-0072(00)00019-1}} \n*{{citation|title=Some Combinatorial Properties of L and V |url=http://www.mathematik.hu-berlin.de/~raesch/org/jensen.html|first=R. B. |last=Jensen|first2=K.|last2=Kunen|publisher=Unpublished manuscript|year=1969}}\n*{{Citation | last1=Ketonen | first1=Jussi | title=Some combinatorial principles |mr=0332481 | year=1974 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=188 | pages=387–394 | doi=10.2307/1996785 | publisher=Transactions of the American Mathematical Society, Vol. 188 | jstor=1996785}}\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Supercompact cardinal",
      "url": "https://en.wikipedia.org/wiki/Supercompact_cardinal",
      "text": "In [[set theory]], a '''supercompact cardinal''' is a type of [[large cardinal]].  They display a variety of reflection properties.\n\n==Formal definition==\n\nIf &lambda; is any [[ordinal number|ordinal]], &kappa; is '''&lambda;-supercompact''' means that there exists an elementary embedding ''j'' from the universe ''V'' into a transitive [[inner model]] ''M'' with [[critical point (set theory)|critical point]] &kappa;, ''j''(&kappa;)>&lambda; and\n\n:<math>{ }^\\lambda M\\subseteq M \\,.</math>\n\nThat is, ''M'' contains all of its &lambda;-sequences.  Then &kappa; is '''supercompact''' means that it is &lambda;-supercompact for all ordinals &lambda;.\n\nAlternatively, an uncountable cardinal &kappa; is '''supercompact''' if for every ''A'' such that |''A''| ≥ &kappa; there exists a [[normal measure]] over [''A'']<sup>< &kappa;</sup>, in the following sense.\n\n[''A'']<sup>< &kappa;</sup> is defined as follows:\n\n:<math>[A]^{< \\kappa} := \\{x \\subseteq A| |x| < \\kappa\\} \\,.</math>\n\nAn [[ultrafilter]] ''U'' over [''A'']<sup>< &kappa;</sup> is ''fine'' if it is &kappa;-complete and <math>\\{x \\in [A]^{< \\kappa}| a \\in x\\} \\in U</math>, for every <math>a \\in A</math>. A normal measure over [''A'']<sup>< &kappa;</sup> is a fine ultrafilter ''U'' over [''A'']<sup>< &kappa;</sup> with the additional property that every function <math>f:[A]^{< \\kappa} \\to A </math> such that <math>\\{x \\in [A]^{< \\kappa}| f(x)\\in x\\} \\in U</math> is constant on a set in <math>U</math>. Here \"constant on a set in ''U''\" means that there is <math>a \\in A</math> such that <math>\\{x \\in [A]^{< \\kappa}| f(x)= a\\} \\in U </math>.\n\n==Properties==\nSupercompact cardinals have reflection properties. If a cardinal with some property (say a 3-[[huge cardinal]]) that is witnessed by a structure of limited rank exists above a supercompact cardinal &kappa;, then a cardinal with that property exists below &kappa;. For example, if &kappa; is supercompact and the [[Generalized Continuum Hypothesis]] holds below &kappa; then it holds everywhere because a bijection between the powerset of &nu; and a cardinal at least &nu;<sup>++</sup> would be a witness of limited rank for the failure of GCH at &nu; so it would also have to exist below &kappa;.\n\nFinding a canonical inner model for supercompact cardinals is one of the major problems of [[inner model theory]].\n\n==See also==\n* [[Indestructibility]]\n* [[Strongly compact cardinal]]\n\n==References==\n\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n* {{cite book|author=Jech, Thomas|title=Set theory, third millennium edition (revised and expanded)|publisher=Springer|year=2002|isbn=3-540-44085-2|authorlink=Thomas Jech}}\n* {{cite book|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Superstrong cardinal",
      "url": "https://en.wikipedia.org/wiki/Superstrong_cardinal",
      "text": "In [[mathematics]], a [[cardinal number]] &kappa; is called '''superstrong''' [[if and only if]] there exists an [[elementary embedding]] ''j'' : ''V'' &rarr; ''M'' from ''V'' into a transitive inner model ''M'' with [[critical point (set theory)|critical point]] &kappa; and <math>V_{j(\\kappa)}</math> &sube; ''M''.\n\nSimilarly, a cardinal κ is '''n-superstrong''' if and only if there exists an [[elementary embedding]] ''j'' : ''V'' &rarr; ''M'' from ''V'' into a transitive inner model ''M'' with [[critical point (set theory)|critical point]] &kappa; and <math>V_{j^n(\\kappa)}</math> &sube; ''M''. [[Akihiro Kanamori]] has shown that the consistency strength of an n+1-superstrong cardinal exceeds that of an [[n-huge cardinal]] for each n > 0.\n\n== References ==\n\n* {{cite book|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n\n[[Category:Set theory]]\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Tall cardinal",
      "url": "https://en.wikipedia.org/wiki/Tall_cardinal",
      "text": "In mathematics, a '''tall cardinal''' is a [[large cardinal]] ''κ'' that is ''θ''-tall for all ordinals ''θ'', where a cardinal is called ''θ''-tall if there is an [[elementary embedding]] ''j''&nbsp;:&nbsp;''V''&nbsp;→&nbsp;''M'' with critical point ''κ'' such that ''j''(''κ'')&nbsp;>&nbsp;θ and ''M''<sup>''κ''</sup>&nbsp;⊆&nbsp;M.\n\nTall cardinals are [[equiconsistent]] with [[strong cardinal]]s.\n\n==References==\n\n*{{Citation | last1=Hamkins | first1=Joel David | author-link=Joel David Hamkins | title=Tall cardinals | doi=10.1002/malq.200710084 |mr=2489293 | year=2009 | journal=MLQ. Mathematical Logic Quarterly | issn=0942-5616 | volume=55 | issue=1 | pages=68–86}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Unfoldable cardinal",
      "url": "https://en.wikipedia.org/wiki/Unfoldable_cardinal",
      "text": "In [[mathematics]], an '''unfoldable cardinal''' is a certain kind of [[large cardinal]] number.\n\nFormally, a [[cardinal number]] κ is '''λ-unfoldable''' if and only if for every [[inner model|transitive model]] ''M'' of cardinality κ of [[ZFC]]-minus-[[power set]] such that κ is in ''M'' and ''M'' contains all its sequences of length less than κ, there is a non-trivial [[elementary embedding]] ''j'' of ''M'' into a transitive model with the [[critical point (set theory)|critical point]] of ''j'' being κ and ''j''(κ) ≥ λ.\n\nA cardinal is '''unfoldable''' if and only if it is an λ-unfoldable for all [[ordinal number|ordinals]] λ.\n\nA [[cardinal number]] κ is '''strongly λ-unfoldable''' if and only if for every [[inner model|transitive model]] ''M'' of cardinality κ of [[ZFC]]-minus-[[power set]] such that κ is in ''M'' and ''M'' contains all its sequences of length less than κ, there is a non-trivial [[elementary embedding]] ''j'' of ''M'' into a transitive model \"N\" with the [[critical point (set theory)|critical point]] of ''j'' being κ, ''j''(κ) ≥ λ, and V(λ) is a subset of ''N''. Without loss of generality, we can demand also that ''N'' contains all its sequences of length λ.\n\nLikewise, a cardinal is '''strongly unfoldable''' if and only if it is strongly λ-unfoldable for all λ.\n\nThese properties are essentially weaker versions of [[strong cardinal|strong]] and [[supercompact cardinal|supercompact]] cardinals, consistent with [[Axiom of constructibility|V = L]]. Many theorems related to these cardinals have generalizations to their unfoldable or strongly unfoldable counterparts. For example, the existence of a strongly unfoldable implies the consistency of a slightly weaker version of the [[proper forcing axiom]].\n\nA [[Ramsey cardinal]] is unfoldable, and will be strongly unfoldable in L. It may fail to be strongly unfoldable in V, however.\n\nIn L, any unfoldable cardinal is strongly unfoldable; thus unfoldables and strongly unfoldables have the same [[consistency strength]].\n\nA cardinal k is κ-strongly unfoldable, and κ-unfoldable, if and only if it is [[Weakly compact cardinal|weakly compact]]. A κ+ω-unfoldable cardinal is [[totally indescribable cardinal|totally indescribable]] and preceded by a stationary set of totally indescribable cardinals.\n\n==References==\n* ''Unfoldable Cardinals and the GCH'', [[Joel David Hamkins]]. [[The Journal of Symbolic Logic]], Vol. 66, No. 3 (Sep., 2001), pp.&nbsp;1186–1198 {{doi|10.2307/2695100}}\n* ''Strongly unfoldable cardinals made indestructible'', Thomas A. Johnstone.  [[Journal of Symbolic Logic]], Volume 73, Issue 4 (2008), 1215-1248. {{doi|10.2178/jsl/1230396915}}\n* ''Diamond (on the regulars) can fail at any strongly unfoldable cardinal'', Joel David Hamkins (The City University of New York), Mirna Džamonja (University of East Anglia). (Submitted to arxiv (https://arxiv.org/abs/math/0409304) on 17 Sep 2004)\n\n[[Category:Large cardinals]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Vopěnka's principle",
      "url": "https://en.wikipedia.org/wiki/Vop%C4%9Bnka%27s_principle",
      "text": "In [[mathematics]], '''Vopěnka's principle''' is a [[large cardinal]] axiom. \nThe intuition behind the axiom is that the set-theoretical universe is so large that in every [[proper class]], some members are similar to others, with this similarity formalized through [[elementary embedding]]s.\n\nVopěnka's principle was first introduced by [[Petr Vopěnka]] and independently considered by [[H. Jerome Keisler]], and was written up by {{harvtxt|Solovay|Reinhardt|Kanamori|1978}}.\nAccording to {{harvtxt|Pudlák|2013|loc=p. 204}}, Vopěnka's principle was originally intended as a joke: Vopěnka was apparently unenthusiastic about large cardinals and introduced his principle as a bogus large cardinal property, planning to show later that it was not consistent. However, before publishing his inconsistency proof he found a flaw in it.\n\n==Definition==\n\nVopěnka's principle asserts that for every [[proper class]] of [[binary relation]]s (each with set-sized domain), there is one [[elementary embedding|elementarily embeddable]] into another. This cannot be stated as a single sentence of [[Zermelo–Fraenkel set theory|ZFC]] as it involves a quantification over classes.  A cardinal κ is called a '''Vopěnka cardinal''' if it is [[inaccessible cardinal|inaccessible]] and Vopěnka's principle holds in the rank ''V''<sub>κ</sub> (allowing arbitrary ''S'' ⊂ ''V''<sub>κ</sub> as \"classes\").\n<ref name=\"Kan\">{{cite book|last1=Kanamori|first1=Akihiro||author-link=Akihiro Kanamori|title=The higher infinite: large cardinals in set theory from their beginnings|date=2003|publisher=Springer|location=Berlin [u.a.]|isbn=9783540003847|edition=2nd}}</ref>\n\nMany equivalent formulations are possible.\nFor example, Vopěnka's principle is equivalent to each of the following statements. \n* For every proper class of [[Directed graph|simple directed graphs]], there are two members of the class with a homomorphism between them.<ref name=\"AdR\">{{cite book|last1=Rosicky|first1=Jiří Adámek ; Jiří|title=Locally presentable and accessible categories|date=1994|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=0521422612|edition=Digital print. 2004.}}</ref>\n* For any [[Signature (logic)|signature]] ''Σ'' and any proper class of ''Σ''-structures, there are two members of the class with an elementary embedding between them.<ref name=\"Kan\" /><ref name=\"AdR\" />\n* For every predicate ''P'' and proper class ''S'' of [[ordinal number|ordinals]], there is a non-trivial elementary embedding ''j'':(''V''<sub>κ</sub>, ∈, ''P'') → (V<sub>λ</sub>, ∈, ''P'') for some κ and λ in ''S''.<ref name=\"Kan\" />\n* The [[Category (mathematics)|category]] of ordinals cannot be fully embedded in the category of graphs.<ref name=\"AdR\" />\n* Every subfunctor of an [[Accessible category|accessible functor]] is accessible.<ref name=\"AdR\" />\n* (In a definable classes setting) For every natural number ''n'', there exists a [[Extendible cardinal#Variants and relation to other cardinals|''C<sup>(n)</sup>''-extendible cardinal]].<ref name=\"Bag\">{{cite journal|last1=Bagaria|first1=Joan|title=''C<sup>(n)</sup>''-cardinals|journal=Archive for Mathematical Logic|date=23 December 2011|volume=51|issue=3–4|pages=213–240|doi=10.1007/s00153-011-0261-8}}</ref>\n\n==Strength==\n\nEven when restricted to predicates and proper classes definable in first order set theory, the principle implies existence of Σ<sub>n</sub> correct [[extendible cardinal]]s for every ''n''.\n\nIf κ is an [[huge cardinal|almost huge cardinal]], then a strong form of Vopěnka's principle holds in ''V''<sub>κ</sub>:\n\n:There is a  κ-complete [[ultrafilter]] ''U'' such that for every {''R''<sub>''i''</sub>: ''i''&nbsp;&lt;&nbsp;κ} where each ''R''<sub>''i''</sub> is a binary relation and ''R''<sub>''i''</sub> ∈ ''V''<sub>κ</sub>, there is ''S''&nbsp;∈&nbsp;''U'' and a non-trivial elementary embedding ''j'': ''R''<sub>''a''</sub> → ''R''<sub>''b''</sub> for every ''a''&nbsp;&lt;&nbsp;''b'' in ''S''.\n\n==References==\n\n<references />\n*{{citation|mr=0519809 \n|last=Kanamori|first=Akihiro\n|chapter=On Vopěnka's and related principles|title= Logic Colloquium '77 (Proc. Conf., Wrocław, 1977)|pages=145–153, \n|series=Stud. Logic Foundations Math.|volume= 96|publisher= North-Holland|place= Amsterdam-New York|year= 1978|isbn=0-444-85178-X }}\n*{{citation|mr=3076860|last= Pudlák|first= Pavel|title= Logical foundations of mathematics and computational complexity. A gentle introduction|series= Springer Monographs in Mathematics|publisher= Springer|year= 2013|isbn= 978-3-319-00118-0|doi=10.1007/978-3-319-00119-7}}\n*{{citation|last=Solovay|first=Robert M.|first2=William N. |last2=Reinhardt|first3= Akihiro |last3=Kanamori|year=1978|title=Strong axioms of infinity and elementary embeddings|journal=Annals of Mathematical Logic|volume=13|issue=1|pages=73–116|authorlink=Robert M. Solovay|url=http://math.bu.edu/people/aki/d.pdf|doi=10.1016/0003-4843(78)90031-1|postscript=<!--None--> }}\n\n==External links==\n{{citation|title=EMBEDDING AXIOMS|first=Harvey M. |last=Friedman|author-link=Harvey M. Friedman |year=2005|url=http://www.cs.nyu.edu/pipermail/fom/2005-August/009023.html}} gives a number of equivalent definitions of Vopěnka's principle.\n\n{{DEFAULTSORT:Vopenka's principle}}\n[[Category:Large cardinals]]\n[[Category:Mathematical principles]]\n\n\n{{settheory-stub}}"
    },
    {
      "title": "Weakly compact cardinal",
      "url": "https://en.wikipedia.org/wiki/Weakly_compact_cardinal",
      "text": "In [[mathematics]], a '''weakly compact cardinal''' is a certain kind of [[cardinal number]] introduced by {{harvtxt|Erdős|Tarski|1961}}; weakly compact cardinals are [[large cardinal]]s, meaning that their existence cannot be proven  from the [[ZFC|standard axioms of set theory]]. (Tarski originally called them \"not strongly incompact\" cardinals.)\n\nFormally, a cardinal κ is defined to be weakly compact if it is uncountable and for every function ''f'': [κ] <sup> 2 </sup> → {0, 1} there is a [[Set (mathematics)|set]] of [[cardinality]] κ that is [[Homogeneous (large cardinal property)|homogeneous]] for ''f''.  In this context, [κ] <sup> 2 </sup> means the set of 2-element subsets of κ, and a subset ''S'' of κ is homogeneous for ''f'' [[if and only if]] either all of [''S'']<sup>2</sup> maps to 0 or all of it maps to 1.\n\nThe name \"weakly compact\" refers to the fact that if a cardinal is weakly compact then a certain related [[infinitary language]] satisfies a version of the [[compactness theorem]]; see below. \n\nEvery weakly compact cardinal is a [[reflecting cardinal]], and is also a limit of reflecting cardinals. This means also that weakly compact cardinals are [[Mahlo cardinal]]s, and the set of Mahlo cardinals less than a given weakly compact cardinal is [[Stationary set|stationary]].\n\n== Equivalent formulations ==\n\nThe following are equivalent for any [[uncountable]] cardinal κ:\n\n# κ is weakly compact.\n# for every λ<κ, natural number n ≥ 2, and function f: [κ]<sup>n</sup> → λ, there is a set of cardinality κ that is [[Homogeneous (large cardinal property)|homogeneous]] for f. {{harv|Drake|1974|loc=chapter 7 theorem 3.5}}\n# κ is [[Inaccessible cardinal|inaccessible]] and has the [[tree property]], that is, every [[Tree (set theory)|tree]] of height κ has either a level of size κ or a branch of size κ.\n# Every linear order of cardinality κ has an ascending or a descending sequence of order type κ.\n# κ is <math>\\Pi^1_1</math>-[[totally indescribable cardinal|indescribable]].\n# κ has the extension property. In other words, for all ''U'' ⊂ ''V''<sub>κ</sub> there exists a transitive set ''X'' with κ ∈ ''X'', and a subset ''S'' ⊂ ''X'', such that (''V''<sub>κ</sub>, ∈, ''U'') is an [[elementary substructure]] of (''X'', ∈, ''S''). Here, ''U'' and ''S'' are regarded as unary [[Predicate (mathematical logic)|predicates]].\n# For every set S of cardinality κ of subsets of κ, there is a non-trivial κ-complete filter that decides S.\n# κ is κ-[[unfoldable cardinal|unfoldable]].\n# κ is inaccessible and the [[infinitary language]] ''L''<sub>κ,κ</sub> satisfies the weak compactness theorem. \n# κ is inaccessible and the [[infinitary language]] ''L''<sub>κ,ω</sub> satisfies the weak compactness theorem.\n# κ is inaccessible and for every [[transitive set]] <math>M</math> of cardinality κ with κ <math>\\in M</math>, <sup><κ</sup><math>M\\subset M</math>, and satisfying a sufficiently large fragment of [[ZFC]], there is an [[elementary embedding]] <math>j</math> from <math>M</math> to a transitive set <math>N</math> of cardinality κ such that <sup><κ</sup><math>N\\subset N</math>, with [[Critical point (set theory)|critical point]] <math>crit(j)=</math>κ. {{harv|Hauser|1991|loc=Theorem 1.3}}\n\nA language ''L''<sub>κ,κ</sub> is said to satisfy the weak compactness theorem if whenever Σ is a set of sentences of cardinality at most κ and every subset with less than κ elements has a model, then Σ has a model. [[Strongly compact cardinal]]s are defined in a similar way without the restriction on the cardinality of the set of sentences.\n\n==See also==\n*[[List of large cardinal properties]]\n\n== References ==\n\n* {{citation|last=Drake|title=Set Theory: An Introduction to Large Cardinals |series=Studies in Logic and the Foundations of Mathematics |volume= 76|publisher=Elsevier Science Ltd|year=1974|isbn =0-444-10535-2|first =F. R.}}\n*{{citation|authorlink1=Paul Erdős|authorlink2=Alfred Tarski|last= Erdős|first= Paul|last2=Tarski|first2= Alfred|chapter= On some problems involving inaccessible cardinals|year=  1961 |title= Essays on the foundations of mathematics  |pages= 50–82 |publisher=Magnes Press, Hebrew Univ.|publication-place= Jerusalem|url=http://www.renyi.hu/~p_erdos/|mr=0167422}}\n* {{citation|last=Hauser|first=Kai|year=1991|title=Indescribable Cardinals and Elementary Embeddings|journal=Journal of Symbolic Logic|volume=56|publisher=Association for Symbolic Logic|pages=439-457|doi=10.2307/2274692}}\n* {{citation|last=Kanamori|first=Akihiro|authorlink=Akihiro Kanamori|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd |isbn= 3-540-00384-3}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Wholeness axiom",
      "url": "https://en.wikipedia.org/wiki/Wholeness_axiom",
      "text": "In mathematics, the '''wholeness axiom''' is a strong axiom of set theory introduced by Paul Corazza in 2000.<ref>{{citation|title=The Wholeness Axiom and Laver Sequences|first=Paul|last= Corazza|journal=Annals of Pure and Applied Logic |volume=  105|year=2000|pages= 157–260|doi=10.1016/S0168-0072(99)}}</ref>\n\n==Statement==\nThe wholeness axiom states roughly that there is an [[elementary embedding]] ''j'' from the [[Von Neumann universe]] ''V'' to itself. This has to be stated carefully to avoid [[Kunen's inconsistency theorem]] stating (roughly) that no such embedding exists.\n\nMore specifically, as Samuel Gomes da Silva states, \"the inconsistency is avoided by omitting from the schema all instances of the Replacement Axiom for ''j''-formulas\".<ref>Samuel Gomes da Silva, Review of \"The wholeness axioms and the class of supercompact cardinals\" by Arthur Apter, {{MR|2914539}}.</ref>\nThus, the wholeness axiom differs from [[Reinhardt cardinal]]s (another way of providing elementary embeddings from ''V'' to itself) by allowing the [[axiom of choice]] and instead modifying the [[axiom of replacement]].\nHowever, {{harvtxt|Holmes|Forster|Libert|2012}} write that Corrazza's theory should be \"naturally viewed as a version of [[Zermelo set theory]] rather than [[ZFC]]\".<ref>{{citation\n | last1 = Holmes | first1 = M. Randall\n | last2 = Forster | first2 = Thomas\n | last3 = Libert | first3 = Thierry\n | contribution = Alternative set theories\n | doi = 10.1016/B978-0-444-51621-3.50008-6\n | mr = 3409865\n | pages = 559–632\n | publisher = Elsevier/North-Holland, Amsterdam\n | series = Handb. Hist. Log.\n | title = Sets and extensions in the twentieth century\n | volume = 6\n | year = 2012}}.</ref>\n\nIf the wholeness axiom is consistent, then it is also consistent to add to the wholeness axiom the assertion that all sets are [[Ordinal definable set|hereditarily ordinal definable]].<ref name=\"h\"/>\nThe consistency of stratified versions of the wholeness axiom, introduced by {{harvtxt|Hamkins|2001}},<ref name=\"h\">{{citation\n | last = Hamkins | first = Joel David | author-link = Joel David Hamkins\n | doi = 10.1007/s001530050169\n | issue = 1\n | journal = Archive for Mathematical Logic\n | mr = 1816602\n | pages = 1–8\n | title = The wholeness axioms and ''V'' = HOD\n | volume = 40\n | year = 2001| arxiv = math/9902079}}.</ref> was studied by {{harvtxt|Apter|2012}}.<ref>{{citation\n | last = Apter | first = Arthur W.\n | doi = 10.4064/ba60-2-1\n | issue = 2\n | journal = Bulletin of the Polish Academy of Sciences\n | mr = 2914539\n | pages = 101–111\n | title = The wholeness axioms and the class of supercompact cardinals\n | volume = 60\n | year = 2012}}.</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://cantorsattic.info/Wholeness_axiom Wholeness axiom] in [http://cantorsattic.info Cantor's attic]\n\n[[Category:Large cardinals]]\n[[Category:Axioms of set theory]]"
    },
    {
      "title": "Woodin cardinal",
      "url": "https://en.wikipedia.org/wiki/Woodin_cardinal",
      "text": "In [[set theory]], a '''Woodin cardinal''' (named for [[W. Hugh Woodin]]) is a [[cardinal number]] λ such that for all functions\n\n:''f'' : λ &rarr; λ\n\nthere exists a cardinal κ < λ with\n\n: {''f''(β)|β < κ} &sube; κ\n\nand an [[elementary embedding]]\n\n:''j'' : ''V'' &rarr; ''M''\n\nfrom the [[Von Neumann universe]] ''V'' into a transitive [[inner model]] ''M'' with [[critical point (set theory)|critical point]] κ and\n\n:V<sub>j(f)(κ)</sub> &sube; ''M''.\n\nAn equivalent definition is this: λ is Woodin [[if and only if]] λ is [[inaccessible cardinal|strongly inaccessible]] and for all <math>A \\subseteq V_\\lambda</math> there exists a <math>\\lambda_A</math> < λ which is <math><\\lambda</math>-<math>A</math>-strong.\n\n<math>\\lambda _A</math> being <math><\\lambda</math>-<math>A</math>-strong means that for all [[ordinal number|ordinals]] α < λ, there exist a <math>j: V \\to M</math> which is an [[elementary embedding]] with [[critical point (set theory)|critical point]] <math>\\lambda _A</math>, <math>j(\\lambda _A) > \\alpha</math>, <math>V_\\alpha \\subseteq M</math> and <math>j(A) \\cap V_\\alpha = A \\cap V_\\alpha</math>.  (See also [[strong cardinal]].)\n\nA Woodin cardinal is preceded by a [[stationary set]] of [[measurable cardinal]]s, and thus it is a [[Mahlo cardinal]]. However, the first Woodin cardinal is not even [[Weakly compact cardinal|weakly compact]].\n\n== Consequences ==\n\nWoodin cardinals are important in [[descriptive set theory]].  By a result<ref>[https://www.jstor.org/stable/1990913 A Proof of Projective Determinacy]</ref> of [[Donald A. Martin|Martin]] and [[John R. Steel|Steel]], existence of infinitely many Woodin cardinals implies [[projective determinacy]], which in turn implies that every projective set is [[measurable]], has the [[Baire property]] (differs from an open set by a [[meagre set|meager set]], that is, a set which is a countable union of [[nowhere dense set]]s), and the [[perfect set property]] (is either countable or contains a [[Perfect set|perfect]] subset).\n\nThe consistency of the existence of Woodin cardinals can be proved using determinacy hypotheses.  Working in [[Zermelo–Fraenkel set theory|ZF]]+[[axiom of determinacy|AD]]+[[axiom of dependent choice|DC]] one can prove that <math>\\Theta _0</math> is Woodin in the class of hereditarily ordinal-definable sets. <math>\\Theta _0</math> is the first ordinal onto which the continuum cannot be mapped by an ordinal-definable surjection (see [[Θ (set theory)]]).\n\n[[Saharon Shelah|Shelah]] proved that if the existence of a Woodin cardinal is consistent then it is consistent that the nonstationary ideal on ω<sub>1</sub> is <math>\\aleph_2</math>-saturated. \nWoodin also proved the equiconsistency of the existence of infinitely many Woodin cardinals and the existence of an <math>\\aleph_1</math>-dense ideal over <math>\\aleph_1</math>.\n\n==Hyper-Woodin cardinals==\nA [[cardinal number|cardinal]] κ is called hyper-Woodin if  there exists a [[normal measure]] ''U'' on κ such that for every set ''S'', the set\n\n:{λ < κ | λ is <κ-''S''-[[strong cardinal|strong]]}\n\nis in ''U''.\n\nλ is <κ-S-strong if and only if for each δ < κ there is a [[transitive class]] ''N'' and an [[elementary embedding]]\n\n:j : V → N\n\nwith\n\n:λ = crit(j),\n:j(λ)&ge; δ, and\n\n:<math>j(S) \\cap H_\\delta = S \\cap H_\\delta</math>.\n\nThe name alludes to the classical result that a cardinal is Woodin if and only if for every set ''S'', the set\n\n:{λ < κ | λ is <κ-''S''-[[strong cardinal|strong]]}\n\nis a [[stationary set]]\n\nThe measure ''U'' will contain the set of all [[Shelah cardinal]]s below κ.\n\n==Weakly hyper-Woodin cardinals==\nA [[cardinal number|cardinal]] κ is called weakly hyper-Woodin if for every set ''S'' there exists a [[normal measure]] ''U'' on κ such that the set {λ < κ | λ is <κ-''S''-strong} is in ''U''. λ is <κ-S-strong if and only if for each δ < κ there is a transitive class N and an elementary\nembedding j : V → N with λ = crit(j), j(λ) >= δ, and <math>j(S) \\cap H_\\delta = S \\cap H_\\delta.</math>\n\nThe name alludes to the classic result that a cardinal is Woodin if  for every set ''S'', the set {λ < κ | λ is <κ-''S''-[[strong cardinal|strong]]} is stationary.\n\nThe difference between hyper-Woodin cardinals and weakly hyper-Woodin cardinals is that the choice of ''U'' does not depend on the choice of the set ''S'' for hyper-Woodin cardinals.\n\n== Notes and references ==\n<references/>\n\n== Further reading ==\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|authorlink=Akihiro Kanamori|publisher=Springer|title=The Higher Infinite: Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n* For proofs of the two results listed in consequences see ''Handbook of Set Theory'' (Eds. Foreman, Kanamori, Magidor) (to appear).  [http://handbook.assafrinot.com/ Drafts] of some chapters are available.\n* Ernest Schimmerling, ''Woodin cardinals, Shelah cardinals and the Mitchell-Steel core model'', Proceedings of the American Mathematical Society 130/11, pp.&nbsp;3385–3391, 2002, [http://www.ams.org/journals/proc/2002-130-11/S0002-9939-02-06455-9/S0002-9939-02-06455-9.pdf online]\n* {{cite journal | last = Steel | first = John R. | authorlink = John R. Steel |date=October 2007 | title = What is a Woodin Cardinal? | journal = [[Notices of the American Mathematical Society]] | volume = 54 | issue = 9 | pages = 1146&ndash;7 | url = http://www.ams.org/notices/200709/tx070901146p.pdf | format = [[PDF]] | accessdate = 2008-01-15 }}\n\n{{DEFAULTSORT:Woodin Cardinal}}\n[[Category:Large cardinals]]\n[[Category:Determinacy]]"
    },
    {
      "title": "Worldly cardinal",
      "url": "https://en.wikipedia.org/wiki/Worldly_cardinal",
      "text": "In mathematical set theory, a '''worldly cardinal''' is a [[Cardinal number|cardinal]] κ such that the [[Rank (set theory)|rank]] ''V''<sub>κ</sub> is a [[Model (model theory)|model]] of [[Zermelo–Fraenkel set theory]].\n\n==References==\n\n*{{citation|mr=3205072\n|last=Hamkins|first= Joel David|author-link=Joel David Hamkins\n|chapter=A multiverse perspective on the axiom of constructibility|title=Infinity and truth|pages= 25–45\n|series=Lect. Notes Ser. Inst. Math. Sci. Natl. Univ. Singap.|volume= 25|publisher= World Sci. Publ.|place= Hackensack, NJ|year= 2014|arxiv=1210.6541|bibcode=2012arXiv1210.6541H}}\n\n==External links==\n\n*[http://cantorsattic.info/Worldly Worldly cardinal] in Cantor's attic\n\n{{settheory-stub}}\n\n[[Category:Large cardinals]]"
    },
    {
      "title": "Zero dagger",
      "url": "https://en.wikipedia.org/wiki/Zero_dagger",
      "text": "{{No footnotes|date=June 2014}}\nIn [[set theory]], '''0<sup>†</sup>''' (zero dagger) is a particular subset of the natural numbers,  first defined by [[Robert M. Solovay]] in unpublished work in the 1960s. (The superscript † should be a [[†|dagger]], but it appears as a plus sign on some browsers.) The definition is a bit awkward, because there might be ''no'' set of natural numbers satisfying the conditions. Specifically, if [[ZFC]] is [[consistency|consistent]], then ZFC + \"0<sup>†</sup> does not exist\" is consistent. ZFC + \"0<sup>†</sup> exists\" is not known to be inconsistent (and most set theorists believe that it is consistent). In other words, it is believed to be independent (see [[Large cardinal property|large cardinal]] for a discussion). It is usually formulated as follows:\n\n:0<sup>†</sup> exists [[if and only if]] there exists a non-trivial [[elementary embedding]] &nbsp;''j'' : ''L[U]'' → ''L[U]'' for the relativized [[Gödel constructible universe]] ''L[U]'', where ''U'' is an [[ultrafilter]] witnessing that some cardinal κ is [[measurable cardinal|measurable]].\n\nIf 0<sup>†</sup> exists, then a careful analysis of the embeddings of ''L[U]'' into itself reveals that there is a closed unbounded subset of κ, and a closed unbounded proper class of ordinals greater than κ, which together are ''[[indiscernibles|indiscernible]]'' for the structure <math>(L,\\in,U)</math>, and 0<sup>†</sup> is defined to be the set of [[Gödel number]]s of the true formulas about the indiscernibles in ''L[U]''.\n\nSolovay showed that the existence of '''0<sup>†</sup>''' follows from the existence of two measurable cardinals. It is traditionally considered a [[large cardinal axiom]], although it is not a large cardinal, nor indeed a cardinal at all.\n\n== See also ==\n\n*[[Zero sharp|0<sup>#</sup>]]: a set of formulas (or subset of the integers) defined in a similar fashion, but  simpler.\n\n== References ==\n*{{Cite journal | last1=Kanamori | first1=Akihiro | author1-link=Akihiro Kanamori | last2=Awerbuch-Friedlander | first2=Tamara | author2-link = Tamara Awerbuch-Friedlander| title=The compleat 0<sup>†</sup> | doi=10.1002/malq.19900360206 |mr=1068949 | year=1990 | journal=Zeitschrift für Mathematische Logik und Grundlagen der Mathematik | issn=0044-3050 | volume=36 | issue=2 | pages=133–141}}\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|publisher=Springer|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n\n== External links ==\n*[http://www.zentralblatt-math.org/zmath/en/search/?q=an:04170902&type=pdf&format=complete Definition by \"Zentralblatt math database\" (PDF)]\n\n[[Category:Large cardinals]]\n\n\n{{mathlogic-stub}}"
    },
    {
      "title": "Zero sharp",
      "url": "https://en.wikipedia.org/wiki/Zero_sharp",
      "text": "In the mathematical discipline of [[set theory]], '''0<sup>#</sup>''' ('''zero sharp''', also '''0#''') is the set of true [[formula (mathematical logic)|formulae]] about [[indiscernibles]] and order-indiscernibles in the [[Gödel constructible universe]]. It is often encoded as a subset of the integers (using [[Gödel numbering]]), or as a subset of the [[hereditarily finite set]]s, or as a [[Baire space (set theory)|real number]].  Its existence is  unprovable in [[ZFC]], the standard form of [[axiomatic set theory]], but follows from a suitable [[large cardinal]] axiom. It was first introduced as a set of formulae in [[Jack Silver|Silver's]] 1966 thesis, later published as {{harvtxt|Silver|1971}}, where it was denoted by Σ, and rediscovered by  {{harvtxt|Solovay|1967|loc=p.52}}, who considered it as a subset of the natural numbers and introduced the notation O<sup>#</sup> (with a capital letter O; this later changed to the numeral '0').\n\nRoughly speaking, if 0<sup>#</sup> exists then the universe ''V'' of sets is much larger than the universe ''L'' of constructible sets, while if it does not exist then the universe of all sets is closely approximated by the constructible sets.\n\n== Definition ==\n\nZero sharp was defined by Silver and [[Robert M. Solovay|Solovay]] as follows. Consider the language of set theory with extra constant symbols ''c''<sub>1</sub>, ''c''<sub>2</sub>, ... for each positive integer. Then 0<sup>#</sup> is defined to be the set of [[Gödel number]]s of the true sentences about the constructible universe, with ''c''<sub>''i''</sub> interpreted as the uncountable cardinal ℵ<sub>''i''</sub>.\n(Here ℵ<sub>''i''</sub> means ℵ<sub>''i''</sub> in the full universe, not the constructible universe.)\n\nThere is a subtlety about this definition:  by [[Tarski's undefinability theorem]] it is not in general possible to define the truth of a formula of set theory in the language of set theory. To solve this, Silver and Solovay assumed the existence of a suitable large cardinal, such as a [[Ramsey cardinal]], and showed that with this extra assumption it is possible to define the truth of statements about the constructible universe. More generally, the definition of 0<sup>#</sup> works provided that there is an uncountable set of indiscernibles for some ''L''<sub>α</sub>, and the phrase \"0<sup>#</sup> exists\" is used as a shorthand way of saying this.\n\nThere are several minor variations of the definition of 0<sup>#</sup>, which make no significant difference to its properties. There are many different choices of Gödel numbering, and 0<sup>#</sup> depends on this choice. Instead of being considered as a subset of the natural numbers, it is also possible to encode 0<sup>#</sup> as a subset of formulae of a language, or as a subset of the hereditarily finite sets, or as a real number.\n\n==Statements implying existence==\n\nThe condition about the existence of a Ramsey cardinal implying that 0<sup>#</sup> exists can be weakened.  The existence of ω<sub>1</sub>-[[Erdős cardinal]]s  implies the existence of 0<sup>#</sup>. This is close to being best possible, because the existence of 0<sup>#</sup> implies that in the constructible universe there is an α-Erdős cardinal for all countable α, so such cardinals cannot be used to prove the existence of 0<sup>#</sup>.\n\n[[Chang's conjecture]] implies the existence of 0<sup>#</sup>.\n\n==Statements equivalent to existence==\nKunen showed that 0<sup>#</sup> exists if and only if there exists a non-trivial elementary embedding for the [[Gödel constructible universe]] ''L'' into itself.\n\n[[Donald A. Martin]] and [[Leo Harrington]] have shown that the existence of 0<sup>#</sup> is equivalent to the determinacy of [[lightface analytic game]]s.  In fact, the strategy for a universal lightface analytic game has the same [[Turing degree]] as 0<sup>#</sup>.\n\nIt follows from [[Jensen's covering theorem]] that the  existence of 0<sup>#</sup> is equivalent to ω<sub>ω</sub> being a [[regular cardinal]] in the constructible universe ''L''.\n\nSilver showed that the existence of an uncountable set of indiscernibles in the constructible universe is equivalent to the existence of 0<sup>#</sup>.\n\n== Consequences of existence and non-existence ==\nIts existence implies that every [[Uncountable set|uncountable]] [[Cardinal number|cardinal]] in the set-theoretic universe ''V'' is an indiscernible in ''L'' and satisfies all [[large cardinal]] axioms that are realized in ''L'' (such as being [[Ineffable cardinal|totally ineffable]]).  It follows that the existence of 0<sup>#</sup> contradicts the ''[[axiom of constructibility]]'': ''V'' = ''L''.\n\nIf 0<sup>#</sup> exists, then it is an example of a non-constructible Δ{{su|p=1|b=3}} set of integers. This is in some sense the simplest possibility for a non-constructible set, since all Σ{{su|p=1|b=2}} and Π{{su|p=1|b=2}} sets of integers are constructible.\n\nOn the other hand, if 0<sup>#</sup> does not exist, then the constructible universe ''L'' is the core model—that is, the canonical inner model that approximates the large cardinal structure of the universe considered. In that case,  [[Jensen's covering lemma]] holds:\n\n:For every uncountable set ''x'' of ordinals there is a constructible ''y'' such that ''x'' &sub; ''y'' and ''y'' has the same [[cardinality]] as ''x''.\n\nThis deep result is due to [[Ronald Jensen]]. Using [[forcing (mathematics)|forcing]] it is easy to see that the condition that ''x'' is uncountable cannot be removed. For example, consider '''[[List of forcing notions#Namba forcing|Namba forcing]]''', that preserves <math>\\omega_1</math> and collapses <math>\\omega_2</math> to an ordinal of [[cofinality]] <math>\\omega</math>. Let <math>G</math> be an <math>\\omega</math>-sequence [[cofinal (mathematics)|cofinal]] on <math>\\omega_2^L</math> and [[generic filter|generic]] over ''L''. Then no set in ''L'' of ''L''-size smaller than <math>\\omega_2^L</math> (which is uncountable in ''V'', since <math>\\omega_1</math> is preserved) can cover <math>G</math>, since <math>\\omega_2</math> is a [[regular cardinal|regular]] cardinal.\n\n== Other sharps ==\nIf x is any set, then x<sup>#</sup> is defined analogously to 0<sup>#</sup> except that one uses L[x] instead of L.  See the section on relative constructibility in [[constructible universe]].\n\n== See also ==\n* [[Zero dagger|0<sup>†</sup>]], a set similar to 0<sup>#</sup> where the constructible universe is replaced by a larger inner model with a [[measurable cardinal]].\n\n== References ==\n* {{cite book|author=Drake, F. R.|title=Set Theory: An Introduction to Large Cardinals (Studies in Logic and the Foundations of Mathematics ; V. 76)|publisher=Elsevier Science Ltd|year=1974|isbn=0-444-10535-2}}\n*{{Citation | last1=Harrington | first1=Leo | title=Analytic determinacy and 0<sup>#</sup> | doi=10.2307/2273508 | mr=518675 | year=1978 | journal=The Journal of Symbolic Logic | issn=0022-4812 | volume=43 | issue=4 | pages=685–693| jstor=2273508 }}\n* {{cite book | last1=Jech | first1=Thomas | author1-link=Thomas Jech | title=Set Theory | edition=Third Millennium | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Springer Monographs in Mathematics | isbn=978-3-540-44085-7 | year=2003 | zbl=1007.03002 }}\n* {{cite book|last=Kanamori|first=Akihiro|year=2003|publisher=Springer|authorlink=Akihiro Kanamori|title=The Higher Infinite : Large Cardinals in Set Theory from Their Beginnings|edition=2nd|isbn=3-540-00384-3}}\n*{{Citation | last1=Martin | first1=Donald A. | title=Measurable cardinals and analytic games | url=http://matwbn.icm.edu.pl/tresc.php?wyd=1&tom=66 | mr=0258637 | year=1970 | journal=Polska Akademia Nauk. Fundamenta Mathematicae | issn=0016-2736 | volume=66 | pages=287–291}}\n*{{Citation | last1=Silver | first1=Jack H. | title=Some applications of model theory in set theory | origyear=1966 | doi=10.1016/0003-4843(71)90010-6     | mr=0409188 | year=1971 | journal=Annals of Pure and Applied Logic | issn=0168-0072 | volume=3 | issue=1 | pages=45–110}}\n*{{Citation | last1=Solovay | first1=Robert M. | title=A nonconstructible Δ{{su|p=1|b=3}} set of integers | jstor=1994631 | mr=0211873 | year=1967 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=127 | pages=50–75 | doi=10.2307/1994631}}\n\n[[Category:Real numbers]]\n[[Category:Determinacy]]\n[[Category:Large cardinals]]\n[[Category:Constructible universe]]"
    },
    {
      "title": "Sorting algorithm",
      "url": "https://en.wikipedia.org/wiki/Sorting_algorithm",
      "text": "{{Short description|An algorithm that arranges lists in order}}\n{{unreferenced section|date=May 2019}}\nIn [[computer science]], a '''sorting algorithm''' is an [[algorithm]] that puts elements of a [[List (computing)|list]] in a certain [[Total order|order]]. The most frequently used orders are [[numerical order]] and [[lexicographical order]]. Efficient [[sorting]] is important for optimizing the efficiency of other algorithms (such as [[search algorithm|search]] and [[merge algorithm|merge]] algorithms) which require input data to be in sorted lists. Sorting is also often useful for [[Canonicalization|canonicalizing]] data and for producing human-readable output. More formally, the output of any sorting algorithm must satisfy two conditions:\n\n# The output is in nondecreasing order (each element is no smaller than the previous element according to the desired [[total order]]);<!-- confusing; this appears to mean that the output must be in ascending order, which isn't a necessary condition -->\n# The output is a [[permutation]] (a reordering, yet retaining all of the original elements) of the input.\n\nFurther, the input data is often stored in an [[Array data type|array]], which allows [[random access]], rather than a list, which only allows [[sequential access]]; though many algorithms can be applied to either type of data after suitable modification.\n\nSorting algorithms are often referred to as a word followed by the word \"sort,\" and grammatically are used in English as noun phrases, for example in the sentence, \"it is inefficient to use insertion sort on large lists,\" the phrase ''insertion sort'' refers to the [[insertion sort]] sorting algorithm.\n\n==History==\nFrom the beginning of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. Among the authors of early sorting algorithms around 1951 was [[Betty Holberton]] (née Snyder), who worked on [[ENIAC]] and [[UNIVAC]].<ref name=\"refrigerator\">{{Cite web|url=http://mentalfloss.com/article/53160/meet-refrigerator-ladies-who-programmed-eniac|title=Meet the 'Refrigerator Ladies' Who Programmed the ENIAC|website=Mental Floss|access-date=2016-06-16|date=2013-10-13}}</ref><ref name=\"NYTimes\">{{cite news|last1=Lohr|first1=Steve|title=Frances E. Holberton, 84, Early Computer Programmer|url=https://www.nytimes.com/2001/12/17/business/frances-e-holberton-84-early-computer-programmer.html|accessdate=16 December 2014|publisher=NYTimes|date=Dec 17, 2001}}</ref> [[Bubble sort]] was analyzed as early as 1956.<ref>Demuth, H. Electronic Data Sorting. PhD thesis, Stanford University, 1956.</ref> Comparison sorting algorithms have a fundamental requirement of [[Big omega notation|Ω(''n'' log ''n'')]] comparisons (some input sequences will require a multiple of ''n'' log ''n'' comparisons); algorithms not based on comparisons, such as [[counting sort]], can have better performance. Asymptotically optimal algorithms have been known since the mid-20th century&mdash;useful new algorithms are still being invented, with the now widely used [[Timsort]] dating to 2002, and the [[library sort]] being first published in 2006.\n\nSorting algorithms are prevalent in introductory [[computer science]] classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as [[big O notation]], [[divide and conquer algorithm]]s, [[data structure]]s such as [[heap (data structure)|heap]]s and [[binary tree]]s, [[randomized algorithm]]s, [[best, worst and average case]] analysis, [[time–space tradeoff]]s, and [[upper and lower bounds]].\n\n==Classification==<!-- This section is linked from [[Merge sort]] -->\nSorting algorithms are often classified by:\n\n* [[Computational complexity theory|Computational complexity]] ([[Best, worst and average case|worst, average and best]] behavior) in terms of the size of the list (''n''). For typical serial sorting algorithms good behavior is O(''n''&nbsp;log&nbsp;''n''), with parallel sort in O(log<sup>2</sup>&nbsp;''n''), and bad behavior is O(''n''<sup>2</sup>). (See [[Big O notation]].) Ideal behavior for a serial sort is O(''n''), but this is not possible in the average case. Optimal parallel sorting is O(log&nbsp;''n''). [[Comparison sort|Comparison-based sorting algorithms]] need at least Ω(''n''&nbsp;log&nbsp;''n'') comparisons for most inputs.\n* [[Computational complexity theory|Computational complexity]] of swaps (for \"in-place\" algorithms).\n* [[Memory (computing)|Memory]] usage (and use of other computer resources). In particular, some sorting algorithms are \"[[In-place algorithm|in-place]]\". Strictly, an in-place sort needs only O(1) memory beyond the items being sorted; sometimes O(log(''n'')) additional memory is considered \"in-place\".\n* Recursion.  Some algorithms are either recursive or non-recursive, while others may be both (e.g., merge sort).\n* Stability: [[#Stability|stable sorting algorithms]] maintain the relative order of records with equal keys (i.e., values).\n* Whether or not they are a [[comparison sort]]. A comparison sort examines the data only by comparing two elements with a comparison operator.\n* General method: insertion, exchange, selection, merging, ''etc.'' Exchange sorts include bubble sort and quicksort. Selection sorts include shaker sort and heapsort.\n* Whether the algorithm is serial or parallel. The remainder of this discussion almost exclusively concentrates upon serial algorithms and assumes serial operation.\n* Adaptability: Whether or not the presortedness of the input affects the running time.  Algorithms that take this into account are known to be [[Adaptive sort|adaptive]].\n\n===Stability===\n[[File:Sorting stability playing cards.svg|thumb|An example of stable sort on playing cards. When the cards are sorted by rank with a stable sort, the two 5s must remain in the same order in the sorted output that they were originally in. When they are sorted with a non-stable sort, the 5s may end up in the opposite order in the sorted output.]]\nStable sort algorithms sort repeated elements in the same order that they appear in the input. When sorting some kinds of data, only part of the data is examined when determining the sort order. For example, in the card sorting example to the right, the cards are being sorted by their rank, and their suit is being ignored. This allows the possibility of multiple different correctly sorted versions of the original list. Stable sorting algorithms choose one of these, according to the following rule: if two items compare as equal, like the two 5 cards, then their relative order will be preserved, so that if one came before the other in the input, it will also come before the other in the output.\n\nMore formally, the data being sorted can be represented as a record or tuple of values, and the part of the data that is used for sorting is called the ''key''. In the card example, cards are represented as a record (rank, suit), and the key is the rank. A sorting algorithm is stable if whenever there are two records R and S with the same key, and R appears before S in the original list, then R will always appear before S in the sorted list.\n\nWhen equal elements are indistinguishable, such as with integers, or more generally, any data where the entire element is the key, stability is not an issue. Stability is also not an issue if all keys are different.\n\nUnstable sorting algorithms can be specially implemented to be stable. One way of doing this is to artificially extend the key comparison, so that comparisons between two objects with otherwise equal keys are decided using the order of the entries in the original input list as a tie-breaker. Remembering this order, however, may require additional time and space.\n\nOne application for stable sorting algorithms is sorting a list using a primary and secondary key. For example, suppose we wish to sort a hand of cards such that the suits are in the order clubs (♣), diamonds (<span style=\"color:#ff0000\">♦</span>), hearts (<span style=\"color:#ff0000\">♥</span>), spades (♠), and within each suit, the cards are sorted by rank. This can be done by first sorting the cards by rank (using any sort), and then doing a stable sort by suit:\n\n[[File:Sorting playing cards using stable sort.svg|400px]]\n\nWithin each suit, the stable sort preserves the ordering by rank that was already done. This idea can be extended to any number of keys and is utilised by [[radix sort]]. The same effect can be achieved with an unstable sort by using a lexicographic key comparison, which, e.g., compares first by suit, and then compares by rank if the suits are the same.\n\n==Comparison of algorithms==\nIn this table, {{mvar|n}} is the number of records to be sorted. The columns \"Average\" and \"Worst\" give the [[time complexity]] in each case, under the assumption that the length of each key is constant, and that therefore all comparisons, swaps, and other needed operations can proceed in constant time. \"Memory\" denotes the amount of auxiliary storage needed beyond that used by the list itself, under the same assumption. The run times and the memory requirements listed below should be understood to be inside [[big O notation]], hence the base of the logarithms does not matter; the notation {{math|log<sup>2</sup> ''n''}} means {{math|(log ''n'')<sup>2</sup>}}.\n=== Comparison sorts ===\nBelow is a table of [[comparison sort]]s. A comparison sort cannot perform better than {{math|''O''(''n'' log ''n'')}} {{Citation needed|date=June 2019}}.\n\n{|class=\"wikitable sortable\"\n|+ [[Comparison sort]]s\n! Name !! Best !! Average !! Worst !! Memory !! Stable !! Method !! Other notes\n<!-- Sorting Guide:\n     00 = constant\n     05 = log n\n     10 = n^c, 0 < c < 1\n     15 = n\n     20 = n*log n or log n!\n     23 = n*(log n)^2 or n^c, 1 < c < 2\n     25 = n^2\n     30 = n^c, c > 2\n     40 = c^n, c > 1\n     45 = n!\n     50 = other -->\n|- align=\"center\"\n| [[Quicksort]]\n|style=\"background:#dfd\"| {{Sort|15|<math>n \\log n</math>}}<br/>variation is {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#ffd\"| {{Sort|05|<math>\\log n</math> on average, worst case space complexity is {{mvar|n}}; Sedgewick variation is <math>\\log n</math> worst case.}}\n|style=\"background:#ffd\"| Typical in-place sort is not stable; stable versions exist.\n| Partitioning\n|align=\"left\"| Quicksort is usually done in-place with {{math|''O''(log ''n'')}} stack space.<ref>{{cite book|last=Sedgewick|first=Robert|authorlink=Robert Sedgewick (computer scientist)|title=Algorithms In C: Fundamentals, Data Structures, Sorting, Searching, Parts 1-4|url=https://books.google.com/books?id=ylAETlep0CwC|accessdate=27 November 2012|edition=3|date=1 September 1998|publisher=Pearson Education|isbn=978-81-317-1291-7}}</ref><ref name=sedgewickQsortPaper>{{Cite journal | last1 = Sedgewick | first1 = R. | authorlink1 = Robert Sedgewick (computer scientist)| title = Implementing Quicksort programs | doi = 10.1145/359619.359631 | journal = [[Communications of the ACM|Comm. ACM]] | volume = 21 | issue = 10 | pages = 847–857 | year = 1978 | pmid =  | pmc = }}</ref>\n|- align=\"center\"\n| [[Merge sort]]\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}<br/>A hybrid [[Block sort|block merge sort]] is ''O''(1) mem.\n|style=\"background:#dfd\"| Yes\n| Merging\n|align=\"left\"| [[Merge sort#Parallel merge sort|Highly parallelizable]] (up to {{math|''O''(log ''n'')}} using the Three Hungarians' Algorithm<ref>{{Cite conference | doi = 10.1145/800061.808726| title = An {{math|''O''(''n'' log ''n'')}} sorting network| work = Proceedings of the fifteenth annual ACM symposium on Theory of computing | conference = [[Symposium on Theory of Computing|STOC]] '83| pages = 1–9| year = 1983| last1 = Ajtai | first1 = M. |author-link1 = Miklós Ajtai| last2 = Komlós | first2 = J. |author-link2 = János Komlós (mathematician)| last3 = Szemerédi | first3 = E. |author-link3 = Endre Szemerédi| isbn = 0-89791-099-0}}</ref> or, more practically, Cole's parallel merge sort) for processing large amounts of data.\n|- align=\"center\"\n|nowrap| [[In-place merge sort]]\n| —\n| —\n|style=\"background:#ffd\"| {{Sort|23|<math>n \\log^2 n</math>}}<br/> See above,  for hybrid, that is {{Sort|23|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes\n| Merging\n|align=\"left\"| Can be implemented as a stable sort based on stable in-place merging.<ref>{{Cite journal | doi = 10.1093/comjnl/35.6.643| title = Fast Stable Merging and Sorting in Constant Extra Space| journal = [[The Computer Journal|Comput. J.]]| volume = 35| issue = 6| pages = 643–650\n| date = December 1992| last1 = Huang | first1 = B. C. | last2 = Langston | first2 = M. A.| citeseerx=10.1.1.54.8381| url = http://comjnl.oxfordjournals.org/content/35/6/643.full.pdf}}</ref>\n|- align=\"center\"\n| [[Heapsort]]\n|style=\"background:#dfd\"| {{Sort|20|{{mvar|n}}}}<br/>If all keys are distinct, {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#fdd\"| No\n| Selection\n|align=\"left\"|\n|- align=\"center\"\n| [[Insertion sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes\n| Insertion\n|align=left| {{math|''O''(''n'' + ''d'')}}, in the worst case over sequences that have ''d'' [[Inversion (discrete mathematics)|inversions]].\n|- align=\"center\"\n| [[Introsort]]\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#ffd\"| {{Sort|05|<math>\\log n</math>}}\n|style=\"background:#fdd\"| No\n| Partitioning & Selection\n|align=\"left\"| Used in several [[Standard Template Library|STL]] implementations.\n|- align=\"center\"\n| [[Selection sort]]\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#fdd\"| No\n| Selection\n|align=left| Stable with {{tmath|O(n)}} extra space or when using linked lists.<ref>{{cite web|url=http://www.algolist.net/Algorithms/Sorting/Selection_sort|title=SELECTION SORT (Java, C++) -  Algorithms and Data Structures|author=|date=|website=www.algolist.net|accessdate=14 April 2018}}</ref>\n|- align=\"center\"\n| [[Timsort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| Yes\n| Insertion & Merging\n|align=\"left\"| Makes ''n'' comparisons when the data is already sorted or reverse sorted.\n|- align=\"center\"\n| [[Cubesort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| Yes\n| Insertion\n|align=\"left\"| Makes ''n'' comparisons when the data is already sorted or reverse sorted.\n|- align=\"center\"\n| [[Shell sort]]\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#ffd\"| {{Sort|23|Depends on gap sequence}}\n|style=\"background:#ffd\"| {{Sort|23|Depends on gap sequence;<br>best known is <math>n^{4/3}</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#fdd\"| No\n| Insertion\n|align=left| Small code size, no use of call stack, reasonably fast, useful where memory is at a premium such as embedded and older mainframe applications. There is a worst case {{tmath|O(n (\\log n)^2)}} gap sequence but it loses {{tmath|O(n \\log n)}} best case time.\n|- align=\"center\"\n| [[Bubble sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes <!-- Disputed: No. Equal values are never swapped, so they never get out of order -->\n| Exchanging\n|align=left| Tiny code size.\n|- align=\"center\"\n| [[Binary tree sort]]\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math><wbr/>(balanced)}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| Yes\n| Insertion\n|align=\"left\"| When using a [[self-balancing binary search tree]].\n|- align=\"center\"\n| [[Cycle sort]]\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#fdd\"| No\n| Insertion\n|align=left| In-place with theoretically optimal number of writes.\n|- align=\"center\"\n| [[Library sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| Yes\n| Insertion\n|align=left|\n|- align=\"center\"\n| [[Patience sorting]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n| —\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| No\n| Insertion & Selection\n|align=\"left\"| Finds all the [[longest increasing subsequence]]s in {{math|''O''(''n'' log ''n'')}}.\n|- align=\"center\"\n| [[Smoothsort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#fdd\"| No\n| Selection\n|align=\"left\"| An [[adaptive sort|adaptive]] variant of heapsort based upon the [[Leonardo number|Leonardo sequence]] rather than a traditional [[binary heap]].\n|- align=\"center\"\n| [[Strand sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| Yes\n| Selection\n|align=\"left\"|\n|- align=\"center\"\n| [[Tournament sort]]\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}<ref>http://dbs.uni-leipzig.de/skripte/ADS1/PDF4/kap4.pdf</ref>}}\n|style=\"background:#fdd\"| No\n| Selection\n|align=\"left\"| Variation of Heap Sort.\n|- align=\"center\"\n| [[Cocktail sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes\n| Exchanging\n|align=left|\n|- align=\"center\"\n| [[Comb sort]]\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#fdd\"| No\n| Exchanging\n|align=\"left\"| Faster than bubble sort on average.\n|- align=\"center\"\n| [[Gnome sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes\n| Exchanging\n|align=left| Tiny code size.\n|- align=\"center\"\n|nowrap|UnShuffle Sort<ref>{{cite journal|last=Kagel|first=Art|title=Unshuffle, Not Quite a Sort|journal=Computer Language|volume=2|issue=11|date=November 1985}}</ref>\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|{{mvar|kn}}}}\n|style=\"background:#fdd\"| {{Sort|25|{{mvar|kn}}}}\n|style=\"background:#ffd\"| {{Sort|00|In-place for linked lists. {{mono|n * [[sizeof]](link)}} for array. {{mono|n+1}} for array?}}\n|style=\"background:#fdd\"| No\n|Distribution and Merge\n|align=left|No exchanges are performed. The parameter ''k'' is proportional to the entropy in the input. ''k'' = 1 for ordered or reverse ordered input.\n|- align=\"center\"\n|nowrap| Franceschini's method<ref>{{Cite journal | doi = 10.1007/s00224-006-1311-1| title = Sorting Stably, in Place, with O(n log n) Comparisons and O(n) Moves| journal = Theory of Computing Systems| volume = 40| issue = 4| pages = 327–353\n| date = June 2007| last1 = Franceschini | first1 = G. }}</ref>\n| —\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes\n|{{dunno}}\n|align=\"left\"|\n|- align=\"center\"\n| [[Block sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|20|<math>n \\log n</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes\n| Insertion & Merging\n|align=left| Combine a block-based {{tmath|O(n)}} in-place merge algorithm<ref>{{Cite conference | doi = 10.1007/978-3-540-79228-4_22| title = Ratio Based Stable In-Place Merging| work = Theory and Applications of Models of Computation| conference = [[International Conference on Theory and Applications of Models of Computation|TAMC]] 2008| volume = 4978| pages = 246–257| series = [[Lecture Notes in Computer Science|LNCS]]| year = 2008| last1 = Kim | first1 = P. S. | last2 = Kutzner | first2 = A. | isbn = 978-3-540-79227-7| citeseerx = 10.1.1.330.2641}}</ref> with a [[Merge sort#Bottom-up implementation|bottom-up merge sort]].\n|- align=\"center\"\n| [[Odd–even sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n|style=\"background:#dfd\"| Yes\n| Exchanging\n|align=left| Can be run on parallel processors easily.\n|- align=\"center\"\n| [[Curve sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n|style=\"background:#fdd\"| {{Sort|00|<math>\\Omega(1) \\cap \\mathcal{O}(n)</math>}}\n|style=\"background:#dfd\"| Yes\n| Insertion & counting\n|align=left| Adapts to the smoothness of data.\n|}\n\n=== Non-comparison sorts ===\nThe following table describes [[integer sorting]] algorithms and other sorting algorithms that are not [[comparison sort]]s. As such, they are not limited to {{math|''Ω''(''n'' log ''n'')}} {{Citation needed|date=June 2019}}. Complexities below assume {{mvar|n}} items to be sorted, with keys of size {{mvar|k}}, digit size {{mvar|d}}, and {{mvar|r}} the range of numbers to be sorted. Many of them are based on the assumption that the key size is large enough that all entries have unique key values, and hence that {{math|''n'' ≪ 2<sup>''k''</sup>}}, where ≪ means \"much less than\". In the unit-cost [[random access machine]] model, algorithms with running time of <math>\\scriptstyle n \\cdot \\frac{k}{d}</math>, such as radix sort, still take time proportional to {{math|Θ(''n'' log ''n'')}}, because {{mvar|n}} is limited to be not more than <math>2^\\frac{k}{d}</math>, and a larger number of elements to sort would require a bigger {{mvar|k}} in order to store them in the memory.<ref>{{cite journal |first=Stefan |last=Nilsson |title=The Fastest Sorting Algorithm? |journal=[[Dr Dobbs]] |year=2000 |url=http://www.drdobbs.com/architecture-and-design/the-fastest-sorting-algorithm/184404062}}</ref>\n\n{|class=\"wikitable sortable\"\n|+ Non-comparison sorts\n! Name !! Best !! Average !! Worst !! Memory !! Stable !! {{math|''n'' ≪ 2<sup>''k''</sup>}} !! Notes\n|- align=\"center\"\n| [[Pigeonhole sort]]\n| —\n|style=\"background:#dfd\"| <math>n + 2^k</math>\n|style=\"background:#dfd\"| <math>n + 2^k</math>\n| <math>2^k</math>\n| {{Yes}}\n| {{Yes}}\n|align=\"left\"|\n|- align=\"center\"\n| [[Bucket sort]] (uniform keys)\n| —\n|style=\"background:#dfd\"| <math>n+k</math>\n|style=\"background:#fdd\"| <math>n^2 \\cdot k</math>\n| <math>n \\cdot k</math>\n| {{Yes}}\n| {{No}}\n|align=\"left\"| Assumes uniform distribution of elements from the domain in the array.<ref name=\"clrs\">{{Introduction to Algorithms|edition=2}}</ref>\n|- align=\"center\"\n| [[Bucket sort]] (integer keys)\n| —\n|style=\"background:#dfd\"| <math>n+r</math>\n|style=\"background:#dfd\"| <math>n+r</math>\n| <math>n+r</math>\n| {{Yes}}\n| {{Yes}}\n|align=\"left\"| If ''r'' is {{tmath|O(n)}}, then average time complexity is {{tmath|O(n)}}.<ref name=\"gt\">{{cite book\n | last1 = Goodrich | first1 = Michael T. | author1-link = Michael T. Goodrich\n | last2 = Tamassia | first2 = Roberto | author2-link = Roberto Tamassia\n | contribution = 4.5 Bucket-Sort and Radix-Sort\n | pages = 241–243\n | publisher = John Wiley & Sons\n | title = Algorithm Design: Foundations, Analysis, and Internet Examples\n | year = 2002\n | isbn = 978-0-471-38365-9}}</ref>\n|- align=\"center\"\n| [[Counting sort]]\n| —\n|style=\"background:#dfd\"| <math>n+r</math>\n|style=\"background:#dfd\"| <math>n+r</math>\n| <math>n+r</math>\n| {{Yes}}\n| {{Yes}}\n|align=\"left\"| If ''r'' is {{tmath|O(n)}}, then average time  complexity is {{tmath|O(n)}}.<ref name=\"clrs\" />\n|- align=\"center\"\n| [[Radix sort#Least significant digit radix sorts|LSD Radix Sort]]\n| —\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n| <math>n + 2^d</math>\n| {{Yes}}\n| {{No}}\n|align=\"left\"|<ref name=\"clrs\" /><ref name=\"gt\" />, <math>\\frac{k}{d}</math> recursion levels, 2<sup>''d''</sup> for count array.\n|- align=\"center\"\n| [[Radix sort#Most significant digit radix sorts|MSD Radix Sort]]\n| —\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n| <math>n + 2^d</math>\n| {{Yes}}\n| {{No}}\n|align=\"left\"| Stable version uses an external array of size {{mvar|n}} to hold all of the bins.\n|- align=\"center\"\n| [[Radix sort#Most significant digit radix sorts|MSD Radix Sort]] (in-place)\n| —\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n| <math>2^d</math>\n| {{No}}\n| {{No}}\n|align=\"left\"| d=1 for in-place, <math>k/1</math> recursion levels, no count array.\n|- align=\"center\"\n| [[Spreadsort]]\n|style=\"background:#dfd\"| {{mvar|n}}\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n|style=\"background:#dfd\"| <math>n \\cdot \\left( {\\frac{k}{s} + d} \\right)</math>\n| <math>\\frac{k}{d} \\cdot 2^d</math>\n| {{No}}\n| {{No}}\n|align=\"left\"| Asymptotic are based on the assumption that {{math|''n'' ≪ 2<sup>''k''</sup>}}, but the algorithm does not require this.\n|- align=\"center\"\n| [[Burstsort]]\n| —\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n| <math>n \\cdot \\frac{k}{d}</math>\n| {{No}}\n| {{No}}\n|align=\"left\"| Has better constant factor than radix sort for sorting strings. Though relies somewhat on specifics of commonly encountered strings.\n|- align=\"center\"\n| [[Flashsort]]\n|style=\"background:#dfd\"| {{mvar|n}}\n|style=\"background:#dfd\"| <math>n+r</math>\n|style=\"background:#fdd\"| <math>n^2</math>\n| {{mvar|n}}\n| {{No}}\n| {{No}}\n|align=\"left\"| Requires uniform distribution of elements from the domain in the array to run in linear time. If distribution is extremely skewed then it can go quadratic if underlying sort is quadratic (it is usually an insertion sort). In-place version is not stable.\n|- align=\"center\"\n| [[Postman sort]]\n| —\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n|style=\"background:#dfd\"| <math>n \\cdot \\frac{k}{d}</math>\n| <math>n+2^d</math>\n| —\n| {{No}}\n|align=\"left\"| A variation of bucket sort, which works very similar to MSD Radix Sort. Specific to post service needs.\n|}\n\n[[Samplesort]] can be used to parallelize any of the non-comparison sorts, by efficiently distributing data into several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other.\n\n=== Others ===\nSome algorithms are slow compared to those discussed above, such as the [[bogosort]] with unbounded run time and the [[stooge sort]] which has ''O''(''n''<sup>2.7</sup>) run time. These sorts are usually described for educational purposes in order to demonstrate how run time of algorithms is estimated. The following table describes some sorting algorithms that are impractical for real-life use in traditional software contexts due to extremely poor performance or specialized hardware requirements.\n\n{|class=\"wikitable sortable\"\n! Name !! Best !! Average !! Worst !! Memory !! Stable !! Comparison !! Other notes\n|- align=\"center\"\n| [[Bead sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#ffd\"| {{Sort|23|{{mvar|S}}}}\n|style=\"background:#ffd\"| {{Sort|23|{{mvar|S}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}\n| {{N/A}}\n| {{No}}\n| align=\"left\"| Works only with positive integers. Requires specialized hardware for it to run in guaranteed {{tmath|O(n)}} time. There is a possibility for software implementation, but running time will be {{tmath|O(S)}}, where {{mvar|S}} is sum of all integers to be sorted, in case of small integers it can be considered to be linear.\n|- align=\"center\"\n| [[Pancake sorting|Simple pancake sort]]\n| —\n| {{Sort|15|{{mvar|n}}}}\n| {{Sort|15|{{mvar|n}}}}\n| {{Sort|05|<math>\\log n</math>}}\n| {{No}}\n| {{Yes}}\n|align=\"left\"| Count is number of flips.\n|- align=\"center\"\n| [[Spaghetti sort|Spaghetti (Poll) sort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|25|<math>n^2</math>}}<!-- space should reflect amount of spaghetti needed; one rod must be at least n units long; n rods are needed. -->\n| {{Yes}}\n| Polling\n|align=\"left\"| This is a linear-time, analog algorithm for sorting a sequence of items, requiring ''O''(''n'') stack space, and the sort is stable. This requires ''n'' parallel processors. See [[spaghetti sort#Analysis]].<!-- see talk page discussion for June 2011 -->\n|- align=\"center\"\n| [[Sorting network]]\n| {{Sort|06|<math>\\log^2 n</math>}}\n| {{Sort|06|<math>\\log^2 n</math>}}\n| {{Sort|06|<math>\\log^2 n</math>}}\n| {{Sort|21|<math>n \\log^2 n</math>}}\n| {{Varies}} (stable sorting networks require more comparisons)\n| {{Yes}}\n|align=\"left\"| Order of comparisons are set in advance based on a fixed network size. Impractical for more than 32 items.{{disputed inline|reason=Sorting networks are highly practical; the \"specialized hardware\" required is a consumer-grade GPU.|date=November 2015}}\n|- align=\"center\"\n| [[Bitonic sorter]]\n| {{Sort|06|<math>\\log^2 n</math>}}\n| {{Sort|06|<math>\\log^2 n</math>}}\n| {{Sort|06|<math>\\log^2 n</math>}}\n| {{Sort|21|<math>n \\log^2 n</math>}}\n| {{No}}\n| {{Yes}}\n|align=\"left\"| An effective variation of Sorting networks.\n|- align=\"center\"\n| [[Bogosort]]\n|style=\"background:#dfd\"| {{Sort|15|{{mvar|n}}}}\n|style=\"background:#fdd\"| {{Sort|99|<math>(n\\times n!)</math>}}\n|style=\"background:#fdd\"| {{Sort|99|<math>\\infty</math>}}\n|style=\"background:#dfd\"| {{Sort|00|1}}\n| {{No}}\n| {{Yes}}\n|align=left| Random shuffling. Used for example purposes only, as sorting with unbounded worst case running time.\n|- align=\"center\"\n| [[Stooge sort]]\n|style=\"background:#fdd\"| {{Sort|30|<math>n^{\\log 3/\\log 1.5}</math>}}\n|style=\"background:#fdd\"| {{Sort|30|<math>n^{\\log 3/\\log 1.5}</math>}}\n|style=\"background:#fdd\"| {{Sort|30|<math>n^{\\log 3/\\log 1.5}</math>}}\n|style=\"background:#fdd\"| {{Sort|15|{{mvar|n}}}}\n| {{No}}\n| {{Yes}}\n|align=\"left\"| Slower than most of the sorting algorithms (even naive ones) with a time complexity of {{math|1=''O''(''n''<sup>log 3 / log 1.5 </sup>) = ''O''(''n''<sup>2.7095...</sup>)}}.\n|}\n\nTheoretical computer scientists have detailed other sorting algorithms that provide better than ''O''(''n'' log ''n'') time complexity assuming additional constraints, including:\n\n* '''Han's algorithm''', a deterministic algorithm for sorting keys from a [[Domain of a function|domain]] of finite size, taking {{math|''O''(''n'' log log ''n'')}} time and ''O''(''n'') space.<ref>{{Cite journal | doi = 10.1016/j.jalgor.2003.09.001| title = Deterministic sorting in O(n log log n) time and linear space| journal = Journal of Algorithms| volume = 50| pages = 96–105\n| date = January 2004| last1 = Han | first1 = Y. }}</ref>\n* '''Thorup's algorithm''', a randomized algorithm for sorting keys from a domain of finite size, taking {{math|''O''(''n'' log log ''n'')}} time and ''O''(''n'') space.<ref>{{Cite journal |doi=10.1006/jagm.2002.1211 |title=Randomized Sorting in O(n log log n) Time and Linear Space Using Addition, Shift, and Bit-wise Boolean Operations |journal=Journal of Algorithms |volume=42 |issue=2 |pages=205–230 |date=February 2002 |last1=Thorup |first1=M. |author1-link = Mikkel Thorup}}</ref>\n* A randomized [[integer sorting]] algorithm taking <math>O\\left(n \\sqrt{\\log \\log n}\\right)</math> expected time and ''O''(''n'') space.<ref>{{Cite conference |doi=10.1109/SFCS.2002.1181890 |title=Integer sorting in O(n√(log log n)) expected time and linear space |conference=The 43rd Annual IEEE [[Symposium on Foundations of Computer Science]] |pages=135–144 |year=2002 |first1=Yijie |last1=Han |last2=Thorup |first2=M. |author2-link = Mikkel Thorup |isbn=0-7695-1822-2}}</ref>\n\n==Popular sorting algorithms==\nWhile there are a large number of sorting algorithms, in practical implementations a few algorithms predominate. Insertion sort is widely used for small data sets, while for large data sets an asymptotically efficient sort is used, primarily heap sort, merge sort, or quicksort. Efficient implementations generally use a [[hybrid algorithm]], combining an asymptotically efficient algorithm for the overall sort with insertion sort for small lists at the bottom of a recursion. Highly tuned implementations use more sophisticated variants, such as [[Timsort]] (merge sort, insertion sort, and additional logic), used in Android, Java, and Python, and [[introsort]] (quicksort and heap sort), used (in variant forms) in some [[sort (C++)|C++ sort]] implementations and in .NET.\n\nFor more restricted data, such as numbers in a fixed interval, [[#Distribution sort|distribution sorts]] such as counting sort or radix sort are widely used. Bubble sort and variants are rarely used in practice, but are commonly found in teaching and theoretical discussions.\n\nWhen physically sorting objects, such as alphabetizing papers (such as tests or books), people intuitively generally use insertion sorts for small sets. For larger sets, people often first bucket, such as by initial letter, and multiple bucketing allows practical sorting of very large sets. Often space is relatively cheap, such as by spreading objects out on the floor or over a large area, but operations are expensive, particularly moving an object a large distance – locality of reference is important. Merge sorts are also practical for physical objects, particularly as two hands can be used, one for each list to merge, while other algorithms, such as heap sort or quick sort, are poorly suited for human use. Other algorithms, such as [[library sort]], a variant of insertion sort that leaves spaces, are also practical for physical use.\n\n===Simple sorts===\nTwo of the simplest sorts are insertion sort and selection sort, both of which are efficient on small data, due to low overhead, but not efficient on large data. Insertion sort is generally faster than selection sort in practice, due to fewer comparisons and good performance on almost-sorted data, and thus is preferred in practice, but selection sort uses fewer writes, and thus is used when write performance is a limiting factor.\n\n====Insertion sort====\n{{Main|Insertion sort}}\n''[[Insertion sort]]'' is a simple sorting algorithm that is relatively efficient for small lists and mostly sorted lists, and is often used as part of more sophisticated algorithms. It works by taking elements from the list one by one and inserting them in their correct position into a new sorted list.<ref>{{citation |last=Wirth |first=Niklaus |authorlink=Niklaus Wirth |title=Algorithms & Data Structures |place=Upper Saddle River, NJ |publisher=Prentice-Hall |year=1986 |isbn=978-0130220059 |pages=76–77}}</ref> In arrays, the new list and the remaining elements can share the array's space, but insertion is expensive, requiring shifting all following elements over by one. [[#Shellsort|Shellsort]] (see below) is a variant of insertion sort that is more efficient for larger lists.\n\n====Selection sort====\n{{Main|Selection sort}}\n\n''Selection sort'' is an [[in-place algorithm|in-place]] [[comparison sort]]. It has [[Big O notation|O]](''n''<sup>2</sup>) complexity, making it inefficient on large lists, and generally performs worse than the similar [[insertion sort]]. Selection sort is noted for its simplicity, and also has performance advantages over more complicated algorithms in certain situations.\n\nThe algorithm finds the minimum value, swaps it with the value in the first position, and repeats these steps for the remainder of the list.<ref>{{harvnb|Wirth|1986|pp=79–80}}</ref> It does no more than ''n'' swaps, and thus is useful where swapping is very expensive.\n\n===Efficient sorts===\nPractical general sorting algorithms are almost always based on an algorithm with average time complexity (and generally worst-case complexity) O(''n'' log ''n''), of which the most common are heap sort, merge sort, and quicksort. Each has advantages and drawbacks, with the most significant being that simple implementation of merge sort uses O(''n'') additional space, and simple implementation of quicksort has O(''n''<sup>2</sup>) worst-case complexity. These problems can be solved or ameliorated at the cost of a more complex algorithm.\n\nWhile these algorithms are asymptotically efficient on random data, for practical efficiency on real-world data various modifications are used. First, the overhead of these algorithms becomes significant on smaller data, so often a hybrid algorithm is used, commonly switching to insertion sort once the data is small enough. Second, the algorithms often perform poorly on already sorted data or almost sorted data – these are common in real-world data, and can be sorted in O(''n'') time by appropriate algorithms. Finally, they may also be [[unstable sort|unstable]], and stability is often a desirable property in a sort. Thus more sophisticated algorithms are often employed, such as [[Timsort]] (based on merge sort) or [[introsort]] (based on quicksort, falling back to heap sort).\n\n====Merge sort====\n{{Main|Merge sort}}\n''Merge sort'' takes advantage of the ease of merging already sorted lists into a new sorted list. It starts by comparing every two elements (i.e., 1 with 2, then 3 with 4...) and swapping them if the first should come after the second. It then merges each of the resulting lists of two into lists of four, then merges those lists of four, and so on; until at last two lists are merged into the final sorted list.<ref>{{harvnb|Wirth|1986|pp=101–102}}</ref> Of the algorithms described here, this is the first that scales well to very large lists, because its worst-case running time is O(''n'' log ''n''). It is also easily applied to lists, not only arrays, as it only requires sequential access, not random access. However, it has additional O(''n'') space complexity, and involves a large number of copies in simple implementations.\n\nMerge sort has seen a relatively recent surge in popularity for practical implementations, due to its use in the sophisticated algorithm [[Timsort]], which is used for the standard sort routine in the programming languages [[Python (programming language)|Python]]<ref>{{cite web |url=http://svn.python.org/projects/python/trunk/Objects/listsort.txt|title=Tim Peters's original description of timsort|author=|date= |website =python.org|accessdate=14 April 2018}}</ref> and [[Java (programming language)|Java]] (as of [[JDK7]]<ref>{{cite web|url= http://cr.openjdk.java.net/~martin/webrevs/openjdk7/timsort/raw_files/new/src/share/classes/java/util/TimSort.java|title=OpenJDK's TimSort.java |author=|date=|website=java.net|accessdate=14 April 2018}}</ref>). Merge sort itself is the standard routine in [[Perl]],<ref>{{cite web|url=http://perldoc.perl.org/functions/sort.html|title=sort - perldoc.perl.org|author=|date=|website=perldoc.perl.org |accessdate=14 April 2018}}</ref> among others, and has been used in Java at least since 2000 in [[Java version history#J2SE 1.3|JDK1.3]].<ref name=\"mergesort_in_jdk13\">[http://java.sun.com/j2se/1.3/docs/api/java/util/Arrays.html#sort(java.lang.Object%5B%5D) Merge sort in Java 1.3], Sun. {{Webarchive|url=https://web.archive.org/web/20090304021927/http://java.sun.com/j2se/1.3/docs/api/java/util/Arrays.html#sort(java.lang.Object%5B%5D)#sort(java.lang.Object%5B%5D) |date=2009-03-04 }}</ref>\n\n====Heapsort====\n{{Main|Heapsort}}\n''Heapsort'' is a much more efficient version of [[selection sort]]. It also works by determining the largest (or smallest) element of the list, placing that at the end (or beginning) of the list, then continuing with the rest of the list, but accomplishes this task efficiently by using a data structure called a [[heap (data structure)|heap]], a special type of [[binary tree]].<ref>{{harvnb|Wirth|1986|pp=87–89}}</ref> Once the data list has been made into a heap, the root node is guaranteed to be the largest (or smallest) element. When it is removed and placed at the end of the list, the heap is rearranged so the largest element remaining moves to the root. Using the heap, finding the next largest element takes O(log ''n'') time, instead of O(''n'') for a linear scan as in simple selection sort. This allows Heapsort to run in O(''n'' log ''n'') time, and this is also the worst case complexity.\n\n====Quicksort====\n{{Main|Quicksort}}\n''Quicksort'' is a [[divide and conquer algorithm|divide and conquer]] [[algorithm]] which relies on a ''partition'' operation: to partition an array an element called a ''pivot'' is selected.<ref>{{harvnb|Wirth|1986|p=93}}</ref><ref>{{citation |last1=Cormen |first1=Thomas H. |author1-link=Thomas H. Cormen |last2=Leiserson |first2=Charles E. |author2-link=Charles E. Leiserson |last3=Rivest |first3=Ronald L. |author3-link=Ron Rivest |last4=Stein |first4=Clifford |author4-link=Clifford Stein |title=Introduction to Algorithms |edition=3rd |place=Cambridge, MA |publisher=The MIT Press |year=2009 |isbn=978-0262033848 |pages=171–172}}</ref> All elements smaller than the pivot are moved before it and all greater elements are moved after it. This can be done efficiently in linear time and [[in-place algorithm|in-place]]. The lesser and greater sublists are then recursively sorted. This yields average time complexity of O(''n'' log ''n''), with low overhead, and thus this is a popular algorithm. Efficient implementations of quicksort (with in-place partitioning) are typically unstable sorts and somewhat complex, but are among the fastest sorting algorithms in practice. Together with its modest O(log ''n'') space usage, quicksort is one of the most popular sorting algorithms and is available in many standard programming libraries.\n\nThe important caveat about quicksort is that its worst-case performance is O(''n''<sup>2</sup>); while this is rare, in naive implementations (choosing the first or last element as pivot) this occurs for sorted data, which is a common case. The most complex issue in quicksort is thus choosing a good pivot element, as consistently poor choices of pivots can result in drastically slower O(''n''<sup>2</sup>) performance, but good choice of pivots yields O(''n'' log ''n'') performance, which is asymptotically optimal. For example, if at each step the [[median]] is chosen as the pivot then the algorithm works in O(''n''&nbsp;log&nbsp;''n''). Finding the median, such as by the [[median of medians]] [[selection algorithm]] is however an O(''n'') operation on unsorted lists and therefore exacts significant overhead with sorting. In practice choosing a random pivot almost certainly yields O(''n''&nbsp;log&nbsp;''n'') performance.\n\n==== Shellsort ====\n[[File:Shell_sorting_algorithm_color_bars.svg|link=https://en.wikipedia.org/wiki/File:Shell_sorting_algorithm_color_bars.svg|right|thumb|A Shell sort, different from bubble sort in that it moves elements to numerous [[Swap (computer science)|swapping positions]].]]\n{{Main|Shellsort|l1=Shell sort}}\n''Shellsort'' was invented by [[Donald Shell]] in 1959. It improves upon insertion sort by moving out of order elements more than one position at a time. The concept behind Shellsort is that insertion sort performs in {{tmath|O(kn)}} time, where k is the greatest distance between two out-of-place elements. This means that generally, they perform in ''O''(''n''<sup>2</sup>), but for data that is mostly sorted, with only a few elements out of place, they perform faster. So, by first sorting elements far away, and progressively shrinking the gap between the elements to sort, the final sort computes much faster. One implementation can be described as arranging the data sequence in a two-dimensional array and then sorting the columns of the array using insertion sort.\n\nThe worst-case time complexity of Shellsort is an [[open problem]] and depends on the gap sequence used, with known complexities ranging from ''O''(''n''<sup>2</sup>) to ''O''(''n''<sup>4/3</sup>) and Θ(''n'' log<sup>2</sup> ''n''). This, combined with the fact that Shellsort is [[in-place]], only needs a relatively small amount of code, and does not require use of the [[call stack]], makes it is useful in situations where memory is at a premium, such as in [[Embedded system|embedded systems]] and [[Operating system kernel|operating system kernels]].\n\n===Bubble sort and variants===\n{{unreferenced section|date=May 2019}}\nBubble sort, and variants such as the [[cocktail sort]], are simple but highly inefficient sorts. They are thus frequently seen in introductory texts, and are of some theoretical interest due to ease of analysis, but they are rarely used in practice, and primarily of recreational interest. Some variants, such as the Shell sort, have open questions about their behavior.\n\n====Bubble sort====\n[[File:Bubblesort-edited-color.svg|thumb|right|A bubble sort, a sorting algorithm that continuously steps through a list, [[Swap (computer science)|swapping]] items until they appear in the correct order.]]\n{{Main|Bubble sort}}\n\n''Bubble sort'' is a simple sorting algorithm. The algorithm starts at the beginning of the data set. It compares the first two elements, and if the first is greater than the second, it swaps them. It continues doing this for each pair of adjacent elements to the end of the data set. It then starts again with the first two elements, repeating until no swaps have occurred on the last pass.<ref>{{harvnb|Wirth|1986|pp=81–82}}</ref> This algorithm's average time and worst-case performance is O(''n''<sup>2</sup>), so it is rarely used to sort large, unordered data sets. Bubble sort can be used to sort a small number of items (where its asymptotic inefficiency is not a high penalty). Bubble sort can also be used efficiently on a list of any length that is nearly sorted (that is, the elements are not significantly out of place). For example, if any number of elements are out of place by only one position (e.g. 0123546789 and 1032547698), bubble sort's exchange will get them in order on the first pass, the second pass will find all elements in order, so the sort will take only 2''n'' time.\n\n<ref>{{Cite web\n  | url=https://github.com/torvalds/linux/blob/72932611b4b05bbd89fafa369d564ac8e449809b/kernel/groups.c#L105\n  | title=kernel/groups.c\n  | accessdate=2012-05-05}}</ref>\n\n====Comb sort====\n{{Main|Comb sort}}\n''Comb sort'' is a relatively simple sorting algorithm based on [[bubble sort]] and originally designed by Wlodzimierz Dobosiewicz in 1980.<ref name=BB>{{Cite journal | doi = 10.1016/S0020-0190(00)00223-4| title = Analyzing variants of Shellsort| journal = [[Information Processing Letters|Inf. Process. Lett.]]| volume = 79| issue = 5| pages = 223–227\n| date = 15 September 2001| last1 = Brejová | first1 = B. }}</ref> It was later rediscovered and popularized by Stephen Lacey and Richard Box with a [[Byte Magazine|''Byte'' Magazine]] article published in April 1991. The basic idea is to eliminate ''turtles'', or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously. (''Rabbits'', large values around the beginning of the list, do not pose a problem in bubble sort) It accomplishes this by initially swapping elements that are a certain distance from one another in the array, rather than only swapping elements if they are adjacent to one another, and then shrinking the chosen distance until it is operating as a normal bubble sort. Thus, if Shellsort can be thought of as a generalized version of insertion sort that swaps elements spaced a certain distance away from one another, comb sort can be thought of as the same generalization applied to bubble sort.\n\n===Distribution sort===\n{{see also|External sorting}}\n''Distribution sort'' refers to any sorting algorithm where data is distributed from their input to multiple intermediate structures which are then gathered and placed on the output. For example, both [[bucket sort]] and [[flashsort]] are distribution based sorting algorithms. Distribution sorting algorithms can be used on a single processor, or they can be a [[distributed algorithm]], where individual subsets are separately sorted on different processors, then combined. This allows [[external sorting]] of data too large to fit into a single computer's memory.\n\n====Counting sort====\n{{Main|Counting sort}}\nCounting sort is applicable when each input is known to belong to a particular set, ''S'', of possibilities.  The algorithm runs in O(|''S''| + ''n'') time and O(|''S''|) memory where ''n'' is the length of the input.  It works by creating an integer array of size |''S''| and using the ''i''th bin to count the occurrences of the ''i''th member of ''S'' in the input.  Each input is then counted by incrementing the value of its corresponding bin.  Afterward, the counting array is looped through to arrange all of the inputs in order.  This sorting algorithm often cannot be used because ''S'' needs to be reasonably small for the algorithm to be efficient, but it is extremely fast and demonstrates great asymptotic behavior as ''n'' increases.  It also can be modified to provide stable behavior.\n\n====Bucket sort====\n{{Main|Bucket sort}}\nBucket sort is a [[divide and conquer algorithm|divide and conquer]] sorting algorithm that generalizes [[counting sort]] by partitioning an array into a finite number of buckets.  Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm.\n\nA bucket sort works best when the elements of the data set are evenly distributed across all buckets.\n\n====Radix sort====\n{{Main|Radix sort}}\n''Radix sort'' is an algorithm that sorts numbers by processing individual digits. ''n'' numbers consisting of ''k'' digits each are sorted in O(''n'' · ''k'') time.  Radix sort can process digits of each number either starting from the [[least significant digit]] (LSD) or starting from the [[most significant digit]] (MSD).  The LSD algorithm first sorts the list by the least significant digit while preserving their relative order using a stable sort. Then it sorts them by the next digit, and so on from the least significant to the most significant, ending up with a sorted list. While the LSD radix sort requires the use of a stable sort, the MSD radix sort algorithm does not (unless stable sorting is desired).  In-place MSD radix sort is not stable.  It is common for the [[counting sort]] algorithm to be used internally by the radix sort.  A [[hybrid algorithm|hybrid]] sorting approach, such as using [[insertion sort]] for small bins improves performance of radix sort significantly.\n\n==Memory usage patterns and index sorting==\nWhen the size of the array to be sorted approaches or exceeds the available primary memory, so that (much slower) disk or swap space must be employed, the memory usage pattern of a sorting algorithm becomes important, and an algorithm that might have been fairly efficient when the array fit easily in RAM may become impractical. In this scenario, the total number of comparisons becomes (relatively) less important, and the number of times sections of memory must be copied or swapped to and from the disk can dominate the performance characteristics of an algorithm. Thus, the number of passes and the localization of comparisons can be more important than the raw number of comparisons, since comparisons of nearby elements to one another happen at [[computer bus|system bus]] speed (or, with caching, even at [[Central Processing Unit|CPU]] speed), which, compared to disk speed, is virtually instantaneous.\n\nFor example, the popular recursive [[quicksort]] algorithm provides quite reasonable performance with adequate RAM, but due to the recursive way that it copies portions of the array it becomes much less practical when the array does not fit in RAM, because it may cause a number of slow copy or move operations to and from disk. In that scenario, another algorithm may be preferable even if it requires more total comparisons.\n\nOne way to work around this problem, which works well when complex records (such as in a [[relational database]]) are being sorted by a relatively small key field, is to create an index into the array and then sort the index, rather than the entire array. (A sorted version of the entire array can then be produced with one pass, reading from the index, but often even that is unnecessary, as having the sorted index is adequate.)  Because the index is much smaller than the entire array, it may fit easily in memory where the entire array would not, effectively eliminating the disk-swapping problem. This procedure is sometimes called \"tag sort\".<ref>{{cite web|url=https://www.pcmag.com/encyclopedia_term/0,2542,t=tag+sort&i=52532,00.asp|title=tag sort Definition from PC Magazine Encyclopedia|author=|date=|website=www.pcmag.com|accessdate=14 April 2018}}</ref>\n\nAnother technique for overcoming the memory-size problem is using [[external sorting]], for example one of the ways is to combine two algorithms in a way that takes advantage of the strength of each to improve overall performance. For instance, the array might be subdivided into chunks of a size that will fit in RAM, the contents of each chunk sorted using an efficient algorithm (such as [[quicksort]]), and the results merged using a ''k''-way merge similar to that used in [[mergesort]]. This is faster than performing either mergesort or quicksort over the entire list.<ref>[[Donald Knuth]], ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Second Edition. Addison-Wesley, 1998, {{ISBN|0-201-89685-0}}, Section 5.4: External Sorting, pp. 248–379.</ref><ref>[[Ellis Horowitz]] and [[Sartaj Sahni]], ''Fundamentals of Data Structures'', H. Freeman & Co., {{ISBN|0-7167-8042-9}}.</ref>\n\nTechniques can also be combined. For sorting very large sets of data that vastly exceed system memory, even the index may need to be sorted using an algorithm or combination of algorithms designed to perform reasonably with [[virtual memory]], i.e., to reduce the amount of swapping required.\n\n==Related algorithms==\nRelated problems include [[partial sorting]] (sorting only the ''k'' smallest elements of a list, or alternatively computing the ''k'' smallest elements, but unordered) and [[selection algorithm|selection]] (computing the ''k''th smallest element). These can be solved inefficiently by a total sort, but more efficient algorithms exist, often derived by generalizing a sorting algorithm. The most notable example is [[quickselect]], which is related to [[quicksort]]. Conversely, some sorting algorithms can be derived by repeated application of a selection algorithm; quicksort and quickselect can be seen as the same pivoting move, differing only in whether one recurses on both sides (quicksort, [[divide and conquer algorithm|divide and conquer]]) or one side (quickselect, [[decrease and conquer]]).\n\nA kind of opposite of a sorting algorithm is a [[shuffling algorithm]]. These are fundamentally different because they require a source of random numbers. Shuffling can also be implemented by a sorting algorithm, namely by a random sort: assigning a random number to each element of the list and then sorting based on the random numbers. This is generally not done in practice, however, and there is a well-known simple and efficient algorithm for shuffling: the [[Fisher–Yates shuffle]].\n\n==See also==\n* {{annotated link|Collation}}\n* {{annotated link|Schwartzian transform}}\n* {{annotated link|Search algorithm}}\n* {{annotated link|Quantum sort}}\n\n==References==\n{{More footnotes|date=September 2009}}\n{{Reflist|30em}}\n\n==Further reading==\n* {{citation |last=Knuth |first=Donald E. |authorlink=Donald Knuth |series=The Art of Computer Programming |volume=3 |title=Sorting and Searching |edition=2nd |place=Boston |publisher=Addison-Wesley |year=1998 |isbn=978-0201896855}}\n\n==External links==\n{{wikibooks|Algorithm implementation|Sorting|Sorting algorithms}}\n{{wikibooks|A-level Mathematics|OCR/D1/Algorithms#Sorting Algorithms|Sorting algorithms}}\n{{Commons category|Sort algorithms|Sorting algorithms}}\n* {{webarchive |url=https://web.archive.org/web/20150303022622/http://www.sorting-algorithms.com/ |date=3 March 2015 |title=Sorting Algorithm Animations}}\n* [http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/algoen.htm Sequential and parallel sorting algorithms] – explanations and analyses of many sorting algorithms\n* [https://www.nist.gov/dads/ Dictionary of Algorithms, Data Structures, and Problems] – dictionary of algorithms, techniques, common functions, and problems\n* [http://www.softpanorama.org/Algorithms/sorting.shtml Slightly Skeptical View on Sorting Algorithms] – Discusses several classic algorithms and promotes alternatives to the [[quicksort]] algorithm\n* [https://www.youtube.com/watch?v=kPRA0W1kECg 15 Sorting Algorithms in 6 Minutes (Youtube)] – visualization and \"audibilization\" of 15 Sorting Algorithms in 6 Minutes\n* [https://oeis.org/A036604 A036604 sequence in OEIS database titled \"Sorting numbers: minimal number of comparisons needed to sort n elements\"] – which performed by [[Ford–Johnson algorithm]]\n* [https://www.youtube.com/watch?v=d2d0r1bArUQ Sorting Algorithms Used on Famous Paintings (Youtube)] - Visualization of Sorting Algorithms on Many Famous Paintings.\n{{sorting}}\n\n{{DEFAULTSORT:Sorting Algorithm}}\n[[Category:Sorting algorithms| ]]\n[[Category:Data processing]]"
    },
    {
      "title": "Adaptive heap sort",
      "url": "https://en.wikipedia.org/wiki/Adaptive_heap_sort",
      "text": "The '''adaptive heap sort''' is a [[sorting algorithm]] that is similar to [[heap sort]], but uses a [[randomized binary search tree]] to structure the input according to any preexisting order.  The randomized binary search tree is used to select candidates that are put into the heap, so the heap doesn't need to keep track of all elements.  Adaptive heap sort is a part of the [[Adaptive sort|adaptive sorting family]].\n\n==See also==\n* [[Adaptive sort]]\n\n==External links==\n*{{DADS|Adaptive heap sort|adaptiveHeapSort}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Heaps (data structures)]]\n\n{{datastructure-stub}}"
    },
    {
      "title": "Adaptive sort",
      "url": "https://en.wikipedia.org/wiki/Adaptive_sort",
      "text": "A [[sorting algorithm]] falls into the '''adaptive sort''' family if it takes advantage of existing order in its input.  It benefits from the presortedness in the input sequence – or a limited amount of [[randomness|disorder]] for various definitions of measures of disorder – and sorts faster.  Adaptive sorting is usually performed by modifying existing sorting algorithms.\n\n== Motivation ==\n[[Comparison sort|Comparison-based sorting algorithms]] have traditionally dealt with achieving an optimal bound of ''[[Big O notation|O]]''(''n'' log ''n'') when dealing with [[time complexity]].  Adaptive sort takes advantage of the existing order of the input to try to achieve better times, so that the time taken by the algorithm to sort is a smoothly growing function of the size of the sequence ''and'' the disorder in the sequence.  In other words, the more presorted the input is, the faster it should be sorted.\n\nThis is an attractive algorithm because nearly sorted sequences are common in practice.  Thus, the performance of existing sort algorithms can be improved by taking into account the existing order in the input.\n\nNote that most worst-case sorting algorithms that do optimally well in the worst-case, notably [[heap sort]] and [[merge sort]], do not take existing order within their input into account, although this deficiency is easily rectified in the case of [[merge sort]] by checking if the last element of the lefthand group is less than (or equal) to the first element of the righthand group, in which case a merge operation may be replaced by simple concatenation – a modification that is well within the scope of making an algorithm adaptive.\n\n== Examples ==\nA classic example of an adaptive sorting algorithm is ''Straight Insertion Sort''.  In this sorting algorithm, we scan the input from left to right, repeatedly finding the position of the current item, and insert it into an array of previously sorted items.\n\nIn [[pseudo-code]] form, the ''Straight Insertion Sort'' algorithm could look something like this (array X is zero-based):\n\n '''procedure''' Straight Insertion Sort (X):\n     '''for''' j := 1 '''downto''' length(X) - 1 '''do'''\n         t := X[j]\n         i := j\n         '''while''' i > 0 '''and''' X[i - 1] > t '''do'''\n             X[i] := X[i - 1]\n             i := i - 1\n         '''end'''\n         X[i] := t\n     '''end'''\n\nThe performance of this algorithm can be described in terms of the number of [[Inversion (discrete mathematics)|inversions]] in the input, and then {{tmath|T(n)}} will be roughly equal to {{tmath|I(A) + (n - 1)}}, where {{tmath|I(A)}} is the number of Inversions.  Using this measure of presortedness – being relative to the number of inversions – ''Straight Insertion Sort'' takes less time to sort the closer it is to being sorted.\n\nOther examples of adaptive sorting algorithms are [[adaptive heap sort]], [[Merge_sort#Natural_merge_sort|adaptive merge sort]], [[patience sort]],<ref name=\"Chandramouli\">{{Cite conference |last1=Chandramouli |first1=Badrish |last2=Goldstein |first2=Jonathan |title=Patience is a Virtue: Revisiting Merge and Sort on Modern Processors |conference=SIGMOD/PODS |year=2014}}</ref> [[Shellsort]], [[smoothsort]], [[splaysort]] and [[Timsort]].{{r|Chandramouli}}\n\n== See also ==\n* [[Sorting algorithms]]\n* [[Smoothsort]]\n\n== References ==\n* {{cite book\n  | last = Hagerup\n  | first = Torben\n  |author2= Jyrki Katjainen\n  | title = Algorithm Theory – SWAT 2004\n  | publisher = Springer-Verlag\n  | year = 2004\n  | location = Berlin Heidelberg\n  | pages = 221–222\n  | url = https://books.google.com/books?id=2E-KNhKl3gEC&pg=PA221&lpg=PA221&dq=adaptive+sort\n  | isbn = 3-540-22339-8}}\n* {{cite book\n  | last = Mehta\n  | first = Dinesh P.\n  |author2= [[Sartaj Sahni]]\n  | title = Data Structures and Applications\n  | publisher = Chapman & Hall/CRC\n  | year = 2005\n  | location = USA\n  | pages = 11‑8–11‑9\n  | url = https://books.google.com/books?id=fQVZy1zcpJkC&pg=PT230&lpg=PT230&dq=adaptive+sort\n  | isbn = 1-58488-435-5}}\n* {{cite journal\n  | doi = 10.1145/146370.146381\n  | last = Estivill-Castro\n  | first = Vladmir\n  |first2= Derick |last2=Wood | author2-link = Derick Wood\n  | title = A survey of adaptive sorting algorithms\n  | journal = ACM\n  | volume = 24\n  | issue = 4\n  | pages = 441–476\n  | publisher = ACM\n  | location = New York, NY, USA\n  | date = December 1992\n  | citeseerx = 10.1.1.45.8017\n  | issn = 0360-0300}}\n* {{cite journal\n  | last = Petersson\n  | first = Ola\n  |author2= Alistair Moffat\n  | title = A framework for adaptive sorting\n  | journal = [[Lecture Notes in Computer Science]]\n  | volume = 621\n  | pages = 422–433\n  | publisher = Springer Berlin / Heidelberg\n  | location = Berlin\n  | year = 1992\n  | url = http://www.springerlink.com/content/yv85w0u75777j021/\n  | issn = 1611-3349\n  | doi = 10.1007/3-540-55706-7_38\n  | accessdate = February 23, 2009| series = Lecture Notes in Computer Science\n  | isbn = 978-3-540-55706-7\n  }}\n<references />\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Batcher odd–even mergesort",
      "url": "https://en.wikipedia.org/wiki/Batcher_odd%E2%80%93even_mergesort",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Batcher Odd-Even Mergesort for eight inputs.svg|Visualization of the odd–even mergesort network with eight inputs]]\n|caption=Visualization of the odd–even mergesort network with eight inputs\n|data=[[Array data structure|Array]]\n|best-time= <math>O(\\log^2(n))</math> parallel time\n|average-time= <math>O(\\log^2(n))</math> parallel time\n|time=<math>O(\\log^2(n))</math> parallel time\n|space=<math>O(n\\log^2(n))</math> non-parallel time\n|optimal=No\n}}\n\n'''Batcher's odd–even mergesort''' is a generic construction devised by [[Ken Batcher]] for [[sorting network]]s of size O(''n''&nbsp;(log&nbsp;''n'')<sup>2</sup>) and depth O((log&nbsp;''n'')<sup>2</sup>), where ''n'' is the number of items to be sorted. Although it is not asymptotically optimal, [[Donald Knuth|Knuth]] concluded in 1998, with respect to the [[Sorting network#Optimal sorting networks|AKS network]] that \"Batcher's method is much better, unless ''n'' exceeds the total memory capacity of all computers on earth!\"<ref>[[Donald Knuth|D.E. Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1998. {{ISBN|0-201-89685-0}}. Section 5.3.4: Networks for Sorting, pp. 219&ndash;247.</ref>\n\nIt is popularized by the second ''[[GPU Gems]]'' book,<ref>https://developer.nvidia.com/gpugems/GPUGems2/gpugems2_chapter46.html</ref> as an easy way of doing reasonably efficient sorts on graphics-processing hardware.\n\n== Example code ==\n\nThe following is an implementation of odd–even mergesort algorithm in [[Python (programming language)|Python]]. The input is a list ''x'' of length a power of 2. The output is a list sorted in ascending order.\n<source lang=\"python\">\ndef oddeven_merge(lo, hi, r):\n    step = r * 2\n    if step < hi - lo:\n        yield from oddeven_merge(lo, hi, step)\n        yield from oddeven_merge(lo + r, hi, step)\n        yield from [(i, i + r) for i in range(lo + r, hi - r, step)]\n    else:\n        yield (lo, lo + r)\n\ndef oddeven_merge_sort_range(lo, hi):\n    \"\"\" sort the part of x with indices between lo and hi.\n\n    Note: endpoints (lo and hi) are included.\n    \"\"\"\n    if (hi - lo) >= 1:\n        # if there is more than one element, split the input\n        # down the middle and first sort the first and second\n        # half, followed by merging them.\n        mid = lo + ((hi - lo) // 2)\n        yield from oddeven_merge_sort_range(lo, mid)\n        yield from oddeven_merge_sort_range(mid + 1, hi)\n        yield from oddeven_merge(lo, hi, 1)\n\ndef oddeven_merge_sort(length):\n    \"\"\" \"length\" is the length of the list to be sorted.\n    Returns a list of pairs of indices starting with 0 \"\"\"\n    yield from oddeven_merge_sort_range(0, length - 1)\n\ndef compare_and_swap(x, a, b):\n    if x[a] > x[b]:\n        x[a], x[b] = x[b], x[a]\n</source>\n<source lang=\"pycon\">\n>>> data = [2, 4, 3, 5, 6, 1, 7, 8]\n>>> pairs_to_compare = list(oddeven_merge_sort(len(data)))\n>>> pairs_to_compare\n[(0, 1), (2, 3), (0, 2), (1, 3), (1, 2), (4, 5), (6, 7), (4, 6), (5, 7), (5, 6), (0, 4), (2, 6), (2, 4), (1, 5), (3, 7), (3, 5), (1, 2), (3, 4), (5, 6)]\n>>> for i in pairs_to_compare: compare_and_swap(data, *i)\n>>> data\n[1, 2, 3, 4, 5, 6, 7, 8]\n</source>\n\nMore concise and non-recursive calculation of partner node is possible. Here is a [[Scala (programming language)|Scala]] implementation to get the partner of an index at each step:<ref>{{cite web|url=https://gist.github.com/Bekbolatov/c8e42f5fcaa36db38402|title=Sorting network from Batcher's Odd-Even merge: partner calculation|publisher=Renat Bekbolatov|accessdate=7 May 2015}}</ref>\n<source lang=\"scala\">\n  def partner(index: Int, merge: Int, step: Int): Int = {\n    if (step == 1)\n      index ^ (1 << (merge - 1))\n    else {\n      val (scale, box) = (1 << (merge - step), 1 << step)\n      val sn = index / scale - (index / scale / box) * box\n\n      if (sn == 0 || sn == box - 1) index // no exchange at this level\n      else if (sn % 2 == 0) index - scale else index + scale\n    }\n  }\n</source>\n\n==See also==\n* [[Bitonic sorter]]\n\n== References ==\n<references/>\n\n==External links==\n*[http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/networks/oemen.htm Odd–even mergesort] at fh-flensburg.de\n*[https://bekbolatov.github.io/sorting/ Odd-even mergesort network generator] Interactive Batcher's Odd-Even merge-based sorting network generator.\n\n{{sorting}}\n\n{{DEFAULTSORT:Batcher odd-even mergesort}}\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Bead sort",
      "url": "https://en.wikipedia.org/wiki/Bead_sort",
      "text": "{{refimprove|date=October 2017}}\n{{notability|date=October 2017}}\n'''Bead sort''', also called '''gravity sort''', is a [[natural algorithm|natural]] [[sorting algorithm]], developed by [[Joshua J. Arulanandham]], [[Cristian S. Calude]] and [[Michael J. Dinneen]] in 2002, and published in The Bulletin of the [[European Association for Theoretical Computer Science]].  Both [[Digital data|digital]] and [[analog computer|analog]] hardware [[implementation]]s of bead sort can achieve a sorting time of ''[[Big O Notation|O]]''(''n''); however, the implementation of this algorithm tends to be significantly slower in [[software]] and can only be used to sort lists of [[positive integer]]s.  Also, it would seem that even in the best case, the algorithm requires ''[[Big O Notation|O]]''(''n<sup>2</sup>'') space.\n\n==Algorithm overview==\n[[File:BeadSort-Figure1.svg|thumb|right|Step 1: Suspended beads on vertical poles.]]\n[[File:BeadSort-Figure2.svg|thumb|right|Step 2: The beads have been allowed to fall.]]\nThe bead sort operation can be compared to the manner in which beads slide on parallel poles, such as on an [[abacus]].  However, each pole may have a distinct number of beads.  Initially, it may be helpful to imagine the beads suspended on vertical poles.  In Step 1, such an arrangement is displayed using ''n=5'' rows of beads on ''m=4'' vertical poles.  The numbers to the right of each row indicate the number that the row in question represents; rows 1 and 2 are representing the positive integer 3 (because they each contain three beads) while the top row represents the positive integer 2 (as it only contains two beads).<ref name=\"rowconventions\">By convention, a row representing the positive integer ''k'' should have beads on poles 1..''k'' and poles ''k''+1..''m'' should be empty.  This is not a strict requirement, but will most likely simplify implementation.</ref>\n\nIf we then allow the beads to fall, the rows now represent the same integers in sorted order.  Row 1 contains the largest number in the set, while row ''n'' contains the smallest.  If the above-mentioned convention of rows containing a series of beads on poles 1..''k'' and leaving poles ''k''+1..''m'' empty has been followed, it will continue to be the case here.\n\nThe action of allowing the beads to \"fall\" in our physical example has allowed the larger values from the higher rows to propagate to the lower rows.  If the value represented by row ''a'' is smaller than the value contained in row ''a+1'', some of the beads from row ''a+1'' will fall into row ''a''; this is certain to happen, as row ''a'' does not contain beads in those positions to stop the beads from row ''a+1'' from falling.\n\nThe mechanism underlying bead sort is similar to that behind [[counting sort]]; the number of beads on each pole corresponds to the number of elements with value equal or greater than the index of that pole.\n\n==Complexity==\n\nBead sort can be implemented with four general levels of complexity, among others:\n*''[[Big O Notation|O]]''(1): The beads are all moved simultaneously in the same time unit, as would be the case with the simple physical example above. This is an abstract complexity, and cannot be implemented in practice.\n*''[[Big O Notation|O]]''(<math>\\sqrt{n}</math>): In a realistic physical model that uses gravity, the time it takes to let the beads fall is proportional to the square root of the maximum height, which is proportional to n.\n*''[[Big O Notation|O]]''(n): The beads are moved one row at a time.  This is the case used in the analog and [[digital hardware]] solutions.\n*''[[Big O Notation|O]]''(S), where S is the sum of the integers in the input set: Each bead is moved individually.  This is the case when bead sort is implemented without a mechanism to assist in finding empty spaces below the beads, such as in software implementations.\n\nLike the [[Pigeonhole sort]], bead sort is unusual in that in worst case it can perform faster than ''[[Big O Notation|O]]''(''n'' [[logarithm|log]] ''n''), the fastest performance possible for a [[comparison sort]] in worst case. This is possible because the key for a bead sort is always a positive integer and bead sort exploits its structure.\n\n== Implementation ==\nThis implementation is written in Python; it is assumed that the {{code|input_list}} will be a sequence of integers. The function returns a new list rather than mutating the one passed in, but it can be trivially modified to operate in place efficiently.\n<syntaxhighlight lang=\"python3\" line=\"1\">\ndef beadsort(input_list):\n    return_list = []\n    # Initialize a 'transposed list' to contain as many elements as\n    # the maximum value of the input -- in effect, taking the 'tallest'\n    # column of input beads and laying it out flat\n    transposed_list = [0] * max(input_list)\n    for num in input_list:\n        # For each element (each 'column of beads') of the input list,\n        # 'lay the beads flat' by incrementing as many elements of the\n        # transposed list as the column is tall.\n        # These will accumulate atop previous additions.\n        transposed_list[:num] = [n + 1 for n in transposed_list[:num]]\n    # We've now dropped the beads. To de-transpose, we count the\n    # 'bottommost row' of dropped beads, then mimic removing this\n    # row by subtracting 1 from each 'column' of the transposed list.\n    # When a column does not reach high enough for the current row,\n    # its value in transposed_list will be <= 0.\n    for _ in input_list:\n        # Counting values > 0 is how we tell how many beads are in the\n        # current 'bottommost row'. Note that Python's bools can be\n        # evaluated as integers; True == 1 and False == 0.\n        return_list.append(sum(n > 0 for n in transposed_list))\n        # Remove the 'bottommost row' by subtracting 1 from each element.\n        transposed_list = [n - 1 for n in transposed_list]\n    # The resulting list is sorted in descending order\n    return return_list\n</syntaxhighlight>\n\n==References and notes==\n<references/>\n\n==External links==\n*{{cite web|url= http://www.cs.auckland.ac.nz/~jaru003/research/publications/journals/beadsort.pdf |title=Bead-Sort: A Natural Sorting Algorithm }}&nbsp;{{small|(114&nbsp;[[Kibibyte|KiB]])}}\n*[http://mgs.spatial-computing.org/ImageGallery/EXEMPLES/BeadSort/index.html Bead Sort in MGS], a visualization of a bead sort implemented in the [http://mgs.spatial-computing.org MGS] programming language\n*[http://mathworld.wolfram.com/Bead-Sort.html Bead Sort on MathWorld]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Binary prioritization",
      "url": "https://en.wikipedia.org/wiki/Binary_prioritization",
      "text": "{{multiple issues|\n{{Unreferenced|date=October 2013}}\n{{Orphan|date=October 2013}}\n}}\n\n'''Binary prioritization''' is a [[sorting algorithm]] which prioritizes to-do tasks.\n\nUnlike other [[Binary Sort]] methods (e.g. [[binary search]]) this method assumes that the deferred work will be prioritized in a later process, but their order is not relevant in the first [[iteration]]. The faster processing of classified and important tasks is achieved by reducing the cost of sorting by not sorting the subset of the less important tasks. In each iteration, the cost is reduced by the sorted elements.\n\n== Application requirements ==\nFor the application of the binary prioritization it is assumed that the elements to be prioritized are present in an unsorted heap.\n\n== Algorithm ==\n<gallery widths=\"220px\" style=\"text-align:center;\">\nFile:Binprio1.gif|Step 1\nFile:Binprio2.gif|Step 2\nFile:Binprio3.gif|Step 3\nFile:Binprio4.gif|Step 4\n</gallery>\nThe [[algorithm]] of the binary prioritization is as follows:\n* One element is taken from the stack (or list).\n* The element (the object) is analyzed regarding its priority (importance) relative to the other elements.\n* If the item is judged as ''important'', it is sorted on a new batch (to a new list) of important elements; if not, it is sorted on a different stack (in a different list) of unimportant elements. This is repeated in the first iteration until all items have been evaluated into two new stacks (or lists).\n* The two stacks (lists) are then reunited by the stacking the important elements on the elements that were considered to be unimportant. The last element of the first stack is memorized, e.g. by a ''[[Delimiter|separator]]''.\n* The algorithm is then applied in an additional [[iteration]] on the united stack up to the separator element.\n* If there was only one element in the last step, the algorithm is complete.\n\n== Application ==\nAn example of the application of Binary sorting is the following scenario: A mail inbox has to be sorted and important mails have to be answered first, no matter the ''relative importance'' of rather unimportant mails.\n\n[[Category:Sorting algorithms]]\n[[Category:Data processing]]"
    },
    {
      "title": "Bitonic sorter",
      "url": "https://en.wikipedia.org/wiki/Bitonic_sorter",
      "text": "{{refimprove|date=October 2017}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Batcher Bitonic Mergesort for eight inputs.svg| bitonic sort network with eight inputs]]\n|caption=Bitonic sort network with eight inputs.\n|data=[[Array data structure|Array]]\n|best-time= <math>O(\\log^2(n))</math> parallel time\n|average-time= <math>O(\\log^2(n))</math> parallel time\n|time=<math>O(\\log^2(n))</math> parallel time\n|space=<math>O(n \\log^2(n))</math> non-parallel time\n|optimal=No\n}}\n\n'''Bitonic mergesort''' is a [[parallel algorithm]] for sorting. It is also used as a construction method for building a [[sorting network]]. The algorithm was devised by [[Ken Batcher]]. The resulting sorting networks consist of <math>O(n\\log^2(n))</math> comparators and have a delay of <math>O(\\log^2(n))</math>, where <math>n</math> is the number of items to be sorted.<ref>[http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/bitonic/oddn.htm Bitonic sorting network for n not a power of 2<!-- Bot generated title -->]</ref>\n\nA sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A ''bitonic'' sequence is a sequence with <math>x_0 \\leq \\cdots \\leq x_k \\geq \\cdots \\geq x_{n-1}</math> for some <math>k, 0 \\leq k < n</math>, or a circular shift of such a sequence.\n\n== How the algorithm works ==\n\nThe following is a bitonic sorting network with 16 inputs:\n\n[[File:bitonicSort1.svg|Diagram of the bitonic sorting network with 16 inputs and arrows]]\n\nThe 16 numbers enter as the inputs at the left end, slide along each of the 16 horizontal wires, and exit at the outputs at the right end.  The network is designed to sort the elements, with the largest number at the bottom.\n\nThe arrows are comparators.  Whenever two numbers reach the two ends of an arrow, they are compared to ensure that the arrow points toward the larger number.  If they are out of order, they are swapped.  The colored boxes are just for illustration and have no effect on the algorithm.\n\nEvery red box has the same structure: each input in the top half is compared to the corresponding input in the bottom half, with all arrows pointing down (dark red) or all up (light red).  If the inputs happen to form a bitonic sequence (a single nondecreasing sequence followed by a single nonincreasing one or vice versa), then the output will form two bitonic sequences.  The top half of the output will be bitonic, and the bottom half will be bitonic, with every element of the top half less than or equal to every element of the bottom half (for dark red) or vice versa (for light red).  This theorem is not obvious, but can be verified by carefully considering all the cases of how the various inputs might compare, using the [[Sorting_network#Zero-one_principle|zero-one principle]], where a bitonic sequence is a sequence of 0s and 1s that contains no more than two \"10\" or \"01\" subsequences.\n\nThe red boxes combine to form blue and green boxes.  Every such box has the same structure: a red box is applied to the entire input sequence, then to each half of the result, then to each half of each of those results, and so on.  All arrows point down (blue) or all point up (green). This structure is known as a [[butterfly network]].  If the input to this box happens to be bitonic, then the output will be completely sorted in increasing order (blue) or decreasing order (green).  If a number enters the blue or green box, then the first red box will sort it into the correct half of the list.  It will then pass through a smaller red box that sorts it into the correct quarter of the list within that half.  This continues until it is sorted into exactly the correct position.  Therefore, the output of the green or blue box will be completely sorted.\n\nThe green and blue boxes combine to form the entire sorting network.  For any arbitrary sequence of inputs, it will sort them correctly, with the largest at the bottom.  The output of each green or blue box will be a sorted sequence, so the output of each pair of adjacent lists will be bitonic, because the top one is blue and the bottom one is green.  Each column of blue and green boxes takes N sorted sequences and concatenates them in pairs to form N/2 bitonic sequences, which are then sorted by the boxes in that column to form N/2 sorted sequences.  This process starts with each input considered to be a sorted list of one element, and continues through all the columns until the last merges them into a single, sorted list.  Because the last stage was blue, this final list will have the largest element at the bottom.\n\n=== Alternative representation ===\n\nEach green box performs the same operation as a blue box, but with the sort in the opposite direction.  So, each green box could be replaced by a blue box followed by a crossover where all the wires move to the opposite position.  This would allow all the arrows to point the same direction, but would prevent the horizontal lines from being straight.  However, a similar crossover could be placed to the right of the bottom half of the outputs from any red block, and the sort would still work correctly, because the reverse of a bitonic sequence is still bitonic.  If a red box then has a crossover before and after it, it can be rearranged internally so the two crossovers cancel, so the wires become straight again.  Therefore, the following diagram is equivalent to the one above, where each green box has become a blue plus a crossover, and each orange box is a red box that absorbed two such crossovers:\n\n[[File:bitonicSort.svg|Diagram of the bitonic sorting network with 16 inputs (and no arrows)]]\n\nThe arrowheads are not drawn, because every comparator sorts in the same direction.  The blue and red blocks perform the same operations as before.  The orange blocks are equivalent to red blocks where the sequence order is reversed for the bottom half of its inputs and the bottom half of its outputs.  This is the most common representation of a bitonic sorting network\n\n== Example code  ==\nThe following is an implementation of the bitonic mergesort sorting algorithm in [[Python (programming language)|Python]]. The input is a boolean value ''up'', and a list ''x'' of length a power of 2. The output is a sorted list that is ascending if ''up'' is true, and decreasing otherwise. \n<source lang=\"python\">\ndef bitonic_sort(up, x):\n    if len(x) <= 1:\n        return x\n    else: \n        first = bitonic_sort(True, x[:len(x) // 2])\n        second = bitonic_sort(False, x[len(x) // 2:])\n        return bitonic_merge(up, first + second)\n\ndef bitonic_merge(up, x): \n    # assume input x is bitonic, and sorted list is returned \n    if len(x) == 1:\n        return x\n    else:\n        bitonic_compare(up, x)\n        first = bitonic_merge(up, x[:len(x) // 2])\n        second = bitonic_merge(up, x[len(x) // 2:])\n        return first + second\n\ndef bitonic_compare(up, x):\n    dist = len(x) // 2\n    for i in range(dist):  \n        if (x[i] > x[i + dist]) == up:\n            x[i], x[i + dist] = x[i + dist], x[i] #swap\n</source>\n\n<source lang=\"pycon\">\n>>> bitonic_sort(True, [10, 30, 11, 20, 4, 330, 21, 110])\n[4, 10, 11, 20, 21, 30, 110, 330]\n>>> bitonic_sort(False, [10, 30, 11, 20, 4, 330, 21, 110])\n[330, 110, 30, 21, 20, 11, 10, 4]       \n</source>\n\n\nThe following is another implementation in [[Java (programming language)|Java]].\n\n<source lang=\"java\">\npublic class BitonicSort {\n    static void kernel(int[] a, final int p, final int q) {\n        final int d = 1 << (p-q);\n\n        for(int i = 0; i < a.length; i++) {\n            boolean up = ((i >> p) & 2) == 0;\n\n            if ((i & d) == 0 && (a[i] > a[i | d]) == up) {\n                int t = a[i];\n                a[i] = a[i | d];\n                a[i | d] = t;\n            }\n        }\n    }\n\n    static void bitonicSort(final int logn, int[] a) {\n        assert a.length == 1 << logn;\n\n        for(int i = 0; i < logn; i++) {\n            for(int j = 0; j <= i; j++) {\n                kernel(a, i, j);\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        final int logn = 5, n = 1 << logn;\n\n        int[] a0 = new int[n];\n        for(int i = 0; i < n; i++) {\n            a0[i] = (int)(Math.random() * 1000);\n        }\n\n        for(int k = 0; k < a0.length; k++) {\n            System.out.print(a0[k] + \" \");\n        }\n        System.out.println();\n\n        bitonicSort(logn, a0);\n\n        for(int k = 0; k < a0.length; k++) {\n            System.out.print(a0[k] + \" \");\n        }\n        System.out.println();\n    }\n}\n</source>\n\n==See also==\n* [[Batcher odd–even mergesort]]\n\n== References ==\n<References />\n\n==External links==\n*[http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/bitonic/bitonicen.htm A discussion of this algorithm]\n*[https://xlinux.nist.gov/dads/HTML/bitonicSort.html Reference code] at [[NIST]]\n*[http://www.tools-of-computing.com/tc/CS/Sorts/bitonic_sort.htm Tutorial with animated pictures and working code]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Block sort",
      "url": "https://en.wikipedia.org/wiki/Block_sort",
      "text": "{{Distinguish|Block-sorting compression}}\n{{Infobox Algorithm\n|image=[[File:Block sort with numbers 1 to 16 (thumb).gif|link=File:Block_sort_with_numbers_1_to_16.gif|border]]\n|caption=Block sort stably sorting numbers 1 to 16.<br />Insertion sort groups of 16, extract two internal buffers, tag the {{mvar|A}} blocks (of size {{math|1={{sqrt|16}} = 4}} each), roll the {{mvar|A}} blocks through {{mvar|B}}, locally merge them, sort the second buffer, and redistribute the buffers.\n|class=[[Sorting algorithm]]\n|data=[[Array data structure|Array]]\n|time={{math|''O''(''n'' log ''n'')}}\n|best-time={{math|''O''(''n'')}}\n|average-time={{math|''O''(''n'' log ''n'')}}\n|space={{math|''O''(1)}}}}\n\n'''Block sort''', or '''block merge sort''', is a [[sorting algorithm]] combining at least two [[merge algorithm|merge]] operations with an [[insertion sort]] to arrive at {{math|''O''(''n'' log ''n'')}} [[in-place]] [[sorting algorithm#Stability|stable]] sorting. It gets its name from the observation that merging two sorted lists, {{mvar|A}} and {{mvar|B}}, is equivalent to breaking {{mvar|A}} into evenly sized ''blocks'', inserting each {{mvar|A}} block into {{mvar|B}} under special rules, and merging {{mvar|AB}} pairs.\n\nOne practical algorithm for O(log n) in place merging was proposed by Pok-Son Kim and Arne Kutzner in 2008.<ref name=\"kim2008\">{{cite book|last1=Kutzner|first1=Arne|last2=Kim|first2=Pok-Son|year=2008|publisher=Springer Berlin Heidelberg|title=Ratio Based Stable In-Place Merging|series=[[Lecture Notes in Computer Science]]|volume=4978|pages=246–257|url=http://itbe.hanyang.ac.kr/ak/papers/tamc2008.pdf|accessdate=2016-09-07}}</ref>\n\n== Overview ==\nThe outer loop of block sort is identical to a [[Merge sort#Bottom-up implementation|bottom-up merge sort]], where each ''level'' of the sort merges pairs of subarrays, {{mvar|A}} and {{mvar|B}}, in sizes of 1, then 2, then 4, 8, 16, and so on, until both subarrays combined are the array itself.\n\nRather than merging {{mvar|A}} and {{mvar|B}} directly as with traditional methods, a block-based merge algorithm divides {{mvar|A}} into discrete blocks of size {{math|{{sqrt|''A''}}}} (resulting in {{math|{{sqrt|''A''}}}} ''number'' of blocks as well),<ref name=\"mannilla\">{{cite book|last1=Mannila|first=Heikki|last2=Ukkonen|first2=Esko|title=A Simple Linear Time Algorithm for In-Situ Merging|series=[[Information Processing Letters]]|volume=18|year=1984|pages=203–208|publisher=Elsevier B.V.|doi=10.1016/0020-0190(84)90112-1}}</ref> inserts each {{mvar|A}} block into {{mvar|B}} such that the first value of each {{mvar|A}} block is less than or equal (≤) to the {{mvar|B}} value immediately after it, then ''locally merges'' each {{mvar|A}} block with any {{mvar|B}} values between it and the next {{mvar|A}} block.\n\nAs merges still require a separate buffer large enough to hold the {{mvar|A}} block to be merged, two areas within the array are reserved for this purpose (known as ''internal buffers'').<ref name=\"kronrod\">{{cite book|last=Kronrod|first=Alexander|year=1969|title=An Optimal Ordering Algorithm without a Field Operation|periodical=Dokladi Akad Nauk SSSR|volume=186|pages=1256–1258}}</ref> The first two {{mvar|A}} blocks are thus modified to contain the first instance of each value within {{mvar|A}}, with the original contents of those blocks shifted over if necessary. The remaining {{mvar|A}} blocks are then inserted into {{mvar|B}} and merged using one of the two buffers as swap space. This process causes the values in that buffer to be rearranged.\n\nOnce every {{mvar|A}} and {{mvar|B}} block of every {{mvar|A}} and {{mvar|B}} subarray have been merged for that level of the merge sort, the values in that buffer must be sorted to restore their original order, so an insertion sort must be applied. The values in the buffers are then redistributed to their first sorted position within the array. This process repeats for each level of the outer bottom-up merge sort, at which point the array will have been stably sorted.\n\n== Algorithm ==\nThe following operators are used in the code examples:\n{| class=\"wikitable\"\n|-\n! &#124;\n| [[Bitwise operation#OR|bitwise OR]]\n|-\n! >>\n| [[Bitwise operation#Logical shift|shift right]]\n|-\n! %\n| [[Modulo operation|modulo]]\n|-\n! ++ and +=\n| [[Increment and decrement operators|increment]]\n|-\n! [''x'', ''y'')\n| [[Interval (mathematics)#Excluding the endpoints|range]] from ≥ ''x'' and &lt; ''y''\n|-\n! &#124;range&#124;\n| range.end - range.start\n|-\n! array[i]\n| ''i''-th item of ''array''\n|}\n\nAdditionally, block sort relies on the following operations as part of its overall algorithm:\n* '''[[Swap (computer science)|Swap]]''': exchange the positions of two values in an array.\n* '''Block swap''': exchange a range of values within an array with values in a different range of the array.\n* '''[[Binary search algorithm|Binary search]]''': assuming the array is sorted, check the middle value of the current search range, then if the value is lesser check the lower range, and if the value is greater check the upper range. Block sort uses two variants: one which finds the ''first'' position to insert a value in the sorted array, and one which finds the ''last'' position.\n* '''[[Linear search]]''': find a particular value in an array by checking every single element in order, until it is found.\n* '''[[Insertion sort]]''': for each item in the array, loop backward and find where it needs to be inserted, then insert it at that position.\n* '''Array rotation''': move the items in an array to the left or right by some number of spaces, with values on the edges wrapping around to the other side. Rotations can be implemented as three [[In-place algorithm#Examples|reversals]].<ref>{{cite book|last=Bentley|first=Jon|title=[[Programming Pearls]]|edition=2nd|year=2006}}</ref>\n\n    '''Rotate'''(array, amount, range)\n        '''Reverse'''(array, range)\n        '''Reverse'''(array, [range.start, range.start + amount))\n        '''Reverse'''(array, [range.start + amount, range.end))\n\n* '''Floor power of two''': [[Floor and ceiling functions|floor]] a value to the next power of two. 63 becomes 32, 64 stays 64, and so forth.<ref name=\"Warren_2013\">{{Cite book |title=Hacker's Delight |title-link=Hacker's Delight |first=Henry S. |last=Warren Jr. |date=2013 |orig-year=2002 |edition=2 |publisher=[[Addison Wesley]] - [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8 |id=0-321-84268-5}}</ref>\n\n    '''FloorPowerOfTwo'''(x)\n        x = x | (x >> 1)\n        x = x | (x >> 2)\n        x = x | (x >> 4)\n        x = x | (x >> 8)\n        x = x | (x >> 16)\n        '''if''' (this is a [[64-bit computing|64-bit]] system)\n            x = x | (x >> 32)\n        return x - (x >> 1)\n\n=== Outer Loop ===\nAs previously stated, the outer loop of a block sort is identical to a bottom-up merge sort. However, it benefits from the variant that ensures each A and B subarray are the same size to within one item:\n\n    '''BlockSort'''(array)\n        power_of_two = '''FloorPowerOfTwo'''(array.size)\n        scale = array.size/power_of_two <span style=\"color:green;\">// 1.0 ≤ scale < 2.0</span>\n       \n        <span style=\"color:green;\">// insertion sort 16–31 items at a time</span>\n        '''for''' (merge = 0; merge < power_of_two; merge += 16)\n            start = merge * scale\n            end = start + 16 * scale\n            '''InsertionSort'''(array, [start, end))\n       \n        '''for''' (length = 16; length < power_of_two; length += length)\n            '''for''' (merge = 0; merge < power_of_two; merge += length * 2)\n                start = merge * scale\n                mid = (merge + length) * scale\n                end = (merge + length * 2) * scale\n               \n                '''if''' (array[end − 1] < array[start])\n                    <span style=\"color:green;\">// the two ranges are in reverse order, so a rotation is enough to merge them</span>\n                    '''Rotate'''(array, mid − start, [start, end))\n                '''else if''' (array[mid − 1] > array[mid])\n                    '''Merge'''(array, A = [start, mid), B = [mid, end))\n                <span style=\"color:green;\">// else the ranges are already correctly ordered</span>\n\n[[Fixed-point arithmetic|Fixed-point math]] may also be used, by representing the scale factor as a fraction <code>integer_part + numerator/denominator</code>:\n\n    power_of_two = '''FloorPowerOfTwo'''(array.size)\n    denominator = power_of_two/16\n    numerator_step = array.size % denominator\n    integer_step = '''floor'''(array.size/denominator)\n   \n    <span style=\"color:green;\">// insertion sort 16&ndash;31 items at a time</span>\n   \n    '''while''' (integer_step < array.size)\n        integer_part = numerator = 0\n        '''while''' (integer_part < array.size)\n            <span style=\"color:green;\">// get the ranges for A and B</span>\n            start = integer_part\n           \n            integer_part += integer_step\n            numerator += numerator_step\n            '''if''' (numerator ≥ denominator)\n                numerator &minus;= denominator\n                integer_part++\n           \n            mid = integer_part\n           \n            integer_part += integer_step\n            numerator += numerator_step\n            '''if''' (numerator ≥ denominator)\n                numerator −= denominator\n                integer_part++\n           \n            end = integer_part\n           \n            '''if''' (array[end − 1] < array[start])\n                '''Rotate'''(array, mid − start, [start, end))\n            '''else if''' (array[mid − 1] > array[mid])\n                '''Merge'''(array, A = [start, mid), B = [mid, end))\n       \n        integer_step += integer_step\n        numerator_step += numerator_step\n        '''if''' (numerator_step ≥ denominator)\n            numerator_step −= denominator\n            integer_step++\n\n=== Extract Buffers ===\n[[File:Buffer extraction for block sort.gif|thumb|right|The buffer extraction process for block sort.]]\n\nThe two internal buffers needed for each level of the merge step are created by moving the first 2{{sqrt|A}} instances of each value within an A subarray to the start of A. First it iterates over the elements in A and counts off the unique values it needs, then it applies array rotations to move those unique values to the start.<ref name=\"pardo\">{{cite book|last=Pardo|first=Luis Trabb|title=Stable Sorting and Merging with Optimal Space and Time Bounds|series=[[SIAM Journal on Computing]]|volume=6|year=1977|pages=351–372}}</ref> If A did not contain enough unique values to fill the two buffers (of size {{sqrt|A}} each), B can be used just as well. In this case it moves the ''last'' instance of each value to the ''end'' of B, with that part of B not being included during the merges.\n\n    '''while''' (integer_step < array.size)\n        block_size = {{sqrt|integer_step}}\n        buffer_size = integer_step/block_size + 1\n        <span style=\"color:green;\">[extract two buffers of size 'buffer_size' each]</span>\n\nIf B does not contain enough unique values either, it pulls out the largest number of unique values it ''could'' find, then adjusts the size of the A and B blocks such that the number of resulting A blocks is less than or equal to the number of unique items pulled out for the buffer. Only one buffer will be used in this case – the second buffer won't exist.\n\n    buffer_size = <span style=\"color:green;\">[number of unique values found]</span>\n    block_size = integer_step/buffer_size + 1\n   \n    integer_part = numerator = 0\n    '''while''' (integer_part < array.size)\n        <span style=\"color:green;\">[get the ranges for A and B]</span>\n        <span style=\"color:green;\">[adjust A and B to not include the ranges used by the buffers]</span>\n\n=== Tag A Blocks ===\n[[File:Block sort tagging A blocks.gif|thumb|right|Tagging the A blocks using values from the first internal buffer. Note that the first A block and last B block are unevenly sized.]]\nOnce the one or two internal buffers have been created, it begins merging each A and B subarray for this level of the merge sort. To do so, it divides each A and B subarray into evenly sized blocks of the size calculated in the previous step, where the first A block and last B block are unevenly sized if needed. It then loops over each of the evenly sized A blocks and swaps the second value with a corresponding value from the first of the two internal buffers. This is known as ''tagging'' the blocks.\n\n    <span style=\"color:green;\">// blockA is the range of the remaining A blocks,</span>\n    <span style=\"color:green;\">// and firstA is the unevenly sized first A block</span>\n    blockA = [A.start, A.end)\n    firstA = [A.start, A.start + |blockA| % block_size)\n   \n    <span style=\"color:green;\">// swap the second value of each A block with the value in buffer1</span>\n    '''for''' (index = 0, indexA = firstA.end + 1; indexA < blockA.end; indexA += block_size)\n        '''Swap'''(array[buffer1.start + index], array[indexA])\n        index++\n   \n    lastA = firstA\n    blockB = [B.start, B.start + '''minimum'''(block_size, |B|))\n    blockA.start += |firstA|\n\n=== Roll and Drop ===\n[[File:Block sort roll and drop.gif|thumb|right|Two A blocks rolling through the B blocks. Once the first A block is dropped behind, the unevenly sized A block is locally merged with the B values that follow it.]]\nAfter defining and tagging the A blocks in this manner, the A blocks are ''rolled'' through the B blocks by block swapping the first evenly sized A block with the next B block. This process repeats until the first value of the A block with the smallest tag value is less than or equal to the last value of the B block that was just swapped with an A block.\n\nAt that point, the minimum A block (the A block with the smallest tag value) is swapped to the start of the rolling A blocks and the tagged value is restored with its original value from the first buffer. This is known as ''dropping'' a block behind, as it will no longer be rolled along with the remaining A blocks. That A block is then inserted into the previous B block, first by using a binary search on B to find the index where the first value of A is less than or equal to the value at that index of B, and then by rotating A into B at that index.\n\n    minA = blockA.start\n    indexA = 0\n   \n    '''while''' (true)\n        <span style=\"color:green;\">// if there's a previous B block and the first value of the minimum A block is ≤</span>\n        <span style=\"color:green;\">// the last value of the previous B block, then drop that minimum A block behind.</span>\n        <span style=\"color:green;\">// or if there are no B blocks left then keep dropping the remaining A blocks.</span>\n        '''if''' ((|lastB| > 0 '''and''' array[lastB.end - 1] ≥ array[minA]) '''or''' |blockB| = 0)\n            <span style=\"color:green;\">// figure out where to split the previous B block, and rotate it at the split</span>\n            B_split = '''BinaryFirst'''(array, array[minA], lastB)\n            B_remaining = lastB.end - B_split\n           \n            <span style=\"color:green;\">// swap the minimum A block to the beginning of the rolling A blocks</span>\n            '''BlockSwap'''(array, blockA.start, minA, block_size)\n           \n            <span style=\"color:green;\">// restore the second value for the A block</span>\n            '''Swap'''(array[blockA.start + 1], array[buffer1.start + indexA])\n            indexA++\n           \n            <span style=\"color:green;\">// rotate the A block into the previous B block</span>\n            '''Rotate'''(array, blockA.start - B_split, [B_split, blockA.start + block_size))\n           \n            <span style=\"color:green;\">// locally merge the previous A block with the B values that follow it,</span>\n            <span style=\"color:green;\">// using the second internal buffer as swap space (if it exists)</span>\n            '''if''' (|buffer2| > 0)\n                '''MergeInternal'''(array, lastA, [lastA.end, B_split), buffer2)\n            '''else'''\n                '''MergeInPlace'''(array, lastA, [lastA.end, B_split))\n           \n            <span style=\"color:green;\">// update the range for the remaining A blocks,</span>\n            <span style=\"color:green;\">// and the range remaining from the B block after it was split</span>\n            lastA = [blockA.start - B_remaining, blockA.start - B_remaining + block_size)\n            lastB = [lastA.end, lastA.end + B_remaining)\n           \n            <span style=\"color:green;\">// if there are no more A blocks remaining, this step is finished</span>\n            blockA.start = blockA.start + block_size\n            '''if''' (|blockA| = 0)\n                '''break'''\n           \n            minA = <span style=\"color:green;\">[new minimum A block]</span> ''(see below)''\n        '''else if''' (|blockB| < block_size)\n            <span style=\"color:green;\">// move the last B block, which is unevenly sized,</span>\n            <span style=\"color:green;\">// to before the remaining A blocks, by using a rotation</span>\n            '''Rotate'''(array, blockB.start - blockA.start, [blockA.start, blockB.end))\n           \n            lastB = [blockA.start, blockA.start + |blockB|)\n            blockA.start += |blockB|\n            blockA.end += |blockB|\n            minA += |blockB|\n            blockB.end = blockB.start\n        '''else'''\n            <span style=\"color:green;\">// roll the leftmost A block to the end by swapping it with the next B block</span>\n            '''BlockSwap'''(array, blockA.start, blockB.start, block_size)\n            lastB = [blockA.start, blockA.start + block_size)\n            '''if''' (minA = blockA.start)\n                minA = blockA.end\n           \n            blockA.start += block_size\n            blockA.end += block_size\n            blockB.start += block_size\n           \n            <span style=\"color:green;\">// this is equivalent to '''minimum'''(blockB.end + block_size, B.end),</span>\n            <span style=\"color:green;\">// but that has the potential to [[Integer overflow|overflow]]</span>\n            '''if''' (blockB.end > B.end - block_size)\n                blockB.end = B.end\n            '''else'''\n                blockB.end += block_size\n   \n    <span style=\"color:green;\">// merge the last A block with the remaining B values</span>\n    '''if''' (|buffer2| > 0)\n        '''MergeInternal'''(array, lastA, [lastA.end, B.end), buffer2)\n    '''else'''\n        '''MergeInPlace'''(array, lastA, [lastA.end, B.end))\n\nOne optimization that can be applied during this step is the ''floating-hole technique''.<ref name=\"geffert\">{{cite journal |last1=Geffert |first1=Viliam |last2=Katajainen |first2=Jykri |last3=Pasanen |first3=Tomi |title=Asymptotically efficient in-place merging |journal=[[Theoretical Computer Science (journal)|Theoretical Computer Science]] |volume=237 |issue=1–2 |date=April 2000 |pages=159–181 |doi=10.1016/S0304-3975(98)00162-5 |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.5750}}</ref> When the minimum A block is dropped behind and needs to be rotated into the previous B block, after which its contents are swapped into the second internal buffer for the local merges, it would be faster to swap the A block to the buffer beforehand, and to take advantage of the fact that the contents of that buffer do not need to retain any order. So rather than rotating the second buffer (which used to be the A block before the block swap) into the previous B block at position ''index'', the values in the B block after ''index'' can simply be block swapped with the last items of the buffer.\n\nThe ''floating hole'' in this case refers to the contents of the second internal buffer ''floating'' around the array, and acting as a ''hole'' in the sense that the items do not need to retain their order.\n\n=== Local Merges ===\nOnce the A block has been rotated into the B block, the previous A block is then merged with the B values that follow it, using the second buffer as swap space. When the first A block is dropped behind this refers to the unevenly sized A block at the start, when the second A block is dropped behind it means the first A block, and so forth.\n\n    '''MergeInternal'''(array, A, B, buffer)\n        <span style=\"color:green;\">// block swap the values in A with those in 'buffer'</span>\n        '''BlockSwap'''(array, A.start, buffer.start, |A|)\n       \n        A_count = 0, B_count = 0, insert = 0\n        '''while''' (A_count < |A| '''and''' B_count < |B|)\n            '''if''' (array[buffer.start + A_count] ≤ array[B.start + B_count])\n                '''Swap'''(array[A.start + insert], array[buffer.start + A_count])\n                A_count++\n            '''else'''\n                '''Swap'''(array[A.start + insert], array[B.start + B_count])\n                B_count++\n            insert++\n       \n        <span style=\"color:green;\">// block swap the remaining part of the buffer with the remaining part of the array</span>\n        '''BlockSwap'''(array, buffer.start + A_count, A.start + insert, |A| - A_count)\n\nIf the second buffer does not exist, a strictly in-place merge operation must be performed, such as a rotation-based version of the Hwang and Lin algorithm,<ref name=\"geffert\" /><ref>{{cite book|last1=Hwang|first1=F. K.|last2=Lin|first2=S.|title=A Simple Algorithm for Merging Two Disjoint Linearly Ordered Sets|series=[[SIAM Journal on Computing]]|volume=1|year=1972|pages=31–39|doi=10.1137/0201004|issn=0097-5397}}</ref> the Dudzinski and Dydek algorithm,<ref name=\"dydek\">{{cite book|last1=Dudzinski|first1=Krzysztof|last2=Dydek|first2=Andrzej|title=On a Stable Storage Merging Algorithm|series=[[Information Processing Letters]]|volume=12|year=1981|pages=5–8}}</ref> or a repeated binary search and rotate.\n\n    '''MergeInPlace'''(array, A, B)\n        '''while''' (|A| > 0 '''and''' |B| > 0)\n            <span style=\"color:green;\">// find the first place in B where the first item in A needs to be inserted</span>\n            mid = '''BinaryFirst'''(array, array[A.start], B)\n           \n            <span style=\"color:green;\">// rotate A into place</span>\n            amount = mid - A.end\n            '''Rotate'''(array, amount, [A.start, mid))\n           \n            <span style=\"color:green;\">// calculate the new A and B ranges</span>\n            B = [mid, B.end)\n            A = [A.start + amount, mid)\n            A.start = '''BinaryLast'''(array, array[A.start], A)\n\nAfter dropping the minimum A block and merging the previous A block with the B values that follow it, the new minimum A block must be found within the blocks that are still being rolled through the array. This is handled by running a linear search through those A blocks and comparing the tag values to find the smallest one.\n\n    minA = blockA.start\n    '''for''' (findA = minA + block_size; findA < blockA.end - 1; findA += block_size)\n        '''if''' (array[findA + 1] < array[minA + 1])\n            minA = findA\n\nThese remaining A blocks then continue rolling through the array and being dropped and inserted where they belong. This process repeats until all of the A blocks have been dropped and rotated into the previous B block.\n\nOnce the last remaining A block has been dropped behind and inserted into B where it belongs, it should be merged with the remaining B values that follow it. This completes the merge process for that particular pair of A and B subarrays. However, it must then repeat the process for the remaining A and B subarrays for the current level of the merge sort.\n\nNote that the internal buffers can be reused for every set of A and B subarrays for this level of the merge sort, and do not need to be re-extracted or modified in any way.\n\n=== Redistribute ===\nAfter all of the A and B subarrays have been merged, the one or two internal buffers are still left over. The first internal buffer was used for tagging the A blocks, and its contents are still in the same order as before, but the second internal buffer may have had its contents rearranged when it was used as swap space for the merges. This means the contents of the second buffer will need to be sorted using a different algorithm, such as insertion sort. The two buffers must then be redistributed back into the array using the opposite process that was used to create them.\n\nAfter repeating these steps for every level of the bottom-up merge sort, the block sort is completed.\n\n== Variants ==\nBlock sort works by extracting two internal buffers, breaking A and B subarrays into evenly sized blocks, rolling and dropping the A blocks into B (using the first buffer to track the order of the A blocks), locally merging using the second buffer as swap space, sorting the second buffer, and redistributing both buffers. While the steps do not change, these subsystems can vary in their actual implementation.\n\nOne variant of block sort allows it to use any amount of additional memory provided to it, by using this ''external buffer'' for merging an A subarray or A block with B whenever A fits into it. In this situation it would be identical to a merge sort.\n\nGood choices for the buffer size include:\n{| class=\"wikitable\"\n|-\n! Size\n! Notes\n|-\n! (count + 1)/2\n| turns into a full-speed merge sort since all of the A subarrays will fit into it\n|-\n! {{sqrt|(count + 1)/2}} + 1\n| this will be the size of the A blocks at the largest level of merges, so block sort can skip using internal or in-place merges for anything\n|-\n! 512\n| a fixed-size buffer large enough to handle the numerous merges at the smaller levels of the merge sort\n|-\n! 0\n| if the system cannot allocate any extra memory, no memory works well\n|}\n\nRather than tagging the A blocks using the contents of one of the internal buffers, an indirect ''movement-imitation buffer'' can be used instead.<ref name=\"kim2008\" /><ref name=\"symvonis\">{{cite journal |last=Symvonis |first=Antonios |title=Optimal Stable Merging|journal=The Computer Journal |volume=38 |issue=8 |pages=681&ndash;690 |year=1995 |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.6058 |doi=10.1093/comjnl/38.8.681}}</ref> This is an internal buffer defined as ''s1 t s2'', where ''s1'' and ''s2'' are each as large as the number of A and B blocks, and ''t'' contains any values immediately following ''s1'' that are equal to the last value of ''s1'' (thus ensuring that no value in ''s2'' appears in ''s1''). A second internal buffer containing {{sqrt|A}} unique values is still used. The first {{sqrt|A}} values of ''s1'' and ''s2'' are then swapped with each other to encode information into the buffer about which blocks are A blocks and which are B blocks. When an A block at index ''i'' is swapped with a B block at index ''j'' (where the first evenly sized A block is initially at index 0), s1[i] and s1[j] are swapped with s2[i] and s2[j], respectively. This ''imitates the movements'' of the A blocks through B. The unique values in the second buffer are used to determine the original order of the A blocks as they are rolled through the B blocks. Once all of the A blocks have been dropped, the movement-imitation buffer is used to decode whether a given block in the array is an A block or a B block, each A block is rotated into B, and the second internal buffer is used as swap space for the local merges.\n\nThe ''second'' value of each A block doesn't necessarily need to be tagged – the first, last, or any other element could be used instead. However, if the first value is tagged, the values will need to be read from the first internal buffer (where they were swapped) when deciding where to drop the minimum A block.\n\nMany sorting algorithms can be used to sort the contents of the second internal buffer, including unstable sorts like [[quicksort]], since the contents of the buffer are guaranteed to unique. Insertion sort is still recommended, though, for its situational performance and lack of recursion.\n\n== Analysis ==\nBlock sort is a well-defined and testable class of algorithms, with working implementations available as a merge and as a sort.<ref name=\"implementation\">{{cite web|author=Arne Kutzner|title=In-place Merging Algorithm Benchmarking Tool|url=http://ak.hanyang.ac.kr/research/benchmarking-tool/benchmarking-tool.html|accessdate=2014-03-23|deadurl=yes|archiveurl=https://archive.is/20140415030845/http://ak.hanyang.ac.kr/research/benchmarking-tool/benchmarking-tool.html|archivedate=2014-04-15|df=}}</ref><ref name=\"download\">{{cite web|author=Arne Kutzner|title=In-place Merging Algorithm Benchmarking Tool|url=http://itbe.hanyang.ac.kr/research-articles/in-place-merging-algorithm-benchmarking-tool/|accessdate=2016-12-11}}</ref><ref name=\"wikisort\">{{cite web|title=Public domain implementations of block sort for C, C++, and Java|url=https://github.com/BonzaiThePenguin/WikiSort|accessdate=2014-03-23}}</ref> This allows its characteristics to be measured and considered.\n\n=== Complexity ===\nBlock sort begins by insertion sorting groups of 16&ndash;31 items in the array. Insertion sort is an {{math|[[Big O notation#Orders of common functions|''O'']](''n''<sup>2</sup>)}} operation, so this leads to anywhere from {{math|''O''(16<sup>2</sup> × ''n''/16)}} to {{math|''O''(31<sup>2</sup> × ''n''/31)}}, which is {{math|''O''(''n'')}} once the [[Big O notation#Example|constant factors are omitted]]. It must also apply an insertion sort on the second internal buffer after each level of merging is completed. However, as this buffer was limited to {{sqrt|A}} in size, the {{math|''O''({{sqrt|''n''}}<sup>2</sup>)}} operation also ends up being {{math|''O''(''n'')}}.\n\nNext it must extract two internal buffers for each level of the merge sort. It does so by iterating over the items in the A and B subarrays and incrementing a counter whenever the value changes, and upon finding enough values it rotates them to the start of A or the end of B. In the worst case this will end up searching the entire array before finding {{sqrt|A}} non-contiguous unique values, which requires {{math|''O''(''n'')}} comparisons and {{sqrt|A}} rotations for {{sqrt|A}} values. This resolves to {{math|''O''(''n'' + {{sqrt|''n''}} × {{sqrt|''n''}})}}, or {{math|''O''(''n'')}}.\n\nWhen none of the A or B subarrays contained {{sqrt|A}} unique values to create the internal buffers, a normally suboptimal in-place merge operation is performed where it repeatedly binary searches and rotates A into B. However, the known lack of unique values within any of the subarrays places a hard limit on the number of binary searches and rotations that will be performed during this step, which is again {{sqrt|A}} items rotated up to {{sqrt|A}} times, or {{math|''O''(''n'')}}. The size of each block is also adjusted to be smaller in the case where it found {{sqrt|A}} unique values but not 2{{sqrt|A}}, which further limits the number of unique values contained within any A or B block.\n\nTagging the A blocks is performed {{sqrt|A}} times for each A subarray, then the A blocks are rolled through and inserted into the B blocks up to {{sqrt|A}} times. The local merges retain the same {{math|''O''(''n'')}} complexity of a standard merge, albeit with more assignments since the values must be swapped rather than copied. The linear search for finding the new minimum A block iterates over {{sqrt|A}} blocks {{sqrt|A}} times. And the buffer redistribution process is identical to the buffer extraction but in reverse, and therefore has the same {{math|''O''(''n'')}} complexity.\n\nAfter [[Big O notation#Example|omitting all but the highest complexity]] and considering that there are ''log n'' levels in the outer merge loop, this leads to a final asymptotic complexity of {{math|''O''(''n'' log ''n'')}} for the worst and average cases. For the best case, where the data is already in order, the merge step performs {{math|''n''/16}} comparisons for the first level, then {{math|''n''/32}}, {{math|''n''/64}}, {{math|''n''/128}}, etc. This is a [[1/2 + 1/4 + 1/8 + 1/16 + · · ·|well-known mathematical series]] which resolves to {{math|''O''(''n'')}}.\n\n=== Memory ===\nAs block sort is [[Recursion (computer science)|non-recursive]] and does not require the use of [[Memory management#Dynamic memory allocation|dynamic allocations]], this leads to constant [[Stack (abstract data type)|stack]] and heap space. It uses O(1) auxiliary memory in a [[transdichotomous model]], which accepts that the O(log ''n'') bits needed to keep track of the ranges for A and B cannot be any greater than 32 or 64 on 32-bit or 64-bit computing systems, respectively, and therefore simplifies to O(1) space for any array that can feasibly be allocated.\n\n=== Stability ===\nAlthough items in the array are moved out of order during a block sort, each operation is fully reversible and will have restored the original order of equivalent items by its completion.\n\nStability requires the first instance of each value in an array before sorting to still be the first instance of that value after sorting. Block sort moves these first instances to the start of the array to create the two internal buffers, but when all of the merges are completed for the current level of the block sort, those values are distributed back to the first sorted position within the array. This maintains stability.\n\nBefore rolling the A blocks through the B blocks, each A block has its second value swapped with a value from the first buffer. At that point the A blocks are moved out of order to roll through the B blocks. However, once it finds where it should insert the smallest A block into the previous B block, that smallest A block is moved back to the start of the A blocks and its second value is restored. By the time all of the A blocks have been inserted, the A blocks will be in order again and the first buffer will contain its original values in the original order.\n\nUsing the second buffer as swap space when merging an A block with some B values causes the contents of that buffer to be rearranged. However, as the algorithm already ensured the buffer only contains unique values, sorting the contents of the buffer is sufficient to restore their original stable order.\n\n=== Adaptivity ===\nBlock sort is an [[adaptive sort]] on two levels: first, it skips merging A and B subarrays that are already in order. Next, when A and B need to be merged and are broken into evenly sized blocks, the A blocks are only rolled through B as far as is necessary, and each block is only merged with the B values immediately following it. The more ordered the data originally was, the fewer B values there will be that need to be merged into A.\n\n== Advantages ==\nBlock sort is a stable sort that does not require additional memory, which is useful in cases where there is not enough free memory to allocate the O(n) buffer. When using the ''external buffer'' variant of block sort, it can scale from using O(n) memory to progressively smaller buffers as needed, and will still work efficiently within those constraints.\n\n== Disadvantages ==\nBlock sort does not exploit sorted ranges of data on as fine a level as some other algorithms, such as [[Timsort]].<ref name=\"tim\">{{cite web|author=Tim Peters|title=Re: WikiSort|url=http://www.gossamer-threads.com/lists/python/dev/1126593?do=post_view_threaded#1126593|accessdate=2014-03-23}}</ref> It only checks for these sorted ranges at the two predefined levels: the A and B subarrays, and the A and B blocks. It is also harder to implement and parallelize compared to a merge sort.\n\n== References ==\n{{Reflist|30em}}\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Bogosort",
      "url": "https://en.wikipedia.org/wiki/Bogosort",
      "text": "{{Infobox Algorithm\n|image =\n|class=[[Sorting]]\n|data=[[Array data structure|Array]]\n|time=Unbounded (randomized version),<ref name=\"Fun07\"/> {{math|''O''((''n''+1)!)}} (deterministic version)\n|average-time={{math|''O''((''n''+1)!)}}<ref name=\"Fun07\"/>\n|best-time={{math|''O''(''n'')}}<ref name=\"Fun07\"/>\n|space={{math|''O''(''n'')}}\n|optimal=No\n}}\n\nIn [[computer science]], '''bogosort'''<ref name=\"Fun07\"/><ref name=\"KSFS\">{{citation\n | last1 = Kiselyov\n | first1 = Oleg\n | last2 = Shan\n | first2 = Chung-chieh\n | last3 = Friedman\n | first3 = Daniel P.\n | last4 = Sabry\n | first4 = Amr\n | contribution = Backtracking, interleaving, and terminating monad transformers: (functional pearl)\n | doi = 10.1145/1086365.1086390\n | pages = 192–203\n | series = SIGPLAN Notices\n | title = Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming (ICFP '05)\n | url = http://www.dicta.org.uk/programming/LogicT.pdf\n | year = 2005\n | access-date = 22 June 2011\n | archive-url = https://web.archive.org/web/20120326002456/http://www.dicta.org.uk/programming/LogicT.pdf\n | archive-date = 26 March 2012\n | dead-url = yes\n }}</ref> (also known as '''permutation sort''', '''stupid sort''',<ref>E. S. Raymond. \"bogo-sort\". ''The New Hacker’s Dictionary''. MIT Press, 1996.</ref> '''slowsort''',<ref name=\"Naish86\">{{citation\n | last = Naish | first = Lee\n | contribution = Negation and quantifiers in NU-Prolog\n | doi = 10.1007/3-540-16492-8_111\n | pages = 624–634\n | publisher = Springer-Verlag\n | series = [[Lecture Notes in Computer Science]]\n | title = Proceedings of the Third International Conference on Logic Programming\n | volume = 225\n | year = 1986}}.</ref> '''shotgun sort''' or '''monkey sort''') is a highly ineffective [[sorting]] algorithm based on the [[generate and test]] paradigm. The function successively generates [[permutation]]s of its input until it finds one that is sorted. It is not useful for sorting, but may be used for educational purposes, to contrast it with more efficient algorithms.\n\nTwo versions of this algorithm exist: a deterministic version that enumerates all permutations until it hits a sorted one,<ref name=\"KSFS\"/><ref name=\"Naish86\"/> and a [[randomized algorithm|randomized]] version that randomly permutes its input. An analogy for the working of the latter version is to sort a [[deck of cards]] by throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name is a portmanteau of the words ''bogus'' and ''sort''.<ref>{{cite web|title=Bogosort|url=http://www.catb.org/jargon/html/B/bogo-sort.html|work=The Jargon File 4.4.8|accessdate=11 April 2013|year=2003}}</ref>\n\n==Description of the algorithm ==\nThe following is a description of the randomized algorithm in [[pseudocode]]:\n\n '''while not''' isInOrder(deck):\n     shuffle(deck)\n\nHere is the above pseudocode re-written in [[Python (programming language)|Python 3]]:\n<syntaxhighlight lang=\"python3\" line=\"1\">\nimport random\n\ndef is_sorted(data):\n    for i in range(len(data) - 1):\n        if data[i] > data[i + 1]:\n            return False\n    return True\n\ndef bogosort(data):\n    while not is_sorted(data):\n        random.shuffle(data)\n    return data\n\n</syntaxhighlight>\nThis code assumes that data is a simple, mutable datatype—like Python's built-in {{code|list}}—whose elements can be compared without issue.\n\nHere is an example with shuffle in [[Standard ML]]:\n\n<syntaxhighlight lang=\"sml\">\n val _ = load \"Random\";\n load \"Int\";\n val rng = Random.newgen ();\n\n fun select (y::xs, 0) = (y, xs)\n   | select (x::xs, i) = let val (y, xs') = select (xs, i-1) in (y, x::xs') end\n   | select (_, i) = raise Fail (\"Short by \" ^ Int.toString i ^ \" elements.\");\n\n (* Recreates a list in random order by removing elements in random positions *)\n fun shuffle xs =\n    let fun rtake [] _ = []\n          | rtake ys max =\n             let val (y, ys') = select (ys, Random.range (0, max) rng)\n             in y :: rtake ys' (max-1)\n             end\n    in rtake xs (length xs) end;\n\n fun bogosort xs comp = \n let fun isSorted (x::y::xs) comp = comp(x,y) <> GREATER andalso isSorted (y::xs) comp\n       | isSorted _ comp = true;\n     val a = ref xs;\n in while(not(isSorted (!a) comp)) do (\n  a := shuffle (!a)\n  ); (!a) end;\n</syntaxhighlight>\n\n==Running time and termination==\nIf all elements to be sorted are distinct, the expected number of comparisons performed in the average case by randomized bogosort is [[Asymptotic analysis|asymptotically equivalent to]] <math>(e-1) n!</math>, and the expected number of swaps in the average case equals <math>(n-1) n!</math>.<ref name=\"Fun07\">{{citation\n | last1 = Gruber | first1 = H.\n | last2 = Holzer | first2 = M.\n | last3 = Ruepp | first3 = O.\n | contribution = Sorting the slow way: an analysis of perversely awful randomized sorting algorithms\n | doi = 10.1007/978-3-540-72914-3_17\n | pages = 183–197\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = 4th International Conference on Fun with Algorithms, Castiglioncello, Italy, 2007\n | url = http://www.hermann-gruber.com/pdf/fun07-final.pdf\n | volume = 4475}}.</ref> The expected number of swaps grows faster than the expected number of comparisons, because if the elements are not in order, this will usually be discovered after only a few comparisons, no matter how many elements there are; but the work of shuffling the collection is proportional to its size. In the worst case, the number of comparisons and swaps are both unbounded, for the same reason that a tossed coin might turn up heads any number of times in a row.\n\nThe best case occurs if the list as given is already sorted; in this case the expected number of comparisons is <math>n-1</math>, and no swaps at all are carried out.<ref name=\"Fun07\"/>\n\nFor any collection of fixed size, the expected running time of the algorithm is finite for much the same reason that the [[infinite monkey theorem]] holds: there is some probability of getting the right permutation, so given an unbounded number of tries it will [[almost surely]] eventually be chosen.\n\n==Related algorithms==\n;Gorosort: is a sorting algorithm introduced in the 2011 [[Google Code Jam]].<ref>[https://code.google.com/codejam/contest/dashboard?c=975485#s=p3 Google Code Jam 2011, Qualification Rounds, Problem D]</ref> As long as the list is not in order, a subset of all elements is randomly permuted. If this subset is optimally chosen each time this is performed, the [[expected value]] of the total number of times this operation needs to be done is equal to the number of misplaced elements.\n;Bogobogosort: is an algorithm that was designed not to succeed before the [[heat death of the universe]] on any sizable list. It works by recursively calling itself with smaller and smaller copies of the beginning of the list to see if they are sorted.  The best case is a single element, which is always sorted.  For other cases, it compares the last element to the maximum element from the previous elements in the list.  If the last element is greater or equal, it checks if the order of the copy matches the previous version, copies back if not, and returns.  Otherwise, it reshuffles the current copy of the list and goes back to its recursive check.<ref>[http://www.dangermouse.net/esoteric/bogobogosort.html Bogobogosort]</ref>\n<!-- Bogobogosort needs to be implemented recursively, as in the iterative version it is too easy to optimize out the second bogo. -->\n;Bozosort: is another sorting algorithm based on random numbers. If the list is not in order, it picks two items at random and swaps them, then checks to see if the list is sorted. The running time analysis of a bozosort is more difficult, but some estimates are found in H. Gruber's analysis of \"perversely awful\" randomized sorting algorithms.<ref name=\"Fun07\"/> O(n!) is found to be the expected average case.\n<!--It also faces the same pseudo-random problems as bogosort—it may never terminate in a real-world implementation.-->\n;Worstsort:is a pessimal sorting algorithm that is guaranteed to complete in finite time; however, there is no computable limit to the inefficiency of the sorting algorithm, and therefore it is more pessimal than the other algorithms described herein. The <math>\\text{worstsort}</math> algorithm is based on a bad sorting algorithm, <math>\\text{badsort}</math>. The badsort algorithm accepts two parameters: <math>L</math>, which is the list to be sorted, and <math>k</math>, which is a recursion depth. At recursion level <math>k = 0</math>, <math>\\text{badsort}</math> merely uses a common sorting algorithm, such as [[Bubble sort|bubblesort]], to sort its inputs and return the sorted list. That is to say, <math>\\text{badsort}(L,0) = \\text{bubblesort}(L)</math>. Therefore, badsort's time complexity is <math>O\\left(n^2\\right)</math> if <math>k = 0</math>. However, for any <math>k > 0</math>, <math>\\text{badsort}(L,k)</math> first generates <math>P</math>, the list of all permutations of <math>L</math>. Then, <math>\\text{badsort}</math> calculates <math>\\text{badsort}(P, k-1)</math>, and returns the first element of the sorted <math>P</math>.  To make <math>\\text{worstsort}</math> truly pessimal, <math>k</math> may be assigned to the value of a computable increasing function such as <math>f\\colon\\mathbb{N} \\to \\mathbb{N}</math> (e.g. <math>f(n) = A(n,n)</math>, where <math>A</math> is [[Ackermann's function]]).  Ergo, to sort a list arbitrarily badly, you would execute <math>\\text{worstsort}(L,f) = \\text{badsort}(L,f(\\text{length}(L)))</math>, where <math>\\text{length}(L)</math> = number of elements in <math>L</math>. The resulting algorithm has complexity <math>\\Omega\\left(\\left(n!^{(f(n))}\\right)^2\\right)</math>, where <math>n!^{(m)} = (\\dotso((n!)!)!\\dotso)!</math> = factorial of <math>n</math> iterated <math>m</math> times. This algorithm can be made as inefficient as we wish by picking a fast enough growing function <math>f</math>.<ref>https://www.arxiv.org/abs/1406.1077</ref>\n\n==Analysis of Runtime==\n\nHere is some [[Python (programming language)|Python]] code to efficiently test the average complexity of Bogosort.\n[[File:ExperimentalBogosort.png|thumb|Experimental Runtime of Bogosort]]\n<syntaxhighlight lang=\"python\">\n#!/usr/bin/env python3.6\n\nimport sys\nimport time\nimport random\nfrom multiprocessing import Process, Queue\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\nfrom tqdm import tqdm\n\nWORKER_COUNT = 8\nTRIAL_COUNT = 10\n\n\ndef is_sorted(some_list):\n    for x, y in zip(some_list[:-1], some_list[1:]):\n        if x > y:\n            return False\n    return True\n\n\nclass Sorter(Process):\n    def __init__(self, array, output, counts, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.array = array\n        self.length = len(array)\n        self.output = output\n        self.count = 0\n        self.counts = counts\n\n    def run(self):\n        while True:\n            if self.output.empty():\n                new_list = random.sample(self.array, k=len(self.array))\n                self.count += self.length  # to check all items\n                if is_sorted(new_list):\n                    self.counts.put(self.count)\n                    self.output.put(new_list)\n                    break\n            else:\n                self.counts.put(self.count)\n                break\n                \n\ndef run_trial(list_len):\n    trials = {\"time\": [], \"cycles\": []}\n    \n    for _ in tqdm(range(TRIAL_COUNT)):\n        start_time = time.time()\n        array = random.sample(list(range(list_len)), k=list_len)\n\n        workers = []\n        output = Queue()\n        counts = Queue()\n        for _ in range(WORKER_COUNT):\n            w = Sorter(array, output, counts)\n            workers.append(w)\n            w.start()\n\n        _result = output.get()\n\n        total_count = 0\n        for _ in range(WORKER_COUNT):\n            total_count += counts.get()\n\n        for _ in range(WORKER_COUNT):\n            output.put(\"DEATH\")\n\n        for w in workers:\n            w.join()\n\n        end_time = time.time()\n        trials[\"time\"].append(end_time - start_time)\n        trials[\"cycles\"].append(total_count)\n\n    return trials\n\n\ndef plot_chart(list_lengths, results):\n    # init chart\n    fig, axarr = plt.subplots(2, 1, figsize=(8, 6))\n\n    # Average time graph\n    # plot runtime to the graph\n    for i, (length, trial) in enumerate(zip(list_lengths, results)):\n        trial_time = np.ones(TRIAL_COUNT) * length\n        axarr[0].plot(trial_time, np.log(trial[\"time\"]), \"rx\", alpha=0.4)\n\n    # plot average result\n    avg_result = [np.log(sum(t[\"time\"]) / len(t[\"time\"])) for t in results]\n    axarr[0].plot(list_lengths, avg_result, label=\"Average Result\")\n\n    # chart labels\n    axarr[0].legend(loc=0)\n    axarr[0].set_xlabel(\"Length of Initial List\")\n    axarr[0].set_ylabel(\"Average Time Elapsed - ln(seconds)\")\n\n    # average operation graph\n    # plot cycles to graph\n    for i, (length, trial) in enumerate(zip(list_lengths, results)):\n        trial_ops = np.ones(TRIAL_COUNT) * length\n        axarr[1].plot(trial_ops, np.log(trial[\"cycles\"]), \"rx\", alpha=0.4)\n\n    # plot average result\n    avg_result = [np.log(sum(t[\"cycles\"]) / len(t[\"cycles\"])) for t in results]\n    axarr[1].plot(list_lengths, avg_result, label=\"Average Result\")\n\n    # plot n . n!\n    n_dot = np.log([n * factorial(n) for n in list_lengths])\n    axarr[1].plot(list_lengths, n_dot, label=r\"$n \\cdot n!$\",)\n\n    # chart labels\n    axarr[1].legend(loc=0)\n    axarr[1].set_xlabel(\"Length of Initial List\")\n    axarr[1].set_ylabel(\"Average Time Elapsed - ln(Operations)\")\n\n    # Headings and set layout\n    fig.suptitle(\"Parallel Bogosort\")\n    plt.tight_layout()\n\n    # Save plot\n    plt.savefig(\"bogosort.png\")\n    \n    \ndef main():\n    list_lengths = range(2, 10)  # random length for the unsorted lists\n    trial_results = []\n\n    # run trials and add results to array\n    for list_len in list_lengths:\n        trial_info = run_trial(list_len)\n        trial_results.append(trial_info)\n\n    # plot info to chart and save as png\n    plot_chart(list_lengths, trial_results)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</syntaxhighlight>\n\n==See also==\n* [[Las Vegas algorithm]]\n* [[Stooge sort]]\n\n==References==\n<references/>\n\n==External links==\n{{wikibooks|Algorithm Implementation|Sorting/Bogosort|Bogosort}}\n* [[c2:BogoSort|BogoSort]] on [[WikiWikiWeb]]\n* [https://web.archive.org/web/20131210012102/http://richardhartersworld.com/cri_d/cri/2001/badsort.html Inefficient sort algorithms]\n* [http://www.lysator.liu.se/~qha/bogosort/ Bogosort]: an implementation that runs on [[Unix-like]] systems, similar to the standard [[sort (Unix)|sort]] program.\n* [https://archive.is/20130103015524/http://github.com/versesane/algorithms-and-data-structures-in-c/tree/master/bogosort.c Bogosort] and [http://libjmmcg.cvs.sourceforge.net/viewvc/libjmmcg/libjmmcg/core/bogo_sort.hpp jmmcg::bogosort]: Simple, yet perverse, C++ implementations of the bogosort algorithm.\n* [https://github.com/spugachev/random-sort Bogosort NPM package]: bogosort implementation for Node.js ecosystem.\n* Max Sherman [https://sites.math.washington.edu/~morrow/336_13/papers/max.pdf Bogo-sort is Sort of Slow], June 2013\n{{sorting}}\n{{Use dmy dates|date=June 2011}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Computer humor]]"
    },
    {
      "title": "Bubble sort",
      "url": "https://en.wikipedia.org/wiki/Bubble_sort",
      "text": "{{Short description|Simple comparison sorting algorithm}}\n{{refimprove|date=November 2016}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=Bubblesort-edited-color.svg|\n| caption=Static visualization of bubble sort<ref>{{cite web |last=Cortesi |first=Aldo |title=Visualising Sorting Algorithms |url=https://corte.si/posts/code/visualisingsorting/index.html |date=27 April 2007 |accessdate=16 March 2017}}</ref>\n|data=[[Array data structure|Array]]\n|time=<math>O(n^2)</math> comparisons, <math>O(n^2)</math> swaps\n|average-time=<math>O(n^2)</math> comparisons, <math>O(n^2)</math> swaps\n|best-time=<math>O(n^2)</math> comparisons for basic version and <math>O(n)</math> for the optimized one, <math>O(1)</math> swaps\n|space=<math>O(1)</math> auxiliary\n|optimal=No\n}}\n'''Bubble sort''', sometimes referred to as '''sinking sort''', is a simple [[sorting algorithm]] that repeatedly steps through the list, compares adjacent pairs and [[Swap (computer science)|swaps]] them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The algorithm, which is a [[comparison sort]], is named for the way smaller or larger elements \"bubble\" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to [[insertion sort]].<ref name=\"Knuth\"/>  Bubble sort can be practical if the input is in mostly sorted order with some out-of-order elements nearly in position.<!-- sorted scan line display list stepped to next line is usually nearly sorted.\n\nBubble sort may be been found as early as 1956; then referred to as \"sorting by exchange\"<ref name=\"Astrachan\">{{cite web |last1=Astrachan |first1=Owen |title=Bubble Sort: An Archaeological Algorithmic Analysis |url=https://users.cs.duke.edu/~ola/bubble/bubble.html |accessdate=8 February 2019}}</ref> -->\n\n==Analysis==\n[[File:Bubble-sort-example-300px.gif|300px|thumb|right|An example of bubble sort. Starting from the beginning of the list, compare every adjacent pair, swap their position if they are not in the right order (the latter one is smaller than the former one). After each [[iteration]], one less element (the last one) is needed to be compared until there are no more elements left to be compared.]]\n\n=== Performance ===\nBubble sort has a worst-case and average complexity of ''[[big o notation|О]]''(''n''<sup>2</sup>), where ''n'' is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often ''O''(''n''&nbsp;log&nbsp;''n''). Even other ''О''(''n''<sup>2</sup>) sorting algorithms, such as [[insertion sort]], generally run faster than bubble sort, and are no more complex. Therefore, bubble sort is not a practical sorting algorithm.\n\nThe only significant advantage that bubble sort has over most other algorithms, even [[quicksort]], but not [[insertion sort]], is that the ability to detect that the list is sorted efficiently is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort with a swap-checking variable is only ''O''(''n'') (although the most basic implementation is ''O''(''n''<sup>2</sup>) even in the best-case<ref name=\"CLRS\">{{cite book |last1=Cormen |first1=Thomas H. |last2=Leiserson |first2=Charles E. |last3=Rivest |first3=Ronald L. |last4=Stein |first4=Clifford |title=[[Introduction to Algorithms]] |date=2001 |publisher=MIT Press and McGraw-Hill |location=Problem 2-2 |isbn=0-262-03293-7 |page=40 |edition=2nd}}</ref>). By contrast, most other algorithms, even those with better [[average-case complexity]], perform their entire sorting process on the set and thus are more complex. However, not only does [[insertion sort]] share this advantage, but it also performs better on a list that is substantially sorted (having a small number of [[inversion (discrete mathematics)|inversions]]).\n\nBubble sort should be avoided in the case of large collections. It will not be efficient in the case of a reverse-ordered collection.\n\n===Rabbits and turtles===\nThe distance and direction that elements must move during the sort determine bubble sort's performance because elements move in different directions at different speeds. An element that must move toward the end of the list can move quickly because it can take part in successive swaps. For example, the largest element in the list will win every swap, so it moves to its sorted position on the first pass even if it starts near the beginning. On the other hand, an element that must move toward the beginning of the list cannot move faster than one step per pass, so elements move toward the beginning very slowly. If the smallest element is at the end of the list, it will take {{math|''n''−1}} passes to move it to the beginning. This has led to these types of elements being named rabbits and turtles, respectively, after the characters in Aesop's fable of [[The Tortoise and the Hare]].\n\nVarious efforts have been made to eliminate turtles to improve upon the speed of bubble sort. [[Cocktail sort]] is a bi-directional bubble sort that goes from beginning to end, and then reverses itself, going end to beginning. It can move turtles fairly well, but it retains ''[[Big O notation|O(n<sup>2</sup>)]]'' worst-case complexity. [[Comb sort]] compares elements separated by large gaps, and can move turtles extremely quickly before proceeding to smaller and smaller gaps to smooth out the list. Its average speed is comparable to faster algorithms like [[quicksort]].\n\n===Step-by-step example===\nTake an array of numbers \" 5 1 4 2 8\", and sort the array from lowest number to greatest number using bubble sort. In each step, elements written in '''bold''' are being compared. Three passes will be required;\n\n;First Pass\n:( '''5''' '''1''' 4 2 8 ) &rarr; ( '''1''' '''5''' 4 2 8 ), Here, algorithm compares the first two elements, and swaps since 5 > 1.\n:( 1 '''5''' '''4''' 2 8 ) &rarr; ( 1 '''4''' '''5''' 2 8 ), Swap since 5 > 4 \n:( 1 4 '''5''' '''2''' 8 ) &rarr; ( 1 4 '''2''' '''5''' 8 ), Swap since 5 > 2 \n:( 1 4 2 '''5''' '''8''' ) &rarr; ( 1 4 2 '''5''' '''8''' ), Now, since these elements are already in order (8 > 5), algorithm does not swap them.\n;Second Pass\n:( '''1''' '''4''' 2 5 8 ) &rarr; ( '''1''' '''4''' 2 5 8 )\n:( 1 '''4''' '''2''' 5 8 ) &rarr; ( 1 '''2''' '''4''' 5 8 ), Swap since 4 > 2 \n:( 1 2 '''4''' '''5''' 8 ) &rarr; ( 1 2 '''4''' '''5''' 8 )\n:( 1 2 4 '''5''' '''8''' ) &rarr; ( 1 2 4 '''5''' '''8''' )\nNow, the array is already sorted, but the algorithm does not know if it is completed. The algorithm needs one '''whole''' pass without '''any''' swap to know it is sorted.\n;Third Pass\n:( '''1''' '''2''' 4 5 8 ) &rarr; ( '''1''' '''2''' 4 5 8 )\n:( 1 '''2''' '''4''' 5 8 ) &rarr; ( 1 '''2''' '''4''' 5 8 )\n:( 1 2 '''4''' '''5''' 8 ) &rarr; ( 1 2 '''4''' '''5''' 8 )\n:( 1 2 4 '''5''' '''8''' ) &rarr; ( 1 2 4 '''5''' '''8''' )\n\n== Implementation ==\n\n=== Pseudocode implementation ===\nIn [[pseudocode]], the basic algorithm can be expressed as follows (using 0-based arrays)<ref name=\"CLRS\" />:\n<source lang=\"pascal\">\nprocedure bubbleSort(A : list of sortable items )\n    n = length(A)\n    for i = 1 to n-1 inclusive do\n       for j = n-1 downto 0 inclusive do\n            /* if this pair is out of order */\n            if A[j-1] > A[j] then\n                /* swap them */\n                swap( A[j-1], A[j] )\n            end if\n        end for\n    end for\nend procedure\n</source>\n\nA little modification can be done to improve the best-case time complexity from <math>O(n^2)</math> to <math>O(n)</math>, that is, one can create a variable to check if some swap was performed in the current step:<ref name=\"KnuthVolume3\">{{cite book |last1=Knuth |first1=Donald E. |title=The Art of Computer Programming: Volume 3 |date=1998 |publisher=Addison-Wesley Professional |location=Chapter 5, Section 5.2.2 |isbn=978-0201896855 |page=107 |edition=2nd}}</ref>\n\n<source lang=\"pascal\">\nprocedure bubbleSort(A : list of sortable items )\n    n = length(A)\n    repeat\n        swapped = false\n        for i = 1 to n-1 inclusive do\n            /* if this pair is out of order */\n            if A[i-1] > A[i] then\n                /* swap them and remember something changed */\n                swap( A[i-1], A[i] )\n                swapped = true\n            end if\n        end for\n    until not swapped\nend procedure\n</source>\n\n=== Optimizing bubble sort ===\nThe bubble sort algorithm can be easily optimized by observing that the ''n''-th pass finds the ''n''-th largest element and puts it into its final place. So, the inner loop can avoid looking at the last ''n'' − 1 items when running for the ''n''-th time:\n\n<source lang=\"pascal\">\nprocedure bubbleSort( A : list of sortable items )\n    n = length(A)\n    repeat\n        swapped = false\n        for i = 1 to n-1 inclusive do\n            if A[i-1] > A[i] then\n                swap(A[i-1], A[i])\n                swapped = true\n            end if\n        end for\n        n = n - 1\n    until not swapped\nend procedure\n</source>\n\nMore generally, it can happen that more than one element is placed in their final position on a single pass. In particular, after every pass, all elements after the last swap are sorted, and do not need to be checked again. This allows to skip over many elements, resulting in about a worst case 50% improvement in comparison count (though no improvement in swap counts), and adds very little complexity because the new code subsumes the \"swapped\" variable:\n\nTo accomplish this in pseudocode, the following can be written:\n<source lang=\"pascal\">\nprocedure bubbleSort( A : list of sortable items )\n    n = length(A)\n    repeat\n        newn = 0\n        for i = 1 to n-1 inclusive do\n            if A[i-1] > A[i] then\n                swap(A[i-1], A[i])\n                newn = i\n            end if\n        end for\n        n = newn\n    until n <= 1\nend procedure\n</source>\n\nAlternate modifications, such as the [[cocktail shaker sort]] attempt to improve on the bubble sort performance while keeping the same idea of repeatedly comparing and swapping adjacent items.\n\n==Use==\n[[File:Bubble sort animation.gif|thumb|right|280px|A bubble sort, a sorting algorithm that continuously steps through a list, [[Swap (computer science)|swapping]] items until they appear in the correct order. The list was plotted in a Cartesian coordinate system, with each point (''x'', ''y'') indicating that the value ''y'' is stored at index ''x''. Then the list would be sorted by bubble sort according to every pixel's value. Note that the largest end gets sorted first, with smaller elements taking longer to move to their correct positions.]]\nAlthough bubble sort is one of the simplest sorting algorithms to understand and implement, its [[Big O notation|''O''(''n''<sup>2</sup>)]] complexity means that its efficiency decreases dramatically on lists of more than a small number of elements. Even among simple ''O''(''n''<sup>2</sup>) sorting algorithms, algorithms like [[insertion sort]] are usually considerably more efficient.\n\nDue to its simplicity, bubble sort is often used to introduce the concept of an algorithm, or a sorting algorithm, to introductory [[computer science]] students. However, some researchers such as [[Owen Astrachan]] have gone to great lengths to disparage bubble sort and its continued popularity in computer science education, recommending that it no longer even be taught.<ref name=\"Astrachan2003\" />\n\nThe [[Jargon File]], which famously calls [[bogosort]] \"the archetypical [sic] perversely awful algorithm\", also calls bubble sort \"the generic bad algorithm\".<ref>{{cite web|url=http://www.jargon.net/jargonfile/b/bogo-sort.html|title=jargon, node: bogo-sort|publisher=}}</ref> [[Donald Knuth]], in ''[[The Art of Computer Programming]]'', concluded that \"the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems\", some of which he then discusses.<ref name=\"Knuth\">[[Donald Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Second Edition. Addison-Wesley, 1998. {{ISBN|0-201-89685-0}}. Pages 106–110 of section 5.2.2: Sorting by Exchanging. \"[A]lthough the techniques used in the calculations [to analyze the bubble sort] are instructive, the results are disappointing since they tell us that the bubble sort isn't really very good at all. Compared to straight insertion […], bubble sorting requires a more complicated program and takes about twice as long!\" (Quote from the first edition, 1973.)</ref>\n\nBubble sort is [[Big O notation|asymptotically]] equivalent in running time to insertion sort in the worst case, but the two algorithms differ greatly in the number of swaps necessary. Experimental results such as those of Astrachan have also shown that insertion sort performs considerably better even on random lists. For these reasons many modern algorithm textbooks avoid using the bubble sort algorithm in favor of insertion sort.\n\nBubble sort also interacts poorly with modern CPU hardware. It produces at least twice as many writes as insertion sort, twice as many cache misses, and asymptotically more [[Branch predictor|branch mispredictions]].{{citation needed|date=August 2015}} Experiments by Astrachan sorting strings in [[Java (programming language)|Java]] show bubble sort to be roughly one-fifth as fast as an insertion sort and 70% as fast as a [[selection sort]].<ref name=\"Astrachan2003\">{{cite journal |last1=Astrachan |first1=Owen |title=Bubble sort: an archaeological algorithmic analysis |url=http://www.cs.duke.edu/~ola/papers/bubble.pdf |journal=ACM SIGCSE Bulletin |volume=35 |issue=1 |year=2003 |pages=1–5 |issn=0097-8418 |doi=10.1145/792548.611918}}</ref>\n\nIn computer graphics bubble sort is popular for its capability to detect a very small error (like swap of just two elements) in almost-sorted arrays and fix it with just linear complexity (2''n''). For example, it is used in a polygon filling algorithm, where bounding lines are sorted by their ''x'' coordinate at a specific scan line (a line parallel to the ''x'' axis) and with incrementing ''y'' their order changes (two elements are swapped) only at intersections of two lines. Bubble sort is a stable sort algorithm, like insertion sort.\n\n== Variations ==\n* [[Odd–even sort]] is a parallel version of bubble sort, for message passing systems.\n* Passes can be from right to left, rather than left to right. This is more efficient for lists with unsorted items added to the end.\n* [[Cocktail shaker sort]] alternates leftwards and rightwards passes.\n\n== Debate over name ==\nBubble sort has been occasionally referred to as a \"sinking sort\".<ref>{{cite web |url=https://xlinux.nist.gov/dads/HTML/bubblesort.html |title=bubble sort |last=Black |first=Paul E. |date=24 August 2009 |work=[[Dictionary of Algorithms and Data Structures]] |publisher=[[National Institute of Standards and Technology]] |accessdate=1 October 2014}}</ref>\n\nFor example, in Donald Knuth's ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'' he states in section 5.2.1 'Sorting by Insertion', that [the value] \"settles to its proper level\" and that this method of sorting has sometimes been called the ''sifting'' or ''sinking'' technique.{{clarify|date=October 2014}}\n\nThis debate is perpetuated by the ease with which one may consider this algorithm from two different but equally valid perspectives:\n# The ''larger'' values might be regarded as ''heavier'' and therefore be seen to progressively ''sink'' to the ''bottom'' of the list\n# The ''smaller'' values might be regarded as ''lighter'' and therefore be seen to progressively ''bubble up'' to the ''top'' of the list.\n\n== Notes ==\n<references />\n\n== References ==\n* [https://www.cs.tcd.ie/publications/tech-reports/reports.05/TCD-CS-2005-57.pdf Sorting in the Presence of Branch Prediction and Caches]\n* Fundamentals of Data Structures by Ellis Horowitz, [[Sartaj Sahni]] and Susan Anderson-Freed {{ISBN|81-7371-605-6}}\n* [[Owen Astrachan]]. [https://users.cs.duke.edu/~ola/bubble/bubble.html Bubble Sort: An Archaeological Algorithmic Analysis]\n\n==External links==\n{{wikibooks|Algorithm implementation|Sorting/Bubble_sort|Bubble sort}}\n{{commons category|Bubble sort}}\n{{wikiversity|Bubble sort}}\n* {{cite web |last=Martin |first=David R. |url=http://www.sorting-algorithms.com/bubble-sort |archivedate=2015-03-03 |archiveurl=https://web.archive.org/web/20150303084352/http://www.sorting-algorithms.com/bubble-sort |date=2007 |title=Animated Sorting Algorithms: Bubble Sort}} – graphical demonstration\n* {{cite web | url= http://lecture.ecc.u-tokyo.ac.jp/~ueda/JavaApplet/BubbleSort.html |title= Lafore's Bubble Sort}} (Java applet animation)\n* {{OEIS el|1=A008302|2=Table (statistics) of the number of permutations of <nowiki>[n]</nowiki> that need k pair-swaps during the sorting|formalname=Triangle of Mahonian numbers T(n,k): coefficients in expansion of Product_{i=0..n-1} (1 + x + ... + x^i), where k ranges from 0 to A000217(n-1)}} \n{{sorting}}\n\n[[Category:Articles with example pseudocode]]\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n\n[[no:Sorteringsalgoritme#Boblesortering]]"
    },
    {
      "title": "Bucket sort",
      "url": "https://en.wikipedia.org/wiki/Bucket_sort",
      "text": "{{about|a variation of bucket sorting that allows multiple keys per bucket|the variation with one key per bucket|pigeonhole sort}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|data=[[Array data structure|Array]]\n|time=<math>O(n^2)</math>\n|average-time=<math>O(n+\\frac{n^2}{k}+k)</math>, where k is the number of buckets. <math>O(n), \\text {when } k \\approx n</math>.\n|space=<math>O(n\\cdot k)</math>\n}}\n\n{{Expert needed| Computer science|date=November 2008}}\n[[File:Bucket sort 1.svg|right|frame|Elements are distributed among bins]]\n[[File:Bucket sort 2.svg|right|frame|Then, elements are sorted within each bin]]\n'''Bucket sort''', or '''bin sort''', is a [[sorting algorithm]] that works by distributing the elements of an [[Array data structure|array]] into a number of [[bucket (computing)|bucket]]s. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a [[distribution sort]],  a generalization of [[pigeonhole sort]], and is a cousin of [[radix sort]] in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a [[comparison sort]] algorithm. The [[Analysis of algorithms|computational complexity]] depends on the algorithm used to sort each bucket, the number of buckets to use, and whether the input is uniformly distributed.\n\nBucket sort works as follows:\n# Set up an array of initially empty \"buckets\".\n# '''Scatter''': Go over the original array, putting each object in its bucket.\n# Sort each non-empty bucket.\n# '''Gather''': Visit the buckets in order and put all elements back into the original array.\n\n==Pseudocode==\n\n '''function''' bucketSort(array, k) '''is'''\n   buckets ← new array of k empty lists\n   M ← the maximum key value in the array\n   '''for''' i = 1 '''to''' length(array) '''do'''\n     insert ''array[i]'' into ''buckets[floor(array[i] / M * k)]''\n   '''for''' i = 1 '''to''' k '''do'''\n     nextSort(buckets[i])\n   '''return''' the concatenation of buckets[1], ...., buckets[k]\n\nHere ''array'' is the array to be sorted and ''k'' is the number of buckets to use. The maximum key value can be computed in [[linear time]] by looking up all the keys once. The [[floor function]] must be used to convert a floating number to an integer. The function ''nextSort'' is a sorting function used to sort each bucket. Conventionally, [[insertion sort]] would be used, but other algorithm could be used as well. Using ''bucketSort'' itself as ''nextSort'' produces a relative of [[radix sort]]; in particular, the case ''n = 2'' corresponds to [[quicksort]] (although potentially with poor pivot choices). \n== Analysis ==\n\n=== Worst-case analysis ===\nBucket sort is mainly useful when input is uniformly distributed over a range. When the input contains several keys that are close to each other (clustering), those elements are likely to be placed in the same bucket, which results in some buckets containing more elements than average. The worst-case scenario occurs when all the elements are placed in a single bucket. The overall performance would then be dominated by the algorithm used to sort each bucket, which is typically <math>O(n^2)</math> [[insertion sort]], making bucket sort less optimal than <math>O(n \\log(n))</math> [[comparison sort]] algorithms like [[Quicksort]].\n\n=== Average-case analysis ===\nConsider the case that the input is uniformly distributed. The first step, which is '''initialize''' the buckets and '''find the maximum key value''' in the array, can be done in <math>O(n)</math> time. If division and multiplication can be done in constant time, then '''scattering''' each element to its bucket also costs <math>O(n)</math>. Assume insertion sort is used to sort each bucket, then the third step costs <math>O(\\textstyle \\sum_{i=1}^k \\displaystyle n_i^2)</math>, where <math>n_i</math> is the length of the bucket indexed <math>i</math>. Since we are concerning the average time, the expectation <math>E(n_i^2)</math> have to be evaluated instead. Let <math>X_{ij}</math> be the random variable of element <math>j</math> being placed in bucket <math>i</math>. We have <math>n_i = \\sum_{j=1}^n X_{ij}</math>. Therefore,\n\n:<math>\\begin{align}\nE(n_i^2) & = E\\left(\\sum_{j=1}^n X_{ij} \\sum_{k=1}^n X_{ik}\\right) \\\\\n& = E\\left(\\sum_{j=1}^n \\sum_{k=1}^n X_{ij}X_{ik}\\right) \\\\\n& = E\\left(\\sum_{j=1}^n X_{ij}^2\\right) + E\\left(\\sum_{1\\leq j,k\\leq n}\\sum_{j\\neq k}X_{ij}X_{ik}\\right)\n\\end{align} </math>\n\nThe last line separates the summation into the case <math>j=k</math> and the case <math>j\\neq k</math>. Since the chance of an object distributed to bucket <math>i</math> is <math>1/k</math>, <math>X_{ij} </math> is 1 with probability <math>1/k</math> and 0 otherwise. \n\n:<math>E(X_{ij}^2) = 1^2\\cdot \\left(\\frac{1}{k}\\right) + 0^2\\cdot \\left(1-\\frac{1}{k}\\right) = \\frac{1}{k}</math>\n:<math>E(X_{ij}X_{ik}) = 1\\cdot \\left(\\frac{1}{k}\\right)\\left(\\frac{1}{k}\\right) = \\frac{1}{k^2} </math>\n\nWith the summation, it would be\n\n:<math>E\\left(\\sum_{j=1}^n X_{ij}^2\\right) + E\\left(\\sum_{1\\leq j,k\\leq n}\\sum_{j\\neq k}X_{ij}X_{ik}\\right) = n\\cdot\\frac{1}{k} + n(n-1)\\cdot\\frac{1}{k^2} = \\frac{n^2+nk-n}{k^2}</math>\n\nFinally, the complexity would be <math>O\\left(\\sum_{i=1}^kE(n_i^2)\\right) = O\\left(\\sum_{i=1}^k \\frac{n^2+nk-n}{k^2}\\right) = O\\left(\\frac{n^2}{k}+n\\right) </math>.\n\nThe last step of bucket sort, which is '''concatenating''' all the sorted objects in each buckets, requires <math>O(k)</math> time. Therefore, the total complexity is <math>O\\left(n+\\frac{n^2}{k}+k\\right)</math>. Note that if k is chosen to be <math>k = \\Theta(n)</math>, then bucket sort runs in <math>O(n)</math> average time, given a uniformly distributed input.<ref name=\"lfcs\" />\n\n==Optimizations==\n\nA common optimization is to put the unsorted elements of the buckets back in the original array ''first'', then run [[insertion sort]] over the complete array; because insertion sort's runtime is based on how far each element is from its final position, the number of comparisons remains relatively small, and the memory hierarchy is better exploited by storing the list contiguously in memory.<ref>Corwin, E. and Logar, A. \"Sorting in linear time &#x2014; variations on the bucket sort\". ''Journal of Computing Sciences in Colleges'', 20, 1, pp.197&#x2013;202. October 2004.</ref>\n\n==  Variants  ==\n\n===Generic bucket sort===\nThe most common variant of bucket sort operates on a list of ''n'' numeric inputs between zero and some maximum value ''M'' and divides the value range into ''n'' buckets each of size ''M''/''n''. If each bucket is sorted using [[insertion sort]], the sort can be shown to run in expected linear time (where the average is taken over all possible inputs).<ref>[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{ISBN|0-262-03293-7}}. Section 8.4: Bucket sort, pp.174&ndash;177.</ref> However, the performance of this sort degrades with clustering; if many values occur close together, they will all fall into a single bucket and be sorted slowly.  This performance degradation is avoided in the original bucket sort algorithm by assuming that the input is generated by a random process that distributes elements uniformly over the interval ''[0,1)''.<ref name=\"lfcs\">{{cite book|title=[[Introduction to Algorithms]]|author=Thomas H. Cormen|author-link=Thomas H. Cormen|author2=Charles E. Leiserson|author2-link=Charles E. Leiserson|author3=Ronald L. Rivest|author3-link=Ronald L. Rivest|author4=Clifford Stein|author4-link=Clifford Stein|last-author-amp=yes|quote=Bucket sort runs in linear time on the average. Like counting sort, bucket sort is fast because it assumes something about the input. Whereas counting sort assumes that the input consists of integers in a small range, bucket sort assumes that the input is generated by a random process that distributes elements uniformly over the interval ''[0,1)''. The idea of bucket sort is to divide the interval ''[0, 1)'' into ''n'' equal-sized subintervals, or buckets, and then distribute the ''n'' input numbers into the buckets. Since the inputs are uniformly distributed over ''[0, 1)'', we don't expect many numbers to fall into each bucket. To produce the output, we simply sort the numbers in each bucket and then go through the buckets in order, listing the elements in each.}}</ref> \n\n===ProxmapSort===\n{{Main article|Proxmap sort}}\nSimilar to generic bucket sort as described above, '''ProxmapSort''' works by dividing an array of keys into subarrays via the use of a \"map key\" function that preserves a partial ordering on the keys; as each key is added to its subarray, insertion sort is used to keep that subarray sorted, resulting in the entire array being in sorted order when ProxmapSort completes. ProxmapSort differs from bucket sorts in its use of the map key to place the data approximately where it belongs in sorted order, producing a \"proxmap\" — a proximity mapping — of the keys.\n\n===Histogram sort===\nAnother variant of bucket sort known as histogram sort or [[counting sort]] adds an initial pass that counts the number of elements that will fall into each bucket using a count array. Using this information, the array values can be arranged into a sequence of buckets in-place by a sequence of exchanges, leaving no space overhead for bucket storage.<ref>[https://xlinux.nist.gov/dads/HTML/histogramSort.html NIST's Dictionary of Algorithms and Data Structures: histogram sort]</ref>\n\n===Postman's sort===<!-- This section is linked from [[Radix sort]] -->\nThe '''Postman's sort''' is a variant of bucket sort that takes advantage of a hierarchical structure of elements, typically described by a set of attributes. This is the algorithm used by letter-sorting machines in [[post office]]s: mail is sorted first between domestic and international; then by state, province or territory; then by destination post office; then by routes, etc. Since keys are not compared against each other, sorting time is O(''cn''), where ''c'' depends on the size of the key and number of buckets. This is similar to a [[radix sort]] that works \"top down,\" or \"most significant digit first.\"<ref>http://www.rrsd.com/psort/cuj/cuj.htm</ref>\n\n===Shuffle sort===<!-- This section is linked from [[J sort]] -->\nThe '''shuffle sort'''<ref>[https://groups.google.com/group/fido7.ru.algorithms/msg/26084cdb04008ab3 A revolutionary new sort from John Cohen Nov 26, 1997]</ref> is a variant of bucket sort that begins by removing the first 1/8 of the ''n'' items to be sorted, sorts them recursively, and puts them in an array. This creates ''n''/8 \"buckets\" to which the remaining 7/8 of the items are distributed. Each \"bucket\" is then sorted, and the \"buckets\" are concatenated into a sorted array.\n\n==Comparison with other sorting algorithms==\n\nBucket sort can be seen as a generalization of [[counting sort]]; in fact, if each bucket has size 1 then bucket sort degenerates to counting sort. The variable bucket size of bucket sort allows it to use O(''n'') memory instead of O(''M'') memory, where ''M'' is the number of distinct values; in exchange, it gives up counting sort's O(''n'' + ''M'') worst-case behavior.\n\nBucket sort with two buckets is effectively a version of [[quicksort]] where the pivot value is always selected to be the middle value of the value range. While this choice is effective for uniformly distributed inputs, other means of choosing the pivot in quicksort such as randomly selected pivots make it more resistant to clustering in the input distribution.\n\nThe ''n''-way [[mergesort]] algorithm also begins by distributing the list into ''n'' sublists and sorting each one; however, the sublists created by mergesort have overlapping value ranges and so cannot be recombined by simple concatenation as in bucket sort. Instead, they must be interleaved by a merge algorithm. However, this added expense is counterbalanced by the simpler scatter phase and the ability to ensure that each sublist is the same size, providing a good worst-case time bound.\n\nTop-down [[radix sort]] can be seen as a special case of bucket sort where both the range of values and the number of buckets is constrained to be a power of two. Consequently, each bucket's size is also a power of two, and the procedure can be applied recursively. This approach can accelerate the scatter phase, since we only need to examine a prefix of the bit representation of each element to determine its bucket.\n\n==References==\n<references />\n\n* Paul E. Black [https://xlinux.nist.gov/dads/HTML/postmansort.html \"Postman's Sort\"] from [[Dictionary of Algorithms and Data Structures]] at [[National Institute of Standards and Technology|NIST]].\n* Robert Ramey [http://www.rrsd.com/software_development/postmans_sort/cuj/cuj.htm '\"The Postman's Sort\"] ''C Users Journal'' Aug. 1992\n* [https://xlinux.nist.gov/dads/HTML/bucketsort.html NIST's Dictionary of Algorithms and Data Structures: bucket sort]\n\n==External links==\n* [http://www.dcc.uchile.cl/~rbaeza/handbook/algs/4/423.sort.c.html Bucket Sort Code for Ansi C]\n* [http://www1bpt.bridgeport.edu/~dichter/lilly/bucketsort.htm Variant of Bucket Sort with Demo]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Stable sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Cache-oblivious distribution sort",
      "url": "https://en.wikipedia.org/wiki/Cache-oblivious_distribution_sort",
      "text": "{{Multiple issues|{{primary sources|date=May 2014}}{{technical|date=May 2014}}\n}}\n\nThe cache-oblivious '''distribution sort''' is a [[comparison sort|comparison-based]] [[sorting algorithm]]. It is similar to [[quicksort]], but it is a [[cache-oblivious algorithm]], designed for a setting where the number of elements to sort is too large to fit in a [[cache (computing)|cache]] where operations are done. In the [[external memory model]], the number of memory transfers it needs to perform a sort of <math>N</math> items on a machine with cache of size <math>Z</math> and cache lines of length <math>L</math> is <math> O(\\frac{N}{L} \\log_Z N)</math>, under the tall cache assumption that <math>Z = \\Omega(L^2)</math>. This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. This distribution sort also achieves the asymptotically optimal runtime complexity of <math>\\Theta(N \\log N)</math>.\n\n== Algorithm ==\n\n=== Overview ===\nDistribution sort operates on a contiguous array of <math>N</math> elements. To sort the elements, it performs the following:\n\n# Partition the array into <math>\\sqrt{N}</math> contiguous subarrays of size <math>\\sqrt{N}</math>, and recursively sort each subarray.\n# Distribute the elements of the sorted subarrays into <math>q \\le \\sqrt{N}</math> buckets <math>B_1, B_2, \\ldots, B_q</math> each of size at most <math>2 \\sqrt{N}</math> such that for every i from 1 to q-1,  every element of bucket <math>B_i</math> is not larger than any element in <math>B_{i+1}.</math> This distribution step is the main step of this algorithm, and is covered in more detail below.\n# Recursively sort each bucket.\n# Output the concatenation of the buckets.\n\n<!-- mention stacked-based memory allocation? -->\n\n=== Distribution step ===\nAs mentioned in step 2 above, the goal of the distribution step is to distribute the sorted subarrays into q buckets <math>B_1, B_2, \\ldots, B_q.</math> The distribution step algorithm maintains two invariants. The first is that each bucket has size at most <math> 2 \\sqrt{N}</math> at any time, and any element in bucket <math>B_i</math> is no larger than any element in bucket <math>B_{i+1}.</math> The second is that every bucket has an associated ''pivot'', a value which is greater than all elements in the bucket.\n\nInitially, the algorithm starts with one empty bucket with pivot <math>\\infty</math>. As it fills buckets, it creates new buckets by splitting a bucket into two when it would be made overfull (by having at least <math>(2\\sqrt{N}+1)</math> elements placed into it). The split is done by performing the [[Median of medians|linear time median finding]] algorithm, and partitioning based on this median. The pivot of the lower bucket will be set to the median found, and the pivot of the higher bucket will be set to the same as the bucket before the split. At the end of the distribution step, all elements are in the buckets, and the two invariants will still hold.\n\nTo accomplish this, each subarray and bucket will have a state associated with it. The state of a subarray consists of an index ''next'' of the next element to be read from the subarray, and a bucket number ''bnum'' indicating which bucket index the element should be copied to. By convention, <math>bnum = \\infty</math> if all elements in the subarray have been distributed. (Note that when we split a bucket, we have to increment all ''bnum'' values of all subarrays whose ''bnum'' value is greater than the index of the bucket that is split.) The state of a bucket consists of the value of the bucket's pivot, and the number of elements currently in the bucket.\n\nConsider the follow basic strategy: iterate through each subarray, attempting to copy over its element at position ''next''. If the element is smaller than the pivot of bucket ''bnum'', then place it in that bucket, possibly incurring a bucket split. Otherwise, increment ''bnum'' until a bucket whose pivot is large enough is found. Though this correctly distributes all elements, it does not exhibit a good cache performance.\n\nInstead, the distribution step is performed in a recursive divide-and-conquer. The step will be performed as a call to the function '''distribute''', which takes three parameters i, j, and m. '''distribute'''(i,j,m) will distribute elements from the i-th through (i+m-1)-th subarrays into buckets, starting from <math>B_j</math>. It requires as a precondition that each subarray r in the range <math>i, \\ldots, i+m-1</math> has its <math>bnum[r] \\ge j</math>. The execution of '''distribute'''(i,j,m) will guarantee that each <math>bnum[r] \\ge j+m</math>. The whole distribution step is '''distribute'''<math>(1,1,\\sqrt{N})</math>. Pseudocode for the implementation of distribute is shown below:\n\n<syntaxhighlight lang=python>\ndef distribute(i,j,m):\n  if m == 1:\n    copy_elems(i,j)\n  else:\n    distribute(i,j,m/2)\n    distribute(i+m/2,j,m/2)\n    distribute(i,j+m/2,m/2)\n    distribute(i+m/2,j+m/2,m/2)\n</syntaxhighlight>\n\nThe base case, where m=1, has a call to the subroutine '''copy_elems'''. In this base case, all elements from subarray i that belong to bucket j are added at once. If this leads to bucket j having too many elements, it splits the bucket with the procedure described beforehand.\n\n== See also ==\n* [[Cache-oblivious algorithm]]\n* [[Funnelsort]]\n* [[External sorting]]\n\n==References==\n*Harald Prokop. [http://supertech.csail.mit.edu/papers/Prokop99.pdf Cache-Oblivious Algorithms]. Masters thesis, MIT. 1999.\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:External memory algorithms]]\n[[Category:Models of computation]]\n[[Category:Cache (computing)]]\n[[Category:Analysis of algorithms]]"
    },
    {
      "title": "Cartesian tree",
      "url": "https://en.wikipedia.org/wiki/Cartesian_tree",
      "text": "[[File:Cartesian tree.svg|thumb|240px|A sequence of numbers and the Cartesian tree derived from them.]]\n\nIn [[computer science]], a '''Cartesian tree''' is a [[binary tree]] derived from a sequence of numbers; it can be uniquely defined from the properties that it is [[Heap (data structure)|heap]]-ordered and that a [[Tree traversal|symmetric (in-order) traversal]] of the tree returns the original sequence. Introduced by {{harvtxt|Vuillemin|1980}} in the context of geometric [[range searching]] [[data structure]]s, Cartesian trees have also been used in the definition of the [[treap]] and [[randomized binary search tree]] data structures for [[binary search]] problems. The Cartesian tree for a sequence may be constructed in [[linear time]] using a [[Stack (data structure)|stack]]-based algorithm for finding [[all nearest smaller values]] in a sequence.\n\n==Definition==\nThe Cartesian tree for a sequence of distinct numbers can be uniquely defined by the following properties:\n#The Cartesian tree for a sequence has one node for each number in the sequence. Each node is associated with a single sequence value.\n#A [[Tree traversal|symmetric (in-order) traversal]] of the tree results in the original sequence. That is, the left subtree consists of the values earlier than the root in the sequence order, while the right subtree consists of the values later than the root, and a similar ordering constraint holds at each lower node of the tree.\n#The tree has the [[Binary heap|heap property]]: the parent of any non-root node has a smaller value than the node itself.<ref>In some references, the ordering is reversed, so the parent of any node always has a larger value and the root node holds the maximum value.</ref>\nBased on the heap property, the root of the tree must be the smallest number in the sequence. From this, the tree itself may also be defined recursively: the root is the minimum value of the sequence, and the left and right subtrees are the Cartesian trees for the subsequences to the left and right of the root value. Therefore, the three properties above uniquely define the Cartesian tree.\n\nIf a sequence of numbers contains repetitions, the Cartesian tree may be defined by determining a consistent tie-breaking rule (for instance, determining that the first of two equal elements is treated as the smaller of the two) before applying the above rules.\n\nAn example of a Cartesian tree is shown in the figure above.\n\n==Range searching and lowest common ancestors==\n[[File:Cartesian tree range searching.svg|thumb|300px|Two-dimensional range-searching using a Cartesian tree: the bottom point (red in the figure) within a three-sided region with two vertical sides and one horizontal side (if the region is nonempty) may be found as the nearest common ancestor of the leftmost and rightmost points (the blue points in the figure) within the slab defined by the vertical region boundaries. The remaining points in the three-sided region may be found by splitting it by a vertical line through the bottom point and recursing.]]\nCartesian trees may be used as part of an efficient [[data structure]] for [[Range Minimum Query|range minimum queries]], a [[range searching]] problem involving queries that ask for the minimum value in a contiguous subsequence of the original sequence.<ref>{{harvtxt|Gabow|Bentley|Tarjan|1984}}; {{harvtxt|Bender|Farach-Colton|2000}}.</ref> In a Cartesian tree, this minimum value may be found at the [[lowest common ancestor]] of the leftmost and rightmost values in the subsequence. For instance, in the subsequence (12,10,20,15) of the sequence shown in the first illustration, the minimum value of the subsequence (10) forms the lowest common ancestor of the leftmost and rightmost values (12 and 15). Because lowest common ancestors may be found in constant time per query, using a data structure that takes linear space to store and that may be constructed in linear time,<ref>{{harvtxt|Harel|Tarjan|1984}}; {{harvtxt|Schieber|Vishkin|1988}}.</ref> the same bounds hold for the range minimization problem.\n\n{{harvtxt|Bender|Farach-Colton|2000}} reversed this relationship between the two data structure problems by showing that lowest common ancestors in an input tree could be solved efficiently applying a non-tree-based technique for range minimization. Their data structure uses an [[Euler tour]] technique to transform the input tree into a sequence and then finds range minima in the resulting sequence.  The sequence resulting from this transformation has a special form (adjacent numbers, representing heights of adjacent nodes in the tree, differ by ±1) which they take advantage of in their data structure; to solve the range minimization problem for sequences that do not have this special form, they use Cartesian trees to transform the range minimization problem into a lowest common ancestor problem, and then apply the Euler tour technique to transform the problem again into one of range minimization for sequences with this special form.\n\nThe same range minimization problem may also be given an alternative interpretation in terms of two dimensional range searching. A collection of finitely many points in the [[Cartesian plane]] may be used to form a Cartesian tree, by sorting the points by their ''x''-coordinates and using the ''y''-coordinates in this order as the sequence of values from which this tree is formed. If ''S'' is the subset of the input points within some vertical slab defined by the inequalities ''L''&nbsp;≤&nbsp;''x''&nbsp;≤&nbsp;''R'', ''p'' is the leftmost point in ''S'' (the one with minimum ''x''-coordinate), and ''q'' is the rightmost point in ''S'' (the one with maximum ''x''-coordinate) then the lowest common ancestor of ''p'' and ''q'' in the Cartesian tree is the bottommost point in the slab. A three-sided range query, in which the task is to list all points within a region bounded by the three inequalities ''L''&nbsp;≤&nbsp;''x''&nbsp;≤&nbsp;''R'' and ''y''&nbsp;≤&nbsp;''T'', may be answered by finding this bottommost point ''b'', comparing its ''y''-coordinate to ''T'', and (if the point lies within the three-sided region) continuing recursively in the two slabs bounded between ''p'' and ''b'' and between ''b'' and ''q''. In this way, after the leftmost and rightmost points in the slab are identified, all points within the three-sided region may be listed in constant time per point.<ref name=\"gbt\">{{harvtxt|Gabow|Bentley|Tarjan|1984}}.</ref>\n\nThe same construction, of lowest common ancestors in a Cartesian tree, makes it possible to construct a data structure with linear space that allows the distances between pairs of points in any [[ultrametric space]] to be queried in constant time per query. The distance within an ultrametric is the same as the [[widest path problem|minimax path]] weight in the [[minimum spanning tree]] of the metric.<ref>{{harvtxt|Hu|1961}}; {{harvtxt|Leclerc|1981}}</ref> From the minimum spanning tree, one can construct a Cartesian tree, the root node of which represents the heaviest edge of the minimum spanning tree. Removing this edge partitions the minimum spanning tree into two subtrees, and Cartesian trees recursively constructed for these two subtrees form the children of the root node of the Cartesian tree. The leaves of the Cartesian tree represent points of the metric space, and the lowest common ancestor of two leaves in the Cartesian tree is the heaviest edge between those two points in the minimum spanning tree, which has weight equal to the distance between the two points. Once the minimum spanning tree has been found and its edge weights sorted, the Cartesian tree may be constructed in linear time.<ref>{{harvtxt|Demaine|Landau|Weimann|2009}}.</ref>\n\n==Treaps==\n{{main article|Treap}}\nBecause a Cartesian tree is a binary tree, it is natural to use it as a [[binary search tree]] for an ordered sequence of values.  However, defining a Cartesian tree based on the same values that form the search keys of a binary search tree does not work well: the Cartesian tree of a sorted sequence is just a [[path graph|path]], rooted at its leftmost endpoint, and binary searching in this tree degenerates to [[sequential search]] in the path. However, it is possible to generate more-balanced search trees by generating ''priority'' values for each search key that are different than the key itself, sorting the inputs by their key values, and using the corresponding sequence of priorities to generate a Cartesian tree. This construction may equivalently be viewed in the geometric framework described above, in which the ''x''-coordinates of a set of points are the search keys and the ''y''-coordinates are the priorities.\n\nThis idea was applied by {{harvtxt|Seidel|Aragon|1996}}, who suggested the use of random numbers as priorities. The data structure resulting from this random choice is called a [[treap]], due to its combination of binary search tree and binary heap features. An insertion into a treap may be performed by inserting the new key as a leaf of an existing tree, choosing a priority for it, and then performing [[tree rotation]] operations along a path from the node to the root of the tree to repair any violations of the heap property caused by this insertion; a deletion may similarly be performed by a constant amount of change to the tree followed by a sequence of rotations along a single path in the tree.\n\nIf the priorities of each key are chosen randomly and independently once whenever the key is inserted into the tree, the resulting Cartesian tree will have the same properties as a [[random binary search tree]], a tree computed by inserting the keys in a randomly chosen [[permutation]] starting from an empty tree, with each insertion leaving the previous tree structure unchanged and inserting the new node as a leaf of the tree. Random binary search trees had been studied for much longer, and are known to behave well as search trees (they have [[logarithm]]ic depth with high probability); the same good behavior carries over to treaps. It is also possible, as suggested by Aragon and Seidel, to reprioritize frequently-accessed nodes, causing them to move towards the root of the treap and speeding up future accesses for the same keys.\n\n==Efficient construction==\nA Cartesian tree may be constructed in [[linear time]] from its input sequence.\nOne method is to simply process the sequence values in left-to-right order, maintaining the Cartesian tree of the nodes processed so far, in a structure that allows both upwards and downwards traversal of the tree. To process each new value ''x'', start at the node representing the value prior to ''x'' in the sequence and follow the path from this node to the root of the tree until finding a value ''y'' smaller than ''x''. This node ''y'' is the parent of ''x'', and the previous right child of ''y'' becomes the new left child of ''x''. The total time for this procedure is linear, because the time spent searching for the parent ''y'' of each new node ''x'' can be [[Potential method|charged]] against the number of nodes that are removed from the rightmost path in the tree.<ref name=\"gbt\"/>\n\nAn alternative linear-time construction algorithm is based on the [[all nearest smaller values]] problem. In the input sequence, one may define the ''left neighbor'' of a value ''x'' to be the value that occurs prior to ''x'', is smaller than ''x'', and is closer in position to ''x'' than any other smaller value. The ''right neighbor'' is defined symmetrically. The sequence of left neighbors may be found by an algorithm that maintains a [[stack (data structure)|stack]] containing a subsequence of the input. For each new sequence value ''x'', the stack is popped until it is empty or its top element is smaller than ''x'', and then ''x'' is pushed onto the stack. The left neighbor of ''x'' is the top element at the time ''x'' is pushed. The right neighbors may be found by applying the same stack algorithm to the reverse of the sequence. The parent of ''x'' in the Cartesian tree is either the left neighbor of ''x'' or the right neighbor of ''x'', whichever exists and has a larger value. The left and right neighbors may also be constructed efficiently by [[parallel algorithm]]s, so this formulation may be used to develop efficient parallel algorithms for Cartesian tree construction.<ref>{{harvtxt|Berkman|Schieber|Vishkin|1993}}.</ref>\n\nAnother linear-time algorithm for Cartesian tree construction is based on divide-and-conquer. In particular, the algorithm recursively constructs the tree on each half of the input, and then merging the two trees by taking the right spine of the left tree and left spine of the right tree and performing a standard [[Merge algorithm#Merging two lists|merging]] operation. The algorithm is also parallelizable since on each level of recursion, each of the two sub-problems can be computed in parallel, and the merging operation can be [[Merge algorithm#Parallel merge|efficiently parallelized]] as well.{{sfnp|Shun|Blelloch|2014}}\n\n==Application in sorting==\n[[File:Bracketing pairs.svg|thumb|300px|Pairs of consecutive sequence values (shown as the thick red edges) that bracket a sequence value ''x'' (the darker blue point). The cost of including ''x'' in the sorted order produced by the Levcopoulos–Petersson algorithm is proportional to the [[logarithm]] of this number of bracketing pairs.]]\n{{harvtxt|Levcopoulos|Petersson|1989}} describe a [[sorting algorithm]] based on Cartesian trees. They describe the algorithm as based on a tree with the maximum at the root, but it may be modified straightforwardly to support a Cartesian tree with the convention that the minimum value is at the root. For consistency, it is this modified version of the algorithm that is described below.\n\nThe Levcopoulos–Petersson algorithm can be viewed as a version of [[selection sort]] or [[heap sort]] that maintains a [[priority queue]] of candidate minima, and that at each step finds and removes the minimum value in this queue, moving this value to the end of an output sequence. In their algorithm, the priority queue consists only of elements whose parent in the Cartesian tree has already been found and removed. Thus, the algorithm consists of the following steps:\n# Construct a Cartesian tree for the input sequence\n# Initialize a priority queue, initially containing only the tree root\n# While the priority queue is non-empty:\n#* Find and remove the minimum value ''x'' in the priority queue\n#* Add ''x'' to the output sequence\n#* Add the Cartesian tree children of ''x'' to the priority queue\n\nAs Levcopoulos and Petersson show, for input sequences that are already nearly sorted, the size of the priority queue will remain small, allowing this method to take advantage of the nearly-sorted input and run more quickly. Specifically, the worst-case running time of this algorithm is O(''n''&nbsp;log&nbsp;''k''), where ''k'' is the average, over all values ''x'' in the sequence, of the number of consecutive pairs of sequence values that bracket ''x''. They also prove a lower bound stating that, for any ''n'' and ''k''&nbsp;=&nbsp;ω(1), any comparison-based sorting algorithm must use Ω(''n''&nbsp;log&nbsp;''k'') comparisons for some inputs.\n\n==History==\nCartesian trees were introduced and named by {{harvtxt|Vuillemin|1980}}. The name is derived from the [[Cartesian coordinate]] system for the plane: in Vuillemin's version of this structure, as in the two-dimensional range searching application discussed above, a Cartesian tree for a point set has the sorted order of the points by their ''x''-coordinates as its symmetric traversal order, and it has the heap property according to the ''y''-coordinates of the points.\n{{harvtxt|Gabow|Bentley|Tarjan|1984}} and subsequent authors followed the definition here in which a Cartesian tree is defined from a sequence; this change generalizes the geometric setting of Vuillemin to allow sequences other than the sorted order of ''x''-coordinates, and allows the Cartesian tree to be applied to non-geometric problems as well.\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n{{refbegin|30em}}\n*{{citation\n | last1 = Bender | first1 = Michael A.\n | last2 = Farach-Colton | first2 = Martin | author2-link = Martin Farach-Colton\n | contribution = The LCA problem revisited\n | pages = 88–94\n | publisher = Springer-Verlag, [[Lecture Notes in Computer Science]] 1776\n | title = Proceedings of the 4th Latin American Symposium on Theoretical Informatics\n | url = http://www.cs.sunysb.edu/~bender/pub/lca.ps\n | year = 2000}}.\n*{{citation\n | last1 = Berkman | first1 = Omer\n | last2 = Schieber | first2 = Baruch\n | last3 = Vishkin | first3 = Uzi | author3-link = Uzi Vishkin\n | doi = 10.1006/jagm.1993.101\n | issue = 3\n | journal = Journal of Algorithms\n | pages = 344–370\n | title = Optimal doubly logarithmic parallel algorithms based on finding all nearest smaller values\n | volume = 14\n | year = 1993}}.\n*{{citation|contribution=On Cartesian trees and range minimum queries|first1=Erik D.|last1=Demaine|author1-link=Erik Demaine|first2=Gad M.|last2=Landau|first3=Oren|last3=Weimann|series=Lecture Notes in Computer Science|volume=5555|year=2009|pages=341–353|doi=10.1007/978-3-642-02927-1_29|title=Automata, Languages and Programming, 36th International Colloquium, ICALP 2009, Rhodes, Greece, July 5-12, 2009|isbn=978-3-642-02926-4}}.\n*{{citation\n | last1 = Fischer | first1 = Johannes\n | last2 = Heun | first2 = Volker\n | contribution = Theoretical and Practical Improvements on the RMQ-Problem, with Applications to LCA and LCE\n | title = Proceedings of the 17th Annual Symposium on Combinatorial Pattern Matching\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | volume = 4009\n | year = 2006\n | pages = 36–48\n | doi = 10.1007/11780441_5\n | isbn = 978-3-540-35455-0}}\n*{{citation\n | last1 = Fischer | first1 = Johannes\n | last2 = Heun | first2 = Volker\n | contribution = A New Succinct Representation of RMQ-Information and Improvements in the Enhanced Suffix Array.\n | title = Proceedings of the International Symposium on Combinatorics, Algorithms, Probabilistic and Experimental Methodologies\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | volume = 4614\n | year = 2007\n | pages = 459–470\n | doi = 10.1007/978-3-540-74450-4_41\n | isbn = 978-3-540-74449-8}}\n*{{citation\n | last1 = Gabow | first1 = Harold N.\n | last2 = Bentley | first2 = Jon Louis | author2-link = Jon Bentley (computer scientist)\n | last3 = Tarjan | first3 = Robert E. | author3-link = Robert Tarjan\n | contribution = Scaling and related techniques for geometry problems\n | doi = 10.1145/800057.808675\n | isbn = 0-89791-133-4\n | location = New York, NY, USA\n | pages = 135–143\n | publisher = ACM\n | title = [[Symposium on Theory of Computing|STOC '84: Proc. 16th ACM Symp. Theory of Computing]]\n | year = 1984}}.\n*{{citation\n | last1 = Harel | first1 = Dov\n | last2 = Tarjan | first2 = Robert E. | author2-link = Robert Tarjan\n | doi = 10.1137/0213024\n | issue = 2\n | journal = [[SIAM Journal on Computing]]\n | pages = 338–355\n | title = Fast algorithms for finding nearest common ancestors\n | volume = 13\n | year = 1984}}.\n*{{citation|title=The maximum capacity route problem|first=T. C.|last=Hu|journal=Operations Research|volume=9|issue=6|year=1961|pages=898–900|jstor=167055|doi=10.1287/opre.9.6.898}}.\n*{{citation\n | last = Leclerc | first = Bruno\n | mr = 623034\n | issue = 73\n | journal = Centre de Mathématique Sociale. École Pratique des Hautes Études. Mathématiques et Sciences Humaines\n | language = French\n | pages = 5–37, 127\n | title = Description combinatoire des ultramétriques\n | year = 1981}}.\n*{{citation\n | last1 = Levcopoulos | first1 = Christos\n | last2 = Petersson | first2 = Ola\n | contribution = Heapsort - Adapted for Presorted Files\n | doi = 10.1007/3-540-51542-9_41\n | location = London, UK\n | pages = 499–509\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = WADS '89: Proceedings of the Workshop on Algorithms and Data Structures\n | volume = 382\n | year = 1989}}.\n*{{citation\n | last1 = Seidel | first1 = Raimund | author1-link = Raimund Seidel\n | last2 = Aragon | first2 = Cecilia R.\n | doi = 10.1007/s004539900061\n | issue = 4/5\n | journal = Algorithmica\n | pages = 464–497\n | title = Randomized Search Trees\n | url = http://citeseer.ist.psu.edu/seidel96randomized.html\n | volume = 16\n | year = 1996}}.\n*{{citation\n | last1 = Schieber | first1 = Baruch\n | last2 = Vishkin | first2 = Uzi | author2-link = Uzi Vishkin\n | doi = 10.1137/0217079\n | journal = [[SIAM Journal on Computing]]\n | pages = 1253–1262\n | title = On finding lowest common ancestors: simplification and parallelization\n | volume = 17\n | year = 1988\n | issue = 6}}.\n*{{citation\n | last1 = Shun | first1 = Julian\n | last2 = Blelloch | first2 = Guy E.\n | title = A Simple Parallel Cartesian Tree Algorithm and its Application to Parallel Suffix Tree Construction\n | journal = ACM Transactions on Parallel Computing\n | year = 2014}}.\n*{{citation\n | last = Vuillemin | first = Jean\n | doi = 10.1145/358841.358852\n | issue = 4\n | journal = [[Communications of the ACM]]\n | location = New York, NY, USA\n | pages = 229–239\n | publisher = ACM\n | title = A unifying look at data structures\n | volume = 23\n | year = 1980}}.\n{{refend}}\n\n{{CS-Trees}}\n{{Sorting}}\n\n[[Category:Binary trees]]\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Cascade merge sort",
      "url": "https://en.wikipedia.org/wiki/Cascade_merge_sort",
      "text": "'''Cascade merge sort''' is similar to the [[polyphase merge sort]] but uses a simpler distribution.  The merge is slower than a polyphase merge when there are fewer than six files, but faster when there are more than six.<ref>{{harvnb|Bradley|1982|pp=189&ndash;190}}</ref>\n\n==References==\n{{Reflist}}\n\n*{{Citation |last=Bradley |first=James |year=1982 |title=File and Data Base Techniques |publisher=Holt, Rinehart and Winston |isbn=0-03-058673-9 |doi= }}\n<!--\n*{{Citation |last=Flores |first=Ivan |year=1969 |title=Computer Sorting |publisher=Prentice-Hall |isbn=978-0-13165746-5 |doi= }}\n*{{Citation |last=Knuth |first=D. E. |year=1975 |title=Sorting and Searching |series=[[The Art of Computer Programming]] |volume=3 |publisher=Addison Wesley |isbn= |doi= }}\n*{{Citation |last=Martin |first=W. A. |year=1971 |title=Sorting |journal=Computing Surveys |publisher=ACM |doi= }}\n-->\n\n==External links==\n* http://www.minkhollow.ca/Courses/461/Notes/Cosequential/Cascade.html\n\n{{sorting}}\n\n<!-- [[Category:Articles with example pseudocode]] -->\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]"
    },
    {
      "title": "Cocktail shaker sort",
      "url": "https://en.wikipedia.org/wiki/Cocktail_shaker_sort",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Sorting shaker sort anim.gif|Visualization of shaker sort]]\n|data=[[Array data structure|Array]]\n|best-time= <math>O(n)</math>\n|average-time= <math>O(n^2)</math>\n|time=<math>O(n^2)</math>\n|space=<math>O(1)</math>\n|optimal=No\n}}\n\n'''Cocktail shaker sort''',<ref name=\"Knuth\">{{cite book|title=Art of Computer Programming|edition=1st|pages=110&ndash;111|last=Knuth|first=Donald E.|authorlink=Donald Knuth|volume=3. Sorting and Searching|chapter=Sorting by Exchanging|publisher=[[Addison-Wesley]]|date=1973|isbn=0-201-03803-X}}</ref> also known as '''bidirectional bubble sort''',<ref>{{cite book|first=Paul E.|last=Black|first2=Bob|last2=Bockholt|url=http://xlinux.nist.gov/dads/HTML/bidirectionalBubbleSort.html|chapter=bidirectional bubble sort|title=Dictionary of Algorithms and Data Structures|editor1-first=Paul E.|editor1-last=Black|publisher=[[National Institute of Standards and Technology]]|date=24 August 2009|accessdate=5 February 2010}}</ref> '''cocktail sort''', '''shaker sort''' (which can also refer to a variant of [[selection sort]]), '''ripple sort''', '''shuffle sort''',<ref name=\"Duhl1986\">{{cite book|first=Martin|last=Duhl|contribution=Die schrittweise Entwicklung und Beschreibung einer Shuffle-Sort-Array Schaltung|title=HYPERKARL aus der Algorithmischen Darstellung des BUBBLE-SORT-ALGORITHMUS|language=German|journal=Projektarbeit|year=1986|publisher=Technical University of Kaiserslautern}}</ref> or '''shuttle sort''', is a variation of [[bubble sort]] that is both a [[stable sort|stable]] [[sorting algorithm]] and a [[comparison sort]]. The algorithm differs from a [[bubble sort]] in that it sorts in both directions on each pass through the list. This sorting algorithm is only marginally more difficult to implement than a bubble sort, and solves the problem of [[Bubble sort#Rabbits and turtles|turtles]] in bubble sorts. It provides only marginal performance improvements, and does not improve asymptotic performance; like the bubble sort, it is not of practical interest ([[insertion sort]] is preferred for simple sorts), though it finds some use in education.\n\n== Pseudocode ==\nThe simplest form goes through the whole list each time:\n\n '''procedure''' cocktailShakerSort( A ''':''' list of sortable items ) '''defined as:'''\n   '''do'''\n     swapped := false\n     '''for each''' i '''in''' 0 '''to''' length( A ) - 2 '''do:'''\n       '''if''' A[ i ] > A[ i + 1 ] '''then''' <span style=\"color:green\">// test whether the two elements are in the wrong order</span>\n         swap( A[ i ], A[ i + 1 ] ) <span style=\"color:green\">// let the two elements change places</span>\n         swapped := true\n       '''end if'''\n     '''end for'''\n     '''if not''' swapped '''then'''\n       <span style=\"color:green\">// we can exit the outer loop here if no swaps occurred.</span>\n       '''break do-while loop'''\n     '''end if'''\n     swapped := false\n     '''for each''' i '''in''' length( A ) - 2 '''to''' 0 '''do:'''\n       '''if''' A[ i ] > A[ i + 1 ] '''then'''\n         swap( A[ i ], A[ i + 1 ] )\n         swapped := true\n       '''end if'''\n     '''end for'''\n   '''while''' swapped <span style=\"color:green\">// if no elements have been swapped, then the list is sorted</span>\n '''end procedure'''\n\nThe first rightward pass will shift the largest element to its correct place at the end, and the following leftward pass will shift the smallest element to its correct place at the beginning. The second complete pass will shift the second largest and second smallest elements to their correct places, and so on. After ''i'' passes, the first ''i'' and the last ''i'' elements in the list are in their correct positions, and do not need to be checked. By shortening the part of the list that is sorted each time, the number of operations can be halved (see [[Bubble_sort#Alternative_implementations|bubble sort]]).\n\nThis is an example of the algorithm in MATLAB/OCTAVE with the optimization of remembering the last swap index and updating the bounds.\n\n<source lang=\"matlab\">\nfunction A = cocktailShakerSort(A)\n% `beginIdx` and `endIdx` marks the first and last index to check\nbeginIdx = 1;\nendIdx = length(A)-1;\nwhile beginIdx <= endIdx\n    newBeginIdx = endIdx;\n    newEndIdx = beginIdx;\n    for ii= beginIdx:endIdx\n        if A(ii) > A(ii + 1)\n            [A(ii+1), A(ii)] = deal(A(ii), A(ii+1));\n            newEndIdx=ii;\n        end\n    end\n\n    % decreases `endIdx` because the elements after `newEndIdx` are in correct order\n    endIdx = newEndIdx - 1;\n\n    for ii= endIdx:-1:beginIdx\n        if A(ii) > A(ii + 1)\n            [A(ii+1), A(ii)] = deal(A(ii), A(ii+1));\n            newBeginIdx = ii;\n        end\n    end\n    % increases `beginIdx` because the elements before `newBeginIdx` are in correct order\n    beginIdx = newBeginIdx + 1;\nend\nend\n</source>\n\n== Differences from bubble sort ==\nCocktail shaker sort is a slight variation of [[bubble sort]].<ref name=\"Knuth\"/> It differs in that instead of repeatedly passing through the list from bottom to top, it passes alternately from bottom to top and then from top to bottom. It can achieve slightly better performance than a standard bubble sort. The reason for this is that [[bubble sort]] only passes through the list in one direction and therefore can only move items backward one step each iteration.\n\nAn example of a list that proves this point is the list (2,3,4,5,1), which would only need to go through one pass of cocktail sort to become sorted, but if using an ascending [[bubble sort]] would take four passes. However one cocktail sort pass should be counted as two bubble sort passes. Typically cocktail sort is less than two times faster than bubble sort.\n\nAnother optimization can be that the algorithm remembers where the last actual swap has been done. In the next iteration, there will be no swaps beyond this limit and the algorithm has shorter passes. As the cocktail shaker sort goes bidirectionally, the range of possible swaps, which is the range to be tested, will reduce per pass, thus reducing the overall running time slightly.\n\n==Complexity==\nThe complexity of the cocktail shaker sort in [[big O notation]] is <math>O(n^2)</math> for both the worst case and the average case, but it becomes closer to <math>O(n)</math> if the list is mostly ordered before applying the sorting algorithm. For example, if every element is at a position that differs by at most k (k ≥ 1) from the position it is going to end up in, the complexity of cocktail shaker sort becomes <math>O(kn).</math>\n\nThe cocktail shaker sort is also briefly discussed in the book ''[[The Art of Computer Programming]]'', along with similar refinements of bubble sort. In conclusion, Knuth states about bubble sort and its improvements:\n{{quote|But none of these refinements leads to an algorithm better than straight insertion [that is, [[insertion sort]]]; and we already know that straight insertion isn't suitable for large&nbsp;''N''. [...] In short, the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems.|D. E. Knuth<ref name=\"Knuth\"/>\n}}\n\n== References ==\n<references />\n==Sources==\n*{{cite journal|first=R.|last=Hartenstein|journal=THE GRAND CHALLENGE TO REINVENT COMPUTING|title=A new World Model of Computing|publisher=CSBC|date=July 2010|place=[[Belo Horizonte]], Brazil|url=http://www.inf.pucminas.br/sbc2010/anais/pdf/semish/st03_02.pdf}}\n\n==External links==\n{{Wikibooks|Algorithm implementation|Sorting/Cocktail sort|Cocktail sort}}\n*[http://www.hermann-gruber.com/lehre/sorting/Shaker/Shaker-en.html Interactive demo of cocktail sort]\n*[http://www.cs.ubc.ca/~harrison/Java/sorting-demo.html Java source code and an animated demo of cocktail sort (called bi-directional bubble sort) and several other algorithms]\n*{{cite web |url=http://www.sharpdeveloper.net/content/archive/2007/08/14/dot-net-data-structures-and-algorithms.aspx |archiveurl=https://web.archive.org/web/20120212173240/http://www.sharpdeveloper.net/content/archive/2007/08/14/dot-net-data-structures-and-algorithms.aspx |title=.NET Implementation of cocktail sort and several other algorithms |archivedate=2012-02-12}}\n{{sorting}}\n\n{{DEFAULTSORT:Cocktail Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Comb sort",
      "url": "https://en.wikipedia.org/wiki/Comb_sort",
      "text": "{{Short description|Interval based sorting algorithm}}\n{{More citations needed | date = March 2011}}\n{{Infobox algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:comb_sort_demo.gif|Visualisation of comb sort]]\n|data=[[Array data structure|Array]]\n|time=<math>O(n^2)</math><ref name=BB>{{Cite journal | doi = 10.1016/S0020-0190(00)00223-4| title = Analyzing variants of Shellsort| journal = [[Information Processing Letters|Inf. Process. Lett.]]| volume = 79| issue = 5| pages = 223–227\n| date = 15 September 2001| last1 = Brejová | first1 = B. }}</ref>\n|average-time=<math>\\Omega(n^2/2^p)</math>, where {{math|''p''}} is the number of increments<ref name=BB/>\n|best-time=<math>\\Theta(n \\log n)</math>\n|space=<math>O(1)</math>\n|optimal=\n}}\n'''Comb sort''' is a relatively simple [[sorting algorithm]] originally designed by Włodzimierz Dobosiewicz in 1980.<ref name=BB/><ref>{{Cite journal\n |title=An efficient variation of bubble sort\n |author=Wlodzimierz Dobosiewicz\n |journal=Information Processing Letters\n |volume=11\n |year=1980\n |pages=5–6\n |doi=10.1016/0020-0190(80)90022-8\n}}</ref> Later it was rediscovered by Stephen Lacey and Richard Box in 1991.<ref>[http://cs.clackamas.cc.or.us/molatore/cs260Spr03/combsort.htm \"A Fast Easy Sort\"], [[Byte Magazine|''Byte'' Magazine]], April 1991</ref> Comb sort improves on [[bubble sort]].\n\n==Algorithm==\nThe basic idea is to eliminate ''turtles'', or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously. ''Rabbits'', large values around the beginning of the list, do not pose a problem in bubble sort.\n\nIn [[bubble sort]], when any two elements are compared, they always have a ''gap'' (distance from each other) of 1. The basic idea of comb sort is that the gap can be much more than 1. The inner loop of bubble sort, which does the actual ''swap'', is modified such that the gap between swapped elements goes down (for each iteration of outer loop) in steps of a \"shrink factor\" ''k'': [ ''n''/''k'', ''n''/''k''<sup>2</sup>, ''n''/''k''<sup>3</sup>, ..., 1 ].\n\nThe gap starts out as the length of the list ''n'' being sorted divided by the shrink factor ''k'' (generally 1.3; see below) and one pass of the aforementioned modified bubble sort is applied with that gap. Then the gap is divided by the shrink factor again, the list is sorted with this new gap, and the process repeats until the gap is 1. At this point, comb sort continues using a gap of 1 until the list is fully sorted. The final stage of the sort is thus equivalent to a bubble sort, but by this time most turtles have been dealt with, so a bubble sort will be efficient.\n\nThe shrink factor has a great effect on the efficiency of comb sort. ''k'' = 1.3 has been suggested as an ideal shrink factor by the authors of the original article after empirical testing on over 200,000 random lists. A value too small slows the algorithm down by making unnecessarily many comparisons, whereas a value too large fails to effectively deal with turtles, making it require many passes with 1 gap size.\n\nThe pattern of repeated sorting passes with decreasing gaps is similar to [[Shellsort]], but in Shellsort the array is sorted completely each pass before going on to the next-smallest gap.  Comb sort's passes do not completely sort the elements.  This is the reason that [[Shellsort#Gap sequences|Shellsort gap sequences]] have a larger optimal shrink factor of about 2.2.\n\n=== Pseudocode ===\n \n  '''function''' combsort('''array''' input)\n \n     gap := input.size <span style=\"color:green\">// Initialize gap size</span>\n     shrink := 1.3 <span style=\"color:green\">// Set the gap shrink factor</span>\n     sorted := false\n \n     '''loop while''' sorted = false\n         <span style=\"color:green\">// Update the gap value for a next comb</span>\n         gap := floor(gap / shrink)\n         '''if''' gap ≤ 1\n             gap := 1\n             sorted := true <span style=\"color:green\">// If there are no swaps this pass, we are done</span>\n         '''end if'''\n \n         <span style=\"color:green\">// A single \"comb\" over the input list</span>\n         i := 0\n         '''loop while''' i + gap < input.size<span style=\"color:green\"> // See [[Shell sort]] for a similar idea</span>\n             '''if''' input[i] > input[i+gap]\n                 [[Swap (computer science)|swap]](input[i], input[i+gap])\n                 sorted := false\n                 <span style=\"color:green\">// If this assignment never happens within the loop,\n                 // then there have been no swaps and the list is sorted.</span>\n              '''end if'''\n \n              i := i + 1\n          '''end loop'''\n          <br />\n      '''end loop'''\n  '''end function'''\n<!-- Please do not modify the pseudocode to make corrections unless you have verified, by translating the pseudocode to an actual programming language, that the corrections actually *are* corrections. Especially don't try to optimise it at the expense of clarity or understandability. -->\n\n== See also ==\n{{Wikibooks|Algorithm Implementation|Sorting/Comb sort|Comb sort}}\n* [[Bubble sort]], a generally slower algorithm, is the basis of comb sort.\n* [[Cocktail sort]], or bidirectional bubble sort, is a variation of bubble sort that also addresses the problem of turtles, albeit less effectively.\n\n== References ==\n{{reflist}}\n\n{{sorting}}\n\n{{DEFAULTSORT:Comb Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Comparison sort",
      "url": "https://en.wikipedia.org/wiki/Comparison_sort",
      "text": "[[Image:Balance à tabac 1850.JPG|thumb|right|300px|Sorting a set of unlabelled weights by weight using only a balance scale requires a comparison sort algorithm.]]\nA '''comparison sort''' is a type of [[sorting algorithm]] that only reads the list elements through a single abstract comparison operation (often a \"less than or equal to\" operator or a [[three-way comparison]]) that determines which of two elements should occur first in the final sorted list. The only requirement is that the operator forms a [[total preorder]] over the data, with:\n# if ''a'' ≤ ''b'' and ''b'' ≤ ''c'' then ''a'' ≤ ''c'' (transitivity)\n# for all ''a'' and ''b'', ''a'' ≤ ''b'' or ''b'' ≤ ''a'' ([[connex relation|connexity]]).\n\nIt is possible that both ''a'' ≤ ''b'' and ''b'' ≤ ''a''; in this case either may come first in the sorted list. In a [[sorting algorithm#Stability|stable sort]], the input order determines the sorted order in this case.\n\nA metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a [[balance scale]]. Their goal is to line up the weights in order by their weight without any information except that obtained by placing two weights on the scale and seeing which one is heavier (or if they weigh the same).\n\n==Examples==\n[[File:Sorting quicksort anim.gif|thumb|[[Quicksort]] in action on a list of numbers. The horizontal lines are pivot values.]]\nSome of the most well-known comparison sorts include:\n*[[Quicksort]]\n*[[Heapsort]]\n*[[Shellsort]]\n*[[Merge sort]]\n*[[Introsort]]\n*[[Insertion sort]]\n*[[Selection sort]]\n*[[Bubble sort]]\n*[[Odd–even sort]]\n*[[Cocktail shaker sort]]\n*[[Cycle sort]]\n*[[Merge-insertion sort]]\n*[[Smoothsort]]\n*[[Timsort]]\n\n==Performance limits and advantages of different sorting techniques==\n\nThere are fundamental limits on the performance of comparison sorts. A comparison sort must have an average-case lower bound of [[big-O notation|Ω]](''n'' log ''n'') comparison operations,<ref>{{Introduction to Algorithms|3|pages=191–193}}</ref> which is known as [[linearithmic]] time. This is a consequence of the limited information available through comparisons alone — or, to put it differently, of the vague algebraic structure of totally ordered sets. In this sense, mergesort, heapsort, and introsort are [[asymptotically optimal]] in terms of the number of comparisons they must perform, although this metric neglects other operations. Non-comparison sorts (such as the examples discussed below) can achieve [[big-O notation|O]](''n'') performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized).\n\nComparison sorts may run faster on some lists; many [[adaptive sort]]s such as [[insertion sort]] run in O(''n'') time on an already-sorted or nearly-sorted list. The [[big-O notation|Ω]](''n'' log ''n'') lower bound applies only to the case in which the input list can be in any possible order.\n\nReal-world measures of sorting speed may need to take into account the ability of some algorithms to optimally use relatively fast cached [[Random Access Memory|computer memory]], or the application may benefit from sorting methods where sorted data begins to appear to the user quickly (and then user's speed of reading will be the limiting factor) as opposed to sorting methods where no output is available until the whole list is sorted.\n\nDespite these limitations, comparison sorts offer the notable practical advantage that control over the comparison function allows sorting of many different datatypes and fine control over how the list is sorted. For example, reversing the result of the comparison function allows the list to be sorted in reverse; and one can sort a list of [[tuple]]s in [[lexicographic order]] by just creating a comparison function that compares each part in sequence:\n '''function''' tupleCompare((lefta, leftb, leftc), (righta, rightb, rightc))\n     '''if''' lefta ≠ righta\n         '''return''' compare(lefta, righta)\n     '''else if''' leftb ≠ rightb\n         '''return''' compare(leftb, rightb)\n     '''else'''\n         '''return''' compare(leftc, rightc)\n\n[[Balanced ternary]] notation allows comparisons to be made in one step, whose result will be one of \"less than\", \"greater than\" or \"equal to\".\n\nComparison sorts generally adapt more easily to complex orders such as the order of [[floating-point number]]s. Additionally, once a comparison function is written, any comparison sort can be used without modification; non-comparison sorts typically require specialized versions for each datatype.\n\nThis flexibility, together with the efficiency of the above comparison sorting algorithms on modern computers, has led to widespread preference for comparison sorts in most practical work.\n\n==Alternatives==\nSome sorting problems admit a strictly faster solution than the {{math|Ω(''n'' log ''n'')}} bound for comparison sorting; an example is [[integer sorting]], where all keys are integers. When the keys form a small (compared to {{mvar|n}}) range, [[counting sort]] is an example algorithm that runs in linear time. Other integer sorting algorithms, such as [[radix sort]], are not asymptotically faster than comparison sorting, but can be faster in practice.\n\nThe problem of [[X + Y sorting|sorting pairs of numbers by their sum]] is not subject to the {{math|Ω(''n''² log ''n'')}} bound either (the square resulting from the pairing up); the best known algorithm still takes {{math|O(''n''² log ''n'')}} time, but only {{math|O(''n''²)}} comparisons.\n\n==Number of comparisons required to sort a list==\n<div class=\"thumb tright\">\n<div class=\"thumbinner\" style=\"width:270px;\">\n{| class=\"wikitable\" style=\"margin-left:auto; margin-right:auto;\"\n! ''n'' !! <math>\\left\\lceil\\log_2(n!)\\right\\rceil</math> !! Minimum\n|-\n|style=\"text-align:right;\"| 1\n|style=\"text-align:right;\"| 0\n|style=\"text-align:center;\"| 0\n|-\n|style=\"text-align:right;\"| 2\n|style=\"text-align:right;\"| 1\n|style=\"text-align:center;\"| 1\n|-\n|style=\"text-align:right;\"| 3\n|style=\"text-align:right;\"| 3\n|style=\"text-align:center;\"| 3\n|-\n|style=\"text-align:right;\"| 4\n|style=\"text-align:right;\"| 5\n|style=\"text-align:center;\"| 5\n|-\n|style=\"text-align:right;\"| 5\n|style=\"text-align:right;\"| 7\n|style=\"text-align:center;\"| 7\n|-\n|style=\"text-align:right;\"| 6\n|style=\"text-align:right;\"| 10\n|style=\"text-align:center;\"| 10\n|-\n|style=\"text-align:right;\"| 7\n|style=\"text-align:right;\"| 13\n|style=\"text-align:center;\"| 13\n|-\n|style=\"text-align:right;\"| 8\n|style=\"text-align:right;\"| 16\n|style=\"text-align:center;\"| 16\n|-\n|style=\"text-align:right;\"| 9\n|style=\"text-align:right;\"| 19\n|style=\"text-align:center;\"| 19\n|-\n|style=\"text-align:right;\"| 10\n|style=\"text-align:right;\"| 22\n|style=\"text-align:center;\"| 22\n|-\n|style=\"text-align:right;\"| 11\n|style=\"text-align:right;\"| 26\n|style=\"text-align:center;\"| 26\n|-\n|style=\"text-align:right;\"| 12\n|style=\"text-align:right;\"| 29\n|style=\"text-align:center;\"| 30<ref name=\"Wells1965\">Mark Wells, Applications of a language for computing in combinatorics, Information Processing 65 (Proceedings of the 1965 IFIP Congress), 497–498, 1966.</ref><ref name=\"Wells1971\">Mark Wells, Elements of Combinatorial Computing, Pergamon Press, Oxford, 1971.</ref>\n|-\n|style=\"text-align:right;\"| 13\n|style=\"text-align:right;\"| 33\n|style=\"text-align:center;\"| 34<ref name=\"KasaiSawatoIwata1994\">Takumi Kasai, Shusaku Sawato, Shigeki Iwata, Thirty four comparisons are required to sort 13 items, LNCS 792, 260-269, 1994.</ref><ref name=\"Peczarski2002\">Marcin Peczarski, Sorting 13 elements requires 34 comparisons, LNCS 2461, 785–794, 2002.</ref><ref name=\"Peczarski2004\">Marcin Peczarski, New results in minimum-comparison sorting, Algorithmica 40 (2), 133–145, 2004.</ref>\n|-\n|style=\"text-align:right;\"| 14\n|style=\"text-align:right;\"| 37\n|style=\"text-align:center;\"| 38<ref name=\"Peczarski2004\" />\n|-\n|style=\"text-align:right;\"| 15\n|style=\"text-align:right;\"| 41\n|style=\"text-align:center;\"| 42<ref name=\"Peczarski2006\">Marcin Peczarski, Computer assisted research of posets, PhD thesis, University of Warsaw, 2006.</ref><ref name=\"Peczarski2007\">{{cite journal | last1 = Peczarski | first1 = Marcin | year = 2007 | title = The Ford-Johnson algorithm is still unbeaten for less than 47 elements | url = | journal = Inf. Process. Lett. | volume = 101 | issue = 3| pages = 126–128 | doi = 10.1016/j.ipl.2006.09.001 }}</ref><ref name=\"ChengLiuWang2007\">{{cite journal | first1 = Weiyi | last1 = Cheng | first2 = Xiaoguang | last2 = Liu | first3 = Gang | last3 = Wang | first4 = Jing | last4 = Liu | date = October 2007 | title = 最少比较排序问题中S（15）和S（19）的解决 |language=zh |trans-title=The results of S(15) and S(19) to minimum-comparison sorting problem | url = http://fcst.ceaj.org/EN/abstract/abstract47.shtml | journal = Journal of Frontiers of Computer Science and Technology | volume = 1 | issue = 3| pages = 305–313 }}<!--The PDF download works, although the web server misclassifies it as a RAR file.--></ref>\n|-\n|style=\"text-align:right;\"| 16\n|style=\"text-align:right;\"| 45 || 45 or 46<ref>{{Cite journal|last=Peczarski|first=Marcin|title=Towards Optimal Sorting of 16 Elements|journal=Acta Universitatis Sapientiae |volume=4|issue=2|pages=215–224|date=3 August 2011|arxiv=1108.0866|bibcode=2011arXiv1108.0866P}}</ref>\n|-\n|style=\"text-align:right;\"| 17\n|style=\"text-align:right;\"| 49 || 49 or 50\n|-\n|style=\"text-align:right;\"| 18\n|style=\"text-align:right;\"| 53 || 53 or 54\n|-\n|style=\"text-align:right;\"| 19\n|style=\"text-align:right;\"| 57\n|style=\"text-align:center;\"| 58<ref name=\"ChengLiuWang2007\" />\n|-\n|style=\"text-align:right;\"| 20\n|style=\"text-align:right;\"| 62\n|style=\"text-align:center;\"| 62\n|-\n|style=\"text-align:right;\"| 21\n|style=\"text-align:right;\"| 66\n|style=\"text-align:center;\"| 66\n|-\n|style=\"text-align:right;\"| 22\n|style=\"text-align:right;\"| 70\n|style=\"text-align:center;\"| 71<ref name=\"Peczarski2004\" />\n|-\n|style=\"height:5px;\"| || ||\n|-\n! ''n'' !! <math>\\left\\lceil\\log_2(n!)\\right\\rceil</math> !! <math>n \\log_2 n - \\frac{n}{\\ln 2}</math>\n|-\n|style=\"text-align:right;\"| 10\n|style=\"text-align:right;\"| 22\n|style=\"text-align:center;\"| 19\n|-\n|style=\"text-align:right;\"| 100\n|style=\"text-align:right;\"| 525\n|style=\"text-align:center;\"| 521\n|-\n|style=\"text-align:right;\"| 1 000\n|style=\"text-align:right;\"| 8 530\n|style=\"text-align:center;\"| 8 524\n|-\n|style=\"text-align:right;\"| 10 000\n|style=\"text-align:right;\"| 118 459\n|style=\"text-align:center;\"| 118 451\n|-\n|style=\"text-align:right;\"| 100 000\n|style=\"text-align:right;\"| 1 516 705\n|style=\"text-align:center;\"| 1 516 695\n|-\n|style=\"text-align:right;\"| {{nobr|1 000 000}}\n|style=\"text-align:right;\"| 18 488 885\n|style=\"text-align:center;\"| 18 488 874\n|}\n<div class=\"thumbcaption\">\nAbove: A comparison of the lower bound <math>\\left\\lceil\\log_2(n!)\\right\\rceil</math> to the actual minimum number of comparisons (from {{OEIS2C|id=A036604}}) required to sort a list of ''n'' items (for the worst case). Below: Using [[Stirling's approximation]], this lower bound is well-approximated by <math>n \\log_2 n - \\frac{n}{\\ln 2}</math>.</div>\n</div>\n</div>\n\nThe number of comparisons that a comparison sort algorithm requires increases in proportion to <math>n\\log(n)</math>, where <math>n</math> is the number of elements to sort.  This bound is [[asymptotic computational complexity|asymptotically tight]].\n\nGiven a list of distinct numbers (we can assume this because this is a worst-case analysis), there are ''n'' [[factorial]] permutations exactly one of which is the list in sorted order. The sort algorithm must gain enough information from the comparisons to identify the correct permutation. If the algorithm always completes after at most ''f''(''n'') steps, it cannot distinguish more than 2<sup>''f''(''n'')</sup> cases because the keys are distinct and each comparison has only two possible outcomes. Therefore,\n:<math>2^{f(n)}\\geq n!</math>, or equivalently <math>f(n)\\geq\\log_2(n!).</math>\n\nBy looking at the first <math>n/2</math> factors of <math>n! = n (n-1) \\cdots  1</math>, we obtain\n:<math>\\log_2(n!) \\geq \\log_2\\left(\\left(\\frac{n}{2}\\right)^\\frac{n}{2}\\right) = \\frac{n}{2} \\log n - \\frac{n}{2} = \\Theta(n \\log n).</math>\n:<math>\\log_2(n!) =  \\Omega(n \\log n).</math>\n\nThis provides the lower-bound part of the claim. A better bound can be given via [[Stirling's approximation]].\n\nAn identical upper bound follows from the existence of the algorithms that attain this bound in the worst case, like [[heapsort]] and [[merge sort|mergesort]].\n\nThe above argument provides an ''absolute'', rather than only asymptotic lower bound on the number of comparisons, namely <math>\\left\\lceil\\log_2(n!)\\right\\rceil</math> comparisons. This lower bound is fairly good (it can be approached within a linear tolerance by a simple merge sort), but it is known to be inexact. For example, <math>\\left\\lceil\\log_2(13!)\\right\\rceil = 33</math>, but the minimal number of comparisons to sort 13 elements has been proved to be 34.\n\nDetermining the ''exact'' number of comparisons needed to sort a given number of entries is a computationally hard problem even for small ''n'', and no simple formula for the solution is known. For some of the few concrete values that have been computed, see {{OEIS2C|id=A036604}}.\n\n===Lower bound for the average number of comparisons===\nA similar bound applies to the average number of comparisons. Assuming that\n* all keys are distinct, i.e. every comparison will give either ''a''>''b'' or ''a''<''b'', and\n* the input is a random permutation, chosen uniformly from the set of all possible permutations of ''n'' elements,\nit is impossible to determine which order the input is in with fewer than {{math|log<sub>2</sub>(''n''!)}} comparisons on average.\n\nThis can be most easily seen using concepts from [[information theory]]. The [[Shannon entropy]] of such a random permutation is {{math|log<sub>2</sub>(''n''!)}} bits. Since a comparison can give only two results, the maximum amount of information it provides is 1 bit. Therefore, after ''k'' comparisons the remaining entropy of the permutation, given the results of those comparisons, is at least {{math|log<sub>2</sub>(''n''!)&nbsp;−&nbsp;''k''}} bits on average. To perform the sort, complete information is needed, so the remaining entropy must be 0. It follows that ''k'' must be at least {{math|log<sub>2</sub>(''n''!)}}.\n\nThis differs from the worst case argument given above, in that it does not allow rounding up to the nearest integer. For example, for {{math|1=''n''&nbsp;=&nbsp;3}}, the lower bound for the worst case is 3, the lower bound for the average case as shown above is approximately 2.58, while the highest lower bound for the average case is 8/3, approximately 2.67.{{clarify|date=August 2017|post-text=See [[Talk:Comparison sort#Clarification for section on Lower bound for the average number of comparisons|talk page]]}}\n\nIn the case that multiple items may have the same key, there is no obvious statistical interpretation for the term \"average case\", so an argument like the above cannot be applied without making specific assumptions about the distribution of keys.\n\n==Notes==\n<references/>\n\n==References==\n\n* [[Donald Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Second Edition. Addison-Wesley, 1997. {{ISBN|0-201-89685-0}}. Section 5.3.1: Minimum-Comparison Sorting, pp.&nbsp;180–197.\n\n{{sorting}}\n\n{{DEFAULTSORT:Comparison Sort}}\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Counting sort",
      "url": "https://en.wikipedia.org/wiki/Counting_sort",
      "text": "{{Infobox algorithm|class=[[Sorting Algorithm]]|data=[[Array data structure|Array]]|time=<math>O(n+k)</math>, where k is the range of the non-negative key values.|space=<math>O(n+k)</math>}}\n\nIn [[computer science]], '''counting sort''' is an [[algorithm]] for [[sorting algorithm|sorting]] a collection of objects according to keys that are small [[integer]]s; that is, it is an [[integer sorting]] algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum and minimum key values, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. However, it is often used as a subroutine in another sorting algorithm, [[radix sort]], that can handle larger keys more efficiently.<ref name=\"clrs\">{{citation\n | last1 = Cormen | first1 = Thomas H. | author1-link = Thomas H. Cormen\n | last2 = Leiserson | first2 = Charles E. | author2-link = Charles E. Leiserson\n | last3 = Rivest | first3 = Ronald L. | author3-link = Ron Rivest\n | last4 = Stein | first4 = Clifford | author4-link = Clifford Stein\n | contribution = 8.2 Counting Sort\n | edition = 2nd\n | isbn = 0-262-03293-7\n | pages = 168–170\n | publisher = [[MIT Press]] and [[McGraw-Hill]]\n | title = [[Introduction to Algorithms]]\n | year = 2001}}. See also the historical notes on page 181.</ref><ref name=\"edmonds\">{{citation|first=Jeff|last=Edmonds|contribution=5.2 Counting Sort (a Stable Sort)|pages=72–75|title=How to Think about Algorithms|publisher=Cambridge University Press|year=2008|isbn=978-0-521-84931-9}}.</ref><ref name=\"sedgewick\">{{citation|first=Robert|last=Sedgewick|authorlink=Robert Sedgewick (computer scientist)|contribution=6.10 Key-Indexed Counting|title=Algorithms in Java, Parts 1-4: Fundamentals, Data Structures, Sorting, and Searching|edition=3rd|publisher=Addison-Wesley|year=2003|pages=312–314}}.</ref>\n\nBecause counting sort uses key values as indexes into an array, it is not a [[comparison sort]], and the [[Big O notation#Family of Bachmann.E2.80.93Landau notations|Ω]](''n'' log ''n'') [[lower bound]] for comparison sorting does not apply to it.<ref name=\"clrs\"/> [[Bucket sort]] may be used for many of the same tasks as counting sort, with a similar time analysis; however, compared to counting sort, bucket sort requires [[linked list]]s, [[dynamic array]]s or a large amount of preallocated memory to hold the sets of items within each bucket, whereas counting sort instead stores a single number (the count of items) per bucket.<ref name=\"knuth\"/>\n\n==Input and output assumptions==\nIn the most general case, the input to counting sort consists of a [[collection (computing)|collection]] of {{mvar|n}} items, each of which has a non-negative integer key whose maximum value is at most {{mvar|k}}.<ref name=\"sedgewick\"/>\nIn some descriptions of counting sort, the input to be sorted is assumed to be more simply a sequence of integers itself,<ref name=\"clrs\"/> but this simplification does not accommodate many applications of counting sort. For instance, when used as a subroutine in [[radix sort]], the keys for each call to counting sort are individual digits of larger item keys; it would not suffice to return only a sorted list of the key digits, separated from the items.\n\nIn applications such as in radix sort, a bound on the maximum key value {{mvar|k}} will be known in advance, and can be assumed to be part of the input to the algorithm. However, if the value of {{mvar|k}} is not already known then it may be computed, as a first step, by an additional loop over the data to determine the maximum key value that actually occurs within the data.\n\nThe output is an [[Array data structure|array]] of the items, in order by their keys. Because of the application to radix sorting, it is important for counting sort to be a [[stable sort]]: if two items have the same key as each other, they should have the same relative position in the output as they did in the input.<ref name=\"clrs\"/><ref name=\"edmonds\"/>\n\n==The algorithm==\n\n '''function''' countingSort(array, k) '''is'''\n   count ← new array of k zeros\n   '''for''' i = 1 '''to''' length(array) '''do'''\n     count[array[i]] ← count[array[i]] + 1\n   '''for''' i = 2 '''to''' k '''do'''\n     count[i] ← count[i] + count[i - 1]\n   '''for''' i = length(array) '''downto''' 1 '''do'''\n     output[count[array[i]]] ← array[i]\n     count[array[i]] ← count[array[i]] - 1\n   '''return''' output\n\nHere <code>array</code> is the input array to be sorted, <code>k</code> is the range of the non-negative key values and <code>output</code> is the sorted array.\n\nIn summary, the algorithm loops over the items in the first loop, computing a [[histogram]] of the number of times each key occurs within the input collection. After that, it then performs a [[prefix sum]] computation on <code>count</code> to determine, for each key, the position range where the items having that key should be placed in; i.e. items of key <math>i</math> should be placed in the range of <math>(count_{i-1}, count_{i}]</math>. This is done through the second loop. Finally, it loops over the items again in the third loop, moving each item into its sorted position in the output array.<ref name=\"clrs\"/><ref name=\"edmonds\"/><ref name=\"sedgewick\"/>\nNote that after the second loop, <code>count[i]</code> now stores the number of items with key <math>\\leq i</math>, which is just the position the last item with key <math>i</math> should be stored. Since the third loop iterates the array from the back to the front, every item is then moved to the correct position.<ref name=\"clrs\"/><ref name=\"edmonds\"/><ref name=\"sedgewick\"/> The relative order of items with equal keys is preserved here; i.e., this is a [[:Category:Stable sorts|stable sort]].\n\n==Complexity analysis==\nBecause the algorithm uses only simple for loops, without recursion or subroutine calls, it is straightforward to analyze. The initialization of the count array, and the second for loop which performs a prefix sum on the count array, each iterate at most {{math|''k'' + 1}} times and therefore take {{math|''O''(''k'')}} time. The other two for loops, and the initialization of the output array, each take {{math|''O''(''n'')}} time. Therefore, the time for the whole algorithm is the sum of the times for these steps, {{math|''O''(''n'' + ''k'')}}.<ref name=\"clrs\"/><ref name=\"edmonds\"/>\n\nBecause it uses arrays of length {{math|''k'' + 1}} and {{mvar|n}}, the total space usage of the algorithm is also {{math|''O''(''n'' + ''k'')}}.<ref name=\"clrs\"/> For problem instances in which the maximum key value is significantly smaller than the number of items, counting sort can be highly space-efficient, as the only storage it uses other than its input and output arrays is the Count array which uses space {{math|''O''(''k'')}}.<ref>{{citation\n | last1 = Burris | first1 = David S.\n | last2 = Schember | first2 = Kurt\n | contribution = Sorting sequential files with limited auxiliary storage\n | doi = 10.1145/503838.503855\n | location = New York, NY, USA\n | pages = 23–31\n | publisher = ACM\n | title = Proceedings of the 18th annual Southeast Regional Conference\n | year = 1980| isbn = 0897910141\n }}.</ref>\n\n==Variant algorithms==\nIf each item to be sorted is itself an integer, and used as key as well, then the second and third loops of counting sort can be combined; in the second loop, instead of computing the position where items with key <code>i</code> should be placed in the output, simply append <code>Count[i]</code> copies of the number <code>i</code> to the output.\n\nThis algorithm may also be used to eliminate duplicate keys, by replacing the <code>Count</code> array with a [[bit vector]] that stores a <code>one</code> for a key that is present in the input and a <code>zero</code> for a key that is not present. If additionally the items are the integer keys themselves, both second and third loops can be omitted entirely and the bit vector will itself serve as output, representing the values as offsets of the non-<code>zero</code> entries, added to the range's lowest value. Thus the keys are sorted and the duplicates are eliminated in this variant just by being placed into the bit array.\n\nFor data in which the maximum key size is significantly smaller than the number of data items, counting sort may be [[parallel algorithm|parallelized]] by splitting the input into subarrays of approximately equal size, processing each subarray in parallel to generate a separate count array for each subarray, and then merging the count arrays. When used as part of a parallel radix sort algorithm, the key size (base of the radix representation) should be chosen to match the size of the split subarrays.<ref>{{citation\n | last1 = Zagha | first1 = Marco\n | last2 = Blelloch | first2 = Guy E. | author2-link = Guy Blelloch\n | contribution = Radix sort for vector multiprocessors\n | doi = 10.1145/125826.126164\n | pages = 712–721\n | publisher = IEEE Computer Society / ACM\n | title = Proceedings of Supercomputing '91, November 18-22, 1991, Albuquerque, NM, USA\n | year = 1991| isbn = 0897914597\n | url = http://www.cs.cmu.edu/~scandal/papers/cray-sort-supercomputing91.ps.gz}}.</ref> The simplicity of the counting sort algorithm and its use of the easily parallelizable prefix sum primitive also make it usable in more fine-grained parallel algorithms.<ref>{{citation\n | last = Reif | first = John H. | author-link = John Reif\n | contribution = An optimal parallel algorithm for integer sorting\n | doi = 10.1109/SFCS.1985.9\n | pages = 496–504\n | title = [[Symposium on Foundations of Computer Science|Proc. 26th Annual Symposium on Foundations of Computer Science (FOCS 1985)]]\n | year = 1985| isbn = 0-8186-0644-4 }}.</ref>\n\nAs described, counting sort is not an [[in-place algorithm]]; even disregarding the count array, it needs separate input and output arrays. It is possible to modify the algorithm so that it places the items into sorted order within the same array that was given to it as the input, using only the count array as auxiliary storage; however, the modified in-place version of counting sort is not stable.<ref name=\"sedgewick\"/>\n\n==History==\nAlthough radix sorting itself dates back far longer,\ncounting sort, and its application to radix sorting, were both invented by [[Harold H. Seward]] in 1954.<ref name=\"clrs\"/><ref name=\"knuth\">{{citation|first=D. E.|last=Knuth|authorlink=Donald Knuth|title=[[The Art of Computer Programming]], Volume 3: Sorting and Searching|edition=2nd|publisher=Addison-Wesley|year=1998|isbn=0-201-89685-0}}. Section 5.2, Sorting by counting, pp.&nbsp;75–80, and historical notes, p.&nbsp;170.</ref><ref>{{citation|first=H. H.|last=Seward|title=Information sorting in the application of electronic digital computers to business operations|series=Master's thesis, Report R-232|year=1954|publisher=[[Massachusetts Institute of Technology]], Digital Computer Laboratory|url=http://bitsavers.org/pdf/mit/whirlwind/R-series/R-232_Information_Sorting_in_the_Application_of_Electronic_Digital_Computers_to_Business_Operations_May54.pdf|contribution=2.4.6 Internal Sorting by Floating Digital Sort|pages=25–28}}.</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n{{Wikibooks|Algorithm implementation|Sorting/Counting sort|Counting sort}}\n* [http://www.cs.usfca.edu/~galles/visualization/CountingSort.html Counting Sort html5 visualization]\n* [http://users.cs.cf.ac.uk/C.L.Mumford/tristan/CountingSort.html Demonstration applet from Cardiff University]\n*{{citation|first=Art S.|last=Kagel|contribution=counting sort|title=Dictionary of Algorithms and Data Structures|editor-first=Paul E.|editor-last=Black|publisher=U.S. National Institute of Standards and Technology|date=2 June 2006|url=https://xlinux.nist.gov/dads/HTML/countingsort.html|accessdate=2011-04-21}}.\n* [http://www.codenlearn.com/2011/07/simple-counting-sort.html A simple Counting Sort implementation.]\n\n{{sorting}}\n\n{{DEFAULTSORT:Counting Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Stable sorts]]"
    },
    {
      "title": "Cubesort",
      "url": "https://en.wikipedia.org/wiki/Cubesort",
      "text": "{{notability|date=September 2014}}\n{{primary sources|date=September 2014}}\n\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|data=[[Array data structure|Array]]\n|time={{math|''O''(''n'' log ''n'')}}\n|space={{math|Θ(''n'')}}\n|optimal=?\n}}\n\n'''Cubesort''' is a parallel [[sorting algorithm]] that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.<ref>{{ cite web\n| title = Cubesort: A parallel algorithm for sorting N data items with S-sorters\n| url = http://www.sciencedirect.com/science/article/pii/0196677492900166\n| first1=Robert |last1=Cypher |first2=Jorge L.C |last2=Sanz\n| year = 1992\n}}</ref>\n\nA cubesort implementation written in C was published in 2014.<ref>{{ cite web\n| title = Cubesort\n| url = http://www.codeproject.com/Forums/326859/Algorithms.aspx?select=4846424&tid=4846424\n}}</ref>\n\n==Operation==\nCubesort's algorithm uses a specialized [[binary search]] on each axis to find the location to insert an element. When an axis grows too large it is split. Locality of reference is optimal as only four binary searches are performed on small arrays for each insertion. By using many small dynamic arrays the high cost for insertion on single large arrays is avoided.\n\n== References ==\n\n{{Reflist}}\n\n==External links==\n\n* [https://sites.google.com/site/binarysearchcube Cubesort description and implementation in C]\n* Algorithms and Computation: 7th International Symposium, ISAAC '96, Osaka ... edited by Tetsuo Asano et al, pp 187-188, https://books.google.com/books?id=vilOl8JCpFUC&pg=PA188&lpg=PA188&hl=en&f=false (passing mention)\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n[[Category:Online sorts]]\n\n{{algorithm-stub}}"
    },
    {
      "title": "Cycle sort",
      "url": "https://en.wikipedia.org/wiki/Cycle_sort",
      "text": "{{multiple issues|\n{{more footnotes|date=July 2017}}\n{{refimprove|date=July 2017}}\n}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Cyclesort.png|none|280px|Example of cycle sort sorting a list of random numbers.]]\n|caption=Example of cycle sort sorting a list of random numbers.\n|data=[[Array data structure|Array]]\n|time=&Theta;(''n''<sup>2</sup>)\n|best-time=&Theta;(''n''<sup>2</sup>)\n|average-time=&Theta;(''n''<sup>2</sup>)\n|space=&Theta;(''n'') total, &Theta;(''1'') auxiliary\n|optimal=No\n}}\n\n'''Cycle sort''' is an in-place, [[Sorting algorithm#Stability|unstable]] [[sorting algorithm]], a [[comparison sort]] that is theoretically optimal in terms of the total number of writes to the original [[Array data structure|array]], unlike any other in-place sorting algorithm. It is based on the idea that the [[permutation]] to be sorted can be factored into [[cyclic permutation|cycle]]s, which can individually be rotated to give a sorted result.\n\nUnlike nearly every other sort, items are ''never'' written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.\n\nMinimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with [[EEPROM]]s like [[Flash memory]] where [[Flash memory#Memory wear|each write reduces the lifespan of the memory]].\n\n== Algorithm ==\nTo illustrate the idea of cycle sort, consider a list with distinct elements. Given an element {{mono|a}}, we can find the index at which it will occur in the ''sorted list'' by simply counting the number of elements in the entire list that are smaller than {{mono|a}}. Now\n\n# If the element is already at the correct position, do nothing.\n# If it is not, we will write it to its intended position. That position is inhabited by a different element {{mono|b}}, which we then have to move to ''its'' correct position. This process of displacing elements to their correct positions continues until an element is moved to the original position of {{mono|a}}. This completes a cycle.\n\n[[File:Cyclesort-cycle.svg|2048x204px|frame|center|Displacement cycle for list \"bdeac\", when shifting the first letter ''b'' to its correct position:]]\n\nRepeating this process for every element sorts the list, with a single writing operation if and only if an element is not already at its correct position. While computing the correct positions takes <math>O(n)</math> time for every single element, thus resulting in a quadratic time algorithm, the number of writing operations is minimized.\n\n=== Implementation ===\n\nTo create a working implementation from the above outline, two issues need to be addressed:\n# When computing the correct positions, we have to make sure not to double-count the first element of the cycle.\n# If there are duplicate elements present, we could try to move an element {{mono|a}} to its correct position, which already happens to be inhabited by an {{mono|a}}. Simply swapping these would cause the algorithm to cycle indefinitely. Instead, we have to insert the element ''after any of its duplicates''.\n\nThe following [[Python (programming language)|Python]] implementation<ref>[[:sr:Ciklično sortiranje#Algoritam]]</ref>{{Circular reference|date=February 2018}} performs cycle sort on an array, counting the number of writes to that array that were needed to sort it.\n\n<source lang=\"python\">\n# Sort an array in place and return the number of writes.\ndef cycleSort(array):\n  writes = 0\n  \n  # Loop through the array to find cycles to rotate.\n  for cycleStart in range(0, len(array) - 1):\n    item = array[cycleStart]\n    \n    # Find where to put the item.\n    pos = cycleStart\n    for i in range(cycleStart + 1, len(array)):\n      if array[i] < item:\n        pos += 1\n    \n    # If the item is already there, this is not a cycle.\n    if pos == cycleStart:\n      continue\n    \n    # Otherwise, put the item there or right after any duplicates.\n    while item == array[pos]:\n      pos += 1\n    array[pos], item = item, array[pos]\n    writes += 1\n    \n    # Rotate the rest of the cycle.\n    while pos != cycleStart:\n      \n      # Find where to put the item.\n      pos = cycleStart\n      for i in range(cycleStart + 1, len(array)):\n        if array[i] < item:\n          pos += 1\n      \n      # Put the item there or right after any duplicates.\n      while item == array[pos]:\n        pos += 1\n      array[pos], item = item, array[pos]\n      writes += 1\n  \n  return writes\n</source>\n\n== Situation-specific optimizations ==\nWhen the array contains only duplicates of a relatively small number of items, a [[Time complexity#Constant time|constant-time]] [[perfect hash function]] can greatly speed up finding where to put an item{{ref|origpaper|1}}, turning the sort from Θ(''n''<sup>2</sup>) time to Θ(''n'' + ''k'') time, where ''k'' is the total number of hashes. The array ends up sorted in the order of the hashes, so choosing a hash function that gives you the right ordering is important.\n\nBefore the sort, create a [[histogram]], sorted by hash, counting the number of occurrences of each hash in the array. Then create a table with the cumulative sum of each entry in the histogram. The cumulative sum table will then contain the position in the array of each element. The proper place of elements can then be found by a constant-time hashing and cumulative sum table lookup rather than a [[linear search]].\n\n==References==\n{{Reflist}}\n\n== External links ==\n{{note|origpaper}} [http://comjnl.oxfordjournals.org/content/33/4/365.full.pdf+html \"Cycle-Sort: A Linear Sorting Method\", The Computer Journal (1990) 33 (4): 365-367.]\n* [https://stackoverflow.com/questions/3623509/how-to-sort-an-array-using-minimum-number-of-writes/3852666#3852666 Original source of unrestricted variant]\n* [https://corte.si/posts/code/cyclesort/index.html Cyclesort - a curious little sorting algorithm]\n\n{{sorting}}\n\n{{DEFAULTSORT:Cycle Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Dutch national flag problem",
      "url": "https://en.wikipedia.org/wiki/Dutch_national_flag_problem",
      "text": "{{for|the article about the Dutch national flag|Flag of the Netherlands}}\n{{refimprove|date=July 2014}}\n[[File:Flag of the Netherlands.svg|thumb|The Dutch national flag]]\nThe '''Dutch national flag problem''' ('''DNF''')<ref name=\"monash\"/> is a [[computer science]] programming problem proposed by [[Edsger Dijkstra]].<ref>In a chapter of his book ''A Discipline of Programming'' Prentice-Hall, 1976</ref> The [[flag of the Netherlands]] consists of three colors: red, white and blue. Given balls of these three colors arranged randomly in a line (the actual number of balls does not matter), the task is to arrange them such that all balls of the same color are together and their collective color groups are in the correct order.\n\nThe solution to this problem is of interest for designing [[sorting algorithm]]s; in particular, variants of the [[quicksort]] algorithm that must be [[Quicksort#Repeated elements|robust to repeated elements]] may use a three-way partitioning function that groups items less than a given key (red), equal to the key (white) and greater than the key (blue). Several solutions exist that have varying performance characteristics, tailored to sorting arrays with either small or large numbers of repeated elements.<ref>The latter case occurs in [[string (computer science)|string]] sorting with [[multi-key quicksort]]. {{Cite journal | doi = 10.1016/j.ipl.2009.01.007| title = Improving multikey Quicksort for sorting strings with many equal elements| journal = Information Processing Letters| volume = 109| issue = 9| pages = 454–459| year = 2009| last1 = Kim | first1 = Eunsang| last2 = Park | first2 = Kunsoo}}</ref>\n\n== The array case ==\nThis problem can also be viewed in terms of rearranging elements of an [[Array data structure|array]].\nSuppose each of the possible elements could be classified into exactly one of three categories (bottom, middle, and top).\nFor example, if all the elements are in 0 ... 1, the bottom could be defined as elements in 0 ... 0.1 (not including 0.1), the middle as 0.1 ... 0.3 (not including 0.3)\nand the top as 0.3 and greater.  (The choice of these values illustrates that the categories need not be equal ranges). The problem is then to produce an array such that all \"bottom\" elements come before (have an index less than the index of) all \"middle\" elements, which come before all \"top\" elements.\n\nOne [[algorithm]] is to have the top group grow down from the top of the array, the bottom group grow up from the bottom, and keep the middle group just above the bottom. The algorithm indexes three locations, the bottom of the top group, the top of the bottom group, and the top of the middle group. Elements that are yet to be sorted fall between the middle and the top group.<ref name=NIST>{{DADS|Dutch national flag|DutchNationalFlag}}</ref> At each step, examine the element just above the middle. If it belongs to the top group, swap it with the element just below the top. If it belongs in the bottom, swap it with the element just above the bottom. If it is in the middle, leave it. Update the appropriate index. Complexity is Θ(n) moves and examinations.<ref name=\"monash\">{{Cite web|url=http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Sort/Flag/\n|title=Dutch National Flag problem and algorithm|publisher=Faculty of Information Technology (Clayton), Monash University, Australia|date=1998}}</ref>\n\n===Pseudocode===\nThe following [[pseudocode]] for three-way partitioning assumes zero-based array indexing. It uses three indices {{mvar|i}}, {{mvar|j}} and {{mvar|n}}, maintaining the [[loop invariant|invariant]] that {{math|''i'' ≤ ''j''}}.\n \n{{mvar|n}} holds the lower boundary of numbers greater than {{mvar|mid}}. \n\n{{mvar|j}} is the position of the number under consideration. And {{mvar|i}} is the boundary for the numbers less than the {{mvar|mid}} value.\n '''procedure''' three-way-partition(A : array of values, mid : value):\n     i ← 0\n     j ← 0\n     n ← '''size of''' A - 1\n \n     '''while''' j ≤ n:\n         '''if''' A[j] < mid:\n             '''swap''' A[i] and A[j]\n             i ← i + 1\n             j ← j + 1\n         '''else if''' A[j] > mid:\n             '''swap''' A[j] and A[n]\n             n ← n - 1\n         '''else''':\n             j ← j + 1\n\nNote that {{mvar|j}} will be greater than {{mvar|i}} only if the {{mvar|mid}} is hit.\n\n== See also ==\n*[[American flag sort]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Sort/Flag/ Explanation and interactive explanatory execution of the algorithm], sorting two or three colors\n\n[[Category:Sorting algorithms]]\n[[Category:Computational problems]]\n[[Category:Edsger W. Dijkstra]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Elevator algorithm",
      "url": "https://en.wikipedia.org/wiki/Elevator_algorithm",
      "text": "{{Refimprove|date=November 2007}}\n\nThe '''elevator algorithm''' (also '''SCAN''') is a [[hard disk|disk]]-[[I/O scheduling|scheduling]] algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\n\nThis algorithm is named after the behavior of a building [[elevator]], where the elevator continues to travel in its current direction (up or down) until empty, stopping only to let individuals off or to pick up new individuals heading in the same direction.  \n\nFrom an implementation perspective, the [[disk drive|drive]] maintains a [[data buffer|buffer]] of pending read/write requests, along with the associated [[Cylinder (disk drive)|cylinder]] number of the request. (Lower cylinder numbers generally indicate that the cylinder is closer to the spindle, and higher numbers indicate the cylinder is farther away.)\n\n== Description ==\n\nWhen a new request arrives while the drive is idle, the initial arm/head movement will be in the direction of the cylinder where the data is stored, either ''in'' or ''out''.  As additional requests arrive, requests are serviced only in the current direction of arm movement until the arm reaches the edge of the disk. When this happens, the direction of the arm reverses, and the requests that were remaining in the opposite direction are serviced, and so on.<ref>{{cite web |url=http://www.dcs.ed.ac.uk/home/stg/pub/D/disk.html  |title=Disk scheduling |accessdate=2008-01-21 |archiveurl = https://web.archive.org/web/20080606005055/http://www.dcs.ed.ac.uk/home/stg/pub/D/disk.html |archivedate = 2008-06-06}}</ref>\n\n== Variations ==\nOne variation of this method ensures all requests are serviced in only one direction, that is, once the head has arrived at the outer edge of the disk, it returns to the beginning and services the new requests in this one direction only (or vice versa).  This is known as the \"Circular Elevator Algorithm\" or C-SCAN.  Although the time of the return seek is wasted, this results in more equal performance for all head positions, as the expected distance from the head is always half the maximum distance, unlike in the standard elevator algorithm where cylinders in the middle will be serviced as much as twice as often as the innermost or outermost cylinders.\n\nOther variations include:\n*[[FSCAN]]\n*[[LOOK algorithm|LOOK]] (and '''C-LOOK''')\n*[[N-Step-SCAN]]\n\n== Example ==\nThe following is an example of how to calculate average disk seek times for both the SCAN and C-SCAN algorithms.\n\n* Example list of pending disk requests (listed by track number): 100, 50, 10, 20, 75.\n* The starting track number for the examples will be 35.\n* The list will need to be sorted in ascending order: 10, 20, 50, 75, 100.\n\nBoth SCAN and C-SCAN behave in the same manner until they reach the last track queued. For the sake of this example let us assume that the SCAN algorithm is currently going from a lower track number to a higher track number (like the C-SCAN is doing). For both methods, one takes the difference in magnitude (i.e. absolute value) between the next track request and the current track.\n\n* '''Seek 1:''' 50 − 35 = 15\n*  '''Seek 2:''' 75 − 50 = 25\n*  '''Seek 3:''' 100 − 75 = 25\n\nAt this point both have reached the highest (end) track request. SCAN will just reverse direction and service the next closest disk request (in this example, 20) and C-SCAN will always go back to track 0 and start going to higher track requests.\n\n*  '''Seek 4 (SCAN):''' 20 − 100 = 80\n*  '''Seek 5 (SCAN):''' 10 − 20 = 10\n*  '''Total (SCAN):''' 155\n*  '''Average (SCAN):''' 155 ÷ 5 = 31\n*  '''**Seek 4 (C-SCAN):''' 0 − 100 = 0 head movement as cylinders are treated as a  circular list (C-SCAN always goes back to the first track)\n*  '''Seek 5 (C-SCAN):''' 10 − 0 = 10\n*  '''Seek 6 (C-SCAN):''' 20 − 10 = 10\n*  '''Total (C-SCAN):''' 85\n*  '''Average (C-SCAN):''' 85 ÷ 5 = 17\n\n''Note:'' Even though six seeks were performed using the C-SCAN algorithm, only five I/Os were actually done.\n\n''Definition of C-SCAN:'' C-SCAN moves the head from one end of the Disk to the other end, servicing requests along the way. The head on reaching the other end, however immediately returns to the beginning of the Disk without servicing any requests on the return.\n**The huge jump from one end to the other is not considered as a head movement as cylinders are treated as a circular list.\n\n== Analysis ==\nThe arm movement is thus always less than twice the number of total cylinders then, for both versions of the elevator algorithm. The variation has the advantage to have a smaller variance in response time. The algorithm is also relatively simple. \n\nHowever, the elevator algorithm is not always better than [[shortest seek first]], which is slightly closer to optimal, but can result in high variance in response time and even in [[Resource starvation|starvation]] when new requests continually get serviced prior to existing requests. \n\nAnti-starvation techniques can be applied to the shortest seek time first algorithm to guarantee an optimum response time.\n\n==See also==\n*[[FIFO (computing and electronics)|FCFS]]\n*[[Shortest seek first|Shortest seek time first]]\n\n== References ==\n<references />\n[[Category:Disk scheduling algorithms]]\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "External sorting",
      "url": "https://en.wikipedia.org/wiki/External_sorting",
      "text": "'''External sorting''' is a class of [[sorting]] [[algorithm]]s that can handle massive amounts of [[data]]. External sorting is required when the data being sorted do not fit into the [[main memory]] of a computing device (usually [[RAM]]) and instead they must reside in the slower [[Auxiliary memory|external memory]], usually a [[hard disk drive]]. Thus, external sorting algorithms are [[external memory algorithm]]s and thus applicable in the [[external memory model|external memory]] [[model of computation]]. \n\nExternal sorting algorithms generally fall into two types, distribution sorting, which resembles [[quicksort]], and external merge sort, which resembles [[merge sort]]. The latter typically uses a [[hybrid algorithm|hybrid]] sort-merge strategy.  In the sorting phase, chunks of data small enough to fit in main memory are read, sorted, and written out to a temporary file.  In the merge phase, the sorted subfiles are combined into a single larger file.\n\n== Model ==\n{{See also|External memory model}}\nExternal sorting algorithms can be analyzed in the [[external memory model]]. In this model, a [[Cache (computing)|cache]] or internal memory of size {{mvar|M}} and an unbounded external memory are divided into [[Blocking (data storage)|blocks]] of size {{mvar|B}}, and the [[running time]] of an algorithm is determined by the number of memory transfers between internal and external memory. Like their [[cache-oblivious algorithm|cache-oblivious]] counterparts, [[asymptotically optimal]] external sorting algorithms achieve a [[running time]] (in [[Big O notation]]) of <math>O \\left(\\tfrac{N}{B}\\log_{\\tfrac{M}{B}} \\tfrac{N}{B} \\right)</math>.\n\n== External merge sort ==\nOne example of external sorting is the external [[merge sort]] algorithm, which is a [[K-way merge algorithm]]. It sorts chunks that each fit in RAM, then merges the sorted chunks together.<ref>[[Donald Knuth]], ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Second Edition. Addison-Wesley, 1998, {{ISBN|0-201-89685-0}}, Section 5.4: External Sorting, pp.248–379.</ref><ref>[[Ellis Horowitz]] and [[Sartaj Sahni]], ''Fundamentals of Data Structures'', H. Freeman & Co., {{ISBN|0-7167-8042-9}}.</ref>\n\nThe algorithm first sorts {{mvar|M}} items at a time and puts the sorted lists back into external memory. It then [[recursion|recursively]] does a [[K-way merge algorithm|<math>\\tfrac{M}{B}</math>-way merge]] on those sorted lists. To do this merge, {{mvar|B}} elements from each sorted list are loaded into internal memory, and the minimum is repeatedly outputted.\n\nFor example, for sorting 900 [[megabyte]]s of data using only 100 megabytes of RAM:\n# Read 100 MB of the data in main memory and sort by some conventional method, like [[quicksort]].\n# Write the sorted data to disk.\n# Repeat steps 1 and 2 until all of the data is in sorted 100 MB chunks (there are 900MB / 100MB = 9 chunks), which now need to be merged into one single output file.\n# Read the first 10 MB (= 100MB / (9 chunks + 1)) of each sorted chunk into input buffers in main memory and allocate the remaining 10 MB for an output buffer.  (In practice, it might provide better performance to make the output buffer larger and the input buffers slightly smaller.)\n# Perform a [[k-way merging|9-way merge]] and store the result in the output buffer. Whenever the output buffer fills, write it to the final sorted file and empty it. Whenever any of the 9 input buffers empties, fill it with the next 10 MB of its associated 100 MB sorted chunk until no more data from the chunk is available. This is the key step that makes external merge sort work externally -- because the merge algorithm only makes one pass sequentially through each of the chunks, each chunk does not have to be loaded completely; rather, sequential parts of the chunk can be loaded as needed.\n\nHistorically, instead of a sort, sometimes a replacement-selection algorithm<ref>[[Donald Knuth]], ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'', Second Edition. Addison-Wesley, 1998, {{ISBN|0-201-89685-0}}, Section 5.4: External Sorting, pp.254–ff.</ref> was used to perform the initial distribution, to produce on average half as many output chunks of double the length.\n\n===Additional passes===\nThe previous example is a two-pass sort: first sort, then merge. The sort ends with a single ''k''-way merge, rather than a series of two-way merge passes as in a typical in-memory merge sort. This is because each merge pass reads and writes ''every value'' from and to disk.\n\nThe limitation to single-pass merging is that as the number of chunks increases, memory will be divided into more buffers, so each buffer is smaller. This causes many smaller reads rather than fewer larger ones. Thus, for sorting, say, 50 GB in 100 MB of RAM, using a single merge pass isn't efficient: the disk seeks to fill the input buffers with data from each of the 500 chunks (we read 100MB / 501 ~ 200KB from each chunk at a time) take up most of the sort time. Using two merge passes solves the problem. Then the sorting process might look like this:\n\n# Run the initial chunk-sorting pass as before.\n# Run a first merge pass combining 25 chunks at a time, resulting in 20 larger sorted chunks.\n# Run a second merge pass to merge the 20 larger sorted chunks.\n\nLike in-memory sorts, efficient external sorts require [[Big O notation|O]](''n'' log ''n'') time: linear increases in data size require logarithmic increases in the number of passes, and each pass takes a linear number of reads and writes. Using the large memory sizes provided by modern computers the logarithmic factor grows very slowly. Under reasonable assumptions at least 500 GB of data can be sorted using 1 GB of main memory before a third pass becomes advantageous, and  many times that much data can be sorted before a fourth pass becomes useful.<ref>Assume a single disk with 200 MB/s transfer, 20 ms seek time, 1 GB of buffers, 500 GB to sort. The merging phase will have 500 buffers of 2M each, need to do 250K seeks and read then write 500 GB. It will spend 5,000 sec seeking and 5,000 sec transferring. Doing two passes as described above would nearly eliminate the seek time but add an additional 5,000 sec reading and writing, so this is approximately the break-even point between a two-pass and three-pass sort.</ref> Low-seek-time media like [[solid-state drive]]s (SSDs) also increase the amount that can be sorted before additional passes improve performance.\n\nMain memory size is important. Doubling memory dedicated to sorting halves the number of chunks and the number of reads per chunk, reducing the number of seeks required by about three-quarters. The ratio of RAM to disk storage on servers often makes it convenient to do huge sorts on a cluster of machines<ref>Chris Nyberg, Mehul Shah, [http://sortbenchmark.org/ Sort Benchmark Home Page] (links to examples of parallel sorts)</ref> rather than on one machine with multiple passes.\n\n== External distribution sort ==\nExternal distribution sort is analogous to [[quicksort]]. The algorithm finds approximately <math>\\tfrac{M}{B}</math> pivots and uses them to divide the {{mvar|N}} elements into approximately equally sized subarrays, each of whose elements are all smaller than the next, and then recurse until the sizes of the subarrays are less than the [[blocking (data storage)|block size]]. When the subarrays are less than the block size, sorting can be done quickly because all reads and writes are done in the [[cache (computing)|cache]], and in the [[external memory model]] requires <math>O(1)</math> operations.\n\nHowever, finding exactly <math>\\tfrac{M}{B}</math> pivots would not be fast enough to make the external distribution sort [[asymptotically optimal]]. Instead, we find slightly less pivots. To find these pivots, the algorithm splits the {{mvar|N}} input elements into <math>\\tfrac{N}{M}</math> chunks, and takes every <math>\\sqrt{\\tfrac{M}{16B}}</math> elements, and [[recursion|recursively]] uses the [[median of medians]] algorithm to find <math>\\sqrt{\\tfrac{M}{B}}</math> pivots.<ref name=\"Aggarwal88\">{{cite journal|last1=Aggarwal|first1=Alok|last2=Vitter|first2=Jeffrey|author2-link=Jeffrey Vitter|title=The input/output complexity of sorting and related problems|journal=[[Communications of the ACM]]|volume=31|issue=9|pages=1116–1127|date=1988|doi=10.1145/48529.48535}}</ref>\n\nThere is a [[Duality (mathematics)|duality]], or fundamental similarity, between merge- and distribution-based algorithms.<ref>[[J. S. Vitter]], ''[http://www.ittc.ku.edu/~jsv/Papers/Vit.IO_book.pdf Algorithms and Data Structures for External Memory]'', Series on Foundations and Trends in Theoretical Computer Science, now Publishers, Hanover, MA, 2008, {{ISBN|978-1-60198-106-6}}.</ref>\n\n== Performance ==\nThe [http://sortbenchmark.org/ Sort Benchmark], created by computer scientist [[Jim Gray (computer scientist)|Jim Gray]], compares external sorting algorithms implemented using finely tuned hardware and software.  Winning implementations use several techniques:\n\n* '''Using parallelism'''\n** Multiple disk drives can be used in parallel in order to improve sequential read and write speed.  This can be a very cost-efficient improvement: a Sort Benchmark winner in the cost-centric Penny Sort category uses six hard drives in an otherwise midrange machine.<ref>Nikolas Askitis, [http://sortbenchmark.org/ozsort-2010.pdf OzSort 2.0: Sorting up to 252GB for a Penny]</ref>\n** Sorting software can use [[Thread (computer science)|multiple threads]], to speed up the process on modern multicore computers.\n** Software can use [[asynchronous I/O]] so that one run of data can be sorted or merged while other runs are being read from or written to disk.\n** Multiple machines connected by fast network links can each sort part of a huge dataset in parallel.<ref>Rasmussen et al., [http://sortbenchmark.org/tritonsort_2010_May_15.pdf TritonSort]</ref>\n* '''Increasing hardware speed'''\n** Using more RAM for sorting can reduce the number of disk seeks and avoid the need for more passes.\n** Fast external memory like [[solid-state drives]] can speed sorts, either if the data is small enough to fit entirely on SSDs or, more rarely, to accelerate sorting SSD-sized chunks in a three-pass sort.\n** ''Many'' other factors can affect hardware's maximum sorting speed: CPU speed and number of cores, RAM access latency, input/output bandwidth, disk read/write speed, disk seek time, and others. \"Balancing\" the hardware to minimize bottlenecks is an important part of designing an efficient sorting system.\n** Cost-efficiency as well as absolute speed can be critical, especially in cluster environments where lower node costs allow purchasing more nodes.\n* '''Increasing software speed'''\n** Some Sort Benchmark entrants use a variation on [[radix sort]] for the first phase of sorting: they separate data into one of many \"bins\" based on the beginning of its value.  Sort Benchmark data is random and especially well-suited to this optimization.\n** Compacting the input, intermediate files, and output can reduce time spent on I/O, but is not allowed in the Sort Benchmark.\n** Because the Sort Benchmark sorts long (100-byte) records using short (10-byte) keys, sorting software sometimes rearranges the keys separately from the values to reduce memory I/O volume.\n\n==References==\n{{reflist|30em}}\n\n==See also==\n* [[Mainframe sort merge]]\n* [[External memory algorithm]]\n* [[Funnelsort]]\n* [[Cache-oblivious distribution sort]]\n\n==External links==\n*[http://stxxl.sourceforge.net/ STXXL, an algorithm toolkit including external mergesort]\n*[http://cis.stvincent.edu/html/tutorials/swd/extsort/extsort.html An external mergesort example]\n*[http://code.google.com/p/kway A K-Way Merge Implementation]\n*[http://code.google.com/p/externalsortinginjava/ External-Memory Sorting in Java]\n*[http://code.google.com/p/judyarray A sample pennysort implementation using Judy Arrays]\n*[http://sortbenchmark.org/ Sort Benchmark]\n\n[[Category:Sorting algorithms]]\n[[Category:External memory algorithms]]"
    },
    {
      "title": "Flashsort",
      "url": "https://en.wikipedia.org/wiki/Flashsort",
      "text": "{{short description|O(n) sorting algorithm}}\n'''Flashsort''' is a [[Sorting algorithm#Distribution sort|distribution sorting]] algorithm showing [[O notation|linear computational complexity <math>O(n)</math>]] for uniformly distributed data sets and relatively little additional memory requirement. The original work was published in 1998 by Karl-Dietrich Neubert.<ref name=\"neubert_journal\">{{cite journal |last=Neubert |first=Karl-Dietrich |date=February 1998 |title=The Flashsort Algorithm |journal=Dr. Dobb's Journal |pages=123 |url=http://www.ddj.com/architect/184410496 |accessdate=2007-11-06}}</ref>\n\n== Concept ==\nThe basic idea behind flashsort is that in a data set with a known [[Probability distribution|distribution]], it is easy to immediately estimate where an element should be placed after sorting when the range of the set is known. For example, if given a uniform data set where the minimum is 1 and the maximum is 100 and 50 is an element of the set, it’s reasonable to guess that 50 would be near the middle of the set after it is sorted. This approximate location is called a class. If numbered 1 to <math>m</math>, the class of an item <math>A_i</math> is the [[quantile]], computed as:\n\n&nbsp;&nbsp;<math>K(A_i) = 1+\\textrm{INT}\\left((m-1)\\frac{A_i-A_\\textrm{min}}{A_\\textrm{max}-A_\\textrm{min}}\\right)</math>\n\nwhere <math>A</math> is the input set. The range covered by every class is equal, except the last class which includes only the maximum(s). The classification ensures that every element in a class is greater than any element in a lower class. This partially orders the data and reduces the number of inversions. Insertion sort is then applied to the classified set. As long as the data is uniformly distributed, class sizes will be consistent and insertion sort will be computationally efficient.<ref name=\"neubert_journal\" />\n\n== Memory efficient implementation ==\nTo execute flashsort with its low memory benefits, the algorithm does not use additional data structures to store the classes. Instead it stores the upper bounds of each class on the input array <math>A</math> in an auxiliary vector <math>L</math>. These upper bounds are obtained by counting the number of elements in each class, and the upper bound of a class is the number of elements in that class and every class before it. These bounds serve as pointers into the classes.\n\nClassification is implemented through a series of cycles, where a cycle-leader is taken from the input array <math>A</math> and its class is calculated. The pointers in vector <math>L</math> are used to insert the cycle-leader into the correct class, and the class’s pointer in <math>L</math> is decremented after each insertion. Inserting the cycle-leader will evict another element from array <math>A</math>, which will be classified and inserted into another location and so on. The cycle terminates when an element is inserted into the cycle-leader’s starting location.\n\nAn element is a valid cycle-leader if it has not yet been classified. As the algorithm iterates on array <math>A</math>, previously classified elements are skipped and unclassified elements are used to initiate new cycles. It is possible to discern whether an element has been classified or not without using additional tags: An element has been classified if and only if its index is greater than the class’s pointer value in <math>L</math>. To prove this, consider the current index of array <math>A</math> the algorithm is processing. Let this index be <math>i</math>. Elements <math>A_0</math> through <math>A_\\textrm{i-1}</math> have already been classified and inserted into the correct class. Suppose that <math>i</math> is greater than the current pointer to <math>A_i</math>’s class. Now suppose that the <math>A_i</math> is unclassified and could be legally inserted into the index indicated by its class pointer, which would replace a classified element in another class. This is impossible since the initial pointers of each class are their upper bounds, which ensures that the exact needed amount of space is allocated for each class on the array <math>A</math>. Therefore, every element in <math>A_i</math>’s class, including <math>A_i</math> itself, has already been classified. Also, if an element has already been classified, the class’s pointer would have been decremented below the element’s new index.<ref name=\"neubert_journal\" /><ref name=\"neubert_code\">{{cite web |url=http://www.neubert.net/FSOIntro.html |title=The FlashSort Algorithm |accessdate=2007-11-06 |author=Karl-Dietrich Neubert |year=1998}}</ref>\n\n== Performance ==\nThe only extra memory requirements are the auxiliary vector <math>L</math> for storing class bounds and the constant number of other variables used.\n\nIn the ideal case of a balanced data set, each class will be approximately the same size, and sorting an individual class by itself has complexity <math>O(1)</math>. If the number <math>m</math> of classes is proportional to the input set size <math>n</math>, the running time of the final insertion sort is <math>m \\cdot O(1) = O(m) = O(n)</math>. In the worst-case scenarios where almost all the elements are in a few or one class, the complexity of the algorithm as a whole is limited by the performance of the final-step sorting method. For insertion sort, this is <math>O(n^2)</math>. Variations of the algorithm improve worst-case performance by using better-performing sorts such as quicksort or recursive flashsort on classes that exceed a certain size limit.<ref name=\"neubert_code\" /><ref>{{cite web|url=http://jea.acm.org/ARTICLES/Vol5Nbr3/node4.html |title=Cache-Effective Quicksort |accessdate=2007-11-06 |author1=Li Xiao |author2=Xiaodong Zhang |author3=Stefan A. Kubricht |work=Improving Memory Performance of Sorting Algorithms |publisher=Department of Computer Science, College of William and Mary, Williamsburg, VA 23187-8795 |archiveurl=https://web.archive.org/web/20071102070431/http://jea.acm.org/ARTICLES/Vol5Nbr3/node4.html |archivedate=2007-11-02 |deadurl=yes |df= }}</ref>\n\nChoosing a value for <math>m</math>, the number of classes, trades off time spent classifying elements (high <math>m</math>) and time spent in the final insertion sort step (low <math>m</math>). Based on his research, Neubert found <math>m=0.42n</math> to be optimal.\n\nMemory-wise, flashsort avoids the overhead needed to store classes in the very similar bucketsort. For <math>m=0.1n</math> with uniform random data, flashsort is faster than heapsort for all <math>n</math> and faster than quicksort for <math>n>80</math>. It becomes about as twice as fast as quicksort at <math>n=10000</math>.<ref name=\"neubert_journal\" />\n\nDue to the ''in situ'' permutation that flashsort performs in its classification process, flashsort is not [[Stable sort#Stability|stable]]. If stability is required, it is possible to use a second, temporary, array so elements can be classified sequentially. However, in this case, the algorithm will require <math>O(n)</math> space.\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://citeseer.ist.psu.edu/91922.html Implementations of Randomized Sorting on Large Parallel Machines (1992)]\n* [http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA253638 Implementation of Parallel Algorithms (1992)]\n* [http://home.westman.wave.ca/~rhenry/sort/#flashsort Visualization of Flashsort]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Funnelsort",
      "url": "https://en.wikipedia.org/wiki/Funnelsort",
      "text": "{{Multiple issues|{{more footnotes|date=May 2014}}{{primary sources|date=May 2014}}{{technical|date=May 2014}}}}\n\n'''Funnelsort''' is a [[comparison sort|comparison-based]] [[sorting algorithm]]. It is similar to [[mergesort]], but it is a [[cache-oblivious algorithm]], designed for a setting where the number of elements to sort is too large to fit in a [[cache (computing)|cache]] where operations are done. It was introduced by Matteo Frigo, [[Charles Leiserson]], [[Harald Prokop]], and Sridhar Ramachandran in 1999 in the context of the [[Cache-oblivious algorithm|cache oblivious model]].<ref>M. Frigo, C.E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In ''Proceedings of the 40th IEEE Symposium on Foundations of Computer Science'' (FOCS 99), pp. 285-297. 1999. [http://ieeexplore.ieee.org/iel5/6604/17631/00814600.pdf?arnumber=814600 Extended abstract at IEEE], [http://citeseer.ist.psu.edu/307799.html at Citeseer].</ref><ref>Harald Prokop. [http://supertech.csail.mit.edu/papers/Prokop99.pdf Cache-Oblivious Algorithms]. Masters thesis, MIT. 1999.</ref>\n\n== Mathematical properties ==\n\nIn the [[external memory model]], the number of memory transfers it needs to perform a sort of <math>N</math> items on a machine with cache of size <math>Z</math> and cache lines of length <math>L</math> is <math> O \\left(\\tfrac{N}{L} \\log_{Z} N \\right)</math>, under the tall cache assumption that <math>Z = \\Omega(L^2)</math>. This number of memory transfers has been shown to be [[asymptotically optimal]] for comparison sorts. Funnelsort also achieves the asymptotically optimal runtime complexity of <math>\\Theta(N \\log N)</math>.\n\n== Algorithm ==\n\n=== Basic overview ===\nFunnelsort operates on a contiguous array of <math>N</math> elements. To sort the elements, it performs the following:\n\n# Split the input into <math>N^{1/3}</math> arrays of size <math>N^{2/3}</math>, and sort the arrays recursively.\n# Merge the <math>N^{1/3}</math> sorted sequences using a <math>N^{1/3}</math>-merger. (This process will be described in more detail.)\n\nFunnelsort is similar to [[merge sort]] in that some number of subarrays are recursively sorted, after which a merging step combines the subarrays into one sorted array. Merging is performed by a device called a k-merger, which is described in the section below.\n\n=== k-mergers ===\nA k-merger takes <math>k</math> sorted sequences. Upon one invocation of a k-merger, it outputs the first <math>k^3</math> elements of the sorted sequence obtained by merging the k input sequences.\n\nAt the top level, funnelsort uses a <math>N^{1/3}</math>-merger on <math>N^{1/3}</math> sequences of length <math>N^{2/3}</math>, and invokes this merger once.\n\nThe k-merger is built recursively out of <math>\\sqrt{k}</math>-mergers. It consists of <math>\\sqrt{k}</math> input <math>\\sqrt{k}</math>-mergers <math>I_1, I_2, \\ldots, I_\\sqrt{k}</math>, and a single output <math>\\sqrt{k}</math>-merger <math>O</math>.\nThe k inputs are separated into <math>\\sqrt{k}</math> sets of <math>\\sqrt{k}</math> inputs each. Each of these sets is an input to one of the input mergers. The output of each input merger is connected to a buffer, a [[FIFO (computing and electronics)|FIFO]] [[Queue (data structure)|queue]] that can hold <math>2k^{3/2}</math> elements. The buffers are implemented as [[Circular buffer|circular queues]].\nThe outputs of the <math>\\sqrt{k}</math> buffers are connected to the inputs of the output merger <math>O</math>. Finally, the output of <math>O</math> is the output of the entire k-merger.\n\nIn this construction, any input merger only outputs <math>k^{3/2}</math> items at once, but the buffer it outputs to has double the space. This is done so that an input merger can be called only when its buffer does not have enough items, but that when it is called, it outputs a lot of items at once (namely, <math>k^{3/2}</math> of them).\n\nA k-merger works recursively in the following way. To output <math>k^3</math> elements, it recursively invokes its output merger <math>k^{3/2}</math> times. However, before it makes a call to <math>O</math>, it checks all of its buffers, filling each of them that are less than half full. To fill the i-th buffer, it recursively invokes the corresponding input merger <math>I_i</math> once. If this cannot be done (due to the merger running out of inputs), this step is skipped. Since this call outputs <math>k^{3/2}</math> elements, the buffer contains at least <math>k^{3/2}</math> elements. At the end of all these operations, the k-merger has output the first <math>k^3</math> of its input elements, in sorted order.\n<!-- add image that is not copyrighted? -->\n\n== Analysis ==\nMost of the analysis of this algorithm revolves around analyzing the space and cache miss complexity of the k-merger.\n\nThe first important bound is that a k-merger can be fit in <math>O(k^2)</math> space. To see this, we let <math>S(k)</math> denote the space needed for a k-merger. To fit the <math>k^{1/2}</math> buffers of size <math>2k^{3/2}</math> takes <math>O(k^2)</math> space. To fit the <math>\\sqrt{k} + 1</math> smaller buffers takes <math>(\\sqrt{k} + 1) S(\\sqrt{k})</math> space. Thus, the space satisfies the recurrence <math> S(k) = (\\sqrt{k} + 1) S(\\sqrt{k}) + O(k^2)</math>. This recurrence has solution <math>S(k) = O(k^2)</math>.\n\nIt follows that there is a positive constant <math>\\alpha</math> such that a problem of size at most <math>\\alpha \\sqrt{Z}</math> fits entirely in cache, meaning that it incurs no additional cache misses.\n\nLetting <math>Q_M(k)</math> denote the number of cache misses incurred by a call to a k-merger, one can show that <math>Q_M(k) = O((k^3 \\log_Z k)/L).</math> This is done by an induction argument. It has <math>k \\le \\alpha \\sqrt{Z}</math> as a base case. For larger k, we can bound the number of times a <math>\\sqrt{k}</math>-merger is called. The output merger is called exactly <math>k^{3/2}</math> times. The total number of calls on input mergers is at most <math>k^{3/2} + 2\\sqrt{k}</math>. This gives a total bound of <math>2 k^{3/2} + 2 \\sqrt{k}</math> recursive calls. In addition, the algorithm checks every buffer to see if needs to be filled. This is done on <math>\\sqrt{k}</math> buffers every step for <math>k^{3/2}</math> steps, leading to a max of <math>k^2</math> cache misses for all the checks.\n\nThis leads to the recurrence <math>Q_M(k) \\le (2k^{3/2} + 2 \\sqrt{k}) Q_M(\\sqrt{k}) + k^2</math>, which can be shown to have the solution given above.\n\nFinally, the total cache misses <math>Q(N)</math> for the entire sort can be analyzed. It satisfies the recurrence <math>Q(N) = N^{1/3} Q(N^{2/3}) + Q_M(N^{1/3}).</math> This can be shown to have solution <math>Q(N) = O((N/L) \\log_Z N).</math>\n\n== Lazy funnelsort ==\n'''Lazy funnelsort''' is a modification of the funnelsort, introduced by [[Gerth Stølting Brodal]] and Rolf Fagerberg in 2002.<ref>{{cite book\n| first1 = Gerth Stølting\n| title = Automata, Languages and Programming\n| last1 = Brodal\n| author1-link = Gerth Stølting Brodal\n| first2 = Rolf\n| last2 = Fagerberg\n| chapter = Cache Oblivious Distribution Sweeping\n| <!--title = Proceedings of the International Conference on Automata, Languages and Programming -->\n| series = [[Lecture Notes in Computer Science]]\n| publisher = [[Springer Science+Business Media|Springer]]\n| volume = 2380\n| pages = 426–438\n| doi = 10.1007/3-540-45465-9_37\n| date = 25 June 2002\n<!--Automata, Languages and Programming-->\n| isbn = 978-3-540-43864-9\n| citeseerx = 10.1.1.117.6837\n}}.  See also the [http://www.cs.au.dk/~gerth/papers/brics-rs-02-18.pdf longer technical report].</ref>\nThe modification is that when a merger is invoked, it does not have to fill each of its buffers. Instead, it lazily fills a buffer only when it is empty. This modification has the same asymptotic runtime and memory transfers as the original funnelsort, but has applications in cache-oblivious algorithms for problems in computational geometry in a method known as distribution sweeping.\n\n== See also ==\n* [[Cache-oblivious algorithm]]\n* [[Cache-oblivious distribution sort]]\n* [[External sorting]]\n\n== References ==\n{{reflist}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:External memory algorithms]]\n[[Category:Analysis of algorithms]]\n[[Category:Cache (computing)]]\n[[Category:Models of computation]]"
    },
    {
      "title": "Gnome sort",
      "url": "https://en.wikipedia.org/wiki/Gnome_sort",
      "text": "{{Refimprove|date=August 2010}}<br />{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Sorting gnomesort anim.gif]]\n|caption=Visualisation of Gnome sort.\n|data=[[Array data structure|Array]]\n|time=<math>O(n^2)</math>\n|best-time=<math>\\Omega(n)</math>\n|average-time= <math>O(n^2)</math>\n|space= <math>O(1)</math> auxiliary\n|optimal= No\n}}\n'''Gnome sort''' (dubbed '''stupid sort''') is a [[sorting algorithm]] originally proposed by an [[Iran|Iranian]] computer scientist [[Hamid Sarbazi-Azad]] (professor of Computer Engineering at [[Sharif University of Technology]])<ref>{{Cite web|url=http://sharif.edu/~azad/|title=Hamid Sarbazi-Azad profile page|last=Hamid|first=Sarbazi-Azad|date=|website=|archive-url=https://web.archive.org/web/20181016164904/http://sharif.edu/~azad/#|archive-date=2018-10-16|dead-url=no|access-date=October 16, 2018|df=}}</ref> in 2000. The sort was first called ''stupid sort''<ref>{{cite journal\n |last         = Sarbazi-Azad\n |first        = Hamid\n |last2        = \n |first2       = \n |date         = 2 October 2000\n |title        = Stupid Sort: A new sorting algorithm\n |url          = http://sina.sharif.edu/~azad/stupid-sort.PDF\n |journal      = Newsletter\n |publisher    = Computing Science Department, Univ. of Glasgow\n |volume       = \n |issue        = 599\n |page         = 4\n |bibcode      = \n |doi          = \n |accessdate   = 25 November 2014\n |archive-url  = https://web.archive.org/web/20120307235904/http://sina.sharif.edu/~azad/stupid-sort.PDF#\n |archive-date = 2012-03-07\n |dead-url     = no\n |df           = \n}}</ref> (not to be confused with [[bogosort]]), and then later described by [[Dick Grune]] and named ''gnome sort''.<ref name=\"DGrune\">{{cite web |url=http://www.dickgrune.com/Programs/gnomesort.html |title=Gnome Sort - The Simplest Sort Algorithm |website=Dickgrune.com |date=2000-10-02 |accessdate=2017-07-20 |archive-url=https://web.archive.org/web/20170831222005/https://dickgrune.com/Programs/gnomesort.html# |archive-date=2017-08-31 |dead-url=no |df= }}</ref> \n\nThe gnome sort is a sorting algorithm which is similar to [[insertion sort]] in that it works with one item at a time but gets the item to the proper place by a series of swaps, similar to a [[bubble sort]]. It is conceptually simple, requiring no [[Nested loop join|nested loops]]. The average running time is [[Big O notation|''O'']](''n''<sup>2</sup>) but tends towards ''O''(''n'') if the list is initially almost sorted.<ref>{{cite web\n |url          = http://xlinux.nist.gov/dads/HTML/gnomeSort.html\n |title        = gnome sort\n |work         = Dictionary of Algorithms and Data Structures\n |publisher    = U.S. National Institute of Standards and Technology\n |author       = Paul E. Black\n |accessdate   = 2011-08-20\n |archive-url  = https://web.archive.org/web/20110811012120/http://xlinux.nist.gov/dads//HTML/gnomeSort.html#\n |archive-date = 2011-08-11\n |dead-url     = no\n |df           = \n}}</ref><ref group=\"note\">''Almost sorted'' means that each item in the list is not far from its proper position (not farther than some small constant distance).</ref>\n\nThe algorithm finds the first place where two adjacent elements are in the wrong order and swaps them. It takes advantage of the fact that performing a swap can introduce a new out-of-order adjacent pair next to the previously swapped elements. It does not assume that elements forward of the current position are sorted, so it only needs to check the position directly previous to the swapped elements.\n\n== Description ==\n\n[[Dick Grune]] described the sorting method with the following story:<ref name=\"DGrune\"/>\n{{quote|\nGnome Sort is based on the technique used by the standard Dutch [[garden gnome|Garden Gnome]] (Du.: [[:nl:tuinkabouter|tuinkabouter]]). <br />\nHere is how a garden gnome sorts a line of [[flowerpot|flower pots]]. <br />\nBasically, he looks at the flower pot next to him and the previous one; if they are in the right order he steps one pot forward, otherwise, he swaps them and steps one pot backward. <br />\nBoundary conditions: if there is no previous pot, he steps forwards; if there is no pot next to him, he is done.\n|sign=| source=\"Gnome Sort - The Simplest Sort Algorithm\". ''Dickgrune.com''}}\n\n=== Code ===\n\nHere is [[pseudocode]] for the gnome sort using a [[zero-based array]]:\n\n<source lang=\"text\">\nprocedure gnomeSort(a[]):\n    pos := 0\n    while pos < length(a):\n        if (pos == 0 or a[pos] >= a[pos-1]):\n            pos := pos + 1\n        else:\n            swap a[pos] and a[pos-1]\n            pos := pos - 1\n</source>\n\n===Example===\nGiven an unsorted array, a = [5, 3, 2, 4], the gnome sort takes the following steps during the while loop. The current position is highlighted in bold and indicated as a value of the variable <code>pos</code>.\n\n{|    class=\"wikitable\"\n|-\n! Current array\n! <tt>pos</tt>\n! Condition in effect\n! Action to take\n|-\n||    ['''5''', 3, 2, 4] || 0 ||    pos == 0  || increment pos\n|-\n||    [5, '''3''', 2, 4] || 1 ||    a[pos] < a[pos-1]  || swap, decrement pos\n|-\n||    ['''3''', 5, 2, 4] || 0 ||    pos == 0  || increment pos\n|-\n||    [3, '''5''', 2, 4] || 1 ||    a[pos] ≥ a[pos-1] || increment pos\n|-\n||    [3, 5, '''2''', 4] || 2 ||    a[pos] < a[pos-1] || swap, decrement pos\n|-\n||    [3, '''2''', 5, 4] || 1 ||    a[pos] < a[pos-1] || swap, decrement pos\n|-\n||    ['''2''', 3, 5, 4] || 0 ||    pos == 0  || increment pos\n|-\n||    [2, '''3''', 5, 4] || 1 ||    a[pos] ≥ a[pos-1] || increment pos\n|-\n||    [2, 3, '''5''', 4] || 2 ||    a[pos] ≥ a[pos-1] || increment pos:\n|-\n||    [2, 3, 5, '''4'''] || 3 ||    a[pos] < a[pos-1] || swap, decrement pos\n|-\n||    [2, 3, '''4''', 5] || 2 ||    a[pos] ≥ a[pos-1] || increment pos\n|-\n||    [2, 3, 4, '''5'''] || 3 ||    a[pos] ≥ a[pos-1] || increment pos\n|-\n||    [2, 3, 4, 5]       || 4 ||    pos == length(a) || finished\n|}\n\n==Optimization==\n{{unreferenced section|date=November 2015}}\nThe gnome sort may be optimized by introducing a variable to store the position before traversing back toward the beginning of the list. With this optimization, the gnome sort would become a variant of the [[insertion sort]].\n\nHere is [[pseudocode]] for an optimized gnome sort using a [[zero-based array]]:\n\n<source lang=\"text\" line=\"1\">\nprocedure optimizedGnomeSort(a[]):\n    for pos in 1 to length(a):\n        gnomeSort(a, pos)\n\nprocedure gnomeSort(a[], upperBound):\n    pos := upperBound\n    while pos > 0 and a[pos-1] > a[pos]:\n        swap a[pos-1] and a[pos]\n        pos := pos - 1\n</source>\n\n==Notes==\n{{reflist|group=note}}\n==References==\n{{Reflist}}\n\n==External links==\n{{wikibooks|Algorithm implementation|Sorting/Gnome_sort|Gnome sort}}\n* [http://dickgrune.com/Programs/gnomesort.html Gnome sort]\n\n{{sorting}}\n\n{{DEFAULTSORT:Gnome Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]"
    },
    {
      "title": "Heapsort",
      "url": "https://en.wikipedia.org/wiki/Heapsort",
      "text": "{{Short description|A sorting algorithm which uses the heap data structure}}\n{{Use dmy dates|date=July 2012}}\n{{Infobox Algorithm|class=[[Sorting algorithm]]\n|image=[[Image:Sorting heapsort anim.gif]]\n|caption=A run of heapsort sorting an array of randomly permuted values. In the first stage of the algorithm the array elements are reordered to satisfy the [[Heap (data structure)|heap property]]. Before the actual sorting takes place, the heap tree structure is shown briefly for illustration.\n|data=[[Array data structure|Array]]\n|time=<math>O(n\\log n)</math>\n|average-time=<math>O(n\\log n)</math>\n|best-time=<math>O(n\\log n)</math> (distinct keys)<br />or <math>O(n)</math> (equal keys)\n|space=<math>O(n)</math> total <math>O(1)</math> auxiliary \n|optimal=Never\n}}\nIn [[computer science]], '''heapsort''' is a [[comparison sort|comparison-based]] [[sorting algorithm]]. Heapsort can be thought of as an improved [[selection sort]]: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a [[heap (data structure)|heap]] data structure rather than a linear-time search to find the maximum.<ref>{{cite book |first=Steven |last=Skiena |authorlink=Steven Skiena |title=The Algorithm Design Manual |publisher=Springer |year=2008 |page=109 |chapter=Searching and Sorting |isbn=978-1-84800-069-8 |quote=[H]eapsort is nothing but an implementation of selection sort using the right data structure.|doi=10.1007/978-1-84800-070-4_4}}<!--DOI for chapter--></ref>\n\nAlthough somewhat slower in practice on most machines than a well-implemented [[quicksort]], it has the advantage of a more favorable worst-case [[big O notation|{{math|O(''n'' log ''n'')}}]] runtime.  Heapsort is an [[in-place algorithm]], but it is not a [[stable sort]].\n\nHeapsort was invented by [[J. W. J. Williams]] in 1964.<ref>{{harvnb|Williams|1964}}</ref> This was also the birth of the heap, presented already by Williams as a useful data structure in its own right.<ref name=\"brass\">{{cite book |first=Peter |last=Brass |title=Advanced Data Structures |publisher=Cambridge University Press |year=2008 |isbn=978-0-521-88037-4 |page=209}}</ref> In the same year, [[Robert Floyd|R. W. Floyd]] published an improved version that could sort an array in-place, continuing his earlier research into the [[treesort]] algorithm.<ref name=\"brass\"/>\n\n== Overview ==\nThe heapsort algorithm can be divided into two parts.\n\nIn the first step, a [[Heap (data structure)|heap]] is [[Binary heap#Building a heap|built]] out of the data. The heap is often placed in an array with the layout of a complete [[Binary tree#Types of binary trees|binary tree]]. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions.  For a zero-based array, the root node is stored at index 0; if <code>i</code> is the index of the current node, then\n<pre>\n  iParent(i)     = floor((i-1) / 2) where floor functions map a real number to the smallest leading integer.\n  iLeftChild(i)  = 2*i + 1\n  iRightChild(i) = 2*i + 2\n</pre>\n\nIn the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\n\nHeapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed [[Binary heap#Heap implementation|here]].  The heap's invariant is preserved after each extraction, so the only cost is that of extraction.\n\n== Algorithm ==\nThe Heapsort algorithm involves preparing the list by first turning it into a [[Binary heap|max heap]]. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\n\nThe steps are:\n# Call the buildMaxHeap() function on the list. Also referred to as heapify(), this builds a heap from a list in O(n) operations.\n# Swap the first element of the list with the final element. Decrease the considered range of the list by one.\n# Call the siftDown() function on the list to sift the new first element to its appropriate index in the heap.\n# Go to step (2) unless the considered range of the list is one element.\nThe buildMaxHeap() operation is run once, and is {{math|O(''n'')}} in performance. The siftDown() function is {{math|O(log&nbsp;''n'')}}, and is called {{mvar|n}} times. Therefore, the performance of this algorithm is {{math|1=O(''n'' + ''n'' log ''n'') = O(''n'' log ''n'')}}.\n\n=== Pseudocode ===\n\nThe following is a simple way to implement the algorithm in [[pseudocode]]. Arrays are [[Comparison of programming languages (array)|zero-based]] and <code>swap</code> is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at <code>a[0]</code>, while at the end of the sort, the largest element is in <code>a[end]</code>.\n\n '''procedure''' heapsort(a, count) '''is'''\n     '''input:''' an unordered array ''a'' of length ''count''\n  \n     ''(Build the heap in array a so that largest value is at the root)''\n     heapify(a, count)\n \n     ''(The following loop maintains the [[Loop invariant|invariants]] that a[0:end] is a heap and every element''\n      ''beyond end is greater than everything before it (so a[end:count] is in sorted order))''\n     end ← count - 1\n     '''while''' end > 0 '''do'''\n         ''(a[0] is the root and largest value. The swap moves it in front of the sorted elements.)''\n         swap(a[end], a[0])\n         ''(the heap size is reduced by one)''\n         end ← end - 1\n         ''(the swap ruined the heap property, so restore it)''\n         siftDown(a, 0, end)\n\nThe sorting routine uses two subroutines, <code>heapify</code> and <code>siftDown</code>. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing <code>heapify</code>.\n \n ''(Put elements of 'a' in heap order, in-place)''\n '''procedure''' heapify(a, count) '''is'''\n     ''(start is assigned the index in 'a' of the last parent node)''\n     ''(the last element in a 0-based array is at index count-1; find the parent of that element)''\n     start ← iParent(count-1)\n     \n     '''while''' start ≥ 0 '''do'''\n         ''(sift down the node at index 'start' to the proper place such that all nodes below''\n         '' the start index are in heap order)''\n         siftDown(a, start, count - 1)\n         ''(go to the next parent node)''\n         start ← start - 1\n     ''(after sifting down the root all nodes/elements are in heap order)''\n \n ''(Repair the heap whose root element is at index 'start', assuming the heaps rooted at its children are valid)\n '''procedure''' siftDown(a, start, end) '''is'''\n     root ← start\n \n     '''while''' iLeftChild(root) ≤ end '''do'''    ''(While the root has at least one child)''\n         child ← iLeftChild(root)   ''(Left child of root)''\n         swap ← root                ''(Keeps track of child to swap with)''\n \n         '''if''' a[swap] < a[child]\n             swap ← child\n         ''(If there is a right child and that child is greater)''\n         '''if''' child+1 ≤ end '''and''' a[swap] < a[child+1]\n             swap ← child + 1\n         '''if''' swap = root\n             ''(The root holds the largest element. Since we assume the heaps rooted at the''\n             '' children are valid, this means that we are done.)''\n             '''return'''\n         '''else'''\n             swap(a[root], a[swap])\n             root ← swap            ''(repeat to continue sifting down the child now)''\n\nThe <code>heapify</code> procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the [[Heap (data structure)|heap property]]. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This <code>siftUp</code> version can be visualized as starting with an empty heap and successively inserting elements, whereas the <code>siftDown</code> version given above treats the entire input array as a full but \"broken\" heap and \"repairs\" it starting from the last non-trivial sub-heap (that is, the last parent node).\n\n[[File:Binary heap bottomup vs topdown.svg|thumb|right|Difference in time complexity between the \"siftDown\" version and the \"siftUp\" version.]]\nAlso, the <code>siftDown</code> version of heapify [[Binary heap#Building a heap|has {{math|''O''(''n'')}} time complexity]], while the <code>siftUp</code> version given below has {{math|''O''(''n'' log ''n'')}} time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.<ref>{{cite web|title=Priority Queues|url=http://faculty.simpson.edu/lydia.sinapova/www/cmsc250/LN250_Weiss/L10-PQueues.htm|accessdate=24 May 2011}}</ref>\nThis may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never affects asymptotic analysis.\n\nTo grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call ''increases'' with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more \"deep\" nodes than there are \"shallow\" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the \"bottom\" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call ''decreases'' as the depth of the node on which the call is made increases. Thus, when the <code>siftDown</code> <code>heapify</code> begins and is calling <code>siftDown</code> on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the \"height\" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.\n\nThe heapsort algorithm itself has {{math|''O''(''n'' log ''n'')}} time complexity using either version of heapify.\n\n  '''procedure''' heapify(a,count) is\n      ''(end is assigned the index of the first (left) child of the root)''\n      end := 1\n      \n      '''while''' end < count\n          ''(sift up the node at index end to the proper place such that all nodes above''\n          '' the end index are in heap order)''\n          siftUp(a, 0, end)\n          end := end + 1\n      ''(after sifting up the last node all nodes are in heap order)''\n  \n  '''procedure''' siftUp(a, start, end) '''is'''\n      '''input: ''' ''start represents the limit of how far up the heap to sift.''\n                    ''end is the node to sift up.''\n      child := end \n      '''while''' child > start\n          parent := iParent(child)\n          '''if''' a[parent] < a[child] '''then''' ''(out of max-heap order)''\n              swap(a[parent], a[child])\n              child := parent ''(repeat to continue sifting up the parent now)''\n          '''else'''\n              '''return'''\n\n== Variations ==\n=== Floyd's heap construction ===\nThe most important variation to the basic algorithm, which is included in all practical implementations, is a heap-construction algorithm by Floyd which runs in {{math|''O''(''n'')}} time and uses [[Binary heap#Extract|siftdown]] rather than [[Binary heap#Insert|siftup]], avoiding the need to implement siftup at all.\n\nRather than starting with a trivial heap and repeatedly adding leaves, Floyd's algorithm starts with the leaves, observing that they are trivial but valid heaps by themselves, and then adds parents.  Starting with element {{math|''n''/2}} and working backwards, each internal node is made the root of a valid heap by sifting down.  The last step is sifting down the first element, after which the entire array obeys the heap property.\n\nThe worst-case number of comparisons during the Floyd's heap-construction phase of Heapsort is known to be equal to {{math|2''n'' − 2''s''<sub>2</sub>(''n'') − ''e''<sub>2</sub>(''n'')}}, where {{math|''s''<sub>2</sub>(''n'')}} is the number of 1 bits in the binary representation of {{mvar|n}} and {{math|''e''<sub>2</sub>(''n'')}} is number of trailing 0 bits.<ref>{{citation\n | last1 = Suchenek | first1 = Marek A.\n | title = Elementary Yet Precise Worst-Case Analysis of Floyd's Heap-Construction Program\n | doi = 10.3233/FI-2012-751\n | pages = 75–92\n | journal = [[Fundamenta Informaticae]]\n | volume = 120\n | issue = 1\n | year = 2012}}</ref>\n\nThe standard implementation of Floyd's heap-construction algorithm causes a large number of [[cache miss]]es once the size of the data exceeds that of the [[CPU cache]].  Much better performance on large data sets can be obtained by merging in [[depth-first]] order, combining subheaps as soon as possible, rather than combining all subheaps on one level before proceeding to the one above.<ref name=Bojesen00>{{cite journal |title=Performance Engineering Case Study: Heap Construction |first1=Jesper |last1=Bojesen |first2=Jyrki |last2=Katajainen |first3=Maz |last3=Spork |journal=ACM Journal of Experimental Algorithmics |date=2000 |volume=5 |pages=15–es |number=15 |doi=10.1145/351827.384257 |citeseerx=10.1.1.35.3248 |url=http://hjemmesider.diku.dk/~jyrki/Paper/katajain.ps |format=PostScript}} [https://www.semanticscholar.org/paper/Performance-Engineering-Case-Study-Heap-Bojesen-Katajainen/6f4ada5912c1da64e16453d67ec99c970173fb5b Alternate PDF source].</ref><ref>{{cite conference |title=In-place Heap Construction with Optimized Comparisons, Moves, and Cache Misses |first1=Jingsen |last1=Chen |first2=Stefan |last2=Edelkamp |first3=Amr |last3=Elmasry |first4=Jyrki |last4=Katajainen |doi=10.1007/978-3-642-32589-2_25 |conference=37th international conference on Mathematical Foundations of Computer Science |pages=259–270 |location=Bratislava, Slovakia |date=August 27–31, 2012 |isbn=978-3-642-32588-5 |url=https://pdfs.semanticscholar.org/9cc6/36d7998d58b3937ba0098e971710ff039612.pdf#page=11}}  See particularly Fig. 3.</ref>\n\n=== Bottom-up heapsort ===\nBottom-up heapsort is a variant which reduces the number of comparisons required by a significant factor.  While ordinary heapsort requires {{math|2''n''&thinsp;log<sub>2</sub>''n'' + ''O''(''n'')}} comparisons worst-case and on average,{{r|Wegener}} the bottom-up variant requires {{math|''n''&thinsp;log<sub>2</sub>''n'' + ''O''(1)}} comparisons on average,{{r|Wegener}} and {{math|1.5''n''&thinsp;log<sub>2</sub>''n'' + ''O''(''n'')}} in the worst case.{{r|fleischer}}\n\nIf comparisons are cheap (e.g. integer keys) then the difference is unimportant,{{r|Melhorn}} as top-down heapsort compares values that have already been loaded from memory.  If, however, comparisons require a [[function call]] or other complex logic, then bottom-up heapsort is advantageous.\n\nThis is accomplished by improving the <code>siftDown</code> procedure.  The change improves the linear-time heap-building phase somewhat,{{r|McDiarmid}} but is more significant in the second phase.  Like ordinary heapsort, each iteration of the second phase extracts the top of the heap, {{math|''a''[0]}}, and fills the gap it leaves with {{math|''a''[''end'']}}, then sifts this latter element down the heap.  But this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. In ordinary heapsort, each step of the sift-down requires two comparisons, to find the minimum of three elements: the new node and its two children.\n\nBottom-up heapsort instead finds the path of largest children to the leaf level of the tree (as if it were inserting −∞) using only one comparison per level.  Put another way, it finds a leaf which has the property that it and all of its ancestors are greater than or equal to their siblings.  (In the absence of equal keys, this leaf is unique.)  Then, from this leaf, it searches ''upward'' (using one comparison per level) for the correct position in that path to insert {{math|''a''[''end'']}}.  This is the same location as ordinary heapsort finds, and requires the same number of exchanges to perform the insert, but fewer comparisons are required to find that location.<ref name=\"fleischer\">{{cite journal |last=Fleischer |first=Rudolf |title=A tight lower bound for the worst case of Bottom-Up-Heapsort |journal=Algorithmica |volume=11 |issue=2 |date=February 1994 |pages=104–115 |doi=10.1007/bf01182770 |url=http://staff.gutech.edu.om/~rudolf/Paper/buh_algorithmica94.pdf |hdl=11858/00-001M-0000-0014-7B02-C}}  Also available as {{cite techreport |last=Fleischer |first=Rudolf |title=A tight lower bound for the worst case of Bottom-Up-Heapsort |date=April 1991 |institution=[[Max Planck Institute for Informatics|MPI-INF]] |number=MPI-I-91-104 |url=http://pubman.mpdl.mpg.de/pubman/item/escidoc:1834997:3/component/escidoc:2463941/MPI-I-94-104.pdf}}</ref>\n\nBecause it goes all the way to the bottom and then comes back up, it is called '''heapsort with bounce''' by some authors.<ref>{{cite book |first1=Bernard |last1=Moret |authorlink1=Bernard Moret |first2=Henry D. |last2=Shapiro |title=Algorithms from P to NP Volume 1: Design and Efficiency |chapter=8.6 Heapsort |page=528 |publisher=Benjamin/Cummings |year=1991 |isbn=0-8053-8008-6 |quote=For lack of a better name we call this enhanced program 'heapsort with bounce.{{'-}}}}</ref>\n\n '''function''' leafSearch(a, i, end) '''is'''\n     j ← i\n     '''while''' iRightChild(j) ≤ end '''do'''\n         ''(Determine which of j's two children is the greater)''\n         '''if''' a[iRightChild(j)] > a[iLeftChild(j)] '''then'''\n             j ← iRightChild(j)\n         '''else'''\n             j ← iLeftChild(j)\n     ''(At the last level, there might be only one child)''\n     '''if''' iLeftChild(j) ≤ end '''then'''\n         j ← iLeftChild(j)\n     '''return''' j\n\nThe return value of the <code>leafSearch</code> is used in the modified <code>siftDown</code> routine:<ref name=\"fleischer\"/>\n\n '''procedure''' siftDown(a, i, end) '''is'''\n     j ← leafSearch(a, i, end)\n     '''while''' a[i] > a[j] '''do'''\n         j ← iParent(j)\n     x ← a[j]\n     a[j] ← a[i]\n     '''while''' j > i '''do'''\n         swap x, a[iParent(j)]\n         j ← iParent(j)\n\nBottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000.{{refn|name=Wegener|{{cite journal |last=Wegener |first=Ingo |authorlink=Ingo Wegener |title={{sc|Bottom-Up Heapsort}}, a new variant of {{sc|Heapsort}} beating, on an average, {{sc|Quicksort}} (if {{mvar|n}} is not very small) |journal=Theoretical Computer Science |volume=118 |issue=1 |date=13 September 1993 |pages=81–98 |doi=10.1016/0304-3975(93)90364-y |doi-access=free |url=https://core.ac.uk/download/pdf/82350265.pdf}}  Although this is a reprint of work first published in 1990 (at the Mathematical Foundations of Computer Science conference), the technique was published by Carlsson in 1987.<ref name=Carlsson/>}}\n\nA 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort for integer keys, presumably because modern [[branch prediction]] nullifies the cost of the predictable comparisons which bottom-up heapsort manages to avoid.<ref name=Melhorn>{{cite book |last1=Mehlhorn |first1=Kurt |author1-link=Kurt Mehlhorn |first2=Peter |last2=Sanders |author2-link=Peter Sanders (computer scientist) |title=Algorithms and Data Structures: The Basic Toolbox |chapter=Priority Queues |publisher=Springer |year=2008 |page=142 |isbn=978-3-540-77977-3 |url=http://people.mpi-inf.mpg.de/~mehlhorn/Toolbox.html |chapter-url=http://people.mpi-inf.mpg.de/~mehlhorn/ftp/Toolbox/PriorityQueues.pdf#page=16}}</ref>\n\nA further refinement does a binary search in the path to the selected leaf, and sorts in a worst case of {{math|(''n''+1)(log<sub>2</sub>(''n''+1) + log<sub>2</sub> log<sub>2</sub>(''n''+1) + 1.82) + ''O''(log<sub>2</sub>''n'')}} comparisons, approaching [[Comparison sort#Number of comparisons required to sort a list|the information-theoretic lower bound]] of {{math|''n''&thinsp;log<sub>2</sub>''n'' − 1.4427''n''}} comparisons.<ref name=Carlsson>{{cite journal |first=Scante |last=Carlsson |title=A variant of heapsort with almost optimal number of comparisons |journal=Information Processing Letters |volume=24 |issue=4 |pages=247–250 |date=March 1987 |doi=10.1016/0020-0190(87)90142-6 |url=https://pdfs.semanticscholar.org/caec/6682ffd13c6367a8c51b566e2420246faca2.pdf}}</ref>\n\nA variant which uses two extra bits per internal node (''n''−1 bits total for an ''n''-element heap) to cache information about which child is greater (two bits are required to store three cases: left, right, and unknown)<ref name=McDiarmid>{{Cite journal |title=Building heaps fast |last1=McDiarmid |first1=C.J.H. |last2=Reed |first2=B.A. |date=September 1989 |journal=Journal of Algorithms |volume=10 |issue=3 |pages=352–365 |doi=10.1016/0196-6774(89)90033-3 |url=http://cgm.cs.mcgill.ca/~breed/2016COMP610/BUILDINGHEAPSFAST.pdf}}</ref> uses less than {{math|''n''&thinsp;log<sub>2</sub>''n'' + 1.1''n''}} compares.<ref>{{cite journal |title=The worst case complexity of McDiarmid and Reed's variant of {{sc|Bottom-Up Heapsort}} is less than ''n''&nbsp;log&nbsp;''n'' + 1.1''n'' |first=Ingo |last=Wegener |authorlink=Ingo Wegener |journal=Information and Computation |volume=97 |issue=1 |pages=86–96 |date=March 1992 |doi=10.1016/0890-5401(92)90005-Z |doi-access=free}}</ref>\n\n=== Other variations ===\n*Ternary heapsort<ref>\"Data Structures Using Pascal\", 1991, page 405,{{Full citation needed|date=December 2016}}{{author?|date=December 2016}}{{ISBN?|date=December 2016}}<!--I can find books by this title by Tenenbaum (1981, 1982) and Rhoads & Gearen (1989, 1992), but nothing from 1991.--> gives a ternary heapsort as a student exercise. \"Write a sorting routine similar to the heapsort except that it uses a ternary heap.\"</ref> uses a [[ternary heap]] instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations.  This is because each sift-down step in a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. Two levels in a ternary heap cover 3<sup>2</sup> = 9 elements, doing more work with the same number of comparisons as three levels in the binary heap, which only cover 2<sup>3</sup> = 8.{{Citation needed|date=September 2014}}  This is primarily of academic interest, as the additional complexity is not worth the minor savings, and bottom-up heapsort beats both.\n*The [[smoothsort]] algorithm<ref>{{Cite EWD|796a|Smoothsort – an alternative to sorting in situ}}</ref> is a variation of heapsort developed by [[Edsger W. Dijkstra|Edsger Dijkstra]] in 1981. Like heapsort, smoothsort's upper bound is [[Big O notation|{{math|''O''(''n''&thinsp;log&thinsp;''n'')}}]]. The advantage of smoothsort is that it comes closer to {{math|''O''(''n'')}} time if the [[Adaptive sort|input is already sorted to some degree]], whereas heapsort averages {{math|''O''(''n''&thinsp;log&thinsp;''n'')}} regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.{{citation needed|date=November 2016}}\n*Levcopoulos and Petersson<ref>{{citation\n | last1 = Levcopoulos | first1 = Christos\n | last2 = Petersson | first2 = Ola\n | contribution = Heapsort—Adapted for Presorted Files\n | location = London, UK\n | pages = 499–509\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = WADS '89: Proceedings of the Workshop on Algorithms and Data Structures\n | volume = 382 | doi = 10.1007/3-540-51542-9_41\n | year = 1989| isbn = 978-3-540-51542-5\n }} {{Q|56049336}}.</ref> describe a variation of heapsort based on a heap of [[Cartesian tree]]s.  First, a Cartesian tree is built from the input in {{math|''O''(''n'')}} time, and its root is placed in a 1-element binary heap.   Then we repeatedly extract the minimum from the binary heap, output the tree's root element, and add its left and right children (if any) which are themselves Cartesian trees, to the binary heap.<ref>{{cite web\n |title=CartesianTreeSort.hh\n |website=Archive of Interesting Code\n |url=http://www.keithschwarz.com/interesting/code/?dir=cartesian-tree-sort\n |first=Keith |last=Schwartz\n |date=27 December 2010 |access-date=2019-03-05\n}}</ref> As they show, if the input is already nearly sorted, the Cartesian trees will be very unbalanced, with few nodes having left and right children, resulting in the binary heap remaining small, and allowing the algorithm to sort more quickly than {{math|''O''(''n''&thinsp;log&thinsp;''n'')}} for inputs that are already nearly sorted.\n* Several variants such as [[weak heap]]sort require {{math|''n&thinsp;''log<sub>2</sub>''n''+''O''(1)}} comparisons in the worst case, close to the theoretical minimum, using one extra bit of state per node.  While this extra bit makes the algorithms not truly in-place, if space for it can be found inside the element, these algorithms are simple and efficient,{{r|Bojesen00|p=40}} but still slower than binary heaps if key comparisons are cheap enough (e.g. integer keys) that a constant factor does not matter.<ref name=Kat2014-11-14P>{{cite conference |title=Seeking for the best priority queue: Lessons learnt |first=Jyrki |last=Katajainen |date=23 September 2013 |conference=Algorithm Engineering (Seminar 13391) |location=Dagstuhl |pages=19–20, 24 |url=http://hjemmesider.diku.dk/~jyrki/Myris/Kat2013-09-23P.html}}</ref>\n* Katajainen's \"ultimate heapsort\" requires no extra storage, performs {{math|''n&thinsp;''log<sub>2</sub>''n''+''O''(1)}} comparisons, and a similar number of element moves.<ref>{{cite conference |title=The Ultimate Heapsort |date=2–3 February 1998 |first=Jyrki |last=Katajainen |conference=Computing: the 4th Australasian Theory Symposium |url=http://hjemmesider.diku.dk/~jyrki/Myris/Kat1998C.html |journal=Australian Computer Science Communications |volume=20 |issue=3 |pages=87–96 |location=Perth }}</ref>  It is, however, even more complex and not justified unless comparisons are very expensive.\n\n== Comparison with other sorts ==\nHeapsort primarily competes with [[quicksort]], another very efficient general purpose nearly-in-place comparison-based sort algorithm.\n\nQuicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is {{math|O(''n''<sup>2</sup>)}}, which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See [[quicksort]] for a detailed discussion of this problem and possible solutions.\n\nThus, because of the {{math|O(''n'' log ''n'')}} upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort, such as the Linux kernel.<ref>https://github.com/torvalds/linux/blob/master/lib/sort.c Linux kernel source</ref>\n\nHeapsort also competes with [[merge sort]], which has the same time bounds. Merge sort requires {{math|Ω(''n'')}} auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow [[data cache]]s, and does not require as much external memory. On the other hand, merge sort has several advantages over heapsort:\n* Merge sort on arrays has considerably better data cache performance, often outperforming heapsort on modern desktop computers because merge sort frequently accesses contiguous memory locations (good [[locality of reference]]); heapsort references are spread throughout the heap.\n* Heapsort is not a [[stable sort]]; merge sort is stable.\n* Merge sort [[parallel algorithm|parallelizes]] well and can achieve close to [[linear speedup]] with a trivial implementation; heapsort is not an obvious candidate for a parallel algorithm.\n* Merge sort can be adapted to operate on '''singly''' [[linked list]]s with {{math|O(1)}} extra space. Heapsort can be adapted to operate on '''doubly''' linked lists with only {{math|O(1)}} extra space overhead.{{Citation needed|date=June 2012}}\n* Merge sort is used in [[external sorting]]; heapsort is not. Locality of reference is the issue.\n\n[[Introsort]] is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.\n\n== Example ==\nLet { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)\n[[File:Heapsort-example.gif|350px|thumb|right|An example on heapsort.]]\n\n{| class=\"wikitable\"\n|+ style=\"text-align: left;\" | 1. Build the heap\n|-\n! Heap !! newly added element !! swap elements\n|-\n| null || 6 ||  \n|-\n| 6 || 5 ||  \n|-\n| 6, 5 || 3 ||  \n|-\n| 6, 5, 3 || 1 ||  \n|-\n| 6, 5, 3, 1 || 8 ||  \n|-\n| 6, '''5''', 3, 1, '''8 '''||   || 5, 8 \n|-\n| '''6''', '''8''', 3, 1, 5 ||   || 6, 8 \n|-\n| 8, 6, 3, 1, 5 || 7 ||  \n|-\n| 8, 6, '''3''', 1, 5, '''7''' ||   || 3, 7 \n|-\n| 8, 6, 7, 1, 5, 3 || 2 ||   \n|-\n| 8, 6, 7, 1, 5, 3, 2 || 4 ||  \n|-\n| 8, 6, 7, '''1''', 5, 3, 2, '''4''' ||   || 1, 4\n|-\n| 8, 6, 7, 4, 5, 3, 2, 1 ||   ||  \n|}\n\n{| class=\"wikitable\"\n|+ style=\"text-align: left;\" | 2. Sorting\n|-\n! Heap !! swap elements !! delete element !! sorted array !! details\n|-\n| '''8''', 6, 7, 4, 5, 3, 2, '''1''' || 8, 1 ||   ||   || swap 8 and 1 in order to delete 8 from heap\n|-\n| 1, 6, 7, 4, 5, 3, 2, '''8''' ||   || 8 ||   || delete 8 from heap and add to sorted array\n|-\n| '''1''', 6, '''7''', 4, 5, 3, 2 || 1, 7 ||   || style=\"text-align: right;\" | 8 || swap 1 and 7 as they are not in order in the heap\n|-\n| 7, 6, '''1''', 4, 5, '''3''', 2 || 1, 3 ||   || style=\"text-align: right;\" | 8 || swap 1 and 3 as they are not in order in the heap \n|-\n| '''7''', 6, 3, 4, 5, 1, '''2''' || 7, 2 ||   || style=\"text-align: right;\" | 8 || swap 7 and 2 in order to delete 7 from heap\n|-\n| 2, 6, 3, 4, 5, 1, '''7'''       ||      || 7 || style=\"text-align: right;\" | 8 || delete 7 from heap and add to sorted array\n|-\n| '''2''', '''6''', 3, 4, 5, 1 || 2, 6 ||   || style=\"text-align: right;\" | 7, 8 || swap 2 and 6 as they are not in order in the heap\n|-\n| 6, '''2''', 3, 4, '''5''', 1 || 2, 5 ||   || style=\"text-align: right;\" | 7, 8 || swap 2 and 5 as they are not in order in the heap\n|-\n| '''6''', 5, 3, 4, 2, '''1''' || 6, 1 ||   || style=\"text-align: right;\" | 7, 8 || swap 6 and 1 in order to delete 6 from heap\n|-\n| 1, 5, 3, 4, 2, '''6'''          ||   || 6 || style=\"text-align: right;\" | 7, 8 || delete 6 from heap and add to sorted array\n|-\n| '''1''', '''5''', 3, 4, 2 || 1, 5 ||   || style=\"text-align: right;\" | 6, 7, 8 || swap 1 and 5 as they are not in order in the heap \n|-\n| 5, '''1''', 3, '''4''', 2 || 1, 4 ||   || style=\"text-align: right;\" | 6, 7, 8 || swap 1 and 4 as they are not in order in the heap \n|-\n| '''5''', 4, 3, 1, '''2''' || 5, 2 ||   || style=\"text-align: right;\" | 6, 7, 8 || swap 5 and 2 in order to delete 5 from heap \n|-\n| 2, 4, 3, 1, '''5''' ||            || 5 || style=\"text-align: right;\" | 6, 7, 8 || delete 5 from heap and add to sorted array \n|-\n| '''2''', '''4''', 3, 1 || 2, 4 ||   || style=\"text-align: right;\" | 5, 6, 7, 8 || swap 2 and 4 as they are not in order in the heap \n|-\n| '''4''', 2, 3, '''1''' || 4, 1 ||   || style=\"text-align: right;\" | 5, 6, 7, 8 || swap 4 and 1 in order to delete 4 from heap \n|-\n| 1, 2, 3, '''4'''          ||   || 4 || style=\"text-align: right;\" | 5, 6, 7, 8 || delete 4 from heap and add to sorted array \n|-\n| '''1''', 2, '''3''' || 1, 3 ||   || style=\"text-align: right;\" | 4, 5, 6, 7, 8 || swap 1 and 3 as they are not in order in the heap \n|-\n| '''3''', 2, '''1''' || 3, 1 ||   || style=\"text-align: right;\" | 4, 5, 6, 7, 8 || swap 3 and 1 in order to delete 3 from heap\n|-\n| 1, 2, '''3'''          ||   || 3 || style=\"text-align: right;\" | 4, 5, 6, 7, 8 || delete 3 from heap and add to sorted array \n|-\n| '''1''', '''2''' || 1, 2 ||   || style=\"text-align: right;\" | 3, 4, 5, 6, 7, 8 || swap 1 and 2 as they are not in order in the heap \n|-\n| '''2''', '''1''' || 2, 1 ||   || style=\"text-align: right;\" | 3, 4, 5, 6, 7, 8 || swap 2 and 1 in order to delete 2 from heap\n|-\n| 1, '''2''' ||   || 2 || style=\"text-align: right;\" | 3, 4, 5, 6, 7, 8 || delete 2 from heap and add to sorted array \n|-\n| '''1''' ||   || 1 || style=\"text-align: right;\" | 2, 3, 4, 5, 6, 7, 8 || delete 1 from heap and add to sorted array \n|-\n|      ||   ||   || style=\"text-align: right;\" | 1, 2, 3, 4, 5, 6, 7, 8 || completed\n|}\n\n== Notes ==\n{{reflist|30em}}\n\n== References ==\n* {{Citation |first=J. W. J. |last=Williams |author-link=J. W. J. Williams |title=Algorithm 232 - Heapsort |year=1964 |journal=[[Communications of the ACM]] |volume=7 |issue=6 |pages=347–348 |doi= 10.1145/512274.512284}}\n* {{Citation |first=Robert W. |last=Floyd |author-link=Robert W. Floyd |title=Algorithm 245 - Treesort 3 |year=1964 |journal=[[Communications of the ACM]] |volume=7 |issue=12 |page=701 |doi= 10.1145/355588.365103}}\n* {{Citation |first=Svante |last=Carlsson |author-link=Svante Carlsson |title=Average-case results on heapsort |year=1987 |journal=BIT |volume=27 |issue=1 |pages=2–17 |doi= 10.1007/bf01937350}}\n* {{Citation |first=Donald |last=Knuth |author-link=Donald Knuth |series=[[The Art of Computer Programming]] |volume=3 |title=Sorting and Searching |edition=third |publisher=Addison-Wesley |year=1997 |isbn=978-0-201-89685-5 |pages=144–155 |contribution=&sect;5.2.3, Sorting by Selection }}\n* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{ISBN|0-262-03293-7}}. Chapters 6 and 7 Respectively: Heapsort and Priority Queues\n* [http://www.cs.utexas.edu/users/EWD/ewd07xx/EWD796a.PDF A PDF of Dijkstra's original paper on Smoothsort]\n* [http://cis.stvincent.edu/html/tutorials/swd/heaps/heaps.html Heaps and Heapsort Tutorial] by David Carlson, St. Vincent College\n\n== External links ==\n{{wikibooks|Algorithm implementation|Sorting/Heapsort|Heapsort}}\n* {{webarchive |url=https://web.archive.org/web/20150306071556/http://www.sorting-algorithms.com/heap-sort |date=6 March 2015 |title=Animated Sorting Algorithms: Heap Sort}} – graphical demonstration\n* [http://olli.informatik.uni-oldenburg.de/heapsort_SALA/english/start.html Courseware on Heapsort from Univ. Oldenburg] - With text, animations and interactive exercises\n* [https://xlinux.nist.gov/dads/HTML/heapSort.html NIST's Dictionary of Algorithms and Data Structures: Heapsort]\n* [http://www.codecodex.com/wiki/Heapsort Heapsort implemented in 12 languages]\n* [http://www.azillionmonkeys.com/qed/sort.html Sorting revisited] by Paul Hsieh\n* [http://employees.oneonta.edu/zhangs/powerPointPlatform/index.php A PowerPoint presentation demonstrating how Heap sort works] that is for educators.\n* [http://opendatastructures.org/versions/edition-0.1e/ods-java/11_1_Comparison_Based_Sorti.html#SECTION001413000000000000000 Open Data Structures - Section 11.1.3 - Heap-Sort]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Heaps (data structures)]]\n[[Category:Articles with example pseudocode]]\n\n[[no:Sorteringsalgoritme#Heap sort]]"
    },
    {
      "title": "Insertion sort",
      "url": "https://en.wikipedia.org/wiki/Insertion_sort",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Insertion_sort.gif|300px]]\n|caption=Animated GIF of the insertion sort<ref>{{cite web |last=Simpsons |first=Unknown |title=Visualising Sorting Algorithms |url=https://upload.wikimedia.org/wikipedia/commons/4/42/Insertion_sort.gif |date=28 November 2011|accessdate=16 November 2017|mode=cs2}}</ref>\n|data=[[Array data structure|Array]]\n|time=О(''n''<sup>2</sup>) comparisons and swaps\n|best-time=O(''n'') comparisons, O(''1'') swaps\n|average-time=О(''n''<sup>2</sup>) comparisons and swaps\n|space=О(''n'') total, O(''1'') auxiliary\n|optimal=No\n}}\n\n'''Insertion sort''' is a simple [[sorting algorithm]] that builds the final [[sorted array]] (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as [[quicksort]], [[heapsort]], or [[merge sort]]. However, insertion sort provides several advantages:\n\n* Simple implementation: [[Jon Bentley (computer scientist)|Jon Bentley]] shows a three-line [[C (programming language)|C]] version, and a five-line [[Program optimization|optimized]] version<ref name=\"pearls\">{{cite book |first=Jon |last=Bentley |title=Programming Pearls |year=2000 |publisher=ACM Press/Addison–Wesley|pages=116, 121|mode=cs2}}</ref>\n* Efficient for (quite) small data sets, much like other quadratic sorting algorithms\n* More efficient in practice than most other simple quadratic (i.e., [[big O notation|O]](''n''<sup>2</sup>)) algorithms such as [[selection sort]] or [[bubble sort]] \n* [[Adaptive sort|Adaptive]], i.e., efficient for data sets that are already substantially sorted: the [[time complexity]] is [[big O notation|O]](''kn'') when each element in the input is no more than {{mvar|k}} places away from its sorted position\n* [[Stable sort|Stable]]; i.e., does not change the relative order of elements with equal keys\n* [[In-place algorithm|In-place]]; i.e., only requires a constant amount O(1) of additional memory space\n* [[Online algorithm|Online]]; i.e., can sort a list as it receives it\n\nWhen people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.<ref>{{Citation |last=Sedgewick |first=Robert |author-link=Robert Sedgewick (computer scientist) | title = Algorithms |year=1983 |publisher=Addison-Wesley |isbn=978-0-201-06672-2 | pages = 95ff}}.</ref>\n\n==Algorithm==\n[[File:Insertion-sort-example-300px.gif|300px|thumb|right|A graphical example of insertion sort. The partial sorted list (black) initially contains only the first element in the list. With each iteration one element (red) is removed from the input data and inserted in-place into the sorted list.]]\n\nInsertion sort [[Iteration|iterates]], consuming one input element each repetition, and growing a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\n\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\n\nThe resulting array after ''k'' iterations has the property where the first ''k'' + 1 entries are sorted (\"+1\" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:\n\n[[Image:insertionsort-before.png|Array prior to the insertion of x]]\n\nbecomes\n\n[[Image:insertionsort-after.png|Array after the insertion of x]]\n\nwith each element greater than ''x'' copied to the right as it is compared against ''x''.\n\nThe most common variant of insertion sort, which operates on arrays, can be described as follows:\n# Suppose there exists a function called ''Insert'' designed to insert a value into a sorted sequence at the beginning of an array. It operates by beginning at the end of the sequence and shifting each element one place to the right until a suitable position is found for the new element. The function has the side effect of overwriting the value stored immediately after the sorted sequence in the array.\n# To perform an insertion sort, begin at the left-most element of the array and invoke ''Insert'' to insert each element encountered into its correct position. The ordered sequence into which the element is inserted is stored at the beginning of the array in the set of indices already examined. Each insertion overwrites a single value: the value being inserted.\n\n[[Pseudocode]] of the complete algorithm follows, where the arrays are [[Zero-based numbering|zero-based]]:<ref name=\"pearls\"/>\n<!--\n***************************************\n\nNOTE TO WOULD-BE BUG FIXERS:\n\nThe variable i is meant to range over all but the first element of A. This is intentional. If you think you see a bug, please double- and triple-check your logic (and consider implementing and testing your idea), then discuss your fix on the Talk page, *BEFORE* changing this code.\n\n***************************************\n-->\n i ← 1\n '''while''' i < length(A)\n     j ← i\n     '''while''' j > 0 '''and''' A[j-1] > A[j]\n         '''swap''' A[j] and A[j-1]\n         j ← j - 1\n     '''end while'''\n     i ← i + 1\n '''end while'''\n\nThe outer loop runs over all the elements except the first one, because the single-element prefix <code>A[0:1]</code> is trivially sorted, so the [[Invariant_(computer_science)|invariant]] that the first <code>i</code> entries are sorted is true from the start. The inner loop moves element <code>A[i]</code> to its correct place so that after the loop, the first <code>i+1</code> elements are sorted. Note that the <code>'''and'''</code>-operator in the test must use [[short-circuit evaluation]], otherwise the test might result in an [[Bounds checking|array bounds error]], when <code>j=0</code> and it tries to evaluate <code>A[j-1] > A[j]</code> (i.e. accessing <code>A[-1]</code> fails).\n\nAfter expanding the <code>'''swap'''</code> operation in-place as <code>x ← A[j]; A[j] ← A[j-1]; A[j-1] ← x</code> (where <code>x</code> is a temporary variable), a slightly faster version can be produced that moves <code>A[i]</code> to its position in one go and only performs one assignment in the inner loop body:<ref name=\"pearls\"/>\n\n i ← 1\n '''while''' i < length(A)\n     x ← A[i]\n     j ← i - 1\n     '''while''' j >= 0 '''and''' A[j] > x\n         A[j+1] ← A[j]\n         j ← j - 1\n     '''end while'''\n     A[j+1] ← x<ref>{{Introduction to Algorithms|edition=3|chapter=Section 2.1: Insertion sort|pages=16–18|mode=cs2}}. See in particular p.&nbsp;18.</ref>\n     i ← i + 1\n '''end while'''\n\nThe new inner loop shifts elements to the right to clear a spot for <code>x = A[i]</code>.\n\nThe algorithm can also be implemented in a recursive way. The recursion just replaces the outer loop, calling itself and storing successively smaller values of ''n'' on the stack until ''n'' equals 0, where the function then returns back up the call chain to execute the code after each recursive call starting with ''n'' equal to 1, with ''n'' increasing by 1 as each instance of the function returns to the prior instance. The initial call would be ''insertionSortR(A, length(A)-1)'' .\n\n  '''function''' insertionSortR(array A, int n)\n      '''if''' n>0\n         insertionSortR(A,n-1)\n         x ← A[n]\n         j ← n-1\n         '''while''' j >= 0 '''and''' A[j] > x\n             A[j+1] ← A[j]\n             j ← j-1\n         '''end while'''\n         A[j+1] ← x\n      '''end if'''\n  '''end function'''\n\n==Best, worst, and average cases==\nThe best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O(''n'')). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array.\n\nThe simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it. In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O(''n''<sup>2</sup>)).\n\nThe average case is also quadratic<ref>{{Cite web|url=https://stackoverflow.com/a/17055342|first=Keith|last=Schwarz|title=Why is insertion sort Θ(n^2) in the average case? (answer by \"templatetypedef\")|publisher=Stack Overflow}}</ref>, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than [[quicksort]]; indeed, good [[quicksort]] implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.\n\nExample:\nThe following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}.  In each step, the key under consideration is underlined. The key that was moved (or left in place because it was biggest yet considered) in the previous step is marked with an asterisk.\n\n <u>3</u>  7  4  9  5  2  6  1\n 3* <u>7</u>  4  9  5  2  6  1\n 3  7* <u>4</u>  9  5  2  6  1\n 3  4* 7  <u>9</u>  5  2  6  1\n 3  4  7  9* <u>5</u>  2  6  1\n 3  4  5* 7  9  <u>2</u>  6  1\n 2* 3  4  5  7  9  <u>6</u>  1\n 2  3  4  5  6* 7  9  <u>1</u>\n 1* 2  3  4  5  6  7  9\n\n==Relation to other sorting algorithms==\nInsertion sort is very similar to [[selection sort]]. As in selection sort, after ''k'' passes through the array, the first ''k'' elements are in sorted order. However, the fundamental difference between the two algorithms is that for selection sort these are the ''k'' smallest elements of the unsorted input, while in insertion sort they are simply the first ''k'' elements of the input. The primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the ''k''+1th element is greater than the ''k''th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. On average (assuming the rank of the ''k''+1th element rank is random), insertion sort will require comparing and shifting half of the previous ''k'' elements, meaning insertion sort will perform about half as many comparisons as selection sort on average. In the worst case for insertion sort (when the input array is reverse-sorted), insertion sort performs just as many comparisons as selection sort. However, a disadvantage of insertion sort over selection sort is that it requires more writes due to the fact that, on each iteration, inserting the ''k''+1th element into the sorted portion of the array requires many element swaps to shift all of the following elements, while only a single swap is required for each iteration of selection sort. In general, insertion sort will write to the array O(''n''<sup>2</sup>) times, whereas selection sort will write only O({{mvar|n}}) times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with [[EEPROM]] or [[flash memory]].\n\nWhile some [[divide-and-conquer algorithm]]s such as [[quicksort]] and [[mergesort]] outperform insertion sort for larger arrays, non-recursive sorting algorithms such as insertion sort or selection sort are generally faster for very small arrays (the exact size varies by environment and implementation, but is typically between seven and fifty elements). Therefore, a useful optimization in the implementation of those algorithms is a hybrid approach, using the simpler algorithm when the array has been divided to a small size.<ref name=\"pearls\"/>\n\n==Variants==\n[[Donald Shell|D.L. Shell]] made substantial improvements to the algorithm; the modified version is called [[Shellsort|Shell sort]].  The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O(''n''<sup>3/2</sup>) and O(''n''<sup>4/3</sup>) running time{{Citation needed|date=October 2017}}.\n\nIf the cost of comparisons exceeds the cost of swaps, as is the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using ''binary insertion sort''{{Citation needed|date=September 2014}} may yield better performance. Binary insertion sort employs a [[binary search algorithm|binary search]] to determine the correct location to insert new elements, and therefore performs ⌈log<sub>2</sub>(''n'')⌉ comparisons in the worst case, which is O(''n'' log ''n''). The algorithm as a whole still has a running time of O(''n''<sup>2</sup>) on average because of the series of swaps required for each insertion.\n\nThe number of swaps can be reduced by calculating the position of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the right position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to [[merge sort]].\n\nA variant named ''binary merge sort'' uses a ''binary insertion sort'' to sort groups of 32 elements, followed by a final sort using [[merge sort]]. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets.<ref>{{cite web\n| title = Binary Merge Sort\n| url = https://docs.google.com/file/d/0B8KIVX-AaaGiYzcta0pFUXJnNG8\n|mode=cs2\n}}</ref>\n\nTo avoid having to make a series of swaps for each insertion, the input could be stored in a [[linked list]], which allows elements to be spliced into or out of the list in constant-time when the position in the list is known.<!-- Troubling statement when insert/delete is used. The typical insertion and deletion operations take O(n) if position is not known. The O(1) operations are cons/splice/rplacd. -->  However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search.  Therefore, the running time required for searching is O(''n'') and the time for sorting is O(''n''<sup>2</sup>).<!-- Instead of search, a max operation is more appropriate. --> If a more sophisticated [[data structure]] (e.g., [[heap (data structure)|heap]] or [[binary tree]]) is used, the time required for searching and insertion can be reduced significantly; this is the essence of [[heap sort]] and [[binary tree sort]].\n\nIn 2006 Bender, [[Farach-Colton|Martin Farach-Colton]], and Mosteiro published a new variant of insertion sort called ''[[library sort]]'' or ''gapped insertion sort'' that leaves a small number of unused spaces (i.e., \"gaps\") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O(''n''&nbsp;log&nbsp;''n'') time.<ref>{{citation\n | last1 = Bender | first1 = Michael A.\n | last2 = Farach-Colton | first2 = Martín | authorlink2 = Martin Farach-Colton\n | last3 = Mosteiro | first3 = Miguel A.\n | year = 2006\n | title = Insertion sort is ''O''(''n''&nbsp;log&nbsp;''n'')\n | journal = Theory of Computing Systems\n | volume = 39 | issue = 3 | pages = 391–397\n | arxiv = cs/0407003\n | doi = 10.1007/s00224-005-1237-z\n | mr = 2218409\n}}</ref>\n\nIf a [[skip list]] is used, the insertion time is brought down to O(log ''n''), and swaps are not needed because the skip list is implemented on a linked list structure.  The final running time for insertion would be O(''n'' log ''n'').\n\n''List insertion sort'' is a variant of insertion sort. It reduces the number of movements.{{Citation needed |date=September 2011}}\n\n===List insertion sort code in C===\nIf the items are stored in a linked list, then the list can be sorted with O(1) additional space.  The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list is empty, the sorted list has the desired result.\n\n<source lang=\"c\">\nstruct LIST * SortList1(struct LIST * pList) \n{\n    // zero or one element in list\n    if(pList == NULL || pList->pNext == NULL)\n        return pList;\n    // head is the first element of resulting sorted list\n    struct LIST * head = NULL;\n    while(pList != NULL) {\n        struct LIST * current = pList;\n        pList = pList->pNext;\n        if(head == NULL || current->iValue < head->iValue) {\n            // insert into the head of the sorted list\n            // or as the first element into an empty sorted list\n            current->pNext = head;\n            head = current;\n        } else {\n            // insert current element into proper position in non-empty sorted list\n            struct LIST * p = head;\n            while(p != NULL) {\n                if(p->pNext == NULL || // last element of the sorted list\n                   current->iValue < p->pNext->iValue) // middle of the list\n                {\n                    // insert into middle of the sorted list or as the last element\n                    current->pNext = p->pNext;\n                    p->pNext = current;\n                    break; // done\n                }\n                p = p->pNext;\n            }\n        }\n    }\n    return head;\n}\n</source>\n\nThe algorithm below uses a trailing pointer<ref>{{Citation |editor-last=Hill |editor-first=Curt |contribution=Trailing Pointer Technique |url=http://euler.vcsu.edu:7000/11421/ |title=Euler |publisher=Valley City State University |date= |accessdate=22 September 2012 |doi= }}.</ref> for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O(''n'') stack space.\n\n<source lang=\"c\">\nstruct LIST\n{\n  struct LIST * pNext;\n  int           iValue;\n};\n\nstruct LIST * SortList(struct LIST * pList)\n{\n  // zero or one element in list\n  if(!pList || !pList->pNext)\n      return pList;\n\n  /* build up the sorted array from the empty list */\n  struct LIST * pSorted = NULL;\n\n  /* take items off the input list one by one until empty */\n  while (pList != NULL)\n  {\n      /* remember the head */\n      struct LIST *   pHead  = pList;\n      /* trailing pointer for efficient splice */\n      struct LIST ** ppTrail = &pSorted;\n\n      /* pop head off list */\n      pList = pList->pNext;\n\n      /* splice head into sorted list at proper place */\n      while (!(*ppTrail == NULL || pHead->iValue < (*ppTrail)->iValue)) /* does head belong here? */\n      {\n          /* no - continue down the list */\n          ppTrail = &(*ppTrail)->pNext;\n      }\n\n      pHead->pNext = *ppTrail;\n      *ppTrail = pHead;\n  }\n\n  return pSorted;\n}\n</source>\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{Citation | author-link = Donald Knuth | first = Donald | last = Knuth | title = [[The Art of Computer Programming]] | volume = 3. Sorting and Searching | edition = second | publisher = Addison-Wesley | year = 1998 | ISBN = 0-201-89685-0 | chapter = 5.2.1: Sorting by Insertion | pages = 80–105}}.\n\n==External links==\n{{wikibooks|Algorithm implementation|Sorting/Insertion_sort|Insertion sort}}\n{{Commons category|Insertion sort}}\n* {{webarchive |url=https://web.archive.org/web/20150308232109/http://www.sorting-algorithms.com/insertion-sort |date=8 March 2015 |title=Animated Sorting Algorithms: Insertion Sort}} – graphical demonstration\n* {{Citation | publisher = Pathcom | url = http://www.pathcom.com/~vadco/binary.html | title = Binary Insertion Sort – Scoreboard – Complete Investigation and C Implementation | first = John Paul | last = Adamovsky}}.\n* {{Citation | url = http://corewar.co.uk/assembly/insertion.htm | title = Insertion Sort – a comparison with other O(n^2) sorting algorithms | publisher = Core war | place = [[United Kingdom|UK]]}}.\n* {{Citation | url = http://literateprograms.org/Category:Insertion_sort | title = Category:Insertion Sort | publisher = LiteratePrograms | type = wiki}} – implementations of insertion sort in various programming languages\n\n\n{{sorting}}\n\n{{DEFAULTSORT:Insertion Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n[[Category:Articles with example pseudocode]]\n[[Category:Online sorts]]\n\n[[no:Sorteringsalgoritme#Innstikksortering]]"
    },
    {
      "title": "Integer sorting",
      "url": "https://en.wikipedia.org/wiki/Integer_sorting",
      "text": "{{good article}}\nIn [[computer science]], '''integer sorting''' is the [[algorithm]]ic problem of [[Sorting algorithm|sorting]] a collection of data values by numeric keys, each of which is an [[integer]]. Algorithms designed for integer sorting may also often be applied to sorting problems in which the keys are [[floating point]] numbers, [[rational number]]s, or text strings.<ref name=\"ht02\">{{harvtxt|Han|Thorup|2002}}.</ref> The ability to perform integer arithmetic on the keys allows integer sorting algorithms to be faster than [[comparison sort]]ing algorithms in many cases, depending on the details of which operations are allowed in the model of computing and how large the integers to be sorted are.\n\nInteger sorting algorithms including [[pigeonhole sort]], [[counting sort]], and [[radix sort]] are widely used and practical. Other integer sorting algorithms with smaller worst-case time bounds are not believed to be practical for computer architectures with 64 or fewer bits per word. Many such algorithms are known, with performance depending on a combination of the number of items to be sorted, number of bits per key, and number of bits per word of the computer performing the sorting algorithm.\n\n==General considerations==\n\n===Models of computation===\nTime bounds for integer sorting algorithms typically depend on three parameters: the number {{mvar|n}} of data values to be sorted, the magnitude {{mvar|K}} of the largest possible key to be sorted, and the number {{mvar|w}} of bits that can be represented in a single machine word of the computer on which the algorithm is to be performed. Typically, it is assumed that {{math|''w'' ≥ log<sub>2</sub>(max(''n'', ''K''))}}; that is, that machine words are large enough to represent an index into the sequence of input data, and also large enough to represent a single key.<ref>{{harvtxt|Fredman|Willard|1993}}.</ref>\n\nInteger sorting algorithms are usually designed to work in either the [[pointer machine]] or [[random access machine]] models of computing. The main difference between these two models is in how memory may be addressed. The random access machine allows any value that is stored in a register to be used as the address of memory read and write operations, with unit cost per operation. This ability allows certain complex operations on data to be implemented quickly using table lookups. In contrast, in the pointer machine model, read and write operations use addresses stored in pointers, and it is not allowed to perform arithmetic operations on these pointers. In both models, data values may be added, and bitwise Boolean operations and binary shift operations may typically also be performed on them, in unit time per operation. Different integer sorting algorithms make different assumptions, however, about whether integer multiplication is also allowed as a unit-time operation.<ref>The question of whether integer multiplication or table lookup operations should be permitted goes back to {{harvtxt|Fredman|Willard|1993}}; see also {{harvtxt|Andersson|Miltersen|Thorup|1999}}.</ref> Other more specialized models of computation such as the [[parallel random access machine]] have also been considered.<ref>{{harvtxt|Reif|1985}}; comment in {{harvtxt|Cole|Vishkin|1986}}; {{harvtxt|Hagerup|1987}}; {{harvtxt|Bhatt|Diks|Hagerup|Prasad|1991}}; {{harvtxt|Albers|Hagerup|1997}}.</ref>\n\n{{harvtxt|Andersson|Miltersen|Thorup|1999}} showed that in some cases the multiplications or table lookups required by some integer sorting algorithms could be replaced by customized operations that would be more easily implemented in hardware but that are not typically available on general-purpose computers. {{harvtxt|Thorup|2003}} improved on this by showing how to replace these special operations by the [[bit field]] manipulation instructions already available on [[Pentium]] processors.\n\n===Sorting versus integer priority queues===\nA [[priority queue]] is a data structure for maintaining a collection of items with numerical priorities, having operations for finding and removing the item with the minimum priority value. Comparison-based priority queues such as the [[binary heap]] take logarithmic time per update, but other structures such as the [[van Emde Boas tree]] or [[bucket queue]] may be faster for inputs whose priorities are small integers. These data structures can be used in the [[selection sort]] algorithm, which sorts a collection of elements by repeatedly finding and removing the smallest element from the collection, and returning the elements in the order they were found. A priority queue can be used to maintain the collection of elements in this algorithm, and the time for this algorithm on a collection of {{mvar|n}} elements can be bounded by the time to initialize the priority queue and then to perform {{mvar|n}} find and remove operations. For instance, using a [[binary heap]] as a priority queue in selection sort leads to the [[heap sort]] algorithm, a comparison sorting algorithm that takes {{math|''O''(''n'' log ''n'')}} time. Instead, using selection sort with a bucket queue gives a form of [[pigeonhole sort]], and using van Emde Boas trees or other integer priority queues leads to other fast integer sorting algorithms.{{sfnp|Chowdhury|2008}}\n\nInstead of using an integer priority queue in a sorting algorithm, it is possible to go the other direction, and use integer sorting algorithms as subroutines within an integer priority queue data structure. {{harvtxt|Thorup|2007}} used this idea to show that, if it is possible to perform integer sorting in time {{math|''T''(''n'')}} per key, then the same time bound applies to the time per insertion or deletion operation in a priority queue data structure. Thorup's reduction is complicated and assumes the availability of either fast multiplication operations or table lookups, but he also provides an alternative priority queue using only addition and Boolean operations with time {{math|''T''(''n'') + ''T''(log ''n'') + ''T''(log log ''n'') + ...}} per operation, at most multiplying the time by an [[iterated logarithm]].{{sfnp|Chowdhury|2008}}\n\n===Usability===\nThe classical integer sorting algorithms of [[pigeonhole sort]], [[counting sort]], and [[radix sort]] are widely used and practical.<ref>{{harvtxt|McIlroy|Bostic|McIlroy|1993}}; {{harvtxt|Andersson|Nilsson|1998}}.</ref> Much of the subsequent research on integer sorting algorithms has focused less on practicality and more on theoretical improvements in their [[worst case analysis]], and the algorithms that come from this line of research are not believed to be practical for current [[64-bit]] computer architectures, although\nexperiments have shown that some of these methods may be an improvement on radix sorting for data with 128 or more bits per key.<ref name=\"rr98\">{{harvtxt|Rahman|Raman|1998}}.</ref> Additionally, for large data sets, the near-random [[memory access pattern]]s of many integer sorting algorithms can handicap them compared to comparison sorting algorithms that have been designed with the [[memory hierarchy]] in mind.<ref>{{harvtxt|Pedersen|1999}}.</ref>\n\nInteger sorting provides one of the six [[benchmark (computing)|benchmark]]s in the [[DARPA]] [[High Productivity Computing Systems]] Discrete Mathematics benchmark suite,<ref>[http://www.cse.sc.edu/~buell/Public_Data/DARPA_HPCS/DARPA_discrete_math.html DARPA HPCS Discrete Mathematics Benchmarks], Duncan A. Buell, University of South Carolina, retrieved 2011-04-20.</ref> and one of eleven benchmarks in the [[NAS Parallel Benchmarks]] suite.\n\n==Practical algorithms==\n[[Pigeonhole sort]] or [[counting sort]] can both sort {{mvar|n}} data items having keys in the range from {{math|0}} to {{math|''K'' &minus; 1}} in time {{math|''O''(''n'' + ''K'')}}. In [[pigeonhole sort]] (often called bucket sort), pointers to the data items are distributed to a table of buckets, represented as [[Collection (computing)|collection]] data types such as [[linked list]]s, using the keys as indices into the table. Then, all of the buckets are concatenated together to form the output list.<ref>{{harvtxt|Goodrich|Tamassia|2002}}. Although {{harvtxt|Cormen|Leiserson|Rivest|Stein|2001}} also describe a version of this sorting algorithm, the version they describe is adapted to inputs where the keys are real numbers with a known distribution, rather than to integer sorting.</ref> Counting sort uses a table of counters in place of a table of buckets, to determine the number of items with each key. Then, a [[prefix sum]] computation is used to determine the range of positions in the sorted output at which the values with each key should be placed. Finally, in a second pass over the input, each item is moved to its key's position in the output array.<ref>{{harvtxt|Cormen|Leiserson|Rivest|Stein|2001}}, 8.2 Counting Sort, pp. 168–169.</ref> Both algorithms involve only simple loops over the input data (taking time {{math|''O''(''n'')}}) and over the set of possible keys (taking time {{math|''O''(''K'')}}), giving their {{math|''O''(''n'' + ''K'')}} overall time bound.\n\n[[Radix sort]] is a sorting algorithm that works for larger keys than pigeonhole sort or counting sort by performing multiple passes over the data. Each pass sorts the input using only part of the keys, by using a different sorting algorithm (such as pigeonhole sort or counting sort) that is suited only for small keys. To break the keys into parts, the radix sort algorithm computes the [[positional notation]] for each key,\naccording to some chosen [[radix]]; then, the part of the key used for the {{mvar|i}}th pass of the algorithm is the {{mvar|i}}th digit in the positional notation for the full key, starting from the least significant digit and progressing to the most significant. For this algorithm to work correctly, the sorting algorithm used in each pass over the data must be [[Sorting algorithm#Stability|stable]]: items with equal digits should not change positions with each other. For greatest efficiency, the radix should be chosen to be near the number of data items, {{mvar|''n''}}. Additionally, using a [[power of two]] near {{mvar|n}} as the radix allows the keys for each pass to be computed quickly using only fast binary shift and mask operations. With these choices,  and with pigeonhole sort or counting sort as the base algorithm, the radix sorting algorithm can sort {{mvar|n}} data items having keys in the range from {{math|0}} to {{math|''K'' &minus; 1}} in time {{math|''O''(''n'' log<sub>''n''</sub> ''K'')}}.<ref>{{harvtxt|Comrie|1929–1930}}; {{harvtxt|Cormen|Leiserson|Rivest|Stein|2001}}, 8.3 Radix Sort, pp. 170–173.</ref>\n\n==Theoretical algorithms==\nMany integer sorting algorithms have been developed whose theoretical analysis shows them to behave better than comparison sorting, pigeonhole sorting, or radix sorting for large enough\ncombinations of the parameters defining the number of items to be sorted, range of keys, and machine word size.\nWhich algorithm has the best performance depends on the values of these parameters.\nHowever, despite their theoretical advantages, these algorithms are not an improvement for the typical ranges of these parameters that arise in practical sorting problems.<ref name=\"rr98\"/>\n\n===Algorithms for small keys===\nA [[Van Emde Boas tree]] may be used as a priority queue to sort a set of {{mvar|n}} keys, each in the range from {{math|0}} to {{math|''K'' &minus; 1}}, in time {{math|''O''(''n'' log log ''K'')}}. This is a theoretical improvement over radix sorting when {{mvar|K}} is sufficiently large. However, in order to use a Van Emde Boas tree, one either needs a directly addressable memory of {{mvar|K}} words, or one needs to simulate it using a [[hash table]], reducing the space to linear but making the algorithm be randomized. Another priority queue with similar performance (including the need for randomization in the form of hash tables) is the [[Y-fast trie]] of {{harvtxt|Willard|1983}}.\n\nA more sophisticated technique with a similar flavor and with better theoretical performance was developed by {{harvtxt|Kirkpatrick|Reisch|1984}}. They observed that each pass of radix sort can be interpreted as a range reduction technique that, in linear time, reduces the maximum key size by a factor of&nbsp;{{mvar|n}}; instead, their technique reduces the key size to the square root of its previous value (halving the number of bits needed to represent a key), again in linear time. As in radix sort, they interpret the keys as two-digit base-{{mvar|b}} numbers for a base {{mvar|b}} that is approximately {{math|{{sqrt|''K''}}}}. They then group the items to be sorted into buckets according to their high digits, in linear time, using either a large but uninitialized direct addressed memory or a hash table. Each bucket has a representative, the item in the bucket with the largest key; they then sort the list of items using as keys the high digits for the representatives and the low digits for the non-representatives. By grouping the items from this list into buckets again, each bucket may be placed into sorted order, and by extracting the representatives from the sorted list the buckets may be concatenated together into sorted order. Thus, in linear time, the sorting problem is reduced to another recursive sorting problem in which the keys are much smaller, the square root of their previous magnitude. Repeating this range reduction until the keys are small enough to bucket sort leads to an algorithm with running time {{math|O(''n'' log log<sub>''n''</sub> ''K'')}}.\n\nA complicated randomized algorithm of {{harvtxt|Han|Thorup|2002}} in the [[word RAM]] [[model of computation]] allows these time bounds to be reduced even farther, to {{math|O(''n''{{sqrt|log log ''K''}})}}.\n\n===Algorithms for large words===\nAn integer sorting algorithm is said to be ''non-conservative'' if it requires a word size {{mvar|w}} that is significantly larger than {{math|log max(''n'', ''K'')}}.<ref>{{harvtxt|Kirkpatrick|Reisch|1984}}; {{harvtxt|Albers|Hagerup|1997}}.</ref> As an extreme instance, if {{math|''w'' ≥ ''K''}}, and all keys are distinct, then the set of keys may be sorted in linear time by representing it as a [[bitvector]], with a 1 bit in position {{mvar|i}} when {{mvar|i}} is one of the input keys, and then repeatedly removing the least significant bit.<ref>{{harvtxt|Kirkpatrick|Reisch|1984}}.</ref>\n\nThe non-conservative packed sorting algorithm of {{harvtxt|Albers|Hagerup|1997}} uses a subroutine, based on [[Ken Batcher]]'s [[bitonic sorter|bitonic sorting network]], for [[merge algorithm|merging]] two sorted sequences of keys that are each short enough to be packed into a single machine word. The input to the packed sorting algorithm, a sequence of items stored one per word, is transformed into a packed form, a sequence of words each holding multiple items in sorted order, by using this subroutine repeatedly to double the number of items packed into each word. Once the sequence is in packed form, Albers and Hagerup use a form of [[merge sort]] to sort it; when two sequences are being merged to form a single longer sequence, the same bitonic sorting subroutine can be used to repeatedly extract packed words consisting of the smallest remaining elements of the two sequences. This algorithm gains enough of a speedup from its packed representation to sort its input in linear time whenever it is possible for a single word to contain  {{math|&Omega;(log ''n'' log log ''n'')}} keys; that is, when {{math|log ''K'' log ''n'' log log ''n'' ≤ ''cw''}} for some constant {{math|''c'' > 0}}.\n\n===Algorithms for few items===\nPigeonhole sort, counting sort, radix sort, and Van Emde Boas tree sorting all work best when the key size is small; for large enough keys, they become slower than comparison sorting algorithms. However, when the key size or the word size is very large relative to the number of items (or equivalently when the number of items is small), it may again become possible to sort quickly, using different algorithms that take advantage of the parallelism inherent in the ability to perform arithmetic operations on large words.\n\nAn early result in this direction was provided by {{harvtxt|Ajtai|Fredman|Komlós|1984}} using the [[cell-probe model]] of computation (an artificial model in which the complexity of an algorithm is measured only by the number of memory accesses it performs). Building on their work, {{harvtxt|Fredman|Willard|1994}} described two data structures, the Q-heap and the atomic heap, that are implementable on a random access machine. The Q-heap is a bit-parallel version of a binary [[trie]], and allows both priority queue operations and successor and predecessor queries to be performed in constant time for sets of {{math|''O''((log ''N'')<sup>1/4</sup>)}} items, where {{math|''N'' ≤ 2<sup>''w''</sup>}} is the size of the precomputed tables needed to implement the data structure. The atomic heap is a [[B-tree]] in which each tree node is represented as a Q-heap; it allows constant time priority queue operations (and therefore sorting) for sets of {{math|(log ''N'')<sup>''O''(1)</sup>}} items.\n\n{{harvtxt|Andersson|Hagerup|Nilsson|Raman|1998}} provide a randomized algorithm called signature sort that allows for linear time sorting of sets of up to {{math|2<sup>''O''((log ''w'')<sup>1/2 &minus; ε</sup>)</sup>}} items at a time, for any constant {{math|ε > 0}}. As in the algorithm of Kirkpatrick and Reisch, they perform range reduction using a representation of the keys as numbers in base&nbsp;{{mvar|b}} for a careful choice of {{mvar|b}}. Their range reduction algorithm replaces each digit by a signature, which is a hashed value with {{math|''O''(log ''n'')}} bits such that different digit values have different signatures. If {{mvar|n}} is sufficiently small, the numbers formed by this replacement process will be significantly smaller than the original keys, allowing the non-conservative packed sorting algorithm of {{harvtxt|Albers|Hagerup|1997}} to sort the replaced numbers in linear time. From the sorted list of replaced numbers, it is possible to form a compressed [[trie]] of the keys in linear time, and the children of each node in the trie may be sorted recursively using only keys of size {{mvar|b}}, after which a tree traversal produces the sorted order of the items.\n\n===Trans-dichotomous algorithms===\n{{harvtxt|Fredman|Willard|1993}} introduced the [[transdichotomous model]] of analysis for integer sorting algorithms, in which nothing is assumed about the range of the integer keys and one must bound the algorithm's performance by a function of the number of data values alone. Alternatively, in this model, the running time for an algorithm on a set of {{mvar|n}} items is assumed to be the [[worst case]] running time for any possible combination of values of {{mvar|K}} and&nbsp;{{mvar|w}}. The first algorithm of this type was Fredman and Willard's [[fusion tree]] sorting algorithm, which runs in time {{math|O(''n'' log ''n'' / log log ''n'')}}; this is an improvement over comparison sorting for any choice of {{mvar|K}} and&nbsp;{{mvar|w}}. An alternative version of their algorithm that includes the use of random numbers and integer division operations improves this to {{math|O(''n''{{sqrt|log ''n''}})}}.\n\nSince their work, even better algorithms have been developed. For instance, by repeatedly applying the Kirkpatrick–Reisch range reduction technique until the keys are small enough to apply the Albers–Hagerup packed sorting algorithm, it is possible to sort in time {{math|''O''(''n'' log log ''n'')}}; however, the range reduction part of this algorithm requires either a large memory (proportional to {{math|{{sqrt|''K''}}}}) or randomization in the form of hash tables.<ref>{{harvtxt|Andersson|Hagerup|Nilsson|Raman|1998}}.</ref>\n\n{{harvtxt|Han|Thorup|2002}} showed how to sort in randomized time {{math|O(''n''{{sqrt|log log ''n''}})}}. Their technique involves using ideas related to signature sorting to partition the data into many small sublists, of a size small enough that signature sorting can sort each of them efficiently. It is also possible to use similar ideas to sort integers deterministically in time {{math|''O''(''n'' log log ''n'')}} and linear space.<ref>{{harvtxt|Han|2004}}.</ref> Using only simple arithmetic operations (no multiplications or table lookups) it is possible to sort in randomized expected time {{math|''O''(''n'' log log ''n'')}}<ref>{{harvtxt|Thorup|2002}}</ref> or deterministically in time {{math|''O''(''n'' (log log ''n'')<sup>1 + ε</sup>)}} for any constant {{math|ε > 0}}.<ref name=\"ht02\"/>\n\n==References==\n;Footnotes\n{{reflist|30em}}\n\n;Secondary sources\n{{refbegin|30em}}\n*{{citation\n | last = Chowdhury | first = Rezaul A.\n | editor-last = Kao | editor-first = Ming-Yang\n | contribution = Equivalence between priority queues and sorting\n | contribution-url = https://books.google.com/books?id=i3S9_GnHZwYC&pg=PA278\n | isbn = 9780387307701\n | pages = 278–281\n | publisher = Springer\n | title = Encyclopedia of Algorithms\n | year = 2008}}.\n*{{citation\n | last1 = Cormen | first1 = Thomas H. | author1-link = Thomas H. Cormen\n | last2 = Leiserson | first2 = Charles E. | author2-link = Charles E. Leiserson\n | last3 = Rivest | first3 = Ronald L. | author3-link = Ron Rivest\n | last4 = Stein | first4 = Clifford | author4-link = Clifford Stein\n | edition = 2nd\n | isbn = 0-262-03293-7\n | publisher = [[MIT Press]] and [[McGraw-Hill]]\n | title = [[Introduction to Algorithms]]\n | year = 2001}}.\n*{{citation\n | last1 = Goodrich | first1 = Michael T. | author1-link = Michael T. Goodrich\n | last2 = Tamassia | first2 = Roberto | author2-link = Roberto Tamassia\n | contribution = 4.5 Bucket-Sort and Radix-Sort\n | pages = 241–243\n | publisher = John Wiley & Sons\n | title = Algorithm Design: Foundations, Analysis, and Internet Examples\n | year = 2002}}.\n{{refend}}\n\n;Primary sources\n{{refbegin|30em}}\n*{{citation\n | last1 = Ajtai | first1 = M. | author1-link = Miklós Ajtai\n | last2 = Fredman | first2 = M. | author2-link = Michael Fredman\n | last3 = Komlós | first3 = J. | author3-link = János Komlós (mathematician)\n | doi = 10.1016/S0019-9958(84)80015-7\n | mr = 837087\n | issue = 3\n | journal = [[Information and Computation|Information and Control]]\n | pages = 217–225\n | title = Hash functions for priority queues\n | volume = 63\n | year = 1984}}.\n*{{citation\n | last1 = Albers | first1 = Susanne | authorlink = Susanne Albers\n | last2 = Hagerup | first2 = Torben\n | doi = 10.1006/inco.1997.2632\n | mr = 1457693\n | issue = 1\n | journal = [[Information and Computation]]\n | pages = 25–51\n | title = Improved parallel integer sorting without concurrent writing\n | volume = 136\n | year = 1997}}.\n*{{citation\n | last1 = Andersson | first1 = Arne\n | last2 = Hagerup | first2 = Torben\n | last3 = Nilsson | first3 = Stefan\n | last4 = Raman | first4 = Rajeev\n | doi = 10.1006/jcss.1998.1580\n | mr = 1649809\n | issue = 1\n | journal = [[Journal of Computer and System Sciences]]\n | pages = 74–93\n | title = Sorting in linear time?\n | volume = 57\n | year = 1998}}.\n*{{citation\n | last1 = Andersson | first1 = Arne\n | last2 = Nilsson | first2 = Stefan\n | doi = 10.1145/297096.297136\n | mr = 1717389\n | journal = ACM Journal of Experimental Algorithmics\n | title = Implementing radixsort\n | volume = 3\n | year = 1998}}.\n*{{citation\n | last1 = Andersson | first1 = Arne\n | last2 = Miltersen | first2 = Peter Bro\n | last3 = Thorup | first3 = Mikkel | author3-link = Mikkel Thorup\n | doi = 10.1016/S0304-3975(98)00172-8\n | mr = 1678804\n | issue = 1-2\n | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]\n | pages = 337–344\n | title = Fusion trees can be implemented with {{math|AC<sup>0</sup>}} instructions only\n | volume = 215\n | year = 1999}}.\n*{{citation\n | last1 = Bhatt | first1 = P. C. P.\n | last2 = Diks | first2 = K.\n | last3 = Hagerup | first3 = T.\n | last4 = Prasad | first4 = V. C.\n | last5 = Radzik | first5 = T.\n | last6 = Saxena | first6 = S.\n | doi = 10.1016/0890-5401(91)90031-V\n | mr = 1123154\n | issue = 1\n | journal = [[Information and Computation]]\n | pages = 29–47\n | title = Improved deterministic parallel integer sorting\n | volume = 94\n | year = 1991}}.\n*{{citation\n | last1 = Cole | first1 = R.\n | last2 = Vishkin | first2 = U. | author2-link = Uzi Vishkin\n | doi = 10.1016/S0019-9958(86)80023-7\n | issue = 1\n | journal = [[Information and Computation|Information and Control]]\n | pages = 32–53\n | title = Deterministic coin tossing with applications to optimal parallel list ranking\n | volume = 70\n | year = 1986}}.\n*{{citation\n | last = Comrie | first = L. J.\n | journal = Trans. Office Mach. Users’ Assoc., Ltd.\n | pages = 25–37\n | title = The Hollerith and Powers tabulating machines\n | year = 1929–1930|ref={{harvid|Comrie|1929–1930}}}}. Cited by {{harvtxt|Thorup|2007}} as an early source for [[radix sort]].\n*{{citation\n | last1 = Fredman | first1 = Michael L. | author1-link = Michael Fredman\n | last2 = Willard | first2 = Dan E. | author2-link = Dan Willard\n | doi = 10.1016/0022-0000(93)90040-4\n | mr = 1248864\n | issue = 3\n | journal = [[Journal of Computer and System Sciences]]\n | pages = 424–436\n | title = Surpassing the information-theoretic bound with fusion trees\n | volume = 47\n | year = 1993}}.\n*{{citation\n | last1 = Fredman | first1 = Michael L. | author1-link = Michael Fredman\n | last2 = Willard | first2 = Dan E. | author2-link = Dan Willard\n | doi = 10.1016/S0022-0000(05)80064-9\n | mr = 1279413\n | issue = 3\n | journal = [[Journal of Computer and System Sciences]]\n | pages = 533–551\n | title = Trans-dichotomous algorithms for minimum spanning trees and shortest paths\n | volume = 48\n | year = 1994}}.\n*{{citation\n | last = Hagerup | first = Torben\n | mr = 910976\n | issue = 1\n | journal = [[Information and Computation]]\n | pages = 39–51\n | title = Towards optimal parallel bucket sorting\n | volume = 75\n | year = 1987\n | doi=10.1016/0890-5401(87)90062-9}}.\n*{{citation\n | last = Han | first = Yijie\n | doi = 10.1016/j.jalgor.2003.09.001\n | mr = 2028585\n | issue = 1\n | journal = Journal of Algorithms\n | pages = 96–105\n | title = Deterministic sorting in {{math|''O''(''n'' log log ''n'')}} time and linear space\n | volume = 50\n | year = 2004}}.\n*{{citation\n | last1 = Han | first1 = Yijie\n | last2 = Thorup | first2 = M. | author2-link = Mikkel Thorup\n | contribution = Integer sorting in {{math|O(''n''{{sqrt|log log ''n''}})}} expected time and linear space\n | doi = 10.1109/SFCS.2002.1181890\n | pages = 135–144\n | publisher = IEEE Computer Society\n | title = [[Symposium on Foundations of Computer Science|Proceedings of the 43rd Annual Symposium on Foundations of Computer Science (FOCS 2002)]]\n | year = 2002}}.\n*{{citation\n | last1 = Kirkpatrick | first1 = David | author1-link = David G. Kirkpatrick\n | last2 = Reisch | first2 = Stefan\n | doi = 10.1016/0304-3975(83)90023-3\n | mr = 742289\n | issue = 3\n | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]\n | pages = 263–276\n | title = Upper bounds for sorting integers on random access machines\n | volume = 28\n | year = 1984}}.\n*{{citation\n | last1 = McIlroy | first1 = Peter M.\n | last2 = Bostic | first2 = Keith\n | last3 = McIlroy | first3 = M. Douglas\n | issue = 1\n | journal = Computing Systems\n | pages = 5–27\n | title = Engineering Radix Sort\n | url = http://www.usenix.org/publications/compsystems/1993/win_mcilroy.pdf\n | volume = 6\n | year = 1993}}.\n*{{citation\n | last = Pedersen | first = Morten Nicolaj\n | publisher = Department of Computer Science, University of Copenhagen, Denmark\n | series = Masters thesis\n | title = A study of the practical significance of word RAM algorithms for internal integer sorting\n | url = http://www.diku.dk/forskning/performance-engineering/Publications/pedersen99.ps\n | year = 1999}}.\n*{{citation\n | last1 = Rahman | first1 = Naila\n | last2 = Raman | first2 = Rajeev\n | contribution = An experimental study of word-level parallelism in some sorting algorithms\n | pages = 193–203\n | publisher = [[Max Planck Institute for Computer Science]]\n | title = Algorithm Engineering, 2nd International Workshop, WAE '92, Saarbrücken, Germany, August 20–22, 1998, Proceedings\n | url = http://www.cs.ru.nl/~elenam/WAEMS.pdf\n | year = 1998}}.\n*{{citation\n | last = Reif | first = John H. | author-link = John Reif\n | contribution = An optimal parallel algorithm for integer sorting\n | doi = 10.1109/SFCS.1985.9\n | pages = 496–504\n | publisher = IEEE Computer Society\n | title = [[Symposium on Foundations of Computer Science|Proceedings of the 26th Annual Symposium on Foundations of Computer Science (FOCS 1985)]]\n | year = 1985}}.\n*{{citation\n | last = Thorup | first = Mikkel | authorlink = Mikkel Thorup\n | doi = 10.1006/jagm.2002.1211\n | mr = 1895974\n | issue = 2\n | journal = Journal of Algorithms\n | pages = 205–230\n | title = Randomized sorting in {{math|''O''(''n'' log log ''n'')}} time and linear space using addition, shift, and bit-wise Boolean operations\n | volume = 42\n | year = 2002}}.\n*{{citation\n | last = Thorup | first = Mikkel | authorlink = Mikkel Thorup\n | contribution = On {{math|AC<sup>0</sup>}} implementations of fusion trees and atomic heaps\n | mr = 1974982\n | location = New York\n | pages = 699–707\n | publisher = ACM\n | title = Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (Baltimore, MD, 2003)\n | url = http://portal.acm.org/citation.cfm?id=644221\n | year = 2003}}.\n*{{citation\n | last = Thorup | first = Mikkel | authorlink = Mikkel Thorup\n | doi = 10.1145/1314690.1314692\n | mr = 2374029\n | issue = 6\n | journal = [[Journal of the ACM]]\n | page = Art. 28\n | title = Equivalence between priority queues and sorting\n | volume = 54\n | year = 2007}}.\n*{{citation\n | last = Willard | first = Dan E. | authorlink = Dan Willard\n | doi = 10.1016/0020-0190(83)90075-3\n | mr = 731126\n | issue = 2\n | journal = [[Information Processing Letters]]\n | pages = 81–84\n | title = Log-logarithmic worst-case range queries are possible in space {{math|Θ(''N'')}}\n | volume = 17\n | year = 1983}}.\n{{refend}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Internal sort",
      "url": "https://en.wikipedia.org/wiki/Internal_sort",
      "text": "An '''internal sort''' is any data sorting process that takes place entirely within the [[main memory]] of a computer. This is possible whenever the data to be sorted is small enough to all be held in the main memory. For sorting larger datasets, it may be necessary to hold only a chunk of data in memory at a time, since it won’t all fit. The rest of the data is normally held on some larger, but slower medium, like a hard-disk. Any reading or writing of data to and from this slower media can slow the sortation process considerably. This issue has implications for different [[sort algorithms]].\n\nSome common internal sorting algorithms include:\n# [[Bubble Sort]]\n# [[Insertion Sort]]\n# [[Quick Sort]]\n# [[Heap Sort]]\n# [[Radix Sort]]\n# [[Selection sort]]\nConsider a [[Bubblesort]], where adjacent records are swapped in order to get them into the right order, so that records appear to “bubble” up and down through the dataspace. If this has to be done in chunks, then when we have sorted all the records in chunk 1, we move on to chunk 2, but we find that some of the records in chunk 1 need to “bubble through” chunk 2, and vice versa (i.e., there are records in chunk 2 that belong in chunk 1, and records in chunk 1 that belong in chunk 2 or later chunks). This will cause the chunks to be read and written back to disk many times as records cross over the boundaries between them, resulting in a considerable degradation of performance. If the data can all be held in memory as one large chunk, then this performance hit is avoided.\n\nOn the other hand, some algorithms handle [[external sorting]] rather better. A [[Merge sort]] breaks the data up into chunks, sorts the chunks by some other algorithm (maybe bubblesort or [[Quick sort]]) and then recombines the chunks two by two so that each recombined chunk is in order. This approach minimises the number or reads and writes of data-chunks from disk, and is a popular external sort method.\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Interpolation sort",
      "url": "https://en.wikipedia.org/wiki/Interpolation_sort",
      "text": "Interpolation sort is a [[sorting algorithm]] that uses interpolation formulas to disperse data and manipulates the recursive method with label arrays.Interpolation sort is a recursive sorting method for [[bucket sort]] variant [[histogram sort]] under the distribution sorts class.\n==Algorithm==\n\n\n{{Infobox Algorithm\n|name=Interpolaion Sort\n|class=[[Sorting Algorithm]]\n|image=\n|data=[[Array]]\n|best-time=<math>O(n)</math>\n|time=<math>O(n^2)</math> \n|average-time=<math>O(n+k)</math>\n|space=<math>O(n*3)</math>\n|optimal= <math>O(n)</math>\n}} \n\n'''Interpolation sort''' (or histogram sort).\n<ref>{{cite web |url = https://xlinux.nist.gov/dads/HTML/interpolationSort.html |title =interpolation sort  |author = NIST Algorithm |language = en |quote = \"Definition: See histogram sort.\"}}\n</ref>\nIt is a sorting algorithm that uses the [[interpolation]] formula to disperse data [[Divide-and-conquer algorithm|divide and conquer]]. Interpolation sort is also a variant of [[bucket sort]] [[algorithm]].\n<ref>\n{{cite web |url = https://en.wikipedia.org/wiki/Bucket_sort#Histogram_sort |title = Histogram sort |author = en.wikipedia |language = en |quote =\"Another variant of bucket sort known as histogram sort or counting sort adds an initial pass that counts the number of elements that will fall into each bucket using a count array. Using this information, the array values can be arranged into a sequence of buckets in-place by a sequence of exchanges, leaving no space overhead for bucket storage.\" }}\n</ref> \nThe interpolation sort method uses an array of record bucket lengths corresponding to the original number column. By operating the maintenance length array, the [[recursive]] algorithm can be prevented from changing the space complexity to <math>O(n^ 2)</math> due to memory stacking. The segmentation record of the length array can using secondary function dynamically declare and delete the memory space of the [[array]]. The space complexity required to control the recursive program is <math>O(3n)</math>. Contains a two-dimensional array of dynamically allocated memories and an array of record lengths. However the execution complexity can still be maintained as an efficient sorting method of <math>O(n + k)</math>.\n<ref>\n{{Cite web \n|url=https://zh.wikipedia.org/wiki/%E6%A1%B6%E6%8E%92%E5%BA%8F\n|title = 桶排序（Bucket sort）\n|author = zh.wikipedia \n|language = chinese\n|quote = 平均時間複雜度（Average performance）<math>O(n+k)</math>}}\n</ref>\n[[Array]] of dynamically allocated memory can be implemented by [[linked list]], [[Stack (abstract data type)|stack]], [[Queue (abstract data type)|queue]], [[associative array]], [[tree structure]], etc. An array object such as [[JavaScript]] is applicable. The difference in [[data structure]] is related to the speed of data access and thus the time required for [[sorting]].When the values in the ordered array are uniformly distributed approximately the [[arithmetic progression]], the linear time of interpolation sort ordering is <math>O(n)</math>.\n<ref>\n{{cite web |url = https://en.wikipedia.org/wiki/Bucket_sort#Average-case_analysis |title = Bucket sort Average-case analysis |author = Wikipedia |publisher = en.wikipedia |language = en |quote = \"Note that if k is chosen to be <math>k = n</math> , then bucket sort runs in <math>O(n)</math> average time, given a uniformly distributed input.\"}}\n</ref>\n\n===Interpolation sort algorithm===\n#Set a length array to record the length of the uncompleted sorting bucket,Set a quantitative array as an empty bucket.\n#The main sorting program determines whether the length array is emptied.Not cleared execution divide.\n#Take a length from the end of the length array to perform divide.If the maximum value is equal to the minimum value, the sequence sorting is completed to stop the divide.\n#Search for the sequence and place the items one by one into the bucket corresponding to the interpolation.\n#Put the items back into the original sequence one by one from a bucket that is not empty.And put the length array into the length of the bucket linked list.\n#Recursive return to the main sorting processing.\n\n==Practice==\nJavaScript code:\n<source lang=\"javascript\">\nArray.prototype.interpolationSort = function(){\n\tvar divideSize = new Array();\n\tvar end = this.length;\n\tdivideSize[0] = this.length;\n\twhile (divideSize.length > 0) divide(this);\n\n\tfunction divide(A){\n\t\tvar size = divideSize.pop();\n\t\tvar start = end - size;\n\t\tvar min = A[start];\n\t\tvar max = A[start];\n\t\tfor ( i = start + 1; i < end; i++)\n\t\t\tif (A[i] < min) min = A[i];\n\t\t\telse if (A[i] > max) max = A[i];\n\t\tif (min == max) end = end - size;\n\t\telse{\n\t\t\tvar p = 0;\n\t\t\tvar bucket = new Array(size);\n\t\t\tfor ( i = 0; i < size; i++)\n\t\t\t\tbucket[i] = new Array();\n\t\t\tfor ( i = start; i < end; i++){\n\t\t\t\tp = Math.floor(((A[i] - min ) / (max - min ) ) * (size - 1 ));\n\t\t\t\tbucket[p].push(A[i]);\n\t\t\t}\n\t\t\tfor ( i = 0; i < size; i++){\n\t\t\t\tif (bucket[i].length > 0) {\n\t\t\t\t\tfor ( j = 0; j < bucket[i].length; j++){\n\t\t\t\t\t    A[start++] = bucket[i][j];\n\t\t\t\t\t    divideSize.push(bucket[i].length);}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n};\n</source>\n\n==Variant==\n\n===Interpolation Tag Sort===\n{{Infobox Algorithm\n|name=Interpolaion Tag Sort\n|class=[[Sorting Algorithm]]\n|image=\n|data=[[Array]]\n|time=<math>O(n^2)</math>\n|best-time=<math>O(n)</math> \n|average-time=<math>O(n+k)</math>\n|space=<math>O(2n+(n)bits)</math>\n|optimal= <math>O(n)</math>\n}} \n\nInterpolation Tag Sort is a variant of Interpolation Sort. Applying the bucket sorting and dividing method, the array data is distributed into a limited number of buckets by mathematical interpolation formula, and the bucket then [[recursively]] the original processing program until the sorting is completed.\n\nBucket[ p ] is interpolation corresponding position.\nInterpolation formula: \np = ( A[ i ] - minimum) / (maximum - minimum) * (size - 1)\n\nInterpolation tag sort is a recursive sorting method for interpolation sorting. To avoid stacking overflow caused by recursion, the memory crashes. Instead, use a Boolean data type tag array to operate the recursive function to release the memory. The extra memory space required is close to <math>2n+(n)bits</math>. Contains a two-dimensional array of dynamically allocated memory and a Boolean data type tag array. Stack, queue, associative array, and tree structure can be implemented as buckets.\n\nAs the JavaScript array object is suitable for this sorting method, the difference in data structure is related to the speed of data access and thus the time required for sorting. The linear time Θ(n) is used when the values in the array to be sorted are evenly distributed.The bucket sort algorithm does not limit the sorting to the lower limit of <math>O(n log n)</math>. Interpolation tag sort average performance complexity is <math>O(n + k)</math>.\n<ref>\n{{Cite web \n|url=https://zh.wikipedia.org/wiki/%E6%A1%B6%E6%8E%92%E5%BA%8F\n|title = 桶排序（Bucket sort）\n|author = zh.wikipedia \n|language = chinese\n|quote = 平均時間複雜度（Average performance）<math>O(n+k)</math>}}\n</ref>\n\n====Interpolation Tag Sort Algorithm====\n#Set an equal number of tag arrays to initialize to false values.\n#The main sorting program determines whether all the sections of the original sequence have been recursively completed, Execution bucket sort when not completed.\n#Execute bucket sort; Stop bucket sort if the section has been sorted.\n#Set a two-dimensional array as an empty bucket, and the search sequence puts the data one by one into the corresponding bucket.\n# Put the item back into the original sequence one by one from the empty bucket, and mark the starting position of the section as true in the tag array.\n#Return the next section header location for the main sorting process.\n\n====Practice====\nJavaScript code:\n<source lang=\"javascript\">\nArray.prototype.InterpolaionTagSort = function()\n{//Whale Chen agrees to \"Wikipedia: Text of Creative Commons Attribution-ShareAlike 3.0 Unported License.\" Authorization Date: 2019/06/21//\n  var end = this.length; \n  if ( end > 1 ){ \n    var start = 0 ; \n    var Tag = new Array( end ); //Algorithm step-1 (Set an equal amount of Tag array to initialize to a false value)\n    for ( i = 0; i < end; i++ ){ Tag[ i ] = false; } \n    Divide( this ); } \n  while ( end > 1 ){ //Algorithm step-2  (The main sorting determines whether all sections of the original sequence have been recursively completed.)  \n    while ( Tag[ --start ] == false ){ } //When Tag[start] = true, start is the bucket sort start position (array header)\n    Divide( this ); } //Algorithm step-6 (Return the next section header position recursively to the main sorting [Algorithm step-2] processing)\n\n  function Divide( A ){   \n    var min = A[ start ]; \n    var max = A[ start ];\n    for( i = start + 1; i < end; i++ ){ if ( A[ i ] < min ){ min = A[ i ]; } else{ if ( A [ i ] > max ){ max = A[ i ]; } } }\n    if ( min == max ){ end = start; } //Algorithm step-3 (Stop bucket sorting if the array has been sorted)\n    else{  //Algorithm step-3 (Perform bucket sorting)                                         \n         var p = 0; \n         var size = end - start; \n         var Bucket = new Array( size ); \n         for ( i = 0; i < size; i++ ){ Bucket[ i ] = new Array( ); } //Algorithm step-4 (Set a two-dimensional array as an empty bucket)\n         for ( i = start; i < end; i++ ){//Algorithm step-4 (The search sequence puts the number into the bucket corresponding to the interpolation)  \n           p = Math.floor ( ( ( A[ i ] - min ) / ( max - min ) ) * ( size - 1 ) ); //(Interpolation computing. p is array pointer position)\n           Bucket[ p ].push( A[ i ] ); \n         } \n         for ( i = 0; i < size; i++ ){\n           if ( Bucket[ i ].length > 0){//Algorithm step-5 (Put the numbers back into the original array one by one from not empty buckets)     \n             Tag[ start ] = true; //Algorithm step-5 (Mark the start position of the section as true in the tag array)\n             for ( j = 0; j < Bucket[ i ].length; j++ ){ A[ start++ ] = Bucket[ i ][ j ]; } \n           } \n         }  \n    }\n  }\n};\n</source>\n\n===In-Place Interpolaion Tag Sort===\n{{Infobox Algorithm\n|name=In-Place Interpolaion Tag Sort\n|class=[[Sorting Algorithm]]\n|image=\n|data=[[Array]]\n|best-time=<math>O(n)</math>\n|time=<math>O(n)</math> \n|average-time=<math>O(n)</math>\n|space=<math>O(n)bits</math>\n|optimal= <math>O(n)</math>\n}} \n\nThe in-place interpolation tag sort is an [[in-place algorithm]] of interpolation sort. In-place Interpolation Tag Sort can achieve sorting by only N times of swapping by maintaining N bit tags; however, the array to be sorted must be a continuous integer sequence and not repeated, or the series is completely evenly distributed to approximate The number of [[arithmetical progression]].\n\nThe factor column data must not be repeated. For example, sorting 0~100 can be sorted in one step. The number of exchanges is: <math>O(n)</math>, the calculation time complexity is: <math>O(n)</math>, and the worst space complexity is <math>O(n)bits</math>. If the characteristics of the series meet the conditional requirements of this sorting method: \"The [[array]] is a continuous integer or an arithmetical progression that does not repeat\", the in-place interpolation tag sort will be an excellent sorting method that is extremely fast and saves memory space.\n\n====In-place Interpolation Tag Sort Algorithm====\nIn-place Interpolation Tag Sort sorts non-repeating consecutive integer series, only one Boolean data type tag array with the same length as the original array, the array calculates the interpolation of the data from the beginning, and the interpolation points to a new position of the array. Position, the position that has been swapped is marked as true in the corresponding position of the tag array, and is incremented until the end of the array is sorted.\n\nAlgorithm process:\n#Set an equal number of tag arrays to initialize to false values.\n#Visit the array when tag[ i ] is false, calculate the position corresponding to the interpolation=p.\n#Swap a[ i ] and a[ p ],let tag[ p ] = true.\n#The tour array is completed and the sorting is completed.\n\n====Practice====\nJavaScript code:\n<source lang=\"javascript\">\nArray.prototype.In-PlaceInterpolaionTagSort = function(a)\n{\n  var temp = 0;\n  var p = 0;\n  var n = a.length;\n  var tag = new Array( n );\n  for ( i = 0; i < n; i++ ){ tag[ i ] = false; }\n  var min = a[ 0 ];\n  var max = a[ 0 ];\n  for ( i = 1; i < n; i++ ){ if ( a[ i ] < min ){ min = a[ i ]; }else{ if (a[ i ] > max){ max = a[ i ]; } } } \n \n  for ( i = 0; i < n; i++ ){\n    while ( tag[ i ] == false ){ \n      p = Math.floor(( (a[ i ]-­min) / (max-min) ) * (n-1) );\n      temp = a[ i ];\n      a[ i ] = a[ p ]; \n      a[ p ] = temp;\n      tag[ p ] = true; \n    } \n  } \n};\n</source>\n\n====The origin of In-place sorting performed in <math>O(n)</math> time====\nIn \"Mathematical Analysis of Algorithms\", (Information Processing '71, North Holland Publ.'72) Donald Knuth remarked \"... that research on computional complexity is an interesting way to sharpen our tools for more routine problems we face from day to day.\" \n<ref name=\"neubert_code\">{{cite web |url=http://www.neubert.net/FSOIntro.html |title=The FlashSort Algorithm |accessdate=2007-11-06 |author=Karl-Dietrich Neubert |year=1998}}</ref>\n\nThe famous American computer scientist [[Donald Knuth]] in the mathematical analysis of algorithms pointed out that:\"With respect to the sorting problem, Knuth points out, that time effective in-situ permutation is inherently connected with the problem of finding the cycle leaders, and in-situ permutations could easily be performed in <math>O(n)</math> time if we would be allowed to manipulate n extra \"tag\" bits specifying how much of the permutation has been carried out at any time. Without such tag bits, he concludes \"it seems reasonable to conjecture that every algorithm will require for in-situ permutation at least <math>O(n log n)</math> steps on the average.\" \n<ref name=\"neubert_code\">{{cite web |url=http://www.neubert.net/FSOIntro.html |title=The FlashSort Algorithm |accessdate=2007-11-06 |author=Karl-Dietrich Neubert |year=1998}}</ref>\n\nThe In-place Interpolation Tag Sort is one of the sorting algorithms that the [[Donald  Knuth]] professor said: \"manipulate n extra \"tag\" bits...finding the cycle leaders, and in-situ permutations could easily be performed in <math>O(n)</math> time\".\n\n==Similar sorting method==\n#[[Flashsort]]\n#[[Proxmap sort]]\n#[[American flag sort]]\n\n===Bucket sort mixing other sorting methods and recursive algorithm===\nBucket sort can be mixed with other sorting methods to complete sorting. If it is sorted by bucket sort and insert sort, also is a fairly efficient sorting method. But when the series appears a large deviation from the value: For example, when the maximum value of the series is greater than N times the next largest value. After the series of columns are processed, the distribution is that all the elements except the maximum value fall into the same bucket. The second sorting method uses insert sort. May cause execution complexity to fall into <math>O(n^2)</math>. This has lost the meaning and high-speed performance of using bucket sort.\n\nInterpolation sort is a way of recursively using bucket sort. After performing recursion, still use bucket sort to disperse the series. This can avoid the above situation. If you want to make the recursive interpolation sort execution complexity fall into <math>O(n^2)</math>,It is necessary to present a [[factorial]] amplification in the entire series. In fact, there is very little chance that a series of special distributions will occur.\n\nIt is worth noting that: Bucket sort hybrid insert sort method, The average time complexity of the algorithm is <math>O(n+n^2/k+k)</math>.\n<ref>\n{{Cite web \n|url = https://en.wikipedia.org/wiki/Bucket_sort#Average-case_analysis \n|title = Bucket sort Average-case analysis \n|author = en.wikipedia \n|publisher = Wikipedia \n|language = en \n|quote = \nAssume insertion sort is used to sort each bucket, then the third step costs <math>O(\\textstyle \\sum_{i=1}^k \\displaystyle n_i^2)</math>, where <math>n_i</math> is the length of the bucket indexed <math>i</math>. Since we are concerning the average time, the expectation <math>E(n_i^2)</math> have to be evaluated instead. Let <math>X_{ij}</math> be the random variable of element <math>j</math> being placed in bucket <math>i</math>. We have <math>n_i = \\sum_{j=1}^n X_{ij}</math>. Therefore,\n\n:<math>\\begin{align}\nE(n_i^2) & = E\\left(\\sum_{j=1}^n X_{ij} \\sum_{k=1}^n X_{ik}\\right) \\\\\n& = E\\left(\\sum_{j=1}^n \\sum_{k=1}^n X_{ij}X_{ik}\\right) \\\\\n& = E\\left(\\sum_{j=1}^n X_{ij}^2\\right) + E\\left(\\sum_{1\\leq j,k\\leq n}\\sum_{j\\neq k}X_{ij}X_{ik}\\right)\n\\end{align} </math>\n\nThe last line separates the summation into the case <math>j=k</math> and the case <math>j\\neq k</math>. Since the chance of an object distributed to bucket <math>i</math> is <math>1/k</math>, <math>X_{ij} </math> is 1 with probability <math>1/k</math> and 0 otherwise. \n\n:<math>E(X_{ij}^2) = 1^2\\cdot \\left(\\frac{1}{k}\\right) + 0^2\\cdot \\left(1-\\frac{1}{k}\\right) = \\frac{1}{k}</math>\n:<math>E(X_{ij}X_{ik}) = 1\\cdot \\left(\\frac{1}{k}\\right)\\left(\\frac{1}{k}\\right) = \\frac{1}{k^2} </math>\n\nWith the summation, it would be\n\n:<math>E\\left(\\sum_{j=1}^n X_{ij}^2\\right) + E\\left(\\sum_{1\\leq j,k\\leq n}\\sum_{j\\neq k}X_{ij}X_{ik}\\right) = n\\cdot\\frac{1}{k} + n(n-1)\\cdot\\frac{1}{k^2} = \\frac{n^2+nk-n}{k^2}</math>\n\nFinally, the complexity would be <math>O\\left(\\sum_{i=1}^kE(n_i^2)\\right) = O\\left(\\sum_{i=1}^k \\frac{n^2+nk-n}{k^2}\\right) = O\\left(\\frac{n^2}{k}+n\\right)</math>.\n<br>\nThe last step of bucket sort, which is '''concatenating''' all the sorted objects in each buckets, requires <math>O(k)</math> time. Therefore, the total complexity is <math>O\\left(n+\\frac{n^2}{k}+k\\right)</math>.}}\n</ref>\nWhen <math>k=n</math>, Use <math>n</math> buckets just like interpolation sort. The average time complexity is <math> O(3n)</math>.\nHowever, interpolation sort is a recursive method of bucket sort. The average time complexity of the algorithm is the same as pure bucket sort is only <math>O(n+k)</math>.\n<ref>\n{{Cite web \n|url=https://zh.wikipedia.org/wiki/%E6%A1%B6%E6%8E%92%E5%BA%8F\n|title = 桶排序(Bucket sort) \n|author = zh.wikipedia \n|language = Chinese\n|quote = 平均時間複雜度(average time complexity)<math> O(n+k)</math>}}\n</ref>\nThis is still somewhat different in sorting execution efficiency.\n\n==Extra link==\n*[[Donald Knuth]]\n\n*[https://www.facebook.com/WhaleChen.1969/posts/2252485301532790 桶排序遞迴方式演算法 Bucket sort Recursive method. Whale Chen 2012/09/16]\n\n*[https://www.facebook.com/WhaleChen.1969/posts/2131925703588751 插值標簽排序演算法 Interpolation Tag Sort Algorithm. Whale Chen 2013/03/24]\n\n*[http://users.dcc.uchile.cl/~rbaeza/handbook/algs/4/416.sort.c.html interpolation sort (Pascal version available)]\n\n==Reference==\n{{reflist}}\n\n==Extra References==\n#[http://xlinux.nist.gov/dads/HTML/interpolationSort.html interpolationSort.html]\n#[http://xlinux.nist.gov/dads/HTML/histogramSort.html histogramSort.html]\n#[http://www.neubert.net/FSOIntro.html The FlashSort Algorithm]\n#[https://web.archive.org/web/20160304084930/http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=AD0726158 Mathematical Analysis of Algorithms]\n#http://www.drdobbs.com/database/the-flashsort1-algorithm/184410496\n\n{{Sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Stable sorts]]"
    },
    {
      "title": "K-way merge algorithm",
      "url": "https://en.wikipedia.org/wiki/K-way_merge_algorithm",
      "text": "{{DISPLAYTITLE:''k''-way merge algorithm}}\nIn [[computer science]], '''''k''-way merge algorithms''' or multiway merges are a specific type of [[Merge algorithm|sequence merge algorithms]] that specialize in taking in multiple sorted lists and merging them into a single sorted list. These merge algorithms generally refer to merge algorithms that take in a number of sorted lists greater than two. 2-way merges are also referred to as binary merges.\n\n== 2-way merge ==\nA 2-way merge, or a binary merge, has been studied extensively due to its key role in [[merge sort]]. An example of such is the classic merge that appears frequently in merge sort examples. The classic merge outputs the data item with the lowest key at each step; given some sorted lists, it produces a sorted list containing all the elements in any of the input lists, and it does so in time proportional to the sum of the lengths of the input lists.\n\nDenote by A[1..p] and B[1..q] two arrays sorted in increasing order.\nFurther, denote by C[1..n] the output array.\nThe canonical 2-Way merge algorithm<ref>{{cite book|author1=Thomas H. Cormen|authorlink1=Thomas H. Cormen|author2=Charles E. Leiserson|author3=Ronald L. Rivest|authorlink3=Ron Rivest|author4=Clifford Stein|title=Introduction To Algorithms|url=https://books.google.com/books?id=NLngYyWFl_YC&pg=PA11|year=2001|publisher=MIT Press|isbn=978-0-262-03293-3|pages=28–29}}</ref> stores indices i, j, and k into A, B, and C respectively.\nInitially, these indices refer to the first element, i.e., are 1.\nIf A[i] < B[j], then the algorithm copies A[i] into C[k] and increases i and k.  \nOtherwise, the algorithm copies B[j] into C[k] and increases j and k.\nA special case arises if either i or j have reached the end of A or B.\nIn this case the algorithm copies the remaining elements of B or A into C and terminates.\n\n== ''k''-way merge ==\nThe ''k''-way merge problem consists of merging k sorted arrays to produce a single sorted array with the same elements.\nDenote by n the total number of elements.\nn is equal to the size of the output array and the sum of the sizes of the k input arrays.\nFor simplicity, we assume that none of the input arrays is empty.\nAs a consequence k < n, which simplifies the reported running times.\nThe problem can be solved in O(n log k) running time with O(n) space.\nSeveral algorithms that achieve this running time exist.\n\n=== Iterative 2-Way merge ===\n\nThe problem can be solved by iteratively merging two of the k arrays using a 2-way merge until only a single array is left. If the arrays are merged in arbitrary order, then the resulting running time is only O(kn). This is suboptimal.\n\nThe running time can be improved by iteratively merging the first with the second, the third with the fourth, and so on. As the number of arrays is halved in each iteration, there are only Θ(log k) iteration. In each iteration every element is moved exactly once. The running time per iteration is therefore in Θ(n) as n is the number of elements. The total running time is therefore in Θ(n log k).\n\nWe can further improve upon this algorithm, by iteratively merging the two shortest arrays. It is clear that this minimizes the running time and can therefore not be worse than the strategy described in the previous paragraph. The running time is therefore in O(n log k). Fortunately, in border cases the running time can be better. Consider for example the degenerate case, where all but one array contain only one element. The strategy explained in the previous paragraph needs Θ(n log k) running time, while the improved one only needs Θ(n) running time.\n\n=== Direct ''k''-way merge ===\n\nIn this case,we would simultaneously merge k-runs together.\n\nA straightforward implementation would scan all k arrays to determine the minimum.\nThis straightforward implementation results in a running time of Θ(kn).\nNote that this is mentioned only as a possibility, for the sake of discussion. Although it would work, it is not efficient.\n\nWe can improve upon this by computing the smallest element faster.\nBy using either [[Heap (data structure)|heaps]], tournament trees, or [[Splay_tree|splay trees]], the smallest element can be determined in O(log k) time.\nThe resulting running times are therefore in O(n log k).\n\nThe heap is more commonly used, although a tournament tree is faster in practice. A heap uses approximately 2*log(k) comparisons in each step because it handles the tree from the root down to the bottom and needs to compare both children of each node. Meanwhile, a tournament tree only needs log(k) comparisons because it starts on the bottom of the tree and works up to the root, only making a single comparison in each layer. The tournament tree should therefore be the preferred implementation.\n\n==== Heap ====\n\nThe heap algorithm \n<ref>{{cite book\n|last1=Bentley\n|first1=Jon Louis\n|authorlink=Jon_Bentley_(computer_scientist)\n|title=Programming Pearls\n|date=2000\n|publisher=Addison Wesley\n|isbn=0201657880\n|pages=147-162\n|edition=2nd}}</ref>\nallocates a min-heap of pointers into the input arrays.\nInitially these pointers point to the smallest elements of the input array.\nThe pointers are sorted by the value that they point to.\nIn an O(k) preprocessing step the heap is created using the standard heapify procedure.\nAfterwards, the algorithm iteratively transfers the element that the root pointer points to, increases this pointer and executes the standard decrease key procedure upon the root element.\nThe running time of the increase key procedure is bounded by O(log k).\nAs there are n elements, the total running time is O(n log k).\n\nNote that the operation of replacing the key and iteratively doing decrease-key or sift-down are not supported by many Priority Queue libraries such as C++ stl and Java. Doing an extract-min and insert function is less efficient.\n\n==== Tournament Tree ====\n\n[[File:Tournament tree.png|thumb|Tournament tree]]\nThe Tournament Tree <ref name=\"knuth98\">\n{{cite book| last = Knuth| first = Donald| authorlink = Donald Knuth| series = [[The Art of Computer Programming]]| volume= 3| title= Sorting and Searching| edition = 2nd| publisher = Addison-Wesley| year=  1998| chapter = Chapter 5.4.1. Multiway Merging and Replacement Selection| pages = 252-255| isbn = 0-201-89685-0| ref = harv}}</ref> is based on an elimination tournament, like it can be found in sports. In each game, two of the input elements compete. The winner is promoted to the next round. Therefore, we get a [[binary tree]] of games. The list is sorted in ascending order, so the winner of a game is the smaller one of both elements.\n\n[[File:Loser tree.png|thumb|Loser tree]]\n\nFor k-way merging, it is more efficient to only store the loser of each game (see image). The data structure is therefore called a loser tree. When building the tree or replacing an element with the next one from its list, we still promote the winner of the game to the top. The tree is filled like in a sports match but the nodes only store the loser. Usually, an additional node above the root is added that represents the overall winner. Every leaf stores a pointer to one of the input arrays. Every inner node stores a value and an index. The index of an inner node indicates which input array the value comes from. The value contains a copy of the first element of the corresponding input array.\n\nThe algorithm iteratively appends the minimum element to the result and then removes the element from the corresponding input list. It updates the nodes on the path from the updated leaf to the root (''replacement selection''). The removed element is the overall winner. Therefore, it has won each game on the path from the input array to the root. When selecting a new element from the input array, the element needs to compete against the previous losers on the path to the root. When using a loser tree, the partner for replaying the games is already stored in the nodes. The loser of each replayed game is written to the node and the winner is iteratively promoted to the top. When the root is reached, the new overall winner was found and can be used in the next round of merging.\n\nThe images of the tournament tree and the loser tree in this section use the same data and can be compared to understand the way a loser tree works.\n\n===== Algorithm =====\n\nA tournament tree can be represented as a balanced binary tree by adding sentinels to the input lists and by adding lists until the number of lists is a power of two. The balanced tree can be stored in a single array. The parent element can be reached by dividing the current index by two.\n\nWhen one of the leaves is updated, all games from the leaf to the root are replayed. In the following [[pseudocode]], an object oriented tree is used instead of an array because it is easier to understand. Additionally, the number of lists to merge is assumed to be a power of two.\n\n '''function''' merge(L1, …, Ln)\n   buildTree(heads of L1, …, Ln)\n   '''while''' tree has elements\n     winner := tree.winner\n     output winner.value\n     new := winner.index.next\n     replayGames(winner, new) // Replacement selection\n\n '''function''' replayGames(node, new)\n   loser, winner := playGame(node, new)\n   node.value := loser.value\n   node.index := loser.index\n   '''if''' node != root\n     replayGames(node.parent, winner)\n\n '''function''' buildTree(elements)\n   nextLayer := new Array()\n   '''while''' elements not empty\n     el1 := elements.take()\n     el2 := elements.take()\n     loser, winner := playGame(el1, el2)\n     parent := new Node(el1, el2, loser)\n     nextLayer.add(parent)\n   '''if''' nextLayer.size == 1\n     '''return''' nextLayer // only root\n   '''else'''\n     '''return''' buildTree(nextLayer)\n\n===== Running time =====\n\nIn the beginning, the tree is first created in time Θ(k). In each step of merging, only the games on the path from the new element to the root need to be replayed. In each layer, only one comparison is needed. As the tree is balanced, the path from one of the input arrays to the root contains only Θ(log k) elements. In total, there are n elements that need to be transferred. The resulting total running time is therefore in Θ(n log k). <ref name=\"knuth98\" />\n     \n===== Example =====\n\nThe following section contains a detailed example for the replacement selection step and one example for a complete merge containing multiple replacement selections.\n\n====== Replacement selection ======\n\nGames are replayed from the bottom to the top. In each layer of the tree, the currently stored element of the node and the element that was provided from the layer below compete. The winner is promoted to the top until we found the new overall winner. The loser is stored in the node of the tree.\n\n[[File:Loser tree replacement selection.gif|centre|thumb|Example for replacement selection]]\n\n{| class=\"wikitable\"\n|-\n! Step !! Action\n|-\n| 1 || Leaf 1 (overall winner) is replaced by 9, the next element from the input list.\n|-\n| 2 || Replaying the game 9 vs 7 (previous loser). 7 wins because it is smaller. Therefore, 7 is promoted to the top while 9 is saved in the node.\n|-\n| 3 || Replaying the game 7 vs 3 (previous loser). 3 wins because it is smaller. Therefore, 3 is promoted to the top while 7 is saved in the node.\n|-\n| 4 || Replaying the game 3 vs 2 (previous loser). 2 wins because it is smaller. Therefore, 2 is promoted to the top while 3 is saved in the node.\n|-\n| 5 || The new overall winner 2 is saved above the root.\n|}\n\n====== Merge ======\nTo execute the merge itself, the overall smallest element is repeatedly replaced with the next input element. After that, the games to the top are replayed.\n\nThis example uses four sorted arrays as input.\n\n {2, 7, 16}\n {5, 10, 20}\n {3, 6, 21}\n {4, 8, 9}\n\nThe algorithm is initiated with the heads of each input list. Using these elements, a binary tree of losers is built. For merging, the lowest list element 2 is determined by looking at the overall minimum element at the top of the tree. That value is then popped off, and its leaf is refilled with 7, the next value in the input list. The games on the way to the top are replayed like in the previous section about replacement selection. The next element that is removed is 3. Starting from the next value in the list, 6, the games are replayed up until the root. This is being repeated until the minimum of the tree equals infinity.\n\n[[File:Loser tree merge.gif|centre|thumb|Visualization for the whole algorithm]]\n\n=== Lower Running Time Bound ===\n\nOne can show that no comparison-based ''k''-way merge algorithm exists with a running time in O(n f(k)) where f grows asymptotically slower than a logarithm.\n(Excluding data with desirable distributions such as disjoint ranges.)\nThe proof is a straightforward reduction from comparison-based sorting.\nSuppose that such an algorithm existed, then we could construct a comparison-based sorting algorithm with running time O(n f(n)) as follows:\nChop the input array into n arrays of size 1. \nMerge these n arrays with the ''k''-way merge algorithm.\nThe resulting array is sorted and the algorithm has a running time in O(n f(n)).\nThis is a contradiction to the well-known result that no comparison-based sorting algorithm with a worst case running time below O(n log n) exists.\n\n== External sorting ==\n{{See also|External sorting}}\n''k''-way merges are used in external sorting procedures.<ref>{{Cite book|title = Data Structures and Algorithm Analysis in C++, Third Edition|url = https://books.google.com/books?id=AijEAgAAQBAJ|publisher = Courier Corporation|date = 2012-07-26|isbn = 9780486172620|first = Clifford A.|last = Shaffer}}</ref> [[External sorting|External sorting algorithms]] are a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory (usually a hard drive). ''k''-way merge algorithms usually take place in the second stage of external sorting algorithms, much like they do for merge sort.\n\nA multiway merge allows for the files outside of memory to be merged in fewer passes than in a binary merge. If there are 6 runs that need be merged then a binary merge would need to take 3 merge passes, as opposed to a 6-way merges single merge pass. This reduction of merge passes is especially important considering the large amount of information that is usually being sorted in the first place, allowing for greater speed-ups while also reducing the amount of accesses to slower memory.\n\n== References ==\n{{Reflist}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Library sort",
      "url": "https://en.wikipedia.org/wiki/Library_sort",
      "text": "{{refimprove|date=October 2017}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|data=[[Array data structure|Array]]\n|time=<math>O(n^2)</math>\n|best-time=<math>O(n)</math>\n|average-time=<math>O(n\\log n)</math>\n|space=<math>O(n)</math>\n|optimal=?\n}}\n'''Library sort''', or '''gapped insertion sort''' is a [[sorting algorithm]] that uses an [[insertion sort]], but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:\n<blockquote>Suppose a librarian were to store his books alphabetically on a long shelf, starting with the As at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Zs. If the librarian acquired a new book that belongs to the B section, once he finds the correct space in the B section, he will have to move every book over, from the middle of the Bs all the way down to the Zs in order to make room for the new book. This is an insertion sort. However, if he were to leave a space after every letter, as long as there was still space after B, he would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.</blockquote>\n\nThe algorithm was proposed by [[Michael A. Bender]], [[Martín Farach-Colton]], and [[Miguel Mosteiro]] in 2004<ref>{{cite arxiv |arxiv=cs/0407003 |title=Insertion Sort is O(n log n) |date=1 July 2004 |last1=Bender |first1=Michael A. |last2=Farach-Colton |first2=Martín |authorlink2=Martin Farach-Colton |last3=Mosteiro |first3=Miguel A.}}</ref> and was published in 2006.<ref name=\"definition\">{{cite journal | journal=Theory of Computing Systems | volume=39 | issue=3 | pages=391–397 | date=June 2006 | last1=Bender | first1=Michael A. | last2=Farach-Colton | first2=Martín | authorlink2 = Martin Farach-Colton | last3=Mosteiro  | first3=Miguel A. | title=Insertion Sort is O(n log n) | doi=10.1007/s00224-005-1237-z | url=http://csis.pace.edu/~mmosteiro/pub/paperToCS06.pdf | arxiv=cs/0407003 }}</ref>\n\nLike the insertion sort it is based on, library sort is a [[stable sort|stable]] [[comparison sort]] and can be run as an [[online algorithm]]; however, it was shown to have a high probability of running in O(n log n) time (comparable to [[quicksort]]), rather than an insertion sort's O(n<sup>2</sup>). The mechanism used for this improvement is very similar to that of a [[skip list]]. There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.\n\nCompared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is ''(1 + ε)n'',<ref name=\"definition\" /> but with no further recommendations on how to choose ε.\n\nOne weakness of [[insertion sort]] is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to [[mergesort]] as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.\n\n==Implementation==\n\n===Algorithm ===\nLet us say we have an array of n elements. We choose the gap we intend to\ngive. Then we would have a final array of size (1 + ε)n. The algorithm works\nin log n rounds. In each round we insert as many elements as there are in\nthe final array already, before re-balancing the array. For finding the position\nof inserting, we apply Binary Search in the final array and then swap the\nfollowing elements till we hit an empty space. Once the round is over, we\nre-balance the final array by inserting spaces between each element.\n\nFollowing are three important steps of the algorithm:\n\n1. Binary Search:\nFinding the position of insertion by applying binary search within the\nalready inserted elements. This can be done by linearly moving towards\nleft or right side of the array if you hit an empty space in the middle\nelement.\n\n2. Insertion:\nInserting the element in the position found and swapping the following\nelements by 1 position till an empty space is hit.\n\n3. Re-Balancing:\nInserting spaces between each pair of elements in the array. This takes linear time, and\nbecause there are log n rounds in the algorithm, total re-balancing takes\nO(n log n) time only.\n\n===Pseudocode===\n\n '''proc''' rebalance(A, begin, end)\n     r ← end\n     w ← end * 2\n     '''while''' r >= begin\n         A[w+1] ← gap\n         A[w] ← A[r]\n         r ← r - 1\n         w ← w - 2\n\n '''proc''' sort(A)\n     n ← length(A)\n     S ← new array of n gaps\n     '''for''' i ← 1 to floor(log2(n) + 1)\n         '''for''' j ← 2^i to 2^(i+1)\n             ins ← binarysearch(A[j], S, 2^(i-1))\n             insert A[j] at S[ins]\n\nHere, <code>binarysearch(el, A, k)</code> performs [[binary search]] in the first {{mvar|k}} elements of {{mvar|A}}, skipping over gaps, to find a place where to locate element {{mvar|el}}. Insertion should favor gaps over filled-in elements.\n\n== References ==\n{{reflist}}\n\n==External links==\n*[http://www.cs.sunysb.edu/~bender/newpub/BenderFaMo06-librarysort.pdf Gapped Insertion Sort]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n[[Category:Online sorts]]"
    },
    {
      "title": "Median cut",
      "url": "https://en.wikipedia.org/wiki/Median_cut",
      "text": "'''Median cut''' is an [[sorting algorithm|algorithm to sort data]] of an arbitrary number of dimensions into series of sets by [[recursion|recursively]] cutting each set of data at the [[median]] point along the longest dimension. Median cut is typically used for [[color quantization]].  For example, to reduce a 64k-colour image to 256 colours, median cut is used to find 256 colours that match the original data well. <ref>{{cite web|url=http://web.cs.wpi.edu/~matt/courses/cs563/talks/color_quant/CQindex.html|title=An Overview of Color Quantization Techniques|author=Steven Segenchuk|date=5 May 1997|accessdate=24 April 2014|page=4}}</ref>\n\n== Implementation of color quantization ==\nSuppose we have an image with an arbitrary number of [[pixel]]s and want to generate a [[palette (computing)|palette]] of 16 colors. Put all the pixels of the image (that is, their [[RGB color model|RGB values]]) in a [[bucket sort|bucket]]. Find out which color channel (red, green, or blue) among the pixels in the bucket has the greatest range, then sort the pixels according to that channel's values. For example, if the blue channel has the greatest range, then a pixel with an RGB value of (32, 8, 16) is less than a pixel with an RGB value of (1, 2, 24), because 16 < 24. After the bucket has been sorted, move the upper half of the pixels into a new bucket. (It is this step that gives the median cut algorithm its name; the buckets are divided into two at the [[median]] of the list of pixels.) Repeat the process on both buckets, giving you 4 buckets, then repeat on all 4 buckets, giving you 8 buckets, then repeat on all 8, giving you 16 buckets. Average the pixels in each bucket and you have a palette of 16 colors.\n\nSince the number of buckets doubles with each iteration, this algorithm can only generate a palette with a number of colors that is a [[power of two]]. To generate, say, a 12-color palette, one might first generate a 16-color palette and merge some of the colors in some way.\n\n== See also ==\n* [[k-d tree]]\n\n== References ==\n\n{{reflist}}\n\n== External links ==\n*[http://micro.magnet.fsu.edu/primer/java/digitalimaging/processing/colorreduction/index.html Image quantization]\n*[http://www.leptonica.com/papers/mediancut.pdf Median cut + variations]\n*[https://metacpan.org/pod/Image::Pngslimmer Image::Pngslimmer Perl module at CPAN]\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Merge algorithm",
      "url": "https://en.wikipedia.org/wiki/Merge_algorithm",
      "text": "{{Short description|Algorithm that combines multiple sorted lists into one}}\n'''Merge algorithms''' are a family of [[algorithm]]s that take multiple [[sorting algorithm|sorted]] lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as [[subroutine]]s in various [[sorting algorithm]]s, most famously [[merge sort]].\n\n== Application ==\n[[File:Merge sort algorithm diagram.svg|thumb|upright=1.5|An example for merge sort]]\nThe merge algorithm plays a critical role in the [[merge sort]] algorithm, a [[comparison sort|comparison-based sorting algorithm]]. Conceptually, merge sort algorithm consists of two steps:\n\n# [[Recursion (computer science)|Recursively]] divide the list into sublists of (roughly) equal length, until each sublist contains only one element, or in the case of iterative (bottom up) merge sort, consider a list of ''n'' elements as ''n'' sub-lists of size 1. A list containing a single element is, by definition, sorted.\n# Repeatedly merge sublists to create a new sorted sublist until the single list contains all elements. The single list is the sorted list.\n\nThe merge algorithm is used repeatedly in the merge sort algorithm.\n\nAn example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left.\n\n== Merging two lists ==\n\nMerging two sorted lists into one can be done in [[linear time]] and linear space. The following [[pseudocode]] demonstrates an algorithm that merges input lists (either [[linked list]]s or [[Array data structure|arrays]]) {{mvar|A}} and {{mvar|B}} into a new list {{mvar|C}}.<ref name=\"skiena\">{{cite book |last=Skiena |first=Steven |authorlink=Steven Skiena |title=The Algorithm Design Manual |publisher=[[Springer Science+Business Media]] |edition=2nd |year=2010 |isbn=1-849-96720-2 |page=123}}</ref>{{r|toolbox}}{{rp|104}} The function {{mono|head}} yields the first element of a list; \"dropping\" an element means removing it from its list, typically by incrementing a pointer or index.\n\n '''algorithm''' merge(A, B) '''is'''\n     '''inputs''' A, B : list\n     '''returns''' list\n \n     C := new empty list\n     '''while''' A is not empty and B is not empty '''do'''\n         '''if''' head(A) ≤ head(B) '''then'''\n             append head(A) to C\n             drop the head of A\n         '''else'''\n             append head(B) to C\n             drop the head of B\n \n     ''// By now, either A or B is empty. It remains to empty the other input list.''\n     '''while''' A is not empty '''do'''\n         append head(A) to C\n         drop the head of A\n     '''while''' B is not empty '''do'''\n         append head(B) to C\n         drop the head of B\n \n     '''return''' C\n\nWhen the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list.\n\nIn the merge sort algorithm, this [[subroutine]] is typically used to merge two sub-arrays {{mono|A[lo..mid]}}, {{mono|A[mid..hi]}} of a single array {{mono|A}}. This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above.{{r|skiena}} The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised,<ref>{{cite journal |last1=Katajainen |first1=Jyrki |first2=Tomi |last2=Pasanen |first3=Jukka |last3=Teuhola |title=Practical in-place mergesort |journal=Nordic J. Computing |volume=3 |issue=1 |year=1996 |pages=27–40 |citeseerx=10.1.1.22.8523}}</ref> sometimes sacrificing the linear-time bound to produce an {{math|''O''(''n'' log ''n'')}} algorithm;<ref>{{Cite conference| doi = 10.1007/978-3-540-30140-0_63| title = Stable Minimum Storage Merging by Symmetric Comparisons| conference = European Symp. Algorithms| volume = 3221| pages = 714–723| series = Lecture Notes in Computer Science| year = 2004| last1 = Kim | first1 = Pok-Son| last2 = Kutzner | first2 = Arne| isbn = 978-3-540-23025-0| citeseerx=10.1.1.102.4612}}</ref> see {{slink|Merge sort|Variants}} for discussion.\n\n==K-way merging==\n{{Main|K-way merge algorithm}}\n{{mvar|k}}-way merging generalizes binary merging to an arbitrary number {{mvar|k}} of sorted input lists. Applications of {{mvar|k}}-way merging arise in various sorting algorithms, including [[patience sorting]]<ref name=\"Chandramouli\">{{Cite conference |last1=Chandramouli |first1=Badrish |last2=Goldstein |first2=Jonathan |title=Patience is a Virtue: Revisiting Merge and Sort on Modern Processors |conference=SIGMOD/PODS |year=2014}}</ref> and an [[external sorting]] algorithm that divides its input into {{math|''k'' {{=}} {{sfrac|1|''M''}} − 1}} blocks that fit in memory, sorts these one by one, then merges these blocks.{{r|toolbox}}{{rp|119–120}}\n\nSeveral solutions to this problem exist. A naive solution is to do a loop over the {{mvar|k}} lists to pick off the minimum element each time, and repeat this loop until all lists are empty:\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Input: a list of {{mvar|k}} lists.\n* While any of the lists is non-empty:\n** Loop over the lists to find the one with the minimum first element.\n** Output the minimum element and remove it from its list.\n{{frame-footer}}\n</div>\n\n[[Best, worst and average case|In the worst case]], this algorithm performs {{math|(''k''−1)(''n''−{{sfrac|''k''|2}})}} element comparisons to perform its work if there are a total of {{mvar|n}} elements in the lists.<ref name=\"greene\">{{cite conference |last=Greene |first=William A. |year=1993 |title=k-way Merging and k-ary Sorts |conference=Proc. 31-st Annual ACM Southeast Conf |pages=127–135 |url=http://www.cs.uno.edu/people/faculty/bill/k-way-merge-n-sort-ACM-SE-Regl-1993.pdf}}</ref>\nIt can be improved by storing the lists in a [[priority queue]] ([[heap (data structure)|min-heap]]) keyed by their first element:\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Build a min-heap {{mvar|h}} of the {{mvar|k}} lists, using the first element as the key.\n* While any of the lists is non-empty:\n** Let {{math|''i'' {{=}} find-min(''h'')}}.\n** Output the first element of list {{mvar|i}} and remove it from its list.\n** Re-heapify {{mvar|h}}.\n{{frame-footer}}\n</div>\n\nSearching for the next smallest element to be output (find-min) and restoring heap order can now be done in {{math|''O''(log ''k'')}} time (more specifically, {{math|2⌊log ''k''⌋}} comparisons{{r|greene}}), and the full problem can be solved in {{math|''O''(''n'' log ''k'')}} time (approximately {{math|2''n''⌊log ''k''⌋}} comparisons).{{r|greene}}<ref name=\"toolbox\">{{cite book|author1=[[Kurt Mehlhorn]]|author2=[[Peter Sanders (computer scientist)|Peter Sanders]]|title=Algorithms and Data Structures: The Basic Toolbox |date=2008 |publisher=Springer |isbn=978-3-540-77978-0 |url=http://people.mpi-inf.mpg.de/~mehlhorn/ftp/Toolbox/}}</ref>{{rp|119–120}}\n\nA third algorithm for the problem is a [[divide and conquer algorithm|divide and conquer]] solution that builds on the binary merge algorithm:\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* If {{math|''k'' {{=}} 1}}, output the single input list.\n* If {{math|''k'' {{=}} 2}}, perform a binary merge.\n* Else, recursively merge the first {{math|⌊''k''/2⌋}} lists and the final {{math|⌈''k''/2⌉}} lists, then binary merge these.\n{{frame-footer}}\n</div>\n\nWhen the input lists to this algorithm are ordered by length, shortest first, it requires fewer than {{math|''n''⌈log ''k''⌉}} comparisons, i.e., less than half the number used by the heap-based algorithm; in practice, it may be about as fast or slow as the heap-based algorithm.{{r|greene}}\n\n== Parallel merge ==\nA [[task parallelism|parallel]] version of the binary merge algorithm can serve as a building block of a [[Merge sort#Parallel merge sort|parallel merge sort]]. The following pseudocode demonstrates this algorithm in a [[fork-join model|parallel divide-and-conquer]] style (adapted from Cormen ''et al.''<ref name=\"clrs\">{{Introduction to Algorithms|3}}</ref>{{rp|800}}). It operates on two sorted arrays {{mvar|A}} and {{mvar|B}} and writes the sorted output to array {{mvar|C}}. The notation {{mono|A[i...j]}} denotes the part of {{mvar|A}} from index {{mvar|i}} through {{mvar|j}}, exclusive.\n\n '''algorithm''' merge(A[i...j], B[k...ℓ], C[p...q]) '''is'''\n     '''inputs''' A, B, C : array\n            i, j, k, ℓ, p, q : indices\n \n     '''let''' m = j - i,\n         n = ℓ - k\n \n     '''if''' m < n '''then'''\n         swap A and B  ''// ensure that A is the larger array: i, j still belong to A; k, ℓ to B''\n         swap m and n\n \n     '''if''' m ≤ 0 '''then'''\n         '''return'''  ''// base case, nothing to merge''\n \n     '''let''' r = ⌊(i + j)/2⌋\n     '''let''' s = binary-search(A[r], B[k...ℓ])\n     '''let''' t = p + (r - i) + (s - k)\n     C[t] = A[r]\n \n     '''in parallel do'''\n         merge(A[i...r], B[k...s], C[p...t])\n         merge(A[r+1...j], B[s...ℓ], C[t+1...q])\n\nThe algorithm operates by splitting either {{mvar|A}} or {{mvar|B}}, whichever is larger, into (nearly) equal halves. It then splits the other array into a part with values smaller than the midpoint of the first, and a part with larger or equal values. (The [[binary search]] subroutine returns the index in {{mvar|B}} where {{math|''A''[''r'']}} would be, if it were in {{mvar|B}}; that this always a number between {{mvar|k}} and {{mvar|ℓ}}.) Finally, each pair of halves is merged [[Divide and conquer algorithm|recursively]], and since the recursive calls are independent of each other, they can be done in parallel. Hybrid approach, where serial algorithm is used for recursion base case has been shown to perform well in practice <ref name=\"vjd\">{{cite| author=Victor J. Duvanenko| title=Parallel Merge| journal=Dr. Dobb's Journal| date=2011| url=http://www.drdobbs.com/parallel/parallel-merge/229204454}}</ref>\n\nThe [[Analysis of parallel algorithms#Overview|work]] performed by the algorithm for two arrays holding a total of {{mvar|n}} elements, i.e., the running time of a serial version of it, is {{math|''O''(''n'')}}. This is optimal since {{mvar|n}} elements need to be copied into {{mvar|C}}. Its critical path length, however, is {{math|Θ(log<sup>2</sup> ''n'')}}, meaning that it takes that much time on an ideal machine with an unbounded number of processors.{{r|clrs}}{{rp|801–802}}\n\n'''Note:''' The routine is not [[Sorting algorithm#Stability|stable]]: if equal items are separated by splitting {{mvar|A}} and {{mvar|B}}, they will become interleaved in {{mvar|C}}; also swapping {{mvar|A}} and {{mvar|B}} will destroy the order, if equal items are spread among both input arrays. As a result, when used for sorting, this algorithm produces a sort that is not stable.\n\n== Language support ==\n\nSome [[computer language]]s provide built-in or library support for merging sorted [[Collection (abstract data type)|collections]].\n\n=== C++ ===\nThe  [[C++]]'s [[Standard Template Library]] has the function {{mono|std::merge}}, which merges two sorted ranges of [[iterator]]s, and {{mono|std::inplace_merge}}, which merges two consecutive sorted ranges ''in-place''. In addition, the {{mono|std::list}} (linked list) class has its own {{mono|merge}} method which merges another list into itself. The type of the elements merged must support the less-than ({{mono|<}}) operator, or it must be provided with a custom comparator.\n\nC++17 allows for differing execution policies, namely sequential, parallel, and parallel-unsequenced.<ref>{{cite web| url=http://en.cppreference.com/w/cpp/algorithm/merge| title=std:merge| publisher=cppreference.com| date=2018-01-08| access-date=2018-04-28}}</ref>\n\n=== Python ===\n[[Python (programming language)|Python]]'s standard library (since 2.6) also has a {{mono|merge}} function in the {{mono|heapq}} module, that takes multiple sorted iterables, and merges them into a single iterator.<ref>https://docs.python.org/library/heapq.html#heapq.merge</ref>\n\n== See also ==\n* [[Merge (revision control)]]\n* [[Join (relational algebra)]]\n* [[Join (SQL)]]\n* [[Join (Unix)]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* [[Donald Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. {{ISBN|0-201-89685-0}}. Pages 158–160 of section 5.2.4: Sorting by Merging. Section 5.3.2: Minimum-Comparison Merging, pp.&nbsp;197–207.\n\n==External links==\n*[https://duvanenko.tech.blog/2018/05/23/faster-sorting-in-c/ High Performance Implementation] of Parallel and Serial Merge in [[C Sharp (programming language)|C#]] with source in [https://github.com/DragonSpit/HPCsharp/ GitHub] and in [[C++]] [https://github.com/DragonSpit/ParallelAlgorithms GitHub]\n\n{{sorting}}\n\n{{DEFAULTSORT:Merge Algorithm}}\n[[Category:Articles with example pseudocode]]\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Merge sort",
      "url": "https://en.wikipedia.org/wiki/Merge_sort",
      "text": "{{Short description|A divide and combine sorting algorithm}}\n{{Original research|date=May 2016}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=Merge-sort-example-300px.gif\n|caption=An example of merge sort. First divide the list into the smallest unit (1 element), then compare each element with the adjacent list to sort and merge the two adjacent lists. Finally  all the elements are sorted and merged.\n|data=[[Array data structure|Array]]\n|time=O(''n'' log ''n'')\n|best-time=O(''n'' log ''n'') typical,\nO(''n'') natural variant\n|average-time=O(''n'' log ''n'')\n|space=О(''n'') total with O(''n'') auxiliary, O(''1'') auxiliary with linked lists<ref>{{Harvtxt|Skiena|2008|p=122}}</ref>\n|optimal=Yes\n}}\nIn [[computer science]], '''merge sort''' (also commonly spelled '''mergesort''') is an efficient, general-purpose, [[comparison sort|comparison-based]] [[sorting algorithm]]. Most implementations produce a [[Sorting algorithm#Stability|stable sort]], which means that the order of equal elements is the same in the input and output. Merge sort is a [[divide and conquer algorithm]] that was invented by [[John von Neumann]] in 1945.<ref>{{Harvtxt|Knuth|1998|p=158}}</ref> A detailed description and analysis of bottom-up mergesort appeared in a report by [[Herman Goldstine|Goldstine]] and [[John von Neumann|von Neumann]] as early as 1948.<ref>{{cite conference |title=A meticulous analysis of mergesort programs |date=March 1997 |first1=Jyrki |last1=Katajainen |first2=Jesper Larsson |last2=Träff |conference=Italian Conference on Algorithms and Complexity |location=Rome |book-title=Proceedings of the 3rd Italian Conference on Algorithms and Complexity |pages=217–228 |doi=10.1007/3-540-62592-5_74 |citeseerx=10.1.1.86.3154  |url=http://hjemmesider.diku.dk/~jyrki/Paper/CIAC97.pdf |ref=harv}}</ref>\n\n==Algorithm==\nConceptually, a merge sort works as follows:\n#Divide the unsorted list into ''n'' sublists, each containing one element (a list of one element is considered sorted).\n#Repeatedly [[Merge algorithm|merge]] sublists to produce new sorted sublists until there is only one sublist remaining.  This will be the sorted list.\n\n===Top-down implementation ===\nExample [[C-like]] code using indices for top-down merge sort algorithm that recursively splits the list (called ''runs'' in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alternating the direction of the merge with each level of recursion.\n\n<source lang=\"C\">\n// Array A[] has the items to sort; array B[] is a work array.\nvoid TopDownMergeSort(A[], B[], n)\n{\n    CopyArray(A, 0, n, B);           // duplicate array A[] into B[]\n    TopDownSplitMerge(B, 0, n, A);   // sort data from B[] into A[]\n}\n\n// Sort the given run of array A[] using array B[] as a source.\n// iBegin is inclusive; iEnd is exclusive (A[iEnd] is not in the set).\nvoid TopDownSplitMerge(B[], iBegin, iEnd, A[])\n{\n    if(iEnd - iBegin < 2)                       // if run size == 1\n        return;                                 //   consider it sorted\n    // split the run longer than 1 item into halves\n    iMiddle = (iEnd + iBegin) / 2;              // iMiddle = mid point\n    // recursively sort both runs from array A[] into B[]\n    TopDownSplitMerge(A, iBegin,  iMiddle, B);  // sort the left  run\n    TopDownSplitMerge(A, iMiddle,    iEnd, B);  // sort the right run\n    // merge the resulting runs from array B[] into A[]\n    TopDownMerge(B, iBegin, iMiddle, iEnd, A);\n}\n\n//  Left source half is A[ iBegin:iMiddle-1].\n// Right source half is A[iMiddle:iEnd-1   ].\n// Result is            B[ iBegin:iEnd-1   ].\nvoid TopDownMerge(A[], iBegin, iMiddle, iEnd, B[])\n{\n    i = iBegin, j = iMiddle;\n \n    // While there are elements in the left or right runs...\n    for (k = iBegin; k < iEnd; k++) {\n        // If left run head exists and is <= existing right run head.\n        if (i < iMiddle && (j >= iEnd || A[i] <= A[j])) {\n            B[k] = A[i];\n            i = i + 1;\n        } else {\n            B[k] = A[j];\n            j = j + 1;\n        }\n    }\n}\n\nvoid CopyArray(A[], iBegin, iEnd, B[])\n{\n    for(k = iBegin; k < iEnd; k++)\n        B[k] = A[k];\n}\n</source>\n\n===Bottom-up implementation===\n\nExample C-like code using indices for bottom-up merge sort algorithm which treats the list as an array of ''n'' sublists (called ''runs'' in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:\n\n<source lang=\"C\">\n// array A[] has the items to sort; array B[] is a work array\nvoid BottomUpMergeSort(A[], B[], n)\n{\n    // Each 1-element run in A is already \"sorted\".\n    // Make successively longer sorted runs of length 2, 4, 8, 16... until whole array is sorted.\n    for (width = 1; width < n; width = 2 * width)\n    {\n        // Array A is full of runs of length width.\n        for (i = 0; i < n; i = i + 2 * width)\n        {\n            // Merge two runs: A[i:i+width-1] and A[i+width:i+2*width-1] to B[]\n            // or copy A[i:n-1] to B[] ( if(i+width >= n) )\n            BottomUpMerge(A, i, min(i+width, n), min(i+2*width, n), B);\n        }\n        // Now work array B is full of runs of length 2*width.\n        // Copy array B to array A for next iteration.\n        // A more efficient implementation would swap the roles of A and B.\n        CopyArray(B, A, n);\n        // Now array A is full of runs of length 2*width.\n    }\n}\n\n//  Left run is A[iLeft :iRight-1].\n// Right run is A[iRight:iEnd-1  ].\nvoid BottomUpMerge(A[], iLeft, iRight, iEnd, B[])\n{\n    i = iLeft, j = iRight;\n    // While there are elements in the left or right runs...\n    for (k = iLeft; k < iEnd; k++) {\n        // If left run head exists and is <= existing right run head.\n        if (i < iRight && (j >= iEnd || A[i] <= A[j])) {\n            B[k] = A[i];\n            i = i + 1;\n        } else {\n            B[k] = A[j];\n            j = j + 1;    \n        }\n    } \n}\n\nvoid CopyArray(B[], A[], n)\n{\n    for(i = 0; i < n; i++)\n        A[i] = B[i];\n}\n</source>\n\n===Top-down implementation using lists ===\n[[Pseudocode]] for top-down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.\n\n '''function''' merge_sort(''list'' m)\n     // ''Base case. A list of zero or one elements is sorted, by definition.''\n     '''if''' length of m ≤ 1 '''then'''\n         '''return''' m\n \n     // ''Recursive case. First, divide the list into equal-sized sublists''\n     // ''consisting of the first half and second half of the list.''\n     // ''This assumes lists start at index 0.''\n     '''var''' left := empty list\n     '''var''' right := empty list\n     '''for each''' x '''with index''' i '''in''' m '''do'''\n         '''if''' i < (length of m)/2 '''then'''\n             add x to left\n         '''else'''\n             add x to right\n \n     // ''Recursively sort both sublists.''\n     left := merge_sort(left)\n     right := merge_sort(right)\n \n     // Then merge the now-sorted sublists.\n     '''return''' merge(left, right)\n\nIn this example, the {{mono|merge}} function merges the left and right sublists.\n\n '''function''' merge(left, right)\n     '''var''' result := empty list\n \n     '''while''' left is not empty '''and''' right is not empty '''do'''\n         '''if''' first(left) ≤ first(right) '''then'''\n             append first(left) to result\n             left := rest(left)\n         '''else'''\n             append first(right) to result\n             right := rest(right)\n \n     // ''Either left or right may have elements left; consume them.''\n     // ''(Only one of the following loops will actually be entered.)''\n     '''while''' left is not empty '''do'''\n         append first(left) to result\n         left := rest(left)\n     '''while''' right is not empty '''do'''\n         append first(right) to result\n         right := rest(right)\n     '''return''' result\n\n===Bottom-up implementation using lists===\n[[Pseudocode]] for bottom-up merge sort algorithm which uses a small fixed size array of references to nodes, where array[i] is either a reference to a list of size 2<sup>''i''</sup> or ''[[Null pointer|nil]]''. ''node'' is a reference or pointer to a node. The merge() function would be similar to the one shown in the top-down merge lists example, it merges two already sorted lists, and handles empty lists. In this case, merge() would use ''node'' for its input parameters and return value.\n\n '''function''' merge_sort(''node'' head)\n     // return if empty list\n     '''if''' (head == nil)\n         '''return''' nil\n     '''var''' ''node'' array[32]; initially all nil\n     '''var''' ''node'' result\n     '''var''' ''node'' next\n     '''var''' ''int''  i\n     result = head\n     // merge nodes into array\n     '''while''' (result != nil)\n          next = result.next;\n          result.next = nil\n          '''for'''(i = 0; (i < 32) && (array[i] != nil); i += 1)\n               result = merge(array[i], result)\n               array[i] = nil\n          // do not go past end of array\n          '''if''' (i == 32)\n                i -= 1\n          array[i] = result\n          result = next\n     // merge array into single list\n     result = nil\n     '''for''' (i = 0; i < 32; i += 1)\n          result = merge(array[i], result)\n     '''return''' result\n\n== Natural merge sort ==\n\nA natural merge sort is similar to a bottom-up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as [[Queue (abstract data type)|FIFO queues]] or [[Stack (abstract data type)|LIFO stacks]]).<ref>Powers, David M. W. and McMahon Graham B. (1983), \"A compendium of interesting prolog programs\", DCS Technical Report 8313, Department of Computer Science, University of New South Wales.</ref> In the bottom-up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of [[Timsort]]. Example:\n\n Start       :  3  4  2  1  7  5  8  9  0  6\n Select runs : (3  4)(2)(1  7)(5  8  9)(0  6)\n Merge       : (2  3  4)(1  5  7  8  9)(0  6)\n Merge       : (1  2  3  4  5  7  8  9)(0  6)\n Merge       : (0  1  2  3  4  5  6  7  8  9)\n\n[[Tournament sort|Tournament replacement selection sorts]] are used to gather the initial runs for external sorting algorithms.\n\n==Analysis==\n[[Image:merge sort algorithm diagram.svg|thumb|right|300px|A recursive merge sort algorithm used to sort an array of 7 integer values. These are the steps a human would take to emulate merge sort (top-down).]]\n\nIn sorting ''n'' objects, merge sort has an [[average performance|average]] and [[worst-case performance]] of [[big O notation|O]](''n''&nbsp;log&nbsp;''n''). If the running time of merge sort for a list of length ''n'' is ''T''(''n''), then the recurrence ''T''(''n'') = 2''T''(''n''/2) + ''n'' follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the ''n'' steps taken to merge the resulting two lists). The closed form follows from the [[master theorem (analysis of algorithms)|master theorem for divide-and-conquer recurrences]].\n\nIn the worst case, the number of comparisons merge sort makes is given by the [[sorting number]]s. These numbers are equal to or slightly smaller than (''n''&nbsp;⌈[[Binary logarithm|lg]]&nbsp;''n''⌉ − 2<sup>⌈lg&nbsp;''n''⌉</sup> + 1), which is between (''n''&nbsp;lg&nbsp;''n'' − ''n'' + 1) and (''n''&nbsp;lg&nbsp;''n'' + ''n'' + O(lg&nbsp;''n'')).<ref>The worst case number given here does not agree with that given in [[Donald Knuth|Knuth]]'s ''[[Art of Computer Programming]], Vol 3''. The discrepancy is due to Knuth analyzing a variant implementation of merge sort that is slightly sub-optimal</ref>\n\nFor large ''n'' and a randomly ordered input list,  merge sort's expected (average) number of comparisons approaches ''α''·''n'' fewer than the worst case where <math>\\alpha = -1 + \\sum_{k=0}^\\infty \\frac1{2^k+1} \\approx 0.2645.</math>\n\nIn the ''worst'' case, merge sort does about 39% fewer comparisons than [[quicksort]] does in the ''average'' case.  In terms of moves, merge sort's worst case complexity is [[big O notation|O]](''n''&nbsp;log&nbsp;''n'')—the same complexity as quicksort's best case, and merge sort's best case takes about half as many iterations as the worst case.{{Citation needed|date=June 2008}}\n\nMerge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as [[Lisp programming language|Lisp]], where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.\n\nMerge sort's most common implementation does not sort in place;<ref>{{cite book|last1=Cormen|last2=Leiserson|last3=Rivest|last4=Stein|title=Introduction to Algorithms|isbn=978-0-262-03384-8|page=151}}</ref> therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for versions that need only ''n''/2 extra spaces).\n\n==Variants==\nVariants of merge sort are primarily concerned with reducing the space complexity and the cost of copying.\n\nA simple alternative for reducing the space overhead to ''n''/2 is to maintain ''left'' and ''right'' as a combined structure, copy only the ''left'' part of ''m'' into temporary space, and to direct the ''merge'' routine to place the merged output into ''m''. With this version it is better to allocate the temporary space outside the ''merge'' routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the ''return result'' statement (function '' merge ''in the pseudo code above) become superfluous.\n\nOne drawback of merge sort, when implemented on arrays, is its {{math|''O''(''n'')}} working memory requirement. Several [[In-place algorithm|in-place]] variants have been suggested:\n* Katajainen ''et al.'' present an algorithm that requires a constant amount of working memory: enough storage space to hold one element of the input array, and additional space to hold {{math|''O''(1)}} pointers into the input array. They achieve an {{math|''O''(''n'' log ''n'')}} time bound with small constants, but their algorithm is not stable.<ref>{{cite journal |last1=Katajainen |first1=Jyrki |first2=Tomi |last2=Pasanen |first3=Jukka |last3=Teuhola |title=Practical in-place mergesort |journal=Nordic J. Computing |volume=3 |issue=1 |year=1996 |pages=27–40 |citeseerx=10.1.1.22.8523}}</ref>\n* Several attempts have been made at producing an ''in-place merge'' algorithm that can be combined with a standard (top-down or bottom-up) merge sort to produce an in-place merge sort. In this case, the notion of \"in-place\" can be relaxed to mean \"taking logarithmic stack space\", because standard merge sort requires that amount of space for its own stack usage. It was shown by Geffert ''et al.'' that in-place, stable merging is possible in {{math|''O''(''n'' log ''n'')}} time using a constant amount of scratch space, but their algorithm is complicated and has high constant factors: merging arrays of length {{mvar|n}} and {{mvar|m}} can take {{math|5''n'' + 12''m'' + ''o''(''m'')}} moves.<ref>{{Cite journal | doi = 10.1016/S0304-3975(98)00162-5| title = Asymptotically efficient in-place merging| journal = Theoretical Computer Science| volume = 237| pages = 159–181| year = 2000| last1 = Geffert | first1 = Viliam| last2 = Katajainen | first2 = Jyrki| last3 = Pasanen | first3 = Tomi}}</ref> This high constant factor and complicated in-place algorithm was made simpler and easier to understand. Bing-Chao Huang and Michael A. Langston<ref name=\"Research Contributions\">{{cite journal |first1=Bing-Chao |last1=Huang |first2=Michael A. |last2=Langston |title=Practical In-Place Merging |date=March 1988 |journal=Communications of the ACM |volume=31 |issue=3 |pages=348–352 |url= |ref= |doi=10.1145/42392.42403}}</ref> presented a straightforward linear time algorithm ''practical in-place merge'' to merge a sorted list using fixed amount of additional space. They both have used the work of Kronrod and others. It merges in linear time and constant extra space. The algorithm takes little more average time than standard merge sort algorithms, free to exploit O(n) temporary extra memory cells, by less than a factor of two. Though the algorithm is much faster in a practical way but it is unstable also for some lists. But using similar concepts, they have been able to solve this problem. Other in-place algorithms include SymMerge, which takes {{math|''O''((''n'' + ''m'') log (''n'' + ''m''))}} time in total and is stable.<ref>{{Cite conference| doi = 10.1007/978-3-540-30140-0_63| title = Stable Minimum Storage Merging by Symmetric Comparisons| conference = European Symp. Algorithms| volume = 3221| pages = 714–723| series = Lecture Notes in Computer Science| year = 2004| last1 = Kim | first1 = Pok-Son| last2 = Kutzner | first2 = Arne| isbn = 978-3-540-23025-0| citeseerx=10.1.1.102.4612}}</ref> Plugging such an algorithm into merge sort increases its complexity to the non-linearithmic, but still [[quasilinear time|quasilinear]], {{math|''O''(''n'' (log ''n'')<sup>2</sup>)}}.\n* A modern stable linear and in-place merging is [[block merge sort]].\n\nAn alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in ''m'' are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.\n\n==Use with tape drives==\n[[File:IBM 729 Tape Drives.nasa.jpg|thumb|Merge sort type algorithms allowed large data sets to be sorted on early computers that had small random access memories by modern standards. Records were stored on [[magnetic tape]] and processed on banks of magnetic tape drives, such as these [[IBM 729]]s.]]\nAn [[External sorting|external]] merge sort is practical to run using [[disk storage|disk]] or [[tape drive|tape]] drives when the data to be sorted is too large to fit into [[primary storage|memory]]. [[External sorting]] explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just two record buffers and a few program variables.\n\nNaming the four tape drives as A, B, C, D, with the original data on A, and using only 2 record buffers, the algorithm is similar to [[#Bottom-up_implementation|Bottom-up implementation]], using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:\n\n# Merge pairs of records from A; writing two-record sublists alternately to C and D.\n# Merge two-record sublists from C and D into four-record sublists; writing these alternately to A and B.\n# Merge four-record sublists from A and B into eight-record sublists; writing these alternately to C and D\n# Repeat until you have one list containing all the data, sorted—in log<sub>2</sub>(''n'') passes.\n\nInstead of starting with very short runs, usually a [[hybrid algorithm]] is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save nine passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory.<ref>Selection sort. Knuth's snowplow. Natural merge.</ref>\n\nWith some overhead, the above algorithm can be modified to use three tapes.  ''O''(''n'' log ''n'') running time can also be achieved using two [[queue (abstract data type)|queues]], or a [[stack (abstract data type)|stack]] and a queue, or three stacks.  In the other direction, using ''k'' > two tapes (and ''O''(''k'') items in memory), we can reduce the number of tape operations in ''O''(log ''k'') times by using a [[k-way merge algorithm|k/2-way merge]].\n\nA more sophisticated merge sort that optimizes tape (and disk) drive usage is the [[polyphase merge sort]].\n\n==Optimizing merge sort==\n[[Image:Merge sort animation2.gif|thumb|Tiled merge sort applied to an array of random integers.  The horizontal axis is the array index and the vertical axis is the integer.]]\nOn modern computers, [[locality of reference]] can be of paramount importance in [[software optimization]], because multilevel [[Memory hierarchy|memory hierarchies]] are used. [[Cache (computing)|Cache]]-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the '''{{visible anchor|tiled merge sort}}''' algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as [[insertion sort]], to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance {{Examples|date=August 2016}} on machines that benefit from cache optimization. {{Harv|LaMarca|Ladner|1997}}\n\n{{Harvtxt|Kronrod|1969}} suggested an alternative version of merge sort that uses constant additional space. This algorithm was later refined. {{Harv|Katajainen|Pasanen|Teuhola|1996}}\n\nAlso, many applications of [[external sorting]] use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of [[page (computer memory)|pages]] fit into main memory.\n\n==Parallel merge sort==\nMerge sort parallelizes well due to use of the divide-and-conquer method.  Several parallel variants are discussed in the third edition of Cormen, Leiserson, Rivest, and Stein's ''[[Introduction to Algorithms]]''.<ref name=\"clrs\">{{Harvnb|Cormen|Leiserson|Rivest|Stein|2009|pp=797–805}}</ref> The first of these can be very easily expressed in a pseudocode with [[fork–join model|fork and join]] keywords:\n\n // ''Sort elements lo through hi (exclusive) of array A.''\n '''algorithm''' mergesort(A, lo, hi) '''is'''\n     '''if''' lo+1 < hi '''then'''  // ''Two or more elements.''\n         mid = ⌊(lo + hi) / 2⌋\n         '''fork''' mergesort(A, lo, mid)\n         mergesort(A, mid, hi)\n         '''join'''\n         merge(A, lo, mid, hi)\n\nThis algorithm is a trivial modification from the serial version, and its speedup is not impressive: when executed on an [[Analysis of parallel algorithms|infinite number of processors]], it runs in {{math|Θ(''n'')}} time, which is only a {{math|Θ(log ''n'')}} improvement on the serial version. A better result can be obtained by using a parallelized [[merge algorithm]], which gives parallelism {{math|Θ(''n'' / (log ''n'')<sup>2</sup>)}}, meaning that this type of parallel merge sort runs in\n\n:<math>\\Theta \\left((n \\log n) \\cdot \\frac{(\\log n)^2}{n}\\right) = \\Theta((\\log n)^3)</math>\n\ntime if enough processors are available.{{r|clrs}}  Such a sort can perform well in practice when combined with a fast stable sequential sort, such as [[insertion sort]], and a fast sequential merge as a base case for merging small arrays.<ref>Victor J. Duvanenko \"Parallel Merge Sort\" Dr. Dobb's Journal & blog[https://duvanenko.tech.blog/2018/01/13/parallel-merge-sort/] and GitHub repo C++ implementation [https://github.com/DragonSpit/ParallelAlgorithms]</ref>\n\nMerge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure {{math|''O''(1)}} merge.<ref>{{Cite journal|first1=Richard|last1=Cole|title=Parallel merge sort|journal=SIAM J. Comput.|volume=17|issue=4|date=August 1988|pages=770–785|doi=10.1137/0217049|ref=harv|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}|citeseerx=10.1.1.464.7118}}</ref> Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized [[quicksort]] (and a related [[radix sort]]) that can operate in ''O''(log ''n'') time on a [[CRCW]] [[parallel random-access machine]] (PRAM) with ''n'' processors by performing partitioning implicitly.<ref>Powers, David M. W. [http://citeseer.ist.psu.edu/327487.html Parallelized Quicksort and Radixsort with Optimal Speedup], ''Proceedings of International Conference on Parallel Computing Technologies''. [[Novosibirsk]]. 1991.</ref> Powers<ref>David M. W. Powers, [http://david.wardpowers.info/Research/AI/papers/199501-ACAW-PUPC.pdf Parallel Unification: Practical Complexity], Australasian Computer Architecture Workshop, Flinders University, January 1995</ref> further shows that a pipelined version of Batcher's [[Bitonic sorter|Bitonic Mergesort]] at ''O''((log ''n'')<sup>2</sup>) time on a butterfly [[sorting network]] is in practice actually faster than his ''O''(log ''n'') sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.\n\n==Comparison with other sort algorithms==\nAlthough [[heapsort]] has the same time bounds as merge sort, it requires only  Θ(1) auxiliary space instead of merge sort's Θ(''n''). On typical modern architectures, efficient [[quicksort]] implementations generally outperform mergesort for sorting RAM-based arrays.{{Citation needed|date=March 2014}} On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a [[linked list]]: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.\n\nAs of [[Perl]] 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In [[Java platform|Java]], the [https://docs.oracle.com/javase/9/docs/api/java/util/Arrays.html#sort-java.lang.Object:A- Arrays.sort()] methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to [[insertion sort]] when fewer than seven array elements are being sorted.<ref>[https://hg.openjdk.java.net/jdk/jdk/file/9c3fe09f69bc/src/java.base/share/classes/java/util/Arrays.java#l1331 OpenJDK src/java.base/share/classes/java/util/Arrays.java @ 53904:9c3fe09f69bc]</ref> The [[Linux]] kernel uses merge sort for its linked lists.<ref>[https://github.com/torvalds/linux/blob/master/lib/list_sort.c linux kernel /lib/list_sort.c]</ref> [[Python (programming language)|Python]] uses [[Timsort]], another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in [[Java 7|Java SE 7]] (for arrays of non-primitive types),<ref>{{cite web\n |title       = Commit 6804124: Replace \"modified mergesort\" in java.util.Arrays.sort with timsort\n |url         = http://hg.openjdk.java.net/jdk7/jdk7/jdk/rev/bfd7abda8f79\n |last        = jjb\n |work        = Java Development Kit 7 Hg repo\n |accessdate  = 24 Feb 2011\n |deadurl     = no\n |archiveurl  = https://www.webcitation.org/6wkoy1LbO?url=http://hg.openjdk.java.net/jdk7/jdk7/jdk/rev/bfd7abda8f79\n |archivedate = 2018-01-26\n |df          = \n}}</ref> on the [[Android (operating system)|Android platform]],<ref>{{cite web|title=Class: java.util.TimSort<T> |url=https://android.googlesource.com/platform/libcore/+/jb-mr2-release/luni/src/main/java/java/util/TimSort.java |work=Android JDK Documentation |accessdate=19 Jan 2015 |deadurl=yes |archiveurl=https://web.archive.org/web/20150120063131/https://android.googlesource.com/platform/libcore/%2B/jb-mr2-release/luni/src/main/java/java/util/TimSort.java |archivedate=January 20, 2015 |df= }}</ref> and in [[GNU Octave]].<ref>{{cite web\n| title = liboctave/util/oct-sort.cc\n| url = http://hg.savannah.gnu.org/hgweb/octave/file/0486a29d780f/liboctave/util/oct-sort.cc\n| work = Mercurial repository of Octave source code\n| accessdate = 18 Feb 2013\n| quote = Code stolen in large part from Python's, listobject.c, which itself had no license header. However, thanks to [[Tim Peters (software engineer)|Tim Peters]] for the parts of the code I ripped-off.\n| at = Lines 23-25 of the initial comment block.\n}}</ref>\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n*{{Introduction to Algorithms |edition=3 |ref=harv}}\n*{{Cite news\n |last1        = Katajainen\n |first1       = Jyrki\n |last2        = Pasanen\n |first2       = Tomi\n |last3        = Teuhola\n |first3       = Jukka\n |year         = 1996\n |title        = Practical in-place mergesort\n |periodical   = Nordic Journal of Computing\n |volume       = 3\n |pages        = 27–40\n |url          = http://www.diku.dk/hjemmesider/ansatte/jyrki/Paper/mergesort_NJC.ps\n |accessdate   = 2009-04-04\n |issn         = 1236-6064\n |ref          = harv\n |archive-url  = https://web.archive.org/web/20110807033704/http://www.diku.dk/hjemmesider/ansatte/jyrki/Paper/mergesort_NJC.ps#\n |archive-date = 2011-08-07\n |dead-url     = yes\n |df           = \n}}. Also [http://citeseer.ist.psu.edu/katajainen96practical.html Practical In-Place Mergesort]. Also [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.8523]\n* {{cite book\n  | last = Knuth\n  | first = Donald\n  | authorlink = Donald Knuth\n  | series = [[The Art of Computer Programming]]\n  | volume= 3\n  | title= Sorting and Searching\n  | edition = 2nd\n  | publisher = Addison-Wesley\n  | year=  1998\n  | chapter = Section 5.2.4: Sorting by Merging\n  | pages = 158–168\n  | isbn = 0-201-89685-0\n  | ref = harv}}\n*{{Cite news\n | last = Kronrod\n | first = M. A.\n | year = 1969\n | title = Optimal ordering algorithm without operational field\n | periodical = Soviet Mathematics - Doklady\n | volume = 10\n | page = 744\n | ref = harv}}\n*{{Cite journal\n |first= A.\n |last= LaMarca\n |first2= R. E.\n |last2= Ladner\n |title= The influence of caches on the performance of sorting\n |journal= Proc. 8th Ann. ACM-SIAM Symp. on Discrete Algorithms (SODA97)\n |year= 1997\n |pages= 370&ndash;379\n |ref= harv|citeseerx= 10.1.1.31.1153\n }}\n* {{cite book\n  | last = Skiena\n  | first = Steven S.\n  | authorlink = Steven Skiena\n  | title= The Algorithm Design Manual\n  | edition = 2nd\n  | publisher = Springer\n  | year= 2008\n  | chapter = 4.5: Mergesort: Sorting by Divide-and-Conquer\n  | pages = 120-125\n  | isbn = 978-1-84800-069-8\n  | ref = harv}}\n* {{cite web\n | author=Sun Microsystems\n |title=Arrays API (Java SE 6)\n |url=http://java.sun.com/javase/6/docs/api/java/util/Arrays.html\n |accessdate=2007-11-19 }}\n* {{cite web\n |author     = Oracle Corp.\n |title      = Arrays (Java SE 10 & JDK 10)\n |url        = https://docs.oracle.com/javase/10/docs/api/java/util/Arrays.html\n |accessdate = 2018-07-23\n}}\n\n==External links==\n{{wikibooks|Algorithm implementation|Sorting/Merge_sort|Merge sort}}\n* {{webarchive |url=https://web.archive.org/web/20150306071601/http://www.sorting-algorithms.com/merge-sort |date=6 March 2015 |title=Animated Sorting Algorithms: Merge Sort}} – graphical demonstration\n* [http://opendatastructures.org/versions/edition-0.1e/ods-java/11_1_Comparison_Based_Sorti.html#SECTION001411000000000000000 Open Data Structures - Section 11.1.1 - Merge Sort]\n\n{{sorting}}\n\n{{DEFAULTSORT:Merge sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n[[Category:Articles with example pseudocode]]"
    }
  ]
}