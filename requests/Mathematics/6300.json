{
  "pages": [
    {
      "title": "Abel–Goncharov interpolation",
      "url": "https://en.wikipedia.org/wiki/Abel%E2%80%93Goncharov_interpolation",
      "text": "In [[mathematics]], '''Abel–Goncharov interpolation''' determines a [[polynomial]] such that various higher derivatives are the same as those of a given [[Function (mathematics)|function]] at given points. It was introduced by {{harvs|txt|last=Whittaker|authorlink=John Macnaghten Whittaker|year=1935}} and rediscovered by {{harvs|txt|last=Goncharov|year=1954}}.\n\n==References==\n{{reflist}}\n\n*{{Citation | last1=Whittaker | first1=J. M. | title=Interpolatory function theory | url=https://books.google.com/books?id=yyPvAAAAMAAJ | publisher=[[Cambridge University Press]] | series=Cambridge Tracts in Mathematics and Mathematical Physics, No. 33 | mr=0185330  | year=1935}}\n*{{Citation | last1=Goncharov | first1=V. L. | title=Teoriya interpolirovaniya i približeniya funkcii | publisher=Gosudarstv. Izdat. Tehn.-Teor. Lit., Moscow | language=Russian | mr=0067947  | year=1954}}\n\n{{DEFAULTSORT:Abel-Goncharov interpolation}}\n[[Category:Interpolation]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Aitken interpolation",
      "url": "https://en.wikipedia.org/wiki/Aitken_interpolation",
      "text": "'''Aitken interpolation''' is an algorithm used for [[polynomial interpolation]] that was derived by the mathematician [[Alexander Aitken]]. It is similar to [[Neville's algorithm]].\n\n== Use ==\n[[Arthur H. Robinson]] did not specify any particular interpolation method for his [[Robinson projection]], but it is reported that he used Aitken interpolation himself.<ref>{{cite journal |last=Richardson |first=R. T. |title=Area deformation on the Robinson projection |journal=The American Cartographer |date=1989 |volume=16 |pages=294–296}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{MathWorld|title=Aitken Interpolation|urlname=AitkenInterpolation}}\n\n[[Category:Polynomials]]\n[[Category:Interpolation]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "B-spline",
      "url": "https://en.wikipedia.org/wiki/B-spline",
      "text": "{{lead rewrite|date=August 2014}}\n[[File:B-spline curve.svg|thumb|right|400px| B-spline with control points/control polygon, and marked component curves]]\n\nIn the [[mathematics|mathematical]] subfield of [[numerical analysis]], a '''B-spline''', or '''basis spline''', is a [[spline (mathematics)|spline]] function that has minimal [[Support (mathematics)|support]] with respect to a given [[Degree of a polynomial|degree]], [[Smooth function|smoothness]], and [[Domain (mathematics)|domain]] partition. Any spline function of given degree can be expressed as a [[linear combination]] of B-splines of that degree. Cardinal B-splines have knots that are equidistant from each other. B-splines can be used for [[curve-fitting]] and [[numerical differentiation]] of experimental data.\n\nIn [[computer-aided design]] and [[computer graphics]], spline functions are constructed as linear combinations of B-splines with a set of control points.\n\n==Introduction==\nThe [[Term (language)|term]] \"B-spline\" was coined by [[Isaac Jacob Schoenberg]]<ref>de Boor, p. 114</ref> and is short for basis spline.<ref>Gary D. Knott (2000), ''[https://books.google.com/books?id=qkGlfJRuRs8C&dq=The+global+cubic+spline+basis+based+on+the+particular+join+order+2+piece-+wise+polynomials+known+as+B-splines+(B+stands+for+%22basis%22)+is+developed+by&source=gbs_navlinks_s Interpolating cubic splines]''. Springer. p. 151</ref> \nA spline function of order <math> n </math> is a [[piecewise]] [[polynomial]] function of degree <math> n-1 </math> in a variable <math> x </math>. The places where the pieces meet are known as knots. The key property of spline functions is that they and their derivatives may be continuous, depending on the multiplicities of the knots.\n\nB-splines of order <math>n</math> are [[basis function]]s for spline functions of the same order defined over the same knots, meaning that all possible spline functions can be built from a [[linear combination]] of B-splines, and there is only one unique combination for each spline function.<ref>{{Cite book|url=https://www.worldcat.org/oclc/851370272|title=Bézier and B-Spline Techniques|last=Hartmut.|first=Prautzsch,|date=2002|publisher=Springer Berlin Heidelberg|others=Boehm, Wolfgang., Paluszny, Marco.|year=|isbn=9783662049198|location=Berlin, Heidelberg|pages=63|doi=10.1007/978-3-662-04919-8|oclc=851370272}}</ref>\n\n==Definition==\n[[File:Cardinal quadratic B spline.svg|thumb|Cardinal quadratic B-spline with knot vector (0, 0, 0, 1, 2, 3, 3, 3) and control points (0, 0, 1, 0, 0), and its first derivative]]\n[[File:Cardinal cubic B-spline2.svg|thumb|Cardinal cubic B-spline with knot vector (−2, −2, −2, −2, −1, 0, 1, 2, 2, 2, 2) and control points (0, 0, 0, 6, 0, 0, 0), and its first derivative]]\n[[File:Cardinal quartic B-spline.svg|thumb|right|216px|Cardinal quartic B-spline with knot vector (0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 5, 5) and control points (0, 0, 0, 0, 1, 0, 0, 0, 0), and its first and second derivatives]]\n\nA spline of order <math>n</math> is a [[piecewise]] [[polynomial]] function of degree <math>n-1</math> in a variable <math>x</math>. The values of <math>x</math> where the pieces of polynomial meet are known as knots, denoted <math> \\ldots, t_0, t_1, t_2, \\ldots</math> and sorted into non-decreasing order. When the knots are distinct, the first <math>n-1</math> derivatives of the polynomial pieces are continuous across each knot. When <math>r</math> knots are coincident, then only the first <math>n-r</math> derivatives of the spline are continuous across that knot.\n\nFor a given sequence of knots, there is, up to a scaling factor, a unique spline <math>B_{i,n}(x)</math> satisfying\n\n:<math>B_{i,n}(x) = \\left\\{\n\\begin{array}{ll}\n0 & \\mathrm{if} \\quad x \\leq t_i \\quad \\mathrm{or} \\quad x \\geq t_{i+n} \\\\\n\\mathrm{non zero} & \\mathrm{otherwise}\n\\end{array}\n\\right.\n</math>\n\nIf we add the additional constraint that <math> \\sum_i B_{i,n}(x) = 1 </math> for all <math>x</math> between the first and last knot, then the scaling factor of <math>B_{i,n}(x)</math> becomes fixed. The resulting <math>B_{i,n}(x)</math> spline functions are called B-splines.\n\nAlternatively, B-splines can be defined by construction by means of the Cox-de Boor recursion formula. Given a knot sequence <math> \\ldots, t_0, t_1, t_2, \\ldots</math>, then the B-splines of order 1 are defined by\n\n:<math>B_{i,1}(x) := \\left\\{\n\\begin{matrix} \n1 & \\mathrm{if} \\quad t_i \\leq x < t_{i+1} \\\\\n0 & \\mathrm{otherwise} \n\\end{matrix}\n\\right.\n</math>\n\nNote that these satisfy <math>\\sum_i B_{i,1}(x)= 1</math> for all <math>x</math> because for any <math>x</math> exactly one of the <math>B_{i,1}(x) = 1</math>, and all the others are zero.\n\nThe higher order B-splines are defined by recursion\n:<math>B_{i,k+1}(x) := \\frac{x - t_i}{t_{i+k} - t_i} B_{i,k}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} B_{i+1,k}(x).</math>\n\n==Properties==\n\nB-spline function is a combination of flexible bands that passes through the number of points that are called control points and creates smooth curves. These functions enable the creation and management of complex shapes and surfaces using a number of points. B-spline function and Bézier functions are applied extensively in shape optimization methods.<ref name=\"ssdfgds\">{{cite journal | last1 = Talebitooti | first1 = R.| last2 = shojaeefard | first2 = M.H.| last3 = Yarmohammadisatri | first3 = Sadegh | year = 2015 | title = Shape design optimization of cylindrical tank using b-spline curves | url = | journal = Computer & Fluids | volume = 109 | pages = 100–112 | doi= 10.1016/j.compfluid.2014.12.004 }}</ref>\n\nA B-spline of order <math>n</math> is a piecewise polynomial function of degree <math>n-1</math> in a variable <math>x</math>. It is defined over <math> 1 + n </math> locations <math> t_j </math>, called knots or breakpoints, which must be in non-descending order <math>t_j \\leq t_{j+1}</math>. The B-spline contributes only in the range between the first and last of these knots and is zero elsewhere. If each knot is separated by the same distance <math> h </math> (where <math> h = t_{j+1}-t_j </math>) from its predecessor, the knot vector and the corresponding B-splines are called 'uniform' (see cardinal B-spline below).\n\nFor each finite knot interval where it is non-zero, a B-spline is a polynomial of degree <math> n-1 </math>. A B-spline is a [[continuous function]] at the knots.<ref group=note>Strictly speaking, B-splines are usually defined as being left-continuous.</ref> When all knots belonging to the B-spline are distinct, its derivatives are also continuous up to the derivative of degree <math>n-1</math>. If the knots are coincident at a given value of <math>x</math>, the continuity of derivative order is reduced by 1 for each additional coincident knot. B-splines may share a subset of their knots, but two B-splines defined over exactly the same knots are identical. In other words, a B-spline is uniquely defined by its knots.\n\nOne distinguishes internal knots and end points. Internal knots cover the <math>x</math>-domain one is interested in. Since a single B-spline already extends over <math> 1 + n </math> knots, it follows that the internal knots need to be extended with <math>n-1</math> end points on each side, to give full support to the first and last B-spline which affect the internal knot intervals. The values of the endpoints do not matter, usually the first or last internal knot is just repeated.\n\nThe usefulness of B-splines lies in the fact that any spline function of order <math>n</math> on a given set of knots can be expressed as a linear combination of B-splines:\n: <math>S_{n,\\mathbf t}(x) =\\sum_i \\alpha_i B_{i,n}(x).</math>\nB-splines play the role of [[basis function]]s for the spline function space, hence the name. This property follows from the fact that all pieces have the same continuity properties, within their individual range of support, at the knots.<ref>de Boor, p 113.</ref>\n\nExpressions for the polynomial pieces can be derived by means of the Cox-de Boor recursion formula<ref>de Boor, p 131.</ref>\n\n:<math>B_{i,0}(x) := \\left\\{\n\\begin{matrix} \n1 & \\mathrm{if} \\quad t_i \\leq x < t_{i+1} \\\\\n0 & \\mathrm{otherwise} \n\\end{matrix}\n\\right.\n</math>\n\n:<math>B_{i,k}(x) := \\frac{x - t_i}{t_{i+k} - t_i} B_{i,k-1}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} B_{i+1,k-1}(x).</math><ref>de Boor, p. 131</ref>\n\nThat is, <math>B_{j,0}(x)</math> is piecewise constant one or zero indicating which knot span ''x'' is in (zero if knot span ''j'' is repeated). The recursion equation is in two parts: \n:<math>\\frac{x - t_i}{t_{i+k} - t_i}</math> \nramps from zero to one as ''x'' goes from <math>t_i</math> to <math>t_{i+k}</math> and\n:<math>\\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}}</math>\nramps from one to zero as ''x'' goes from <math>t_{i+1}</math> to <math>t_{i+k+1}</math>. The corresponding ''B''s are zero outside those respective ranges. For example, <math>B_{i,1}(x)</math> is a [[triangular function]] that is zero below <math>x=t_i</math>, ramps to one at <math>x=t_{i+1}</math> and back to zero at and beyond <math>x=t_{i+2}</math>. However, because B-spline basis functions have local [[support (mathematics)|support]], B-splines are typically computed by algorithms that do not need to evaluate basis functions where they are zero, such as [[de Boor's algorithm]].\n\nThis relation leads directly to the [[FORTRAN]]-coded algorithm BSPLV which generates values of the B-splines of order ''n'' at ''x''.<ref>de Boor, p. 134.</ref> The following scheme illustrates how each piece of order ''n'' is a linear combination of the pieces of B-splines of order ''n''-1 to its left.\n\n:<math>\n\\begin{matrix}\n& & 0\\\\\n &0 & \\\\\n0& &B_{i-2,2}\\\\\n &B_{i-1,1}& \\\\\nB_{i,0}& &B_{i-1,2}\\\\\n &B_{i,1}& \\\\\n0& &B_{i,2}\\\\\n &0& \\\\\n& & 0\\\\\n\\end{matrix}\n</math>\n\nApplication of the recursion formula with the knots at <math>(0, 1, 2, 3)</math> gives the pieces of the uniform B-spline of order 3 \n:<math>B_1=x^2/2 \\qquad 0 \\le x \\le 1</math>\n:<math>B_2=(-2x^2+6x-3)/2 \\qquad 1 \\le x \\le 2</math>\n:<math>B_3=(3-x)^2/2 \\qquad 2 \\le x \\le 3</math>\nThese pieces are shown in the diagram. The continuity property of a quadratic spline function and its first derivative at the internal knots are illustrated, as follows\n: <math>\\mbox{At }x=1, B_1=B_2=0.5; \\frac{dB_1}{dx}=\\frac{dB_2}{dx}=1</math>\n: <math>\\mbox{At }x=2, B_2=B_3=0.5; \\frac{dB_2}{dx}=\\frac{dB_3}{dx}=-1</math>\nThe second derivative of a B-spline of degree 2 is discontinuous at the knots:\n: <math>\\frac{d^2B_1}{dx^2}=1, \\frac{d^2B_2}{dx^2}=-2,\\frac{d^2B_3}{dx^2}=-1.</math>\n\nFaster variants of the de Boor algorithm have been proposed but they suffer from comparatively lower stability.<ref>{{cite journal |last=Lee |first=E. T. Y. |date=December 1982 |title=A Simplified B-Spline Computation Routine |journal=Computing |volume=29 |issue=4 |pages=365–371 |doi=10.1007/BF02246763}}</ref><ref>{{cite journal | author = Lee, E. T. Y. | journal = Computing | issue = 3 | \tpages = 229–238 | doi=10.1007/BF02240069|title = Comments on some B-spline algorithms | volume = 36 | year = 1986}}</ref>\n\n===Cardinal B-spline===\n\nA cardinal B-spline has a constant separation, ''h'', between knots. The cardinal B-splines for a given order ''n'' are just shifted copies of each other. They can be obtained from the simpler definition.<ref>de Boor, p 322.</ref>\n\n:<math>B_{i,n,t}(x) = \\frac{x-t_i}{h} n[0,\\dots,n](. - t_i)^{n-1}_+</math>\nThe \"placeholder\" notation is used to indicate that the ''n''th [[divided difference]] of the function <math>(t-x)^{n-1}_+</math> of the two variables ''t'' and ''x'' is to be taken by fixing ''x'' and considering <math>(t - x)^{n-1}_+</math> as a function of ''t'' alone.\n\nA cardinal B-spline has uniform spaced knots, therefore interpolation between the knots equals convolution with a smoothing kernel.\n\nExample, if we want to interpolate three values in between B-spline nodes (<math>\\textbf{b}</math>), we can write the signal as:\n\n<math>\\textbf{x} = [\\textbf{b}_1,0,0,\\textbf{b}_2,0,0,\\textbf{b}_3,0,0,...., \\textbf{b}_n,0,0]</math>\n\nConvolution of the signal <math>\\textbf{x}</math> with a rectangle function <math>\\textbf{h}=[1/3,1/3,1/3]</math> gives first order interpolated b-spline values. Second-order B-spline interpolation is convolution with a rectangle function twice <math>\\textbf{y} =\\textbf{x} * \\textbf{h} * \\textbf{h}</math>, by iterative filtering with a rectangle function higher order interpolation is obtained.\n\nFast b-spline interpolation on a uniform sample domain can be done by iterative mean-filtering. Alternatively, a rectangle function equals Sinc in Fourier domain. Therefore, cubic spline interpolation equals multiplying the signal in Fourier domain with Sinc^4.\n\nSee [[Irwin–Hall distribution#Special cases]] for algebraic expressions for the cardinal B-splines of degree 1-4.\n\n===P-spline===\n\nThe term P-spline stands for \"penalized B-spline\". It refers to using the B-spline representation where the coefficients are determined partly by the data to be [[Curve fitting|fitted]], and partly by an additional [[penalty function]] that aims to impose [[smoothness]] to avoid [[overfitting]].<ref>Eilers, P.H.C. and Marx, B.D. (1996). Flexible smoothing with B-splines and penalties (with comments and rejoinder). Statistical Science 11(2): 89-121.</ref>\n\n==Derivative expressions==\nThe derivative of a B-spline of degree ''k'' is simply a function of B-splines of degree ''k''-1.<ref>de Boor, p. 115</ref>\n:<math>\\frac{dB_{i,k}(x)}{dx}=k\\left(\\frac{B_{i,k-1}(x)}{t_{i+k}-t_{i}}-\\frac{B_{i+1,k-1}(x)}{t_{i+k+1}-t_{i+1}}\n\\right)</math>\nThis implies that\n:<math>\\frac{d}{dx}\\sum_i\\alpha_i B_{i,k}=\\sum_{i=r-k+2}^{s-1}k\\frac{\\alpha_i-\\alpha_{i-1}}{t_{i+k}-t_i}B_{i,k-1} \\ on [t_r.t_s]</math>\nwhich shows that there is a simple relationship between the derivative of a spline function and the B-splines of degree one less.\n\n==Moments of univariate B-Splines==\n\nUnivariate B-Splines, i.e. B-Splines where the knot positions lie in a single dimension, can be used to represent 1-d probability density functions <math>p(x)</math>. An example is a weighted sum of <math>i</math> B-Spline basis functions of order <math>n</math>, which each are area-normalized to unity (i.e. not directly evaluated using the standard de-Boor algorithm)\n\n: <math>p(x) = \\sum_{i} c_i \\cdot B_{i,n,\\textbf{norm}}(x) </math>\n\nand with normalization constant constraint <math>\\sum_i c_i = 1</math>.\nThe k-th raw moment <math>\\mu_k</math> of a normalized B-Spline <math>B_{i,n,\\textbf{norm}}</math> can be written as Carlson's Dirichlet Average <math>R_k</math>,<ref>{{cite journal |last1=Carlson|first=B.C.|year=1991|title=B-splines, hypergeometric functions, and Dirichlet averages |journal=Journal of Approximation Theory |volume=67|issue=3|pages=311–325|doi=10.1016/0021-9045(91)90006-V}}</ref> which in turn can be solved exactly via a contour integral and an iterative sum <ref>{{cite journal |last1=Glüsenkamp |first=T.|year=2018|title=Probabilistic treatment of the uncertainty from the finite size of weighted Monte Carlo data |journal=EPJ Plus |volume=133 |issue=6|pages=218|doi=10.1140/epjp/i2018-12042-x |arxiv=1712.01293}})</ref> as\n\n: <math>\\mu_k = R_k(\\mathbf{m};\\mathbf{t}) = \\int_{-\\infty}^{\\infty} x^k \\cdot B_{i,n,\\textbf{norm}}(x|t_1 \\dots t_j) dx = \\frac{\\Gamma(k+1) \\Gamma(m)}{\\Gamma(m+k)} \\cdot D_k(\\mathbf{m},\\mathbf{t}) </math>\n\nwith\n\n: <math> D_{k}= \\frac{1}{k}\\sum\\limits_{u=1}^{k} \\left[\\left(\\sum\\limits_{i=1}^{j} m_{i} \\cdot {t_{i}}^u \\right) D_{k-u}\\right] </math>\nand <math>D_0=1</math>. Here, <math>\\mathbf{t}</math> represents a vector with the <math>j</math> knot positions and <math>\\mathbf{m}</math> a vector with the respective knot multiplicities. One can therefore calculate any moment of a probability density function <math>p(x)</math> represented by a sum of B-Spline basis functions exactly, without resorting to numerical techniques.\n\n==Relationship to piecewise/composite Bézier==\nA [[Composite Bézier curve|piecewise/composite Bézier curve]] is a series of Bézier curves joined with at least [[Smooth function#Differentiability classes|C0 continuity]] (the last point of one curve coincides with the starting point of the next curve). Depending on the application, additional smoothness requirements (such as C1 or C2 continuity) may be added.<ref name=\"ShikinPlis1995\">{{cite book|author1=Eugene V. Shikin|author2=Alexander I. Plis|title=Handbook on Splines for the User|url=https://books.google.com/books?id=DL88KouJCQkC&pg=PA96|date=14 July 1995|publisher=CRC Press|isbn=978-0-8493-9404-1|pages=96–}}</ref> C1 continuous curves have identical tangents at the breakpoint (where the two curves meet). C2 continuous curves have identical curvature at the breakpoint.<ref name=\"Wernecke1993\">{{cite book |last=Wernecke |first=Josie |date=1993 |title=The Inventor Mentor: Programming Object-Oriented 3D Graphics with Open Inventor, Release 2 |url=http://www-evasion.imag.fr/~Francois.Faure/doc/inventorMentor/sgi_html/index.html |location=Boston, MA, USA |edition=1st |publisher=Addison-Wesley Longman Publishing Co., Inc. |chapter=8 |chapterurl=http://www-evasion.imag.fr/~Francois.Faure/doc/inventorMentor/sgi_html/ch08.html |isbn=978-0201624953 }}</ref>\n\nTo gain C2 continuity the Bézier curve loses local control, because to enforce C2 continuity the control points are dependent on each other. If a single control point moves, the whole spline needs to be re-evaluated. B-splines have both C2 continuity and local control, but they lose the interpolation property of a piecewise Bézier.<ref name=\"Zorin2002\">{{Citation| last =Zorin| first =Denis| title =Bezier Curves and B-splines, Blossoming| publisher =New York University| date =2002| url =http://mrl.nyu.edu/~dzorin/geom04/lectures/lect02.pdf| accessdate =4 January 2015 }}</ref>\n\n==Curve fitting==\nUsually in [[curve fitting]], a set of data points is fitted with a curve defined by some mathematical function. For example, common types of curve fitting use a polynomial or a set of [[exponential function]]s. When there is no theoretical basis for choosing a fitting function, the curve may be fitted with a spline function composed of a sum of B-splines, using the method of [[least squares]].<ref>de Boor, Chapter XIV, p. 235</ref><ref group=note>de Boor gives FORTRAN routines for least-squares fitting of experimental data.</ref> Thus, the [[objective function]] for least squares minimization is, for a spline function of degree ''k'',\n: <math>U=\\sum_{\\mathrm{all}\\,x}\\left\\{ W(x)\\left[y(x) - \\sum_i \\alpha_i B_{i,k,t}(x)\\right] \\right\\}^2</math>\n''W(x)'' is a weight and ''y(x)'' is the datum value at ''x''. The coefficients <math>\\alpha_i</math> are the parameters to be determined. The knot values may be fixed or they too may be treated as parameters.\n\nThe main difficulty in applying this process is in determining the number of knots to use and where they should be placed. de Boor suggests various strategies to address this problem. For instance, the spacing between knots is decreased in proportion to the curvature (2nd. derivative) of the data.{{Citation needed|date=November 2016}} A few applications have been published. For instance, the use of B-splines for fitting single [[Cauchy distribution|Lorentzian]] and [[Normal distribution|Gaussian]] curves has been investigated. Optimal spline functions of degrees 3-7 inclusive, based on symmetric arrangements of 5, 6, and 7 knots, have been computed and the method was applied for smoothing and differentiation of spectroscopic curves.<ref>{{cite journal |last1=Gans |first1=Peter |last2=Gill |first2=J. Bernard |year=1984 |title=Smoothing and Differentiation of Spectroscopic Curves Using Spline Functions |journal=Applied Spectroscopy |volume=38 |issue=3 |pages=370–376|doi=10.1366/0003702844555511}}</ref> In a comparable study, the two-dimensional version of the [[Savitzky-Golay filter]]ing and the spline method produced better results than [[moving average]] or [[Chebyshev filter]]ing.<ref>{{cite journal |last1=Vicsek |first1=Maria |last2=Neal |first2=Sharon L. |last3=Warner |first3=Isiah M. |year=1986 |title=Time-Domain Filtering of Two-Dimensional Fluorescence Data |journal=Applied Spectroscopy |volume=40 |issue=4 |pages=542–548|doi=10.1366/0003702864508773|url=http://www.dtic.mil/get-tr-doc/pdf?AD=ADA164954 }}</ref>\n\n==Computer Aided Design and Computer Graphics ==\nIn Computer Aided Design and Computer Graphics applications, a spline curve is sometimes represented as <math>C(t)</math>, a parametric curve of some real parameter <math>t</math>. In this case the curve <math>C(t)</math> can be treated as two or three separate coordinate functions <math>(x(t), y(t))</math>, or <math>(x(t), y(t), z(t))</math>. The coordinate functions <math> x(t)</math>, <math>y(t)</math> and <math>z(t)</math> are each spline functions, with a common set of knot values <math>t_1, t_2, \\ldots, t_n</math>.\n\nBecause a B-splines form basis functions, each of the coordinate functions can be expressed as a linear sum of B-splines, so we have\n\n:<math>\n\\begin{array}{lcl}\nX(t) & = & \\sum_i x_i B_{i,n}(t) \\\\\nY(t) & = & \\sum_i y_i B_{i,n}(t) \\\\\nZ(t) & = & \\sum_i z_i B_{i,n}(t)\n\\end{array}\n</math>\n\nThe weights <math>x_i</math>, <math>y_i</math> and <math>z_i</math> can be combined to form points <math>P_i=(x_i,y_i,z_i)</math> in 3-d space. These points <math>P_i</math> are commonly known as control points.\n\nWorking in reverse, a sequence of control points, knot values, and order of the B-spline define a parametric curve. This representation of a curve by control points has a several useful properties:\n\n1. The control points <math>P_i</math> define a curve. If the control points are all transformed together in some way, such as being translated, rotated, scaled, or moved by any affine transformation, then the corresponding curve is transformed in the same way.\n\n2. Because the B-splines are non-zero for just a finite number of knot intervals, if a single control point is moved, the corresponding change to the parametric curve is just over the parameter range of a small number knot intervals.\n\n3. Because <math>\\sum_i(B_{i,n}(x)) = 1</math>, and at all times each <math>B_{i,n}(x) \\geq 0</math>, then the curve remains inside the bounding box of the control points. Also, in some sense, the curve broadly follows the control points.\n\nA less desirable feature is that the parametric curve does not interpolate the control points. Usually the curve does not pass through the control points.\n\n==NURBS==\n{{main|Non-uniform rational B-spline}}\n[[File:RationalBezier2D.svg|thumb|NURBS curve &ndash; polynomial curve defined in homogeneous coordinates (blue) and its projection on plane &ndash; rational curve (red)]]\n\nIn [[computer aided design]], [[computer aided manufacturing]], and [[computer graphics]], a powerful extension of B-splines is non-uniform rational B-splines (NURBS). NURBS are essentially B-splines in [[homogeneous coordinates]]. Like B-splines, they are defined by their order, and a knot vector, and a set of control points, but unlike simple B-splines, the control points each have a weight. When the weight is equal to 1, a NURBS is simply a B-spline and as such NURBS generalizes both B-splines and [[Bézier curve]]s and surfaces, the primary difference being the weighting of the control points which makes NURBS curves \"rational\".\n\n[[Image:Surface modelling.svg|300px|right]]\nBy evaluating a NURBS at various values of the parameters, the curve can be traced through space; likewise, by evaluating a NURBS surface at various values of the two parameters, the surface can be represented in Cartesian space.\n\nLike B-splines, NURBS control points determine the shape of the curve. Each point of the curve is computed by taking a weighted sum of a number of control points. The weight of each point varies according to the governing parameter. For a curve of degree ''d'', the influence of any control point is only nonzero in ''d''+1 intervals (knot spans) of the parameter space. Within those intervals, the weight changes according to a polynomial function (basis functions) of degree ''d''. At the boundaries of the intervals, the basis functions go smoothly to zero, the smoothness being determined by the degree of the polynomial.\n\nThe knot vector is a sequence of parameter values that determines where and how the control points affect the NURBS curve. The number of knots is always equal to the number of control points plus curve degree plus one. Each time the parameter value enters a new knot span, a new control point becomes active, while an old control point is discarded.\n\nA NURBS curve takes the following form:<ref>Piegl and Tiller, chapter 4, sec. 2</ref>\n\n: <math>C(u) = \\frac {\\sum_{i=1}^k {N_{i,n}w_i P}_i} {\\sum_{i=1}^k {N_{i,n}w_i}}</math>\n\nHere the notation is as follows. ''u'' is the independent variable (instead of ''x''), ''k'' is the number of control points, ''N'' is a B-spline (used instead of ''B''), ''n'' is the polynomial degree, ''P'' is a control point and ''w'' is a weight. The denominator is a normalizing factor that evaluates to one if all weights are one.\n\nIt is customary to write this as\n\n: <math>C(u)=\\sum_{i=1}^k R_{i,n}(u)P_i</math>\n\nin which the functions\n\n: <math>R_{i,n}(u) = {N_{i,n}(u)w_i \\over \\sum_{j=1}^k N_{j,n}(u)w_j}</math>\n\nare known as the rational basis functions.\n\nA NURBS surface is obtained as the [[tensor product]] of two NURBS curves, thus using two independent parameters ''u'' and ''v'' (with indices ''i'' and ''j'' respectively):<ref>Piegl and Tiller, chapter 4, sec. 4</ref>\n\n: <math>S(u,v) = \\sum_{i=1}^k \\sum_{j=1}^l R_{i,j}(u,v) P_{i,j} </math>\n\nwith\n\n: <math>R_{i,j}(u,v) = \\frac {N_{i,n}(u) N_{j,m}(v) w_{i,j}} {\\sum_{p=1}^k \\sum_{q=1}^l N_{p,n}(u) N_{q,m}(v) w_{p,q}}</math>\n\nas rational basis functions.\n\n==See also==\n*[[Bézier curve]]\n*[[Box spline]]\n*[[De Boor algorithm]]\n*[[I-spline]]\n*[[M-spline]]\n*[[Spline wavelet]]\n*[[T-spline]]\n\n== Notes ==\n{{Reflist|group=note}}\n\n==References==\n{{Reflist|30em}}\n\n'''Works cited'''\n*{{cite book | author = Carl de Boor | title = A Practical Guide to Splines | publisher = Springer-Verlag | year = 1978|isbn=978-3-540-90356-7}}\n*{{cite book |last1=Piegl |first1=Les|last2=Tiller |first2=Wayne|title= The NURBS Book|edition=2nd. |year=1997|publisher=Springer|isbn=978-3-540-61545-3}}\n\n==Further reading==\n* {{cite book|author1=Richard H. Bartels|author2=John C. Beatty|author3=Brian A. Barsky|title=An Introduction to Splines for Use in Computer Graphics and Geometric Modeling|year=1987|publisher=Morgan Kaufmann|isbn=978-1-55860-400-1}}\n* {{cite book|url=http://www.cis.upenn.edu/~jean/gbooks/geom1.html|title=Curves and Surfaces in Geometric Modeling: Theory and Algorithms|author=Jean Gallier|authorlink= Jean Gallier |publisher=Morgan Kaufmann|year=1999|at=Chapter 6. B-Spline Curves}} This book is out of print and freely available from the author.\n* {{cite book|author1=Hartmut Prautzsch|author2=Wolfgang Boehm|author3=Marco Paluszny|title=Bézier and B-Spline Techniques|year=2002|publisher=Springer Science & Business Media|isbn=978-3-540-43761-1}}\n* {{cite book|author=David Salomon|title=Curves and Surfaces for Computer Graphics|year=2006|publisher=Springer|isbn=978-0-387-28452-1|at=Chapter 7. B-Spline Approximation}}\n\n==External links==\n* {{MathWorld|title=B-Spline|id=B-Spline}}\n* {{cite web | url=http://www.oxford-man.ox.ac.uk/~jruf/papers/ruf_bspline.pdf | title=B-splines of third order on a non-uniform grid | first1=Johannes | last1=Ruf | access-date=2012-05-02 | archive-url=https://web.archive.org/web/20131106031347/http://www.oxford-man.ox.ac.uk/~jruf/papers/ruf_bspline.pdf | archive-date=2013-11-06 | dead-url=yes | df= }}\n*[http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.bisplrep.html bivariate B-spline from numpy ]\n*[http://jsxgraph.uni-bayreuth.de/wiki/index.php/B-splines Interactive B-splines with JSXGraph]\n*[https://github.com/msteinbeck/tinyspline TinySpline: Opensource C-library with bindings for various languages]\n*[http://www.qwerkop.de/projects/bspline/doku.pdf Uniform non rational B-Splines, Modelling curves in 2D space. Author:Stefan G. Beck]\n*[https://editbspline.github.io/ B-Spline Editor by Shukant Pal]\n\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]\n\n[[de:Spline#B-Splines]]"
    },
    {
      "title": "Barnes interpolation",
      "url": "https://en.wikipedia.org/wiki/Barnes_interpolation",
      "text": "'''Barnes interpolation''', named after Stanley L. Barnes, is the [[interpolation]] of unstructured data points from a set of measurements of an unknown function in two dimensions into an [[analytic function]] of two variables. An example of a situation where the Barnes scheme is important is in [[weather forecasting]]<ref name=\"Objective Rainfall Analysis System\">{{cite web|url=http://www.bom.gov.au/bmrc/mdev/expt/rainanal/rainanal.shtml |archive-url=https://archive.is/20120722172541/http://www.bom.gov.au/bmrc/mdev/expt/rainanal/rainanal.shtml |dead-url=yes |archive-date=22 July 2012 |title=Objective Rainfall Analysis System |accessdate=6 May 2009 }}</ref><ref>{{cite journal | title=Thunderstorm distribution and frequency in Australia|series=Australian Meteorological Magazine|year=2002|pages=145–154|author=Y.Kuleshov|author2=G. de Hoedt|author3=W.Wright|author4=A.Brewster|last-author-amp=yes}}</ref> where measurements are made wherever monitoring stations may be located, the positions of which are constrained by [[topography]]. Such interpolation is essential in data visualisation, e.g. in the construction of [[contour plot]]s or other representations of analytic surfaces.\n\n== Introduction ==\nBarnes proposed an objective scheme for the interpolation of two dimensional data using a multi-pass scheme.<ref>{{cite journal|first=S. L | last =Barnes |title=A technique for maximizing details in numerical weather-map analysis|journal=Journal of Applied Meteorology|volume=3|issue=4|pages=396&ndash;409|year=1964|url=http://ams.allenpress.com/perlserv/?request=get-abstract&doi=10.1175%2F1520-0450(1964)003%3C0396:ATFMDI%3E2.0.CO%3B2|doi=10.1175/1520-0450(1964)003<0396:ATFMDI>2.0.CO;2|bibcode = 1964JApMe...3..396B }}</ref><ref>{{cite journal | last=Barnes | title=Mesoscale objective analysis using weighted time-series observations|series=NOAA Technical Memorandum|publisher=National Severe Storms laboratory|year=1964|first=S.L}}</ref>  This provided a method to interpolating sea-level pressures across the entire United States of America, producing a [[synoptic chart]] across the country using dispersed monitoring stations. Researchers have subsequently improved the Barnes method to reduce the number of parameters required for calculation of the interpolated result, increasing the objectivity of the method.<ref name=\"Koch_et_al\">{{citation |author=Koch, S. E. |author2=DesJardins, M |author3=Kocin, P |last-author-amp=yes|title=An interactive Barnes Objective Map Analysis Scheme for Use with Satellite and Conventional Data|year=1983|journal=Journal of Climate and Applied Meteorology}}</ref>\n\nThe method constructs a grid of size determined by the distribution of the two dimensional data points. Using this grid, the function values are calculated at each grid point. To do this the method utilises a series of [[Gaussian function]]s, given a [[distance weighting]] in order to determine the relative importance of any given measurement on the determination of the function values. Correction passes are then made to optimise the function values, by accounting for the [[spectral response]] of the interpolated points.\n\n== Method ==\nHere we describe the method of interpolation used in a multi-pass Barnes interpolation.\n\n=== First pass ===\nFor a given grid point ''i'',&nbsp;''j'' the interpolated function ''g''(''x''<sub>''i''</sub>,&nbsp;''y''<sub>''i''</sub>) is first approximated by the inverse weighting of the data points. To do this as weighting values is assigned to each Gaussian for each grid point, such that\n\n: <math>w_{ij} = \\exp\\left(-\\frac{r_m^2}{\\kappa}\\right), \\, </math>\n\nwhere <math>\\kappa</math> is a falloff parameter that controls the width of the Gaussian function. This parameter is controlled by the characteristic data spacing, for a fixed Gaussian cutoff radius ''w''<sub>''ij''</sub>&nbsp;=&nbsp;''e''<sup>&minus;1</sup> giving Δ''n'' such that:\n\n: <math>\\kappa = 5.052\\,\\left(\\frac{2\\, \\Delta n}{\\pi}\\right)^2. \\, </math>\n\nThe initial interpolation for the function from the measured values <math>f_k(x,y)</math> then becomes:\n\n: <math> g_0(x_i,y_j) = \\frac{\\displaystyle \\sum_k w_{ij} f_k(x,y)}{\\displaystyle \\sum_k w_{ij}}.</math>\n\n=== Second pass ===\nThe correction for the next pass then utilises the difference between the observed field and the interpolated values at the measurement points to optimise the result:<ref name=\"Objective Rainfall Analysis System\"/>\n\n: <math>g_1(x_i,y_j) = g_0(x_i,y_j)  +  \\sum( f(x,y) - g_0(x,y)) \\exp\\left(-\\frac{r_m^2}{\\gamma\\kappa}\\right). \\,</math>\n\nIt is worth to note that successive correction steps can be used in order to achieve better agreement between the interpolated function and the measured values at the experimental points.\n\n== Parameter selection ==\nAlthough described as an objective method, there are many parameters which control the interpolated field. The choice of Δ''n'', grid spacing Δ''x'' and <math>\\gamma</math> as well influence the final result. Guidelines for the selection of these parameters have been suggested,<ref name=\"Koch_et_al\"/> however the final values used are free to be chosen within these guidelines.\n\nThe data spacing used in the analysis, Δ''n'' may be chosen either by calculating the true experimental data inter-point spacing, or by the use of a [[complete spatial randomness]] assumption, depending upon the degree of [[Cluster analysis|clustering]] in the observed data. The smoothing parameter <math>\\gamma</math> is constrained to be between 0.2 and&nbsp;1.0.  For reasons of interpolation integrity, Δ''x'' is argued to be constrained between 0.3 and&nbsp;0.5.\n\n== Notes ==\n<references/>\n\n[[Category:Interpolation]]\n[[Category:Spatial data analysis]]"
    },
    {
      "title": "Bézier curve",
      "url": "https://en.wikipedia.org/wiki/B%C3%A9zier_curve",
      "text": "{{short description|Curve used in computer graphics and related fields}}\n[[Image:Bezier curve.svg|thumb|right|Cubic Bézier curve with four control points]]\n[[Image:Bezier basis.svg|thumb|right|The [[basis function]]s on the range {{nowrap|''t'' in [0,1]}} for cubic Bézier curves:\n{{nowrap|blue: ''y'' {{=}} (1 − ''t'')<sup>3</sup>}},\n{{nowrap|green: ''y''{{=}} 3(1 − ''t'')<sup>2</sup> ''t''}},\n{{nowrap|red: ''y''{{=}} 3(1 − ''t'') ''t''<sup>2</sup>}}, and\n{{nowrap|cyan: ''y'' {{=}} ''t''<sup>3</sup>}}.]]\nA '''Bézier curve'''  (pronounced {{IPA-fr|bezje|}} in French) is a [[parametric equation|parametric curve]] used in [[computer graphics]] and related fields.<ref name=\"Mortenson\">{{cite book |url=https://books.google.co.uk/books?id=YmQy799flPkC&pg=PA264 |p=264 |title=Mathematics for Computer Graphics Applications |author=Michael E. Mortenson |publisher=Industrial Press Inc. |year=1999|isbn=9780831131111 }}</ref> The curve, which is related to the [[Bernstein polynomial]], is named after [[Pierre Bézier]], who used it in the 1960s for designing curves for the bodywork of [[Renault]] cars.<ref name=\"Hazewinkel\">{{cite book |url=https://books.google.co.uk/books?id=3ndQH4mTzWQC&pg=PA119 |p=119 |title=Encyclopaedia of Mathematics: Supplement |volume=1 |author=Michiel Hazewinkel |publisher=Springer Science & Business Media |year=1997|isbn=9780792347095 }}</ref> Other uses include the design of computer [[font]]s and animation.<ref name=\"Hazewinkel\"/> Bézier curves can be combined to form a [[Composite Bézier curve|Bézier spline]], or generalized to higher dimensions to form [[Bézier surface]]s.<ref name=\"Hazewinkel\"/> The [[Bézier triangle]] is a special case of the latter.\n\nIn [[vector graphics]], Bézier curves are used to model smooth curves that can be scaled indefinitely. \"Paths\", as they are commonly referred to in image manipulation programs,<ref group=\"note\">Image manipulation programs such as [[Inkscape]], [[Adobe Photoshop]], and [[GIMP]].</ref> are combinations of linked Bézier curves. Paths are not bound by the limits of [[raster graphics|rasterized]] images and are intuitive to modify.\n\nBézier curves are also used in the time domain, particularly in [[animation]], [[user interface]]<ref group=\"note\">In animation applications such as [[Adobe Flash]], [[Adobe After Effects]], [[Microsoft Expression Blend]], [[Blender (software)|Blender]], [[Autodesk Maya]] and [[Autodesk 3ds max]].</ref> design and smoothing cursor trajectory in eye gaze controlled interfaces.<ref>{{Cite journal|title = Multimodal Intelligent Eye-Gaze Tracking System|journal = International Journal of Human-Computer Interaction|date = 2015-04-03|issn = 1044-7318|pages = 277–294|volume = 31|issue = 4|doi = 10.1080/10447318.2014.1001301|first = Pradipta|last = Biswas|first2 = Pat|last2 = Langdon}}</ref> For example, a Bézier curve can be used to specify the velocity over time of an object such as an icon moving from A to B, rather than simply moving at a fixed number of pixels per step. When animators or [[Graphical user interface|interface]] designers talk about the \"physics\" or \"feel\" of an operation, they may be referring to the particular Bézier curve used to control the velocity over time of the move in question.\n\nThis also applies to robotics where the motion of a welding arm, for example, should be smooth to avoid unnecessary wear.\n\nThe mathematical basis for Bézier curves—the [[Bernstein polynomial]]—had been known since 1912, but the polynomials were not applied to graphics until some 50 years later, when they were widely publicised by the [[France|French]] engineer [[Pierre Bézier]], who used them to design [[automobile]] bodies at [[Renault]].  The study of these curves was however first developed in 1959 by mathematician [[Paul de Casteljau]] using [[de Casteljau's algorithm]], a [[numerical stability|numerically stable]] method to evaluate Bézier curves at [[Citroën]], another French automaker.<ref name=\"FarinHoschek2002\">{{cite book |author1=Gerald E. Farin |author2=Josef Hoschek |author3=Myung-Soo Kim |title=Handbook of Computer Aided Geometric Design |url=https://books.google.com/books?id=0SV5G8fgxLoC&pg=PA4 |year=2002 |publisher=Elsevier |isbn=978-0-444-51104-1 |pages=4–6}}</ref>\n\n==Specific cases==\nA Bézier curve is defined by a set of ''control points'' '''P'''<sub>0</sub> through '''P'''<sub>''n''</sub>, where ''n'' is called its order (''n'' = 1 for linear, 2 for quadratic, etc.). The first and last control points are always the end points of the curve; however, the intermediate control points (if any) generally do not lie on the curve. The sums in the following sections are to be understood as [[Affine space#Affine combinations and barycenter|affine combinations]], the coefficients sum to 1.\n\n===Linear Bézier curves===\nGiven distinct points '''P'''<sub>0</sub> and '''P'''<sub>1</sub>, a linear Bézier curve is simply a [[straight line]] between those two points. The curve is given by\n\n:<math>\\mathbf{B}(t)=\\mathbf{P}_0 + t(\\mathbf{P}_1-\\mathbf{P}_0)=(1-t)\\mathbf{P}_0 + t\\mathbf{P}_1 \\mbox{ , } 0 \\le t \\le 1</math>\nand is equivalent to [[linear interpolation]].\n\n===Quadratic Bézier curves===\n[[File:Quadratic Beziers in string art.svg|thumb|upright|Quadratic Béziers in string art: The end points ('''&bull;''') and control point ('''&times;''') define the quadratic Bézier curve ('''&#8943;''').]]\nA quadratic Bézier curve is the path traced by the function '''B'''(''t''), given points '''P'''<sub>0</sub>, '''P'''<sub>1</sub>, and '''P'''<sub>2</sub>,\n\n: <math>\\mathbf{B}(t) = (1 - t)[(1 - t) \\mathbf P_0 + t \\mathbf P_1] + t [(1 - t) \\mathbf P_1 + t \\mathbf P_2] \\mbox{ , } 0 \\le t \\le 1</math>,\n\nwhich can be interpreted as the linear interpolant of corresponding points on the linear Bézier curves from '''P'''<sub>0</sub> to '''P'''<sub>1</sub> and from '''P'''<sub>1</sub> to '''P'''<sub>2</sub> respectively. Rearranging the preceding equation yields:\n\n: <math>\\mathbf{B}(t) = (1 - t)^{2}\\mathbf{P}_0 + 2(1 - t)t\\mathbf{P}_1 + t^{2}\\mathbf{P}_2 \\mbox{ , } 0 \\le t \\le 1.</math>\n\nThis can be written in a way that highlights the symmetry with respect to '''P'''<sub>1</sub>:\n\n: <math>\\mathbf{B}(t) = \\mathbf{P}_1+(1 - t)^{2}(\\mathbf{P}_0-\\mathbf{P}_1) + t^{2}(\\mathbf{P}_2-\\mathbf{P}_1) \\mbox{ , } 0 \\le t \\le 1.</math>\n\nWhich immediately gives the derivative of the Bézier curve with respect to ''t'':\n\n: <math>\\mathbf{B}'(t) = 2 (1 - t) (\\mathbf{P}_1 - \\mathbf{P}_0) + 2 t (\\mathbf{P}_2 - \\mathbf{P}_1) \\,.</math>\n\nfrom which it can be concluded that the tangents to the curve at '''P'''<sub>0</sub> and '''P'''<sub>2</sub> intersect at '''P'''<sub>1</sub>. As ''t'' increases from 0 to 1, the curve departs from '''P'''<sub>0</sub> in the direction of '''P'''<sub>1</sub>, then bends to arrive at '''P'''<sub>2</sub> from the direction of '''P'''<sub>1</sub>.\n\nThe second derivative of the Bézier curve with respect to ''t'' is\n\n: <math>\\mathbf{B}''(t) = 2(\\mathbf{P}_2 - 2 \\mathbf{P}_1 + \\mathbf{P}_0) \\,.</math>\n\n===Cubic Bézier curves===\nFour points '''P'''<sub>0</sub>, '''P'''<sub>1</sub>, '''P'''<sub>2</sub> and '''P'''<sub>3</sub> in the plane or in higher-dimensional space define a cubic Bézier curve.\nThe curve starts at '''P'''<sub>0</sub> going toward '''P'''<sub>1</sub> and arrives at '''P'''<sub>3</sub> coming from the direction of '''P'''<sub>2</sub>. Usually, it will not pass through '''P'''<sub>1</sub> or '''P'''<sub>2</sub>; these points are only there to provide directional information. The distance between '''P'''<sub>1</sub> and '''P'''<sub>2</sub> determines \"how far\" and \"how fast\" the curve moves towards '''P'''<sub>1</sub> before turning towards '''P'''<sub>2</sub>.\n\nWriting '''B'''<sub>'''P'''<sub>i</sub>,'''P'''<sub>j</sub>,'''P'''<sub>k</sub></sub>(''t'') for the quadratic Bézier curve defined by points '''P'''<sub>i</sub>, '''P'''<sub>j</sub>, and '''P'''<sub>k</sub>, the cubic Bézier curve can be defined as an affine combination of two quadratic Bézier curves:\n\n:<math>\\mathbf{B}(t)=(1-t)\\mathbf{B}_{\\mathbf P_0,\\mathbf P_1,\\mathbf P_2}(t) + t \\mathbf{B}_{\\mathbf P_1,\\mathbf P_2,\\mathbf P_3}(t) \\mbox{ , } 0 \\le t \\le 1.</math>\n\nThe explicit form of the curve is:\n\n:<math>\\mathbf{B}(t)=(1-t)^3\\mathbf{P}_0+3(1-t)^2t\\mathbf{P}_1+3(1-t)t^2\\mathbf{P}_2+t^3\\mathbf{P}_3 \\mbox{ , } 0 \\le t \\le 1.</math>\n\nFor some choices of '''P'''<sub>1</sub> and '''P'''<sub>2</sub> the curve may intersect itself, or contain a [[Cusp (singularity)|cusp]].\n\nAny series of any 4 distinct points can be converted to a cubic Bézier curve that goes through all 4 points in order.\nGiven the starting and ending point of some cubic Bézier curve, and the points along the curve corresponding to ''t'' = 1/3 and ''t'' = 2/3, the control points for the original Bézier curve can be recovered.<ref>{{cite web |author=John Burkardt |deadurl=yes |archive-url=https://web.archive.org/web/20131225210855/http://people.sc.fsu.edu/~jburkardt/html/bezier_interpolation.html |url=http://people.sc.fsu.edu/~jburkardt/html/bezier_interpolation.html |title=Forcing Bezier Interpolation |archive-date=2013-12-25}}</ref>\n\nThe derivative of the cubic Bézier curve with respect to ''t'' is\n: <math>\\mathbf{B}'(t) = 3(1-t)^2(\\mathbf{P}_1 - \\mathbf{P}_0) + 6(1-t)t(\\mathbf{P}_2 - \\mathbf{P}_1) + 3t^2(\\mathbf{P}_3 - \\mathbf{P}_2) \\,.</math>\n\nThe second derivative of the Bézier curve with respect to ''t'' is\n: <math>\\mathbf{B}''(t) = 6(1-t)(\\mathbf{P}_2 - 2 \\mathbf{P}_1 + \\mathbf{P}_0) +  6t(\\mathbf{P}_3 - 2 \\mathbf{P}_2 + \\mathbf{P}_1) \\,.</math>\n\n==General definition==\nBézier curves can be defined for any degree ''n.''\n\n===Recursive definition===\nA recursive definition for the Bézier curve of degree ''n'' expresses it as a point-to-point [[linear combination]] ([[linear interpolation]]) of a pair of corresponding points in two Bézier curves of degree ''n''&nbsp;&minus;&nbsp;1.\n\nLet <math>\\mathbf{B}_{\\mathbf{P}_0\\mathbf{P}_1\\ldots\\mathbf{P}_n}</math> denote the Bézier curve determined by any selection of points '''P'''<sub>0</sub>, '''P'''<sub>1</sub>,&nbsp;...,&nbsp;'''P'''<sub>''n''</sub>.  Then to start,\n\n:<math>\\mathbf{B}_{\\mathbf{P}_0}(t) = \\mathbf{P}_0 \\text{, and}</math>\n\n:<math>\\mathbf{B}(t) = \\mathbf{B}_{\\mathbf{P}_0\\mathbf{P}_1\\ldots\\mathbf{P}_n}(t) = (1-t)\\mathbf{B}_{\\mathbf{P}_0\\mathbf{P}_1\\ldots\\mathbf{P}_{n-1}}(t) + t\\mathbf{B}_{\\mathbf{P}_1\\mathbf{P}_2\\ldots\\mathbf{P}_n}(t)</math>\n\nThis recursion is elucidated in the animations below.\n\n===Explicit definition===\nThe formula can be expressed explicitly as follows:\n\n:<math>\\begin{align}\n  \\mathbf{B}(t) &= \\sum_{i=0}^n {n\\choose i}(1 - t)^{n - i}t^i\\mathbf{P}_i \\\\\n                &=(1 - t)^n\\mathbf{P}_0 + {n\\choose 1}(1 - t)^{n - 1}t\\mathbf{P}_1 + \\cdots + {n\\choose n - 1}(1 - t)t^{n - 1}\\mathbf{P}_{n - 1} + t^n\\mathbf{P}_n && 0 \\leqslant t \\leqslant 1\n\\end{align}</math>\n\nwhere <math>\\scriptstyle {n \\choose i}</math> are the [[binomial coefficient]]s.\n\nFor example, for ''n''&nbsp;=&nbsp;5:\n\n:<math>\\begin{align}\n\\mathbf{B}(t) &= (1 - t)^5\\mathbf{P}_0 + 5t(1 - t)^4\\mathbf{P}_1 + 10t^2(1 - t)^3 \\mathbf{P}_2  + 10t^3 (1-t)^2 \\mathbf{P}_3 + 5t^4(1-t) \\mathbf{P}_4 + t^5 \\mathbf{P}_5 && 0 \\leqslant t \\leqslant 1\n\\end{align}</math>\n\n===Terminology===\nSome terminology is associated with these parametric curves. We have\n\n:<math>\\mathbf{B}(t) = \\sum_{i=0}^n b_{i, n}(t)\\mathbf{P}_i,\\quad 0 \\le t \\le 1</math>\n\nwhere the polynomials\n\n:<math>b_{i,n}(t) = {n\\choose i} t^i (1 - t)^{n - i},\\quad i = 0, \\ldots, n</math>\n\nare known as [[Bernstein polynomial|Bernstein basis polynomials]] of degree ''n''.\n\nNote that ''t''<sup>0</sup>&nbsp;=&nbsp;1, (1&nbsp;−&nbsp;''t'')<sup>0</sup>&nbsp;=&nbsp;1, and that the [[binomial coefficient]], <math>\\scriptstyle {n \\choose i}</math>, also expressed as <math>^n{\\mathbf{C}}_i </math> or <math>{\\mathbf{C}}_i^n </math>, is:\n\n:<math>{n \\choose i} = \\frac{n!}{i!(n - i)!}</math>\n\nThe points '''P'''<sub>''i''</sub> are called ''control points'' for the Bézier curve. The [[polygon]] formed by connecting the Bézier points with [[line (mathematics)|lines]], starting with '''P'''<sub>0</sub> and finishing with '''P'''<sub>''n''</sub>, is called the ''Bézier polygon'' (or ''control polygon''). The [[convex hull]] of the Bézier polygon contains the Bézier curve.\n\n===Polynomial form===\nSometimes it is desirable to express the Bézier curve as a [[polynomial]] instead of a sum of less straightforward [[Bernstein polynomial]]s. Application of the [[binomial theorem]] to the definition of the curve followed by some rearrangement will yield:\n\n:<math>\n\\mathbf{B}(t) = \\sum_{j = 0}^n {t^j \\mathbf{C}_j}\n</math>\n\nwhere\n\n:<math>\n\\mathbf{C}_j = \\frac{n!}{(n - j)!} \\sum_{i = 0}^j \\frac{(-1)^{i + j} \\mathbf{P}_i}{i! (j - i)!} =\n\\prod_{m = 0}^{j - 1} (n - m) \\sum_{i = 0}^j \\frac{(-1)^{i + j} \\mathbf{P}_i}{i! (j - i)!}\n.</math>\n\nThis could be practical if <math>\\mathbf{C}_j</math> can be computed prior to many evaluations of <math>\\mathbf{B}(t)</math>; however one should use caution as high order curves may lack [[numeric stability]] ([[de Casteljau's algorithm]] should be used if this occurs). Note that the [[empty product]] is 1.\n\n===Properties===\n[[File:quadratic_to_cubic_Bezier_curve.svg|thumb|upright|A cubic Bézier curve (yellow) can be made identical to a quadratic one (black) by\n<div style=\"margin-left:1em;text-indent:-1em\">1. Copying the end points, and</div>\n<div style=\"margin-left:1em;text-indent:-1em\">2. Placing its 2 middle control points (yellow circles) 2/3 along line segments from the end points to the quadratic curve's middle control point (black rectangle)</div>]]\n* The curve begins at '''P'''<sub>0</sub> and ends at '''P'''<sub>''n''</sub>; this is the so-called ''endpoint interpolation'' property.\n* The curve is a straight line if and only if all the control points are [[collinear]].\n* The start and end of the curve is [[tangent]] to the first and last section of the Bézier polygon, respectively.\n* A curve can be split at any point into two subcurves, or into arbitrarily many subcurves, each of which is also a Bézier curve.\n* Some curves that seem simple, such as the [[circle]], cannot be described exactly by a Bézier or [[piecewise]] Bézier curve; though a four-piece cubic Bézier curve can approximate a circle (see [[composite Bézier curve]]), with a maximum radial error of less than one part in a thousand, when each inner control point (or offline point) is the distance <math>\\textstyle\\frac{4\\left(\\sqrt {2}-1\\right)}{3}</math> horizontally or vertically from an outer control point on a unit circle.  More generally, an ''n''-piece cubic Bézier curve can approximate a circle, when each inner control point is the distance <math>\\textstyle\\frac{4}{3}\\tan(t/4)</math> from an outer control point on a unit circle, where ''t'' is 360/''n'' degrees, and ''n'' > 2.\n* Every quadratic Bézier curve is also a cubic Bézier curve, and more generally, every degree ''n'' Bézier curve is also a degree ''m'' curve for any ''m'' > ''n''. In detail, a degree ''n'' curve with control points '''P'''<sub>0</sub>, …, '''P'''<sub>''n''</sub> is equivalent (including the parametrization) to the degree ''n'' + 1 curve with control points '''P''''<sub>0</sub>, …, '''P''''<sub>''n'' + 1</sub>, where <math>\\mathbf P'_k=\\tfrac{k}{n+1}\\mathbf P_{k-1}+\\left(1-\\tfrac{k}{n+1}\\right)\\mathbf P_k</math>.\n* Bézier curves have the [[variation diminishing property]]. What this means in intuitive terms is that a Bézier curves does not \"undulate\" more than the polygon of its control points, and may actually \"undulate\" less than that.<ref name=\"GonzalezDiaz-Herrera2014\">{{cite book|author1=Teofilo Gonzalez|author1-link= Teofilo F. Gonzalez |author2=Jorge Diaz-Herrera|author3=Allen Tucker|title=Computing Handbook, Third Edition: Computer Science and Software Engineering|url=https://books.google.com/books?id=vMqSAwAAQBAJ&pg=SA32-PA14|year=2014|publisher=CRC Press|isbn=978-1-4398-9852-9|at=page 32-14<!--this is a single page number-->}}</ref>\n* There is no [[local control]] in degree ''n'' Bézier curves—meaning that any change to a control point requires recalculation of and thus affects the aspect of the entire curve, \"although the further that one is from the control point that was changed, the smaller is the change in the curve.\"<ref name=\"Agoston2005\">{{cite book|author=Max K. Agoston|title=Computer Graphics and Geometric Modelling: Implementation & Algorithms|url=https://books.google.com/books?id=TAYw3LEs5rgC&pg=PA404|year=2005|publisher=Springer Science & Business Media|isbn=978-1-84628-108-2|page=404}}</ref>\n\n===Second order curve is a parabolic segment===\n\n[[File:Quadratic_Bezier_parabola_equivalence.svg|thumb|upright|Equivalence of a quadratic Bézier curve and a parabolic segment]]\n\nA quadratic Bézier curve is also a segment of a [[parabola]]. As a parabola is a [[conic section]], some sources refer to quadratic Béziers as \"conic arcs\".<ref name=freetype/> With reference to the figure on the right, the important features of the parabola can be derived as follows:<ref>{{cite book |author=Duncan Marsh |title=Applied Geometry for Computer Graphics and CAD |series=Springer Undergraduate Mathematics Series |edition=2nd |date=2005 |isbn=978-1-85233-801-5|id= {{ASIN|1852338016|country=uk}} }}</ref>\n\n# Tangents to the parabola at the end-points of the curve (A and B) intersect at its control point (C).\n# If D is the midpoint of AB, the tangent to the curve which is perpendicular to CD (dashed cyan line) defines its vertex (V). Its axis of symmetry (dash-dot cyan) passes through V and is perpendicular to the tangent.\n# E is either point on the curve with a tangent at 45&#176; to CD (dashed green). If G is the intersection of this tangent and the axis, the line passing through G and perpendicular to CD is the directrix (solid green).\n# The focus (F) is at the intersection of the axis and a line passing through E and perpendicular to CD (dotted yellow). The latus rectum is the line segment within the curve (solid yellow).\n\n===Derivative===\n\nThe derivative for a curve of order ''n'' is\n\n:<math>\\mathbf{B}'(t) = n \\sum_{i=0}^{n-1} b_{i,n-1}(t) (\\mathbf{P}_{i+1} - \\mathbf{P}_i)</math>\n\n==Constructing Bézier curves==\n\n===Linear curves===\nThe ''t'' in the function for a linear Bézier curve can be thought of as describing how far '''B'''(''t'') is from '''P'''<sub>0</sub> to '''P'''<sub>1</sub>. For example, when ''t=0.25'', '''B'''(''t'') is one quarter of the way from point '''P'''<sub>0</sub> to '''P'''<sub>1</sub>. As ''t'' varies from 0 to 1, '''B'''(''t'') describes a straight line from '''P'''<sub>0</sub> to '''P'''<sub>1</sub>.\n<center>\n{|  style=\"text-align:center; float:none; clear:both; font-size:95%; vertical-align:top;\"\n|-\n| style=\"border-bottom:1px solid #222;\"|[[File:Bézier 1 big.gif|240px|Animation of a linear Bézier curve, ''t'' in [0,1]]]\n|-\n|Animation of a linear Bézier curve, ''t'' in [0,1]\n|}</center>\n\n===Quadratic curves===\nFor quadratic Bézier curves one can construct intermediate points '''Q'''<sub>0</sub> and '''Q'''<sub>1</sub> such that as ''t'' varies from 0 to 1:\n* Point '''Q'''<sub>0</sub>(''t'') varies from '''P'''<sub>0</sub> to '''P'''<sub>1</sub> and describes a linear Bézier curve.\n* Point '''Q'''<sub>1</sub>(''t'') varies from '''P'''<sub>1</sub> to '''P'''<sub>2</sub> and describes a linear Bézier curve.\n* Point '''B'''(''t'') is interpolated linearly between '''Q'''<sub>0</sub>(''t'') to '''Q'''<sub>1</sub>(''t'') and describes a quadratic Bézier curve.\n\n<center>\n{|  style=\"text-align:center; float:none; clear:both; font-size:95%; vertical-align:top;\"\n|-\n| style=\"border-bottom:1px solid #2f2;\"|[[File:Bézier 2 big.svg|240px|Construction of a quadratic Bézier curve]]||\n| style=\"border-bottom:1px solid #2f2;\"|[[File:Bézier 2 big.gif|240px|Animation of a quadratic Bézier curve, ''t'' in [0,1]]]\n|-\n|Construction of a quadratic Bézier curve||\n|Animation of a quadratic Bézier curve, ''t'' in [0,1]\n|}</center>\n\n===Higher-order curves===\nFor higher-order curves one needs correspondingly more intermediate points. For cubic curves one can construct intermediate points '''Q'''<sub>0</sub>,  '''Q'''<sub>1</sub>, and '''Q'''<sub>2</sub> that describe linear Bézier curves, and points '''R'''<sub>0</sub> & '''R'''<sub>1</sub> that describe quadratic Bézier curves:\n\n<center>\n{|  style=\"text-align:center; float:none; clear:both; font-size:95%; vertical-align:top;\"\n|-\n| style=\"border-bottom:1px solid #22f;\"|[[File:Bézier 3 big.svg|240px|Construction of a cubic Bézier curve]]||\n| style=\"border-bottom:1px solid #22f;\"|[[File:Bézier 3 big.gif|240px|Animation of a cubic Bézier curve, ''t'' in [0,1]]]\n|-\n|Construction of a cubic Bézier curve||\n|Animation of a cubic Bézier curve, ''t'' in [0,1]\n|}</center>\n\nFor fourth-order curves one can construct intermediate points '''Q'''<sub>0</sub>,  '''Q'''<sub>1</sub>, '''Q'''<sub>2</sub> & '''Q'''<sub>3</sub> that describe linear Bézier curves, points '''R'''<sub>0</sub>, '''R'''<sub>1</sub> & '''R'''<sub>2</sub> that describe quadratic Bézier curves, and points '''S'''<sub>0</sub> & '''S'''<sub>1</sub> that describe cubic Bézier curves:\n\n<center>\n{|  style=\"text-align:center; float:none; clear:both; font-size:95%; vertical-align:top;\"\n|-\n| style=\"border-bottom:1px solid #f2f;\"|[[File:Bézier 4 big.svg|240px|Construction of a quartic Bézier curve]]||\n| style=\"border-bottom:1px solid #f2f;\"|[[File:Bézier 4 big.gif|240px|Animation of a quartic Bézier curve, ''t'' in [0,1]]]\n|-\n|Construction of a quartic Bézier curve||\n|Animation of a quartic Bézier curve, ''t'' in [0,1]\n|}</center>\n\nFor fifth-order curves, one can construct similar intermediate points.\n<center>\n{|  style=\"text-align:center; float:none; clear:both; font-size:95%; vertical-align:top;\"\n|-\n|style=\"border-bottom: 1px solid silver\"|[[File:BezierCurve.gif|240px|Animation of the construction of a fifth-order Bézier curve]]\n|-\n|Animation of a fifth-order Bézier curve, ''t'' in [0,1] in red. The Bézier curves for each of the lower orders are also shown.\n|}</center>\n\nThese representations rest on the process used in [[De Casteljau's algorithm]] to calculate Bézier curves.<ref>{{cite web |last=Shene |first=C.K. |title=Finding a Point on a Bézier Curve: De Casteljau's Algorithm |url=http://www.cs.mtu.edu/~shene/COURSES/cs3621/NOTES/spline/Bezier/de-casteljau.html |accessdate=6 September 2012}}</ref>\n\n=== Offsets (a.k.a. stroking) of Bézier curves ===\nThe curve at a fixed offset from a given Bézier curve, called an [[parallel curve|offset or parallel curve]] in mathematics (lying \"parallel\" to the original curve, like the offset between rails in a [[railroad track]]), cannot be exactly formed by a Bézier curve (except in some trivial cases). In general, the two-sided offset curve of a cubic Bézier is a 10th-order [[algebraic curve]]<ref name=\"Kilgard\">{{cite web |author=Mark Kilgard |date=April 10, 2012 |title=CS 354 Vector Graphics & Path Rendering |url=http://www.slideshare.net/Mark_Kilgard/22pathrender |page=28}}</ref> and more generally for a Bézier of degree ''n'' the two-sided offset curve is an algebraic curve of degree 4''n''-2.<ref>{{cite web |title=Introduction to Pythagorean-hodograph curves |author=Rida T. Farouki |url=http://faculty.engineering.ucdavis.edu/farouki/wp-content/uploads/sites/41/2013/02/Introduction-to-PH-curves.pdf}}, particularly p.&nbsp;16 \"taxonomy of offset curves\".</ref> However, there are [[heuristic]] methods that usually give an adequate approximation for practical purposes.<ref>For example: [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.1724] or [http://www.google.com/patents/US20110285719]. For a survey see [http://www.computer.org/csdl/mags/cg/1997/03/mcg1997030062.pdf].</ref>\n\nIn the field of [[vector graphics]], painting two symmetrically distanced offset curves is called ''stroking'' (the Bézier curve or in general a path of several Bézier segments).<ref name=\"Kilgard\"/> The conversion from offset curves to filled Bézier contours is of practical importance in converting [[font]]s defined in [[METAFONT]], which allows stroking of Bézier curves, to the more widely used [[PostScript fonts|PostScript type 1 font]]s, which only allow (for efficiency purposes) the mathematically simpler operation of filling a contour defined by (non-self-intersecting) Bézier curves.<ref>{{cite journal |url=https://www.tug.org/TUGboat/tb16-3/tb48kinc.pdf |title=MetaFog: Converting Metafont shapes to contours |author=Richard J. Kinch |date=1995 |journal=TUGboat |volume=16 |issue=3–Proceedings of the 1995 Annual Meeting }}</ref>\n\n==Degree elevation==\nA Bézier curve of degree ''n'' can be converted into a Bézier curve of degree ''n''&nbsp;+&nbsp;1 ''with the same shape''. This is useful if software supports Bézier curves only of specific degree. For example, systems that can only work with cubic Bézier curves can implicitly work with quadratic curves by using their equivalent cubic representation.\n\nTo do degree elevation, we use the equality <math>\\mathbf{B}(t) = (1-t)\\mathbf{B}(t) + t\\mathbf{B}(t).</math> Each component <math>\\mathbf{b}_{i,n}(t)\\mathbf{P}_i</math> is multiplied by (1&nbsp;&minus;&nbsp;''t'') and&nbsp;''t'', thus increasing a degree by one, without changing the value. Here is the example of increasing degree from 2 to&nbsp;3.\n\n:<math>\\begin{align}\n(1 - t)^2 \\mathbf{P}_0 + 2(1 - t)t\\mathbf{P}_1 + t^2 \\mathbf{P}_2 &= (1 - t)^3 \\mathbf{P}_0  + 2(1 - t)^2 t\\mathbf{P}_1 + (1 - t)t^2 \\mathbf{P}_2 + (1 - t)^{2}t\\mathbf{P}_0 + 2(1 - t)t^2 \\mathbf{P}_1  + t^3 \\mathbf{P}_2 \\\\\n  &= (1 - t)^3 \\mathbf{P}_0 + (1 - t)^2 t   \\left( \\mathbf{P}_0 + 2\\mathbf{P}_1\\right) + (1 - t)   t^2 \\left(2\\mathbf{P}_1 +  \\mathbf{P}_2\\right) + t^{3}\\mathbf{P}_2 \\\\\n  &= (1 - t)^3 \\mathbf{P}_0 + 3(1 - t)^2 t   \\tfrac{1}{3} \\left(  \\mathbf{P}_0 + 2\\mathbf{P}_1 \\right) + 3(1 - t)   t^2 \\tfrac{1}{3} \\left( 2\\mathbf{P}_1 +  \\mathbf{P}_2 \\right) + t^{3}\\mathbf{P}_2\n\\end{align}</math>\n\nFor arbitrary ''n'' we use equalities\n\n:<math>\\begin{cases}  {n + 1 \\choose i}(1 - t)\\mathbf{b}_{i, n} = {n \\choose i} \\mathbf{b}_{i, n + 1} \\\\  {n + 1 \\choose i + 1} t\\mathbf{b}_{i, n} = {n \\choose i} \\mathbf{b}_{i + 1, n + 1} \\end{cases} \\quad \\Rightarrow \\quad \\begin{cases}  (1 - t)\\mathbf{b}_{i, n} = \\frac{n + 1 - i}{n + 1} \\mathbf{b}_{i, n + 1} \\\\ t\\mathbf{b}_{i, n} = \\frac{i + 1}{n + 1} \\mathbf{b}_{i + 1, n + 1} \\end{cases} </math>\n\nTherefore:\n\n:<math>\\begin{align}\n\\mathbf{B}(t) &= (1 - t)\\sum_{i=0}^n \\mathbf{b}_{i, n}(t)\\mathbf{P}_i + t\\sum_{i=0}^n \\mathbf{b}_{i, n}(t)\\mathbf{P}_i \\\\\n                &= \\sum_{i=0}^n \\frac{n + 1 - i}{n + 1}\\mathbf{b}_{i, n + 1}(t)\\mathbf{P}_i + \\sum_{i=0}^n \\frac{i + 1}{n + 1}\\mathbf{b}_{i + 1, n + 1}(t)\\mathbf{P}_i \\\\\n                &= \\sum_{i=0}^{n + 1} \\left(\\frac{i}{n + 1}\\mathbf{P}_{i - 1} + \\frac{n + 1 - i}{n + 1}\\mathbf{P}_i\\right) \\mathbf{b}_{i, n + 1}(t) \\\\\n                &= \\sum_{i=0}^{n + 1} \\mathbf{b}_{i, n + 1}(t)\\mathbf{P'}_i\n\\end{align}</math>\n\nintroducing arbitrary <math>\\mathbf{P}_{-1}</math> and <math>\\mathbf{P}_{n + 1}</math>.\n\nTherefore, new control points are <ref name=\"farin1997\">{{Cite book |title=Curves and surfaces for computer-aided geometric design |first=Gerald |last=Farin |publisher=[[Elsevier]] Science & Technology Books |year=1997 |isbn=978-0-12-249054-5 |edition=4}}</ref>\n\n: <math>\\mathbf{P'}_i = \\frac{i}{n + 1}\\mathbf{P}_{i - 1} + \\frac{n + 1 - i}{n + 1}\\mathbf{P}_i,\\quad i=0, \\ldots, n + 1</math>\n\n===Repeated Degree Elevation===\nThe concept of Degree Elevation can be repeated on a control polygon '''R''' to get a sequence of control polygons '''R''','''R'''<sub>1</sub>,'''R'''<sub>2</sub>, and so on. After ''r'' degree elevations, the polygon '''R'''<sub>r</sub> has the vertices '''P'''<sub>0,r</sub>,'''P'''<sub>1,r</sub>,'''P'''<sub>2,r</sub>,...,'''P'''<sub>n+r,r</sub> given by <ref name=\"farin1997\"/>\n\n:<math>\\mathbf{P}_{i,r} = \\sum_{j=0}^n \\mathbf{P}_j \\tbinom nj \\frac{\\tbinom{r}{i-j}}{\\tbinom{n+r}{i}}</math>\n\nIt can also be shown that for the underlying Bézier curve ''B'',\n\n:<math>\\mathbf{\\lim_{r \\to \\infty}R_r} = \\mathbf{B}</math>\n\n==Rational Bézier curves==\n[[Image:Rational Bezier curve-conic sections.svg|thumb|Segments of conic sections represented exactly by rational Bézier curves]]\nThe rational Bézier curve adds adjustable weights to provide closer approximations to arbitrary shapes. The numerator is a weighted Bernstein-form Bézier curve and the denominator is a weighted sum of [[Bernstein polynomial]]s. Rational Bézier curves can, among other uses, be used to represent segments of [[conic section]]s exactly, including circular arcs.<ref>{{cite web|url=http://www.cl.cam.ac.uk/teaching/2000/AGraphHCI/SMEG/node5.html|title=Some Mathematical Elements of Graphics: Rational B-splines|author=Neil Dodgson|date=2000-09-25|accessdate=2009-02-23}}</ref>\n\nGiven ''n'' + 1 control points '''P'''<sub>''i''</sub>, the rational Bézier curve can be described by:\n:<math>\n\\mathbf{B}(t) =\n\\frac{\n\\sum_{i=0}^n b_{i,n}(t) \\mathbf{P}_{i}w_i\n}\n{\n\\sum_{i=0}^n b_{i,n}(t) w_i\n}\n</math>\nor simply\n:<math>\n\\mathbf{B}(t) =\n\\frac{\n\\sum_{i=0}^n {n \\choose i} t^i (1-t)^{n-i}\\mathbf{P}_{i}w_i\n}\n{\n\\sum_{i=0}^n {n \\choose i} t^i (1-t)^{n-i}w_i\n}.\n</math>\n\n==Applications==\n\n===Computer graphics===\n[[Image:Bézier curve in Adobe Illustrator CS2.png|left|thumb|Bézier path in [[Adobe Illustrator]]]]\nBézier curves are widely used in computer graphics to model smooth curves. As the curve is completely contained in the [[convex hull]] of its [[Control point (mathematics)|control points]], the points can be graphically displayed and used to manipulate the curve intuitively. [[Affine transformation]]s such as [[translation (geometry)|translation]] and [[rotation]] can be applied on the curve by applying the respective transform on the control points of the curve.\n\n[[Quadratic function|Quadratic]] and [[Cubic function|cubic]] Bézier curves are most common. Higher degree curves are more [[computationally expensive]] to evaluate. When more complex shapes are needed, low order Bézier curves are patched together, producing a [[composite Bézier curve]]. A composite Bézier curve is commonly referred to as a \"path\" in [[vector graphics]] languages (like [[PostScript]]), vector graphics standards (like [[SVG]]) and vector graphics programs (like [[Artline (program)|Artline]], [[Timeworks Publisher]], [[Adobe Illustrator]], [[CorelDraw]] and [[Inkscape]]). To guarantee [[smoothness]] (''C''<sup>1</sup> continuity{{Clarify|date=September 2017}}), the control point at which two curves meet must be on the line between the two control points on either side.\n\nThe simplest method for scan converting ([[Rasterisation|rasterizing]]) a Bézier curve is to evaluate it at many closely spaced points and scan convert the approximating sequence of line segments. However, this does not guarantee that the rasterized output looks sufficiently smooth, because the points may be spaced too far apart. Conversely it may generate too many points in areas where the curve is close to linear. A common adaptive method is recursive subdivision, in which a curve's control points are checked to see if the curve approximates a straight line to within a small tolerance. If not, the curve is subdivided parametrically into two segments, 0 ≤ ''t'' ≤ 0.5 and 0.5 ≤ ''t'' ≤ 1, and the same procedure is applied recursively to each half. There are also forward differencing methods, but great care must be taken to analyse error propagation.{{citation needed|date=October 2015}}\n\nAnalytical methods where a Bézier is intersected with each scan line involve finding roots of cubic polynomials (for cubic Béziers) and dealing with multiple roots, so they are not often used in practice.<ref>{{cite web |title=Complex Quadratic Bézier Curve on Unit Circle |author1=Xuexiang Li |last-author-amp=yes |author2=Junxiao Xue |publisher=School of Software, Zhengzhou University |location=Zhengzhou, China |url=https://www.atlantis-press.com/php/download_paper.php?id=12403}}</ref>\n\n===Animation===\nIn animation applications, such as [[Adobe Flash]] and [[Synfig]], Bézier curves are used to outline, for example, movement. Users outline the wanted path in Bézier curves, and the application creates the needed frames for the object to move along the path.<ref>{{cite web |url=https://www.adobe.com/devnet/flash/learning_guide/animation/part07.html |title=Using motion paths in animations |website=Adobe |accessdate=2019-04-11}}</ref><ref>{{cite web |url=https://wiki.synfig.org/Doc:Following_a_Spline |title=Following a Spline |website=Synfig Wiki |accessdate=2019-04-11}}</ref>\n\nFor 3D animation Bézier curves are often used to define 3D paths as well as 2D curves for keyframe interpolation.{{citation needed|date=October 2014}}. Bézier curves are now very frequently used to control the animation easing in [[CSS]], [[JavaScript]] and [[JavaFx]].\n\n===Fonts===\n[[TrueType]] fonts use composite Bézier curves composed of '''quadratic''' Bézier curves. Other languages and imaging tools (such as [[PostScript]], [[Asymptote (vector graphics language)|Asymptote]], [[Metafont]], and [[SVG]]) use composite Béziers composed of '''cubic''' Bézier curves for drawing curved shapes. [[OpenType]] fonts can use either kind, depending on the flavor of the font.<ref>{{cite web |title=The difference between CFF and TTF |url=https://www.linotype.com/8120/the-difference-between-cff-and-ttf.html |archiveurl=https://web.archive.org/web/20170703232935/https://www.linotype.com/8120/the-difference-between-cff-and-ttf.html |archivedate=2017-07-03 |website=Know How |publisher=Linotype |accessdate=3 July 2018 |ref=CFF_TTF |quote=The OpenType format was formulated in 1996. By 2003, it began to replace two competing formats: the Type1 fonts, developed by Adobe and based on [P]ost[S]cript, and the TrueType fonts, specified by Microsoft and Apple. (...) TTF stands for TrueTypeFont and indicates that the font data is the same as in the TrueType fonts. CFF stands for the Type1 font format. Strictly speaking, it refers to the Compact Font Format, which is used in the compression processes for the Type2 fonts. (...) the cubic Bézier format of the Type1 fonts is more space-saving compared to the quadratic format of the TrueType fonts. Some kilobytes can be saved in large, elaborate fonts, which may represent an advantage on the Web. On the other hand, the more detailed hinting information of the TrueType fonts allows very extensive optimization for screen use.}}</ref>\n\nThe internal rendering of all Bézier curves in font or vector graphics renderers will split them recursively up to the point where the curve is flat enough to be drawn as a series of linear or circular segments. The exact splitting algorithm is implementation dependent, only the flatness criteria must be respected to reach the necessary precision and to avoid non-monotonic local changes of curvature. The \"smooth curve\" feature of charts in [[Microsoft Excel]] also uses this algorithm.<ref>{{cite web |url=http://www.xlrotor.com/resources/files.shtml |title=Smooth_curve_bezier_example_file.xls |website=Rotating Machinery Analysis, Inc. |accessdate=2011-02-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20110718131613/http://www.xlrotor.com/resources/files.shtml |archivedate=2011-07-18 |df= }}</ref>\n\nBecause arcs of circles and ellipses cannot be exactly represented by Bézier curves, they are first approximated by Bézier curves, which are in turn approximated by arcs of circles. This is inefficient as there exists also approximations of all Bézier curves using arcs of circles or ellipses, which can be rendered incrementally with arbitrary precision. Another approach, used by modern hardware graphics adapters with accelerated geometry, can convert exactly all Bézier and conic curves (or surfaces) into [[Non-uniform rational B-spline|NURBS]], that can be rendered incrementally without first splitting the curve recursively to reach the necessary flatness condition. This approach also allows preserving the curve definition under all linear or perspective 2D and 3D transforms and projections.{{citation needed|date=October 2015}}\n\nFont engines, like [[FreeType]], draw the font's curves (and lines) on a pixellated surface using a process known as [[font rasterization]].<ref name=freetype>{{cite web |title=FreeType Glyph Conventions / VI. FreeType outlines |website=The Free Type Project |date=13 Feb 2018 |url=http://www.freetype.org/freetype2/docs/glyphs/glyphs-6.html}}\n<br>{{cite web |title=FreeType Glyph Conventions – Version 2.1 / VI. FreeType outlines |date=6 Mar 2011 |url=http://www.freetype.org/freetype2/docs/glyphs/glyphs-6.html |archive-url=https://web.archive.org/web/20110929201958/http://www.freetype.org/freetype2/docs/glyphs/glyphs-6.html |dead-url=yes}}</ref>\n\n==See also==\n* [[Bézier surface]]\n* [[B-spline]]\n* [[GEM/4]]\n* [[Hermite curve]]\n* [[NURBS]]\n* [[String art]] – Bézier curves are also formed by many common forms of string art, where strings are looped across a frame of nails.<ref>{{cite book |last=Gross |first=R. |date=2014 |chapter=Bridges, String Art, and Bézier Curves |others=Penrose, R. (Author) & Pitici, M. (Ed.) |title=The Best Writing on Mathematics 2013 |journal=The Best Writing on Mathematics 2013 |pages=77–89 |publisher=Princeton University Press |jstor=j.ctt4cgb74.13}}</ref>\n* [[Variation diminishing property of Bézier curves]]\n\n==Notes==\n{{Reflist|group=\"note\"|1}}\n\n==References==\n{{Reflist}}\n* {{cite journal |author=Rida T. Farouki |url=http://mae.engr.ucdavis.edu/~farouki/bernstein.pdf |title=The Bernstein polynomial basis: A centennial retrospective |journal=Computer Aided Geometric Design |volume=29 |issue=6 |date=August 2012 |pages=379–419 |doi=10.1016/j.cagd.2012.03.001}}\n* {{cite book |author=Paul Bourke |title=Bézier Surfaces (in 3D) |url=http://local.wasp.uwa.edu.au/~pbourke/geometry/bezier/index.html |archive-url=https://web.archive.org/web/20090719011857/http://local.wasp.uwa.edu.au/~pbourke/geometry/bezier/index.html |dead-url=yes |archive-date=2009-07-19 |date=2009-07-19 }}\n* {{cite book |author=[[Donald Knuth]] |title=Metafont: the Program |publisher=Addison-Wesley |date=1986 |pages=123–131}} Excellent discussion of implementation details; available for free as part of the TeX distribution.\n* {{cite book |author=Thomas Sederberg |title=Bézier curves |url=http://www.tsplines.com/resources/class_notes/Bezier_curves.pdf |access-date=2005-09-14 |archive-url=https://web.archive.org/web/20060221000535/http://www.tsplines.com/resources/class_notes/Bezier_curves.pdf |archive-date=2006-02-21 |dead-url=yes |df= }}\n* {{cite book |author=J.D. Foley |display-authors=etal |title=Computer Graphics: Principles and Practice in C |edition=2nd |publisher=Addison Wesley |date=1992|title-link=Computer Graphics: Principles and Practice#Second Edition in C }}\n* {{cite web |author=Rajiv Chandel |url=http://codingg.blogspot.in/2014/03/implementing-bezier-curves-in-games.html |title=Implementing Bezier Curves in games|date=2014-03-20 }}\n\n==Computer code==\n*[https://github.com/msteinbeck/tinyspline TinySpline: Open source C-library for NURBS, B-Splines and Bezier Splines with bindings for various languages]\n\n==Further reading and external links==\n* [https://pomax.github.io/bezierinfo A Primer on Bézier Curves] &mdash; An open source online book explaining Bézier curves and associated graphics algorithms, with interactive graphics.\n* [http://vimeo.com/106757336 Cubic Bezier Curves - Under the Hood (video)] Video shows how computers render a cubic Bézier curve, by Peter Nowell\n* [http://www.ams.org/featurecolumn/archive/bezier.html#2 From Bézier to Bernstein] Feature Column from [[American Mathematical Society]]\n* {{springer|title=Bézier curve|id=p/b110460}}\n* {{cite book|author1=Hartmut Prautzsch|author2=Wolfgang Boehm|author3=Marco Paluszny|title=Bézier and B-Spline Techniques|year=2002|publisher=Springer Science & Business Media|isbn=978-3-540-43761-1}}\n* {{cite book|url=http://www.cis.upenn.edu/~jean/gbooks/geom1.html|title=Curves and Surfaces in Geometric Modeling: Theory and Algorithms|author=Jean Gallier|authorlink= Jean Gallier |publisher=Morgan Kaufmann|year=1999|at=Chapter 5. Polynomial Curves as Bézier Curves}} This book is out of print and freely available from the author.\n* {{cite book|author=Gerald E. Farin|title=Curves and Surfaces for CAGD: A Practical Guide|year=2002|publisher=Morgan Kaufmann|isbn=978-1-55860-737-8|edition=5th|url=http://www.farinhansford.com/books/cagd/}}\n* {{Mathworld|urlname=BezierCurve|title=Bézier Curve}}\n* Gernot Hoffmann [https://web.archive.org/web/20061202151511/http://www.fho-emden.de/~hoffmann/bezier18122002.pdf Bézier Curves] (pdf, 60 pages)\n* {{cite journal | doi = 10.1016/j.cam.2003.10.008 | title=Approximation of circular arcs and offset curves by Bézier curves of high degree | journal=Journal of Computational and Applied Mathematics | date=2004 | volume=167 | issue=2 | pages=405–416 | first=Young Joon | last=Ahn}}\n* Jason Davies [https://www.jasondavies.com/animated-bezier/ Animated Bézier curves]\n\n{{DEFAULTSORT:Bezier Curve}}\n[[Category:Graphic design]]\n[[Category:Interpolation]]\n[[Category:Curves]]"
    },
    {
      "title": "Birkhoff interpolation",
      "url": "https://en.wikipedia.org/wiki/Birkhoff_interpolation",
      "text": "{{confusing|date=December 2010}}\n\nIn [[mathematics]], '''Birkhoff interpolation''' is an extension of [[polynomial interpolation]]. It refers to the problem of finding a polynomial ''p'' of degree ''d'' such that certain [[derivative]]s have specified values at specified points:\n:<math> p^{(n_i)}(x_i) = y_i \\qquad\\mbox{for } i=1,\\ldots,d, </math>\nwhere the data points <math>(x_i,y_i)</math> and the nonnegative integers <math>n_i</math> are given. It differs from [[Hermite interpolation]] in that it is possible to specify derivatives of ''p'' at some points without specifying the lower derivatives or the polynomial itself. The name refers to [[George David Birkhoff]], who first studied the problem in {{harvtxt|Birkhoff|1906}}.\n\n==Existence and uniqueness of solutions==\nIn contrast to [[Lagrange interpolation]] and [[Hermite interpolation]], a Birkhoff interpolation problem does not always have a unique solution. For instance, there is no quadratic polynomial <math>\\textstyle p</math> such that <math>\\textstyle p(-1)=p(1)=0</math> and <math>\\textstyle p'(0)=1</math>. On the other hand, the Birkhoff interpolation problem where the values of <math>\\textstyle p'(-1)</math>, <math>\\textstyle p(0)</math> and <math>\\textstyle p'(1)</math> are given always has a unique solution {{harv|Passow|1983}}.\n\nAn important problem in the theory of Birkhoff interpolation is to classify those problems that have a unique solution. {{harvtxt|Schoenberg|1966}} formulates the problem as follows. Let ''d'' denote the number of conditions (as above) and let ''k'' be the number of interpolation points. Given a ''d''-by-''k'' matrix ''E'', all of whose entries are either 0 or 1, such that exactly ''d'' entries are 1, then the corresponding problem is to determine ''p'' such that\n:<math> p^{(j)}(x_i) = y_{i,j} \\qquad\\text{for all } (i,j) \\text{ with } e_{ij} = 1. </math>\nThe matrix ''E'' is called the incidence matrix. For example, the incidence matrices for the interpolation problems mentioned in the previous paragraph are:\n:<math> \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix} \\quad\\text{and}\\quad \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}. </math>\nNow the question is: does a Birkhoff interpolation problem with a given incidence matrix have a unique solution for any choice of the interpolation points?\n\nThe case with ''k'' = 2 interpolation points was tackled by {{harvtxt|Pólya|1931}}. Let ''S<sub>m</sub>'' denote the sum of the entries in the first ''m'' columns of the incidence matrix:\n:<math> S_m = \\sum_{i=1}^k \\sum_{j=1}^m e_{ij}. </math>\nThen the Birkhoff interpolation problem with ''k'' = 2 has a unique solution if and only if ''S<sub>m</sub>'' ≥ ''m'' for all ''m''. {{harvtxt|Schoenberg|1966}} showed that this is a necessary condition for all values of ''k''.\n\n==References==\n* {{Citation | last1=Birkhoff | first1=George David | author1-link=George David Birkhoff | title=General mean value and remainder theorems with applications to mechanical differentiation and quadrature | jstor=1986339 | year=1906 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=7 | pages=107–136 | issue=1 | publisher=American Mathematical Society | doi=10.2307/1986339}}.\n* {{Citation | last1=Passow | first1=Eli | title=Book Review: Birkhoff interpolation by G. G. Lorentz, K. Jetter and S. D. Riemenschneider  | doi=10.1090/S0273-0979-1983-15204-7 | year=1983 | journal=American Mathematical Society. Bulletin. New Series | issn=0002-9904 | volume=9 | issue=3 | pages=348–351}}.\n* {{Citation | doi=10.1002/zamm.19310110620 | last1=Pólya | first1=George | author1-link=George Pólya | title=Bemerkung zur Interpolation und zur Naherungstheorie der Balkenbiegung | year=1931 | journal=[[Journal of Applied Mathematics and Mechanics]] | issn=0044-2267 | volume=11 | pages=445–449}}.\n* {{Citation | last1=Schoenberg | first1=Isaac Jacob | author1-link=Isaac Jacob Schoenberg | title=On Hermite-Birkhoff interpolation | doi=10.1016/0022-247X(66)90160-0 | year=1966 | journal=Journal of Mathematical Analysis and Applications | issn=0022-247X | volume=16 | pages=538–543}}.\n\n[[Category:Interpolation]]"
    },
    {
      "title": "Brahmagupta's interpolation formula",
      "url": "https://en.wikipedia.org/wiki/Brahmagupta%27s_interpolation_formula",
      "text": "'''Brahmagupta's interpolation formula''' is a second-order polynomial [[interpolation formula]] developed by the [[India]]n [[mathematician]] and [[astronomer]] [[Brahmagupta]] (598–668 [[Common era|CE]]) in the early 7th century [[Common Era|CE]]. The [[Sanskrit]] couplet describing the formula can be found in the supplementary part of ''Khandakadyaka'' a work of [[Brahmagupta]] completed in 665 CE.<ref name=\"Gupta\">{{cite journal|last=Gupta|first=R. C. |title=Second-order interpolation in Indian mathematics upto the fifteenth century|journal=Indian Journal of History of Science|volume=4|issue=1 & 2|pages=86–98}}</ref> The same couplet appears in Brahmagupta's earlier ''Dhyana-graha-adhikara'', which was probably written \"near the beginning of the second quarter of the 7th century CE, if not earlier.\"<ref name=\"Gupta\"/> Brahmagupta was the one of the first to describe and use an [[interpolation formula]] using second-order [[finite difference|differences]].<ref>{{cite book|last=Van Brummelen|first=Glen|authorlink=Glen Van Brummelen|title=The mathematics of the heavens and the earth: the early history of trigonometry|year=2009|publisher=Princeton University Press|isbn=9780691129730|pages=329}} (p.111)</ref><ref>{{cite journal|last=Meijering|first=Erik|title=A Chronology of Interpolation From Ancient Astronomy to Modern Signal and Image Processing|journal=Proceedings of the IEEE|date=March 2002|volume=90|issue=3|pages=319–321|doi=10.1109/5.993400}}<!--|accessdate=27 October 2010--></ref>\n\nBrahmagupa's interpolation formula is equivalent to modern-day second-order Newton–Stirling [[interpolation formula]].\n\n==Preliminaries==\n\nGiven a set of tabulated values of a function {{math|''f''(''x'')}} in the table below, let it be required to compute the value of {{math|''f''(''a'')}}, {{math|''x''<sub>''r''</sub> < ''a'' < ''x''<sub>''r''+1</sub>}}.\n\n{| class=\"wikitable\"\n|-\n|&nbsp; {{math|''x''}} &nbsp; ||&nbsp; {{math|''x''<sub>1</sub>}}&nbsp; ||&nbsp; {{math|''x''<sub>2</sub>}}&nbsp; ||&nbsp; ... &nbsp;||&nbsp; {{math|''x''<sub>''r''</sub>}} &nbsp;||&nbsp; {{math|''x''<sub>''r''+1</sub>}}&nbsp; ||&nbsp; {{math|''x''<sub>''r''+2</sub>}}&nbsp; ||&nbsp; ... &nbsp;||&nbsp; {{math|''x''<sub>''n''</sub>}}&nbsp;\n|-\n|&nbsp; {{math|''f''(''x''<sub>''r''</sub>)}} || &nbsp; {{math|''f''<sub>1</sub>}} ||&nbsp; {{math|''f''<sub>2</sub>}} ||&nbsp; ... ||&nbsp; {{math|''f''<sub>''r''</sub>}} || &nbsp; {{math|''f''<sub>''r''+1</sub>}} || &nbsp; {{math|''f''<sub>''r''+2</sub>}} ||&nbsp; ... ||&nbsp; {{math|''f''<sub>''n''</sub>}}\n|}\n\nAssuming that the successively tabulated values of {{math|''x''}} are equally spaced with a common spacing of {{math|''h''}}, [[Aryabhata]] had considered the table of first differences of the table of values of a function. Writing\n\n: <math>D_r=f_{r+1}-f_r</math>\n\nthe following table can be formed:\n\n{| class=\"wikitable\"\n|-\n|&nbsp; {{math|''x''}}&nbsp; ||&nbsp; {{math|''x''<sub>2</sub>}}&nbsp; ||&nbsp; ... &nbsp; || &nbsp; {{math|''x''<sub>''r''</sub>}}&nbsp; || &nbsp; {{math|''x''<sub>''r''+1</sub>}}&nbsp; ||&nbsp; ... &nbsp; ||&nbsp; {{math|''x''<sub>''n''</sub>}}&nbsp;\n|-\n|&nbsp; Differences ||&nbsp; {{math|''D''<sub>1</sub>}} ||&nbsp; ... ||&nbsp; {{math|''D''<sub>''r''</sub>}} || &nbsp;{{math|''D''<sub>''r''+1</sub>}} || ... || &nbsp; {{math|''D''<sub>''n''</sub>}}\n|}\n\nMathematicians prior to Brahmagupta used a simple [[linear interpolation]] formula. The  linear interpolation formula to compute {{math|''f''(''a'')}} is\n\n: <math>f(a)=f_r+ t D_r</math> where <math>t=\\frac{a-x_r}{h}</math>.\n\nFor the computation of {{math|''f''(''a'')}},  Brahmagupta replaces {{math|''D''<sub>''r''</sub>}}  with another expression which gives more accurate values and which amounts to using a second-order interpolation formula.\n\n==Brahmagupta's description of the scheme==\n\nIn Brahmagupta's terminology the difference {{math|''D''<sub>''r''</sub>}} is the ''gatakhanda'', meaning ''past difference'' or the difference that was crossed over, the difference {{math|''D''<sub>''r''+1</sub>}} is the ''bhogyakhanda'' which is the ''difference yet to come''. ''Vikala'' is the amount in minutes by which the interval has been covered at the point where we want to interpolate. In the present notations it is {{math|''a'' − ''x''<sub>''r''</sub>}}. The new expression which replaces {{math|''f''<sub>''r''+1</sub> − ''f''<sub>''r''</sub>}} is called ''sphuta-bhogyakhanda''. The description of ''sphuta-bhogyakhanda'' is contained in the following Sanskrit couplet (''Dhyana-Graha-Upadesa-Adhyaya, 17; Khandaka Khadyaka, IX, 8''):<ref name=\"Gupta\"/>\n\n[[File:Brahmagupas Interpolation Formula In Devanagari.jpg]]{{clarify|date=January 2016|reason=This is an image of text and should be replaced by the text itself.|post-text=(text needed)}}\n\nThis has been translated using Bhattolpala's (10th century CE) commentary as follows:<ref name=\"Gupta\"/><ref name=\"Raju\">{{cite book|last=Raju |first=C K|title=Cultural foundations of mathematics: the nature of mathematical proof and the transmission of the calculus from India to Europe in the 16th c. CE|year=2007|publisher=Pearson Education India|isbn=9788131708712|pages=138–140}}</ref>\n\n:Multiply the ''vikala'' by the half the difference of the ''gatakhanda'' and the ''bhogyakhanda'' and divide the product by 900. Add the result to half the sum of the ''gatakhanda'' and the ''bhogyakhanda'' if their half-sum is less than the ''bhogyakhanda'', subtract if greater. (The result in each case is ''sphuta-bhogyakhanda'' the correct tabular difference.)\n\nThis formula was originally stated for the computation of the values of the sine function for which the common interval in the underlying base table was 900 minutes or 15 degrees. So the reference to 900 is in fact a reference to the common interval {{math|''h''}}.\n\n==In modern notation==\nBrahmagupta's method computation of ''shutabhogyakhanda'' can be formulated in modern notation as follows:\n\n:''sphuta-bhogyakhanda'' <math>\\displaystyle = \\frac{D_r + D_{r-1}}{2} \\pm t\\frac{|D_r - D_{r-1}|}{2}.</math>\n\nThe [[plus or minus|±]] sign is to be taken according to whether {{math|{{sfrac|1|2}}(''D''<sub>''r''</sub> + ''D''<sub>''r''+1</sub>)}} is less than or greater than {{math|''D''<sub>''r''+1</sub>}}, or equivalently, according to whether {{math|''D''<sub>''r''</sub> < ''D''<sub>''r''+1</sub>}} or {{math|''D''<sub>''r''</sub> > ''D''<sub>''r''+1</sub>}}. Brahmagupta's expression can be put in the following form:\n\n:''sphuta-bhogyakhanda'' <math> \\displaystyle = \\frac{D_r + D_{r-1}}{2} + t\\frac{D_r-D_{r-1}}{2}.</math>\n\nThis correction factor yields the following approximate value for {{math|''f''(''a'')}}:\n\n: <math> \n\\begin{align}\nf(a) & = f_r + t\\times\\text{sphuta-bhogyakhanda}\\\\\n&  = f_r + t \\frac{D_r + D_{r-1}}{2} + t^2\\frac{D_r - D_{r-1}}{2}.\n\\end{align}\n</math>\n\nThis is Stirling's [[interpolation formula]] truncated at the second-order differences.<ref>{{cite book|last=Milne-Thomson|first=Louis Melville|title=The Calculus of Finite Differences|year=2000|publisher=AMS Chelsea Publishing|isbn=9780821821077|pages=67–68}}</ref><ref>{{cite book|last=Hildebrand|first=Francis Begnaud|title=Introduction to numerical analysis|year=1987|publisher=Courier Dover Publications|isbn=9780486653631|pages=138–139}}</ref> It is not known how Brahmagupta arrived at his interpolation formula.<ref name=\"Gupta\"/> Brahmagupta has given a separate formula for the case where the values of the independent variable are not equally spaced.\n\n==See also==\n* [[Brahmagupta's identity]]\n* [[Brahmagupta matrix]]\n* [[Brahmagupta–Fibonacci identity]]\n\n==References==\n{{reflist}}\n\n[[Category:Interpolation]]\n[[Category:Indian mathematics]]\n[[Category:History of mathematics]]"
    },
    {
      "title": "Cubic Hermite spline",
      "url": "https://en.wikipedia.org/wiki/Cubic_Hermite_spline",
      "text": "{{not to be confused|Hermite polynomial}}\n\nIn [[numerical analysis]], a '''cubic Hermite spline''' or '''cubic Hermite interpolator''' is a [[spline (mathematics)|spline]] where each piece is a third-degree [[polynomial]] specified in [[Hermite interpolation|Hermite form]]:<ref name=kreyszig>\n{{cite book\n | title = Advanced Engineering Mathematics\n | author = Erwin Kreyszig\n | publisher = Wiley\n | year = 2005\n | isbn =  9780471488859\n | pages = 816\n | edition = 9\n }}</ref> i.e., by its values and first [[derivative (mathematics)|derivatives]] at the end points of the corresponding [[domain (mathematics)|domain]] interval.\n\nCubic Hermite splines are typically used for [[interpolation]] of numeric data specified at given argument values <math>x_1,x_2,\\ldots,x_n</math>, to obtain a smooth [[continuous function]].  The data should consist of the desired function value and derivative at each <math>x_k</math>. (If only the values are provided, the derivatives must be estimated from them.)  The Hermite formula is applied to each interval <math>(x_k, x_{k+1})</math> separately. The resulting spline will be continuous and will have continuous first derivative.\n\nCubic polynomial splines can be specified in other ways, the [[Bézier form]] being the most common.  However, these two methods provide the same set of splines, and data can be easily converted between the Bézier and Hermite forms; so the names are often used as if they were synonymous.  \n\nCubic polynomial splines are extensively used in [[computer graphics]] and [[geometric modeling]] to obtain [[curve]]s or motion [[trajectory|trajectories]] that pass through specified points of the [[plane (geometry)|plane]] or three-dimensional [[space (geometry)|space]].  In these applications, each coordinate of the plane or space is separately interpolated by a cubic spline function of a separate parameter&nbsp;''t''. \nCubic polynomial splines are also used extensively in structural analysis applications, such as [[Euler–Bernoulli beam theory]].\n\nCubic splines can be extended to functions of two or more parameters, in several ways.  Bicubic splines ([[Bicubic interpolation]]) are often used to interpolate data on a regular rectangular grid, such as [[pixel]] values in a [[digital image]] or [[altitude]] data on a terrain.  [[Bézier patch|Bicubic surface patches]], defined by three bicubic splines, are an essential tool in computer graphics.\n\nCubic splines are often called '''csplines''', especially in computer graphics. Hermite splines are named after [[Charles Hermite]].\n\n==Interpolation on a single interval==\n===Unit interval (0, 1)===\nOn the unit interval <math>(0,1)</math>, given a starting point <math>\\boldsymbol{p}_0</math> at <math>t=0</math> and an ending point <math>\\boldsymbol{p}_1</math> at <math>t=1</math> with starting tangent <math>\\boldsymbol{m}_0</math> at <math>t=0</math> and ending tangent <math>\\boldsymbol{m}_1</math> at <math>t=1</math>, the polynomial can be defined by\n:<math>\\boldsymbol{p}(t) = (2t^3-3t^2+1)\\boldsymbol{p}_0 + (t^3-2t^2+t)\\boldsymbol{m}_0 + (-2t^3+3t^2)\\boldsymbol{p}_1 +(t^3-t^2)\\boldsymbol{m}_1</math>\n[[File:HermiteBasis.svg|thumb|300px|right|The four Hermite basis functions. The interpolant in each subinterval is a linear combination of these four functions.]]\nwhere ''t'' ∈ [0, 1].\n\n===Interpolation on an arbitrary interval===\nInterpolating <math>x</math> in an arbitrary interval <math>(x_k, x_{k+1})</math> is done by mapping the latter to <math>[0,1]</math> through an [[affine function|affine]] (degree 1) change of variable.  The formula is\n:<math>\\boldsymbol{p}(x) = h_{00}(t)\\boldsymbol{p}_{k} + h_{10}(t)(x_{k+1}-x_k)\\boldsymbol{m}_{k} + h_{01}(t)\\boldsymbol{p}_{k+1} + h_{11}(t)(x_{k+1}-x_k)\\boldsymbol{m}_{k+1}.</math>\nwith <math>t = (x-x_k)/(x_{k+1}-x_k)</math> and <math>h</math> refers to the basis functions, defined below. Note that the tangent values have been scaled by <math>x_{k+1}-x_k</math> compared to the equation on the unit interval.\n\n===Uniqueness===\nThe formulae specified above provide the unique third-degree polynomial path between the two points with the given tangents. <br />\n\n'''Proof.''' Let <math>P, Q</math> be two third degree polynomials satisfying the given boundary conditions. Define <math>R = Q-P,</math> then:\n:<math>R(0) = Q(0)-P(0) = 0,</math>\n:<math>R(1) = Q(1) - P(1) = 0.</math>\nSince both <math>Q</math> and <math>P</math> are third degree polynomials, <math>R</math> is at most a third degree polynomial. So <math>R</math> must be of the form:\n:<math>R(x) = ax(x-1)(x-r),</math>\ncalculating the derivative gives:\n:<math>R'(x) = ax(x-1) + ax(x-r) + a(x-1)(x-r).</math>\n\nWe know furthermore that:\n\n:<math>R'(0) = Q'(0) - P'(0) = 0</math>\n{{NumBlk|:|<math>R'(0) = 0 = ar</math>|{{EquationRef|1}}}}\n\n:<math>R'(1) = Q'(1) - P'(1) = 0</math>\n{{NumBlk|:|<math>R'(1) = 0 = a(1-r)</math>|{{EquationRef|2}}}}\n\nPutting ({{EquationNote|1}}) and ({{EquationNote|2}}) together, we deduce that <math>a = 0</math> and therefore <math>R = 0,</math> thus <math>P = Q.</math>\n\n===Representations===\n\nWe can write the interpolation polynomial as\n:<math>\\boldsymbol{p}(t) = h_{00}(t)\\boldsymbol{p}_0 + h_{10}(t)\\boldsymbol{m}_0 + h_{01}(t)\\boldsymbol{p}_1 + h_{11}(t)\\boldsymbol{m}_1</math>\nwhere <math>h_{00}</math>, <math>h_{10}</math>, <math>h_{01}</math>, <math>h_{11}</math> are Hermite basis functions.\nThese can be written in different ways, each way revealing different properties.\n\n{| class=\"wikitable\" border=\"1\"\n |-\n !\n !  expanded\n !  factorized\n !  Bernstein\n |-\n |  <math>h_{00}(t)</math>\n |  <math>2t^3-3t^2+1</math>\n |  <math>(1 + 2 t) ( 1 - t)^2</math>\n |  <math>B_0(t) + B_1(t)</math>\n |-\n |  <math>h_{10}(t)</math>\n |  <math>t^3-2t^2+t</math>\n |  <math>t (1 - t)^2</math>\n |  <math>\\frac{1}{3}\\cdot B_1(t)</math>\n |-\n |  <math>h_{01}(t)</math>\n |  <math>-2t^3+3t^2</math>\n |  <math>t^2 (3 - 2 t)</math>\n |  <math>B_3(t) + B_2(t)</math>\n |-\n |  <math>h_{11}(t)</math>\n |  <math>t^3-t^2</math>\n |  <math>t^2 (t - 1)</math>\n |  <math>-\\frac{1}{3}\\cdot B_2(t)</math>\n |}\n\nThe \"expanded\" column shows the representation used in the definition above.\nThe \"factorized\" column shows immediately, that <math>h_{10}</math> and <math>h_{11}</math> are zero at the boundaries.\nYou can further conclude\nthat <math>h_{01}</math> and <math>h_{11}</math> have a [[Multiplicity_(mathematics)#Multiplicity_of_a_zero_of_a_function|zero of multiplicity 2]] at 0\nand <math>h_{00}</math> and <math>h_{10}</math> have such a zero at 1,\nthus they have slope 0 at those boundaries.\nThe \"Bernstein\" column shows the decomposition of the Hermite basis functions into [[Bernstein polynomial]]s of order 3:\n\n:<math>B_k(t)={3 \\choose k} \\cdot t^k \\cdot (1-t)^{3-k}</math>\n\nUsing this connection you can express cubic Hermite interpolation in terms of cubic [[Bézier curve]]s\nwith respect to the four values <math>\\boldsymbol{p}_0, \\boldsymbol{p}_0 + \\frac{\\boldsymbol{m}_0}{3}, \\boldsymbol{p}_1 - \\frac{\\boldsymbol{m}_1}{3}, \\boldsymbol{p}_1</math>\nand do Hermite interpolation using the [[de Casteljau algorithm]].\nIt shows that in a cubic Bézier patch the two control points in the middle\ndetermine the tangents of the interpolation curve at the respective outer points.\n\n==Interpolating a data set==\nA data set, <math>(t_k,\\boldsymbol{p}_k)</math> for <math>k=1,\\ldots,n</math>, can be interpolated by applying the above procedure on each interval, where the tangents are chosen in a sensible manner, meaning that the tangents for intervals sharing endpoints are equal. The interpolated curve then consists of piecewise cubic Hermite splines, and is globally continuously differentiable in <math>(t_1,t_n)</math>.\n\nThe choice of tangents is non-unique, and there are several options available.\n\n===Finite difference===\n[[File:Finite difference spline example.png|thumb|Example with finite difference tangents]]\nThe simplest choice is the three-point difference, not requiring constant interval lengths,<!-- See talk page -->\n:<math>\\boldsymbol{m}_k = \\frac{1}{2}\\left(\\frac{\\boldsymbol{p}_{k+1}-\\boldsymbol{p}_{k}}{t_{k+1}-t_{k}} + \\frac{\\boldsymbol{p}_{k}-\\boldsymbol{p}_{k-1}}{t_{k}-t_{k-1}}\\right)</math>\nfor internal points <math>k=2,\\ldots,n-1</math>, and one-sided difference at the endpoints of the data set.\n\n=== Cardinal spline === <!-- Redirect \"Cardinal spline\" points directly to this section -->\n\n[[File:Cardinal Spline Example.png|thumb|right|Cardinal spline example in 2D. The line represents the curve, and the squares represent the control points <math>\\boldsymbol{p}_k</math>. Notice that the curve does not reach the first and last points; these points do however affect the shape of the curve. The tension parameter used is 0.1]]\nA '''cardinal spline''', sometimes called a '''canonical spline''',<ref>{{cite web |last=Petzold |first=Charles |author-link=Charles Petzold |url=http://www.charlespetzold.com/blog/2009/01/Canonical-Splines-in-WPF-and-Silverlight.html |title=Canonical Splines in WPF and Silverlight |date=2009}}</ref> is obtained<ref>{{cite web |url=http://msdn2.microsoft.com/en-us/library/ms536358.aspx |title=Cardinal Splines |website=Microsoft Developer Network |access-date=2018-05-27}}</ref> if\n:<math> \\boldsymbol{m}_k = (1-c)\\frac{\\boldsymbol{p}_{k+1}-\\boldsymbol{p}_{k-1}}{t_{k+1}-t_{k-1}}</math>\nis used to calculate the tangents. The parameter {{mvar|c}} is a ''tension'' parameter that must be in the interval {{math|[0,1]}}. In some sense, this can be interpreted as the \"length\" of the tangent. Choosing {{math|1=''c''=1}} yields all zero tangents, and choosing {{math|1=''c''=0}} yields a Catmull–Rom spline.\n\n=== Catmull–Rom spline === <!-- Redirect \"Catmull-Rom spline\" points directly to this section -->\n\n{{seealso|Centripetal Catmull–Rom spline}}\n\nFor tangents chosen to be\n:<math>\\boldsymbol{m}_k = \\frac{\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k-1}}{t_{k+1} - t_{k-1}}</math>\na '''Catmull–Rom spline''' is obtained, being a special case of a cardinal spline. This assumes uniform parameter spacing.\n\nThe curve is named after [[Edwin Catmull]] and [[Raphael Rom]].  The principal advantage of this technique is that the points along the original set of points also make up the control points for the spline curve.<ref>{{citation |last1=Catmull |first1=Edwin |author1link=Edwin Catmull  |last2=Rom |first2=Raphael |author2link=Raphael Rom |chapter=A class of local interpolating splines |editor1-first=R. E. |editor1-last=Barnhill |editor2-first=R. F. |editor2-last=Riesenfeld |title=Computer Aided Geometric Design |publisher=Academic Press |location=New York |year=1974 |pages=317–326}}</ref>  Two additional points are required on either end of the curve.  The default implementation of the Catmull–Rom algorithm can produce loops and self intersections.  The chordal and [[centripetal Catmull–Rom spline|centripetal Catmull–Rom]] implementations <ref>N. Dyn, M. S. Floater, and K. Hormann. Four-point curve subdivision based on iterated chordal and centripetal parameterizations. Computer Aided Geometric Design, 26(3):279{286, 2009</ref> solve this problem, but use a slightly different calculation.<ref> P. J. Barry and R. N. Goldman. A recursive evaluation algorithm for a class of Catmull-Rom splines. SIGGRAPH Computer Graphics, 22(4):199{204, 1988.</ref> In [[computer graphics]], Catmull–Rom splines are frequently used to get smooth interpolated motion between [[key frame]]s. For example, most camera path animations generated from discrete key-frames are handled using Catmull–Rom splines. They are popular mainly for being relatively easy to compute, guaranteeing that each key frame position will be hit exactly, and also guaranteeing that the tangents of the generated curve are continuous over multiple segments.\n\n===Kochanek–Bartels spline===\n{{main|Kochanek–Bartels spline}}\nA Kochanek–Bartels spline is a further generalization on how to choose the tangents given the data points <math>\\boldsymbol{p}_{k-1}</math>, <math>\\boldsymbol{p}_k</math> and <math>\\boldsymbol{p}_{k+1}</math>, with three parameters possible, tension, bias and a continuity parameter.\n\n===Monotone cubic interpolation===\n{{main|Monotone cubic interpolation}}\nIf a cubic Hermite spline of any of the above listed types is used for [[interpolation]] of a [[Monotonic function|monotonic]] data set, the interpolated function will not necessarily be monotonic, but monotonicity can be preserved by adjusting the tangents.\n\n==Interpolation on the unit interval with matched derivatives at endpoints==\nConsidering a single coordinate of the points <math>\\boldsymbol{p}_{n-1}, \\boldsymbol{p}_{n}, \\boldsymbol{p}_{n+1}</math> and <math>\\boldsymbol{p}_{n+2}</math> as the values that a function, ''f''(''x''), takes at integer ordinates ''x''=''n''−1, ''n'', ''n''+1 and ''n''+2,\n\n:<math> p_n = f(n) \\quad \\forall n \\in \\mathbb{Z} </math>\n\nIf, in addition, the tangents at the endpoints are defined as the centered differences of the adjacent points,\n\n:<math> m_n = \\frac{f(n+1)-f(n-1)}{2} = \\frac{p_{n+1}-p_{n-1}}{2} \\quad \\forall n \\in \\mathbb{Z} </math>\n\nTo evaluate the interpolated ''f''(''x'') for a real ''x'', first separate ''x'' into the integer portion, ''n'', and fractional portion, ''u''\n\n:<math> x = n + u </math>\n:<math> n = \\lfloor x \\rfloor  =  \\operatorname{floor}(x) </math>\n:<math> u = x - n = x - \\lfloor x \\rfloor </math>\n:<math> 0 \\le u < 1 </math>\n\nThen the Catmull–Rom spline is <ref>[https://arxiv.org/abs/0905.3564 Two hierarchies of spline interpolations. Practical algorithms for multivariate higher order splines]</ref>  \n\n:<math>\\begin{align}\nf(x) = f(n+u) &= \\mathrm{CINT}_u \\left (p_{n-1}, p_n, p_{n+1}, p_{n+2} \\right ) \\\\[8pt]\n& = \\begin{bmatrix} 1 & u & u^2 & u^3 \\\\ \\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n-\\tfrac12 & 0 & \\tfrac12 & 0 \\\\\n1 & -\\tfrac52 & 2 & -\\tfrac12 \\\\\n-\\tfrac12 & \\tfrac32 & -\\tfrac32 & \\tfrac12 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\np_{n-1} \\\\\np_n \\\\\np_{n+1} \\\\\np_{n+2} \\\\\n\\end{bmatrix} \\\\\n\\\\\n& =  \\frac 12 \\begin{bmatrix} -u^3 +2u^2 - u \\\\ 3u^3 - 5u^2 + 2 \\\\ -3u^3 + 4u^2 + u \\\\ u^3 - u^2 \\end{bmatrix}^\\mathrm{T} \\cdot \\begin{bmatrix} p_{n-1}\\\\p_n\\\\p_{n+1}\\\\p_{n+2} \\end{bmatrix} \\\\\n\\\\\n& = \\frac 12 \\begin{bmatrix} u((2-u)u - 1) \\\\ u^2(3u-5) + 2 \\\\ u((4-3u)u + 1) \\\\ u^2(u-1) \\end{bmatrix}^\\mathrm{T} \\cdot \\begin{bmatrix}  p_{n-1}\\\\p_n\\\\p_{n+1}\\\\p_{n+2} \\end{bmatrix} \\\\\n\\\\ \n& = \\tfrac12 \\bigg( \\big(u^2(2-u)-u\\big)p_{n-1} \\ + \\ \\big(u^2(3u-5)+2\\big)p_n + \\ \\big(u^2(4-3u)+u\\big)p_{n+1} \\ + \\ u^2(u-1)p_{n+2} \\bigg) \\\\\n\\\\ \n& = \\tfrac12 \\bigg( (-u^3 +2u^2 - u )p_{n-1} \\ + \\ (3u^3 - 5u^2 + 2)p_n + \\ (-3u^3 + 4u^2 + u)p_{n+1} \\ + \\ (u^3 - u^2)p_{n+2} \\bigg) \\\\\n\\\\ \n& = \\tfrac12 \\bigg( (-p_{n-1}+3p_n-3p_{n+1}+p_{n+2})u^3  \\ + \\ (2p_{n-1}-5p_n+4p_{n+1}-p_{n+2})u^2 \\ + \\ (-p_{n-1}+p_{n+1})u \\ + \\ 2p_n \\bigg) \\\\\n\\\\ \n& = \\tfrac12 \\bigg( \\Big((-p_{n-1}+3p_n-3p_{n+1}+p_{n+2})u  \\ + \\ (2p_{n-1}-5p_n+4p_{n+1}-p_{n+2})\\Big)u \\ + \\ (-p_{n-1}+p_{n+1}) \\bigg)u \\ + \\ p_n  \\\\\n\\end{align}</math>\n\n<math>\\lfloor x \\rfloor</math> denotes the [[floor function]] which returns the largest integer no larger than ''x'' and <math>\\mathrm{T}</math> denotes the [[matrix transpose]].  The bottom equality is depicting the application of [[Horner's method]].\n\nThis writing is relevant for [[tricubic interpolation]], where one optimization requires you to compute CINT<sub>''u''</sub> sixteen times with the same ''u'' and different ''p''.\n\n==See also==\n* [[Bicubic interpolation]], a generalization to two dimensions\n* [[Tricubic interpolation]], a generalization to three dimensions\n* [[Hermite interpolation]]\n* [[Multivariate interpolation]]\n* [[Spline interpolation]]\n* [[Discrete spline interpolation]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.cs.clemson.edu/~dhouse/courses/405/notes/splines.pdf Spline Curves], Prof. Donald H. House [[Clemson University]]\n* [http://cvcweb.ices.utexas.edu/ccv/papers/1993/conference/multidim.pdf Multi-dimensional Hermite Interpolation and Approximation], Prof. Chandrajit Bajaj, [[Purdue University]]\n* [http://www.mvps.org/directx/articles/catmull/ Introduction to Catmull–Rom Splines], MVPs.org\n* [http://www.ibiblio.org/e-notes/Splines/Cardinal.htm Interpolating Cardinal and Catmull–Rom splines]\n* [http://paulbourke.net/miscellaneous/interpolation/ Interpolation methods: linear, cosine, cubic and hermite (with C sources)]\n* [http://www.blackpawn.com/texts/splines/ Common Spline Equations ]\n\n{{DEFAULTSORT:Cubic Hermite Spline}}\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]"
    },
    {
      "title": "Curve-fitting compaction",
      "url": "https://en.wikipedia.org/wiki/Curve-fitting_compaction",
      "text": "{{no footnotes|date=January 2013}}\n'''Curve-fitting compaction''' is  [[data compaction]] accomplished by replacing data to be stored or transmitted with an [[analytical expression]].\n\nExamples of curve-fitting compaction consisting of [[discretization]] and then [[interpolation]] are:\n* Breaking of a continuous curve into a series of straight line segments and specifying the [[slope]], [[Y-intercept|intercept]], and range for each segment  \n* Using a mathematical [[Expression (mathematics)|expression]], such as a [[polynomial]] or a [[trigonometric function]], and a single point on the corresponding curve instead of storing or transmitting the entire graphic curve or a series of points on it.\n\n==References==\n{{FS1037C}}\n\n[[Category:Curves]]\n[[Category:Interpolation]]\n[[Category:Data compression]]\n\n\n{{compu-stub}}"
    },
    {
      "title": "Discrete spline interpolation",
      "url": "https://en.wikipedia.org/wiki/Discrete_spline_interpolation",
      "text": "In the mathematical field of [[numerical analysis]], '''discrete spline interpolation''' is a form of [[interpolation]] where the [[interpolant]] is a special type of [[piecewise]] [[polynomial]] called a discrete spline. A discrete spline is a piecewise polynomial such that its [[central difference]]s are [[Continuous function|continuous]] at the knots whereas a [[Spline (mathematics)|spline]] is a piecewise polynomial such that its [[derivative]]s are continuous at the knots. Discrete cubic splines are discrete splines where the central differences of orders 0, 1, and 2 are required to be continuous.<ref name=Tom>{{cite journal|last1=Tom Lyche|title=Discrete Cubic Spline Interpolation|journal=BIT|date=1979|volume=16|issue=3|pages=281–290|doi=10.1007/bf01932270}}</ref>\n\nDiscrete splines were introduced by Mangasarin and Schumaker in 1971 as solutions of certain minimization problems involving differences.<ref name=Mangasarin>{{cite journal|author1=Mangasarian, O. L.  |author2=Schumaker, L. L.|title=Discrete splines via mathematical programming|journal=SIAM J. Control|date=1971|volume=9|issue=2|pages=174–183|doi=10.1137/0309015}}</ref>\n\n==Discrete cubic splines==\nLet ''x''<sub>1</sub>, ''x''<sub>2</sub>, . . ., ''x''<sub>''n''-1</sub> be an increasing sequence of real numbers. Let ''g''(''x'') be a piecewise polynomial defined by\n\n:<math>\ng(x)=\n\\begin{cases}\ng_1(x) & x<x_1 \\\\\ng_i(x) & x_{i-1}\\le x < x_i \\text{ for } i = 2,3, \\ldots, n-1\\\\\ng_n(x) & x\\ge x_{n-1}\n\\end{cases}\n</math>\n\nwhere ''g''<sub>1</sub>(''x''), . . ., ''g''<sub>''n''</sub>(''x'') are polynomials of degree 3. Let ''h'' > 0. If\n\n:<math>\n(g_{i+1}-g_i)(x_i +jh)=0 \\text{ for } j=-1,0,1 \\text{ and } i=1,2,\\ldots, n-1\n</math>\n\nthen ''g''(''x'') is called a discrete cubic spline.<ref name=Tom/>\n\n===Alternative formulation 1===\nThe conditions defining a discrete cubic spline are equivalent to the following:\n\n:<math> g_{i+1}(x_i-h) = g_i(x_i-h)</math>\n\n:<math> g_{i+1}(x_i) = g_i(x_i)</math>\n\n:<math> g_{i+1}(x_i+h) = g_i(x_i+h)</math>\n\n===Alternative formulation 2===\nThe central differences of orders 0, 1, and 2 of a function ''f''(''x'') are defined as follows:\n\n:<math>D^{(0)}f(x) = f(x) </math>\n\n:<math>D^{(1)}f(x)=\\frac{f(x+h)-f(x-h)}{2h}</math>\n\n:<math>D^{(2)}f(x)=\\frac{f(x+h)-2f(x)+f(x-h)}{h^2}</math>\n\nThe conditions defining a discrete cubic spline are also equivalent to<ref name=Tom/>\n\n:<math>D^{(j)}g_{i+1}(x_i)=D^{(j)}g_i(x_i) \\text{ for } j=0,1,2 \\text{ and } i=1,2, \\ldots, n-1.</math>\n\nThis states that the central differences <math>D^{(j)}g(x)</math> are continuous at ''x''<sub>''i''</sub>.\n\n===Example===\nLet ''x''<sub>1</sub> = 1 and ''x''<sub>2</sub> = 2 so that ''n'' = 3. The following function defines a discrete cubic spline:<ref name=Tom/>\n\n<math>\ng(x) =\n\\begin{cases}\nx^3 & x<1 \\\\\nx^3 - 2(x-1)((x-1)^2-h^2) & 1\\le x < 2\\\\\nx^3 - 2(x-1)((x-1)^2-h^2)+(x-2)((x-2)^2-h^2) & x \\ge 2\n\\end{cases}\n</math>\n\n==Discrete cubic spline interpolant==\n\nLet ''x''<sub>0</sub> < ''x''<sub>1</sub> and ''x''<sub>''n''</sub> > ''x''<sub>''n''-1</sub> and ''f''(''x'') be a function defined in the closed interval [''x''<sub>0</sub> - h, ''x''<sub>''n<sub>''</sub></sub> + h]. Then there is a unique cubic discrete spline ''g''(''x'') satisfying the following conditions:\n\n:<math>g(x_i) = f(x_i) \\text{ for } i=0,1,\\ldots, n.</math>\n:<math>D^{(1)}g_1(x_0) = D^{(1)}f(x_0).</math>\n:<math>D^{(1)}g_n(x_n) = D^{(1)}f(x_n).</math>\n\nThis  unique discrete cubic spline is the discrete spline interpolant to ''f''(''x'') in the interval [''x''<sub>0</sub> - h, ''x''<sub>''n''</sub> + h]. This interpolant agrees with the values of ''f''(''x'') at ''x''<sub>0</sub>, ''x''<sub>1</sub>, . . ., ''x''<sub>n</sub>.\n\n==Applications==\n* Discrete cubic splines were originally introduced as solutions of certain minimization problems.<ref name=Tom/><ref name= Mangasarin/>\n* They have applications in computing nonlinear splines.<ref name=Tom/><ref>{{cite journal|last1=Michael A. Malcolm|title=On the computation of nonlinear spline functions|journal=SIAM Journal on Numerical Analysis|date=April 1977|volume=14|issue=2|pages=254–282|doi=10.1137/0714017}}</ref>\n* They are used to obtain approximate solution of a second order boundary value problem.<ref>{{cite journal|last1=Fengmin Chen, Wong, P.J.Y.|title=Solving second order boundary value problems by discrete cubic splines|journal=Control Automation Robotics & Vision (ICARCV), 2012 12th International Conference|date=Dec 2012|pages=1800–1805}}</ref>\n* Discrete interpolatory splines have been used to construct biorthogonal wavelets.<ref>{{cite journal|last1=Averbuch, A.Z., Pevnyi, A.B., Zheludev, V.A.|title=Biorthogonal Butterworth wavelets derived from discrete interpolatory splines|journal=IEEE Transactions on Signal Processing|date=Nov 2001|volume=49|issue=11|pages=2682–2692|doi=10.1109/78.960415|citeseerx=10.1.1.332.7428}}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Interpolation]]\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "Extrapolation",
      "url": "https://en.wikipedia.org/wiki/Extrapolation",
      "text": "{{about||the journal of speculative fiction|Extrapolation (journal)|the John McLaughlin album|Extrapolation (album)}}\n\nIn [[mathematics]], '''extrapolation''' is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to [[interpolation]], which produces estimates between known observations, but extrapolation is subject to greater [[uncertainty]] and a higher risk of producing meaningless results.  Extrapolation may also mean extension of a [[wikt:method|method]], assuming similar methods will be applicable. Extrapolation may also apply to human [[experience]] to project, extend, or expand known experience into an area not known or previously experienced so as to arrive at a (usually conjectural) knowledge of the unknown <ref name=\"merrian-webster\">[http://www.merriam-webster.com/dictionary/extrapolation Extrapolation], entry at [[Webster's Dictionary|Merriam–Webster]]</ref> (e.g. a driver extrapolates road conditions beyond his sight while driving). The extrapolation method can be applied in the [[interior reconstruction]] problem.\n\n[[Image:Extrapolation example.svg|thumb|right|Example illustration of the extrapolation problem, consisting of assigning a meaningful value at the blue box, at <math>x=7</math>, given the red data points.]]\n\n==Methods==\n\nA sound choice of which extrapolation method to apply relies on ''a prior knowledge'' of the process that created the existing data points. Some experts have proposed the use of causal forces in the evaluation of extrapolation methods.<ref>{{cite journal|url=http://marketing.wharton.upenn.edu/ideas/pdf/armstrong2/causal.pdf | title = Causal Forces: Structuring Knowledge for Time-series Extrapolation |author1=J. Scott Armstrong |author2=Fred Collopy | journal = Journal of Forecasting | volume = 12 | issue = 2 | pages = 103–115 | year = 1993 | doi=10.1002/for.3980120205| citeseerx = 10.1.1.42.40 }}</ref> Crucial questions are, for example, if the data can be assumed to be continuous, smooth, possibly periodic etc.\n\n===Linear===\n\nLinear extrapolation means creating a tangent line at the end of the known data and extending it beyond that limit. Linear extrapolation will only provide good results when used to extend the graph of an approximately linear function or not too far beyond the known data.\n\nIf the two data points nearest the point <math>x_*</math> to be extrapolated are <math>(x_{k-1},y_{k-1})</math> and <math>(x_k, y_k)</math>, linear extrapolation gives the function:\n\n:<math>y(x_*) = y_{k-1} + \\frac{x_* - x_{k-1}}{x_{k}-x_{k-1}}(y_{k} - y_{k-1}).</math>\n\n(which is identical to [[linear interpolation]] if <math>x_{k-1} < x_* < x_k</math>). It is possible to include more than two points, and averaging the slope of the linear interpolant, by [[Regression analysis|regression]]-like techniques, on the data points chosen to be included. This is similar to [[linear prediction]].\n\n===Polynomial===\n[[File:Lagrange polynomials for continuations of sequence 1,2,3.gif|thumb|right|Lagrange extrapolations of the sequence 1,2,3. Extrapolating by 4 leads to a polynomial of minimal degree ({{color|#006060|cyan}} line).]]\nA polynomial curve can be created through the entire known data or just near the end.  The resulting curve can then be extended beyond the end of the known data.  Polynomial extrapolation is typically done by means of  [[Lagrange interpolation]] or using Newton's method of [[finite differences]] to create a [[Newton series]] that fits the data. The resulting polynomial may be used to extrapolate the data.\n\nHigh-order polynomial extrapolation must be used with due care. For the example data set and problem in the figure above, anything above order 1 (linear extrapolation) will possibly yield unusable values; an error estimate of the extrapolated value will grow with the degree of the polynomial extrapolation. This is related to [[Runge's phenomenon]].\n\n===Conic===\n\nA [[conic section]] can be created using five points near the end of the known data.  If the conic section created is an [[ellipse]] or [[circle]], when extrapolated it will loop back and rejoin itself.  An extrapolated [[parabola]] or [[hyperbola]] will not rejoin itself, but may curve back relative to the X-axis.  This type of extrapolation could be done with a conic sections template (on paper) or with a computer.\n\n===French curve===\n\n[[French curve]] extrapolation is a method suitable for any distribution that has a tendency to be exponential, but with accelerating or decelerating factors.<ref>[http://www.AIDSCJDUK.info AIDSCJDUK.info Main Index<!-- Bot generated title -->]</ref> This method has been used successfully in providing forecast projections of the growth of HIV/AIDS in the UK since 1987 and variant CJD in the UK for a number of years. Another study has shown that extrapolation can produce the same quality of forecasting results as more complex forecasting strategies.<ref>{{cite journal|url=http://marketing.wharton.upenn.edu/documents/research/Forecasting%20by%20extrapolation-25%20years.pdf | title = Forecasting by Extrapolation: Conclusions from Twenty-Five Years of Research | author = J. Scott Armstrong| journal = Interfaces | volume = 14 | issue = 6 | pages = 52–66 | year = 1984 | doi=10.1287/inte.14.6.52| citeseerx = 10.1.1.715.6481 }}</ref>\n\n==Quality==\n\nTypically, the quality of a particular method of extrapolation is limited by the assumptions about the function made by the method.  If the method assumes the data are smooth, then a non-[[smooth function]] will be poorly extrapolated.\n\nIn terms of complex time series, some experts have discovered that extrapolation is more accurate when performed through the decomposition of causal forces.<ref>{{cite web|url= http://www.forecastingprinciples.com/paperpdf/Decomposition%20by%20Causal%20Forces.pdf | title = Decomposition by Causal Forces: A Procedure for Forecasting Complex Time Series |author1=J. Scott Armstrong |author2=Fred Collopy |author3=J. Thomas Yokum | year = 2004}}</ref>\n\nEven for proper assumptions about the function, the extrapolation can diverge severely from the function.  The classic example is truncated [[power series]] representations of sin(''x'') and related [[trigonometric function]]s.  For instance, taking only data from near the ''x''&nbsp;=&nbsp;0, we may estimate that the function behaves as sin(''x'')&nbsp;~&nbsp;''x''.  In the neighborhood of ''x''&nbsp;=&nbsp;0, this is an excellent estimate.  Away from ''x''&nbsp;=&nbsp;0 however, the extrapolation moves arbitrarily away from the ''x''-axis while sin(''x'') remains in the [[interval (mathematics)|interval]] [&minus;1,{{nbsp}}1].  I.e., the error increases without bound.\n\nTaking more terms in the power series of sin(''x'') around ''x''&nbsp;=&nbsp;0 will produce better agreement over a larger interval near ''x''&nbsp;=&nbsp;0, but will produce extrapolations that eventually diverge away from the ''x''-axis even faster than the linear approximation.\n\nThis divergence is a specific property of extrapolation methods and is only circumvented when the functional forms assumed by the extrapolation method (inadvertently or intentionally due to additional information) accurately represent the nature of the function being extrapolated.  For particular problems, this additional information may be available, but in the general case, it is impossible to satisfy all possible function behaviors with a workably small set of potential behavior.\n\n==In the complex plane==\n\nIn [[complex analysis]], a problem of extrapolation may be converted into an [[interpolation]] problem by the change of variable <math>\\hat{z} = 1/z</math>.  This transform exchanges the part of the [[complex plane]] inside the [[unit circle]] with the part of the complex plane outside of the unit circle.  In particular, the [[compactification (mathematics)|compactification]] [[point at infinity]] is mapped to the origin and vice versa.  Care must be taken with this transform however, since the original function may have had \"features\", for example [[pole (complex analysis)|poles]] and other [[mathematical singularity|singularities]], at infinity that were not evident from the sampled data.\n\nAnother problem of extrapolation is loosely related to the problem of [[analytic continuation]], where (typically) a [[power series]] representation of a [[function (mathematics)|function]] is expanded at one of its points of [[limit of a function|convergence]] to produce a [[power series]] with a larger [[radius of convergence]].  In effect, a set of data from a small region is used to extrapolate a function onto a larger region.\n\nAgain, [[analytic continuation]] can be thwarted by [[function (mathematics)|function]] features that were not evident from the initial data.\n\nAlso, one may use   [[sequence transformation]]s like [[Padé approximant]]s and [[Levin-type sequence transformation]]s as extrapolation methods that lead to a [[summation]] of  [[power series]] that are divergent outside the original [[radius of convergence]]. In this case, one often obtains \n[[rational approximant]]s.\n\n==Fast==\nThe extrapolated data often convolute to a kernel function. After data is extrapolated, the size of data is increased N times, here N is approximately 2–3. If this data needs to be convoluted to a known kernel function, the numerical calculations will increase N{{nbsp}}log(N) times even with fast Fourier transform (FFT). There exists an algorithm, it analytically calculates the contribution from the part of the extrapolated data. The calculation time can be omitted compared with the original convolution calculation. Hence with this algorithm the calculations of a convolution using the extrapolated data is nearly not increased. This is referred as the fast extrapolation. The fast extrapolation has been applied to CT image reconstruction.<ref>\n{{cite journal\n | url = http://imrecons.com/wp-content/uploads/2013/02/extrapolation.pdf\n | title = Reconstruction from truncated projections using mixed extrapolations of exponential and quadratic functions.\n | author1 = Shuangren Zhao\n | author2 = Kang Yang\n | author3 = Xintie Yang\n | journal = J Xray Sci Technol.\n | volume = 19 | issue = 2 | pages = 155–72 | year = 2011\n| pmid = 21606580\n | doi = 10.3233/XST-2011-0284\n }}</ref>\n\n==Extrapolation arguments==\nExtrapolation arguments are informal and unquantified arguments which assert that something is true beyond the range of values for which it is known to be true. For example, we believe in the reality of what we see through magnifying glasses because it agrees with what we see with the naked eye but extends beyond it; we believe in what we see through light microscopes because it agrees with what we see through magnifying glasses but extends beyond it; and similarly for electron microscopes.\n\nLike [[slippery slope]] arguments, extrapolation arguments may be strong or weak depending on such factors as how far the extrapolation goes beyond the known range.<ref>J. Franklin, [http://ojs.uwindsor.ca/ojs/leddy/index.php/informal_logic/article/view/3610/3000 Arguments whose strength depends on continuous variation], ''Journal of Informal Logic'' 33 (2013), 33-56.</ref>\n\n==See also==\n{{Wiktionary|extrapolation}}\n{{Commons category|Extrapolation}}\n*[[Forecasting]]\n*[[Minimum polynomial extrapolation]]\n*[[Multigrid method]]\n*[[Prediction interval]]\n*[[Regression analysis]]\n*[[Richardson extrapolation]]\n*[[Static analysis]]\n*[[Trend estimation]]\n*[[Extrapolation domain analysis]]\n*[[Dead reckoning]]\n*[[Interior reconstruction]]\n*[[Extreme value theory]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*''Extrapolation Methods. Theory and Practice'' by C. Brezinski and M. Redivo Zaglia, North-Holland, 1991.\n\n[[Category:Interpolation]]\n[[Category:Asymptotic analysis]]"
    },
    {
      "title": "Hermite spline",
      "url": "https://en.wikipedia.org/wiki/Hermite_spline",
      "text": "In the [[mathematics|mathematical]] subfield of [[numerical analysis]], a '''Hermite spline''' is a [[spline curve]] where each polynomial of the spline is in [[Hermite form]].\n\n==See also==\n*[[Cubic Hermite spline]]\n*[[Hermite polynomials]]\n*[[Hermite interpolation]]\n\n{{mathapplied-stub}}\n\n[[Category:Splines (mathematics)]]\n\n[[Category:Interpolation]]"
    },
    {
      "title": "Hierarchical RBF",
      "url": "https://en.wikipedia.org/wiki/Hierarchical_RBF",
      "text": "In [[computer graphics]], a '''hierarchical RBF''' is an [[interpolation]] method based on [[Radial basis function]]s (RBF). Hierarchical RBF interpolation has applications in the construction of shape models in [[3d computer graphics|3D computer graphics]] (see [[Stanford Bunny]] image below), treatment of results from a [[3D scanner]], [[terrain]] reconstruction and others.\n\n[[Image:MyBunny.gif]]\n\nThis problem is informally named \"large scattered data point set interpolation\".\n\nThe idea of method (for example in 3D) consists of the following:\n* Let the scattered points be presented as set <math>\\mathbf{P}=\\{\\mathbf{c}_i=(\\mathbf{x}_i,\\mathbf{y}_i,\\mathbf{z}_i)\\vert^{N}_{i=1} \\subset \\mathbb{R}^3\\}</math>\n* Let there exist a set of values of some function in scattered points <math>\\mathbf{H}=\\{\\mathbf{h}_i \\vert^{N}_{i=1}\\subset \\mathbb{R}\\}</math>\n* Find a function <math>\\mathbf{f}(\\mathbf{x})</math> which will meet the condition <math>\\mathbf{f}(\\mathbf{x})=1</math> for points lying on the shape and <math>\\mathbf{f}(\\mathbf{x})\\neq1</math> for points not lying on the shape.\n* As J. C. Carr et al. showed <ref>Carr, J.C.; Beatson, R.K.; Cherrie, J.B.; Mitchell, T.J.; Fright, W.R.; McCallum B.C.; Evans, T.R. (2001),  “Reconstruction and Representation of 3D Objects with Radial Basis Functions” ACM SIGGRAPH 2001, Los Angeles, CA, P. 67–76.</ref> this function looks like <math>\\mathbf{f}(\\mathbf{x})=\\sum_{i=1}^N \\lambda_i \\varphi(\\mathbf{x},\\mathbf{c}_i)</math> where:\n<math>\\varphi</math> &mdash; it is [[Radial basis function|RBF]];\n<math>\\lambda</math> &mdash; it is coefficients which are the solution of the [[Linear system of equations|system]] show on picture:\n\n[[Image:System.gif]]\n\nfor determination of surface it is necessary to estimate the value of function <math>\\mathbf{f}(\\mathbf{x})</math> in interesting  points ''x''.\nA lack of such method is considerable complication <ref>Bashkov, E.A.; Babkov, V.S. (2008) “Research of RBF-algorithm and his modifications apply\npossibilities for the construction of shape computer models in medical practice”. Proc Int.\nConference \"Simulation-2008\", Pukhov Institute for Modelling in Energy Engineering, [http://babkov.name/article/2008-09.pdf] (in Russian)</ref> <math>\\mathbf{O}(\\mathbf{n}^2)</math> for calculate [[Radial basis function|RBF]], solve [[Linear system of equations|system]] and determine surface.\n\n==Other similar methods==\n* Reduce interpolation centres (<math>\\mathbf{O}(\\mathbf{n}^2)</math> for calculate [[Radial basis function|RBF]] and solve [[Linear system of equations|system]], <math>\\mathbf{O}(\\mathbf{m}\\mathbf{n})</math> for determine surface)\n* Compactly supported [[Radial basis function|RBF]] (<math>\\mathbf{O}(\\mathbf{n}\\log{\\mathbf{n}})</math> for calculate [[Radial basis function|RBF]], <math>\\mathbf{O}(\\mathbf{n}^{1.2..1.5})</math> for solve [[Linear system of equations|system]], <math>\\mathbf{O}(\\mathbf{m}\\log{\\mathbf{n}})</math> for determine surface)\n* [[Fast multipole method|FMM]]  (<math>\\mathbf{O}(\\mathbf{n}^2)</math> for calculate [[Radial basis function|RBF]], <math>\\mathbf{O}(\\mathbf{n}\\log{\\mathbf{n}})</math> for solve [[Linear system of equations|system]], <math>\\mathbf{O}(\\mathbf{m}+\\mathbf{n}\\log{\\mathbf{n}})</math> for determine surface)\n\n==Hierarchical algorithm==\nAn idea of [[hierarchical]] [[Algorythm|algorithm]] is an acceleration of calculations due to [[Decomposition (computer science)|decomposition]] of intricate problem on the great number of simple (see picture). [[File:Hierarchical algorithm flow chart.gif]]\n\nIn this case [[hierarchical]] division of space containing points on elementary parts, the [[Linear system of equations|system]] of small dimension solves in each of which. The calculation of surface in this case is taken to the [[hierarchical]] (on the basis of [[Tree (data structure)|tree-structure]]) calculation of interpolant. A method for a [[2D computer graphics|2D]] case is offered Pouderoux J. et al.<ref>Pouderoux, J. et al. (2004), “Adaptive hierarchical RBF interpolation for creating smooth digital elevathion models”, Proc. 12-th ACM Int. Symp. Advances in Geographical information Systems 2004, ACP Press, P. 232&ndash;240</ref> For a [[3D computer graphics|3D]] case a method is used in the tasks of [[3D computer graphics|3D graphics]] by W. Qiang et al.<ref>Qiang, W.; Pan, Z.; Chun, C.; Jiajun, B. (2007), “Surface rendering for parallel slice of contours from medical imaging”, Computing in science & engineering, 9(1), January&ndash;February 2007, P 32&ndash;37</ref> and modified by Babkov V.<ref>Babkov, V.S. (2008) “Modification of hierarchical RBF method for 3D-modelling based on laser scan result”. Proc. Int. Conference “Modern problems and achievement of radio, communication\nand informatics”, Zaporizhzhya National Technical University, [http://babkov.name/article/2008-08.pdf] (in Ukrainian)</ref>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Hierarchical Rbf}}\n[[Category:Geometric algorithms]]\n[[Category:Computer graphics]]\n[[Category:Interpolation]]"
    },
    {
      "title": "Interpolation (computer graphics)",
      "url": "https://en.wikipedia.org/wiki/Interpolation_%28computer_graphics%29",
      "text": "{{Unreferenced|date=May 2007}}\n{{about|interpolation in computer graphics|other types of interpolation|interpolation (disambiguation)}}\nIn the context of [[computer animation]], '''interpolation''' is [[inbetweening]], or filling in frames between the key frames. It typically calculates the in between frames through use of (usually) piecewise [[polynomial interpolation]] to draw images semi-automatically.\n\nFor all applications of this type, a set of \"key points\" is defined by the graphic artist.  These are values that are rather widely separated in space or time, and represent the desired result, but only in very coarse steps.  The computed interpolation process is then used to insert many new values in between these key points to give a \"smoother\" result.  \n \nIn its simplest form, this is the drawing of two-dimensional curves.  The key points, placed by the artist, are used by the computer algorithm to form a smooth curve either through, or near these points.  For a typical example of 2-D interpolation through key points see [[cardinal spline]].  For examples which go near key points see [[nonuniform rational B-spline]], or [[Bézier curve]].  This is extended to the forming of three-dimensional curves, shapes and complex, dynamic artistic patterns such as used in laser light shows.\n\nThe process can be extended to motions. The path of an object can be interpolated by providing some key locations, then calculating many in between locations for a smooth motion.  In addition to position, the speed or velocity, as well as accelerations along a path, can be calculated to mimic real-life motion dynamics.  Where the subjects are too large or complex to move, the camera position and orientation can be moved by this process.  This last is commonly called [[motion control]].\n\nGoing further, orientations (rotations) of objects and parts of objects can be interpolated as well as parts of complete characters.  This process mimics that used in early cartoon films.  Master animators would draw key frames of the film, then, junior animators would draw the in-between frames.  This is called [[inbetweening]] or tweening and the overall process is called \"[[key frame]] animation\".  To make these motions appear realistic, interpolation algorithms have been sought which follow, or approximate real life motion dynamics.  This applies to things such as the motion of arms and legs from frame to frame, or the motion of all parts of a face, given the motion of the important, key points of the face.  Defining the motion of key strands of hair, spread around an animal, can be made into full fur. Using custom algorithms, motions with unique, unnatural and entertaining visual characteristics can be formed. The color of an object can be defined by key color-locations or frames allowing the computation of smooth color gradients around an object or varying in time.  Algorithms such as the [[Kochanek–Bartels spline]] provide additional adjustment parameters which allow customizing the in-between behavior to suit a wide variety of situations.  The article on [[tweening]] currently demonstrates several of these.\n\nAnother important area of this subject is the computational burden of these algorithms.  Algorithms with faster execution times are sought to produce more of these results in less time in order to complete these projects quicker.  As the resolution increases to produce animated feature films, the amount of processing can increase greatly.... and parts of objects can be defined in a key colour\n\n==See also==\n* [[Morphing]] \n\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]"
    },
    {
      "title": "Kochanek–Bartels spline",
      "url": "https://en.wikipedia.org/wiki/Kochanek%E2%80%93Bartels_spline",
      "text": "[[File:Kochanek bartels spline.svg|350px|right]]\n\nIn [[mathematics]], a '''Kochanek–Bartels spline''' or '''Kochanek–Bartels curve''' is a [[cubic Hermite spline]] with tension, bias, and continuity parameters defined to change the behavior of the [[tangent]]s.\n\nGiven ''n'' + 1 [[Spline (mathematics)|knots]],\n\n:'''p'''<sub>0</sub>, ...,  '''p'''<sub>''n''</sub>,\n\nto be interpolated with ''n'' cubic Hermite curve segments, for each curve we have a starting point '''p'''<sub>''i''</sub> and an ending point '''p'''<sub>''i''+1</sub> with starting tangent '''d'''<sub>''i''</sub> and ending tangent '''d'''<sub>''i''+1</sub> defined by\n\n:<math>\\mathbf{d}_i = \\frac{(1-t)(1+b)(1+c)}{2}(\\mathbf{p}_i-\\mathbf{p}_{i-1}) + \\frac{(1-t)(1-b)(1-c)}{2}(\\mathbf{p}_{i+1}-\\mathbf{p}_i)\n</math>\n\n:<math>\\mathbf{d}_{i+1} = \\frac{(1-t)(1+b)(1-c)}{2}(\\mathbf{p}_{i+1}-\\mathbf{p}_{i}) + \\frac{(1-t)(1-b)(1+c)}{2}(\\mathbf{p}_{i+2}-\\mathbf{p}_{i+1})\n</math>\n\nwhere...\n{|\n|-\n| <big>'''{{mvar|t}}'''</big>\n| ''tension''\n| Changes the '''length''' of the ''tangent vector''\n|\n|-\n| <big>'''{{mvar|b}}'''</big>\n| ''bias''\n| Primarily changes the '''direction''' of the ''tangent vector''\n|-\n| <big>'''{{mvar|c}}'''</big>\n| ''continuity''   \n| Changes the '''sharpness''' in change between tangents\n|}\nSetting each parameter to zero would give a [[Catmull–Rom spline]].\n\nThe [http://news.povray.org/povray.binaries.tutorials/attachment/%3CXns91B880592482seed7@povray.org%3E/Splines.bas.txt source code found here] of Steve Noskowicz in 1996 actually describes the impact that each of these values has on the drawn curve:\n{|\n|-\n| '''Tension'''\n| ''T'' = +1&rarr; Tight\n| ''T'' = &minus;1&rarr; Round\n|\n|-\n| '''Bias'''\n| ''B'' = +1&rarr; Post Shoot\n| ''B'' = &minus;1&rarr; Pre shoot\n|-\n| '''Continuity'''\n| ''C'' = +1&rarr; Inverted corners\n| ''C'' = &minus;1&rarr; Box corners\n|}\nThe code includes matrix summary needed to generate these splines in a [[BASIC]] dialect.\n\n== External links ==\n\n*{{cite web\n |url=http://www.shaneaherne.com/research/splines.html \n |title=Kochanek and Bartels Splines \n |accessdate=2009-04-15 \n |author=Shane Aherne \n |date= \n |work=Motion Capture &mdash; exploring the past, present and future \n |publisher= \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20070705165751/http://www.shaneaherne.com/research/splines.html \n |archivedate=2007-07-05 \n |df= \n}}\n\n*{{cite web \n| url        = http://dl.acm.org/citation.cfm?id=808575\n| title      = Interpolating splines with local tension, continuity, and bias control\n| accessdate = 2014-09-23 \n| author     = Doris H. U. Kochanek, Richard H. Bartels\n| date       =\n| work       = SIGGRAPH '84 Proceedings of the 11th annual conference on Computer graphics and interactive techniques\n| pages      = 33–41\n| publisher  = ACM\n}}\n\n{{DEFAULTSORT:Kochanek-Bartels spline}}\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]"
    },
    {
      "title": "Kriging",
      "url": "https://en.wikipedia.org/wiki/Kriging",
      "text": "In [[statistics]], originally in [[geostatistics]], '''kriging''' or '''Gaussian process regression''' is a method of [[interpolation]] for which the interpolated values are modeled by a [[Gaussian process]] governed by prior [[covariance]]s.  Under suitable assumptions on the priors, kriging gives the [[best linear unbiased prediction]] of the intermediate values.  Interpolating methods based on other criteria such as [[smoothness]] (e.g., [[smoothing spline]]) need not yield the most likely intermediate values.  The method is widely used in the domain of [[Spatial analysis#Sampling|spatial analysis]] and [[computer experiment]]s. The technique is also known as '''Wiener–Kolmogorov prediction''', after [[Norbert Wiener]] and [[Andrey Kolmogorov]].\n\n[[File:Example_of_kriging_interpolation_in_1D.png|thumb|400px|Example of one-dimensional data interpolation by kriging, with confidence intervals. Squares indicate the location of the data. The kriging interpolation, shown in red, runs along the means of the normally distributed confidence intervals shown in gray.  The dashed curve shows a spline that is smooth, but departs significantly from the expected intermediate values given by those means.]]\n\nThe theoretical basis for the method was developed by the French mathematician [[Georges Matheron]] in 1960, based on the Master's thesis of [[Danie G. Krige]], the pioneering plotter of distance-weighted average gold grades at the [[Witwatersrand]] reef complex in [[South Africa]]. Krige sought to estimate the most likely distribution of gold based on samples from a few boreholes.  The English verb is ''to krige'' and the most common noun is ''kriging''; both are often pronounced with a [[Hard and soft g|hard \"g\"]], following the pronunciation of the name \"Krige\". The word is sometimes capitalized as ''Kriging'' in the literature.\n\n==Main principles==\n\n===Related terms and techniques===\nThe basic idea of kriging is to predict the value of a function at a given point by computing a weighted average of the known values of the function in the neighborhood of the point. The method is mathematically closely related to [[regression analysis]]. Both theories derive a [[best linear unbiased estimator]], based on assumptions on [[covariance]]s, make use of [[Gauss–Markov theorem]] to prove independence of the estimate and error, and make use of very similar formulae. Even so, they are useful in different frameworks: kriging is made for estimation of a single realization of a random field, while regression models are based on multiple observations of a multivariate data set.\n\nThe kriging estimation may also be seen as a [[spline (mathematics)|spline]] in a [[reproducing kernel Hilbert space]], with the reproducing kernel given by the covariance function.<ref>{{cite book |first=Grace |last=Wahba |title=Spline Models for Observational Data |publisher=SIAM |volume=59 |year=1990 |doi=10.1137/1.9781611970128}}</ref> The difference with the classical kriging approach is provided by the interpretation: while the spline is motivated by a minimum norm interpolation based on a Hilbert space structure, kriging is motivated by an expected squared prediction error based on a stochastic model.\n\nKriging with ''polynomial trend surfaces'' is mathematically identical to [[generalized least squares]]  polynomial [[curve fitting]].\n\nKriging can also be understood as a form of [[Bayesian inference]].<ref>{{Cite book |last1=Williams |first1=C. K. I. |chapter=Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond |doi=10.1007/978-94-011-5014-9_23 |title=Learning in Graphical Models |pages=599–621 |year=1998 |isbn=978-94-010-6104-9}}</ref> Kriging starts with a [[prior probability distribution|prior]] [[probability distribution|distribution]] over [[Function (mathematics)|function]]s. This prior takes the form of a Gaussian process: <math>N</math> samples from a function will be [[normal distribution|normally distributed]], where the [[covariance]] between any two samples is the covariance function (or [[kernel (set theory)|kernel]]) of the Gaussian process evaluated at the spatial location of two points. A [[Set (mathematics)|set]] of values is then observed, each value associated with a spatial location. Now, a new value can be predicted at any new spatial location, by combining the Gaussian prior with a Gaussian [[likelihood function]] for each of the observed values. The resulting [[Posterior probability|posterior]] distribution is also Gaussian, with a mean and covariance that can be simply computed from the observed values, their variance, and the kernel matrix derived from the prior.\n\n===Geostatistical estimator===\nIn geostatistical models, sampled data is interpreted as the result of a random process. The fact that these models incorporate uncertainty in their conceptualization doesn't mean that the phenomenon – the forest, the aquifer, the mineral deposit – has resulted  from a random process, but rather it allows one to build a methodological basis for the spatial inference of quantities in unobserved locations, and to quantify the uncertainty associated with the estimator.\n\nA [[stochastic process]] is, in the context of this model, simply a way to approach the set of data collected from the samples. The first step in geostatistical modulation is to create a random process that best describes the set of observed data.\n\nA value from location <math>x_1</math> (generic denomination of a set of [[Geographic coordinate system|geographic coordinates]]) is interpreted as a realization <math>z(x_1)</math> of the [[random variable]] <math>Z(x_1)</math>. In the space <math>A</math>, where the set of samples is dispersed, there are <math>N</math> realizations of the random variables <math>Z(x_1), Z(x_2), \\ldots, Z(x_N)</math>, correlated between themselves.\n\nThe set of random variables constitutes a random function of which only one realization is known <math>z(x_i)</math> – the set of observed data. With only one realization of each random variable it's theoretically impossible to determine any statistical parameter of the individual variables or the function.\n\n:The proposed solution in the geostatistical formalism consists in assuming various degrees of stationarity in the random function, in order to make possible the inference of some statistic values.\n\nFor instance, if one assumes, based on the homogeneity of samples in area <math>A</math> where the variable is distributed, the hypothesis that the [[Moment (mathematics)#Mean|first moment]] is stationary (i.e. all random variables have the same mean), then one is assuming that the mean can be estimated by the arithmetic mean of sampled values. Judging such a hypothesis as appropriate is equivalent to assuming the sample values are sufficiently homogeneous{{Citation needed|date=August 2018}}.\n\nThe hypothesis of stationarity related to the [[Moment (mathematics)#Variance|second moment]] is defined in the following way: the correlation between two random variables solely depends on the spatial distance between them, and is independent of their location.  Thus if <math>\\mathbf{h}=x_2-x_1</math> and <math> |\\mathbf{h}|=h </math> then:\n\n:<math>C(Z(x_1),Z(x_2)) = C(Z(x_i),Z(x_i+\\mathbf{h})) = C(h)</math>\n\n:<math>\\gamma(Z(x_1),Z(x_2)) = \\gamma(Z(x_i),Z(x_i+\\mathbf{h})) = \\gamma(h)</math>\n\nand, for simplicity, we define <math>C(x_i,x_j)=C(Z(x_i),Z(x_j))</math> and <math>\\gamma(x_i,x_j)=\\gamma(Z(x_i),Z(x_j))</math>. \n\nThis hypothesis allows one to infer those two measures – the [[variogram]] and the [[covariogram]]:\n\n:<math>\\gamma(h)=\\frac{1}{2|N(h)|}\\sum_{(i,j)\\in N(h)} \\left(Z(x_i)-Z(x_j)\\right)^2</math>\n:\n:<math>C(h)=\\frac{1}{|N(h)|}\\sum_{(i,j)\\in N(h)} \\left(Z(x_i)-m(h)\\right)\\left(Z(x_j)-m(h)\\right)</math>\n\nwhere:\n* <math>m(h)=\\frac{1}{2|N(h)|}\\sum_{(i,j)\\in N(h)}Z(x_i)+Z(x_j)</math>;\n* <math>N(h)</math> denotes the set of pairs of observations <math>i,\\;j</math> such that <math>|x_i-x_j| = h</math>, and <math>|N(h)|</math> is the number of pairs in the set. In this set, <math>(i,\\;j)</math> and <math>(j,\\;i)</math> denote the same element. Generally an  \"approximate distance\" <math>h</math> is used, implemented using a certain tolerance.\n\n===Linear estimation===\nSpatial inference, or estimation, of a quantity <math>Z\\colon \\mathbb{R}^n\\rightarrow\\mathbb{R}</math>, at an unobserved location <math>x_0</math>, is calculated from a linear combination of the observed values <math>z_i=Z(x_i)</math> and weights <math>w_i(x_0),\\;i=1,\\ldots,N</math>:\n\n<math>\\hat{Z}(x_0) = \\begin{bmatrix}\n    w_1 & w_2 & \\cdots & w_N\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nz_1\\\\\nz_2\\\\\n\\vdots\\\\\nz_N\n\\end{bmatrix} = \\sum_{i=1}^n w_i(x_0) \\times Z(x_i)</math>\n\nThe weights <math>w_i</math> are intended to summarize two extremely important procedures in a spatial inference process:\n\n* reflect the structural \"proximity\" of samples to the estimation location, <math>x_0</math>\n* at the same time, they should have a desegregation effect, in order to avoid bias caused by eventual sample ''clusters''\n\nWhen calculating the weights <math>w_i</math>, there are two objectives in the geostatistical formalism: ''unbias'' and ''minimal variance of estimation''.\n\nIf the cloud of real values <math>Z(x_0)</math> is plotted against the estimated values <math>\\hat{Z}(x_0)</math>, the criterion for global unbias, ''intrinsic stationarity'' or [[stationary process|wide sense stationarity]] of the field, implies that the mean of the estimations must be equal to mean of the real values.\n\nThe second criterion says that the mean of the squared deviations <math>(\\hat{Z}(x)-Z(x))</math> must be minimal, which means that when the cloud of estimated values ''versus'' the cloud real values is more disperse, the estimator is more imprecise.\n\n==Methods==\nDepending on the stochastic properties of the random field and the various degrees of stationarity assumed, different methods for calculating the weights can be deduced, i.e. different types of kriging apply. Classical methods are:\n\n* ''Ordinary kriging'' assumes constant unknown mean only over the search neighborhood of <math>x_0</math>.\n* ''Simple kriging'' assumes stationarity of the [[Moment (mathematics)#Mean|first moment]] over the entire domain with a known mean: <math>E\\{Z(x)\\}=E\\{Z(x_0)\\}=m</math>, where <math>m</math> is the known mean.\n* ''Universal kriging''{{anchor|Universal}} assumes a general polynomial trend model, such as linear trend model <math>E\\{Z(x)\\}=\\sum_{k=0}^p \\beta_k f_k(x)</math>.\n* ''IRFk-kriging''{{anchor|IRFk}} assumes <math>E\\{Z(x)\\}</math> to be an unknown [[polynomial]] in <math>x</math>.\n* ''Indicator kriging''{{anchor|Indicator}} uses [[indicator function]]s instead of the process itself, in order to estimate transition probabilities.\n** ''Multiple-indicator kriging''{{anchor|Multiple-indicator}} is a version of indicator kriging working with a family of indicators. Initially, MIK showed considerable promise as a new method that could more accurately estimate overall global mineral deposit concentrations or grades.  However, these benefits have been outweighed by other inherent problems of practicality in modelling due to the inherently large block sizes used and also the lack of mining scale resolution. Conditional simulation is fast becoming the accepted replacement technique in this case.{{citation needed|date=March 2016}}\n* ''Disjunctive kriging''{{anchor|Disjunctive}} is a nonlinear generalisation of kriging.\n* ''[[Log-normal distribution|Lognormal]] kriging''{{anchor|Lognormal}} interpolates positive data by means of [[logarithm]]s.\n\n===Ordinary kriging{{anchor|Ordinary}}===\nThe unknown value <math>Z(x_0)</math> is interpreted as a random variable located in <math>x_0</math>, as well as the values of neighbors samples <math>Z(x_i),  i=1,\\ldots ,N</math>. The estimator <math>\\hat{Z}(x_0)</math> is also interpreted as a random variable located in <math>x_0</math>, a result of the linear combination of variables.\n\nIn order to deduce the kriging system for the assumptions of the model, the following error committed while estimating <math>Z(x)</math> in <math>x_0</math> is declared:\n\n:<math>\\epsilon(x_0) = \\hat{Z}(x_0) - Z(x_0) =\n\\begin{bmatrix}W^T&-1\\end{bmatrix} \\cdot \\begin{bmatrix}Z(x_1)&\\cdots&Z(x_N)&Z(x_0)\\end{bmatrix}^T =\n\\sum^{N}_{i=1}w_i(x_0) \\times Z(x_i) - Z(x_0)</math>\n\nThe two quality criteria referred to previously can now be expressed in terms of the mean and variance of the new random variable <math>\\epsilon(x_0)</math>:\n\n'''Lack of bias''':\n\nSince the random function is stationary, <math>E(Z(x_i))=E(Z(x_0))=m</math>, the following constraint is observed:\n\n:<math>E\\left(\\epsilon(x_0)\\right)=0 \\Leftrightarrow \\sum^{N}_{i=1}w_i(x_0) \\times E(Z(x_i)) - E(Z(x_0))=0 \\Leftrightarrow</math>\n:\n:<math>\\Leftrightarrow m\\sum^{N}_{i=1}w_i(x_0) - m=0 \\Leftrightarrow \\sum^N_{i=1} w_i(x_0) = 1 \\Leftrightarrow \\mathbf{1}^T \\cdot W = 1</math>\n\nIn order to ensure that the model is unbiased, the weights must sum to one.\n\n'''Minimum variance''':\n\nTwo estimators can have <math>E\\left[\\epsilon(x_0)\\right]=0</math>, but the dispersion around their mean determines the difference between the quality of estimators. To find an estimator with minimum variance, we need to minimize <math>E\\left(\\epsilon(x_0)^2\\right)</math>.\n\n:<math>\\begin{array}{rl}\n\\operatorname{Var}(\\epsilon(x_0)) &= \\operatorname{Var}\\left(\\begin{bmatrix}W^T&-1\\end{bmatrix} \\cdot\n\\begin{bmatrix}Z(x_1)&\\cdots&Z(x_N)&Z(x_0)\\end{bmatrix}^T\\right) =\\\\\n&\\overset{*}{=} \\begin{bmatrix}W^T&-1\\end{bmatrix} \\cdot\n\\operatorname{Var}\\left(\\begin{bmatrix}Z(x_1)&\\cdots&Z(x_N)&Z(x_0)\\end{bmatrix}^T\\right) \\cdot\n\\begin{bmatrix}W\\\\-1\\end{bmatrix}\n\\end{array}</math>\n\n'''*''' see [[Covariance matrix#As a linear operator|covariance matrix]] {{Failed verification|date=December 2015}} for a detailed explanation\n\n:<math>\\operatorname{Var}(\\epsilon(x_0)) \\overset{*}{=} \\begin{bmatrix}W^T&-1\\end{bmatrix} \\cdot \n\\begin{bmatrix} \\operatorname{Var}_{x_i}& \\operatorname{Cov}_{x_ix_0}\\\\ \\operatorname{Cov}_{x_ix_0}^T & \\operatorname{Var}_{x_0}\\end{bmatrix} \\cdot\n\\begin{bmatrix}W\\\\-1\\end{bmatrix}</math>\n\n'''*''' where the literals <math>\\left\\{\\operatorname{Var}_{x_i}, \\operatorname{Var}_{x_0}, \\operatorname{Cov}_{x_ix_0}\\right\\}</math> stand for <math>\n\\left\\{\\operatorname{Var}\\left(\\begin{bmatrix}Z(x_1)&\\cdots&Z(x_N)\\end{bmatrix}^T\\right), \\operatorname{Var}(Z(x_0)), \\operatorname{Cov} \\left(\\begin{bmatrix}Z(x_1)&\\cdots&Z(x_N)\\end{bmatrix}^T,Z(x_0)\\right)\\right\\}</math>.\n\nOnce defined the covariance model or [[variogram]], <math>C(\\mathbf{h})</math> or <math>\\gamma(\\mathbf{h})</math>, valid in all field of analysis of <math>Z(x)</math>, then we can write an expression for the estimation variance of any estimator in function of the covariance between the samples and the covariances between the samples and the point to estimate:\n\n:<math>\\left\\{\\begin{array}{l}\n\\operatorname{Var}(\\epsilon(x_0)) = W^T \\cdot \\operatorname{Var}_{x_i} \\cdot W - \\operatorname{Cov}_{x_ix_0}^T \\cdot W - W^T \\cdot \\operatorname{Cov}_{x_ix_0} + \\operatorname{Var}_{x_0}\\\\\n\\operatorname{Var}(\\epsilon(x_0)) = \\operatorname{Cov}(0) + \\sum_{i}\\sum_j w_i w_j \\operatorname{Cov}(x_i,x_j) - 2 \\sum_iw_i C(x_i,x_0)\\end{array} \\right.</math>\n\nSome conclusions can be asserted from this expression. The variance of estimation:\n\n* is not quantifiable to any linear estimator, once the stationarity of the mean and of the spatial covariances, or variograms, are assumed.\n* grows when the covariance between the samples and the point to estimate decreases. This means that, when the samples are farther away from <math>x_0</math>, the worse the estimation.\n* grows with the ''a priori'' variance <math>C(0)</math> of the variable <math>Z(x)</math>. When the variable is less disperse, the variance is lower in any point of the area <math>A</math>.\n* does not depend on the values of the samples. This means that the same spatial configuration (with the same geometrical relations between samples and the point to estimate) always reproduces the same estimation variance in any part of the area <math>A</math>. This way, the variance does not measure the uncertainty of estimation produced by the local variable.\n\n;System of equations\n:<math>\\begin{align}\n&\\underset{W}{\\text{minimize}}& & W^T \\cdot \\operatorname{Var}_{x_i} \\cdot W - \\operatorname{Cov}_{x_ix_0}^T \\cdot W - W^T \\cdot \\operatorname{Cov}_{x_ix_0} + \\operatorname{Var}_{x_0} \\\\\n&\\text{subject to}\n& &\\mathbf{1}^T \\cdot W = 1\n\\end{align}</math>\n\nSolving this optimization problem (see [[Lagrange multipliers]]) results in the ''kriging system'':\n\n:<math>\\begin{bmatrix}\\hat{W}\\\\\\mu\\end{bmatrix} = \\begin{bmatrix}\n\\operatorname{Var}_{x_i}& \\mathbf{1}\\\\\n\\mathbf{1}^T& 0\n\\end{bmatrix}^{-1}\\cdot \\begin{bmatrix} \\operatorname{Cov}_{x_ix_0}\\\\ 1\\end{bmatrix} = \\begin{bmatrix}\n\\gamma(x_1,x_1) & \\cdots & \\gamma(x_1,x_n) &1 \\\\\n\\vdots & \\ddots & \\vdots  & \\vdots \\\\\n\\gamma(x_n,x_1) & \\cdots & \\gamma(x_n,x_n) & 1 \\\\\n1 &\\cdots& 1 & 0 \n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\\gamma(x_1,x^*) \\\\ \\vdots \\\\ \\gamma(x_n,x^*) \\\\ 1\\end{bmatrix}\n</math>\n\nthe additional parameter <math>\\mu</math> is a [[Lagrange multiplier]] used in the minimization of the kriging error <math>\\sigma_k^2(x)</math> to honor the unbiasedness condition.\n\n===Simple kriging{{anchor|Simpler}}===\n{{notice|this section is very poor and needs to be improved}}\nSimple kriging is mathematically the simplest, but the least general.<ref>{{cite book |last=Olea |first=Ricardo A. |date=1999 |title=Geostatistics for Engineers and Earth Scientists |publisher= Kluwer Academic |isbn=978-1-4615-5001-3}}</ref> It assumes the [[expected value|expectation]] of the [[random field]] to be known, and relies on a [[covariance function]]. However, in most applications neither the expectation nor the covariance are known beforehand.\n\nThe practical assumptions for the application of ''simple kriging'' are:\n* [[stationary process|wide sense stationarity]] of the field, (variance stationary).\n* The expectation is zero everywhere: <math>\\mu(x)=0</math>.\n* Known [[covariance function]] <math>c(x,y)=\\operatorname{Cov}(Z(x),Z(y))</math>\n\n;System of equations\nThe ''kriging weights'' of ''simple kriging'' have no unbiasedness condition \nand are given by the ''simple kriging equation system'':\n:<math>\\begin{pmatrix}w_1 \\\\ \\vdots \\\\ w_n \\end{pmatrix}=\n\\begin{pmatrix}c(x_1,x_1) & \\cdots & c(x_1,x_n) \\\\\n\\vdots & \\ddots & \\vdots  \\\\\nc(x_n,x_1) & \\cdots & c(x_n,x_n) \n\\end{pmatrix}^{-1}\n\\begin{pmatrix}c(x_1,x_0) \\\\ \\vdots \\\\ c(x_n,x_0) \\end{pmatrix}\n</math>\n\nThis is analogous to a linear regression of <math>Z(x_0)</math> on the other <math>z_1 , \\ldots, z_n</math>.\n\n;Estimation\nThe interpolation by simple kriging is given by:\n:<math>\\hat{Z}(x_0)=\\begin{pmatrix}z_1 \\\\ \\vdots \\\\ z_n  \\end{pmatrix}'\n\\begin{pmatrix}c(x_1,x_1) & \\cdots & c(x_1,x_n)  \\\\\n\\vdots & \\ddots & \\vdots   \\\\\nc(x_n,x_1) & \\cdots & c(x_n,x_n)   \n\\end{pmatrix}^{-1}\n\\begin{pmatrix}c(x_1,x_0) \\\\ \\vdots \\\\ c(x_n,x_0)\\end{pmatrix}\n</math>\n\nThe kriging error is given by:\n:<math>\\operatorname{Var}\\left(\\hat{Z}(x_0)-Z(x_0)\\right)=\\underbrace{c(x_0,x_0)}_{\\operatorname{Var}(Z(x_0))}-\n\\underbrace{\\begin{pmatrix}c(x_1,x_0) \\\\ \\vdots \\\\ c(x_n,x_0)\\end{pmatrix}'\n\\begin{pmatrix}\nc(x_1,x_1) & \\cdots & c(x_1,x_n)  \\\\\n\\vdots & \\ddots & \\vdots  \\\\\nc(x_n,x_1) & \\cdots & c(x_n,x_n) \n\\end{pmatrix}^{-1}\n\\begin{pmatrix}c(x_1,x_0) \\\\ \\vdots \\\\ c(x_n,x_0) \\end{pmatrix}}_{\\operatorname{Var}(\\hat{Z}(x_0))}\n</math>\nwhich leads to the generalised least squares version of the [[Gauss–Markov theorem]] (Chiles & Delfiner 1999, p.&nbsp;159):\n:<math>\\operatorname{Var}(Z(x_0))=\\operatorname{Var}(\\hat{Z}(x_0)) + \\operatorname{Var}\\left(\\hat{Z}(x_0)-Z(x_0)\\right).</math>\n\n=== Properties ===\n{{notice|this section needs revision. Incorrect or confusing text should be removed.}}\n(Cressie 1993, Chiles&Delfiner 1999, Wackernagel 1995)\n* The kriging estimation is unbiased: <math>E[\\hat{Z}(x_i)]=E[Z(x_i)]</math>\n* The kriging estimation honors the actually observed value: <math>\\hat{Z}(x_i)=Z(x_i)</math> (assuming no measurement error is incurred)\n* The kriging estimation <math>\\hat{Z}(x)</math> is the [[best linear unbiased estimator]] of <math>Z(x)</math> if the assumptions hold. However (e.g. Cressie 1993):\n** As with any method: If the assumptions do not hold, kriging might be bad.\n** There might be better nonlinear and/or biased methods.\n** No properties are guaranteed, when the wrong variogram is used. However typically still a 'good' interpolation is achieved.\n** Best is not necessarily good: e.g. In case of no spatial dependence the kriging interpolation is only as good as the arithmetic mean.\n* Kriging provides <math>\\sigma_k^2</math> as a measure of precision. However this measure relies on the correctness of the variogram.\n\n==Applications==\n{{notice|this section is very poor and needs to be improved}}\n<!-- [[File:Syncline and anticline.jpg|thumb|300px|An illustrated depiction of a [[syncline]] and [[anticline]] commonly studied in [[Structural geology]] and [[Geomorphology]].]] -->\nAlthough kriging was developed originally for applications in geostatistics, it is a general method of statistical interpolation that can be applied within any discipline to sampled data from random fields that satisfy the appropriate mathematical assumptions.  It can be used where spatially-related data has been collected (in 2-D or 3-D) and estimates of \"fill-in\" data are desired in the locations (spatial gaps) between the actual measurements.\n\nTo date kriging has been used in a variety of disciplines, including the following:\n* [[Environmental science]]<ref>{{cite journal | last1 = Bayraktar | first1 = Hanefi | last2 = Sezer | first2 = Turalioglu | year = 2005 | title = A Kriging-based approach for locating a sampling site—in the assessment of air quality | url = | journal = SERRA | volume = 19 | issue = 4| pages = 301–305 | doi = 10.1007/s00477-005-0234-8 }}</ref>\n* [[Hydrogeology]]<ref name=\"Chiles\">Chiles, J.-P. and P. Delfiner (1999) ''Geostatistics, Modeling Spatial Uncertainty'', Wiley Series in Probability and statistics.</ref><ref>{{Cite journal | last1 = Zimmerman | first1 = D. A. | last2 = De Marsily | first2 = G. | last3 = Gotway | first3 = C. A. |author3-link= Carol A. Gotway Crawford | last4 = Marietta | first4 = M. G. | last5 = Axness | first5 = C. L. | last6 = Beauheim | first6 = R. L. | last7 = Bras | first7 = R. L. | last8 = Carrera | first8 = J. | last9 = Dagan | first9 = G. | last10 = Davies | first10 = P. B. | last11 = Gallegos | first11 = D. P. | last12 = Galli | first12 = A. | last13 = Gómez-Hernández | first13 = J. | last14 = Grindrod | first14 = P. | last15 = Gutjahr | first15 = A. L. | last16 = Kitanidis | first16 = P. K. | last17 = Lavenue | first17 = A. M. | last18 = McLaughlin | first18 = D. | last19 = Neuman | first19 = S. P. | last20 = Ramarao | first20 = B. S. | last21 = Ravenne | first21 = C. | last22 = Rubin | first22 = Y. | doi = 10.1029/98WR00003 | title = A comparison of seven geostatistically based inverse approaches to estimate transmissivities for modeling advective transport by groundwater flow | journal = Water Resources Research | volume = 34 | issue = 6 | pages = 1373–1413 | year = 1998 | url = http://web.mit.edu/dennism/www/Publications/M25_1998_Zimmerman_etal_WRR.pdf| pmid =  | pmc = | bibcode=1998WRR....34.1373Z}}</ref><ref>{{Cite journal | last1 = Tonkin | first1 = M. J. | last2 = Larson | first2 = S. P. | doi = 10.1111/j.1745-6584.2002.tb02503.x | title = Kriging Water Levels with a Regional-Linear and Point-Logarithmic Drift | journal = Ground Water | volume = 40 | issue = 2 | pages = 185–193 | year = 2002 | pmid =  11916123| pmc = }}</ref>\n* [[Mining]]<ref name=\"Journel\">Journel, A.G. and C.J. Huijbregts (1978) ''Mining Geostatistics'', Academic Press London</ref><ref>{{Cite journal | last1 = Richmond | first1 = A. | journal = [[Mathematical Geology]]| title = Financially Efficient Ore Selections Incorporating Grade Uncertainty| volume = 35 | issue = 2 | pages = 195–215 | doi = 10.1023/A:1023239606028 | year = 2003 | pmid =  | pmc = }}</ref>\n* [[Natural resource]]s<ref name=\"multiple\">Goovaerts (1997) ''Geostatistics for natural resource evaluation'', OUP. {{ISBN|0-19-511538-4}}</ref><ref>{{Cite journal | last1 = Emery | first1 = X. | title = Simple and Ordinary Multigaussian Kriging for Estimating Recoverable Reserves | doi = 10.1007/s11004-005-1560-6 | journal = [[Mathematical Geology]]| volume = 37 | issue = 3 | pages = 295–319 | year = 2005 | pmid =  | pmc = }}</ref>\n* [[Remote sensing]]<ref>{{Cite book | last1 = Papritz | first1 = A. | last2 = Stein | first2 = A. | chapter = Spatial prediction by linear kriging | doi = 10.1007/0-306-47647-9_6 | title = Spatial Statistics for Remote Sensing | series = Remote Sensing and Digital Image Processing | volume = 1 | page = 83 | year = 2002 | isbn = 0-7923-5978-X | pmid =  | pmc = }}</ref>\n* [[Real estate appraisal]]<ref>Barris, J. (2008) ''An expert system for appraisal by the method of comparison''. PhD Thesis, UPC, Barcelona</ref><ref>Barris, J. and Garcia Almirall, P. (2010) ''A density function of the appraisal value'', UPC, Barcelona</ref>\n* [[Integrated Circuit Analysis and Optimization]]<ref>Oghenekarho Okobiah, [[Saraju Mohanty]], and Elias Kougianos (2013) ''[http://www.cse.unt.edu/~smohanty/Publications_Journals/2013/Mohanty_IET-CDS-2013Sep_Thermal-Sensor-Geostatistical.pdf Geostatistical-Inspired Fast Layout Optimization of a Nano-CMOS Thermal Sensor] {{webarchive|url=https://web.archive.org/web/20140714173450/http://www.cse.unt.edu/~smohanty/Publications_Journals/2013/Mohanty_IET-CDS-2013Sep_Thermal-Sensor-Geostatistical.pdf |date=2014-07-14 }}'', IET Circuits, Devices and Systems (CDS), Vol. 7, No. 5, Sep 2013, pp. 253-262.</ref>\n* [[Modelling of Microwave Devices]]<ref>{{cite journal|doi=10.1002/jnm.803 | volume=25 | title=Accurate modeling of microwave devices using kriging-corrected space mapping surrogates | year=2011 | journal=International Journal of Numerical Modelling: Electronic Networks, Devices and Fields | pages=1–14 | last1 = Koziel | first1 = Slawomir}}</ref>\n* [[Astronomy]]<ref>{{cite journal|doi=10.1093/mnras/stu937 | volume=442 | title=The SLUGGS survey: exploring the metallicity gradients of nearby early-type galaxies to large radii | year=2014 | journal=Monthly Notices of the Royal Astronomical Society | pages=1003-1039 | last1=Pastorello | first1=Nicola| arxiv=1405.2338 }}</ref><ref>{{cite journal|doi=10.1093/mnras/stv2947 | volume=457 | title=The SLUGGS survey: stellar kinematics, kinemetry and trends at large radii in 25 early-type galaxies | year=2016 | journal=Monthly Notices of the Royal Astronomical Society | pages=147-171 | last1=Foster | first1=Caroline| last2=Pastorello | first2=Nicola | last3=Roediger | first3=Joel | last4=Brodie | first4=Jean | last5=Forbes | first5=Duncan | last6=Kartha | first6=Sreeja | last7=Pota | first7=Vincenzo | last8=Romanowsky | first8=Aaron | last9=Spitler | first9=Lee | last10=Strader | first10=Jay | last11=Usher | first11=Christopher | last12=Arnold | first12=Jacob | arxiv=1512.06130 }}</ref><ref>{{cite journal|doi=10.1093/mnras/stx418 | volume=467 | title=The SLUGGS survey: using extended stellar kinematics to disentangle the formation histories of low-mass S) galaxies | year=2017 | journal=Monthly Notices of the Royal Astronomical Society | pages=4540-4557 | last1=Bellstedt | first1=Sabine | last2=Forbes | first2=Duncan | last3=Foster | first3=Caroline | last4=Romanowsky | first4=Aaron | last5=Brodie | first5=Jean | last6=Pastorello | first6=Nicola | last7=Alabi | first7=Adebusola | last8=Villaume | first8=Alexa}}</ref>\n\n===Design and analysis of computer experiments===\nAnother very important and rapidly growing field of application, in [[engineering]], is the interpolation of data coming out as response variables of deterministic computer simulations,<ref>{{cite journal |last1=Sacks |first1=J. |last2=Welch |first2=W.J. |last3=Mitchell |first3=T.J. |last4=Wynn |first4=H.P. |title=Design and Analysis of Computer Experiments |publisher=Statistical Science |volume=4 |number=4 |pages=409–435 |year=1989 |jstor=2245858}}</ref> e.g. [[finite element method]] (FEM) simulations. In this case, kriging is used as a [[metamodeling]] tool, i.e. a black box model built over a designed set of [[computer experiment]]s. In many practical engineering problems, such as the design of a [[metal forming]] process, a single FEM simulation might be several hours or even a few days long. It is therefore more efficient to design and run a limited number of computer simulations, and then use a kriging interpolator to rapidly predict the response in any other design point. Kriging is therefore used very often as a so-called [[surrogate model]], implemented inside [[optimization]] routines.<ref name=sheetforming>{{Cite journal |last1=Strano |first1=M. |doi=10.1007/s12289-008-0001-8 |title=A technique for FEM optimization under reliability constraint of process variables in sheet metal forming |journal=International Journal of Material Forming |volume=1 |issue=1 |pages=13–20 |date=March 2008 |pmid= |pmc=}}</ref>\n\n==See also==\n{{Commons category|Kriging}}\n* [[Bayes linear statistics]]\n* [[Gaussian process]]\n* [[Multivariate interpolation]]\n* [[Nonparametric regression]]\n* [[Radial basis function]]\n* [[Regression-kriging]]\n* [[Space mapping]]\n* [[Spatial dependence]]\n* [[Variogram]]\n* [[Gradient-Enhanced Kriging (GEK)]]\n* [[Surrogate model]]\n* [[Information field theory]]\n\n==References==\n<references />\n\n==Further reading==\n{{further cleanup|date=November 2014}}\n\n===Historical references===\n# {{cite book | last=Chilès | first=Jean-Paul | last2=Desassis | first2=Nicolas | title=Handbook of Mathematical Geosciences | chapter=Fifty Years of Kriging | publisher=Springer International Publishing | publication-place=Cham | year=2018 | isbn=978-3-319-78998-9 | doi=10.1007/978-3-319-78999-6_29 | ref=harv}}\n# Agterberg, F P, ''Geomathematics, Mathematical Background and Geo-Science Applications'', Elsevier Scientific Publishing Company, Amsterdam, 1974\n# Cressie, N. A. C., ''The origins of kriging, Mathematical Geology'', v. 22, pp 239–252, 1990\n# Krige, D.G, ''A statistical approach to some mine valuations and allied problems at the Witwatersrand'', Master's thesis of the University of Witwatersrand, 1951\n# Link, R F and Koch, G S, ''Experimental Designs and Trend-Surface Analsysis, Geostatistics'', A colloquium, Plenum Press, New York, 1970\n# Matheron, G., \"Principles of geostatistics\", ''Economic Geology'', 58, pp 1246–1266, 1963\n# Matheron, G., \"The intrinsic random functions, and their applications\", ''Adv. Appl. Prob''., 5, pp 439–468, 1973\n# Merriam, D F, Editor, ''Geostatistics'', a colloquium, Plenum Press, New York, 1970\n\n===Books===\n* Abramowitz, M., and Stegun, I. (1972), Handbook of Mathematical Functions, Dover Publications, New York.\n* Banerjee, S., Carlin, B.P. and Gelfand, A.E. (2004). Hierarchical Modeling and Analysis for Spatial Data. Chapman and Hall/CRC Press, Taylor and Francis Group.\n* Chiles, J.-P. and P. Delfiner (1999) ''Geostatistics, Modeling Spatial uncertainty'', Wiley Series in Probability and statistics.\n*Clark, I, and Harper, W.V., (2000) ''Practical Geostatistics 2000'', Ecosse North America, USA\n* Cressie, N (1993) ''Statistics for spatial data'', Wiley, New York\n* David, M (1988) ''Handbook of Applied Advanced Geostatistical Ore Reserve Estimation'', Elsevier Scientific Publishing\n* Deutsch, C.V., and Journel, A. G. (1992), GSLIB - Geostatistical Software Library and User's Guide, Oxford University Press, New York, 338 pp.\n* Goovaerts, P. (1997) ''Geostatistics for Natural Resources Evaluation'', Oxford University Press, New York {{ISBN|0-19-511538-4}}\n* Isaaks, E. H., and Srivastava, R. M. (1989), An Introduction to Applied Geostatistics, Oxford University Press, New York, 561 pp.\n* Journel, A. G. and C. J. Huijbregts (1978) ''Mining Geostatistics'', Academic Press London\n* Journel, A. G. (1989), Fundamentals of Geostatistics in Five Lessons, American Geophysical Union, Washington D.C.\n* {{Citation |last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 3.7.4. Interpolation by Kriging|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=144}}. Also, [http://apps.nrbook.com/empanel/index.html?pg=836 \"Section 15.9. Gaussian Process Regression\"].\n* Stein, M. L. (1999), ''Statistical Interpolation of Spatial Data: Some Theory for Kriging'', Springer, New York.\n* Wackernagel, H. (1995) ''Multivariate Geostatistics - An Introduction with Applications'', Springer Berlin\n\n[[Category:Geostatistics]]\n[[Category:Interpolation]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Lagrange polynomial",
      "url": "https://en.wikipedia.org/wiki/Lagrange_polynomial",
      "text": "[[Image:Lagrange polynomial.svg|thumb|upright=2|This image shows, for four points (<span style=\"color:#5e81B5;\">(&minus;9,&nbsp;5)</span>, <span style=\"color:#e19c24;\">(&minus;4,&nbsp;2)</span>, <span style=\"color:#8FB131;\">(&minus;1,&nbsp;&minus;2)</span>, <span style=\"color:#EC6235;\">(7,&nbsp;9)</span>), the (cubic) interpolation polynomial <span style=\"color:black;\">''L''(''x'')</span> (dashed, black), which is the sum of the ''scaled'' basis polynomials <span style=\"color:#5e81B5;\">y<sub>0</sub>''ℓ''<sub>0</sub>(''x'')</span>, <span style=\"color:#e19c24;\">y<sub>1</sub>''ℓ''<sub>1</sub>(''x'')</span>, <span style=\"color:#8FB131;\">y<sub>2</sub>''ℓ''<sub>2</sub>(''x'')</span> and <span style=\"color:#EC6235;\">y<sub>3</sub>''ℓ''<sub>3</sub>(''x'')</span>. The interpolation polynomial passes through all four control points, and each ''scaled'' basis polynomial passes through its respective control point and is 0 where ''x'' corresponds to the other three control points.]]\n\nIn [[numerical analysis]], '''Lagrange polynomials''' are used for [[polynomial interpolation]]. For a given set of points <math>(x_j,y_j)</math> with no two <math>x_j</math> values equal, the Lagrange polynomial is the polynomial of lowest [[degree of a polynomial|degree]] that assumes at each value <math>x_j</math> the corresponding value <math>y_j</math> (i.e. the functions coincide at each point).\nThe interpolating polynomial of the least degree is unique, however, and since it can be arrived at through multiple methods, referring to \"the Lagrange polynomial\" is perhaps not as correct as referring to \"the Lagrange form\" of that unique polynomial.\n\nAlthough named after [[Joseph Louis Lagrange]], who published it in 1795, the method was first discovered in 1779 by [[Edward Waring]]<ref>{{cite journal\n |title=Problems concerning interpolations\n |first=Edward |last=Waring |authorlink=Edward Waring\n |journal=[[Philosophical Transactions of the Royal Society]]\n |date=9 January 1779 |volume=69 |pages=59–67\n |doi=10.1098/rstl.1779.0008\n |url=http://rstl.royalsocietypublishing.org/content/69/59.full.pdf\n}}</ref> It is also an easy consequence of a formula published in 1783 by [[Leonhard Euler]].<ref>{{Cite journal\n | last1=Meijering | first1=Erik\n | title=A chronology of interpolation: from ancient astronomy to modern signal and image processing\n | doi=10.1109/5.993400 | year=2002\n | journal=Proceedings of the IEEE | volume=90 | issue=3 | pages=319–342\n | url = http://bigwww.epfl.ch/publications/meijering0201.pdf}}</ref>\n\nUses of Lagrange polynomials include the [[Newton–Cotes formulas|Newton–Cotes method]] of [[numerical integration]] and [[Shamir's Secret Sharing|Shamir's secret sharing scheme]] in [[cryptography]].\n\nLagrange interpolation is susceptible to [[Runge's phenomenon]] of large oscillation. As changing the points <math>x_j</math> requires recalculating the entire interpolant, it is often easier to use [[Newton polynomials]] instead.\n\n==Definition==\n[[File:Lagrange basis functions.svg|thumb|Here we plot the Lagrange basis functions of 1st, 2nd, and 3rd order on a bi-unit domain. Linear combinations of Lagrange basis functions are used to construct Lagrange interpolating polynomials. Lagrange basis functions are commonly used in [[finite element analysis]] as the bases for the element shape-functions. Furthermore, it is common to use a bi-unit domain as the natural space for the finite-element's definition.]]\n\nGiven a set of ''k''&nbsp;+&nbsp;1 data points\n:<math>(x_0, y_0),\\ldots,(x_j, y_j),\\ldots,(x_k, y_k)</math>\nwhere no two <math>x_j</math> are the same, the '''interpolation polynomial in the Lagrange form''' is a [[linear combination]]\n:<math>L(x) := \\sum_{j=0}^{k} y_j \\ell_j(x)</math>\nof Lagrange basis polynomials\n:<math>\\ell_j(x) := \\prod_{\\begin{smallmatrix}0\\le m\\le k\\\\ m\\neq j\\end{smallmatrix}} \\frac{x-x_m}{x_j-x_m} = \\frac{(x-x_0)}{(x_j-x_0)} \\cdots \\frac{(x-x_{j-1})}{(x_j-x_{j-1})} \\frac{(x-x_{j+1})}{(x_j-x_{j+1})} \\cdots \\frac{(x-x_k)}{(x_j-x_k)},</math>\nwhere <math>0\\le j\\le k</math>. Note how, given the initial assumption that no two <math>x_j</math> are the same, <math>x_j - x_m \\neq 0</math>, so this expression is always well-defined. The reason pairs <math>x_i = x_j</math> with <math>y_i\\neq y_j</math> are not allowed is that no interpolation function <math>L</math> such that <math>y_i = L(x_i)</math> would exist; a function can only get one value for each argument <math>x_i</math>. On the other hand, if also <math>y_i = y_j</math>, then those two points would actually be one single point.\n\nFor all <math>i\\neq j</math>, <math>\\ell_j(x)</math> includes the term <math>(x-x_i)</math> in the numerator, so the whole product will be zero at <math>x=x_i</math>:\n:<math>\\ell_{j\\ne i}(x_i) = \\prod_{m\\neq j} \\frac{x_i-x_m}{x_j-x_m} = \\frac{(x_i-x_0)}{(x_j-x_0)} \\cdots \\frac{(x_i-x_i)}{(x_j-x_i)} \\cdots \\frac{(x_i-x_k)}{(x_j-x_k)} = 0.</math>\n\nOn the other hand,\n:<math>\\ell_i(x_i) := \\prod_{m\\neq i} \\frac{x_i-x_m}{x_i-x_m} = 1</math>\n\nIn other words, all basis polynomials are zero at <math>x=x_i</math>, except <math>\\ell_i(x)</math>, for which it holds that <math>\\ell_i(x_i)=1</math>, because it lacks the <math>(x-x_i)</math> term.\n\nIt follows that <math>y_i \\ell_i(x_i)=y_i</math>, so at each point <math>x_i</math>, <math>L(x_i)=y_i+0+0+\\dots +0=y_i</math>, showing that <math>L</math> interpolates the function exactly.\n\n==Proof==\n\nThe function ''L''(''x'') being sought is a polynomial in <math>x</math> of the least degree that interpolates the given data set; that is, assumes value <math>y_j</math> at the corresponding <math>x_j</math> for all data points <math>j</math>:\n\n:<math>L(x_j) = y_j \\qquad j=0,\\ldots,k</math>\n\nObserve that:\n# In <math>\\ell_j(x)</math> there are ''k'' factors in the product and each factor contains one ''x'', so ''L''(''x'') (which is a sum of these ''k''-degree polynomials) must be a polynomial of degree at most ''k''.\n# <math>\\ell_j(x_i)\n= \\prod_{m=0,\\, m\\neq j}^{k} \\frac{x_i-x_m}{x_j-x_m}\n</math>\n\nWe consider what happens when this product is expanded.  Because the product skips <math>m = j</math>, if <math>i = j</math> then all terms are <math>\\frac{x_j-x_m}{x_j-x_m} = 1</math> (except where <math>x_j = x_m</math>, but that case is impossible, as pointed out in the definition section—in that term, <math>m=j</math>, and since <math>m\\neq j</math>, <math>i\\neq j</math>, contrary to <math>i=j</math>).\nAlso if <math>i \\neq j</math> then since <math>m \\neq j</math> does not preclude it, one term in the product '''will''' be for <math>m=i</math>, i.e. <math>\\frac{x_i-x_i}{x_j-x_i} = 0</math>, zeroing the entire product. So\n# <math>\\ell_j(x_i)\n = \\delta_{ji} = \\begin{cases} \n1, & \\text{if } j=i   \\\\ \n0, & \\text{if } j \\ne i \\end{cases}\n</math>\n\nwhere <math>\\delta_{ij}</math> is the [[Kronecker delta]]. So:\n\n: <math>L(x_i) = \\sum_{j=0}^k y_j \\ell_j(x_i) = \\sum_{j=0}^{k} y_j \\delta_{ji} = y_i.</math>\n\nThus the function ''L''(''x'') is a polynomial with degree at most ''k'' and where <math>L(x_i) = y_i</math>.\n\nAdditionally, the interpolating polynomial is unique, as shown by the unisolvence theorem at the [[Polynomial interpolation#Uniqueness of the interpolating polynomial|polynomial interpolation]] article.\n\n==A perspective from linear algebra==\n\nSolving an [[Polynomial interpolation#Constructing the interpolation polynomial|interpolation problem]] leads to a problem in [[linear algebra]] amounting to inversion of a matrix. Using a standard [[monomial basis]] for our interpolation polynomial <math>L(x) = \\sum_{j=0}^k x^j m_j</math>, we must invert the [[Vandermonde matrix]] <math>(x_i)^j</math> to solve <math>L(x_i) = y_i</math> for the coefficients <math>m_j</math> of <math>L(x)</math>. By choosing a better basis, the Lagrange basis,  <math>L(x) = \\sum_{j=0}^k l_j(x) y_j</math>, we merely get the [[identity matrix]], [[Kronecker delta|<math>\\delta_{ij}</math>]], which is its own inverse: the Lagrange basis automatically ''inverts'' the analog of the Vandermonde matrix.\n\nThis construction is analogous to the [[Chinese Remainder Theorem]]. Instead of checking for remainders of integers modulo prime numbers, we are checking for remainders of polynomials when divided by linears.\n\n\nFurthermore, when the order is large, [[Fast Fourier transform|Fast Fourier Transformation]] can be used to solve for the coefficients of the interpolated polynomial.\n\n==Examples==\n\n===Example 1===\nWe wish to interpolate ''ƒ''(''x'')&nbsp;=&nbsp;''x''<sup>2</sup> over the range 1&nbsp;≤&nbsp;''x''&nbsp;≤&nbsp;3, given these three points:\n\n: <math>\n\\begin{align}\nx_0 & = 1 & & & f(x_0) & = 1 \\\\\nx_1 & = 2 & & & f(x_1) & = 4 \\\\\nx_2 & = 3 & & & f(x_2) & =9.\n\\end{align}\n</math>\n\nThe interpolating polynomial is:\n:<math> \\begin{align}\nL(x) &= {1}\\cdot{x - 2 \\over 1 - 2}\\cdot{x - 3 \\over 1 - 3}+{4}\\cdot{x - 1 \\over 2 - 1}\\cdot{x - 3 \\over 2 - 3}+{9}\\cdot{x - 1 \\over 3 - 1}\\cdot{x - 2 \\over 3 - 2} \\\\[10pt]\n&= x^2.\n\\end{align} </math>\n\n===Example 2===\nWe wish to interpolate ''ƒ''(''x'')&nbsp;=&nbsp;''x''<sup>3</sup> over the range 1&nbsp;≤&nbsp;''x''&nbsp;≤&nbsp;3, given these three points:\n{| cellpadding=10px\n|-\n| <math>x_0=1</math>  || <math>f(x_0)=1</math>\n|-\n| <math>x_1=2</math> || <math>f(x_1)=8</math>\n|-\n| <math>x_2=3</math>     || <math>f(x_2)=27</math>\n|}\n\nThe interpolating polynomial is:\n:<math> \\begin{align}\nL(x) &= {1}\\cdot{x - 2 \\over 1 - 2}\\cdot{x - 3 \\over 1 - 3}+{8}\\cdot{x - 1 \\over 2 - 1}\\cdot{x - 3 \\over 2 - 3}+{27}\\cdot{x - 1 \\over 3 - 1}\\cdot{x - 2 \\over 3 - 2} \\\\[8pt]\n&=  6x^2 - 11x + 6.\n\\end{align} </math>\n\n===Notes===\n[[File:Runge's phenomenon in Lagrange polynomials.svg|thumb|upright=2|Example of interpolation divergence for a set of Lagrange polynomials.]]\n\nThe Lagrange form of the interpolation polynomial shows the linear character of polynomial interpolation and the uniqueness of the interpolation polynomial.  Therefore, it is preferred in proofs and theoretical arguments.  Uniqueness can also be seen from the invertibility of the Vandermonde matrix, due to the non-vanishing of the [[Vandermonde determinant]].\n\nBut, as can be seen from the construction, each time a node ''x''<sub>''k''</sub> changes, all Lagrange basis polynomials have to be recalculated. A better form of the interpolation polynomial for practical (or computational) purposes is the barycentric form of the Lagrange interpolation (see below) or [[Newton polynomial]]s. <!-- Using [[Horner scheme|nested multiplication]] amounts to the same idea. -->\n\nLagrange and other interpolation at equally spaced points, as in the example above, yield a polynomial oscillating above and below the true function. This behaviour tends to grow with the number of points, leading to a divergence known as [[Runge's phenomenon]]; the problem may be eliminated by choosing interpolation points at [[Chebyshev nodes]].<ref>{{cite book|title=Scientific Computing with MATLAB|volume=2|series=Texts in computational science and engineering|first1=Alfio|last1=Quarteroni|first2=Fausto|last2=Saleri|publisher=Springer|year=2003|isbn=978-3-540-44363-6|page=66|url=https://books.google.com/books?id=fE1W5jsU4zoC&pg=PA66}}.</ref>\n\nThe Lagrange basis polynomials can be used in [[numerical integration]] to derive the [[Newton–Cotes formulas]].\n<!-- Lagrange interpolation is often used in [[digital signal processing]] of audio for the implementation of fractional delay [[finite impulse response|FIR]] filters (e.g., to precisely tune [[digital waveguide synthesis|digital waveguides]] in [[physical modelling synthesis]]). -->\n\n==Barycentric form==\n\nUsing\n\n:<math>\\ell(x) = (x - x_0)(x - x_1) \\cdots (x - x_k)</math>\n:<math>\\ell'(x_j) = \\frac{\\mathrm{d} \\ell(x)}{\\mathrm{d} x}\\Big|_{x=x_j} = \\prod_{i=0,i \\neq j}^k(x_j-x_i) </math>\n\nwe can rewrite the Lagrange basis polynomials as\n\n:<math>\\ell_j(x) = \\frac{\\ell(x)}{\\ell'(x_j)(x-x_j)} </math>\nor, by defining the ''barycentric weights''<ref>{{cite journal\n | first1 = Jean-Paul | last1 = Berrut\n | first2 = Lloyd N. | last2 = Trefethen |authorlink = Lloyd N. Trefethen\n | year = 2004\n | title = Barycentric Lagrange Interpolation\n | journal = SIAM Review\n | volume = 46\n | issue = 3\n | pages = 501&ndash;517\n | doi = 10.1137/S0036144502417715\n | url = https://people.maths.ox.ac.uk/trefethen/barycentric.pdf\n }}</ref>\n\n:<math>w_j = \\frac{1}{\\ell'(x_j)}</math>\n\nwe can simply write\n\n:<math>\\ell_j(x) = \\ell(x)\\frac{w_j}{x-x_j}</math>\n\nwhich is commonly referred to as the ''first form'' of the barycentric interpolation formula.\n\nThe advantage of this representation is that the interpolation polynomial may now be evaluated as\n\n:<math>L(x) = \\ell(x) \\sum_{j=0}^k \\frac{w_j}{x-x_j}y_j</math>\n\nwhich, if the weights <math>w_j</math> have been pre-computed, requires only <math>\\mathcal O(n)</math> operations (evaluating <math>\\ell(x)</math> and the weights <math>w_j/(x-x_j)</math>) as opposed to <math>\\mathcal O(n^2)</math> for evaluating the Lagrange basis polynomials <math>\\ell_j(x)</math> individually.\n\nThe barycentric interpolation formula can also easily be updated to incorporate a new node <math>x_{k+1}</math> by dividing each of the <math>w_j</math>, <math>j=0 \\dots k</math> by <math>(x_j - x_{k+1})</math> and constructing the new <math>w_{k+1}</math> as above.\n\nWe can further simplify the first form by first considering the barycentric interpolation of the constant function <math>g(x)\\equiv 1</math>:\n\n:<math>g(x) = \\ell(x) \\sum_{j=0}^k \\frac{w_j}{x-x_j}.</math>\n\nDividing <math>L(x)</math> by <math>g(x)</math> does not modify the interpolation, yet yields\n\n:<math>L(x) = \\frac{\\sum_{j=0}^k \\frac{w_j}{x-x_j}y_j}{\\sum_{j=0}^k \\frac{w_j}{x-x_j}}</math>\n\nwhich is referred to as the ''second form'' or ''true form'' of the barycentric interpolation formula. This second form has the advantage that <math>\\ell(x)</math> need not be evaluated for each evaluation of <math>L(x)</math>.\n\n==Remainder in Lagrange interpolation formula==\nWhen interpolating a given function ''f'' by a polynomial of degree {{mvar|k}} at the nodes <math>x_0,...,x_k</math> we get the remainder <math>R(x) = f(x) - L(x)</math> which can be expressed as<ref>{{AS ref|25, eqn 25.2.3|878}}</ref>\n\n:<math> R(x) = f[x_0,\\ldots,x_k,x] \\ell(x) = \\ell(x) \\frac{f^{k+1}(\\xi)}{(k+1)!},  \\quad \\quad x_0 < \\xi < x_k,</math>\n\nwhere <math>f[x_0,\\ldots,x_k,x]</math> is the notation for [[divided differences]]. Alternatively, the remainder can be expressed as a contour integral in complex domain as\n\n:<math>R(z) = \\frac{\\ell(z)}{2\\pi i} \\int_C \\frac{f(t)}{(t-z)(t-z_0) \\cdots (t-z_k)} dt = \\frac{\\ell(z)}{2\\pi i} \\int_C \\frac{f(t)}{(t-z)\\ell(t)} dt.</math>\n\nThe remainder can be bound as\n\n:<math>|R(x)| \\leq \\frac{(x_k-x_0)^{k+1}}{(k+1)!}\\max_{x_0 \\leq \\xi \\leq x_k} |f^{(k+1)}(\\xi)|. </math>\n\n=== Derivation<ref>{{Cite web|url=https://sam.nitk.ac.in/sites/default/Numerical_Methods/Interpolation/interpolation.pdf|title=Interpolation|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>===\nClearly, <math>R(x) </math> is zero at nodes. Suppose we want to find <math>R(x)</math> at a point <math>x_p  </math>. Define a new function <math>F(x)=f(x)-L(x)-R(x)</math> and choose <math>R(x)=C\\cdot\\prod_{i=0}^k(x-x_i)</math> (This ensures <math>R(x)=0 </math> at nodes) where <math>C</math> is the constant we are required to determine for a given <math>x_p</math>. Now <math>F(x)</math> has <math>k+2</math> zeroes (at all nodes and <math>x_p</math>) between <math>x_0</math> and <math>x_k</math> (including endpoints). Let us assume that <math>f(x)</math> is <math>k+1</math>-times differentiable, <math>L(x)</math> and <math>R(x)</math> are polynomials hence are infinitely differentiable, by [[Rolle's theorem|Rolle's theoram]] <math>F^{(1)}(x)</math> has <math>k+1</math> zeroes, <math>F^{(2)}(x)</math> has <math>k</math> zeroes... <math>F^{(k+1)}</math> has 1 zero, say <math>\\xi,\\, x_0<\\xi<x_k</math>. Explicitly writing <math>F^{(k+1)}(\\xi)</math>:\n\n:<math>F^{(k+1)}(\\xi)=f^{(k+1)}(\\xi)-L^{(k+1)}(\\xi)-R^{(k+1)}(\\xi)</math>\n\nwe have:\n\n:<math>L^{(k+1)}=0,R^{(k+1)}=C\\cdot(k+1)!</math> (Because the highest power of <math>x</math> in <math>R(x)</math> is <math>k+1</math>)\n\nWe get:\n\n:<math>0=f^{(k+1)}(\\xi)-C\\cdot(k+1)!</math>\n\nRearranging:\n\n:<math>C=\\frac{f^{(k+1)}(\\xi)}{(k+1)!}</math>\n\n==Derivatives==\nThe <math>d</math>th derivatives of the Lagrange polynomial can be written as\n\n:<math>L^{(d)}(x) := \\sum_{j=0}^{k} y_j \\ell_j^{(d)}(x)</math>.\n\nFor the first derivative, the coefficients are given by\n\n:<math>\\ell_j^{(1)}(x) := \\sum_{i=0, i\\not=j}^k \\left[ \\frac{1}{x_j-x_i}\\prod_{m=0,m\\not = (i , j)}^k \\frac{x-x_m}{x_j-x_m} \\right]</math>\n\nand for the second derivative \n\n:<math>\\ell^{(2)}_j(x) := \\sum_{i=0, i\\ne j}^{k} \\frac{1}{x_j-x_i} \\left[ \\sum_{m=0,m\\ne(i,j)}^{k} \\left( \\frac{1}{x_j-x_m}\\prod_{l=0, l\\ne(i,j,m)}^{k} \\frac{x-x_l}{x_j-x_l} \\right) \\right] </math>.\n\nThrough recursion, one can compute formulas for higher derivatives.\n\n==Finite fields==\nThe Lagrange polynomial can also be computed in [[finite field]]s.  This has applications in [[cryptography]], such as in [[Shamir's Secret Sharing]] scheme.\n\n==See also==\n*[[Neville's algorithm]]\n*[[Newton polynomial|Newton form]] of the interpolation polynomial\n*[[Bernstein polynomial]]\n*[[Carlson's theorem]]\n*[[Lebesgue constant (interpolation)]]\n*[[Chebfun|The Chebfun system]]\n*[[Table of Newtonian series]]\n*[[Frobenius covariant]]\n*[[Sylvester's formula]]\n*[[Finite difference coefficient]]\n\n==References==\n{{reflist}}\n\n==External links==\n* {{springer|title=Lagrange interpolation formula|id=p/l057170}}\n* [http://www.alglib.net/interpolation/polynomial.php ALGLIB] has an implementations in C++ / C# / VBA / Pascal.\n* [https://www.gnu.org/software/gsl/ GSL] has a polynomial interpolation code in C\n* [https://stackoverflow.com/questions/11029615/lagrange-interpolation-method/11552763 SO] has a MATLAB example that demonstrates the algorithm and recreates the first image in this article\n* [http://numericalmethods.eng.usf.edu/topics/lagrange_method.html Lagrange Method of Interpolation &mdash; Notes, PPT, Mathcad, Mathematica, MATLAB, Maple] at [http://numericalmethods.eng.usf.edu Holistic Numerical Methods Institute]\n*[http://www.math-linux.com/spip.php?article71 Lagrange interpolation polynomial] on www.math-linux.com\n* {{MathWorld|urlname=LagrangeInterpolatingPolynomial|title=Lagrange Interpolating Polynomial}}\n{{ProofWiki|id=Lagrange_Polynomial_Approximation|title=Estimate of the error in Lagrange Polynomial Approximation}}\n* [http://jsxgraph.uni-bayreuth.de/wiki/index.php/Lagrange_interpolation Dynamic Lagrange interpolation with JSXGraph]\n* Numerical computing with functions: [https://web.archive.org/web/20101013180326/http://www2.maths.ox.ac.uk/chebfun/ The Chebfun Project]\n* [http://mathformeremortals.wordpress.com/2013/01/15/bicubic-interpolation-excel-worksheet-function/ Excel Worksheet Function for Bicubic Lagrange Interpolation]\n* [http://pastebin.com/bNVcQt4x Lagrange polynomials in Python]\n\n{{DEFAULTSORT:Lagrange Polynomial}}\n[[Category:Interpolation]]\n[[Category:Polynomials]]\n[[Category:Articles containing proofs]]\n\n[[de:Lagrange-Polynom]]\n[[he:אינטרפולציה#צורת לגראנז']]"
    },
    {
      "title": "Lebesgue constant (interpolation)",
      "url": "https://en.wikipedia.org/wiki/Lebesgue_constant_%28interpolation%29",
      "text": "{{More footnotes|date=May 2017}}{{other uses|Lebesgue constant (disambiguation)}}\n\nIn [[mathematics]], the '''Lebesgue constants''' (depending on a set of nodes and of its size) give an idea of how good the [[interpolation|interpolant]] of a [[Function (mathematics)|function]] (at the given nodes) is in comparison with the best [[polynomial]] [[approximation]] of the function (the degree of the polynomials are obviously fixed).  The Lebesgue constant for polynomials of degree at most {{mvar|n}} and for the set of {{math|''n'' + 1}} nodes {{mvar|T}} is generally denoted by {{math|Λ<sub>''n''</sub>(''T''&thinsp;)}}.  These constants are named after [[Henri Lebesgue]].\n\n==Definition==\nWe fix the interpolation nodes <math>x_0, ..., x_n</math>and an [[Interval (mathematics)|interval]] <math>[a,\\,b]</math>containing all the interpolation nodes. The process of interpolation maps the function <math>f</math> to a polynomial <math>p</math>. This defines a mapping <math>X</math> from the space ''C''([''a'', ''b'']) of all continuous functions on [''a'', ''b''] to itself. The map ''X'' is linear and it is a [[projection (linear algebra)|projection]] on the subspace {{math|Π<sub>''n''</sub>}} of polynomials of degree {{mvar|n}} or less.\n\nThe Lebesgue constant <math>\\Lambda_n(T)</math> is defined as the [[operator norm]] of ''X''. This definition requires us to specify a norm on ''C''([''a'', ''b'']).  The [[uniform norm]] is usually the most convenient.\n\n==Properties==\nThe Lebesgue constant bounds the interpolation error: let {{math|''p''<sup>∗</sup>}} denote the best approximation of ''f'' among the polynomials of degree {{mvar|n}} or less. In other words, {{math|''p''<sup>∗</sup>}} minimizes {{math|{{!!}}&thinsp;''p'' − &thinsp;''f''&thinsp;{{!!}}}} among all ''p'' in Π<sub>''n''</sub>. Then\n\n:<math> \\|f-X(f)\\| \\le (\\Lambda_n(T)+1) \\left \\|f-p^* \\right \\|. </math>\n\nWe will here prove this statement with the maximum norm.\n\n:<math> \\| f-X(f) \\| \\le \\| f-p^* \\| + \\| p^* - X(f) \\|</math>\n\nby the [[triangle inequality]]. But ''X'' is a projection on Π<sub>''n''</sub>, so\n\n:{{math|''p''<sup>∗</sup> − ''X''(&thinsp;''f''&thinsp;) {{=}} ''X''(''p''<sup>∗</sup>) − ''X''(&thinsp;''f''&thinsp;) {{=}} ''X''(''p''<sup>∗</sup> − ''f''&thinsp;)}}.\n\nThis finishes the proof since <math>\\|X(p^*-f)\\| \\le \\|X\\| \\|p^*-f\\|=\\|X\\| \\|f-p^*\\|</math>.  Note that this relation comes also as a special case of [[Lebesgue's lemma]].\n\nIn other words, the interpolation polynomial is at most a factor {{math|Λ<sub>''n''</sub>(''T''&thinsp;) + 1}} worse than the best possible approximation. This suggests that we look for a set of interpolation nodes with a small Lebesgue constant.\n\nThe Lebesgue constant can be expressed in terms of the [[Lagrange polynomial|Lagrange basis]] polynomials:\n\n:<math>l_j(x) := \\prod_{\\begin{smallmatrix}i=0\\\\ j\\neq i\\end{smallmatrix}}^{n} \\frac{x-x_i}{x_j-x_i}. </math>\n\nIn fact, we have the Lebesgue function\n\n:<math> \\lambda_n(x) = \\sum_{j=0}^n |l_j(x)|. </math>\n\nand the Lebesgue constant (or Lebesgue number) for the grid is its maximum value\n\n:<math>\\Lambda_n(T)=\\max_{x\\in[a,b]} \\lambda_n(x) </math>\n\nNevertheless, it is not easy to find an explicit expression for {{math|Λ<sub>''n''</sub>(''T''&thinsp;)}}.\n\n==Minimal Lebesgue constants==\nIn the case of equidistant nodes, the Lebesgue constant [[exponential growth|grows exponentially]]. More precisely, we have the following asymptotic estimate\n\n:<math> \\Lambda_n(T) \\sim \\frac{2^{n+1}}{e \\, n \\log n} \\qquad \\text{ as } n \\to \\infty. </math>\n\nOn the other hand, the Lebesgue constant grows only logarithmically if [[Chebyshev nodes]] are used, since we have\n\n:<math> \\tfrac{2}{\\pi} \\log(n+1)+a < \\Lambda_n(T) < \\tfrac{2}{\\pi} \\log(n+1) + 1, \\qquad a = 0.9625\\cdots</math>\n\nWe conclude again that Chebyshev nodes are a very good choice for polynomial interpolation.  However, there is an easy (linear) transformation of Chebyshev nodes that gives a better Lebesgue constant.  Let {{math|''t<sub>i</sub>''}} denote the {{mvar|i}}-th Chebyshev node.  Then, define\n\n:<math> s_i = \\frac{t_i}{\\cos \\left ( \\frac{\\pi}{2(n+1)} \\right)}.</math>\n\nFor such nodes:\n\n:<math>\\Lambda_n(S)<\\tfrac{2}{\\pi} \\log(n+1)+b, \\qquad b = 0.7219\\cdots</math>\n\nThose nodes are, however, not optimal (i.e. they do not minimize the Lebesgue constants) and the search for an optimal set of nodes (which has already been proved to be unique under some assumptions) is still an intriguing topic in mathematics today.  However, this set of nodes is optimal for interpolation over <math> C_M^n[-1,1]</math> the set of {{mvar|n}} times differentiable functions whose {{mvar|n}}-th derivatives are bounded in absolute values by a constant {{mvar|M}} as shown by N. S. Hoang. \nUsing a [[computer]], one can approximate the values of the minimal Lebesgue constants, here for the canonical interval {{math|[−1, 1]}}:\n\n:{| class=\"wikitable\"\n|- style=\"text-align:center\"\n! {{mvar|n}}\n| 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9\n|-\n! Λ<sub>''n''</sub>(''T'')\n| 1.0000 || 1.2500 || 1.4229 || 1.5595 || 1.6722 || 1.7681  || 1.8516 || 1.9255 || 1.9917 \n|}\n\nThere are uncountable infinitely many sets of nodes in [-1,1] that minimize, for fixed {{mvar|n}} > 1, the Lebesgue constant.  Though if we assume that we always take −1 and 1 as nodes for interpolation (which is called a ''canonical'' node configuration), then such a set is unique and zero-symmetric.  To illustrate this property, we shall see what happens when ''n'' = 2 (i.e. we consider 3 interpolation nodes in which case the property is not trivial).  One can check that each set of (zero-symmetric) nodes of type {{math|(−''a'', 0, ''a'')}} is optimal when {{math|{{sfrac|{{radic|8}}|3}} ≤ ''a'' ≤ 1}} (we consider only nodes in [−1, 1]).  If we force the set of nodes to be of the type {{math|(−1, ''b'', 1)}}, then ''b'' must equal 0 (look at the Lebesgue function, whose maximum is the Lebesgue constant).  All ''arbitrary'' (i.e. zero-symmetric or zero-asymmetric) optimal sets of nodes in [-1,1] when ''n'' = 2 have been determined by F. Schurer, and in an alternative fashion by H.-J. Rack and R. Vajda (2014).\n\nIf we assume that we take −1 and 1 as nodes for interpolation, then as shown by H.-J. Rack (1984 and 2013), for the case ''n'' = 3, the explicit values of the optimal (unique and zero-symmetric) 4 interpolation nodes and the explicit value of the minimal Lebesgue constant are known. All ''arbitrary'' optimal sets of 4 interpolation nodes in [-1,1] when ''n'' = 3 have been explicitly determined, in two different but equivalent fashions, by H.-J. Rack and R. Vajda (2015).\n\nThe [[Padua points]] provide another set of nodes with slow growth (although not as slow as the Chebyshev nodes) and with the additional property of being a [[unisolvent point set]].\n\n==Sensitivity of the values of a polynomial==\nThe Lebesgue constants also arise in another problem.  Let ''p''(''x'') be a polynomial of degree {{mvar|n}} expressed in the [[Lagrange polynomial|Lagrangian form]] associated with the points in the vector ''t'' (i.e. the vector ''u'' of its coefficients is the vector containing the values <math>p(t_i)</math>).  Let <math>\\hat{p}(x)</math> be a polynomial obtained by slightly changing the coefficients ''u'' of the original polynomial ''p''(''x'') to <math>\\hat{u}</math>.  Consider the inequality:\n\n:<math> \\frac{\\|p-\\hat{p}\\|}{\\|p\\|}\\leq \\Lambda_n(T)\\frac{\\|u-\\hat{u}\\|}{\\|u\\|}</math>\n\nThis means that the (relative) error in the values of <math>\\hat{p}(x)</math> will not be higher than the appropriate Lebesgue constant times the relative error in the coefficients. In this sense, the Lebesgue constant can be viewed as the relative [[condition number]] of the operator mapping each coefficient vector ''u'' to the set of the values of the polynomial with coefficients ''u'' in the Lagrange form.  We can actually define such an operator for each polynomial basis but its condition number is greater than the optimal Lebesgue constant for most convenient bases.\n\n==References==\n* {{Citation\n | last = Brutman\n | first = L.\n | title = Lebesgue functions for polynomial interpolation &mdash; a survey<!--\n | dedication = The heritage of P. L. Chebyshev: A Festschrift in honor of the 70th birthday of T. J. Rivlin-->\n | journal = Annals of Numerical Mathematics\n | volume = 4\n | year = 1997\n | pages = 111–127\n | issn = 1021-2655\n}}\n* {{Citation\n | last = Smith\n | first = Simon J.\n | author-link=\n | title = Lebesgue constants in polynomial interpolation\n | journal = Annales Mathematicae et Informaticae\n | volume = 33\n | year = 2006\n | pages = 109–123\n | issn = 1787-5021\n | url = http://www.emis.de/journals/AMI/2006/smith.pdf\n}}\n* {{Citation\n | last = Ibrahimoglu\n | first = Bayram Ali\n | author-link=\n | title = Lebesgue functions and Lebesgue constants in polynomial interpolation\n | journal = Journal of Inequalities and Applications\n | year = 2016\n | pages = 2016:93\n | issn = 1029-242X\n | doi = 10.1186/s13660-016-1030-3\n}}\n* {{Citation\n | last = Rack\n | first = H.-J.\n | author-link=\n | title = An example of optimal nodes for interpolation\n | journal = International Journal of Mathematical Education in Science and Technology\n | volume = 15\n | issue = 3\n | year = 1984\n | pages = 355–357\n | issn = 1464-5211\n | url = http://www.informaworld.com/openurl?genre=article&issn=0020%2d739X&volume=15&issue=3&spage=355\n | doi = 10.1080/0020739840150312\n}}\n* {{Citation\n | last = Rack\n | first = H.-J.\n | author-link=\n | title = An example of optimal nodes for interpolation revisited\n | journal = Advances in Applied Mathematics and Approximation Theory\n | series = Springer Proceedings in Mathematics and Statistics\n | volume = 41\n | year = 2013\n | pages = 117–120\n | issn = 2194-1009\n | doi = 10.1007/978-1-4614-6393-1_7\n}}\n* {{Citation\n | last1 = Rack\n | first1 = H.-J.\n | last2 = Vajda\n | first2 = R.\n | title = On optimal quadratic Lagrange interpolation: Extremal node systems with minimal Lebesgue constant via symbolic computation\n | journal = Serdica Journal of Computing\n | volume = 8\n | year = 2014\n | pages = 71–96\n | issn = 1312-6555\n | url = http://serdica-comp.math.bas.bg/index.php/serdicajcomputing/article/view/214\n}}\n* {{Citation\n | last1 = Rack\n | first1 = H.-J.\n | last2 = Vajda\n | first2 = R.\n | title = On optimal cubic Lagrange interpolation: Extremal node systems with minimal Lebesgue constant\n | journal = Studia Universitatis Babeş-Bolyai Mathematica\n | volume = 60\n | issue = 2\n | year = 2015\n | pages = 151–171\n | issn = 0252-1938\n | url = http://www.cs.ubbcluj.ro/~studia-m/2015-2/01-Rack-Vajda-final.pdf\n}}\n* {{Citation\n | last = Schurer\n | first = F.\n | title = A remark on extremal sets in the theory of polynomial interpolation\n | journal = Studia Scientiarum Mathematicarum Hungarica\n | volume = 9\n | year = 1974\n | pages = 77–79\n | issn = 0081-6906\n}}\n* {{Citation\n | last = Hoang\n | first = N. S.\n | title = On node distribution for interpolation and spectral methods. \n | arxiv = 1305.6104| bibcode = 2013arXiv1305.6104H}}\n* [http://mathworld.wolfram.com/LebesgueConstants.html Lebesgue constants] on [[MathWorld]].\n\n[[Category:Interpolation]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Linear interpolation",
      "url": "https://en.wikipedia.org/wiki/Linear_interpolation",
      "text": "[[Image:LinearInterpolation.svg|thumb|right|300px|Given the two red points, the blue line is the linear interpolant between the points, and the value ''y'' at ''x'' may be found by linear interpolation.]]\nIn mathematics, '''linear interpolation''' is a method of [[curve fitting]] using [[linear polynomial]]s to construct new data points within the range of a discrete set of known data points.\n\n==Linear interpolation between two known points==\n[[Image:Linear interpolation visualisation.svg|thumb|In this geometric visualisation, the value at the green circle multiplied by the horizontal distance between the red and blue circles is equal to the sum of the value at the red circle multiplied by the horizontal distance between the green and blue circles, and the value at the blue circle multiplied by the horizontal distance between the green and red circles.]]\nIf the two known points are given by the coordinates <math>(x_0,y_0)</math> and <math>(x_1,y_1)</math>, the '''linear interpolant''' is the straight line between these points. For a value ''x'' in the interval <math>(x_0, x_1)</math>, the value ''y'' along the straight line is given from the equation of slopes \n\n:<math>\n\\frac{y - y_0}{x - x_0} = \\frac{y_1 - y_0}{x_1 - x_0},</math>\n\nwhich can be derived geometrically from the figure on the right. It is a special case of [[Polynomial interpolation#Constructing the interpolation polynomial|polynomial interpolation]] with ''n''&nbsp;=&nbsp;1.\n\nSolving this equation for ''y'', which is the unknown value at ''x'', gives\n\n:<math>y = y_0 + (x-x_0)\\frac{y_1 - y_0}{x_1-x_0} = \\frac{y_0(x_1-x)+y_1(x-x_0)}{x_1-x_0},</math>\n\nwhich is the formula for linear interpolation in the interval <math>(x_0,x_1)</math>. Outside this interval, the formula is identical to [[linear extrapolation]].\n\nThis formula can also be understood as a weighted average.  The weights are inversely related to the distance from the end points to the unknown point; the closer point has more influence than the farther point.  Thus, the weights are <math display=\"inline\">\\frac{x-x_0}{x_1-x_0}</math> and <math display=\"inline\">\\frac{x_1-x}{x_1-x_0}</math>, which are normalized distances between the unknown point and each of the end points. Because these sum to 1, \n\n:<math>y = y_0 \\left(1 - \\frac{x - x_0}{x_1 - x_0}\\right) + y_1 \\left(1 - \\frac{x_1 - x}{x_1 - x_0}\\right) =\n  y_0 \\left(1 - \\frac{x - x_0}{x_1 - x_0}\\right) + y_1 \\left(\\frac{x - x_0}{x_1 - x_0}\\right),</math>\n\nwhich yields the formula for linear interpolation given above.\n\n==Interpolation of a data set==\n[[Image:Interpolation example linear.svg|thumb|right|300px|Linear interpolation on a data set (red points) consists of pieces of linear interpolants (blue lines).]] \nLinear interpolation on a set of data points (''x''<sub>0</sub>, ''y''<sub>0</sub>), (''x''<sub>1</sub>, ''y''<sub>1</sub>), ..., (''x''<sub>n</sub>, ''y''<sub>n</sub>) is defined as the concatenation of linear interpolants between each pair of data points. This results in a [[Continuous function|continuous curve]], with a discontinuous derivative (in general), thus of [[differentiability class]] <math>C^0</math>.\n\n==Linear interpolation as approximation==\n\nLinear interpolation is often used to approximate a value of some [[Function (mathematics)|function]] ''f'' using two known values of that function at other points. The ''error'' of this approximation is defined as\n\n:<math>R_T = f(x) - p(x),</math>\n\nwhere ''p'' denotes the linear interpolation [[polynomial]] defined above:\n\n:<math>p(x) = f(x_0) + \\frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_0).</math>\n\nIt can be proven using [[Rolle's theorem]] that if ''f'' has a continuous second derivative, then the error is bounded by\n\n:<math>|R_T| \\leq \\frac{(x_1 - x_0)^2}{8} \\max_{x_0 \\leq x \\leq x_1} |f''(x)|.</math>\n\nThat is, the approximation between two points on a given function gets worse with the second derivative of the function that is approximated. This is intuitively correct as well: the \"curvier\" the function is, the worse the approximations made with simple linear interpolation become.\n\n==History and applications==\nLinear interpolation has been used since antiquity for filling the gaps in tables. Suppose that one has a table listing the population of some country in 1970, 1980, 1990 and 2000, and that one wanted to estimate the population in 1994. Linear interpolation is an easy way to do this. The technique of using linear interpolation for tabulation was believed to be used by [[Babylonian astronomy|Babylonian astronomers]] and [[Babylonian mathematics|mathematicians]] in [[Seleucid Empire|Seleucid]] [[Mesopotamia]] (last three centuries BC), and by the [[Greek astronomy|Greek astronomer]] and [[Greek mathematics|mathematician]], [[Hipparchus]] (2nd century BC). A description of linear interpolation can be found in the ''[[Almagest]]'' (2nd century AD) by [[Ptolemy]].\n\nThe basic operation of linear interpolation between two values is commonly used in [[computer graphics]]. In that field's jargon it is sometimes called a '''lerp'''. The term can be used as a [[verb]] or [[noun]] for the operation. e.g. \"[[Bresenham's algorithm]] lerps incrementally between the two endpoints of the line.\"\n\nLerp operations are built into the hardware of all modern computer graphics processors. They are often used as building blocks for more complex operations: for example, a [[bilinear interpolation]] can be accomplished in three lerps. Because this operation is cheap, it's also a good way to implement accurate [[lookup table]]s with quick lookup for [[smooth function]]s without having too many table entries.\n\n==Extensions==\n{{comparison_of_1D_and_2D_interpolation.svg|250px|linear and bilinear interpolation}}\n\n===Accuracy===\n\nIf a [[differentiability class|''C''<sup>0</sup>]] function is insufficient, for example if the process that has produced the data points is known to be smoother than ''C''<sup>0</sup>, it is common to replace linear interpolation with [[spline interpolation]] or, in some cases, [[polynomial interpolation]].\n\n===Multivariate===\n\nLinear interpolation as described here is for data points in one spatial dimension. For two spatial dimensions, the extension of linear interpolation is called [[bilinear interpolation]], and in three dimensions, [[trilinear interpolation]]. Notice, though, that these interpolants are no longer [[linear functions]] of the spatial coordinates, rather products of linear functions; this is illustrated by the clearly non-linear example of [[bilinear interpolation]] in the figure below. Other extensions of linear interpolation can be applied to other kinds of [[polygon mesh|mesh]] such as triangular and tetrahedral meshes, including [[Bézier surface]]s. These may be defined as indeed higher-dimensional [[piecewise linear function]] (see second figure below).\n\n[[Image:Bilininterp.png|right|thumb|Example of [[bilinear interpolation]] on the unit square with the ''z'' values 0, 1, 1 and 0.5 as indicated. Interpolated values in between represented by colour.]]\n[[Image:Piecewise linear function2D.svg|right|thumbnail|A piecewise linear function in two dimensions (top) and the convex polytopes on which it is linear (bottom)]]\n\n==Programming language support==\nMany libraries and [[shading language]]s (in [[GLSL]] known instead as '''mix''') have a \"lerp\" helper-function, returning an interpolation between two inputs (v0, v1) for a parameter (t) in the closed unit interval [0, 1]. Signatures between lerp functions are variously implemented in both the forms (v0, v1, t) and (t, v0, v1).\n<source lang=\"Cpp\">\n// Imprecise method, which does not guarantee v = v1 when t = 1, due to floating-point arithmetic error.\n// This form may be used when the hardware has a native fused multiply-add instruction.\nfloat lerp(float v0, float v1, float t) {\n  return v0 + t * (v1 - v0);\n}\n\n// Precise method, which guarantees v = v1 when t = 1.\nfloat lerp(float v0, float v1, float t) {\n  return (1 - t) * v0 + t * v1;\n}\n</source>\nThis lerp function is commonly used for [[alpha blending]] (the parameter \"t\" is the \"alpha value\"), and the formula may be extended to blend multiple components of a vector (such as spatial ''x'', ''y'', ''z'' axes or ''r'', ''g'', ''b'' colour components) in parallel.\n\n==See also==\n* [[Bilinear interpolation]]\n* [[Spline interpolation]]\n* [[Polynomial interpolation]]\n* [[de Casteljau's algorithm]]\n* [[First-order hold]]\n* [[Bézier curve]]\n\n==References==\n{{Reflist}}\n* {{Citation | last1=Meijering | first1=Erik | title=A chronology of interpolation: from ancient astronomy to modern signal and image processing | doi=10.1109/5.993400 | year=2002 | journal=Proceedings of the IEEE | volume=90 | issue=3 | pages=319–342}}.\n\n==External links==\n\n* [http://www.cut-the-knot.org/Curriculum/Calculus/StraightLine.shtml Equations of the Straight Line] at [[cut-the-knot]]\n* {{springer|title=Linear interpolation|id=p/l059330}}\n* {{springer|title=Finite-increments formula|id=p/f040300}}\n\n\n{{DEFAULTSORT:Linear Interpolation}}\n[[Category:Interpolation]]\n\n[[de:Interpolation (Mathematik)#Lineare Interpolation]]"
    },
    {
      "title": "Linear predictive analysis",
      "url": "https://en.wikipedia.org/wiki/Linear_predictive_analysis",
      "text": "{{unreferenced|date=August 2012}}\n\n'''Linear predictive analysis''' is a simple form of first-order [[extrapolation]]:  if it has been changing at this rate then it will probably continue to change at approximately the same rate, at least in the short term.  This is equivalent to fitting a [[tangent]] to the graph and extending the line.\n\nOne use of this is in [[Linear predictive coding]] which can be used as a method of reducing the amount of data needed to approximately [[Code|encode]] a series.  Suppose it is desired to store or transmit a series of values representing voice.  The value at each [[Sampling (signal processing)|sampling]] point could be transmitted (if 256 values are possible then 8 bits of data for each point are required, if the precision of  65536 levels are desired then 16 bits per sample are required).  If it is known that the value rarely changes more than +/- 15 values between successive samples (-15 to +15 is 31 steps, counting the zero) then we could encode the change in 5 bits.  As long as the change is less than +/- 15 values in successive steps the value will exactly reproduce the desired sequence.  When the rate of change exceeds +/-15 then the reconstructed values will temporarily differ from the desired value; provided fast changes that exceed the limit are rare it may be acceptable to use the approximation in order to attain the improved coding density.\n\n[[Category:Interpolation]]\n[[Category:Asymptotic analysis]]\n\n\n{{math-stub}}"
    },
    {
      "title": "Markov chain geostatistics",
      "url": "https://en.wikipedia.org/wiki/Markov_chain_geostatistics",
      "text": "'''[[Markov chain]] [[geostatistics]]''' uses [[Markov chain]] spatial models, [[simulation]] [[algorithm]]s and associated spatial [[correlation]] measures (e.g., [[transiogram]]) based on the Markov chain random field theory, which extends a single [[Markov chain]] into a multi-dimensional random field for geostatistical modeling. A Markov chain random field is still a single spatial Markov chain. The spatial Markov chain moves or jumps in a space and decides its state at any unobserved location through interactions with its nearest known neighbors in different directions. The data interaction process can be well explained as a local sequential Bayesian updating process within a neighborhood. Because single-step transition probability [[Matrix (mathematics)|matrices]] are difficult to estimate from sparse [[Sample (statistics)|sample]] data and are impractical in representing the complex spatial [[heterogeneity]] of states, the '''[[transiogram]]''', which is defined as a [[transition probability]] [[Function (mathematics)|function]] over the distance [[lag]], is proposed as the accompanying spatial measure of Markov chain random fields.\n\n== References ==\n\n# Li, W. 2007. Markov chain random fields for estimation of categorical variables. Math. Geol., 39(3): 321–335.\n# Li, W. et al. 2015. Bayesian Markov chain random field cosimulation for improving land cover classification accuracy. Math. Geosci., 47(2): 123–148. \n# http://gis.geog.uconn.edu/weidong/Markov_chain_spatial_statistics.htm\n\n[[Category:Geostatistics]]\n[[Category:Interpolation]]\n[[Category:Markov models]]"
    },
    {
      "title": "Monotone cubic interpolation",
      "url": "https://en.wikipedia.org/wiki/Monotone_cubic_interpolation",
      "text": "In the [[mathematics|mathematical]] field of [[numerical analysis]], '''monotone cubic interpolation''' is a variant of [[cubic interpolation]] that preserves [[Monotone function|monotonicity]] of the [[data set]] being interpolated.\n\nMonotonicity is preserved by [[linear interpolation]] but not guaranteed by [[cubic interpolation]].\n\n==Monotone cubic Hermite interpolation==\n[[Image:MonotCubInt.png|thumb|300px|right|Example showing non-monotone cubic interpolation (in red) and monotone cubic interpolation (in blue) of a monotone data set.]]\nMonotone interpolation can be accomplished using [[cubic Hermite spline]] with the tangents <math>m_i</math> modified to ensure the monotonicity of the resulting Hermite spline.\n\nAn algorithm is also available for monotone [[quintic function|quintic]] Hermite interpolation.\n===Interpolant selection===\nThere are several ways of selecting interpolating tangents for each data point. This section will outline the use of the Fritsch–Carlson method. Note that only one pass of the algorithm is required.\n\nLet the data points be <math>(x_k,y_k)</math> indexed in sorted order for <math>k=1,\\,\\dots\\,n</math>.\n\n# Compute the slopes of the [[secant line]]s between successive points:<blockquote><math>\\delta_k =\\frac{y_{k+1}-y_k}{x_{k+1}-x_k}</math></blockquote> for <math>k=1,\\,\\dots\\,n-1</math>.<br/><br/>\n# These assignments are provisional, and may be superceded in the remaining steps. Initialize the tangents at every interior data point as the average of the secants,<blockquote><math>m_k = \\frac{\\delta_{k-1}+\\delta_k}{2}</math></blockquote> for <math>k=2,\\,\\dots\\,n-1</math>.<br/><br/>For the endpoints, use one-sided differences: <blockquote><math>m_1 = \\delta_1 \\quad \\text{ and } \\quad m_n = \\delta_{n-1}\\,</math>.</blockquote>If <math>\\delta_{k-1}</math> and <math>\\delta_k</math> have opposite signs, set <math>m_k = 0 </math>.<br/><br/>\n# For <math>k=1,\\,\\dots\\,n-1</math>, where ever <math>\\delta_k = 0</math> (where ever two successive <math>y_k=y_{k+1}</math> are equal),<br/>set <math>m_k = m_{k+1} = 0,</math> as the spline connecting these points must be flat to preserve monotonicity.<br/>Ignore steps 4 and 5 for those <math>k\\,</math>.<br/><br/>\n# Let <blockquote><math>\\alpha_k = m_k/\\delta_k \\quad \\text{ and } \\quad \\beta_k = m_{k+1}/\\delta_k</math>.</blockquote>If either <math>\\alpha_k</math> or <math>\\beta_k</math> is negative, then the input data points are not strictly monotone, and <math>(x_k,\\,y_k)</math> is a local extremum. In such cases, ''piecewise'' monotone curves can still be generated by choosing <math>m_{k}=0\\,</math>, although strict monotonicity is not possible globally.<br/><br/>\n# To prevent [[overshoot (signal)|overshoot]] and ensure monotonicity, at least one of the following three conditions must be met:\n::(a) the function <blockquote><math>\\phi_k = \\alpha_k - \\frac{(2 \\alpha_k + \\beta_k - 3)^2}{3(\\alpha_k + \\beta_k - 2)} > 0\\,</math>,  '''or'''</blockquote>\n::(b) <math>\\alpha_k + 2\\beta_k - 3 \\le 0\\,</math>,  '''or'''\n::(c) <math>2\\alpha_k + \\beta_k - 3 \\le 0\\,</math>.<br/>\n::Only condition (a) is sufficient to ensure strict monotonicity: <math>\\phi_k</math> must be positive.<br/><br/>\n::One simple way to satisfy this constraint is to restrict the vector <math>(\\alpha_k,\\,\\beta_k)</math> to a circle of radius&nbsp;3. That is, if <math>\\alpha_k^2 + \\beta_k^2 > 9\\,</math>, then set <blockquote><math>\\tau_k = \\frac{3}{\\sqrt{\\alpha_k^2 + \\beta_k^2}}\\,</math>,</blockquote> and rescale the tangents via <blockquote><math>m_k = \\tau_k\\, \\alpha_k \\,\\delta_k \\quad \\text{ and } \\quad m_{k+1} = \\tau_k\\, \\beta_k\\, \\delta_k\\,</math>.</blockquote>\n::Alternatively it is sufficient to restrict <math>\\alpha_k \\le 3</math> and <math>\\beta_k \\le 3\\,</math>. To accomplish this if <math>\\alpha_k > 3\\text{   or   }\\beta_k > 3\\,</math>, then set <math>m_k = 3 \\, \\delta_k\\,</math>.\n\n===Cubic interpolation===\nAfter the preprocessing above, evaluation of the interpolated spline is equivalent to [[cubic Hermite spline]], using the data <math>x_k</math>,  <math>y_k</math>, and <math>m_k</math> for <math>k=1,\\,\\dots\\,n</math>.\n\nTo evaluate at <math>x</math>, find the index <math>k</math> in the sequence where <math>x</math>, lies between <math>x_k</math>, and <math>x_{k+1}</math>, that is: <math>x_k \\leq x \\leq x_{k+1}</math>. Calculate\n:<math>\\Delta = x_{k+1}-x_k \\quad \\text{ and } \\quad t = \\frac{x - x_k}{\\Delta}</math>\nthen the interpolated value is\n:<math>f_\\text{interpolated}(x) = y_k\\cdot h_{00}(t) + \\Delta\\cdot m_k\\cdot h_{10}(t) + y_{k+1}\\cdot h_{01}(t) + \\Delta\\cdot m_{k+1}\\cdot h_{11}(t)</math>\nwhere <math>h_{ii}</math> are the basis functions for the [[cubic Hermite spline]].\n\n==Example implementation==\nThe following [[JavaScript]] implementation takes a data set and produces a monotone cubic spline interpolant function:\n<syntaxhighlight lang=\"javascript\">\n/* Monotone cubic spline interpolation\n   Usage example:\n\tvar f = createInterpolant([0, 1, 2, 3, 4], [0, 1, 4, 9, 16]);\n\tvar message = '';\n\tfor (var x = 0; x <= 4; x += 0.5) {\n\t\tvar xSquared = f(x);\n\t\tmessage += x + ' squared is about ' + xSquared + '\\n';\n\t}\n\talert(message);\n*/\nvar createInterpolant = function(xs, ys) {\n\tvar i, length = xs.length;\n\t\n\t// Deal with length issues\n\tif (length != ys.length) { throw 'Need an equal count of xs and ys.'; }\n\tif (length === 0) { return function(x) { return 0; }; }\n\tif (length === 1) {\n\t\t// Impl: Precomputing the result prevents problems if ys is mutated later and allows garbage collection of ys\n\t\t// Impl: Unary plus properly converts values to numbers\n\t\tvar result = +ys[0];\n\t\treturn function(x) { return result; };\n\t}\n\t\n\t// Rearrange xs and ys so that xs is sorted\n\tvar indexes = [];\n\tfor (i = 0; i < length; i++) { indexes.push(i); }\n\tindexes.sort(function(a, b) { return xs[a] < xs[b] ? -1 : 1; });\n\tvar oldXs = xs, oldYs = ys;\n\t// Impl: Creating new arrays also prevents problems if the input arrays are mutated later\n\txs = []; ys = [];\n\t// Impl: Unary plus properly converts values to numbers\n\tfor (i = 0; i < length; i++) { xs.push(+oldXs[indexes[i]]); ys.push(+oldYs[indexes[i]]); }\n\t\n\t// Get consecutive differences and slopes\n\tvar dys = [], dxs = [], ms = [];\n\tfor (i = 0; i < length - 1; i++) {\n\t\tvar dx = xs[i + 1] - xs[i], dy = ys[i + 1] - ys[i];\n\t\tdxs.push(dx); dys.push(dy); ms.push(dy/dx);\n\t}\n\t\n\t// Get degree-1 coefficients\n\tvar c1s = [ms[0]];\n\tfor (i = 0; i < dxs.length - 1; i++) {\n\t\tvar m = ms[i], mNext = ms[i + 1];\n\t\tif (m*mNext <= 0) {\n\t\t\tc1s.push(0);\n\t\t} else {\n\t\t\tvar dx_ = dxs[i], dxNext = dxs[i + 1], common = dx_ + dxNext;\n\t\t\tc1s.push(3*common/((common + dxNext)/m + (common + dx_)/mNext));\n\t\t}\n\t}\n\tc1s.push(ms[ms.length - 1]);\n\t\n\t// Get degree-2 and degree-3 coefficients\n\tvar c2s = [], c3s = [];\n\tfor (i = 0; i < c1s.length - 1; i++) {\n\t\tvar c1 = c1s[i], m_ = ms[i], invDx = 1/dxs[i], common_ = c1 + c1s[i + 1] - m_ - m_;\n\t\tc2s.push((m_ - c1 - common_)*invDx); c3s.push(common_*invDx*invDx);\n\t}\n\t\n\t// Return interpolant function\n\treturn function(x) {\n\t\t// The rightmost point in the dataset should give an exact result\n\t\tvar i = xs.length - 1;\n\t\tif (x == xs[i]) { return ys[i]; }\n\t\t\n\t\t// Search for the interval x is in, returning the corresponding y if x is one of the original xs\n\t\tvar low = 0, mid, high = c3s.length - 1;\n\t\twhile (low <= high) {\n\t\t\tmid = Math.floor(0.5*(low + high));\n\t\t\tvar xHere = xs[mid];\n\t\t\tif (xHere < x) { low = mid + 1; }\n\t\t\telse if (xHere > x) { high = mid - 1; }\n\t\t\telse { return ys[mid]; }\n\t\t}\n\t\ti = Math.max(0, high);\n\t\t\n\t\t// Interpolate\n\t\tvar diff = x - xs[i], diffSq = diff*diff;\n\t\treturn ys[i] + c1s[i]*diff + c2s[i]*diffSq + c3s[i]*diff*diffSq;\n\t};\n};\n</syntaxhighlight>\n\n==References==\n*{{cite journal\n  | last = Fritsch\n  | first = F. N.\n  |author2=Carlson, R. E.\n  | title = Monotone Piecewise Cubic Interpolation\n  | journal = [[SIAM Journal on Numerical Analysis]]\n  | volume = 17\n  | issue = 2\n  | pages = 238–246\n  | publisher = SIAM\n  | date = 1980\n  | doi = 10.1137/0717021\n}}\n*{{cite journal\n  | last = Dougherty\n  | first = R.L.\n  |author2=Edelman, A.\n  |author3=Hyman, J.M.\n  | title = Positivity-, monotonicity-, or convexity-preserving cubic and quintic Hermite interpolation\n  | journal = [[Mathematics of Computation]]\n  | volume = 52\n  | number = 186\n  | pages = 471–494\n  | date = April 1989\n  | doi=10.2307/2008477\n}}\n\n==External links==\n* [[GPL]]v2 licensed [[C++]] implementation: [https://github.com/bska/opm-origins/blob/master/dune-cornerpoint/common/MonotCubicInterpolator.cpp MonotCubicInterpolator.cpp] [https://github.com/bska/opm-origins/blob/master/dune-cornerpoint/common/MonotCubicInterpolator.hpp MonotCubicInterpolator.hpp]\n\n[[Category:Interpolation]]\n[[Category:Splines (mathematics)]]\n[[Category:Articles with example JavaScript code]]"
    },
    {
      "title": "Multivariate interpolation",
      "url": "https://en.wikipedia.org/wiki/Multivariate_interpolation",
      "text": "In [[numerical analysis]], '''multivariate interpolation''' or '''spatial interpolation''' is [[interpolation]] on functions of more than one variable.\n\nThe function to be interpolated is known at given points <math>(x_i, y_i, z_i, \\dots)</math> and the interpolation problem consist of yielding values at arbitrary points <math>(x,y,z,\\dots)</math>.\n\nMultivariate interpolation is particularly important in [[geostatistics]], where it is used to create a [[digital elevation model]] from a set of points on the Earth's surface (for example, spot heights in a [[topographic survey]] or depths in a [[hydrographic survey]]).\n\n==Regular grid==\n{{comparison_of_1D_and_2D_interpolation.svg|300px|}}\nFor function values known on a [[regular grid]] (having predetermined, not necessarily uniform, spacing), the following methods are available.\n\n===Any dimension===\n* [[Nearest-neighbor interpolation]]\n* [[Kriging]]\n* [[Inverse distance weighting]]\n* [[Natural neighbor interpolation]]\n* [[Spline interpolation]]\n* [[Radial basis function interpolation]]\n\n===2 dimensions===\n* [[Barnes interpolation]]\n* [[Bilinear interpolation]]\n* [[Bicubic interpolation]] \n* [[Bézier surface]]\n* [[Lanczos resampling]] \n* [[Delaunay triangulation]] \n\n\n[[Resampling (bitmap)|Bitmap resampling]] is the application of 2D multivariate interpolation in [[image processing]].\n\nThree of the methods applied on the same dataset, from 25 values located at the black dots. The colours represent the interpolated values.\n{{Gallery\n|title=\n|width=300px\n  |File:Interpolation-nearest.svg|Nearest neighbor\n  |File:Interpolation-bilinear.svg|Bilinear\n  |File:Interpolation-bicubic.svg|Bicubic\n}}\n\nSee also [[Padua points]], for [[polynomial interpolation]] in two variables.\n\n===3 dimensions===\n* [[Trilinear interpolation]]\n* [[Tricubic interpolation]]\n\nSee also [[Resampling (bitmap)|bitmap resampling]].\n\n===Tensor product splines for ''N'' dimensions===\n\nCatmull-Rom splines can be easily generalized to any number of dimensions.\nThe [[cubic Hermite spline]] article will remind you that <math>\\mathrm{CINT}_x(f_{-1}, f_0, f_1, f_2) = \\mathbf{b}(x) \\cdot \\left( f_{-1} f_0 f_1 f_2 \\right)</math> for some 4-vector <math>\\mathbf{b}(x)</math> which is a function of ''x'' alone, where <math>f_j</math> is the value at <math>j</math> of the function to be interpolated.\nRewrite this approximation as\n:<math>\n\\mathrm{CR}(x) \\subseteq \\sum_{i=-1}^2 f_i b_i(x)\n</math>\nThis formula can be directly generalized to N dimensions:<ref>[https://arxiv.org/abs/0905.3564 Two hierarchies of spline interpolations. Practical algorithms for multivariate higher order splines]</ref>\n:<math>\n\\mathrm{CR}(x_1,\\dots,x_N) \\in \\sum_{i_1,\\dots,i_N=-1}^2 f_{i_1\\dots i_N} \\sum_{j=1}^N b_{i_j}(x_j)\n</math>\nNote that similar generalizations can be made for other types of spline interpolations, including Hermite splines.\nIn regards to efficiency, the general formula can in fact be computed as a composition of successive <math>\\mathrm{CINT}</math>-type operations for any type of tensor product splines, as explained in the [[tricubic interpolation]] article.\nHowever, the fact remains that if there are <math>n</math> terms in the 1-dimensional <math>\\mathrm{CR}</math>-like summation, then there will be <math>n^N</math> terms in the <math>N</math>-dimensional summation.\n\n== Irregular grid (scattered data) ==\nSchemes defined for scattered data on an [[irregular grid]] should all work on a regular grid, typically reducing to another known method.\n* [[Nearest-neighbor interpolation]]\n* [[Triangulated irregular network]]-based [[natural neighbor]]\n* [[Triangulated irregular network]]-based [[linear interpolation]] (a type of [[piecewise linear function]])\n* [[Inverse distance weighting]]\n* [[Kriging]]\n* [[Gradient-Enhanced Kriging (GEK)]]\n* [[Thin plate spline]]\n* [[Polyharmonic spline]] (the thin-plate-spline is a special case of a polyharmonic spline)\n* [[Radial basis function]] ([[Polyharmonic spline|Polyharmoic splines]] are a special case of radial basis functions with low degree polynomial terms)\n* Least-squares [[spline (mathematics)|spline]]\n* [[Natural neighbour interpolation]]\n\n==Notes==\n<references />\n\n==External links==\n* [http://chichi.lalescu.ro/splines.html Example C++ code for several 1D, 2D and 3D spline interpolations (including Catmull-Rom splines).]\n* [https://web.archive.org/web/20060915111500/http://www.ices.utexas.edu/CVC/papers/multidim.pdf Multi-dimensional Hermite Interpolation and Approximation], Prof. Chandrajit Bajaja, [[Purdue University]]\n* [https://github.com/DurhamDecLab/ARBInterp Python library containing 3D and 4D spline interpolation methods.]\n\n[[Category:Interpolation]]\n[[Category:Multivariate interpolation| ]]"
    },
    {
      "title": "Nearest-neighbor interpolation",
      "url": "https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation",
      "text": "\n[[Image:Piecewise_constant.svg|right|thumb|Nearest neighbor interpolation (blue lines) in one dimension on a (uniform) dataset (red points).]]\n[[Image:Interpolation-nearest.svg|right|thumb|Nearest neighbor interpolation on a uniform 2D grid (black points). Each coloured cell indicates the area in which all the points have the black point in the cell as their nearest black point.]]\n\n'''Nearest-neighbor interpolation''' (also known as '''proximal interpolation''' or, in some contexts, '''point sampling''') is a simple method of [[multivariate interpolation]] in one or more [[dimension]]s.\n\n[[Interpolation]] is the problem of approximating the value of a function for a non-given point in some space when given the value of that function in points around (neighboring) that point. The nearest neighbor algorithm selects the value of the nearest point and does not consider the values of neighboring points at all, yielding a piecewise-constant interpolant.  The algorithm is very simple to implement and is commonly used (usually along with [[mipmap|mipmapping]]) in [[Real-time computing|real-time]] [[3D rendering]] to select color values for a [[texture filtering|textured]] surface.\n\n==Connection to Voronoi diagram==\nFor a given set of points in space, a [[Voronoi diagram]] is a decomposition of space into cells, one for each given point, so that anywhere in space, the closest given point is inside the cell. This is equivalent to nearest neighbour interpolation, by assigning the function value at the given point to all the points inside the cell. The figures on the right side show by colour the shape of the cells.\n\n{{comparison_of_1D_and_2D_interpolation.svg|left}}\n[[Image:Coloured_Voronoi_2D.svg|none|thumb|This [[Voronoi diagram]] is an example of nearest neighbor interpolation of a random set of points (black dots) in 2D.]]\n{{-}}\n\n==See also==\n* [[Interpolation]]\n* [[Natural neighbor interpolation]]\n* [[Image scaling]]\n* [[Nearest neighbor search]]\n* [[Zero-order hold]]\n* [[Rounding]]\n\n[[Category:Interpolation]]\n[[Category:Multivariate interpolation]]\n[[Category:Texture filtering]]\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Nevanlinna–Pick interpolation",
      "url": "https://en.wikipedia.org/wiki/Nevanlinna%E2%80%93Pick_interpolation",
      "text": "In [[complex analysis]], given ''initial data'' consisting of <math>n</math> points <math>\\lambda_1, \\ldots, \\lambda_n</math> in the complex unit disc <math>\\mathbb{D}</math> and ''target data'' consisting of <math>n</math> points <math>z_1, \\ldots, z_n</math> in <math>\\mathbb{D}</math>,  the '''Nevanlinna–Pick interpolation problem''' is to find a [[holomorphic function]] <math>\\varphi</math> that [[interpolate]]s the data, that is for all <math>i</math>,\n:<math>\\varphi(\\lambda_i) = z_i</math>,\nsubject to the constraint <math>\\left\\vert \\varphi(\\lambda) \\right\\vert \\le 1</math> for all <math>\\lambda \\in \\mathbb{D}</math>.\n\n[[Georg Alexander Pick|Georg Pick]] and [[Rolf Nevanlinna]] solved the problem independently in 1916 and 1919 respectively, showing that an interpolating function exists if and only if a matrix defined in terms of the initial and target data is [[positive-definite matrix|positive semi-definite]].\n\n==Background==\nThe Nevanlinna–Pick theorem represents an <math>n</math>-point generalization of the [[Schwarz lemma]]. The [[Schwarz lemma#Schwarz–Pick theorem|invariant form of the Schwarz lemma]] states that for a holomorphic function <math>f:\\mathbb{D}\\to\\mathbb{D}</math>, for all <math>\\lambda_1, \\lambda_2 \\in \\mathbb{D}</math>,\n:<math> \\left|\\frac{f(\\lambda_1) - f(\\lambda_2)}{1 - \\overline{f(\\lambda_2)}f(\\lambda_1)}\\right| \\leq \\left|\\frac{\\lambda_1 - \\lambda_2}{1 - \\overline{\\lambda_2}\\lambda_1}\\right|.</math>\n\nSetting <math>f(\\lambda_i)=z_i</math>, this inequality is equivalent to the statement that the matrix given by\n:<math>\\begin{bmatrix} \\frac{1 - |z_1|^2}{1 - |\\lambda_1|^2} & \\frac{1 - \\overline{z_1}z_2}{1 - \\overline{\\lambda_1}\\lambda_2}\n\\\\[5pt] \\frac{1 - \\overline{z_2}z_1}{1 - \\overline{\\lambda_2}\\lambda_1} & \\frac{1 - |z_2|^2}{1 - |\\lambda_2|^2} \\end{bmatrix} \\geq 0,</math>\nthat is the ''Pick matrix'' is positive semidefinite.\n\nCombined with the Schwarz lemma, this leads to the observation that for <math>\\lambda_1, \\lambda_2, z_1, z_2 \\in \\mathbb{D}</math>, there exists a holomorphic function <math>\\varphi:\\mathbb{D} \\to \\mathbb{D}</math> such that <math>\\varphi(\\lambda_1) = z_1</math> and <math>\\varphi(\\lambda_2)=z_2</math> if and only if the Pick matrix\n:<math>\\left(\\frac{1 - \\overline{z_j}z_i}{1 - \\overline{\\lambda_j}\\lambda_i}\\right)_{i,j = 1, 2} \\geq 0.</math>\n\n==The Nevanlinna–Pick theorem==\nThe Nevanlinna–Pick theorem states the following. Given <math>\\lambda_1, \\ldots, \\lambda_n, z_1, \\ldots, z_n \\in \\mathbb{D}</math>, there exists a holomorphic function <math>\\varphi:\\mathbb{D} \\to \\overline{\\mathbb{D}}</math> such that <math>\\varphi(\\lambda_i) = z_i</math> if and only if the Pick matrix\n\n:<math>\\left( \\frac{1-\\overline{z_j} z_i}{1-\\overline{\\lambda_j} \\lambda_i} \\right)_{i,j=1}^n</math>\n\nis positive semi-definite. Furthermore, the function <math>\\varphi</math> is unique if and only if the Pick matrix has zero [[determinant]]. In this case, <math>\\varphi</math> is a [[Blaschke product]], with degree equal to the rank of the Pick matrix (except in the trivial case where\nall the <math>z_i</math>'s are the same).\n\n==Generalisation==\nThe generalization of the Nevanlinna–Pick theorem became an area of active research in [[operator theory]] following the work of [[Donald Sarason]] on the [[Sarason interpolation theorem]].<ref>{{cite journal|last=Sarason|first=Donald|title=Generalized Interpolation in <math>H^\\infty</math>|journal=Trans. Amer. Math. Soc.|date=1967|volume=127|pages=179–203|doi=10.1090/s0002-9947-1967-0208383-8}}</ref> Sarason gave a new proof of the Nevanlinna–Pick theorem using [[Hilbert space]] methods in terms of [[contraction (operator theory)|operator contractions]]. Other approaches were developed in the work of [[Louis de Branges de Bourcia|L. de Branges]], and [[Béla Szőkefalvi-Nagy|B. Sz.-Nagy]] and [[Ciprian Foias|C. Foias]].\n\nIt can be shown that the [[Hardy space]] ''H''<sup>&nbsp;2</sup> is a [[reproducing kernel Hilbert space]], and that its reproducing kernel (known as the [[Gábor Szegő|Szegő]] kernel) is\n\n:<math>K(a,b)=\\left(1-b \\bar{a} \\right)^{-1}.\\,</math>\n\nBecause of this, the Pick matrix can be rewritten as\n\n:<math>\\left( (1-z_i \\overline{z_j}) K(\\lambda_j,\\lambda_i)\\right)_{i,j=1}^N.\\,</math>\n\nThis description of the solution has motivated various attempts to generalise Nevanlinna and Pick's result.\n\nThe Nevanlinna–Pick problem can be generalised to that of finding a holomorphic function <math>f:R\\to\\mathbb{D}</math> that interpolates a given set of data, where ''R'' is now an arbitrary region of the complex plane.\n\nM. B. Abrahamse showed that if the boundary of ''R'' consists of finitely many analytic curves (say ''n''&nbsp;+&nbsp;1), then an interpolating function ''f'' exists if and only if\n\n:<math>\\left( (1-z_i \\overline{z_j}) K_\\tau (\\lambda_j,\\lambda_i)\\right)_{i,j=1}^N\\,</math>\n\nis a positive semi-definite matrix, for all <math>\\tau</math> in the [[n-torus|''n''-torus]]. Here, the <math>K_\\tau</math>s are the reproducing kernels corresponding to a particular set of reproducing kernel Hilbert spaces, which are related to the set ''R''. It can also be shown that ''f'' is unique if and only if one of the Pick matrices has zero determinant.\n\n==Notes==\n\n* Pick's original proof concerned functions with positive real part. Under a linear fractional [[Cayley transform]], his result holds on maps from the disc to the disc.\n\n* Pick–Nevanlinna interpolation was introduced into [[robust control]] by [[Allen Tannenbaum]].\n\n==References==\n{{reflist}}\n*{{cite book\n  | last = Agler\n  | first = Jim\n  |author2=John E. McCarthy\n   | title = Pick Interpolation and Hilbert Function Spaces\n  | publisher = [[American Mathematical Society|AMS]]\n  | series = [[Graduate Studies in Mathematics]]\n  | year = 2002\n  | isbn = 0-8218-2898-3 }}\n*{{cite journal\n | first = M. B.\n | last = Abrahamse\n | title = The Pick interpolation theorem for finitely connected domains\n | year = 1979\n | journal = Michigan Math. J.\n | volume = 26\n | issue = 2\n | pages = 195–203\n | doi = 10.1307/mmj/1029002212\n }}\n*{{cite journal\n | first = Allen\n | last = Tannenbaum\n | title = Feedback stabilization of linear dynamical plants with uncertainty in the gain factor\n | year = 1980\n | journal = Int. J. Control\n | volume = 32\n | issue = 1\n | pages = 1–16\n | doi=10.1080/00207178008922838\n }}\n\n{{DEFAULTSORT:Nevanlinna-Pick interpolation}}\n[[Category:Interpolation]]"
    },
    {
      "title": "Neville's algorithm",
      "url": "https://en.wikipedia.org/wiki/Neville%27s_algorithm",
      "text": "In mathematics, '''Neville's algorithm''' is an algorithm used for [[polynomial interpolation]] that was derived by the mathematician [[Eric Harold Neville]]. Given ''n'' + 1 points, there is a unique polynomial of degree ''≤ n'' which goes through the given points. Neville's algorithm evaluates this polynomial.\n\nNeville's algorithm is based on the [[Newton polynomial|Newton form]] of the interpolating polynomial and the recursion relation for the [[divided differences]]. It is similar to [[Aitken interpolation|Aitken's algorithm]] (named after [[Alexander Aitken]]), which is nowadays not used.\n\n==The algorithm==\n\nGiven a set of ''n''+1 data points (''x''<sub>''i''</sub>, ''y''<sub>''i''</sub>) where no two ''x''<sub>''i''</sub> are the same, the interpolating polynomial is the polynomial ''p'' of degree at most ''n'' with the property\n<!--:<math>p(x_i) = y_i \\mbox{ , } i=0,\\ldots,n.</math>-->\n:''p''(''x''<sub>''i''</sub>) = ''y''<sub>''i''</sub> for all ''i'' = 0,&hellip;,''n''\nThis polynomial exists and it is unique. Neville's algorithm evaluates the polynomial at some point ''x''.\n\nLet ''p''<sub>''i'',''j''</sub> denote the polynomial of degree ''j'' &minus; ''i'' which goes through the points (''x''<sub>''k''</sub>, ''y''<sub>''k''</sub>) for ''k'' = ''i'', ''i'' + 1, &hellip;, ''j''. The \n''p''<sub>''i'',''j''</sub> satisfy the recurrence relation\n:{|\n| <math> p_{i,i}(x) = y_i, \\, </math> || <math> 0 \\le i \\le n, \\, </math>\n|-\n| <math> p_{i,j}(x) = \\frac{(x-x_j)p_{i,j-1}(x) - (x-x_i)p_{i+1,j}(x)}{x_i-x_j}, \\, </math> || <math> 0\\le i < j \\le n. \\, </math>\n|}\nThis recurrence can calculate\n<!--<math>p_{0,n}(x)</math>,-->\n''p''<sub>0,''n''</sub>(''x''),\nwhich is the value being sought. This is Neville's algorithm.\n\nFor instance, for ''n'' = 4, one can use the recurrence to fill the triangular tableau below from the left to the right.\n:{| \n| <math> p_{0,0}(x) = y_0 \\, </math>\n|-\n| || <math> p_{0,1}(x) \\, </math>\n|-\n| <math> p_{1,1}(x) = y_1 \\, </math> || || <math> p_{0,2}(x) \\, </math>\n|-\n| || <math> p_{1,2}(x) \\, </math> || || <math> p_{0,3}(x) \\, </math>\n|-\n| <math> p_{2,2}(x) = y_2 \\, </math> || || <math> p_{1,3}(x) \\, </math> || || style=\"border: 1px solid;\" | <math> p_{0,4}(x) \\, </math>\n|-\n| || <math> p_{2,3}(x) \\, </math> || || <math> p_{1,4}(x) \\, </math>\n|-\n| <math> p_{3,3}(x) = y_3 \\, </math> || || <math> p_{2,4}(x) \\, </math>\n|-\n| || <math> p_{3,4}(x) \\, </math>\n|-\n| <math> p_{4,4}(x) = y_4 \\, </math>\n|}\n\nThis process yields \n<!--<math>p_{0,4}(x)</math>,-->\n''p''<sub>0,4</sub>(''x''),\nthe value of the polynomial going through the ''n'' + 1 data points (''x''<sub>''i''</sub>, ''y''<sub>''i''</sub>) at the point ''x''.\n\nThis algorithm needs [[big O notation|O]](''n''<sup>2</sup>) floating point operations.\n\nThe derivative of the polynomial can be obtained in the same manner, i.e:\n \n:{|\n| <math> p'_{i,i}(x) = 0, \\, </math> || <math> 0 \\le i \\le n, \\, </math>\n|-\n| <math> p'_{i,j}(x) = \\frac{(x_j-x)p'_{i,j-1}(x) - p_{i,j-1}(x) + (x-x_i)p'_{i+1,j}(x) + p_{i+1,j}(x)}{x_j-x_i}, \\, </math> || <math> 0\\le i < j \\le n. \\, </math>\n|}\n\n==Application to numerical differentiation==\n\nLyness and Moler showed in 1966 that using undetermined coefficients for the polynomials in Neville's algorithm, one can compute the Maclaurin expansion of the final interpolating polynomial, which yields numerical approximations for the derivatives of the function at the origin. While \"this process requires more arithmetic operations than is required in finite difference methods\", \"the choice of points for function evaluation is not restricted in any way\". They also show that their method can be applied directly to the solution of linear systems of the Vandermonde type.\n\n==References==\n*{{cite book | last = Press | first = William |author2=Saul Teukolsky |author3=William Vetterling |author4=Brian Flannery | title = Numerical Recipes in C. The Art of Scientific Computing | edition = 2nd | year = 1992 | publisher = Cambridge University Press | isbn = 978-0-521-43108-8 | doi=10.2277/0521431085 | chapter = §3.1 Polynomial Interpolation and Extrapolation (encrypted) | chapterurl = http://www.nrbook.com/ub30001/nr3-3-2.pdf | title-link = Numerical Recipes }} (link is bad)\n* J. N. Lyness and C.B. Moler, Van Der Monde Systems and Numerical Differentiation, Numerische Mathematik 8 (1966) 458-464 (doi: [https://dx.doi.org/10.1007/BF02166671 10.1007/BF02166671])\n\n==External links==\n*{{MathWorld|title=Neville's Algorithm|urlname=NevillesAlgorithm}}\n*[https://s3.amazonaws.com/torkian/torkian/Site/Research/Entries/2008/2/29_Nevilles_algorithm_Java_Code.html Java Code by Behzad Torkian]\n\n[[Category:Polynomials]]\n[[Category:Interpolation]]\n\n[[de:Polynominterpolation#Algorithmus von Neville-Aitken]]"
    },
    {
      "title": "Non-uniform rational B-spline",
      "url": "https://en.wikipedia.org/wiki/Non-uniform_rational_B-spline",
      "text": "[[Image:NURBstatic.svg|thumb|250px|A NURBS curve. See also [[:Image:Spline01.gif|this animated version]].]]\n'''Non-uniform rational [[b-spline|basis spline]]''' ('''NURBS''') is a mathematical model commonly used in [[computer graphics]] for generating and representing curves and surfaces. It offers great flexibility and precision for handling both analytic ([[surface (mathematics)|surface]]s defined by common mathematical [[formula]]e) and [[3D modeling|modeled]] shapes. NURBS are commonly used in computer-aided design ([[Computer-aided design|CAD]]), manufacturing ([[Computer-aided manufacturing|CAM]]), and engineering ([[Computer-aided engineering|CAE]]) and are part of numerous industry wide standards, such as [[IGES]], [[ISO 10303|STEP]], [[ACIS]], and [[PHIGS]]. NURBS tools are also found in various [[3D computer graphics software|3D modeling]] and [[animation]] software packages.\n\nThey can be efficiently handled by  the computer programs and yet allow for easy human interaction. NURBS surfaces are functions of two parameters mapping to a surface in three-dimensional space. The shape of the surface is determined by control points. NURBS surfaces can represent, in a compact form, simple geometrical shapes. [[T-Spline (mathematics)|T-splines]] and [[subdivision surfaces]] are more suitable for complex organic shapes because they reduce the number of control points twofold in comparison with the NURBS surfaces.\n\nIn general, editing NURBS curves and surfaces is highly intuitive and predictable. Control points are always either connected directly to the curve/surface, or act as if they were connected by a rubber band. Depending on the type of user interface, editing can be realized via an element’s control points, which are most obvious and common for [[Bézier curve]]s, or via higher level tools such as spline modeling or hierarchical editing.\n\n{{TOC limit|2}}\n\n== Historical background ==\n[[File:Spline (PSF).png|thumb|right|A spline]]\nBefore computers, designs were drawn by hand on paper with various [[Technical drawing|drafting tool]]s. [[Ruler]]s were used for straight lines, [[Compass (drafting)|compass]]es for circles, and [[protractor]]s for angles. But many shapes, such as the [[freeform curve]] of a ship's bow, could not be drawn with these tools. Although such curves could be drawn freehand at the drafting board, shipbuilders often needed a life-size version which could not be done by hand. Such large drawings were done with the help of flexible strips of wood, called splines. The splines were held in place at a number of predetermined points, called \"ducks\"; between the ducks, the [[Elasticity (physics)|elasticity]] of the spline material caused the strip to take the shape that minimized the energy of bending, thus creating the smoothest possible shape that fit the constraints. The shape could be tweaked by moving the ducks.<ref name=mactech>{{cite web|last1=Schneider|first1=Philip|title=NURB Curves: A Guide for the Uninitiated|url=http://www.mactech.com/articles/develop/issue_25/schneider.html|website=MACTECH|accessdate=26 September 2014}}</ref>\n\nIn 1946, mathematicians started studying the spline shape, and derived the piecewise [[polynomial]] formula known as the [[spline (mathematics)|spline curve]] or spline function. [[Isaac Jacob Schoenberg|I. J. Schoenberg]] gave the spline function its name after its resemblance to the mechanical spline used by draftsmen.<ref>{{cite journal\n|first = I. J.\n|last = Schoenberg\n|title = Spline Functions and the Problem of Graduation\n|journal = [[Proceedings of the National Academy of Sciences of the United States of America]]\n|publisher = [[National Academy of Sciences]]\n|date = August 19, 1964\n|volume = 52\n|issue = 4\n|pages = 947–950\n|url = http://www.pnas.org/cgi/reprint/52/4/947\n|format = PDF\n|accessdate = 2012-02-24\n|doi=10.1073/pnas.52.4.947|pmc = 300377\n}}</ref>\n\nAs computers were introduced into the design process, the physical properties of such splines were investigated so that they could be modelled with mathematical precision and reproduced where needed. Pioneering work was done in [[France]] by [[Renault]] engineer [[Pierre Bézier]], and [[Citroën]]'s physicist and mathematician [[Paul de Casteljau]]. They worked nearly parallel to each other, but because Bézier published the results of his work, Bézier curves were named after him, while de Casteljau’s name is only associated with related algorithms.\n\nAt first NURBS were only used in the proprietary [[Computer aided design|CAD]] packages of car companies. Later they became part of standard computer graphics packages.\n\nReal-time, interactive rendering of NURBS curves and surfaces was first made  commercially available on [[Silicon Graphics]] workstations in 1989. In 1993, the first interactive NURBS modeller for PCs, called NöRBS, was developed by CAS Berlin, a small startup company cooperating with the [[Technical University of Berlin]]. Today most professional computer graphics applications available for desktop offer NURBS technology, which is most often realized by integrating a NURBS engine from a specialized company.\n\n== Continuity ==\n{{main|Smooth function}}\nA surface under construction, e.g. the hull of a motor yacht, is usually composed of several NURBS surfaces known as ''patches''. These patches should be fitted together in such a way that the boundaries are invisible. This is mathematically expressed by the concept of [[geometric continuity]].\n\nHigher-level tools exist that benefit from the ability of NURBS to create and establish geometric continuity of different levels:\n* '''Positional continuity''' (G<sup>0</sup>) holds whenever the end positions of two curves or surfaces coincide.  The curves or surfaces may still meet at an angle, giving rise to a sharp corner or edge and causing broken highlights.\n* '''Tangential continuity''' (G¹) requires the end vectors of the curves or surfaces to be parallel and pointing the same way, ruling out sharp edges.  Because highlights falling on a tangentially continuous edge are always continuous and thus look natural, this level of continuity can often be sufficient.\n* '''Curvature continuity''' (G²) further requires the end vectors to be of the same length and rate of length change.  Highlights falling on a curvature-continuous edge do not display any change, causing the two surfaces to appear as one.  This can be visually recognized as \"perfectly smooth\".  This level of continuity is very useful in the creation of models that require many [[bi-cubic]] patches composing one continuous surface.\n\nGeometric continuity mainly refers to the shape of the resulting surface; since NURBS surfaces are functions, it is also possible to discuss the derivatives of the surface with respect to the parameters. This is known as [[parametric continuity]]. Parametric continuity of a given degree implies geometric continuity of that degree.\n \nFirst- and second-level parametric continuity (C<sup>0</sup> and C¹) are for practical purposes identical to positional and tangential (G<sup>0</sup> and G¹) continuity.  Third-level parametric continuity (C²), however, differs from curvature continuity in that its parameterization is also continuous.  In practice, C² continuity is easier to achieve if uniform B-splines are used.\n\nThe definition of C<sup>''n''</sup> continuity requires that the ''n''th derivative of adjacent curves/surfaces (<math>d^n C(u)/du^n</math>) are equal at a joint.<ref>Foley, van Dam, Feiner & Hughes: ''[[Computer Graphics: Principles and Practice]]'', section 11.2, [[Addison-Wesley]] 1996 (2nd ed.).</ref> Note that the (partial) derivatives of curves and surfaces are vectors that have a direction and a magnitude; both should be equal.\n\nHighlights and reflections can reveal the perfect smoothing, which is otherwise practically impossible to achieve without NURBS surfaces that have at least G² continuity. This same principle is used as one of the surface evaluation methods whereby a [[Ray tracing (graphics)|ray-traced]] or [[reflection mapping|reflection-mapped]] image of a surface with white stripes reflecting on it will show even the smallest deviations on a surface or set of surfaces.  This method is derived from car prototyping wherein surface quality is inspected by checking the quality of reflections of a neon-light ceiling on the car surface. This method is also known as \"Zebra analysis\".\n\n== Technical specifications ==\n[[Image:Surface modelling.svg|250px|right]]\nA NURBS curve is defined by its order, a set of weighted control points, and a knot vector.<ref>{{cite book\n | title = Bio-Inspired Self-Organizing Robotic Systems\n | url = https://www.springer.com/engineering/computational+intelligence+and+complexity/book/978-3-642-20759-4\n | accessdate = 2014-01-06\n | page = 9\n}}</ref>  NURBS curves and surfaces are generalizations of both [[B-spline]]s and [[Bézier curve]]s and surfaces, the primary difference being the weighting of the control points, which makes NURBS curves ''rational''. (''Non-rational'', aka ''simple'', B-splines are a special case/subset of rational B-splines, where each control point is a regular non-homogenous coordinate [no 'w'] rather than a [[Homogeneous coordinates|homogeneous coordinate]].<ref>{{cite web|url=https://www.cl.cam.ac.uk/teaching/2000/AGraphHCI/SMEG/node5.html|title=Rational B-splines|author=|date=|website=www.cl.cam.ac.uk}}</ref> That is equivalent to having weight \"1\" at each control point; ''Rational'' B-splines use the 'w' of each control point as a '''weight'''.<ref>{{cite web|url=https://www.cs.mtu.edu/~shene/COURSES/cs3621/NOTES/spline/NURBS/NURBS-def.html|title=NURBS: Definition|author=|date=|website=www.cs.mtu.edu}}</ref>)\nBy using a two-dimensional grid of control points, NURBS surfaces including planar patches and sections of spheres can be created. These are parametrized with two variables (typically called ''s'' and ''t'' or ''u'' and ''v''). This can be extended to arbitrary dimensions to create NURBS mapping <math>R^n\\to R^n</math>.\n\nNURBS curves and surfaces are useful for a number of reasons:-\n* The set of NURBS for a given order is [[invariant (mathematics)|invariant]] under [[affine transformation|affine]] transformations:<ref>David F. Rogers: An Introduction to NURBS with Historical Perspective, section 7.1</ref> operations like rotations and translations can be applied to NURBS curves and surfaces by applying them to their control points.\n* They offer one common mathematical form for both standard analytical shapes (e.g., [[conics]]) and free-form shapes.\n* They provide the flexibility to design a large variety of shapes.\n* They reduce the memory consumption when storing shapes (compared to simpler methods).\n* They can be evaluated reasonably quickly by [[numerically stable]] and accurate [[algorithm]]s.\n\nIn the next sections, NURBS is discussed in one dimension (curves). All of it can be generalized to two or even more dimensions.\n\n=== Order ===\nThe ''order'' of a NURBS curve defines the number of nearby control points that influence any given point on the curve.  The curve is represented mathematically by a polynomial of degree one less than the order of the curve.  Hence, second-order curves (which are represented by linear polynomials) are called linear curves, third-order curves are called quadratic curves, and fourth-order curves are called cubic curves.  The number of control points must be greater than or equal to the order of the curve.\n\nIn practice, cubic curves are the ones most commonly used.  Fifth- and sixth-order curves are sometimes useful, especially for obtaining continuous higher order derivatives, but curves of higher orders are practically never used because they lead to internal numerical problems and tend to require disproportionately large calculation times.\n\n=== Control points ===\n[[Image:NURBS 3-D surface.gif|250px|right|thumb|Three-dimensional NURBS surfaces can have complex, organic shapes. Control points influence the directions the surface takes. The outermost square below delineates the X/Y extents of the surface.]]\nThe control points determine the shape of the curve.<ref>{{cite book |last=Gershenfeld |first=Neil |authorlink=Neil Gershenfeld |year=1999 |page=141 |title=The Nature of Mathematical Modeling |publisher=[[Cambridge University Press]] |isbn=0-521-57095-6}}</ref> Typically, each point of the curve is computed by taking a [[weighted]] sum of a number of control points. The weight of each point varies according to the governing parameter. For a curve of degree d, the weight of any control point is only nonzero in d+1 intervals of the parameter space. Within those intervals, the weight changes according to a polynomial function (''basis functions'') of degree d. At the boundaries of the intervals, the basis functions go smoothly to zero, the smoothness being determined by the degree of the polynomial.\n\nAs an example, the basis function of degree one is a triangle function. It rises from zero to one, then falls to zero again. While it rises, the basis function of the previous control point falls. In that way, the curve interpolates between the two points, and the resulting curve is a polygon, which is [[continuous function|continuous]], but not [[Differentiable function|differentiable]] at the interval boundaries, or knots. Higher degree polynomials have correspondingly more continuous derivatives. Note that within the interval the polynomial nature of the basis functions and the linearity of the construction make the curve perfectly smooth, so it is only at the knots that discontinuity can arise.\n\nIn many applications the fact that a single control point only influences those intervals where it is active is a highly desirable property, known as '''local support'''. In modeling, it allows the changing of one part of a surface while keeping other parts unchanged.\n\nAdding more control points allows better approximation to a given curve, although only a certain class of curves can be represented exactly with a finite number of control points. NURBS curves also feature a scalar '''weight''' for each control point. This allows for more control over the shape of the curve without unduly raising the number of control points.  In particular, it adds conic sections like circles and ellipses to the set of curves that can be represented exactly. The term ''rational'' in NURBS refers to these weights.\n\nThe control points can have any [[dimensionality]]. One-dimensional points just define a [[scalar (mathematics)|scalar]] function of the parameter. These are typically used in image processing programs to tune the brightness and color curves. Three-dimensional control points are used abundantly in 3D modeling, where they are used in the everyday meaning of the word 'point', a location in 3D space.\nMulti-dimensional points might be used to control sets of time-driven values, e.g. the different positional and rotational settings of a robot arm. NURBS surfaces are just an application of this. Each control 'point' is actually a full vector of control points, defining a curve. These curves share their degree and the number of control points, and span one dimension of the parameter space. By interpolating these control vectors over the other dimension of the parameter space, a continuous set of curves is obtained, defining the surface.\n\n=== Knot vector ===\nThe knot vector is a sequence of parameter values that determines where and how the control points affect the NURBS curve. The number of knots is always equal to the number of control points plus curve degree plus one (i.e. number of control points plus curve order). The knot vector divides the parametric space in the intervals mentioned before, usually referred to as ''knot spans''. Each time the parameter value enters a new knot span, a new control point becomes active, while an old control point is discarded.  \nIt follows that the values in the knot vector should be in nondecreasing order, so (0, 0, 1, 2, 3, 3) is valid while (0, 0, 2, 1, 3, 3) is not.\n\nConsecutive knots can have the same value. This then defines a knot span of zero length, which implies that two control points are activated at the same time (and of course two control points become deactivated). This has impact on continuity of the resulting curve or its higher derivatives; for instance, it allows the creation of corners in an otherwise smooth NURBS curve.\nA number of coinciding knots is sometimes referred to as a knot with a certain '''multiplicity'''. Knots with multiplicity two or three are known as double or triple knots. \nThe multiplicity of a knot is limited to the degree of the curve; since a higher multiplicity would split the curve into disjoint parts and it would leave control points unused. For first-degree NURBS, each knot is paired with a control point.\n\nThe knot vector usually starts with a knot that has multiplicity equal to the order. This makes sense, since this activates the control points that have influence on the first knot span. Similarly, the knot vector usually ends with a knot of that multiplicity.\nCurves with such knot vectors start and end in a control point.\n\nThe values of the knots control the mapping between the input parameter and the corresponding NURBS value. For example, if a NURBS describes a path through space over time, the knots control the time that the function proceeds past the control points. For the purposes of representing shapes, however, only the ratios of the difference between the knot values matter; in that case, the knot vectors (0, 0, 1, 2, 3, 3) and (0, 0, 2, 4, 6, 6) produce the same curve. The positions of the knot values influences the mapping of parameter space to curve space. Rendering a NURBS curve is usually done by stepping with a fixed stride through the parameter range. By changing the knot span lengths, more sample points can be used in regions where the curvature is high. Another use is in situations where the parameter value has some physical significance, for instance if the parameter is time and the curve describes the motion of a robot arm. The knot span lengths then translate into velocity and acceleration, which are essential to get right to prevent damage to the robot arm or its environment. This flexibility in the mapping is what the phrase ''non uniform'' in NURBS refers to.\n\nNecessary only for internal calculations, knots are usually not helpful to the users of modeling software.  Therefore, many modeling applications do not make the knots editable or even visible. It's usually possible to establish reasonable knot vectors by looking at the variation in the control points. More recent versions of NURBS software (e.g., [[Autodesk Maya]] and [[Rhinoceros 3D]]) allow for interactive editing of knot positions, but this is significantly less intuitive than the editing of control points.\n\n=== Construction of the basis functions ===\nThe B-spline basis functions used in the construction of NURBS curves are usually denoted as <math>N_{i,n}(u)</math>, in which <math>i</math> corresponds to the <math>i</math><sup>th</sup>\ncontrol point, and <math>n</math> corresponds with the degree of the basis function.<ref name=nurbs-book>{{cite book |last1=Piegl |first1=Les |last2=Tiller |first2=Wayne |title=The NURBS Book|date=1997 |publisher=Springer |location=Berlin |isbn=3-540-61545-8 |edition=2.}}</ref> The parameter dependence is frequently left out, so we can write <math>N_{i,n}</math>.\nThe definition of these basis functions is recursive in <math>n</math>. \nThe degree-0 functions <math>N_{i,0}</math> are [[piecewise]] [[constant function]]s. They are one on the corresponding knot span and zero everywhere else.\nEffectively, <math>N_{i,n}</math> is a linear interpolation of <math>N_{i,n-1}</math> and <math>N_{i+1,n-1}</math>. The latter two functions are non-zero for\n<math>n</math> knot spans, overlapping for <math>n-1</math> knot spans.  The function <math>N_{i,n}</math> is computed as\n\n[[Image:nurbsbasisconstruct.svg|thumb|right|From top to bottom: Linear basis functions <math>N_{1,1}</math> (blue) and <math>N_{2,1}</math> (green) (top), their weight functions <math>f</math> and <math>g</math> (middle) and the resulting quadratic basis function (bottom). The knots are 0, 1, 2, and 2.5]]\n\n: <math>N_{i,n} = f_{i,n} N_{i,n-1} + g_{i+1,n} N_{i+1,n-1}</math>\n\n<math>f_i</math> rises linearly from zero to one on the interval where <math>N_{i,n-1}</math> is non-zero, while <math>g_{i+1}</math> falls from one to zero on the interval where <math>N_{i+1,n-1}</math> is non-zero. As mentioned before, <math>N_{i,1}</math> is a triangular function, nonzero over two knot spans rising from zero to one on the first, and falling to zero on the second knot span. Higher order basis functions are non-zero over corresponding more knot spans and have correspondingly higher degree. If <math>u</math> is the parameter, and <math>k_i</math> is the <math>i</math><sup>th</sup> knot, we can write the functions <math>f</math> and <math>g</math> as\n\n: <math>f_{i,n}(u) = {{u - k_i} \\over {k_{i+n} - k_i}}</math>\n\nand\n\n: <math>g_{i,n}(u) = 1 - f_{i,n}(u) = {{k_{i+n} - u} \\over {k_{i+n} - k_{i}}}</math>\n\nThe functions <math>f</math> and <math>g</math> are positive when the corresponding lower order basis functions are non-zero. By [[mathematical induction|induction]] on n it follows that the basis functions are non-negative for all values of <math>n</math> and <math>u</math>. This makes the computation of the basis functions numerically stable.\n\nAgain by induction, it can be proved that the sum of the basis functions for a particular value of the parameter is unity. This is known as the '''partition of unity''' property of the basis functions.\n\n[[Image:nurbsbasislin2.png|frame|Linear basis functions]]\n\n[[Image:nurbsbasisquad2.png|frame|Quadratic basis functions]]\n\nThe figures show the linear and the quadratic basis functions for the knots {..., 0, 1, 2, 3, 4, 4.1, 5.1, 6.1, 7.1, ...}\n\nOne knot span is considerably shorter than the others. On that knot span, the peak in the quadratic basis function is more distinct, reaching almost one. Conversely, the adjoining basis functions fall to zero more quickly. In the geometrical interpretation, this means that the curve approaches the corresponding control point closely. In case of a double knot, the length of the knot span becomes zero and the peak reaches one exactly. The basis function is no longer differentiable at that point. The curve will have a sharp corner if the neighbour control points are not collinear.\n\n=== General form of a NURBS curve ===\n\nUsing the definitions of the basis functions <math>N_{i,n}</math> from the previous paragraph, a NURBS curve takes the following form:<ref name=nurbs-book/>\n\n: <math>C(u) = \\sum_{i=1}^{k} {\\frac\n\t{N_{i,n}w_i}\n\t{\\sum_{j=1}^k N_{j,n}w_j}}\n\t\\mathbf{P}_i = \\frac\n\t{\\sum_{i=1}^k {N_{i,n}w_i \\mathbf{P}_i}}\n\t{\\sum_{i=1}^k {N_{i,n}w_i}}\n\t</math>\n\nIn this, <math>k</math> is the number of control points <math>\\mathbf{P}_i</math> and <math>w_i</math> are the corresponding weights. The denominator is a normalizing factor that evaluates to one if all weights are one. This can be seen from the partition of unity property of the basis functions. It is customary to write this as\n\n: <math>C(u)=\\sum_{i=1}^k R_{i,n}(u)\\mathbf{P}_i</math>\n\nin which the functions\n\n: <math>R_{i,n}(u) = {N_{i,n}(u)w_i \\over \\sum_{j=1}^k N_{j,n}(u)w_j}</math>\n\nare known as the ''rational basis functions''.\n\n=== General form of a NURBS surface ===\n\nA NURBS surface is obtained as the [[tensor product]] of two NURBS curves, thus using two independent parameters <math>u</math> and <math>v</math> (with indices <math>i</math> and <math>j</math> respectively):<ref name=nurbs-book/>\n\n: <math>S(u,v) = \\sum_{i=1}^k \\sum_{j=1}^l R_{i,j}(u,v) \\mathbf{P}_{i,j} </math>\n\nwith\n\n: <math>R_{i,j}(u,v) = \\frac {N_{i,n}(u) N_{j,m}(v) w_{i,j}} {\\sum_{p=1}^k \\sum_{q=1}^l N_{p,n}(u) N_{q,m}(v) w_{p,q}}</math>\n\nas rational basis functions.\n\n== Manipulating NURBS objects ==\n[[Image:motoryacht design i.png|thumb|250px|right|Motoryacht design.]]\nA number of transformations can be applied to a NURBS object. For instance, if some curve is defined using a certain degree and N control points, the same curve can be expressed using the same degree and N+1 control points. In the process a number of control points change position and a knot is inserted in the knot vector.\nThese manipulations are used extensively during interactive design. When adding a control point, the shape of the curve should stay the same, forming the starting point for further adjustments. A number of these operations are discussed below.<ref name=nurbs-book/><ref>{{cite journal | last1 = Piegl | first1 = L. | year = | title = Modifying the shape of rational B-splines. Part 1: curves | url = | journal = Computer-Aided Design | volume = 21 | issue = 8| pages = 509–518 | doi = 10.1016/0010-4485(89)90059-6 }}</ref>\n\n=== Knot insertion ===\nAs the term suggests, '''knot insertion''' inserts a knot into the knot vector. If the degree of the curve is <math>n</math>, then <math>n-1</math> control points are replaced by <math>n</math> new ones. The shape of the curve stays the same.\n\nA knot can be inserted multiple times, up to the maximum multiplicity of the knot. This is sometimes referred to as '''knot refinement''' and can be achieved by an algorithm that is more efficient than repeated knot insertion.\n\n=== Knot removal ===\n'''Knot removal''' is the reverse of knot insertion. Its purpose is to remove knots and the associated control points in order to get a more compact representation. Obviously, this is not always possible while retaining the exact shape of the curve. In practice, a tolerance in the accuracy is used to determine whether a knot can be removed. The process is used to clean up after an interactive session in which control points may have been added manually, or after importing\na curve from a different representation, where a straightforward conversion process leads to redundant control points.\n\n=== Degree elevation ===\nA NURBS curve of a particular degree can always be represented by a NURBS curve of higher degree. This is frequently used when combining separate NURBS curves,\ne.g., when creating a NURBS surface interpolating between a set of NURBS curves or when unifying adjacent curves. In the process, the different curves should be brought to the same degree, usually the maximum degree of the set of curves. The process is known as '''degree elevation'''.\n\n=== Curvature ===\nThe most important property in [[differential geometry]] is the [[curvature]] <math>\\kappa</math>. It describes the local properties (edges, corners, etc.) and relations between the first and second derivative, and thus, the precise curve shape. Having determined the derivatives it is easy to compute <math>\\kappa=\\frac{|r'(t) \\times r''(t)|}{|r'(t)|^3}</math> or approximated as the arclength from the second derivative <math>\\kappa=|r''(s_o)|</math>. The direct computation of the curvature <math>\\kappa</math> with these equations is the big advantage of parameterized curves against their polygonal representations.\n\n== Example: a circle ==\n[[File:NURBS-circle-3D.svg|thumb|300px|NURBS have the ability to exactly describe circles. Here, the black triangle is the control polygon of a NURBS curve (shown at w=1). The Blue dotted line shows the corresponding control polygon of a B-spline curve in 3D [[homogeneous coordinates]], formed by multiplying the NURBS by the control points by the corresponding weights. The blue parabolas are the corresponding B-spline curve in 3D, consisting of three parabolas. By choosing the NURBS control points and weights, the parabolas are parallel to the opposite face of the gray cone (with its tip at the 3D origin), so dividing by ''w'' to project the parabolas onto the ''w''=1 plane results in circular arcs (red circle; see [[conic section]]).]]\nNon-rational splines or [[Bézier curve]]s may approximate a circle, but they cannot represent it exactly. Rational splines can represent any conic section, including the circle, exactly. This representation is not unique, but one possibility appears below:\n<center>\n{| class=\"wikitable\"  style=\"text-align: center\"\n! style=\"width:6em;\" | ''x''\n! style=\"width:6em;\" | ''y''\n! style=\"width:6em;\" | ''z''\n! style=\"width:6em;\" | Weight\n|-\n|  1 ||  0 || 0 || 1\n|-\n|  1 ||  1 || 0 || <math>\\scriptstyle\\frac{\\sqrt{2}}{2}</math>\n|-\n|  0 ||  1 || 0 || 1\n|-\n| -1 ||  1 || 0 || <math>\\scriptstyle\\frac{\\sqrt{2}}{2}</math>\n|-\n| -1 ||  0 || 0 || 1\n|-\n| -1 || -1 || 0 || <math>\\scriptstyle\\frac{\\sqrt{2}}{2}</math>\n|-\n|  0 || -1 || 0 || 1\n|-\n|  1 || -1 || 0 || <math>\\scriptstyle\\frac{\\sqrt{2}}{2}</math>\n|-\n|  1 ||  0 || 0 || 1\n|}\n</center>\n\nThe order is three, since a circle is a quadratic curve and the spline's order is one more than the degree of its piecewise polynomial segments. The knot vector is <math>\\{0, 0, 0, \\pi/2, \\pi/2, \\pi, \\pi, 3\\pi/2, 3\\pi/2, 2\\pi, 2\\pi, 2\\pi\\}\\,</math>. The circle is composed of four quarter circles, tied together with double knots. Although double knots in a third order NURBS curve would normally result in loss of continuity in the first derivative, the control points are positioned in such a way that the first derivative is continuous. In fact, the curve is infinitely differentiable everywhere, as it must be if it exactly represents a circle.\n\nThe curve represents a circle exactly, but it is not exactly parametrized in the circle's arc length. This means, for example, that the point at <math>t</math> does not lie at <math>(\\sin(t), \\cos(t))</math> (except for the start, middle and end point of each quarter circle, since the representation is symmetrical). This would be impossible, since the ''x'' coordinate of the circle would provide an exact rational polynomial expression for <math>\\cos(t)</math>, which is impossible. The circle does make one full revolution as its parameter <math>t</math> goes from 0 to <math>2\\pi</math>, but this is only because the knot vector was arbitrarily chosen as multiples of <math>\\pi/2</math>.\n\n== See also ==\n* [[Spline (mathematics)|Spline]]\n* [[Bézier surface]]\n* [[de Boor's algorithm]]\n* [[Triangle mesh]]\n* [[Point cloud]]\n* [[Rational motion]]\n* [[Isogeometric analysis]]\n\n== References ==\n<references/>\n\n== External links ==\n* [http://www.rw-designer.com/NURBS Clear explanation of NURBS for non-experts]\n* [http://geometrie.foretnik.net/files/NURBS-en.swf Interactive NURBS demo]\n* [http://www.cs.wpi.edu/~matt/courses/cs563/talks/nurbs.html About Nonuniform Rational B-Splines - NURBS]\n* [https://github.com/msteinbeck/tinyspline TinySpline: Opensource C-library with bindings for various languages]\n\n{{DEFAULTSORT:Non-Uniform Rational B-Spline}}\n[[Category:Computer-aided design]]\n[[Category:Splines (mathematics)]]\n[[Category:3D computer graphics]]\n[[Category:Interpolation]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Padua points",
      "url": "https://en.wikipedia.org/wiki/Padua_points",
      "text": "In [[polynomial interpolation]] of [[multivariate interpolation|two variables]], the '''Padua points''' are the first known example (and up to now the only one) of a [[unisolvent point set]] (that is, the interpolating polynomial is unique) with ''minimal growth'' of their [[Lebesgue constant (interpolation)|Lebesgue constant]], proven to be O(log<sup>2</sup>&nbsp;''n'')\n.<ref name=\"bosGenCurve\">\n{{citation\n|first1 = Marco \n|last1 = Caliari\n|first2 = Len\n|last2 = Bos\n|first3 = Stefano\n|last3 = de Marchi\n|first4 = Marco \n|last4 = Vianello\n|first5 = Yuan\n|last5 = Xu\n|title = Bivariate Lagrange interpolation at the Padua points: the generating curve approach\n|journal = J. Approx. Theory\n|volume = 143\n|issue = 1\n|pages = 15–25\n|year = 2006\n|doi = 10.1016/j.jat.2006.03.008\n|arxiv = math/0604604\n}}</ref>\nTheir name is due to the [[University of Padua]], where they were originally discovered.<ref name=\"newNodalSet\">\n{{citation\n|first1 = Stefano \n|last1 = de Marchi\n|first2 = Marco\n|last2 = Caliari\n|first3 = Marco\n|last3 = Vianello\n|title = Bivariate polynomial interpolation at new nodal sets\n|journal = Appl. Math. Comput.\n|volume = 165\n|issue = 2\n|pages = 261–274\n|year = 2005\n|doi = 10.1016/j.amc.2004.07.001\n}}</ref>\n\nThe points are defined in the [[domain (mathematics)|domain]] <math>\\scriptstyle [-1,1]\\times [-1,1]\\subset \\mathbb{R}^2</math>. It is possible to use the points with four orientations, obtained with subsequent 90-degree rotations: this way we get four different families of Padua points.\n\n== The four families ==\n[[Image:Padua points fam 1 degree 5.png|thumb|Padua points of the first family and of degree 5, plotted with their generating curve.]]\n[[Image:Padua points fam 1 degree 6.png|thumb|Padua points of the first family and of degree 6, plotted with their generating curve.]]\n\nWe can see the Padua point as a \"[[sampling (signal processing)|sampling]]\" of a [[parametric curve]], called ''generating curve'', which is slightly different for each of the four families, so that the points for interpolation degree <math>n</math> and family <math>s</math> can be defined as\n\n:<math>\\text{Pad}_n^s=\\lbrace\\mathbf{\\xi}=(\\xi_1,\\xi_2)\\rbrace=\\left\\lbrace\\gamma_s\\left(\\frac{k\\pi}{n(n+1)}\\right),k=0,\\ldots,n(n+1)\\right\\rbrace.</math>\n\nActually, the Padua points lie exactly on the self-intersections of the curve, and on the intersections of the curve with the boundaries of the square <math>[-1,1]^2</math>. The [[cardinality]] of the set <math>\\scriptstyle \\text{Pad}_n^s</math> is <math>\\scriptstyle |\\text{Pad}_n^s|=N=\\frac{(n+1)(n+2)}{2}</math>. Moreover, for each family of Padua points, two points lie on consecutive vertices of the square <math>[-1,1]^2</math>, <math>2n-1</math> points lie on the edges of the square, and the remaining points lie on the self-intersections of the generating curve inside the square.<ref name=\"bivLagrange\">\n{{citation\n|first1 = Marco\n|last1 = Caliari\n|first2 = Stefano\n|last2 = de Marchi\n|first3 = Marco\n|last3 = Vianello\n|title = Algorithm 886: Padua2D: Lagrange interpolation at Padua points on bivariate domains\n|journal = ACM T. Math. Software\n|year = 2008\n|volume = 35\n|issue = 3\n}}\n</ref><ref name=\"idealTheory\">\n{{citation\n|first1 = Len\n|last1 = Bos\n|first2 = Stefano\n|last2 = de Marchi\n|first3 = Marco\n|last3 = Vianello\n|first4 = Yuan\n|last4 = Xu\n\n|title = Bivariate Lagrange interpolation at the Padua points: the ideal theory approach\n|journal = [[Numerische Mathematik]]\n|year = 2007\n|volume = 108\n|issue = 1\n|pages = 43–57\n|doi = 10.1007/s00211-007-0112-z\n|arxiv = math/0604604\n}}</ref>\n\nThe four generating curves are ''closed'' parametric curves in the interval <math>[0,2\\pi]</math>, and are a special case of [[Lissajous curve]]s.\n\n=== The first family ===\nThe generating curve of Padua points of the first family is\n\n:<math>\\gamma_1(t)=[-\\cos((n+1)t),-\\cos(nt)],\\quad t\\in [0,\\pi].</math>\n\nIf we sample it as written above, we have:\n\n:<math>\\text{Pad}_n^1=\\lbrace\\mathbf{\\xi}=(\\mu_j,\\eta_k), 0\\le j\\le n; 1\\le k\\le\\lfloor\\frac{n}{2}\\rfloor+1+\\delta_j\\rbrace,</math>\nwhere <math>\\delta_j=0</math> when <math>n</math> is even or odd but <math>j</math> is even, <math>\\delta_j=1</math>\nif <math>n</math> and <math>k</math> are both odd \n\nwith\n\n:<math>\\mu_j=\\cos\\left(\\frac{j\\pi}{n}\\right), \\eta_k=\n\\begin{cases}\n\\cos\\left(\\frac{(2k-2)\\pi}{n+1}\\right) & j\\mbox{ odd} \\\\\n\\cos\\left(\\frac{(2k-1)\\pi}{n+1}\\right) & j\\mbox{ even.}\n\\end{cases}\n</math>\n\nFrom this follows that the Padua points of first family will have two vertices on the bottom if <math>n</math> is even, or on the left if <math>n</math> is odd.\n\n=== The second family ===\nThe generating curve of Padua points of the second family is\n\n:<math>\\gamma_2(t)=[-\\cos(nt),-\\cos((n+1)t)],\\quad t\\in [0,\\pi],</math>\n\nwhich leads to have vertices on the left if <math>n</math> is even and on the bottom if <math>n</math> is odd.\n\n=== The third family ===\nThe generating curve of Padua points of the third family is\n\n:<math>\\gamma_3(t)=[\\cos((n+1)t),\\cos(nt)],\\quad t\\in [0,\\pi],</math>\n\nwhich leads to have vertices on the top if <math>n</math> is even and on the right if <math>n</math> is odd.\n\n=== The fourth family ===\nThe generating curve of Padua points of the fourth family is\n\n:<math>\\gamma_4(t)=[\\cos(nt),\\cos((n+1)t)],\\quad t\\in [0,\\pi],</math>\n\nwhich leads to have vertices on the right if <math>n</math> is even and on the top if <math>n</math> is odd.\n\n== The interpolation formula ==\nThe explicit representation of their fundamental [[Lagrange polynomial]] is based on the [[reproducing kernel]] <math>\\scriptstyle K_n(\\mathbf{x},\\mathbf{y})</math>, <math>\\scriptstyle \\mathbf{x}=(x_1,x_2)</math> and <math>\\scriptstyle \\mathbf{y}=(y_1,y_2)</math>, of the [[function space|space]] <math>\\scriptstyle\\Pi_n^2([-1,1]^2)</math> equipped with the [[inner product]]\n\n:<math>\\langle f,g\\rangle =\\frac{1}{\\pi^2} \\int_{[-1,1]^2} f(x_1,x_2)g(x_1,x_2)\\frac{dx_1}{\\sqrt{1-x_1^2}}\\frac{dx_2}{\\sqrt{1-x_2^2}}\n</math>\n\ndefined by\n\n:<math>K_n(\\mathbf{x},\\mathbf{y})=\\sum_{k=0}^n\\sum_{j=0}^k \\hat T_j(x_1)\\hat T_{k-j}(x_2)\\hat T_j(y_1)\\hat T_{k-j}(y_2)\n</math>\n\nwith <math>\\scriptstyle \\hat T_j</math> representing the normalized [[Chebyshev polynomial]] of degree <math>j</math> (that is, <math>\\scriptstyle \\hat T_0=T_0</math>, <math>\\scriptstyle \\hat T_p=\\sqrt{2}T_p</math> where <math>\\scriptstyle T_p(\\cdot)=\\cos(p\\arccos(\\cdot))</math> is the classical Chebyshev polynomial ''of first kind'' of degree <math>p</math>).<ref name=\"bivLagrange\" /> For the four families of Padua points, which we may denote by <math>\\scriptstyle \\text{Pad}_n^s=\\lbrace\\mathbf{\\xi}=(\\xi_1,\\xi_2)\\rbrace</math>, <math>s=\\lbrace 1,2,3,4\\rbrace</math>, the interpolation formula of order <math>n</math> of the function <math>\\scriptstyle f\\colon [-1,1]^2\\to\\mathbb{R}^2</math> on the generic target point <math>\\scriptstyle \\mathbf{x}\\in [-1,1]^2</math> is then\n\n:<math>\n\\mathcal{L}_n^s f(\\mathbf{x})=\\sum_{\\mathbf{\\xi}\\in\\text{Pad}_n^s}f(\\mathbf{\\xi})L^s_{\\mathbf\\xi}(\\mathbf{x})\n</math>\n\nwhere <math>\\scriptstyle L^s_{\\mathbf\\xi}(\\mathbf{x})</math> is the fundamental Lagrange polynomial\n\n:<math>L^s_{\\mathbf\\xi}(\\mathbf{x})=w_{\\mathbf\\xi}(K_n(\\mathbf\\xi,\\mathbf{x})-T_n(\\xi_i)T_n(x_i)),\\quad s=1,2,3,4,\\quad i=2-(s\\mod 2).\n</math>\n\nThe weights <math>\\scriptstyle w_{\\mathbf\\xi}</math> are defined as\n\n:<math>\nw_{\\mathbf\\xi}=\\frac{1}{n(n+1)}\\cdot\n\\begin{cases}\n\\frac{1}{2}\\text{ if }\\mathbf\\xi\\text{ is a vertex point}\\\\\n1\\text{ if }\\mathbf\\xi\\text{ is an edge point}\\\\\n2\\text{ if }\\mathbf\\xi\\text{ is an interior point.}\n\\end{cases}\n</math>\n\n== References ==\n<references/>\n\n== External links ==\n* [http://www.math.unipd.it/~marcov/CAApadua.html List of publications related to the Padua points and some interpolation software].\n\n[[Category:Interpolation]]"
    },
    {
      "title": "Perfect spline",
      "url": "https://en.wikipedia.org/wiki/Perfect_spline",
      "text": "{{Unreferenced|date=December 2006}}\n\nIn the [[mathematical]] subfields [[Complex analysis|function theory]] and [[numerical analysis]], a univariate polynomial [[Spline (mathematics)|spline]] of order <math>m</math> is called a '''perfect spline''' if its <math>m</math>-th derivative is equal to <math>+1</math> or <math>-1</math> between knots and changes its sign at every knot.\n\nThe term was coined by [[Isaac Jacob Schoenberg]].\n\nPerfect splines often give solutions to various [[extremal problem]]s in mathematics. For example, [[norm (mathematics)|norms]] of periodic perfect splines (they are sometimes called [[Euler spline|Euler perfect splines]]) are equal to [[Favard constant|Favard's constants]].\n\n{{DEFAULTSORT:Perfect Spline}}\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]\n\n{{Mathapplied-stub}}"
    },
    {
      "title": "Polyharmonic spline",
      "url": "https://en.wikipedia.org/wiki/Polyharmonic_spline",
      "text": "'''Polyharmonic splines''' are used for [[function approximation]] and data [[interpolation]].  They are very useful for interpolating and fitting scattered data in many dimensions. Special cases include [[thin plate spline|thin plate splines]] and<ref>R.L. Harder and R.N. Desmarais: [http://www.mendeley.com/research/interpolation-using-surface-splines-1 Interpolation using surface splines]. Journal of Aircraft, 1972, Issue 2, pp. 189-191</ref><ref>J. Duchon: Splines minimizing rotation-invariant semi-norms in Sobolev spaces. Constructive Theory of Functions of Several Variables, W. Schempp and K. Zeller (eds), Springer, Berlin, pp.85-100</ref> and natural cubic splines in one dimension.<ref>{{cite book |last1=Wendland |first1=Holger |title=Scattered Data Approximation |date=2005 |publisher=Cambridge University Press |isbn=0521843359 |page=9}}</ref>\n\n== Definition ==\nA polyharmonic spline is a linear combination of polyharmonic [[radial basis function]]s (RBFs) denoted by <math>\\phi</math> plus a polynomial term:\n{{NumBlk|:|<math>\nf(\\mathbf{x})  =  \n\\sum_{i=1}^N w_i  \\phi(|\\mathbf{x} -\\mathbf{c}_i|) + \n                  \\mathbf{v}^{\\textrm{T}} \n \\begin{bmatrix} 1 \\\\ \\mathbf{x} \\end{bmatrix} \n</math>|{{EquationRef|1}}}}\n\nwhere\n\n[[Image:Polyharmonic-splines-basic-functions.png|thumb|350px|right|Polyharmonic basis functions]]\n* <math>\\mathbf{x} = [x_1 \\ x_2 \\ \\cdots \\ x_{d}]^{\\textrm{T}}</math> (<math>\\textrm{T}</math> denotes matrix transpose, meaning <math>\\mathbf{x}</math> is a column vector) is a real-valued vector of <math>d</math> independent variables,\n* <math>\\mathbf{c}_i = [c_{i,1} \\ c_{i,2} \\ \\cdots \\ c_{i,d}]^{\\textrm{T}}</math> are <math>N</math> vectors of the same size as <math>\\mathbf{x}</math> (often called centers) that the curve or surface must interpolate,\n* <math>\\mathbf{w} = [w_1 \\ w_2 \\ \\cdots \\ w_N]^{\\textrm{T}}</math> are the <math>N</math> weights of the RBFs,\n* <math>\\mathbf{v} = [v_1 \\ v_2 \\ \\cdots \\ v_{d+1}]^{\\textrm{T}}</math> are the <math>d+1</math> weights of the polynomial.\n\nThe polynomial with the coefficients <math>\\mathbf{v}</math> improves fitting accuracy for polyharmonic smoothing splines and also improves extrapolation away from the centers <math>\\mathbf{c}_i.</math> See figure below for comparison of splines with polynomial term and without polynomial term.\n\nThe polyharmonic RBFs are of the form:\n\n:<math> \n\\begin{matrix}\n  \\phi(r) = \\begin{cases}\n                r^k & \\mbox{with } k=1,3,5,\\dots, \\\\\n                r^k \\ln(r) & \\mbox{with } k=2,4,6,\\dots\n            \\end{cases} \\\\[5mm]\n   r = |\\mathbf{x} - \\mathbf{c}_i| \n    = \\sqrt{ (\\mathbf{x} - \\mathbf{c}_i)^T \\, (\\mathbf{x} - \\mathbf{c}_i) }.\n \\end{matrix} \n</math>\n\nOther values of the exponent <math>k</math> are not useful (such as <math> \\phi(r) = r^2 </math>), because a solution of the interpolation problem might not exist. To avoid problems at <math>r=0</math> (since <math>\\log(0) = -\\infty</math>), the polyharmonic RBFs with the natural logarithm might be implemented as:\n\n:<math>\n\\phi(r) = \\begin{cases}\n             r^{k-1} \\ln(r^r) & \\mbox{for } r < 1 \\\\\n             r^k \\ln(r)       & \\mbox{for } r \\ge 1\n          \\end{cases}.\n</math>\n\nThe weights <math>w_i</math> and <math>v_j</math> are determined such that the function interpolates <math>N</math> given points <math>(\\mathbf{c}_i, f_i)</math>  (for <math>i=1,2,\\dots,N</math>) and fulfills the <math>d+1</math> orthogonality conditions\n\n:<math> \n  \\sum_{i=1}^N w_i=0, \\;\\; \\sum_{i=1}^N w_i \\mathbf{c}_i=\\mathbf{0}.\n</math>\n\nAll together, these constraints are equivalent to the symmetric linear system of equations\n\n{{NumBlk|:|<math> \n\\begin{bmatrix}\n  A & B \\\\\n  B^{\\textrm{T}} & 0 \\end{bmatrix}\n\\;\n\\begin{bmatrix}\n  \\mathbf{w} \\\\\n  \\mathbf{v}\n\\end{bmatrix} \\; = \\; \n\\begin{bmatrix}\n  \\mathbf{f} \\\\\n  \\mathbf{0}\n\\end{bmatrix}\\;\\;\\;\\;\n</math>|{{EquationRef|2}}}}\n\nwhere\n\n:<math> \n  A_{i,j} =  \\phi(|\\mathbf{c}_i - \\mathbf{c}_j|), \\quad\n  B =  \n  \\begin{bmatrix}\n         1       &       1      & \\cdots & 1 \\\\\n    \\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_{N}\n  \\end{bmatrix}^{\\textrm{T}}, \\quad\n  \\mathbf{f}  = [f_1, f_2, \\cdots, f_N]^{\\textrm{T}}.\n</math>\n\nIn order for this system of equations to have a unique solution, <math>B</math> must be full rank.  <math>B</math> is full rank for very mild conditions on the input data.  For example, in two dimensions, three centers forming a non-degenerate triangle ensure that <math>B</math> is full rank, and in three dimensions, four centers forming a non-degenerate tetrahedron ensure that B is full rank.  As explained later, the linear transformation resulting from the restriction of the domain of the linear transformation <math>A</math> to the [[Kernel (linear algebra)|null space]] of <math>\\textstyle{ B^{\\textrm{T}} }</math> is positive definite.  This means that if <math>B</math> is full rank, the system of equations ({{EquationNote|2}}) always has a unique solution and it can be solved using the [[Cholesky decomposition]] after a suitable transformation.  The computed weights allow evaluation of the spline for any <math>\\mathbf{x}\\in\\mathbb{R}^d</math> using equation ({{EquationNote|1}}).  Many practical details of implementing and using polyharmonic splines are explained in Fasshauer.<ref>G.F. Fasshauer G.F.: [https://books.google.com/books?id=gtqBdMEqryEC Meshfree Approximation Methods with MATLAB]. World Scientific Publishing Company, 2007, ISPN-10: 9812706348</ref> In Iske<ref>A. Iske: [http://www.springeronline.com/sgw/cda/frontpage/0,10735,5-10042-22-22344683-0,00.html Multiresolution Methods in Scattered Data Modelling], Lecture Notes in Computational Science and Engineering, 2004, Vol. 37, {{ISBN|3-540-20479-2}}, Springer-Verlag, Heidelberg.</ref> polyharmonic splines are treated as special cases of other multiresolution methods in scattered data modelling.\n\n== Reason for the name \"polyharmonic\" ==\nA polyharmonic equation is a [[partial differential equation]] of the form <math>\\Delta^m f = 0</math> for any natural number <math>m</math>.  For example, the [[biharmonic equation]] is <math>\\Delta^2 f = 0</math> and the triharmonic equation is <math>\\Delta^3 f = 0</math>.  All the polyharmonic radial basis functions are solutions of a polyharmonic equation (or more accurately, a modified polyharmonic equation with a [[Dirac delta function]] on the right hand side instead of 0).  For example, the thin plate radial basis function is a solution of the modified 2-dimensional biharmonic equation.<ref name=\":0\">{{Cite journal|url = http://www.univie.ac.at/nuhag-php/bibtex/open_files/po94_M%20J%20D%20Powell%2003%2093.pdf|title = Some algorithms for thin plate spline interpolation to functions of two variables|last = Powell|first = M. J. D.|date = 1993|journal = Cambridge University Dept. of Applied Mathematics and Theoretical Physics technical report|doi = |pmid = |access-date = January 7, 2016}}</ref>  Applying the 2D [[Laplace operator]] (<math>\\Delta = \\partial_{xx} + \\partial_{yy}</math>) to the thin plate radial basis function <math>f_{\\text{tp}}(x,y) = (x^2+y^2) \\log \\sqrt{x^2+y^2}</math> either by hand or using a [[computer algebra system]] shows that <math>\\Delta f_{\\text{tp}} = 4 + 4\\log r</math>.  Applying the Laplace operator to <math>\\Delta f_{\\text{tp}}</math> (this is <math>\\Delta^2 f_{\\text{tp}}</math>) yields 0.  But 0 is not exactly correct.   To see this, replace <math>r^2=x^2+y^2</math> with <math>\\rho^2 = x^2+y^2+h^2</math> (where <math>h</math> is some small number tending to 0).  The Laplace operator applied to <math>4 \\log \\rho</math> yields <math>\\Delta^2 f_{\\text{tp}} = 8h^2 / \\rho^4</math>.  For <math>(x,y)=(0,0),</math> the right hand side of this equation approaches infinity as <math>h</math> approaches 0.  For any other <math>(x,y)</math>, the right hand side approaches 0 as <math>h</math> approaches 0.  This indicates that the right hand side is a Dirac delta function.  A computer algebra system will show that\n\n:<math>\\lim_{h \\to 0}\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} 8h^2/(x^2+y^2+h^2)^2 \\,dx \\,dy = 8\\pi.</math>\n\nSo the thin plate radial basis function is a solution of the equation <math>\\Delta^2 f_{\\text{tp}} = 8\\pi\\delta(x,y)</math>.\n\nApplying the 3D Laplacian (<math>\\Delta = \\partial_{xx} + \\partial_{yy} + \\partial_{zz}</math>) to the biharmonic RBF <math>f_{\\text{bi}}(x,y,z)=\\sqrt{x^2+y^2+z^2}</math> yields <math>\\Delta f_{\\text{bi}} = 2/r</math> and applying the 3D <math>\\Delta^2</math> operator to the triharmonic RBF <math>f_{\\text{tri}}(x,y,z) = (x^2+y^2+z^2)^{3/2}</math> yields <math>\\Delta^2 f_{\\text{tri}} = 24/r</math>.  Letting <math>\\rho^2 = x^2+y^2+z^2+h^2</math> and computing <math>\\Delta(1/\\rho) = -3h^2 / \\rho^5</math> again indicates that the right hand side of the PDEs for the biharmonic and triharmonic RBFs are Dirac delta functions.  Since\n\n:<math>\\lim_{h \\to 0}\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}-3h^2/(x^2+y^2+z^2+h^2)^{5/2} \\,dx \\,dy \\,dz = -4\\pi,</math>\n\nthe exact PDEs satisfied by the biharmonic and triharmonic RBFs are <math>\\Delta^2 f_{\\text{bi}} = -8\\pi\\delta(x,y,z)</math> and <math>\\Delta^3 f_{\\text{tri}} = -96\\pi\\delta(x,y,z)</math>.\n\n== Polyharmonic smoothing splines ==\nPolyharmonic splines minimize\n\n{{NumBlk|:|<math>\\sum_{i=1}^N (f(\\mathbf{c}_i) - f_i)^2 + \\lambda \\int\\limits_{\\mathcal{B} \\subset \\mathbb{R}^d} |\\nabla^m f|^2 \\,d\\mathbf{x}</math> |{{EquationRef|3}}}}\n\nwhere <math>\\mathcal{B}</math> is some box in <math>\\mathbb{R}^d</math> containing a neighborhood of all the centers, <math>\\lambda</math> is some positive constant, and <math>\\nabla^m f</math> is the vector of all <math>m</math>th order partial derivatives of <math>f.</math>  For example, in 2D <math>\\nabla^1 f = (f_x\\ f_y)</math> and  <math>\\nabla^2 f = (f_{xx} \\ f_{xy} \\ f_{yx} \\ f_{yy})</math> and in 3D <math>\\nabla^2 f = (f_{xx} \\ f_{xy} \\ f_{xz} \\ f_{yx} \\ f_{yy} \\ f_{yz} \\ f_{zx} \\ f_{zy} \\ f_{zz})</math>.   In 2D <math>|\\nabla^2 f|^2 = f_{xx}^2 + 2f_{xy}^2 + f_{yy}^2,</math> making the integral the simplified [[thin plate energy functional]].\n\nTo show that polyharmonic splines minimize equation ({{EquationNote|3}}), the fitting term must be transformed into an integral using the definition of the Dirac delta function:\n\n:<math>\\sum_{i=1}^N (f(\\mathbf{c}_i) - f_i)^2 = \\int\\limits_{\\mathcal{B}}\\sum_{i=1}^N (f(\\mathbf{x}) - f_i)^2 \\delta(\\mathbf{x} - \\mathbf{c}_i) \\,d\\mathbf{x}.</math>\n\nSo equation ({{EquationNote|3}}) can be written as the functional\n\n:<math>J[f] = \\int\\limits_{\\mathcal{B}} F(\\mathbf{x},f, \\partial^{\\alpha_1}f, \\partial^{\\alpha_2}f, \\dots \\partial^{\\alpha_n}f ) \\,d\\mathbf{x} = \\int\\limits_{\\mathcal{B}} \\left[ \\sum_{i=1}^N (f(\\mathbf{x}) - f_i)^2 \\delta(\\mathbf{x} - \\mathbf{c}_i) + \\lambda |\\nabla^m f|^2 \\right]\\,d\\mathbf{x}.</math>\n\nwhere <math>\\alpha_i</math> is a [[Multi-index notation|multi-index]] that ranges over all partial derivatives of order <math>m</math> for <math>\\mathbb{R}^d.</math>  In order to apply the Euler-Lagrange equation for a single function of multiple variables and higher order derivatives, the quantities\n\n:<math>{\\partial F \\over\\partial f} = 2\\sum_{i=1}^N (f(\\mathbf{x}) - f_i) \\delta(\\mathbf{x} - x_i)</math>\n\nand\n\n:<math>\\sum_{i=1}^{m^2} \\partial^{\\alpha_i} {\\partial F \\over\\partial (\\partial^{\\alpha_i}f)} = 2\\lambda \\Delta^m f</math>\n\nare needed.  Inserting these quantities into the  E-L equation shows that\n\n{{NumBlk|:|<math>\\left( \\sum_{i=1}^N (f(\\mathbf{x}) - f_i) \\delta(\\mathbf{x} - \\mathbf{c}_i) \\right) + (-1)^{m} \\lambda \\Delta^m f = 0.</math> |{{EquationRef|4}}}}\n\nA weak solution <math>f(\\mathbf{x})</math> of ({{EquationNote|4}}) satisfies\n\n:{{NumBlk|:|\n<math>\\int\\limits_{\\mathcal{B}}  \n\\left( \\sum_{i=1}^N (f(\\mathbf{x}) - f_i) \\delta(\\mathbf{x} - \\mathbf{c}_i) \\right) g(\\mathbf{x}) + (-1)^m\\lambda (\\Delta^m f) g(\\mathbf{x}) \\,d\\mathbf{x} = 0</math> \n| {{EquationRef|5}}}}\n\nfor all smooth test functions <math>g</math> that vanish outside of <math>\\mathcal{B}.</math>  A weak solution of equation ({{EquationNote|4}}) will still minimize ({{EquationNote|3}}) while getting rid of the delta function through integration.<ref>{{Cite book|title=Partial Differential Equations|last=Evans|first=Lawrence|publisher=American Mathematical Society|year=1998|isbn=0-8218-0772-2|location=Providence|pages=450-452|via=}}</ref>\n\nLet <math>f</math> be a polyharmonic spline as defined by equation ({{EquationNote|1}}).  The following calculations will show that <math>f</math> satisfies ({{EquationNote|5}}).  Applying the <math>\\Delta^m</math> operator to equation ({{EquationNote|1}}) yields\n\n:<math> \\Delta^m f  = \\sum_{i=1}^M w_i C_{m,d} \\delta(\\mathbf{x} - \\mathbf{c}_i)</math>\n\nwhere <math>C_{2,2} = 8\\pi, </math> <math>C_{2,3}=-8\\pi,</math> and <math>C_{3,3}=-96\\pi.</math>  So ({{EquationNote|5}}) is equivalent to\n\n{{NumBlk|:|\n<math>\n\\begin{align}\n\\int\\limits_{\\mathcal{B}} & \\sum_{i=1}^N \\delta(\\mathbf{x} - \\mathbf{c}_i) (f(\\mathbf{x}) - f_i + (-1)^m\\lambda C_{m,d} w_i) g(\\mathbf{x}) \\,d\\mathbf{x} \\\\\n&= \\sum_{i=1}^N  (f(\\mathbf{c}_i) - f_i + (-1)^m\\lambda C_{m,d} w_i) g(\\mathbf{c}_i) \\\\\n&= 0.\n\\end{align}\n</math> |{{EquationRef|6}}}}\n\nThe only possible solution to ({{EquationNote|6}}) for all test functions <math>g</math> is\n\n{{NumBlk|:|<math> f(\\mathbf{c}_j) - f_j + (-1)^m\\lambda C_{m,d} w_j = 0 \\quad \\text{for} \\ j=1,2,\\dots,N </math> |{{EquationRef|7}}}}\n\n(which implies interpolation if <math>\\lambda=0</math>).  Combining the definition of <math> f </math> in equation ({{EquationNote|1}}) with equation ({{EquationNote|7}}) results in almost the same linear system as equation ({{EquationNote|2}}) except that the matrix <math> A </math> is replaced with <math> A + (-1)^m C_{m,d}\\lambda I </math> where <math> I </math> is the <math> N\\times N </math> identity matrix.  For example, for the 3D triharmonic RBFs, <math>A</math> is replaced with <math>A + 96\\pi\\lambda I.</math>\n\n== Explanation of additional constraints ==\nIn ({{EquationNote|2}}), the bottom half of the system of equations (<math>B^{\\textrm{T}}\\mathbf{w} = 0</math>) is given without explanation.  The explanation first requires deriving a simplified form of <math>\\textstyle{ \\int_{\\mathcal{B}} |\\nabla^m f|^2 \\,d\\mathbf{x}}</math> when <math>\\mathcal{B}</math> is all of <math>\\mathbb{R}^d.</math>\n\nFirst, require that <math>\\textstyle{ \\sum_{i=1}^N w_i =0. }</math>  This ensures that all derivatives of order <math>m</math> and higher of <math>\\textstyle{ f(\\mathbf{x}) = \\sum_{i=1}^N w_i \\phi(|\\mathbf{x} - \\mathbf{c}_i|) }</math> vanish at infinity.  For example, let <math>m=3</math> and <math>d=3</math> and <math>\\phi</math> be the triharmonic RBF.  Then <math>\\phi_{zzy} = 3y(x^2+y^2) / (x^2+y^2+z^2)^{3/2}</math> (considering <math>\\phi</math> as a mapping from <math>\\mathbb{R}^3</math> to <math>\\mathbb{R}</math>).  For a given center <math>\\mathbf{P} = (P_1,P_2,P_3),</math>\n\n:<math>\\phi_{zzy}(\\mathbf{x} - \\mathbf{P}) = \\frac{3(y-P_2)((y-P_2)^2 + (x-P_1)^2)}{((x-P_1)^2 + (y-P_2)^2 + (z-P_3)^2)^{3/2}}.</math>\n\nOn a line <math>\\mathbf{x} = \\mathbf{a} + t\\mathbf{b}</math> for arbitrary point <math>\\mathbf{a}</math> and unit vector <math>\\mathbf{b},</math>\n\n:<math>\\phi_{zzy}(\\mathbf{x} - \\mathbf{P}) = \\frac{3(a_2+b_2t - P_2)((a_2+b_2t-P_2)^2 + (a_1+b_1t-P_1)^2)}{((a_1+b_1t-P_1)^2 + (a_2+b_2t-P_2)^2 + (a_3+b_3t-P_3)^2)^{3/2}}.</math>\n\nDividing both numerator and denominator of this by <math>t^3</math> shows that <math>\\textstyle { \\lim_{t \\to \\infty} \\phi_{zyy}(\\mathbf{x}-\\mathbf{P}) = 3b_2(b_2^2 + b_1^2) / (b_1^2 + b_2^2 + b_3^2)^{3/2}},</math> a quantity independent of the center <math>\\mathbf{P}.</math>  So on the given line,\n\n:<math> \\lim_{t\\rightarrow\\infty} f_{zyy}(\\mathbf{x}) = \\lim_{t\\rightarrow\\infty}\\sum_{i=1}^N w_i \\phi_{zyy}(\\mathbf{x} - \\mathbf{c}_i) = \\left(\\sum_{i=1}^N w_i\\right)3b_2(b_2^2 + b_1^2) / (b_1^2 + b_2^2 + b_3^2)^{3/2} = 0.  </math>\n\nIt is not quite enough to require that  <math>\\textstyle{ \\sum_{i=1}^N w_i =0, }</math> because in what follows it is necessary for <math>f_{\\alpha}g_{\\beta}</math> to vanish at infinity, where <math>\\alpha</math> and <math>\\beta</math> are multi-indices such that <math>|\\alpha|+|\\beta|=2m-1.</math>  For triharmonic <math>\\phi,</math> <math>w_i u_j\\phi_{\\alpha}(\\mathbf{x}-\\mathbf{c}_i)\\phi_{\\beta}(\\mathbf{x} - \\mathbf{d}_j)</math> (where <math>u_j</math> and <math>\\mathbf{d}_j</math> are the weights and centers of <math>g</math>) is always a sum of total degree 5 polynomials in <math>x,</math> <math>y,</math> and <math>z</math> divided by the square root of a total degree 8 polynomial.  Consider the behavior of these terms on the line <math>\\mathbf{x} = \\mathbf{a} + t\\mathbf{b}</math> as <math>t</math> approaches infinity.  The numerator is a degree 5 polynomial in <math>t.</math>  Dividing numerator and denominator by <math>t^4</math> leaves the degree 4 and 5 terms in the numerator and a function of <math>\\mathbf{b}</math> only in the denominator.  A degree 5 term divided by <math>t^4</math> is a product of five <math>b</math> coordinates and <math>t.</math>  The <math>\\textstyle{ \\sum w = 0}</math> (and <math>\\textstyle{\\sum u=0}</math>) constraint makes this vanish everywhere on the line.  A degree 4 term divided by <math>t^4</math> is either a product of four <math>b</math> coordinates and an <math>a</math> coordinate or a product of four <math>b</math> coordinates and a single <math>c_i</math> or <math>d_j</math> coordinate.  The <math>\\textstyle{ \\sum w = 0}</math> constraint makes the first type of term vanish everywhere on the line.  The additional constraints <math>\\textstyle{ \\sum_{i=1}^N w_i \\mathbf{c}_i = 0 }</math> will make the second type of term vanish.\n\nNow define the inner product of two functions <math>f,g:\\mathbb{R}^d \\rightarrow \\mathbb{R}</math> defined as a linear combination of polyharmonic RBFs <math>\\phi_{m,d}</math> with <math>\\textstyle{\\sum w = 0}</math> and <math>\\textstyle{\\sum w\\mathbf{c}}=0</math> as\n\n:<math>\\langle f,g\\rangle = \\int\\limits_{\\mathbb{R}^d} (\\nabla^m f)\\cdot (\\nabla^m g) \\,d\\mathbf{x}.</math>\n\nIntegration by parts shows that\n\n{{NumBlk|:|\n<math> \\langle f,g\\rangle = (-1)^m \\int\\limits_{\\mathbb{R}^d} f (\\Delta^mg) \\,d\\mathbf{x}.  </math>\n|{{EquationRef|8}}}}\n\nFor example, let <math> m=2  </math> and <math> d=2.  </math>  Then\n\n{{NumBlk|:|\n<math>\\langle f,g\\rangle = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (f_{xx}g_{xx} + 2f_{xy}g_{xy} + f_{yy}g_{yy}) \\,dx \\,dy.</math>\n|{{EquationRef|9}}}}\n\nIntegrating the first term of this by parts once yields\n\n:<math>\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f_{xx}g_{xx} \\,dx \\,dy = \\int_{-\\infty}^{\\infty} f_x g_{xx}\\big|_{-\\infty}^{\\infty} \\,dy - \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f_x g_{xxx} \\,dx \\,dy = - \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f_x g_{xxx} \\,dx \\,dy</math>\n\nsince <math>f_x g_{xx}</math> vanishes at infinity.  Integrating by parts again results in <math>\\textstyle{\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f g_{xxxx} \\,dx \\,dy.}</math>\n\nSo integrating by parts twice for each term of ({{EquationNote|9}}) yields\n\n:<math> \\langle f,g\\rangle = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f (g_{xxxx} + 2g_{xxyy} + g_{yyyy}) \\,dx \\,dy = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f (\\Delta^2 g) \\,dx \\,dy.</math>\n\nSince <math>\\textstyle{ (\\Delta^m f)(\\mathbf{x}) = \\sum_{i=1}^N w_i C_{m,d}\\delta(\\mathbf{x - \\mathbf{c}_i}), }</math> ({{EquationNote|8}}) shows that\n\n:<math> \n\\begin{align}\n\\langle f,f\\rangle &= (-1)^m \\int\\limits_{\\mathbb{R}^d} f(\\mathbf{x}) \\sum_{i=1}^N w_i (-1)^m C_{m,d}\\delta(\\mathbf{x - \\mathbf{c}_i}) \\,d\\mathbf{x}\n   = (-1)^m C_{m,d} \\sum_{i=1}^N w_i f(\\mathbf{c}_i) \\\\\n      &= (-1)^m C_{m,d} \\sum_{i=1}^N \\sum_{j=1}^N w_i w_j \\phi(\\mathbf{c}_i - \\mathbf{c}_j) = (-1)^m C_{m,d} \\mathbf{w}^{\\textrm{T}} A \\mathbf{w}. \n\\end{align}\n</math>\n\nSo if <math> \\textstyle{ \\sum w = 0} </math> and <math> \\textstyle{\\sum w\\mathbf{c}=0 }, </math>\n\n{{NumBlk|:|\n<math>\n\\int\\limits_{\\mathbb{R}^d} |\\nabla^m f|^2 \\,d\\mathbf{x} = \n(-1)^m C_{m,d} \\mathbf{w}^{\\textrm{T}} A \\mathbf{w}.\n</math>\n|{{EquationRef|10}}}}\n\nNow the origin of the constraints <math>B^{\\textrm{T}}\\mathbf{w} = 0</math> can be explained.  Here <math>B</math> is a generalization of the <math>B</math> defined above to possibly include monomials up to degree <math>m-1.</math>  In other words,\n\n<math>B=\\begin{bmatrix} 1 & 1 & \\dots & 1\\\\ \\mathbf{c}_1 & \\mathbf{c}_2 & \\dots & \\mathbf{c}_N \\\\ \n\\vdots & \\vdots & \\dots & \\vdots \\\\\n\\mathbf{c}_1^{m-1} & \\mathbf{c}_2^{m-1} & \\dots & \\mathbf{c}_N^{m-1}\n\\end{bmatrix} ^ {\\textrm{T}}</math>\n\nwhere <math>\\mathbf{c}_i^j</math> is a column vector of all degree <math>j</math> monomials of the coordinates of <math>\\mathbf{c}_i.</math>  The top half of ({{EquationNote|2}}) is equivalent to <math>A\\mathbf{w} + B\\mathbf{v} - \\mathbf{f} = 0.</math>  So to obtain a smoothing spline, one should minimize the scalar field <math>F:\\mathbb{R}^{N+d+1}\\rightarrow \\mathbb{R}</math> defined by\n\n:<math>\nF(\\mathbf{w}, \\mathbf{v}) = |A\\mathbf{w} + B\\mathbf{v} - \\mathbf{f}|^2 + \\lambda C \\mathbf{w}^{\\textrm{T}} A \\mathbf{w}.\n</math>\n\nThe equations\n\n:<math>\n\\frac{\\partial F}{\\partial w_i} = 2 A_{i*} (A\\mathbf{w} + B\\mathbf{v} - \\mathbf{f}) + 2\\lambda C A_{i*}\\mathbf{w}=0 \\quad \n\\textrm{for} \\ i=1,2,\\dots,N\n</math>\n\nand\n\n:<math>\n\\frac{\\partial F}{\\partial v_i} = 2 B^{\\textrm{T}}_{i*} (A\\mathbf{w} + B\\mathbf{v} - \\mathbf{f})=0 \\quad\n\\textrm{for} \\ i=1,2,\\dots,d+1\n</math>\n\n(where <math>A_{i*}</math> denotes row <math>i</math> of <math>A</math>) are equivalent to the two systems of linear equations <math> A(A\\mathbf{w} + B\\mathbf{v} - \\mathbf{f} +\\lambda C \\mathbf{w}) = 0 </math> and <math> B^{\\textrm{T}}(A\\mathbf{w} + B\\mathbf{v} - \\mathbf{f}) = 0. </math>  Since <math>A</math> is invertible, the first system is equivalent to <math> A\\mathbf{w} + B\\mathbf{v} - \\mathbf{f} +\\lambda C \\mathbf{w} = 0. </math>  So the first system implies the second system is equivalent to <math>B^{\\textrm{T}}\\mathbf{w} = 0.</math>  Just as in the previous smoothing spline coefficient derivation, the top half of ({{EquationNote|2}}) becomes <math>(A+\\lambda C I)\\mathbf{w} + B\\mathbf{v} = \\mathbf{f}.</math>\n\nThis derivation of the polyharmonic smoothing spline equation system did not assume the constraints necessary to guarantee that <math>\\textstyle{ \\int_{\\mathcal{\\mathbb{R}}^d} |\\nabla^m f|^2 \\,d\\mathbf{x}} = C w^{\\textrm{T}}Aw.</math>  But the constraints necessary to guarantee this, <math>\\textstyle{ \\sum w = 0}</math> and <math>\\textstyle{ \\sum w \\mathbf{c} = 0 },</math> are a subset of <math>B^{\\textrm{T}}w=0 </math> which is true for the critical point <math>w</math> of <math>F.</math>  So <math>\\textstyle{ \\int_{\\mathcal{\\mathbb{R}}^d} |\\nabla^m f|^2 \\,d\\mathbf{x}} = C w^{\\textrm{T}}Aw</math> is true for the <math>f</math> formed from the solution of the polyharmonic smoothing spline equation system.  Because the integral is positive for all <math>w\\neq 0,</math> the linear transformation resulting from the restriction of the domain of linear transformation <math>A</math> to <math>w</math> such that <math>B^T w = 0</math> must be positive definite.  This fact enables transforming the polyharmonic smoothing spline equation system to a symmetric positive definite system of equations that can be solved twice as fast using the Cholesky decomposition.<ref name=\":0\" />\n\n== Examples ==\nThe next figure shows the interpolation through four points (marked by \"circles\") using different types of polyharmonic splines. The \"curvature\" of the interpolated curves grows with the order of the spline and the extrapolation at the left boundary (x < 0) is reasonable. The figure also includes the radial basis functions phi = exp(-r<sup>2</sup>) which gives a good interpolation as well. Finally, the figure includes also\nthe non-polyharmonic spline phi = r<sup>2</sup> to demonstrate, that this\nradial basis function is not able to pass through the predefined points\n(the linear equation has no solution and is solved in a least squares sense).\n\n[[Image:Polyharmonic-splines-example1.png|frame|none|Interpolation with different polyharmonic splines that shall pass the 4 predefined points marked by a circle (the interpolation with phi = r<sup>2</sup> is not useful, since the linear equation system of the interpolation problem has no solution; it is solved in a least squares\nsense, but then does not pass the centers)]]\n\nThe next figure shows the same interpolation as in the first figure, with the only exception that the points to be interpolated are scaled by a factor of 100 (and the case phi = r<sup>2</sup> is no longer included). Since phi = (scale*r)<sup>k</sup> =\n(scale<sup>k</sup>)*r<sup>k</sup>, the factor (scale<sup>k</sup>) can be extracted from matrix '''A''' of the linear equation system and therefore the solution is not influenced by the scaling. This is different for the logarithmic form of the spline, although the scaling has not much influence. This analysis is reflected in the figure, where the interpolation shows not much differences. Note, for other radial basis functions, such as phi = exp(-k*r<sup>2</sup>) with k=1, the interpolation is no longer reasonable and it would be necessary to adapt k.\n\n[[Image:Polyharmonic-splines-example1-scale100.png|frame|none| The same interpolation as in the first figure, but the points to be interpolated are scaled by 100]]\n\nThe next figure shows the same interpolation as in the first figure, with\nthe only exception that the polynomial term of the function is not\ntaken into account (and the case phi = r<sup>2</sup> is no longer included). As can be seen from the figure, the extrapolation for x < 0 is no longer as \"natural\" as in the first figure for some of the basis functions. This indicates, that the polynomial term is useful if extrapolation occurs.\n\n[[Image:Polyharmonic-splines-example1-no-polynomial.png|frame|none| The same interpolation as in the first figure, but without the polynomial term]]\n\n== Discussion ==\nThe main advantage of polyharmonic spline interpolation is that usually very good interpolation results are obtained for scattered data without performing any \"tuning\", so automatic interpolation is feasible. This is not the case for other radial basis functions. For example, the Gaussian function <math>e^{-k\\cdot r^2}</math> needs to be tuned, so that <math>k</math> is selected according to the underlying grid of the independent variables. If this grid is non-uniform, a proper selection of <math>k</math> to achieve a good interpolation result is difficult or impossible.\n\nMain disadvantages are:\n\n* To determine the weights, a dense linear system of equations must be solved.  Solving a dense linear system becomes impractical if the dimension <math>N</math> is large, since the memory required is <math>O(N^2)</math> and the number of operations required is <math>O(N^3).</math> \n* Evaluating the computed polyharmonic spline function at <math>M</math> data points requires <math>O(MN)</math> operations. In many applications (image processing is an example), <math>M</math> is much larger than <math>N,</math> and if both numbers are large, this is not practical.\n\nRecently, methods have been developed to overcome the aforementioned difficulties. For example Beatson et al.<ref>R.K. Beatson, M.J.D. Powell, and A.M. Tan: [http://imajna.oxfordjournals.org/cgi/content/short/27/3/427 Fast evaluation of polyharmonic splines in three dimensions]. IMA Journal of Numerical Analysis, 2007, 27, pp. 427-450.</ref> present a method to interpolate polyharmonic splines at one point in 3 dimensions in <math>O(\\log N)</math> operations instead of <math>O(N)</math> operations.\n\n== See also ==\n*[[Inverse distance weighting]], \n*[[Radial basis function]],\n*[[Subdivision surface]] (emerging alternative to spline-based surfaces),\n*[[Spline (mathematics)|Spline]].\n\n==References==\n{{Reflist}}\n\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Polynomial and rational function modeling",
      "url": "https://en.wikipedia.org/wiki/Polynomial_and_rational_function_modeling",
      "text": "In [[statistical modeling]] (especially [[process modeling]]), polynomial functions and rational functions are sometimes used as an empirical technique for [[curve fitting]].\n\n==Polynomial function models==\n{{Main|polynomial interpolation|polynomial regression}}\nA [[polynomial function]] is one that has the form\n\n:<math>\ny = a_{n}x^{n} + a_{n-1}x^{n-1} + \\cdots + a_{2}x^{2} + a_{1}x + a_{0} \n</math>\n\nwhere ''n'' is a non-negative [[integer]] that defines the degree of the polynomial. A polynomial with a degree of 0 is simply a [[constant function]]; with a degree of 1 is a [[linear function|line]]; with a degree of 2 is a [[quadratic function|quadratic]]; with a degree of 3 is a [[cubic function|cubic]], and so on.\n\nHistorically, polynomial models are among the most frequently used empirical models for [[curve fitting]].\n\n===Advantages===\nThese models are popular for the following reasons.\n#Polynomial models have a simple form.\n#Polynomial models have well known and understood properties.\n#Polynomial models have moderate flexibility of shapes.\n#Polynomial models are a closed family. [[translation (geometry)|Changes of location]] and [[scaling (geometry)|scale]] in the raw data result in a polynomial model being mapped to a polynomial model. That is, polynomial models are not dependent on the underlying [[metric (mathematics)|metric]].\n#Polynomial models are computationally easy to use.\n\n===Disadvantages===\nHowever, polynomial models also have the following limitations.\n#Polynomial models have poor [[interpolation|interpolatory]] properties. High-degree polynomials are notorious for [[Runge's phenomenon|oscillations between exact-fit values]].\n#Polynomial models have poor [[extrapolation|extrapolatory]] properties. Polynomials may provide good fits within the range of data, but they will frequently deteriorate rapidly outside the range of the data.\n#Polynomial models have poor [[asymptote|asymptotic]] properties. By their nature, polynomials have a finite response for finite ''x'' values and have an infinite response if and only if the ''x'' value is infinite. Thus polynomials may not model asymptotic phenomena very well.\n#While no procedure is immune to the [[bias of an estimator|bias]]-[[variance]] tradeoff, polynomial models exhibit a particularly poor tradeoff between shape and degree. In order to model data with a complicated structure, the degree of the model must be high, indicating that the associated number of [[statistical parameter|parameter]]s to be [[estimation theory|estimated]] will also be high. This can result in highly unstable models.\n\nWhen modeling via polynomial functions is inadequate due to any of the limitations above, the use of rational functions for modeling may give a better fit.\n\n==Rational function models==\nA [[rational function]] is simply the ratio of two polynomial functions.\n:<math>\ny = \\frac{a_{n}x^{n} + a_{n-1}x^{n-1} + \\ldots + a_{2}x^{2} + a_{1}x + a_{0}} {b_{m}x^{m} + b_{m-1}x^{m-1} + \\ldots + b_{2}x^{2} + b_{1}x + b_{0}} \n</math>\nwith ''n'' denoting a non-negative integer that defines the degree of the numerator and ''m'' denoting a non-negative integer that defines the degree of the denominator. For fitting rational function models, the constant term in the denominator is usually set to 1. Rational functions are typically identified by the degrees of the numerator and denominator. For example, a quadratic for the numerator and a cubic for the denominator is identified as a quadratic/cubic rational function. The rational function model is a generalization of the polynomial model: rational function models contain polynomial models as a subset (i.e., the case when the denominator is a constant).\n\n===Advantages===\nRational function models have the following advantages:\n#Rational function models have a moderately simple form.\n#Rational function models are a closed family. As with polynomial models, this means that rational function models are not dependent on the underlying metric.\n#Rational function models can take on an extremely wide range of shapes, accommodating a much wider range of shapes than does the polynomial family.\n#Rational function models have better interpolatory properties than polynomial models. Rational functions are typically smoother and less oscillatory than polynomial models.\n#Rational functions have excellent extrapolatory powers. Rational functions can typically be tailored to model the function not only within the domain of the data, but also so as to be in agreement with theoretical/asymptotic behavior outside the domain of interest.\n#Rational function models have excellent asymptotic properties. Rational functions can be either finite or infinite for finite values, or finite or infinite for infinite ''x'' values. Thus, rational functions can easily be incorporated into a rational function model.\n#Rational function models can often be used to model complicated structure with a fairly low degree in both the numerator and denominator. This in turn means that fewer coefficients will be required compared to the polynomial model.\n#Rational function models are moderately easy to handle computationally. Although they are [[nonlinear regression|nonlinear models]], rational function models are particularly easy nonlinear models to fit.\n#One common difficulty in fitting nonlinear models is finding adequate starting values. A major advantage of rational function models is the ability to compute starting values using a [[Ordinary least squares|linear least squares]] fit. To do this, ''p'' points are chosen from the data set, with ''p'' denoting the number of parameters in the rational model. For example, given the linear/quadratic model\n:::<math>y=\\frac{A_0 + A_1x} {1 + B_1x + B_2x^{2}} ,</math>\n::one would need to select four representative points, and perform a linear fit on the model\n:::<math>\ny = A_0 + A_1x  - B_1xy - B_2x^2y ,\n</math>\n::which is derived from the previous equation by clearing the denominator. Here, the ''x'' and ''y'' contain the subset of points, not the full data set. The estimated coefficients from this linear fit are used as the starting values for fitting the nonlinear model to the full data set.\n\n::This type of fit, with the response variable appearing on both sides of the function, should only be used to obtain starting values for the nonlinear fit. The statistical properties of fits like this are not well understood.\n\n::The subset of points should be selected over the range of the data. It is not critical which points are selected, although obvious outliers should be avoided.\n\n===Disadvantages===\nRational function models have the following disadvantages:\n#The properties of the rational function family are not as well known to engineers and scientists as are those of the polynomial family. The literature on the rational function family is also more limited. Because the properties of the family are often not well understood, it can be difficult to answer the following modeling question: ''Given that data has a certain shape, what values should be chosen for the degree of the numerator and the degree on the denominator?''\n#Unconstrained rational function fitting can, at times, result in undesired vertical [[asymptote]]s due to roots in the denominator polynomial. The range of ''x'' values affected by the function \"blowing up\" may be quite narrow, but such asymptotes, when they occur, are a nuisance for local interpolation in the neighborhood of the asymptote point. These asymptotes are easy to detect by a simple plot of the fitted function over the range of the data. These nuisance asymptotes occur occasionally and unpredictably, but practitioners argue that the gain in flexibility of shapes is well worth the chance that they may occur, and that such asymptotes should not discourage choosing rational function models for empirical modeling.\n\n==See also==\n* [[Response surface methodology]]\n\n==Bibliography==\n* {{cite book |author=[http://stats.lse.ac.uk/atkinson/ Atkinson, A. C.] and [http://www.maths.manchester.ac.uk/~adonev/ Donev, A. N.] and [http://support.sas.com/publishing/bbu/companion_site/index_author.html#tobias Tobias, R. D.]|title=Optimum Experimental Designs, with SAS| url=https://books.google.com/books?id=oIHsrw6NBmoC| publisher=Oxford University Press|year=2007 |pages=511+xvi |isbn=978-0-19-929660-6 |oclc= |doi=}}\n* Box, G. E. P. and Draper, Norman. 2007. ''Response Surfaces, Mixtures, and Ridge Analyses'', Second Edition [of ''Empirical Model-Building and Response Surfaces'', 1987], Wiley.\n* {{cite book |authorlink=Jack Kiefer (statistician)| last=Kiefer| first=Jack Carl| title=Collected Papers III Design of Experiments |editorlink=Lawrence D. Brown| editor=L. D. Brown|publisher=Springer-Verlag|year=1985|isbn=978-0-387-96004-3|display-editors=etal}}\n* R. H. Hardin and [[Neil Sloane|N. J. A. Sloane]], [http://neilsloane.com/doc/design.pdf \"A New Approach to the Construction of Optimal Designs\", ''Journal of Statistical Planning and Inference'', vol. 37, 1993, pp. 339-369]\n* R. H. Hardin and [[Neil Sloane|N. J. A. Sloane]], [http://neilsloane.com/doc/doeh.pdf \"Computer-Generated Minimal (and Larger) Response Surface Designs: (I) The Sphere\"]\n* R. H. Hardin and [[Neil Sloane|N. J. A. Sloane]], [http://neilsloane.com/doc/meatball.pdf \"Computer-Generated Minimal (and Larger) Response Surface Designs: (II) The Cube\"]\n*  {{Cite book| title=Design and Analysis of Experiments | series=Handbook of Statistics| volume=13|editor=Ghosh, S. |editor2=[[Calyampudi Radhakrishna Rao|Rao, C. R.]] | publisher=North-Holland| year=1996| isbn=978-0-444-82061-7}}\n**  {{Cite book|author1=Draper, Norman  |author2=Lin, Dennis K. J. |lastauthoramp=yes | chapter=Response Surface Designs |pages=343–375}}\n**  {{Cite book|author1=Gaffke, N.  |author2=Heiligers, B |lastauthoramp=yes | chapter=Approximate Designs for [[Linear regression|Polynomial Regression]]: [[Invariant estimator|Invariance]], [[Admissible decision rule|Admissibility]], and [[Optimal design|Optimality]] |pages=1149–1199}}\n* {{cite book |author=Melas, Viatcheslav B.|title=Functional Approach to Optimal Experimental Design |series=Lecture Notes in Statistics| volume=184 | publisher=Springer-Verlag | year=2006 |isbn=978-0-387-98741-5}} (Modeling with rational functions)\n\n===Historical===\n*{{cite journal\n|title=Application de la méthode des moindre quarrés a l'interpolation des suites<!--  [The application of the method of least squares to the interpolation of sequences] -->\n|author=[[Joseph Diaz Gergonne|Gergonne, J. D.]]\n|journal=[[Annales de mathématiques pures et appliquées]]\n|volume=6\n|year=1815 \n|pages=242–252\n}}\n*{{cite journal\n|title=The application of the method of least squares to the interpolation of sequences\n|author=[[Joseph Diaz Gergonne|Gergonne, J. D.]]\n|journal=Historia Mathematica\n|volume=1\n|issue=4 <!-- |month=November -->\n|year=1974 |origyear=1815 \n|pages=439–447\n|edition=Translated by Ralph St. John and [[Stephen M. Stigler|S. M. Stigler]] from the 1815 French\n|doi=10.1016/0315-0860(74)90034-2\n|url=http://www.sciencedirect.com/science/article/B6WG9-4D7JMHH-20/2/df451ec5fbb7c044d0f4d900af80ec86\n}}\n*{{cite journal\n|title=Gergonne's 1815 paper on the design and analysis of polynomial regression experiments\n|author=[[Stephen M. Stigler|Stigler, Stephen M.]]\n|journal=[[Historia Mathematica]]\n|volume=1\n|issue=4 <!-- |month=November -->\n|year=1974\n|pages=431–439\n|doi=10.1016/0315-0860(74)90033-0\n|url=http://www.sciencedirect.com/science/article/B6WG9-4D7JMHH-1Y/2/680c7ada0198761e9866197d53512ab4}}\n* {{cite journal\n|author=Smith, Kirstine\n|title=On the Standard Deviations of Adjusted and Interpolated Values of an Observed Polynomial Function and its Constants and the Guidance They Give Towards a Proper Choice of the Distribution of the Observations\n|year=1918\n|journal=[[Biometrika]]\n|volume=12 \n|issue=1/2\n|pages=1–85\n|url=http://biomet.oxfordjournals.org/cgi/content/citation/12/1-2/1\n|jstor=2331929\n|doi=10.1093/biomet/12.1-2.1}}\n\n==External links==\n*[http://www.itl.nist.gov/div898/handbook/pmd/section6/pmd642.htm Rational Function Models]\n\n{{Least squares and regression analysis}}\n{{Statistics}}\n{{NIST-PD}}\n\n[[Category:Regression models]]\n[[Category:Interpolation]]\n[[Category:Statistical ratios]]"
    },
    {
      "title": "Polynomial interpolation",
      "url": "https://en.wikipedia.org/wiki/Polynomial_interpolation",
      "text": "In [[numerical analysis]], '''polynomial interpolation''' is the [[interpolation]] of a given [[data set]] by the [[polynomial]] of lowest possible degree that passes through the points of the dataset.<ref>{{cite journal|last1=Tiemann|first1=Jerome J.|title=Polynomial Interpolation|journal=I/O News|date=May–June 1981|volume=1|issue=5|page=16|issn=0274-9998  |url=https://archive.org/stream/IoNewsVolume1Number5#page/n0/mode/2up|accessdate=3 November 2017}}</ref>\n\n== Applications ==\nPolynomials can be used to approximate complicated curves, for example, the shapes of letters in [[typography]],{{Citation needed|date=May 2014}} given a few points. A relevant application is the evaluation of the [[natural logarithm]] and [[trigonometric function]]s: pick a few known data points, create a [[lookup table]], and interpolate between those data points. This results in significantly faster computations.{{specify|Faster than what?|date=July 2016}} Polynomial interpolation also forms the basis for algorithms in [[numerical quadrature]] and [[numerical ordinary differential equations]] and [[Secure multi-party computation|Secure Multi Party Computation]],  [[Secret sharing|Secret Sharing]] schemes.\n\nPolynomial interpolation is also essential to perform sub-quadratic multiplication and squaring such as [[Karatsuba multiplication]] and [[Toom–Cook multiplication]], where an interpolation through points on a polynomial which defines the product yields the product itself. For example, given ''a'' = ''f''(''x'') = ''a''<sub>0</sub>''x''<sup>0</sup> + ''a''<sub>1</sub>''x''<sup>1</sup> + ... and ''b'' = ''g''(''x'') = ''b''<sub>0</sub>''x''<sup>0</sup> + ''b''<sub>1</sub>''x''<sup>1</sup> + ..., the product ''ab'' is equivalent to ''W''(''x'') = ''f''(''x'')''g''(''x''). Finding points along ''W''(''x'') by substituting ''x'' for small values in ''f''(''x'') and ''g''(''x'') yields points on the curve. Interpolation based on those points will yield the terms of ''W''(''x'') and subsequently the product ''ab''. In the case of Karatsuba multiplication this technique is substantially faster than quadratic multiplication, even for modest-sized inputs. This is especially true when implemented in parallel hardware.\n\n==Definition==\nGiven a set of {{math|''n'' + 1}} data points {{math|(''x<sub>i</sub>'', ''y<sub>i</sub>'')}} where no two {{math|''x<sub>i</sub>''}} are the same, one is looking for a polynomial {{mvar|p}} of degree at most {{mvar|n}} with the property\n\n:<math>p(x_i) = y_i, \\qquad  i=0,\\ldots,n.</math>\n\nThe [[Unisolvent functions|unisolvence]] theorem {{anchor|unisolvence theorem}} states that such a polynomial ''p'' exists and is unique, and can be proved by the [[Vandermonde matrix]], as described below.\n\nThe theorem states that for {{math|''n'' + 1}} interpolation nodes {{math|(''x<sub>i</sub>'')}}, polynomial interpolation defines a linear [[bijection]]\n\n:<math>L_n:\\mathbb{K}^{n+1} \\to \\Pi_n</math>\n\nwhere Π<sub>''n''</sub> is the [[vector space]] of polynomials (defined on any interval containing the nodes) of degree at most {{mvar|n}}.\n\n==Constructing the interpolation polynomial==\n{{main article|Lagrange polynomial}}\n[[File:Interpolation example polynomial.svg|thumb|right|The red dots denote the data points {{math|(''x<sub>k</sub>'', ''y<sub>k</sub>'')}}, while the blue curve shows the interpolation polynomial.]]\nSuppose that the interpolation polynomial is in the form\n:<math>p(x) = a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_2 x^2 + a_1 x + a_0. \\qquad (1) </math>\nThe statement that ''p'' interpolates the data points means that\n:<math>p(x_i) = y_i \\qquad\\mbox{for all } i \\in \\left\\{ 0, 1, \\dots, n\\right\\}.</math>\nIf we substitute equation (1) in here, we get a [[system of linear equations]] in the coefficients {{math|''a<sub>k</sub>''}}. The system in matrix-vector form reads the following [[Matrix multiplication|multiplication]]:\n\n:<math>\\begin{bmatrix}\nx_0^n  & x_0^{n-1} & x_0^{n-2} & \\ldots & x_0 & 1 \\\\\nx_1^n  & x_1^{n-1} & x_1^{n-2} & \\ldots & x_1 & 1 \\\\\n\\vdots & \\vdots    & \\vdots    &        & \\vdots & \\vdots \\\\\nx_n^n  & x_n^{n-1} & x_n^{n-2} & \\ldots & x_n & 1\n\\end{bmatrix}\n\\begin{bmatrix} a_n \\\\ a_{n-1} \\\\ \\vdots \\\\ a_0 \\end{bmatrix}  =\n\\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}.</math>\n\nWe have to solve this system for {{math|''a<sub>k</sub>''}} to construct the interpolant ''p''(''x''). The matrix on the left is commonly referred to as a [[Vandermonde matrix]].\n\nThe [[condition number]] of the Vandermonde matrix may be large,<ref>{{cite journal|last=Gautschi|first=Walter|title=Norm Estimates for Inverses of Vandermonde Matrices|journal=Numerische Mathematik|volume=23|issue=4|pages=337–347|year=1975|doi=10.1007/BF01438260}}</ref> causing large errors when computing the coefficients {{math|''a<sub>i</sub>''}} if the system of equations is solved using [[Gaussian elimination]].\n\nSeveral authors have therefore proposed algorithms which exploit the structure of the Vandermonde matrix to compute numerically stable solutions in O(''n''<sup>2</sup>) operations instead of the O(''n''<sup>3</sup>) required by Gaussian elimination.<ref>{{cite journal| last=Higham| first=N. J.|title=Fast Solution of Vandermonde-Like Systems Involving Orthogonal Polynomials|journal=IMA Journal of Numerical Analysis| volume=8| issue=4| pages=473–486|year=1988|doi=10.1093/imanum/8.4.473}}</ref><ref>{{cite journal|last=Björck|first=Å|author2=V. Pereyra| title=Solution of Vandermonde Systems of Equations|journal=Mathematics of Computation|volume=24| pages=893–903|year=1970| doi=10.2307/2004623| issue=112| publisher=American Mathematical Society|jstor=2004623}}</ref><ref>{{cite journal|author1=Calvetti, D  |author2=Reichel, L |lastauthoramp=yes |title=Fast Inversion of Vanderomnde-Like Matrices Involving Orthogonal Polynomials|journal=BIT|pages=473–484|year=1993|doi=10.1007/BF01990529|volume=33|issue=33}}</ref> These methods rely on constructing first a [[Newton polynomial|Newton interpolation]] of the polynomial and then converting it to the [[monomial form]] above.\n\nAlternatively, we may write down the polynomial immediately in terms of [[Lagrange polynomial]]s:\n\n:<math>\\begin{align}\np(x) &= \\frac{(x-x_1)(x-x_2)\\cdots(x-x_n)}{(x_0-x_1)(x_0-x_2)\\cdots(x_0-x_n)} y_0 + \\frac{(x-x_0)(x-x_2)\\cdots(x-x_n)}{(x_1-x_0)(x_1-x_2)\\cdots(x_1-x_n)}y_1 +\\ldots+\\frac{(x-x_0)(x-x_1)\\cdots(x-x_{n-1})}{(x_n-x_0)(x_n-x_1)\\cdots(x_n-x_{n-1})}y_n \\\\\n&=\\sum_{i=0}^{n}\\left ( \\prod_{\\stackrel{\\!0\\leq j\\leq n}{j\\neq i}}\\frac{x-x_j}{x_i-x_j}\\right ) y_i\n\\end{align}</math>\n\nFor matrix arguments, this formula is called [[Sylvester's formula]] and the matrix-valued Lagrange polynomials are the [[Frobenius covariant]]s.\n\n==Uniqueness of the interpolating polynomial==\n\n===Proof 1===\nSuppose we interpolate through {{math|''n'' + 1}} data points with an at-most {{mvar|n}} degree polynomial ''p''(''x'') (we need at least {{math|''n'' + 1}} datapoints or else the polynomial cannot be fully solved for). Suppose also another polynomial exists also of degree at most {{mvar|n}} that also interpolates the {{math|''n'' + 1}} points; call it ''q''(''x'').\n\nConsider <math>r(x) = p(x) - q(x)</math>. We know,\n# ''r''(''x'') is a polynomial\n# ''r''(''x'') has degree at most {{mvar|n}}, since ''p''(''x'') and ''q''(''x'') are no higher than this and we are just subtracting them.\n# At the {{math|''n'' + 1}} data points, <math>r(x_i) = p(x_i) - q(x_i) = y_i - y_i = 0</math>. Therefore, ''r''(''x'') has {{math|''n'' + 1}} roots.\n\nBut ''r''(''x'') is a polynomial of degree {{math|≤ ''n''}}. It has one root too many. Formally, if ''r''(''x'') is any non-zero polynomial, it must be writable as <math>r(x) = A(x-x_0)(x-x_1)\\cdots(x-x_n)</math>, for some constant ''A''. By distributivity, the {{math|''n'' + 1}} ''x'''s multiply together to give leading term <math>Ax^{n+1}</math>, i.e. one degree higher than the maximum we set. So the only way ''r''(''x'') can exist is if {{math|''A'' {{=}} 0}}, or equivalently, {{math|''r''(''x'') {{=}} 0}}.\n: <math>r(x) = 0 = p(x) - q(x) \\implies p(x) = q(x)</math>\n\nSo ''q''(''x'') (which could be any polynomial, so long as it interpolates the points) is identical with ''p''(''x''), and ''q''(''x'') is unique.\n\n===Proof 2===\nGiven the Vandermonde matrix used above to construct the interpolant, we can set up the system\n\n: <math>V a = y</math>\n\nTo prove that V is [[Invertible matrix|nonsingular]] we use the Vandermonde determinant formula:\n\n: <math>\\det(V) = \\prod_{i,j=0, i<j}^n (x_i - x_j) </math>\n\nsince the {{math|''n'' + 1}} points are distinct, the [[determinant]] can't be zero as <math>x_i - x_j</math> is never zero, therefore ''V'' is nonsingular and the system has a unique solution.\n\nEither way this means that no matter what method we use to do our interpolation: direct, [[Lagrange polynomial|Lagrange]] etc., (assuming we can do all our calculations perfectly) we will always get the same polynomial.\n\n==Non-Vandermonde solutions==\nWe are trying to construct our unique interpolation polynomial in the vector space Π<sub>''n''</sub> of polynomials of degree {{mvar|n}}. When using a [[monomial basis]] for Π<sub>''n''</sub> we have to solve the Vandermonde matrix to construct the coefficients {{math|''a<sub>k</sub>''}} for the interpolation polynomial. This can be a very costly operation (as counted in clock cycles of a computer trying to do the job). By choosing another basis for Π<sub>''n''</sub> we can simplify the calculation of the coefficients but then we have to do additional calculations when we want to express the interpolation polynomial in terms of a [[monomial basis]].\n\nOne method is to write the interpolation polynomial in the [[Newton form]] and use the method of [[divided differences]] to construct the coefficients, e.g. [[Neville's algorithm]]. The cost is [[Big O notation|O(''n''<sup>2</sup>)]] operations, while Gaussian elimination costs O(''n''<sup>3</sup>) operations. Furthermore, you only need to do O(''n'') extra work if an extra point is added to the data set, while for the other methods, you have to redo the whole computation.\n\nAnother method is to use the [[Lagrange form]] of the interpolation polynomial. The resulting formula immediately shows that the interpolation polynomial exists under the conditions stated in the above theorem. Lagrange formula is to be preferred to Vandermonde formula when we are not interested in computing the coefficients of the polynomial, but in computing the value of ''p''(''x'') in a given ''x'' not in the original data set. In this case, we can reduce complexity to O(''n''<sup>2</sup>).<ref>R.Bevilaqua, D. Bini, M.Capovani and O. Menchi (2003). ''Appunti di Calcolo Numerico''. Chapter 5, p. 89. Servizio Editoriale Universitario Pisa - Azienda Regionale Diritto allo Studio Universitario.</ref>\n\nThe [[Bernstein form]] was used in a constructive proof of the [[Weierstrass approximation theorem]] by [[Sergei Natanovich Bernstein|Bernstein]] and has gained great importance in computer graphics in the form of [[Bézier curve]]s.\n\n== Linear combination of the given values ==\nThe [[Lagrange form]] of the interpolating polynomial is a linear combination of the given values. In many scenarios, an efficient and convenient polynomial interpolation is '''a linear combination of the given values''', using previously known coefficients. Given a set of <math>k+1</math> data points <math>(x_0, y_0),\\ldots,(x_j, y_j),\\ldots,(x_k, y_k)</math> where each data point is a (position, value) pair and where no two positions <math>x_j</math> are the same, the interpolation polynomial in the Lagrange form is a [[linear combination]]\n\n:<math>y(x) := \\sum_{j=0}^{k} y_j c_j(x)</math>\n\nof the given values <math>y_j</math> with each coefficient <math>c\n_j(x)</math> given by evaluating the corresponding Lagrange basis polynomial using the given <math>k+1</math> positions <math>x_j</math>.\n\n:<math>c_j(x) = \\ell_j(x,x_0,x_1,\\ldots,x_k) := \\prod_{\\begin{smallmatrix}0\\le m\\le k\\\\ m\\neq j\\end{smallmatrix}} \\frac{x-x_m}{x_j-x_m} = \\frac{(x-x_0)}{(x_j-x_0)} \\cdots \\frac{(x-x_{j-1})}{(x_j-x_{j-1})} \\frac{(x-x_{j+1})}{(x_j-x_{j+1})} \\cdots \\frac{(x-x_k)}{(x_j-x_k)}.</math>\n\nEach coefficient <math>c\n_j(x)</math> in the linear combination depends on the given positions <math>x_j</math> and the desired position <math>x</math>, but not on the given values <math>y_j</math>. For each coefficient, inserting the values of the given positions <math>x_j</math> and simplifying yields an expression <math>c\n_j(x)</math>, which depends only on <math>x</math>. Thus the same coefficient expressions <math>c_j(x)</math> can be used in a polynomial interpolation of a given second set of <math>k+1</math> data points <math>(x_0, v_0),\\ldots,(x_j, v_j),\\ldots,(x_k, v_k)</math> at the same given positions <math>x_j</math>, where the second given values <math>v_j</math> differ from the first given values <math>y_j</math>. Using the same coefficient expressions <math>c_j(x)</math> as for the first set of data points, the interpolation polynomial of the second set of data points is the linear combination\n\n:<math>v(x) := \\sum_{j=0}^{k} v_j c_j(x).</math>\n\nFor each coefficient <math>c\n_j(x)</math> in the linear combination, the expression resulting from the Lagrange basis polynomial <math>\\ell_j(x,x_0,x_1,\\ldots,x_k)</math> only depends on the relative spaces between the given positions, not on the individual value of any position. Thus the same coefficient expressions <math>c_j(x)</math> can be used in a polynomial interpolation of a given third set of <math>k+1</math> data points</p>\n\n:<math>(t_0, w_0),\\ldots,(t_j, w_j),\\ldots,(t_k, w_k)</math>\n\nwhere each position <math>t_j</math> is related to the corresponding position <math>x_j</math> in the first set by <math>t_i = a*x_i + b</math> and the desired positions are related by <math>t = a*x + b</math>, for a constant scaling factor ''a'' and a constant shift ''b'' for all positions. Using the same coefficient expressions <math>c_j(t)</math> as for the first set of data points, the interpolation polynomial of the third set of data points is the linear combination\n\n:<math>w(t) := \\sum_{j=0}^{k} w_j c_j(t).</math>\n\nIn many applications of polynomial interpolation, the given set of <math>k+1</math> data points is at equally spaced positions. In this case, it can be convenient to define the ''x''-axis of the positions such that <math>x_i = i</math>. For example, a given set of 3 equally-spaced data points <math>(x_0,y_0),(x_1,y_1),(x_2,y_2)</math> is then <math>(0,y_0),(1,y_1),(2,y_2)</math>.\n\nThe interpolation polynomial in the Lagrange form is the [[linear combination]]\n\n:[[linear combination|<math>\\begin{align} y(x) := \\sum_{j=0}^{2} y_j c_j(x) = y_0 \\frac{(x-1)(x-2)}{(0-1)(0-2)} + y_1 \\frac{(x-0)(x-2)}{(1-0)(1-2)} + y_2 \\frac{(x-0)(x-1)}{(2-0)(2-1)} \\\\ = y_0 \\frac{(x-1)(x-2)}{2} + y_1 \\frac{(x-0)(x-2)}{-1} + y_2 \\frac{(x-0)(x-1)}{2}. \\end{align} </math>]]\n\nThis quadratic interpolation is valid for any position ''x'', near or far from the given positions. So, given 3 equally-spaced data points at <math>x=0,1,2</math> defining a quadratic polynomial, at an example desired position <math>x=1.5</math>,  the interpolated value after simplification is given by [[linear combination|<math>y(1.5)=y_{1.5}= (-y_0 + 6y_1 + 3y_2)/8</math>]]\n\nThis is a quadratic interpolation typically used in the Multigrid method. Again given 3 equally-spaced data points at <math>x=0,1,2</math> defining a quadratic polynomial, at the next equally spaced position <math>x=3</math>, the interpolated value after simplification is given by\n\n:<math>y(3)=y_{3}= y_0 - 3y_1 + 3y_2</math>\n\nIn the above polynomial interpolations using a linear combination of the given values, the coefficients were determined using the Lagrange method. In some scenarios, the coefficients can be more easily determined using other methods. Examples follow.\n\nAccording to the [[method of finite differences]], for any polynomial of degree <i>d</i> or less, any sequence of <math>d+2</math> values at equally spaced positions has a <math>(d+1)</math>th difference exactly equal to 0. The element ''s''<sub>''d+1''</sub> of the [[Binomial transform]] is such a <math>(d+1)</math>th difference. This area is surveyed here<ref>{{Cite journal|last=Boyadzhiev|first=Boyad|date=2012|title=Close Encounters with the Stirling Numbers of the Second Kind|url=https://www.maa.org/sites/default/files/pdf/upload_library/2/Boyadzhiev-2013.pdf|journal=Math. Mag.|volume=85|pages=252-266|via=}}</ref>. The [[binomial transform]], ''T'', of a sequence of values {''v''<sub href=\"Weierstrass approximation theorem\">''n''</sub>}, is the sequence {''s''<sub>''n''</sub>} defined by\n\n:<math>s_n = \\sum_{k=0}^n (-1)^k {n\\choose k} v_k.</math>\n\nIgnoring the sign term <math>(-1)^k</math>, the <math>n+1</math> coefficients of the element ''s''<sub>''n''</sub> are the respective <math>n+1</math> elements of the row ''n'' of Pascal's Triangle. The [[Pascal's triangle#The Triangle of Binomial Transform Coefficients is like Pascal's Triangle.|triangle of binomial transform coefficients]] is like Pascal's triangle. The entry in the ''n''th row and ''k''th column of the BTC triangle is <math>(-1)^k\\tbinom{n}{k}</math> for any non-negative integer ''n'' and any integer ''k'' between 0 and ''n''. This results in the following example rows ''n''&nbsp;= <span class=\"\" data-ve-attributes=\"{&quot;typeof&quot;:&quot;mw:Entity&quot;}\">0</span> through ''n''&nbsp;= 7, top to bottom, for the BTC triangle:\n                              1                                  // Row n = 0.\n                          1      -1                              // Row n = 1 or d = 0.\n                      1      -2       1                          // Row n = 2 or d = 1.\n                  1      -3       3      -1                      // Row n = 3 or d = 2.\n              1      -4       6      -4       1                  // Row n = 4 or d = 3.\n          1      -5      10     -10       5      -1              // Row n = 5 or d = 4.\n      1      -6      15     -20      15      -6       1          // Row n = 6 or d = 5.\n  1      -7      21     -35      35     -21       7      -1      // Row n = 7 or d = 6.\nFor convenience, each row ''n'' of the above example BTC triangle also has a label <math>d=n-1</math>. Thus for any polynomial of degree ''d'' or less, any sequence of <math>d+2</math> values at equally spaced positions has a [[linear combination]] result of 0, when using the <math>d+2</math> elements of row <i>d</i> as the corresponding linear coefficients.\n\nFor example, 4 equally spaced data points of a quadratic polynomial obey the [[linear equation]] given by row <math>d=2</math> of the BTC triangle. <math>0 = y_0 - 3y_1 + 3y_2 -y_3</math> This is the same linear equation as obtained above using the Lagrange method.\n\nThe BTC triangle can also be used to derive other polynomial interpolations. For example, the above quadratic  interpolation\n\n:<math>y(1.5)=y_{1.5}= (-y_0 + 6y_1 + 3y_2)/8</math>\n\ncan be derived in 3 simple steps as follows. The equally spaced points of a quadratic polynomial obey the rows of the BTC triangle with <math>d=2</math> or higher. First, the row <math>d=3</math> spans the given and desired data points <math>y_0, y_1, y_{1.5}, y_2</math> with the linear equation\n\n:<math>0 = 1y_0 - 4y_{0.5} + 6y_1 - 4y_{1.5} + 1y_2</math>\n\nSecond, the unwanted data point <math>y_{0.5}</math> is replaced by an expression in terms of wanted data points. The row <math>d=2</math> provides a linear equation with a term <math>1y_{0.5}</math>, which results in a term <math>4y_{0.5}</math> by multiplying the linear equation by 4. <math>0 = 1y_{0.5} - 3y_1 + 3y_{1.5} - 1y_2\n  = 4y_{0.5} -12y_1 +12y_{1.5} - 4y_2\n</math> Third, the above two linear equations are added to yield a linear equation equivalent to the above quadratic interpolation for <math>y_{1.5}</math>. <math>0 = (1+0)y_0 + (-4+4)y_{0.5} + (6-12)y_1 + (-4+12)y_{1.5} + (1-4)y_2\n= y_0 -6y_1 + 8y_{1.5} - 3y_2</math>\n\nSimilar to other uses of linear equations, the above derivation scales and adds vectors of coefficients. In polynomial interpolation as a linear combination of values, the elements of a vector correspond to a contiguous sequence of regularly spaced positions. The <i>p</i> non-zero elements of a vector are the ''p'' coefficients in a linear equation obeyed by any sequence of ''p'' data points from any degree ''d'' polynomial on any regularly spaced grid, where ''d'' is noted by the subscript of the vector. For any vector of coefficients, the subscript obeys <math>d\\leq p-2</math>. When adding vectors with various subscript values, the lowest subscript applies for the resulting vector. So, starting with the vector of row <math>d=3</math> and the vector of row <math>d=2</math> of the BTC triangle, the above quadratic interpolation for <math>y_{1.5}</math> is derived by the vector calculation</p>\n\n:<math>(1,-4,6,-4,1)_3 +4*(0,1,-3,3,-1)_2=(1,0,-6,+8,-3)_2</math>\n\nSimilarly, the cubic interpolation typical in the [[Multigrid method]],\n\n:<math>y_{1.5}= (-y_0 + 9y_1 + 9y_2 - y_3)/16,</math>\n\ncan be derived by a vector calculation starting with the vector of row <math>d=5</math> and the vector of row <math>d=3</math> of the BTC triangle.\n\n:<math>    (1,-6,15,-20,15,-6,1)_5\n+ 6*(0, 1,-4,  6,-4, 1,0)_3\n=   (1, 0,-9, 16,-9, 0,1)_3</math>\n\n==Interpolation error==\n{{clarify section|date=June 2011}}\n\nWhen interpolating a given function ''f'' by a polynomial of degree {{mvar|n}} at the nodes ''x''<sub>0</sub>,...,''x''<sub>''n''</sub> we get the error\n\n:<math>f(x) - p_n(x) = f[x_0,\\ldots,x_n,x] \\prod_{i=0}^n (x-x_i) </math>\n\nwhere\n:<math>f[x_0,\\ldots,x_n,x]</math>\n\nis the notation for [[divided differences]].\n\nIf ''f'' is {{math|''n'' + 1}} times continuously differentiable on a closed interval ''I'' and <math>p_n(x)</math> is a polynomial of degree at most {{mvar|n}} that interpolates ''f'' at {{math|''n'' + 1}} distinct points {''x''<sub>''i''</sub>} (''i''=0,1,...,n) in that interval, then for each x in the interval there exists {{mvar|ξ}} in that interval such that\n\n:<math> f(x) - p_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!} \\prod_{i=0}^n (x-x_i). </math>\n\nThe above error bound suggests choosing the interpolation points {{math|''x<sub>i</sub>''}} such that the product <math>\\left | \\prod (x - x_i) \\right |,</math> is as small as possible. The [[Chebyshev nodes]] achieve this.\n\n=== Proof ===\nSet the error term as\n\n:<math> R_n(x) = f(x) - p_n(x) </math>\n\nand set up an auxiliary function:\n\n:<math> Y(t) = R_n(t) - \\frac{R_n(x)}{W(x)} W(t) </math>\n\nwhere\n\n:<math> W(t) = \\prod_{i=0}^n (t-x_i) </math>\n\nSince {{math|''x<sub>i</sub>''}} are roots of <math>R_n(t)</math> and <math>W(t)</math>, we have {{math|''Y''(''x'') {{=}} ''Y''(''x<sub>i</sub>'') {{=}} 0}}, which means {{mvar|Y}} has at least {{math|''n'' + 2}} roots. From [[Rolle's theorem]], <math>Y^\\prime(t)</math> has at least {{math|''n'' + 1}} roots, then <math>Y^{(n+1)}(t)</math> has at least one root {{mvar|ξ}}, where {{mvar|ξ}} is in the interval {{mvar|I}}.\n\nSo we can get\n\n:<math> Y^{(n+1)}(t) = R_n^{(n+1)}(t) - \\frac{R_n(x)}{W(x)} \\ (n+1)!  </math>\n\nSince <math>p_n(x)</math> is a polynomial of degree at most {{mvar|n}}, then\n\n:<math> R_n^{(n+1)}(t) = f^{(n+1)}(t) </math>\n\nThus\n\n:<math> Y^{(n+1)}(t) = f^{(n+1)}(t) - \\frac{R_n(x)}{W(x)} \\ (n+1)!  </math>\n\nSince {{mvar|ξ}} is the root of <math>Y^{(n+1)}(t)</math>, so\n\n:<math> Y^{(n+1)}(\\xi) = f^{(n+1)}(\\xi) - \\frac{R_n(x)}{W(x)} \\ (n+1)! = 0 </math>\n\nTherefore,\n\n:<math> R_n(x) = f(x) - p_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!} \\prod_{i=0}^n (x-x_i) </math>.\n\nThus the remainder term in the Lagrange form of the [[Taylor's theorem|Taylor theorem]] is a special case of interpolation error when all interpolation nodes {{math|''x<sub>i</sub>''}} are identical.<ref>{{cite web|url=http://www.math.okstate.edu/~binegar/4513-F98/4513-l16.pdf| title=Errors in Polynomial Interpolation}}</ref> Note that the error will be zero when <math>x = x_i</math> for any ''i''. Thus, the maximum error will occur at some point in the interval between two successive nodes.\n\n===For equally spaced intervals===\nIn the case of equally spaced interpolation nodes where <math>x_i = a + ih</math>, for <math>i=0,1,\\ldots,n,</math> and where <math>h = (b-a)/n,</math> the product term in the interpolation error formula can be bound as<ref>{{cite web|url=https://www.nada.kth.se/kurser/kth/2D1250/interp.pdf| title=Notes on Polynomial Interpolation}}</ref>\n\n:<math>\\left|\\prod_{i=0}^n (x-x_i)\\right| = \\prod_{i=0}^n \\left|x-x_i\\right| \\leq \\frac{n!}{4} h^{n+1}</math>.\n\nThus the error bound can be given as\n\n:<math> \\left|R_n(x)\\right| \\leq \\frac{h^{n+1}}{4(n+1)} \\max_{\\xi\\in[a,b]} \\left|f^{(n+1)}(\\xi)\\right| </math>\n\nHowever, this assumes that <math>f^{(n+1)}(\\xi)</math> is dominated by <math>h^{n+1}</math>, i.e. <math>f^{(n+1)}(\\xi) h^{n+1} \\ll 1</math>. In several cases, this is not true and the error actually increases as {{math|''n'' → ∞}} (see [[Runge's phenomenon]]). That question is treated in the [[#Convergence properties|section ''Convergence properties'']].\n\n==Lebesgue constants==\n:''See the main article: [[Lebesgue constant (interpolation)|Lebesgue constant]].''\n\nWe fix the interpolation nodes ''x''<sub>0</sub>, ..., ''x''<sub>''n''</sub> and an interval [''a'', ''b''] containing all the interpolation nodes. The process of interpolation maps the function ''f'' to a polynomial ''p''. This defines a mapping ''X'' from the space ''C''([''a'', ''b'']) of all continuous functions on [''a'', ''b''] to itself. The map ''X'' is linear and it is a [[projection (linear algebra)|projection]] on the subspace Π<sub>''n''</sub> of polynomials of degree ''n'' or less.\n\nThe Lebesgue constant ''L'' is defined as the [[operator norm]] of ''X''. One has (a special case of [[Lebesgue's lemma]]):\n\n:<math> \\|f-X(f)\\| \\le (L+1) \\|f-p^*\\|. </math>\n\nIn other words, the interpolation polynomial is at most a factor (''L''&nbsp;+&nbsp;1) worse than the best possible approximation. This suggests that we look for a set of interpolation nodes that makes ''L'' small. In particular, we have for [[Chebyshev nodes]]:\n\n:<math> L \\le \\frac2\\pi \\log(n+1) + 1.</math>\n\nWe conclude again that Chebyshev nodes are a very good choice for polynomial interpolation, as the growth in ''n'' is exponential for equidistant nodes.  However, those nodes are not optimal.\n\n==Convergence properties==\nIt is natural to ask, for which classes of functions and for which interpolation nodes the sequence of interpolating polynomials converges to the interpolated function as {{math|''n'' → ∞}}? Convergence may be understood in different ways, e.g. pointwise, uniform or in some integral norm.\n\nThe situation is rather bad for equidistant nodes, in that uniform convergence is not even guaranteed for infinitely differentiable functions. One [[Runge's phenomenon|classical example, due to Carl Runge]], is the function ''f''(''x'') = 1 / (1 + ''x''<sup>2</sup>) on the interval {{math|[−5, 5]}}. The interpolation error {{math|{{!!}}&nbsp;''f'' − ''p<sub>n</sub>''{{!!}}<sub>∞</sub>}} grows without bound as {{math|''n'' → ∞}}. Another example is the function ''f''(''x'') = |''x''| on the interval {{math|[−1, 1]}}, for which the interpolating polynomials do not even converge pointwise except at the three points ''x'' = ±1, 0.<ref>{{Harvtxt|Watson|1980|p=21}} attributes the last example to {{Harvtxt|Bernstein|1912}}.</ref>\n\nOne might think that better convergence properties may be obtained by choosing different interpolation nodes. The following result seems to give a rather encouraging answer:\n\n:'''Theorem.''' For any function ''f''(''x'') continuous on an interval [''a'',''b''] there exists a table of nodes for which the sequence of interpolating polynomials <math>p_n(x)</math> converges to ''f''(''x'') uniformly on [''a'',''b''].\n\n'''Proof'''. It's clear that the sequence of polynomials of best approximation <math>p^*_n(x)</math> converges to ''f''(''x'') uniformly (due to [[Weierstrass approximation theorem]]). Now we have only to show that each <math>p^*_n(x)</math> may be obtained by means of interpolation on certain nodes. But this is true due to a special property of polynomials of best approximation known from the [[equioscillation theorem]]. Specifically, we know that such polynomials should intersect ''f''(''x'') at least {{math|''n'' + 1}} times. Choosing the points of intersection as interpolation nodes we obtain the interpolating polynomial coinciding with the best approximation polynomial.\n\nThe defect of this method, however, is that interpolation nodes should be calculated anew for each new function ''f''(''x''), but the algorithm is hard to be implemented numerically. Does there exist a single table of nodes for which the sequence of interpolating polynomials converge to any continuous function ''f''(''x'')? The answer is unfortunately negative:\n\n:'''Theorem.''' For any table of nodes there is a continuous function ''f''(''x'') on an interval [''a'', ''b''] for which the sequence of interpolating polynomials diverges on [''a'',''b''].<ref>{{Harvtxt|Watson|1980|p=21}} attributes this theorem to {{Harvtxt|Faber|1914}}.</ref>\n\nThe proof essentially uses the lower bound estimation of the Lebesgue constant, which we defined above to be the operator norm of ''X''<sub>''n''</sub> (where ''X''<sub>''n''</sub> is the projection operator on Π<sub>''n''</sub>). Now we seek a table of nodes for which\n\n:<math>\\lim_{n \\to \\infty} X_n f = f,\\text{ for every }f \\in C([a,b]).</math>\n\nDue to the [[Banach–Steinhaus theorem]], this is only possible when norms of ''X''<sub>''n''</sub> are uniformly bounded, which cannot be true since we know that\n\n:<math>\\|X_n\\|\\geq \\tfrac{2}{\\pi} \\log(n+1)+C.</math>\n\nFor example, if equidistant points are chosen as interpolation nodes, the function from [[Runge's phenomenon]] demonstrates divergence of such interpolation. Note that this function is not only continuous but even infinitely differentiable on {{math|[−1, 1]}}. For better [[Chebyshev nodes]], however, such an example is much harder to find due to the following result:\n\n:'''Theorem.''' For every [[absolute continuity|absolutely continuous]] function on {{math|[−1, 1]}} the sequence of interpolating polynomials constructed on Chebyshev nodes converges to&nbsp;''f''(''x'') uniformly.<ref>{{cite journal|last=Krylov |first=V. I. |title=Сходимость алгебраического интерполирования покорням многочленов Чебышева для абсолютно непрерывных функций и функций с ограниченным изменением |trans-title=Convergence of algebraic interpolation with respect to the roots of Chebyshev's polynomial for absolutely continuous functions and functions of bounded variation |journal=Doklady Akademii Nauk SSSR (N.S.) |volume=107 |pages=362–365 |date=1956 |language=ru |url=https://books.google.com/books?id=tCMwAAAAMAAJ}} MR 18-32.</ref>\n\n==Related concepts==\n[[Runge's phenomenon]] shows that for high values of {{mvar|n}}, the interpolation polynomial may oscillate wildly between the data points. This problem is commonly resolved by the use of [[spline interpolation]]. Here, the interpolant is not a polynomial but a [[spline (mathematics)|spline]]: a chain of several polynomials of a lower degree.\n\nInterpolation of [[periodic function]]s by [[harmonic analysis|harmonic]] functions is accomplished by [[Fourier transform]]. This can be seen as a form of polynomial interpolation with harmonic base functions, see [[trigonometric interpolation]] and [[trigonometric polynomial]].\n\n[[Hermite interpolation]] problems are those where not only the values of the polynomial ''p'' at the nodes are given, but also all derivatives up to a given order. This turns out to be equivalent to a system of simultaneous polynomial congruences, and may be solved by means of the [[Chinese remainder theorem]] for polynomials. [[Birkhoff interpolation]] is a further generalization where only derivatives of some orders are prescribed, not necessarily all orders from 0 to a ''k''.\n\n[[Collocation method]]s for the solution of differential and integral equations are based on polynomial interpolation.\n\nThe technique of [[rational function modeling]] is a generalization that considers ratios of polynomial functions.\n\nAt last, [[multivariate interpolation]] for higher dimensions.\n\n==See also==\n* [[Newton series]]\n* [[Polynomial regression]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n* {{Citation |first=Kendell A. |last=Atkinson |year=1988 |title=An Introduction to Numerical Analysis |edition=2nd |chapter=Chapter 3. |publisher= John Wiley and Sons |isbn=0-471-50023-2 |doi= }}\n* {{Citation |first=Sergei N. |last=Bernstein |authorlink=Sergei Natanovich Bernstein |year=1912 |title=Sur l'ordre de la meilleure approximation des fonctions continues par les polynômes de degré donné |language=French |trans-title=On the order of the best approximation of continuous functions by polynomials of a given degree |journal=Mem. Acad. Roy. Belg. |issn= |volume=4 |issue= |pages=1&ndash;104 |doi=}}\n* {{Citation |first=L. |last=Brutman |year=1997 |title=Lebesgue functions for polynomial interpolation — a survey |journal=Ann. Numer. Math. |issn= |volume=4 |issue= |pages=111&ndash;127 |doi= }}\n* {{Citation|first=Georg |last=Faber |authorlink=Georg Faber |year=1914 |title=Über die interpolatorische Darstellung stetiger Funktionen | language=German |trans-title=On the Interpolation of Continuous Functions |journal=Deutsche Math. Jahr. |volume=23 |issue= |pages=192&ndash;210 | doi=}}\n* {{Citation |first=M. J. D. |last=Powell |authorlink=Michael J. D. Powell |year=1981 |title=Approximation Theory and Methods |chapter=Chapter 4 |publisher=Cambridge University Press |isbn=0-521-29514-9 |doi= }}\n* {{Citation |first=Michelle |last=Schatzman |authorlink=Michelle Schatzman |year=2002 |title=Numerical Analysis: A Mathematical Introduction |chapter=Chapter 4 |publisher=Clarendon Press |location=Oxford |isbn=0-19-850279-6 |doi=}}\n* {{Citation |first=Endre |last=Süli |authorlink=Endre Süli |first2=David |last2=Mayers |year=2003 |title=An Introduction to Numerical Analysis |chapter=Chapter 6 |publisher=Cambridge University Press |isbn=0-521-00794-1 |doi=}}\n* {{Citation |first=G. Alistair |last=Watson |year=1980 |title=Approximation Theory and Numerical Methods |publisher=John Wiley |isbn=0-471-27706-1 |doi=}}\n\n== External links ==\n* {{springer|title=Interpolation process|id=p/i051970}}\n* [http://www.alglib.net/interpolation/polynomial.php ALGLIB] has an implementations in C++ / C#.\n* [https://www.gnu.org/software/gsl/ GSL] has a polynomial interpolation code in C\n* [http://demonstrations.wolfram.com/InterpolatingPolynomial/ Interpolating Polynomial] by [[Stephen Wolfram]], the [[Wolfram Demonstrations Project]].\n\n[[Category:Interpolation]]\n[[Category:Polynomials]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Regionalized variable theory",
      "url": "https://en.wikipedia.org/wiki/Regionalized_variable_theory",
      "text": "{{Unreferenced|auto=yes|date=December 2009}}\n\n'''Regionalized variable theory (RVT)''' is a [[geostatistics|geostatistical]] method used for interpolation in space.\n\nThe concept of the theory is that interpolation from points in space should not be based on a smooth continuous object. It should be, however, based on a [[stochastic]] model that takes into consideration the various trends in the original set of points. The theory considers that within any dataset, three types of relationships can be detected:\n#Structural part, which is also called the trend.\n#Correlated variation.\n#Uncorrelated variation, or noise.\n\nAfter defining the above three relationships, RVT then applies the [[first law of geography]], in order to predict the unknown values of points. The major application of this theory is the [[Kriging]] method for interpolation.\n\n[[Category:Interpolation]]\n\n{{DEFAULTSORT:Regionalized Variable Theory}}\n\n\n{{Science-stub}}"
    },
    {
      "title": "Regression-kriging",
      "url": "https://en.wikipedia.org/wiki/Regression-kriging",
      "text": "In [[applied statistics]], '''regression-kriging (RK)''' is a spatial prediction technique that combines a [[regression analysis|regression]] of the dependent variable on auxiliary variables (such as parameters derived from digital elevation modelling, remote sensing/imagery, and thematic maps) with [[kriging]] of the regression residuals. It is mathematically equivalent to the interpolation method variously called ''[[universal kriging]]'' and ''kriging with external drift'', where auxiliary predictors are used directly to solve the kriging weights.<ref name=\"Pebesma2006IJGIS\">{{cite journal|last=Pebesma|first=Edzer J|title=The Role of External Variables and GIS Databases in Geostatistical Analysis|journal=Transactions in GIS|date=1 July 2006|volume=10|issue=4|pages=615–632|doi=10.1111/j.1467-9671.2006.01015.x}}</ref>\n\n== BLUP for spatial data ==\n\n[[File:The universal model of spatial variation.jpg|thumb|400px|The universal model of spatial variation scheme.]]\n\nRegression-kriging is an implementation of the [[best linear unbiased prediction|best linear unbiased predictor (BLUP)]] for spatial data, i.e. the best linear interpolator assuming the [[universal model of spatial variation]]. Matheron (1969) proposed that a value of a target variable at some location can be modeled as a sum of the deterministic and stochastic components:<ref>{{cite book|last=Matheron|first=Georges|title=Le krigeage universel|year=1969|publisher=École nationale supérieure des mines de Paris|chapter=Part 1 of Cahiers du Centre de morphologie mathématique de Fontainebleau}}</ref>\n\n:<math>\nZ(\\mathbf{s}) = m(\\mathbf{s}) + \\varepsilon '(\\mathbf{s}) + \\varepsilon ''\n</math>\n\nwhich he termed ''universal model of spatial variation''. Both [[deterministic system|deterministic]] and [[stochastic process|stochastic components]] of spatial variation can be modeled separately. By combining the two approaches, we obtain:\n\n:<math>\n\\hat z(\\mathbf{s}_0 ) = \\hat m(\\mathbf{s}_0 ) + \\hat e(\\mathbf{s}_0 )= \\sum\\limits_{k = 0}^p {\\hat \\beta _k \\cdot q_k (\\mathbf{s}_0 )} + \\sum\\limits_{i = 1}^n \\lambda_i \\cdot e(\\mathbf{s}_i )\n</math>\n\nwhere <math>\\hat m(\\mathbf{s}_0)</math> is the fitted deterministic part, <math>\\hat e(\\mathbf{s}_0)</math> is the interpolated residual, <math>\\hat \\beta _k</math> are estimated deterministic model coefficients (<math>\\hat \\beta _0</math> is the estimated intercept), <math>\\lambda_i</math> are kriging weights determined by the spatial dependence structure of the residual and where <math>e(\\mathbf{s}_i)</math> is the residual at location <math>{\\mathbf{s}}_i</math>. The regression coefficients <math>\\hat \\beta _k</math> can be estimated from the sample by some fitting method, e.g. [[ordinary least squares]] (OLS) or, optimally, using [[generalized least squares]] (GLS):<ref name=Cressie2012Wiley>{{cite book|last=Cressie|first=Noel|title=Statistics for spatio-temporal data|year=2012|publisher=Wiley|location=Hoboken, N.J.|isbn=9780471692744}}</ref>\n\n:<math>\n\\mathbf{\\hat \\beta }_\\mathtt{GLS} = \\left( \\mathbf{q}^\\mathbf{T} \\cdot\n\\mathbf{C}^{ - \\mathbf{1}} \\cdot \\mathbf{q} \\right)^{ - \\mathbf{1}} \\cdot\n\\mathbf{q}^\\mathbf{T} \\cdot \\mathbf{C}^{ - \\mathbf{1}} \\cdot \\mathbf{z}\n</math>\n\nwhere <math>\\mathbf{\\hat \\beta}_\\mathtt{GLS}</math> is the vector of estimated regression coefficients, <math>\\mathbf{C}</math> is the covariance matrix of the residuals, <math>{\\mathbf{q}}</math> is a matrix of predictors at the sampling locations and <math>\\mathbf{z}</math> is the vector of measured values of the target variable. The GLS estimation of regression coefficients is, in fact, a special case of the geographically weighted regression. In the case, the weights are determined objectively to account for the spatial auto-correlation between the residuals.\n\nOnce the deterministic part of variation has been estimated (regression-part), the residual can be interpolated with kriging and added to the estimated trend. The estimation of the residuals is an iterative process: first the deterministic part of variation is estimated using OLS, then the covariance function of the residuals is used to obtain the GLS coefficients. Next, these are used to re-compute the residuals, from which an updated covariance function is computed, and so on. Although this is by many geostatisticians recommended as the proper procedure, Kitanidis (1994) showed that use of the covariance function derived from the OLS residuals (i.e. a single iteration) is often satisfactory, because it is not different enough from the function derived after several iterations; i.e. it does not affect much the final predictions. Minasny and McBratney (2007) report similar results—it seems that using more higher quality data is more important then to use more sophisticated statistical methods.<ref name=\"MinasnyMcBratney2007Geoderma\">{{cite journal|last=Minasny|first=Budiman|author2=McBratney, Alex B. |title=Spatial prediction of soil properties using EBLUP with the Matérn covariance function|journal=Geoderma|date=31 July 2007|volume=140|issue=4|pages=324–336|doi=10.1016/j.geoderma.2007.04.028}}</ref>\n\nIn matrix notation, regression-kriging is commonly written as:<ref name=\"Christensen2001Springer\">{{cite book|last=Christensen|first=Ronald|title=Advanced linear modeling : multivariate, time series, and spatial data; nonparametric regression and response surface maximization|year=2001|publisher=Springer|location=New York, NY [u.a.]|isbn=9780387952963|edition=2.}}</ref>\n\n:<math>\n\\hat z_\\mathtt{RK}(\\mathbf{s}_0 ) = \\mathbf{q}_\\mathbf{0}^\\mathbf{T} \\cdot \\mathbf{\\hat \\beta}_\\mathtt{GLS} + \\mathbf{\\lambda }_\\mathbf{0}^\\mathbf{T} \\cdot (\\mathbf{z}\n- \\mathbf{q} \\cdot \\mathbf{\\hat \\beta }_\\mathtt{GLS} )\n</math>\n\nwhere <math>\\hat z({\\mathbf{s}}_0 )</math> is the predicted value at location <math>{\\mathbf{s}}_0</math>, <math>{\\mathbf{q}}_{\\mathbf{0}}</math> is the vector of <math>p+1</math> predictors and <math>\\mathbf{\\lambda}_{\\mathbf{0}}</math> is the vector of <math>n</math> kriging weights used to interpolate the residuals. The RK model is considered to be the ''Best Linear Predictor of spatial data''.<ref name=\"Christensen2001Springer\" /><ref name=Goldberger1962>{{cite journal|last=Goldberger|first=A.S.|title=Best Linear Unbiased Prediction in the Generalized Linear Regression Model|journal=Journal of the American Statistical Association|year=1962|volume=57|issue=298|pages=369–375|jstor=2281645|doi=10.1080/01621459.1962.10480665}}</ref> It has a prediction variance that reflects the position of new locations (extrapolation) in both geographical and feature space:\n\n:<math>\n\\hat \\sigma_\\mathtt{RK}^2 (\\mathbf{s}_0)\n= (C_0  + C_1 ) - \\mathbf{c}_\\mathbf{0}^\\mathbf{T} \\cdot \\mathbf{C}^\\mathbf{1}\n\\cdot \\mathbf{c}_\\mathbf{0} + \\left( \\mathbf{q}_\\mathbf{0}\n- \\mathbf{q}^\\mathbf{T} \\cdot \\mathbf{C}^{ - \\mathbf{1}} \\cdot\n\\mathbf{c}_\\mathbf{0} \\right)^\\mathbf{T} \\cdot \\left( \\mathbf{q}^\\mathbf{T}\n\\cdot \\mathbf{C}^{ - \\mathbf{1}} \\cdot \\mathbf{q} \\right)^\\mathbf{ - 1} \\cdot \\left(\\mathbf{q}_\\mathbf{0} - \\mathbf{q}^\\mathbf{T} \\cdot\n\\mathbf{C}^{ - \\mathbf{1}} \\cdot \\mathbf{c}_\\mathbf{0} \\right)\n</math>\n\nwhere <math>C_0 + C_1</math> is the sill variation and <math>{\\mathbf{c}}_0</math> is the vector of covariances of residuals at the unvisited location.\n\n[[File:Decision tree for selecting a suitable spatial prediction model.jpg|thumb|400px|Decision tree for selecting a suitable spatial prediction model.]]\n\nMany (geo)statisticians believe that there is only one Best Linear Unbiased Prediction model for spatial data (e.g. regression-kriging), all other techniques such as ordinary kriging, environmental correlation, averaging of values per polygons or inverse distance interpolation can be seen as its special cases. If the residuals show no spatial auto-correlation (pure nugget effect), the regression-kriging converges to pure multiple linear regression, because the covariance matrix (<math>\\mathbf{C}</math>) becomes an identity matrix. Likewise, if the target variable shows no correlation with the auxiliary predictors, the regression-kriging model reduces to ordinary kriging model because the deterministic part equals the (global) mean value. Hence, pure kriging and pure regression should be considered as only special cases of regression-kriging (see figure).\n\n== RK and UK/KED ==\n\nThe geostatistical literature uses many different terms for what are essentially the same or at least very similar techniques. This confuses the users and distracts them from using the right technique for their mapping projects. In fact, both universal kriging, kriging with external drift, and regression-kriging are basically the same technique.\n\nMatheron (1969) originally termed the technique ''Le krigeage universel'', however, the technique was intended as a generalized case of kriging where the trend is modelled as a function of coordinates. Thus, many authors reserve the term ''universal kriging'' (UK) for the case when only the coordinates are used as predictors. If the deterministic part of variation (''drift'') is defined externally as a linear function of some auxiliary variables, rather than the coordinates, the term ''kriging with external drift'' (KED) is preferred (according to Hengl 2007, \"About regression-kriging: From equations to case studies\"). In the case of UK or KED, the predictions are made as with kriging, with the difference that the covariance matrix of residuals is extended with the auxiliary predictors. However, the drift and residuals can also be estimated separately and then summed. This procedure was suggested by Ahmed et al. (1987) and Odeh et al. (1995) later named it ''regression-kriging'', while Goovaerts (1997) uses the term ''kriging with a trend model'' to refer to a family of interpolators, and refers to RK as ''simple kriging with varying local means''. Minasny and McBratney (2007) simply call this technique Empirical Best Linear Unbiased Predictor i.e. ''E-BLUP''.<ref>{{cite journal|last=Ahmed|first=Shakeel|author2=De Marsily, Ghislain|title=Comparison of geostatistical methods for estimating transmissivity using data on transmissivity and specific capacity|journal=Water Resources Research|date=1 January 1987|volume=23|issue=9|pages=1717|doi=10.1029/WR023i009p01717}}</ref><ref name=Odeh1995>{{cite journal|last=Odeh|first=I.O.A.|author2=McBratney, A.B. |author3=Chittleborough, D.J. |title=Further results on prediction of soil properties from terrain attributes: heterotopic cokriging and regression-kriging|journal=Geoderma|date=31 July 1995|volume=67|issue=3-4|pages=215–226|doi=10.1016/0016-7061(95)00007-B}}</ref><ref name=Hengl2004Geoderma>{{cite journal|last=Hengl|first=Tomislav|author2=Heuvelink, Gerard B.M. |author3=Stein, Alfred |title=A generic framework for spatial prediction of soil variables based on regression-kriging|journal=Geoderma|date=30 April 2004|volume=120|issue=1-2|pages=75–93|doi=10.1016/j.geoderma.2003.08.018}}</ref><ref name=\"MinasnyMcBratney2007Geoderma\" />\n\nIn the case of KED, predictions at new locations are made by:\n\n:<math>\n\\hat{z}_\\mathtt{KED} (\\mathbf{s}_0 ) = \\sum\\limits_{i = 1}^n\nw_i^\\mathtt{KED} (\\mathbf{s}_0 ) \\cdot z(\\mathbf{s}_i )\n</math>\n\nfor\n\n:<math>\n\\sum\\limits_{i = 1}^n w_i^\\mathtt{KED} (\\mathbf{s}_0 ) \\cdot q_k (\\mathbf{s}_i ) = q_k (\\mathbf{s}_0 )\n</math>\n\nfor <math>k = 1,\\ldots,p</math> or in matrix notation:\n\n:<math>\n\\hat z_\\mathtt{KED} (\\mathbf{s}_0 ) = \\mathbf{\\delta}_\\mathbf{0}^\\mathbf{T} \\cdot \\mathbf{z}\n</math>\n\nwhere <math>z</math> is the target variable, <math>q_k</math>'s are the predictor variables i.e. values at a new location <math>({\\mathbf{s}}_0)</math>, <math>{\\mathbf{\\delta }}_{\\mathbf{0}}</math> is the vector of KED weights (<math>w_i^{\\mathtt{KED}}</math>), <math>p</math> is the number of predictors and <math>\\mathbf{z}</math> is the vector of <math>n</math> observations at primary locations. The KED weights are solved using the extended matrices:\n\n:<math>\n\\mathbf{\\lambda }_\\mathbf{0}^\\mathtt{KED} = \\left\\{ w_1^\\mathtt{KED} (\\mathbf{s}_0 ), \\ldots ,w_n^\\mathtt{KED} (\\mathbf{s}_0 ),\\varphi_0 (\\mathbf{s}_0 ), \\ldots ,\\varphi _p (\\mathbf{s}_0 ) \\right\\}^\\mathbf{T} = \\mathbf{C}^{\\mathtt{KED} -1} \\cdot \\mathbf{c}_\\mathbf{0}^\\mathtt{KED}\n</math>\n\nwhere <math>{\\mathbf{\\lambda }}_{\\mathbf{0}}^{\\mathtt{KED}}</math> is the vector of solved weights, <math>\\varphi _p</math> are the Lagrange multipliers, <math>{\\mathbf{C}}^{\\mathtt{KED}}</math> is the extended covariance matrix of residuals and <math>{\\mathbf{c}}_{\\mathbf{0}}^{\\mathtt{KED}}</math> is the extended vector of covariances at new location.\n\nIn the case of KED, the extended covariance matrix of residuals looks like this (Webster and Oliver, 2007; p.&nbsp;183):<ref name=WebsterOliver2007>{{cite book|last=Webster|first=Richard|title=Geostatistics for environmental scientists|year=2007|publisher=Wiley|location=Chichester|isbn=9780470028582|edition=2nd|author2=Oliver, Margaret A. }}</ref>\n\n:<math>\n\\mathbf{C}^\\mathtt{KED} = \\left[\n\\begin{array}{ccccccc}\nC(\\mathbf{s}_1 , \\mathbf{s}_1) & \\cdots & C(\\mathbf{s}_1, \\mathbf{s}_n ) & 1 & q_1 (\\mathbf{s}_1 ) & \\cdots & q_p (\\mathbf{s}_1 ) \\\\\n\\vdots  &  &  \\vdots & \\vdots & \\vdots &  & \\vdots \\\\\nC(\\mathbf{s}_n, \\mathbf{s}_1 ) & \\cdots  & C(\\mathbf{s}_n ,\\mathbf{s}_n ) & 1 & q_1 (\\mathbf{s}_n ) &  \\cdots  & q_p (\\mathbf{s}_n ) \\\\\n1 &  \\cdots  & 1 & 0 & 0 & \\cdots & 0 \\\\\nq_1 (\\mathbf{s}_1 ) & \\cdots  & q_1 (\\mathbf{s}_n ) & 0 & 0 & \\cdots & 0 \\\\\n\\vdots  &  &  \\vdots & \\vdots & \\vdots &  & \\vdots \\\\\nq_p (\\mathbf{s}_1 ) & \\cdots  & q_p (\\mathbf{s}_n ) & 0 & 0 & \\cdots  & 0 \n\\end{array}\n\\right]\n</math>\n\nand <math>\\mathbf{c}_{\\mathbf{0}}^{\\mathtt{KED}}</math> like this:\n\n:<math>\n\\mathbf{c}_\\mathbf{0}^\\mathtt{KED} = \\left\\{ C(\\mathbf{s}_0, \\mathbf{s}_1\n), \\ldots , C(\\mathbf{s}_0, \\mathbf{s}_n ), q_0 (\\mathbf{s}_0 ), q_1 (\\mathbf{s}_0 ), \\ldots ,q_p (\\mathbf{s}_0 )\n\\right\\}^\\mathbf{T};  q_0 (\\mathbf{s}_0 ) = 1\n</math>\n\nHence, KED looks exactly as ordinary kriging, except the covariance matrix/vector are extended with values of auxiliary predictors.\n\nAlthough the KED seems, at first glance, to be computationally more straightforward than RK, the parameters of the [[variogram]] for KED must also be estimated from regression residuals, thus requiring a separate regression modelling step. This regression should be GLS because of the likely spatial correlation between residuals. Note that many analyst use instead the OLS residuals, which may not be too different from the GLS residuals. However, they are not optimal if there is any spatial correlation, and indeed they may be quite different for clustered sample points or if the number of samples is relatively small (<math>\\ll 200</math>).\n\nA limitation of KED is the instability of the extended matrix in the case that the covariate does not vary smoothly in space. RK has the advantage that it explicitly separates trend estimation from spatial prediction of residuals, allowing the use of arbitrarily-complex forms of regression, rather than the simple linear techniques that can be used with KED. In addition, it allows the separate interpretation of the two interpolated components. The emphasis on regression is important also because fitting of the deterministic part of variation (regression) is often more beneficial for the quality of final maps than fitting of the stochastic part (residuals).\n\n== Software to run regression-kriging ==\n\n[[File:A generic framework for spatial prediction of soil variables.png|thumb|400px|Example of a generic framework for spatial prediction of soil variables based on regression-kriging.<ref name=\"Hengl2004Geoderma\" />]]\n\nRegression-kriging can be automated e.g. in [http://r-project.org R statistical computing] environment, by using gstat and/or geoR package. Typical inputs/outputs include:\n\nINPUTS:\n* Interpolation set (point map) — <math>z(\\mathbf{s}_i)</math> <math>i=1,\\ldots ,n</math> at primary locations;\n* Minimum and maximum expected values and measurement precision (<math>\\Delta z</math>);\n* Continuous predictors (raster map) — <math>q(\\mathbf{s})</math>; at new unvisited locations\n* Discrete predictors (polygon map);\n* Validation set (point map) — <math>z*(\\mathbf{s}_j)</math> <math>j=1,\\ldots ,l</math> (optional);\n* Lag spacing and limiting distance (required to fit the variogram);\n\nOUTPUTS:  \t\n* Map of predictions and relative prediction error;\n* Best subset of predictors and correlation significance (adjusted R-square);\n* Variogram model parameters (e.g. <math>C_0</math>, <math>C_1</math>, <math>R</math>)\n* GLS drift model coefficients;\n* Accuracy of prediction at validation points: mean prediction error (MPE) and root mean square prediction error (RMSPE);\n\n== Application of regression-kriging ==\n\nRegression-kriging is used in various applied fields, from meteorology, climatology, soil mapping, geological mapping, species distribution modeling and similar. The only requirement for using regression-kriging versus e.g. ordinary kriging is that one or more covariate layers exist, and which are significantly correlated with the feature of interest. Some general applications of regression-kriging are:\n\n* Geostatistical mapping: Regression-kriging allows for use of hybrid geostatistical techniques to model e.g. spatial distribution of soil properties.\n* [[Downscaling]] of maps: Regression-kriging can be used a framework to downscale various existing gridded maps. In this case the covariate layers need to be available at better resolution (which corresponds to the sampling intensity) than the original point data.<ref name=\"Hengl2008CG\">{{cite journal|last=Hengl|first=Tomislav|author2=Bajat, Branislav |author3=Blagojević, Dragan |author4= Reuter, Hannes I. |title=Geostatistical modeling of topography using auxiliary maps|journal=Computers & Geosciences|date=1 December 2008|volume=34|issue=12|pages=1886–1899|doi=10.1016/j.cageo.2008.01.005}}</ref> \n* [[Error propagation]]: Simulated maps generated by using a regression-kriging model can be used for scenario testing and for estimating propagated uncertainty.\n\n[[File:Simulations of zinc using regression-kriging model.png|thumb|none|Simulations of zinc concentrations derived using a regression-Kriging model. This model uses one continuous (distance to the river) and one categorical (flooding frequency) covariate. Code used to produce these maps is available [http://r-spatial.sourceforge.net/gallery/#fig07.R here].]]\n\nRegression-kriging-based algorithms play more and more important role in geostatistics because the number of possible covariates is increasing every day.<ref name=\"Pebesma2006IJGIS\" /> For example, [[Digital Elevation Model|DEM]]s are now available from a number of sources. Detailed and accurate images of topography can now be ordered from remote sensing systems such as [[SPOT (satellite)|SPOT]] and [[Advanced Spaceborne Thermal Emission and Reflection Radiometer|ASTER]]; SPOT5 offers the High Resolution Stereoscopic (HRS) scanner, which can be used to produce DEMs at resolutions of up to 5 m.<ref>{{cite journal|last=Toutin|first=Thierry|title=Generation of DSMs from SPOT-5 in-track HRS and across-track HRG stereo data using spatiotriangulation and autocalibration|journal=ISPRS Journal of Photogrammetry and Remote Sensing|date=30 April 2006|volume=60|issue=3|pages=170–181|doi=10.1016/j.isprsjprs.2006.02.003}}</ref> Finer differences in elevation can also be obtained with airborne laser-scanners. The cost of data is either free or dropping in price as technology advances. NASA recorded most of the world's topography in the [[Shuttle Radar Topographic Mission]] in 2000.<ref>{{cite journal|last=Rabus|first=Bernhard|author2=Eineder, Michael |author3=Roth, Achim |author4= Bamler, Richard |title=The shuttle radar topography mission—a new class of digital elevation models acquired by spaceborne radar|journal=ISPRS Journal of Photogrammetry and Remote Sensing|date=31 January 2003|volume=57|issue=4|pages=241–262|doi=10.1016/S0924-2716(02)00124-7}}</ref> From summer of 2004, these data has been available (e.g. via [https://lpdaac.usgs.gov/get_data/data_pool USGS ftp]) for almost whole globe at resolution of about 90 m (for the North American continent at resolution of about 30 m). Likewise, [[MODIS]] multispectral images are freely available for download at resolutions of 250 m. A large free repository of Landsat images is also available for download via the [http://glcf.umiacs.umd.edu/ Global Land Cover Facility] (GLCF).\n\n==References==\n<references />\n\n== Further reading ==\n* Chapter 2, ''Regression-kriging'', in Tomislav Hengl (2009), ''A Practical Guide to Geostatistical Mapping'', 291 p., {{ISBN|978-90-9024981-0}}. [http://spatial-analyst.net/book/system/files/Hengl_2009_GEOSTATe2c1w.pdf]\n* {{cite journal |authors=Hengl T., Heuvelink G. B. M., Rossiter D. G. |year=2007 |title=About regression-kriging: from equations to case studies |journal=Computers & Geosciences |volume=33 |number=10 |pages=1301–1315 |doi=10.1016/j.cageo.2007.05.001}}\n\n== External links ==\n* [http://gstat.org Gstat] package (implements KED)\n* [http://leg.ufpr.br/geoR/ GeoR] package (implements KED)\n\n[[Category:Interpolation]]\n[[Category:Geostatistics]]"
    },
    {
      "title": "Sarason interpolation theorem",
      "url": "https://en.wikipedia.org/wiki/Sarason_interpolation_theorem",
      "text": "{{multiple issues|\n{{primary sources|date=August 2012}}\n{{context|date=August 2012}}\n}}\nIn mathematics, the '''Sarason interpolation theorem''', introduced by {{harvs|txt|last=Sarason||authorlink=Donald Sarason|year=1967}}, is a generalization of the [[Caratheodory interpolation theorem]] and [[Nevanlinna–Pick interpolation]].\n\n==References==\n\n*{{Cite journal | last1=Sarason | first1=Donald | title=Generalized interpolation in H<sup>∞</sup> | jstor=1994641 |mr=0208383 | year=1967 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=127 | issue=2 | pages=179–203 | doi=10.2307/1994641}}\n\n[[Category:Theorems in analysis]]\n[[Category:Interpolation]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Simple rational approximation",
      "url": "https://en.wikipedia.org/wiki/Simple_rational_approximation",
      "text": "'''Simple rational approximation (SRA)''' is a subset of [[Interpolation|interpolating]] methods using [[rational function]]s. Especially, SRA interpolates a given function with a specific rational function whose [[pole (complex analysis)|pole]]s and [[root of a function|zeros]] are simple, which means that there is no multiplicity in poles and zeros. Sometimes, it only implies simple poles.\n\nThe main application of SRA lies in finding the [[root of a function|zeros]] of [[secular function]]s. A [[divide-and-conquer algorithm]] to find the [[eigenvalues]] and [[eigenvectors]] for various kinds of [[Matrix (mathematics)|matrices]] is well known in [[numerical analysis]]. In a strict sense, SRA implies a specific [[interpolation]] using simple rational functions as a part of the divide-and-conquer algorithm. Since such secular functions consist of a series of rational functions with simple poles, SRA is the best candidate to interpolate the zeros of the secular function. Moreover, based on previous researches, a simple zero that lies between two adjacent poles can be considerably well interpolated by using a two-dominant-pole rational function as an approximating function.\n\n== One-point third-order iterative method: Halley's formula ==\nThe origin of the interpolation with rational functions can be found in the previous work done by [[Edmond Halley]]. [[Halley's method|Halley's formula]] is known as one-point third-order iterative method to solve <math>\\,f(x)=0</math> by means of approximating a rational function defined by\n:<math>h(z)=\\frac{a}{z+b}+c.</math>\nWe can determine a, b, and c so that \n:<math>h^{(i)}(x)=f^{(i)}(x), \\qquad i=0,1,2.</math>\nThen solving <math>\\,h(z)=0</math> yields the iteration\n:<math>x_{n+1}=x_{n}-\\frac{f(x_n)}{f'(x_n)} \\left({\\frac{1}{1-\\frac{f(x_n)f''(x_n)}{2(f'(x_n))^2}}}\\right).</math>\nThis is referred to as Halley's formula.\nThis ''geometrical interpretation'' <math>h(z)</math> was derived by Gander(1978), where the equivalent iteration also was derived by applying Newton's method to\n:<math>g(x)=\\frac{f(x)}{\\sqrt{f'(x)}}=0.</math>\nWe call this ''algebraic interpretation'' <math>g(x)</math> of Halley's formula.\n\n==One-point second-order iterative method: Simple rational approximation==\nSimilarly, we can derive a variation of Halley's formula based on a one-point ''second-order'' iterative method to solve <math>\\,f(x)=\\alpha(\\neq 0)</math> using simple rational approximation by\n:<math>h(z)=\\frac{a}{z+b}.</math>\nThen we need to evaluate\n:<math>h^{(i)}(x)=f^{(i)}(x), \\qquad i=0,1.</math>\nThus we have\n:<math>x_{n+1}=x_{n}-\\frac{f(x_n)-\\alpha}{f'(x_n)} \\left(\\frac{f(x_n)}{\\alpha}\\right).</math>\nThe algebraic interpretation of this iteration is obtained by solving \n:<math>g(x)=1-\\frac{\\alpha}{{f(x)}}=0.</math>\nThis one-point second-order method is known to show a locally quadratic convergence if the root of equation is simple.\nSRA strictly implies this one-point second-order interpolation by a simple rational function.\n\nWe can notice that even third order method is a variation of Newton's method. We see the Newton's steps are multiplied by some factors. These factors are called the ''convergence factors'' of the variations, which are useful for analyzing the rate of convergence. See Gander(1978).\n\n== References ==\n*{{citation\n | last = Demmel | first = James W. | authorlink = James Demmel\n | mr = 1463942\n | isbn = 0-89871-389-7\n | location = Philadelphia, PA\n | publisher = [[Society for Industrial and Applied Mathematics]]\n | title = Applied Numerical Linear Algebra\n | year = 1997}}.\n*{{citation\n | last1 = Elhay | first1 = S.\n | last2 = Golub | first2 = G. H. | author2-link = Gene H. Golub\n | last3 = Ram | first3 = Y. M.\n | doi = 10.1016/S0898-1221(03)90229-X\n | mr = 2020255\n | issue = 8–9\n | journal = Computers & Mathematics with Applications\n | pages = 1413–1426\n | title = The spectrum of a modified linear pencil\n | volume = 46\n | year = 2003}}.\n*{{citation\n | last1 = Gu | first1 = Ming\n | last2 = Eisenstat | first2 = Stanley C.\n | doi = 10.1137/S0895479892241287\n | mr = 1311425\n | issue = 1\n | journal = SIAM Journal on Matrix Analysis and Applications\n | pages = 172–191\n | title = A divide-and-conquer algorithm for the symmetric tridiagonal eigenproblem\n | volume = 16\n | year = 1995| url = https://zenodo.org/record/1236142/files/article.pdf\n }}.\n*{{citation\n | last = Gander | first = Walter\n | publisher = [[Stanford University]], School of Humanities and Sciences, Computer Science Dept.\n | title = On the linear least squares problem with a quadratic constraint\n | year = 1978}}.\n\n[[Category:Interpolation]]"
    },
    {
      "title": "Spline (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Spline_%28mathematics%29",
      "text": "{{For|the [[Technical drawing|drafting tool]]|Flat spline}}\n\n[[Image:Parametic Cubic Spline.svg|thumb|Single knots at 1/3 and 2/3 establish a spline of three cubic polynomials meeting with ''C<sup>2</sup>'' continuity. Triple knots at both ends of the interval ensure that the curve interpolates the end points]]\nIn [[mathematics]], a '''spline''' is a special [[Function (mathematics)|function]] defined [[piecewise]] by [[polynomial]]s.\nIn [[interpolation|interpolating]] problems, [[spline interpolation]] is often preferred to [[polynomial interpolation]] because it yields similar results, even when using low degree polynomials, while avoiding [[Runge's phenomenon]] for higher degrees.\n\nIn the [[computer science]] subfields of [[computer-aided design]] and [[computer graphics]], the term spline more frequently refers to a piecewise polynomial [[parametric curve|(parametric)]] [[curve]]. Splines are popular curves in these subfields\nbecause of the simplicity of their construction, their ease and accuracy of evaluation, and their capacity to approximate complex shapes through [[curve fitting]] and interactive curve design.\n\nThe term spline comes from the flexible [[Flat spline|spline]] devices used by shipbuilders and [[technical drawing|draftsmen]] to draw smooth shapes.\n\n==Introduction==\n\nThe term \"spline\" is used to refer to a wide class of functions that are used in applications requiring data interpolation and/or smoothing. The data may be either one-dimensional or multi-dimensional. Spline functions for interpolation are normally determined as the minimizers of suitable measures of roughness (for example integral squared curvature) subject to the interpolation constraints. Smoothing splines may be viewed as generalizations of interpolation splines where the functions are determined to minimize a weighted combination of the average squared approximation error over observed data and the roughness measure. For a number of meaningful definitions of the roughness measure, the spline functions are found to be finite dimensional in nature, which is the primary reason for their utility in computations and representation. For the rest of this section, we focus entirely on one-dimensional, polynomial splines and use the term \"spline\" in this restricted sense.\n\n== Definition ==\n{{Confusing|date=February 2009}}\nWe begin by limiting our discussion to the [[univariate]] polynomial case.  In this case, a spline is a [[piecewise]] [[polynomial function]].\nThis function, call it ''S'', takes values from an interval [''a'',''b''] and maps them to <math>\\mathbb{R}</math>, the set of [[real numbers]],<br>\n:<math>S: [a,b]\\to \\mathbb{R}.</math>\nWe want ''S'' to be piecewise defined.  To accomplish this, let the interval [''a'',''b''] be covered by ''k'' ordered, [[Disjoint sets|disjoint]] subintervals,  \n:<math>[t_i, t_{i+1}] \\mbox{ , } i = 0,\\ldots, k-1</math>\n:<math>[a,b] = [t_0,t_1] \\cup [t_1,t_2] \\cup \\cdots \\cup [t_{k-2},t_{k-1}] \\cup [t_{k-1},t_k]</math>\n:<math>a = t_0 \\le t_1 \\le \\cdots \\le t_{k-1} \\le t_k = b</math>\nOn each of these ''k'' \"pieces\" of [''a'',''b''], we want to define a polynomial, call it ''P''<sub>''i''</sub>.<br>\n:<math>P_i: [t_i, t_{i+1}]\\to \\mathbb{R}</math>.\nOn the ''i''th subinterval of [''a'',''b''], ''S'' is defined by ''P''<sub>''i''</sub>,<br>\n:<math>S(t) = P_0 (t) \\mbox{ , } t_0 \\le t < t_1,</math>\n:<math>S(t) = P_1 (t) \\mbox{ , } t_1 \\le t < t_2,</math>\n:<math>\\vdots</math>\n:<math>S(t) = P_{k-1} (t) \\mbox{ , } t_{k-1} \\le t \\le t_k.</math>\n\nThe given ''k+1'' points ''t''<sub>''i''</sub> are called '''knots'''. The vector\n<math>{\\bold t}=(t_0, \\dots, t_k)</math> is called a '''knot vector''' for the spline.\nIf the knots are equidistantly distributed in the interval [''a'',''b''] we say the spline is '''uniform''', otherwise we say it is '''non-uniform'''.\n\nIf the polynomial pieces ''P''<sub>''i''</sub> each have degree at most ''n'', then the spline is said to be of '''degree''' <math>\\leq n</math> (or of\n'''order''' ''n+1'').\n\nIf <math>S\\in C^{r_i}</math> in a neighborhood of ''t''<sub>''i''</sub>, then the spline is said to be\nof [[Smooth function|smoothness]] (at least) <math>C^{r_i}</math> at ''t''<sub>''i''</sub>. That is,\nat ''t''<sub>''i''</sub> the two pieces ''P''<sub>''i-1''</sub> and ''P''<sub>''i''</sub> share common\nderivative values from the derivative of order 0 (the function value)\nup through the derivative of order ''r''<sub>''i''</sub> (in other words, the two adjacent polynomial pieces connect with '''loss of smoothness''' of at most ''n'' - ''r''<sub>''i''</sub>).\n\nA vector\n<math>{\\bold r}=(r_1, \\dots, r_{k-1})</math> such that the spline has smoothness <math>C^{r_i}</math> at ''t''<sub>''i''</sub> for <math>i = 0,\\ldots, k-1</math> is called a '''smoothness vector''' for the spline.\n\nGiven a knot vector <math>{\\bold t}</math>, a degree ''n'', and a smoothness vector <math>{\\bold r}</math> for <math>{\\bold t}</math>, one can consider the set of all splines of degree <math>\\leq n</math> having knot vector \n<math>{\\bold t}</math> and smoothness vector <math>{\\bold r}</math>. Equipped with the operation of adding two functions (pointwise addition) and taking real multiples of functions, this set becomes a real vector space. This '''spline space''' is commonly denoted by <math>S^{\\bold r}_n({\\bold t})</math>.\n\nIn the mathematical study of polynomial splines the question of what happens when two knots,\nsay ''t''<sub>''i''</sub> and ''t''<sub>''i''+1</sub>,\nare moved together has an easy answer. The polynomial piece\n''P''<sub>''i''</sub>(''t'')\ndisappears, and the pieces\n''P''<sub>''i''−1</sub>(''t'') and ''P''<sub>''i''+1</sub>(''t'')\njoin with the sum of the continuity losses for\n''t''<sub>''i''</sub> and ''t''<sub>''i''+1</sub>.\nThat is,\n:<math> S(t) \\in C^{n-j_i-j_{i+1}} [t_i = t_{i+1}],</math> where <math>j_i = n - r_i</math>\nThis leads to a more general understanding of a knot vector.\nThe continuity loss at any point can be considered to be the result of\n'''multiple knots''' located at that point, and a spline type can be completely\ncharacterized by its degree ''n'' and its '''extended''' knot vector\n\n:<math>\n(t_0 , t_1 , \\cdots , t_1 , t_2, \\cdots , t_2 , t_3 , \\cdots , t_{k-2} , t_{k-1} , \\cdots , t_{k-1} , t_k)\n</math>\n\nwhere ''t''<sub>''i''</sub> is repeated ''j''<sub>''i''</sub> times\nfor <math>i = 1, \\dots , k-1</math>.\n\nA [[parametric curve]] on the interval [''a'',''b'']\n:<math>G(t) = ( X(t), Y(t) ) \\mbox{ , } t \\in [ a , b ]</math>\nis a '''spline curve''' if both ''X'' and ''Y'' are spline functions\nof the same degree with the same extended knot vectors on that interval.\n\n==Examples==\nSuppose the interval [''a'',''b''] is [0,3] and the subintervals\nare [0,1], [1,2], and [2,3]. Suppose the polynomial pieces are\nto be of degree 2, and the pieces on [0,1] and [1,2] must join in value and first derivative\n(at ''t''=1)\nwhile the pieces on [1,2] and [2,3] join simply in value (at ''t'' = 2).\nThis would define a type of spline ''S''(''t'') for which\n:<math>S(t) = P_0 (t) = -1+4t-t^2 \\mbox{ , } 0 \\le t < 1</math>\n:<math>S(t) = P_1 (t) = 2t \\mbox{ , } 1 \\le t < 2</math>\n:<math>S(t) = P_2 (t) = 2-t+t^2 \\mbox{ , } 2 \\le t \\le 3</math>\nwould be a member of that type, and also\n:<math>S(t) = P_0 (t) = -2-2t^2 \\mbox{ , } 0 \\le t < 1</math>\n:<math>S(t) = P_1 (t) = 1-6t+t^2 \\mbox{ , } 1 \\le t < 2</math>\n:<math>S(t) = P_2 (t) = -1+t-2t^2 \\mbox{ , } 2 \\le t \\le 3</math>\nwould be a member of that type.\n(Note: while the polynomial piece 2''t'' is not quadratic, the result is still called a quadratic spline. This demonstrates that the degree of a spline is the maximum degree of its polynomial parts.)\nThe extended knot vector for this type of spline would be (0, 1, 2, 2, 3).\n\nThe simplest spline has degree 0. It is also called a [[step function]].\nThe next most simple spline has degree 1. It is also called a '''linear spline'''. A closed linear spline (i.e, the first knot and the last are the same) in the plane is just a [[polygon]].\n\nA common spline is the '''natural cubic spline''' of degree 3 with continuity ''C''<sup>2</sup>.\nThe word \"natural\" means that the second derivatives of\nthe spline polynomials\nare set equal to zero at the endpoints of the interval of interpolation\n\n:<math>S''(a) \\, = S''(b) = 0.</math>\n\nThis forces the spline to be a straight line outside of the interval, while not disrupting its smoothness.\n\n===Algorithm for computing natural cubic splines===\n\nCubic splines are of the form <math>{S}_{j} \\left ( x  \\right ) =  a_j + b_j \\left ( x-x_j \\right ) +  c_j  {\\left ( x-x_j \\right ) }^{2} + d_j {\\left ( x-x_j \\right ) }^{3}</math>.<br />\nGiven set of coordinates <math>C=  \\left[    \\left ( {x}_{0},{y}_{0}  \\right ) ,  \\left ( {x}_{1},{y}_{1}   \\right ) , .... ,  \\left ( {x}_{n},{y}_{n}  \\right ) \\right ]</math> we wish to find set of <math>n \\,</math> splines <math>{S}_{i} \\left ( x  \\right )</math> for <math>i = 0 , \\ldots , n-1.</math>\n\nThese must satisfy:\n*<math> S_i \\left (x_i \\right) = y_i = S_{i-1}\\left (x_i \\right ), i = 1 , \\ldots , n-1.</math>\n*<math>{S^'}_i \\left (x_i \\right) = {S^'}_{i-1}\\left (x_i \\right ), i = 1 , \\ldots , n-1.</math>\n*<math>{S^{''}}_i \\left (x_i \\right) = {S^{''}}_{i-1}\\left (x_i \\right ), i = 1 , \\ldots , n-1.</math>\n*<math>{S^{''}}_0 \\left (x_0 \\right) = {S^{''}}_{n-1} \\left (x_n \\right ) =0</math>.\n\nLet us define one cubic spline <math>S \\,</math> as a 5-tuple <math>(a,b,c,d,x_t) \\,</math> where <math>a,b,c \\,</math> and <math>d \\,</math> correspond to coefficients in the form shown earlier and <math>x_t \\,</math> is equal to <math>x_j. \\,</math>\n\n'''Algorithm for computing Natural Cubic Splines:'''<br />\nInput: set of coordinates <math>C \\,</math>, with <math>\\left | C  \\right | =n+1</math><br />\nOutput: set splines which is composed of ''n'' 5-tuples.\n# Create new array ''a'' of size ''n + 1'' and for <math>i = 0 , \\ldots , n</math> set <math>a_i = y_i \\,</math>\n# Create new arrays ''b'' and ''d'' each of size ''n''.\n# Create new array ''h'' of size ''n'' and for <math>i = 0 , \\ldots , n-1</math> set <math>h_i = x_{i+1} - x_i \\,</math>\n# Create new array ''α'' of size ''n'' and for <math>i = 1 , \\ldots , n-1</math> set <math>{ \\alpha }_{i}= \\frac{3 }{{h}_{i} }  \\left (  {a}_{i+1}-{a}_{i} \\right )  -  \\frac{3 }{{h}_{i-1} }  \\left (  {a}_{i}-{a}_{i-1} \\right ) </math>.\n# Create new arrays ''c'', ''l'', ''μ'', and ''z'' each of size <math>n+1 \\,</math>.\n# Set <math> l_0 = 1, {\\mu}_0 = z_0 = 0 \\,</math>\n# For <math> i = 1 , \\ldots , n-1 \\,</math>\n## Set  <math>{ l}_{i } =2 \\left ( {x}_{i+1}-{x}_{i-1}  \\right ) - {h}_{i-1}{\\mu}_{i-1}</math>.\n## Set <math>{\\mu}_{i}= \\frac{ {h}_{i}}{{l}_{i} } </math>.\n## Set <math>{z}_{i} =  \\frac{ {\\alpha}_{i}-{h}_{i-1}{z}_{i-1}}{{l}_{i} } </math>.\n# Set <math> l_n = 1; z_n = c_n = 0. \\,</math>\n# For <math> j = n-1 , n-2 , \\ldots , 0 </math>\n## Set <math> c_j = z_j - {\\mu}_j c_{j+1} \\,</math>\n## Set <math> b_j = \\frac{{a}_{j+1}-{a}_{j} }{{h}_{j} } -  \\frac{ {h}_{j} \\left ( {c}_{j+1} +2{c}_{j}  \\right ) }{ 3} </math>\n## Set <math> d_j = \\frac{{c}_{j+1}-{c}_{j} }{3{h}_{j} } </math>\n# Create new set Splines and call it output_set.  Populate it with ''n'' splines ''S''.\n# For <math>i = 0 , \\ldots , n-1</math>\n## Set ''S''<sub>''i'',''a''</sub> = ''a''<sub>''i''</sub>\n## Set ''S''<sub>''i'',''b''</sub> = ''b''<sub>''i''</sub>\n## Set ''S''<sub>''i'',''c''</sub> = ''c''<sub>''i''</sub>\n## Set ''S''<sub>''i'',''d''</sub> = ''d''<sub>''i''</sub>\n## Set ''S''<sub>''i'',''x''</sub> = ''x''<sub>''i''</sub>\n# Output output_set\n\n==Notes==\nIt might be asked what meaning more than ''n'' multiple knots in a knot vector have, since this would lead to continuities like\n:<math>S(t) \\in C^{-m} \\mbox{ , } m > 0</math>\nat the location of this high multiplicity. By convention, any such situation indicates a simple discontinuity between the two adjacent polynomial pieces. This means that if a knot ''t''<sub>''i''</sub> appears more than ''n'' + 1 times in an extended knot vector, all instances of it in excess of the (''n'' + 1)th can be removed without changing the character of the spline, since all multiplicities ''n'' + 1, ''n'' + 2, ''n'' + 3, etc. have the same meaning. It is commonly assumed that any knot vector defining any type of spline has been culled in this fashion.\n\nThe classical spline type of degree ''n'' used in numerical analysis has continuity\n:<math>S(t) \\in \\mathrm{C}^{n-1} [a,b],\\,</math>\nwhich means that every two adjacent polynomial pieces meet in their value and first ''n'' - 1 derivatives at each knot. The mathematical spline that most closely models the [[flat spline]] is a cubic (''n'' = 3), twice continuously differentiable (''C''<sup>2</sup>), natural spline, which is a spline of this classical type with additional conditions imposed at endpoints ''a'' and ''b''.\n\nAnother type of spline that is much used in graphics, for example in drawing programs such as [[Adobe Illustrator]] from [[Adobe Systems]], has pieces that are cubic but has continuity only at most\n:<math>S(t) \\in \\mathrm{C}^{1} [a,b].</math>\nThis spline type is also used in [[PostScript]] as well as in the definition of some computer typographic fonts.\n\nMany computer-aided design systems that are designed for high-end graphics and animation use extended knot vectors,\nfor example [[Maya (software)|Maya]] from [[Alias Systems Corporation|Alias]].\nComputer-aided design systems often use an extended concept of a spline known as a [[Nonuniform rational B-spline]] (NURBS).\n\nIf sampled data from a function or a physical object is available, [[spline interpolation]] is an approach to creating a spline that approximates that data.\n\n==General Expression For a ''C''<sup>2</sup> Interpolating Cubic Spline==\n   \nThe general expression for the ''i''th ''C''<sup>2</sup> interpolating cubic spline at a point ''x'' with the natural condition can be found using the formula\n\n:<math>S_i(x)= \\frac{z_i(x-t_{i-1})^3}{6h_i} +\\frac{z_{i-1}(t_i-x)^3}{6h_i}+\\left[ \\frac{f(t_i)}{h_i}-\\frac{z_ih_i}{6}\\right](x-t_{i-1})+\\left[ \\frac{f(t_{i-1})}{h_i}-\\frac{z_{i-1}h_i}{6}\\right](t_i-x)</math>\n\nwhere \n* <math>z_i = f^{\\prime\\prime}(t_i)</math> are the values of the second derivative at the ''i''th knot.\n* <math> h_i^{} = t_i-t_{i-1} </math>\n* <math> f(t_i^{}) </math> are the values of the function at the ''i''th knot.\n\n==Representations and Names ==\nFor a given interval [''a'',''b''] and a given extended knot vector on that interval, the splines of degree ''n'' form a [[vector space]]. Briefly this means that adding any two splines of a given type produces spline of that given type, and multiplying a spline of a given type by any constant produces a spline of that given type. The [[Hamel dimension|dimension]] of\nthe space containing all splines of a certain type can be counted from the extended knot vector:\n:<math>\na = t_0\n< \\underbrace{t_1 = \\cdots = t_1}_{j_1}\n< \\cdots\n< \\underbrace{t_{k-2} =\\cdots =t_{k-2}}_{j_{k-2}}\n< t_{k-1} = b\n</math>\n:<math>\nj_i \\le n+1 ~,~~ i=1,\\ldots,k-2.\n</math>\nThe dimension is equal to the sum of the degree plus the multiplicities\n:<math>d = n + \\sum_{i=1}^{k-2} j_i.</math>\nIf a type of spline has additional linear conditions imposed upon it, then the resulting spline will lie in a subspace. The space of all natural cubic splines, for instance, is a subspace of the space of all cubic ''C''<sup>2</sup> splines.\n\nThe literature of splines is replete with names for special types of splines.\nThese names have been associated with:\n* The choices made for representing the spline, for example:\n** using [[basis (linear algebra)|basis]] functions for the entire spline (giving us the name [[B-spline]]s)\n** using [[Bernstein polynomial]]s as employed by Pierre Bézier to represent each polynomial piece (giving us the name [[Bézier spline (disambiguation)|Bézier spline]]s<!--intentional link to DAB page-->)\n* The choices made in forming the extended knot vector, for example:\n** using single knots for ''C''<sup>''n''-1</sup> continuity and spacing these knots evenly on [''a'',''b''] (giving us '''uniform splines''')\n** using knots with no restriction on spacing (giving us '''nonuniform splines''')\n* Any special conditions imposed on the spline, for example:\n** enforcing zero second derivatives at ''a'' and ''b'' (giving us '''natural splines''')\n** requiring that given data values be on the spline (giving us '''interpolating splines''')\nOften a special name was chosen for a type of spline satisfying two or more of the main items above. For example, the [[Hermite spline]] is a spline that is expressed using Hermite polynomials to represent each of the individual polynomial pieces. These are most often used with ''n'' = 3; that is, as [[Cubic Hermite spline]]s. In this degree they may additionally be chosen to be only tangent-continuous (''C''<sup>1</sup>); which implies that all interior knots are double. Several methods have been invented to fit such splines to given data points; that is, to make them into interpolating splines, and to do so by estimating plausible tangent values where each two polynomial pieces meet (giving us [[Cardinal spline]]s, [[Catmull-Rom spline]]s, and [[Kochanek-Bartels spline]]s, depending on the method used).\n\nFor each of the representations, some means of evaluation must be found so that values of the spline can be produced on demand. For those representations that express each individual polynomial piece ''P''<sub>''i''</sub>(''t'') in terms of\nsome basis for the degree ''n'' polynomials, this is conceptually straightforward:\n* For a given value of the argument ''t'', find the interval in which it lies <math>t \\in [t_i,t_{i+1}]</math>\n* Look up the polynomial basis chosen for that interval <math>P_0, \\ldots, P_{k-2}</math>\n* Find the value of each basis polynomial at ''t'': <math>P_0(t), \\ldots, P_{k-2}(t)</math>\n* Look up the coefficients of the linear combination of those basis polynomials that give the spline on that interval ''c''<sub>0</sub>, ..., ''c''<sub>''k''-2</sub>\n* Add up that linear combination of basis polynomial values to get the value of the spline at ''t'':\n:<math>\\sum_{j=0}^{k-2} c_j P_j(t).</math>\nHowever, the evaluation and summation steps are often combined in clever ways. For example, Bernstein polynomials are a basis for polynomials that can be evaluated in linear combinations efficiently using special recurrence relations. This is the essence of [[De Casteljau's algorithm]], which features in [[Bézier curve]]s and [[Bézier spline]]s.\n\nFor a representation that defines a spline as a linear combination of basis splines, however, something more sophisticated is needed. The [[de Boor algorithm]] is an efficient method for evaluating [[B-spline]]s.\n\n==History==\nBefore computers were used, numerical calculations were done by hand. Although piecewise-defined functions like the [[sign function]] or [[step function]] were used, polynomials were generally preferred because they were easier to work with. Through the advent of computers splines have gained importance. They were first used as a replacement for polynomials in interpolation, then as a tool to construct smooth and flexible shapes in computer graphics.\n\nIt is commonly accepted that the first mathematical reference to splines is the 1946 paper by [[Isaac Jacob Schoenberg|Schoenberg]], which is probably the first place that the word \"spline\" is used in connection with smooth, piecewise polynomial approximation. However, the ideas have their roots in the aircraft and shipbuilding industries. In the foreword to (Bartels et al., 1987), [[Robin Forrest]] describes \"[[lofting]]\", a technique used in the British aircraft industry during [[World War II]] to construct templates for airplanes by passing thin wooden strips (called \"[[flat spline|spline]]s\") through points laid out on the floor of a large design loft, a technique borrowed from ship-hull design. For years the practice of ship design had employed models to design in the small. The successful design was then plotted on graph paper and the key points of the plot were re-plotted on larger graph paper to full size. The thin wooden strips provided an interpolation of the key points into smooth curves. The strips would be held in place at discrete points (called \"ducks\" by Forrest; Schoenberg used \"dogs\" or \"rats\") and between these points would assume shapes of minimum strain energy. According to Forrest, one possible impetus for a mathematical model for this process was the potential loss of the critical design components for an entire aircraft should the loft be hit by an enemy bomb. This gave rise to \"conic lofting\", which used conic sections to model the position of the curve between the ducks. Conic lofting was replaced by what we would call splines in the early 1960s based on work by [[J. C. Ferguson]] at [[Boeing]] and (somewhat later) by [[Malcolm Sabin|M.A. Sabin]] at [[British Aircraft Corporation]]. \n\nThe word \"spline\" was originally an [[East Anglian English|East Anglian]] dialect word.\n\nThe use of splines for modeling automobile bodies seems to have several independent beginnings. Credit is claimed on behalf of [[Paul de Casteljau|de Casteljau]] at [[Citroën]], [[Pierre Bézier]] at [[Renault]], and [[Garrett Birkhoff|Birkhoff]], [[Garabedian]], and [[Carl R. de Boor|de Boor]] at [[General Motors Corporation|General Motors]] (see Birkhoff and de Boor, 1965), all for work occurring in the very early 1960s or late 1950s. At least one of de Casteljau's papers was published, but not widely, in 1959. De Boor's work at [[General Motors Corporation|General Motors]] resulted in a number of papers being published in the early 1960s, including some of the fundamental work on [[B-spline]]s. \n\nWork was also being done at Pratt & Whitney Aircraft, where two of the authors of (Ahlberg et al., 1967) — the first book-length treatment of splines — were employed, and the [[David Taylor Model Basin]], by Feodor Theilheimer. The work at [[General Motors Corporation|General Motors]] is detailed nicely in (Birkhoff, 1990) and (Young, 1997). Davis (1997) summarizes some of this material.\n\n==References==\n* Ferguson, James C, ''Multi-variable curve interpolation,''  J. ACM, vol. 11, no. 2, pp. 221-228, Apr. 1964.\n* Ahlberg, Nielson, and Walsh, ''The Theory of Splines and Their Applications,'' 1967.\n* Birkhoff, Fluid dynamics, reactor computations, and surface representation, in: Steve Nash (ed.), ''A History of Scientific Computation'', 1990.\n* Bartels, Beatty, and Barsky, ''An Introduction to Splines for Use in Computer Graphics and Geometric Modeling,'' 1987.\n* Birkhoff and de Boor, Piecewise polynomial interpolation and approximation, in: H. L. Garabedian (ed.), ''Proc. General Motors Symposium of 1964,'' pp. 164–190. Elsevier, New York and Amsterdam, 1965.\n* Davis, [http://www.wpi.edu/~pwdavis/sinews/spline17.htm B-splines and Geometric design], ''SIAM News,'' vol. 29, no. 5, 1997.\n* Epperson, [http://www.netlib.org/na-digest-html/98/v98n26.html#1 History of Splines], ''NA Digest,'' vol. 98, no. 26, 1998.\n* Stoer & Bulirsch, Introduction to Numerical Analysis. [[Springer Science+Business Media|Springer-Verlag]]. p. 93-106. {{ISBN|0387904204}}\n* Schoenberg, Contributions to the problem of approximation of equidistant data by analytic functions, ''Quart. Appl. Math.,'' vol. 4, pp. 45–99 and 112–141, 1946.\n* Young, Garrett Birkhoff and applied mathematics, ''Notices of the AMS,'' vol. 44, no. 11, pp. 1446–1449, 1997.\n* Chapra, Canale, \"Numerical Methods for Engineers\" 5th edition.\n\n== External links ==\n\n'''Theory'''\n*[http://math.fullerton.edu/mathews/n2003/CubicSplinesMod.html Cubic Splines Module] Prof. John H. Mathews [[California State University, Fullerton]]\n* [http://ibiblio.org/e-notes/Splines/Intro.htm An Interactive Introduction to Splines], ibiblio.org\n\n'''Excel Function'''\n* [http://www.pimpmyexcel.com/ XLL Excel Addin Function Implementation of cubic spline]\n\n'''Online utilities'''\n* [http://www.akiti.ca/CubicSpline.html Online Cubic Spline Interpolation Utility]\n* [http://www.vias.org/simulations/simusoft_spline.html Learning by Simulations] Interactive simulation of various cubic splines\n* [http://demonstrations.wolfram.com/SymmetricalSplineCurves/ Symmetrical Spline Curves], an animation by [[Theodore Gray]], [[The Wolfram Demonstrations Project]], 2007.\n\n'''Computer Code'''\n* [http://numericalmethods.eng.usf.edu/topics/spline_method.html Notes, PPT, Mathcad, Maple, Mathematica, Matlab], ''Holistic Numerical Methods Institute''\n* [http://w3.pppl.gov/ntcc/PSPLINE/ various routines], NTCC\n* [http://www.sintef.no/sisl Sisl: Opensource C-library for NURBS], SINTEF\n* [http://www.vbnumericalmethods.com/math/ VBA Spline Interpolation], vbnumericalmethods.com\n\n[[Category:Splines| ]]\n[[Category:Interpolation]]\n\n[[de:Spline]]\n[[es:Spline]]\n[[fr:Spline]]\n[[it:Funzione spline]]\n[[hu:Spline]]\n[[nl:Spline]]\n[[ja:スプライン曲線]]\n[[no:Spline]]\n[[pl:Funkcja sklejana]]\n[[pt:Spline]]\n[[ru:Сплайн]]\n[[sl:Zlepek]]\n[[sr:Сплајн]]\n[[sv:Spline]]\n[[uk:Сплайн]]\n[[zh:样条函数]]"
    },
    {
      "title": "Spline interpolation",
      "url": "https://en.wikipedia.org/wiki/Spline_interpolation",
      "text": "{{short description|Mathematical method}}\n{{broader|Spline (mathematics)}}\nIn the [[mathematics|mathematical]] field of [[numerical analysis]], '''spline interpolation''' is a form of [[interpolation]] where the interpolant is a special type of [[piecewise]] [[polynomial]] called a [[spline (mathematics)|spline]]. Spline interpolation is often preferred over [[polynomial interpolation]] because the [[interpolation error]] can be made small even when using low degree polynomials for the spline.<ref>{{cite journal |last1=Hall |first1=Charles A. |last2=Meyer |first2=Weston W. |title=Optimal Error Bounds for Cubic Spline Interpolation |journal=Journal of Approximation Theory |date=1976 |volume=16 |issue=2 |pages=105–122|url=https://www.sciencedirect.com/science/article/pii/002190457690040X}}</ref> Spline interpolation avoids the problem of [[Runge's phenomenon]], in which oscillation can occur between points when interpolating using high degree polynomials.\n\n==Introduction==\nOriginally, ''[[Flat spline|spline]]'' was a term for [[wikt:elastic|elastic]] [[ruler]]s that were bent to pass through a number of predefined points (\"knots\").  These were used to make [[technical drawing]]s for [[shipbuilding]] and construction by hand, as illustrated by Figure 1.\n[[Image:Cubic spline.svg|thumb|upright=1.8|right|Figure 1: Interpolation with cubic splines between eight points. Hand-drawn technical drawings were made for shipbuilding etc. using flexible rulers that were bent to follow pre-defined points]]\n\nThe approach to mathematically model the shape of such elastic rulers fixed by {{math|''n'' + 1}} knots <math>\\left \\{ (x_i,y_i) :  i=0,1,\\cdots,n \\right \\}</math> is to interpolate between all the pairs of knots <math>(x_{i-1}, y_{i-1})</math> and <math>(x_{i}, y_{i})</math> with polynomials <math>y=q_i(x), i=1,2,\\cdots,n</math>.\n\nThe [[curvature]] of a curve  <math>y=f(x)</math> is given by:\n\n:<math>\\kappa= \\frac{y''}{(1+y'^2)^{3/2}}</math>\n\nAs the spline will take a shape that minimizes the bending (under the constraint of passing through all knots) both <math>y'</math> and <math>y''</math> will be continuous everywhere and at the knots. To achieve this one must have that\n\n:<math>\\begin{cases} q'_i(x_i) = q'_{i+1}(x_i) \\\\ q''_i(x_i) = q''_{i+1}(x_i) \\end{cases} \\qquad 1 \\le i \\le n-1</math>\n\nThis can only be achieved if polynomials of degree 3 or higher are used. The classical approach is to use polynomials of degree 3&nbsp;&mdash; the case of [[cubic spline]]s.\n\n==Algorithm to find the interpolating cubic spline==\n\nA third-order polynomial <math>q(x)</math> for which\n:<math>q(x_1)=y_1</math>\n:<math>q(x_2)=y_2</math>\n:<math>q'(x_1)=k_1</math>\n:<math>q'(x_2)=k_2</math>\n\ncan be written in the symmetrical form\n{{NumBlk|:|<math>q(x) =(1-t(x))y_1 + t(x)y_2+ t(x)(1-t(x)) (a(1-t(x)) + bt(x))</math>|{{EquationRef|1}}}}\nwhere\n{{NumBlk|:|<math>t(x)=\\frac{x-x_1}{x_2-x_1},</math>|{{EquationRef|2}}}}\n{{NumBlk|:|<math>a= k_1 (x_2 - x_1)-(y_2 - y_1),</math>|{{EquationRef|3}}}}\n{{NumBlk|:|<math>b=-k_2 (x_2 - x_1)+(y_2 - y_1).</math>|{{EquationRef|4}}}}\nAs\n:<math>q'= \\frac{d q}{d x} = \\frac{d q}{d t} \\frac{d t}{d x} = \\frac{d q}{d t} \\frac{1}{x_2-x_1}</math>\none gets that:\n{{NumBlk|:|<math>q'=\\frac {y_2-y_1}{x_2-x_1} +(1-2t) \\frac {a(1-t) + b t}{x_2-x_1}  +t(1-t) \\frac {b-a}{x_2-x_1},</math>|{{EquationRef|5}}}}\n{{NumBlk|:|<math>q''=2\\frac {b-2a+(a-b)3t}{{(x_2-x_1)}^2}.</math>|{{EquationRef|6}}}}\n\nSetting {{math|''x'' {{=}} ''x''<sub>1</sub>}} and {{math|''x'' {{=}} ''x''<sub>2</sub>}} respectively in equations ({{EquationNote|5}}) and ({{EquationNote|6}}) one gets from ({{EquationNote|2}}) that indeed first derivatives {{math|''q′''(''x''<sub>1</sub>) {{=}} ''k''<sub>1</sub>}} and {{math|''q′''(''x''<sub>2</sub>) {{=}} ''k''<sub>2</sub>}} and also second derivatives\n\n{{NumBlk|:|<math>q''(x_1)=2\\frac {b-2a}{{(x_2-x_1)}^2}</math>|{{EquationRef|7}}}}\n{{NumBlk|:|<math>q''(x_2)=2\\frac {a-2b}{{(x_2-x_1)}^2}</math>|{{EquationRef|8}}}}\n\nIf now {{math|(''x<sub>i</sub>'', ''y<sub>i</sub>''), ''i'' {{=}} 0, 1, ..., ''n''}} are {{math|''n'' + 1}} points and\n\n{{NumBlk|:|<math>q_i = (1-t) y_{i-1} + ty_i + t(1-t)(a_i (1-t) + b_i t)</math>|{{EquationRef|9}}}}\n\nwhere ''i'' = 1, 2, ..., ''n'' and <math>t=\\tfrac{x-x_{i-1}}{x_{i}-x_{i-1}}</math> are ''n'' third degree polynomials interpolating {{mvar|y}} in the interval {{math|''x''<sub>''i''−1</sub> ≤ ''x'' ≤ ''x<sub>i</sub>''}} for ''i'' = 1, ..., ''n'' such that {{math|''q′<sub>i</sub>'' (''x<sub>i</sub>'') {{=}} ''q′''<sub>''i''+1</sub>(''x<sub>i</sub>'')}} for ''i'' = 1, ..., ''n''−1  then the ''n'' polynomials together define a differentiable function in the interval {{math|''x''<sub>0</sub> ≤ ''x'' ≤ ''x<sub>n</sub>''}} and\n\n{{NumBlk|:|<math>a_i=k_{i-1}(x_i-x_{i-1})-(y_i - y_{i-1})</math>|{{EquationRef|10}}}}\n{{NumBlk|:|<math>b_i=-k_i(x_i-x_{i-1})+(y_i - y_{i-1})</math>|{{EquationRef|11}}}}\nfor ''i'' = 1, ..., ''n'' where\n{{NumBlk|:|<math>k_0=q_1'(x_0)</math>|{{EquationRef|12}}}}\n{{NumBlk|:|<math>k_i=q_i'(x_i)=q_{i+1}'(x_i) \\qquad i=1,\\dotsc ,n-1</math>|{{EquationRef|13}}}}\n{{NumBlk|:|<math>k_n=q_n'(x_n)</math>|{{EquationRef|14}}}}\n\nIf the sequence {{math|''k''<sub>0</sub>, ''k''<sub>1</sub>, ..., ''k<sub>n</sub>''}} is such that, in addition, {{math|''q′′<sub>i</sub>''(''x<sub>i</sub>'') {{=}} ''q′′''<sub>''i''+1</sub>(''x<sub>i</sub>'')}} holds for ''i'' = 1, ..., ''n''-1, then the resulting function will even have a continuous second derivative.\n\nFrom ({{EquationNote|7}}), ({{EquationNote|8}}), ({{EquationNote|10}}) and ({{EquationNote|11}}) follows that this is the case if and only if\n\n{{NumBlk|:|<math>\\frac {k_{i-1}}{x_i-x_{i-1}} + \\left(\\frac {1}{x_i-x_{i-1}}+ \\frac {1}{{x_{i+1}-x_i}}\\right) 2k_i+ \\frac {k_{i+1}}{{x_{i+1}-x_i}} =\n   3 \\left(\\frac {y_i - y_{i-1}}{{(x_i-x_{i-1})}^2}+\\frac {y_{i+1} - y_i}{{(x_{i+1}-x_i)}^2}\\right)</math>|{{EquationRef|15}}}}\n\nfor ''i'' = 1, ..., ''n''-1. The relations ({{EquationNote|15}}) are {{math|''n'' − 1}} linear equations for the {{math|''n'' + 1}} values {{math|''k''<sub>0</sub>, ''k''<sub>1</sub>, ..., ''k<sub>n</sub>''}}.\n\nFor the elastic rulers being the model for the spline interpolation one has that to the left of the left-most \"knot\" and to the right of the right-most \"knot\" the ruler can move freely and will therefore take the form of a straight line with {{math|''q′′'' {{=}} 0}}. As {{mvar|q′′}} should be a continuous function of {{mvar|x}} one gets that for \"Natural Splines\" one in addition to the {{math|''n'' − 1}} linear equations ({{EquationNote|15}}) should have that\n:<math>q''_1(x_0) =2 \\frac {3(y_1 - y_0)-(k_1+2k_0)(x_1-x_0)}{{(x_1-x_0)}^2}=0,</math>\n:<math>q''_n(x_n) =-2 \\frac {3(y_n - y_{n-1})-(2k_n+k_{n-1})(x_n-x_{n-1})}{{(x_n-x_{n-1})}^2}=0,</math>\ni.e. that\n{{NumBlk|:|<math>\\frac{2}{x_1-x_0} k_0 +\\frac{1}{x_1-x_0}k_1 = 3 \\frac{y_1-y_0}{(x_1-x_0)^2},</math>|{{EquationRef|16}}}}\n{{NumBlk|:|<math>\\frac{1}{x_n-x_{n-1}}k_{n-1} +\\frac{2}{x_n-x_{n-1}}k_n = 3 \\frac{y_n-y_{n-1}}{(x_n-x_{n-1})^2}.</math>|{{EquationRef|17}}}}\n\nEventually, ({{EquationNote|15}}) together with ({{EquationNote|16}}) and ({{EquationNote|17}}) constitute {{math|''n'' + 1}} linear equations that uniquely define the {{math|''n'' + 1}} parameters {{math|''k''<sub>0</sub>, ''k''<sub>1</sub>, ..., ''k<sub>n</sub>''}}.\n\nThere exist other end conditions: \"Clamped spline\", that specifies the slope at the ends of the spline, and the popular \"not-a-knot spline\", that requires that the third derivative is also continuous at the {{math|''x''<sub>1</sub>}} and {{math|''x''<sub>''N''−1</sub>}} points.\nFor the \"not-a-knot\" spline, the additional equations will read:\n\n:<math>q'''_1(x_1) =q'''_2(x_1) \\Rightarrow \\frac{1}{\\Delta x_1^2} k_0 + \\left( \\frac{1}{\\Delta x_1^2} - \\frac{1}{\\Delta x_2^2} \\right) k_1 - \\frac{1}{\\Delta x_2^2} k_2 = 2 \\left( \\frac{\\Delta y_1}{\\Delta x_1^3} - \\frac{\\Delta y_2}{\\Delta x_2^3} \\right)</math>\n:<math>q'''_{n-1}(x_{n-1}) =q'''_n(x_{n-1}) \\Rightarrow \\frac{1}{\\Delta x_{n-1}^2} k_{n-2} + \\left( \\frac{1}{\\Delta x_{n-1}^2} - \\frac{1}{\\Delta x_n^2} \\right) k_{n-1} - \\frac{1}{\\Delta x_n^2} k_n = 2\\left( \\frac{\\Delta y_{n-1} }{\\Delta x_{n-1}^3 }- \\frac{ \\Delta y_n}{ \\Delta x_n^3 } \\right) </math>\n\nwhere <math> \\Delta x_i = x_i - x_{i-1}, \\Delta y_i = y_i - y_{i-1} </math>.\n\n==Example==\n\n[[Image:Cubic splines three points.svg|frame|right|Figure 2: Interpolation with cubic \"natural\" splines between three points.]]\nIn case of three points the values for <math>k_0,k_1,k_2</math> are found by solving the [[Tridiagonal matrix|tridiagonal linear equation system]]\n:<math>\n\\begin{bmatrix}\na_{11} & a_{12} & 0       \\\\\na_{21} & a_{22} & a_{23}  \\\\\n0      & a_{32} & a_{33}  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nk_0 \\\\\nk_1 \\\\\nk_2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\nb_3 \\\\\n\\end{bmatrix}\n</math>\nwith\n:<math>a_{11}=\\frac{2}{x_1-x_0}</math>\n:<math>a_{12}=\\frac{1}{x_1-x_0}</math>\n:<math>a_{21}=\\frac{1}{x_1-x_0}</math>\n:<math>a_{22}=2\\ \\left(\\frac {1}{x_1-x_0}+ \\frac {1}{{x_2-x_1}}\\right)</math>\n:<math>a_{23}=\\frac {1}{{x_2-x_1}}</math>\n:<math>a_{32}=\\frac{1}{x_2-x_1}</math>\n:<math>a_{33}=\\frac{2}{x_2-x_1}</math>\n:<math>b_1=3\\ \\frac{y_1-y_0}{(x_1-x_0)^2}</math>\n:<math>b_2=3\\ \\left(\\frac {y_1 - y_0}{{(x_1-x_0)}^2}+\\frac {y_2 - y_1}{{(x_2-x_1)}^2}\\right)</math>\n:<math>b_3=3\\ \\frac{y_2-y_1}{(x_2-x_1)^2}</math>\n\nFor the three points\n:<math>(-1,0.5)\\ ,\\ (0,0)\\ ,\\ (3,3)</math>,\none gets that\n:<math>k_0=-0.6875\\ ,\\ k_1=-0.1250\\ ,\\ k_2=1.5625</math>\nand from ({{EquationNote|10}}) and ({{EquationNote|11}}) that\n:<math>a_1= k_0(x_1-x_0)-(y_1 - y_0)=-0.1875</math>\n:<math>b_1=-k_1(x_1-x_0)+(y_1 - y_0)=-0.3750</math>\n:<math>a_2= k_1(x_2-x_1)-(y_2 - y_1)=-3.3750</math>\n:<math>b_2=-k_2(x_2-x_1)+(y_2 - y_1)=-1.6875</math>\n\nIn Figure 2, the spline function consisting of the two cubic polynomials <math>q_1(x)</math> and <math>q_2(x)</math> given by ({{EquationNote|9}}) is displayed.\n\n==See also==\n*[[Cubic Hermite spline]]\n*[[Centripetal Catmull–Rom spline]]\n*[[Discrete spline interpolation]]\n*[[Monotone cubic interpolation]]\n*[[NURBS]]\n*[[Multivariate interpolation]]\n*[[Polynomial interpolation]]\n*[[Smoothing spline]]\n*[[Spline wavelet]]\n*[[Thin plate spline]]\n*[[Polyharmonic spline]]\n\n==Computer code==\n[https://github.com/msteinbeck/tinyspline TinySpline: Open source C-library for splines which implements cubic spline interpolation]\n\n[https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html#id5 SciPy Spline Interpolation: a Python package that implements interpolation]\n\n[https://github.com/ValexCorp/Cubic-Interpolation Cubic Interpolation: Open source C#-library for cubic spline interpolation by Vadim A. Onuchin, Valex Corp.]\n\n==References==\n{{Reflist}}\n*{{cite journal |last=Schoenberg |first=Isaac J. |url=http://www.ams.org/journals/qam/1946-04-01/S0033-569X-1946-15914-5/S0033-569X-1946-15914-5.pdf |title=Contributions to the Problem of Approximation of Equidistant Data by Analytic Functions: Part A.—On the Problem of Smoothing or Graduation. A First Class of Analytic Approximation Formulae |journal=Quarterly of Applied Mathematics |volume=4 |issue=2 |pages=45–99 |year=1946 |ref=harv }}\n*{{cite journal |last=Schoenberg |first=Isaac J. |url=http://www.ams.org/journals/qam/1946-04-02/S0033-569X-1946-16705-2/S0033-569X-1946-16705-2.pdf |title=Contributions to the Problem of Approximation of Equidistant Data by Analytic Functions: Part B.—On the Problem of Osculatory Interpolation. A Second Class of Analytic Approximation Formulae |journal=Quarterly of Applied Mathematics |volume=4 |issue=2 |pages=112–141 |year=1946 |ref=harv }}\n\n==External links==\n* [http://tools.timodenk.com/?p=cubic-spline-interpolation Cubic Spline Interpolation Online Calculation and Visualization Tool (with JavaScript source code)]\n* {{springer|title=Spline interpolation|id=p/s086820}}\n* [http://jsxgraph.uni-bayreuth.de/wiki/index.php/Cubic_spline_interpolation Dynamic cubic splines with JSXGraph]\n* [https://www.youtube.com/view_play_list?p=DAB608CD1A9A0D55 Lectures on the theory and practice of spline interpolation]\n* [https://web.archive.org/web/20090408054627/http://online.redwoods.cc.ca.us/instruct/darnold/laproj/Fall98/SkyMeg/Proj.PDF Paper which explains step by step how cubic spline interpolation is done, but only for equidistant knots.]\n* [http://apps.nrbook.com/c/index.html Numerical Recipes in C, Go to Chapter 3 Section 3-3]\n* [http://www.cs.tau.ac.il/~turkel/notes/numeng/spline_note.pdf A note on cubic splines]\n* [https://websites.pmc.ucsc.edu/~fnimmo/eart290c_17/NumericalRecipesinF77.pdf Information about spline interpolation (including code in Fortran 77)]\n\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]"
    },
    {
      "title": "Transfinite interpolation",
      "url": "https://en.wikipedia.org/wiki/Transfinite_interpolation",
      "text": "In [[numerical analysis]], '''transfinite interpolation''' is a means to construct [[Function (mathematics)|functions]] over a planar domain in such a way that they match a given function on the boundary. This method is applied in [[geometric model]]ling and in the field of [[finite element method]].<ref name=\"Dyken2009\"/>\n\nThe transfinite interpolation method, first introduced by William J. Gordon and Charles A. Hall,<ref name=\"Hall73\"/> receives its name due to how a function belonging to this class is able to match the primitive function at a nondenumerable number of points.<ref name=\"Gordon82\"/>\nIn the authors' words:\n{{centered pull quote| We use the term ‘transfinite’ to describe the general class of interpolation schemes studied herein since, unlike the classical methods of higher dimensional interpolation which match the primitive function F at a finite number of distinct points, these methods match F at a non-denumerable (transfinite) number of points.}}\n\nTransfinite interpolation is similar to the [[Coons patch]], invented in 1967. <ref name=\"coons\">Steven A. Coons, Surfaces for computer-aided design of space forms, Technical Report MAC-TR-41, Project MAC, MIT, June 1967.  </ref>\n\n\n\n== Formula ==\n\nWith parametrized curves <math>\\vec{c}_1(u)</math>, <math>\\vec{c}_3(u)</math> describing one pair of opposite sides of a domain, and\n<math>\\vec{c}_2(v)</math>, <math>\\vec{c}_4(v)</math> describing the other pair. the position of point (u,v) in the domain is\n\n<math>\n\\begin{array}{rcl}\n\\vec{S}(u,v)&=&(1-v)\\vec{c}_1(u)+v\\vec{c}_3(u)+(1-u)\\vec{c}_2(v)+u\\vec{c}_4(v)\\\\\n&& -\n\\left[\n(1-u)(1-v)\\vec{P}_{1,2}+uv\\vec{P}_{3,4}+u(1-v)\\vec{P}_{1,4}+(1-u)v\\vec{P}_{3,2}\n\\right]\n\\end{array}\n</math>\n\nwhere, e.g., <math>\\vec{P}_{1,2}</math> is the point where curves <math>\\vec{c}_1</math> and <math>\\vec{c}_2</math> meet.\n\n== References ==\n<references>\n <ref name=\"Hall73\">{{cite journal \n      | first1 = William\n      | last1 = Gordon\n      | first2 = Charles\n      | last2 = Hall\n      | title = Construction of curvilinear coordinate systems and application to mesh generation\n      | journal = International Journal for Numerical Methods in Engineering\n      | volume = 7\n      | issue = 4\n      | pages = 461–477\n      | year = 1973\n      | doi=10.1002/nme.1620070405\n      }}\n</ref>\n<ref name=\"Gordon82\">{{cite journal\n    | first1 = William\n    | last1 = Gordon\n    | first2 = Linda\n    | last2 = Thiel\n    | title = Transfinite mapping and their application to grid generation\n    | journal = Applied Mathematics and Computation\n    | year =1982\n    | pages =171–233\n    | number = 10\n    | publisher =\n    | url =\n    | doi = 10.1016/0096-3003(82)90191-6\n    | volume=10–11}}\n</ref>\n<ref name=\"Dyken2009\">{{cite journal\n    | first = Christopher\n    | last1 = Dyken\n    | first2 = Michael S.\n    | last2 = Floater\n    | title = Transfinite mean value interpolation\n    | journal = Computer Aided Geometric Design\n    | number = 26\n    | volume = 1\n    | year = 2009\n    | pages = 117–134\n    | doi = 10.1016/j.cagd.2007.12.003| citeseerx = 10.1.1.137.4822\n    }}\n</ref>\n</references>\n\n[[Category:Interpolation]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Variation diminishing property",
      "url": "https://en.wikipedia.org/wiki/Variation_diminishing_property",
      "text": "[[File:Bezier curves.png|thumb|right|Sample curves (red) with their polygons (grey).]]\nIn mathematics, the '''variation diminishing property''' of certain mathematical objects involves diminishing the number of changes in sign (positive to negative or vice versa).\n\n== Variation diminishing property for Bézier curves ==\n\nThe variation diminishing property of [[Bézier curve]]s is that they are smoother than the polygon formed by their control points.  If a line is drawn through the curve, the number of intersections with the curve will be less than or equal to the number of intersections with the control polygon.  In other words, for a Bézier curve ''B'' defined by the control polygon '''P''', the curve will have no more intersection with any plane as that plane has with '''P'''. This may be generalised into higher dimensions.<ref>{{citation |chapter=Variation-diminishing Property |page=298 |title=Pythagorean-[[Hodograph]] Curves: Algebra and Geometry Inseparable |author=Rida T. Farouki |publisher=Springer |year=2007 |isbn=9783540733973}}</ref>\n\nThis property was first studied by [[Isaac Jacob Schoenberg]] in his 1930 paper, ''Über variationsvermindernde lineare Transformationen''.  He went on to derive it by a transformation of [[Descartes' rule of signs]].<ref>{{citation |page=62 |author=T. N. T. Goodman |title=Shape Preserving Representations in Computer-Aided Geometric Design |year=1999 |isbn=9781560726913 |chapter=Shape properties of normalized totally positive bases}}</ref>\n\n===Proof===\n\nThe proof uses the process of repeated degree elevation of [[Bézier curve#Repeated Degree Elevation|Bézier curve]]. The process of degree elevation for [[Bézier curve#Degree elevation|Bézier curve]]s can be considered an instance of piecewise [[linear interpolation]]. Piecewise linear interpolation can be shown to be variation diminishing.<ref>{{Cite book\n|title=Curves and surfaces for computer-aided geometric design\n|first=Gerald\n|last=Farin\n|publisher=[[Elsevier]] Science & Technology Books\n|year=1997\n|isbn=978-0-12-249054-5\n|edition=4\n|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}\n}}</ref>\nThus, if '''R'''<sub>1</sub>,'''R'''<sub>2</sub>,'''R'''<sub>3</sub> and so on denote the set of polygons obtained by the degree elevation of the initial control polygon '''R''', then it can be shown that\n* <math>\\mathbf{\\lim_{r \\to \\infty}R_r} = \\mathbf{B}</math>\n* Each '''R'''<sub>r</sub> has fewer intersections with a given plane than '''R'''<sub>r-1</sub> (since degree elevation is a form of linear interpolation which can be shown to follow the variation diminishing property)\n\nUsing the above points, we say that since the Bézier curve ''B'' is the limit of these polygons as ''r'' goes to <math>\\infty</math>, it will have fewer intersections with a given plane than '''R'''<sub>i</sub> for all ''i'', and in particular fewer intersections that the original control polygon '''R'''. This is the statement of the variation diminishing property.\n\n== Totally positive matrices ==\n\n{{expand section|date=August 2012}}\n\nThe variation diminishing property of [[totally positive matrix|totally positive matrices]] is a consequence of their decomposition into products of [[Jacobi operator|Jacobi matrices]].\n\nThe existence of the decomposition follows from the [[Gauss–Jordan triangulation]] algorithm.  It follows that we need only prove the VD property for a Jacobi matrix.\n\nThe blocks of [[Dirichlet-to-Neumann map]]s of [[planar graph]]s have the variation diminishing property.\n\n==References==\n{{reflist}}\n\n[[Category:Curves]]\n[[Category:Graphic design]]\n[[Category:Interpolation]]\n[[Category:Splines (mathematics)]]\n[[Category:Matrices]]"
    },
    {
      "title": "Bézier surface",
      "url": "https://en.wikipedia.org/wiki/B%C3%A9zier_surface",
      "text": "{{no footnotes|date=March 2013}}\n'''Bézier surfaces''' are a species of [[spline (mathematics)|mathematical spline]] used in [[computer graphics]], [[computer-aided design]], and [[finite element]] modeling. \nAs with the [[Bézier curve]], a Bézier surface is defined by a set of control points. Similar to interpolation in many respects, a key difference is that the surface does not, in general, pass through the central control points; rather, it  is \"stretched\" toward them as though each were an attractive force. They are  visually intuitive, and for many applications, mathematically convenient.\n\n==History==\n\nBézier surfaces were first described in 1962 by the [[France|French]] engineer [[Pierre Bézier]] who used them to design [[automobile]] bodies.  Bézier surfaces can be of any degree, but bicubic Bézier surfaces generally provide enough [[degrees of freedom (physics and chemistry)|degrees of freedom]] for most applications.\n\n==Equation==\n[[File:Bézier_surface_example.svg|300px|thumb|right|Sample Bézier surface; red - control points, blue - control grid, black - surface approximation]]\nA given Bézier surface of degree (''n'',&nbsp;''m'') is defined by a set of (''n''&nbsp;+&nbsp;1)(''m''&nbsp;+&nbsp;1) [[Control point (mathematics)|control point]]s '''k'''<sub>''i,j''</sub>. It maps the [[unit square]] into a smooth-continuous surface embedded within a space of the same [[dimensionality]] as { '''k'''<sub>''i,j''</sub> }. For example, if '''k''' are all points in a four-dimensional space, then the surface will be within a four-dimensional space.\n\nA two-dimensional Bézier surface can be defined as a [[parametric surface]] where the position of a point '''p''' as a function of the parametric coordinates ''u'', ''v'' is given by: <ref> {{cite book |first=Gerald |last=Farin |title=Curves and Surfaces for CAGD |edition=5th |publisher=Academic Press |location= |isbn=1-55860-737-4 }} </ref>\n\n:<math>\\mathbf{p}(u, v) = \n     \\sum_{i=0}^n \\sum_{j=0}^m \n     B_i^n(u) \\; B_j^m(v) \\; \\mathbf{k}_{i,j}\n</math>\n\nevaluated over the [[unit square]], where\n\n:<math>\n B_i^n(u) = {n \\choose i} \\; u^i (1-u)^{n-i}\n</math>\n\nis a [[Bernstein polynomial]], and \n\n:<math> {n \\choose i} = \\frac{n!}{i! (n-i)!} </math> \n\nis the [[binomial coefficient]].\n\nSome properties of Bézier surfaces:\n*  A Bézier surface will transform in the same way as its control points under all [[linear transformation]]s and [[translation]]s.\n* All ''u'' = constant and ''v'' = constant lines in the (''u'',&nbsp;''v'') space, and, in particular, all four edges of the deformed (''u'',&nbsp;''v'') unit square are Bézier curves.\n* A Bézier surface will lie completely within the [[convex hull]] of its control points, and therefore also completely within the [[bounding box]] of its control points in any given [[Cartesian coordinate system]].\n* The points in the patch corresponding to the corners of the deformed unit square coincide with four of the control points.\n* However, a Bézier surface does not generally pass through its other control points.\n\nGenerally, the most common use of Bézier surfaces is as nets of '''bicubic patches''' (where ''m'' = ''n'' = 3). The geometry of a single bicubic patch is thus completely defined by a set of 16 control points. These are typically linked up to form a [[B-spline surface]] in a similar way as Bézier curves are linked up to form a [[B-spline]] curve.\n\nSimpler Bézier surfaces are formed from '''biquadratic patches''' (''m'' = ''n'' = 2), or [[Bézier triangle]]s.\n\n==Bézier surfaces in computer graphics==\n\n[[Image:Bicubic_Patches.png|right|thumb|250px|[[Ed Catmull]]'s \"Gumbo\" model, composed from patches]]\n\nBézier patch meshes are superior to triangle meshes as a representation of smooth surfaces. They require fewer points (and thus less memory) to represent curved surfaces, are easier to manipulate, and have much better [[continuous function|continuity]] properties. In addition, other common parametric surfaces such as [[sphere]]s and [[cylinder (geometry)|cylinder]]s can be well approximated by relatively small numbers of cubic Bézier patches. \n\nHowever, Bézier patch meshes are difficult to render directly. One problem with Bézier patches is that calculating their intersections with lines is difficult, making them awkward for pure [[Ray tracing (graphics)|ray tracing]] or other direct geometric techniques which do not use subdivision or successive approximation techniques.\nThey are also difficult to combine directly with perspective projection algorithms.\n\nFor this reason, Bézier patch meshes are in general eventually decomposed into meshes of flat triangles by 3D [[rendering pipeline]]s. In high-quality rendering, the subdivision is adjusted to be so fine that the individual triangle boundaries cannot be seen. To avoid a \"blobby\" look, fine detail is usually applied to Bézier surfaces at this stage using [[texture map]]s, [[bump map]]s and other [[pixel shader]] techniques.\n\n<!-- The following two paragraphs are probably correct, can someone verify? -->\nA Bézier patch of degree (''m'', ''n'') may be constructed out of two [[Bézier triangle]]s of degree m+n, or out of a single Bézier triangle of degree ''m''&nbsp;+&nbsp;''n'', with the input domain as a [[square (geometry)|square]] instead of as a [[triangle]].\n\nA Bézier triangle of degree ''m'' may also be constructed out of a Bézier surface of degree (''m'', ''m''), with the control points so that one edge is squashed to a point, or with the input domain as a triangle instead of as a square.\n\n== See also ==\n* [[NURBS]]\n* [[Computational geometry]]\n* [[Bicubic interpolation]]\n* [[Bézier curve]]\n* [[Bézier triangle]]\n\n== Bibliography ==\n{{Reflist}}\n\n[[Category:Surfaces]]\n[[Category:Multivariate interpolation]]\n\n{{DEFAULTSORT:Bezier Surface}}"
    },
    {
      "title": "Bézier triangle",
      "url": "https://en.wikipedia.org/wiki/B%C3%A9zier_triangle",
      "text": "A '''Bézier triangle''' is a special type of [[Bézier surface]], which is created by ([[Linearity|linear]], [[Square (algebra)|quadratic]], [[Cube (algebra)|cubic]] or higher degree) interpolation of control points.\n\n==''n''th-order Bézier triangle==\nA general ''n''th-order Bézier triangle has (''n''&nbsp;+&nbsp;1)(''n''&nbsp;+&nbsp;2)/2 [[Control point (mathematics)|control points]] ''a''<sup>&nbsp;''i''</sup>&nbsp;''β''<sup>&nbsp;''j''</sup>&nbsp;''γ''<sup>&nbsp;''k''</sup> where ''i'',&nbsp;''j'',&nbsp;''k'' are nonnegative integers such that ''i''&nbsp;+&nbsp;''j''&nbsp;+&nbsp;''k''&nbsp;=&nbsp;''n'' <ref name=\":0\" />. The surface is then defined as\n\n: <math>\n(\\alpha s + \\beta t + \\gamma u)^n \n= \\sum_{\\begin{smallmatrix} i+j+k=n \\\\ i,j,k \\ge 0\\end{smallmatrix}} {n \\choose i\\ j\\ k } s^i t^j u^k \\alpha^i \\beta^j \\gamma^k \n= \\sum_{\\begin{smallmatrix} i+j+k=n \\\\ i,j,k \\ge 0\\end{smallmatrix}} \\frac{n!}{i!j!k!} s^i t^j u^k \\alpha^i \\beta^j \\gamma^k \n</math>\n\nfor all nonnegative real numbers ''s''&nbsp;+&nbsp;''t''&nbsp;+&nbsp;''u''&nbsp;=&nbsp;1.\n\nWith [[Linearity|linear]] order (<math display=\"inline\">n=1</math>), the resulting Bézier triangle is actually a regular flat [[triangle]], with the triangle vertices equaling the three control points. A [[Square (algebra)|quadratic]] (<math display=\"inline\">n=2</math>) Bézier triangle features 6 control points which are all located on the edges. The [[Cube (algebra)|cubic]] (<math display=\"inline\">n=3</math>) Bézier triangle is defined by 10 control points and is the lowest order Bézier triangle that has an internal control point, not located on the edges. In all cases, the edges of the triangle will be Bézier curves of the same degree.\n\n==Cubic Bézier triangle==\n[[File:Bezier triangle.png|thumb|180px|An example Bézier triangle with control points marked]]\n\nA '''cubic Bézier triangle''' is a [[Surface (mathematics)|surface]] with the equation\n\n:<math>\\begin{align}\np(s, t, u) = (\\alpha s+\\beta t+\\gamma u)^3 =&\n\\beta^3\\ t^3 + 3\\ \\alpha\\beta^2\\ st^2 + 3\\ \\beta^2\\gamma\\ t^2 u + \\\\\n&3\\ \\alpha^2\\beta\\ s^2 t + 6\\ \\alpha\\beta\\gamma\\ stu + 3\\ \\beta\\gamma^2\\ tu^2 + \\\\\n&\\alpha^3\\ s^3+ 3\\ \\alpha^2\\gamma\\ s^2 u + 3\\ \\alpha\\gamma^2\\ su^2 + \\gamma^3\\ u^3\n\\end{align}</math>\n\nwhere α<sup>3</sup>, β<sup>3</sup>, γ<sup>3</sup>, α<sup>2</sup>β, αβ<sup>2</sup>, β<sup>2</sup>γ, βγ<sup>2</sup>, αγ<sup>2</sup>, α<sup>2</sup>γ and αβγ are the control points of the triangle and s, t, u (with 0 ≤ s, t, u ≤ 1 and s+t+u=1) the [[Barycentric coordinates (mathematics)|barycentric coordinates]] inside the triangle.<ref>{{citation|url=http://www.math.ubc.ca/~cass/courses/m308-03b/projects-03b/drader/main.htm|title=3D Surface Rendering in Postscript}}</ref>\n<ref name=\":0\">{{citation\n|title=Curves and surfaces for computer-aided geometric design\n|first=Gerald\n|last=Farin\n|publisher=[[Academic Press]] Science & Technology Books\n|year=2002\n|isbn=978-1-55860-737-8\n|edition=5\n}}</ref>\n\nAlternatively, a cubic Bézier triangle can be expressed as a more generalized formulation as\n\n:<math>\\begin{align}\np(s, t, u) &= \\sum_{\\begin{smallmatrix} i+j+k=3 \\\\ i,j,k \\ge 0\\end{smallmatrix}} {3 \\choose i\\ j\\ k } s^i t^j u^k \\alpha^i \\beta^j \\gamma^k \n= \\sum_{\\begin{smallmatrix} i+j+k=3 \\\\ i,j,k \\ge 0\\end{smallmatrix}} \\frac{6}{i!j!k!} s^i t^j u^k \\alpha^i \\beta^j \\gamma^k\n\\end{align}</math>\n\nin accordance with the formulation of the a {{Section link|Bézier triangle|nth-order Bézier triangle|nopage=y}}.    \n\nThe corners of the triangle are the points α<sup>3</sup>, β<sup>3</sup> and γ<sup>3</sup>. The edges of the triangle are themselves [[Bézier curve]]s, with the same control points as the Bézier triangle.\n\nBy removing the γu term, a regular Bézier curve results. Also, while not very useful for display on a physical computer screen, by adding extra terms, a Bézier [[tetrahedron]] or Bézier [[polytope]] results.\n\nDue to the nature of the equation, the entire triangle will be contained within the volume surrounded by the control points, and [[affine transformation]]s of the control points will correctly transform the whole triangle in the same way.\n\n===Halving a cubic Bézier triangle===\nAn advantage of Bézier triangles in computer graphics is that dividing the Bézier triangle into two separate Bézier triangles requires only addition and division by two, rather than [[floating point]] arithmetic. This means that while Bézier triangles are smooth, they can easily be approximated using regular triangles by [[recursion|recursively]] dividing the triangle in two until the resulting triangles are considered sufficiently small. \n\nThe following computes the new control points for the half of the full Bézier triangle with the corner α<sup>3</sup>, a corner halfway along the Bézier curve between α<sup>3</sup> and β<sup>3</sup>, and the third corner γ<sup>3</sup>.\n:<math>\n\\begin{pmatrix}\n\\boldsymbol{\\alpha^3}{'}\\\\\n\\boldsymbol{\\alpha^2\\beta}{'}\\\\\n\\boldsymbol{\\alpha\\beta^2}{'}\\\\\n\\boldsymbol{\\beta^3}{'}\\\\\n\\boldsymbol{\\alpha^2\\gamma}{'}\\\\\n\\boldsymbol{\\alpha\\beta\\gamma}{'}\\\\\n\\boldsymbol{\\beta^2\\gamma}{'}\\\\\n\\boldsymbol{\\alpha\\gamma^2}{'}\\\\\n\\boldsymbol{\\beta\\gamma^2}{'}\\\\\n\\boldsymbol{\\gamma^3}{'} \n\\end{pmatrix}=\\begin{pmatrix}\n1&0&0&0&0&0&0&0&0&0\\\\\n{1\\over 2}&{1\\over 2}&0&0&0&0&0&0&0&0\\\\\n{1\\over 4}&{2\\over 4}&{1\\over 4}&0&0&0&0&0&0&0\\\\\n{1\\over 8}&{3\\over 8}&{3\\over 8}&{1\\over 8}&0&0&0&0&0&0\\\\\n0&0&0&0&1&0&0&0&0&0\\\\\n0&0&0&0&{1\\over 2}&{1\\over 2}&0&0&0&0\\\\\n0&0&0&0&{1\\over 4}&{2\\over 4}&{1\\over 4}&0&0&0\\\\\n0&0&0&0&0&0&0&1&0&0\\\\\n0&0&0&0&0&0&0&{1\\over 2}&{1\\over 2}&0\\\\\n0&0&0&0&0&0&0&0&0&1\n\\end{pmatrix}\\cdot\\begin{pmatrix}\n\\boldsymbol{\\alpha^3}\\\\\n\\boldsymbol{\\alpha^2\\beta}\\\\\n\\boldsymbol{\\alpha\\beta^2}\\\\\n\\boldsymbol{\\beta^3}\\\\\n\\boldsymbol{\\alpha^2\\gamma}\\\\\n\\boldsymbol{\\alpha\\beta\\gamma}\\\\\n\\boldsymbol{\\beta^2\\gamma}\\\\\n\\boldsymbol{\\alpha\\gamma^2}\\\\\n\\boldsymbol{\\beta\\gamma^2}\\\\\n\\boldsymbol{\\gamma^3}\n\\end{pmatrix}</math>\n:equivalently, using addition and division by two only,\n:{|\n|-----\n| align=\"center\" |\n<math>\n\\begin{matrix}\n&& \\beta^3 :=(\\alpha\\beta^2 + \\beta^3)/2 \\\\\n& \\alpha\\beta^2 :=(\\alpha^2\\beta + \\alpha\\beta^2)/2 & \\beta^3 :=(\\alpha\\beta^2 + \\beta^3)/2 \\\\\n\\alpha^2\\beta :=(\\alpha^3 + \\alpha^2\\beta)/2 & \\alpha\\beta^2 :=(\\alpha^2\\beta + \\alpha\\beta^2)/2 & \\beta^3 :=(\\alpha\\beta^2 + \\beta^3)/2 \\\\\n\\end{matrix}\n</math>\n|-----\n| align=\"center\" |\n<math>\n\\begin{matrix}\n& \\beta^2\\gamma := (\\alpha\\beta\\gamma + \\beta^2\\gamma)/2 \\\\\n\\alpha\\beta\\gamma :=(\\alpha^2\\gamma + \\alpha\\beta\\gamma)/2 & \\beta^2\\gamma :=(\\alpha\\beta\\gamma + \\beta^2\\gamma)/2 \\\\\n\\end{matrix}\n</math>\n|-----\n| align=\"center\" |\n<math>\n\\beta\\gamma^2 :=(\\alpha\\gamma^2 + \\beta\\gamma^2)/2\n</math>\n|}\n:where := means to replace the vector on the left with the vector on the right.\n:Note that halving a bézier triangle is similar to halving Bézier curves of all orders up to the order of the Bézier triangle.\n\n==See also==\n* [[Bézier curve]]\n* [[Bézier surface]] (biquadratic patches are Bézier rectangles)\n* [[Surface (mathematics)|Surface]]\n\n== References ==\n{{reflist}}\n\n==External links==\n* [http://www.graphicshardware.org/previous/www_1998/presentations/bruijns/index.htm Quadratic Bézier Triangles As Drawing Primitives] Contains more info on planar and quadratic Bézier triangles.\n* [http://othes.univie.ac.at/11497/ Paper about the use of cubic Bézier patches in raytracing (German)]\n* {{cite web | title = Ray Tracing Triangular Bézier Patches | citeseerx = 10.1.1.18.5646 }}\n* {{cite web | title = Triangular Bézier Clipping | citeseerx = 10.1.1.62.8062 }}\n* [http://alex.vlachos.com/graphics/CurvedPNTriangles.pdf Curved PN triangles (a special kind of cubic Bézier triangles)]\n* [https://www.researchgate.net/publication/257406710_Shape_aware_normal_interpolation_for_curved_surface_shading_from_polyhedral_approximation Shape aware normal interpolation for curved surface shading from polyhedral approximation ]\n* [https://web.archive.org/web/20110103235512/http://www.mpi-inf.mpg.de/~mschwarz/papers/pscurvedtris-sig06.pdf Pixel-Shader-Based Curved Triangles]\n* {{cite web | title = Surface Construction with Near Least Square Acceleration based on Vertex Normals on Triangular Meshes | citeseerx = 10.1.1.6.2521 }}\n\n{{DEFAULTSORT:Bezier Triangle}}\n[[Category:Surfaces]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Bicubic interpolation",
      "url": "https://en.wikipedia.org/wiki/Bicubic_interpolation",
      "text": "{{comparison_of_1D_and_2D_interpolation.svg}}\nIn [[mathematics]], '''bicubic interpolation''' is an extension of [[cubic interpolation]] for [[interpolation|interpolating]] data points on a [[two-dimensional]] [[regular grid]]. The interpolated surface is [[Smooth function|smoother]] than corresponding surfaces obtained by [[bilinear interpolation]] or [[nearest-neighbor interpolation]]. Bicubic interpolation can be accomplished using either [[Lagrange polynomial]]s, [[cubic spline]]s, or [[#Bicubic convolution algorithm|cubic convolution]] algorithm.\n\nIn [[image processing]], bicubic interpolation is often chosen over bilinear or nearest-neighbor interpolation in [[resampling (bitmap)|image resampling]], when speed is not an issue. In contrast to bilinear interpolation, which only takes 4 [[pixel]]s (2×2) into account, bicubic interpolation considers 16 pixels (4×4). Images resampled with bicubic interpolation are smoother and have fewer interpolation [[Spatial anti-aliasing|artifacts]].\n\n==Computation==\n\n[[Image:Interpolation-bicubic.svg|thumb|right|Bicubic interpolation on the square <math>[0,4] \\times [0,4]</math> consisting of 25 unit squares patched together. Bicubic interpolation as per [[Matplotlib]]'s implementation. Colour indicates function value. The black dots are the locations of the prescribed data being interpolated. Note how the color samples are not radially symmetric.]]\n[[Image:Interpolation-bilinear.svg|thumb|right|[[Bilinear interpolation]] on the same dataset as above. Derivatives of the surface are not continuous over the square boundaries.]]\n[[Image:Interpolation-nearest.svg|thumb|right|[[Nearest-neighbor interpolation]] on the same dataset as above.]]\n\nSuppose the function values <math>f</math> and the derivatives <math>f_x</math>, <math>f_y</math> and <math>f_{xy}</math> are known at the four corners <math>(0,0)</math>, <math>(1,0)</math>, <math>(0,1)</math>, and <math>(1,1)</math> of the unit square. The interpolated surface can then be written as\n:<math>p(x,y) = \\sum\\limits_{i=0}^3 \\sum_{j=0}^3 a_{ij} x^i y^j.</math>\n\nThe interpolation problem consists of determining the 16 coefficients <math>a_{ij}</math>.\nMatching <math>p(x,y)</math> with the function values yields four equations:\n# <math>f(0,0)      = p(0,0)   = a_{00},</math>\n# <math>f(1,0)      = p(1,0)   = a_{00} + a_{10} + a_{20} + a_{30},</math>\n# <math>f(0,1)      = p(0,1)   = a_{00} + a_{01} + a_{02} + a_{03},</math>\n# <math>f(1,1)      = p(1,1)   = \\textstyle \\sum\\limits_{i=0}^3 \\sum\\limits_{j=0}^3 a_{ij}.</math>\n\nLikewise, eight equations for the derivatives in the <math>x</math> and the <math>y</math> directions:\n# <math>f_x(0,0)    = p_x(0,0) = a_{10},</math>\n# <math>f_x(1,0)    = p_x(1,0) =  a_{10} + 2a_{20} + 3a_{30},</math>\n# <math>f_x(0,1)    = p_x(0,1) = a_{10} + a_{11} + a_{12} + a_{13},</math>\n# <math>f_x(1,1)    = p_x(1,1) = \\textstyle \\sum\\limits_{i=1}^3 \\sum\\limits_{j=0}^3 a_{ij} i,</math>\n# <math>f_y(0,0)    = p_y(0,0) = a_{01},</math>\n# <math>f_y(1,0)    = p_y(1,0) = a_{01} + a_{11} + a_{21} + a_{31},</math>\n# <math>f_y(0,1)    = p_y(0,1) = a_{01} + 2a_{02} + 3a_{03},</math>\n# <math>f_y(1,1)    = p_y(1,1) = \\textstyle \\sum\\limits_{i=0}^3 \\sum\\limits_{j=1}^3 a_{ij} j.</math>\n\nAnd four equations for the <math>xy</math> [[mixed partial derivative]]:\n# <math>f_{xy}(0,0) = p_{xy}(0,0) = a_{11},</math>\n# <math>f_{xy}(1,0) = p_{xy}(1,0) = a_{11} + 2a_{21} + 3a_{31},</math>\n# <math>f_{xy}(0,1) = p_{xy}(0,1) = a_{11} + 2a_{12} + 3a_{13},</math>\n# <math>f_{xy}(1,1) = p_{xy}(1,1) = \\textstyle \\sum\\limits_{i=1}^3 \\sum\\limits_{j=1}^3 a_{ij} i j.</math>\n\nThe expressions above have used the following identities:\n:<math>p_x(x,y) = \\textstyle \\sum\\limits_{i=1}^3 \\sum\\limits_{j=0}^3 a_{ij} i x^{i-1} y^j,</math>\n:<math>p_y(x,y) = \\textstyle \\sum\\limits_{i=0}^3 \\sum\\limits_{j=1}^3 a_{ij} x^i j y^{j-1},</math>\n:<math>p_{xy}(x,y) = \\textstyle \\sum\\limits_{i=1}^3 \\sum\\limits_{j=1}^3 a_{ij} i x^{i-1} j y^{j-1}.</math>\n\nThis procedure yields a surface <math>p(x,y)</math> on the [[unit square]] <math>[0,1] \\times [0,1]</math> that is continuous and has continuous derivatives. Bicubic interpolation on an arbitrarily sized [[regular grid]] can then be accomplished by patching together such bicubic surfaces, ensuring that the derivatives match on the boundaries.\n\nGrouping the unknown parameters <math>a_{ij}</math> in a vector\n:<math>\\alpha=\\left[\\begin{smallmatrix}a_{00}&a_{10}&a_{20}&a_{30}&a_{01}&a_{11}&a_{21}&a_{31}&a_{02}&a_{12}&a_{22}&a_{32}&a_{03}&a_{13}&a_{23}&a_{33}\\end{smallmatrix}\\right]^T</math>\nand letting\n:<math>x=\\left[\\begin{smallmatrix}f(0,0)&f(1,0)&f(0,1)&f(1,1)&f_x(0,0)&f_x(1,0)&f_x(0,1)&f_x(1,1)&f_y(0,0)&f_y(1,0)&f_y(0,1)&f_y(1,1)&f_{xy}(0,0)&f_{xy}(1,0)&f_{xy}(0,1)&f_{xy}(1,1)\\end{smallmatrix}\\right]^T,</math>\nthe above system of equations can be reformulated into a matrix for the linear equation <math>A\\alpha=x</math>.\n\nInverting the matrix gives the more useful linear equation <math>A^{-1}x=\\alpha</math>, where\n:<math>A^{-1}=\\left[\\begin{smallmatrix}\n 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n -3 & 3 & 0 & 0 & -2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n 2 & -2 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -3 & 3 & 0 & 0 & -2 & -1 & 0 & 0 \\\\\n 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & -2 & 0 & 0 & 1 & 1 & 0 & 0 \\\\\n -3 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & -2 & 0 & -1 & 0 & 0 & 0 & 0 & 0 \\\\\n 0 & 0 & 0 & 0 & -3 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & -2 & 0 & -1 & 0 \\\\\n 9 & -9 & -9 & 9 & 6 & 3 & -6 & -3 & 6 & -6 & 3 & -3 & 4 & 2 & 2 & 1 \\\\\n -6 & 6 & 6 & -6 & -3 & -3 & 3 & 3 & -4 & 4 & -2 & 2 & -2 & -2 & -1 & -1 \\\\\n 2 & 0 & -2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n 0 & 0 & 0 & 0 & 2 & 0 & -2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n -6 & 6 & 6 & -6 & -4 & -2 & 4 & 2 & -3 & 3 & -3 & 3 & -2 & -1 & -2 & -1 \\\\\n 4 & -4 & -4 & 4 & 2 & 2 & -2 & -2 & 2 & -2 & 2 & -2 & 1 & 1 & 1 & 1\n\\end{smallmatrix}\\right],</math>\nwhich allows <math>\\alpha</math> to be calculated quickly and easily.\n\nThere can be another concise matrix form for 16 coefficients:\n:<math>\\begin{bmatrix}\nf(0,0)&f(0,1)&f_y (0,0)&f_y (0,1)\\\\f(1,0)&f(1,1)&f_y (1,0)&f_y (1,1)\\\\f_x (0,0)&f_x (0,1)&f_{xy} (0,0)&f_{xy} (0,1)\\\\f_x (1,0)&f_x (1,1)&f_{xy} (1,0)&f_{xy} (1,1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n1&0&0&0\\\\1&1&1&1\\\\0&1&0&0\\\\0&1&2&3\n\\end{bmatrix}\n\\begin{bmatrix}\na_{00}&a_{01}&a_{02}&a_{03}\\\\a_{10}&a_{11}&a_{12}&a_{13}\\\\a_{20}&a_{21}&a_{22}&a_{23}\\\\a_{30}&a_{31}&a_{32}&a_{33}\n\\end{bmatrix}\n\\begin{bmatrix}\n1&1&0&0\\\\0&1&1&1\\\\0&1&0&2\\\\0&1&0&3\n\\end{bmatrix},</math>\nor\n:<math>\n\\begin{bmatrix}\na_{00}&a_{01}&a_{02}&a_{03}\\\\a_{10}&a_{11}&a_{12}&a_{13}\\\\a_{20}&a_{21}&a_{22}&a_{23}\\\\a_{30}&a_{31}&a_{32}&a_{33}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1&0&0&0\\\\0&0&1&0\\\\-3&3&-2&-1\\\\2&-2&1&1\n\\end{bmatrix}\n\\begin{bmatrix}\nf(0,0)&f(0,1)&f_y (0,0)&f_y (0,1)\\\\f(1,0)&f(1,1)&f_y (1,0)&f_y (1,1)\\\\f_x (0,0)&f_x (0,1)&f_{xy} (0,0)&f_{xy} (0,1)\\\\f_x (1,0)&f_x (1,1)&f_{xy} (1,0)&f_{xy} (1,1)\n\\end{bmatrix}\n\\begin{bmatrix}\n1&0&-3&2\\\\0&0&3&-2\\\\0&1&-2&1\\\\0&0&-1&1\n\\end{bmatrix},\n</math>\nwhere\n:<math>p(x,y)=\\begin{bmatrix}1\n&x&x^2&x^3\\end{bmatrix}\n\\begin{bmatrix}\na_{00}&a_{01}&a_{02}&a_{03}\\\\a_{10}&a_{11}&a_{12}&a_{13}\\\\a_{20}&a_{21}&a_{22}&a_{23}\\\\a_{30}&a_{31}&a_{32}&a_{33}\n\\end{bmatrix}\n\\begin{bmatrix}1\\\\y\\\\y^2\\\\y^3\\end{bmatrix}.</math>\n\n==Finding derivatives from function values==\n\nIf the derivatives are unknown, they are typically approximated from the function values at points neighbouring the corners of the unit square, e.g. using [[finite differences]].\n\nTo find either of the single derivatives, <math>f_x</math> or <math>f_y</math>, using that method, find the slope between the two ''surrounding'' points in the appropriate axis. For example, to calculate <math>f_x</math> for one of the points, find <math>f(x,y)</math> for the points to the left and right of the target point and calculate their slope, and similarly for <math>f_y</math>.\n\nTo find the cross derivative <math>f_{xy}</math>, take the derivative in both axes, one at a time. For example, one can first use the <math>f_x</math> procedure to find the <math>x</math> derivatives of the points above and below the target point, then use the <math>f_y</math> procedure on those values (rather than, as usual, the values of <math>f</math> for those points) to obtain the value of <math>f_{xy}(x,y)</math> for the target point.  (Or one can do it in the opposite direction, first calculating <math>f_y</math> and then <math>f_x</math> from those.  The two give equivalent results.)\n\nAt the edges of the dataset, when one is missing some of the surrounding points, the missing points can be approximated by a number of methods. A simple and common method is to assume that the slope from the existing point to the target point continues without further change, and using this to calculate a hypothetical value for the missing point.\n\n==Bicubic convolution algorithm==\n\nBicubic spline interpolation requires the solution of the linear system described above for each grid cell. An interpolator with similar properties can be obtained by applying a [[convolution]] with the following kernel in both dimensions:\n:<math>W(x) = \n\\begin{cases}\n (a+2)|x|^3-(a+3)|x|^2+1 & \\text{for } |x| \\leq 1, \\\\\n a|x|^3-5a|x|^2+8a|x|-4a & \\text{for } 1 < |x| < 2, \\\\\n 0                       & \\text{otherwise},\n\\end{cases}\n</math>\nwhere <math>a</math> is usually set to −0.5 or −0.75. Note that <math>W(0)=1</math> and <math>W(n)=0</math> for all nonzero integers <math>n</math>.\n\nThis approach was proposed by Keys, who showed that <math>a=-0.5</math> produces third-order convergence with respect to the sampling interval of the original function.<ref name=Keys>{{cite journal\n | author = R. Keys\n | year = 1981\n | title = Cubic convolution interpolation for digital image processing\n | journal = IEEE Transactions on Acoustics, Speech, and Signal Processing\n | doi = 10.1109/TASSP.1981.1163711\n | volume = 29\n | pages = 1153–1160\n | issue = 6\n| citeseerx = 10.1.1.320.776\n }}</ref>\n\nIf we use the matrix notation for the common case <math>a=-0.5</math>, we can express the equation in a more friendly manner:\n:<math>p(t) =\n\\tfrac{1}{2}\n\\begin{bmatrix}\n\n1 & t & t^2 & t^3 \\\\\n\n\\end{bmatrix}\n\\begin{bmatrix}\n\n0 & 2 & 0 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n2 & -5 & 4 & -1 \\\\\n-1 & 3 & -3 & 1 \\\\\n\n\\end{bmatrix}\n\\begin{bmatrix}\n\nf_{-1} \\\\\nf_0 \\\\\nf_1 \\\\\nf_2 \\\\\n\n\\end{bmatrix}\n</math>\nfor <math>t</math> between 0 and 1 for one dimension. Note that for 1-dimensional cubic convolution interpolation 4 sample points are required. For each inquiry two samples are located on its left and two samples on the right. These points are indexed from −1 to 2 in this text. The distance from the point indexed with 0 to the inquiry point is denoted by <math>t</math> here.\n\nFor two dimensions first applied once in <math>x</math> and again in <math>y</math>:\n\n:<math>b_{-1} = p(t_x, f_{(-1,-1)}, f_{(0,-1)}, f_{(1,-1)}, f_{(2,-1)}),</math>\n\n:<math>b_{0} = p(t_x, f_{(-1,0)}, f_{(0,0)}, f_{(1,0)}, f_{(2,0)}),</math>\n\n:<math>b_{1} = p(t_x, f_{(-1,1)}, f_{(0,1)}, f_{(1,1)}, f_{(2,1)}),</math>\n\n:<math>b_{2} = p(t_x, f_{(-1,2)}, f_{(0,2)}, f_{(1,2)}, f_{(2,2)}),</math>\n\n:<math>p(x,y) = p(t_y, b_{-1}, b_{0}, b_{1}, b_{2}).</math>\n\n==Use in computer graphics==\n\n[[Image:Accutance.svg|250px|thumb|The lower half of this figure is a magnification of the upper half, showing how the apparent sharpness of the left-hand line is created. Bicubic interpolation causes overshoot, which increases [[acutance]].]]<!--Don't scale the image, it will greatly reduce the effect-->\n\nThe bicubic algorithm is frequently used for scaling images and video for display (see [[Resampling (bitmap)|bitmap resampling]]). It preserves fine detail better than the common [[bilinear filtering|bilinear]] algorithm.\n\nHowever, due to the negative lobes on the kernel, it causes [[overshoot (signal)|overshoot]] (haloing). This can cause [[Clipping (signal processing)|clipping]], and is an artifact (see also [[ringing artifacts]]), but it increases [[acutance]] (apparent sharpness), and can be desirable.\n\n==See also==\n\n{{portal|Mathematics|Computer graphics}}\n\n* [[Spatial anti-aliasing]]\n* [[Bézier surface]]\n* [[Bilinear interpolation]]\n* [[Cubic Hermite spline]], the one-dimensional analogue of bicubic spline\n* [[Lanczos resampling]]\n* [[Natural neighbor interpolation]]\n* [[Sinc filter]]\n* [[Spline interpolation]]\n* [[Tricubic interpolation]]\n* [[Directional Cubic Convolution Interpolation]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [https://web.archive.org/web/20051024202307/http://www.geovista.psu.edu/sites/geocomp99/Gc99/082/gc_082.htm Application of interpolation to elevation samples]\n* [http://sepwww.stanford.edu/public/docs/sep107/paper_html/node20.html Interpolation theory]\n* [http://www.paulinternet.nl/?page=bicubic Explanation and Java/C++ implementation of (bi)cubic interpolation]\n* [http://mathformeremortals.wordpress.com/2013/01/15/bicubic-interpolation-excel-worksheet-function/ Excel Worksheet Function for Bicubic Lagrange Interpolation]\n\n{{DEFAULTSORT:Bicubic Interpolation}}\n[[Category:Image processing]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Bilinear interpolation",
      "url": "https://en.wikipedia.org/wiki/Bilinear_interpolation",
      "text": "[[Image:BilinearInterpolation.svg|right|thumb|The four red dots show the data points and the green dot is the point at which we want to interpolate.]]\n[[Image:Bilininterp.png|right|thumb|Example of bilinear interpolation on the unit square with the ''z'' values 0, 1, 1 and 0.5 as indicated. Interpolated values in between represented by color.]]\nIn [[mathematics]], '''bilinear interpolation''' is an extension of [[linear interpolation]] for [[interpolation|interpolating]] functions of two variables (e.g., ''x'' and ''y'') on a [[rectilinear grid|rectilinear 2D grid]].\n\nThe key idea is to perform linear interpolation first in one direction, and then again in the other direction. Although each step is linear in the sampled values and in the position, the interpolation as a whole is not linear but rather [[Quadratic function|quadratic]] in the sample location.\n\n==Algorithm==\nSuppose that we want to find the value of the unknown function ''f'' at the point (''x'', ''y''). It is assumed that we know the value of ''f'' at the four points ''Q''<sub>11</sub> = (''x''<sub>1</sub>,&nbsp;''y''<sub>1</sub>), ''Q''<sub>12</sub> = (''x''<sub>1</sub>,&nbsp;''y''<sub>2</sub>), ''Q''<sub>21</sub> = (''x''<sub>2</sub>,&nbsp;''y''<sub>1</sub>), and ''Q''<sub>22</sub> = (''x''<sub>2</sub>,&nbsp;''y''<sub>2</sub>). \n\nWe first do linear interpolation in the ''x''-direction. This yields\n:<math>\\begin{align}\nf(x, y_1) &\\approx \\frac{x_2-x}{x_2-x_1} f(Q_{11}) + \\frac{x-x_1}{x_2-x_1} f(Q_{21}), \\\\\nf(x, y_2) &\\approx \\frac{x_2-x}{x_2-x_1} f(Q_{12}) + \\frac{x-x_1}{x_2-x_1} f(Q_{22}).\n\\end{align}</math>\nWe proceed by interpolating in the ''y'' direction to obtain the desired estimate:\n:<math>\\begin{align}\nf(x,y) &\\approx \\frac{y_2-y}{y_2-y_1} f(x, y_1) + \\frac{y-y_1}{y_2-y_1} f(x, y_2) \\\\\n&= \\frac{y_2-y}{y_2-y_1} \\left ( \\frac{x_2-x}{x_2-x_1} f(Q_{11}) + \\frac{x-x_1}{x_2-x_1} f(Q_{21}) \\right ) + \\frac{y-y_1}{y_2-y_1} \\left ( \\frac{x_2-x}{x_2-x_1} f(Q_{12}) + \\frac{x-x_1}{x_2-x_1} f(Q_{22}) \\right ) \\\\\n&= \\frac{1}{(x_2-x_1)(y_2-y_1)} \\big( f(Q_{11})(x_2-x)(y_2-y) + f(Q_{21})(x-x_1)(y_2-y)+  f(Q_{12})(x_2-x)(y-y_1) + f(Q_{22})(x-x_1)(y-y_1) \\big)\\\\\n&=\\frac{1}{(x_2-x_1)(y_2-y_1)}  \\begin{bmatrix} x_2-x & x-x_1 \\end{bmatrix} \\begin{bmatrix} f(Q_{11}) & f(Q_{12}) \\\\ f(Q_{21})& f(Q_{22}) \\end{bmatrix} \\begin{bmatrix}\ny_2-y \\\\ y-y_1 \\end{bmatrix}.\n\\end{align}</math>\nNote that we will arrive at the same result if the interpolation is done first along the ''y'' direction and then along the ''x'' direction. \n\n===Alternative algorithm===\nAn alternative way to write the solution to the interpolation problem is\n:<math> f(x,y) \\approx a_0 + a_1 x + a_2 y + a_3 xy,</math>\nwhere the coefficients are found by solving the linear system\n:<math>\\begin{align}\n\\begin{bmatrix}\n1 & x_1 & y_1 & x_1 y_1 \\\\\n1 & x_1 & y_2 & x_1 y_2 \\\\\n1 & x_2 & y_1 & x_2 y_1 \\\\\n1 & x_2 & y_2 & x_2 y_2 \n\\end{bmatrix}\\begin{bmatrix}\na_0\\\\a_1\\\\a_2\\\\a_3\n\\end{bmatrix}=\\begin{bmatrix}\nf(Q_{11})\\\\f(Q_{12})\\\\f(Q_{21})\\\\f(Q_{22})\n\\end{bmatrix},\n\\end{align}</math>\n\nyielding the result\n\n:<math> \\ a_0 = \\frac{f(Q_{11}) x_2 y_2}{(x_1 - x_2) (y_1 - y_2)} + \\frac{f(Q_{12}) x_2 y_1}{(x_1 - x_2) (y_2 - y_1)} + \\frac{f(Q_{21}) x_1 y_2}{(x_1 - x_2) (y_2 - y_1)} + \\frac{f(Q_{22}) x_1 y_1}{(x_1 - x_2) (y_1 - y_2)},</math>\n:<math> \\ a_1 = \\frac{f(Q_{11}) y_2}{(x_1 - x_2) (y_2 - y_1)} + \\frac{f(Q_{12}) y_1}{(x_1 - x_2) (y_1 - y_2)} + \\frac{f(Q_{21}) y_2}{(x_1 - x_2) (y_1 - y_2)} + \\frac{f(Q_{22}) y_1}{(x_1 - x_2) (y_2 - y_1)},</math>\n:<math> \\ a_2 = \\frac{f(Q_{11}) x_2}{(x_1 - x_2) (y_2 - y_1)} + \\frac{f(Q_{12}) x_2}{(x_1 - x_2) (y_1 - y_2)} + \\frac{f(Q_{21}) x_1}{(x_1 - x_2) (y_1 - y_2)} + \\frac{f(Q_{22}) x_1}{(x_1 - x_2) (y_2 - y_1)},</math>\n:<math> \\ a_3 = \\frac{f(Q_{11})}{(x_1 - x_2) (y_1 - y_2)} + \\frac{f(Q_{12})}{(x_1 - x_2) (y_2 - y_1)} + \\frac{f(Q_{21})}{(x_1 - x_2) (y_2 - y_1)} + \\frac{f(Q_{22})}{(x_1 - x_2) (y_1 - y_2)}.</math>\n\nIf a solution is preferred in terms of ''f''(''Q''), then we can write\n:<math> f(x,y) \\approx b_{11} f(Q_{11}) + b_{12} f(Q_{12}) + b_{21} f(Q_{21}) + b_{22} f(Q_{22}),</math>\nwhere the coefficients are found by solving\n:<math>\n\\begin{bmatrix}\nb_{11}\\\\b_{12}\\\\b_{21}\\\\b_{22}\n\\end{bmatrix}=\n\\left(\\begin{bmatrix}\n1 & x_1 & y_1 & x_1 y_1 \\\\\n1 & x_1 & y_2 & x_1 y_2 \\\\\n1 & x_2 & y_1 & x_2 y_1 \\\\\n1 & x_2 & y_2 & x_2 y_2\n\\end{bmatrix}^{-1}\\right)^{\\rm T} \\begin{bmatrix}\n1\\\\x\\\\y\\\\xy\n\\end{bmatrix}.\n</math>\n\n===Unit square===\nIf we choose a coordinate system in which the four points where ''f'' is known are (0,&nbsp;0), (0,&nbsp;1), (1,&nbsp;0), and (1,&nbsp;1), then the interpolation formula simplifies to \n:<math> f(x,y) \\approx f(0,0) (1-x)(1-y) + f(1,0) x(1-y) + f(0,1) (1-x)y + f(1,1) xy, </math>\nor equivalently, in matrix operations:\n:<math> f(x,y) \\approx \\begin{bmatrix} 1-x & x \\end{bmatrix} \\begin{bmatrix} f(0,0) & f(0,1) \\\\ f(1,0) & f(1,1) \\end{bmatrix} \\begin{bmatrix}\n1-y \\\\ y \\end{bmatrix}.</math>\n\n[[File:bilinear_interpolation_visualisation.svg|thumb|upright|A geometric visualisation of bilinear interpolation. The product of the value at the desired point and the entire area is equal to the sum of the products of the value at each corner and the partial area diagonally opposite the corner.]]\n\n===Nonlinear===\nAs the name suggests, the bilinear interpolant is ''not'' linear; but it is the product of two [[linear functions]]. For example, the bilinear interpolation derived above is a product of the values of <math>x</math> and  <math>y</math>.\n\nAlternatively, the interpolant on the unit square can be written as\n:<math> f(x,y) = \\sum_{i=0}^1 \\sum_{j=0}^1 a_{ij} x^i y^j = a_{00} + a_{10} x + a_{01} y + a_{11} x y,</math>\nwhere \n:<math> a_{00} = f(0,0),</math>\n:<math> a_{10} = f(1,0)-f(0,0),</math>\n:<math> a_{01} = f(0,1)-f(0,0),</math>\n:<math> a_{11} = f(1,1)+f(0,0)-\\big(f(1,0)+f(0,1)\\big).</math>\n\nIn both cases, the number of constants (four) correspond to the number of data points where ''f'' is given. The interpolant is linear along lines [[Parallel (geometry)|parallel]] to either the ''x'' or the ''y'' direction, equivalently if ''x'' or ''y'' is set constant. Along any other straight line, the interpolant is [[Quadratic function|quadratic]].  However, even if the interpolation is ''not'' linear in the position (''x'' and ''y''), it ''is'' linear in the amplitude, as it is apparent from the equations above: all the coefficients ''a<sub>j</sub>'', ''j'' = 1...4, are proportional to the value of the function ''f''.\n\nThe result of bilinear interpolation is independent of which axis is interpolated first and which second. If we had first performed the linear interpolation in the ''y'' direction and then in the ''x'' direction, the resulting approximation would be the same.\n\nThe obvious extension of bilinear interpolation to three dimensions is called [[trilinear interpolation]].\n\n== Application in image processing ==\n{{comparison_of_1D_and_2D_interpolation.svg}}\nIn [[computer vision]] and [[image processing]], bilinear interpolation is one of the basic [[resampling (bitmap)|resampling]] techniques.\n\nIn [[texture mapping]], it is also known as [[bilinear filtering]] or ''bilinear texture mapping'' and can be used to produce a reasonably realistic image. An algorithm is used to map a screen pixel location to a corresponding point on the texture map. A weighted average of the attributes (color, transparency, etc.) of the four surrounding [[Texel (graphics)|texels]] is computed and applied to the screen pixel. This process is repeated for each pixel forming the object being textured.<ref>[https://www.pcmag.com/encyclopedia_term/0,2542,t=bilinear+interpolation&i=38607,00.asp Bilinear interpolation definition (popular article on www.pcmag.com].</ref>\n\nWhen an image needs to be scaled up, each pixel of the original image needs to be moved in a certain direction based on the scale constant. However, when scaling up an image by a non-integral scale factor, there are pixels (i.e., ''holes'') that are not assigned appropriate pixel values. In this case, those ''holes'' should be assigned appropriate [[RGB]] or [[grayscale]] values so that the output image does not have non-valued pixels.\n\nBilinear interpolation can be used where perfect image transformation with pixel matching is impossible, so that one can calculate and assign appropriate intensity values to pixels. Unlike other interpolation techniques such as [[nearest-neighbor interpolation]] and [[bicubic interpolation]], bilinear interpolation uses values of only the 4 nearest pixels, located in diagonal directions from a given pixel, in order to find the appropriate color intensity values of that pixel.\n\nBilinear interpolation considers the closest 2 × 2 neighborhood of known pixel values surrounding the unknown pixel's computed location. It then takes a weighted average of these 4 pixels to arrive at its final, interpolated value.<ref>[http://www.cambridgeincolour.com/tutorials/image-interpolation.htm \"Web tutorial: Digital Image Interpolation\"].</ref>\n\n[[File:Bilin3.png|thumb|right|alt=Bilinear interpolation|Example of bilinear interpolation in grayscale values]]\n\nAs seen in the example on the right, the intensity value at the pixel computed to be at row 20.2, column 14.5 can be calculated by first linearly interpolating between the values at column 14 and 15 on each rows 20 and 21, giving\n:<math>\\begin{align}\nI_{20, 14.5} & = \\tfrac{15 - 14.5}{15 - 14} \\cdot 91  + \\tfrac{14.5 - 14}{15 - 14} \\cdot 210 = 150.5, \\\\\nI_{21, 14.5} & = \\tfrac{15 - 14.5}{15 - 14} \\cdot 162 + \\tfrac{14.5 - 14}{15 - 14} \\cdot 95  = 128.5,\n\\end{align}</math>\nand then interpolating linearly between these values, giving\n:<math>I_{20.2, 14.5} = \\tfrac{21 - 20.2}{21 - 20} \\cdot 150.5 + \\tfrac{20.2 - 20}{21 - 20} \\cdot 128.5 = 146.1.</math>\nThis algorithm reduces some of the visual distortion caused by resizing an image to a non-integral zoom factor, as opposed to nearest-neighbor interpolation, which will make some pixels appear larger than others in the resized image.\n\n== See also ==\n* [[Bicubic interpolation]]\n* [[Trilinear interpolation]]\n* [[Spline interpolation]]\n* [[Lanczos resampling]]\n* [[Stairstep interpolation]]\n* [[Barycentric coordinate system (mathematics)|Barycentric coordinates - for interpolating within a triangle or tetrahedron]]\n\n==References==\n<references/>\n* [http://www.aip.de/groups/soe/local/numres/bookcpdf/c3-6.pdf Numerical Recipes in C, 1988–92 Cambridge University Press, {{isbn|0-521-43108-5}}, pp. 123–128].\n\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Catmull–Clark subdivision surface",
      "url": "https://en.wikipedia.org/wiki/Catmull%E2%80%93Clark_subdivision_surface",
      "text": "[[File:Catmull-Clark subdivision of a cube.svg|thumbnail|First three steps of Catmull–Clark subdivision of a cube with subdivision surface below (note that the Catmull-Clark bi-cubic interpolation above cannot approach an actual sphere, as a sphere would be quadric).]]\nThe '''Catmull–Clark''' algorithm is a technique used in [[computer graphics]] to create smooth surfaces by [[subdivision surface]] modeling. It was devised by [[Edwin Catmull]] and [[James H. Clark|Jim Clark]] in 1978 as a generalization of bi-cubic uniform [[B-spline]] surfaces to arbitrary topology.<ref name=CatmullClark/> In 2005, Edwin Catmull received an [[Academy Award for Technical Achievement]] together with \nTony DeRose and [[Jos Stam]] for their invention and application of subdivision surfaces.\n\n== Recursive evaluation ==\nCatmull–Clark surfaces are defined recursively, using the following refinement scheme:<ref name=CatmullClark>{{Cite journal | last1 = Catmull | first1 = E. | authorlink1= Edwin Catmull| last2 = Clark | first2 = J. |authorlink2 = James H. Clark| doi = 10.1016/0010-4485(78)90110-0 | title = Recursively generated B-spline surfaces on arbitrary topological meshes | journal = Computer-Aided Design | volume = 10 | issue = 6 | pages = 350 | year = 1978 | url = http://www.cs.berkeley.edu/~sequin/CS284/PAPERS/CatmullClark_SDSurf.pdf| pmid =  | pmc = }}</ref>\n\nStart with a [[Polygon mesh|mesh]] of an arbitrary [[polyhedron]]. All the [[vertex (geometry)|vertices]] in this mesh shall be called original points.\n* For each face, add a ''face point''\n** Set each face point to be the ''[[average]] of all original points for the respective face''.\n* For each edge, add an ''edge point''.\n** Set each edge point to be the ''average of the two neighbouring face points and its two original endpoints''.\n* For each ''face point'', add an edge for every edge of the face, connecting the ''face point'' to each ''edge point'' for the face.\n* For each original point ''P'', take the average ''F'' of all ''n'' (recently created) face points for faces touching ''P'', and take the average ''R'' of all ''n'' edge midpoints for (original) edges touching ''P'', where each edge midpoint is the average of its two endpoint vertices (not to be confused with new \"edge points\" above). ''Move each original point'' to the point\n:: <math>\\frac{F + 2R + (n-3)P}{n}.</math>\n:This is the [[barycenter]] of ''P'', ''R'' and ''F'' with respective weights (''n'' − 3), 2 and 1.\n* Connect each new vertex point to the new edge points of all original edges incident on the original vertex.\n* Define new faces as enclosed by edges.\n\nThe new mesh will consist only of [[quadrilateral]]s, which in general will not be [[Plane (mathematics)|planar]]. The new mesh will generally look smoother than the old mesh.\n\nRepeated subdivision results in smoother meshes. It can be shown that the limit surface obtained by this refinement process is at least <math>\\mathcal{C}^1</math> at extraordinary vertices and <math>\\mathcal{C}^2</math> everywhere else (when ''n'' indicates how many derivatives are [[Smooth function#Differentiability classes|continuous]], we speak of <math>\\mathcal{C}^n</math> continuity). After one iteration, the number of extraordinary points on the surface remains constant.\n\nThe arbitrary-looking barycenter formula was chosen by Catmull and Clark based on the aesthetic appearance of the resulting surfaces rather than on a mathematical derivation, although Catmull and Clark do go to great lengths to rigorously show that the method converges to bicubic B-spline surfaces.<ref name=CatmullClark />\n\n== Exact evaluation ==\nThe limit surface of Catmull–Clark subdivision surfaces can also be evaluated directly, without any recursive refinement. This can be accomplished by means of the technique of [[Jos Stam]].<ref name=Stam>{{cite book| last1 = Stam | first1 = J. | author1-link = Jos Stam| chapter = Exact evaluation of Catmull-Clark subdivision surfaces at arbitrary parameter values| doi = 10.1145/280814.280945| title = Proceedings of the 25th annual conference on Computer graphics and interactive techniques - SIGGRAPH '98| chapter-url = http://www.dgp.toronto.edu/people/stam/reality/Research/pdf/sig98.pdf| pages = 395–404| year = 1998| isbn = 978-0-89791-999-9| pmid = | pmc = | citeseerx = 10.1.1.20.7798 }}</ref> This method reformulates the recursive refinement process into a [[matrix exponential]] problem, which can be solved directly by means of [[matrix diagonalization]].\n\n== Software using Catmull–Clark subdivision surfaces ==\n{{Refimprove section|date=April 2013}}\n{{colbegin|colwidth=20em}}\n* [[Autodesk 3ds Max|3ds Max]]\n* [[3D-Coat]]\n* [[AC3D]]\n* [[Anim8or]]\n* [[AutoCAD]]\n* [[Blender (software)|Blender]]\n* [[Carrara (software)|Carrara]]\n* [[CATIA|CATIA (Imagine and Shape)]]\n* [[CGAL]]\n* [[Cheetah3D]]\n* [[Cinema4D]]\n* [[Clara.io]]\n* [[PTC Creo|Creo (Freestyle)]]<ref>{{Cite web |url=http://www.ptc.com/File%20Library/Community/Academic%20Program/College_Student_Primer_Creo_2.pdf |title=Archived copy |access-date=2016-12-04 |archive-url=https://web.archive.org/web/20161123062105/http://www.ptc.com/File%20Library/Community/Academic%20Program/College_Student_Primer_Creo_2.pdf |archive-date=2016-11-23 |dead-url=yes }}</ref>\n* [[DAZ Studio|DAZ Studio, 2.0]]\n* [[DeleD Community Edition]]\n* [[DeleD Designer]]\n* [[Gelato (software)|Gelato]]\n* [[Valve Hammer Editor|Hammer]]\n* [[Hexagon (software)|Hexagon]]\n* [[Houdini (software)|Houdini]]\n<!--* JPatch-->\n* [[LightWave|LightWave 3D, version 9]]\n* [[Makehuman]]\n* [[Maya (software)|Maya]]\n* [[Metasequoia (software)|Metasequoia]]\n* [[Modo (software)|MODO]]\n* [[Mudbox]]\n* Pixar's [[OpenSubdiv]]<ref name=\"WattCoumans2014\">{{cite book|editors=Martin Watt, Erwin Coumans, George ElKoura, Ronald Henderson, Manuel Kraemer, Jeff Lait, James Reinders|title=Multithreading for Visual Effects|year=2014|publisher=CRC Press|isbn=978-1-4822-4356-7|pages=163–199|author=Manuel Kraemer|chapter=OpenSubdiv: Interoperating GPU Compute and Drawing}}</ref><ref>https://www.youtube.com/watch?v=xFZazwvYc5o</ref><ref>http://www.fxguide.com/featured/pixars-opensubdiv-v2-a-detailed-look/</ref><ref>http://on-demand.gputechconf.com/gtc/2014/video/S4856-subdivision-surfaces-industry-standard.mp4</ref><ref>https://www.youtube.com/watch?v=dzIl_S-qHIQ</ref>\n* [[PhotoRealistic RenderMan|PRMan]]\n* [[Realsoft3D]]\n* [[Remo 3D]]\n* [[Shade 3D|Shade]]\n* [[Rhinoceros 3D]] - Grasshopper 3D Plugin - Weaverbird Plugin\n* [[Silo (software)|Silo]]\n* [[SketchUp]] - Requires a Plugin.\n* [[Softimage XSI]]\n* [[Strata 3D|Strata 3D CX]]\n<!--* Vue 9-->\n* [[Wings 3D]]\n* [[Zbrush]]\n<!--* TopoGun-->\n<!--* CREO 1.0 - PTC - (Freestyle)-->\n{{colend}}\n\n== See also ==\n* [[Conway polyhedron notation]] - A set of related topological polyhedron and polygonal mesh operators.\n\n== References ==\n{{reflist}}\n\n==Further reading==\n* {{Cite book | doi = 10.1145/280814.280826| chapter = Subdivision surfaces in character animation| title = Proceedings of the 25th annual conference on Computer graphics and interactive techniques - SIGGRAPH '98| pages = 85| year = 1998| last1 = Derose | first1 = T. | last2 = Kass | first2 = M. | last3 = Truong | first3 = T. | isbn = 978-0897919999| chapter-url = http://graphics.pixar.com/library/Geri/paper.pdf| citeseerx = 10.1.1.679.1198}}\n* {{Cite journal | doi = 10.1145/1330511.1330519| title = Approximating Catmull-Clark subdivision surfaces with bicubic patches| journal = ACM Transactions on Graphics| volume = 27| pages = 1–11| year = 2008| last1 = Loop | first1 = C. | last2 = Schaefer | first2 = S. | url = http://research.microsoft.com/en-us/um/people/cloop/accTOG.pdf| citeseerx = 10.1.1.153.2047}}\n* {{Cite journal | doi = 10.1109/TVCG.2010.31| pmid = 20616390| title = Real-Time Creased Approximate Subdivision Surfaces with Displacements| journal = IEEE Transactions on Visualization and Computer Graphics| volume = 16| issue = 5| pages = 742–51| year = 2010| last1 = Kovacs | first1 = D. | last2 = Mitchell | first2 = J. | last3 = Drone | first3 = S. | last4 = Zorin | first4 = D. | url = http://www.computer.org/csdl/trans/tg/2010/05/ttg2010050742.pdf}} [http://mrl.nyu.edu/~dzorin/papers/kovacs2010rcs.pdf preprint]\n* Matthias Nießner, Charles Loop, Mark Meyer, Tony DeRose, \"[http://research.microsoft.com/en-us/um/people/cloop/tog2012.pdf Feature Adaptive GPU Rendering of Catmull-Clark Subdivision Surfaces]\", ACM Transactions on Graphics Volume 31 Issue 1, January 2012, {{doi|10.1145/2077341.2077347}}, [https://www.youtube.com/watch?v=uogAzQoVdNU demo]\n* Nießner, Matthias ; Loop, Charles ; Greiner, Günther: [http://research.microsoft.com/en-us/um/people/cloop/EG2012.pdf Efficient Evaluation of Semi-Smooth Creases in Catmull-Clark Subdivision Surfaces]: Eurographics 2012 Annex: Short Papers (Eurographics 2012, Cagliary). 2012, pp 41–44.\n* Wade Brainerd, [https://archive.org/details/GDC2014Brainerd Tessellation in Call of Duty: Ghosts] also presented as a SIGGRAPH2014 tutorial [http://advances.realtimerendering.com/s2014/wade/]\n*D. Doo and M. Sabin: ''Behavior of recursive division surfaces near extraordinary points'', Computer-Aided Design, 10 (6) 356&ndash;360 (1978), ([https://dx.doi.org/10.1016/0010-4485(78)90111-2 doi], [http://www.cs.caltech.edu/~cs175/cs175-02/resources/DS.pdf pdf])\n\n{{DEFAULTSORT:Catmull-Clark Subdivision Surface}}\n[[Category:3D computer graphics]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Coons patch",
      "url": "https://en.wikipedia.org/wiki/Coons_patch",
      "text": "[[Image:Example of coons surface.svg|thumb|400px|right|Sample Coons patch]]\n\nIn [[mathematics]], a '''Coons patch''', is a type of [[manifold]] [[Parametrization (geometry)|parametrization]] used in [[computer graphics]] to smoothly join other [[Surface (topology)|surface]]s together, and in [[computational mechanics]] applications, particularly in [[finite element method]] and [[boundary element method]], to mesh problem domains into elements.\n\nCoons patches are named after [[Steven Anson Coons]], and date to 1967.<ref name=\"coons\">Steven A. Coons, Surfaces for computer-aided design of space forms, Technical Report MAC-TR-41, Project MAC, MIT, June 1967.</ref>\n\n==Bilinear blending==\nGiven four [[space]] curves ''c''<sub>0</sub>(''s''), ''c''<sub>1</sub>(''s''), ''d''<sub>0</sub>(''t''), ''d''<sub>1</sub>(''t'') which meet at four corners ''c''<sub>0</sub>(0) = ''d''<sub>0</sub>(0), ''c''<sub>0</sub>(1) = ''d''<sub>1</sub>(0), ''c''<sub>1</sub>(0) = ''d''<sub>0</sub>(1), ''c''<sub>1</sub>(1) = ''d''<sub>1</sub>(1); [[linear interpolation]] can be used to interpolate between ''c''<sub>0</sub> and ''c''<sub>1</sub>, that is\n\n:<math>L_c(s,t)=(1-t) c_0(s)+ t c_1(s) </math>\n\nand between ''d''<sub>0</sub>, ''d''<sub>1</sub>\n\n:<math>L_d(s,t)=(1-s) d_0(t)+ s d_1(t) </math>\n\nproducing two [[ruled surface]]s defined on the unit square.\n\nThe [[bilinear interpolation]] on the four corner points is another surface\n\n:<math> B(s,t) = c_0(0) (1-s)(1-t) + c_0(1) s(1-t) +  c_1(0) (1-s)t + c_1(1) s t. </math>\n\nA '''bilinearly blended Coons patch''' is the surface\n\n:<math>C(s,t)=L_c(s,t)+L_d(s,t)-B(s,t). </math>\n\n==Bicubic blending==\nAlthough the bilinear Coons patch exactly meets its four boundary curves, it does not necessarily have the same [[tangent plane]] at those curves as the surfaces to be joined, leading to creases in the joined surface along those curves. To fix this problem, the linear interpolation can be replaced with [[cubic Hermite spline]]s with the weights chosen to match the partial derivatives at the corners. This forms a '''bicubically blended Coons patch'''.\n\n==See also==\n\n* [[Surface (topology)|Surface]]\n* [[Atlas (topology)]]\n* [[Interpolation]]\n\n==References==\n{{Reflist}}\n\n*{{cite web|title=Surface Construction Schemes | author=Weiqing Gu | url=http://www.math.hmc.edu/~gu/math142/mellon/Application_to_CAGD/Surface_Construction_Schem.html | accessdate= 8 April 2012}}\n\n[[Category:Multivariate interpolation]]\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "Doo–Sabin subdivision surface",
      "url": "https://en.wikipedia.org/wiki/Doo%E2%80%93Sabin_subdivision_surface",
      "text": "In [[computer graphics]], '''Doo–Sabin subdivision surface''' is a type of [[subdivision surface]] based on a generalization of bi-quadratic uniform [[B-spline]]s.  It was developed in 1978 by Daniel Doo and Malcolm Sabin.<ref name=Doo>D. Doo: ''A subdivision algorithm for smoothing down irregularly shaped polyhedrons'', Proceedings on Interactive Techniques in Computer Aided Design, pp. 157 - 165, 1978 ([http://trac2.assembla.com/DooSabinSurfaces/export/12/trunk/docs/Doo%201978%20Subdivision%20algorithm.pdf pdf])</ref><ref name=DooSabin>D.Doo, M.Sabin: ''Behaviour of recursive division surfaces near extraordinary points'', Computer Aided Design, pp. 356-360, 1978 ([http://courses.cms.caltech.edu/cs175/cs175-02/resources/DS.pdf])</ref>\n\nThis process generates one new face at each original vertex, ''n'' new faces along each original edge, and ''n''<sup>2</sup> new faces at each original face. A primary characteristic of the Doo–Sabin subdivision method is the creation of four faces around every vertex. A drawback is that the faces created at the vertices are not necessarily [[coplanar]].\n\n== Evaluation ==\nDoo–Sabin surfaces are defined recursively. Each refinement iteration replaces the current mesh with a smoother, more refined mesh, following the procedure described in. After many iterations, the surface will gradually converge onto a smooth limit surface. The figure below show the effect of two refinement iterations on a T-shaped quadrilateral mesh.\n[[Image:DooSabin subdivision.png|none|500px|Two Doo–Sabin refinement iterations on a T-shaped quadrilateral mesh.]]\n\nJust as for [[Catmull–Clark subdivision surface|Catmull–Clark surfaces]], Doo–Sabin limit surfaces can also be evaluated directly without any recursive refinement, by means of the technique of [[Jos Stam]].<ref name=Stam>\nJos Stam, ''Exact Evaluation of Catmull–Clark Subdivision Surfaces at Arbitrary Parameter Values'', Proceedings of SIGGRAPH'98. In Computer Graphics Proceedings, ACM SIGGRAPH, 1998,  395–404 ([http://www.dgp.toronto.edu/people/stam/reality/Research/pdf/sig98.pdf pdf], [http://www.dgp.toronto.edu/~stam/reality/Research/SubdivEval/index.html downloadable eigenstructures])</ref> The solution is, however, not as computationally efficient as for Catmull-Clark surfaces because the Doo–Sabin subdivision matrices are not in general diagonalizable.\n\n== See also ==\n* [[Expansion (geometry)|Expansion]] - Equivalent geometric operation - truncates vertices and beveling edges.\n* [[Conway polyhedron notation]] - A set of related topological polyhedron and polygonal mesh operators.\n\n== External links ==\n<references/>\n* [http://graphics.cs.ucdavis.edu/education/CAGDNotes/Doo-Sabin/Doo-Sabin.html Doo–Sabin surfaces]\n\n{{DEFAULTSORT:Doo-Sabin subdivision surface}}\n[[Category:3D computer graphics]]\n[[Category:Multivariate interpolation]]\n\n\n{{Comp-sci-stub}}"
    },
    {
      "title": "Inverse distance weighting",
      "url": "https://en.wikipedia.org/wiki/Inverse_distance_weighting",
      "text": "'''Inverse distance weighting''' ('''IDW''') is a type of [[Deterministic algorithm|deterministic method]] for [[multivariate interpolation]] with a known scattered set of points. The assigned values to unknown points are calculated with a [[Weighted mean|weighted average]] of the values available at the known points.\n\nThe name given to this type of methods was motivated by the [[Weighted mean|weighted average]] applied, since it resorts to the inverse of the distance to each known point (\"amount of proximity\") when assigning weights.\n\n==Definition of the problem==\nThe expected result is a discrete assignment of the unknown function <math>u</math> in a study region:\n\n:<math>u(x): x \\to \\mathbb{R}, \\quad x \\in \\mathbf{D} \\sub \\mathbb{R}^n,</math>\n\nwhere <math>\\mathbf{D}</math> is the study region.\n\nThe set of <math>N</math> known data points can be described as a list of [[tuple]]s:\n\n:<math>[(x_1, u_1), (x_2, u_2), ..., (x_N, u_N)].</math>\n\nThe function is to be \"smooth\" (continuous and once differentiable), to be exact (<math>u(x_i) = u_i</math>) and to meet the user's intuitive expectations about the phenomenon under investigation. Furthermore, the function should be suitable for a computer application at a reasonable cost (nowadays, a basic implementation will probably make use of [[parallel computing|parallel resources]]).\n\n== Shepard's method ==\n\n=== Historical reference ===\nAt the Harvard Laboratory for Computer Graphics and Spatial Analysis, beginning in 1965, a varied collection of scientists converged to rethink, among other things, what we now call [[geographic information system]]s.<ref>{{cite news |last=Chrisman |first=Nicholas |title=History of the Harvard Laboratory for Computer Graphics: a Poster Exhibit | url=http://isites.harvard.edu/fs/docs/icb.topic39008.files/History_LCG.pdf }}</ref>\n\nThe motive force behind the Laboratory, Howard Fisher, conceived an improved computer mapping program that he called SYMAP, which, from the start, Fisher wanted to improve on the interpolation. He showed Harvard College freshmen his work on SYMAP, and many of them participated in Laboratory events. One freshman, Donald Shepard, decided to overhaul the interpolation in SYMAP, resulting in his famous article from 1968.<ref name=shepardArticle>{{cite conference |last=Shepard |first=Donald |year=1968 |title=A two-dimensional interpolation function for irregularly-spaced data |booktitle=Proceedings of the 1968 [[Association for Computing Machinery|ACM]] National Conference |pages = 517–524 |doi=10.1145/800186.810616 }}</ref>\n\nShepard’s algorithm was also influenced by the theoretical approach of [[William Warntz]] and others at the Lab who worked with spatial analysis. He conducted a number of experiments with the exponent of distance, deciding on something closer to the gravity model (exponent of -2). Shepard implemented not just basic inverse distance weighting, but also he allowed barriers (permeable and absolute) to interpolation.\n\nOther research centers were working on interpolation at this time, particularly University of Kansas and their SURFACE II program. Still, the features of SYMAP were state-of-the-art, even though programmed by an undergraduate.\n\n=== Basic form ===\n[[Image:Shepard interpolation 2.png|thumb|640px|center|Shepard's interpolation for different power parameters ''p'', from scattered points on the surface <math>z=\\exp(-x^2-y^2)</math>]]\n\nA general form of finding an interpolated value <math>u</math> at a given point <math>x</math> based on samples <math>u_i=u(x_i)</math> for <math>i=1,2,...,N</math> using IDW is an interpolating function:\n\n:<math>u(\\mathbf{x}) = \\begin{cases}\n \\dfrac{\\sum_{i = 1}^{N}{ w_i(\\mathbf{x}) u_i } }{ \\sum_{i = 1}^{N}{ w_i(\\mathbf{x}) } }, & \\text{if } d(\\mathbf{x},\\mathbf{x}_i) \\neq 0 \\text{ for all } i, \\\\\n u_i, & \\text{if } d(\\mathbf{x},\\mathbf{x}_i) = 0 \\text{ for some } i,\n\\end{cases} </math>\n\nwhere\n\n:<math>w_i(\\mathbf{x}) =  \\frac{1}{d(\\mathbf{x},\\mathbf{x}_i)^p}</math>\n\nis a simple IDW weighting function, as defined by Shepard,<ref name=shepardArticle/> '''x''' denotes an interpolated (arbitrary) point, '''x'''<sub>''i''</sub> is an interpolating (known) point, <math>d</math> is a given distance ([[Metric (mathematics)|metric]] operator) from the known point '''x'''<sub>''i''</sub> to the unknown point '''x''', ''N'' is the total number of known points used in interpolation and <math>p</math> is a positive real number, called the power parameter.\n\nHere weight decreases as distance increases from the interpolated points. Greater values of <math>p</math> assign greater influence to values closest to the interpolated point, with the result turning into a mosaic of tiles (a [[Voronoi diagram]]) with nearly constant interpolated value for large values of ''p''. For two dimensions, power parameters <math>p \\leq 2</math> cause the interpolated values to be dominated by points far away, since with a density <math>\\rho</math> of data points and neighboring points between distances <math>r_0</math> to <math>R</math>, the summed weight is approximately\n:<math>\\sum_j w_j \\approx \\int_{r_0}^R \\frac{2\\pi r\\rho \\,dr}{r^p} = 2\\pi\\rho\\int_{r_0}^R r^{1-p} \\,dr,</math>\nwhich diverges for <math>R\\rightarrow\\infty</math> and <math>p\\leq2</math>. For ''M'' dimensions,  the same argument holds for <math>p\\leq M</math>.  For the choice of value for ''p'', one can consider the degree of smoothing desired in the interpolation, the density and distribution of samples being interpolated, and the maximum distance over which an individual sample is allowed to influence the surrounding ones.\n\n''Shepard's method'' is a consequence of minimization of a functional related to a measure of deviations between [[tuple]]s of interpolating points {'''x''', ''u''} and ''i'' tuples of interpolated points {'''x'''<sub>''i''</sub>, ''u<sub>i</sub>''}, defined as:\n:<math>\\phi(\\mathbf{x}, u) = \\left( \\sum_{i = 0}^{N}{\\frac{(u-u_i)^2}{d(\\mathbf{x},\\mathbf{x}_i)^p}} \\right)^{\\frac{1}{p}} ,</math>\nderived from the minimizing condition:\n:<math>\\frac{\\partial \\phi(\\mathbf{x}, u)}{\\partial u} = 0.</math>\n\nThe method can easily be extended to other dimensional spaces and it is in fact a generalization of Lagrange\napproximation into a multidimensional spaces. A modified version of the algorithm designed for trivariate interpolation was developed by Robert J. Renka and is available in [[Netlib]] as algorithm 661 in the toms library.\n\n=== Example in 1 dimension ===\n[[Image:Shepard interpolation 1 dimension.png|thumb|640px|center|Shepard's interpolation in 1 dimension, from 4 scattered points and using ''p'' = 2]]\n\n=== Łukaszyk–Karmowski metric ===\nYet another modification of the Shepard's method was proposed by Łukaszyk<ref>{{cite journal|journal=Computational Mechanics| volume= 33| issue=4| pages=299–304| doi=10.1007/s00466-003-0532-2| title=A new concept of probability metric and its applications in approximation of scattered data sets| author=Łukaszyk S.}}</ref> also in applications to experimental mechanics. The proposed weighting function had the form\n\n:<math>w_k(\\mathbf{x}) =  \\frac{1}{(D_{**}(\\mathbf{x}, \\mathbf{x}_k)  )^\\frac{1}{2}},</math>\n\nwhere <math>D_{**}(\\mathbf{x}, \\mathbf{x}_k)</math> is the [[Łukaszyk–Karmowski metric]] chosen also with regard to the [[statistical error]] [[probability distribution]]s of measurement of the interpolated points.\n\n=== Modified Shepard's method ===\n\nAnother modification of Shepard's method calculates interpolated value using only nearest neighbors within ''R''-sphere (instead of full sample). Weights are slightly modified in this case:\n\n:<math>w_k(\\mathbf{x}) =  \\left( \\frac{\\max(0,R-d(\\mathbf{x},\\mathbf{x}_k))}{R d(\\mathbf{x},\\mathbf{x}_k)} \\right)^2.</math>\n\nWhen combined with fast spatial search structure (like [[kd-tree]]), it becomes efficient ''N'' log ''N'' interpolation method suitable for large-scale problems.\n\n== References ==\n<references/>\n\n== See also ==\n* [[Multivariate interpolation]]\n* [[Kernel density estimation]]\n\n{{DEFAULTSORT:Inverse Distance Weighting}}\n[[Category:Geostatistics]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Lanczos resampling",
      "url": "https://en.wikipedia.org/wiki/Lanczos_resampling",
      "text": "{{multiple image \n | footer = Partial plot of a discrete signal (black dots) and of its Lanczos interpolation (solid blue curve), with size parameter ''a'' equal to 1 (top), 2 (middle) and 3 (bottom).  Also shown are two copies of the Lanczos kernel, shifted and scaled, corresponding to samples 4 and 11 (dashed curves).\n | width = 320\n | direction = vertical\n | image1 = Lanczos-r01-filtering.svg | alt1 = Lanczos interpolation with radius 1\n | image2 = Lanczos-r02-filtering.svg | alt2 = Lanczos interpolation with radius 2\n | image3 = Lanczos-r03-filtering.svg | alt3 = Lanczos interpolation with radius 3\n}}\n\n'''Lanczos filtering''' and '''Lanczos resampling''' are two applications of a mathematical formula. It can be used as a low-pass filter or used to smoothly [[interpolation|interpolate]] the value of a [[Digital signal (signal processing)|digital signal]] between its [[sample (signal)|samples]].  In the latter case it maps each sample of the given signal to a translated and scaled copy of the '''Lanczos kernel''', which is a [[sinc function]] [[window function|windowed]] by the central lobe of a second, longer, sinc function.  The sum of these translated and scaled kernels is then evaluated at the desired points.\n\nLanczos resampling is typically used to increase the [[sampling rate]] of a digital signal, or to shift it by a fraction of the sampling interval.  It is often used also for [[multivariate interpolation]], for example to [[image scaling|resize]] or [[rotation (geometry)|rotate]] a [[digital image]].  It has been considered the \"best compromise\" among several simple filters for this purpose.<ref name=\"turkgab\">{{cite book\n | first1 = Ken | last1 = Turkowski | first2 = Steve | last2 = Gabriel\n | year = 1990\n | editor-first = Andrew S. | editor-last = Glassner\n | chapter = Filters for Common Resampling Tasks\n | title = Graphics Gems I\n | publisher = Academic Press\n | pages = 147–165\n | isbn = 978-0-12-286165-9\n | citeseerx = 10.1.1.116.7898\n }}</ref>\n\nThe filter is named after its inventor, [[Cornelius Lanczos]] ({{IPA-hu|ˈlaːnt͡soʃ}}).\n\n== Definition ==\n\n===Lanczos kernel===\n[[File:Lanczos-windows.svg|thumb|Lanczos windows for ''a'' = 1, 2, 3.]]\n\n[[Image:Lanczos-kernel.svg|thumb|right|Lanczos kernels for the cases ''a'' = 2 and ''a'' = 3. Note that the function obtains negative values.]]\n\nThe effect of each input sample on the interpolated values is defined by the filter's [[signal reconstruction|reconstruction kernel]] {{math|''L''(''x'')}}, called the Lanczos kernel.  It is the normalized [[sinc]] function {{math|sinc(''x'')}}, [[window function|windowed]] (multiplied) by the '''Lanczos window''',{{anchor|Lanczos window}} or '''sinc window''', which is the central lobe of a horizontally stretched sinc function {{math|sinc(''x''/''a'')}} for {{math|&nbsp;&minus;''a''&nbsp;≤&nbsp;''x''&nbsp;≤&nbsp;''a''}}.\n\n: <math> L(x) = \\begin{cases}\n \\operatorname{sinc}(x)\\, \\operatorname{sinc}(x/a) & \\text{if}\\ -a < x < a, \\\\\n 0 & \\text{otherwise}.\n\\end{cases} </math>\n\nEquivalently,\n\n:<math>L(x) = \\begin{cases}\n 1  & \\text{if}\\ x = 0, \\\\\n \\dfrac{a \\sin(\\pi x) \\sin(\\pi x / a)}{\\pi^2 x^2} & \\text{if}\\ -a \\leq x < a \\ \\text{and}\\ x \\neq 0, \\\\\n 0 & \\text{otherwise}.\n\\end{cases}</math>\n\nThe parameter {{math|''a''}} is a positive integer, typically 2 or 3, which determines the size of the kernel. The Lanczos kernel has  {{math|2''a'' − 1}} lobes: a positive one at the center, and {{math|''a'' − 1}} alternating negative and positive lobes on each side.\n\n=== Interpolation formula ===\nGiven a one-dimensional signal with samples {{math|''s''<sub>''i''</sub>}}, for integer values of {{math|''i''}}, the value {{math|''S''(''x'')}} interpolated  at an arbitrary real argument {{math|''x''}} is obtained by the discrete [[convolution]] of those samples with the Lanczos kernel:<ref name=\"burger\">{{cite book\n | title = Principles of digital image processing: core algorithms\n | first1 = Wilhelm | last1 = Burger | first2 = Mark J. | last2 = Burge\n | publisher = Springer\n | year = 2009\n | isbn = 978-1-84800-194-7\n | pages = 231–232\n | url = {{google books|plainurl=yes|id=s5CBZLBakawC|page=231}}\n }}</ref>\n: <math>S(x) = \\sum_{i=\\lfloor x \\rfloor - a + 1}^{\\lfloor x \\rfloor + a} s_{i} L(x - i),</math>\nwhere {{math|''a''}} is the filter size parameter, and <math>\\lfloor x \\rfloor </math> is the [[floor function]]. The bounds of this sum are such that the kernel is zero outside of them.\n\n== Properties ==\nAs long as the parameter {{math|''a''}} is a positive integer, the Lanczos kernel is [[continuous function|continuous]] everywhere, and its [[derivative]] is defined and continuous everywhere (even at {{math|''x''}}&nbsp;=&nbsp;{{math|±''a''}}, where both sinc functions go to zero).  Therefore, the reconstructed signal {{math|''S''(''x'')}} too will be continuous, with continuous derivative.\n\nThe Lanczos kernel is zero at every integer argument {{math|''x''}},  except at {{math|''x''}}&nbsp;=&nbsp;0, where it has value 1.  Therefore, the reconstructed signal exactly interpolates the given samples: we will have {{math|''S''(''x'')}}&nbsp;=&nbsp;{{math|''s''<sub>''i''</sub>}} for every integer argument {{math|''x''}}&nbsp;=&nbsp;{{math|''i''}}.\n\n== Multidimensional interpolation==\n[[Image:Lanczos interpolation - Sheet music, original.jpg|thumb| The {{lang|la|[[incipit]]}} of a black-and-white image. Original, low-quality expansion with JPEG artifacts.]]\n[[Image:Lanczos interpolation - Sheet music, interpolated.jpg|thumb|The same image resampled to five times as many samples in each direction, using Lanczos resampling. Pixelation artifacts were removed changing the image's transfer function.]]\nLanczos filter's kernel in two dimensions is\n:<math>L(x, y) = L(x)L(y).</math>\n\n== Evaluation ==\n\n=== Advantages ===\n[[File:Window function and frequency response - Lanczos.svg|thumb|A discrete Lanczos window and its [[frequency response]]; see [[Window function]] for comparison with other windows.]]\n\nThe theoretically optimal reconstruction filter for [[bandlimiting|band-limited signal]]s is the [[sinc filter]], which has infinite [[support (mathematics)|support]]. The Lanczos filter is one of many practical (finitely supported) approximations of the sinc filter.  Each interpolated value is the weighted sum of {{math|2''a''}} consecutive input samples.  Thus, by varying the {{math|2''a''}} parameter one may trade computation speed for improved frequency response.  The parameter also allows one to choose between a smoother interpolation or a preservation of sharp transients in the data.  For image processing, the trade-off is between the reduction of [[aliasing]] artefacts and the preservation of sharp edges. Also as with any such processing, there are no results for the borders of the image. Increasing the length of the kernel increases the cropping of the edges of the image.\n\nThe Lanczos filter has been compared with other interpolation methods for discrete signals, particularly other windowed versions of the sinc filter.  [[Kenneth Turkowski|Turkowski]] and [[Steven Gabriel|Gabriel]] claimed that the Lanczos filter (with {{math|''a''}} = 2) the \"best compromise in terms of reduction of aliasing, sharpness, and minimal ringing\", compared with truncated sinc and the [[Bartlett window|Bartlett]], [[cosine window|cosine-]], and [[Hann window|Hann-window]]ed sinc, for decimation and interpolation of 2-dimensional image data.<ref name=\"turkgab\"/> According to [[Jim Blinn]], the Lanczos kernel (with {{math|''a''}} = 3) \"keeps low frequencies and rejects high frequencies better than any (achievable) filter we've seen so far.\"<ref>{{cite book\n | first = Jim | last = Blinn | authorlink = Jim Blinn\n | title = Jim Blinn's corner: dirty pixels\n | publisher = Morgan Kaufmann\n | year = 1998\n | isbn = 978-1-55860-455-1\n | pages = 26–27\n | url = {{google books|plainurl=yes|id=4fjFQs3cPckC|page=27|text=blinn lanczos}}\n }}</ref>\n\nLanczos interpolation is a popular filter for \"upscaling\" videos in various media utilities, such as [[AviSynth]]<ref>{{cite web|url=http://avisynth.nl/index.php/LanczosResize#LanczosResize |title=Resize |publisher=Avisynth |date=2015-01-01 |accessdate=2015-07-27}}</ref> and [[FFmpeg]].<ref>{{cite web|url=http://www.neowin.net/forum/topic/422992-a-how-to-guide-upconverting-video-using-ffdshow |title=A How To guide: Upconverting video using FFDShow - Neowin Forums |publisher=Neowin.net |date=2006-04-18 |accessdate=2012-07-31}}</ref>\n\n=== Limitations ===\nSince the kernel assumes negative values for {{math|''a'' > 1}}, the interpolated signal can be negative even if all samples are positive.  More generally, the range of values of the interpolated signal may be wider than the range spanned by the discrete sample values.  In particular, there may be [[ringing artifacts]] just before and after abrupt changes in the sample values, which may lead to [[clipping (signal processing)|clipping artifacts]].  However, these effects are reduced compared to the (non-windowed) sinc filter. For ''a''&nbsp;=&nbsp;2 (a three-lobed kernel) the ringing is&nbsp;<&nbsp;1%.\n\nThe method is one of the interpolation options available in the free software GNU Image Manipulation Program (GIMP). One way to visualise the ringing effect is to rescale a black and white block graphic and select Lanczos interpolation. \n\nWhen using the Lanczos filter for image resampling, the ringing effect will create light and dark halos along any strong edges.  While these bands may be visually annoying, they help increase the [[acutance|perceived sharpness]], and therefore provide a form of [[edge enhancement]].  This may improve the subjective quality of the image, given the special role of edge sharpness in [[visual perception|vision]].<ref>{{cite web|url=http://www.ipol.im/pub/algo/g_linear_methods_for_image_interpolation/#index19h1 |title=IPOL: Linear Methods for Image Interpolation |publisher=Ipol.im |date=2011-09-27 |accessdate=2012-07-31}}</ref>\n\nIn some applications, the low-end clipping artifacts can be ameliorated by transforming the data to a logarithmic domain prior to filtering.  In this case the interpolated values will be a weighted geometric mean, rather than an arithmetic mean, of the input samples.\n\nThe Lanczos kernel does not have the [[partition of unity]] property.  That is, the sum <math>U(x) = \\sum_{i\\in \\mathbb{Z}} L(x-i)</math> of all integer-translated copies of the kernel is not always 1.  Therefore, the Lanczos interpolation of a discrete signal with constant samples does not yield a constant function.  This defect is most evident when&nbsp;{{math|''a''}}&nbsp;=&nbsp;1.  Also, for {{math|''a''}}&nbsp;=&nbsp;1 the interpolated signal has zero derivative at every integer argument. This is rather academic, since using a single-lobe kernel (''a''&nbsp;=&nbsp;1) loses all the benefits of the Lanczos approach and provides a poor filter. There are many better single-lobe, bell-shaped windowing functions.\n\n==See also==\n\n*[[Bicubic interpolation]]\n*[[Bilinear interpolation]]\n*[[Spline interpolation]]\n*[[Nearest-neighbor interpolation]]\n*[[Sinc filter]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://www.antigrain.com/demo/index.html Anti-Grain Geometry examples]: <tt>image_filters.cpp</tt> shows comparisons of repeatedly resampling an image with various kernels.\n* [https://github.com/rwohleb/imageresampler imageresampler]: A public domain image resampling class in C++ with support for several windowed Lanczos filter kernels.\n\n[[Category:Signal processing]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Loop subdivision surface",
      "url": "https://en.wikipedia.org/wiki/Loop_subdivision_surface",
      "text": "[[Image:Loop_Subdivision_Icosahedron.svg|thumbnail|upright=0.7|alt=Loop subdivsion of an icosahedron|Loop Subdivision of an [[icosahedron]] (top) after one and after two refinement steps]]\nIn [[computer graphics]], '''Loop [[subdivision surface]]''' is an approximating subdivision scheme developed by Charles Loop in 1987 for [[Triangle mesh|triangular meshes]].\n\nLoop subdivision surfaces are defined recursively, dividing each triangle into four smaller ones.  The method is based on a quartic [[box spline]], which generate [[Parametric continuity|C<sup>2</sup>]] continuous limit surfaces everywhere except at extraordinary vertices where they are [[Parametric continuity|C<sup>1</sup>]] continuous.\n\n[[Geologists]] have also applied Loop Subdivision Surfaces to [[erosion]] on mountain faces, specifically in the [[Appalachians]]. {{Citation needed|date=January 2018}}\n\n== See also==\n* [[Geodesic polyhedron]]\n\n== References ==\n* Charles Loop: ''Smooth Subdivision Surfaces Based on Triangles'', M.S. Mathematics thesis, University of Utah, 1987 ([https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/thesis-10.pdf pdf]).\n* Jos Stam: ''Evaluation of Loop Subdivision Surfaces'', Computer Graphics Proceedings ACM SIGGRAPH 1998, ([http://www.dgp.toronto.edu/people/stam/reality/Research/pdf/loop.pdf pdf], [http://www.dgp.toronto.edu/~stam/reality/Research/SubdivEval/index.html downloadable eigenstructures]).\n* Antony Pugh, ''Polyhedra: a visual approach'', 1976, Chapter 6. ''The Geodesic Polyhedra of R. Buckminster Fuller and Related Polyhedra''\n\n== External links ==\n* Homepage of [http://charlesloop.com/ Charles Loop].\n\n[[Category:3D computer graphics]]\n[[Category:Multivariate interpolation]]\n\n{{compu-stub}}"
    },
    {
      "title": "Natural neighbor interpolation",
      "url": "https://en.wikipedia.org/wiki/Natural_neighbor_interpolation",
      "text": "[[image:Natural-neighbors-coefficients-example.png|200px|thumb|right|Natural neighbor interpolation with Sibson weights. The area of the green circles are the interpolating weights, ''w''<sub>''i''</sub>. The purple-shaded region is the new Voronoi cell, after inserting the point to be interpolated (black dot). The weights represent the intersection areas of the purple-cell with each of the seven surrounding cells.]]\n'''Natural neighbor interpolation''' is a method of [[spatial interpolation]], developed by [[Robin Sibson]].<ref>{{cite book |last=Sibson |first=R. |editor=V. Barnett |title=Interpolating Multivariate Data |year=1981 |publisher=John Wiley |location=Chichester |pages=21–36 |chapter=A brief description of natural neighbor interpolation (Chapter 2) }}</ref> The method is based on [[Voronoi diagram|Voronoi tessellation]] of a discrete set of spatial points. This has advantages over simpler methods of interpolation, such as [[nearest-neighbor interpolation]], in that it provides a smoother approximation to the underlying \"true\" function.\n\nThe basic equation is:\n:<math>G(x)=\\sum^n_{i=1}{w_i(x)f(x_i)}</math>\nwhere <math>G(x)</math> is the estimate at <math>x</math>, <math>w_i</math> are the weights and <math>f(x_i)</math> are the known data at <math>(x_i)</math>. The weights, <math>w_i</math>, are calculated by finding how much of each of the surrounding areas is \"stolen\" when inserting <math>x</math> into the tessellation.\n\n;Sibson weights\n:<math>w_i(\\mathbf{x})=\\frac{A(\\mathbf{x}_i)}{A(\\mathbf{x})}</math>\n\nwhere {{math|''A(x)''}} is the volume of the new cell centered in {{math|''x''}}, and {{math|''A(x''<sub>''i''</sub>'')''}} is the volume of the intersection between the new cell centered in {{math|''x''}} and the old cell centered in {{math|''x''<sub>''i''</sub>}}.\n\n[[image:Natural-neighbors-coefficients-Laplace-example.png|200px|thumb|right|Natural neighbor interpolation with Laplace weights. The interface {{math|''l(x''<sub>''i''</sub>'')''}} between the cells linked to {{math|''x''}} and {{math|''x''<sub>''i''</sub>}} is in blue, while the distance {{math|''d(x''<sub>''i''</sub>'')''}} between {{math|''x''}} and {{math|''x''<sub>''i''</sub>}} is in red.]]\n;Laplace weights<ref>{{cite journal|author1=N.H. Christ|author2=R. Friedberg, R.|author3=T.D. Lee|year=1982|title=Weights of links and plaquettes in a random lattice|journal=Nuclear Physics B|volume=210|number=3|pages=337-346}}</ref><ref>{{cite journal|author1=V.V. Belikov|author2=V.D. Ivanov|author3=V.K. Kontorovich|author4=S.A. Korytnik|author5=A.Y. Semenov|year=1997|title=The non-Sibsonian interpolation: A new method of interpolation of the values of a function on an arbitrary set of points|journal=Computational mathematics and mathematical physics|volume=37|number=1|pages=9-15}}</ref>\n:<math>w_i(\\mathbf{x})=\\frac{\\frac{l(\\mathbf{x}_i)}{d(\\mathbf{x}_i)}}{\\sum_{k=1}^n \\frac{l(\\mathbf{x}_k)}{d(\\mathbf{x}_k)}}</math>\n\nwhere {{math|''f(x''<sub>''i''</sub>'')''}} is the [[Measure_(mathematics)|measure]] of the interface between the cells linked to {{math|''x''}} and {{math|''x''<sub>''i''</sub>}} in the [[Voronoi diagram]] (length in 2D, surface in 3D) and {{math|''d(x''<sub>''i''</sub>'')''}}, the distance between {{math|''x''}} and {{math|''x''<sub>''i''</sub>}}.\n\n==See also==\n* [[Inverse distance weighting]]\n* [[Multivariate interpolation]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://dilbert.engr.ucdavis.edu/~suku/nem/nem_intro/node3.html Natural Neighbor Interpolation]\n* [https://web.archive.org/web/20160110032017/http://interpolate3d.googlecode.com/files/Report.pdf Implementation notes for natural neighbor, and comparison to other interpolation methods]\n* [http://alexbeutel.com/webgl/voronoi.html Interactive Voronoi diagram and natural neighbor interpolation visualization]\n* [https://github.com/innolitics/natural-neighbor-interpolation Fast, discrete natural neighbor interpolation in 3D on the CPU]\n\n[[Category:Multivariate interpolation]]\n\n\n{{Mathapplied-stub}}"
    },
    {
      "title": "Nearest neighbor value interpolation",
      "url": "https://en.wikipedia.org/wiki/Nearest_neighbor_value_interpolation",
      "text": "In mathematics applied to computer graphics, '''nearest neighbor value interpolation''' is an advanced method of image [[interpolation]].<ref>{{cite web|url=http://www.getcited.org/pub/103502379|title=Getcited|accessdate=May 1, 2012|deadurl=yes|archiveurl=https://archive.is/20120801235144/http://www.getcited.org/pub/103502379|archivedate=August 1, 2012|df=}}</ref><ref>{{cite journal|title=Nearest Neighbor Value Interpolation|year=2012| doi=10.14569/IJACSA.2012.030405 | volume=3 | journal=International Journal of Advanced Computer Science and Applications|arxiv=1211.1768}}</ref><ref>{{cite web|url=http://journals.indexcopernicus.com/index.php/ajol/karta.php?action=masterlist&id=5370/|title=Copernicus|accessdate=May 1, 2012|deadurl=yes|archiveurl=https://web.archive.org/web/20131215033318/http://journals.indexcopernicus.com/index.php/ajol/karta.php?action=masterlist&id=5370%2F|archivedate=December 15, 2013|df=}}</ref> This method uses the pixel value corresponding to the smallest [[absolute difference]] when a set of four known value pixels has no mode. Proposed by Olivier Rukundo in 2012 in his PhD dissertation,<ref>{{cite web|url=http://cdmd.cnki.com.cn/Article/CDMD-10487-1012361696.htm|title=China National Knowledge Infrastructure|accessdate=May 9, 2012}}</ref> the first work presented at the fourth International Workshop on Advanced [[Computational Intelligence]],<ref>{{cite web|url=http://www.iwaci.org/iwaci2011/|title=IWACI 2011|accessdate=October 19, 2011|deadurl=yes|archiveurl=https://archive.is/20120803062910/http://www.iwaci.org/iwaci2011/|archivedate=August 3, 2012|df=}}</ref> was based only on the pixel value corresponding to the smallest [[absolute difference]]<ref>{{cite web|url=http://www.mendeley.com/research/image-interpolation-based-pixel-value-corresponding-smallest-absolute-difference/|title=MENDELEY|accessdate= February 2012}}</ref> to achieve high resolution and visually pleasant image. This approach was since upgraded to deal with a wider class of image interpolation [[artifact (error)|artefacts]] which reduce the quality of image, and as a result, several future developments have emerged, drawing on various aspects of the pixel value corresponding to the smallest [[absolute difference]].\n\n==References==\n{{reflist}}\n\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "PDE surface",
      "url": "https://en.wikipedia.org/wiki/PDE_surface",
      "text": "'''PDE surfaces''' are used in [[geometric modelling]] and [[computer graphics]] for creating smooth surfaces conforming to a given boundary configuration. PDE surfaces use [[partial differential equations]] to generate a surface which usually satisfy a mathematical [[boundary value problem]].\n\nPDE surfaces were first introduced into the area of [[geometric modelling]] and [[computer graphics]] by two British mathematicians, Malcolm Bloor and Michael Wilson. \n\n==Technical details==\nThe PDE method involves generating a surface for some boundary by means of solving an [[Elliptic operator|elliptic partial differential equation]] of the form\n\n:<math>\n\\left( \\frac{\\partial ^{2}}{ \\partial\n u^{2}} + a^{2}\\frac {\\partial^{2}}{\\partial v^{2}} \\right)^{2}\nX(u,v) = 0.\n</math>\n\nHere <math>X(u,v) </math> is a function parameterised by the two [[parameter]]s <math> u </math> and <math> v </math> such that <math>X(u,v) = (x(u,v), y(u,v), z(u,v)) </math> where <math> x </math>, <math> y </math> and <math> z </math> are the usual [[Cartesian coordinate system|cartesian coordinate]] space. The boundary conditions on the function <math>X(u,v) </math> and its \nnormal derivatives <math> \\partial{X}/\\partial{{n}} </math>  \nare imposed at the edges of the surface patch.\n\nWith the above formulation it is notable that the elliptic partial differential operator in the above PDE represents a smoothing process in which the value of the function at any point on the surface is, in some sense, a weighted average of the surrounding \nvalues. In this way a surface is obtained as a smooth transition between \nthe chosen set of [[boundary condition]]s.  The parameter <math> a </math> is a special design parameter which controls the relative smoothing of the surface in the <math> u </math> and <math> v </math> directions.\n\nWhen <math> a=1 </math>, the PDE is the [[biharmonic equation]]: <math> X_{uuuu} + 2X_{uuvv} + X_{vvvv}=0 </math>.  The biharmonic equation is the equation produced by applying the [[Euler–Lagrange equation|Euler-Lagrange equation]] to the simplified [[thin plate energy functional]] <math> X_{uu}^2 + 2X_{uv}^2 + X_{vv}^2 </math>.  So solving the PDE with <math> a=1 </math> is equivalent to minimizing the thin plate energy functional subject to the same boundary conditions.\n\n==Applications==\nPDE surfaces can be used in many application areas. These include [[computer-aided design]], interactive design,  parametric design, [[computer animation]], computer-aided physical analysis and design optimisation.\n\n==References==\n#M.I.G. Bloor and M.J. Wilson, ''Generating Blend Surfaces using Partial Differential Equations'', Computer Aided Design, 21(3), 165-171, (1989).\n#[[Hassan Ugail|H. Ugail]], M.I.G. Bloor, and M.J. Wilson, ''Techniques for Interactive Design Using the PDE Method'', [[ACM Transactions on Graphics]], 18(2), 195-212, (1999).  \n#J. Huband, W. Li and R. Smith, ''An Explicit Representation of Bloor-Wilson PDE Surface Model by using Canonical Basis for Hermite Interpolation'', Mathematical Engineering in Industry, 7(4), 421-33 (1999). \n#H. Du and H. Qin, ''Direct Manipulation and Interactive Sculpting of PDE surfaces'', Computer Graphics Forum, 19(3), C261-C270, (2000).\n#H. Ugail, ''Spine Based Shape Parameterisations for PDE surfaces'', Computing, 72, 195--204, (2004).\n#L. You, P. Comninos, J.J. Zhang, ''PDE Blending Surfaces with C2 Continuity'', Computers and Graphics, 28(6), 895-906, (2004).\n\n==External links==\n* [http://www.inf.brad.ac.uk/research/dve-sbd.php Simulation based design, DVE research (University of Bradford, UK)]. (A java applet demonstrating the properties of PDE surfaces) \n* [http://www.maths.leeds.ac.uk/Applied/ Dept Applied Mathematics, University of Leeds] details on Bloor and Wilsons work.\n\n[[Category:Surfaces]]\n[[Category:Computer graphics]]\n[[Category:Elliptic partial differential equations]]\n[[Category:Computer-aided design]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Subdivision surface",
      "url": "https://en.wikipedia.org/wiki/Subdivision_surface",
      "text": "{{citation style|date=August 2014}}\nA '''subdivision surface''', in the field of [[3D computer graphics]], is a method of representing a [[smooth surface]] via the specification of a coarser [[piecewise linear manifold|piecewise linear]] [[polygon mesh]].  The smooth surface can be calculated from the coarse mesh as the [[Limit of a sequence|limit]] of [[recursive subdivision]] of each polygonal [[Face (geometry)|face]] into smaller faces that better approximate the smooth surface.\n\n[[File:Catmull-Clark subdivision of a cube.svg|thumbnail|First three steps of [[Catmull–Clark subdivision surface|Catmull–Clark]] subdivision of a cube with subdivision surface below]]\n\n==Overview==\n\nSubdivision surfaces are defined recursively. The process starts with a given polygonal mesh. A '''refinement scheme''' is then applied to this mesh. This process takes that mesh and subdivides it, creating new [[vertex (geometry)|vertices]] and new faces. The positions of the new vertices in the mesh are computed based on the positions of nearby old vertices.  In some refinement schemes, the positions of old vertices might also be altered (possibly based on the positions of new vertices).\n\nThis process produces a finer mesh than the original one, containing more polygonal faces. This resulting mesh can be passed through the same refinement scheme again and so on.\n\nThe limit subdivision surface is the surface produced from this process being iteratively applied infinitely many times. In practical use however, this algorithm is only applied a limited number of times. The limit surface can also be calculated directly for most subdivision surfaces using the technique of [[Jos Stam]],<ref name=Stam/> which eliminates the need for recursive refinement. Subdivision surfaces and [[T-Spline (mathematics)|T-Splines]] are competing technologies. Mathematically, subdivision surfaces are [[spline (mathematics)|spline]] surfaces with [[Mathematical singularity|singularities]].<ref name=PR/>\n\n==Refinement schemes==\n\nSubdivision surface refinement schemes can be broadly classified into two categories: interpolating and approximating. Interpolating schemes are required to match the original position of vertices in the original mesh. Approximating schemes are not; they can and will adjust these positions as needed. In general, approximating schemes have greater smoothness, but editing applications that allow users to set exact surface constraints require an optimization step.\n\nThere is another division in subdivision surface schemes as well, the type of polygon that they operate on. Some function for quadrilaterals (quads), while others operate on triangles.\n\n===Approximating schemes===\n\nApproximating means that the limit surfaces approximate the initial meshes and that after subdivision, the newly generated [[Control points (computer graphics)|control points]] are not in the limit surfaces. Examples of approximating subdivision schemes are:\n* [[Catmull–Clark subdivision surface|Catmull–Clark]] (1978) generalized [[bi-cubic uniform B-spline]] to produce their subdivision scheme. For arbitrary initial meshes, this scheme generates limit surfaces that are [[Parametric continuity|C<sup>2</sup>]] continuous everywhere except at extraordinary vertices where they are [[Parametric continuity|C<sup>1</sup>]] continuous (Peters and Reif 1998).\n* [[Doo–Sabin subdivision surface|Doo–Sabin]] – The second subdivision scheme was developed by Doo and Sabin (1978) who successfully extended Chaikin's corner-cutting method for curves to surfaces. They used the analytical expression of [[bi-quadratic uniform B-spline]] surface to generate their subdivision procedure to produce [[Parametric continuity|C<sup>1</sup>]] limit surfaces with arbitrary topology for arbitrary initial meshes.\n* [[Loop subdivision surface|Loop]], Triangles – Loop (1987) proposed his subdivision scheme based on a quartic [[box spline]] of six direction vectors to provide a rule to generate [[Parametric continuity|C<sup>2</sup>]] continuous limit surfaces everywhere except at extraordinary vertices where they are [[Parametric continuity|C<sup>1</sup>]] continuous.\n* [[Mid-Edge subdivision scheme]] – The mid-edge subdivision scheme was proposed independently by Peters–Reif (1997) and Habib–Warren (1999). The former used the midpoint of each edge to build the new mesh. The latter used a four-directional [[box spline]] to build the scheme. This scheme generates [[Parametric continuity|C<sup>1</sup>]] continuous limit surfaces on initial meshes with arbitrary topology.\n* [[√3 subdivision scheme]] – This scheme has been developed by Kobbelt (2000): it handles arbitrary triangular meshes, it is [[Parametric continuity|C<sup>2</sup>]] continuous everywhere except at extraordinary vertices where it is [[Parametric continuity|C<sup>1</sup>]] continuous and it offers a natural adaptive refinement when required. It exhibits at least two specificities: it is a ''Dual'' scheme for triangle meshes and it has a slower refinement rate than primal ones.\n\n===Interpolating schemes===\nAfter subdivision, the control points of the original mesh and the new generated control points are interpolated on the limit surface. The earliest work was the [[butterfly scheme]] by [[Nira Dyn|Dyn]], Levin and Gregory (1990), who extended the four-point interpolatory subdivision scheme for curves to a subdivision scheme for surface. Zorin, Schröder and Sweldens (1996) noticed that the butterfly scheme cannot generate smooth surfaces for irregular triangle meshes and thus modified this scheme. Kobbelt (1996) further generalized the four-point interpolatory subdivision scheme for curves to the tensor product subdivision scheme for surfaces. Deng and Ma (2013) further generalized the four-point interpolatory subdivision scheme to arbitrary degree.\n* [[Butterfly subdivision surfaces|Butterfly]], Triangles – named after the scheme's shape\n* [[Midedge]], Quads\n* [[Leif Kobbelt|Kobbelt]], Quads – a variational subdivision method that tries to overcome uniform subdivision drawbacks\n* [[Deng-Ma]], Quads – 2n point subdivision generalized to arbitrary odd degree<ref>{{Cite journal | doi=10.1145/2487228.2487231| title=A unified interpolatory subdivision scheme for quadrilateral meshes| journal=ACM Transactions on Graphics| volume=32| issue=3| pages=1–11| year=2013| last1=Deng| first1=Chongyang| last2=Ma| first2=Weiyin}}</ref>\n\n==Editing a subdivision surface==\n\nSubdivision surfaces can be naturally edited at different levels of subdivision. Starting with basic shapes you can use binary operators to create the correct topology. Then edit the coarse mesh to create the basic shape, then edit the offsets for the next subdivision step, then repeat this at finer and finer levels. You can always see how your edits affect the limit surface via [[GPU]] evaluation of the surface.\n\nA surface designer may also start with a scanned in object or one created from a NURBS surface. The same basic optimization algorithms are used to create a coarse base mesh with the correct topology and then add details at each level so that the object may be edited at different levels. These types of surfaces may be difficult to work with because the base mesh does not have control points in the locations that a human designer would place them. With a scanned object this surface is easier to work with than a raw triangle mesh, but a NURBS object probably had well laid out control points which behave less intuitively after the conversion than before.\n\n==Key developments==\n{{primary sources|section|date=August 2014}}\n* 1978: Subdivision surfaces were discovered simultaneously by [[Edwin Catmull]] and [[James H. Clark|Jim Clark]] (see [[Catmull–Clark subdivision surface]]). In the same year, Daniel Doo and Malcom Sabin published a paper building on this work (see [[Doo–Sabin subdivision surface]]).\n* 1995: [[Ulrich Reif]] characterized subdivision surfaces near extraordinary vertices<ref name=Reif>{{Cite journal | last1 = Reif | first1 = U. | title = A unified approach to subdivision algorithms near extraordinary vertices | doi = 10.1016/0167-8396(94)00007-F | journal = Computer Aided Geometric Design | volume = 12 | issue = 2 | pages = 153–201 | year = 1995 | pmid =  | pmc = }}</ref> by treating them as splines with singularities.<ref name=PR>{{Cite book | last1 = Peters | first1 = J. R. | last2 = Reif | first2 = U. | doi = 10.1007/978-3-540-76406-9 | title = Subdivision Surfaces | series = Geometry and Computing | volume = 3 | year = 2008 | isbn = 978-3-540-76405-2 | pmid =  | pmc = }}</ref>\n* 1998: [[Jos Stam]] contributed a method for exact evaluation for ''Catmull–Clark'' and ''Loop'' subdivision surfaces under arbitrary parameter values.<ref name=Stam>{{cite book| last1 = Stam | first1 = J. | author1-link = Jos Stam| chapter = Exact evaluation of Catmull-Clark subdivision surfaces at arbitrary parameter values| doi = 10.1145/280814.280945| title = Proceedings of the 25th annual conference on Computer graphics and interactive techniques - SIGGRAPH '98| chapter-url = http://www.dgp.toronto.edu/people/stam/reality/Research/pdf/sig98.pdf| pages = 395–404| year = 1998| isbn = 978-0-89791-999-9| pmid = | pmc = | citeseerx = 10.1.1.20.7798 }} ([http://www.dgp.toronto.edu/~stam/reality/Research/SubdivEval/index.html downloadable eigenstructures])</ref>\n\n==See also==\n* [[CGAL]], an open source geometry library that implements subdivision (see 3d Polyhedra)\n* [[CGoGN]], an open source geometry library that implements subdivision and hierarchical/multiresolution data structures\n* [[Level of detail]]\n* [[OpenSubdiv]], an open source subdivision surface library released by [[Pixar]]\n* [[Smoothing]]\n* [[T-vertices]], a problem resulting from a finer mesh attaching to a coarser mesh.\n== References ==\n{{reflist}}\n{{refbegin}}\n* {{cite journal| last1 = Peters | first1 = J.| last2 = Reif | first2 = U.| doi = 10.1145/263834.263851| title = The simplest subdivision scheme for smoothing polyhedra| journal = [[ACM Transactions on Graphics]]| volume = 16| issue = 4| pages = 420–431| date=October 1997 | pmid = | pmc = | citeseerx = 10.1.1.33.1310}}\n* {{cite journal| last1 = Habib | first1 = A.| last2 = Warren | first2 = J.| doi = 10.1016/S0167-8396(98)00045-4| title = Edge and vertex insertion for a class C<sup>1</sup> of subdivision surfaces| journal = Computer Aided Geometric Design| volume = 16| issue = 4| pages = 223–247| date=May 1999 | pmid = | pmc = | title-link = Parametric continuity}}\n* {{cite book| last1 = Kobbelt | first1 = L.| chapter = √3-subdivision| doi = 10.1145/344779.344835| title = Proceedings of the 27th annual conference on Computer graphics and interactive techniques - SIGGRAPH '00| pages = 103–112| year = 2000| isbn = 978-1-58113-208-3| pmid = | pmc = }}\n{{refend}}\n\n== Further reading ==\n* {{cite book|author1=Joe D. Warren|author2=Henrik Weimer|title=Subdivision Methods for Geometric Design: A Constructive Approach|year=2002|publisher=Morgan Kaufmann|isbn=978-1-55860-446-9}}\n* {{cite book|author1=Jörg Peters|author2=Ulrich Reif|title=Subdivision Surfaces|year=2008|publisher=Springer Science & Business Media|isbn=978-3-540-76405-2}}\n* {{cite book|author1=Lars-Erik Andersson|author2=Neil Frederick Stewart|title=Introduction to the Mathematics of Subdivision Surfaces|year=2010|publisher=SIAM|isbn=978-0-89871-761-7}}\n\n== External links ==\n{{external links|date=July 2016}}\n* [https://web.archive.org/web/20150905190444/http://www.pixar.com/short_films/Theatrical-Shorts/Geri's-Game Geri's Game]: Oscar-winning animation by [[Pixar]] completed in 1997 that introduced subdivision surfaces (along with cloth simulation)\n* [http://www.multires.caltech.edu/pubs/sig99notes.pdf Subdivision for Modeling and Animation]: tutorial, [[SIGGRAPH]] 1999 course notes\n* [http://www.hakenberg.de/subdivision/ultimate_consumer.htm Subdivision of Surface and Volumetric Meshes]: software to perform subdivision using the most popular schemes\n* [http://www.cgal.org/Pkg/SurfaceSubdivisionMethods3 Surface Subdivision Methods] in CGAL, the Computational Geometry Algorithms Library\n* [http://cgogn.unistra.fr Surface and Volumetric Subdivision Meshes], hierarchical/multiresolution data structures in CGoGN\n* [https://bitbucket.org/rukletsov/b Modified Butterfly method implementation in C++]\n* [http://hakenberg.de/subdivision/enclosed_volume.htm Volume Enclosed by Subdivision Surfaces]\n* {{cite AV media\n  | url     = http://on-demand.gputechconf.com/gtc/2014/video/S4856-subdivision-surfaces-industry-standard.mp4\n  | author1 = Bill Polson\n  | author2 = David G. Yu\n  | date    = March 2014\n  | title   = OpenSubdiv\n  | medium  = Slideshow presentation\n  | series  = Nvidia GPU technology conference\n  | format  = mp4\n  }}\n* [https://threejs.org/examples/webgl_modifier_subdivision.html Interactive web-based demo of subdivision surfaces] using the Loop subdivision scheme. Part of the [[Three.js]] examples library\n\n{{DEFAULTSORT:Subdivision Surface}}\n[[Category:3D computer graphics]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Thin plate spline",
      "url": "https://en.wikipedia.org/wiki/Thin_plate_spline",
      "text": "'''Thin plate splines''' ('''TPS''') are a [[spline (mathematics)|spline]]-based technique for data [[interpolation]] and [[smoothing]]. They were introduced to [[geometric design]] by Duchon.<ref name=\"Duchon76\" /> They are an important special case of a [[polyharmonic spline]]. Robust Point Matching (RPM) is a common extension and shortly known as the TPS-RPM algorithm.<ref name=\"PhDThesis_Chui\" />\n\n==Physical analogy==\nThe name ''thin plate spline'' refers to a physical analogy involving the bending of a thin sheet of metal. Just as the metal has rigidity, the TPS fit resists bending also, implying a penalty involving the smoothness of the fitted surface. In the physical setting, the deflection is in the <math>z</math> direction, orthogonal to the plane. In order to apply this idea to the problem of coordinate transformation, one interprets the lifting of the plate as a displacement of the <math>x</math> or <math>y</math> coordinates within the plane. In 2D cases, given a set of <math>K</math> corresponding points, the TPS warp is described by <math>2(K+3)</math> parameters which include 6 global affine motion parameters and <math>2K</math> coefficients for correspondences of the control points. These parameters are computed by solving a linear system, in other words, TPS has a [[closed-form solution]].\n\n==Smoothness measure==\n\nThe TPS arises from consideration of the integral of the square of the second derivative—this forms its smoothness measure. In the case where <math>x</math> is two dimensional, for interpolation, the TPS fits a mapping function <math>f(x)</math> between corresponding point-sets <math>\\{y_i\\}</math> and <math>\\{x_i\\}</math> that minimizes the following energy function: \n:<math>\n\tE_{\\mathrm{tps}}(f) = \\sum_{i=1}^K \\|y_i - f(x_i) \\|^2\n</math>\n\nThe smoothing variant, correspondingly, uses a tuning parameter <math>\\lambda</math> to control the rigidity of the deformation, balancing the aforementioned criterion with the measure of goodness of fit, thus minimizing:\n\n:<math>\n\tE_{\\mathrm{tps},\\mathrm{smooth}}(f) = \\sum_{i=1}^K \\|y_i - f(x_i) \\|^2 + \\lambda \\iint\\left[\\left(\\frac{\\partial^2 f}{\\partial x_1^2}\\right)^2 + 2\\left(\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}\\right)^2 + \\left(\\frac{\\partial^2 f}{\\partial x_2^2}\\right)^2 \\right] \\textrm{d} x_1 \\, \\textrm{d}x_2\n</math>\n\nFor this variational problem, it can be shown that there exists a unique minimizer <math>f</math> .<ref name=\"Wahba90\" /> The [[finite element]] discretization of this variational problem, the method of [[elastic map]]s, is used for [[data mining]] and [[nonlinear dimensionality reduction]].\n\n==Radial basis function==\n{{Main article|Radial basis function}}\n\nThe thin plate spline has a natural representation in terms of radial basis functions. Given a set of control points <math>\\{c_{i}, i = 1,2, \\ldots,K\\}</math>, a radial basis function defines a spatial mapping which maps any location <math>x</math> in space to a new location <math>f(x)</math>, represented by\n:<math>\n\tf(x) = \\sum_{i = 1}^K w_{i}\\varphi(\\left\\| x - c_{i}\\right\\|)\n</math>\nwhere <math>\\left\\|\\cdot\\right\\|</math> denotes the usual [[Norm (mathematics)|Euclidean norm]] and <math>\\{w_{i}\\}</math> is a set of mapping coefficients. The TPS corresponds to the radial basis kernel <math>\\varphi(r) = r^2 \\log r</math>.\n\n===Spline===\nSuppose the points are in 2 dimensions (<math>D = 2</math>). One can use ''homogeneous coordinates'' for the point-set where a point <math>y_{i}</math> is represented as a vector <math>(1, y_{ix}, y_{iy})</math>. The unique minimizer <math>f</math> is parameterized by <math>\\alpha</math> which consists of two matrices <math>d</math> and <math>c</math> (<math>\\alpha = \\{d,c\\}</math>).\n:<math>\n\tf_{tps}(z, \\alpha) = f_{tps}(z, d, c) = z\\cdot d + \\phi(z) \\cdot c = z\\cdot d + \\sum_{i = 1}^K \\phi_i(z) c_i\t\n</math>\nwhere d is a <math>(D+1)\\times(D+1)</math> matrix representing the affine transformation (hence <math>z</math> is a <math>1\\times (D+1)</math> vector) and c is a <math>K\\times (D+1)</math> warping coefficient matrix representing the non-affine deformation. The kernel function <math>\\phi(z)</math> is a <math>1\\times K</math> vector for each point <math>z</math>, where each entry <math>\\phi_i(z) = \\|z - x_i\\|^2 \\log \\|z - x_i\\|</math>.  Note that for TPS, the control points <math>\\{c_i\\}</math> are chosen to be the same as the set of points to be warped <math>\\{x_i\\}</math>, so we already use <math>\\{x_i\\}</math> in the place of the control points.\n\nIf one substitutes the solution for <math>f</math>, <math>E_{tps}</math> becomes:\n:<math>\n\tE_{tps}(d,c) = \\|Y - Xd - \\Phi c\\|^2 + \\lambda  c^T\\Phi c\n</math>\nwhere <math>Y</math> and <math>X</math> are just concatenated versions of the point coordinates <math>y_i</math> and <math>x_i</math>, and <math>\\Phi</math> is a <math>(K\\times K)</math> matrix formed from the <math>\\phi (\\|x_i - x_j\\|)</math>. Each row of each newly formed matrix comes from one of the original vectors. The matrix <math>\\Phi</math> represents the TPS kernel. Loosely speaking, the TPS kernel contains the information about the point-set's internal structural relationships. When it is combined with the warping coefficients <math>c</math>, a non-rigid warping is generated.\n\nA nice property of the TPS is that it can always be decomposed into a global affine and a local non-affine component. Consequently, the TPS smoothness term is solely dependent on the non-affine components. This is a desirable property, especially when compared to other splines, since the global pose parameters included in the affine transformation are not penalized.\n\n==Applications==\nTPS has been widely used as the non-rigid transformation model in image\nalignment and shape matching.<ref name=\"Bookstein89\" />\nAn additional application is the analysis and comparisons of archaeological findings in 3D<ref name=\"VISAPP19\" /> and was implemented for [[Polygon mesh|triangular meshes]] in the [[GigaMesh Software Framework]]<ref name=\"GigaMeshTutorialTPS\" />.\n\nThe thin plate spline has a number of properties which have contributed to its popularity:\n#It produces smooth surfaces, which are infinitely differentiable.\n#There are no free parameters that need manual tuning.\n#It has closed-form solutions for both warping and parameter estimation.\n#There is a physical explanation for its energy function.\n\nHowever, note that splines already in one dimension can cause severe \"overshoots\". In 2D such effects can be much more critical, because TPS are not objective.\n\n==See also==\n*[[Inverse distance weighting]]\n*[[Radial basis function]]\n*[[Subdivision surface]] (emerging alternative to spline-based surfaces)\n*[[Elastic map]] (a discrete version of the thin plate approximation for [[manifold learning]])\n*[[Spline (mathematics)|Spline]] \n*[[Polyharmonic spline]] (the thin plate spline is a special case of a polyharmonic spline)\n*[[Smoothing spline]]\n\n== References ==\n<references>\n<ref name=\"Duchon76\">J. Duchon, 1976, Splines minimizing rotation invariant semi-norms in Sobolev spaces. pp 85–100, In: Constructive Theory of Functions of Several Variables, Oberwolfach 1976, W. Schempp and [[Karl Longin Zeller|K. Zeller]], eds., Lecture Notes in Math., Vol. 571, Springer, Berlin, 1977. [[Digital object identifier|doi]]:[https://doi.org/10.1007/BFb0086566 10.1007/BFb0086566] </ref>\n<ref name=\"Bookstein89\">{{cite journal|last1=Bookstein|first1=F. L.|title=Principal warps: thin plate splines and the decomposition of deformations|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|date=June 1989|volume=11|issue=6|pages=567–585|doi=10.1109/34.24792}}</ref>\n<ref name=\"VISAPP19\">{{citation|surname1=Bogacz|given1=Bartosz|given2=Nikolas|surname2=Papadimitriou|given3=Diamantis|surname3=[[Diamantis Panagiotopoulos|Panagiotopoulos]]|surname4=Mara|given4=Hubert|periodical=Proc. of the 14th International Conference on Computer Vision Theory and Application (VISAPP)|title=Recovering and Visualizing Deformation in 3D Aegean Sealings|location=Prague, Czech Republic|date=2019|url=http://insticc.org/node/TechnicalProgram/visigrapp/presentationDetails/73858|access-date=28 March 2019}}</ref>\n<ref name=\"Wahba90\">{{citation|surname=[[Grace Wahba|Wahba]]|given=Grace|title=Spline models for observational data|location=Philadelphia, PA, USA|date=1990|doi=10.1137/1.9781611970128\n|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.470.5213&rep=rep1&type=pdf|accessdate=3 March 2019|publisher=[[Society for Industrial and Applied Mathematics]] (SIAM)}}</ref>\n<ref name=\"PhDThesis_Chui\">{{citation|surname=Chui|given=Haili|title=Non-Rigid Point Matching: Algorithms, Extensions and Applications|location=Yale University, New Haven, CO, USA|date=2001\n|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.6855|accessdate=3 March 2019}}</ref>\n<ref name=\"GigaMeshTutorialTPS\">{{cite web |title=Tutorial No. 13: Apply TPS-RPM Tranformation |url=https://gigamesh.eu/?page=tutorials&topic=13._Apply_TPS_RPM_Transformation |website=GigaMesh Software Framework|accessdate=3 March 2019}}</ref>\n</references>\n\n==External links==\n*[http://www-cse.ucsd.edu/classes/fa01/cse291/hhyu-presentation.pdf Explanation for a simplified variation problem]\n*[http://mathworld.wolfram.com/ThinPlateSpline.html TPS at MathWorld]\n*[http://elonen.iki.fi/code/tpsdemo/index.html TPS in C++]\n*[http://launchpad.net/templatedtps TPS in templated C++]\n*[http://www.dg1an3.net/2015/03/warptps.html TPS interactive morphing demo]\n*[https://cran.r-project.org/web/packages/Momocs/index.html TPS in R]\n\n[[Category:Splines (mathematics)]]\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Trend surface analysis",
      "url": "https://en.wikipedia.org/wiki/Trend_surface_analysis",
      "text": "[[File:RSS troposphere stratosphere trend.png|thumb|Worldwide temperature trend analysis]]'''Trend surface analysis''' is a mathematical technique used in environmental sciences (archeology, geology, soil science, etc.). Trend surface analysis (also called trend surface mapping) is a method based on low-order polynomials of spatial coordinates for estimating a regular grid of points from scattered observations - for example, from archeological finds or from soil survey.\n\n[[Category:Methods in archaeology]]\n[[Category:Multivariate interpolation]]\n\n\n{{mathapplied-stub}}\n{{archaeology-stub}}"
    },
    {
      "title": "Tricubic interpolation",
      "url": "https://en.wikipedia.org/wiki/Tricubic_interpolation",
      "text": "<!-- Tricubic interpolation uses 64 (4x4x4) sample points, and not just the 8 shown in this graphic:\n[[Image:Enclosing points.svg|right|thumb|3D interpolation tries to assign a value at the red point <math>C</math> given values at the blue corner points.]]\n-->\nIn the [[mathematical]] subfield [[numerical analysis]], '''tricubic interpolation''' is a method for obtaining values at arbitrary points in [[Three-dimensional space|3D space]] of a function defined on a [[regular grid]].  The approach involves approximating the function locally by an expression of the form \n:<math>f(x,y,z)=\\sum_{i=0}^3 \\sum_{j=0}^3 \\sum_{k=0}^3 a_{ijk} x^i y^j z^k.</math>\n\nThis form has 64 coefficients <math>a_{ijk}</math>; requiring the function to have a given value or given [[directional derivative]] at a point places one linear constraint on the 64 coefficients.\n\nThe term ''tricubic interpolation'' is used in more than one context; some experiments measure both the value of a function and its spatial derivatives, and it is desirable to interpolate preserving the values and the measured derivatives at the grid points.  Those provide 32 constraints on the coefficients, and another 32 constraints can be provided by requiring smoothness of higher derivatives.<ref name=\":0\">[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.7835 Tricubic Interpolation in Three Dimensions (2005), by F. Lekien, J. Marsden, Journal of Numerical Methods and Engineering]</ref>\n\nIn other contexts, we can obtain the 64 coefficients by considering a 3&times;3&times;3 grid of small cubes surrounding the cube inside which we evaluate the function, and fitting the function at the 64 points on the corners of this grid.\n\nThe [[cubic interpolation]] article indicates that the method is equivalent to a sequential application of one-dimensional cubic interpolators. Let <math>\\mathrm{CINT}_x(a_{-1}, a_0, a_1, a_2)</math> be the value of a monovariable cubic polynomial (e.g. constrained by values, <math>a_{-1}</math>, <math>a_{0}</math>, <math>a_{1}</math>, <math>a_{2}</math> from consecutive grid points) evaluated at <math>x</math>. In many useful cases, these cubic polynomials have the form <math>\\mathrm{CINT}_x(u_{-1}, u_0, u_1, u_2) = \\mathbf{v}_x \\cdot \\left( u_{-1}, u_0, u_1, u_2 \\right)</math> for some vector <math>\\mathbf{v}_x</math> which is a function of <math>x</math> alone. The tricubic interpolator is equivalent to:\n\n<math>\n\\begin{align}\ns(i,j,k) & {} = \\text{The value at grid point } (i,j,k)\\\\\nt(i,j,z) & {} = \\mathrm{CINT}_z\\left( s(i,j,-1), s(i,j,0), s(i,j,1), s(i,j,2)\\right) \\\\\nu(i,y,z) & {} = \\mathrm{CINT}_y\\left( t(i,-1,z), t(i,0,z), t(i,1,z), t(i,2,z)\\right) \\\\\nf(x,y,z) & {} = \\mathrm{CINT}_x\\left( u(-1,y,z), u(0,y,z), u(1,y,z), u(2,y,z)\\right)\n\\end{align}\n</math>\nwhere <math>i,j,k\\in\\{-1,0,1,2\\}</math> and <math>x,y,z\\in[0,1]</math>.\n\nAt first glance, it might seem more convenient to use the 21 calls to <math>\\mathrm{CINT}</math> described above instead of the <math>{64 \\times 64}</math> matrix described in Lekien and Marsden.<ref name=\":0\" /> However, a proper implementation using a sparse format for the matrix (that is fairly sparse) makes the latter more efficient. This aspect is even much more pronounced when interpolation is needed at several locations inside the same cube. In this case, the <math>{64 \\times 64}</math> matrix is used once to compute the interpolation coefficients for the entire cube. The coefficients are then stored and used for interpolation at any location inside the cube. In comparison, sequential use of one-dimensional integrators <math>\\mathrm{CINT}_x</math> performs extremely poorly for repeated interpolations because each computational step must be repeated for each new location.\n\n==See also==\n* [[Cubic interpolation]]\n* [[Bicubic interpolation]]\n* [[Trilinear interpolation]]\n\n==References==\n<references/>\n\n==External links==\n* [http://www.paulinternet.nl/?page=bicubic Java/C++ implementation of tricubic interpolation]\n* [https://github.com/deepzot/likely/blob/master/likely/TriCubicInterpolator.h C++ implementation of tricubic interpolation]\n* [https://github.com/danielguterding/pytricubic Python implementation]\n* [https://github.com/DurhamDecLab/ARBInterp NumPy implementation]\n* [http://einspline.sourceforge.net/index.shtml] einspline library\n\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Trilinear interpolation",
      "url": "https://en.wikipedia.org/wiki/Trilinear_interpolation",
      "text": "'''Trilinear interpolation''' is a method of [[multivariate interpolation]] on a [[Three dimensional space|3-dimensional]] [[regular grid]].  It approximates the value of a function at an intermediate point <math>(x, y, z)</math> within the local axial rectangular [[prism (geometry)|prism]] linearly, using function data on the lattice points.   For an arbitrary, [[unstructured grid|unstructured mesh]] (as used in [[finite element]] analysis), other methods of interpolation must be used; if all the mesh elements are [[tetrahedron|tetrahedra]] (3D [[simplex|simplices]]), then [[barycentric_coordinates_(mathematics)#Barycentric_coordinates_on_tetrahedra|barycentric coordinates]] provide a straightforward procedure.\n\nTrilinear interpolation is frequently used in [[numerical analysis]], [[data analysis]], and [[computer graphics]].\n\n== Compared to linear and bilinear interpolation ==\n\nTrilinear interpolation is the extension of [[linear interpolation]], which operates in spaces with [[dimension]] <math>D=1</math>, and [[bilinear interpolation]], which operates with dimension <math>D=2</math>, to dimension <math>D=3</math>. These interpolation schemes all use polynomials of order 1, giving an accuracy of order 2, and it requires <math>2^D = 8</math> adjacent pre-defined values surrounding the interpolation point. There are several ways to arrive at trilinear interpolation, which is equivalent to 3-dimensional [[tensor]] [[B-spline]] interpolation of order 1, and the trilinear interpolation operator is also a tensor product of 3 linear interpolation operators.\n\n==Method==\n[[Image:Enclosing_points.svg|right|thumb|Eight corner points on a cube surrounding the interpolation point C]]\n[[Image:3D_interpolation2.svg|right|thumb|Depiction of 3D interpolation]]\n[[File:Trilinear_interpolation_visualisation.svg|thumb|A geometric visualisation of trilinear interpolation. The product of the value at the desired point and the entire volume is equal to the sum of the products of the value at each corner and the partial volume diagonally opposite the corner.]]\n\nOn a periodic and cubic lattice, let <math>x_d</math>, <math>y_d</math>, and <math>z_d</math> \nbe the differences between each of <math>x</math>, <math>y</math>, <math>z</math> and the smaller coordinate related, that is:\n\n:<math> \\ x_d = (x - x_0)/(x_1 - x_0)</math>\n:<math> \\ y_d = (y - y_0)/(y_1 - y_0)</math>\n:<math> \\ z_d = (z - z_0)/(z_1 - z_0)</math>\n\nwhere <math> x_0 </math> indicates the lattice point below <math> x </math>, and <math>  x_1 </math> indicates the lattice point above <math> x </math> and similarly for\n<math>y_0, y_1, z_0</math> and <math>z_1</math>.\n\nFirst we interpolate along <math>x</math> (imagine we are \"pushing\" the face of the cube defined by <math>C_{0jk}</math> to the opposing face, defined by <math>C_{1jk}</math>), giving:\n: <math> \\ c_{00} = c_{000}  (1 - x_d) + c_{100} x_d </math>\n: <math> \\ c_{01} = c_{001}  (1 - x_d) + c_{101} x_d </math>\n: <math> \\ c_{10} = c_{010}  (1 - x_d) + c_{110} x_d </math>\n: <math> \\ c_{11} = c_{011}  (1 - x_d) + c_{111} x_d </math>\nWhere <math>c_{000}</math> means the function value of <math> (x_0,y_0,z_0). </math> Then we interpolate these values (along <math>y</math>, \"pushing\" from <math>C_{i0k}</math> to <math>C_{i1k}</math>), giving:\n: <math> \\ c_0 = c_{00}(1 - y_d) + c_{10}y_d</math>\n: <math> \\ c_1 = c_{01}(1 - y_d) + c_{11}y_d</math>\nFinally we interpolate these values along <math>z</math> (walking through a line):\n:<math> \\ c = c_0(1 - z_d) + c_1z_d .</math>\n\nThis gives us a predicted value for the point.\n\nThe result of trilinear interpolation is independent of the order of the interpolation steps along the three axes: any other order, for instance along <math>x</math>, then along <math>y</math>, and finally along <math>z</math>, produces the same value.\n\nThe above operations can be visualized as follows: First we find the eight corners of a cube that surround our point of interest. These corners have the values <math>c_{000}</math>, <math>c_{100}</math>, <math>c_{010}</math>, <math>c_{110}</math>, <math>c_{001}</math>, <math>c_{101}</math>, <math>c_{011}</math>, <math>c_{111}</math>.\n\nNext, we perform linear interpolation between <math>c_{000}</math> and <math>c_{100}</math> to find <math>c_{00}</math>, <math>c_{001}</math> and <math>c_{101}</math> to find <math>c_{01}</math>, <math>c_{011}</math> and <math>c_{111}</math> to find <math>c_{11}</math>, <math>c_{010}</math> and <math>c_{110}</math> to find <math>c_{10}</math>.\n\nNow we do interpolation between <math>c_{00}</math> and <math>c_{10}</math> to find <math>c_{0}</math>, <math>c_{01}</math> and <math>c_{11}</math> to find <math>c_{1}</math>. Finally, we calculate the value <math>c</math> via linear interpolation of <math>c_{0}</math> and <math>c_{1}</math>\n\nIn practice, a trilinear interpolation is identical to two [[bilinear interpolation]] combined with a linear interpolation:\n:<math> c \\approx\\ l( b(c_{000}, c_{010}, c_{100}, c_{110}), b(c_{001}, c_{011}, c_{101}, c_{111}))</math>\n\n===Alternative algorithm===\nAn alternative way to write the solution to the interpolation problem is\n:<math> f(x,y,z) \\approx a_0 + a_1 x + a_2 y + a_3 z + a_4 x y + a_5 x z + a_6 y z + a_7 x y z</math>\nwhere the coefficients are found by solving the linear system\n:<math>\\begin{align}\n\\begin{bmatrix}\n1 & x_0 & y_0 & z_0 & x_0 y_0 & x_0 z_0 & y_0 z_0 & x_0 y_0 z_0 \\\\\n1 & x_1 & y_0 & z_0 & x_1 y_0 & x_1 z_0 & y_0 z_0 & x_1 y_0 z_0 \\\\\n1 & x_0 & y_1 & z_0 & x_0 y_1 & x_0 z_0 & y_1 z_0 & x_0 y_1 z_0 \\\\\n1 & x_1 & y_1 & z_0 & x_1 y_1 & x_1 z_0 & y_1 z_0 & x_1 y_1 z_0 \\\\\n1 & x_0 & y_0 & z_1 & x_0 y_0 & x_0 z_1 & y_0 z_1 & x_0 y_0 z_1 \\\\\n1 & x_1 & y_0 & z_1 & x_1 y_0 & x_1 z_1 & y_0 z_1 & x_1 y_0 z_1 \\\\\n1 & x_0 & y_1 & z_1 & x_0 y_1 & x_0 z_1 & y_1 z_1 & x_0 y_1 z_1 \\\\\n1 & x_1 & y_1 & z_1 & x_1 y_1 & x_1 z_1 & y_1 z_1 & x_1 y_1 z_1 \n\\end{bmatrix}\\begin{bmatrix}\na_0\\\\a_1\\\\a_2\\\\a_3\\\\a_4\\\\a_5\\\\a_6\\\\a_7\n\\end{bmatrix}=\\begin{bmatrix}\nc_{000}\\\\c_{100}\\\\c_{010}\\\\c_{110}\\\\c_{001}\\\\c_{101}\\\\c_{011}\\\\c_{111}\n\\end{bmatrix},\n\\end{align}</math>\n\nyielding the result\n\n:<math>\\begin{align} \\ a_0 =\n& \\frac{c_{000} x_1 y_1 z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{001} x_1 y_1 z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{010} x_1 y_0 z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{011} x_1 y_0 z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{100} x_0 y_1 z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{101} x_0 y_1 z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{110} x_0 y_0 z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{111} x_0 y_0 z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)},\\end{align}</math>\n:<math>\\begin{align} \\ a_1 =\n& \\frac{c_{000} y_1 z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{001} y_1 z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{010} y_0 z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{011} y_0 z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{100} y_1 z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{101} y_1 z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{110} y_0 z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{111} y_0 z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)},\\end{align}</math>\n:<math>\\begin{align} \\ a_2 =\n& \\frac{c_{000} x_1 z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{001} x_1 z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{010} x_1 z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{011} x_1 z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{100} x_0 z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{101} x_0 z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{110} x_0 z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{111} x_0 z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)},\\end{align}</math>\n:<math>\\begin{align} \\ a_3 =\n& \\frac{c_{000} x_1 y_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{001} x_1 y_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{010} x_1 y_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{011} x_1 y_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{100} x_0 y_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{101} x_0 y_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{110} x_0 y_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{111} x_0 y_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)},\\end{align}</math>\n:<math>\\begin{align} \\ a_4 =\n& \\frac{c_{000} z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{001} z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{010} z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{011} z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{100} z_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{101} z_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{110} z_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{111} z_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)},\\end{align}</math>\n:<math>\\begin{align} \\ a_5 =\n& \\frac{c_{000} y_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{001} y_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{010} y_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{011} y_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{100} y_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{101} y_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{110} y_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{111} y_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)},\\end{align}</math>\n:<math>\\begin{align} \\ a_6 =\n& \\frac{c_{000} x_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{001} x_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{010} x_1}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{011} x_1}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{100} x_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{101} x_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{110} x_0}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{111} x_0}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)},\\end{align}</math>\n:<math>\\begin{align} \\ a_7 =\n& \\frac{c_{000}}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{001}}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\\\\n& \\frac{c_{010}}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{011}}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{100}}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)} + \\frac{c_{101}}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\\\\n& \\frac{c_{110}}{(x_0 - x_1) (y_0 - y_1) (z_0 - z_1)} + \\frac{c_{111}}{(x_0 - x_1) (y_0 - y_1) (z_1 - z_0)}.\\end{align}</math>\n\n==See also==\n* [[Linear interpolation]]\n* [[Bilinear interpolation]]\n* [[Tricubic interpolation]]\n* [[Radial interpolation]]\n* [[Tetrahedral interpolation]]\n\n==External links==\n*[http://www.grc.nasa.gov/WWW/winddocs/utilities/b4wind_guide/trilinear.html pseudo-code from NASA], describes an iterative inverse trilinear interpolation (given the vertices and the value of C find Xd, Yd and Zd).\n*Paul Bourke, [http://paulbourke.net/miscellaneous/interpolation/ Interpolation methods], 1999. Contains a very clever and simple method to find trilinear interpolation that is based on binary logic and can be extended to any dimension (Tetralinear, Pentalinear, ...).\n*Kenwright, Free-Form Tetrahedron Deformation. International Symposium on Visual Computing. Springer International Publishing, 2015 [https://link.springer.com/chapter/10.1007/978-3-319-27863-6_74].\n\n[[Category:Multivariate interpolation]]"
    },
    {
      "title": "Akima spline",
      "url": "https://en.wikipedia.org/wiki/Akima_spline",
      "text": "In applied mathematics, an '''Akima spline''' is a type of [[cubic spline]] that is relatively robust to [[outlier]]s.<ref>{{Cite web|url=http://www.alglib.net/interpolation/spline3.php#header5|title=Spline interpolation and fitting – ALGLIB, C++ and C# library|website=www.alglib.net}}</ref> The Akima spline was published by Hiroshi Akima in 1970.<ref>http://www.leg.ufpr.br/lib/exe/fetch.php/wiki:internas:biblioteca:akima.pdf</ref>\n\n==Method==\nGiven a set of points <math>(x_i, y_i)</math>, where the <math>x_i</math> are strictly increasing, the Akima spline will go through each of the given points.  At those points, its slope, <math>s_i</math>, is a function of the locations of the points <math>(x_{i-2}, y_{i-2})</math> through <math>(x_{i+2}, y_{i+2})</math>.  Specifically, we define <math>m_i</math> as the slope of the line segment from <math>(x_i, y_i)</math> to <math>(x_{i+1},y_{i+1})</math>, namely <math>(y_{i+1} - y_i)/(x_{i+1}-x_i)</math>.  Then, <math>s_i</math> is defined as the following [[weighted arithmetic mean|weighted average]] of <math>m_{i-1}</math> and <math>m_i</math>:\n\n:<math>s_i = \\frac{|m_{i+1} - m_i|m_{i-1} + |m_{i-1} - m_{i-2}|m_i}{|m_{i+1} - m_i| + |m_{i-1} - m_{i-2}|}</math>\n\nThe spline is then defined as the piecewise cubic function whose value between <math>x_i</math> and <math>x_{i+1}</math> is the unique cubic polynomial <math>P(x)</math> that satisfies the four constraints: <math>P(x_i) = y_i</math>, <math>P(x_{i+1}) = y_{i+1}</math>, <math>P'(x_i) = s_i</math>, and <math>P'(x_{i+1}) = s_{i+1}</math>.\n\n==References==\n{{reflist }}\n\n[[Category:Splines (mathematics)]]\n\n{{math-stub}}"
    },
    {
      "title": "Composite Bézier curve",
      "url": "https://en.wikipedia.org/wiki/Composite_B%C3%A9zier_curve",
      "text": "{{See also |Bézier curve}}\n[[Image:Beziergon.svg|thumb|right|200px|Beziergon - The red beziergon passes through the blue vertices, the green points are control points that determine the shape of the connecting Bézier curves]]\nIn [[geometric modelling]] and in [[computer graphics]], a '''composite Bézier curve''' is a piecewise [[Bézier curve]] that is at least [[continuous function|continuous]]. In other words, a composite Bézier curve is a series of Bézier curves joined end to end where the last point of one curve coincides with the starting point of the next curve. Depending on the application, additional smoothness requirements (such as C1 or C2 continuity) may be added.<ref name=\"ShikinPlis1995\">{{cite book|author1=Eugene V. Shikin|author2=Alexander I. Plis|title=Handbook on Splines for the User|url=https://books.google.com/books?id=DL88KouJCQkC&pg=PA96|date=14 July 1995|publisher=CRC Press|isbn=978-0-8493-9404-1|pages=96–}}</ref>\n\nA continuous composite Bézier is also called a '''polybezier''', by similarity to [[polyline]], but whereas in polylines the points are connected by straight lines, in a polybezier the points are connected by Bézier curves. A '''beziergon''' (also called '''bezigon''') is a closed path composed of [[Bezier curves|Bézier curves]]. It is similar to a [[polygon]] in that it connects a set of [[vertex (geometry)|vertices]] by lines, but whereas in polygons the vertices are connected by straight lines, in a beziergon the vertices are connected by Bézier curves.<ref>[http://msdn2.microsoft.com/en-us/library/ms534244.aspx Microsoft polybezier API]\n</ref><ref>[http://libpapyrus.sourceforge.net/guide_beziergon.html Papyrus beziergon API reference]\n</ref><ref>[https://books.google.com/books?id=nFAEAAAAMBAJ&pg=PA85&lpg=PA85&dq=bezigon+curve&source=bl&ots=NgfxXCVYQq&sig=RVvSI3SH2j6LXNBh_Y10KjCLlEs&hl=en&sa=X&ei=XygXUfnWI6jm2gX36ICYBA&ved=0CC4Q6AEwADgU#v=onepage&q=bezigon%20curve&f=false \"A better box of crayons\"].\nInfoWorld.\n1991.</ref> Some authors even call a C0 composite Bézier curve a \"Bézier spline\";<ref>{{Cite book|url=https://books.google.com/books?id=lFwXglfyoIQC|title=A First Course in Applied Mathematics|last=Rebaza|first=Jorge|date=2012-04-24|publisher=John Wiley & Sons|isbn=9781118277157|language=en}}</ref> the latter term is however used by other authors as a synonym for the (non-composite) Bézier curve, and they add \"composite\" in front of \"Bézier spline\" to denote the composite case.<ref>{{Cite book|url=https://books.google.com/books?id=VnH0UzTycTcC|title=Mathematica ® 3.0 Standard Add-on Packages|last=(Firm)|first=Wolfram Research|date=1996-09-13|publisher=Cambridge University Press|isbn=9780521585859|language=en}}</ref>\n\nPerhaps the most common use of composite Béziers is to describe the outline of each letter in a [[PostScript]] or [[PDF]] file. Such outlines are composed of one beziergon for [[typeface anatomy|open letters]], or multiple beziergons for closed letters. Modern [[vector graphics]] and [[computer font]] systems like [[PostScript]], [[Asymptote (vector graphics language)|Asymptote]], [[Metafont]], [[OpenType]], and [[SVG]] use composite Bézier curves composed of cubic Bézier curves (3rd order curves) for drawing curved shapes.\n\n[[File:Sinc Function Approximation with Bezier Splines.svg|thumb|[[Sinc]] function approximated using a smooth Bézier spline, i.e., a series of smoothly-joined Bézier curves]]\n\n==Smooth joining==\n{{Expand section|date=August 2014}}\nComposite Bezier curves can be smoothed to any desired degree of [[smoothness]] using Stärk's construction.<ref>{{Cite book|url=https://books.google.com/books?id=xP7A8F6NZGQC|title=Bézier and B-Spline Techniques|last=Prautzsch|first=Hartmut|last2=Boehm|first2=Wolfgang|last3=Paluszny|first3=Marco|date=2002-08-06|publisher=Springer Science & Business Media|isbn=9783540437611|language=en}}</ref>\n\nC2 continuous composite cubic Bezier curves are actually cubic [[B-spline]]s,<ref>{{Cite book|url=https://books.google.com/books?id=9bQ0f8sYqaAC|title=An Introduction to Splines for Use in Computer Graphics and Geometric Modeling|last=Bartels|first=Richard H.|last2=Beatty|first2=John C.|last3=Barsky|first3=Brian A.|date=1987-01-01|publisher=Morgan Kaufmann|isbn=9781558604001|language=en}}</ref> and vice versa.<ref>{{Cite book|url=https://books.google.com/books?id=TAYw3LEs5rgC|title=Computer Graphics and Geometric Modelling: Implementation & Algorithms|last=Agoston|first=Max K.|date=2005-12-06|publisher=Springer Science & Business Media|isbn=9781846281082|language=en}}</ref>\n\n==Approximating circular arcs==\nIn case circular arc primitives are not supported in a particular environment, they may be approximated by [[Bézier curve]]s.<ref>{{Cite web\n  | last = Stanislav\n  | first = G. Adam\n  | title = Drawing a circle with Bézier Curves\n  | url=http://whizkidtech.redprince.net/bezier/circle/\n  | accessdate = 10 April 2010 }}\n</ref> Commonly, eight quadratic segments<ref>{{Cite web\n|publisher=Apple\n|title=Digitizing letterform designs\n|url=https://developer.apple.com/fonts/ttrefman/RM01/Chap1.html\n|accessdate=26 July 2014\n}}</ref> or four cubic segments are used to approximate a circle. It is desirable to find the length <math>\\mathbf{k}</math> of control points which result in the least approximation error for a given number of cubic segments.\n\n===Using four curves===\nConsidering only the 90-degree [[unit circle|unit-circular]] arc in the [[Cartesian coordinate system#Quadrants and octants|first quadrant]], we define the endpoints <math>\\mathbf{A}</math> and <math>\\mathbf{B}</math> with control points <math>\\mathbf{A'}</math> and <math>\\mathbf{B'}</math>, respectively, as:\n\n:<math>\n\\begin{align}\n\\mathbf{A} & = [0, 1] \\\\\n\\mathbf{A'} & = [\\mathbf{k}, 1] \\\\\n\\mathbf{B'} & = [1, \\mathbf{k}] \\\\\n\\mathbf{B} & = [1, 0] \\\\\n\\end{align}\n</math>\n\nFrom the definition of the cubic Bézier curve, we have:\n\n:<math>\\mathbf{C}(t)=(1-t)^3\\mathbf{A} + 3(1-t)^2t\\mathbf{A'}+3(1-t)t^2\\mathbf{B'}+t^3\\mathbf{B}</math>\n\nWith the point <math>\\mathbf{C}(t=0.5)</math> as the midpoint of the arc, we may write the following two equations:\n\n:<math>\n\\begin{align}\n\\mathbf{C} &= \\frac{1}{8}\\mathbf{A} + \\frac{3}{8}\\mathbf{A'}+\\frac{3}{8}\\mathbf{B'}+\\frac{1}{8}\\mathbf{B} \\\\\n\\mathbf{C} &= \\sqrt{1/2} = \\sqrt{2}/2\n\\end{align}\n</math>\n\nSolving these equations for the x-coordinate (and identically for the y-coordinate) yields:\n\n:<math>\\frac{0}{8}\\mathbf + \\frac{3}{8}\\mathbf{k}+\\frac{3}{8} + \\frac{1}{8} = \\sqrt{2}/2</math>\n:<math>\\mathbf{k} = \\frac{4}{3}(\\sqrt{2} - 1) \\approx 0.5522847498</math>\n\n===General case===\nWe may compose a circle of radius <math>R</math> from an arbitrary number of cubic Bézier curves.<ref>{{Cite journal\n| last        = Riškus\n| first       = Aleksas\n|date=October 2006\n| title       = APPROXIMATION OF A CUBIC BEZIER CURVE BY CIRCULAR ARCS AND VICE VERSA\n| journal     = INFORMATION TECHNOLOGY AND CONTROL\n| volume      = 35\n| issue       = 4\n| pages       = 371–378\n| location    = Department of Multimedia Engineering, Kaunas University of Technology\n| issn        = 1392-124X\n| url         = https://people-mozilla.org/~jmuizelaar/Riskus354.pdf}}</ref>\nLet the arc start at point <math>\\mathbf{A}</math> and end at point <math>\\mathbf{B}</math>, placed at equal distances above and below the x-axis, spanning an arc of angle <math>\\theta = 2\\phi</math>:\n\n:<math>\\begin{align}\n\\mathbf{A}_x &= R\\cos(\\phi) \\\\\n\\mathbf{A}_y &= R\\sin(\\phi) \\\\\n\\mathbf{B}_x &= \\mathbf{A}_x \\\\\n\\mathbf{B}_y &= -\\mathbf{A}_y\n\\end{align}</math>\n\nThe control points may be written as:<ref>\n{{Cite web\n  | last = DeVeneza\n  | first = Richard\n  | title = Drawing a circle with Bézier Curves\n  | url=http://www.tinaja.com/glib/bezcirc2.pdf\n  | accessdate = 10 April 2010 }}</ref>\n\n:<math>\\begin{align}\n\\mathbf{A'}_x &= \\frac{4R - \\mathbf{A}_x}{3} \\\\\n\\mathbf{A'}_y &= \\frac{(R - \\mathbf{A}_x)(3R - \\mathbf{A}_x)}{3\\mathbf{A}_y} \\\\\n\\mathbf{B'}_x &= \\mathbf{A'}_x \\\\\n\\mathbf{B'}_y &= -\\mathbf{A'}_y\n\\end{align}</math>\n\n===Examples===\n<gallery widths=\"256px\" heights=\"256px\">\nFile:Circle and quadratic bezier.svg|Eight-segment quadratic polyBézier (red) approximating a circle (black) with control points\nFile:Circle and cubic bezier.svg|Four-segment cubic polyBézier (red) approximating a circle (black) with control points\n</gallery>\n\n==Fonts==\n[[TrueType]] fonts use composite Béziers composed of '''quadratic''' Bézier curves (2nd order curves). To describe a typical [[type design]] as a [[computer font]] to any given accuracy, 3rd order Beziers require less data than 2nd order Beziers; and these in turn require less data than a series of straight lines. This is true even though any one straight line segment requires less data than any one segment of a parabola; and that parabolic segment in turn requires less data than any one segment of a 3rd order curve.\n\n==See also==\n* [[B-spline]]\n\n==References==\n{{Reflist}}\n{{Use dmy dates|date=July 2011}}\n\n{{DEFAULTSORT:Bezier Spline}}\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "Biarc",
      "url": "https://en.wikipedia.org/wiki/Biarc",
      "text": "[[File:Biarc.svg|thumb|Fig. 1]]\nA '''biarc''' is a [[smooth curve]] formed from two [[circular arc]]s.<ref>\n{{cite journal\n|last=Bolton |first=K. M.\n|url=http://www.sciencedirect.com/science/article/pii/001044857590086X\n|title=Biarc curves\n|journal=Computer-Aided Design\n|volume=7\n|date=1975\n|issue=2\n|pages=89–92\n|doi=10.1016/0010-4485(75)90086-X\n}}\n</ref> In order to make the biarc smooth ([[Geometric continuity|''G''<sup>1</sup> continuous]]), the two arcs should have the same [[tangent]] at the connecting point where they meet.\n\nBiarcs are commonly used in [[geometric modeling]] and [[computer graphics]]. They can be used to [[approximation|approximate]] [[Spline (mathematics)|splines]] and other [[curve|plane curves]] by placing the two outer endpoints of the biarc along the curve to be approximated, with a tangent that matches the curve, and then choosing a middle point that best fits the curve.  This choice of three points and two tangents determines a unique pair of circular arcs, and the [[Locus (mathematics)|locus]] of middle points for which these two arcs form a biarc is itself a circular arc. In particular, to approximate a [[Bézier curve]] in this way, the middle point of the biarc should be chosen as the [[incenter]] of the triangle formed by the two endpoints of the Bézier curve and the point where their two tangents meet. More generally, one can approximate a curve by a smooth sequence of biarcs; using more biarcs in the sequence will in general improve the approximation's closeness to the original curve.\n\n== Examples of biarc curves ==\n<!--\n************************************************************************\n*  HTML-markup <ol>...</ol> is used below instead of wiki-markup * ...,\n*  because the latter produces incorrect formatting (indentation) after\n*  a displayed formula, :<math>...</math>. \n************************************************************************\n-->\n<ol>\n<li>\nIn the below examples biarcs <math>A(J)B</math> are subtended by the chord <math>AB,</math> and <math>J</math> is the join point. Tangent vector at the start point <math>A</math> is <math>\\mathbf{n}(\\alpha)</math>, and <math>\\mathbf{n}(\\beta)</math> is the tangent at the end point <math>B:</math>\n: <math>\\mathbf{n}(\\alpha)=||\\cos\\alpha,\\,\\sin\\alpha||^T,\\quad\n\\mathbf{n}(\\beta)=||\\cos\\beta,\\,\\sin\\beta||^T.\\qquad(1)</math>\n</li>\n<li>\nFig.&nbsp;2 shows six examples of biarcs <math>AJB.</math>\n* Biarc 1 is drawn with <math>\\alpha=100^\\circ,\\;\\beta=30^\\circ.</math> Biarcs 2-6 have <math>\\alpha=100^\\circ,\\;\\beta=-30^\\circ.</math>\n* In examples 1, 2, 6 curvature changes sign, and the join point <math>J</math> is also the inflection point. Biarc&nbsp;3 includes the straight line segment <math>JB</math>.\n* Biarcs 1–4 are ''short'' in the sense that they do not turn near endpoints. Alternatively, biarcs&nbsp;5,6 are ''long'': turning near one of endpoints means that they intersect the left or the right complement of the chord to the infinite straight line.\n* Biarcs 2–6 share end tangents. They can be found in the lower fragment of Fig.&nbsp;3, among the family of biarcs with common tangents.\n</li>\n<li>\nFig.&nbsp;3 shows two examples of biarc families, sharing end points and end tangents.\n</li>\n<li>\nFig.&nbsp;4 shows two examples of biarc families, sharing end points and end tangents, end tangents being parallel: <math>\\alpha=\\beta.</math>\n</li>\n<li>\nFig.&nbsp;5 shows specific families with either <math>|\\alpha|=\\pi</math> &nbsp;or&nbsp;  <math>|\\beta|=\\pi.</math>\n</li>\n</ol>\n\n{|align=\"left\"\n |-valign=\"bottom\"\n |[[File:Biarcs_6_Examples.png|thumb|Fig. 2. Examples of biarcs]]\n |[[File:BiarcsWiki1.png|thumb|Fig. 3. Biarcs families with common tangents (two examples)]]\n |[[File:BiarcsWiki2.png|thumb|Fig. 4. Biarcs families with parallel end tangents]]\n<!-- [[File:BiarcsWikiPi.png|thumb|Fig. 5 shows biarc families with either <math>|\\alpha|=\\pi</math> or <math>|\\beta|=\\pi</math>]] -->\n |}\n{{clear}}\n\n[[File:BiarcsWikiPi.png|thumb|Fig 5. Biarcs families with either <math>|\\alpha|=\\pi</math> or  <math>|\\beta|=\\pi</math>]]\nDifferent colours in figures&nbsp;3,&nbsp;4,&nbsp;5 are explained below as subfamilies \n<math>\\color{sienna}\\mathcal{B}^{\\,+}</math>,\n<math>\\color{blue}\\mathcal{B}^{\\,-}_1</math>,\n<math>\\color{green}\\mathcal{B}^{\\,-}_2</math>.\nIn particular, for biarcs, shown in brown on shaded background ([[Lens (geometry)|lens]]-like or [[Lune (geometry)|lune]]-like), the following holds:\n* the total rotation (turning angle) of the curve is exactly <math>\\beta-\\alpha</math> (not <math>\\beta-\\alpha\\pm 2\\pi</math>, which is the rotation for other biarcs);\n* <math>\\sgn(\\alpha+\\beta)=\\sgn(k_2-k_1)</math>: the sum <math>\\alpha+\\beta</math> is the angular width of the lens/lune, covering the biarc, whose sign corresponds to either increasing (+1) or decreasing curvature (-1) of the biarc, according to generalized [[Vogt's theorem]] ([[:ru:Теорема Фогта#Обобщение теоремы|ru]]).\n\n== Family of biarcs with common end tangents ==\nA family of biarcs with common end points <math>A=(-c,0)</math>, <math>B=(c,0)</math>, and common end tangents (1) is denoted as <math>\\mathcal{B}(p;\\,\\alpha,\\beta,c),</math> or, briefly, as <math>\\mathcal{B}(p),</math> <math>p</math>&nbsp;being the family parameter. Biarcs properties are described below in terms of article.<ref name=\"Kurnosenko2013\">\n{{cite journal\n|last=Kurnosenko\n|first=A. I.\n|date=2013\n|title=Biarcs and bilens\n|journal=Computer Aided Geometric Design\n|volume=30\n|issue=3\n|pages=310–330\n|doi=10.1016/j.cagd.2012.12.002\n}}\n[https://www.dropbox.com/s/oiph2nyji38v2kr/Biarcs-and-Bilens-preprint.pdf]\n</ref>\n\n<ol>\n<li>\nConstructing of a biarc is possible if\n: <math>-\\pi\\leqslant\\alpha\\leqslant\\pi,\\quad -\\pi\\leqslant\\beta\\leqslant\\pi,\\qquad 0<|\\alpha+\\beta|<2\\pi.\\qquad\\qquad(2)</math>\n</li>\n<li>\nDenote\n* <math>k_1</math>, <math>\\theta_1</math> and <math>L_1</math>&nbsp; the curvature, the turning angle and the length of the arc <math>AJ</math>:&nbsp;&nbsp;&nbsp; <math>\\theta_1=k_1L_1</math>;\n* <math>k_2</math>, <math>\\theta_2</math> and <math>L_2</math>&nbsp; the same for the arc <math>JB</math>:&nbsp;&nbsp;&nbsp; <math>\\theta_2=k_2L_2</math>.\n\nThen\n: <math>k_1(p)=-\\frac1c\\left(\\sin\\alpha+p^{-1}\\sin\\omega\\right),\\quad k_2(p)=\\frac1c\\left(\\sin\\beta+p\\sin\\omega\\right),\\quad\\text{where}\\quad \\omega=\\frac{\\alpha+\\beta}2</math>\n(due to (2), <math>\\sin\\omega\\not=0</math>).\nTurning angles:\n:<math>\n   \\theta_1(p)=2\\arg\\left(\\mathrm{e}^{-\\mathrm{i}\\alpha} +p^{-1}{\\mathrm{e}^{-\\mathrm{i}\\omega}}\\right),\\quad\n   \\theta_2(p)=2\\arg\\left(\\mathrm{e}^{ \\mathrm{i}\\beta } +p\\,   \\mathrm{e}^{\\mathrm{i}\\omega}\\right).\n</math>\n</li>\n<li>\nThe locus of join points <math>J</math> is the circle\n: <math>\n   X_J(p)=\\frac{c(p^2-1)}{{p^2+2p\\cos\\gamma+1}},\\quad\n   Y_J(p)=\\frac{2cp\\sin\\gamma}{p^2+2p\\cos\\gamma+1},\\quad\\text{where}\\quad\n   \\gamma=\\frac{\\alpha-\\beta}2\n</math>\n(shown dashed in Fig.3, Fig.5).\nThis circle (straight line if <math>\\gamma=0</math>,  Fig.4) passes through points <math>A,B,</math> the tangent at <math>A</math> being&nbsp;<math>\\mathbf{n}(\\gamma).</math>\nBiarcs intersect this circle under the constant angle&nbsp;&nbsp;<math>-\\omega.</math>\n</li>\n<li>\nTangent vector to the biarc <math>\\mathcal{B}(p)</math> at the join point is <math>\\mathbf{n}\\left(\\tau_{{}_J}\\right)</math>, where\n: <math>\n      \\tau_{{}_J}(p)={-2}\\operatorname{arctan}\\dfrac{p\\sin\\frac{\\alpha}2+\\sin\\frac{\\beta}2}{p\\cos\\frac{\\alpha}2+\\cos\\frac{\\beta}2}.\n</math>\n</li>\n\n<li>\nBiarcs with <math>p=\\pm1</math> have the join point on the Y-axis <math>(X_J=0),</math> and yield the '''minimal curvature jump''', <math>\\min\\left|k_2(p)-k_1(p)\\right|,</math> at&nbsp;<math>J.</math>\n</li>\n\n<li>\n''Degenerate biarcs'' are:\n* Biarc <math>\\mathcal{B}(0)</math>: as <math>p\\to0</math>, <math>J(p)\\to A</math>, arc <math>AJ</math> vanishes.\n* Biarc <math>\\mathcal{B}(\\infty)</math>: as <math>p\\to\\pm\\infty</math>, <math>J(p)\\to B</math>, arc <math>JB</math> vanishes.\n* Discontinuous biarc <math>\\mathcal{B}(p^\\ast)</math> includes straight line <math>AP_\\infty J</math> or <math>JP_\\infty B,</math> and passes through the infinite point&nbsp;<math>P_\\infty</math>:\n: <math>\n  \\qquad\n    p^\\ast=\n  \\left\\{\n   \\begin{array}{lll}\n      -\\dfrac{\\sin\\omega}{\\sin\\alpha},\\quad& \\text{if}\\; |\\alpha|\\geqslant|\\beta|\\quad(|\\alpha|=\\pi\\;\\Longrightarrow\\; p^\\ast=-\\infty),\\\\\n      -\\dfrac{\\sin\\beta}{\\sin\\omega},\\quad&  \\text{if}\\; |\\alpha|\\leqslant|\\beta|\\quad(|\\beta|=\\pi\\;\\Longrightarrow\\; p^\\ast=0).\n   \\end{array} \\right.\n</math>\nDarkened lens-like region in Figs.3,4 is bounded by biarcs <math>\\mathcal{B}(0),\\,\\mathcal{B}(\\infty).</math> It covers biarcs with <math>p>0.</math>\n\nDiscontinuous biarc is shown by red dash-dotted line.\n</li>\n<li>\nThe whole family <math>\\mathcal{B}(p;\\,\\alpha,\\beta,c)</math> can be subdivided into three subfamilies of non-degenerate biarcs:\n: <math>\n\\begin{array}{l}\n\\mathcal{B}^{\\,+}(p){:}\\quad p\\in(0;\\infty);\\\\\n\\mathcal{B}^{\\,-}_1(p){:}\\quad p\\in(p^\\ast;0);\\\\\n\\mathcal{B}^{\\,-}_2(p){:}\\quad p\\in(-\\infty;p^\\ast);\\\\\n\\left[\\mathcal{B}^{\\,-}(p) = \\mathcal{B}^{\\,-}_1(p) \\cup \\mathcal{B}^{\\,-}_2(p)\\right].\n\\end{array}\n</math>\n\nSubfamily <math>\\mathcal{B}^{\\,-}_1</math> vanishes if <math>p^\\ast=0</math>&nbsp;&nbsp;&nbsp;&nbsp; <math>(|\\beta|=\\pi).</math>\n\nSubfamily <math>\\mathcal{B}^{\\,-}_2</math> vanishes if <math>p^\\ast=-\\infty</math> <math>(|\\alpha|=\\pi).</math>\n\nIn figures&nbsp;3,&nbsp;4,&nbsp;5\nbiarcs <math>\\color{sienna}\\mathcal{B}^{\\,+}</math> are shown in brown,\nbiarcs <math>\\color{blue}\\mathcal{B}^{\\,-}_1</math> in blue,\nand biarcs <math>\\color{green}\\mathcal{B}^{\\,-}_2</math> in green.\n</li>\n</ol>\n\n== References ==\n{{Reflist}}\n\n*{{cite book\n|last1=Nutbourne|first1=A. W.\n|last2=Martin|first2=R. R.\n|title=Differential geometry applied to curve and surface design. Vol.1: Foundations.\n|publisher=Ellis Horwood\n|year=1988\n|isbn=978-0132118224\n}}\n\n== External links==\n* [http://www.ryanjuckett.com/programming/biarc-interpolation/ Biarc Interpolation]\n\n[[Category:Circles]]\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "Box spline",
      "url": "https://en.wikipedia.org/wiki/Box_spline",
      "text": "In the mathematical fields of [[numerical analysis]] and [[approximation theory]], '''box splines''' are [[piecewise]] [[polynomial]] [[Function (mathematics)|functions]] of several variables.<ref name=\"thebook\">{{Cite journal | doi = 10.1007/978-1-4757-2244-4| title = Box Splines| volume = 98| series = Applied Mathematical Sciences| year = 1993| last1 = Boor | first1 = C. | last2 = Höllig | first2 = K. | last3 = Riemenschneider | first3 = S. | isbn = 978-1-4419-2834-4}}</ref> Box splines are considered as a multivariate generalization of [[B-spline|basis splines (B-splines)]] and are generally used for multivariate approximation/interpolation. Geometrically, a box spline is the shadow (X-ray) of a hypercube projected down to a lower-dimensional space.<ref>{{Cite book | last1 = Prautzsch | first1 = H. | last2 = Boehm | first2 = W. | last3 = Paluszny | first3 = M. | doi = 10.1007/978-3-662-04919-8_17 | chapter = Box splines | title = Bézier and B-Spline Techniques | series = Mathematics and Visualization | pages = 239 | year = 2002 | isbn = 978-3-642-07842-2 | pmid =  | pmc = }}</ref> Box splines and simplex splines are well studied special cases of polyhedral splines which are defined as shadows of general [[polytopes]].\n\n==Definition==\nA box spline is a multivariate [[Function (mathematics)|function]] (<math>\\mathbb{R}^d \\to \\mathbb{R} </math>) defined for a set of vectors, <math>\\xi \\in \\mathbb{R}^d</math>, usually gathered in a matrix <math>\\mathbf{\\Xi} := \\left[\\xi_1 \\dots \\xi_N\\right] </math>.\n\nWhen the number of vectors is the same as the dimension of the domain (i.e., <math> N = d </math>) then the box spline is simply the (normalized) [[indicator function]] of the parallelepiped formed by the vectors in <math>\\mathbf{\\Xi}</math>:\n:<math> M_{\\mathbf{\\Xi}}(\\mathbf{x}) := \\frac{1}{\\mid{\\det{\\Xi}}\\mid}\\chi_{\\mathbf{\\Xi}}(\\mathbf{x}) = \\begin{cases} \\frac{1}{\\mid{\\det{\\Xi}}\\mid} & \\mathbf{x} = \\sum_{n=1}^d{t_n \\xi_n} \\text{ for some } 0 \\le t_n < 1 \\\\ 0 & \\text{otherwise}\\end{cases}.</math>\nAdding a new direction, <math>\\xi</math>, to <math>\\mathbf{\\Xi}</math>, or generally when <math>N > d</math>, the box spline is defined recursively:<ref name=\"thebook\" />\n:<math> M_{\\mathbf{\\Xi} \\cup \\xi}(\\mathbf{x}) = \\int_0^1{M_{\\mathbf{\\Xi}}(\\mathbf{x}- t \\xi) \\,  {\\rm d}t}</math>.\n\n[[File:Box Splines Square Grid Annotated Dark.png|thumb|right|Examples of bivariate box splines corresponding to 1, 2, 3 and 4 vectors in 2-D.]]\n\nThe box spline <math>M_{\\mathbf{\\Xi}}</math> can be interpreted as the shadow of the [[indicator function]] of the unit [[hypercube]] in <math>\\mathbb{R}^N</math> when projected down into <math>\\mathbb{R}^d</math>. In this view, the vectors <math>\\xi \\in \\mathbf{\\Xi}</math> are the geometric projection of the [[standard basis]] in <math>\\mathbb{R}^N</math> (i.e., the edges of the hypercube) to <math>\\mathbb{R}^d</math>.\n\nConsidering [[tempered distributions]] a box spline associated with a single direction vector is a [[Dirac delta function|Dirac]]-like [[generalized function]] supported on <math>t\\xi</math> for <math>0 \\le t < 1</math>. Then the general box spline is defined as the convolution of distributions associated the single-vector box splines:\n:<math>M_{\\mathbf{\\Xi}} = M_{\\xi_1} \\ast M_{\\xi_2} \\dots \\ast M_{\\xi_N}. </math>\n\n==Properties==\n* Let <math>\\kappa</math> be the minimum number of directions whose removal from <math>\\Xi</math> makes the remaining directions ''not'' span <math>\\mathbb{R}^d</math>. Then the box spline has <math>\\kappa-2</math> degrees of continuity: <math>M_{\\mathbf{\\Xi}} \\in C^{\\kappa-2}(\\mathbb{R}^d)</math>.<ref name=\"thebook\" />\n* When <math>N\\ge d</math> (and vectors in <math>\\Xi</math> span <math>\\mathbb{R}^d</math>) the box spline is a compactly supported function whose support is a [[Zonohedron|zonotope]] in <math>\\mathbb{R}^d</math> formed by the [[Minkowski sum]] of the direction vectors <math>{\\xi} \\in \\mathbf{\\Xi}</math>.\n* Since [[Zonohedron|zonotopes]] are centrally symmetric, the support of the box spline is symmetric with respect to its center: <math>\\mathbf{c}_\\Xi := \\frac{1}{2}\\sum_{n=1}^N \\xi_n .</math>\n* [[Fourier transform]] of the box spline, in <math>d</math> dimensions, is given by\n:: <math>\\hat{M}_{\\Xi}(\\omega) = \\exp{(-j\\mathbf{c}_{\\Xi}\\cdot\\omega)}\\prod_{n=1}^N{{\\rm sinc}(\\xi_n\\cdot\\omega)}.</math>\n\n==Applications==\n\nFor applications, linear combinations of shifts of one or more box splines on a lattice are used. Such splines are efficient, more so than linear combinations of simplex splines, because they are refinable and, by definition, shift invariant. They therefore form the starting point for many [[subdivision surface]] constructions.\n\nBox splines have been useful in characterization of hyperplane arrangements.<ref name=\"boxHyperplane\">{{Cite journal | doi = 10.1007/978-0-387-78963-7| title = Topics in Hyperplane Arrangements, Polytopes and Box-Splines| year = 2010| last1 = De Concini | first1 = C. | last2 = Procesi | first2 = C. | isbn = 978-0-387-78962-0}}</ref> Also, box splines can be\nused to compute the volume of polytopes.<ref name=\"boxpolytope\">{{Cite journal | doi = 10.1016/j.jat.2010.10.005| title = Multivariate splines and polytopes| journal = Journal of Approximation Theory| volume = 163| issue = 3| pages = 377| year = 2011| last1 = Xu | first1 = Z. }}</ref>\n\nIn the context of [[Multidimensional sampling|multidimensional signal processing]], box splines can provide [[Reconstruction filter|multivariate interpolation kernels]] (reconstruction filters) tailored to non-Cartesian [[Multidimensional sampling|sampling lattices]],<ref name=\"summit.sfu.ca\">Entezari, Alireza. Optimal sampling lattices and trivariate box splines. [Vancouver, BC.]: Simon Fraser University, 2007. <http://summit.sfu.ca/item/8178>.</ref> and [[root systems|crystallographic lattices]] (root lattices) that include many information-theoretically optimal sampling lattices.<ref name=\"optSamp\">{{Cite journal | last1 = Kunsch | first1 = H. R. | last2 = Agrell | first2 = E. | last3 = Hamprecht | first3 = F. A. | doi = 10.1109/TIT.2004.840864 | title = Optimal Lattices for Sampling | journal = IEEE Transactions on Information Theory | volume = 51 | issue = 2 | pages = 634 | year = 2005 | pmid =  | pmc = }}</ref> Generally, optimal [[sphere packing]] and sphere covering lattices<ref>J. H. Conway, N. J. A. Sloane. Sphere packings, lattices and groups. Springer, 1999.</ref> are useful for sampling multivariate functions in 2-D, 3-D and higher dimensions.<ref name=\"petmid62\">{{Cite journal | doi = 10.1016/S0019-9958(62)90633-2| title = Sampling and reconstruction of wave-number-limited functions in N-dimensional euclidean spaces| journal = Information and Control| volume = 5| issue = 4| pages = 279| year = 1962| last1 = Petersen | first1 = D. P. | last2 = Middleton | first2 = D. }}</ref>\nIn the 2-D setting the three-direction box spline<ref>{{Cite journal | last1 = Condat | first1 = L. | last2 = Van De Ville | first2 = D. | doi = 10.1109/LSP.2006.871852 | title = Three-directional box-splines: Characterization and efficient evaluation | journal = IEEE Signal Processing Letters | volume = 13 | issue = 7 | pages = 417 | year = 2006 | pmid =  | pmc = | bibcode = 2006ISPL...13..417C }}</ref> is used for interpolation of hexagonally sampled images. In the 3-D setting, four-direction<ref name=\"fourDir\">{{Cite journal | last1 = Entezari | first1 = A. | last2 = Van De Ville | first2 = D. | last3 = Moller | first3 = T. | doi = 10.1109/TVCG.2007.70429 | title = Practical Box Splines for Reconstruction on the Body Centered Cubic Lattice | journal = IEEE Transactions on Visualization and Computer Graphics | volume = 14 | issue = 2 | pages = 313–328 | year = 2008 | pmid =  18192712| pmc = }}</ref> and six-direction<ref name=\"sixDir\">{{Cite journal | last1 = Minho Kim | first1 = M. | last2 = Entezari | first2 = A. | last3 = Peters | first3 = Jorg | doi = 10.1109/TVCG.2008.115 | title = Box Spline Reconstruction on the Face-Centered Cubic Lattice | journal = IEEE Transactions on Visualization and Computer Graphics | volume = 14 | issue = 6 | pages = 1523–1530 | year = 2008 | pmid =  18989005| pmc = }}</ref> box splines are used for interpolation of data sampled on the (optimal) [[body centered cubic]] and [[face centered cubic]] lattices respectively.<ref name=\"summit.sfu.ca\"/> The seven-direction box spline<ref>{{Cite book | doi = 10.1145/267734.267783| chapter = Box-spline based CSG blends| title = Proceedings of the fourth ACM symposium on Solid modeling and applications  - SMA '97| pages = 195| year = 1997| last1 = Peters | first1 = Jorg | last2 = Wittman | first2 = M. | isbn = 0897919467}}</ref> has been used for modelling surfaces and can be used for interpolation of data on the Cartesian lattice<ref>{{Cite journal | last1 = Entezari | first1 = A. | last2 = Moller | first2 = T. | doi = 10.1109/TVCG.2006.141 | title = Extensions of the Zwart-Powell Box Spline for Volumetric Data Reconstruction on the Cartesian Lattice | journal = IEEE Transactions on Visualization and Computer Graphics | volume = 12 | issue = 5 | pages = 1337–1344 | year = 2006 | pmid =  17080870| pmc = }}</ref> as well as the [[body centered cubic]] lattice.<ref>{{Cite journal | last1 = Minho Kim | doi = 10.1109/TVCG.2012.130 | title = Quartic Box-Spline Reconstruction on the BCC Lattice | journal = IEEE Transactions on Visualization and Computer Graphics | volume = 19 | issue = 2 | pages = 319–330 | year = 2013 | pmid =  | pmc = }}</ref> Generalization of the four-<ref name=\"fourDir\" /> and six-direction<ref name=\"sixDir\" /> box splines to higher dimensions<ref>Kim, Minho. Symmetric Box-Splines on Root Lattices. [Gainesville, Fla.]: University of Florida, 2008. <http://uf.catalog.fcla.edu/permalink.jsp?20UF021643670>.</ref> can be used to build splines on [[Root system|root lattices]].<ref name=\"rootlattice\">{{Cite journal | doi = 10.1016/j.cam.2010.11.027| title = Symmetric box-splines on root lattices| journal = Journal of Computational and Applied Mathematics| volume = 235| issue = 14| pages = 3972| year = 2011| last1 = Kim | first1 = M. | last2 = Peters | first2 = Jorg }}</ref> Box splines are key ingredients of hex-splines<ref>{{Cite journal | last1 = Van De Ville | first1 = D. | last2 = Blu | first2 = T. | last3 = Unser | first3 = M. | last4 = Philips | first4 = W. | last5 = Lemahieu | first5 = I. | last6 = Van De Walle | first6 = R. | doi = 10.1109/TIP.2004.827231 | title = Hex-Splines: A Novel Spline Family for Hexagonal Lattices | journal = IEEE Transactions on Image Processing | volume = 13 | issue = 6 | pages = 758–772 | year = 2004 | pmid =  15648867| pmc = | bibcode = 2004ITIP...13..758V }}</ref> and Voronoi splines<ref>{{Cite journal | last1 = Mirzargar | first1 = M. | last2 = Entezari | first2 = A. | doi = 10.1109/TSP.2010.2051808 | title = Voronoi Splines | journal = IEEE Transactions on Signal Processing | volume = 58 | issue = 9 | pages = 4572 | year = 2010 | pmid =  | pmc = | bibcode = 2010ITSP...58.4572M }}</ref> that, however, are not refinable.\n\nBox splines have found applications in high-dimensional filtering, specifically for fast bilateral filtering and non-local means algorithms.<ref>{{Cite journal | last1 = Baek | first1 = J. | last2 = Adams | first2 = A. | last3 = Dolson | first3 = J. | doi = 10.1007/s10851-012-0379-2 | title = Lattice-Based High-Dimensional Gaussian Filtering and the Permutohedral Lattice | journal = Journal of Mathematical Imaging and Vision | volume = 46 | issue = 2 | pages = 211 | year = 2012 | pmid =  | pmc = }}</ref> Moreover, box splines are used for designing efficient space-variant (i.e., non-convolutional) filters.<ref>{{Cite journal | last1 = Chaudhury | first1 = K. N. | last2 = MuñOz-Barrutia | first2 = A. | last3 = Unser | first3 = M. | doi = 10.1109/TIP.2010.2046953 | title = Fast Space-Variant Elliptical Filtering Using Box Splines | journal = IEEE Transactions on Image Processing | volume = 19 | issue = 9 | pages = 2290–2306 | year = 2010 | pmid =  20350851| pmc = | arxiv = 1003.2022 | bibcode = 2010ITIP...19.2290C }}</ref>\n\nBox splines are useful basis functions for image representation in the context of [[tomographic reconstruction]] problems as the spline spaces generated by box splines spaces are closed under [[X-ray transform|X-ray]] and [[Radon transform|Radon]] transforms.<ref name=\"boxTomo\">{{Cite journal | last1 = Entezari | first1 = A. | last2 = Nilchian | first2 = M. | last3 = Unser | first3 = M. | doi = 10.1109/TMI.2012.2191417 | title = A Box Spline Calculus for the Discretization of Computed Tomography Reconstruction Problems | journal = IEEE Transactions on Medical Imaging | volume = 31 | issue = 8 | pages = 1532–1541 | year = 2012 | pmid =  22453611| pmc = }}</ref><ref>{{Cite book | last1 = Entezari | first1 = A. | last2 = Unser | first2 = M. | doi = 10.1109/ISBI.2010.5490105 | chapter = A box spline calculus for computed tomography | title = 2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro | pages = 600 | year = 2010 | isbn = 978-1-4244-4125-9 | pmid =  | pmc = }}</ref> In this application while the signal is represented in shift-invariant spaces, the projections are obtained, in closed-form, by non-uniform translates of box splines.<ref name=\"boxTomo\" />\n\nIn the context of image processing, box spline frames have been shown to be effective in edge detection.<ref name=\"edgeDetection\">{{Cite journal | doi = 10.1137/120881348| title = Box Spline Wavelet Frames for Image Edge Analysis| journal = SIAM Journal on Imaging Sciences| volume = 6| issue = 3| pages = 1553| year = 2013| last1 = Guo | first1 = W. | last2 = Lai | first2 = M. J. }}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "Centripetal Catmull–Rom spline",
      "url": "https://en.wikipedia.org/wiki/Centripetal_Catmull%E2%80%93Rom_spline",
      "text": "In [[computer graphics]], '''centripetal Catmull–Rom spline''' is a variant form of [[Catmull-Rom spline]] <ref>{{Cite book |title=Computer Aided Geometric Design |last=Catmull |first=Edwin |last2=Rom |first2=Raphael |date=1974 |isbn=978-0-12-079050-0 |editor-last=Barnhill |editor-first=Robert E. |pages=317–326 |chapter=A class of local interpolating splines |doi=10.1016/B978-0-12-079050-0.50020-5 |author-link=Edwin Catmull |author-link2=Raphael Rom |editor-last2=Riesenfeld |editor-first2=Richard F.}}</ref> formulated by [[Edwin Catmull]] and [[Raphael Rom]] according to the work of Barry and Goldman.<ref>{{Cite conference|last=Barry |first=Phillip J. |last2=Goldman |first2=Ronald N. |date=August 1988|title=A recursive evaluation algorithm for a class of Catmull–Rom splines |conference=Proceedings of the 15st Annual Conference on Computer Graphics and Interactive Techniques, [[SIGGRAPH]] 1988|doi=10.1145/378456.378511|publisher=[[Association for Computing Machinery]]|url=https://www.researchgate.net/profile/Ronald_Goldman2/publication/220720141_Recursive_evaluation_algorithm_for_a_class_of_Catmull-Rom_splines/links/559d5d3d08ae76bed0bb3523/Recursive-evaluation-algorithm-for-a-class-of-Catmull-Rom-splines.pdf |volume=22 |issue=4 |pages=199–204}}</ref> It is a type of interpolating spline (a curve that goes through its control points) defined by four control points  <math>\\mathbf{P}_0, \\mathbf{P}_1, \\mathbf{P}_2, \\mathbf{P}_3</math>, with the curve drawn only from  <math>\\mathbf{P}_1</math> to <math>\\mathbf{P}_2</math>.\n\n[[File:Catmull-Rom Spline.png|thumb|Catmull–Rom spline interpolation with four points]]\n\n==Definition==\n[[File:Cubic Catmull-Rom formulation.png|thumb|Barry and Goldman's pyramidal formulation]]\n[[File:Catmull-Rom Parameterized Time.png|thumb|Knot parameterization for the Catmull–Rom algorithm.]]\n\nLet <math>\\mathbf{P}_i = [x_i \\quad y_i]^T</math> denote a point. For a curve segment <math>\\mathbf{C}</math> defined by points <math>\\mathbf{P}_0, \\mathbf{P}_1, \\mathbf{P}_2, \\mathbf{P}_3</math> and knot sequence <math>t_0, t_1, t_2, t_3</math>, the centripetal Catmull-Rom spline can be produced by:\n\n: <math>\\mathbf{C} = \\frac{t_{2}-t}{t_{2}-t_1}\\mathbf{B}_1+\\frac{t-t_1}{t_{2}-t_1}\\mathbf{B}_2</math>\n\nwhere\n\n: <math>\\mathbf{B}_1 = \\frac{t_{2}-t}{t_{2}-t_0}\\mathbf{A}_1+\\frac{t-t_0}{t_{2}-t_0}\\mathbf{A}_2</math>\n: <math>\\mathbf{B}_2 = \\frac{t_{3}-t}{t_{3}-t_1}\\mathbf{A}_2+\\frac{t-t_1}{t_{3}-t_1}\\mathbf{A}_3</math>\n: <math>\\mathbf{A}_1 = \\frac{t_{1}-t}{t_{1}-t_0}\\mathbf{P}_0+\\frac{t-t_0}{t_{1}-t_0}\\mathbf{P}_1</math>\n: <math>\\mathbf{A}_2 = \\frac{t_{2}-t}{t_{2}-t_1}\\mathbf{P}_1+\\frac{t-t_1}{t_{2}-t_1}\\mathbf{P}_2</math>\n: <math>\\mathbf{A}_3 = \\frac{t_{3}-t}{t_{3}-t_2}\\mathbf{P}_2+\\frac{t-t_2}{t_{3}-t_2}\\mathbf{P}_3</math>\n\nand\n\n:<math>t_{i+1} = \\left[\\sqrt{(x_{i+1}-x_i)^2+(y_{i+1}-y_i)^2}\\right]^{\\alpha} + t_i</math>\n\nin which <math>\\alpha</math> ranges from 0 to 1 for knot parameterization, and <math>i = 0,1,2,3</math> with <math>t_0 = 0 </math>. For centripetal Catmull-Rom spline, the value of <math>\\alpha</math> is <math>0.5</math>. When <math>\\alpha = 0</math>, the resulting curve is the standard [[uniform Catmull-Rom spline]]; when <math>\\alpha = 1</math>, the product is a [[chordal Catmull-Rom spline]].\n[[File:Uniform, Centripetal and Chordal Parameterization for Catmull-Rom Spline.gif|thumb|Gif animation for ''uniform'', ''centripetal'' and ''chordal'' parameterization of Catmull-Rom spline depending on the ''α'' value]]\nPlugging <math>t = t_1</math> into the spline equations <math> \\mathbf{A}_1, \\mathbf{A}_2, \\mathbf{A}_3, \\mathbf{B}_1, \\mathbf{B}_2,</math> and <math> \\mathbf{C}</math> shows that the value of the spline curve at <math>t_1</math> is <math>\\mathbf{C} = \\mathbf{P}_1</math>. Similarly, substituting <math>t = t_2</math> into the spline equations shows that <math> \\mathbf{C} = \\mathbf{P}_2 </math> at <math>t_2</math>. This is true independent of the value of <math>\\alpha</math> since the equation for <math>t_{i+1}</math> is not needed to calculate the value of <math>\\mathbf{C}</math> at points <math>t_1</math> and <math>t_2</math>.\n\n==Advantages==\nCentripetal Catmull–Rom spline has several desirable mathematical properties compared to the original and the other types of Catmull-Rom formulation.<ref name=\"Yuksel\">{{Cite journal |last=Yuksel |first=Cem |last2=Schaefer |first2=Scott |last3=Keyser |first3=John |date=July 2011 |title=Parameterization and applications of Catmull-Rom curves |url=http://www.cemyuksel.com/research/catmullrom_param/ |journal=Computer-Aided Design |volume=43 |issue=7 |pages=747–755 |doi=10.1016/j.cad.2010.08.008|citeseerx=10.1.1.359.9148 }}</ref> First, it will not form loop or self-intersection within a curve segment. Second, [[cusp_(singularity)|cusp]] will never occur within a curve segment. Third, it follows the control points more tightly.{{vague|date=September 2016}}\n\n[[File:Catmull-Rom examples with parameters..png|thumb|In this figure, there is a self-intersection/loop on the uniform Catmull-Rom spline (green), whereas for chordal Catmull-Rom spline (red), the curve does not follow tightly through the control points.]]\n\n==Other uses==\nIn [[computer vision]], centripetal Catmull-Rom spline has been used to formulate an active model for segmentation. The method is termed '''active spline model'''.<ref>{{Cite journal |last=Jen Hong |first=Tan |last2=Acharya |first2=U. Rajendra |date=2014 |title=Active spline model: A shape based model-interactive segmentation |url=https://doppiomovimento.files.wordpress.com/2015/01/active-spline-model-s.pdf |journal=[[Digital Signal Processing (journal)|Digital Signal Processing]] |volume=35 |pages=64–74 |arxiv=1402.6387 |doi=10.1016/j.dsp.2014.09.002}}</ref> The model is devised on the basis of [[active shape model]], but uses centripetal Catmull-Rom spline to join two successive points (active shape model uses simple straight line), so that the total number of points necessary to depict a shape is less. The use of centripetal Catmull-Rom spline makes the training of a shape model much simpler, and it enables a better way to edit a contour after segmentation.\n\n==Code example in Python==\nThe following is an implementation of the Catmull–Rom spline in Python.\n\n<source lang=\"python\">\nimport numpy\nimport pylab as plt\n\ndef CatmullRomSpline(P0, P1, P2, P3, nPoints=100):\n  \"\"\"\n  P0, P1, P2, and P3 should be (x,y) point pairs that define the Catmull-Rom spline.\n  nPoints is the number of points to include in this curve segment.\n  \"\"\"\n  # Convert the points to numpy so that we can do array multiplication\n  P0, P1, P2, P3 = map(numpy.array, [P0, P1, P2, P3])\n\n  # Calculate t0 to t4\n  alpha = 0.5\n  def tj(ti, Pi, Pj):\n    xi, yi = Pi\n    xj, yj = Pj\n    return ( ( (xj-xi)**2 + (yj-yi)**2 )**0.5 )**alpha + ti\n\n  t0 = 0\n  t1 = tj(t0, P0, P1)\n  t2 = tj(t1, P1, P2)\n  t3 = tj(t2, P2, P3)\n\n  # Only calculate points between P1 and P2\n  t = numpy.linspace(t1,t2,nPoints)\n\n  # Reshape so that we can multiply by the points P0 to P3\n  # and get a point for each value of t.\n  t = t.reshape(len(t),1)\n  print(t)\n  A1 = (t1-t)/(t1-t0)*P0 + (t-t0)/(t1-t0)*P1\n  A2 = (t2-t)/(t2-t1)*P1 + (t-t1)/(t2-t1)*P2\n  A3 = (t3-t)/(t3-t2)*P2 + (t-t2)/(t3-t2)*P3\n  print(A1)\n  print(A2)\n  print(A3)\n  B1 = (t2-t)/(t2-t0)*A1 + (t-t0)/(t2-t0)*A2\n  B2 = (t3-t)/(t3-t1)*A2 + (t-t1)/(t3-t1)*A3\n\n  C  = (t2-t)/(t2-t1)*B1 + (t-t1)/(t2-t1)*B2\n  return C\n\ndef CatmullRomChain(P):\n  \"\"\"\n  Calculate Catmull Rom for a chain of points and return the combined curve.\n  \"\"\"\n  sz = len(P)\n\n  # The curve C will contain an array of (x,y) points.\n  C = []\n  for i in range(sz-3):\n    c = CatmullRomSpline(P[i], P[i+1], P[i+2], P[i+3])\n    C.extend(c)\n\n  return C\n\n# Define a set of points for curve to go through\nPoints = [[0,1.5],[2,2],[3,1],[4,0.5],[5,1],[6,2],[7,3]]\n\n# Calculate the Catmull-Rom splines through the points\nc = CatmullRomChain(Points)\n\n# Convert the Catmull-Rom curve points into x and y arrays and plot\nx,y = zip(*c)\nplt.plot(x,y)\n\n# Plot the control points\npx, py = zip(*Points)\nplt.plot(px,py,'or')\n\nplt.show()\n</source>\n\n==Code example in Unity C#==\n<source lang=\"c#\">\nusing UnityEngine;\nusing System.Collections;\nusing System.Collections.Generic;\n\npublic class Catmul : MonoBehaviour {\n\n    //Use the transforms of GameObjects in 3d space as your points or define array with desired points\n\tpublic Transform[] points;\n\t\n\t//Store points on the Catmull curve so we can visualize them\n\tList<Vector2> newPoints = new List<Vector2>();\n\t\n\t//How many points you want on the curve\n\tfloat amountOfPoints = 10.0f;\n\t\n\t//set from 0-1\n\tpublic float alpha = 0.5f;\n\t\n\t/////////////////////////////\n\t\n\tvoid Update()\n\t{\n\t    CatmulRom();\n\t}\n\t\n\tvoid CatmulRom()\n\t{\n\t\tnewPoints.Clear();\n\n\t\tVector2 p0 = points[0].position; // Vector3 has an implicit conversion to Vector2\n\t\tVector2 p1 = points[1].position;\n\t\tVector2 p2 = points[2].position;\n\t\tVector2 p3 = points[3].position;\n\n\t\tfloat t0 = 0.0f;\n\t\tfloat t1 = GetT(t0, p0, p1);\n\t\tfloat t2 = GetT(t1, p1, p2);\n\t\tfloat t3 = GetT(t2, p2, p3);\n\n\t\tfor(float t=t1; t<t2; t+=((t2-t1)/amountOfPoints))\n\t\t{\n\t\t    Vector2 A1 = (t1-t)/(t1-t0)*p0 + (t-t0)/(t1-t0)*p1;\n\t\t    Vector2 A2 = (t2-t)/(t2-t1)*p1 + (t-t1)/(t2-t1)*p2;\n\t\t    Vector2 A3 = (t3-t)/(t3-t2)*p2 + (t-t2)/(t3-t2)*p3;\n\t\t    \n\t\t    Vector2 B1 = (t2-t)/(t2-t0)*A1 + (t-t0)/(t2-t0)*A2;\n\t\t    Vector2 B2 = (t3-t)/(t3-t1)*A2 + (t-t1)/(t3-t1)*A3;\n\t\t    \n\t\t    Vector2 C = (t2-t)/(t2-t1)*B1 + (t-t1)/(t2-t1)*B2;\n\t\t    \n\t\t    newPoints.Add(C);\n\t\t}\n\t}\n\n\tfloat GetT(float t, Vector2 p0, Vector2 p1)\n\t{\n\t    float a = Mathf.Pow((p1.x-p0.x), 2.0f) + Mathf.Pow((p1.y-p0.y), 2.0f);\n\t    float b = Mathf.Pow(a, 0.5f);\n\t    float c = Mathf.Pow(b, alpha);\n\t   \n\t    return (c + t);\n\t}\n\t\n\t//Visualize the points\n\tvoid OnDrawGizmos()\n\t{\n\t    Gizmos.color = Color.red;\n\t    foreach(Vector2 temp in newPoints)\n\t    {\n\t        Vector3 pos = new Vector3(temp.x, temp.y, 0);\n\t        Gizmos.DrawSphere(pos, 0.3f);\n\t    }\n\t}\n}\n</source>\n\nFor an implementation in 3D space, after converting Vector2 to Vector3 points, the float could be extended a in function GetT to this : Mathf.Pow((p1.x-p0.x), 2.0f) + Mathf.Pow((p1.y-p0.y), 2.0f) + Mathf.Pow((p1.z-p0.z), 2.0f).\n\n== See also ==\n* [[Catmull–Rom spline#Catmull.E2.80.93Rom spline|Cubic Hermite splines]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [https://stackoverflow.com/questions/9489736/catmull-rom-curve-with-no-cusps-and-no-self-intersections/19283471#19283471  Implementation in Java]\n* [https://stackoverflow.com/a/23980479/2929337 Simplified implementation in C++]\n* [http://nbviewer.jupyter.org/github/empet/geom_modeling/blob/master/Catmull-Rom-splines.ipynb Interactive generation via Python, in a Jupyter Notebook]\n\n{{DEFAULTSORT:Centripetal Catmull-Rom spline}}\n[[Category:Splines (mathematics)]]\n[[Category:Articles with example C Sharp code]]\n[[Category:Articles with example Python code]]"
    },
    {
      "title": "Control point (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Control_point_%28mathematics%29",
      "text": "In [[computer-aided geometric design]] a '''control point''' is a member of a set of [[Point (geometry)|points]] used to determine the shape of a [[spline curve]] or, more generally, a  [[computer representation of surfaces|surface]] or higher-dimensional object.<ref>{{citation|title=Curves and Surfaces for Computer Graphics|first=David|last=Salomon|publisher=Springer|year=2007|isbn=9780387284521|page=11|url=https://books.google.com/books?id=r5o5biZPDKEC&pg=PA11}}.</ref>\n\nFor [[Bézier curve]]s, it has become customary to refer to the <math>d</math>-vectors '''p'''<math>_i</math> in a parametric representation <math> \\sum_i</math> '''p'''<math>_i \\phi_i</math> of a curve or surface in <math>d</math>-space as '''control points''',  while the scalar-valued functions <math>\\phi_i</math>, defined over the relevant parameter domain, are the corresponding ''weight'' or ''blending functions''. \nSome would reasonably insist, in order to give intuitive geometric meaning to the word `control', that the blending functions form a [[partition of unity]], i.e., that the <math>\\phi_i</math> are nonnegative and sum to one. This property implies that the curve lies within the [[convex hull]] of its control points.<ref>{{citation|title=Computer Graphics Through OpenGL: From Theory to Experiments|first=Sumanta|last=Guha|publisher=CRC Press|year=2010|isbn=9781439846209|page=663|url=https://books.google.com/books?id=7bCiFepXle0C&pg=PA663}}.</ref> This is the case for Bézier's representation of a polynomial curve as well as for the [[B-spline]] representation of a spline curve or tensor-product spline surface.\n\n==References==\n{{reflist}}\n\n[[Category:Splines (mathematics)]]\n\n\n{{geometry-stub}}"
    },
    {
      "title": "I-spline",
      "url": "https://en.wikipedia.org/wiki/I-spline",
      "text": "In the [[mathematics|mathematical]] subfield of [[numerical analysis]], an '''I-spline'''<ref>{{cite journal | title = On Polya frequency functions. IV. The fundamental spline functions and their limits | last=Curry | first =H.B. |author2=Schoenberg, I.J. | year=1966 | volume=17 | pages=71&ndash;107 | journal=J. Analyse Math. | doi = 10.1007/BF02788653}}</ref><ref>{{cite journal | last=Ramsay | first=J.O. | journal=Statistical Science | year = 1988 | volume=3 | pages=425&ndash;441 | title = Monotone Regression Splines in Action | jstor=2245395 | doi=10.1214/ss/1177012761 | issue=4}}</ref> is a monotone [[Spline (mathematics)|spline]] function.\n\n[[Image:Ispline order3.svg|thumb|325px|An ''I-spline'' family of order three with four interior knots.]]\n\n==Definition==\n\nA family of ''I-spline'' functions of degree ''k'' with ''n'' free parameters is defined in terms of the [[M-spline]]s ''M''<sub>''i''</sub>(''x''|''k'',&nbsp;''t'')\n\n:<math>\nI_i(x|k,t) = \\int_L^x M_i(u|k,t)du,\n</math>\n\nwhere ''L'' is the lower limit of the domain of the splines.\n\nSince M-splines are non-negative, ''I-splines'' are monotonically non-decreasing.\n\n==Computation==\n\nLet ''j'' be the index such that ''t''<sub>''j''</sub>&nbsp;≤&nbsp;''x''&nbsp;<&nbsp;''t''<sub>''j''+1</sub>.  Then ''I''<sub>''i''</sub>(''x''|''k'',&nbsp;''t'') is zero if ''i''&nbsp;>&nbsp;''j'', and equals one if ''j''&nbsp;&minus;&nbsp;''k''&nbsp;+&nbsp;1&nbsp;>&nbsp;''i''.  Otherwise,\n\n:<math>\nI_i(x|k,t) = \\sum_{m=i}^j (t_{m+k+1}-t_m)M_m(x|k+1,t)/(k+1).\n</math>\n\n==Applications==\n\n''I-splines'' can be used as basis splines for regression analysis and [[data transformation (statistics)|data transformation]] when monotonicity is desired (constraining the regression coefficients to be non-negative for a non-decreasing fit, and non-positive for a non-increasing fit).\n\n==References==\n<references/>\n\n[[Category:Splines (mathematics)]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "M-spline",
      "url": "https://en.wikipedia.org/wiki/M-spline",
      "text": "In the [[mathematics|mathematical]] subfield of [[numerical analysis]], an '''M-spline'''<ref>{{cite journal | title = On Polya frequency functions. IV. The fundamental spline functions and their limits | last=Curry | first =H.B. |author2=Schoenberg, I.J.  | year=1966 | volume=17 | pages=71&ndash;107 | journal=J. Analyse Math. | doi = 10.1007/BF02788653}}</ref><ref>{{cite journal | last=Ramsay | first=J.O. | journal=Statistical Science | year = 1988 | volume=3 | issue=4 | pages=425&ndash;441 | title = Monotone Regression Splines in Action | jstor=2245395 | doi=10.1214/ss/1177012761}}</ref> is a non-negative [[Spline (mathematics)|spline]] function.\n\n[[Image:Mspline order3.svg|thumb|325px|An ''M-spline'' family of order three with four interior knots.]]\n\n==Definition==\n\nA family of ''M-spline'' functions of order ''k'' with ''n'' free parameters is defined by a set of knots ''t''<sub>1</sub> &nbsp;≤&nbsp;''t''<sub>2</sub> &nbsp;≤&nbsp; ... &nbsp;≤&nbsp; ''t''<sub>''n''+''k''</sub> such that\n\n* ''t''<sub>1</sub>&nbsp;=&nbsp;...&nbsp;=&nbsp;''t''<sub>''k''</sub>\n* ''t''<sub>''n''+1</sub>&nbsp;=&nbsp;...&nbsp;=&nbsp;''t''<sub>''n''+''k''</sub>\n* ''t''<sub>''i''</sub>&nbsp;<&nbsp;''t''<sub>''i''+''k''</sub> for all ''i''\n\nThe family includes ''n'' members indexed by ''i''&nbsp;=&nbsp;1,...,''n''.\n\n==Properties==\n\nAn ''M-spline'' ''M''<sub>''i''</sub>(''x''|''k'',&nbsp;''t'') has the following mathematical properties\n\n* ''M''<sub>''i''</sub>(''x''|''k'',&nbsp;''t'') is non-negative\n* ''M''<sub>''i''</sub>(''x''|''k'',&nbsp;''t'') is zero unless ''t''<sub>''i''</sub>&nbsp;≤&nbsp;''x''&nbsp;<&nbsp;''t''<sub>''i''+''k''</sub>\n* ''M''<sub>''i''</sub>(''x''|''k'',&nbsp;''t'') has ''k''&nbsp;&minus;&nbsp;2 continuous derivatives at interior knots ''t''<sub>''k''+1</sub>, ..., ''t''<sub>''n''&minus;1</sub>\n* ''M''<sub>''i''</sub>(''x''|''k'',&nbsp;''t'') integrates to 1\n\n==Computation==\n\n''M-splines'' can be efficiently and stably computed using the following recursions:\n\nFor ''k''&nbsp;=&nbsp;1,\n\n:<math>\nM_i(x|1,t) = \\frac{1}{t_{i+1}-t_i}\n</math>\n\nif ''t''<sub>''i''</sub>&nbsp;≤&nbsp;''x''&nbsp;<&nbsp;''t''<sub>''i''+1</sub>, and ''M''<sub>''i''</sub>(''x''|1,''t'')&nbsp;=&nbsp;0 otherwise.\n\nFor ''k''&nbsp;>&nbsp;1,\n\n:<math>\nM_i(x|k,t) = \\frac{k\\left[(x-t_i)M_i(x|k-1,t) + (t_{i+k}-x)M_{i+1}(x|k-1,t)\\right]}{(k-1)(t_{i+k}-t_i)}.\n</math>\n\n==Applications==\n\n''M-splines'' can be integrated to produce a family of monotone splines called [[I-spline]]s.  ''M-splines'' can also be used directly as basis splines for regression analysis involving positive response data (constraining the regression coefficients to be non-negative).\n\n==References==\n<references/>\n\n[[Category:Splines (mathematics)]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Smoothing spline",
      "url": "https://en.wikipedia.org/wiki/Smoothing_spline",
      "text": "{{Use dmy dates|date=October 2012}}\n{{broader|Spline (mathematics)}}\n\n'''Smoothing splines''' are function estimates, <math>\\hat f(x)</math>, obtained from a set of noisy observations <math>y_i</math> of the target <math>f(x_i)</math>, in order to balance a measure of goodness of fit of <math>\\hat f(x_i)</math> to <math>y_i</math> with a derivative based measure of the smoothness of <math>\\hat f(x)</math>. They provide a means for smoothing noisy <math>x_i, y_i</math> data. The most familiar example is the cubic smoothing spline, but there are many other possibilities, including for the case where <math>x</math> is a vector quantity.\n\n==Cubic spline definition==\nLet <math>\\{x_i,Y_i: i = 1,\\dots,n\\}</math> be a set of observations, modeled by the relation <math>Y_i = f(x_i) + \\epsilon_i</math> where the  <math>\\epsilon_i </math> are independent, zero mean random variables (usually assumed to have constant variance). The cubic smoothing spline estimate <math>\\hat f</math> of the function <math>f</math> is defined to be the minimizer (over the class of twice differentiable functions) of<ref name=GS>{{Cite book|title=Nonparametric Regression and Generalized Linear Models: A roughness penalty approach|last=Green|first=P. J.|last2=Silverman|first2=B.W.|year=1994|publisher=Chapman and Hall}}</ref><ref>{{Cite book|title=Generalized Additive Models|last=Hastie|first=T. J.|author2=Tibshirani, R. J. |year=1990|publisher=Chapman and Hall|isbn=978-0-412-34390-2}}</ref>\n:<math>\n\\sum_{i=1}^n \\{Y_i - \\hat f(x_i)\\}^2 + \\lambda \\int \\hat f''(x)^2 \\,dx.\n</math>\n\nRemarks:\n* <math>\\lambda \\ge 0</math> is a smoothing parameter, controlling the trade-off between fidelity to the data and roughness of the function estimate. This is often estimated by generalized cross-validation,<ref>{{cite journal|first=P.|last=Craven|first2=G.|last2=Wahba|title=Smoothing noisy data with spline functions|journal=Numerische Mathematik|year=1979|volume=31|issue=4|pages=377–403|doi=10.1007/bf01404567}}</ref> or by restricted marginal likelihood (REML) which exploits the link between spline smoothing and Bayesian estimation (the smoothing penalty can be viewed as being induced by a prior on the <math>f</math>).<ref>{{cite journal|first=G.S.|last=Kimeldorf|first2=G.|last2=Wahba|title=A Correspondence between Bayesian Estimation on Stochastic Processes and Smoothing by Splines|journal=The Annals of Mathematical Statistics|year=1970|volume=41|issue=2|pages=495–502|doi=10.1214/aoms/1177697089}}</ref>\n* The integral is often evaluated over the whole real line although it is also possible to restrict the range to that of <math>x_i</math>.\n* As <math>\\lambda\\to 0</math> (no smoothing), the smoothing spline converges to the [[interpolating spline]].\n* As <math>\\lambda\\to\\infty</math> (infinite smoothing), the roughness penalty becomes paramount and the estimate converges to a [[Ordinary least squares|linear least squares]] estimate.\n* The roughness penalty based on the [[second derivative]] is the most common in modern statistics literature, although the method can easily be adapted to penalties based on other derivatives.\n* In early literature, with equally-spaced ordered <math>x_i</math>, second or third-order differences were used in the penalty, rather than derivatives.<ref>{{cite journal|first=E.T.|last=Whittaker|title=On a new method of graduation|journal=Proceedings of the Edinburgh Mathematical Society|year=1922|volume=41|pages=63–75}}</ref>\n* The penalized sum of squares smoothing objective can be replaced by a  ''penalized likelihood'' objective in which the sum of squares terms is replaced by another log-likelihood based measure of fidelity to the data.<ref name=GS/> The sum of squares term corresponds to penalized likelihood with a Gaussian assumption on the <math>\\epsilon_i</math>.\n\n==Derivation of the cubic smoothing spline==\n\nIt is useful to think of fitting a smoothing spline in two steps:\n# First, derive the values <math>\\hat f(x_i);i=1,\\ldots,n</math>.\n# From these values, derive <math>\\hat f(x)</math> for all ''x''.\n\nNow, treat the second step first.\n\nGiven the vector <math>\\hat{m} = (\\hat f(x_1),\\ldots,\\hat f(x_n))^T</math> of fitted values, the sum-of-squares part of the spline criterion is fixed. It remains only to minimize <math>\\int \\hat f''(x)^2 \\, dx</math>, and the minimizer is a natural cubic [[Spline (mathematics)|spline]] that interpolates the points <math>(x_i,\\hat f(x_i))</math>. This interpolating spline is a linear operator, and can be written in the form\n:<math>\n  \\hat f(x) = \\sum_{i=1}^n \\hat f(x_i) f_i(x)\n</math>\nwhere <math>f_i(x)</math> are a set of spline basis functions. As a result, the roughness penalty has the form\n:<math>\n\\int \\hat f''(x)^2 dx = \\hat{m}^T A \\hat{m}.\n</math>\nwhere the elements of ''A'' are <math>\\int f_i''(x) f_j''(x)dx</math>. The basis functions, and hence the matrix ''A'', depend on the configuration of the predictor variables <math>x_i</math>, but not on the responses <math>Y_i</math> or <math>\\hat m</math>.\n\n''A'' is an ''n''×''n'' matrix given by <math>A = \\Delta^T W^{-1} \\Delta</math>.\n\n''&Delta;'' is an ''(n-2)''×''n'' matrix of second differences with elements:\n\n<math>\\Delta_{ii} = 1/h_i</math>, <math>\\Delta_{i,i+1} = -1/h_i - 1/h_{i+1}</math>, <math>\\Delta_{i,i+2} = 1/h_{i+1}</math>\n\n''W'' is an ''(n-2)''×''(n-2)'' symmetric tri-diagonal matrix with elements:\n\n<math>W_{i-1,i}=W_{i,i-1}=h_i/6</math>, <math>W_{ii}=(h_i+h_{i+1})/3</math> and <math>h_i=\\xi_{i+1} - \\xi_i</math>, the distances between successive knots (or x values).\n\nNow back to the first step. The penalized sum-of-squares can be written as\n:<math>\n\\{Y - \\hat m\\}^T \\{Y - \\hat m\\} + \\lambda \\hat{m}^T A \\hat m,\n</math>\n\nwhere <math>Y=(Y_1,\\ldots,Y_n)^T</math>.\n\nMinimizing over <math>\\hat m</math> by differentiating against <math>\\hat m</math>. This results in:\n<math> -2 \\{ Y - \\hat m \\} + 2 \\lambda A \\hat m = 0</math> <ref name=\"Rodriguez\">{{cite web|last1=Rodriguez|first1=German|title=Smoothing and Non-Parametric Regression|url=http://data.princeton.edu/eco572/smoothing.pdf|accessdate=28 August 2017|location=2.3.1 Computation|pages=12|language=English|date=Spring 2001}}</ref> and\n<math>\n\\hat m = (I + \\lambda A)^{-1} Y.\n</math>\n\n==De Boor's approach==\n\nDe Boor's approach exploits the same idea, of finding a balance between having a smooth curve and being close to the given data.<ref name=\"DeBoor2001\">{{Cite book|title=A Practical Guide to Splines (Revised Edition)|last=De Boor|first=C.|year=2001|publisher=Springer|pages=207–214|isbn=978-0-387-90356-9}}</ref>\n\n<math>p\\sum_{i=1}^n \\left ( \\frac{Y_i - \\hat f \\left (x_i \\right )}{\\delta_i} \\right )^2+\\left ( 1-p \\right )\\int \\left ( \\hat f^{\\left (m \\right )}\\left ( x \\right ) \\right )^2 \\, dx</math>\n\nwhere <math>p</math> is a parameter called smooth factor and belongs to the interval <math>[0,1]</math>, and <math>\\delta_i;i=1,\\dots,n</math> are the quantities controlling the extent of smoothing (they represent the weight <math>\\delta_i^{-2}</math> of each point <math>Y_i</math>). In practice, since [[cubic splines]] are mostly used, <math>m</math> is usually <math>2</math>. The solution for <math>m=2</math> was proposed by Reinsch in 1967.<ref name=\"Reinsch1967\" /> For <math>m=2</math>, when <math>p</math> approaches <math>1</math>, <math>\\hat f</math> converges to the \"natural\" spline interpolant to the given data.<ref name=\"DeBoor2001\" /> As <math>p</math> approaches <math>0</math>, <math>\\hat f</math> converges to a straight line (the smoothest curve). Since finding a suitable value of <math>p</math> is a task of trial and error, a redundant constant <math>S</math> was introduced for convenience.<ref name=\"Reinsch1967\">{{Cite journal|title=Smoothing by Spline Functions|author=Reinsch, Christian H|doi=10.1007/BF02162161|volume=10|issue = 3|journal=Numerische Mathematik|pages=177–183|year = 1967}}</ref>\n<math>S</math> is used to numerically determine the value of <math>p</math> so that the function <math>\\hat f</math> meets the following condition:\n\n<math>\\sum_{i=1}^n \\left ( \\frac{Y_i - \\hat f \\left (x_i \\right )}{\\delta_i} \\right )^2 \\le S</math>\n\nThe algorithm described by de Boor starts with <math>p=0</math> and increases <math>p</math> until the condition is met.<ref name=\"DeBoor2001\" /> If <math>\\delta_i</math> is an estimation of the standard deviation for <math>Y_i</math>, the constant <math>S</math> is recommended to be chosen in the interval <math>\\left [ n-\\sqrt{2n},n+\\sqrt{2n} \\right ]</math>. Having <math>S=0</math> means the solution is the \"natural\" spline interpolant.<ref name=\"Reinsch1967\" /> Increasing <math>S</math> means we obtain a smoother curve by getting farther from the given data.\n\n==Multidimensional splines==\n\nThere are two main classes of method for generalizing from smoothing with respect to a scalar <math>x</math> to smoothing with respect to a vector \n<math>x</math>. The first approach simply generalizes the spline smoothing penalty to the multidimensional setting. For example, if trying to estimate <math>f(x,z)</math> we might use the [[Thin plate spline]] penalty and find the <math>\\hat f(x,z)</math> minimizing\n:<math>\n\\sum_{i=1}^n \\{y_i - f(x_i,z_i)\\}^2 + \\lambda \\int \\left[\\left(\\frac{\\partial^2 f}{\\partial x^2}\\right)^2 + 2\\left(\\frac{\\partial^2 f}{\\partial x \\partial z}\\right)^2 + \\left(\\frac{\\partial^2 f}{\\partial z^2}\\right)^2 \\right] \\textrm{d} x \\, \\textrm{d}z.\n </math>\nThe thin plate spline approach can be generalized to smoothing with respect to more than two dimensions and to other orders of differentiation in the penalty.<ref name=GS/> As the dimension increases there are some restrictions on the smallest order of differential that can be used,<ref name=GS/> but actually Duchon's original paper,<ref>J. Duchon, 1976, Splines minimizing rotation invariant semi-norms in Sobolev spaces. pp 85–100, In: Constructive Theory of Functions of Several Variables, Oberwolfach 1976, W. Schempp and [[Karl Longin Zeller|K. Zeller]], eds., Lecture Notes in Math., Vol. 571, Springer, Berlin, 1977</ref> gives slightly more complicated penalties that can avoid this restriction.\n\nThe thin plate splines are isotropic, meaning that if we rotate the  <math>x,z</math> co-ordinate system the estimate will not change, but also that we are assuming that the same level of smoothing is appropriate in all directions. This is often considered reasonable when smoothing with respect to spatial location, but in many other cases isotropy is not an appropriate assumption and can lead to sensitivity to apparently arbitrary choices of measurement units. For example, if smoothing with respect to distance and time an isotropic smoother will give different results if distance is measure in metres and time in seconds, to what will occur if we change the units to centimetres and hours.\n\nThe second class of generalizations to multi-dimensional smoothing deals directly with this scale invariance issue using tensor product spline constructions.<ref name=Wahba1990>{{cite book|first=Grace|last=Wahba|title=Spline Models for Observational Data|publisher=SIAM}}</ref><ref name=Gu2013>{{cite book|first=Chong|last=Gu|year=2013|title=Smoothing Spline ANOVA Models (2nd ed.)|publisher=Springer}}</ref><ref name=Wood2017>{{cite book|author = Wood, S. N.|title = Generalized Additive Models: An Introduction with R (2nd ed)|publisher = Chapman & Hall/CRC|year = 2017|isbn=978-1-58488-474-3}}</ref> Such splines have smoothing penalties with multiple smoothing parameters, which is the price that must be paid for not assuming that the same degree of smoothness is appropriate in all directions.\n\n==Related methods==\n{{see also|Curve fitting}}\nSmoothing splines are related to, but distinct from:\n* Regression splines. In this method, the data is fitted to a set of spline basis functions with a reduced set of knots, typically by least squares. No roughness penalty is used. (See also [[multivariate adaptive regression splines]].)\n* Penalized Splines. This combines the reduced knots of regression splines, with the roughness penalty of smoothing splines.<ref>{{Cite book|title=Semiparametric Regression|last=Ruppert|first=David |author2=Wand, M. P. |author3=Carroll, R. J.|publisher=Cambridge University Press|year=2003|isbn=978-0-521-78050-6}}</ref>\n* [[Elastic map]]s method for [[manifold learning]]. This method combines the [[least squares]] penalty for approximation error with the bending and stretching penalty of the approximating manifold and uses the coarse discretization of the optimization problem; see [[thin plate splines]].\n\n==Source code==\n\nSource code for [[Spline (mathematics)|spline]] smoothing can be found in the examples from [[Carl R. de Boor|Carl de Boor's]] book ''A Practical Guide to Splines''. The examples are in the [[Fortran]] [[programming language]]. The updated sources are available also on Carl de Boor's official site [http://pages.cs.wisc.edu/~deboor/].\n\n==References==\n{{Reflist}}\n\n==Further reading==\n\n* Wahba, G. (1990). ''Spline Models for Observational Data''. SIAM, Philadelphia.\n* Green, P. J. and Silverman, B. W. (1994). ''Nonparametric Regression and Generalized Linear Models''. CRC Press.\n* De Boor, C. (2001). ''A Practical Guide to Splines (Revised Edition)''. Springer.\n\n[[Category:Regression analysis]]\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "Spline wavelet",
      "url": "https://en.wikipedia.org/wiki/Spline_wavelet",
      "text": "[[File:Animation showing images of compactly supported B spline wavelets.gif|thumb|right|Animation showing the compactly supported cardinal B-spline wavelets of orders 1, 2, 3, 4 and 5.]]\nIn the [[mathematics|mathematical theory]] of [[wavelet]]s, a '''spline wavelet''' is a wavelet constructed using a [[spline function]].<ref name=ten>{{cite journal|last1=Michael Unser|title=Ten good reasons for using spline wavelets|journal=Proc. SPIE Vol. 3169, Wavelets Applications in Signal and Image Processing V|date=1997|pages=422–431|url=http://bigwww.epfl.ch/publications/unser9702.pdf|accessdate=21 December 2014}}</ref> There are different types of spline wavelets. The interpolatory spline wavelets introduced by C.K. Chui and J.Z. Wang are based on a certain [[Spline (mathematics)|spline]] [[interpolation]] formula.<ref>{{cite journal|last1=Chui, Charles K, and Jian-zhong Wang|title=A cardinal spline approach to wavelets|journal=Proceedings of the American Mathematical Society|date=1991|volume=113|issue=3|pages=785–793|url=http://www.ams.org/journals/proc/1991-113-03/S0002-9939-1991-1077784-X/S0002-9939-1991-1077784-X.pdf|accessdate=22 January 2015|doi=10.2307/2048616|jstor=2048616}}</ref> Though these wavelets are [[Orthogonality|orthogonal]], they do not have [[Compact space|compact]] [[Support (mathematics)|support]]s. There is a certain class of wavelets, unique in some sense, constructed using [[B-spline]]s and having compact supports.  Even though these wavelets are not orthogonal they have some special properties that  have made them quite popular.<ref name=ChuiCompact>{{cite journal|last1=Charles K. Chui and Jian-Zhong Wang|title=On Compactly Supported Spline Wavelets and a Duality Principle|journal=Transactions of the American Mathematical Society|date=April 1992|volume=330|issue=2|pages=903–915|url=http://www.shsu.edu/~mth_jxw/pdfflies/CWTRAN.pdf|accessdate=21 December 2014|doi=10.1090/s0002-9947-1992-1076613-3}}</ref>  The terminology ''spline wavelet'' is sometimes used to refer to the wavelets in this  class of  spline wavelets. These special wavelets are  also called '''B-spline wavelets''' and '''cardinal B-spline wavelets'''.<ref>{{cite book|last1=Charles K Chui|title=An Introduction to Wavelets|date=1992|publisher=Academic Press|page=177|ref=Chui}}</ref> The Battle-Lemarie wavelets are also wavelets constructed using spline functions.<ref name=Daubechies>{{cite book|last1=Ingrid Daubechies|title=Ten Lectures on Wavelets|date=1992|publisher=Society for Industrial and Applied Mathematics|location=Philadelphia|pages=146–153}}</ref>\n\n==Cardinal B-splines==\n\nLet ''n'' be a fixed non-negative [[integer]]. Let ''C''<sup>''n''</sup> denote the set of all [[real-valued function]]s defined over the set of  [[real number]]s such that each  function in the set as well its first ''n'' [[derivative]]s are [[continuous function|continuous]] everywhere. A [[bi-infinite sequence]] . . . ''x''<sub>−2</sub>, ''x''<sub>−1</sub>, ''x''<sub>0</sub>, ''x''<sub>1</sub>, ''x''<sub>2</sub>, . . .  such that ''x''<sub>''r''</sub> < ''x''<sub>''r''+1</sub> for all ''r'' and such that ''x''<sub>''r''</sub> approaches ±∞ as r approaches ±∞ is said to define a set of knots. A ''spline'' of order ''n''  with a set of knots {''x''<sub>''r''</sub>} is a function ''S''(''x'') in ''C''<sup>''n''</sup> such that, for each ''r'',  the restriction of ''S''(''x'') to the interval [''x''<sub>r</sub>, ''x''<sub>''r''+1</sub>) coincides with a [[polynomial]] with real coefficients of degree at most ''n'' in ''x''.\n\nIf the separation ''x''<sub>''r''+1</sub> - ''x''<sub>''r''</sub>, where ''r'' is any integer,   between the successive knots in the set of knots is a constant, the spline is called a ''cardinal spline''. The set of integers ''Z'' = {. . ., -2, -1, 0, 1, 2, . . .} is a standard choice for the set of knots of a cardinal spline. Unless otherwise specified, it is generally assumed that the set of knots is the set of integers.\n\nA cardinal B-spline is a special type of cardinal spline. For any positive integer ''m'' the cardinal B-spline of order ''m'', denoted by ''N''<sub>''m''</sub>(''x''), is defined recursively as follows.\n:<math>N_1(x)=\\begin{cases}1  & 0\\le x <1 \\\\ 0  & \\text{otherwise}\\end{cases}</math>\n:<math>N_m(x)=\\int_0^1 N_{m-1}(x-t)dt</math>, for <math>m>1</math>.\nConcrete expressions for the cardinal B-splines of all orders up to 5 and their graphs are given later in this article.\n\n==Properties of the cardinal B-splines==\n\n===Elementary properties===\n# The [[support (mathematics)|support]] of <math>N_m(x)</math> is the closed interval <math>[0,m]</math>.\n# The function <math>N_m(x)</math> is non-negative, that is, <math>N_m(x)>0</math> for <math>0<x<m</math>.\n# <math>\\sum_{k=-\\infty}^\\infty N_m(x-k)=1</math> for all <math>x</math>.\n# The cardinal B-splines of orders ''m'' and ''m-1'' are related by the identity: <math>N_m(x)=\\frac{x}{m-1}N_{m-1}(x) + \\frac{m-x}{m-1}N_{m-1}(x-1)</math>.\n# The function <math>N_m(x)</math> is symmetrical about <math>x=\\frac{m}{2}</math>, that is, <math>N_m\\left(\\frac{m}{2}-x\\right)=N_m\\left(\\frac{m}{2}+x\\right)</math>.\n# The derivative of <math>N_m(x)</math> is given by <math>N_m^\\prime(x)=N_{m-1}(x)-N_{m-1}(x-1)</math>.\n# <math>\\int_{-\\infty}^\\infty N_m(x)\\, dx =1 </math>\n\n===Two-scale relation===\nThe cardinal B-spline of order ''m'' satisfies the following two-scale relation:\n:<math>N_m(x)=\\sum_{k=0}^m 2^{-m+1}{m \\choose k}N_m(2x-k)</math>.\n\n===Riesz property===\n\nThe cardinal B-spline of order ''m'' satisfies the following property, known as the Riesz property: There exists two positive real numbers <math>A</math> and <math>B</math> such that for any square summable two-sided sequence <math>\\{c_k\\}_{k=-\\infty}^\\infty </math> and for any ''x'',\n\n:<math>A \\left\\Vert \\{c_k \\} \\right\\Vert^2 \\le \\left \\Vert  \\sum_{k=-\\infty}^\\infty c_k N_m(x-k) \\right\\Vert^2 \\le B \\left\\Vert\\{c_k\\}\\right\\Vert^2</math>\n\nwhere <math>\\Vert \\cdot \\Vert</math> is the norm in the ℓ<sup>2</sup>-space.\n\n==Cardinal B-splines of small orders==\nThe cardinal B-splines are defined recursively starting from the B-spline of order 1, namely <math>N_1(x)</math>,  which takes the value 1 in the interval [0, 1) and 0 elsewhere. Computer algebra systems may have to be employed to obtain concrete expressions for higher order cardinal B-splines. The concrete expressions for cardinal B-splines of all orders up to 6 are given below. The graphs of cardinal B-splines of orders up to 4 are also exhibited. In the images, the graphs of the terms contributing to the corresponding two-scale relations are also shown.  The two dots in each image indicate the extremities of the interval supporting the B-spline.\n\n===Constant B-spline===\n\nThe B-spline of order 1, namely <math>N_1(x)</math>, is the constant B-spline. It is defined by\n:<math>N_1(x)=\\begin{cases}1 & 0\\le x < 1 \\\\ 0 &\\text{otherwise}\\end{cases}</math>\nThe two-scale relation for this B-spline is\n:<math>N_1(x)=N_1(2x)+N_1(2x-1)</math>\n\n{| class=\"wikitable\"\n|-\n| style=\"text-align: center;\" |Constant B-spline<br><math>N_1(x)</math> || valign=\"top\" style=\"background: #ffffff;\" | [[File:BSplineOfOrder1.png|250px|center]] || valign=\"top\" style=\"background: #ffffff;\" |[[File:TwoScaleRelationForBSplineOfOrder1.png|center|268px]]\n|}\n\n===Linear B-spline===\nThe B-spline of order 2, namely <math>N_2(x)</math>, is the linear B-spline. It is given by\n\n:<math>N_2(x)=\\begin{cases}x & 0\\le x < 1 \\\\ -x+2 & 1\\le x<2 \\\\ 0 &\\text{otherwise}\\end{cases}</math>\n\nThe two-scale relation for this wavelet is\n\n:<math>N_2(x)=\\frac{1}{2}N_2(2x)+N_2(2x-1)+\\frac{1}{2}N_2(2x-2)</math>\n\n{| class=\"wikitable\"\n|-\n| style=\"text-align: center;\" |Linear B-spline<br><math>N_2(x)</math> || valign=\"top\" style=\"background: #ffffff;\" | [[File:CardinalBSplineOfOrder2.png|250px|center]] || valign=\"top\" style=\"background: #ffffff;\" |[[File:TwoScaleRelationForCardinalBSplineOfOrder2.png|center|278px]]\n|}\n\n===Quadratic B-spline===\nThe B-spline of order 3, namely <math>N_3(x)</math>, is the quadratic B-spline. It is given by\n\n:<math>\nN_3(x)=\n\\begin{cases}\n\\frac{1}{2}x^2 & 0\\le x < 1 \\\\ \n-x^2 +3x-\\frac{3}{2} & 1\\le x<2 \\\\ \n\\frac{1}{2}x^2 -3x + \\frac{9}{2} & 2\\le x<3 \\\\ 0 &\\text{otherwise}\\end{cases}</math>\n\nThe two-scale relation for this wavelet is\n\n:<math>N_3(x)=\\frac{1}{4}N_3(2x)+\\frac{3}{4}N_3(2x-1)+\\frac{3}{4}N_3(2x-2)+\\frac{1}{4}N_3(2x-3)</math>\n\n{| class=\"wikitable\"\n|-\n| style=\"text-align: center;\" |Quadratic B-spline<br><math>N_3(x)</math> || valign=\"top\" style=\"background: #ffffff;\" | [[File:CardinalBSplineOfOrder3.png|250px|center]] || valign=\"top\" style=\"background: #ffffff;\" |[[File:TwoScaleRelationForCardinalBSplineOfOrder3.png|center|295px]]\n|}\n\n===Cubic B-spline===\nThe cubic B-spline is the cardinal B-spline of order 4, denoted by <math>N_4(x)</math>. It is given by the following expressions:\n\n:<math>\nN_4(x)=\n\\begin{cases}\n\\frac{1}{6}x^3 & 0\\le x < 1 \\\\\n-\\frac{1}{2}x^3+2x^2-2x+\\frac{2}{3} & 1\\le x < 2 \\\\\n\\frac{1}{2}x^3-4x^2+10x-\\frac{22}{3} & 2\\le x< 3 \\\\\n- \\frac{1}{6}x^3 +2x^2 -8x +\\frac{32}{3} & 3\\le x < 4 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n</math>\n\nThe two-scale relation for the cubic B-spline is\n\n:<math>\nN_4(x)=\\frac{1}{8}N_4(2x)+\\frac{1}{2}N_4(2x-1)+\\frac{3}{4}N_4(2x-2)+\\frac{1}{2}N_4(2x-3)+\\frac{1}{8}N_4(2x-4)\n</math>\n\n{| class=\"wikitable\"\n|-\n| style=\"text-align: center;\" |Cubic B-spline<br><math>N_4(x)</math> || valign=\"top\" style=\"background: #ffffff;\" | [[File:CardinalBSplineOfOrder4.png|250px|center]] || valign=\"top\" style=\"background: #ffffff;\" |[[File:TwoScaleRelationForCardinalBSplineOfOrder4.png|center|295px]]\n|}\n\n===Bi-quadratic B-spline===\nThe bi-quadratic B-spline is the cardinal B-spline of order 5 denoted by <math>N_5(x)</math>. It is given by\n\n:<math>\nN_5(x)=\n\\begin{cases}\n\\frac{1}{24}x^4 & 0 \\le x < 1 \\\\\n-\\frac{1}{6}x^4+\\frac{5}{6}x^3-\\frac{5}{4}x^2 +\\frac{5}{6}x-\\frac{5}{24} & 1\\le x < 2 \\\\\n\\frac{1}{4}x^4 -\\frac{5}{2} x^3 +\\frac{35}{4}x^2 -\\frac{25}{2}x +\\frac{155}{24} & 2\\le x < 3 \\\\\n-\\frac{1}{6}x^4 +\\frac{5}{2}x^3 -\\frac{55}{4}x^2 +\\frac{65}{2}x -\\frac{655}{24} & 3 \\le x < 4 \\\\\n\\frac{1}{24}x^4 - \\frac{5}{6}x^3 + \\frac{25}{4}x^2 - \\frac{125}{6} x + \\frac{625}{24} & 4 \\le x < 5 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n</math>\n\nThe two-scale relation is\n\n:<math>\nN_5(x)=\\frac{1}{16}N_5(2x)+\\frac{5}{16}N_5(2x-1)+\\frac{10}{16}N_5(2x-2)+\\frac{10}{16}N_5(2x-3)+\\frac{5}{16}N_5(2x-4)+\\frac{1}{16}N_5(2x-5)\n</math>\n\n===Quintic B-spline===\nThe quintic B-spline is the cardinal B-spline of order 6 denoted by <math>N_6(x)</math>. It is given by\n\n:<math>\nN_6(x) =\n\\begin{cases}\n\\frac{1}{120}x^5 & 0\\le x < 1 \\\\\n-\\frac{1}{24}x^5+\\frac{1}{4}x^4 -\\frac{1}{2}x^3 +\\frac{1}{2}x^2 - \\frac{1}{4}x  +\\frac{1}{20} & 1 \\le x < 2 \\\\\n\\frac{1}{12}x^5 - x^4 +\\frac{9}{2} x^3  -\\frac{19}{2}x^2 +\\frac{39}{4}x -\\frac{79}{20} & 2 \\le x < 3 \\\\\n-\\frac{1}{12}x^5 +\\frac{3}{2}x^4 - \\frac{21}{2}x^3 +\\frac{71}{2}x^2 -\\frac{231}{4}x+\\frac{731}{20} & 3 \\le x < 4 \\\\\n\\frac{1}{24}x^5 -x^4 +\\frac{19}{2}x^3 - \\frac{89}{2}x^2 +\\frac{409}{4}x -\\frac{1829}{20} & 4 \\le x < 5 \\\\\n-\\frac{1}{120}x^5 +\\frac{1}{4}x^4 -3x^3 +18x^2 -54 x +\\frac{324}{5} & 5 \\le x < 6 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n</math>\n\n==Multi-resolution analysis generated by cardinal B-splines==\n\nThe cardinal B-spline <math>N_m(x)</math> of order ''m'' generates a [[multi-resolution analysis]]. In fact, from the elementary properties of these functions enunciated above, it follows that the function <math>N_m(x)</math> is [[square integrable]] and is an element of the space <math>L^2(R)</math> of square integrable functions. To set up the multi-resolution analysis the following notations used.\n \n:* For any integers <math>k,j</math>, define the function <math>N_{m,kj}(x)=N_m(2^kx-j)</math>.\n:* For each integer <math>k</math>, define the subspace <math>V_k</math> of <math>L^2(R)</math> as the [[Closure (mathematics)|closure]] of the [[linear span]] of the set <math>\\{ N_{m,kj}(x): j=\\cdots,-2,-1,0,1,2,\\cdots\\}</math>.\n\nThat these define a multi-resolution analysis follows from the following:\n\n# The spaces <math>V_k</math> satisfy the property: <math>\\cdots \\subset V_{-2}\\subset V_{-1}\\subset V_0 \\subset V_1\\subset V_2 \\subset \\cdots</math>.\n# The closure in <math>L^2(R)</math> of the union of all the subspaces <math>V_k</math> is the whole space <math>L^2(R)</math>.\n# The intersection of all the subspaces <math>V_k</math> is the singleton set containing only the zero function.\n# For each integer <math>k</math> the set <math> \\{N_{m,kj}(x): j= \\cdots,-2,-1,0,1,2,\\cdots\\}</math> is an unconditional basis for <math>V_k</math>. (A sequence {''x''<sub>''n''</sub>} in a Banach space ''X'' is an unconditional basis for the space ''X'' if every permutation of the sequence {''x''<sub>''n''</sub>} is also a basis for the same space ''X''.<ref>{{cite book|last1=Christopher Heil|title=A Basis Theory Primer|publisher=Birkhauser|pages=177–188}}</ref>)\n\n==Wavelets from cardinal B-splines==\nLet ''m'' be a fixed positive integer and <math>N_m(x)</math> be the cardinal  B-spline of order ''m''. A function <math>\\psi_m(x)</math> in <math>L^2(R)</math> is a basic wavelet relative to the cardinal B-spline function <math>N_m(x)</math> if the closure in <math>L^2(R)</math> of the linear span of the set <math>\\{\\psi_m(x-j):j=\\cdots, -2,-1,0,1,2,\\cdots\\}</math> (this closure is denoted by <math>W_0</math>) is the [[orthogonal complement]] of <math>V_0</math> in <math>V_1</math>. The subscript ''m'' in <math>\\psi_m(x)</math> is used to indicate that <math>\\psi_m(x)</math> is a basic wavelet relative the cardinal B-spline of order ''m''. There is no unique basic wavelet <math>\\psi_m(x)</math> relative to the cardinal B-spline <math>N_m(x)</math>. Some of these are discussed in the following sections.\n\n==Wavelets relative to cardinal B-splines using fundamental interpolatory splines==\n\n===Fundamental interpolatory spline===\n\n====Definitions====\n\nLet ''m'' be a fixed positive integer and let <math>N_m(x)</math> be the cardinal B-spline of order ''m''. Given a sequence <math>\\{f_j:j=\\cdots, -2,-1,0,1,2,\\cdots \\}</math> of real numbers, the problem of finding a sequence <math>\\{c_{m,k}: k=\\cdots, -2,-1,0,1,2,\\cdots \\}</math> of real numbers such that\n\n:<math>\\sum_{k=-\\infty}^\\infty c_{m,k} N_m\\left(j+\\frac{m}{2}-k\\right) = f_j</math> for all <math>j</math>,\n\nis known as the ''cardinal spline interpolation problem''. The special case of this problem where the sequence <math>\\{f_j\\}</math> is the sequence <math>\\delta_{0j}</math>, where <math>\\delta_{ij}</math> is the Kronecker delta function <math>\\delta_{ij}</math>  defined by\n:<math>\\delta_{ij}=\\begin{cases}1,&\\text{ if } i=j  \\\\ 0, & \\text{ if } i\\ne j \\end{cases}</math>,\nis the ''fundamental cardinal spline interpolation problem''. The solution of the problem yields the ''fundamental cardinal interpolatory spline'' of order ''m''. This spline is denoted by <math>L_m(x)</math> and is given by\n\n:<math> L_m(x) = \\sum_{k=-\\infty}^\\infty c_{m,k}  N_m\\left(x+\\frac{m}{2}-k\\right)</math>\n\nwhere the sequence <math>\\{c_{m,k}\\}</math> is now the solution of the following system of equations:\n\n:<math>\\sum_{k=-\\infty}^\\infty  c_{m,k} N_m\\left(j+\\frac{m}{2}-k\\right) = \\delta_{0j}</math>\n\n====Procedure to find the fundamental cardinal interpolatory spline====\n\nThe fundamental cardinal interpolatory spline <math>L_m(x)</math> can be determined using [[Z-transform]]s. Using the following notations\n\n:<math>A(z)=\\sum_{k=-\\infty}^\\infty \\delta_{k0}z^k =1,</math>\n\n:<math>B_m(z)=\\sum_{k=-\\infty}^\\infty N_m\\left(k+\\frac{m}{2}\\right)z^k,</math>\n\n:<math>C_m(z)=\\sum_{k=-\\infty}^\\infty c_{m,k} z^k,</math>\n\nit can be seen from the equations defining the sequence <math>c_{m,k}</math> that\n\n:<math>B_m(z)C_m(z)=A(z)</math>\n\nfrom which we get\n\n:<math>C_m(z)=\\frac{1}{B_m(z)}</math>.\n\nThis can be used to obtain concrete expressions for <math>c_{m,k}</math>.\n\n====Example====\n\nAs a concrete example, the case <math>L_4(x)</math> may be investigated. The definition of <math>B_m(z)</math> implies that\n\n:<math>B_4(x)=\\sum_{k=-\\infty}^\\infty N_4(2+k)z^k</math>\n\nThe only nonzero values of <math>N_4(k+2)</math> are given by <math>k =-1,0,1</math> and the corresponding values are \n:<math>N_4(1)= \\frac{1}{6}, N_4(2) = \\frac{4}{6}, N_4(3)=\\frac{1}{6}.</math>\nThus <math>B_4(z)</math> reduces to\n\n:<math>B_4(z)=\\frac{1}{6}z^{-1}+\\frac{4}{6}z^0+\\frac{1}{6}z^1=\\frac{1+4z+z^2}{6z}</math>\n\nThis yields the following expression for <math>C_4(z)</math>. \n:<math>C_4(z)=\\frac{6z}{1+4z+z^2}</math>\nSplitting this expression into partial fractions and expanding each term in powers of ''z'' in an annular region the values of <math>c_{4,k}</math> can be computed. These values are then substituted in the expression for <math>L_4(x)</math> to yield\n\n:<math>L_4(x)= \\sum_{k=-\\infty}^\\infty (-1)^k \\sqrt{3}(2-\\sqrt{3})^{|k|}N_4(x+2-k)\n</math>\n\n===Wavelet using fundamental interpolatory spline===\nFor a positive integer ''m'', the function <math>\\psi_m(x)</math> defined by\n\n:<math>\\psi_{I,m}(x)=\\frac{d^m}{dx^m}L_{2m}(2x-1)</math>\n\nis a basic wavelet relative to the cardinal B-spline of order <math>N_m(x)</math>. The subscript ''I'' in <math>\\psi_{I,m}</math> is used to indicate that it is based in the interpolatory spline formula. This basic wavelet is not compactly supported.\n\n===Example===\n\nThe wavelet of order 2 using interpolatory spline is given by\n\n:<math>\\psi_{I,2}(x)=\\frac{d^2}{dx^2}L_4(2x-1)</math>\n \nThe expression for <math>L_4(x)</math> now yields the following formula:\n\n:<math>\\psi_{I,2}(x)=\\frac{d^2}{dx^2}\\sum_{k=-\\infty}^\\infty (-1)^k \\sqrt{3}(2-\\sqrt{3})^{|k|}N_4(2x+1-k) </math>\n\nNow, using the expression for the derivative of <math>N_m(x)</math> in terms of <math>N_{m-1}(x)</math> the function <math>\\psi_2(x)</math> can be put in the following form:\n\n:<math>\\psi_{I,2}(x)=\\sum_{k=-\\infty}^\\infty (-1)^k 4 \\sqrt{3}(2-\\sqrt{3})^{|k|}\\Big((N_2(2x+k-1)-2N_2(2x+k-2)+N_2(2x+k-3)\\Big )</math>\n\nThe following piecewise linear function is the approximation to <math>\\psi_2(x)</math> obtained by taking the sum of the terms corresponding to <math>k=-3, \\ldots, 3</math> in the infinite series expression for <math>\\psi_2(x)</math>.\n\n:<math>\n\\psi_{I,2}(x)\\approx \\begin{cases}\n0.07142668x + 0.17856670 & -2.5< x \\le -2 \\\\ \n-0.48084803 x  -0.92598272 & -2 < x \\le -1.5 \\\\ \n2.0088293 x  +  2.8085333 & -1.5 < x \\le -1 \\\\ \n-7.5684795 x -6.7687755  & -1 < x \\le - 0.5  \\\\\n28.245949 x + 11.138439 & -0.5 < x \\le 0 \\\\ \n-57.415316 x + 11.138439& 0<x \\le 0.5 \\\\ \n57.415316 x  -46.276878& 0.5 < x \\le 1 \\\\\n-28.245949x + 39.384388 & 1< x \\le 1.5\\\\ \n7.5684795 x-14.337255  & 1.5 <x \\le 2\\\\ \n-2.0088293 x +   4.8173625 & 2 < x \\le 2.5 \\\\ \n0.48084803x -1.4068308& 2.5 < x \\le 3\\\\\n-0.07142668 x +0.24999338& 3 < x \\le 3.5 \\\\ \n0 & {otherwise} \n\\end{cases}\n</math>\n\n===Two-scale relation ===\n\nThe  two-scale relation for the wavelet function <math>\\psi_m(x)</math> is given by\n\n:<math>\\psi_{I,m}(x)=\\sum_{-\\infty}^\\infty q_nN_m(2x-n)</math> where <math>q_n= \\sum_{j=0}^m (-1)^j{m \\choose j}c_{m+n-j-1}.</math>\n\n==Compactly supported B-spline wavelets==\nThe spline wavelets generated using the interpolatory wavelets are not compactly supported. Compactly supported B-spline wavelets were discovered by  Charles K. Chui and Jian-zhong Wang and published in 1991.<ref name=ChuiCompact/><ref>{{cite book|last1=Charles K Chui|title=An Introduction to Wavelets|date=1992|publisher=Academic Press|page=249|ref=Chui}}</ref> The compactly supported B-spline wavelet relative to the cardinal B-spline <math>N_m(x)</math> of order ''m'' discovered by Chui and Wong and  denoted by <math>\\psi_{C,m}(x)</math>, has as its support the interval <math>[0, 2m-1]</math>. These wavelets are essentially  unique in a certain sense explained below.\n\n===Definition===\nThe compactly supported B-spline wavelet of order ''m'' is given by\n\n:<math>\\psi_{C,m}(x)=\\frac{1}{2^{2m-1}}\\sum_{j=0}^{2m-2} (-1)^j N_{2m}(j+1)\\frac{d^m}{dx^m}N_{2m}(2x-j)</math>\n\nThis is an ''m''-th order spline. As a special case, the compactly supported B-spline wavelet of order 1 is\n\n:<math>\\psi_{C,1}(x)=\\frac{1}{2}N_2(1)\\frac{d}{dx}N_2(2x) = \\begin{cases}1 & 0\\le x < \\frac{1}{2} \\\\ -1 & \\frac{1}{2} \\le x < 1 \\\\ 0 & \\text{otherwise}\\end{cases} </math>\n\nwhich is the well-known [[Haar wavelet]].\n\n===Properties===\n# The support of <math>\\psi_{C,m}(x)</math> is the closed interval <math>[0, 2m-1]</math>.\n# The wavelet <math>\\psi_{C,m}(x)</math> is the unique wavelet with minimum support in the following sense: If <math>\\eta(x) \\in W_0</math> generates <math>W_0</math> and has support not exceeding <math>2m-1</math> in length then <math>\\eta(x)=c_0\\psi_{C,m}(x-n_0)</math> for some nonzero constant <math>c_0</math> and for some integer <math>n_0</math>.<ref>{{cite book|last1=Charles K Chui|title=An Introduction to Wavelets|date=1992|publisher=Academic Press|page=184}}</ref>\n# <math>\\psi_{C,m}(x)</math> is symmetric for even ''m'' and antisymmetric for odd ''m''.\n\n===Two-scale relation===\n<math>\\psi_m(x)</math> satisfies the two-scale relation:\n\n:<math>\\psi_{C,m}(x)=\\sum_{n=0}^{3m-2}q_nN_m(2x-n)</math> where <math>q_n=\\frac{(-1)^n}{2^{m-1}}\\sum_{j=0}^m {m \\choose j}N_{2m}(n-j+1)</math>.\n\n===Decomposition relation===\n\nThe decomposition relation for the compactly supported B-spline wavelet has the following form:\n\n:<math>N_m(2x-l) = \\sum_{k=-\\infty}^{\\infty} \\left[ a_{m, l-2k}N_m(x-k) + b_{m, l-2k}\\psi_{C,m}(x-k)\\right] </math>\n\nwhere the coefficients <math>a_{m,j}</math> and <math>b_{m,j}</math> are given by\n\n: <math>a_{m,j}= - \\frac{(-1)^j}{2}\\sum_{l=-\\infty}^\\infty q_{-j+2m-2l+1}c_{2m,l},</math>\n\n: <math>b_{m,j}=  \\frac{(-1)^j}{2}\\sum_{l=-\\infty}^\\infty p_{-j+2m-2l+1}c_{2m,l}.</math>\n\nHere the sequence <math>c_{2m,l}</math> is the sequence of coefficients in the fundamental interpolatoty cardinal spline wavelet of order ''m''.\n\n== Compactly supported B-spline wavelets of small orders==\n\n===Compactly supported B-spline wavelet of order 1 ===\n\nThe two-scale relation for the compactly supported B-spline wavelet of order 1 is\n\n:<math>\\psi_{C,1}(x)= N_1(2x)-N_1(2x-1)</math>\n\nThe closed form expression for compactly supported B-spline wavelet of order 1 is\n\n:<math>\n\\psi_{C,1}(x)=\n\\begin{cases}\n1 & 0\\le x < \\frac{1}{2} \\\\\n-1 & \\frac{1}{2} \\le x < 1\\\\\n0 & \\text{otherwise}\n\\end{cases}\n</math>\n\n===Compactly supported B-spline wavelet of order 2 ===\n\nThe two-scale relation for the compactly supported B-spline wavelet of order 2 is\n\n:<math>\\psi_{C,2}(x)= \\frac{1}{12}\\left(N_2(2x)-6 N_2(2x-1)+ 10 N_2(2x-2)-6 N_2(2x-3)+ N_2(2x-4)\\right)</math>\n\nThe closed form expression for compactly supported B-spline wavelet of order 2 is\n\n:<math>\n\\psi_{C,2}(x)=\n\\begin{cases}\n\\frac{1}{6}x                    & 0\\le x < \\frac{1}{2}\\\\ \n -\\frac{7}{6}x + \\frac{2}{3}     & \\frac{1}{2} \\le x < 1\\\\ \n \\frac{8}{3}x - \\frac{19}{6}    & 1 \\le x < \\frac{3}{2}\\\\  \n-\\frac{8}{3} x + \\frac{29}{6}   & \\frac{3}{2} \\le x < 2  \\\\  \n\\frac{7}{6} x- \\frac{17}{6}     & 2 \\le x < \\frac{5}{2}\\\\  \n -\\frac{1}{6} x + \\frac{1}{2}   & \\frac{5}{2} \\le x < 3 \\\\  \n0                               & \\text{otherwise}\n\\end{cases}\n</math>\n\n===Compactly supported B-spline wavelet of order 3 ===\n\nThe two-scale relation for the compactly supported B-spline wavelet of order 3 is\n\n:<math>\\psi_{C,3}(x)= \\frac{1}{480}\\Big[ (N_3(2x)-29 N_3(2x-1)+ 147 N_3(2x-2)- 303 N_3(2x-3)+   </math>\n:::::<math>303N_3(2x-4) - 147N_3(2x-5) + 29 N_3(2x-6) - N_3(2x-7)\\Big]</math>\nThe closed form expression for compactly supported B-spline wavelet of order 3 is\n\n:<math>\n\\psi_{C,3}(x)=\n\\begin{cases}\n \\frac{1}{240}x^2                & 0\\le x < \\frac{1}{2}\\\\ \n- \\frac{31}{240}x^2+ \\frac{2}{15}x- \\frac{1}{30}    & \\frac{1}{2} \\le x < 1\\\\ \n\\frac{103}{120}x^2- \\frac{221}{120}x + \\frac{229}{240}   & 1 \\le x < \\frac{3}{2}\\\\  \n -\\frac{313}{120} x^2+ \\frac{1027}{120}x- \\frac{1643}{240}  & \\frac{3}{2} \\le x < 2  \\\\  \n \\frac{22}{5} x^2 - \\frac{779}{40} x + \\frac{339}{16}    & 2 \\le x < \\frac{5}{2}\\\\  \n -\\frac{22}{5} x^2 + \\frac{981}{40} x- \\frac{541}{16}  & \\frac{5}{2} \\le x < 3 \\\\  \n \\frac{313}{120}x^2-\\frac{701}{40}x+ \\frac{2341}{80 }                              & 3 \\le x < \\frac{7}{2} \\\\\n -\\frac{103}{120}x^2 +\\frac{809}{120}x- \\frac{3169}{240 }                             & \\frac{7}{2} \\le x < 4 \\\\\n \\frac{31}{240}x^2-\\frac{139}{120}x+\\frac{623}{240}                               & 4 \\le x < \\frac{9}{2} \\\\\n -\\frac{1}{240}x^2+\\frac{1}{24}x-\\frac{5}{48}                             & \\frac{9}{2} \\le x < 5 \\\\\n    0                            & \\text{otherwise}\n\\end{cases}\n</math>\n\n===Compactly supported B-spline wavelet of order 4 ===\n\nThe two-scale relation for the compactly supported B-spline wavelet of order 4 is\n\n:<math>\\psi_{C,4}(x)= \\frac{1}{40320}\\Big[  N_4(2x)- 124 N_4(2x-1)+ 1677 N_4(2x-2)- 7904 N_4(2x-3)+  18482 N_4(2x-4) - </math>\n:::::<math>24264 N_4(2x-5) + 18482N_4(2x-6) - 7904 N_4(2x-7) + 1677 N_4(2x-8) - 124N_4(2x-9) + N_4(2x-10)\\Big]</math>\nThe closed form expression for compactly supported B-spline wavelet of order 4 is\n\n:<math>\n\\psi_{C,4}(x)=\n\\begin{cases}\n \\frac{1}{30240}x^3            & 0\\le x < \\frac{1}{2}\\\\ \n-\\frac{127}{30240}x^3+\\frac{2}{315}x^2-\\frac{1}{315}x+\\frac{1}{1890 }                              & \\frac{1}{2} \\le x < 1\\\\ \n\\frac{19}{280}x^3-\\frac{47}{224}x^2+\\frac{2147}{10080}x-\\frac{103}{1440 }                              & 1 \\le x < \\frac{3}{2}\\\\  \n -\\frac{1109}{2520}x^3+\\frac{465}{224}x^2-\\frac{32413}{10080}x+\\frac{16559}{10080 }                             & \\frac{3}{2} \\le x < 2  \\\\  \n \\frac{5261}{3360}x^3-\\frac{33463}{3360}x^2+\\frac{42043}{2016}x-\\frac{145193}{10080}                              & 2 \\le x < \\frac{5}{2}\\\\  \n-\\frac{35033}{10080}x^3+\\frac{93577}{3360} x^2- \\frac{148517}{2016}x+ \\frac{216269}{3360}                              & \\frac{5}{2} \\le x < 3 \\\\  \n\\frac{4832}{945}x^3- \\frac{27691}{560}x^2+ \\frac{113923}{720}x-\\frac{28145}{168}                             & 3 \\le x < \\frac{7}{2} \\\\\n -\\frac{4832}{945}x^3+\\frac{58393}{1008}x^2-\\frac{52223}{240}x+\\frac{2048227}{7560}                           & \\frac{7}{2} \\le x < 4 \\\\\n \\frac{35033}{10080}x^3-\\frac{75827}{1680}x^2+\\frac{981101}{5040}x- \\frac{234149}{840}                           & 4 \\le x < \\frac{9}{2} \\\\\n -\\frac{5261}{3360}x^3+\\frac{38509}{1680}x^2-\\frac{112487}{1008}x+ \\frac{30347}{168}                          & \\frac{9}{2} \\le x < 5 \\\\\n \\frac{1109}{2520}x^3-\\frac{24077}{3360}x^2+\\frac{78311}{2016}x- \\frac{141311}{2016}                          & 5 \\le x < \\frac{11}{2} \\\\\n -\\frac{19}{280}x^3+\\frac{1361}{1120}x^2-\\frac{14617}{2016}x+\\frac{4151}{288}                         & \\frac{11}{2} \\le x < 6 \\\\\n \\frac{127}{30240}x^3-\\frac{55}{672}x^2+\\frac{5359}{10080}x-\\frac{11603}{10080}                        & 6 \\le x < \\frac{13}{2} \\\\\n -\\frac{1}{30240}x^3+\\frac{1}{1440}x^2-\\frac{7}{1440}x+ \\frac{49}{4320}                          & \\frac{13}{2} \\le x < 7 \\\\\n    0                            & \\text{otherwise}\n\\end{cases}\n</math>\n\n===Compactly supported B-spline wavelet of order 5 ===\n\nThe two-scale relation for the compactly supported B-spline wavelet of order 5 is\n\n:<math> \\psi_{C,5}(x)= \\frac{1}{5806080}\\Big[N_5(2x)-507 N_5(2x-1)+17128 N_5(2x-2)-166304 N_5(2x-3)+ 748465N_5(2x-4) </math>\n:::::<math> -1900115N_5(2x-5)+2973560 N_5(2x-6)-2973560 N_5(2x-7)+1900115N_5(2x-8)</math>\n:::::<math> -748465 N_5(2x-9)+ 166304 N_5(2x-10)-17128N_5(2x-11)+507N_5(2x-12)-N_5(2x-13)\\Big]</math>\nThe closed form expression for compactly supported B-spline wavelet of order 5 is\n:<math>\n\\psi_{C,5}(x)=\n\\begin{cases}\n\\frac{1}{8709120}x^4 &  0\\le x < \\frac{1}{2} \\\\\n - \\frac{73}{1244160}x^4+\\frac{1}{8505}x^3-\\frac{1}{11340}x^2+\\frac{1}{34020}x-\\frac{1}{272160} & \\frac{1}{2} \\le x < 1 \\\\\n \\frac{9581}{4354560}x^4-\\frac{19417}{2177280}x^3+\\frac{1303}{96768}x^2-\\frac{19609}{2177280}x+\\frac{6547}{2903040} & 1\\le x < \\frac{3}{2} \\\\\n -\\frac{118931}{4354560}x^4+\\frac{366119}{2177280}x^3-\\frac{186253}{483840}x^2+\\frac{121121}{311040}x-\\frac{427181}{2903040} & \\frac{3}{2} \\le x < 2 \\\\\n \\frac{759239}{4354560}x^4-\\frac{3146561}{2177280}x^3+\\frac{6466601}{1451520}x^2-\\frac{13202873}{2177280}x+\\frac{26819897}{8709120} & 2\\le x < \\frac{5}{2} \\\\\n -\\frac{2980409}{4354560}x^4+\\frac{5183893}{725760}x^3-\\frac{13426333}{483840}x^2+\\frac{426589}{8960}x-\\frac{12635243}{414720} & \\frac{5}{2}\\le x < 3 \\\\\n \\frac{7873577}{4354560}x^4-\\frac{16524079}{725760}x^3+\\frac{7385369}{69120}x^2-\\frac{17868671}{80640}x+\\frac{497668543}{290304} &  3\\le x < \\frac{7}{2} \\\\\n - \\frac{14714327}{4354560}x^4+\\frac{108543091}{2177280}x^3-\\frac{56901557}{207360}x^2+\\frac{1454458651}{2177280}x-\\frac{5286189059}{8709120} & \\frac{7}{2}\\le x < 4 \\\\\n \\frac{15619}{3402}x^4-\\frac{33822017}{435456}x^3+\\frac{15828929}{32256}x^2-\\frac{597598433}{435456}x+\\frac{277413649}{193536} & 4\\le x < \\frac{9}{2} \\\\\n -\\frac{15619}{3402}x^4+\\frac{38150335}{435456}x^3-\\frac{20157247}{32256}x^2+ \\frac{859841695}{435456}x- \\frac{64472345}{27648} &\\frac{9}{2}\\le x < 5 \\\\\n \\frac{14714327}{4354560}x^4-\\frac{4466137}{62208}x^3+\\frac{165651247}{290304}x^2-\\frac{875490655}{435456}x+\\frac{4614904015}{1741824} & 5\\le x < \\frac{11}{2} \\\\\n -\\frac{7873577}{4354560}x^4+\\frac{30717383}{725760}x^3- \\frac{179437319}{483840}x^2+ \\frac{16606729}{11520}x- \\frac{869722273}{414720} &  \\frac{11}{2}\\le x < 6 \\\\\n \\frac{2980409}{4354560}x^4- \\frac{12698561}{725760}x^3+ \\frac{16211669}{96768}x^2-\\frac{19138891}{26880}x+ \\frac{3289787993}{2903040} & 6\\le x < \\frac{13}{2} \\\\\n -\\frac{759239}{4354560}x^4+\\frac{10519741}{2177280}x^3- \\frac{10403603}{207360}x^2+ \\frac{71964499}{311040}x-\\frac{3481646837}{8709120} & \\frac{13}{2} \\le x < 7 \\\\\n \\frac{118931}{4354560}x^4-\\frac{1774639}{2177280}x^3+\\frac{630259}{69120}x^2-\\frac{14096161}{311040}x+\\frac{245108501}{2903040} & 7\\le x < \\frac{15}{2} \\\\\n -\\frac{9581}{4354560}x^4+\\frac{21863}{311040}x^3-\\frac{407387}{483840}x^2+\\frac{9758873}{2177280}x-\\frac{25971499}{2903040} & \\frac{15}{2} \\le x < 8 \\\\\n\\frac{73}{1244160}x^4-\\frac{4343}{2177280}x^3+ \\frac{5273}{207360}x^2-\\frac{313703}{2177280}x+ \\frac{380873}{1244160} & 8\\le x < \\frac{17}{2} \\\\\n -\\frac{1}{8709120}x^4+ \\frac{1}{241920}x^3- \\frac{1}{17920}x^2+\\frac{3}{8960}x-\\frac{27}{35840} &  \\frac{17}{2} \\le x < 9\\\\\n 0 & \\text{otherwise}\n\\end{cases}\n</math>\n\n===Images of compactly supported B-spline wavelets===\n\n{| class=\"wikitable\"\n|-\n| valign=\"top\" style=\"background: #ffffff;\" | [[File:CardinalBSplineWaveletOfOrder1.png|250px|center]] || valign=\"top\" style=\"background: #ffffff;\" |[[File:CardinalBSplineWaveletOfOrder2.png|center|250px]] ||  \n|-\n|B-spline wavelet of order 1 || B-spline wavelet of order 2 ||  \n|-\n| valign=\"top\" style=\"background: #ffffff;\" |[[File:CardinalBSplineWaveletOfOrder3.png|center|250px]] || valign=\"top\" style=\"background: #ffffff;\" |[[File:CardinalBSplineWaveletOfOrder4.png|center|250px]] || valign=\"top\" style=\"background: #ffffff;\" |[[File:CardinalBSplineWaveletOfOrder5.png|center|250px]]\n|-\n|B-spline wavelet of order 3 || B-spline wavelet of order 4 || B-spline wavelet of order 5\n|}\n\n==Battle-Lemarie wavelets==\nThe Battle-Lemarie wavelets form a class of orthonormal wavelets constructed using the class of cardinal B-splines. The expressions for these wavelets are given in the frequency domain; that is, they are defined by specifying their Fourier transforms. The Fourier transform of a function of ''t'', say, <math>F(t)</math>, is denoted by <math>\\hat{F}(\\omega)</math>.\n\n===Definition===\nLet ''m'' be a positive integer and let <math>N_m(x)</math> be the cardinal B-spline of order ''m''. The Fourier transform of <math>N_m(x)</math> is <math>\\hat{N}_m(\\omega)</math>. The scaling function <math>\\phi_m(t)</math> for the ''m''-th order Battle-Lemarie wavelet is that function whose Fourier transform is\n\n:<math>\\hat{\\phi}_m(\\omega) = \\frac{\\hat{N}_m(\\omega)}{\\left(\\sum_{k=-\\infty}^\\infty \\vert \\hat{N}_m(\\omega +2\\pi k) \\vert^2\\right)^{1/2}}.</math>\n\nThe ''m''-th order Battle-Lemarie wavelet is the function <math>\\psi_{BL,m}(t)</math> whose Fourier transform is\n\n:<math>\\hat{\\psi}_{BL,m}(\\omega) = - \\frac{e^{-i\\omega/2}\\,\\, \\overline{\\hat{\\phi}_m(\\omega + 2\\pi)}\\,\\,\\hat{\\phi}_m\\left(\\frac{\\omega}{2}\\right)}{\\overline{ \\hat{\\phi}_m\\left(\\frac{\\omega}{2}+\\pi\\right)}}</math>\n\n==References==\n{{reflist}}\n\n==Further reading==\n\n*{{cite journal|last1=Amir Z Averbuch and Valery A Zheludev|title=Wavelet transforms generated by splines|journal=International Journal of Wavelets, Multiresolution and Information Processing|date=2007|volume=257|issue=5|url=http://www.cs.tau.ac.il/~zhel/PS/splitr3AA.pdf|accessdate=21 December 2014}}\n*{{cite book|last1=Amir Z. Averbuch, Pekka Neittaanmaki, and Valery A. Zheludev|title=Spline and Spline Wavelet Methods with Applications to Signal and Image Processing Volume I|date=2014|publisher=Springer|isbn=978-94-017-8925-7}}\n\n[[Category:Wavelets]]\n[[Category:Continuous wavelets]]\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "T-spline",
      "url": "https://en.wikipedia.org/wiki/T-spline",
      "text": "A '''T-spline''' surface can be thought of as a [[NURBS]] surface for which a row of control points is allowed to terminate without traversing the entire surface. The control net at a terminated row resembles the letter \"T\". Modeling surfaces with T-splines can reduce the number of control points in comparison to NURBS surfaces and make pieces easier to merge, but increases the book-keeping effort to keep track of the irregular connectivity. T-splines can be converted into NURBS surfaces, by knot insertion, and NURBS can be represented as T-splines without T's or by removing knots.<ref>Thomas W. Sederberg, Jianmin Zheng, Tom Lyche, David Cardon, G. Thomas Finnigan, Nicholas North: T-Splines Simplification and Local Refinment, from ACM Trans. Graph. (SIGGraph 2004)</ref> T-splines can therefore, in theory, do everything that NURBS can do. In practice, enormous amount of programming was required to make NURBS work as well as they do, and creating the equivalent T-Spline functionality would require similar effort. To smoothly join at points where more than three surface pieces meet, T-splines have been combined with [[Smooth function#Geometric continuity|geometrically continuous]] constructions of degree 3 by 3 (bi-cubic)<ref>J. Fan, J Peters, On Smooth Bicubic Surfaces from Quad Meshes, ISVC 2008, see also: Computer Aided Design 2011, 43(2): 180-187</ref> and, more recently, of degree 4 by 4 (bi-quartic).<ref>J Peters,Biquartic C^1 spline surfaces over irregular meshes, Computer Aided Design 1995 27 (12) p 895--903</ref><ref>M.A. Scott and R.N. Simpson and J.A. Evans and S. Lipton and S.P.A. Bordas and T.J.R. Hughes and T.W. Sederberg, Isogeometric boundary element analysis using unstructured T-splines, Computer Methods in Applied Mechanics and Engineering, 2013 254. p 197-221</ref><ref>G. Westgaard, H Nowacki, Construction of fair surfaces over irregular meshes, Symposium on Solid Modeling and Applications 2001: 88-98</ref>\n\nT-splines, [[subdivision surface]]s, [[NURBS]] surfaces, and [[polygon mesh]]es are alternative technologies. Subdivision surfaces, as well as T-spline and NURBS surfaces with the addition of geometrically continuous constructions, can represent everywhere-smooth surfaces of any connectivity and topology, such as holes, branches, and handles. However, none of T-splines, subdivision surfaces, NURBS surfaces can always accurately represent the (exact, algebraic) intersection of two surfaces within the same surface representation. Polygon meshes can represent exact intersections but lack the shape quality required in industrial design. Subdivision surfaces are widely adopted in the animation industry.  Pixar's variant of the subdivision surfaces has the advantage of edge weights. T-splines do not yet have edge weights.\n\nT-splines were initially defined in 2003.<ref>Thomas W. Sederberg, Jianmin Zheng, Almaz Bakenov, Ahmad Nasri: T-Splines and T-NURCCS, from ACM Trans. Graph. (SIGGraph 2003)</ref>  In 2007 the U.S. patent office granted patent number 7,274,364 for technologies related to T-Splines. T-Splines, Inc. was founded in 2004 to commercialize the technologies and acquired by Autodesk, Inc. in 2011.<ref>http://www.businesswire.com/news/home/20111222005259/en/Autodesk-Acquires-T-Splines-Modeling-Technology-Assets</ref>\n\n== External links ==\n* [https://web.archive.org/web/20120713095920/http://www.tsplines.com/technicalpapers.html Technical articles about T-splines]\n* [https://www.youtube.com/watch?v=suv90ahXa5s Transitioning from NURBS to T-splines (67-minute video)]\n* [http://isicad.net/articles.php?article_num=14940 NURBS and CAD: 30 Years Together]\n* [https://sourceforge.net/projects/tspline/ An open source T-spline kernel]\n\n== References ==\n{{Reflist}}\n\n[[Category:Computer-aided design]]\n[[Category:Splines (mathematics)]]"
    },
    {
      "title": "Fixed-point iteration",
      "url": "https://en.wikipedia.org/wiki/Fixed-point_iteration",
      "text": "{{refimprove|date=May 2010}}\n{{main|Infinite compositions of analytic functions}}\nIn [[numerical analysis]], '''fixed-point iteration''' is a method of computing [[fixed point (mathematics)|fixed points]] of [[iterated function]]s.\n\nMore specifically, given a  function <math>f</math> defined on the [[real number]]s with real values and given a point <math>x_0</math> in the [[domain of a function|domain]] of <math>f</math>, the fixed point [[iteration]] is\n\n:<math>x_{n+1}=f(x_n), \\, n=0, 1, 2, \\dots</math>\n\nwhich gives rise to the [[sequence]] <math>x_0, x_1, x_2, \\dots</math> which is hoped to [[limit (mathematics)|converge]] to a point <math>x</math>. If <math>f</math> is continuous, then one can prove that the obtained <math>x</math> is a fixed point of <math>f</math>, i.e.,\n\n:<math>f(x)=x. \\, </math>\n\nMore generally, the function <math>f</math> can be defined on any [[metric space]] with values in that same space.\n\n==Examples==\n\n* A first simple and useful example is the [[Babylonian method]] for computing the [[square root]] of ''a''>0, which consists in taking <math>f(x)=\\frac 12\\left(\\frac ax + x\\right)</math>, i.e. the mean value of ''x'' and ''a/x'', to approach the limit <math>x = \\sqrt a</math> (from whatever starting point <math>x_0 \\gg 0 </math>). This is a special case of [[Newton's method]] quoted below.\n\n[[File:Sine fixed point.svg|250px|thumb|The fixed-point iteration ''x''<sub>''n''+1</sub>&nbsp;=&nbsp;sin ''x''<sub>''n''</sub> with initial value ''x''<sub>0</sub> = 2 converges to 0. This example does not satisfy the assumptions of the [[Banach fixed point theorem]] and so its speed of convergence is very slow.]]\n\n* The fixed-point iteration <math>x_{n+1}=\\cos x_n\\,</math> converges to the unique fixed point of the function <math>f(x)=\\cos x\\,</math> for any starting point <math>x_0.</math> This example does satisfy the assumptions of the [[Banach fixed point theorem]]. Hence, the error after n steps satisfies <math>|x_n-x| \\leq { q^n \\over 1-q } | x_1 - x_0 | = C q^n</math> (where we can take <math>q = 0.85</math>, if we start from <math>x_0=1</math>.) When the error is less than a multiple of <math>q^n</math> for some constant ''q'', we say that we have [[linear convergence]]. The Banach fixed-point theorem allows one to obtain fixed-point iterations with linear convergence.\n* The fixed-point iteration <math>x_{n+1}=2x_n\\,</math> will diverge unless <math>x_0=0</math>. We say that the fixed point of <math>f(x)=2x\\,</math> is repelling.\n* The requirement that ''f'' is continuous is important, as the following example shows. The iteration\n:<math> x_{n+1} = \n\\begin{cases}\n\\frac{x_n}{2}, & x_n \\ne 0\\\\\n1, & x_n=0\n\\end{cases}</math>\nconverges to 0 for all values of  <math>x_0</math>.\nHowever, 0 is ''not'' a fixed point of the function\n:<math>f(x) = \n\\begin{cases}\n\\frac{x}{2}, & x \\ne 0\\\\\n1, & x = 0\n\\end{cases}</math>\nas this function is ''not'' continuous at <math>x=0</math>, and in fact has no fixed points.\n\n==Applications==\n* [[Newton's method]] for finding roots of a given differentiable function {{tmath|f(x)}}  is\n:: <math>x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}.</math>\n:If we write <math>g(x)=x-\\frac{f(x)}{f'(x)}</math>, we may rewrite the Newton iteration as the fixed-point iteration\n:: <math>x_{n+1}=g(x_n)</math>.\n:If this iteration converges to a fixed point {{mvar|x}} of {{mvar|g}}, then\n:: <math>x=g(x)=x-\\frac{f(x)}{f'(x)}</math>, so <math>f(x)/f'(x)=0.</math>\n:The reciprocal of anything is nonzero, therefore {{math|''f''(''x'') {{=}} 0}}: {{mvar|x}} is a ''root'' of {{mvar|f}}. Under the assumptions of the [[Banach fixed point theorem]], the Newton iteration, framed as the fixed point method, demonstrates [[linear convergence]]. However, a more detailed analysis shows [[quadratic convergence]], i.e., \n:: <math>|x_n-x|<Cq^{2^n}</math>, under certain circumstances.\n\n* [[Halley's method]] is similar to [[Newton's method]] but, when it works correctly, its error is <math>|x_n-x|<Cq^{3^n}</math> ([[cubic convergence]]). In general, it is possible to design methods that converge with speed <math>Cq^{k^n}</math> for any <math>k\\in \\mathbb N</math>. As a general rule, the higher the {{mvar|k}}, the less stable it is, and the more computationally expensive it gets. For these reasons, higher order methods are typically not used.\n* [[Runge–Kutta method]]s and numerical [[ordinary differential equation]] solvers in general can be viewed as fixed point iterations. Indeed, the core idea when analyzing the [[A-stability]] of ODE solvers is to start with the special case <math>y'=ay</math>, where a is a [[complex number]], and to check whether the ODE solver converges to the fixed point <math>y=0</math> whenever the real part of a is negative.{{efn|One may also consider certain iterations A-stable if the iterates stay bounded for a long time, which is beyond the scope of this article.}}\n* The [[Picard–Lindelöf theorem]], which shows that ordinary differential equations have solutions, is essentially an application of the [[Banach fixed point theorem]] to a special sequence of functions which forms a fixed point iteration, constructing the solution to the equation. Solving an ODE in this way is called '''Picard iteration''', '''Picard's method''', or the '''Picard iterative process'''.\n* The iteration capability in Excel can be used to find solutions to the [[Colebrook equation]] to an accuracy of 15 significant figures.<ref>M A Kumar (2010), Solve Implicit Equations (Colebrook) Within Worksheet, Createspace, {{ISBN|1-4528-1619-0}}\n</ref><ref>Brkic, Dejan (2017) Solution of the Implicit Colebrook Equation for Flow Friction Using Excel, Spreadsheets in Education (eJSiE): Vol. 10: Iss. 2, Article 2. Available at: https://sie.scholasticahq.com/article/4663-solution-of-the-implicit-colebrook-equation-for-flow-friction-using-excel</ref>\n\n* Some of the \"successive approximation\" schemes used in [[dynamic programming]] to solve  [[Bellman equation|Bellman's functional equation]] are based on fixed point iterations in the space of the return function.<ref>Bellman, R. (1957). Dynamic programming, Princeton University Press.</ref><ref>Sniedovich, M. (2010). Dynamic Programming: Foundations and Principles, [[Taylor & Francis]].</ref>\n\n==Contractions==\nIf a function <math>f</math> defined on the real line with real values is [[Lipschitz continuity|Lipschitz continuous]] with Lipschitz constant <math>L<1</math>, then this function has precisely one fixed point, and the fixed-point iteration converges towards that fixed point for any initial guess <math>x_0.</math> This theorem can be generalized to any metric space.\n\nProof of this theorem:\n\nSince <math>f</math> is Lipschitz continuous with Lipschitz constant <math>L<1</math>, then for the sequence <math>\\{x_n,n=0,1,2,\\ldots\\}</math>, we have:\n\n: <math>\n\\begin{align}\n|x_2-x_1|=|f(x_1)-f(x_0)| & \\leq L|x_1-x_0|, \\\\\n|x_3-x_2|=|f(x_2)-f(x_1)| & \\leq L|x_2-x_1|, \\\\\n& \\,\\,\\, \\vdots\n\\end{align}\n</math>\n\nand\n\n: <math>|x_n-x_{n-1}|=|f(x_{n-1})-f(x_{n-2})|\\leq L|x_{n-1}-x_{n-2}|.</math>\n\nCombining the above inequalities yields:\n\n: <math>|x_n-x_{n-1}|\\leq L^{n-1}|x_1-x_0|.</math>\n\nSince <math>L<1</math>, <math>L^{n-1}\\rightarrow 0</math> as <math>n \\rightarrow \\infty.</math>\n\nTherefore, we can show  <math>\\{x_n\\}</math> is a [[Cauchy sequence]] and thus it converges to a point <math>x^*</math>.\n\nFor the iteration <math>x_n=f(x_{n-1})</math>, let <math>n</math> go to infinity on both sides of the equation, we obtain <math>x^*=f(x^*)</math>. This shows that <math>x^*</math> is the fixed point for <math>f</math>. So we proved the iteration will eventually converge to a fixed-point.\n\nThis property is very useful because not all iterations can arrive at a convergent fixed-point. When constructing a fixed-point iteration, it is very important to make sure it converges. There are several [[fixed-point theorem]]s to guarantee the existence of the fixed point, but since the iteration function is continuous, we can usually use the above theorem to test if an iteration converges or not. The proof of the generalized theorem to metric space is similar.\n\n==Convergence acceleration==\nThe speed of convergence of the iteration sequence can be increased by using a [[convergence acceleration]] method such as [[Aitken's delta-squared process]]. The application of Aitken's method to fixed-point iteration is known as [[Steffensen's method]], and it can be shown that Steffensen's method yields a rate of convergence that is at least quadratic.\n\n==See also==\n{{Div col|colwidth=20em}}\n* [[Contraction mapping]]\n* [[Root-finding algorithm]]\n* [[Fixed-point theorem]]\n* [[Fixed-point combinator]]\n* [[Banach fixed-point theorem]]\n* [[Cobweb plot]]\n* [[Markov chain]]\n* [[Infinite compositions of analytic functions]]\n* [[Iterated function]]\n* [[Limit (mathematics)#Convergence and fixed point|Convergence and fixed point]]\n{{Div col end}}\n\n==References==\n{{reflist|group=lower-alpha}}\n{{reflist}}\n\n==Further reading==\n* {{cite book| last1=Burden | first1=Richard L. | last2=Faires | first2=J. Douglas |title=Numerical Analysis |publisher=PWS Publishers |edition=Third |isbn=0-87150-857-5 \n| year=1985 | chapter=Fixed-Point Iteration }}\n* {{cite book |first=Joe D. |last=Hoffman |first2=Steven |last2=Frankel |chapter=Fixed-Point Iteration |title=Numerical Methods for Engineers and Scientists |edition=Second |location=New York |publisher=CRC Press |year=2001 |pages=141–145 |isbn=0-8247-0443-6 |chapterurl=https://books.google.com/books?id=VKs7Afjkng4C&pg=PA141 }}\n* {{cite book |first=Kenneth L. |last=Judd |authorlink=Kenneth Judd |chapter=Fixed-Point Iteration |title=Numerical Methods in Economics |location=Cambridge |publisher=MIT Press |year=1998 |isbn=0-262-10071-1 |pages=165–167 |chapterurl=https://books.google.com/books?id=9Wxk_z9HskAC&pg=PA165 }}\n\n==External links==\n*[https://algonum.appspot.com/#fixpoint.picard Fixed-point algorithms online]\n*[http://user.mendelu.cz/marik/maw/index.php?lang=en&form=banach Fixed-point iteration online calculator (Mathematical Assistant on Web)]\n\n{{DEFAULTSORT:Fixed-Point Iteration}}\n[[Category:Root-finding algorithms]]\n[[Category:Iterative methods]]\n[[Category:Fixed-point theorems| ]]"
    },
    {
      "title": "Fixed-point theorem",
      "url": "https://en.wikipedia.org/wiki/Fixed-point_theorem",
      "text": "In [[mathematics]], a '''fixed-point theorem''' is a result saying that a [[function (mathematics)|function]] ''F'' will have at least one [[fixed point (mathematics)|fixed point]] (a point ''x'' for which ''F''(''x'') = ''x''), under some conditions on ''F'' that can be stated in general terms.<ref>{{cite book\n | editor     = Brown, R. F.\n | title      = Fixed Point Theory and Its Applications\n | year       = 1988\n | publisher  = American Mathematical Society\n | isbn         = 0-8218-5080-6\n}}\n</ref> Results of this kind are amongst the most generally useful in mathematics.<ref>{{cite book\n |author1=Dugundji, James |author2=Granas, Andrzej | title = Fixed Point Theory\n | year = 2003\n | publisher = Springer-Verlag\n | isbn         = 0-387-00173-5\n}}</ref>\n\n== In mathematical analysis ==\nThe [[Banach fixed-point theorem]] gives a general criterion guaranteeing that, if it is satisfied, the procedure of [[iteration|iterating]] a function yields a fixed point.<ref>{{cite book\n | author = Giles, John R.\n | title = Introduction to the Analysis of Metric Spaces\n | year = 1987\n | publisher = Cambridge University Press\n | isbn         = 978-0-521-35928-3\n}}</ref>\n\nBy contrast, the [[Brouwer fixed-point theorem]] is a non-[[Constructivism (mathematics)|constructive result]]: it says that any [[continuous function|continuous]] function from the closed [[unit ball]] in ''n''-dimensional [[Euclidean space]] to itself must have a fixed point,<ref>Eberhard Zeidler, ''Applied Functional Analysis: main principles and their applications'', Springer, 1995.</ref> but it doesn't describe how to find the fixed point (See also [[Sperner's lemma]]).\n\nFor example, the [[cosine]] function is continuous in [&minus;1,1] and maps it into [&minus;1, 1], and thus must have a fixed point.  This is clear when examining a sketched graph of the cosine function; the fixed point occurs where the cosine curve ''y''=cos(''x'') intersects the line ''y''=''x''.  Numerically, the fixed point is approximately ''x''=0.73908513321516 (thus ''x''=cos(''x'') for this value of ''x'').\n\nThe [[Lefschetz fixed-point theorem]]<ref>{{cite journal |author=Solomon Lefschetz |title=On the fixed point formula |journal=[[Annals of Mathematics|Ann. of Math.]] |year=1937 |volume=38 |pages=819–822 |doi=10.2307/1968838 |issue=4}}</ref> (and the [[Nielsen theory|Nielsen fixed-point theorem]])<ref>{{cite book\n| last1=Fenchel\n| first=Werner\n| author1link=Werner Fenchel\n| last2=Nielsen\n| first2=Jakob\n| author2link=Jakob Nielsen (mathematician)\n| editor-last=Schmidt\n| editor-first=Asmus L.\n| title=Discontinuous groups of isometries in the hyperbolic plane\n| series=De Gruyter Studies in mathematics\n| volume=29\n| publisher=Walter de Gruyter & Co.\n| location=Berlin\n| year=2003\n}}</ref> from [[algebraic topology]] is notable because it gives, in some sense, a way to count fixed points.\n\nThere are a number of generalisations to [[Banach fixed-point theorem]] and further; these are applied in [[Partial differential equation|PDE]] theory. See [[fixed-point theorems in infinite-dimensional spaces]].\n\nThe [[collage theorem]] in [[fractal compression]] proves that, for many images, there exists a relatively small description of a function that, when iteratively applied to any starting image, rapidly converges on the desired image.<ref>{{cite book\n | author     = Barnsley, Michael.\n | title      = Fractals Everywhere\n | year       = 1988\n | publisher  = Academic Press, Inc.\n | isbn         = 0-12-079062-9\n}}</ref>\n\n== In algebra and discrete mathematics ==\n\nThe [[Knaster&ndash;Tarski theorem]] states that any [[monotonic|order-preserving function]] on a [[complete lattice]] has a fixed point, and indeed a ''smallest'' fixed point.<ref>{{cite journal | author=Alfred Tarski | url=http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.pjm/1103044538 | title=A lattice-theoretical fixpoint theorem and its applications | journal = Pacific Journal of Mathematics | volume=5:2 | year=1955 | pages=285&ndash;309}}</ref> See also [[Bourbaki&ndash;Witt theorem]].\n\nThe theorem has applications in [[abstract interpretation]], a form of [[static program analysis]].\n\nA common theme in [[lambda calculus]] is to find fixed points of given lambda expressions. Every lambda expression has a fixed point, and a [[fixed-point combinator]] is a \"function\" which takes as input a lambda expression and produces as output a fixed point of that expression.<ref>{{cite book|last=Peyton Jones|first=Simon L.|title=The Implementation of Functional Programming|year=1987|publisher=Prentice Hall International|url=http://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/}}</ref> An important fixed-point combinator is the [[Fixed-point combinator#Y combinator|Y combinator]] used to give [[Recursion (computer science)|recursive]] definitions.\n\nIn [[denotational semantics]] of programming languages, a special case of the Knaster&ndash;Tarski theorem is used to establish the semantics of recursive definitions. While the fixed-point theorem is applied to the \"same\" function (from a logical point of view), the development of the theory is quite different.\n\nThe same definition of recursive function can be given, in [[computability theory]], by applying [[Kleene's recursion theorem]].<ref>Cutland, N.J., ''Computability: An introduction to recursive function theory'', Cambridge University Press, 1980. {{isbn|0-521-29465-7}}</ref> These results are not equivalent theorems; the Knaster&ndash;Tarski theorem is a much stronger result than what is used in denotational semantics.<ref>''The foundations of program verification'', 2nd edition, Jacques Loeckx and Kurt Sieber, John Wiley & Sons, {{isbn|0-471-91282-4}}, Chapter 4; theorem 4.24, page 83, is what is used in denotational semantics, while Knaster&ndash;Tarski theorem is given to prove as exercise 4.3&ndash;5 on page 90.</ref> However, in light of the [[Church&ndash;Turing thesis]] their intuitive meaning is the same: a recursive function can be described as the least fixed point of a certain functional, mapping functions to functions.\n\nThe above technique of iterating a function to find a fixed point can also be used in [[set theory]]; the [[fixed-point lemma for normal functions]] states that any continuous strictly increasing function from [[ordinal number|ordinals]] to ordinals has one (and indeed many) fixed points.\n\nEvery [[closure operator]] on a [[poset]] has many fixed points; these are the \"closed elements\" with respect to the closure operator, and they are the main reason the closure operator was defined in the first place.\n\nEvery [[involution (mathematics)|involution]] on a [[finite set]] with an odd number of elements has a fixed point; more generally, for every involution on a finite set of elements, the number of elements and the number of fixed points have the same [[parity (mathematics)|parity]]. [[Don Zagier]] used these observations to give a one-sentence proof of [[Fermat's theorem on sums of two squares]], by describing two involutions on the same set of triples of integers, one of which can easily be shown to have only one fixed point and the other of which has a fixed point for each representation of a given prime (congruent to 1 mod 4) as a sum of two squares. Since the first involution has an odd number of fixed points, so does the second, and therefore there always exists a representation of the desired form.<ref>{{citation\n | last = Zagier | first = D. | authorlink = Don Zagier\n | doi = 10.2307/2323918\n | issue = 2\n | journal = [[American Mathematical Monthly]]\n | mr = 1041893\n | page = 144\n | title = A one-sentence proof that every prime ''p''&nbsp;≡&nbsp;1&nbsp;(mod&nbsp;4) is a sum of two squares\n | volume = 97\n | year = 1990}}.</ref>\n\n== List of fixed-point theorems ==\n{{Div col|colwidth=25em}}\n*[[Atiyah–Bott fixed-point theorem]]\n*[[Banach fixed-point theorem]]\n*[[Borel fixed-point theorem]]\n*[[Browder fixed-point theorem]]\n*[[Brouwer fixed-point theorem]]\n*[[Caristi fixed-point theorem]]\n*[[Diagonal lemma]], also known as the fixed-point lemma, for producing self-referential sentences of [[first-order logic]]\n*[[Fixed-point combinator]], which shows that every term in untyped [[lambda calculus]] has a fixed point\n*[[Fixed-point lemma for normal functions]]\n*[[Fixed-point property]]\n*[[Injective metric space]]\n*[[Kakutani fixed-point theorem]]\n*[[Kleene fixed-point theorem]]\n*[[Knaster–Tarski theorem]]\n*[[Lefschetz fixed-point theorem]]\n*[[Nielsen fixed-point theorem]]\n*[[Poincaré–Birkhoff theorem]] proves the existence of two fixed points\n*[[Ryll-Nardzewski fixed-point theorem]]\n*[[Schauder fixed-point theorem]]\n*[[Topological degree theory]]\n*[[Tychonoff fixed-point theorem]]\n{{div col end}}\n\n== Footnotes ==\n<references/>\n\n== References ==\n*{{cite book\n |author1=Agarwal, Ravi P. |author2=Meehan, Maria |author3=O'Regan, Donal | title      = Fixed Point Theory and Applications\n | year       = 2001\n | publisher  = Cambridge University Press\n | isbn         = 0-521-80250-4\n}}\n*{{cite book\n |author1=Aksoy, Asuman |author2=Khamsi, Mohamed A. | title      = Nonstandard Methods in fixed point theory\n | year       = 1990\n | publisher  = Springer Verlag\n | isbn         = 0-387-97364-8 \n}}\n*{{cite book\n | author     = Berinde, Vasile\n | title      = Iterative Approximation of Fixed Point\n | year       = 2005\n | publisher  = Springer Verlag\n | isbn         = 978-3-540-72233-5\n}}\n*{{cite book\n | author     = Border, Kim C.\n | title      = Fixed Point Theorems with Applications to Economics and Game Theory\n | year       = 1989\n | publisher  = Cambridge University Press\n | isbn         = 0-521-38808-2\n}}\n*{{cite book\n |author1=Kirk, William A. |author2=Goebel, Kazimierz | title      = Topics in Metric Fixed Point Theory\n | year       = 1990\n | publisher  = Cambridge University Press\n | isbn         = 0-521-38289-0\n}}\n*{{cite book\n |author1=Kirk, William A. |author2=Khamsi, Mohamed A. | title      = An Introduction to Metric Spaces and Fixed Point Theory\n | year       = 2001\n | publisher  = John Wiley, New York.\n | isbn         = 978-0-471-41825-2\n}}\n*{{cite book\n |author1=Kirk, William A. |author2=Sims, Brailey | title      = Handbook of Metric Fixed Point Theory\n | year       = 2001\n | publisher  = Springer-Verlag\n | isbn         = 0-7923-7073-2\n}}\n*{{cite book\n |author1=Šaškin, Jurij A |author2=Minachin, Viktor |author3=Mackey, George W. | title      = Fixed Points\n | year       = 1991\n | publisher  = American Mathematical Society\n | isbn         = 0-8218-9000-X\n}}\n\n==External links==\n*[http://www.math-linux.com/spip.php?article60 Fixed Point Method]\n\n[[Category:Closure operators]]\n[[Category:Fixed-point theorems| ]]\n[[Category:Iterative methods]]"
    },
    {
      "title": "Relaxation (iterative method)",
      "url": "https://en.wikipedia.org/wiki/Relaxation_%28iterative_method%29",
      "text": "{{About|iterative methods for solving systems of equations|other uses|Relaxation (disambiguation)}}\nIn [[numerical mathematics]], '''relaxation methods''' are [[iterative method]]s for solving [[simultaneous equations|systems of equations]], including nonlinear systems.<ref name=\"OrtegaRheinboldt\">{{Cite book|last1=Ortega|first1=J. M.|last2=Rheinboldt|first2=W. C.|title=Iterative solution of nonlinear equations in several variables|edition=Reprint of the 1970 Academic Press|series=Classics in Applied Mathematics|volume=30|publisher=Society for Industrial and Applied Mathematics (SIAM)|location=Philadelphia, PA|year=2000|pages=xxvi+572|isbn=0-89871-461-3|mr=1744713|ref=harv}}</ref>\n\nRelaxation methods were developed for solving large [[sparse matrix|sparse]] [[linear system]]s, which arose as [[finite difference|finite-difference]] [[discretization]]s of [[differential equation]]s.<ref name=\"Varga \"/><ref name=\"Young \"/> They are also used for the solution of linear equations for [[linear least squares (mathematics)|linear least-squares]] problems<ref name=\"BP\"/> and also for systems of linear inequalities, such as those arising in [[linear programming]].<ref name=\"Murty\">{{Cite book|last=Murty|first=Katta&nbsp;G.|authorlink=Katta G. Murty|chapter=16 Iterative methods for linear inequalities and linear programs (especially 16.2 Relaxation methods, and 16.4 Sparsity-preserving iterative SOR algorithms for linear programming)| title=Linear programming|publisher=John Wiley & Sons Inc.|location=New York|year=1983|pages=453–464|isbn=0-471-09725-X|mr=720547|ref=harv}}</ref><ref>{{Cite journal|last=Goffin|first=J.-L.|title=The relaxation method for solving systems of linear inequalities|journal=Math. Oper. Res.|volume=5|year=1980|number=3|pages=388–414|jstor=3689446|doi=10.1287/moor.5.3.388|mr=594854|ref=harv}}</ref><ref name=\"Minoux\">{{Cite book|last=Minoux|first=M.|authorlink=Michel Minoux|title=Mathematical programming: Theory and algorithms|others=Egon Balas (foreword)|edition=Translated  by Steven Vajda from the (1983 Paris: Dunod) French|publisher=A Wiley-Interscience Publication. John Wiley & Sons, Ltd.|location=Chichester|year=1986|pages=xxviii+489|isbn=0-471-90170-9|mr=868279|ref=harv|id=(2008 Second ed., in French: ''Programmation mathématique: Théorie et algorithmes''. Editions Tec & Doc, Paris,  2008. xxx+711 pp. {{isbn|978-2-7430-1000-3}}. {{MR|2571910}})}}</ref> They have also been developed for solving nonlinear systems of equations.<ref name=\"OrtegaRheinboldt\"/>\n\nRelaxation methods are important especially in the solution of linear systems used to model [[elliptic partial differential equation]]s, such as [[Laplace's equation]] and its generalization, [[Poisson's equation]]. These equations describe [[boundary-value problem]]s, in which the solution-function's values are specified on boundary of a domain; the problem is to compute a solution also on its interior. Relaxation methods are used to solve the linear equations resulting from a discretization of the differential equation, for example by finite differences.<ref name=\"BP\">Abraham Berman, [[Robert J. Plemmons]], ''Nonnegative Matrices in the Mathematical Sciences'', 1994, SIAM. {{isbn|0-89871-321-8}}.</ref><ref name=\"Young\">[[David M. Young, Jr.]] ''Iterative Solution of Large Linear Systems'', Academic Press, 1971. (reprinted by Dover, 2003)</ref><ref name=\"Varga\">[[Richard S. Varga]] 2002 ''Matrix Iterative Analysis'', Second ed. (of 1962 Prentice Hall edition), Springer-Verlag.</ref>\n\nIterative relaxation of solutions is commonly dubbed [[smoothing]] because with certain equations, such as [[Laplace's equation]], it resembles repeated application of a local smoothing filter to the solution vector.  These are not to be confused with [[relaxation (approximation)|relaxation]] methods in [[mathematical optimization]], which [[approximation theory|approximate]] a difficult problem by a simpler problem whose \"relaxed\" solution provides information about the solution of the original problem.<ref name=\"Minoux\"/>\n\n==Model problem of potential theory==<!-- Chapter 1 of Varga -->\nWhen φ is a smooth real-valued function on the real numbers, its second derivative can be approximated by:\n:<math>\\frac{d^2\\varphi(x)}{{dx}^2} = \\frac{\\varphi(x{-}h)-2\\varphi(x)+\\varphi(x{+}h)}{h^2}\\,+\\,\\mathcal{O}(h^2)\\,.</math>\nUsing this in both dimensions for a function φ of two arguments at the point (''x'', ''y''), and solving for φ(''x'', ''y''), results in:\n:<math>\\varphi(x, y) = \\tfrac{1}{4}\\left(\\varphi(x{+}h,y)+\\varphi(x,y{+}h)+\\varphi(x{-}h,y)+\\varphi(x,y{-}h)\n\\,-\\,h^2{\\nabla}^2\\varphi(x,y)\\right)\\,+\\,\\mathcal{O}(h^4)\\,.</math>\nTo approximate the solution of the Poisson equation:\n:<math>{\\nabla}^2 \\varphi = f\\,</math>\nnumerically on a two-dimensional grid with grid spacing ''h'', the relaxation method assigns the given values of function φ to the grid points near the boundary and arbitrary values to the interior grid points, and then repeatedly performs the assignment\nφ := φ* on the interior points, where φ* is defined by:\n:<math>\\varphi^*(x, y) = \\tfrac{1}{4}\\left(\\varphi(x{+}h,y)+\\varphi(x,y{+}h)+\\varphi(x{-}h,y)+\\varphi(x,y{-}h)\n\\,-\\,h^2f(x,y)\\right)\\,,</math>\nuntil convergence.<ref name=\"Young\"/><ref name=\" Varga\"/>\n\nThe method, sketched here for two dimensions,<ref name=\"Young\"/><ref name=\" Varga\"/> is readily generalized to other numbers of dimensions.\n\n==Convergence and acceleration==\n\nWhile the method converges under general conditions, it typically makes slower progress than competing methods. Nonetheless, the study of relaxation methods remains a core part of linear algebra, because the transformations of relaxation theory provide excellent [[preconditioner]]s for new methods. Indeed, the choice of preconditioner is often more important than the choice of iterative method.<ref name=\"Saad\">[[Yousef Saad]], ''[http://www-users.cs.umn.edu/%7Esaad/books.html Iterative Methods for Sparse Linear Systems]'', 1st edition, PWS, 1996.</ref>\n\n[[Multigrid methods]] may be used to accelerate the methods. One can first compute an approximation on a coarser grid – usually the double spacing 2''h'' – and use that solution with [[interpolation|interpolated]] values for the other grid points as the initial assignment. This can then also be done recursively for the coarser computation.<ref name=\"Saad\"/><ref>William L. Briggs, Van Emden Henson, and Steve F. McCormick (2000), ''[http://www.llnl.gov/casc/people/henson/mgtut/welcome.html A Multigrid Tutorial] {{webarchive|url=https://web.archive.org/web/20061006153457/http://www.llnl.gov/casc/people/henson/mgtut/welcome.html |date=2006-10-06 }}'' (2nd ed.), Philadelphia: [[Society for Industrial and Applied Mathematics]], {{isbn|0-89871-462-1}}.</ref>\n\n==See also==\n* In linear systems, the two main classes of relaxation methods are [[Iterative method#Stationary iterative methods|stationary iterative methods]], and the more general [[Iterative method#Krylov subspace methods|Krylov subspace methods]].\n* The [[Jacobi method]] is a simple relaxation method.\n* The [[Gauss–Seidel method]] is an improvement upon the Jacobi method.\n* [[Successive over-relaxation]] can be applied to either of the Jacobi and Gauss–Seidel methods to speed convergence.\n* [[Multigrid methods]]\n\n==Notes==\n<references/>\n\n==References==\n* Abraham Berman, Robert J. Plemmons, ''Nonnegative Matrices in the Mathematical Sciences'', 1994, SIAM. {{isbn|0-89871-321-8}}.\n* {{Cite book|last1=Ortega|first1=J. M.|last2=Rheinboldt|first2=W. C.|title=Iterative solution of nonlinear equations in several variables|edition=Reprint of the 1970 Academic Press|series=Classics in Applied Mathematics|volume=30|publisher=Society for Industrial and Applied Mathematics (SIAM)|location=Philadelphia, PA|year=2000|pages=xxvi+572|isbn=0-89871-461-3|mr=1744713|ref=harv}}\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 18.3. Relaxation Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=964}}\n* [[Yousef Saad]], ''[http://www-users.cs.umn.edu/%7Esaad/books.html Iterative Methods for Sparse Linear Systems]'', 1st edition, PWS, 1996.\n* [[Richard S. Varga]] 2002 ''Matrix Iterative Analysis'', Second ed. (of 1962 Prentice Hall edition), Springer-Verlag.\n* [[David M. Young, Jr.]] ''Iterative Solution of Large Linear Systems'', Academic Press, 1971. (reprinted by Dover, 2003)\n\n==Further reading==\n* Southwell, R.V. (1940) ''Relaxation Methods in Engineering Science''. Oxford University Press, Oxford.\n* Southwell, R.V. (1946) ''Relaxation Methods in Theoretical Physics''. Oxford University Press, Oxford.\n* {{cite book | author= John. D. Jackson | title=Classical Electrodynamics | location=New Jersey | publisher=Wiley | year=1999| isbn=0-471-30932-X}}\n* {{cite book | author= M.N.O. Sadiku | title=Numerical Techniques in Electromagnetics | location=Boca Raton | publisher=CRC Pres | year=1992}}\n* {{cite book | author= P.-B. Zhou | title=Numerical Analysis of Electromagnetic Fields | location=New York | publisher=Springer | year=1993}}\n\n[[Category:Iterative methods]]\n[[Category:Numerical linear algebra]]\n[[Category:Relaxation (iterative methods)| ]]"
    },
    {
      "title": "Superiorization",
      "url": "https://en.wikipedia.org/wiki/Superiorization",
      "text": "{{COI|date=December 2018}}\n{{technical|date=December 2018}}\n'''Superiorization''' is an [[iterative method]] for [[constrained optimization]]. It is used for improving the efficacy of an iterative method whose convergence is resilient to certain kinds of perturbations. Such perturbations are designed to \"force\" the perturbed [[algorithm]] to produce more useful results for the intended application than the ones that are produced by the original iterative algorithm. The perturbed algorithm is called the '''superiorized version''' of the original unperturbed algorithm. If the original algorithm is computationally efficient and useful in terms of the target application and if the perturbations are inexpensive to calculate, the method may be used to steer iterates without additional computation cost.\n\n== Areas of application ==\nThe superiorization methodology is very general and has been used successfully in many important practical applications, such as [[iterative reconstruction]] of images from their projections,<ref name=\"ref1\">G.T. Herman, Fundamentals of Computerized Tomography: Image Reconstruction from Projections, Springer-Verlag, London, UK, 2nd Edition, 2009. {{doi|10.1007/978-1-84628-723-7}}</ref><ref name=\"ref2\">E.S. Helou, M.V.W. Zibetti and E.X. Miqueles, Superiorization of incremental optimization algorithms for statistical tomographic image reconstruction, Inverse Problems, Vol. 33 (2017), 044010. {{doi|10.1088/1361-6420/33/4/044010}}</ref><ref name=\"ref3\">Q. Yang, W. Cong and G. Wang, Superiorization-based multi-energy CT image reconstruction, Inverse Problems, Vol. 33 (2017), 044014. {{doi|10.1088/1361-6420/aa5e0a}}</ref> [[single-photon emission computed tomography]],<ref name=\"ref4\">S. Luo and T. Zhou, Superiorization of EM algorithm and its application in single-photon emission computed tomography (SPECT), Inverse Problems and Imaging, Vol. 8, pp. 223–246, (2014). {{doi|10.3934/ipi.2014.8.223}}</ref> [[radiation therapy]]<ref name=\"ref5\">R. Davidi, Y. Censor, R.W. Schulte, S. Geneser and L. Xing, Feasibility-seeking and superiorization algorithms applied to inverse treatment planning in radiation therapy, Contemporary Mathematics, Vol. 636, pp. 83–92, (2015). {{doi|10.1090/conm/636/12729}}</ref><ref name=\"ref6\">E. Bonacker, A. Gibali, K-H. Küfer and P. Süss, Speedup of lexicographic optimization by superiorization and its applications to cancer radiotherapy treatment, Inverse Problems, Vol. 33 (2017), 044012. {{doi|10.1088/1361-6420/33/4/044012}}</ref><ref name=\"ref7\">J. Zhu and S. Penfold, Total variation superiorization in dual-energy CT reconstruction for proton therapy treatment planning, Inverse Problems, Vol. 33 (2017), 044013. {{doi|10.1088/1361-6420/33/4/04401}}</ref> and [[nondestructive testing]],<ref name=\"ref8\">M.J. Schrapp and G.T. Herman, Data fusion in X-ray computed tomography using a superiorization approach, Review of Scientific Instruments, Vol. 85, 053701 (9pp), (2014). {{doi|10.1063/1.4872378}}</ref> just to name a few. A special issue of the journal ''[[Inverse Problems]]''<ref name=\"ref9\">Superiorization: Theory and Applications, Special Issue of the journal Inverse Problems, Volume 33, Number 4, April 2017</ref> is devoted to superiorization, both theory<ref name=\"ref10\">H. He and H-K. Xu, Perturbation resilience and superiorization methodology of averaged mappings, Inverse Problems, Vol. 33 (2017), 044007. {{doi|10.1088/1361-6420/33/4/044007}}</ref><ref name=\"ref11\">H-K. Xu, Bounded perturbation resilience and superiorization techniques for the projected scaled gradient method, Inverse Problems, Vol. 33 (2017), 044008. {{doi|10.1088/1361-6420/33/4/044008}}</ref><ref name=\"ref12\">A. Cegielski and F. Al-Musallam, Superiorization with level control, Inverse Problems, Vol. 33 (2017), 044009. {{doi|10.1088/1361-6420/aa5d79}}</ref> and applications.<ref name=\"ref3\" /><ref name=\"ref6\" /><ref name=\"ref7\" />\n\n== Objective function reduction and relation with constrained optimization ==\nAn important case of superiorization is when the original algorithm is \"feasibility-seeking\" (in the sense that it strives to find some point in  a [[feasible region]] that is compatible with a family of constraints) and the perturbations that are introduced into the original iterative algorithm aim at reducing (not necessarily minimizing) a given merit function. In this case, superiorization has a unique place in [[optimization]] theory and practice. \n\nMany [[constrained optimization]] methods are based on methods for unconstrained optimization that are adapted to deal with constraints. Such is, for example, the class of projected gradient methods wherein the unconstrained minimization inner step \"leads\" the process and a projection onto the whole constraints set (the feasible region) is performed after each minimization step in order to regain feasibility. This projection onto the constraints set is in itself a non-trivial optimization problem and the need to solve it in every iteration hinders projected gradient methods and limits their efficacy to only feasible sets that are \"simple to project onto\". Barrier methods or [[penalty methods]] likewise are based on unconstrained optimization combined with various \"add-on\"s that guarantee that the constraints are preserved. Regularization methods embed the constraints into a \"regularized\" [[objective function]] and proceed with unconstrained solution methods for the new regularized objective function.\n\nIn contrast to these approaches, the superiorization methodology can be viewed as an antipodal way of thinking. Instead of adapting unconstrained minimization algorithms to handling constraints, it adapts feasibility-seeking algorithms to reduce merit function values. This is done while retaining the feasibility-seeking nature of the algorithm and without paying a high computational price. Furthermore, general-purpose approaches have been developed for automatically superiorizing iterative algorithms for large classes of constraints sets and merit functions; these provide algorithms for many application tasks.\n\n== Further sources ==\nThe superiorization methodology and perturbation resilience of algorithms are reviewed in<ref name=\"ref13\">G.T. Herman, E. Garduño, R. Davidi and Y. Censor, Superiorization: An optimization heuristic for medical physics, Medical Physics, Vol. 39, pp. 5532–5546, (2012). {{doi|10.1118/1.4745566}}</ref><ref name=\"ref14\">G.T. Herman, Superiorization for image analysis, in: Combinatorial Image Analysis, Lecture Notes in Computer Science Vol. 8466, Springer, 2014, pp. 1–7. {{doi|10.1007/978-3-319-07148-0_1}}</ref><ref name=\"ref15\">Y. Censor, Weak and strong superiorization: Between feasibility-seeking and minimization, Analele Stiintifice ale Universitatii Ovidius Constanta-Seria Matematica, Vol. 23, pp. 41–54, (2015). {{doi|10.1515/auom-2015-0046}}</ref>, see also<ref name=\"ref16\">Y. Censor, R. Davidi, G.T. Herman, R.W. Schulte and L. Tetruashvili, Projected subgradient minimization versus superiorization, Journal of Optimization Theory and Applications, Vol. 160, pp. 730–747, (2014). {{doi|10.1007/s10957-013-0408-3}}</ref>. Current work on superiorization can be appreciated from a continuously updated Internet page.<ref name=\"ref17\">{{cite web|url=http://math.haifa.ac.il/YAIR/bib-superiorization-censor.html|title=Superiorization|website=math.haifa.ac.il}}</ref> SNARK14<ref name=\"ref18\">{{cite web|url=http://turing.iimas.unam.mx/SNARK14M/|title=Snark14 – Home|website=turing.iimas.unam.mx}}</ref> is a software package for the reconstruction if 2D images from 1D projections that has a built-in capability of superiorizing any iterative algorithm for any merit function. \n\n== References ==\n{{Reflist}}\n\n\n\n\n[[Category:Iterative methods]]\n[[Category:Mathematical optimization]]"
    },
    {
      "title": "List of algorithms",
      "url": "https://en.wikipedia.org/wiki/List_of_algorithms",
      "text": "{{More citations needed|date=July 2017}}\nThe following is a '''list of [[algorithm]]s''' along with one-line descriptions for each.\n\n==Automated planning==\n{{further|List of algorithms for automated planning}}\n==Combinatorial algorithms==\n{{further| Combinatorics}}\n\n===General combinatorial algorithms===\n* [[Cycle detection#Brent's algorithm|Brent's algorithm]]: finds a cycle in function value iterations using only two iterators\n* [[Floyd's cycle-finding algorithm]]: finds a cycle in function value iterations\n* [[stable marriage problem|Gale–Shapley algorithm]]: solves the stable marriage problem\n* [[Pseudorandom number generator]]s (uniformly distributed - see also [[List_of_random_number_generators#Pseudorandom_number_generators_(PRNGs)|List of pseudorandom number generators]] for other PRNGs with varying degrees of convergence and varying statistical quality):\n** [[ACORN_(PRNG)|ACORN generator]]\n** [[Blum Blum Shub]]\n** [[Lagged Fibonacci generator]]\n** [[Linear congruential generator]]\n** [[Mersenne Twister]]\n\n===Graph algorithms===\n{{further|Graph theory|:Category:Graph algorithms}}\n* [[Coloring algorithm]]: Graph coloring algorithm.\n* [[Hopcroft–Karp algorithm]]: convert a bipartite graph to a [[maximum cardinality matching]]\n* [[Hungarian algorithm]]: algorithm for finding a [[perfect matching]]\n* [[Prüfer sequence|Prüfer coding]]: conversion between a labeled tree and its [[Prüfer sequence]]\n* [[Tarjan's off-line lowest common ancestors algorithm]]: compute [[lowest common ancestor]]s for pairs of nodes in a tree\n* [[Topological sorting|Topological sort]]: finds linear order of nodes (e.g. jobs) based on their dependencies.\n\n====Graph drawing====\n{{further|Graph drawing}}\n* [[Force-based algorithms (graph drawing)|Force-based algorithms]] (also known as force-directed algorithms or spring-based algorithm)\n* [[Spectral layout]]\n\n====Network theory====\n{{further|Network theory}}\n* Network analysis\n** Link analysis\n*** [[Girvan–Newman algorithm]]: detect communities in complex systems\n*** Web link analysis\n**** [[Hyperlink-Induced Topic Search]] (HITS) (also known as [[Hubs and authorities]])\n**** [[PageRank]]\n**** [[TrustRank]]\n* [[Flow network]]s\n** [[Dinic's algorithm]]: is a [[strongly polynomial]] algorithm for computing the [[maximum flow]] in a [[flow network]].\n** [[Edmonds–Karp algorithm]]: implementation of Ford–Fulkerson\n** [[Ford–Fulkerson algorithm]]: computes the [[maximum flow problem|maximum flow]] in a graph\n** [[Karger's algorithm]]:  a Monte Carlo method to compute the [[minimum cut]] of a connected graph\n** [[Push–relabel algorithm]]: computes a [[maximum flow problem|maximum flow]] in a graph\n\n====Routing for graphs====\n* [[Edmonds' algorithm]] (also known as Chu–Liu/Edmonds' algorithm): find maximum or minimum branchings\n* [[Euclidean minimum spanning tree]]: algorithms for computing the minimum spanning tree of a set of points in the plane\n* [[Euclidean shortest path problem]]: find the shortest path between two points that does not intersect any obstacle\n* [[Longest path problem]]: find a simple path of maximum length in a given graph\n* [[Minimum spanning tree]]\n** [[Borůvka's algorithm]]\n** [[Kruskal's algorithm]]\n** [[Prim's algorithm]]\n** [[Reverse-delete algorithm]]\n* [[Nonblocking minimal spanning switch]] say, for a [[telephone exchange]]\n* [[Shortest path problem]]\n** [[Bellman–Ford algorithm]]: computes [[shortest path problem|shortest paths]] in a weighted graph (where some of the edge weights may be negative)\n** [[Dijkstra's algorithm]]: computes [[shortest path problem|shortest paths]] in a graph with non-negative edge weights\n** [[Floyd–Warshall algorithm]]: solves the [[all pairs shortest path]] problem in a weighted, directed graph\n** [[Johnson's algorithm]]: All pairs shortest path algorithm in sparse weighted directed graph\n<!-- ** [[Perturbation methods]]: an algorithm that computes a locally [[shortest path problem|shortest paths]] in a graph -->\n* [[Transitive closure]] problem: find the [[transitive closure]] of a given binary relation\n* [[Traveling salesman problem]]\n** [[Christofides algorithm]]\n** [[Nearest neighbour algorithm]]\n* [[Warnsdorff's rule]]: A heuristic method for solving the [[Knight's tour]] problem.\n\n====Graph search====\n{{further|State space search|Graph search algorithm}}\n* [[A* search algorithm|A*]]: special case of best-first search that uses heuristics to improve speed\n* [[B*]]: a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals)\n* [[Backtracking]]: abandons partial solutions when they are found not to satisfy a complete solution\n* [[Beam search]]: is a heuristic search algorithm that is an optimization of [[best-first search]] that reduces its memory requirement\n* [[Beam stack search]]: integrates backtracking with [[beam search]]\n* [[Best-first search]]: traverses a graph in the order of likely importance using a [[priority queue]]\n* [[Bidirectional search]]: find the shortest path from an initial vertex to a goal vertex in a directed graph\n* [[Breadth-first search]]: traverses a graph level by level\n* [[Brute-force search]]: An exhaustive and reliable search method, but computationally inefficient in many applications.\n* [[D*]]: an [[incremental heuristic search]] algorithm\n* [[Depth-first search]]: traverses a graph branch by branch\n* [[Dijkstra's algorithm]]: A special case of A* for which no heuristic function is used\n* [[General Problem Solver]]: a seminal theorem-proving algorithm intended to work as a universal problem solver machine.\n* [[Iterative deepening depth-first search]] (IDDFS): a state space search strategy\n* [[Jump point search]]: An optimization to A* which may reduce computation time by an order of magnitude using further heuristics.\n* [[Lexicographic breadth-first search]] (also known as Lex-BFS): a linear time algorithm for ordering the vertices of a graph\n* [[Uniform-cost search]]: a [[Tree traversal|tree search]] that finds the lowest-cost route where costs vary\n* [[SSS*]]: state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm\n* [[F* Merge Algorithm|F*]]: Special algorithm to merge the two arrays\n\n====Subgraphs====\n* [[Clique (graph theory)|Cliques]]\n** [[Bron–Kerbosch algorithm]]: a technique for finding [[maximal clique]]s in an undirected graph\n** [[MaxCliqueDyn maximum clique algorithm]]: find a [[maximum clique]] in an undirected graph\n* [[Strongly connected components]]\n** [[Path-based strong component algorithm]]\n** [[Kosaraju's algorithm]]\n** [[Tarjan's strongly connected components algorithm]]\n* [[Subgraph isomorphism problem]]\n\n===Sequence algorithms===\n{{further|Sequences}}\n\n====Approximate sequence matching====\n* [[Bitap algorithm]]: fuzzy algorithm that determines if strings are approximately equal.\n* [[Phonetic algorithm]]s\n** [[Daitch–Mokotoff Soundex]]: a [[Soundex]] refinement which allows matching of Slavic and Germanic surnames\n** [[Double Metaphone]]: an improvement on Metaphone\n** [[Match rating approach]]: a phonetic algorithm developed by Western Airlines\n** [[Metaphone]]: an algorithm for indexing words by their sound, when pronounced in English\n** [[New York State Identification and Intelligence System|NYSIIS]]: [[phonetic algorithm]], improves on [[Soundex]]\n** [[Soundex]]: a phonetic algorithm for indexing names by sound, as pronounced in English\n* [[String metric]]s: compute a similarity or dissimilarity (distance) score between two pairs of text strings\n** [[Damerau–Levenshtein distance]] compute a distance measure between two strings, improves on [[Levenshtein distance]]\n** [[Dice's coefficient]] (also known as the Dice coefficient): a similarity measure related to the [[Jaccard index]]\n** [[Hamming distance]]: sum number of positions which are different\n** [[Jaro–Winkler distance]]:  is a measure of similarity between two strings\n** [[Levenshtein distance|Levenshtein edit distance]]: compute a metric for the amount of difference between two sequences\n* [[Trigram search]]: search for text when the exact syntax or spelling of the target object is not precisely known\n\n====Selection algorithms====\n{{main|Selection algorithm}}\n\n* [[Quickselect]]\n* [[Introselect]]\n\n====Sequence search====\n* [[Linear search]]: finds an item in an unsorted sequence\n* [[Selection algorithm]]: finds the ''k''th largest item in a sequence\n* [[Ternary search]]: a technique for finding the minimum or maximum of a function that is either strictly increasing and then strictly decreasing or vice versa\n* [[Sorted list]]s\n** [[Binary search algorithm]]: locates an item in a sorted sequence\n** [[Fibonacci search technique]]: search a sorted sequence using a divide and conquer algorithm that narrows down possible locations with the aid of [[Fibonacci numbers]]\n** [[Jump search]] (or block search): linear search on a smaller subset of the sequence\n** [[Interpolation search|Predictive search]]: binary-like search which factors in [[magnitude (mathematics)|magnitude]] of search term versus the high and low values in the search.  Sometimes called dictionary search or interpolated search.\n** [[Uniform binary search]]: an optimization of the classic binary search algorithm\n\n====Sequence merging====\n{{main|Merge algorithm}}\n* Simple merge algorithm\n* k-way merge algorithm\n* Union (merge, with elements on the output not repeated)\n\n====Sequence permutations====\n{{further|Permutation}}\n* [[Fisher–Yates shuffle]] (also known as the Knuth shuffle): randomly shuffle a finite set\n* [[Schensted algorithm]]: constructs a pair of [[Young tableau]]x from a permutation\n* [[Steinhaus–Johnson–Trotter algorithm]] (also known as the Johnson–Trotter algorithm): generate permutations by transposing elements\n* [[Heap's algorithm|Heap's permutation generation algorithm]]: interchange elements to generate next permutation\n\n====Sequence alignment====\n* [[Dynamic time warping]]: measure similarity between two sequences which may vary in time or speed\n* [[Hirschberg's algorithm]]: finds the least cost [[sequence alignment]] between two sequences, as measured by their [[Levenshtein distance]]\n* [[Needleman–Wunsch algorithm]]: find global alignment between two sequences\n* [[Smith–Waterman algorithm]]: find local sequence alignment\n\n====Sequence sorting====\n{{main|Sorting algorithm}}\n{{contradict-other|Sorting_algorithm#Comparison_of_algorithms|date=March 2011}}\n\n* Exchange sorts\n** [[Bubble sort]]: for each pair of indices, swap the items if out of order\n** [[Cocktail shaker sort]] or bidirectional bubble sort, a bubble sort traversing the list alternately from front to back and back to front\n** [[Comb sort]]\n** [[Gnome sort]]\n** [[Odd–even sort]]\n** [[Quicksort]]: divide list into two, with all items on the first list coming before all items on the second list.; then sort the two lists. Often the method of choice\n* Humorous or ineffective\n** [[Bogosort]]\n** [[Stooge sort]]\n* Hybrid\n** [[Flashsort]]\n** [[Introsort]]: begin with quicksort and switch to heapsort when the recursion depth exceeds a certain level\n** [[Timsort]]: adaptative algorithm derived from merge sort and insertion sort. Used in Python 2.3 and up, and Java SE 7.\n* Insertion sorts\n** [[Insertion sort]]: determine where the current item belongs in the list of sorted ones, and insert it there\n** [[Library sort]]\n** [[Patience sorting]]\n** [[Shellsort|Shell sort]]: an attempt to improve insertion sort\n** [[Tree sort]] (binary tree sort): build binary tree, then traverse it to create sorted list\n** [[Cycle sort]]: in-place with theoretically optimal number of writes\n* Merge sorts\n** [[Merge sort]]: sort the first and second half of the list separately, then merge the sorted lists\n** [[Strand sort]]\n* Non-comparison sorts\n** [[Bead sort]]\n** [[Bucket sort]]\n** [[Burstsort]]: build a compact, cache efficient [[burst trie]] and then traverse it to create sorted output\n** [[Counting sort]]\n** [[Pigeonhole sort]]\n** [[Postman sort]]: variant of Bucket sort which takes advantage of hierarchical structure\n** [[Radix sort]]: sorts strings letter by letter\n* Selection sorts\n** [[Heapsort]]: convert the list into a heap, keep removing the largest element from the heap and adding it to the end of the list\n** [[Selection sort]]: pick the smallest of the remaining elements, add it to the end of the sorted list\n** [[Smoothsort]]\n* Other\n** [[Bitonic sorter]]\n** [[Pancake sorting]]\n** [[Spaghetti sort]]\n** [[Topological sorting|Topological sort]]\n* Unknown class\n** [[Samplesort]]\n\n====Subsequences====\n{{further|Subsequence}}\n*[[Kadane's algorithm]]: finds maximum sub-array of any size\n*[[Longest common subsequence problem]]: Find the longest subsequence common to all sequences in a set of sequences\n*[[Longest increasing subsequence problem]]: Find the longest increasing subsequence of a given sequence\n*[[Shortest common supersequence problem]]: Find the shortest supersequence that contains two or more sequences as subsequences\n\n====Substrings====\n{{further|Substring}}\n* [[Longest common substring problem]]:  find the longest string (or strings) that is a substring (or are substrings) of two or more strings\n* [[Substring search]]\n** [[Aho–Corasick string matching algorithm]]: [[trie]] based algorithm for finding all substring matches to any of a finite set of strings\n** [[Boyer–Moore string-search algorithm]]: amortized linear ([[sublinear]] in most times) algorithm for substring search\n** [[Boyer–Moore–Horspool algorithm]]: Simplification of Boyer–Moore\n** [[Knuth–Morris–Pratt algorithm]]: substring search which bypasses reexamination of matched characters\n** [[Rabin–Karp string search algorithm]]: searches multiple patterns efficiently\n** [[Zhu–Takaoka string matching algorithm]]: a variant of Boyer–Moore\n* [[Ukkonen's algorithm]]: a [[linear-time]], [[online algorithm]] for constructing [[suffix tree]]s\n* [[Matching wildcards]]\n** [[InterNetNews|Rich Salz]]' [[wildmat]]: a widely used [[Open-source software|open-source]] [[recursion|recursive]] algorithm\n** [[Krauss matching wildcards algorithm]]: an open-source non-recursive algorithm\n\n==Computational mathematics==\n{{further|Computational mathematics}}\n{{see also|List of algorithms#Combinatorial algorithms|l1=Combinatorial algorithms|List of algorithms#Computational science|l2=Computational science}}\n\n===Abstract algebra===\n{{further|Abstract algebra}}\n* [[Chien search]]: a recursive algorithm for determining roots of polynomials defined over a finite field\n* [[Schreier–Sims algorithm]]: computing a base and [[strong generating set]] (BSGS) of a [[permutation group]]\n* [[Todd–Coxeter algorithm]]: Procedure for generating [[coset]]s.\n\n===Computer algebra===\n{{further|Computer algebra}}\n* [[Buchberger's algorithm]]: finds a [[Gröbner basis]]\n* [[Cantor–Zassenhaus algorithm]]: factor polynomials over finite fields\n* [[Faugère F4 algorithm]]: finds a Gröbner basis (also mentions the F5 algorithm)\n* [[Gosper's algorithm]]: find sums of hypergeometric terms that are themselves hypergeometric terms\n* [[Knuth–Bendix completion algorithm]]: for [[rewriting]] rule systems\n* [[Multivariate division algorithm]]: for [[polynomial]]s in several indeterminates\n* [[Pollard's kangaroo algorithm]] (also known as Pollard's lambda algorithm ): an algorithm for solving the discrete logarithm problem\n* [[Polynomial long division]]: an algorithm for dividing a polynomial by another polynomial of the same or lower degree\n* [[Risch algorithm]]: an algorithm for the calculus operation of indefinite integration (i.e. finding [[antiderivatives]])\n\n===Geometry===\n{{main category|Geometric algorithms}}\n{{further|Computational geometry}}\n* [[Closest pair problem]]:  find the pair of points (from a set of points) with the smallest distance between them\n* [[Collision detection]] algorithms: check for the collision or intersection of two given solids\n* [[Cone algorithm]]: identify surface points\n* [[Convex hull algorithms]]: determining the [[convex hull]] of a [[Set (mathematics)|set]] of points\n** [[Graham scan]]\n** [[Quickhull]]\n** [[Gift wrapping algorithm]] or Jarvis march\n** [[Chan's algorithm]]\n** [[Kirkpatrick–Seidel algorithm]]\n* [[Euclidean distance map|Euclidean Distance Transform]] - Computes the distance between every point in a grid and a discrete collection of points.\n* [[Geometric hashing]]: a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an [[affine transformation]]\n* [[Gilbert–Johnson–Keerthi distance algorithm]]: determining the smallest distance between two [[convex set|convex]] shapes.\n* [[Jump-and-Walk algorithm]]: an algorithm for point location in triangulations\n* [[Laplacian smoothing]]: an algorithm to smooth a polygonal mesh\n* [[Line segment intersection]]: finding whether lines intersect, usually with a [[sweep line algorithm]]\n** [[Bentley–Ottmann algorithm]]\n** [[Shamos–Hoey algorithm]]\n* [[Minimum bounding box algorithms]]: find the [[Minimum bounding box#Arbitrarily oriented minimum bounding box|oriented minimum bounding box]] enclosing a set of points\n* [[Nearest neighbor search]]:  find the nearest point or points to a query point\n* [[Point in polygon]] algorithms: tests whether a given point lies within a given polygon\n* [[Point set registration]] algorithms: finds the transformation between two [[point cloud|point sets]] to optimally align them.\n* [[Rotating calipers]]: determine all [[antipodal point|antipodal]] pairs of points and vertices on a [[convex polygon]] or [[convex hull]].\n* [[Shoelace algorithm]]: determine the area of a polygon whose vertices are described by ordered pairs in the plane\n* [[Triangulation (geometry)|Triangulation]]\n** [[Delaunay triangulation]]\n*** [[Ruppert's algorithm]] (also known as Delaunay refinement): create quality Delaunay triangulations\n*** [[Chew's second algorithm]]: create quality [[constrained Delaunay triangulation]]s\n** [[Marching triangles]]: reconstruct two-dimensional surface geometry from an unstructured [[point cloud]]\n** [[Polygon triangulation]] algorithms: decompose a polygon into a set of triangles\n** [[Voronoi diagram]]s, geometric [[duality (mathematics)|dual]] of [[Delaunay triangulation]]\n*** [[Bowyer–Watson algorithm]]: create voronoi diagram in any number of dimensions\n*** [[Fortune's Algorithm]]: create voronoi diagram\n** [[Quasitriangulation]]\n\n===Number theoretic algorithms===\n{{further|Number theory}}\n* [[Binary GCD algorithm]]: Efficient way of calculating GCD.\n* [[Booth's multiplication algorithm]]\n* [[Chakravala method]]: a cyclic algorithm to solve indeterminate quadratic equations, including [[Pell's equation]]\n* [[Discrete logarithm]]:\n** [[Baby-step giant-step]]\n** [[Index calculus algorithm]]\n** [[Pollard's rho algorithm for logarithms]]\n** [[Pohlig&ndash;Hellman algorithm]]\n* [[Euclidean algorithm]]: computes the [[greatest common divisor]]\n* [[Extended Euclidean algorithm]]: Also solves the equation ''ax''&nbsp;+&nbsp;''by''&nbsp;=&nbsp;''c''.\n* [[Integer factorization]]: breaking an integer into its [[prime number|prime]] factors\n** [[Congruence of squares]]\n** [[Dixon's algorithm]]\n** [[Fermat's factorization method]]\n** [[General number field sieve]]\n** [[Lenstra elliptic curve factorization]]\n** [[Pollard's p &minus; 1 algorithm|Pollard's ''p''&nbsp;−&nbsp;1 algorithm]]\n** [[Pollard's rho algorithm]]\n** [[prime factorization algorithm]]\n** [[Quadratic sieve]]\n** [[Shor's algorithm]]\n** [[Special number field sieve]]\n** [[Trial division]]\n* [[Multiplication algorithm]]s: fast multiplication of two numbers\n** [[Karatsuba algorithm]]\n** [[Schönhage–Strassen algorithm]]\n** [[Toom–Cook multiplication]]\n* [[Modular square root]]: computing square roots modulo a prime number\n** [[Tonelli–Shanks algorithm]]\n** [[Cipolla's algorithm]]\n* [[Odlyzko&ndash;Schönhage algorithm]]: calculates nontrivial zeroes of the [[Riemann zeta function]]\n* [[Lenstra–Lenstra–Lovász lattice basis reduction algorithm|Lenstra–Lenstra–Lovász algorithm]] (also known as LLL algorithm): find a short, nearly orthogonal [[Lattice (group)|lattice]] [[Basis (linear algebra)|basis]] in polynomial time\n* [[Primality test]]s: determining whether a given number is [[prime number|prime]]\n** [[AKS primality test]]\n** [[Baillie-PSW primality test]]\n** [[Fermat primality test]]\n** [[Lucas primality test]]\n** [[Miller&ndash;Rabin primality test]]\n** [[Sieve of Atkin]]\n** [[Sieve of Eratosthenes]]\n** [[Sieve of Sundaram]]\n\n===Numerical algorithms===\n{{further|Numerical analysis|List of numerical analysis topics}}\n\n==== Differential equation solving ====\n{{further|Differential equation}}\n* [[Euler method]]\n* [[Backward Euler method]]\n* [[Trapezoidal rule (differential equations)]]\n* [[Linear multistep method]]s\n* [[Runge–Kutta methods]]\n** [[Euler integration]]\n* [[Multigrid method]]s (MG methods), a group of algorithms for solving differential equations using a hierarchy of discretizations\n* [[Partial differential equation]]:\n** [[Finite difference method]]\n** [[Crank–Nicolson method]] for diffusion equations\n** [[Lax–Wendroff method|Lax-Wendroff]] for wave equations\n* [[Verlet integration]] ({{IPA-fr|vɛʁˈlɛ}}): integrate Newton's equations of motion\n\n==== Elementary and special functions ====\n{{further|Special functions}}\n* [[Computing π|Computation of π]]:\n** [[Borwein's algorithm]]: an algorithm to calculate the value of 1/π\n** [[Gauss–Legendre algorithm]]: computes the digits of [[pi]]\n** [[Chudnovsky algorithm]]: A fast method for calculating the digits of π\n** [[Bailey–Borwein–Plouffe formula]]: (BBP formula) a spigot algorithm for the computation of the nth binary digit of π\n*[[Division algorithm]]s: for computing quotient and/or remainder of two numbers\n**[[Long division]]\n**[[Restoring division]]\n**[[Non-restoring division]]\n**[[SRT division]]\n**[[Newton–Raphson division]]: uses [[Newton's method]] to find the [[Multiplicative inverse|reciprocal]] of D, and multiply that reciprocal by N to find the final quotient Q.\n**[[Goldschmidt division]]\n* Hyperbolic and Trigonometric Functions:\n** [[BKM algorithm]]:  compute [[Elementary function (differential algebra)|elementary functions]] using a table of logarithms\n** [[CORDIC]]:  compute hyperbolic and trigonometric functions using a table of arctangents\n* Exponentiation:\n** [[Addition-chain exponentiation]]: exponentiation by positive integer powers that requires a minimal number of multiplications\n** [[Exponentiating by squaring]]: an algorithm used for the fast computation of [[Arbitrary-precision arithmetic|large integer]] powers of a number\n* [[Montgomery reduction]]: an algorithm that allows [[modular arithmetic]] to be performed efficiently when the modulus is large\n* [[Multiplication algorithm]]s: fast multiplication of two numbers\n** [[Booth's multiplication algorithm]]: a multiplication algorithm that multiplies two signed binary numbers in two's complement notation\n** [[Fürer's algorithm]]:  an integer multiplication algorithm for very large numbers possessing a very low [[Computational complexity theory|asymptotic complexity]]\n** [[Karatsuba algorithm]]:  an efficient procedure for multiplying large numbers\n** [[Schönhage–Strassen algorithm]]: an asymptotically fast multiplication algorithm for large integers\n** [[Toom–Cook multiplication]]: (Toom3) a multiplication algorithm for large integers\n* [[Multiplicative inverse#Algorithms|Multiplicative inverse Algorithms]]: for computing a number's multiplicative inverse (reciprocal).\n**[[Newton's method#Multiplicative inverses of numbers and power series|Newton's method]]\n* [[Rounding functions]]: the classic ways to round numbers\n* [[Spigot algorithm]]: A way to compute the value of a [[mathematical constant]] without knowing preceding digits\n* Square and Nth root of a number:\n** [[Alpha max plus beta min algorithm]]: an approximation of the square-root of the sum of two squares\n** [[Methods of computing square roots]]\n** [[Nth root algorithm|''n''th root algorithm]]\n** [[Shifting nth-root algorithm]]: digit by digit root extraction\n* Summation:\n** [[Binary splitting]]:  a [[Divide and conquer algorithm|divide and conquer]] technique which speeds up the numerical evaluation of many types of series with rational terms\n** [[Kahan summation algorithm]]: a more accurate method of summing floating-point numbers\n* [[Unrestricted algorithm]]\n\n==== Geometric ====\n* [[Radon transform#Filtered back-projection|Filtered back-projection]]: efficiently compute the inverse 2-dimensional [[Radon transform]].\n* [[Level set method]] (LSM):  a numerical technique for tracking interfaces and shapes\n\n==== Interpolation and extrapolation ====\n{{further|Interpolation|Extrapolation}}\n* [[Birkhoff interpolation]]: an extension of polynomial interpolation\n* [[Cubic interpolation]]\n* [[Hermite interpolation]]\n* [[Lagrange interpolation]]: interpolation using [[Lagrange polynomial]]s\n* [[Linear interpolation]]: a method of curve fitting using linear polynomials\n* [[Monotone cubic interpolation]]: a variant of cubic interpolation that preserves monotonicity of the data set being interpolated.\n* [[Multivariate interpolation]]\n** [[Bicubic interpolation]], a generalization of [[cubic interpolation]] to two dimensions\n** [[Bilinear interpolation]]: an extension of [[linear interpolation]] for interpolating functions of two variables on a regular grid\n** [[Lanczos resampling]] (\"Lanzosh\"): a multivariate interpolation method used to compute new values for any digitally sampled data\n** [[Nearest-neighbor interpolation]]\n** [[Tricubic interpolation]], a generalization of [[cubic interpolation]] to three dimensions\n* [[Pareto interpolation]]: a method of estimating the median and other properties of a population that follows a [[Pareto distribution]].\n* [[Polynomial interpolation]]\n** [[Neville's algorithm]]\n* [[Spline interpolation]]: Reduces error with [[Runge's phenomenon]].\n** [[De Boor algorithm]]: [[B-spline]]s\n** [[De Casteljau's algorithm]]: [[Bézier curve]]s\n* [[Trigonometric interpolation]]\n\n==== Linear algebra ====\n{{further|Numerical linear algebra}}\n* [[Eigenvalue algorithm]]s\n** [[Arnoldi iteration]]\n** [[Inverse iteration]]\n** [[Jacobi eigenvalue algorithm|Jacobi method]]\n** [[Lanczos iteration]]\n** [[Power iteration]]\n** [[QR algorithm]]\n** [[Rayleigh quotient iteration]]\n* [[Gram–Schmidt process]]: orthogonalizes a set of vectors\n* [[Matrix multiplication algorithm]]s\n** [[Cannon's algorithm]]: a [[distributed algorithm]] for [[matrix multiplication]] especially suitable for computers laid out in an N × N mesh\n** [[Coppersmith–Winograd algorithm]]: square [[matrix multiplication]]\n** [[Freivalds' algorithm]]: a randomized algorithm used to verify matrix multiplication\n** [[Strassen algorithm]]: faster [[matrix multiplication]]\n{{anchor|Solving systems of linear equations}}\n* Solving [[system of linear equations|systems of linear equations]]\n** [[Biconjugate gradient method]]: solves systems of linear equations\n** [[Conjugate gradient]]: an algorithm for the numerical solution of particular systems of linear equations\n** [[Gaussian elimination]]\n** [[Gauss–Jordan elimination]]: solves systems of linear equations\n** [[Gauss–Seidel method]]: solves systems of linear equations iteratively\n** [[Levinson recursion]]: solves equation involving a [[Toeplitz matrix]]\n** [[Stone's method]]: also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equations\n** [[Successive over-relaxation]] (SOR): method used to speed up convergence of the [[Gauss–Seidel method]]\n** [[Tridiagonal matrix algorithm]] (Thomas algorithm): solves systems of tridiagonal equations\n* [[Sparse matrix]] algorithms\n** [[Cuthill–McKee algorithm]]: reduce the [[bandwidth (matrix theory)|bandwidth]] of a [[symmetric sparse matrix]]\n** [[Minimum degree algorithm]]: permute the rows and columns of a [[symmetric sparse matrix]] before applying the [[Cholesky decomposition]]\n** [[Symbolic Cholesky decomposition]]: Efficient way of storing [[sparse matrix]]\n\n==== Monte Carlo ====\n{{further|Monte Carlo method}}\n* [[Gibbs sampling]]: generate a sequence of samples from the joint probability distribution of two or more random variables\n* [[Hybrid Monte Carlo]]: generate a sequence of samples using [[Hamiltonian dynamics|Hamiltonian]] weighted [[Markov chain Monte Carlo]], from a probability distribution which is difficult to sample directly.\n* [[Metropolis–Hastings algorithm]]: used to generate a sequence of samples from the [[probability distribution]] of one or more variables\n* [[Wang and Landau algorithm]]: an extension of [[Metropolis–Hastings algorithm]] sampling\n\n==== Numerical integration ====\n{{further|Numerical integration}}\n* [[MISER algorithm]]: Monte Carlo simulation, [[numerical integration]]\n\n==== Root finding ====\n{{main|Root-finding algorithm}}\n* [[Bisection method]]\n* [[False position method]]: approximates roots of a function\n* [[Newton's method]]: finds zeros of functions with [[calculus]]\n* [[Halley's method]]: uses first and second derivatives\n* [[Secant method]]: 2-point, 1-sided\n* [[False position method]] and Illinois method: 2-point, bracketing\n* [[Ridder's method]]: 3-point, exponential scaling\n* [[Muller's method]]: 3-point, quadratic interpolation\n\n===Optimization algorithms===\n{{main|Mathematical optimization}}\n* [[Alpha-beta pruning]]: search to reduce number of nodes in minimax algorithm\n* [[Branch and bound]]\n* [[Bruss algorithm]]: see [[odds algorithm]]\n* [[Chain matrix multiplication]]\n* [[Combinatorial optimization]]: optimization problems where the set of feasible solutions is discrete\n** [[Greedy randomized adaptive search procedure]] (GRASP): successive constructions of a greedy randomized solution and subsequent iterative improvements of it through a local search\n** [[Hungarian method]]: a combinatorial optimization algorithm which solves the [[assignment problem]] in polynomial time\n* [[Constraint satisfaction]]{{anchor|Constraint satisfaction}}\n** General algorithms for the constraint satisfaction\n*** [[AC-3 algorithm]]\n*** [[Difference map algorithm]]\n*** [[Min conflicts algorithm]]\n** [[Chaff algorithm]]: an algorithm for solving instances of the boolean satisfiability problem\n** [[Davis–Putnam algorithm]]: check the validity of a first-order logic formula\n** [[DPLL algorithm|Davis–Putnam–Logemann–Loveland algorithm]] (DPLL): an algorithm for deciding the satisfiability of propositional logic formula in [[conjunctive normal form]], i.e. for solving the [[CNF-SAT]] problem\n** [[Exact cover]] problem\n*** [[Algorithm X]]: a [[nondeterministic algorithm]]\n*** [[Dancing Links]]: an efficient implementation of Algorithm X\n* [[Cross-entropy method]]: a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and [[importance sampling]]\n* [[Differential evolution]]\n* [[Dynamic Programming]]: problems exhibiting the properties of [[overlapping subproblem]]s and [[optimal substructure]]\n* [[Ellipsoid method]]: is an algorithm for solving convex optimization problems\n* [[Evolutionary computation]]: optimization inspired by biological mechanisms of evolution\n** [[Evolution strategy]]\n** [[Gene expression programming]]\n** [[Genetic algorithms]]\n*** [[Fitness proportionate selection]] - also known as roulette-wheel selection\n*** [[Stochastic universal sampling]]\n*** [[Truncation selection]]\n*** [[Tournament selection]]\n** [[Memetic algorithm]]\n** [[Swarm intelligence]]\n*** [[Ant colony optimization]]\n*** [[Bees algorithm]]: a search algorithm which mimics the food foraging behavior of swarms of honey bees\n*** [[Particle swarm optimization|Particle swarm]]\n* [[golden section search]]: an algorithm for finding the maximum of a real function\n* [[Gradient descent]]\n* [[Harmony search]] (HS): a [[metaheuristic]] algorithm mimicking the improvisation process of musicians\n* [[Interior point method]]\n* {{anchor|Linear programming}}[[Linear programming]]\n** [[Benson's algorithm]]: an algorithm for solving linear [[vector optimization]] problems\n** [[Dantzig–Wolfe decomposition]]: an algorithm for solving linear programming problems with special structure\n** [[Delayed column generation]]\n** [[Integer linear programming]]: solve linear programming problems where some or all the unknowns are restricted to integer values\n*** [[Branch and cut]]\n*** [[Cutting-plane method]]\n** [[Karmarkar's algorithm]]: The first reasonably efficient algorithm that solves the [[linear programming]] problem in [[polynomial time]].\n** [[Simplex algorithm]]: An algorithm for solving [[linear programming]] problems\n* [[Line search]]\n* [[Local search (optimization)|Local search]]: a metaheuristic for solving computationally hard optimization problems\n** [[Random-restart hill climbing]]\n** [[Tabu search]]\n* [[Minimax#Minimax algorithm with alternate moves|Minimax]] used in game programming\n* [[Nearest neighbor search]] (NNS): find closest points in a [[metric space]]\n** [[Best Bin First]]: find an approximate solution to the [[Nearest neighbor search]] problem in very-high-dimensional spaces\n* [[Newton's method in optimization]]\n* [[Nonlinear optimization]]\n** [[BFGS method]]: A [[nonlinear optimization]] algorithm\n** [[Gauss–Newton algorithm]]: An algorithm for solving nonlinear [[least squares]] problems.\n** [[Levenberg–Marquardt algorithm]]: An algorithm for solving nonlinear [[least squares]] problems.\n** [[Nelder–Mead method]] (downhill simplex method): A [[nonlinear optimization]] algorithm\n* [[Odds algorithm]] (Bruss algorithm) : Finds the optimal strategy to predict a last specific event in a random sequence event\n* [[Simulated annealing]]\n* [[Stochastic tunneling]]\n* [[Subset sum problem|Subset sum]] algorithm\n\n==Computational science==\n{{further|Computational science}}\n\n===Astronomy===\n{{main|Astronomical algorithms}}\n* [[Doomsday algorithm]]: day of the week\n* [[Zeller's congruence]] is an algorithm to calculate the day of the week for any Julian or Gregorian calendar date\n* various [[Computus|Easter algorithms]] are used to calculate the day of Easter\n\n===Bioinformatics===\n{{further|Bioinformatics}}\n{{see also|List of algorithms#Sequence alignment|l1=Sequence alignment algorithms}}\n* [[Basic Local Alignment Search Tool]] also known as BLAST: an algorithm for comparing primary biological sequence information\n* [[Kabsch algorithm]]: calculate the optimal alignment of two sets of points in order to compute the [[RMSD|root mean squared deviation]] between two protein structures.\n* [[Velvet (algorithm)|Velvet]]: a set of algorithms manipulating [[de Bruijn graph]]s for genomic [[sequence assembly]]\n* [[Sorting by signed reversals]]: an algorithm for understanding genomic evolution.\n* [[Maximum parsimony (phylogenetics)]]: an algorithm for finding the simplest phylogenetic tree to explain a given character matrix.\n* [[UPGMA]]: a distance-based phylogenetic tree construction algorithm.\n\n===Geoscience===\n{{further|Geoscience}}\n* [[Vincenty's formulae]]: a fast algorithm to calculate the distance between two latitude/longitude points on an ellipsoid\n* [[Geohash]]: a public domain algorithm that encodes a decimal latitude/longitude pair as a hash string\n\n===Linguistics===\n{{further|Computational linguistics|Natural language processing}}\n* [[Lesk algorithm]]: word sense disambiguation\n* [[Stemming|Stemming algorithm]]: a method of reducing words to their stem, base, or root form\n* [[Sukhotin's algorithm]]: a statistical classification algorithm for classifying characters in a text as vowels or consonants\n\n===Medicine===\n{{further|Medical algorithms}}\n* [[ESC algorithm]] for the diagnosis of heart failure\n* [[Manning Criteria]] for irritable bowel syndrome\n* [[Pulmonary embolism#Algorithms|Pulmonary embolism]] diagnostic algorithms\n* [[Texas Medication Algorithm Project]]\n\n===Physics===\n{{further|Computational physics}}\n\n* [[Constraint algorithm]]: a class of algorithms for satisfying constraints for bodies that obey Newton's equations of motion\n* [[Demon algorithm]]:  a [[Monte Carlo method]] for efficiently sampling members of a [[microcanonical ensemble]] with a given energy\n* [[Featherstone's algorithm]]: compute the effects of forces applied to a structure of joints and links\n* [[Ground state]] approximation\n** [[Variational method]]\n*** [[Ritz method]]\n* [[N-body problem]]s\n** [[Barnes–Hut simulation]]: Solves the n-body problem in an approximate way that has the order {{math|O(<var>n</var> log <var>n</var>)}} instead of {{math|O(<var>n</var><sup>2</sup>)}} as in a direct-sum simulation.\n** [[Fast multipole method]] (FMM): speeds up the calculation of long-ranged forces\n* [[Rainflow-counting algorithm]]: Reduces a complex [[stress (physics)|stress]] history to a count of elementary stress-reversals for use in [[fatigue (material)|fatigue]] analysis\n* [[Sweep and prune]]: a broad phase algorithm used during [[collision detection]] to limit the number of pairs of solids that need to be checked for collision\n* [[VEGAS algorithm]]: a method for reducing error in [[Monte Carlo simulation]]s\n\n===Statistics===\n{{further|Computational statistics}}\n* [[Algorithms for calculating variance]]: avoiding instability and numerical overflow\n* [[Approximate counting algorithm]]: Allows counting large number of events in a small register\n* [[Bayesian statistics]]\n** [[Nested sampling algorithm]]: a computational approach to the problem of comparing models in Bayesian statistics\n* [[Data clustering|Clustering Algorithms]]\n** [[UPGMA|Average-linkage clustering]]: a simple agglomerative clustering algorithm\n** [[Canopy clustering algorithm]]:  an unsupervised pre-clustering algorithm related to the K-means algorithm\n** [[Complete-linkage clustering]]: a simple agglomerative clustering algorithm\n** [[DBSCAN]]: a density based clustering algorithm\n** [[Expectation-maximization algorithm]]\n** [[Fuzzy clustering]]: a class of clustering algorithms where each point has a degree of belonging to clusters\n*** [[Fuzzy clustering#Fuzzy c-means clustering|Fuzzy c-means]]\n*** [[FLAME clustering]] (Fuzzy clustering by Local Approximation of MEmberships): define clusters in the dense parts of a dataset and perform cluster assignment solely based on the neighborhood relationships among objects\n** [[KHOPCA clustering algorithm]]: a local clustering algorithm, which produces hierarchical multi-hop clusters in static and mobile environments.\n** [[k-means clustering]]: cluster objects based on attributes into partitions\n** [[k-means++]]: a variation of this, using modified random seeds\n** [[k-medoids]]: similar to k-means, but chooses datapoints or [[medoid]]s as centers\n** [[Linde–Buzo–Gray algorithm]]: a vector quantization algorithm to derive a good codebook\n** [[Lloyd's algorithm]] (Voronoi iteration or relaxation): group data points into a given number of categories, a popular algorithm for [[k-means clustering]]\n** [[OPTICS algorithm|OPTICS]]: a density based clustering algorithm with a visual evaluation method\n** [[Single-linkage clustering]]: a simple agglomerative clustering algorithm\n** [[SUBCLU]]: a subspace clustering algorithm\n** [[Ward's method]] : an agglomerative clustering algorithm, extended to more general Lance–Williams algorithms\n** [[WACA clustering algorithm]]: a local clustering algorithm with potentially multi-hop structures; for dynamic networks\n* [[Estimation theory|Estimation Theory]]\n** [[Expectation-maximization algorithm]] A class of related algorithms for finding maximum likelihood estimates of parameters in probabilistic models\n*** [[Ordered subset expectation maximization]] (OSEM): used in [[medical imaging]] for [[positron emission tomography]], [[single photon emission computed tomography]] and [[X-ray]] computed tomography.\n** [[Odds algorithm]] (Bruss algorithm) Optimal online search for distinguished value in sequential random input\n** [[Kalman filter]]: estimate the state of a linear [[Dynamical system|dynamic system]] from a series of noisy measurements\n* [[False nearest neighbor algorithm]] (FNN) estimates [[fractal dimension]]\n* [[Hidden Markov model]]\n** [[Baum–Welch algorithm]]: compute maximum likelihood estimates and [[Maximum a posteriori|posterior mode]] estimates for the parameters of a [[hidden markov model]]\n** [[Forward-backward algorithm]] a dynamic programming algorithm for computing the probability of a particular observation sequence\n** [[Viterbi algorithm]]: find the most likely sequence of hidden states in a [[hidden markov model]]\n* [[Partial least squares regression]]:  finds a linear model describing some predicted variables in terms of other observable variables\n* [[Queuing theory]]\n** [[Buzen's algorithm]]: an algorithm for calculating the normalization constant G(K) in the [[Gordon–Newell theorem]]\n* [[RANSAC]] (an abbreviation for \"RANdom SAmple Consensus\"): an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers\n* [[Scoring algorithm]]: is a form of [[Newton's method]] used to solve [[maximum likelihood]] equations numerically\n* [[Yamartino method]]: calculate an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data\n* [[Ziggurat algorithm]]: generate random numbers from a non-uniform distribution\n\n==Computer science==\n{{further|Computer science}}\n\n===Computer architecture===\n{{further|Computer architecture}}\n* [[Tomasulo algorithm]]: allows sequential instructions that would normally be stalled due to certain dependencies to execute non-sequentially\n\n===Computer graphics===\n{{further|Computer graphics}}\n* [[Clipping (computer graphics)|Clipping]]\n** [[Line clipping]]\n*** [[Cohen–Sutherland]]\n*** [[Cyrus–Beck]]\n*** [[Fast clipping|Fast-clipping]]\n*** [[Liang–Barsky]]\n*** [[Nicholl–Lee–Nicholl]]\n** Polygon clipping\n*** [[Sutherland–Hodgman]]\n*** [[Vatti clipping algorithm|Vatti]]\n*** [[Weiler–Atherton]]\n* [[Contour line]]s and [[Isosurface]]s\n** [[Marching cubes]]: extract a polygonal mesh of an isosurface from a three-dimensional scalar field (sometimes called voxels)\n** [[Marching squares]]: generate contour lines for a two-dimensional scalar field\n** [[Marching tetrahedrons]]: an alternative to [[Marching cubes]]\n* [[Discrete Green's Theorem]]: is an algorithm for computing double integral over a generalized rectangular domain in constant time.  It is a natural extension to the summed area table algorithm\n* [[Flood fill]]: fills a connected region of a multi-dimensional array with a specified symbol\n* [[Global illumination]] algorithms: Considers direct illumination and reflection from other objects.\n** [[Ambient occlusion]]\n** [[Beam tracing]]\n** [[Cone tracing]]\n** [[Image-based lighting]]\n** [[Metropolis light transport]]\n** [[Path tracing]]\n** [[Photon mapping]]\n** [[Radiosity (3D computer graphics)|Radiosity]]\n** [[Ray tracing (graphics)|Ray tracing]]\n* [[Hidden surface determination|Hidden surface removal]] or [[Hidden surface determination|Visual surface determination]]\n** [[Newell's algorithm]]: eliminate polygon cycles in the depth sorting required in hidden surface removal\n** [[Painter's algorithm]]: detects visible parts of a 3-dimensional scenery\n** [[Scanline rendering]]: constructs an image by moving an imaginary line over the image\n** [[Warnock algorithm]]\n* [[Line drawing algorithm|Line Drawing]]: graphical algorithm for approximating a line segment on discrete graphical media.\n** [[Bresenham's line algorithm]]: plots points of a 2-dimensional array to form a straight line between 2 specified points (uses decision variables)\n** [[Digital Differential Analyzer (graphics algorithm)|DDA line algorithm]]: plots points of a 2-dimensional array to form a straight line between 2 specified points (uses floating-point math)\n** [[Xiaolin Wu's line algorithm]]: algorithm for line antialiasing.\n* [[Midpoint circle algorithm]]: an algorithm used to determine the points needed for drawing a circle\n* [[Ramer–Douglas–Peucker algorithm]]: Given a 'curve' composed of line segments to find a curve not too dissimilar but that has fewer points\n* [[Shading]]\n** [[Gouraud shading]]: an algorithm to simulate the differing effects of light and colour across the surface of an object in 3D computer graphics\n** [[Phong shading]]: an algorithm to interpolate surface normal-vectors for surface shading in 3D computer graphics\n* [[Slerp]] (spherical linear interpolation): quaternion interpolation for the purpose of animating 3D rotation\n* [[Summed area table]] (also known as an integral image): an algorithm for computing the sum of values in a rectangular subset of a grid in constant time\n\n===Cryptography===\n{{further|Cryptography|Topics in cryptography}}\n\n* [[Asymmetric key algorithm|Asymmetric (public key) encryption]]:\n** [[ElGamal encryption|ElGamal]]\n** [[Elliptic curve cryptography]]\n** [[Matei Array Encreption 1|MAE1]]\n** [[NTRUEncrypt]]\n** [[RSA (cryptosystem)|RSA]]\n* [[Digital signature]]s (asymmetric authentication):\n** [[Digital Signature Algorithm|DSA]], and its variants:\n*** [[ECDSA]] and [https://tools.ietf.org/html/rfc6979 Deterministic ECDSA]\n*** [[EdDSA]] (Ed25519)\n** [[RSA (cryptosystem)|RSA]]\n* [[Cryptographic hash function]]s (see also the section on message authentication codes):\n** [[BLAKE (hash function)|BLAKE]]\n** [[MD5]] – Note that there is now a method of generating collisions for MD5\n** [[RIPEMD-160]]\n** [[SHA-1]] – Note that there is now a method of generating collisions for SHA-1\n** [[SHA-2]] (SHA-224, SHA-256, SHA-384, SHA-512)\n** [[SHA-3]] (SHA3-224, SHA3-256, SHA3-384, SHA3-512, SHAKE128, SHAKE256)\n** [[Tiger (hash)|Tiger]] (TTH), usually used in [[Hash tree (persistent data structure)|Tiger tree hashes]]\n** [[WHIRLPOOL]]\n* [[Cryptographically secure pseudo-random number generator]]s\n** [[Blum Blum Shub]] - based on the hardness of [[integer factorization|factorization]]\n** [[Fortuna (PRNG)|Fortuna]], intended as an improvement on [[Yarrow algorithm]]\n** [[Linear-feedback shift register]] (note: many LFSR-based algorithms are weak or have been broken)\n** [[Yarrow algorithm]]\n* [[Key exchange]]\n** [[Diffie–Hellman key exchange]]\n** [[Elliptic-curve Diffie-Hellman]] (ECDH)\n* [[Key derivation function]]s, often used for [[password hashing]] and [[key stretching]]\n** [[bcrypt]]\n** [[PBKDF2]]\n** [[scrypt]]\n** [[Argon2]]\n* [[Message authentication code]]s (symmetric authentication algorithms, which take a key as a parameter):\n** [[keyed-hash message authentication code|HMAC]]: keyed-hash message authentication\n** [[Poly1305]]\n** [[SipHash]]\n*[[Secret sharing]], Secret Splitting, Key Splitting, M of N algorithms\n** Blakey's Scheme\n** [[Shamir's Secret Sharing|Shamir's Scheme]]\n* [[symmetric key algorithm|Symmetric (secret key) encryption]]:\n** [[Advanced Encryption Standard]] (AES), winner of [[NIST]] competition, also known as [[Rijndael]]\n** [[Blowfish (cipher)|Blowfish]]\n** [[Twofish]]\n** [[Threefish]]\n** [[Data Encryption Standard]] (DES), sometimes DE Algorithm, winner of NBS selection competition, replaced by AES for most purposes\n** [[International Data Encryption Algorithm|IDEA]]\n** [[RC4 (cipher)]]\n** [[Tiny Encryption Algorithm]] (TEA)\n** [[Salsa20]], and its updated variant [[Salsa20#ChaCha_variant|ChaCha20]]\n* [[Post-quantum cryptography]]\n* [[Proof-of-work system|Proof-of-work algorithms]]\n\n===Digital logic===\n*   Boolean minimization\n** [[Quine–McCluskey algorithm]]: Also called as Q-M algorithm, programmable method for simplifying the boolean equations.\n** [[Petrick's method]]: Another algorithm for boolean simplification.\n** [[Espresso heuristic logic minimizer]]: Fast algorithm for boolean function minimization.\n\n=== Machine learning and statistical classification ===\n{{Main|List of machine learning algorithms}}\n{{further|Machine learning|Statistical classification}}\n* [[ALOPEX]]: a correlation-based [[Machine learning|machine-learning algorithm]]\n* [[Association rule learning]]: discover interesting relations between variables, used in [[data mining]]\n** [[Apriori algorithm]]\n** [[Eclat algorithm]]\n** [[Association rule learning#FP-growth algorithm|FP-growth algorithm]]\n** [[One-attribute rule]]\n** [[Association rule learning#Zero-attribute rule|Zero-attribute rule]]\n* [[Boosting (meta-algorithm)]]: Use many weak learners to boost effectiveness\n** [[AdaBoost]]: adaptive boosting\n** [[BrownBoost]]:a boosting algorithm that may be robust to noisy datasets\n** [[LogitBoost]]: [[logistic regression]] boosting\n** [[LPBoost]]: [[linear programming]] boosting\n* [[Bootstrap aggregating]] (bagging): technique to improve stability and classification accuracy\n* [[Computer Vision]]\n** [[Grabcut]] based on [[Graph cuts in computer vision|Graph cuts]]\n* [[Decision tree learning|Decision Trees]]\n** [[C4.5 algorithm]]: an extension to ID3\n** [[ID3 algorithm]] (Iterative Dichotomiser 3):  Use heuristic to generate small decision trees\n* [[Cluster analysis|Clustering]]: Class of [[unsupervised learning]] algorithms for grouping and bucketing related input vector.\n** [[k-nearest neighbors]] (k-NN): a method for classifying objects based on closest training examples in the [[feature space]]\n* [[Linde–Buzo–Gray algorithm]]: a vector quantization algorithm used to derive a good codebook\n* [[Locality-sensitive hashing]] (LSH): a method of performing probabilistic dimension reduction of high-dimensional data\n* [[Artificial neural network|Neural Network]]\n** [[Backpropagation]]: A [[supervised learning]] method which requires a teacher that knows, or can calculate, the desired output for any given input\n** [[Hopfield net]]:  a [[Recurrent neural network]] in which all connections are symmetric\n** [[Perceptron]]: the simplest kind of feedforward neural network: a [[linear classifier]].\n** [[Pulse-coupled neural networks]] (PCNN): [[Artificial neural network|Neural models]] proposed by modeling a cat's [[visual cortex]] and developed for high-performance [[Bionics|biomimetic]] image processing.\n** [[Radial basis function network]]: an artificial neural network that uses radial [[basis function]]s as activation functions\n** [[Self-organizing map]]: an unsupervised network that produces a low-dimensional representation of the input space of the training samples\n* [[Random forest]]: classify using many decision trees\n* [[Reinforcement Learning]]:\n** [[Q-learning]]:  learn an action-value function that gives the expected utility of taking a given action in a given state and following a fixed policy thereafter\n** [[State-Action-Reward-State-Action]] (SARSA): learn a [[Markov decision process]] policy\n** [[Temporal difference learning]]\n* [[Relevance Vector Machine]] (RVM): similar to SVM, but provides probabilistic classification\n* [[Supervised Learning]]: Learning by examples (labelled data-set split into training-set and test-set)\n* [[Support Vector Machines]] (SVM): a set of methods which divide multidimensional data by finding a dividing hyperplane with the maximum margin between the two sets\n** [[Structured SVM]]: allows training of a classifier for general structured output labels.\n* [[Winnow algorithm]]: related to the perceptron, but uses a [[Multiplicative Weight Update Method|multiplicative weight-update scheme]]\n\n===Programming language theory===\n{{further|Programming language theory}}\n* [[C3 linearization]]: an algorithm used primarily to obtain a consistent linearization of a multiple inheritance hierarchy in object-oriented programming\n* [[Chaitin's algorithm]]: a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric\n* [[Hindley-Milner type inference|Hindley–Milner type inference algorithm]]\n* [[Rete algorithm]]: an efficient pattern matching algorithm for implementing [[Start symbol (formal languages)|production rule]] systems\n* [[Sethi-Ullman algorithm]]: generate optimal code for arithmetic expressions\n\n====Parsing====\n{{further|Parsing}}\n* [[CYK algorithm]]: An O(n<sup>3</sup>) algorithm for parsing [[context-free grammar]]s in [[Chomsky normal form]]\n* [[Earley parser]]: Another O(n<sup>3</sup>) algorithm for parsing any [[context-free grammar]]\n* [[GLR parser]]:An algorithm for parsing any [[context-free grammar]] by [[Masaru Tomita]]. It is tuned for deterministic grammars, on which it performs almost [[linear time]] and O(n<sup>3</sup>) in worst case.\n* [[Inside-outside algorithm]]: An O(n<sup>3</sup>) algorithm for re-estimating production probabilities in [[probabilistic context-free grammar]]s\n* [[LL parser]]: A relatively simple [[linear time]] parsing algorithm for a limited class of [[context-free grammar]]s\n* [[LR parser]]: A more complex [[linear time]] parsing algorithm for a larger class of [[context-free grammar]]s.  Variants:\n** [[Canonical LR parser]]\n** [[Look-ahead LR parser|LALR (Look-ahead LR) parser]]\n** [[Operator-precedence parser]]\n** [[Simple LR parser|SLR (Simple LR) parser]]\n** [[Simple precedence parser]]\n* [[Packrat parser]]: A [[linear time]] parsing algorithm supporting some [[context-free grammar]]s and [[parsing expression grammar]]s\n* [[Recursive descent parser]]: A [[top-down parsing|top-down parser]] suitable for LL(''k'') grammars\n* [[Shunting yard algorithm]]: convert an infix-notation math expression to postfix\n* [[Pratt parser]]\n* [[Lexical analysis]]\n\n===Quantum algorithms===\n{{further|Quantum algorithm}}\n* [[Deutsch-Jozsa algorithm]]: criterion of balance for Boolean function\n* [[Grover's algorithm]]: provides quadratic speedup for many search problems\n* [[Shor's algorithm]]: provides [[exponential function|exponential]] speedup (relative to currently known non-quantum algorithms) for factoring a number\n* [[Simon's algorithm]]: provides a provably [[exponential function|exponential]] speedup (relative to any non-quantum algorithm) for a black-box problem\n\n===Theory of computation and automata===\n{{further|Theory of computation}}\n* [[DFA minimization#Hopcroft's algorithm|Hopcroft's algorithm]], [[DFA minimization#Moore's algorithm|Moore's algorithm]], and [[DFA minimization#Brzozowski's algorithm|Brzozowski's algorithm]]: algorithms for [[DFA minimization|minimizing the number of states in a deterministic finite automaton]]\n* [[Powerset construction]]: Algorithm to convert nondeterministic automaton to [[deterministic automaton]].\n* [[Tarski–Kuratowski algorithm]]: a [[non-deterministic algorithm]] which provides an upper bound for the complexity of formulas in the [[arithmetical hierarchy]] and [[analytical hierarchy]]\n\n==Information theory and signal processing==\n{{main|Information theory|Signal processing}}\n\n===Coding theory===\n{{further|Coding theory}}\n\n====Error detection and correction====\n{{further|Error detection and correction}}\n* [[BCH Code]]s\n** [[Berlekamp–Massey algorithm]]\n** [[Peterson–Gorenstein–Zierler algorithm]]\n** [[Reed–Solomon error correction]]\n* [[BCJR algorithm]]: decoding of error correcting codes defined on trellises (principally convolutional codes)\n* [[Forward error correction]]\n* [[Gray code]]\n* [[Hamming code]]s\n** [[Hamming(7,4)]]:  a [[Hamming code]] that encodes 4 bits of data into 7 bits by adding 3 parity bits\n** [[Hamming distance]]: sum number of positions which are different\n** [[Hamming weight]] (population count): find the number of 1 bits in a binary word\n* [[Redundancy check]]s\n** [[Adler-32]]\n** [[Cyclic redundancy check]]\n** [[Damm algorithm]]\n** [[Fletcher's checksum]]\n** [[Longitudinal redundancy check]] (LRC)\n** [[Luhn algorithm]]: a method of validating identification numbers\n** [[Luhn mod N algorithm]]: extension of Luhn to non-numeric characters\n** [[Parity bit|Parity]]: simple/fast error detection technique\n** [[Verhoeff algorithm]]\n\n====Lossless compression algorithms====\n{{main|:Category:Lossless compression algorithms|l1=Lossless compression algorithms}}\n* [[Burrows–Wheeler transform]]: preprocessing useful for improving [[Lossless data compression|lossless compression]]\n* [[Context tree weighting]]\n* [[Delta encoding]]: aid to compression of data in which sequential data occurs frequently\n* [[Dynamic Markov compression]]: Compression using predictive arithmetic coding\n* [[Dictionary coder]]s\n** [[Byte pair encoding]] (BPE)\n** [[DEFLATE (algorithm)|DEFLATE]]\n** [[Lempel–Ziv]]\n*** [[LZ77 and LZ78]]\n*** [[LZJB|Lempel–Ziv Jeff Bonwick]] (LZJB)\n*** [[Lempel–Ziv–Markov chain algorithm]] (LZMA)\n*** [[Lempel–Ziv–Oberhumer]] (LZO): speed oriented\n*** [[Lempel–Ziv–Stac]] (LZS)\n*** [[Lempel–Ziv–Storer–Szymanski]] (LZSS)\n*** [[Lempel–Ziv–Welch]] (LZW)\n*** [[LZWL]]: syllable-based variant\n*** [[LZX]]\n*** [[LZRW|Lempel–Ziv Ross Williams]] (LZRW)\n* [[Entropy encoding]]: coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols\n** [[Arithmetic coding]]: advanced [[entropy]] coding\n*** [[Range encoding]]: same as [[arithmetic coding]], but looked at in a slightly different way\n** [[Huffman coding]]: simple lossless compression taking advantage of relative character frequencies\n*** [[Adaptive Huffman coding]]: [[adaptive coding]] technique based on Huffman coding\n*** [[Package-merge algorithm]]: Optimizes Huffman coding subject to a length restriction on code strings\n** [[Shannon–Fano coding]]\n** [[Shannon–Fano–Elias coding]]: precursor to arithmetic encoding<ref>[https://wayback.archive-it.org/all/20090325200343/http://www.ece.msstate.edu/~fowler/Classes/ECE8813/Handouts/shannon_fano_elias.pdf] </ref>\n* [[Entropy encoding|Entropy coding with known entropy characteristics]]\n** [[Golomb coding]]: form of entropy coding that is optimal for alphabets following geometric distributions\n** [[Rice coding]]: form of entropy coding that is optimal for alphabets following geometric distributions\n** [[Truncated binary encoding]]\n** [[Unary coding]]: code that represents a number n with n ones followed by a zero\n** [[Universal code (data compression)|Universal codes]]: encodes positive integers into binary code words\n*** Elias [[Elias delta coding|delta]], [[Elias gamma coding|gamma]], and [[Elias omega coding|omega]] coding\n*** [[Exponential-Golomb coding]]\n*** [[Fibonacci coding]]\n*** [[Levenshtein coding]]\n* [[FELICS|Fast Efficient & Lossless Image Compression System]] (FELICS): a lossless image compression algorithm\n* [[Incremental encoding]]: delta encoding applied to sequences of strings\n* [[PPM compression algorithm|Prediction by partial matching]] (PPM): an adaptive statistical data compression technique based on context modeling and prediction\n* [[Run-length encoding]]: lossless data compression taking advantage of strings of repeated characters\n* [[SEQUITUR algorithm]]: lossless compression by incremental grammar inference on a string\n\n====Lossy compression algorithms====\n{{main|:Category:Lossy compression algorithms|l1=Lossy compression algorithms}}\n* [[3Dc]]: a lossy data compression algorithm for [[Normal mapping|normal maps]]\n* [[Audio data compression|Audio]] and [[speech encoding|Speech]] compression\n** [[A-law algorithm]]: standard companding algorithm\n** [[Code-excited linear prediction]] (CELP): low bit-rate speech compression\n** [[Linear predictive coding]] (LPC): lossy compression by representing the [[spectral envelope]] of a digital signal of speech in compressed form\n** [[Mu-law algorithm]]: standard analog signal compression or companding algorithm\n** [[Warped Linear Predictive Coding]] (WLPC)\n* [[Image Compression]]\n** [[Block Truncation Coding]] (BTC): a type of lossy image compression technique for greyscale images\n** [[Embedded Zerotree Wavelet]] (EZW)\n** [[Fast Cosine Transform|Fast Cosine Transform algorithm]]s (FCT algorithms): compute Discrete Cosine Transform (DCT) efficiently\n** [[Fractal compression]]: method used to compress images using fractals\n** [[Set Partitioning in Hierarchical Trees]] (SPIHT)\n** [[Wavelet compression]]: form of data compression well suited for [[image compression]] (sometimes also video compression and audio compression)\n* [[Transform coding]]: type of data compression for \"natural\" data like audio signals or photographic images\n* [[Video compression]]\n* [[Vector quantization]]: technique often used in lossy data compression\n\n===Digital signal processing===\n{{further|Digital signal processing}}\n* [[Adaptive-additive algorithm]] (AA algorithm): find the spatial frequency phase of an observed wave source\n* [[Discrete Fourier transform]]: determines the frequencies contained in a (segment of a) signal\n** [[Bluestein's FFT algorithm]]\n** [[Bruun's FFT algorithm]]\n** [[Cooley&ndash;Tukey FFT algorithm]]\n** [[Fast Fourier transform]]\n** [[Prime-factor FFT algorithm]]\n** [[Rader's FFT algorithm]]\n* [[Fast folding algorithm]]: an efficient algorithm for the detection of approximately periodic events within time series data\n* [[Gerchberg–Saxton algorithm]]: Phase retrieval algorithm for optical planes\n* [[Goertzel algorithm]]: identify a particular frequency component in a signal.  Can be used for [[DTMF]] digit decoding.\n* [[Karplus-Strong string synthesis]]: physical modelling synthesis to simulate the sound of a hammered or plucked string or some types of percussion\n\n====Image processing====\n{{further|Digital image processing}}\n* Contrast Enhancement\n** [[Histogram equalization]]: use histogram to improve image contrast\n** [[Adaptive histogram equalization]]: histogram equalization which adapts to local changes in contrast\n* [[Connected-component labeling]]: find and label disjoint regions\n* [[Dithering]] and [[half-toning]]\n** [[Error diffusion]]\n** [[Floyd–Steinberg dithering]]\n** [[Ordered dithering]]\n** [[Riemersma dithering]]\n* Elser [[difference-map algorithm]]: a search algorithm for general constraint satisfaction problems.  Originally used for [[X-ray crystallography|X-Ray diffraction]] microscopy\n* [[Feature detection (computer vision)|Feature detection]]\n** [[Canny edge detector]]: detect a wide range of edges in images\n** [[Generalised Hough transform]]\n** [[Hough transform]]\n** [[Marr–Hildreth algorithm]]: an early [[edge detection]] algorithm\n** [[Scale-invariant feature transform|SIFT]] (Scale-invariant feature transform): is an algorithm to detect and describe local features in images.\n** [[SURF (Speeded Up Robust Features)]]: is a robust local feature detector, first presented by Herbert Bay et al. in 2006, that can be used in computer vision tasks like object recognition or 3D reconstruction. It is partly inspired by the SIFT descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT.<ref>[https://web.archive.org/web/20070221214147/http://www.vision.ee.ethz.ch/~surf/eccv06.pdf] </ref><ref>{{cite web |url=http://glorfindel.mavrinac.com/~aaron/school/pdf/bay06_surf.pdf |title=Archived copy |accessdate=2013-10-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20131006113018/http://glorfindel.mavrinac.com/~aaron/school/pdf/bay06_surf.pdf |archivedate=2013-10-06 |df= }}</ref>\n* [[Richardson–Lucy deconvolution]]: image de-blurring algorithm\n* [[Blind deconvolution]]: image de-blurring algorithm when [[point spread function]] is unknown.\n* [[Median filtering]]\n* [[Seam carving]]: content-aware image resizing algorithm\n* [[Segmentation (image processing)|Segmentation]]: partition a digital image into two or more regions\n** [[GrowCut algorithm]]: an interactive segmentation algorithm\n** [[Random walker algorithm]]\n** [[Region growing]]\n** [[Watershed (algorithm)|Watershed transformation]]: a class of algorithms based on the watershed analogy\n\n==Software engineering==\n{{further|Software engineering}}\n\n* [[Cache algorithms]]\n* [[CHS conversion]]: converting between disk addressing systems\n* [[Double dabble]]: Convert binary numbers to BCD\n* [[Hash Function]]: convert a large, possibly variable-sized amount of data into a small datum, usually a single integer that may serve as an index into an array\n** [[Fowler–Noll–Vo hash function]]: fast with low collision rate\n** [[Pearson hashing]]: computes 8 bit value only, optimized for 8 bit computers\n** [[Zobrist hashing]]: used in the implementation of [[transposition table]]s\n* [[Unicode Collation Algorithm]]\n* [[Xor swap algorithm]]: swaps the values of two variables without using a buffer\n\n==Database algorithms==\n{{further|Database}}\n* [[Algorithms for Recovery and Isolation Exploiting Semantics]] (ARIES): [[transaction (database)|transaction]] recovery\n* [[Join (SQL)|Join algorithms]]\n** [[Block nested loop]]\n** [[Hash join]]\n** [[Nested loop join]]\n** [[Sort-Merge Join]]\n\n==Distributed systems algorithms==\n{{further|Distributed algorithm|Distributed systems}}\n* [[Bully algorithm]]:  a method for dynamically selecting a coordinator\n* [[Byzantine fault tolerance]]: good [[Fault-tolerant system|fault tolerance]].\n* [[Clock synchronization]]\n** [[Berkeley algorithm]]\n** [[Cristian's algorithm]]\n** [[Intersection algorithm]]\n** [[Marzullo's algorithm]]\n* Detection of Process Termination\n** [[Dijkstra-Scholten algorithm]]\n** [[Huang's algorithm]]\n* [[Lamport ordering]]: a [[partial order]]ing of events based on the ''happened-before'' relation\n* [[Mutual exclusion]]\n** [[Lamport's Distributed Mutual Exclusion Algorithm]]\n**[[Naimi-Trehel's log(n) Algorithm]]\n** [[Maekawa's Algorithm]]\n** [[Raymond's Algorithm]]\n** [[Ricart-Agrawala Algorithm]]\n* [[Paxos algorithm]]: a family of protocols for solving consensus in a network of unreliable processors\n* [[Snapshot algorithm]]: record a consistent global state for an asynchronous system\n** [[Chandy-Lamport algorithm]]\n* [[Vector clocks]]: generate a [[partial ordering]] of events in a distributed system and detect [[causality]] violations\n\n===Memory allocation and deallocation algorithms===\n* [[Buddy memory allocation]]: Algorithm to allocate memory such that fragmentation is less.\n* [[Garbage collection (computer science)|Garbage collectors]]\n** [[Cheney's algorithm]]: An improvement on the [[Semi-space collector]]\n** [[garbage collection (computer science)|Generational garbage collector]]: Fast garbage collectors that segregate memory by age\n** [[Mark-compact algorithm]]:  a combination of the [[Mark and sweep|mark-sweep algorithm]]  and [[Cheney's algorithm|Cheney's copying algorithm]]\n** [[Mark and sweep]]\n** [[Semi-space collector]]: An early copying collector\n* [[Reference counting]]\n\n==Networking==\n{{further|Network scheduler}}\n* [[Karn's algorithm]]: addresses the problem of getting accurate estimates of the round-trip time for messages when using TCP\n* [[Luleå algorithm]]: a technique for storing and searching internet routing tables efficiently\n* [[Network congestion]]\n** [[Exponential backoff]]\n** [[Nagle's algorithm]]: improve the efficiency of TCP/IP networks by coalescing packets\n** [[Truncated binary exponential backoff]]\n\n==Operating systems algorithms==\n{{further|Operating systems}}\n* [[Banker's algorithm]]: Algorithm used for deadlock avoidance.\n* [[Page replacement algorithms]]: Selecting the victim page under low memory conditions.\n** [[Adaptive replacement cache]]: better performance than LRU\n** [[Clock with Adaptive Replacement]] (CAR): is a page replacement algorithm that has performance comparable to [[Adaptive replacement cache]]\n\n===Process synchronization===\n{{further|Process synchronization}}\n{{further|Process scheduler}}\n* [[Dekker's algorithm]]\n* [[Lamport's Bakery algorithm]]\n* [[Peterson's algorithm]]\n\n===Scheduling===\n{{further|Scheduling (computing)}}\n\n* [[Earliest deadline first scheduling]]\n* [[Fair-share scheduling]]\n* [[Least slack time scheduling]]\n* [[List scheduling]]\n* [[Multi level feedback queue]]\n* [[Rate-monotonic scheduling]]\n* [[Round-robin scheduling]]\n* [[Shortest job next]]\n* [[Shortest remaining time]]\n* [[Top-nodes algorithm]]: resource calendar management\n\n===I/O scheduling===\n{{further|I/O scheduling}}\n{{expand section|date=July 2017}}\n\n====Disk scheduling====\n* [[Elevator algorithm]]: Disk scheduling algorithm that works like an elevator.\n* [[Shortest seek first]]: Disk scheduling algorithm to reduce [[seek time]].\n\n==See also==\n* [[List of data structures]]\n* [[List of machine learning algorithms]]\n* [[List of pathfinding algorithms]]\n* [[List of algorithm general topics]]\n* [[List of terms relating to algorithms and data structures]]\n* [[Heuristic]]\n\n==References==\n{{Reflist}}\n\n[[Category:Algorithms|*]]\n[[Category:Mathematics-related lists|Algorithms]]\n[[Category:Optimization algorithms and methods| ]]"
    },
    {
      "title": "Active set method",
      "url": "https://en.wikipedia.org/wiki/Active_set_method",
      "text": "{{redirect|Active set|the band|The Active Set}}\n\nIn mathematical [[Optimization (mathematics)|optimization]], a problem is defined using an objective function to minimize or maximize, and a set of constraints\n\n:<math>g_1(x)\\ge 0, \\dots, g_k(x)\\ge 0</math>\n\nthat define the [[feasible region]], that is, the set of all ''x'' to search for the optimal solution. Given a point <math>x</math> in the feasible region, a constraint \n:<math>g_i(x) \\ge 0</math>\nis called '''active''' at <math>x</math> if <math>g_i(x)=0</math> and '''inactive''' at <math>x</math> if <math>g_i(x)>0.</math> Equality constraints are always active. The '''active set''' at <math>x</math> is made up of those constraints <math>g_i(x)</math> that are active at the current point {{harv|Nocedal|Wright|2006|p=308}}.\n\nThe active set is particularly important in optimization theory as it determines which constraints will influence the final result of optimization. For example, in solving the [[linear programming]] problem, the active set gives the [[hyperplane]]s that intersect at the solution point. In [[quadratic programming]], as the solution is not necessarily on one of the edges of the bounding polygon, an estimation of the active set gives us a subset of inequalities to watch while searching the solution, which reduces the complexity of the search.\n\n==Active set methods==\nIn general an active set algorithm has the following structure:\n\n:Find a feasible starting point\n:'''repeat until''' \"optimal enough\"\n::''solve'' the equality problem defined by the active set (approximately)\n::''compute'' the [[Lagrange multipliers]] of the active set\n::''remove'' a subset of the constraints with negative Lagrange multipliers\n::''search'' for infeasible constraints\n:'''end repeat'''\n\nMethods that can be described as '''active set methods''' include:<ref>{{harvnb|Nocedal|Wright|2006|pp=467–480}}</ref>\n* [[Successive linear programming]] (SLP) <!-- acc. to: Leyffer... - alt: acc. to \"MPS glossary\", http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Main_Page: Successive approximation -->\n* [[Sequential quadratic programming]] (SQP) <!-- acc. to: Leyffer... - alt: acc. to \"MPS glossary\", http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Main_Page: Successive approximation  -->\n* [[Sequential linear-quadratic programming]] (SLQP) <!-- acc. to: Leyffer... - alt: acc. to \"MPS glossary\", http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Main_Page: Successive approximation  -->\n* [[Frank–Wolfe algorithm|Reduced gradient method]] (RG) <!-- acc. to: MPS glossary, http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Main_Page - alt: acc. to \"Optimization - Theory and Practice\" (Forst, Hoffmann): Projection method -->\n* [[Generalized Reduced Gradient|Generalized reduced gradient method]] (GRG) <!-- acc. to: MPS glossary, http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Main_Page - alt: acc. to \"Optimization - Theory and Practice\" (Forst, Hoffmann): Projection method -->\n<!-- ? Wilson's Lagrange-newton method -->\n<!-- ? Method of feasible directions (MFD) -->\n<!-- ? Gradient projection method -  alt: acc. to \"Optimization - Theory and Practice\" (Forst, Hoffmann): Projection method -->\n\n==References==\n{{Reflist|30em}}\n\n==Bibliography==\n* {{cite book|last=Murty|first=K. G.|title=Linear complementarity, linear and nonlinear programming|series=Sigma Series in Applied Mathematics|volume=3|publisher=Heldermann Verlag|location=Berlin|year=1988|pages=xlviii+629 pp.|isbn=3-88538-403-5|url=http://ioe.engin.umich.edu/people/fac/books/murty/linear_complementarity_webbook/|ref=harv|MR=949214|access-date=2010-04-03|archive-url=https://web.archive.org/web/20100401043940/http://ioe.engin.umich.edu/people/fac/books/murty/linear_complementarity_webbook/|archive-date=2010-04-01|dead-url=yes|df=}} \n* {{Cite book | last1=Nocedal | first1=Jorge | last2=Wright | first2=Stephen J. | title=Numerical Optimization | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-30303-1 | year=2006 | ref=harv | postscript=<!--None-->}}.\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Adaptive coordinate descent",
      "url": "https://en.wikipedia.org/wiki/Adaptive_coordinate_descent",
      "text": "'''Adaptive coordinate descent'''<ref>{{cite conference\n| first = I.\n| last = Loshchilov\n|author2=M. Schoenauer|author3=M. Sebag\n| title = Adaptive Coordinate Descent\n| booktitle = Genetic and Evolutionary Computation Conference (GECCO)\n| pages = 885–892\n| publisher = ACM Press\n| date = 2011\n| location = \n| url = http://www.loshchilov.com/publications/GECCO2011_AdaptiveCoordinateDescent.pdf\n| accessdate = \n| id = \n}}</ref> is an improvement of the [[coordinate descent]] [[algorithm]] to non-separable [[Mathematical optimization|optimization]] by the use of [[adaptive encoding]].<ref>\nNikolaus Hansen. \"[https://hal.inria.fr/inria-00287351 Adaptive Encoding: How to Render Search Coordinate System Invariant]\". Parallel Problem Solving from Nature - PPSN X, Sep 2008, Dortmund, Germany. pp.205-214, 2008.\n</ref> The adaptive coordinate descent approach gradually builds a transformation of the coordinate system such that the new coordinates are as decorrelated as possible with respect to the objective function. The adaptive coordinate descent was shown to be competitive to the state-of-the-art [[evolutionary algorithms]] and has the following invariance properties:\n# Invariance with respect to monotonous transformations of the function (scaling)\n# Invariance with respect to [[orthogonal transform]]ations of the search space (rotation).\n\n[[CMA-ES|CMA]]-like Adaptive Encoding Update (b) mostly based on [[principal component analysis]] (a) is used to extend the coordinate descent method (c) to the optimization of non-separable problems (d).\n\n[[File:Adaptive Coordinate Descent illustration.png|center|x400px]]\n\nThe adaptation of an appropriate coordinate system allows adaptive coordinate descent to outperform coordinate descent on non-separable functions. The following figure illustrates the convergence of both algorithms on 2-dimensional [[Rosenbrock function]] up to a target function value <math>10^{-10}</math>, starting from the initial point <math>x_0=(-3,-4)</math>.\n\n[[File:Rosenbrock2D.png|center|x400px]]\n\nThe adaptive coordinate descent method reaches the target value after only 325 function evaluations (about 70 times faster than coordinate descent), that is comparable to [[Gradient descent|gradient-based methods]]. The algorithm has linear time complexity if update coordinate system every D iterations, it is also suitable for large-scale (D>>100) non-linear optimization.\n\n==Relevant approaches==\n\nFirst approaches to optimization using adaptive coordinate system were proposed already in the 1960s (see, e.g., [[Rosenbrock methods|Rosenbrock's method]]). PRincipal Axis (PRAXIS) algorithm, also referred to as Brent's algorithm, is an derivative-free algorithm which assumes quadratic form of the optimized function and repeatedly updates a set of conjugate search directions.<ref>\n{{cite conference\n| first = R.P.\n| last = Brent \n| booktitle = Algorithms for minimization without derivatives\n| publisher = Prentice-Hall\n| date = 1972\n}}</ref>\nThe algorithm, however, is not invariant to scaling of the objective function and may fail under its certain rank-preserving transformations (e.g., will lead to a non-quadratic shape of the objective function). A recent analysis of PRAXIS can be found in\n.<ref>\n{{cite conference\n| first = U.\n| last = Ali\n|author2= Kickmeier-Rust, M.D.\n| title = Implementation and Applications of a Three-Round User Strategy for Improved Principal Axis Minimization\n| booktitle = Journal of Applied Quantitative Methods\n| pages = 505–513\n| date = 2008\n}}</ref>\nFor practical applications see,<ref>\n{{cite conference\n| first = D.\n| last = Pavlov\n| title = Manipulator path planning in 3-dimensional space\n| booktitle = Computer Science--Theory and Applications\n| pages = 505–513\n| publisher = Springer\n| date = 2006\n}}</ref> where an adaptive coordinate descent approach with step-size adaptation and local coordinate system rotation was proposed\nfor robot-manipulator path planning in 3D space with static polygonal obstacles.\n\n==See also==\n* [[Coordinate descent]]\n* [[CMA-ES]]\n* [[Rosenbrock methods]]\n* [[Mathematical optimization]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://www.loshchilov.com/acid.html SOURCE CODE ACD] ACD is a MATLAB source code for Adaptive Coordinate Descent\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Adaptive dimensional search",
      "url": "https://en.wikipedia.org/wiki/Adaptive_dimensional_search",
      "text": "{{technical|date=October 2015}}\n\n'''Adaptive dimensional search''' [[algorithm]]s differ from nature-inspired [[metaheuristic]] techniques in the sense that they do not use any [[metaphor]] as an underlying principle for implementation. Rather, they utilize a simple, performance-oriented [[methodology]] based on the update of the search dimensionality ratio (SDR) parameter at each iteration.<ref>Hasançebi, O., Kazemzadeh Azad, S. (2015), [https://www.sciencedirect.com/science/article/pii/S0045794915001042 Adaptive Dimensional Search: A New Metaheuristic Algorithm for Discrete Truss Sizing Optimization], Computers and Structures, 154, 1-16.</ref>\n\nMany robust metaheuristic techniques, such as [[simulated annealing]], [[evolutionary algorithm]]s, [[particle swarm optimization]], and [[ant colony optimization algorithms|ant colony optimization]], have been introduced by researchers in the last few decades through clearly identifying and formulating similarities between algorithms and the processes they are modeled on. However, over time this trend of developing new search methods has made researchers feel obligated to associate their innovative ideas with some [[nature|natural]] event to provide a basis for justification of their thoughts and the originality of their algorithms. As a result, literature has abounded with metaheuristic algorithms that have weak or no similarities to the natural processes which they are purported to derive from.\n\n==References==\n{{Reflist}}\n\n\n\n[[Category:Heuristic algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Adaptive simulated annealing",
      "url": "https://en.wikipedia.org/wiki/Adaptive_simulated_annealing",
      "text": "{{no footnotes|date=November 2012}}\n'''Adaptive simulated annealing''' ('''ASA''') is a variant of [[simulated annealing]] (SA) algorithm in which the algorithm parameters that control temperature schedule and random step selection are automatically adjusted according to algorithm progress. This makes the algorithm more efficient and less sensitive to user defined parameters than canonical SA. These are in the standard variant often selected on the basis of experience and experimentation (since optimal values are problem dependent), which represents a significant deficiency in practice.\n\nThe algorithm works by representing the parameters of the function to be optimized as continuous numbers, and as dimensions of a hypercube (N dimensional space). Some SA algorithms apply [[Gaussian]] moves to the state, while others have distributions permitting faster temperature schedules. Imagine the state as a point in a box and the moves as a rugby-ball shaped cloud around it.  The temperature and the step size are adjusted so that all of the search space is sampled to a coarse resolution in the early stages, whilst the state is directed to favorable areas in the late stages. Another ASA variant, thermodynamic simulated annealing, automatically adjusts the temperature at each step based on the energy difference between the two states, according to the laws of thermodynamics.\n\n==See also==\n* [[Simulated annealing]] \n* [[Combinatorial optimization]] \n* [[Optimization (mathematics)|Optimization]]\n\n==References==\n* L. Ingber, [http://www.ingber.com/#ASA ASA-CODE, ASA-REPRINTS, ASA-INFO] Global optimization C-code, Caltech Alumni Association, Pasadena,  CA,  1993. \n* L. Ingber, [http://www.ingber.com/asa89_vfsr.ps.gz Very fast simulated re-annealing], Mathl. Comput. Modelling,Vol. 12 No. 8, pp.&nbsp;967–973, 1989.\n* L. Ingber, [http://www.ingber.com/asa93_sapvt.ps.gz Simulated annealing: Practice versus theory], Mathl. Comput. Modelling, Vol. 18 No. 11, pp.&nbsp;29–57, 1993.\n* L. Ingber, [http://www.ingber.com/asa96_lessons.ps.gz Adaptive simulated annealing (ASA): Lessons learned], Control and Cybernetics,Vol. 25 No. 1,pp.&nbsp;33–54, 1996.\n\n==External links==\n* [http://www.sinopt.com/learning1/desnotes/globopt.htm Global optimization] Explains some ideas behind ASA.\n* [http://www.ingber.com/ASA-README.html Adaptive Simulated Annealing (ASA)]  Explains history and use of the ASA code, first published as Very Fast Simulated Reannealing (VFSR) in 1989, and made available to the public at no charge since 1993 under the name ASA.  This ASA algorithm is not the same as the algorithm described at the top of Adaptive simulated annealing.\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Affine scaling",
      "url": "https://en.wikipedia.org/wiki/Affine_scaling",
      "text": "[[File:Karmarkar.svg|thumb|The affine scaling method is an ''interior point method'', meaning that it forms a trajectory of points strictly inside the [[feasible region]] of a linear program (as opposed to the [[simplex algorithm]], which walks the corners of the feasible region).]]\nIn [[mathematical optimization]], '''affine scaling''' is an [[algorithm]] for solving [[linear programming]] problems. Specifically, it is an [[interior point method]], discovered by [[Soviet Union|Soviet]] mathematician I. I. Dikin in 1967 and reinvented in the [[United States|U.S.]] in the mid-1980s.\n\n==History==\nAffine scaling has a history of [[multiple discovery]]. It was first published by I. I. Dikin in the 1967 ''[[Doklady Akademii Nauk SSSR]]'', followed by a proof of its convergence in 1974.{{r|vanderbei90}} Dikin's work went largely unnoticed until the 1984 discovery of [[Karmarkar's algorithm]], the first practical [[polynomial time]] algorithm for linear programming. The importance and complexity of Karmarkar's method prompted mathematicians to search for a simpler version.\n\nSeveral groups then independently came up with a variant of Karmarkar's algorithm. E. R. Barnes at [[IBM]],<ref>{{cite journal |first=Earl R. |last=Barnes |title=A variation on Karmarkar's algorithm for solving linear programming problems |journal=Mathematical Programming |volume=36 |issue=2 |pages=174–182 |year=1986 |doi=10.1007/BF02592024}}</ref> a team led by [[Robert J. Vanderbei|R. J. Vanderbei]] at [[AT&T]],<ref>{{cite journal |doi=10.1007/BF01840454 |first1=Robert J. |last1=Vanderbei |last2=Meketon |first2=Marc S. |last3=Freedman |first3=Barry A. |year=1986 |title=A Modification of Karmarkar's Linear Programming Algorithm |journal=Algorithmica |volume=1 |issue=1–4 |pages=395–407 |url=https://www.princeton.edu/~rvdb/tex/myPapers/VanderbeiMeketonFreedman.pdf}}</ref> and several others replaced the [[projective geometry|projective transformations]] that [[Narendra Karmarkar|Karmarkar]] used by [[Affine transformation|affine]] ones. After a few years, it was realized that the \"new\" affine scaling algorithms were in fact reinventions of the decades-old results of Dikin.<ref name=\"vanderbei90\">{{cite conference\n | last1 = Vanderbei | first1 = R. J.\n | last2 = Lagarias | first2 = J. C.\n | contribution = I. I. Dikin's convergence result for the affine-scaling algorithm\n | doi = 10.1090/conm/114/1097868\n | mr = 1097868\n | pages = 109–119\n | publisher = American Mathematical Society | location = Providence, RI\n | series = Contemporary Mathematics\n | title = Mathematical developments arising from linear programming (Brunswick, ME, 1988)\n | volume = 114\n | year = 1990}}</ref><ref>{{cite journal |first1=D. A. |last1=Bayer |first2=J. C. |last2=Lagarias |title=The nonlinear geometry of linear programming I: Affine and projective scaling trajectories |journal=[[Transactions of the American Mathematical Society]] |volume=314 |issue=2 |year=1989 |url=http://www.ams.org/journals/tran/1989-314-02/S0002-9947-1989-1005525-6/S0002-9947-1989-1005525-6.pdf |doi=10.1090/S0002-9947-1989-1005525-6 |pages=499}}</ref> Of the re-discoverers, only Barnes and Vanderbei ''et al.'' managed to produce an analysis of affine scaling's convergence properties. Karmarkar, who had also came with affine scaling in this timeframe, mistakenly believed that it converged as quickly as his own algorithm.<ref name=\"lpfe\">{{cite book |first=Robert J. |last=Vanderbei |title=Linear Programming: Foundations and Extensions |year=2001 |publisher=Springer Verlag |pages=333–347}}</ref>{{rp|346}}\n\n==Algorithm==\nAffine scaling works in two phases, the first of which finds a [[feasible region|feasible]] point from which to start optimizing, while the second does the actual optimization while staying strictly inside the feasible region.\n\nBoth phases solve linear programs in equality form, viz.\n\n:minimize {{math|''c'' ⋅ ''x''}}\n:subject to {{math|''Ax'' {{=}} ''b''}}, {{math|''x'' ≥ 0}}.\n\nThese problems are solved using an [[iterative method]], which conceptually proceeds by plotting a trajectory of points strictly inside the feasible region of a problem, computing [[projected gradient|projected]] [[gradient descent]] steps in a re-scaled version of the problem, then scaling the step back to the original problem. The scaling ensures that the algorithm can continue to do large steps even when the point under consideration is close to the feasible region's boundary.{{r|lpfe}}{{rp|337}}\n\nFormally, the iterative method at the heart of affine scaling takes as inputs {{mvar|A}}, {{mvar|b}}, {{mvar|c}}, an initial guess {{math|''x''<sup>0</sup> > 0}} that is strictly feasible (i.e., {{math|''Ax''<sup>0</sup> {{=}} ''b''}}), a tolerance {{mvar|ε}} and a stepsize {{mvar|β}}. It then proceeds by iterating{{r|vanderbei90}}{{rp|111}}\n\n* Let {{mvar|D<sub>k</sub>}} be the [[diagonal matrix]] with {{mvar|x<sup>k</sup>}} on its diagonal.\n* Compute a vector of [[Duality (optimization)#The linear case|dual]] variables:\n*: <math>w^k = (A D_k^2 A^\\operatorname{T})^{-1} A D_k^2 c.</math>\n* Compute a vector of ''reduced costs'', which measure the [[Slack variable|slackness]] of inequality constraints in the dual:\n*: <math>r^k = c - A^\\operatorname{T} w^k.</math>\n* If <math>r^k > 0</math> and <math>\\mathbf{1}^\\operatorname{T} D_k r^k < \\varepsilon</math>, the current solution {{math|''x<sup>k</sup>''}} is {{mvar|ε}}-optimal.\n* If <math>-D_k r^k \\ge 0</math>, the problem is unbounded.\n* Update <math>x^{k+1} = x^k - \\beta \\frac{D_k^2 r^k}{\\|D_k r^k\\|}</math>\n\n===Initialization===\nPhase I, the initialization, solves an auxiliary problem with an additional variable {{mvar|u}} and uses the result to derive an initial point for the original problem. Let {{math|''x''<sup>0</sup>}} be an arbitrary, strictly positive point; it need not be feasible for the original problem. The infeasibility of {{math|''x''<sup>0</sup>}} is measured by the vector\n\n:<math>v = b - Ax^0</math>.\n\nIf {{math|''v'' {{=}} 0}}, {{math|''x''<sup>0</sup>}} is feasible. If it is not, phase I solves the auxiliary problem\n\n:minimize {{math|''u''}}\n:subject to {{math|''Ax'' + ''uv'' {{=}} ''b''}}, {{math|''x'' ≥ 0}}, {{math|''u'' ≥ 0}}.\n\nThis problem has the right form for solution by the above iterative algorithm,{{efn|The structure in the auxiliary problem permits some simplification of the formulas.{{r|lpfe}}{{rp|344}}}} and\n\n:<math>\\begin{pmatrix} x^0 \\\\ 1 \\end{pmatrix}</math>\n\nis a feasible initial point for it. Solving the auxiliary problem gives\n\n:<math>\\begin{pmatrix} x^* \\\\ u^* \\end{pmatrix}</math>.\n\nIf {{math|''u''<sup>*</sup> {{=}} 0}}, then {{math|''x''<sup>*</sup> {{=}} 0}} is feasible in the original problem (though not necessarily strictly interior), while if {{math|''u''<sup>*</sup> > 0}}, the original problem is infeasible.{{r|lpfe}}{{rp|343}}\n\n==Analysis==\nWhile easy to state, affine scaling was found hard to analyze. Its convergence depends on the step size, {{mvar|β}}. For step sizes {{math|{{Var|β}} ≤ {{sfrac|2|3}}}}, Vanderbei's variant of affine scaling has been proven to converge, while for {{math|{{Var|β}} > 0.995}}, an example problem is known that converges to a suboptimal value.{{r|lpfe}}{{rp|342}} Other variants of the algorithm have been shown to exhibit [[Chaos theory|chaotic]] behavior even on small problems when {{math|{{Var|β}} > {{sfrac|2|3}}}}.<ref>{{cite journal |first1=H. |last1=Bruin |first2=R.J. |last2=Fokkink |first3=G. |last3=Gu |first4=C. |last4=Roos |title=On the chaotic behavior of the primal–dual affine–scaling algorithm for linear optimization |journal=[[Chaos (journal)|Chaos]] |year=2014 |doi=10.1063/1.4902900 |pmid=25554052 |url=http://www.mat.univie.ac.at/~bruin/papers/chaosopt.pdf |volume=24 |issue=4 |pages=043132|arxiv=1409.6108 |bibcode=2014Chaos..24d3132B }}</ref><ref>{{cite journal |first1=Ileana |last1=Castillo |first2=Earl R. |last2=Barnes |title=Chaotic Behavior of the Affine Scaling Algorithm for Linear Programming |year=2006 |journal=SIAM J. Optim. |volume=11 |issue=3 |pages=781–795 |doi=10.1137/S1052623496314070}}</ref>\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n* {{cite journal |first1=Ilan |last1=Adler |first2=Renato D. C. |last2=Monteiro |title=Limiting behavior of the affine scaling continuous trajectories for linear programming problems |journal=Mathematical Programming |volume=50 |issue=1–3 |year=1991 |pages=29–51 |url=http://www2.isye.gatech.edu/~monteiro/publications/tech_reports/waffine.pdf |doi=10.1007/bf01594923}}\n* {{cite journal |first=Romesh |last=Saigal |title=A simple proof of a primal affine scaling method |journal=Annals of Operations Research |volume=62 |year=1996 |pages=303–324 |url=http://deepblue.lib.umich.edu/bitstream/handle/2027.42/44263/10479_2005_Article_BF02206821.pdf |doi=10.1007/bf02206821}}\n* {{cite journal |first1=Paul |last1=Tseng |first2=Zhi-Quan |last2=Luo |title=On the convergence of the affine-scaling algorithm |journal=Mathematical Programming |volume=56 |issue=1–3 |year=1992 |pages=301–319 |url=http://dspace.mit.edu/bitstream/handle/1721.1/3161/P-1920-20783110.pdf |citeseerx=10.1.1.94.7852 |doi=10.1007/bf01580904|hdl=1721.1/3161 }}\n\n==External links==\n* {{cite web |title=15.093 Optimization Methods, Lecture 21: The Affine Scaling Algorithm |url=http://ocw.mit.edu/courses/sloan-school-of-management/15-093j-optimization-methods-fall-2009/lecture-notes/MIT15_093J_F09_lec21.pdf |website=[[MIT OpenCourseWare]] |year=2009}}\n* {{cite web |url=http://homepages.rpi.edu/~mitchj/handouts/interior_html/interior.html |first=John |last=Mitchell |title=Interior Point Methods |date=November 2010 |publisher=[[Rensselaer Polytechnic Institute]]}}\n* {{Cite web |url=http://ocw.nctu.edu.tw/course/lp992/Lecture6.pdf |title=Lecture 6: Interior point method |website=NCTU OpenCourseWare}}\n\n{{Optimization algorithms|convex}}\n\n[[Category:Linear programming]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Soviet inventions]]"
    },
    {
      "title": "Alpha–beta pruning",
      "url": "https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning",
      "text": "{{other uses2|Alphabeta}}\n{{Infobox algorithm\n|class=[[Search algorithm]]\n|image=\n|data=\n|time=<math>O(b^d)</math>\n|best-time=<math>O\\left(\\sqrt{b^d}\\right)</math>\n|average-time=\n|space=\n|optimal=\n|complete=\n}}\n\n{{Tree search algorithm}}\n\n'''Alpha–beta pruning''' is a [[search algorithm]] that seeks to decrease the number of nodes that are evaluated by the [[Minimax#Minimax algorithm with alternate moves|minimax algorithm]] in its [[game tree|search tree]]. It is an adversarial search algorithm used commonly for machine playing of two-player games ([[Tic-tac-toe]], [[Chess]], [[Go (board game)|Go]], etc.). It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.<ref name=\"RN10\">{{Cite book\n| first = Stuart J.\n| last = Russell\n| first2 = Peter\n| last2 = Norvig\n| title = Artificial Intelligence: A Modern Approach\n| url = http://aima.cs.berkeley.edu/\n| year = 2010\n| edition = 3rd\n| publisher = Pearson Education, Inc.\n| publication-place = Upper Saddle River, New Jersey\n| isbn = 0-13-604259-7\n| author-link=Stuart J. Russell\n| author2-link=Peter Norvig\n| page=167\n\n}}</ref>\n\n== History ==\n[[Allen Newell]] and [[Herbert A. Simon]] who used what [[John McCarthy (computer scientist)|John McCarthy]] calls an \"approximation\"<ref name=\"JMC\">{{cite web\n  | author = McCarthy, John\n  | title = Human Level AI Is Harder Than It Seemed in 1955\n  | date = 27 November 2006\n  | url = http://www-formal.stanford.edu/jmc/slides/wrong/wrong-sli/wrong-sli.html\n  | accessdate = 2006-12-20\n}}</ref> in 1958 wrote that alpha–beta \"appears to have been reinvented a number of times\".<ref name=NS>\n{{cite journal\n |author1      = Newell, Allen\n |author2      = Herbert A. Simon\n |title        = Computer Science as Empirical Inquiry: Symbols and Search\n |journal      = Communications of the ACM\n |volume       = 19\n |issue        = 3\n |pages        = 113\n |date         = March 1976\n |url          = http://archive.computerhistory.org/projects/chess/related_materials/text/2-3.Computer_science_as_empirical_inquiry/2-3.Computer_science_as_empirical_inquiry.newell_simon.1975.ACM.062303007.pdf\n |format       = PDF\n |accessdate   = 2006-12-21\n |doi          = 10.1145/360018.360022\n |archive-url  = https://www.webcitation.org/5PwLLXQSN?url=http://archive.computerhistory.org/projects/chess/related_materials/text/2-3.Computer_science_as_empirical_inquiry/2-3.Computer_science_as_empirical_inquiry.newell_simon.1975.ACM.062303007.pdf\n |archive-date = 2007-06-28\n |dead-url     = yes\n |df           = \n}}\n</ref> [[Arthur Samuel]] had an early version for a checkers simulation. Richards, Timothy Hart, [[Michael Levin]] and/or Daniel Edwards also invented alpha–beta independently in the [[United States]].<ref name=\"AIM30\">{{cite web\n  |author1=Edwards, D.J.  |author2=Hart, T.P.\n  | title = The Alpha–beta Heuristic (AIM-030)\n  | publisher = Massachusetts Institute of Technology\n  | date = 4 December 1961\n  | url = http://hdl.handle.net/1721.1/6098\n  | accessdate = 2006-12-21\n}}</ref> McCarthy proposed similar ideas during the [[Dartmouth workshop]] in 1956 and suggested it to a group of his students including [[Alan Kotok]] at MIT in 1961.<ref name=\"AIM41\">{{cite web | last=Kotok | first=Alan | title=MIT Artificial Intelligence Memo 41 | date=3 December 2004 | url=http://www.kotok.org/AI_Memo_41.html | accessdate=2006-07-01}}</ref> [[Alexander Brudno]] independently conceived the alpha–beta algorithm, publishing his results in 1963.<ref name=Marsland>{{cite web|author=[http://www.cs.ualberta.ca/~tony/ Marsland, T.A.] |title=Computer Chess Methods (PDF) from Encyclopedia of Artificial Intelligence. S. Shapiro (editor) |publisher=J. Wiley & Sons |date=May 1987 |pages=159–171 |url=http://www.cs.ualberta.ca/~tony/OldPapers/encyc.mac.pdf |format=PDF |accessdate=2006-12-21 |deadurl=yes |archiveurl=https://web.archive.org/web/20081030023047/http://www.cs.ualberta.ca/~tony/OldPapers/encyc.mac.pdf |archivedate=October 30, 2008 }}</ref> [[Donald Knuth]] and Ronald W. Moore refined the algorithm in 1975<ref name=Knuth-Moore>\n{{cite journal\n  | author1 = Knuth, D. E.\n  | author2 = Moore, R. W.\n  | title = An Analysis of Alpha–Beta Pruning\n  | journal = Artificial Intelligence\n  | volume = 6\n  | issue = 4\n  | year = 1975\n  | pages = 293–326\n  | url = http://www.fileserve.com/file/ZgR5t3j/An_Analysis_of_Alpha-Beta_Pruning.pdf\n  | doi = 10.1016/0004-3702(75)90019-3\n  }}{{Dead link|date=May 2019 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n:* Reprinted as Chapter 9 in {{cite book\n | last = Knuth\n | first = Donald E.\n | title = Selected Papers on Analysis of Algorithms\n | year = 2000\n | publisher = Stanford, California: Center for the Study of Language and Information – CSLI Lecture Notes, no. 102\n | url = http://www-cs-faculty.stanford.edu/~knuth/aa.html\n | isbn = 1-57586-212-3\n | oclc = 222512366\n | accessdate = 2016-03-13\n }}\n</ref><ref name=Abramson>{{cite journal|author=Abramson, Bruce |title=Control Strategies for Two-Player Games |journal=ACM Computing Surveys |date=June 1989 |url=http://www.theinformationist.com/pdf/constrat.pdf/ |accessdate=2008-08-20 |doi=10.1145/66443.66444 |volume=21 |issue=2 |page=137 |deadurl=yes |archiveurl=https://web.archive.org/web/20080820030859/http://www.theinformationist.com/pdf/constrat.pdf/ |archivedate=August 20, 2008 }}</ref>. [[Judea Pearl]] proved its optimality for trees with randomly assigned leaf values in terms of the expected running time in two papers.<ref name=\"Pearl1980\" /><ref name=\"Pearl1982\" /> The optimality of the randomized version of alpha-beta was shown by Michael Saks and Avi Wigderson in 1986.<ref name=\"SaksWigderson\" />\n\n== Core idea ==\nThe algorithm maintains two values, alpha and beta, which represent the minimum score that the maximizing player is assured of and the maximum score that the minimizing player is assured of respectively. Initially, alpha is negative infinity and beta is positive infinity, i.e. both players start with their worst possible score. Whenever the maximum score that the minimizing player (i.e. the \"beta\" player) is assured of becomes less than the minimum score that the maximizing player (i.e., the \"alpha\" player) is assured of (i.e. beta &le; alpha), the maximizing player need not consider further descendants of this node, as they will never be reached in the actual play.\n\n== Improvements over naive minimax ==\n[[File:AB pruning.svg|thumb|400px|An illustration of alpha–beta pruning. The grayed-out subtrees don’t need to be explored (when moves are evaluated from left to right), since we know the group of subtrees as a whole yields the value of an equivalent subtree or worse, and as such cannot influence the final result. The max and min levels represent the turn of the player and the adversary, respectively.]]\nThe benefit of alpha–beta pruning lies in the fact that branches of the search tree can be eliminated. This way, the search time can be limited to the 'more promising' subtree, and a deeper search can be performed in the same time. Like its predecessor, it belongs to the [[branch and bound]] class of algorithms. The optimization reduces the effective depth to slightly more than half that of simple minimax if the nodes are evaluated in an optimal or near optimal order (best choice for side on move ordered first at each node).\n\nWith an (average or constant) [[branching factor]] of ''b'', and a search depth of ''d'' [[Ply (game theory)|plies]], the maximum number of leaf node positions evaluated (when the move ordering is [[wiktionary:pessimal|pessimal]]) is [[Big O notation|''O'']](''b''×''b''×...×''b'') = ''O''(''b''<sup>''d''</sup>) – the same as a simple minimax search. If the move ordering for the search is optimal (meaning the best moves are always searched first), the number of leaf node positions evaluated is about ''O''(''b''×1×''b''×1×...×''b'') for odd depth and ''O''(''b''×1×''b''×1×...×1) for even depth, or <math>O(b^{d/2}) = O(\\sqrt{b^d})</math>. In the latter case, where the ply of a search is even, the effective branching factor is reduced to its [[square root]], or, equivalently, the search can go twice as deep with the same amount of computation.<ref name=\"RN03\">{{Russell Norvig 2003}}</ref> The explanation of ''b''×1×''b''×1×... is that all the first player's moves must be studied to find the best one, but for each, only the best second player's move is needed to refute all but the first (and best) first player move—alpha–beta ensures no other second player moves need be considered. When nodes are considered in a random order (i.e., the algorithm randomizes), asymptotically,\nthe expected number of nodes evaluated in uniform trees with binary leaf-values is <math>\\Theta( ((b-1+\\sqrt{b^2+14b+1})/4 )^d )</math>\n.<ref name=\"SaksWigderson\">{{cite book |last=Saks |first=M. |first2=A. |last2=Wigderson |chapter=Probabilistic Boolean Decision Trees and the Complexity of Evaluating Game Trees |title=27th Annual Symposium on Foundations of Computer Science |pages=29–38 |year=1986 |doi=10.1109/SFCS.1986.44 }}</ref>\nFor the same trees, when the values are assigned to the leaf values independently of each other and say zero and one are both equally probable, the expected number of nodes evaluated is <math>\\Theta( (b/2)^{d} )</math>, which is much smaller than the work done by the randomized algorithm, mentioned above, and is again optimal for such random trees.<ref name=\"Pearl1980\">{{cite journal |last=Pearl |first=Judea |title=Asymptotic Properties of Minimax Trees and Game-Searching Procedures |journal=[[Artificial Intelligence (journal)|Artificial Intelligence]] |volume=14 |issue=2 |year=1980 |pages=113–138 |doi=10.1016/0004-3702(80)90037-5 }}</ref> When the leaf values are chosen independently of each other but from the <math>[0,1]</math> interval uniformly at random, the expected number of nodes evaluated increases to <math>\\Theta( b^{d/log(d)} )</math> in the <math>d\\to\\infty</math> limit<ref name=\"Pearl1982\">{{cite journal |last=Pearl |first=Judea |title=The Solution for the Branching Factor of the Alpha-Beta Pruning Algorithm and Its Optimality |journal=Communications of the ACM |volume=25 |issue=8 |year=1982 |pages=559–64 |doi=10.1145/358589.358616 }}</ref>, which is again optimal for these kind random trees. Note that the actual work for \"small\" values of <math>d</math> is better approximated using <math>0.925 d^{0.747}</math>.<ref name=\"Pearl1982\" /><ref name=\"Pearl1980\" />\n\n[[File:Minmaxab.gif|thumb|400px|An animated pedagogical example that attempts to be human-friendly by substituting initial infinite (or arbitrarily large) values for emptiness and by avoiding using the [[negamax]] coding simplifications.]]\nNormally during alpha–beta, the subtrees are temporarily dominated by either a first player advantage (when many first player moves are good, and at each search depth the first move checked by the first player is adequate, but all second player responses are required to try to find a refutation), or vice versa. This advantage can switch sides many times during the search if the move ordering is incorrect, each time leading to inefficiency. As the number of positions searched decreases exponentially each move nearer the current position, it is worth spending considerable effort on sorting early moves. An improved sort at any depth will exponentially reduce the total number of positions searched, but sorting all positions at depths near the root node is relatively cheap as there are so few of them.  In practice, the move ordering is often determined by the results of earlier, smaller searches, such as through [[Iterative deepening depth-first search|iterative deepening]].\n\nAdditionally, this algorithm can be trivially modified to return an entire [[principal variation]] in addition to the score. Some more aggressive algorithms such as [[MTD(f)]] do not easily permit such a modification.\n\n== Pseudocode ==\nThe pseudo-code for depth limited minimax with alpha-beta pruning is as follows:<ref name=\"RN03\" />\n\n '''function''' alphabeta(node, depth, α, β, maximizingPlayer) '''is'''\n     '''if''' depth = 0 '''or''' node is a terminal node '''then'''\n         '''return''' the heuristic value of node\n     '''if''' maximizingPlayer '''then'''\n         value := &minus;∞\n         '''for each''' child of node '''do'''\n             value := max(value, alphabeta(child, depth &minus; 1, α, β, FALSE))\n             α := max(α, value)\n             '''if''' α ≥ β '''then'''\n                 '''break''' ''(* β cut-off *)''\n         '''return''' value\n     '''else'''\n         value := +∞\n         '''for each''' child of node '''do'''\n             value := min(value, alphabeta(child, depth &minus; 1, α, β, TRUE))\n             β := min(β, value)\n             '''if''' α ≥ β '''then'''\n                 '''break''' ''(* α cut-off *)''\n         '''return''' value\n\n ''(* Initial call *)''\n alphabeta(origin, depth, &minus;[[Infinity|∞]], +[[Infinity|∞]], TRUE)\n\nImplementations of alpha-beta pruning can often be delineated by whether they are \"fail-soft,\" or \"fail-hard\". The pseudo-code illustrates the fail-soft variation. With fail-soft alpha-beta, the alphabeta function may return values (v) that exceed (v < α or v > β) the α and β bounds set by its function call arguments. In comparison, fail-hard alpha-beta limits its function return value into the inclusive range of α and β.\n\n== Heuristic improvements ==\nFurther improvement can be achieved without sacrificing accuracy by using ordering [[heuristic]]s to search earlier parts of the tree that are likely to force alpha–beta cutoffs. For example, in chess, moves that capture pieces may be examined before moves that do not, and moves that have scored highly in [[Iterative deepening depth-first search|earlier passes]] through the game-tree analysis may be evaluated before others. Another common, and very cheap, heuristic is the [[killer heuristic]], where the last move that caused a beta-cutoff at the same tree level in the tree search is always examined first. This idea can also be generalized into a set of [[refutation table]]s.\n\nAlpha–beta search can be made even faster by considering only a narrow search window (generally determined by guesswork based on experience). This is known as ''aspiration search''. In the extreme case, the search is performed with alpha and beta equal; a technique known as ''[[MTD-f#Zero-Window Searches|zero-window search]]'', ''null-window search'', or ''scout search''. This is particularly useful for win/loss searches near the end of a game where the extra depth gained from the narrow window and a simple win/loss evaluation function may lead to a conclusive result. If an aspiration search fails, it is straightforward to detect whether it failed ''high'' (high edge of window was too low) or ''low'' (lower edge of window was too high). This gives information about what window values might be useful in a re-search of the position.\n\nOver time, other improvements have been suggested, and indeed the Falphabeta (fail-soft alpha-beta) idea of John Fishburn is nearly universal and is already incorporated above in a slightly modified form. Fishburn also suggested a combination of the killer heuristic and zero-window search under the name Lalphabeta (\"last move with minimal window alpha-beta search\").\n\n== Example implementation ==\n[[Stockfish (chess)]] is a [[C++]] open source chess program that implements the alpha-beta pruning algorithm.  Stockfish is a mature implementation that is rated as one of the strongest chess engines available today, and it won the [[Top Chess Engine Championship]] in 2016 and 2018.<ref>{{cite web|url=http://www.inwoba.de/bayes.html|title=IPON Rating List|date=6 June 2014|accessdate=31 October 2018|archive-url=https://web.archive.org/web/20140529051651/http://www.inwoba.de/bayes.html|archive-date=29 May 2014|dead-url=yes|df=dmy-all}}</ref><ref>{{cite web|title=Stockfish is the TCEC Season 9 Grand Champion|url=http://www.chessdom.com/stockfish-is-the-tcec-season-9-grand-champion/|website=Chessdom|accessdate=31 October 2018}}</ref><ref>{{cite web|title=Stockfish convincingly wins TCEC Season 11|url=http://www.chessdom.com/stockfish-convincingly-wins-tcec-season-11/|website=Chessdom|accessdate=31 October 2018}}</ref>\n\n== Other algorithms ==\nSince the minimax algorithm and its variants are inherently [[depth-first search|depth-first]], a strategy such as [[Iterative deepening depth-first search|iterative deepening]] is usually used in conjunction with alpha–beta so that a reasonably good move can be returned even if the algorithm is interrupted before it has finished execution. Another advantage of using iterative deepening is that searches at shallower depths give move-ordering hints, as well as shallow alpha and beta estimates, that both can help produce cutoffs for higher depth searches much earlier than would otherwise be possible.\n\nAlgorithms like [[SSS*]], on the other hand, use the [[best first search|best-first]] strategy.  This can potentially make them more time-efficient, but typically at a heavy cost in space-efficiency.<ref>{{citation|doi=10.1146/annurev.cs.02.060187.002315|title=Search techniques|journal=Annual Review of Computer Science|volume=2|pages=451–467|year=1987|first1=Judea|last1=Pearl|author1-link=Judea Pearl|first2=Richard|last2=Korf|quote=Like its A* counterpart for single-player games, SSS* is optimal in terms of the average number of nodes examined; but its superior pruning power is more than offset by the substantial storage space and bookkeeping required.}}</ref>\n\n== See also ==\n* [[Minimax]]\n* [[Expectiminimax]]\n* [[Negamax]]\n* [[Pruning (algorithm)]]\n* [[Branch and bound]]\n* [[Combinatorial optimization]]\n* [[Principal variation search]]\n* [[Transposition table]]\n\n== References ==\n{{Reflist}}\n\n== Bibliography ==\n* {{cite book |author1=George T. Heineman |author2=Gary Pollice |author3=Stanley Selkow | title= Algorithms in a Nutshell | publisher=[[Oreilly Media]] | year=2008 | chapter=Chapter 7: Path Finding in AI | pages = 217–223 | isbn=978-0-596-51624-6 }}\n* [[Judea Pearl]], ''Heuristics'', Addison-Wesley, 1984\n* {{cite book | author=John P. Fishburn | title= Analysis of Speedup in Distributed Algorithms (revision of 1981 PhD thesis) | publisher=UMI Research Press | year=1984 | chapter=Appendix A: Some Optimizations of α-β Search | pages = 107–111 | isbn=0-8357-1527-2 }}\n\n\n\n{{Game theory}}\n\n{{DEFAULTSORT:Alpha-Beta Pruning}}\n[[Category:Game artificial intelligence]]\n[[Category:Graph algorithms]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Search algorithms]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Auction algorithm",
      "url": "https://en.wikipedia.org/wiki/Auction_algorithm",
      "text": "The term \"'''auction algorithm'''\"<ref name=\"Bert79\">[[Dimitri P. Bertsekas]]. \"A distributed algorithm for the assignment problem\", [http://www.mit.edu/~dimitrib/Orig_Auction.pdf original paper, 1979].</ref> applies to several variations of a [[Optimization (mathematics)|combinatorial optimization]] [[algorithm]] which solves [[assignment problem]]s, and network optimization problems with linear and convex/nonlinear cost.  An ''auction algorithm'' has been used in a business setting to determine the best prices on a set of products offered to multiple buyers. It is an iterative procedure, so the name \"auction algorithm\" is related to a sales [[auction]], where multiple bids are compared to determine the best offer, with the final sales going to the highest bidders.\n\nThe original form of the auction algorithm is an iterative method to find the optimal prices and an assignment that maximizes the net benefit in a [[bipartite graph]], the ''[[maximum weight matching]] problem'' (MWM).<ref name=\"GBooksH\">M.G. Resende, P.M. Pardalos. \"Handbook of optimization in telecommunications\", [https://books.google.com/books?id=fp7N4Pk6TG4C, 2006]</ref><ref>M. Bayati, D. Shah, M. Sharma. \"A Simpler Max-Product Maximum Weight Matching Algorithm and the Auction Algorithm\", 2006, webpage PDF: [http://www.mit.edu/~devavrat/bpmwm2.pdf MIT-bpmwm-PDF].</ref>\nThis algorithm was first proposed by [[Dimitri Bertsekas]] in 1979. Detailed analysis and extensions to more general network optimization problems (ε-relaxation in 1986, and network auction in 1992) are provided in his network optimization books [http://web.mit.edu/dimitrib/www/net.html Linear Network Optimization] 1991, and [http://www.athenasc.com/netbook.html Network Optimization: Continuous and Discrete Models] 1998. The auction algorithm has excellent computational complexity, as given in these books, and is reputed to be among the fastest for solving single commodity network optimization problems. In addition, the original version of this algorithm is known to possess a distributed nature particularly suitable for distributed systems, since its basic computational primitives (bidding and auctioning) are localized rather than relying on queries of global information.<ref name=\"Bert79\"/> However, the original version that is intrinsically distributable has a pseudo-polynomial time complexity, which means that the running time depends on the input data pattern. Later versions have improved the time complexity to the state-of-the-art level by using techniques such as ''ε-scaling'' (also discussed in the original 1979 paper)<ref name=\"Bert90\">Dimitri P. Bertsekas. \"The auction algorithm for assignment and other network\nflow problems: A tutorial\". ''Interfaces'', 1990</ref> but at the sacrifice of undermining its distributed characteristics.  In order to retain the distributed nature and also attain a polynomial time complexity, recently some researchers from the multi-agent community have been trying to improve the earlier version of the auction algorithm by switching to a different economic model, namely, from the selfish bidders' perspective to a merchant’s point of view, where the merchant of a market adjusts the article prices in order to quickly clear the inventory.<ref name=\"Liu-RSS-13\">L. Liu, D. Shell. \"Optimal Market-based Multi-Robot Task Allocation via Strategic Pricing\", 2013. [http://www.roboticsproceedings.org/rss09/p33.pdf online PDF]</ref>\n\nThe ideas of the auction algorithm and ε-scaling<ref name=\"Bert79\"/> are also central in preflow-push algorithms for single commodity linear network flow problems. In fact the preflow-push algorithm for max-flow can be derived by applying the original 1979 auction algorithm to the max flow problem after reformulation as an assignment problem; see the  [http://web.mit.edu/dimitrib/www/net.html 1998 Network Optimization book], by Bertsekas, Section 7.3.3. Moreover, the preflow-push algorithm for the linear minimum cost flow problem is mathematically equivalent to the ε-relaxation method, which is obtained by applying the original auction algorithm after the problem is reformulated as an equivalent assignment problem.<ref name=\"Bert86\">\nDimitri P. Bertsekas. \"Distributed Relaxation Algorithms for Linear Network Flow Problems,\" Proc. of 25th IEEE CDC, Athens, Greece, 1986, pp. 2101-2106, online from IEEEXplore [http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=4049175&url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D4049175]</ref>\n\nA later variation of the auction algorithm that solves  [[shortest path problem]]s was introduced by Bertsekas in 1991.<ref name=\"Bert91\">\nDimitri P. Bertsekas. \"An auction algorithm for shortest paths\", ''SIAM Journal on Optimization'', 1:425—447, 1991,[http://citeseer.ist.psu.edu/bertsekas91auction.html PSU-bertsekas91auction]</ref>\nIt is a simple algorithm for finding shortest paths in a [[directed graph]]. In the single origin/single destination case, the auction algorithm maintains a single path starting at the origin, which is then extended or contracted by a single node at each iteration. Simultaneously, at most one dual variable will be adjusted at each iteration, in order to either improve or maintain the value of a dual function. In the case of multiple origins, the auction algorithm is well-suited for parallel computation.<ref name=Bert91/> The algorithm is closely related to auction algorithms for other network flow problems.<ref name=Bert91/> According to computational experiments, the auction algorithm is generally inferior to other state-of-the-art algorithms for the all destinations shortest path problem, but is very fast for problems with few destinations (substantially more than one and substantially less than the total number of nodes); see the article by Bertsekas, Pallottino, and Scutella, [http://www.springerlink.com/content/r5717w511x222053/ Polynomial Auction Algorithms for Shortest Paths].\n\nAuction algorithms for shortest hyperpath problems have been defined by De Leone and Pretolani in 1998. This is also a parallel auction algorithm for weighted bipartite matching, described by E. Jason Riedy in 2004.<ref name=\"BerkPA\">\"The Parallel Auction Algorithm for Weighted Bipartite Matching\", E. Jason Riedy, UC Berkeley, February 2004, [http://www.cs.berkeley.edu/~ejr/tmp/pres-pp04-draft.pdf Berkeley-para4-PDF]{{Dead link|date=October 2018 |bot=InternetArchiveBot |fix-attempted=yes }}.</ref>\n\n==Comparisons==\nThe (sequential) auction algorithms for the shortest path problem have been the subject of experiments which have been reported in technical papers.<ref name=DTUauc/> Experiments clearly show that the auction algorithm is inferior to the state-of-the-art shortest-path algorithms for finding the optimal solution of single-origin to all-destinations problems.<ref name=\"DTUauc\">{{cite journal\n |title   = Experiments with the auction algorithm for the shortest path problem\n |first1  = Jesper\n |last1   = Larsen\n |first2  = Ib\n |last2   = Pedersen\n |journal = Nordic Journal of Computing\n |volume  = 6\n |number  = 4\n |year    = 1999\n |issn    = 1236-6064\n |pages   = 403–42\n |url     = http://portal.acm.org/citation.cfm?id=642163\n}}, see also [http://www.diku.dk/OLD/publikationer/tekniske.rapporter/rapporter/97-07.pdf A note on the practical performance of the auction algorithm for the shortest path] {{Webarchive|url=https://web.archive.org/web/20110605004126/http://www.diku.dk/OLD/publikationer/tekniske.rapporter/rapporter/97-07.pdf |date=2011-06-05 }} (1997) by the first author.</ref>\n\nAlthough with the auction algorithm the total benefit is [[Monotonic function|monotonically increasing]] with each iteration, in the ''[[Hungarian algorithm]]'' (from Kuhn, 1955; Munkres, 1957) the total benefit strictly increases with each iteration.\n\nThe auction algorithm of Bertsekas for finding shortest paths within a directed graph is reputed to perform very well on random graphs and on problems with few destinations.<ref name=Bert91/>\n\n==See also==\n* [[Hungarian algorithm]]\n\n==References==\n\n<references/>\n\n==External links==\n* Dimitri P. Bertsekas. \"Linear Network Optimization\", MIT Press, 1991, [http://web.mit.edu/dimitrib/www/net.html on-line].\n* Dimitri P. Bertsekas. \"Network Optimization: Continuous and Discrete Models\",  [http://www.athenasc.com/netbook.html Athena Scientific, 1998].\n* Dimitri P. Bertsekas. \"An auction algorithm for shortest paths\", ''SIAM Journal on Optimization'', 1:425—447, 1991, webpage: [http://citeseer.ist.psu.edu/bertsekas91auction.html PSU-bertsekas91auction].\n* D.P. Bertsekas, S. Pallottino, M. G. Scutella. \"Polynomial Auction Algorithms for Shortest Paths,\" [http://www.springerlink.com/content/r5717w511x222053/ ,  Computational Optimization and Applications, Vol. 4, 1995, pp. 99-125].\n* Implementation of Bertsekas' Auction algorithm in Matlab by Florian Bernard, webpage: [http://de.mathworks.com/matlabcentral/fileexchange/48448-fast-linear-assignment-problem-using-auction-algorithm--mex- Matlab File Exchange].\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Augmented Lagrangian method",
      "url": "https://en.wikipedia.org/wiki/Augmented_Lagrangian_method",
      "text": "'''Augmented Lagrangian methods''' are a certain class of [[algorithm]]s for solving [[Constraint (mathematics)|constrained]] [[optimization (mathematics)|optimization]] problems.  They have similarities to [[penalty method]]s in that they replace a constrained optimization problem by a series of unconstrained problems and add a penalty term to the [[objective function|objective]]; the difference is that the augmented Lagrangian method adds yet another term, designed to mimic a [[Lagrange multiplier]].  The augmented Lagrangian is not the same as the [[Lagrange multiplier|method of Lagrange multipliers]].\n\nViewed differently, the unconstrained objective is the [[Lagrange multipliers#The strong Lagrangian principle: Lagrange duality|Lagrangian]] of the constrained problem, with an additional penalty term (the '''augmentation''').\n\nThe method was originally known as the '''method of multipliers''', and was studied much in the 1970 and 1980s as a good alternative to penalty methods. It was first discussed by [[Magnus Hestenes]] in 1969<ref>[[Magnus Hestenes|M.R. Hestenes]], \"Multiplier and gradient methods\", ''Journal of Optimization Theory and Applications'', 4, 1969, pp. 303–320</ref> and by [[Michael J. D. Powell|Powell]] in 1969.<ref>M.J.D. Powell, \"A method for nonlinear constraints in minimization problems\", in ''Optimization'' ed. by R. Fletcher, Academic Press, New York, NY, 1969, pp. 283–298.</ref> The method was studied by [[R. Tyrrell Rockafellar]] in relation to [[Fenchel duality]], particularly in relation to proximal-point methods, [[Moreau–Yosida regularization]], and [[monotone operator|maximal monotone operator]]s: These methods were used in [[structural engineering|structural optimization]]<!-- French applied mathematicians, at least one in Texas, whose names escape senile K.W. -->.  The method was also studied by [[Dimitri Bertsekas]], notably in his 1982 book,<ref>Dimitri P. Bertsekas, ''Constrained optimization and Lagrange multiplier methods'', Athena Scientific, 1996 (first published 1982)</ref> together with extensions involving nonquadratic regularization functions, such as [[Bregman divergence|entropic regularization]], which gives rise to the \"exponential method of multipliers,\" a method that handles inequality constraints with a twice differentiable augmented Lagrangian function.\n\nSince the 1970s, [[sequential quadratic programming]] (SQP) and [[interior point method]]s (IPM) have had increasing attention, in part because they more easily use [[sparse matrix]] [[subroutine]]s from [[numerical linear algebra|numerical]] [[numerical software|software libraries]], and in part because IPMs have proven complexity results via the theory of [[self-concordant function]]s. The augmented Lagrangian method was rejuvenated by the optimization systems [[Galahad library|LANCELOT]] and [[AMPL]], which allowed sparse matrix techniques to be used on seemingly dense but \"partially separable\" problems. The method is still useful for some problems.<ref name=\"Nocedal 2006\">{{harvtxt|Nocedal|Wright|2006}}, chapter 17</ref>\nAround 2007, there was a resurgence of augmented Lagrangian methods in fields such as [[Total variation denoising|total-variation denoising]] and [[compressed sensing]].\nIn particular, a variant of the standard augmented Lagrangian method that uses partial updates (similar to the [[Gauss-Seidel method]] for solving linear equations) known as the '''[[Augmented Lagrangian method#Alternating direction method of multipliers|alternating direction method of multipliers]]''' or '''ADMM''' gained some attention.\n\n== General method ==\n\nLet us say we are solving the following constrained problem:\n:<math> \\min f(\\mathbf{x}) </math>\nsubject to\n:<math> c_i(\\mathbf{x}) = 0 ~\\forall  i \\in I. </math>\n\nThis problem can be solved as a series of unconstrained minimization problems.  For reference, we first list the [[penalty method]] approach:\n:<math> \\min \\Phi_k (\\mathbf{x}) = f (\\mathbf{x}) + \\mu_k ~ \\sum_{i\\in I} ~ c_i(\\mathbf{x})^2 </math>\nThe penalty method solves this problem, then at the next iteration it re-solves the problem\nusing a larger value of <math>\\mu_k</math> (and using the old solution as the initial guess or \"warm-start\").\n\nThe augmented Lagrangian method uses the following unconstrained objective:\n:<math> \\min \\Phi_k (\\mathbf{x}) = f (\\mathbf{x}) + \\frac{\\mu_k}{2} ~ \\sum_{i\\in I} ~ c_i(\\mathbf{x})^2  - \\sum_{i\\in I} ~ \\lambda_i c_i(\\mathbf{x})</math>\nand after each iteration, in addition to updating <math>\\mu_k</math>, the variable <math>\\lambda</math> is also updated according to the rule\n:<math>\\lambda_i \\leftarrow \\lambda_i - \\mu_k c_i(\\mathbf{x}_k) </math>\nwhere <math>\\mathbf{x}_k</math> is the solution to the unconstrained problem at the ''k''th step, i.e. <math>\\mathbf{x}_k=\\text{argmin} \\Phi_k(\\mathbf{x}) </math>\n\nThe variable <math>\\lambda</math> is an estimate of the [[Lagrange multiplier]], and the accuracy of this estimate improves at every step.  The major advantage of the method is that unlike the [[penalty method]], it is not necessary to take <math>\\mu \\rightarrow \\infty</math> in order to solve the original constrained problem.  Instead, because of the presence of the Lagrange multiplier term, <math>\\mu</math> can stay much smaller, thus avoiding ill-conditioning.<ref name=\"Nocedal 2006\"/>\n\nThe method can be extended to handle inequality constraints.  For a discussion of practical improvements, see.<ref name=\"Nocedal 2006\"/>\n\n== Alternating direction method of multipliers ==\nThe alternating direction method of multipliers (ADMM) is a variant of the augmented Lagrangian scheme that uses partial updates for the dual variables. This method is often applied to solve problems such as\n\n<math> \\min_x f(x) + g(x). </math>\n\nThis is equivalent to the constrained problem\n\n<math> \\min_{x,y} f(x) + g(y), \\quad \\text{subject to}\\quad  x = y. </math>\n\nThough this change may seem trivial, the problem can now be attacked using methods of constrained optimization (in particular, the augmented Lagrangian method), and the objective function is separable in ''x'' and ''y''.  The dual update requires solving a proximity function in ''x'' and ''y'' at the same time; the ADMM technique allows this problem to be solved approximately by first solving for ''x'' with ''y'' fixed, and then solving for ''y'' with ''x'' fixed. Rather than iterate until convergence (like the [[Jacobi method]]), the algorithm proceeds directly to updating the dual variable and then repeating the process.  This is not equivalent to the exact minimization, but surprisingly, it can still be shown that this method converges to the right answer (under some assumptions). Because of this approximation, the algorithm is distinct from the pure augmented Lagrangian method.\n\nThe ADMM can be viewed as an application of the [[Douglas-Rachford splitting algorithm]], and the Douglas-Rachford algorithm is in turn an instance of the [[Proximal point algorithm]]; details can be found here.<ref>{{Cite journal | last1 = Eckstein | first1 = J. | last2 = Bertsekas | first2 = D. P. | doi = 10.1007/BF01581204 | title = On the Douglas—Rachford splitting method and the proximal point algorithm for maximal monotone operators | journal = Mathematical Programming | volume = 55 | issue = 1–3 | pages = 293–318 | year = 1992 | pmid =  | pmc = | citeseerx = 10.1.1.141.6246 }}</ref>  There are several modern software packages that solve [[Basis pursuit]] and variants and use the ADMM; such packages include [http://yall1.blogs.rice.edu/ YALL1] (2009), [http://www.lx.it.pt/~mtf/SpaRSA/ SpaRSA] (2009) and [http://cascais.lx.it.pt/~mafonso/salsa.html SALSA] (2009). There are also packages that use the ADMM to solve more general problems, some of which can exploit multiple computing cores  [http://snap.stanford.edu/snapvx/ SNAPVX] (2015), [https://github.com/parADMM/engine parADMM] (2016).\n\n==Stochastic optimization==\n{{Advert section|date=April 2019}}\nStochastic optimization considers the problem of minimizing a loss function with access to noisy samples of the (gradient of the) function. The goal is to have an estimate of the optimal parameter (minimizer) per new sample.\nADMM is originally a batch method. However, with some modifications it can also be used for stochastic optimization. Since in stochastic setting we only have access to noisy samples of gradient, we use an inexact approximation of the Lagrangian as\n\n<math>\n\\hat{\\mathcal{L}}_{\\rho,k} = f_1(x_k)+\\langle \\nabla f(x_k,\\zeta_{k+1}),x \\rangle+g(y)-z^T (Ax + By - c)+\\frac{\\rho}{2} \\Vert Ax + By - c \\Vert^2+\\frac{\\Vert x-x_k \\Vert^2}{2\\eta_{k+1}},\n</math>\n\nwhere <math> \\eta_{k+1}</math> is a time-varying step size.<ref>{{cite journal|author=Ouyang, H.|author2=He, N.|author3=Tran, L.|author4=Gray, A. G|last-author-amp=yes|title=Stochastic alternating direction method of multipliers|journal=Proceedings of the 30th International Conference on Machine Learning|date=2013|pages=80–88}}</ref>\n\nThe alternating direction method of multipliers\n(ADMM)  is a popular method for online and distributed optimization on a large scale,<ref>{{cite journal|author=Boyd, S.|author2=Parikh, N.|author3=Chu, E.|author4=Peleato, B.|author5=Eckstein, J.|last-author-amp=yes|title=Distributed optimization and statistical learning via the alternating direction method of multipliers|journal=Foundations and Trends{\\textregistered} in Machine Learning|date=2011|volume=3|issue=1|pages=1–122|doi=10.1561/2200000016|citeseerx=10.1.1.360.1664}}</ref> and is employed in many applications, e.g.<ref>{{cite arXiv|author=Wahlberg, B. |author2=Boyd, S. |author3=Annergren, M. |author4=Wang, Y.|title=An ADMM algorithm for a class of total variation regularized estimation problems |eprint=1203.1828|date=2012|class=stat.ML }}</ref><ref>{{cite journal|author=Esser, E. |author2=Zhang, X. |author3=Chan, T.|title=A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science|journal=SIAM Journal on Imaging Sciences|date=2010|volume=3|issue=4}}</ref><ref>{{cite journal|author=Mota, J. FC |author2=Xavier, J. MF |author3=Aguiar, P. MQ |author4=Puschel, M.|title=Distributed ADMM for model predictive control and congestion control|journal=Decision and Control (CDC), 2012 IEEE 51st Annual Conference O|date=2012}}</ref>\nADMM is often applied to solve regularized problems, where the function optimization and regularization can be carried out locally, and then coordinated globally via constraints.\nRegularized optimization problems are especially relevant in the high dimensional regime since regularization is a natural mechanism to overcome ill-posedness and to encourage parsimony in the optimal solution, e.g., sparsity and low rank. Due to the efficiency of ADMM in solving regularized problems, it has a good potential for stochastic optimization in high dimensions. However, conventional stochastic ADMM methods suffer from curse of dimensionality. Their convergence rate is proportional to square of the dimension and in practice they scale poorly. See figure\n[http://newport.eecs.uci.edu/anandkumar/Lab/Lab_sub/Projects_sub/Reason.png REASON vs Stochastic ADMM]\n\nRecently, a general framework has been proposed for stochastic optimization in high-dimensions that solves this bottleneck by adding simple and cheap modifications to ADMM.,<ref name=\"reason-nips\">{{cite journal|last1=Sedghi|first1=Hanie|last2=Anandkumar|first2=Anima|last3=Jonckheere|first3=Edmond|title=Multi-step stochastic ADMM in high dimensions: Applications to sparse optimization and matrix decomposition|journal=Advances in Neural Information Processing Systems|date=2014|pages=2771–2779}}</ref><ref>{{cite arXiv |author=Sedghi, Hanie |author2=Anandkumar, Anima |author3=Jonckheere, Edmond|title=Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Noisy Matrix Decomposition |eprint=1402.5131|date=2014|class=cs.LG }}</ref> The method is called [[REASON]] (Regularized Epoch-based Admm for Stochastic Optimization in high-dimensioN). The modifications are in terms of added projection which goes a long way and results in logarithmic dimension dependency. REASON can be performed on any regularized optimization with any number of regularizers. The specific cases of sparse optimization framework and noisy decomposition framework are discussed further. In both cases, REASON obtains minimax optimal convergence rate. REASON provides the first online guarantees for noisy matrix decomposition. Experiment results show that in aforementioned cases, REASON outperforms state-of-the-art.\n\n==Alternative approaches==\n* [[Sequential quadratic programming]]\n* [[Sequential linear programming]]\n* [[Sequential linear-quadratic programming]]\n\n== Software ==\nOpen source and non-free/commercial implementations of the augmented Lagrangian method:\n* [[Accord.NET]] (C# implementation of augmented Lagrangian optimizer)\n* [[ALGLIB]] (C# and C++ implementations of preconditioned augmented Lagrangian solver)\n* [[PENOPT|PENNON]] (GPL 3, commercial license available)\n* [[Galahad library|LANCELOT]] (free \"internal use\" license, paid commercial options)\n* [[MINOS (optimization software)|MINOS]] (also uses an augmented Lagrangian method for some types of problems).\n* The code for Apache 2.0 licensed [[REASON]] is available online.<ref>{{cite web|title=REASON code|url=https://bitbucket.org/megaDataLab/reason2}}</ref>\n\n== See also ==\n* [[Penalty method]]\n* [[Interior point method]]\n* [[Barrier function]]\n* [[Lagrange multiplier]]\n\n==References==\n<references/>\n\n==Bibliography==\n* {{Citation | last1=Bertsekas | first1=Dimitri P. | title=Nonlinear Programming | publisher=[[Athena Scientific]] | location=Belmont, Mass | edition=2nd | isbn=978-1-886529-00-7 | year=1999}}\n* {{Citation | last1=Nocedal | first1=Jorge | last2=Wright | first2=Stephen J. | title=Numerical Optimization | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-30303-1 | year=2006}}\n\n{{optimization algorithms|constrained}}\n\n{{DEFAULTSORT:Augmented Lagrangian Method}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Automatic label placement",
      "url": "https://en.wikipedia.org/wiki/Automatic_label_placement",
      "text": "'''Automatic label placement''', sometimes called '''text placement''' or '''name placement''', comprises the computer methods of placing labels automatically on a map or chart. This is related to the [[Labeling (map design)|typographic design of such labels]].\n\nThe typical features depicted on a geographic [[map]] are line features (e.g. roads), area features (countries, parcels, forests, lakes, etc.), and point features (villages, cities, etc.). In addition to depicting the map's features in a geographically accurate manner, it is of critical importance to place the names that identify these features, in a way that the reader knows instantly which name describes which feature.\n\nAutomatic text placement is one of the most difficult, complex, and time-consuming problems in mapmaking and [[Geographic information system|GIS (Geographic Information System)]]. Other kinds of computer-generated graphics – like [[chart]]s, [[graph of a function|graph]]s etc. – require good placement of labels as well, not to mention engineering drawings, and professional programs which produce these drawings and charts, like [[spreadsheets]] (e.g. [[Microsoft Excel]]) or computational software programs (e.g. [[Mathematica]]).\n\nNaively placed labels overlap excessively, resulting in a map that is difficult or even impossible to read. Therefore, a GIS must allow a few possible placements of each label, and often also an option of resizing, rotating, or even removing (suppressing) the label. Then, it selects a set of placements that results in the least overlap, and has other desirable properties. For all but the most trivial setups, the problem is [[NP-hard]].\n\n==Rule-based algorithms==\nRule-based algorithms try to emulate an experienced human cartographer. Over centuries, cartographers have developed the art of mapmaking and label placement. For example, an experienced cartographer repeats road names several times for long roads, instead of placing them once, or in the case of Ocean City depicted by a point very close to the shore, the cartographer would place the label \"Ocean City\" over the land to emphasize that it is a coastal town.<ref>{{cite book|last=Slocum|first=Terry|title=Thematic Cartography and Geovisualization|year=2010|publisher=Pearson|location=Upper Saddle River, NJ|isbn=0-13-801006-4|pages=576|url=https://www.amazon.de/dp/0138010064}}</ref>\n\nCartographers work based on accepted conventions and rules and they place labels in order of importance. For example, New York City, Vienna, Berlin, Paris, or Tokyo must show up on country maps because they are high-priority labels. Once those are placed, the cartographer places the next most important class of labels, for example major roads, rivers, and other large cities. In every step they ensure that (1) the text is placed in a way that the reader easily associates it with the feature, and (2) the label does not overlap with those already placed on the map.\n\n==Local optimization algorithms==\nThe simplest [[greedy algorithm]] places consecutive labels on the map in positions that result in minimal overlap of labels. Its results are not perfect even for very simple problems, but it is extremely fast.\n\nSlightly more complex algorithms rely on local optimization to reach a local optimum of a placement evaluation function – in each iteration placement of a single label is moved to another position, and if it improves the result, the move is preserved. It performs reasonably well for maps that are not too densely labelled. Slightly more complex variations try moving 2 or more labels at the same time. The algorithm ends after reaching some local optimum.\n\nA simple algorithm – [[simulated annealing]] – yields good results with relatively good performance. It works like local optimization, but it may keep a change even if it worsens the result. The chance of keeping such a change is <math>\\exp \\frac{-\\Delta E}{T}</math>, where <math>\\Delta E</math> is the change in the evaluation function, and <math>T</math> is the ''temperature''. The temperature is gradually lowered according to the ''annealing schedule''. When the temperature is high, simulated annealing performs almost random changes to the label placement, being able to escape a [[local optimum]]. Later, when hopefully a very good local optimum has been found, it behaves in a manner similar to local optimization. The main challenges in developing a simulated annealing solution are choosing a good evaluation function and a good annealing schedule. Generally too fast cooling will degrade the solution, and too slow cooling will degrade the performance, but the schedule is usually quite a complex algorithm, with more than just one parameter.\n\nAnother class of direct search algorithms are the various [[evolutionary algorithm]]s, e.g. [[genetic algorithm]]s.\n\n==Divide-and-conquer algorithms==\nOne simple optimization that is important on real maps is dividing a set of labels into smaller sets that can be solved independently. Two labels are ''rivals'' if they can overlap in one of the possible placements. [[transitive relation|Transitive]] closure of this relation divides the set of labels into possibly much smaller sets. On uniformly and densely labelled maps, usually the single set will contain the majority of labels, and on maps for which the labelling is not uniform it may bring very big performance benefits. For example, when labelling a map of the world, [[United States|America]] is labelled independently from [[Eurasia]] etc.\n\n==2-satisfiability algorithms==\nIf a map labeling problem can be reduced to a situation in which each remaining label has only two potential positions in which it can be placed, then it may be solved efficiently by using an instance of [[2-satisfiability]] to find a placement avoiding any conflicting pairs of placements; several exact and approximate label placement algorithms for more complex types of problems are based on this principle.<ref>{{citation|first1=Srinivas|last1=Doddi|first2=Madhav V.|last2=Marathe|first3=Andy|last3=Mirzaian|first4=Bernard M. E.|last4=Moret|first5=Binhai|last5=Zhu|contribution=Map labeling and its generalizations|title=Proc. 8th ACM-SIAM Symp. Discrete Algorithms (SODA)|year=1997|pages=148–157|url=http://portal.acm.org/citation.cfm?id=314250}}; {{citation|first1=M.|last1=Formann|first2=F.|last2=Wagner|contribution=A packing problem with applications to lettering of maps|title=Proc. 7th ACM Symp. Computational Geometry|year=1991|pages=281–288}}; {{citation|first1=Chung Keung|last1=Poon|first2=Binhai|last2=Zhu|first3=Francis|last3=Chin|author3-link=Y. L. Chin|title=A polynomial time solution for labeling a rectilinear map|journal=Information Processing Letters|volume=65|issue=4|year=1998|pages=201–207|doi=10.1016/S0020-0190(98)00002-7}}; {{citation|first1=Frank|last1=Wagner|first2=Alexander|last2=Wolff|title=A practical map labeling algorithm|journal=[[Computational Geometry (journal)|Computational Geometry: Theory and Applications]]|volume=7|issue=5–6|year=1997|pages=387–404|doi=10.1016/S0925-7721(96)00007-7}}.</ref>\n\n==Other algorithms==\nAutomatic label placement algorithms can use any of the algorithms for finding the [[maximum disjoint set]] from the set of potential labels. Other algorithms can also be used, like various graph solutions, [[integer programming]] etc.\n\n==Notes==\n{{reflist}}\n\n==References==\n* Imhof, E., “Die Anordnung der Namen in der Karte,” Annuaire International de Cartographie II, Orell-Füssli Verlag, Zürich, 93–129, 1962.\n* Freeman, H., Map data processing and the annotation problem, Proc. 3rd Scandinavian Conf. on Image Analysis, Chartwell-Bratt Ltd. Copenhagen, 1983.\n* Ahn, J. and Freeman, H., “A program for automatic name placement,” Proc. AUTO-CARTO 6, Ottawa, 1983. 444–455.\n* Freeman, H., “Computer Name Placement,” ch. 29, in Geographical Information Systems, 1, D.J. Maguire, M.F. Goodchild, and D.W. Rhind, John Wiley, New York, 1991, 449–460.\n* Podolskaya N. N. Automatic Label De-Confliction Algorithms for Interactive Graphics Applications. Information technologies (ISSN 1684-6400), 9, 2007, p.&nbsp;45–50. In Russian: Подольская Н.Н. Алгоритмы автоматического отброса формуляров для интерак тивных графических приложений. Информационные технологии, 9, 2007, с. 45–50.\n\n==External links==\n* [http://i11www.iti.uni-karlsruhe.de/~awolff/map-labeling/  Alexander Wolff's Map Labeling Site]\n* [http://i11www.iti.uni-karlsruhe.de/~awolff/map-labeling/bibliography/ The Map-Labeling Bibliography]\n* [http://www.cs.uu.nl/docs/vakken/gd/steven2.pdf Label placement]\n* [http://www.eecs.harvard.edu/shieber/Biblio/Papers/tog-final.pdf An Empirical Study of Algorithms for Point-Feature Label Placement]\n\n{{DEFAULTSORT:Automatic Label Placement}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Geographic information systems]]"
    },
    {
      "title": "Bacterial colony optimization",
      "url": "https://en.wikipedia.org/wiki/Bacterial_colony_optimization",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Optimization algorithm}}\n{{Use mdy dates|date = March 2019}}\n\nThe '''bacterial colony optimization''' algorithm is an [[optimization algorithm]] which is based on a lifecycle model that simulates some typical behaviors of [[Escherichia coli|E. coli]] bacteria during their whole lifecycle, including [[chemotaxis]], communication, elimination, reproduction, and migration.<ref>{{cite journal|last1=Niu|first1=Ben|last2=Wang|first2=Hong|title=Bacterial Colony Optimization|journal=Discrete Dynamics in Nature and Society|date=2012|volume=2012|doi=10.1155/2012/698057|url=https://www.hindawi.com/journals/ddns/2012/698057/|accessdate=31 December 2016}}</ref>\n\n==References==\n{{reflist}}\n\n{{bacteria-stub}}\n\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Escherichia coli]]"
    },
    {
      "title": "Basin-hopping",
      "url": "https://en.wikipedia.org/wiki/Basin-hopping",
      "text": "In applied mathematics, '''Basin-hopping''' is a [[global optimization]] technique that iterates by performing random perturbation of coordinates, performing [[Local search (optimization)|local optimization]], and accepting or rejecting new coordinates based on a minimized function value.<ref>{{Cite web|url=https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html#scipy.optimize.basinhopping|title=scipy.optimize.basinhopping — SciPy v1.0.0 Reference Guide|website=docs.scipy.org|access-date=2018-04-20}}</ref> The algorithm was described in 1997 by [[David J. Wales]] and [[Jonathan Doye]].<ref>{{Cite journal|last=Wales|first=David J.|last2=Doye|first2=Jonathan P. K.|date=1997-07-10|title=Global Optimization by Basin-Hopping and the Lowest Energy Structures of Lennard-Jones Clusters Containing up to 110 Atoms|journal=The Journal of Physical Chemistry A|language=en|volume=101|issue=28|pages=5111–5116|doi=10.1021/jp970984n|arxiv=cond-mat/9803344|bibcode=1997JPCA..101.5111W}}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Randomized algorithms]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Benson's algorithm",
      "url": "https://en.wikipedia.org/wiki/Benson%27s_algorithm",
      "text": "{{distinguish|text=[[Benson's algorithm (Go)]], a method to find the unconditionally alive stones in the game [[Go (game)|Go]]}}\n\n'''Benson's algorithm''', named after [[Harold Benson]], is a method for solving [[multi-objective linear programming]] problems and vector linear programs.  This works by finding the \"efficient extreme points in the outcome set\".<ref name=\"Benson\">{{cite journal | author = Harold P. Benson | year = 1998 | title = An Outer Approximation Algorithm for Generating All Efficient Extreme Points in the Outcome Set of a Multiple Objective Linear Programming Problem | journal = Journal of Global Optimization | volume = 13 | issue = 1 | pages = 1–24 | doi = 10.1023/A:1008215702611 }}</ref>  The primary concept in Benson's algorithm is to evaluate the upper image of the [[vector optimization]] problem by [[cutting-plane method|cutting planes]].<ref name=\"Lohne\">{{cite book|title=Vector Optimization with Infimum and Supremum|author=Andreas Löhne|publisher=Springer|year=2011|isbn=9783642183508|pages=162–169}}</ref>\n\n== Idea of algorithm ==\nConsider a vector linear program\n:<math>\\min_C Px \\; \\text{ subject to }  A x \\geq b</math>\nfor <math>P \\in \\mathbb{R}^{q \\times n}</math>, <math>A \\in \\mathbb{R}^{m \\times n}</math>, <math>b \\in \\mathbb{R}^m</math> and a polyhedral convex ordering cone <math>C</math> having nonempty interior and containing no lines. The feasible set is <math>S=\\{x \\in \\mathbb{R}^n:\\; A x \\geq b\\}</math>. In particular, Benson's algorithm finds the [[extreme point]]s of the set <math>P[S] + C</math>, which is called upper image.<ref name=\"Lohne\"/>\n\nIn case of <math>C=\\mathbb{R}^q_+:=\\{y \\in \\mathbb{R}^q : y_1 \\geq 0,\\dots, y_q \\geq 0\\}</math>, one obtains the special case of a multi-objective linear program ([[multiobjective optimization]]).\n\n== Dual algorithm ==\nThere is a dual variant of Benson's algorithm,<ref name=\"EhrgottLöhne2011\">{{cite journal|last1=Ehrgott|first1=Matthias|last2=Löhne|first2=Andreas|last3=Shao|first3=Lizhen|title=A dual variant of Benson's \"outer approximation algorithm\" for multiple objective linear programming|journal=Journal of Global Optimization|volume=52|issue=4|year=2011|pages=757–778|issn=0925-5001|doi=10.1007/s10898-011-9709-y}}</ref> which is based on geometric duality<ref name=\"HeydeLöhne2008\">{{cite journal|last1=Heyde|first1=Frank|last2=Löhne|first2=Andreas|title=Geometric Duality in Multiple Objective Linear Programming|journal=SIAM Journal on Optimization|volume=19|issue=2|year=2008|pages=836–845|issn=1052-6234|doi=10.1137/060674831|url=http://webdoc.sub.gwdg.de/ebook/serien/e/reports_Halle-Wittenberg_math/06-15report.pdf}}</ref> for multi-objective linear programs.\n\n== Implementations ==\nBensolve - a free VLP solver\n* [http://bensolve.org www.bensolve.org]\nInner\n* [https://github.com/lcsirmaz/inner Link to github]\n\n== References ==\n{{Reflist}}\n\n[[Category:Linear programming]]\n[[Category:Optimization algorithms and methods]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Berndt–Hall–Hall–Hausman algorithm",
      "url": "https://en.wikipedia.org/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm",
      "text": "The '''Berndt–Hall–Hall–Hausman''' ('''BHHH''') '''algorithm''' is a [[numerical optimization]] [[algorithm]] similar to the [[Newton's method in optimization|Newton–Raphson algorithm]], but it replaces the observed negative [[Hessian matrix]] with the [[outer product]] of the [[gradient]]. This approximation is based on the [[information matrix equality]] and therefore only valid while maximizing a [[likelihood function]].<ref>{{cite journal |first=A. |last=Henningsen |first2=O. |last2=Toomet |title=maxLik: A package for maximum likelihood estimation in R |journal=Computational Statistics |year=2011 |volume=26 |issue=3 |pages=443–458 [p. 450] |doi=10.1007/s00180-010-0217-1 }}</ref> The BHHH algorithm is named after the four originators: [[Ernst R. Berndt]], [[Bronwyn Hall]], [[Robert Hall (economist)|Robert Hall]], and [[Jerry Hausman]].<ref>{{cite journal |last=Berndt |first=E. |first2=B. |last2=Hall |first3=R. |last3=Hall |first4=J. |last4=Hausman |year=1974 |url=https://www.nber.org/chapters/c10206.pdf |title=Estimation and Inference in Nonlinear Structural Models |journal=Annals of Economic and Social Measurement |volume=3 |issue=4 |pages=653–665 }}</ref>\n\n==Usage==\nIf a [[nonlinear]] model is fitted to the [[data]] one often needs to estimate [[coefficient]]s through [[Optimization (mathematics)|optimization]]. A number of optimisation algorithms have the following general structure. Suppose that the function to be optimized is ''Q''(''β''). Then the algorithms are iterative, defining a sequence of approximations, ''β<sub>k</sub>'' given by\n:<math>\\beta_{k+1}=\\beta_{k}-\\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\beta}(\\beta_{k}),</math>,\n\nwhere <math>\\beta_{k}</math> is the parameter estimate at step k, and <math>\\lambda_{k}</math> is a parameter (called step size) which partly determines the particular algorithm. For the BHHH algorithm ''λ<sub>k</sub>'' is determined by calculations within a given iterative step, involving a line-search until a point ''β''<sub>''k''+1</sub> is found satisfying certain criteria. In addition, for the BHHH algorithm, ''Q'' has the form\n\n:<math>Q = \\sum_{i=1}^{N} Q_i</math>\nand ''A'' is calculated using\n:<math>A_{k}=\\left[\\sum_{i=1}^{N}\\frac{\\partial \\ln Q_i}{\\partial \\beta}(\\beta_{k})\\frac{\\partial \\ln Q_i}{\\partial \\beta}(\\beta_{k})'\\right]^{-1} .</math>\nIn other cases, e.g. [[Newton–Raphson]], <math>A_{k}</math> can have other forms. The BHHH algorithm has the advantage that, if certain conditions apply, convergence of the iterative procedure is guaranteed.{{citation needed|date=March 2013}}\n\n==See also==\n*[[Davidon–Fletcher–Powell algorithm|Davidon–Fletcher–Powell (DFP) algorithm]]\n*[[Broyden–Fletcher–Goldfarb–Shanno algorithm|Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*{{cite book |last=Amemiya |first=Takeshi |authorlink=Takeshi Amemiya |title=Advanced Econometrics |location=Cambridge |publisher=Harvard University Press |year=1985 |isbn=0-674-00560-0 |pages=137–138 }}\n*{{cite book |last=Gill |first=P. |first2=W. |last2=Murray |first3=M. |last3=Wright |year=1981 |title=Practical Optimization |publisher=Harcourt Brace |location=London }}\n*{{cite book |first=Christian |last=Gourieroux |first2=Alain |last2=Monfort |title=Statistics and Econometric Models |chapter=Gradient Methods and ML Estimation |location=New York |publisher=Cambridge University Press |year=1995 |isbn=0-521-40551-3 |pages=452–458 |chapterurl=https://books.google.com/books?id=gqI-pAP2JZ8C&pg=PA452 }}\n*{{cite book |last=Harvey |first=A. C. |title=The Econometric Analysis of Time Series |location=Cambridge |publisher=MIT Press |year=1990 |edition=Second |isbn=0-262-08189-X |pages=137–138 }}\n\n{{Optimization algorithms|unconstrained}}\n\n{{DEFAULTSORT:Bhhh Algorithm}}\n[[Category:Estimation methods]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Bin packing problem",
      "url": "https://en.wikipedia.org/wiki/Bin_packing_problem",
      "text": "{{short description|Operations research problem of packing items into the fewest number of bins}}\n{{Covering/packing-problem pairs}}\n\nIn the '''bin packing problem''', items of different volumes must be packed into a finite number of bins or containers each of volume ''V'' in a way that minimizes the number of bins used. In [[computational complexity theory]], it is a [[combinatorics|combinatorial]] [[NP-hard]] problem.<ref>{{cite book|last1=Korte|first1=Bernhard|last2=Vygen|first2=Jens|chapter=Bin-Packing|title=Combinatorial Optimization: Theory and Algorithms|isbn=978-3-540-25684-7|series=Algorithms and Combinatorics 21|year=2006|pages=426–441|doi=10.1007/3-540-29297-7_18|publisher=Springer}}</ref>  The [[decision problem]] (deciding if items will fit into a specified number of bins) is [[NP-complete]].<ref>{{cite web|title=Bin Packing|first=David Mix|last=Barrington|url=https://people.cs.umass.edu/~barring/cs311/disc/11.html|year=2006}}</ref>\n\nThere are many [[Packing problem|variations]] of this problem, such as 2D packing, linear packing, packing by weight, packing by cost, and so on. They have many applications, such as filling up containers, loading trucks with weight capacity constraints, creating file [[backup]]s in  media and technology mapping in [[field-programmable gate array]] [[semiconductor chip]] design.\n\nThe bin packing problem can also be seen as a special case of the [[cutting stock problem]]. When the number of bins is restricted to 1 and each item is characterised by both a volume and a value, the problem of maximising the value of items that can fit in the bin is known as the [[knapsack problem]].\n\nDespite the fact that the bin packing problem has an NP-hard [[Computational complexity theory|computational complexity]], optimal solutions to very large instances of the problem can be produced with sophisticated algorithms. In addition, many [[heuristic (computer science)|heuristics]] have been developed: for example, the '''first fit algorithm''' provides a fast but often non-optimal solution, involving placing each item into the first bin in which it will fit. It requires '''[[big O notation|Θ]]'''(''n''&nbsp;log&nbsp;''n'') time, where ''n'' is the number of items to be packed. The algorithm can be made much more effective by first [[sorting]] the list of items into decreasing order (sometimes known as the first-fit decreasing algorithm), although this still does not guarantee an optimal solution, and for longer lists may increase the running time of the algorithm. It is known, however, that there always exists at least one ordering of items that allows first-fit to produce an optimal solution.<ref>{{harvnb|Lewis|2009}}</ref>\n\nA variant of bin packing that occurs in practice is when items can share space when packed into a bin. Specifically,  a set of items could occupy less space when packed together than the sum of their individual sizes. This variant is known as VM packing<ref>{{harvnb|Sindelar|Sitaraman|Shenoy|2011|pp=367–378}}</ref> since when [[virtual machines]] (VMs) are packed in a server, their total [[Memory management|memory requirement]] could decrease due to [[page (computer memory)|pages]] shared by the VMs that need only be stored once. If items can share space in arbitrary ways, the bin packing problem is hard to even approximate. However, if the space sharing fits into a hierarchy, as is the case with memory sharing in virtual machines, the bin packing problem can be efficiently approximated.\nAnother variant of bin packing of interest in practice is the so-called [[online algorithm|online]] bin packing. Here the items of different volume are supposed to arrive sequentially and the decision maker has to decide whether to select and pack the currently observed item, or else to let it pass. Each decision is without recall.\n\n==Formal statement==\nGiven a set of bins <math>S_1, S_2...</math> with the same size <math>V</math> and a list of <math>n</math> items with sizes <math>a_1,\\,\\dots,\\,a_n</math> to pack, find an integer number of bins <math>B</math> and a <math>B</math>-[[Partition of a set|partition]] <math>S_1 \\cup \\cdots \\cup S_B</math> of the set <math>\\{1,\\,\\dots,\\,n\\}</math> such that <math>\\sum_{i \\in S_k} a_i \\leq V</math> for all <math>k=1,\\,\\dots,\\,B.</math> A solution is ''optimal'' if it has minimal <math>B</math>. The <math>B</math>-value for an optimal solution is denoted '''OPT''' below. A possible Integer Linear Programming formulation of the problem is:\n{|\n|-\n|colspan=\"2\"|minimize <math> B = \\sum_{i=1}^n y_i</math>\n|\n|-\n|subject to\n|<math>B \\geq 1,</math>\n|-\n|\n|<math>\\sum_{j=1}^n a_j x_{ij} \\leq V y_i,</math>\n|<math>\\forall i \\in \\{1,\\ldots,n\\}</math>\n|-\n|\n|<math>\\sum_{i=1}^n x_{ij} = 1,</math>\n|<math>\\forall j \\in \\{1,\\ldots,n\\}</math>\n|-\n|\n|<math> y_i \\in \\{0,1\\},</math>\n|<math>\\forall i \\in \\{1,\\ldots,n\\}</math>\n|-\n|\n|<math> x_{ij} \\in \\{0,1\\},</math>\n|<math>\\forall i \\in \\{1,\\ldots,n\\} \\, \\forall j \\in \\{1,\\ldots,n\\}</math>\n|}\nwhere <math> y_i = 1</math> if bin <math>i</math> is used and <math> x_{ij} = 1</math> if item <math>j</math> is put into bin <math>i</math>.<ref name=Martello1990>{{harvnb|Martello|Toth|1990|p=221}}</ref>\n\n==First-fit algorithm==\nThis is a straightforward [[greedy algorithm|greedy]] [[approximation algorithm]]. The algorithm processes the items in arbitrary order. For each item, it attempts to place the item in the first bin that can accommodate the item. If no bin is found, it opens a new bin and puts the item within the new bin.\n\nThis algorithm achieves an [[APX|approximation factor]] of 2; the number of bins used by this algorithm is no more than twice the optimal number of bins. In other words, it is impossible for 2 bins to be at most half full because such a possibility implies that at some point, exactly one bin was at most half full and a new one was opened to accommodate an item of size at most V/2. But since the first one has at least a space of ''V''&nbsp;/&nbsp;2, the algorithm will not open a new bin for any item whose size is at most ''V''&nbsp;/&nbsp;2. Only after the bin fills with more than ''V''&nbsp;/&nbsp;2 or if an item with a size larger than ''V''&nbsp;/&nbsp;2 arrives, the algorithm may open a new bin.\n\nThus if we have ''B'' bins, at least ''B''&nbsp;−&nbsp;1 bins are more than half full. Therefore, <math>\\sum_{i=1}^n a_i>\\tfrac{B-1}{2}V</math>. Because <math>\\tfrac{\\sum_{i=1}^n a_i}{V}</math> is a lower bound of the optimum value ''OPT'', we get that ''B''&nbsp;−&nbsp;1&nbsp;<&nbsp;2''OPT'' and therefore ''B''&nbsp;≤&nbsp;2''OPT''.<ref>{{harvnb|Vazirani|2003|p=74}}.</ref> See the analysis below for better approximation results.\n\nModified first fit decreasing (MFFD)<ref>{{harvnb|Garey|Johnson|1985|pp=65–106}}.</ref> improves on FFD for items larger than half a bin by classifying items by size into four size classes large, medium, small, and tiny, corresponding to items with size > 1/2 bin, > 1/3 bin, > 1/6 bin, and smaller items respectively. Then it proceeds through five phases:\n\n# Allot a bin for each large item, ordered largest to smallest.\n# Proceed forward through the bins. On each: If the smallest remaining medium item does not fit, skip this bin. Otherwise, place the largest remaining medium item that fits.\n# Proceed backward through those bins that do not contain a medium item. On each: If the two smallest remaining small items do not fit, skip this bin. Otherwise, place the smallest remaining small item and the largest remaining small item that fits.\n# Proceed forward through all bins. If the smallest remaining item of any size class does not fit, skip this bin. Otherwise, place the largest item that fits ''and stay on this bin.''\n# Use FFD to pack the remaining items into new bins.\n\n==Analysis of approximate algorithms==\nThe ''best fit decreasing'' and ''first fit decreasing'' strategies are among the simplest heuristic algorithms for solving the bin packing problem.  They have been shown to use no more than 11/9&nbsp;'''OPT'''&nbsp;+&nbsp;1 bins (where '''OPT''' is the number of bins given by the optimal solution).<ref>{{harvnb|Yue|1991|pp=321–331}}.</ref> The simpler of these, the ''First Fit Decreasing'' (FFD) strategy, operates by first sorting the items to be inserted in decreasing order by their sizes, and then inserting each item into the first bin in the list with sufficient remaining space. Sometimes, however, one does not have the option to sort the input, for example, when faced with an [[online algorithm|online]] bin packing problem. In 2007, it was proven that the bound 11/9&nbsp;'''OPT'''&nbsp;+&nbsp;6/9 for FFD is [[Asymptotically tight bound|tight]].<ref>{{harvnb|Dósa|2007|pp=1–11}}.</ref>  MFFD uses no more than 71/60&nbsp;'''OPT'''&nbsp;+&nbsp;1 bins<ref>{{harvnb|Yue|Zhang|1995|pp=318–330}}.</ref> (i.e. bounded by about 1.18&nbsp;'''OPT''', compared to about 1.22&nbsp;'''OPT''' for FFD). In 2013, Dósa and Sgall gave a tight upper bound for the first-fit (FF) strategy, showing that it never needs more than 17/10&nbsp;'''OPT''' bins for any input.{{sfn|Dósa|Sgall|2013}}\n\n==Exact algorithm==\nMartello and Toth<ref>{{harvnb|Martello|Toth|1990|pp=237–240}}.</ref> developed an exact algorithm for the 1-D bin-packing problem, called MTP. A faster alternative is the Bin Completion algorithm proposed by Korf in 2002<ref>{{harvnb|Korf|2002}}</ref> and later improved;<ref name=Korf2003Korf>R. E. Korf (2003), ''An improved algorithm for optimal bin packing''. Proceedings of the International Joint Conference on Artificial Intelligence, (pp. 1252–1258)</ref> this second paper reports the average time to solve one million instances with 80 items on a 440&nbsp;MHz [[Sun Ultra series|Sun Ultra]] 10 workstation was 31 ms.\n\nA further improvement was presented by Schreiber and Korf in 2013.<ref>{{harvnb|Schreiber|Korf|2013}}</ref> The new Improved Bin Completion algorithm is shown to be up to five orders of magnitude faster than Bin Completion on non-trivial problems with 100 items, and outperforms the BCP (branch-and-cut-and-price) algorithm by Belov and Scheithauer on problems that have fewer than 20 bins as the optimal solution. Which algorithm performs best depends on problem properties like number of items, optimal number of bins, unused space in the optimal solution and value precision.\n\n==See also==\n* If the number of bins is to be fixed or constrained, and the size of the bins is to be minimised, that is a different problem which is equivalent to the [[Multiprocessor scheduling|Multiprocessor scheduling problem]]\n* [[Guillotine problem]]\n* [[Packing problems]]\n* [[Partition problem]]\n* [[Subset sum problem]]\n\n==References==\n{{reflist|30em}}\n\n'''Bibliography'''\n# {{citation\n  | last = Korf\n  | first = Richard E.\n  | title = A new algorithm for optimal bin packing. \n  | conference = AAAI-02\n  | year = 2002\n  | url = http://www.aaai.org/Papers/AAAI/2002/AAAI02-110.pdf }}\n# {{citation\n  | last = Vazirani\n  | first = Vijay V.\n  | authorlink = Vijay Vazirani\n  | title = Approximation Algorithms\n  | publisher = Springer\n  | year = 2003\n  | location = Berlin\n  | isbn = 3-540-65367-8 }}\n# {{citation\n  | last = Yue\n  | first = Minyi\n  | journal = Acta Mathematicae Applicatae Sinica\n  | volume = 7\n  |date=October 1991\n  | pages = 321–331\n  | doi = 10.1007/BF02009683\n  | issn = 0168-9673\n  | issue = 4\n  | title = A simple proof of the inequality FFD (L) ≤ 11/9 OPT (L) + 1, ∀L for the FFD bin-packing algorithm }}\n# {{citation\n  | last = Dósa\n  | first = György\n  | chapter = The Tight Bound of First Fit Decreasing Bin-Packing Algorithm Is FFD(I)≤(11/9)OPT(I)+6/9\n  | title = Combinatorics, Algorithms, Probabilistic and Experimental Methodologies\n  | publisher = Springer Berlin / Heidelberg\n  | volume = 4614/2007\n  | year = 2007\n  | pages = 1–11\n  | isbn = 978-3-540-74449-8\n  | doi = 10.1007/978-3-540-74450-4\n  | issn = 0302-9743\n  | editor1-last = Chen\n  | editor1-first = Bo\n  | editor2-last = Paterson\n  | editor2-first = Mike\n  | editor3-last = Zhang\n  | editor3-first = Guochuan }}\n# {{citation\n  | last1 = Xia\n  | first1 = Binzhou\n  | last2 = Tan\n  | first2 = Zhiyi\n  | journal = Discrete Applied Mathematics\n  | volume = 158\n  | year = 2010\n  | pages = 1668–1675\n  | doi = 10.1016/j.dam.2010.05.026\n  | issn = 0166-218X\n  | issue = 15\n  | title = Tighter bounds of the First Fit algorithm for the bin-packing problem}}\n# {{citation\n  | last1 = Garey\n  | first1 = Michael R.\n  | authorlink1 = Michael R. Garey\n  | last2 = Johnson\n  | first2 = David S.\n  | authorlink2 = David S. Johnson\n  | journal = Journal of Complexity\n  | volume = 1\n  | year = 1985\n  | pages = 65–106\n  | doi = 10.1016/0885-064X(85)90022-6\n  | title = A 71/60 theorem for bin packing*1 }}\n# {{citation\n  | last1 = Yue\n  | first1 = Minyi\n  | last2 = Zhang\n  | first2 = Lei\n  | journal = Acta Mathematicae Applicatae Sinica\n  | volume = 11\n  |date=July 1995\n  | pages = 318–330\n  | doi = 10.1007/BF02011198\n  | issn = 0168-9673\n  | issue = 3\n  | title = A simple proof of the inequality MFFD(L)≤71/60 OPT(L) + 1,L for the MFFD bin-packing algorithm }}\n# {{citation\n  | last1 = Fernandez de la Vega\n  | first1 = W.\n  | last2 = Lueker\n  | first2 = G. S.\n  | journal = Combinatorica\n  | publisher = Springer Berlin / Heidelberg\n  | volume = 1\n  |date=December 1981\n  | pages = 349–355\n  | doi = 10.1007/BF02579456\n  | issn = 0209-9683\n  | issue = 4\n  | title = Bin packing can be solved within 1 + ε in linear time }}\n# {{citation\n  | last = Lewis\n  | first = R.\n  | journal = Computers and Operations Research\n  | volume = 36\n  | year = 2009\n  | pages = 2295–2310\n  | doi = 10.1016/j.cor.2008.09.004\n  | issue = 7\n  | title = A General-Purpose Hill-Climbing Method for Order Independent Minimum Grouping Problems: A Case Study in Graph Colouring and Bin Packing }}\n# {{citation\n  | first1=Silvano\n  | last1=Martello\n  | first2=Paolo\n  | last2=Toth\n  | chapter = Bin-packing problem\n  | title=Knapsack Problems: Algorithms and Computer Implementations\n  | year=1990\n  | publisher=John Wiley and Sons\n  | location=Chichester, UK\n  | isbn=0471924202\n  | chapter-url=http://www.or.deis.unibo.it/kp/Chapter8.pdf }}\n# Michael R. Garey and David S. Johnson (1979), Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. Freeman. {{ISBN|0-7167-1045-5}}. A4.1: SR1, p.&nbsp;226.\n# David S. Johnson, Alan J. Demers, Jeffrey D. Ullman, M. R. Garey, Ronald L. Graham. [http://www.math.ucsd.edu/~fan/ron/papers/74_04_one_dimensional_packing.pdf Worst-Case Performance Bounds for Simple One-Dimensional Packing Algorithms]. SICOMP, Volume 3, Issue 4. 1974.\n# Lodi A., Martello S., Monaci, M., Vigo, D. (2010) \"Two-Dimensional Bin Packing Problems\". In V.Th. Paschos (Ed.), ''Paradigms of Combinatorial  Optimization'', Wiley/ISTE, pp.&nbsp;107–129\n# {{cite conference\n  | last1 = Dósa\n  | first1 = György\n  | last2 = Sgall\n  | first2 = Jiří\n  | year = 2013\n  | title = First Fit bin packing: A tight analysis\n  | book-title = 30th International Symposium on Theoretical Aspects of Computer Science (STACS 2013)\n  | location = Dagstuhl, Germany\n  | url = http://drops.dagstuhl.de/opus/volltexte/2013/3963\n  | isbn = 978-3-939897-50-7\n  | pages = 538–549}}\n# Benkő A., Dósa G., Tuza Z. (2010) \"Bin Packing/Covering with Delivery, Solved with the Evolution of Algorithms,\" ''Proceedings 2010 IEEE 5th International Conference on Bio-Inspired Computing: Theories and Applications, BIC-TA 2010'', art. no. 5645312, pp.&nbsp;298–302.\n# {{citation\n  | last1 = Sindelar\n  | first1 = Michael\n  | last2 = Sitaraman\n  | first2 = Ramesh\n  | authorlink2 = Ramesh Sitaraman\n  | last3 = Shenoy\n  | first3 = Prashant\n  | journal = Proceedings of 23rd ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), San Jose, CA, June 2011\n  |pages = 367–378\n  | year = 2011\n  | title = Sharing-Aware Algorithms for Virtual Machine Colocation}}\n# {{citation\n  | last1 = Schreiber\n  | first1 = Ethan L.\n  | last2 = Korf\n  | first2 = Richard E.\n  | title = Improved Bin Completion for Optimal Bin Packing and Number Partitioning\n  | booktitle = Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence\n  | series = IJCAI '13\n  | year = 2013\n  | isbn = 978-1-57735-633-2\n  | location = Beijing, China\n  | pages = 651–658\n  | url = https://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6851\n  | publisher = AAAI Press}}\n\n==External links==\n*[https://pdfs.semanticscholar.org/bb99/86af2f26f7726fcef1bc684eac8239c9b853.pdf?_ga=1.50320358.1394974689.1485463187 Optimizing Three-Dimensional Bin Packing]\n*[http://www.martinbroadhurst.com/bin-packing.html Implementation of 7 classic approximate bin packing algorithms in C with results and images]\n*[http://www.phpclasses.org/package/2027-PHP-Pack-files-without-exceeding-a-given-size-limit.html PHP Class to pack files without exceeding a given size limit]\n*[http://www.cs.unc.edu/~bbb/#bin-packing An implementation of several bin packing heuristics in Haskell], including FFD and MFFD.\n*[http://www.binpacking.4fan.cz/ Visualization of heuristics for 1D and 2D bin packing]\n*[https://code.google.com/p/caparf/ Cutting And Packing Algorithms Research Framework], including a number of bin packing algorithms and test data.\n*[http://dl.acm.org/citation.cfm?id=3833&jmp=abstract&dl=portal&dl=ACM A simple on-line bin-packing algorithm]\n*[http://sourceforge.net/projects/fpart/ Fpart : open-source command-line tool to pack files (C, BSD-licensed)]\n*[http://www.codeproject.com/Articles/706136/Csharp-Bin-Packing-Cutting-Stock-Solver Bin Packing and Cutting Stock Solver Algorithm]\n{{Packing problem}}\n\n{{DEFAULTSORT:Bin Packing Problem}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Packing problems]]\n[[Category:Strongly NP-complete problems]]"
    },
    {
      "title": "Bland's rule",
      "url": "https://en.wikipedia.org/wiki/Bland%27s_rule",
      "text": "In [[mathematical optimization]], '''Bland's rule''' (also known as '''Bland's algorithm''', '''Bland's anti-cycling rule''' or '''Bland's pivot rule''') is an algorithmic refinement of the [[simplex method]] for [[linear programming|linear optimization]].\n\nWith Bland's rule, the simplex algorithm solves feasible [[linear optimization]] problems without cycling.<ref name=\"Bland\">{{harvtxt|Bland|1977}}.</ref><ref name=\"co\">{{cite book | title = Combinatorial Optimization: Algorithms and Complexity | publisher = Dover Publications | author = Christos H. Papadimitriou, Kenneth Steiglitz | date = 1998-01-29 | pages = 53–55 }}</ref><ref name=\"brown\">{{cite web | url = http://www.cs.brown.edu/courses/csci1490/notes/day9.pdf | title = Notes on the Simplex Algorithm| accessdate = 2007-12-17 | author = [[Brown University]] - Department of Computer Science |date = 2007-10-18 }}</ref>\n\nThe original simplex algorithm starts with an arbitrary [[basic feasible solution]], and then changes the basis in order to increase the maximization target and find an optimal solution. Usually, the target indeed increases in every step, and thus after a bounded number of steps an optimal solution is found. However, there are examples of degenerate linear programs, on which the original simplex algorithm cycles forever. It gets stuck at a [[basic feasible solution]] (a corner of the feasible polytope) and changes bases in a cyclic way without increasing the maximization target.\n\nSuch cycles are avoided by Bland's rule for choosing a column to enter the basis.\n\nBland's rule was developed by [[Robert G. Bland]], now a professor of operations research at [[Cornell University]], while he was a research fellow at the [[Center for Operations Research and Econometrics]] in Belgium.<ref name=\"Bland\" />\n\n==Algorithm==\nOne uses Bland's rule during an iteration of the simplex method to decide first what column (known as the ''entering variable'') and then row (known as the ''leaving variable'') in the tableau to pivot on. Assuming that the problem is to minimize the objective function, the algorithm is loosely defined as follows:\n\n# Choose the lowest-numbered (i.e., leftmost) nonbasic column with a negative (reduced) cost.\n# Now among the rows, choose the one with the lowest ratio between the (transformed) right hand side and the coefficient in the pivot tableau where the coefficient is greater than zero. If the minimum ratio is shared by several rows, choose the row with the lowest-numbered column (variable) basic in it.\n\nIt can be formally proved that, with Bland's selection rule, the simplex algorithm never cycles, so it is guaranteed to terminate in a bounded time.\n\nWhile Bland's pivot rule is theoretically important, from a practical perspective it is quite inefficient and takes a long time to converge. In practice, other pivot rules are used, and cycling almost never happens.<ref name=\"gm06\">{{Cite Gartner Matousek 2006}}{{rp|44–48}}</ref>{{rp|72–76}}\n\n==Extensions to oriented matroids==\nIn the abstract setting of [[oriented matroid]]s, Bland's rule cycles on some examples. A restricted class of oriented matroids on which Bland's rule avoids cycling has been termed \"Bland oriented matroids\" by [[Jack Edmonds]]. Another pivoting rule, the [[criss-cross algorithm]], avoids cycles on all oriented-matroid linear-programs.<ref>\n{{cite journal|first1=Komei|last1=Fukuda|first2=Tamás|last2=Terlaky|title=Criss-cross methods: A fresh view on pivot algorithms |journal=Mathematical Programming, Series B|volume=79|number=1–3|pages=369–395|editors=Thomas&nbsp;M. Liebling and Dominique de&nbsp;Werra|publisher=North-Holland Publishing&nbsp;Co. |location=Amsterdam|year=1997|doi=10.1007/BF02614325|MR=1464775}}\n</ref>\n\n== Notes ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite journal|title=New finite pivoting rules for the simplex method|first=Robert G.|last=Bland|authorlink=Robert G. Bland|journal=Mathematics of Operations Research|volume=2|date=May 1977|pages=103–107|doi=10.1287/moor.2.2.103|jstor=3689647|mr=459599|ref=harv|issue=2}}\n* [[George B. Dantzig]] and Mukund N. Thapa. 2003. ''Linear Programming 2: Theory and Extensions''. Springer-Verlag.\n* Kattta G. Murty, ''Linear Programming'', Wiley, 1983.\n* Evar D. Nering and [[Albert W. Tucker]], 1993, ''Linear Programs and Related Problems'', Academic Press.\n* M. Padberg, ''Linear Optimization and Extensions'', Second Edition, Springer-Verlag, 1999.\n* [[Christos H. Papadimitriou]] and Kenneth Steiglitz, ''Combinatorial Optimization: Algorithms and Complexity'', Corrected republication with a new preface, Dover. (computer science)\n* [[Alexander Schrijver]], ''Theory of Linear and Integer Programming''. John Wiley & sons, 1998, {{ISBN|0-471-98232-6}} (mathematical)\n* {{cite journal|author=Michael J. Todd |date=February 2002 | title = The many facets of linear programming | journal = Mathematical Programming | volume = 91 | issue = 3 | doi = 10.1007/s101070100261 | pages=417–436}} (Invited survey, from the International Symposium on Mathematical Programming.)\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Exchange algorithms]]\n[[Category:Oriented matroids]]"
    },
    {
      "title": "BOBYQA",
      "url": "https://en.wikipedia.org/wiki/BOBYQA",
      "text": "'''BOBYQA''' ('''B'''ound '''O'''ptimization '''BY''' '''Q'''uadratic '''A'''pproximation)<ref name=\"report\">{{Cite report |author= Powell, M. J. D. |date= June 2009 | title=The BOBYQA algorithm for bound constrained optimization without derivatives | url=http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2009_06.pdf |publisher=Department of Applied Mathematics and Theoretical Physics, Cambridge University |docket=DAMTP 2009/NA06|accessdate= 2014-02-14}}</ref> is a [[numerical analysis|numerical]] [[optimization (mathematics)|optimization]] [[algorithm]] by [[Michael J. D. Powell]]. It is also the name of Powell's [[Fortran#FORTRAN 77|Fortran 77]] implementation of the algorithm.\n\nBOBYQA solves bound [[constrained optimization]] problems without using [[derivative]]s of the [[Optimization problem|objective function]], which makes it a [[derivative-free optimization|derivative-free]] algorithm. The algorithm solves the problem using a [[trust region]] method that forms [[quadratic function|quadratic]] models by [[interpolation]]. One new point is computed on each iteration, usually by solving a [[trust region]] subproblem subject to the bound constraints, or alternatively, by choosing a point to replace an [[interpolation]] point so as to promote good linear independence in the interpolation conditions.\n\nThe same as [[NEWUOA]], BOBYQA constructs the quadratic models by the least [[Frobenius norm]] updating <ref>{{cite journal|last=Powell|first=M. J. D. |title=Least Frobenius norm updating of quadratic models that satisfy interpolation conditions |journal=Mathematical Programming |publisher= Springer |year=2004 |volume=100 |pages=183–215|doi=10.1007/s10107-003-0490-7}}</ref> technique.\n\nBOBYQA [[software]] was released on January 5, 2009.<ref name=\"repository\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#powell_software |title=A repository of Professor M. J. D. Powell's software |publisher= |date= |accessdate=2014-01-18}}</ref>\n\nIn the [[Comment (computer programming)|comment]] of the software's [[source code]],<ref name=\"code\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#bobyqa |title=Source code of BOBYQA software |publisher= |date= |accessdate=2014-02-14}}</ref> it is said that the name BOBYQA denotes \"Bound Approximation BY Quadratic\nApproximation\", which seems to be a typo of \"Bound Optimization BY Quadratic Approximation\".\n\nThe BOBYQA software is distributed under [[GNU Lesser General Public License|The GNU Lesser General Public License]] (LGPL).<ref name=\"code\"/>\n\n==See also==\n* [[TOLMIN (optimization software)|TOLMIN]]\n* [[COBYLA]]\n* [[UOBYQA]]\n* [[NEWUOA]]\n* [[LINCOA]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://ccpforge.cse.rl.ac.uk/gf/project/powell Optimization software by Professor M. J. D. Powell at CCPForge]\n* [https://sourceforge.net/projects/bobyqa-fortran95 M.Powells BOBYQA-fortran77 code ported to fortran 95 with a more modern, easier user interface]\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{mathapplied-stub}}\n{{software-stub}}"
    },
    {
      "title": "Branch and bound",
      "url": "https://en.wikipedia.org/wiki/Branch_and_bound",
      "text": "{{graph search algorithm}}\n'''Branch and bound''' ('''BB''', '''B&B''', or '''BnB''') is an [[algorithm]] design paradigm for [[discrete optimization|discrete]] and [[combinatorial optimization]] problems, as well as [[mathematical optimization]]. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of [[state space search]]: the set of candidate solutions is thought of as forming a [[Tree (graph theory)|rooted tree]] with the full set at the root. The algorithm explores ''branches'' of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated ''bounds'' on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.\n\nThe algorithm depends on efficient estimation of the lower and upper bounds of regions/branches of the search space. If no bounds are available, the algorithm degenerates to an exhaustive search.\n\nThe method was first proposed by [[Ailsa Land]] and [[Alison Harcourt|Alison Doig]] whilst carrying out research at the [[London School of Economics]] sponsored by [[BP|British Petroleum]]<ref name=land_doig>{{cite news |author = A. H. Land and A. G. Doig | year = 1960 | title = An automatic method of solving discrete programming problems | journal = Econometrica | volume = 28 | issue = 3 | pages = 497–520 | doi=10.2307/1910129}}</ref><ref>{{Cite web|url=http://www.lse.ac.uk/newsletters/pressAndInformation/staffNews/2010/20100218.htm|title=Staff News|website=www.lse.ac.uk|access-date=2018-10-08}}</ref> in 1960 for [[discrete optimization|discrete programming]], and has become the most commonly used tool for solving [[NP-hard]] optimization problems.<ref name=\"clausen99\"/> The name \"branch and bound\" first occurred in the work of Little ''et al.'' on the [[traveling salesman problem]].<ref name=\"little\"/><ref>{{cite report |last1=Balas |first1=Egon |first2=Paolo |last2=Toth |year=1983 |title=Branch and bound methods for the traveling salesman problem |issue=Management Science Research Report MSRR-488 |publisher=[[Carnegie Mellon University]] Graduate School of Industrial Administration |url=http://www.dtic.mil/dtic/tr/fulltext/u2/a126957.pdf}}</ref>\n\n==Overview==\nThe goal of a branch-and-bound algorithm is to find a value {{mvar|x}} that maximizes or minimizes the value of a real-valued function {{math|''f''(''x'')}}, called an objective function, among some set {{mvar|S}} of admissible, or [[candidate solution]]s. The set {{mvar|S}} is called the search space, or [[feasible region]]. The rest of this section assumes that minimization of {{math|''f''(''x'')}} is desired; this assumption comes [[without loss of generality]], since one can find the maximum value of {{math|''f''(''x'')}} by finding the minimum of {{math|''g''(''x'') {{=}} −''f''(''x'')}}. A B&B algorithm operates according to two principles:\n\n* It recursively splits the search space into smaller spaces, then minimizing {{math|''f''(''x'')}} on these smaller spaces; the splitting is called ''branching''.\n* Branching alone would amount to [[Brute-force search|brute-force]] enumeration of candidate solutions and testing them all. To improve on the performance of brute-force search, a B&B algorithm keeps track of ''bounds'' on the minimum that it is trying to find, and uses these bounds to \"[[pruning (decision trees)|prune]]\" the search space, eliminating candidate solutions that it can prove will not contain an optimal solution.\n\nTurning these principles into a concrete algorithm for a specific optimization problem requires some kind of [[data structure]] that represents sets of candidate solutions. Such a representation is called an ''instance'' of the problem. Denote the set of candidate solutions of an instance {{mvar|I}} by {{mvar|S<sub>I</sub>}}. The instance representation has to come with three operations:\n\n* {{math|branch(''I'')}} produces two or more instances that each represent a subset of {{mvar|S<sub>I</sub>}}. (Typically, the subsets are [[disjoint sets|disjoint]] to prevent the algorithm from visiting the same candidate solution twice, but this is not required. However, the optimal solution among {{mvar|S<sub>I</sub>}} must be contained in at least one of the subsets.<ref name=\"bader\">{{cite encyclopedia |first1=David A. |last1=Bader |first2=William E. |last2=Hart |first3=Cynthia A. |last3=Phillips|author3-link= Cynthia A. Phillips |title=Parallel Algorithm Design for Branch and Bound |editor-first=H. J. |editor-last=Greenberg |encyclopedia=Tutorials on Emerging Methodologies and Applications in Operations Research |publisher=Kluwer Academic Press |year=2004 |url=http://www.cc.gatech.edu/~bader/papers/ParallelBranchBound.pdf}}</ref>)\n* {{math|bound(''I'')}} computes a lower bound on the value of any candidate solution in the space represented by {{mvar|I}}, that is, {{math|bound(''I'') ≤ ''f''(''x'')}} for all {{mvar|x}} in {{mvar|S<sub>I</sub>}}.\n* {{math|solution(''I'')}} determines whether {{mvar|I}} represents a single candidate solution. (Optionally, if it does not, the operation may choose to return some feasible solution from among {{mvar|S<sub>I</sub>}}.{{r|bader}})\n<!--(For example, {{mvar|S}} could be the set of all possible trip schedules for a bus fleet, and {{math|''f''(''x'')}} could be the expected revenue for schedule {{mvar|x}}.)-->\n\nUsing these operations, a B&B algorithm performs a top-down recursive search through the [[search tree|tree]] of instances formed by the branch operation. Upon visiting an instance {{mvar|I}}, it checks whether {{math|bound(''I'')}} is greater than the lower bound for some other instance that it already visited; if so, {{mvar|I}} may be safely discarded from the search and the recursion stops. This pruning step is usually implemented by maintaining a global variable that records the minimum lower bound seen among all instances examined so far.\n\n===Generic version===\nThe following is the skeleton of a generic branch and bound algorithm for minimizing an arbitrary objective function {{mvar|f}}.<ref name=\"clausen99\">{{cite techreport |first=Jens |last=Clausen |title=Branch and Bound Algorithms—Principles and Examples |year=1999 |publisher=[[University of Copenhagen]] |url=http://www.diku.dk/OLD/undervisning/2003e/datV-optimer/JensClausenNoter.pdf |accessdate=2014-08-13 |archiveurl=https://web.archive.org/web/20150923214803/http://www.diku.dk/OLD/undervisning/2003e/datV-optimer/JensClausenNoter.pdf |archivedate=2015-09-23 |deadurl=yes }}</ref> To obtain an actual algorithm from this, one requires a bounding function {{mvar|g}}, that computes lower bounds of {{mvar|f}} on nodes of the search tree, as well as a problem-specific branching rule. As such, the generic algorithm presented here is a [[higher order function]].\n\n# Using a [[heuristic]], find a solution {{mvar|x<sub>h</sub>}} to the optimization problem. Store its value, {{math|''B'' {{=}} ''f''(''x<sub>h</sub>'')}}. (If no heuristic is available, set {{mvar|B}} to infinity.) {{mvar|B}} will denote the best solution found so far, and will be used as an upper bound on candidate solutions.\n# Initialize a queue to hold a partial solution with none of the variables of the problem assigned.\n# Loop until the queue is empty:\n## Take a node {{mvar|N}} off the queue.\n## If {{mvar|N}} represents a single candidate solution {{mvar|x}} and {{math|''f''(''x'') < ''B''}}, then {{mvar|x}} is the best solution so far. Record it and set {{math|''B'' ← ''f''(''x'')}}.\n## Else, ''branch'' on {{mvar|N}} to produce new nodes {{mvar|N<sub>i</sub>}}. For each of these:\n### If {{math|''bound''(''N<sub>i</sub>'') > ''B''}}, do nothing; since the lower bound on this node is greater than the upper bound of the problem, it will never lead to the optimal solution, and can be discarded.\n### Else, store {{mvar|N<sub>i</sub>}} on the queue.\n\nSeveral different [[queue (abstract data type)|queue]] data structures can be used. This [[FIFO (computing and electronics)|FIFO queue]]-based implementation yields a [[breadth-first search]]. A [[Stack (data structure)|stack]] (LIFO queue) will yield a [[depth-first search|depth-first]] algorithm. A [[Best-first search|best-first]] branch and bound algorithm can be obtained by using a [[priority queue]] that sorts nodes on their lower bound.<ref name=\"clausen99\"/> Examples of best-first search algorithms with this premise are [[Dijkstra's algorithm]] and its descendant [[A* search]]. The depth-first variant is recommended when no good heuristic is available for producing an initial solution, because it quickly produces full solutions, and therefore upper bounds.<ref>{{cite book |last1=Mehlhorn |first1=Kurt |authorlink1=Kurt Mehlhorn |first2=Peter |last2=Sanders|author2-link=Peter Sanders (computer scientist) |title=Algorithms and Data Structures: The Basic Toolbox |publisher=Springer |year=2008 |page=249 |url=http://people.mpi-inf.mpg.de/~mehlhorn/ftp/Toolbox/GenericMethods.pdf}}</ref>\n\n==== {{Anchor|Code}}Pseudocode ====\nA [[C++]]-like pseudocode implementation of the above is:\n<syntaxhighlight lang=\"c++\" line=\"1\">\n// C++-like implementation of branch and bound, \n// assuming the objective function f is to be minimized\nCombinatorialSolution branch_and_bound_solve(\n    CombinatorialProblem problem, \n    ObjectiveFunction objective_function /*f*/,\n    BoundingFunction lower_bound_function /*g*/) \n{\n    // Step 1 above\n    double problem_upper_bound = std::numeric_limits<double>::infinity; // = B\n    CombinatorialSolution heuristic_solution = heuristic_solve(problem); // x_h\n    problem_upper_bound = objective_function(heuristic_solution); // B = f(x_h)\n    CombinatorialSolution current_optimum = heuristic_solution;\n    // Step 2 above\n    queue<CandidateSolutionTree> candidate_queue;\n    // problem-specific queue initialization\n    candidate_queue = populate_candidates(problem);\n    while (!candidate_queue.empty()) { // Step 3 above\n        // Step 3.1\n        CandidateSolutionTree node = candidate_queue.pop();\n        // \"node\" represents N above\n        if (node.represents_single_candidate()) { // Step 3.2\n            if (objective_function(node.candidate()) < problem_upper_bound) {\n                current_optimum = node.candidate();\n                problem_upper_bound = objective_function(current_optimum);\n            }\n            // else, node is a single candidate which is not optimum\n        }\n        else { // Step 3.3: node represents a branch of candidate solutions\n            // \"child_branch\" represents N_i above\n            for (auto&& child_branch : node.candidate_nodes) {\n                if (lower_bound_function(child_branch) <= problem_upper_bound) {\n                    candidate_queue.enqueue(child_branch); // Step 3.3.2\n                }\n                // otherwise, g(N_i) > B so we prune the branch; step 3.3.1\n            }\n        }\n    }\n    return current_optimum;\n}\n</syntaxhighlight>\n\nIn the above pseudocode, the functions <code>heuristic_solve</code> and <code>populate_candidates</code> called as subroutines must be provided as applicable to the problem. The functions {{mvar|''f''}} (<code>objective_function</code>) and {{mvar|''g''}} (<code>lower_bound_function</code>) are treated as [[function object]]s as written, and could correspond to [[anonymous function|lambda expression]]s, [[function pointer]]s or [[functor (C++)|functor]]s in the C++ programming language, among other types of [[callable object]]s.\n\n===Improvements===\nWhen <math>\\mathbf{x}</math> is a vector of <math>\\mathbb{R}^n</math>, branch and bound algorithms can be combined with [[Interval arithmetic|interval analysis]]<ref>{{cite book|last1=Moore|first1=R. E.|\ntitle=Interval Analysis|\nyear=1966|publisher=Prentice-Hall|\nlocation=Englewood Cliff, New Jersey|isbn=0-13-476853-1}}\n</ref> and [[interval contractor|contractor]] techniques in order to provide guaranteed enclosures of the global minimum.<ref>\n{{cite book|last1=Jaulin|first1=L.|last2=Kieffer|first2=M.|last3=Didrit|first3=O.|last4=Walter|first4=E.|\ntitle=Applied Interval Analysis|year=2001|publisher=Springer|location=Berlin|isbn=1-85233-219-0}}\n</ref><ref>\n{{cite book|last=Hansen|first=E.R.|\ntitle=Global Optimization using Interval Analysis|year=1992|\npublisher=Marcel Dekker|location=New York}}\n</ref>\n\n==Applications==\nThis approach is used for a number of [[NP-hard]] problems\n* [[Integer programming]]\n* [[Nonlinear programming]]\n* [[Travelling salesman problem]] (TSP)<ref name=\"little\">{{cite journal |last1=Little |first1=John D. C. |last2=Murty |first2=Katta G. |last3=Sweeney |first3=Dura W. |last4=Karel |first4=Caroline |title=An algorithm for the traveling salesman problem |journal=Operations Research |volume=11 |issue=6 |year=1963 |pages=972–989 |doi=10.1287/opre.11.6.972 |url=http://dspace.mit.edu/bitstream/handle/1721.1/46828/algorithmfortrav00litt.pdf}}</ref><ref>{{cite book |first1=Richard Walter |last1=Conway |first2=William L. |last2=Maxwell |first3=Louis W. |last3=Miller |year=2003 |title=Theory of Scheduling |publisher=Courier Dover Publications |pages=56–61}}</ref>\n* [[Quadratic assignment problem]] (QAP)\n* [[Maximum satisfiability problem]] (MAX-SAT)\n* [[Nearest neighbor search]]<ref>{{cite journal |last1=Fukunaga |first1=Keinosuke |first2=Patrenahalli M. |last2=Narendra |title=A branch and bound algorithm for computing {{mvar|k}}-nearest neighbors |journal=IEEE Transactions on Computers |year=1975 |pages=750–753|doi=10.1109/t-c.1975.224297 }}</ref> (by [[Keinosuke Fukunaga]])\n* [[Flow shop scheduling]]\n* [[Cutting stock problem]]\n* [[False noise analysis]] (FNA)\n* [[Computational phylogenetics]]\n* [[Set inversion]]\n* [[Set estimation|Parameter estimation]]\n* [[0/1 knapsack problem]]\n* [[Feature selection]] in [[machine learning]]<ref>{{cite journal |title=A branch and bound algorithm for feature subset selection |last1=Narendra |first1=Patrenahalli M. |last2=Fukunaga |first2=K. |journal=IEEE Transactions on Computers |volume=C-26 |issue=9 |year=1977 |pages=917–922 |doi=10.1109/TC.1977.1674939 |url=http://www.computer.org/csdl/trans/tc/1977/09/01674939.pdf}}</ref>\n* [[Structured prediction]] in [[computer vision]]<ref>{{Cite journal | first1 = Sebastian | last1 = Nowozin | first2 = Christoph H. | last2 = Lampert | title = Structured Learning and Prediction in Computer Vision | journal = Foundations and Trends in Computer Graphics and Vision | volume = 6 | issue = 3–4 | year = 2011  | pages = 185–365  | doi = 10.1561/0600000033 | isbn = 978-1-60198-457-9| citeseerx = 10.1.1.636.2651 }}</ref>{{rp|267–276}}\n\nBranch-and-bound may also be a base of various [[heuristic]]s. For example, one may wish to stop branching when the gap between the upper and lower bounds becomes smaller than a certain threshold. This is used when the solution is \"good enough for practical purposes\" and can greatly reduce the computations required. This type of solution is particularly applicable when the cost function used is [[noise|''noisy'']] or is the result of [[statistics|statistical estimates]] and so is not known precisely but rather only known to lie within a range of values with a specific [[probability]].{{Citation needed|date=September 2015}}\n\n==Relation to other algorithms==\nNau ''et al.'' present a generalization of branch and bound that also subsumes the [[A* search algorithm|A*]], [[B*]] and [[Alpha–beta pruning|alpha-beta]] search algorithms from [[artificial intelligence]].<ref>{{cite journal |last1=Nau |first1=Dana S. |first2=Vipin |last2=Kumar |first3=Laveen |last3=Kanal |title=General branch and bound, and its relation to A∗ and AO∗ |journal=Artificial Intelligence |volume=23 |issue=1 |year=1984 |pages=29–58 |url=https://www.cs.umd.edu/~nau/papers/nau1984general.pdf | doi = 10.1016/0004-3702(84)90004-3 }}</ref>\n\n== External links ==\n* [http://sourceforge.net/projects/lipside/ LiPS] – Free easy-to-use GUI program intended for solving linear, integer and goal programming problems.\n* [https://projects.coin-or.org/Cbc Cbc] – (Coin-or branch and cut) is an open-source mixed integer programming solver written in C++.\n\n==See also==\n* [[Backtracking]]\n* [[Branch and cut|Branch-and-cut]], a hybrid between branch-and-bound and the [[cutting plane]] methods that is used extensively for solving [[integer linear programs]].\n\n==References==\n{{Reflist|30em}}\n\n{{Optimization algorithms|combinatorial|state=expanded}}\n\n{{DEFAULTSORT:Branch And Bound}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Branch and cut",
      "url": "https://en.wikipedia.org/wiki/Branch_and_cut",
      "text": "'''Branch and cut'''<ref>{{Cite journal|last=Padberg|first=Manfred|last2=Rinaldi|first2=Giovanni|date=1991|title=A Branch-and-Cut Algorithm for the Resolution of Large-Scale Symmetric Traveling Salesman Problems|url=https://epubs.siam.org/doi/10.1137/1033004|journal=SIAM Review|language=en|volume=33|issue=1|pages=60–100|doi=10.1137/1033004|issn=0036-1445|via=Society for Industrial and Applied Mathematics}}</ref> is a method of [[combinatorial optimization]] for solving [[integer linear program]]s (ILPs), that is, [[linear programming]] (LP) problems where some or all the unknowns are restricted to [[integer]] values.<ref>{{cite journal|last=John E.|first=Mitchell|title=Branch-and-Cut Algorithms for Combinatorial Optimization Problems|journal=Handbook of Applied Optimization|year=2002|pages=65–77|url=http://eaton.math.rpi.edu/faculty/Mitchell/papers/bc_hao.pdf}}</ref> Branch and cut involves running a [[branch and bound]] algorithm and using [[cutting plane]]s to tighten the linear programming relaxations.  Note that if cuts are only used to tighten the initial LP relaxation, the algorithm is called '''cut and branch.'''\n\n==Algorithm==\nThis description assumes the ILP is a maximization problem.\n\nThe method solves the [[Linear programming relaxation|linear program without the integer constraint]] using the regular [[simplex algorithm]]. When an optimal solution is obtained, and this solution has a non-integer value for a variable that is supposed to be integer, a [[Cutting-plane method|cutting plane algorithm]] may be used to find further linear constraints which are satisfied by all [[Feasible solution|feasible]] integer points but violated by the current fractional solution. These inequalities may be added to the linear program, such that resolving it will yield a different solution which is hopefully \"less fractional\".\n\nAt this point, the [[branch and bound]] part of the algorithm is started. The problem is split into multiple (usually two) versions. The new linear programs are then solved using the simplex method and the process repeats.  During the branch and bound process, non-integral solutions to LP relaxations serve as upper bounds and integral solutions serve as lower bounds.  A node can be [[Search tree pruning|pruned]] if an upper bound is lower than an existing lower bound.  Further, when solving the LP relaxations, additional cutting planes may be generated, which may be either ''global cuts'', i.e., valid for all feasible integer solutions, or ''local cuts'', meaning that they are satisfied by all solutions fulfilling the side constraints from the currently considered branch and bound subtree.\n\n{{anchor|Algorithm summary}}The algorithm is summarized below.\n\n#Add the initial ILP to <math>L</math>, the list of active problems\n#Set <math>x^* = \\text{null}</math> and <math>v^* = -\\infty</math>\n#[[While loop|''while'']] <math>L</math> is not empty\n##Select and remove (de-queue) a problem from <math>L</math>\n##Solve the LP relaxation of the problem.\n##If the solution is infeasible, go back to 3 (while).  Otherwise denote the solution by <math>x</math> with objective value <math>v</math>.\n##If <math>v\\le v^*</math>, go back to 3.\n##If <math>x</math> is integer, set <math>v^*\\leftarrow v, x^* \\leftarrow x</math> and go back to 3.\n##If desired, search for cutting planes that are violated by <math>x</math>.  If any are found, add them to the LP relaxation and return to 3.2.\n##Branch to partition the problem into new problems with restricted feasible regions.  Add these problem to <math>L</math> and go back to 3\n#return <math>x^*</math>\n\n==== {{Anchor|Code}}Pseudocode ====\nIn [[C++]]-like [[pseudocode]], this could be written:\n\n<syntaxhighlight lang=\"c++\" line=\"1\">\n// ILP branch and cut solution pseudocode, assuming objective is to be maximized\nILP_solution branch_and_cut_ILP(IntegerLinearProgram initial_problem) {\n    queue active_list; // L, above\n    active_list.enqueue(initial_problem); // step 1\n    // step 2\n    ILP_solution optimal_solution; // this will hold x* above\n    double best_objective = -std::numeric_limits<double>::infinity; // will hold v* above\n    while (!active_list.empty()) { // step 3 above\n        LinearProgram& curr_prob = active_list.dequeue(); // step 3.1\n        do { // steps 3.2-3.7\n            RelaxedLinearProgram& relaxed_prob = LP_relax(curr_prob); // step 3.2\n            LP_solution curr_relaxed_soln = LP_solve(relaxed_prob); // this is x above\n            bool cutting_planes_found = false;\n            if (!curr_relaxed_soln.is_feasible()) { // step 3.3\n                continue; // try another solution; continues at step 3\n            }\n            double current_objective_value = curr_relaxed_soln.value(); // v above\n            if (current_objective_value <= best_objective) { // step 3.4\n                continue; // try another solution; continues at step 3\n            }\n            if (curr_relaxed_soln.is_integer()) { // step 3.5\n                best_objective = current_objective_value;\n                optimal_solution = cast_as_ILP_solution(curr_relaxed_soln);\n                continue; // continues at step 3\n            }\n            // current relaxed solution isn't integral\n            if (hunting_for_cutting_planes) { // step 3.6\n                violated_cutting_planes = search_for_violated_cutting_planes(curr_relaxed_soln);\n                if (!violated_cutting_planes.empty()) { // step 3.6\n                    cutting_planes_found = true; // will continue at step 3.2\n                    for (auto&& cutting_plane : violated_cutting_planes) {\n                        active_list.enqueue(LP_relax(curr_prob, cutting_plane));\n                    }\n                    continue; // continues at step 3.2\n                }\n            }\n            // step 3.7: either violated cutting planes not found, or we weren't looking for them\n            auto&& branched_problems = branch_partition(curr_prob);\n            for (auto&& branch : branched_problems) {\n                active_list.enqueue(branch);\n            }\n            continue; // continues at step 3\n        } while (hunting_for_cutting_planes /* parameter of the algorithm; see 3.6 */\n               && cutting_planes_found);\n        // end step 3.2 do-while loop\n    } // end step 3 while loop\n    return optimal_solution; // step 4\n}\n</syntaxhighlight>\n\nIn the above pseudocode, the functions <code>LP_relax</code>, <code>LP_solve</code> and <code>branch_partition</code> called as subroutines must be provided as applicable to the problem. For example, <code>LP_solve</code> could call the [[revised simplex algorithm|simplex algorithm]].  Branching strategies for <code>branch_partition</code> are discussed below.\n\n==Branching strategies==\n\nAn important step in the branch and cut algorithm is the branching step.  At this step, there are a variety of branching heuristics that can be used.  The branching strategies described below all involve what is called '''branching on a variable.'''<ref>{{Cite journal|last=Achterberg|first=Tobias|last2=Koch|first2=Thorsten|last3=Martin|first3=Alexander|date=2005|title=Branching rules revisited|url=https://linkinghub.elsevier.com/retrieve/pii/S0167637704000501|journal=Operations Research Letters|language=en|volume=33|issue=1|pages=42–54|doi=10.1016/j.orl.2004.04.002|via=Elsevier Science Direct}}</ref>  Branching on a variable involves choosing a variable, <math>x_i</math>, with a fractional value, <math>x_i'</math>, in the optimal solution to the current LP relaxation and then adding the constraints <math>x_i\\le \\lfloor x_i' \\rfloor</math> and <math> x_i \\ge \\lceil x_i' \\rceil</math>\n*'''Most Infeasible Branching''' This branching strategy chooses the variable with the fractional part closest to 0.5.\n*'''Pseudo Cost Branching''' The basic idea of this strategy is to keep track for each variable <math>x_i</math> the change in the objective function when this variable was previously chosen as the variable to branch on.  The strategy then chooses the variable that is predicted to have the most change on the objective function based on past changes when it was chosen as the branching variable.  Note that pseudo cost branching is initially uninformative in the search since few variables have been branched on.\n*'''Strong Branching''' Strong branching involves testing which of the candidate variable gives the best improvement to the objective function before actually branching on them.  '''Full strong branching''' tests all candidate variables and can be computationally expensive.  The computational cost can be reduced by only considering a subset of the candidate variables and not solving each of the corresponding LP relaxations to completion.\n\nThere are also a large number of variations of these branching strategies, such as using strong branching early on when pseudo cost branching is relatively uninformative and then switching to pseudo cost branching later when there is enough branching history for pseudo cost to be informative.\n\n==External links==\n*[https://web.archive.org/web/20050901073653/http://www.cs.sandia.gov/opt/survey/mip.html Mixed Integer Programming]\n*[http://scip.zib.de SCIP]: framework for branch-cut-and-price and a mixed integer programming solver \n*[https://www.informatik.uni-koeln.de/abacus/ ABACUS - A Branch-And-CUt System] - open source software\n*[https://github.com/coin-or/Cbc COIN-OR Cbc] - open source software on [[GitHub]]\n\n==References==\n{{Reflist}}\n\n{{Optimization algorithms|combinatorial|state=expanded}}\n\n[[Category:Combinatorial optimization]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Branch and price",
      "url": "https://en.wikipedia.org/wiki/Branch_and_price",
      "text": "In [[applied mathematics]], '''branch and price''' is a method of [[combinatorial optimization]] for solving [[integer linear programming]] (ILP)  and [[mixed integer linear programming]] (MILP) problems with many variables. The method is a hybrid of [[branch and bound]] and [[column generation]] methods.\n\n==Description of the algorithm==\n\nBranch and price is a branch and bound method in which at each node of the search tree, columns may be added to the [[linear programming relaxation]] (LP relaxation).  At the start of the algorithm, sets of columns are excluded from the LP relaxation in order to reduce the computational and memory requirements and then columns are added back to the LP relaxation as needed. The approach is based on the observation that for large problems most columns will be nonbasic and have their corresponding variable equal to zero in any optimal solution.  Thus, the large majority of the columns are irrelevant for solving the problem.\n\n[[File:branch and price diagram.png|400px|thumb|Outline of branch and price algorithm.  Adapted from <ref name=lectureSlides />]]\n\nThe algorithm typically begins by using a reformulation, such as [[Dantzig–Wolfe decomposition]], to form what is known as the '''Master Problem'''.  The decomposition is performed to obtain a problem formulation that gives better bounds when the relaxation is solved than when the relaxation of the original formulation is solved.  But, the decomposition usually contains many variables and so a modified version, called the '''Restricted Master Problem''', that only considers a subset of the columns is solved.<ref name=tutorial>{{cite journal|last=Feillet|first=Dominique|title=A tutorial on column generation and branch-and-price for vehicle routing problems|journal=4OR|year=2010|volume=8|issue=4|pages=407–424|doi=10.1007/s10288-010-0130-z}}</ref>  Then, to check for optimality, a subproblem called the '''pricing problem''' is solved to find columns that can enter the basis and reduce the objective function (for a minimization problem).  This involves finding a column that has a negative [[reduced cost]].  Note that the pricing problem itself may be difficult to solve but since it is not necessary to find the column with the most negative reduced cost, heuristic and local search methods can be used.<ref name=multicoloring>{{cite book|last=Mehrota|first=Anuj|author2=M.A. Trick|author2-link= Michael Trick |title=A Branch-and-price Approach for Graph Multi-Coloring|journal=Extending the Horizons: Advances in Computing, Optimization, and Decision Technologies|volume=37|year=2007|pages=15–29|doi=10.1007/978-0-387-48793-9_2|citeseerx=10.1.1.163.6870|series=Operations Research/Computer Science Interfaces Series|isbn=978-0-387-48790-8}}</ref>  The subproblem must only be solved to completion in order to prove that an optimal solution to the Restricted Master Problem is also an optimal solution to the Master Problem.  Each time a column is found with negative reduced cost, it is added to the Restricted Master Problem and the relaxation is reoptimized.  If no columns can enter the basis and the solution to the relaxation is not integer, then branching occurs.<ref name=lectureSlides>{{cite web|last=Akella|first=M.|author2=S. Gupta|author3=A. Sarkar|title=Branch and Price: Column Generation for Solving Huge Integer Programs|url=http://www.acsu.buffalo.edu/~nagi/courses/684/price.pdf|access-date=2012-12-19|archive-url=https://web.archive.org/web/20100821132913/http://www.acsu.buffalo.edu/~nagi/courses/684/price.pdf|archive-date=2010-08-21|dead-url=yes}}</ref>\n\nMost branch and price algorithms are problem specific since the problem must be formulated in such a way so that effective branching rules can be formulated and so that the pricing problem is relatively easy to solve.<ref>{{cite web|last=Lubbecke|first=M.|title=Generic Branch-Cut-and-Price|url=http://www.zib.de/gamrath/publications/gamrath2010_genericBCP.pdf}}</ref> \n\nIf cutting planes are used to tighten LP relaxations within a branch and price algorithm, the method is known as '''branch price and cut'''.<ref name=branchPriceAndCut>{{cite journal|last=Desrosiers|first=J.|author2=M.E. Lubbecke|title=Branch-Price-and-Cut Algorithms|journal=Wiley Encyclopedia of Operations Research and Management Science|year=2010}}</ref>\n\n==Applications of branch and price==\n\nThe branch and price method can be used to solve problems in a variety of application areas, including:\n*Graph multi-coloring.<ref name=multicoloring />   This is a generalization of the [[graph coloring]] problem in which each node in a graph must be assigned a preset number of colors and any nodes that share an edge cannot have a color in common.  The objective is then to find the minimum number of colors needed to have a valid coloring.  The multi-coloring problem can be used to model a variety of applications including job scheduling and telecommunication channel assignment.\n*[[Vehicle routing problem|Vehicle routing problems]].<ref name=tutorial />  \n*[[Generalized assignment problem]].<ref>{{cite journal|last=Savelsbergh|first=M.|title=A branch-and-price algorithm for the generalized assignment problem|journal=Operations Research|volume=45|issue=6|pages=831–841|year=1997|series=831-841|doi=10.1287/opre.45.6.831 }}</ref>\n\n==See also==\n*[[Branch and cut]]\n*[[Branch and bound]]\n*[[Delayed column generation]]\n\n==External references==\n*[https://web.archive.org/web/20100821132913/http://www.acsu.buffalo.edu/~nagi/courses/684/price.pdf Lecture slides on branch and price]\n*[http://www.lancs.ac.uk/staff/letchfoa/talks/BranchAndPriceI.pdf More lecture slides]{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n*[http://www.lancs.ac.uk/staff/letchfoa/talks/BranchAndPriceII.pdf Even more lecture slides]{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n*[https://wiki.bordeaux.inria.fr/realopt/pmwiki.php/Project/BaPCod Prototype code for a generic branch and price algorithm]\n*[http://branchandcut.org/faq.htm BranchAndCut.org FAQ]\n*[http://scip.zib.de SCIP] an open source framework for branch-cut-and-price and a mixed integer programming solver \n*[http://www.informatik.uni-koeln.de/abacus/ ABACUS – A Branch-And-CUt System] – open source software\n\n==References==\n{{Reflist}}\n*{{citation\n | last1 = Barnhart | first1 = Cynthia\n | last2 = Johnson | first2 = Ellis L.\n | last3 = Nemhauser | first3 = George L. | author3-link = George Nemhauser\n | last4 = Savelsbergh | first4 = Martin W. P.\n | last5 = Vance | first5 = Pamela H.\n | issue = 3\n | journal = Operations Research\n | jstor = 222825\n | pages = 316–329\n | title = Branch-and-price: column generation for solving huge integer programs\n | volume = 46\n | year = 1998 | doi=10.1287/opre.46.3.316| citeseerx = 10.1.1.197.7390\n }}\n\n{{Optimization algorithms}}\n\n[[Category:Combinatorial optimization]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Bregman Lagrangian",
      "url": "https://en.wikipedia.org/wiki/Bregman_Lagrangian",
      "text": "{{Multiple issues|\n{{Underlinked|date=October 2016}}\n{{Orphan|date=October 2016}}\n{{notability|date=January 2017}}\n}}\n\nThe [[Bregman method|Bregman]]-[[Joseph-Louis Lagrange|Lagrangian]] framework permits a systematic understanding of the matching rates associated with higher-order [[Gradient method|gradient methods]] in discrete and continuous time.<ref name=\"Michael I. Jordan\">{{cite news |title=A Variational Perspective on Accelerated Methods in Optimization |url=https://arxiv.org/pdf/1603.04245v1.pdf |publisher=''[[Cornell University]]'' |date=March 14, 2016 |accessdate=October 4, 2016}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{physics-stub}}"
    },
    {
      "title": "Bregman method",
      "url": "https://en.wikipedia.org/wiki/Bregman_method",
      "text": "{{unreferenced|date=March 2009}}\n\n'''Bregman's method''' is an [[iterative algorithm]] to solve certain [[convex optimization]] problems.  The algorithm is a [[row-action method]] accessing [[constraint function]]s one by one and the method is particularly suited for large optimization problems where constraints can be efficiently enumerated. The original version is due to [[Lev M. Bregman]].<ref>Bregman L. \"A Relaxation Method of Finding a Common Point of Convex Sets and its Application to Problems of Optimization\". Dokl. Akad. Nauk SSSR, v. 171, No. 5, 1966, p.p. 1019-1022. (English translation: Soviet Math. Dokl., v. 7, 1966, p.p. 1578-1581)</ref>\n\nThe algorithm starts with a pair of primal and dual variables.  Then, for each constraint a [[Bregman divergence|generalized projection]] onto its feasible set is performed, updating both the constraint's dual variable and all primal variables for which there are non-zero coefficients in the constraint functions gradient.  In case the objective is strictly convex and all constraint functions are convex, the limit of this iterative projection converges to the optimal primal dual pair.\n\nThe method has links to the [[method of multipliers]] and [[dual ascent method]] and multiple generalizations exist.\n\nOne drawback of the method is that it is only provably convergent if the objective function is ''strictly'' convex.  In case this can not be ensured, as for [[linear program]]s or non-strictly convex quadratic programs, additional methods such as [[proximal gradient method]]s have been developed.\n\n==External links==\n* [http://www.caam.rice.edu/~optimization/L1/bregman/ The Bregman Iterative Procedure at Rice]\n\n==References==\n{{refs}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Convex optimization]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Broyden–Fletcher–Goldfarb–Shanno algorithm",
      "url": "https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm",
      "text": "{{Redirect|BFGS|the Canadian hardcore punk band|Bunchofuckingoofs}}\n{{Multiple issues|\n{{Technical|date=September 2010}}\n{{Refimprove|date=March 2016}}\n}}\n\nIn [[numerical analysis|numerical]] [[optimization (mathematics)|optimization]], the '''Broyden–Fletcher–Goldfarb–Shanno''' ('''BFGS''') '''algorithm''' is an [[iterative method]] for solving unconstrained [[nonlinear optimization]] problems.<ref>{{Citation | last1=Fletcher | first1=Roger | title=Practical methods of optimization | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0-471-91547-8 | year=1987}}</ref>\n\nThe BFGS method belongs to [[quasi-Newton methods]], a class of [[hill climbing|hill-climbing optimization]] techniques that seek a [[stationary point]] of a (preferably twice continuously differentiable) function. For such problems, a [[Kuhn–Tucker conditions|necessary condition for optimality]] is that the [[gradient]] be zero. [[Newton's method in optimization|Newton's method]] and the BFGS methods are not guaranteed to converge unless the function has a quadratic [[Taylor expansion]] near an [[Local optimum|optimum]]. However, BFGS can have acceptable performance even for non-smooth optimization instances.<ref>{{citation |last=Curtis |first=Frank E. |first2=Xiaocun |last2=Que |title=A Quasi-Newton Algorithm for Nonconvex, Nonsmooth Optimization with Global Convergence Guarantees |journal=Mathematical Programming Computation |volume=7 |issue=4 |year=2015 |pages=399–428 |doi=10.1007/s12532-015-0086-2 }}</ref>\n\nIn quasi-Newton methods, the [[Hessian matrix]] of second [[derivative]]s is not computed.  Instead, the Hessian matrix is approximated using updates specified by gradient evaluations (or approximate gradient evaluations). [[Quasi-Newton methods]] are generalizations of the [[secant method]] to find the root of the first derivative for multidimensional problems. In multi-dimensional problems, the secant equation does not specify a unique solution, and quasi-Newton methods differ in how they constrain the solution. The BFGS method is one of the most popular members of this class.<ref>{{harvtxt|Nocedal|Wright|2006}}, page 24</ref> Also in common use is [[L-BFGS]], which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). The BFGS-B variant handles simple box constraints.<ref>{{Citation|url=http://www.ece.northwestern.edu/~nocedal/PSfiles/limited.ps.gz |pages= 1190–1208|doi=10.1137/0916069|title= A Limited Memory Algorithm for Bound Constrained Optimization|journal= SIAM Journal on Scientific Computing|volume= 16|issue= 5|year= 1995|last1= Byrd|first1= Richard H.|last2= Lu|first2= Peihuang|last3= Nocedal|first3= Jorge|last4= Zhu|first4= Ciyou |citeseerx= 10.1.1.645.5814}}</ref>\n\nThe algorithm is named after [[Charles George Broyden]], [[Roger Fletcher (mathematician)|Roger Fletcher]], [[Donald Goldfarb]] and [[David Shanno]].<ref>{{Citation| last=Broyden | first=C. G. | authorlink=Charles George Broyden | year=1970 | title=The convergence of a class of double-rank minimization algorithms | journal=Journal of the Institute of Mathematics and Its Applications | volume=6 | pages=76–90 | doi=10.1093/imamat/6.1.76}}</ref><ref>{{Citation | last1=Fletcher|first1= R.|title= A New Approach to Variable Metric Algorithms|journal=Computer Journal |year=1970|volume=13|pages=317–322 | doi=10.1093/comjnl/13.3.317 | issue=3}}</ref><ref>{{Citation|author-link=Donald Goldfarb|last=Goldfarb|first= D.|title=A Family of Variable Metric Updates Derived by Variational Means|journal=Mathematics of Computation|year=1970|volume=24|pages=23–26|doi=10.1090/S0025-5718-1970-0258249-6|issue=109}}</ref><ref>{{Citation | last1=Shanno|first1= David F.|title=Conditioning of quasi-Newton methods for function minimization|date=July 1970|journal=Mathematics of Computation|volume=24|pages= 647–656|mr=274029|doi=10.1090/S0025-5718-1970-0274029-X | issue=111 }}</ref>\n\n== Rationale ==\nThe optimization problem is to minimize <math>f(\\mathbf{x})</math>, where <math>\\mathbf{x}</math> is a vector in <math>\\mathbb{R}^n</math>, and <math>f</math> is a differentiable scalar function. There are no constraints on the values that <math>\\mathbf{x}</math> can take.\n\nThe algorithm begins at an initial estimate for the optimal value <math>\\mathbf{x}_0</math> and proceeds iteratively to get a better estimate at each stage.\n\nThe search direction '''p'''<sub>''k''</sub> at stage ''k'' is given by the solution of the analogue of the Newton equation:\n\n:<math>B_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k),</math>\n\nwhere <math>B_k</math> is an approximation to the [[Hessian matrix]], which is updated iteratively at each stage, and  <math>\\nabla f(\\mathbf{x}_k)</math> is the gradient of the function evaluated at '''x'''<sub>''k''</sub>.  A [[line search]] in the direction '''p'''<sub>''k''</sub> is then used to find the next point '''x'''<sub>''k''+1</sub> by minimizing <math>f(\\mathbf{x}_k + \\gamma\\mathbf{p}_k)</math> over the scalar <math>\\gamma > 0.</math>\n\nThe quasi-Newton condition imposed on the update of <math>B_k</math> is\n\n:<math>B_{k+1} (\\mathbf{x}_{k+1} - \\mathbf{x}_k) = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k).</math>\n\nLet <math>\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)</math> and <math>\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k</math>, then <math>B_{k+1}</math> satisfies <math>B_{k+1} \\mathbf{s}_k = \\mathbf{y}_k</math>, which is the secant equation. The curvature condition <math>\\mathbf{s}_k^\\top \\mathbf{y}_k > 0</math> should be satisfied for <math>B_{k+1}</math> to be positive definite, which can be verified by pre-multiplying the secant equation with <math>\\mathbf{s}_k^T</math>. If the function is not strongly convex, then the condition has to be enforced explicitly.\n\nInstead of requiring the full Hessian matrix at the point <math>\\mathbf{x}_{k+1}</math> to be computed as <math>B_{k+1}</math>, the approximate Hessian at stage ''k'' is updated by the addition of two matrices:\n\n:<math>B_{k+1} = B_k + U_k + V_k.</math>\n\nBoth <math>U_k</math> and <math>V_k</math> are symmetric rank-one matrices, but their sum is a rank-two update matrix. BFGS and [[Davidon–Fletcher–Powell formula|DFP]] updating matrix both differ from its predecessor by a rank-two matrix. Another simpler rank-one method is known as [[symmetric rank-one]] method, which does not guarantee the [[positive definiteness]]. In order to maintain the symmetry and positive definiteness of <math>B_{k+1}</math>, the update form can be chosen as <math>B_{k+1} = B_k + \\alpha\\mathbf{u}\\mathbf{u}^\\top + \\beta\\mathbf{v}\\mathbf{v}^\\top</math>. Imposing the secant condition, <math>B_{k+1} \\mathbf{s}_k = \\mathbf{y}_k </math>. Choosing <math>\\mathbf{u} = \\mathbf{y}_k</math> and <math>\\mathbf{v} = B_k \\mathbf{s}_k</math>, we can obtain:<ref>{{Citation | last1=Fletcher | first1=Roger | title=Practical methods of optimization | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0-471-91547-8 | year=1987}}</ref>\n\n:<math>\\alpha = \\frac{1}{\\mathbf{y}_k^T \\mathbf{s}_k},</math>\n:<math>\\beta = -\\frac{1}{\\mathbf{s}_k^T  B_k \\mathbf{s}_k}.</math>\n\nFinally, we substitute <math>\\alpha</math> and <math>\\beta</math> into <math>B_{k+1} = B_k + \\alpha\\mathbf{u}\\mathbf{u}^\\top + \\beta\\mathbf{v}\\mathbf{v}^\\top</math> and get the update equation of <math>B_{k+1}</math>:\n\n:<math>B_{k+1} = B_k + \\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} - \\frac{B_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} B_k^{\\mathrm{T}} }{\\mathbf{s}_k^{\\mathrm{T}} B_k \\mathbf{s}_k}.</math>\n\n== Algorithm ==\n\nFrom an initial guess <math>\\mathbf{x}_0</math> and an approximate Hessian matrix <math>B_0</math> the following steps are repeated as <math>\\mathbf{x}_k</math> converges to the solution:\n# Obtain a direction <math>\\mathbf{p}_k</math> by solving <math>B_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)</math>.\n# Perform a one-dimensional optimization ([[line search]]) to find an acceptable stepsize <math>\\alpha_k</math> in the direction found in the first step, so <math>\\alpha_k=\\arg \\min f(\\mathbf{x}_k+\\alpha\\mathbf{p}_k)</math>.\n# Set <math> \\mathbf{s}_k = \\alpha_k \\mathbf{p}_k</math> and update <math>\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{s}_k</math>.\n# <math>\\mathbf{y}_k = {\\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)}</math>.\n# <math>B_{k+1} = B_k + \\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} - \\frac{B_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} B_k^{\\mathrm{T}} }{\\mathbf{s}_k^{\\mathrm{T}} B_k \\mathbf{s}_k}</math>.\n\n<math>f(\\mathbf{x})</math> denotes the objective function to be minimized. Convergence can be checked by observing the norm of the gradient, <math>||\\nabla f(\\mathbf{x}_k)||</math>.  If <math>B_0</math>is initialized with <math>B_0 = I</math>, the first step will be equivalent to a [[gradient descent]], but further steps are more and more refined by <math>B_{k}</math>, the approximation to the Hessian.\n\nThe first step of the algorithm is carried out using the inverse of the matrix <math>B_k</math>, which can be obtained efficiently by applying the [[Sherman–Morrison formula]] to the step 5 of the algorithm, giving\n\n: <math>B_{k+1}^{-1} = \\left(I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) B_{k}^{-1} \\left(I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k}.</math>\n\nThis can be computed efficiently without temporary matrices, recognizing that <math>B_k^{-1}</math> is symmetric,\nand that <math>\\mathbf{y}_k^{\\mathrm{T}} B_k^{-1} \\mathbf{y}_k</math> and <math>\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k</math> are scalars, using an expansion such as\n\n: <math>B_{k+1}^{-1} = B_k^{-1} + \\frac{(\\mathbf{s}_k^{\\mathrm{T}}\\mathbf{y}_k+\\mathbf{y}_k^{\\mathrm{T}} B_k^{-1} \\mathbf{y}_k)(\\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}})}{(\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k)^2} - \\frac{B_k^{-1} \\mathbf{y}_k \\mathbf{s}_k^{\\mathrm{T}} + \\mathbf{s}_k \\mathbf{y}_k^{\\mathrm{T}}B_k^{-1}}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k}.</math>\n\nIn statistical estimation problems (such as maximum likelihood or Bayesian inference), [[credible interval]]s or [[confidence interval]]s for the solution can be estimated from the [[matrix inverse|inverse]] of the final Hessian matrix. However, these quantities are technically defined by the true Hessian matrix, and the BFGS approximation may not converge to the true Hessian matrix.<ref>{{Citation | last1=Ren-pu | last2=Powell| first1=GE| first2=M.J.D | title=The Convergence of Variable Metric Matrices in Unconstrained Optimization | publisher=[[Mathematical Programming]] | location=North-Holland | edition=27| year=1982}}</ref>\n\n== Notable implementations ==\n\n* The large scale nonlinear optimization software [[Artelys Knitro]] implements, among others, both BFGS and L-BFGS algorithms.\n* The [[GNU Scientific Library|GSL]] implements BFGS as gsl_multimin_fdfminimizer_vector_bfgs2.<ref>https://www.gnu.org/software/gsl/manual/html_node/Multimin-Algorithms-with-Derivatives.html</ref>\n* In the MATLAB [[Optimization Toolbox]], the fminunc function<ref>http://www.mathworks.com/help/toolbox/optim/ug/fminunc.html</ref> uses BFGS with cubic [[line search]] when the problem size is set to \"medium scale.\"<ref>https://web.archive.org/web/20101028140551/http://www.mathworks.com/help/toolbox/optim/ug/brnoxr7-1.html#brnpcye</ref>\n* In [[R (programming language)|R]], the BFGS algorithm (and the L-BFGS-B version that allows box constraints) is implemented as an option of the base function optim().<ref>https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html</ref>\n* In [[SciPy]], the scipy.optimize.fmin_bfgs function implements BFGS.<ref>http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html#scipy.optimize.fmin_bfgs</ref> It is also possible to run BFGS using any of the [[L-BFGS]] algorithms by setting the parameter L to a very large number.\n\n== See also ==\n{{Div col|colwidth=25em}}\n* [[BHHH algorithm]]\n* [[Davidon–Fletcher–Powell formula]]\n* [[Gradient descent]]\n* [[L-BFGS]]\n* [[Levenberg–Marquardt algorithm]]\n* [[Nelder–Mead method]]\n* [[Pattern search (optimization)]]\n* [[Quasi-Newton methods]]\n* [[Symmetric rank-one]]\n{{Div col end}}\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* {{Citation | last1=Avriel |first1=Mordecai |year=2003|title=Nonlinear Programming: Analysis and Methods|publisher= Dover Publishing|isbn= 978-0-486-43227-4}}\n* {{Citation|last1=Bonnans |first1=J.&nbsp;Frédéric |last2=Gilbert |first2=J.&nbsp;Charles |last3=Lemaréchal |first3=Claude |authorlink3=Claude Lemaréchal |last4=Sagastizábal |first4=Claudia&nbsp;A. |author4-link=Claudia Sagastizábal |title=Numerical Optimization: Theoretical and Practical Aspects |edition=Second |publisher=Springer |location=Berlin|year=2006 |isbn=3-540-35445-X |chapter=Newtonian Methods |pages=51–66 }}\n* {{citation |first=J. E., Jr. |last=Dennis |authorlink=John E. Dennis |first2=Robert B. |last2=Schnabel |authorlink2=Robert B. Schnabel |title=Numerical Methods for Unconstrained Optimization and Nonlinear Equations |chapter=Secant Methods for Unconstrained Minimization |location=Englewood Cliffs, NJ |publisher=Prentice-Hall |year=1983 |isbn=0-13-627216-9 |pages=194–215 }}\n* {{Citation | last1=Fletcher | first1=Roger | title=Practical Methods of Optimization | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0-471-91547-8 | year=1987}}\n* {{Citation|last1=Luenberger|first1=David G.|authorlink1=David G. Luenberger|last2=Ye|first2=Yinyu|authorlink2=Yinyu Ye|title=Linear and nonlinear programming|edition=Third|series=International Series in Operations Research & Management Science|volume=116|publisher=Springer|location=New York|year=2008|pages=xiv+546|isbn=978-0-387-74502-2| mr = 2423726}}\n* {{citation |last=Kelley |first=C. T. |title=Iterative Methods for Optimization |location=Philadelphia |publisher=Society for Industrial and Applied Mathematics |year=1999 |isbn=0-89871-433-8 |pages=71–86 }}\n* {{Citation | last1=Nocedal | first1=Jorge | last2=Wright | first2=Stephen J. | title=Numerical Optimization | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-30303-1 | year=2006}}\n* {{Citation | last1=Ren-pu | last2=Powell| first1=GE | first2=M.J.D | title=The Convergence of Variable Metric Matrices in Unconstrained Optimization | publisher=[[Mathematical Programming]] | location=North-Holland | edition=27| year=1982}}</ref>.\n\n{{Optimization algorithms|unconstrained}}\n\n{{DEFAULTSORT:Broyden-Fletcher-Goldfarb-Shanno algorithm}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "CMA-ES",
      "url": "https://en.wikipedia.org/wiki/CMA-ES",
      "text": "'''CMA-ES''' stands for ''covariance matrix adaptation evolution strategy''. [[Evolution strategies]] (ES) are [[stochastic]], [[Derivative-free optimization|derivative-free methods]] for [[numerical optimization]]  of non-[[Linear map|linear]] or non-[[Convex function|convex]] [[continuous optimization]] problems. They belong to the class of [[evolutionary algorithms]] and [[evolutionary computation]].  An [[evolutionary algorithm]] is broadly based on the principle of [[biological evolution]], namely the repeated interplay of variation (via recombination and mutation) and selection: in each generation (iteration) new individuals (candidate solutions, denoted as <math>x</math>) are generated by variation, usually in a stochastic way, of the current parental individuals. Then, some individuals are selected to become the parents in the next generation based on their fitness or [[objective function]] value <math>f(x)</math>. Like this, over the generation sequence, individuals with better and better <math>f</math>-values are generated.\n\nIn an [[evolution strategy]], new candidate solutions are sampled according to a [[multivariate normal distribution]] in <math>\\mathbb{R}^n</math>. Recombination amounts to selecting a new mean value for the distribution. Mutation amounts to adding a random vector, a perturbation with zero mean. Pairwise dependencies between the variables in the distribution are represented by a [[covariance matrix]]. The covariance matrix adaptation (CMA) is a method to update the [[covariance matrix]] of this distribution. This is particularly useful if the function <math>f</math> is [[ill-conditioned]].\n\nAdaptation of the [[covariance matrix]] amounts to learning a second order model of the underlying [[objective function]] similar to the approximation of the inverse [[Hessian matrix]] in the [[Quasi-Newton method]] in classical [[Optimization (mathematics)|optimization]]. In contrast to most classical methods, fewer assumptions on the nature of the underlying objective function are made. Only the ranking between candidate solutions is exploited for learning the sample distribution and neither derivatives nor even the function values themselves are required by the method.\n\n== Principles ==\n[[Image:Concept of directional optimization in CMA-ES algorithm.png|thumb|right|400px|Illustration of an actual optimization run with covariance matrix adaptation on a simple two-dimensional problem. The spherical optimization landscape is depicted with solid lines of equal <math>f</math>-values. The population (dots) is much larger than necessary, but clearly shows how the distribution of the population (dotted line) changes during the optimization. On this simple problem, the population concentrates over the global optimum within a few generations.]]\nTwo main principles for the adaptation of parameters of the search distribution are exploited in the CMA-ES algorithm.\n\nFirst, a [[maximum-likelihood]] principle, based on the idea to increase the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the [[likelihood]] of previously successful candidate solutions is maximized. The [[covariance matrix]] of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a [[Information geometry#Natural gradient|natural gradient]] descent. Also, in consequence, the CMA conducts an iterated [[principal components analysis]] of successful search steps while retaining ''all'' principal axes. [[Estimation of Distribution Algorithms|Estimation of distribution algorithms]] and the [[Cross-Entropy Method]] are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution ''points'' instead of successful search ''steps''.\n\nSecond, two paths of the time evolution of the distribution mean of the strategy are recorded, called search or evolution paths. These paths contain significant information about the correlation between consecutive steps. Specifically, if consecutive steps are taken in a similar direction, the evolution paths become long. The evolution paths are exploited in two ways. One path is used for the covariance matrix adaptation procedure in place of single successful search steps and facilitates a possibly much faster variance increase of favorable directions. The other path is used to conduct an additional step-size control. This step-size control aims to make consecutive movements of the distribution mean orthogonal in expectation. The step-size control effectively prevents [[premature convergence]] yet allowing fast convergence to an optimum.\n\n== Algorithm ==\nIn the following the most commonly used (μ/μ<sub>w</sub>, λ)-CMA-ES is outlined, where in each iteration step a weighted combination of the μ best out of λ new candidate solutions is used to update the distribution parameters. The main loop consists of three main parts: 1) sampling of new solutions, 2) re-ordering of the sampled solutions based on their fitness, 3) update of the internal state variables based on the re-ordered samples. A [[pseudocode]] of the algorithm looks as follows.\n\n  '''set''' <math>\\lambda</math>  // number of samples per iteration, at least two, generally > 4\n  '''initialize''' <math>m</math>, <math>\\sigma</math>, <math>C=I</math>, <math>p_\\sigma=0</math>, <math>p_c=0</math>  // initialize state variables\n  '''while''' ''not terminate''  // iterate\n     '''for''' <math>i</math> '''in''' <math>\\{1...\\lambda\\}</math>  // sample <math>\\lambda</math> new solutions and evaluate them\n        <math>x_i</math> = sample_multivariate_normal(mean=<math>m</math>, covariance_matrix=<math>\\sigma^2 C </math>)\n        <math>f_i</math> = fitness(<math>x_i</math>)\n     <math>x_{1...\\lambda}</math> &larr; <math>x_{s(1)...s(\\lambda)}</math> with <math>s(i)</math> = argsort(<math>f_{1...\\lambda}</math>, <math>i</math>)  // sort solutions\n     <math>m'</math> = <math>m</math>  // we need later <math>m - m'</math> and <math>x_i - m'</math>       \n     <math>m</math> &larr; update_m<math>(x_1, ... ,</math> <math>x_\\lambda)</math>  // move mean to better solutions \n     <math>p_\\sigma</math> &larr; update_ps<math>(p_\\sigma,</math> <math>\\sigma^{-1} C^{-1/2} (m - m'))</math>  // update isotropic evolution path\n     <math>p_c</math> &larr; update_pc<math>(p_c,</math> <math>\\sigma^{-1}(m - m'),</math> <math>||p_\\sigma||)</math>  // update anisotropic evolution path\n     <math>C</math> &larr; update_C<math>(C,</math> <math>p_c,</math> <math>{(x_1 - m')}/{\\sigma},... ,</math> <math>{(x_\\lambda - m')}/{\\sigma})</math>  // update covariance matrix\n     <math>\\sigma</math> &larr; update_sigma<math>(\\sigma,</math> <math>||p_\\sigma||)</math>  // update step-size using isotropic path length\n  '''return''' <math>m</math> or <math>x_1</math>\n\nThe order of the last four update assignments is relevant. In the following, the update equations for the five state variables are specified.\n\nGiven are the search space dimension <math>n</math> and the iteration step <math>k</math>. The five state variables are\n\n: <math>m_k\\in\\mathbb{R}^n</math>, the distribution mean and current favorite solution to the optimization problem,\n\n: <math>\\sigma_k>0</math>, the step-size,\n\n: <math> C_k</math>, a symmetric and [[Positive-definite matrix|positive definite]] <math>n\\times n</math> [[covariance matrix]] with <math> C_0 = I</math> and\n\n: <math> p_\\sigma\\in\\mathbb{R}^n, p_c\\in\\mathbb{R}^n</math>, two evolution paths, initially set to the zero vector.\n\nThe iteration starts with sampling <math>\\lambda>1</math> candidate solutions <math>x_i\\in\\mathbb{R}^n </math> from a [[multivariate normal distribution]] <math>\\textstyle \\mathcal{N}(m_k,\\sigma_k^2 C_k)</math>, i.e. \nfor <math>i=1,...,\\lambda</math>\n\n:: <math>\n\\begin{align}\n  x_i \\ &\\sim\\ \\mathcal{N}(m_k,\\sigma_k^2 C_k)      \n     \\\\&\\sim\\ m_k + \\sigma_k\\times\\mathcal{N}(0,C_k) \n\\end{align}\n  </math>\n\nThe second line suggests the interpretation as perturbation (mutation) of the current favorite solution vector <math>m_k</math> (the distribution mean vector). The candidate solutions <math> x_i</math> are evaluated on the objective function <math>f:\\mathbb{R}^n\\to\\mathbb{R}</math> to be minimized.  Denoting the <math>f</math>-sorted candidate solutions as\n\n: <math>\n    \\{x_{i:\\lambda}\\;|\\;i=1\\dots\\lambda\\} = \\{x_i\\;|\\;i=1\\dots\\lambda\\} \\;\\;\\text{and}\\;\\; \n     f(x_{1:\\lambda})\\le\\dots\\le f(x_{\\mu:\\lambda})\\le f(x_{\\mu+1:\\lambda}) \\dots,       \n  </math>\n\nthe new mean value is computed as\n\n:: <math>\n\\begin{align}\n  m_{k+1} &= \\sum_{i=1}^{\\mu} w_i\\, x_{i:\\lambda} \n  \\\\ &= m_k + \\sum_{i=1}^{\\mu} w_i\\, (x_{i:\\lambda} - m_k) \n\\end{align}\n  </math>\n\nwhere the positive (recombination) weights <math> w_1 \\ge w_2 \\ge \\dots \\ge w_\\mu > 0</math> sum to one. Typically, <math>\\mu \\le \\lambda/2</math> and the weights are chosen such that <math>\\textstyle \\mu_w := 1 / \\sum_{i=1}^\\mu w_i^2 \\approx \\lambda/4</math>. The only feedback used from the objective function here and in the following is an ordering of the sampled candidate solutions due to the indices <math>i:\\lambda</math>.\n\nThe step-size <math>\\sigma_k</math> is updated using ''cumulative step-size adaptation'' (CSA), sometimes also denoted as ''path length control''. The evolution path (or search path) <math>p_\\sigma</math> is updated first.\n\n:: <math>\n  p_\\sigma \\gets \\underbrace{(1-c_\\sigma)}_{\\!\\!\\!\\!\\!\\text{discount factor}\\!\\!\\!\\!\\!}\\, p_\\sigma \n    + \\overbrace{\\sqrt{1 - (1-c_\\sigma)^2}}^{\n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\text{complements for discounted variance}\n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!} \\underbrace{\\sqrt{\\mu_w} \n     \\,C_k^{\\;-1/2} \\, \\frac{\\overbrace{m_{k+1} - m_k}^{\\!\\!\\!\\text{displacement of}\\; m\\!\\!\\!}}{\\sigma_k}}_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n                      \\text{distributed as}\\; \\mathcal{N}(0,I)\\;\\text{under neutral selection}\n                      \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\n  </math>\n:: <math>\n  \\sigma_{k+1} = \\sigma_k \\times \\exp\\bigg(\\frac{c_\\sigma}{d_\\sigma}\n                          \\underbrace{\\left(\\frac{\\|p_\\sigma\\|}{E\\|\\mathcal{N}(0,I)\\|} - 1\\right)}_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n    \\text{unbiased about 0 under neutral selection}\n    \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n}\\bigg)\n  </math>\n\nwhere\n\n: <math>c_\\sigma^{-1}\\approx n/3</math> is the backward time horizon for the evolution path <math>p_\\sigma</math> and larger than one (<math>c_\\sigma \\ll 1</math> is reminiscent of an [[exponential decay]] constant as <math>(1-c_\\sigma)^k\\approx\\exp(-c_\\sigma k)</math> where <math>c_\\sigma^{-1}</math> is the associated lifetime and <math>c_\\sigma^{-1}\\ln(2)\\approx0.7c_\\sigma^{-1}</math> the half-life),\n\n: <math>\\mu_w=\\left(\\sum_{i=1}^\\mu w_i^2\\right)^{-1}</math> is the variance effective selection mass and <math>1 \\le \\mu_w \\le \\mu</math> by definition of <math>w_i</math>,\n\n: <math>C_k^{\\;-1/2} = \\sqrt{C_k}^{\\;-1} = \\sqrt{C_k^{\\;-1}}</math> is the unique symmetric [[Square root of a matrix|square root]] of the [[Invertible matrix|inverse]] of <math>C_k</math>, and\n\n: <math>d_\\sigma</math> is the damping parameter usually close to one. For <math>d_\\sigma=\\infty</math> or <math>c_\\sigma=0</math> the step-size remains unchanged.\n\nThe step-size <math>\\sigma_k</math> is increased if and only if <math>\\|p_\\sigma\\|</math> is larger than the [[expected value]]\n\n: <math>\\begin{align}E\\|\\mathcal{N}(0,I)\\| &= \\sqrt{2}\\,\\Gamma((n+1)/2)/\\Gamma(n/2) \n  \\\\&\\approx \\sqrt{n}\\,(1-1/(4\\,n)+1/(21\\,n^2)) \\end{align}</math>\n\nand decreased if it is smaller. For this reason, the step-size update tends to make consecutive steps [[Conjugate gradient#The conjugate gradient method as a direct method|<math>C_k^{-1}</math>-conjugate]], in that  after the adaptation has been successful <math>\\textstyle\\left(\\frac{m_{k+2}-m_{k+1}}{\\sigma_{k+1}}\\right)^T\\! C_k^{-1} \\frac{m_{k+1}-m_{k}}{\\sigma_k} \\approx 0</math>.<ref>{{Citation\n| first = N.\n| last = Hansen\n| chapter = The CMA evolution strategy: a comparing review\n| title = Towards a new evolutionary computation. Advances on estimation of distribution algorithms\n| pages = 1769–1776\n| publisher = Springer\n| year = 2006\n| location = \n| id = \n}}</ref>\n\nFinally, the [[covariance matrix]] is updated, where again the respective evolution path is updated first.\n\n:: <math>\n  p_c \\gets \\underbrace{(1-c_c)}_{\\!\\!\\!\\!\\!\\text{discount factor}\\!\\!\\!\\!\\!}\\, \n            p_c + \n     \\underbrace{\\mathbf{1}_{[0,\\alpha\\sqrt{n}]}(\\|p_\\sigma\\|)}_{\\text{indicator function}} \n     \\overbrace{\\sqrt{1 - (1-c_c)^2}}^{\n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\text{complements for discounted variance}\n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\n     \\underbrace{\\sqrt{\\mu_w} \n     \\, \\frac{m_{k+1} - m_k}{\\sigma_k}}_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n                      \\text{distributed as}\\; \\mathcal{N}(0,C_k)\\;\\text{under neutral selection}\n                      \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\n  </math>\n\n:: <math>\n  C_{k+1} = \\underbrace{(1 - c_1 - c_\\mu + c_s)}_{\\!\\!\\!\\!\\!\\text{discount factor}\\!\\!\\!\\!\\!}\n               \\, C_k + c_1 \\underbrace{p_c p_c^T}_{\n   \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n   \\text{rank one matrix}\n   \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!} \n         + \\,c_\\mu \\underbrace{\\sum_{i=1}^\\mu w_i \\frac{x_{i:\\lambda} - m_k}{\\sigma_k} \n             \\left( \\frac{x_{i:\\lambda} - m_k}{\\sigma_k} \\right)^T}_{\n                     \\text{rank} \\;\\min(\\mu,n)\\; \\text{matrix}}\n  </math>\n\nwhere <math>T</math> denotes the transpose and\n\n: <math>c_c^{-1}\\approx n/4</math> is the backward time horizon for the evolution path <math>p_c</math> and larger than one,\n\n: <math>\\alpha\\approx 1.5</math> and the [[indicator function]] <math>\\mathbf{1}_{[0,\\alpha\\sqrt{n}]}(\\|p_\\sigma\\|)</math> evaluates to one [[if and only if|iff]] <math>\\|p_\\sigma\\|\\in[0,\\alpha\\sqrt{n}]</math> or, in other words, <math>\\|p_\\sigma\\|\\le\\alpha\\sqrt{n}</math>, which is usually the case,\n\n: <math>c_s = (1 - \\mathbf{1}_{[0,\\alpha\\sqrt{n}]}(\\|p_\\sigma\\|)^2) \\,c_1 c_c (2-c_c) </math> makes partly up for the small variance loss in case the indicator is zero,\n\n: <math>c_1 \\approx 2 / n^2</math> is the learning rate for the rank-one update of the [[covariance matrix]] and\n\n: <math>c_\\mu \\approx \\mu_w / n^2 </math> is the learning rate for the rank-<math>\\mu</math> update of the [[covariance matrix]] and must not exceed <math>1 - c_1</math>.\n\nThe [[covariance matrix]] update tends to increase the [[Likelihood function|likelihood]] for <math>p_c</math> and for <math>(x_{i:\\lambda} - m_k)/\\sigma_k</math> to be sampled from <math>\\mathcal{N}(0,C_{k+1})</math>. This completes the iteration step.\n\nThe number of candidate samples per iteration, <math>\\lambda</math>, is not determined a priori and can vary in a wide range. Smaller values, for example <math>\\lambda=10</math>, lead to more local search behavior. Larger values, for example <math>\\lambda=10n</math> with default value <math>\\mu_w \\approx \\lambda/4</math>, render the search more global. Sometimes the algorithm is repeatedly restarted with increasing <math>\\lambda</math> by a factor of two for each restart.<ref>{{cite conference\n| first = A.\n| last = Auger\n|author2=N. Hansen \n| title = A Restart CMA Evolution Strategy With Increasing Population Size\n| booktitle = 2005 IEEE Congress on Evolutionary Computation, Proceedings\n| pages = 1769–1776\n| publisher = IEEE\n| year = 2005\n| location = \n| url=http://www.lri.fr/~auger/cec-restartcma.pdf\n| accessdate = \n| id = \n}}</ref> Besides of setting <math>\\lambda</math> (or possibly <math>\\mu</math> instead, if for example <math>\\lambda</math> is predetermined by the number of available processors), the above introduced parameters are not specific to the given objective function and therefore not meant to be modified by the user.\n\n==Example code in MATLAB/Octave==\n<syntaxhighlight lang=\"matlab\">\nfunction xmin=purecmaes   % (mu/mu_w, lambda)-CMA-ES\n  % --------------------  Initialization --------------------------------  \n  % User defined input parameters (need to be edited)\n  strfitnessfct = 'frosenbrock';  % name of objective/fitness function\n  N = 20;               % number of objective variables/problem dimension\n  xmean = rand(N,1);    % objective variables initial point\n  sigma = 0.3;          % coordinate wise standard deviation (step size)\n  stopfitness = 1e-10;  % stop if fitness < stopfitness (minimization)\n  stopeval = 1e3*N^2;   % stop after stopeval number of function evaluations\n  \n  % Strategy parameter setting: Selection  \n  lambda = 4+floor(3*log(N));  % population size, offspring number\n  mu = lambda/2;               % number of parents/points for recombination\n  weights = log(mu+1/2)-log(1:mu)'; % muXone array for weighted recombination\n  mu = floor(mu);        \n  weights = weights/sum(weights);     % normalize recombination weights array\n  mueff=sum(weights)^2/sum(weights.^2); % variance-effectiveness of sum w_i x_i\n\n  % Strategy parameter setting: Adaptation\n  cc = (4+mueff/N) / (N+4 + 2*mueff/N);  % time constant for cumulation for C\n  cs = (mueff+2) / (N+mueff+5);  % t-const for cumulation for sigma control\n  c1 = 2 / ((N+1.3)^2+mueff);    % learning rate for rank-one update of C\n  cmu = min(1-c1, 2 * (mueff-2+1/mueff) / ((N+2)^2+mueff));  % and for rank-mu update\n  damps = 1 + 2*max(0, sqrt((mueff-1)/(N+1))-1) + cs; % damping for sigma \n                                                      % usually close to 1\n  % Initialize dynamic (internal) strategy parameters and constants\n  pc = zeros(N,1); ps = zeros(N,1);   % evolution paths for C and sigma\n  B = eye(N,N);                       % B defines the coordinate system\n  D = ones(N,1);                      % diagonal D defines the scaling\n  C = B * diag(D.^2) * B';            % covariance matrix C\n  invsqrtC = B * diag(D.^-1) * B';    % C^-1/2 \n  eigeneval = 0;                      % track update of B and D\n  chiN=N^0.5*(1-1/(4*N)+1/(21*N^2));  % expectation of \n                                      %   ||N(0,I)|| == norm(randn(N,1)) \n  % -------------------- Generation Loop --------------------------------\n  counteval = 0;  % the next 40 lines contain the 20 lines of interesting code \n  while counteval < stopeval\n    \n      % Generate and evaluate lambda offspring\n      for k=1:lambda,\n          arx(:,k) = xmean + sigma * B * (D .* randn(N,1)); % m + sig * Normal(0,C) \n          arfitness(k) = feval(strfitnessfct, arx(:,k)); % objective function call\n          counteval = counteval+1;\n      end\n    \n      % Sort by fitness and compute weighted mean into xmean\n      [arfitness, arindex] = sort(arfitness); % minimization\n      xold = xmean;\n      xmean = arx(:,arindex(1:mu))*weights;   % recombination, new mean value\n    \n      % Cumulation: Update evolution paths\n      ps = (1-cs)*ps ... \n            + sqrt(cs*(2-cs)*mueff) * invsqrtC * (xmean-xold) / sigma; \n      hsig = norm(ps)/sqrt(1-(1-cs)^(2*counteval/lambda))/chiN < 1.4 + 2/(N+1);\n      pc = (1-cc)*pc ...\n            + hsig * sqrt(cc*(2-cc)*mueff) * (xmean-xold) / sigma;\n\n      % Adapt covariance matrix C\n      artmp = (1/sigma) * (arx(:,arindex(1:mu))-repmat(xold,1,mu));\n      C = (1-c1-cmu) * C ...                  % regard old matrix  \n           + c1 * (pc*pc' ...                 % plus rank one update\n                   + (1-hsig) * cc*(2-cc) * C) ... % minor correction if hsig==0\n           + cmu * artmp * diag(weights) * artmp'; % plus rank mu update\n\n      % Adapt step size sigma\n      sigma = sigma * exp((cs/damps)*(norm(ps)/chiN - 1)); \n    \n      % Decomposition of C into B*diag(D.^2)*B' (diagonalization)\n      if counteval - eigeneval > lambda/(c1+cmu)/N/10  % to achieve O(N^2)\n          eigeneval = counteval;\n          C = triu(C) + triu(C,1)'; % enforce symmetry\n          [B,D] = eig(C);           % eigen decomposition, B==normalized eigenvectors\n          D = sqrt(diag(D));        % D is a vector of standard deviations now\n          invsqrtC = B * diag(D.^-1) * B';\n      end\n    \n      % Break, if fitness is good enough or condition exceeds 1e14, better termination methods are advisable \n      if arfitness(1) <= stopfitness || max(D) > 1e7 * min(D)\n          break;\n      end\n\n  end % while, end generation loop\n\n  xmin = arx(:, arindex(1)); % Return best point of last iteration.\n                             % Notice that xmean is expected to be even\n                             % better.\n% ---------------------------------------------------------------  \nfunction f=frosenbrock(x)\n    if size(x,1) < 2 error('dimension must be greater one'); end\n    f = 100*sum((x(1:end-1).^2 - x(2:end)).^2) + sum((x(1:end-1)-1).^2);\n</syntaxhighlight>\n\n== Theoretical foundations ==\nGiven the distribution parameters&mdash;mean, variances and covariances&mdash;the [[multivariate normal distribution|normal probability distribution]] for sampling new candidate solutions is the [[maximum entropy probability distribution]] over <math>\\mathbb{R}^n</math>, that is, the sample distribution with the minimal amount of prior information built into the distribution. More considerations on the update equations of CMA-ES are made in the following.\n\n=== Variable metric ===\nThe CMA-ES implements a stochastic [[variable-metric]] method. In the very particular case of a convex-quadratic objective function\n\n:: <math> f(x) = {\\textstyle\\frac{1}{2}}(x-x^*)^T H (x-x^*)</math>\n\nthe covariance matrix <math>C_k</math> adapts to the inverse of the [[Hessian matrix]] <math>H</math>, [[up to]] a scalar factor and small random fluctuations. More general, also on the function <math>g \\circ f</math>, where <math> g </math> is strictly increasing and therefore order preserving and <math>f</math> is convex-quadratic, the covariance matrix <math>C_k</math> adapts to <math>H^{-1}</math>, [[up to]] a scalar factor and small random fluctuations.\n\n=== Maximum-likelihood updates ===\n\nThe update equations for mean and covariance matrix maximize a [[likelihood]] while resembling an [[Expectation maximization|expectation-maximization]] algorithm. The update of the mean vector <math>m</math> maximizes a log-likelihood, such that\n\n:: <math> m_{k+1} = \\arg\\max_{m} \\sum_{i=1}^\\mu w_i \\log p_\\mathcal{N}(x_{i:\\lambda} | m) </math>\n\nwhere\n\n:: <math> \\log p_\\mathcal{N}(x) = \n   - \\frac{1}{2} \\log\\det(2\\pi C) - \\frac{1}{2} (x-m)^T C^{-1} (x-m) </math>\n\ndenotes the log-likelihood of <math>x</math> from a multivariate normal distribution with mean <math>m</math> and any positive definite covariance matrix <math>C</math>. To see that <math>m_{k+1}</math> is independent of <math>C</math> remark first that this is the case for any diagonal matrix <math>C</math>, because the coordinate-wise maximizer is independent of a scaling factor. Then, rotation of the data points or choosing <math>C</math> non-diagonal are equivalent.\n\nThe rank-<math>\\mu</math> update of the covariance matrix, that is, the right most summand in the update equation of <math>C_k</math>, maximizes a log-likelihood in that\n\n:: <math> \\sum_{i=1}^\\mu w_i \\frac{x_{i:\\lambda} - m_k}{\\sigma_k} \n             \\left( \\frac{x_{i:\\lambda} - m_k}{\\sigma_k} \\right)^T \n     = \\arg\\max_{C} \\sum_{i=1}^\\mu w_i \\log p_\\mathcal{N}\\left(\\left.\\frac{x_{i:\\lambda} - m_k}{\\sigma_k} \\right| C\\right) </math>\n\nfor <math>\\mu\\ge n</math> (otherwise <math>C</math> is singular, but substantially the same result holds for <math>\\mu < n</math>). Here, <math> p_\\mathcal{N}(x | C) </math> denotes the likelihood of <math>x</math> from a multivariate normal distribution with zero mean and covariance matrix <math>C</math>. Therefore, for <math>c_1=0</math> and <math>c_\\mu=1</math>, <math>C_{k+1}</math> is the above [[maximum-likelihood]] estimator. See [[estimation of covariance matrices]] for details on the derivation.\n\n=== Natural gradient descent in the space of sample distributions ===\n\nAkimoto ''et al.''<ref name=akimoto2010>{{cite conference\n| first = Y.\n| last = Akimoto |author2=Y. Nagata |author3=I. Ono |author4=S. Kobayashi\n| title = Bidirectional Relation between CMA Evolution Strategies and Natural Evolution Strategies\n| booktitle = Parallel Problem Solving from Nature, PPSN XI\n| pages = 154–163\n| publisher = Springer\n| year = 2010\n| location = \n| url = \n| accessdate = \n| id = \n}}</ref> and Glasmachers ''et al.''<ref name=glasmachers2010>{{cite conference\n| first = T.\n| last = Glasmachers |author2=T. Schaul |author3=Y. Sun |author4=D. Wierstra |author5=J. Schmidhuber\n| title = Exponential Natural Evolution Strategies\n| booktitle = Genetic and Evolutionary Computation Conference GECCO\n| year = 2010\n| location = Portland, OR\n| url = http://www.idsia.ch/~tom/publications/xnes.pdf\n}}</ref> discovered independently that the update of the distribution parameters resembles the descent in direction of a sampled [[Information geometry#Natural gradient|natural gradient]] of the expected objective function value <math>E f(x)</math> (to be minimized), where the expectation is taken under the sample distribution. With the parameter setting of <math>c_\\sigma=0</math> and <math>c_1=0</math>, i.e. without step-size control and rank-one update, CMA-ES can thus be viewed as an instantiation of [[Natural Evolution Strategies]] (NES).<ref name=akimoto2010/><ref name=glasmachers2010/>\nThe [[Information geometry#Natural gradient|''natural'' gradient]] is independent of the parameterization of the distribution. Taken with respect to the parameters {{math|<var>&theta;</var>}} of the sample distribution {{math|<var>p</var>}}, the gradient of<math>E f(x)</math> can be expressed as\n\n:: <math> \\begin{align} \n  {\\nabla}_{\\!\\theta} E(f(x) | \\theta) \n     &= \\nabla_{\\!\\theta} \\int_{\\mathbb R^n}f(x) p(x) \\mathrm{d}x\n  \\\\ &= \\int_{\\mathbb R^n}f(x) \\nabla_{\\!\\theta} p(x) \\mathrm{d}x\n  \\\\ &= \\int_{\\mathbb R^n}f(x) p(x) \\nabla_{\\!\\theta} \\ln p(x) \\mathrm{d}x\n  \\\\ &= E(f(x) \\nabla_{\\!\\theta} \\ln p(x|\\theta))\n\\end{align}</math>\n\nwhere <math>p(x)=p(x|\\theta)</math> depends on the parameter vector <math>\\theta</math>. The so-called [[Score (statistics)|score function]], <math>\\nabla_{\\!\\theta} \\ln p(x|\\theta) = \\frac{\\nabla_{\\!\\theta} p(x)}{p(x)} </math>, indicates the relative sensitivity of {{math|<var>p</var>}} w.r.t. {{math|<var>&theta;</var>}}, and the expectation is taken with respect to the distribution {{math|<var>p</var>}}. The [[Information geometry#Natural gradient|''natural'' gradient]] of <math>E f(x)</math>, complying with the [[Fisher information metric]] (an informational distance measure between probability distributions and the curvature of the [[relative entropy]]), now reads\n\n:: <math> \\begin{align} \n  \\tilde{\\nabla} E(f(x) | \\theta) \n  &= F^{-1}_\\theta \\nabla_{\\!\\theta} E(f(x) | \\theta) \n\\end{align}</math>\n\nwhere the [[Fisher information]] matrix <math> F_{\\theta} </math> is the expectation of the [[Hessian matrix|Hessian]] of {{math|-ln<var>p</var>}} and renders the expression independent of the chosen parameterization. Combining the previous equalities we get\n\n:: <math> \\begin{align} \n  \\tilde{\\nabla} E(f(x) | \\theta) \n  &= F^{-1}_\\theta E(f(x) \\nabla_{\\!\\theta} \\ln p(x|\\theta))\n  \\\\ &= E(f(x) F^{-1}_\\theta \\nabla_{\\!\\theta} \\ln p(x|\\theta))\n\\end{align}</math>\n\nA Monte Carlo approximation of the latter expectation takes the average over {{math|<var>&lambda;</var>}} samples from {{math|<var>p</var>}}\n\n:: <math> \\tilde{\\nabla} \\widehat{E}_\\theta(f) := -\\sum_{i=1}^\\lambda \\overbrace{w_i}^{\\!\\!\\!\\!\\text{preference weight}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!} \\underbrace{F^{-1}_\\theta \\nabla_{\\!\\theta} \\ln p(x_{i:\\lambda}|\\theta)}_{\\!\\!\\!\\!\\!\\text{candidate direction from }x_{i:\\lambda}\\!\\!\\!\\!\\!}\n\\quad\\mathrm{with~}w_i = -f(x_{i:\\lambda})/\\lambda</math>\n\nwhere the notation <math>i:\\lambda</math> from above is used and therefore <math>w_i</math> are monotonously decreasing in <math>i</math>.\n\nOllivier ''et al.''<ref>{{cite arXiv \n|last1=Ollivier |first1=Y.\n|last2=Arnold |first2=L.\n|last3=Auger |first3=A.\n|last4=Hansen |first4=N.\n|eprint=1106.3708v2 \n|title=Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles\n|year=2013 \n|class=math.OC\n}}</ref>\nfinally found a rigorous derivation for the more robust weights, <math>w_i</math>, as they are defined in the CMA-ES (weights are often zero for {{math|<var>i</var> > <var>&mu;</var>}}). They are formulated as the consistent estimator for the [[Cumulative distribution function|CDF]] of <math>f(X), X\\sim p(.|\\theta)</math> at the point <math>f(x_{i:\\lambda})</math>, composed with a fixed monotonous decreased transformation <math>w</math>, that is,\n\n:: <math>w_i = w\\left(\\frac{\\mathsf{rank}(f(x_{i:\\lambda})) - 1/2}{\\lambda}\\right)</math>\n\nThis makes the algorithm insensitive to the specific <math>f</math>-values. More concisely, using the [[Cumulative distribution function|CDF]] estimator of <math>f</math> instead of <math>f</math> itself let the algorithm only depend on the ranking of <math>f</math>-values but not on their underlying distribution. It renders the algorithm invariant to monotonous <math>f</math>-transformations. Let\n\n:: <math>\\theta = [m_k^T \\mathrm{vec}(C_k)^T \\sigma_k]^T \\in \\mathbb{R}^{n+n^2+1}</math>\n\nsuch that <math> p(.|\\theta) </math> is the density of the [[multivariate normal distribution]] <math>\\mathcal N(m_k,\\sigma_k^2 C_k)</math>. Then, we have an explicit expression for the inverse of the Fisher information matrix where <math>\\sigma_k</math> is fixed\n\n:: <math>F^{-1}_{\\theta | \\sigma_k} = \\left[\\begin{array}{cc}\\sigma_k^2 C_k&0\\\\ 0&2 C_k\\otimes C_k\\end{array}\\right]</math>\n\nand for\n\n:: <math>\\ln p(x|\\theta) = \\ln p(x|m_k,\\sigma_k^2 C_k) = -\\frac{1}{2}(x-m_k)^T \\sigma_k^{-2} C_k^{-1} (x-m_k) \n         \\,-\\, \\frac{1}{2}\\ln\\det(2\\pi\\sigma_k^2 C_k)</math>\n\nand, after some calculations, the updates in the CMA-ES turn out as<ref name=akimoto2010/>\n\n<div id=\"update_in_gradient_formulation\">\n:: <math> \\begin{align} \n   m_{k+1} \n     &= m_k - \\underbrace{[\\tilde{\\nabla} \\widehat{E}_\\theta(f)]_{1,\\dots, n}}_{\n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n     \\text{natural gradient for mean}\n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n     } \n   \\\\\n     &= m_k + \\sum_{i=1}^\\lambda w_i (x_{i:\\lambda} - m_k) \n   \\end{align} </math>\n\nand\n\n:: <math> \\begin{align} \n   C_{k+1} \n     &= C_k + c_1(p_c p_c^T - C_k)  \n         - c_\\mu\\,\\mathrm{mat}(\\overbrace{[\\tilde{\\nabla} \\widehat{E}_\\theta(f)]_{n+1,\\dots,n+n^2}}^{\n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n        \\text{natural gradient for covariance matrix} \n     \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n     })\\\\\n     &= C_k + c_1(p_c p_c^T - C_k) \n        + c_\\mu \\sum_{i=1}^\\lambda w_i \\left(\\frac{x_{i:\\lambda} - m_k}{\\sigma_k} \\left(\\frac{x_{i:\\lambda} - m_k}{\\sigma_k}\\right)^T - C_k\\right) \n   \\end{align}</math>\n</div>\n\nwhere mat forms the proper matrix from the respective natural gradient sub-vector. That means, setting <math>c_1=c_\\sigma=0</math>, the CMA-ES updates descend in direction of the approximation <math> \\tilde{\\nabla} \\widehat{E}_\\theta(f)</math> of the natural gradient while using different step-sizes (learning rates 1 and <math>c_\\mu</math>) for the [[Fisher information#Orthogonal parameters|orthogonal parameters]] <math>m</math> and <math>C</math> respectively. The most recent version of CMA-ES also use a different function <math>w</math> for <math>m</math> and <math>C</math> with negative values only for the latter (so-called active CMA).\n\n=== Stationarity or unbiasedness ===\nIt is comparatively easy to see that the update equations of CMA-ES satisfy some stationarity conditions, in that they are essentially unbiased. Under neutral selection, where <math>x_{i:\\lambda} \\sim \\mathcal N(m_k,\\sigma_k^2 C_k)</math>,  we find that\n\n:: <math> E(m_{k+1}\\,|\\, m_k) = m_k </math>\n\nand under some mild additional assumptions on the initial conditions\n\n:: <math> E(\\log \\sigma_{k+1} \\,|\\, \\sigma_k) = \\log \\sigma_k </math>\n\nand with an additional minor correction in the covariance matrix update for the case where the indicator function evaluates to zero, we find\n\n:: <math> E(C_{k+1} \\,|\\, C_k) = C_k </math>\n\n=== Invariance ===\n[[Invariant (mathematics)|Invariance properties]] imply uniform performance on a class of objective functions. They have been argued to be an advantage, because they allow to generalize and predict the behavior of the algorithm and therefore strengthen the meaning of empirical results obtained on single functions. The following invariance properties have been established for CMA-ES.\n\n* Invariance under order-preserving transformations of the objective function value <math>f</math>, in that for any <math>h:\\mathbb{R}^n\\to\\mathbb{R}</math> the behavior is identical on <math>f:x\\mapsto g(h(x))</math> for all strictly increasing <math>g:\\mathbb{R}\\to\\mathbb{R}</math>. This invariance is easy to verify, because only the <math>f</math>-ranking is used in the algorithm, which is invariant under the choice of <math>g</math>.\n* [[Scale-invariance]], in that for any <math>h:\\mathbb{R}^n\\to \\mathbb{R}</math> the behavior is independent of <math>\\alpha>0</math> for the objective function <math>f:x\\mapsto h(\\alpha x)</math> given <math>\\sigma_0\\propto1/\\alpha</math> and <math>m_0\\propto1/\\alpha</math>.\n* Invariance under rotation of the search space in that for any <math>h:\\mathbb{R}^n\\to \\mathbb{R}</math> and any <math>z\\in\\mathbb{R}^n</math> the behavior on <math>f:x\\mapsto h(R x)</math>  is independent of the [[orthogonal matrix]] <math>R</math>, given <math>m_0=R^{-1} z</math>. More general, the algorithm is also invariant under general linear transformations <math>R</math> when additionally the initial covariance matrix is chosen as <math>R^{-1}{R^{-1}}^T</math>.\n\nAny serious parameter optimization method should be translation invariant, but most methods do not exhibit all the above described invariance properties. A prominent example with the same invariance properties is the [[Nelder–Mead method]], where the initial simplex must be chosen respectively.\n\n=== Convergence ===\n\nConceptual considerations like the scale-invariance property of the algorithm, the analysis of simpler [[evolution strategies]], and overwhelming empirical evidence suggest that the algorithm converges on a large class of functions fast to the global optimum, denoted as <math>x^*</math>. On some functions, convergence occurs independently of the initial conditions with probability one. On some functions the probability is smaller than one and typically depends on the initial <math>m_0</math> and <math>\\sigma_0</math>. Empirically, the fastest possible convergence rate in <math>k</math> for rank-based direct search methods can often be observed (depending on the context denoted as ''[[Rate of convergence#linear convergence|linear]]'' or ''log-linear'' or ''exponential'' convergence). Informally, we can write\n\n:: <math>\\|m_k - x^*\\| \\;\\approx\\; \\|m_0 - x^*\\| \\times e^{-ck}\n   </math>\n\nfor some <math>c>0</math>, and more rigorously\n\n:: <math>\\frac{1}{k}\\sum_{i=1}^k\\log\\frac{\\|m_i - x^*\\|}{\\|m_{i-1} - x^*\\|} \n   \\;=\\; \\frac{1}{k}\\log\\frac{\\|m_k - x^*\\|}{\\|m_{0} - x^*\\|}\n    \\;\\to\\; -c < 0 \\quad\\text{for}\\; k\\to\\infty\\;, \n   </math>\n\nor similarly,\n\n:: <math>E\\log\\frac{\\|m_k - x^*\\|}{\\|m_{k-1} - x^*\\|}\n    \\;\\to\\; -c < 0 \\quad\\text{for}\\; k\\to\\infty\\;. \n   </math>\n\nThis means that on average the distance to the optimum decreases in each iteration by a \"constant\" factor, namely by <math>\\exp(-c)</math>. The convergence rate <math>c</math> is roughly <math>0.1\\lambda/n</math>, given <math>\\lambda</math> is not much larger than the dimension <math>n</math>. Even with optimal <math>\\sigma</math> and <math>C</math>, the convergence rate <math>c</math>  cannot largely exceed <math>0.25\\lambda/n</math>, given the above recombination weights <math>w_i</math> are all non-negative. The actual linear dependencies in <math>\\lambda</math> and <math>n</math> are remarkable and they are in both cases the best one can hope for in this kind of algorithm. Yet, a rigorous proof of convergence is missing.\n\n===Interpretation as coordinate-system transformation===\nUsing a non-identity covariance matrix for the [[multivariate normal distribution]] in [[evolution strategies]] is equivalent to a coordinate system transformation of the solution vectors,<ref name=hansen2008 /> mainly because the sampling equation\n\n:<math>\n\\begin{align}\n  x_i &\\sim\\ m_k + \\sigma_k\\times\\mathcal{N}(0,C_k)\n    \\\\\n  &\\sim\\ m_k + \\sigma_k \\times C_k^{1/2}\\mathcal{N}(0,I) \n\\end{align}\n</math>\n\ncan be equivalently expressed in an \"encoded space\" as \n:<math>\n  \\underbrace{C_k^{-1/2}x_i}_{\\text{represented in the encode space}\n    \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!} \n     \\sim\\ \\underbrace{C_k^{-1/2} m_k} {} + \\sigma_k \\times\\mathcal{N}(0,I)\n</math>\n\nThe covariance matrix defines a [[bijective]] transformation (encoding) for all solution vectors into a space, where the sampling takes place with identity covariance matrix. Because the update equations in the CMA-ES are invariant under linear coordinate system transformations, the CMA-ES can be re-written as an adaptive encoding procedure applied to a simple [[evolution strategy]] with identity covariance matrix.<ref name=hansen2008>{{cite conference\n| first = N.\n| last = Hansen\n| title = Adpative Encoding: How to Render Search Coordinate System Invariant\n| booktitle = Parallel Problem Solving from Nature, PPSN X\n| pages = 205–214\n| publisher = Springer\n| year = 2008\n| location = \n| url = http://hal.archives-ouvertes.fr/inria-00287351/en/\n| accessdate = \n| id = \n}}</ref>\nThis adaptive encoding procedure is not confined to algorithms that sample from a multivariate normal distribution (like evolution strategies), but can in principle be applied to any iterative search method.\n\n== Performance in practice ==\n\nIn contrast to most other [[evolutionary algorithms]], the CMA-ES is, from the user's perspective, quasi parameter-free. The user has to choose an initial solution point, <math>m_0\\in\\mathbb{R}^n</math>, and the initial step-size, <math>\\sigma_0>0</math>. Optionally, the number of candidate samples λ (population size) can be modified by the user in order to change the characteristic search behavior (see above) and termination conditions can or should be adjusted to the problem at hand.\n\nThe CMA-ES has been empirically successful in hundreds of applications and is considered to be useful in particular on non-convex, non-separable, ill-conditioned, multi-modal or noisy objective functions.<ref>{{Cite web|url=http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaapplications.pdf|title=References to CMA-ES Applications|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref> The search space dimension ranges typically between two and a few hundred. Assuming a black-box optimization scenario, where gradients are not available (or not useful) and function evaluations are the only considered cost of search, the CMA-ES method is likely to be outperformed by other methods in the following conditions:\n\n* on low-dimensional functions, say <math>n < 5 </math>, for example by the [[Nelder-Mead method|downhill simplex method]] or surrogate-based methods (like [[kriging]] with expected improvement);\n* on separable functions without or with only negligible dependencies between the design variables in particular in the case of multi-modality or large dimension, for example by [[differential evolution]];\n* on (nearly) [[Convex function|convex]]-quadratic functions with low or moderate [[condition number]] of the [[Hessian matrix]], where [[BFGS method|BFGS]] or [[NEWUOA]] are typically ten times faster;\n* on functions that can already be solved with a comparatively small number of function evaluations, say no more than <math>10 n</math>, where CMA-ES is often slower than, for example, [[NEWUOA]] or [[MCS algorithm|Multilevel Coordinate Search]] (MCS).\n\nOn separable functions, the performance disadvantage is likely to be most significant in that CMA-ES might not be able to find at all comparable solutions. On the other hand, on non-separable functions that are ill-conditioned or rugged or can only be solved with more than <math>100 n</math> function evaluations, the CMA-ES shows most often superior performance.\n\n== Variations and extensions ==\nThe (1+1)-CMA-ES<ref>{{cite conference\n| first = C.\n| last = Igel |author2=T. Suttorp |author3=N. Hansen\n| title = A Computational Efficient Covariance Matrix Update and a (1+1)-CMA for Evolution Strategies\n| booktitle = Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)\n| pages = 453–460\n| publisher = ACM Press\n| year = 2006\n| location = \n| url = http://www.cs.york.ac.uk/rts/docs/GECCO_2006/docs/p453.pdf\n| accessdate = \n| id = \n}}</ref> generates only one candidate solution per iteration step which becomes the new distribution mean if it is better than the current mean. For <math>c_c=1</math> the (1+1)-CMA-ES is a close variant of [[Gaussian adaptation]]. Some [[Natural Evolution Strategies]] are close variants of the CMA-ES with specific parameter settings. Natural Evolution Strategies do not utilize evolution paths (that means in CMA-ES setting <math>c_c=c_\\sigma=1</math>) and they formalize the update of variances and covariances on a [[Cholesky decomposition|Cholesky factor]] instead of a covariance matrix. The CMA-ES has also been extended to [[multiobjective optimization]] as MO-CMA-ES.<ref>{{cite journal\n| doi = 10.1162/evco.2007.15.1.1\n| first = C.\n| last = Igel |author2=N. Hansen |author3=S. Roth \n| title = Covariance Matrix Adaptation for Multi-objective Optimization\n| journal = Evolutionary Computation\n| volume = 15\n| issue = 1\n| pages = 1–28\n| year = 2007\n| pmid = 17388777\n| id = \n}}</ref> Another remarkable extension has been the addition of a negative update of the covariance matrix with the so-called active CMA.<ref>{{cite conference\n| first = G.A.\n| last = Jastrebski\n|author2=D.V. Arnold \n| title = Improving Evolution Strategies through Active Covariance Matrix Adaptation\n| booktitle = 2006 IEEE World Congress on Computational Intelligence, Proceedings\n| pages = 9719–9726\n| publisher = IEEE\n| year = 2006\n| location = \n| doi = 10.1109/CEC.2006.1688662\n| id = \n}}</ref>\n\nWith the advent of niching methods in evolutionary strategies, the question of an optimal niche radius arises. An \"adaptive individual niche radius\" is introduced in <ref>\n{{cite book\n| first = Ofer M.\n| last = Shir\n| last2 = Bäck\n| first2 = Thomas\n| title = Parallel Problem Solving from Nature-PPSN IX\n| pages = 142–151\n| year = 2006\n| location = \n| publisher= Springer \n}}</ref>\n\n== See also ==\n* [[Global optimization]]\n* [[Stochastic optimization]]\n* [[Derivative-free optimization]]\n* [[Estimation of distribution algorithm]]\n\n== References ==\n{{Reflist}}\n\n==Bibliography==\n*Hansen N, Ostermeier A (2001). Completely derandomized self-adaptation in evolution strategies. [http://www.mitpressjournals.org/toc/evco/9/2 ''Evolutionary Computation'', '''9'''(2)] pp.&nbsp;159–195. [http://www.lri.fr/~hansen/cmaartic.pdf]\n*Hansen N, Müller SD, Koumoutsakos P (2003). Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). [http://www.mitpressjournals.org/toc/evco/11/1 ''Evolutionary Computation'', '''11'''(1)] pp.&nbsp;1–18.  [http://mitpress.mit.edu/journals/pdf/evco_11_1_1_0.pdf]\n*Hansen N, Kern S (2004). Evaluating the CMA evolution strategy on multimodal test functions. In Xin Yao et al., editors, ''Parallel Problem Solving from Nature - PPSN VIII'', pp.&nbsp;282–291, Springer. [http://www.lri.fr/~hansen/ppsn2004hansenkern.pdf]\n*Igel C, Hansen N, Roth S (2007). Covariance Matrix Adaptation for Multi-objective Optimization. [http://www.mitpressjournals.org/toc/evco/15/1 ''Evolutionary Computation'', '''15'''(1)] pp.&nbsp;1–28. [http://www.mitpressjournals.org/doi/pdfplus/10.1162/evco.2007.15.1.1]\n\n==External links==\n* [http://cma.gforge.inria.fr/cmaesintro.html A short introduction to CMA-ES by N. Hansen]\n* [https://arxiv.org/pdf/1604.00772.pdf The CMA Evolution Strategy: A Tutorial]\n* [http://cma.gforge.inria.fr/cmaes_sourcecode_page.html CMA-ES source code page]\n\n{{Major subfields of optimization}}\n{{Evolutionary computation}}\n\n{{DEFAULTSORT:Cma-Es}}\n[[Category:Evolutionary algorithms]]\n[[Category:Stochastic optimization]]\n[[Category:Optimization algorithms and methods]]\n\n[[fr:Stratégie d'évolution#CMA-ES]]"
    },
    {
      "title": "COBYLA",
      "url": "https://en.wikipedia.org/wiki/COBYLA",
      "text": "'''Constrained optimization by linear approximation''' ('''COBYLA''') is a [[Mathematical optimization|numerical optimization]] method for [[constrained optimization|constrained problems]] where the [[derivative]] of the objective function is not known, invented by [[Michael J. D. Powell]]. That is, COBYLA can find the vector <math>\\vec{x} \\in \\mathcal{S}</math> with <math>\\mathcal{S} \\subseteq \\mathbb{R}^n</math> that has the minimal (or maximal) <math>f(\\vec{x})</math> without knowing the [[gradient]] of <math>f</math>. COBYLA is also the name of Powell's software implementation of the algorithm in [[Fortran]].<ref>{{cite encyclopedia |author1=Andrew R. Conn |author2=Katya Scheinberg |author2-link= Katya Scheinberg |author3=Ph. L. Toint |title=On the convergence of derivative-free methods for unconstrained optimization |encyclopedia=Approximation theory and optimization: tributes to MJD Powell |year=1997 |pages=83–108}}</ref>\n\nPowell invented COBYLA while working for [[Westland Helicopters]].<ref>M. J. D. Powell (2007). [http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2007_03.pdf A view of algorithms for optimization without derivatives]. Cambridge University Technical Report DAMTP 2007.</ref>\n\nIt works by iteratively approximating the actual constrained optimization problem with [[Linear programming|linear programming problems]]. During an iteration, an approximating linear programming problem is solved to obtain a candidate for the optimal solution. The candidate solution is evaluated using the original objective and constraint functions, yielding a new data point in the optimization space. This information is used to improve the approximating linear programming problem used for the next iteration of the algorithm. When the solution cannot be improved anymore, the step size is reduced, refining the search. When the step size becomes sufficiently small, the algorithm finishes.\n\nThe COBYLA software is distributed under [[GNU Lesser General Public License|The GNU Lesser General Public License]] (LGPL).<ref name=\"code\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#cobyla |title=Source code of COBYLA software |publisher= |date= |accessdate=2015-04-30}}</ref>\n\n==See also==\n* [[TOLMIN (optimization software)|TOLMIN]]\n* [[UOBYQA]]\n* [[NEWUOA]]\n* [[BOBYQA]]\n* [[LINCOA]]\n* [[Nelder–Mead method]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [https://ccpforge.cse.rl.ac.uk/gf/project/powell Optimization software by Professor M. J. D. Powell at CCPForge] \n* [http://mat.uc.pt/~zhang/software.html#powell_software A repository of Professor M. J. D. Powell's software]\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Coffman–Graham algorithm",
      "url": "https://en.wikipedia.org/wiki/Coffman%E2%80%93Graham_algorithm",
      "text": "In [[job shop scheduling]] and [[graph drawing]], the '''Coffman–Graham algorithm''' is an [[algorithm]], named after [[Edward G. Coffman, Jr.]] and [[Ronald Graham]], for arranging the elements of a [[partially ordered set]] into a sequence of levels. The algorithm chooses an arrangement such that an element that comes after another in the order is assigned to a lower level, and such that each level has a number of elements that does not exceed a fixed width bound {{mvar|W}}. When {{math|1=''W'' = 2}}, it uses the minimum possible number of distinct levels,<ref name=\"cg72\"/> and in general it uses at most {{math|2 &minus; 2/''W''}} times as many levels as necessary.<ref name=\"ls77\"/><ref name=\"bt94\"/>\n\n==Problem statement and applications==\nIn the version of the job shop scheduling problem solved by the Coffman–Graham algorithm, one is given a set of {{mvar|n}} jobs {{math|''J''<sub>1</sub>, ''J''<sub>2</sub>, ..., ''J''<sub>''n''</sub>}}, together with a system of precedence constraints {{math|''J<sub>i</sub>'' < ''J<sub>j</sub>''}} requiring that job {{math|''J<sub>i</sub>''}} be completed before job {{math|''J<sub>j</sub>''}} begins. Each job is assumed to take unit time to complete. The scheduling task is to assign each of these jobs to time slots on a system of {{mvar|W}} identical processors, minimizing the [[makespan]] of the assignment (the time from the beginning of the first job until the completion of the final job). Abstractly, the precedence constraints define a partial order on the jobs, so the problem can be rephrased as one of assigning the elements of this partial order to levels (time slots) in such a way that each time slot has at most as many jobs as processors (at most {{mvar|W}} elements per level), respecting the precedence constraints. This application was the original motivation for Coffman and Graham to develop their algorithm.<ref name=\"cg72\">{{citation\n | last1 = Coffman | first1 = E. G., Jr. | author1-link = Edward G. Coffman, Jr.\n | last2 = Graham | first2 = R. L. | author2-link = Ronald Graham\n | mr = 0334913\n | journal = [[Acta Informatica]]\n | pages = 200–213\n | title = Optimal scheduling for two-processor systems\n | url = http://www.math.ucsd.edu/~ronspubs/72_04_two_processors.pdf\n | volume = 1\n | year = 1972 | doi=10.1007/bf00288685}}.</ref><ref>{{citation|contribution=Some basic scheduling algorithms|first=Joseph Y.-T.|last=Leung|title=Handbook of Scheduling: Algorithms, Models, and Performance Analysis|publisher=CRC Press|year=2004|isbn=978-1-58488-397-5}}.</ref>\n\nIn the [[layered graph drawing]] framework outlined by {{harvtxt|Sugiyama|Tagawa|Toda|1981}}<ref>{{citation|first1=Kozo|last1=Sugiyama|first2=Shôjirô|last2=Tagawa|first3=Mitsuhiko|last3=Toda|title=Methods for visual understanding of hierarchical system structures|journal=[[IEEE Systems, Man & Cybernetics Society|IEEE Transactions on Systems, Man, and Cybernetics]]|volume=SMC-11|issue=2|pages=109–125|year=1981|mr=0611436|doi=10.1109/TSMC.1981.4308636}}.</ref> the input is a [[directed graph]], and a drawing of a graph is constructed in several stages:<ref name=\"bett\"/><ref>{{citation|contribution=Layered drawings of digraphs|first1=Oliver|last1=Bastert|first2=Christian|last2=Matuszewski|title=Drawing Graphs: Methods and Models|editor1-first=Michael|editor1-last=Kaufmann|editor2-first=Dorothea|editor2-last=Wagner|editor2-link=Dorothea Wagner|publisher=Springer-Verlag|series=Lecture Notes in Computer Science|volume=2025|year=2001|pages=87–120|doi=10.1007/3-540-44969-8_5}}. Bastert and Matuszewski also include a description of the Coffman–Graham algorithm; however, they omit the transitive reduction stage of the algorithm.</ref>\n#A [[feedback arc set]] is chosen, and the edges of this set reversed, in order to convert the input into a [[directed acyclic graph]].\n#The vertices of the graph are given integer {{mvar|y}}-coordinates in such a way that, for each edge, the starting vertex of the edge has a higher coordinate than the ending vertex, with at most {{mvar|W}} vertices sharing the same {{mvar|y}}-coordinate.\n#Dummy vertices are introduced within each edge so that the subdivided edges all connect pairs of vertices that are in adjacent levels of the drawing.\n#Within each group of vertices with the same {{mvar|y}}-coordinate, the vertices are [[permutation|permuted]] in order to minimize the [[crossing number (graph theory)|number of crossings]] in the resulting drawing, and the vertices are assigned {{mvar|x}}-coordinates consistently with this permutation.\n#The vertices and edges of the graph are drawn with the coordinates assigned to them.\nIn this framework, the {{mvar|y}}-coordinate assignment again involves grouping elements of a partially ordered set (the vertices of the graph, with the [[reachability]] ordering on the vertex set) into layers (sets of vertices with the same {{mvar|y}}-coordinate), which is the problem solved by the Coffman–Graham algorithm.<ref name=\"bett\">{{citation|contribution=Chapter 9: Layered drawings of digraphs|title=Graph Drawing: Algorithms for the Visualization of Graphs|first1=Giuseppe|last1=di Battista|first2=Peter|last2=Eades|author2-link=Peter Eades|first3=Roberto|last3=Tamassia|author3-link=Roberto Tamassia|first4=Ioannis G.|last4=Tollis|publisher=Prentice Hall|year=1999|pages=265–302}}.</ref> Although there exist alternative approaches than the Coffman–Graham algorithm to the layering step, these alternatives in general are either not able to incorporate a bound on the maximum width of a level or rely on complex [[integer programming]] procedures.<ref>{{citation\n | last1 = Healy | first1 = Patrick\n | last2 = Nikolov | first2 = Nikola S.\n | contribution = How to layer a directed acyclic graph\n | doi = 10.1007/3-540-45848-4_2\n | mr = 1962416\n | pages = 16–30\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = [[International Symposium on Graph Drawing|Graph Drawing: 9th International Symposium, GD 2001 Vienna, Austria, September 23–26, 2001, Revised Papers]]\n | volume = 2265\n | year = 2002}}.</ref>\n\nMore abstractly, both of these problems can be formalized as a problem in which the input consists of a partially ordered set and an integer {{mvar|W}}. The desired output is an assignment of integer level numbers to the elements of the partially ordered set such that, if {{math|''x'' < ''y''}} is an ordered pair of related elements of the partial order, the number assigned to {{mvar|x}} is smaller than the number assigned to {{mvar|y}}, such that at most {{mvar|W}} elements are assigned the same number as each other, and minimizing the difference between the smallest and the largest assigned numbers.\n\n==The algorithm==\nThe Coffman–Graham algorithm performs the following steps.<ref name=\"bett\"/>\n#Represent the partial order by its [[transitive reduction]] or [[covering relation]], a directed acyclic graph {{mvar|G}} that has an edge from ''x'' to ''y'' whenever {{math|''x'' < ''y''}} and there does not exist any third element {{mvar|z}} of the partial order for which {{math|''x'' < ''z'' < ''y''}}. In the graph drawing applications of the Coffman–Graham algorithm, the resulting directed acyclic graph may not be the same as the graph being drawn, and in the scheduling applications it may not have an edge for every precedence constraint of the input: in both cases, the transitive reduction removes redundant edges that are not necessary for defining the partial order.\n#Construct a [[Topological sorting|topological ordering]] of {{mvar|G}} in which the vertices are ordered [[lexicographic order|lexicographically]] by the set of positions of their incoming neighbors. To do so, add the vertices one at a time to the ordering, at each step choosing a vertex {{mvar|v}} to add such that the incoming neighbors of {{mvar|v}} are all already part of the partial ordering, and such that the most recently added incoming neighbor of {{mvar|v}} is earlier than the most recently added incoming neighbor of any other vertex that could be added in place of {{mvar|v}}. If two vertices have the same most recently added incoming neighbor, the algorithm breaks the tie in favor of the one whose second most recently added incoming neighbor is earlier, etc.\n#Assign the vertices of {{mvar|G}} to levels in the reverse of the topological ordering constructed in the previous step. For each vertex {{mvar|v}}, add {{mvar|v}} to a level that is at least one step higher than the highest level of any outgoing neighbor of {{mvar|v}}, that does not already have {{mvar|W}} elements assigned to it, and that is as low as possible subject to these two constraints.\n\n==Analysis==\nAs {{harvtxt|Coffman|Graham|1972}} originally proved, their algorithm computes an optimal assignment for {{math|1=''W'' = 2}}; that is, for scheduling problems with unit length jobs on two processors, or for layered graph drawing problems with at most two vertices per layer.<ref name=\"cg72\"/> A closely related algorithm also finds the optimal solution for scheduling of jobs with varying lengths, allowing pre-emption of scheduled jobs, on two processors.<ref>{{citation\n | last1 = Muntz | first1 = R. R.\n | last2 = Coffman | first2 = E. G. | author2-link = Edward G. Coffman, Jr.\n | doi = 10.1109/T-C.1969.222573\n | journal = [[IEEE Transactions on Computers]]\n | pages = 1014–1020\n | title = Optimal preemptive scheduling on two-processor systems\n | volume = 18\n | year = 1969}}.</ref> For {{math|''W'' > 2}}, the Coffman–Graham algorithm uses a number of levels (or computes a schedule with a makespan) that is within a factor of {{math|2 &minus; 2/''W''}} of optimal.<ref name=\"ls77\">{{citation\n | last1 = Lam | first1 = Shui\n | last2 = Sethi | first2 = Ravi | author2-link = Ravi Sethi\n | doi = 10.1137/0206037\n | mr = 0496614\n | issue = 3\n | journal = [[SIAM Journal on Computing]]\n | pages = 518–536\n | title = Worst case analysis of two scheduling algorithms\n | volume = 6\n | year = 1977}}.</ref><ref name=\"bt94\">{{citation\n | last1 = Braschi | first1 = Bertrand\n | last2 = Trystram | first2 = Denis\n | doi = 10.1137/S0097539790181889\n | mr = 1274650\n | issue = 3\n | journal = [[SIAM Journal on Computing]]\n | pages = 662–669\n | title = A new insight into the Coffman–Graham algorithm\n | volume = 23\n | year = 1994}}.</ref> For instance, for {{math|1=''W'' = 3}}, this means that it uses at most {{math|4/3}} times as many levels as is optimal. When the partial order of precedence constraints is an [[interval order]], or belongs to several related classes of partial orders, the Coffman–Graham algorithm finds a solution with the minimum number of levels regardless of its width bound.<ref>{{citation\n | last1 = Chardon | first1 = Marc\n | last2 = Moukrim | first2 = Aziz\n | doi = 10.1137/S0895480101394999\n | mr = 2178187\n | issue = 1\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | pages = 109–121\n | title = The Coffman-Graham algorithm optimally solves UET task systems with overinterval orders\n | volume = 19\n | year = 2005}}.</ref>\n\nAs well as finding schedules with small makespan, the Coffman–Graham algorithm (modified from the presentation here so that it topologically orders the [[reverse graph]] of {{mvar|G}} and places the vertices as early as possible rather than as late as possible) minimizes the [[total flow time]] of two-processor schedules, the sum of the completion times of the individual jobs. A related algorithm can be used to minimize the total flow time for a version of the problem in which preemption of jobs is allowed.<ref>{{citation\n | last1 = Coffman | first1 = E. G., Jr. | author1-link = Edward G. Coffman, Jr.\n | last2 = Sethuraman | first2 = J.\n | last3 = Timkovsky | first3 = V. G.\n | doi = 10.1007/s00236-003-0119-6\n | mr = 1996238\n | issue = 8\n | journal = [[Acta Informatica]]\n | pages = 597–612\n | title = Ideal preemptive schedules on two processors\n | volume = 39\n | year = 2003}}.</ref>\n\n{{harvtxt|Coffman|Graham|1972}} and {{harvtxt|Lenstra|Rinnooy Kan|1978}}<ref>{{citation\n | last1 = Lenstra | first1 = J. K. | author1-link = Jan Karel Lenstra\n | last2 = Rinnooy Kan | first2 = A. H. G. | author2-link = Alexander Rinnooy Kan\n | jstor = 169889\n | mr = 0462553\n | issue = 1\n | journal = Operations Research\n | pages = 22–35\n | title = Complexity of scheduling under precedence constraints\n | volume = 26\n | year = 1978 | doi=10.1287/opre.26.1.22}}.</ref> state the time complexity of the Coffman–Graham algorithm, on an {{mvar|n}}-element partial order, to be {{math|''O''(''n''<sup>2</sup>)}}. However, this analysis omits the time for constructing the transitive reduction, which is not known to be possible within this bound.  {{harvtxt|Sethi|1976}} shows how to implement the topological ordering stage of the algorithm in [[linear time]], based on the idea of [[partition refinement]].<ref>{{citation\n | last = Sethi | first = Ravi | authorlink = Ravi Sethi\n | doi = 10.1137/0205005\n | mr = 0398156\n | issue = 1\n | journal = SIAM Journal on Computing\n | pages = 73–82\n | title = Scheduling graphs on two processors\n | volume = 5\n | year = 1976}}.</ref> Sethi also shows how to implement the level assignment stage of the algorithm efficiently by using a [[disjoint-set data structure]]. In particular, with a version of this structure published later by {{harvtxt|Gabow|Tarjan|1985}}, this stage also takes linear time.<ref>{{citation\n | last1 = Gabow | first1 = Harold N.\n | last2 = Tarjan | first2 = Robert Endre | author2-link = Robert Tarjan\n | doi = 10.1016/0022-0000(85)90014-5\n | mr = 801823\n | issue = 2\n | journal = Journal of Computer and System Sciences\n | pages = 209–221\n | title = A linear-time algorithm for a special case of disjoint set union\n | volume = 30\n | year = 1985}}.</ref>\n\n==References==\n{{reflist|35em}}\n\n{{DEFAULTSORT:Coffman-Graham algorithm}}\n[[Category:Graph drawing]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Scheduling algorithms]]"
    },
    {
      "title": "Column generation",
      "url": "https://en.wikipedia.org/wiki/Column_generation",
      "text": "{{Short description|algorithm for solving linear programs}}\n\n'''Column generation''' or '''delayed column generation''' is an efficient algorithm for solving larger [[linear programming|linear programs]].\n\nThe overarching idea is that many linear programs are too large to consider all the variables explicitly.  The premise is that most of the variables will be non-basic and assume a value of zero in the optimal solution.  Because of this, only a subset of variables need to be considered in theory when solving the problem.  Column generation leverages this idea to generate only the variables which have the potential to improve the [[objective function]]—that is, to find variables with negative [[reduced cost]] (assuming [[without loss of generality]] that the problem is a minimization problem).\n\nThe problem being solved is split into two problems: the master problem and the subproblem.  The master problem is the original problem with only a subset of variables being considered.  The subproblem is a new problem created to identify a new variable.  The objective function of the subproblem is the reduced cost of the new variable with respect to the current dual variables, and the constraints require that the variable obeys the naturally occurring constraints.\n\nThe process works as follows.  The master problem is solved—from this solution, we are able to obtain dual prices for each of the constraints in the master problem.  This information is then utilized in the objective function of the subproblem.  The subproblem is solved.  If the objective value of the subproblem is negative, a variable with negative reduced cost has been identified.  This variable is then added to the master problem, and the master problem is re-solved.  Re-solving the master problem will generate a new set of dual values, and the process is repeated until no negative reduced cost variables are identified.  The subproblem returns a solution with non-negative reduced cost, we can conclude that the solution to the master problem is optimal.  \n\nIn many cases, this allows large linear programs that had been previously considered intractable to be solved.  The classical example of a problem where this is successfully used is the [[cutting stock problem]]. One particular technique in linear programming which uses this kind of approach is the [[Dantzig–Wolfe decomposition]] algorithm.  Additionally, column generation has been applied to many problems such as [[crew scheduling]], [[vehicle routing]], and the [[capacitated p-median problem]].\n\n[[Category:Optimization algorithms and methods]]\n{{Optimization algorithms}}\n{{mathapplied-stub}}"
    }
  ]
}