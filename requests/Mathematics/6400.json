{
  "pages": [
    {
      "title": "Communication-avoiding algorithms",
      "url": "https://en.wikipedia.org/wiki/Communication-avoiding_algorithms",
      "text": "'''Communication-Avoiding Algorithms''' minimize movement of data within a [[memory hierarchy]] for improving its running-time and energy consumption. These minimize the total of two costs (in terms of time and energy): arithmetic and communication. Communication, in this context refers to moving data, either between levels of memory or between multiple processors over a network. It is much more expensive than arithmetic.<ref name=\"demmel2012\">Demmel, Jim. \"Communication avoiding algorithms.\" 2012 SC Companion: High Performance Computing, Networking Storage and Analysis. IEEE, 2012.</ref>\n\n== Motivation ==\nConsider the following running-time model:<ref>Demmel, James, and Kathy Yelick. \"Communication Avoiding (CA) and Other Innovative Algorithms.\" The Berkeley Par Lab: Progress in the Parallel Computing Landscape: 243-250.</ref>\n* Measure of computation = Time per [[FLOP]] = γ\n* Measure of communication = No. of words of data moved = β\n⇒ Total running time = γ*(no. of [[FLOP]]s) + β*(no. of words)\n\nFrom the fact that '''β >> γ''' as measured in time and energy, communication cost dominates computation cost. Technological trends<ref name=\"darpa2008\">Bergman, Keren, et al. \"[http://staff.kfupm.edu.sa/ics/ahkhan/Resources/Articles/ExaScale%20Computing/TR-2008-13.pdf Exascale computing study: Technology challenges in exascale computing systems].\" [[Defense Advanced Research Projects Agency]] [[Information Processing Techniques Office]] (DARPA IPTO), Tech. Rep 15 (2008).</ref> indicate that the relative cost of communication is increasing on a variety of platforms, from [[cloud computing]] to [[supercomputers]] to mobile devices. The report also predicts that gap between [[DRAM]] access time and FLOPs will increase 100x over coming decade to balance power usage between processors and DRAM.<ref name=\"demmel2012\"/>\n\n{| class=\"wikitable\"\n|-\n! FLOP rate (γ) !! DRAM Bandwidth (β) !! Network Bandwidth (β)\n|-\n| 59% / year || 23% / year || 26% / year\n|}\n\n[[File:Energy_cost_of_data_movement_in_2010_-_on_chip_vs_off_chip.png|thumb|right|Energy cost of data movement in 2010: On-Chip vs Off-Chip]]\nEnergy consumption increases by orders of magnitude as we go higher in the memory hierarchy.<ref>Shalf, John, Sudip Dosanjh, and John Morrison. \"Exascale computing technology challenges.\" High Performance Computing for Computational Science–VECPAR 2010. Springer Berlin Heidelberg, 2011. 1-25.</ref> United States president Barack Obama cited Communication-Avoiding Algorithms in the FY 2012 Department of Energy budget request to Congress:<ref name=\"demmel2012\"/> ''“New Algorithm Improves Performance and Accuracy on Extreme-Scale Computing Systems. '''On modern computer architectures, communication between processors takes longer than the performance of a floating point arithmetic operation by a given processor'''. ASCR researchers have developed a new method, derived from commonly used linear algebra methods, '''to minimize communications between processors and the memory hierarchy, by reformulating the communication patterns specified within the algorithm.''' This method has been implemented in the TRILINOS framework, a highly-regarded suite of software, which provides functionality for researchers around the world to solve large scale, complex multi-physics problems.”''\n\n== Objective ==\nCommunication-Avoiding algorithms are designed with the following objectives:\n* Reorganize algorithms to reduce communication across all memory hierarchies.\n* Attain the lower-bound on communication when possible.\n\nThe following simple example<ref name=\"demmel2012\" /> demonstrates how these are achieved.\n\n=== Matrix Multiplication Example ===\nLet A, B and C be square matrices of order n x n. The following naive algorithm implements C = C + A * B:\n\n[[File:Matrix_multiplication_algorithm_diagram.png|500px|]]\n  for i = 1 to n\n      for j = 1 to n\n          for k = 1 to n\n              C(i,j) = C(i,j) + A(i,k) * B(k,j)\n\nArithmetic cost (time-complexity): n² (2n-1) for sufficiently large n or O(n³).\n\nRewriting this algorithm with communication cost labelled at each step\n\n  for i = 1 to n\n      {read row i of A into fast memory}               - n² reads\n      for j = 1 to n\n          {read C(i,j) into fast memory}               - n² reads\n          {read column j of B into fast memory}        - n³ reads\n          for k = 1 to n\n              C(i,j) = C(i,j) + A(i,k) * B(k,j)\n          {write C(i,j) back to slow memory}           - n² writes\n\nFast memory may be defined as the local processor memory ([[CPU cache]]) of size M and slow memory may be defined as the DRAM.\n\nCommunication cost (reads/writes): n³ + 3n² or O(n³)\n\nSince total running time = γ*O(n³) + β*O(n³) and β >> γ the communication cost is dominant. The Blocked (Tiled) Matrix Multiplication algorithm<ref name=\"demmel2012\"/> reduces this dominant term.\n\n==== Blocked (Tiled) Matrix Multiplication ====\n\nConsider A,B,C to be n/b-by-n/b matrices of b-by-b sub-blocks where b is called the block size;\tassume 3 b-by-b blocks fit in fast memory.\n\n[[File:Tiled_matrix_multiplication_diagram.png|500px|]]\n  for i = 1 to n/b\n      for j = 1 to n/b\n          {read block C(i,j) into fast memory}           - b² × (n/b)² = n² reads\n          for k = 1 to n/b\n              {read block A(i,k) into fast memory}       - b² × (n/b)³ = n³/b reads \n              {read block B(k,j) into fast memory}       - b² × (n/b)³ = n³/b reads\n              C(i,j) = C(i,j) + A(i,k) * B(k,j)          - {do a matrix multiply on blocks}\n          {write block C(i,j) back to slow memory}       - b² × (n/b)² = n² writes\n\nCommunication cost: 2n³/b + 2n² reads/writes << 2n³ arithmetic cost\n\nMaking b as large possible:\n3b<sup>2</sup> ≤ M \nWe achieve the following communication lowerbound:\n3<sup>1/2</sup>n<sup>3</sup>/M<sup>1/2</sup>  + 2n<sup>2</sup>  or Ω(no. of FLOPs / M<sup>1/2</sup> )\n\n== Previous approaches for reducing communication == \nMost of the approaches investigated in the past to address this problem rely on scheduling or tuning techniques that aim at overlapping communication with computation. However, this approach can lead to an improvement of at most a factor of two. Ghosting is a different technique for reducing communication, in which a processor stores and computes redundantly data from neighboring processors for future computations. Cache-oblivious algorithms represent a different approach introduced in 1999 for Fast Fourier Transforms,<ref>M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran, “Cacheoblivious algorithms,” In FOCS ’99: Proceedings of the 40th Annual Symposium on Foundations of Computer Science, 1999. IEEE Computer Society.</ref> and then extended to graph algorithms, dynamic programming, etc. They were also applied to several operations in linear algebra<ref>S. Toledo, “[https://pdfs.semanticscholar.org/d198/43912d46f6a25de815eadb1fb43d5ca6f61c.pdf Locality of reference in LU Decomposition with partial pivoting],”\nSIAM J. Matrix Anal. Appl., vol. 18, no. 4, 1997.</ref><ref>F. Gustavson, “Recursion Leads to Automatic Variable Blocking for Dense Linear-Algebra Algorithms,” IBM Journal of Research and Development, vol. 41, no. 6, pp. 737–755, 1997.</ref><ref>E. Elmroth, F. Gustavson, I. Jonsson, and B. Kagstrom, “[http://www.csc.kth.se/utbildning/kth/kurser/2D1253/matalg06/SIR000003.pdf Recursive blocked algorithms and hybrid data structures for dense matrix library software],” SIAM Review, vol. 46, no. 1, pp. 3–45, 2004.</ref> as dense LU and QR factorizations. The design of architecture specific algorithms is another approach that can be used for reducing the communication in parallel algorithms, and there are many examples in the literature of algorithms that are adapted to a given communication topology.<ref>Grigori, Laura. \"[http://www.lifl.fr/jncf2014/files/lecture-notes/grigori.pdf Introduction to communication avoiding linear algebra algorithms in high performance computing].</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Parallel computing]]\n[[Category:Algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Constructive heuristic",
      "url": "https://en.wikipedia.org/wiki/Constructive_heuristic",
      "text": "A '''constructive heuristic''' is a type of [[heuristic]] method which starts with an empty solution and repeatedly extends the current solution until a complete solution is obtained. It differs from local search heuristics which start with a complete solution and then try to improve the current solution further via local moves. Examples of some constructive heuristics developed for famous problems are: [[flow shop scheduling]],<ref>{{cite journal|title=Koulamas, Christos. \"A new constructive heuristic for the flowshop scheduling problem.\" European Journal of Operational Research 105.1 (1998): 66-71.}}</ref> [[vehicle routing problem]],<ref>{{cite journal|title=Petch, Russel J., and Said Salhi. \"A multi-phase constructive heuristic for the vehicle routing problem with multiple trips.\" Discrete Applied Mathematics 133.1 (2003): 69-92.|journal=Discrete Applied Mathematics|volume=133|issue=1–3|pages=69–92|url=http://www.sciencedirect.com/science/article/pii/S0166218X03004347|doi=10.1016/S0166-218X(03)00434-7|year=2003|last1=Petch|first1=R.J|last2=Salhi|first2=S.}}</ref> open shop problem.<ref>{{cite journal|title=Bräsel, H., T. Tautenhahn, and F. Werner. \"Constructive heuristic algorithms for the open shop problem.\" Computing 51.2 (1993): 95-110.}}</ref>\n\n== See also ==\n* [[Evolutionary algorithms]]\n* [[Genetic algorithms]]\n* [[Local search (optimization)]]\n* [[Metaheuristics]]\n\n==References==\n{{Reflist}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Heuristics]]"
    },
    {
      "title": "Crew scheduling",
      "url": "https://en.wikipedia.org/wiki/Crew_scheduling",
      "text": "{{refimprove|date=February 2011}}\n\n'''Crew scheduling''' is the process of assigning crews to operate transportation systems, such as rail lines or [[airlines]].\n\n== Complex ==\nMost transportation systems use software to manage the crew scheduling process. Crew scheduling becomes more and more complex as you add variables to the problem. These variables can be as simple as 1 location, 1 skill requirement, 1 shift of work and 1 set roster of people.  In the Transportation industries, such as Rail or mainly Air Travel, these variables become very complex. In Air Travel for instance, there are numerous rules or \"constraints\" that are introduced.  These mainly deal with legalities relating to work shifts and time, and a crew members qualifications for working on a particular aircraft. Add numerous locations to the equation and Collective Bargaining and Federal labor laws and these become new considerations for the problem solving method.  Fuel is also a major consideration as aircraft and other vehicles require a lot of costly fuel to operate. Finding the most efficient route and staffing it with properly qualified personnel is a critical financial consideration. The same applies to rail travel.\n\nThe problem is computationally difficult and there are competing mathematical methods of solving the problem. Although not easy to describe in one sentence, the goal is the essentially same for any method of attacking the problem: \n\"Within a set of constraints and rules, move a set roster of people with certain qualifications, from place to place with the fewest personnel and aircraft or vehicles in the least amount of time.\" Lowest cost has traditionally been the major driver for any crew scheduling solution.\n\n== 4 Parts ==\nAlthough not a \"rule\", We can describe  at least 4 parts of the equation that are ingested by the computational process:\n* People and their qualifications and abilities.\n* Aircraft or vehicles and their \"People\" qualification requirements and their cost to operate over distance.\n* Locations and the time and distance between each location.\n* Work rules for the personnel, including Shift hours and seniority.\n\nIn crew scheduling the rules and constraints are typically a combination of:\n* government regulations concerning flight time, duty time and required rest, designed to promote [[aviation safety]] and limit [[pilot fatigue]],\n* crew bid requests, vacations,\n* [[labor agreements]]\n*  [[aircraft maintenance]] schedules\n*  crew member qualification and licensing\n*  other constraints related to training\n*  pairing experienced crew members with more junior crew members\n*  returning crew to their base at the end of their trip (called [[Deadheading (aviation)|deadheading]])\n\nThe first phase in crew planning is building the crew pairings (also known as trips, rotations, among other popular descriptions).  This process pairs a generic crew member with a flight so that at the end of this process all aircraft flights are covered and all trips (combination of flights starting at a crew base and returning to that crew base or co-terminal are crew legal.  The next step is the allocation of those trips to the individual crewmember.  For the US, Canada and Australia, seniority generally rules.  The two processes (which are completely different) are referred to as bid lines and preferential bidding.  In seniority order, pilots bid for either a line of time (bidline) or trips and days off (preferential bidding.  these are awarded based on seniority and modified only when their selections have already been taken by a more senior crew member (bidlines) or their trip and day off selections (preferential bidding) do not make up a complete line (hours, days off, etc. parameters agreed to by the company and the union).  The senior folks have more time off, better choice of time off and fly better trips than the junior crew members, generally speaking. In the US, this is considered fair. For European airlines and other airlines in the rest of the world, the allocation process is completely different.  The company builds the pilot schedules directly to meet their needs, not the pilot's needs.  Before assigning a single trip, the schedulers put all planned absences (vacation, training, etc.) onto the crew members' schedule.  Only then are trips assigned to the individual crew members.  As such, fairness means that the most senior captain and the most junior captain have the same amount of duty time, block hours, night time, time away from base, layovers, expense pay, etc. in a given schedule period.  Seniority is out and all work is completely homogenized.  For them, anything else is unfair, undemocratic. Slowly over the last thirty years, foreign airlines using the \"no seniority\" rostering system have allowed some measure of seniority to creep into the allocation process from pilots who may now ask for a specific day off or trip once a quarter or make multiple requests within a schedule period.  Although this may sound very much like preferential bidding, it is not.  The disparity between junior and senior crew members is still very limited and thus achievement of your choices is limited.\n\n== Disruptions ==\nAdditional unplanned disruptions in schedules due to weather and [[air traffic control]] delays can disrupt schedules, so crew scheduling software remains an area for ongoing research.<ref>http://www.engr.pitt.edu/~schaefer/Papers/UncertainCrewSched.pdf \"[[Airline crew]] Scheduling under Uncertainty\"\n</ref>\n\n== See also ==\n* [[Preferential bidding system]]\n* [[Fatigue (safety)]]\n* [[Automated planning and scheduling]]\n* [[Linear programming]]\n* [[Column generation]]\n* [[Tabu search]]\n* [[Fatigue Avoidance Scheduling Tool]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* {{cite news |url= https://airwaysmag.com/industry/solving-the-nightmare-of-crew-scheduling/ |title= Solving The Nightmare Of Crew Scheduling |date= January 17, 2019 |work= Airways Mag |author= Alex Osleger}}\n\n[[Category:Aviation safety]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Transportation planning]]\n[[Category:Transport safety]]\n[[Category:Scheduling (transportation)]]"
    },
    {
      "title": "Criss-cross algorithm",
      "url": "https://en.wikipedia.org/wiki/Criss-cross_algorithm",
      "text": "{{about|an algorithm for mathematical optimization|the naming of chemicals|crisscross method | the American rap duo of the 1990s | Kris Kross}}\n{{Use dmy dates|date=December 2013}}\n<!-- {{Context|date=May 2012}} -->\n[[File:Unitcube.svg|thumb|right|alt=A three-dimensional cube|The criss-cross algorithm visits all&nbsp;8 corners of the [[Klee–Minty cube]] in the worst case. It visits&nbsp;3 additional corners on&nbsp;average. The Klee–Minty cube is a perturbation of the cube shown here.]]\nIn [[optimization (mathematics)|mathematical optimization]], the '''criss-cross algorithm''' is any of a family of [[algorithm]]s for [[linear programming]]. Variants of the criss-cross algorithm also solve more general problems with [[linear programming|linear inequality constraints]] and [[nonlinear programming|nonlinear]] [[optimization (mathematics)|objective functions]]; there are criss-cross algorithms for [[linear-fractional programming]] problems,<ref name=\"LF99Hyperbolic\">{{harvtxt|Illés|Szirmai|Terlaky|1999}}</ref><ref name=\"Bibl\" >{{cite journal|first=I.&nbsp;M.|last=Stancu-Minasian|title=A sixth bibliography of fractional programming|journal=Optimization|volume=55|number=4|date=August 2006|pages=405–428|doi=10.1080/02331930600819613|mr=2258634}}</ref> [[quadratic programming|quadratic-programming]] problems, and [[linear complementarity problem]]s.<ref name=\"FukudaTerlaky\" >{{harvtxt|Fukuda|Terlaky|1997}}</ref>\n\nLike the [[simplex algorithm]] of [[George Dantzig|George B. Dantzig]], the criss-cross algorithm is not a [[time complexity|polynomial-time algorithm]] for linear programming. Both algorithms visit all&nbsp;2<sup>''D''</sup>&nbsp;corners of a (perturbed) [[unit cube|cube]] in dimension&nbsp;''D'', the [[Klee–Minty cube]] (after [[Victor Klee]] and [[George J. Minty]]), in the [[worst-case complexity|worst case]].<ref name=\"Roos\" >{{harvtxt|Roos|1990}}</ref><ref name=\"KleeMinty\"/> However, when it is started at a random corner, the criss-cross algorithm [[Average-case complexity|on&nbsp;average]] visits only&nbsp;''D'' additional corners.<ref name=\"FTNamiki\"/><ref name=\"FukudaNamiki\"/><ref name=\"Borgwardt\">The simplex algorithm takes on average&nbsp;''D'' steps for a cube. {{harvtxt|Borgwardt|1987}}: {{cite book|last=Borgwardt|first=Karl-Heinz|title=The simplex method: A probabilistic analysis|series=Algorithms and Combinatorics (Study and Research Texts)|volume=1|publisher=Springer-Verlag|location=Berlin|year=1987|pages=xii+268|isbn=978-3-540-17096-9|mr=868467|ref=harv}}</ref> Thus, for the three-dimensional cube, the algorithm visits all&nbsp;8 corners in the worst case and exactly&nbsp;3 additional corners on&nbsp;average.\n\n==History==\nThe criss-cross algorithm was published independently by [[Tamás Terlaky]]<ref>{{harvtxt|Terlaky|1985}} and {{harvtxt|Terlaky|1987}}</ref> and by Zhe-Min Wang;<ref name=\"Wang\" >{{harvtxt|Wang|1987}}</ref> related algorithms appeared in unpublished reports by other authors.<ref name=\"FukudaTerlaky\"/>\n\n==Comparison with the simplex algorithm for linear optimization==\n{{See also|Linear programming|Simplex algorithm|Bland's rule}}\n[[File:Simplex description.png|thumb|240px|In its second phase, the ''simplex algorithm'' crawls along the edges of the polytope until it finally reaches an optimum [[vertex (geometry)|vertex]]. The ''criss-cross algorithm'' considers bases that are not associated with vertices, so that some iterates can be in the ''interior ''of the feasible region, like interior-point algorithms; the criss-cross algorithm can also have ''infeasible'' iterates ''outside'' the feasible region.]]\nIn linear programming, the criss-cross algorithm pivots between a sequence of bases but differs from the [[simplex algorithm]] of [[George Dantzig]]. The simplex algorithm first finds a (primal-) feasible basis by solving a \"''phase-one'' problem\"; in \"phase two\", the simplex algorithm pivots between a sequence of basic ''feasible ''solutions so that the objective function is non-decreasing with each pivot, terminating when with an optimal solution (also finally finding a \"dual feasible\" solution).<ref name=\"FukudaTerlaky\"/><ref name=\"TerlakyZhang\">{{harvtxt|Terlaky|Zhang|1993}}</ref>\n\nThe criss-cross algorithm is simpler than the simplex algorithm, because the criss-cross algorithm only has one phase. Its pivoting rules are similar to the [[Bland's rule|least-index pivoting rule of Bland]].<ref name=\"Bland\">\n{{cite journal|title=New finite pivoting rules for the simplex method|first=Robert G.|last=Bland|journal=Mathematics of Operations Research|volume=2|number=2|date=May 1977|pages=103–107|doi=10.1287/moor.2.2.103|jstor=3689647|mr=459599|ref=harv}}</ref> Bland's rule uses only [[sign function|sign]]s of coefficients rather than their [[real number#Axiomatic approach|(real-number) order]] when deciding eligible pivots. Bland's rule selects an entering variables by comparing values of reduced costs, using the real-number ordering of the eligible pivots.<ref name=\"Bland\"/><ref>Bland's rule is also related to an earlier least-index rule, which was proposed by Katta&nbsp;G. Murty for the [[linear complementarity problem]], according to {{harvtxt|Fukuda|Namiki|1994}}.</ref> Unlike Bland's rule, the criss-cross algorithm is \"purely combinatorial\", selecting an entering variable and a leaving variable by considering only the signs of coefficients rather than their real-number ordering.<ref name=\"FukudaTerlaky\"/><ref name=\"TerlakyZhang\"/> The  criss-cross algorithm has been applied to furnish constructive proofs of basic results in [[real number|real]] [[linear algebra]], such as <!-- [[Steinitz's theorem|Steinitz's lemma]], --> the [[Farkas lemma|lemma of Farkas]]<!-- , [[Weyl's theorem]] on the finite generation of [[convex polytope]]s by linear inequalities ([[halfspace]]s), and the [[Krein–Milman theorem|Minkowski's theorem]] on [[extreme point]]s -->.<ref name=\"KT91\" >{{harvtxt|Klafszky|Terlaky|1991}}</ref>\n\nWhile most simplex variants are monotonic in the objective (strictly in the non-degenerate case), most variants of the criss-cross algorithm lack a monotone merit function which can be a disadvantage in practice.\n\n==Description==\n{{Expand section|date=April 2011}}\nThe criss-cross algorithm works on a standard pivot tableau (or on-the-fly calculated parts of a tableau, if implemented like the revised simplex method). In a general step, if the tableau is primal or dual infeasible, it selects one of the infeasible rows / columns as the pivot row / column using an index selection rule. An important property is that the selection is made on the union of the infeasible indices and the standard version of the algorithm does not distinguish column and row indices (that is, the column indices basic in the rows). If a row is selected then the algorithm uses the index selection rule to identify a position to a dual type pivot, while if a column is selected then it uses the index selection rule to find a row position and carries out a primal type pivot.\n\n==Computational complexity: Worst and average cases==\n[[File:Ellipsoid 2.png|thumb|right<!-- 400px -->|The worst-case computational complexity of Khachiyan's ''ellipsoidal algorithm'' is a polynomial. The ''criss-cross algorithm'' has exponential complexity.]]\nThe [[time complexity]] of an [[algorithm]] counts the number of [[arithmetic operation]]s sufficient for the algorithm to solve the problem. For example, [[Gaussian elimination]] requires on the [[Big oh|order&nbsp;of]]''&nbsp;D''<sup>3</sup> operations, and so it is said to have polynomial time-complexity, because its complexity is bounded by a [[cubic polynomial]]. There are examples of algorithms that do not have polynomial-time complexity. For example, a generalization of Gaussian elimination called [[Buchberger's algorithm]] has for its complexity an <!--doubly --> exponential function of the problem data (the [[degree of a polynomial|degree of the polynomial]]s and the number of variables of the [[multivariate polynomial]]s). Because exponential functions eventually grow much faster than polynomial functions, an<!-- attained rather than upper bound --> exponential complexity implies that an algorithm has slow performance on large problems.\n\nSeveral algorithms for linear programming—[[Khachiyan]]'s [[ellipsoidal algorithm]], [[Karmarkar]]'s [[Karmarkar's algorithm|projective algorithm]], and [[interior-point method|central-path algorithm]]s—have polynomial time-complexity (in the [[worst case complexity|worst case]] and thus [[average case complexity|on&nbsp;average]]). The ellipsoidal and projective algorithms were published before the  criss-cross algorithm.\n\nHowever, like the simplex algorithm of Dantzig, the criss-cross algorithm is ''not'' a polynomial-time algorithm for linear programming. Terlaky's criss-cross algorithm visits all the&nbsp;2<sup>''D''</sup>&nbsp;corners of a (perturbed) cube in dimension&nbsp;''D'', according to a paper of Roos; Roos's paper modifies the [[Victor Klee|Klee]]–Minty construction of a [[unit cube|cube]] on which the simplex algorithm takes&nbsp;2<sup>''D''</sup>&nbsp;steps.<ref name=\"FukudaTerlaky\"/><ref name=\"Roos\"/><ref name=\"KleeMinty\">{{cite book|title=Inequalities&nbsp;III (Proceedings of the Third Symposium on Inequalities held at the University of California, Los Angeles, Calif., September&nbsp;1–9,&nbsp;1969, dedicated to the memory of Theodore&nbsp;S. Motzkin)|editor-first=Oved|editor-last=Shisha|publisher=Academic Press|location=New York-London|year=1972|mr=332165|last1=Klee|first1=Victor|authorlink1=Victor Klee|last2=Minty|first2= George&nbsp;J.|authorlink2=George J. Minty|chapter=How good is the simplex algorithm?|pages=159–175|ref=harv}}</ref> Like the simplex algorithm, the criss-cross algorithm visits all&nbsp;8 corners of the three-dimensional cube in the worst case.\n\nWhen it is initialized at a random corner of the cube, the criss-cross algorithm visits only&nbsp;''D'' additional corners, however, according to a&nbsp;1994 paper by Fukuda and Namiki.<ref name=\"FTNamiki\" >{{harvtxt|Fukuda|Terlaky|1997|p=385}}</ref><ref name=\"FukudaNamiki\" >{{harvtxt|Fukuda|Namiki|1994|p=367}}</ref> Trivially, the simplex algorithm takes on average&nbsp;''D'' steps for a cube.<ref name=\"Borgwardt\"/><ref>More generally, for the simplex algorithm, the expected number of steps is  proportional to&nbsp;''D'' for linear-programming problems that are randomly drawn from the [[Euclidean metric|Euclidean]] [[unit&nbsp;sphere]], as proved by Borgwardt and by [[Stephen Smale|Smale]].</ref> Like the simplex algorithm, the criss-cross algorithm visits exactly&nbsp;3 additional corners of the three-dimensional cube on&nbsp;average.\n\n==Variants==\nThe criss-cross algorithm has been extended to solve more general problems than linear programming problems.\n\n===Other optimization problems with linear constraints===\nThere are variants of the criss-cross algorithm for linear programming, for [[quadratic programming]], and for the [[linear complementarity problem|linear-complementarity problem]] with \"sufficient matrices\";<ref name=\"FukudaTerlaky\"/><ref name=\"FTNamiki\"/><ref name=\"FukudaNamikiLCP\" >{{harvtxt|Fukuda|Namiki|1994|}}</ref><ref name=\"OMBook\" >{{cite book|last=Björner|first=Anders|last2=Las&nbsp;Vergnas|first2=Michel|author2-link=Michel Las Vergnas|last3=Sturmfels|first3=Bernd|authorlink3=Bernd Sturmfels|last4=White|first4=Neil|last5=Ziegler|first5=Günter|authorlink5=Günter M. Ziegler|title=Oriented Matroids|chapter=10 Linear programming|publisher=Cambridge University Press|year=1999|isbn=978-0-521-77750-6|chapter-url=http://ebooks.cambridge.org/ebook.jsf?bid=CBO9780511586507|pages=417–479|doi=10.1017/CBO9780511586507|mr=1744046}}</ref><ref name=\"HRT\">{{cite journal|first1=D. |last1=den&nbsp;Hertog|first2=C.|last2=Roos|first3=T.|last3=Terlaky|title=The linear complementarity problem, sufficient matrices, and the criss-cross method|journal=Linear Algebra and its Applications|volume=187|date=1 July 1993|pages=1–14|url=http://core.ac.uk/download/pdf/6714737.pdf|doi=10.1016/0024-3795(93)90124-7}}</ref><ref name=\"CIsufficient\">{{cite journal|first1=Zsolt|last1=Csizmadia|first2=Tibor|last2=Illés|title=New criss-cross type algorithms for linear complementarity problems with sufficient matrices|journal=Optimization Methods and Software|volume=21|year=2006|number=2|pages=247–266|doi=10.1080/10556780500095009|\nurl=http://www.cs.elte.hu/opres/orr/download/ORR03_1.pdf|format=pdf<!--|eprint=http://www.tandfonline.com/doi/pdf/10.1080/10556780500095009-->|mr=2195759|<!-- ref=harv -->}}</ref> conversely, for linear complementarity problems, the criss-cross algorithm terminates finitely only if the matrix is a sufficient matrix.<ref name=\"HRT\"/><ref name=\"CIsufficient\"/> A [[sufficient&nbsp;matrix]] is a generalization both of a [[positive-definite matrix]] and of a [[P-matrix]], whose [[principal&nbsp;minor]]s are each positive.<ref name=\"HRT\"/><ref name=\"CIsufficient\"/><ref>{{cite journal|last1=Cottle|first1=R.&nbsp;W.|authorlink1=Richard W. Cottle|last2=Pang|first2=J.-S.|last3=Venkateswaran|first3=V.|title=Sufficient matrices and the linear&nbsp;complementarity problem|journal=Linear Algebra and its Applications|volume=114–115|date=March–April 1989|pages=231–249|doi=10.1016/0024-3795(89)90463-1|url=http://www.sciencedirect.com/science/article/pii/0024379589904631|mr=986877|ref=harv}}</ref> The criss-cross algorithm has been adapted also for [[linear-fractional programming]].<ref name=\"LF99Hyperbolic\"/><ref name=\"Bibl\"/>\n\n===Vertex enumeration===\nThe criss-cross algorithm was used in an algorithm for [[Vertex enumeration problem|enumerating all the vertices of a polytope]], which was published by [[David Avis]] and Komei Fukuda in&nbsp;1992.<ref>{{harvtxt|Avis|Fukuda|1992|p=297}}</ref> Avis and Fukuda presented an algorithm which finds the&nbsp;''v'' vertices of a [[polyhedron]] defined by a nondegenerate system of&nbsp;''n'' [[linear inequality|linear inequalities]] in&nbsp;''D'' [[dimension (vector space)|dimension]]s (or, dually, the&nbsp;''v'' [[facet]]s of the [[convex hull]] of&nbsp;''n'' points in&nbsp;''D'' dimensions, where each facet contains exactly&nbsp;''D'' given points) in time&nbsp;[[Big Oh notation|O]](''nDv'') and&nbsp;O(''nD'') [[space complexity|space]].<ref>The&nbsp;''v'' vertices in a simple arrangement of&nbsp;''n'' [[hyperplane]]s in&nbsp;''D'' dimensions can be found in&nbsp;O(''n''<sup>2</sup>''Dv'') time and O(''nD'') [[space complexity]].</ref>\n\n===Oriented matroids===\n[[File:max-flow min-cut example.svg|frame|right|The [[max-flow min-cut theorem]] states that the maximum flow through a network is exactly the capacity of its minimum cut. This theorem can be proved using the criss-cross algorithm for oriented matroids.]]\nThe criss-cross algorithm is often studied using the theory of [[oriented matroid]]s (OMs), which is a [[combinatorics|combinatorial]] abstraction of linear-optimization theory.<ref name=\"OMBook\"/><ref>The theory of [[oriented matroid]]s was initiated by [[R.&nbsp;Tyrrell Rockafellar]]. {{harv|Rockafellar|1969}}:<p>{{cite book\n|first=R.&nbsp;T.\n|last=Rockafellar\n|authorlink=R. Tyrrell Rockafellar\n|chapter=The elementary vectors of a subspace of <math>R^N</math> (1967)\n|pages=104–127\n|editor=[[R. C. Bose]] and T.&nbsp;A. Dowling \n|year=1969 \n|title=Combinatorial Mathematics and its Applications\n|series=The University of North Carolina Monograph Series in Probability and Statistics\n|location=Chapel Hill, North Carolina\n|publisher=University of North Carolina Press.\n|issue=4\n|mr=278972\n|chapter-url=http://www.math.washington.edu/~rtr/papers/rtr-ElemVectors.pdf|ref=harv|id=[http://www.math.washington.edu/~rtr/papers/rtr-ElemVectors.pdf PDF reprint]}}</p><p>Rockafellar was influenced by the earlier studies of [[Albert W. Tucker]] and [[George J. Minty]]. Tucker and Minty had studied the sign patterns of the matrices arising through the pivoting operations of Dantzig's simplex algorithm.</p> \n</ref> Indeed, Bland's pivoting rule was based on his previous papers on oriented-matroid theory. However, Bland's rule exhibits cycling on some oriented-matroid linear-programming problems.<ref name=\"OMBook\"/> The first purely combinatorial algorithm for linear programming was devised by [[Michael J. Todd (mathematician)|Michael&nbsp;J. Todd]].<ref name=\"OMBook\"/><ref name=\"Todd\"/> Todd's algorithm was developed not only for linear-programming in the setting of oriented matroids, but also for  [[quadratic programming|quadratic-programming problems]] and [[linear complementarity problem|linear-complementarity problem]]s.<ref name=\"OMBook\"/><ref name=\"Todd\" >{{cite journal|last=Todd|first=Michael&nbsp;J.|authorlink=Michael J. Todd (mathematician)|title=Linear and quadratic programming in oriented matroids|journal=Journal of Combinatorial Theory|series=Series&nbsp;B|volume=39|year=1985|number=2|pages=105–133|mr=811116|doi=10.1016/0095-8956(85)90042-5|ref=harv}}</ref> Todd's algorithm is complicated even to state, unfortunately, and its finite-convergence proofs are somewhat complicated.<ref name=\"OMBook\"/>\n\nThe criss-cross algorithm and its proof of finite termination can be simply stated and readily extend the setting of oriented matroids. The algorithm can be further simplified for ''linear feasibility problems'', that is for [[linear system]]s with [[linear inequality|nonnegative variable]]s; these problems can be formulated for oriented matroids.<ref name=\"KT91\"/>  The criss-cross algorithm has been adapted for problems that are more complicated than linear programming: There are oriented-matroid variants also for the quadratic-programming problem and for the linear-complementarity problem.<ref name=\"FukudaTerlaky\"/><ref name=\"FukudaNamikiLCP\"/><ref name=\"OMBook\"/>\n\n==Summary==\nThe criss-cross algorithm is a simply stated algorithm for linear programming. It was the second fully combinatorial algorithm for linear programming. The partially combinatorial simplex algorithm of Bland cycles on some (nonrealizable) oriented matroids. The first fully combinatorial algorithm was published by Todd, and it is also like the simplex algorithm in that it preserves feasibility after the first feasible basis is generated; however, Todd's rule is complicated. The criss-cross algorithm is not a simplex-like algorithm, because it need not maintain feasibility. The criss-cross algorithm does not have polynomial time-complexity, however.\n\nResearchers have extended the criss-cross algorithm for many optimization-problems, including linear-fractional programming. The criss-cross algorithm can solve quadratic programming problems and linear complementarity problems, even in the setting of oriented matroids. Even when generalized, the criss-cross algorithm remains simply stated.\n\n==See also==\n* [[Jack Edmonds]] (pioneer of combinatorial optimization and oriented-matroid theorist; doctoral advisor of Komei Fukuda)\n\n==Notes==\n<references/>\n\n==References==\n* {{cite journal |first1=David |last1=Avis |first2=Komei |last2=Fukuda |authorlink2=Komei Fukuda |authorlink1=David Avis |title=A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra|journal=[[Discrete and Computational Geometry]] |volume=8 |date=December 1992 |pages=295–313 |doi=10.1007/BF02293050 |issue=ACM Symposium on Computational Geometry (North Conway, NH, 1991) number 1 |mr=1174359|ref=harv}}\n* {{cite journal|first1=Zsolt|last1=Csizmadia|first2=Tibor|last2=Illés|title=New criss-cross type algorithms for linear complementarity problems with sufficient matrices|journal=Optimization Methods and Software|volume=21|year=2006|number=2|pages=247–266|doi=10.1080/10556780500095009|\nurl=http://www.cs.elte.hu/opres/orr/download/ORR03_1.pdf|format=pdf<!--|eprint=http://www.tandfonline.com/doi/pdf/10.1080/10556780500095009--> |mr=2195759|ref=harv}}\n* {{cite journal|last1=Fukuda|first1=Komei|authorlink1=Komei Fukuda|last2=Namiki|first2=Makoto|title=On extremal behaviors of Murty's least index method|journal=Mathematical Programming|date=March 1994|pages=365–370|volume=64|number=1|doi=10.1007/BF01582581|ref=harv|mr=1286455}}\n* {{cite journal|first1=Komei|last1=Fukuda| authorlink1=Komei Fukuda  |first2=Tamás|last2=Terlaky| authorlink2=Tamás Terlaky |title=Criss-cross methods: A fresh view on pivot algorithms |journal=Mathematical Programming, Series B|volume=79|pages=369–395|issue=Papers from the&nbsp;16th International Symposium on Mathematical Programming held in Lausanne,&nbsp;1997, number 1–3 |editor1-first=Thomas&nbsp;M.|editor1-last=Liebling|editor2-first=Dominique|editor2-last=de&nbsp;Werra|year=1997|doi=10.1007/BF02614325|mr=1464775|ref=harv|id=[http://www.cas.mcmaster.ca/~terlaky/files/crisscross.ps Postscript preprint]|citeseerx=10.1.1.36.9373}}\n* {{cite journal|first1=D.|last1=den&nbsp;Hertog|first2=C.|last2=Roos|first3=T.|last3=Terlaky|title=The linear complementarity problem, sufficient matrices, and the criss-cross method|journal=Linear Algebra and its Applications|volume=187|date=1 July 1993|pages=1–14|url=http://core.ac.uk/download/pdf/6714737.pdf|doi=10.1016/0024-3795(93)90124-7|ref=harv|mr=1221693}}\n* {{<!-- citation -->cite journal|title=The finite criss-cross method for hyperbolic programming|journal=European Journal of Operational Research|volume=114|number=1|\npages=198–214|year=1999|<!-- issn=0377-2217 -->|doi=10.1016/S0377-2217(98)00049-6|url=http://www.sciencedirect.com/science/article/B6VCT-3W3DFHB-M/2/4b0e2fcfc2a71e8c14c61640b32e805a\n|first1=Tibor|last1=Illés|first2=Ákos|last2=Szirmai|first3=Tamás|last3=Terlaky|zbl=0953.90055|id=[http://www.cas.mcmaster.ca/~terlaky/files/dut-twi-96-103.ps.gz Postscript preprint]|ref=harv}}\n*{{cite journal|first1=Emil|last1=Klafszky|first2=Tamás|last2=Terlaky|title=The role of pivoting in proving some fundamental theorems of linear algebra|journal=Linear Algebra and its Applications|volume=151|date=June 1991|pages=97–118|doi=10.1016/0024-3795(91)90356-2|url=http://www.cas.mcmaster.ca/~terlaky/files/pivot-la.ps|format=postscript|ref=harv|mr=1102142}}\n* {{cite journal|last=Roos|first=C.|title=An exponential example for Terlaky's pivoting rule for the criss-cross simplex method|journal=Mathematical Programming|volume=46|year=1990|number=1|series=Series&nbsp;A|pages=79–84|doi=10.1007/BF01585729|mr=1045573|ref=harv|<!-- Google scholar reported no free versions -->}}\n* {{cite journal|last=Terlaky|first=T.|title=A convergent criss-cross method|journal=Optimization: A Journal of Mathematical Programming and Operations Research|volume=16|year=1985|number=5|pages=683–690|issn=0233-1934|doi=10.1080/02331938508843067|ref=harv|mr=798939|<!-- Google scholar reported no free versions -->}}\n* {{cite journal|last=Terlaky|first=Tamás|authorlink=Tamás Terlaky|title=A finite crisscross method for oriented matroids|volume=42|year=1987|number=3|pages=319–327|journal=Journal of Combinatorial Theory|series=Series&nbsp;B|issn=0095-8956|doi=10.1016/0095-8956(87)90049-9|mr=888684|ref=harv|<!-- Google scholar reported no free versions -->}}\n* {{cite journal|last1=Terlaky|first1=Tamás| authorlink1=Tamás Terlaky |last2=Zhang|first2=Shu&nbsp;Zhong|title=Pivot rules for linear programming: A Survey on recent theoretical developments|issue=Degeneracy in optimization problems, number 1 |journal=Annals of Operations Research|volume=46–47|year=1993|pages=203–233 |doi=10.1007/BF02096264|mr=1260019 |citeseerx  = 10.1.1.36.7658 | origyear = 1991 |issn=0254-5330|ref=harv}}\n* {{cite journal|last=Wang|first=Zhe&nbsp;Min|title=A finite conformal-elimination free algorithm over oriented&nbsp;matroid programming|journal=Chinese Annals of Mathematics (Shuxue Niankan&nbsp;B&nbsp;Ji)|series=Series&nbsp;B|volume=8|year=1987|number=1|pages=120–125|issn=0252-9599|mr=886756|ref=harv|<!-- Google scholar reported no free versions -->}}\n\n==External links==\n* [http://www.ifor.math.ethz.ch/~fukuda/ Komei Fukuda (ETH Zentrum, Zurich)] with [http://www.ifor.math.ethz.ch/~fukuda/publ/publ.html publications]\n* [http://coral.ie.lehigh.edu/~terlaky/ Tamás Terlaky  (Lehigh University)] with [http://coral.ie.lehigh.edu/~terlaky/publications publications]\n\n{{Mathematical programming|state=expanded}}\n{{Optimization algorithms|convex|state=collapsed}}\n\n[[Category:Linear programming]]\n[[Category:Oriented matroids]]\n[[Category:Combinatorial optimization]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Combinatorial algorithms]]\n[[Category:Geometric algorithms]]\n[[Category:Exchange algorithms]]"
    },
    {
      "title": "Critical line method",
      "url": "https://en.wikipedia.org/wiki/Critical_line_method",
      "text": "#REDIRECT [[Portfolio_optimization#Specific_approaches]]\n[[Category:Financial economics]]\n[[Category:Portfolio theories]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Cross-entropy method",
      "url": "https://en.wikipedia.org/wiki/Cross-entropy_method",
      "text": "The '''cross-entropy''' ('''CE''') '''method''' is a [[Monte Carlo method|Monte Carlo]] method for [[importance sampling]] and [[Optimization (mathematics)|optimization]]. It is applicable to both [[Combinatorial optimization|combinatorial]] and [[Continuous optimization|continuous]] problems, with either a static or noisy objective.\n\nThe method approximates the optimal importance sampling estimator by repeating two phases:<ref>Rubinstein, R.Y. and  Kroese, D.P. (2004), The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning, Springer-Verlag, New York {{ISBN|978-0-387-21240-1}}.</ref>\n\n#Draw a sample from a probability distribution.\n#Minimize the [[cross entropy|''cross-entropy'']] between this distribution and a target distribution to produce a better sample in the next iteration.\n\n[[Reuven Rubinstein]] developed the method in the context of ''rare event simulation'', where tiny probabilities must be estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The method has also been applied to the [[traveling salesman problem|traveling salesman]], [[quadratic assignment problem|quadratic assignment]], [[Sequence alignment|DNA sequence alignment]], [[Maxcut|max-cut]] and buffer allocation problems.\n\n==Estimation via importance sampling==\nConsider the general problem of estimating the quantity\n\n<math>\\ell = \\mathbb{E}_{\\mathbf{u}}[H(\\mathbf{X})] = \\int H(\\mathbf{x})\\, f(\\mathbf{x}; \\mathbf{u})\\, \\textrm{d}\\mathbf{x}</math>,\n\nwhere <math>H</math> is some ''performance function'' and <math>f(\\mathbf{x};\\mathbf{u})</math> is a member of some [[parametric family]] of distributions. Using [[importance sampling]] this quantity can be estimated as\n\n<math>\\hat{\\ell} = \\frac{1}{N} \\sum_{i=1}^N H(\\mathbf{X}_i) \\frac{f(\\mathbf{X}_i; \\mathbf{u})}{g(\\mathbf{X}_i)}</math>,\n\nwhere <math>\\mathbf{X}_1,\\dots,\\mathbf{X}_N</math> is a random sample from <math>g\\,</math>. For positive <math>H</math>, the theoretically ''optimal'' importance sampling [[probability density function|density]] (PDF) is given by\n\n<math> g^*(\\mathbf{x}) = H(\\mathbf{x}) f(\\mathbf{x};\\mathbf{u})/\\ell</math>.\n\nThis, however, depends on the unknown <math>\\ell</math>. The CE method aims to approximate the optimal PDF by adaptively selecting members of the parametric family that are closest (in the [[Kullback–Leibler divergence|Kullback–Leibler]] sense) to the optimal PDF <math>g^*</math>.\n\n==Generic CE algorithm==\n# Choose initial parameter vector <math>\\mathbf{v}^{(0)}</math>; set t = 1.\n# Generate a random sample <math>\\mathbf{X}_1,\\dots,\\mathbf{X}_N</math> from <math>f(\\cdot;\\mathbf{v}^{(t-1)})</math>\n# Solve for <math>\\mathbf{v}^{(t)}</math>, where<br><math>\\mathbf{v}^{(t)} = \\mathop{\\textrm{argmax}}_{\\mathbf{u}} \\frac{1}{N} \\sum_{i=1}^N H(\\mathbf{X}_i)\\frac{f(\\mathbf{X}_i;\\mathbf{u})}{f(\\mathbf{X}_i;\\mathbf{v}^{(t-1)})} \\log f(\\mathbf{X}_i;\\mathbf{v}^{(t-1)})</math>\n# If convergence is reached then '''stop'''; otherwise, increase t by 1 and reiterate from step 2.\n\nIn several cases, the solution to step 3 can be found ''analytically''.  Situations in which this occurs are\n* When <math>f\\,</math> belongs to the [[Exponential family|natural exponential family]]\n* When <math>f\\,</math> is [[discrete space|discrete]] with finite [[Support (mathematics)|support]]\n* When <math>H(\\mathbf{X}) = \\mathrm{I}_{\\{\\mathbf{x}\\in A\\}}</math> and <math>f(\\mathbf{X}_i;\\mathbf{u}) = f(\\mathbf{X}_i;\\mathbf{v}^{(t-1)})</math>, then <math>\\mathbf{v}^{(t)}</math> corresponds to the [[Maximum likelihood|maximum likelihood estimator]] based on those <math>\\mathbf{X}_k \\in A</math>.\n\n== Continuous optimization&mdash;example==\nThe same CE algorithm can be used for optimization, rather than estimation. \nSuppose the problem is to maximize some function <math>S(x)</math>, for example, \n<math>S(x) = \\textrm{e}^{-(x-2)^2} + 0.8\\,\\textrm{e}^{-(x+2)^2}</math>. \nTo apply CE, one considers first the ''associated stochastic problem'' of estimating\n<math>\\mathbb{P}_{\\boldsymbol{\\theta}}(S(X)\\geq\\gamma)</math>\nfor a given ''level'' <math>\\gamma\\,</math>, and parametric family <math>\\left\\{f(\\cdot;\\boldsymbol{\\theta})\\right\\}</math>, for example the 1-dimensional \n[[Gaussian distribution]],\nparameterized by its mean <math>\\mu_t\\,</math> and variance <math>\\sigma_t^2</math> (so <math>\\boldsymbol{\\theta} = (\\mu,\\sigma^2)</math> here).\nHence, for a given <math>\\gamma\\,</math>, the goal is to find <math>\\boldsymbol{\\theta}</math> so that\n<math>D_{\\mathrm{KL}}(\\textrm{I}_{\\{S(x)\\geq\\gamma\\}}\\|f_{\\boldsymbol{\\theta}})</math>\nis minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above.\nIt turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and\nparametric family are the sample mean and sample variance corresponding to the ''elite samples'', which are those samples that have objective function value <math>\\geq\\gamma</math>.\nThe worst of the elite samples is then used as the level parameter for the next iteration.\nThis yields the following randomized algorithm that happens to coincide with the so-called Estimation of Multivariate Normal Algorithm (EMNA), an [[estimation of distribution algorithm]].\n\n===Pseudo-code===\n // Initialize parameters\n mu:=-6\n sigma2:=100\n t:=0\n maxits:=100\n N:=100\n Ne:=10\n // While maxits not exceeded and not converged\n '''while''' t < maxits and sigma2 > epsilon\n   // Obtain N samples from current sampling distribution\n   X:=SampleGaussian(mu,sigma2,N)\n   // Evaluate objective function at sampled points\n   S:=exp(-(X-2)^2) + 0.8 exp(-(X+2)^2)\n   // Sort X by objective function values in descending order\n   X:=sort(X,S)\n   // Update parameters of sampling distribution                  \n   mu:=mean(X(1:Ne))\n   sigma2:=var(X(1:Ne))\n   t:=t+1\n // Return mean of final sampling distribution as solution\n '''return''' mu\n\n==Related methods==\n*[[Simulated annealing]]\n*[[Genetic algorithms]]\n*[[Harmony search]]\n*[[Estimation of distribution algorithm]]\n*[[Tabu search]]\n*[[Natural Evolution Strategy]]\n\n==See also==\n*[[Cross entropy]]\n*[[Kullback–Leibler divergence]]\n*[[Randomized algorithm]]\n*[[Importance sampling]]\n\n== Journal Papers ==\n* De Boer, P-T., Kroese, D.P, Mannor, S. and Rubinstein, R.Y. (2005). A Tutorial on the Cross-Entropy Method. ''Annals of Operations Research'', '''134''' (1), 19–67.[http://www.maths.uq.edu.au/~kroese/ps/aortut.pdf]\n*Rubinstein, R.Y. (1997). Optimization of Computer simulation Models with Rare Events, ''European Journal of Operational Research'', '''99''', 89–112.\n\n==Software Implementations==\n*[https://cran.r-project.org/web/packages/CEoptim/index.html '''CEoptim''' R package]\n\n==References==\n{{reflist}}\n\n[[Category:Heuristics]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Monte Carlo methods]]\n[[Category:Machine learning]]"
    },
    {
      "title": "Cunningham's rule",
      "url": "https://en.wikipedia.org/wiki/Cunningham%27s_rule",
      "text": "In [[mathematical optimization]], '''Cunningham's rule''' (also known as '''least recently considered rule''' or '''round-robin rule''') is an algorithmic refinement of the [[simplex method]] for [[linear programming|linear optimization]].\n\nThe rule was proposed 1979 by [[W. H. Cunningham]] to defeat the deformed hypercube constructions by Klee and Minty et. al. (see, e.g. [[Klee–Minty cube]]).<ref>{{cite journal|first1=W.H.|last1=Cunningham|title=Theoretical properties of the network simplex method.|journal=Mathematics of Operations Research|date=1979}}</ref>\n\nCunningham's rule assigns a cyclic order to the variables and remembers the last variable to enter the basis. The next entering variable is chosen to be the first allowable candidate starting from the last chosen variable and following the given circular order. History-based rules defeat the deformed hypercube constructions because they tend to average out how many times a variable pivots.\n\nIt has recently been shown by [[David Avis]] and [[Oliver Friedmann]] that there is a family of linear programs on which the simplex algorithm equipped with Cunningham's rule requires exponential time.<ref>{{cite journal|first1=David|last1=Avis|first2=Oliver|last2=Friedmann|title=An exponential lower bound for Cunningham's rule|journal=Mathematical Programming|date=2017|url=https://doi.org/10.1007/s10107-016-1008-4}}</ref>\n\n== Notes ==\n{{Reflist}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Exchange algorithms]]\n[[Category:Oriented matroids]]\n[[Category:Linear programming]]"
    },
    {
      "title": "Cutting-plane method",
      "url": "https://en.wikipedia.org/wiki/Cutting-plane_method",
      "text": "{{short description|optimization technique for solving (mixed) integer linear programs}}\n\n[[File:TSP cutting plane.png|thumb|The intersection of the unit cube with the cutting plane <math>x_1 + x_2 + x_3 \\geq 2</math>. In the context of the Traveling salesman problem on three nodes, this (rather weak) inequality states that every tour must have at least two edges.]]\n\nIn [[mathematics|mathematical]] [[Optimization (mathematics)|optimization]], the '''cutting-plane method''' is any of a variety of optimization methods that iteratively refine a [[feasible set]] or objective function by means of linear inequalities, termed ''cuts''.  Such procedures are commonly used to find [[integer]] solutions to [[mixed integer linear programming]] (MILP) problems, as well as to solve general, not necessarily differentiable [[convex optimization]] problems.  The use of cutting planes to solve MILP was introduced by [[Ralph E. Gomory]].\n\nCutting plane methods for MILP work by solving a non-integer linear program, the [[Linear programming relaxation|linear relaxation]] of the given integer program. The theory of Linear Programming dictates that under mild assumptions (if the linear program has an optimal solution, and if the feasible region does not contain a line), one can always find an extreme point or a corner point that is optimal. The obtained [[Optimization (mathematics)|optimum]] is tested for being an integer solution. If it is not, there is guaranteed to exist a linear inequality that ''separates'' the optimum from the [[convex hull]] of the true feasible set. Finding such an inequality is the ''separation problem'', and such an inequality is a ''cut''.  A cut can be added to the relaxed linear program. Then, the current non-integer solution is no longer feasible to the relaxation. This process is repeated until an optimal integer solution is found.\n\nCutting-plane methods for general convex continuous optimization and variants are known under various names: Kelley's method, Kelley–Cheney–Goldstein method, and [[bundle method]]s.  They are popularly used for non-differentiable convex minimization, where a convex objective function and its [[subgradient]] can be evaluated efficiently but usual gradient methods for differentiable optimization can not be used.  This situation is most typical for the concave maximization of [[Lagrange multipliers|Lagrangian dual]] functions.  Another common situation is the application of the [[Dantzig–Wolfe decomposition]] to a structured optimization problem in which formulations with an exponential number of variables are obtained.  Generating these variables on demand by means of [[delayed column generation]] is identical to performing a cutting plane on the respective dual problem.\n\n== Gomory's cut ==\nCutting planes were proposed by [[Ralph E. Gomory|Ralph Gomory]] in the 1950s as a method for solving integer programming and mixed-integer programming problems. However most experts, including Gomory himself, considered them to be impractical due to numerical instability, as well as ineffective because many rounds of cuts were needed to make progress towards the solution.  Things turned around when in the mid-1990s [[Gérard Cornuéjols]] and co-workers showed them to be very effective in combination with [[Branch and bound|branch-and-bound]] (called [[Branch and cut|branch-and-cut]]) and ways to overcome numerical instabilities.  Nowadays, all commercial MILP solvers use Gomory cuts in one way or another. Gomory cuts are very efficiently generated from a simplex tableau, whereas many other types of cuts are either expensive or even NP-hard to separate.  Among other general cuts for MILP, most notably [[lift-and-project]] dominates Gomory cuts.{{Citation needed|date=July 2014}}\n\nLet an integer programming problem be formulated (in [[Integer programming#Canonical and standard form for ILPs|Standard Form]]) as:\n\n: <math>\\begin{align}\n\\text{Maximize  } & c^Tx \\\\\n\\text{Subject to  } & Ax = b, \\\\\n & x\\geq 0,\\, x_i \\text{ all integers}.\n\\end{align}\n</math>\n\nThe method proceeds by first dropping the requirement that the x<sub>i</sub> be integers and solving the associated linear programming problem to obtain a basic feasible solution. Geometrically, this solution will be a vertex of the convex polytope consisting of all feasible points. If this vertex is not an integer point then the method finds a hyperplane with the vertex on one side and all feasible integer points on the other. This is then added as an additional linear constraint to exclude the vertex found, creating a modified linear program. The new program is then solved and the process is repeated until an integer solution is found.\n\nUsing the [[simplex method]] to solve a linear program produces a set of equations of the form\n\n:<math>x_i+\\sum \\bar a_{i,j}x_j=\\bar b_i</math>\n\nwhere ''x<sub>i</sub>'' is a basic variable and the ''x<sub>j</sub>'''s are the nonbasic variables. Rewrite this equation so that the integer parts are on the left side and the fractional parts are on the right side:\n\n:<math>x_i+\\sum \\lfloor \\bar a_{i,j} \\rfloor x_j - \\lfloor \\bar b_i \\rfloor  = \\bar b_i - \\lfloor \\bar b_i \\rfloor - \\sum ( \\bar a_{i,j} -\\lfloor \\bar a_{i,j} \\rfloor) x_j.</math>\n\nFor any integer point in the feasible region the right side of this equation is less than 1 and the left side is an integer, therefore the common value must be less than or equal to 0. So the inequality\n\n:<math>\\bar b_i - \\lfloor \\bar b_i \\rfloor - \\sum ( \\bar a_{i,j} -\\lfloor \\bar a_{i,j} \\rfloor) x_j \\le 0</math>\n\nmust hold for any integer point in the feasible region. Furthermore, nonbasic variables are equal to 0s in any basic solution and if ''x<sub>i</sub>'' is not an integer for the basic solution ''x'',\n\n:<math>\\bar b_i - \\lfloor \\bar b_i \\rfloor - \\sum ( \\bar a_{i,j} -\\lfloor \\bar a_{i,j} \\rfloor) x_j = \\bar b_i - \\lfloor \\bar b_i \\rfloor > 0.</math>\n\nSo the inequality above excludes the basic feasible solution and thus is a cut with the desired properties. Introducing a new slack variable x<sub>k</sub> for this inequality, a new constraint is added to the linear program, namely\n\n:<math>x_k + \\sum (\\lfloor \\bar a_{i,j} \\rfloor - \\bar a_{i,j}) x_j = \\lfloor \\bar b_i \\rfloor - \\bar b_i,\\, x_k \\ge 0,\\, x_k \\mbox{ an integer}.</math>\n\n== Convex optimization ==\n\nCutting plane methods are also applicable in [[nonlinear programming]]. The underlying principle is to approximate the [[feasible region]] of a nonlinear (convex) program by a finite set of closed half spaces and to solve a sequence of approximating [[linear program]]s.\n\n== See also ==\n\n*[[Benders' decomposition]]\n*[[Branch and cut]]\n*[[Branch and bound]]\n*[[Column generation]]\n*[[Dantzig–Wolfe decomposition]]\n\n== References ==\n\n{{cite journal\n| title=Cutting planes in integer and mixed integer programming\n| first1=Hugues\n| last1=Marchand\n| first2=Alexander\n| last2=Martin\n| first3=Robert\n| last3=Weismantel\n| first4=Laurence\n| last4=Wolsey\n| journal=Discrete Applied Mathematics\n| year=2002\n| pages=387–446\n| volume=123\n| issue=1–3\n| url=http://www.sciencedirect.com/science/article/pii/S0166218X01003481/pdfft?md5=00a44d960a8b9554636dfab57f1dac06&pid=1-s2.0-S0166218X01003481-main.pdf\n| doi=10.1016/s0166-218x(01)00348-1\n}}\n\nAvriel, Mordecai (2003). ''Nonlinear Programming: Analysis and Methods.'' Dover Publications. {{ISBN|0-486-43227-0}}\n\n[[Gérard Cornuéjols|Cornuéjols, Gérard]] (2008).  Valid Inequalities for Mixed Integer Linear Programs.  ''Mathematical Programming Ser. B'', (2008) 112:3–44.  [http://integer.tepper.cmu.edu/webpub/integerRioMPSjuly.pdf]\n\n[[Gérard Cornuéjols|Cornuéjols, Gérard]] (2007).  Revival of the Gomory Cuts in the 1990s.  ''Annals of Operations Research'', Vol. 149 (2007), pp.&nbsp;63–66.  [http://integer.tepper.cmu.edu/webpub/gomory.pdf]\n\n==External links==\n*[http://web.mit.edu/15.053/www/AMP-Chapter-09.pdf \"Integer Programming\" Section 9.8] ''Applied Mathematical Programming'' Chapter 9 Integer Programming (full text). Bradley, Hax, and Magnanti (Addison-Wesley, 1977) \n\n{{Optimization algorithms|convex}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "DATADVANCE",
      "url": "https://en.wikipedia.org/wiki/DATADVANCE",
      "text": "{{Notability|date=June 2015}}\n''' DATADVANCE ''' Is a software development company, evolved out of a collaborative research program between [[Airbus]] and Institute for Information Transmission Problems <ref>[http://iitp.ru/en/about Institute for Information Transmission Problems] </ref> of [[Российская_академия_наук| the Russian Academy of Sciences (IITP RAS)]]. \n\n==Product==\n\npSeven Core, embedded in company main product pSeven,<ref>[https://www.datadvance.net/product/pseven/ pSeven]</ref> provides unique proprietary and state-of-the-art algorithms for dimension reduction, [[design of experiments]], [[sensitivity analysis]], [[metamodeling|meta-modeling]], [[uncertainty quantification]] as well as modern single, [[multiobjective_optimization|multi-objective]] and [[robust_optimization|robust]] optimization strategies. \n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Official website|http://www.datadvance.net/}}\n\n[[Category:Computer system optimization software]]\n[[Category:Mathematical optimization software]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Davidon–Fletcher–Powell formula",
      "url": "https://en.wikipedia.org/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula",
      "text": "The '''Davidon–Fletcher–Powell formula''' (or '''DFP'''; named after [[William C. Davidon]], [[Roger Fletcher (mathematician)|Roger Fletcher]], and [[Michael J. D. Powell]]) finds the solution to the secant equation that is closest to the current estimate and satisfies the curvature condition. It was the first [[quasi-Newton method]] to generalize the [[secant method]] to a multidimensional problem. This update maintains the symmetry and positive definiteness of the [[Hessian matrix]].\n\nGiven a function <math>f(x)</math>, its [[gradient]] (<math>\\nabla f</math>), and [[positive-definite matrix|positive-definite]] [[Hessian matrix]] <math>B</math>, the [[Taylor series]] is\n\n:<math>f(x_k+s_k) = f(x_k) + \\nabla f(x_k)^T s_k + \\frac{1}{2} s^T_k {B} s_k + \\dots,</math>\n\nand the [[Taylor series]] of the gradient itself (secant equation)\n\n:<math>\\nabla f(x_k+s_k) = \\nabla f(x_k) + B s_k + \\dots</math>\n\nis used to update <math>B</math>.  \n\nThe DFP formula finds a solution that is symmetric, positive-definite and closest to the current approximate value of <math>B_k</math>:\n\n:<math>B_{k+1}=\n(I - \\gamma_k y_k s_k^T) B_k (I - \\gamma_k s_k y_k^T) + \\gamma_k y_k y_k^T,</math>\n\nwhere\n\n:<math>y_k = \\nabla f(x_k+s_k) - \\nabla f(x_k),</math>\n:<math>\\gamma_k = \\frac{1}{y_k^T s_k},</math>\n\nand <math>B_k</math> is a symmetric and [[positive-definite matrix]].\n\nThe corresponding update to the inverse Hessian approximation <math>H_k = B_k^{-1}</math> is given by\n\n:<math>H_{k+1} = H_k - \\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k} + \\frac{s_k s_k^T}{y_k^{T} s_k}.</math>\n\n<math>B</math> is assumed to be positive-definite, and the vectors <math>s_k^T</math> and <math>y</math> must satisfy the curvature condition\n\n: <math>s_k^T y_k = s_k^T B s_k > 0.</math>\n\nThe DFP formula is quite effective, but it was soon superseded by the [[BFGS method|BFGS formula]], which is its [[Duality (mathematics)|dual]] (interchanging the roles of ''y'' and ''s'').\n\n==See also==\n* [[Newton's method]]\n* [[Newton's method in optimization]]\n* [[Quasi-Newton method]]\n* [[BFGS method|Broyden–Fletcher–Goldfarb–Shanno (BFGS) method]]\n* [[L-BFGS|L-BFGS method]]\n* [[SR1 formula]]\n* [[Nelder–Mead method]]\n\n==References==\n* {{Cite journal |doi=10.1137/0801001 |first1=W. C.|last1= Davidon|title=Variable metric method for minimization|journal= SIAM Journal on Optimization |volume=1|pages=1–17 |year=1991|citeseerx=10.1.1.693.272}}\n* {{Cite book |last1=Fletcher | first1=Roger | title=Practical methods of optimization | publisher=John Wiley & Sons | location=New York | edition=2nd | isbn=978-0-471-91547-8 | year=1987}}\n* {{cite book |last1=Kowalik |first=J. |last2=Osborne |first2=M. R. |title=Methods for Unconstrained Optimization Problems |location=New York |publisher=Elsevier |year=1968 |isbn=0-444-00041-0 |pages=45–48 }} \n* {{Cite book |last1=Nocedal |first1=Jorge |last2=Wright |first2=Stephen J. |year=1999|title=Numerical Optimization|publisher= Springer-Verlag| isbn= 0-387-98793-2}}\n\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Davidon-Fletcher-Powell formula}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Derivative-free optimization",
      "url": "https://en.wikipedia.org/wiki/Derivative-free_optimization",
      "text": "'''Derivative-free optimization''' is a discipline in [[mathematical optimization]] that does not use [[derivative]] information in the classical sense to find optimal solutions: Sometimes information about the derivative of the objective function ''f'' is unavailable, unreliable or impractical to obtain. For example, ''f'' might be non-smooth, or time-consuming to evaluate, or in some way noisy, so that methods that rely on derivatives or approximate them via [[finite difference]]s are of little use. The problem to find optimal points in such situations is referred to as derivative-free optimization, algorithms that do not use derivatives or finite differences are called '''derivative-free algorithms'''.<ref name=\"CSV\" >{{cite book|last=Conn |first=A. R. |last2=Scheinberg |first2=K.|author2-link= Katya Scheinberg |last3=Vicente |first3=L. N. |year=2009 |title=Introduction to Derivative-Free Optimization  |url=http://www.mat.uc.pt/~lnv/idfo/|accessdate=2014-01-18 |series= MPS-SIAM Book Series on Optimization| publisher=SIAM|location=Philadelphia}}</ref>\n\n==Introduction==\nThe problem to be solved is to numerically optimize an objective function <math>f\\colon A\\to\\mathbb{R}</math> for some [[Set (mathematics)|set]] <math>A</math> (usually <math>A\\subset\\mathbb{R}^n</math>), i.e. find <math>x_0\\in A</math> such that without loss of generality <math>f(x_0)\\leq f(x)</math> for all <math>x\\in A</math>.\n\nWhen applicable, a common approach is to iteratively improve a parameter guess by local hill-climbing in the objective function landscape. Derivative-based algorithms use derivative information of <math>f</math> to find a good search direction, since for example the gradient gives the direction of steepest ascent. Derivative-based optimization is efficient at finding local optima for continuous-domain smooth single-modal problems. However, they can have problems when e.g. <math>A</math> is disconnected, or (mixed-)integer, or when <math>f</math> is expensive to evaluate, or is non-smooth, or noisy, so that (numeric approximations of) derivatives do not provide useful information. A slightly different problem is when <math>f</math> is multi-modal, in which case local derivative-based methods only give local optima, but might miss the global one.\n\nIn derivative-free optimization, various methods are employed to address these challenges using only function values of <math>f</math>, but no derivatives. Some of these methods can be proved to discover optima, but some are rather metaheuristic since the problems are in general more difficult to solve compared to [[convex optimization]]. For these, the ambition is rather to efficiently find \"good\" parameter values which can be near-optimal given enough resources, but optimality guarantees can typically not be given. One should keep in mind that the challenges are diverse, so that one can usually not use one algorithm for all kinds of problems.\n\n==Algorithms==\nNotable derivative-free optimization algorithms include:\n* [[Bayesian optimization]]\n* [[Coordinate descent]] and [[adaptive coordinate descent]]\n* [[Cuckoo search]]\n* [[DONE]]\n* [[Evolution strategies]], [[Natural evolution strategies]] ([[CMA-ES]], xNES, SNES)\n* [[Genetic algorithm]]s\n* [[MCS algorithm]]\n* [[Nelder-Mead method]]\n* [[Particle swarm optimization]]\n* [[Pattern search (optimization)|Pattern search]]\n* Powell's [[COBYLA]], [[UOBYQA]], [[NEWUOA]], [[BOBYQA]] and [[LINCOA]] algorithms\n* [[Random search]] (including [[Luus-Jaakola]])\n* [[Simulated annealing]]\n* [[Subgradient method]]\n\n==See also==\n* [[Mathematical optimization]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*{{cite journal |last=Audet |first=Charles |last2=Kokkolaras |first2=Michael |year=2016 |title=Blackbox and derivative-free optimization: theory, algorithms and applications |journal=Optimization and Engineering |volume=17 |pages=1–2 |doi=10.1007/s11081-016-9307-4 }}\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Destination dispatch",
      "url": "https://en.wikipedia.org/wiki/Destination_dispatch",
      "text": "{{refimprove|date=July 2012}}\n[[File:DestinationDispatchElevator2.jpg|thumb|A destination dispatch elevator.]]\n'''Destination dispatch''' is an optimization technique used for multi-[[elevator]] installations, which groups passengers for the same destinations into the same elevators, thereby reducing waiting and travel times when compared to a traditional approach where all passengers wishing to ascend or descend enter any available lift and then request their destination.\n\nUsing destination dispatch, passengers request travel to a particular floor using a [[keypad]], [[touch screen]], or [[proximity card]] room-key prior in the lobby and are immediately directed to an appropriate elevator car.\n\n==Algorithms==\n[[File:Destination Dispatch.jpg|thumb|Destination dispatch controls]]\nBased on information about the trips that passengers wish to make, the controller will dynamically allocate individuals to elevators to avoid excessive intermediate stops. Overall trip-times can be reduced by 25% with capacity up by 30%.<ref>{{cite web|url=http://www.thyssenkruppelevator.com/destdist.asp|title=Destination Dispatch}}</ref> \n\nControllers can also offer different levels of service to passengers based on information contained in their key-cards. A high-privilege user may be allocated the nearest available elevator and always be guaranteed a direct service to their floor, and may be allocated an elevator with exclusive use; other users may be provided with extended door-opening times.<ref>{{cite web|url=http://www.schindlerportna.com/keyfeatures/personalization/|title=Personalization}}</ref>\n\n==Limitations==\nThe smooth operation of a destination dispatch system depends upon each passenger indicating their destination intention separately. In most cases, the elevator system has no way of differentiating a group of passengers from a single passenger if the group's destination is only keyed in a single time. This could potentially lead to an elevator stopping to pick up more passengers than the elevator actually has capacity for, creating delays for other users. This situation is handled by two solutions, a load vane sensor on the elevator or a group function button on keypad. The load vane tells the elevator controller that there is a high load in car and doesn't stop at other floors until the load is low enough to pick up more passengers. The group function button asks for how many passengers are going to a floor, and then the system sends the correct number of elevators to that floor if available.{{Citation needed|date=November 2016}}\n\n==References==\n<references/>\n\n==External links==\n{{commons category|Destination control elevators}}\n*[http://www.peters-research.com/index.php?option=com_content&view=article&id=43%3Aunderstanding-the-benefits-and-limitations-of-destination-dispatch&catid=3%3Apapers&Itemid=1 Understanding the Benefits and Limitations of Destination Dispatch]\n\n{{DEFAULTSORT:Destination Dispatch}}\n[[Category:Elevators]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Divide-and-conquer algorithm",
      "url": "https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm",
      "text": "In [[computer science]], '''divide and conquer''' is an [[algorithm design paradigm]] based on multi-branched [[recursion]]. A divide-and-conquer [[algorithm]] works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.\n\nThis divide-and-conquer technique is the basis of efficient algorithms for all kinds of problems, such as [[sorting algorithm|sorting]] (e.g., [[quicksort]], [[merge sort]]), [[multiplication algorithm|multiplying large numbers]] (e.g. the [[Karatsuba algorithm]]), finding the [[Closest pair of points problem|closest pair of points]], [[syntactic analysis]] (e.g., [[top-down parser]]s), and computing the [[discrete Fourier transform]] ([[fast Fourier transform|FFT]]).{{Citation needed|date=October 2017}}\n\nUnderstanding and designing divide-and-conquer algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a [[theorem]] by [[Mathematical induction|induction]], it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization.{{Clarify|date=October 2017}} These divide-and-conquer complications are seen when optimizing the calculation of a [[Fibonacci number#Matrix form|Fibonacci number with efficient double recursion]].{{Why|date=October 2017}}{{Citation needed|date=October 2017}}\n\nThe correctness of a divide-and-conquer algorithm is usually proved by [[mathematical induction]], and its computational cost is often determined by solving [[recurrence relation]]s.\n\n== Divide and conquer ==\n[[File:Merge sort algorithm diagram.svg|thumb|Divide-and-conquer approach to sort the list (38, 27, 43, 3, 9, 82, 10) in increasing order. ''Upper half:'' splitting into sublists; ''mid:'' a one-element list is trivially sorted; ''lower half:'' composing sorted sublists.]] \nThe divide-and-conquer paradigm is often used to find an optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. Problems of sufficient simplicity are solved directly. \nFor example, to sort a given list of ''n'' natural numbers, split it into two lists of about ''n''/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (see the picture). This approach is known as the [[merge sort]] algorithm.\n\nThe name \"divide and conquer\" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the [[binary search]] algorithm for finding a record in a sorted list (or its analog in [[numerical algorithm|numerical computing]], the [[bisection algorithm]] for [[root-finding algorithm|root finding]]).<ref name=\"CormenLeiserson2009\">{{cite book|author1=Thomas H. Cormen|author2=Charles E. Leiserson|author3=Ronald L. Rivest|coauthors=Clifford Stein|title=Introduction to Algorithms|url=https://books.google.com/books?id=aefUBQAAQBAJ&printsec=frontcover#v=onepage&q=%22Divide-and-conquer%22&f=false|date=31 July 2009|publisher=MIT Press|isbn=978-0-262-53305-8}}</ref>  These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use [[tail recursion]], they can be converted into simple [[loop (computing)|loops]].  Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a \"divide-and-conquer algorithm\".  Therefore, some authors consider that the name \"divide and conquer\" should be used only when each problem may generate two or more subproblems.<ref>Brassard, G. and Bratley, P. Fundamental of Algorithmics, Prentice-Hall, 1996.</ref> The name '''decrease and conquer''' has been proposed instead for the single-subproblem class.<ref>Anany V. Levitin, ''Introduction to the Design and Analysis of Algorithms'' (Addison Wesley, 2002).</ref>\n\nAn important application of divide and conquer is in optimization,{{Examples|date=October 2017}} where if the search space is reduced (\"pruned\") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the [[geometric series]]); this is known as [[prune and search]].\n\n== Early historical examples ==\nEarly examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into ''single'' subproblems, and indeed can be solved iteratively.\n\nBinary search, a decrease-and-conquer algorithm where the subproblems are of roughly half the original size, has a long history. While a clear description of the algorithm on computers appeared in 1946 in an article by [[John Mauchly]], the idea of using a sorted list of items to facilitate searching dates back at least as far as [[Babylonia]] in 200&nbsp;BC.<ref name=Knuth3/> Another ancient decrease-and-conquer algorithm is the [[Euclidean algorithm]] to compute the [[greatest common divisor]] of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.\n\nAn early example of a divide-and-conquer algorithm with multiple subproblems is [[Carl Friedrich Gauss|Gauss]]'s 1805 description of what is now called the [[Cooley–Tukey FFT algorithm|Cooley–Tukey fast Fourier transform]] (FFT) algorithm,<ref name=Heideman84>Heideman, M. T., D. H. Johnson, and C. S. Burrus, \"[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.4791&rep=rep1&type=pdf Gauss and the history of the fast Fourier transform]\", IEEE ASSP Magazine, 1, (4), 14–21 (1984).</ref> although he did not [[analysis of algorithms|analyze its operation count]] quantitatively, and FFTs did not become widespread until they were rediscovered over a century later.\n\nAn early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the [[merge sort]] algorithm, invented by [[John von Neumann]] in 1945.<ref>{{ cite book | last=Knuth | first=Donald | authorlink=Donald Knuth | year=1998 | title=[[The Art of Computer Programming]]: Volume 3 Sorting and Searching | page=159 | isbn=0-201-89685-0 }}</ref>\n\nAnother notable example is the [[Karatsuba algorithm|algorithm]] invented by [[Anatolii Alexeevitch Karatsuba|Anatolii A. Karatsuba]] in 1960<ref>{{cite journal| last=Karatsuba | first=Anatolii A. | authorlink=Anatolii Alexeevitch Karatsuba |author2=Yuri P. Ofman |authorlink2=Yuri Petrovich Ofman | year=1962 | title=Умножение многозначных чисел на автоматах | journal=[[Doklady Akademii Nauk SSSR]] | volume=146 | pages=293–294}} Translated in {{cite journal| title=Multiplication of Multidigit Numbers on Automata | journal=Soviet Physics Doklady | volume=7 | year=1963 | pages=595–596 |url={{Google books|MrkOAAAAIAAJ|plainurl=true}} }}</ref> that could multiply two ''n''-digit numbers in <math>O(n^{\\log_2 3})</math> operations (in [[Big O notation]]). This algorithm disproved [[Andrey Kolmogorov]]'s 1956 conjecture that <math>\\Omega(n^2)</math> operations would be required for that task.\n\nAs another example of a divide-and-conquer algorithm that did not originally involve computers, [[Donald Knuth]] gives the method a [[post office]] typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered.<ref name=Knuth3>Donald E. Knuth, ''The Art of Computer Programming: Volume 3, Sorting and Searching'', second edition (Addison-Wesley, 1998).</ref> This is related to a [[radix sort]], described for [[IBM 80 series Card Sorters|punch-card sorting]] machines as early as 1929.<ref name=Knuth3/>\n\n== Advantages ==\n\n=== Solving difficult problems ===\nDivide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem. Similarly, decrease and conquer only requires reducing the problem to a single smaller problem, such as the classic [[Tower of Hanoi]] puzzle, which reduces moving a tower of height ''n'' to moving a tower of height ''n''&nbsp;−&nbsp;1.\n\n=== Algorithm efficiency ===\nThe divide-and-conquer paradigm often helps in the discovery of efficient algorithms.  It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the [[Strassen algorithm]] for matrix multiplication, and fast Fourier transforms.\n\nIn all these examples, the D&C approach led to an improvement in the [[asymptotic complexity|asymptotic cost]] of the solution.\nFor example, if (a) the [[Recursion (computer science)|base cases]] have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size ''n'', and (b) there is a bounded number ''p'' of subproblems of size ~ ''n''/''p'' at each stage, then the cost of the divide-and-conquer algorithm will be O(''n'' log<sub>''p''</sub>''n'').\n\n=== Parallelism ===\nDivide-and-conquer algorithms are naturally adapted for execution in multi-processor machines, especially [[shared-memory]] systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.\n\n=== Memory access ===\nDivide-and-conquer algorithms naturally tend to make efficient use of [[memory cache]]s. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called ''[[cache-oblivious algorithm|cache-oblivious]]'', because it does not contain the cache size as an explicit parameter.<ref name=\"cahob\">{{cite journal | author = M. Frigo |author2=C. E. Leiserson |author3=H. Prokop | title = Cache-oblivious algorithms | journal = Proc. 40th Symp. on the Foundations of Computer Science | year = 1999|url=https://dspace.mit.edu/bitstream/handle/1721.1/80568/43558192-MIT.pdf;sequence=2}}</ref>\nMoreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be ''optimal'' cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is ''blocking'', as in [[loop nest optimization]], where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine.\n\nThe same advantage exists with regards to other hierarchical storage systems, such as [[Non-uniform memory access|NUMA]] or [[virtual memory]], as well as for multiple levels of cache: once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.\n\n=== Roundoff control ===\nIn computations with rounded arithmetic, e.g. with [[floating-point]] numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add ''N'' numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called [[pairwise summation]] that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums.  While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.<ref>Nicholas J. Higham, \"[https://pdfs.semanticscholar.org/5c17/9d447a27c40a54b2bf8b1b2d6819e63c1a69.pdf The accuracy of floating point summation]\", ''SIAM J. Scientific Computing'' '''14''' (4), 783–799 (1993).</ref>\n\n== Implementation issues ==\n\n=== Recursion ===\nDivide-and-conquer algorithms are naturally implemented as [[Recursion (computer science)|recursive procedures]]. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the [[call stack|procedure call stack]]. A recursive function is a function that calls itself within its definition.\n\n=== Explicit stack ===\nDivide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a [[stack (data structure)|stack]], [[queue (data structure)|queue]], or [[priority queue]].  This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in [[breadth first recursion|breadth-first recursion]] and the [[branch-and-bound]] method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.\n\n=== Stack size ===\nIn recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of [[stack overflow]].  D&C algorithms that are time-efficient often have relatively small recursion depth.  For example, the quicksort algorithm can be implemented so that it never requires more than <math>\\log_2 n</math> nested recursive calls to  sort <math>n</math> items.\n\nStack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it.  Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure.  Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure or by using an explicit stack structure.\n\n=== Choosing the base cases ===\nIn any recursive algorithm, there is considerable freedom in the choice of the ''base cases'', the small subproblems that are solved directly in order to terminate the recursion.\n\nChoosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve.  For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.\n\nOn the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a [[hybrid algorithm]]. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is ''short-circuiting the base case'', also known as ''[[arm's-length recursion]]''. In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.\n\nThus, for example, many library implementations of quicksort will switch to a simple loop-based [[insertion sort]] (or similar) algorithm once the number of items to be sorted is sufficiently small.  Note that, if the empty list were the only base case, sorting a list with ''n'' entries would entail maximally ''n'' quicksort calls that would do nothing but return immediately.  Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.\n\nAlternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely [[loop unwinding|unrolled]] into code that has no recursion, loops, or [[Conditional (programming)|conditionals]] (related to the technique of [[partial evaluation]]).  For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes.<ref name=\"fftw\">{{cite journal | author = Frigo, M. |author2=Johnson, S. G. | url = http://www.fftw.org/fftw-paper-ieee.pdf | title = The design and implementation of FFTW3 | journal = Proceedings of the IEEE | volume = 93 | issue = 2 |date=February 2005 | pages = 216–231 | doi = 10.1109/JPROC.2004.840301}}</ref>  [[Source-code generation]] methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.<ref name=\"fftw\"/>\n\nThe generalized version of this idea is known as recursion \"unrolling\" or \"coarsening\", and various techniques have been proposed for automating the procedure of enlarging the base case.<ref>Radu Rugina and Martin Rinard, \"[http://people.csail.mit.edu/rinard/paper/lcpc00.pdf Recursion unrolling for divide and conquer programs]\" in ''Languages and Compilers for Parallel Computing'', chapter 3, pp. 34–48.  ''Lecture Notes in Computer Science'' vol. 2017 (Berlin: Springer, 2001).</ref>\n\n=== Sharing repeated subproblems ===\nFor some problems, the branched recursion may end up evaluating the same sub-problem many times over.  In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as [[memoization]].  Followed to the limit, it leads to [[bottom-up design|bottom-up]] divide-and-conquer algorithms such as [[dynamic programming]] and [[chart parsing]].\n\n== See also ==\n{{Portal|Computer Science}}\n{{commons category|Divide-and-conquer algorithms}}\n* [[Akra–Bazzi method]]\n* [[Decomposable aggregation function]]\n* [[Fork–join model]]\n* [[Master theorem (analysis of algorithms)]]\n* [[Mathematical induction]]\n* [[MapReduce]]\n* [[Heuristic (computer science)]]\n\n== References ==\n<references/>\n\n{{DEFAULTSORT:Divide And Conquer Algorithm}}\n[[Category:Algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Dykstra's projection algorithm",
      "url": "https://en.wikipedia.org/wiki/Dykstra%27s_projection_algorithm",
      "text": "{{distinguish|Dijkstra's algorithm}}\n\n'''Dykstra's algorithm''' is a method that computes a point in the intersection of [[convex set]]s, and is a variant of the [[alternating projection]] method (also called the [[projections onto convex sets]] method).  In its simplest form, the method finds a point in the intersection of two convex sets by iteratively projecting onto each of the convex set; it differs from the alternating projection method in that there are intermediate steps. A parallel version of the algorithm was developed by Gaffke and Mathar.\n\nThe method is named after R. L. Dykstra who proposed it in the 1980s.\n\nA key difference between Dykstra's algorithm and the standard alternating projection method occurs when there is more than one point in the intersection of the two sets. In this case, the alternating projection method gives some arbitrary point in this intersection, whereas Dykstra's algorithm gives a specific point: the projection of ''r'' onto the intersection, where ''r'' is the initial point used in the algorithm,\n\n== Algorithm ==\n[[File:Dykstra algorithm.svg|400px|right|thumb]]\n\nDykstra's algorithm finds for each <math>r</math> the only <math> \\bar{x} \\in C\\cap D</math> such that:\n: <math> \\|\\bar{x}-r\\|^2\\le \\|x-r\\|^2, \\text{for all } x\\in C \\cap D, </math>\n\nwhere <math>C,D</math> are [[convex set]]s.  This problem is equivalent to finding the [[Projection (mathematics)|projection]] of <math>r</math> onto the set <math>C\\cap D</math>, which we denote by <math>\\mathcal{P}_{C \\cap D}</math>.\n\nTo use Dykstra's algorithm, one must know how to project onto the sets <math>C</math> and <math>D</math> separately.\n\nFirst, consider the basic [[alternating projection]] (aka POCS) method (first studied, in the case when the sets <math>C,D</math> were linear subspaces, by [[John von Neumann]]<ref>J. von Neumann, On rings of operators. Reduction theory, Ann. of Math. 50 (1949) 401–485 (a reprint of lecture notes first distributed in 1933).</ref>), which initializes <math>x_0=r</math> and then generates the sequence\n\n: <math>x_{k+1} = \\mathcal{P}_C \\left( \\mathcal{P}_D ( x_k ) \\right) </math>.\n\nDykstra's algorithm is of a similar form, but uses additional auxiliary variables. Start with <math>x_0 =r, p_0=q_0=0</math> and update by\n\n: <math> y_k = \\mathcal{P}_D( x_k + p_k ) </math>\n\n: <math> p_{k+1} = x_k + p_k - y_k </math>\n\n: <math> x_{k+1} = \\mathcal{P}_C( y_k + q_k ) </math>\n\n: <math> q_{k+1} =y_k + q_k - x_{k+1}. </math>\n\nThen the sequence <math>(x_k)</math> converges to the solution of the original problem. For convergence results and a modern perspective on the literature, see <ref>P. L. Combettes and J.-C. Pesquet, \"Proximal splitting methods in signal processing,\" in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering, (H. H. Bauschke, [[Regina S. Burachik|R. S. Burachik]], P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, Editors), pp. 185–212. Springer, New York, 2011 [https://link.springer.com/chapter/10.1007/978-1-4419-9569-8_10]</ref>\n\n== References ==\n\n* {{cite book | first1 = J. P. | last1 = Boyle | first2 = R. L. | last2 = Dykstra | title = A method for finding projections onto the intersection of convex sets in Hilbert spaces | journal = Lecture Notes in Statistics | volume = 37 | year = 1986 | pages = 28–47 | doi=10.1007/978-1-4613-9940-7_3| isbn = 978-0-387-96419-5 }}\n* {{cite journal | first1 = N. | last1 = Gaffke | first2 = R. | last2 = Mathar | title = A cyclic projection algorithm via duality | journal = Metrika | volume = 36 | year = 1989 | pages = 29–54 | doi=10.1007/bf02614077}}\n<references>\n</references>\n\n[[Category:Convex geometry]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Dynamic programming",
      "url": "https://en.wikipedia.org/wiki/Dynamic_programming",
      "text": "{{Distinguish|dynamic programming language|dynamic problem}}\n[[Image:Shortest path optimal substructure.svg|200px|right|thumb|'''Figure 1.''' Finding the shortest path in a graph using optimal substructure; a straight line indicates a single edge; a wavy line indicates a shortest path between the two vertices it connects (among other paths, not shown, sharing the same two vertices); the bold line is the overall shortest path from start to goal.]]\n'''Dynamic programming''' is both a [[mathematical optimization]] method and a computer programming method. The method was developed by [[Richard Bellman]] in the 1950s and has found applications in numerous fields, from [[aerospace engineering]] to [[economics]]. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a [[Recursion|recursive]] manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have [[optimal substructure]].\n\nIf sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.<ref name=\":0\">Cormen, T. H.; Leiserson, C. E.; Rivest, R. L.; Stein, C. (2001), Introduction to Algorithms (2nd ed.), MIT Press & McGraw–Hill, {{ISBN|0-262-03293-7}} . pp. 344.</ref> In the optimization literature this relationship is called the [[Bellman equation]].\n\n== Overview ==\n\n=== Mathematical optimization ===\nIn terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of '''value functions''' ''V''<sub>1</sub>, ''V''<sub>2</sub>, ..., ''V''<sub>''n''</sub> taking ''y'' as an argument representing the '''[[State variable|state]]''' of the system at times ''i'' from 1 to ''n''. The definition of ''V''<sub>''n''</sub>(''y'') is the value obtained in state ''y'' at the last time ''n''. The values ''V''<sub>''i''</sub> at earlier times ''i''&nbsp;=&nbsp;''n''&nbsp;&minus;1,&nbsp;''n''&nbsp;&minus;&nbsp;2,&nbsp;...,&nbsp;2,&nbsp;1 can be found by working backwards, using a [[Recursion|recursive]] relationship called the [[Bellman equation]]. For ''i''&nbsp;=&nbsp;2,&nbsp;...,&nbsp;''n'', ''V''<sub>''i''&minus;1</sub> at any state ''y'' is calculated from ''V''<sub>''i''</sub> by maximizing a simple function (usually the sum) of the gain from a decision at time ''i''&nbsp;&minus;&nbsp;1 and the function ''V''<sub>''i''</sub> at the new state of the system if this decision is made. Since ''V''<sub>''i''</sub> has already been calculated for the needed states, the above operation yields ''V''<sub>''i''&minus;1</sub> for those states. Finally, ''V''<sub>1</sub> at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed.\n\n=== Control theory ===\nIn [[control theory]], a typical problem is to find an admissible control <math>\\mathbf{u}^{\\ast}</math> which causes the system <math>\\dot{\\mathbf{x}}(t) = \\mathbf{g} \\left( \\mathbf{x}(t), \\mathbf{u}(t), t \\right)</math> to follow an admissible trajectory <math>\\mathbf{x}^{\\ast}</math> on a continuous time interval <math>t_{0} \\leq t \\leq t_{1}</math> that minimizes a [[Loss function|cost function]]\n:<math>J = b \\left( \\mathbf{x}(t_{1}), t_{1} \\right) + \\int_{t_{0}}^{t_{1}} f \\left( \\mathbf{x}(t), \\mathbf{u}(t), t \\right) \\mathrm{d} t</math> \nThe solution to this problem is an optimal control law or policy <math>\\mathbf{u}^{\\ast} = h(\\mathbf{x}(t), t)</math>, which produces an optimal trajectory <math>\\mathbf{x}^{\\ast}</math> and an optimized loss function <math>J^{\\ast}</math>. The latter obeys the fundamental equation of dynamic programming:\n:<math>- J_{t}^{\\ast} = \\min_{\\mathbf{u}} \\left\\{ f \\left( \\mathbf{x}(t), \\mathbf{u}(t), t \\right) + J_{x}^{\\ast \\mathsf{T}} \\mathbf{g} \\left( \\mathbf{x}(t), \\mathbf{u}(t), t \\right) \\right\\}</math>\na [[partial differential equation]] known as the [[Hamilton–Jacobi–Bellman equation]], in which <math>J_{x}^{\\ast} = \\frac{\\partial J^{\\ast}}{\\partial \\mathbf{x}} = \\left[ \\frac{\\partial J^{\\ast}}{\\partial x_{1}} ~~~~ \\frac{\\partial J^{\\ast}}{\\partial x_{2}} ~~~~ \\dots ~~~~  \\frac{\\partial J^{\\ast}}{\\partial x_{n}} \\right]^{\\mathsf{T}}</math> and <math>J_{t}^{\\ast} = \\frac{\\partial J^{\\ast}}{\\partial t}</math>. One finds the minimizing <math>\\mathbf{u}</math> in terms of <math>t</math>, <math>\\mathbf{x}</math>, and the unknown function <math>J_{x}^{\\ast}</math> and then substitutes the result into the Hamilton–Jacobi–Bellman equation to get the partial differential equation to be solved with boundary condition <math>J \\left( t_{1} \\right) = b \\left( \\mathbf{x}(t_{1}), t_{1} \\right)</math>.<ref>{{cite book |first=M. I. |last=Kamien |authorlink=Morton Kamien |first2=N. L. |last2=Schwartz |authorlink2=Nancy Schwartz |title=Dynamic Optimization: The Calculus of Variations and Optimal Control in Economics and Management |location=New York |publisher=Elsevier |edition=Second |year=1991 |isbn=978-0-444-01609-6 |url=https://books.google.com/books?id=0IoGUn8wjDQC&pg=PA261 |page=261 }}</ref> In practice, this generally requires [[Numerical partial differential equations|numerical techniques]] for some discrete approximation to the exact optimization relationship.\n\nAlternatively, the continuous process can be approximated by a discrete system, which leads to a following recurrence relation analog to the Hamilton–Jacobi–Bellman equation:\n:<math>J_{k}^{\\ast} \\left( \\mathbf{x}_{n-k} \\right) = \\min_{\\mathbf{u}_{n-k}} \\left\\{ \\hat{f} \\left( \\mathbf{x}_{n-k}, \\mathbf{u}_{n-k} \\right) + J_{k-1}^{\\ast} \\left( \\hat{g} \\left( \\mathbf{x}_{n-k}, \\mathbf{u}_{n-k} \\right) \\right) \\right\\}</math>\nat the <math>k</math>-th stage of <math>n</math> equally spaced discrete time intervals, and where <math>\\hat{f}</math> and <math>\\hat{g}</math> denote discrete approximations to <math>f</math> and <math>\\mathbf{g}</math>. This functional equation is known as the [[Bellman equation]], which can be solved for an exact solution of the discrete approximation of the optimization equation.<ref>{{cite book |first=Donald E. |last=Kirk |title=Optimal Control Theory: An Introduction |location=Englewood Cliffs, NJ |publisher=Prentice-Hall |year=1970 |isbn=978-0-13-638098-6 |pages=94–95 |url=https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA94 }}</ref>\n\n==== Example from economics: Ramsey's problem of optimal saving ====\n{{See also|Ramsey–Cass–Koopmans model}}\nIn economics, the objective is generally to maximize (rather than minimize) some dynamic [[social welfare function]]. In Ramsey's problem, this function relates amounts of consumption to levels of [[utility]]. Loosely speaking, the planner faces the trade-off between contemporaneous consumption and future consumption (via investment in [[Physical capital|capital stock]] that is used in production), known as [[intertemporal choice]]. Future consumption is discounted at a constant rate <math>\\beta \\in (0,1)</math>. A discrete approximation to the transition equation of capital is given by\n:<math>k_{t+1} = \\hat{g} \\left( k_{t}, c_{t} \\right) = f(k_{t}) - c_{t}</math>\nwhere <math>c</math> is consumption, <math>k</math> is capital, and <math>f</math> is a [[production function]] satisfying the [[Inada conditions]]. An initial capital stock <math>k_{0} > 0</math> is assumed.\n\nLet <math>c_t</math> be consumption in period {{mvar|t}}, and assume consumption yields [[utility]] <math>u(c_t)=\\ln(c_t)</math> as long as the consumer lives. Assume the consumer is impatient, so that he [[discounting|discounts]] future utility by a factor {{mvar|b}} each period, where <math>0<b<1</math>. Let <math>k_t</math> be [[capital (economics)|capital]] in period {{mvar|t}}. Assume initial capital is a given amount <math>k_0>0</math>, and suppose that this period's capital and consumption determine next period's capital as <math>k_{t+1}=Ak^a_t - c_t</math>, where {{mvar|A}} is a positive constant and <math>0<a<1</math>. Assume capital cannot be negative. Then the consumer's decision problem can be written as follows:\n\n: <math>\\max \\sum_{t=0}^T b^t \\ln(c_t)</math> subject to <math>k_{t+1}=Ak^a_t - c_t \\geq 0</math> for all <math>t=0,1,2,\\ldots,T</math>\n\nWritten this way, the problem looks complicated, because it involves solving for all the choice variables <math>c_0, c_1, c_2, \\ldots , c_T</math>. (Note that <math>k_0</math> is not a choice variable—the consumer's initial capital is taken as given.)\n\nThe dynamic programming approach to solve this problem involves breaking it apart into a sequence of smaller decisions. To do so, we define a sequence of ''value functions'' <math>V_t(k)</math>, for <math>t=0,1,2,\\ldots,T,T+1</math> which represent the value of having any amount of capital {{mvar|k}} at each time {{mvar|t}}. Note that <math>V_{T+1}(k)=0</math>, that is, there is (by assumption) no utility from having capital after death.\n\nThe value of any quantity of capital at any previous time can be calculated by [[backward induction]] using the [[Bellman equation]]. In this problem, for each <math>t=0,1,2,\\ldots,T</math>, the Bellman equation is\n\n: <math>V_t(k_t) \\, = \\, \\max \\left( \\ln(c_t) + b V_{t+1}(k_{t+1}) \\right)</math> subject to <math>k_{t+1}=Ak^a_t - c_t \\geq 0</math>\n\nThis problem is much simpler than the one we wrote down before, because it involves only two decision variables, <math>c_t</math> and <math>k_{t+1}</math>. Intuitively, instead of choosing his whole lifetime plan at birth, the consumer can take things one step at a time. At time {{mvar|t}}, his current capital <math>k_t</math> is given, and he only needs to choose current consumption <math>c_t</math> and saving <math>k_{t+1}</math>.\n\nTo actually solve this problem, we work backwards. For simplicity, the current level of capital is denoted as {{mvar|k}}. <math>V_{T+1}(k)</math> is already known, so using the Bellman equation once we can calculate <math>V_T(k)</math>, and so on until we get to <math>V_0(k)</math>, which is the ''value'' of the initial decision problem for the whole lifetime. In other words, once we know <math>V_{T-j+1}(k)</math>, we can calculate <math>V_{T-j}(k)</math>, which is the maximum of <math>\\ln(c_{T-j}) + b V_{T-j+1}(Ak^a-c_{T-j})</math>, where <math>c_{T-j}</math> is the choice variable and <math>Ak^a-c_{T-j} \\ge 0</math>.\n\nWorking backwards, it can be shown that the value function at time <math>t=T-j</math> is\n\n: <math>V_{T-j}(k) \\, = \\, a \\sum_{i=0}^j a^ib^i \\ln k + v_{T-j}</math>\n\nwhere each <math>v_{T-j}</math> is a constant, and the optimal amount to consume at time <math>t=T-j</math> is\n\n: <math>c_{T-j}(k) \\, = \\, \\frac{1}{\\sum_{i=0}^j a^ib^i} Ak^a</math>\n\nwhich can be simplified to\n\n: <math>\\begin{align}\nc_{T}(k) & = Ak^a\\\\\nc_{T-1}(k) & = \\frac{Ak^a}{1+ab}\\\\\nc_{T-2}(k) & = \\frac{Ak^a}{1+ab+a^2b^2}\\\\\n&\\dots\\\\\nc_2(k) & = \\frac{Ak^a}{1+ab+a^2b^2+\\ldots+a^{T-2}b^{T-2}}\\\\\nc_1(k) & = \\frac{Ak^a}{1+ab+a^2b^2+\\ldots+a^{T-2}b^{T-2}+a^{T-1}b^{T-1}}\\\\\nc_0(k) & = \\frac{Ak^a}{1+ab+a^2b^2+\\ldots+a^{T-2}b^{T-2}+a^{T-1}b^{T-1}+a^Tb^T}\n\\end{align}</math>\n\nWe see that it is optimal to consume a larger fraction of current wealth as one gets older, finally consuming all remaining wealth in period {{mvar|T}}, the last period of life.\n\n=== Computer programming ===\n{{More citations needed section|date=April 2014}}\nThere are two key attributes that a problem must have in order for dynamic programming to be applicable: [[optimal substructure]] and [[overlapping subproblem|overlapping sub-problem]]s. If a problem can be solved by combining optimal solutions to ''non-overlapping'' sub-problems, the strategy is called \"[[Divide and conquer algorithm|divide and conquer]]\" instead.<ref name=\":0\" />  This is why [[mergesort|merge sort]] and [[quicksort|quick sort]] are not classified as dynamic programming problems.\n\n''Optimal substructure'' means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of [[recursion]]. For example, given a graph ''G=(V,E)'', the shortest path ''p'' from a vertex ''u'' to a vertex ''v'' exhibits optimal substructure: take any intermediate vertex ''w'' on this shortest path ''p''. If ''p'' is truly the shortest path, then it can be split into sub-paths ''p<sub>1</sub>'' from ''u'' to ''w'' and ''p<sub>2</sub>'' from ''w'' to ''v'' such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in ''[[Introduction to Algorithms]]''). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the [[Bellman–Ford algorithm]] or the [[Floyd–Warshall algorithm]] does.\n\n''Overlapping'' sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the recursive formulation for generating the Fibonacci series: ''F''<sub>''i''</sub> = ''F''<sub>''i''&minus;1</sub> + ''F''<sub>''i''&minus;2</sub>, with base case ''F''<sub>1</sub>&nbsp;=&nbsp;''F''<sub>2</sub>&nbsp;=&nbsp;1. Then ''F''<sub>43</sub> =&nbsp;''F''<sub>42</sub>&nbsp;+&nbsp;''F''<sub>41</sub>, and ''F''<sub>42</sub> =&nbsp;''F''<sub>41</sub>&nbsp;+&nbsp;''F''<sub>40</sub>. Now ''F''<sub>41</sub> is being solved in the recursive sub-trees of both ''F''<sub>43</sub> as well as ''F''<sub>42</sub>. Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once.\n\n[[Image:Fibonacci dynamic programming.svg|thumb|108px|'''Figure 2.''' The subproblem graph for the Fibonacci sequence. The fact that it is not a [[tree structure|tree]] indicates overlapping subproblems.]]\n\nThis can be achieved in either of two ways:{{Citation needed|date=June 2009}}\n\n* ''[[Top-down and bottom-up design|Top-down approach]]'': This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively using the solution to its sub-problems, and if its sub-problems are overlapping, then one can easily [[memoize]] or store the solutions to the sub-problems in a table. Whenever we attempt to solve a new sub-problem, we first check the table to see if it is already solved. If a solution has been recorded, we can use it directly, otherwise we solve the sub-problem and add its solution to the table.\n* ''[[Top-down and bottom-up design|Bottom-up approach]]'': Once we formulate the solution to a problem recursively as in terms of its sub-problems, we can try reformulating the problem in a bottom-up fashion: try solving the sub-problems first and use their solutions to build-on and arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger and bigger sub-problems by using the solutions to small sub-problems. For example, if we already know the values of ''F''<sub>41</sub> and ''F''<sub>40</sub>, we can directly calculate the value of ''F''<sub>42</sub>.\n\nSome [[programming language]]s can automatically [[memoization|memoize]] the result of a function call with a particular set of arguments, in order to speed up [[call-by-name]] evaluation (this mechanism is referred to as ''[[call-by-need]]''). Some languages make it possible portably (e.g. [[Scheme (programming language)|Scheme]], [[Common Lisp]], [[Perl]] or [[D (programming language)|D]]). Some languages have automatic [[memoization]] <!-- still not a typo for \"memor-\" --> built in, such as tabled [[Prolog]] and [[J (programming language)|J]], which supports memoization with the ''M.'' adverb.<ref>{{cite web|title=M. Memo|url=http://www.jsoftware.com/help/dictionary/dmcapdot.htm|work=J Vocabulary|publisher=J Software|accessdate=28 October 2011}}</ref> In any case, this is only possible for a [[referential transparency (computer science)|referentially transparent]] function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages such as [[Wolfram Language]].\n\n=== Bioinformatics ===\nDynamic programming is widely used in bioinformatics for the tasks such as [[sequence alignment]], [[protein folding]], RNA structure prediction and protein-DNA binding. The first dynamic programming algorithms for protein-DNA binding were developed in the 1970s independently by [[Charles DeLisi]] in USA<ref>DeLisi, Biopolymers, 1974, Volume 13, Issue 7, pages 1511–1512, July 1974</ref> and Georgii Gurskii and Alexander Zasedatelev in USSR.<ref>Gurskiĭ GV, Zasedatelev AS, Biofizika, 1978 Sep-Oct;23(5):932-46</ref> Recently these algorithms have become very popular in bioinformatics and computational biology, particularly in the studies of [[nucleosome]] positioning and [[transcription factor]] binding.\n\n== Examples: Computer algorithms ==\n\n=== Dijkstra's algorithm for the shortest path problem ===\nFrom a dynamic programming point of view, [[Dijkstra's algorithm]] for the [[shortest path problem]]  is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the '''Reaching''' method.<ref name=sniedovich_06>{{Citation | last = Sniedovich | first = M. | title = Dijkstra's algorithm revisited: the dynamic programming connexion | journal = Journal of Control and Cybernetics | volume = 35 | issue = 3 | pages = 599–620 | year = 2006 | url = http://matwbn.icm.edu.pl/ksiazki/cc/cc35/cc3536.pdf | format = PDF | postscript = .}} [http://www.ifors.ms.unimelb.edu.au/tutorial/dijkstra_new/index.html Online version of the paper with interactive computational modules.]</ref><ref name=denardo_03>{{Citation | last = Denardo | first = E.V. | title = Dynamic Programming: Models and Applications | publisher = [[Dover Publications]] | location = Mineola, NY | year = 2003 | isbn = 978-0-486-42810-9}}</ref><ref name=sniedovich_10>{{Citation | last = Sniedovich | first = M. | title = Dynamic Programming: Foundations and Principles | publisher = [[Taylor & Francis]] | year = 2010 | isbn = 978-0-8247-4099-3  }}</ref>\n\nIn fact, Dijkstra's explanation of the logic behind the algorithm,<ref>{{harvnb|Dijkstra|1959|p=270}}</ref> namely\n{{quote|\n'''Problem 2.''' Find the path of minimum total length between two given nodes <math>P</math> and <math>Q</math>.\n\nWe use the fact that, if <math>R</math> is a node on the minimal path from <math>P</math> to <math>Q</math>, knowledge of the latter implies the knowledge of the minimal path from <math>P</math> to <math>R</math>.\n}}\n\nis a paraphrasing of [[Richard Bellman|Bellman's]] famous [[Principle of Optimality]] in the context of the [[shortest path problem]].\n\n=== Fibonacci sequence ===\n\nHere is a naïve implementation of a function finding the ''n''th member of the [[Fibonacci sequence]], based directly on the mathematical definition:\n\n    '''function''' fib(n)\n        '''if''' n <= 1 '''return''' n\n        '''return''' fib(n − 1) + fib(n − 2)\n\nNotice that if we call, say, <code>fib(5)</code>, we produce a call tree that calls the function on the same value many different times:\n\n# <code>fib(5)</code>\n# <code>fib(4) + fib(3)</code>\n# <code>(fib(3) + fib(2)) + (fib(2) + fib(1))</code>\n# <code>((fib(2) + fib(1)) + (fib(1) + fib(0))) + ((fib(1) + fib(0)) + fib(1))</code>\n# <code>(((fib(1) + fib(0)) + fib(1)) + (fib(1) + fib(0))) + ((fib(1) + fib(0)) + fib(1))</code>\n\nIn particular, <code>fib(2)</code> was calculated three times from scratch. In larger examples, many more values of <code>fib</code>, or ''subproblems'', are recalculated, leading to an exponential time algorithm.\n\nNow, suppose we have a simple [[Associative array|map]] object, ''m'', which maps each value of <code>fib</code> that has already been calculated to its result, and we modify our function to use it and update it. The resulting function requires only [[Big-O notation|O]](''n'') time instead of exponential time (but requires [[Big-O notation|O]](''n'') space):\n\n    '''var''' m := '''''map'''''(0 → 0, 1 → 1)\n    '''function''' fib(n)\n        '''if ''key''''' n '''is not in ''map''''' m \n            m[n] := fib(n − 1) + fib(n − 2)\n        '''return''' m[n]\n\nThis technique of saving values that have already been calculated is called ''[[memoization]]''; <!-- Yes, memoization, not memorization. Not a typo. --> this is the top-down approach, since we first break the problem into subproblems and then calculate and store values.\n\nIn the '''bottom-up''' approach, we calculate the smaller values of <code>fib</code> first, then build larger values from them. This method also uses O(''n'') time since it contains a loop that repeats n − 1 times, but it only takes constant (O(1)) space, in contrast to the top-down approach which requires O(''n'') space to store the map.\n\n    '''function''' fib(n)\n        '''if''' n = 0\n            '''return''' 0\n        '''else'''\n            '''var''' previousFib := 0, currentFib := 1\n            '''repeat''' n − 1 '''times''' ''// loop is skipped if n = 1''\n                '''var''' newFib := previousFib + currentFib\n                previousFib := currentFib\n                currentFib  := newFib\n        '''return''' currentFib\n\nIn both examples, we only calculate <code>fib(2)</code> one time, and then use it to calculate both <code>fib(4)</code> and <code>fib(3)</code>, instead of computing it every time either of them is evaluated.\n\nNote that the above method actually takes <math>\\Omega(n^2)</math> time for large n because addition of two integers with <math>\\Omega(n)</math> bits each takes <math>\\Omega(n)</math> time. (The ''n''<sup>th</sup> fibonacci number has <math>\\Omega(n)</math> bits.) Also, there is a closed form for the Fibonacci sequence, [[Jacques Philippe Marie Binet#Binet's Fibonacci number formula|known as Binet's formula]], from which the <math>n</math>-th term can be [[Computational complexity of mathematical operations|computed]] in approximately <math>O(n(\\log n)^2)</math> time, which is more efficient than the above dynamic programming technique. However, the simple recurrence directly gives [[Fibonacci sequence#Matrix form|the matrix form]] that leads to an approximately <math>O(n\\log n)</math> algorithm by fast [[matrix exponentiation]].\n\n=== A type of balanced 0–1 matrix ===\n{{unreferenced section|date=May 2013}}\nConsider the problem of assigning values, either zero or one, to the positions of an {{math|<var>n</var> &times; <var>n</var>}} matrix, with {{math|<var>n</var>}} even, so that each row and each column contains exactly {{math|<var>n</var> / 2}} zeros and {{math|<var>n</var> / 2}} ones. We ask how many different assignments there are for a given <math>n</math>. For example, when {{math|<var>n</var> {{=}} 4}}, four possible solutions are\n\n:<math>\\begin{bmatrix}\n0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0\n\\end{bmatrix} \\text{ and } \\begin{bmatrix}\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0\n\\end{bmatrix} \\text{ and } \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix} \\text{ and } \\begin{bmatrix}\n1 & 0 & 0 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n1 & 0 & 0 & 1\n\\end{bmatrix}.</math>\n\nThere are at least three possible approaches: [[Brute-force search|brute force]], [[backtracking]], and dynamic programming.\n\nBrute force consists of checking all assignments of zeros and ones and counting those that have balanced rows and columns ({{math|<var>n</var> / 2}} zeros and {{math|<var>n</var> / 2}} ones). As there are <math>\\tbinom{n}{n/2}^n</math> possible assignments, this strategy is not practical except maybe up to <math>n=6</math>.\n\nBacktracking for this problem consists of choosing some order of the matrix elements and recursively placing ones or zeros, while checking that in every row and column the number of elements that have not been assigned plus the number of ones or zeros are both at least {{math|<var>n</var> / 2}}. While more sophisticated than brute force, this approach will visit every solution once, making it impractical for {{math|<var>n</var>}} larger than six, since the number of solutions is already 116,963,796,250 for {{math|<var>n</var>}}&nbsp;=&nbsp;8, as we shall see.\n\nDynamic programming makes it possible to count the number of solutions without visiting them all. Imagine backtracking values for the first row – what information would we require about the remaining rows, in order to be able to accurately count the solutions obtained for each first row value? We consider {{math|<var>k</var> &times; <var>n</var>}} boards, where {{math|1 &le; <var>k</var> &le; <var>n</var>}}, whose <math>k</math> rows contain <math>n/2</math> zeros and <math>n/2</math> ones. The function ''f'' to which [[memoization]] is applied maps vectors of ''n'' pairs of integers to the number of admissible boards (solutions). There is one pair for each column, and its two components indicate respectively the number of zeros and ones that have yet to be placed in that column. We seek the value of <math> f((n/2, n/2), (n/2, n/2), \\ldots (n/2, n/2)) </math> (<math>n</math> arguments or one vector of <math>n</math> elements). The process of subproblem creation involves iterating over every one of  <math>\\tbinom{n}{n/2}</math> possible assignments for the top row of the board, and going through every column, subtracting one from the appropriate element of the pair for that column, depending on whether the assignment for the top row contained a zero or a one at that position. If any one of the results is negative, then the assignment is invalid and does not contribute to the set of solutions (recursion stops). Otherwise, we have an assignment for the top row of the {{math|<var>k</var> &times; <var>n</var>}} board and recursively compute the number of solutions to the remaining {{math|(<var>k</var> &minus; 1) &times; <var>n</var>}} board, adding the numbers of solutions for every admissible assignment of the top row and returning the sum, which is being memoized. The base case is the trivial subproblem, which occurs for a {{math|1 &times; <var>n</var>}} board. The number of solutions for this board is either zero or one, depending on whether the vector is a permutation of {{math|<var>n</var> / 2}} <math>(0, 1)</math> and {{math|<var>n</var> / 2}} <math>(1, 0)</math> pairs or not.\n\nFor example, in the first two boards shown above the sequences of vectors would be\n<PRE WIDTH=80>\n((2, 2) (2, 2) (2, 2) (2, 2))       ((2, 2) (2, 2) (2, 2) (2, 2))     k = 4\n  0      1      0      1              0      0      1      1\n\n((1, 2) (2, 1) (1, 2) (2, 1))       ((1, 2) (1, 2) (2, 1) (2, 1))     k = 3\n  1      0      1      0              0      0      1      1\n\n((1, 1) (1, 1) (1, 1) (1, 1))       ((0, 2) (0, 2) (2, 0) (2, 0))     k = 2\n  0      1      0      1              1      1      0      0\n\n((0, 1) (1, 0) (0, 1) (1, 0))       ((0, 1) (0, 1) (1, 0) (1, 0))     k = 1\n  1      0      1      0              1      1      0      0\n\n((0, 0) (0, 0) (0, 0) (0, 0))       ((0, 0) (0, 0), (0, 0) (0, 0))\n</PRE>\n\nThe number of solutions {{OEIS|id=A058527}} is\n\n:<math> 1,\\, 2,\\,  90,\\, 297200,\\, 116963796250,\\, 6736218287430460752, \\ldots </math>\n\nLinks to the MAPLE implementation of the dynamic programming approach may be found among the [[Dynamic programming#External links|external links]].\n\n=== Checkerboard ===\n{{unreferenced section|date=May 2013}}\nConsider a [[checkerboard]] with ''n'' × ''n'' squares and a cost function <code>c(i, j)</code> which returns a cost associated with square <code>(i,j)</code> (<code>''i''</code> being the row, <code>''j''</code> being the column). For instance (on a 5 × 5 checkerboard),\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! 5\n| 6 || 7 || 4 || 7 || 8\n|-\n! 4\n| 7 || 6 || 1 || 1 || 4\n|-\n! 3\n| 3 || 5 || 7 || 8 || 2\n|-\n! 2\n| – || 6 || 7 || 0 || –\n|-\n! 1\n| – || – || *5* || – || –\n|-\n!width=\"15\"| !! style=\"width:15px;\"|1 !! style=\"width:15px;\"|2 !! style=\"width:15px;\"|3 !! style=\"width:15px;\"|4 !! style=\"width:15px;\"|5\n|}\nThus <code>c(1, 3) = 5</code>\n\nLet us say there was a checker that could start at any square on the first rank (i.e., row) and you wanted to know the shortest path (the sum of the minimum costs at each visited rank) to get to the last rank; assuming the checker could move only diagonally left forward, diagonally right forward, or straight forward. That is, a checker on <code>(1,3)</code> can move to <code>(2,2)</code>, <code>(2,3)</code> or <code>(2,4)</code>.\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! 5\n|  ||  ||  ||  ||\n|-\n! 4\n|  ||  ||  ||  ||\n|-\n! 3\n|  ||  ||  ||  ||\n|-\n! 2\n|  || x  || x || x ||\n|-\n! 1\n|  ||  || o ||  ||\n|-\n!width=\"15\"| !! style=\"width:15px;\"|1 !! style=\"width:15px;\"|2 !! style=\"width:15px;\"|3 !! style=\"width:15px;\"|4 !! style=\"width:15px;\"|5\n|}\n\nThis problem exhibits '''optimal substructure'''. That is, the solution to the entire problem relies on solutions to subproblems. Let us define a function <code>q(i, j)</code> as\n\n:''q''(''i'', ''j'') = the minimum cost to reach square (''i'', ''j'').\n\nStarting at rank <code>n</code> and descending to rank <code>1</code>, we compute the value of this function for all the squares at each successive rank. Picking the square that holds the minimum value at each rank gives us the shortest path between rank <code>n</code> and rank <code>1</code>.\n\nNote that <code>q(i, j)</code> is equal to the minimum cost to get to any of the three squares below it (since those are the only squares that can reach it) plus <code>c(i, j)</code>. For instance:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! 5\n|  ||  ||  ||  ||\n|-\n! 4\n|  ||  || A  ||  ||\n|-\n! 3\n|  || B || C || D ||\n|-\n! 2\n|  ||  ||  ||  ||\n|-\n! 1\n|  ||  ||  ||  ||\n|-\n!width=\"15\"| !! style=\"width:15px;\"|1 !! style=\"width:15px;\"|2 !! style=\"width:15px;\"|3 !! style=\"width:15px;\"|4 !! style=\"width:15px;\"|5\n|}\n\n: <math>q(A) = \\min(q(B),q(C),q(D))+c(A) \\, </math>\n\nNow, let us define <code>q(i, j)</code> in somewhat more general terms:\n\n: <math>q(i,j)=\\begin{cases} \\infty & j < 1 \\text{ or }j > n \\\\ c(i, j) & i = 1 \\\\ \\min(q(i-1, j-1), q(i-1, j), q(i-1, j+1)) + c(i,j) & \\text{otherwise.}\\end{cases}</math>\n\nThe first line of this equation deals with a board modeled as squares indexed on <code>1</code> at the lowest bound and <code>n</code> at the highest bound. The second line specifies what happens at the last rank; providing a base case. The third line, the recursion, is the important part. It represents the <code>A,B,C,D</code> terms in the example. From this definition we can derive straightforward recursive code for <code>q(i, j)</code>. In the following pseudocode, <code>n</code> is the size of the board, <code>c(i, j)</code> is the cost function, and <code>min()</code> returns the minimum of a number of values:\n\n '''function''' minCost(i, j)\n     '''if''' j &lt; 1 '''or''' j > n\n         '''return''' infinity\n     '''else if''' i = 1\n         '''return''' c(i, j)\n     '''else'''\n         '''return''' '''min'''( minCost(i-1, j-1), minCost(i-1, j), minCost(i-1, j+1) ) + c(i, j)\n\nThis function only computes the path cost, not the actual path. We discuss the actual path below. This, like the Fibonacci-numbers example, is horribly slow because it too exhibits the '''overlapping sub-problems''' attribute. That is, it recomputes the same path costs over and over. However, we can compute it much faster in a bottom-up fashion if we store path costs in a two-dimensional array <code>q[i, j]</code> rather than using a function. This avoids recomputation; all the values needed for array <code>q[i, j]</code> are computed ahead of time only once. Precomputed values for <code>(i,j)</code> are simply looked-up whenever needed.\n\nWe also need to know what the actual shortest path is. To do this, we use another array <code>p[i, j]</code>; a ''predecessor array''. This array records the path to any square <code>s</code>. The predecessor of <code>s</code> is modeled as an offset relative to the index (in <code>q[i, j]</code>) of the precomputed path cost of <code>s</code>. To reconstruct the complete path, we lookup the predecessor of <code>s</code>, then the predecessor of that square, then the predecessor of that square, and so on recursively, until we reach the starting square. Consider the following code:\n\n  '''function''' computeShortestPathArrays()\n      '''for''' x '''from''' 1 '''to''' n\n          q[1, x] := c(1, x)\n      '''for''' y '''from''' 1 '''to''' n\n          q[y, 0]     := infinity\n          q[y, n + 1] := infinity\n      '''for''' y '''from''' 2 '''to''' n\n          '''for''' x '''from''' 1 '''to''' n\n              m := min(q[y-1, x-1], q[y-1, x], q[y-1, x+1])\n              q[y, x] := m + c(y, x)\n              '''if''' m = q[y-1, x-1]\n                  p[y, x] := -1\n              '''else if''' m = q[y-1, x]\n                  p[y, x] :=  0\n              '''else'''\n                  p[y, x] :=  1\n\nNow the rest is a simple matter of finding the minimum and printing it.\n\n  '''function''' computeShortestPath()\n      computeShortestPathArrays()\n      minIndex := 1\n      min := q[n, 1]\n      '''for''' i '''from''' 2 '''to''' n\n          '''if''' q[n, i] &lt; min\n              minIndex := i\n              min := q[n, i]\n      printPath(n, minIndex)\n\n  '''function''' printPath(y, x)\n      '''print'''(x)\n      '''print'''(\"&lt;-\")\n      '''if''' y = 2\n          '''print'''(x + p[y, x])\n      '''else'''\n          printPath(y-1, x + p[y, x])\n\n=== Sequence alignment ===\nIn [[genetics]], [[sequence alignment]] is an important application where dynamic programming is essential.<ref name=\"Eddy\"/>  Typically, the problem consists of transforming one sequence into another using edit operations that replace, insert, or remove an element.  Each operation has an associated cost, and the goal is to find the [[edit distance|sequence of edits with the lowest total cost]].\n\nThe problem can be stated naturally as a recursion, a sequence A is optimally edited into a sequence B by either:\n\n# inserting the first character of B, and performing an optimal alignment of A and the tail of B\n# deleting the first character of A, and performing the optimal alignment of the tail of A and B\n# replacing the first character of A with the first character of B, and performing optimal alignments of the tails of A and B.\n\nThe partial alignments can be tabulated in a matrix, where cell (i,j) contains the cost of the optimal alignment of A[1..i] to B[1..j].  The cost in cell (i,j) can be calculated by adding the cost of the relevant operations to the cost of its neighboring cells, and selecting the optimum.\n\nDifferent variants exist, see [[Smith–Waterman algorithm]] and [[Needleman–Wunsch algorithm]].\n\n=== Tower of Hanoi puzzle ===\n[[Image:Tower of Hanoi.jpeg|300px|thumb|A model set of the Towers of Hanoi (with 8 disks)]]\n[[Image:Tower of Hanoi 4.gif|300px|thumb|An animated solution of the '''Tower of Hanoi''' puzzle for ''T(4,3)''.]]\nThe '''[[Tower of Hanoi]]''' or '''Towers of [[Hanoi]]''' is a [[mathematical game]] or [[puzzle]]. It consists of three rods, and a number of disks of different sizes which can slide onto any rod. The puzzle starts with the disks in a neat stack in ascending order of size on one rod, the smallest at the top, thus making a conical shape.\n\nThe objective of the puzzle is to move the entire stack to another rod, obeying the following rules:\n\n* Only one disk may be moved at a time.\n* Each move consists of taking the upper disk from one of the rods and sliding it onto another rod, on top of the other disks that may already be present on that rod.\n* No disk may be placed on top of a smaller disk.\n\nThe dynamic programming solution consists of solving the [[Bellman equation|functional equation]]\n\n: S(n,h,t) = S(n-1,h, not(h,t)) ; S(1,h,t) ; S(n-1,not(h,t),t)\n\nwhere n denotes the number of disks to be moved, h denotes the home rod, t denotes the target rod, not(h,t) denotes the third rod (neither h nor t), \";\" denotes concatenation, and\n\n: S(n, h, t) := solution to a problem consisting of n disks that are to be moved from rod h to rod t.\n\nNote that for n=1 the problem is trivial, namely S(1,h,t) = \"move a disk from rod h to rod t\" (there is only one disk left).\n\nThe  number of moves required by this solution is 2<sup>''n''</sup>&nbsp;&minus;&nbsp;1. If the objective is to '''maximize''' the number of moves (without cycling) then the dynamic programming [[Bellman equation|functional equation]]  is slightly more complicated and  3<sup>''n''</sup>&nbsp;&minus;&nbsp;1 moves are required.<ref>{{Citation |author=Moshe Sniedovich |title= OR/MS Games: 2. The Towers of Hanoi Problem |journal=INFORMS Transactions on Education |volume=3 |issue=1 |year=2002 |pages=34–51 |url=http://archive.ite.journal.informs.org/Vol3No1/Sniedovich/ |postscript=.}}</ref>\n\n=== Egg dropping puzzle ===\nThe following is a description of the instance of this famous [[puzzle]] involving N=2 eggs and a building with H=36 floors:<ref>Konhauser J.D.E., Velleman, D., and Wagon, S. (1996). [http://www.cambridge.org/uk/catalogue/catalogue.asp?isbn=9780883853252 Which way did the Bicycle Go?] Dolciani Mathematical Expositions – No 18.  [[The Mathematical Association of America]].</ref> \n:Suppose that we wish to know which stories in a 36-story building are safe to drop eggs from, and which will cause the eggs to break on landing (using [[U.S. English]] terminology, in which the first floor is at ground level). We make a few assumptions:\n\n:* An egg that survives a fall can be used again.\n:* A broken egg must be discarded.\n:* The effect of a fall is the same for all eggs.\n:* If an egg breaks when dropped, then it would break if dropped from a higher window.\n:* If an egg survives a fall, then it would survive a shorter fall.\n:* It is not ruled out that the first-floor windows break eggs, nor is it ruled out that eggs can survive the 36th-floor windows.\n\n: If only one egg is available and we wish to be sure of obtaining the right result, the experiment can be carried out in only one way. Drop the egg from the first-floor window; if it survives, drop it from the second-floor window. Continue upward until it breaks. In the worst case, this method may require 36 droppings. Suppose 2 eggs are available. What is the lowest number of egg-droppings that is guaranteed to work in all cases?\n\nTo derive a dynamic programming [[Bellman equation|functional equation]] for this puzzle, let the '''state''' of the dynamic programming model be a pair s = (n,k), where\n\n: ''n'' = number of test eggs available, ''n'' = 0, 1, 2, 3, ..., ''N''&nbsp;&minus;&nbsp;1.\n: ''k'' = number of (consecutive) floors yet to be tested, ''k'' = 0, 1, 2, ..., ''H''&nbsp;&minus;&nbsp;1.\n\nFor instance, ''s'' = (2,6) indicates that two test eggs are available and 6 (consecutive) floors are yet to be tested. The initial state of the process is ''s'' = (''N'',''H'') where ''N'' denotes the number of test eggs available at the commencement of the experiment. The process terminates either when there are no more test eggs (''n'' = 0) or when ''k'' = 0, whichever occurs first. If termination occurs at state ''s'' = (0,''k'') and ''k''&nbsp;>&nbsp;0, then the test failed.\n\nNow, let\n\n: ''W''(''n'',''k'') = minimum number of trials required to identify the value of the critical floor under the worst-case scenario given that the process is in state ''s'' = (''n'',''k'').\n\nThen it can be shown that<ref name=\"sniedovich_03\">Sniedovich, M. (2003). [http://pubsonline.informs.org/toc/ited/4/1 The joy of egg-dropping in Braunschweig and Hong Kong].  INFORMS Transactions on Education,  4(1) 48–64.</ref>\n\n: ''W''(''n'',''k'') = 1 + min{max(''W''(''n'' &minus; 1, ''x'' &minus; 1), ''W''(''n'',''k'' &minus; ''x'')): ''x'' = 1, 2, ..., ''k'' }\n\nwith ''W''(''n'',0) = 0 for all ''n''&nbsp;>&nbsp;0 and ''W''(1,''k'') = ''k'' for all&nbsp;''k''. It is easy to solve this equation iteratively by systematically increasing the values of ''n'' and&nbsp;''k''.\n\nAn interactive online facility is available for experimentation with this model as well as with other versions of this puzzle (e.g. when the objective is to minimize the '''expected value''' of the number of trials.)<ref name=\"sniedovich_03\" />\n\n==== Faster DP solution using a different parametrization ====\nNotice that the above solution takes <math>O( n k^2 )</math> time with a DP solution. This can be improved to <math>O( n k \\log k )</math> time by binary searching on the optimal <math>x</math> in the above recurrence, since <math>W(n-1,x-1)</math> is increasing in <math>x</math> while <math>W(n,k-x)</math> is decreasing in <math>x</math>, thus a local minimum of <math>\\max(W(n-1,x-1),W(n,k-x))</math> is a global minimum. Also, by storing the optimal <math>x</math> for each cell in the DP table and referring to its value for the previous cell, the optimal <math>x</math> for each cell can be found in constant time, improving it to <math>O( n k )</math> time. However, there is an even faster solution that involves a different parametrization of the problem:\n\nLet <math>k</math> be the total number of floors such that the eggs break when dropped from the <math>k</math>th floor (The example above is equivalent to taking <math>k=37</math>).\n\nLet <math>m</math> be the minimum floor from which the egg must be dropped to be broken.\n\nLet <math>f(t,n)</math> be the maximum number of values of <math>m</math> that are distinguishable using <math>t</math> tries and <math>n</math> eggs.\n\nThen <math>f(t,0) = f(0,n) = 1</math> for all <math>t,n \\geq 0</math>.\n\nLet <math>a</math> be the floor from which the first egg is dropped in the optimal strategy.\n\nIf the first egg broke, <math>m</math> is from <math>1</math> to <math>a</math> and distinguishable using at most <math>t-1</math> tries and <math>n-1</math> eggs.\n\nIf the first egg did not break, <math>m</math> is from <math>a+1</math> to <math>k</math> and distinguishable using <math>t-1</math> tries and <math>n</math> eggs.\n\nTherefore, <math>f(t,n) = f(t-1,n-1) + f(t-1,n)</math>.\n\nThen the problem is equivalent to finding the minimum <math>x</math> such that <math>f(x,n) \\geq k</math>.\n\nTo do so, we could compute <math>\\{ f(t,i) : 0 \\leq i \\leq n \\}</math> in order of increasing <math>t</math>, which would take <math>O( n x )</math> time.\n\nThus, if we separately handle the case of <math>n=1</math>, the algorithm would take <math>O( n \\sqrt{k} )</math> time.\n\nBut the recurrence relation can in fact be solved, giving <math>f(t,n) = \\sum_{i=0}^{n}{ \\binom{t}{i} }</math>, which can be computed in <math>O(n)</math> time using the identity <math>\\binom{t}{i+1} = \\binom{t}{i} \\frac{t-i}{i+1}</math> for all <math>i \\geq 0</math>.\n\nSince <math>f(t,n) \\leq f(t+1,n)</math> for all <math>t \\geq 0</math>, we can binary search on <math>t</math> to find <math>x</math>, giving an <math>O( n \\log k )</math> algorithm.\n<ref>{{Citation |author=Dean Connable Wills |title=Connections between combinatorics of permutations and algorithms and geometry |url=https://ir.library.oregonstate.edu/xmlui/handle/1957/11929?show=full}}</ref>\n\n=== Matrix chain multiplication ===\n{{unreferenced section|date=May 2013}}\n{{Main|Matrix chain multiplication}}\n<!--Show how the placement of parentheses affects the number of scalar multiplications required when multiplying a bunch of matrices.\nShow how to write a dynamic program to calculate the optimal parentheses placement.\nThis is such a long example that it might be better to make it its own article.-->\n\nMatrix chain multiplication is a well-known example that demonstrates utility of dynamic programming. For example,  engineering applications often have to multiply a chain of matrices. It is not surprising to find matrices of large dimensions, for example 100×100. Therefore, our task is to multiply matrices {{tmath|A_1, A_2, .... A_n}}. As we know from basic linear algebra, matrix multiplication is not commutative, but is associative; and we can multiply only two matrices at a time. So, we can multiply this chain of matrices in many different ways, for example:\n\n: {{math|((A<sub>1</sub> × A<sub>2</sub>) × A<sub>3</sub>) × ... A<sub>n</sub>}}\n\n: {{math|A<sub>1</sub>×(((A<sub>2</sub>×A<sub>3</sub>)× ... ) × A<sub>n</sub>)}}\n\n: {{math|(A<sub>1</sub> × A<sub>2</sub>) × (A<sub>3</sub> × ... A<sub>n</sub>)}}\n\nand so on. There are numerous ways to multiply this chain of matrices. They will all produce the same final result, however they will take more or less time to compute, based on which particular matrices are multiplied. If matrix A has dimensions m×n and matrix B has dimensions n×q, then matrix C=A×B will have dimensions m×q, and will require m*n*q scalar multiplications (using a simplistic matrix multiplication algorithm for purposes of illustration).\n\nFor example, let us multiply matrices A, B and C. Let us assume that their dimensions are m×n, n×p, and p×s, respectively. Matrix A×B×C will be of size m×s and can be calculated in two ways shown below:\n\n# Ax(B×C)    This order of matrix multiplication will require nps + mns scalar multiplications.  \n# (A×B)×C    This order of matrix multiplication will require mnp + mps scalar calculations.\n\nLet us assume that m = 10, n = 100, p = 10 and s = 1000. So, the first way to multiply the chain will require 1,000,000 + 1,000,000 calculations. The second way will require only 10,000+100,000 calculations. Obviously, the second way is faster, and we should multiply the matrices using that arrangement of parenthesis.\n\nTherefore, our conclusion is that the order of parenthesis matters, and that our task is to find the optimal order of parenthesis.\n\nAt this point, we have several choices, one of which is to design a dynamic programming algorithm that will split the problem into overlapping problems and calculate the optimal arrangement of parenthesis. The dynamic programming solution is presented below.\n\nLet's call m[i,j] the minimum number of scalar multiplications needed to multiply a chain of matrices from matrix i to matrix j (i.e. A<sub>i</sub> × .... × A<sub>j</sub>, i.e. i<=j). We split the chain at some matrix k, such that i <= k < j, and try to find out which combination produces minimum m[i,j].\n\nThe formula is:\n\n        '''if''' i = j, m[i,j]= 0\n        '''if''' i < j, m[i,j]= min over all possible values of k {{nowrap|(m[i,k]+m[k+1,j] + <math>p_{i-1}*p_k*p_j</math>)}} \nwhere ''k'' ranges from ''i'' to ''j''&nbsp;&minus;&nbsp;1.\n\n*{{tmath|p_{{(}}i-1{{)}}}} is the row dimension of matrix i,\n*{{tmath|p_k}} is the column dimension of matrix k,\n*{{tmath|p_j}} is the column dimension of matrix j.\n\nThis formula can be coded as shown below, where input parameter \"chain\" is the chain of matrices, i.e. {{tmath|A_1, A_2, ... A_n}}:\n\n  '''function''' OptimalMatrixChainParenthesis(chain)\n      n = length(chain)\n      '''for''' i = 1, n\n          m[i,i] = 0    ''// Since it takes no calculations to multiply one matrix''\n      '''for''' len = 2, n\n          '''for''' i = 1, n - len + 1\n              j = i + len -1\n              m[i,j] = infinity      ''// So that the first calculation updates''\n              '''for''' k = i, j-1\n                  {{nowrap|1=q = m[i, k] + m[k+1, j] + <math>p_{i-1}*p_k*p_j</math>}}\n                  '''if''' q < m[i, j]     ''// The new order of parentheses is better than what we had''\n                      m[i, j] = q    ''// Update''\n                      s[i, j] = k    ''// Record which k to split on, i.e. where to place the parenthesis''\n\nSo far, we have calculated values for all possible {{math|''m''[''i'', ''j'']}}, the minimum number of calculations to multiply a chain from matrix ''i'' to matrix ''j'', and we have recorded the corresponding \"split point\"{{math|''s''[''i'', ''j'']}}. For example, if we are multiplying chain {{math|A<sub>1</sub>×A<sub>2</sub>×A<sub>3</sub>×A<sub>4</sub>}}, and it turns out that {{math|1=''m''[1, 3] = 100}} and {{math|1=''s''[1, 3] = 2}}, that means that the optimal placement of parenthesis for matrices 1 to 3 is {{tmath|(A_1\\times A_2)\\times A_3}} and to multiply those matrices will require 100 scalar calculation.\n\nThis algorithm will produce \"tables\" ''m''[, ] and ''s''[, ] that will have entries for all possible values of i and j. The final solution for the entire chain is m[1, n], with corresponding split at s[1, n]. Unraveling the solution will be recursive, starting from the top and continuing until we reach the base case, i.e. multiplication of single matrices.\n\nTherefore, the next step is to actually split the chain, i.e. to place the parenthesis where they (optimally) belong. For this purpose we could use the following algorithm:\n\n  '''function''' PrintOptimalParenthesis(s, i, j)\n      '''if''' i = j\n         print \"A\"i\n      '''else'''\n         print \"(\" PrintOptimalParenthesis(s, i, s[i, j]) PrintOptimalParenthesis(s, s[i, j] + 1, j) \")\"\n\nOf course, this algorithm is not useful for actual multiplication. This algorithm is just a user-friendly way to see what the result looks like.\n\nTo actually multiply the matrices using the proper splits, we need the following algorithm:\n<source lang=\"javascript\" enclose=\"div\">\n   function MatrixChainMultiply(chain from 1 to n)       // returns the final matrix, i.e. A1×A2×... ×An\n      OptimalMatrixChainParenthesis(chain from 1 to n)   // this will produce s[ . ] and m[ . ] \"tables\"\n      OptimalMatrixMultiplication(s, chain from 1 to n)  // actually multiply\n\n   function OptimalMatrixMultiplication(s, i, j)   // returns the result of multiplying a chain of matrices from Ai to Aj in optimal way\n      if i < j\n         // keep on splitting the chain and multiplying the matrices in left and right sides\n         LeftSide = OptimalMatrixMultiplication(s, i, s[i, j])\n         RightSide = OptimalMatrixMultiplication(s, s[i, j] + 1, j)\n         return MatrixMultiply(LeftSide, RightSide) \n      else if i = j\n         return Ai   // matrix at position i\n      else \n         print \"error, i <= j must hold\"\n\n    function MatrixMultiply(A, B)    // function that multiplies two matrices\n      if columns(A) = rows(B) \n         for i = 1, rows(A)\n            for j = 1, columns(B)\n               C[i, j] = 0\n               for k = 1, columns(A)\n                   C[i, j] = C[i, j] + A[i, k]*B[k, j] \n               return C \n      else \n          print \"error, incompatible dimensions.\"\n</source>\n\n== History ==\nThe term ''dynamic programming'' was originally used in the 1940s by [[Richard Bellman]] to describe the process of solving problems where one needs to find the best decisions one after another.  By 1953, he refined this to the modern meaning, referring specifically to nesting smaller decision problems inside larger decisions,<ref>Stuart Dreyfus. [https://web.archive.org/web/20050110161049/http://www.wu-wien.ac.at/usr/h99c/h9951826/bellman_dynprog.pdf \"Richard Bellman on the birth of Dynamical Programming\"].</ref>  and the field was thereafter recognized by the [[IEEE]] as a [[systems analysis]] and [[engineering]] topic.  Bellman's contribution is remembered in the name of the [[Bellman equation]], a central result of dynamic programming which restates an optimization problem in [[Recursion (computer science)|recursive]] form.\n\nBellman explains the reasoning behind the term ''dynamic programming'' in his autobiography, ''Eye of the Hurricane: An Autobiography'' (1984, page 159). He explains:\n\n:\"I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, Where did the name, dynamic programming, come from? The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named [[Charles Erwin Wilson|Wilson]]. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word research. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let's kill two birds with one stone. Let's take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it's impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It's impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.\"\n\nThe word ''dynamic'' was chosen by Bellman to capture the time-varying aspect of the problems, and because it sounded impressive.<ref name=\"Eddy\">{{cite journal |last=Eddy |first=S. R. |authorlink=Sean Eddy |title=What is Dynamic Programming? |journal=Nature Biotechnology |volume=22 |issue= 7|pages=909–910 |year=2004 |doi=10.1038/nbt0704-909 |pmid=15229554 }}</ref> The word ''programming'' referred to the use of the method to find an optimal ''program'', in the sense of a military schedule for training or logistics.  This usage is the same as that in the phrases ''[[linear programming]]'' and ''mathematical programming'', a synonym for [[mathematical optimization]].<ref>{{cite book |last=Nocedal |first=J. |last2=Wright |first2=S. J. |title=Numerical Optimization |page=9 |publisher=Springer |year=2006 }}</ref>\n\nThe above explanation of the origin of the term is lacking. As Russell and Norvig in their book have written, referring to the above story: \"''This cannot be strictly true, because his first paper using the term (Bellman, 1952) appeared before Wilson became Secretary of Defense in 1953.''”<ref>{{cite book |last=Russell |first=S. |last2=Norvig |first2=P. |title=Artificial Intelligence: A Modern Approach |edition=3rd |publisher=Prentice Hall |year=2009 |isbn=978-0-13-207148-2 }}</ref> Also, there is a comment in a speech by [http://a2c2.org/awards/richard-e-bellman-control-heritage-award/2004-00-00t000000/harold-j-kushner Harold J. Kushner], where he remembers Bellman. Quoting Kushner as he speaks of Bellman: \"''On the other hand, when I asked him the same question, he replied that he was trying to upstage Dantzig's linear programming by adding dynamic. Perhaps both motivations were true.''\"\n\n== Algorithms that use dynamic programming ==\n{{unreferenced section|date=May 2013}}\n* Recurrent solutions to [[lattice models]] for protein-DNA binding\n* [[Backward induction]] as a solution method for finite-horizon [[discrete-time]] dynamic optimization problems\n* [[Method of undetermined coefficients]] can be used to solve the [[Bellman equation]] in infinite-horizon, discrete-time, [[discounting|discounted]], [[Time-invariant system|time-invariant]] dynamic optimization problems\n* Many [[String (computer science)|string]] algorithms including [[longest common subsequence problem|longest common subsequence]], [[Longest increasing subsequence problem|longest increasing subsequence]], [[Longest common substring problem|longest common substring]], [[Levenshtein distance]] (edit distance)\n* Many algorithmic problems on [[undirected graph|graphs]] can be solved efficiently for graphs of bounded [[treewidth]] or bounded [[clique-width]] by using dynamic programming on a [[tree decomposition]] of the graph.\n* The [[CYK algorithm|Cocke–Younger–Kasami (CYK) algorithm]] which determines whether and how a given string can be generated by a given [[context-free grammar]]\n* [[Word wrap|Knuth's word wrapping algorithm]] that minimizes raggedness when word wrapping text\n* The use of [[transposition table]]s and [[refutation table]]s in [[computer chess]]\n* The [[Viterbi algorithm]] (used for [[hidden Markov model]]s, and particularly in [[part of speech tagging]])\n* The [[Earley algorithm]] (a type of [[chart parser]])\n* The [[Needleman–Wunsch algorithm]] and other algorithms used in [[bioinformatics]], including [[sequence alignment]], [[structural alignment]], [[RNA structure|RNA structure prediction]]\n* [[Floyd–Warshall algorithm|Floyd's all-pairs shortest path algorithm]]\n* Optimizing the order for [[chain matrix multiplication]]\n* [[Pseudo-polynomial time]] algorithms for the [[subset sum problem|subset sum]], [[Knapsack problem|knapsack]] and [[Partition problem|partition]] problems\n* The [[dynamic time warping]] algorithm for computing the global distance between two time series\n* The [[Patricia Selinger|Selinger]] (a.k.a. [[IBM System R|System R]]) algorithm for relational database query optimization\n* [[De Boor algorithm]] for evaluating B-spline curves\n* [[Duckworth–Lewis method]] for resolving the problem when games of cricket are interrupted\n* The value iteration method for solving [[Markov decision process]]es\n* Some graphic image edge following selection methods such as the \"magnet\" selection tool in [[Photoshop]]\n* Some methods for solving [[interval scheduling]] problems\n* Some methods for solving the [[travelling salesman problem]], either exactly (in [[exponential time]]) or approximately (e.g. via the [[bitonic tour]])\n* [[Recursive least squares]] method\n* [[Beat (music)|Beat]] tracking in [[music information retrieval]]\n* Adaptive-critic training strategy for [[artificial neural networks]]\n* Stereo algorithms for solving the [[correspondence problem]] used in stereo vision\n* [[Seam carving]] (content-aware image resizing)\n* The [[Bellman–Ford algorithm]] for finding the shortest distance in a graph\n* Some approximate solution methods for the [[linear search problem]]\n* Kadane's algorithm for the [[maximum subarray problem]]\n*Optimization of electric generation expansion plans in the [https://www-pub.iaea.org/MTCD/publications/PDF/CMS-16.pdf Wein Automatic System Planning (WASP)] package\n\n== See also ==\n{{Portal|Computer science}}\n* [[Convexity in economics]]\n* [[Greedy algorithm]]\n* [[Non-convexity (economics)]]\n* [[Stochastic programming]]\n* [[Stochastic dynamic programming]]\n\n== References ==\n{{Reflist|30em}}\n\n== Further reading ==\n*{{citation|last1=Adda|first1=Jerome|last2=Cooper|first2=Russell|year=2003|url=http://www.eco.utexas.edu/~cooper/dynprog/dynprog1.html|title=Dynamic Economics|publisher=MIT Press}}. An accessible introduction to dynamic programming in economics. The link contains sample programs.\n*{{citation|first=Richard|last=Bellman|authorlink=Richard Bellman|title=The theory of dynamic programming|journal=[[Bulletin of the American Mathematical Society]]|year=1954|volume=60|pages=503–516|doi=10.1090/S0002-9904-1954-09848-8|mr=0067459|issue=6|pmid=16589166|pmc=1063639}}. Includes an extensive bibliography of the literature in the area, up to the year 1954.\n*{{citation|first=Richard|last=Bellman|authorlink=Richard Bellman|year=1957|title=Dynamic Programming|publisher=Princeton University Press}}. Dover paperback edition (2003), {{isbn|0-486-42809-5}}.\n*{{citation|last=Bertsekas|first=D. P.|year=2017|title=Dynamic Programming and Optimal Control|edition=4th|publisher=Athena Scientific|isbn=978-1-886529-08-3}}. In two volumes.\n*{{citation|last1=Cormen|first1=Thomas H.|author1-link=Thomas H. Cormen|last2=Leiserson|first2=Charles E.|author2-link=Charles E. Leiserson|last3=Rivest|first3=Ronald L.|author3-link=Ronald L. Rivest|last4=Stein|first4=Clifford|author4-link=Clifford Stein|year=2001|title=Introduction to Algorithms|edition=2nd|publisher=MIT Press & McGraw–Hill|isbn=978-0-262-03293-3|title-link=Introduction to Algorithms}}. Especially pp.&nbsp;323–69.\n*{{citation|first1=Stuart E.|last1=Dreyfus|first2=Averill M.|last2=Law|year=1977|title=The Art and Theory of Dynamic Programming|publisher=Academic Press|isbn=978-0-12-221860-6}}.\n*{{citation|last1=Giegerich|first1=R.|last2=Meyer|first2=C.|last3=Steffen|first3=P.|year=2004|url=http://bibiserv.techfak.uni-bielefeld.de/adp/ps/GIE-MEY-STE-2004.pdf|title=A Discipline of Dynamic Programming over Sequence Data|journal=Science of Computer Programming|volume=51|pages=215–263|issue=3|doi=10.1016/j.scico.2003.12.005}}.\n*{{citation|first=Sean|last=Meyn|url=https://netfiles.uiuc.edu/meyn/www/spm_files/CTCN/CTCN.html|title=Control Techniques for Complex Networks|publisher=Cambridge University Press|year=2007|isbn=978-0-521-88441-9|deadurl=yes|archiveurl=https://web.archive.org/web/20100619011046/https://netfiles.uiuc.edu/meyn/www/spm_files/CTCN/CTCN.html|archivedate=2010-06-19|df=}}.\n*{{cite journal | last1 = Sritharan | first1 = S. S. | year = 1991 | title = Dynamic Programming of the Navier-Stokes Equations | url = | journal = Systems and Control Letters | volume = 16 | issue = 4| pages = 299–307 | doi=10.1016/0167-6911(91)90020-f}}\n*{{citation|last1=Stokey|first1=Nancy|author1-link=Nancy Stokey|last2=Lucas|first2=Robert E.|author2-link=Robert E. Lucas|last3=Prescott|first3=Edward|author3-link=Edward Prescott|year=1989|title=Recursive Methods in Economic Dynamics|publisher=Harvard Univ. Press|isbn=978-0-674-75096-8}}.\n\n== External links ==\n{{external cleanup|date=March 2016}}\n* [http://blog.refdash.com/dynamic-programming-tutorial-example/ 7 Step Framework to Solve Dynamic Programming Interview Problems]\n* [https://www.interviewbit.com/courses/programming/topics/dynamic-programming/tutorial/dynamic-programming-dp-introduction An Introduction to Dynamic Programming]\n* [http://apmonitor.com/do Dynamic Optimization Online Course]\n* [http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html A Tutorial on Dynamic programming]\n* [http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-046JFall-2005/VideoLectures/detail/embed15.htm MIT course on algorithms] – Includes a video lecture on DP along with lecture notes\n* [http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Dynamic More DP Notes]\n* King, Ian, 2002 (1987), \"[http://researchspace.auckland.ac.nz/bitstream/handle/2292/190/230.pdf A Simple Introduction to Dynamic Programming in Macroeconomic Models.]\" An introduction to dynamic programming as an important tool in economic theory.\n* [http://www.topcoder.com/tc?module=Static&d1=tutorials&d2=dynProg Dynamic Programming: from novice to advanced] A TopCoder.com article by Dumitru on Dynamic Programming\n* [https://bibiserv.cebitec.uni-bielefeld.de/adp/welcome.html Algebraic Dynamic Programming] – a formalized framework for dynamic programming, including an [https://bibiserv.cebitec.uni-bielefeld.de/cgi-bin/dpcourse entry-level course] to DP, University of Bielefeld\n* Dreyfus, Stuart, \"[http://www.cas.mcmaster.ca/~se3c03/journal_papers/dy_birth.pdf Richard Bellman on the birth of Dynamic Programming.]\"\n* [http://www.avatar.se/lectures/molbioinfo2001/dynprog/dynamic.html Dynamic programming tutorial]\n* [http://www.cambridge.org/resources/0521882672/7934_kaeslin_dynpro_new.pdf  A Gentle Introduction to Dynamic Programming and the Viterbi Algorithm]\n* Tabled Prolog [http://www.probp.com BProlog] and [http://xsb.sourceforge.net/ XSB]\n* [http://ifors.org/tutorial/category/dynamic-programming/ Online interactive dynamic programming modules] including, shortest path, traveling salesman, knapsack, false coin, egg dropping, bridge and torch, replacement, chained matrix products, and critical path problem.\n\n{{optimization algorithms|combinatorial|state=expanded}}\n{{Parsers}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Dynamic Programming}}\n[[Category:Dynamic programming| ]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Equations]]\n[[Category:Systems engineering]]\n[[Category:Optimal control]]"
    },
    {
      "title": "Evolutionary algorithm",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
      "text": "In [[artificial intelligence]], an '''evolutionary algorithm''' ('''EA''') is a [[subset]] of [[evolutionary computation]],<ref name=\"EVOALG\">{{cite article|last=Vikhar|first=P. A.|title=Evolutionary algorithms: A critical review and its future prospects|url= http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7955308&isnumber=7955253|journal=Proceedings of the 2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC)|publisher= Jalgaon, 2016, pp. 261-265|isbn=978-1-5090-0467-6}}</ref> a generic population-based [[metaheuristic]] [[optimization (mathematics)|optimization]] [[algorithm]]. An EA uses mechanisms inspired by [[biological evolution]], such as [[reproduction]], [[mutation]], [[genetic recombination|recombination]], and [[natural selection|selection]]. [[Candidate solution]]s to the [[optimization problem]] play the role of individuals in a population, and the [[fitness function]] determines the quality of the solutions (see also [[loss function]]). [[Evolution]] of the population then takes place after the repeated application of the above operators.\n\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying [[fitness landscape]]. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of [[microevolution|microevolutionary processes]] and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor.<ref name=\"VLSI\">{{cite book|last=Cohoon|first=J|display-authors=etal|title=Evolutionary algorithms for the physical design of VLSI circuits|url= https://www.ifte.de/mitarbeiter/lienig/cohoon.pdf|journal=Advances in Evolutionary Computing: Theory and Applications|publisher= Springer, pp. 683-712, 2003|isbn=978-3-540-43330-9}}</ref> In fact, this computational complexity is due to fitness function evaluation. [[Fitness approximation]] is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems{{Citation needed|date=June 2018}}; therefore, there may be no direct link between algorithm complexity and problem complexity.\n\n==Implementation==\n\nStep One: Generate the initial [[population]] of [[individual]]s randomly. (First generation)\n\nStep Two: Evaluate the [[Fitness function|fitness]] of each individual in that population (time limit, sufficient fitness achieved, etc.)\n\nStep Three: Repeat the following regenerational steps until termination:\n#Select the best-fit individuals for [[reproduce|reproduction]]. (Parents)\n# [[Breed]] new individuals through [[crossover (genetic algorithm)|crossover]] and [[mutation (genetic algorithm)|mutation]] operations to give birth to [[offspring]].\n# Evaluate the individual fitness of new individuals.\n# Replace least-fit population with new individuals.\n\n==Types==\nSimilar techniques differ in [[genetic representation]] and other implementation details, and the nature of the particular applied problem.\n*[[Genetic algorithm]] – This is the most popular type of EA. One seeks the solution of a problem in the form of strings of numbers (traditionally binary, although the best representations are usually those that reflect something about the problem being solved),<ref name=VLSI/> by applying operators such as recombination and mutation (sometimes one, sometimes both).  This type of EA is often used in [[Optimization (mathematics)|optimization]] problems. \n*[[Genetic programming]] – Here the solutions are in the form of computer programs, and their fitness is determined by their ability to solve a computational problem.\n*[[Evolutionary programming]] – Similar to genetic programming, but the structure of the program is fixed and its numerical parameters are allowed to evolve.\n*[[Gene expression programming]] – Like genetic programming, GEP also evolves computer programs but it explores a genotype-phenotype system, where computer programs of different sizes are encoded in linear chromosomes of fixed length.\n*[[Evolution strategy]] – Works with vectors of real numbers as representations of solutions, and typically uses self-adaptive mutation rates.\n*[[Differential evolution]] – Based on vector differences and is therefore primarily suited for [[numerical optimization]] problems.\n*[[Neuroevolution]] – Similar to genetic programming but the genomes represent artificial neural networks by describing structure and connection weights. The genome encoding can be direct or indirect.\n*[[Learning classifier system]] – Here the solution is a set of classifiers (rules or conditions). A Michigan-LCS evolves at the level of individual classifiers whereas a Pittsburgh-LCS uses populations of classifier-sets. Initially, classifiers were only binary, but now include real, neural net, or [[S-expression]] types. Fitness is typically determined with either a strength or accuracy based [[reinforcement learning]] or [[supervised learning]] approach.\n\n==Comparison to biological processes==\n\nA possible limitation{{According to whom|date=May 2013}} of many evolutionary algorithms is their lack of a clear [[genotype-phenotype distinction]]. In nature, the fertilized egg cell undergoes a complex process known as [[embryogenesis]] to become a mature [[phenotype]]. This indirect [[encoding]] is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the [[evolvability]] of the organism.<ref>G.S. Hornby and J.B. Pollack. \"Creating high-level components with a generative representation for body-brain evolution\". ''[[Artificial Life (journal)|Artificial Life]]'', 8(3):223–246, 2002.</ref><ref>Jeff Clune, Benjamin Beckmann, Charles Ofria, and Robert Pennock. [http://www.ofria.com/pubs/2009CluneEtAl.pdf \"Evolving Coordinated Quadruped Gaits with the HyperNEAT Generative Encoding\"]. ''Proceedings of the IEEE Congress on Evolutionary Computing Special Section on Evolutionary Robotics'', 2009. Trondheim, Norway.</ref> Such indirect (a.k.a. generative or developmental) encodings also enable evolution to exploit the regularity in the environment.<ref>J. Clune, C. Ofria, and R. T. Pennock, [http://jeffclune.com/publications/Clune-HyperNEATandRegularity.pdf \"How a generative encoding fares as problem-regularity decreases\"], in ''PPSN'' (G. Rudolph, T. Jansen, S. M. Lucas, C. Poloni, and N. Beume, eds.), vol. 5199 of ''Lecture Notes in Computer Science'', pp. 358–367, Springer, 2008.</ref> Recent work in the field of [[artificial development|artificial embryogeny]], or artificial developmental systems, seeks to address these concerns. And [[gene expression programming]] successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.<ref>Ferreira, C., 2001. [http://www.gene-expression-programming.com/webpapers/GEP.pdf \"Gene Expression Programming: A New Adaptive Algorithm for Solving Problems\"]. ''Complex Systems'', Vol. 13, issue 2: 87–129.</ref> {{Synthesis inline|date=May 2013}}\n\n==Related techniques==\n[[Swarm intelligence|Swarm algorithms]]{{clarify|reason=Why are swarm algorithms associated with evolutionary ones?|date=January 2018}} include\n* [[Ant colony optimization]] – Based on the ideas of ant foraging by pheromone communication to form paths. Primarily suited for [[combinatorial optimization]] and [[graph theory|graph]] problems.\n* The runner-root algorithm (RRA) is inspired by the function of runners and roots of plants in nature<ref>F. Merrikh-Bayat, \"The runner-root algorithm: A metaheuristic for solving unimodal and multimodal optimization problems inspired by runners and roots of plants in nature\", ''Applied Soft Computing'', Vol. 33, pp. 292–303, 2015</ref>\n* [[Artificial bee colony algorithm]] – Based on the honey bee foraging behaviour. Primarily proposed for numerical optimization and extended to solve combinatorial, constrained and multi-objective optimization problems.\n* [[Bees algorithm]] is based on the foraging behaviour of honey bees. It has been applied in many applications such as routing and scheduling.\n* [[Cuckoo search]] is inspired by the brooding parasitism of the [[cuckoo]] species. It also uses [[Lévy flight]]s, and thus it suits for global [[optimization]] problems.\n* Electimize optimization - Based on the behavior of electron flow through electric circuit branches with the least electric resistance.<ref>{{Cite journal|last=Khalafallah Ahmed|last2=Abdel-Raheem Mohamed|date=2011-05-01|title=Electimize: New Evolutionary Algorithm for Optimization with Application in Construction Engineering|url=https://ascelibrary.org/doi/full/10.1061/(ASCE)CP.1943-5487.0000080|journal=Journal of Computing in Civil Engineering|volume=25|issue=3|pages=192–201|doi=10.1061/(ASCE)CP.1943-5487.0000080}}</ref> \n*[[Particle swarm optimization]] – Based on the ideas of animal flocking behaviour. Also primarily suited for [[numerical optimization]] problems.\n\n==Other population-based [[metaheuristic]] methods==\n*[[Hunting Search]] – A method inspired by the group hunting of some animals such as wolves that organize their position to surround the prey, each of them relative to the position of the others and especially that of their leader. It is a continuous optimization method<ref>R. Oftadeh et al. (2010), [https://www.sciencedirect.com/science/article/pii/S0898122110005419 \"A novel meta-heuristic optimization algorithm inspired by group hunting of animals: Hunting search\"], 60, 2087–2098.</ref> adapted as a combinatorial optimization method.<ref>{{cite journal |author1=Amine Agharghor |author2=Mohammed Essaid Riffi |year=2017 |title=First Adaptation of Hunting Search Algorithm for the Quadratic Assignment Problem |journal=Europe and MENA Cooperation Advances in Information and Communication Technologies |pages=263–267 |doi=10.1007/978-3-319-46568-5_27 |isbn=978-3-319-46567-8}}</ref>\n*[[Adaptive dimensional search]] – Unlike nature-inspired metaheuristic techniques, an adaptive dimensional search algorithm does not implement any metaphor as an underlying principle. Rather it uses a simple performance-oriented method, based on the update of the search dimensionality ratio (SDR) parameter at each iteration.<ref>Hasançebi, O., Kazemzadeh Azad, S. (2015), \"Adaptive Dimensional Search: A New Metaheuristic Algorithm for Discrete Truss Sizing Optimization\", ''Computers and Structures'', 154, 1–16.</ref>\n*[[Firefly algorithm]] is inspired by the behavior of fireflies, attracting each other by flashing light. This is especially useful for multimodal optimization.\n*[[Harmony search]] – Based on the ideas of musicians' behavior in searching for better harmonies. This algorithm is suitable for combinatorial optimization as well as parameter optimization.\n*[[Gaussian adaptation]] – Based on information theory. Used for maximization of manufacturing yield, [[mean fitness]] or [[average information]]. See for instance [[Entropy in thermodynamics and information theory]].\n*[[Memetic algorithm]] – A hybrid method, inspired by [[Richard Dawkins]]'s notion of a meme, it commonly takes the form of a population-based algorithm coupled with individual learning procedures capable of performing local refinements. Emphasizes the exploitation of problem-specific knowledge, and tries to orchestrate local and global search in a synergistic way.\n*Emperor Penguins Colony – A method inspired by the behavior of emperor penguins in their colony. The emperor penguins in the colony seek to create the appropriate heat and regulate their body temperature, and this heat is completely coordinated and controlled by the movement of the penguins.<ref>{{Cite journal|last=Harifi|first=Sasan|last2=Khalilian|first2=Madjid|last3=Mohammadzadeh|first3=Javad|last4=Ebrahimnejad|first4=Sadoullah|date=2019-02-25|title=Emperor Penguins Colony: a new metaheuristic algorithm for optimization|url=https://doi.org/10.1007/s12065-019-00212-x|journal=Evolutionary Intelligence|language=en|doi=10.1007/s12065-019-00212-x|issn=1864-5917}}</ref>\n\n==Examples==\n\nThe computer simulations ''[[Tierra (computer simulation)|Tierra]]'' and ''[[Avida]]'' attempt to model [[macroevolution]]ary dynamics.\n\n==Gallery ==\n<ref>{{Cite journal |last1=Simionescu |first1=P.A. |last2=Beale |first2=D.G. |last3=Dozier |first3=G.V. |title=Constrained optimization problem solving using estimation of distribution algorithms |series=Proc. of the 2004 Congress on Evolutionary Computation - CEC2004|place=Portland, OR |pages=1647–1653 |year=2004 |doi=10.1109/CEC.2006.1688506 |url=http://faculty.tamucc.edu/psimionescu/PDFs/WCCI2004-Paper1361.pdf |access-date=7 January 2017}}</ref>\n<ref>{{Cite journal |last1=Simionescu |first1=P.A. |last2=Dozier |first2=G.V. |last3=Wainwright |first3=R.L. |title=A Two-Population Evolutionary Algorithm for Constrained Optimization Problems |series=Proc 2006 IEEE International Conference on Evolutionary Computation|place=Vancouver, Canada |pages=1647–1653 |year=2006 |doi=10.1109/CEC.2006.1688506 |url=http://faculty.tamucc.edu/psimionescu/PDFs/WCCI2006-Paper7204(1).pdf |access-date=7 January 2017}}</ref>\n<ref>{{cite book|last=Simionescu|first=P.A.|title=Computer Aided Graphing and Simulation Tools for AutoCAD Users|year=2014|publisher=[[CRC Press]]|location=Boca Raton, FL|isbn=978-1-4822-5290-3|edition=1st}}</ref>\n\n<gallery>\nFile:Two-population EA search (2).gif|A two-population EA search over a constrained [[Rosenbrock function]] with bounded global optimum.\nFile:Two-population EA search (3).gif|A two-population EA search over a constrained [[Rosenbrock function]]. Global optimum is not bounded.\nFile:Estimation of Distribution Algorithm animation.gif|[[Estimation of distribution algorithm]] over [[Keane's function]]\nFile:Two population EA animation.gif|A two-population EA search of a bounded optima of [[Test_functions_for_optimization#Test_functions_for_constrained_optimization|Simionescu's function]]. \n</gallery>\n\n==References==\n<references/>\n\n==Bibliography==\n* Ashlock, D. (2006), ''Evolutionary Computation for Modeling and Optimization'', Springer, {{ISBN|0-387-22196-4}}.\n* Bäck, T. (1996), ''[https://books.google.com/books?id=htJHI1UrL7IC&printsec=frontcover#v=onepage&q&f=false Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms]'', Oxford Univ. Press.\n* Bäck, T., Fogel, D., Michalewicz, Z. (1997), ''Handbook of Evolutionary Computation'', Oxford Univ. Press.\n* Banzhaf, W., Nordin, P., Keller, R., Francone, F. (1998), ''Genetic Programming - An Introduction'', Morgan Kaufmann, San Francisco\n* Eiben, A.E., Smith, J.E. (2003), ''Introduction to Evolutionary Computing'', Springer.\n* Holland, J. H. (1992), ''[https://books.google.com/books?id=5EgGaBkwvWcC&printsec=frontcover#v=onepage&q&f=false Adaptation in Natural and Artificial Systems]'', The University of Michigan Press, Ann Arbor\n* Michalewicz Z., Fogel D.B. (2004). How To Solve It: Modern Heuristics, Springer.\n* Benkő A., Dósa G., Tuza Z. (2010), ''Bin Packing/Covering with Delivery, Solved with the Evolution of Algorithms'', Proc. 2010 IEEE 5th International Conference on Bio-Inspired Computing: Theories and Applications, BIC-TA 2010, pp.&nbsp;298–302.\n* {{cite book |author1=Poli, R. |author2=Langdon, W. B. |author3=McPhee, N. F. |year=2008 |title=A Field Guide to Genetic Programming | publisher=Lulu.com, freely available from the internet | url= http://cswww.essex.ac.uk/staff/rpoli/gp-field-guide/ | isbn = 978-1-4092-0073-4}}\n* Price, K., Storn, R.M., Lampinen, J.A., (2005). [https://books.google.com/books?id=hakXI-dEhTkC&printsec=frontcover#v=onepage&q&f=false \"Differential Evolution: A Practical Approach to Global Optimization\"], Springer.\n* [[Ingo Rechenberg]] (1971): Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution (PhD thesis). Reprinted by Fromman-Holzboog (1973).\n* Hans-Paul Schwefel (1974): Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977).\n* Simon, D. (2013): [http://academic.csuohio.edu/simond/EvolutionaryOptimization Evolutionary Optimization Algorithms], Wiley.\n* ''[https://books.google.com/books?id=yQVGAAAAQBAJ&printsec=frontcover#v=onepage&q&f=false Computational Intelligence: A Methodological Introduction]'' by Kruse, Borgelt, Klawonn, Moewes, Steinbrecher, Held, 2013, Springer, {{ISBN|978-1-4471-5012-1}}\n* Rosshairy Abd Rahman, Graham Kendall,  Razamin Ramli,  Zainoddin Jamari & Ku Ruhana Ku-Mahamud. (2017). Shrimp Feed Formulation via Evolutionary Algorithm with Power Heuristics for Handling Constraints, Vol 2017, 1-12.  https://www.hindawi.com/journals/complexity/2017/7053710/\n\n{{DEFAULTSORT:Evolutionary Algorithm}}\n[[Category:Cybernetics]]\n[[Category:Evolution]]\n[[Category:Evolutionary algorithms| ]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Evolutionary programming",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_programming",
      "text": "'''Evolutionary programming''' is one of the four major [[evolutionary algorithm]] paradigms.  It is similar to [[genetic programming]], but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.\n\nIt was first used by [[Lawrence J. Fogel]] in the US in 1960 in order to use simulated [[evolution]] as a learning process aiming to generate [[artificial intelligence]]. Fogel used [[finite-state machine]]s as predictors and evolved them.\nCurrently evolutionary programming is a wide [[evolutionary computing]] dialect with no fixed structure or ([[Genetic representation|representation]]), in contrast with some of the other dialects. It is becoming harder to distinguish from [[Evolution strategy|evolutionary strategies]].\n\nIts main variation operator is [[Mutation (genetic algorithm)|mutation]]; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ){{Explain|date=April 2017}} [[Selection (genetic algorithm)|survivor selection]].\n\n==See also==\n* [[Artificial intelligence]]\n* [[Genetic algorithm]]\n* [[Genetic operator]]\n\n==References==\n* Fogel, L.J., Owens, A.J., Walsh, M.J. (1966), ''Artificial Intelligence through Simulated Evolution'', John Wiley.\n* Fogel, L.J. (1999), ''Intelligence through Simulated Evolution : Forty Years of Evolutionary Programming'', John Wiley.\n* Eiben, A.E., Smith, J.E. (2003), [http://www.cs.vu.nl/~gusz/ecbook/ecbook.html ''Introduction to Evolutionary Computing''], [http://www.springer.de Springer]. {{ISBN|3-540-40184-9}}\n\n==External links==\n* [http://www.aip.de/~ast/EvolCompFAQ/Q1_2.htm The Hitch-Hiker's Guide to Evolutionary Computation: What's Evolutionary Programming (EP)?]\n* [http://www.cleveralgorithms.com/nature-inspired/evolution/evolutionary_programming.html Evolutionary Programming by Jason Brownlee (PhD)]\n\n{{Evolutionary computation}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Evolutionary Programming}}\n[[Category:Evolutionary algorithms]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Machine learning]]\n\n{{compu-sci-stub}}\n\n[[de:Evolutionäre Programmierung]]"
    },
    {
      "title": "Exact algorithm",
      "url": "https://en.wikipedia.org/wiki/Exact_algorithm",
      "text": "In [[computer science]] and [[operations research]], '''exact algorithms''' are [[algorithm]]s that always solve an optimization problem to optimality.\n\nUnless [[P = NP]], an exact algorithm for an [[NP-hardness | NP-hard]] optimization problem cannot run in worst-case [[polynomial time]]. There has been extensive research on finding exact algorithms whose running time is exponential with a low base.<ref>{{citation\n | last1 = Fomin | first1 = Fedor V.\n | last2 = Kaski | first2 = Petteri\n | date = March 2013\n | doi = 10.1145/2428556.2428575\n | issue = 3\n | journal = [[Communications of the ACM]]\n | pages = 80–88\n | title = Exact Exponential Algorithms\n | url = http://cacm.acm.org/magazines/2013/3/161189-exact-exponential-algorithms/fulltext\n | volume = 56}}.</ref>\n<ref>{{cite book\n|last1=Fomin|first1=Fedor V.\n|last2=Kratsch|first2=Dieter\n|title=Exact Exponential Algorithms\n|publisher=Springer\n|year=2010\n|isbn=978-3-642-16532-0\n|page=203\n}}</ref>\n\n== See also ==\n* [[Approximation-preserving reduction]]\n* [[APX]] is the class of problems with some constant-factor approximation algorithm\n* [[Heuristic algorithm]]\n* [[Polynomial-time approximation scheme|PTAS]] - a type of approximation algorithm that takes the approximation ratio as a parameter\n\n==References==\n{{reflist}}\n\n[[Category:Computational complexity theory]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Expectation–maximization algorithm",
      "url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm",
      "text": "{{Machine learning bar}}\n\nIn [[statistics]], an '''expectation–maximization''' ('''EM''') '''algorithm''' is an [[iterative method]] to find [[maximum likelihood]] or [[maximum a posteriori]] (MAP) estimates of [[parameter]]s in [[statistical model]]s, where the model depends on unobserved [[latent variable]]s. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the [[Likelihood function#Log-likelihood|log-likelihood]] evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the ''E'' step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\n\n[[File:EM Clustering of Old Faithful data.gif|right|frame|EM clustering of [[Old Faithful]] eruption data. The random initial model (which, due to the different scales of the axes, appears to be two very flat and wide spheres) is fit to the observed data. In the first iterations, the model changes substantially, but then converges to the two modes of the [[geyser]]. Visualized using [[ELKI]].]]\n\n==History==\nThe EM algorithm was explained and given its name in a classic 1977 paper by [[Arthur P. Dempster|Arthur Dempster]], [[Nan Laird]], and [[Donald Rubin]].<ref name=\"Dempster1977\">\n{{cite journal\n |last1=Dempster  |first1= A.P. |author-link1=Arthur P. Dempster\n |last2=Laird |first2=N.M. |author-link2=Nan Laird |last3=Rubin\n |first3=D.B. |author-link3=Donald Rubin\n |title=Maximum Likelihood from Incomplete Data via the EM Algorithm\n |journal=[[Journal of the Royal Statistical Society, Series B]]\n |year=1977 |volume=39 |issue=1 |pages=1–38\n |jstor=2984875 |mr= 0501537\n}}\n</ref> They pointed out that the method had been \"proposed many times in special circumstances\" by earlier authors. One of the earliest is the gene-counting method for estimating allele frequencies by Cedric Smith.<ref>{{cite journal |last1=Ceppelini |first1=R.M. |title=The estimation of gene frequencies in a random-mating population |journal=Ann. Hum. Genet. |volume=20 |issue=2 |pages=97–115 |doi=10.1111/j.1469-1809.1955.tb01360.x|year=1955 }}</ref>  A very detailed treatment of the EM method for exponential families was published by Rolf Sundberg in his thesis and several papers<ref name=\"Sundberg1974\">{{cite journal\n |last=Sundberg |first=Rolf\n |title=Maximum likelihood theory for incomplete data from an exponential family\n |journal=Scandinavian Journal of Statistics\n |volume=1 |year=1974 |issue=2 |pages=49–58\n |jstor=4615553 |mr= 381110\n}}</ref><ref name=\"Sundberg1971\">\nRolf Sundberg. 1971. ''Maximum likelihood theory and applications for distributions generated when observing a function of an exponential family variable''. Dissertation, Institute for Mathematical Statistics, Stockholm University.</ref><ref name=\"Sundberg1976\">{{cite journal\n |last=Sundberg |first=Rolf\n |year=1976\n |title=An iterative method for solution of the likelihood equations for incomplete data from exponential families\n |journal=Communications in Statistics – Simulation and Computation\n |volume=5 |issue=1 |pages=55–64\n |doi=10.1080/03610917608812007 |mr=443190\n}}</ref> following his collaboration with [[Per Martin-Löf]] and [[Anders Martin-Löf]].<ref>See the acknowledgement by Dempster, Laird and Rubin on pages 3, 5 and 11.</ref><ref>G. Kulldorff. 1961.'' Contributions to the theory of estimation from grouped and partially grouped samples''. Almqvist & Wiksell.</ref><ref name=\"Martin-Löf1963\">Anders Martin-Löf. 1963. \"Utvärdering av livslängder i subnanosekundsområdet\" (\"Evaluation of sub-nanosecond lifetimes\"). (\"Sundberg formula\")</ref><ref name=\"Martin-Löf1966\">[[Per Martin-Löf]]. 1966. ''Statistics from the point of view of statistical mechanics''. Lecture notes, Mathematical Institute, Aarhus University. (\"Sundberg formula\" credited to Anders Martin-Löf).</ref><ref name=\"Martin-Löf1970\">[[Per Martin-Löf]]. 1970. ''Statistika Modeller (Statistical Models): Anteckningar från seminarier läsåret 1969–1970 (Notes from seminars in the academic year 1969-1970), with the assistance of Rolf Sundberg.'' Stockholm University. (\"Sundberg formula\")</ref><!-- * Martin-Löf, P. \"Exact tests, confidence regions and estimates\", with a discussion by [[A. W. F. Edwards]], [[George A. Barnard|G. A. Barnard]], D. A. Sprott, O. Barndorff-Nielsen, [[D. Basu]] and [[Rasch model|G. Rasch]]. ''Proceedings of Conference on Foundational Questions in Statistical Inference'' (Aarhus, 1973), pp. 121–138. Memoirs, No. 1, Dept. Theoret. Statist., Inst. Math., Univ. Aarhus, Aarhus, 1974. --><ref name=\"Martin-Löf1974a\">Martin-Löf, P. The notion of redundancy and its use as a quantitative measure of the deviation between a statistical hypothesis and a set of observational data. With a discussion by F. Abildgård, [[Arthur P. Dempster|A. P. Dempster]], [[D. Basu]], [[D. R. Cox]], [[A. W. F. Edwards]], D. A. Sprott, [[George A. Barnard|G. A. Barnard]], O. Barndorff-Nielsen, J. D. Kalbfleisch and [[Rasch model|G. Rasch]] and a reply by the author. ''Proceedings of Conference on Foundational Questions in Statistical Inference'' (Aarhus, 1973), pp. 1–42. Memoirs, No. 1, Dept. Theoret. Statist., Inst. Math., Univ. Aarhus, Aarhus, 1974.</ref><ref name=\"Martin-Löf1974b\">{{cite journal |last=Martin-Löf |first=Per |title=The notion of redundancy and its use as a quantitative measure of the discrepancy between a statistical hypothesis and a set of observational data |journal=Scand. J. Statist. |volume=1 |year=1974 |issue=1 |pages=3–18 |doi= }}</ref> The Dempster–Laird–Rubin paper in 1977 generalized the method and sketched a convergence analysis for a wider class of problems. Regardless of earlier inventions, the innovative Dempster–Laird–Rubin paper in the ''Journal of the Royal Statistical Society'' received an enthusiastic discussion at the Royal Statistical Society meeting with Sundberg calling the paper \"brilliant\". The Dempster–Laird–Rubin paper established the EM method as an important tool of statistical analysis.\n\nThe convergence analysis of the Dempster–Laird–Rubin algorithm was flawed and a correct convergence analysis was published by [[C. F. Jeff Wu]] in 1983.<ref name=\"Wu\">\n{{cite journal\n|first=C. F. Jeff\n|last=Wu\n|title=On the Convergence Properties of the EM Algorithm\n|journal=[[Annals of Statistics]]\n|volume=11\n|issue=1\n|date=Mar 1983\n|pages=95–103\n|jstor=2240463\n|doi=10.1214/aos/1176346060\n|mr= 684867\n}}</ref>\nWu's proof established the EM method's convergence outside of the [[exponential family]], as claimed by Dempster–Laird–Rubin.<ref name=\"Wu\" />\n\n==Introduction==\nThe EM algorithm is used to find (local) [[maximum likelihood]] parameters of a [[statistical model]] in cases where the equations cannot be solved directly.  Typically these models involve [[latent variable]]s in addition to unknown [[parameters]] and known data observations.  That is, either [[missing values]] exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a [[mixture model]] can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.\n\nFinding a maximum likelihood solution typically requires taking the [[derivative]]s of the [[likelihood function]] with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. In statistical models with latent variables, this is usually impossible. Instead, the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa, but substituting one set of equations into the other produces an unsolvable equation.\n\nThe EM algorithm proceeds from the observation that there is a way to solve these two sets of equations numerically. One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points.  It's not obvious that this will work, but it can be proven that in this context it does, and that the derivative of the likelihood is (arbitrarily close to) zero at that point, which in turn means that the point is either a maximum or a [[saddle point]].<ref name=\"Wu\" /> In general, multiple maxima may occur, with no guarantee that the global maximum will be found.  Some likelihoods also have [[Mathematical singularity|singularities]] in them, i.e., nonsensical maxima.  For example, one of the ''solutions'' that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points.\n\n==Description==\nGiven the [[statistical model]] which generates a set <math>\\mathbf{X}</math> of observed data, a set of unobserved latent data or [[missing values]] <math>\\mathbf{Z}</math>, and a vector of unknown parameters <math>\\boldsymbol\\theta</math>, along with a [[likelihood function]] <math>L(\\boldsymbol\\theta; \\mathbf{X}, \\mathbf{Z}) = p(\\mathbf{X}, \\mathbf{Z}\\mid\\boldsymbol\\theta)</math>, the [[maximum likelihood estimate]] (MLE) of the unknown parameters is determined by maximizing the [[marginal likelihood]] of the observed data\n\n:<math>L(\\boldsymbol\\theta; \\mathbf{X}) = p(\\mathbf{X}\\mid\\boldsymbol\\theta) = \\int  p(\\mathbf{X},\\mathbf{Z} \\mid \\boldsymbol\\theta) \\, d\\mathbf{Z} </math>\n\nHowever, this quantity is often intractable (e.g. if <math>\\mathbf{Z}</math> is a sequence of events, so that the number of values grows exponentially with the sequence length, the exact calculation of the sum will be extremely difficult).\n\nThe EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying these two steps:\n:''Expectation step (E step)'': Define <math>Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)})</math> as the [[expected value]] of the log [[likelihood function]] of <math>\\boldsymbol\\theta</math>, with respect to the current [[conditional probability distribution|conditional distribution]] of <math>\\mathbf{Z}</math> given <math>\\mathbf{X}</math> and the current estimates of the parameters <math>\\boldsymbol\\theta^{(t)}</math>:\n::<math>Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) = \\operatorname{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\boldsymbol\\theta^{(t)}}\\left[ \\log L (\\boldsymbol\\theta; \\mathbf{X},\\mathbf{Z})  \\right] \\,</math>\n\n:''Maximization step (M step)'': Find the parameters that maximize this quantity:\n::<math>\\boldsymbol\\theta^{(t+1)} = \\underset{\\boldsymbol\\theta}{\\operatorname{arg\\,max}} \\ Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) \\, </math>\n\nThe typical models to which EM is applied use <math>\\mathbf{Z}</math> as a latent variable indicating membership in one of a set of groups:\n#The observed data points <math>\\mathbf{X}</math> may be [[discrete random variable|discrete]] (taking values in a finite or countably infinite set) or [[continuous random variable|continuous]] (taking values in an uncountably infinite set). Associated with each data point may be a vector of observations.\n#The [[missing values]] (aka [[latent variables]]) <math>\\mathbf{Z}</math> are [[discrete random variable|discrete]], drawn from a fixed number of values, and with one latent variable per observed unit. \n#The parameters are continuous, and are of two kinds: Parameters that are associated with all data points, and those associated with a specific value of a latent variable (i.e., associated with all data points which corresponding latent variable has that value).\nHowever, it is possible to apply EM to other sorts of models.\n\nThe motive is as follows.  If the value of the parameters <math>\\boldsymbol\\theta</math> is known, usually the value of the latent variables <math>\\mathbf{Z}</math> can be found by maximizing the log-likelihood over all possible values of <math>\\mathbf{Z}</math>, either simply by iterating over <math>\\mathbf{Z}</math> or through an algorithm such as the [[Baum–Welch algorithm]] for [[hidden Markov model]]s.  Conversely, if we know the value of the latent variables <math>\\mathbf{Z}</math>, we can find an estimate of the parameters <math>\\boldsymbol\\theta</math> fairly easily, typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values, or some function of the values, of the points in each group.  This suggests an iterative algorithm, in the case where both <math>\\boldsymbol\\theta</math> and <math>\\mathbf{Z}</math> are unknown:\n#First, initialize the parameters <math>\\boldsymbol\\theta</math> to some random values.\n#Compute the probability of each possible value of <math>\\mathbf{Z}</math> , given <math>\\boldsymbol\\theta</math>.\n#Then, use the just-computed values of <math>\\mathbf{Z}</math> to compute a better estimate for the parameters <math>\\boldsymbol\\theta</math>.\n#Iterate steps 2 and 3 until convergence.\nThe algorithm as just described monotonically approaches a local minimum of the cost function.\n\n== Properties ==\nSpeaking of an expectation (E) step is a bit of a [[misnomer]]. What are calculated in the first step are the fixed, data-dependent parameters of the function ''Q''. Once the parameters of ''Q'' are known, it is fully determined and is maximized in the second (M) step of an EM algorithm.\n\nAlthough an EM iteration does increase the observed data (i.e., marginal) likelihood function, no guarantee exists that the sequence converges to a [[maximum likelihood estimator]]. For [[bimodal distribution|multimodal distributions]], this means that an EM algorithm may converge to a [[local maximum]] of the observed data likelihood function, depending on starting values. A variety of heuristic or [[metaheuristic]] approaches exist to escape a local maximum, such as random-restart [[hill climbing]] (starting with several different random initial estimates ''θ''<sup>(''t'')</sup>), or applying [[simulated annealing]] methods.\n\nEM is especially useful when the likelihood is an [[exponential family]]: the E step becomes the sum of expectations of [[sufficient statistic]]s, and the M step involves maximizing a linear function. In such a case, it is usually possible to derive [[closed-form expression]] updates for each step, using the Sundberg formula (published by Rolf Sundberg using unpublished results of [[Per Martin-Löf]] and [[Anders Martin-Löf]]).<ref name=\"Sundberg1971\"/><ref name=\"Sundberg1976\"/><ref name=\"Martin-Löf1963\"/><ref name=\"Martin-Löf1966\"/><ref name=\"Martin-Löf1970\"/><ref name=\"Martin-Löf1974a\"/><ref name=\"Martin-Löf1974b\"/>\n\nThe EM method was modified to compute [[maximum a posteriori]] (MAP) estimates for [[Bayesian inference]] in the original paper by Dempster, Laird, and Rubin.\n\nOther methods exist to find maximum likelihood estimates, such as [[gradient descent]], [[conjugate gradient]], or variants of the [[Gauss–Newton algorithm]]. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.\n\n== Proof of correctness ==\nExpectation-maximization works to improve <math>Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)})</math> rather than directly improving <math>\\log p(\\mathbf{X}\\mid\\boldsymbol\\theta)</math>.  Here is shown that improvements to the former imply improvements to the latter.<ref name=\"Little1987\">{{cite book |last1=Little |first1= Roderick J.A. |author1-link= |last2= Rubin |first2= Donald B. |author2-link= Donald Rubin |title= Statistical Analysis with Missing Data | series = Wiley Series in Probability and Mathematical Statistics |year= 1987 |publisher= John Wiley & Sons |location= New York |isbn= 978-0-471-80254-9 |pages= 134–136}}</ref>\n\nFor any <math>\\mathbf{Z}</math> with non-zero probability <math>p(\\mathbf{Z}\\mid\\mathbf{X},\\boldsymbol\\theta)</math>, we can write\n::<math>\n\\log p(\\mathbf{X}\\mid\\boldsymbol\\theta) = \\log p(\\mathbf{X},\\mathbf{Z}\\mid\\boldsymbol\\theta) - \\log p(\\mathbf{Z}\\mid\\mathbf{X},\\boldsymbol\\theta) \\,.\n</math>\nWe take the expectation over possible values of the unknown data <math>\\mathbf{Z}</math> under the current parameter estimate <math>\\theta^{(t)}</math> by multiplying both sides by <math>p(\\mathbf{Z}\\mid\\mathbf{X},\\boldsymbol\\theta^{(t)})</math> and summing (or integrating) over <math>\\mathbf{Z}</math>.  The left-hand side is the expectation of a constant, so we get:\n::<math>\n\\begin{align}\n\\log p(\\mathbf{X}\\mid\\boldsymbol\\theta) &\n= \\sum_{\\mathbf{Z}} p(\\mathbf{Z}\\mid\\mathbf{X},\\boldsymbol\\theta^{(t)}) \\log p(\\mathbf{X},\\mathbf{Z}\\mid\\boldsymbol\\theta)\n- \\sum_{\\mathbf{Z}} p(\\mathbf{Z}\\mid\\mathbf{X},\\boldsymbol\\theta^{(t)}) \\log p(\\mathbf{Z}\\mid\\mathbf{X},\\boldsymbol\\theta) \\\\\n& = Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) + H(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) \\,,\n\\end{align}\n</math>\nwhere <math>H(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)})</math> is defined by the negated sum it is replacing.\nThis last equation holds for every value of <math>\\boldsymbol\\theta</math> including <math>\\boldsymbol\\theta = \\boldsymbol\\theta^{(t)}</math>,\n::<math>\n\\log p(\\mathbf{X}\\mid\\boldsymbol\\theta^{(t)})\n= Q(\\boldsymbol\\theta^{(t)}\\mid\\boldsymbol\\theta^{(t)}) + H(\\boldsymbol\\theta^{(t)}\\mid\\boldsymbol\\theta^{(t)}) \\,,\n</math>\nand subtracting this last equation from the previous equation gives\n::<math>\n\\log p(\\mathbf{X}\\mid\\boldsymbol\\theta) - \\log p(\\mathbf{X}\\mid\\boldsymbol\\theta^{(t)})\n= Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) - Q(\\boldsymbol\\theta^{(t)}\\mid\\boldsymbol\\theta^{(t)})\n + H(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) - H(\\boldsymbol\\theta^{(t)}\\mid\\boldsymbol\\theta^{(t)}) \\,,\n</math>\nHowever, [[Gibbs' inequality]] tells us that <math>H(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) \\ge H(\\boldsymbol\\theta^{(t)}\\mid\\boldsymbol\\theta^{(t)})</math>, so we can conclude that\n::<math>\n\\log p(\\mathbf{X}\\mid\\boldsymbol\\theta) - \\log p(\\mathbf{X}\\mid\\boldsymbol\\theta^{(t)})\n\\ge Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)}) - Q(\\boldsymbol\\theta^{(t)}\\mid\\boldsymbol\\theta^{(t)}) \\,.\n</math>\nIn words, choosing <math>\\boldsymbol\\theta</math> to improve <math>Q(\\boldsymbol\\theta\\mid\\boldsymbol\\theta^{(t)})</math> causes <math>\\log p(\\mathbf{X}\\mid\\boldsymbol\\theta)</math> to improve at least as much.\n\n== As a maximization–maximization procedure ==\nThe EM algorithm can be viewed as two alternating maximization steps, that is, as an example of [[coordinate descent|coordinate ascent]].<ref name=\"neal1999\">{{cite book|last1=Neal |first=Radford |last2=Hinton |first2=Geoffrey |author-link2=Geoffrey Hinton |title=A view of the EM algorithm that justifies incremental, sparse, and other variants |journal=Learning in Graphical Models |editor=[[Michael I. Jordan]] |pages= 355–368 |publisher= MIT Press |location=Cambridge, MA |year=1999 |isbn=978-0-262-60032-3 |url=ftp://ftp.cs.toronto.edu/pub/radford/emk.pdf |accessdate=2009-03-22}}</ref><ref name=\"hastie2001\">{{cite book|last1=Hastie |first1= Trevor|author-link1=Trevor Hastie|last2=Tibshirani|first2=Robert|author-link2=Robert Tibshirani|last3=Friedman|first3=Jerome |year=2001 |title=The Elements of Statistical Learning |isbn=978-0-387-95284-0 |publisher=Springer |location=New York |chapter=8.5 The EM algorithm |pages=236–243}}</ref> Consider the function:\n:<math> F(q,\\theta) := \\operatorname{E}_q [ \\log L (\\theta ; x,Z) ] + H(q), </math> \nwhere ''q'' is an arbitrary probability distribution over the unobserved data ''z'' and ''H(q)'' is the [[Entropy (information theory)|entropy]] of the distribution ''q''. This function can be written as\n:<math> F(q,\\theta) = -D_{\\mathrm{KL}}\\big(q \\parallel p_{Z\\mid X}(\\cdot\\mid x;\\theta ) \\big) + \\log L(\\theta;x), </math>\nwhere  <math>p_{Z\\mid X}(\\cdot\\mid x;\\theta )</math> is the conditional distribution of the unobserved data given the observed data <math>x</math> and <math>D_{KL}</math> is the [[Kullback–Leibler divergence]].\n\nThen the steps in the EM algorithm may be viewed as:\n:''Expectation step'': Choose <math>q</math> to maximize <math>F</math>:\n::<math> q^{(t)} = \\operatorname{arg\\,max}_q \\ F(q,\\theta^{(t)}) </math>\n:''Maximization step'': Choose <math>\\theta</math> to maximize <math>F</math>:\n::<math> \\theta^{(t+1)} = \\operatorname{arg\\,max}_\\theta \\ F(q^{(t)},\\theta) </math>\n\n== Applications ==\nEM is frequently used for [[data clustering]] in [[machine learning]] and [[computer vision]]. In [[natural language processing]], two prominent instances of the algorithm are the [[Baum–Welch algorithm]] for [[hidden Markov models]], and the [[inside-outside algorithm]] for unsupervised induction of [[probabilistic context-free grammar]]s.\n\nEM is frequently used for parameter estimation of [[mixed model]]s,<ref>{{cite journal |doi=10.1080/01621459.1988.10478693 |title=Newton—Raphson and EM Algorithms for Linear Mixed-Effects Models for Repeated-Measures Data |journal=Journal of the American Statistical Association |volume=83 |issue=404 |pages=1014 |year=1988 |last1=Lindstrom |first1=Mary J |last2=Bates |first2=Douglas M }}</ref><ref>{{cite journal |doi=10.2307/1390614 |jstor=1390614 |title=Fitting Mixed-Effects Models Using Efficient EM-Type Algorithms |journal=Journal of Computational and Graphical Statistics |volume=9 |issue=1 |pages=78–98 |year=2000 |last1=Van Dyk |first1=David A }}</ref> notably in [[quantitative genetics]].<ref>{{cite journal |doi=10.1111/anzs.12208 |title=A new REML (parameter expanded) EM algorithm for linear mixed models |journal=Australian & New Zealand Journal of Statistics |volume=59 |issue=4 |pages=433 |year=2017 |last1=Diffey |first1=S. M |last2=Smith |first2=A. B |last3=Welsh |first3=A. H |last4=Cullis |first4=B. R }}</ref>\n\nIn [[psychometrics]], EM is almost indispensable for estimating item parameters and latent abilities of [[item response theory]] models.\n\nWith the ability to deal with missing data and observe unidentified variables, EM is becoming a useful tool to price and manage risk of a portfolio.{{Citation needed|date=November 2017}}\n\nThe EM algorithm (and its faster variant [[ordered subset expectation maximization]]) is also widely used in [[medical imaging|medical image]] reconstruction, especially in [[positron emission tomography]] and [[single photon emission computed tomography]]. See below for other faster variants of EM.\n\nIn [[structural engineering]], the Structural Identification using Expectation Maximization (STRIDE)  <ref>Matarazzo, T. J., and Pakzad, S. N. (2016). “STRIDE for Structural Identification using Expectation Maximization: Iterative Output-Only Method for Modal Identification.” Journal of Engineering Mechanics.http://ascelibrary.org/doi/abs/10.1061/(ASCE)EM.1943-7889.0000951</ref> algorithm is an output-only method for identifying natural vibration properties of a structural system using sensor data (see [[Operational Modal Analysis]]).\n\n== Filtering and smoothing EM algorithms ==\nA [[Kalman filter]] is typically used for on-line state estimation and a minimum-variance smoother may be employed for off-line or batch state estimation. However, these minimum-variance solutions require estimates of the state-space model parameters. EM algorithms can be used for solving joint state and parameter estimation problems.\n\nFiltering and smoothing EM algorithms arise by repeating this two-step procedure:\n\n;E-step\n: Operate a Kalman filter or a minimum-variance smoother designed with current parameter estimates to obtain updated state estimates.\n\n;M-step\n: Use the filtered or smoothed state estimates within maximum-likelihood calculations to obtain updated parameter estimates.\n\nSuppose that a [[Kalman filter]] or minimum-variance smoother operates on measurements of a single-input-single-output system that possess additive white noise. An updated measurement noise variance estimate can be obtained from the [[maximum likelihood]] calculation\n:<math>\\widehat{\\sigma}^2_v = \\frac{1}{N} \\sum_{k=1}^N {(z_k-\\widehat{x}_k)}^2</math>\n\nwhere <math>\\widehat{x}_k</math> are scalar output estimates calculated by a filter or a smoother from N scalar measurements <math>z_k</math>. The above update can also be applied to updating a Poisson measurement noise intensity. Similarly, for a first-order auto-regressive process, an updated process noise variance estimate can be calculated by\n:<math>\\widehat{\\sigma}^2_w =   \\frac{1}{N} \\sum_{k=1}^N {(\\widehat{x}_{k+1}-\\widehat{F}\\widehat{{x}}_k)}^2</math>\n\nwhere <math>\\widehat{x}_k</math> and <math>\\widehat{x}_{k+1}</math> are scalar state estimates calculated by a filter or a smoother. The updated model coefficient estimate is obtained via\n:<math>\\widehat{F} = \\frac{\\sum_{k=1}^N (\\widehat{x}_{k+1}-\\widehat{F} \\widehat{x}_k)}{\\sum_{k=1}^N \\widehat{x}_k^2}. </math>\n\nThe convergence of parameter estimates such as those above are well studied.<ref>{{Cite journal\n|last = Einicke\n|first =  G.A.\n|last2 = Malos\n|first2 = J.T.\n|last3 = Reid\n|first3 =  D.C.\n|last4 = Hainsworth\n|first4 = D.W.\n|title = Riccati Equation and EM Algorithm Convergence for Inertial Navigation Alignment\n|journal = IEEE Trans. Signal Process.\n|volume = 57\n|issue = 1\n|pages = 370–375\n|date=January 2009\n|postscript = <!--None-->\n|doi = 10.1109/TSP.2008.2007090\n|bibcode = 2009ITSP...57..370E\n}}</ref><ref>{{Cite journal\n|last = Einicke\n|first =  G.A.\n|last2 = Falco\n|first2 =  G.\n|last3 = Malos\n|first3 = J.T.\n|title = EM Algorithm State Matrix Estimation for Navigation\n|journal = IEEE Signal Processing Letters\n|volume = 17\n|issue = 5\n|pages = 437–440\n|date=May 2010\n|postscript = <!--None-->\n|doi = 10.1109/LSP.2010.2043151\n|bibcode = 2010ISPL...17..437E }}</ref><ref>{{Cite journal\n|last = Einicke\n|first =  G.A.\n|last2 = Falco\n|first2 =  G.\n|last3 = Dunn\n|first3 = M.T.\n|last4 = Reid\n|first4 = D.C.\n|title = Iterative Smoother-Based Variance Estimation\n|journal = IEEE Signal Processing Letters\n|volume = 19\n|issue = 5\n|pages = 275–278\n|date=May 2012\n|postscript = <!--None-->\n|bibcode = 2012ISPL...19..275E\n|doi = 10.1109/LSP.2012.2190278\n}}</ref><ref>{{Cite journal\n|last = Einicke\n|first =  G.A.\n|title = Iterative Filtering and Smoothing of Measurements Possessing Poisson Noise\n|journal = IEEE Transactions on Aerospace and Electronic Systems\n|volume = 51\n|issue = 3\n|pages = 2205–2011\n|date=Sep 2015\n|postscript = <!--None-->\n|doi = 10.1109/TAES.2015.140843\n|bibcode = 2015ITAES..51.2205E\n}}</ref>\n\n== Variants ==\nA number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm, such as those using [[conjugate gradient]] and modified [[Newton's method]]s (Newton–Raphson).<ref>{{cite journal |first1= Mortaza |last1=Jamshidian |first2=Robert I. |last2=Jennrich|title=Acceleration of the EM Algorithm by using Quasi-Newton Methods |year=1997 |journal=[[Journal of the Royal Statistical Society, Series B]] |volume=59 |issue=2 |pages=569–587 |doi=10.1111/1467-9868.00083 |mr=1452026 }}</ref> Also, EM can be used with constrained estimation methods.\n\n''Parameter-expanded expectation maximization (PX-EM)'' algorithm often provides speed up by \"us[ing] a `covariance adjustment' to correct the analysis of the M step, capitalising on extra information captured in the imputed complete data\".<ref>{{cite journal |doi=10.1093/biomet/85.4.755 |title=Parameter expansion to accelerate EM: The PX-EM algorithm |journal=Biometrika |volume=85 |issue=4 |pages=755–770 |year=1998 |last1=Liu |first1=C |citeseerx=10.1.1.134.9617 }}</ref>\n\n''Expectation conditional maximization (ECM)'' replaces each M step with a sequence of conditional maximization (CM) steps in which each parameter ''θ''<sub>''i''</sub> is maximized individually, conditionally on the other parameters remaining fixed.<ref>{{cite journal|last1=Meng  |first1= Xiao-Li |last2=Rubin |first2=Donald B. |author-link2=Donald Rubin |title=Maximum likelihood estimation via the ECM algorithm: A general framework |year=1993 |journal=[[Biometrika]] |volume=80 |issue=2 |pages=267–278 |doi=10.1093/biomet/80.2.267 |mr=1243503}}</ref> Itself can be extended into the ''Expectation conditional maximization either (ECME)'' algorithm.<ref>{{cite journal |doi=10.2307/2337067 |jstor=2337067 |title=The ECME Algorithm: A Simple Extension of EM and ECM with Faster Monotone Convergence |journal=Biometrika |volume=81 |issue=4 |pages=633 |year=1994 |last1=Liu |first1=Chuanhai |last2=Rubin |first2=Donald B }}</ref>\n\nThis idea is further extended in ''generalized expectation maximization (GEM)'' algorithm, in which is sought only an increase in the objective function ''F'' for both the E step and M step as described in the [[#As a maximization-maximization procedure|As a maximization-maximization procedure]] section.<ref name=\"neal1999\"/> GEM is further developed in a distributed environment and shows promising results.<ref>\n{{cite journal\n |author1=Jiangtao Yin |author2=Yanfeng Zhang |author3=Lixin Gao |title= Accelerating Expectation-Maximization Algorithms with Frequent Updates\n |journal= Proceedings of the IEEE International Conference on Cluster Computing\n |year= 2012\n |url= http://rio.ecs.umass.edu/mnilpub/papers/cluster2012-yin.pdf\n}}\n</ref>\n\nIt is also possible to consider the EM algorithm as a subclass of the '''[[MM algorithm|MM]]''' (Majorize/Minimize or Minorize/Maximize, depending on context) algorithm,<ref>Hunter DR and Lange K (2004), [http://www.stat.psu.edu/~dhunter/papers/mmtutorial.pdf A Tutorial on MM Algorithms], The American Statistician, 58: 30-37</ref> and therefore use any machinery developed in the more general case.\n\n===α-EM algorithm===\nThe Q-function used in the EM algorithm is based on the log likelihood. Therefore, it is regarded as the log-EM algorithm. The use of the log likelihood can be generalized to that of the α-log likelihood ratio. Then, the α-log likelihood ratio of the observed data can be exactly expressed as equality by using the Q-function of the α-log likelihood ratio and the α-divergence. Obtaining this Q-function is a generalized E step. Its maximization is a generalized M step. This pair is called the α-EM algorithm\n<ref>\n{{cite journal\n|last=Matsuyama |first=Yasuo\n|title=The α-EM algorithm: Surrogate likelihood maximization using α-logarithmic information measures\n|journal=IEEE Transactions on Information Theory\n|volume=49 | year=2003 |pages=692–706 |issue=3\n|doi=10.1109/TIT.2002.808105\n}}\n</ref>\nwhich contains the log-EM algorithm as its subclass. Thus, the α-EM algorithm by [[Yasuo Matsuyama]] is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The α-EM shows faster convergence than the log-EM algorithm by choosing an appropriate α. The α-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm α-HMM.\n<ref>\n{{cite journal\n|last=Matsuyama |first=Yasuo\n|title=Hidden Markov model estimation based on alpha-EM algorithm: Discrete and continuous alpha-HMMs\n|journal=International Joint Conference on Neural Networks\n| year=2011 |pages=808–816\n}}\n</ref>\n\n== Relation to variational Bayes methods ==\nEM is a partially non-Bayesian, maximum likelihood method.  Its final result gives a [[probability distribution]] over the latent variables (in the Bayesian style) together with a point estimate for ''θ'' (either a [[maximum likelihood estimation|maximum likelihood estimate]] or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over ''θ'' and the latent variables.  The Bayesian approach to inference is simply to treat ''θ'' as another latent variable.  In this paradigm, the distinction between the E and M steps disappears.  If using the factorized Q approximation as described above ([[variational Bayes]]), solving can iterate over each latent variable (now including ''θ'') and optimize them one at a time.  Now, ''k'' steps per iteration are needed, where ''k'' is the number of latent variables.  For [[graphical models]] this is easy to do as each variable's new ''Q'' depends only on its [[Markov blanket]], so local [[Message passing (disambiguation)|message passing]] can be used for efficient inference.\n\n== Geometric interpretation ==\n{{details|Information geometry}}\nIn [[information geometry]], the E step and the M step are interpreted as projections under dual [[affine connection]]s, called the e-connection and the m-connection; the [[Kullback–Leibler divergence]] can also be understood in these terms.\n\n== Examples ==\n=== Gaussian mixture === <!--This section is linked from [[Matrix calculus]] -->\n\n[[File:ClusterAnalysis_Mouse.svg|thumb|400px|Comparison of [[K-means clustering|k-means]] and EM on artificial data  visualized with [[Environment for DeveLoping KDD-Applications Supported by Index-Structures|ELKI]]. Using the variances, the EM algorithm can describe the normal distributions exactly, while k-means splits the data in [[Voronoi diagram|Voronoi]]-cells. The cluster center is indicated by the lighter, bigger symbol.]]\n\n[[File:Em old faithful.gif|thumb|240px|An animation demonstrating the EM algorithm fitting a two component Gaussian [[mixture model]] to the [[Old Faithful]] dataset. The algorithm steps through from a random initialization to convergence. ]]\n\nLet <math>\\mathbf{x} = (\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n)</math> be a sample of <math>n</math> independent observations from a [[mixture model|mixture]] of two [[multivariate normal distribution]]s of dimension <math>d</math>, and let <math>\\mathbf{z} = (z_1,z_2,\\ldots,z_n)</math> be the latent variables that determine the component from which the observation originates.<ref name=\"hastie2001\"/>\n:<math>X_i \\mid(Z_i = 1) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_1,\\Sigma_1)</math> and <math>X_i \\mid(Z_i = 2) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_2,\\Sigma_2)</math>\nwhere\n:<math>\\operatorname{P} (Z_i = 1 ) = \\tau_1 \\, </math> and <math>\\operatorname{P} (Z_i=2) = \\tau_2 = 1-\\tau_1</math>\n\nThe aim is to estimate the unknown parameters representing the ''mixing'' value between the Gaussians and the means and covariances of each:\n:<math>\\theta = \\big( \\boldsymbol{\\tau},\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\Sigma_1,\\Sigma_2 \\big)</math>\nwhere the incomplete-data likelihood function is\n:<math>L(\\theta;\\mathbf{x}) =  \\prod_{i=1}^n \\sum_{j=1}^2 \\tau_j \\ f(\\mathbf{x}_i;\\boldsymbol{\\mu}_j,\\Sigma_j)</math>,\n\nand the complete-data likelihood function is\n:<math>L(\\theta;\\mathbf{x},\\mathbf{z}) = p(\\mathbf{x},\\mathbf{z} \\mid \\theta) = \\prod_{i=1}^n  \\prod_{j=1}^2  \\ [f(\\mathbf{x}_i;\\boldsymbol{\\mu}_j,\\Sigma_j) \\tau_j] ^{\\mathbb{I}(z_i=j)} </math>\n\nor\n\n:<math>L(\\theta;\\mathbf{x},\\mathbf{z}) = \\exp \\left\\{ \\sum_{i=1}^n \\sum_{j=1}^2 \\mathbb{I}(z_i=j) \\big[ \\log \\tau_j -\\tfrac{1}{2} \\log |\\Sigma_j| -\\tfrac{1}{2}(\\mathbf{x}_i-\\boldsymbol{\\mu}_j)^\\top\\Sigma_j^{-1} (\\mathbf{x}_i-\\boldsymbol{\\mu}_j) -\\tfrac{d}{2} \\log(2\\pi) \\big] \\right\\}. </math>\n\nwhere <math>\\mathbb{I}</math> is an [[indicator function]] and <math>f</math> is the [[probability density function]] of a multivariate normal.\n\nIn the last equality, for each {{math|''i''}}, one indicator <math>\\mathbb{I}(z_i=j)</math> is equal to zero, and one indicator is equal to one. The inner sum thus reduces to one term.\n\n==== E step ====\n\nGiven our current estimate of the parameters ''θ''<sup>(''t'')</sup>, the conditional distribution of the ''Z''<sub>''i''</sub> is determined by [[Bayes theorem]] to be the proportional height of the normal [[probability density function|density]] weighted by ''τ'':\n:<math>T_{j,i}^{(t)} := \\operatorname{P}(Z_i=j \\mid X_i=\\mathbf{x}_i ;\\theta^{(t)}) = \\frac{\\tau_j^{(t)} \\ f(\\mathbf{x}_i;\\boldsymbol{\\mu}_j^{(t)},\\Sigma_j^{(t)})}{\\tau_1^{(t)} \\ f(\\mathbf{x}_i;\\boldsymbol{\\mu}_1^{(t)},\\Sigma_1^{(t)}) + \\tau_2^{(t)} \\ f(\\mathbf{x}_i;\\boldsymbol{\\mu}_2^{(t)},\\Sigma_2^{(t)})} </math>.\n\nThese are called the \"membership probabilities\" which are normally considered the output of the E step (although this is not the Q function of below).\n\nThis E step corresponds with setting up this function for Q:\n:<math>\\begin{align}Q(\\theta\\mid\\theta^{(t)})\n&= \\operatorname{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\mathbf{\\theta}^{(t)}} [\\log L(\\theta;\\mathbf{x},\\mathbf{Z}) ] \\\\\n&= \\operatorname{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\mathbf{\\theta}^{(t)}} [\\log \\prod_{i=1}^{n}L(\\theta;\\mathbf{x}_i,\\mathbf{z}_i) ] \\\\\n&= \\operatorname{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\mathbf{\\theta}^{(t)}} [\\sum_{i=1}^n \\log L(\\theta;\\mathbf{x}_i,\\mathbf{z}_i) ] \\\\\n&= \\sum_{i=1}^n\\operatorname{E}_{\\mathbf{Z_i}\\mid\\mathbf{X};\\mathbf{\\theta}^{(t)}} [\\log L(\\theta;\\mathbf{x}_i,\\mathbf{z}_i) ] \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^2 P(Z_i =j \\mid X_i = \\mathbf{x}_i; \\theta^{(t)}) \\log L(\\theta_j;\\mathbf{x}_i,\\mathbf{z}_i) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^2 T_{j,i}^{(t)} \\big[ \\log \\tau_j  -\\tfrac{1}{2} \\log |\\Sigma_j| -\\tfrac{1}{2}(\\mathbf{x}_i-\\boldsymbol{\\mu}_j)^\\top\\Sigma_j^{-1} (\\mathbf{x}_i-\\boldsymbol{\\mu}_j) -\\tfrac{d}{2} \\log(2\\pi) \\big]\n\\end{align}</math>\nThe expectation of <math>\\log L(\\theta;\\mathbf{x}_i,\\mathbf{z}_i)</math> inside the sum is taken with respect to the probability density function <math>P(Z_i \\mid X_i = \\mathbf{x}_i; \\theta^{(t)})</math>, which might be different for each  <math>\\mathbf{x}_i</math> of the training set. Everything in the E step is known before the step is taken except <math>T_{j,i}</math>, which is computed according to the equation at the beginning of the E step section.\n\nThis full conditional expectation does not need to be calculated in one step, because ''τ'' and '''μ'''/'''Σ''' appear in separate linear terms and can thus be maximized independently.\n\n==== M step ====\n''Q''(''θ''&nbsp;|&nbsp;''θ''<sup>(''t'')</sup>) being quadratic in form means that determining the maximizing values of ''θ'' is relatively straightforward. Also, ''τ'', ('''μ'''<sub>1</sub>,''Σ''<sub>1</sub>) and ('''μ'''<sub>2</sub>,''Σ''<sub>2</sub>) may all be maximized independently since they all appear in separate linear terms.\n\nTo begin, consider ''τ'', which has the constraint ''τ''<sub>1</sub> + ''τ''<sub>2</sub>=1:\n:<math>\\begin{align}\\boldsymbol{\\tau}^{(t+1)}\n&= \\underset{\\boldsymbol{\\tau}} {\\operatorname{arg\\,max}}\\  Q(\\theta \\mid \\theta^{(t)} ) \\\\\n&= \\underset{\\boldsymbol{\\tau}} {\\operatorname{arg\\,max}} \\ \\left\\{ \\left[  \\sum_{i=1}^n T_{1,i}^{(t)} \\right] \\log \\tau_1 + \\left[  \\sum_{i=1}^n T_{2,i}^{(t)} \\right] \\log \\tau_2  \\right\\}\n\\end{align}</math>\nThis has the same form as the MLE for the [[binomial distribution]], so\n:<math>\\tau^{(t+1)}_j = \\frac{\\sum_{i=1}^n T_{j,i}^{(t)}}{\\sum_{i=1}^n (T_{1,i}^{(t)} + T_{2,i}^{(t)} ) } = \\frac{1}{n} \\sum_{i=1}^n T_{j,i}^{(t)}</math>.\n\nFor the next estimates of ('''μ'''<sub>1</sub>,Σ<sub>1</sub>):\n:<math>\\begin{align}(\\boldsymbol{\\mu}_1^{(t+1)},\\Sigma_1^{(t+1)})\n&= \\underset{\\boldsymbol{\\mu}_1,\\Sigma_1} \\operatorname{arg\\,max} Q(\\theta \\mid \\theta^{(t)} ) \\\\\n&= \\underset{\\boldsymbol{\\mu}_1,\\Sigma_1} \\operatorname{arg\\,max} \\sum_{i=1}^n T_{1,i}^{(t)} \\left\\{ -\\tfrac{1}{2} \\log |\\Sigma_1| -\\tfrac{1}{2}(\\mathbf{x}_i-\\boldsymbol{\\mu}_1)^\\top\\Sigma_1^{-1} (\\mathbf{x}_i-\\boldsymbol{\\mu}_1) \\right\\}\n\\end{align}</math>.\nThis has the same form as a weighted MLE for a normal distribution, so\n:<math>\\boldsymbol{\\mu}_1^{(t+1)} = \\frac{\\sum_{i=1}^n T_{1,i}^{(t)} \\mathbf{x}_i}{\\sum_{i=1}^n T_{1,i}^{(t)}} </math> and <math>\\Sigma_1^{(t+1)} = \\frac{\\sum_{i=1}^n T_{1,i}^{(t)} (\\mathbf{x}_i - \\boldsymbol{\\mu}_1^{(t+1)}) (\\mathbf{x}_i - \\boldsymbol{\\mu}_1^{(t+1)})^\\top }{\\sum_{i=1}^n T_{1,i}^{(t)}} </math>\nand, by symmetry\n:<math>\\boldsymbol{\\mu}_2^{(t+1)} = \\frac{\\sum_{i=1}^n T_{2,i}^{(t)} \\mathbf{x}_i}{\\sum_{i=1}^n T_{2,i}^{(t)}} </math> and <math>\\Sigma_2^{(t+1)} = \\frac{\\sum_{i=1}^n T_{2,i}^{(t)} (\\mathbf{x}_i - \\boldsymbol{\\mu}_2^{(t+1)}) (\\mathbf{x}_i - \\boldsymbol{\\mu}_2^{(t+1)})^\\top }{\\sum_{i=1}^n T_{2,i}^{(t)}} </math>.\n\n==== Termination ====\n\nConclude the iterative process if <math> E_{Z\\mid\\theta^{(t)},\\mathbf{x}}[\\log L(\\theta^{(t)};\\mathbf{x},\\mathbf{Z})] \\leq E_{Z\\mid\\theta^{(t-1)},\\mathbf{x}}[\\log L(\\theta^{(t-1)};\\mathbf{x},\\mathbf{Z})] + \\varepsilon</math> for <math> \\varepsilon </math> below some preset threshold.\n\n==== Generalization ====\n\nThe algorithm illustrated above can be generalized for mixtures of more than two [[multivariate normal distribution]]s.\n\n===Truncated and censored regression===\n\nThe EM algorithm has been implemented in the case where an underlying [[linear regression]] model exists explaining the variation of some quantity, but where the values actually observed are censored or truncated versions of those represented in the model.<ref name=Wolynetz>{{cite journal |title= Maximum likelihood estimation in a linear model from confined and censored normal data |last1= Wolynetz |first1= M.S. |journal= [[Journal of the Royal Statistical Society, Series C]] |year= 1979 |volume= 28 |issue= 2 |pages= 195–206 |doi= 10.2307/2346749|jstor= 2346749 }}</ref>  Special cases of this model include censored or truncated observations from one [[normal distribution]].<ref name=Wolynetz/>\n\n== Alternatives ==\nEM typically converges to a local optimum, not necessarily the global optimum, with no bound on the convergence rate in general. It is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima. Hence, a need exists for alternative methods for guaranteed learning, especially in the high-dimensional setting. Alternatives to EM exist with better guarantees for consistency, which are termed ''moment-based approaches''<ref>{{Cite journal|last=Pearson|first=Karl|date=1894|title=Contributions to the Mathematical Theory of Evolution|url=https://www.jstor.org/stable/90667|journal=Philosophical Transactions of the Royal Society of London A|volume=185|pages=71–110|issn=0264-3820}}</ref> or the so-called ''spectral techniques''<ref>{{Cite journal|last=Shaban|first=Amirreza|last2=Mehrdad|first2=Farajtabar|last3=Bo|first3=Xie|last4=Le|first4=Song|last5=Byron|first5=Boots|date=2015|title=Learning Latent Variable Models by Improving Spectral Solutions with Exterior Point Method|url=https://www.cc.gatech.edu/~bboots3/files/SpectralExteriorPoint-NIPSWorkshop.pdf|journal=UAI|volume=|pages=792–801|via=}}</ref><ref>{{Cite book|url=http://worldcat.org/oclc/815865081|title=Local Loss Optimization in Operator Models: A New Insight into Spectral Learning|last=Balle, Borja Quattoni, Ariadna Carreras, Xavier|date=2012-06-27|oclc=815865081}}</ref>{{Citation needed|date=April 2019}}. Moment-based approaches to learning the parameters of a probabilistic model are of increasing interest recently since they enjoy guarantees such as global convergence under certain conditions unlike EM which is often plagued by the issue of getting stuck in local optima. Algorithms with guarantees for learning can be derived for a number of important models such as mixture models, HMMs etc. For these spectral methods, no spurious local optima occur, and the true parameters can be consistently estimated under some regularity conditions{{Citation needed|date=April 2019}}.\n\n== See also ==\n* [[mixture distribution]]\n* [[compound distribution]]\n* [[density estimation]]\n* [[total absorption spectroscopy]]\n* The EM algorithm can be viewed as a special case of the [[MM algorithm|majorize-minimization (MM) algorithm]].<ref>{{cite web|last=Lange|first=Kenneth|title=The MM Algorithm|url=http://www.stat.berkeley.edu/~aldous/Colloq/lange-talk.pdf}}</ref>\n\n==References==\n{{reflist}}\n\n== Further reading ==\n* {{cite book |first=Robert |last=Hogg |first2=Joseph |last2=McKean |authorlink3=Allen Craig |first3=Allen |last3=Craig |title=Introduction to Mathematical Statistics |pages=359–364 |location=Upper Saddle River, NJ |publisher=Pearson Prentice Hall |year=2005 }}\n* {{cite journal |citeseerx= 10.1.1.9.9735 |title= The Expectation Maximization Algorithm|first=Frank |last= Dellaert |author-link= Frank Dellaert |year= 2002}} gives an easier explanation of EM algorithm as to lowerbound maximization.\n* {{cite book\n |last1 = Bishop |first1 = Christopher M.\n |author-link = Christopher Bishop\n |title = Pattern Recognition and Machine Learning\n |year = 2006\n |publisher = Springer\n |ref = CITEREFBishop2006\n |isbn = 978-0-387-31073-2\n }}\n* {{cite journal |doi=10.1561/2000000034 |title=Theory and Use of the EM Algorithm |journal=Foundations and Trends in Signal Processing |volume=4 |issue=3 |pages=223–296 |first1=M. R. |last1=Gupta |first2=Y. |last2=Chen |year=2010 |citeseerx=10.1.1.219.6830 }} A well-written short book on EM, including detailed derivation of EM for GMMs, HMMs, and Dirichlet. \n* {{cite journal |citeseerx= 10.1.1.28.613 |title= A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models|first=Jeff |last= Bilmes |year= 1998}} includes a simplified derivation of the EM equations for Gaussian Mixtures and Gaussian Mixture Hidden Markov Models.\n* {{Cite book\n |author= Einicke, G.A.\n |year= 2012\n |title= Smoothing, Filtering and Prediction: Estimating the Past, Present and Future\n |publisher= Intech\n |location= Rijeka, Croatia\n |isbn= 978-953-307-752-9\n |url= http://www.intechopen.com/books/smoothing-filtering-and-prediction-estimating-the-past-present-and-future|doi= 10.5772/2706\n }}\n* {{cite book |first=Geoffrey J. |last=McLachlan |first2=Thriyambakam |last2=Krishnan |title=The EM Algorithm and Extensions |location=Hoboken |publisher=Wiley |edition=2nd |year=2008 |isbn=978-0-471-20170-0 }}\n\n== External links ==\n* Various 1D, 2D and 3D [http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture demonstrations of EM together with Mixture Modeling] are provided as part of the paired [[SOCR]] activities and applets. These applets and activities show empirically the properties of the EM algorithm for parameter estimation in diverse settings.\n* [https://arxiv.org/abs/1203.5181 k-MLE: A fast algorithm for learning statistical mixture models] \n* [https://github.com/l-/CommonDataAnalysis Class hierarchy in [[C++]] (GPL) including Gaussian Mixtures]\n* [http://www.inference.phy.cam.ac.uk/mackay/itila/ The on-line textbook: Information Theory, Inference, and Learning Algorithms], by [[David J.C. MacKay]] includes simple examples of the EM algorithm such as clustering using the soft ''k''-means algorithm, and emphasizes the variational view of the EM algorithm, as described in Chapter 33.7 of version 7.2 (fourth edition).\n* [http://www.cse.buffalo.edu/faculty/mbeal/papers/beal03.pdf Variational Algorithms for Approximate Bayesian Inference], by M. J. Beal includes comparisons of EM to Variational Bayesian EM and derivations of several models including Variational Bayesian HMMs  ([http://www.cse.buffalo.edu/faculty/mbeal/thesis/index.html chapters]).\n* [http://www.seanborman.com/publications/EM_algorithm.pdf The Expectation Maximization Algorithm: A short tutorial], A self-contained derivation of the EM Algorithm by Sean Borman.\n* [http://pages.cs.wisc.edu/~jerryzhu/cs838/EM.pdf The EM Algorithm], by Xiaojin Zhu.\n* [https://arxiv.org/pdf/1105.1476.pdf EM algorithm and variants: an informal tutorial] by Alexis Roche.  A concise and very clear description of EM and many interesting variants.\n\n{{DEFAULTSORT:Expectation-maximization Algorithm}}\n[[Category:Estimation methods]]\n[[Category:Machine learning algorithms]]\n[[Category:Missing data]]\n[[Category:Statistical algorithms]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Cluster analysis algorithms]]"
    },
    {
      "title": "Extremal optimization",
      "url": "https://en.wikipedia.org/wiki/Extremal_optimization",
      "text": "'''Extremal optimization (EO)''' is an [[Optimization (mathematics)|optimization]] [[heuristic (computer science)|heuristic]] inspired by the [[Bak–Sneppen model]] of [[self-organized criticality]] from the field of statistical physics. This [[heuristic (computer science)|heuristic]] was designed initially to address [[combinatorial optimization]] problems such as the [[travelling salesman problem]] and [[spin glass]]es, although the technique has been demonstrated to function in optimization domains.\n\n== Relation to self-organized criticality ==\n[[Self-organized criticality]] (SOC) is a statistical physics concept to describe a class of dynamical systems that have a critical point as an attractor. Specifically, these are non-equilibrium systems that evolve through avalanches of change and dissipations that reach up to the highest scales of the system. SOC is said to govern the dynamics behind some natural systems that have these burst-like phenomena including landscape formation, earthquakes, evolution, and the granular dynamics of rice and sand piles. Of special interest here is the [[Bak–Sneppen model]] of SOC, which is able to describe evolution via [[punctuated equilibrium]] (extinction events) – thus modelling evolution as a self-organised critical process.\n\n== Relation to computational complexity ==\nAnother piece in the puzzle is work on computational complexity, specifically that critical points have been shown to exist in [[NP-complete]] problems, where near-optimum solutions are widely dispersed and separated by barriers in the search space causing local search algorithms to get stuck or severely hampered. It was the [[Bak–Sneppen model|evolutionary self-organised criticality model by Bak and Sneppen]] and the observation of critical points in combinatorial optimisation problems that lead to the development of Extremal Optimization by Stefan Boettcher and Allon Percus.\n\n== The technique ==\nEO was designed as a  [[local search (optimization)|local search]]  algorithm for [[combinatorial optimization]] problems. Unlike [[genetic algorithms]], which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). This differs from holistic approaches such as [[ant colony optimization]] and [[evolutionary computation]] that assign equal-fitness to all components of a solution based upon their collective evaluation against an objective function. The algorithm is initialized with an initial solution, which can be constructed randomly, or derived from another search process.\n\nThe technique is a fine-grained search, and superficially resembles a [[hill climbing]] (local search) technique. A more detailed examination reveals some interesting principles, which may have applicability and even some similarity to broader population-based approaches ([[evolutionary computation]] and [[artificial immune system]]). The governing principle behind this algorithm is that of improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is obviously at odds with [[genetic algorithms]], the quintessential evolutionary computation algorithm that selects good solutions in an attempt to make better solutions.\n\nThe resulting dynamics of this simple principle is firstly a robust hill climbing search behaviour, and secondly a diversity mechanism that resembles that of multiple-restart search. Graphing holistic solution quality over time (algorithm iterations) shows periods of improvement followed by quality crashes (avalanche) very much in the manner as described by [[punctuated equilibrium]]. It is these crashes or dramatic jumps in the search space that permit the algorithm to escape local optima and differentiate this approach from other local search procedures. Although such punctuated-equilibrium behaviour can be \"designed\" or \"hard-coded\", it should be stressed that this is an ''emergent'' effect of the negative-component-selection principle fundamental to the algorithm.\n\nEO has primarily been applied to combinatorial problems such as [[graph partitioning]] and the [[travelling salesman problem]], as well as problems from statistical physics such as [[spin glass]]es.\n\n== Variations on the theme and applications ==\nGeneralised extremal optimization (GEO) was developed to operate on bit strings where component quality is determined by the absolute rate of change of the bit, or the bits contribution to holistic solution quality. This work includes application to standard function optimisation problems as well as engineering problem domains. Another similar extension to EO is Continuous Extremal Optimization (CEO).\n\nEO has been applied to image rasterization as well as used as a local search after using [[ant colony optimization]]. EO has been used to identify structures in complex networks. EO has been used on a multiple target tracking problem. Finally, some work has been done on investigating the probability distribution used to control selection.\n\n== See also ==\n* [[Genetic algorithm]]\n* [[Simulated annealing]]\n\n== References ==\n* Per Bak, Chao Tang, and Kurt Wiesenfeld, [http://prola.aps.org/abstract/PRL/v59/i4/p381_1 \"Self-organized criticality: An explanation of the 1/f noise\"], Physical Review Letters '''59''', 381–384 (1987)\n* Per Bak and Kim Sneppen, [http://prola.aps.org/abstract/PRL/v71/i24/p4083_1 \"Punctuated equilibrium and criticality in a simple model of evolution\"], Physical Review Letters '''71''',  4083–4086 (1993)\n* P Cheeseman, B Kanefsky, WM Taylor, [http://www.cs.usask.ca/grads/jiz194/References/cheeseman91where.ps \"Where the really hard problems are\"], Proceedings of the 12th IJCAI, (1991)\n* G Istrate, \"[https://arxiv.org/pdf/cs/0005032 Computational complexity and phase transitions]\", Proceedings. 15th Annual IEEE Conference on Computational Complexity, 104–115 (2000)\n* Stefan Boettcher, Allon G. Percus, [https://arxiv.org/abs/math/9904056 \"Extremal Optimization: Methods derived from Co-Evolution\"], Proceedings of the Genetic and Evolutionary Computation Conference (1999)\n* Stefan Boettcher, [http://www.iop.org/EJ/abstract/0305-4470/32/28/302 \"Extremal optimization of graph partitioning at the percolation threshold\"], J. Phys. A: Math. Gen. '''32''', 5201–5211 (1999)\n* {{citation|doi=10.1016/S0004-3702(00)00007-2|title=Nature's way of optimizing|journal=Artificial Intelligence|volume=119|issue=1–2|pages=275–286|year=2000|last1=Boettcher|first1=Stefan|last2=Percus|first2=Allon}}\n* S Boettcher, [http://ieeexplore.ieee.org/iel5/5992/19077/00881710.pdf \"Extremal Optimization – Heuristics via Co-Evolutionary Avalanches\"], Computing in Science & Engineering '''2''', pp.&nbsp;75–82, 2000\n* Stefan Boettcher and Allon G. Percus, [http://prola.aps.org/abstract/PRL/v86/i23/p5211_1 \"Optimization with Extremal Dynamics\"], Phys. Rev. Lett. '''86''',  5211–5214 (2001)\n* Jesper Dall and Paolo Sibani, [https://arxiv.org/abs/cond-mat/0107475 \"Faster Monte Carlo Simulations at Low Temperatures. The Waiting Time Method\"], Computer Physics Communications '''141''' (2001) 260–267\n* Stefan Boettcher and Michelangelo Grigni, [http://www.iop.org/EJ/abstract/0305-4470/35/5/301 \"Jamming Model for the Extremal Optimization Heuristic\"], J. Phys. A: Math. Gen. '''35''', 1109–1123 (2002)\n* Souham Meshoul and Mohamed Batouche, [http://www.springerlink.com/(24ia1q55gtqkcy45pgzuxm45)/app/home/contribution.asp?referrer=parent&backto=issue,40,74;journal,1497,3835;linkingpublicationresults,1:105633,1 \"Robust Point Correspondence for Image Registration Using Optimization with Extremal Dynamics\"], Lecture Notes in Computer Science '''2449''', 330–337 (2002)\n* Roberto N. Onody and Paulo A. de Castro, [https://arxiv.org/abs/q-bio.PE/0410007 \"Self-Organized Criticality, Optimization and Biodiversity\"], Int. J. Mod. Phys. C '''14''', 911–916 (2002)\n* Stefan Boettcher and Allon G. Percus, [http://link.aps.org/abstract/PRE/v69/e066703 \"Extremal Optimization at the Phase Transition of the 3-Coloring Problem\"], Phys. Rev. E '''69''', 066703 (2004)\n* A. Alan Middleton, [http://link.aps.org/abstract/PRE/v69/e055701 \"Improved extremal optimization for the Ising spin glass\"], Phys. Rev. E '''69''', 055701 (2004)\n* F. Heilmann, K. H. Hoffmann and P. Salamon, [http://www.edpsciences.org/10.1209/epl/i2004-10011-3 \"Best possible probability distribution over extremal optimization ranks\"], Europhys. Lett. '''66''', pp.&nbsp;305–310 (2004)\n* [https://arxiv.org/abs/cs.AI/0411072]  Pontus Svenson, \"Extremal optimization for sensor report pre-processing\", Proc SPIE  '''5429''', 162–171 (2004)\n* Tao Zhou, Wen-Jie Bai, Long-Jiu Cheng, Bing-Hong Wang, [http://link.aps.org/abstract/PRE/v72/e016702 \"Continuous extremal optimization for Lennard–Jones Clusters\"], Phys. Rev. E '''72''', 016702 (2004)\n* Jordi Duch and Alex Arenas, [http://link.aps.org/abstract/PRE/v72/e027104 \"Community detection in complex networks using extremal optimization\"], Phys. Rev. E '''72''', 027104 (2005)\n* E. Ahmed and M.F. Elettreby, [https://dx.doi.org/10.1016/j.amc.2005.01.122 \"On combinatorial optimization motivated by biology\"], Applied Mathematics and Computation, Volume 172, Issue 1, 1 January 2006, Pages 40–48\n\n== External links ==\n* [http://www.physics.emory.edu/faculty/boettcher Stefan Boettcher] – Physics Department, Emory University\n* [https://web.archive.org/web/20060916003131/http://wwwc3.lanl.gov/~percus/ Allon Percus] – University of California, Los Angeles\n* [http://www.it-weise.de/projects/book.pdf Global Optimization Algorithms – Theory and Application –] – Thomas Weise\n\n{{Optimization algorithms}}\n\n[[Category:Heuristics]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Artificial intelligence]]"
    },
    {
      "title": "Fernandez’s method",
      "url": "https://en.wikipedia.org/wiki/Fernandez%E2%80%99s_method",
      "text": "{{Multiple issues|\n{{unreferenced|date=April 2012}}\n{{orphan|date=April 2012}}\n{{context|date=April 2012}}\n}}\n\n'''Fernandez's method''' (FB) is a method which is used in the [[multiprocessor scheduling]] [[algorithm]].  It is actually used to improve the quality of the lower bounding schemes which are adopted by [[branch and bound]] algorithms for solving multiprocessor scheduling problem. Fernandez's problem derives a better lower bound than HF,and propose a quadratic-time algorithm from calculating the bound. It is known that a straightforward calculation of FB takes O<math>(n^3)</math> time, since it must examine O<math>(n^2)</math> combinations each of which takes O<math>(n)</math> time in the worst case.\n\n==Further reading==\n*''A Comparison of List Scheduling for Parallel Processing Systems''\n\n{{DEFAULTSORT:Fernandez's method}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Fireworks algorithm",
      "url": "https://en.wikipedia.org/wiki/Fireworks_algorithm",
      "text": "The '''Fireworks Algorithm (FWA)''' is an approach to exploring a very large solution space by choosing a set of random points confined by some distance metric in the hopes that one or more of them will yield promising results, allowing for a more concentrated search nearby.\n\n==External links==\n* [http://www.cil.pku.edu.cn/research/swgence/271228.htm Fireworks Algorithm]\n* [http://www.cil.pku.edu.cn Computational Intelligence Laboratory, Peking University]\n\n[[Category:Optimization algorithms and methods]]\n\n{{algorithm-stub}}"
    },
    {
      "title": "Fly algorithm",
      "url": "https://en.wikipedia.org/wiki/Fly_algorithm",
      "text": "{{Evolutionary algorithms}}\n{{COI|date=July 2018}}\n\n== History ==\n\nThe Fly Algorithm is a type of [[cooperative coevolution]] based on the Parisian approach.<ref name=Collet2009>{{cite book |title=Optimization in Signal and Image Processing|chapter=Artificial evolution and the Parisian approach: applications in the processing of signals and images|last2=Louchet|first2=Jean|last1=Collet|first1=Pierre|publisher=Wiley-ISTE|date=Oct 2009|isbn=9781848210448|editor1-last=Siarry|editor1-first=Patrick}}</ref> The Fly Algorithm has first been developed in 1999 in the scope of the application of [[Evolutionary algorithms]] to [[computer stereo vision]].<ref name=LouchetRFIA2000>{{cite conference |title=L’algorithme des mouches : une stratégie d’évolution individuelle appliquée en stéréovision|last1=Louchet|first1=Jean|conference=Reconnaissance des Formes et Intelligence Artificielle (RFIA2000)|date=Feb 2000}}</ref><ref name=Louchet2000ICPR>{{cite conference |title=Stereo analysis using individual evolution strategy|last1=Louchet|first1=Jean|publisher=IEEE|conference=Proceedings of 15th International Conference on  Pattern Recognition, 2000 (ICPR’00)|pages=908–911|date=Sep 2000|doi=10.1109/ICPR.2000.905580|isbn=0-7695-0750-6|location=Barcelona, Spain}}</ref> Unlike the classical image-based approach to stereovision, which extracts image primitives then matches them in order to obtain 3-D information, the Fly Agorithm is based on the direct exploration of the 3-D space of the scene. A fly is defined as a 3-D point described by its coordinates (''x'', ''y'', ''z''). Once a random population of flies has been created in a search space corresponding to the field of view of the cameras, its evolution (based on the Evolutionary Strategy paradigm) used a [[fitness function]] that evaluates how likely the fly is lying on the visible surface of an object, based on the consistency of its image projections. To this end, the fitness function uses the grey levels, colours and/or textures of the calculated fly's projections.\n\nThe first application field of the Fly Algorithm has been stereovision.<ref name=LouchetRFIA2000 /><ref name=Louchet2000ICPR /><ref name=Louchet2001>{{cite journal |title=Using an Individual Evolution Strategy for Stereovision|last1=Louchet|first1=Jean|pages=101–109|date=Jun 2001|doi=10.1023/A:1011544128842|volume=2|issue=2|journal=Genetic Programming and Evolvable Machines}}</ref><ref name=Boumaza2003>{{cite conference |title=Mobile robot sensor fusion using flies|last1=Boumaza|first1=Amine|last2=Louchet|first2=Jean|publisher=Springer|conference=European Conference on Genetic Programming (EuroGP 2003)|pages=357–367|date=Apr 2003|doi=10.1007/3-540-36605-9_33|isbn=978-3-540-00976-4|location=Essex, UK|volume=2611|book-title=Lecture Notes on Computer Science}}</ref> While classical `image priority' approaches use matching features from the stereo images in order to build a 3-D model, the Fly Algorithm directly explores the 3-D space and uses image data to evaluate the validity of 3-D hypotheses. A variant called the \"Dynamic Flies\" defines the fly as a 6-uple (''x'', ''y'', ''z'', ''x’'', ''y’'', ''z’'') involving the fly's velocity.<ref name=Louchet2002>{{cite book |title=Apprentissage Automatique et Evolution Artificielle|chapter=L’algorithme des mouches dynamiques: guider un robot par évolution artificielle en temps réel|last1=Louchet|first1=Jean|last2=Guyon|first2=Maud|last3=Lesot|first3=Marie-Jeanne|last4=Boumaza|first4=Amine|publisher=Hermes Sciences Publications|pages=|date=Mar 2002|isbn=978-2746203600|language=French|chapter-url=http://jean.louchet.free.fr/publis/2001ECA.pdf|editor1-last=Lattaud|editor1-first=Claude}}</ref><ref name=Louchet2002PatterRec>{{cite journal |title=Dynamic Flies: a new pattern recognition tool applied to stereo sequence processing|last1=Louchet|first1=Jean|last2=Guyon|first2=Maud|last3=Lesot|first3=Marie-Jeanne|last4=Boumaza|first4=Amine|pages=335–345|date=Jan 2002|doi=10.1016/S0167-8655(01)00129-5|volume=23|issue=1–3|journal=Pattern Recognition Letters|url=http://jean.louchet.free.fr/publis/2001PRL.pdf}}</ref> The velocity components are not explicitly taken into account in the fitness calculation but are used in the flies' positions updating and are subject to similar genetic operators (mutation, crossover).\n\nThe application of Flies to obstacle avoidance in vehicles<ref name=Bomaza2001EvoIASP>{{cite conference |title=Dynamic Flies: Using Real-time evolution in Robotics|last1=Boumaza|first1=Amine|last2=Louchet|first2=Jean|publisher=Springer|conference=Artificial Evolution in Image Analysis and Signal Processing (EVOIASP2001)|pages=288–297|date=Apr 2001|doi=10.1007/3-540-45365-2_30|isbn=978-3-540-41920-4|location=Como, Italy|volume=2037|book-title=Lecture Notes on Computer Science}}</ref> exploits the fact that the population of flies is a time compliant, quasi-continuously evolving representation of the scene to directly generate vehicle control signals from the flies. The use of the Fly Algorithm is not strictly restricted to stereo images, as other sensors may be added (e.g. acoustic proximity sensors, etc.) as additional terms to the fitness function being optimised. Odometry information can also be used to speed up the updating of flies' positions, and conversely the flies positions can be used to provide localisation and mapping information.<ref name=Louchet2009EvoIASP>{{cite conference |title=Flies Open a Door to SLAM.|last1=Louchet|first1=Jean|last2=Sapin|first2=Emmanuel|publisher=Springer|conference=Applications of Evolutionary Computation (EvoApplications 2009)|pages=385–394|date=2009|doi= 10.1007/978-3-642-01129-0_43|location=Tübingen, Germany|volume=5484|book-title=Lecture Notes in Computer Science}}</ref>\n\nAnother application field of the Fly Algorithm is reconstruction for emission Tomography in [[nuclear medicine]]. The Fly Algorithm has been successfully applied in [[single-photon emission computed tomography]]<ref name=Bousquet2007EA>{{cite conference |title=Fully Three-Dimensional Tomographic Evolutionary Reconstruction in Nuclear Medicine|last1=Bousquet|first1=Aurélie|last2=Louchet|first2=Jean-Marie|last3=Rocchisani|first3=Jean|publisher=Springer, Heidelberg|conference=Proceedings of the 8th international conference on Artificial Evolution (EA’07)|pages=231–242|date=Oct 2007|doi=10.1007/978-3-540-79305-2_20|isbn=978-3-540-79304-5|location=Tours, France|volume=4926|book-title=Lecture Notes in Computer Science|url=http://jean.louchet.free.fr/publis/EA07Bousquet.pdf|}}</ref> and [[positron emission tomography]]<ref name=Vidal2009EA>{{cite conference |title=Artificial evolution for 3D PET reconstruction|last1=Vidal|first1=Franck P.|last2=Lazaro-Ponthus|first2=Delphine|last3=Legoupil|first3=Samuel|first4=Jean|first5=Évelyne|first6=Jean-Marie|last4=Louchet|last5=Lutton|last6=Rocchisani|publisher=Springer, Heidelberg|conference=Proceedings of the 9th international conference on Artificial Evolution (EA’09)|pages=37–48|date=Oct 2009|doi=10.1007/978-3-642-14156-0_4|isbn=978-3-642-14155-3|location=Strasbourg, France|volume=5975|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Vidal2009EA.pdf|}}</ref>\n.<ref name=Vidal2009MIC>{{cite conference |title=PET reconstruction using a cooperative coevolution strategy in LOR space|last1=Vidal|first1=Franck P.|last2=Louchet|first2=Jean|last3=Lutton|first3=Évelyne|last4=Rocchisani|first4=Jean-Marie|publisher=IEEE|conference=Medical Imaging Conference (MIC)|pages=3363–3366|date=Oct-Nov 2009|doi=10.1109/NSSMIC.2009.5401758|location=Orlando, Florida|book-title=IEEE Nuclear Science Symposium Conference Record (NSS/MIC), 2009}}</ref> Here, each fly is considered a photon emitter and its fitness is based on the conformity of the simulated illumination of the sensors with the actual pattern observed on the sensors. Within this application, the fitness function has been re-defined to use the new concept of 'marginal evaluation'. Here, the fitness of one individual is calculated as its (positive or negative) contribution to the quality of the global population. It is based on the [[Cross-validation (statistics)|leave-one-out cross-validation]] principle. A ''global fitness function'' evaluates the quality of the population as a whole; only then the fitness of an individual (a fly) is calculated as the difference between the global fitness values of the population with and without the particular fly whose ''individual fitness function''  has to be evaluated.<ref name=Vidal2010EvoIASP>{{cite conference |title=New genetic operators in the Fly Algorithm: application to medical PET image reconstruction|last1=Vidal|first1=Franck P.|last2=Louchet|first2=Jean|last4=Lutton|first4=Évelyne|last3=Rocchisani|first3=Jean-Marie|publisher=Springer, Heidelberg|conference=European Workshop on Evolutionary Computation in Image Analysis and Signal Processing (EvoIASP’10)|pages=292–301|date=Apr 2010|doi=10.1007/978-3-642-12239-2_30|isbn=978-3-642-12238-5|location=Istanbul, Turkey|volume=6024|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Vidal2010EvoIASP.pdf|}}</ref><ref name=Vidal2010PPSN>{{cite conference |title=Threshold selection, mitosis and dual mutation in cooperative coevolution: application to medical 3D tomography|last1=Vidal|first1=Franck P.|last2=Lutton|first2=Évelyne|last3=Louchet|first3=Jean|last4=Rocchisani|first4=Jean-Marie|publisher=Springer, Heidelberg|conference=International Conference on Parallel Problem Solving From Nature (PPSN'10)|pages=414–423|date=Sep 2010|doi=10.1007/978-3-642-15844-5_42|location=Krakow, Poland|volume=6238|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Vidal2010PPSN.pdf|}}</ref> In <ref name=Abbood2017SWEVO>>{{cite journal |title=Voxelisation in the 3-D Fly Algorithm for PET|last1=Ali Abbood|first1=Zainab|last2=Lavauzelle|first2=Julien|last3=Lutton|first3=Évelyne|last4=Rocchisani|first4=Jean-Marie|last5=Louchet|first5=Jean|last6=Vidal|first6=Franck P.|date=2017|doi=10.1016/j.swevo.2017.04.001|issn=2210-6502|volume=???|issue=|pages=91–105|journal=Swarm and Evolutionary Computation|url=http://fly4pet.fpvidal.net/pdf/Abbood2017SWEVO.pdf}}</ref> the fitness of each fly is considered as a `level of confidence'. It is used during the voxelisation process to tweak the fly's individual footprint using implicit modelling (such as [[metaballs]]). It produces smooth results that are more accurate.\n\nMore recently it has been used in digital art to generate mosaic-like images or spray paint.<ref name=Abbood2017EvoIASP>{{cite conference |title=Evolutionary Art Using the Fly Algorithm|last1=Ali Abbood|first1=Zainab|last2=Amlal|first2=Othman|last3=Vidal|first3=Franck P.|publisher=Springer|conference=Applications of Evolutionary Computation (EvoApplications 2017)|pages=455–470|date=Apr 2017|doi=10.1007/978-3-319-55849-3_30|location=Amsterdam, The Netherlands|volume=10199|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Abbood017EvoIASP.pdf|}}</ref> Examples of images can be found on [https://www.youtube.com/playlist?list=PLR_5kWb9r72g-XnYdrdJhRle8aGtqX6CE YouTube]\n<!--ref>{{cite conference |title=|last1=|first1=|last2=|first2=|last3=|first3=|last4=|first4=|publisher=|conference=|pages=|date=|doi=|isbn=|location=|volume=|book-title=|url=|}}</ref-->\n<!-- \n<ref name=Vidal2010>{{cite journal |title=Flies for PET: An artificial evolution strategy for image reconstruction in nuclear medicine|last1=Vidal|first1=Franck P.|last4=Lutton|first4=Évelyne|last2=Louchet|first2=Jean|last3=Rocchisani|first3=Jean-Marie|publisher=American Association of Physicists in Medicine|page=3139|date=2010|doi=10.1118/1.3468200|isbn=|volume=37|issue=6|journal=Medical Physics|url=http://fly4pet.fpvidal.net/pdf/Vidal2010MedPhys-A.pdf|}}</ref>\n-->\n\n== Parisian Evolution ==\n\nHere, the population of individuals is considered as a ''society'' where the individuals collaborate toward a common goal. \nThis is implemented using an evolutionary algorithm that includes all the common [[genetic operators]] (e.g. mutation, cross-over, selection). \nThe main difference is in the fitness function. \nHere two levels of fitness function are used:\n* A local fitness function to assess the performance of a given individual (usually used during the selection process).\n* A global fitness function to assess the performance of the whole population. Maximising (or minimising depending on the problem considered) this global fitness is the goal of the population.\nIn addition, a diversity mechanism is required to avoid individuals gathering in only a few areas of the search space. \nAnother difference is in the extraction of the problem solution once the evolutionary loop terminates. In classical evolutionary approaches, the best individual corresponds to the solution and the rest of the population is discarded. \nHere, all the individuals (or individuals of a sub-group of the population) are collated to build the problem solution.\nThe way the fitness functions are constructed and the way the solution extraction is made are of course problem-dependent.\n\nExamples of Parisian Evolution applications include:\n* [http://evelyne.lutton.free.fr/FlyAlgo.html The Fly algorithm].\n* [http://evelyne.lutton.free.fr/TextRetrieval.html Text-mining].\n* [http://evelyne.lutton.free.fr/HandGesture.html Hand gesture recognition].\n* [http://evelyne.lutton.free.fr/Cheese.html Modelling complex interactions in industrial agrifood process].\n* [http://fpvidal.net/fly4pet/fly4pet.php Positron Emission Tomography reconstruction].\n\n== Disambiguation ==\n\n=== Parisian approach ''vs'' [[cooperative coevolution]] ===\n\n[[Cooperative coevolution]] is a broad class of [[evolutionary algorithm]]s where a complex problem is solved by decomposing it into subcomponents that are solved independently. \nThe Parisian approach shares many similarities with the [[cooperative coevolution|cooperative coevolutionary algorithm]]. The Parisian approach makes use of a single-population whereas multi-species may be used in [[cooperative coevolution|cooperative coevolutionary algorithm]]. \nSimilar internal evolutionary engines are considered in classical [[evolutionary algorithm]], [[cooperative coevolution|cooperative coevolutionary algorithm]] and Parisian evolution. \nThe difference between [[cooperative coevolution|cooperative coevolutionary algorithm]] and Parisian evolution resides in the population's semantics. \n[[cooperative coevolution|Cooperative coevolutionary algorithm]] divides a big problem into sub-problems (groups of individuals) and solves them separately toward the big problem.<ref name=Mesejo2015>{{cite journal |title=Artificial Neuron – Glia Networks Learning Approach Based on Cooperative Coevolution|last1=Mesejo|first1=Pablo|last2=Ibanez|first2=Oscar|last3=Fernandez-blanco|first3=Enrique|last4=Cedron|first4=Francisco|last5=Pazos|first5=Alejandro|last6=Porto-pazos|first6=Ana|pages=1550012|date=2015|doi=10.1142/S0129065715500124|volume=25|issue=4|journal=International Journal of Neural Systems}}</ref> There is no interaction/breeding between individuals of the different sub-populations, only with individuals of the same sub-population. \nHowever, Parisian [[evolutionary algorithms]] solve a whole problem as a big component. \nAll population's individuals cooperate together to drive the whole population toward attractive areas of the search space.\n\n=== Fly Algorithm ''vs'' [[particle swarm optimisation]] ===\n[[Cooperative coevolution]] and [[particle swarm optimisation|particle swarm optimisation (PSO)]] share many similarities. [[particle swarm optimisation|PSO]] is inspired by the social behaviour of bird flocking or fish schooling.<ref name=Kennedy1995>{{cite conference |conference=Proceedings of IEEE International Conference on Neural Networks|title=Particle swarm optimization|last1=Kennedy,|first1=J|last2=Eberhart|first2=R|publisher=IEEE|date=1995|doi=10.1109/ICNN.1995.488968|pages=1942–1948}}</ref><ref name=Shi1998>{{cite conference |conference=Proceedings of IEEE International Conference on Evolutionary Computation|title=A modified particle swarm optimizer|last1=Shi,|first1=Y|last2=Eberhart|first2=R|publisher=IEEE|date=1998|doi=10.1109/ICEC.1998.699146|pages=69–73}}</ref> \nIt was initially introduced as a tool for realistic animation in computer graphics. \nIt uses complex individuals that interact with each other in order to build visually realistic collective behaviours through adjusting the individuals' behavioural rules (which may use random generators). \nIn mathematical optimisation, every particle of the swarm somehow follows its own random path biased toward the best particle of the swarm. \nIn the Fly Algorithm, the flies aim at building spatial representations of a scene from actual sensor data; flies do not communicate or explicitly cooperate, and do not use any behavioural model.\n\nBoth algorithms are search methods that start with a set of random solutions, which are iteratively corrected toward a global optimum. \nHowever, the solution of the optimisation problem in the Fly Algorithm is the population (or a subset of the population): The flies implicitly collaborate to build the solution. In [[particle swarm optimisation|PSO]] the solution is a single particle, the one with the best fitness. Another main difference between the Fly Algorithm and with [[particle swarm optimisation|PSO]] is that the Fly Algorithm is not based on any behavioural model but only builds a geometrical representation.\n\n== Applications of the Fly Algorithnm ==\n* [[Computer stereo vision]] <ref name=LouchetRFIA2000 /><ref name=Louchet2000ICPR /><ref name=Louchet2001 /><ref name=Boumaza2003 />\n* [[Obstacle avoidance]] <ref name=Louchet2002 /><ref name=Bomaza2001EvoIASP /><ref name=Louchet2002PatterRec />\n* [[Simultaneous localization and mapping|Simultaneous localization and mapping (SLAM)]] <ref name=Louchet2009EvoIASP />\n* [[Single-photon emission computed tomography|Single-photon emission computed tomography (SPECT)]] reconstruction <ref name=Bousquet2007EA />\n* [[Positron emission tomography|Positron emission tomography (PET)]] reconstruction <ref name=Vidal2009EA /><ref name=Vidal2009MIC /><ref name=Vidal2010EvoIASP /><ref name=Vidal2010PPSN /><ref name=Abbood2017SWEVO /><ref name=Abbood2017EA>{{cite conference |title=Basic, Dual, Adaptive, and Directed Mutation Operators in the Fly Algorithm|last1=Abbood|first1=Zainab Ali|last2=Vidal|first2=Franck P.|conference=13th Biennal International Conference on Artificial Evolution (EA-2017)|pages=106–119|date=2017|location=Paris, France|book-title=Lecture Notes in Computer Science|ISBN=978-2-9539267-7-4}}</ref>\n* [[Digital art]] <ref name=Abbood2017EvoIASP /><ref name=Abbood2017ArtAndScience>{{cite journal |title=Fly4Arts: Evolutionary Digital Art with the Fly Algorithm|last1=Abbood|first1=Zainab Ali|last2=Vidal|first2=Franck P.|pages=1–6|date=Oct 2017|doi=10.21494/ISTE.OP.2017.0177|volume=17- 1|issue=1|journal=Art and Science}}</ref>\n\n<!--== Methodology ==\n=== Fitness functions ===\n=== Initialisation ===\n=== Selection ===\n=== Genetic operators ===\n=== Stopping criteria ===\n=== Extraction of the solution ===\n-->\n\n== Example: Tomography reconstruction ==\n<!-- === Pseudocode === -->\n\n{{multiple image\n | width = 100\n | right\n | image1=\n | caption1=Example of image to reconstruct <math>(f)</math>, which is unknown.\n | image2=sinogram-hot_spheres.png\n | caption2=Sinogram <math>(Y)</math> of <math>f</math>, which is known.\n | image3=\n | caption3=Image reconstructed using the Fly algorithm <math>(\\hat{f})</math>.\n | image4=\n | caption4=Sinogram <math>(\\hat{Y})</math> of <math>\\hat{f}</math>.\n | footer=Example of reconstruction of a hot rod phantom using the Fly Algorithm. \n}}\n\nTomography reconstruction is an [[inverse problem]] that is often [[ill-posed]] due to missing data and/or noise. The answer to the inverse problem is not unique, and in case of extreme noise level it may not even exist. The input data of a reconstruction algorithm may be given as the [[Radon transform]] or sinogram <math>\\left(Y\\right)</math> of the data to reconstruct <math>\\left(f\\right)</math>. <math>f</math> is unknown; <math>Y</math> is known. \nThe data acquisition in tomography can be modelled as:\n\n<math>\nY = P[f] + \\epsilon\n</math>\n\nwhere <math>P</math> is the system matrix or projection operator and <math>\\epsilon</math> corresponds to some [[Shot noise|Poisson noise]]. \nIn this case the reconstruction corresponds to the inversion of the [[Radon transform]]:\n\n<math>\nf = P^{-1}[Y]\n</math>\n\nNote that <math>P^{-1}</math> can account for noise, acquisition geometry, etc. \nThe Fly Algorithm is an example of [[iterative reconstruction]]. Iterative methods in [[tomographic reconstruction]] are relatively easy to model:\n\n<math>\n    \\hat{f} = \\operatorname{arg\\,min} || Y - \\hat{Y}||^2_2\n</math>\n\nwhere <math>\\hat{f}</math> is an estimate of <math>f</math>, that minimises an error metrics (here [[Norm (mathematics)|{{math|''ℓ<sup>2</sup>''}}-norm]], but other error metrics could be used) between <math>Y</math> and <math>\\hat{Y}</math>. Note that a [[Regularization (mathematics)|regularisation term]] can be introduced to prevent overfitting and to smooth noise whilst preserving edges. \nIterative methods can be implemented as follows:\n[[File:Iterative-algorithm.svg|512x122px|thumb|right|Iterative correction in tomography reconstruction.]]\n   (i) The reconstruction starts using an initial estimate of the image (generally a constant image),\n   (ii) Projection data is computed from this image,\n   (iii) The estimated projections are compared with the measured projections,\n   (iv) Corrections are made to correct the estimated image, and\n   (v) The algorithm iterates until convergence of the estimated and measured projection sets.\n\nThe [[pseudocode]] below is a step-by-step description of the Fly Algorithm for [[tomographic reconstruction]]. The algorithm follows the steady-state paradigm. For illustrative purposes, advanced genetic operators, such as mitosis, dual mutation, etc.<ref>{{cite conference |title=Threshold selection, mitosis and dual mutation in cooperative co-evolution: Application to medical 3D tomography|last1=Vidal|first1=Franck P.|last2=Lutton|first2=Évelyne|last3=Louchet|first3=Jean|last4=Rocchisani|first4=Jean-Marie|publisher=Springer Berlin / Heidelberg|conference=Parallel Problem Solving from Nature - PPSN XI|pages=414–423|date=Sep 2010|doi=10.1007/978-3-642-15844-5_42|isbn=978-3-642-15843-8|location=Kraków, Poland|volume=6238|book-title=Lecture Notes in Computer Science|url=http://www.fpvidal.net/fly4pet/pdf/Vidal2010PPSN.pdf|}}</ref><ref>{{cite conference |title=Basic, Dual, Adaptive, and Directed Mutation Operators in the Fly Algorithm|last1=Ali Abbood|first1=Zainab|last2=Vidal|first2=Franck P.|publisher=Springer-Verlag|conference=13th Biennal International Conference on Artificial Evolution|date=Oct 2017|location=Paris, France|book-title=Lecture Notes in Computer Science}}</ref> are ignored. A [[JavaScript]] implementation can be found on [http://www.fpvidal.net/fly4pet/demo.php Fly4PET].\n\n<!-- [[File:Fly4pet.svg|512x616px|thumb|right|Flowchart of the Fly Algorithm for Positron Emission Tomography.]] -->\n<!-- [[File:evolutionary_PET_reconstructions.png|299x501px|thumb|right|Example of evolutionary reconstructions at successive resolutions (with ''N'' the number of flies and ''TS'' the time-stamp in minutes).]]-->\n\n '''algorithm''' fly-algorithm '''is'''\n     '''input:''' number of flies (''N''), \n            input projection data (''p''<sub>''reference''</sub>)\n     \n     '''output:''' the fly population (''F''), \n             the projections estimated from ''F'' (''p''<sub>''estimated''</sub>)\n             the 3-D volume corresponding to the voxelisation of ''F'' (''V''<sub>''F''</sub>)\n     \n     '''postcondition:''' the difference between ''p''<sub>''estimated''</sub> and ''p''<sub>reference</sub> is minimal.\n     \n     '''START'''\n     \n  1.   ''// Initialisation''\n  2.   ''// Set the position of the ''N'' flies, i.e. create initial guess''\n  3.   '''for each''' fly ''i'' '''in''' fly population ''F'' '''do'''\n  4.       ''F''(''i'')<sub>x</sub> &larr; random(0, 1)\n  5.       ''F''(''i'')<sub>y</sub> &larr; random(0, 1)\n  6.       ''F''(''i'')<sub>z</sub> &larr; random(0, 1)\n  7.       Add ''F''(''i'')'s projection in ''p''<sub>''estimated''</sub>\n  8.   \n  9.   ''// Compute the population's performance (i.e. the global fitness)''\n 10.   ''G''<sub>''fitness''</sub>(''F'') &larr; ''Error''<sub>metrics</sub>(''p''<sub>''reference''</sub>, ''p''<sub>''estimated''</sub>)\n 11.    \n 12.   ''f''<sub>''kill''</sub> &larr; Select a random fly of ''F''\n 13.    \n 14.   Remove ''f''<sub>''kill''</sub>'s contribution from ''p''<sub>''estimated''</sub>\n 15.    \n 16.   ''// Compute the population's performance without f<sub>kill</sub>''\n 17.   ''G''<sub>''fitness''</sub>(''F''-<nowiki/>{''f''<sub>''kill''</sub>}) &larr; ''Error''<sub>metrics</sub>(''p''<sub>''reference''</sub>, ''p''<sub>''estimated''</sub>)\n 18.    \n 19.   // Compare the performances, i.e. compute the fly's local fitness\n 20.   ''L''<sub>''fitness''</sub>(''f''<sub>''kill''</sub>) &larr; ''G''<sub>''fitness''</sub>(''F''-<nowiki/>{''f''<sub>''kill''</sub>}) - ''G''<sub>''fitness''</sub>(''F'')\n 21.    \n 22.   '''If''' the local fitness is greater than 0, // Thresholded-selection of a bad fly that can be killed\n 23.       '''then''' go to Step 26.   ''// f<sub>kill</sub> is a good fly (the population's performance is better when f<sub>kill</sub> is included): we should not kill it''\n 24.       '''else''' go to Step 28.   ''// f<sub>kill</sub> is a bad fly (the population's performance is worse when f<sub>kill</sub> is included): we can get rid of it''\n 25.    \n 26.   Restore the fly's contribution, then go to Step 12.\n 27.    \n 28.   Select a genetic operator\n 29.    \n 30.   '''If''' the genetic operator is mutation,\n 31.       '''then''' go to Step 34.\n 32.       '''else''' go to Step 50.\n 33.    \n 34.   ''f''<sub>''reproduce''</sub> &larr; Select a random fly of ''F''\n 35.    \n 14.   Remove ''f''<sub>''reproduce''</sub>'s contribution from ''p''<sub>''estimated''</sub>\n 37.    \n 38.   ''// Compute the population's performance without f<sub>reproduce</sub>''\n 39.   ''G''<sub>''fitness''</sub>(''F''-<nowiki/>{''f''<sub>''reproduce''</sub>}) &larr; ''Error''<sub>metrics</sub>(''p''<sub>''reference''</sub>, ''p''<sub>''estimated''</sub>)\n 40.    \n 41.   // Compare the performances, i.e. compute the fly's local fitness\n 42.   ''L''<sub>''fitness''</sub>(''f''<sub>''reproduce''</sub>) &larr; ''G''<sub>''fitness''</sub>(''F''-<nowiki/>{''f''<sub>''reproduce''</sub>}) - ''G''<sub>''fitness''</sub>(''F'')\n 43.    \n 44.   Restore the fly's contribution\n 45.    \n 46.   '''If''' the local fitness is lower than or equal to 0, // Thresholded-selection of a good fly that can reproduce\n 47.       '''else''' go to Step 34.   ''// f<sub>kill</sub> is a bad fly: we should not allow it to reproduce''\n 48.       '''then''' go to Step 53.   ''// f<sub>kill</sub> is a good fly: we can allow it to reproduce''\n 49.    \n 50.   ''// New blood / Immigration''\n 51.   Replace ''f''<sub>''kill''</sub> by a new fly with a random position, go to Step 57.\n 52.    \n 53.   ''// Mutation''\n 54.   Copy ''f''<sub>''reproduce''</sub> into ''f''<sub>''kill''</sub>\n 55.   Slightly and randomly alter ''f''<sub>''kill''</sub>'s position\n 56.    \n 57.   Add the new fly's contribution to the population\n 58.    \n 59.   '''If''' stop the reconstruction,\n 60.       '''then''' go to Step 63.\n 61.       '''else''' go to Step 10.\n 62.    \n 63.   ''// Extract solution''\n 64.   ''V''<sub>''F''</sub> &larr; voxelisation of ''F''\n 65.    \n 66.   '''return''' ''V''<sub>''F''</sub>\n    \n    '''END'''\n\n== Example: Digital arts ==\n<!-- [[File:Initial_random_fly_population.png|200x200px|thumb|left|Image of the initial population (random fly positions).]] -->\n\n{{multiple image\n | width = 200\n | right\n | image1=evolutionary_search_Y_Lliwedd_flies.gif\n | caption1=Evolutionary search.\n | image2=Y_Lliwedd_flies.png\n | caption2=Image reconstructed after optimisation using a set of stripes as the pattern for each tile.\n}}\n\nIn this example, an input image is to be approximated by a set of tiles (for example as in an ancient [[mosaic]]). A tile has an orientation (angle θ), a three colour components (R, G, B), a size (w, h) and a position (x, y, z). If there are ''N'' tiles, there are 9''N'' unknown floating point numbers to guess. In other words for 5,000 tiles, there are 45,000 numbers to find. Using a classical evolutionary algorithm where the answer of the optimisation problem is the best individual, the genome of an individual would made of 45,000 genes. This approach would be extremely costly in term of complexity and computing time. The same applies for any classical optimisation algorithm. Using the Fly Algorithm, every individual mimics a tile and can be individually evaluated using its local fitness to assess its contribution to the population's performance (the global fitness). Here an individual has 9 genes instead of 9''N'', and there are ''N'' individuals. It can be solved as a reconstruction problem as follows:\n\n<math>reconstruction = \\operatorname{arg\\,min} \\overset{x<W}{\\underset{y=0}{\\sum}}\\overset{j<H}{\\underset{j=0}{\\sum}}|input(x,y) - P[F](x,y)|</math>\n\nwhere <math>input</math> is the input image, <math>x</math> and <math>y</math> are the pixel coordinates along the horizontal and vertical axis respectively, <math>W</math> and <math>H</math> are the image width and height in number of pixels respectively, <math>F</math> is the fly population, and <math>P</math> is a projection operator that creates an image from flies. This projection operator <math>P</math> can take many forms. In her work, Z. Ali Aboodd <ref name=Abbood2017EvoIASP /> uses [[OpenGL]] to generate different effects (e.g. mosaics, or spray paint). For speeding up the evaluation of the fitness functions, [[OpenCL]] is used too.\nThe algorithm starts with a population <math>F</math> that is randomly generated (see Line 3 in the algorithm above). <math>F</math> is then assessed using the global fitness to compute <math>G_{fitness}(F) = \\overset{x<W}{\\underset{y=0}{\\sum}}\\overset{j<H}{\\underset{j=0}{\\sum}}|input(x,y) - P[F](x,y)|</math> (see Line 10). <math>G_{fitness}</math> is an error metrics, it has to be minimised.\n\n== See also ==\n\n* [[Mathematical optimization]]\n* [[Metaheuristic]]\n* [[Search algorithm]]\n* [[Stochastic optimization]]\n* [[Evolutionary computation]]\n* [[Evolutionary algorithm]]\n* [[Genetic algorithm]]\n* [[Mutation (genetic algorithm)]]\n* [[Crossover (genetic algorithm)]]\n* [[Selection (genetic algorithm)]]\n\n== References ==\n<!-- Inline citations added to your article will automatically display here. See https://en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. -->\n{{reflist}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Genetic algorithms]]\n[[Category:Evolutionary algorithms]]\n[[Category:Heuristics]]\n[[Category:Nature-inspired metaheuristics]]\n[[Category:Evolutionary computation]]"
    },
    {
      "title": "Fourier–Motzkin elimination",
      "url": "https://en.wikipedia.org/wiki/Fourier%E2%80%93Motzkin_elimination",
      "text": "{{expand German|topic=|otherarticle=|date=September 2013}}\n'''Fourier–Motzkin elimination''', also known as the '''FME method''', is a mathematical [[algorithm]] for eliminating variables from a [[system of linear inequalities]]. It can output [[real number|real]] solutions.\n\nThe algorithm is named after [[Joseph Fourier]]<ref>{{Cite book|chapter-url=http://gallica.bnf.fr/ark:/12148/bpt6k32227/f53|title=Mémoires de l'Académie des sciences de l'Institut de France|last=Fourier|first=Joseph|publisher=Gauthier-Villars|year=1827|volume=7|chapter=Histoire de l'Académie, partie mathématique (1824)|authorlink=Joseph Fourier}}</ref> and [[Theodore Motzkin]] who independently discovered the method in 1827 and in 1936, respectively.\n\n==Elimination==\nThe elimination of a set of variables, say ''V'', from a system of relations (here linear inequalities) refers to the creation of another system of the same sort, but without the variables in ''V'', such that both systems have the same solutions over the remaining variables.\n\nIf all variables are eliminated from a system of linear inequalities, then one obtains a system of constant inequalities. It is then trivial to decide whether the resulting system is true or false. It is true if and only if the original system has solutions. As a consequence, elimination of all variables can be used to detect whether a system of inequalities has solutions or not.\n\nConsider a system <math>S</math> of <math>n</math> inequalities with <math>r</math> variables <math>x_1</math> to <math>x_r</math>, with <math>x_r</math> the variable to be eliminated. The linear inequalities in the system can be grouped into three classes depending on the sign (positive, negative or null) of the coefficient for <math>x_r</math>.\n* those inequalities that are of the form <math>x_r \\geq b_i-\\sum_{k=1}^{r-1} a_{ik} x_k</math>; denote these by <math>x_r \\geq A_j(x_1, \\dots, x_{r-1})</math>, for <math>j</math> ranging from 1 to <math>n_A</math> where <math>n_A</math> is the number of such inequalities;\n* those inequalities that are of the form <math>x_r \\leq b_i-\\sum_{k=1}^{r-1} a_{ik} x_k</math>; denote these by <math>x_r \\leq B_j(x_1, \\dots, x_{r-1})</math>, for <math>j</math> ranging from 1 to <math>n_B</math> where <math>n_B</math> is the number of such inequalities;\n* those inequalities in which <math>x_r</math> plays no role, grouped into a single conjunction <math>\\phi</math>.\n\nThe original system is thus equivalent to\n:<math>\\max(A_1(x_1, \\dots, x_{r-1}), \\dots, A_{n_A}(x_1, \\dots, x_{r-1})) \\leq x_r \\leq \\min(B_1(x_1, \\dots, x_{r-1}), \\dots, B_{n_B}(x_1, \\dots, x_{r-1})) \\wedge \\phi</math>.\n\nElimination consists in producing a system equivalent to <math>\\exists x_r~S</math>. Obviously, this formula is equivalent to \n:<math>\\max(A_1(x_1, \\dots, x_{r-1}), \\dots, A_{n_A}(x_1, \\dots, x_{r-1})) \\leq \\min(B_1(x_1, \\dots, x_{r-1}), \\dots, B_{n_B}(x_1, \\dots, x_{r-1})) \\wedge \\phi</math>.\n\nThe inequality \n:<math>\\max(A_1(x_1, \\dots, x_{r-1}), \\dots, A_{n_A}(x_1, \\dots, x_{r-1})) \\leq \\min(B_1(x_1, \\dots, x_{r-1}), \\dots, B_{n_B}(x_1, \\dots, x_{r-1}))</math> \nis equivalent to <math>n_A n_B</math> inequalities <math>A_i(x_1, \\dots, x_{r-1}) \\leq B_j(x_1, \\dots, x_{r-1})</math>, for <math>1 \\leq i \\leq n_A</math> and <math>1 \\leq j \\leq n_B</math>.\n\nWe have therefore transformed the original system into another system where <math>x_r</math> is eliminated. Note that the output system has <math>(n-n_A-n_B)+n_A n_B</math> inequalities. In particular, if <math>n_A = n_B = n/2</math>, then the number of output inequalities is <math>n^2/4</math>.\n\n==Example==\nConsider the following system of inequalities:<ref name=\"gm06\">{{Cite Gartner Matousek 2006}} Pages 81–104.</ref>{{Rp|100–102}}\n\n2''x'' − 5''y'' + 4''z'' ≤ 10\n\n3''x'' − 6''y'' + 3''z'' ≤ 9\n\n−''x'' + 5''y'' − 2''z'' ≤ −7\n\n−3''x'' + 2''y'' + 6''z'' ≤ 12\n\nTo eliminate ''x'', we can write the inequalities in terms of ''x'':\n\n''x''  ≤ (10 + 5''y'' − 4''z'')/''2''\n\n''x''  ≤ (9 + 6''y'' − 3''z'')/3\n\n''x''  ≥ 7 + 5''y'' − 2''z''\n\n''x''  ≥ (−12 ''+ 2y + 6z'')/3\n\nWe have two inequalities with \"≤\" and two with \"≥\"; the system has a solution iff the right-hand side of each \"≤\" inequality is at least the right-hand side of each \"≥\" inequality. We have 2*2 such combinations:\n\n''7 + 5y − 2z''  ≤ (10 + 5''y'' − 4''z'')/''2''\n\n''7 + 5y − 2z''  ≤ (9 + 6''y'' − 3''z'')/3\n\n''(−12 + 2y + 6z)/3''  ≤ (10 + 5''y'' − 4''z'')/''2''\n\n''(−12 + 2y + 6z)/3''  ≤ (9 + 6''y'' − 3''z'')/3\n\nWe now have a new system of inequalities, with one fewer variable.\n\n==Complexity==\nRunning an elimination step over <math>n</math> inequalities can result in at most <math>n^2/4</math> inequalities in the output, thus running <math>d</math> successive steps can result in at most <math>4(n/4)^{2^d}</math>, a double exponential complexity. This is due to the algorithm producing many unnecessary constraints (constraints that are implied by other constraints). The number of necessary constraints grows as a single exponential.<ref>David Monniaux, [http://hal.archives-ouvertes.fr/hal-00472831 ''Quantifier elimination by lazy model enumeration''], Computer aided verification (CAV) 2010.</ref> Unnecessary constraints may be detected using [[linear programming]].\n\n== Imbert's acceleration theorems ==\n\nTwo \"acceleration\" theorems due to Imbert<ref>Jean-Louis Imbert, ''About Redundant Inequalities Generated by Fourier's Algorithm'', Artificial Intelligence IV: Methodology, Systems, Applications, 1990.</ref> permit the elimination of  redundant inequalities based solely on syntactic properties of the formula derivation tree, thus curtailing the need to solve linear programs or compute matrix ranks.\n\nDefine the ''history'' <math>H_i</math> of an inequality <math>i</math> as the set of indexes of inequalities from the initial system <math>S</math> used to produce <math>i</math>. Thus, <math>H_i=\\{i\\}</math> for inequalities <math>i \\in S</math> of the initial system. When adding a new inequality <math>k: A_i(x_1, \\dots, x_{r-1}) \\leq B_j(x_1, \\dots, x_{r-1})</math> (by eliminating <math>x_r</math>), the new history <math>H_k</math> is constructed as <math>H_k = H_i \\cup H_j</math>.\n\nSuppose that the variables <math>O_k = \\{x_{r}, \\ldots, x_{r - k + 1}\\}</math> have been ''officially'' eliminated. Each inequality <math>i</math> partitions the set <math>O_k</math> into <math>E_i \\cup I_i \\cup R_i</math>:\n* <math>E_i</math>, the set of ''effectively eliminated'' variables, ''i.e.'' on purpose. A variable <math>x_j</math> is in the set as soon as at least one inequality in the history <math>H_i</math> of <math>i</math> results from the elimination of <math>x_j</math>.\n* <math>I_i</math>, the set of ''implicitly eliminated'' variables, ''i.e.'' by accident. A variable is implicitly eliminated when it appears in at least one inequality of <math>H_i</math>, but appears neither in inequality <math>i</math> nor in <math>E_i</math>\n*<math>R_i</math>,  all remaining variables.\n\nA non-redundant inequality has the property that its history is ''minimal''.<ref name=\"Imbert2\">Jean-Louis Imbert, [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.493&rep=rep1&type=pdf ''Fourier Elimination: Which to Choose?''].</ref>\n\n'''Theorem (Imbert's first acceleration theorem).''' If the history <math>H_i</math> of an inequality <math>i</math> is minimal, then <math> 1 + |E_i| \\ \\leq \\ |H_i| \\ \\leq 1 + \\left| E_i \\cup (I_i \\cap O_k)\\right|</math>.\n\nAn inequality that does not satisfy these bounds is necessarily redundant, and can be removed from the system without changing its solution set.\n\nThe second acceleration theorem detects minimal history sets:\n\n'''Theorem (Imbert's second acceleration theorem).''' If the inequality <math>i</math> is such that <math>1 + |E_i| = |H_i|</math>, then <math>H_i</math> is minimal.\n\nThis theorem provides a quick detection criterion and is used in practice to avoid more costly checks, such as those based on matrix ranks. See the reference for implementation details.<ref name=\"Imbert2\"/>\n\n== Applications in information theory ==\n\n[[Information theory|Information-theoretic]] achievability proofs result in conditions under which the existence of a well-performing coding scheme is guaranteed. These conditions are often described by linear system of inequalities. The variables of the system include both the transmission rates (that are part of the problem's formulation) and additional auxiliary rates used for the design of the scheme. Commonly, one aims to describe the fundamental limits of communication in terms of the problem's parameters only. This gives rise to the need of eliminating the aforementioned auxiliary rates, which is executed via Fourier–Motzkin elimination. However, the elimination process results in a new system that possibly contains more inequalities than the original. Yet, often some of the inequalities in the reduced system are redundant. Redundancy may be implied by other inequalities or by [[inequalities in information theory]] (a.k.a. Shannon type inequalities). A recently developed open-source [http://www.ee.bgu.ac.il/~fmeit/ software for MATLAB]<ref>{{Cite arxiv|last=Gattegno|first=Ido B.|last2=Goldfeld|first2=Ziv|last3=Permuter|first3=Haim H.|date=2015-09-25|title=Fourier-Motzkin Elimination Software for Information Theoretic Inequalities|eprint=1610.03990|class=cs.IT}}</ref> performs the elimination, while identifying and removing redundant inequalities.  Consequently, the software's outputs a simplified system (without redundancies) that involves the communication rates only.\n\nRedundant constraint can be identified by solving a linear program as follows. Given a linear constraints system, if the <math>i</math>-th inequality is satisfied for any solution of all other inequalities, then it is redundant. Similarly, STIs refers to inequalities that are implied by the non-negativity of information theoretic measures and basic identities they satisfy. For instance, the STI <math>I(X_1;X_2) \\leq H(X_1) </math> is a consequence of the identity <math> I(X_1;X_2) = H(X_1) - H(X_1 | X_2)</math> and the non-negativity of conditional entropy, i.e., <math>H(X_1|X_2) \\geq 0</math>. Shannon-type inequalities define a [[Convex cone|cone]] in <math>\\mathbb R^{2^n-1}</math>, where <math>n</math> is the number of random variables appearing in the involved information measures. Consequently, any STI can be proven via linear programming by checking if it is implied by the basic identities and non-negativity constraints. The described algorithm first performs Fourier–Motzkin elimination to remove the auxiliary rates. Then, it imposes the information theoretic non-negativity constraints on the reduced output system and removes redundant inequalities.\n\n==See also==\n*[[Farkas' lemma]] – can be proved using FM elimination.\n* [[Real closed field]] – the cylindrical algebraic decomposition algorithm performs quantifier elimination over ''polynomial'' inequalities, not just linear.\n== References ==\n<references/>\n\n== Further reading ==\n\n*{{Cite book |last=Schrijver |first=Alexander|authorlink=Alexander Schrijver |title=Theory of Linear and Integer Programming |location= |publisher=John Wiley & sons |year=1998 |isbn=978-0-471-98232-6 |pages=155–156 }}\n*{{Cite web |last=Keßler |first=Christoph W. |title=Parallel Fourier–Motzkin Elimination |work=Universität Trier |citeseerx = 10.1.1.54.657 }}\n*{{Cite journal |last=Williams |first=H. P. |title=Fourier's Method of Linear Programming and its Dual |journal=American Mathematical Monthly |volume=93 |year=1986 |issue= 9|pages=681–695 |doi=10.2307/2322281 |jstor=2322281 }}\n\n== External links ==\n* [http://www.fmf.uni-lj.si/~lavric/lauritzen.pdf Lectures on Convex Sets], notes by Niels Lauritzen, at [[Aarhus University]], March 2010.\n* [http://www.ee.bgu.ac.il/~fmeit/ FME software for Information theory], open-source code in MATLAB by Ido B. Gattegno, Ziv Goldfeld and Haim H. Permuter.\n\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Fourier-Motzkin elimination}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Real algebraic geometry]]"
    },
    {
      "title": "Fractional programming",
      "url": "https://en.wikipedia.org/wiki/Fractional_programming",
      "text": "In [[mathematical optimization]], '''fractional programming''' is a generalization of [[linear-fractional programming]]. The [[objective function]] in a fractional program is a ratio of two functions that are in general nonlinear. The ratio to be optimized often describes some kind of efficiency of a system.\n\n==Definition==\n\nLet <math>f, g, h_j, j=1, \\ldots, m</math> be [[real-valued function]]s defined on a set <math>\\mathbf{S}_0 \\subset \\mathbb{R}^n</math>. Let <math>\\mathbf{S} = \\{\\boldsymbol{x} \\in \\mathbf{S}_0: h_j(\\boldsymbol{x}) \\leq 0, j=1, \\ldots, m\\}</math>. The [[nonlinear programming|nonlinear program]]\n\n:<math>\n\\underset{\\boldsymbol{x} \\in \\mathbf{S}}{\\text{maximize}} \\quad \\frac{f(\\boldsymbol{x})}{g(\\boldsymbol{x})},\n</math>\n\nwhere <math>g(\\boldsymbol{x}) > 0</math> on <math>\\mathbf{S}</math>, is called a fractional program.\n\n==Concave fractional programs==\n\nA fractional program in which ''f'' is nonnegative and concave, ''g'' is positive and convex, and '''S''' is a [[convex set]] is called a '''concave fractional program'''. If ''g'' is affine, ''f'' does not have to be restricted in sign. The linear fractional program is a special case of a concave fractional program where all functions <math>f, g, h_j, j=1, \\ldots, m</math> are affine.\n\n===Properties===\n\nThe function <math>q(\\boldsymbol{x}) = f(\\boldsymbol{x}) / g(\\boldsymbol{x})</math> is semistrictly [[Quasiconcave function|quasiconcave]] on '''S'''. If ''f'' and ''g'' are differentiable, then ''q'' is [[Pseudoconcave function|pseudoconcave]]. In a linear fractional program, the objective function is [[Pseudolinear function|pseudolinear]].\n\n===Transformation to a concave program===\n\nBy the transformation <math>\\boldsymbol{y} = \\frac{\\boldsymbol{x}}{g(\\boldsymbol{x})}; t = \\frac{1}{g(\\boldsymbol{x})}</math>, any concave fractional program can be transformed to the equivalent parameter-free [[concave program]] <ref>{{cite journal|last1=Schaible |first1=Siegfried |title=Parameter-free Convex Equivalent and Dual Programs|journal=Zeitschrift für Operations Research |volume=18 |year=1974 |number=5 |pages=187–196|ref=harv|doi=10.1007/BF02026600|mr=351464}}</ref>\n\n:<math>\n\\begin{align}\n\\underset{\\frac{\\boldsymbol{y}}{t} \\in \\mathbf{S}_0}{\\text{maximize}} \\quad & t f\\left(\\frac{\\boldsymbol{y}}{t}\\right) \\\\\n\\text{subject to} \\quad & t g\\left(\\frac{\\boldsymbol{y}}{t}\\right) \\leq 1, \\\\\n& t \\geq 0.\n\\end{align}\n</math>\n\nIf ''g'' is affine, the first constraint is changed to <math>t g(\\frac{\\boldsymbol{y}}{t}) = 1</math> and the assumption that ''f'' is nonnegative may be dropped.\n\n===Duality===\n\nThe Lagrangian dual of the equivalent concave program is\n:<math>\n\\begin{align}\n\\underset{\\boldsymbol{u}}{\\text{minimize}} \\quad & \\underset{\\boldsymbol{x} \\in \\mathbf{S}_0}{\\operatorname{sup}} \\frac{f(\\boldsymbol{x}) - \\boldsymbol{u}^T \\boldsymbol{h}(\\boldsymbol{x})}{g(\\boldsymbol{x})} \\\\\n\\text{subject to} \\quad & u_i \\geq 0, \\quad i = 1,\\dots,m.\n\\end{align}\n</math>\n\n== Notes ==\n\n<references/>\n\n==References==\n\n* {{cite book |last1=Avriel |first1=Mordecai |last2=Diewert |first2=Walter E. |last3=Schaible |first3=Siegfried |last4=Zang |first4=Israel |title=Generalized Concavity |publisher=Plenum Press |year=1988}}\n* {{cite journal |title=Fractional programming |last1=Schaible |first1=Siegfried |journal=Zeitschrift für Operations Research |volume=27 |year=1983 |pages=39–54 |doi=10.1007/bf01916898}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Gauss–Newton algorithm",
      "url": "https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm",
      "text": "[[File:Regression pic assymetrique.gif|thumb|300px|Fitting of a noisy curve by an asymmetrical peak model, using the Gauss–Newton algorithm with variable damping factor α. <br /> Top: raw data and model.<br /> Bottom: evolution of the normalised sum of the squares of the errors.]]\n\nThe '''Gauss–Newton algorithm''' is used to solve [[non-linear least squares]] problems. It is a modification of [[newton's method in optimization|Newton's method]] for finding a [[maxima and minima|minimum]] of a [[function (mathematics)|function]]. Unlike Newton's method, the Gauss–Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.<ref>{{cite book |first=Ron C. |last=Mittelhammer |first2=Douglas J. |last2=Miller |first3=George G. |last3=Judge |title=Econometric Foundations |location=Cambridge |publisher=Cambridge University Press |year=2000 |isbn=0-521-62394-4 |pages=197–198 |url=https://books.google.com/books?id=fycmsfkK6RQC&pg=PA197 }}</ref>\n\nNon-linear least squares problems arise, for instance, in [[non-linear regression]], where parameters in a model are sought such that the model is in good agreement with available observations.\n\nThe method is named after the mathematicians [[Carl Friedrich Gauss]] and [[Isaac Newton]], and first appeared in Gauss' 1809 work ''Theoria motus corporum coelestium in sectionibus conicis solem ambientum''.<ref name=optimizationEncyc>{{Cite book| last = Floudas | first = Christodoulos A. | last2=Pardalos |first2=Panos M.|title = Encyclopedia of Optimization  | publisher = Springer | year = 2008 | page = 1130 | isbn = 9780387747583}}</ref>\n\n== Description ==\nGiven ''m'' functions '''r''' = (''r''<sub>1</sub>, …, ''r''<sub>''m''</sub>) (often called residuals) of ''n'' variables '''''β'''''&nbsp;=&nbsp;(''β''<sub>1</sub>, …, ''β''<sub>''n''</sub>), with ''m''&nbsp;≥&nbsp;''n'', the Gauss–Newton algorithm [[iterative method|iteratively]] finds the value of the variables that minimizes the sum of squares<ref name=ab>Björck (1996)</ref>\n:<math> S(\\boldsymbol \\beta) = \\sum_{i=1}^m r_i^2(\\boldsymbol \\beta).</math>\n\nStarting with an initial guess <math>\\boldsymbol \\beta^{(0)}</math> for the minimum, the method proceeds by the iterations\n:<math> \\boldsymbol \\beta^{(s+1)} = \\boldsymbol \\beta^{(s)} - \\left(\\mathbf{J_r}^\\mathsf{T} \\mathbf{J_r} \\right)^{-1} \\mathbf{J_r}^\\mathsf{T} \\mathbf{r}\\left(\\boldsymbol \\beta^{(s)}\\right), </math>\n\nwhere, if '''r''' and '''''β''''' are [[column vectors]], the entries of the [[Jacobian matrix]] are\n:<math> \\left(\\mathbf{J_r}\\right)_{ij} = \\frac{\\partial r_i \\left(\\boldsymbol \\beta^{(s)}\\right)}{\\partial \\beta_j},</math>\n\nand the symbol <math>^\\mathsf{T}</math> denotes the [[matrix transpose]].\n\nIf ''m''&nbsp;=&nbsp;''n'', the iteration simplifies to\n\n:<math> \\boldsymbol \\beta^{(s+1)} = \\boldsymbol \\beta^{(s)} - \\left(\\mathbf{J_r}\\right)^{-1} \\mathbf{r}\\left(\\boldsymbol \\beta^{(s)}\\right),</math>\n\nwhich is a direct generalization of [[Newton's method]] in one dimension.\n\nIn data fitting, where the goal is to find the parameters '''''β''''' such that a given model function ''y''&nbsp;=&nbsp;''f''(''x'', '''''β''''') best fits some data points (''x''<sub>''i''</sub>, ''y''<sub>''i''</sub>), the functions ''r''<sub>''i''</sub> are the [[residual (statistics)|residuals]]:\n: <math>r_i(\\boldsymbol \\beta) = y_i - f\\left(x_i, \\boldsymbol \\beta\\right).</math>\n\nThen, the Gauss–Newton method can be expressed in terms of the Jacobian '''J'''<sub>'''f'''</sub> of the function ''f'' as\n:<math> \\boldsymbol \\beta^{(s+1)} = \\boldsymbol \\beta^{(s)} + \\left(\\mathbf{J_f}^\\mathsf{T} \\mathbf{J_f} \\right)^{-1} \\mathbf{J_f}^\\mathsf{T} \\mathbf{r}\\left(\\boldsymbol \\beta^{(s)}\\right). </math>\n\nNote that <math>\\left(\\mathbf{J_f}^\\mathsf{T} \\mathbf{J_f}\\right)^{-1} \\mathbf{J_f}^\\mathsf{T}</math> is the left [[Moore–Penrose pseudoinverse|pseudoinverse]] of <math>\\mathbf{J_f}</math>.\n\n==Notes==\n\nThe assumption ''m''&nbsp;≥&nbsp;''n'' in the algorithm statement is necessary, as otherwise the matrix '''J<sub>r</sub>'''<sup>T</sup>'''J<sub>r</sub>''' is not invertible and the normal equations cannot be solved (at least uniquely).\n\nThe Gauss–Newton algorithm can be derived by [[linear approximation|linearly approximating]] the vector of functions ''r''<sub>''i''</sub>. Using [[Taylor's theorem]], we can write at every iteration:\n\n: <math>\\mathbf{r}(\\boldsymbol \\beta) \\approx \\mathbf{r}\\left(\\boldsymbol \\beta^{(s)}\\right) + \\mathbf{J_r}\\left(\\boldsymbol \\beta^{(s)}\\right)\\Delta</math>\n\nwith <math>\\Delta = \\boldsymbol \\beta - \\boldsymbol \\beta^{(s)}</math>. The task of finding Δ minimizing the sum of squares of the right-hand side; i.e.,\n: <math>\\min \\left\\|\\mathbf{r}\\left(\\boldsymbol \\beta^{(s)}\\right) + \\mathbf{J_r}\\left(\\boldsymbol \\beta^{(s)}\\right)\\Delta\\right\\|_2^2,</math>\n\nis a [[linear least squares (mathematics)|linear least-squares]] problem, which can be solved explicitly, yielding the normal equations in the algorithm.\n\nThe normal equations are ''n'' simultaneous linear equations in the unknown increments Δ. They may be solved in one step, using [[Cholesky decomposition]], or, better, the [[QR factorization]] of '''J'''<sub>'''r'''</sub>. For large systems, an [[iterative method]], such as the [[conjugate gradient]] method, may be more efficient. If there is a linear dependence between columns of '''J'''<sub>'''r'''</sub>, the iterations will fail, as '''J<sub>r</sub>'''<sup>T</sup>'''J<sub>r</sub>''' becomes singular.\n\n==Example==\n[[File:Gauss Newton illustration.png|thumb|right|280px|Calculated curve obtained with <math>\\hat\\beta_1 = 0.362</math> and <math>\\hat\\beta_2 = 0.556</math> (in blue) versus the observed data (in red).]]\nIn this example, the Gauss–Newton algorithm will be used to fit a model to some data by minimizing the sum of squares of errors between the data and model's predictions.\n\nIn a biology experiment studying the relation between substrate concentration {{math|[''S'']}} and reaction rate in an enzyme-mediated reaction, the data in the following table were obtained. \n:{|class=\"wikitable\" style=\"text-align: center;\" \n! {{mvar|i}} \n| 1 || 2 || 3 || 4 || 5 || 6 || 7 \n|-\n! {{math|[''S'']}}\n| 0.038 || 0.194 || 0.425 || 0.626  || 1.253  || 2.500  || 3.740\n|-\n! Rate \n| 0.050 || 0.127 || 0.094 || 0.2122 || 0.2729 || 0.2665 || 0.3317\n|}\n\nIt is desired to find a curve (model function) of the form\n:<math>\\text{rate} = \\frac{V_\\text{max}[S]}{K_M + [S]}</math>\n\nthat fits best the data in the least-squares sense, with the parameters <math>V_\\text{max}</math> and <math>K_M</math> to be determined.\n\nDenote by <math>x_i</math> and <math>y_i</math> the value of {{math|[''S'']}} and the rate from the table, <math>i = 1, \\dots, 7</math>. Let <math>\\beta_1 = V_\\text{max}</math> and <math>\\beta_2 = K_M</math>. We will find <math>\\beta_1</math> and <math>\\beta_2</math> such that the sum of squares of the residuals\n:<math>r_i = y_i - \\frac{\\beta_1 x_i}{\\beta_2 + x_i} \\quad (i = 1, \\dots, 7)</math>\n\nis minimized.\n\nThe Jacobian <math>\\mathbf{J_r}</math> of the vector of residuals <math>r_i</math> with respect to the unknowns <math>\\beta_j</math> is a <math>7 \\times 2</math> matrix with the <math>i</math>-th row having the entries\n:<math>\\frac{\\partial r_i}{\\partial \\beta_1} = -\\frac{x_i}{\\beta_2 + x_i}; \\frac{\\partial r_i}{\\partial \\beta_2} = \\frac{\\beta_1 x_i}{\\left(\\beta_2 + x_i\\right)^2}.</math>\n\nStarting with the initial estimates of <math>\\beta_1 = 0.9</math> and <math>\\beta_2 = 0.2</math>, after five iterations of the Gauss–Newton algorithm the optimal values <math>\\hat\\beta_1 = 0.362</math> and <math>\\hat\\beta_2 = 0.556</math> are obtained. The sum of squares of residuals decreased from the initial value of 1.445 to 0.00784 after the fifth iteration. The plot in the figure on the right shows the curve determined by the model for the optimal parameters with the observed data.\n\n==Convergence properties==\n\nIt can be shown<ref>Björck (1996), p. 260.</ref> that the increment &Delta; is a [[descent direction]] for ''S'', and, if the algorithm converges, then the limit is a [[stationary point]] of ''S''. However, convergence is not guaranteed, not even [[local convergence]] as in [[Newton's method in optimization|Newton's method]], or convergence under the usual Wolfe conditions.<ref>{{citation |title=The divergence of the BFGS and Gauss Newton Methods |last1=Mascarenhas |journal=Mathematical Programming |date=2013 |volume=147 |issue=1 |pages=253–276 |doi=10.1007/s10107-013-0720-6 |arxiv=1309.7922}}</ref>\n\nThe rate of convergence of the Gauss–Newton algorithm can approach [[rate of convergence|quadratic]].<ref>Björck (1996), p. 341, 342.</ref> The algorithm may converge slowly or not at all if the initial guess is far from the minimum or the matrix <math>\\mathbf{J_r^\\mathsf{T} J_r}</math> is [[ill-conditioned]].  For example, consider the problem with <math>m = 2</math> equations and <math>n = 1</math> variable, given by\n:<math>\\begin{align}\n  r_1(\\beta) &= \\beta + 1, \\\\\n  r_2(\\beta) &= \\lambda \\beta^2 + \\beta - 1.\n\\end{align} </math>\n\nThe optimum is at <math>\\beta = 0</math>. (Actually the optimum is at <math>\\beta = -1</math> for <math>\\lambda = 2</math>, because <math>S(0) = 1^2 + (-1)^2 = 2</math>, but <math>S(-1) = 0</math>.) If <math>\\lambda = 0</math>, then the problem is in fact linear and the method finds the optimum in one iteration. If |λ| < 1, then the method converges linearly and the error decreases asymptotically with a factor |λ| at every iteration. However, if |λ| > 1, then the method does not even converge locally.<ref>Fletcher (1987), p. 113.</ref>\n\n== Derivation from Newton's method ==\n\nIn what follows, the Gauss–Newton algorithm will be derived from [[Newton's method in optimization|Newton's method]] for function optimization via an approximation. As a consequence, the rate of convergence of the Gauss–Newton algorithm can be quadratic under certain regularity conditions. In general (under weaker conditions), the convergence rate is linear.<ref>http://www.henley.ac.uk/web/FILES/maths/09-04.pdf</ref>\n\nThe recurrence relation for Newton's method for minimizing a function ''S'' of parameters <math>\\boldsymbol\\beta</math> is\n:<math> \\boldsymbol\\beta^{(s+1)} = \\boldsymbol\\beta^{(s)} - \\mathbf H^{-1} \\mathbf g,</math>\n\nwhere '''g''' denotes the [[gradient|gradient vector]] of ''S'', and '''H''' denotes the [[Hessian matrix]] of ''S''.\n\nSince <math>S = \\sum_{i=1}^m r_i^2</math>, the gradient is given by\n:<math>g_j = 2 \\sum_{i=1}^m r_i \\frac{\\partial r_i}{\\partial \\beta_j}.</math>\n\nElements of the Hessian are calculated by differentiating the gradient elements, <math>g_j</math>, with respect to <math>\\beta_k</math>:\n:<math>H_{jk} = 2 \\sum_{i=1}^m \\left(\\frac{\\partial r_i}{\\partial \\beta_j} \\frac{\\partial r_i}{\\partial \\beta_k} + r_i \\frac{\\partial^2 r_i}{\\partial \\beta_j \\partial \\beta_k}\\right).</math>\n\nThe Gauss–Newton method is obtained by ignoring the second-order derivative terms (the second term in this expression). That is, the Hessian is approximated by\n:<math>H_{jk} \\approx 2 \\sum_{i=1}^m J_{ij} J_{ik},</math>\n\nwhere <math>J_{ij} = \\frac{\\partial r_i}{\\partial \\beta_j}</math> are entries of the Jacobian '''J<sub>r</sub>'''. The gradient and the approximate Hessian can be written in matrix notation as\n:<math>\\mathbf{g} = 2 {\\mathbf{J}_\\mathbf{r}}^\\mathsf{T} \\mathbf{r}, \\quad \\mathbf{H} \\approx 2 {\\mathbf{J}_\\mathbf{r}}^\\mathsf{T} \\mathbf{J_r}.</math>\n\nThese expressions are substituted into the recurrence relation above to obtain the operational equations\n:<math> \\boldsymbol{\\beta}^{(s+1)} = \\boldsymbol\\beta^{(s)} + \\Delta; \\quad \\Delta = -\\left(\\mathbf{J_r}^\\mathsf{T} \\mathbf{J_r}\\right)^{-1} \\mathbf{J_r}^\\mathsf{T} \\mathbf{r}.</math>\n\nConvergence of the Gauss–Newton method is not guaranteed in all instances. The approximation\n:<math>\\left|r_i \\frac{\\partial^2 r_i}{\\partial \\beta_j \\partial \\beta_k}\\right| \\ll \\left|\\frac{\\partial r_i}{\\partial \\beta_j} \\frac{\\partial r_i}{\\partial \\beta_k}\\right|</math>\n\nthat needs to hold to be able to ignore the second-order derivative terms may be valid in two cases, for which convergence is to be expected:<ref>Nocedal (1999), p. 259.</ref>\n# The function values <math>r_i</math> are small in magnitude, at least around the minimum. \n# The functions are only \"mildly\" nonlinear, so that <math>\\frac{\\partial^2 r_i}{\\partial \\beta_j \\partial \\beta_k}</math> is relatively small in magnitude.\n\n== Improved versions ==\n\nWith the Gauss–Newton method the sum of squares of the residuals ''S'' may not decrease at every iteration. However, since &Delta; is a descent direction, unless <math>S\\left(\\boldsymbol \\beta^s\\right)</math> is a stationary point, it holds that <math>S\\left(\\boldsymbol \\beta^s + \\alpha\\Delta\\right) < S\\left(\\boldsymbol \\beta^s\\right)</math> for all sufficiently small <math>\\alpha>0</math>. Thus, if divergence occurs, one solution is to employ a fraction <math>\\alpha</math> of the increment vector Δ in the updating formula:\n:<math> \\boldsymbol \\beta^{s+1} = \\boldsymbol \\beta^s + \\alpha \\Delta.</math>.\n\nIn other words, the increment vector is too long, but it points in \"downhill\", so going just a part of the way will decrease the objective function ''S''. An optimal value for <math>\\alpha</math> can be found by using a [[line search]] algorithm, that is, the magnitude of <math>\\alpha</math> is determined by finding the value that minimizes ''S'', usually using a [[line search|direct search method]] in the interval <math>0 < \\alpha < 1</math>or a [[backtracking line search]] such as [[Backtracking line search|Armijo-line search]]. Typically, <math>\\alpha</math>should be chosen such that it satisfies the [[Wolfe conditions|Wolfe condtions]] or the [[Goldstein conditions|Goldstein condition]]<nowiki/>s. <ref>{{Cite book|url=https://www.worldcat.org/oclc/54849297|title=Numerical optimization|last=Nocedal, Jorge.|date=1999|publisher=Springer|others=Wright, Stephen J., 1960-|isbn=0387227423|location=New York|oclc=54849297}}</ref> \n\nIn cases where the direction of the shift vector is such that the optimal fraction α is close to zero, an alternative method for handling divergence is the use of the [[Levenberg–Marquardt algorithm]], also known as the \"[[trust region]] method\".<ref name=\"ab\"/> The normal equations are modified in such a way that the increment vector is rotated towards the direction of [[steepest descent]],    \n:<math>\\left(\\mathbf{J^\\mathsf{T} J + \\lambda D}\\right) \\Delta = -\\mathbf{J}^\\mathsf{T} \\mathbf{r},</math>\n\nwhere '''D''' is a positive diagonal matrix. Note that when '''D''' is the identity matrix '''I''' and <math>\\lambda \\to +\\infty</math>, then <math>\\lambda \\Delta = \\lambda \\left(\\mathbf{J^\\mathsf{T} J} + \\lambda \\mathbf{I}\\right)^{-1} \\left(-\\mathbf{J}^\\mathsf{T} \\mathbf{r}\\right) = \\left(\\mathbf{I} - \\mathbf{J^\\mathsf{T} J} / \\lambda + \\cdots \\right) \\left(-\\mathbf{J}^\\mathsf{T} \\mathbf{r}\\right) \\to -\\mathbf{J}^\\mathsf{T} \\mathbf{r}</math>, therefore the [[Direction (geometry, geography)|direction]] of Δ approaches the direction of the negative gradient <math>-\\mathbf{J}^\\mathsf{T} \\mathbf{r}</math>.\n\nThe so-called Marquardt parameter <math>\\lambda</math> may also be optimized by a line search, but this is inefficient, as the shift vector must be recalculated every time <math>\\lambda</math> is changed. A more efficient strategy is this: When divergence occurs, increase the Marquardt parameter until there is a decrease in ''S''. Then retain the value from one iteration to the next, but decrease it if possible until a cut-off value is reached, when the Marquardt parameter can be set to zero; the minimization of ''S'' then becomes a standard Gauss–Newton minimization.\n\n==Large-scale optimization==\n\nFor large-scale optimization, the Gauss–Newton method is of special interest because it is often (though certainly not always) true that the matrix <math>\\mathbf{J}_\\mathbf{r}</math> is more [[Sparse matrix|sparse]] than the approximate Hessian <math>\\mathbf{J}_\\mathbf{r}^\\mathsf{T} \\mathbf{J_r}</math>. In such cases, the step calculation itself will typically need to be done with an approximate iterative method appropriate for large and sparse problems, such as the [[conjugate gradient method]].\n\nIn order to make this kind of approach work, one needs at least an efficient method for computing the product\n:<math>{\\mathbf{J}_\\mathbf{r}}^\\mathsf{T} \\mathbf{J_r} \\mathbf{p}</math>\n\nfor some vector '''p'''. With [[sparse matrix]] storage, it is in general practical to store the rows of <math>\\mathbf{J}_\\mathbf{r}</math> in a compressed form (e.g., without zero entries), making a direct computation of the above product tricky due to the transposition. However, if one defines '''c'''<sub>''i''</sub> as row ''i'' of the matrix <math>\\mathbf{J}_\\mathbf{r}</math>, the following simple relation holds:\n:<math>{\\mathbf{J}_\\mathbf{r}}^\\mathsf{T}\\mathbf{J_r} \\mathbf{p} = \\sum_i \\mathbf c_i \\left(\\mathbf c_i \\cdot \\mathbf{p}\\right),</math>\n\nso that every row contributes additively and independently to the product. In addition to respecting a practical sparse storage structure, this expression is well suited for [[Parallel computing|parallel computations]]. Note that every row '''c'''<sub>''i''</sub> is the gradient of the corresponding residual ''r''<sub>''i''</sub>; with this in mind, the formula above emphasizes the fact that residuals contribute to the problem independently of each other.\n\n== Related algorithms ==\n\nIn a [[quasi-Newton method]], such as that due to [[Davidon–Fletcher–Powell formula|Davidon, Fletcher and Powell]] or Broyden–Fletcher–Goldfarb–Shanno ([[BFGS method]]) an estimate of the full Hessian <math>\\frac{\\partial^2 S}{\\partial \\beta_j \\partial\\beta_k}</math> is built up numerically using first derivatives <math>\\frac{\\partial r_i}{\\partial\\beta_j}</math> only so that after ''n'' refinement cycles the method closely approximates to Newton's method in performance. Note that quasi-Newton methods can minimize general real-valued functions, whereas Gauss–Newton, Levenberg–Marquardt, etc. fits only to nonlinear least-squares problems.\n\nAnother method for solving minimization problems using only first derivatives is [[gradient descent]]. However, this method does not take into account the second derivatives even approximately. Consequently, it is highly inefficient for many functions, especially if the parameters have strong interactions.\n\n==Notes==\n<references />\n\n==References==\n*{{cite book\n | last       = Björck | first      = A.\n | title      = Numerical methods for least squares problems\n | publisher  = SIAM, Philadelphia  | year       = 1996  | isbn       = 0-89871-360-9 }}\n* {{Cite book | last1=Fletcher | first1=Roger | title=Practical methods of optimization | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0-471-91547-8 | year=1987 }}. \n*{{cite book\n | last       = Nocedal  | first      = Jorge  |author2=Wright, Stephen \n | title      = Numerical optimization\n | publisher  = New York: Springer  | year       = 1999  | isbn       = 0-387-98793-2 }}\n\n== External links ==\n\n=== Implementations ===\n*[https://www.artelys.com/en/optimization-tools/knitro Artelys Knitro] is a non-linear solver with an implementation of the Gauss–Newton method. It is written in C and has interfaces to C++/C#/Java/Python/MATLAB/R.\n\n{{Isaac Newton}}\n{{Least Squares and Regression Analysis}}\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Gauss-Newton algorithm}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Least squares]]\n[[Category:Statistical algorithms]]"
    },
    {
      "title": "Genetic algorithms in economics",
      "url": "https://en.wikipedia.org/wiki/Genetic_algorithms_in_economics",
      "text": "{{Main|genetic algorithm}}\n\n'''[[Genetic algorithm]]s''' have increasingly been applied to economics since the pioneering work by John H. Miller in 1986. It has been used to characterize a variety of models including the [[cobweb model]], the [[overlapping generations model]], [[game theory]], [[Genetic algorithm scheduling|schedule optimization]] and [[asset pricing]]. Specifically, it has been used as a model to represent learning, rather than as a means for fitting a model.\n\n== Genetic algorithm in the cobweb model ==\n\nThe [[cobweb model]] is a simple supply and demand model for a good over ''t'' periods. Firms (agents) make a production quantity decision in a given period, however their output is not produced until the following period. Thus, the firms are going to have to use some sort of method to forecast what the future price will be. The GA is used as a sort of learning behaviour for the firms. Initially their quantity production decisions are random, however each period they learn a little more. The result is the agents converge within the area of the [[rational expectations]] (RATEX) equilibrium for the stable and unstable case. If the election operator is used, the GA converges exactly to the RATEX equilibrium.\n\nThere are two types of learning methods these agents can be deployed with: social learning and individual learning. In social learning, each firm is endowed with a single string which is used as its quantity production decision. It then compares this string against other firms' strings. In the individual learning case, agents are endowed with a pool of strings. These strings are then compared against other strings within the agent's population pool. This can be thought of as mutual competing ideas within a firm whereas in the social case, it can be thought of as a firm learning from more successful firms. Note that in the social case and in the individual learning case with identical cost functions, that this is a homogeneous solution, that is all agents' production decisions are identical. However, if the cost functions are not identical, this will result in a heterogeneous solution, where firms produce different quantities (note that they are still locally homogeneous, that is within the firm's own pool all the strings are identical).\n\nAfter all agents have made a quantity production decision, the quantities are aggregated and plugged into a demand function to get a price. Each firm's profit is then calculated. Fitness values are then calculated as a function of profits. After the offspring pool is generated, hypothetical fitness values are calculated. These hypothetical values are based on some sort of estimation of the price level, often just by taking the previous price level.\n\n== References ==\n* J H Miller, 'A Genetic Model of Adaptive Economic Behavior', University of Michigan working paper, 1986.\n* J Arifovic, 'Learning by Genetic Algorithm in Economic Environments', PhD Thesis, University of Chicago, 1991.\n* J Arifovic, 'Genetic Algorithm Learning and the Cobweb Model ', Journal of Economic Dynamics and Control, vol. 18, Issue 1, (January 1994), 3&ndash;28.\n* R Hoffmann, 'The independent localisations of interaction and learning in the repeated prisoner’s dilemma', Theory and Decision, vol. 47, p. 57–72, 1999.\n* R Hoffmann, 'The ecology of cooperation', Theory and Decision, vol. 50, Issue 2. p. 101–118, 2001.\n\n==External links==\n* [https://www.sfu.ca/crabe/ Centre for Adaptive Behaviour in Economics]\n* [http://www.econ.iastate.edu/tesfatsi/getalife.htm Agent-Based Computational Economics and Artificial Life: A Brief Intro]\n\n[[Category:Computational economics]]\n[[Category:Production economics]]\n[[Category:Genetic algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Genetic improvement (computer science)",
      "url": "https://en.wikipedia.org/wiki/Genetic_improvement_%28computer_science%29",
      "text": "In [[Software|computer software]] development, '''genetic Improvement''' is the use of [[Mathematical optimization|optimisation]]  and [[machine learning]] techniques,\nparticularly [[search based software engineering]] techniques such as [[genetic programming]]\nto improve existing [[software]].<ref>{{cite book | doi = 10.1007/978-3-319-20883-1_8 |author=Langdon, William B. | title=Genetically Improved Software | journal=Handbook of Genetic Programming Applications | pages=181–220|year=2015 |isbn=978-3-319-20882-4 }}</ref>\n<ref>{{cite journal | doi = 10.1109/TEVC.2017.2693219 |author=Justyna Petke and Saemundur O. Haraldsson and Mark Harman and William B. Langdon and David R. White and John R. Woodward | title=Genetic Improvement of Software: a Comprehensive Survey | journal=IEEE Transactions on Evolutionary Computation|volume=22 |issue=3 |pages=415–432 |year=2018 }}</ref>\nThe improved [[Computer program|program]] need not behave identically to the original.\nFor example, [[automatic bug fixing]] improves [[program code]] by reducing or eliminating\n[[Software bug|buggy]] behaviour.<ref>{{cite journal | doi = 10.1145/1735223.1735249 |author=Weimer, Westley|display-authors=etal | volume=53 |issue=5| title=Automatic program repair with evolutionary computation | journal=Communications of the ACM | pages=109|year=2010|citeseerx=10.1.1.170.188}}</ref>\nIn other cases the improved software should behave identically to the old version\nbut is better because,\nfor example:\nit runs faster,<ref>{{cite journal | doi = 10.1109/TEVC.2013.2281544 | volume=19 | title=Optimizing Existing Software With Genetic Programming | journal=IEEE Transactions on Evolutionary Computation | pages=118–135| year=2015 | last1=Langdon | first1=William B. | last2=Harman | first2=Mark }}</ref>\nit uses less [[Computer memory|memory]],<ref>{{cite book | doi = 10.1145/2739480.2754648 | title=Deep Parameter Optimisation | journal=Proceedings of the 2015 on Genetic and Evolutionary Computation Conference - GECCO '15| pages=1375–1382 | year=2015 | last1=Wu | first1=Fan | last2=Weimer | first2=Westley | last3=Harman | first3=Mark | last4=Jia | first4=Yue | last5=Krinke | first5=Jens | isbn=9781450334723 }}</ref>\nit uses less [[Electrical energy|energy]]<ref>{{cite book | doi = 10.1145/2739480.2754752 | title=Reducing Energy Consumption Using Genetic Improvement | journal=Proceedings of the 2015 Genetic and Evolutionary Computation Conference - GECCO '15| pages=1327–1334 | year=2015 | last1=Bruce | first1=Bobby R. | last2=Petke | first2=Justyna | last3=Harman | first3=Mark | isbn=9781450334723 }}</ref>\nor\nit runs on a different type of computer.<ref>{{cite book | doi = 10.1007/978-3-662-44303-3_8 | title=Genetically Improved CUDA C++ Software | journal=EuroGP 2014 | volume=8599 | pages=87–99| series=Lecture Notes in Computer Science | year=2014 | last1=Langdon | first1=William B. | last2=Harman | first2=Mark | isbn=978-3-662-44302-6 }}</ref>\nGI differs from,\nfor example,\n[[Formal methods|formal program translation]],\nin that it primarily verifies the behaviour of\nthe new mutant version by running both the new and the old software on \n[[Software testing|test inputs]]\nand comparing their output and performance \nin order to see if the new software can still do what is wanted of the original program and\nis now better.\n\nGenetic improvement can be used to create [[N-version programming|multiple versions]] of programs,\neach [[Custom software|tailored]] to be better for a particular use\nor for a particular computer.\n\nGenetic improvement can by used with [[Multi-objective optimization]]\nto consider improving software along multiple dimensions or\nto consider [[trade-off]]s between several objectives,\nsuch as asking GI to evolve programs which trade speed against\nthe quality of answers they give.\nOf course it may be possible to find programs which are\nboth faster and give better answers.\n\nMostly Genetic Improvement makes typically small changes or edits (also known as [[Mutation testing|mutations]]) to\nthe program's [[source code]] but sometimes the mutations are made to\n[[assembly code]],\n[[Bytecode|byte code]]\n<ref>{{cite journal | doi = 10.1109/TEVC.2010.2052622 | volume=15 | issue=2 | title=Flight of the FINCH Through the Java Wilderness | journal=IEEE Transactions on Evolutionary Computation | pages=166–182| year=2011 | last1=Orlov | first1=Michael | last2=Sipper | first2=Moshe | citeseerx=10.1.1.298.6272 }}</ref>\nor binary [[machine code]].<ref>{{cite book | doi = 10.1145/2739482.2768427 | title=Repairing COTS Router Firmware without Access to Source Code or Test Suites | journal=Proceedings of the Companion Publication of the 2015 on Genetic and Evolutionary Computation Conference - GECCO Companion '15| pages=847–854 | year=2015 | last1=Schulte | first1=Eric M. | last2=Weimer | first2=Westley | last3=Forrest | first3=Stephanie | isbn=9781450334884 }}</ref>\n\n==References==\n<references />\n\n==External links==\n*Open PhD tutorial http://phdopen.mimuw.edu.pl/index.php?page=z15w1 (also covers SBSE and CIT but last of three topics is Genetic Improvement of software).\n*International Workshops on Genetic Improvement: http://www.geneticimprovementofsoftware.com\n*[http://www.cs.ucl.ac.uk/staff/W.Langdon/cec2016/ GI special session of CEC conference:]\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Graduated optimization",
      "url": "https://en.wikipedia.org/wiki/Graduated_optimization",
      "text": "'''Graduated optimization''' is a [[global optimization]] technique that attempts to solve a difficult optimization problem by initially solving a greatly simplified problem, and progressively transforming that problem (while optimizing) until it is equivalent to the difficult optimization problem.<ref>{{cite book |first1=Neil |last1=Thacker |first2=Tim |last2=Cootes |chapterurl=http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/BMVA96Tut/node29.html |chapter=Graduated Non-Convexity and Multi-Resolution Optimization Methods |title=Vision Through Optimization |year=1996}}</ref><ref>{{cite book |first1=Andrew |last1=Blake |first2=Andrew |last2=Zisserman |url=http://research.microsoft.com/en-us/um/people/ablake/papers/VisualReconstruction/ |title=Visual Reconstruction |publisher=MIT Press |year=1987 |isbn=0-262-02271-0}}{{page needed|date=October 2011}}</ref><ref name=\"mobahi2015\">Hossein Mobahi, John W. Fisher III. \n[http://people.csail.mit.edu/hmobahi/pubs/gaussian_convenv_2015.pdf On the Link Between Gaussian Homotopy Continuation and Convex Envelopes], In Lecture Notes in Computer Science (EMMCVPR 2015), Springer, 2015.</ref>\n\n==Technique description==\n{{Unreferenced section|date=October 2011}}\n[[Image:graduated optimization.svg|right|thumb|200px|An illustration of graduated optimization.]]\n\nGraduated optimization is an improvement to [[hill climbing]] that enables a hill climber to avoid settling into local optima. It breaks a difficult optimization problem into a sequence of optimization problems, such that the first problem in the sequence is convex (or nearly convex), the solution to each problem gives a good starting point to the next problem in the sequence, and the last problem in the sequence is the difficult optimization problem that it ultimately seeks to solve. Often, graduated optimization gives better results than simple hill climbing. Further, when certain conditions exist, it can be shown to find an optimal solution to the final problem in the sequence. These conditions are:\n* The first optimization problem in the sequence can be solved given the initial starting point.\n* The locally convex region around the global optimum of each problem in the sequence includes the point that corresponds to the global optimum of the previous problem in the sequence.\nIt can be shown inductively that if these conditions are met, then a hill climber will arrive at the global optimum for the difficult problem. Unfortunately, it can be difficult to find a sequence of optimization problems that meet these conditions. Often, graduated optimization yields good results even when the sequence of problems cannot be proven to strictly meet all of these conditions.\n\n==Some examples==\n\nGraduated optimization is commonly used in image processing for locating objects within a larger image. This problem can be made to be ''more convex'' by blurring the images. Thus, objects can be found by first searching the most-blurred image, then starting at that point and searching within a less-blurred image, and continuing in this manner until the object is located with precision in the original sharp image. The proper choice of the blurring operator depends on the geometric transformation relating the object in one image to the other.<ref>Hossein Mobahi, C. Lawrence Zitnick, Yi Ma. \n[http://research.microsoft.com/en-us/um/people/larryz/blur_cvpr2012.pdf Seeing through the Blur], IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2012.</ref>\n\nGraduated optimization can be used in manifold learning. The Manifold Sculpting algorithm, for example, uses graduated optimization to seek a manifold embedding for non-linear dimensionality reduction.<ref>{{cite book |last1=Gashler |first1=M. |last2=Ventura |first2=D. |last3=Martinez |first3=T. |chapterurl=http://axon.cs.byu.edu/papers/gashler2007nips.pdf |chapter=Iterative Non-linear Dimensionality Reduction with Manifold Sculpting |editor1-last=Platt |editor1-first=J. C. |editor2-last=Koller |editor2-first=D. |editor3-last=Singer |editor3-first=Y. |display-editors = 3 |editor4-last=Roweis |editor4-first=S. |title=Advances in Neural Information Processing Systems 20 |pages=513–20 |publisher=MIT Press |location=Cambridge, MA |year=2008}}</ref> It gradually scales variance out of extra dimensions within a data set while optimizing in the remaining dimensions. It has also been used to calculate conditions for fractionation with tumors,<ref>{{cite journal |pmid=2748803 |year=1989 |last1=Afanasjev |first1=BP |last2=Akimov |first2=AA |last3=Kozlov |first3=AP |last4=Berkovic |first4=AN |title=Graduated optimization of fractionation using a 2-component model |volume=30 |issue=2 |pages=131–5 |journal=Radiobiologia, radiotherapia}}</ref> for object tracking in computer vision,<ref>{{cite journal |doi=10.1109/TPAMI.2003.1251156 |title=Estimating piecewise-smooth optical flow with global matching and graduated optimization |year=2003 |last1=Ming Ye |last2=Haralick |first2=R.M. |last3=Shapiro |first3=L.G.|author3-link=Linda Shapiro |journal=IEEE Transactions on Pattern Analysis and Machine Intelligence |volume=25 |issue=12 |pages=1625–30}}</ref> and other purposes.\n\nA thorough review of the method and its applications can be found in.<ref name=\"mobahi2015\" />\n\n==Related optimization techniques==\n\n[[Simulated annealing]] is closely related to graduated optimization. Instead of smoothing the function over which it is optimizing, simulated annealing randomly perturbs the current solution by a decaying amount, which may have a similar effect.{{citation needed|date=October 2011}} Because simulated annealing relies on random sampling to find improvements, however, its computation complexity is exponential in the number of dimensions being optimized.{{citation needed|reason=MCMC is normally linear with number of dimensions, why would simulated annealing be different?|date=June 2016}} By contrast, graduated optimization smooths the function being optimized, so local optimization techniques that are efficient in high-dimensional space (such as gradient-based techniques, hill climbers, etc.) may still be used.\n\n==See also==\n* [[Numerical continuation]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Graduated Optimization}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Heuristics]]"
    },
    {
      "title": "Great Deluge algorithm",
      "url": "https://en.wikipedia.org/wiki/Great_Deluge_algorithm",
      "text": "The '''Great Deluge algorithm''' ('''GD''') is a generic algorithm applied to [[Optimization (mathematics)|optimization]] problems. It is similar in many ways to the [[hill-climbing]] and [[simulated annealing]] algorithms.\n\nThe name comes from the analogy that in a great deluge a person climbing a hill will try to move in any direction that does not get his/her feet wet in the hope of finding a way up as the water level rises.\n\nIn a typical implementation of the GD, the algorithm starts with a poor approximation, ''S'', of the optimum solution. A numerical value called the ''badness'' is computed based on ''S'' and it measures how undesirable the initial approximation is. The higher the value of ''badness'' the more undesirable is the approximate solution. Another numerical value called the ''tolerance'' is calculated based on a number of factors, often including the initial badness.\n\nA new approximate solution '' S' '', called a neighbour of ''S'', is calculated based on ''S''. The badness of '' S' '', '' b' '', is computed and compared with the tolerance. If '' b' '' is better than tolerance, then the algorithm is recursively  restarted with ''S'' : = '' S' '', and ''tolerance'' := ''decay(tolerance)'' where ''decay'' is a function that lowers the tolerance (representing a rise in water levels). If '' b' '' is worse than tolerance, a different neighbour '' S* '' of ''S'' is chosen and the process repeated. If all the neighbours of ''S'' produce approximate solutions beyond ''tolerance'', then the algorithm is terminated and ''S'' is put forward as the best approximate solution obtained.\n\n==See also==\n* [[:de:Gunter Dueck|de:Gunter Dueck]]\n\n==References==\n* Gunter Dueck: \"New Optimization Heuristics: The Great Deluge Algorithm and the Record-to-Record Travel\", Technical report, IBM Germany, Heidelberg Scientific Center, 1990.\n* Gunter Dueck: \"New Optimization Heuristics The Great Deluge Algorithm and the Record-to-Record Travel\", Journal of Computational Physics, Volume 104, Issue 1, p. 86-92, 1993\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{Optimization algorithms}}"
    },
    {
      "title": "Greedy algorithm",
      "url": "https://en.wikipedia.org/wiki/Greedy_algorithm",
      "text": "{{redirect|Exchange algorithm|key exchange algorithms in cryptography|Key exchange}}{{More citations needed|date=September 2018}}[[Image:Greedy algorithm 36 cents.svg|thumb|280px|right| Greedy algorithms determine minimum number of coins to give while [[Change-making problem|making change]]. These are the steps a human would take to emulate a greedy algorithm to represent 36 cents using only coins with values {1, 5, 10, 20}. The coin of the highest value, less than the remaining change owed, is the local optimum. (In general the change-making problem requires [[dynamic programming]] to find an optimal solution; however, most currency systems, including the Euro and US Dollar, are special cases where the greedy strategy does find an optimal solution.)]]\n\nA '''greedy algorithm''' is an [[algorithmic paradigm]] that follows the [[problem solving]] [[heuristic (computer science)|heuristic]] of making the locally optimal choice at each stage<ref name=\"NISTg\">{{cite web|last=Black|first=Paul E.|title=greedy algorithm|url=http://xlinux.nist.gov/dads//HTML/greedyalgo.html|work=Dictionary of Algorithms and Data Structures|publisher=[[National Institute of Standards and Technology|U.S. National Institute of Standards and Technology]] (NIST) |accessdate=17 August 2012|date=2 February 2005}}</ref> with the intent of finding a [[global optimum]]. In many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.\n\nFor example, a greedy strategy for the [[traveling salesman problem]] (which is of a high [[computational complexity]]) is the following heuristic: \"At each step of the journey, visit the nearest unvisited city.\" This heuristic does not intend to find a best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps. In [[mathematical optimization]], greedy algorithms optimally solve [[combinatorial optimization|combinatorial problem]]s having the properties of [[matroid]]s, and give constant-factor approximations to optimization problems with [[submodular set function|submodular structure]].\n\n==Specifics==\nIn general, greedy algorithms have five components:\n# A candidate set, from which a solution is created\n# A selection function, which chooses the best candidate to be added to the solution\n# A feasibility function, that is used to determine if a candidate can be used to contribute to a solution\n# An objective function, which assigns a value to a solution, or a partial solution, and\n# A solution function, which will indicate when we have discovered a complete solution\n\nGreedy algorithms produce good solutions on some [[mathematical problem]]s, but not on others.  Most problems for which they work will  have two properties:\n\n; Greedy choice property: We can make whatever choice seems best at the moment and then solve the subproblems that arise later. The choice made by a greedy algorithm may depend on choices made so far, but not on future choices or all the solutions to the subproblem.  It iteratively makes one greedy choice after another, reducing each given problem into a smaller one. In other words, a greedy algorithm never reconsiders its choices. This is the main difference from [[dynamic programming]], which is exhaustive and is guaranteed to find the solution. After every stage, dynamic programming makes decisions based on all the decisions made in the previous stage, and may reconsider the previous stage's algorithmic path to solution.\n\n;Optimal substructure: \"A problem exhibits [[optimal substructure]] if an optimal solution to the problem contains optimal solutions to the sub-problems.\"<ref>Introduction to Algorithms (Cormen, Leiserson, Rivest, and Stein) 2001, Chapter 16 \"Greedy Algorithms\".</ref>\n\n===Cases of failure===\n{{multiple image\n   | align = \n| direction = vertical\n   | width     = 300\n   | header    = Examples on how a greedy algorithm may fail to achieve the optimal solution.\n   | image1    = Greedy Glouton.svg\n   | alt1      = \n   | caption1  = Starting from A, a greedy algorithm that tries to find the maximum by following the greatest slope will find the local maximum at \"m\", oblivious to the global maximum at \"M\".\n   | image2    = Greedy-search-path-example.gif\n   | alt2      = \n   | caption2  =\n\nWith a goal of reaching the largest-sum, at each step, the greedy algorithm will choose what appears to be the optimal immediate choice, so it will choose 12 instead of 3 at the second step, and will not reach the best solution, which contains 99.\n}}\n\nFor many other problems, greedy algorithms fail to produce the optimal solution, and may even produce the ''unique worst possible'' solution. One example is the [[traveling salesman problem]] mentioned above: for each number of cities, there is an assignment of distances between the cities for which the nearest-neighbor heuristic produces the unique worst possible tour.<ref>[http://www.sciencedirect.com/science/article/pii/S0166218X01001950 Traveling salesman should not be greedy: domination analysis of greedy-type heuristics for the TSP] (G. Gutin, A. Yeo and A. Zverovich, 2002)</ref>\n\n==Types==\n{{Refimprove section|date=June 2018}}\nGreedy algorithms can be characterized as being 'short sighted', and also as 'non-recoverable'. They are ideal only for problems which have 'optimal substructure'. Despite this, for many simple problems, the best suited algorithms are greedy algorithms. It is important, however, to note that the greedy algorithm can be used as a selection algorithm to prioritize options within a search, or branch-and-bound algorithm. There are a few variations to the greedy algorithm:\n\n* Pure greedy algorithms\n* Orthogonal greedy algorithms\n* Relaxed greedy algorithms\n\n==Theory==\nGreedy algorithms have a long history of study in [[combinatorial optimization]] and [[theoretical computer science]]. Greedy heuristics are known to produce suboptimal results on many problems,<ref>U. Feige. [https://www.cs.duke.edu/courses/cps296.2/spring07/papers/p634-feige.pdf A threshold of ln n for approximating set cover]. Journal of the ACM (JACM), 45(4):634–652, 1998.</ref> and so natural questions are:\n\n* For which problems do greedy algorithms perform optimally?\n* For which problems do greedy algorithms guarantee an approximately optimal solution?\n* For which problems is the greedy algorithm guaranteed ''not'' to produce an optimal solution?\n\nA large body of literature exists answering these questions for general classes of problems, such as [[matroid]]s, as well as for specific problems, such as [[set cover]].\n\n===Matroids===\n{{Main|Matroid}}\nA [[matroid]] is a mathematical structure that generalizes the notion of [[linear independence]] from [[vector spaces]] to arbitrary sets. If an optimization problem has the structure of a matroid, then the appropriate greedy algorithm will solve it optimally.<ref>Papadimitriou, Christos H., and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998.</ref>\n\n===Submodular functions===\n{{Main|Submodular set function#Optimization problems}}\nA function <math>f</math> defined on subsets of a set <math>\\Omega</math> is called [[submodular]] if for every <math>S, T \\subseteq \\Omega</math> we have that <math>f(S)+f(T)\\geq f(S\\cup T)+f(S\\cap T)</math>.\n\nSuppose one wants to find a set <math>S</math> which maximizes <math>f</math>. The greedy algorithm, which builds up a set <math>S</math> by incrementally adding the element which increases <math>f</math> the most at each step, produces as output a set that is at least <math>(1 - 1/e) \\max_{X \\subseteq \\Omega} f(X)</math>.<ref>G. Nemhauser, L.A. Wolsey, and M.L. Fisher. \"[https://www.researchgate.net/profile/George_Nemhauser/publication/242914003_An_Analysis_of_Approximations_for_Maximizing_Submodular_Set_Functions-I/links/53d696490cf228d363ea6bdf.pdf An analysis of approximations for maximizing submodular set functions—I].\" Mathematical Programming 14.1 (1978): 265-294.</ref> That is, greedy performs within a constant factor of <math>(1 - 1/e) \\approx 0.63</math> as good as the optimal solution.\n\nSimilar guarantees are provable when additional constraints, such as cardinality constraints,<ref>N. Buchbinder, et al. \"[http://theory.epfl.ch/moranfe/Publications/SODA2014.pdf Submodular maximization with cardinality constraints].\" Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2014.</ref> are imposed on the output, though often slight variations on the greedy algorithm are required. See <ref>Krause, Andreas, and Daniel Golovin. \"[http://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/dgolovin/papers/submodular_survey12.pdf Submodular function maximization].\" (2014): 71-104.</ref> for an overview.\n\n===Other problems with guarantees===\nOther problems for which the greedy algorithm gives a strong guarantee, but not an optimal solution, include\n\n* [[Set_cover_problem#Greedy_algorithm|Set cover]]\n* The [[Steiner tree problem]]\n* [[Load_balancing_(computing)|Load balancing]]<ref>http://www.win.tue.nl/~mdberg/Onderwijs/AdvAlg_Material/Course%20Notes/lecture5.pdf</ref>\n* [[Independent_set_(graph_theory)#Approximation_algorithms|Independent set]]\n\nMany of these problems have matching lower bounds; i.e., the greedy algorithm does not perform better, in the worst case, than the guarantee.\n\n==Applications==\n{{Expand section|date=June 2018}}\nGreedy algorithms mostly (but not always) fail to find the globally optimal solution because they usually do not operate exhaustively on all the data. They can make commitments to certain choices too early which prevent them from finding the best overall solution later. For example, all known [[greedy coloring]] algorithms for the [[graph coloring problem]] and all other [[NP-complete]] problems do not consistently find optimum solutions. Nevertheless, they are useful because they are quick to think up and often give good approximations to the optimum.\n\nIf a greedy algorithm can be proven to yield the global optimum for a given problem class, it typically becomes the method of choice because it is faster than other optimization methods like [[dynamic programming]]. Examples of such greedy algorithms are [[Kruskal's algorithm]] and [[Prim's algorithm]] for finding [[minimum spanning tree]]s, and the algorithm for finding optimum [[Huffman tree]]s.\n\nGreedy algorithms appear in network [[routing]] as well.  Using greedy routing, a message is forwarded to the neighboring node which is \"closest\" to the destination. The notion of a node's location (and hence \"closeness\") may be determined by its physical location, as in [[geographic routing]] used by [[ad hoc network]]s.  Location may also be an entirely artificial construct as in [[small world routing]] and [[distributed hash table]].\n\n==Examples==\n* The [[activity selection problem]] is characteristic to this class of problems, where the goal is to pick the maximum number of activities that do not clash with each other.\n* In the [[Macintosh computer]] game ''[[Crystal Quest]]'' the objective is to collect crystals, in a fashion similar to the [[travelling salesman problem]]. The game has a demo mode, where the game uses a greedy algorithm to go to every crystal. The [[artificial intelligence]] does not account for obstacles, so the demo mode often ends quickly.\n* The [[matching pursuit]] is an example of greedy algorithm applied on signal approximation.\n* A greedy algorithm finds the optimal solution to [[Malfatti circles|Malfatti's problem]] of finding three disjoint circles within a given triangle that maximize the total area of the circles; it is conjectured that the same greedy algorithm is optimal for any number of circles.\n* A greedy algorithm is used to construct a Huffman tree during [[Huffman coding]] where it finds an optimal solution.\n* In [[decision tree learning]], greedy algorithms are commonly used, however they are not guaranteed to find the optimal solution.\n**One popular such algorithm is the [[ID3 algorithm]] for decision tree construction.\n*[[Dijkstra's algorithm]] and the related [[A* search algorithm]] are verifiably optimal greedy algorithms for [[graph search]] and [[Shortest path problem|shortest path finding]]. \n**A* search is conditionally optimal, requiring an \"[[admissible heuristic]]\" that will not overestimate path costs.\n*[[Kruskal's algorithm]] and [[Prim's algorithm]] are greedy algorithms for constructing [[Minimum spanning tree|minimum spanning trees]] of a given [[connected graph]]. They always find an optimal solution, which may not be unique in general.\n\n==See also==\n{{Portal|Computer Science|Mathematics}}\n*[[Multi-armed bandit#Semi-uniform strategies|Epsilon-greedy strategy]]\n*[[Greedy algorithm for Egyptian fractions]]\n*[[Greedy source]]\n*[[Matroid]]\n\n==Notes==\n<references/>\n\n==References==\n* ''[[Introduction to Algorithms]]'' (Cormen, Leiserson, Rivest, and Stein) 2001, Chapter 16 \"Greedy Algorithms\".\n* G. Gutin, A. Yeo and A. Zverovich, [https://www.sciencedirect.com/science/article/pii/S0166218X01001950 Traveling salesman should not be greedy: domination analysis of greedy-type heuristics for the TSP].  Discrete Applied Mathematics 117 (2002), 81–86.\n* J. Bang-Jensen, G. Gutin and A. Yeo, [https://www.sciencedirect.com/science/article/pii/S1572528604000222 When the greedy algorithm fails]. Discrete Optimization 1 (2004), 121–127.\n* G. Bendall and F. Margot,  [https://www.sciencedirect.com/science/article/pii/S1572528606000430 Greedy Type Resistance of Combinatorial Problems], Discrete Optimization 3 (2006), 288–298.\n* U. Feige. [https://www.cs.duke.edu/courses/cps296.2/spring07/papers/p634-feige.pdf A threshold of ln n for approximating set cover]. Journal of the ACM (JACM), 45(4):634–652, 1998.\n* G. Nemhauser, L.A. Wolsey, and M.L. Fisher. \"[https://www.researchgate.net/profile/George_Nemhauser/publication/242914003_An_Analysis_of_Approximations_for_Maximizing_Submodular_Set_Functions-I/links/53d696490cf228d363ea6bdf.pdf An analysis of approximations for maximizing submodular set functions—I].\" Mathematical Programming 14.1 (1978): 265-294.\n* N. Buchbinder, et al. \"[http://theory.epfl.ch/moranfe/Publications/SODA2014.pdf Submodular maximization with cardinality constraints].\" Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2014.\n* A. Krause and D. Golovin. \"[http://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/dgolovin/papers/submodular_survey12.pdf Submodular function maximization].\" (2014): 71-104.\n\n==External links==\n{{commons category|Greedy algorithms}}\n* {{springer|title=Greedy algorithm|id=p/g110210}}\n* [http://www.oreillynet.com/onlamp/blog/2008/04/python_greedy_coin_changer_alg.html Python greedy coin] example by Noah Gift.\n\n{{optimization algorithms|combinatorial|state=expanded}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Combinatorial algorithms]]\n[[Category:Matroid theory]]\n[[Category:Exchange algorithms]]"
    },
    {
      "title": "Greedy triangulation",
      "url": "https://en.wikipedia.org/wiki/Greedy_triangulation",
      "text": "{{Infobox algorithm\n|class=[[Search algorithm]]\n|image=[[Image:Polygon Greedy triangulation steps.svg|300px|Polygon Greedy triangulation steps]]\n|caption = Polygon Greedy triangulation steps. On each step a new edge (red) is added joining the nearest pair of vertex, without crossing a previously edge\n|data= * [[Priority queue]]\n* [[Spatial database]]\n|time= <math>O(|V| \\cdot \\log |V|)</math>\n|best-time= <math>O(|V|)</math>\n|average-time=\n|space=\n|optimal=\n|complete=\n}}\n\nThe '''Greedy Triangulation''' is a method to compute a [[polygon triangulation]] or a [[Point set triangulation]] using a [[greedy algorithm| greedy schema]], which adds edges one by one to the solution in strict increasing order by length, with the condition that an edge cannot cut a previously inserted edge.<ref name= Loera2010>\n{{Citation|author = [[Jesús A. De Loera|J. Loera]], [[Joerg Rambau|J. Rambau]] and [[Francisco Santos Leal| F. Santos]]| year = 2010 | title = Triangulations: Structures and Algorithms | publisher = [[Springer-Verlag]] | edition = 2nd revised | isbn=9783642129711}} Chapter 3: Polygon Triangulation: pp.103.</ref><ref name= bkos>\n{{Citation|author = [[Mark de Berg]], [[Marc van Kreveld]], [[Mark Overmars]], and [[Otfried Schwarzkopf]] | year = 2000 | title = Computational Geometry | publisher = [[Springer-Verlag]] | edition = 2nd revised | isbn = 3-540-65620-0}}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Triangulation (geometry)]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Guided Local Search",
      "url": "https://en.wikipedia.org/wiki/Guided_Local_Search",
      "text": "{{citations missing|date=December 2009}}\n'''Guided Local Search''' is a [[metaheuristic]] search method. A meta-heuristic method is a method that sits on top of a [[local search (optimization)|local search algorithm]] to change its behavior. \n\nGuided Local Search builds up penalties during a search. It uses penalties to help local search algorithms escape from local minimal and plateaus. When the given local search algorithm settles  in a local optimum, GLS modifies the objective function using a specific scheme (explained below). Then the local search will operate using an augmented objective function, which is designed to bring the search out of the local optimum. The key is in the way that the objective function is modified.\n\n==Overview==\n\n===Solution features===\nTo apply GLS, solution features must be defined for the given problem. Solution features are defined to distinguish between solutions with different characteristics, so that regions of similarity around local optima can be recognized and avoided. The choice of solution features depends on the type of problem, and also to a certain extent on the local search algorithm. For each feature <math>f_i</math> a cost function <math>c_i</math> is defined. \n\nEach feature is also associated with a penalty <math>p_i</math> (initially set to 0) to record the number of occurrences of the feature in local minima. \n\nThe features and costs often come directly from the objective function. For example, in the traveling salesman problem, “whether the tour goes directly from city X to city Y” can be defined to be a feature. The distance between X and Y can be defined to be the cost. In the SAT and weighted MAX-SAT problems, the features can be “whether clause C satisfied by the current assignments”. \n\nAt the implementation level, we define for each feature <math>i</math> an Indicator Function <math>I_i</math> indicating whether the feature is present in the current solution or not. <math>I_i</math> is 1 when solution <math>x</math> exhibits property <math>i</math>, 0 otherwise.\n\n===Selective penalty modifications===\nGLS computes the utility of penalising each feature. When the Local Search algorithm returns a [[maxima and minima|local minimum]] x, GLS penalizes all those features (through increments to the penalty of the features) present in that solution which have maximum utility, <math>\\operatorname{util}(x,i)</math>, as defined below. \n\n: <math>\\operatorname{util}(x,i) = I_i(x)\\frac{c_i(x)}{1+p_i}.</math>\n\nThe idea is to penalise features that have high costs, although the utility of doing so decreases as the feature is penalised more and more often.\n\n===Searching through an augmented cost function===\nGLS uses an augmented cost function (defined below), to allow it to guide the Local Search algorithm out of the local minimum, through penalising features present in that local minimum. The idea is to make the local minimum more costly than the surrounding search space, where these features are not present.\n  \n: <math>g(x) = f(x) + \\lambda a \\sum_{1\\leq i\\leq m} I_i(x) p_i </math>\n\nThe parameter λ may be used to alter the intensification of the search for solutions. A higher value for λ will result in a more diverse search, where plateaus and basins are searched more coarsely; a low value will result in a more intensive search for the solution, where the plateaus and basins in the search landscape are searched in finer detail. The coefficient <math>a</math> is used to make the penalty part of the objective function balanced relative to changes in the objective function and is problem specific. A simple heuristic for setting <math>a</math> is simply to record the average change in objective function up until the first local minimum, and then set <math>a</math> to this value divided by the number of GLS features in the problem instance.\n\n===Extensions of Guided Local Search===\n[http://www.bracil.net/CSP/papers/Mills-GLS-PhD2002.pdf Mills (2002)] has described an Extended Guided Local Search (EGLS) which utilises random moves and an aspiration criterion designed specifically for penalty based schemes. The resulting algorithm improved the robustness of GLS over a range of parameter settings, particularly in the case of the [[quadratic assignment problem]]. A general version of the GLS algorithm, using a min-conflicts based hill climber (Minton et al. 1992) and based partly on GENET for constraint satisfaction and optimisation, has also been implemented in the [http://www.bracil.net/CSP/cacp/ Computer Aided Constraint Programming project].\n\n[http://www.bracil.net/CSP/papers/Alsheddy-PhD-2011.pdf Alsheddy (2011)] extended Guided Local Search to [[multi-objective optimization]], and demonstrated its use in staff empowerment in scheduling {{citation needed|date=July 2014}}.\n\n==Related work==\nGLS was built on GENET, which was developed by Chang Wang, Edward Tsang and Andrew Davenport.\n\nThe [[breakout method]] is very similar to GENET. It was designed for [[constraint satisfaction]]. \n\n[[Tabu search]] is a class of search methods which can be instantiated to specific methods. GLS can be seen as a special case of [[Tabu search]]. \n\nBy sitting GLS on top of [[genetic algorithm]], Tung-leng Lau introduced the Guided Genetic Programming (GGA) algorithm. It was successfully applied to the general assignment problem (in scheduling), processors configuration problem (in electronic design) and a set of radio-link frequency assignment problems (an abstracted military application).\n\nChoi et al. cast GENET as a Lagrangian search.\n\n==Bibliography==\n* Alsheddy, A., Empowerment scheduling: a multi-objective optimization approach using Guided Local Search, PhD Thesis, School of Computer Science and Electronic Engineering, University of Essex, 2011 [http://www.bracil.net/CSP/papers/Alsheddy-PhD-2011.pdf]\n* Choi, K.M.F., Lee, J.H.M. & Stuckey, P.J., A Lagrangian Resconstruction of GENET, Artificial Intelligence, 2000, 123(1-2), 1-39\n* Davenport A., Tsang E.P.K., Kangmin Zhu & C J Wang, GENET: A connectionist architecture for solving constraint satisfaction problems by iterative improvement, Proc., AAAI, 1994, p.325-330 [ftp://ftp.essex.ac.uk/pub/csp/aaai94.ps.Z]\n* Lau, T.L. & Tsang, E.P.K., Solving the processor configuration problem with a mutation-based genetic algorithm, International Journal on Artificial Intelligence Tools (IJAIT), World Scientific, Vol.6, No.4, December 1997, 567-585\n* Lau, T.L. & Tsang, E.P.K., Guided genetic algorithm and its application to radio link frequency assignment problems, Constraints, Vol.6, No.4, 2001, 373-398 [http://www.bracil.net/CSP/papers/LauTsang-Rlfap-Constraints2001.pdf]\n* Lau, T.L. & Tsang, E.P.K., The guided genetic algorithm and its application to the general assignment problems, IEEE 10th International Conference on Tools with Artificial Intelligence (ICTAI'98), Taiwan, November 1998\n* Mills, P. & Tsang, E.P.K., Guided local search for solving SAT and weighted MAX-SAT problems, Journal of Automated Reasoning, Special Issue on Satisfiability Problems, Kluwer, Vol.24, 2000, 205-223 [http://www.bracil.net/CSP/papers/MilTsa-Glssat-Sat2000.pdf]\n* Mills, P. & Tsang, E.P.K. & Ford, J., Applying an Extended Guided Local Search on the Quadratic Assignment Problem, Annals of Operations Research, Kluwer Academic Publishers, Vol.118, 2003, 121-135 [http://www.bracil.net/CSP/papers/MiTsFo-GlsQap-AnOr2002.pdf]\n* Minton, S., Johnston, M., Philips, A.B. & Laird, P., Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems, Artificial Intelligence (Special Volume on Constraint Based Reasoning), Vol.58, Nos.1-3 1992, 161-205\n* Tsang, E.P.K. & Voudouris, C., Fast local search and guided local search and their application to British Telecom's workforce scheduling problem, Operations Research Letters, Elsevier Science Publishers, Amsterdam, Vol.20, No.3, March 1997, 119-127 [http://www.bracil.net/CSP/papers/TsaVou-GlsWfs-ORL1997.pdf]\n* Voudouris, C, Guided local search for combinatorial optimisation problems, PhD Thesis, Department of Computer Science, University of Essex, Colchester, UK, July, 1997 [http://cswww.essex.ac.uk/staff/voudcx/doc/phd_pdf.zip]\n* Voudouris, C., Guided Local Search—An illustrative example in function optimisation, BT Technology Journal, Vol.16, No.3, July 1998, 46-50 [ftp://ftp.essex.ac.uk/pub/csp/Voud-GlsF6-Bttj98.pdf]\n* Voudouris, C. & Tsang, E.P.K., Guided Local Search and its application to the Travelling Salesman Problem,  European Journal of Operational Research,  Anbar Publishing, Vol.113, Issue 2, March 1999, 469-499 [http://www.bracil.net/CSP/papers/CSM-247.ps.Z]\n* Voudouris, C. & Tsang, E.P.K., Guided local search joins the elite in discrete optimisation, DIMACS Series in Discrete Mathematics and Theoretical Computer Science Volume 57, 2001, 29-39 [ftp://ftp.essex.ac.uk/pub/csp/VouTsa-GLSOpt-Dimacs98.ps.Z]\n* Voudouris, C. & Tsang, E.P.K., Guided local search, in F. Glover (ed.), Handbook of metaheuristics, Kluwer, 2003, 185-218 [http://www.bracil.net/CSP/papers/VouTsa-Gls-MetaHeuristic2003.pdf]\n* Voudouris, C., Tsang, E.P.K. & Alsheddy, A., Guided local search, Chapter 11, in M. Gendreau & J-Y Potvin (ed.), Handbook of Metaheuristics, Springer, 2010, 321-361\n\n==External links==\n* [http://www.bracil.net/CSP/gls.html Guided Local Search Home Page]\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Heuristics]]\n\n{{Optimization algorithms}}"
    },
    {
      "title": "Hyper-heuristic",
      "url": "https://en.wikipedia.org/wiki/Hyper-heuristic",
      "text": "A '''hyper-heuristic''' is a [[heuristic]] search method that seeks to automate, often by the incorporation of [[machine learning]] techniques, the process of selecting, combining, generating or adapting several simpler heuristics (or components of such heuristics) to efficiently solve computational search problems. One of the motivations for studying hyper-heuristics is to build systems which can handle classes of problems rather than solving just one problem.<ref>E. K. Burke, E. Hart, [[Graham Kendall|G. Kendall]], J. Newall, P. Ross, and S. Schulenburg, Hyper-heuristics: An emerging direction in modern search technology, Handbook of Metaheuristics (F. Glover and G. Kochenberger, eds.), Kluwer, 2003, pp. 457–474.</ref><ref name=\"Ross05\">P. Ross, Hyper-heuristics, Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques (E. K. Burke and [[Graham Kendall|G. Kendall]], eds.), Springer, 2005, pp. 529-556.</ref><ref name=\"Ozcan08\">E. Ozcan, B. Bilgin, E. E. Korkmaz, A Comprehensive Analysis of Hyper-heuristics, Intelligent Data Analysis, 12:1, pp. 3-23, 2008.</ref>\n\nThere might be multiple heuristics from which one can choose for solving a problem, and each heuristic has its own strength and weakness. The idea is to automatically devise algorithms by combining the strength and compensating for the weakness of known heuristics.<ref name=\"Ozcan06\">E. Ozcan, B. Bilgin, E. E. Korkmaz, Hill Climbers and Mutational Heuristics in Hyperheuristics, Lecture Notes in Computer Science, Springer-Verlag, The 9th International Conference on Parallel Problem Solving From Nature, 2006, pp. 202-211.</ref> In a typical hyper-heuristic framework there is a high-level methodology and a set of low-level heuristics (either constructive or perturbative heuristics). Given a problem instance, the high-level method selects which low-level heuristic should be applied at any given time, depending upon the current problem state, or search stage.<ref name=\"Ross05\"/>\n\n== Hyper-heuristics versus metaheuristics ==\nThe fundamental difference between [[metaheuristics]] and hyper-heuristics is that most implementations of metaheuristics search within a [[Candidate solution|search space]] of problem solutions, whereas hyper-heuristics always search within a search space of heuristics. Thus, when using hyper-heuristics, we are attempting to find the right method or sequence of heuristics in a given situation rather than trying to solve a problem directly. Moreover, we are searching for a generally applicable methodology rather than solving a single problem instance.\n\nHyper-heuristics could be regarded as \"off-the-peg\" methods as opposed to \"made-to-measure\" metaheuristics. They aim to be generic methods, which should produce solutions of acceptable quality, based on a set of easy-to-implement low-level heuristics.\n\n== Motivation ==\nDespite the significant progress in building search methodologies for a wide variety of application areas so far, such approaches still require specialists to integrate their expertise in a given problem domain. Many researchers from [[computer science]], [[artificial intelligence]] and [[operational research]] have already acknowledged the need for developing automated systems to replace the role of a human expert in such situations. One of the main ideas for automating the design of heuristics requires the incorporation of [[machine learning]] mechanisms into algorithms to adaptively guide the search. Both learning and adaptation processes can be realised on-line or off-line, and be based on constructive or perturbative heuristics.\n\nA hyper-heuristic usually aims at reducing the amount of [[domain knowledge]] in the search methodology. The resulting approach should be cheap and fast to implement, requiring less expertise in either the problem domain or heuristic methods, and (ideally) it would be robust enough to effectively handle a range of problem instances from a variety of domains. The goal is to raise the level of generality of decision support methodology perhaps at the expense of reduced - but still acceptable - solution quality when compared to tailor-made metaheuristic approaches.<ref name=\"Burke1995\">Burke E.K., Landa Silva J.D., Soubeiga E.: Multi-objective Hyper-heuristic Approaches for Space Allocation and Timetabling, In Meta-heuristics: Progress as Real Problem Solvers, Selected Papers from the 5th Metaheuristics International Conference (MIC 2003), pp 129-158, 2005.</ref> In order to reduce the gap between tailor-made schemes and hyperheuristic-based strategies, parallel hyperheuristics have been proposed.<ref name=\"Segura2010\">C. Segura, G. Miranda and C. León: Parallel hyperheuristics for the frequency assignment problem\nSpecial issue on nature inspired cooperative strategies for optimization, In Memetic Computing, Special issue on nature inspired cooperative strategies for optimization, ({{doi|10.1007/s12293-010-0044-5}} [https://dx.doi.org/10.1007/s12293-010-0044-5]), 2010.</ref>\n\n== Origins ==\nThe term \"hyperheuristics\" was first coined in a 2000 publication by Cowling and Soubeiga, who used it to describe the idea of \"heuristics to choose heuristics\".<ref name=\"CowlingPATAT2000\">Cowling P. and Soubeiga E. Neighborhood Structures for Personnel Scheduling: A Summit Meeting Scheduling Problem (abstract), in proceedings of the 3rd International Conference on the Practice and Theory of Automated Timetabling, Burke E.K. and Erben W. (eds), 16-18 Aug 2000, Constance, Germany</ref> They used a \"choice function\" machine learning approach which trades off exploitation and exploration in choosing the next heuristic to use.<ref name=\"CowlingKendallSoubeiga2001\">Cowling P., [[Graham Kendall|Kendall G.]] and Soubeiga E., A Hyperheuristic Approach to Scheduling a Sales Summit, 2001, Lecture Notes in Computer Science 2079, Springer-Verlag, pp.&nbsp;176–190, 2001, {{ISBN|3540424210}}, ({{doi|10.1007/3-540-44629-X}}</ref> Subsequently, Cowling, Soubeiga, Kendall, Han, Ross and other authors investigated and extended this idea in areas such as evolutionary algorithms, and pathological low level heuristics. The first journal article to use the term appeared in 2003.<ref>Burke E. K., [[Graham Kendall|Kendall G.]], and Soubeiga E. (2003) A Tabu-Search Hyper-Heuristic for Timetabling and Rostering. Journal of Heuristics, 9(6):451-470. ({{doi|10.1023/B:HEUR.0000012446.94732.b6}} [https://dx.doi.org/10.1023/B:HEUR.0000012446.94732.b6])</ref> The origin of the idea (although not the term) can be traced back to the early 1960s<ref name=\"Fisher61\">H. Fisher and G. L. Thompson, Probabilistic learning combinations of local job-shop scheduling rules, Factory Scheduling Conference (Carnegie Institute of Technology), 1961.</ref><ref name=\"Fisher63\">* H. Fisher and G. L. Thompson, Probabilistic learning combinations of local job-shop scheduling rules,Industrial Scheduling (New Jersey) (J. F. Muth and G. L. Thompson, eds.), Prentice-Hall, Inc, 1963, pp. 225–251.</ref> and was independently re-discovered and extended several times during the 1990s.<ref>R. H. Storer, S. D. Wu, and R. Vaccari, New search spaces for sequencing problems with application to job shop scheduling, Management Science, 38 (10), 1992, 1495–1509.</ref><ref>H. L. Fang, P. Ross, and D. Corne, A promising genetic algorithm approach to job shop scheduling, rescheduling, and open-shop scheduling problems, Fifth International Conference on Genetic Algorithms (San Mateo) (S. Forrest, ed.), Morgan Kaufmann, 1993, pp. 375–382.</ref><ref>U. Dorndorf and E. Pesch, Evolution based learning in a job shop scheduling environment, Computers and Operations Research, 22(1), 1995, 25–40.</ref> In the domain of Job Shop Scheduling, the pioneering work by Fisher and Thompson,<ref name=\"Fisher61\"/><ref name=\"Fisher63\"/> hypothesized  and experimentally proved, using probabilistic learning, that combining scheduling rules (also known as priority or dispatching rules) was superior than any of the rules taken separately.  Although the term was not then in use, this was the first \"hyper-heuristic\" paper. Another root inspiring the concept of hyper-heuristics comes from the field of [[artificial intelligence]]. More specifically, it comes from work on [[automated planning]] systems, and its eventual focus towards the problem of learning control knowledge. The so-called COMPOSER system, developed by Gratch et al.,<ref>J. Gratch, S. Chien, and G. DeJong, Learning search control knowledge for deep space network scheduling, Proceedings of the Tenth International Conference on Machine Learning (Amherst, MA), 1993, pp. 135–142.</ref><ref>J. Gratch and S. Chien, Adaptive problem-solving for large-scale scheduling problems: a case study, Journal of Artificial Intelligence Research, 4, 1996, 365–396.</ref> was used for controlling satellite communication schedules involving a number of earth-orbiting satellites and three ground stations. The system can be characterized as a [[hill-climbing]] search in the space of possible control strategies.\n\n== Classification of approaches ==\nHyper-heuristic approaches so far can be classified into two main categories. In the first class, captured by the phrase ''heuristics to choose heuristics'',<ref name=\"CowlingPATAT2000\" /><ref name=\"CowlingKendallSoubeiga2001\" /> the hyper-heuristic framework is provided with a set of pre-existing, generally widely known heuristics for solving the target problem. The task is to discover a good sequence of applications of these heuristics (also known as low-level heuristics within the domain of hyper-heuristics) for efficiently solving the problem. At each decision stage, a heuristic is selected through a component called selection mechanism and applied to an incumbent solution. The new solution produced from the application of the selected heuristic is accepted/rejected based on another component called acceptance criterion. Rejection of a solution means it is simply discarded while acceptance leads to the replacement of the incumbent solution. In the second class, ''heuristics to generate heuristics'', the key idea is to \"evolve new heuristics by making use of the components of known heuristics.\"<ref>M. Bader-El-Den and R. Poli, Generating sat local-search heuristics using a GP hyper-heuristic framework, Artificial Evolution, 8th International Conference, Evolution Artificielle, EA 2007, Tours, France, October 29–31, 2007, Revised Selected Papers. Lecture Notes in Computer Science 4926 Springer, 2008, pp. 37-49.</ref>  The process requires, as in the first class of hyper-heuristics, the selection of a suitable set of heuristics known to be useful in solving the target problem. However, instead of supplying these directly to the framework, the heuristics are first decomposed into their basic components.\n\nThese two main broad types can be further categorised according to whether they are based on constructive or perturbative search. An\nadditional orthogonal classification of hyper-heuristics considers the source providing feedback during the learning process, which can be either one instance (''on-line learning'') or many instances of the underlying problem studied (''off-line learning'').\n\n=== Methodologies to choose heuristics ===\n\nDiscover good combinations of fixed, human-designed, well-known low-level heuristics.\n* Based on constructive heuristics\n* Based on perturbative heuristics\n\n=== Methodologies to generate heuristics ===\n\nGenerate new heuristic methods using basic components of previously existing heuristic methods.\n* Based on basic components of constructive heuristics\n* Based on basic components of perturbative heuristics\n\n=== On-line learning hyper-heuristics ===\n\nThe learning takes place while the algorithm is solving an instance of a problem, therefore, task-dependent local properties can be used by the high-level strategy to determine the appropriate low-level heuristic to apply. Examples of on-line learning approaches within hyper-heuristics are: the use of [[reinforcement learning]] for heuristic selection, and generally the use of [[metaheuristics]] as high-level search strategies over a search space of heuristics.\n\n=== Off-line learning hyper-heuristics ===\n\nThe idea is to gather knowledge in form of rules or programs, from a set of training instances, which would hopefully generalise to the process of solving unseen instances. Examples of off-line learning approaches\nwithin hyper-heuristics are: [[learning classifier system]]s, case-base reasoning and [[genetic programming]].\n\n== Applications ==\n\nHyper-heuristics have been applied across many different problems. Indeed, one of the motivations of hyper-heuristics is to be able to operate across different problem types. The following list is a non-exhaustive selection of some of the problems and fields in which hyper-heuristics have been explored:\n\n* [[bin packing problem]]\n* [[boolean satisfiability problem]]\n* educational timetabling\n* job shop scheduling\n* multi-objective problem solving and space allocation\n* [[nurse rostering problem|nurse rostering]]\n* personnel scheduling\n* [[traveling salesman problem]]\n* [[vehicle routing problem]]\n* [[List of knapsack problems#Multiple constraints|multidimensional knapsack problem]]\n* [[Knapsack problem|0-1 knapsack problem]]\n* [[Maximum cut|maximum cut problem]]\n* [[quadratic assignment problem]]\n* wind farm layout \n\n== Related areas ==\n\nHyper-heuristics are not the only approach being investigated in the quest for more general and applicable search methodologies. Many researchers from computer science, [[artificial intelligence]] and [[operational research]] have already acknowledged the need for developing automated systems to replace the role of a human expert in the process of tuning and adapting search methodologies. The following list outlines some related areas of research:\n\n* adaptation and self-adaptation of algorithm parameters\n* adaptive [[memetic algorithm]]\n* adaptive large neighborhood search\n* algorithm configuration\n* algorithm control\n* [[algorithm portfolios]]\n* [[autonomous search]]\n* [[genetic programming]]\n* indirect encodings in [[evolutionary algorithms]]\n* variable neighborhood search\n* [[reactive search]]\n\n== See also ==\n* [[Constructive heuristic]]\n* [[Meta-optimization]] is closely related to hyper-heuristics.\n* [[genetic algorithms]]\n* [[genetic programming]]\n* [[evolutionary algorithms]]\n* [[local search (optimization)]]\n* [[machine learning]]\n* [[memetic algorithms]]\n* [[metaheuristics]]\n* [[no free lunch in search and optimization]]\n* [[particle swarm optimization]]\n* [[reactive search]]\n\n== References and notes ==\n{{Reflist|30em}}\n\n== External links ==\n\n=== Hyper-heuristic bibliographies ===\n* [https://mustafamisir.github.io/hh.html https://mustafamisir.github.io/hh.html]\n\n=== Research groups ===\n* [http://cse.yeditepe.edu.tr/ARTI/ Artificial Intelligence (ART+I) Laboratory], [http://www.yeditepe.edu.tr/ Yeditepe University], [[Turkey]]\n* [http://www.asap.cs.nott.ac.uk/ Automated Scheduling, Optimisation and Planning (ASAP) Research Group], [http://www.nottingham.ac.uk/ University of Nottingham], [[UK]]\n* [http://www.kuleuven-kortrijk.be/CODeS/ Combinatorial Optimisation and Decision Support (CODeS) Research Group], [http://www.kuleuven.be/ KU Leuven], [[Belgium]]\n* [http://www.cs.stir.ac.uk/research/groups/chords/ Computational-Heuristics, Operations Research and Decision-Support (CHORDS) Research Group], [http://www.stir.ac.uk/ University of Stirling], [[UK]]\n* [http://ecs.victoria.ac.nz/Groups/ECRG/WebHome Evolutionary Computation Research Group], [https://www.victoria.ac.nz/ Victoria University of Wellington], [[New Zealand]]\n* [http://www.macs.hw.ac.uk/isl/ Intelligent Systems Lab], [http://www.hw.ac.uk/home/ Heriot-Watt University], [[UK]]\n* [https://memoryrlab.github.io/ Machine lEarning and Operations Research (MEmORy) Lab], [http://www.nuaa.edu.cn/ Nanjing University of Aeronautics and Astronautics], [[P.R.China]]\n* [https://web.archive.org/web/20070609155443/http://www.mosaic.brad.ac.uk/home.php Modelling Optimisation Scheduling and Intelligent Control (MOSAIC) Research Group], [http://www.bradford.ac.uk/ University of Bradford], [[UK]]\n* [http://or.eecs.qmul.ac.uk/ Operational Research (OR) Group], [http://www.qmul.ac.uk/ Queen Mary University of London], [[UK]]\n* [http://www.oscar-lab.org/ Optimising Software by Computation from ARtificial intelligence (OSCAR) Research Group], [http://www.dlut.edu.cn/ Dalian University of Technology], [[P.R.China]]\n\n=== Recent activities ===\n* [https://mustafamisir.github.io/ss-hh-euro2019.html Stream on Hyper-heuristics @ EURO 2019]\n* [https://mustafamisir.github.io/ss-aad-mcdm2019.html Invited Session on Automated Algorithm Design for Multi-objective Optimization Problems @ MCDM 2019]\n* [http://web.mst.edu/~tauritzd/ECADA/GECCO2018/index.html 8th Workshop on Evolutionary Computation for the Automated Design of Algorithms (ECADA) @ GECCO 2018]\n* [https://mustafamisir.github.io/ss-hh.html Stream on Hyper-heuristics @ EURO 2018]\n* [https://mustafamisir.github.io/ss-aad.html Special Session on Automated Algorithm Design as Ensemble Techniques @ IEEE CIEL / SSCI 2017]\n* [http://seal2017.com/Tutorial.html Tutorial on Algorithm Selection: Offline + Online Techniques @ SEAL 2017]\n* [https://archive.is/20130419142956/http://people.exeter.ac.uk/km314/aisb2013/index.php 1st AISB Symposium on Meta-Optimisation: Hyper-heuristics and Beyond @ AISB Convention 2013]\n* [https://web.archive.org/web/20120201024944/http://www.lifl.fr/META2012/pmwiki.php?n=Main.SpecialSessions Modern Hyperheuristics for Large Scale Optimization Problems @ META2012]\n* [http://www.sigevo.org/gecco-2012/tutorials.html#hecdo Tutorial on Hyper-heuristics and Cross-domain Optimization @ GECCO 2012]\n* [http://www.sigevo.org/gecco-2012/organizers-tracks.html#ss Self-* Search Track @ GECCO 2012]\n* [http://www.cs.nott.ac.uk/~rxq/CEC2012EHyperheurstics.html Special Session on Evolutionary Based Hyperheuristics and Their Applications @ IEEE CEC2012 (WCCI2012)]\n* [https://web.archive.org/web/20120426054952/http://lion.disi.unitn.it/intelligent-optimization/LION6/lions.php Special Session on Cross-domain Heuristic Search (LION-CHESC) @ LION2012]\n* [http://www.asap.cs.nott.ac.uk/chesc2011/ Cross-domain Heuristic Search Challenge 2011 (CHeSC 2011)]\n* [http://www.cs.nott.ac.uk/~gxo/hhmc10-ss.html Special Session on Systems to Build Systems @ MISTA 2011]\n* [http://www.sigevo.org/gecco-2011/tutorials.html#ahd Tutorial on Automated Heuristic Design @ GECCO 2011]\n* [http://www.mistaconference.org/2011/sessions/index.html Special Session on Hybrid Evolutionary Algorithms, Hyper-heuristics and Memetic Computation @ IEEE CEC2010 (WCCI 2010)]\n* [http://www.cs.nott.ac.uk/~gxo/ppsn2010-selfstar.html Workshop on Self-tuning, self-configuring and self-generating search heuristics (Self* 2010) @ PPSN 2010]\n* [http://www.cs.nott.ac.uk/~gxo/hhworkshop.html Workshop on Hyper-heuristics @ PPSN 2008]\n\n=== Others ===\n* [http://www.cs.nott.ac.uk/~rxq/cis/hyper-heuristics-tf.html Task Force on Hyper-heuristics] in the [http://cis.ieee.org/intelligent-systems-applications-tc.html Technical Committee of Intelligent Systems and Applications] at the [http://cis.ieee.org/ IEEE Computational Intelligence Society].\n\n{{DEFAULTSORT:Hyper-Heuristic}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Heuristics]]"
    },
    {
      "title": "In-crowd algorithm",
      "url": "https://en.wikipedia.org/wiki/In-crowd_algorithm",
      "text": "The '''in-crowd algorithm''' is a numerical method for solving [[basis pursuit denoising]] quickly; faster than any other algorithm for large, sparse problems.<ref name=\"in_crowd\">See ''The In-Crowd Algorithm for Fast Basis Pursuit Denoising'', IEEE Trans Sig Proc 59 (10), Oct 1 2011, pp. 4595 - 4605, [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5940245], demo [[MATLAB]] code available [http://molnargroup.ece.cornell.edu/files/InCrowdBeta1.zip]</ref> This algorithm is an [[active set method]], which minimizes iteratively sub-problems of the global basis pursuit denoising:\n\n<math>\\min_x \\frac{1}{2}\\|y-Ax\\|^2_2+\\lambda\\|x\\|_1.</math>\n\nwhere <math>y</math> is the observed signal, <math>x</math> is the sparse signal to be recovered, <math>Ax</math> is the expected signal under <math>x</math>, and <math>\\lambda</math> is the regularization parameter trading off signal fidelity and simplicity. The simplicity is here measured using the sparsity of the solution <math>x</math>, measure through its <math>\\ell_1</math>-norm. The active set strategies are very efficient in this context as only few coefficient are expected to be non-zero. Thus, if they can be identified, solving the problem restricted to these coefficients yield the solution. Here, the features are greedily selected based on the absolute value of their gradient at the current estimate.\n\nOther active-set methods for the basis pursuit denoising includes BLITZ<ref> Johnson T, Guestrin C. Blitz: ''A principled meta-algorithm for scaling sparse optimization''. In proceedings of the International Conference on Machine Learning (ICML) 2015 (pp. 1171-1179).([http://proceedings.mlr.press/v37/johnson15.pdf])</ref>, where the selection of the active set is performed using the [[duality gap]] of the problem, and The Feature Sign Search<ref>Lee H, Battle A, Raina R, Ng AY. ''Efficient sparse coding algorithms''. In Advances in neural information processing systems 2007 (pp. 801-808). [https://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf]</ref>, where the features are included based on the estimate of their sign.\n\n=In-crowd Algorithm=\n\nIt consists of the following:\n\n# Declare <math>x</math> to be 0, so the unexplained residual <math> r = y</math>\n# Declare the active set <math>I</math> to be the empty set\n# Calculate the usefulness <math>u_j = | \\langle r A_j \\rangle | </math> for each component in <math>I^c</math>\n# If on <math>I^c</math>, no <math>u_j > \\lambda</math>, terminate\n# Otherwise, add <math>L \\approx 25</math> components to <math>I</math> based on their usefulness\n# Solve basis pursuit denoising exactly on <math>I</math>, and throw out any component of <math>I</math> whose value attains exactly 0.  This problem is dense, so quadratic programming techniques work very well for this sub problem.\n# Update <math> r = y - Ax</math> - n.b. can be computed in the subproblem as all elements outside of <math>I</math> are 0\n# Go to step 3.\n\nSince every time the in-crowd algorithm performs a global search it adds up to <math>L</math> components to the active set, it can be a factor of <math>L</math> faster than the best alternative algorithms when this search is computationally expensive.  A theorem<ref name=\"in_crowd\"/> guarantees that the global optimum is reached in spite of the many-at-a-time nature of the in-crowd algorithm.\n\n\n==Notes==\n{{reflist}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{Mathapplied-stub}}"
    },
    {
      "title": "Interior-point method",
      "url": "https://en.wikipedia.org/wiki/Interior-point_method",
      "text": "'''Interior-point methods''' (also referred to as '''barrier methods''' or '''IPMs''') are a certain class of [[algorithm]]s that solve linear and nonlinear [[convex optimization]] problems.\n[[File:karmarkar.svg|thumb|200px|right|Example solution]]\n[[John von Neumann]]<ref>{{Cite book |first1=George B. |last1=Dantzig |first2= Mukund N. |last2=Thapa |year=2003 |title=Linear Programming 2: Theory and Extensions |publisher=Springer-Verlag}}</ref> suggested an interior-point method of linear programming, which was neither a polynomial-time method nor an efficient method in practice. In fact, it turned out to be slower than the commonly used [[Simplex algorithm|simplex method]]. In 1984, [[Narendra Karmarkar]] developed a method for [[linear programming]] called [[Karmarkar's algorithm]], which runs in provably polynomial time and is also very efficient in practice. It enabled solutions of linear programming problems that were beyond the capabilities of the simplex method. Contrary to the simplex method, it reaches a best solution by traversing the interior of the [[feasible region]]. The method can be generalized to convex programming based on a [[self-concordant]] [[barrier function]] used to encode the [[convex set]]. \n\nAny convex optimization problem can be transformed into minimizing (or maximizing) a [[linear function]] over a convex set by converting to the [[Epigraph (mathematics)|epigraph]] form.<ref>{{cite book |last=Boyd |first=Stephen |last2=Vandenberghe |first2=Lieven |title=Convex Optimization |publisher=[[Cambridge University Press]] |location=Cambridge |year=2004 |pages=143 |isbn=978-0-521-83378-3 |mr=2061575}}</ref> The idea of encoding the [[candidate solution|feasible set]] using a barrier and designing barrier methods was studied by Anthony V. \n[[Yurii Nesterov]] and [[Arkadi Nemirovski]] came up with a special class of such barriers that can be used to encode any convex set. They guarantee that the number of [[iteration]]s of the algorithm is bounded by a polynomial in the dimension and accuracy of the solution.<ref>{{Cite journal |mr=2115066 |doi=10.1090/S0273-0979-04-01040-7 |title=The interior-point revolution in optimization: History, recent developments, and lasting consequences |year=2004 |last1=Wright |first1=Margaret H. |journal=Bulletin of the American Mathematical Society |volume=42 |pages=39–57}}</ref>\n\nKarmarkar's breakthrough revitalized the study of interior-point methods and barrier problems, showing that it was possible to create an algorithm for linear programming characterized by [[polynomial time|polynomial complexity]] and, moreover, that was competitive with the simplex method.\nAlready [[Leonid Khachiyan|Khachiyan]]'s [[ellipsoid method]] was a polynomial-time algorithm; however, it was too slow to be of practical interest.\n\nThe class of primal-dual path-following interior-point methods is considered the most successful.\n[[Mehrotra predictor–corrector method|Mehrotra's predictor–corrector algorithm]] provides the basis for most implementations of this class of methods.<ref>{{cite journal |last=Potra |first=Florian A. |author2=Stephen J. Wright |title=Interior-point methods |journal=Journal of Computational and Applied Mathematics |volume=124 |year=2000 |issue=1–2 |pages=281–302 |doi=10.1016/S0377-0427(00)00433-7}}</ref>\n\n==Primal-dual interior-point method for nonlinear optimization==\nThe primal-dual method's idea is easy to demonstrate for constrained [[nonlinear optimization]].\nFor simplicity, consider the all-inequality version of a nonlinear optimization problem:\n\n:minimize <math>f(x)</math> subject to <math>c_i(x) \\ge 0 ~\\text{for}~ i = 1, \\ldots, m, ~ x \\in \\mathbb{R}^n,</math> where <math> f : \\mathbb{R}^{n} \\to \\mathbb{R}, c_i : \\mathbb{R}^{n} \\rightarrow \\mathbb{R} \\quad (1).</math>\n\nThe logarithmic [[barrier function]] associated with (1) is\n:<math>B(x,\\mu) = f(x) - \\mu \\sum_{i=1}^m \\log(c_i(x)). \\quad (2)</math>\n\nHere <math>\\mu</math> is a small positive scalar, sometimes called the \"barrier parameter\". As <math>\\mu</math> converges to zero the minimum of <math>B(x,\\mu)</math> should converge to a solution of (1).\n\nThe barrier function [[gradient]] is\n:<math>g_b = g - \\mu \\sum_{i=1}^m \\frac{1}{c_i(x)} \\nabla c_i(x), \\quad (3)</math>\n\nwhere <math>g</math> is the gradient of the original function <math>f(x)</math>, and <math>\\nabla c_i</math> is the gradient of <math>c_i</math>.\n\nIn addition to the original (\"primal\") variable <math>x</math> we introduce a [[Lagrange multiplier]] inspired [[Lagrange_multiplier#The_strong_Lagrangian_principle:_Lagrange_duality|dual]] variable <math>\\lambda \\in \\mathbb{R} ^m</math>\n:<math>c_i(x) \\lambda_i = \\mu, \\forall i = 1, \\ldots, m. \\quad (4)</math>\n\n(4) is sometimes called the \"perturbed complementarity\" condition, for its resemblance to \"complementary slackness\" in [[KKT conditions]].\n\nWe try to find those <math>(x_\\mu, \\lambda_\\mu)</math> for which the gradient of the barrier function is zero.\n\nApplying (4) to (3), we get an equation for the gradient:\n:<math>g - A^T \\lambda = 0, \\quad (5)</math>\nwhere the matrix <math>A</math> is the [[Jacobian matrix and determinant|Jacobian]] of the constraints <math>c(x)</math>.\n\nThe intuition behind (5) is that the gradient of <math>f(x)</math> should lie in the subspace spanned by the constraints' gradients. The \"perturbed complementarity\" with small <math>\\mu</math> (4) can be understood as the condition that the solution should either lie near the boundary <math>c_i(x) = 0</math>, or that the projection of the gradient <math>g</math> on the constraint component <math>c_i(x)</math> normal should be almost zero.\n\nApplying [[Newton method|Newton's method]] to (4) and (5), we get an equation for <math>(x, \\lambda)</math> update <math>(p_x, p_\\lambda)</math>:\n:<math>\\begin{pmatrix}\n W & -A^T \\\\\n \\Lambda A & C\n\\end{pmatrix}\\begin{pmatrix}\n p_x  \\\\\n p_\\lambda\n\\end{pmatrix}=\\begin{pmatrix}\n -g + A^T \\lambda  \\\\\n \\mu 1 - C \\lambda\n\\end{pmatrix},</math>\n\nwhere <math>W</math> is the [[Hessian matrix]] of <math>B(x, \\mu)</math>, <math>\\Lambda</math> is a [[diagonal matrix]] of <math>\\lambda</math>, and <math>C</math> is a diagonal matrix with <math>C_{ii} = c_i(x)</math>.\n\nBecause of (1), (4) the condition\n:<math>\\lambda \\ge 0</math>\n\nshould be enforced at each step. This can be done by choosing appropriate <math>\\alpha</math>:\n:<math>(x,\\lambda) \\to (x + \\alpha p_x, \\lambda + \\alpha p_\\lambda).</math>\n\n==See also==\n*[[Augmented Lagrangian method]]\n*[[Penalty method]]\n*[[Karush–Kuhn–Tucker conditions]]\n\n==References==\n{{Reflist}}\n== Bibliography ==\n* {{cite book|last1=Bonnans|first1=J.&nbsp;Frédéric|last2=Gilbert|first2=J.&nbsp;Charles|last3=Lemaréchal|first3=Claude| authorlink3=Claude Lemaréchal|last4=Sagastizábal|first4=Claudia&nbsp;A.|author4-link= Claudia Sagastizábal |title=Numerical optimization: Theoretical and practical aspects|url=https://www.springer.com/mathematics/applications/book/978-3-540-35445-1|edition=Second revised ed. of translation of 1997 <!-- ''Optimisation numérique: Aspects théoriques et pratiques'' --> French| series=Universitext|publisher=Springer-Verlag|location=Berlin|year=2006|pages=xiv+490|isbn=978-3-540-35445-1|doi=10.1007/978-3-540-35447-5|mr=2265882}}\n* {{cite conference|doi=10.1145/800057.808695|chapter-url=http://retis.sssup.it/~bini/teaching/optim2010/karmarkar.pdf|chapter=A new polynomial-time algorithm for linear programming|title=Proceedings of the sixteenth annual ACM symposium on Theory of computing  - STOC '84|year=1984|last1=Karmarkar|first1=N.|isbn=0-89791-133-4|pages=302|deadurl=yes|archive-url=https://web.archive.org/web/20131228145520/http://retis.sssup.it/~bini/teaching/optim2010/karmarkar.pdf|archive-date=28 December 2013|df=dmy-all}}\n* {{Cite journal|doi=10.1137/0802028|title=On the Implementation of a Primal-Dual Interior Point Method|year=1992|last1=Mehrotra|first1=Sanjay|journal=SIAM Journal on Optimization|volume=2|issue=4|pages=575–601}}\n*{{cite book|title = Numerical Optimization | first=Jorge| last = Nocedal |author2=Stephen Wright| year=1999 | publisher=Springer | location=New York, NY| isbn=978-0-387-98793-4}}\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 10.11. Linear Programming: Interior-Point Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=537}}\n*{{cite book|title = Primal-Dual Interior-Point Methods | first=Stephen| last = Wright | year=1997 | publisher=SIAM | location=Philadelphia, PA| isbn=978-0-89871-382-4}}\n*{{cite book|title = Convex Optimization |last1=Boyd|first1=Stephen|last2=Vandenberghe|first2=Lieven|year=2004|publisher=Cambridge University Press|url=http://www.stanford.edu/~boyd/cvxbook/}}\n\n{{Use dmy dates|date=February 2011}}\n\n{{Optimization algorithms|convex}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "IOSO",
      "url": "https://en.wikipedia.org/wiki/IOSO",
      "text": "'''IOSO''' (Indirect [[Program optimization|Optimization]] on the basis of [[self-organization|Self-Organization]]) is a [[Multiobjective optimization|multiobjective]], multidimensional nonlinear optimization technology.\n\n==IOSO approach==\nIOSO Technology is based on the [[response surface methodology]] approach.\nAt each IOSO iteration the internally constructed response surface model for the objective is being optimized within the current search region. This step is followed by a direct call to the actual mathematical model of the system for the candidate optimal point obtained from optimizing internal response surface model. During IOSO operation, the information about the system behavior is stored for the points in the neighborhood of the extremum, so that the response surface model becomes more accurate for this search area. The following steps are internally taken while proceeding from one IOSO iteration to another:\n*the modification of the experiment plan;\n*the adaptive adjustment of the current search area;\n*the function type choice (global or middle-range) for the response surface model;\n*the adjustment of the response surface model;\n*the modification of both parameters and structure of the optimization algorithms; if necessary, the selection of the new promising points within the search area.\n\n==History==\nIOSO is based on the technology being developed for more than 20 years by [http://www.iosotech.com Sigma Technology] which grew out of IOSO Technology Center in 2001. Sigma Technology is headed by prof . Egorov I. N., CEO.\n\n==Products==\nIOSO is the name of the group of [[multidisciplinary design optimization]] software  that runs on  [[Microsoft Windows]] as well as on [[Unix]]/[[Linux]] OS and was developed by [http://www.iosotech.com Sigma Technology]. It is used to improve the performance of complex systems and technological processes and to develop new materials based on a search for their optimal parameters. IOSO is easily integrated with almost any [[computer aided engineering]] (CAE) tool.\n\nIOSO group of software consists of:\n* IOSO NM: Multi-objective optimization;\n* IOSO PM: Parallel multi-objective optimization;\n* IOSO LM: Multilevel multi-objective optimization with adaptive change of the object   model fidelity (low-, middle-, high fidelity models);\n* IOSO RM: Robust design optimization and robust optimal control software;\n\n==Purpose==\n\n===Performance improvement and design optimization===\n\nIOSO NM is used to maximize or minimize system or object characteristics which can include the performance or cost of or loads on the object in question. The search for optimal values for object or system characteristics is carried out by means of optimal change to design, geometrical or other parameters of the object.\n\n===Search for optimal system management laws===\n\nIt is often necessary to select or co-ordinate management parameters for the system while it is in operation in order to achieve a certain effect during the operation of the system or to reduce the impact of some factors on the system.\n\n===Identification of mathematical models===\n\nWhen the design process involves the use of any mathematical models of real-life objects, whether commercial or corporate, there is the problem of co-ordinating the experiment findings and model computation results. All models imply a set of unknown factors or constants. Searching for the optimal values thereof makes it possible to co-ordinate the experiment findings and model computation results.\n\n==Robust design optimization and robust optimal control==\n\n===Introduction===\n\nPractical application of the numerical optimization results is difficult because any complex technical system is a stochastic system and the characteristics of this system have probabilistic nature. We would like to emphasize  that, speaking about the stochastic properties of a technical system within the frame of optimization tasks, we imply that the important parameters of any system are stochastically spread. Normally it occurs during the production stage despite of the up-to-date level of modern technology. Random deviations of the system parameters lead to a random change in system efficiency.\n\nAn efficiency extreme value, obtained during the optimization problem while solving in traditional (deterministic) approach, is simply a maximum attainable value and can be considered as just conventional optimum from the point of view of its practical realization. Thus, one can consider two different types of optimization criteria. One of them is an ideal efficiency which can be achieved under the conditions of absolutely precise practical replication of the system parameters under consideration. Other optimization criteria are of probabilistic nature. For example: mathematical expectation of the efficiency; the total probability of assuring preset constraints; variance of the efficiency and so on\nIt is evident that the extreme of the one of these criteria doesn't guarantee the assurance of the high level of another one. Even more, these criteria may contradict to each other. Thus, in this case we have a [[multiobjective optimization]] problem.\n\n===IOSO robust design optimization concept===\n\nIOSO concept of robust design optimization and robust optimal control allows to determine the optimal practical solution that could be implemented with the high probability for the given technology level of the production plants. Many modern probabilistic approaches either employ the estimation of probabilistic efficiency criteria only at the stage of the analysis of obtaining deterministic solution, or use significantly simplified assessments of probabilistic criteria during optimization process. The distinctive feature of our approach is that during robust design optimization we solve the optimization problem involving direct stochastic formulation, where the estimation of probabilistic criteria is accomplished at each iteration. This procedure reliably produces fully robust optimal solution. High efficiency of the robust design optimization is provided by the capabilities of IOSO algorithms to solve stochastic optimization problems with large level of noise.\n\n==References==\n*I.N. Egorov. Indirect Optimization Method on the Basis of Self-Organization. ICOTA'98, Perth, Australia, July 1...3, 1998 Conference Proceedings, vol.2, pp.&nbsp;683–690\n*[http://www.iosotech.com/text/ASME2003-38180.pdf Brian H. Dennis, Igor N. Egorov, Helmut Sobieczky, George S. Dulikravich, Shinobu Yoshimura. PARALLEL THERMOELASTICITY OPTIMIZATION OF 3-D SERPENTINE COOLING PASSAGES IN TURBINE BLADES. GT2003-38180, Proceedings of Turbo Expo 2003; Power for Land, Sea, and Air; June 16–19, 2003, Atlanta, Georgia, USA]\n*[http://www.iosotech.com/text/ASME2003-38051.pdf Brian H. Dennis, Igor N. Egorov, George S. Dulikravich, Shinobu Yoshimura. OPTIMIZATION OF A LARGE NUMBER OF COOLANT PASSAGES LOCATED CLOSE TO THE SURFACE OF A TURBINE BLADE. GT2003-38051, Proceedings of Turbo Expo 2003; 2003 ASME Turbo Expo; Atlanta, Georgia, June 16–19, 2003]\n*[http://www.iosotech.com/text/wccm5_ms149.pdf Egorov, I.N., Kretinin, G.V. and Leshchenko, I.A. \"Robust Design Optimization Strategy of IOSO Technology\". WCCM V, Fifth World Congress on Computational Mechanics, July 7–12, 2002, Vienna, Austria]\n*[http://www.iosotech.com/text/aiaa_4328.pdf Egorov, I.N., Kretinin, G.V. and Leshchenko, I.A. \"How to Execute Robust Design Optimization\" (.pdf, 395Kb), 9th AIAA/ISSMO Symposium on Multidisciplinary Analysis and Optimization, 04–06 September 2002, Atlanta, Georgia]\n\n==External links==\n*[http://iosotech.com IOSO technology website]\n\nApplication examples\n*[http://www.iosotech.com/files/application/mutistage-compressor-optimization.pdf Optimization of the Gas Turbine Engine Parts Using Methods of Numerical Simulation (pdf, 1500Kb)]\n*[http://www.iosotech.com/files/application/Sam146-Stress-Fan-Optimization.pdf Sam146 Fan Stress Characteristics Optimization by IOSO (pdf, 120Kb)]\n*[http://www.iosotech.com/files/application/Thermoelasticity-optimization-cooling-passages.pdf Parallel Thermoelasticity Optimization of 3-D Serpentine Cooling Passages in Turbine Blades (pdf, 260Kb)]\n*[http://www.iosotech.com/files/application/turbine-disk-structural-optimization.pdf Optimization of Turbine Disk aimed to Mass and Stress Reduction (pdf, 680Kb)]\n*[http://www.iosotech.com/files/application/microprocessor-system-calibration.pdf Calibration of Microprocessor Control Systems (pdf, 480Kb)]\n*[http://www.iosotech.com/files/application/Alloys-properties-optimization.pdf Optimization of concentrations of alloying elements in steel (pdf, 370 Kb)]\n*[http://www.iosotech.com/files/application/Paper-AUC2007.pdf Application of IOSO NM and ABAQUS at Civil Structures of NPP (pdf, 550Kb)]\n\n[[Category:Mathematical optimization software]]\n[[Category:Computer system optimization software]]\n[[Category:Computer-aided engineering software]]\n[[Category:Computer-aided design software]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "IPOPT",
      "url": "https://en.wikipedia.org/wiki/IPOPT",
      "text": "{{refimprove|date=June 2017}}\n{{notability|date=June 2017}}\n\n{{Infobox software\n| name                   = IPOPT\n| title                  = \n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo size              = \n| logo alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot size        = \n| screenshot alt         = \n| collapsible            = \n| author                 = \n| developer              = Andreas Wächter, Carl Laird\n| released               = {{Start date and age|2005|8|26}}\n| discontinued           = \n| latest release version = 3.12.11<ref>[http://www.coin-or.org/download/source/Ipopt/ Index of /download/source/Ipopt]</ref>\n| latest release date    = {{Start date and age|2018|9|17}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| status                 = \n| programming language   = \n| operating system       = [[UNIX]], [[Linux]], [[OS X]], [[Microsoft Windows]]\n| platform               = \n| size                   = \n| language               = \n| language count         = <!-- Number only -->\n| language footnote      = \n| genre                  = \n| license                = [[Eclipse Public License]]\n| alexa                  = \n| website                = {{url|https://projects.coin-or.org/Ipopt}}\n| standard               = \n| AsOf                   = \n}}\n\n'''IPOPT''', short for \"'''I'''nterior '''P'''oint '''OPT'''imizer, pronounced I-P-Opt\", is a [[software]] [[Library (computing)|library]] for large scale [[nonlinear optimization]] of continuous systems. It is written in [[Fortran]] and [[C (programming language)|C]] and is released under the [[Eclipse Public License|EPL]] (formerly [[Common Public License|CPL]]). IPOPT implements a [[primal-dual method|primal-dual]] [[interior point method]], and uses line searches based on [[Filter method (optimization)|Filter method]]s ([[Roger Fletcher (mathematician)|Fletcher]] and Leyffer). IPOPT can be called from various modeling environments and [[C (programming language)|C]].\n\nIPOPT is part of the [[COIN-OR]] project.\n\nIPOPT is designed to exploit 1st and 2nd derivative ([[Hessian matrix|Hessians]]) information if provided (usually via [[automatic differentiation]] routines in modeling environments such as [[AMPL]]). If no Hessians are provided, IPOPT will approximate them using a [[quasi-Newton method]]s, specifically a [[BFGS method|BFGS update]].\n\nIPOPT was originally developed<ref>A. Wächter and L.T. Biegler, On the Implementation of a Primal-Dual Interior Point Filter Line Search Algorithm for Large-Scale Nonlinear Programming, Mathematical Programming 106 (2006) 25-57.</ref> by Ph.D. student [http://www.mccormick.northwestern.edu/research-faculty/directory/profiles/waechter-andreas.html Andreas Wächter] and Prof. [http://numero.cheme.cmu.edu/ Lorenz T. Biegler] of the Department of Chemical Engineering at [[Carnegie Mellon University]].  Their work was recognized with the [https://www.informs.org/Recognizing-Excellence/Award-Recipients/Andreas-Wachter INFORMS Computing Society Prize] in 2009.\n\nArvind Raghunathan later created an extension to IPOPT for [[Mathematical programming with equilibrium constraints]] (MPEC) [http://epubs.siam.org/sam-bin/getfile/SIOPT/articles/42908.pdf]. This version of IPOPT is generally known as IPOPT-C (with the 'C' standing for 'complementarity'). While in theory any [[Mixed integer programming|mixed-integer program]] can be recast as an MPEC, it may or may not be solvable with IPOPT-C. Solution of MINLPs (Mixed-Integer Nonlinear Programs) using IPOPT is still being explored [http://egon.cheme.cmu.edu/IBM/page.htm] [https://projects.coin-or.org/Bonmin].\n\n[https://engineering.purdue.edu/ChE/people/ptProfile?id=105202 Carl Laird] and Andreas Wächter are the developers of IPOPT 3.0, which is a re-implementation of IPOPT in [[C++]].\n\n== References ==\n{{reflist}}\n\n== See also ==\n* [[AIMMS]]\n* [[AMPL]]\n* [[APMonitor]]\n* [[General Algebraic Modeling System|GAMS]]\n* [[MATLAB]]\n\n== External links ==\n* [https://projects.coin-or.org/Ipopt IPOPT home page]\n\n{{Mathematical optimization software}}\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]\n[[Category:Optimization algorithms and methods]]\n\n\n{{Compu-library-stub}}"
    },
    {
      "title": "Iterated conditional modes",
      "url": "https://en.wikipedia.org/wiki/Iterated_conditional_modes",
      "text": "In [[statistics]], '''iterated conditional modes''' is a [[deterministic algorithm]] for obtaining a configuration of a local maximum of the [[joint probability]] of a [[Markov random field]]. It does this by iteratively maximizing the probability of each variable [[conditional distribution|conditioned]] on the rest.\n\n== See also ==\n* [[Belief propagation]]\n* [[Graph cuts in computer vision]]\n* [[Optimization problem]]\n\n==References==\n*{{Citation\n | last=Besag | first = J. E. |authorlink=Julian Besag\n | year=1986\n | title=On the Statistical Analysis of Dirty Pictures\n | journal=[[Journal of the Royal Statistical Society]], Series B\n | volume=48 |issue=3 | pages=259–302\n | jstor=2345426\n}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:NP-complete problems]]\n[[Category:Computational statistics]]\n\n\n{{statistics-stub}}"
    },
    {
      "title": "Iterated local search",
      "url": "https://en.wikipedia.org/wiki/Iterated_local_search",
      "text": "[[Image:iterated local search.png|thumb|right|220px|Iterated local search ''kicks'' a solution out from a local optimum]]\n'''Iterated Local Search'''<ref>{{cite book|last=Lourenço|first=H.R. |author2=Martin O. |author3=Stützle T.|title=Iterated Local Search: Framework and Applications|journal=Handbook of Metaheuristics, 2nd. Edition.|year=2010|volume=146|series=Kluwer Academic Publishers, International Series in Operations Research & Management Science|pages=363–397|url=https://www.springer.com/business+%26+management/operations+research/book/978-1-4419-1663-1|doi=10.1007/978-1-4419-1665-5_12|isbn=978-1-4419-1663-1 |citeseerx=10.1.1.187.2089 }}</ref><ref>{{cite journal|last=Lourenço|first=H.R. |author2=Martin O. |author3=Stützle T.|title=Iterated Local Search|journal=Handbook of Metaheuristics|year=2003|volume=57|series=Kluwer Academic Publishers, International Series in Operations Research & Management Science|pages=321–353|url=https://www.springer.com/mathematics/book/978-1-4020-7263-5}}</ref> ('''ILS''') is a term in [[applied mathematics]] and [[computer science]]\ndefining a modification of [[Local search (optimization)|local search]] or [[hill climbing]] methods for solving discrete optimization problems.\n\nLocal search methods can get stuck in a [[local minimum]], where\nno improving neighbors are available.\n\nA simple modification consists of ''iterating'' calls to the local search routine,\neach time starting from a different initial configuration. This is called ''repeated local search'', \nand implies that the knowledge obtained during the previous local search phases\nis not used.\nLearning implies that the previous history, for example the memory about the previously found local minima, \nis mined to produce better and better starting points for local search.\n \nThe implicit assumption is that of a ''clustered distribution of [[local optimum|local minima]]'': \nwhen minimizing a function, determining good local minima is easier when starting from a [[maxima and minima|local minimum]] with a \nlow  value than when starting from a random point. \nThe only caveat is to \navoid confinement in a given attraction basin, so that the ''kick'' to transform \na local minimizer into the starting point for the next run has to be appropriately strong, \nbut not too strong to avoid reverting to memory-less random restarts.\n\nIterated Local Search is based on building a sequence of locally optimal solutions by: \n# perturbing the current local minimum; \n# applying local search after starting from the modified solution.\n\nThe perturbation strength has to be sufficient to lead the trajectory to a different\nattraction basin leading to a different [[local optimum]].\n==Perturbation Algorithm==\nThe perturbation algorithm for ILS is not an easy task. The main aim is not to get stuck into the same local minimum and in order to get this property true, the undo operation is forbidden. Despite this, a good permutation has to consider a lot of values, since exist two kind of bad permutations:\n# too weak: fall back to the same local minimum\n# too strong: random restart\n===Benchmark Perturbation===\nThe procedure consists in fix a number of values for the perturbation such that these values are significant for the instance: on average probability and not rare. After that, on runtime it will be possible to check the benchmark plot in order to get an average idea on the instances passed.\n===Adaptive Perturbation===\nSince there is no function ''a priori'' that tells which one is the most suitable value for our perturbation, the best criteria is to get it adaptive. For instance Battiti and Protasi proposed a reactive search algorithm for [[MAX-SAT]] which fits perfectly into the ILS.\nframework. They perform a “directed” perturbation scheme which is implemented by a tabu search algorithm and after each perturbation they apply\na standard local descent algorithm. Another way of adapting the perturbation is to change deterministically its strength during the search.\n===Optimizing Perturbation===\nAnother procedure is to optimize a sub-part of the problem, having that keeps active the not-undo property. If this procedure is possible, all solutions generated after the perturbations tend to be very good. Furthermore the ''new'' parts are optimized too.\n==Conclusions==\nThe method has been applied to several [[Combinatorial optimization|Combinatorial Optimization]] Problems including the [[Job Shop Scheduling|Job-Shop Scheduling]] Problems,<ref>{{cite book|last=Lourenço|first=H.R.|author2=Zwijnenburg M.|title=Combining the large-step optimization with tabu-search: application to the job-shop scheduling problem|journal=Meta-Heuristics: Theory and Applications|year=1996|series=Kluwer Academic Publishers|pages=219–236|url=https://www.springer.com/business+%26+management/operations+research/book/978-0-7923-9700-7|isbn=9780792397007|publisher=Springer|doi=10.1007/978-1-4613-1361-8_14}}</ref><ref>{{cite journal|last=Lourenço|first=H.R.|title=Job-Shop Scheduling: computational study of local search and large-step optimization methods|journal=European Journal of Operational Research|year=1995|volume=83|issue=2|pages=347–364|url=http://www.sciencedirect.com/science/article/pii/037722179500012F|doi=10.1016/0377-2217(95)00012-F}}</ref> Flow-Shop Problems,<ref>{{cite journal|last1=Juan|first1=A.A.|author2=Lourenço, H. |author3=Mateo, M. |author4=Luo, R. |author5= Castella, Q. |title=Using Iterated Local Search for solving the Flow-Shop Problem: parametrization, randomization and parallelization issues|journal=International Transactions in Operational Research|year=2013|url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-ITOR.html}}</ref> [[Vehicle routing problem|Vehicle Routing Problems]] <ref>{{cite journal|last1=Penna|first1=Puca H.V.|last2=Satori Ochi|first2=L.|last3=Subramanian|first3=A.|title=An Iterated Local Search heuristic for the Heterogeneous Fleet Vehicle Routing Problem|journal=Journal of Heuristics|year=2013|volume=19|issue=2|pages=201–232|doi=10.1007/s10732-011-9186-y}}</ref> as well as many others.\n\n== References ==\n{{reflist}}\n<ref>{{Cite web | url=http://homes.di.unimi.it/~cordone/courses/2017-ae/2017-ae.html | title=Roberto Cordone - Corsi - Algoritmi euristici (A.A. 2016/17)}}</ref>\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Job shop scheduling",
      "url": "https://en.wikipedia.org/wiki/Job_shop_scheduling",
      "text": "{{Other uses|Scheduling (disambiguation){{!}}Scheduling}}\n\n'''Job shop scheduling''' or '''the job-shop problem''' ('''JSP''') is an [[optimization problem]] in [[computer science]] and [[Operations Research|operations research]] in which jobs are assigned to resources at particular times.  The most basic version is as follows: We are given ''n'' jobs ''J''<sub>1</sub>,&nbsp;''J''<sub>2</sub>,&nbsp;...,&nbsp;''J<sub>n</sub>'' of varying processing times, which need to be scheduled on ''m'' machines with varying processing power, while trying to minimize the [[makespan]]. The makespan is the total length of the schedule (that is, when all the jobs have finished processing). \n\nThe standard version of the problem is where you have ''n'' jobs ''J''<sub>1</sub>,&nbsp;''J''<sub>2</sub>,&nbsp;...,&nbsp;''J<sub>n</sub>''. Within each job there is a set of operations ''O''<sub>1</sub>,&nbsp;''O''<sub>2</sub>,&nbsp;...,&nbsp;''O<sub>n</sub>'' which need to be processed in a specific order (known as Precedence constraints). Each operation has a specific machine that it needs to be processed on and only one operation in a job can be processed at a given time. A common relaxation is the '''flexible''' job shop where each operation can be processed on any machine (the machines are identical). \n\nThis problem is one of the best known combinatorial optimization problems, and was the first problem for which [[Competitive analysis (online algorithm)|competitive analysis]] was presented, by Graham in 1966.<ref name=\"graham1966\">{{cite journal|first=R.|last=Graham|title=Bounds for certain multiprocessing anomalies|journal=Bell System Technical Journal|volume=45|issue=9|pages=1563–1581|year=1966 | url=http://www.math.ucsd.edu/~ronspubs/66_04_multiprocessing.pdf |doi=10.1002/j.1538-7305.1966.tb01709.x}}</ref>\nBest problem instances for basic model with makespan objective are due to Taillard.<ref name=\"TaillardIns\">{{cite web | url=http://mistic.heig-vd.ch/taillard/problemes.dir/ordonnancement.dir/ordonnancement.html |title=Taillard Instances}}</ref>\n\nThe name originally came from the scheduling of jobs in a [[job shop]], but the theme has wide applications beyond that type of instance.\n\n== Problem variations ==\nMany variations of the problem exist, including the following:\n* Machines can have duplicates (duplicate machines) or be identical (flexible job shop)  <ref>{{cite article|last1=Maccarthy|title=Addressing the gap in scheduling research: a review of optimization and heuristic methods in production scheduling|date=1993}}</ref>\n* Machines can require a certain gap between jobs or no idle-time\n* Machines can have sequence-dependent setups\n* Objective function can be to minimize the makespan, the [[Lp space|''L<sub>p</sub>'']] norm, tardiness, maximum lateness etc. It can also be multi-objective optimization problem\n* Jobs may have constraints, for example a job ''i'' needs to finish before job ''j'' can be started (see [[workflow]]). Also, the objective function can be multi-criteria.<ref>{{cite book|last1=Malakooti|first1=B|title=Operations and Production Systems with Multiple Objectives|date=2013|publisher=John Wiley & Sons|isbn=978-1-118-58537-5}}</ref>\n* Set of jobs can relate to different set of machines\n* Deterministic (fixed) processing times or probabilistic processing times\n\n==NP-hardness==\nSince the [[travelling salesman problem]] is [[NP-hard]], the job-shop problem with [[sequence-dependent setup]] is clearly also NP-hard, since the TSP is a special case of the JSP with a single job (the cities are the machines and the salesman is the jobs).\n\n== Problem representation ==\n\nThe [[disjunctive graph]] <ref>B. Roy, B. Sussmann, Les problèmes d’ordonnancement avec constraintes disjonctives, SEMA, Note D.S., No. 9, Paris, 1964.</ref> is one of the popular models used for describing the job shop scheduling problem instances.<ref>Jacek Błażewicz, Erwin Pesch, Małgorzata Sterna, The disjunctive graph machine representation of the job shop scheduling problem, European Journal of Operational Research, Volume 127, Issue 2, 1 December 2000, Pages 317-331, ISSN 0377-2217, 10.1016/S0377-2217(99)00486-5.</ref>\n\nA mathematical statement of the problem can be made as follows:\n\nLet <math>M = \\{ M_{1}, M_{2}, \\dots, M_{m} \\}</math> and <math>J = \\{ J_{1}, J_{2}, \\dots, J_{n} \\}</math> be two [[Finite set|finite]] [[set (mathematics)|sets]]. On account of the industrial origins of the problem, the <math>\\displaystyle M_{i}</math> are called '''machines''' and the <math>\\displaystyle J_{j}</math> are called '''jobs'''.\n\nLet <math>\\displaystyle \\ \\mathcal{X}</math> denote the set of all sequential assignments of jobs to machines, such that every job is done by every machine exactly once; elements <math>x \\in \\mathcal{X}</math> may be written as <math>n \\times m</math> matrices, in which column <math>\\displaystyle i</math> lists the jobs that machine <math>\\displaystyle M_{i}</math> will do, in order. For example, the matrix\n\n: <math>x = \\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\\\ 3 & 1 \\end{pmatrix}</math>\n\nmeans that machine <math>\\displaystyle M_{1}</math> will do the three jobs <math>\\displaystyle J_{1}, J_{2}, J_{3}</math> in the order <math>\\displaystyle J_{1}, J_{2}, J_{3}</math>, while machine <math>\\displaystyle M_{2}</math> will do the jobs in the order <math>\\displaystyle J_{2}, J_{3}, J_{1}</math>.\n\nSuppose also that there is some '''cost function''' <math>C : \\mathcal{X} \\to [0, + \\infty]</math>. The cost function may be interpreted as a \"total processing time\", and may have some expression in terms of times <math>C_{ij} : M \\times J \\to [0, + \\infty]</math>, the cost/time for machine <math>\\displaystyle M_{i}</math> to do job <math>\\displaystyle J_{j}</math>.\n\nThe '''job-shop problem''' is to find an assignment of jobs <math>x \\in \\mathcal{X}</math> such that <math>\\displaystyle C(x)</math> is a minimum, that is, there is no <math>y \\in \\mathcal{X}</math> such that <math>\\displaystyle C(x) > C(y)</math>.\n\n==Scheduling efficiency==\nScheduling efficiency can be defined for a schedule through the ratio of total machine idle time to the total processing time as below:\n\n<math>C'=1+{\\sum_{i}l_i \\over \\sum_{j,k}p_{jk}}={C.m \\over \\sum_{j,k}p_{jk}}</math>\n\nHere <math>l_i</math> is the idle time of machine <math>i</math>, <math>C</math> is the makespan and <math>m</math> is the number of machines. Notice that with the above definition, scheduling efficiency is simply the makespan normalized to the number of machines and the total processing time. This makes it possible to compare the usage of resources across JSP instances of different size.<ref name=\":0\">{{Cite journal|last=Mirshekarian|first=Sadegh|last2=Šormaz|first2=Dušan N.|date=2016|title=Correlation of job-shop scheduling problem features with scheduling efficiency|url=http://mirshekarian.me/files/corrJSSP.pdf|journal=Expert Systems with Applications|volume=62|pages=131–147|doi=10.1016/j.eswa.2016.06.014}}</ref>\n\n==The problem of infinite cost==\nOne of the first problems that must be dealt with in the JSP is that many proposed solutions have infinite cost: i.e., there exists <math>x_{\\infty} \\in \\mathcal{X}</math> such that <math>C(x_{\\infty}) = + \\infty</math>. In fact, it is quite simple to concoct examples of such <math>x_{\\infty}</math> by ensuring that two machines will [[deadlock]], so that each waits for the output of the other's next step.\n\n== Major results ==\n{{Generalize|section|date=October 2009}}\nGraham had already provided the List scheduling algorithm in 1966, which is {{math|(2 &minus; 1/''m'')}}-competitive, where m is the number of machines.<ref name=\"graham1966\"/>  Also, it was proved that List scheduling is optimum online algorithm for 2 and 3 machines. The [[Coffman–Graham algorithm]] (1972) for uniform-length jobs is also optimum for two machines, and is {{math|(2 &minus; 2/''m'')}}-competitive.<ref>{{citation\n | last1 = Coffman | first1 = E. G. Jr. | author1-link = Edward G. Coffman Jr.\n | last2 = Graham | first2 = R. L. | author2-link = Ronald Graham\n | mr = 0334913\n | journal = Acta Informatica\n | pages = 200–213\n | title = Optimal scheduling for two-processor systems\n | url = http://www.math.ucsd.edu/~ronspubs/72_04_two_processors.pdf\n | volume = 1\n | issue = 3 | year = 1972 | doi=10.1007/bf00288685}}.</ref><ref>{{citation\n | last1 = Lam | first1 = Shui\n | last2 = Sethi | first2 = Ravi | author2-link = Ravi Sethi\n | doi = 10.1137/0206037\n | mr = 0496614\n | issue = 3\n | journal = [[SIAM Journal on Computing]]\n | pages = 518–536\n | title = Worst case analysis of two scheduling algorithms\n | volume = 6\n | year = 1977}}.</ref> In 1992, Bartal, Fiat, Karloff and Vohra presented an algorithm that is 1.986 competitive.<ref>{{cite conference|first=Y.|last=Bartal|author2=A. Fiat |author3=H. Karloff |author4=R. Vohra |year=1992|title=New Algorithms for an Ancient Scheduling Problem|booktitle=Proc. 24th ACM Symp.|conference=Theory of Computing|pages=51–58|doi=10.1145/129712.129718}}</ref>  A 1.945-competitive algorithm was presented by Karger, Philips and Torng in 1994.<ref>{{cite conference|first=D.|last=Karger|author-link=David Karger|author2=S. Phillips |author3=E. Torng |title=A Better Algorithm for an Ancient Scheduling Problem|booktitle=Proc. Fifth ACM Symp.|conference=Discrete Algorithms|year=1994|url=http://portal.acm.org/citation.cfm?doid=314464.314487}}</ref>  In 1992, Albers provided a different algorithm that is 1.923-competitive.<ref>{{cite conference|title=Improved parallel integer sorting without concurrent writing|conference=Symposium on Discrete Algorithms archive|booktitle=Proceedings of the third annual ACM-SIAM symposium on Discrete algorithms|pages=463–472|year=1992|first=Susanne|last=Albers|author-link=Susanne Albers|author2=Torben Hagerup|url=http://portal.acm.org/citation.cfm?id=139404.139491}}</ref> Currently, the best known result is an algorithm given by Fleischer and Wahl, which achieves a competitive ratio of 1.9201.<ref>{{cite book|last=Fleischer|first=Rudolf|title=Algorithms - ESA 2000|year=2000|publisher=Springer|location=Berlin / Heidelberg|isbn=978-3-540-41004-1|pages=202–210|url=http://www.springerlink.com/content/fdnaut9ahjfe35r5/export-citation/}}</ref>\n\nA lower bound of 1.852 was presented by Albers.<ref>{{cite journal|last=Albers|first=Susanne|author-link=Susanne Albers|title=Better bounds for online scheduling|journal=[[SIAM Journal on Computing]]|volume=29|pages= 459–473|year=1999|doi=10.1137/S0097539797324874|issue=2|citeseerx=10.1.1.685.8756}}</ref>\nTaillard instances has an important role in developing job shop scheduling with makespan objective.\n\nIn 1976 Garey provided a proof<ref name=\"garey1976\">{{cite journal|doi=10.1287/moor.1.2.117|first=M.R.|last=Garey|title=The Complexity of Flowshop and Jobshop Scheduling|journal=Mathematics of Operations Research|volume=1|issue=2|pages=117–129|year=1976|jstor=3689278}}</ref> that this problem is [[NP-complete]] for m>2, that is, no optimal solution can be computed in polynomial time for three or more machines (unless [[P=NP]]).\n\nIn 2011 Xin Chen et al. provided optimal algorithms for online scheduling on two related machines<ref>{{cite journal | last1 = Chen | first1 = Xin | last2 = Lan | first2 = Yan | last3 = Benkő | first3 = Attila | title = [https://scholar.google.com/citations?user=txyI5aAAAAAJ PDF], György Dósa, Xin Hana (2011). ''Optimal algorithms for online scheduling with bounded rearrangement at the end'' | url = | journal = Theoretical Computer Science | volume = 412 | issue = 45| pages = 6269–6278 | doi = 10.1016/j.tcs.2011.07.014 | year = 2011 }}</ref> improving previous results.<ref>{{cite journal | last1 = Liu | first1 = M. | last2 = Xu | first2 = Y. | last3 = Chu | first3 = C. | last4 = Zheng | first4 = F. | year = 2009 | title = Online scheduling on two uniform machines to minimize the makespan | url = | journal = Theoret. Comput. Sci. | volume = 410 | issue = 21–23| pages = 2099–2109 | doi=10.1016/j.tcs.2009.01.007}}</ref>\n\n== Offline makespan minimization ==\n\n=== Atomic jobs ===\n{{see also|Multiprocessor scheduling}}\nThe simplest form of the offline makespan minimisation problem deals with atomic jobs, that is, jobs that are not subdivided into multiple operations. It is equivalent to packing a number of items of various different sizes into a fixed number of bins, such that the maximum bin size needed is as small as possible. (If instead the number of bins is to be minimised, and the bin size is fixed, the problem becomes a different problem, known as the [[bin packing problem]].)\n\n[[Dorit S. Hochbaum]] and [[David Shmoys]] presented a [[polynomial-time approximation scheme]] in 1987 that finds an approximate solution to the offline makespan minimisation problem with atomic jobs to any desired degree of accuracy.<ref name='hs'>{{cite journal|first1=Dorit|last1=Hochbaum|author1-link=Dorit S. Hochbaum|first2=David|last2=Shmoys|authorlink2=David Shmoys|title=Using dual approximation algorithms for scheduling problems: theoretical and practical results|journal=[[Journal of the ACM]]|volume=34|issue=1|pages=144–162|year=1987|url=http://www.columbia.edu/~cs2035/courses/ieor6400.F07/hs.pdf | doi = 10.1145/7531.7535|citeseerx=10.1.1.125.5753}}</ref>\n\n=== Jobs consisting of multiple operations ===\nThe basic form of the problem of scheduling jobs with multiple (M) operations, over M machines, such that all of the first operations must be done on the first machine, all of the second operations on the second, etc., and a single job cannot be performed in parallel, is known as the '''[[open shop scheduling]]''' problem. Various algorithms exist, including [[genetic algorithm]]s.<ref name='ossp-ga'>{{cite conference|citeseerx = 10.1.1.29.4699|title=Genetic Algorithms for Solving Open Shop Scheduling Problems|author=Khuri, Sami|author2=Miryala, Sowmya Rao |year=1999|booktitle=Proceedings of the 9th Portuguese Conference on Artificial Intelligence: Progress in Artificial Intelligence|publisher=[[Springer Verlag]]|location=London}}</ref>\n\n==== Johnson's algorithm====\n{{see also|Johnson's rule}}\n\nA heuristic algorithm by S. M. Johnson can be used to solve the case of a 2 machine N job problem when all jobs are to be processed in the same order.<ref>S.M. Johnson, Optimal two- and three-stage production schedules with setup times included, Naval Res. Log. Quart. I(1954)61-68.</ref>  The steps of algorithm are as follows:\n\nJob P<sub>i</sub> has two operations, of duration P<sub>i1</sub>, P<sub>i2</sub>, to be done on Machine M1, M2 in that sequence.\n\n*''Step 1.'' List A = { 1, 2, …, N }, List L1 = {}, List L2 = {}.\n*''Step 2.'' From all available operation durations, pick the minimum.\n\nIf the minimum belongs to P<sub>k1</sub>,\n\nRemove K from list A; Add K to end of List L1.\n\nIf minimum belongs to P<sub>k2</sub>,\n\nRemove K from list A; Add K to beginning of List L2.\n\n*''Step 3.'' Repeat Step 2 until List A is empty.\n*''Step 4.'' Join List L1, List L2. This is the optimum sequence.\n\nJohnson's method only works optimally for two machines. However, since it is optimal, and easy to compute, some researchers have tried to adopt it for M machines, (''M''&nbsp;>&nbsp;2.)\n\nThe idea is as follows: Imagine that each job requires m operations in sequence, on M1, M2 … Mm. We combine the first ''m''/2 machines into an (imaginary) Machining center, MC1, and the remaining Machines into a Machining Center MC2. Then the total processing time for a Job P on MC1 = sum( operation times on first ''m''/2 machines), and processing time for Job P on MC2 = sum(operation times on last ''m''/2 machines).\n\nBy doing so, we have reduced the m-Machine problem into a Two Machining center scheduling problem. We can solve this using Johnson's method.\n\n== Makespan prediction ==\nMachine learning has been recently used to ''predict'' the optimal makespan of a JSP instance without actually producing the optimal schedule.<ref name=\":0\" /> Preliminary results show an accuracy of around 80% when supervised machine learning methods were applied to classify small randomly generated JSP instances based on their optimal scheduling efficiency compared to the average.\n\n== Example ==\n\nHere is an example of a job shop scheduling problem formulated in [[AMPL]] as a [[linear programming#Integer unknowns|mixed-integer programming]] problem with indicator constraints:\n<source lang=\"ampl\">\n param N_JOBS;\n param N_MACHINES;\n \n set JOBS ordered = 1..N_JOBS;\n set MACHINES ordered = 1..N_MACHINES;\n \n param ProcessingTime{JOBS, MACHINES} > 0;\n \n param CumulativeTime{i in JOBS, j in MACHINES} =\n    sum {jj in MACHINES: ord(jj) <= ord(j)} ProcessingTime[i,jj];\n \n param TimeOffset{i1 in JOBS, i2 in JOBS: i1 <> i2} =\n    max {j in MACHINES}\n      (CumulativeTime[i1,j] - CumulativeTime[i2,j] + ProcessingTime[i2,j]);\n \n var end >= 0;\n var start{JOBS} >= 0;\n var precedes{i1 in JOBS, i2 in JOBS: ord(i1) < ord(i2)} binary;\n \n minimize makespan: end;\n \n subj to makespan_def{i in JOBS}:\n    end >= start[i] + sum{j in MACHINES} ProcessingTime[i,j];\n \n subj to no12_conflict{i1 in JOBS, i2 in JOBS: ord(i1) < ord(i2)}:\n    precedes[i1,i2] ==> start[i2] >= start[i1] + TimeOffset[i1,i2];\n \n subj to no21_conflict{i1 in JOBS, i2 in JOBS: ord(i1) < ord(i2)}:\n    !precedes[i1,i2] ==> start[i1] >= start[i2] + TimeOffset[i2,i1];\n \n data;\n \n param N_JOBS := 4;\n param N_MACHINES := 4;\n \n param ProcessingTime:\n   1 2 3 4 :=\n 1 4 2 1\n 2 3 6 2\n 3 7 2 3\n 4 1 5 8;\n</source>\n\n== See also ==\n{{div col|colwidth=22em}}\n* [[Disjunctive graph]]\n* [[Dynamic programming]]\n* [[Flow shop scheduling]]\n* [[Genetic algorithm scheduling]]\n* [[List of NP-complete problems]]\n* [[Open shop scheduling]]\n* [[Optimal control]]\n* [[Scheduling (production processes)]]\n* [[Truthful job scheduling]]\n{{div col end}}\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [http://www.mat.univie.ac.at/~neum/glopt/software_g.html University of Vienna] Directory of methodologies, systems and software for dynamic optimization.\n* [http://mistic.heig-vd.ch/taillard/problemes.dir/ordonnancement.dir/jobshop.dir/best_lb_up.txt Taillard instances]\n* ''Brucker P.'' [https://link.springer.com/content/pdf/bfm%3A978-3-540-24804-0%2F1.pdf Scheduling Algorithms]. Heidelberg, Springer. Fifth ed. {{ISBN|978-3-540-24804-0}}\n* [http://www.jobshop.72.sk/?m=1EN Job Shop Visual Scheduling]\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Combinatorial optimization]]\n[[Category:Scheduling (computing)]]\n[[Category:Mathematical optimization in business]]\n\n[[pt:Escalonamento de Job Shop]]"
    },
    {
      "title": "Karmarkar's algorithm",
      "url": "https://en.wikipedia.org/wiki/Karmarkar%27s_algorithm",
      "text": "'''Karmarkar's algorithm''' is an [[algorithm]] introduced by [[Narendra Karmarkar]] in 1984 for solving [[linear programming]] problems. It was the first reasonably efficient algorithm that solves these problems in [[polynomial time]]. The [[ellipsoid method]] is also polynomial time but proved to be inefficient in practice.\n\nDenoting <math>n</math> as the number of variables and <math>L</math> as the number of bits of input to the algorithm, Karmarkar's algorithm requires <math>O(n^{3.5} L)</math> operations on <math>O(L)</math> digit numbers, as compared to <math>O(n^6 L)</math> such operations for the ellipsoid algorithm. The runtime of Karmarkar's algorithm is thus \n:<math>O(n^{3.5} L^2 \\cdot \\log L \\cdot \\log \\log L)</math>\n\nusing [[Schönhage–Strassen algorithm|FFT-based multiplication]] (see [[Big O notation]]).\n\nKarmarkar's algorithm falls within the class of [[interior point method]]s: the current guess for the solution does not follow the boundary of the [[feasible set]] as in the [[simplex method]], but it moves through the interior of the feasible region, improving the approximation of the optimal solution by a definite fraction with every iteration, and converging to an optimal solution with rational data.<ref name=\"Strang\">{{cite journal|last=Strang|first=Gilbert|authorlink=Gilbert Strang|title=Karmarkar's algorithm and its place in applied mathematics|journal=[[The Mathematical Intelligencer]]|date=1 June 1987|issn=0343-6993|pages=4–10|volume=9|number=2|doi=10.1007/BF03025891|mr=883185|ref=harv}}</ref>\n\n== The algorithm ==\nConsider a linear programming problem in matrix form:\n{| \n| colspan=\"2\" | maximize {{math|''c''<sup>T</sup>''x''}}\n|-\n| subject to\n| {{math|''Ax'' ≤ ''b''}}.\n|-\n|}\nKarmarkar's algorithm determines the next feasible direction toward optimality and scales back by a factor {{math|0 < γ ≤ 1}}. It is described in a number of sources.<ref>http://dl.acm.org/citation.cfm?id=808695</ref><ref>{{cite journal | doi=10.1007/BF02579150 | volume=4 |issue = 4| title=A new polynomial-time algorithm for linear programming | journal=Combinatorica | pages=373–395 | last1 = Karmarkar | first1 = N.|year = 1984}}</ref><ref>{{cite journal|doi=10.1002/j.1538-7305.1989.tb00316.x | volume=68 | issue=3 | title=Power Series Variants of Karmarkar-Type Algorithms | journal=AT&T Technical Journal | pages=20–36 | last1 = Karmarkar | first1 = Narendra K.| year=1989 }}</ref><ref>{{cite conference\n | last = Karmarkar | first = Narendra\n | contribution = An interior-point approach to NP-complete problems. I\n | doi = 10.1090/conm/114/1097880\n | mr = 1097880\n | pages = 297–308\n | publisher = American Mathematical Society | location = Providence, RI\n | series = Contemporary Mathematics\n | title = Mathematical developments arising from linear programming (Brunswick, ME, 1988)\n | volume = 114\n | year = 1990}}</ref><ref>{{cite conference\n | last = Karmarkar | first = Narendra\n | contribution = Riemannian geometry underlying interior-point methods for linear programming\n | doi = 10.1090/conm/114/1097865\n | mr = 1097865\n | pages = 51–75\n | publisher = American Mathematical Society | location = Providence, RI\n | series = Contemporary Mathematics\n | title = Mathematical developments arising from linear programming (Brunswick, ME, 1988)\n | volume = 114\n | year = 1990}}</ref><ref>Karmarkar N. K., Lagarias, J.C., Slutsman, L., and Wang, P., Power Series Variants of KarmarkarType Algorithm, AT & T technical Journal 68, No. 3, May/June (1989).</ref> Karmarkar also has extended the method<ref>Karmarkar, N.K., Interior Point Methods in Optimization, Proceedings of the Second International Conference on Industrial and Applied Mathematics, SIAM, pp. 160181 (1991)</ref><ref>Karmarkar, N. K. and Kamath, A. P., A continuous Approach to Deriving Upper Bounds in Quadratic Maximization Problems with Integer Constraints, Recent Advances in Global Optimization, pp. 125140, Princeton University Press (1992).</ref><ref>26.\tKarmarkar, N. K., Thakur, S. A., An Interior Point Approach to a Tensor Optimisation Problem with Application to Upper Bounds in Integer Quadratic Optimization Problems, Proceedings of Second Conference on Integer Programming and Combinatorial Optimisation, (May 1992).</ref><ref>27.\tKamath, A., Karmarkar, N. K., A Continuous Method for Computing Bounds in Integer Quadratic Optimisation Problems, Journal of Global Optimization (1992).</ref> to solve problems with integer constraints and non-convex problems.<ref>Karmarkar, N. K., Beyond Convexity: New Perspectives in Computational Optimization. Springer Lecture Notes in Computer Science LNCS 6457, Dec 2010</ref>\n{{algorithm-begin|name=Affine-Scaling}}\n\nSince the actual algorithm is rather complicated, researchers looked for a more intuitive version of it, and in 1985 developed [[affine scaling]], a version of Karmarkar's algorithm that uses [[affine transformation]]s where Karmarkar used [[projective geometry|projective]] ones, only to realize four years later that they had rediscovered an algorithm published by [[Soviet Union|Soviet]] mathematician I. I. Dikin in 1967.<ref>{{cite conference\n | last1 = Vanderbei | first1 = R. J.\n | last2 = Lagarias | first2 = J. C.\n | contribution = I. I. Dikin's convergence result for the affine-scaling algorithm\n | doi = 10.1090/conm/114/1097868\n | mr = 1097868\n | pages = 109–119\n | publisher = American Mathematical Society | location = Providence, RI\n | series = Contemporary Mathematics\n | title = Mathematical developments arising from linear programming (Brunswick, ME, 1988)\n | volume = 114\n | year = 1990}}</ref> The affine-scaling method can be described succinctly as follows.<ref>{{cite journal\n | doi = 10.1007/BF01840454\n | author = [[Robert J. Vanderbei]] |author2=Meketon, Marc |author3=Freedman, Barry\n | year = 1986\n | title =  A Modification of Karmarkar's Linear Programming Algorithm \n | journal = Algorithmica\n | volume = 1\n | issue = 1–4 | pages = 395–407 |url=https://www.princeton.edu/~rvdb/tex/myPapers/VanderbeiMeketonFreedman.pdf\n}}</ref> Note that the affine-scaling algorithm, while applicable to small scale problems, is not a polynomial time algorithm.{{Citation needed|date=February 2016}} \n   {{nowrap|Input:  A, b, c, <math>x^0</math>,}} ''stopping criterion'', {{mvar|&gamma;}}.\n\n   {{nowrap|<math> k \\leftarrow 0 </math>}}\n   {{nowrap|'''do while''' ''stopping criterion'' '''not satisfied'''}}\n      {{nowrap|<math>v^k \\leftarrow b-Ax^k</math>}}\n      {{nowrap|<math>D_v \\leftarrow \\operatorname{diag}(v_1^k,\\ldots,v_m^k)</math>}}\n      {{nowrap|<math>h_x\\leftarrow (A^TD_v^{-2}A)^{-1}c</math>}}\n      {{nowrap|<math>h_v\\leftarrow -Ah_x</math>}}\n      {{nowrap|'''if''' <math>h_v \\ge 0</math> '''then'''}}\n         '''return''' unbounded\n      '''end if'''\n      {{nowrap|<math>\\alpha \\leftarrow \\gamma\\cdot \\min\\{-v_i^k/(h_v)_i \\,\\,|\\,\\, (h_v)_i < 0,\\, i=1,\\ldots,m\\}</math>}}\n      {{nowrap|<math>x^{k+1}\\leftarrow x^k + \\alpha h_x</math>}}\n      {{nowrap|<math>k\\leftarrow k+1</math>}}\n   '''end do'''\n{{algorithm-end}}\n\n== Example ==\n[[File:karmarkar.svg|thumb|200px|right|Example solution]]\nConsider the linear program\n:<math> \\begin{array}{lrclr}\n  \\text{maximize}\n  &\n  x_1\n  +\n  x_2\n  \\\\\n  \\text{subject to}\n  &\n  2p x_1\n  + \n  x_2 \n  & \\leq & p^2+1, & p=0.0, 0.1, 0.2,\\ldots, 0.9, 1.0.\n\\end{array}\n</math>\nThat is, there are 2 variables <math>x_1, x_2</math> and 11 constraints associated with varying values of <math>p</math>. This figure shows each iteration of the algorithm as red circle points. The constraints are shown as blue lines.\n\n== Patent controversy ==\nAt the time he invented the algorithm, Karmarkar was employed by [[IBM]] as a postdoctoral fellow in the [[IBM San Jose Research Laboratory]] in California. On August 11, 1983 he gave a seminar at [[Stanford University]] explaining the algorithm, with his affiliation still listed as IBM. By the fall of 1983 Karmarkar started to work at [[AT&T]] and submitted his paper to the 1984 ACM [[Symposium on Theory of Computing]] (STOC, held April 30 - May 2, 1984) stating [[AT&T Bell Laboratories]] as his affiliation.<ref>{{citation|title=Karmarkar Algorithm|url=http://researcher.watson.ibm.com/researcher/view_page.php?id=6900|publisher=IBM Research|accessdate=2016-06-01}}</ref> After applying the algorithm to optimizing AT&T's telephone network,<ref>Sinha L.P., Freedman, B. A., Karmarkar, N. K., Putcha, A., and Ramakrishnan K.G., Overseas Network Planning, Proceedings of the Third International Network Planning Symposium, NETWORKS' 86, Tarpon Springs, Florida (June 1986).</ref> they realized that his invention could be of practical importance.  In April 1985, AT&T promptly applied for a patent on Karmarkar's algorithm.\n\nThe patent became more fuel for the ongoing controversy over the issue of [[software patent]]s.<ref name=\"kolata\">\n{{cite news\n | first = Gina\n | last = Kolata\n | title = IDEAS & TRENDS; Mathematicians Are Troubled by Claims on Their Recipes\n | url = https://query.nytimes.com/gst/fullpage.html?res=950DEFD61038F931A25750C0A96F948260\n | work = [[The New York Times]]\n | date = 1989-03-12\n}}</ref>  This left many mathematicians uneasy, such as [[Ronald Rivest]] (himself one of the holders of the patent on the [[RSA (algorithm)|RSA]] algorithm), who expressed the opinion that research proceeded on the basis that algorithms should be free. Even before the patent was actually granted, it was argued that there might have been [[prior art]] that was applicable.<ref name=\"saltzman\">[http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15850c-s96/www/interiorpoint.txt Various posts by Matthew Saltzman, Clemson University]</ref> Mathematicians who specialized in [[numerical analysis]], including [[Philip Gill]] and others, claimed that Karmarkar's algorithm is equivalent to a [[projected Newton barrier method]] with a logarithmic [[barrier function]], if the parameters are chosen suitably.<ref>\n{{cite journal\n | last = Gill\n | first = Philip E.\n | last2 = Murray |first2= Walter |last3= Saunders |first3= Michael A. |last4= Tomlin |first4= J. A. |last5= Wright |first5= Margaret H.\n | year = 1986\n | title =  On projected Newton barrier methods for linear programming and an equivalence to Karmarkar's projective method \n | journal = Mathematical Programming\n | volume = 36\n | issue = 2\n | pages = 183–209\n | url = http://www.springerlink.com/content/2781h35w87600923/\n | doi = 10.1007/BF02592025\n}}</ref>  Legal scholar Andrew Chin opines that Gill's argument was flawed, insofar as the method they describe does not constitute an \"algorithm\", since it requires choices of parameters that don't follow from the internal logic of the method, but rely on external guidance, essentially from Karmarkar's algorithm.<ref name=\"chin\">\n{{cite journal\n | author = Andrew Chin\n | year = 2009\n | title =  On Abstraction and Equivalence in Software Patent Doctrine: A Response to Bessen, Meurer and Klemens\n | journal = Journal of Intellectual Property Law\n | volume = 16\n | pages = 214–223\n | url = http://andrewchin.com/chin/scholarship/abstraction-equivalence.pdf\n}}</ref> Furthermore, Karmarkar's contributions are considered far from obvious in light of all prior work, including Fiacco-McCormick, Gill and others cited by Saltzman.<ref name=\"chin\"/><ref name=\"paley\">Mark A. Paley  (1995). \"The Karmarkar Patent:  Why Congress Should \"Open the Door\" to Algorithms as Patentable Subject Matter\". 22 COMPUTER  L. REP. 7</ref><ref name=\"wright\">{{cite journal\n | author = Margaret H. Wright\n | year = 2004\n | title =  The Interior-Point Revolution in Optimization: History, Recent Developments, and Lasting Consequences\n | journal = Bulletin of the American Mathematical Society\n | volume = 42\n | pages = 39–56\n | url = http://www.ams.org/journals/bull/2005-42-01/S0273-0979-04-01040-7/S0273-0979-04-01040-7.pdf\n | doi=10.1090/S0273-0979-04-01040-7\n}}</ref>  The patent was debated in the U.S. Senate and granted in recognition of the essential originality of Karmarkar's work, as {{US patent|4744028}}: \"Methods and apparatus for efficient resource allocation\" in May 1988.\n\nAT&T designed a [[vector processor|vector]] [[multi-processor]] computer system specifically to run Karmarkar's algorithm, calling the resulting combination of hardware and software KORBX,<ref>\n{{cite journal\n | author = Marc S. Meketon\n |author2=Y.C. Cheng |author3=D.J. Houck |author4=J.M.Liu |author5=L. Slutsman |author6=[[Robert J. Vanderbei]] |author7=P. Wang\n | year = 1989\n | title =  The AT&T KORBX System\n | journal = AT&T Technical Journal\n | volume = 68\n |issue=3 | pages = 7–19\n|doi=10.1002/j.1538-7305.1989.tb00315.x }}</ref> and marketed this system at a price of US$8.9 million.<ref>{{cite news |title=AT&T markets problem solver, based on math whiz's find, for $8.9 million |newspaper=Wall Street Journal |date=15 August 1988 |first=Roger |last=Lowenstein |url=http://orfe.princeton.edu/~rvdb/307/lectures/lec19.pdf}}</ref><ref>{{cite news |title=Big A.T.&T. Computer for Complexities |first=John |last=Markoff |date=13 August 1988 |url=https://www.nytimes.com/1988/08/13/business/big-at-t-computer-for-complexities.html}}</ref> Its first customer was the [[United States Department of Defense|Pentagon]].<ref>{{Cite web|url=https://apnews.com/8a376783cd62cdf141de700a7c948f61|title=Military Is First Announced Customer Of AT&T Software|website=AP NEWS|access-date=2019-06-11}}</ref><ref>{{Cite book | chapter-url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=70419&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D70419 | doi=10.1109/CDC.1989.70419| chapter=Using KORBX for military airlift applications| title=Proceedings of the 28th IEEE Conference on Decision and Control| pages=1603–1605| year=1989| last1=Kennington| first1=J.L.}}</ref>\n\nOpponents of software patents have further argued that the patents ruined the positive interaction cycles that previously characterized the relationship between researchers in linear programming and industry, and specifically it isolated Karmarkar himself from the network of mathematical researchers in his field.<ref>{{Cite web\n|url=http://eupat.ffii.org/papers/konno95/index.ja.html\n|title=今野浩: カーマーカー特許とソフトウェア – 数学は 特許に なるか (Konno Hiroshi: The Kamarkar Patent and Software – Has Mathematics Become Patentable?)\n|publisher=[[Foundation for a Free Information Infrastructure|FFII]]\n|accessdate=2008-06-27}}</ref>\n\nThe patent itself expired in April 2006, and the algorithm is presently in the [[public domain]].\n\nThe [[United States Supreme Court]] has held that mathematics cannot be patented in ''[[Gottschalk v. Benson]]'',<ref>409 U.S. 63 (1972). The case concerned an algorithm for converting binary-coded decimal numerals to pure binary.</ref> In that case, the Court first addressed whether computer algorithms could be patented and it held that they could not because the patent system does not protect ideas and similar abstractions. In ''[[Diamond v. Diehr]]'',<ref>450 U.S. 175 (1981).</ref> the Supreme Court stated, \"A mathematical formula as such is not accorded the protection of our patent laws, and this principle cannot be circumvented by attempting to limit the use of the formula to a particular technological environment.<ref>450 U.S. at 191. See also ''[[Parker v. Flook]]'', 437 U.S. 584, 585 (1978) (\"the discovery of a novel and useful mathematical formula may not be patented\").</ref> In ''[[Mayo Collaborative Services v. Prometheus Labs., Inc.]]'',<ref>566 U.S. __, 132 S. Ct. 1289 (2012).</ref> the Supreme Court explained further that \"simply implementing a mathematical principle on a physical machine, namely a computer, [i]s not a patentable application of that principle.\"<ref>Accord ''[[Alice Corp. v. CLS Bank Int’l]]'',  573 U.S. __, 134 S. Ct. 2347  (2014).</ref>\n\n== References ==\n\n* {{cite journal | last1 = Adler | first1 = Ilan | last2 = Karmarkar | first2 = Narendra | last3 = Resende | first3 = Mauricio G.C. | last4 = Veiga | first4 = Geraldo | year = 1989 | title = An Implementation of Karmarkar's Algorithm for Linear Programming | url = | journal = Mathematical Programming | volume = 44 | issue = 1–3| pages = 297–335 | doi=10.1007/bf01587095}}\n* Narendra Karmarkar (1984). \"[https://web.archive.org/web/20131228145520/http://retis.sssup.it/~bini/teaching/optim2010/karmarkar.pdf A New Polynomial Time Algorithm for Linear Programming]\", ''[[Combinatorica]]'', Vol '''4''', nr. 4, p.&nbsp;373&ndash;395.\n\n{{reflist|30em}}\n\n{{Optimization algorithms|convex}}\n\n{{DEFAULTSORT:Karmarkar's Algorithm}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Articles with example pseudocode]]\n[[Category:Software patent law]]\n[[Category:Linear programming]]"
    },
    {
      "title": "Killer heuristic",
      "url": "https://en.wikipedia.org/wiki/Killer_heuristic",
      "text": "{{refimprove|date=April 2018}}\n\nIn competitive two-player games, the '''killer heuristic''' is a technique for improving the efficiency of [[alpha-beta pruning]], which in turn improves the efficiency of the [[minimax algorithm]].  This algorithm has an [[exponential time|exponential]] search time to find the optimal next move, so general methods for speeding it up are very useful. [[Barbara Liskov]] created the general [[heuristic]] to speed [[tree search]] in a computer program to play [[chess endgame]]s for her Ph.D. thesis.<ref>{{Cite journal|last=Huberman (Liskov)|first=Barbara Jane|title=A program to play chess end games|year=1968|publisher=Stanford University Department of Computer Science, Technical Report CS 106, Stanford Artificial Intelligence Project Memo AI-65| url = http://www.dtic.mil/dtic/tr/fulltext/u2/673971.pdf}}</ref>\n\n[[Alpha-beta pruning]] works best when the best moves are considered first.  This is because the best moves are the ones most likely to produce a ''cutoff'', a condition where the game playing program knows that the position it is considering could not possibly have resulted from best play by both sides and so need not be considered further.  I.e. the game playing program will always make its best available move for each position.  It only needs to consider the other player's possible responses to that best move, and can skip evaluation of responses to (worse) moves it will not make.\n\nThe killer heuristic attempts to produce a cutoff by assuming that a move that produced a cutoff in another branch of the [[game tree]] at the same depth is likely to produce a cutoff in the present position, that is to say that a move that was a very good move from a different (but possibly similar) position might also be a good move in the present position.  By trying the ''killer move'' before other moves, a game playing program can often produce an early cutoff, saving itself the effort of considering or even generating all legal moves from a position.\n\nIn practical implementation, game playing programs frequently keep track of two killer moves for each depth of the game tree (greater than depth of 1) and see if either of these moves, if legal, produces a cutoff before the program generates and considers the rest of the possible moves.  If a non-killer move produces a cutoff, it replaces one of the two killer moves at its depth.  This idea can be generalized into a set of [[refutation table]]s.\n\nA generalization of the killer heuristic is the ''history heuristic''. The history heuristic can be implemented as a table that is indexed by some characteristic of the move, for example \"from\" and \"to\" squares or piece moving and the \"to\" square. When there is a cutoff, the appropriate entry in the table is incremented, such as by adding ''d²'' or ''2<sup>d</sup>'' where ''d'' is the current search depth. This information is used when ordering moves.\n\n== References ==\n{{reflist}}\n\n== See also ==\n\n* [[NegaScout|Negascout]]\n\n== External links ==\n* [https://dke.maastrichtuniversity.nl/m.winands/documents/informed_search.pdf Informed Search in Complex Games by Mark Winands]\n\n[[Category:Game artificial intelligence]]\n[[Category:Heuristics]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Lawler's algorithm",
      "url": "https://en.wikipedia.org/wiki/Lawler%27s_algorithm",
      "text": "{{Orphan|date=May 2017}}\n\n'''Lawler’s algorithm''' is a powerful technique for solving a variety of constrained scheduling problems.<ref>Steven Nahmias. Production and Operations Analysis. 2008. {{ISBN|978-0-07-126370-2}}</ref> The [[algorithm]] handles any precedence constraints. It schedules a set of simultaneously arriving tasks on one processor with precedence constraints to minimize maximum tardiness or lateness. Precedence constraints occur when certain jobs must be completed before other jobs can be started.\n\n==Objective Functions==\nThe [[objective function]] is assumed to be in the form <math>min \\, max_{0\\le i \\le n} \\, g_i(F_i)</math>, where <math>g_i</math> is any [[nondecreasing function]] and <math>F_i</math> is the flow time.<ref>Joseph Y-T. Leung. Handbook of scheduling: algorithms, models, and performance analysis. 2004. {{ISBN|978-1-58488-397-5}}</ref> When <math>g_i (F_i) = F_i - d_i = L_i</math>, the objective function corresponds to minimizing the maximum lateness, where <math>d_i</math> is due time for job <math>i</math> and <math>L_i</math> lateness of job <math>i</math>. Another expression is <math>g_i (F_i) = max {(F_i-d_i,0)}</math>, which corresponds to minimizing the maximum tardiness.\n\n==Algorithm==\n\nThe algorithm works by planning the job with the least impact as late as possible. Starting at <math>t = \\sum p_j</math>.\n <math>S</math> set of already scheduled jobs (at start: S = <math>\\emptyset</math>)\n <math>J</math> set of jobs which successors have been scheduled (at start: all jobs without successors)\n <math>t</math> time when the next job will be completed (at start: <math>t = \\sum p_j</math>)\n '''while'''<math>J \\neq \\emptyset</math>:\n     select <math>j \\in J</math> such that <math>f_j(t) = min_{k \\in J}f_k(t)</math>\n     schedule <math>j</math> such that it completes at time <math>t</math>\n     add <math>j</math> to <math>S</math>, delete <math>j</math> from <math>J</math> and update <math>J</math>.\n     <math>t = t - p_j</math>\n '''end while'''\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*Michael Pinedo. Scheduling: theory, algorithms, and systems. 2008. {{ISBN|978-0-387-78934-7}}\n*Conway, Maxwell, Miller. Theory of Scheduling. 1967. {{ISBN|0-486-42817-6}}\n\n[[Category:Production planning]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Industrial engineering]]\n[[Category:Mathematical optimization in business]]"
    },
    {
      "title": "Least squares",
      "url": "https://en.wikipedia.org/wiki/Least_squares",
      "text": "{{short description|Method in statistics}}\n{{distinguish-redirect|Least squares approximation|Least-squares function approximation}}\n{{Regression bar}}\n[[File:Linear least squares2.png|right|thumb|The result of fitting a set of data points with a quadratic function]]\n[[File:X33-ellips-1.svg|thumb|Conic fitting a set of points using least-squares approximation]]\nThe method of '''least squares''' is a standard approach in [[regression analysis]] to approximate the solution of [[overdetermined system]]s, i.e., sets of equations in which there are more equations than unknowns. \"Least squares\" means that the overall solution minimizes the sum of the squares of the residuals made in the results of every single equation.\n\nThe most important application is in [[curve fitting|data fitting]].  The best fit in the least-squares sense minimizes ''the sum of squared [[errors and residuals in statistics|residuals]]'' (a residual being: the difference between an observed value, and the fitted value provided by a model). When the problem has substantial uncertainties in the [[independent variable]] (the ''x'' variable), then simple regression and least-squares methods have problems; in such cases, the methodology required for fitting [[errors-in-variables models]] may be considered instead of that for least squares.\n\nLeast-squares problems fall into two categories: linear or [[ordinary least squares]] and [[nonlinear least squares]], depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical [[regression analysis]]; it has a [[closed-form solution]]. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\n\n[[Polynomial least squares]] describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve.\n\nWhen the observations come from an [[exponential family]] and mild conditions are satisfied, least-squares estimates and [[Maximum likelihood|maximum-likelihood]] estimates are identical.<ref>{{Cite journal | last1 = Charnes | first1 = A. | last2 = Frome | first2 = E. L. | last3 = Yu | first3 = P. L. | doi = 10.1080/01621459.1976.10481508 | title = The Equivalence of Generalized Least Squares and Maximum Likelihood Estimates in the Exponential Family | journal = Journal of the American Statistical Association | volume = 71 | issue = 353 | pages = 169–171 | year = 1976 | pmid =  | pmc = }}</ref> The method of least squares can also be derived as a [[method of moments (statistics)|method of moments]] estimator.\n\nThe following discussion is mostly presented in terms of [[linear]] functions but the use of least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the [[Fisher information]]), the least-squares method may be used to fit a [[generalized linear model]].\n\nThe least-squares method is usually credited to [[Carl Friedrich Gauss]] (1795),<ref name=brertscher>{{cite book |last=Bretscher |first=Otto |title=Linear Algebra With Applications |edition=3rd |publisher=Prentice Hall |year=1995 |location=Upper Saddle River, NJ}}</ref> but it was first published by [[Adrien-Marie Legendre]] (1805).<ref>{{cite journal |first=Stephen M. |last=Stigler |year=1981 |title=Gauss and the Invention of Least Squares |url=http://projecteuclid.org/euclid.aos/1176345451 |journal=Ann. Stat. |publisher= |volume=9 |issue=3 |pages=465–474 |doi=10.1214/aos/1176345451 |pmc= |pmid= }}</ref>\n\n==History==\n\n===Context===\nThe method of least squares grew out of the fields of [[astronomy]] and [[geodesy]], as scientists and mathematicians sought to provide solutions to the challenges of navigating the Earth's oceans during the [[Age of Exploration]].  The accurate description of the behavior of celestial bodies was the key to enabling ships to sail in open seas, where sailors could no longer rely on land sightings for navigation.\n\nThe method was the culmination of several advances that took place during the course of the eighteenth century:<ref name=stigler>{{cite book\n  |last=Stigler |first=Stephen M.\n  | title = The History of Statistics: The Measurement of Uncertainty Before 1900\n  | publisher = Belknap Press of Harvard University Press\n  | year = 1986\n  | location = Cambridge, MA\n  | isbn = 978-0-674-40340-6\n  }}</ref>\n\n*The combination of different observations as being the best estimate of the true value; errors decrease with aggregation rather than increase, perhaps first expressed by [[Roger Cotes]] in 1722.\n*The combination of different observations taken under the ''same'' conditions contrary to simply trying one's best to observe and record a single observation accurately. The approach was known as the method of averages. This approach was notably used by [[Tobias Mayer]] while studying the [[libration]]s of the moon in 1750, and by [[Pierre-Simon Laplace]] in his work in explaining the differences in motion of [[Jupiter]] and [[Saturn]] in 1788.\n*The combination of different observations taken under ''different'' conditions. The method came to be known as the method of least absolute deviation. It was notably performed by [[Roger Joseph Boscovich]] in his work on the shape of the earth in 1757 and by [[Pierre-Simon Laplace]] for the same problem in 1799.\n*The development of a criterion that can be evaluated to determine when the solution with the minimum error has been achieved. Laplace tried to specify a mathematical form of the [[probability]] density for the errors and define a method of estimation that minimizes the error of estimation. For this purpose, Laplace used a symmetric two-sided exponential distribution we now call [[Laplace distribution]] to model the error distribution, and used the sum of absolute deviation as error of estimation. He felt these to be the simplest assumptions he could make, and he had hoped to obtain the arithmetic mean as the best estimate. Instead, his estimator was the posterior median.\n\n===The method===\n[[File:Bendixen - Carl Friedrich Gauß, 1828.jpg|thumb|upright=0.8|[[Carl Friedrich Gauss]]]]\n\nThe first clear and concise exposition of the method of least squares was published by [[Adrien-Marie Legendre|Legendre]] in 1805.<ref>{{Citation |first=Adrien-Marie |last=Legendre |title=Nouvelles méthodes pour la détermination des orbites des comètes |trans-title=New Methods for the Determination of the Orbits of Comets |language=French |publisher=F. Didot |location=Paris |year=1805 |url=https://books.google.com/books/about/Nouvelles_m%C3%A9thodes_pour_la_d%C3%A9terminati.html?id=FRcOAAAAQAAJ }}</ref> The technique is described as an algebraic procedure for fitting linear equations to data and Legendre demonstrates the new method by analyzing the same data as Laplace for the shape of the earth. The value of Legendre's method of least squares was immediately recognized by leading astronomers and geodesists of the time.\n\nIn 1809 [[Carl Friedrich Gauss]] published his method of calculating the orbits of celestial bodies. In that work he claimed to have been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the [[normal distribution]]. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a method of estimation that minimizes the error of estimation. Gauss showed that the [[arithmetic mean]] is indeed the best estimate of the location parameter by changing both the [[probability density]] and the method of estimation. He then turned the problem around by asking what form the density should have and what method of estimation should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.\n\nAn early demonstration of the strength of [[Gauss's method]] came when it was used to predict the future location of the newly discovered asteroid [[Ceres (dwarf planet)|Ceres]]. On 1 January 1801, the Italian astronomer [[Giuseppe Piazzi]] discovered Ceres and was able to track its path for 40 days before it was lost in the glare of the sun. Based on these data, astronomers desired to determine the location of Ceres after it emerged from behind the sun without solving [[Kepler's laws of planetary motion|Kepler's complicated nonlinear equations]] of planetary motion. The only predictions that successfully allowed Hungarian astronomer [[Franz Xaver von Zach]] to relocate Ceres were those performed by the 24-year-old Gauss using least-squares analysis.\n\nIn 1810, after reading Gauss's work, Laplace, after proving the [[central limit theorem]], used it to give a large sample justification for the method of least squares and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal in the sense that in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the [[Gauss–Markov theorem]].\n\nThe idea of least-squares analysis was also independently formulated by the American [[Robert Adrain]] in 1808. In the next two centuries workers in the theory of errors and in statistics found many different ways of implementing least squares.<ref>{{cite journal |doi=10.1111/j.1751-5823.1998.tb00406.x |first=J. |last=Aldrich |year=1998|title=Doing Least Squares: Perspectives from Gauss and Yule |journal=International Statistical Review |volume=66 |issue=1 |pages= 61–81}}</ref>\n\n==Problem statement==\n{{Unreferenced section|date=February 2012}}\nThe objective consists of adjusting the parameters of a model function to best fit a data set. A simple data set consists of ''n'' points (data pairs) <math>(x_i,y_i)\\!</math>, ''i'' = 1, ..., ''n'', where <math>x_i\\!</math> is an [[independent variable]] and <math>y_i\\!</math> is a [[dependent variable]] whose value is found by observation. The model function has the form <math>f(x,\\beta)</math>, where ''m'' adjustable parameters are held in the vector <math>\\boldsymbol \\beta</math>. The goal is to find the parameter values for the model that \"best\" fits the data. The fit of a model to a data point is measured by its [[errors and residuals in statistics|residual]], defined as the difference between the actual value of the dependent variable and the value predicted by the model: \n:<math>r_i=y_i-f(x_i,\\boldsymbol \\beta).</math>\nThe least-squares method finds the optimal parameter values by  minimizing the sum, <math>S</math>, of squared residuals:\n:<math>S=\\sum_{i=1}^{n}{r_i}^2.</math>\n\nAn example of a model in two dimensions is that of the straight line. Denoting the y-intercept as <math>\\beta_0</math> and the slope as <math>\\beta_1</math>, the model function is given by <math>f(x,\\boldsymbol \\beta)=\\beta_0+\\beta_1 x</math>. See [[Linear least squares (mathematics)#Example|linear least squares]] for a fully worked out example of this model.\n\nA data point may consist of more than one independent variable. For example, when fitting a plane to a set of height measurements, the plane is a function of two independent variables, ''x'' and ''z'', say. In the most general case there may be one or more independent variables and one or more dependent variables at each data point. <!-- Also, the residuals may be weighted to take into account differences in the reliability of the measurements.\nmath> S = \\sum_{i=1}^n w_ir_i^2 </math>.\nThis is called '''weighted least squares,''' in contrast to '''ordinary least squares''' in which unit weights are used. -->\n\n==Limitations==\nThis regression formulation considers only observational errors in the dependent variable (but the alternative [[total least squares]] regression can account for errors in both variables). There are two rather different contexts with  different implications:\n\n*Regression for prediction. Here a model is fitted to provide a prediction rule for application in a similar situation to which the data used for fitting apply. Here the dependent variables corresponding to such future application would be subject to the same types of observation error as those in the data used for fitting. It is therefore logically consistent to use the least-squares prediction rule for such data.\n*Regression for fitting a \"true relationship\". In standard [[regression analysis]] that leads to fitting by least squares there is an implicit assumption that errors in the [[independent variable]] are zero or strictly controlled so as to be negligible. When errors in the [[independent variable]] are non-negligible, [[Errors-in-variables models|models of measurement error]] can be used; such methods can lead to [[parameter estimation|parameter estimates]], [[hypothesis testing]] and [[confidence interval]]s that take into account the presence of observation errors in the independent variables.<ref>For a good introduction to error-in-variables, please see {{cite book |last=Fuller |first=W. A. |authorlink=Wayne Fuller |title=Measurement Error Models |location= |publisher=John Wiley & Sons |year=1987 |isbn=978-0-471-86187-4 }}</ref> An alternative approach is to fit a model by [[total least squares]]; this can be viewed as taking a pragmatic approach to balancing the effects of the different sources of error in formulating an objective function for use in model-fitting.\n\n==Solving the least squares problem==\n{{Unreferenced section|date=February 2012}}\nThe [[Maxima and minima|minimum]] of the sum of squares is found by setting the [[gradient]] to zero. Since the model contains ''m'' parameters, there are ''m'' gradient equations:\n\n:<math>\\frac{\\partial S}{\\partial \\beta_j}=2\\sum_i r_i\\frac{\\partial r_i}{\\partial \\beta_j} = 0,\\ j=1,\\ldots,m,</math>\n\nand since <math>r_i=y_i-f(x_i,\\boldsymbol \\beta)</math>, the gradient equations become\n\n:<math>-2\\sum_i r_i\\frac{\\partial f(x_i,\\boldsymbol \\beta)}{\\partial \\beta_j}=0,\\ j=1,\\ldots,m.</math>\n\nThe gradient equations apply to all least squares problems. Each particular problem requires particular expressions for the model and its partial derivatives.\n\n===Linear least squares===\n{{main|Linear_least_squares_(mathematics)|l1=Linear least squares}}\n\nA regression model is a linear one when the model comprises a [[linear combination]] of the parameters, i.e.,\n\n:<math> f(x, \\beta) = \\sum_{j = 1}^m \\beta_j \\phi_j(x),</math>\n\nwhere the function <math>\\phi_j</math> is a function of <math> x </math>.\n\nLetting\n\n:<math> X_{ij}= \\phi_j(x_{i}),</math>\n\nwe can then see that in that case the least square estimate (or estimator, in the context of a random sample), <math> \\boldsymbol \\beta</math> is given by\n\n:<math> \\boldsymbol{\\hat\\beta} =( X ^TX)^{-1}X^T \\boldsymbol y.</math>\n\nFor a derivation of this estimate see [[Linear least squares (mathematics)]].\n\n===Non-linear least squares===\n{{main|Non-linear least squares}}\n\nThere is, in some cases, a [[closed-form solution]] to a non-linear least squares problem – but in general there is not. In  the case of no closed-form solution, numerical algorithms are used to find the value of the parameters <math>\\beta</math> that minimizes the objective.  Most algorithms involve choosing initial values for the parameters.  Then, the parameters are refined iteratively, that is, the values are obtained by successive approximation:\n\n:<math>{\\beta_j}^{k+1}={\\beta_j}^k+\\Delta \\beta_j,</math>\n\nwhere a superscript ''k'' is an iteration number, and the vector of increments <math>\\Delta \\beta_j</math> is called the shift vector.  In some commonly used algorithms, at each iteration the model may be linearized by approximation to a first-order [[Taylor series]] expansion about <math> \\boldsymbol \\beta^k</math>:\n\n:<math>\n\\begin{align}\nf(x_i,\\boldsymbol \\beta) &= f^k(x_i,\\boldsymbol \\beta) +\\sum_j \\frac{\\partial f(x_i,\\boldsymbol \\beta)}{\\partial \\beta_j} \\left(\\beta_j-{\\beta_j}^k \\right) \\\\\n&= f^k(x_i,\\boldsymbol \\beta) +\\sum_j J_{ij} \\,\\Delta\\beta_j.\n\\end{align}\n</math>\n\nThe [[Jacobian matrix and determinant|Jacobian]] '''J''' is a function of constants, the independent variable ''and'' the parameters, so it changes from one iteration to the next. The residuals are given by\n\n:<math>r_i=y_i- f^k(x_i,\\boldsymbol \\beta)- \\sum_{k=1}^{m} J_{ik}\\,\\Delta\\beta_k=\\Delta y_i- \\sum_{j=1}^{m} J_{ij}\\,\\Delta\\beta_j.</math>\n\nTo minimize the sum of squares of <math>r_i</math>, the gradient equation is set to zero and solved for <math> \\Delta \\beta_j</math>:\n\n:<math>-2\\sum_{i=1}^n J_{ij} \\left( \\Delta y_i-\\sum_{k=1}^m J_{ik} \\, \\Delta \\beta_k \\right) = 0,</math>\n\nwhich, on rearrangement, become ''m'' simultaneous linear equations, the '''normal equations''':\n\n: <math>\\sum_{i=1}^{n}\\sum_{k=1}^m J_{ij} J_{ik} \\, \\Delta \\beta_k=\\sum_{i=1}^n J_{ij} \\, \\Delta y_i \\qquad (j=1,\\ldots,m).</math>\n\nThe normal equations are written in matrix notation as\n\n:<math>\\mathbf{(J^T J) \\,\\Delta \\boldsymbol \\beta=J^T\\,\\Delta y}.\\,</math>\n<!-- or\n:<math>\\mathbf{\\left(J^TWJ\\right) \\, \\Delta \\boldsymbol \\beta=J^TW \\, \\Delta y}</math>\nif weights are used. -->\n\nThese are the defining equations of the [[Gauss–Newton algorithm]].\n\n===Differences between linear and nonlinear least squares===\n* The model function, ''f'', in LLSQ (linear least squares) is a linear combination of parameters of the form <math>f = X_{i1}\\beta_1 + X_{i2}\\beta_2 +\\cdots</math> The model may represent a straight line, a parabola or any other linear combination of functions. In NLLSQ  (nonlinear least squares) the parameters appear as functions, such as <math>\\beta^2, e^{\\beta x}</math> and so forth. If the derivatives <math>\\partial f /\\partial \\beta_j</math> are either constant or depend only on the values of the independent variable, the model is linear in the parameters. Otherwise the model is nonlinear.\n*Algorithms for finding the solution to a NLLSQ problem require initial values for the parameters, LLSQ does not.\n*Like LLSQ, solution algorithms for NLLSQ often require that the Jacobian can be calculated. Analytical expressions for the partial derivatives can be complicated. If analytical expressions are impossible to obtain either the partial derivatives must be calculated by numerical approximation or an estimate must be made of the Jacobian.\n*In NLLSQ non-convergence (failure of the algorithm to find a minimum) is a common phenomenon whereas the LLSQ is globally concave so non-convergence is not an issue.\n*NLLSQ is usually an iterative process. The iterative process has to be terminated when a convergence criterion is satisfied. LLSQ solutions can be computed using direct methods, although problems with large numbers of parameters are typically solved with iterative methods, such as the [[Gauss–Seidel]] method.\n*In LLSQ the solution is unique, but in NLLSQ there may be multiple minima in the sum of squares.\n*Under the condition that the errors are uncorrelated with the predictor variables, LLSQ yields unbiased estimates, but even under that condition NLLSQ estimates are generally biased.\nThese differences must be considered whenever the solution to a nonlinear least squares problem is being sought.\n\n==Regression analysis and statistics==\n{{Unreferenced section|date=February 2012}}\n\nThe method of least squares is often used to generate estimators and other statistics in regression analysis.\n\nConsider a simple example drawn from physics. A spring should obey [[Hooke's law]] which states that the extension of a spring {{mvar|y}} is proportional to the force, ''F'', applied to it.\n\n:<math>y = f(F,k)=kF\\!</math>\n\nconstitutes the model, where ''F'' is the independent variable. To estimate the [[force constant]], ''k'', a series of ''n'' measurements with different forces will produce a set of data, <math>(F_i, y_i),\\ i=1,\\dots,n\\!</math>, where ''y<sub>i</sub>'' is a measured spring extension. Each experimental observation will contain some error.  If we denote this error <math>\\varepsilon</math>, we may specify an empirical model for our observations,\n\n: <math> y_i = kF_i + \\varepsilon_i. \\, </math>\n\nThere are many methods we might use to estimate the unknown parameter ''k''. Noting that the ''n'' equations in  the ''m'' variables in our data comprise an [[overdetermined system]] with one unknown and ''n'' equations, we may choose to estimate ''k'' using least squares.  The sum of squares to be minimized is\n\n:<math> S = \\sum_{i=1}^n (y_i - kF_i)^2. </math>\n\nThe least squares estimate of the force constant, ''k'', is given by\n\n:<math>\\hat k=\\frac{\\sum_i F_i y_i}{\\sum_i F_i^2}.</math>\n\nHere it is assumed that application of the force '''''causes''''' the spring to expand and, having derived the force constant by least squares fitting, the extension can be predicted from Hooke's law.\n\nIn regression analysis the researcher specifies an empirical model. For example, a very common model is the straight line model which is used to test if there is a linear relationship between dependent and independent variable. If a linear relationship is found to exist, the variables are said to be [[correlated]]. However, [[Correlation does not imply causation|correlation does not prove causation]], as both variables may be correlated with other, hidden, variables, or the dependent variable may \"reverse\" cause the independent variables, or the variables may be otherwise spuriously correlated.  For example, suppose there is a correlation between deaths by drowning and the volume of ice cream sales at a particular beach. Yet, both the number of people going swimming and the volume of ice cream sales increase as the weather gets hotter, and presumably the number of deaths by drowning is correlated with the number of people going swimming. Perhaps an increase in swimmers causes both the other variables to increase.\n\nIn order to make statistical tests on the results it is necessary to make assumptions about the nature of the experimental errors.  A common (but not necessary) assumption is that the errors belong to a normal distribution. The [[central limit theorem]] supports the idea that this is a good approximation in many cases.\n* The [[Gauss–Markov theorem]]. In a linear model in which the errors have [[expected value|expectation]] zero conditional on the independent variables, are [[uncorrelated]] and have equal [[variance]]s, the best linear [[unbiased]] estimator of any linear combination of the observations, is its least-squares estimator. \"Best\" means that the least squares estimators of the parameters have minimum variance. The assumption of equal variance is valid when the errors all belong to the same distribution.\n*In a linear model, if the errors belong to a normal distribution the least squares estimators are also the [[maximum likelihood estimator]]s.\n\nHowever, if the errors are not normally distributed, a [[central limit theorem]] often nonetheless implies that the parameter estimates will be approximately normally distributed so long as the sample is reasonably large.  For this reason, given the important property that the error mean is independent of the independent variables, the distribution of the error term is not an important issue in regression analysis.  Specifically, it is not typically important whether the error term follows a normal distribution.\n\nIn a least squares calculation with unit weights, or in linear regression, the variance on the ''j''th parameter,\ndenoted <math>\\operatorname{var}(\\hat{\\beta}_j)</math>, is usually estimated with\n\n: <math>\\operatorname{var}(\\hat{\\beta}_j)= \\sigma^2( [X^TX]^{-1})_{jj} \\approx \\frac S {n-m} ([X^TX]^{-1})_{jj},</math>\n\nwhere the true error variance ''σ''<sup>2</sup> is replaced by an estimate based on the minimised value of the sum of squares objective function ''S''. The denominator, ''n''&nbsp;−&nbsp;''m'', is the [[Degrees of freedom (statistics)|statistical degrees of freedom]]; see [[Degrees of freedom (statistics)#Effective degrees of freedom|effective degrees of freedom]] for generalizations.\n\n[[Confidence limits]] can be found if the [[probability distribution]] of the parameters is known, or an asymptotic approximation is made, or assumed. Likewise statistical tests on the residuals can be made if the probability distribution of the residuals is known or assumed. The probability distribution of any linear combination of the dependent variables can be derived if the probability distribution of experimental errors is known or assumed. Inference is particularly straightforward if the errors are assumed to follow a normal distribution, which implies that the parameter estimates and residuals will also be normally distributed conditional on the values of the independent variables.\n\n==Weighted least squares==\n{{main|Weighted least squares}}\n\nA special case of [[generalized least squares]] called '''weighted least squares''' occurs when all the off-diagonal entries of ''Ω'' (the correlation matrix of the residuals) are null; the [[variance]]s of the observations (along the covariance matrix diagonal) may still be unequal ([[heteroscedasticity]]).\n\n==Relationship to principal components==\nThe first [[Principal component analysis|principal component]] about the mean of a set of points can be represented by that line which most closely approaches the data points (as measured by squared distance of closest approach, i.e. perpendicular to the line).  In contrast, linear least squares tries to minimize the distance in the <math>y</math> direction only.  Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally.\n\n==Regularization==\n{{technical|section|date=February 2016}}\n{{Main|Regularized least squares}}\n\n===Tikhonov regularization===\n{{Main|Tikhonov regularization}}\nIn some contexts a [[Regularization (machine learning)|regularized]] version of the least squares solution may be preferable. [[Tikhonov regularization]] (or [[ridge regression]]) adds a constraint that <math>\\|\\beta\\|^2</math>, the [[L2-norm|L<sub>2</sub>-norm]] of the parameter vector, is not greater than a given value. Equivalently, it may solve an unconstrained minimization of the least-squares penalty with <math>\\alpha\\|\\beta\\|^2</math> added, where <math>\\alpha</math> is a constant (this is the [[Lagrange multipliers|Lagrangian]] form of the constrained problem).  In a [[Bayesian statistics|Bayesian]] context, this is equivalent to placing a zero-mean normally distributed [[prior distribution|prior]] on the parameter vector.\n\n===Lasso method===\nAn alternative [[Regularization (machine learning)|regularized]] version of least squares is ''Lasso'' (least absolute shrinkage and selection operator), which uses the constraint that <math>\\|\\beta\\|</math>, the [[L1-norm|L<sub>1</sub>-norm]] of the parameter vector, is no greater than a given value.<ref name=tibsh>{{cite journal |last=Tibshirani |first=R. |authorlink = Rob Tibshirani | year=1996 |title=Regression shrinkage and selection via the lasso |journal=Journal of the Royal Statistical Society, Series B |volume=58|issue=1 |pages=267–288 |jstor=2346178}}</ref><ref name=\"ElementsStatLearn\">{{cite book |url=http://www-stat.stanford.edu/~tibs/ElemStatLearn/ |title=The Elements of Statistical Learning |last1=Hastie |first1=Trevor |last2=Tibshirani |first2=Robert |last3=Friedman |first3=Jerome H. |authorlink1=Trevor Hastie |authorlink3=Jerome H. Friedman |edition=second |date=2009 |publisher=Springer-Verlag |isbn=978-0-387-84858-7 |deadurl=yes |archiveurl=https://web.archive.org/web/20091110212529/http://www-stat.stanford.edu/~tibs/ElemStatLearn/ |archivedate=2009-11-10 |df= }}</ref><ref>{{cite book|last1=Bühlmann|first1=Peter|last2=van de Geer|first2=Sara|author2-link= Sara van de Geer |title=Statistics for High-Dimensional Data: Methods, Theory and Applications|date=2011|publisher=Springer|isbn=9783642201929}}</ref> (As above, this is equivalent to an unconstrained minimization of the least-squares penalty with <math>\\alpha\\|\\beta\\|</math> added.) In a [[Bayesian statistics|Bayesian]] context, this is equivalent to placing a zero-mean [[Laplace distribution|Laplace]] [[prior distribution]] on the parameter vector.<ref>{{cite journal|last1=Park|first1=Trevor|last2=Casella|first2=George|author2-link=George Casella| title=The Bayesian Lasso|journal=Journal of the American Statistical Association|date=2008|volume=103|issue=482|pages=681–686|doi=10.1198/016214508000000337}}</ref> The optimization problem may be solved using [[quadratic programming]] or more general [[convex optimization]] methods, as well as by specific algorithms such as the [[least angle regression]] algorithm.\n\nOne of the prime differences between Lasso and ridge regression is that in ridge regression, as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause more and more of the parameters to be driven to zero. This is an advantage of Lasso over ridge regression, as driving parameters to zero deselects the features from the regression. Thus, Lasso automatically selects more relevant features and discards the others, whereas Ridge regression never fully discards any features. Some feature selection techniques are developed based on the LASSO including Bolasso which bootstraps samples,<ref name=Bolasso>{{cite journal|last1=Bach|first1=Francis R|title=Bolasso: model consistent lasso estimation through the bootstrap|journal=Proceedings of the 25th International Conference on Machine Learning|date=2008|pages=33–40|doi=10.1145/1390156.1390161|url=http://dl.acm.org/citation.cfm?id=1390161|isbn=9781605582054}}</ref>  and FeaLect which analyzes the regression coefficients corresponding to different values of <math>\\alpha</math> to score all the features.<ref name=FeaLect>{{cite journal|last1=Zare|first1=Habil|title=Scoring relevancy of features based on combinatorial analysis of Lasso with application to lymphoma diagnosis|journal=BMC Genomics|date=2013|volume=14|pages=S14|doi=10.1186/1471-2164-14-S1-S14|url=http://www.biomedcentral.com/1471-2164/14/S1/S14|pmid=23369194|pmc=3549810}}</ref>\n\nThe L<sup>1</sup>-regularized formulation is useful in some contexts due to its tendency to prefer solutions where more parameters are zero, which gives solutions that depend on fewer variables.<ref name=tibsh/> For this reason, the Lasso and its variants are fundamental to the field of [[compressed sensing]]. An extension of this approach is [[elastic net regularization]].\n\n==See also==\n{{Div col|colwidth=25em}}\n* [[Adjustment of observations]]\n* [[Minimum mean square error|Bayesian MMSE estimator]]\n* [[Gauss–Markov theorem|Best linear unbiased estimator]] (BLUE)\n* [[Best linear unbiased prediction]] (BLUP)\n* [[Gauss–Markov theorem]]\n* [[L2 norm|''L''<sub>2</sub> norm]]\n* [[Least absolute deviation]]\n* [[Measurement uncertainty]]\n* [[Orthogonal projection]]\n* [[Proximal gradient methods for learning]]\n* [[Quadratic loss function]]\n* [[Root mean square]]\n* [[Squared deviations]]\n{{div col end}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n{{more footnotes|date=June 2014}}\n* {{cite book |first=Å. |last=Björck |isbn=978-0-89871-360-2 |title=Numerical Methods for Least Squares Problems |publisher=SIAM |year=1996 |url= }}\n* {{cite book |first1=T. |last1=Kariya |first2=H. |last2=Kurata |title=Generalized Least Squares |location=Hoboken |publisher=Wiley |year=2004 |isbn=978-0-470-86697-9 }}\n* {{cite book |first=D. G. |last=Luenberger |authorlink=David Luenberger |chapter=Least-Squares Estimation |title=Optimization by Vector Space Methods |location=New York |publisher=John Wiley & Sons |year=1997 |origyear=1969 |isbn=978-0-471-18117-0 |pages=78–102 |chapterurl=https://books.google.com/books?id=lZU0CAH4RccC&pg=PA78 }}\n* {{cite book |first1=C. R. |last1=Rao |authorlink=C. R. Rao |first2=H. |last2=Toutenburg |authorlink2=Helge Toutenburg |title=Linear Models: Least Squares and Alternatives |series=Springer Series in Statistics |location=Berlin |publisher=Springer |edition=3rd |year=2008 |isbn=978-3-540-74226-5 |url=https://books.google.com/books?id=3LK9JoGEyN4C |display-authors=etal}}\n* {{cite book |first=J. |last=Wolberg |title=Data Analysis Using the Method of Least Squares: Extracting the Most Information from Experiments |location=Berlin |publisher=Springer |year=2005|isbn=978-3-540-25674-8}}\n\n==External links==\n*{{Commonscat-inline}}\n\n{{Least Squares and Regression Analysis|state=expanded}}\n{{Statistics|correlation|state=collapsed}}\n{{Authority control}}\n\n{{DEFAULTSORT:Least Squares}}\n[[Category:Least squares| ]]\n[[Category:Single-equation methods (econometrics)]]\n[[Category:Optimization algorithms and methods]]\n\n[[gl:Mínimos cadrados lineais]]\n[[vi:Bình phương tối thiểu tuyến tính]]"
    },
    {
      "title": "Lemke's algorithm",
      "url": "https://en.wikipedia.org/wiki/Lemke%27s_algorithm",
      "text": "In [[mathematical optimization]], '''Lemke's algorithm''' is a [[algorithm|procedure]] for solving [[linear complementarity problem]]s, and more generally [[mixed linear complementarity problem]]s. It is named after [[Carlton E. Lemke]].\n\nLemke's algorithm is of [[pivot element|pivoting]] or [[matroid|basis]]-[[exchange algorithm|exchange]] type. Similar algorithms can compute [[Nash equilibrium#Nash equilibria in a payoff matrix|Nash equilibria]] for [[Normal-form game|two-person matrix and bimatrix game]]s.\n\n== References ==\n* {{cite book|last1=Cottle|first1=Richard W.|last2=Pang|first2=Jong-Shi|last3=Stone|first3=Richard E.|title=The linear complementarity problem | series=Computer Science and Scientific Computing|publisher=Academic Press, Inc.|location=Boston, MA|year=1992|pages=xxiv+762 pp.|isbn=0-12-192350-9 |MR=1150683}}\n* {{cite book|last=Murty|first=K. G.|title=Linear complementarity, linear and nonlinear programming|series=Sigma Series in Applied Mathematics|volume=3|publisher=Heldermann Verlag|location=Berlin|year=1988|pages=xlviii+629 pp.|isbn=3-88538-403-5|url=http://ioe.engin.umich.edu/people/fac/books/murty/linear_complementarity_webbook/|deadurl=yes|archiveurl=https://web.archive.org/web/20100401043940/http://ioe.engin.umich.edu/people/fac/books/murty/linear_complementarity_webbook/|archivedate=2010-04-01|df=}} (Available for download at the website of Professor [http://www-personal.umich.edu/~murty/ Katta G. Murty].) {{MR|949214}}\n\n==External links==\n* [http://www.omatrix.com/manual/lemke.htm OMatrix manual on Lemke]\n* [http://chrishecker.com/The_Mixed_Linear_Complementarity_Problem Chris Hecker's GDC presentation on MLCPs and Lemke]\n* [http://www.math.ubc.ca/~jf/courses/old_kkt.pdf Linear Complementarity and Mathematical (Non-linear) Programming]\n* [[Siconos]]/Numerics open-source GPL  implementation in C of Lemke's algorithm and other methods to solve LCPs and MLCPs\n{{Mathematical programming}}\n{{optimization algorithms|convex}}\n\n[[Category:Optimization algorithms and methods]]\n\n{{algorithm-stub}}"
    },
    {
      "title": "Level-set method",
      "url": "https://en.wikipedia.org/wiki/Level-set_method",
      "text": "[[File:Levelset-mean-curvature-spiral.ogv|thumb|Video of spiral being propagated by level sets ([[mean curvature flow]]) in 2D. LHS shows zero-level solution. RHS shows the level-set scalar field.]]\n\n'''Level-set methods''' ('''LSM''') are a conceptual framework for using [[level set]]s as a tool for [[numerical analysis]] of [[Surface (topology)|surface]]s and [[shape]]s. The advantage of the level-set model is that one can perform numerical computations involving [[curve]]s and [[surface (topology)|surface]]s on a fixed [[Cartesian grid]] without having to [[Parametric surface|parameterize]] these objects (this is called the ''Eulerian approach'').<ref>{{Citation \n | last = Osher | first = S. | last2 = Sethian | first2 = J. A.\n | title = Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton–Jacobi formulations\n | journal = J. Comput. Phys.\n | volume = 79 | issue = 1 | year = 1988 | pages = 12&ndash;49 \n | url = http://math.berkeley.edu/~sethian/Papers/sethian.osher.88.pdf | doi=10.1016/0021-9991(88)90002-2\n|bibcode = 1988JCoPh..79...12O | citeseerx = 10.1.1.46.1266 }}</ref> Also, the level-set method makes it very easy to follow shapes  that change [[topology]], for example, when a shape splits in two, develops holes, or the reverse of these operations.  All these make the level-set method a great tool for modeling time-varying objects, like inflation of an [[airbag]], or a drop of oil floating in water.\n\n[[Image:level set method.png|thumb|right|400px|An illustration of the level-set method]]\n\nThe figure on the right illustrates several important ideas about the level-set method. In the upper-left corner we see a shape; that is, a bounded region with a well-behaved boundary. Below it, the red surface is the graph of a level set function <math>\\varphi</math> determining this shape, and the flat blue region represents the ''xy'' plane. The boundary of the shape is then the zero-level set of <math>\\varphi</math>, while the shape itself is the set of points in the plane for which <math>\\varphi</math> is positive (interior of the shape) or zero (at the boundary).\n\nIn the top row we see the shape changing its topology by splitting in two. It would be quite hard to describe this transformation numerically by parameterizing the boundary of the shape and following its evolution. One would need an algorithm able to detect the moment the shape splits in two, and then construct parameterizations for the two newly obtained curves.  On the other hand, if we look at the bottom row, we see that the level set function merely translated downward. This is an example of when it can be much easier to work with a shape through its level-set function than with the shape directly, where using the shape directly would need to consider and handle all the possible deformations the shape might undergo.\n\nThus, in two dimensions, the level-set method amounts to representing a [[closed curve]] <math>\\Gamma</math> (such as the shape boundary in our example) using an auxiliary function <math>\\varphi</math>, called the level-set function. <math>\\Gamma</math> is represented as the zero-[[level set]] of <math>\\varphi</math> by\n:<math>\\Gamma = \\{(x, y) \\mid \\varphi(x, y) = 0 \\},</math>\nand the level-set method manipulates <math>\\Gamma</math> ''implicitly'', through the function <math>\\varphi</math>.  This function <math>\\varphi</math> is assumed to take positive values inside the region delimited by the curve <math>\\Gamma</math> and negative values outside.<ref name=\"osher\" /><ref name=\"sethian\" />\n\n==The level-set equation==\n\nIf the curve <math>\\Gamma</math> moves in the normal direction with a speed <math>v</math>, then the level-set function <math>\\varphi</math> satisfies the ''level-set equation'' \n:<math>\\frac{\\partial\\varphi}{\\partial t} = v|\\nabla \\varphi|.</math>\nHere, <math>|\\cdot|</math> is the [[Euclidean norm]] (denoted customarily by single bars in PDEs), and <math>t</math> is time. This is a [[partial differential equation]], in particular a [[Hamilton–Jacobi equation]], and can be solved numerically, for example, by using [[finite difference]]s on a Cartesian grid.<ref name=osher>{{cite book |last=Osher |first=Stanley J. | authorlink = Stanley Osher |author2=Fedkiw, Ronald P. |authorlink2=Ronald Fedkiw  |title=Level Set Methods and Dynamic Implicit Surfaces|publisher=[[Springer-Verlag]] |year=2002 |isbn= 978-0-387-95482-0}}</ref><ref name=sethian>{{cite book |last=Sethian |first=James A. | authorlink = James Sethian |title= Level Set Methods and Fast Marching Methods : Evolving Interfaces in Computational Geometry, Fluid Mechanics, Computer Vision, and Materials Science|publisher=[[Cambridge University Press]] |year=1999 |isbn= 978-0-521-64557-7}}</ref>\n\nThe numerical solution of the level-set equation, however, requires sophisticated techniques. Simple finite-difference methods fail quickly. [[Upwinding]] methods, such as the [[Godunov's scheme|Godunov method]], fare better; however, the level-set method does not guarantee the conservation of the volume and the shape of the level set in an advection field that does conserve the shape and size, for example, uniform or rotational velocity field. Instead, the shape of the level set may get severely distorted, and the level set may vanish over several time steps. For this reason, high-order finite-difference schemes are generally required, such as high-order [[essentially non-oscillatory]] (ENO) schemes, and even then the feasibility of long-time simulations is questionable. Further sophisticated methods to deal with this difficulty have been developed, e.g., combinations of the level-set method with tracing marker particles advected by the velocity field.<ref>{{Citation \n | last = Enright  | first = D. | last2 = Fedkiw | first2 = R. P.\n | last3 = Ferziger | first3 = J. H. | authorlink3 = Joel H. Ferziger\n | last4 = Mitchell | first4 = I.\n | title = A hybrid particle level set method for improved interface capturing\n | journal = J. Comput. Phys.\n | volume = 183 | issue = 1 | year = 2002 | pages = 83&ndash;116\n | url = http://www.cs.ubc.ca/~mitchell/Papers/myJCP02.pdf | doi=10.1006/jcph.2002.7166\n|bibcode = 2002JCoPh.183...83E | citeseerx = 10.1.1.15.910 }}</ref>\n\n== Example ==\n\nConsider a unit circle in <math>\\mathbf{R}^2</math>, shrinking in on itself at a constant rate, i.e. each point on the boundary of the circle moves along its inwards pointing normal at some fixed speed. The circle will shrink and eventually collapse down to a point. If an initial distance field is constructed (i.e. a function whose value is the signed euclidean distance to the boundary, positive interior, negative exterior) on the initial circle, the normalised gradient of this field will be the circle normal.\n\nIf the field has a constant value subtracted from it in time, the zero level (which was the initial boundary) of the new fields will also be circular and will similarly collapse to a point.  This is due to this being effectively the temporal integration of the [[Eikonal equation]] with a fixed front velocity.\n\nIn [[combustion]], this method is used to describe the instantaneous flame surface, known as the [[G equation]].\n\n==History==\n\nThe level-set method was developed in the 1980s by the American mathematicians [[Stanley Osher]] and [[James Sethian]]. It has become popular in many disciplines, such as [[image processing]], [[computer graphics]], [[computational geometry]], [[optimization (mathematics)|optimization]], and [[computational fluid dynamics]], [[computational biophysics]].\n\nA number of [[level set (data structures)|level-set data structures]] have been developed to facilitate the use of the level-set method in computer applications.\n\n== Applications ==\n\n* Computational fluid dynamics\n* Combustion\n* Trajectory planning\n* Optimization\n* Image processing\n* Computational biophysics\n\n== Computational fluid dynamics ==\nTo run a Math Model in the interface of two different fluids we need to soften the interactions between the fluids. Therefore we need to apply an specific function: Compact Level Set Method.\n\nAs an “spin off”, the CompactLSM its a complement of the LSM, that helps solving LSM equations. It can be used in numerical simulation of flow, for example, if we are working with discretization of the interface water-air, compacts at sixth order, ensures the accurate and fast calculation of the interface equations (Monteiro 2018).\n\nThe LSM uses a distance function to locate different fluids. A distance function is that whose value represents the smallest distance from the point where it is being analyzed to the interface. This distance function is identified by isolines (2D) or isosurfaces (3D), showing that  the negative values refer to one of the fluids, positive values refer to the other and the\n\nzero value corresponds to the position of the interface.\n\nBut, how Heaviside function its inserted in the ''Compact Level Set Method?''\n\nSince the specific mass and viscosity are discontinuous at the interface, both excess diffusion problem (interface widening) and numerical oscillations are expected if there is no adequate treatment of the fluid near the interface. To minimize these problems, the Level Set method uses a smooth, cell-related Heaviside function that explicitly defines the interface position (∅ = 0).\n\nThe transition in the interface is kept smooth, but with a thickness of the order of magnitude of the cell size, to avoid the introduction of disturbances with a length scale equal to that of the mesh, since the interface infers an abrupt jump property from one cell to the next (Unverdi and Tryggvason, 1992). To reconstruct the material properties of the flow, such as specific mass and viscosity, another marker function, I (∅), of the Heaviside type is used:\n\n<math>I (\\phi) = \\begin{cases} 0, & \\text{if }|\\phi|<-\\delta\\Delta \\\\ 1/2[1+(\\phi/\\delta\\Delta)+1/\\pi(\\sin(\\pi\\phi/\\delta\\Delta))], & \\text{if } |\\phi| \\leq \\delta\\Delta \\\\ 1, & \\text{if }|\\phi|>\\delta\\Delta   \\end{cases}</math>  (1)\n\nwhere δ is an empirical coefficient, usually equal to 1; 5 and Δ is the characteristic discretization of the problem, which varies according to the phenomenon to be simulated. The value of δ represents an interface with a thickness of three cells, and thus δΔ represents half the thickness of the interface. Note that in this method, the interface has a virtual thickness, as it is represented by a smooth function. Physical properties, such as specific mass and kinematic viscosity, are calculated as:\n\n                   <math>\\rho = (1-I)\\rho1+I\\rho2       \\qquad e\\qquad   v=(1-I)v1+Iv2        </math>  (2)\n\nwhere ρ1, ρ2, v1 and v2 are the specific mass and kinematic viscosity of fluids 1 and 2. Equation 2 can be applied analogously to the other properties of the fluids.\n\n==See also==\n* [[Zebra striping (computer graphics)]]\n* [[G equation]]\n* [[Advanced Simulation Library]]\n* [[Volume of fluid method]]\n* [[Image segmentation#Level-set methods]]\n* [[Immersed boundary method]]\n* [[Stochastic Eulerian Lagrangian method]]\n* [[b:Fractals/Iterations in the complex plane/Julia set|LSM/J Level-set method for drawing dynamical plane]]\n* [[b:Fractals/Iterations in the complex plane/Mandelbrot set|LSM/M Level-set method for drawing parameter plane]]\n* [[Level set (data structures)]]\n* [[VISPACK]]\n\n==References==\n{{reflist}}\n\n==External links==\n* See [[Ronald Fedkiw]]'s [http://graphics.stanford.edu/~fedkiw/ academic web page] for many stunning pictures and animations showing how the level-set method can be used to model real-life phenomena, like fire, water, cloth, fracturing materials, etc.\n* [http://vivienmallet.net/fronts/ Multivac] is a C++ library for front tracking in 2D with level-set methods.\n* [[James Sethian]]'s [http://math.berkeley.edu/~sethian/ web page] on level-set method.\n* [[Stanley Osher]]'s [http://www.math.ucla.edu/~sjo/ homepage].\n\n{{Numerical PDE}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Computer graphics algorithms]]\n[[Category:Image processing]]\n[[Category:Computational fluid dynamics]]\n[[Category:Articles containing video clips]]"
    },
    {
      "title": "Levenberg–Marquardt algorithm",
      "url": "https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm",
      "text": "In [[mathematics]] and computing, the '''Levenberg–Marquardt algorithm''' ('''LMA''' or just '''LM'''), also known as the '''damped least-squares''' ('''DLS''') method, is used to solve [[non-linear least squares]] problems.  These minimization problems arise especially in [[least squares]] [[curve fitting]].\n\nThe LMA is used in many software applications for solving generic curve-fitting problems. However, as with many fitting algorithms, the LMA finds only a [[local minimum]], which is not necessarily the [[global minimum]].  The LMA interpolates between the [[Gauss–Newton algorithm]] (GNA) and the method of [[gradient descent]]. The LMA is more [[Robustness (computer science)|robust]] than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. For well-behaved functions and reasonable starting parameters, the LMA tends to be a bit slower than the GNA. LMA can also be viewed as [[Gauss–Newton]] using a [[trust region]] approach.\n\nThe algorithm was first published in 1944 by [[Kenneth Levenberg]],<ref name=\"Levenberg\"/> while working at the [[Frankford Arsenal|Frankford Army Arsenal]]. It was rediscovered in 1963 by [[Donald Marquardt]],<ref name=\"Marquardt\"/> who worked as a [[statistician]] at [[DuPont]], and independently by Girard,<ref name=\"Girard\"/> Wynne<ref name=\"Wynne\"/> and Morrison.<ref name=\"Morrison\"/>\n\n== The problem ==\nThe primary application of the Levenberg–Marquardt algorithm is in the least-squares curve fitting problem: given a set of <math>m</math> empirical datum pairs <math>\\left (x_i, y_i\\right )</math> of independent and dependent variables, find the parameters {{tmath|\\boldsymbol\\beta}} of the model curve <math>f\\left (x, \\boldsymbol\\beta\\right )</math> so that the sum of the squares of the deviations <math>S\\left (\\boldsymbol\\beta\\right )</math> is minimized:\n\n:<math>\\hat{\\boldsymbol\\beta} \\in \\operatorname{argmin}\\limits_{\\boldsymbol\\beta} S\\left (\\boldsymbol\\beta\\right ) \\equiv \\operatorname{argmin}\\limits_{\\boldsymbol\\beta} \\sum_{i=1}^m \\left [y_i - f\\left (x_i, \\boldsymbol\\beta\\right )\\right ]^2,</math> which is assumed to be non-empty.\n\n== The solution ==\nLike other numeric minimization algorithms, the Levenberg–Marquardt algorithm is an [[iteration|iterative]] procedure. To start a minimization, the user has to provide an initial guess for the parameter vector {{tmath|\\boldsymbol\\beta}}. In cases with only one minimum, an uninformed standard guess like <math>\\boldsymbol\\beta^\\text{T} = \\begin{pmatrix}1,\\ 1,\\ \\dots,\\ 1\\end{pmatrix}</math> will work fine; in cases with [[local minimum|multiple minima]], the algorithm converges to the global minimum only if the initial guess is already somewhat close to the final solution.\n\nIn each iteration step, the parameter vector {{tmath|\\boldsymbol\\beta}} is replaced by a new estimate {{tmath|\\boldsymbol\\beta + \\boldsymbol\\delta}}. To determine {{tmath|\\boldsymbol\\delta}}, the function <math>f\\left (x_i, \\boldsymbol\\beta + \\boldsymbol\\delta\\right )</math> is approximated by its [[Gradient#Linear_approximation_to_a_function|linearization]]:\n\n: <math>f\\left (x_i, \\boldsymbol\\beta + \\boldsymbol\\delta\\right ) \\approx f\\left (x _i, \\boldsymbol\\beta\\right ) + \\mathbf J_i \\boldsymbol\\delta,</math>\n\nwhere \n: <math>\\mathbf J_i = \\frac{\\partial f\\left (x_i, \\boldsymbol\\beta\\right )}{\\partial \\boldsymbol\\beta}</math> \nis the [[gradient]] (row-vector in this case) of {{tmath|f}} with respect to {{tmath|\\boldsymbol\\beta}}.\n\nThe sum <math>S\\left (\\boldsymbol\\beta\\right )</math> of square deviations has its minimum at a zero [[gradient]] with respect to {{tmath|\\boldsymbol\\beta}}. The above first-order approximation of <math>f\\left (x_i, \\boldsymbol\\beta + \\boldsymbol\\delta\\right )</math> gives\n: <math>S\\left (\\boldsymbol\\beta + \\boldsymbol\\delta\\right ) \\approx \\sum_{i=1}^m \\left [y_i - f\\left (x_i, \\boldsymbol\\beta\\right ) - \\mathbf J_i \\boldsymbol\\delta\\right ]^2,</math>\nor in vector notation,\n: <math>\\begin{align}\n S\\left (\\boldsymbol\\beta + \\boldsymbol\\delta\\right ) &\\approx \\left \\|\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right ) - \\mathbf J\\boldsymbol\\delta\\right \\|^2\\\\\n  &= \\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right ) - \\mathbf J\\boldsymbol\\delta \\right ]^{\\mathrm T}\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right ) - \\mathbf J\\boldsymbol\\delta\\right ]\\\\\n  &= \\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ]^{\\mathrm T}\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ] - \\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ]^{\\mathrm T} \\mathbf J \\boldsymbol\\delta - \\left (\\mathbf J \\boldsymbol\\delta\\right )^{\\mathrm T} \\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ] + \\boldsymbol\\delta^{\\mathrm T} \\mathbf J^{\\mathrm T} \\mathbf J \\boldsymbol\\delta\\\\\n  &= \\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ]^{\\mathrm T}\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ] - 2\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ]^{\\mathrm T} \\mathbf J \\boldsymbol\\delta + \\boldsymbol\\delta^{\\mathrm T} \\mathbf J^{\\mathrm T} \\mathbf J\\boldsymbol\\delta.\n\\end{align}</math>\nTaking the derivative of <math>S\\left (\\boldsymbol\\beta + \\boldsymbol\\delta\\right )</math> with respect to {{tmath|\\boldsymbol\\delta}} and setting the result to zero gives\n\n:<math>\\left (\\mathbf J^{\\mathrm T} \\mathbf J\\right )\\boldsymbol\\delta = \\mathbf J^{\\mathrm T}\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ],</math>\n\nwhere <math>\\mathbf J</math> is the [[Jacobian matrix and determinant|Jacobian matrix]], whose {{tmath|i}}-th row equals <math>\\mathbf J_i</math>, and where <math>\\mathbf f\\left (\\boldsymbol\\beta\\right )</math> and <math>\\mathbf y</math> are vectors with {{tmath|i}}-th component \n<math>f\\left (x_i, \\boldsymbol\\beta\\right )</math> and <math>y_i</math> respectively.\nThis is a set of linear equations, which can be solved for {{tmath|\\boldsymbol\\delta}}.\n\nLevenberg's contribution is to replace this equation by a \"damped version\":\n\n:<math>\\left (\\mathbf J^{\\mathrm T} \\mathbf J + \\lambda\\mathbf I\\right ) \\boldsymbol\\delta = \\mathbf J^{\\mathrm T}\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right],</math>\n\nwhere {{tmath|\\mathbf I}} is the identity matrix, giving as the increment {{tmath|\\boldsymbol\\delta}} to the estimated parameter vector {{tmath|\\boldsymbol\\beta}}.\n\nThe (non-negative) damping factor {{tmath|\\lambda}} is adjusted at each iteration. If reduction of {{tmath|S}} is rapid, a smaller value can be used, bringing the algorithm closer to the [[Gauss–Newton algorithm]], whereas if an iteration gives insufficient reduction in the residual, {{tmath|\\lambda}} can be increased, giving a step closer to the gradient-descent direction. Note that the [[gradient]] of {{tmath|S}} with respect to {{tmath|\\boldsymbol\\delta}} equals <math>-2\\left (\\mathbf J^{\\mathrm T}\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ]\\right )^{\\mathrm T}</math>. Therefore, for large values of {{tmath|\\lambda}}, the step will be taken approximately in the direction of the gradient. If either the length of the calculated step {{tmath|\\boldsymbol\\delta}} or the reduction of sum of squares from the latest parameter vector {{tmath|\\boldsymbol\\beta + \\boldsymbol\\delta}} fall below predefined limits, iteration stops, and the last parameter vector {{tmath|\\boldsymbol\\beta}} is considered to be the solution.\n\nLevenberg's algorithm has the disadvantage that if the value of damping factor {{tmath|\\lambda}} is large, inverting {{tmath|\\mathbf J^\\text{T}\\mathbf J + \\lambda\\mathbf I}} is not used at all. R. Fletcher provided the insight that we can scale each component of the gradient according to the curvature, so that there is larger movement along the directions where the gradient is smaller. This avoids slow convergence in the direction of small gradient. Therefore, Fletcher in his 1971 paper ''A modified Marquardt subroutine for non-linear least squares'' replaced the identity matrix {{tmath|\\mathbf I}} with the diagonal matrix consisting of the diagonal elements of {{tmath|\\mathbf J^\\text{T}\\mathbf J}}, thus making the solution scale invariant:\n\n:<math>\\left [\\mathbf J^{\\mathrm T} \\mathbf J + \\lambda \\operatorname{diag}\\left (\\mathbf J^{\\mathrm T} \\mathbf J\\right )\\right ] \\boldsymbol\\delta = \\mathbf J^{\\mathrm T}\\left [\\mathbf y - \\mathbf f\\left (\\boldsymbol\\beta\\right )\\right ].</math>\n\nA similar damping factor appears in [[Tikhonov regularization]], which is used to solve linear [[ill-posed problems]], as well as in [[ridge regression]], an [[estimation theory|estimation]] technique in [[statistics]].\n\n=== Choice of damping parameter ===\nVarious more or less heuristic arguments have been put forward for the best choice for the damping parameter {{tmath|\\lambda}}.  Theoretical arguments exist showing why some of these choices guarantee local convergence of the algorithm; however, these choices can make the global convergence of the algorithm suffer from the undesirable properties of [[gradient descent|steepest descent]], in particular, very slow convergence close to the optimum.\n\nThe absolute values of any choice depend on how well-scaled the initial problem is.  Marquardt recommended starting with a value {{tmath|\\lambda_0}} and a factor {{tmath|\\nu > 1}}. Initially setting <math>\\lambda = \\lambda_0</math> and computing the residual sum of squares <math>S\\left (\\boldsymbol\\beta\\right )</math> after one step from the starting point with the damping factor of <math>\\lambda = \\lambda_0</math> and secondly with {{tmath|\\lambda_0 / \\nu}}.  If both of these are worse than the initial point, then the damping is increased by successive multiplication by {{tmath|\\nu}} until a better point is found with a new damping factor of {{tmath|\\lambda_0\\nu^k}} for some {{tmath|k}}.\n\nIf use of the damping factor {{tmath|\\lambda / \\nu}} results in a reduction in squared residual, then this is taken as the new value of {{tmath|\\lambda}} (and the new optimum location is taken as that obtained with this damping factor) and the process continues; if using {{tmath|\\lambda / \\nu}} resulted in a worse residual, but using {{tmath|\\lambda}} resulted in a better residual, then {{tmath|\\lambda}} is left unchanged and the new optimum is taken as the value obtained with {{tmath|\\lambda}} as damping factor.\n\n==Example==\n\n[[Image:Lev-Mar-poor-fit.png|thumb|Poor fit]]\n[[Image:Lev-Mar-better-fit.png|thumb|Better fit]]\n[[Image:Lev-Mar-best-fit.png|thumb|Best fit]]\n\nIn this example we try to fit the function <math>y = a \\cos\\left (bX\\right ) + b \\sin\\left (aX\\right )</math> using the Levenberg–Marquardt algorithm implemented in [[GNU Octave]] as the ''leasqr'' function. The graphs show progressively better fitting for the parameters <math>a = 100</math>, <math>b = 102</math> used\nin the initial curve. Only when the parameters in the last graph are chosen closest to the original, are the curves fitting exactly. This equation\nis an example of very sensitive initial conditions for the Levenberg–Marquardt algorithm. One reason for this sensitivity is the existence of multiple minima — the function <math>\\cos\\left (\\beta x\\right )</math> has minima at parameter value <math>\\hat\\beta</math> and <math>\\hat\\beta + 2n\\pi</math>.\n\n== See also ==\n* [[Trust region]]\n* [[Nelder–Mead method]] (aka simplex)\n* Variants of the Levenberg–Marquardt algorithm have also been used for solving nonlinear systems of equations.<ref>{{cite journal |doi=10.1016/j.cam.2004.02.013|title=Levenberg–Marquardt methods with strong local convergence properties for solving nonlinear equations with convex constraints|journal=Journal of Computational and Applied Mathematics|volume=172|issue=2|pages=375–397|year=2004|last1=Kanzow|first1=Christian|last2=Yamashita|first2=Nobuo|last3=Fukushima|first3=Masao}}</ref>\n\n==References==\n{{Reflist|refs=\n<ref name=\"Levenberg\">{{cite journal\n  | last=Levenberg |first=Kenneth |authorlink=Kenneth Levenberg\n  | year    = 1944\n  | title   = A Method for the Solution of Certain Non-Linear Problems in Least Squares\n  | journal = Quarterly of Applied Mathematics\n  | volume  = 2\n  |issue=2 | pages   = 164–168\n  |doi=10.1090/qam/10666 }}</ref>\n<ref name=\"Girard\">{{cite journal\n  | last=Girard |first=André\n  | year    = 1958\n  | title   = Excerpt from ''Revue d'optique théorique et instrumentale''\n  | journal = Rev. Opt.\n  | volume  = 37\n  | pages   = 225–241, 397–424\n  }}\n</ref>\n<ref name=\"Wynne\">{{cite journal\n  | last=Wynne |first=C. G.\n  | year    = 1959\n  | title   = Lens Designing by Electronic Digital Computer: I\n  | journal = Proc. Phys. Soc. Lond.\n  | doi     = 10.1088/0370-1328/73/5/310\n  | volume  = 73\n  | issue   = 5\n  | pages   = 777–787\n  |bibcode=1959PPS....73..777W\n }}\n</ref>\n<ref name=\"Morrison\">{{cite journal\n  | last    = Morrison  | first   = David D.\n  | year    = 1960\n  | title   = Methods for nonlinear least squares problems and convergence proofs\n  | journal = Proceedings of the Jet Propulsion Laboratory Seminar on Tracking Programs and Orbit Determination\n  | pages   = 1–9\n  }}</ref>\n<ref name=\"Marquardt\">{{cite journal\n  | last=Marquardt |first=Donald |authorlink=Donald Marquardt\n  | year    = 1963\n  | title   = An Algorithm for Least-Squares Estimation of Nonlinear Parameters\n  | journal = SIAM Journal on Applied Mathematics\n  | doi     = 10.1137/0111030\n  | volume  = 11\n  | issue   = 2\n  | pages   = 431–441\n  }}</ref>\n}}\n\n==Further reading==\n{{Refbegin}}\n* {{cite journal\n  | last1=Moré |first1=Jorge J. \n  | last2=Sorensen |first2=Daniel C.\n  | year    = 1983\n  | title   = Computing a Trust-Region Step\n  | journal = SIAM J. Sci. Stat. Comput.\n  |volume=4 \n | pages   = 553–572\n  | issue   = 4\n  |doi=10.1137/0904038 \n }}\n* {{cite journal\n  | last1=Gill |first1= Philip E.\n  | last2=Murray |first2=Walter |authorlink2=Walter Murray (mathematician)\n  | year    = 1978\n  | title   = Algorithms for the solution of the nonlinear least-squares problem\n  | journal = [[SIAM Journal on Numerical Analysis]]\n  | doi     = 10.1137/0715063\n  | volume  = 15\n  | issue   = 5\n  | pages   = 977–992\n  |bibcode= 1978SJNA...15..977G\n }}\n* {{cite journal\n  | last=Pujol |first= Jose\n  | year      = 2007\n  | title     = The solution of nonlinear inverse problems and the Levenberg-Marquardt method\n  | journal   = Geophysics\n  | publisher = SEG\n  | doi       = 10.1190/1.2732552\n  | volume    = 72\n  | number    = 4\n  | pages     = W1–W16\n  | url       = http://link.aip.org/link/?GPY/72/W1/1\n  |bibcode= 2007Geop...72W...1P\n }}\n* {{cite book\n  | last1     = Nocedal  | first1     = Jorge\n  | last2     = Wright   | first2     = Stephen J.\n  | year      = 2006\n  | title     = Numerical Optimization |edition=2nd\n  | publisher = Springer\n  | isbn      = 978-0-387-30303-1\n  }}\n{{Refend}}\n\n== External links ==\n\n===Descriptions===\n* Detailed description of the algorithm can be found in [http://www.nrbook.com/a/bookcpdf.php Numerical Recipes in C, Chapter 15.5: Nonlinear models]\n* C. T. Kelley, ''Iterative Methods for Optimization'', SIAM Frontiers in Applied Mathematics, no 18, 1999, {{isbn|0-89871-433-8}}. [http://www.siam.org/books/textbooks/fr18_book.pdf Online copy]\n* [https://web.archive.org/web/20140301154319/http://www3.villanova.edu/maple/misc/mtc1093.html History of the algorithm in SIAM news]\n* [http://ananth.in/docs/lmtut.pdf A tutorial by Ananth Ranganathan]\n* [http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3215/pdf/imm3215.pdf Methods for Non-Linear Least Squares Problems] by K. Madsen, H. B. Nielsen, O. Tingleff is a tutorial discussing nonlinear least-squares in general and the Levenberg–Marquardt method in particular\n* T. Strutz: ''Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond).'' 2nd edition, Springer Vieweg, 2016, {{isbn|978-3-658-11455-8}}.\n* H. P. Gavin, [http://people.duke.edu/~hpgavin/ce281/lm.pdf ''The Levenberg-Marquardt method for nonlinear least squares curve-fitting problems''] ([[MATLAB]] implementation included)\n\n=== Implementations ===\nLevenberg-Marquardt is a built-in algorithm in [[SciPy]], [[GNU Octave]], [[Scilab]], [[Mathematica]] <!-- <ref>[http://reference.wolfram.com/mathematica/tutorial/UnconstrainedOptimizationIntroductionLocalMinimization.html Unconstrained optimization methods in Mathematica.</ref> -->, [[Matlab]], [[NeuroSolutions]], [[Origin (data analysis software)|Origin]], [[Fityk]], [[IGOR Pro]], [[LabVIEW]] and  [[SAS (software)|SAS]] numerical computing environments. There also exist numerous software libraries which allow to use LM algorithm in standalone applications. Some of them support only basic unconstrained optimization, whilst other ones support different combinations of box and linear constraints.\n\nBox and linearly constrained implementations:\n* [[ALGLIB]] has box and linearly constrained [http://www.alglib.net/optimization/levenbergmarquardt.php implementation] of improved LM in C# and C++. Improved algorithm takes less time to converge and can use either Jacobian or exact [[Hessian matrix|Hessian]].\n* [[Artelys Knitro]] is a non-linear solver with an implementation of the box-constrained Levenberg–Marquardt algorithm. It is written in C and has interfaces to C++/C#/Java/Python/MATLAB/R.\n* [http://ceres-solver.org/ ceres] is a non-linear minimization library with an implementation of the box-constrained Levenberg–Marquardt algorithm. It is written in C++ and uses [http://eigen.tuxfamily.org/index.php?title=Main_Page eigen].\n* [[IDL (programming language)|IDL]], add-on [http://cow.physics.wisc.edu/~craigm/idl/fitting.html MPFIT] supports box constraints.\n* [http://www.ics.forth.gr/%7elourakis/levmar/ levmar] is an implementation in [[C (programming language)|C]]/[[C++]] with support for box and general linear constraints, distributed under the [[GNU General Public License]].\n** levmar includes a [[MEX file]] interface for [[MATLAB]].\n** [[Perl]] ([[Perl Data Language|PDL]]), [[Python (programming language)|python]], [[Haskell (programming language)|Haskell]] and [[.NET Framework|.NET]] interfaces to levmar are available: see [http://www.johnlapeyre.com/pdl/index.html PDL::Fit::Levmar] or [https://metacpan.org/module/PDL::Fit::LM PDL::Fit::LM], [https://github.com/bjodah/levmar levmar (for python)], [http://hackage.haskell.org/package/levmar HackageDB levmar] and [https://github.com/AvengerDr/LevmarSharp LevmarSharp].\n* [[R (programming language)]] has the [https://cran.r-project.org/web/packages/minpack.lm/index.html minpack.lm] package (box constrained).\n\nUnconstrained implementations:\n* The oldest implementation still in use is [http://www.netlib.org/minpack/ lmdif], from [[MINPACK]], in [[Fortran]], in the [[public domain]]. See also:\n** [http://apps.jcns.fz-juelich.de/lmfit lmfit], a self-contained [[C programming language|C]] implementation of the MINPACK algorithm, with an easy-to-use wrapper for curve fitting, liberal licence (freeBSD).\n** [http://eigen.tuxfamily.org/index.php?title=Main_Page eigen], a C++ linear-algebra library, includes an adaptation of the minpack algorithm in the \"NonLinearOptimization\" module.\n** The [[GNU Scientific Library]] has a C interface to MINPACK.\n** [http://devernay.free.fr/hacks/cminpack.html C/C++ Minpack] includes the Levenberg–Marquardt algorithm.\n** Python library [[scipy]], module <code>scipy.optimize.leastsq</code> provides wrapper for the [[MINPACK]] routines.\n* [http://www.ics.forth.gr/%7elourakis/sparseLM/ sparseLM] is a [[C (programming language)|C]] implementation aimed at minimizing functions with large, arbitrarily [[Sparse matrix|sparse]] Jacobians. Includes a MATLAB MEX interface. Unconstrained only.\n* [https://web.archive.org/web/20130722142233/http://www2.imm.dtu.dk/~hbni/Software/SMarquardt.m SMarquardt.m] is a stand-alone routine for Matlab or Octave.\n* [http://www.bnikolic.co.uk/inmin/inmin-library.html InMin] library contains a C++ implementation of the algorithm based on the [http://eigen.tuxfamily.org/index.php?title=Main_Page eigen] C++ linear-algebra library. It has a pure C-language API, as well as a Python binding.\n* [[NMath]] has an implementation for the [[.NET Framework]].\n* [[gnuplot]] uses its own implementation [http://www.gnuplot.info/ gnuplot.info].\n* [[Java (programming language)|Java programming language]] implementations: 1) [http://scribblethink.org/Computer/Javanumeric/index.html Javanumerics], 2) [https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math4/fitting/leastsquares/LevenbergMarquardtOptimizer.html Apache Commons Math], 3) [http://finmath.net/finmath-lib/ finmath lib].\n* [http://oooconv.free.fr/fitoo/fitoo_en.html OOoConv] implements the L–M algorithm as an OpenOffice.org Calc spreadsheet.\n* [https://github.com/namp/lmam-olmam-matlab-toolbox LMAM/OLMAM Matlab toolbox] implements Levenberg–Marquardt with adaptive momentum for training feedforward neural networks.\n* [https://raullaasner.github.io/gadfit GADfit] is a Fortran implementation of global fitting based on a modified Levenberg–Marquardt. Uses automatic differentiation. Allows fitting functions of arbitrary complexity, including integrals.\n\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Levenberg-Marquardt algorithm}}\n[[Category:Statistical algorithms]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Least squares]]"
    },
    {
      "title": "Limited-memory BFGS",
      "url": "https://en.wikipedia.org/wiki/Limited-memory_BFGS",
      "text": "'''Limited-memory BFGS''' ('''L-BFGS''' or '''LM-BFGS''') is an [[optimization (mathematics)|optimization]] [[algorithm]] in the family of [[quasi-Newton method]]s that approximates the [[BFGS method|Broyden–Fletcher–Goldfarb–Shanno (BFGS)]] algorithm using a limited amount of [[computer memory]]. It is a popular algorithm for parameter estimation in [[machine learning]].<ref name=\"malouf\"\n>{{cite conference |url=https://dl.acm.org/citation.cfm?id=1118871 |title=A comparison of algorithms for maximum entropy parameter estimation |last1=Malouf |first1=Robert |date= 2002|book-title= Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002) |pages= 49–55 |doi=10.3115/1118853.1118871 }}</ref><ref name=\"owlqn\"/> The algorithm's target problem is to minimize <math>f(\\mathbf{x})</math> over unconstrained values of the real-vector <math>\\mathbf{x}</math> where <math>f</math> is a differentiable scalar function.\n\nLike the original BFGS, L-BFGS uses an estimation to the inverse [[Hessian matrix]] to steer its search through variable space, but where BFGS stores a dense <math>n\\times n</math> approximation to the inverse Hessian (''n'' being the number of variables in the problem), L-BFGS stores only a few vectors that represent the approximation implicitly. Due to its resulting linear memory requirement, the L-BFGS method is particularly well suited for optimization problems with a large number of variables. Instead of the inverse Hessian '''H'''''<sub>k</sub>'', L-BFGS maintains a history of the past ''m'' updates of the position '''x''' and gradient ∇''f''('''x'''), where generally the history size ''m'' can be small (often <math>m<10</math>). These updates are used to implicitly do operations requiring the '''H'''''<sub>k</sub>''-vector product.\n\n==Algorithm==\nThe algorithm starts with an initial estimate of the optimal value, <math>\\mathbf{x}_0</math>, and proceeds iteratively to refine that estimate with a sequence of better estimates <math>\\mathbf{x}_1,\\mathbf{x}_2,\\ldots</math>. The derivatives of the function <math>g_k:=\\nabla f(\\mathbf{x}_k)</math> are used as a key driver of the algorithm to identify the direction of steepest descent, and also to form an estimate of the Hessian matrix (second derivative) of <math>f(\\mathbf{x})</math>.\n\nL-BFGS shares many features with other quasi-Newton algorithms, but is very different in how the matrix-vector multiplication <math>d_k=-H_k g_k</math> is carried out, where <math>d_k</math> is the approximate Newton's direction,  <math>g_k</math> is the current gradient, and <math>H_k</math> is the inverse of the Hessian matrix. There are multiple published approaches using a history of updates to form this direction vector. Here, we give a common approach, the so-called \"two loop recursion.\"<ref>{{Cite journal|doi=10.1002/nme.1620141104|first1=H.|last1= Matthies |first2= G.|last2= Strang|title=The solution of non linear finite element equations |year=1979|journal= International Journal for Numerical Methods in Engineering |volume=14|pages=1613–1626|issue=11|bibcode=1979IJNME..14.1613M}}</ref><ref>{{cite journal|doi=10.1090/S0025-5718-1980-0572855-7|first=J. |last=Nocedal|title= Updating Quasi-Newton Matrices with Limited Storage |year=1980|journal=Mathematics of Computation |volume=35|pages=773–782|issue=151}}</ref>\n\nWe take as given <math>x_k</math>, the position at the {{mvar|k}}-th iteration, and <math>g_k\\equiv\\nabla f(x_k)</math> where <math>f</math> is the function being minimized, and all vectors are column vectors. We also assume that we have stored the last {{mvar|m}} updates of the form \n\n:<math>s_k = x_{k+1} - x_k</math> \n\n:<math>y_k = g_{k+1} - g_k</math>. \n\nWe define <math>\\rho_k = \\frac{1}{y^{\\top}_k s_k} </math>, and <math>H^0_k</math> will be the 'initial' approximate of the inverse Hessian that our estimate at iteration {{mvar|k}} begins with.\n\nThe algorithm is based on the BFGS recursion for the inverse Hessian as\n\n:<math>H_{k+1} = (I-\\rho_k s_k y_k^\\top)H_k(I-\\rho_k y_k s_k^\\top) + \\rho_k s_k s_k^\\top. </math>\n\nFor a fixed {{mvar|k}} we define a sequence of vectors <math>q_{k-m},\\ldots,q_k</math> as <math>q_k:=g_k</math> and <math>q_i:=(I-\\rho_i y_i s_i^\\top)q_{i+1}</math>. Then a recursive algorithm for calculating <math>q_i</math> from <math>q_{i+1}</math> is to define <math>\\alpha_i := \\rho_i s_i^\\top q_{i+1}</math> and <math>q_i=q_{i+1}-\\alpha_i y_i</math>. We also define another sequence of vectors <math>z_{k-m},\\ldots,z_k</math> as <math>z_i:=-H_iq_i</math>. There is another recursive algorithm for calculating these vectors which is to define <math>z_{k-m}=-H_k^0 q_{k-m}</math> and then recursively define <math>\\beta_i:=\\rho_i y_i^\\top z_i</math> and <math>z_{i+1}=z_i + (\\alpha_i - \\beta_i)s_i</math>. The value of <math>z_k</math> is then our descent direction.\n\nThus we can compute the descent direction as follows:\n \n: <math>\\begin{array}{l}\nq = g_k\\\\ \n\\mathtt{For}\\ i=k-1, k-2, \\ldots, k-m\\\\ \n\\qquad \\alpha_i = \\rho_i s^\\top_i q\\\\ \n\\qquad q = q - \\alpha_i y_i\\\\ \n\\gamma_k = \\frac{s_{k - 1}^{\\top} y_{k - 1}}{y_{k - 1}^{\\top} y_{k - 1}} \\\\ \nH^0_k=  \\gamma_k I\\\\ \nz = H^0_k q\\\\ \n\\mathtt{For}\\ i=k-m, k-m+1, \\ldots, k-1\\\\ \n\\qquad \\beta_i = \\rho_i y^\\top_i z\\\\ \n\\qquad z = z + s_i (\\alpha_i - \\beta_i)\n\\end{array}</math>\n\nThis formulation gives the search direction for the maximization problem, i.e., <math>z = H_k g_k</math>. For minimization problems, one should thus take {{mvar|-z}}. Note that the initial approximate inverse Hessian <math>H^0_k</math> is chosen as a diagonal matrix or even a multiple of the identity matrix since this is numerically efficient.\n\nThe scaling of the initial matrix <math>\\gamma_k</math> ensures that the search direction is well scaled and therefore the unit step length is accepted in most iterations. A [[Wolfe conditions|Wolfe line search]] is used to ensure that the curvature condition is satisfied and the BFGS updating is stable. Note that some software implementations use an Armijo [[backtracking line search]], but cannot guarantee that the curvature condition <math>y_k^{\\top} s_k > 0</math> will be satisfied by the chosen step since a step length greater than <math>1</math> may be needed to satisfy this condition. Some implementations address this by skipping the BFGS update when <math>y_k^{\\top} s_k</math> is negative or too close to zero, but this approach is not generally recommended since the updates may be skipped too often to allow the Hessian approximation <math>H_k</math> to capture important curvature information.\n\nThis two loop update only works for the inverse Hessian. Approaches to implementing L-BFGS using the direct approximate Hessian <math>B_k</math> have also been developed, as have other means of approximating the inverse Hessian.<ref>{{cite journal|doi=10.1007/BF01582063|last1=Byrd|first1= R. H.|last2= Nocedal|first2=J.|last3= Schnabel|first3= R. B.|title=Representations of Quasi-Newton Matrices and their use in Limited Memory Methods|year=1994|journal= Mathematical Programming|volume=63|issue= 4|pages=129–156}}</ref>\n\n==Applications==\nL-BFGS has been called \"the algorithm of choice\" for fitting [[Multinomial logit|log-linear (MaxEnt) models]] and [[conditional random field]]s with [[Regularization (mathematics)|<math>\\ell_2</math>-regularization]].<ref name=\"malouf\"/><ref name=\"owlqn\">{{Cite book | last1 = Andrew | first1 = Galen| last2 = Gao | first2 = Jianfeng| doi = 10.1145/1273496.1273501 | chapter = Scalable training of L₁-regularized log-linear models | title = Proceedings of the 24th International Conference on Machine Learning| pages = | year = 2007 | isbn = 9781595937933 | pmid =  | pmc = | chapter-url = http://research.microsoft.com/apps/pubs/default.aspx?id=78900}}</ref>\n\n==Variants==\nSince BFGS (and hence L-BFGS) is designed to minimize [[smooth function|smooth]] functions without [[Constraint (mathematics)|constraints]], the L-BFGS algorithm must be modified to handle functions that include non-[[Differentiable function|differentiable]] components or constraints. A popular class of modifications are called active-set methods, based on the concept of the [[active set]]. The idea is that when restricted to a small neighborhood of the current iterate, the function and constraints can be simplified.\n\n===L-BFGS-B===\nThe '''L-BFGS-B''' algorithm extends L-BFGS to handle simple box constraints (aka bound constraints) on variables; that is, constraints of the form {{math|''l<sub>i</sub>'' ≤ ''x<sub>i</sub>'' ≤ ''u<sub>i</sub>''}} where {{mvar|l<sub>i</sub>}} and {{mvar|u<sub>i</sub>}} are per-variable constant lower and upper bounds, respectively (for each {{mvar|x<sub>i</sub>}}, either or both bounds may be omitted).<ref name=\"LBFGSB1\">{{Cite journal | last1 = Byrd | first1 = R. H. | last2 = Lu | first2 = P. | last3 = Nocedal | first3 = J. | last4 = Zhu | first4 = C. | title = A Limited Memory Algorithm for Bound Constrained Optimization | doi = 10.1137/0916069 | journal = [[SIAM Journal on Scientific Computing|SIAM J. Sci. Comput.]] | volume = 16 | issue = 5 | pages = 1190–1208 | year = 1995 | pmid =  | pmc = }}</ref><ref name=\"algo778\">{{cite journal|last1=Zhu|doi=10.1145/279232.279236|first1=C.|last2=Byrd|first2=Richard H.|last3=Lu|first3=Peihuang|last4=Nocedal|first4=Jorge |title=L-BFGS-B: Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization |year=1997|journal= ACM Transactions on Mathematical Software|volume= 23|issue= 4|pages= 550–560}}</ref> The method works by identifying fixed and free variables at every step (using a simple gradient method), and then using the L-BFGS method on the free variables only to get higher accuracy, and then repeating the process.\n\n===OWL-QN===\n'''Orthant-wise limited-memory quasi-Newton''' ('''OWL-QN''') is an L-BFGS variant for fitting [[Taxicab geometry|<math>\\ell_1</math>]]-[[Regularization (mathematics)|regularized]] models, exploiting the inherent [[Sparse matrix|sparsity]] of such models.<ref name=\"owlqn\"/>\nIt minimizes functions of the form\n\n:<math>f(\\vec x) = g(\\vec x) + C \\|\\vec x\\|_1</math>\n\nwhere <math>g</math> is a [[Differentiable function|differentiable]] [[Convex function|convex]] [[loss function]]. The method is an active-set type method: at each iterate, it estimates the [[Sign (mathematics)|sign]] of each component of the variable, and restricts the subsequent step to have the same sign. Once the sign is fixed, the non-differentiable <math> \\|\\vec x\\|_1</math> term becomes a smooth linear term which can be handled by L-BFGS. After an L-BFGS step, the method allows some variables to change sign, and repeats the process.\n\n===O-LBFGS===\nSchraudolph ''et al.'' present an [[online machine learning|online]] approximation to both BFGS and L-BFGS.<ref>{{cite conference |title=A stochastic quasi-Newton method for online convex optimization |first1=N. |last1=Schraudolph |first2=J. |last2=Yu |first3=S. |last3=Günter |conference=AISTATS |year=2007}}</ref> Similar to [[stochastic gradient descent]], this can be used to reduce the computational complexity by evaluating the error function and gradient on a randomly drawn subset of the overall dataset in each iteration. It has been shown that O-LBFGS has a global almost sure convergence <ref>{{cite journal |title=Global convergence of online limited memory BFGS |first1=A. |last1=Mokhtari |first2=A. |last2=Ribeiro |journal= Journal of Machine Learning Research  |volume=16|pages=3151–3181|year=2015|url=http://www.jmlr.org/papers/volume16/mokhtari15a/mokhtari15a.pdf}}</ref> while the online approximation of BFGS (O-BFGS) is not necessarily convergent.<ref>{{cite journal |title=RES: Regularized Stochastic BFGS Algorithm |doi=10.1109/TSP.2014.2357775 |first1=A. |last1=Mokhtari |first2=A. |last2=Ribeiro |journal= IEEE Transactions on Signal Processing |volume=62 |number=23 |pages=6089–6104 |year=2014 |url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6899692&url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D6899692|arxiv=1401.7625|bibcode=2014ITSP...62.6089M|citeseerx=10.1.1.756.3003 }}</ref>\n\n==Implementation of variants==\nThe L-BFGS-B variant also exists as ACM TOMS algorithm 778.<ref name=\"algo778\" /><ref>http://toms.acm.org/</ref> In February 2011, some of the authors of the original L-BFGS-B code posted a major update (version 3.0).\n\nA reference implementation is available in [[Fortran 77#FORTRAN 77|Fortran 77]] (and with a [[Fortran#Fortran 90|Fortran 90]] interface).<ref name=\"LBFGSB_update\">{{Cite journal | last1 = Morales | first1 = J. L. | last2 = Nocedal | first2 = J. | doi = 10.1145/2049662.2049669 | title = Remark on \"algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound constrained optimization\" | journal = ACM Transactions on Mathematical Software | volume = 38 | pages = 1–4 | year = 2011 | pmid =  | pmc = }}</ref><ref>http://users.eecs.northwestern.edu/~nocedal/lbfgsb.html</ref> This version, as well as older versions, has been converted to many other languages.\n\nAn OWL-QN implementation is available as C++ implementation by its designers.<ref name=\"owlqn\" /><ref>http://research.microsoft.com/en-us/downloads/b1eb1016-1738-4bd5-83a9-370c9d498a03/</ref>\n==Works cited==\n{{reflist|2}}\n\n==Further reading==\n*{{cite journal|doi= 10.1007/BF01589116|first=D. C.|last1= Liu |first2= J.|last2= Nocedal|url=http://www.ece.northwestern.edu/~nocedal/PSfiles/limited-memory.ps.gz|title= On the Limited Memory Method for Large Scale Optimization|year=1989|journal= Mathematical Programming B|volume=45|issue= 3|pages= 503–528|citeseerx=10.1.1.110.6443}}\n*{{cite journal|doi=10.1137/0916069|last1=Byrd|url=http://www.ece.northwestern.edu/~nocedal/PSfiles/limited.ps.gz|first1=Richard H.|last2=Lu|first2=Peihuang|last3=Nocedal|first3=Jorge|last4=Zhu|first4=Ciyou|title=A Limited Memory Algorithm for Bound Constrained Optimization|year=1995|journal= SIAM Journal on Scientific and Statistical Computing|pages=1190–1208|volume=16|issue= 5}}\n*{{cite web|url=http://aria42.com/blog/2014/12/understanding-lbfgs|first1=Aria|last1=Haghighi|title=Numerical Optimization: Understanding L-BFGS|date=2 Dec 2014}}\n\n{{Optimization algorithms|unconstrained}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "LINCOA",
      "url": "https://en.wikipedia.org/wiki/LINCOA",
      "text": "'''LINCOA''' ('''LIN'''early '''C'''onstrained '''O'''ptimization '''A'''lgorithm) is a [[numerical analysis|numerical]] [[optimization (mathematics)|optimization]] [[algorithm]] by [[Michael J. D. Powell]]. It is also the name of [[Michael J. D. Powell|Powell]]'s [[Fortran#FORTRAN 77|Fortran 77]] implementation of the algorithm.\n\nLINCOA solves linearly [[constrained optimization]] problems without using [[derivative]]s of the [[Optimization problem|objective function]], which makes it a [[derivative-free optimization|derivative-free]] algorithm. The algorithm solves the problem using a [[trust region]] method that forms [[quadratic function|quadratic]] models by [[interpolation]]. One new point is computed on each iteration, usually by solving a [[trust region]] subproblem subject to the linear constraints, or alternatively, by choosing a point to replace an [[interpolation]] point that may be too far away for reliability. In the second case, the new point may not satisfy the linear constraints.\n\nThe same as [[NEWUOA]], LINCOA constructs the quadratic models by the least [[Frobenius norm]] updating <ref>{{cite journal|last=Powell|first=M. J. D. |title=Least Frobenius norm updating of quadratic models that satisfy interpolation conditions |journal=Mathematical Programming |publisher= Springer |year=2004 |volume=100 |pages=183–215|doi=10.1007/s10107-003-0490-7}}</ref> technique. A model function is determined by interpolating the [[Optimization problem|objective function]] at <math> m </math> (an integer between <math> n+2 </math> and <math> (n+1)(n+2)/2 </math>) points; the remaining freedom, if any, is taken up by minimizing the [[Frobenius norm]] of the change to the model's [[Hessian matrix|Hessian]] (with respect to the last iteration).\n\nLINCOA [[software]] was released on December 6, 2013.<ref name=\"repository\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#powell_software |title=A repository of Professor M. J. D. Powell's software |publisher= |date= |accessdate=2014-01-18}}</ref> In the [[Comment (computer programming)|comment]] of the [[source code]],<ref name=\"code\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#lincoa |title=Source code of LINCOA software |publisher= |date= |accessdate=2014-01-18}}</ref> it is said that LINCOA is not suitable for very large numbers of variables (which is typically true for algorithms not using derivatives), but \"a few calculations with 1000 variables, however, have been run successfully overnight, and the performance of LINCOA is satisfactory usually for small numbers of variables.\"<ref name=\"code\" /> It is also pointed out that the author's typical choices of <math> m </math> are <math> n+6 </math> and <math> 2n+1 </math>, the latter \"being recommended for a start\", and \"larger values tend to be highly inefficent when the number of variables is substantial, due to the amount of work and extra difficulty of adjusting more points.\"<ref name=\"code\" />\n\nThe trust region subproblem is solved by the truncated conjugate gradient method described in [[Michael J. D. Powell|Powell]]'s report,<ref name=\"lincoa_tr_subp\">{{Cite report |author= Powell, M. J. D. |date= August 2014 | title=On fast trust region methods for quadratic models with linear constraints | url=http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2014_02.pdf |publisher=Department of Applied Mathematics and Theoretical Physics, Cambridge University |docket=DAMTP 2014/NA02 |accessdate= 2014-08-29}}</ref> but [[Michael J. D. Powell|Powell]] did not write a report on the other details of LINCOA.\n\nThe LINCOA software is distributed under [[GNU Lesser General Public License|The GNU Lesser General Public License]] (LGPL).<ref name=\"code\"/>\n\n==See also==\n* [[TOLMIN (optimization software)|TOLMIN]]\n* [[COBYLA]]\n* [[UOBYQA]]\n* [[NEWUOA]]\n* [[BOBYQA]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://ccpforge.cse.rl.ac.uk/gf/project/powell Optimization software by Professor M. J. D. Powell at CCPForge]\n* [https://sourceforge.net/projects/lincoa-fortran95 M.Powell's f77 code ported to fortran 95, as one f95 module, with a more modern easy interface.]\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Line search",
      "url": "https://en.wikipedia.org/wiki/Line_search",
      "text": "In [[optimization (mathematics)|optimization]], the '''line search''' strategy is one of two basic [[iteration|iterative]] approaches to find a [[maxima and minima|local minimum]] <math>\\mathbf{x}^*</math> of an [[objective function]] <math>f:\\mathbb R^n\\to\\mathbb R</math>. The other approach is [[trust region]].\n\nThe line search approach first finds a descent direction along which the objective function <math>f</math> will be reduced and then computes a step size that determines how far <math>\\mathbf{x}</math> should move along that direction. The descent direction can be computed by various methods, such as [[gradient descent]], [[Newton's method in optimization|Newton's method]] and [[quasi-Newton method]]. The step size can be determined either exactly or inexactly.\n\n==Example use ==\n\nHere is an example gradient method that uses a line search in step 4.\n\n# Set iteration counter <math>\\displaystyle k=0</math>, and make an initial guess <math>\\mathbf{x}_0</math> for the minimum\n# Repeat:\n# &nbsp;&nbsp;&nbsp;&nbsp;Compute a [[descent direction]] <math>\\mathbf{p}_k</math>\n# &nbsp;&nbsp;&nbsp;&nbsp;Choose <math>\\displaystyle \\alpha_k</math> to 'loosely' minimize <math>h(\\alpha)=f(\\mathbf{x}_k+\\alpha\\mathbf{p}_k)</math> over <math>\\alpha\\in\\mathbb R_+</math>\n# &nbsp;&nbsp;&nbsp;&nbsp;Update <math>\\mathbf{x}_{k+1}=\\mathbf{x}_k+\\alpha_k\\mathbf{p}_k</math>, and <math display=\"inline\"> k=k+1</math>\n# &nbsp;&nbsp;&nbsp;&nbsp;Until <math>\\|\\nabla f(\\mathbf{x}_k)\\|</math> < tolerance\n\nAt the line search step (4) the algorithm might either ''exactly'' minimize ''h'', by solving <math>h'(\\alpha_k)=0</math>, or ''loosely'', by asking for a sufficient decrease in ''h''. One example of the former is [[conjugate gradient method]]. The latter is called inexact line search and may be performed in a number of ways, such as a [[backtracking line search]] or using the [[Wolfe conditions]].\n\nLike other optimization methods, line search may be combined with [[simulated annealing]] to allow it to jump over some [[local minimum|local minima]].\n\n==Algorithms==\n\n=== Direct search methods ===\nIn this method, the minimum must first be bracketed, so the algorithm must identify points ''x''<sub>1</sub> and ''x''<sub>2</sub> such that the sought minimum lies between them. The interval is then divided by computing <math>f(x)</math> at two internal points, ''x''<sub>3</sub> and ''x''<sub>4</sub>, and rejecting whichever of the two outer points is not adjacent to that of ''x''<sub>3</sub> and ''x''<sub>4</sub> which has the lowest function value. In subsequent steps, only one extra internal point needs to be calculated. Of the various methods of dividing the interval,<ref>{{Cite book|first1=M. J. |last1=Box|first2=D. |last2=Davies |first3=W. H. |last3=Swann|title=Non-Linear optimisation Techniques|publisher= Oliver & Boyd|year= 1969}}</ref> [[golden section search]] is particularly simple and effective, as the interval proportions are preserved regardless of how the search proceeds:\n:<math>\\frac{1}{\\varphi}(x_2-x_1)=x_4-x_1=x_2-x_3=\\varphi(x_2-x_4)=\\varphi(x_3-x_1)=\\varphi^2(x_4-x_3)</math>\nwhere\n: <math>\\varphi=\\frac{1}{2}(1+\\sqrt 5) \\approx 1.618</math>\n\n==See also==\n* [[Backtracking line search]]\n* [[Secant method]]\n* [[Newton's method in optimization]]\n* [[Pattern search (optimization)]]\n* [[Nelder–Mead method]]\n* [[Golden section search]]\n\n==References==\n<references />\n\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Line Search}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Linear-fractional programming",
      "url": "https://en.wikipedia.org/wiki/Linear-fractional_programming",
      "text": "In [[mathematical optimization]], '''linear-fractional programming''' ('''LFP''') is a generalization of [[linear programming]] (LP). Whereas the objective function in a linear program is a [[linear functional|linear function]], the objective function in a linear-fractional program is a ratio of two linear functions. A linear program can be regarded as a special case of a linear-fractional program in which the denominator is the constant function one.\n\n==Relation to linear programming==\nBoth linear programming and linear-fractional programming represent optimization problems using linear equations and linear inequalities, which for each problem-instance define a [[feasible set]]. Fractional linear programs have a richer set of objective functions. Informally, linear programming computes a policy delivering the best outcome, such as maximum profit or lowest cost. In contrast, a linear-fractional programming is used to achieve the highest ''ratio'' of outcome to cost, the ratio representing the highest efficiency. For example, in the context of LP we maximize the objective function '''profit&nbsp;=&nbsp;income&nbsp;&minus;&nbsp;cost''' and might obtain maximal profit of $100 (=&nbsp;$1100&nbsp;of&nbsp;income&nbsp;&minus;&nbsp;$1000 of cost). Thus, in LP we have an efficiency of $100/$1000&nbsp;=&nbsp;0.1. Using LFP we might obtain an efficiency of $10/$50&nbsp;=&nbsp;0.2 with a profit of only $10, which requires only $50 of investment however.\n\n==Definition==\nFormally, a linear-fractional program is defined as the problem of maximizing (or minimizing) a ratio of [[affine function]]s over a [[polyhedron]],\n:<math>\n\\begin{align}\n\\text{maximize} \\quad & \\frac{\\mathbf{c}^T \\mathbf{x} + \\alpha}{\\mathbf{d}^T \\mathbf{x} + \\beta} \\\\\n\\text{subject to} \\quad & A\\mathbf{x} \\leq \\mathbf{b},\n\\end{align}\n</math>\nwhere <math>\\mathbf{x} \\in \\mathbb{R}^n</math> represents the vector of variables to be determined, <math>\\mathbf{c}, \\mathbf{d} \\in \\mathbb{R}^n</math> and <math>\\mathbf{b} \\in \\mathbb{R}^m</math> are vectors of (known) coefficients, <math>A \\in \\mathbb{R}^{m \\times n}</math> is a (known) matrix of coefficients and <math>\\alpha, \\beta \\in \\mathbb{R}</math> are constants. The constraints have to restrict the [[feasible region]] to <math>\\{\\mathbf{x} | \\mathbf{d}^T\\mathbf{x} + \\beta > 0\\}</math>, i.e. the region on which the denominator is positive.<ref name=\"CC\">{{cite journal |last1=Charnes |first1=A. |last2=Cooper |first2=W. W. |author2-link=William W. Cooper|title=Programming with Linear Fractional Functionals|journal=Naval Research Logistics Quarterly |volume=9 |issue=3–4 |year=1962 |pages=181–186|ref=harv|mr=152370|doi=10.1002/nav.3800090303}}</ref><ref name=\"BV\">{{cite book|title=Convex Optimization|first1=Stephen P.|last1=Boyd|first2=Lieven|last2=Vandenberghe|year=2004|publisher=Cambridge University Press|isbn=978-0-521-83378-3|url=http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf|accessdate=October 15, 2011|page=151}}</ref> Alternatively, the denominator of the objective function has to be strictly negative in the entire feasible region.\n\n==Transformation to a linear program==\nUnder the assumption that the feasible region is non-empty and bounded, the Charnes-Cooper transformation<ref name=\"CC\"/>\n\n:<math>\\mathbf{y} = \\frac{1}{\\mathbf{d}^T \\mathbf{x} + \\beta} \\cdot \\mathbf{x}\\;;\\;\\; t = \\frac{1}{\\mathbf{d}^T \\mathbf{x} + \\beta}</math>\n\ntranslates the linear-fractional program above to the equivalent linear program:\n\n:<math>\n\\begin{align}\n\\text{maximize} \\quad & \\mathbf{c}^T  \\mathbf{y} + \\alpha t \\\\\n\\text{subject to} \\quad & A\\mathbf{y} \\leq \\mathbf{b} t \\\\\n& \\mathbf{d}^T \\mathbf{y} + \\beta t = 1 \\\\\n& t \\geq 0.\n\\end{align}\n</math>\n\nThen the solution for <math>\\mathbf{y}</math> and <math>t </math> yields the solution of the original problem as\n\n:<math>\\mathbf{x}=\\frac{1}{t}\\mathbf{y}.</math>\n\n==Duality==\nLet the [[duality (optimization)|dual variables]] associated with the constraints <math>A\\mathbf{y} - \\mathbf{b} t \\leq \\mathbf{0}</math> and <math>\\mathbf{d}^T \\mathbf{y} + \\beta t - 1 = 0</math> be denoted by <math>\\mathbf{u}</math> and <math>\\lambda</math>, respectively. Then the dual of the LFP above is <ref>{{cite journal|last1=Schaible |first1=Siegfried |title=Parameter-free Convex Equivalent and Dual Programs|journal=Zeitschrift für Operations Research |volume=18 |year=1974 |issue=5 |pages=187–196|ref=harv|doi=10.1007/BF02026600|mr=351464}}</ref><ref>{{cite journal|title=Fractional programming&nbsp;I: Duality |last1=Schaible |first1=Siegfried | journal=Management Science |volume=22 |issue=8 |pages=858–867 |year=1976|jstor=2630017|mr=421679|ref=harv |doi=10.1287/mnsc.22.8.858}}</ref>\n:<math>\n\\begin{align}\n\\text{minimize} \\quad & \\lambda \\\\\n\\text{subject to} \\quad & A^T\\mathbf{u} + \\lambda \\mathbf{d} = \\mathbf{c} \\\\\n& -\\mathbf{b}^T \\mathbf{u} + \\lambda \\beta \\geq \\alpha \\\\\n& \\mathbf{u} \\in \\mathbb{R}_+^m, \\lambda \\in \\mathbb{R},\n\\end{align}\n</math>\nwhich is an LP and which coincides with the dual of the equivalent linear program resulting from the Charnes-Cooper transformation.\n\n==Properties and algorithms==\nThe objective function in a linear-fractional problem is both quasiconcave and [[quasiconvex function|quasiconvex]] (hence quasilinear) with a [[monotonicity|monotone]] property, [[pseudoconvex function|pseudoconvexity]], which is a stronger property than [[quasiconvex function|quasiconvexity]]. A linear-fractional objective function is both pseudoconvex and pseudoconcave, hence [[pseudolinear function|pseudolinear]]. Since an LFP can be transformed to an LP, it can be solved using any LP solution method, such as the [[simplex algorithm]] (of [[George B. Dantzig]]),<ref>\nChapter five: {{cite book| last=Craven|first=B. D.|title=Fractional programming|series=Sigma Series in Applied Mathematics|volume=4|publisher=Heldermann Verlag|location=Berlin|year=1988|pages=145|isbn=978-3-88538-404-5 |mr=949209|ref=harv}}</ref><ref>{{cite journal| last1=Kruk | first1=Serge|last2=Wolkowicz|first2=Henry|title=Pseudolinear programming |journal=[[SIAM Review]]|volume=41 |year=1999 |issue=4 |pages=795–805 |mr=1723002|jstor=2653207|doi=10.1137/S0036144598335259|ref=harv|citeseerx=10.1.1.53.7355}}\n</ref><ref>{{cite journal | last1=Mathis|first1=Frank H.|last2=Mathis|first2=Lenora Jane|title=A nonlinear programming algorithm for hospital management |journal=[[SIAM Review]]|volume=37 |year=1995 |issue=2 |pages=230–234|mr=1343214|jstor=2132826|doi=10.1137/1037046|ref=harv}}\n</ref><ref>{{harvtxt|Murty|1983|loc=Chapter&nbsp;3.20 (pp.&nbsp;160–164) and pp.&nbsp;168 and&nbsp;179}}</ref> the [[criss-cross algorithm]],<ref>{{cite journal|title=The finite criss-cross method for hyperbolic programming|journal=European Journal of Operational Research|volume=114|issue=1|\npages=198–214|year=1999|<!-- issn=0377-2217 -->|doi=10.1016/S0377-2217(98)00049-6|url=http://www.sciencedirect.com/science/article/B6VCT-3W3DFHB-M/2/4b0e2fcfc2a71e8c14c61640b32e805a|first1=Tibor|last1=Illés|first2=Ákos|last2=Szirmai|first3=Tamás|last3=Terlaky|ref=harv|zbl=0953.90055|id=[http://www.cas.mcmaster.ca/~terlaky/files/dut-twi-96-103.ps.gz Postscript preprint]|citeseerx=10.1.1.36.7090}}</ref> or [[interior-point method]]s.\n\n==Notes==\n<references />\n\n==References==\n*{{cite book| last=Craven|first=B. D.|title=Fractional programming|series=Sigma Series in Applied Mathematics|volume=4|publisher=Heldermann Verlag|location=Berlin|year=1988|pages=145|isbn=978-3-88538-404-5 |mr=949209}}\n*{{cite journal|title=The finite criss-cross method for hyperbolic programming|journal=European Journal of Operational Research|volume=114|issue=1|\npages=198–214|year=1999|<!-- issn=0377-2217 -->|doi=10.1016/S0377-2217(98)00049-6|url=http://www.sciencedirect.com/science/article/B6VCT-3W3DFHB-M/2/4b0e2fcfc2a71e8c14c61640b32e805a|first1=Tibor|last1=Illés|first2=Ákos|last2=Szirmai|first3=Tamás|last3=Terlaky|ref=harv|zbl=0953.90055|id=[http://www.cas.mcmaster.ca/~terlaky/files/dut-twi-96-103.ps.gz Postscript preprint]|citeseerx=10.1.1.36.7090}}\n*{{cite journal|last1=Kruk|first1=Serge|last2=Wolkowicz|first2=Henry|title=Pseudolinear programming|journal=[[SIAM Review]]|volume=41 |year=1999 |issue=4|pages=795–805|mr=1723002 | jstor = 2653207|doi=10.1137/S0036144598335259}}\n*{{cite journal | last1=Mathis|first1=Frank H.|last2=Mathis|first2=Lenora Jane|title=A nonlinear programming algorithm for hospital management|journal=[[SIAM Review]]|volume=37 |year=1995 | issue=2|pages=230–234|mr=1343214 | jstor = 2132826|doi=10.1137/1037046}}\n*{{cite book|last=Murty|first=Katta&nbsp;G.|authorlink=Katta G. Murty|chapter=3.10 Fractional programming (pp. 160–164)|title=Linear programming|publisher=John Wiley & Sons, Inc.|location=New York|year=1983|pages=xix+482|isbn=978-0-471-09725-9|mr=720547|ref=harv}}\n\n==Further reading==\n*{{cite book|first=E. B.|last=Bajalinov|title=Linear-Fractional Programming: Theory, Methods, Applications and Software| publisher=Kluwer Academic Publishers|location=Boston|year=2003}}\n*{{cite book|last=Barros|first=Ana Isabel|title=Discrete and fractional programming techniques for location models|series=Combinatorial Optimization|volume=3|publisher=Kluwer Academic Publishers|location=Dordrecht|year=1998|pages=xviii+178|isbn=978-0-7923-5002-6|mr=1626973}}\n*{{cite book|last=Martos|first=Béla|title=Nonlinear programming: Theory and methods|publisher=North-Holland Publishing Co.|location=Amsterdam-Oxford|year=1975|pages=279|isbn=978-0-7204-2817-9|mr=496692}}\n*{{cite book|last=Schaible|first=S.|chapter=Fractional programming|pages=495–608|mr=1377091|title=Handbook of global optimization|editor=Reiner Horst and Panos M. Pardalos|\nseries=Nonconvex optimization and its applications|volume=2|publisher=Kluwer Academic Publishers|location=Dordrecht|year=1995|isbn=978-0-7923-3120-9}}\n*{{cite book | last=Stancu-Minasian | first=I. M.| title=Fractional programming: Theory, methods and applications | others=Translated by Victor Giurgiutiu from the 1992 Romanian | series=Mathematics and its applications|volume=409|publisher=Kluwer Academic Publishers Group | location=Dordrecht | year=1997 | pages=viii+418 | isbn=978-0-7923-4580-0 | mr=1472981 }}\n\n==Software==\n*[http://zeus.nyf.hu/~bajalinov/WinGulf/wingulf.html WinGULF] – educational interactive linear and linear-fractional programming solver with a lot of special options (pivoting, pricing, branching variables, etc.)\n*JOptimizer – Java convex optimization library (open source)\n\n{{DEFAULTSORT:Linear-Fractional Programming}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Linear programming]]\n[[Category:Generalized convexity]]"
    },
    {
      "title": "Lloyd's algorithm",
      "url": "https://en.wikipedia.org/wiki/Lloyd%27s_algorithm",
      "text": "{{multiple image\n | align     = right\n | direction = vertical\n | header    = Example of Lloyd's algorithm.  The Voronoi diagram of the current points at each iteration is shown.  The plus signs denote the centroids of the Voronoi cells.\n | header_align = left\n | header_background = \n | footer    = In the last image, the points are very near the centroids of the Voronoi cells.  A centroidal Voronoi tessellation has been found.\n | footer_align = \n | footer_background = \n | width     = 200\n\n | image1    = LloydsMethod1.svg\n | alt1      = Lloyd's method, iteration 1\n | caption1  = Iteration 1\n\n | image2    = LloydsMethod2.svg\n | alt2      = Lloyd's method, iteration 2\n | caption2  = Iteration 2\n\n | image3    = LloydsMethod3.svg\n | alt3      = Lloyd's method, iteration 3\n | caption3  = Iteration 3\n\n | image4    = LloydsMethod15.svg\n | alt4      = Lloyd's method, iteration 15\n | caption4  = Iteration 15\n}}\n\nIn [[computer science]] and [[electrical engineering]], '''Lloyd's algorithm''', also known as '''Voronoi iteration''' or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of [[Euclidean space]]s and partitions of these subsets into well-shaped and uniformly sized convex cells.<ref name=\"l82\"/>  Like the closely related [[k-means clustering|''k''-means clustering]] algorithm, it repeatedly finds the [[centroid]] of each set in the partition and then re-partitions the input according to which of these centroids is closest. However, Lloyd's algorithm differs from ''k''-means clustering in that its input is a continuous geometric region rather than a discrete set of points. Thus, when re-partitioning the input, Lloyd's algorithm uses [[Voronoi diagram]]s rather than simply determining the nearest center to each of a finite set of points as the ''k''-means algorithm does.\n\nAlthough the algorithm may be applied most directly to the [[Euclidean plane]], similar algorithms may also be applied to higher-dimensional spaces or to spaces with other [[Non-Euclidean geometry|non-Euclidean]] metrics. Lloyd's algorithm can be used to construct close approximations to [[centroidal Voronoi tessellation]]s of the input,<ref name=\"dfg99\"/> which can be used for [[Quantization (signal processing)|quantization]], [[dithering]], and [[stippling]]. Other applications of Lloyd's algorithm include smoothing of [[triangle mesh]]es in the [[finite element method]].\n\n==Algorithm description==\nLloyd's algorithm starts by an initial placement of some number ''k'' of point sites in the input domain. In mesh-smoothing applications, these would be the vertices of the mesh to be smoothed; in other applications they may be placed at random or by intersecting a uniform triangular mesh of the appropriate size with the input domain.\n\nIt then repeatedly executes the following relaxation step:\n* The [[Voronoi diagram]] of the ''k'' sites is computed.\n* Each cell of the Voronoi diagram is integrated, and the centroid is computed.\n* Each site is then moved to the centroid of its Voronoi cell.\n\n==Integration and centroid computation==\nBecause Voronoi diagram construction algorithms can be highly non-trivial, especially for inputs of dimension higher than two, the steps of calculating this diagram and finding the exact centroids of its cells may be replaced by an approximation.\n\n===Approximation===\nA common simplification is to employ a suitable discretization of space like a fine pixel-grid, e.g. the texture [[Z-buffering|buffer]] in graphics hardware. Cells are materialized as pixels, labeled with their corresponding site-ID. A cell's new center is approximated by averaging the positions of all pixels assigned with the same label.\nAlternatively, [[Monte Carlo methods]] may be used, in which random sample points are generated according to some fixed underlying probability distribution, assigned to the closest site, and averaged to approximate the centroid for each site.\n\n===Exact computation===\nAlthough embedding in other spaces is also possible, this elaboration assumes [[Euclidean distance|Euclidean space]] using the [[Lp space|''L<sup>2</sup>'' norm]] and discusses the two most relevant scenarios, which are two, and respectively three dimensions.\n\nSince a Voronoi cell is of convex shape and always encloses its site, there exist trivial decompositions into easy integratable simplices:\n* In two dimensions, the edges of the polygonal cell are connected with its site, creating an umbrella-shaped set of triangles.\n* In three dimensions, the cell is enclosed by several planar polygons which have to be triangulated first:\n** Compute a center for the polygon face, e.g. the average of all its vertices.\n** Connecting the vertices of a polygon face with its center gives a planar umbrella-shaped triangulation.\n** Trivially, a set of [[Tetrahedron|tetrahedra]] is obtained by connecting triangles of the cell's hull with the cell's site.\n\nIntegration of a cell and computation of its centroid (center of mass) is now given as a weighted combination of its simplices' [[Circumscribed circle|circumcenters]] (in the following called <math display=\"inline\">\\mathbf{c}_i</math>).\n* Two dimensions:\n** For a triangle the circumcircle and its center are unique and can be easily computed, e.g. using [[Circumscribed circle#Circumcenter coordinates|cartesian coordinates]].\n** Weighting computes as simplex-to-cell '''area''' ratios.\n* Three dimensions:\n** The [[Tetrahedron#Centroid|centroid of a tetrahedron]] is found as the intersection of three bisector planes and can be expressed as a matrix-vector product.\n** Weighting computes as simplex-to-cell '''volume''' ratios.\n\nFor a 2D cell with {{math|''n''}} triangular simplices and an accumulated area <math display=\"inline\">A_C = \\sum_{i=0}^n a_i</math> (where <math display=\"inline\">a_i</math> is the [[Triangle#Computing the area of a triangle|area of a triangle]] simplex), the new cell centroid computes as:\n:<math>\nC = \\frac{1}{A_C}\\sum_{i=0}^{n}\\mathbf{c}_i a_i\n</math>\nAnalogously, for a 3D cell with a volume of <math display=\"inline\">V_C = \\sum_{i=0}^n v_i</math> (where <math display=\"inline\">v_i</math> is the [[Tetrahedron#Volume|volume of a tetrahedron]] simplex), the centroid computes as:\n:<math>\nC = \\frac{1}{V_C}\\sum_{i=0}^{n}\\mathbf{c}_i v_i\n</math>\n\n==Convergence==\nEach time a relaxation step is performed, the points are left in a slightly more even distribution: closely spaced points move farther apart, and widely spaced points move closer together. In one dimension, this algorithm has been shown to converge to a centroidal Voronoi diagram, also named a [[centroidal Voronoi tessellation]].<ref name=\"dej06\"/> In higher dimensions, some slightly weaker convergence results are known.<ref name=\"sg86\"/><ref name=\"ejr09\"/>\n\nThe algorithm converges slowly or, due to limitations in numerical precision, may not converge. Therefore, real-world applications of Lloyd's algorithm typically stop once the distribution is \"good enough.\" One common termination criterion is to stop when the maximum distance moved by any site in an iteration falls below a preset threshold. Convergence can be accelerated by over-relaxing the points, which is done by moving each point '''ω''' times the distance to the center of mass, typically using a value slightly less than 2 for '''ω'''.<ref>Xiao, Xiao. \"Over-relaxation Lloyd method for computing centroidal Voronoi tessellations.\" (2010).</ref>\n\n==Applications==\nLloyd's method was originally used for scalar quantization, but it is clear that the method extends for vector quantization as well. As such, it is extensively used in [[data compression]] techniques in [[information theory]]. Lloyd's method is used in computer graphics because the resulting distribution has [[blue noise]] characteristics (see also [[Colors of noise]]), meaning there are few low-frequency components that could be interpreted as artifacts. It is particularly well-suited to picking sample positions for [[dithering]]. Lloyd's algorithm is also used to generate dot drawings in the style of [[stippling]].<ref name=\"dhos00\"/> In this application, the centroids can be weighted based on a reference image to produce stipple illustrations matching an input image.<ref name=\"s02\"/>\n\nIn the [[finite element method]], an input domain with a complex geometry is partitioned into elements with simpler shapes; for instance, two-dimensional domains (either subsets of the Euclidean plane or surfaces in three dimensions) are often partitioned into triangles. It is important for the convergence of the finite element methods that these elements be well shaped; in the case of triangles, often elements that are nearly equilateral triangles are preferred. Lloyd's algorithm \ncan be used to smooth a mesh generated by some other algorithm, moving its vertices and changing the connection pattern among its elements in order to produce triangles that are more closely equilateral.<ref name=\"dg02\"/> These applications typically use a smaller number of iterations of Lloyd's algorithm, stopping it to convergence, in order to preserve other features of the mesh such as differences in element size in different parts of the mesh. In contrast to a different smoothing method, [[Laplacian smoothing]] (in which mesh vertices are moved to the average of their neighbors' positions), Lloyd's algorithm can change the topology of the mesh, leading to more nearly equilateral elements as well as avoiding the problems with tangling that can arise with Laplacian smoothing. However, Laplacian smoothing can be applied more generally to meshes with non-triangular elements.\n\n==Different distances==\nLloyd's algorithm is usually used in a [[Euclidean space]]. The Euclidean distance plays two roles in the algorithm: it is used to define the Voronoi cells, but it also corresponds to the choice of the centroid as the representative point of each cell, since the centroid is the point that minimizes the average squared Euclidean distance to the points in its cell. Alternative distances, and alternative central points than the centroid, may be used instead. For example, {{harvtxt|Hausner|2001}} used a variant of the [[Manhattan metric]] (with locally varying orientations) to find a tiling of an image by approximately square tiles whose orientation aligns with features of an image, which he used to simulate the construction of tiled [[mosaic]]s.<ref name=\"h01\"/> In this application, despite varying the metric, Hausner continued to use centroids as the representative points of their Voronoi cells. However, for metrics that differ more significantly from Euclidean, it may be appropriate to choose the minimizer of average squared distance as the representative point, in place of the centroid.<ref name=\"dew10\"/>\n\n== See also ==\n* The [[Linde–Buzo–Gray algorithm]], a generalization of this algorithm for vector quantization\n* [[Farthest-first traversal]], a different method for generating evenly spaced points in geometric spaces\n* [[Mean shift]], a related method for finding maxima of a density function\n* [[K-means++]]\n\n==References==\n{{reflist|refs=\n<ref name=\"dhos00\">{{citation\n | last1 = Deussen | first1 = Oliver\n | last2 = Hiller | first2 = Stefan\n | last3 = van Overveld | first3 = Cornelius\n | last4 = Strothotte | first4 = Thomas\n | doi = 10.1111/1467-8659.00396\n | id = Proceedings of [[Eurographics]]\n | issue = 3\n | journal = Computer Graphics Forum\n | pages = 41–50\n | title = Floating points: a method for computing stipple drawings\n | volume = 19\n | year = 2000}}.</ref>\n<ref name=\"dew10\">{{citation\n | last1 = Dickerson | first1 = Matthew T. | author1-link = Matthew T. Dickerson\n | last2 = Eppstein | first2 = David | author2-link = David Eppstein\n | last3 = Wortman | first3 = Kevin A.\n | arxiv = 0812.0607\n | contribution = Planar Voronoi diagrams for sums of convex functions, smoothed distance and dilation\n | doi = 10.1109/ISVD.2010.12\n | pages = 13–22\n | title = Proc. 7th International Symposium on Voronoi Diagrams in Science and Engineering (ISVD 2010)\n | year = 2010}}.</ref>\n<ref name=\"dej06\">{{citation\n | last1 = Du | first1 = Qiang\n | last2 = Emelianenko | first2 = Maria\n | last3 = Ju | first3 = Lili\n | doi = 10.1137/040617364\n | journal = SIAM Journal on Numerical Analysis\n | pages = 102–119\n | title = Convergence of the Lloyd algorithm for computing centroidal Voronoi tessellations\n | volume = 44\n | year = 2006| citeseerx = 10.1.1.591.9903}}.</ref>\n<ref name=\"dfg99\">{{citation\n | last1 = Du | first1 = Qiang\n | last2 = Faber | first2 = Vance\n | last3 = Gunzburger | first3 = Max\n | doi = 10.1137/S0036144599352836\n | issue = 4\n | journal = SIAM Review\n | pages = 637–676\n | title = Centroidal Voronoi tessellations: applications and algorithms\n | volume = 41\n | year = 1999| bibcode = 1999SIAMR..41..637D}}.</ref>\n<ref name=\"dg02\">{{citation\n | last1 = Du | first1 = Qiang\n | last2 = Gunzburger | first2 = Max\n | doi = 10.1016/S0096-3003(01)00260-0\n | issue = 2–3\n | journal = Applied Mathematics and Computation\n | pages = 591–607\n | title = Grid generation and optimization based on centroidal Voronoi tessellations\n | volume = 133\n | year = 2002}}.</ref>\n<ref name=\"ejr09\">{{citation\n | last1 = Emelianenko | first1 = Maria\n | last2 = Ju | first2 = Lili\n | last3 = Rand | first3 = Alexander\n | journal = SIAM Journal on Numerical Analysis\n | pages = 1423–1441\n | title = Nondegeneracy and Weak Global Convergence of the Lloyd Algorithm in '''R'''<sup>d</sup>\n | volume = 46\n | year = 2009 | doi=10.1137/070691334}}.</ref>\n<ref name=\"l82\">{{citation\n | last = Lloyd | first = Stuart P.\n | doi = 10.1109/TIT.1982.1056489\n | issue = 2\n | journal = [[IEEE Transactions on Information Theory]]\n | pages = 129–137\n | title = Least squares quantization in PCM\n | volume = 28\n | year = 1982}}.</ref>\n<ref name=\"sg86\">{{citation\n | last1 = Sabin | first1 = M. J.\n | last2 = Gray | first2 = R. M.\n | doi = 10.1109/TIT.1986.1057168\n | issue = 2\n | journal = [[IEEE Transactions on Information Theory]]\n | pages = 148–155\n | title = Global convergence and empirical consistency of the generalized Lloyd algorithm\n | volume = 32\n | year = 1986}}.</ref>\n<ref name=\"s02\">{{citation\n | last = Secord | first = Adrian\n | contribution = Weighted Voronoi stippling\n | doi = 10.1145/508530.508537\n | pages = 37–43\n | publisher = [[ACM SIGGRAPH]]\n | title = Proceedings of the Symposium on Non-Photorealistic Animation and Rendering (NPAR)\n | year = 2002}}.</ref>\n<ref name=\"h01\">{{citation\n | last = Hausner| first = Alejo\n | doi = 10.1145/383259.383327\n | publisher = [[ACM SIGGRAPH]]\n | title = Proceedings of the 28th annual conference on Computer graphics and interactive techniques \n | pages = 573–580 \n | contribution = Simulating decorative mosaics\n | year = 2001}}.</ref>\n}}\n\n==External links==\n* [http://demogng.de/js/demogng.html?model=LBG&showAutoRestart&showVoronoi DemoGNG.js] Graphical Javascript simulator for LBG algorithm and other models, includes display of Voronoi regions\n\n[[Category:Geometric algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Local search (optimization)",
      "url": "https://en.wikipedia.org/wiki/Local_search_%28optimization%29",
      "text": "{{more footnotes|date=May 2015}}\n\nIn [[computer science]], '''local search''' is a [[heuristic]] method for solving computationally hard [[Mathematical optimization|optimization]] problems. Local search can be used on problems that can be formulated as finding a solution maximizing a criterion among a number of [[candidate solution]]s. Local search algorithms move from solution to solution in the space of candidate solutions (the ''search space'') by applying local changes, until a solution deemed optimal is found or a time bound is elapsed.\n\nLocal search algorithms are widely applied to numerous hard computational problems, including problems from [[computer science]] (particularly [[artificial intelligence]]), [[mathematics]], [[operations research]], [[engineering]], and [[bioinformatics]]. Examples of local search algorithms are [[WalkSAT]], the [[2-opt|2-opt algorithm for the Traveling Salesman Problem]] and the [[Metropolis–Hastings algorithm]].<ref>{{cite web|url=https://www.cs.princeton.edu/~wayne/kleinberg-tardos/pdf/12LocalSearch.pdf|title=12LocalSearch.key}}</ref>\n\n== Examples ==\nSome problems where local search has been applied are:\n\n# The [[vertex cover problem]], in which a solution is a [[vertex cover]] of a [[Graph (discrete mathematics)|graph]], and the target is to find a solution with a minimal number of nodes\n# The [[travelling salesman problem]], in which a solution is a [[Cycle (graph theory)|cycle]] containing all nodes of the graph and the target is to minimize the total length of the cycle\n# The [[boolean satisfiability problem]], in which a candidate solution is a truth assignment, and the target is to maximize the number of [[Clause (logic)|clauses]] satisfied by the assignment; in this case, the final solution is of use only if it satisfies ''all'' clauses\n# The [[nurse scheduling problem]] where a solution is an assignment of nurses to [[Shift work|shifts]] which satisfies all established [[Constraint satisfaction|constraints]]\n# The [[k-medoid]] clustering problem and other related [[facility location]] problems for which local search offers the best known approximation ratios from a worst-case perspective\n# The Hopfield Neural Networks problem for which finding stable configurations in [[Hopfield network]].\n\n== Description ==\nMost problems can be formulated in terms of search space and target in several different manners. For example, for the travelling salesman problem  a solution can be a cycle and the criterion to maximize is a combination of the number of nodes and the length of the cycle. But a solution can also be a path, and being a cycle is part of the target.\n\nA local search algorithm starts from a candidate solution and then [[iterative method|iteratively]] moves to a [[Neighbourhood (mathematics)|neighbor]] solution. This is only possible if a [[neighborhood relation]] is defined on the search space. As an example, the neighborhood of a vertex cover is another vertex cover only differing by one node. For boolean satisfiability, the neighbors of a truth assignment are usually the truth assignments only differing from it by the evaluation of a variable. The same problem may have multiple different neighborhoods defined on it; local optimization with neighborhoods that involve changing up to \n''k'' components of the solution is often referred to as '''k-opt'''.\n\nTypically, every candidate solution has more than one neighbor solution; the choice of which one to move to is taken using only information about the solutions in the [[Neighbourhood (mathematics)|neighborhood]] of the current one, hence the name ''local'' search. When the choice of the neighbor solution is done by taking the one locally maximizing the criterion, the metaheuristic takes the name [[hill climbing]].\nWhen no improving configurations are present in the neighborhood, local search is stuck at a [[Local optimum|locally optimal point]].\nThis local-optima problem can be cured by using restarts (repeated local search with different initial conditions), or more complex schemes based on iterations, like [[iterated local search]], on memory, like reactive search optimization, on memory-less stochastic modifications, like [[simulated annealing]].\n\nTermination of local search can be based on a time bound. Another common choice is to terminate when the best solution found by the algorithm has not been improved in a given number of steps. Local search is an [[anytime algorithm]]:\nit can return a valid solution even if it's interrupted at any time before it ends. \nLocal search algorithms are typically [[approximation algorithm|approximation]] or [[incomplete algorithm]]s, as the search may stop even if the best solution found by the algorithm is not optimal. This can happen even if termination is due to the impossibility of improving the solution, as the optimal solution can lie far from the neighborhood of the solutions crossed by the algorithms.\n\nFor specific problems it is possible to devise neighborhoods which are very large, possibly exponentially sized.  If the best solution within the neighborhood can be found efficiently, such algorithms are referred to as [[very large-scale neighborhood search]] algorithms.\n\n==See also==\nLocal search is a sub-field of:\n* [[Metaheuristic]]s\n* [[Stochastic optimization]]\n* [[Mathematical optimization|Optimization]]\n\nFields within local search include:\n* [[Hill climbing]]\n* [[Simulated annealing]] (suited for either local or global search)\n* [[Tabu search]]\n* [[Late acceptance hill climbing]]\n* Reactive search optimization (combining [[machine learning]] and local search heuristics)\n\n===Real-valued search-spaces===\nSeveral methods exist for performing local search of [[real number|real-valued]] search-spaces:\n* [[Luus–Jaakola]] searches locally using a [[Uniform distribution (continuous)|uniform distribution]] and an exponentially decreasing search-range.\n* [[Random optimization]] searches locally using a [[normal distribution]].\n* [[Random search]] searches locally by sampling a [[hypersphere]] surrounding the current position.\n* [[Pattern search (optimization)|Pattern search]] takes steps along the axes of the search-space using exponentially decreasing step sizes.\n\n== References ==\n{{Reflist}}\n\n==Bibliography==\n*{{cite book\n |title       = Reactive Search and Intelligent Optimization\n |last        = Battiti\n |first       = Roberto\n |authorlink  = \n |author2     = Mauro Brunato\n |author3     = Franco Mascia\n |url         = http://reactive-search.org/thebook\n |year        = 2008\n |publisher   = [[Springer Verlag]]\n |location    = \n |isbn        = 978-0-387-09623-0\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20120316104607/http://reactive-search.org/thebook\n |archivedate = 2012-03-16\n |df          = \n}}\n*[[Holger H. Hoos|Hoos, H.H.]] and Stutzle, T. (2005) Stochastic Local Search: Foundations and Applications, Morgan Kaufmann.\n*Vijay Arya and Naveen Garg and Rohit Khandekar and Adam Meyerson and Kamesh Munagala and Vinayaka Pandit, (2004): [http://www.cse.iitd.ernet.in/~pandit/lsearchSICOMP.pdf Local Search Heuristics for ''k''-Median and Facility Location Problems], SIAM Journal of Computing 33(3).\n*[[Juraj Hromkovič]]: Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics (Springer)\n* Wil Michiels, Emile Aarts, Jan Korst: Theoretical Aspects of Local Search (Springer)\n{{Major subfields of optimization}}\n\n[[Category:Metaheuristics]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Luus–Jaakola",
      "url": "https://en.wikipedia.org/wiki/Luus%E2%80%93Jaakola",
      "text": "In [[computational engineering]], '''Luus–Jaakola (LJ)''' denotes a [[heuristic algorithm|heuristic]] for [[global optimization|global]] [[optimization (mathematics)|optimization]] of a real-valued function.<ref name=\"nair79convergence\"/> In engineering use, LJ is not an [[algorithm]] that terminates with an optimal solution; nor is it an [[iterative method]] that generates a sequence of points that converges to an optimal solution (when one exists). However, when applied to a twice continuously differentiable function, the LJ heuristic is a proper iterative method, that generates a sequence that has a convergent subsequence; for this class of problems, [[Newton's method]] is recommended and enjoys a quadratic rate of convergence, while no convergence rate analysis has been given for the LJ heuristic.<ref name=\"nair79convergence\"/> In practice, the LJ heuristic has been recommended for functions that need be neither [[convex function|convex]] nor [[Differentiable function|differentiable]] nor [[Lipschitz continuity|locally Lipschitz]]: The LJ heuristic does not use a [[gradient]] or [[subgradient]] when one be available, which allows its application to non-differentiable and non-convex problems.\n\nProposed by Luus and Jaakola,<ref name=luus73optimization/> LJ generates a sequence of iterates. The next iterate is selected from a sample from a neighborhood of the current position using a [[uniform distribution (continuous)|uniform distribution]]. With each iteration, the neighborhood decreases, which forces a subsequence of iterates to converge to a cluster point.<ref name=\"nair79convergence\"/>\n\nLuus has applied LJ  in [[optimal control]],<ref name=bojkov93application/> <ref name=Hein/> [[transformer|transformer design]],<ref name=spaans92importance/> [[metallurgy|metallurgical processes]],<ref name=papangelakis93reactor/> and [[chemical engineering]].<ref name=lee99phase/>\n\n== Motivation ==\n\n[[Image:Random sampling unimodal function (1).JPG|thumb|When the current position ''x'' is far from the optimum the probability is 1/2 for finding an improvement through uniform random sampling.]]\n\n[[Image:Random sampling unimodal function (2).JPG|thumb|As we approach the optimum the probability of finding further improvements through uniform sampling decreases towards zero if the sampling-range ''d'' is kept fixed.]]\n\nAt each step, the LJ heuristic maintains a box from which it samples points randomly, using a uniform distribution on the box. For a [[unimodal function]], the probability of reducing the objective function decreases as the box approach a minimum. The picture displays a one-dimensional example.\n\n== Heuristic ==\n\nLet ''f'':&nbsp;ℝ<sup>''n''</sup>&nbsp;→ ℝ be the fitness or cost function which must be minimized. Let '''x'''&nbsp;∈&nbsp;ℝ<sup>''n''</sup> designate a position or candidate solution in the search-space. The LJ heuristic iterates the following steps:\n\n* Initialize '''x'''&nbsp;~&nbsp;''U''('''b<sub>lo</sub>''','''b<sub>up</sub>''') with a random [[uniform distribution (continuous)|uniform]] position in the search-space, where '''b<sub>lo</sub>''' and '''b<sub>up</sub>''' are the lower and upper boundaries, respectively.\n* Set the initial sampling range to cover the entire search-space (or a part of it): '''d'''&nbsp;=&nbsp;'''b<sub>up</sub>'''&nbsp;&minus;&nbsp;'''b<sub>lo</sub>'''\n* Until a termination criterion is met (e.g. number of iterations performed, or adequate fitness reached), repeat the following:\n** Pick a random vector '''a'''&nbsp;~&nbsp;''U''(&minus;'''d''',&nbsp;'''d''')\n** Add this to the current position '''x''' to create the new potential position '''y'''&nbsp;=&nbsp;'''x'''&nbsp;+&nbsp;'''a'''\n** If (''f''('''y''')&nbsp;<&nbsp;''f''('''x''')) then move to the new position by setting '''x'''&nbsp;=&nbsp;'''y''', otherwise decrease the sampling-range: '''d'''&nbsp;=&nbsp;''0.95''&nbsp;'''d'''\n* Now '''x''' holds the best-found position.\n\n== Variations ==\n\nLuus notes that ARS (Adaptive Random Search) algorithms proposed to date differ in regard to many aspects.<ref>{{cite book |last1=Luus |first1=Rein |editor1-last=Rangalah |editor1-first=Gade Pandu |title=Stochastic Global Optimization: Techniques and Applications in Chemical Engineering |date=2010 |publisher=World Scientific Pub Co Inc |isbn=978-9814299206 |pages=17-56 |chapter=Formulation and Illustration of Luus-Jaakola Optimization Procedure}}</ref> \n* Procedure of generating random trial points.\n* Number of internal loops (NIL, the number of random search points in each cycle).\n* Number of cycles (NEL, number of external loops).\n* Contraction coefficient of the search region size. (Some example values are 0.95 to 0.60.)\n** Whether the region reduction rate is the same for all variables or a different rate for each variable (called the M-LJ algorithm).\n** Whether the region reduction rate is a constant or follows another distribution (e.g. Gaussian).\n* Whether to incorporate a line search.\n* Whether to consider constraints of the random points as acceptance criteria, or to incorporate a quadratic penalty.\n\n== Convergence ==\n\nNair proved a convergence analysis. For twice continuously differentiable functions, the LJ heuristic generates a sequence of iterates having a convergent subsequence.<ref name=nair79convergence/>  For this class of problems, Newton's method is the usual optimization method, and it has [[quadratic convergence]] (''regardless of the dimension'' of the space, which can be a [[Banach space]], according to [[Kantorovich]]'s analysis).\n\nThe worst-case complexity of minimization on the class of unimodal functions grows exponentially in the dimension of the problem, according to the analysis of Yudin and Nemirovsky, however. The Yudin-Nemirovsky analysis implies that no method can be fast on high-dimensional problems that lack convexity:\n<blockquote>\"The catastrophic growth [in the number of iterations needed to reach an approximate solution of a given accuracy] as [the number of dimensions increases to infinity] shows that it is meaningless to pose the question of constructing universal methods of solving ... problems of any appreciable dimensionality 'generally'. It is interesting to note that the same [conclusion] holds for ... problems generated by uni-extremal [that is, unimodal] (but not convex) functions.\"<ref>{{harvtxt|Nemirovsky|Yudin|1983|p=7}}<p>Page 7 summarizes the later discussion of {{harvtxt|Nemirovksy|Yudin|1983|pp=36–39}}: \n{{cite book|mr=702836|last1=Nemirovsky|first1=A. S.|last2=Yudin|first2=D. B.|title=Problem complexity and method efficiency in optimization|edition=Translated by E. R. Dawson from the (1979) Russian (Moscow: Nauka)|series=Wiley-Interscience Series in Discrete Mathematics|publisher=John Wiley & Sons, Inc.|location=New York|year=1983|pages=xv+388|isbn=0-471-10345-4|ref=harv}}\n</ref></blockquote>\nWhen applied to twice continuously differentiable problems, the LJ heuristic's rate of convergence decreases as the number of dimensions increases.<ref>{{harvtxt|Nair|1979|p=433}}</ref>\n\n== See also ==\n\n* [[Random optimization]] is a related family of optimization methods that sample from general distributions, for example the uniform distribution.\n* [[Random search]] is a related family of optimization methods that sample from general distributions, for example, a uniform distribution on the [[unit sphere|unit]] [[hypersphere|sphere]].\n* [[Pattern search (optimization)|Pattern search]] are used on noisy observations, especially in [[response surface methodology]] in [[chemical engineering]]. They do not require users to program gradients or hessians.\n\n== References ==\n\n{{reflist|refs=\n<ref name=luus73optimization>\n{{cite journal\n|last1=Luus\n|first1=R.\n|last2=Jaakola\n|first2=T.H.I.\n|title=Optimization by direct search and systematic reduction of the size of search region\n|journal=American Institute of Chemical Engineers Journal (AIChE)\n|year=1973\n|volume=19\n|number=4\n|pages=760&ndash;766\n|doi=10.1002/aic.690190413\n}}\n</ref>\n\n<ref name=bojkov93application>\n{{cite journal\n|last1=Bojkov\n|first1=R.\n|last2=Hansel\n|first2=B.\n|last3=Luus\n|first3=R.\n|title=Application of direct search optimization to optimal control problems\n|journal=Hungarian Journal of Industrial Chemistry\n|year=1993\n|volume=21\n|pages=177&ndash;185\n}}\n</ref>\n\n<ref name=Hein>\n{{cite book \n|last1=Heinänen \n|first1=Eero \n|title=A Method for automatic tuning of PID controller following Luus-Jaakola optimization \n|date=October 2018 \n|publisher=Tampere University of Technology \n|location=Tampere, Finland \n|edition=Master's Thesis \n|url=https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/26690/Heinanen.pdf \n|accessdate=Feb 1, 2019 \n|ref=Hein}}\n</ref>\n\n<ref name=spaans92importance>\n{{cite journal\n|last1=Spaans\n|first1=R.\n|last2=Luus\n|first2=R.\n|title=Importance of search-domain reduction in random optimization\n|journal=Journal of Optimization Theory and Applications\n|year=1992\n|volume=75\n|pages=635&ndash;638\n|doi=10.1007/BF00940497\n|mr=1194836\n}}</ref>\n\n<ref name=lee99phase>\n{{cite journal\n|last1=Lee\n|first1=Y.P.\n|last2=Rangaiah\n|first2=G.P.\n|last3=Luus\n|first3=R.\n|title=Phase and chemical equilibrium calculations by direct search optimization\n|journal=Computers & Chemical Engineering\n|year=1999\n|volume=23\n|number=9\n|pages=1183&ndash;1191\n|doi=10.1016/s0098-1354(99)00283-5\n}}\n</ref>\n\n<ref name=papangelakis93reactor>\n{{cite conference\n|last1=Papangelakis\n|first1=V.G.\n|last2=Luus\n|first2=R.\n|title=Reactor optimization in the pressure oxidization process\n|booktitle=Proc. Int. Symp. on Modelling, Simulation and Control of Metallurgical Processes\n|year=1993\n|pages=159&ndash;171\n}}\n</ref>\n\n<ref name=nair79convergence>\n{{cite journal\n|last=Nair\n|first=G. Gopalakrishnan\n|title=On the convergence of the LJ search method\n|journal=Journal of Optimization Theory and Applications\n|year=1979\n|volume=28\n|issue=3\n|pages=429&ndash;434\n|doi=10.1007/BF00933384\n|mr=543384\n|ref=harv\n}}\n</ref>\n}}\n\n{{cite book|mr=702836|last1=Nemirovsky|first1=A. S.|last2=Yudin|first2=D. B.|title=Problem complexity and method efficiency in optimization|edition=Translated by E. R. Dawson from the (1979) Russian (Moscow: Nauka)|series=Wiley-Interscience Series in Discrete Mathematics|publisher=John Wiley & Sons, Inc.|location=New York|year=1983|pages=xv+388|isbn=0-471-10345-4}}\n\n{{Major subfields of optimization}}\n\n{{DEFAULTSORT:Luus-Jaakola}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Heuristic algorithms]]"
    },
    {
      "title": "Matheuristics",
      "url": "https://en.wikipedia.org/wiki/Matheuristics",
      "text": "'''Matheuristics''' are [[optimization algorithm]]s made by the interoperation of [[metaheuristics]] and [[mathematical programming]] (MP) techniques. An essential feature is the exploitation in some part of the algorithms of features derived from the mathematical model of the problems of interest, thus the definition \"''model-based heuristics''\" appearing in the title of some events of the conference series dedicated to matheuristics [http://astarte.csr.unibo.it/Matheuristics/ matheuristics web page].\n\nThe topic has attracted the interest of a community of researchers, and this led to the publication of dedicated volumes and journal special issues<ref>Hybridizing Metaheuristics and Mathematical Programming. Series: Annals of Information Systems , Vol. 10 Maniezzo, Vittorio; Stützle, Thomas; Voß, Stefan (Eds.), Springer, 2009. [https://www.springer.com/business/operations+research/book/978-1-4419-1305-0]</ref><ref>Special Issue on Mathematical Contributions to Metaheuristics. Guest Editors: Vittorio Maniezzo, Stefan Voß, and Pierre Hansen, Journal of Heuristics, Volume 15, Number 3 / June, 2009 [http://www.springerlink.com/content/q48270w133l3/?p=a4c50f42c8f446f9a829f85ff4e8371d&pi=1]</ref><ref>Marco A. Boschetti, V. Maniezzo, M. Roffilli and Antonio Bolufé Röhler. Matheuristics: Optimization, Simulation and Control. ''Proc. of HM 2009'', LNCS 5818, pp. 171–177, 2009. Springer-Verlag Berlin Heidelberg 2009 [https://dx.doi.org/10.1007/978-3-642-04918-7_13]</ref> besides to dedicated tracks and sessions on wider scope conferences.\n\nA word of caution is needed before delving into the subject, because obviously the use of MP for solving optimization problems, albeit in a heuristic way, is much older and much more widespread than matheuristics. However, this is not the case for metaheuristics. Even the very idea of designing MP methods specifically for heuristic solution has innovative traits, when opposed to exact methods which turn into heuristics when enough computational resources are not available.\n\nSome approaches using MP combined with metaheuristics have begun to appear regularly in the matheuristics literature. This combination can go two-ways, both in MP used to improve or design metaheuristics and in metaheuristics used for improving known MP techniques, even though the first of these two directions is by far more studied.\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://astarte.csr.unibo.it/Matheuristics2006/ Matheuristics 2006] 1st International Workshop on Mathematical Contributions to Metaheuristics.\n*[http://astarte.csr.unibo.it/Matheuristics2008/ Matheuristics 2008] 2nd International Workshop on Model-Based Metaheuristics\n*[http://homepage.univie.ac.at/matheuristics2010/ Matheuristics 2010] 3rd International Workshop on Model-Based Metaheuristics\n*[http://www.ic.uff.br/matheuristics2012/ Matheuristics 2012] 4th International Workshop on Model-Based Metaheuristics\n*[https://web.archive.org/web/20160113172534/http://iwi.econ.uni-hamburg.de/mh14/ Matheuristics 2014] 5th International Workshop on Model-Based Metaheuristics\n*[http://iridia.ulb.ac.be/matheuristics2016/ Matheuristics 2016] 6th International Workshop on Model-Based Metaheuristics\n\n==Selected publications==\n*[http://www.springerlink.com/content/e4k72271701430g4/] M. Caserta, S. Voß: A math-heuristic algorithm for the DNA sequencing problem. Lecture Notes in Computer Science 6073 (2010), 25 - 36\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Matrix chain multiplication",
      "url": "https://en.wikipedia.org/wiki/Matrix_chain_multiplication",
      "text": "'''Matrix chain multiplication''' (or Matrix Chain Ordering Problem, MCOP) is an [[optimization problem]] that can be solved using [[dynamic programming]].  Given a sequence of matrices, the goal is to find the most efficient way to [[matrix multiplication|multiply these matrices]]. The problem is not actually to ''perform'' the multiplications, but merely to decide the sequence of the matrix multiplications involved.\n\nThere are many options because matrix multiplication is [[associativity|associative]]. In other words, no matter how the product is [[Bracket (mathematics)|parenthesize]]d, the result obtained will remain the same. For example, for four matrices ''A'', ''B'', ''C'', and ''D'', we would have:\n\n:((''AB'')''C'')''D'' = (''A''(''BC''))''D'' = (''AB'')(''CD'') = ''A''((''BC'')''D'') = ''A''(''B''(''CD'')).\n\nHowever, the order in which the product is parenthesized affects the number of simple arithmetic operations needed to compute the product, that is the [[computational complexity]]. \nFor example, if ''A'' is a 10 &times; 30 matrix, ''B'' is a 30 &times; 5 matrix, and ''C'' is a 5 &times; 60 matrix, then\n\n:computing (''AB'')''C'' needs (10&times;30&times;5) + (10&times;5&times;60)  = 1500 + 3000 = 4500 operations, while\n:computing ''A''(''BC'') needs (30&times;5&times;60) + (10&times;30&times;60) = 9000 + 18000 = 27000 operations.\n\nClearly the first method is more efficient. With this information, the problem statement can be refined as \"how to determine the optimal parenthesization of a product of ''n'' matrices?\" Checking each possible parenthesization ([[Brute-force search|brute force]]) would require a [[Time complexity|run-time]] that is exponential in the number of matrices, which is very slow and impractical for large ''n''. A quicker solution to this problem can be achieved by breaking up the problem into a set of related subproblems. By solving subproblems once and reusing the solutions, the required run-time can be drastically reduced. This concept is known as [[dynamic programming]].\n\n== A dynamic programming algorithm ==\n\nTo begin, let us assume that all we really want to know is the minimum cost, or minimum number of arithmetic operations needed to multiply out the matrices. If we are only multiplying two matrices, there is only one way to multiply them, so the minimum cost is the cost of doing this. In general, we can find the minimum cost using the following [[recursion|recursive algorithm]]:\n\n* Take the sequence of matrices and separate it into two subsequences.\n* Find the minimum cost of multiplying out each subsequence.\n* Add these costs together, and add in the cost of multiplying the two result matrices.\n* Do this for each possible position at which the sequence of matrices can be split, and take the minimum over all of them.\n\nFor example, if we have four matrices ''ABCD'', we compute the cost required to find each of (''A'')(''BCD''), (''AB'')(''CD''), and (''ABC'')(''D''), making recursive calls to find the minimum cost to compute ''ABC'', ''AB'', ''CD'', and ''BCD''. We then choose the best one. Better still, this yields not only the minimum cost, but also demonstrates the best way of doing the multiplication: group it the way that yields the lowest total cost, and do the same for each factor.\n\nHowever, if we implement this algorithm we discover that it is just as slow as the naive way of trying all permutations! What went wrong? The answer is that we're doing a lot of redundant work. For example, above we made a recursive call to find the best cost for computing both ''ABC'' and ''AB''. But finding the best cost for computing ABC also requires finding the best cost for ''AB''. As the recursion grows deeper, more and more of this type of unnecessary repetition occurs.\n\nOne simple solution is called [[memoization]]: each time we compute the minimum cost needed to multiply out a specific subsequence, we save it. If we are ever asked to compute it again, we simply give the saved answer, and do not recompute it. Since there are about ''n''<sup>2</sup>/2 different subsequences, where ''n'' is the number of matrices, the space required to do this is reasonable. It can be shown that this simple trick brings the runtime down to O(''n''<sup>3</sup>) from O(2<sup>''n''</sup>), which is more than efficient enough for real applications. This is [[Top-down and bottom-up design|top-down]] dynamic programming.\n\nThe following bottom-up approach <ref name=\"Cormen\">\n{{cite book\n  | last1 = Cormen\n  | first1 =  Thomas H\n  | authorlink1 = Thomas H. Cormen\n  | last2=Leiserson\n  | first2=Charles E\n  | authorlink2 = Charles E. Leiserson\n  | last3=Rivest\n  | first3=Ronald L\n  | authorlink3 = Ron Rivest\n  | last4=Stein\n  | first4=Clifford\n  | authorlink4 = Clifford Stein\n  | title = Introduction to Algorithms\n  | volume = Second Edition\n  | chapter = 15.2: Matrix-chain multiplication\n  | pages = 331–338\n  | publisher = MIT Press and McGraw-Hill\n  | year = 2001\n  | isbn = 978-0-262-03293-3\n}}\n</ref> computes, for each 2 ≤ ''k'' ≤ n, the minimum costs of all subsequences of length ''k'' using the costs of smaller subsequences already computed.\nIt has the same asymptotic runtime and requires no recursion.\n\nPseudocode:\n<source lang=\"java\">\n// Matrix A[i] has dimension dims[i-1] x dims[i] for i = 1..n\nMatrixChainOrder(int dims[])\n{\n    // length[dims] = n + 1\n    n = dims.length - 1;\n    // m[i,j] = Minimum number of scalar multiplications (i.e., cost)\n    // needed to compute the matrix A[i]A[i+1]...A[j] = A[i..j]\n    // The cost is zero when multiplying one matrix\n    for (i = 1; i <= n; i++)\n        m[i, i] = 0;\n\n    for (len = 2; len <= n; len++) { // Subsequence lengths\n        for (i = 1; i <= n - len + 1; i++) {\n            j = i + len - 1;\n            m[i, j] = MAXINT;\n            for (k = i; k <= j - 1; k++) {\n                cost = m[i, k] + m[k+1, j] + dims[i-1]*dims[k]*dims[j];\n                if (cost < m[i, j]) {\n                    m[i, j] = cost;\n                    s[i, j] = k; // Index of the subsequence split that achieved minimal cost\n                }\n            }\n        }\n    }\n}\n</source>\n*Note : The first index for dims is 0 and the first index for m and s is 1.\n\nA [[Java (programming language)|Java]] implementation using zero based array indexes along with a convenience method for printing the solved order of operations:\n<source lang=\"java\">\npublic class MatrixOrderOptimization {\n    protected int[][]m;\n    protected int[][]s;\n    public void matrixChainOrder(int[] dims) {\n        int n = dims.length - 1;\n        m = new int[n][n];\n        s = new int[n][n];\n\n        for (int lenMinusOne = 1; lenMinusOne < n; lenMinusOne++) {\n            for (int i = 0; i < n - lenMinusOne; i++) {\n                int j = i + lenMinusOne;\n                m[i][j] = Integer.MAX_VALUE;\n                for (int k = i; k < j; k++) {\n                    int cost = m[i][k] + m[k+1][j] + dims[i]*dims[k+1]*dims[j+1];\n                    if (cost < m[i][j]) {\n                        m[i][j] = cost;\n                        s[i][j] = k;\n                    }\n                }\n            }\n        }\n    }\n\n    public void printOptimalParenthesizations() {\n        boolean[] inAResult = new boolean[s.length];\n        printOptimalParenthesizations(s, 0, s.length - 1, inAResult);\n    }\n\n    void printOptimalParenthesizations(int[][]s, int i, int j,  /* for pretty printing: */ boolean[] inAResult) {\n        if (i != j) {\n            printOptimalParenthesizations(s, i, s[i][j], inAResult);\n            printOptimalParenthesizations(s, s[i][j] + 1, j, inAResult);\n            String istr = inAResult[i] ? \"_result \" : \" \";\n            String jstr = inAResult[j] ? \"_result \" : \" \";\n            System.out.println(\" A_\" + i + istr + \"* A_\" + j + jstr);\n            inAResult[i] = true;\n            inAResult[j] = true;\n        }\n    }\n}\n</source>\n\nAt the end of this program, we have the minimum cost for the full sequence.\n\n== More efficient algorithms ==\nThere are algorithms that are more efficient than the ''O''(''n''<sup>3</sup>) dynamic programming algorithm, though they are more complex.\n\n=== Hu &amp; Shing (1981) ===\nAn algorithm published in 1981 by Hu and Shing achieves ''O''(''n''&nbsp;log&nbsp;''n'') [[computational complexity]].<ref>\n{{cite journal\n  | last1 = Hu\n  | first1 = TC\n  | last2 = Shing\n  | first2 = MT\n  | title = Computation of Matrix Chain Products, Part I\n  | journal = SIAM Journal on Computing\n  | volume = 11\n  | issue = 2\n  | pages = 362–373\n  | year = 1982\n  | url = http://www.cs.ust.hk/mjg_lib/bibs/DPSu/DPSu.Files/0211028.pdf\n  | issn = 0097-5397\n  | doi=10.1137/0211028\n| citeseerx = 10.1.1.695.2923\n  }}\n</ref><ref>\n{{cite journal\n  | last1 = Hu\n  | first1 = TC\n  | last2 = Shing\n  | first2 = MT\n  | title = Computation of Matrix Chain Products, Part II\n  | journal = SIAM Journal on Computing\n  | volume = 13\n  | issue = 2\n  | pages = 228–251\n  | year = 1984\n  | url = http://www.cs.ust.hk/mjg_lib/bibs/DPSu/DPSu.Files/0213017.pdf\n  | issn = 0097-5397\n  | doi=10.1137/0213017\n| citeseerx = 10.1.1.695.4875\n  }}\n</ref><ref>\n{{cite journal\n  | first = Czumaj\n  | last = Artur\n  | title = Very Fast Approximation of the Matrix Chain Product Problem\n  | journal = Journal of Algorithms\n  | volume = 21\n  | pages = 71–79\n  | year = 1996\n  | url = https://pdfs.semanticscholar.org/e8b8/40a921f7967b30ac161b3dd9654b27998ddb.pdf\n  | doi = 10.1006/jagm.1996.0037\n  | citeseerx = 10.1.1.218.8168\n  }}\n</ref>\nThey showed how the matrix chain multiplication problem can be transformed (or [[Reduction (complexity)|reduced]]) into the problem of [[polygon triangulation|triangulation]] of a [[regular polygon]].  The polygon is oriented such that there is a horizontal bottom side, called the base, which represents the final result. The other ''n'' sides of the polygon, in the clockwise direction, represent the matrices. The vertices on each end of a side are the dimensions of the matrix represented by that side.  With ''n'' matrices in the multiplication chain there are ''n''−1 [[binary operation]]s and ''C''<sub>''n''−1</sub> ways of placing parentheses, where ''C''<sub>''n''−1</sub> is the (''n''−1)-th [[Catalan number]]. The algorithm exploits that there are also ''C''<sub>''n''−1</sub> possible triangulations of a polygon with ''n''+1 sides.\n\nThis image illustrates possible triangulations of a regular [[hexagon]].  These correspond to the different ways that parentheses can be placed to order the multiplications for a product of 5 matrices.\n[[Image:Catalan-Hexagons-example.svg|400px|center]]\n\nFor the example below, there are four sides: A, B, C and the final result ABC.  A is a 10×30 matrix, B is a 30×5 matrix, C is a 5×60 matrix, and the final result is a 10×60 matrix. The regular polygon for this example is a 4-gon, i.e. a square:\n[[File:Matrix chain multiplication polygon example.svg|125px|center]]\n\nThe matrix product AB is a 10x5 matrix and BC is a 30x60 matrix. The two possible triangulations in this example are:\n<gallery mode=packed>\nFile:Matrix chain multiplication polygon example AB.svg|alt=(AB)C|Polygon representation of (AB)C\nFile:Matrix chain multiplication polygon example BC.svg|alt=A(BC)|Polygon representation of A(BC)\n</gallery>\n\nThe cost of a single triangle in terms of the number of multiplications needed is the product of its vertices. The total cost of a particular triangulation of the polygon is the sum of the costs of all its triangles:\n\n:(''AB'')''C'': (10×30×5) + (10×5×60) = 1500 + 3000 = 4500 multiplications\n:''A''(''BC''): (30×5×60) + (10×30×60) = 9000 + 18000 = 27000 multiplications\n\nHu & Shing developed an algorithm that finds an optimum solution for the minimum cost partition problem in ''O''(''n''&nbsp;log&nbsp;''n'') time.\n\n{{Expand section|date=April 2009}}\n\n== Generalizations ==\n\nThe matrix chain multiplication problem generalizes to solving a more abstract problem: given a linear sequence of objects, an associative binary operation on those objects, and a way to compute the cost of performing that operation on any two given objects (as well as all partial results), compute the minimum cost way to group the objects to apply the operation over the sequence.<ref>G. Baumgartner, D. Bernholdt, D. Cociorva, R. Harrison, M. Nooijen, J. Ramanujam and P. Sadayappan. A Performance Optimization Framework for Compilation of Tensor Contraction Expressions into Parallel Programs. 7th International Workshop on High-Level Parallel Programming Models and Supportive Environments (HIPS '02). Fort Lauderdale, Florida. 2002 available at http://citeseer.ist.psu.edu/610463.html and at http://www.csc.lsu.edu/~gb/TCE/Publications/OptFramework-HIPS02.pdf</ref> One somewhat contrived special case of this is [[string concatenation]] of a list of strings. In [[C (programming language)|C]], for example, the cost of concatenating two strings of length ''m'' and ''n'' using ''strcat'' is O(''m''&nbsp;+&nbsp;''n''), since we need O(''m'') time to find the end of the first string and O(''n'') time to copy the second string onto the end of it. Using this cost function, we can write a dynamic programming algorithm to find the fastest way to concatenate a sequence of strings. However, this optimization is rather useless because we can straightforwardly concatenate the strings in time proportional to the sum of their lengths. A similar problem exists for singly [[linked lists]].\n\nAnother generalization is to solve the problem when parallel processors are available. In this case, instead of adding the costs of computing each factor of a matrix product, we take the maximum because we can do them simultaneously. This can drastically affect both the minimum cost and the final optimal grouping; more \"balanced\" groupings that keep all the processors busy are favored. There are even more sophisticated approaches.<ref>Heejo Lee, Jong Kim, Sungje Hong, and Sunggu Lee. [http://ccs.korea.ac.kr/pds/tpds03.pdf Processor Allocation and Task Scheduling of Matrix Chain Products on Parallel Systems] {{webarchive|url=https://web.archive.org/web/20110722132611/http://ccs.korea.ac.kr/pds/tpds03.pdf |date=2011-07-22 }}. ''IEEE Trans. on Parallel and Distributed Systems,'' Vol. 14, No. 4, pp.&nbsp;394–407, Apr. 2003</ref>\n\n==See also==\n\n*[[Associahedron]]\n*[[Tamari lattice]]\n\n== References ==\n\n<references/>\n\n{{DEFAULTSORT:Matrix Chain Multiplication}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Matrices]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Maximum subarray problem",
      "url": "https://en.wikipedia.org/wiki/Maximum_subarray_problem",
      "text": "{{multiple issues|\n{{citation style|date=January 2018}}\n{{more footnotes|date=January 2018}}\n{{more citations needed|date=January 2018}}\n}}\n[[File:Maximum Subarray Visualization.svg|thumbnail|Visualization of how sub-arrays change based on start and end positions of a sample.  Each possible contiguous sub-array is represented by a point on a colored line.  That point's y-coordinate represents the sum of the sample.  Its x-coordinate represents the end of the sample, and the leftmost point on that colored line represents the start of the sample. In this case, the array from which samples are taken is [2, 3, -1, -20, 5, 10]. ]]\n\nIn [[computer science]], the '''maximum subarray problem''' is the task of finding a contiguous subarray with the largest sum, within a given one-dimensional [[array data structure|array]] A[1...n] of numbers.  Formally, the task is to find indices <math>i</math> and <math>j</math> with <math>1 \\leq i \\leq j \\leq n </math>, such that the sum\n: <math>\\sum_{x=i}^j A[x] </math>\nis as large as possible. (Some formulations of the problem also allow the empty subarray to be considered; by convention, [[empty sum|the sum of all values of the empty subarray]] is zero.)  Each number in the input array A could be positive, negative, or zero.\n\nFor example, for the array of values [&minus;2, 1, &minus;3, 4, &minus;1, 2, 1, &minus;5, 4], the contiguous subarray with the largest sum is [4, &minus;1, 2, 1], with sum 6.\n\nSome properties of this problem are: \n# If the array contains all non-negative numbers, then the problem is trivial; the maximum subarray is the entire array.\n# If the array contains all non-positive numbers, then the solution is the number in the array with the smallest absolute value (or the empty subarray, if it is permitted). \n# Several different sub-arrays may have the same maximum sum.\n\nThis problem can be solved using several different algorithmic techniques, including brute force, divide and conquer, dynamic programming, and reduction to shortest paths.\n\n== History ==\nThe maximum subarray problem was proposed by [[Ulf Grenander]] in 1977 as a simplified model for [[maximum likelihood]] estimation of patterns in digitized images.<ref name=\"Bentley\">{{cite journal\n | first = Jon | last = Bentley\n | authorlink = Jon Bentley (computer scientist)\n | title = Programming Pearls: Algorithm Design Techniques\n | journal = [[Communications of the ACM]]\n | volume = 27 | issue = 9 | year = 1984\n | pages = 865–873\n | doi = 10.1145/358234.381162 }}</ref>  Grenander was looking to find a rectangular subarray with maximum sum, in a two-dimensional array of real numbers.  A brute-force algorithm for the two-dimensional problem runs in ''O''(''n''<sup>6</sup>) time; because this was prohibitively slow, Grenander proposed the one-dimensional problem to gain insight into its structure.  Grenander derived an algorithm that solves the one-dimensional problem in ''O''(''n''<sup>2</sup>) time, improving the brute force running time of ''O''(''n''<sup>3</sup>).\n\n[[Jay Kadane]] of [[Carnegie Mellon University]] soon after designed an ''O''(''n'')-time algorithm for the one-dimensional problem,<ref name=\"Bentley\" /> which is clearly as fast as possible.  The same ''O''(''n'')-time algorithm was later automatically derived by algebraic manipulation of the brute-force algorithm using the [[Bird–Meertens formalism]].<ref>{{cite journal \n| url=http://comjnl.oxfordjournals.org/content/32/2/122.full.pdf \n| author=Richard S. Bird \n| author-link=Richard S. Bird\n| title=Algebraic Identities for Program Calculation \n| journal=[[The Computer Journal]]\n| volume=32 \n| number=2 \n| pages=122–126 \n| month= \n| year=1989 \n}} Here: Sect.8, p.126</ref>\n\nGrenander's two-dimensional generalization can be solved in O(''n''<sup>3</sup>) time either by using Kadane's algorithm as a subroutine, or through a divide-and-conquer approach.  Slightly faster algorithms based on [[Min-plus matrix multiplication|distance matrix multiplication]] have been proposed by [[Hisao Tamaki]] and [[Takeshi Tokuyama]] in 1998<ref>{{cite journal\n| last       = Tamaki\n| first      = Hisao\n| last2      = Tokuyama\n| first2     = Takeshi\n| date       = 1998\n| title      = Algorithms for the Maximum Subarray Problem Based on Matrix Multiplication\n| url        = http://dl.acm.org/citation.cfm?id=314613.314823\n| journal    = Proceedings of the 9th SODA (Symposium on Discrete Algorithms)\n| volume     = \n| issue      = \n| pages      = 446–452\n| doi        = \n| access-date = November 17, 2018\n}}</ref> and by [[Tadao Takaoka]] in 2002.<ref>{{cite journal\n| last       = Takaoka\n| first      = Tadao\n| date       = 2002\n| title      = Efficient Algorithms for the Maximum Subarray Problem by Distance Matrix Multiplication\n| url        = \n| journal    = Electronic Notes in Theoretical Computer Science\n| volume     = 61\n| issue      = \n| pages      = 191–200\n| doi        = 10.1016/S1571-0661(04)00313-5\n}}</ref>  There is some evidence that no significantly faster algorithm exists; an algorithm that solves the two-dimensional maximum subarray problem in O(''n''<sup>3&minus;ε</sup>) time, for any ε>0, would imply a similarly fast algorithm for the [[Shortest path problem#All-pairs shortest paths|all-pairs shortest paths]] problem.<ref>{{cite journal\n| last       = Backurs\n| first      = Arturs\n| last2      = Dikkala\n| first2     = Nishanth\n| last3      = Tzamos\n| first3     = Christos\n| date       = 2016\n| title      = Tight Hardness Results for Maximum Weight Rectangles\n| url        = \n| journal    = Proc. 43rd International Colloquium on Automata, Languages, and Programming\n| volume     =\n| issue      = \n| pages      = 81:1–81:13\n| doi        = 10.4230/LIPIcs.ICALP.2016.81\n}}</ref>\n\n== Applications ==\nMaximum subarray algorithms are used for data analysis in many fields, such as genomic [[sequence analysis]], [[computer vision]], and [[data mining]].\n\n'''Genomic sequence analysis''' employs maximum subarray algorithms to identify important biological segments of protein sequences and {{clarify span|the information for the purpose of predicting outcomes|Outcomes of what? The Phrase 'information for the purpose of' is probably redundant.|date=October 2017}}. As an example, {{clarify span|specific information of a protein sequence can be organized into a linear function which can be used to understand the structure and function of a protein.|How is an array of positive and negative numbers obtained from a protein sequence? What is the biological meaning of such a number? What is therefore the meaning of a subarray with maximal sum?|date=October 2017}} {{weasel inline|text=Biologists find this approach to be efficient and easy to analyze their data.|date=October 2017}}\n\nThe score-based technique of finding the segment with the highest total sum is used in many problems similar to the MSP. In genomic sequence analysis, these problems include conserved segments, GC-rich regions, tandem repeats, low-complexity filter, DNA binding domains, and regions of high charge.{{citation needed|date=October 2017}}\n\nFor '''computer vision''' , maximum subarray algorithms are used in bitmap images to detect the highest score subsequence which represents the brightest area in an image. The image is a two dimensional array of positive values that corresponds to the brightness of a pixel. The algorithm is evaluated after normalizing every value in the array by subtracting them from the mean brightness.\n\n'''Data mining''' is an application of maximum subarray algorithms with numerical attributes. To understand the role of the maximum subarray problem in data mining it is important to be familiar with the association rule and its parts. The association rule is an if/then statement that creates relationships between unrelated pieces of data. The two parts of the association rule are the antecedent (if statement) and the consequent (then statement). In business applications of data mining, association rules are important in examining and foreseeing customer's actions/behavior. Data mining is used for companies to anticipate common trends, by representing problems with maximum subarray problem into an sub-array to be normalized and solved. The highest {{clarify span|result|What is the meaning of a score number? What is the meaning of a number array? How are if/then rules obtained from a maximum score subarray? Probably, this can't be explained here in full generality, but it should be possible to give a particular example.|date=October 2017}} of the array will give companies information of what customers are responding well to and will influence the companies' actions as a result.{{citation needed|date=October 2017}}\n\n==Kadane's algorithm==\nA bit of a background:\n[[Joseph Born Kadane|Kadane's]] algorithm is based on splitting up the set of possible solutions into mutually exclusive (disjoint) sets.\nIt exploits the fact that any solution (i.e., any member of the set of solutions) will always have a last element <math>i</math> (this is what is meant by \"sum ending at position <math>i</math>\"). Thus, we simply have to examine, one by one, the set of solutions whose last element's index is <math>1</math>, the set of solutions whose last element's index is <math>2</math>, then <math>3</math>, and so forth to <math>n</math>. It turns out that this process can be carried out in linear time.\n\nKadane's algorithm begins with a simple [[Mathematical induction|inductive]] question: if we know the maximum subarray sum ending at position <math>i</math> (call this <math>B_i</math>), what is the maximum subarray sum ending at position <math>i+1</math> (equivalently, what is <math>B_{i+1}</math>)?  The answer turns out to be relatively straightforward: either the maximum subarray sum ending at position <math>i+1</math> includes the maximum subarray sum ending at position <math>i</math> as a prefix, or it doesn't (equivalently, <math>B_{i+1} = \\max( A_{i+1}, A_{i+1} + B_i)</math>, where <math>A_{i+1}</math> is the element at index <math>i+1</math>).\n\nThus, we can compute the maximum subarray sum ending at position <math>i</math> for all positions <math>i</math> by iterating once over the array.  As we go, we simply keep track of the maximum sum we've ever seen.  Thus, the problem can be solved with the following code, expressed here in [[Python (programming language)|Python]]:\n\n<source lang=\"python\" line=\"1\">\n def maxSubArray(nums):\n     \"\"\"\n     :type nums: List[int]\n     :rtype: int\n     \"\"\"\n     for i in range(1, len(nums)):\n            if nums[i-1] > 0:\n                nums[i] += nums[i-1]\n     return max(nums)\n</source>\n'''Note:''' this function will perform in-place modification of array in order to calculate the sums of contiguous sequences of array elements, wherever those sums are positive. This is ensured by <code>nums[i-1] > 0</code> condition. The last positive member of a sequence will contain sum of that particular sequence. If an array contains all negative members the function will simply return the largest number.\n\nThe algorithm can also be easily modified to keep track of the starting and ending indices of the maximum subarray as well as the case where we want to allow zero-length subarrays (with implicit sum 0) if all elements are negative.\n\n<source lang=\"python\" line=\"1\">\ndef max_subarray( row ):\n    largest_ending_here = 0\n    best_start = this_start = end = best_so_far = 0\n    for i, x in enumerate( row ):\n        largest_ending_here += x\n        best_so_far = max( best_so_far, largest_ending_here )\n        if largest_ending_here <= 0:\n            this_start = i + 1\n            largest_ending_here = 0\n        \n        elif largest_ending_here == best_so_far:\n            best_start = this_start\n            end = i + 1 # the +1 is to have 'end' be exclusive\n    \n    return best_so_far, best_start, end\n</source>\n\nBecause of the way this algorithm uses optimal substructures (the maximum subarray ending at each position is calculated in a simple way from a related but smaller and overlapping subproblem: the maximum subarray ending at the previous position) this algorithm can be viewed as a simple/trivial example of [[dynamic programming]].\n\nThe runtime complexity of Kadane's algorithm is <math>O(n)</math><ref>https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation</ref>.\n\n== Generalizations ==\nSimilar problems may be posed for higher-dimensional arrays, but their solutions are more complicated; see, e.g., {{harvtxt|Takaoka|2002}}. {{harvtxt|Brodal|Jørgensen|2007}} showed how to find the ''k'' largest subarray sums in a one-dimensional array, in the optimal time bound <math>O(n + k)</math>.\n\nThe Maximum sum ''k''-disjoint subarrays can also be computed in the optimal time bound <math>O(n + k)</math> \n.<ref>{{cite journal|last1=Bengtsson|first1=Fredrik|last2=Chen|first2=Jingsen|date=2007|title=Computing maximum-scoring segments optimally|url=http://ltu.diva-portal.org/smash/get/diva2:995901/FULLTEXT01.pdf|journal=Luleå University of Technology|volume=|issue=3|pages=|via=}}</ref>\n\n== See also ==\n\n* [[Subset sum problem]]\n\n==References==\n{{Reflist}}\n*{{citation\n | first1 = Gerth Stølting | last1 = Brodal | first2 = Allan Grønlund | last2 = Jørgensen\n | contribution = A linear time algorithm for the ''k'' maximal sums problem\n | title = Mathematical Foundations of Computer Science 2007\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | volume = 4708 | year = 2007 | pages = 442–453 | doi = 10.1007/978-3-540-74456-6_40}}.\n*{{citation\n | first = T. | last = Takaoka\n | title = Efficient algorithms for the maximum subarray problem by distance matrix multiplication\n | journal = Electronic Notes in Theoretical Computer Science | volume = 61 | year = 2002\n | url = http://www.cosc.canterbury.ac.nz/tad.takaoka/cats02.pdf}}.\n\n* {{Cite website|url=http://www.picb.ac.cn/~xiaohang/vimwiki/study/tanlirong/Algorithm/project/Report.pdf|title=Maximum Contiguous Subarray Sum Problems|date=|access-date=|website=|last=TAN|first=Lirong|archive-url=|archive-date=|dead-url=}}\n* {{Cite blog|url=https://www.iis.sinica.edu.tw/~scm/2010/maximum-segment-sum-origin-and-derivation|title=The Maximum Segment Sum Problem: Its Origin, and a Derivation|date=2010|access-date=|website=|last=Mu|first=Shin-Cheng|archive-url=|archive-date=|dead-url=}}\n* {{Cite web|url=https://pdfs.semanticscholar.org/bea4/1795adaf240b9db4195b9dc511bd8d46bff1.pdf|title=Sequential and Parallel Algorithms for the Generalized Maximum Subarray Problem|last=Bae|first=Sung Eun|date=2007|website=|archive-url=|archive-date=|dead-url=|access-date=}}\n* {{Cite web|url=http://cs.slu.edu/~goldwamh/courses/slu/csci314/2012_Fall/lectures/maxsubarray/|title=Notes on Maximum Subarray Problem|last=|first=|date=2012|website=|archive-url=|archive-date=|dead-url=|access-date=}}\n\n== External links ==\n* [http://www.algorithmist.com/index.php/Kadane's_Algorithm www.algorithmist.com]\n* [http://alexeigor.wikidot.com/kadane alexeigor.wikidot.com]\n* [http://rosettacode.org/wiki/Greatest_subsequential_sum greatest subsequential sum problem on Rosetta Code]\n* [https://www.geeksforgeeks.org/maximum-sum-rectangle-in-a-2d-matrix-dp-27/ geeksforgeeks page on Kadane's Algorithm]\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example Python code]]"
    },
    {
      "title": "MCS algorithm",
      "url": "https://en.wikipedia.org/wiki/MCS_algorithm",
      "text": "{{unreferenced|date=July 2012}}\n{{Context|date=October 2009}}\n\n[[File:MCS algorithm.gif|thumb|400px|alt=A cartoon centipede reads books and types on a laptop.|''Figure 1:'' MCS algorithm (''without'' local search) applied to the two-dimensional [[Rosenbrock function]]. The global minimum <math>f_{min}=0</math> is located at <math>(x,y)=(1,1)</math>. MCS identifies a position with <math>f\\approx 0.002</math> within 21 function evaluations. After additional 21 evaluations the optimal value is not improved and the algorithm terminates. Observe dense clustering of samples around potential minima - this effect can be reduced significantly by employing local searches appropriately.]]\n\n\n'''Multilevel Coordinate Search''' ('''MCS''') is an efficient [[algorithm]] for bound constrained [[global optimization]] using [[function (mathematics)|function]] values [[derivative-free optimization|only]].\n\nTo do so, the n-dimensional [[Candidate solution|search space]] is represented by a set of non-intersecting hypercubes (boxes). The boxes are then iteratively split along an axis plane according to the value of the function at a representative point of the box (and its neighbours) and the box's size. These two splitting criteria combine to form a global search by splitting large boxes and a local search by splitting areas for which the function value is good. \n\nAdditionally, a local search combining a (multi-dimensional) quadratic interpolant of the function and [[line search]]es can be used to augment performance of the algorithm (''MCS with local search''); in this case the ''plain'' MCS is used to provide the starting (initial) points. The information provided by local searches (namely local minima of the objective function) is then fed back to the optimizer and influences the splitting criteria, resulting in reduced sample clustering around local minima and faster convergence.\n\n==Simplified workflow==\nThe basic workflow is presented in figure 1. Generally, each step can be characterized by three stages:\n# Identify a potential candidate for splitting (magenta, thick).\n# Identify the optimal splitting direction and the expected optimal position of splitting points (green).\n# Evaluate the objective function at splitting points not considered previously. Generate new boxes (magenta, thin) based on the values of the objective function at splitting (green) points.\n\nAt every step at least one splitting point (yellow) is a known function sample (red) hence the objective is never evaluated there again.\n\nIn order to determine if a box will be split two separate splitting criteria are used. The first one, ''splitting by rank'', ensures that large boxes that have not been split too often will be split eventually. If it applies then the splitting point can be determined a priori. The second one, ''splitting by expected gain'', employs a local one-dimensional quadratic model (surrogate) along a single coordinate. In this case the splitting point is defined as the minimum of the surrogate and the box is split only if the interpolant value there is lower than the current best sampled function value.\n\n==Convergence==\nThe algorithm is guaranteed to converge to the global minimum in the long run (i.e. when the number of function evaluations and the ''search depth'' are arbitrarily large) if the objective function is continuous in the neighbourhood of the global minimizer. This follows from the fact that any box will become arbitrarily small eventually, hence the spacing between samples tends to zero as the number of function evaluations tends to infinity.\n\n==Recursive implementation==\nMCS was designed to be implemented in an efficient [[recursion (computer science)|recursive]] way with the aid of [[tree (data structure)|trees]]. With this approach the amount of memory required is independent of problem dimensionality since the sampling points are not stored explicitly. Instead, just a single coordinate of each sample is saved and the remaining coordinates can be recovered by tracing the history of a box back to the root (initial box). This method was suggested by the authors and used in their original implementation.\n\nWhen implemented carefully this also allows for avoiding repeated function evaluations. More precisely, if a sampling point is placed along the boundary of two adjacent boxes then this information can often be extracted through backtracing the point's history for a small number of steps. Consequently, new subboxes can be generated without evaluating the (potentially expensive) objective function. This scenario is visualised in figure 1 whenever a green (but not yellow) and a red point coincide e.g. when the box with corners around <math>(-0.5,0.5)</math> and <math>(1.0,3.5)</math> is split.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.mat.univie.ac.at/~neum/software/mcs/ Homepage of the algorithm]\n* [http://www.mat.univie.ac.at/~neum/glopt/janka/dix_sze_eng.html Performance of the algorithm relative to others]\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Mehrotra predictor–corrector method",
      "url": "https://en.wikipedia.org/wiki/Mehrotra_predictor%E2%80%93corrector_method",
      "text": "'''Mehrotra's predictor–corrector method''' in [[Optimization (mathematics)|optimization]] is a specific [[interior point method]] for [[linear programming]]. It was proposed in 1989 by Sanjay Mehrotra.<ref>{{cite journal|last=Mehrotra|first=S.|title=On the implementation of a primal–dual interior point method|journal=SIAM Journal on Optimization|volume=2|year=1992|issue=4|pages=575–601|doi=10.1137/0802028}}</ref>\n\nThe method is based on the fact that at each [[iteration]] of an interior point algorithm it is necessary to compute the [[Cholesky decomposition]] (factorization) of a large matrix to find the search direction. The factorization step is the most computationally expensive step in the algorithm. Therefore, it makes sense to use the same decomposition more than once before recomputing it.\n\nAt each iteration of the algorithm, Mehrotra's predictor–corrector method uses the same Cholesky decomposition to find two different directions: a predictor and a corrector.\n\nThe idea is to first compute an optimizing search direction based on a first order term (predictor). The step size that can be taken in this direction is used to evaluate how much centrality correction is needed. Then, a corrector term is computed: this contains both a centrality term and a second order term.\n\nThe complete search direction is the sum of the predictor direction and the corrector direction.\n\nAlthough there is no theoretical complexity bound on it yet, Mehrotra's predictor–corrector method is widely used in practice.<ref>\"In 1989, Mehrotra described a practical algorithm for linear programming that remains the basis of most current software; his work appeared in 1992.\"<p>{{cite journal|last=Potra|first=Florian A.|author2=Stephen J. Wright|title=Interior-point methods|journal=Journal of Computational and Applied Mathematics|volume=124|year=2000|issue=1–2|pages=281–302|doi=10.1016/S0377-0427(00)00433-7}}</ref> Its corrector step uses the same [[Cholesky decomposition]] found during the predictor step in an effective way, and thus it is only marginally more expensive than a standard interior point algorithm. However, the additional overhead per iteration is usually paid off by a reduction in the number of iterations needed to reach an optimal solution. It also appears to converge very fast when close to the optimum.\n\n== Derivation ==\nThe derivation of this section follows the outline by Nocedal and Wright.<ref name=\":0\">{{Cite book|title=Numerical Optimisation|last=Nocedal|first=Jorge|last2=Wright|first2=Stephen J.|publisher=Springer|year=2006|isbn=978-0387-30303-1|location=United States of America|pages=392–417, 448–496}}</ref>\n\n=== Predictor step - Affine scaling direction ===\nA linear program can always be formulated in the standard form\n\n<math>\\begin{align}\n&\\underset{x}{\\min}&q(x) &= c^Tx,\\\\\n&\\text{s.t.}&Ax&=b,\\\\\n           &\\;& x&\\geq0,\n\\end{align}</math>\n\nwhere <math>c\\in\\mathbb{R}^{n \\times 1},\\;A\\in\\mathbb{R}^{m \\times n} </math> and <math>b\\in\\mathbb{R}^{m \\times 1}</math> define the problem with <math>m </math> constraints and <math>n </math> equations while <math>x\\in\\mathbb{R}^{n \\times 1} </math> is a vector of variables.\n\nThe [[Karush–Kuhn–Tucker conditions|Karush-Kuhn-Tucker (KKT) conditions]] for the problem are\n\n<math>\\begin{align}\nA^T\\lambda + s &= c,\\;\\;\\;\\text{(Lagrange gradient condition)}\\\\\nAx &= b,\\;\\;\\;\\text{(Feasibility condition)}\\\\\nXSe &= 0,\\;\\;\\;\\text{(Complementarity condition)}\\\\\n(x,s) &\\geq 0,\n\\end{align}</math>\n\nwhere <math>X=\\text{diag}(x)</math> and <math>S=\\text{diag}(s)</math> whence <math>e=(1,1,\\dots,1)^T\\in\\mathbb{R}^{n \\times 1}</math>.\n\nThese conditions can be reformulated as a mapping <math>F: \\mathbb{R}^{2n+m}\\rightarrow\\mathbb{R}^{2n+m}</math> as follows\n\n<math>\\begin{align}\nF(x,\\lambda,s) = \\begin{bmatrix} A^T\\lambda+s-c\\\\Ax-b\\\\XSe\\end{bmatrix} &= 0\\\\\n(x,s)&\\geq0\n\\end{align}</math>\n\nThe predictor-corrector method then works by using Newton's method to obtain the affine scaling direction. This is achieved by solving the following system of linear equations\n\n<math>J(x,\\lambda,s) \\begin{bmatrix} \\Delta x^\\text{aff}\\\\\\Delta\\lambda^\\text{aff} \\\\\\Delta s^\\text{aff}\\end{bmatrix} = -F(x,\\lambda,s)</math>\n\nwhere <math>J</math>, defined as\n\n<math>J(x,\\lambda,s) = \\begin{bmatrix} \\nabla_x F & \\nabla_\\lambda F & \\nabla_s F\\end{bmatrix},</math>\n\nis the Jacobian of F.\n\nThus, the system becomes\n\n<math>\\begin{bmatrix} 0 & A^T & I \\\\ A & 0 & 0 \\\\ S & 0 & X \\end{bmatrix}\\begin{bmatrix} \\Delta x^\\text{aff}\\\\\\Delta\\lambda^\\text{aff} \\\\\\Delta s^\\text{aff}\\end{bmatrix} = \\begin{bmatrix}-r_c\\\\-r_b\\\\-XSe\\end{bmatrix},\\;\\;\\; r_c = A^T\\lambda+s-c,\\;\\;\\; r_b = Ax-b</math>\n\n=== Centering step ===\nThe average value of the products <math>x_is_i,\\;i=1,2,\\dots,n</math> constitute an important measure of the desirability of a certain set <math>(x^k,s^k)</math> (the superscripts denote the value the iteration number, <math>k</math>, of the method). This is called the duality measure and is defined by\n\n<math>\\mu=\\frac{1}{n}\\sum_{i=1}^n x_is_i = \\frac{x^Ts}{n}.</math>\n\nFor a value of the centering parameter, <math>\\sigma\\in[0,1],</math> the centering step can be computed as the solution to\n\n<math>\\begin{bmatrix} 0 & A^T & I \\\\ A & 0 & 0 \\\\ S & 0 & X \\end{bmatrix}\n\\begin{bmatrix} \\Delta x^\\text{cen}\\\\\\Delta\\lambda^\\text{cen} \\\\\\Delta s^\\text{cen}\\end{bmatrix}\n= \\begin{bmatrix}-r_c\\\\-r_b\\\\-XSe+\\sigma\\mu e\\end{bmatrix}</math>\n\n=== Corrector step ===\nConsidering the system used to compute the affine scaling direction defined in the above, one can note that taking a full step in the affine scaling direction results in the complementarity condition not being satisfied:\n\n<math>\\left(x_i+\\Delta x_i^\\text{aff}\\right)\\left(s_i+\\Delta s_i^\\text{aff}\\right) = x_is_i + x_i\\Delta s_i^\\text{aff} + s_i\\Delta x_i^\\text{aff} + \\Delta x_i^\\text{aff}\\Delta s_i^\\text{aff} = \\Delta x_i^\\text{aff}\\Delta s_i^\\text{aff} \\ne 0.</math>\n\nAs such, a system can be defined to compute a step that attempts to correct for this error. This system relies on the previous computation of the affine scaling direction.\n\n<math>\\begin{bmatrix} 0 & A^T & I \\\\ A & 0 & 0 \\\\ S & 0 & X \\end{bmatrix}\n\\begin{bmatrix} \\Delta x^\\text{cor}\\\\\\Delta\\lambda^\\text{cor} \\\\\\Delta s^\\text{cor}\\end{bmatrix}\n= \\begin{bmatrix}0\\\\0\\\\-\\Delta X^\\text{aff}\\Delta S^\\text{aff}e\\end{bmatrix}</math>\n\n=== Aggregated system - Center-corrector direction ===\nThe predictor, corrector and centering contributions to the system right hand side can be aggregated into a single system. This system will depend on the previous computation of the affine scaling direction, however, the system matrix will be identical to that of the predictor step such that its factorization can be reused.\n\nThe aggregated system is\n\n<math>\\begin{bmatrix} 0 & A^T & I \\\\ A & 0 & 0 \\\\ S & 0 & X \\end{bmatrix}\n\\begin{bmatrix} \\Delta x\\\\\\Delta\\lambda \\\\\\Delta s\\end{bmatrix}\n= \\begin{bmatrix}-r_c\\\\-r_b\\\\-XSe-\\Delta X^\\text{aff}\\Delta S^\\text{aff}e+\\sigma\\mu e\\end{bmatrix}</math>\n\nThe predictor-corrector algorithm then first computes the affine scaling direction. Secondly, it solves the aggregated system to obtain the search direction of the current iteration.\n\n== Adaptive selection of centering parameter ==\nThe affine scaling direction can be used to define a heuristic to adaptively choose the centering parameter as\n\n<math>\\sigma = \\left(\\frac{\\mu_\\text{aff}}{\\mu}\\right)^3,</math>\n\nwhere\n\n<math>\\begin{align}\n\\mu_\\text{aff} &= (x+\\alpha^\\text{pri}_\\text{aff}\\Delta x^\\text{aff})^T(s+\\alpha^\\text{dual}_\\text{aff}\\Delta s^\\text{aff})^T/n,\\\\\n\\alpha^\\text{pri}_\\text{aff} &= \\min\\left(1, \\underset{i:\\Delta x_i^\\text{aff}<0}{\\min} -\\frac{x_i}{\\Delta x_i^\\text{aff}}\\right),\\\\\n\\alpha^\\text{dual}_\\text{aff} &= \\min\\left(1, \\underset{i:\\Delta s_i^\\text{aff}<0}{\\min} -\\frac{s_i}{\\Delta s_i^\\text{aff}}\\right),\n\\end{align}</math>\n\nHere, <math>\\mu_\\text{aff}</math> is the duality measure of the affine step and <math>\\mu</math> is the duality measure of the previous iteration.<ref name=\":0\" />\n\n== Step lengths ==\nIn practical implementations, a version of line search is performed to obtain the maximal step length that can be taken in the search direction without violating nonnegativity, <math>(x,s) \\geq 0</math>.<ref name=\":0\" />\n\n== Adaptation to Quadratic Programming ==\nAlthough the modifications presented by Mehrotra were intended for interior point algorithms for linear programming, the ideas have been extended and successfully applied to [[quadratic programming]] as well.<ref name=\":0\" />\n\n==References==\n\n<references/>\n\n{{DEFAULTSORT:Mehrotra predictor-corrector method}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Minimax",
      "url": "https://en.wikipedia.org/wiki/Minimax",
      "text": "{{about|the decision theory concept}}\n'''Minimax''' (sometimes '''MinMax''', '''MM'''<ref>[http://www.fraserinstitute.org/uploadedFiles/fraser-ca/Content/research-news/research/publications/provincial-healthcare-index-2013.pdf Provincial Healthcare Index 2013] (Bacchus Barua, Fraser Institute, January 2013 -see page 25-)</ref> or '''saddle point'''<ref>[https://www.youtube.com/watch?v=fJltiCjPeMA&t=12m0s Turing and von Neumann - Professor Raymond Flood - Gresham College at 12:00]</ref>) is a decision rule used in [[artificial intelligence]], [[decision theory]], [[game theory]], [[statistics]] and [[philosophy]] for ''mini''mizing the possible [[loss function|loss]] for a [[Worst-case scenario|worst case (''max''imum loss) scenario]].  When dealing with gains, it is referred to as \"maximin\"—to maximize the minimum gain.  Originally formulated for two-player [[zero-sum]] [[game theory]], covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.\n\n==Game theory ==\n\n=== In general games ===\nThe '''maximin value''' of a player is the highest value that the player can be sure to get without knowing the actions of the other players; equivalently, it is the lowest value the other players can force the player to receive when they know the player's action. Its formal definition is:<ref name=ZMS2013>{{cite book |title=Game Theory |authors=Michael Maschler, [[Eilon Solan]] & Shmuel Zamir |publisher=[[Cambridge University Press]] |isbn=9781107005488 |year=2013 |pages=176–180}}</ref>\n\n:<math>\\underline{v_i} = \\max_{a_i} \\min_{a_{-i}} {v_i(a_i,a_{-i})}</math>\n\nWhere:\n* {{mvar|i}} is the index of the player of interest.\n* <math>-i</math> denotes all other players except player {{mvar|i}}.\n* <math>a_i</math> is the action taken by player {{mvar|i}}.\n* <math>a_{-i}</math> denotes the actions taken by all other players.\n* <math>v_i</math> is the value function of player {{mvar|i}}.\n\nCalculating the maximin value of a player is done in a worst-case approach: for each possible action of the player, we check all possible actions of the other players and determine the worst possible combination of actions—the one that gives player {{mvar|i}} the smallest value. Then, we determine which action player {{mvar|i}} can take in order to make sure that this smallest value is the highest possible.\n\nFor example, consider the following game for two players, where the first player (\"row player\") may choose any of three moves, labelled {{mvar|T}}, {{mvar|M}}, or {{mvar|B}}, and the second player (\"column\" player) may choose either of two moves, {{mvar|L}} or {{mvar|R}}. The result of the combination of both moves is expressed in a payoff table:\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n!   !! {{mvar|L}} !! {{mvar|R}}\n|-\n| {{mvar|T}} || 3,1    || 2,-20\n|-\n| {{mvar|M}} || 5,0    || -10,1\n|-\n| {{mvar|B}} || -100,2 || 4,4\n|}\n(where the first number in each cell is the pay-out of the row player and the second number is the pay-out of the column player).\n\nFor the sake of example, we consider only pure strategies. Check each player in turn:\n* The row player can play {{mvar|T}}, which guarantees them a payoff of at least {{mvar|2}} (playing {{mvar|B}} is risky since it can lead to payoff {{val|-100}}, and playing {{mvar|M}} can result in a payoff of {{val|-10}}). Hence: <math>\\underline{v_{row}} = 2</math>.\n* The column player can play {{mvar|L}} and secure a payoff of at least {{val|0}} (playing {{mvar|R}} puts them in the risk of getting <math>-20</math>). Hence: <math>\\underline{v_{col}} = 0</math>.\n\nIf both players play their respective maximin strategies <math>(T,L)</math>, the payoff vector is <math>(3,1)</math>.\n\nThe '''minimax value''' of a player is the smallest value that the other players can force the player to receive, without knowing the player's actions; equivalently, it is the largest value the player can be sure to get when they ''know'' the actions of the other players. Its formal definition is:<ref name=ZMS2013/>\n\n:<math>\\overline{v_i} = \\min_{a_{-i}} \\max_{a_i} {v_i(a_i,a_{-i})}</math>\n\nThe definition is very similar to that of the maximin value—only the order of the maximum and minimum operators is inverse. In the above example:\n* The row player can get a maximum value of {{mvar|4}} (if the other player plays {{mvar|R}}) or {{mvar|5}} (if the other player plays {{mvar|L}}), so: <math>\\overline{v_{row}} = 4</math>.\n* The column player can get a maximum value of {{mvar|1}} (if the other player plays {{mvar|T}}), {{mvar|1}} (if {{mvar|M}}) or {{mvar|4}} (if {{mvar|B}}). Hence: <math>\\overline{v_{col}} = 1</math>.\n\nFor every player {{mvar|i}}, the maximin is at most the minimax:\n:<math>\\underline{v_i} \\leq \\overline{v_i}</math>\nIntuitively, in maximin the maximization comes before the minimization, so player {{mvar|i}} tries to maximize their value before knowing what the others will do; in minimax the maximization comes after the minimization, so player {{mvar|i}} is in a much better position—they maximize their value knowing what the others did.\n\nAnother way to understand the ''notation'' is by reading from right to left: when we write\n:<math>\\overline{v_i} = \\min_{a_{-i}} \\max_{a_i} {v_i(a_i,a_{-i})} = \\min_{a_{-i}} \\Big( \\max_{a_i} {v_i(a_i,a_{-i})} \\Big) </math>\nthe initial set of outcomes <math>v_i(a_i,a_{-i})</math> depends on both <math>{a_{i}}</math> and <math>{a_{-i}}</math>.  We first ''marginalize away'' <math>{a_{i}}</math> from <math>v_i(a_i,a_{-i})</math>, by maximizing over <math>{a_{i}}</math> (for every possible value of <math>{a_{-i}}</math>) to yield a set of marginal outcomes <math>v'_i(a_{-i})</math>, which depends only on <math>{a_{-i}}</math>.  We then minimize over <math>{a_{-i}}</math> over these outcomes.  (Conversely for maximin.)\n\nAlthough it is always the case that <math>\\underline{v_{row}} \\leq \\overline{v_{row}}</math> and <math>\\underline{v_{col}} \\leq \\overline{v_{col}}</math>, the payoff vector resulting from both players playing their minimax strategies, <math>(2,-20)</math> in the case of <math>(T,R)</math> or <math>(-10,1)</math> in the case of <math>(M,R)</math>, cannot similarly be ranked against the payoff vector <math>(3,1)</math> resulting from both players playing their maximin strategy.\n\n=== In zero-sum games ===\n<span id='Minimax theorem'></span><!-- added label in order not to break incoming links -->\nIn two-player [[Zero-sum_game|zero-sum games]], the minimax solution is the same as the [[Nash equilibrium]].\n\nIn the context of zero-sum games, the [[minimax theorem]] is equivalent to:<ref name=Osborne>Osborne, Martin J., and [[Ariel Rubinstein]]. ''A Course in Game Theory''. Cambridge, MA: MIT, 1994. Print.</ref>{{Failed verification|date=February 2015}}\n\n<blockquote>For every two-person, [[zero-sum]] game with finitely many strategies, there exists a value V and a mixed strategy for each player, such that\n:(a) Given player 2's strategy, the best payoff possible for player 1 is V, and\n:(b) Given player 1's strategy, the best payoff possible for player 2 is −V.\n</blockquote>\nEquivalently, Player 1's strategy guarantees them a payoff of V regardless of Player 2's strategy, and similarly Player 2 can guarantee themselves a payoff of −V.  The name minimax arises because each player minimizes the maximum payoff possible for the other—since the game is zero-sum, they also minimize their own maximum loss (i.e. maximize their minimum payoff).\nSee also [[example of a game without a value]].\n\n===Example===\n{| class=\"wikitable\" style=\"text-align:center; float:right; margin-left:1em\"\n ! \n ! B chooses B1\n ! B chooses B2\n ! B chooses B3\n |-\n ! A chooses A1\n |  +3\n |  −2\n |  +2\n |-\n ! A chooses A2\n |  −1\n |  {{0|+}}0\n |  +4\n |-\n ! A chooses A3\n |  −4\n |  −3\n |  +1\n|}\nThe following example of a zero-sum game, where '''A''' and '''B''' make simultaneous moves, illustrates ''minimax'' solutions. Suppose each player has three choices and consider the [[payoff matrix]] for '''A''' displayed on the right. Assume the payoff matrix for '''B''' is the same matrix with the signs reversed (i.e. if the choices are A1 and B1 then '''B''' pays 3 to '''A'''). Then, the minimax choice for '''A''' is A2 since the worst possible result is then having to pay 1, while the simple minimax choice for '''B''' is B2 since the worst possible result is then no payment.  However, this solution is not stable, since if '''B''' believes '''A''' will choose A2 then '''B''' will choose B1 to gain 1; then if '''A''' believes '''B''' will choose B1 then '''A''' will choose A1 to gain 3; and then '''B''' will choose B2; and eventually both players will realize the difficulty of making a choice. So a more stable strategy is needed.\n\nSome choices are ''dominated'' by others and can be eliminated: '''A''' will not choose A3 since either A1 or A2 will produce a better result, no matter what '''B''' chooses; '''B''' will not choose B3 since some mixtures of B1 and B2 will produce a better result, no matter what '''A''' chooses.\n\n'''A''' can avoid having to make an expected payment of more than 1∕3 by choosing A1 with probability 1∕6 and A2 with probability 5∕6: The expected payoff for '''A''' would be 3 × (1∕6) − 1 × (5∕6) = −1∕3 in case '''B''' chose B1 and −2 × (1∕6) + 0 × (5∕6) = −1/3 in case '''B''' chose B2.  Similarly, '''B''' can ensure an expected gain of at least 1/3, no matter what '''A''' chooses, by using a randomized strategy of choosing B1 with probability 1∕3 and B2 with probability 2∕3. These [[mixed strategy|mixed]] minimax strategies are now stable and cannot be improved.\n\n===Maximin===\nFrequently, in game theory, '''maximin''' is distinct from minimax. Minimax is used in zero-sum games to denote minimizing the opponent's maximum payoff. In a [[zero-sum game]], this is identical to minimizing one's own maximum loss, and to maximizing one's own minimum gain.\n\n\"Maximin\" is a term commonly used for non-zero-sum games to describe the strategy which maximizes one's own minimum payoff. In non-zero-sum games, this is not generally the same as minimizing the opponent's maximum gain, nor the same as the [[Nash equilibrium]] strategy.\n\n=== In repeated games ===\nThe minimax values are very important in the theory of [[repeated games]]. One of the central theorems in this theory, the [[folk theorem (game theory)|folk theorem]], relies on the minimax values.\n\n==Combinatorial game theory==\n\nIn [[combinatorial game theory]], there is a minimax algorithm for game solutions.\n\nA '''simple''' version of the minimax ''algorithm'', stated below, deals with games such as [[tic-tac-toe]], where each player can win, lose, or draw.\nIf player A ''can'' win in one move, their best move is that winning move.\nIf player B knows that one move will lead to the situation where player A ''can'' win in one move, while another move will lead to the situation where player A can, at best, draw, then player B's best move is the one leading to a draw.\nLate in the game, it's easy to see what the \"best\" move is.\nThe Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to '''maximize''' the chances of A winning, while on the next turn player B is trying to '''minimize''' the chances of A winning (i.e., to maximize B's own chances of winning).\n\n===Minimax algorithm with alternate moves===<!-- This section is linked from [[Alpha-beta pruning]] -->\n\nA '''minimax algorithm'''<ref>{{Russell Norvig 2003|pages=163–171}}</ref> is a recursive [[algorithm]] for choosing the next move in an n-player [[game theory|game]], usually a two-player game. A value is associated with each position or state of the game. This value is computed by means of a [[evaluation function|position evaluation function]] and it indicates how good it would be for a player to reach that position. The player then makes the move that maximizes the minimum value of the position resulting from the opponent's possible following moves. If it is '''A'''<nowiki>'s</nowiki> turn to move, '''A''' gives a value to each of their legal moves.\n\nA possible allocation method consists in assigning a certain win for '''A''' as +1 and for '''B''' as −1.  This leads to [[combinatorial game theory]] as developed by [[John Horton Conway]]. An alternative is using a rule that if the result of a move is an immediate win for '''A''' it is assigned positive infinity and, if it is an immediate win for '''B''', negative infinity. The value to '''A''' of any other move is the minimum of the values resulting from each of '''B'''<nowiki>'s</nowiki> possible replies. For this reason, '''A''' is called the ''maximizing player'' and '''B''' is called the ''minimizing player'', hence the name ''minimax algorithm''. The above algorithm will assign a value of positive or negative infinity to any position since the value of every position will be the value of some final winning or losing position.  Often this is generally only possible at the very end of complicated games such as [[chess]] or [[Go (board game)|go]], since it is not computationally feasible to look ahead as far as the completion of the game, except towards the end, and instead positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another.\n\nThis can be extended if we can supply a [[heuristic]] evaluation function which gives values to non-final game states without considering all possible following complete sequences. We can then limit the minimax algorithm to look only at a certain number of moves ahead. This number is called the \"look-ahead\", measured in \"[[Ply (chess)|plies]]\". For example, the chess computer [[IBM Deep Blue|Deep Blue]] (the first one to beat a reigning world champion, [[Garry Kasparov]] at that time) looked ahead at least 12 plies, then applied a heuristic evaluation function.<ref>{{citation\n | last = Hsu | first = Feng-hsiung\n | doi = 10.1109/40.755469\n | issue = 2\n | journal = IEEE Micro\n | location = Los Alamitos, CA, USA\n | quote = During the 1997 match, the software search extended the search to about 40 plies along the forcing lines, even though the nonextended search reached only about 12 plies.\n | pages = 70–81\n | publisher = IEEE Computer Society\n | title = IBM's Deep Blue Chess Grandmaster Chips\n | volume = 19\n | year = 1999}}</ref>\n\nThe algorithm can be thought of as exploring the [[node (computer science)|node]]s of a ''[[game tree]]''. The ''effective [[branching factor]]'' of the tree is the average number of [[child node|children]] of each node (i.e., the average number of legal moves in a position).  The number of nodes to be explored usually [[exponential growth|increases exponentially]] with the number of plies (it is less than exponential if evaluating [[forced move]]s or repeated positions). The number of nodes to be explored for the analysis of a game is therefore approximately the branching factor raised to the power of the number of plies. It is therefore [[Computational complexity theory#Intractability|impractical]] to completely analyze games such as chess using the minimax algorithm.\n\nThe performance of the naïve minimax algorithm may be improved dramatically, without affecting the result, by the use of [[alpha-beta pruning]].\nOther heuristic pruning methods can also be used, but not all of them are  guaranteed to give the same result as the un-pruned search.\n\nA naive minimax algorithm may be trivially modified to additionally return an entire [[Variation (game tree)#Principal variation|Principal Variation]] along with a minimax score.\n\n=== Pseudocode ===\n\nThe [[pseudocode]] for the depth limited minimax algorithm is given below.\n\n '''function''' minimax(node, depth, maximizingPlayer) '''is'''\n     '''if''' depth = 0 '''or''' node is a terminal node '''then'''\n         '''return''' the heuristic value of node\n     '''if''' maximizingPlayer '''then'''\n         value := &minus;∞\n         '''for each''' child of node '''do'''\n             value := max(value, minimax(child, depth &minus; 1, FALSE))\n         '''return''' value\n     '''else''' ''(* minimizing player *)''\n         value := +∞\n         '''for each''' child of node '''do'''\n             value := min(value, minimax(child, depth &minus; 1, TRUE))\n         '''return''' value\n\n ''(* Initial call *)''\n minimax(origin, depth, TRUE)\n\nThe minimax function returns a heuristic value for [[leaf nodes]] (terminal nodes and nodes at the maximum search depth).\nNon leaf nodes inherit their value from a descendant leaf node.\nThe heuristic value is a score measuring the favorability of the node for the maximizing player.\nHence nodes resulting in a favorable outcome, such as a win, for the maximizing player have higher scores than nodes more favorable for the minimizing player.\nThe heuristic value for terminal (game ending) leaf nodes are scores corresponding to win, loss, or draw, for the maximizing player.\nFor non terminal leaf nodes at the maximum search depth, an evaluation function estimates a heuristic value for the node.\nThe quality of this estimate and the search depth determine the quality and accuracy of the final minimax result.\n\nMinimax treats the two players (the maximizing player and the minimizing player) separately in its code. Based on the observation that <math>\\max(a,b) = -\\min(-a,-b)</math>, minimax may often be simplified into the [[negamax]] algorithm.\n\n=== Example ===\n\n[[Image:Minimax.svg|thumb|400px|A minimax tree example]]\n[[File:Plminmax.gif|thumb|400px|An animated pedagogical example that attempts to be human-friendly by substituting initial infinite (or arbitrarily large) values for emptiness and by avoiding using the [[negamax]] coding simplifications.]]\n\nSuppose the game being played only has a maximum of two possible moves per player each turn. The algorithm generates the [[game tree|tree]] on the right, where the circles represent the moves of the player running the algorithm (''maximizing player''), and squares represent the moves of the opponent (''minimizing player''). Because of the limitation of computation resources, as explained above, the tree is limited to a ''look-ahead'' of 4 moves.\n\nThe algorithm evaluates each ''[[leaf node]]'' using a heuristic evaluation function, obtaining the values shown. The moves where the ''maximizing player'' wins are assigned with positive infinity, while the moves that lead to a win of the ''minimizing player'' are assigned with negative infinity. At level 3, the algorithm will choose, for each node, the '''smallest''' of the ''[[child node]]'' values, and assign it to that same node (e.g. the node on the left will choose the minimum between \"10\" and \"+∞\", therefore assigning the value \"10\" to itself). The next step, in level 2, consists of choosing for each node the '''largest''' of the ''child node'' values. Once again, the values are assigned to each ''[[parent node]]''. The algorithm continues evaluating the maximum and minimum values of the child nodes alternately until it reaches the ''[[root node]]'', where it chooses the move with the largest value (represented in the figure with a blue arrow). This is the move that the player should make in order to ''minimize'' the ''maximum'' possible [[loss function|loss]].\n\n==Minimax for individual decisions==\n\n===Minimax in the face of uncertainty===\n\nMinimax theory has been extended to decisions where there is no other player, but where the consequences of decisions depend on unknown facts.  For example, deciding to prospect for minerals entails a cost which will be wasted if the minerals are not present, but will bring major rewards if they are.  One approach is to treat this as a game against ''nature'' (see [[move by nature]]), and using a similar mindset as [[Murphy's law]] or [[resistentialism]], take an approach which minimizes the maximum expected loss, using the same techniques as in the two-person zero-sum games.\n\nIn addition, [[expectiminimax tree]]s have been developed, for two-player games in which chance (for example, dice) is a factor.\n\n===Minimax criterion in statistical decision theory===\n{{main article|Minimax estimator}}\nIn classical statistical [[decision theory]], we have an [[estimator]] <math>\\delta</math> that is used to estimate a [[parameter]] <math>\\theta \\in \\Theta</math>. We also assume a [[risk function]] <math>R(\\theta,\\delta)</math>, usually specified as the integral of a [[loss function]]. In this framework, <math>\\tilde{\\delta}</math> is called '''minimax''' if it satisfies\n\n: <Math>\\sup_\\theta R(\\theta,\\tilde{\\delta}) = \\inf_\\delta \\sup_\\theta R(\\theta,\\delta).</math>\n\nAn alternative criterion in the decision theoretic framework is the [[Bayes estimator]] in the presence of a [[prior distribution]] <math>\\Pi</math>. An estimator is Bayes if it minimizes the ''[[average]]'' risk\n\n: <Math>\\int_\\Theta R(\\theta,\\delta)\\,d\\Pi(\\theta).</math>\n\n=== Non-probabilistic decision theory ===\nA key feature of minimax decision making is being non-probabilistic: in contrast to decisions using [[expected value]] or [[expected utility]], it makes no assumptions about the probabilities of various outcomes, just [[scenario analysis]] of what the possible outcomes are. It is thus [[:wikt:robust|robust]] to changes in the assumptions, as these other decision techniques are not. Various extensions of this non-probabilistic approach exist, notably [[minimax regret]] and [[Info-gap decision theory]].\n\nFurther, minimax only requires [[ordinal measurement]] (that outcomes be compared and ranked), not ''interval'' measurements (that outcomes include \"how much better or worse\"), and returns ordinal data, using only the modeled outcomes: the conclusion of a minimax analysis is: \"this strategy is minimax, as the worst case is (outcome), which is less bad than any other strategy\". Compare to expected value analysis, whose conclusion is of the form: \"this strategy yields E(''X'')=''n.''\" Minimax thus can be used on ordinal data, and can be more transparent.\n\n== Maximin in philosophy ==\n\nIn philosophy, the term \"maximin\" is often used in the context of [[John Rawls]]'s ''[[A Theory of Justice]],'' where he refers to it (Rawls (1971, p.&nbsp;152)) in the context of The [[Difference Principle]].\nRawls defined this principle as the rule which states that social and economic inequalities should be arranged so that \"they are to be of the greatest benefit to the least-advantaged members of society\".<ref>[[Kenneth Arrow|Arrow]], \"[https://www.pdcnet.org/jphil/content/jphil_1973_0070_0009_0245_0263 Some Ordinalist-Utilitarian Notes on Rawls's Theory of Justice], Journal of Philosophy 70, 9 (May 1973), pp. 245-263.</ref><ref>[[John Harsanyi|Harsanyi]], \"[http://piketty.pse.ens.fr/files/Harsanyi1975.pdf Can the Maximin Principle Serve as a Basis for Morality?] a Critique of John Rawls's Theory, American Political Science Review 69, 2 (June 1975), pp. 594-606.</ref>\n\n== See also ==\n{{div col|colwidth=20em}} \n* [[Alpha-beta pruning]]\n* [[Expectiminimax]]\n* [[Negamax]]\n* [[Sion's minimax theorem]]\n* [[Minimax Condorcet]]\n* [[Computer chess]]\n* [[Horizon effect]]\n* [[Monte Carlo tree search]]\n* [[Regret (decision theory)|Minimax regret]]\n* [[Negascout]]\n* [[Tit for Tat]]\n* [[Transposition table]]\n* [[Wald's maximin model]]\n{{div col end}}\n\n==Notes==\n{{reflist}}\n\n== External links ==\n{{Wiktionary}}\n* {{springer|title=Minimax principle|id=p/m063950}}\n* [http://www.cut-the-knot.org/Curriculum/Games/MixedStrategies.shtml A visualization applet]\n* [http://www.swif.uniba.it/lei/foldop/foldoc.cgi?maximin+principle Maximin principle] at Dictionary of Philosophical Terms and Names\n* [http://www.bewersdorff-online.de/quaak/rules.htm Play a betting-and-bluffing game against a mixed minimax strategy]\n* [https://xlinux.nist.gov/dads/HTML/minimax.html Minimax] at [[Dictionary of Algorithms and Data Structures]]\n* [http://ksquared.de/gamevisual/launch.php Minimax] (with or without alpha-beta pruning) algorithm visualization &mdash; game tree solving (Java Applet), for balance or off-balance trees.\n* [http://apmonitor.com/me575/index.php/Main/MiniMax Minimax Tutorial with a Numerical Solution Platform]\n* [https://github.com/ykaragol/checkersmaster/blob/master/CheckersMaster/src/checkers/algorithm/MinimaxAlgorithm.java Java implementation used in a Checkers Game]\n\n{{Game theory}}\n\n[[Category:Detection theory]]\n[[Category:Game artificial intelligence]]\n[[Category:Graph algorithms]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Search algorithms]]\n[[Category:Game theory]]\n[[Category:Theorems in discrete mathematics]]\n[[Category:Decision theory]]\n[[Category:Fixed points (mathematics)]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "MM algorithm",
      "url": "https://en.wikipedia.org/wiki/MM_algorithm",
      "text": "The '''MM algorithm''' is an iterative [[optimization]] method which exploits the [[convex function|convexity]] of a function in order to find their maxima or minima. The MM stands for “Majorize-Minimization” or “Minorize-Maximization”, depending on whether the desired optimization is a maximization or a minimization. MM itself is not an algorithm, but a description of how to construct an [[optimization algorithm]].\n\nThe [[expectation–maximization algorithm]] can be treated as a special case of the MM algorithm.<ref>{{cite web|last=Lange|first=Kenneth|title=The MM Algorithm|url=http://www.stat.berkeley.edu/~aldous/Colloq/lange-talk.pdf}}</ref><ref>Kenneth Lange: \"MM Optimization Algorithms\", SIAM, {{ISBN|978-1-611974-39-3}}  (2016).</ref>\nHowever, in the EM algorithm [[conditional expectation]]s are usually involved, while in the MM algorithm convexity and inequalities are the main focus, and it is easier to understand and apply in most cases.\n\n==History==\nThe historical basis for the MM algorithm can be dated back to at least 1970, when Ortega and Rheinboldt were performing studies related to [[line search]] methods.<ref>{{cite book\n |last1=Ortega |first1=J.M.\n |last2=Rheinboldt|first2=W.C.\n |title=Iterative Solutions of Nonlinear Equations in Several Variables\n |location=New York |publisher=Academic\n |year=1970  |pages=253–255\n |url=https://books.google.com/books?id=GA1P9UNnrmMC&pg=PA253\n|isbn=9780898719468\n }}</ref> The same concept continued to reappear in different areas in different forms. In 2000, Hunter and Lange put forth \"MM\" as a general framework.<ref>{{cite journal\n |last1=Hunter|first1=D.R.\n |last2=Lange|first2=K.\n |title=Quantile Regression via an MM Algorithm\n |journal=[[Journal of Computational and Graphical Statistics]]\n |year=2000 |volume=9 |issue=1\n |pages=60–77\n |doi=10.2307/1390613\n|jstor=1390613\n |citeseerx=10.1.1.206.1351\n }}</ref> Recent studies{{who?|date=September 2018}} have applied the method in a wide range of subject areas, such as [[mathematics]], [[statistics]], [[machine learning]] and [[engineering]].{{cn|date=September 2018}}\n\n==Algorithm==\nThe MM algorithm works by finding a surrogate function that minorizes or majorizes the objective function. Optimizing the surrogate function will drive the objective function upward or downward until a local [[optimum]] is reached.\n\nTaking the minorize-maximization version, let <math> f(\\theta) </math> be the objective concave function to be maximized. At the {{mvar|m}} step of the algorithm, <math> m=0,1... </math>, the constructed function <math> g(\\theta|\\theta_m) </math> will be called the minorized version of the objective function (the surrogate function) at <math> \\theta_m </math> if\n\n:    <math> g(\\theta|\\theta_m) \\le f(\\theta) \\text{ for all } \\theta </math>\n:    <math> g(\\theta_m|\\theta_m)=f(\\theta_m) </math>\n\nThen, maximize <math> g(\\theta|\\theta_m) </math> instead of <math> f(\\theta) </math>, and let\n\n:    <math> \\theta_{m+1}=\\arg\\max_{\\theta}g(\\theta|\\theta_m) </math>\n\nThe above iterative method will guarantee that <math> f(\\theta_m) </math> will converge to a local optimum or a saddle point as {{mvar|m}} goes to infinity.<ref>{{cite journal |last=Wu |first=C. F. Jeff |year=1983 |title=On the Convergence Properties of the EM Algorithm |journal=[[Annals of Statistics]] |volume=11 |issue=1 |pages=95–103 |doi= 10.1214/aos/1176346060|jstor=2240463 }}</ref> By the above construction\n:   <math> f(\\theta_{m+1}) \\ge g(\\theta_{m+1}|\\theta_m) \\ge g(\\theta_m|\\theta_m)= f(\\theta_m)</math>\n\nThe marching of <math>\\theta_m </math> and the surrogate functions relative to the objective function is shown in the figure. [[File:Mmalgorithm.jpg|right|thumb|MM algorithm]]\n\nMajorize-Minimization is the same procedure but with a convex objective to be minimised.\n\n==Constructing the surrogate function==\nOne can use any inequality to construct the desired majorized/minorized version of the objective function. Typical choices include\n* [[Jensen's inequality]]\n* [[Convexity inequality]]\n* [[Cauchy–Schwarz inequality]]\n* [[Inequality of arithmetic and geometric means]]\n* Quadratic majorization/mininorization via second order [[Taylor expansion]] of twice-differentiable functions with bounded curvature.\n\n==References==\n{{reflist}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Natural evolution strategy",
      "url": "https://en.wikipedia.org/wiki/Natural_evolution_strategy",
      "text": "{{Evolutionary algorithms}}\n{{no footnotes|date=March 2015}}\n'''Natural evolution strategies''' ('''NES''') are a family of [[numerical optimization]] algorithms for [[black box]] problems. Similar in spirit to [[evolution strategies]], they iteratively update the (continuous) parameters of a ''search distribution'' by following the [[information geometry#Natural gradient|natural gradient]] towards higher expected fitness.\n\n==Method==\n\nThe general procedure is as follows: the ''parameterized'' search distribution is used to produce a batch of search points, and the [[fitness function]] is evaluated at each such point. The distribution’s parameters (which include ''strategy parameters'') allow the algorithm to adaptively capture the (local) structure of the fitness function. For example, in the case of a [[Gaussian distribution]], this comprises the mean and the [[covariance matrix]]. From the samples, NES estimates a search gradient on the parameters towards higher expected fitness. NES then performs a gradient ascent step along the '''[[Information_geometry#Natural_gradient|natural gradient]]''', a second order method which, unlike the plain gradient, renormalizes the update w.r.t. uncertainty. This step is crucial, since it prevents oscillations, premature convergence, and undesired effects stemming from a given parameterization. The entire process reiterates until a stopping criterion is met.\n\nAll members of the NES family operate based on the same principles. They differ in the type of [[probability distribution]] and the gradient [[approximation]] method used. Different search spaces require different search distributions; for example, in low dimensionality it can be highly beneficial to model the full covariance matrix. In high dimensions, on the other hand, a more scalable alternative is to limit the covariance to the [[diagonal matrix|diagonal]] only. In addition, highly multi-modal search spaces may benefit from more [[Heavy-tailed_distribution|heavy-tailed distributions]] (such as [[Cauchy_distribution|Cauchy]], as opposed to the Gaussian). A last distinction arises between distributions where we can analytically compute the natural gradient, and more general distributions where we need to estimate it from samples.\n\n===Search gradients===\nLet <math>\\theta</math> denote the parameters of the search distribution <math>\\pi(x \\,|\\, \\theta)</math> and <math>f(x)</math> the fitness function evaluated at <math>x</math>. NES then pursues the objective of maximizing the ''expected fitness under the search distribution''\n:: <math>J(\\theta) = \\operatorname{E}_\\theta[f(x)] = \\int f(x) \\; \\pi(x \\,|\\, \\theta) \\; dx</math>\nthrough [[gradient ascent]]. The gradient can be rewritten as\n::<math> \\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} \\int f(x) \\; \\pi(x \\,|\\, \\theta) \\; dx </math>\n:::<math>   = \\int f(x) \\; \\nabla_{\\theta} \\pi(x \\,|\\, \\theta) \\; dx </math>\n:::<math>   = \\int f(x) \\; \\nabla_{\\theta} \\pi(x \\,|\\, \\theta) \\; \\frac{\\pi(x \\,|\\, \\theta)}{\\pi(x \\,|\\, \\theta)} \\; dx </math>\n:::<math>   = \\int \\Big[ f(x) \\; \\nabla_{\\theta} \\log\\pi(x \\,|\\, \\theta) \\Big] \\; \\pi(x \\,|\\, \\theta) \\; dx</math>\n:::<math>   = \\operatorname{E}_\\theta \\left[ f(x) \\; \\nabla_{\\theta} \\log\\pi(x \\,|\\, \\theta)\\right]</math>\nthat is, the [[expected value]] of <math>f(x)</math> times the [[Logarithmic derivative|log-derivatives]] at <math>x</math>. In practice, it is possible to use the [[Monte Carlo method|Monte Carlo]] approximation based on a finite number of <math>\\lambda</math> samples\n::<math>\\nabla_{\\theta} J(\\theta) \\approx \n   \\frac{1}{\\lambda} \n\\sum_{k=1}^{\\lambda} f(x_k) \\; \\nabla_{\\theta} \n\\log\\pi(x_k \\,|\\, \\theta)</math>.\nFinally, the parameters of the search distribution can be updated iteratively\n::<math>\\theta \\leftarrow \\theta + \\eta \\nabla_{\\theta} J(\\theta) </math>\n\n===Natural gradient ascent===\nInstead of using the plain stochastic gradient for updates, NES\nfollows the '''natural gradient''', which has been shown to \npossess numerous advantages over the plain (''vanilla'') gradient, e.g.:\n* the gradient direction is independent of the parameterization of the search distribution \n* the updates magnitudes are automatically adjusted based on uncertainty, in turn speeding convergence on [[Plateau_(mathematics)|plateaus]] and ridges.\n\nThe NES update is therefore\n::<math>\\theta \\leftarrow \\theta + \\eta \\mathbf{F}^{-1}\\nabla_{\\theta} J(\\theta) </math>,\nwhere <math>\\mathbf{F}</math> is the [[Fisher_information#Matrix_form|Fisher information matrix]].\nThe Fisher matrix can sometimes be computed exactly, otherwise it is estimated from samples, reusing the log-derivatives <math>\\nabla_\\theta \\log\\pi(x|\\theta)</math>.\n\n===Fitness shaping===\nNES utilizes [[Ranking|rank]]-based fitness shaping in order to render the\nalgorithm more robust, and ''invariant'' under monotonically\nincreasing transformations of the fitness function. \nFor this purpose, the fitness of the population is transformed into a set of [[utility]] values\n<math>u_1 \\geq \\dots \\geq u_\\lambda</math>. Let <math>x_i</math> denote the i<sup>th</sup> best individual.\nReplacing fitness with utility, the gradient estimate becomes\n::<math> \\nabla_{\\theta} J (\\theta) = \\sum_{k=1}^\\lambda u_k \\; \\nabla_{\\theta} \\log\\pi(x_k \\,|\\, \\theta) </math>.\nThe choice of utility function is a free parameter of the algorithm.\n\n===Pseudocode===\n {{nowrap|'''input''': <math>f, \\; \\; \\theta_{init}</math>}}\n \n 1  '''repeat'''\n    \n 2     {{nowrap|'''for ''' <math>k=1\\ldots\\lambda</math> '''do'''}}                                              ''// {{mvar|&lambda;}} is the population size''\n        \n 3         {{nowrap|draw sample <math>x_k \\sim \\pi(\\cdot | \\theta)</math>}}\n        \n 4         {{nowrap|evaluate fitness <math>f(x_k)</math>}}\n        \n 5         {{nowrap|calculate log-derivatives <math>\\nabla_\\theta \\log\\pi(x_k | \\theta)</math>}}\n        \n 6     '''end'''\n    \n 7     {{nowrap|assign the utilities <math>u_k</math>}}                                          ''// based on rank''\n    \n 8     {{nowrap|estimate the gradient <math>\\nabla_\\theta J \\leftarrow \\frac{1}{\\lambda}\\sum_{k=1}^{\\lambda} u_k \\cdot \\nabla_\\theta\\log\\pi(x_k | \\theta) </math>}}\n    \n 9     {{nowrap|estimate <math>\\mathbf{F}\\leftarrow \\frac{1}{\\lambda}\\sum_{k=1}^{\\lambda}\n\\nabla_\\theta\\log\\pi(x_k | \\theta)  \n\\nabla_\\theta\\log\\pi(x_k | \\theta)^{\\top}</math>}}           ''// or compute it exactly'' \n    \n 10    {{nowrap|update parameters <math>\\theta \\leftarrow \\theta + \\eta \\cdot \\mathbf{F}^{-1} \\nabla_\\theta J</math>}}                        ''// {{mvar|&eta;}} is the learning rate''\n \n 11 '''until''' stopping criterion is met\n\n==See also==\n\n* [[Evolutionary computation]]\n* [[CMA-ES|Covariance matrix adaptation evolution strategy (CMA-ES)]]\n\n==Bibliography==\n\n* D. Wierstra, T. Schaul, J. Peters and J. Schmidhuber (2008). [http://www.idsia.ch/~tom/publications/nes.pdf Natural Evolution Strategies]. IEEE Congress on Evolutionary Computation (CEC).\n* Y. Sun, D. Wierstra, T. Schaul and J. Schmidhuber (2009). [http://www.idsia.ch/~tom/publications/ssng.pdf Stochastic Search using the Natural Gradient]. International Conference on Machine Learning (ICML).\n* T. Glasmachers, T. Schaul, Y. Sun, D. Wierstra and J. Schmidhuber (2010). [http://www.idsia.ch/~tom/publications/xnes.pdf Exponential Natural Evolution Strategies]. Genetic and Evolutionary Computation Conference (GECCO).\n* T. Schaul, T. Glasmachers and J. Schmidhuber (2011). [http://www.idsia.ch/~tom/publications/snes.pdf High Dimensions and Heavy Tails for Natural Evolution Strategies]. Genetic and Evolutionary Computation Conference (GECCO).\n* T. Schaul (2012). [http://www.idsia.ch/~tom/publications/nesproof.pdf Natural Evolution Strategies Converge on Sphere Functions]. Genetic and Evolutionary Computation Conference (GECCO).\n\n==External links==\n* [http://schaul.site44.com/nes.html Collection of NES implementations in different languages]\n\n{{Evolutionary computation}}\n\n[[Category:Evolutionary algorithms]]\n[[Category:Stochastic optimization]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Negamax",
      "url": "https://en.wikipedia.org/wiki/Negamax",
      "text": "'''Negamax''' search is a variant form of [[minimax]] search that relies on the [[Zero-sum (Game theory)|zero-sum]] property of a [[two-player game]].\n\nThis algorithm relies on the fact that {{tmath|1=\\max(a, b) = -\\min(-a, -b)}} to simplify the implementation of the [[minimax]] algorithm. More precisely, the value of a position to player A in such a game is the negation of the value to player B. Thus, the player on move looks for a move that maximizes the negation of the value resulting from the move: this successor position must by definition have been valued by the opponent. The reasoning of the previous sentence works regardless of whether A or B is on move. This means that a single procedure can be used to value both positions. This is a coding simplification over minimax, which requires that A selects the move with the maximum-valued successor while B selects the move with the minimum-valued successor.\n\nIt should not be confused with [[negascout]], an algorithm to compute the minimax or negamax value quickly by clever use of [[alpha-beta pruning]] discovered in the 1980s. Note that alpha-beta pruning is itself a way to compute the minimax or negamax value of a position quickly by avoiding the search of certain uninteresting positions.\n\nMost [[adversarial search]] engines are coded using some form of negamax search.\n\n== Negamax base algorithm ==\n[[File:Plain Negamax.gif|thumb|An animated pedagogical example showing the plain negamax algorithm (that is, without alpha-beta pruning). The person performing the game tree search is considered to be the one that has to move first from the current state of the game (''player'' in this case)]]\nNegaMax operates on the same game trees as those used with the minimax search algorithm. Each node and root node in the tree are game states (such as game board configuration) of a two player game. Transitions to child nodes represent moves available to a player who's about to play from a given node.\n\nThe negamax search objective is to find the node score value for the player who is playing at the root node. The [[pseudocode]] below shows the negamax base algorithm,<ref name=\"Breuker\">Breuker, Dennis M. [http://breukerd.home.xs4all.nl/thesis/ ''Memory versus Search in Games''], Maastricht University, October 16, 1998</ref> with a configurable limit for the maximum search depth:\n\n '''function''' negamax(node, depth, color) '''is'''\n     '''if''' depth = 0 '''or''' node is a terminal node '''then'''\n         '''return''' color &times; the heuristic value of node\n     value := &minus;∞\n     '''for each''' child of node '''do'''\n         value := max(value, &minus;negamax(child, depth &minus; 1, &minus;color))\n     '''return''' value\n\n ''(* Initial call for Player A's root node *)''\n negamax(rootNode, depth, 1)\n\n ''(* Initial call for Player B's root node *)''\n negamax(rootNode, depth, &minus;1)\n\nThe root node inherits its score from one of its immediate child nodes. The child node that ultimately sets the root node's best score also represents the best move to play. Although the negamax function shown only returns the node's best score, practical negamax implementations will retain and return both best move and best score for the root node. Only the node's best score is essential with non-root nodes. And a node's best move isn't necessary to retain nor return for non-root nodes.\n\nWhat can be confusing is how the heuristic value of the current node is calculated. In this implementation, this value is always calculated from the point of view of player A, whose color value is one. In other words, higher heuristic values always represent situations more favorable for player A. This is the same behavior as the normal [[minimax]] algorithm. The heuristic value is not necessarily the same as a node's return value due to value negation by negamax and the color parameter. The negamax node's return value is a heuristic score from the point of view of the node's current player.\n\nNegamax scores match minimax scores for nodes where player A is about to play, and where player A is the maximizing player in the minimax equivalent. Negamax always searches for the maximum value for all its nodes. Hence for player B nodes, the minimax score is a negation of its negamax score. Player B is the minimizing player in the minimax equivalent.\n\nVariations in negamax implementations may omit the color parameter. In this case, the heuristic evaluation function must return values from the point of view of the node's current player.\n\n== Negamax with alpha beta pruning ==\n[[File:Negamax AlphaBeta.gif|thumb|An animated pedagogical example showing the negamax algorithm with alpha-beta pruning. The person performing the game tree search is considered to be the one that has to move first from the current state of the game (''player'' in this case)]]\n\nAlgorithm optimizations for [[minimax]] are also equally applicable for Negamax. [[Alpha-beta pruning]] can decrease the number of nodes the negamax algorithm evaluates in a search tree in a manner similar with its use with the minimax algorithm.\n\nThe pseudocode for depth-limited negamax search with alpha-beta pruning follows:<ref name=Breuker />\n\n '''function''' negamax(node, depth, α, β, color) '''is'''\n     '''if''' depth = 0 '''or''' node is a terminal node '''then'''\n         '''return''' color &times; the heuristic value of node\n \n     childNodes := generateMoves(node)\n     childNodes := orderMoves(childNodes)\n     value := &minus;∞\n     '''foreach''' child in childNodes '''do'''\n         value := max(value, &minus;negamax(child, depth &minus; 1, &minus;β, &minus;α, &minus;color))\n         α := max(α, value)\n         '''if''' α ≥ β '''then'''\n             '''break''' ''(* cut-off *)''\n     '''return''' value\n\n ''(* Initial call for Player A's root node *)''\n negamax(rootNode, depth, &minus;∞, +∞, 1)\n\nAlpha (α) and beta (β) represent lower and upper bounds for child node values at a given tree depth. Negamax sets the arguments α and β for the root node to the lowest and highest values possible. Other search algorithms, such as [[negascout]] and [[MTD-f]], may initialize α and β with alternate values to further improve tree search performance.\n\nWhen negamax encounters a child node value outside an alpha/beta range, the negamax search cuts off thereby pruning portions of the game tree from exploration. Cut offs are implicit based on the node return value. A node value found within the range of its initial α and β is the node's exact (or true) value. This value is identical to the result the negamax base algorithm would return, without cut offs and without any α and β bounds. If a node return value is out of range, then the value represents an upper (if value ≤ α) or lower (if value ≥ β) bound for the node's exact value. Alpha-beta pruning eventually discards any value bound results. Such values do not contribute nor affect the negamax value at its root node.\n\nThis pseudocode shows the fail-soft variation of alpha-beta pruning. Fail-soft never returns α or β directly as a node value. Thus, a node value may be outside the initial α and β range bounds set with a negamax function call. In contrast, fail-hard alpha-beta pruning always limits a node value in the range of α and β.\n\nThis implementation also shows optional move ordering prior to the [[foreach loop]] that evaluates child nodes. Move ordering<ref name=\"Schaeffer\">Schaeffer, Jonathan [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.9124&rep=rep1&type=pdf ''The History Heuristic and Alpha-Beta Search Enhancements in Practice''], IEEE Transactions on Pattern Analysis and Machine Intelligence, 1989</ref> is an optimization for alpha beta pruning that attempts to guess the most probable child nodes that yield the node's score. The algorithm searches those child nodes first. The result of good guesses is earlier and more frequent alpha/beta cut offs occur, thereby pruning additional game tree branches and remaining child nodes from the search tree.\n\n== Negamax with alpha beta pruning and transposition tables ==\n[[Transposition table]]s selectively [[Memoization|memoize]] the values of nodes in the game tree. ''Transposition'' is a term reference that a given game board position can be reached in more than one way with differing game move sequences.\n\nWhen negamax searches the game tree, and encounters the same node multiple times, a transposition table can return a previously computed value of the node, skipping potentially lengthy and duplicate re-computation of the node's value. Negamax performance improves particularly for game trees with many paths that lead to a given node in common.\n\nThe pseudo code that adds transposition table functions to negamax with alpha/beta pruning is given as follows:<ref name=Breuker />\n\n '''function''' negamax(node, depth, α, β, color) '''is'''\n     alphaOrig := α\n \n     ''(* Transposition Table Lookup; node is the lookup key for ttEntry *)''\n     ttEntry := transpositionTableLookup(node)\n     '''if''' ttEntry is valid '''and''' ttEntry.depth ≥ depth '''then'''\n         '''if''' ttEntry.flag = EXACT '''then'''\n             '''return''' ttEntry.value\n         '''else if''' ttEntry.flag = LOWERBOUND '''then'''\n             α := max(α, ttEntry.value)\n         '''else if''' ttEntry.flag = UPPERBOUND '''then'''\n             β := min(β, ttEntry.value)\n \n         '''if''' α ≥ β '''then'''\n             '''return''' ttEntry.value\n \n     '''if''' depth = 0 '''or''' node is a terminal node '''then'''\n         '''return''' color &times; the heuristic value of node\n \n     childNodes := generateMoves(node)\n     childNodes := orderMoves(childNodes)\n     value := &minus;∞\n     '''for each''' child in childNodes '''do'''\n         value := max(value, &minus;negamax(child, depth &minus; 1, &minus;β, &minus;α, &minus;color))\n         α := max(α, value)\n         '''if''' α ≥ β '''then'''\n             '''break'''\n \n     ''(* Transposition Table Store; node is the lookup key for ttEntry *)''\n     ttEntry.value := value\n     '''if''' value ≤ alphaOrig '''then'''\n         ttEntry.flag := UPPERBOUND\n     '''else if''' value ≥ β '''then'''\n         ttEntry.flag := LOWERBOUND\n     '''else'''\n         ttEntry.flag := EXACT\n     ttEntry.depth := depth\t\n     transpositionTableStore(node, ttEntry)\n \n     '''return''' value\n\n ''(* Initial call for Player A's root node *)''\n negamax(rootNode, depth, &minus;∞, +∞, 1)\n\nAlpha/beta pruning and maximum search depth constraints in negamax can result in partial, inexact, and entirely skipped evaluation of nodes in a game tree. This complicates adding transposition table optimizations for negamax. It is insufficient to track only the node's ''value'' in the table, because ''value'' may not be the node's true value. The code therefore must preserve and restore the relationship of ''value'' with alpha/beta parameters and the search depth for each transposition table entry.\n\nTransposition tables are typically lossy and will omit or overwrite previous values of certain game tree nodes in its tables. This is necessary since the number of nodes negamax visits often far exceeds the transposition table size. Lost or omitted table entries are non-critical and will not affect the negamax result. However, lost entries may require negamax to re-compute certain game tree node values more frequently, thus affecting performance.\n\n== References ==\n\n* {{cite book |author1=George T. Heineman |author2=Gary Pollice |author3=Stanley Selkow  |last-author-amp=yes | title= Algorithms in a Nutshell | publisher=[[Oreilly Media]] | year=2008 | chapter=Chapter 7:Path Finding in AI | pages = 213–217 | isbn=978-0-596-51624-6 }}\n* {{cite book | author=John P. Fishburn | title= Analysis of Speedup in Distributed Algorithms (revision of 1981 PhD thesis) | publisher=[[UMI Research Press]] | year=1984 | chapter=Appendix A: Some Optimizations of α-β Search | pages = 107–111 | isbn=0-8357-1527-2 }}\n\n<references />\n\n== External links ==\n* [https://www.chessprogramming.org/Negamax Negamax at the Chess Programming Wiki]\n* [http://xojoc.pw/dailyprogrammer/173.c A C99 implementation of the Negamax algorithm for the Tic-Tac-Toe game]\n\n[[Category:Game artificial intelligence]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Nelder–Mead method",
      "url": "https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method",
      "text": "{{distinguish|text=Dantzig's [[simplex algorithm]] for the problem of linear optimization}}\n{| class=\"infobox bordered\" style=\"width: 22em; text-align: left; font-size: 95%;\"\n|-\n| colspan=\"2\" style=\"text-align:center;\" | [[Image:Nelder-Mead Rosenbrock.gif|320px|]]\n|-\n| colspan=\"2\" style=\"text-align:center;\" | [[Image:Nelder-Mead Himmelblau.gif|320px|]]\n'''Nelder–Mead simplex search over the [[Rosenbrock function|Rosenbrock banana function]] '''(above)''' and [[Himmelblau's function]]''' (below)<br>\n|- \n|}\n\n[[File:Nelder-Mead Simionescu.gif|thumb|320px|Nelder-Mead minimum search of [[Test functions for optimization|Simionescu's function]]. Simplex vertices are ordered by their value, with 1 having the lowest (best) value.]]\n\nThe '''Nelder–Mead method''' (also '''downhill simplex method''', '''amoeba method''', or '''polytope method''') is a commonly applied [[numerical method]] used to find the minimum or maximum of an [[objective function]] in a multidimensional space. It is a ''direct search method'' (based on function comparison) and is often applied to nonlinear [[Optimization (mathematics)|optimization]] problems for which derivatives may not be known. However, the Nelder–Mead technique is a [[heuristic]] search method that can converge to non-stationary points<ref name=\"PM\">\n* {{cite journal | last1 = Powell | first1 = Michael J. D. | authorlink = Michael J. D. Powell | year = 1973 | title = On Search Directions for Minimization Algorithms | url = | journal = Mathematical Programming | volume = 4 | issue = | pages = 193–201 | doi=10.1007/bf01584660}}\n* {{ cite journal | last = McKinnon | first = K.I.M. | title =Convergence of the Nelder–Mead simplex method to a non-stationary point | journal =SIAM Journal on Optimization | year = 1999 | volume = 9 | pages =148–158 | doi = 10.1137/S1052623496303482| citeseerx = 10.1.1.52.3900 }} (algorithm summary online).\n</ref> on problems that can be solved by alternative methods.<ref name=\"YKL\">\n*Yu, Wen Ci. 1979. \"Positive basis and a class of direct search techniques\". ''Scientia Sinica'' [''Zhongguo Kexue'']: 53—68. \n*Yu, Wen Ci. 1979. \"The convergent property of the simplex evolutionary technique\". ''Scientia Sinica'' [''Zhongguo Kexue'']: 69–77.\n* {{ cite journal | last = Kolda | first = Tamara G. |authorlink1=Tamara G. Kolda|author2=Lewis, Robert Michael |author3=Torczon, Virginia | year =2003 | title = Optimization by direct search: new perspectives on some classical and modern methods | journal = SIAM Rev. | volume =45 | issue = 3 | pages = 385–482 | doi = 10.1137/S003614450242889|citeseerx=10.1.1.96.8672 }} \n* {{ cite journal | last = Lewis | first = Robert Michael |author2=Shepherd, Anne |author3=Torczon, Virginia | year = 2007 | title = Implementing generating set search methods for linearly constrained minimization | journal = SIAM J. Sci. Comput. | volume = 29 | issue = 6 | pages = 2507–2530 | doi = 10.1137/050635432| citeseerx = 10.1.1.62.8771 }}\n</ref>\n\nThe Nelder–Mead technique was proposed by [[John Nelder]] and [[Roger Mead]] in 1965,<ref name=\"NM\">{{cite journal | last = Nelder | first = John A. |author2=R. Mead | title = A simplex method for function minimization | journal = Computer Journal | volume = 7 | issue = 4 | year = 1965 | pages = 308–313 | doi =  10.1093/comjnl/7.4.308}}</ref> as a development of the method of Spendley et al.<ref name=\"SHH\">\n{{ cite journal | last1 = Spendley |  first1 = W | last2 = Hext | first2 = GR | last3 = Himsworth | first3 = FR | year = 1962 | title = Sequential Application of Simplex Designs in Optimisation and Evolutionary Operation | journal =  Technometrics | volume = 4 |  issue = 4 | pages = 441–461 | doi = 10.1080/00401706.1962.10490033 }}</ref>\n\n== Overview ==\nThe method uses the concept of a [[simplex]], which is a special [[polytope]] of ''n''&nbsp;+&nbsp;1 vertices in ''n'' dimensions. Examples of simplices include a line segment on a line, a triangle on a plane, a [[tetrahedron]] in three-dimensional space and so forth.\n\nThe method approximates a local optimum of a problem with ''n'' variables when the objective function varies smoothly and is [[unimodal]]. Typical implementations minimize functions, and we maximize <math>f(\\mathbf x)</math> by minimizing <math>- f(\\mathbf x)</math>.\n\nFor example, a suspension bridge engineer has to choose how thick each strut, cable, and pier must be.  These elements are interdependent, but it is not easy to visualize the impact of changing any specific element. Simulation of such complicated structures is often extremely computationally expensive to run, possibly taking upwards of hours per execution. The Nelder–Mead method requires, in the original variant, no more than two evaluations per iteration\nexcept for the '''shrink''' operation described later,\nwhich is attractive compared to some other direct-search optimization methods. However, the overall number of iterations to proposed optimum may be high.\n\nNelder–Mead in ''n'' dimensions maintains a set of ''n+1'' test points arranged as a [[simplex]]. It then extrapolates the behavior of the objective function measured at each test point, in order to find a new test point and to replace one of the old test points with the new one, and so the technique progresses.  The simplest approach is to replace the worst point with a point reflected through the [[centroid]] of the remaining ''n'' points.  If this point is better than the best current point, then we can try stretching exponentially out along this line.  On the other hand, if this new point isn't much better than the previous value, then we are stepping across a valley, so we shrink the simplex towards a better point. An intuitive explanation of the algorithm is presented in: \n<ref name=\"NR\">\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 10.5. Downhill Simplex Method in Multidimensions | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=502}}\n</ref>\n<blockquote>\nThe downhill simplex method now takes a series of steps, most steps just moving the point of the simplex where the function is largest (“highest point”) through the opposite face of the simplex to a lower point. These steps are called reflections, and they are constructed to conserve the volume of the simplex (and hence maintain its nondegeneracy). When it can do so, the method expands the simplex in one or another direction to take larger steps. When it reaches a “valley floor,” the method contracts itself in the transverse direction and tries to ooze down the valley. If there is a situation where the simplex is trying to “pass through the eye of a needle,” it contracts itself in all directions, pulling itself in around its lowest (best) point.\n</blockquote>\n\nUnlike modern optimization methods, the Nelder–Mead heuristic can converge to a non-stationary point unless the problem satisfies stronger conditions than are necessary for modern methods.<ref name=\"PM\" /> <!--  NONALGORITHM follows: The standard approach to handle this is to restart the algorithm with a new simplex starting at the current best value.  This can be extended in a similar way to [[simulated annealing]] to escape small local minima. --> Modern improvements over the Nelder–Mead heuristic have been known since 1979.<ref name=\"YKL\" />\n\nMany variations exist depending on the actual nature of the problem being solved. A common variant uses a constant-size, small simplex that roughly follows the gradient direction (which gives [[Gradient descent|steepest descent]]). Visualize a small triangle on an elevation map flip-flopping its way down a valley to a local bottom.  This method is also known as the Flexible Polyhedron Method. This, however, tends to perform poorly against the method described in this article(?) because it makes small, unnecessary steps in areas of little interest.\n\n== One possible variation of the NM algorithm ==\n(This approximates the procedure in the original Nelder-Mead paper.)\n[[File:Rosenbrock function Nelder-Mead.gif|thumb|text-top|right|alt=Rosenbrock function Nelder-Mead|Nelder–Mead method applied to the [[Rosenbrock function]]]]\n\nWe are trying to minimize the function <math>f(\\mathbf x)</math>, where <math>\\mathbf x \\in \\mathbb{R}^n</math>. Our current test points are <math>\\mathbf x_1, \\ldots, \\mathbf x_{n+1}</math>.\n\n'''1. Order''' according to the values at the vertices:\n:<math>f(\\mathbf x_1) \\leq f(\\mathbf x_2) \\leq \\cdots \\leq f(\\mathbf x_{n+1}).</math>\n\n:Check if method should stop. See [[#Termination|'''Termination''']] below. Sometimes inappropriately called \"convergence\".\n\n'''2.''' Calculate <math>\\mathbf x_o</math>, the [[centroid]] of all points except <math>\\mathbf x_{n+1}</math>.\n\n'''3. Reflection'''\n: Compute reflected point <math>\\mathbf x_r = \\mathbf x_o + \\alpha (\\mathbf x_o - \\mathbf x_{n+1})</math> with <math>\\alpha > 0</math>.\n: If the reflected point is better than the second worst, but not better than the best, i.e. <math>f(\\mathbf x_1) \\leq f(\\mathbf x_r) < f(\\mathbf x_n)</math>,\n:: then obtain a new simplex by replacing the worst point <math>\\mathbf x_{n+1}</math> with the reflected point <math>\\mathbf x_r</math>, and go to step&nbsp;1.\n\n'''4. Expansion'''\n: If the reflected point is the best point so far, <math>f(\\mathbf x_r) < f(\\mathbf x_1)</math>,\n:: then compute the expanded point <math>\\mathbf x_e = \\mathbf x_o + \\gamma (\\mathbf x_r - \\mathbf x_o)</math> with <math>\\gamma > 1</math>.\n:: If the expanded point is better than the reflected point, <math>f(\\mathbf x_e) < f(\\mathbf x_r)</math>,\n::: then obtain a new simplex by replacing the worst point <math>\\mathbf x_{n+1}</math> with the expanded point <math>\\mathbf x_e</math> and go to step&nbsp;1;\n::: else obtain a new simplex by replacing the worst point <math>\\mathbf x_{n+1}</math> with the reflected point <math>\\mathbf x_r</math> and go to step&nbsp;1.\n\n'''5. Contraction'''\n: Here it is certain that <math>f(\\mathbf x_{r}) \\geq f(\\mathbf x_n)</math>. (Note that <math>\\mathbf x_n</math> is second or \"next\" to highest.)\n: Compute contracted point <math>\\mathbf x_c = \\mathbf x_o + \\rho(\\mathbf x_{n+1} - \\mathbf x_o)</math> with <math>0 < \\rho \\leq 0.5</math>.\n: If the contracted point is better than the worst point, i.e. <math>f(\\mathbf x_c) < f(\\mathbf x_{n+1})</math>,\n:: then obtain a new simplex by replacing the worst point <math>\\mathbf x_{n+1}</math> with the contracted point <math>\\mathbf x_c</math> and go to step&nbsp;1;\n\n'''6. Shrink'''\n: Replace all points except the best (<math>\\mathbf x_1</math>) with\n:<math>\\mathbf x_i = \\mathbf x_1 + \\sigma(\\mathbf x_i - \\mathbf x_1)</math> and go to step&nbsp;1.\n\n'''Note''': <math>\\alpha</math>, <math>\\gamma</math>, <math>\\rho</math> and <math>\\sigma</math> are respectively the reflection, expansion, contraction and shrink coefficients. Standard values are <math>\\alpha = 1</math>, <math>\\gamma = 2</math>, <math>\\rho = 1/2</math> and <math>\\sigma = 1/2</math>.\n\nFor the '''reflection''', since <math>\\mathbf x_{n+1}</math> is the vertex with the higher associated value among the vertices, we can expect to find a lower value at the reflection of <math>\\mathbf x_{n+1}</math> in the opposite face formed by all vertices <math>\\mathbf x_i</math> except <math>\\mathbf x_{n+1}</math>.\n\nFor the '''expansion''', if the reflection point <math>\\mathbf x_r</math> is the new minimum along the vertices, we can expect to find interesting values along the direction from <math>\\mathbf x_o</math> to <math>\\mathbf x_r</math>.\n\nConcerning the '''contraction''', if <math>f(\\mathbf x_r) > f(\\mathbf x_n)</math>, we can expect that a better value will be inside the simplex formed by all the vertices <math>\\mathbf x_i</math>.\n\nFinally, the '''shrink''' handles the rare case that contracting away from the largest point increases <math>f</math>, something that cannot happen sufficiently close to a non-singular minimum. In that case we contract towards the lowest point in the expectation of finding a simpler landscape. However, Nash notes that finite-precision arithmetic can sometimes fail to actually shrink the simplex, and implemented a check that the size is actually reduced.<ref name=\"CNM\">{{Cite book | last1=Nash | first1=JC | year=1979 | title=Compact Numerical Methods: Linear Algebra and Function Minimisation | publisher=Adam Hilger |  location=Bristol | isbn=978-0-85274-330-0}}</ref>\n\n== Initial simplex ==\n\nThe initial simplex is important. Indeed, a too small initial simplex can lead to a local search, consequently the NM can get more easily stuck. So this simplex should depend on the nature of the problem. However, the original paper suggested a simplex where an initial point is given as <math>\\mathbf x_{1}</math>, with the others generated with a fixed step along each dimension in turn. Thus the method is sensitive to scaling of the variables that make up \n<math>\\mathbf x</math>.\n\n== Termination ==\n\nCriteria are needed to break the iterative cycle. Nelder and Mead used the sample standard deviation of the function values of the current simplex. If these fall below some tolerance, then the\ncycle is stopped and the lowest point in the simplex returned as a proposed optimum. Note that a very \"flat\" function may have almost equal function values over a large domain, so that the solution \nwill be sensitive to the tolerance. Nash <ref name=\"CNM\" /> adds the test for shrinkage as another termination criterion. Note that programs terminate, while iterations may converge.\n\n== See also ==\n{{Div col|colwidth=20em}}\n* [[Derivative-free optimization]]\n* [[COBYLA]]\n* [[NEWUOA]]\n* [[LINCOA]]\n* [[Nonlinear conjugate gradient method]]\n* [[Levenberg–Marquardt algorithm]]\n* Broyden–Fletcher–Goldfarb–Shanno or [[BFGS method]]\n* [[Differential evolution]]\n* [[Pattern search (optimization)]]\n* [[CMA-ES]]\n{{Div col end}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*{{cite book |last=Avriel |first=Mordecai |year=2003 |title=Nonlinear Programming: Analysis and Methods |location= |publisher=Dover Publishing |isbn=978-0-486-43227-4 }}\n*{{cite journal |last=Coope |first=I. D. |first2=C. J. |last2=Price |year=2002 |title=Positive Bases in Numerical Optimization |journal=Computational Optimization & Applications |volume=21 |issue=2 |pages=169–176 |doi=10.1023/A:1013760716801 }}\n*{{cite book |first=Philip E. |last=Gill |first2=Walter |last2=Murray |first3=Margaret H. |last3=Wright |chapter=Methods for Multivariate Non-Smooth Functions |title=Practical Optimization |location=New York |publisher=Academic Press |year=1981 |isbn=978-0-12-283950-4 |pages=93–96 }}\n* {{cite book |last1=Kowalik |first=J. |last2=Osborne |first2=M. R. |title=Methods for Unconstrained Optimization Problems |location=New York |publisher=Elsevier |year=1968 |isbn=0-444-00041-0 |pages=24–27 }} \n*{{cite book |first=W. H. |last=Swann |chapter=Direct Search Methods |title=Numerical Methods for Unconstrained Optimization |editor-last=Murray |editor-first=W. |location=New York |publisher=Academic Press |year=1972 |pages=13–28 |isbn=978-0-12-512250-4 }}\n\n==External links==\n* [http://www.boomer.org/c/p3/c11/c1106.html Nelder–Mead (Simplex) Method]\n* [http://www.brnt.eu/phd/node10.html#SECTION00622200000000000000  Nelder–Mead (Downhill Simplex) explanation and visualization with the Rosenbrock banana function]\n*[http://people.sc.fsu.edu/~burkardt/m_src/asa047/nelmin.m John Burkardt: Nelder–Mead code in Matlab] - note that a variation of the Nelder–Mead method is also implemented by the Matlab function fminsearch.\n*[https://github.com/fchollet/nelder-mead nelder-mead] - A Python implementation of the Nelder–Mead method\n*[http://people.fsv.cvut.cz/~svobodal/sova/  SOVA 1.0 (freeware)] - Simplex Optimization for Various Applications  \n* [http://www.hillstormer.es] - HillStormer, a practical tool for nonlinear, multivariate and linear constrained Simplex Optimization by Nelder Mead.\n\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Nelder-Mead method}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Network simplex algorithm",
      "url": "https://en.wikipedia.org/wiki/Network_simplex_algorithm",
      "text": "In [[mathematical optimization]], the '''network simplex algorithm''' is a [[graph theory|graph theoretic]] specialization of the [[simplex algorithm]]. The algorithm is usually formulated in terms of a standard problem, [[minimum-cost flow problem]] and can be efficiently solved in polynomial time. The network simplex method works very well in practice, typically 200 to 300 times faster than the simplex method applied to general linear program of same dimensions.{{citation needed|date=May 2015}}\n\n== History ==\nFor a long time, the existence of a provably efficient network simplex algorithm was one of the major open problems in complexity theory, even though efficient-in-practice versions were available. In 1995 [[James B. Orlin|Orlin]] provided the first polynomial algorithm with runtime of <math>O(V^2 E \\log(VC))</math> where <math>C</math> is maximum cost of any edges.<ref>{{Cite journal|title = A polynomial time primal network simplex algorithm for minimum cost flows|journal = Mathematical Programming|date = 1997-08-01|issn = 0025-5610|pages = 109–129|volume = 78|issue = 2|doi = 10.1007/BF02614365|first = James B.|last = Orlin|hdl = 1721.1/2584}}</ref> Later [[Robert Tarjan|Tarjan]] improved this to <math>O(VE \\log V \\log(VC))</math> using [[dynamic trees]] in 1997.<ref>{{Cite journal|title = Dynamic trees as search trees via euler tours, applied to the network simplex algorithm|journal = Mathematical Programming|date = 1997-08-01|issn = 0025-5610|pages = 169–177|volume = 78|issue = 2|doi = 10.1007/BF02614369|first = Robert E.|last = Tarjan}}</ref> Strongly polynomial dual network simplex algorithms for the same problem, but with a higher dependence on the numbers of edges and vertices in the graph, have been known for longer.<ref>{{citation\n | last1 = Orlin | first1 = James B. | author1-link = James B. Orlin\n | last2 = Plotkin | first2 = Serge A.\n | last3 = Tardos | first3 = Éva | author3-link = Éva Tardos\n | date = June 1993\n | doi = 10.1007/bf01580615\n | issue = 1–3\n | journal = [[Mathematical Programming]]\n | pages = 255–276\n | title = Polynomial dual network simplex algorithms\n | volume = 60| citeseerx = 10.1.1.297.5730 }}</ref>\n\n== Overview ==\nThe network simplex method is an adaptation of the bounded variable primal simplex  algorithm. The basis is represented as a rooted spanning  tree of the underlying network, in which variables are represented by arcs, and the simplex  multipliers by node potentials. At each iteration, an entering variable is selected by some  pricing strategy, based on the dual multipliers (node potentials), and forms a cycle with  the arcs of the tree. The leaving variable is the arc of the cycle with the least augmenting  flow. The substitution of entering for leaving arc, and the reconstruction of the tree is called  a pivot. When no non-basic arc remains eligible to enter, the optimal solution has been reached.\n\n== Applications ==\nThe network simplex algorithm can be used to solve many practical problems including,<ref>{{Cite book|title = Linear Programming|last = Chvatal|first = Vasek|publisher = Macmillan|year = 1983|isbn = 9780716715870|location = |pages = 320–351|chapter = 20|chapter-url = https://books.google.com/books?id=DN20_tW_BV0C&lpg=PA320&ots=4eHDEubUhH&dq=network%20simplex%20applications&pg=PA320#v=onepage&q&f=false}}</ref>\n* [[Transshipment problem]]\n* [[Transportation theory (mathematics)|Hitchcock transportation problem]]\n* [[Assignment problem]]\n* Chains and antichains in [[partially ordered set]]s\n* [[System of distinct representatives]]\n* Covers and matching in [[bipartite graph]]s\n* Caterer problem\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://www.4er.org/CourseNotes/Book%20B/B-IV.pdf Solving Network Problems] Section 14, p B-113 shows an example execution\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Linear programming]]\n[[Category:Network flow problem]]\n[[Category:Mathematical problems]]\n[[Category:Network theory]]\n[[Category:Polynomial-time problems]]\n[[Category:Graph algorithms]]\n[[Category:Computational problems in graph theory]]"
    },
    {
      "title": "Newton's method",
      "url": "https://en.wikipedia.org/wiki/Newton%27s_method",
      "text": "{{About|Newton's method for finding roots|Newton's method for finding minima|Newton's method in optimization}}\n\nIn [[numerical analysis]], '''Newton's method''', also known as the '''Newton–Raphson method''', named after [[Isaac Newton]] and [[Joseph Raphson]], is a  [[root-finding algorithm]] which produces successively better [[Numerical analysis|approximations]] to the [[root of a function|roots]] (or zeroes) of a [[Real number|real]]-valued [[function (mathematics)|function]]. The most basic version starts with a single-variable function {{math|''f''}} defined for a [[real number|real variable]] {{math|''x''}}, the function's [[derivative]] {{math|''f&thinsp;′''}}, and an initial guess {{math|''x''<sub>0</sub>}} for a [[Zero_of_a_function|root]] of {{math|''f''}}. If the function satisfies sufficient assumptions and the initial guess is close, then \n\n:<math>x_{1} = x_0 - \\frac{f(x_0)}{f'(x_0)} \\,.</math>\n\nis a better approximation of the root than {{math|''x''<sub>0</sub>}}. Geometrically, {{math|(''x''<sub>1</sub>, 0)}} is the intersection of the {{math|''x''}}-axis and the [[tangent]] of the [[graph of a function|graph]] of {{math|''f''}} at {{math|(''x''<sub>0</sub>, ''f''&thinsp;(''x''<sub>0</sub>))}}: that is, the improved guess is the unique root of the [[linear approximation]] at the initial point. The process is repeated as\n\n:<math>x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} \\,</math>\n\nuntil a sufficiently precise value is reached. This algorithm is first in the class of [[Householder's method]]s, succeeded by [[Halley's method]]. The method can also be extended to [[Complex-valued function|complex functions]] and to [[systems of equations]].\n\n==Description==\n<!-- [[Image:newton iteration.svg|alt Illustration of Newton's method|thumb|right|300px|An illustration of one iteration of Newton's method (the function {{mvar|f}} is shown in blue and the tangent line is in red). We see that {{math|''x''<sub>''n'' + 1</sub>}} is a better approximation than {{math|''x''<sub>''n''</sub>}} for the root {{math|''x''}} of the function {{math|''f''}}.]] -->\n[[Image:NewtonIteration Ani.gif|alt Illustration of Newton's method|thumb|right|300px|The function {{mvar|f}} is shown in blue and the tangent line is in red. We see that {{math|''x''<sub>''n'' + 1</sub>}} is a better approximation than {{math|''x''<sub>''n''</sub>}} for the root {{mvar|x}} of the function {{mvar|f}}.]]\n\nThe idea is to start with an initial guess which is reasonably close to the true root, then to approximate the function by its [[tangent line]] using [[calculus]], and finally to compute the {{mvar|x}}-intercept of this tangent line by [[elementary algebra]]. This {{mvar|x}}-intercept will typically be a better approximation to the original function's root than the first guess, and the method can be [[iterative method|iterated]].\n\nMore formally, suppose {{math|''f'' : (''a'', ''b'') → ℝ}} is a [[derivative|differentiable]] function defined on the [[interval (mathematics)|interval]] {{math|(''a'', ''b'')}} with values in the [[real number]]s&nbsp;{{math|ℝ}}, and we have some current approximation {{math|''x''<sub>''n''</sub>}}. Then we can derive the formula for a better approximation, {{math|''x''<sub>''n'' + 1</sub>}} by referring to the diagram on the right. The equation of the [[tangent line]] to the curve {{math|''y'' {{=}} ''f''&thinsp;(''x'')}} at {{math|''x'' {{=}} ''x''<sub>''n''</sub>}} is\n\n:<math>y = f'(x_n) \\, (x-x_n) + f(x_n),</math>\n\nwhere {{mvar|f′}} denotes the [[derivative]]. The {{mvar|x}}-intercept of this line (the value of {{mvar|x}} which makes {{math|''y'' {{=}} 0}}) is taken as the next approximation,{{math|''x''<sub>''n'' + 1</sub>}}, to the root, so that the equation of the tangent line is satisfied when <math>(x, y) = (x_{n + 1}, 0 )</math>: \n\n:<math>0 = f'(x_n) \\, (x_{n+1}-x_n) + f(x_n).</math>\n\nSolving for {{math|''x''<sub>''n'' + 1</sub>}} gives\n:<math>x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}. </math>\n\nWe start the process with some arbitrary initial value {{math|''x''<sub>0</sub>}}. (The closer to the zero, the better. But, in the absence of any intuition about where the zero might lie, a \"guess and check\" method might narrow the possibilities to a reasonably small interval by appealing to the [[intermediate value theorem]].) The method will usually converge, provided this initial guess is close enough to the unknown zero, and that {{math|''f&thinsp;′''(''x''<sub>0</sub>) ≠ 0}}. Furthermore, for a zero of [[Multiplicity (mathematics)|multiplicity]]&nbsp;1, the convergence is at least quadratic (see [[rate of convergence]]) in a [[neighbourhood (mathematics)|neighbourhood]] of the zero, which intuitively means that the number of correct digits roughly doubles in every step. More details can be found in the [[#Analysis|analysis section]] below.\n\n[[Householder's method]]s are similar but have higher order for even faster convergence. However, the extra computations required for each step can slow down the overall performance relative to Newton's method, particularly if {{mvar|f}} or its derivatives are computationally expensive to evaluate.\n\n==History==\nThe name \"Newton's method\" is derived from [[Isaac Newton]]'s description of a special case of the method in ''[[De analysi per aequationes numero terminorum infinitas]]'' (written in 1669, published in 1711 by [[William Jones (mathematician)|William Jones]]) and in ''De metodis fluxionum et serierum infinitarum'' (written in 1671, translated and published as ''[[Method of Fluxions]]'' in 1736 by [[John Colson]]). However, his method differs substantially from the modern method given above: Newton applies the method only to polynomials. He does not compute the successive approximations {{math|''x<sub>n</sub>''}}, but computes a sequence of polynomials, and only at the end arrives at an approximation for the root {{mvar|x}}. Finally, Newton views the method as purely algebraic and makes no mention of the connection with calculus. Newton may have derived his method from a similar but less precise method by [[Franciscus Vieta|Vieta]]. The essence of Vieta's method can be found in the work of the [[Mathematics in medieval Islam|Persian mathematician]] [[Sharaf al-Din al-Tusi]], while his successor [[Jamshīd al-Kāshī]] used a form of Newton's method to solve {{math|''x<sup>P</sup>'' − ''N'' {{=}} 0}} to find roots of {{mvar|N}} (Ypma 1995). A special case of Newton's method for calculating square roots was known since ancient times and is often called the [[Babylonian method]].\n\nNewton's method was used by 17th-century Japanese mathematician [[Seki Kōwa]] to solve single-variable equations, though the connection with calculus was missing.<ref>{{cite web |title=Chapter 2. Seki Takakazu |url=http://www.ndl.go.jp/math/e/s1/2.html |website=Japanese Mathematics in the Edo Period |publisher=National Diet Library |accessdate=24 February 2019}}</ref>{{Citation needed|date=October 2018}}\n\nNewton's method was first published in 1685 in ''A Treatise of Algebra both Historical and Practical'' by [[John Wallis]].<ref>{{cite book |first=John |last=Wallis |authorlink=John Wallis |title=A Treatise of Algebra, both Historical and Practical. Shewing the Original, Progress, and Advancement thereof, from time to time, and by what Steps it hath attained to the Heighth at which it now is |publisher=Richard Davis |location=Oxford |date=1685 |url=http://www.e-rara.ch/zut/content/titleinfo/2507537 |doi=10.3931/e-rara-8842}}</ref> In 1690, [[Joseph Raphson]] published a simplified description in ''Analysis aequationum universalis''.<ref>{{cite book |last=Raphson |first=Joseph |authorlink=Joseph Raphson |title=Analysis Æequationum Universalis seu ad Æequationes Algebraicas Resolvendas Methodus Generalis, & Expedita, Ex nova Infinitarum Serierum Methodo, Deducta ac Demonstrata |edition=secunda |language=la |date=1697 |location=London |url=https://archive.org/details/bub_gb_4nlbAAAAQAAJ |doi=10.3931/e-rara-13516}}</ref> Raphson again viewed Newton's method purely as an algebraic method and restricted its use to polynomials, but he describes the method in terms of the successive approximations {{math|''x''<sub>''n''</sub>}} instead of the more complicated sequence of polynomials used by Newton. Finally, in 1740, [[Thomas Simpson]] described Newton's method as an iterative method for solving general nonlinear equations using calculus, essentially giving the description above. In the same publication, Simpson also gives the generalization to systems of two equations and notes that Newton's method can be used for solving optimization problems by setting the gradient to zero.\n\n[[Arthur Cayley]] in 1879 in ''The Newton–Fourier imaginary problem'' was the first to notice the difficulties in generalizing Newton's method to complex roots of polynomials with degree greater than 2 and complex initial values. This opened the way to the study of the [[Julia set|theory of iterations]] of rational functions.\n\n==Practical considerations==\nNewton's method is an extremely powerful technique—in general the [[rate of convergence|convergence]] is quadratic: as the method converges on the root, the difference between the root and the approximation is squared (the number of accurate digits roughly doubles) at each step. However, there are some difficulties with the method.\n\n===Difficulty in calculating derivative of a function===\nNewton's method requires that the derivative can be calculated directly. An analytical expression for the derivative may not be easily obtainable or could be expensive to evaluate. In these situations, it may be appropriate to approximate the derivative by using the slope of a line through two nearby points on the function. Using this approximation would result in something like the [[secant method]] whose convergence is slower than that of Newton's method.\n\n===Failure of the method to converge to the root===\nIt is important to review the [[#Proof of quadratic convergence for Newton's iterative method|proof of quadratic convergence]] of Newton's Method before implementing it. Specifically, one should review the assumptions made in the proof. For [[#Failure analysis|situations where the method fails to converge]], it is because the assumptions made in this proof are not met.\n\n====Overshoot====\nIf the first derivative is not well behaved in the neighborhood of a particular root, the method may overshoot, and diverge from that root. An example of a function with one root, for which the derivative is not well behaved in the neighborhood of the root, is\n\n:<math>f(x)=|x|^a,\\quad 0 < a < \\tfrac{1}{2}</math>\n\nfor which the root will be overshot and the sequence of {{mvar|x}} will diverge. For {{math|{{var|a}} {{=}} {{sfrac|1|2}}}}, the root will still be overshot, but the sequence will oscillate between two values. For {{math|{{sfrac|1|2}} < {{var|a}} < 1}}, the root will still be overshot but the sequence will converge, and for {{math|{{var|a}} ≥ 1}} the root will not be overshot at all.\n\nIn some cases, Newton's method can be stabilized by using [[successive over-relaxation#Other applications of the method|successive over-relaxation]], or the speed of convergence can be increased by using the same method.\n\n====Stationary point====\nIf a [[stationary point]] of the function is encountered, the derivative is zero and the method will terminate due to [[division by zero]].\n\n====Poor initial estimate====\nA large error in the initial estimate can contribute to non-convergence of the algorithm. To overcome this problem one can often linearise the function that is being optimized using calculus, logs, differentials, or even using evolutionary algorithms, such as the [[stochastic funnel algorithm]]. Good initial estimates lie close to the final globally optimal parameter estimate. In nonlinear regression, the sum of squared errors (SSE) is only \"close to\" parabolic in the region of the final parameter estimates. Initial estimates found here will allow the Newton–Raphson method to quickly converge. It is only here that the [[Hessian matrix]] of the SSE is positive and the first derivative of the SSE is close to zero.\n\n====Mitigation of non-convergence====\nIn a robust implementation of Newton's method, it is common to place limits on the number of iterations, bound the solution to an interval known to contain the root, and combine the method with a more robust root finding method.\n\n===Slow convergence for roots of multiplicity greater than 1===\nIf the root being sought has [[Multiplicity (mathematics)#Multiplicity of a root of a polynomial|multiplicity]] greater than one, the convergence rate is merely linear (errors reduced by a constant factor at each step) unless special steps are taken. When there are two or more roots that are close together then it may take many iterations before the iterates get close enough to one of them for the quadratic convergence to be apparent. However, if the multiplicity <math>m</math> of the root is known, the following modified algorithm preserves the quadratic convergence rate:<ref>{{cite web|title=Accelerated and Modified Newton Methods|url=http://mathfaculty.fullerton.edu/mathews/n2003/newtonacceleratemod.html}}</ref>\n\n:<math>x_{n+1} = x_n - m\\frac{f(x_n)}{f'(x_n)}. </math>\n\nThis is equivalent to using [[successive over-relaxation#Other applications of the method|successive over-relaxation]]. On the other hand, if the multiplicity {{mvar|m}} of the root is not known, it is possible to estimate <math>m</math> after carrying out one or two iterations, and then use that value to increase the rate of convergence.\n\n==Analysis==\nSuppose that the function {{mvar|f}} has a zero at {{mvar|α}}, i.e., {{math|''f''&thinsp;(''α'') {{=}} 0}}, and {{mvar|f}} is differentiable in a [[topological neighborhood|neighborhood]] of {{mvar|α}}.\n\nIf {{mvar|f}} is continuously differentiable and its derivative is nonzero at&nbsp;{{mvar|α}}, then there exists a [[topological neighborhood|neighborhood]] of {{mvar|α}} such that for all starting values {{math|''x''<sub>0</sub>}} in that neighborhood, the [[sequence]] {{math|{''x''<sub>''n''</sub>}|}} will [[limit of a sequence|converge]] to {{mvar|α}}.<ref>{{citation|title=A Theoretical Introduction to Numerical Analysis|first1=Victor S.|last1=Ryaben'kii|first2=Semyon V.|last2=Tsynkov|publisher=CRC Press|year=2006|isbn=9781584886075|page=243|url=https://books.google.com/books?id=V8gWP031-hEC&pg=PA243}}.</ref>\n\nIf the function is continuously differentiable and its derivative is not 0 at {{mvar|α}} and it has a [[second derivative]] at {{mvar|α}} then the convergence is quadratic or faster. If the second derivative is not 0 at {{mvar|α}} then the convergence is merely quadratic. If the third derivative exists and is bounded in a neighborhood of {{mvar|α}}, then:\n:<math>\\Delta x_{i+1} = \\frac{f'' (\\alpha)}{2 f' (\\alpha)} (\\Delta x_{i})^2 + O(\\Delta x_{i})^3 \\,,</math>\n\nwhere\n\n:<math>\\Delta x_i \\triangleq x_i - \\alpha \\,.</math>\n\nIf the derivative is 0 at {{mvar|α}}, then the convergence is usually only linear. Specifically, if {{mvar|f}} is twice continuously differentiable, {{math|''f&thinsp;′''(''α'') {{=}} 0}} and {{math|''f&thinsp;″''(''α'') ≠ 0}}, then there exists a neighborhood of {{mvar|α}} such that for all starting values {{math|''x''<sub>0</sub>}} in that neighborhood, the sequence of iterates converges linearly, with [[rate of convergence|rate]] 1/2<ref>{{harvnb|Süli|Mayers|2003|loc=Exercise 1.6}}</ref> Alternatively if {{math|''f&thinsp;′''(''α'') {{=}} 0}} and {{math|''f&thinsp;′''(''x'') ≠ 0}} for {{math|''x'' ≠ ''α''}}, {{mvar|x}}&nbsp;in a [[topological neighborhood|neighborhood]] {{mvar|U}} of {{mvar|α}}, {{mvar|α}} being a zero of [[Multiplicity (mathematics)|multiplicity]] {{mvar|r}}, and if {{math|''f'' ∈ ''C''<sup>''r''</sup>(''U'')}} then there exists a neighborhood of {{mvar|α}} such that for all starting values {{math|''x''<sub>0</sub>}} in that neighborhood, the sequence of iterates converges linearly.\n\nHowever, even linear convergence is not guaranteed in pathological situations.\n\nIn practice these results are local, and the neighborhood of convergence is not known in advance. But there are also some results on global convergence: for instance, given a right neighborhood {{math|''U''<sub>+</sub>}} of {{mvar|α}}, if {{mvar|f}} is twice differentiable in {{math|''U''<sub>+</sub>}} and if {{math|''f&thinsp;′'' ≠ 0}}, {{math|''f'' · ''f&thinsp;″'' > 0}} in {{math|''U''<sub>+</sub>}}, then, for each {{math|''x''<sub>0</sub>}} in {{math|''U''<sub>+</sub>}} the sequence {{math|''x<sub>k</sub>''}} is monotonically decreasing to {{mvar|α}}.\n\n===Proof of quadratic convergence for Newton's iterative method===\nAccording to [[Taylor's theorem]], any function {{math|''f''&thinsp;(''x'')}} which has a continuous second derivative can be represented by an expansion about a point that is close to a root of {{math|''f''&thinsp;(''x'')}}. Suppose this root is {{mvar|α}}. Then the expansion of {{math|''f''&thinsp;(''α'')}} about {{math|''x''<sub>''n''</sub>}} is:\n{{NumBlk|:|<math>f(\\alpha) = f(x_n) + f'(x_n)(\\alpha - x_n) + R_1 \\,</math>|{{EquationRef|1}}}}\n\nwhere the [[Lagrange remainder|Lagrange form of the Taylor series expansion remainder]] is\n:<math>R_1 = \\frac{1}{2!}f''(\\xi_n)(\\alpha - x_n)^{2} \\,,</math>\n\nwhere {{math|''ξ''<sub>''n''</sub>}} is in between {{math|''x''<sub>''n''</sub>}} and {{mvar|α}}.\n\nSince {{mvar|α}} is the root, ({{EquationNote|1}}) becomes:\n{{NumBlk|:|<math>0 = f(\\alpha) = f(x_n) + f'(x_n)(\\alpha - x_n) + \\tfrac12f''(\\xi_n)(\\alpha - x_n)^2 \\,</math>|{{EquationRef|2}}}}\n\nDividing equation ({{EquationNote|2}}) by {{math|''f&thinsp;′''(''x<sub>n</sub>'')}} and rearranging gives\n{{NumBlk|:|<math> \\frac {f(x_n)}{f'(x_n)}+\\left(\\alpha-x_n\\right) = \\frac {- f'' (\\xi_n)}{2 f'(x_n)}\\left(\\alpha-x_n\\right)^2 </math>|{{EquationRef|3}}}}\n\nRemembering that {{math|''x''<sub>''n'' + 1</sub>}} is defined by\n{{NumBlk|:|<math> x_{n+1} = x_{n} - \\frac {f(x_n)}{f'(x_n)} \\,,</math>|{{EquationRef|4}}}}\n\none finds that\n:<math> \\underbrace{\\alpha - x_{n+1}}_{\\varepsilon_{n+1}} = \\frac {- f'' (\\xi_n)}{2 f'(x_n)} (\\,\\underbrace{\\alpha - x_n}_{\\varepsilon_{n}}\\,)^2 \\,.</math>\n\nThat is,\n{{NumBlk|:|<math> \\varepsilon_{n+1} = \\frac {- f'' (\\xi_n)}{2 f'(x_n)} \\cdot {\\varepsilon_n}^2 \\,.</math>|{{EquationRef|5}}}}\n\nTaking absolute value of both sides gives\n{{NumBlk|:|<math> \\left| {\\varepsilon_{n+1}}\\right| = \\frac {\\left| f'' (\\xi_n) \\right| }{2 \\left| f'(x_n) \\right|} \\cdot {\\varepsilon_n}^2 \\,. </math> |{{EquationRef|6}}}}\n\nEquation ({{EquationNote|6}}) shows that the [[rate of convergence]] is quadratic if the following conditions are satisfied:\n\n# {{math|''f&thinsp;′''(''x'') ≠ 0}}; for all {{math|''x'' ∈ ''I''}}, where {{mvar|I}} is the interval {{math|[''α'' − ''r'', ''α'' + ''r'']}} for some {{math|''r'' ≥ {{abs|''α'' − ''x''<sub>0</sub>}}}};\n# {{math|''f&thinsp;″''(''x'')}} is continuous, for all {{math|''x'' ∈ ''I''}};\n# {{math|''x''<sub>0</sub>}} ''sufficiently'' close to the root {{mvar|α}}.\n\nThe term ''sufficiently'' close in this context means the following:\n{{numbered list|list_style_type=lower-alpha\n|Taylor approximation is accurate enough such that we can ignore higher order terms;\n|<math>\\frac12 \\left |{\\frac {f'' (x_n)}{f'(x_n)}}\\right |<C\\left |{\\frac {f''(\\alpha)}{f'(\\alpha)}}\\right |,</math> for some {{math|''C'' < ∞}};\n|<math>C \\left |{\\frac {f''(\\alpha)}{f'(\\alpha)}}\\right |\\varepsilon_n<1,</math> for {{math|''n'' ∈ '''ℤ''', ''n'' ≥ 0}} and {{mvar|C}} satisfying condition b.}}\n\nFinally, ({{EquationNote|6}}) can be expressed in the following way:\n:<math> \\left | {\\varepsilon_{n+1}}\\right | \\le M{\\varepsilon_n}^2 \\, </math>\nwhere {{mvar|M}} is the [[w:supremum|supremum]] of the variable coefficient of {{math|''ε<sub>n</sub>''<sup>2</sup>}} on the interval {{mvar|I}} defined in condition 1, that is:\n\n: <math>M = \\sup_{x \\in I} \\frac12 \\left |\\frac {f'' (x)}{f'(x)}\\right |. \\,</math>\n\nThe initial point {{math|''x''<sub>0</sub>}} has to be chosen such that conditions 1 to 3 are satisfied, where the third condition requires that {{math|''M'' {{abs|''ε''<sub>0</sub>}} < 1}}.\n\n===Basins of attraction===\nThe disjoint subsets of the [[basin of attraction|basins of attraction]]—the regions of the real number line such that within each region iteration from any point leads to one particular root—can be infinite in number and arbitrarily small. For example,<ref>{{cite journal|last=Dence |first=Thomas |title=Cubics, chaos and Newton's method |journal=[[Mathematical Gazette]] |volume=81 |date=Nov 1997 |pages=403–408 |doi=10.2307/3619617}}</ref> for the function {{math|''f''&thinsp;(''x'') {{=}} ''x''<sup>3</sup> − 2''x''<sup>2</sup> − 11''x'' + 12 {{=}} (''x'' − 4)(''x'' − 1)(''x'' + 3)}}, the following initial conditions are in successive basins of attraction:\n\n:{|\n|{{val|2.35287527}}||converges to||align=right|4;\n|-\n|{{val|2.35284172}}||converges to||align=right|−3;\n|-\n|{{val|2.35283735}}||converges to||align=right|4;\n|-\n|{{val|2.352836327}}||converges to||align=right|−3;\n|-\n|{{val|2.352836323}}||converges to||align=right|1.\n|}\n\n==Failure analysis==\nNewton's method is only guaranteed to converge if certain conditions are satisfied. If the assumptions made in the proof of quadratic convergence are met, the method will converge. For the following subsections, failure of the method to converge indicates that the assumptions made in the proof were not met.\n\n===Bad starting points===\nIn some cases the conditions on the function that are necessary for convergence are satisfied, but the point chosen as the initial point is not in the interval where the method converges. This can happen, for example, if the function whose root is sought approaches zero asymptotically as {{mvar|x}} goes to {{math|∞}} or {{math|−∞}}. In such cases a different method, such as [[Bisection method|bisection]], should be used to obtain a better estimate for the zero to use as an initial point.\n\n====Iteration point is stationary====\nConsider the function:\n\n:<math>f(x) = 1-x^2.</math>\n\nIt has a maximum at {{math|''x'' {{=}} 0}} and solutions of {{math|''f''&thinsp;(''x'') {{=}} 0}} at {{math|''x'' {{=}} ±1}}. If we start iterating from the [[stationary point]] {{math|''x''<sub>0</sub> {{=}} 0}} (where the derivative is zero), {{math|''x''<sub>1</sub>}} will be undefined, since the tangent at (0,1) is parallel to the {{mvar|x}}-axis:\n\n:<math>x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} = 0 - \\frac{1}{0}.</math>\n\nThe same issue occurs if, instead of the starting point, any iteration point is stationary. Even if the derivative is small but not zero, the next iteration will be a far worse approximation.\n\n====Starting point enters a cycle====\n[[Image:NewtonsMethodConvergenceFailure.svg|thumb|300px|The tangent lines of {{math|''x''<sup>3</sup> − 2''x'' + 2}} at 0 and 1 intersect the {{mvar|x}}-axis at 1 and 0 respectively, illustrating why Newton's method oscillates between these values for some starting points.]]\nFor some functions, some starting points may enter an infinite cycle, preventing convergence. Let\n\n:<math>f(x) = x^3 - 2x + 2 \\!</math>\n\nand take 0 as the starting point. The first iteration produces 1 and the second iteration returns to 0 so the sequence will alternate between the two without converging to a root. In fact, this 2-cycle is stable: there are neighborhoods around 0 and around 1 from which all points iterate asymptotically to the 2-cycle (and hence not to the root of the function). In general, the behavior of the sequence can be very complex (see [[Newton fractal]]). The real solution of this equation is {{val|−1.76929235}}….\n\n===Derivative issues===\nIf the function is not continuously differentiable in a neighborhood of the root then it is possible that Newton's method will always diverge and fail, unless the solution is guessed on the first try.\n\n====Derivative does not exist at root====\nA simple example of a function where Newton's method diverges is trying to find the cube root of zero. The cube root is continuous and infinitely differentiable, except for {{math|''x'' {{=}} 0}}, where its derivative is undefined:\n\n:<math>f(x) = \\sqrt[3]{x}.</math>\n\nFor any iteration point {{math|''x<sub>n</sub>''}}, the next iteration point will be:\n\n:<math>x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} = x_n - \\frac{{x_n}^\\frac13}{\\frac13{x_n}^{\\frac13-1}} = x_n - 3x_n = -2x_n.</math>\n\nThe algorithm overshoots the solution and lands on the other side of the {{mvar|y}}-axis, farther away than it initially was; applying Newton's method actually doubles the distances from the solution at each iteration.\n\nIn fact, the iterations diverge to infinity for every {{math|''f''&thinsp;(''x'') {{=}} {{abs|''x''}}<sup>''α''</sup>}}, where {{math|0 < ''α'' < {{sfrac|1|2}}}}. In the limiting case of {{math|''α'' {{=}} {{sfrac|1|2}}}} (square root), the iterations will alternate indefinitely between points {{math|''x''<sub>0</sub>}} and {{math|−''x''<sub>0</sub>}}, so they do not converge in this case either.\n\n====Discontinuous derivative====\nIf the derivative is not continuous at the root, then convergence may fail to occur in any neighborhood of the root. Consider the function\n\n:<math>f(x) = \\begin{cases}\n0 & \\text{if } x = 0,\\\\\nx + x^2\\sin \\frac{2}{x} & \\text{if } x \\neq 0.\n\\end{cases}</math>\n\nIts derivative is:\n:<math>f'(x) = \\begin{cases}\n1 & \\text{if } x = 0,\\\\\n1 + 2x\\sin \\frac{2}{x} - 2\\cos \\frac{2}{x} & \\text{if } x \\neq 0.\n\\end{cases}</math>\n\nWithin any neighborhood of the root, this derivative keeps changing sign as {{math|''x''}} approaches 0 from the right (or from the left) while {{math|''f''&thinsp;(''x'') ≥ ''x'' − ''x''<sup>2</sup> > 0}} for {{math|0 < ''x'' < 1}}.\n\nSo {{math|{{sfrac|''f''&thinsp;(''x'')|''f&thinsp;′''(''x'')}}}} is unbounded near the root, and Newton's method will diverge almost everywhere in any neighborhood of it, even though:\n*the function is differentiable (and thus continuous) everywhere;\n*the derivative at the root is nonzero;\n*{{mvar|f}} is infinitely differentiable except at the root; and\n*the derivative is bounded in a neighborhood of the root (unlike {{math|{{sfrac|''f''&thinsp;(''x'')|''f&thinsp;′''(''x'')}}}}).\n\n===Non-quadratic convergence===\nIn some cases the iterates converge but do not converge as quickly as promised. In these cases simpler methods converge just as quickly as Newton's method.\n\n====Zero derivative====\nIf the first derivative is zero at the root, then convergence will not be quadratic. Let\n\n:<math>f(x) = x^2 \\!</math>\n\nthen {{math|''f&thinsp;′''(''x'') {{=}} 2''x''}} and consequently\n\n:<math>x - \\frac{f(x)}{f'(x)} = \\frac{x}{2} .</math>\n\nSo convergence is not quadratic, even though the function is infinitely differentiable everywhere.\n\nSimilar problems occur even when the root is only \"nearly\" double. For example, let\n\n:<math>f(x) = x^2(x-1000)+1.</math>\n\nThen the first few iterations starting at {{math|''x''<sub>0</sub> {{=}} 1}} are\n:{{math|''x''<sub>0</sub>}} = 1\n:{{math|''x''<sub>1</sub>}} = {{val|0.500250376}}…\n:{{math|''x''<sub>2</sub>}} = {{val|0.251062828}}…\n:{{math|''x''<sub>3</sub>}} = {{val|0.127507934}}…\n:{{math|''x''<sub>4</sub>}} = {{val|0.067671976}}…\n:{{math|''x''<sub>5</sub>}} = {{val|0.041224176}}…\n:{{math|''x''<sub>6</sub>}} = {{val|0.032741218}}…\n:{{math|''x''<sub>7</sub>}} = {{val|0.031642362}}…\nit takes six iterations to reach a point where the convergence appears to be quadratic.\n\n====No second derivative====\nIf there is no second derivative at the root, then convergence may fail to be quadratic. Let\n:<math>f(x) = x + x^\\frac43.</math>\nThen\n:<math>f'(x) = 1 + \\tfrac43 x^\\frac13.</math>\nAnd\n:<math>f''(x) = \\tfrac49 x^{-\\frac23} </math>\nexcept when {{math|''x'' {{=}} 0}} where it is undefined. Given {{math|''x<sub>n</sub>''}},\n\n:<math>x_{n+1} = x_n - \\frac{f(x_n)}{f '(x_n)} = \\frac{\\frac13{x_n}^\\frac43}{1 + \\tfrac43{x_n}^\\frac13} </math>\n\nwhich has approximately {{sfrac|4|3}} times as many bits of precision as {{math|''x<sub>n</sub>''}} has. This is less than the 2 times as many which would be required for quadratic convergence. So the convergence of Newton's method (in this case) is not quadratic, even though: the function is continuously differentiable everywhere; the derivative is not zero at the root; and {{mvar|f}} is infinitely differentiable except at the desired root.\n\n==Generalizations==\n===Complex functions===\n[[Image:newtroot 1 0 0 0 0 m1.png|thumb|Basins of attraction for {{math|''x''<sup>5</sup> − 1 {{=}} 0}}; darker means more iterations to converge.]]\n{{main|Newton fractal}}\nWhen dealing with [[complex analysis|complex functions]], Newton's method can be directly applied to find their zeroes.<ref>{{cite journal|last=[[Peter Henrici (mathematician) |Henrici]] |first=Peter |title= Applied and Computational Complex Analysis |volume=1 |date=1974}}</ref> Each zero has a [[basin of attraction]] in the complex plane, the set of all starting values that cause the method to converge to that particular zero. These sets can be mapped as in the image shown. For many complex functions, the boundaries of the basins of attraction are [[fractal]]s.\n\nIn some cases there are regions in the complex plane which are not in any of these basins of attraction, meaning the iterates do not converge. For example,<ref>{{cite journal|last=Strang |first=Gilbert |title=A chaotic search for {{mvar|i}} |journal=[[The College Mathematics Journal]] |volume=22 |date=Jan 1991 |pages=3–12 |doi=10.2307/2686733}}</ref> if one uses a real initial condition to seek a root of {{math|''x''<sup>2</sup> + 1}}, all subsequent iterates will be real numbers and so the iterations cannot converge to either root, since both roots are non-real. In this case [[almost all]] real initial conditions lead to [[chaos theory|chaotic behavior]], while some initial conditions iterate either to infinity or to repeating cycles of any finite length.\n\nCurt McMullen has shown that for any possible purely iterative algorithm similar to Newton's Method, the algorithm will diverge on some open regions of the complex plane when applied to some polynomial of degree 4 or higher. However, McMullen gave a generally convergent algorithm for polynomials of degree 3.<ref>{{cite journal|last=McMullen |first=Curt |title=Families of rational maps and iterative root-finding algorithms |journal=Annals of Mathematics |series=Second Series |volume=125 |date=1987 |issue=3 |pages=467–493 |doi=10.2307/1971408|url=https://dash.harvard.edu/bitstream/handle/1/9876064/McMullen_FamiliesRationalMap.pdf?sequence=1 }}</ref>\n\n====Chebyshev's third -order method====\n{{empty section|date=February 2019}}\n\n====Nash–Moser iteration====\n{{empty section|date=February 2019}}\n\n===Nonlinear systems of equations===\n===={{math|''k''}} variables, {{math|''k''}} functions{{anchor|multidimensional}}====\nOne may also use Newton's method to solve systems of {{math|''k''}} (nonlinear) equations, which amounts to finding the zeroes of continuously differentiable functions {{math|''F'' : ℝ<sup>''k''</sup> → ℝ<sup>''k''</sup>}}. In the formulation given above, one then has to left multiply with the inverse of the {{math|''k'' × ''k''}} [[Jacobian matrix]] {{math|''J''<sub>''F''</sub>(''x''<sub>''n''</sub>)}} instead of dividing by {{math|''f&thinsp;′''(''x''<sub>''n''</sub>)}}:\n\n:<math>x_{n+1} = x_{n} - J_F(x_n)^{-1}F(x_n)</math>\n\nRather than actually computing the inverse of the Jacobian matrix, one can save time by solving the [[system of linear equations]]\n\n:<math>J_F(x_n) (x_{n+1} - x_n) = -F(x_n)</math>\n\nfor the unknown {{math|''x''<sub>''n'' + 1</sub> − ''x''<sub>''n''</sub>}}.\n\n===={{math|''k''}} variables, {{math|''m''}} equations, with {{math|''m'' > ''k''}}====\nThe {{mvar|k}}-dimensional variant of Newton's method can be used to solve systems of greater than {{mvar|k}} (nonlinear) equations as well if the algorithm uses the [[generalized inverse]] of the non-square [[Jacobian matrix and determinant|Jacobian]] matrix {{math|''J''<sup>+</sup> {{=}} (''J''<sup>T</sup>''J'')<sup>−1</sup>''J''<sup>T</sup>}} instead of the inverse of {{mvar|J}}{{mvar|}}. If the [[system of nonlinear equations|nonlinear system]] has no solution, the method attempts to find a solution in the [[non-linear least squares]] sense. See [[Gauss–Newton algorithm]] for more information.\n\n===Nonlinear equations in a Banach space===\nAnother generalization is Newton's method to find a root of a [[Functional (mathematics)|functional]] {{mvar|F}} defined in a [[Banach space]]. In this case the formulation is\n\n:<math>X_{n+1}=X_n-\\bigl(F'(X_n)\\bigr)^{-1}F(X_n),\\,</math>\n\nwhere {{math|''F′''(''X<sub>n</sub>'')}} is the [[Fréchet derivative]] computed at {{math|''X<sub>n</sub>''}}. One needs the Fréchet derivative to be boundedly invertible at each {{math|''X<sub>n</sub>''}} in order for the method to be applicable. A condition for existence of and convergence to a root is given by the [[Kantorovich theorem|Newton–Kantorovich theorem]].\n\n===Nonlinear equations over {{math|''p''}}-adic numbers===\nIn {{math|''p''}}-adic analysis, the standard method to show a polynomial equation in one variable has a {{math|''p''}}-adic root is [[Hensel's lemma]], which uses the recursion from Newton's method on the {{math|''p''}}-adic numbers. Because of the more stable behavior of addition and multiplication in the {{math|''p''}}-adic numbers compared to the real numbers (specifically, the unit ball in the {{math|''p''}}-adics is a ring), convergence in Hensel's lemma can be guaranteed under much simpler hypotheses than in the classical Newton's method on the real line.\n\n===Newton–Fourier method===\n\nThe Newton–Fourier method is [[Joseph Fourier]]'s extension of Newton's method to provide bounds on the absolute error of the root approximation, while still providing quadratic convergence.\n\nAssume that {{math|''f''&thinsp;(''x'')}} is twice continuously differentiable on {{math|[''a'', ''b'']}} and that {{mvar|f}} contains a root in this interval. Assume that {{math|''f&thinsp;′''(''x''), ''f&thinsp;″''(x) ≠ 0}} on this interval (this is the case for instance if {{math|''f''&thinsp;(''a'') < 0}}, {{math|''f''&thinsp;(''b'') > 0}}, and {{math|''f&thinsp;′''(''x'') > 0}}, and {{math|''f&thinsp;″''(''x'') > 0}} on this interval). This guarantees that there is a unique root on this interval, call it {{mvar|α}}. If it is concave down instead of concave up then replace {{math|''f''&thinsp;(''x'')}} by {{math|−''f''&thinsp;(''x'')}} since they have the same roots.\n\nLet {{math|''x''<sub>0</sub> {{=}} ''b''}} be the right endpoint of the interval and let {{math|''z''<sub>0</sub> {{=}} ''a''}} be the left endpoint of the interval. Given {{math|''x<sub>n</sub>''}}, define\n\n:<math>x_{n + 1} = x_n - \\frac{f(x_n)}{f'(x_n)},</math>\n\nwhich is just Newton's method as before. Then define\n\n:<math>z_{n + 1} = z_n - \\frac{f(z_n)}{f'(x_n)},</math>\n\nwhere the denominator is {{math|''f&thinsp;′''(''x<sub>n</sub>'')}} and not {{math|''f&thinsp;′''(''z<sub>n</sub>'')}}. The iterations {{mvar|x<sub>n</sub>}} will be strictly decreasing to the root while the iterations {{mvar|z<sub>n</sub>}} will be strictly increasing to the root. Also,\n\n:<math>\\lim_{n\\to \\infty} \\frac{x_{n + 1} - z_{n + 1}}{(x_n - z_n)^2} = \\frac{f''(\\alpha)}{2f'(\\alpha)}</math>\n\nso that distance between {{mvar|x<sub>n</sub>}} and {{mvar|z<sub>n</sub>}} decreases quadratically.\n\n===Quasi-Newton methods===\nWhen the Jacobian is unavailable or too expensive to compute at every iteration, a [[quasi-Newton method]] can be used.\n\n===q-analog===\nNewton's method can be generalized with the [[q-analog]] of the usual derivative.<ref>{{harvnb|Rajkovic|Stankovic|Marinkovic|2002}}{{Incomplete short citation|date=February 2019}}</ref>\n\n===Modified Newton methods===\n====Maehly's procedure====\nA nonlinear equation has multiple solutions in general. But if the initial value is not appropriate, Newton's method may not converge to the desired solution or may converge to the same solution found earlier. When we have already found N solutions of <math>f(x)=0</math>, then the next root can be found by applying Newton's method to the next equation:<ref>{{harvnb|Press|Teukolsky|Vetterling|Flannery|1992}}{{Incomplete short citation|date=February 2019}}</ref><ref>{{harvnb|Stoer|Bulirsch|1980}}{{Incomplete short citation|date=February 2019}}</ref>\n\n:<math>F(x) = \\frac{f(x)}{\\prod_{i=1}^N(x-x_i)} = 0 .</math>\n\nThis method is applied to obtain zeros of the [[Bessel function]] of the second kind.<ref>{{harvnb|Zhang|Jin|1996}}{{Incomplete short citation|date=February 2019}}</ref>\n\n====Hirano's modified Newton method====\nHirano's modified Newton method is a modification conserving the convergence of Newton method and avoiding unstableness.<ref>{{cite journal |first=Kazuo |last=Murota |date=1982 |doi=10.1137/0719055 |title=Global Convergence of a Modified Newton Iteration for Algebraic Equations |journal=SIAM J. Numer. Anal. |volume=19 |issue=4 |pages=793–799}}</ref> It is developed to solve complex polynomials.\n\n====Interval Newton's method====\n{{cite check|section|date=February 2019}}\nCombining Newton's method with [[interval arithmetic]] is very useful in some contexts. This provides a stopping criterion that is more reliable than the usual ones (which are a small value of the function or a small variation of the variable between consecutive iterations). Also, this may detect cases where Newton's method converges theoretically but diverges numerically because of an insufficient [[floating-point arithmetic|floating-point precision]] (this is typically the case for polynomials of large degree, where a very small change of the variable may change dramatically the value of the function; see [[Wilkinson's polynomial]]).<ref>Moore, R. E. (1979). ''Methods and applications of interval analysis'' (Vol. 2). Siam.</ref><ref>Hansen, E. (1978). Interval forms of Newtons method. ''Computing'', 20(2), 153-163.</ref>\n\nConsider <math>f \\in \\mathcal{C}^1(X)</math>, where <math>X</math> is a real interval, and suppose that we have an interval extension <math>F'</math> of <math>f'</math>, meaning that <math>F'</math> takes as input an interval <math>Y \\subseteq X</math> and outputs an interval <math>F'(Y)</math> such that:\n: <math>\\begin{align}\n    F'([y,y]) &= \\{f'(y)\\}\\\\[5pt]\n    F'(Y) &\\supseteq \\{f'(y)\\mid y \\in Y\\}.\n\\end{align}</math>\nWe also assume that <math>0 \\notin F'(X)</math>, so in particular <math>f</math> has at most one root in <math>X</math>.\nWe then define the interval Newton operator by:\n\n: <math>N(Y) = m - \\frac{f(m)}{F'(Y)} = \\left\\{\\left.m - \\frac{f(m)}{z} ~\\right|~ z \\in F'(Y)\\right\\}</math>\n\nwhere <math>m \\in Y</math>. Note that the hypothesis on <math>F'</math> implies that <math>N(Y)</math> is well defined and is an interval (see [[interval arithmetic]] for further details on interval operations). This naturally leads to the following sequence:\n: <math>\n\\begin{align}\nX_0 &= X\\\\\nX_{k+1} &= N(X_k) \\cap X_k.\n\\end{align}\n</math>\nThe [[mean value theorem]] ensures that if there is a root of <math>f</math> in <math>X_k</math>, then it is also in <math>X_{k+1}</math>. Moreover, the hypothesis on <math>F'</math> ensures that <math>X_{k+1}</math> is at most half the size of <math>X_k</math> when <math>m</math> is the midpoint of <math>Y</math>, so this sequence converges towards <math>[x^*, x^*]</math>, where <math>x^*</math> is the root of <math>f</math> in <math>X</math>.\n\nIf <math>F'(X)</math> strictly contains <math>0</math>, the use of extended interval division produces a union of two intervals for <math>N(X)</math> ; multiple roots are therefore automatically separated and bounded.\n\n==Applications==\n===Minimization and maximization problems===\n{{main|Newton's method in optimization}}\nNewton's method can be used to find a minimum or maximum of a function <math>f(x)</math>. The derivative is zero at a minimum or maximum, so local minima and maxima can be found by applying Newton's method to the derivative. The iteration becomes:\n\n:<math>x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}. </math>\n\n===Multiplicative inverses of numbers and power series===\nAn important application is [[Division algorithm#Newton–Raphson division|Newton–Raphson division]], which can be used to quickly find the [[Multiplicative inverse|reciprocal]] of a number, using only multiplication and subtraction.\n\nFinding the reciprocal of {{mvar|a}} amounts to finding the root of the function\n:<math>f(x)=a-\\frac{1}{x}</math>\nNewton's iteration is\n:<math> \\begin{align}\nx_{n+1}&=x_n-\\frac{f(x_n)}{f'(x_n)}\\\\[5pt]\n&=x_n-\\frac{a-\\frac{1}{x_n}}{\\frac{1}{x_n^2}}\\\\[5pt]\n&=x_n(2-ax_n)\n\\end{align}\n</math>\nTherefore, Newton's iteration needs only two multiplications and one subtraction.\n\nThis method is also very efficient to compute the multiplicative inverse of a [[power series]].\n\n===Solving transcendental equations===\nMany [[transcendental equation]]s can be solved using Newton's method. Given the equation\n:<math>g(x) = h(x), </math>\nwith {{math|''g''(''x'')}} and/or {{math|''h''(''x'')}} a [[transcendental function]], one writes\n:<math>f(x) = g(x) - h(x). </math>\nThe values of {{mvar|x}} that solve the original equation are then the roots of {{math|''f''&thinsp;(''x'')}}, which may be found via Newton's method.\n\n===Obtaining zeros of special functions===\nNewton's method is applied to the ratio of Bessel functions in order to obtain its root.<ref>{{harvtxt|Gil|Segura|Temme|2007}}{{Incomplete short citation|date=February 2019}}</ref>\n\n===Numerical verification for solutions of nonlinear equations===\nA numerical verification for solutions of nonlinear equations has been established by using Newton's method multiple times and forming a set of solution candidates.<ref>{{harvs|txt|authorlink=William Kahan|last=Kahan|year=1968}}{{Incomplete short citation|date=February 2019}}</ref><ref>{{harvtxt|Krawczyk|1969}}{{Incomplete short citation|date=February 2019}}{{Incomplete short citation|date=February 2019}}</ref>\n\n==Examples==\n===Square root of a number===\nConsider the problem of finding the square root of a number ''S''. Newton's method is one of many [[methods of computing square roots#Babylonian method|methods of computing square roots]].\n\nFor example, if one wishes to find the square root of 612, this is equivalent to finding the solution to\n\n:<math>x^2 = 612</math>\n\nThe function to use in Newton's method is then\n\n:<math>f(x) = x^2 - 612</math>\n\nwith derivative\n\n:<math> f'(x) = 2x. \\, </math>\n\nWith an initial guess of 10, the sequence given by Newton's method is\n\n:<math>\\begin{matrix}\n x_1 & = & x_0 - \\dfrac{f(x_0)}{f'(x_0)} & = & 10 - \\dfrac{10^2 - 612}{2 \\times 10} & = & 35.6\\qquad\\qquad\\qquad\\quad\\;\\,{} \\\\\n x_2 & = & x_1 - \\dfrac{f(x_1)}{f'(x_1)} & = & 35.6 - \\dfrac{35.6^2 - 612}{2 \\times 35.6} & = & \\underline{2}6.395\\,505\\,617\\,978\\dots \\\\\n x_3 & = & \\vdots & = & \\vdots & = & \\underline{24.7}90\\,635\\,492\\,455\\dots \\\\\n x_4 & = & \\vdots & = & \\vdots & = & \\underline{24.738\\,6}88\\,294\\,075\\dots \\\\\n x_5 & = & \\vdots & = & \\vdots & = & \\underline{24.738\\,633\\,753\\,7}67\\dots\n\\end{matrix}\n</math>\n\nwhere the correct digits are underlined. With only a few iterations one can obtain a solution accurate to many decimal places.\n\nRearranging the formula as follows yields the [[Methods_of_computing_square_roots#Babylonian_method|Babylonian method of finding square roots]]:\n\n:<math>x_{n+1} = x_n - \\dfrac{f(x_n)}{f'(x_n)} = x_n - \\dfrac{x_n^2 - S}{2 x_n} = \\dfrac{1}{2}\\Bigl(2 x_n - \\bigl(x_n - \\dfrac{S}{x_n}\\bigr)\\Bigr) = \\dfrac{1}{2}\\Bigl(x_n + \\dfrac{S}{x_n}\\Bigr)</math>\n\ni.e. the [[arithmetic mean]] of the guess, ''x<sub>n</sub>'' and {{sfrac|''S''|''x<sub>n</sub>''}} .\n\n=== Solution of {{math|cos ''x'' {{=}} ''x''<sup>3</sup>}} ===\nConsider the problem of finding the positive number {{math|''x''}} with {{math|cos ''x'' {{=}} ''x''<sup>3</sup>}}. We can rephrase that as finding the zero of {{math|''f''&thinsp;(''x'') {{=}} cos(''x'') − ''x''<sup>3</sup>}}. We have {{math|''f&thinsp;′''(''x'') {{=}} −sin(''x'') − 3''x''<sup>2</sup>}}. Since {{math|cos(''x'') ≤ 1}} for all {{math|''x''}} and {{math|''x''<sup>3</sup> > 1}} for {{math|''x'' > 1}}, we know that our solution lies between 0 and 1. We try a starting value of {{math|''x''<sub>0</sub> {{=}} 0.5}}. (Note that a starting value of 0 will lead to an undefined result, showing the importance of using a starting point that is close to the solution.)\n\n:<math>\\begin{matrix}\n x_1 & = & x_0 - \\dfrac{f(x_0)}{f'(x_0)} & = & 0.5 - \\dfrac{\\cos 0.5 - 0.5^3}{-\\sin 0.5 - 3 \\times 0.5^2} & = & 1.112\\,141\\,637\\,097\\dots \\\\\n x_2 & = & x_1 - \\dfrac{f(x_1)}{f'(x_1)} & = & \\vdots & = & \\underline{0.}909\\,672\\,693\\,736\\dots \\\\\n x_3 & = & \\vdots & = & \\vdots & = & \\underline{0.86}7\\,263\\,818\\,209\\dots \\\\\n x_4 & = & \\vdots & = & \\vdots & = & \\underline{0.865\\,47}7\\,135\\,298\\dots \\\\\n x_5 & = & \\vdots & = & \\vdots & = & \\underline{0.865\\,474\\,033\\,1}11\\dots \\\\\n x_6 & = & \\vdots & = & \\vdots & = & \\underline{0.865\\,474\\,033\\,102}\\dots\n\\end{matrix}\n</math>\n\nThe correct digits are underlined in the above example. In particular, {{math|''x''<sub>6</sub>}} is correct to 12 decimal places. We see that the number of correct digits after the decimal point increases from 2 (for {{math|''x''<sub>3</sub>}}) to 5 and 10, illustrating the quadratic convergence.\n\n==Pseudocode==\nThe following is an example of an implementation of the Newton's Method for finding a root of a function <code>f</code> which has derivative <code>fprime</code>.\n\nThe initial guess will be {{math|''x''<sub>0</sub> {{=}} 1}} and the function will be {{math|''f''&thinsp;(''x'') {{=}} ''x''<sup>2</sup> − 2}} so that {{math|''f&thinsp;′''(''x'') {{=}} 2''x''}}.\n\nEach new iteration of Newton's method will be denoted by <code>x1</code>. We will check during the computation whether the denominator (<code>yprime</code>) becomes too small (smaller than <code>epsilon</code>), which would be the case if {{math|''f&thinsp;′''(''x<sub>n</sub>'') ≈ 0}}, since otherwise a large amount of error could be introduced.\n\n<source lang=\"matlab\">\n%These choices depend on the problem being solved\nx0 = 1 %The initial value\nf = @(x) x^2 - 2 %The function whose root we are trying to find\nfprime = @(x) 2*x %The derivative of f(x)\ntolerance = 10^(-7) %7 digit accuracy is desired\nepsilon = 10^(-14) %Don't want to divide by a number smaller than this\n\nmaxIterations = 20 %Don't allow the iterations to continue indefinitely\nhaveWeFoundSolution = false %Have not converged to a solution yet\n\nfor i = 1 : maxIterations\n\n y = f(x0)\n yprime = fprime(x0)\n\n if(abs(yprime) < epsilon) %Don't want to divide by too small of a number\n % denominator is too small\n break; %Leave the loop\n end\n\n x1 = x0 - y/yprime %Do Newton's computation\n\n if(abs(x1 - x0) <= tolerance * abs(x1)) %If the result is within the desired tolerance\n haveWeFoundSolution = true\n break; %Done, so leave the loop\n end\n\n x0 = x1 %Update x0 to start the process again\n\nend\n\nif (haveWeFoundSolution)\n ... % x1 is a solution within tolerance and maximum number of iterations\nelse\n ... % did not converge\nend\n</source>\n\n==See also==\n{{Div col|colwidth=20em}}\n* [[Aitken's delta-squared process]]\n* [[Bisection method]]\n* [[Euler method]]\n* [[Fast inverse square root]]\n* [[Scoring algorithm#Fisher scoring|Fisher scoring]]\n* [[Gradient descent]]\n* [[Integer square root]]\n* [[Kantorovich theorem]]\n* [[Laguerre's method]]\n* [[Methods of computing square roots]]\n* [[Newton's method in optimization]]\n* [[Richardson extrapolation]]\n* [[Root-finding algorithm]]\n* [[Secant method]]\n* [[Steffensen's method]]\n* [[Subgradient method]]\n{{Div col end}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite book |last1=Gil |first1=A. |last2=Segura |first2=J. |last3=Temme |first3=N. M. |url=https://www.researchgate.net/profile/Nico_Temme/publication/220693008_Numerical_Methods_for_Special_Functions/links/0912f5093e6b002a3d000000.pdf |title=Numerical methods for special functions |isbn=978-0-89871-634-4 |date=2007 |publisher=Society for Industrial and Applied Mathematics |ref=harv}}\n* {{cite book |author1-link=Endre Süli |first1=Endre |last1=Süli |first2=David |last2=Mayers |title=An Introduction to Numerical Analysis |publisher=Cambridge University Press |date=2003 |isbn=0-521-00794-1 |ref=harv}}\n\n==Further reading==\n* Kendall E. Atkinson, ''An Introduction to Numerical Analysis'', (1989) John Wiley & Sons, Inc, {{isbn|0-471-62489-6}}\n* Tjalling J. Ypma, Historical development of the Newton–Raphson method, ''SIAM Review'' '''37''' (4), 531–551, 1995. {{doi|10.1137/1037125}}.\n* {{cite book|last1=Bonnans|first1=J.&nbsp;Frédéric|last2=Gilbert|first2=J.&nbsp;Charles|last3=Lemaréchal|first3=Claude| authorlink3=Claude Lemaréchal|last4=Sagastizábal|first4=Claudia&nbsp;A.|author4-link= Claudia Sagastizábal |title=Numerical optimization: Theoretical and practical aspects|url=https://www.springer.com/mathematics/applications/book/978-3-540-35445-1|edition=Second revised ed. of translation of 1997 <!-- ''Optimisation numérique: Aspects théoriques et pratiques'' --> French| series=Universitext|publisher=Springer-Verlag|location=Berlin|year=2006|pages=xiv+490|isbn=3-540-35445-X|doi=10.1007/978-3-540-35447-5|mr=2265882}}\n* P. Deuflhard, ''Newton Methods for Nonlinear Problems. Affine Invariance and Adaptive Algorithms.'' Springer Series in Computational Mathematics, Vol. 35. Springer, Berlin, 2004. {{isbn|3-540-21099-7}}.\n* C. T. Kelley, ''Solving Nonlinear Equations with Newton's Method'', no 1 in Fundamentals of Algorithms, SIAM, 2003. {{isbn|0-89871-546-6}}.\n* J. M. Ortega, W. C. Rheinboldt, ''Iterative Solution of Nonlinear Equations in Several Variables.'' Classics in Applied Mathematics, SIAM, 2000. {{isbn|0-89871-461-3}}.\n*{{Cite book |last1=Press |first1=W. H. |last2=Teukolsky |first2=S. A. |last3=Vetterling |first3=W. T. |last4=Flannery |first4=B. P. |year=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3rd |publisher=Cambridge University Press |publication-place=New York |isbn=978-0-521-88068-8 |chapter=Chapter 9. Root Finding and Nonlinear Sets of Equations Importance Sampling |chapter-url=http://apps.nrbook.com/empanel/index.html#pg=442}}. See especially Sections [http://apps.nrbook.com/empanel/index.html#pg=456 9.4], [http://apps.nrbook.com/empanel/index.html#pg=473 9.6], and [http://apps.nrbook.com/empanel/index.html#pg=477 9.7].\n* {{Cite document | last1=Kaw | first1=Autar | last2=Kalu | first2=Egwu | year=2008 | title=Numerical Methods with Applications | edition=1st | publisher= | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}.\n\n==External links==\n{{Commons category|Newton Method}}\n{{Wikibooks category|Newton's method}}\n* {{springer|title=Newton method|id=p/n066560}}\n*{{MathWorld|title=Newton's Method|urlname=NewtonsMethod}}\n*[http://en.citizendium.org/wiki/Newton%27s_method Newton's method, Citizendium.]\n*[http://mathfaculty.fullerton.edu/mathews/n2003/newtonacceleratemod.html Mathews, J., The Accelerated and Modified Newton Methods, Course notes.]\n*[http://www.ece.mcmaster.ca/~xwu/part2.pdf Wu, X., Roots of Equations, Course notes.]\n{{Isaac Newton}}\n{{Optimization algorithms}}\n{{Root-finding algorithms}}\n\n{{use dmy dates|date=January 2012}}\n\n{{DEFAULTSORT:Newton's Method}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Newton's method in optimization",
      "url": "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization",
      "text": "[[Image:Newton optimization vs grad descent.svg|right|thumb|A comparison of [[gradient descent]] (green) and Newton's method (red) for minimizing a function (with small step sizes). Newton's method uses [[curvature]] information (i.e. the second derivative) to take a more direct route.]] \n<!--__NOTOC__-->\n\nIn [[calculus]], '''[[Newton's method]]''' is an [[iterative method]] for finding the [[Zero of a function|roots]] of a [[differentiable function]] {{math|''f''}}, which are solutions to the [[equation]] {{math|''f'' (''x'') {{=}} 0}}. In [[Mathematical optimization|optimization]], Newton's method is applied to the [[derivative]] {{math|''f'' ′}} of a [[smooth function|twice-differentiable function]] {{math|''f''}} to find the roots of the derivative (solutions to {{math|''f'' ′(''x'') {{=}} 0}}), also known as the [[stationary point]]s of {{math|''f''}}. These solutions may be minima, maxima, or saddle points.{{Citation needed|date=September 2018}}\n\n==Method==\nIn the one-dimensional problem, Newton's method attempts to find the roots of {{math|''f'' ′}} by constructing a [[sequence]] {{math|''x''<sub>''n''</sub>}} from an initial guess {{math|''x''<sub>0</sub>}} that converges towards some value {{math|''x*''}} satisfying {{math|''f'' ′(''x*'') {{=}} 0}}. This {{math|''x*''}} is a [[stationary point]] of {{math|''f''}}.\n\nThe second-order [[Taylor expansion]] {{math|''f''<sub>''T'' </sub>(''x'')}} of a function {{math|''f''}} around {{math|''x''<sub>''n''</sub>}} is\n\n:<math>f_T(x)=f_T(x_n+\\Delta x) \\approx f(x_n)+f'(x_n)\\Delta x+\\frac 1 2 f''(x_n) \\Delta x^2.</math>\n\nIdeally, we want to pick a {{math|Δ''x''}} such that {{math|''x''<sub>''n''</sub> + Δ''x''}} is a [[stationary point]] of {{math|''f''}}. Using this Taylor expansion as an approximation, we can at least solve for the {{math|Δ''x''}} corresponding to the root of the expansion's derivative:\n\n:<math>\\displaystyle 0 = \\frac{\\rm d}{\\rm d\\Delta x} \\left(f(x_n)+f'(x_n)\\Delta x+\\frac 1 2 f''(x_n) \\Delta x^2\\right) = f'(x_n)+f'' (x_n) \\Delta x</math>\n:<math>\\Delta x = -\\frac{f'(x_n)}{f''(x_n)}.</math>\n\nProvided the Taylor approximation is fairly accurate, then incrementing by the above {{math|Δ''x''}} should yield a point fairly close to an actual stationary point of {{math|''f''}}. This point, {{math|''x''<sub>''n''+1</sub> {{=}} ''x''<sub>''n''</sub> + Δ''x'' {{=}} ''x''<sub>''n''</sub> − ''f'' ′(''x''<sub>''n''</sub>) / ''f'' {{''}}(''x''<sub>''n''</sub>)}}, we define to be the {{math|''n'' + 1}}th guess in Newton's method; as {{math|''n''}} tends toward infinity, {{math|''x''<sub>''n''</sub>}} should approach a stationary point {{math|''x*''}} of the actual function {{Math|''f''}}. Indeed, it is proven that if {{math|''f''}} is a [[smooth function|twice-differentiable function]] and other technical conditions are satisfied, the sequence {{math|''x''<sub>1</sub>, ''x''<sub>2</sub>, …}} will converge to a point {{math|''x*''}} satisfying {{math|''f'' ′(''x*'') {{=}} 0}}.{{Citation needed|date=September 2018}}\n\n==Geometric interpretation==\nThe geometric interpretation of Newton's method is that at each iteration, it amounts to the fitting of a [[paraboloid]] to the [[Surface (mathematics)|surface]] of {{math|''f'' ('''x''')}} at the trial value {{math|'''x'''<sub>''n''</sub>}}, having the same slopes and curvature as the surface at that point, and then proceeding to the maximum or minimum of that paraboloid (in higher dimensions, this may also be a [[saddle point]]).<ref>{{cite book |first=A. W. F. |last=Edwards |authorlink=A. W. F. Edwards |title=Likelihood |location=Baltimore |publisher=Johns Hopkins University Press |edition=Expanded |year=1992 |isbn=0-8018-4443-6 |page=129 |url= }}</ref> Note that if {{math|''f'' ('''x''')}} happens to {{em|be}} a quadratic function, then the exact extremum is found in one step.\n\n==Higher dimensions==\nThe above [[iteration|iterative scheme]] can be generalized to several dimensions by replacing the derivative with the [[gradient]], {{math|∇''f'' ('''x''')}}, and the [[Multiplicative inverse|reciprocal]] of the second derivative with the [[Invertible matrix|inverse]] of the [[Hessian matrix]], {{math|'''H''' ''f'' ('''x''')}}. One obtains the iterative scheme\n\n:<math>\\mathbf{x}_{n+1} = \\mathbf{x}_n - [\\mathbf{H}f(\\mathbf{x}_n)]^{-1} \\nabla f(\\mathbf{x}_n), \\ n \\ge 0.</math>\n\nOften Newton's method is modified to include a small step size {{math|''γ'' ∈ (0,1)}} instead of {{math|''γ'' {{=}} 1}}\n:<math>\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\gamma[\\mathbf{H} f(\\mathbf{x}_n)]^{-1} \\nabla f(\\mathbf{x}_n).</math>\nThis is often done to ensure that the [[Wolfe conditions]] are satisfied at each step {{math|'''x'''<sub>''n''</sub> → '''x'''<sub>''n''+1</sub>}} of the iteration. For step sizes other than 1, the method is often referred to as the relaxed Newton's method.\n\nWhere applicable, Newton's method converges much faster towards a local maximum or minimum than [[gradient descent]]. In fact, every local minimum has a neighborhood {{math|''N''}} such that, if we start with {{math|'''x'''<sub>0</sub> ∈ ''N''}}, Newton's method with step size {{math|''γ'' {{=}} 1}} converges [[rate of convergence|quadratically]] (if the Hessian is [[invertible matrix|invertible]] and a [[Lipschitz continuity|Lipschitz continuous]] function of {{math|'''x'''}} in that neighborhood).\n\nFinding the inverse of the Hessian in high dimensions can be an expensive operation. In such cases, instead of directly inverting the Hessian it's better to calculate the vector {{math|Δ''x'' {{=}} '''x'''<sub>''n + 1''</sub> - '''x'''<sub>''n''</sub>}} as the solution to the [[system of linear equations]]\n\n:<math>[\\mathbf{H} f(\\mathbf{x}_n)] \\mathbf{\\Delta x} = -\\nabla f(\\mathbf{x}_n)</math>\n\nwhich may be solved by various factorizations or approximately (but to great accuracy) using [[iterative methods]]. Many of these methods are only applicable to certain types of equations, for example the [[Cholesky factorization]] and [[Conjugate gradient method|conjugate gradient]] will only work if {{math|['''H''' ''f''('''x'''<sub>''n''</sub>)]}} is a positive definite matrix. While this may seem like a limitation, it's often a useful indicator of something gone wrong; for example if a minimization problem is being approached and {{math|['''H''' ''f''('''x'''<sub>''n''</sub>)]}} is not positive definite, then the iterations are converging to a [[saddle point]] and not a minimum.\n\nOn the other hand, if a [[constrained optimization]] is done (for example, with [[Lagrange multipliers]]), the problem may become one of saddle point finding, in which case the Hessian will be symmetric indefinite and the solution of {{math|'''x'''<sub>''n''+1</sub>}} will need to be done with a method that will work for such, such as the {{math|'''LDL'''<sup>'''T'''</sup>}} variant of [[Cholesky factorization]] or the [[conjugate residual method]].\n\nThere also exist various [[quasi-Newton method]]s, where an approximation for the Hessian (or its inverse directly) is built up from changes in the gradient.\n\nIf the Hessian is close to a non-[[invertible matrix]], the inverted Hessian can be numerically unstable and the solution may diverge. In this case, certain workarounds have been tried in the past, which have varied success with certain problems. One can,  for example, modify the Hessian by adding a correction matrix {{math|''B''<sub>''n''</sub>}} so as to make {{math|H''f''('''x'''<sub>''n''</sub>) + ''B''<sub>''n''</sub>}} positive definite. One approach is to diagonalize {{math|'''H''' ''f''('''x'''<sub>''n''</sub>)}} and choose {{math|''B''<sub>''n''</sub>}} so that {{math|'''H''' ''f''('''x'''<sub>''n''</sub>) + ''B''<sub>''n''</sub>}} has the same eigenvectors as {{math|'''H''' ''f''('''x'''<sub>''n''</sub>)}}, but with each negative eigenvalue replaced by {{math|ϵ > 0}}.\n\nAn approach exploited in the [[Levenberg–Marquardt algorithm]] (which uses an approximate Hessian) is to add a scaled identity matrix to the Hessian, {{math|''μ'''''I'''}}, with the scale adjusted at every iteration as needed. For large {{math|''μ''}} and small Hessian, the iterations will behave like [[gradient descent]] with step size {{math|1 / ''μ''}}. This results in slower but more reliable convergence where the Hessian doesn't provide useful information.\n\n==See also==\n\n*[[Quasi-Newton method]]\n*[[Gradient descent]]\n*[[Gauss–Newton algorithm]]\n*[[Levenberg–Marquardt algorithm]]\n*[[Trust region]]\n*[[Optimization (mathematics)|Optimization]]\n*[[Nelder–Mead method]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book |last= Avriel |first= Mordecai |date= 2003 |title= Nonlinear Programming: Analysis and Methods |publisher= Dover Publishing |isbn= 0-486-43227-0 }}\n* {{cite book |last1= Bonnans |first1= J.&nbsp;Frédéric |last2= Gilbert |first2= J.&nbsp;Charles |last3= Lemaréchal |first3= Claude | authorlink3= Claude Lemaréchal |last4= Sagastizábal |first4= Claudia&nbsp;A. |author4-link= Claudia Sagastizábal |title= Numerical optimization: Theoretical and practical aspects |edition= Second revised ed. of translation of 1997 French <!-- |orig-year= 1997 |language= fr  |trans-title= Optimisation numérique: Aspects théoriques et pratiques --> |series= Universitext |publisher= Springer-Verlag |location= Berlin |year= 2006 |isbn= 3-540-35445-X |doi= 10.1007/978-3-540-35447-5 |mr= 2265882 }}\n* {{Cite book |last1= Fletcher |first1= Roger |title= Practical Methods of Optimization |publisher= [[John Wiley & Sons]] |location= New York |edition= 2nd |isbn= 978-0-471-91547-8 |year= 1987 }}\n* {{cite book |first=Geof H. |last=Givens |first2=Jennifer A. |last2=Hoeting |title=Computational Statistics |location=Hoboken, New Jersey |publisher=John Wiley & Sons |year=2013 |isbn=978-0-470-53331-4 |pages=24–58 }}\n* {{cite book |last1= Nocedal |first1= Jorge |last2= Wright |first2= Stephen J. |date= 1999 |title= Numerical Optimization |publisher= Springer-Verlag |isbn= 0-387-98793-2 }}\n\n==External links==\n* {{Cite web |url= http://bl.ocks.org/dannyko/ffe9653768cb80dfc0da/ |title= Newton-Raphson visualization (1D) |first= Daniel |last= Korenblum |id= ffe9653768cb80dfc0da |date= Aug 29, 2015 |website= [[Mike Bostock|Bl.ocks]] }}\n\n{{Isaac Newton}}\n{{optimization algorithms}}\n\n{{DEFAULTSORT:Newton's Method In Optimization}}\n[[Category:Optimization algorithms and methods]]\n\n[[fr:Méthode de Newton]]"
    },
    {
      "title": "NEWUOA",
      "url": "https://en.wikipedia.org/wiki/NEWUOA",
      "text": "'''NEWUOA'''<ref name=\"report\">{{Cite report |author= Powell, M. J. D. |date= November 2004 | title=The NEWUOA software for unconstrained optimization without derivatives | url=http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2004_08.pdf |publisher=Department of Applied Mathematics and Theoretical Physics, Cambridge University |docket=DAMTP 2004/NA05|accessdate= 2014-01-14}}</ref> is a [[numerical analysis|numerical]] [[optimization (mathematics)|optimization]] [[algorithm]] by [[Michael J. D. Powell]]. It is also the name of [[Michael J. D. Powell|Powell]]'s [[Fortran#FORTRAN 77|Fortran 77]] implementation of the algorithm.\n\nNEWUOA solves unconstrained [[optimization (mathematics)|optimization]] problems without using [[derivative]]s, which makes it a [[derivative-free optimization|derivative-free]] algorithm. The algorithm is [[Iterative method|iterative]] and exploits [[trust region|trust-region]] technique. On each iteration, the algorithm establishes a model function <math> Q_k </math> by [[quadratic interpolation]] and then minimizes <math> Q_k </math> within a [[trust region]].\n\nOne important feature of NEWUOA algorithm is the least [[Frobenius norm]] updating<ref>{{cite journal|last=Powell|first=M. J. D. |title=Least Frobenius norm updating of quadratic models that satisfy interpolation conditions |journal=Mathematical Programming |year=2004 |volume=100 |pages=183–215|doi=10.1007/s10107-003-0490-7}}</ref> technique. Suppose that the [[Optimization problem|objective function]] <math> f </math> has <math> n </math> variables, and one wants to uniquely determine the quadratic model <math> Q_k </math> by purely [[interpolation|interpolating]] the function values of <math> f </math>, then it is necessary to evaluate <math> f </math> at <math> (n+1)(n+2)/2 </math> points, as a quadratic polynomial of <math>n</math> variables has this amount of independent coefficients. But this is impractical when <math> n </math> is large, because the function values are supposed to be expensive in [[derivative-free optimization]]. In NEWUOA, the model <math> Q_k </math> [[interpolation|interpolates]] only <math> m </math> (an integer between <math> n+2 </math> and <math> (n+1)(n+2)/2 </math>, typically of [[Orders of approximation|order]] <math> n </math>) function values of <math> f </math>, and the remaining degrees of freedom are taken up by minimizing the [[Frobenius norm]] of <math> \\nabla^2 Q_k -\\nabla^2 Q_{k-1}</math>. This technique mimics the least-change secant updates<ref>{{cite journal |last= Dennis, Jr. |first= J. E. |last2= Schnabel|first2= R. B.|year= 1979 |title=Least Change Secant Updates for Quasi-Newton Methods |jstor= 2030103 |journal= SIAM Review |volume= 21 |issue= 4 |pages= 443–459 |doi=10.1137/1021091|hdl= 1813/7463 }}</ref> for [[quasi-Newton methods]] and can be considered as the derivative-free version of [[PSB update]] ([[Michael J. D. Powell|Powell]]'s symmetric [[Charles George Broyden|Broyden]] update).<ref>{{cite journal|last=Powell|first=M. J. D. |title=Beyond symmetric Broyden for updating quadratic models in minimization without derivatives |journal=Mathematical Programming |year=2013 |volume=138 |issue=1–2 |pages=475–500 |doi=10.1007/s10107-011-0510-y}}</ref>\n\nTo construct the models, NEWUOA maintains a set of [[interpolation]] points throughout the iterations. The update of this set is another feature of NEWUOA.<ref name=\"report\"/>\n\nNEWUOA algorithm was developed from [[UOBYQA]] (Unconstrained Optimization BY Quadratic Approximation).<ref>{{cite journal|last=Powell|first=M. J. D. |title= UOBYQA: unconstrained optimization by quadratic approximation|journal=Mathematical Programming |year=2002 |volume=92 |issue=3 |pages=555–582 |doi=10.1007/s101070100290|citeseerx=10.1.1.28.1756 }}</ref><ref name=\"code\"/> A major difference between them is that [[UOBYQA]] constructs quadratic models by [[interpolation|interpolating]] the [[Optimization problem|objective function]]  at <math> (n+1)(n+2)/2 </math> points.\n\nNEWUOA software was released on December 16, 2004.<ref name=\"repository\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#powell_software |title=A repository of Professor M. J. D. Powell's software |publisher= |date= |accessdate=2014-01-14}}</ref> It can solve unconstrained optimization problems of a few hundreds variables to high precision without using [[derivative]]s.<ref name=\"report\"/> In the software, <math> m </math> is set to <math> 2n+1 </math> by default.<ref name=\"code\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#newuoa |title=Source code of NEWUOA software |publisher= |date= |accessdate=2014-01-14}}</ref>\n\nOther [[derivative-free optimization]] algorithms by [[Michael J. D. Powell|Powell]] include [[COBYLA]], [[UOBYQA]], [[BOBYQA]], and [[LINCOA]].<ref name=\"repository\"/> [[BOBYQA]] and [[LINCOA]] are extensions of NEWUOA to bound constrained and linearly [[constrained optimization]] respectively.\n\n[[Michael J. D. Powell|Powell]] did not explain how he coined the name \"NEWUOA\" either in the introducing report<ref name=\"report\"/> nor in the software,<ref name=\"code\"/> although [[COBYLA]], [[UOBYQA]], [[BOBYQA]] and [[LINCOA]] are all named by [[acronyms]]. Probably \"NEWUOA\" means \"NEW Unconstrained Optimization Algorithm\".\n\nThe NEWUOA software is distributed under [[GNU Lesser General Public License|The GNU Lesser General Public License]] (LGPL).<ref name=\"code\"/>\n\n==See also==\n* [[TOLMIN (optimization software)|TOLMIN]]\n* [[COBYLA]]\n* [[UOBYQA]]\n* [[BOBYQA]]\n* [[LINCOA]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://ccpforge.cse.rl.ac.uk/gf/project/powell Optimization software by Professor M. J. D. Powell at CCPForge]\n* [https://www.sourceforge.net/projects/newuoa-fortran95 M.Powells f77-coded newuoa ported to Fortran 90/95 with easy user interface]\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Nonlinear programming",
      "url": "https://en.wikipedia.org/wiki/Nonlinear_programming",
      "text": "{{short description|solution process for some optimization problems}}\nIn [[mathematics]], '''nonlinear programming''' ('''NLP''') is the process of solving an [[optimization problem]] where some of the constraints or the objective function are [[nonlinear]]. An [[optimization problem]] is one of calculation of the extrema (maxima, minima or stationary points) of an [[objective function]] over a set of unknown [[Function of a real variable|real variables]] and conditional to the satisfaction of a [[Simultaneous equations|system]] of [[equation|equalities]] and [[inequality (mathematics)|inequalities]], collectively termed [[Constraint (mathematics)|constraints]].<ref>{{cite book\n  | last = Bertsekas\n  | first = Dimitri P.\n  | authorlink = Dimitri P. Bertsekas\n  | title = Nonlinear Programing\n  | edition = Third\n  | publisher = Athena Scientific\n  | year = 2016\n  | location = Cambridge, Massachusetts.\n  | isbn = 978-1-886529-05-2 \n}}</ref>  It is the sub-field of [[mathematical optimization]] that deals with problems that are not linear.\n\n==Applicability==\nA typical non-[[Convex optimization|convex]] problem is that of optimizing transportation costs by selection from a set of transportation methods, one or more of which exhibit [[Economy of scale|economies of scale]], with various connectivities and capacity constraints. An example would be petroleum product transport given a selection or combination of pipeline, rail tanker, road tanker, river barge, or coastal tankship. Owing to economic batch size the cost functions may have discontinuities in addition to smooth changes.\n\nIn experimental science, some simple data analysis (such as fitting a spectrum with a sum of peaks of known location and shape but unknown magnitude) can be done with linear methods, but in general these problems, also, are nonlinear.  Typically, one has a theoretical model of the system under study with variable parameters in it and a model the experiment or experiments, which may also have unknown parameters.  One tries to find a best fit numerically.  In this case one often wants a measure of the precision of the result, as well as the best fit itself.\n\n== Definition ==\n\nLet ''n'', ''m'', and ''p'' be positive integers. Let ''X'' be a subset of ''R<sup>n</sup>'', let ''f'', ''g<sub>i</sub>'', and ''h<sub>j</sub>'' be [[real-valued function]]s on ''X'' for each ''i'' in {''1'', &hellip;, ''m''} and each ''j'' in {''1'', &hellip;, ''p''}, with at least one of ''f'', ''g<sub>i</sub>'', and ''h<sub>j</sub>'' being nonlinear.\n\nA nonlinear minimization problem is an [[optimization problem]] of the form\n\n:<math>\n\\begin{align}\n  \\text{minimize }   & f(x) \\\\\n  \\text{subject to } & g_i(x) \\leq 0 \\text{ for each } i \\in \\{1, \\dotsc, m\\} \\\\\n                    & h_j(x) = 0 \\text{ for each } j \\in \\{1, \\dotsc, p\\} \\\\\n                    & x \\in X.\n\\end{align}\n</math>\n\nA nonlinear maximization problem is defined in a similar way.\n\n== Possible types of constraint set ==\n\nThere are several possibilities for the nature of the constraint set, also known as the feasible set or [[feasible region]].\n\nAn ''infeasible'' problem is one for which no set of values for the choice variables satisfies all the constraints. That is, the constraints are mutually contradictory, and no solution exists; the feasible set is the [[empty set]].\n\nA ''feasible'' problem is one for which there exists at least one set of values for the choice variables satisfying all the constraints.\n\nAn ''unbounded'' problem is a feasible problem for which the objective function can be made to be better than any given finite value. Thus there is no optimal solution, because there is always a feasible solution that gives a better objective function value than does any given proposed solution.\n\n==Methods for solving the problem==\nIf the objective function ''f'' is linear and the constrained [[Euclidean space|space]] is a [[polytope]], the problem is a [[linear programming]] problem, which may be solved using well-known linear programming techniques such as the [[simplex method]].\n\nIf the objective function is [[Concave function|concave]] (maximization problem), or [[Convex function|convex]] (minimization problem) and the constraint set is [[Convex set|convex]], then the program is called convex and general methods from [[convex optimization]] can be used in most cases.\n\nIf the objective function is [[quadratic function|quadratic]] and the constraints are linear, [[quadratic programming]] techniques are used.\n\nIf the objective function is a ratio of a concave and a convex function (in the maximization case) and the constraints are convex, then the problem can be transformed to a convex optimization problem using [[fractional programming]] techniques.\n\nSeveral methods are available for solving nonconvex problems. One approach is to use  special formulations of linear programming problems. Another method involves the use of [[branch and bound]] techniques, where the program is divided into subclasses to be solved with convex (minimization problem) or linear approximations that form a lower bound on the overall cost within the subdivision. With subsequent divisions, at some point an actual solution will be obtained whose cost is equal to the best lower bound obtained for any of the approximate solutions. This solution is optimal, although possibly not unique. The algorithm may also be stopped early, with the assurance that the best possible solution is within a tolerance from the best point found; such points are called ε-optimal. Terminating to ε-optimal points is typically necessary to ensure finite termination. This is especially useful for large, difficult problems and problems with uncertain costs or values where the uncertainty can be estimated with an appropriate reliability estimation.\n\nUnder [[differentiability]] and [[constraint qualification]]s, the [[Karush–Kuhn–Tucker conditions|Karush–Kuhn–Tucker (KKT) conditions]] provide necessary conditions for a solution to be optimal. Under convexity, these conditions are also sufficient. If some of the functions are non-differentiable, [[subderivative|subdifferential]] versions of\n[[Karush–Kuhn–Tucker conditions|Karush–Kuhn–Tucker (KKT) conditions]] are available.<ref>\n{{cite book|last=[[Andrzej Piotr Ruszczyński|Ruszczyński]]|first=Andrzej|title=Nonlinear Optimization|publisher=[[Princeton University Press]]|location=Princeton, NJ|year=2006|pages=xii+454|isbn=978-0691119151 |mr=2199043}}</ref>\n\n==Examples==\n\n===2-dimensional example===\n\n[[Image:Nonlinear programming.svg|200px|thumb|right|The blue region is the [[feasible region]]. The [[tangent line|tangency]] of the line with the feasible region represents the solution. The line is the best achievable [[contour line]] (locus with a given value of the objective function).]]\nA simple problem (shown in the diagram) can be defined by the constraints\n:''x''<sub>1</sub> &ge; 0\n:''x''<sub>2</sub> &ge; 0\n:''x''<sub>1</sub><sup>2</sup> + ''x''<sub>2</sub><sup>2</sup> &ge; 1\n:''x''<sub>1</sub><sup>2</sup> + ''x''<sub>2</sub><sup>2</sup> &le; 2\nwith an objective function to be maximized\n:''f''('''x''') = ''x''<sub>1</sub> + ''x''<sub>2</sub>\nwhere '''x''' = (''x''<sub>1</sub>, ''x''<sub>2</sub>).\n\n===3-dimensional example===\n[[Image:Nonlinear programming 3D.svg|thumb|right|The tangency of the top surface with the constrained space in the center represents the solution.]]\nAnother simple problem (see diagram) can be defined by the constraints\n:''x''<sub>1</sub><sup>2</sup> &minus; ''x''<sub>2</sub><sup>2</sup> + ''x''<sub>3</sub><sup>2</sup> &le; 2\n:''x''<sub>1</sub><sup>2</sup> + ''x''<sub>2</sub><sup>2</sup> + ''x''<sub>3</sub><sup>2</sup> &le; 10\nwith an objective function to be maximized\n:''f''('''x''') = ''x''<sub>1</sub>''x''<sub>2</sub> + ''x''<sub>2</sub>''x''<sub>3</sub>\nwhere '''x''' = (''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>).\n\n==See also==\n* [[Curve fitting]]\n* [[Least squares|Least squares minimization]]\n* [[Linear programming]]\n* [[nl (format)]]\n* [[Nonlinear least squares]]\n* [[List of optimization software]]\n* [[Quadratically constrained quadratic programming]]\n*[[Werner Fenchel]], who created the foundation for nonlinear programming\n\n==References==\n<references/>\n\n==Further reading==\n\n* Avriel, Mordecai (2003). ''Nonlinear Programming: Analysis and Methods.'' Dover Publishing. {{isbn|0-486-43227-0}}.\n* Bazaraa, Mokhtar S. and Shetty, C. M. (1979). ''Nonlinear programming. Theory and algorithms.'' John Wiley & Sons. {{isbn|0-471-78610-1}}.\n* Bertsekas, Dimitri P. (2016). ''Nonlinear Programming: 3rd Edition.'' Athena Scientific. {{isbn|978-1-886529-05-2}}.\n* {{cite book|last1=Bonnans|first1=J.&nbsp;Frédéric|last2=Gilbert|first2=J.&nbsp;Charles|last3=Lemaréchal|first3=Claude| authorlink3=Claude Lemaréchal|last4=Sagastizábal|first4=Claudia&nbsp;A.|author4-link= Claudia Sagastizábal |title=Numerical optimization: Theoretical and practical aspects|url=https://www.springer.com/mathematics/applications/book/978-3-540-35445-1|edition=Second revised ed. of  translation of 1997 <!-- ''Optimisation numérique: Aspects théoriques et pratiques'' --> French| series=Universitext|publisher=Springer-Verlag|location=Berlin|year=2006|pages=xiv+490|isbn=3-540-35445-X|doi=10.1007/978-3-540-35447-5|mr=2265882}}\n* {{cite book|last1=Luenberger|first1=David G.|authorlink1=David G. Luenberger|last2=Ye|first2=Yinyu|authorlink2=Yinyu Ye|title=Linear and nonlinear programming|edition=Third|series=International Series in Operations Research & Management Science|volume=116|publisher=Springer|location=New York|year=2008|pages=xiv+546|isbn=978-0-387-74502-2 | mr = 2423726}}\n* Nocedal, Jorge and Wright, Stephen J. (1999). ''Numerical Optimization.'' Springer. {{isbn|0-387-98793-2}}.\n* [[Jan Brinkhuis]] and Vladimir Tikhomirov, ''Optimization: Insights and Applications'', 2005, Princeton University Press\n\n==External links==\n* [http://glossary.computing.society.informs.org/ Mathematical Programming Glossary]\n\n{{optimization algorithms}}\n\n{{DEFAULTSORT:Nonlinear Programming}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Odds algorithm",
      "url": "https://en.wikipedia.org/wiki/Odds_algorithm",
      "text": "The '''odds-algorithm'''  is a mathematical method for computing optimal\nstrategies for a class of problems that belong to the domain of [[optimal stopping]] problems.  Their solution follows from the ''odds-strategy'', and the importance of the odds-strategy lies in its optimality, as explained below. \nThe odds-algorithm applies to a class of problems called  ''last-success''-problems.  Formally, the objective in these problems is to maximize the probability of identifying in a \nsequence of sequentially observed independent events the last event satisfying a specific criterion (a \"specific event\").  This identification must be done at the time of observation.  No revisiting of preceding observations is permitted.  Usually, a specific\nevent is defined by the decision maker as an event that is of true interest in the view of \"stopping\" to take a well-defined action. Such problems are encountered  in several situations.\n\n==Examples==\nTwo different situations exemplify the interest in maximizing the probability to stop on a last specific event.\n \n# Suppose a car is advertised for sale to the highest bidder (best \"offer\"). Let n potential buyers respond and ask to see the car. Each insists upon an immediate decision from the seller to accept the bid, or not. Define a bid as ''interesting'', and coded 1 if it is better than all preceding bids, and coded 0 otherwise. The bids will form a random sequence of 0s and 1s. Only 1s interest the seller, who may fear that each successive 1 might be the last. It follows from the definition that the very last 1 is the highest bid. Maximizing the probability of selling on the last 1 therefore means maximizing the probability of selling ''best''.\n# A physician, using a special treatment, may use the code 1 for a successful treatment, 0 otherwise. The physician treats a sequence of n patients the same way, and wants to minimize any suffering, and to treat every responsive patient in the sequence. Stopping on the last 1 in such a random sequence of 0s and 1s would achieve this objective.  Since the physician is no prophet, the objective is to maximize the probability of stopping on the last 1. (See [[Compassionate use]].)\n\n== Definitions ==\nConsider a sequence of n independent events. Associate with this sequence another sequence <math> I_1,\\, I_2,\\, \\dots ,\\, I_n </math>  with values 1 or 0.   Here  <math> \\,I_k =1</math> stands for\nthe event that the kth observation is interesting (as defined by the decision maker), and <math>\\, I_k=0</math> for non-interesting.\nLet <math> \\,p_k = P( \\,I_k\\,=1)</math> be the probability that the kth event is interesting. Further let\n<math> \\,q_k = \\,1- p_k </math> and <math> \\,r_k = p_k/q_k</math>. Note that \n<math> \\,r_k</math> represents the [[odds]] of the kth event turning out to be interesting, \nexplaining the name of the odds-algorithm.\n\n== Algorithmic procedure==\n\nThe odds-algorithm sums up the odds in reverse order\n\n:<math> r_n + r_{n-1} + r_{n-2}\\, +\\cdots, \\,  </math>\n\nuntil this sum reaches or exceeds the value 1 for the first time. If this happens at index ''s'', it saves ''s'' and the corresponding sum\n\n:<math> R_s = \\,r_n + r_{n-1} + r_{n-2}  + \\cdots + r_s. \\, </math>\n\nIf the sum of the odds  does not reach 1, it sets ''s''&nbsp;=&nbsp;1.  At the same time it computes\n\n:<math> Q_{s}=q_n q_{n-1}\\cdots q_s.\\,</math>\n\nThe output is\n\n# <math>\\,s</math>, the stopping threshold\n# <math>\\,w = Q_s R_s</math>, the win probability.\n\n== Odds-strategy==\nThe odds-strategy is the rule to observe the events one after the other and to stop\non the first interesting event from index ''s'' onwards (if any), where ''s'' is the stopping threshold of output a.\n\nThe importance of the odds-strategy, and hence of the odds-algorithm, lies in the following odds-theorem.\n\n==Odds-theorem ==\nThe odds-theorem states that\n\n# The odds-strategy is ''optimal'', that is, it maximizes the probability of stopping on the last 1.\n# The win probability of the odds-strategy equals   <math>\\,w= Q_s R_s  </math>\n# If <math>\\, R_s \\ge \\,1 </math>, the win probability <math>\\, w</math> is always at least <math> \\,1/e = 0.368\\dots</math>, and this lower bound is ''best possible''.\n\n==Features==\nThe odds-algorithm computes the optimal ''strategy'' and the optimal ''win probability'' at the same time. Also, the number of operations of the odds-algorithm is (sub)linear in n. Hence no quicker algorithm can possibly\nexist for all sequences, so that the odds-algorithm is, at the same time, optimal as an algorithm.\n\n==Sources==\nF. T. Bruss (2000) devised the odds algorithm, and coined its name. It is also known as Bruss-algorithm (strategy). Free implementations can be found on the web.\n\n==Applications==\nApplications reach from medical questions in [[clinical trial]]s over sales problems, [[secretary problems]], [[portfolio (finance)|portfolio]] selection, (one-way) search strategies, trajectory problems and the [[parking problem]] to problems in on-line maintenance and others.\n\nThere exists, in the same spirit, an Odds-Theorem for continuous-time arrival processes with [[independent increments]] such as the [[Poisson process]] (Bruss (2000)). In some cases, the odds are not necessarily known in advance (as in Example 2 above) so that the application of the odds-algorithm is not directly possible. In this case each step can use [[sequential estimate]]s of the odds. This is meaningful, if the number of unknown parameters is not large compared with the number n of observations. The question of optimality is then more complicated, however, and requires additional studies.  Generalizations of the odds-algorithm allow for different rewards for failing to stop\nand wrong stops as well as replacing independence assumptions by weaker ones (Ferguson (2008)).\n\n== Variations ==\nBruss and Paindaveine (2000) discussed a problem of selecting the last <math> k </math> successes. Tamaki (2010) proved a multiplicative odds theorem which deals with a problem of stopping at any of the last <math> k </math> successes.\n\n== See also ==\n* [[Clinical trial]]\n* [[Expanded access]]\n* [[Secretary problem]]\n\n== References ==\n\n* [[F. Thomas Bruss]]: \"Sum the Odds to One and Stop\", ''[[Annals of Probability]]'' Vol. 28, 1384&ndash;1391 (2000).\n* &mdash;: \"A note on Bounds for the Odds-Theorem of Optimal Stopping\", ''[[Annals of Probability]]'' Vol. 31,  1859&ndash;1862,   (2003).\n* &mdash;: \"The art of a right decision\", ''Newsletter of the [[European Mathematical Society]]'', Issue 62, 14&ndash;20,  (2005).\n* Mitsushi Tamaki: \"Optimal Stopping on Trajectories and the Ballot Problem\", ''[[Journal of Applied Probability]]'' Vol.  38, 946&ndash;959 (2001).\n* Shoo-Ren Hsiao and Jiing-Ru. Yang: \"Selecting the Last Success in Markov-Dependent Trials\", ''[[Journal of Applied Probability]]'', Vol. 93,  271&ndash;281, (2002).\n* E. Thomas, E. Levrat, B. Iung: \"L'algorithme de Bruss comme contribution à une maintenance préventive\", ''[[Sciences et Technologies de l'automation]]'', Vol. 4, 13-18 (2007).\n*T.S. Ferguson: (2008, unpublished)\n*F. T. Bruss and D. Paindaveine: \"Selecting a sequence of last successes in independent trials,\" Journal of Applied Probability, Vol. 37, 389-399, 2000.  \n*M. Tamaki: \"Sum the multiplicative odds to one and stop,\" Journal of Applied Probability, Vol. 47, 761–777, 2010.  \n*T. Matsui and K. Ano: \"A note on a lower bound for the multiplicative odds theorem of optimal stopping,\" Journal of Applied Probability, Vol. 51, 885–889, 2014.\n\n==External links==\n* Bruss-Algorithmus http://www.p-roesler.de/odds.html\n\n{{DEFAULTSORT:Odds Algorithm}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Statistical algorithms]]\n[[Category:Optimal decisions]]"
    },
    {
      "title": "Ordered subset expectation maximization",
      "url": "https://en.wikipedia.org/wiki/Ordered_subset_expectation_maximization",
      "text": "{{about|an algorithm|Israeli food corporation|Osem (company)}}\nIn [[mathematical optimization]], the '''ordered subset expectation maximization''' (OSEM) method <!-- NOT algorithm -->  is an [[iterative method]] that is used in [[computed tomography]].\n\nIn applications in [[medical imaging]], the OSEM method is used  for [[positron emission tomography]], for [[single photon emission computed tomography]], and for [[Computed tomography|X-ray computed tomography]].\n\nThe OSEM method is related to the [[expectation maximization]] (EM) method of statistics. The OSEM method is also related to methods of [[filtered back projection]].\n\n==References==\n*Hudson, H.M., Larkin, R.S. (1994) \"Accelerated image reconstruction using ordered subsets of projection data\", ''IEEE Trans. Medical Imaging'', 13 (4), 601&ndash;609 {{doi|10.1109/42.363108}}\n*Hutton, Brian F., Hudson, H. Malcolm, Beekman, Freek J. (1997) \"A clinical perspective of accelerated statistical reconstruction\", ''European Journal of Nuclear Medicine and Molecular Imaging'', 24 (7), 797&ndash;808 {{doi|10.1007/BF00879671}}\n* Xuan Liu; Comtat, C. ;  Michel, C. ;  Kinahan, P. ;  Defrise, M. ;  Townsend, D. (2001) \"Comparison of 3-D reconstruction with 3D-OSEM and with FORE+OSEM for PET\", ''IEEE Transactions on Medical Imaging'', 20 (8), 804&ndash;814 {{doi|10.1109/42.938248}}\n\n==External links==\n*[http://osem.s-pla.net Home page with IEEE paper and more information]\n*[http://osem.s-pla.net Software]\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Medical statistics]]\n[[Category:Medical imaging]]\n\n\n{{statistics-stub}}"
    },
    {
      "title": "Parallel metaheuristic",
      "url": "https://en.wikipedia.org/wiki/Parallel_metaheuristic",
      "text": "{{Multiple issues|\n{{COI|date=June 2015}}\n{{no footnotes|date=June 2015}}\n{{refimprove|date=June 2015}}\n}}\n\n'''Parallel metaheuristic''' is a class of techniques that are capable of reducing both the numerical effort{{clarify|date=June 2015}} and the run time of a [[metaheuristic]]. To this end, concepts and technologies from the field of parallelism in [[computer science]] are used to enhance and even completely modify the behavior of existing metaheuristics.  Just as it exists a long list of metaheuristics like [[evolutionary algorithm]]s, [[particle swarm]], [[ant colony optimization]], [[simulated annealing]], etc. it also exists a large set of different techniques strongly or loosely based in these ones, whose behavior encompasses the multiple parallel execution of algorithm components that cooperate in some way to solve a problem on a given parallel hardware platform.\n\n==Background==\n\n[[File:Parallel models.png|x300px|thumb | An example of different implementations of the same PSO metaheuristic model.]]\n\nIn practice, optimization (and searching, and learning) problems are often [[NP-hard]], complex, and time consuming.\nTwo major approaches are traditionally used to tackle these problems: exact methods\nand [[metaheuristics]].{{disputed inline|date=June 2015}} Exact methods allow to find exact solutions but are often impractical as they are\nextremely time-consuming for real-world problems (large dimension, hardly constrained, multimodal,\ntime-varying, epistatic problems). Conversely, metaheuristics provide sub-optimal (sometimes optimal)\nsolutions in a reasonable time. Thus, metaheuristics usually allow to meet the resolution delays imposed\nin the industrial field as well as they allow to study general problem classes instead that particular\nproblem instances. In general, many of the best performing techniques in precision and effort to solve\ncomplex and real-world problems are metaheuristics. Their fields of application range from combinatorial\noptimization, bioinformatics, and telecommunications to economics, software engineering, etc. These fields are full of many\ntasks needing fast solutions of high quality. See [http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470293322.html] for more details on complex applications.\n\nMetaheuristics fall in two categories: '''trajectory-based''' metaheuristics and '''population-based''' metaheuristics. The main difference of these two kind of methods relies in the number of tentative\nsolutions used in each step of the (iterative) algorithm. A trajectory-based technique starts with a single\ninitial solution and, at each step of the search, the current solution is replaced by another (often the best)\nsolution found in its neighborhood. It is usual that trajectory-based metaheuristics allow to quickly find\na locally optimal solution, and so they are called '''exploitation-oriented''' methods promoting intensification\nin the search space. On the other hand, population-based algorithms make use of a population of solutions.\nThe initial population is in this case randomly generated (or created with a [[greedy algorithm]]),\nand then enhanced through an iterative process. At each generation of the process, the whole population\n(or a part of it) is replaced by newly generated individuals (often the best ones). These techniques are\ncalled '''exploration-oriented''' methods, since their main ability resides in the diversification in the search\nspace.\n\nMost basic metaheuristics are sequential. Although their utilization allows to significantly reduce\nthe temporal complexity of the search process, this latter remains high for real-world problems arising\nin both academic and industrial domains. Therefore, parallelism comes as a natural way not to only\nreduce the search time, but also to improve the quality of the provided solutions.\n\nFor a comprehensive discussion on how parallelism can be mixed with metaheuristics see [http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471678066.html].\n\n==Parallel trajectory-based metaheuristics==\nMetaheuristics for solving optimization problems could be viewed as ''walks through neighborhoods''\ntracing search trajectories through the solution domains of the problem at hands:\n\n'''Algorithm:''' ''Sequential trajectory-based general pseudo-code''<br />\nGenerate(''s''(0));        // Initial solution <br />\n''t'' := 0;                // Numerical step <br />\n'''while''' '''not''' Termination Criterion(s(t)) '''do''' <br />\n...s′(''t'') := SelectMove(s(''t'')); // Exploration of the neighborhood <br />\n...'''if''' AcceptMove(s′(''t'')) '''then''' <br />\n...s(''t'') := ApplyMove(s′(''t'')); <br />\n...''t'' := ''t''+1; <br />\n'''endwhile'''\n\nWalks are performed by iterative procedures that allow moving from one solution to another one in\nthe solution space (see the above algorithm). This kind of metaheuristics perform the moves in the neighborhood\nof the current solution, i.e., they have a perturbative nature. The walks start from a solution randomly\ngenerated or obtained from another optimization algorithm. At each iteration, the current solution\nis replaced by another one selected from the set of its neighboring candidates. The search process is\nstopped when a given condition is satisfied (a maximum number of generation, find a solution with a\ntarget quality, stuck for a given time, . . . ).\n\nA powerful way to achieve high computational efficiency with trajectory-based methods is the use of\nparallelism. Different parallel models have been proposed for trajectory-based metaheuristics, and three\nof them are commonly used in the literature: the parallel ''multi-start'' model, the parallel\nexploration and evaluation of the ''neighborhood'' (or parallel moves model), and the parallel ''evaluation'' of\na single solution (or move acceleration model):\n\n• '''Parallel multi-start model''': It consists in simultaneously launching several trajectory-based\nmethods for computing better and robust solutions. They may be heterogeneous or homogeneous,\nindependent or cooperative, start from the same or different solution(s), and configured with the\nsame or different parameters.\n\n• '''Parallel moves model''': It is a low-level master-slave model that does not alter the behavior of\nthe heuristic. A sequential search would compute the same result but slower. At the beginning\nof each iteration, the master duplicates the current solution between distributed nodes. Each one\nseparately manages their candidate/solution and the results are returned to the master.\n\n• '''Move acceleration model''': The quality of each move is evaluated in a parallel centralized way.\nThat model is particularly interesting when the evaluation function can be itself parallelized as\nit is CPU time-consuming and/or I/O intensive. In that case, the function can be viewed as an\naggregation of a certain number of partial functions{{clarify|date=June 2015}} that can be run in parallel.\n\n==Parallel population-based metaheuristics==\n\nPopulation-based metaheuristic are stochastic search techniques that have been successfully applied in many real and complex applications (epistatic, multimodal, multi-objective, and highly constrained problems).\nA population-based algorithm is an iterative technique that applies stochastic operators on a pool\nof individuals: the population (see the algorithm below). Every individual in the population is the encoded version of a tentative solution. An evaluation function associates a fitness value to every individual indicating its suitability to the problem. Iteratively, the probabilistic application of variation operators on selected individuals guides the population to tentative solutions of higher quality. The most well-known metaheuristic families based on the manipulation of a population of solutions are [[evolutionary algorithm]]s (EAs), [[ant colony optimization]] (ACO), [[particle swarm optimization]] (PSO), [[scatter search]] (SS), [[differential evolution]] (DE), and [[estimation distribution algorithms]] (EDA).\n\n'''Algorithm:''' ''Sequential population-based metaheuristic pseudo-code'' <br />\nGenerate(P(0)); // Initial population <br />\n''t'' := 0;          // Numerical step <br />\n'''while not''' Termination Criterion(P(''t'')) '''do''' <br />\n...Evaluate(P(''t'')); // Evaluation of the population <br />\n...P′′(''t'') := Apply Variation Operators(P′(''t'')); // Generation of new solutions <br />\n...P(''t'' + 1) := Replace(P(''t''), P′′(''t'')); // Building the next population <br />\n...''t'' := ''t'' + 1; <br />\n'''endwhile'''\n\nFor non-trivial problems, executing the reproductive cycle of a simple population-based method on\nlong individuals and/or large populations usually requires high computational resources. In general, evaluating a ''fitness'' function for every individual is frequently the most costly operation of this algorithm.\nConsequently, a variety of algorithmic issues are being studied to design efficient techniques. These issues usually consist of defining new operators, hybrid algorithms, parallel models, and so on.\n\nParallelism arises naturally when dealing with populations, since each of the individuals belonging to it is an independent unit (at least according to the ''Pittsburg'' style, although there are other approaches like the ''Michigan'' one which do not consider the individual as independent units). Indeed, the performance of population-based algorithms is often improved when running in parallel. Two parallelizing strategies are specially focused on population-based algorithms:\n\n(1) '''Parallelization of computations''', in which the operations commonly applied to each of the individuals are performed in parallel, and\n\n(2) '''Parallelization of population''', in which the population is split in different parts that can be simply exchanged or evolved separately, and then joined later.\n\nIn the beginning of the parallelization history of these algorithms, the well-known '''master-slave''' (also known as ''global parallelization'' or ''farming'') method was used. In this approach, a central processor performs the selection operations while the associated slave processors (workers) run the variation operator and the evaluation of the fitness function. This algorithm has the same behavior as the sequential one, although its computational efficiency is improved, especially for time consuming objective functions. On the other hand, many researchers use a pool of processors to speed up the execution of a sequential algorithm, just because independent runs can be made more rapidly by using several processors than by using a single one. In this case, no interaction at all exists between the independent runs.\n\nHowever, actually most parallel population-based techniques found in the literature utilize some\nkind of spatial disposition for the individuals, and then parallelize the resulting chunks in a pool of processors. Among the most widely known types of structured metaheuristics, the '''distributed''' (or coarse grain) and '''cellular''' (or fine grain) algorithms are very popular optimization procedures.\n\nIn the case of distributed ones, the population is partitioned in a set of subpopulations (islands) in which isolated serial algorithms are executed. Sparse exchanges of individuals are performed among these islands with the goal of introducing some diversity into the subpopulations, thus preventing search of getting stuck in local optima. In order to design a distributed metaheuristic, we{{Who|date=April 2012}} must take several decisions. Among them, a chief decision is to determine the migration policy: topology (logical links between the islands), migration rate (number of individuals that undergo migration in every exchange), migration frequency (number of steps in every subpopulation between two successive exchanges), and the selection/replacement of the migrants.\n\nIn the case of a cellular method, the concept of neighborhood is introduced, so that an individual\nmay only interact with its nearby neighbors in the breeding loop. The overlapped small neighborhood in the algorithm helps in exploring the search space because a slow diffusion of solutions through the population provides a kind of exploration, while exploitation takes place inside each neighborhood. See [https://www.springer.com/business/operations+research/book/978-0-387-77609-5] for more information on cellular Genetic Algorithms and related models.\n\nAlso, hybrid models are being proposed in which a two-level approach of parallelization is undertaken. In general, the higher level for parallelization is a coarse-grained implementation and the basic island performs a cellular, a master-slave method or even another distributed one.\n\n== See also ==\n* [[Cellular Evolutionary Algorithms]]\n* [[Enrique Alba]]\n\n== References ==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n* G. Luque, E. Alba, Parallel Genetic Algorithms. Theory and Real World Applications, Springer-Verlag, {{ISBN|978-3-642-22083-8}}, July 2011\n* [http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470293322.html Alba E., Blum C., Isasi P., León C. Gómez J.A. (eds.), Optimization Techniques for Solving Complex Problems, Wiley], {{ISBN|978-0-470-29332-4}}, 2009\n* [https://www.springer.com/business/operations+research/book/978-0-387-77609-5 E. Alba, B. Dorronsoro, Cellular Genetic Algorithms, Springer-Verlag], {{ISBN|978-0-387-77609-5}}, 2008\n* [https://www.springer.com/east/home?SGWID=5-102-22-138979270-0 N. Nedjah, E. Alba, L. de Macedo Mourelle, Parallel Evolutionary Computations, Springer-Verlag], {{ISBN|3-540-32837-8}}, 2006\n* [http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471678066.html E. Alba, Parallel Metaheuristics: A New Class of Algorithms, Wiley], {{ISBN|0-471-67806-6}}, July 2005\n* [http://neo.lcc.uma.es/software/mallba/index.php MALLBA]\n* [http://neo.lcc.uma.es/software/jgds/index.php JGDS]\n* [http://neo.lcc.uma.es/software/deme/index.php DEME]\n* [http://neo.lcc.uma.es/software/xxga/index.php xxGA]\n* [http://wwwelfin.diees.unict.it/esg/ricerca/psalhe/index.phtml PSALHE-EA]\n* [[Paradiseo]]\n\n[[Category:Genetic algorithms]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Search algorithms]]"
    },
    {
      "title": "Parametric programming",
      "url": "https://en.wikipedia.org/wiki/Parametric_programming",
      "text": "'''Parametric programming''' is a type of [[mathematical optimization]], where the [[optimization problem]] is solved as a function of one or multiple [[parameters]].<ref>{{cite book |last1=Gal |first1=Tomas |year=1995 |title=Postoptimal Analyses, Parametric Programming, and Related Topics: Degeneracy, Multicriteria Decision Making, Redundancy |publisher=W. de Gruyter |location=Berlin |isbn=978-3-11-087120-3 |edition=2nd}}</ref> Developed in parallel to [[sensitivity analysis]], its earliest mention can be found in a [[thesis]] from 1952.<ref>{{cite book |last1=Gal |first1=Tomas |last2=Greenberg |first2=Harvey J. |date=1997 |title=Advances in Sensitivity Analysis and Parametric Programming |publisher=Kluwer Academic Publishers |location=Boston |doi=10.1007/978-1-4615-6103-3 |isbn=978-0-7923-9917-9 |series=International Series in Operations Research & Management Science |volume=6}}</ref> Since then, there have been considerable developments for the cases of multiple parameters, presence of [[integer]] variables as well as nonlinearities. In particular the connection between parametric programming and [[model predictive control]] established in 2000 has contributed to an increased interest in the topic.<ref>{{cite book |last1=Bemporad |first1=Alberto |last2=Morari |first2=Manfred |last3=Dua |first3=Vivek |last4=Pistikopoulos |first4=Efstratios  N. |year=2000 |chapter=The explicit solution of model predictive control via multiparametric quadratic programming |title=Proceedings of the 2000 American Control Conference |pages=872 |doi=10.1109/ACC.2000.876624 |isbn=0-7803-5519-9 }}</ref><ref>{{cite journal |last1=Bemporad |first1=Alberto |last2=Morari |first2=Manfred |last3=Dua |first3=Vivek |last4=Pistikopoulos |first4=Efstratios N. |title=The explicit linear quadratic regulator for constrained systems |journal=Automatica |date=January 2002 |volume=38 |issue=1 |pages=3–20 |citeseerx=10.1.1.67.2946 |doi=10.1016/S0005-1098(01)00174-1}}</ref>\n\n== Notation ==\nIn general, the following optimization problem is considered\n\n: <math>\n\\begin{align}\nJ^*(\\theta) = {} \\min_{x\\in\\mathbb R^n} \\ & f(x,\\theta) \\\\\n\\text{subject to}\\ & g(x,\\theta)\\leq 0. \\\\\n\\theta \\in \\Theta \\subset \\mathbb R^m\n\\end{align}\n</math>\n\nwhere <math>x</math> is the optimization variable, <math>\\theta</math> are the parameters, <math>f(x,\\theta)</math> is the [[objective function]] and <math>g(x,\\theta)</math> denote the [[constraint (mathematics)|constraint]]s. The set <math>\\Theta</math> is generally referred to as parameter space.\n\n== Classification ==\n\nDepending on the nature of <math>f(x,\\theta)</math> and <math>g(x,\\theta)</math> and whether the optimization problem features integer variables, parametric programming problems are classified into different sub-classes:\n* If more than one parameter is present, i.e. <math>m > 1</math>, then it is often referred to as multiparametric programming problem<ref>{{cite journal|last1=Gal|first1=Tomas|last2=Nedoma|first2=Josef|title=Multiparametric Linear Programming|journal=Management Science|date=1972|volume=18|issue=7|pages=406–422|jstor=2629358}}</ref>\n* If integer variables are present, then the problem is referred to as (multi)parametric mixed-integer programming problem<ref>{{cite journal|last1=Dua|first1=Vivek|last2=Pistikopoulos|first2=Efstratios N.|title=Algorithms for the Solution of Multiparametric Mixed-Integer Nonlinear Optimization Problems|journal=Industrial & Engineering Chemistry Research|date=October 1999|volume=38|issue=10|pages=3976–3987|doi=10.1021/ie980792u}}</ref>\n* If constraints are [[Affine transformation|affine]], then additional classifications depending to nature of the objective function in (multi)parametric (mixed-integer) linear, quadratic and nonlinear programming problems is performed. Note that this generally assumes the constraints to be affine.<ref>{{cite book|last1=Pistikopoulos |first1=Efstratios  N. |last2=Georgiadis |first2=Michael C. |last3=Dua |first3=Vivek |date=2007 |title=Multi-parametric Programming Theory, Algorithms and Applications |publisher=Wiley-VCH |location=Weinheim |doi=10.1002/9783527631216 |isbn=9783527316915 }}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Pattern search (optimization)",
      "url": "https://en.wikipedia.org/wiki/Pattern_search_%28optimization%29",
      "text": "{{Other uses|Pattern search (disambiguation){{!}}Pattern search}}\n[[File:Direct search BROYDEN.gif|right|thumb|300px|Example of convergence of a direct search method on the [[Broyden function]]. At each iteration, the pattern either moves to the point which best minimizes its objective function, or shrinks in size if no point is better than the current point, until the desired accuracy has been achieved, or the algorithm reaches a predetermined number of iterations.]]\n'''Pattern search''' (also known as direct search, derivative-free search, or black-box search) is a family of numerical [[Optimization (mathematics)|optimization]] methods that does not require a [[gradient]]. As a result, it can be used on functions that are not [[Continuous function|continuous]] or [[Differentiable function|differentiable]]. One such pattern search method is \"convergence\" (see below), which is based on the theory of positive bases. Optimization attempts to find the best match (the solution that has the lowest error value) in a [[multidimensional analysis]] space of possibilities.\n\n== History ==\nThe name \"pattern search\" was coined by Hooke and Jeeves.<ref name=\"hooke61direct\" /> An early and simple variant is attributed to [[Enrico Fermi|Fermi]] and [[Nicholas Metropolis|Metropolis]] when they worked at the [[Los Alamos National Laboratory]]. It is described by Davidon,<ref name=\"davidon91variable\" /> as follows:\n{{quote|They varied one theoretical parameter at a time by steps of the same magnitude, and when no such increase or decrease in any one parameter further improved the fit to the experimental data, they halved the step size and repeated the process until the steps were deemed sufficiently small.}}\n\n== Convergence ==\nConvergence is a pattern search method proposed by Yu, who proved that it converges using the theory of positive bases.<ref>*Yu, Wen Ci. 1979. “[http://engine.scichina.com/doi/10.1360/za1979-9-S1-53 Positive basis and a class of direct search techniques]”. ''Scientia Sinica'' [''Zhongguo Kexue'']: 53—68. \n*Yu, Wen Ci. 1979. “[http://engine.scichina.com/doi/10.1360/za1979-9-S1-69?scroll=true The convergent property of the simplex evolutionary technique]”. ''Scientia Sinica'' [''Zhongguo Kexue'']: 69–77.</ref> Later, Torczon, Lagarias and co-authors<ref name=torczon97convergence/><ref name=dolan03convergence/> used positive-basis techniques to prove the convergence of another pattern-search method on specific classes of functions. Outside of such classes, pattern search is a [[heuristic#Computer science|heuristic]] that can provide useful approximate solutions for some issues, but can fail on others. Outside of such classes, pattern search is not an [[iterative method]] that converges to a solution; indeed, pattern-search methods can converge to non-stationary points on some relatively tame problems.<ref name=\"PM\">* [[Michael J. D. Powell|Powell, Michael J. D.]] 1973. ”[https://link.springer.com/article/10.1007/BF01584660 On Search Directions for Minimization Algorithms].” ''Mathematical Programming'' 4: 193—201.</ref><ref name=\"McKnnon99\">* {{ cite journal | last = McKinnon | first = K. I. M. | title =Convergence of the Nelder–Mead simplex method to a non-stationary point | journal =SIAM J. Optim. | year = 1999 | volume = 9 | pages =148–158 | doi = 10.1137/S1052623496303482| citeseerx = 10.1.1.52.3900 }} (algorithm summary online).</ref>\n\n== See also ==\n* [[Golden-section search]] conceptually resembles PS in its narrowing of the search range, only for single-dimensional search spaces.\n* [[Nelder&ndash;Mead method]] aka. the simplex method conceptually resembles PS in its narrowing of the search range for multi-dimensional search spaces but does so by maintaining ''n''&nbsp;+&nbsp;1 points for ''n''-dimensional search spaces, whereas PS methods computes 2''n''&nbsp;+&nbsp;1 points (the central point and 2 points in each dimension).\n* [[Luus–Jaakola]] samples from a [[Uniform distribution (continuous)|uniform distribution]] surrounding the current position and uses a simple formula for exponentially decreasing the sampling range.\n* [[Random search]] is a related family of optimization methods that sample from a [[hypersphere]] surrounding the current position.\n* [[Random optimization]] is a related family of optimization methods that sample from a [[normal distribution]] surrounding the current position.\n\n== References ==\n{{reflist|refs=\n<ref name=hooke61direct>\n{{cite journal\n|last1=Hooke\n|first1=R.\n|last2=Jeeves\n|first2=T.A.\n|title=\"Direct search\" solution of numerical and statistical problems\n|journal=Journal of the ACM\n|year=1961\n|volume=8\n|number=2\n|pages=212–229\n|doi=10.1145/321062.321069\n}}\n</ref>\n\n<ref name=davidon91variable>\n{{cite journal\n|last=Davidon\n|first=W.C.\n|title=Variable metric method for minimization\n|journal=SIAM Journal on Optimization\n|year=1991\n|volume=1\n|number=1\n|pages=1–17\n|doi=10.1137/0801001\n|citeseerx=10.1.1.693.272\n}}\n</ref>\n\n<ref name=torczon97convergence>\n{{cite journal\n|last=Torczon\n|first=V.J.\n|title=On the convergence of pattern search algorithms\n|journal=SIAM Journal on Optimization\n|year=1997\n|volume=7\n|number=1\n|pages=1–25\n|doi=10.1137/S1052623493250780\n|url=http://www.cs.wm.edu/~va/research/unc.pdf\n|citeseerx=10.1.1.50.3173\n}}\n</ref>\n\n<ref name=dolan03convergence>\n{{cite journal\n|last1=Dolan\n|first1=E.D.\n|last2=Lewis\n|first2=R.M.\n|last3=Torczon\n|first3=V.J.\n|title=On the local convergence of pattern search\n|journal=SIAM Journal on Optimization\n|year=2003\n|volume=14\n|number=2\n|pages=567–583\n|doi=10.1137/S1052623400374495\n|url=http://www.cs.wm.edu/~va/research/local.pdf\n|citeseerx=10.1.1.78.2407\n}}\n</ref>\n}}\n\n{{Major subfields of optimization}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Penalty method",
      "url": "https://en.wikipedia.org/wiki/Penalty_method",
      "text": "'''Penalty methods''' are a certain class of [[algorithm]]s for solving [[constrained optimization]] problems. \n\nA penalty method replaces a constrained optimization problem by a series of unconstrained problems whose solutions ideally converge to the solution of the original constrained problem.  The unconstrained problems are formed by adding a term, called a '''penalty function''', to the [[objective function]] that consists of a ''penalty parameter'' multiplied by a measure of violation of the constraints.  The measure of violation is nonzero when the constraints are violated and is zero in the region where constraints are not violated.\n\n== Example ==\n\nLet us say we are solving the following constrained problem:\n:<math> \\min f(\\mathbf x) </math>\nsubject to\n:<math> c_i(\\mathbf x) \\le 0 ~\\forall  i \\in I. </math>\n\nThis problem can be solved as a series of unconstrained minimization problems\n:<math> \\min \\Phi_k (\\mathbf x) = f (\\mathbf x) + \\sigma_k ~ \\sum_{i\\in I} ~ g(c_i(\\mathbf x)) </math>\nwhere  \n:<math> g(c_i(\\mathbf x))=\\max(0,c_i(\\mathbf x ))^2. </math>\n\nIn the above equations, <math> g(c_i(\\mathbf x))</math> is the ''exterior penalty function'' while <math>\\sigma_k</math> are the ''penalty coefficients''. In each iteration ''k'' of the method, we increase the penalty coefficient <math>\\sigma_k</math> (e.g. by a factor of 10), solve the unconstrained problem and use the solution as the initial guess for the next iteration. Solutions of the successive unconstrained problems will eventually converge to the solution of the original constrained problem.\n\n== Practical application ==\n\n[[Image compression]] optimization algorithms can make use of penalty functions for selecting how best to compress zones of colour to single representative values.<ref>{{cite journal|last=Galar|first=M.|last2=Jurio|first2=A.|last3=Lopez-Molina|first3= C.|last4=Paternain|first4= D.|last5=Sanz|first5= J.|last6=Bustince|first6=H.|title=Aggregation functions to combine RGB color channels in stereo matching|journal=Optics Express|year=2013|volume=21|pages=1247–1257|doi=10.1364/oe.21.001247}}</ref><ref>{{cite web|title=Researchers restore image using version containing between 1 and 10 percent of information|url=http://phys.org/news/2013-10-image-version-percent.html|publisher=Phys.org (Omicron Technology Limited)|accessdate=26 October 2013}}</ref> \n\n== Barrier methods ==\n\n[[Barrier method (mathematics)|Barrier method]]s constitute an alternative class of algorithms for constrained optimization.  These methods also add a penalty-like term to the objective function, but in this case the iterates are forced to remain interior to the feasible domain and the barrier is in place to bias the iterates to remain away from the boundary of the feasible region.\n\n== See also ==\n* [[Barrier function]]\n* [[Interior point method]]\n* [[Augmented Lagrangian method]]\n\n==References==\n{{reflist}}\n\nSmith, Alice E.; Coit David W. [http://140.138.143.31/teachers/ycliang/heuristic%20optimization%20912/penalty%20function.pdf Penalty functions] Handbook of Evolutionary Computation, Section C 5.2. Oxford University Press and Institute of Physics Publishing, 1996.\n\nCourant, R. [http://www.ams.org/bull/1943-49-01/S0002-9904-1943-07818-4/S0002-9904-1943-07818-4.pdf Variational methods for the solution of problems of equilibrium and vibrations]. Bull. Amer. Math. Soc., 49, 1&ndash;23, 1943.\n\nWotao, Y. [https://web.archive.org/web/20170306141802/http://www.math.ucla.edu/~wotaoyin/math164/slides/wotao_yin_optimization_lec13_algorithms_for_constrained_optimization.pdf Optimization Algorithms for constrained optimization]. Department of Mathematics, UCLA, 2015.\n\n{{optimization algorithms}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Powell's method",
      "url": "https://en.wikipedia.org/wiki/Powell%27s_method",
      "text": "{{Refimprove|date=August 2009}}\n'''Powell's method''', strictly '''Powell's conjugate direction method''', is an [[algorithm]] proposed by [[Michael J. D. Powell]] for finding a [[maxima and minima|local minimum]] of a function. The function need not be differentiable, and no derivatives are taken.\n\nThe function must be a real-valued function of a fixed number of real-valued inputs. The caller passes in the initial point. The caller also passes in a set of initial search vectors. Typically ''N'' search vectors (say<math display=\"inline\">\n\\{s_1, \\dots, s_n\\}</math>) are passed in which are simply the normals aligned to each axis.<ref name=\"mathews\">{{cite web|last1=Mathews|first1=John H.|title=Module  for  Powell Search Method for a Minimum|url=http://mathfaculty.fullerton.edu/mathews/n2003/PowellMethodMod.html|website=California State University, Fullertron|accessdate=16 June 2017}}</ref>\n\nThe method minimises the function by a bi-directional search along each search vector, in turn. The bi-directional line search along each search vector can be done by [[Golden-section search]] or [[Brent's method]]. Let the minima found during each bi-directional line search be <math display=\"inline\">\\{ x_0 + \\alpha_1s_1, {x}_0 + \\sum_{i=1}^{2}\\alpha_i{s}_i, \\dots, {x}_0 +\\sum_{i=1}^{N}\\alpha_i{s}_i \\}</math>, where <math display=\"inline\">{x}_0</math> is the initial starting point and <math display=\"inline\">\\alpha_i</math> is the scalar determined during bi-directional search along <math display=\"inline\">{s}_i</math>. The new position (<math display=\"inline\">x_1</math>) can then be expressed as a linear combination of the search vectors i.e. <math display=\"inline\">x_1 = x_0 + \\sum_{i=1}^N \\alpha_i s_i</math>. The new displacement vector (<math display=\"inline\">\\sum_{i=1}^N \\alpha_i s_i</math>) becomes a new search vector, and is added to the end of the search vector list.  Meanwhile, the search vector which contributed most to the new direction, i.e. the one which was most successful (<math display=\"inline\">i_{d} = \\arg \\max_{i=1}^N \\alpha_i\\|s_i\\|</math>), is deleted from the search vector list. The new set of ''N'' search vectors is <math display=\"inline\">\\{ s_1, \\dots, s_{i_d - 1}, s_{i_d + 1}, \\dots, s_N, \\sum_{i=1}^N \\alpha_i s_i \\}</math>.The algorithm iterates an arbitrary number of times until no significant improvement is made.<ref name=\"mathews\" />\n\nThe method is useful for calculating the local minimum of a continuous but complex function, especially one without an underlying mathematical definition, because it is not necessary to take derivatives. The basic algorithm is simple; the complexity is in the linear searches along the search vectors, which can be achieved via [[Brent's method]].\n\n==References==\n{{reflist}}\n*{{cite journal | last=Powell | first=M. J. D. | year=1964 | title=An efficient method for finding the minimum of a function of several variables without calculating derivatives | journal=Computer Journal | volume=7 | issue=2 | pages=155–162| doi=10.1093/comjnl/7.2.155 }} \n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 10.7. Direction Set (Powell's) Methods in Multidimensions | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=509}}\n*{{Cite book | last=Brent | first=Richard P. | year=1973 | title=Algorithms for minimization without derivatives | publisher=Prentice-Hall | publication-place=Englewood Cliffs, N.J. | isbn=0-486-41998-3 | chapter=Section 7.3: Powell's algorithm | url=https://books.google.com/books?id=6Ay2biHG-GEC&lpg=PP1}}\n\n{{Optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "PSeven",
      "url": "https://en.wikipedia.org/wiki/PSeven",
      "text": "{{lowercase title}}\n{{Infobox software\n| name = pSeven\n| title = pSeven\n| logo = [[File:PSeven logo.png|150px]]\n| logo caption = \n| logo size = \n| logo alt = \n| screenshot = <!-- Image name is enough -->\n| caption = \n| screenshot size = \n| screenshot alt = \n| collapsible = \n| author = \n| developer = DATADVANCE LLC\n| released = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued = \n| latest release version = 6.15.1\n| latest release date = {{Start date and age|2019|06|04|}}\n| latest preview version = \n| latest preview date = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| programming language = \n| operating system = Cross-platform (Windows, Linux)\n| platform = \n| size = \n| language = English\n| language count = <!-- Number only -->\n| language footnote = \n| genre = \n| license = Proprietary\n| alexa = \n| website = {{URL|www.datadvance.net}}\n| standard = \n| AsOf = \n}}\n'''pSeven''' is a [[design space exploration]] software platform developed by DATADVANCE, extending design, simulation and analysis capabilities and assisting in smarter and faster design decisions. It provides a seamless integration with third party [[Computer-aided design|CAD]] and [[Computer-aided engineering|CAE]] software tools, powerful [[Multi-objective optimization|multi-objective]] and [[robust optimization]] algorithms, [[data analysis]] and [[uncertainty quantification]] tools.<ref>{{Cite web|url=http://rustrade.org.uk/eng/?p=2646|title=Software development company DATADVANCE   - Trade Delegation of the Russian Federation in the United Kingdom|website=rustrade.org.uk|access-date=2016-08-03}}</ref>\n\npSeven comes under the notion of [[PIDO]] (Process Integration and Design Optimization) software. [[Design Space Exploration]] functionality is based on the mathematical algorithms of pSeven Core (formerly known as MACROS) Python library,<ref>{{Cite web|url=https://www.datadvance.net/product/pseven-core/|title=pSeven Core - DATADVANCE|website=www.datadvance.net|access-date=2016-08-03}}</ref> also developed by DATADVANCE.\n\nDesign Space Exploration with pSeven allows creating predictive models, integrating CAD/CAE tools, analyze data and models, explore design alternatives and make smart decisions. SmartSelection technology implemented in pSeven automatically selects the most efficient method for a given data or optimization problem that makes advance math easy to use to a wide range of experts.\n\n==History==\nThe foundation for the pSeven Core library as pSeven's background was laid in 2003, when the researchers from the Institute for Information Transmission Problems of the [[Russian Academy of Sciences]] <ref>[http://www.iitp.ru/ru/news/1361.htm Institute of Information Transmission Problems]</ref>  started collaborating with [[Airbus]] to perform R&D in the domains of simulation and data analysis. The first version of pSeven Core library was created in association with [[Airbus|EADS]] Innovation Works in 2009.<ref>[https://www.datadvance.net/assets/files/publications/Interview_2014.pdf Interview of Sergey Morozov, CTO of DATADVANCE], \"The Moscow Times\", issue № 3 (45) 2014</ref> Since 2012,<ref>OraResearch, [http://oraresearch.com/2015/02/design-space-exploration-industry-timeline/#more-908 Design Space Exploration Industry Timeline]</ref> pSeven software platform for simulation automation, [[data analysis]] and [[Mathematical optimization|optimization]] is developed and marketed by DATADVANCE, incorporating pSeven Core.\n\n==Functionality==\npSeven's functionality can be divided into following blocks: '''Data & Model Analysis''', '''Predictive Modeling''', '''Design Optimization''' and '''Process Integration'''.\n\n=== Data & Model Analysis ===\n\npSeven provides a variety of tools for data and model analysis:\n[[File:Design of Experiment.png|thumb|Design of Experiments allows exploring design space using as small number of observations as possible, enables reliable surrogate-based optimization and generates a training sample for building an accurate approximation model.|217x217px]]\n\n==== [[Design of Experiments]] ====\nDesign of Experiments includes the following techniques:\n* Space Filling \n** Batch techniques (Random, [[Factorial experiment|Full-Factorial]], [[Latin hypercube sampling]], Optimal LHS)\n** Sequential techniques ([[Random sequence|Random]], [[Halton sequence|Halton]], [[Sobol sequence|Sobol]], Faure sequences)\n* [[Optimal design|Optimal Designs]] for [[Response surface methodology|RSM]]\n** Composite, D-optimal, IV-optimal, [[Box-Behnken design|Box Behnken]]\n* Adaptive [[Design of experiments|DoE]] with Uniform, Maximum Variance and Integrated Mean Squared Errors Gain - Maximum Variance criteria.\n\nDesign of Experiments allows controlling the process of surrogate modeling via adaptive sampling plan, which benefits the quality of approximation. As a result, it ensures time and resource saving on experiments and smarter decision-making based on the detailed knowledge of the design space.\n\n==== [[Sensitivity analysis|Sensitivity]] and [[Dependence analysis|Dependency Analysis]] ====\nSensitivity and Dependency Analysis is used to filter non-informative design parameters in the study, ranking the informative ones with respect to their influence on the given response function and selecting parameters that provide the best approximation. It is applied to better understand the variables affecting the design process.\n\n==== [[Uncertainty quantification|Uncertainty Quantification]] ====\nUncertainty Quantification capabilities in pSeven <ref>[http://www.tenlinks.com/news/datadvance-ships-pseven-v4-0-for-data-analysis-optimization/ DATADVANCE Ships pSeven v4.0 for Data Analysis, Optimization], TenLinks CAD, CAM and CAE news</ref> are based on [http://www.openturns.org/ OpenTURNS library]. They are used to improve the quality of the designed products, manage potential risks at the design, manufacturing and operating stages and to guarantee product reliability.\n\n==== [[Dimensionality reduction|Dimension Reduction]] ====\nDimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables.\n\n=== Predictive Modeling ===\n[[Predictive analytics|Predictive modeling]] capabilities in pSeven are based on building, exploring and managing approximation models. pSeven incorporates several proprietary approximation techniques, including methods for ordered and structured data, allowing to understand behavior of user's system with minimal costs, replace expensive computations by approximation models (metamodels, RSM, surrogate models etc.) and make smarter decisions based on detailed knowledge of the design space.<ref>Burnaev E., Prikhodko P., Struzik A., [https://www.datadvance.net/assets/files/publications/eucass2013_-_struzik_-_paper_-_issue_a-1.pdf \"Surrogate models for helicopter loads problems\"], Proceedings of 5th European Conference for Aerospace Science\"</ref>\n\n=== Design Optimization ===\n[[Engineering optimization|Optimization]] algorithms implemented in pSeven allow solving single and [[Multi-objective optimization|multi-objective]] constrained optimization problems as well as [[Robust optimization|robust]] and reliability based design optimization problems. Users can solve both engineering optimization problems with cheap to evaluate semi-analytical models and the problems with expensive (in terms of [[Central processing unit|CPU]] time) objective functions and constraints.<ref>F. Gubarev, V. Kunin, A. Pospelov, [https://arxiv.org/pdf/1304.7226v1.pdf \"Lay-up Optimization of Laminated Composites: Mixed Approach with Exact Feasibility Bounds on Lamination Parameters\"]</ref><ref>Dmitry Khominich, Fedor Gubarev, Alexis Pospelov, [https://www.datadvance.net/assets/files/publications/IFORS_CONFERENCE_DMITRY_KHOMINICH_v1.pdf \"Shape Optimization of Rotating Disks\"], 20th Conference of the International Federation of Operational Research Societies, 2014</ref>\n<!-- Deleted image removed: [[File:Multi-objective optimization.png|thumb|right|Mullti-Objective optimization process]] -->\nSmartSelection technique automatically and adaptively selects the most suitable optimization algorithm for a given optimization problem from a pool of optimization methods and algorithms in pSeven.\n\n=== Process Integration ===\nProcess integration capabilities are used to capture the design process by automating single simulation, trade-off studies and design space exploration. For that, pSeven provides tools to build and automatically run the workflow, to configure and share workflows with other design team members, to distribute computation over different computing resources, including HPC. \nMain process integration tools of pSeven:\n* Graphical user interface and command-line interface for advanced users \n* Comprehensive library of workflow building blocks \n* CAD/CAE integration adapters ([[SolidWorks]], [[CATIA]], [[Siemens NX|NX]], [[PTC Creo]], [[ASCON|KOMPAS-3D]], [[Ansys|ANSYS Workbench]]), CAE solvers and other engineering tools ([[Ansys|ANSYS]] Mechanical, [[Ansys|ANSYS]] CFD, [[Mentor Graphics|FloEFD]], [[Computer Simulation Technology|CST]] [[Computer Simulation Technology|Microwave Studio]], [[MSC ADAMS|ADAMS]], [[Simulink]], [[MATLAB]], [[Scilab]], [[Abaqus]], Unified FEA, [[Nastran]], [[LS-DYNA]], [[CD-adapco|STAR-CCM+]], [[OpenFOAM]], etc.)\n* [[High Performance Computing]] (HPC) capabilities (supported batch systems: [[SLURM]], [[TORQUE]], [[Platform LSF|LSF]])\n* [[Functional Mock-up Interface]] (FMI) for model exchange and [[co-simulation]]\n\n==Application Areas==\npSeven's application areas are different industries such as aerospace,<ref>[http://www.airbus.com/presscentre/pressreleases/press-release-detail/detail/airbus-achieves-multi-objective-optimization-of-its-aircraft-families-with-datadvances-macros/ Airbus achieves multi-objective optimization of its aircraft families with DATADVANCE's \"Macros\" software]</ref><ref>[http://sk.ru/news/b/news/archive/2014/09/10/skolkovo-resident-datadvance-tows-airbus-through-structural-testing-on-new-a350.aspx Skolkovo resident DATADVANCE tows Airbus through structural testing on new A350]</ref> automotive, energy, electronics, biomedical and others.\n\nApplication examples:\n* Multidisciplinary and multi-objective optimization of an aircraft family <ref>Alestra S., Brand C., Druot T., Morozov S., [https://www.datadvance.net/assets/files/publications/druot13.pdf \"Multi-objective Optimization of {{sic|nolink=Y|Aircrafts}} Family at Conceptual Design Stage\"], IPDO 2013 : 4th Inverse problems, design and optimization symposium, 2013 June 26–28, Albi, ed. by O. Fudym, J.-L. Battaglia, G.S. Dulikravich et al., Albi ; Ecole des Mines d'Albi-Carmaux, 2013 ({{ISBN|979-10-91526-01-2}})</ref>\n* Sizing of composite structures in order to reduce their mass subject to various mechanical and manufacturing constraints\n* Construction of quick and accurate behavioral models (surrogate models) in order to enable efficient and secure exchange of models across Extended Enterprise\n* Optimization of the gas path of the steam turbine in order to improve overall turbine efficiency\n* Optimization of layered composite armor in order to reduce its weight <ref>A. Bragov, F. Antonov, S. Morozov, D. Khominich, [https://www.datadvance.net/assets/files/publications/NUMERICAL%20OPTIMIZATION%20OF%20MULTI-LAYERED%20COMPOSITE%20ARMOR.pdf \"Numerical optimization of the multi-layered composite armor\"], Light-Weight Armour Group (LWAG) conference-2014</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.datadvance.net/ Official website]\n\n[[Category:Computer system optimization software]]\n[[Category:Mathematical optimization software]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Quadratic programming",
      "url": "https://en.wikipedia.org/wiki/Quadratic_programming",
      "text": "{{Expert needed|1=Mathematics|reason=Some items in this page need clarification and/or expert verification|talk=quadratic programming|date=February 2017}}\n\n'''Quadratic programming''' (QP) is the process of solving a special type of [[mathematical optimization]] [[optimization problem|problem]]—specifically, a (linearly constrained) quadratic optimization problem, that is, the problem of optimizing (minimizing or maximizing) a [[quadratic function]] of several variables subject to linear [[constrained optimization|constraints]] on these variables. Quadratic programming is a particular type of [[nonlinear programming]].\n\n==Problem formulation==\nThe quadratic programming problem with {{mvar|n}} variables and {{mvar|m}} constraints can be formulated as follows.<ref>{{Cite book | last1=Nocedal | first1=Jorge | last2=Wright | first2=Stephen J. | title=Numerical Optimization | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-30303-1 | year=2006 | page=449 | postscript=<!--None--> }}.</ref>\nGiven:\n* a real-valued, {{mvar|n}}-dimensional vector {{math|'''c'''}},\n* an {{math|''n'' × ''n''}}-dimensional real [[symmetric matrix]] {{mvar|Q}},\n* an {{math|''m'' × ''n''}}-dimensional real matrix {{mvar|A}}, and\n* an {{mvar|m}}-dimensional real vector {{math|'''b'''}},\nthe objective of quadratic programming is to find an {{mvar|n}}-dimensional vector {{math|'''x'''}}, that will\n\n{| cellspacing=\"10\"\n|-\n| <math>\\text{minimize}</math>\n| <math>\\tfrac{1}{2} \\mathbf{x}^\\mathrm{T} Q\\mathbf{x} + \\mathbf{c}^\\mathrm{T} \\mathbf{x}</math>\n|-\n| <math>\\text{subject to}</math>\n| <math> A \\mathbf{x} \\preceq \\mathbf{b},</math>\n|}\nwhere {{math|'''x'''<sup>T</sup>}} denotes the vector [[transpose]] of <math>\\mathbf{x}</math>. The notation {{math|''A'''''x''' ⪯ '''b'''}} means that every entry of the vector {{math|''A'''''x'''}} is less than or equal to the corresponding entry of the vector {{math|'''b'''}}.\n\nA related programming problem, [[quadratically constrained quadratic program]]ming, can be posed by adding quadratic constraints on the variables.\n\n==Solution methods==\n\nFor general problems a variety of methods are commonly used, including\n\n:*[[interior point method|interior point]],\n:*[[active set]],<ref name=\"ioe.engin.umich\">{{cite book|last=Murty|first=Katta G.|title=Linear complementarity, linear and nonlinear programming|series=Sigma Series in Applied Mathematics|volume=3|publisher=Heldermann Verlag|location=Berlin|year=1988|pages=xlviii+629 pp|isbn=978-3-88538-403-8|url=http://ioe.engin.umich.edu/people/fac/books/murty/linear_complementarity_webbook/|mr=949214|deadurl=yes|archiveurl=https://web.archive.org/web/20100401043940/http://ioe.engin.umich.edu/people/fac/books/murty/linear_complementarity_webbook/|archivedate=2010-04-01|df=}}</ref>\n:*[[Augmented Lagrangian method|augmented Lagrangian]],<ref>{{cite journal | first1 = F. | last1 = Delbos | first2 = J.Ch. | last2 = Gilbert | year = 2005 | title = Global linear convergence of an augmented Lagrangian algorithm for solving convex quadratic optimization problems | journal = Journal of Convex Analysis | volume = 12 | pages = 45–69 |url=http://www.heldermann-verlag.de/jca/jca12/jca1203_b.pdf}}</ref>\n:*[[Conjugate gradient method|conjugate gradient]],\n:*[[Gradient projection method|gradient projection]],\n:*extensions of the [[simplex algorithm]].<ref name=\"ioe.engin.umich\" />\n\nIn the case in which ''Q'' is [[positive definite matrix|positive definite]], the problem is a special case of the more general field of [[convex optimization]].\n\n===Equality constraints===\n\nQuadratic programming is particularly simple when ''Q'' is [[positive definite matrix|positive definite]] and there are only equality constraints; specifically, the solution process is linear. By using [[Lagrange multipliers]] and seeking the extremum of the Lagrangian, it may be readily shown that the solution to the equality constrained problem\n\n:<math>\\text{Minimize} \\quad \\tfrac{1}{2} \\mathbf{x}^\\mathrm{T} Q\\mathbf{x} + \\mathbf{c}^\\mathrm{T} \\mathbf{x}</math>\n\n:<math>\\text{subject to} \\quad E\\mathbf{x} =\\mathbf{d}</math>\n\nis given by the linear system\n\n:<math>\n\\begin{bmatrix}\n   Q & E^T \\\\\n   E & 0\n\\end{bmatrix} \n\\begin{bmatrix}\n   \\mathbf x \\\\\n   \\lambda\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n   -\\mathbf c \\\\\n   \\mathbf d\n\\end{bmatrix}\n</math>\n\nwhere <math>\\lambda</math> is a set of Lagrange multipliers which come out of the solution alongside <math>\\mathbf x</math>.\n\nThe easiest means of approaching this system is direct solution (for example, [[LU factorization]]), which for small problems is very practical. For large problems, the system poses some unusual difficulties, most notably that the problem is never positive definite (even if <math>Q</math> is), making it potentially very difficult to find a good numeric approach, and there are many approaches to choose from dependent on the problem.<ref>[https://scholar.google.com/scholar?hl=en&q=saddle+point+indefinite+constrained+linear Google search.]</ref>\n\nIf the constraints don't couple the variables too tightly, a relatively simple attack is to change the variables so that constraints are unconditionally satisfied. For example, suppose <math>\\mathbf d = 0</math> (generalizing to nonzero is straightforward). Looking at the constraint equations:\n\n:<math>E\\mathbf{x} = 0</math>\n\nintroduce a new variable <math>\\mathbf y</math> defined by\n\n:<math>Z \\mathbf{y} = \\mathbf x</math>\n\nwhere <math>\\mathbf y</math> has dimension of <math>\\mathbf x</math> minus the number of constraints. Then\n\n:<math>E Z \\mathbf{y} = 0</math>\n\nand if <math>Z</math> is chosen so that <math>E Z = 0</math> the constraint equation will be always satisfied. Finding such <math>Z</math> entails finding the [[null space]] of <math>E</math>, which is more or less simple depending on the structure of <math>E</math>. Substituting into the quadratic form gives an unconstrained minimization problem:\n\n:<math>\n\\tfrac{1}{2} \\mathbf{x}^T Q\\mathbf{x} + \\mathbf{c}^T \\mathbf{x} \\quad \\Rightarrow \\quad\n\\tfrac{1}{2} \\mathbf{y}^T Z^T Q Z \\mathbf{y} + (Z^T \\mathbf{c})^T \\mathbf{y}\n</math>\n\nthe solution of which is given by:\n\n:<math>\nZ^T Q Z \\mathbf{y} = -Z^T \\mathbf{c}\n</math>\n\nUnder certain conditions on <math>Q</math>, the reduced matrix <math>Z^T Q Z</math> will be positive definite. It is possible to write a variation on the [[conjugate gradient method]] which avoids the explicit calculation of <math>Z</math>.<ref>{{Cite journal\n| last1 = Gould\n| first1 = Nicholas I. M.\n| last2 = Hribar\n| first2 = Mary E.\n| last3 = Nocedal\n| first3 = Jorge\n|date=April 2001\n| title = On the Solution of Equality Constrained Quadratic Programming Problems Arising in Optimization\n| journal = SIAM J. Sci. Comput.\n| pages = 1376–1395\n| volume = 23\n| issue = 4\n| citeseerx = 10.1.1.129.7555\n| accessdate =\n| doi = 10.1137/S1064827598345667\n}}</ref>\n\n==Lagrangian duality==\n{{See also|Dual problem}}\n\nThe Lagrangian [[Dual problem|dual]] of a QP is also a QP. To see that let us focus on the case where <math>c=0</math> and Q is positive definite. We write the [[Lagrange multipliers|Lagrangian]] function as \n:<math>L(x,\\lambda) = \\tfrac{1}{2} x^{T}Qx + \\lambda^{T}(Ax-b). </math>\nDefining the (Lagrangian) dual function <math>g(\\lambda)</math> as <math>g(\\lambda) = \\inf_{x} L(x,\\lambda) </math>, we find an [[infimum]] of <math>L</math>, using <math>\\nabla_{x} L(x,\\lambda)=0</math> and positive-definiteness of Q:\n\n:<big><math> x^* =  -Q^{-1}A^{T}\\lambda.  </math></big>\n\nHence the dual function is \n:<math>g(\\lambda) = -\\tfrac{1}{2}\\lambda^{T}AQ^{-1}A^{T}\\lambda - \\lambda^{T}b,</math>\nand so the Lagrangian dual of the QP is\n\n:<math>\\text{ maximize} \\quad  -\\tfrac{1}{2}\\lambda^{T}AQ^{-1}A^{T}\\lambda - \\lambda^{T}b</math>\n\nBesides the Lagrangian duality theory, there are other duality pairings (e.g. [[Wolfe duality|Wolfe]], etc.).\n\n==Complexity==\n\nFor [[positive-definite matrix|positive definite]] ''Q'', the [[ellipsoid method]] solves the problem in (weakly) [[polynomial time]].<ref>{{cite journal| last=Kozlov | first=M. K. |author2=S. P. Tarasov |author3=[[Leonid Khachiyan|Leonid G. Khachiyan]] | year=1979 | title=[Polynomial solvability of convex quadratic programming] | journal=[[Doklady Akademii Nauk SSSR]] | volume=248 | pages=1049–1051}} Translated in: {{cite journal| journal=Soviet Mathematics - Doklady | volume=20 | pages=1108–1111}}</ref>  If, on the other hand, ''Q'' is indefinite, then the problem is [[NP-hard]].<ref>{{cite journal | last = Sahni | first = S. | title = Computationally related problems | journal = SIAM Journal on Computing | volume = 3 | issue = 4 | pages = 262–279 | year = 1974 | doi=10.1137/0203021| url = http://www.cise.ufl.edu/~sahni/papers/comp.pdf | citeseerx = 10.1.1.145.8685 }}</ref>  In fact, even if ''Q'' has only one negative [[eigenvalue]], the problem is (strongly) [[NP-hard]].<ref>{{cite journal | title = Quadratic programming with one negative eigenvalue is (strongly) NP-hard | first1 = Panos M. | last1 = Pardalos | first2 = Stephen A. | last2 = Vavasis | journal = Journal of Global Optimization | volume = 1 | issue = 1 | year = 1991 | pages = 15–22 | doi=10.1007/bf00120662}}</ref>\n\n==Solvers and scripting (programming) languages==\n\n{| class=\"wikitable\"\n|-\n!Name\n!Brief info\n|-\n|[[AIMMS]]|| A software system for modeling and solving optimization and scheduling-type problems\n|-\n|[[ALGLIB]]|| Dual licensed (GPL/proprietary) numerical library (C++, .NET).\n|-\n|[[AMPL]]|| A popular modeling language for large-scale mathematical optimization.\n|-\n|[[APMonitor]]|| Modeling and optimization suite for [[Linear programming|LP]], QP, [[Nonlinear programming|NLP]], [[Integer programming|MILP]], [[Mixed Integer Nonlinear Programming|MINLP]], and [[Differential algebraic equation|DAE]] systems in MATLAB and Python.\n|-\n|[[CGAL]]|| An open source computational geometry package which includes a quadratic programming solver. \n|-\n|[[CPLEX]]|| Popular solver with an API (C, C++, Java, .Net, Python, Matlab and R).  Free for academics.\n|-\n|[[Microsoft Excel|Excel]] Solver Function|| A nonlinear solver adjusted to spreadsheets in which function evaluations are based on the recalculating cells. Basic version available as a standard add-on for Excel.\n|-\n|[[General Algebraic Modeling System|GAMS]] || A high-level modeling system for mathematical optimization\n|-\n|[[Gurobi]]|| Solver with parallel algorithms for large-scale linear programs, quadratic programs and mixed-integer programs. Free for academic use.\n|-\n|[[IMSL Numerical Libraries|IMSL]]|| A set of mathematical and statistical functions that programmers can embed into their software applications.\n|-\n|[[IPOPT]]|| Ipopt (Interior Point OPTimizer) is a software package for large-scale nonlinear optimization\n|- \n|[[Artelys Knitro]] || An Integrated Package for Nonlinear Optimization\n|-\n|[[Maple (software)|Maple]]|| General-purpose programming language for mathematics.  Solving a quadratic problem in Maple is accomplished via its [http://www.maplesoft.com/support/help/Maple/view.aspx?path=Optimization/QPSolve QPSolve] command.\n|-\n|[[MATLAB]]|| A general-purpose and matrix-oriented programming-language for numerical computing.  Quadratic programming in MATLAB requires the Optimization Toolbox in addition to the base   MATLAB product\n|-\n|[[Mathematica]]|| A general-purpose programming-language for mathematics, including symbolic and numerical capabilities.\n|-\n|[[MOSEK]]|| A solver for large scale optimization with API for several languages (C++, Java, .Net, Matlab and Python)\n|-\n|[[NAG Numerical Library]]|| A collection of mathematical and statistical routines developed by the [[Numerical Algorithms Group]] for multiple programming languages (C, C++, Fortran, Visual Basic, Java and C#) and packages (MATLAB, Excel, R, LabVIEW). The Optimization chapter of the NAG Library includes routines for quadratic programming problems with both sparse and non-sparse linear constraint matrices, together with routines for the optimization of linear, nonlinear, sums of squares of linear or nonlinear functions with nonlinear, bounded or no constraints.  The NAG Library has routines for both local and global optimization, and for continuous or integer problems.\n|-\n|[[GNU Octave]]|| A free (its licence is [[GPL]]v3) general-purpose and matrix-oriented programming-language for numerical computing, similar to MATLAB. Quadratic programming in GNU Octave is available via its [https://www.gnu.org/software/octave/doc/interpreter/Quadratic-Programming.html qp] command\n|-\n|[[R (programming language)|R]] (Fortran)||[[GNU General Public License|GPL]] licensed universal cross-platform statistical computation framework, see its [https://cran.r-project.org/web/packages/quadprog/index.html quadprog] page. [https://github.com/albertosantini/node-quadprog Ported to javascript] under [[MIT License]]. [http://accord-framework.net/ Ported to C#] under [[GNU Lesser General Public License|LGPL]].\n|-\n|[[SAS System|SAS]]/OR|| A suite of solvers for Linear, Integer, Nonlinear, Derivative-Free, Network, Combinatorial and Constraint Optimization; the [[Algebraic modeling language]] [http://support.sas.com/documentation/cdl/en/ormpug/63975/HTML/default/ormpug_optmodel_sect005.htm OPTMODEL]; and a variety of vertical solutions aimed at specific problems/markets, all of which are fully integrated with the [[SAS System]].\n|-\n|[[TK Solver]]|| Mathematical modeling and problem solving software system based on a declarative, rule-based language, commercialized by [http://www.uts.com  Universal Technical Systems, Inc.].\n|-\n|[[TOMLAB]]||Supports global optimization, integer programming, all types of least squares, linear, quadratic and unconstrained programming for [[MATLAB]]. TOMLAB supports solvers like [[Gurobi]], [[CPLEX]], [[SNOPT]] and [[KNITRO]].\n|-\n|[[FICO Xpress|XPRESS]]||Solver for large-scale linear programs, quadratic programs, general nonlinear and mixed-integer programs. Has API for several programming languages, also has a modelling language Mosel and works with AMPL, [[General Algebraic Modeling System|GAMS]]. Free for academic use.\n|}\n\n==See also==\n*[[Support vector machine]]\n*[[Sequential quadratic programming]]\n*[[Linear programming]]\n*[[Critical line method]]\n\n==References==\n\n===Notes===\n{{Reflist}}\n\n===Bibliography===\n* {{cite book|last1=Cottle|first1=Richard W.|last2=Pang|first2=Jong-Shi|last3=Stone|first3=Richard E.|title=The linear complementarity problem | series=Computer Science and Scientific Computing|publisher=Academic Press, Inc.|location=Boston, MA|year=1992|pages=xxiv+762 pp|isbn=978-0-12-192350-1 | mr = 1150683 }}\n* {{cite book|authorlink1=Michael R. Garey|first1=Michael R.|last1=Garey|authorlink2=David S. Johnson|last2=Johnson|first2=David S.| year = 1979 | title = Computers and Intractability: A Guide to the Theory of NP-Completeness | publisher = W.H. Freeman | isbn = 978-0-7167-1045-5}} A6: MP2, pg.245.\n* {{cite web\n |first1=Nicholas I. M.|last1=Gould\n |first2=Philippe L.|last2=Toint|title=A Quadratic Programming Bibliography\n |publisher=RAL Numerical Analysis Group Internal Report 2000-1\n |url=ftp://ftp.numerical.rl.ac.uk/pub/qpbook/qp.pdf\n |date=2000\n}}\n \n==External links==\n*[http://www.numerical.rl.ac.uk/qp/qp.html A page about QP]\n*[http://neos-guide.org/content/quadratic-programming NEOS Optimization Guide: Quadratic Programming]\n\n{{Mathematical programming}}\n{{Optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Quantum annealing",
      "url": "https://en.wikipedia.org/wiki/Quantum_annealing",
      "text": "{{Other uses|Annealing (disambiguation)}}\n{{Use American English|date=January 2019}}\n{{Short description|Quantum physics-based metaheuristic for optimization problems}}\n\n'''Quantum annealing''' ('''QA''') is a [[metaheuristic]] for finding the [[global minimum]] of a given [[objective function]] over a given set of candidate solutions (candidate states), by a process using [[quantum fluctuation]]s.  Quantum annealing is used mainly for problems where the search space is discrete ([[combinatorial optimization]] problems) with many [[local minimum|local minima]]; such as finding the [[ground state]] of a [[spin glass]].<ref name=\"Chakrabarti89\">P. Ray, B. K. Chakrabarti, A. Chakrabarti \"[http://journals.aps.org/prb/abstract/10.1103/PhysRevB.39.11828 Sherrington–Kirkpatrick model in a transverse field: Absence of replica symmetry breaking due to quantum fluctuations]\" Phys. Rev. B '''39''', 11828 (1989). {{doi|10.1103/PhysRevB.39.11828}}.</ref> It was formulated in its present form by T. Kadowaki and H. Nishimori ([[:ja:西森秀稔|ja]]) in \"Quantum annealing in the transverse Ising model\"<ref name=\"Kadowaki98\">T. Kadowaki and H. Nishimori, \"[https://archive.is/20130811142150/http://pre.aps.org/abstract/PRE/v58/i5/p5355_1 Quantum annealing in the transverse Ising model]\"  Phys. Rev. E '''58''', 5355 (1998). {{doi|10.1103/PhysRevE.58.5355}}.</ref> though a proposal in a different form had been made by A. B. Finilla, M. A. Gomez, C. Sebenik and J. D. Doll, in \"Quantum annealing: A new method for minimizing multidimensional functions\".<ref name=\"Finilla94\">A. B. Finilla, M. A. Gomez, C. Sebenik and J. D. Doll, \"[http://www.sciencedirect.com/science/article/pii/0009261494001170 Quantum annealing: A new method for minimizing multidimensional functions]\" Chem. Phys. Lett. '''219''', 343 (1994)</ref>\n\nQuantum annealing starts from a quantum-mechanical superposition of all possible states (candidate states) with equal weights.  Then the system evolves following the time-dependent [[Schrödinger equation]], a natural quantum-mechanical evolution of physical systems. The amplitudes of all candidate states keep changing, realizing a quantum parallelism, according to the time-dependent strength of the transverse field, which causes quantum tunneling between states. If the rate of change of the transverse field is slow enough, the system stays close to the ground state of the instantaneous Hamiltonian (also see [[adiabatic quantum computation]]).<ref name=\"Farhi01\">E. Farhi, J. Goldstone, S. Gutmann, J. Lapan, A. Ludgren and D. Preda, \"[http://www.sciencemag.org/content/292/5516/472 A Quantum adiabatic evolution algorithm applied to random instances of an NP-Complete problem]\" Science '''292''', 472 (2001). {{doi|10.1126/science.1057726}}.</ref> If the rate of change of the transverse field is accelerated, the system may leave the ground state temporarily but produce a higher likelihood of concluding in the ground state of the final problem Hamiltonian, i.e., diabatic quantum computation.<ref name=\"Crosson14\">E. Crosson, E. Farhi, C.Y-Y. Lin, H-H. Lin, and P. Shor, \"Different Strategies for Optimization Using the Quantum Adiabatic Algorithm\"  {{arxiv|1401.7320}}</ref><ref name=\"Muthukrishnan15\">D. Muthukrishnan, T. Albash, and D.A. Lidar, \"When Diabatic Trumps Adiabatic in Quantum Optimization\" {{arxiv|1505.01249}}</ref> The transverse field is finally switched off, and the system is expected to have reached the ground state of the classical [[Ising model]] that corresponds to the solution to the original optimization problem.  An experimental demonstration of the success of quantum annealing for random magnets was reported immediately after the initial theoretical proposal.<ref name=\"Brooke99\">J. Brooke, D. Bitko, T. F. Rosenbaum and G. Aeppli, \"[http://www.sciencemag.org/content/284/5415/779 Quantum annealing of a disordered magnet]\",  Science  '''284''' 779 (1999)</ref> An introduction to combinatorial optimization ([[NP-hardness|NP-hard]]) problems, the general structure of quantum annealing-based algorithms and two examples of this kind of algorithms for solving instances of the max-SAT and Minimum Multicut problems together with an overview of the quantum annealing systems manufactured by [[D-Wave Systems]] are presented in.<ref name=\"VenegasAndraca2018\">S.E. Venegas-Andraca, W. Cruz-Santos, C. McGeoch, and M. Lanzagorta, \"A cross-disciplinary introduction to quantum annealing-based algorithms\".  Contemporary Physics '''59'''(2), pp. 174–196 (2018). {{doi|10.1080/00107514.2018.1450720}}. {{arxiv|1803.03372}}</ref>\n\n==Comparison to simulated annealing==\nQuantum annealing can be compared to [[simulated annealing]], whose \"temperature\" parameter plays a similar role to QA's tunneling field strength. In simulated annealing, the temperature determines the probability of moving to a state of higher \"energy\" from a single current state.  In quantum annealing, the strength of transverse field  determines the quantum-mechanical probability to change the amplitudes of all states in parallel.  Analytical <ref name=\"Morita08\">S. Morita and H. Nishimori, \"[http://scitation.aip.org/content/aip/journal/jmp/49/12/10.1063/1.2995837 Mathematical foundation of quantum annealing]\",  J.Math. Phys. '''49''', 125210 (2008). {{doi|10.1063/1.2995837}}.</ref> and numerical <ref name=\"Santoro06\">G. E. Santoro and E. Tosatti, \"[http://iopscience.iop.org/0305-4470/39/36/R01 Optimization using quantum mechanics: quantum annealing through adiabatic evolution]\"  J. Phys. A '''39''', R393 (2006)</ref> evidence suggests that quantum annealing outperforms simulated annealing under certain conditions (see <ref name=\"Heim15\">B. Heim, T. F. Rønnow, S. V. Isakov and M. Troyer, \"[http://www.sciencemag.org/content/348/6231/215.abstract Quantum versus classical annealing of Ising spin glasses]\" Science '''348''', pp. 215–217  (2015)</ref> for a careful analysis).\n\n== Quantum mechanics: analogy and advantage ==\n[[Image:quant-annl.jpg|right|thumb|300px]]\nThe tunneling field is basically a kinetic energy term that does not commute with the classical potential energy part of the original glass. The whole process can be simulated in a computer using [[quantum Monte Carlo]] (or other stochastic technique), and thus obtain a heuristic algorithm for finding the ground state of the classical glass.\n\nIn the case of annealing a purely mathematical ''objective function'', one may consider the variables in the problem to be classical degrees of freedom, and the cost functions to be the potential energy function (classical Hamiltonian). Then a suitable term consisting of non-commuting variable(s) (i.e. variables that have non-zero commutator with the variables of the original mathematical problem) has to be introduced artificially in the Hamiltonian to play the role of the tunneling field (kinetic part). Then one may carry out the simulation with the quantum Hamiltonian thus constructed (the original function + non-commuting part) just as described above. Here, there is a choice in selecting the non-commuting term and the efficiency of annealing may depend on that.\n\nIt has been demonstrated experimentally as well as theoretically, that quantum annealing can indeed outperform thermal  annealing (simulated annealing) in certain cases, especially where the potential energy (cost) landscape consists of very high but thin barriers surrounding shallow local minima {{Citation needed|date=May 2018}}. Since thermal transition probabilities (proportional to <math>e^{-\\frac{\\Delta}{k_B T}}</math>, with <math>T</math> the temperature and <math>k_B</math> the [[Boltzmann constant]]) depend only on the height <math>\\Delta</math> of the barriers, for very high barriers, it is extremely difficult for thermal fluctuations to get the system out from such local minima. However, as argued earlier in 1989 by Ray, Chakrabarti & Chakrabarti,<ref name=\"Chakrabarti89\" /> the quantum tunneling probability through the same barrier depends not only on the height <math>\\Delta</math> of the barrier, but also on its width <math>w</math> and is approximately given by <math>e^{-\\frac{\\sqrt{\\Delta} w}{\\Gamma}}</math>, where <math>\\Gamma</math> is the tunneling field.<ref name=\"Das05\">A. Das, B.K. Chakrabarti, and R.B. Stinchcombe, \"[https://archive.is/20140113104409/http://pre.aps.org/abstract/PRE/v72/i2/e026701 Quantum annealing in a kinetically constrained system]\", Phys. Rev. E '''72''' art. 026701 (2005)</ref> If the barriers are thin enough (i.e. <math>w \\ll \\sqrt{\\Delta}</math>), quantum fluctuations can surely bring the system out of the shallow local minima. For <math>N</math>-spin glasses, <math>\\Delta</math> is proportional to <math>N</math>, and with a linear annealing schedule for the transverse field, one gets <math>\\tau</math> proportional to <math>e^{\\sqrt{N}}</math> for the annealing time (instead of <math>\\tau</math> proportional to <math>e^N</math> for thermal annealing),<ref name=\"Mukherjee15\">See e.g., S. Mukherjee, and B. K. Chakrabarti  \"Multivariable Optimization: Quantum Annealing & Computation\", Eur. Phys. J. ST 224 pp 17–24 (2015) {{arxiv|1408.3262}}</ref> and can even become <math>N</math>-independent for cases where <math>w </math> decreases faster than (or equal to) <math>1/\\sqrt{N}</math>.\n\nIt is speculated that in a [[quantum computer]], such simulations would be much more efficient and exact than that done in a classical computer, because it can perform the tunneling directly, rather than needing to add it by hand. Moreover, it may be able to do this without the tight error controls needed to harness the [[quantum entanglement]] used in more traditional quantum algorithms. Some confirmation of this is found in exactly solvable models.<ref name='Li-18prl'>{{cite journal|doi=10.1103/PhysRevLett.121.190601|pmid=30468584|title=Quantum annealing and thermalization: insights from integrability|author1=F. Li |author2=V. Y. Chernyak  |author3=N. A. Sinitsyn |journal=[[Physical Review Letters]]|volume=121|issue=19|year=2018|pages=190601|bibcode= 2018arXiv180400371L|arxiv=1804.00371}}</ref>\n\n==D-Wave implementations==\n{{Further|D-Wave Systems#Computer systems|D-Wave Two}}\n\n[[File:DWave 128chip.jpg|thumb|Photograph of a chip constructed by [[D-Wave Systems]], mounted and wire-bonded in a sample holder. The [[D-Wave One]]'s processor is designed to use 128 [[superconducting]] logic elements that exhibit controllable and tunable coupling to perform operations.]]\n\nIn 2011, [[D-Wave Systems]] announced the first commercial quantum annealer on the market by the name D-Wave One and published a paper in Nature on its performance.<ref name=\"Johnson11\">M. W. Johnson et al., \"[http://www.nature.com/nature/journal/v473/n7346/abs/nature10012.html Quantum annealing with manufactured spins]\",  Nature '''473''' 194 (2011)</ref> The company claims this system uses a 128 [[qubit]] processor chipset.<ref name=\"DWave11\">{{cite web |title=Learning to program the D-Wave One |url=http://dwave.wordpress.com/2011/05/11/learning-to-program-the-d-wave-one/ |accessdate=11 May 2011}}</ref>  On May 25, 2011 D-Wave announced that [[Lockheed Martin]] Corporation entered into an agreement to purchase a D-Wave One system.<ref name=\"DWavesys11\">{{cite web | url=http://www.dwavesys.com/en/pressreleases.html#lm_2011 |title= D-Wave Systems sells its first Quantum Computing System to Lockheed Martin Corporation |accessdate=2011-05-30 |date=2011-05-25}}</ref> On October 28, 2011 [[University of Southern California|USC]]'s [[Information Sciences Institute]] took delivery of Lockheed's D-Wave One.\n\nIn May 2013 it was announced that a consortium of [[Google]], [[NASA Ames]] and the non-profit [[Universities Space Research Association]] purchased an adiabatic quantum computer from D-Wave Systems with 512 qubits.<ref name=\"Jones13\">N. Jones, \"[http://www.nature.com/news/google-and-nasa-snap-up-quantum-computer-1.12999. Google and NASA snap up quantum computer]\", Nature (2013), {{doi|10.1038/nature.2013.12999}}</ref><ref name=\"Smelyanskiy12\">V. N. Smelyanskiy, [[Eleanor Rieffel|E. G. Rieffel]], S. I. Knysh, C. P. Williams, M. W. Johnson, M. C. Thom, W. G. Macready, K. L. Pudenz, \"A Near-Term Quantum Computing Approach for Hard Computational Problems in Space Exploration\", {{arxiv|1204.2821}}</ref> An extensive study of its performance as quantum annealer, compared to some classical annealing algorithms, is already available.<ref name=\"Boixo14\">S. Boixo, T. F. Rønnow, S. V. Isakov, Z. Wang, D. Wecker, D. A. Lidar, J. M. Martinis, M. Troyer, \"[http://www.nature.com/nphys/journal/v10/n3/full/nphys2900.html Evidence for quantum annealing with more than one hundred qubits]\", Nature Physics, '''10''', pp. 218–224 (2014)\"</ref>\n\nIn June 2014, D-Wave announced a new quantum applications ecosystem with computational finance firm [[1QB Information Technologies]] (1QBit) and cancer research group DNA-SEQ to focus on solving real-world problems with quantum hardware.<ref name=\"DWavesys14\">{{cite web|title=D-Wave Systems Building Quantum Application Ecosystem, Announces Partnerships with DNA-SEQ Alliance and 1QBit|url=http://dwavesys.com/press-releases/d-wave-systems-building-quantum-application-ecosystem-announces-partnerships-dna-seq|website=D-Wave Systems|accessdate=22 June 2014}}</ref>  As the first company dedicated to producing software applications for commercially available quantum computers, 1QBit's research and development arm has focused on D-Wave's quantum annealing processors and has successfully demonstrated that these processors are suitable for solving real-world applications.<ref name=\"1qbit14\">{{cite web|title=1QBit Research|url=http://1qbit.com/research.html|website=1QBit|accessdate=22 June 2014}}</ref>\n\nWith demonstrations of entanglement published,<ref name=\"prx14\">{{cite journal |author=T. Lanting |display-authors=etal |title=Entanglement in a quantum annealing processor |journal=Physical Review X |date=2014-05-29 |publisher=Phys. Rev. X |volume=4 |issue=2 |doi=10.1103/PhysRevX.4.021041|arxiv=1401.3500 }}</ref> the question of whether or not the D-Wave machine can demonstrate quantum speedup over all classical computers remains unanswered. A study published in [[Science (magazine)|Science]] in June 2014, described as \"likely the most thorough and precise study that has been done on the performance of the D-Wave machine\"<ref name=\"Harv14\">Helmut Katzgraber, quoted in {{Harv|Cho|2014}}.</ref> and \"the fairest comparison yet\", attempted to define and measure quantum speedup. Several definitions were put forward as some may be unverifiable by empirical tests, while others, though falsified, would nonetheless allow for the existence of performance advantages. The study found that the D-Wave chip \"produced no quantum speedup\" and did not rule out the possibility in future tests.<ref name=\"Cho14\">{{Citation\n |first1= Adrian |last1= Cho\n |date=  20 June 2014\n |title= Quantum or not, controversial computer yields no speedup\n |journal= Science\n |volume= 344 |issue= 6190 |pages= 1330–1331\n |doi= 10.1126/science.344.6190.1330\n |pmid= 24948715\n |url= http://www.sciencemag.org/content/344/6190/1330.full\n}}.</ref> The researchers, led by Matthias Troyer at the Swiss Federal Institute of Technology, found \"no quantum speedup\" across the entire range of their tests, and only inconclusive results when looking at subsets of the tests. Their work illustrated \"the subtle nature of the quantum speedup question\". Further work<ref name=\"Amin15\">Mohammad H. Amin, \"Searching for quantum speedup in quasistatic quantum annealers\" {{arxiv|1503.04216}}</ref> has advanced understanding of these test metrics and their reliance on equilibrated systems, thereby missing any signatures of advantage due to quantum dynamics.\n\nThere are many open questions regarding quantum speedup. The ETH reference in the previous section is just for one class of benchmark problems. Potentially\nthere may be other classes of problems where quantum speedup might occur. Researchers at Google, LANL, USC, Texas\nA&M, and D-Wave are working hard to find such problem classes.<ref name=\"Steiger15\">{{Citation\n |first1= Damian |title= Electro-Optical and Infrared Systems: Technology and Applications XII; and Quantum Information Science and Technology\n |volume= 9648\n |pages= 964816\n |last1= Steiger\n |first2= Bettina |last2= Heim\n |first3= Troels |last3= Rønnow\n |first4= Matthias |last4= Troyer\n |date=  October 22, 2015\n |chapter= Performance of quantum annealing hardware \n |doi= 10.1117/12.2202661\n |pmid= \n}}</ref>\n\nIn December 2015, Google announced that the [[D-Wave 2X]] outperforms both [[simulated annealing]] and [[Quantum Monte Carlo]] by up to a factor of 100,000,000 on a set of hard optimization problems.<ref name=\"Google15\">{{Cite web|title = When can Quantum Annealing win?|url = http://googleresearch.blogspot.com/2015/12/when-can-quantum-annealing-win.html|website = Research Blog|access-date = 2016-01-21}}</ref>\n\nD-Wave's architecture differs from traditional quantum computers. It is not known to be polynomially equivalent to a [[universal quantum computer]] and, in particular, cannot execute [[Shor's algorithm]] because Shor's algorithm is not a hillclimbing process. Shor's algorithm requires a universal quantum computer. D-Wave claims only to do quantum annealing.{{Citation needed|date=May 2016|reason=Would be helpful to know why Shor's algorithm cannot run on a quantum annealing system.}}\n\n== References ==\n{{Reflist|30em}}\n\n== Further reading ==\n*S.E. Venegas-Andraca, W. Cruz-Santos, C. McGeoch, and M. Lanzagorta, \"A cross-disciplinary introduction to quantum annealing-based algorithms\". Contemporary Physics '''59'''(2), pp.&nbsp;174–196 (2018). {{doi|10.1080/00107514.2018.1450720}}. {{arxiv|1803.03372}}\n* G. E. Santoro and E. Tosatti, \"[http://iopscience.iop.org/0305-4470/39/36/R01 Optimization using quantum mechanics: quantum annealing through adiabatic evolution]\"  J. Phys. A '''39''', R393 (2006).\n* Arnab Das and B. K. Chakrabarti, \"[https://web.archive.org/web/20100918094255/http://rmp.aps.org/abstract/RMP/v80/i3/p1061_1 Colloquium: Quantum annealing and analog quantum computation]\"  Rev. Mod. Phys. '''80''', 1061 (2008).\n* S. Suzuki, J.-i. Inoue & B. K. Chakrabarti, ''[https://www.amazon.com/Quantum-Transitions-Transverse-Lecture-Physics/dp/364233038X/ref=la_B00BJ8LIAS_1_1?s=books&ie=UTF8&qid=1389576675&sr=1-1 Quantum Ising Phases & Transitions in Transverse Ising Models]'', Springer, Heidelberg (2013), Chapter 8 on Quantum Annealing.\n* V. Bapst, L. Foini, F. Krzakala, G. Semerjian and F. Zamponi, \"[http://www.sciencedirect.com/science/article/pii/S037015731200347X The quantum adiabatic algorithm applied to random optimization problems: The quantum spin glass perspective]\", Physics Reports '''523''' 127 (2013).\n* Arnab Das and [[Bikas K Chakrabarti]] (Eds.), ''[https://www.amazon.com/Quantum-Annealing-Related-Optimization-Methods/dp/3540279873/ref=sr_1_1?s=books&ie=UTF8&qid=1389577046&sr=1-1 Quantum Annealing and Related Optimization Methods]'', Lecture Note in Physics, '''679''', Springer, Heidelberg (2005).\n* Anjan K. Chandra, Arnab Das and [[Bikas K Chakrabarti]] (Eds.), ''[https://www.amazon.com/Quantum-Quenching-Annealing-Computation-Lecture/dp/3642114695/ref=sr_1_1?s=books&ie=UTF8&qid=1389576962&sr=1-1 Quantum Quenching, Annealing and Computation]'', Lecture Note in Physics, '''802''', Springer, Heidelberg (2010).\n* A. Ghosh and S. Mukherjee,  \"Quantum Annealing and Computation: A Brief Documentary Note\",  {{arxiv|1310.1339}}.\n* A. Dutta, G. Aeppli, B. K. Chakrabarti, U. Divakaran, T.F. Rosenbaum & D. Sen, ''[http://www.cambridge.org/us/academic/subjects/physics/quantum-physics-quantum-information-and-quantum-computation/quantum-phase-transitions-transverse-field-spin-models-statistical-physics-quantum-information \"Quantum Phase Transitions in Transverse Field Spin Models: From Statistical Physics to Quantum Information]'', Cambridge University Press, Cambridge & Delhi (2015).\n* S. Tanaka, R. Tamura & B. K. Chakrabarti, ''[http://www.cambridge.org/us/academic/subjects/physics/condensed-matter-physics-nanoscience-and-mesoscopic-physics/quantum-spin-glasses-annealing-and-computation?format=HB#EsrdfXLEbPQs0trI.97 Quantum Spin Glasses, Annealing & Computation]'', Cambridge University Press, Cambridge & Delhi (2017).\n* Indian Science News Association, ''[http://www.scienceandculture-isna.org/latestissue.htm Special Issue of \"Science & Culture\" on 'A Quantum Jump in Computation']'', Vol. 85 (5-6), May-June (2019)\n\n{{Optimization algorithms}}\n{{Quantum information}}\n\n[[Category:Stochastic optimization]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Quantum algorithms]]"
    },
    {
      "title": "Quasi-Newton inverse least squares method",
      "url": "https://en.wikipedia.org/wiki/Quasi-Newton_inverse_least_squares_method",
      "text": "In numerical analysis, the '''quasi-Newton inverse least squares method''' is a [[quasi-Newton method]] for [[root-finding algorithm|finding roots]] of functions of several variables. It was originally described by Degroote et al. in 2009.<ref>{{cite journal |author1=J. Degroote |author2=R. Haelterman |author3=S. Annerel |author4=A. Swillens |author5=P. Segers |author6=J. Vierendeels | title= An interface quasi-Newton algorithm for partitioned simulation of fluid-structure interaction| journal=Proceedings of the International Workshop on Fluid–Structure Interaction. Theory, Numerics and Applications. S. Hartmann, A. Meister, M. Schfer, S. Turek (Eds.), Kassel University Press, Germany | year=2008}}</ref>\n\n[[Newton's method]] for solving {{math|'''f'''('''x''') {{=}} '''0'''}} uses the [[Jacobian matrix]], {{math|'''J'''}}, at every iteration. However, computing this Jacobian is a difficult (sometimes even impossible) and expensive operation. The idea behind the quasi-Newton inverse least squares method is to build up an approximate Jacobian based on known input–output pairs of the function {{math|'''f'''}}.\n\nHaelterman et al. also showed that when the quasi-Newton inverse least squares method is applied to a linear system of size {{math|''n''&nbsp;×&nbsp;''n''}}, it converges in at most {{math|''n''&nbsp;+&nbsp;1}} steps, although like all quasi-Newton methods, it may not converge for nonlinear systems.<ref>{{cite journal | doi=10.1016/j.cam.2013.08.020|author1=R. Haelterman |author2=J. Petit |author3=B. Lauwens |author4=H. Bruyninckx |author5=J. Vierendeels | title=  On the Non-Singularity of the Quasi-Newton-Least Squares Method| journal=Journal of Computational and Applied Mathematics| volume=257 | year=2014 | pages=129–131}}</ref>\n\nThe method is closely related to the [[quasi-Newton least squares method]].\n\n==References==\n{{reflist}}\n\n{{applied-math-stub}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Least squares]]"
    },
    {
      "title": "Quasi-Newton least squares method",
      "url": "https://en.wikipedia.org/wiki/Quasi-Newton_least_squares_method",
      "text": "{{Technical|date=May 2015}}\nIn numerical analysis, '''the quasi-Newton least squares method''' is a [[quasi-Newton method]] for [[root-finding algorithm|finding roots]] in {{math|''n''}} variables. It was originally described by Rob Haelterman et al. in 2009.<ref>{{cite journal | doi=10.1137/070710469|author1=R. Haelterman |author2=J. Degroote |author3=D. Van Heule |author4=J. Vierendeels | title= The quasi-Newton Least Squares method: a new and fast secant method analyzed for linear systems | journal=SIAM J. Numer. Anal.| volume=47 | issue=3 | year=2009 | pages=2347–2368}}</ref>\n\n[[Newton's method]] for solving {{math|'''f'''('''x''') {{=}} '''0'''}} uses the [[Jacobian matrix]], {{math|'''J'''}}, at every iteration. However, computing this Jacobian is a difficult (sometimes even impossible) and expensive operation. The idea behind the quasi-Newton least squares method is to build up an approximate Jacobian based on known input–output pairs of the function {{math|'''f'''}}.\n\nHaelterman et al. also showed that when the quasi-Newton least squares method is applied to a linear system of size {{math|''n'' × ''n''}}, it converges in at most {{math|''n'' + 1}} steps, although like all quasi-Newton methods, it may not converge for nonlinear systems.\n\nThe method is closely related to the [[quasi-Newton inverse least squares method]].\n\n==References==\n{{reflist}}\n\n{{applied-math-stub}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Least squares]]"
    },
    {
      "title": "Quasi-Newton method",
      "url": "https://en.wikipedia.org/wiki/Quasi-Newton_method",
      "text": "'''Quasi-Newton methods''' are methods used to either find zeroes or local maxima and minima of functions, as an alternative to Newton's method. They can be used if the [[Jacobian matrix|Jacobian]] or [[Hessian matrix|Hessian]] is unavailable or is too expensive to compute at every iteration. The \"full\" [[Newton's method]] requires the Jacobian in order to search for zeros, or the Hessian for finding extrema.\n\n==Search for zeros: root finding==\n\n[[Newton's method]] to find zeroes of a function <math>g</math> of multiple variables is given by <math>x_{n+1} = x_n -[J_g(x_n)]^{-1} g(x_n)</math>, where <math>[J_g(x_n)]^{-1}</math> is the [[Inverse element#Matrices|left inverse]] of the [[Jacobian matrix]] <math>J_g(x_n)</math> of <math>g</math> evaluated for <math>x_n</math>.\n\nStrictly speaking, any method that replaces the exact Jacobian <math>J_g(x_n)</math> with an approximation is a quasi-Newton method. For instance, the chord method (where <math>J_g(x_n)</math> is replaced by <math>J_g(x_0)</math> for all iterations) is a simple example. The methods given below for [[#Search for extrema|optimization]] refer to an important subclass of quasi-Newton methods, secant methods.<ref name=\"Haelterman\">{{ cite web | last = Haelterman | first = Rob | title = Analytical study of the least squares quasi-Newton method for interaction problems | date = 2009 | work = PhD Thesis, Ghent University | url = https://biblio.ugent.be/input/download?func=downloadFile&recordOId=720660&fileOId=1886177 | accessdate = 2014-08-14 }}</ref>\n\nUsing methods developed to find extrema in order to find zeroes is not always a good idea, as the majority of the methods used to find extrema require that the matrix that is used is symmetrical. While this holds in the context of the search for extrema, it rarely holds when searching for zeroes. [[Broyden's method|Broyden's \"good\" and \"bad\" methods]] are two methods commonly used to find extrema that can also be applied to find zeroes. Other methods that can be used are the [[column-updating method]], the [[inverse column-updating method]], the [[quasi-Newton least squares method]] and the [[quasi-Newton inverse least squares method]].\n\nMore recently quasi-Newton methods have been applied to find the solution of multiple coupled systems of equations (e.g. fluid–structure interaction problems or interaction problems in physics). They allow the solution to be found by solving each constituent system separately (which is simpler than the global system) in a cyclic, iterative fashion until the solution of the global system is found.<ref name=\"Haelterman\"/><ref>{{cite journal |doi=10.1016/j.cam.2014.11.005 |authors=Rob Haelterman, Dirk Van Eester, Daan Verleyen |title=Accelerating the solution of a physics model inside a tokamak using the (Inverse) Column Updating Method |journal=Journal of Computational and Applied Mathematics |volume=279 |year=2015 |pages=133–144}}</ref>\n\n==Search for extrema: optimization==\nNoting that the search for a minimum or maximum of a scalar-valued function is nothing else than the search for the zeroes of the [[gradient]] of that function, quasi-Newton methods can be readily applied to find extrema of a function. In other words, if <math>g</math> is the gradient of <math>f</math>, then searching for the zeroes of the vector-valued function <math>g</math> corresponds to the search for the extrema of the scalar-valued function <math>f</math>; the Jacobian of <math>g</math> now becomes the Hessian of <math>f</math>. The main difference is that [[Hessian matrix#Mixed derivatives and symmetry of the Hessian|the Hessian matrix is a symmetric matrix]], unlike the Jacobian when [[#Search for zeroes|searching for zeroes]]. Most quasi-Newton methods used in optimization exploit this property.\n\nIn [[Optimization (mathematics)|optimization]], '''quasi-Newton methods''' (a special case of '''variable-metric methods''') are algorithms for finding local [[maxima and minima]] of [[function (mathematics)|functions]]. Quasi-Newton methods are based on [[Newton's method in optimization|Newton's method]] to find the [[stationary point]] of a function, where the gradient is 0. Newton's method assumes that the function can be locally approximated as a [[quadratic function|quadratic]] in the region around the optimum, and uses the first and second derivatives to find the stationary point. In higher dimensions, Newton's method uses the gradient and the [[Hessian matrix]] of second [[derivative]]s of the function to be minimized.\n \nIn quasi-Newton methods the Hessian matrix does not need to be computed. The Hessian is updated by analyzing successive gradient vectors instead. Quasi-Newton methods are a generalization of the [[secant method]] to find the root of the first derivative for multidimensional problems. In multiple dimensions the secant equation is [[Underdetermined system|under-determined]], and quasi-Newton methods differ in how they constrain the solution, typically by adding a simple low-rank update to the current estimate of the Hessian.\n\nThe first quasi-Newton algorithm was proposed by [[William C. Davidon]], a physicist working at [[Argonne National Laboratory]]. He developed the first quasi-Newton algorithm in 1959: the [[DFP updating formula]], which was later popularized by Fletcher and Powell in 1963, but is rarely used today. The most common quasi-Newton algorithms are currently the [[SR1 formula]] (for \"symmetric rank-one\"), the [[BHHH]] method, the widespread [[BFGS method]] (suggested independently by Broyden, Fletcher, Goldfarb, and Shanno, in 1970), and its low-memory extension [[L-BFGS]]. The Broyden's class is a linear combination of the DFP and BFGS methods.\n\nThe SR1 formula does not guarantee the update matrix to maintain [[Positive-definite matrix|positive-definiteness]] and can be used for indefinite problems. The [[Broyden's method]] does not require the update matrix to be symmetric and is used to find the root of a general system of equations (rather than the gradient) by updating the [[Jacobian matrix and determinant|Jacobian]] (rather than the Hessian).\n\nOne of the chief advantages of quasi-Newton methods over [[Newton's method in optimization|Newton's method]] is that the [[Hessian matrix]] (or, in the case of quasi-Newton methods, its approximation) <math>B</math> does not need to be inverted. Newton's method, and its derivatives such as [[interior point method]]s, require the Hessian to be inverted, which is typically implemented by solving a [[system of linear equations]] and is often quite costly. In contrast, quasi-Newton methods usually generate an estimate of <math>B^{-1}</math> directly.\n\nAs in [[Newton's method in optimization|Newton's method]], one uses a second-order approximation to find the minimum of a function <math>f(x)</math>. The [[Taylor series]] of <math>f(x)</math> around an iterate is\n:<math>f(x_k + \\Delta x) \\approx f(x_k) + \\nabla f(x_k)^{\\mathrm T} \\,\\Delta x + \\frac{1}{2} \\Delta x^{\\mathrm T} B \\,\\Delta x,</math>\nwhere (<math>\\nabla f</math>) is the [[gradient]], and <math>B</math> an approximation to the [[Hessian matrix]]. The gradient of this approximation (with respect to <math>\\Delta x</math>) is\n:<math>\\nabla f(x_k + \\Delta x) \\approx \\nabla f(x_k) + B \\,\\Delta x,</math>\nand setting this gradient to zero (which is the goal of optimisation) provides the Newton step:\n:<math>\\Delta x = -B^{-1} \\nabla f(x_k).</math>\n\nThe Hessian approximation <math>B</math> is chosen to satisfy\n:<math>\\nabla f(x_k + \\Delta x) = \\nabla f(x_k) + B \\,\\Delta x,</math>\nwhich is called the ''secant equation'' (the Taylor series of the gradient itself). In more than one dimension <math>B</math> is [[Underdetermined system|underdetermined]]. In one dimension, solving for <math>B</math> and applying the Newton's step with the updated value is equivalent to the [[secant method]]. The various quasi-Newton methods differ in their choice of the solution to the secant equation (in one dimension, all the variants are equivalent). Most methods (but with exceptions, such as [[Broyden's method]]) seek a symmetric solution (<math>B^T = B</math>); furthermore, the variants listed below can be motivated by finding an update <math>B_{k+1}</math> that is as close as possible to <math> B_{k}</math> in some [[Norm (mathematics)|norm]]; that is, <math>B_{k+1} = \\operatorname{argmin}_B \\|B - B_k\\|_V</math>, where <math>V </math> is some [[positive-definite matrix]] that defines the norm. An approximate initial value <math>B_0 = \\beta I </math> is often sufficient to achieve rapid convergence, although there is no general strategy to choose <math> \\beta </math> <ref> {{cite book |last=Nocedal |first=Jorge |last2=Wright |first2=Stephen J. |year=2006 |pages=142 |title=Numerical Optimization |location=New York |publisher=Springer |isbn=0-387-98793-2 }} </ref>. Note that <math>B_0</math> should be positive-definite. The unknown <math>x_k</math> is updated applying the Newton's step calculated using the current approximate Hessian matrix <math>B_{k}</math>:\n* <math>\\Delta x_k = -\\alpha_k B_k^{-1} \\nabla f(x_k)</math>, with <math>\\alpha</math> chosen to satisfy the [[Wolfe conditions]];\n* <math>x_{k+1} = x_k + \\Delta x_k</math>;\n* The gradient computed at the new point <math>\\nabla f(x_{k+1})</math>, and\n::<math>y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)</math>\nis used to update the approximate Hessian <math>B_{k+1}</math>, or directly its inverse <math>H_{k+1} = B_{k+1}^{-1}</math> using the [[Sherman–Morrison formula]]. \n* A key property of the BFGS and DFP updates is that if <math>B_k</math> is positive-definite, and <math>\\alpha_k</math> is chosen to satisfy the Wolfe conditions, then <math>B_{k+1}</math> is also positive-definite.\n\nThe most popular update formulas are:\n:{| class=\"wikitable\"\n|-\n! Method\n! <math>\\displaystyle B_{k+1}=</math>\n! <math>H_{k+1}=B_{k+1}^{-1}=</math>\n|-\n|[[BFGS method|BFGS]]\n|<math>B_k + \\frac{y_k y_k^{\\mathrm T}}{y_k^{\\mathrm T} \\Delta x_k} - \\frac{B_k \\Delta x_k (B_k \\Delta x_k)^{\\mathrm T}}{\\Delta x_k^{\\mathrm T} B_k \\, \\Delta x_k}</math>\n|<math>\\left(I - \\frac{\\Delta x_k y_k^{\\mathrm T}}{y_k^{\\mathrm T} \\Delta x_k}\\right) H_k \\left(I - \\frac{y_k \\Delta x_k^{\\mathrm T}}{y_k^{\\mathrm T} \\Delta x_k}\\right) + \\frac{\\Delta x_k \\Delta x_k^{\\mathrm T}}{y_k^{\\mathrm T} \\, \\Delta x_k}</math>\n|-\n|[[Broyden's method|Broyden]]\n|<math>B_k + \\frac{y_k - B_k \\Delta x_k}{\\Delta x_k^{\\mathrm T} \\, \\Delta x_k} \\, \\Delta x_k^{\\mathrm T}</math>\n|<math>H_k + \\frac{(\\Delta x_k - H_k y_k) \\Delta x_k^{\\mathrm T} H_k}{\\Delta x_k^{\\mathrm T} H_k \\, y_k}</math>\n|-\n| Broyden family\n|<math>(1 - \\varphi_k) B_{k+1}^\\text{BFGS} + \\varphi_k B_{k+1}^\\text{DFP}, \\quad \\varphi \\in [0, 1]</math>\n|\n|-\n| [[DFP updating formula|DFP]]\n| <math>\\left(I - \\frac{y_k \\, \\Delta x_k^{\\mathrm T}}{y_k^{\\mathrm T} \\, \\Delta x_k}\\right) B_k \\left(I - \\frac{\\Delta x_k y_k^{\\mathrm T}}{y_k^{\\mathrm T} \\, \\Delta x_k}\\right) + \\frac{y_k y_k^{\\mathrm T}}{y_k^{\\mathrm T} \\, \\Delta x_k}</math>\n| <math>H_k + \\frac{\\Delta x_k \\Delta x_k^{\\mathrm T}}{\\Delta x_k^{\\mathrm T} \\, y_k} - \\frac{H_k y_k y_k^{\\mathrm T} H_k}{y_k^{\\mathrm T} H_k y_k}</math>\n|-\n| [[SR1 formula|SR1]]\n| <math>B_k + \\frac{(y_k - B_k \\, \\Delta x_k) (y_k - B_k \\, \\Delta x_k)^{\\mathrm T}}{(y_k - B_k \\, \\Delta x_k)^{\\mathrm T} \\, \\Delta x_k}</math>\n| <math>H_k + \\frac{(\\Delta x_k - H_k y_k) (\\Delta x_k - H_k y_k)^{\\mathrm T}}{(\\Delta x_k - H_k y_k)^{\\mathrm T} y_k}</math>\n|}\n\nOther methods are Pearson's method, McCormick's method, the Powell symmetric Broyden (PSB) method and Greenstadt's method.<ref name=\"Haelterman\" />\n\n==Relationship to matrix inversion==\n\nWhen <math>f </math> is a convex quadratic function with positive-definite Hessian <math>B</math>, one would expect the matrices <math>H_k</math> generated by a quasi-Newton method to converge to the inverse Hessian <math>H = B^{-1}</math>. This is indeed the case for the class of quasi-Newton methods based on least-change updates.<ref name=\"Gower and Richtarik\">{{ cite arxiv | author1 = Robert Mansel Gower |author2= Peter Richtarik | title = Randomized Quasi-Newton Updates are Linearly Convergent Matrix Inversion Algorithms | date = 2015 |eprint=1602.01768| class = math.NA }}</ref>\n\n==Notable implementations==\n\nImplementations of quasi-Newton methods are available in many programming languages. Notable implementations include:\n\n*[[GNU Octave]] uses a form of BFGS in its <code>fsolve</code> function, with [[trust region]] extensions.\n*[[Mathematica]] includes quasi-Newton solvers.<ref>http://reference.wolfram.com/mathematica/tutorial/UnconstrainedOptimizationQuasiNewtonMethods.html</ref>\n* The [[NAG Numerical Library|NAG Library]] contains several routines<ref>{{cite web | last = The Numerical Algorithms Group | first = | title = Keyword Index: Quasi-Newton | date = | work = NAG Library Manual, Mark 23 | url = http://www.nag.co.uk/numeric/fl/nagdoc_fl23/html/INDEXES/KWIC/quasi-newton.html | accessdate = 2012-02-09 }}</ref> for minimizing or maximizing a function<ref>{{cite web | last = The Numerical Algorithms Group | first = | title = E04 – Minimizing or Maximizing a Function | date = | work = NAG Library Manual, Mark 23 | url = http://www.nag.co.uk/numeric/fl/nagdoc_fl23/pdf/E04/e04intro.pdf | accessdate = 2012-02-09 }}</ref> which use quasi-Newton algorithms.\n* In MATLAB's [[Optimization Toolbox]], the <code>fminunc</code> function uses (among other methods) the [[BFGS]] quasi-Newton method.<ref>http://www.mathworks.com/help/toolbox/optim/ug/fminunc.html</ref> Many of the constrained methods of the Optimization toolbox use [[BFGS]] and the variant [[L-BFGS]].<ref>http://www.mathworks.com/help/toolbox/optim/ug/brnoxzl.html</ref>\n*[[R (programming language)|R]]'s <code>optim</code> general-purpose optimizer routine uses the [[BFGS]] method by using <code>method=\"BFGS\"</code>.<ref>[http://finzi.psych.upenn.edu/R/library/stats/html/optim.html]</ref>\n*[[Scipy]].optimize has fmin_bfgs. In the [[SciPy]] extension to [[Python (programming language)|Python]], the <code>scipy.optimize.minimize</code> function includes, among other methods, a [[BFGS]] implementation.<ref>http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</ref>\n\n==See also==\n*[[BFGS method]]\n**[[Limited-memory BFGS|L-BFGS]]\n**[[Orthant-wise limited-memory quasi-Newton|OWL-QN]]\n*[[Broyden's method]]\n*[[DFP updating formula]]\n*[[Newton's method]]\n*[[Newton's method in optimization]]\n*[[Quasi-Newton least squares method]]\n*[[SR1 formula]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n*Bonnans, J. F., Gilbert, J. Ch., [[Claude Lemaréchal|Lemaréchal, C.]] and [[Claudia Sagastizábal|Sagastizábal, C. A.]] (2006), ''Numerical optimization, theoretical and numerical aspects''. Second edition. Springer. {{isbn|978-3-540-35445-1}}.\n*{{Citation | last1=Fletcher | first1=Roger | title=Practical methods of optimization | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0-471-91547-8 | year=1987}}. \n*{{cite book |last=Nocedal |first=Jorge |last2=Wright |first2=Stephen J. |year=1999 |chapter=Quasi-Newton Methods |title=Numerical Optimization |location=New York |publisher=Springer |isbn=0-387-98793-2 |pages=192–221 |chapterurl=https://books.google.com/books?id=7wDpBwAAQBAJ&pg=PA192 }}\n*{{Cite book | last1=Press | first1=W. H. | last2=Teukolsky | first2=S. A. | last3=Vetterling | first3=W. T. | last4=Flannery | first4=B. P. | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press | publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 10.9. Quasi-Newton or Variable Metric Methods in Multidimensions | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=521 }}\n*{{cite book |last=Scales |first=L. E. |title=Introduction to Non-Linear Optimization |location=New York |publisher=MacMillan |year=1985 |isbn=0-333-32552-4 |pages=84–106 |url=https://books.google.com/books?id=AEJdDwAAQBAJ&pg=PA84 }}\n<!-- * Edwin K. P. Chong and Stanislaw H. Zak, An Introduction to Optimization 2ed, John Wiley & Sons Pte. Ltd. August 2001. -->\n\n{{Optimization algorithms|unconstrained}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Random optimization",
      "url": "https://en.wikipedia.org/wiki/Random_optimization",
      "text": "'''Random optimization (RO)''' is a family of numerical [[Optimization (mathematics)|optimization]] methods that do not require the [[gradient]] of the problem to be optimized and RO can hence be used on functions that are not [[Continuous function|continuous]] or [[Differentiable function|differentiable]]. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.\n\nThe name random optimization is attributed to Matyas <ref name=matyas65random/> who made an early presentation of RO along with basic mathematical analysis. RO works by iteratively moving to better positions in the search-space which are sampled using e.g. a [[normal distribution]] surrounding the current position.\n\n== Algorithm ==\n\nLet ''f'':&nbsp;ℝ<sup>''n''</sup>&nbsp;→ ℝ be the fitness or cost function which must be minimized. Let '''x'''&nbsp;∈ ℝ<sup>''n''</sup> designate a position or candidate solution in the search-space. The basic RO algorithm can then be described as:\n\n* Initialize '''x''' with a random position in the search-space.\n* Until a termination criterion is met (e.g. number of iterations performed, or adequate fitness reached), repeat the following:\n** Sample a new position '''y''' by adding a [[normal distribution|normally distributed]] random vector to the current position '''x'''\n** If (''f''('''y''')&nbsp;<&nbsp;''f''('''x''')) then move to the new position by setting '''x'''&nbsp;=&nbsp;'''y'''\n* Now '''x''' holds the best-found position.\n\nThis algorithm corresponds to a (1+1) [[evolution strategy]] with constant step-size.\n\n== Convergence and variants ==\nMatyas showed the basic form of RO converges to the optimum of a simple [[unimodal function]] by using a [[Limit (mathematics)|limit-proof]] which shows convergence to the optimum is certain to occur if a potentially infinite number of iterations are performed. However, this proof is not useful in practice because a finite number of iterations can only be executed. In fact, such a theoretical limit-proof will also show that purely random sampling of the search-space will inevitably yield a sample arbitrarily close to the optimum.\n\nMathematical analyses are also conducted by Baba <ref name=baba81convergence/> and Solis and Wets <ref name=solis81random/> to establish that convergence to a region surrounding the optimum is inevitable under some mild conditions for RO variants using other [[probability distribution]]s for the sampling. An estimate on the number of iterations required to approach the optimum is derived by Dorea.<ref name=dorea83expected/> These analyses are criticized through empirical experiments by Sarma <ref name=sarma90convergence/> who used the optimizer variants of Baba and Dorea on two real-world problems, showing the optimum to be approached very slowly and moreover that the methods were actually unable to locate a solution of adequate fitness, unless the process was started sufficiently close to the optimum to begin with.\n\n== See also ==\n* [[Random search]] is a closely related family of optimization methods which sample from a [[hypersphere]] instead of a normal distribution.\n* [[Luus–Jaakola]] is a closely related optimization method using a [[Uniform distribution (continuous)|uniform distribution]] in its sampling and a simple formula for exponentially decreasing the sampling range.\n* [[Pattern search (optimization)|Pattern search]] takes steps along the axes of the search-space using exponentially decreasing step sizes.\n* [[Stochastic optimization]]\n\n== References ==\n{{reflist|refs=\n<ref name=matyas65random>\n{{cite journal\n|last=Matyas\n|first=J.\n|title=Random optimization\n|journal=Automation and Remote Control\n|year=1965\n|volume=26\n|number=2\n|pages=246–253\n|url=http://www.mathnet.ru/eng/at11288\n}}\n</ref>\n\n<ref name=baba81convergence>\n{{cite journal\n|last=Baba\n|first=N.\n|title=Convergence of a random optimization method for constrained optimization problems\n|journal=Journal of Optimization Theory and Applications\n|year=1981\n|volume=33\n|number=4\n|pages=451–461\n|doi=10.1007/bf00935752\n}}\n</ref>\n\n<ref name=solis81random>\n{{cite journal\n|last1=Solis\n|first1=F.J.\n|last2=Wets\n|first2=R.J-B.\n|title=Minimization by random search techniques\n|journal=Mathematics of Operations Research\n|year=1981\n|volume=6\n|number=1\n|pages=19–30\n|doi=10.1287/moor.6.1.19\n}}\n</ref>\n\n<ref name=dorea83expected>\n{{cite journal\n|last1=Dorea\n|first1=C.C.Y.\n|title=Expected number of steps of a random optimization method\n|journal=Journal of Optimization Theory and Applications\n|year=1983\n|volume=39\n|number=3\n|pages=165–171\n|doi=10.1007/bf00934526\n}}\n</ref>\n\n<ref name=sarma90convergence>\n{{cite journal\n|last1=Sarma\n|first1=M.S.\n|title=On the convergence of the Baba and Dorea random optimization methods\n|journal=Journal of Optimization Theory and Applications\n|year=1990\n|volume=66\n|number=2\n|pages=337–343\n|doi=10.1007/bf00939542\n}}\n</ref>\n}}\n\n{{Major subfields of optimization}}\n\n{{DEFAULTSORT:Random Optimization}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Random search",
      "url": "https://en.wikipedia.org/wiki/Random_search",
      "text": "'''Random search (RS)''' is a family of numerical [[Optimization (mathematics)|optimization]] methods that [[Derivative-free optimization|do not require the gradient]] of the problem to be optimized, and RS can hence be used on functions that are not [[Continuous function|continuous]] or [[Differentiable function|differentiable]]. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.\n\nThe name \"random search\" is attributed to Rastrigin<ref name=rastrigin63convergence/> who made an early presentation of RS along with basic mathematical analysis. RS works by iteratively moving to better positions in the search-space, which are sampled from a [[hypersphere]] surrounding the current position.\n\n== Algorithm ==\n\nLet {{math|''f'':&nbsp;&#x211D;<sup>''n''</sup>&nbsp;→ &#x211D;}} be the fitness or cost function which must be minimized. Let {{math|'''x'''&nbsp;∈ &#x211D;<sup>''n''</sup>}} designate a position or candidate solution in the search-space. The basic RS algorithm can then be described as:\n\n# Initialize '''x''' with a random position in the search-space.\n# Until a termination criterion is met (e.g. number of iterations performed, or adequate fitness reached), repeat the following:\n## Sample a new position '''y''' from the [[hypersphere]] of a given radius surrounding the current position '''x''' (see e.g. [[N-sphere#Generating_random_points|Marsaglia's technique]] for sampling a hypersphere.)\n## If {{math|''f''('''y''')&nbsp;<&nbsp;''f''('''x''')}} then move to the new position by setting {{math|1='''x'''&nbsp;=&nbsp;'''y'''}}\n#\n\n== Variants ==\nA number of RS variants have been introduced in the literature:\n\n* Fixed Step Size Random Search (FSSRS) is Rastrigin's <ref name=rastrigin63convergence/> basic algorithm which samples from a hypersphere of fixed radius.\n* Optimum Step Size Random Search (OSSRS) by Schumer and Steiglitz <ref name=schumer68adaptive/> is primarily a theoretical study on how to optimally adjust the radius of the hypersphere so as to allow for speedy convergence to the optimum. An actual implementation of the OSSRS needs to approximate this optimal radius by repeated sampling and is therefore expensive to execute.\n* Adaptive Step Size Random Search (ASSRS) by Schumer and Steiglitz <ref name=schumer68adaptive/> attempts to heuristically adapt the hypersphere's radius: two new candidate solutions are generated, one with the current nominal step size and one with a larger step-size. The larger step size becomes the new nominal step size if and only if it leads to a larger improvement. If for several iterations neither of the steps leads to an improvement, the nominal step size is reduced.\n* Optimized Relative Step Size Random Search (ORSSRS) by Schrack and Choit <ref name=schrack76optimized/> approximate the optimal step size by a simple exponential decrease. However, the formula for computing the decrease-factor is somewhat complicated.\n\n== See also ==\n* [[Random optimization]] is a closely related family of optimization methods which sample from a [[normal distribution]] instead of a hypersphere.\n* [[Luus–Jaakola]] is a closely related optimization method using a [[Uniform_distribution_(continuous)|uniform distribution]] in its sampling and a simple formula for exponentially decreasing the sampling range.\n* [[Pattern search (optimization)|Pattern search]] takes steps along the axes of the search-space using exponentially decreasing step sizes.\n\n== References ==\n{{reflist|refs=\n<ref name=rastrigin63convergence>\n{{cite journal\n|last=Rastrigin\n|first=L.A.\n|title=The convergence of the random search method in the extremal control of a many parameter system\n|journal=Automation and Remote Control\n|year=1963\n|volume=24\n|number=10\n|pages=1337–1342\n}}\n</ref>\n\n<ref name=schumer68adaptive>\n{{cite journal\n|last1=Schumer\n|first1=M.A.\n|last2=Steiglitz\n|first2=K.\n|title=Adaptive step size random search\n|journal=IEEE Transactions on Automatic Control\n|year=1968\n|volume=13\n|number=3\n|pages=270–276\n|doi=10.1109/tac.1968.1098903\n|citeseerx=10.1.1.118.9779\n}}\n</ref>\n\n<ref name=schrack76optimized>\n{{cite journal\n|last1=Schrack\n|first1=G.\n|last2=Choit\n|first2=M.\n|title=Optimized relative step size random searches\n|journal=Mathematical Programming\n|year=1976\n|volume=10\n|number=1\n|pages=230–244\n|doi=10.1007/bf01580669\n}}\n</ref>\n}}\n\n{{Major subfields of optimization}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Stochastic optimization]]\n[[Category:Metaheuristics]]"
    },
    {
      "title": "Robust fuzzy programming",
      "url": "https://en.wikipedia.org/wiki/Robust_fuzzy_programming",
      "text": "{{Orphan|date=June 2016}}\n\n'''Robust fuzzy programming (ROFP)''' is a powerful [[mathematical optimization]] approach to deal with optimization problems under [[uncertainty]]. This approach is firstly introduced at 2012 by Pishvaee, Razmi & Torabi<ref name=\":0\">{{Cite journal|title = Robust possibilistic programming for socially responsible supply chain network design: A new approach|url = http://www.sciencedirect.com/science/article/pii/S0165011412001819|journal = Fuzzy Sets and Systems|date = 2012-11-01|pages = 1–20|volume = 206|series = Theme : Operational Research|doi = 10.1016/j.fss.2012.04.010|first = M. S.|last = Pishvaee|first2 = J.|last2 = Razmi|first3 = S. A.|last3 = Torabi}}</ref> in the Journal of Fuzzy Sets and Systems. ROFP enables the decision makers to be benefited from the capabilities of both [[fuzzy set|fuzzy]] mathematical programming and [[robust optimization]] approaches. At 2016 Pishvaee and Fazli<ref name=\":1\">{{Cite journal|title = Novel robust fuzzy mathematical programming methods|url = http://www.sciencedirect.com/science/article/pii/S0307904X15003686|journal = Applied Mathematical Modelling|date = 2016-01-01|pages = 407–418|volume = 40|issue = 1|doi = 10.1016/j.apm.2015.04.054|first = Mir Saman|last = Pishvaee|first2 = Mohamadreza|last2 = Fazli Khalaf}}</ref> put a significant step forward by extending the ROFP approach to handle flexibility of constraints and goals.ROFP is able to achieve a ''robust solution'' for an optimization problem under uncertainty.\n\n== Definition of robust solution ==\nRobust solution is defined as a solution which has \"both ''feasibility robustness'' and ''optimality robustness''; Feasibility robustness means that the solution should remain feasible for (almost) all possible values of uncertain parameters and flexibility degrees of constraints and optimality robustness means that the value of objective function for the solution should remain close to optimal value or have minimum (undesirable) deviation from the optimal value for (almost) all possible values of uncertain parameters and flexibility degrees on target value of goals\".<ref name=\":1\" />\n\n== Classification of ROFP methods ==\nAs fuzzy mathematical programming is categorized into ''Possibilistic programming'' and ''Flexible programming'', ROFP also can be classified into:<ref name=\":1\" />\n\n# Robust possibilistic programming (RPP)\n# Robust flexible programming (RFP)  \n# Mixed possibilistic-flexible robust programming (MPFRP)\n\nThe first category is used to deal with imprecise input parameters in optimization problems while the second one is employed to cope with flexible constraints and goals. Also, the last category is capable to handle both uncertain parameters and flexibility in goals and constraints.\n\nFrom another point of view, it can be said that different ROFP models developed in the literature can be classified in three categories according to degree of conservatism against uncertainty. These categories include:<ref name=\":0\" />\n\n# Hard worst case ROFP\n# Soft worst case ROFP \n# Realistic ROFP\n\nHard worst case ROFP has the most conservative nature among ROFP methods since it provides maximum safety or immunity against uncertainty. Ignoring the chance of infeasibility, this method immunizes the solution for being infeasible for all possible values of uncertain parameters. Regarding the optimality robustness, this method minimizes the worst possible value of objective function (min-max logic). On the other hand, Soft worst case ROFP method behaves similar to hard worst case method regarding optimality robustness, however does not satisfy the constraints in their extreme worst case. Lastly, realistic method establishes a reasonable trade-off between the robustness, the cost of robustness and other objectives such as improving the average system performance (cost-benefit logic).\n\n== Applications ==\nROFP is successfully implemented in different practical application areas such as the following ones.\n\n* [[Supply chain management]] such as the work by Pishvaee et al.<ref name=\":0\" /> which addresses the design of a social responsible supply chain network under epistemic uncertainty.\n* Healthcare management such as the works by Zahiri et al.<ref>{{Cite journal|title = A robust possibilistic programming approach to multi-period location–allocation of organ transplant centers under uncertainty|url = http://www.sciencedirect.com/science/article/pii/S0360835214001533|journal = Computers & Industrial Engineering|date = 2014-08-01|pages = 139–148|volume = 74|doi = 10.1016/j.cie.2014.05.008|first = Behzad|last = Zahiri|first2 = Reza|last2 = Tavakkoli-Moghaddam|first3 = Mir Saman|last3 = Pishvaee}}</ref> and Mousazadeh et al.<ref>{{Cite journal|title = A robust possibilistic programming approach for pharmaceutical supply chain network design|url = http://www.sciencedirect.com/science/article/pii/S0098135415002203|journal = Computers & Chemical Engineering|date = 2015-11-02|pages = 115–128|volume = 82|doi = 10.1016/j.compchemeng.2015.06.008|first = M.|last = Mousazadeh|first2 = S. A.|last2 = Torabi|first3 = B.|last3 = Zahiri}}</ref> which consider the planning of an organ transplantation network and a pharmaceutical supply chain, respectively. \n* [[Energy planning]] such as Bairamzadeh et al.<ref>{{Cite journal|title = Multiobjective Robust Possibilistic Programming Approach to Sustainable Bioethanol Supply Chain Design under Multiple Uncertainties|journal = Industrial & Engineering Chemistry Research|date = 2015-12-22|pages = 237–256|volume = 55|issue = 1|doi = 10.1021/acs.iecr.5b02875|language = EN|first = Samira|last = Bairamzadeh|first2 = Mir Saman|last2 = Pishvaee|first3 = Mohammad|last3 = Saidi-Mehrabad}}</ref> which uses a multi-objective possibilistic programming model to deal with the design of a bio-ethanol production-distribution network. Also in another research, Zhou et al.<ref>{{Cite journal|title = A robust possibilistic mixed-integer programming method for planning municipal electric power systems|url = http://www.sciencedirect.com/science/article/pii/S0142061515002653|journal = International Journal of Electrical Power & Energy Systems|date = 2015-12-15|pages = 757–772|volume = 73|doi = 10.1016/j.ijepes.2015.06.009|language = EN|first = Y.|last = Zhou|first2 = Y.P.|last2 = Li|first3 = G.H.|last3 = Huang}}</ref> developed a robust possibilistic programming model to deal with the planning problem of municipal electric power system. \n* [[Sustainability]] such as Xu and Huang<ref>{{Cite journal|title = Development of an Improved Fuzzy Robust Chance-Constrained Programming Model for Air Quality Management|journal = Environmental Modeling & Assessment|date = 2015-10-15|pages = 535–548|volume = 20|issue = 5|doi = 10.1007/s10666-014-9441-3|language = EN|first = Ye|last = Xu|first2 = Guohe|last2 = Huang}}</ref> which employ ROFP to cope with an air quality management problem.\n\n== References ==\n{{Reflist}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Rosenbrock methods",
      "url": "https://en.wikipedia.org/wiki/Rosenbrock_methods",
      "text": "'''Rosenbrock methods''' refers to either of two distinct ideas in numerical computation, both named for [[Howard Harry Rosenbrock|Howard H. Rosenbrock]].\n\n==Numerical solution of differential equations==\n'''Rosenbrock methods''' for [[stiff equation|stiff differential equations]] are a family of [[multistep methods]] for solving [[ordinary differential equation]]s that contain a wide range of characteristic timescales.<ref>H. H. Rosenbrock, \"Some general implicit processes for the numerical solution of differential equations\", The Computer Journal (1963) 5(4): 329-330</ref><ref>{{Cite book | last1=Press | first1=WH |  authorlink1=William H. Press| last2=Teukolsky | first2=SA | authorlink2=Saul Teukolsky|last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press | publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 17.5.1. Rosenbrock Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=935}}</ref> They are related to the [[Runge–Kutta_methods#Implicit_Runge-Kutta_methods|implicit Runge-Kutta methods]]<ref>http://www.cfm.brown.edu/people/jansh/page5/page10/page40/assets/Yu_Talk.pdf</ref> and are also known as Kaps-Rentrop methods.<ref>http://mathworld.wolfram.com/RosenbrockMethods.html</ref>\n\n==Search method==\n'''Rosenbrock search''' is a [[numerical optimization]] algorithm applicable to optimization problems in which the [[objective function]] is inexpensive to compute and the derivative either does not exist or cannot be computed efficiently.<ref>H. H. Rosenbrock, \"An Automatic Method for Finding the Greatest or Least Value of a Function\", The Computer Journal (1960) 3(3): 175-184</ref> The idea of Rosenbrock search is also used to initialize some [[Root-finding algorithm|root-finding]] routines, such as '''fzero''' (based on [[Brent's method]]) in [[Matlab]]. Rosenbrock search is a form of [[Pattern search (optimization)|derivative-free search]] but may perform better on functions with sharp ridges.<ref>{{cite book |last=Leader |first=Jeffery J. | authorlink=Jeffery J. Leader| title=Numerical Analysis and Scientific Computation |year=2004 |publisher=Addison Wesley |location= |isbn= 0-201-73499-0}}</ref> The method often identifies such a ridge which, in many applications, leads to a solution.<ref>Shoup, T., Mistree, F., Optimization methods: with applications for personal computers, 1987, Prentice Hall, pg. 120 [https://books.google.com/books?id=q0zvAAAAMAAJ&q=%22Rosenbrock+search%22&dq=%22Rosenbrock+search%22&hl=en]</ref>\n\n==See also==\n*[[Rosenbrock function]]\n*[[Adaptive coordinate descent]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* http://www.applied-mathematics.net/optimization/rosenbrock.html\n\n{{Optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Ruzzo–Tompa algorithm",
      "url": "https://en.wikipedia.org/wiki/Ruzzo%E2%80%93Tompa_algorithm",
      "text": "The '''Ruzzo–Tompa algorithm''' is a [[Time_complexity#Linear_time|linear-time]] [[algorithm]] for finding all non-overlapping, contiguous, maximal scoring subsequences in a sequence of real numbers.<ref name=\"ruzzo_tompa\">{{cite journal|last1=Ruzzo|first1=Walter L.|last2=Martin|first2=Tompa|title=A Linear Time Algorithm for Finding All Maximal Scoring Subsequences|journal=Proceedings. International Conference on Intelligent Systems for Molecular Biology|date=1999|pages=234–241|pmid=10786306|url=https://dl.acm.org/citation.cfm?id=660812|ref=ruzzo-tompa}}</ref> This algorithm is an improvement over previously known quadratic time algorithms. The maximum scoring subsequence from the set produced by the algorithm is also a solution to the [[maximum subarray problem]].\n\nThe Ruzzo–Tompa algorithm has applications in [[bioinformatics]]<ref name=\"karlin\" />, [[web scraping]]<ref name=\"pasternack\">{{cite book|last1=Pasternack|first1=Jeff|last2=Roth|first2=Dan|title=Extracting Article Text from the Web with Maximum Subsequence Segmentation|journal=Proceedings of the 18th International Conference on World Wide Web|date=2009|pages=971–980|doi=10.1145/1526709.1526840|isbn=9781605584874}}</ref>, and [[information retrieval]]<ref name=\"liang\">{{cite book|last1=Liang|first1=Shangsong|last2=Ren|first2=Zhaochun|last3=Weerkamp|first3=Wouter|last4=Meij|first4=Edgar|last5=de Rijke|first5=Maarten|title=Time-Aware Rank Aggregation for Microblog Search|journal=Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management|date=2014|pages=989–998|doi=10.1145/2661829.2661905|isbn=9781450325981|citeseerx=10.1.1.681.6828}}</ref>.\n\n==Applications==\n===Bioinformatics===\nThe Ruzzo–Tompa algorithm has been used in [[Bioinformatics]] tools to study biological data. The problem of finding disjoint maximal subsequences is of practical importance in the analysis of [[DNA]]. Maximal subsequences algorithms have been used in the identification of transmembrane segments and the evaluation of [[sequence homology]]<ref name=\"karlin\">{{cite journal|last1=Karlin|first1=S|last2=Altschul|first2=SF|title=Applications and statistics for multiple high-scoring segments in molecular sequences|journal=Proceedings of the National Academy of Sciences of the United States of America|date=Jun 15, 1993|volume=90|issue=12|pages=5873–5877|pmid=8390686|pmc=46825|doi=10.1073/pnas.90.12.5873}}</ref>\n\nThe algorithm is used in [[sequence alignment]] which is used as a method of identifying similar [[DNA]], [[RNA]], or [[protein]] sequences. Accounting for the ordering of pairs of high-scoring subsequences in two sequences creates better sequence alignments. This is because the biological model suggests that separate high-scoring subsequence pairs arise from insertions or deletions within a matching region. Requiring consistent ordering of high-scoring subsequence pairs increases their statistical significance.<ref name=\"karlin\" />\n\n===Web scraping===\nThe Ruzzo–Tompa algorithm is used in [[Web scraping]] to extract information from web pages. Pasternack and Roth proposed a method for extracting important blocks of text from HTML documents. The web pages are first [[Lexical_analysis#Tokenization|tokenized]] and the score for each token is found using local, token-level classifiers. A modified version of the Ruzzo–Tompa algorithm is then used to find the k highest-valued subsequences of tokens. These subsequences are then used as predictions of important blocks of text in the article.<ref name=\"pasternack\" />\n\n===Information retrieval===\nThe Ruzzo–Tompa algorithm has been used in [[Information retrieval]] search algorithms. Liang et al. proposed a [[data fusion]] method to combine the search results of several microblog search algorithms. In their method, the Ruzzo–Tompa algorithm is used detect [[Bursting|bursts]] of information. <ref name=\"liang\" />\n\n==Problem definition==\n\nThe problem of finding all maximal subsequences is defined as follows: Given a list of real numbered scores <math>x_1,x_2,\\ldots,x_n</math>, find the list of contiguous subsequences that gives the great total score, where the score of each subsequence <math>S_{i,j} = \\sum_{i\\leq k\\leq j} x_k</math>. The subsequences must be disjoint (non-overlapping) and have a positive score.\n\n==Other algorithms==\n\nThere are several approaches to solving the all maximal scoring subsequences problem. A natural approach is to use existing, linear time algorithms to find the maximum subsequence (see [[maximum subarray problem]]) and then recursively find the maximal subsequences to the left and right of the maximum subsequence. The analysis of this algorithm is similar to that of [[Quicksort]]: The maximum subsequence could be small in comparison to the rest of sequence, leading to a running time of <math>O(n^2)</math> in the worst case.\n\n==Algorithm==\n\n[[File:Animation of Ruzzo-Tompa Algorithm.ogv|300px|thumb|This animation shows the Ruzzo–Tompa algorithm running with an input sequence of 11 integers each represented by a line segment in the graph. Segments with bold lines represent maximal segments found so far. The animation shows the state of <math>I, R </math> and <math>L</math> at each step. Below that it shows the current state the algorithm which correspond to steps 1–4 in the [[#Algorithm|Algorithm]] section of this page. The red highlight shows the algorithm finding a value for <math>j</math> in steps 1 and 3. If the value of <math>j</math> satisfies the inequalities in those steps the highlight turns green. \n\nAt the end of the animation the maximal subsequences will be bolded and displayed in <math>I</math>.\n]]\n\nThe standard implementation of the Ruzzo–Tompa algorithm runs in <math>O(n)</math> time and uses ''O''(''n'') space, where ''n'' is the length of the list of scores. The algorithm uses [[dynamic programming]] to progressively build the final solution by incrementally solving progressively larger subsets of the problem. The description of the algorithm provided by Ruzzo and Tompa is as follows:\n\n: Read the scores left to right and maintain the cumulative sum of the scores read. Maintain an ordered list <math>I_1,I_2,\\ldots,I_j</math> of disjoint subsequences. For each subsequence <math>I_j</math>, record the cumulative total <math>L_j</math> of all scores up to but not including the leftmost score of <math>I_j</math>, and the total <math>R_j</math> up to and including the rightmost score of <math>I_j</math>.\n\n: The lists are initially empty. Scores are read from left to right and are processed as follows. Nonpositive scores are require no special processing, so the next score is read. A positive score is incorporated into a new sub-sequence <math>I_k</math> of length one that is then integrated into the list by the following process.\n\n# The list <math>I</math> is searched from right to left for the maximum value of <math>j</math> satisfying <math>L_j<L_k</math>\n# If there is no such <math>j</math>, then add <math>I_k</math> to the end of the list.\n# If there is such a <math>j</math>, and <math>R_j \\geq R_k</math>, then add <math>I_k</math> to the end of the list.\n# Otherwise (i.e., there is such a j, but <math>R_j < R_k</math>), extend the subsequence <math>I_k</math> to the left to encompass everything up to and including the leftmost score in <math>I_j</math>. Delete subsequences <math>I_j,I_j+1,\\ldots,I_k-1</math> from the list, and append <math>I_k</math> to the end of the list. Reconsider the newly extended subsequence <math>I_k</math> (now renumbered <math>I_j</math>) as in step 1.\n\n:Once the end of the input is reached, all subsequences remaining on the list <math>I</math> are maximal.<ref name=\"ruzzo_tompa\" />\n\nThe following [[Python (programming language)|Python]] code implements the Ruzzo–Tompa algorithm:\n\n<source lang=\"python\" line=\"1\">\ndef RuzzoTompa(scores):\n\tk=0\n\ttotal = 0;\n\t# Allocating arrays of size n\n\tI,L,R,Lidx = [[0]*len(scores) for _ in range(4)]\n\tfor i,s in enumerate(scores):\n\t\ttotal += s\n\t\tif s > 0:\n\t\t\t# store I[k] by (start,end) indices of scores\n\t\t\tI[k] = (i,i+1)\n\t\t\tLidx[k] = i\n\t\t\tL[k] = total-s\n\t\t\tR[k] = total\n\t\t\twhile(True):\n\t\t\t\tmaxj = None\n\t\t\t\tfor j in range(k-1,-1,-1):\n\t\t\t\t\tif L[j] < L[k]:\n\t\t\t\t\t\tmaxj = j\n\t\t\t\t\t\tbreak;\n\t\t\t\tif maxj != None and R[maxj] < R[k]:\n\t\t\t\t\tI[maxj] = (Lidx[maxj],i+1)\n\t\t\t\t\tR[maxj] = total\n\t\t\t\t\tk = maxj\n\t\t\t\telse:\n\t\t\t\t\tk+=1\n\t\t\t\t\tbreak;\n\t# Getting maximal subsequences using stored indices\n\treturn [scores[I[l][0]:I[l][1]] for l in range(k)]\n</source>\n\n== References ==\n<!-- Inline citations added to your article will automatically display here. See https://en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. -->\n{{reflist}}\n\n{{DEFAULTSORT:Ruzzo-Tompa algorithm}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example Python code]]"
    },
    {
      "title": "Search-based software engineering",
      "url": "https://en.wikipedia.org/wiki/Search-based_software_engineering",
      "text": "{{Use dmy dates|date=November 2011}}\n\n'''Search-based software engineering''' ('''SBSE''') applies [[metaheuristic]] search techniques such as [[genetic algorithms]], [[simulated annealing]] and [[tabu search]] to [[software engineering]] problems. Many activities in [[software engineering]] can be stated as [[Optimization (mathematics)|optimization]] problems. [[Optimization (mathematics)|Optimization]] techniques of [[operations research]] such as [[linear programming]] or [[dynamic programming]] are often impractical for large scale [[software engineering]] problems because of their [[Computational complexity theory|computational complexity]]. Researchers and practitioners use [[metaheuristic]] search techniques to find near-optimal or \"good-enough\" solutions.\n\nSBSE problems can be divided into two types:\n\n* black-box optimization problems, for example, assigning people to tasks (a typical [[combinatorial optimization]] problem).\n* white-box problems where operations on source code need to be considered.<ref>\n{{Cite conference\n| doi = 10.1109/SCAM.2010.28\n| conference = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\n| pages = 7–19\n| last = Harman\n| first = Mark\n| title = Why Source Code Analysis and Manipulation Will Always be Important\n| booktitle = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\n| year = 2010\n}}</ref>\n\n==Definition==\nSBSE converts a software engineering problem into a computational search problem that can be tackled with a [[metaheuristic]]. This involves defining a search space, or the set of possible solutions. This space is typically too large to be explored exhaustively, suggesting a [[metaheuristic]] approach. A metric <ref>\n{{Cite conference\n| doi = 10.1109/METRIC.2004.1357891\n| conference = 10th International Symposium on Software Metrics, 2004\n| pages = 58–69\n| last = Harman\n| first = Mark\n|author2=John A. Clark\n | title = Metrics are fitness functions too\n| booktitle = Proceedings of the 10th International Symposium on Software Metrics, 2004 \n| year = 2004\n}}</ref> (also called a fitness function, cost function, objective function or quality measure) is then used to measure the quality of potential solutions. Many software engineering problems can be reformulated as a computational search problem.<ref>{{Cite journal\n| doi = 10.1049/ip-sen:20030559\n| issn = 1462-5970\n| volume = 150\n| issue = 3\n| pages = 161–175\n| last1 = Clark\n| first1 = John A.\n| last2 = Dolado\n| first2 = José Javier\n| last3 = Harman\n| first3 = Mark\n| last4 = Hierons\n| first4 = Robert M.\n| last5 = Jones\n| first5 = Bryan F.\n| last6 = Lumkin\n| first6 = M.\n| last7 = Mitchell\n| first7 = Brian S.\n| last8 = Mancoridis\n| first8 = Spiros\n| last9 = Rees\n| first9 = K.\n| last10 = Roper\n| first10 = Marc\n| last11 = Shepperd\n| first11 = Martin J.\n| title = Reformulating software engineering as a search problem\n| journal = [[IEE Proceedings - Software]]\n| year = 2003\n| citeseerx = 10.1.1.144.3059\n}}</ref>\n\nThe term \"[[search-based application]]\", in contrast, refers to using [[search engine technology]], rather than search techniques, in another industrial application.\n\n==Brief history==\nOne of the earliest attempts to apply [[Optimization (mathematics)|optimization]] to a [[software engineering]] problem was reported by [[Webb Miller]] and David Spooner in 1976 in the area of [[software testing]].<ref>\n{{Cite journal\n| doi = 10.1109/TSE.1976.233818\n| issn = 0098-5589\n| volume = SE-2\n| issue = 3\n| pages = 223–226\n| last = Miller\n| first = Webb\n| last2 = Spooner\n| first2 = David L. \n| title = Automatic Generation of Floating-Point Test Data\n| journal = IEEE Transactions on Software Engineering\n| year = 1976\n}}</ref> In 1992, S. Xanthakis and his colleagues applied a search technique to a [[software engineering]] problem for the first time.<ref>S. Xanthakis, C. Ellis, C. Skourlas, A. Le Gall, S. Katsikas and K. Karapoulios, \"Application of genetic algorithms to software testing,\" in ''Proceedings of the 5th International Conference on Software Engineering and its Applications'', Toulouse, France, 1992, pp.&nbsp;625–636</ref> The term SBSE was first used in 2001 by [[Mark Harman (computer scientist)|Harman]] and Jones.<ref>\n{{Cite journal\n| doi = 10.1016/S0950-5849(01)00189-6\n| issn = 0950-5849\n| volume = 43\n| issue = 14\n| pages = 833–839\n| last = Harman\n| first = Mark\n| last2 = Jones\n| first2 = Bryan F.\n| title = Search-based software engineering\n| journal = Information and Software Technology\n| date = 2001-12-15\n| citeseerx = 10.1.1.143.9716\n}}</ref> The research community grew to include more than 800 authors by 2013, spanning approximately 270 institutions in 40 countries.{{Citation needed|date=October 2013}}\n\n==Application areas==\n\nSearch-based software engineering is applicable to almost all phases of the [[software life cycle|software development process]]. [[Software testing]] has been one of the major applications.<ref>\n{{Cite journal\n| doi = 10.1002/stvr.294\n| issn = 1099-1689\n| volume = 14\n| issue = 2\n| pages = 105–156\n| last = McMinn\n| first = Phil\n| title = Search-based software test data generation: a survey\n| journal = Software Testing, Verification and Reliability\n| year = 2004\n| citeseerx = 10.1.1.122.33\n}}</ref> Search techniques have been applied to other [[software engineering]] activities, for instance, [[requirements analysis]],<ref>\n{{Cite journal\n| doi = 10.1016/j.infsof.2003.07.002\n| issn = 0950-5849\n| volume = 46\n| issue = 4\n| pages = 243–253\n| last = Greer\n| first = Des\n| last2 = Ruhe\n| first2 = Guenther\n| title = Software release planning: an evolutionary and iterative approach\n| journal = Information and Software Technology\n| date = 2004-03-15\n| citeseerx = 10.1.1.195.321\n}}</ref><ref>{{Cite conference\n| doi = 10.1109/SBES.2009.23\n| conference = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\n| pages = 207–215\n| last = Colares\n| first = Felipe\n| last2 = Souza\n| first2 = Jerffeson\n| last3 = Carmo\n| first3 = Raphael\n| last4 = Pádua\n| first4 = Clarindo\n| last5 = Mateus\n| first5 = Geraldo R.\n| title = A New Approach to the Software Release Planning\n| booktitle = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\n| year = 2009\n}}</ref> [[software design|design]],<ref>\n{{Cite journal\n| doi = 10.1016/S0950-5849(01)00195-1\n| issn = 0950-5849\n| volume = 43\n| issue = 14\n| pages = 891–904\n| last = Clark\n| first = John A.\n| last2 = Jacob\n| first2 = Jeremy L. \n| title = Protocols are programs too: the meta-heuristic search for security protocols\n| journal = Information and Software Technology\n| date = 2001-12-15\n| citeseerx = 10.1.1.102.6016\n}}</ref> [[software development|development]],<ref>\n{{Cite journal\n| doi = 10.1016/j.ins.2006.12.020\n| issn = 0020-0255\n| volume = 177\n| issue = 11\n| pages = 2380–2401\n| last = Alba\n| first = Enrique\n| last2 = Chicano\n| first2 = J. Francisco \n| title = Software project management with GAs\n| journal = Information Sciences\n| date = 2007-06-01\n| hdl = 10630/8145\n}}</ref> and [[software maintenance|maintenance]].<ref>\n{{Cite conference\n| doi = 10.1109/ICSM.2005.79\n| conference = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\n| pages = 240–249\n| last = Antoniol\n| first = Giuliano\n| last2 = Di Penta\n| first2 = Massimiliano \n| last3 = Harman\n| first3 = Mark\n| title = Search-based techniques applied to optimization of project planning for a massive maintenance project\n| booktitle = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\n| year = 2005\n}}</ref>\n\n===Requirements engineering===\n[[Requirements engineering]] is the process by which the needs of a software's users and environment are determined and managed. Search-based methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches user requests amid constraints such as limited resources and interdependencies between requirements. This problem is often tackled as a [[MCDM|multiple-criteria decision-making]] problem and, generally involves presenting the decision maker with a set of good compromises between cost and user satisfaction as well as the requirements risk.<ref>\n{{Cite thesis\n| type = Ph.D.\n| publisher = University of London\n| last = Zhang\n| first = Yuanyuan\n| title = Multi-Objective Search-based Requirements Selection and Optimisation\n| location = Strand, London, UK\n| date = February 2010\n| url = http://eprints.ucl.ac.uk/170695/\n}}</ref><ref>\nY.&nbsp;Zhang and M.&nbsp;Harman and S.&nbsp;L.&nbsp;Lim, \"[http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/images/Research_Student_Information/RN_11_12.pdf Search Based Optimization of Requirements Interaction Management],\" Department of Computer Science, University College London, Research Note RN/11/12, 2011.\n</ref>\n<ref>{{cite book|last1=Li|first1=Lingbo|last2=Harman|first2=Mark|last3=Letier|first3=Emmanuel|last4=Zhang|first4=Yuanyuan|title=Robust Next Release Problem: Handling Uncertainty During Optimization|journal=Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation|date=2014|pages=1247–1254|doi=10.1145/2576768.2598334|isbn=9781450326629|series=Gecco '14}}</ref><ref>{{cite journal|last1=Li|first1=L.|last2=Harman|first2=M.|last3=Wu|first3=F.|last4=Zhang|first4=Y.|title=The Value of Exact Analysis in Requirements Selection|journal=IEEE Transactions on Software Engineering|date=2017|volume=43|issue=6|pages=580–596|doi=10.1109/TSE.2016.2615100|url=http://ieeexplore.ieee.org/abstract/document/7582553/?reload=true|issn=0098-5589}}</ref>\n\n===Debugging and maintenance===\nIdentifying a [[software bug]] (or a [[code smell]]) and then [[debugging]] (or [[refactoring]]) the software is largely a manual and labor-intensive endeavor, though the process is tool-supported. One objective of SBSE is to automatically identify and fix bugs (for example via [[mutation testing]]).\n\n[[Genetic programming]], a biologically-inspired technique that involves evolving programs through the use of crossover and mutation, has been used to search for repairs to programs by altering a few lines of source code. The [http://dijkstra.cs.virginia.edu/genprog/ GenProg Evolutionary Program Repair] software repaired 55 out of 105 bugs for approximately $8 each in one test.<ref>{{Cite conference\n| doi = 10.1109/ICSE.2012.6227211\n| conference = 2012 34th International Conference on Software Engineering (ICSE)\n| pages = 3–13\n| last = Le Goues\n| first = Claire\n| last2 = Dewey-Vogt\n| first2 = Michael\n| last3 = Forrest\n| first3 = Stephanie\n| last4 = Weimer\n| first4 = Westley \n| title = A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each\n| booktitle = 2012 34th International Conference on Software Engineering (ICSE)\n| year = 2012\n}}</ref>\n\n[[Coevolution]] adopts a \"predator and prey\" [[metaphor]] in which a suite of programs and a suite of [[Unit Testing|unit tests]] evolve together and influence each other.<ref>{{Cite conference\n| doi = 10.1109/CEC.2008.4630793\n| conference = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\n| pages = 162–168\n| last = Arcuri\n| first = Andrea\n| last2 = Yao\n| first2 = Xin \n| title = A novel co-evolutionary approach to automatic software bug fixing\n| booktitle = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\n| year = 2008\n}}</ref>\n\n===Testing===\nSearch-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. [[Regression testing]] has also received some attention.\n\n===Optimizing software===\nThe use of SBSE in [[program optimization]], or modifying a piece of software to make it more efficient in terms of speed and resource use, has been the object of successful research.<ref>{{cite journal |last1=Memeti |first1=Suejb |last2=Pllana |first2=Sabri |last3=Binotto |first3=Alecio |last4=Kolodziej |first4=Joanna |last5=Brandic |first5=Ivona |author5-link= Ivona Brandić |title=Using meta-heuristics and machine learning for software optimization of parallel computing systems: a systematic literature review |journal=Computing |date=2018 |pages=1–44 |doi=10.1007/s00607-018-0614-9 |arxiv=1801.09444 }}</ref> In one instance, a 50,000 line program was genetically improved, resulting in a program 70 times faster on average.<ref>\n{{Cite journal\n| last = Langdon\n| first = William B.\n| last2 = Harman\n| first2 = Mark \n| title = Optimising Existing Software with Genetic Programming\n| journal = IEEE Transactions on Evolutionary Computation\n| url = http://www0.cs.ucl.ac.uk/staff/w.langdon/ftp/papers/Langdon_2013_ieeeTEC.pdf\n}}</ref>\nA recent work by Basios et al. shows that by optimising the data structure, Google Guava found 9% improvement on execution time, 13% improvement on memory consumption and 4% improvement on CPU usage separately<ref>{{cite book|last1=Basios|first1=Michail|last2=Li|first2=Lingbo|last3=Wu|first3=Fan|last4=Kanthan|first4=Leslie|last5=Barr|first5=Earl T.|title=Optimising Darwinian Data Structures on Google Guava|journal=Search Based Software Engineering|volume=10452|date=9 September 2017|pages=161–167|doi=10.1007/978-3-319-66299-2_14|language=en|series=Lecture Notes in Computer Science|isbn=978-3-319-66298-5|url=http://discovery.ucl.ac.uk/10062895/1/ssbse_dariwnian_guava.pdf}}</ref>.\n\n===Project management===\nA number of decisions that are normally made by a project manager can be done automatically, for example, project scheduling.<ref>\n{{Cite conference\n| publisher = ACM\n| doi = 10.1145/2330163.2330332\n| isbn = 978-1-4503-1177-9\n| pages = 1221–1228\n| last = Minku\n| first = Leandro L.\n| last2 = Sudholt\n| first2 = Dirk\n| last3 = Yao\n| first3 = Xin \n| title = Evolutionary algorithms for the project scheduling problem: runtime analysis and improved design\n| booktitle = Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference\n| location = New York, NY, USA\n| series = GECCO '12\n| year = 2012\n}}</ref>\n\n==Tools==\nTools available for SBSE include OpenPAT.<ref>\n{{cite conference\n|ref        = harv\n|last1      = Mayo\n|first1     = M.\n|last2      = Spacey\n|first2     = S.\n|title      = Predicting Regression Test Failures Using Genetic Algorithm-Selected Dynamic Performance Analysis Metrics\n|journal    = Proceedings of the 5th International Symposium on Search-Based Software Engineering (SSBSE)\n|volume     = 8084\n|pages      = 158–171\n|year       = 2013\n|doi      = 10.1007/978-3-642-39742-4_13\n}}</ref> and [[EvoSuite]] <ref>(http://www.evosuite.org/)</ref> and [https://coverage.readthedocs.io/ Coverage], a code coverage measurement tool for Python<ref>{{Citation|last=others|first=Ned Batchelder and 100|title=coverage: Code coverage measurement for Python|url=https://bitbucket.org/ned/coveragepy|accessdate=2018-03-14}}\n</ref>\n\n==Methods and techniques==\nA number of methods and techniques are available, including:\n* [[profiling (computer programming)|Profiling]]<ref>{{Cite web | url=http://java-source.net/open-source/profilers | title=Open Source Profilers in Java}}</ref> via [[instrumentation]] in order to monitor certain parts of a program as it is executed.\n* Obtaining an [[abstract syntax tree]] associated with the program, which can be automatically examined to gain insights into its structure.\n* Applications of [[program slicing]] relevant to SBSE include [[software maintenance]], [[Optimization (computer science)|optimization]] and [[Program analysis (computer science)|program analysis]].\n* [[Code coverage]] allows measuring how much of the code is executed with a given set of input data.\n* [[Static program analysis]]\n\n==Industry acceptance==\nAs a relatively new area of research, SBSE does not yet experience broad industry acceptance. Software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are unlike those that humans produce.<ref>\n{{cite web\n |url        = http://shape-of-code.coding-guidelines.com/2013/10/18/programming-using-genetic-algorithms-isnt-that-what-humans-already-do/\n |title      = Programming using genetic algorithms: isn't that what humans already do ;-)\n |last       = Jones\n |first      = Derek\n |date       = 18 October 2013\n |website    = The Shape of Code\n |accessdate = 31 October 2013\n}}\n</ref> In the context of SBSE use in fixing or improving programs, developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a system's requirements and testing environment. Considering that fully automated programming has yet to be achieved, a desirable property of such modifications would be that they need to be easily understood by humans to support maintenance activities.<ref>\n{{Cite journal\n| doi = 10.1007/s11219-013-9208-0\n| issn = 1573-1367\n| volume = 21\n| issue = 3\n| pages = 421–443\n| last = Le Goues\n| first = Claire\n| last2 = Forrest \n| first2 = Stephanie \n| last3 = Weimer\n| first3 = Westley\n| title = Current challenges in automatic software repair\n| journal = Software Quality Journal\n| date = 2013-09-01\n| citeseerx = 10.1.1.371.5784\n}}\n</ref>\n\nAnother concern is that SBSE might make the software engineer redundant. Supporters claim that the motivation for SBSE is to enhance the relationship between the engineer and the program.<ref>\n{{Cite conference\n| publisher = IEEE Press\n| conference = First International Workshop on Combining Modelling with Search-Based Software Engineering,First International Workshop on Combining Modelling with Search-Based Software Engineering\n| pages = 49–50\n| last = Simons\n| first = Christopher L.\n| title = Whither (away) software engineers in SBSE?\n| location = San Francisco, USA\n| accessdate = 2013-10-31\n| date = May 2013\n| url = http://eprints.uwe.ac.uk/19938/\n}}</ref>\n\n==See also==\n{{Portal|Software Testing}}\n*[[Program analysis (computer science)]]\n*[[Dynamic program analysis]]\n*[[Genetic improvement]]\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n*[http://crestweb.cs.ucl.ac.uk/resources/sbse_repository/ Repository of publications on SBSE]\n*[http://neo.lcc.uma.es/mase/ Metaheuristics and Software Engineering]\n*[http://sir.unl.edu/portal/index.php  Software-artifact Infrastructure Repository]\n*[http://2013.icse-conferences.org/ International Conference on Software Engineering]\n*[http://www.sigevo.org/wiki/tiki-index.php Genetic and Evolutionary Computation (GECCO)]\n*[https://scholar.google.co.uk/citations?view_op=search_authors&hl=en&mauthors=label:sbse Google Scholar page on Search-based software engineering]\n\n[[Category:Computer-related introductions in 2001]]\n[[Category:Software engineering]]\n[[Category:Software testing]]\n[[Category:Search algorithms]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Genetic algorithms]]\n[[Category:Software articles needing expert attention]]\n[[Category:Software quality]]\n[[Category:Program analysis]]"
    },
    {
      "title": "Second-order cone programming",
      "url": "https://en.wikipedia.org/wiki/Second-order_cone_programming",
      "text": "{{Technical|date=October 2011}}\n\nA '''second-order cone program''' ('''SOCP''') is a [[convex optimization]] problem of the form\n\n:minimize <math>\\ f^T x \\ </math> \n:subject to\n::<math>\\lVert A_i x + b_i \\rVert_2 \\leq c_i^T x + d_i,\\quad i = 1,\\dots,m</math>\n::<math>Fx = g \\ </math>\n\nwhere the problem parameters are <math>f \\in \\mathbb{R}^n, \\ A_i \\in \\mathbb{R}^{{n_i}\\times n}, \\ b_i \\in \\mathbb{R}^{n_i}, \\ c_i \\in  \\mathbb{R}^n, \\ d_i \\in \\mathbb{R}, \\ F \\in \\mathbb{R}^{p\\times n}</math>, and <math>g \\in \\mathbb{R}^p</math>. <math>x\\in\\mathbb{R}^n</math> is the optimization variable.\n<math>\\lVert x \\rVert_2 </math> is the [[Euclidean norm]] and <math>^T</math> indicates [[Transpose]]. \n<ref name=\"boyd\">{{cite book |last1=Boyd |first1=Stephen |last2=Vandenberghe |first2=Lieven |title=Convex Optimization |publisher=Cambridge University Press |year=2004 |isbn=978-0-521-83378-3 |url=http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf |format=pdf |accessdate=October 3, 2011}}</ref>\nWhen <math>A_i = 0</math> for <math>i = 1,\\dots,m</math>, the SOCP reduces to a [[linear program]].  When <math>c_i = 0 </math> for <math>i = 1,\\dots,m</math>, the SOCP is equivalent to a convex quadratically constrained linear program. \n\nConvex [[quadratically constrained quadratic program]]s can also be formulated as SOCPs by reformulating the objective function as a constraint. \n[[Semidefinite programming]] subsumes SOCPs as the SOCP constraints can be written as [[linear matrix inequality|linear matrix inequalities]] (LMI) and can be reformulated as an instance of semi definite program. \n\nSOCPs can be solved with great efficiency by [[interior point methods]].\n\n==Example: Quadratic constraint==\nConsider a [[quadratically constrained quadratic program|quadratic constraint]] of the form\n\n:<math> x^T A^T A x + b^T x + c \\leq 0. </math>\n\nThis is equivalent to the SOC constraint\n\n:<math> \\left\\|\n\\begin{matrix}\n(1 + b^T x +c)/2\\\\\nAx\n\\end{matrix} \\right\\|_2\n\n\\leq (1 - b^T x -c)/2.</math>\n\n==Example: Stochastic linear programming==\nConsider a [[stochastic linear program]] in inequality form\n\n:minimize <math>\\ c^T x \\ </math> \n:subject to\n:: <math>P(a_i^Tx \\leq b_i) \\geq p, \\quad i = 1,\\dots,m </math>\n\nwhere the parameters <math>a_i \\ </math> are independent Gaussian random vectors with mean <math>\\bar{a}_i</math> and covariance <math>\\Sigma_i \\ </math> and <math>p\\geq0.5</math>.  This problem can be expressed as the SOCP\n\n:minimize <math>\\ c^T x \\ </math> \n:subject to\n:: <math>\\bar{a}_i^T x + \\Phi^{-1}(p) \\lVert \\Sigma_i^{1/2} x \\rVert_2 \\leq b_i  , \\quad i = 1,\\dots,m </math>\n\nwhere <math>\\Phi^{-1} \\ </math> is the inverse [[normal cumulative distribution function]].<ref name=\"boyd\"/>\n\n==Example: Stochastic second-order cone programming==\nWe refer to second-order cone programs\nas deterministic second-order cone programs since data defining them are deterministic.\nStochastic second-order cone programs<ref name=\"Alzalg\">{{cite journal|last=Alzalg|first=Baha |title=Stochastic second-order cone programming: Application models|journal=Applied Mathematical Modelling|year=2012|volume=36|issue=10|pages=5122–5134|doi=10.1016/j.apm.2011.12.053}}</ref> is a class of optimization problems that defined to handle uncertainty in data defining deterministic second-order cone programs.\n\n==Solvers and scripting (programming) languages==\n\n{| class=\"wikitable sortable\"\n|-\n!Name\n!License\n!Brief info\n|-\n|[[AMPL]]||commercial|| An algebraic modeling language with SOCP support\n|-\n|[[Artelys Knitro]]||commercial||\n|-\n|[[CPLEX]]||commercial||\n|-\n|[[FICO Xpress]]||commercial||\n|-\n|[[Gurobi]]||commercial||parallel SOCP barrier algorithm\n|-\n|[[MOSEK]]||commercial||\n|-\n|[https://github.com/embotech/ecos/ ECOS]||open-source||embedded SOCP solver\n|}\n\n==References==\n{{reflist}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Convex optimization]]"
    },
    {
      "title": "Semidefinite embedding",
      "url": "https://en.wikipedia.org/wiki/Semidefinite_embedding",
      "text": "'''Semidefinite embedding''' (SDE) or '''maximum variance unfolding (MVU)''' is an [[algorithm]] in [[computer science]] that uses [[semidefinite programming]] to perform [[non-linear dimensionality reduction]] of high-dimensional [[coordinate vector|vector]]ial input data. MVU can be viewed as a non-linear generalization of [[Principal component analysis]].\n\nNon-linear dimensionality reduction algorithms attempt to map high-dimensional data onto a low-dimensional [[Euclidean space|Euclidean]] [[vector space]]. Maximum variance Unfolding is a member of the [[manifold learning]] family, which also include algorithms such as [[isomap]] and [[locally linear embedding]]. In manifold learning, the input data is assumed to be sampled from a low dimensional [[manifold]] that is embedded inside of a higher-dimensional vector space. The main intuition behind MVU is to exploit the local linearity of manifolds and create a mapping that preserves local neighbourhoods at every point of the underlying manifold. \n\nMVU creates a mapping from the high dimensional input vectors to some low dimensional Euclidean vector space in the following steps:\n\nA [[neighbourhood (topology)|neighbourhood]] graph is created. Each input is connected with its k-nearest input vectors (according to Euclidean distance metric) and all k-nearest neighbors are connected with each other. If the data is sampled well enough, the resulting graph is a discrete approximation of the underlying manifold. \n\nThe neighbourhood graph is \"unfolded\" with the help of semidefinite programming. Instead of learning the output vectors directly, the semidefinite programming aims to find an inner product matrix that maximizes the pairwise distances between any two inputs that are not connected in the neighbourhood graph while preserving the nearest neighbors distances. \n\nThe low-dimensional embedding is finally obtained by application of [[multidimensional scaling]] on the learned inner product matrix.\n\nThe steps of applying semidefinite programming followed by a linear dimensionality reduction step to recover a low-dimensional embedding into a Euclidean space were first proposed by Linial, London, and Rabinovich.\n\n==Optimization Formulation==\n\nLet <math>X \\,\\!</math> be the original input and <math>Y\\,\\!</math> be the embedding. If <math>i,j\\,\\!</math> are two neighbors, then the local isometry constraint that needs to be satisfied is:\n\n:<math>|X_{i}-X_{j}|^{2}=|Y_{i}-Y_{j}|^{2}\\,\\!</math>\n\nLet <math>G, K\\,\\!</math> be the [[Gramian matrix|Gram matrices]] of <math> X \\,\\!</math> and <math> Y \\,\\!</math> (i.e.: <math>G_{ij}=X_i \\cdot X_j,K_{ij}=Y_i \\cdot Y_j \\,\\!</math>). We can express the above constraint for every neighbor points <math>i,j\\,\\!</math> in term of <math>G, K\\,\\!</math>:\n\n:<math>G_{ii}+G_{jj}-G_{ij}-G_{ji}=K_{ii}+K_{jj}-K_{ij}-K_{ji}\\,\\!</math>\n\nIn addition, we also want to constrain the embedding <math> Y \\,\\!</math> to center at the origin:\n\n<math>\\sum_{i}Y_{i}=0\\Leftrightarrow(\\sum_{i}Y_{i}) \\cdot (\\sum_{i}Y_{i})=0\\Leftrightarrow\\sum_{i,j}Y_{i} \\cdot Y_{j}=0\\Leftrightarrow\\sum_{i,j}K_{ij}=0</math>\n\nAs described above, except the distances of neighbor points are preserved, the algorithm aims to maximize the pairwise distance of every pair of points. The objective function to be maximized is:\n\n<math>T(Y)=\\dfrac{1}{2N}\\sum_{i,j}|Y_{i}-Y_{j}|^{2}</math>\n\nIntuitively, maximizing the function above is equivalent to pulling the points as far away from each other as possible and therefore \"unfold\" the manifold. The local isometry constraint prevents the objective function from going to infinity. Proof:\n\nLet <math>\\tau = max \\{\\eta_{ij}|Y_{i}-Y_{j}|^2\\} \\,\\!</math> where <math> \\eta_{ij} = 1 \\,\\!</math> if i and j are neighbors and <math> \\eta_{ij} = 0 \\,\\!</math> otherwise.\n\nSince the graph has N points, the distance between any two points <math>|Y_{i}-Y_{j}|^2 \\leq N \\tau \\,\\!</math>. We can then bound the objective function as follow:\n\n:<math>T(Y)=\\dfrac{1}{2N}\\sum_{i,j}|Y_{i}-Y_{j}|^{2} \\leq \\dfrac{1}{2N}\\sum_{i,j}(N\\tau)^2 = \\dfrac{N^3\\tau^2}{2} \\,\\!</math>\n\nThe objective function can be rewritten purely in the form of the Gram matrix:\n\n:<math> \n\\begin{align}\n  T(Y) &{}=  \\dfrac{1}{2N}\\sum_{i,j}|Y_{i}-Y_{j}|^{2} \\\\\n          &{}= \\dfrac{1}{2N}(\\sum_{i,j}(Y_{i}^2+Y_{j}^2-Y_{i} \\cdot Y_{j} - Y_{j} \\cdot Y_{i})\\\\\n          &{}= \\dfrac{1}{2N}(\\sum_{i,j}Y_{i}^2+\\sum_{i,j}Y_{j}^2-\\sum_{i,j}Y_{i} \\cdot Y_{j} -\\sum_{i,j}Y_{j} \\cdot Y_{i})\\\\  \n          &{}= \\dfrac{1}{2N}(\\sum_{i,j}Y_{i}^2+\\sum_{i,j}Y_{j}^2-0 -0)\\\\\n          &{}= \\dfrac{1}{N}(\\sum_{i}Y_{i}^2)=\\dfrac{1}{N}(Tr(K))\\\\\n\\end{align}\n\\,\\!</math>\n\nFinally, the optimization can be formulated as:\n\n'''Maximize''' <math> Tr(K) \\,\\!</math>\n\n'''Subject to''' <math> K \\succeq 0\\,\\!</math> and\n<math> \\forall i,j \\,\\!</math> where <math> \\eta_{ij} =1, G_{ii}+G_{jj}-G_{ij}-G_{ji}=K_{ii}+K_{jj}-K_{ij}-K_{ji} \\,\\!</math> \n\nAfter the Gram matrix <math>K \\,\\!</math> is learned by semidefinite programming, the output <math>Y \\,\\!</math> can be obtained via [[Cholesky decomposition]]. In particular, the Gram matrix can be written as <math> K_{ij}=\\sum_{\\alpha = 1}^{N}(\\lambda_{\\alpha } V_{\\alpha i} V_{\\alpha j}) \\,\\!</math> where <math> V_{\\alpha i} \\,\\!</math> is the i-th element of eigenvector <math> V_{\\alpha} \\,\\!</math> of the eigenvalue <math> \\lambda_{\\alpha } \\,\\!</math>.\n\nIt follows that the <math> \\alpha \\,\\!</math>-th element of the output <math> Y_i \\,\\!</math> is <math> \\sqrt{\\lambda_{\\alpha }} V_{\\alpha i} \\,\\!</math>.\n\n==See also==\n* [[Locally linear embedding]]\n* [[Isometry (mathematics) (disambiguation)|Isometry (mathematics)]]\n* [[Local Tangent Space Alignment]]\n* [[Riemannian manifold]]\n* [[Energy minimization]]\n\n==References==\n*[http://repository.upenn.edu/cgi/viewcontent.cgi?article=1000&context=cis_papers Unsupervised learning of image manifolds by semidefinite programming] K. Q. Weinberger and L. K. Saul (2004). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-04), Washington D.C.\n*[http://www.springerlink.com/content/t21q747q278qx4x1/ Unsupervised learning of image manifolds by semidefinite programming] K. Q. Weinberger and L. K. Saul (2005), International Journal of Computer Vision - In Special Issue: Computer Vision and Pattern Recognition-CVPR 2005 Guest Editor(s): [[Aaron Bobick]], [[Rama Chellappa]], [[Larry S. Davis|Larry Davis]], pages 77–90, Volume 70, Number 1, [[Springer Netherlands]]\n*[http://citeseer.ist.psu.edu/170127.html The geometry of graphs and some of its algorithmic applications], [[Nathan Linial]], [[Eran London]], [[Yuri Rabinovich]], [[IEEE]] Symposium on Foundations of Computer Science.\n\n==External links==\n*[http://www.cse.wustl.edu/~kilian/code/code.html MVU Matlab code online]\n\n[[Category:Computational statistics]]\n[[Category:Dimension reduction]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Sequential linear-quadratic programming",
      "url": "https://en.wikipedia.org/wiki/Sequential_linear-quadratic_programming",
      "text": "{{refimprove|date=November 2017}}\n\n'''Sequential linear-quadratic programming''' ('''SLQP''') is an [[iterative method]] for [[nonlinear programming|nonlinear optimization problems]] where [[objective function]] and constraints are twice [[continuously differentiable]]. Similarly to [[sequential quadratic programming]] (SQP), SLQP proceeds by solving a sequence of optimization subproblems. The difference between the two approaches is that:\n\n* in SQP, each subproblem is a [[quadratic program]], with a quadratic model of the objective subject to a linearization of the constraints\n* in SLQP, two subproblems are solved at each step: a [[linear program]] (LP) used to determine an [[active set]], followed by an equality-constrained quadratic program (EQP) used to compute the total step\n\nThis decomposition makes SLQP suitable to large-scale optimization problems, for which efficient LP and EQP solvers are available, these problems being easier to scale than full-fledge quadratic programs.\n\n==Algorithm basics==\nConsider a [[nonlinear programming]] problem of the form:\n\n:<math>\\begin{array}{rl}\n\\min\\limits_{x} & f(x) \\\\\n\\mbox{s.t.} & b(x) \\ge 0 \\\\\n  & c(x) = 0.\n\\end{array}</math>\n\nThe Lagrangian for this problem is<ref>{{ cite book | year=2006|url=http://www.ece.northwestern.edu/~nocedal/book/num-opt.html| title= Numerical Optimization| publisher=Springer.|isbn=0-387-30303-0| author=Jorge Nocedal and Stephen J. Wright}}</ref>\n:<math>\\mathcal{L}(x,\\lambda,\\sigma) = f(x) - \\lambda^T b(x) - \\sigma^T c(x),</math>\nwhere <math>\\lambda</math> and <math>\\sigma</math> are [[Lagrange multipliers]]. \n\n=== LP phase ===\n\nIn the LP phase of SLQP, the following linear program is solved:\n:<math>\\begin{array}{rl} \n\\min\\limits_{d} & f(x_k) + \\nabla f(x_k)^Td\\\\\n\\mathrm{s.t.} & b(x_k) + \\nabla b(x_k)^Td \\ge 0 \\\\\n  & c(x_k) + \\nabla c(x_k)^T d = 0. \\end{array}</math>\n\nLet <math>{\\cal A}_k</math> denote the ''active set'' at the optimum <math>d^*_{\\text{LP}}</math> of this problem, that is to say, the set of constraints that are equal to zero at <math>d^*_{\\text{LP}}</math>. Denote by <math>b_{{\\cal A}_k}</math> and <math>c_{{\\cal A}_k}</math> the sub-vectors of <math>b</math> and <math>c</math> corresponding to elements of <math>{\\cal A}_k</math>.\n\n=== EQP phase ===\n\nIn the EQP phase of SLQP, the search direction <math>d_k</math> of the step is obtained by solving the following quadratic program:\n:<math>\\begin{array}{rl} \n\\min\\limits_{d} & f(x_k) + \\nabla f(x_k)^Td + \\tfrac{1}{2} d^T \\nabla_{xx}^2 \\mathcal{L}(x_k,\\lambda_k,\\sigma_k) d\\\\\n\\mathrm{s.t.} & b_{{\\cal A}_k}(x_k) + \\nabla b_{{\\cal A}_k}(x_k)^Td = 0 \\\\\n  & c_{{\\cal A}_k}(x_k) + \\nabla c_{{\\cal A}_k}(x_k)^T d = 0. \\end{array}</math>\n\nNote that the term <math>f(x_k)</math> in the objective functions above may be left out for the minimization problems, since it is constant.\n\n==See also==\n* [[Newton's method]]\n* [[Secant method]]\n* [[Sequential linear programming]]\n* [[Sequential quadratic programming]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{ cite book | year=2006|url=http://www.ece.northwestern.edu/~nocedal/book/num-opt.html| title= Numerical Optimization| publisher=Springer.|isbn=0-387-30303-0| author=Jorge Nocedal and Stephen J. Wright}}\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{Mathapplied-stub}}"
    },
    {
      "title": "Sequential minimal optimization",
      "url": "https://en.wikipedia.org/wiki/Sequential_minimal_optimization",
      "text": "{{Infobox Algorithm\n|image=\n|class=[[Optimization algorithm]] for training support vector machines\n|data=\n|time=O(''n''³)\n|space=\n}}\n'''Sequential minimal optimization''' ('''SMO''') is an algorithm for solving the [[quadratic programming]] (QP) problem that arises during the training of [[support-vector machine]]s (SVM). It was invented by [[John Platt (computer scientist)|John Platt]] in 1998 at [[Microsoft Research]].<ref name = \"Platt\">{{Cite paper\n | last = Platt | first = John\n | year = 1998\n | title = Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines\n | citeseerx = 10.1.1.43.4376\n | url = https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf\n}}</ref> SMO is widely used for training support vector machines and is implemented by the popular [[LIBSVM]] tool.<ref>{{cite journal\n|last1=Chang |first1=Chih-Chung\n|last2=Lin |first2=Chih-Jen\n|title=LIBSVM: A library for support vector machines\n|journal=ACM Transactions on Intelligent Systems and Technology\n|volume=2 |issue=3 |year=2011\n}}</ref><ref>{{cite web |first=Luca |last=Zanni |date=2006 |url=http://jmlr.csail.mit.edu/papers/volume7/zanni06a/zanni06a.pdf |title=Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems }}</ref> The publication of the SMO algorithm in 1998 has generated a lot of excitement in the SVM community, as previously available methods for SVM training were much more complex and required expensive third-party QP solvers.<ref>{{cite thesis\n | last = Rifkin | first = Ryan\n | year = 2002\n | hdl=1721.1/17549\n | title = Everything Old is New Again: a Fresh Look at Historical Approaches in Machine Learning\n | type = Ph.D. Thesis |publisher=Massachusetts Institute of Technology\n | pages = 18\n}}</ref>\n\n== Optimization problem ==\n{{main|Support vector machine}}\nConsider a [[binary classification]] problem with a dataset (''x''<sub>1</sub>, ''y''<sub>1</sub>), ..., (''x''<sub>''n''</sub>, ''y''<sub>''n''</sub>), where ''x''<sub>''i''</sub> is an input vector and {{nobr|''y''<sub>''i''</sub> ∈ {-1, +1} }} is a binary label corresponding to it. A soft-margin [[support vector machine]] is trained by solving a quadratic programming problem, which is expressed in the [[Dual problem|dual form]] as follows:\n\n:<math>\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac12 \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j K(x_i, x_j) \\alpha_i \\alpha_j,</math>\n:subject to:\n:<math>0 \\leq \\alpha_i \\leq C, \\quad \\mbox{ for } i=1, 2, \\ldots, n,</math>\n:<math>\\sum_{i=1}^n y_i \\alpha_i = 0</math>\n\nwhere ''C'' is an SVM hyperparameter and ''K''(''x''<sub>''i''</sub>, ''x''<sub>''j''</sub>) is the [[kernel function]], both supplied by the user; and the variables <math>\\alpha_i</math> are [[Lagrange multiplier]]s.\n\n== Algorithm ==\nSMO is an iterative algorithm for solving the optimization problem described above. SMO breaks this problem into a series of smallest possible sub-problems, which are then solved analytically. Because of the linear equality constraint involving the Lagrange multipliers <math>\\alpha_i</math>, the smallest possible problem involves two such multipliers. Then, for any two multipliers <math>\\alpha_1</math> and <math>\\alpha_2</math>, the constraints are reduced to:\n\n:<math>0 \\leq \\alpha_1, \\alpha_2 \\leq C,</math>\n:<math>y_1 \\alpha_1 + y_2 \\alpha_2 = k,</math>\n\nand this reduced problem can be solved analytically: one needs to find a minimum of a one-dimensional quadratic function. <math>k</math> is the negative of the sum over the rest of terms in the equality constraint, which is fixed in each iteration.\n\nThe algorithm proceeds as follows:\n\n# Find a Lagrange multiplier <math>\\alpha_1</math> that violates the [[Karush–Kuhn–Tucker conditions|Karush–Kuhn–Tucker (KKT) conditions]] for the optimization problem.\n# Pick a second multiplier <math>\\alpha_2</math> and optimize the pair <math>(\\alpha_1,\\alpha_2)</math>.\n# Repeat steps 1 and 2 until convergence.\n\nWhen all the Lagrange multipliers satisfy the KKT conditions (within a user-defined tolerance), the problem has been solved. Although this algorithm is guaranteed to converge, heuristics are used to choose the pair of multipliers so as to accelerate the rate of convergence. This is critical  for large data sets since there are <math>n(n-1)/2</math> possible choices for <math>\\alpha_i</math> and <math>\\alpha_j</math>.\n\n== Related Work ==\nThe first approach to splitting large SVM learning problems into a series of smaller optimization tasks was proposed by [[Bernhard E Boser]], [[Isabelle M Guyon]], [[Vladimir Vapnik]].<ref name=\"ReferenceA\">{{Cite book | doi = 10.1145/130385.130401| chapter = A training algorithm for optimal margin classifiers| title = Proceedings of the fifth annual workshop on Computational learning theory  - COLT '92| pages = 144| year = 1992| last1 = Boser | first1 = B. E. | last2 = Guyon | first2 = I. M. | last3 = Vapnik | first3 = V. N. | isbn = 978-0897914970| citeseerx = 10.1.1.21.3818}}</ref> It is known as the \"chunking algorithm\". The algorithm starts with a random subset of the data, solves this problem, and iteratively adds examples which violate the optimality conditions. One disadvantage of this algorithm is that it is necessary to solve QP-problems scaling with the number of SVs. On real world sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm.<ref name = \"Platt\"/>\n\nIn 1997, [[E. Osuna]], [[R. Freund]], and [[F. Girosi]] proved a theorem which suggests a whole new set of QP algorithms for SVMs.<ref>{{Cite book | doi = 10.1109/NNSP.1997.622408| chapter = An improved training algorithm for support vector machines| title = Neural Networks for Signal Processing [1997] VII. Proceedings of the 1997 IEEE Workshop| pages = 276–285| year = 1997| last1 = Osuna | first1 = E. | last2 = Freund | first2 = R. | last3 = Girosi | first3 = F. | isbn = 978-0-7803-4256-9| citeseerx = 10.1.1.392.7405}}</ref>  By the virtue of this theorem a large QP problem can be broken down into a series of smaller QP sub-problems. A sequence of QP sub-problems that always add at least one violator of the [[Karush–Kuhn–Tucker conditions|Karush–Kuhn–Tucker (KKT) conditions]] is guaranteed to converge. The chunking algorithm obeys the conditions of the theorem, and hence will converge.<ref name = \"Platt\"/> The SMO algorithm can be considered a special case of the Osuna algorithm, where the size of the optimization is two and both Lagrange multipliers are replaced at every step with new multipliers that are chosen via good heuristics.<ref name = \"Platt\"/>\n\nThe SMO algorithm is closely related to a family of optimization algorithms called [[Bregman method]]s  or row-action methods. These methods solve convex programming problems with linear constraints. They are iterative methods where each step projects the current primal point onto each constraint.<ref name = \"Platt\"/>\n\n== See also ==\n* [[Kernel perceptron]]\n\n== References ==\n{{reflist|30em}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Support vector machines]]"
    },
    {
      "title": "Sequential quadratic programming",
      "url": "https://en.wikipedia.org/wiki/Sequential_quadratic_programming",
      "text": "{{Context|date=October 2009}}\n'''Sequential quadratic programming''' ('''SQP''') is an [[iterative method]] for [[nonlinear programming|constrained nonlinear optimization]]. SQP methods are used on [[mathematics|mathematical]] problems for which the [[objective function]] and the constraints are twice [[continuously differentiable]].\n\nSQP methods solve a sequence of optimization subproblems, each of which optimizes a quadratic model of the objective subject to a linearization of the constraints. If the problem is unconstrained, then the method reduces to [[Newton's method]] for finding a point where the gradient of the objective vanishes.  If the problem has only equality constraints, then the method is equivalent to applying [[Newton's method]] to the first-order optimality conditions, or [[Karush–Kuhn–Tucker conditions]], of the problem.\n\n==Algorithm basics==\nConsider a [[nonlinear programming]] problem of the form:\n\n:<math>\\begin{array}{rl}\n\\min\\limits_{x} & f(x) \\\\\n\\mbox{s.t.} & b(x) \\ge 0 \\\\\n  & c(x) = 0.\n\\end{array}</math>\n\nThe Lagrangian for this problem is<ref>{{ cite book | year=2006|url=http://www.ece.northwestern.edu/~nocedal/book/num-opt.html| title= Numerical Optimization| publisher=Springer.|isbn=978-0-387-30303-1| author=Jorge Nocedal and Stephen J. Wright}}</ref>\n:<math>\\mathcal{L}(x,\\lambda,\\sigma) = f(x) - \\lambda^T b(x) - \\sigma^T c(x),</math>\n\nwhere <math>\\lambda</math> and <math>\\sigma</math> are [[Lagrange multipliers]].  At an iterate <math>x_k</math>, a basic sequential quadratic programming algorithm defines an appropriate search direction <math>d_k</math> as a solution to the [[quadratic programming]] subproblem\n\n:<math>\\begin{array}{rl} \\min\\limits_{d} & f(x_k) + \\nabla f(x_k)^Td + \\tfrac{1}{2} d^T \\nabla_{xx}^2 \\mathcal{L}(x_k,\\lambda_k,\\sigma_k) d \\\\\n\\mathrm{s.t.} & b(x_k) + \\nabla b(x_k)^Td \\ge 0 \\\\\n  & c(x_k) + \\nabla c(x_k)^T d = 0. \\end{array}</math>\n\nNote that the term <math>f(x_k)</math> in the expression above may be left out for the minimization problem, since it is constant.\n\n==Alternative approaches==\n* [[Sequential linear programming]]\n* [[Sequential linear-quadratic programming]]\n* [[Augmented Lagrangian method]]\n\n==Implementations==\nSQP methods have been implemented such well known numerical environments as [[MATLAB]] and [[GNU Octave]]. There also exist numerous software libraries, including open source\n* [[SciPy]] (de facto standard for scientific Python) has scipy.optimize.minimize(method=’SLSQP’) solver.\n* [https://nlopt.readthedocs.io/en/latest/ NLopt] (C/C++ implementation, with numerous interfaces including Python, R, MATLAB/Octave), implemented by Dieter Kraft as part of a package for optimal control, and modified by S. G. Johnson.<ref name=\"Kraft\">{{cite journal |last1=Kraft |first1=Dieter |title=Algorithm 733: TOMP–Fortran modules for optimal control calculations |journal=Transactions on Mathematical Software |date=Sep 1994 |volume=20 |issue=3 |pages=262–281 |doi=10.1145/192115.192124 |url=https://github.com/scipy/scipy/blob/master/scipy/optimize/slsqp/slsqp_optmz.f |accessdate=1 February 2019|citeseerx=10.1.1.512.2567 }}</ref><ref>{{cite document |last1=Kraft |first1=Dieter |title=A software package for sequential quadratic programming |work=Technical Report DFVLR-FB 88-28 |publisher=Institut für Dynamik der Flugsysteme |location=Oberpfaffenhofen |date=July 1988 |url=https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp |accessdate=1 February 2019 }}</ref>\n* [[LabVIEW]]\n* [[KNITRO]]<ref>[https://www.artelys.com/tools/knitro_doc/2_userGuide/algorithms.html KNITRO User Guide: Algorithms]</ref> (C, C++, C#, Java, Python, Fortran)\n* [[NPSOL]] (Fortran)\n* [[SNOPT]] (Fortran)\n* [[NLPQL]] (Fortran)\n* [[MATLAB]]\n[http://www.numericalmethod.com/javadoc/suanshu/com/numericalmethod/suanshu/optimization/multivariate/constrained/convex/sdp/pathfollowing/package-frame.html SuanShu] (Java)\n\n==See also==\n* [[:fr:Algorithme de Josephy-Newton|Josephy-Newton algorithm]]\n* [[Newton's method]]\n* [[Secant method]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite book|last1=Bonnans|first1=J.&nbsp;Frédéric|last2=Gilbert|first2=J.&nbsp;Charles|last3=Lemaréchal|first3=Claude|last4=Sagastizábal|first4=Claudia&nbsp;A.|author4-link= Claudia Sagastizábal |title=Numerical optimization: Theoretical and practical aspects|url=https://www.springer.com/mathematics/applications/book/978-3-540-35445-1|edition=Second revised ed. of  translation of 1997 <!-- ''Optimisation numérique: Aspects théoriques et pratiques'' --> French| series=Universitext|publisher=Springer-Verlag|location=Berlin|year=2006|pages=xiv+490|isbn=978-3-540-35445-1|doi=10.1007/978-3-540-35447-5|mr=2265882|authorlink3=Claude Lemaréchal}}\n* {{ cite book | year=2006|url=http://www.ece.northwestern.edu/~nocedal/book/num-opt.html| title= Numerical Optimization| publisher=Springer.|isbn=978-0-387-30303-1| author=Jorge Nocedal and Stephen J. Wright}}\n\n==External links==\n* [http://www.neos-guide.org/content/sequential-quadratic-programming  Sequential Quadratic Programming at NEOS guide]\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{Mathapplied-stub}}"
    },
    {
      "title": "Simplex algorithm",
      "url": "https://en.wikipedia.org/wiki/Simplex_algorithm",
      "text": "{{about|the linear programming algorithm|the non-linear optimization heuristic|Nelder–Mead method}}\n<!-- {{Context|date=March 2012}} -->\nIn [[optimization (mathematics)|mathematical optimization]], [[George Dantzig|Dantzig]]'s '''simplex algorithm''' (or '''simplex method''') is a popular [[algorithm]] for [[linear programming]].<ref name=\"Murty\">{{cite book|last=Murty|first=Katta G.|authorlink=Katta G. Murty|title=Linear programming|publisher=John Wiley & Sons Inc.1, 2000 |url=http://www.computer.org/csdl/mags/cs/2000/01/c1022.html}}</ref>\n\nThe name of the algorithm is derived from the concept of a [[simplex]] and was suggested by [[Theodore Motzkin|T. S. Motzkin]].<ref name=\"Murty22\" >{{harvtxt|Murty|1983|loc=Comment 2.2}}</ref> Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial ''[[cone (geometry)|cone]]s'', and these become proper simplices with an additional constraint.<ref name=\"Murty39\">{{harvtxt|Murty|1983|loc=Note 3.9}}</ref><ref name=\"StoneTovey\">{{cite journal|last1=Stone|first1=Richard E.|last2=Tovey|first2=Craig A.|title=The simplex and projective scaling algorithms as iteratively reweighted least squares methods|journal=SIAM Review|volume=33|year=1991|issue=2|pages=220–237\n|mr=1124362|jstor=2031142|doi=10.1137/1033049}}</ref><ref>{{cite journal|last1=Stone|first1=Richard E.|last2=Tovey|first2=Craig A.|title=Erratum: The simplex and projective scaling algorithms as iteratively reweighted least squares methods|journal=SIAM Review|volume=33|year=1991|issue=3|pages=461|mr=1124362|doi=10.1137/1033100|jstor=2031443|ref=harv}}</ref><ref  name=\"Strang\">{{cite journal|last=Strang|first=Gilbert|authorlink=Gilbert Strang|title=Karmarkar's algorithm and its place in applied mathematics|journal=[[The Mathematical Intelligencer]]|date=1 June 1987|issn=0343-6993|pages=4–10|volume=9|doi=10.1007/BF03025891|mr=883185|ref=harv|issue=2}}</ref> The simplicial cones in question are the corners (i.e., the neighborhoods of the vertices) of a geometric object called a [[polytope]]. The shape of this polytope is defined by the [[System of linear inequalities|constraints]] applied to the objective function.\n\n== Overview ==\n{{further|Linear programming}}\n[[Image:Simplex-description-en.svg|thumb|240px|A [[system of linear inequalities]] defines a [[polytope]] as a feasible region. The simplex algorithm begins at a starting [[vertex (geometry)|vertex]] and moves along the edges of the polytope until it reaches the vertex \nof the optimal solution.]]\n\n[[Image:Simplex-method-3-dimensions.png|thumb|240px|Polyhedron of simplex algorithm in 3D]]\n\nThe simplex algorithm operates on linear programs in the [[canonical form]]\n\n:maximize <math display=\"inline\">\\mathbf{c^T} \\mathbf{x}</math>\n:subject to <math>A\\mathbf{x} \\leq \\mathbf{b}</math> and <math>\\mathbf{x} \\ge 0</math>\n\nwith <math> \\mathbf{x} = (x_1,\\, \\dots,\\, x_n)</math> the variables of the problem, <math>\\mathbf{c} = (c_1,\\, \\dots,\\, c_n)</math> the coefficients of the objective function, <math>A</math> a ''p×n'' matrix, and <math> \\mathbf{b} = (b_1,\\, \\dots,\\, b_p)</math> nonnegative constants (<math>\\forall j, b_j \\geq 0\\ </math>). There is a straightforward process to convert any linear program into one in standard form, so using this form of linear programs results in no loss of generality.\n\nIn geometric terms, the [[feasible region]] defined by all values of <math>\\mathbf{x}</math> such that <math display=\"inline\">A\\mathbf{x} \\le \\mathbf{b}</math> and <math>\\forall i, x_i \\ge 0 </math> is a (possibly unbounded) [[convex polytope]]. An extreme point or vertex of this polytope is known as ''[[basic feasible solution]]'' (BFS).\n\nIt can be shown that for a linear program in standard form, if the objective function has a maximum value on the feasible region, then it has this value on (at least) one of the extreme points.<ref>{{harvtxt|Murty|1983|loc=Theorem 3.3}}</ref> This in itself reduces the problem to a finite computation since there is a finite number of extreme points, but the number of extreme points is unmanageably large for all but the smallest linear programs.<ref>{{harvtxt|Murty|1983|loc=Section 3.13|p=143}}</ref>\n \nIt can also be shown that, if an extreme point is not a maximum point of the objective function, then there is an edge containing the point so that the objective function is strictly increasing on the edge moving away from the point.<ref name=\"Murty137\">{{harvtxt|Murty|1983|loc=Section 3.8|p=137}}</ref> If the edge is finite, then the edge connects to another extreme point where the objective function has a greater value, otherwise the objective function is unbounded above on the edge and the linear program has no solution. The simplex algorithm applies this insight by walking along edges of the polytope to extreme points with greater and greater objective values. This continues until the maximum value is reached, or an unbounded edge is visited (concluding that the problem has no solution). The algorithm always terminates because the number of vertices in the polytope is finite; moreover since we jump between vertices always in the same direction (that of the objective function), we hope that the number of vertices visited will be small.<ref name=\"Murty137\"/>\n\nThe solution of a linear program is accomplished in two steps. In the first step, known as Phase I, a starting extreme point is found. Depending on the nature of the program this may be trivial, but in general it can be solved by applying the simplex algorithm to a modified version of the original program. The possible results of Phase I are either that a basic feasible solution is found or that the feasible region is empty. In the latter case the linear program is called ''infeasible''. In the second step, Phase II, the simplex algorithm is applied using the basic feasible solution found in Phase I as a starting point. The possible results from Phase II are either an optimum basic feasible solution or an infinite edge on which the objective function is unbounded above.<ref name=\"DantzigThapa1\">[[George B. Dantzig]] and Mukund N. Thapa. 1997. ''Linear programming 1: Introduction''. Springer-Verlag.</ref><ref name=\"NeringTucker\"/><ref name=\"Vanderbei\">Robert J. Vanderbei, [http://www.princeton.edu/~rvdb/LPbook/ ''Linear Programming: Foundations and Extensions''], 3rd ed., International Series in Operations Research & Management Science, Vol. 114, Springer Verlag, 2008. {{isbn|978-0-387-74387-5}}. <!-- (An on-line second edition was formerly available. Vanderbei's site still contains extensive materials.) --></ref>\n\n== History ==\nGeorge Dantzig worked on planning methods for the US Army Air Force during World War II using a desk calculator. During 1946 his colleague challenged him to mechanize the planning process to distract him from taking another job. Dantzig formulated the problem as linear inequalities inspired by the work of [[Wassily Leontief]], however, at that time he didn't include an objective as part of his formulation. Without an objective, a vast number of solutions can be feasible, and therefore to find the \"best\" feasible solution, military-specified \"ground rules\" must be used that describe how goals can be achieved as opposed to specifying a goal itself. Dantzig's core insight was to realize that most such ground rules can be translated into a linear objective function that needs to be maximized.<ref>{{Cite journal|url = http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&doc=GetTRDoc.pdf&AD=ADA112060|title = Reminiscences about the origins of linear programming|date = April 1982|journal = Operations Research Letters|doi = 10.1016/0167-6377(82)90043-8|pmid = |access-date = |volume = 1|issue = 2 |pages=43–48|last1 = Dantzig|first1 = George B.}}</ref> Development of the simplex method was evolutionary and happened over a period of about a year.<ref>{{Cite journal|url = http://www.phpsimplex.com/en/Dantzig_interview.htm|title = An Interview with George B. Dantzig: The Father of Linear Programming|last = Albers and Reid|date = 1986|journal = College Mathematics Journal|doi = |pmid = |access-date = |pages = 292–314}}</ref>\n\nAfter Dantzig included an objective function as part of his formulation during mid-1947, the problem was mathematically more tractable. Dantzig realized that one of the unsolved problems that [[George Dantzig#Mathematical statistics|he had mistaken]] as homework in his professor [[Jerzy Neyman]]'s class (and actually later solved), was applicable to finding an algorithm for linear programs. This problem involved finding the existence of [[Lagrange multipliers on Banach spaces|Lagrange multipliers]] for general linear programs over a continuum of variables, each bounded between zero and one, and satisfying linear constraints expressed in the form of [[Lebesgue integral]]s. Dantzig later published his \"homework\" as a thesis to earn his doctorate. The column geometry used in this thesis gave Dantzig insight that made him believe that the Simplex method would be very efficient.<ref>{{Cite book|url = http://www.dtic.mil/dtic/tr/fulltext/u2/a182708.pdf|title = Origins of the simplex method|last = Dantzig|first = George|date = May 1987|journal = A History of Scientific Computing|doi = 10.1145/87252.88081|pmid = |access-date = |isbn = 978-0-201-50814-7|doi-broken-date = 2019-02-08}}</ref>\n\n==Standard form==\nThe transformation of a linear program to one in standard form may be accomplished as follows.<ref>{{harvtxt|Murty|1983|loc=Section 2.2}}</ref> First, for each variable with a lower bound other than 0, a new variable is introduced representing the difference between the variable and bound. The original variable can then be eliminated by substitution. For example, given the constraint\n:<math>x_1 \\ge 5</math>\n\na new variable, <math>y_1</math>, is introduced with \n:<math> \\begin{align} y_1 = x_1 - 5\\\\x_1 = y_1 + 5 \\end{align}</math>\n\nThe second equation may be used to eliminate <math>x_1</math> from the linear program. In this way, all lower bound constraints may be changed to non-negativity restrictions.\n\nSecond, for each remaining inequality constraint, a new variable, called a ''slack variable'', is introduced to change the constraint to an equality constraint. This variable represents the difference between the two sides of the inequality and is assumed to be non-negative. For example, the inequalities\n:<math> \\begin{align}\n  x_2 + 2x_3 &\\le 3\\\\\n -x_4 + 3x_5 &\\ge 2\n\\end{align}</math>\n\nare replaced with\n:<math> \\begin{align}\n  x_2 + 2x_3 + s_1 &= 3\\\\\n -x_4 + 3x_5 - s_2 &= 2\\\\\n  s_1,\\, s_2 &\\ge 0\n\\end{align}</math>\n\nIt is much easier to perform algebraic manipulation on inequalities in this form. In inequalities where ≥ appears such as the second one, some authors refer to the variable introduced as a {{anchor|Surplus variable}}''surplus variable''.\n\nThird, each unrestricted variable is eliminated from the linear program. This can be done in two ways, one is by solving for the variable in one of the equations in which it appears and then eliminating the variable by substitution. The other is to replace the variable with the difference of two restricted variables. For example, if <math>z_1</math> is unrestricted then write\n:<math>\\begin{align}\n  &z_1 = z_1^+ - z_1^-\\\\\n  &z_1^+,\\, z_1^- \\ge 0\n\\end{align}</math>\n\nThe equation may be used to eliminate <math>z_1</math> from the linear program.\n\nWhen this process is complete the feasible region will be in the form\n:<math>\\mathbf{A}\\mathbf{x} = \\mathbf{b},\\, \\forall i \\ x_i \\ge 0</math>\n\nIt is also useful to assume that the rank of <math>\\mathbf{A}</math> is the number of rows. This results in no loss of generality since otherwise either the system <math>\\mathbf{A}\\mathbf{x} = \\mathbf{b}</math> has redundant equations which can be dropped, or the system is inconsistent and the linear program has no solution.<ref>{{harvtxt|Murty|1983|p=173}}</ref>\n\n==Simplex tableau==\nA linear program in standard form can be represented as a ''tableau'' of the form \n:<math>\n  \\begin{bmatrix}\n    1 & -\\mathbf{c}^T & 0 \\\\\n    0 & \\mathbf{A} & \\mathbf{b}\n  \\end{bmatrix}\n</math>\n\nThe first row defines the objective function and the remaining rows specify the constraints. The zero in the first column represents the zero vector of the same dimension as vector ''b''.(Note, different authors use different conventions as to the exact layout.) If the columns of A can be rearranged so that it contains the [[identity matrix]] of order ''p'' (the number of rows in A) then the tableau is said to be in ''canonical form''.<ref>{{harvtxt|Murty|1983|loc=section 2.3.2}}</ref> The variables corresponding to the columns of the identity matrix are called ''basic variables'' while the remaining variables are called ''nonbasic'' or ''free variables''. If the values of the nonbasic variables are set to 0, then the values of the basic variables are easily obtained as entries in ''b'' and this solution is a basic feasible solution. The algebraic interpretation here is that the coefficients of the linear equation represented by each row are either <math>0</math>, <math>1</math>, or some other number. Each row will have <math>1</math> column with value <math>1</math>, <math>p-1</math> columns with coefficients <math>0</math>, and the remaining columns with some other coefficients (these other variables represent our non-basic variables). By setting the values of the non-basic variables we ensure in each row that the value of the variable represented by a <math>1</math> in its column is equal to the <math>b</math> value at that row.\n\nConversely, given a basic feasible solution, the columns corresponding to the nonzero variables can be expanded to a nonsingular matrix. If the corresponding tableau is multiplied by the inverse of this matrix then the result is a tableau in canonical form.<ref>{{harvtxt|Murty|1983|loc=section 3.12}}</ref>\n\nLet \n:<math>\n  \\begin{bmatrix}\n    1 & -\\mathbf{c}^T_B & -\\mathbf{c}^T_D & 0 \\\\\n    0 & I & \\mathbf{D} & \\mathbf{b}\n  \\end{bmatrix}\n</math>\n\nbe a tableau in canonical form. Additional [[Elementary matrix#Row-addition transformations|row-addition transformations]] can be applied to remove the coefficients {{SubSup|'''c'''|''B''|T|s=0}} from the objective function. This process is called ''pricing out'' and results in a canonical tableau\n:<math>\n  \\begin{bmatrix}\n    1 & 0 & -\\bar{\\mathbf{c}}^T_D & z_B \\\\\n    0 & I & \\mathbf{D} & \\mathbf{b}\n  \\end{bmatrix}\n</math>\n\nwhere ''z''<sub>''B''</sub> is the value of the objective function at the corresponding basic feasible solution. The updated coefficients, also known as ''relative cost coefficients'', are the rates of change of the objective function with respect to the nonbasic variables.<ref name=\"NeringTucker\" >\nEvar D. Nering and [[Albert W. Tucker]], 1993, ''Linear Programs and Related Problems'', Academic Press. (elementary<!-- but profound -->)</ref>\n\n==Pivot operations==\nThe geometrical operation of moving from a basic feasible solution to an adjacent basic feasible solution is implemented as a ''pivot operation''. First, a nonzero ''pivot element'' is selected in a nonbasic column. The row containing this element is [[Elementary matrix#Row-multiplying transformations|multiplied]] by its reciprocal to change this element to 1, and then multiples of the row are added to the other rows to change the other entries in the column to 0. The result is that, if the pivot element is in row ''r'', then the column becomes the ''r''-th column of the identity matrix. The variable for this column is now a basic variable, replacing the variable which corresponded to the ''r''-th column of the identity matrix before the operation. In effect, the variable corresponding to the pivot column enters the set of basic variables and is called the ''entering variable'', and the variable being replaced leaves the set of basic variables and is called the ''leaving variable''. The tableau is still in canonical form but with the set of basic variables changed by one element.<ref name=\"DantzigThapa1\"/><ref name=\"NeringTucker\"/>\n\n==Algorithm==\nLet a linear program be given by a canonical tableau. The simplex algorithm proceeds by performing successive pivot operations each of which give an improved basic feasible solution; the choice of pivot element at each step is largely determined by the requirement that this pivot improves the solution.\n\n===Entering variable selection===\nSince the entering variable will, in general, increase from 0 to a positive number, the value of the objective function will decrease if the derivative of the objective function with respect to this variable is negative. Equivalently, the value of the objective function is decreased if the pivot column is selected so that the corresponding entry in the objective row of the tableau is positive.\n\nIf there is more than one column so that the entry in the objective row is positive then the choice of which one to add to the set of basic variables is somewhat arbitrary and several ''entering variable choice rules''<ref name=\"Murty66\">{{harvtxt|Murty|1983|p=66}}</ref> such as [[Devex algorithm]]<ref>Harris, Paula MJ. \"Pivot selection methods of the Devex LP code.\" Mathematical programming 5.1 (1973): 1–28</ref> have been developed.\n\nIf all the entries in the objective row are less than or equal to 0 then no choice of entering variable can be made and the solution is in fact optimal. It is easily seen to be optimal since the objective row now corresponds to an equation of the form \n:<math>z(\\mathbf{x})=z_B+\\text{nonnegative terms corresponding to nonbasic variables}</math>\n\nNote that by changing the entering variable choice rule so that it selects a column where the entry in the objective row is negative, the algorithm is changed so that it finds the maximum of the objective function rather than the minimum.\n\n===Leaving variable selection===\nOnce the pivot column has been selected, the choice of pivot row is largely determined by the requirement that the resulting solution be feasible. First, only positive entries in the pivot column are considered since this guarantees that the value of the entering variable will be nonnegative. If there are no positive entries in the pivot column then the entering variable can take any nonnegative value with the solution remaining feasible. In this case the objective function is unbounded below and there is no minimum.\n\nNext, the pivot row must be selected so that all the other basic variables remain positive. A calculation shows that this occurs when the resulting value of the entering variable is at a minimum. In other words, if the pivot column is ''c'', then the pivot row ''r'' is chosen so that\n:<math>b_r / a_{rc}\\,</math>\n\nis the minimum over all ''r'' so that ''a''<sub>''rc''</sub> > 0. This is called the ''minimum ratio test''.<ref name=\"Murty66\"/> If there is more than one row for which the minimum is achieved then a ''dropping variable choice rule''<ref>{{harvtxt|Murty|1983|p=67}}</ref> can be used to make the determination.\n\n=== Example ===\n{{see also|Revised simplex algorithm#Numerical example}}\nConsider the linear program\n:Minimize\n::<math>Z = -2 x - 3 y - 4 z\\,</math>\n:Subject to\n::<math>\\begin{align}\n  3 x + 2 y + z &\\le 10\\\\\n  2 x + 5 y + 3 z &\\le 15\\\\\n  x,\\,y,\\,z &\\ge 0\n\\end{align}</math>\n\nWith the addition of slack variables ''s'' and ''t'', this is represented by the canonical tableau\n:<math>\n  \\begin{bmatrix}\n    1 & 2 & 3 & 4 & 0 & 0 &  0 \\\\   \n    0 & 3 & 2 & 1 & 1 & 0 & 10 \\\\\n    0 & 2 & 5 & 3 & 0 & 1 & 15\n  \\end{bmatrix}\n</math>\n\nwhere columns 5 and 6 represent the basic variables ''s'' and ''t'' and the corresponding basic feasible solution is \n:<math>x=y=z=0,\\,s=10,\\,t=15.</math>\n\nColumns 2, 3, and 4 can be selected as pivot columns, for this example column 4 is selected. The values of ''z'' resulting from the choice of rows 2 and 3 as pivot rows are 10/1&nbsp;=&nbsp;10 and 15/3&nbsp;=&nbsp;5 respectively. Of these the minimum is 5, so row 3 must be the pivot row. Performing the pivot produces\n:<math>\n  \\begin{bmatrix}\n    3 & -2 & -11 & 0 & 0 & -4 & -60 \\\\   \n    0 &  7 &   1 & 0 & 3 & -1 &  15  \\\\\n    0 &  2 &   5 & 3 & 0 &  1 &  15\n  \\end{bmatrix}\n</math>\n\nNow columns 4 and 5 represent the basic variables ''z'' and ''s'' and the corresponding basic feasible solution is \n:<math>x=y=t=0,\\,z=5,\\,s=5.</math>\n\nFor the next step, there are no positive entries in the objective row and in fact\n:<math>Z = \\tfrac{-60+2x+11y+4t}{3} = -20 + \\tfrac{2x+11y+4t}{3}</math>\nso the minimum value of ''Z'' is&nbsp;&minus;20.\n\n==Finding an initial canonical tableau==\nIn general, a linear program will not be given in canonical form and an equivalent canonical tableau must be found before the simplex algorithm can start. This can be accomplished by the introduction of ''artificial variables''. Columns of the identity matrix are added as column vectors for these variables. If the b value for a constraint equation is negative, the equation is negated before adding the identity matrix columns.  This does not change the set of feasible solutions or the optimal solution, and it ensures that the slack variables will constitute an initial feasible solution.  The new tableau is in canonical form but it is not equivalent to the original problem. So a new objective function, equal to the sum of the artificial variables, is introduced and the simplex algorithm is applied to find the minimum; the modified linear program is called the ''Phase&nbsp;I'' problem.<ref>{{harvtxt|Murty|1983|p=60}}</ref>\n\nThe simplex algorithm applied to the Phase I problem must terminate with a minimum value for the new objective function since, being the sum of nonnegative variables, its value is bounded below by 0. If the minimum is 0 then the artificial variables can be eliminated from the resulting canonical tableau producing a canonical tableau equivalent to the original problem. The simplex algorithm can then be applied to find the solution; this step is called ''Phase&nbsp;II''. If the minimum is positive then there is no feasible solution for the Phase I problem where the artificial variables are all zero. This implies that the feasible region for the original problem is empty, and so the original problem has no solution.<ref name=\"DantzigThapa1\"/><ref name=\"NeringTucker\"/><ref name=\"Padberg\"/>\n\n===Example===\nConsider the linear program\n:Minimize\n::<math>Z = -2 x - 3 y - 4 z\\,</math>\n\n:Subject to\n::<math>\\begin{align}\n 3 x + 2 y + z &= 10\\\\\n 2 x + 5 y + 3 z &= 15\\\\\n x,\\, y,\\, z &\\ge 0\n\\end{align}</math>\n\nThis is represented by the (non-canonical) tableau\n:<math>\n  \\begin{bmatrix}\n    1 & 2 & 3 & 4 &  0 \\\\   \n    0 & 3 & 2 & 1 & 10 \\\\\n    0 & 2 & 5 & 3 & 15\n  \\end{bmatrix}\n</math>\n\nIntroduce artificial variables ''u'' and ''v'' and objective function ''W''&nbsp;=&nbsp;''u''&nbsp;+&nbsp;''v'', giving a new tableau\n:<math>\n  \\begin{bmatrix}\n    1 & 0 & 0 & 0 & 0 & -1 & -1 &  0 \\\\  \n    0 & 1 & 2 & 3 & 4 &  0 &  0 &  0 \\\\   \n    0 & 0 & 3 & 2 & 1 &  1 &  0 & 10 \\\\\n    0 & 0 & 2 & 5 & 3 &  0 &  1 & 15\n  \\end{bmatrix}\n</math>\n\nNote that the equation defining the original objective function is retained in anticipation of Phase II.\n\nBy construction, ''u'' and ''v'' are both non-basic variables since they are part of the initial identity matrix. However, the objective function ''W'' currently assumes that ''u'' and ''v'' are both ''0''. In order to adjust the objective function to be the correct value where ''u''&nbsp;=&nbsp;''10'' and ''v''&nbsp;=&nbsp;''15'', add the third and fourth rows to the first row giving\n:<math>\n  \\begin{bmatrix}\n    1 & 0 & 5 & 7 & 4 & 0 & 0 & 25 \\\\  \n    0 & 1 & 2 & 3 & 4 & 0 & 0 &  0 \\\\   \n    0 & 0 & 3 & 2 & 1 & 1 & 0 & 10 \\\\\n    0 & 0 & 2 & 5 & 3 & 0 & 1 & 15\n  \\end{bmatrix}\n</math>\n\nSelect column 5 as a pivot column, so the pivot row must be row 4, and the updated tableau is \n:<math>\n  \\begin{bmatrix}\n    3 & 0 &  7 &   1 & 0 & 0 & -4 &   15 \\\\  \n    0 & 3 & -2 & -11 & 0 & 0 & -4 & -60 \\\\   \n    0 & 0 &  7 &   1 & 0 & 3 & -1 &   15 \\\\\n    0 & 0 &  2 &   5 & 3 & 0 &  1 &   15\n  \\end{bmatrix}\n</math>\n\nNow select column 3 as a pivot column, for which row 3 must be the pivot row, to get\n:<math>\n  \\begin{bmatrix}\n    1 & 0 & 0 &              0 & 0 &            -1 &            -1  &               0 \\\\  \n    0 & 7 & 0 & -25 & 0 &  2 & -10 & -130 \\\\   \n    0 & 0 & 7 &   1 & 0 &  3 &  -1 &   15 \\\\\n    0 & 0 & 0 & 11 & 7 & -2 &   3 &   25\n  \\end{bmatrix}\n</math>\n\nThe artificial variables are now 0 and they may be dropped giving a canonical tableau equivalent to the original problem:\n:<math>\n  \\begin{bmatrix}\n    7 & 0 & -25 & 0 &  -130 \\\\   \n    0 & 7 &   1 & 0 &    15 \\\\\n    0 & 0 &  11 & 7 &    25 \n  \\end{bmatrix}\n</math>\n\nThis is, fortuitously, already optimal and the optimum value for the original linear program is&nbsp;−130/7.\n\n==Advanced topics==\n\n===Implementation===\n{{main|Revised simplex algorithm}}\nThe tableau form used above to describe the algorithm lends itself to an immediate implementation in which the tableau is maintained as a rectangular (''m''&nbsp;+&nbsp;1)-by-(''m''&nbsp;+&nbsp;''n''&nbsp;+&nbsp;1) array. It is straightforward to avoid storing the m explicit columns of the identity matrix that will occur within the tableau by virtue of '''B''' being a subset of the columns of ['''A''',&nbsp;'''I''']. This implementation is referred to as the \"''standard'' simplex algorithm\". The storage and computation overhead are such that the standard simplex method is a prohibitively expensive approach to solving large linear programming problems.\n\nIn each simplex iteration, the only data required are the first row of the tableau, the (pivotal) column of the tableau corresponding to the entering variable and the right-hand-side. The latter can be updated using the pivotal column and the first row of the tableau can be updated using the (pivotal) row corresponding to the leaving variable. Both the pivotal column and pivotal row may be computed directly using the solutions of linear systems of equations involving the matrix '''B''' and a matrix-vector product using '''A'''. These observations motivate the \"[[Revised simplex algorithm|''revised'' simplex algorithm]]\", for which implementations are distinguished by their invertible representation of&nbsp;'''B'''.<ref name=\"DantzigThapa2\" >\n[[George B. Dantzig]] and Mukund N. Thapa. 2003. ''Linear Programming 2: Theory and Extensions''. Springer-Verlag.</ref>\n\nIn large linear-programming problems '''A''' is typically a [[sparse matrix]] and, when the resulting sparsity of '''B''' is exploited when maintaining its invertible representation, the revised simplex algorithm is much more efficient than the standard simplex method. Commercial simplex solvers are based on the revised simplex algorithm.<ref name=\"Padberg\" >M. Padberg, ''Linear Optimization and Extensions'', Second Edition, Springer-Verlag, 1999.</ref><ref name=\"DantzigThapa2\"/><ref>Dmitris Alevras and Manfred W. Padberg, ''Linear Optimization and Extensions: Problems and Extensions'', Universitext, Springer-Verlag, 2001. (Problems from Padberg with solutions.)</ref><ref name=\"MarosMitra\" >{{cite book|last1=Maros|first1=István|last2=Mitra|first2=Gautam|chapter=Simplex algorithms|mr=1438309|title=Advances in linear and integer programming|pages=1–46|editor=J. E. Beasley|publisher=Oxford Science|year=1996}}</ref><ref>{{cite book|mr=1960274|last=Maros|first=István|title=Computational techniques of the simplex method|series=International Series in Operations Research & Management Science|volume=61|publisher=Kluwer Academic Publishers|location=Boston, MA|year=2003|pages=xx+325|isbn=978-1-4020-7332-8}}</ref>\n\n===Degeneracy: stalling and cycling===\nIf the values of all basic variables are strictly positive, then a pivot must result in an improvement in the objective value. When this is always the case no set of basic variables occurs twice and the simplex algorithm must terminate after a finite number of steps. Basic feasible solutions where at least one of the ''basic ''variables is zero are called ''degenerate'' and may result in pivots for which there is no improvement in the objective value. In this case there is no actual change in the solution but only a change in the set of basic variables. When several such pivots occur in succession, there is no improvement; in large industrial applications, degeneracy is common and such \"''stalling''\" is notable. \nWorse than stalling is the possibility the same set of basic variables occurs twice, in which case, the deterministic pivoting rules of the simplex algorithm will produce an infinite loop, or \"cycle\". While degeneracy is the rule in practice and stalling is common, cycling is rare in practice. A discussion of an example of practical cycling occurs in Padberg.<ref name=\"Padberg\"/> [[Bland's rule]]  prevents cycling and thus guarantees that the simplex algorithm always terminates.<ref name=\"Padberg\"/><ref name=\"Bland\">\n{{cite journal|title=New finite pivoting rules for the simplex method|first=Robert G.|last=Bland|journal=Mathematics of Operations Research|volume=2|issue=2|date=May 1977|pages=103–107|doi=10.1287/moor.2.2.103|jstor=3689647|mr=459599|ref=harv}}</ref><ref>{{harvtxt|Murty|1983|p=79}}</ref> Another pivoting algorithm, the [[criss-cross algorithm]] never cycles on linear programs.<ref>There are abstract optimization problems, called [[oriented matroid]] programs, on which Bland's rule cycles (incorrectly) while the [[criss-cross algorithm]] terminates correctly.</ref>\n\nHistory-based pivot rules such as [[Zadeh's rule]] and [[Cunningham's rule]] also try to circumvent the issue of stalling and cycling by keeping track how often particular variables are being used, and then favor such variables that have been used least often.\n\n===Efficiency===\nThe simplex method is remarkably efficient in practice and was a great improvement over earlier methods such as [[Fourier–Motzkin elimination]]. However, in 1972, [[Victor Klee|Klee]] and Minty<ref name=\"KleeMinty\">{{cite book|title=Inequalities III (Proceedings of the Third Symposium on Inequalities held at the University of California, Los Angeles, Calif., September 1–9, 1969, dedicated to the memory of Theodore S. Motzkin)|editor-first=Oved|editor-last=Shisha|publisher=Academic Press|location=New York-London|year=1972|mr=332165|last1=Klee|first1=Victor|authorlink1=Victor Klee|last2=Minty|first2= George J.|authorlink2=George J. Minty|chapter=How good is the simplex algorithm?|pages=159–175|ref=harv}}</ref> gave an example, the [[Klee–Minty cube]],  showing that the worst-case complexity of simplex method as formulated by Dantzig is [[exponential time]]. Since then, for almost every variation on the method, it has been shown that there is a family of linear programs for which it performs badly.  It is an open question if there is a variation with [[polynomial time]], although sub-exponential pivot rules are known.<ref>{{Citation\n | last = Hansen\n | first = Thomas\n | last2 = Zwick\n | first2 = Uri\n | author2-link = Uri Zwick\n | title = An Improved Version of the Random-Facet Pivoting Rule for the Simplex Algorithm\n | journal = Proceedings of the forty-seventh annual ACM symposium on Theory of Computing\n | pages = 209-218\n | year = 2015\n | doi = 10.1145/2746539.2746557\n | url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.697.2526&rep=rep1&type=pdf\n}}\n</ref>\n\nIn 2018, it was proved that a particular variant of the simplex method is [[NP-mighty]], i.e., it can be used to solve, with polynomial overhead, any problem in NP implicitly during the algorithm's execution.  Moreover, deciding whether a given variable ever enters the basis during the algorithm's execution on a given input, and determining the number of iterations needed for solving a given problem, are both [[NP-hardness|NP-hard]] problems.<ref>{{Cite journal|last=Disser|first=Yann|last2=Skutella|first2=Martin|date=2018-11-01|title=The Simplex Algorithm Is NP-Mighty|journal=ACM Trans. Algorithms|volume=15|issue=1|pages=5:1–5:19|doi=10.1145/3280847|issn=1549-6325|arxiv=1311.5935}}</ref> Computing the output of some other pivot rules was already known to be [[PSPACE-complete]]<ref>{{Citation | last = Adler | first = Ilan | last2 = Christos | first2 = Papadimitriou | author2-link = Christos Papadimitriou | last3 = Rubinstein | first3 = Aviad | title = On Simplex Pivoting Rules and Complexity Theory | journal = International Conference on Integer Programming and Combinatorial Optimization | volume = 17 | pages = 13-24 | year = 2014 | arxiv = 1404.3320 | doi = 10.1007/978-3-319-07557-0_2}}</ref><ref>{{Citation | last = Fearnly | first = John | last2 = Savani | first2 = Rahul | title = The Complexity of the Simplex Method | journal = Proceedings of the forty-seventh annual ACM symposium on Theory of computing | pages = 201-208 | year = 2015 | arxiv = 1404.0605 | doi = 10.1145/2746539.2746558}}</ref>.\n\nAnalyzing and quantifying the observation that the simplex algorithm is efficient in practice despite its exponential worst-case complexity has led to the development of other measures of complexity. The simplex algorithm has polynomial-time [[Best, worst and average case|average-case complexity]] under various [[probability distribution]]s, with the precise average-case performance of the simplex algorithm depending on the choice of a probability distribution for the [[random matrix|random matrices]].<ref name=\"Schrijver\" >[[Alexander Schrijver]], ''Theory of Linear and Integer Programming''. John Wiley & sons, 1998, {{isbn|0-471-98232-6}} (mathematical)</ref><ref name=\"Borgwardt\">The simplex algorithm takes on average ''D'' steps for a cube. {{harvtxt|Borgwardt|1987}}: {{cite book|last=Borgwardt|first=Karl-Heinz|title=The simplex method: A probabilistic analysis|series=Algorithms and Combinatorics (Study and Research Texts)|volume=1|publisher=Springer-Verlag|location=Berlin|year=1987|pages=xii+268|isbn=978-3-540-17096-9|mr=868467|ref=harv}}</ref> Another approach to studying \"[[porous set|typical phenomena]]\" uses [[Baire category theory]] from [[general topology]], and to show that (topologically) \"most\" matrices can be solved by the simplex algorithm in a polynomial number of steps.{{Citation needed|date=June 2019}} Another method to analyze the performance of the simplex algorithm studies the behavior of worst-case scenarios under small perturbation – are worst-case scenarios stable under a small change (in the sense of [[structural stability]]), or do they become tractable? Formally, this method uses fixed problems to which is added an amount of random noise, generalizing certain average-case models (\"[[smoothed complexity]]\").<ref>{{Cite book | last1=Spielman | first1=Daniel | last2=Teng | first2=Shang-Hua | author2-link=Shanghua Teng | title=Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing | publisher=ACM | isbn=978-1-58113-349-3 | doi=10.1145/380752.380813 | year=2001 | chapter=Smoothed analysis of algorithms: why the simplex algorithm usually takes polynomial time| pages=296–305 | arxiv=cs/0111050}}</ref>\n\n==Other algorithms==\nOther algorithms for solving linear-programming problems are described in the [[linear programming|linear-programming]] article. Another basis-exchange pivoting algorithm is the [[criss-cross algorithm]].<ref>{{cite journal|last1=Terlaky|first1=Tamás|last2=Zhang|first2=Shu Zhong|title=Pivot rules for linear programming: A Survey on recent theoretical developments|issue=1|journal=Annals of Operations Research|volume=46–47|year=1993|pages=203–233|doi=10.1007/BF02096264|mr=1260019|citeseerx = 10.1.1.36.7658 |issn=0254-5330}}</ref><ref>{{cite news|first1=Komei|last1=Fukuda|first2=Tamás|last2=Terlaky|title=Criss-cross methods: A fresh view on pivot algorithms |journal=Mathematical Programming, Series B|volume=79|number=1—3|pages=369–395|editors=Thomas&nbsp;M. Liebling and Dominique de&nbsp;Werra|publisher=North-Holland Publishing&nbsp;Co. |location=Amsterdam|year=1997|doi=10.1007/BF02614325|mr=1464775}}</ref> There are polynomial-time algorithms for linear programming that use interior point methods: these include [[Khachiyan]]'s [[ellipsoidal algorithm]], [[Karmarkar]]'s [[Karmarkar's algorithm|projective algorithm]], and [[interior point method|path-following algorithm]]s.<ref name=\"Vanderbei\"/>\n\n==Linear-fractional programming==\n{{Main|Linear-fractional programming}}\n[[Linear-fractional programming|Linear–fractional programming]] (LFP) is a generalization of [[linear programming]] (LP). In LP the objective function is a  [[linear functional|linear function]], while the objective function of a linear–fractional program is a ratio of two linear functions. In other words, a linear program is a fractional–linear program in which the denominator is the constant function having the value one everywhere. A linear–fractional program can be solved by a variant of the simplex algorithm<ref>{{harvtxt|Murty|1983|loc=Chapter 3.20 (pp. 160–164) and pp. 168 and 179}}</ref><ref>Chapter five: {{cite book|last=Craven|first=B. D.|title=Fractional programming|series=Sigma Series in Applied Mathematics|volume=4|publisher=Heldermann Verlag|location=Berlin|year=1988|pages=145|isbn=978-3-88538-404-5|mr=949209}}</ref><ref>{{cite journal|last1=Kruk|first1=Serge|last2=Wolkowicz|first2=Henry|title=Pseudolinear programming|journal=[[SIAM Review]]|volume=41|year=1999|issue=4|pages=795–805|mr=1723002|jstor=2653207|doi=10.1137/S0036144598335259|citeseerx=10.1.1.53.7355|bibcode=1999SIAMR..41..795K}}\n</ref><ref>{{cite journal|last1=Mathis|first1=Frank H.|last2=Mathis|first2=Lenora Jane|title=A nonlinear programming algorithm for hospital management|journal=[[SIAM Review]]|volume=37 |year=1995 |issue=2 |pages=230–234|mr=1343214|jstor=2132826|doi=10.1137/1037046}}\n</ref> or by the [[criss-cross algorithm]].<ref>{{cite journal|title=The finite criss-cross method for hyperbolic programming|journal=European Journal of Operational Research|volume=114|issue=1|\npages=198–214|year=1999|issn=0377-2217|doi=10.1016/S0377-2217(98)00049-6|url=http://www.sciencedirect.com/science/article/B6VCT-3W3DFHB-M/2/4b0e2fcfc2a71e8c14c61640b32e805a|first1=Tibor|last1=Illés|first2=Ákos|last2=Szirmai|first3=Tamás|last3=Terlaky|ref=harv|id=[http://www.cas.mcmaster.ca/~terlaky/files/dut-twi-96-103.ps.gz PDF preprint]|citeseerx=10.1.1.36.7090}}</ref>\n\n== See also ==\n{{div col}}\n* [[Criss-cross algorithm]]\n* [[Cutting-plane method]]\n* [[Devex algorithm]]\n* [[Fourier–Motzkin elimination]]\n* [[Karmarkar's algorithm]]\n* [[Nelder–Mead method|Nelder–Mead simplicial heuristic]]\n* [[Bland's rule|Pivoting rule of Bland]], which avoids cycling\n{{colend}}\n\n==Notes==\n{{reflist|2}}\n\n==References==\n* {{cite book|last=Murty|first=Katta G.|authorlink=Katta G. Murty|title=Linear programming|publisher=John Wiley & Sons, Inc.|location=New York|year=1983|pages=xix+482|isbn=978-0-471-09725-9|mr=720547|ref=harv}}\n\n== Further reading ==\nThese introductions are written for students of [[computer science]] and [[operations research]]:\n*[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''Introduction to Algorithms'', Second Edition. MIT Press and McGraw-Hill, 2001. {{isbn|0-262-03293-7}}. Section 29.3: The simplex algorithm, pp.&nbsp;790&ndash;804.\n* Frederick S. Hillier and Gerald J. Lieberman: ''Introduction to Operations Research'', 8th edition. McGraw-Hill. {{isbn|0-07-123828-X}}\n* {{cite book|title=Optimization in operations research|first=Ronald L.|last=Rardin|year=1997|publisher=Prentice Hall|pages=919|isbn=978-0-02-398415-0}}\n\n==External links==\n{{wikibooks|Operations Research|The Simplex Method}}\n*[http://www.isye.gatech.edu/~spyros/LP/LP.html An Introduction to Linear Programming and the Simplex Algorithm] by Spyros Reveliotis of the Georgia Institute of Technology.\n*Greenberg, Harvey J., ''Klee–Minty Polytope Shows Exponential Time Complexity of Simplex Method'' University of Colorado at Denver (1997) [http://glossary.computing.society.informs.org/notes/Klee-Minty.pdf PDF download]\n*[http://www.lokminglui.com/lpch3.pdf Simplex Method] A tutorial for Simplex Method with examples (also two-phase and M-method).\n* [https://www.mathstools.com/section/main/simplex_online_calculator Mathstools] Simplex Calculator from www.mathstools.com\n*[http://math.uww.edu/~mcfarlat/s-prob.htm Example of Simplex Procedure for a Standard Linear Programming Problem] by Thomas McFarland of the University of Wisconsin-Whitewater.\n*[http://www.phpsimplex.com/simplex/simplex.htm?l=en PHPSimplex: online tool to solve Linear Programming Problems] by Daniel Izquierdo and Juan José Ruiz of the University of Málaga (UMA, Spain)\n* [http://www.simplex-m.com simplex-m] Online Simplex Solver\n\n{{Optimization algorithms|convex}}\n{{Mathematical programming}}\n\n{{DEFAULTSORT:Simplex Algorithm}}\n[[Category:Optimization algorithms and methods]]\n[[Category:1947 in computer science]]\n[[Category:Exchange algorithms]]\n[[Category:Linear programming]]\n[[Category:Computer-related introductions in 1947]]"
    },
    {
      "title": "Simulated annealing",
      "url": "https://en.wikipedia.org/wiki/Simulated_annealing",
      "text": "{{Short description|Probabilistic optimization technique and metaheuristic\n}}\n{{More citations needed|date=December 2009}}\n[[File:Travelling salesman problem solved with simulated annealing.gif|thumb|Simulated Annealing can be used to solve combinatorial problems. Here it is applied to the [[travelling salesman problem]] to minimize the length of a route that connects all 125 points.]]\n\n'''Simulated annealing''' ('''SA''') is a [[probabilistic algorithm|probabilistic technique]] for approximating the [[global optimum]] of a given [[function (mathematics)|function]]. Specifically, it is a [[metaheuristic]] to approximate [[global optimization]] in a large [[solution space|search space]] for an [[optimization problem]].  It is often used when the search space is discrete (e.g., all tours that visit a given set of cities).  For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as [[gradient descent]].\n\nThe name and inspiration come from [[annealing (metallurgy)|annealing in metallurgy]], a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce their defects. Both are attributes of the material that depend on its thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy.\nThe simulation of annealing can be used to find an approximation of a global minimum for a function with a large number of variables.<ref name=\":0\">{{cite journal |pages=519–524 |last1=Khachaturyan |first1=A. |last2=Semenovskaya |first2=S. |last3=Vainshtein |first3=B. |title=Statistical-Thermodynamic Approach to Determination of Structure Amplitude Phases |journal=Sov.Phys. Crystallography |year=1979 |volume=24 |issue=5 }}</ref><ref name=\":1\">{{cite journal |pages=742–754 |last1=Khachaturyan |first1=A. |last2=Semenovskaya |first2=S. |last3=Vainshtein |first3=B. |title=The Thermodynamic Approach to the Structure Analysis of Crystals |issue=A37 |journal=Acta Crystallographica |volume=37 |year=1981 |doi=10.1107/S0567739481001630|bibcode=1981AcCrA..37..742K }}</ref>\n\nThis notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the global optimal solution. In general, the simulated annealing algorithms work as follows. At each time step, the algorithm randomly selects a solution close to the current one, measures its quality, and then decides to move to it or to stay with the current solution based on either one of two probabilities between which it chooses on the basis of the fact that the new solution is better or worse than the current one. During the search, the temperature is progressively decreased from an initial positive value to zero and affects the two probabilities: at each step, the probability of moving to a better new solution is either kept to 1 or is changed towards a positive value; on the other hand, the probability of moving to a worse new solution is progressively changed towards zero.\n\nThe simulation can be performed either by a solution of kinetic equations for density functions<ref name=\":0\" /><ref name=\":1\" /> or by using the stochastic sampling method.<ref>{{cite journal |jstor=1690046 |pages=671–680 |last1=Kirkpatrick |first1=S. |last2=Gelatt Jr |first2=C. D. |last3=Vecchi |first3=M. P. |title=Optimization by Simulated Annealing |volume=220 |issue=4598 |journal=Science |year=1983 |pmid=17813860 |doi=10.1126/science.220.4598.671|bibcode=1983Sci...220..671K |citeseerx=10.1.1.123.7607 }}</ref><ref>{{cite journal |doi=10.1007/BF00940812 |title=Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm |year=1985 |last1=Černý |first1=V. |journal=Journal of Optimization Theory and Applications |volume=45 |pages=41–51}}</ref><ref>{{cite journal |last2=Khachaturyan |first2=K. |last1=Semenovskaya |first1=S. |last3=Khachaturyan |first3=A. |title=Statistical Mechanics Approach to the Determination of a Crystal |journal=Acta Crystallographica |volume=41 |year=1985 |issue=A41 |pages=268–273 |doi=10.1107/S0108767385000563}}</ref> The method is an adaptation of the [[Metropolis–Hastings algorithm]], a [[Monte Carlo method]] to generate sample states of a thermodynamic system, published by [[Nicholas Metropolis|N. Metropolis]] et al. in 1953.<ref>{{cite journal |doi=10.1063/1.1699114 |title=Equation of State Calculations by Fast Computing Machines |year=1953 |last1=Metropolis |first1=Nicholas |last2=Rosenbluth |first2=Arianna W. |last3=Rosenbluth |first3=Marshall N. |last4=Teller |first4=Augusta H. |last5=Teller |first5=Edward |journal=The Journal of Chemical Physics |volume=21 |issue=6 |pages=1087|bibcode=1953JChPh..21.1087M }}</ref>\n\n==Overview==\nThe [[thermodynamic state|state]] of some [[physical system]]s, and the function ''E''(''s'') to be minimized, is analogous to the [[internal energy]] of the system in that state. The goal is to bring the system, from an arbitrary ''initial state'', to a state with the minimum possible energy.\n\n[[File:Hill Climbing with Simulated Annealing.gif|center|thumb|Simulated annealing searching for a maximum. The objective here is to get to the highest point; however, it is not enough to use a simple [[hill climbing|hill climb algorithm]], as there are many [[Hill climbing algorithm#Local maxima|local maxima]]. By cooling the temperature slowly the global maximum is found.|500px]]\n\n===The basic iteration===\nAt each step, the simulated annealing heuristic considers some neighboring state ''s*'' of the current state ''s'', and [[probabilistic]]ally decides between moving the system to state ''s*'' or staying in-state ''s''.  These probabilities ultimately lead the system to move to states of lower energy.  Typically this step is repeated until the system reaches a state that is good enough for the application, or until a given computation budget has been exhausted.\n\n===The neighbours of a state===\nOptimization of a solution involves evaluating the neighbours of a state of the problem, which are new states produced through conservatively altering a given state. For example, in the [[travelling salesman problem]] each state is typically defined as a [[permutation]] of the cities to be visited, and its neighbours are the set of permutations produced by reversing the order of any two successive cities. The well-defined way in which the states are altered to produce neighbouring states is called a \"move\", and different moves give different sets of neighbouring states. These moves usually result in minimal alterations of the last state, in an attempt to progressively improve the solution through iteratively improving its parts (such as the city connections in the traveling salesman problem).\n\nSimple [[heuristic]]s like [[hill climbing]], which move by finding better neighbour after better neighbour and stop when they have reached a solution which has no neighbours that are better solutions, cannot guarantee to lead to any of the existing better solutions{{snd}} their outcome may easily be just a [[local optimum]], while the actual best solution would be a [[global optimum]] that could be different. [[Metaheuristic]]s use the neighbours of a solution as a way to explore the solutions space, and although they prefer better neighbours, they also accept worse neighbours in order to avoid getting stuck in local optima; they can find the global optimum if run for a long enough amount of time.\n\n===Acceptance probabilities===\nThe probability of making the [[state transition|transition]] from the current state <math>s</math> to a candidate new state <math>s'</math> is specified by an ''acceptance probability function'' <math>P(e, e', T)</math>, that depends on the energies <math>e = E(s)</math> and <math>e' = E(s')</math> of the two states, and on a global time-varying parameter <math>T</math> called the ''temperature''.  States with a smaller energy are better than those with a greater energy.  The probability function <math>P</math> must be positive even when <math>e'</math> is greater than <math>e</math>.  This feature prevents the method from becoming stuck at a local minimum that is worse than the global one.\n\nWhen <math>T</math> tends to zero, the probability <math>P(e, e', T)</math> must tend to zero if <math>e' > e</math> and to a positive value otherwise. For sufficiently small values of <math>T</math>, the system will then increasingly favor moves that go \"downhill\" (i.e., to lower energy values), and avoid those that go \"uphill.\"  With <math>T=0</math> the procedure reduces to the [[greedy algorithm]], which makes only the downhill transitions.\n\nIn the original description of simulated annealing, the probability <math>P(e, e', T)</math> was equal to 1 when <math>e' < e</math>—i.e., the procedure always moved downhill when it found a way to do so, irrespective of the temperature.  Many descriptions and implementations of simulated annealing still take this condition as part of the method's definition.  However, this condition is not essential for the method to work.\n\nThe <math>P</math> function is usually chosen so that the probability of accepting a move decreases when the difference\n<math>e'-e</math> increases—that is, small uphill moves are more likely than large ones.  However, this requirement is not strictly necessary, provided that the above requirements are met.\n\nGiven these properties, the temperature <math>T</math> plays a crucial role in controlling the evolution of the state <math>s</math> of the system with regard to its sensitivity to the variations of system energies. To be precise, for a large <math>T</math>, the evolution of <math>s</math> is sensitive to coarser energy variations, while it is sensitive to finer energy variations when <math>T</math> is small.\n\n===The annealing schedule===\nThe name and inspiration of the algorithm demand an interesting feature related to the temperature variation to be embedded in the operational characteristics of the algorithm. This necessitates a gradual reduction of the temperature as the simulation proceeds. The algorithm starts initially with <math>T</math> set to a high value (or infinity), and then it is decreased at each step following some ''annealing schedule''—which may be specified by the user, but must end with <math>T=0</math> towards the end of the allotted time budget.  In this way, the system is expected to wander initially towards a broad region of the search space containing good solutions, ignoring small features of the energy function; then drift towards low-energy regions that become narrower and narrower; and finally move downhill according to the [[steepest descent]] heuristic.\n\n{| style=\"margin:0.5em auto; max-width:90%;\"\n|-\n| [[File:SimulatedAnnealingFast.jpg|center|thumb|200px|Fast]]||[[File:SimulatedAnnealingSlow.jpg|center|thumb|200px|Slow]]\n|-\n| colspan=2 |\nExample illustrating the effect of cooling schedule on the performance of simulated annealing.  The problem is to rearrange the [[pixel]]s of an image so as to minimize a certain [[potential energy]] function, which causes similar [[colour]]s to attract at short range and repel at a slightly larger distance.  The elementary moves swap two adjacent pixels.  These images were obtained with a fast cooling schedule (left) and a slow cooling schedule (right), producing results similar to [[amorphous solid|amorphous]] and [[crystalline solid]]s, respectively.\n|}\n\nFor any given finite problem, the probability that the simulated annealing algorithm terminates with a [[global optimum|global optimal]] solution approaches 1 as the annealing schedule is extended.<ref>{{cite journal |doi=10.1109/34.295910 |title=Simulated annealing: A proof of convergence |year=1994 |last1=Granville |first1=V. |last2=Krivanek |first2=M. |last3=Rasson |first3=J.-P. |journal=IEEE Transactions on Pattern Analysis and Machine Intelligence |volume=16 |issue=6 |pages=652–656}}</ref> This theoretical result, however, is not particularly helpful, since the time required to ensure a significant probability of success will usually exceed the time required for a [[Brute-force search|complete search]] of the [[solution space]].{{Citation needed|date=June 2011}}\n\n==Pseudocode==\nThe following pseudocode presents the simulated annealing heuristic as described above. It starts from a state {{math|''s''<sub>0</sub>}} and continues until a maximum of {{math|''k''<sub>max</sub>}} steps have been taken. In the process, the call {{math|neighbour(''s'')}} should generate a randomly chosen neighbour of a given state {{mvar|s}}; the call {{math|random(0, 1)}} should pick and return a value in the range {{math|[0, 1]}}, [[Uniform distribution (continuous)|uniformly at random]]. The annealing schedule is defined by the call {{math|temperature(''r'')}}, which should yield the temperature to use, given the fraction {{mvar|r}} of the time budget that has been expended so far.\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Let {{math|''s'' {{=}} ''s''<sub>0</sub>}}\n* For {{math|''k'' {{=}} 0}} through {{math|''k''<sub>max</sub>}} (exclusive):\n** {{math|''T'' ← temperature( ''k''<sub>max</sub>/''(k+1)'' )}}\n** Pick a random neighbour, {{math|''s''<sub>new</sub> ← neighbour(''s'')}}\n** If {{math|''P''(''E''(''s''), ''E''(''s''<sub>new</sub>), ''T'') ≥ random(0, 1)}}:\n*** {{math|''s'' ← ''s''<sub>new</sub>}}\n* Output: the final state {{mvar|s}}\n{{frame-footer}}\n</div>\n\n==Selecting the parameters==\nIn order to apply the simulated annealing method to a specific problem, one must specify the following parameters: the state space, the energy (goal) function {{code|E()}}, the candidate generator procedure {{code|neighbour()}}, the acceptance probability function {{code|P()}}, and the annealing schedule {{code|temperature()}} AND initial temperature <init temp>. These choices can have a significant impact on the method's effectiveness.  Unfortunately, there are no choices of these parameters that will be good for all problems, and there is no general way to find the best choices for a given problem.  The following sections give some general guidelines.\n\n===Sufficiently near neighbour===\nSimulated annealing may be modeled as a random walk on a search graph, whose vertices are all possible states, and whose edges are the candidate moves. An essential requirement for the {{code|neighbour()}} function is that it must provide a sufficiently short path on this graph from the initial state to any state which may be the global optimum{{snd}} the diameter of the search graph must be small. In the traveling salesman example above, for instance, the search space for n = 20 cities has n! = 2,432,902,008,176,640,000 (2.4 quintillion) states; yet the neighbor generator function that swaps two consecutive cities can get from any state (tour) to any other state in at most <math>\\sum_{k=1}^{n-1} k=\\frac{n(n-1)}{2}=190</math> steps.\n\n===Transition probabilities===\nTo investigate the behavior of simulated annealing on a particular problem, it can be useful to consider the ''transition probabilities'' that result from the various design choices made in the implementation of the algorithm.  For each edge <math>(s,s')</math> of the search graph, the transition probability is defined as the probability that the simulated annealing algorithm will move to state  <math>s'</math> when its current state is <math>s</math>.  This probability depends on the current temperature as specified by {{code|temperature()}}, on the order in which the candidate moves are generated by the {{code|neighbour()}} function, and on the acceptance probability function {{code|P()}}. (Note that the transition probability is '''not''' simply <math>P(e, e', T)</math>, because the candidates are tested serially.)\n\n===Acceptance probabilities===\nThe specification of {{code|neighbour()}}, {{code|P()}}, and {{code|temperature()}} is partially redundant.  In practice, it's common to use the same acceptance function {{code|P()}} for many problems, and adjust the other two functions according to the specific problem.\n\nIn the formulation of the method by Kirkpatrick et al., the acceptance probability function <math>P(e, e', T)</math> was defined as 1 if <math>e' < e</math>, and <math>\\exp(-(e'-e)/T)</math> otherwise. This formula was superficially justified by analogy with the transitions of a physical system; it corresponds to the [[Metropolis–Hastings algorithm]], in the case where T=1 and the proposal distribution of Metropolis→Hastings is symmetric. However, this acceptance probability is often used for simulated annealing even when the {{code|neighbour()}} function, which is analogous to the proposal distribution in Metropolis–Hastings, is not symmetric, or not probabilistic at all.  As a result, the transition probabilities of the simulated annealing algorithm do not correspond to the transitions of the analogous physical system, and the long-term distribution of states at a constant temperature <math>T</math> need not bear any resemblance to the thermodynamic equilibrium distribution over states of that physical system, at any temperature. Nevertheless, most descriptions of simulated annealing assume the original acceptance function, which is probably hard-coded in many implementations of SA.\n\n===Efficient candidate generation===\nWhen choosing the candidate generator {{code|neighbour()}}, one must consider that after a few iterations of the simulated annealing algorithm, the current state is expected to have much lower energy than a random state.  Therefore, as a general rule, one should skew the generator towards candidate moves where the energy of the destination state <math>s'</math> is likely to be similar to that of the current state.  This [[heuristic]] (which is the main principle of the [[Metropolis–Hastings algorithm]]) tends to exclude \"very good\" candidate moves as well as \"very bad\" ones; however, the former are usually much less common than the latter, so the heuristic is generally quite effective.\n\nIn the traveling salesman problem above, for example, swapping two ''consecutive'' cities in a low-energy tour is expected to have a modest effect on its energy (length); whereas swapping two ''arbitrary'' cities is far more likely to increase its length than to decrease it.  Thus, the consecutive-swap neighbour generator is expected to perform better than the arbitrary-swap one, even though the latter could provide a somewhat shorter path to the optimum (with <math>n-1</math> swaps, instead of <math>n(n-1)/2</math>).\n\nA more precise statement of the heuristic is that one should try first candidate states <math>s'</math> for which <math>P(E(s), E(s'), T)</math> is large.  For the \"standard\" acceptance function <math>P</math> above, it means that <math>E(s') - E(s)</math> is on the order of <math>T</math> or less. Thus, in the traveling salesman example above, one could use a {{code|neighbour()}} function that swaps two random cities, where the probability of choosing a city pair vanishes as their distance increases beyond <math>T</math>.\n\n===Barrier avoidance===\nWhen choosing the candidate generator {{code|neighbour()}} one must also try to reduce the number of \"deep\" local minima—states (or sets of connected states) that have much lower energy than all its neighbouring states.  Such \"closed [[drainage basin|catchment]] basins\" of the energy function may trap the simulated annealing algorithm with high probability (roughly proportional to the number of states in the basin) and for a very long time (roughly exponential on the energy difference between the surrounding states and the bottom of the basin).\n\nAs a rule, it is impossible to design a candidate generator that will satisfy this goal and also prioritize candidates with similar energy.  On the other hand, one can often vastly improve the efficiency of simulated annealing by relatively simple changes to the generator.  In the traveling salesman problem, for instance, it is not hard to exhibit two tours <math>A</math>, <math>B</math>, with nearly equal lengths, such that (1) <math>A</math> is optimal, (2) every sequence of city-pair swaps that converts <math>A</math> to <math>B</math> goes through tours that are much longer than both, and (3) <math>A</math> can be transformed into <math>B</math> by flipping (reversing the order of) a set of consecutive cities.  In this example, <math>A</math> and <math>B</math> lie in different \"deep basins\" if the generator performs only random pair-swaps; but they will be in the same basin if the generator performs random segment-flips.\n\n===Cooling schedule===\nThe physical analogy that is used to justify simulated annealing assumes that the cooling rate is low enough for the probability distribution of the current state to be near [[thermodynamic equilibrium]] at all times.  Unfortunately, the ''relaxation time''—the time one must wait for the equilibrium to be restored after a change in temperature—strongly depends on the \"topography\" of the energy function and on the current temperature.  In the simulated annealing algorithm, the relaxation time also depends on the candidate generator, in a very complicated way.  Note that all these parameters are usually provided as [[procedural parameter|black box functions]] to the simulated annealing algorithm. Therefore, the ideal cooling rate cannot be determined beforehand, and should be empirically adjusted for each problem. [[Adaptive simulated annealing]] algorithms address this problem by connecting the cooling schedule to the search progress. Other adaptive approach as Thermodynamic Simulated Annealing<ref>{{cite journal |doi=10.1016/j.physleta.2003.08.070 |title=Placement by thermodynamic simulated annealing |year=2003 |last1=De Vicente |first1=Juan |last2=Lanchares |first2=Juan |last3=Hermida |first3=Román |journal=Physics Letters A |volume=317 |issue=5–6 |pages=415–423|bibcode=2003PhLA..317..415D }}</ref>, automatically adjusts the temperature at each step based on the energy difference between the two states, according to the laws of thermodynamics.\n\n==Restarts==\nSometimes it is better to move back to a solution that was significantly better rather than always moving from the current state.  This process is called ''restarting'' of simulated annealing.  To do this we set <code>s</code> and <code>e</code> to <code>sbest</code> and <code>ebest</code> and perhaps restart the annealing schedule.  The decision to restart could be based on several criteria. Notable among these include restarting based on a fixed number of steps, based on whether the current energy is too high compared to the best energy obtained so far, restarting randomly, etc.\n\n==Related methods==\n* [[Metropolis–Hastings algorithm|Interacting Metropolis–Hasting algorithms]] (a.k.a. [[Particle filter|Sequential Monte Carlo]]<ref name=\":3\">{{Cite journal|title = Sequential Monte Carlo samplers - P. Del Moral - A. Doucet - A. Jasra - 2006 - Journal of the Royal Statistical Society: Series B (Statistical Methodology) - Wiley Online Library| doi=10.1111/j.1467-9868.2006.00553.x|volume=68| issue=3|journal=Journal of the Royal Statistical Society, Series B|pages=411–436|arxiv=cond-mat/0212648|year = 2006|last1 = Del Moral|first1 = Pierre| last2=Doucet| first2=Arnaud| last3=Jasra| first3=Ajay}}</ref>) combined simulated annealing moves with an acceptance-rejection of the best fitted individuals equipped with an interacting recycling mechanism.\n* [[Quantum annealing]] uses \"quantum fluctuations\" instead of thermal fluctuations to get through high but thin barriers in the target function.\n* [[Stochastic tunneling]] attempts to overcome the increasing difficulty simulated annealing runs have in escaping from local minima as the temperature decreases, by 'tunneling' through barriers.\n* [[Tabu search]] normally moves to neighbouring states of lower energy, but will take uphill moves when it finds itself stuck in a local minimum; and avoids cycles by keeping a \"taboo list\" of solutions already seen.\n* [[Dual-phase evolution]] is a family of algorithms and processes (to which simulated annealing belongs) that mediate between local and global search by exploiting phase changes in the search space.\n* [[LIONsolver|Reactive search optimization]] focuses on combining machine learning with optimization, by adding an internal feedback loop to self-tune the free parameters of an algorithm to the characteristics of the problem, of the instance, and of the local situation around the current solution.\n* [[Stochastic gradient descent]] runs many greedy searches from random initial locations.\n* [[Genetic algorithms]] maintain a pool of solutions rather than just one. New candidate solutions are generated not only by \"mutation\" (as in SA), but also by \"recombination\" of two solutions from the pool. Probabilistic criteria, similar to those used in SA, are used to select the candidates for mutation or combination, and for discarding excess solutions from the pool.\n* [[Graduated optimization]] digressively \"smooths\" the target function while optimizing.\n* [[Ant colony optimization]] (ACO) uses many ants (or agents) to traverse the solution space and find locally productive areas.\n* The [[cross-entropy method]] (CE) generates candidates solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.\n* [[Harmony search]] mimics musicians in improvisation process where each musician plays a note for finding a best harmony all together.\n* [[Stochastic optimization]] is an umbrella set of methods that includes simulated annealing and numerous other approaches.\n* [[Particle swarm optimization]] is an algorithm modelled on swarm intelligence that finds a solution to an optimization problem in a search space, or model and predict social behavior in the presence of objectives.\n* The runner-root algorithm (RRA) is a meta-heuristic optimization algorithm for solving unimodal and multimodal problems inspired by the runners and roots of plants in nature.\n* [[Intelligent water drops algorithm]] (IWD) which mimics the behavior of natural water drops to solve optimization problems\n* [[Parallel tempering]] is a simulation of model copies at different temperatures (or [[Hamiltonian (quantum mechanics)|Hamiltonian]]s) to overcome the potential barriers.\n\n==See also==\n{{columns-list|colwidth=30em|\n* [[Adaptive simulated annealing]]\n* [[Markov chain]]\n* [[Combinatorial optimization]]\n* [[Dual-phase evolution]]\n* [[Automatic label placement]]\n* [[Multidisciplinary optimization]]\n* [[Place and route]]\n* [[Molecular dynamics]]\n* [[Traveling salesman problem]]\n* [[Graph cuts in computer vision]]\n* [[Particle swarm optimization]]\n* [[Intelligent water drops algorithm]]\n}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*A. Das and B. K. Chakrabarti (Eds.), ''[ftp://nozdr.ru/biblio/kolxoz/M/MP/Das%20A.,%20Chakrabarti%20B.K.%20(eds.)%20Quantum%20Annealing%20and%20Related%20Optimization%20Methods%20(LNP0679,%20Springer,%202005)(384s)_MP_.pdf Quantum Annealing and Related Optimization Methods],'' Lecture Note in Physics, Vol. 679, Springer, Heidelberg (2005)\n*{{cite journal |doi=10.1007/BF00202749 |title=Correlated and uncorrelated fitness landscapes and how to tell the difference |year=1990 |last1=Weinberger |first1=E. |journal=Biological Cybernetics |volume=63 |issue=5 |pages=325–336}}\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 10.12. Simulated Annealing Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=549 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n*{{cite journal |doi=10.1016/j.ympev.2016.05.001 |title=On simulated annealing phase transitions in phylogeny reconstruction |year=2016 |last1=Strobl |first1=M.A.R. |last2=Barker |first2=D. |journal=Molecular Phylogenetics and Evolution |volume=101 |pages=46–55 |pmid=27150349 |pmc=4912009}}\n*V.Vassilev, A.Prahova: \"The Use of Simulated Annealing in the Control of Flexible Manufacturing Systems\", International Journal INFORMATION THEORIES & APPLICATIONS, [http://www.foibg.com/ijita/vol01-09/ijita-fv06.htm VOLUME 6/1999]\n\n==External links==\n* [http://www.heatonresearch.com/aifh/vol1/tsp_anneal.html Simulated Annealing] A Javascript app that allows you to experiment with simulated annealing. Source code included.\n* [http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=10548&objectType=file \"General Simulated Annealing Algorithm\"] An open-source MATLAB program for general simulated annealing exercises.\n* [[Wikiversity:Simulated Annealing Project|Self-Guided Lesson on Simulated Annealing]] A Wikiversity project.\n* [https://arstechnica.com/science/news/2009/12/uncertainty-hovers-over-claim-googles-using-quantum-computer.ars Google in superposition of using, not using quantum computer] Ars Technica discusses the possibility that the D-Wave computer being used by Google may, in fact, be an efficient simulated annealing co-processor.\n\n{{Major subfields of optimization}}\n\n{{DEFAULTSORT:Simulated Annealing}}\n[[Category:Metaheuristics]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Monte Carlo methods]]"
    },
    {
      "title": "Simultaneous perturbation stochastic approximation",
      "url": "https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation",
      "text": "'''Simultaneous perturbation stochastic approximation''' (SPSA) is an [[algorithm]]ic method for optimizing systems with multiple unknown [[parameters]]. It is a type of [[stochastic approximation]] algorithm. As an optimization method, it is appropriately suited to large-scale population models, adaptive modeling, simulation [[optimization]], and [[atmospheric model]]ing. Many examples are presented at the SPSA website http://www.jhuapl.edu/SPSA. A comprehensive recent book on the subject is Bhatnagar et al. (2013). An early paper on the subject is Spall (1987) and the foundational paper providing the key theory and justification is Spall (1992). \n\nSPSA is a descent method capable of finding global minima, sharing this property with other methods as simulated annealing. Its main feature is the gradient approximation that requires only two measurements of the objective function, regardless of the dimension of the optimization problem. Recall that we want to find the optimal control <math>u^*</math> with loss\nfunction <math>J(u)</math>:\n\n:<math>u^* = \\arg  \\min_{u \\in U} J(u).</math>\n\nBoth Finite Differences Stochastic Approximation (FDSA)\nand SPSA use the same iterative process: \n\n:<math>u_{n+1} = u_n - a_n\\hat{g}_n(u_n),</math>\n\nwhere <math>u_n=((u_n)_1,(u_n)_2,\\ldots,(u_n)_p)^T</math>\nrepresents the <math>n^{th}</math> iterate, <math>\\hat{g}_n(u_n)</math> is the estimate of the gradient of the objective function <math>g(u)= \\frac{\\partial}{\\partial u}J(u)</math> evaluated at <math>{u_n}</math>, and <math>\\{a_n\\}</math> is a positive number sequence converging to 0. If <math>u_n</math> is a ''p''-dimensional vector, the <math>i^{th}</math> component of the [[symmetric]] finite difference gradient estimator is:\n\n:'''FD:''' <math>(\\hat{g_n}(u_n))_i = \\frac{J(u_n+c_ne_i)-J(u_n-c_ne_i)}{2c_n},</math>\n\n''1 ≤i ≤p'', where <math>e_i</math> is the unit vector with a 1 in the <math>i^{th}</math>\nplace, and <math>c_n</math>is a small positive number that decreases with ''n''. With this method, ''2p'' evaluations of ''J'' for each <math>g_n</math> are needed. Clearly, when ''p'' is large, this estimator loses efficiency.\n\nLet now  <math>\\Delta_n</math> be a random perturbation vector. The <math>i^{th}</math> component of the stochastic perturbation gradient estimator is:\n\n:'''SP:''' <math>(\\hat{g_n}(u_n))_i = \\frac{J(u_n+c_n\\Delta_n)-J(u_n-c_n\\Delta_n)}{2c_n(\\Delta_n)_i}.</math>\n\nRemark that FD perturbs only one direction at a time, while the SP estimator disturbs all directions at the same time (the numerator is identical in all ''p'' components). The number of loss function measurements needed in the SPSA method for each <math>g_n</math> is always 2, independent of the [[dimension]] ''p''. Thus, SPSA uses ''p'' times fewer function evaluations than FDSA, which makes it a lot more efficient.\n\nSimple experiments with ''p=2'' showed that SPSA converges in the same number of iterations as FDSA. The latter follows [[Approximation|approximately]] the [[steepest]] descent direction, behaving like the gradient method. On the other hand, SPSA, with the random search direction, does not follow exactly the gradient path. In average though, it tracks it nearly because the gradient approximation is an almost [[unbiased]]\nestimator of the gradient, as shown in the following lemma.\n\n== Convergence lemma ==\nDenote by \n\n:<math>b_n = E[\\hat{g}_n|u_n] -\\nabla J(u_n) </math>\n\nthe bias in the estimator <math>\\hat{g}_n</math>. Assume that <math>\\{(\\Delta_n)_i\\}</math> are all mutually independent with zero-mean, bounded second\nmoments, and <math>E(|(\\Delta_n)_i|^{-1})</math> uniformly bounded. Then <math>b_n</math>→0 w.p.&nbsp;1.\n\n== Sketch of the proof ==\nThe main [[idea]] is to use conditioning on <math>\\Delta_n</math> to express <math>E[(\\hat{g}_n)_i]</math> and then to use a second order Taylor expansion of <math>J(u_n+c_n\\Delta_n)_i</math> and <math>J(u_n-c_n\\Delta_n)_i</math>. After algebraic manipulations using the zero mean and the independence of <math>\\{(\\Delta_n)_i\\}</math>, we get \n\n:<math>E[(\\hat{g}_n)_i]=(g_n)_i + O(c_n^2)</math>\n\nThe result follows from the [[hypothesis]] that <math>c_n</math>→0.\n\nNext we resume some of the hypotheses under which <math>u_t</math> converges in [[probability]] to the set of global minima of <math>J(u)</math>. The efficiency of\nthe method depends on the shape of <math>J(u)</math>, the values of the parameters <math>a_n</math> and <math>c_n</math> and the distribution of the perturbation terms <math>\\Delta_{ni}</math>. First, the algorithm parameters must satisfy the\nfollowing conditions:\n\n*  <math>a_n</math> >0, <math>a_n</math>→0 when n→∝ and <math>\\sum_{n=1}^{\\infty} a_n = \\infty </math>. A good choice would be <math>a_n=\\frac{a}{n};</math> a>0;\n*  <math>c_n=\\frac{c}{n^\\gamma}</math>, where c>0, <math> \\gamma \\in \\left[\\frac{1}{6},\\frac{1}{2}\\right]</math>;\n* <math>\\sum_{n=1}^{\\infty} (\\frac {a_n}{c_n})^2 < \\infty </math>\n* <math> \\Delta_{ni} </math> must be mutually independent zero-mean random variables, symmetrically distributed about zero, with <math>\\Delta_{ni} < a_1 < \\infty </math>. The inverse first and second moments of the <math> \\Delta_{ni} </math> must be finite.\nA good choice for <math>\\Delta_{ni}</math> is the [[Rademacher distribution]], i.e. Bernoulli +-1 with probability 0.5. Other choices are possible too, but note that the uniform and normal distributions cannot be used because they do not satisfy the finite inverse moment conditions.\n\nThe loss function ''J(u)'' must be thrice continuously [[Differentiable function|differentiable]] and the individual elements of the third derivative must be bounded: <math>|J^{(3)}(u)| < a_3 < \\infty </math>. Also, <math>|J(u)|\\rightarrow\\infty</math> as <math>u\\rightarrow\\infty</math>.\n\nIn addition, <math>\\nabla J</math> must be Lipschitz continuous, bounded and the ODE <math> \\dot{u}=g(u)</math> must have a unique solution for each initial condition.\nUnder these conditions and a few others, <math>u_k</math> [[Convergence (mathematics)|converges]] in probability to the set of global minima of J(u) (see Maryak and Chin, 2008).\n\n==References==\n* Bhatnagar, S., Prasad, H. L., and Prashanth, L. A. (2013), ''Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods'', Springer [https://link.springer.com/book/10.1007/978-1-4471-4285-0].\n* Hirokami, T., Maeda, Y., Tsukada, H. (2006) \"Parameter estimation using simultaneous perturbation stochastic approximation\", Electrical Engineering in Japan, 154 (2), 30–3 [https://dx.doi.org/10.1002/eej.20239]\n* Maryak, J.L., and Chin, D.C. (2008), \"Global Random Optimization by Simultaneous Perturbation Stochastic Approximation,\" ''IEEE Transactions on Automatic Control'', vol. 53, pp. 780-783.\n* Spall, J. C. (1987), “A Stochastic Approximation Technique for Generating Maximum Likelihood Parameter Estimates,” ''Proceedings of the American Control Conference'', Minneapolis, MN, June 1987, pp. 1161–1167.\n* Spall, J. C. (1992), “Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation,” ''IEEE Transactions on Automatic Control'', vol. 37(3), pp. 332–341. \n* Spall, J.C. (1998). \"Overview of the Simultaneous Perturbation Method for Efficient Optimization\" [http://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF 2]. ''Johns Hopkins APL Technical Digest'', 19(4), 482–492.\n* Spall, J.C. (2003) ''Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control'', Wiley. {{isbn|0-471-33052-3}} (Chapter 7)\n\n<references/>\n\n[[Category:Numerical climate and weather models]]\n[[Category:Randomized algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Social cognitive optimization",
      "url": "https://en.wikipedia.org/wiki/Social_cognitive_optimization",
      "text": "'''Social cognitive optimization''' (SCO) is a population-based [[metaheuristic]] [[optimization]] algorithm which was developed in 2002.<ref name=\"xzy02sco\"/> This algorithm is based on the [[social cognitive theory]], and the key point of the ergodicity is the process of individual [[learning]] of a set of agents with their own [[memory]] and their [[social learning theory|social learning]] with the knowledge points in the social sharing library. It has been used for solving [[continuous optimization]],<ref name=\"xz04sco\"/><ref name=\"xu2011reactive\"/>  [[integer programming]],<ref name=\"fan10scoip\"/> and [[combinatorial optimization]] problems. It has been incorporated into the [https://wiki.openoffice.org/wiki/NLPSolver NLPSolver] extension of Calc in [[Apache OpenOffice]].\n\n== Algorithm ==\n\nLet <math>f(x)</math> be a global optimization problem, where <math>x</math> is a state in the problem space <math>S</math>. In SCO, each state is called a ''knowledge point'', and the function <math>f</math> is the ''goodness function''.\n\nIn SCO, there are a population of <math>N_c</math> cognitive agents solving in parallel, with a social sharing library. Each agent holds a private memory containing one knowledge point, and the social sharing library contains a set of <math>N_L</math> knowledge points. The algorithm runs in ''T'' iterative learning cycles. By running as a [[Markov chain]] process, the system behavior in the ''t''th cycle only depends on the system status in the (''t'' − 1)th cycle. The process flow is in follows:\n\n* [1. Initialization]：Initialize the private knowledge point <math>x_i</math> in the memory of each agent <math>i</math>, and all knowledge points in the social sharing library <math>X</math>, normally at random in the problem space <math>S</math>.\n* [2. Learning cycle]： At each cycle <math>t</math> <math> (t = 1, \\ldots, T)</math>：\n** [2.1. Observational learning] For each agent <math>i</math> <math>(i = 1, \\ldots, N_c)</math>：\n***  [2.1.1. Model selection]：Find a high-quality ''model point'' <math>x_M</math> in <math>X(t)</math> , normally realized using [[tournament selection]], which returns the best knowledge point from randomly selected <math>\\tau_B</math> points.\n*** [2.1.2. Quality Evaluation]：Compare the private knowledge point <math>x_i(t)</math> and the model point <math>x_M</math>，and return the one with higher quality as the ''base point'' <math>x_{Base}</math>，and another as the ''reference point'' <math>x_{Ref}</math>。\n*** [2.1.3. Learning]：Combine <math>x_{Base}</math> and <math>x_{Ref}</math> to generate a new knowledge point <math>x_{i}(t+1)</math>. Normally <math>x_{i}(t+1)</math> should be around <math>x_{Base}</math>，and the distance with <math>x_{Base}</math> is related to the distance between <math>x_{Ref}</math> and <math>x_{Base}</math>, and boundary handling mechanism should be incorporated here to ensure that <math>x_{i}(t+1) \\in S</math>.\n*** [2.1.4. Knowledge sharing]：Share a knowledge point, normally <math>x_i(t+1)</math>, to the social sharing library <math>X</math>.\n*** [2.1.5. Individual update]：Update the private knowledge of agent <math>i</math>, normally replace <math>x_{i}(t)</math> by <math>x_{i}(t+1)</math>. Some Monte Carlo types might also be considered.\n** [2.2. Library Maintenance]：The social sharing library using all knowledge points submitted by agents to update <math>X(t)</math> into <math>X(t+1)</math>. A simple way is one by one tournament selection: for each knowledge point submitted by an agent, replace the worse one among <math>\\tau_W</math> points randomly selected from <math>X(t)</math>.\n* [3. Termination]：Return the best knowledge point found by the agents.\n\nSCO has three main parameters, i.e., the number of agents <math>N_c</math>, the size of social sharing library <math>N_{L}</math>, and the learning cycle <math>T</math>. With the initialization process, the total number of knowledge points to be generated is <math>N_{L}+N_c*(T+1)</math>, and is not related too much with <math>N_{L}</math> if <math>T</math> is large.\n\nCompared to traditional swarm algorithms, e.g. [[particle swarm optimization]], SCO can achieving high-quality solutions as <math>N_c</math> is small, even as <math>N_c=1</math>. Nevertheless, smaller <math>N_c</math> and <math>N_{L}</math> might lead to [[premature convergence]]. Some variants <ref name=\"sun2014guaranteed\"/> were proposed to guaranteed the global convergence. One can also make a hybrid optimization method using SCO combined with other optimizers. For example, SCO was hybridized with [[differential evolution]] to obtain better results than individual algorithms on a common set of benchmark problems <ref name=\"CGOS14\">{{cite journal |last1=Xie |first1=Xiao-Feng |last2=Liu| first2=J.|last3=Wang | first3=Zun-Jing | title=A cooperative group optimization system | journal=Soft Computing | date=2014 |volume=18 |issue=3| pages=469–495 |doi=10.1007/s00500-013-1069-8 }}</ref>.\n\n== References ==\n\n{{reflist|\nrefs=\n<ref name=\"xzy02sco\">Xie, Xiao-Feng; Zhang, Wen-Jun; Yang, Zhi-Lian (2002). [https://dx.doi.org/10.1109/ICMLC.2002.1174487 Social cognitive optimization for nonlinear programming problems]. ''International Conference on Machine Learning and Cybernetics'' (ICMLC), Beijing, China: 779-783.</ref>\n\n<ref name=\"xz04sco\">Xie, Xiao-Feng; Zhang, Wen-Jun (2004). [https://dx.doi.org/10.1007/978-3-540-24854-5_27 Solving engineering design problems by social cognitive optimization]. ''Genetic and Evolutionary Computation Conference'' (GECCO), Seattle, WA, USA: 261-262.</ref>\n\n<ref name=\"fan10scoip\">Fan, Caixia (2010). Solving integer programming based on maximum entropy social cognitive optimization algorithm. ''International Conference on Information Technology and Scientific Management'' (ICITSM), Tianjing, China: 795-798.</ref>\n\n<ref name=\"xu2011reactive\">Xu, Gang-Gang; Han, Luo-Cheng; Yu, Ming-Long; Zhang, Ai-Lan (2011). [https://dx.doi.org/10.1109/MEC.2011.6025409 Reactive power optimization based on improved social cognitive optimization algorithm]. ''International Conference on Mechatronic Science, Electric Engineering and Computer'' (MEC), Jilin, China: 97-100.</ref>\n\n<ref name=\"sun2014guaranteed\">Sun, Jia-ze; Wang, Shu-yan; chen, Hao (2014). [https://dx.doi.org/10.1155/2014/534162 A guaranteed global convergence social cognitive optimizer]. ''Mathematical Problems in Engineering'': Art. No. 534162.</ref>\n}}\n\n[[Category:Heuristic algorithms]]\n[[Category:Optimization_algorithms_and_methods]]\n[[Category:Collective intelligence]]"
    },
    {
      "title": "Space allocation problem",
      "url": "https://en.wikipedia.org/wiki/Space_allocation_problem",
      "text": "{{unreferenced|date=August 2012}}\n\nThe '''Space allocation problem''' (SAP) is the process in [[architecture]], or in any kind of space planning (SP) technique, of determining the position and size of several elements according to the [[information|input]]-specified design program requirements. These are usually [[topology|topological]] and [[geometry|geometric]] constraints, as well as matters related to the positioning of openings according to their [[Geometric design|geometric]] dimensions, in a [[two-dimensional|two-]] or [[three-dimensional]] space.\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Property management]]\n\n\n{{architecture-stub}}"
    },
    {
      "title": "Space mapping",
      "url": "https://en.wikipedia.org/wiki/Space_mapping",
      "text": "{{Use American English|date = April 2019}}\n{{Short description|design optimization methodology}}\nThe '''space mapping''' methodology for modeling and design optimization of [[engineering systems]] was first discovered by [[John Bandler]] in 1993. It uses relevant existing knowledge to speed up model generation and design [[Mathematical optimization|optimization]] of a system. The knowledge is updated with new validation information from the system when available.\n\n==Concept==\nThe space mapping methodology employs a \"quasi-global\" formulation that intelligently links companion \"coarse\" (ideal or low-fidelity) and \"fine\" (practical or high-fidelity) models of different complexities. In engineering design, space mapping aligns a very fast coarse model with the expensive-to-compute fine model so as to avoid direct expensive optimization of the fine model. The alignment can be done either off-line (model enhancement) or on-the-fly with surrogate updates (e.g., aggressive space mapping).\n<!-- Deleted image removed: [[File:Space Mapping Concept.png|thumb|left|400px|The space mapping concept as it has evolved over the years (Bandler et. al 1994-)<ref name = \"feel\"></ref>]] -->\n\n==Methodology==\nAt the core of the process is a pair of models: one very accurate but too expensive to use directly with a conventional optimization routine, and one significantly less expensive and, accordingly, less accurate. The latter (fast model) is usually referred to as the \"coarse\" model ([[Coarse space (numerical analysis)|coarse space]]). The former (slow model) is usually referred to as the \"fine\" model. A validation space (\"reality\") represents the fine model, for example, a high-fidelity physics model. The optimization space, where conventional optimization is carried out, incorporates the coarse model (or [[surrogate model]]), for example, the low-fidelity physics or \"knowledge\" model. In a space-mapping design optimization phase, there is a prediction or \"execution\" step, where the results of an optimized \"mapped coarse model\" (updated surrogate) are assigned to the fine model for validation. After the validation process, if the design specifications are not satisfied, relevant data is transferred to the optimization space (\"[[feedback]]\"), where the mapping-augmented coarse model or surrogate is updated (enhanced, realigned with the fine model) through an iterative optimization process termed \"parameter extraction\". The mapping formulation itself incorporates \"intuition\", part of the engineer's so-called \"feel\" for a problem.<ref name = \"feel\" /> In particular, the Aggressive Space Mapping (ASM) process displays key characteristics of cognition (an expert's approach to a problem), and is often illustrated in simple cognitive terms.\n\n==Development==\nFollowing [[John Bandler]]'s concept in 1993,<ref name = \"feel\">J.W. Bandler, [http://canrev.ieee.ca/cr70/Space-Mapping-IEEE-Canadian-Review-Magazine-70-The-Future-of-Engineering_and-Technology-Education.pdf \"Have you ever wondered about the engineer's mysterious 'feel' for a problem?\"] IEEE Canadian Review, no. 70, pp. 50-60, Summer 2013.</ref><ref>J.W. Bandler, R.M. Biernacki, S.H. Chen, P.A. Grobelny, and R.H. Hemmers, [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=339794 \"Space mapping technique for electromagnetic optimization,\"] IEEE Trans. Microwave Theory Tech., vol. 42, no. 12, pp. 2536-2544, Dec. 1994.</ref> algorithms have utilized Broyden updates (aggressive space mapping),<ref>J.W. Bandler, R.M. Biernacki, S.H. Chen, R.H. Hemmers, and K. Madsen,[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=475649&tag=1 \"Electromagnetic optimization exploiting aggressive space mapping,\"] IEEE Trans. Microwave Theory Tech., vol. 43, no. 12, pp. 2874-2882, Dec. 1995.</ref> trust regions,<ref>M.H. Bakr, J.W. Bandler, R.M. Biernacki, S.H. Chen and K. Madsen, [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=739229 \"A trust region aggressive space mapping algorithm for EM optimization,\"] IEEE Trans. Microwave Theory Tech., vol. 46, no. 12, pp. 2412-2425, Dec. 1998.</ref> and [[artificial neural network]]s.<ref>M.H. Bakr, J.W. Bandler, M.A. Ismail, J.E. Rayas-Sánchez and Q.J. Zhang, [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=863320 \"Neural space mapping EM optimization of microwave structures,\"] IEEE MTT-S Int. Microwave Symp. Digest (Boston, MA, 2000), pp. 879-882.</ref> New developments include implicit space mapping,<ref>J.W. Bandler, Q.S. Cheng, N.K. Nikolova and M.A. Ismail, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1262729 \"Implicit space mapping optimization exploiting preassigned parameters,\"] IEEE Trans. Microwave Theory Tech., vol. 52, no. 1, pp. 378-385, Jan. 2004.</ref> in which we allow preassigned parameters not used in the optimization process to change in the coarse model, and output space mapping, where a transformation is applied to the response of the model. A paper reviews the state of the art after the first ten years of development and implementation.<ref>J.W. Bandler, Q. Cheng, S.A. Dakroury, A.S. Mohamed, M.H. Bakr, K. Madsen and J. Søndergaard, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1262727 \"Space mapping: the state of the art,\"] IEEE Trans. Microwave Theory Tech., vol. 52, no. 1, pp. 337-361, Jan. 2004.</ref> Tuning space mapping<ref>S. Koziel, J. Meng, J.W. Bandler, M.H. Bakr, and Q.S. Cheng, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4760208 \"Accelerated microwave design optimization with tuning space mapping,\"] IEEE Trans. Microwave Theory Tech., vol. 57, no. 2, pp. 383-394, Feb. 2009.</ref> utilizes a so-called tuning model—constructed invasively from the fine model—as well as a calibration process that translates the adjustment of the optimized tuning model parameters into relevant updates of the design variables. The space mapping concept has been extended to neural-based space mapping for [[Large-signal model|large-signal]] [[Statistical model|statistical modeling]] of [[Nonlinear system|nonlinear]] [[microwave]] devices.<ref>L. Zhang, J. Xu, M.C.E. Yagoub, R. Ding, and Q.J. Zhang, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1504999 \"Efficient analytical formulation and sensitivity analysis of neuro-space mapping for nonlinear microwave device modeling,\"] IEEE Trans. Microwave Theory Tech., vol. 53, no. 9, pp. 2752-2767, Sep. 2005.</ref><ref>L. Zhang, Q.J. Zhang, and J. Wood, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4655631 \"Statistical neuro-space mapping technique for large-signal modeling of nonlinear devices,\"] IEEE Trans. Microwave Theory Tech., vol. 56, no. 11, pp. 2453-2467, Nov. 2008.</ref>\n\nA 2016 state-of-the-art review is devoted to aggressive space mapping.<ref>J.E. Rayas-Sanchez,[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7423860&action=search&sortType=&rowsPerPage=&searchField=Search_All&matchBoolean=true&queryText=(%22Document%20Title%22:simplicity%20in%20asm) \"Power in simplicity with ASM: tracing the aggressive space mapping algorithm over two decades of development and engineering applications\"], IEEE Microwave Magazine, vol. 17, no. 4, pp. 64-76, April 2016.</ref> It spans two decades of development and engineering applications.\n\nThe space mapping methodology can also be used to solve [[inverse problems]]. Proven techniques include the Linear Inverse Space Mapping (LISM) algorithm,<ref>J.E. Rayas-Sanchez , F. Lara-Rojo and E. Martanez-Guerrero,[http://ieeexplore.ieee.org/xpl/abstractReferences.jsp?arnumber=6804007&navigation=1 \"A linear inverse space-mapping (LISM) algorithm to design linear and nonlinear RF and microwave circuits\"], IEEE Trans. Microwave Theory Tech.,  vol. 53,  no. 3,  pp. 960-968 2005.</ref> as well as the Space Mapping with Inverse Difference (SM-ID) method.<ref>M. Şimsek and N. Serap Şengör [https://link.springer.com/chapter/10.1007%2F978-3-642-12294-1_56 \"Solving Inverse Problems by Space Mapping with Inverse Difference Method,\"] Mathematics in Industry, vol. 14, 2010, pp 453-460.</ref>\n\n==Category==\n\nSpace mapping optimization belongs to the class of surrogate-based optimization methods,<ref>A.J. Booker, J.E. Dennis, Jr., P.D. Frank, D.B. Serafini, V. Torczon, and M.W. Trosset,[https://link.springer.com/article/10.1007%2FBF01197708 \"A rigorous framework for optimization of expensive functions by surrogates,\"] Structural Optimization, vol. 17, no. 1, pp. 1-13, Feb. 1999.</ref> that is to say, optimization methods that rely on a [[surrogate model]].\n\n==Applications==\nThe space mapping technique has been applied in a variety of disciplines including microwave and [[Electromagnetism|electromagnetic]] design, civil and mechanical applications, [[aerospace engineering]], and biomedical research. Some examples:\n\n*Optimizing aircraft wing curvature<ref>T.D. Robinson, M.S. Eldred, K.E. Willcox, and R. Haimes, [http://arc.aiaa.org/doi/abs/10.2514/1.36043 \"Surrogate-Based Optimization Using Multifidelity Models with Variable Parameterization and Corrected Space Mapping,\"] AIAA Journal, vol. 46, no. 11, November 2008.</ref>\n*Automotive [[crashworthiness]] design.<ref>M. Redhe and L. Nilsson, [https://link.springer.com/article/10.1007%2Fs00158-004-0396-x \"Optimization of the new Saab 9-3 exposed to impact load using a space mapping technique,\"] Structural and Multidisciplinary Optimization, vol. 27, no. 5, pp. 411-420, July 2004.</ref><ref>T. Jansson, L. Nilsson, and M. Redhe, [https://link.springer.com/article/10.1007%2Fs00158-002-0279-y \"Using surrogate models and response surfaces in structural optimization—with application to crashworthiness design and sheet metal forming,\"] Structural and Multidisciplinary Optimization, vol. 25, no.2, pp 129-140, July 2003.</ref>\n*[[EEG]] source analysis<ref>G. Crevecoeur, H. Hallez, P. Van Hese, Y. D'Asseler, L. Dupré, and R. Van de Walle,[http://www.sciencedirect.com/science/article/pii/S0377042706007448 \"EEG source analysis using space mapping techniques,\"] Journal of Computational and Applied Mathematics, vol. 215, no. 2, pp. 339-347, May 2008.</ref><ref>G. Crevecoeur, H. Hallez, P. Van Hese, Y. D'Asseler, L. Dupré, and R. Van de Walle,[https://link.springer.com/article/10.1007/s11517-008-0341-z# \"A hybrid algorithm for solving the EEG inverse problem from spatio-temporal EEG data,\"] Medical & Biological Engineering & Computing, vol. 46, no. 8, pp. 767-777, August 2008.</ref>\n*Handset antenna optimization<ref>S. Tu, Q.S. Cheng, Y. Zhang, J.W. Bandler, and N.K. Nikolova, [http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=06487389 \"Space mapping optimization of handset antennas exploiting thin-wire models,\"] IEEE Trans. Antennas Propag., vol. 61, no. 7, pp. 3797-3807, July 2013.]</ref><ref>N. Friedrich, [http://mwrf.com/software/space-mapping-outpaces-em-optimization-handset-antenna-design \"Space mapping outpaces EM optimization in handset-antenna design,\"] microwaves&rf, Aug. 30, 2013.</ref><ref>Juan C. Cervantes-González, J. E. Rayas-Sánchez, C. A. López, J. R. Camacho-Pérez, Z. Brito-Brito, and J. L. Chavez-Hurtado,[http://onlinelibrary.wiley.com/doi/10.1002/mmce.20945/abstract;jsessionid=7239347CAF47D5F2245CBCC18B7DB03A.f02t01?userIsAuthenticated=false&deniedAccessCustomisedMessage= \"Space mapping optimization of handset antennas considering EM effects of mobile phone components and human body,\"] Int. J. RF and Microwave CAE, vol. 26, no. 2, pp. 121-128, Feb. 2016</ref>\n*Design centering of [[Microwave engineering|microwave circuits]]<ref>Hany L. Abdel-Malek, Abdel-karim S.O. Hassan, Ezzeldin A. Soliman, and Sameh A. Dakroury, [http://140.98.202.196/xpl/abstractCitations.jsp?tp=&arnumber=1705693&filter%3DAND(p_IS_Number%3A35997) \"The Ellipsoidal Technique for Design Centering of Microwave Circuits Exploiting Space-Mapping Interpolating Surrogates,\"] IEEE Trans. Microwave Theory Tech., vol. 54, no. 10, October 2006.</ref>\n*Design of [[electric machine]]<nowiki/>s using multi-physical modeling<ref>R. Khlissa, S. Vivier, L.A. Ospina Vargas, and G. Friedrich, [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6342665&queryText%3Dkhlissa \"Application of Output Space Mapping method for Fast Optimization using Multi-physical Modeling\"].</ref>\n*Control of [[Partial differential equation|partial differential equations]].<ref>M. Hintermüller and L.N. Vicente, [http://www.mat.uc.pt/~lnv/papers/sm2.pdf \"Space Mapping for Optimal Control of Partial Differential Equations\".]</ref>\n*Voice coil actuator design<ref>L. Encica, J. Makarovic, E.A. Lomonova, and A.J.A. Vandenput, [http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?arnumber=4012289 \"Space mapping optimization of a cylindrical voice coil actuator\"], IEEE Trans. Ind. Appl.,  vol. 42,  no. 6,  pp.1437-1444, 2006.</ref>\n*Reconstruction of local magnetic properties<ref>G. Crevecoeur, L. Dupre, L. Vandenbossche, and R. Van de Walle, [http://users.ugent.be/~ldupre/2006_6.pdf \"Reconstruction of local magnetic properties of steel sheets by needle probe methods using space mapping techniques,\"] Journal of Applied Physics, vol. 99, no. 08H905, 2006.</ref>\n*[[Shape optimization|Structural optimization]]<ref>O. Lass , C. Posch , G. Scharrer and S. Volkwein, [http://www.tandfonline.com/doi/abs/10.1080/10556788.2011.582112#.Vdt6pCVViko \"Space mapping techniques for a structural optimization problem governed by the p-Laplace equation\"], Optimization Methods and Software, 26:4-5, pp. 617-642, 2011.</ref>\n*Design of microwave multiplexers<ref>M.A. Ismail, D. Smith, A. Panariello, Y. Wang, and M. Yu, [http://maxwell.uwaterloo.ca/~myu/publications/04spacemapping.pdf \"EM-based design of large-scale dielectric-resonator filters and multiplexers by space mapping,\"] IEEE Trans. Microwave Theory Tech., vol. 52, no. 1, pp. 386-392, Jan. 2004.</ref>\n\n==Simulators==\nVarious simulators can be involved in a space mapping optimization and modeling processes.\n*In the [[microwave]] and [[radio frequency]] (RF) area\n**[[Keysight]] [[Advanced Design System|ADS]] [http://www.keysight.com/en/pc-1297113/advanced-design-system-ads]\n**Keysight [[Momentum]] [http://www.keysight.com/en/pc-1887116/momentum-3d-planar-em-simulator]\n**Ansys [[HFSS]] [http://www.ansys.com/Products/Simulation+Technology/Electromagnetics/High-Performance+Electronic+Design/ANSYS+HFSS/]\n**CST Microwave Studio [http://www.cst.com/Content/Products/MWS/Overview.aspx]\n**[[FEKO]] [http://www.feko.info/]\n**Sonnet '''''em''''' [http://www.sonnetsoftware.com/]\n\n==Conferences==\nThree international workshops have focused significantly on the art, the science and the technology of space mapping.\n\n*First International Workshop on Surrogate Modelling and Space Mapping for Engineering Optimization (Lyngby, Denmark, Nov. 2000)\n*Second International Workshop on Surrogate Modelling and Space Mapping for Engineering Optimization (Lyngby, Denmark, Nov. 2006)\n*Third International Workshop on Surrogate Modelling and Space Mapping for Engineering Optimization (Reykjavik, Iceland, Aug. 2012)\n\n==Terminology==\nThere is a wide spectrum of terminology associated with space mapping: ideal model, coarse model, coarse space, fine model, companion model, cheap model, expensive model, [[surrogate model]], low fidelity (resolution) model, high fidelity (resolution) model, empirical model, simplified physics model, physics-based model, quasi-global model, physically expressive model, device under test, electromagnetics-based model, [[simulation]] model, computational model, tuning model, calibration model, surrogate model, surrogate update, mapped coarse model, surrogate optimization, parameter extraction, target response, optimization space, validation space, neuro-space mapping, implicit space mapping, output space mapping, port tuning, predistortion (of design specifications), manifold mapping, defect correction, model management, multi-fidelity models, variable fidelity/variable complexity, [[multigrid method]], coarse grid, fine grid, surrogate-driven, simulation-driven, model-driven, feature-based modeling.\n\n==See also==\n{{columns-list|colwidth=22em|\n*[[Adaptive control]]\n*[[Cognitive model]]\n*[[Computational electromagnetics]]\n*[[Computer-aided design]]\n*[[Engineering optimization]]\n*[[Finite element method]]\n*[[Kriging]]\n*[[Linear approximation]]\n*[[Mental model]]\n*[[Mental rotation]]\n*[[Model-dependent realism]]\n*[[Multiphysics]]\n*[[Performance tuning]]\n*[[Response surface methodology]]\n*[[Semiconductor device modeling]]\n*[[Spatial cognition]]\n*[[Spatial memory]]\n*[[Support vector machine]]\n}}\n\n==References==\n{{reflist}}\n\n[[Category:Electromagnetic radiation]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Microwave technology]]\n[[Category:Mathematical modeling]]"
    }
  ]
}