{
  "pages": [
    {
      "title": "Vector addition",
      "url": "https://en.wikipedia.org/wiki/Vector_addition",
      "text": "#REDIRECT [[Euclidean vector#Addition and subtraction]] {{R to section}}\n\n[[Category:Binary operations]]"
    },
    {
      "title": "Wedge sum",
      "url": "https://en.wikipedia.org/wiki/Wedge_sum",
      "text": "In [[topology]], the '''wedge sum''' is a \"one-point union\" of a family of [[topological space]]s. Specifically, if ''X'' and ''Y'' are [[pointed space]]s (i.e. topological spaces with distinguished basepoints ''x''<sub>0</sub> and ''y''<sub>0</sub>) the wedge sum of ''X'' and ''Y'' is the [[Quotient space (topology)|quotient space]] of the [[disjoint union (topology)|disjoint union]] of ''X'' and ''Y'' by the identification ''x''<sub>0</sub> ∼ ''y''<sub>0</sub>:\n\n:<math>X\\vee Y = (X\\amalg Y)\\;/{\\sim},</math> <!-- NOTE: In expressions like \"a\\sim b\", the symbols \\sim, being a binary relation symbol, has a certain amount of space before it and after it.  In this present expression, that spacing is not appropriate, and so it is avoided by enclosing \"\\sim\" in curly braces, thus: \"{\\sim}\". -->\n\nwhere ∼ is the [[equivalence closure]] of the relation {(''x''<sub>0</sub>,''y''<sub>0</sub>)}.\nMore generally, suppose (''X''<sub>''i''</sub>{{pad|0.1em}})<sub>''i''&nbsp;∈&nbsp;''I''</sub> is a [[indexed family|family]] of pointed spaces with basepoints {''p''<sub>''i''</sub>{{pad|0.1em}}}. The wedge sum of the family is given by:\n\n:<math>\\bigvee_{i\\in I} X_i = \\coprod_{i\\in I} X_i\\;/{\\sim},</math>\n\nwhere ∼ is the equivalence closure of the relation {(''p<sub>i</sub>''{{pad|0.1em}}, ''p<sub>j</sub>''{{pad|0.1em}}) | ''i,j''&nbsp;∈&nbsp;''I''{{pad|0.1em}}}.\nIn other words, the wedge sum is the joining of several spaces at a single point. This definition is sensitive to the choice of the basepoints {''p''<sub>''i''</sub>}, unless the spaces {''X''<sub>''i''</sub>{{pad|0.1em}}} are [[homogeneous space|homogeneous]].\n\nThe wedge sum is again a pointed space, and the binary operation is [[associative]] and [[commutative]] (up to homeomorphism).\n\nSometimes the wedge sum is called the '''wedge product''', but this is not the same concept as the [[exterior product]], which is also often called the wedge product.\n\n==Examples==\nThe wedge sum of two circles is [[homeomorphic]] to a [[figure-eight space]]. The wedge sum of ''n'' circles is often called a ''[[bouquet of circles]]'', while a wedge product of arbitrary spheres is often called a '''bouquet of spheres'''.\n\nA common construction in [[homotopy]] is to identify all of the points along the equator of an ''n''-sphere <math>S^n</math>. Doing so results in two copies of the sphere, joined at the point that was the equator:\n\n:<math>S^n/{\\sim} = S^n \\vee S^n </math>\n\nLet <math>\\Psi</math> be the map <math>\\Psi:S^n\\to S^n \\vee S^n</math>, that is, of identifying the equator down to a single point. Then addition of two elements <math>f,g\\in\\pi_n(X,x_0)</math> of the ''n''-dimensional [[homotopy group]] <math>\\pi_n(X,x_0)</math> of a space ''X'' at the distinguished point <math>x_0\\in X</math> can be understood as the composition of <math>f</math> and <math>g</math> with <math>\\Psi</math>:\n\n:<math>f+g = (f \\vee g) \\circ \\Psi</math>\n\nHere, <math>f,g:S^n\\to X</math> are maps which take a distinguished point <math>s_0\\in S^n</math> to the point <math>x_0\\in X.</math> Note that the above uses the wedge sum of two functions, which is possible precisely because they agree at <math>s_0,</math> the point common to the wedge sum of the underlying spaces.\n\n==Categorical description==\nThe wedge sum can be understood as the [[coproduct]] in the [[category of pointed spaces]]. Alternatively, the wedge sum can be seen as the [[pushout (category theory)|pushout]] of the diagram ''X'' ← {•} → ''Y'' in the [[category of topological spaces]] (where {•} is any one-point space).\n\n==Properties==\n[[Van Kampen's theorem]] gives certain conditions (which are usually fulfilled for [[well-behaved]] spaces, such as [[CW complex]]es) under which the [[fundamental group]] of the wedge sum of two spaces ''X'' and ''Y'' is the [[free product]] of the fundamental groups of ''X'' and ''Y''.\n\n==See also==\n*[[Smash product]]\n*[[Hawaiian earring]], a topological space resembling, but not the same as, a wedge sum of countably many circles\n\n==References==\n\n* Rotman, Joseph. ''An Introduction to Algebraic Topology'', Springer, 2004, p.&nbsp;153. {{ISBN|0-387-96678-1}}\n\n{{DEFAULTSORT:Wedge Sum}}\n[[Category:Topology]]\n[[Category:Binary operations]]\n[[Category:Homotopy theory]]"
    },
    {
      "title": "Wreath product",
      "url": "https://en.wikipedia.org/wiki/Wreath_product",
      "text": "{{Group theory sidebar |Basics}}\n\nIn [[group theory]], the '''wreath product''' is a specialized product of two groups, based on a [[semidirect product]]. Wreath products are used in the classification of [[permutation group]]s and also provide a way of constructing interesting examples of groups.\n\nGiven two groups ''A'' and ''H'', there exist two variations of the wreath product: the '''unrestricted wreath product''' ''A''&nbsp;Wr&nbsp;''H'' (also written ''A''≀''H'') and the '''restricted wreath product''' ''A'' wr ''H''. Given a set Ω with an [[Group action (mathematics)|''H''-action]] there exists a generalization of the wreath product which is denoted by ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' or ''A''&nbsp;wr<sub>Ω</sub>&nbsp;''H'' respectively.\n\nThe notion generalizes to [[semigroup]]s and is a central construction in the [[Krohn-Rhodes theory|Krohn–Rhodes structure theory]] of finite semigroups.\n\n== Definition ==\n\nLet ''A'' and ''H'' be groups and Ω a set with ''H'' [[Group action (mathematics)|acting]] on it. Let ''K'' be the [[Direct product of groups|direct product]]\n\n: <math>K \\equiv \\prod_{\\omega \\,\\in\\, \\Omega} A_\\omega</math>\n\nof copies of ''A''<sub>ω</sub> := ''A'' indexed by the set Ω. The elements of ''K'' can be seen as arbitrary sequences (''a''<sub>ω</sub>) of elements of ''A'' indexed by Ω with component-wise multiplication. Then the action of ''H'' on Ω extends in a natural way to an action of ''H'' on the group ''K'' by\n\n: <math> h (a_\\omega) \\equiv (a_{h^{-1}\\omega})</math>.\n\nThen the '''unrestricted wreath product''' ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' of ''A'' by ''H'' is the [[semidirect product]] ''K''&nbsp;⋊&nbsp;''H''. The subgroup ''K'' of ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' is called the '''base''' of the wreath product.\n\nThe '''restricted wreath product''' ''A''&nbsp;wr<sub>Ω</sub>&nbsp;''H'' is constructed in the same way as the unrestricted wreath product except that one uses the [[Direct sum of groups|direct sum]]\n\n: <math>K \\equiv \\bigoplus_{\\omega \\,\\in\\, \\Omega} A_\\omega</math>\n\nas the base of the wreath product. In this case the elements of ''K'' are sequences (''a''<sub>ω</sub>) of elements in ''A'' indexed by Ω of which all but finitely many ''a''<sub>ω</sub> are the [[identity element]] of ''A''.\n\nIn the most common case, one takes Ω&nbsp;:=&nbsp;''H'', where ''H'' [[Group action (mathematics)|acts]] in a natural way on itself by left multiplication. In this case, the unrestricted and restricted wreath product may be denoted by ''A''&nbsp;Wr&nbsp;''H'' and ''A''&nbsp;wr&nbsp;''H'' respectively. This is called the '''regular''' wreath product.\n\n== Notation and conventions ==\n\nThe structure of the wreath product of ''A'' by ''H'' depends on the ''H''-set Ω and in case Ω is infinite it also depends on whether one uses the restricted or unrestricted wreath product. However, in literature the notation used may be deficient and one needs to pay attention to the circumstances.\n\n* In literature ''A''≀<sub>Ω</sub>''H'' may stand for the unrestricted wreath product ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' or the restricted wreath product ''A''&nbsp;wr<sub>Ω</sub>&nbsp;''H''.\n* Similarly, ''A''≀''H'' may stand for the unrestricted regular wreath product ''A''&nbsp;Wr&nbsp;''H'' or the restricted regular wreath product ''A''&nbsp;wr&nbsp;''H''.\n* In literature the ''H''-set Ω may be omitted from the notation even if Ω≠H.\n* In the special case that ''H''&nbsp;=&nbsp;''S''<sub>''n''</sub> is the [[symmetric group]] of degree ''n'' it is common in the literature to assume that Ω={1,...,''n''} (with the natural action of ''S''<sub>''n''</sub>) and then omit Ω from the notation. That is, ''A''≀''S''<sub>n</sub> commonly denotes ''A''≀<sub>{1,...,''n''}</sub>''S''<sub>''n''</sub> instead of the regular wreath product ''A''≀<sub>''S''<sub>''n''</sub></sub>''S''<sub>''n''</sub>. In the first case the base group is the product of ''n'' copies of ''A'', in the latter it is the product of [[Factorial|''n''!]] copies of ''A''.\n\n== Properties ==\n\n* Since the finite direct product is the same as the finite direct sum of groups, it follows that the unrestricted ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' and the restricted wreath product ''A''&nbsp;wr<sub>Ω</sub>&nbsp;''H'' agree if the ''H''-set Ω is finite. In particular this is true when Ω = ''H'' is finite.\n* ''A''&nbsp;wr<sub>Ω</sub>&nbsp;''H'' is always a [[subgroup]] of ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H''.\n* Universal Embedding Theorem: If ''G'' is an [[Group extension|extension]] of ''A'' by ''H'', then there exists a subgroup of the unrestricted wreath product ''A''≀''H'' which is isomorphic to ''G''.<ref>M. Krasner and L. Kaloujnine, \"Produit complet des groupes de permutations et le problème d'extension de groupes III\", Acta Sci. Math. Szeged 14, pp. 69-82 (1951)</ref> This is also known as the ''Krasner–Kaloujnine embedding theorem''. The [[Krohn–Rhodes theorem]] involves what is basically the semigroup equivalent of this.<ref name=\"Meldrum1995\">{{cite book|author=J D P Meldrum|title=Wreath Products of Groups and Semigroups|year=1995|publisher=Longman [UK] / Wiley [US]|isbn=978-0-582-02693-3|page=ix}}</ref>\n* If ''A'', ''H'' and Ω are finite, then\n\n:: |''A''≀<sub>Ω</sub>''H''| = |''A''|<sup>|Ω|</sup>|''H''|.<ref>Joseph J. Rotman, An Introduction to the Theory of Groups, p. 172 (1995)</ref>\n\n== Canonical actions of wreath products ==\n\nIf the group ''A'' acts on a set Λ then there are two canonical ways to construct sets from Ω and Λ on which ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' (and therefore also ''A''&nbsp;wr<sub>Ω</sub>&nbsp;''H'') can act.\n\n* The '''imprimitive''' wreath product action on Λ×Ω.\n\n: If ((''a''<sub>ω</sub>),''h'')∈''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' and (λ,ω')∈Λ×Ω, then\n\n:: <math>((a_{\\omega}), h) \\cdot (\\lambda,\\omega') := (a_{h(\\omega')}\\lambda, h\\omega')</math>.\n\n* The '''primitive''' wreath product action on Λ<sup>Ω</sup>.\n\n: An element in Λ<sup>Ω</sup> is a sequence (λ<sub>ω</sub>) indexed by the ''H''-set Ω. Given an element ((''a''<sub>ω</sub>), ''h'') ∈ ''A''&nbsp;Wr<sub>Ω</sub>&nbsp;''H'' its operation on (λ<sub>ω</sub>)∈Λ<sup>Ω</sup> is given by\n\n:: <math>((a_\\omega), h) \\cdot (\\lambda_\\omega) :=  (a_{h^{-1}\\omega}\\lambda_{h^{-1}\\omega})</math>.\n\n== Examples ==\n\n* The [[Lamplighter group]] is the restricted wreath product ℤ<sub>2</sub>≀ℤ.\n* {{math|ℤ<sub>''m''</sub>≀''S''<sub>''n''</sub>}} ([[Generalized symmetric group]]).\n\n: The base of this wreath product is the ''n''-fold direct product\n\n:: ℤ<sub>''m''</sub><sup>''n''</sup> = ℤ<sub>''m''</sub> × ... × ℤ<sub>''m''</sub>\n\n: of copies of ℤ<sub>''m''</sub> where the action φ&nbsp;:&nbsp;''S''<sub>''n''</sub> → Aut(ℤ<sub>''m''</sub><sup>''n''</sup>) of the [[symmetric group]] ''S''<sub>''n''</sub> of degree ''n'' is given by\n\n:: φ(σ)(α<sub>1</sub>,..., α<sub>''n''</sub>) := (α<sub>σ(1)</sub>,..., α<sub>σ(''n'')</sub>).<ref>J. W. Davies and A. O. Morris, \"The Schur Multiplier of the Generalized Symmetric Group\", J. London Math. Soc (2), 8, (1974), pp. 615-620</ref>\n\n* ''S''<sub>2</sub>≀''S''<sub>''n''</sub> ([[Hyperoctahedral group]]).\n\n: The action of ''S''<sub>''n''</sub> on {1,...,''n''} is as above. Since the symmetric group ''S''<sub>2</sub> of degree 2 is [[Group isomorphism|isomorphic]] to ℤ<sub>2</sub> the hyperoctahedral group is a special case of a generalized symmetric group.<ref>P. Graczyk, G. Letac and H. Massam, \"The Hyperoctahedral Group, Symmetric Group Representations and the Moments of the Real Wishart Distribution\",  J. Theoret. Probab.  18  (2005),  no. 1, 1-42.</ref>\n\n* The smallest non-trivial wreath product is ℤ<sub>2</sub>≀ℤ<sub>2</sub>, which is the two-dimensional case of the above hyperoctahedral group. It is the symmetry group of the square, also called ''Dih''<sub>4</sub>, the [[dihedral group]] of order 8.\n* Let ''p'' be a [[Prime number|prime]] and let ''n''≥1. Let ''P'' be a [[Sylow theorems|Sylow ''p''-subgroup]] of the symmetric group ''S''<sub>''p''<sup>''n''</sup></sub> of degree ''p''<sup>''n''</sup>. Then ''P'' is [[Group isomorphism|isomorphic]] to the iterated regular wreath product ''W''<sub>''n''</sub> = ℤ<sub>''p''</sub> ≀ ℤ<sub>''p''</sub>≀...≀ℤ<sub>''p''</sub> of ''n'' copies of ℤ<sub>''p''</sub>. Here ''W''<sub>1</sub> := ℤ<sub>''p''</sub> and ''W''<sub>''k''</sub> := ''W''<sub>''k''-1</sub>≀ℤ<sub>''p''</sub> for all ''k''≥2.<ref>Joseph J. Rotman, An Introduction to the Theory of Groups, p. 176 (1995)</ref><ref>L. Kaloujnine, \"La structure des p-groupes de Sylow des groupes symétriques finis\", Annales Scientifiques de l'École Normale Supérieure. Troisième Série 65, pp. 239–276 (1948)</ref> For instance, the Sylow 2-subgroup of S<sub>4</sub> is the above ℤ<sub>2</sub>≀ℤ<sub>2</sub> group.\n* The [[Rubik's Cube group]] is a subgroup of index 12 in the product of wreath products, (ℤ<sub>3</sub>≀''S''<sub>8</sub>)&nbsp;× (ℤ<sub>2</sub>≀''S''<sub>12</sub>), the factors corresponding to the symmetries of the 8 corners and 12 edges.\n\n== References ==\n\n{{Reflist}}\n\n== External links ==\n* [http://www.encyclopediaofmath.org/index.php?title=Wreath_product&oldid=35297 Wreath product] in ''[[Encyclopedia of Mathematics]]''. \n* [http://www.abstractmath.org/Papers/SAWPCWC.pdf Some Applications of the Wreath Product Construction]. {{webarchive |url=https://web.archive.org/web/20140221081427/http://www.abstractmath.org/Papers/SAWPCWC.pdf |date=21 February 2014}}\n\n[[Category:Group theory]]\n[[Category:Permutation groups]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Difference engine",
      "url": "https://en.wikipedia.org/wiki/Difference_engine",
      "text": "{{For|the novel by William Gibson and Bruce Sterling|The Difference Engine}}\n[[File:Babbage Difference Engine.jpg|thumb|320px| The London [[Science Museum (London)|Science Museum]]'s difference engine, the first one actually built from Babbage's design. The design has the same precision on all columns, but when calculating polynomials, the precision on the higher-order columns could be lower.]]\n\nA '''difference engine''', first created by [[Charles Babbage]], is an automatic [[mechanical calculator]] designed to tabulate [[polynomial|polynomial functions]]. Its name is derived from the method of [[divided differences]], a way to interpolate or tabulate functions by using a small set of [[polynomial]] coefficients. Most [[mathematical function]]s commonly used by engineers, scientists and navigators, including [[logarithm]]ic and [[trigonometric functions]], can be [[Taylor series|approximated]] by polynomials, so a difference engine can compute many useful [[mathematical table|tables of numbers]].\n\nThe historical difficulty in producing error-free tables by teams of mathematicians and [[Human computer|human \"computers\"]] spurred [[Charles Babbage]]'s desire to build a mechanism to automate the process.\n\n== History ==\n{{Wikisource|Astronomische Nachrichten/Volume 46/On Mr. Babbage's new machine for calculating and printing mathematical and astronomical tables}}\n[[File:LondonScienceMuseumsReplicaDifferenceEngine.jpg|thumb|Closeup of the London Science Museum's difference engine showing some of the number wheels and the sector gears between columns. The sector gears on the left show the double-high teeth very clearly. The sector gears on the middle-right are facing the back side of the engine, but the single-high teeth are clearly visible. Notice how the wheels are mirrored, with counting up from left-to-right, or counting down from left-to-right. Also notice the metal tab between \"6\" and \"7\". That tab trips the carry lever in the back when \"9\" passes to \"0\" in the front during the add steps (Step 1 and Step 3).]]\n[[File:Difference engine Scheutz.jpg|thumb|right|250px|Per Georg Scheutz's third difference engine]]\nThe notion of a [[mechanical calculator]] for mathematical functions can be traced back to the [[Antikythera mechanism]] of the 2nd century BC, while early modern examples are attributed to [[Blaise Pascal|Pascal]] and [[Gottfried Leibniz|Leibniz]] in the 17th century. \nIn 1784 [[J. H. Müller]], an engineer in the [[Hessian (soldier)|Hessian]] army, devised and built an [[adding machine]] and described the basic principles of a difference machine in a book published in 1786 (the first written reference to a difference machine is dated to 1784), but he was unable to obtain funding to progress with the idea.<ref>Johann Helfrich von Müller, ''Beschreibung seiner neu erfundenen Rechenmachine, nach ihrer Gestalt, ihrem Gebrauch und Nutzen'' [Description of his newly invented calculating machine, according to its form, its use and benefit] (Frankfurt and Mainz, Germany:  Varrentrapp Sohn & Wenner, 1786); pages 48–50.  The following Web site (in German) contains detailed photos of Müller's calculator as well as a transcription of Müller's booklet, ''Beschreibung …'':  https://www.fbi.h-da.de/fileadmin/vmi/darmstadt/objekte/rechenmaschinen/mueller/index.htm<nowiki/>.  An animated simulation of Müller's machine in operation is available on this Web site (in German):  https://www.fbi.h-da.de/fileadmin/vmi/darmstadt/objekte/rechenmaschinen/mueller/simulation/index.htm<nowiki/>.</ref><ref>Michael Lindgren (Craig G. McKay, trans.), ''Glory and Failure:  The Difference Engines of Johann Müller, Charles Babbage, and Georg and Edvard Scheutz'' (Cambridge, Massachusetts:  MIT Press, 1990), [https://books.google.com/books?id=plgMl2yfVkwC&pg=PA64 pages 64 ff].</ref><ref>{{cite book|url=https://books.google.com/books?id=c1QbNtTz4CYC&pg=PA14|title=Computers: The Life Story of a Technology|author1=Swedin, E.G.|author2=Ferro, D.L.|publisher=Greenwood Press, Westport, CT|year=2005|isbn=978-0-313-33149-7|page=14}}</ref>\n\n===Charles Babbage's difference engines===\n[[Charles Babbage]] began to construct a small difference engine in c. 1819<ref>{{Cite book|url=https://books.google.com/books?id=tXBVAgAAQBAJ&pg=PT22|title=It Began with Babbage: The Genesis of Computer Science|last=Dasgupta|first=Subrata|date=2014|publisher=Oxford University Press|isbn=978-0-19-930943-6|page=22}}</ref> and had completed it by 1822 (Difference Engine 0).<ref name=\":0\">{{Cite book| title=The Turing Guide | authorlink1=Jack Copeland | last1=Copeland |first1=B. Jack | authorlink2=Jonathan Bowen | last2=Bowen | first2=Jonathan P. | authorlink3=Robin Wilson (mathematician) | last3=Wilson | first3=Robin | last4=Sprevak | first4=Mark |date=2017 |publisher=[[Oxford University Press]] | isbn=9780191065002| location=|page=251|language=en| title-link=The Turing Guide }}</ref> He announced his invention on 14 June 1822, in a paper to the [[Royal Astronomical Society]], entitled \"Note on the application of machinery to the computation of astronomical and mathematical tables\".<ref>{{cite web\n |author      = O'Connor, John J.\n |author2     = Robertson, Edmund F.\n |authorlink2 = Edmund F. Robertson\n |year        = 1998\n |url         = http://www-gap.dcs.st-and.ac.uk/~history/Mathematicians/Babbage.html\n |title       = Charles Babbage\n |work        = MacTutor History of Mathematics archive\n |publisher   = School of Mathematics and Statistics, University of St Andrews, Scotland\n |accessdate  = 2006-06-14\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20060616002258/http://www-gap.dcs.st-and.ac.uk/~history/Mathematicians/Babbage.html\n |archivedate = 2006-06-16\n}}</ref> This machine used the decimal number system and was powered by cranking a handle. The [[British government]] was interested, since producing tables was time-consuming and expensive and they hoped the difference engine would make the task more economical.<ref name=\"Campbell-Kelly 2004\">{{cite book|title=Computer: A History of the Information Machine 2nd ed.|last=Campbell-Kelly|first=Martin|publisher=Westview Press|year=2004|isbn=978-0-8133-4264-1|location=Boulder, Co|pages=|author-link=Martin Campbell-Kelly}}</ref>\n\nIn 1823, the British government gave Babbage £1700 to start work on the project. Although Babbage's design was feasible, the metalworking techniques of the era could not economically make parts in the precision and quantity required. Thus the implementation proved to be much more expensive and doubtful of success than the government's initial estimate. In 1832, Babbage and [[Joseph Clement]] produced a small working model (one-seventh of the calculating section of Difference Engine No. 1,<ref name=\":0\" /> which was intended to operate on 20-digit numbers and sixth-order differences) which operated on 6-digit numbers and second-order differences.<ref name=\":1\">{{Cite book|url=https://books.google.com/books?id=QqrItgm351EC&pg=PA204|title=A Brief History of Computing|last=O'Regan|first=Gerard|date=2012|publisher=Springer Science & Business Media|isbn=978-1-4471-2359-0|page=204}}</ref><ref name=\":2\">{{Cite book|url=https://books.google.com/books?id=JckCvpOQDOoC&pg=PP1|title=The Philosophical Breakfast Club: Four Remarkable Friends Who Transformed Science and Changed the World|last=Snyder|first=Laura J.|date=2011|publisher=Crown/Archetype|isbn=978-0-307-71617-0|pages=192, 210, 217}}</ref> [[Lady Byron]] described seeing the working prototype in 1833: \"We both went to see the thinking machine (for so it seems) last Monday. It raised several Nos. to the 2nd and 3rd powers, and extracted the root of a Quadratic equation.\"<ref>{{Cite book|title=Ada, the Enchantress of Numbers|last1=Toole|first1=Betty Alexandra|last2=Lovelace|first2=Ada|date=1998|publisher=Strawberry Press|isbn=978-0912647180|location=Mill Valley, California|oclc=40943907|page=38}}</ref> Work on the larger engine was suspended in 1833.\n\nBy the time the government abandoned the project in 1842,<ref name=\":2\" /><ref>{{Cite book|url=https://books.google.com/books?id=UmNJAAAAYAAJ&pg=PA387|title=A History of the Royal Society: With Memoirs of the Presidents|last=Weld|first=Charles Richard|date=1848|publisher=J. W. Parker|pages=387–390}}</ref> Babbage had received and spent over £17,000 on development, which still fell short of achieving a working engine. The government valued only the machine's output (economically produced tables), not the development (at unknown and unpredictable cost to complete) of the machine itself.  Babbage did not, or was unwilling to, recognize that predicament.<ref name=\"Campbell-Kelly 2004\" /> Meanwhile, Babbage's attention had moved on to developing an [[analytical engine]], further undermining the government's confidence in the eventual success of the difference engine. By improving the concept as an analytical engine, Babbage had made the difference engine concept obsolete, and the project to implement it an utter failure in the view of the government.<ref name=\"Campbell-Kelly 2004\" />\n\nThe incomplete Difference Engine No. 1 was put on display to the public at the [[1862 International Exhibition]] in [[South Kensington]], London.<ref>{{Cite book|url=https://books.google.com/books?id=aDJRAAAAYAAJ|title=Cyclopaedia of useful arts, mechanical and chemical, manufactures, mining and engineering: in three volumes, illustrated by 63 steel engravings and 3063 wood engravings|last=Tomlinson|first=Charles|date=1868|publisher=Virtue & Co.|pages=136}}</ref><ref>{{Cite book|url=https://books.google.com/books?id=j_QHAAAAQAAJ&pg=PA49|title=Official catalogue of the industrial department|last=1862|first=International exhibition|date=1862|page=49}}</ref>\n\nBabbage went on to design his much more general [[Analytical Engine|analytical engine]], but later produced an improved \"Difference Engine No. 2\" design (31-digit numbers and seventh-order differences),<ref name=\":1\" /> between 1846 and 1849. Babbage was able to take advantage of ideas developed for the analytical engine to make the new difference engine calculate more quickly while using fewer parts.<ref>{{cite book|title=The Philosophical Breakfast Club|last=Snyder|first=Laura J.|publisher=Broadway Brooks|year=2011 |isbn=978-0-7679-3048-2|location=New York}}</ref><ref>{{Cite book |title=The Dawn of Innovation: The First American Industrial Revolution |last=Morris |first=Charles R. |url=https://books.google.com/books?id=hRQBAwAAQBAJ&pg=PA63 |date=October 23, 2012 |publisher=PublicAffairs |isbn=9781610393577 |page=63|language=en}}</ref>\n\n=== Scheutzian calculation engine ===\nInspired by Babbage's difference engine in 1834, [[Per Georg Scheutz]] built several experimental models. In 1837 his son Edward proposed to construct a working model in metal, and in 1840 finished the calculating part, capable of calculating series with 5-digit numbers and first-order differences, which was later extended to third-order (1842). In 1843, after adding the printing part, the model was completed.\n\nIn 1851, funded by the government, construction of the larger and improved (15-digit numbers and fourth-order differences) machine began, and finished in 1853. The machine was demonstrated at the [[Exposition Universelle (1855)|World's Fair in Paris, 1855]] and then sold in 1856 to the [[Dudley Observatory]] in [[Albany, New York]]. Delivered in 1857, it was the first printing calculator sold.<ref name=\":3\">{{Cite book|url=https://books.google.com/books?id=Ut1wgt6kSBEC&pg=PA12|title=Specimens of Tables, Calculated, Stereomoulded, and Printed by Machinery|last=Scheutz|first=George|last2=Scheutz|first2=Edward|date=1857|publisher=Whitnig|pages=VIII–XII, XIV–XV, 3}}</ref><ref name=SI>{{cite web |title=Scheutz Difference Engine |url=https://americanhistory.si.edu/collections/search/object/nmah_997042 |website=Smithsonian National Museum of American History |accessdate=June 14, 2019}}</ref>.<ref name=\":6\" /> In 1857 British government ordered next [[Per Georg Scheutz|Scheutz's]] difference machine, which was built in 1859.<ref>{{Cite book|url=https://books.google.com/books?id=b7rvAAAAMAAJ|title=The Difference Engine: Charles Babbage and the Quest to Build the First Computer|last=Swade|first=Doron|date=2002-10-29|publisher=Penguin Books|isbn=9780142001448|location=|pages=4,207|language=en}}</ref><ref>{{Cite book|url=https://books.google.com/books?id=jlmVKZ1psCkC&pg=PA37|title=The Universal Machine: From the Dawn of Computing to Digital Consciousness|last=Watson|first=Ian|date=2012|publisher=Springer Science & Business Media|isbn=978-3-642-28102-0|pages=37–38}}</ref> It had the same basic construction as the previous one. Weighed about {{Convert|10|-Lcwt|lb kg|lk=on|abbr=on}}.<ref name=\":6\">{{Cite book|title=First Printing Calculator|last=Merzbach|first=Uta C.|authorlink= Uta Merzbach |last2=Zoology|first2=Smithsonian Contributions To|last3=Ripley|first3=S. Dillon|last4=Merzbach|first4=Uta C.|publisher=|year=|isbn=|location=|pages=8–9, 13, 25–26, 29–30|citeseerx = 10.1.1.639.3286}}</ref>\n\n=== Others ===\n[[Martin Wiberg]] improved Scheutz's construction (c. 1859, his machine has the same capacity as Scheutz's - 15-digit and fourth-order) but used his device only for producing and publishing printed tables (interest tables in 1860, and [[logarithm]]ic tables in 1875).<ref>Raymond Clare Archibald: ''Martin Wiberg, his Table and Difference Engine'', Mathematical Tables and Other Aids to Computation, 1947(2:20) 371–374. [http://www.ams.org/journals/mcom/1947-02-020/S0025-5718-47-99566-5/S0025-5718-47-99566-5.pdf (online review)] (PDF; 561&nbsp;kB).</ref>\n\nAlfred Deacon of London in c. 1862 produced a small difference engine (20-digit numbers and third-order differences).<ref name=\":3\" /><ref name=\":4\">{{Cite book|url=https://books.google.com/books?id=O170gWPZ7M8C&pg=PA136|title=The History of Mathematical Tables: From Sumer to Spreadsheets|last=Campbell-Kelly|first=Martin|date=2003|publisher=OUP Oxford|isbn=978-0-19-850841-0|pages=132–136}}</ref>\n\nAmerican [[George B. Grant]] started working on his calculating machine in 1869, unaware of the works of Babbage and Scheutz (Schentz). One year later (1870) he learned about difference engines and proceed to design one himself, describing his construction in 1871. In 1874 the Boston Thursday Club raised a subscription for the construction of a large-scale model, which was built in 1876. It could be expanded to enhance precision, weighed about {{convert|2000|lb|kg}}.<ref name=\":4\" /><ref>{{Cite web|url=http://history-computer.com/Babbage/NextDifferentialEngines/Grant.html|title=History of Computers and Computing, Babbage, Next differential engines, George Grant|website=history-computer.com|access-date=2017-08-29}}</ref><ref>{{Cite book|url=https://books.google.com/books?id=LFQ9AQAAIAAJ&pg=PA423|title=The Great Centennial Exhibition Critically Described and Illustrated|last=Sandhurst|first=Phillip T.|date=1876|publisher=P. W. Ziegler & Company|pages=423, 427}}</ref>\n\n[[Christel Hamann]] built one machine (16-digit numbers and second-order differences) in 1909 for the \"Tables of [[Julius Bauschinger|Bauschinger]] and Peters\" (\"Logarithmic-Trigonometrical Tables with eight decimal places\"), which was first published in Leipzig in 1910. Weighed about {{convert|40|kg|lb}}.<ref>{{Cite web|url=http://history-computer.com/Babbage/NextDifferentialEngines/Hamann.html|title=History of Computers and Computing, Babbage, Next differential engines, Hamann|website=history-computer.com|access-date=2017-09-14}}</ref><ref>{{Cite book|url=https://books.google.com/books?id=vD7PAAAAMAAJ|title=Logarithmisch-trigonometrische Tafeln mit acht Dezimalstellen, enthaltend die Logarithmen aller Zahlen von 1 bis 200000 und die Logarithmen der trigonometrischen Funktionen f\"ur jede Sexagesimalsekunde des Quadranten: Bd. Tafel der achtstelligen Logarithmen aller Zahlen von 1 bis 200000|last=Bauschinger|first=Julius|last2=Peters|first2=Jean|date=1958|publisher=H. R. Engelmann|pages=Preface V–VI}}</ref><ref>{{Cite book|url=https://archive.org/details/logarithmischtri01bausuoft|title=Logarithmisch-trigonometrische Tafeln, mit acht Dezimalstellen, enthaltend die Logarithmen aller Zahlen von 1 bis 200000 und die Logarithmen der trigonometrischen Funktionen für jede Sexagesimalsekunde des Quadranten. Neu berechnet und hrsg. von J. Bauschinger und J. Peters. Stereotypausg|last=Bauschinger|first=Julius|last2=Peters|first2=J. (Jean)|date=1910|publisher=Leipzig W. Englemann|others=Gerstein - University of Toronto|pages=Einleitung VI|language=German}}</ref>\n\n[[Burroughs Corporation]] in about 1912 built a machine for [[HM Nautical Almanac Office|Nautical Almanac Office]] which was used as a difference engine of second-order.<ref name=\":5\">{{Cite journal|last=Comrie|first=L. J.|date=1928-03-01|title=On the application of the BrunsvigaDupla calculating machine to double summation with finite differences|bibcode=1928MNRAS..88..447C|journal=Monthly Notices of the Royal Astronomical Society|volume=88|issue=5|pages=451, 453–454, 458–459|doi=10.1093/mnras/88.5.447|issn=0035-8711|via=[[Astrophysics Data System]]}}</ref>{{Rp|451}}<ref>{{Cite book|url=https://archive.org/stream/moderninstrument00horsuoft#page/127/mode/1up/search/hudson|title=Modern instruments and methods of calculation : a handbook of the Napier Tercentenary Exhibition|last=Horsburg|first=E. M. (Ellice Martin)|last2=Napier Tercentenary Exhibition|date=1914|publisher=London : G. Bell|others=Gerstein - University of Toronto|isbn=|location=|pages=127–131}}</ref> It was later replaced in 1929 by a Burroughs Class 11 (13-digit numbers and second-order differences, or 11-digit numbers and <nowiki>[at least up to]</nowiki> fifth-order differences).<ref>{{Cite journal|last=Comrie|first=L. J.|date=1932-04-01|title=The Nautical Almanac Office Burroughs machine|bibcode=1932MNRAS..92..523C|journal=Monthly Notices of the Royal Astronomical Society|volume=92|issue=6|pages=523–524, 537–538|doi=10.1093/mnras/92.6.523|issn=0035-8711|via=[[Astrophysics Data System]]}}</ref>\n\n[[Alexander John Thompson]] about 1927 built ''integrating and differencing machine'' (13-digit numbers and fifth-order differences) for his table of logarithms \"Logarithmetica britannica\". This machine was composed of four modified Triumphator calculators.<ref>{{Cite book|url=https://books.google.pl/books?id=fH48AAAAIAAJ|title=Logarithmetica Britannica: Being a Standard Table of Logarithms to Twenty Decimal Places|last=Thompson|first=Alexander John|date=1924|publisher=CUP Archive|isbn=9781001406893|location=|pages=V/VI, XXIX, LIV–LVI, LXV (archive: pp. 7, 30, 55–59, 68)|language=en|archive-url=https://archive.org/stream/in.ernet.dli.2015.285845/2015.285845.Logarithmetica-Britannica#page/n29/mode/1up/search/Integrating+and+Differencing+machine|archive-date=2015-08-06|dead-url=no}}</ref><ref>{{Cite web|url=https://history-computer.com/Babbage/NextDifferentialEngines/Thompson.html|title=History of Computers and Computing, Babbage, Next differential engines, Alexander John Thompson|website=history-computer.com|access-date=2017-09-22}}</ref><ref>{{Cite web|url=http://mechrech.info/publikat/publikat.html#pub70|title=Publikationen|last=Weiss|first=Stephan|date=|website=mechrech.info|pages=160–163|others=''Difference Engines in the 20th Century''. First published in Proceedings 16th International Meeting of Collectors of Historical Calculating Instruments, Sep. 2010, Leiden|access-date=2017-09-22}}</ref>\n\n[[Leslie Comrie]] in 1928 described how to use the [[Odhner Arithmometer|Brunsviga]]-Dupla calculating machine as a difference engine of second-order (15-digit numbers).<ref name=\":5\" /> He also noted in 1931 that National Accounting Machine Class 3000 could be used as a difference engine of sixth-order.<ref name=\":4\" />{{Rp|137–138}}\n\n===Construction of two working No. 2 difference engines===\nDuring the 1980s, [[Allan G. Bromley]], an associate professor at the [[University of Sydney]], [[Australia]], studied Babbage's original drawings for the Difference and Analytical Engines at the [[Science Museum (London)|Science Museum]] library in London.<ref>[http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/an/&toc=comp/mags/an/2000/04/a4toc.xml ''IEEE Annals of the History of Computing'', 22(4), October–December 2000].</ref>  This work led the Science Museum to construct a working calculating section of difference engine No. 2 from 1985 to 1991, under [[Doron Swade]], the then Curator of Computing. This was to celebrate the 200th anniversary of Babbage's birth in 1999. In 2002, the [[Printer (computing)|printer]] which Babbage originally designed for the difference engine was also completed.<ref>{{cite web |title=A Modern Sequel {{!}}  Babbage Engine {{!}} Computer History Museum |url=http://www.computerhistory.org/babbage/modernsequel/ |website=www.computerhistory.org |language=en}}</ref> The conversion of the original design drawings into drawings suitable for engineering manufacturers' use revealed some minor errors in Babbage's design (possibly introduced as a protection in case the plans were stolen),<ref>[http://news.bbc.co.uk/1/hi/sci/tech/710950.stm Babbage printer finally runs, BBC news quoting Reg Crick] Accessed May 17, 2012</ref> which had to be corrected. Once completed, both the engine and its printer worked flawlessly, and still do. The difference engine and printer were constructed to tolerances achievable with 19th-century technology, resolving a long-standing debate as to whether Babbage's design would actually have worked. (One of the reasons formerly advanced for the non-completion of Babbage's engines had been that engineering methods were insufficiently developed in the Victorian era.)\n\nThe printer's primary purpose is to produce [[Stereotype (printing)|stereotype]] plates for use in printing presses, which it does by pressing type into soft plaster to create a [[flong]]. Babbage intended that the Engine's results be conveyed directly to mass printing, having recognized that many errors in previous tables were not the result of human calculating mistakes but from error in the manual [[typesetting]] process.<ref name=\"Campbell-Kelly 2004\" /> The printer's paper output is mainly a means of checking the Engine's performance.\n\nIn addition to funding the construction of the output mechanism for the Science Museum's Difference Engine, [[Nathan Myhrvold]] commissioned the construction of a second complete Difference Engine No. 2, which was on exhibit at the [[Computer History Museum]] in [[Mountain View, California]] from 10 May 2008 until 31 January 2016.<ref>{{Cite web|url=http://www.computerhistory.org/exhibits/babbage/|title=The Babbage Difference Engine No. 2 {{!}} Computer History Museum|website=www.computerhistory.org|language=en|access-date=2018-10-26}}</ref><ref name=\"chm2\">{{cite web\n|url=http://news.cnet.com/8301-13772_3-9915667-52.html\n|title=Charles Babbage's masterpiece difference engine comes to Silicon Valley\n|author=[[Daniel Terdiman|Terdiman, Daniel]]\n|work=[[CNET News]]\n|date=April 9, 2008\n}}</ref><ref>Press Releases {{!}} Computer History\n*{{Cite web|url=http://www.computerhistory.org/press/babbage-engine-exhibit.html|title=The Computer History Museum Debuts Charles Babbage's Difference Engine No. 2, On Display for the First Time in North America {{!}} Press Releases {{!}} Computer History Museum|last=|first=|date=2008-05-05|website=www.computerhistory.org|language=en|archive-url=|archive-date=|dead-url=|access-date=2018-10-27}}\n*{{cite web|url=http://www.computerhistory.org/press/babbage-engine-extension.html|title=The Computer History Museum Extends Its Exhibition of Babbage's Difference Engine No. 2|last=|first=|date=March 31, 2009|work=press release|publisher=[[Computer History Museum]]|archive-url=https://web.archive.org/web/20160103015136/http://www.computerhistory.org/press/babbage-engine-extension.html|archive-date=2016-01-03|dead-url=|accessdate=2009-11-06}}</ref><ref>[http://www.mv-voice.com/news/2016/01/29/computer-museum-bids-farewell-to-babbage-engine Difference Engine Leaves Computer History Museum], Mark Moack, Mountain View Voice, January 29, 2016</ref>\nIt has since been transferred to [[Intellectual Ventures]] in [[Seattle]] where it is on display just outside the main lobby.\n\n== Operation ==\n{{unreferenced section|date=June 2017}}\n[[File:Babbage Engine Demonstration pt. 3.webm|thumb|The Mountain View machine in action]]\n\nThe difference engine consists of a number of columns, numbered from '''1''' to '''''N'''''. The machine is able to store one decimal number in each column. The machine can only add the value of a column ''n''&nbsp;+&nbsp;1 to column ''n'' to produce the new value of ''n''. Column ''N'' can only store a [[wiktionary:Constant|constant]], column 1 displays (and possibly [[Computer printer|prints]]) the value of the calculation on the current [[iteration]].\n\nThe engine is programmed by setting initial values to the columns. Column 1 is set to the value of the polynomial at the start of computation. Column 2 is set to a value derived from the first and higher [[derivative]]s of the polynomial at the same value of '''''X'''''. Each of the columns from 3 to ''N'' is set to a value derived from the <math>(n-1)</math> first and higher derivatives of the polynomial.\n\n=== Timing ===\nIn the Babbage design, one iteration (i.e., one full set of addition and [[carry (arithmetic)|carry]] operations) happens for each rotation of the main shaft. Odd and even columns alternately perform an addition in one cycle. The sequence of operations for column <math>n</math> is thus:\n\n# Count up, receiving the value from column <math>n+1</math> (Addition step)\n# Perform [[carry propagation]] on the counted up value\n# Count down to zero, adding to column <math>n-1</math>\n# Reset the counted-down value to its original value\n\nSteps 1,2,3,4 occur for every odd column, while steps 3,4,1,2 occur for every even column.\n\nWhile Babbage's original design placed the crank directly on the main shaft, it was later realized that the force required to crank the machine would have been too great for a human to handle comfortably. Therefore, the two models that were built incorporate a 4:1 reduction gear at the crank, and four revolutions of the crank are required to perform one full cycle.\n\n=== Steps ===\nEach iteration creates a new result, and is accomplished in four steps corresponding to four complete turns of the handle shown at the far right in the picture below. The four steps are:\n\n* Step 1. All even numbered columns (2,4,6,8) are added to all odd numbered columns (1,3,5,7) simultaneously. An interior sweep arm turns each even column to cause whatever number is on each wheel to count down to zero. As a wheel turns to zero, it transfers its value to a sector gear located between the odd/even columns. These values are transferred to the odd column causing them to count up. Any odd column value that passes from \"9\" to \"0\" activates a [[carry (arithmetic)|carry]] lever.\n* Step 2.  [[Carry propagation]] is accomplished by a set of spiral arms in the back that poll the carry levers in a helical manner so that a carry at any level can increment the wheel above by one. That can create a carry, which is why the arms move in a spiral. At the same time, the sector gears are returned to their original position, which causes them to increment the even column wheels back to their original values. The sector gears are double-high on one side so they can be lifted to disengage from the odd column wheels while they still remain in contact with the even column wheels.\n* Step 3. This is like Step 1, except it is odd columns (3,5,7) added to even columns (2,4,6), and column one has its values transferred by a sector gear to the print mechanism on the left end of the engine. Any even column value that passes from \"9\" to \"0\" activates a carry lever. The column 1 value, the result for the polynomial, is sent to the attached printer mechanism.\n* Step 4. This is like Step 2, but for doing carries on even columns, and returning odd columns to their original values.\n\n=== Subtraction ===\nThe engine represents negative numbers as [[Method of complements|ten's complements]]. Subtraction amounts to addition of a negative number. This works in the same manner that modern computers perform subtraction, known as [[two's complement]].\n\n== Method of differences ==\n[[File:Difference engine.JPG|thumb|320px|right|Fully operational difference engine at the [[Computer History Museum]] in [[Mountain View, California]]]]\nThe principle of a difference engine is [[Newton polynomial|Newton's method]] of [[divided differences]]. If the initial value of a polynomial (and of its [[finite difference]]s) is calculated by some means for some value of '''''X''''', the difference engine can calculate any number of nearby values, using the method generally known as the '''method of finite differences'''. For example, consider the quadratic [[polynomial]]\n\n: <math>p(x) = 2x^2 - 3x + 2 \\,</math>\n\nwith the goal of tabulating the values ''p''(0), ''p''(1), ''p''(2), ''p''(3), ''p''(4), and so forth. The table below is constructed as follows: the second column contains the values of the polynomial, the third column contains the differences of the two left neighbors in the second column, and the fourth column contains the differences of the two neighbors in the third column:\n\n{| class=\"wikitable\"\n!x!!''p''(''x'') = 2''x''<sup>2</sup> − 3''x'' + 2!!diff1(''x'') = (&nbsp;''p''(''x''&nbsp;+&nbsp;1)&nbsp;−&nbsp;p(''x'') )!!diff2(''x'') = (&nbsp;diff1(''x''&nbsp;+&nbsp;1)&nbsp;−&nbsp;diff1(''x'')&nbsp;)\n|-\n|0||2||−1||4\n|-\n|1||1||3||4\n|-\n|2||4||7||4\n|-\n|3||11||11||\n|-\n|4||22|| ||\n|}\n\nThe numbers in the third values-column are constant. In fact, by starting with any polynomial of degree ''n'', the column number ''n''&nbsp;+&nbsp;1 will always be constant. This is the crucial fact behind the success of the method.\n\nThis table was built from left to right, but it is possible to continue building it from right to left down a diagonal in order to compute more values.  To calculate ''p''(5) use the values from the lowest diagonal. Start with the fourth column constant value of 4 and copy it down the column. Then continue the third column by adding 4 to 11 to get 15. Next continue the second column by taking its previous value, 22 and adding the 15 from the third column. Thus ''p''(5) is 22&nbsp;+&nbsp;15&nbsp;=&nbsp;37.  In order to compute ''p''(6), we iterate the same algorithm on the ''p''(5) values: take 4 from the fourth column, add that to the third column's value 15 to get 19, then add that to the second column's value 37 to get 56, which is ''p''(6).  This process may be continued [[ad infinitum]]. The values of the polynomial are produced without ever having to multiply. A difference engine only needs to be able to add. From one loop to the next, it needs to store 2 numbers—in this example (the last elements in the first and second columns).  To tabulate polynomials of degree ''n'', one needs sufficient storage to hold ''n'' numbers.\n\nBabbage's difference engine No. 2, finally built in 1991, could hold 8 numbers of 31 decimal digits each and could thus tabulate 7th degree polynomials to that precision. The best machines from Scheutz could store 4 numbers with 15 digits each.<ref>{{Cite book|url=https://books.google.com/books?id=QqrItgm351EC&pg=PA201|title=A Brief History of Computing|last=O'Regan|first=Gerard|date=2012|publisher=Springer Science & Business Media|isbn=978-1-4471-2359-0|page=201}}</ref>\n\n== Initial values ==\nThe initial values of columns can be calculated by first manually calculating N consecutive values of the function and by [[backtracking]], i.e. calculating the required differences.\n\nCol <math>1_0</math> gets the value of the function at the start of computation <math>f(0)</math>. Col <math>2_0</math> is the difference between  <math>f(1)</math> and <math>f(0)</math>...<ref name=\"Thelen\">{{cite web\n| url = http://ed-thelen.org/bab/bab-intro.html\n| title  = Babbage Difference Engine #2 – How to Initialize the Machine –\n| author = Thelen, Ed\n| year = 2008}}</ref>\n\nIf the function to be calculated is a [[polynomial function]], expressed as\n: <math> f(x) = a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_2 x^2 + a_1 x + a_0 \\, </math>\nthe initial values can be calculated directly from the constant coefficients ''a''<sub>0</sub>, ''a''<sub>1</sub>,''a''<sub>2</sub>, ..., ''a<sub>n</sub>'' without calculating any data points. The initial values are thus:\n\n* Col <math>1_0</math> = ''a''<sub>0</sub>\n* Col <math>2_0</math> = ''a''<sub>1</sub> + ''a''<sub>2</sub> + ''a''<sub>3</sub> + ''a''<sub>4</sub> + ... + ''a<sub>n</sub>''\n* Col <math>3_0</math> = 2''a''<sub>2</sub> + 6''a''<sub>3</sub> + 14''a''<sub>4</sub> + 30''a''<sub>5</sub> + ...\n* Col <math>4_0</math> = 6''a''<sub>3</sub> + 36''a''<sub>4</sub> + 150''a''<sub>5</sub> + ...\n* Col <math>5_0</math> = 24''a''<sub>4</sub> + 240''a''<sub>5</sub> + ...\n* Col <math>6_0</math> = 120''a''<sub>5</sub> + ...\n* <math>...</math>\n\n=== Use of derivatives ===\nMany commonly used functions are [[analytic function]]s, which can be expressed as [[power series]], for example as a [[Taylor series]]. The initial values can be calculated to any degree of accuracy; if done correctly the engine will give exact results for first N steps. After that, the engine will only give an [[approximation]] of the function.\n\nThe Taylor series expresses the function as a sum obtained from its [[derivative]]s at one point.  For many functions the higher derivatives are trivial to obtain; for instance, the [[sine]] function at 0 has values of 0 or <math>\\pm1</math> for all derivatives. Setting 0 as the start of computation we get the simplified [[Maclaurin series]]\n:<math>\n\\sum_{n=0}^{\\infin} \\frac{f^{(n)}(0)}{n!}\\  x^{n}\n</math>\n\nThe same method of calculating the initial values from the coefficients can be used as for polynomial functions. The polynomial constant coefficients will now have the value\n:<math>\na_n \\equiv \\frac{f^{(n)}(0)}{n!}\n</math>\n\n=== Curve fitting ===\nThe problem with the methods described above is that errors will accumulate and the series will tend to diverge from the true function. A solution which guarantees a constant maximum error is to use [[curve fitting]]. A minimum of ''N'' values are calculated evenly spaced along the range of the desired calculations. Using a curve fitting technique like [[Gaussian reduction]] an ''N''−1th degree [[polynomial interpolation]] of the function is found.<ref name=\"Thelen\" /> With the optimized polynomial, the initial values can be calculated as above.\n\n== The difference engine in other works ==\nWilliam Gibson and Bruce Sterling's ''[[The Difference Engine]]'' is an alternate history<ref>{{cite book|last1=Gibson|first1=William|title=The Difference Engine}}</ref> novel that looks how society would have progressed had the difference engine worked the way Babbage envisioned it.\n\nThe story takes places in Victorian England where technological advancement is on the rise. This is due to the effect of the success of Babbage's analytical machine. The convention of [[steampunk]] where Victorian fashion is combined with the technological elements of the Industrial Revolution is seen throughout the story due to technology being so advanced in that era.\n\n== See also ==\n{{Portal|Mathematics}}\n<!-- Please keep entries in alphabetical order & add a short description [[WP:SEEALSO]] -->\n{{div col}}\n* [[Ada Lovelace]]\n* [[Allan G. Bromley]]\n* [[Analytical Engine]]\n* [[Antikythera mechanism]]\n* [[Johann Helfrich von Müller]]\n* [[Martin Wiberg]]\n* [[Per Georg Scheutz]]\n* [[Pinwheel calculator]]\n{{div col end}}\n\n== References ==\n{{Reflist|30em}}\n\n== Further reading ==\n{{refbegin|}}\n*{{cite book|title = The Philosophical Breakfast Club: Four Remarkable Friends Who Transformed Science and Changed the World|last = Snyder|first = Laura J.|publisher = Broadway|year = 2011|isbn = 978-0-7679-3048-2|location=|pages=}}\n*{{cite book|url=http://ed-thelen.org/bab/bab_tech.html|title=Charles Babbage's Difference Engine No. 2 – Technical Description|last=Swade|first=Doron|date=September 1996|publisher=[[National Museum of Science and Industry]]|isbn=|series=Science Museum Papers in the History of Technology No 5|location=London|pages=|doi=|oclc=|accessdate=2009-01-11}}\n*{{cite book|title = The Difference Engine: Charles Babbage and the Quest to Build the First Computer|last = Swade|first = Doron|publisher = Penguin (reprint)|year = 2002|isbn = 978-0-14-200144-8|location=|pages=|author-link=Doron Swade}}\n*{{cite book|title = The Cogwheel Brain|last = Swade|first = Doron|publisher = Abacus|year = 2001|isbn = 978-0-349-11239-8|location=|pages=}}\n*{{cite video\n  | people=Doron Swade, [[Nathan Myhrvold]]\n  | title=Myhrvold & Swade Discuss Babbage's Difference Engine.\n  | url=https://www.youtube.com/watch?v=p1sEowi1Txc\n  | medium=lecture: [[Len Shustek]], intro; Doron Swade @7:35, Nathan Myhrvold @36:25; discussion @46:45\n  | publisher=[[Computer History Museum]]\n  | date=June 10, 2008\n  | accessdate=2009-11-06 |first=|language=|type=}}\n*{{cite book|chapter-url=https://books.google.com/books?id=O170gWPZ7M8C|title=The History of Mathematical Tables: From Sumer to Spreadsheets|last1=Campbell-Kelly|first1=Martin|date=2003|publisher=OUP Oxford|others=Michael R. Williams|isbn=9780198508410|location=|pages=|language=en|chapter=Difference engines: from Müller to Comrie}}\n{{refend}}\n\n== External links ==\n{{Commons category|Difference engines}}\n*[http://www.computerhistory.org/babbage/ The Computer History Museum exhibition on Babbage and the difference engine]\n*[https://web.archive.org/web/20080511205136/http://www.sciencemuseum.org.uk/onlinestuff/stories/babbage.aspx ''Babbage''] Science Museum, London. Description of Babbage's calculating machine projects and the Science Museum's study of Babbage's works, including modern reconstruction and model-building projects.\n*[http://www.meccano.us/difference_engines/rde_1/ Meccano Difference Engine #1]\n*[http://www.meccano.us/difference_engines/rde_2/index.html Meccano Difference Engine #2]\n*[http://sites.google.com/site/babbagedifferenceengine/howitwasintendedtowork Babbage's First Difference Engine – How it was intended to work]\n*[http://sites.google.com/site/babbagedifferenceengine/analysisofexpenditureondifferenceenginen Analysis of Expenditure on Babbage's Difference Engine No. 1]\n*[http://satyam.com.ar/Babbage/en/index.html Difference engine workings with animations]\n*[http://www.powerhousemuseum.com/collection/database/?irn=150269&img=146773 Difference Engine No1 specimen piece at the Powerhouse Museum, Sydney]\n*[http://www.xrez.com/blog/babbage-difference-engine-in-gigapixel/ Gigapixel Image of the Difference Engine No2]\n*[https://www.youtube.com/watch?v=YtZCYnBlZpk Scheutz Difference Engine in action video.  Purchased by the Dudley Observatory's first director, Benjamin Apthorp Gould, in 1856. Gould was an acquaintance of Babbage. The Difference Engine performed astronomical calculations for the Observatory for many years, and is now part of the national collection at the Smithsonian.]\n*Links to videos about Babbage DE 2 and its construction: {{cite web |title=Computer Histories: To Learn More |url=http://www.computerhistories.org/ToLearnMore.html |website=www.computerhistories.org|at=Topic 5 - Computers in the Steam Era (Not Hackers But Clackers)}}\n\n{{DEFAULTSORT:Difference Engine}}\n[[Category:1822 introductions]]\n[[Category:English inventions]]\n[[Category:Mechanical calculators]]\n[[Category:Collections of the Science Museum, London]]\n[[Category:Replicas]]\n[[Category:Charles Babbage]]\n[[Category:Computer-related introductions in the 19th century]]\n[[Category:Articles containing video clips]]\n[[Category:Subtraction]]<!-- hand calculated -->\n[[Category:Addition]]<!-- the only thing a 'difference' engine does! ---->"
    },
    {
      "title": "Digit sum",
      "url": "https://en.wikipedia.org/wiki/Digit_sum",
      "text": "{{short description|the sum of a number's digits}}\nIn [[mathematics]], the '''digit sum''' of a given [[integer]] is the [[Addition|sum]] of all its [[numerical digit|digit]]s (e.g. the digit sum of 84001 is calculated as 8+4+0+0+1&nbsp;=&nbsp;13). Digit sums are most often computed using the [[decimal]] representation of the given number, but they may be calculated in any other [[base (exponentiation)|base]]. Different bases give different digit sums, with the digit sums for binary being on average smaller than those for any other base.<ref name=lebush>{{citation|doi=10.2307/2304217|first=L. E.|last=Bush|title=An asymptotic formula for the average sum of the digits of integers|journal=[[American Mathematical Monthly]]|year=1940|volume=47|issue=3|pages=154–156|publisher=Mathematical Association of America|jstor=2304217}}.</ref>\n\nThe digit sum of a number <math>x</math> in base <math>b</math> is given by\n\n: <math>\\sum_{n=0}^{\\lfloor \\log_b x\\rfloor} \\frac{1}{b^n}(x \\bmod b^{n + 1} - x \\bmod b^n)</math>.\n\nLet <math>S(r,N)</math> be the digit sum for [[radix]] <math>r</math> of all non-negative integers less than <math>N</math>.  For any <math>2 \\le r_1 \\le r_2</math> and for sufficiently large <math>N</math>, <math>S(r_1,N) \\le S(r_2,N)</math>.<ref name=lebush/>\n\nThe sum of the decimal digits of the integers 0, 1, 2, ... is given by {{OEIS2C|id=A007953}} in the [[On-Line Encyclopedia of Integer Sequences]]. {{harvtxt|Borwein|Borwein|1992}} use the [[generating function]] of this integer sequence (and of the analogous sequence for binary digit sums) to derive several rapidly converging [[series (mathematics)|series]] with [[rational number|rational]] and [[transcendental number|transcendental]] sums.<ref>{{citation|doi=10.2307/2324993|title=Strange series and high precision fraud|first1=J. M.|last1=Borwein|author1-link=Jonathan Borwein|first2=P. B.|last2=Borwein|author2-link=Peter Borwein|journal=[[American Mathematical Monthly]]|volume=99|issue=7|year=1992|pages=622–640|url=http://www.cecm.sfu.ca/personal/pborwein/PAPERS/P56.pdf|jstor=2324993}}.</ref>\n\nThe concept of a decimal digit sum is closely related to, but not the same as, the [[digital root]], which is the result of repeatedly applying the digit sum operation until the remaining value is only a single digit. The digital root of any non-zero integer will be a number in the range 1 to 9, whereas the digit sum can take any value. Digit sums and digital roots can be used for quick [[divisibility rule|divisibility tests]]: a natural number is divisible by 3 or 9 if and only if its digit sum (or digital root) is divisible by 3 or 9, respectively. For divisibility by 9, this test is called the '''rule of nines''' and is the basis of the [[casting out nines]] technique for checking calculations.\n\nDigit sums are also a common ingredient in [[checksum]] algorithms to check the arithmetic operations of early computers.<ref>{{citation|doi=10.2307/2002859|title=The Logical Design of the Raytheon Computer|first1=R. M.|last1=Bloch|first2=R. V. D.|last2=Campbell|first3=M.|last3=Ellis|journal=Mathematical Tables and Other Aids to Computation|volume=3|issue=24|year=1948|pages=286–295|publisher=American Mathematical Society|jstor=2002859}}.</ref> Earlier, in an era of hand calculation, {{harvtxt|Edgeworth|1888}} suggested using sums of 50 digits taken from mathematical tables of [[logarithm]]s as a form of [[random number generation]]; if one assumes that each digit is random, then by the [[central limit theorem]], these digit sums will have a random distribution closely approximating a [[Gaussian distribution]].<ref>{{citation|first=F. Y. |last=Edgeworth |authorlink=Francis Ysidro Edgeworth |title=The Mathematical Theory of Banking |journal=Journal of the Royal Statistical Society |volume=51 |issue=1 |year=1888 |pages=113–127 |url=http://instruct1.cit.cornell.edu/courses/econ719/Edgeworth.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20060913155630/http://instruct1.cit.cornell.edu/courses/econ719/Edgeworth.pdf |archivedate=2006-09-13 |df= }}.</ref>\n\nThe digit sum of the [[Binary numeral system|binary]] representation of a number is known as its [[Hamming weight]] or population count; algorithms for performing this operation have been studied, and it has been included as a built-in operation in some computer architectures and some programming languages. These operations are used in computing applications including [[cryptography]], [[coding theory]], and [[computer chess]].\n\n[[Harshad number]]s are defined in terms of divisibility by their digit sums, and [[Smith number]]s are defined by the equality of their digit sums with the digit sums of their [[integer factorization|prime factorizations]].\n\n==References==\n{{reflist}}\n\n==External links==\n*{{mathworld|urlname=DigitSum|title=Digit Sum}}\n* [http://www.applet-magic.com/Digitsum.htm] Simple applications of digit sum\n\n[[Category:Number theory]]\n[[Category:Addition]]"
    },
    {
      "title": "Interval scale",
      "url": "https://en.wikipedia.org/wiki/Interval_scale",
      "text": "#REDIRECT [[Level of measurement#Interval scale]]\n\n{{R to section}}\n\n[[Category:Addition]]\n[[Category:Subtraction]]"
    },
    {
      "title": "Plus and minus signs",
      "url": "https://en.wikipedia.org/wiki/Plus_and_minus_signs",
      "text": "{{For|these two signs conjoined as the symbol \"±\"|Plus-minus sign}}\n{{Redirect|Minus|the mathematical operation represented by the minus sign|Subtraction|other uses}}\n{{Redirect2|Positive sign|Negative sign|uses in astrology|Positive sign (astrology)|and|Negative sign (astrology)}}\n{{short description|Symbols to represent binary properties and operations like addition and subtraction}}\n{{Punctuation marks|+ −|Plus and minus signs}}\nThe '''plus and minus signs''' ('''+''' and '''−''') are [[mathematical symbol]]s used to represent the notions of [[sign (mathematics)|positive and negative]] as well as the operations of [[addition]] and [[subtraction]]. Their use has been extended to many other meanings, more or less analogous. ''Plus'' and ''minus'' are [[Latin]] terms meaning \"more\" and \"less\", respectively.\n\n== History ==\nThough the signs now seem as familiar as the [[alphabet]] or the [[Hindu-Arabic numerals]], they are not of great antiquity. The [[Egyptian hieroglyph]]ic sign for addition, for example, resembled a pair of legs walking in the direction in which the text was written ([[Egyptian language|Egyptian]] could be written either from right to left or left to right), with the reverse sign indicating subtraction:<ref>\n{{cite journal\n | last = Karpinski | first = Louis C.\n | doi = 10.2307/2973180\n | issue = 6\n | journal = [[The American Mathematical Monthly]]\n | mr = 1518824\n | pages = 257–265\n | title = Algebraical Developments Among the Egyptians and Babylonians\n | volume = 24\n | year = 1917}}</ref>\n{| align=\"center\"\n|<hiero>D54</hiero> or <hiero>D55</hiero>\n|}\n\n[[Nicole Oresme]]'s manuscripts from the 14th century show what may be one of the earliest uses of the plus sign \"+\".<ref>[http://educ.ubc.ca/courses/etec540/Sep02/ResearchAssignment/LustigovaZ/ra-LustigovaZ.htm The birth of symbols – Zdena Lustigova, Faculty of Mathematics and Physics Charles University, Prague] {{webarchive|url=https://archive.is/20130708153352/http://educ.ubc.ca/courses/etec540/Sep02/ResearchAssignment/LustigovaZ/ra-LustigovaZ.htm |date=2013-07-08 }}</ref>\n\nIn Europe in the early 15th century the letters \"P\" and \"M\" were generally used.<ref name=\"ley196504\">{{Cite magazine\n |last=Ley\n |first=Willy\n |date=April 1965\n |title=Symbolically Speaking\n |department=For Your Information\n |url=https://archive.org/stream/Galaxy_v23n04_1965-04#page/n57/mode/2up\n |magazine=Galaxy Science Fiction\n |pages=57–67\n |type=\n}}</ref><ref>{{cite journal|last=Stallings|first=Lynn|date=May 2000|title=A brief history of algebraic notation|journal=School Science and Mathematics|url=http://findarticles.com/p/articles/mi_qa3667/is_200005/ai_n8885415/|accessdate=13 April 2009}}</ref>\nThe symbols (P with line ''p̄'' for ''più'', i.e., plus, and M with line ''m̄'' for ''meno'', i.e., minus) appeared for the first time in [[Luca Pacioli]]’s mathematics compendium, ''[[Summa de arithmetica|Summa de arithmetica, geometria, proportioni et proportionalità]]'', first printed and published in [[Venice]] in 1494.<ref>{{cite journal |first=Alan |last=Sangster |first2=Greg |last2=Stoner |first3=Patricia |last3=McCarthy |title=The market for Luca Pacioli’s Summa Arithmetica |journal=Accounting Historians Journal |volume=35 |issue=1 |year=2008 |pages=111–134 [p. 115] |url=http://eprints.mdx.ac.uk/3201/1/final_final_proof_Market_paper_050308.pdf }}</ref> The '''+''' is a simplification of the Latin \"et\" (comparable to the [[ampersand]] '''&''').<ref>{{cite book|last=Cajori|first=Florian|title=A History of Mathematical Notations, Vol. 1|year=1928|publisher=The Open Court Company, Publishers|chapter=Origin and meanings of the signs + and -}}</ref> The '''−''' may be derived from a [[tilde]] written over '''m''' when used to indicate subtraction; or it may come from a shorthand version of the letter m itself.<ref>{{cite book|title=Intermediate Algebra|edition=4th|year=2000|first1=D. Franklin|last1=Wright|first2=Bill D.|last2=New|publisher=Thomson Learning|page=1|quote=The minus sign or bar, — , is thought to be derived from the habit of early scribes of using a bar to represent the letter m}}</ref> In his 1489 treatise [[Johannes Widmann]] referred to the symbols − and + as ''minus'' and ''mer'' (Modern German ''mehr''; \"more\"): \"was − ist, das ist minus, und das + ist das mer\".<ref name=\"OED\">{{OED|plus}}</ref> They weren't used for addition and subtraction here, but to indicate surplus and deficit; their first use in their modern sense appears in a book by [[Henricus Grammateus]] in 1518.<ref>{{cite book|last=Smith|author-link=David Eugene Smith|first=D.E.|title=History of Mathematics|isbn=0486204308 |publisher=Courier Dover Publications|year=1951|volume=1|pages=258, 330}}</ref><ref>[http://jeff560.tripod.com/operation.html Earliest Uses of Various Mathematical Symbols]</ref>\n\n[[Robert Recorde]], the designer of the [[equals sign]], introduced plus and minus to Britain in 1557 in ''[[The Whetstone of Witte]]'':<ref>{{citation|title=A History of Mathematical Notations|first=Florian|last=Cajori|authorlink=Florian Cajori|publisher=Cosimo|year=2007|isbn=9781602066847|page=164|url=https://books.google.com/books?id=rhEh8jPGQOcC&pg=PA164}}.</ref> \"There be other 2 signes in often use of which the first is made thus + and betokeneth more: the other is thus made – and betokeneth lesse.\"\n\n== Plus sign ==\n{{Redirect|+}}\nThe plus sign ('''+''') is a [[binary operator]] that indicates [[addition]], as in 2 + 3 = 5. It can also serve as a [[unary operator]] that leaves its [[operand]] [[identity function|unchanged]] (+''x'' means the same as ''x''). This notation may be used when it is desired to emphasize the positiveness of a number, especially when contrasting with the negative (+5 versus −5).\n\nThe plus sign can also indicate many other operations, depending on the mathematical system under consideration. Many [[algebraic structure]]s have some operation which is called, or is equivalent to, addition. It is conventional to use the plus sign to only denote [[commutative property|commutative operation]]s.<ref name=\"Fraleigh\">{{cite book\n | last = Fraleigh\n | first = John B.\n | title = A First Course in Abstract Algebra\n | publisher = [[Addison-Wesley]]\n | series =\n | volume =\n | edition = 4\n | year = 1989\n | location = United States\n | pages = 52\n | language =\n | url =\n | doi =\n | id =\n | isbn = 0-201-52821-5\n | mr =\n | zbl =\n | jfm = }}</ref> Moreover, the symbolism has been extended{{by whom|date=June 2019}} to very different operations; plus can also mean:\n* [[exclusive or]] (usually written ⊕): 1 + 1 = 0, 1 + 0 = 1\n* [[logical disjunction]] (usually written ∨): 1 + 1 = 1, 1 + 0 = 1\n\n== Minus sign ==\n\n{{Redirect-distinguish|−|Hyphen|Hyphen-minus|Dash}}\nThe minus sign ('''−''') has three main uses in mathematics:<ref>{{Cite book|\ntitle= The Algebra Lab | url= https://books.google.com/books?id=nzukMBV6ReoC&pg=PA9 | author=Henri Picciotto | publisher=Creative Publications | page=9 | isbn=978-0-88488-964-9}}</ref>\n# The [[subtraction]] operator: a [[binary operator]] to indicate the operation of subtraction, as in 5&nbsp;−&nbsp;3&nbsp;=&nbsp;2. Subtraction is the inverse of addition.\n# The [[Function (mathematics)|function]] whose [[Value (mathematics)|value]] for any [[Real number|real]] or [[Complex number|complex]] [[Argument of a function|argument]] is the [[additive inverse]] of that argument. For example, if ''x'' is 3, then −''x'' is −3, but if ''x'' is −3, then −''x'' is 3. Similarly, {{nowrap|−(−''x'')}} is equal to&nbsp;''x''.\n# {{anchor|negative_sign}}A [[prefix]] of a numeric constant.  When it is placed immediately before an unsigned numeral, the combination names a negative number, the additive inverse of the positive number that the numeral would otherwise name.  In this usage, '−5' names a number just as 'semicircle' names a geometric figure, the difference being that 'semi' does not have a separate use as a function name.\n\nIn many contexts, it does not matter whether the second or the third of these usages is intended.  −5 is the same number either way.  Sometimes, it does make a difference꞉ the programming language [[APL (programming language)|APL]] uses the special character&nbsp;<code>¯</code> ([[Unicode]] U+00AF) for this purpose so that the interpreter of APL has less work when taking <code>¯{{big|5}}</code> as the number −5 rather than inverting the constant 5 by means of the ''function'' − above.  As described in the next section, some educators consider it important that elementary students realize that negative numbers are genuine entities that can be given names and so use a raised minus in the name of a negative number.  Similarly, in the expression language used by [[Texas Instruments]] graphing calculators (definitely at least the early models including the [[TI-81]] and [[TI-82]]) a raised minus sign is used in negative numbers (as in 2&nbsp;−&nbsp;5 shows <sup>−</sup>3).\n\nAll three uses can be referred to as \"minus\" in everyday speech, though the binary operator is sometimes read as \"take away\". In most English-speaking countries, −5 (for example) is normally referred to as \"minus five\", but in modern US usage it is instead usually called \"negative five\"; here, \"minus\" may be used by speakers born before 1950, and is still popular in some contexts, but \"negative\" is usually taught as the only correct reading.<ref>{{Cite book|title=The words of mathematics |first=Steven |last=Schwartzman |year=1994 |publisher=The Mathematical Association of America |page=136}}</ref> Further, a few textbooks in the United States encourage −''x'' to be read as \"the opposite of ''x''\" or \"the additive inverse of ''x''\" to avoid giving the impression that −''x'' is necessarily negative.<ref>{{Cite book|title=Modern Mathematics |first=Ruric E. |last=Wheeler |year=2001 |edition=11 |pages=171}}</ref>\n\nIn mathematics and most programming languages, the rules for the [[order of operations]] mean that −5<sup>2</sup> is equal to −25: Powers bind more strongly than the unary minus, which binds more strongly than multiplication or division. However, in some programming languages and in [[Microsoft Excel]] in particular, unary operators bind strongest, so in those cases −5^2 is 25 but 0−5^2 is −25.<ref>{{cite web|url=http://office.microsoft.com/en-us/excel/HP100788861033.aspx |title=Microsoft Office Excel Calculation operators and precedence |accessdate=2009-07-29 |deadurl=yes |archiveurl=https://web.archive.org/web/20090811090433/http://office.microsoft.com/en-us/excel/HP100788861033.aspx |archivedate=2009-08-11 |df= }}</ref>\n\n== Use in elementary education ==\n\nSome elementary teachers use raised plus and minus signs before numbers to show they are positive or negative numbers.<ref>{{cite book|title=Understanding by design|author1=Grant P. Wiggins|author2=Jay McTighe|page=210|year=2005|publisher=ACSD Publications|isbn=1-4166-0035-3}}</ref> For example, subtracting −5 from 3 might be read as \"positive three take away negative 5\" and be shown as\n\n:3 − <sup>−</sup>5 becomes 3 + 5 = 8,\nor even as\n:<sup>+</sup>3 − <sup>−</sup>5 becomes <sup>+</sup>3 + <sup>+</sup>5 = <sup>+</sup>8.\n\n== Use as a qualifier ==\n\nIn grading systems (such as examination marks), the plus sign indicates a grade one level higher and the minus sign a grade lower. For example, B− (\"B minus\") is one grade lower than B. Sometimes this is extended to two plus or minus signs; for example A++ is two grades higher than A.\n\nPositive and negative are sometimes abbreviated as +ve and −ve.<ref>{{cite book|title=Oxford Handbook of Adult Nursing|first1=George|last1=Castledine|first2=Ann|last2=Close|publisher=Oxford University Press|year=2009|isbn=9780191039676|page=xvii|url=https://books.google.com/books?id=R6icAwAAQBAJ&pg=PR17}}.</ref>\n\nIn mathematics the [[one-sided limit]] ''x''→''a''<sup>+</sup> means ''x'' approaches ''a'' from the right, and ''x''→''a''<sup>−</sup> means ''x'' approaches ''a'' from the left. For example, when calculating what ''x''<sup>−1</sup> is when ''x'' approaches 0, because ''x''<sup>−1</sup>→[[Extended real number line|+∞]] when ''x''→0<sup>+</sup> but ''x''<sup>−1</sup>→−∞ when ''x''→0<sup>−</sup>.\n\n[[Blood types]] are often qualified with a plus or minus to indicate the presence or absence of the [[Rh factor]]; for instance, A+ means [[ABO blood group system|A-type blood]] with the Rh factor present, while B− means B-type blood with the Rh factor absent.\n\nIn music, [[Augmented triad#Augmented chord table|augmented chord]]s are symbolized with a plus sign, although this practice is not universal as there are other methods for spelling those chords. For example, \"C+\" is read \"C augmented chord\". Also used as [[superscript]].\n\n== Uses in computing ==\n\nAs well as the normal mathematical usage plus and minus may be used for a number of other purposes in computing.\n\nPlus and minus signs are often used in [[tree view]] on a computer screen to show if a folder is collapsed or not.\n\nIn some programming languages, [[concatenation]] of [[string (computer science)|string]]s is written <code>\"a\" + \"b\"</code>, and results in <code>\"ab\"</code>.\n\nIn most programming languages, subtraction and negation are indicated with the ASCII [[hyphen-minus]] character, <code>-</code>. In [[APL (programming language)|APL]] a raised minus sign (Unicode U+00AF) is used to denote a negative number, as in <code>¯3</code>. While in [[J (programming language)|J]] a negative number is denoted by an [[underscore]], as in <code>_5</code>.\n\nIn [[C (programming language)|C]] and some other computer programming languages, two plus signs indicate the [[Increment operator|increment]] operator and two minus signs a decrement; the position of the operator before or after the variable indicates whether the new or old value is read from it. For example, if x equals 6, then <code>y = x++</code> increments x to 7 but sets y to 6, whereas <code>y = ++x</code> would set both x and y to 7. By extension, \"++\" is sometimes used in computing terminology to signify an improvement, as in the name of the language [[C++]].\n\nIn [[regular expression]]s, \"+\" is often used to indicate \"1 or more\" in a pattern to be matched. For example, \"x+\" means \"one or more of the letter x\".\n\nThere is no concept of negative zero in mathematics, but in computing [[−0 (number)|−0]] may have a separate representation from zero. In the [[IEEE floating-point standard]], 1&nbsp;/&nbsp;−0 is [[negative infinity]] (−∞) whereas 1&nbsp;/&nbsp;0 is [[Infinity|positive infinity]] ([[∞]]).\n\n== Other uses ==\n\nIn chemistry, superscripted plus and minus signs are used to indicate an ion with a positive or negative charge of 1 (for example, NH<sub>4</sub><sup>+</sup>). If the charge is greater than 1, a number indicating the charge is written before the sign (SO<sub>4</sub><sup>2−</sup>). The minus sign is also used (rather than an [[en dash]]) for a [[covalent bond|single covalent bond]] between two atoms, as in the [[skeletal formula]].\n\nSubscripted plus and minus signs are used as diacritics in the [[International Phonetic Alphabet]] to indicate [[Relative articulation|advanced or retracted articulations]] of speech sounds.\n\nThe minus sign is also used as tone letter in the orthographies of [[Dan language|Dan]], [[Krumen language|Krumen]], [[Karaboro languages|Karaboro]], [[Mwan language|Mwan]], [[Wan language|Wan]], [[Yaouré]], [[Wè language|Wè]], [[Nyabwa language|Nyabwa]] and [[Godie language|Godié]].<ref>Hartell, Rhonda L., ed. (1993), ''The Alphabets of Africa''. Dakar: UNESCO and SIL.</ref> The Unicode character used for the tone letter (U+02D7) is different from the mathematical minus sign.\n\nIn the [[Algebraic notation (chess)|algebraic notation]] used to record games of [[chess]], the plus sign (+) is used to denote a move that puts the opponent into [[Check (chess)|check]]. A double plus (++) is sometimes used to denote [[double check]]. Combinations of the plus and minus signs are used to evaluate a move (+/&minus;, +/=, =/+, &minus;/+).\n\n== Character codes ==\n\n[[File:Plus Minus Hyphen-minus.svg|thumb|Plus, minus, and hyphen-minus.]]\n{| class=\"wikitable\" style=\"text-align:center;\"\n! Read !! Character !! [[Unicode]] !! [[ASCII]] !! in [[Uniform Resource Locator|URL]] !! [[HTML]] notations\n|-\n|''Plus'' ||<nowiki>+</nowiki> || U+002B || <code>&amp;#43;</code> || <code>%2B</code> || <code>&amp;plus;</code>\n|-\n|''Minus'' || <nowiki>−</nowiki> || U+2212 || || <code>%E2%88%92</code> || <code>&amp;minus; &amp;#x2212; &amp;#8722;</code>\n|-\n|''Hyphen-minus'' || <nowiki>-</nowiki> || U+002D || <code>&amp;#45;</code> || <code>%2D</code> ||\n|-\n|''Small Hyphen-minus'' || <nowiki>﹣</nowiki> || U+FE63 || || <code>%EF%B9%A3</code> || <code>&amp;#xfe63; &amp;#65123;</code>\n|-\n|''Full-width Plus'' || <nowiki>＋</nowiki> || U+FF0B || || <code>%EF%BC%8B</code> || <code>&amp;#xff0b; &amp;#65291;</code>\n|-\n|''Full-width Hyphen-minus'' || <nowiki>－</nowiki> || U+FF0D || || <code>%EF%BC%8D</code> || <code>&amp;#xff0d; &amp;#65293;</code>\n|}\n\nThe '''hyphen-minus sign''' (-) is the [[ASCII]] alternative/version of the minus sign, and doubles as a [[hyphen]]. It is usually shorter in length than the plus sign and sometimes at a different height. It can be used as a substitute for the true minus sign when the character set is limited to [[ASCII]]. Most programming languages and other computer readable languages do this, since ASCII is generally available as a subset of most character encodings, while U+2212 is a Unicode feature only.\n\nThere is a '''commercial minus sign''' (⁒), which looks somewhat like an [[obelus]], at U+2052 (HTML <tt>&amp;#x2052;</tt>).\n\nThe <code>&amp;plus;</code> entity is HTML 5.{{Clarify|date=November 2018}}\n\n{{Hatnote|For detailed distinctions between minus signs and dashes, see {{section link|Dash|Similar Unicode characters}}.}}\n\n=== Alternative plus sign ===\n{{See also|Up tack}}\n<div style=\"float:right; margin: 0 0 10px 10px; padding:40px; font-size:100%; font-family: Georgia; background-color: #ddddff; border: 1px solid #aaaaff;\">[[File:Altplus.svg|50px]]</div>\nA [[Jew]]ish tradition that dates from at least the 19th century is to write ''plus'' using a symbol like an inverted T.<ref name=JE>{{cite book|author=Kaufmann Kohler|editor=[[Cyrus Adler]]|display-editors=etal|title=Jewish Encyclopedia|chapter=Cross|year=1901–1906|url=http://www.jewishencyclopedia.com/articles/4776-cross}}</ref> This practice was adopted into [[Israel]]i schools and is still commonplace today in [[elementary school]]s (including [[secular]] schools) but in fewer [[secondary school]]s.<ref name=\"University of California\">[https://books.google.com/books?id=m8sWAAAAIAAJ&dq=Jewish+plus+sign&q=%22plus+sign+used+in+mathematics%22&pgis=1 Christian-Jewish Dialogue: Theological Foundations By Peter von der Osten-Sacken (1986 – Fortress Press)] {{ISBN|0-8006-0771-6}} \"In Israel the plus sign used in mathematics is represented by a horizontal stroke with a vertical hook instead of the sign otherwise used all over the world, because the latter is reminiscent of a cross.\" (Page 96)</ref> It is also used occasionally in books by religious authors, but most books for adults use the international symbol \"+\". The reason for this practice is that it avoids the writing of a symbol \"+\" that looks like a [[Christian cross]].<ref name=JE/><ref name=\"University of California\"/> [[Unicode]] has this symbol at position {{Unichar|FB29|HEBREW LETTER ALTERNATIVE PLUS SIGN}}.<ref>[http://www.decodeunicode.org/U+FB29 Unicode U+FB29 reference page] This form of the plus sign is also used on the control buttons at individual seats on board the El Al Israel Airlines aircraft.</ref>\n\n== See also ==\n* [[Graft-chimaera]] for the meaning of + in [[botanical name]]s\n* [[List of international call prefixes]] that + can represent the numbers required to dial out of a country as seen in a phone number\n* [[Table of mathematical symbols]]\n* [[En dash]], a dash that looks similar to the subtraction symbol but is used for different purposes\n* [[Asterisk]], the star mark {{angle bracket|<sup>*</sup>}} denoting unattested [[linguistic reconstruction]]s, is sometimes replaced by a superscript plus {{angle bracket|<sup>+</sup>}}\n\n== References and footnotes ==\n{{Reflist}}\n\n== External links ==\n*{{Wiktionary-inline|plus sign}}\n*{{Wiktionary-inline|minus sign}}\n\n{{DEFAULTSORT:Plus and minus signs}}\n[[Category:Elementary arithmetic]]\n[[Category:Mathematical symbols]]\n[[Category:Addition]]\n[[Category:Subtraction]]"
    },
    {
      "title": "Plus-minus sign",
      "url": "https://en.wikipedia.org/wiki/Plus-minus_sign",
      "text": "{{Other uses|plus-minus (disambiguation)}}\n{{Punctuation marks|±|variant1=∓|caption1=Minus-plus}}\nThe '''plus-minus sign''' (±) is a mathematical symbol with multiple meanings.\n*In [[mathematics]], it generally indicates a choice of exactly two possible values, one of which is the negation of the other.\n*In [[experimental science]]s, the sign commonly indicates the [[confidence interval]] or [[Errors and residuals in statistics|error]] in a measurement, often the [[standard deviation]] or [[standard error]].<ref name=\"stderror\"/> The sign may also represent an inclusive range of values that a reading might have.\n*In [[engineering]] the sign indicates the [[Tolerance (engineering)|tolerance]], which is the range of values that are considered to be acceptable, safe, or which comply with some standard, or with a contract.<ref>[[Engineering tolerance]]</ref>\n*In [[botany]] it is used in morphological descriptions to notate \"more or less\".\n*In [[chemistry]] the sign is used to indicate a [[racemic mixture]].\n*In [[chess]], the sign indicates a clear advantage for the white player; the complementary sign ∓ indicates the same advantage for the black player.<ref name=\"chess\">{{citation|title=Chess For Dummies|first=James|last=Eade|edition=2nd|publisher=John Wiley & Sons|year=2005|isbn=9780471774334|url=https://books.google.com/books?id=7eZxKNQu-JoC&pg=PA272|page=272}}.</ref>\nThe sign is normally pronounced \"plus or minus\" or \"plus-minus\".{{Citation needed|date=October 2018}}\n\n==History==\nA version of the sign, including also the French word ''ou'' (\"or\") was used in its mathematical meaning by [[Albert Girard]] in 1626, and the sign in its modern form was used as early as [[William Oughtred]]'s ''Clavis Mathematicae'' (1631).<ref>{{citation|title=A History of Mathematical Notations, Volumes 1-2|first=Florian|last=Cajori|authorlink=Florian Cajori|publisher=Dover|year=1928|isbn=9780486677668|url=https://books.google.com/books?id=7juWmvQSTvwC&pg=PA245|page=245}}.</ref>\n\n==Usage==\n\n===In mathematics===\n<!-- [[± shorthand]] redirects here -->\nIn [[mathematical formula]]s, the ± symbol may be used to indicate a symbol that may be replaced by either the [[Plus and minus signs|+ or −]] symbols, allowing the formula to represent two values or two equations.\n\nFor example, given the equation ''x''<sup>2</sup> = 9, one may give the solution as ''x'' = ±3. This indicates that the equation has two solutions, each of which may be obtained by replacing this equation by one of the two equations ''x'' = +3 or ''x'' = −3. Only one of these two replaced equations is true for any valid solution. A common use of this notation is found in the [[quadratic formula]]\n:<math>x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}.</math>\ndescribing the two solutions to the [[quadratic equation]] ''ax''<sup>2</sup>&nbsp;+&nbsp;''bx''&nbsp;+&nbsp;''c''&nbsp;=&nbsp;0.\n\nSimilarly, the [[trigonometric identity]]\n:<math>\\sin(A \\pm B) = \\sin(A) \\cos(B) \\pm \\cos(A) \\sin(B).</math>\ncan be interpreted as a shorthand for two equations: one with \"+\" on both sides of the equation, and one with \"−\" on both sides. The two copies of the ± sign in this identity must both be replaced in the same way: it is not valid to replace one of them with \"+\" and the other of them with \"−\". In contrast to the quadratic formula example, both of the equations described by this identity are simultaneously valid.\n\nA third related usage is found in this presentation of the formula for the [[Taylor series]] of the sine function:\n:<math>\\sin\\left( x \\right) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots \\pm \\frac{1}{(2n+1)!} x^{2n+1} + \\cdots. </math>\nHere, the plus-or-minus sign indicates that the signs of the terms alternate, where (starting the count at&nbsp;0) the terms with an even index&nbsp;''n'' are added while those with an odd index are subtracted.  A more rigorous presentation of the same formula would multiply each term by a factor of (−1)<sup>''n''</sup>, which gives +1 when ''n'' is even and −1 when ''n'' is odd.\n\n===In statistics===\nThe use of {{angbr|±}} for an approximation is most commonly encountered in presenting the numerical value of a quantity together with its [[tolerance (engineering)|tolerance]] or its statistical [[margin of error]].<ref name=\"stderror\">{{citation|title=Standard Deviation, Standard Error: Which 'Standard' Should We Use?|first=George W.|last=Brown|journal=American Journal of Diseases of Children|year=1982|volume=136|issue=10|pages=937–941|doi=10.1001/archpedi.1982.03970460067015}}.</ref>\nFor example, \"{{val|5.7|0.2}}\" may be anywhere in the range from 5.5 to 5.9 inclusive. In scientific usage it sometimes refers to a probability of being within the stated interval, usually corresponding to either 1 or 2 [[standard deviation]]s (a probability of 68.3% or 95.4% in a [[normal distribution]]).\n\nA [[percentage]] may also be used to indicate the error margin. For example, {{Nowrap|230 ± 10% V}} refers to a voltage within 10% of either side of 230&nbsp;V (from 207&nbsp;V to 253&nbsp;V inclusive). Separate values for the upper and lower bounds may also be used. For example, to indicate that a value is most likely 5.7 but may be as high as 5.9 or as low as 5.6, one may write {{val|5.7|+0.2|−0.1}}.\n\n===In chess===\nThe symbols ± and ∓ are used in [[Punctuation (chess)#Position evaluation symbols|chess notation]] to denote an advantage for white and black respectively. However, the more common chess notation would be only + and –.<ref name=\"chess\"/>  If a difference is made, the symbols + and − denote a larger advantage than ± and ∓.\n\n==Minus-plus sign==\nThe '''minus-plus sign''' (∓) is generally used in conjunction with the \"±\" sign, in such expressions as \"x ± y ∓ z\", which can be interpreted as meaning \"''x'' + ''y'' − ''z''\" and/or \"''x'' − ''y'' + ''z''\", but ''not'' \"''x'' + ''y'' + ''z''\" or \"''x'' − ''y'' − ''z''\". The upper \"−\" in \"∓\" is considered to be associated to the \"+\" of \"±\" (and similarly for the two lower symbols) even though there is no visual indication of the dependency.  (However, the \"±\" sign is generally preferred over the \"∓\" sign, so if they both appear in an equation it is safe to assume that they are linked.  On the other hand, if there are two instances of the \"±\" sign in an expression, it is impossible to tell from notation alone whether the intended interpretation is as two or four distinct expressions.)  The original expression can be rewritten as \"''x'' ± (''y'' − ''z'')\" to avoid confusion, but cases such as the trigonometric identity\n\n:<math>\\cos(A \\pm B) = \\cos(A) \\cos(B) \\mp \\sin(A) \\sin(B) </math>\n\nare most neatly written using the \"∓\" sign. The trigonometric equation above thus represents the two equations:\n:<math>\\begin{align}\n\\cos(A + B) &= \\cos(A)\\cos(B) - \\sin(A) \\sin(B) \\\\\n\\cos(A - B) &= \\cos(A)\\cos(B) + \\sin(A) \\sin(B) \\end{align}</math>\nbut ''not''\n:<math>\\begin{align}\n\\cos(A + B) &= \\cos(A)\\cos(B) + \\sin(A) \\sin(B) \\\\\n\\cos(A - B) &= \\cos(A)\\cos(B) - \\sin(A) \\sin(B) \\end{align}</math>\n\nbecause the signs are exclusively alternating.\n\nAnother example is\n:<math>x^3 \\pm 1 = (x \\pm 1)\\left(x^2 \\mp x + 1\\right)</math>\nwhich represents two equations.\n\n==Encodings==\n* In Unicode: {{unichar|00B1|PLUS-MINUS SIGN|html=}}\n*In [[ISO 8859-1]], [[ISO 8859-7|-7]], [[ISO 8859-8|-8]], [[ISO 8859-9|-9]], [[ISO 8859-13|-13]], [[ISO 8859-15|-15]], and [[ISO 8859-16|-16]], the plus-minus symbol is given by the code 0xB1<sub>[[hexadecimal|hex]]</sub>. Since the first 256 code points of Unicode are identical to the contents of ISO-8859-1, this symbol is also at [[Unicode]] code point U+00B1.\n*The symbol also has a [[Character entity reference|HTML entity]] representation of <code>&amp;plusmn;</code>.\n*The rarer minus-plus sign (∓) is not generally found in legacy encodings and does not have a named HTML entity but is available in Unicode with code point U+2213 and so can be used in HTML using <code>&amp;#x2213;</code> or <code>&amp;#8723;</code>.\n*In [[TeX]] 'plus-or-minus' and 'minus-or-plus' symbols are denoted <code>\\pm</code> and <code>\\mp</code>, respectively.\n*These characters may also be produced as an underlined or overlined + symbol (&nbsp;<u>+</u>&nbsp; or {{Overline|+}}&nbsp;), but beware of the formatting being stripped at a later date, changing the meaning.\n\n===Typing===\n*On [[Microsoft Windows|Windows]] systems, it may be entered by means of [[Alt code]]s, by holding the ALT key while typing the numbers 0177 or 241 on the [[numeric keypad]].\n*On Unix-like systems, it can be entered by typing the sequence [[Compose key|compose]] + -.\n*On Macintosh systems, it may be entered by pressing option shift = (on the non-numeric keypad).\n*On a [[Chromebook]], it may be entered by pressing shift, ctrl and u, and then writing the unicode for plus-minus (00B1).\n\n==Similar characters==\nThe plus-minus sign resembles the [[Chinese character]]s {{lang|zh|[[wikt:士|士]]}} and {{lang|zh|[[wikt:土|土]]}}, whereas the minus-plus sign resembles {{lang|zh|[[wikt:干|干]]}}.\n\n==See also==\n*[[Plus and minus signs]]\n*[[Table of mathematical symbols]]\n*[[≈]] (approximately equal to)\n*[[Engineering tolerance]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Plus-Minus Sign}}\n[[Category:Elementary arithmetic]]\n[[Category:Mathematical symbols]]\n[[Category:Addition]]\n[[Category:Subtraction]]"
    },
    {
      "title": "Summation",
      "url": "https://en.wikipedia.org/wiki/Summation",
      "text": "{{about|sums of several elements|more elementary aspects|Addition|infinite sums|Series (mathematics)|other uses}}\n{{Calculation results}}\n\nIn [[mathematics]], '''summation''' is the [[addition]] of a [[sequence]] of any kind of [[number]]s, called ''addends'' or  ''summands''; the result is their ''sum'' or ''total''. Besides numbers, other types of values can be summed as well: [[function (mathematics)|functions]], [[vector space|vectors]], [[matrix (mathematics)|matrices]], [[polynomial]]s and, in general, elements of any types of [[mathematical object]]s on which an [[operation (mathematics)|operation]] denoted \"+\" is defined.\n\nSummations of [[infinite sequence]]s are called [[series (mathematics)|series]]. They involve the concept of [[limit (mathematics)|limit]], and are not considered in this article.\n\nThe summation of an explicit sequence is denoted as a succession of additions. For example, summation of {{math|[1, 2, 4, 2]}} is denoted {{math|1 + 2 + 4 + 2}}, and results in 9, that is, {{math|1=1 + 2 + 4 + 2 = 9}}. Because addition is [[associative]] and [[commutative]], there is no need of parentheses, and the result does not depend on the order of the summands. Summation of a sequence of only one element results in this element itself. Summation of an empty sequence (a sequence with zero element) results, by convention, in 0.\n\nVery often, the elements of a sequence are defined, through regular pattern, as a [[function (mathematics)|function]] of their place in the sequence. For simple patterns, summation of long sequences may be represented with most summands replaced by ellipses. For example, summation of the first 100 natural numbers may be written {{math|1 + 2 + 3 + 4 + ⋅⋅⋅ + 99 + 100}}. Otherwise, summation is denoted by using [[#Capital-sigma notation|Σ notation]], where <math>\\textstyle\\sum</math> is an enlarged capital [[Greek letter]] [[sigma]]. For example, the sum of the first {{mvar|n}} natural integers is denoted <math>\\textstyle \\sum_{i=1}^n i.</math>\n\nFor long summations, and summations of variable length (defined with ellipses or Σ notation), it is a common problem to find [[closed-form expression]]s  for the result. For example,{{efn|For details, see [[Triangular number]].}} \n\n:<math>\\sum_{ i=1}^n i = \\frac{n(n+1)}{2}.</math>\n\nAlthough such formulas do not always exist, many summation formulas have been discovered. Some of the most common and elementary ones are listed in this article.\n\n== Notation ==\n=== Capital-sigma notation ===\n[[File:Greek uc sigma.svg|thumb|74x74px|The summation symbol]]\nMathematical notation uses a symbol that compactly represents summation of many similar terms: the ''summation symbol'',  <math>\\textstyle\\sum</math>, an enlarged form of the upright capital Greek letter [[Sigma (letter)|Sigma]]. This is defined as:\n:<math>\\sum_{i \\mathop =m}^n a_i = a_m + a_{m+1} + a_{m+2} +\\cdots+ a_{n-1} + a_n</math>\n\nwhere ''i'' represents the '''index of summation'''; ''a<sub>i</sub>'' is an indexed variable representing each successive term in the series; ''m'' is the '''lower bound of summation''', and ''n'' is the '''upper bound of summation'''. The ''\"i = m\"'' under the summation symbol means that the index ''i'' starts out equal to ''m''.  The index, ''i'', is incremented by 1 for each successive term, stopping when ''i'' = ''n''.{{efn|For a detailed exposition on summation notation, and arithmetic with sums, see {{cite book |last1=Graham |first1=Ronald L. |last2=Knuth |first2=Donald E. |last3=Patashnik |first3=Oren |year=1994 |title=Concrete Mathematics: A Foundation for Computer Science |edition=2nd |chapter=Chapter 2: Sums |publisher=Addison-Wesley Professional |isbn=978-0201558029 |url=http://www.cse.iitb.ac.in/~vsevani/Concrete%20Mathematics%20-%20R.%20Graham,%20D.%20Knuth,%20O.%20Patashnik.pdf}}{{dead link|date=March 2018 |bot=InternetArchiveBot |fix-attempted=yes }}}}\n\nHere is an example showing the summation of squares:\n:<math>\\sum_{i =3}^6 i^2 = 3^2+4^2+5^2+6^2 = 86.</math>\n\nInformal writing sometimes omits the definition of the index and bounds of summation when these are clear from context, as in:\n:<math>\\sum a_i^2 = \\sum_{ i \\mathop =1}^n a_i^2.</math>\n\nOne often sees generalizations of this notation in which an arbitrary logical condition is supplied, and the sum is intended to be taken over all values satisfying the condition. Here are some common examples:\n:<math>\\sum_{0\\le k< 100} f(k)</math>\nis the sum of <math>f(k)</math> over all (integers) <math>k</math> in the specified range,\n:<math>\\sum_{x \\mathop \\in S} f(x)</math>\nis the sum of <math>f(x)</math> over all elements <math>x</math> in the set <math>S</math>, and\n:<math>\\sum_{d|n}\\;\\mu(d)</math>\nis the sum of <math>\\mu(d)</math> over all positive integers <math>d</math> dividing <math>n</math>.{{efn|Although the name of the [[Free variables and bound variables|dummy variable]] does not matter (by definition), one usually uses letters from the middle of the alphabet (<math>i</math> through <math>q</math>) to denote integers, if there is a risk of confusion. For example, even if there should be no doubt about the interpretation, it could look slightly confusing to many mathematicians to see <math>x</math> instead of <math>k</math> in the above formulae involving <math>k</math>. See also [[typographical conventions in mathematical formulae]].}}\n\nThere are also ways to generalize the use of many sigma signs. For example,\n:<math>\\sum_{i,j}</math>\nis the same as\n:<math>\\sum_{i}\\sum_{j}.</math>\n\nA similar notation is applied when it comes to denoting the [[Multiplication#Capital Pi notation|product]] of a sequence, which is similar to its summation, but which uses the multiplication operation instead of addition (and gives 1 for an empty sequence instead of 0). The same basic structure is used, with <math>\\textstyle\\prod</math>, an enlarged form of the Greek capital letter [[Pi (letter)|Pi]], replacing the <math>\\textstyle\\sum</math>.\n\n===Special cases===\nIt is possible to sum fewer than 2 numbers:\n* If the summation has one summand <math>x</math>, then the evaluated sum is <math>x</math>.\n* If the summation has no summands, then the evaluated sum is [[0 (number)|zero]], because <!-- huh?,  REFERENCE?? --> zero is the [[identity element|identity]] for addition. This is known as the ''[[empty sum]]''.\n\nThese degenerate cases are usually only used when the summation notation gives a degenerate result in a special case.\nFor example, if <math>n=m</math> in the definition above, then there is only one term in the sum; if <math>n=m-1</math>, then there is none.\n\n== Formal definition ==\nSummation may be defined recursively as follows \n:<math>\\sum_{i=a}^b g(i)=0</math> <math></math>, for ''b'' < ''a''.\n: \n:<math>\\sum_{i=a}^b g(i)=g(b)+\\sum_{i=a}^{b-1} g(i)</math>, for ''b'' ≥ ''a''.\n\n==Measure theory notation==\nIn the notation of [[measure theory|measure]] and [[integral|integration]] theory, a sum can be expressed as a [[definite integral]],\n\n:<math>\\sum_{k \\mathop =a}^b f(k) = \\int_{[a,b]} f\\,d\\mu</math>\n\nwhere <math>[a, b]</math> is the subset of the [[integer]]s from <math>a</math> to <math>b</math>, and where <math>\\mu</math> is the [[counting measure]].\n\n==Calculus of finite differences==\n\nGiven a function {{mvar|f}} that is defined over the integers in the [[interval (mathematics)|interval]] {{math|[''m'', ''n'']}}, one has\n:<math>f(n)-f(m)= \\sum_{i=m}^{n-1} (f(i+1)-f(i)).</math>\n\nThis is the analogue in [[calculus of finite differences]] of the [[fundamental theorem of calculus]], which states\n:<math>f(n)-f(m)=\\int_m^n f'(x)\\,dx,</math>\nwhere\n:<math>f'(x)=\\lim_{h\\to 0} \\frac{f(x+h)-f(x)}{h}</math>\nis the [[derivative]] of {{mvar|f}}.\n\nAn example of application of the above equation is \n:<math>n^k=\\sum_{i=0}^{n-1} \\left((i+1)^k-i^k\\right).</math>\nUsing [[binomial theorem]], this may be rewritten \n:<math>n^k=\\sum_{i=0}^{n-1} \\left(\\sum_{j=0}^{i-1} \\binom{k}{j} x^j\\right).</math>\n\nThe above formula is more commonly used for inverting of the [[difference operator]] <math>\\Delta</math> defined by\n:<math>\\Delta(f)(n)=f(n+1)-f(n), </math>\nwhere {{mvar|f}} is a function defined on the nonnegative integers.\nThus, given such a function {{mvar|f}}, the problem is to compute the [[antidifference]] of {{mvar|f}}, that is, a function <math>F=\\Delta^{-1}f</math>  such that <math>\\Delta F=f,</math>, that is,<math>F(n+1)-F(n)=f(n).</math>\nThis function is defined up to the addition of a constant, and may be chosen as<ref>''Handbook of Discrete and Combinatorial Mathematics'', Kenneth H. Rosen, John G. Michaels, CRC Press, 1999, {{isbn|0-8493-0149-1}}.</ref>\n:<math>F(n)=\\sum_{i=0}^{n-1} f(i).</math>\n\nThere is not always a [[closed-form expression]] for such a summation, but [[Faulhaber's formula]] provides a closed form in the case of <math>f(n)=n^k,</math> and, by [[linearity]] for every [[polynomial function]] of {{mvar|n}}.\n\n==Approximation by definite integrals==\nMany such approximations can be obtained by the following connection between sums and [[integral]]s, which holds for any:\n\n[[monotonic function|increasing]] function ''f'':\n\n:<math>\\int_{s=a-1}^{b} f(s)\\ ds \\le \\sum_{i=a}^{b} f(i) \\le \\int_{s=a}^{b+1} f(s)\\ ds.</math>\n\n[[monotonic function|decreasing]] function ''f'':\n\n:<math>\\int_{s=a}^{b+1} f(s)\\ ds \\le \\sum_{i=a}^{b} f(i) \\le \\int_{s=a-1}^{b} f(s)\\ ds.</math>\n\nFor more general approximations, see the [[Euler–Maclaurin formula]].\n\nFor summations in which the summand is given (or can be interpolated) by an [[Riemann integral|integrable]] function of the index, the summation can be interpreted as a [[Riemann sum]] occurring in the definition of the corresponding definite integral. One can therefore expect that for instance\n\n:<math>\\frac{b-a}{n}\\sum_{i=0}^{n-1} f\\left(a+i\\frac{b-a}n\\right) \\approx \\int_a^b f(x)\\ dx,</math>\n\nsince the right hand side is by definition the limit for <math>n\\to\\infty</math> of the left hand side. However, for a given summation ''n'' is fixed, and little can be said about the error in the above approximation without additional assumptions about ''f'': it is clear that for wildly oscillating functions the Riemann sum can be arbitrarily far from the Riemann integral.\n\n== Identities ==\nThe formulae below involve finite sums; for infinite summations or finite summations of expressions involving [[trigonometric function]]s or other [[transcendental function]]s, see [[list of mathematical series]].\n\n=== General identities ===\n: <math>\\sum_{n=s}^t C\\cdot f(n) = C\\cdot \\sum_{n=s}^t f(n) \\quad</math> ([[distributivity]])\n: <math>\\sum_{n=s}^t f(n) \\pm \\sum_{n=s}^{t} g(n) = \\sum_{n=s}^t \\left(f(n) \\pm g(n)\\right)\\quad</math> ([[commutativity]] and [[associativity]])\n: <math>\\sum_{n=s}^t f(n) = \\sum_{n=s+p}^{t+p} f(n-p)\\quad</math> (index shift)\n: <math>\\sum_{n\\in B} f(n) = \\sum_{m\\in A} f(\\sigma(m)), \\quad</math> for a [[bijection]] {{mvar|σ}} from a finite set {{mvar|A}} onto a  set {{mvar|B}} (index change); this generalizes the preceding formula.\n: <math>\\sum_{n=s}^t f(n) =\\sum_{n=s}^j f(n) + \\sum_{n=j+1}^t f(n)\\quad</math> (splitting a sum, using [[associativity]])\n: <math>\\sum_{n=a}^{b}f(n)=\\sum_{n=0}^{b}f(n)-\\sum_{n=0}^{a-1}f(n)\\quad</math> (a variant of the preceding formula)\n: <math>\\sum_{i=k_0}^{k_1}\\sum_{j=l_0}^{l_1} a_{i,j} = \\sum_{j=l_0}^{l_1}\\sum_{i=k_0}^{k_1} a_{i,j}\\quad</math> (commutativity and associativity, again)\n: <math>\\sum_{k\\le j \\le i\\le n} a_{i,j} = \\sum_{i=k}^n\\sum_{j=k}^i a_{i,j} = \\sum_{j=k}^n\\sum_{i=j}^n a_{i,j} =\n\\sum_{j=0}^{n-k}\\sum_{i=k}^{n-j} a_{i+j,i}\\quad</math> (another application of commutativity and associativity)\n: <math>\\sum_{n=0}^{2t+1} f(n) = \\sum_{n=0}^t f(2n) + \\sum_{n=0}^t f(2n+1)\\quad</math> (splitting a sum into its odd and even parts, and changing the indices)\n: <math>\\left(\\sum_{k=0}^{n} a_k\\right) \\left(\\sum_{k=0}^{n} b_k\\right)=\\sum_{i=0}^n \\sum_{j=0}^n a_ib_j \\quad</math> ([[distributivity]])\n: <math>\\sum_{i=s}^m\\sum_{j=t}^n {a_i}{c_j} = \\left(\\sum_{i=s}^m a_i\\right) \\left( \\sum_{j=t}^n c_j \\right)\\quad</math> (distributivity allows factorization)\n: <math>\\sum_{n=s}^t \\log_b f(n) = \\log_b \\prod_{n=s}^t f(n)\\quad</math> (the [[logarithm]] of a product is the sum of the logarithms of the factors)\n: <math>C^{\\sum\\limits_{n=s}^t f(n) } = \\prod_{n=s}^t C^{f(n)}\\quad</math> (the [[exponentiation|exponential]] of a sum is the product of the exponential of the summands)\n\n=== Powers and logarithm of arithmetic progressions ===\n: <math>\\sum_{i=1}^n c = nc\\quad</math> for every {{mvar|c}} that does not depend on {{mvar|i}}\n: <math>\\sum_{i=0}^n i = \\sum_{i=1}^n i = \\frac{n(n+1)}{2}\\qquad</math> (Sum of the simplest [[arithmetic progression]], consisting of the n first [[natural number]]s.)<ref name=CRC>CRC, p 52</ref>{{Full citation needed|date=November 2018}}\n: <math>\\sum_{i=1}^n (2i-1) = n^2\\qquad</math> (Sum of first odd natural numbers)\n: <math>\\sum_{i=0}^{n} 2i = n(n+1)\\qquad</math> (Sum of first even natural numbers)\n: <math>\\sum_{i=1}^{n} \\log i = \\log n!\\qquad</math> (A sum of [[logarithm]]s is the logarithm of the product)\n: <math>\\sum_{i=0}^n i^2 = \\frac{n(n+1)(2n+1)}{6} = \\frac{n^3}{3} + \\frac{n^2}{2} + \\frac{n}{6}\\qquad</math> (Sum of the first [[square number|squares]], see [[square pyramidal number]].) <ref name=CRC/>\n: <math>\\sum_{i=0}^n i^3 = \\left(\\sum_{i=0}^n i \\right)^2 = \\left(\\frac{n(n+1)}{2}\\right)^2 = \\frac{n^4}{4} + \\frac{n^3}{2} + \\frac{n^2}{4}\\qquad</math> ([[Nicomachus's theorem]]) <ref name=CRC/>\n\nMore generally,\n: <math>\\sum_{i=0}^n i^p = \\frac{(n+1)^{p+1}}{p+1} + \\sum_{k=1}^p\\frac{B_k}{p-k+1}{p\\choose k}(n+1)^{p-k+1},</math>\nwhere <math>B_k</math> denotes a [[Bernoulli number]] (that is [[Faulhaber's formula]]).\n\n=== Summation index in exponents ===\nIn the following summations, {{mvar|a}} is assumed to be different from 1.\n\n: <math>\\sum_{i=0}^{n-1} a^i = \\frac{1-a^n}{1-a}</math> (sum of a [[geometric progression]])\n: <math>\\sum_{i=0}^{n-1} \\frac{1}{2^i} = 2-\\frac{1}{2^{n-1}}</math> (special case for {{math|1=''a'' = 1/2}})\n: <math>\\sum_{i=0}^{n-1} i a^i =\\frac{a-na^n+(n-1)a^{n+1}}{(1-a)^2}</math> ({{mvar|a}} times the derivative with respect to {{mvar|a}} of the geometric progression)\n: <math>\\begin {align}\\sum_{i= 0}^{n-1} \\left(b + i d\\right) a^i &= b \\sum_{i= 0}^{n-1} a^i + d \\sum_{i= 0}^{n-1} i a^i\\\\\n & = b \\left(\\frac{1-a^n}{1-a}\\right) + d \\left(\\frac{a-na^n+(n-1)a^{n+1}}{(1-a)^2}\\right)\\\\\n & = \\frac{b(1-a^n) - (n - 1)d a^n}{1 - a}+\\frac{da(1 - a^{n - 1})}{(1 - a)^2}\n\\end {align}</math>\n:::(sum of an [[arithmetico–geometric sequence]])\n\n=== Binomial coefficients and factorials ===\n{{Main|Binomial coefficient#Sums of the binomial coefficients}}\n\nThere exist very many summation identities involving binomial coefficients (a whole chapter of ''[[Concrete Mathematics]]'' is devoted to just the basic techniques). Some of the most basic ones are the following.\n\n====Involving the binomial theorem====\n: <math>\\sum_{i=0}^n {n \\choose i}a^{n-i} b^i=(a + b)^n,</math> the [[binomial theorem]]\n: <math>\\sum_{i=0}^n {n \\choose i} = 2^n,</math> the special case where {{math|1=''a'' = ''b'' = 1}}\n: <math>\\sum_{i=0}^n {n \\choose i}p^i (1-p)^{n-i}=1</math>, the special case where {{math|1=''p'' = ''a'' = 1 – ''b''}}, which, for <math>0 \\le p \\le 1,</math> expresses the sum of the [[binomial distribution]]\n: <math>\\sum_{i=0}^{n} i{n \\choose i} = n(2^{n-1}),</math> the value at {{math|1=''a'' = ''b'' = 1}} of the [[derivative]] with respect to {{mvar|a}} of the binomial theorem\n: <math>\\sum_{i=0}^n \\frac{n \\choose i}{i+1} = \\frac{2^{n+1}-1}{n+1},</math> the value at {{math|1=''a'' = ''b'' = 1}} of the [[antiderivative]] with respect to {{mvar|a}} of the binomial theorem\n\n==== Involving permutation numbers====\nIn the following summations, <math>{}_{n}P_{k}</math> is the number of [[k-permutation|{{math|''k''}}-permutations of {{math|''n''}}]].\n: <math>\\sum_{i=0}^{n} {}_{i}P_{k}{n \\choose i} = {}_{n}P_{k}(2^{n-k})</math>\n: <math>\\sum_{i=1}^n {}_{i+k}P_{k+1} = \\sum_{i=1}^n \\prod_{j=0}^k (i+j) = \\frac{(n+k+1)!}{(n-1)!(k+2)}</math>\n: <math>\\sum_{i=0}^{n} i!\\cdot{n \\choose i} = \\sum_{i=0}^{n} {}_{n}P_{i} = \\lfloor n! \\cdot e \\rfloor, \\quad n \\in \\mathbb{Z}^+</math>, where  and <math>\\lfloor x\\rfloor</math> denotes the [[floor function]].\n\n====Others====\n: <math>\\sum_{k=0}^{m} \\left(\\begin{array}{c} n+k\\\\n\\\\ \\end{array}\\right) = \\left(\\begin{array}{c} n+m+1\\\\n+1\\\\ \\end{array}\\right)</math>\n: <math>\\sum_{i=k}^{n} {i \\choose k} = {n+1 \\choose k+1}</math>\n: <math>\\sum_{i=0}^n i\\cdot i! = (n+1)! - 1</math>\n: <math>\\sum_{i=0}^n {m+i-1 \\choose i} = {m+n \\choose n}</math>\n:<math>\\sum_{i=0}^n {n \\choose i}^2 = {2n \\choose n}</math>\n\n===Harmonic numbers===\n\n: <math>\\sum_{i=1}^n \\frac{1}{i} = H_n</math> (that is the {{mvar|n}}th [[harmonic number]])\n: <math>\\sum_{i=1}^n \\frac{1}{i^k} = H^k_n</math> (that is a [[Harmonic number#Generalized harmonic numbers|generalized harmonic number]])\n\n==Growth rates==\nThe following are useful [[approximation]]s (using [[big O notation|theta notation]]):\n\n: <math>\\sum_{i=1}^n i^c \\in \\Theta(n^{c+1})</math> for real ''c'' greater than −1\n: \n: <math>\\sum_{i=1}^n \\frac{1}{i} \\in \\Theta(\\log_e n)</math> (See [[Harmonic number]])\n: \n: <math>\\sum_{i=1}^n c^i \\in \\Theta(c^n)</math> for real ''c'' greater than 1\n: \n: <math>\\sum_{i=1}^n \\log(i)^c \\in \\Theta(n \\cdot \\log(n)^{c})</math> for [[non-negative]] real ''c''\n: \n: <math>\\sum_{i=1}^n \\log(i)^c \\cdot i^d \\in \\Theta(n^{d+1} \\cdot \\log(n)^{c})</math> for non-negative real ''c'', ''d''\n: \n: <math>\\sum_{i=1}^n \\log(i)^c \\cdot i^d \\cdot b^i \\in \\Theta (n^d \\cdot \\log(n)^c \\cdot b^n)</math> for non-negative real ''b'' > 1, ''c'', ''d''\n\n==See also==\n* [[Einstein notation]]\n* [[Iverson bracket]]\n* [[Iterated binary operation]]\n* [[Kahan summation algorithm]]\n* [[Multiplication#Products of sequences|Products of sequences]]\n* [[Product (mathematics)]]\n\n==Notes==\n{{notelist}}\n\n==Sources==\n{{reflist}}\n\n==External links==\n* {{commonscat-inline}}\n* {{planetmath reference|id=6361|title=Summation}}\n\n\n[[Category:Arithmetic]]\n[[Category:Mathematical notation]]\n[[Category:Addition]]"
    },
    {
      "title": "Additron tube",
      "url": "https://en.wikipedia.org/wiki/Additron_tube",
      "text": "[[File:Additron Tube schematic.png|thumb|Schematic from {{US patent|2784312}} showing a 3-bit adder using Additron tubes]]\nThe '''Additron''' was an electron tube designed by Dr. [[Josef Kates]], circa 1950, to replace the several individual electron tubes and support components required to perform the function of a single bit digital [[full adder]]. Dr. Kates developed the Additron with the intention of increasing the likelihood of success and reliability while reducing the size, power consumption and complexity of the University of Toronto Electronic Computer, ([[UTEC]])<ref>Vardalas JN (2001). ''The Computer Revolution in Canada'' MIT Press, {{ISBN|0-262-22064-4}}</ref>\n\nThe Additron neither went into production at the Canadian [[Rogers Vacuum Tube Company]], where the prototypes were built, nor was it used in the UTEC machine.  It did make a widely publicized appearance at the 1950 [[Canadian National Exhibition]] operating an electronic game of [[Tic-Tac-Toe]], dubbed ''[[Bertie the Brain]]'', to show the marvels of electronic computing.\n\nThe tube was registered with the [[Radio Electronics Television Manufacturers' Association|Radio Television Manufacturing Association ]]<ref>RTMA Engineering Department, Release #951, 20 March 1951</ref> on 20 March 1951 as type 6047.<ref>{{cite web|url=http://www.mif.pg.gda.pl/homepages/frank/sheets/201/6/6047.pdf |publisher=Rogers Majestic Corp.|title= 6047 data sheet, RTMA Engineering Dept. Release #954|date=March 20, 1951|format=PDF|accessdate=14 August 2016}}</ref><ref>Sibley L (2007). Weird Tube of the Month: The 6047 ''Tube Collector'' '''9''' (5):20</ref> <ref>Osborne CS (2008).The Additron: A Binary Full Adder in a Tube ''Tube Collector'' '''10''' (4):12</ref>\n\n==Patents==\n*{{US patent|2784312}}\n\n==References==\n{{Reflist}}\n\n[[Category:Adders (electronics)]]\n[[Category:Digital electronics]]\n[[Category:Vacuum tubes]]\n[[Category:History of computing hardware]]\n\n{{Compu-hardware-stub}}"
    },
    {
      "title": "Brent–Kung adder",
      "url": "https://en.wikipedia.org/wiki/Brent%E2%80%93Kung_adder",
      "text": "{{ALUSidebar|expand=Components|expand-components=Adder}}\nThe '''Brent–Kung adder''' ('''BKA''' or '''BK'''), proposed in 1982,<ref name=\"Brent-Kung_1982\" /> is an advanced [[binary adder]] design, having a gate level depth of <math>O(log_2(n))</math>.\n\n==Introduction==\nThe Brent–Kung adder is a [[parallel prefix adder]] (PPA) form of [[carry-lookahead adder]] (CLA). Proposed by [[Richard Peirce Brent]] and Hsiang Te Kung in 1982 it introduced higher regularity to the adder structure and has less wiring congestion leading to better performance and less necessary chip area to implement compared to the [[Kogge–Stone adder]] (KSA). It is also much quicker than [[ripple-carry adder]]s (RCA).\n\nRipple-carry adders were the initial multi-bit adders which were developed in the early days and got their name from the ripple effect which the carry made while being propagated from right to left. The time taken for addition was directly proportional to the length of the bit being added. This is reverse in Brent–Kung adders where the carry is calculated in parallel thus reducing the addition time drastically. Further work has been done on Brent–Kung adders and other parallel adders to reduce the power consumption and chip area as well as to increase the speed thus making them suitable for low-power designs.\n\nA Brent–Kung adder is a parallel adder made in a regular layout with an aim of minimizing the chip area and ease manufacturing. The addition of n-bit number can be performed in time <math>O(\\log_2 n)</math> with a chip size of area <math>O(n \\log_2 n),</math> thus making it a good-choice adder with constraints on area and maximizing the performance. Its symmetry and regular build structure reduces costs of production effectively and enable it to be used in pipeline architectures.  In parallel adders the critical path is decided by computation of the carry from [[least significant bit]] (LSB) adder to the [[most significant bit]] (MSB) adder, therefore efforts are in reducing the critical path for the carry to reach the MSB.\n\n==Basic model outline==\nIn general, most of the adders use carry-in and the corresponding bits of two numbers (A and B) to get the corresponding sum bit and carry-out - with ripple carry adders taking <math>O(n)</math> time for carry to reach MSB.\n\n* Considering that <big>A&nbsp;= a<sub>n</sub> a<sub>n-1</sub>&nbsp;… a<sub>1</sub></big> and <big>B&nbsp;= b<sub>n</sub> b<sub>n-1</sub>&nbsp;… b<sub>1</sub></big> both be n-bit binary numbers.\n\n* With sum being <big>S&nbsp;= s<sub>n+1</sub> s<sub>n</sub>&nbsp;… s<sub>1</sub></big> and carry generated in each stage <big>C = c<sub>n</sub>&nbsp;… c<sub>0</sub></big> will be carry-in to next stages.\n[[File:Brent kung adder.jpg|thumb]]\n\n* For RCA, <big>c<sub>0</sub>&nbsp;= 0</big>, and ''i'' the sum bit and carry bit generated are <big>c<sub>i</sub>&nbsp;= g<sub>i</sub>&nbsp;∨ (a<sub>i</sub>&nbsp;∧ c<sub>i-1</sub>)&nbsp;∨ (b<sub>i</sub>&nbsp;∧ c<sub>i-1</sub>),<br>s<sub>i</sub>&nbsp;= a<sub>i</sub>&nbsp;⊕ b<sub>i</sub>&nbsp;⊕ c<sub>i-1</sub> for ''i''&nbsp;= 1, 2,&nbsp;… n<br>s<sub>n+1</sub>&nbsp;= c<sub>n</sub></big> respectively.\n* It is possible to transform the above ripple carry into carry-lookahead (CLA) by defining the carry bit ''i'' as <big>c<sub>0</sub>&nbsp;= 0, <br>c<sub>i</sub>&nbsp;= (a<sub>i</sub>&nbsp;∧ b<sub>i</sub>)&nbsp;∨ (p<sub>i</sub>&nbsp;∧ c<sub>i-1</sub>)</big> where <br /><big>g<sub>i</sub>&nbsp;= a<sub>i</sub>&nbsp;∧ b<sub>i</sub></big> and <big>p<sub>i</sub>&nbsp;= a<sub>i</sub>&nbsp;⊕ b<sub>i</sub></big> for i&nbsp;= 1, 2,&nbsp;… n. p and g are known as carry propagate and carry generate. This corresponds to the fact that the carry c<sub>i</sub> is either generated by a<sub>i</sub> and b<sub>i</sub> or propagated from the previous carry c<sub>i-1</sub>.\n\nBrent and Kung further transformed the carry generation and propagation by defining an operator <big>''o''</big> as<br><big>(a<sub>1</sub>, b<sub>1</sub>)&nbsp;''o'' (a<sub>2</sub>, b<sub>2</sub>)&nbsp;= (a<sub>1</sub>&nbsp;∨ (b<sub>1</sub>&nbsp;∧ a<sub>2</sub>), b<sub>1</sub>&nbsp;∧ b<sub>2</sub>)</big>.\n* They also defined a function (G<sub>i</sub>, P<sub>i</sub>)&nbsp;= (g<sub>1</sub>, p<sub>1</sub>) for i&nbsp;= 1;<br>otherwise (gi, pi)&nbsp;o (Gi-1, Pi-1) for <big>i&nbsp;= 2, 3,&nbsp;… n.</big> It can be derived that <big>G<sub>i</sub></big> in the function is equivalent to <big>c<sub>i</sub></big>. Also <big>(G<sub>n</sub>, P<sub>n</sub>)</big> can be non-recursively written as <big>= (g<sub>n</sub>, p<sub>n</sub>)&nbsp;o (g<sub>n-1</sub>, p<sub>n-1</sub>)&nbsp;o&nbsp;…&nbsp;o (g<sub>1</sub>, p<sub>1</sub>)</big>.\nTaking advantage of the associativity of operator <big>''o''</big> <big>(G<sub>n</sub>, P<sub>n</sub>)</big> can be computed in a tree-like manner.\n\nThe design of the white nodes is obvious as they are just buffering the g<sub>i</sub>'s and p<sub>i</sub>'s, and the black nodes are performing operation defined by operator <big>o</big>, which is similar to a one bit adder. \n* This tree-like propagation of carry reduces its critical path to that of tree height. As the carry tree height can be maximum of <math>O(log_2(n))</math>, the critical path of the Brent–Kung parallel adder is also <math>O(log_2(n))</math>, which is better than the normal adder performance of <math>O(n)</math>. The tree-based layout also reduces the chip area and redundant wiring required in general CLA-based adders.\n\n==Final processing stage==\nUsing the carry propagation and generation transformation for working out addition and carry used by Brent and Kung, the performance of the adder increases considerably and also leads to an increase in regularity. The final sum can be calculated as follows: si&nbsp;= pi&nbsp;⊕ ci-1\n\n==Low-power adder==\nThe increase in performance in Brent–Kung adders is attributed to its tree structure of carry propagation which also leads to lower power consumption as the carry signal now has to travel through fewer stages, leading to less switching of transistors. Also, the decrease in amount of wiring and fan-out also contributes largely to its lower power consumption than CLA adders. A Brent–Kung adder can also be used in a pipeline manner which can further reduce the power consumption by reducing the depth of combinatorial logic and glitches stabilization.<ref name=\"Brent-Kung_1982\"/> The graph shows a low-power Brent–Kung adder.<ref name=\"Alexander_2004\"/>\n\n==Comparison with Kogge–Stone adder==\n{{multiple image\n | align     = right\n | direction = vertical\n | header    =\n | header_align = left/right/center\n | header_background =\n | footer    =\n | footer_background =\n | width     =\n | image1    = Kogge-stone-8-bit.png\n | width1    = 158\n | caption1  = Diagram of an 8-bit [[Kogge–Stone adder]].\n | alt1      = \n | image2    = Brent-kung-8-bit.png\n | width2    = 158\n | caption2  = Diagram of an 8-bit Brent–Kung adder. The Brent–Kung adder uses fewer modules and connections than the Kogge–Stone adder.\n | alt2      = \n}}\n\n===Advantages===\nDue to this type of adder requiring fewer modules to implement than the [[Kogge–Stone adder]], the Brent–Kung adder is much simpler to build. It also contains far fewer connections to other modules, which also contributes to its simplicity.<ref name=\"Robey_2012\"/>\n\n===Disadvantages===\nOne major disadvantage of this adder is [[fan-out]]. Fan-out may split and weaken the current propagating through the adder.<ref name=\"Robey_2012\"/>\n\n==References==\n{{reflist|refs=\n<ref name=\"Brent-Kung_1982\">{{cite journal |author-first1=Richard Peirce |author-last1=Brent |author-link1=Richard Peirce Brent |author-first2=Hsiang Te |author-last2=Kung |title=A Regular Layout for Parallel Adders |publisher=Department of Computer Sciences, Carnegie-Mellon University, USA |journal=[[IEEE Transactions on Computers]] |volume=C-31 |issue=3 |date=March 1982 |orig-year=June 1979 |pages=260-264 |issn=0018-9340 |id=CMS-CS-79-131 |doi=10.1109/TC.1982.1675982 |url=http://www.dtic.mil/get-tr-doc/pdf?AD=ADA074455}}</ref>\n<ref name=\"Alexander_2004\">{{cite web |author-last=Alexander |author-first=Jonathan |title=VHDL Design Tips and Low Power Design Techniques |url=https://www.powershow.com/view4/55d38a-ZGUzM/VHDL_Design_Tips_and_Low_Power_Design_Techniques_powerpoint_ppt_presentation |date=2004 |access-date=2018-04-21 |dead-url=no}}<!-- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjWzNHfh8vaAhUHPVAKHeAMCLYQFjAAegQIABAr&url=http%3A%2F%2Fwww.klabs.org%2Fmapld04%2Ftutorials%2Fvhdl%2Fpresentations%2Flow_power_techniques.ppt] [http://www.microsemi.com/document-portal/doc_download/131328-vhdl-design-tips-and-low-power-design-techniques |website=www.microsemi.com] --></ref>\n<ref name=\"Robey_2012\">{{cite web |author-last=Pointer |author-first=Robey |title=How to add numbers (part 2) |date=2012-11-14\n|url=http://robey.lag.net/2012/11/14/how-to-add-numbers-2.html |website=robey.lag.net |access-date=2018-04-21 |dead-url=no |archive-url=https://web.archive.org/web/20180421092756/https://robey.lag.net/2012/11/14/how-to-add-numbers-2.html |archive-date=2018-04-21}}</ref>\n}}\n\n==Further reading==\n*<!-- <ref name=\"ULVD_2015\"> -->{{cite book |title=Ultra-Low-Voltage Design of Energy-Efficient Digital Circuits |author-first1=Nele |author-last1=Reynders |author-first2=Wim |author-last2=Dehaene |series=Analog Circuits And Signal Processing (ACSP) |date=2015 |edition=1 |location=Heverlee, Belgium |publisher=[[Springer International Publishing AG Switzerland]] |publication-place=Cham, Switzerland |isbn=978-3-319-16135-8 |issn=1872-082X |doi=10.1007/978-3-319-16136-5 |lccn=2015935431}}<!-- </ref> -->\n\n==External links==\n* [http://www.acsel-lab.com/Projects/fast_adder/adder_designs.htm Adder Designs]<!-- https://web.archive.org/web/20180421094527/http://www.acsel-lab.com/Projects/fast_adder/adder_designs.htm -->\n\n{{DEFAULTSORT:Brent-Kung adder}}\n\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Carry-lookahead adder",
      "url": "https://en.wikipedia.org/wiki/Carry-lookahead_adder",
      "text": "{{ALUSidebar|expand=Components|expand-components=Adder}}\n[[File:4-bit carry lookahead adder.svg|thumb|right|250px|4-bit adder with carry lookahead]]\n\nA '''carry-lookahead adder''' ('''CLA''') or '''fast adder''' is a type of [[adder (electronics)|adder]] used in [[digital logic]]. A carry-look ahead adder improves speed by reducing the amount of time required to determine carry bits. It can be contrasted with the simpler, but usually slower, [[ripple-carry adder]] (RCA), for which the carry bit is calculated alongside the sum bit, and each stage must wait until the previous carry bit has been calculated to begin calculating its own sum bit and carry bit. The carry-lookahead adder calculates one or more carry bits before the sum, which reduces the wait time to calculate the result of the larger-value bits of the adder. The [[Kogge–Stone adder]] (KSA) and [[Brent–Kung adder]] (BKA) are examples of this type of adder.\n\n[[Charles Babbage]] recognized the performance penalty imposed by ripple-carry and developed mechanisms for ''anticipating carriage'' in his computing engines.<ref name=\"Babbage_1864\"/> Gerald B. Rosenberger of [[IBM]] filed for a patent on a modern binary carry-lookahead adder in 1957.<ref name=\"Rosenberger_1960\"/>\n\n==Theory of operation==\nA ripple-carry adder works in the same way as pencil-and-paper methods of addition. Starting at the rightmost ([[least significant digit|least significant]]) digit position, the two corresponding digits are added and a result obtained. It is also possible that there may be a carry out of this digit position (for example, in pencil-and-paper methods, \"9 + 5 = 4, carry 1\"). Accordingly, all digit positions other than the rightmost one need to take into account the possibility of having to add an extra 1 from a carry that has come in from the next position to the right.\n\nThis means that no digit position can have an absolutely final value until it has been established whether or not a carry is coming in from the right. Moreover, if the sum without a carry is 9 (in pencil-and-paper methods) or 1 (in binary arithmetic), it is not even possible to tell whether or not a given digit position is going to pass on a carry to the position on its left. At worst, when a whole sequence of sums comes to …99999999… (in decimal) or …11111111… (in binary), nothing can be deduced at all until the value of the carry coming in from the right is known, and that carry is then propagated to the left, one step at a time, as each digit position evaluated \"9 + 1 = 0, carry 1\" or \"1 + 1 = 0, carry 1\". It is the \"rippling\" of the carry from right to left that gives a ripple-carry adder its name, and its slowness. When adding 32-bit integers, for instance, allowance has to be made for the possibility that a carry could have to ripple through every one of the 32 one-bit adders.\n\nCarry-lookahead depends on two things:\n# Calculating for each digit position whether that position is going to propagate a carry if one comes in from the right.\n# Combining these calculated values to be able to deduce quickly whether, for each group of digits, that group is going to propagate a carry that comes in from the right.\n\nSupposing that groups of four digits are chosen the sequence of events goes something like this:\n# All 1-bit adders calculate their results. Simultaneously, the lookahead units perform their calculations.\n# Assuming that a carry arises in a particular group, that carry will emerge at the left-hand end of the group within at most five gate delays and start propagating through the group to its left.\n# If that carry is going to propagate all the way through the next group, the lookahead unit will already have deduced this. Accordingly, ''before the carry emerges from the next group'', the lookahead unit is immediately (within one gate delay) able to tell the ''next'' group to the left that it is going to receive a carry – and, at the same time, to tell the next lookahead unit to the left that a carry is on its way.\n\nThe net effect is that the carries start by propagating slowly through each 4-bit group, just as in a ripple-carry system, but then move four times as fast, leaping from one lookahead-carry unit to the next. Finally, within each group that receives a carry, the carry propagates slowly within the digits in that group.\n\nThe more bits in a group, the more complex the lookahead carry logic becomes, and the more time is spent on the \"slow roads\" in each group rather than on the \"fast road\" between the groups (provided by the lookahead carry logic). On the other hand, the fewer bits there are in a group, the more groups have to be traversed to get from one end of a number to the other, and the less acceleration is obtained as a result.\n\nDeciding the group size to be governed by lookahead carry logic requires a detailed analysis of gate and propagation delays for the particular technology being used.\n\nIt is possible to have more than one level of lookahead-carry logic, and this is in fact usually done. Each lookahead-carry unit already produces a signal saying \"if a carry comes in from the right, I will propagate it to the left\", and those signals can be combined so that each group of, say, four lookahead-carry units becomes part of a \"supergroup\" governing a total of 16 bits of the numbers being added. The \"supergroup\" lookahead-carry logic will be able to say whether a carry entering the supergroup will be propagated all the way through it, and using this information, it is able to propagate carries from right to left 16 times as fast as a naive ripple carry. With this kind of two-level implementation, a carry may first propagate through the \"slow road\" of individual adders, then, on reaching the left-hand end of its group, propagate through the \"fast road\" of 4-bit lookahead-carry logic, then, on reaching the left-hand end of its supergroup, propagate through the \"superfast road\" of 16-bit lookahead-carry logic.\n\nAgain, the group sizes to be chosen depend on the exact details of how fast signals propagate within logic gates and from one logic gate to another.\n\nFor very large numbers (hundreds or even thousands of bits), lookahead-carry logic does not become any more complex, because more layers of supergroups and supersupergroups can be added as necessary. The increase in the number of gates is also moderate: if all the group sizes are four, one would end up with one third as many lookahead carry units as there are adders. However, the \"slow roads\" on the way to the faster levels begin to impose a drag on the whole system (for instance, a 256-bit adder could have up to 24 gate delays in its carry processing), and the mere physical transmission of signals from one end of a long number to the other begins to be a problem. At these sizes, [[carry-save adder]]s are preferable, since they spend no time on carry propagation at all.\n\n==Carry lookahead method==\nCarry-lookahead logic uses the concepts of ''generating'' and ''propagating'' carries.  Although in the context of a carry-lookahead adder, it is most natural to think of generating and propagating in the context of binary addition, the concepts can be used more generally than this. In the descriptions below, the word ''digit'' can be replaced by ''bit'' when referring to binary addition of 2.\n\nThe addition of two 1-digit inputs <var>A</var> and <var>B</var> is said to ''generate'' if the addition will always carry, regardless of whether there is an input-carry (equivalently, regardless of whether any less significant digits in the sum carry). For example, in the decimal addition 52 + 67, the addition of the tens digits 5 and 6 ''generates'' because the result carries to the hundreds digit regardless of whether the ones digit carries (in the example, the ones digit does not carry (2 + 7 = 9)).\n\nIn the case of binary addition, <math>A + B</math> generates if and only if both <var>A</var> and <var>B</var> are 1. If we write <math>G(A, B)</math> to represent the binary predicate that is true if and only if <math>A + B</math> generates, we have\n\n:<math>G(A, B) = A \\cdot B</math>\n\nwhere <math>A \\cdot B</math> is a [[logical conjunction]] (i.e., an ''and'').\n\nThe addition of two 1-digit inputs <var>A</var> and <var>B</var> is said to ''propagate'' if the addition will carry whenever there is an input carry (equivalently, when the next less significant digit in the sum carries). For example, in the decimal addition 37 + 62, the addition of the tens digits 3 and 6 ''propagate'' because the result would carry to the hundreds digit ''if'' the ones were to carry (which in this example, it does not).  Note that propagate and generate are defined with respect to a single digit of addition and do not depend on any other digits in the sum.\n\nIn the case of binary addition, <math>A + B</math> propagates if and only if at least one of <var>A</var> or <var>B </var> is 1. If <math>P(A, B)</math> is written to represent the binary predicate that is true if and only if <math>A + B</math> propagates, one has\n\n:<math>P(A, B) = A + B</math>\n\nwhere <math>A + B</math> on the right-hand side of the equation is a [[logical disjunction]] (i.e., an ''or'').\n\nSometimes a slightly different definition of ''propagate'' is used. By this definition <var>A + B</var> is said to propagate if the addition will carry whenever there is an input carry, but will not carry if there is no input carry. Due to the way generate and propagate bits are used by the carry-lookahead logic, it doesn't matter which definition is used. In the case of binary addition, this definition is expressed by\n\n:<math>P'(A, B) = A \\oplus B</math>\n\nwhere <math>A \\oplus B</math> is an [[exclusive or]] (i.e., an ''xor'').\n\nFor binary arithmetic, ''or'' is faster than ''xor'' and takes fewer transistors to implement. However, for a multiple-level carry-lookahead adder, it is simpler to use <math>P'(A, B)</math>.\n\nGiven these concepts of generate and propagate, a digit of addition carries precisely when either the addition generates ''or'' the next less significant bit carries and the addition propagates. Written in boolean algebra, with <math>C_i</math> the carry bit of digit <var>i</var>, and <math>P_i</math> and <math>G_i</math> the propagate and generate bits of digit <var>i</var> respectively,\n\n:<math>C_{i+1} = G_i + (P_i \\cdot C_i).</math>\n\n==Implementation details==\nFor each bit in a binary sequence to be added, the carry-lookahead logic will determine whether that bit pair will generate a carry or propagate a carry. This allows the circuit to \"pre-process\" the two numbers being added to determine the carry ahead of time. Then, when the actual addition is performed, there is no delay from waiting for the ripple-carry effect (or time it takes for the carry from the first [[full adder]] to be passed down to the last full adder). Below is a simple 4-bit generalized carry-lookahead circuit that combines with the 4-bit ripple-carry adder we used above with some slight adjustments:\n\n<!-- Image with unknown copyright status removed: [[File:4-bit carry lookahead adder.png|thumb|400px|4-bit carry lookahead adder]] -->\nFor the example provided, the logic for the generate (g) and propagate (p) values are given below. The numeric value determines the signal from the circuit above, starting from 0 on the far right to 3 on the far left:\n\n:<math>C_1 = G_0 + P_0 \\cdot C_0,</math>\n:<math>C_2 = G_1 + P_1 \\cdot C_1,</math>\n:<math>C_3 = G_2 + P_2 \\cdot C_2,</math>\n:<math>C_4 = G_3 + P_3 \\cdot C_3.</math>\n\nSubstituting <math>C_1</math> into <math>C_2</math>, then <math>C_2</math> into <math>C_3</math>, then <math>C_3</math> into <math>C_4</math> yields the expanded equations:\n\n:<math>C_1 = G_0 + P_0 \\cdot C_0,</math>\n:<math>C_2 = G_1 + G_0 \\cdot P_1 + C_0 \\cdot P_0 \\cdot P_1,</math>\n:<math>C_3 = G_2 + G_1 \\cdot P_2 + G_0 \\cdot P_1 \\cdot P_2 + C_0 \\cdot P_0 \\cdot P_1 \\cdot P_2,</math>\n:<math>C_4 = G_3 + G_2 \\cdot P_3 + G_1 \\cdot P_2 \\cdot P_3 + G_0 \\cdot P_1 \\cdot P_2 \\cdot P_3 + C_0 \\cdot P_0 \\cdot P_1 \\cdot P_2 \\cdot P_3.</math>\n\nTo determine whether a bit pair will generate a carry, the following logic works:\n:<math>G_i = A_i \\cdot B_i.</math>\n\nTo determine whether a bit pair will propagate a carry, either of the following logic statements work:\n:<math>P_i = A_i \\oplus B_i,</math>\n:<math>P_i = A_i + B_i.</math>\n\nThe reason why this works is based on evaluation of <math>C_1 = G_0 + P_0 \\cdot C_0</math>. The only difference in the truth tables between (<math>A \\oplus B</math>) and (<math>A + B</math>) is when both <math>A</math> and <math>B</math> are 1.  However, if both <math>A</math> and <math>B</math> are 1, then the <math>G_0</math> term is 1 (since its equation is <math>A \\cdot B</math>), and the <math>P_0 \\cdot C_0</math> term becomes irrelevant. The XOR is used normally within a basic full adder circuit; the OR is an alternative option (for a carry-lookahead only), which is far simpler in transistor-count terms.\n\nThe carry-lookahead 4-bit adder can also be used in a higher-level circuit by having each CLA logic circuit produce a propagate and generate signal to a higher-level CLA logic circuit. The group propagate (<math>PG</math>) and group generate (<math>GG</math>) for a 4-bit CLA are:\n:<math>PG = P_0 \\cdot P_1 \\cdot P_2 \\cdot P_3,</math>\n:<math>GG = G_3 + G_2 \\cdot P_3 + G_1 \\cdot P_3 \\cdot P_2 + G_0 \\cdot P_3 \\cdot P_2 \\cdot P_1.</math>\n\nThey can then be used to create a carry-out for that particular 4-bit group:\n:<math>CG = GG + PG \\cdot C_{in}.</math>\n\nIt can be seen that this is equivalent to <math>C_4</math> in previous equations.\n\nPutting four 4-bit CLAs together yields four group propagates and four group generates.\nA [[lookahead-carry unit]] (LCU) takes these 8 values and uses identical logic to calculate <math>C_i</math> in the CLAs.\nThe LCU then generates the carry input for each of the 4 CLAs and a fifth equal to <math>C_{16}</math>.\n\nThe calculation of the [[gate delay]] of a 16-bit adder (using 4 CLAs and 1 LCU) is not as straight forward as the ripple carry adder.\n\nStarting at time of zero:\n* calculation of <math>P_i</math> and <math>G_i</math> is done at time 1,\n* calculation of <math>C_i</math> is done at time 3,\n* calculation of the <math>PG</math> is done at time 2,\n* calculation of the <math>GG</math> is done at time 3,\n* calculation of the inputs for the CLAs from the LCU are done at:\n** time 0 for the first CLA,\n** time 5 for the second, third and fourth CLA,\n* calculation of the <math>S_i</math> are done at:\n** time 4 for the first CLA,\n** time 8 for the second, third & fourth CLA,\n* calculation of the final carry bit (<math>C_{16}</math>) is done at time 5.\nThe maximal time is 8 gate delays (for <math>S_{[8-15]}</math>).\n\nA standard 16-bit [[ripple-carry adder]] would take 16 × 3 − 1 = 47 gate delays.\n\n==Manchester carry chain==\nThe Manchester carry chain is a variation of the carry-lookahead adder<ref name=\"Manchester\"/> that uses shared logic to lower the transistor count. As can be seen above in the implementation section, the logic for generating each carry contains all of the logic used to generate the previous carries. A Manchester carry chain generates the intermediate carries by tapping off nodes in the gate that calculates the most significant carry value. However, not all [[logic families]] have these internal nodes, [[CMOS]] being a major example. [[Dynamic logic (digital logic)|Dynamic logic]] can support shared logic, as can [[transmission gate]] logic. One of the major downsides of the Manchester carry chain is that the capacitive load of all of these outputs, together with the resistance of the transistors causes the propagation delay to increase much more quickly than a regular carry lookahead. A Manchester-carry-chain section generally doesn't exceed 4 bits.\n\n==See also==\n* [[Carry-skip adder]]\n* [[Carry operator]]\n* [[Speculative execution]]\n\n==References==\n{{reflist|refs=\n<ref name=\"Babbage_1864\">{{cite book |author-first=Charles |author-last=Babbage |author-link=Charles Babbage |url=https://books.google.com/books?id=Fa1JAAAAMAAJ&pg=PR3 |title=Passages from the Life of a Philosopher |publisher=Longman, Green, Longmand Roberts & Green |location=London |date=1864 |pages=59–63, 114–116}}</ref>\n<ref name=\"Rosenberger_1960\">{{cite web |author-first=Gerald B. |author-last=Rosenberger |url=http://www.google.com/patents?id=SpBnAAAAEBAJ |title=Simultaneous Carry Adder |id=U.S. Patent 2,966,305 |date=1960-12-27}}</ref>\n<ref name=\"Manchester\">{{Cite web |url=https://en.wikichip.org/wiki/Manchester_carry-chain_adder |title=Manchester carry-chain adder - WikiChip |website=en.wikichip.org |access-date=2017-04-24}}</ref>\n}}\n\n==Further reading==\n* [http://www.aoki.ecei.tohoku.ac.jp/arith/mg/algorithm.html#fsa_rcl Hardware algorithms for arithmetic modules], ARITH research group, Aoki lab., Tohoku University\n* {{cite book |author-last=Katz |author-first=Randy |authorlink=Randy Katz |title=Contemporary Logic Design |publisher=[[The Benjamin/Cummings Publishing Company]] |date=1994 |isbn=0-8053-2703-7 |doi=10.1016/0026-2692(95)90052-7 |pages=249–256}}\n* {{cite web |title=Advanced Arithmetic Techniques |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2006 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0202.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703001722/http://www.quadibloc.com/comp/cp0202.htm |archive-date=2018-07-03}}\n\n==External links==\n* [http://www.ecs.umass.edu/ece/koren/arith/simulator/Add/lookahead/ Carry Look Ahead Adder JavaScript simulator]\n\n{{DEFAULTSORT:Carry Look-Ahead Adder}}\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Carry-skip adder",
      "url": "https://en.wikipedia.org/wiki/Carry-skip_adder",
      "text": "{{ALUSidebar|expand=Components|expand-components=Adder}}\nA '''carry-skip adder'''<ref group=\"nb\" name=\"NB_CSA\"/> (also known as a '''carry-bypass adder''') is an [[adder (electronics)|adder]] implementation that improves on the delay of a [[Ripple carry adder#Ripple-carry adder|ripple-carry adder]] with little effort compared to other adders. The improvement of the worst-case delay is achieved by using several carry-skip adders to form a block-carry-skip adder.\n\n==Single carry-skip adder==\nThe worst case for a simple one level carry-ripple-adder occurs, when the propagate-condition<ref name=\"Parhami_2000\"/> is true for each digit pair <math>(a_i, b_i)</math>. Then the carry-in ripples through the <math>n</math>-bit adder and appears as the carry-out after <math>\\tau_{CRA}(n) \\approx n \\cdot \\tau_{VA}</math>.\n[[File:FAwithGP.svg|thumb|Full adder with additional generate and propagate signals.]]\n\nFor each operand input bit pair <math>(a_i,b_i)</math> the propagate-conditions <math>p_i = a_i \\oplus b_i</math> are determined using an XOR-Gate (see ). When all propagate-conditions are ''true'', then the carry-in bit <math>c_0</math> determines the carry-out bit.\n\nThe ''n''-bit-carry-skip adder consists of a ''n''-bit-carry-ripple-chain, a ''n''-input AND-gate and one multiplexer.\nEach propagate bit <math>p_i</math>, that is provided by the carry-ripple-chain is connected to the ''n''-input AND-gate. The resulting bit is used as the select bit of a multiplexer that switches either the last carry-bit <math>c_n</math> or the carry-in <math>c_0</math> to the carry-out signal <math>c_{out}</math>. \n* <math>s = p_{n-1} \\wedge p_{n-2} \\wedge \\dots \\wedge p_1 \\wedge p_0 = p_{[0:n-1]}</math>\n\nThis greatly reduces the latency of the adder through its critical path, since the carry bit for each block can now \"skip\" over blocks with a ''group'' propagate signal set to logic 1 (as opposed to a long ripple-carry chain, which would require the carry to ripple through each bit in the adder).\nThe number of inputs of the AND-gate is equal to the width of the adder. For a large width, this becomes impractical and leads to additional delays, because the AND-gate has to be built as a tree. A good width is achieved, when the sum-logic has the same depth like the ''n''-input AND-gate and the multiplexer.\n\n[[File:CSAdder4Bit.svg|thumb|4 bit carry-skip adder.]]\n\n===Performance===\nThe critical path of a carry-skip-adder begins at the first full-adder, passes through all adders and ends at the sum-bit <math>s_{n-1}</math>. Carry-skip-adders are chained (see block-carry-skip-adders) to reduce the overall critical path, since a single <math>n</math>-bit carry-skip-adder has no real speed benefit compared to a <math>n</math>-bit carry-ripple-adder.\n:<math>\\tau_{CSA}(n) = \\tau_{CRA}(n)</math>\nThe skip-logic consists of a <math>m</math>-input AND-gate and one multiplexer.\n:<math>T_{SK} = T_{AND}(m) + T_{MUX}</math>\nAs the propagate signals are computed in parallel and are early available, the critical path for the skip logic in a carry-skip adder consists only of the delay imposed by the multiplexer (conditional skip).\n:<math>T_{CSK} = T_{MUX} = 2 D</math>.\n\n==Block-carry-skip adders==\n[[File:BCSAdder16Bit.svg|upright=4.0|thumb|center|16-bit fixed-block-carry-skip adder with a block size of 4 bit.]]\nBlock-carry-skip adders are composed of a number of carry-skip adders. There are two types of block-carry-skip adders\nThe two operands <math>A = (a_{n-1}, a_{n-2}, \\dots , a_1, a_0)</math> and <math>B = (b_{n-1}, b_{n-2}, \\dots , b_1, b_0)</math> are split in <math>k</math> blocks of <math>(m_{k}, m_{k-1}, \\dots , m_{2}, m_{1})</math> bits.\n* Why are block-carry-skip-adders used?\n* Should the block-size be constant or variable?\n* Fixed block width vs. variable block width\n\n===Fixed size block-carry-skip adders===\nFixed size block-carry-skip adders split the <math>n</math> bits of the input bits into blocks of <math>m</math> bits each, resulting in <math>k = \\frac{n}{m}</math> blocks.\nThe critical path consists of the ripple path and the skip element of the first block, the skip paths that are enclosed between the first and the last block, and finally the ripple-path of the last block.\n:<math>T_{FCSA}(n) = T_{CRA_{[0:c_{out}]}}(m) + T_{CSK} + (k-2) \\cdot T_{CSK} + T_{CRA}(m) = 3 D + m \\cdot 2 D + (k-1) \\cdot 2 D + (m+2) 2 D = (2m + k) \\cdot 2 D + 5 D </math>\nThe optimal block size for a given adder width ''n'' is derived by equating to 0\n:<math>\\frac{d T_{FCSA}(n)}{d m} = 0</math>\n:<math>2 D \\cdot \\left(2 - n \\cdot \\frac{1}{m^2} \\right) = 0</math>\n:<math>\\Rightarrow m_{1,2} = \\pm \\sqrt{\\frac{n}{2}}</math>\nOnly positive block sizes are realizable\n:<math>\\Rightarrow m = \\sqrt{\\frac{n}{2}}</math>\n\n===Variable size block-carry-skip adders===\n\nThe performance can be improved, i.e. all carries propagated more quickly by varying the block sizes.  Accordingly the initial blocks of the adder are made smaller so as to quickly detect carry generates that must be propagated the furthers, the middle blocks are made larger because they are not the problem case, and then the most significant blocks are again made smaller so that the late arriving carry inputs can be processed quickly.\n\n==Multilevel carry-skip adders==\nBy using additional skip-blocks in an additional layer, the block-propagate signals <math>p_{[i:i+3]}</math> are further summarized and used to perform larger skips:\n:<math>p_{[i:i+15]} = p_{[i:i+3]} \\wedge p_{[i+4:i+7]} \\wedge p_{[i+8:i+11]} \\wedge p_{[i+12:i+15]}</math>\nThus making the adder even faster.\n\n==Carry-skip optimization==\n\nThe problem of determining the block sizes and number of levels required to make the physically fastest carry skip adder is known as the 'carry-skip adder optimization problem'.  This problem is made complex by the fact that a carry-skip adders are implemented with physical devices whose size and other parameters also affects addition time.\n\nThe carry-skip optimization problem for variable block sizes and multiple levels for an arbitrary device process node was solved by Thomas W. Lynch.<ref name=\"Lynch_1996\"/> This reference also shows that carry-skip addition is the same as parallel prefix addition and is thus related to, and for some configurations identical to the [[Han–Carlson adder|Han–Carlson]],<ref name=\"Han-Carlson_Levitan_1981\"/><ref name=\"Han-Carlson_1987\"/> the [[Brent–Kung adder|Brent–Kung]],<ref name=\"Brent-Kung_1982\"/> the [[Kogge-Stone adder]]<ref name=\"Kogge-Stone_1973\"/> and a number of other adder types.\n\n==Implementation overview==\n\nBreaking this down into more specific terms, in order to build a 4-bit carry-bypass adder, 6 [[Full adder#Full adder|full adders]] would be needed. The input buses would be a 4-bit ''A'' and a 4-bit ''B'', with a carry-in (''CIN'') signal. The output would be a 4-bit bus X and a carry-out signal (''COUT'').\n\nThe first two full adders would add the first two bits together. The carry-out signal from the second full adder (<math>C_1</math>)would drive the select signal for three 2 to 1 multiplexers. The second set of 2 full adders would add the last two bits assuming <math>C_1</math> is a logical 0. And the final set of full adders would assume that <math>C_1</math> is a logical 1.\n\nThe multiplexers then control which output signal is used for ''COUT'', <math>X_2</math> and <math>X_3</math>.\n\n==Notes==\n{{reflist|group=\"nb\"|refs=\n<ref group=\"nb\" name=\"NB_CSA\">''Carry-skip adder'' is often abbreviated as CSA, however, this can be confused with [[carry-save adder]].</ref>\n}}\n\n==References==\n{{reflist|refs=\n<ref name=\"Parhami_2000\">{{cite book |title=Computer arithmetic: Algorithms and Hardware Designs |author-first=Behrooz |author-last=Parhami |publisher=[[Oxford University Press]] |date=2000 |isbn=0-19-512583-5 |page=108}}</ref>\n<ref name=\"Lynch_1996\">{{cite web |author-last=Lynch |author-first=Thomas Walker |url=http://repositories.lib.utexas.edu/handle/2152/13960 |title=Binary Adders |publisher=[[University of Texas]] |type=Thesis |date=May 1996 |access-date=2018-04-14 |dead-url=no |archive-url=https://web.archive.org/web/20180414171823/https://repositories.lib.utexas.edu/bitstream/handle/2152/13960/txu-oclc-35052840.pdf?sequence=2&isAllowed=y |archive-date=2018-04-14}}</ref>\n<ref name=\"Kogge-Stone_1973\">{{cite journal |author-last1=Kogge |author-first1=Peter Michael |author-link=Peter Michael Kogge |author-last2=Stone |author-first2=Harold S. |title=A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations |journal=[[IEEE Transactions on Computers]] |date=August 1973 |volume=C-22 |issue=8 |pages=786-793 |doi=10.1109/TC.1973.5009159}}</ref>\n<ref name=\"Brent-Kung_1982\">{{cite journal |author-first1=Richard Peirce |author-last1=Brent |author-link1=Richard Peirce Brent |author-first2=Hsiang Te |author-last2=Kung |title=A regular layout for parallel adders |journal=[[IEEE Transactions on Computers]] |volume=C-31 |issue=3 |date=March 1982 |pages=260-264 |doi=10.1109/TC.1982.1675982|url=http://www.dtic.mil/get-tr-doc/pdf?AD=ADA074455 }}</ref>\n<ref name=\"Han-Carlson_1987\">{{cite journal |author-first1=Tackdon |author-last1=Han |author-first2=David A. |author-last2=Carlson |title=Fast area-efficient VLSI adders |journal=Proceedings 8th Symposium on Computer Arithmetic |pages=49-56 |publisher=[[IEEE]] |date=October 1987}}</ref>\n<ref name=\"Han-Carlson_Levitan_1981\">{{cite journal |author-first1=Tackdon |author-last1=Han |author-first2=David A. |author-last2=Carlson |author-first3=Steven P. |author-last3=Levitan |title=VLSI design of high-speed, low-area addition circuitry |journal=Proceedings 1981 IEEE International Conference on Computer Design: VLSI in Computers & Processors |pages=418-422 |publisher=[[IEEE]] |date=October 1982 |url=https://yonsei.pure.elsevier.com/en/publications/vlsi-design-of-high-speed-low-area-addition-circuitry |isbn=0-81860802-1}}</ref>\n}}\n\n==External links==\n* [http://www.utdallas.edu/~ivor/ce6305/m7p.pdf Explanation for critical path of the variable-skip adder]\n\n{{DEFAULTSORT:Carry-bypass adder}}\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Carry-select adder",
      "url": "https://en.wikipedia.org/wiki/Carry-select_adder",
      "text": "{{ALUSidebar|expand=Components|expand-components=Adder}}\n{{refimprove|date=December 2009}}\n\nIn electronics, a '''carry-select adder''' is a particular way to implement an [[adder (electronics)|adder]], which is a logic element that computes the <math>(n+1)</math>-bit sum of two <math>n</math>-bit numbers. The carry-select adder is simple but rather fast, having a gate level depth of <math>O(\\sqrt n)</math>.\n\n==Construction==\nThe carry-select adder generally consists of two [[Adder (electronics)#Multiple-bit adders|ripple carry adder]]s and a [[multiplexer]]. Adding two n-bit numbers with a carry-select adder is done with two adders (therefore two ripple carry adders), in order to perform the calculation twice, one time with the assumption of the carry-in being zero and the other assuming it will be one. After the two results are calculated, the correct sum, as well as the correct carry-out, is then selected with the multiplexer once the correct carry-in is known.\n\nThe number of bits in each carry select block can be uniform, or variable.  In the uniform case, the optimal delay occurs for a block size of <math>\\lfloor \\sqrt n \\rfloor</math>.  When variable, the block size should have a delay, from addition inputs A and B to the carry out, equal to that of the multiplexer chain leading into it, so that the carry out is calculated just in time. The <math>O(\\sqrt n)</math> delay is derived from uniform sizing, where the ideal number of full-adder elements per block is equal to the square root of the number of bits being added, since that will yield an equal number of MUX delays.\n\n===Basic building block===\n\n[[File:Carry-select-adder-detailed-block.png|300px]]\n\nAbove is the basic building block of a carry-select adder, where the block size is 4. Two 4-bit ripple carry adders are multiplexed together, where the resulting carry and sum bits are selected by the carry-in. Since one ripple carry adder assumes a carry-in of 0, and the other assumes a carry-in of 1, selecting which adder had the correct assumption via the actual carry-in yields the desired result.\n\n===Uniform-sized adder===\n\n[[File:Carry-select-adder-fixed-size.png|800px]]\n\nA 16-bit carry-select adder with a uniform block size of 4 can be created with three of these blocks and a 4-bit ripple carry adder. Since carry-in is known at the beginning of computation, a carry select block is not needed for the first four bits. The delay of this adder will be four full adder delays, plus three MUX delays.\n\n===Variable-sized adder===\n\n[[File:Carry-select-adder-variable-size.png|800px]]\n\nA 16-bit carry-select adder with variable size can be similarly created. Here we show an adder with block sizes of 2-2-3-4-5. This break-up is ideal when the full-adder delay is equal to the MUX delay, which is unlikely. The total delay is two full adder delays, and four mux delays. We try to make the delay through the two carry chains and the delay of the previous stage carry equal.\n\n===Conditional sum adder===\n\nA '''conditional sum adder''' is a recursive structure based on the carry-select adder.  In the conditional sum adder, the MUX level chooses between two ''n/2''-bit inputs that are themselves built as conditional-sum adder.  The bottom level of the tree consists of pairs of 2-bit adders (1 half adder and 3 full adders) plus 2 single-bit multiplexers.\n\nThe conditional sum adder suffers from a very large [[fan-out]] of the intermediate carry outputs. The fan out can be as high as ''n/2'' on the last level, where <math>c_{n/2-1}</math> drives all multiplexers from <math>s_{n/2}</math> to <math>s_{n-1}</math>.\n\n==Combining with other adder structures==\n\nThe carry-select adder design can be complemented with a [[carry-lookahead adder]] structure to generate the MUX inputs, thus gaining even greater performance as a parallel prefix adder while potentially reducing area.\n\nAn example is shown in the [[Kogge–Stone adder]] article.\n\n==Further reading==\n* {{anchor|Sklansky adder}} {{cite web |title=Advanced Arithmetic Techniques |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2006 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0202.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703001722/http://www.quadibloc.com/comp/cp0202.htm |archive-date=2018-07-03}}\n\n{{DEFAULTSORT:Carry-select adder}}\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Kogge–Stone adder",
      "url": "https://en.wikipedia.org/wiki/Kogge%E2%80%93Stone_adder",
      "text": "{{ALUSidebar|expand=Components|expand-components=Adder}}\n[[Image:4 bit Kogge Stone Adder Example new.png|thumb|Example of a 4-bit Kogge–Stone adder with zero carry-in.]]\n\nThe '''Kogge–Stone adder''' ('''KSA''' or '''KS''') is a parallel prefix form [[carry look-ahead adder]].  Other [[parallel prefix adder]]s (PPA) include the ''[[Brent–Kung adder]]'' (BKA),<ref name=\"Brent-Kung_1982\"/> the ''[[Han–Carlson adder]]'' (HCA),<ref name=\"Han-Carlson_Levitan_1981\"/><ref name=\"Han-Carlson_1987\"/> and the fastest known variation, the ''[[Lynch–Swartzlander adder|Lynch–Swartzlander]] [[spanning tree adder]]'' (STA).<ref name=\"Lynch-Swartzlander_1991\"/><ref name=\"Lynch_1996\"/>\n\nThe '''Kogge–Stone adder''' takes more area to implement than the Brent–Kung adder, but has a lower [[fan-out]] at each stage, which increases performance for typical CMOS process nodes. However, wiring congestion is often a problem for Kogge–Stone adders.  The Lynch–Swartzlander design is smaller, has lower [[fan-out]], and does not suffer from wiring congestion; however to be used the process node must support [[Manchester carry chain]] implementations.  The general problem of optimizing parallel prefix adders is identical to the variable block size, multi level, [[carry-skip adder]] optimization problem, a solution of which is found in Thomas Lynch's thesis of 1996.<ref name=\"Lynch_1996\"/>\n\nAn example of a 4-bit Kogge–Stone adder is shown in the diagram. Each vertical stage produces a \"propagate\" and a \"generate\" bit, as shown. The culminating generate bits (the [[Carry (arithmetic)|carries]]) are produced in the last stage (vertically), and these bits are [[XOR]]'d with the initial propagate after the input (the red boxes) to produce the sum bits. E.g., the first (least-significant) sum bit is calculated by XORing the propagate in the farthest-right red box (a \"1\") with the carry-in (a \"0\"), producing a \"1\". The second bit is calculated by XORing the propagate in second box from the right (a \"0\") with C0 (a \"0\"), producing a \"0\".\n\nThe Kogge–Stone adder concept was developed by [[Peter M. Kogge]] and [[Harold S. Stone]], who published it in a seminal 1973 paper titled ''A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations''.<ref name=\"Kogge-Stone_1973\"/>\n\n== {{anchor|SKA}}Enhancements ==\n\nEnhancements to the original implementation include increasing the radix and sparsity of the adder. The ''radix'' of the adder refers to how many results from the previous level of computation are used to generate the next one. The original implementation uses radix-2, although it's possible to create radix-4 and higher. Doing so increases the power and delay of each stage, but reduces the number of required stages. In the so called '''sparse Kogge–Stone adder''' ('''SKA''') the ''sparsity'' of the adder refers to how many carry bits are generated by the carry-tree. Generating every carry bit is called sparsity-1, whereas generating every other is sparsity-2 and every fourth is sparsity-4. The resulting carries are then used as the carry-in inputs for much shorter ripple carry adders or some other adder design, which generates the final sum bits. Increasing sparsity reduces the total needed computation and can reduce the amount of routing congestion.\n\n[[Image:Kogge-stone-sparsity-4.png|600px|Kogge–Stone Adder of sparsity-4. Example of a Kogge–Stone adder with sparsity-4. Elements eliminated by sparsity shown marked with transparency.]]\n\nAbove is an example of a Kogge–Stone adder with sparsity-4. Elements eliminated by sparsity shown marked with transparency. As shown, power and area of the carry generation is improved significantly, and routing congestion is substantially reduced. Each generated carry feeds a multiplexer for a carry select adder or the carry-in of a ripple carry adder.\n\n== Expansion ==\nThis example is a carry look ahead - \nIn a 4 bit adder like the one shown in the introductory image of this article, there are 5 outputs. Below is the expansion:\n\n S0 = (A0 XOR B0) XOR Cin\n S1 = (A1 XOR B1) XOR ((A0 AND B0) OR (A0 XOR B0)  AND Cin)\n S2 = (A2 XOR B2) XOR (((A1 XOR B1) AND ((A0 AND B0) OR (A0 XOR B0) AND Cin)) OR (A1 AND B1))\n S3 = (A3 XOR B3) XOR ((((A2 XOR B2) AND (A1 XOR B1)) AND ((A0 AND B0) OR (A0 XOR B0) AND Cin)) OR (((A2 XOR B2) AND (A1 AND B1)) OR (A2 AND B2)))\n S4 = (A3 AND B3) OR (A3 XOR B3) AND ((((A2 XOR B2) AND (A1 XOR B1)) AND ((A0 AND B0) OR (A0 XOR B0) AND Cin)) OR (((A2 XOR B2) AND (A1 AND B1)) OR (A2 AND B2)))\n\n== References ==\n{{reflist|refs=\n<ref name=\"Lynch-Swartzlander_1991\">{{cite journal |author-first1=Thomas Walker |author-last1=Lynch |author-first2=Earl E. |author-last2=Swartzlander, Jr. |title=A spanning tree carry lookahead adder |journal=[[IEEE Transactions on Computers]] |volume=41 |pages=931-939 |date=August 1992}}</ref>\n<ref name=\"Lynch_1996\">{{cite web |author-last=Lynch |author-first=Thomas Walker |url=http://repositories.lib.utexas.edu/handle/2152/13960 |title=Binary Adders |publisher=[[University of Texas]] |type=Thesis |date=May 1996 |access-date=2018-04-14 |dead-url=no |archive-url=https://web.archive.org/web/20180414171823/https://repositories.lib.utexas.edu/bitstream/handle/2152/13960/txu-oclc-35052840.pdf?sequence=2&isAllowed=y |archive-date=2018-04-14}}</ref>\n<ref name=\"Kogge-Stone_1973\">{{cite journal |author-last1=Kogge |author-first1=Peter Michael |author-link=Peter Michael Kogge |author-last2=Stone |author-first2=Harold S. |title=A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations |journal=[[IEEE Transactions on Computers]] |date=August 1973 |volume=C-22 |issue=8 |pages=786-793 |doi=10.1109/TC.1973.5009159}}</ref>\n<ref name=\"Brent-Kung_1982\">{{cite journal |author-first1=Richard Peirce |author-last1=Brent |author-link1=Richard Peirce Brent |author-first2=Hsiang Te |author-last2=Kung |title=A Regular Layout for Parallel Adders |journal=[[IEEE Transactions on Computers]] |volume=C-31 |issue=3 |date=March 1982 |pages=260-264 |issn=0018-9340 |doi=10.1109/TC.1982.1675982 |url=http://www.dtic.mil/get-tr-doc/pdf?AD=ADA074455}}</ref>\n<ref name=\"Han-Carlson_1987\">{{cite journal |author-first1=Tackdon |author-last1=Han |author-first2=David A. |author-last2=Carlson |title=Fast area-efficient VLSI adders |journal=Proceedings 8th Symposium on Computer Arithmetic |pages=49-56 |publisher=[[IEEE]] |date=October 1987}}</ref>\n<ref name=\"Han-Carlson_Levitan_1981\">{{cite journal |author-first1=Tackdon |author-last1=Han |author-first2=David A. |author-last2=Carlson |author-first3=Steven P. |author-last3=Levitan |title=VLSI design of high-speed, low-area addition circuitry |journal=Proceedings 1981 IEEE International Conference on Computer Design: VLSI in Computers & Processors |pages=418-422 |publisher=[[IEEE]] |date=October 1982 |url=https://yonsei.pure.elsevier.com/en/publications/vlsi-design-of-high-speed-low-area-addition-circuitry |isbn=0-81860802-1}}</ref>\n}}\n\n==Further reading==\n* {{cite book |chapter=Energy-Delay Characteristics of CMOS Adders |title=High-Performance Energy-Efficient Microprocessor Design |date=2006 |editor-last1=Oklobdzija |editor-first1=Vojin G. |editor-first2=Ram K. |editor-last2=Krishnamurth |author-last=Zeydel |author-first=Bart R. |pages=147-169<!-- TBC --> |series=Series on Integrated Circuits and Systems |publisher=[[Springer-Verlag|Springer]] |location=Dordrecht, Netherlands |isbn=0-387-28594-6<!-- book cover shows invalid ISBN-10: 0-397-28594-6 and no ISBN-13 --> |id={{ISBN|978-0-387-28594-8}} |doi=10.1007/978-0-387-34047-0_6 |url=https://books.google.com/books?id=LmfHof1p3qUC&dq=9780387285948}}\n\n{{DEFAULTSORT:Kogge-Stone adder}}\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Ling adder",
      "url": "https://en.wikipedia.org/wiki/Ling_adder",
      "text": "In electronics, an [[adder (electronics)|adder]] is a combinatorial or sequential logic element which computes the n-bit sum of two numbers. The family of '''Ling adders''' is a particularly fast adder and is designed using H. Ling's equations and generally implemented in [[BiCMOS]].  Samuel Naffziger of [[Hewlett Packard]] presented an innovative 64 bit adder in 0.5&nbsp;µm [[CMOS]] based on Ling's equations at [[ISSCC]] 1996.  The Naffziger adder's delay was less than 1 [[nanosecond]], or 7 [[FO4]].  See Naffzinger's paper below for more details.\n\n==External links==\n# H. Ling, \"[https://web.archive.org/web/20060410222046/http://lap.epfl.ch/courses/comparith/Papers/Ling_adder-1966.pdf High Speed Binary Parallel Adder]\", IEEE Transactions on Electronic Computers, EC-15, p.&nbsp;799-809, October, 1966.\n# H. Ling, \"[http://www.research.ibm.com/journal/rd/252/ibmrd2502a3I.pdf High-Speed Binary Adder]\", IBM J. Res. Dev., vol.25, p.&nbsp;156-66, 1981.\n# R. W. Doran, \"[https://web.archive.org/web/20060410221752/http://lap.epfl.ch/courses/comparith/Papers/11-doran_carry.pdf Variants on an Improved Carry Look-Ahead Adder]\", IEEE Transactions on Computers, Vol.37, No.9, September 1988.\n# N. T. Quach, M. J. Flynn, \"[https://web.archive.org/web/20060410221357/http://lap.epfl.ch/courses/comparith/Papers/Quach-Adder.pdf High-Speed Addition in CMOS]\", IEEE Transactions on Computers, Vol.41, No.12, December, 1992.\n# S. Naffziger, \"[https://web.archive.org/web/20060410222424/http://lap.epfl.ch/courses/comparith/Papers/9-Nafziger-Ling-Adder.pdf A Sub-Nanosecond 0.5um 64b Adder Design]\", Digest of Technical Papers, 1996 IEEE International Solid-State Circuits Conference,  San Francisco, 8-10 Feb. 1996, p.&nbsp;362 –363.\n# S. Naffziger, \"[https://web.archive.org/web/20060410222356/http://lap.epfl.ch/courses/comparith/Papers/US5719803-Naffziger.pdf High Speed Addition Using Ling's Equations and Dynamic CMOS Logic]\", U.S. Patent No. 5,719,803, Issued: February 17, 1998.\n\n[[Category:Adders (electronics)]]\n\n\n{{electronics-stub}}"
    },
    {
      "title": "Lookahead carry unit",
      "url": "https://en.wikipedia.org/wiki/Lookahead_carry_unit",
      "text": "A '''lookahead carry unit''' ('''LCU''') is a logical unit in [[digital circuit]] design used to decrease calculation time in [[adder (electronics)|adder]] units and used in conjunction with [[carry look-ahead adder]]s (CLAs).\n\n==4-bit adder==\nA single 4-bit CLA is shown below:\n[[File:4-bit carry lookahead adder.svg|none|framed|4-bit adder with Carry Look Ahead (CLA)]]\n\n==16-bit adder==\nBy combining four 4-bit CLAs, a 16-bit adder can be created but additional logic is needed in the form of an LCU.\n\nThe LCU accepts the group propagate (<math>P_G</math>) and group generate (<math>G_G</math>) from each of the four CLAs. <math>P_G</math> and <math>G_G</math> have the following expressions for each CLA adder:<ref>{{cite web |url=http://www.seas.upenn.edu/~ese171/lab/CarryLookAhead/CarryLookAheadF01.html |title=Archived copy |accessdate=2011-10-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20110925185302/http://www.seas.upenn.edu/~ese171/lab/CarryLookAhead/CarryLookAheadF01.html |archivedate=2011-09-25 |df= }}</ref>\n\n:<math>P_G = P_3 \\cdot P_2 \\cdot P_1 \\cdot P_0</math>\n:<math>G_G = G_3 + P_3 \\cdot G_2 + P_3 \\cdot P_2 \\cdot G_1 + P_3 \\cdot P_2 \\cdot P_1 \\cdot G_0</math>\n\nThe LCU then generates the carry input for each CLA.\n\nAssume that <math>P_i</math> is <math>P_G</math> and <math>G_i</math> is <math>G_G</math> from the i<sup>th</sup> CLA then the output carry bits are\n\n:<math>C_{4} = G_0 + P_0 \\cdot C_0</math>\n:<math>C_{8} = G_{4} + P_{4} \\cdot C_{4}</math>\n:<math>C_{12} = G_{8} + P_{8} \\cdot C_{8}</math>\n:<math>C_{16} = G_{12} + P_{12} \\cdot C_{12}</math>\n\nSubstituting <math>C_{4}</math> into <math>C_{8}</math>, then <math>C_{8}</math> into <math>C_{12}</math>, then <math>C_{12}</math> into <math>C_{16}</math> yields the expanded equations:\n\n:<math>C_{4} = G_0 + P_0 \\cdot C_0</math>\n:<math>C_{8} = G_4 + G_0 \\cdot P_4 + C_0 \\cdot P_0 \\cdot P_4</math>\n:<math>C_{12} = G_8 + G_4 \\cdot P_8 + G_0 \\cdot P_4 \\cdot P_8 + C_0 \\cdot P_0 \\cdot P_4 \\cdot P_8</math>\n:<math>C_{16} = G_{12} + G_8 \\cdot P_{12} + G_4 \\cdot P_8 \\cdot P_{12} + G_0 \\cdot P_4 \\cdot P_8 \\cdot P_{12} + C_0 \\cdot P_0 \\cdot P_4 \\cdot P_8 \\cdot P_{12}</math>\n\n<math>C_{4}</math> corresponds to the carry input into the second CLA; <math>C_{8}</math> to the third CLA; <math>C_{12}</math> to the fourth CLA; and <math>C_{16}</math> to overflow carry bit.\n\nIn addition, the LCU can calculate its own propagate and generate:\n:<math>P_{LCU} = P_0 \\cdot P_4 \\cdot P_8 \\cdot P_{12}</math>\n:<math>G_{LCU} =  G_{12} + G_8 \\cdot P_{12} + G_4 \\cdot P_8 \\cdot P_{12} + G_0 \\cdot P_4 \\cdot P_8 \\cdot P_{12}</math>\n:<math>C_{16} = G_{LCU} + C_0 \\cdot P_{LCU}</math>\n\n[[File:16-bit lookahead carry unit.svg|none|frame|16-bit adder with LCU]]\n\n==64-bit adder==\nBy combining 4 CLAs and an LCU together creates a 16-bit adder.\nFour of these units can be combined to form a 64-bit adder.\nAn additional (second-level) LCU is needed that accepts the propagate (<math>P_{LCU}</math>) and generate (<math>G_{LCU}</math>) from each LCU and the four carry outputs generated by the second-level LCU are fed into the first-level LCUs.\n\n[[File:64-bit lookahead carry unit.svg|none|frame|64-bit adders with a second-level LCU]]\n\n==References==\n{{Reflist}}\n* {{cite book |last=Katz |first=Randy |authorlink=Randy Katz |title=Contemporary Logic Design |publisher=The Benjamin/Cummings Publishing Company |year=1994 |isbn=0-8053-2703-7 |pages=249–256}}\n* {{cite book |last=Vahid | first=Frank |title=Digital Design |publisher=John Wiley and Sons Publishers |year=2006 |isbn=0-470-04437-3 |pages=296–316}}\n\n[[Category:Digital circuits]]\n[[Category:Adders (electronics)]]\n\n[[de:Paralleladdierer mit Übertragsvorausberechnung]]"
    },
    {
      "title": "Sklansky adder",
      "url": "https://en.wikipedia.org/wiki/Sklansky_adder",
      "text": "#redirect [[Carry-select adder#Sklansky adder]] {{R to related topic}} {{R with possibilities}}\n\n[[Category:Adders (electronics)]]"
    },
    {
      "title": "Bilinear map",
      "url": "https://en.wikipedia.org/wiki/Bilinear_map",
      "text": "In [[mathematics]], a '''bilinear map''' is a [[function (mathematics)|function]] combining elements of two [[vector space]]s to yield an element of a third vector space, and is [[Linear map|linear]] in each of its arguments. [[Matrix multiplication]] is an example.\n\n== Definition ==\n\n=== Vector spaces ===\nLet <math>V, W \\text{ and } X</math> be three [[vector space]]s over the same base [[field (mathematics)|field]] <math>\\mathbb{F}</math>. A bilinear map is a [[function (mathematics)|function]]\n:<math> B : V \\times W \\mapsto X</math>\nsuch that <math> \\forall w \\in W </math>, the map <math>B_w</math>\n:<math>v \\mapsto B(v,w)</math>\nis a [[linear map]] from <math>V</math> to <math>X</math>, and <math>\\forall v \\in V</math>, the map <math>B_v</math>\n:<math>w \\mapsto B(v,w)</math>\nis a linear map from ''W'' to ''X''.\n\nIn other words, when we hold the first entry of the bilinear map fixed while letting the second entry vary, the result is a linear operator, and similarly for when we hold the second entry fixed. \n\nIf {{nowrap|1=''V'' = ''W''}} and we have {{nowrap|1=''B''(''v'', ''w'') = ''B''(''w'', ''v'')}} for all ''v'', ''w'' in ''V'', then we say that ''B'' is ''[[symmetric function|symmetric]]''.\n\nThe case where ''X'' is the base field ''F'', and we have a '''[[bilinear form]]''', is particularly useful (see for example [[scalar product]], [[inner product]] and [[quadratic form]]).\n\n=== Modules ===\nThe definition works without any changes if instead of vector spaces over a field ''F'', we use [[module (mathematics)|module]]s over a [[commutative ring]] ''R''. It generalizes to ''n''-ary functions, where the proper term is ''[[Multilinear map|multilinear]]''.\n\nFor non-commutative rings ''R'' and ''S'', a left ''R''-module ''M'' and a right ''S''-module ''N'', a bilinear map is a map {{nowrap|''B'' : ''M'' × ''N'' → ''T''}} with ''T'' an {{nowrap|(''R'', ''S'')}}-[[bimodule]], and for which any ''n'' in ''N'', {{nowrap|''m'' ↦ ''B''(''m'', ''n'')}} is an ''R''-module homomorphism, and for any ''m'' in ''M'', {{nowrap|''n'' ↦ ''B''(''m'', ''n'')}} is an ''S''-module homomorphism.  This satisfies\n\n:''B''(''r'' ⋅ ''m'', ''n'') = ''r'' ⋅ ''B''(''m'', ''n'')\n:''B''(''m'', ''n'' ⋅ ''s'') = ''B''(''m'', ''n'') ⋅ ''s''\n\nfor all ''m'' in ''M'', ''n'' in ''N'', ''r'' in ''R'' and ''s'' in ''S'', as well as ''B'' being [[Additive map|additive]] in each argument.\n\n==Properties==\nA first immediate consequence of the definition is that {{nowrap|1=''B''(''v'', ''w'') = 0<sub>''X''</sub>}} whenever {{nowrap|1=''v'' = 0<sub>''V''</sub>}} or {{nowrap|1=''w'' = 0<sub>''W''</sub>}}. This may be seen by writing the [[zero vector]] 0<sub>''X''</sub> as {{nowrap|0 ⋅ 0<sub>''X''</sub>}} (and similarly for 0<sub>''W''</sub>) and moving the scalar 0 \"outside\", in front of ''B'', by linearity.\n\nThe set {{nowrap|''L''(''V'', ''W''; ''X'')}} of all bilinear maps is a [[linear subspace]] of the space ([[viz.]] [[vector space]], [[module (mathematics)|module]]) of all maps from {{nowrap|''V'' × ''W''}} into ''X''.\n\n[[File:FourfacesofBilinaearmaps.PNG|thumb|320px|A matrix ''M'' determines a bilinear map into the reals by means of a real bilinear form {{nowrap|(''v'', ''w'') ↦ ''v''′''Mw''}}, then [[metric tensor|associates]] of this are taken to the other three possibilities using [[dual space|duality]] and the [[musical isomorphism]]]]\n\nIf ''V'', ''W'', ''X'' are [[finite-dimensional]], then so is {{nowrap|''L''(''V'', ''W''; ''X'')}}. For {{nowrap|1=''X'' = ''F''}}, i.e. bilinear forms, the dimension of this space is {{nowrap|dim ''V'' × dim ''W''}} (while the space {{nowrap|''L''(''V'' × ''W''; ''F'')}} of ''linear'' forms is of dimension {{nowrap|dim ''V'' + dim ''W''}}). To see this, choose a [[Basis (linear algebra)|basis]] for ''V'' and ''W''; then each bilinear map can be uniquely represented by the matrix {{nowrap|''B''(''e''<sub>''i''</sub>, ''f''<sub>''j''</sub>)}}, and vice versa. \nNow, if ''X'' is a space of higher dimension, we obviously have {{nowrap|1=dim ''L''(''V'', ''W''; ''X'') = dim ''V'' × dim ''W'' × dim ''X''}}.\n\n== Examples ==\n* [[matrix (mathematics)|Matrix multiplication]] is a bilinear map {{nowrap|M(''m'', ''n'') × M(''n'', ''p'') → M(''m'', ''p'')}}.\n* If a [[vector space]] ''V'' over the [[real number]]s '''R''' carries an [[inner product space|inner product]], then the inner product is a bilinear map {{nowrap|''V'' × ''V'' → '''R'''}}.\n* In general, for a vector space ''V'' over a field ''F'', a [[bilinear form]] on ''V'' is the same as a bilinear map {{nowrap|''V'' × ''V'' → ''F''}}.\n* If ''V'' is a vector space with [[dual space]] ''V''<sup>∗</sup>, then the application operator, {{nowrap|1=''b''(''f'', ''v'') = ''f''(''v'')}} is a bilinear map from {{nowrap|''V''<sup>∗</sup> × ''V''}} to the base field.\n* Let ''V'' and ''W'' be vector spaces over the same base field ''F''. If ''f'' is a member of ''V''<sup>∗</sup> and ''g'' a member of ''W''<sup>∗</sup>, then {{nowrap|1=''b''(''v'', ''w'') = ''f''(''v'')''g''(''w'')}} defines a bilinear map {{nowrap|''V'' × ''W'' → ''F''}}.\n* The [[cross product]] in '''R'''<sup>3</sup> is a bilinear map {{nowrap|'''R'''<sup>3</sup> × '''R'''<sup>3</sup> → '''R'''<sup>3</sup>}}.\n* Let {{nowrap|''B'' : ''V'' × ''W'' → ''X''}} be a bilinear map, and {{nowrap|''L'' : ''U'' → ''W''}} be a [[linear map]], then {{nowrap|(''v'', ''u'') ↦ ''B''(''v'', ''Lu'')}} is a bilinear map on {{nowrap|''V'' × ''U''}}.\n\n==See also==\n* [[Tensor product]]\n* [[Sesquilinear form]]\n* [[Bilinear filtering]]\n* [[Multilinear map]]\n* [[Multilinear subspace learning]]\n\n==External links==\n* {{springer|title=Bilinear mapping|id=p/b016280}}\n\n{{Functional Analysis}}\n\n{{DEFAULTSORT:Bilinear Map}}\n[[Category:Bilinear operators]]\n[[Category:Multilinear algebra]]"
    },
    {
      "title": "Cross-correlation",
      "url": "https://en.wikipedia.org/wiki/Cross-correlation",
      "text": "{{Correlation and covariance}}\n[[File:Comparison convolution correlation.svg|thumb|400px|Visual comparison of [[convolution]], cross-correlation and [[autocorrelation]].  For the operations involving function {{mvar|f}}, and assuming the height of {{mvar|f}} is 1.0, the value of the result at 5 different points is indicated by the shaded area below each point.  Also, the vertical symmetry of {{mvar|f}} is the reason <math>f*g</math> and <math>f \\star g</math> are identical in this example.]]\n\nIn [[signal processing]], '''cross-correlation''' is a [[Similarity measure|measure of similarity]] of two series as a function of the displacement of one relative to the other. This is also known as a ''sliding [[dot product]]'' or ''sliding inner-product''. It is commonly used for searching a long signal for a shorter, known feature. It has applications in [[pattern recognition]], [[single particle analysis]], [[electron tomography]], [[averaging]], [[cryptanalysis]], and [[neurophysiology]].\n\nThe cross-correlation is similar in nature to the [[convolution]] of two functions.  In an [[autocorrelation]], which is the cross-correlation of a signal with itself, there will always be a peak at a lag of zero, and its size will be the signal energy.\n\nIn [[probability]] and [[statistics]], the term '''cross-correlations''' is used for referring to the [[covariance and correlation|correlations]] between the entries of two [[Multivariate random variable|random vectors]] <math>\\mathbf{X}</math> and <math>\\mathbf{Y}</math>, while the ''correlations'' of a random vector <math>\\mathbf{X}</math> are considered to be the [[covariance and correlation|correlations]] between the entries of <math>\\mathbf{X}</math> itself, those forming the [[correlation matrix]] (matrix of correlations) of <math>\\mathbf{X}</math>. If each of <math>\\mathbf{X}</math> and <math>\\mathbf{Y}</math> is a scalar random variable which is realized repeatedly in temporal sequence (a [[time series]]), then the correlations of the various temporal instances of <math>\\mathbf{X}</math> are known as ''autocorrelations'' of <math>\\mathbf{X}</math>, and the cross-correlations of <math>\\mathbf{X}</math> with <math>\\mathbf{Y}</math> across time are temporal cross-correlations.\n\nFurthermore, in probability and statistics the definition of ''correlation'' always includes a standardising factor in such a way that correlations have values between −1 and +1.\n\nIf <math>X</math> and <math>Y</math> are two [[independent (probability)|independent]] [[random variable]]s with [[probability density function]]s <math>f</math> and <math>g</math>, respectively, then the probability density of the difference <math>Y - X</math> is formally given by the cross-correlation (in the signal-processing sense) <math>f \\star g</math>; however this terminology is not used in probability and statistics. In contrast, the [[convolution]] <math>f * g</math> (equivalent to the cross-correlation of <math>f(t)</math> and <math>g(-t)</math>) gives the probability density function of the sum <math>X + Y</math>.\n\n==Cross-correlation of deterministic signals==\nFor continuous functions <math>f</math> and <math>g</math>, the cross-correlation is defined as:<ref>Bracewell, R. \"Pentagram Notation for Cross Correlation.\" The Fourier Transform and Its Applications. New York: McGraw-Hill, pp. 46 and 243, 1965.</ref><ref>\nPapoulis, A. The Fourier Integral and Its Applications. New York: McGraw-Hill, pp.&nbsp;244–245 and 252-253, 1962.</ref><ref>Weisstein, Eric W. \"Cross-Correlation.\" From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/Cross-Correlation.html</ref>\n\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk||<math>(f \\star g)(\\tau)\\ \\triangleq \\int_{-\\infty}^{\\infty} \\overline{f(t)} g(t+\\tau)\\,dt</math>|{{EquationRef|Eq.1}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nwhich is equivalent to\n\n:<math>(f \\star g)(\\tau)\\ \\triangleq \\int_{-\\infty}^{\\infty} \\overline{f(t-\\tau)} g(t)\\,dt</math>\n\nwhere <math>\\overline{f(t)}</math> denotes the [[complex conjugate]] of <math>f(t)</math>, and <math>\\tau</math> is the displacement, also known as ''lag'' (a feature in <math>f</math> at <math>t</math> occurs in <math>g</math> at <math>t+\\tau</math>).\n\nSimilarly, for discrete functions, the cross-correlation is defined as:<ref>{{cite book | last1 =Rabiner | first1 =L.R. | last2 =Schafer | first2 =R.W. | author-link = | title =Digital Processing of Speech Signals | publisher =Prentice Hall | series =Signal Processing Series | date =1978 | location =Upper Saddle River, NJ | pages =147–148 | isbn =0132136031 }}</ref><ref>{{cite book | last1 =Rabiner | first1 =Lawrence R. | last2 =Gold| first2 =Bernard  | title =Theory and Application of Digital Signal Processing | publisher =Prentice-Hall | date =1975 | location =Englewood Cliffs, NJ | pages =401 | isbn =0139141014 }}</ref>\n\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk||<math>(f \\star g)[n]\\ \\triangleq \\sum_{m=-\\infty}^{\\infty} \\overline{f[m]} g[m+n]</math>|{{EquationRef|Eq.2}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nwhich is equivalent to\n\n:<math>(f \\star g)[n]\\ \\triangleq \\sum_{m=-\\infty}^{\\infty} \\overline{f[m-n]} g[m]</math>.\n\n===Explanation===\nAs an example, consider two real valued functions <math>f</math> and <math>g</math> differing only by an unknown shift along the x-axis. One can use the cross-correlation to find how much <math>g</math>  must be shifted along the x-axis to make it identical to <math>f</math>. The formula essentially slides the <math>g</math> function along the x-axis, calculating the integral of their product at each position. When the functions match, the value of <math>(f\\star g)</math> is maximized. This is because when peaks (positive areas) are aligned, they make a large contribution to the integral. Similarly, when troughs (negative areas) align, they also make a positive contribution to the integral because the product of two negative numbers is positive.\n[[File:Cross correlation animation.gif|center|thumb|500x500px|Animation displaying visually how cross correlation is calculated]]\n\nWith [[complex-valued function]]s <math>f</math> and <math>g</math>, taking the [[Complex conjugate|conjugate]] of <math>f</math> ensures that aligned peaks (or aligned troughs) with imaginary components will contribute positively to the integral.\n\nIn [[econometrics]], lagged cross-correlation is sometimes referred to as cross-autocorrelation.<ref>{{cite book |last=Campbell |last2=Lo |last3=MacKinlay |year=1996 |title=The Econometrics of Financial Markets |location=NJ |publisher=Princeton University Press |isbn=0691043019 }}</ref>{{rp|p. 74}}\n\n===Properties===\n* The cross-correlation of functions <math>f(t)</math> and <math>g(t)</math> is equivalent to the [[convolution]] (denoted by <math>*</math>) of  <math>\\overline{f(-t)}</math> and <math>g(t)</math>. That is:\n*: <math>[f(t) \\star g(t)](t) = [\\overline{f(-t)} * g(t)](t).</math>\n* <math>[f(t) \\star g(t)](t) = [\\overline{g(t)} \\star \\overline{f(t)}](-t).</math>\n* If <math>f</math> is a [[Hermitian function]], then <math>f \\star g = f * g.</math>\n* If both <math>f</math> and <math>g</math> are Hermitian, then <math>f \\star g = g \\star f</math>.\n* <math>\\left(f \\star g\\right) \\star \\left(f \\star g\\right) = \\left(f \\star f\\right) \\star \\left(g \\star g\\right)</math>.\n* Analogous to the [[convolution theorem]], the cross-correlation satisfies\n*: <math>\\mathcal{F}\\left\\{f \\star g\\right\\} = \\overline{\\mathcal{F} \\left\\{f\\right\\}} \\cdot \\mathcal{F}\\left\\{g\\right\\},</math>\n:where <math>\\mathcal{F}</math> denotes the [[Fourier transform]], and an <math>\\overline{f}</math> again indicates the complex conjugate of <math>f</math>, since <math>\\mathcal{F}\\left\\{\\overline{f(-t)}\\right\\}=\\overline{\\mathcal{F}\\left\\{f(t)\\right\\}}</math>.  Coupled with [[fast Fourier transform]] algorithms, this property is often exploited for the efficient numerical computation of cross-correlations <ref name=\"KAP\">{{cite journal|last1=Kapinchev|last2=Bradu|last3=Barnes|last4=Podoleanu|year=2015|title=GPU Implementation of Cross-Correlation for Image Generation in Real Time|journal=ICSPCS 2015|doi=10.1109/ICSPCS.2015.7391783|url=http://ieeexplore.ieee.org/document/7391783/}}</ref> (see [[Discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular cross-correlation]]).\n* The cross-correlation is related to the [[spectral density]] (see [[Wiener–Khinchin theorem]]).\n* The cross-correlation of a convolution of <math>f</math> and <math>h</math> with a function <math>g</math> is the convolution of the cross-correlation of <math>g</math> and <math>f</math> with the kernel <math>h</math>:\n*: <math>g \\star \\left(f * h\\right) = \\left(g \\star f\\right) * h</math>.\n\n===Definition for periodic signals===\nIf <math>f</math> and <math>g</math> are both continuous periodic functions of period <math>T</math>, the integration from <math>-\\infty</math> to <math>\\infty</math> is replaced by integration over any interval <math>[t_0,t_0+T]</math> of length <math>T</math>:\n\n:<math>(f \\star g)(\\tau)\\ \\triangleq \\int_{t_0}^{t_0+T} \\overline{f(t)} g(t+\\tau)\\,dt</math>\n\nwhich is equivalent to\n\n:<math>(f \\star g)(\\tau)\\ \\triangleq \\int_{t_0}^{t_0+T} \\overline{f(t-\\tau)} g(t)\\,dt</math>\n\n==Cross-correlation of random vectors==\n{{main|Cross-correlation matrix}}\n\n===Definition===\nFor [[random vector]]s <math>\\mathbf{X} = (X_1,\\ldots,X_m)^{\\rm T}</math> and <math>\\mathbf{Y} = (Y_1,\\ldots,Y_n)^{\\rm T}</math>, each containing [[random element]]s whose [[expected value]] and [[variance]] exist, the '''cross-correlation matrix''' of <math>\\mathbf{X}</math> and <math>\\mathbf{Y}</math> is defined by<ref name=Gubner>{{cite book |first=John A. |last=Gubner |year=2006 |title=Probability and Random Processes for Electrical and Computer Engineers |publisher=Cambridge University Press |isbn=978-0-521-86470-1}}</ref>{{rp|p.337}}\n\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk||<math>\\operatorname{R}_{\\mathbf{X}\\mathbf{Y}} \\triangleq\\ \\operatorname{E}[\\mathbf{X} \\mathbf{Y}^{\\rm T}]</math>|{{EquationRef|Eq.3}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nand has dimensions <math>m \\times n</math>. Written component-wise:\n\n:<math>\\operatorname{R}_{\\mathbf{X}\\mathbf{Y}} =\n\\begin{bmatrix}\n\\operatorname{E}[X_1 Y_1] & \\operatorname{E}[X_1 Y_2] & \\cdots & \\operatorname{E}[X_1 Y_n] \\\\ \\\\\n\\operatorname{E}[X_2 Y_1] & \\operatorname{E}[X_2 Y_2] & \\cdots & \\operatorname{E}[X_2 Y_n] \\\\ \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n\\operatorname{E}[X_m Y_1] & \\operatorname{E}[X_m Y_2] & \\cdots & \\operatorname{E}[X_m Y_n] \\\\ \\\\\n\\end{bmatrix}\n</math>\n\nThe random vectors <math>\\mathbf{X}</math> and <math>\\mathbf{Y}</math> need not have the same dimension, and either might be a scalar value.\n\n===Example===\nFor example, if <math>\\mathbf{X} = \\left( X_1,X_2,X_3 \\right)^{\\rm T}</math> and <math>\\mathbf{Y} = \\left( Y_1,Y_2 \\right)^{\\rm T}</math> are random vectors, then\n<math>\\operatorname{R}_{\\mathbf{X}\\mathbf{Y}}</math> is a <math>3 \\times 2</math> matrix whose <math>(i,j)</math>-th entry is <math>\\operatorname{E}[X_i Y_j]</math>.\n\n===Definition for complex random vectors===\nIf <math>\\mathbf{Z} = (Z_1,\\ldots,Z_m)^{\\rm T}</math> and <math>\\mathbf{W} = (W_1,\\ldots,W_n)^{\\rm T}</math> are [[complex random vector]]s, each containing random variables whose expected value and variance exist, the cross-correlation matrix of <math>\\mathbf{Z}</math> and <math>\\mathbf{W}</math> is defined by\n\n:<math>\\operatorname{R}_{\\mathbf{Z}\\mathbf{W}} \\triangleq\\ \\operatorname{E}[\\mathbf{Z} \\mathbf{W}^{\\rm H}]</math>\n\nwhere <math>{}^{\\rm H}</math> denotes [[Hermitian transpose|Hermitian transposition]].\n\n==Cross-correlation of stochastic processes==\nIn [[time series analysis]] and [[statistics]], the cross-correlation of a pair of [[random processes|random process]] is the correlation between values of the processes at different times, as a function of the two times. Let <math>(X_t, Y_t)</math> be a pair of random processes, and <math>t</math> be any point in time (<math>t</math> may be an [[integer]] for a [[discrete-time]] process or a [[real number]] for a [[continuous-time]] process). Then <math>X_t</math> is the value (or [[Realization (probability)|realization]]) produced by a given run of the process at time <math>t</math>.\n\n=== Cross-correlation function ===\nSuppose that the process has means <math>\\mu_X(t)</math> and <math>\\mu_Y(t)</math> and variances <math>\\sigma_X^2(t)</math> and <math>\\sigma_Y^2(t)</math> at time <math>t</math>, for each <math>t</math>. Then the definition of the cross-correlation between times <math>t_1</math> and <math>t_2</math> is<ref name=Gubner/>{{rp|p.392}}\n\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk||<math>\\operatorname{R}_{XY}(t_1,t_2) = \\operatorname{E}[X_{t_1} \\overline{Y_{t_2}}]</math>|{{EquationRef|Eq.4}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nwhere <math>\\operatorname{E}</math> is the [[expected value]] operator. Note that this expression may be not defined.\n\n=== Cross-covariance function ===\nSubtracting the mean before multiplication yields the cross-covariance between times <math>t_1</math> and <math>t_2</math>:<ref name=Gubner/>{{rp|p.392}}\n\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk||<math>\\operatorname{K}_{XY}(t_1,t_2) = \\operatorname{E}[(X_{t_1} - \\mu_X(t_1))\\overline{(Y_{t_2} - \\mu_Y(t_2))}]</math>|{{EquationRef|Eq.5}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nNote that this expression is not well-defined for all-time series or processes, because the mean may not exist, or the variance may not exist.\n\n===Definition for wide-sense stationary stochastic process===\nLet <math>(X_t, Y_t)</math> represent a pair of [[stochastic process]]es that are [[Stationary process#Joint stationarity|jointly wide-sense stationary]]. Then the [[Cross-covariance#Cross-covariance of stochastic processes|Cross-covariance function]] and the cross-correlation function are given as follows.\n\n====Cross-correlation function====\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk||<math>\\operatorname{R}_{XY}(\\tau) = \\operatorname{E}\\left[X_t \\overline{Y_{t+\\tau}}\\right]</math>|{{EquationRef|Eq.6}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nor equivalently\n\n:<math>\\operatorname{R}_{XY}(\\tau) = \\operatorname{E}\\left[X_{t-\\tau} \\overline{Y_{t}}\\right]</math>\n\n====Cross-covariance function====\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk||<math>\\operatorname{K}_{XY}(\\tau) = \\operatorname{E}\\left[\\left(X_t - \\mu_X\\right)\\overline{\\left(Y_{t+\\tau} - \\mu_Y\\right)}\\right]</math>|{{EquationRef|Eq.7}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nor equivalently\n\n:<math>\\operatorname{K}_{XY}(\\tau) = \\operatorname{E}\\left[\\left(X_{t-\\tau} - \\mu_X\\right)\\overline{\\left(Y_{t} - \\mu_Y\\right)}\\right]</math>\n\nwhere <math>\\mu_X</math> and <math>\\sigma_X</math> are the mean and standard deviation of the process <math>(X_t)</math>, which are constant over time due to stationarity; and similarly for <math>(Y_t)</math>, respectively. <math>\\operatorname{E}[\\ ]</math> indicates the [[expected value]]. That the cross-covariance and cross-correlation are independent of <math>t</math> is precisely the additional information (beyond being individually wide-sense stationary) conveyed by the requirement that <math>(X_t, Y_t)</math> are ''jointly'' wide-sense stationary.\n\nThe cross-correlation of a pair of jointly [[wide sense stationary]] [[stochastic processes]] can be estimated by averaging the product of samples measured from one process and samples measured from the other (and its time shifts). The samples included in the average can be an arbitrary subset of all the samples in the signal (e.g., samples within a finite time window or a [[sampling (statistics)|sub-sampling]]{{which|date=May 2015}} of one of the signals). For a large number of samples, the average converges to the true cross-correlation.\n\n=== Normalization ===\nIt is common practice in some disciplines (e.g. statistics and [[time series analysis]]) to normalize to cross-correlation function to get a time-dependent [[Pearson correlation coefficient]]. However, in other disciplines (e.g. engineering) the normalization is usually dropped and the terms \"cross-correlation\" and \"cross-covariance\" are used interchangeably.\n\nThe definition of the normalized cross-correlation of a stochastic process is\n\n:<math>\\rho_{XX}(t_1,t_2) = \\frac{\\operatorname{K}_{XX}(t_1,t_2)}{\\sigma_X(t_1)\\sigma_X(t_2)} = \\frac{\\operatorname{E}[(X_{t_1} - \\mu_{t_1})\\overline{(X_{t_2} - \\mu_{t_2})}]}{\\sigma_X(t_1)\\sigma_X(t_2)}</math>.\n\nIf the function <math>\\rho_{XX}</math> is well-defined, its value must lie in the range <math>[-1,1]</math>, with 1 indicating perfect correlation and −1 indicating perfect [[anti-correlation]].\n\nFor jointly wide-sense stationary stochastic processes, the definition is\n\n:<math>\\rho_{XY}(\\tau) = \\frac{\\operatorname{K}_{XY}(\\tau)}{\\sigma_X \\sigma_Y} = \\frac{\\operatorname{E}[ \\left(X_t - \\mu_X\\right) \\overline{\\left(Y_{t+\\tau} - \\mu_Y\\right)}]}{\\sigma_X \\sigma_Y}</math>.\n\nThe normalization is important both because the interpretation of the autocorrelation as a correlation provides a scale-free measure of the strength of [[statistical dependence]], and because the normalization has an effect on the statistical properties of the estimated autocorrelations.\n\n===Properties===\n====Symmetry property====\nFor jointly wide-sense stationary stochastic processes, the cross-correlation function has the following symmetry property:<ref name=KunIlPark>Kun Il Park, Fundamentals of Probability and Stochastic Processes with Applications to Communications, Springer, 2018, 978-3-319-68074-3</ref>{{rp|p.173}}\n:<math>\\operatorname{R}_{XY}(t_1,t_2) = \\overline{\\operatorname{R}_{YX}(t_2,t_1)}</math>\nRespectively for jointly WSS processes:\n:<math>\\operatorname{R}_{XY}(\\tau) = \\overline{\\operatorname{R}_{YX}(-\\tau)}</math>\n\n==Time delay analysis==\n'''Cross-correlations''' are useful for determining the time delay between two signals, e.g., for determining time delays for the propagation of acoustic signals across a microphone array.<ref>{{cite journal|last=Rhudy|first=Matthew|author2=Brian Bucci |author3=Jeffrey Vipperman |author4=Jeffrey Allanach |author5=Bruce Abraham |title=Microphone Array Analysis Methods Using Cross-Correlations|journal=Proceedings of 2009 ASME International Mechanical Engineering Congress, Lake Buena Vista, FL|date=November 2009 |doi=10.1115/IMECE2009-10798}}</ref><ref>{{cite journal |last=Rhudy|first=Matthew|title=Real Time Implementation of a Military Impulse Classifier|publisher=University of Pittsburgh, Master's Thesis|date=November 2009|url=http://d-scholarship.pitt.edu/9773/}}</ref>{{clarify|reason=I doubt that this definition is used for microphone arrays, since it involves an integral over all time. Perhaps integration over a time-window?|date=May 2015}}  After calculating the '''cross-correlation''' between the two signals, the maximum (or minimum if the signals are negatively correlated) of the cross-correlation function indicates the point in time where the signals are best aligned; i.e., the time delay between the two signals is determined by the argument of the maximum, or [[arg max]] of the '''cross-correlation''', as in\n\n: <math>\\tau_\\mathrm{delay}=\\underset{t \\in \\mathbb{R}}{\\operatorname{arg\\,max}}((f \\star g)(t))</math>\n\n==Terminology in image processing==\n===Zero-normalized cross-correlation (ZNCC)===\nFor image-processing applications in which the brightness of the image and template can vary due to lighting and exposure conditions, the images can be first normalized. This is typically done at every step by subtracting the mean and dividing by the [[standard deviation]]. That is, the cross-correlation of a template, <math>t(x,y)</math> with a subimage <math>f(x,y)</math> is\n: <math>\\frac{1}{n} \\sum_{x,y}\\frac{1}{\\sigma_f \\sigma_t}\\left(f(x,y) - \\mu_f \\right)\\left(t(x,y) - \\mu_t \\right)</math>.\n\nwhere <math>n</math> is the number of pixels in <math>t(x,y)</math> and <math>f(x,y)</math>,\n<math>\\mu_f</math> is the average of <math>f</math> and <math>\\sigma_f</math> is [[standard deviation]] of <math>f</math>.\n\nIn [[functional analysis]] terms, this can be thought of as the dot product of two [[Unit vector|normalized vectors]]. That is, if\n: <math>F(x,y) = f(x,y) - \\mu_f</math>\nand\n: <math>T(x,y) = t(x,y) - \\mu_t</math>\nthen the above sum is equal to\n: <math>\\left\\langle\\frac{F}{\\|F\\|},\\frac{T}{\\|T\\|}\\right\\rangle</math>\nwhere <math>\\langle\\cdot,\\cdot\\rangle</math> is the [[inner product]] and <math>\\|\\cdot\\|</math> is the [[Lp space|''L''² norm]].\n\nThus, if <math>f</math> and <math>t</math> are real matrices, their normalized cross-correlation equals the cosine of the angle between the unit vectors <math>F</math> and <math>T</math>, being thus <math>1</math> if and only if <math>F</math> equals <math>T</math> multiplied by a positive scalar.\n\nNormalized correlation is one of the methods used for [[template matching]], a process used for finding incidences of a pattern or object within an image. It is also the 2-dimensional version of [[Pearson product-moment correlation coefficient]].\n\n===Normalized cross-correlation (NCC)===\nNCC is similar to ZNCC with the only difference of not subtracting the local mean value of intensities:\n: <math>\\frac{1}{n} \\sum_{x,y}\\frac{1}{\\sigma_f \\sigma_t} f(x,y) t(x,y)</math>\n\n==Nonlinear systems==\nCaution must be applied when using cross correlation for nonlinear systems. In certain circumstances, which depend on the properties of the input, cross correlation between the input and output of a system with nonlinear dynamics can be completely blind to certain nonlinear effects.<ref name=\"SAB1\">{{cite book |last=Billings |first=S. A. |title=Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains |location= |publisher=Wiley |year=2013 |isbn=978-1-118-53556-1 }}</ref> This problem arises because some quadratic moments can equal zero and this can incorrectly suggest that there is little \"correlation\" (in the sense of statistical dependence) between two signals, when in fact the two signals are strongly related by nonlinear dynamics.\n\n==See also==\n{{Div col|colwidth=25em}}\n* [[Autocorrelation]]\n* [[Autocovariance]]\n* [[Coherence (signal processing)|Coherence]]\n* [[Convolution]]\n* [[Correlation]]\n* [[Correlation function]]\n* [[Cross-correlation matrix]]\n* [[Cross-covariance]]\n* [[Cross-spectrum]]\n* [[Digital image correlation]]\n* [[Phase correlation]]\n* [[Scaled correlation]]\n* [[Spectral density]]\n* [[Wiener–Khinchin theorem]]\n{{div col end}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n*{{cite journal |last=Tahmasebi |first=Pejman |last2=Hezarkhani |first2=Ardeshir |last3=Sahimi |first3=Muhammad |year=2012 |title=Multiple-point geostatistical modeling based on the cross-correlation functions |journal=Computational Geosciences |volume=16 |issue=3 |pages=779–797 |doi=10.1007/s10596-012-9287-1 }}\n\n==External links==\n* [http://mathworld.wolfram.com/Cross-Correlation.html Cross Correlation from Mathworld]\n* http://scribblethink.org/Work/nvisionInterface/nip.html\n* http://www.phys.ufl.edu/LIGO/stochastic/sign05.pdf\n* http://www.staff.ncl.ac.uk/oliver.hinton/eee305/Chapter6.pdf\n\n{{Statistics|analysis}}\n\n[[Category:Bilinear operators]]\n[[Category:Covariance and correlation]]\n[[Category:Signal processing]]\n[[Category:Time domain analysis]]"
    },
    {
      "title": "Paraproduct",
      "url": "https://en.wikipedia.org/wiki/Paraproduct",
      "text": "{{Orphan|date=February 2013}}\n\nIn [[mathematics]], a '''paraproduct''' is a [[non-commutative]] [[bilinear operator]] acting on [[function (mathematics)|functions]] that in some sense is like the [[product (mathematics)|product]] of the two functions it acts on.  According to [[Svante Janson]] and Jaak Peetre, in an article from 1988,<ref>Svante Janson and Jaak Peetre, [https://www.jstor.org/stable/2000875 \"Paracommutators-Boundedness and Schatten-Von Neumann Properties\"], ''Transactions of the American Mathematical Society'', Vol. 305, No. 2 (Feb., 1988), pp. 467–504.</ref> \"the name 'paraproduct' denotes an idea rather than a unique definition; several versions exist and can be used for the same purposes.\" The concept emerged in [[Jean-Michel Bony|J.-M. Bony]]’s theory\nof paradifferential operators<ref>{{cite journal |last1=Bony |first1=J.-M. |title=Calcul symbolique et propagation des singularités pour les équations aux dérivées partielles non linéaires |journal=Ann. Sci. École Norm. Sup. |date=1981 |volume=14 |page=209–246}}</ref>,\n\nThis said, for a given operator <math>\\Lambda</math> to be defined as a paraproduct, it is normally required to satisfy the following properties:\n* It should \"reconstruct the product\" in the sense that for any pair of functions, <math>(f, g)</math> in its domain,\n:: <math>fg = \\Lambda(f, g) + \\Lambda(g, f).</math>\n* For any appropriate functions, <math>f</math> and <math>h</math> with <math>h(0)=0</math>, it is the case that <math>h(f) = \\Lambda(f, h'(f))</math>.\n* It should satisfy some form of the [[Product rule|Leibniz rule]].\n\nA paraproduct may also be required to satisfy some form of [[Hölder's inequality]].\n\n==Notes==\n{{Reflist}}\n\n==Further references==\n* Árpád Bényi, Diego Maldonado, and Virginia Naibo, [http://www.ams.org/notices/201007/rtx100700858p.pdf \"What is a Paraproduct?\"], ''Notices of the American Mathematical Society'', Vol. 57, No. 7 (Aug., 2010), pp.&nbsp;858–860.\n\n[[Category:Bilinear operators]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Aluthge transform",
      "url": "https://en.wikipedia.org/wiki/Aluthge_transform",
      "text": "In mathematics and more precisely in [[functional analysis]], the '''Aluthge transformation''' is an operation defined on the set of [[bounded operators]] of a [[Hilbert space]]. It was introduced by [[Ariyadasa Aluthge]] to study [[P-hyponormal operators|p-hyponormal]] [[linear operator]]s.<ref>{{Cite journal|last=Aluthge|first=Ariyadasa|date=1990|title=On p-hyponormal operators for 0 < ''p'' < 1|url=|journal=Integral Equations Operator Theory|volume=13|pages=307–315|via=}}</ref>\n\n== Definition ==\nLet  <math>H</math> be a [[Hilbert space]] and let  <math>B(H)</math> be the algebra of linear operators from <math>H</math> to <math>H</math>. By the [[polar decomposition]] theorem, there exists an unique [[partial isometry]]  <math>U</math> such that  <math>T=U|T|</math> and  <math>\\ker(U)\\supset\\ker(T)</math>, where  <math>|T|</math> is the [[Square root of a matrix|square root of the operator]]  <math> T^*T</math>.  If  <math>T\\in B(H)</math> and <math> T=U|T|</math> is its polar decomposition, the Aluthge transform of  <math>T</math> is the operator <math>\\Delta(T)</math> defined as:\n: <math>\\Delta(T)=|T|^{\\frac12}U|T|^{\\frac12}.</math>\n\nMore generally, for any real number <math>\\lambda\\in [0,1]</math>, the <math>\\lambda</math>-Aluthge transformation is defined as \n: <math>\\Delta_\\lambda(T):=|T|^{\\lambda}U|T|^{1-\\lambda}\\in B(H).</math>\n\n== Example ==\nFor vectors <math>x,y \\in H</math>, let  <math>x\\otimes y</math> denote the operator defined as \n: <math>\\forall z\\in H\\quad x\\otimes y(z)=\\langle z,y\\rangle x.</math>\n\nAn elementary calculation<ref>{{cite journal |last1=Chabbabi |first1=Fadil |last2=Mbekhta |first2=Mostafa |title=Jordan product maps commuting with the λ-Aluthge transform |journal=Journal of Mathematical Analysis and Applications |date=June 2017 |volume=450 |issue=1 |pages=293–313}}</ref> shows that if <math>y\\ne0</math>, then <math>\\Delta_\\lambda(x\\otimes y)=\\Delta(x\\otimes y)=\\frac{\\langle x,y\\rangle}{\\lVert y \\rVert^2} y\\otimes y.</math>\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n\n* {{Cite journal|last=Antezana|first=Jorge|last2=Pujals|first2=Enrique R.|last3=Stojanoff|first3=Demetrio|date=2008|title=Iterated Aluthge transforms: a brief survey|url=http://www.scielo.org.ar/scielo.php?script=sci_arttext&pid=S0041-69322008000100004|journal=Revista de la Unión Matemática Argentina|volume=49|pages=29–41|via=}}\n\n== External links ==\n\n* {{MathGenealogy|id=59270|59270|title=Ariyadasa Aluthge|Ariyadasa Aluthge}}\n\n[[Category:Bilinear forms]]\n[[Category:Matrices]]\n[[Category:Topology]]"
    },
    {
      "title": "Degenerate bilinear form",
      "url": "https://en.wikipedia.org/wiki/Degenerate_bilinear_form",
      "text": "{{Other uses|Degeneracy (disambiguation){{!}}Degeneracy}}\n{{Unreferenced|date=August 2008}}\nIn [[mathematics]], specifically [[linear algebra]], a '''degenerate bilinear form''' {{nowrap|''f''(''x'', ''y'')}} on a [[vector space]] ''V'' is a [[bilinear form]] such that the map from ''V'' to ''V''<sup>∗</sup> (the [[dual space]] of ''V'') given by {{nowrap|''v'' ↦ (''x'' ↦ ''f''(''x'', ''v''))}} is not an [[isomorphism]].  An equivalent definition when ''V'' is finite-dimensional is that it has a non-trivial [[kernel (algebra)|kernel]]: there exist some non-zero ''x'' in ''V'' such that\n\n:<math>f(x,y)=0\\,</math> for all <math>y \\in V.</math>\n\n==Nondegenerate forms==\n{{main|Nondegenerate form}}\n\nA '''nondegenerate''' or '''nonsingular''' form is one that is not degenerate, meaning that <math>v \\mapsto (x \\mapsto f(x,v))</math>  is an [[isomorphism]], or equivalently in finite dimensions, if and only if\n:<math>f(x,y)=0\\,</math> for all <math>y \\in V</math> implies that <math>x = 0</math>.\n\n==Using the determinant==\nIf ''V'' is [[finite-dimensional]] then, relative to some [[basis (linear algebra)|basis]] for ''V'', a bilinear form is degenerate if and only if the [[determinant]] of the associated [[matrix (mathematics)|matrix]] is zero – if and only if the matrix is ''singular,'' and accordingly degenerate forms are also called '''singular forms'''. Likewise, a nondegenerate form is one for which the associated matrix is [[non-singular matrix|non-singular]], and accordingly nondegenerate forms are also referred to as '''non-singular forms'''. These statements are independent of the chosen basis.\n\n==Related notions==\nThere is the closely related notion of a [[unimodular form]] and a [[perfect pairing]]; these agree over fields but not over general rings.\n\n==Examples==\nThe most important examples of nondegenerate forms are [[inner product]]s and [[symplectic form]]s. Symmetric nondegenerate forms are important generalizations of inner products, in that often all that is required is that the map <math>V \\to V^*</math> be an isomorphism, not positivity. For example, a manifold with an inner product structure on its tangent spaces is a [[Riemannian manifold]], while relaxing this to a symmetric nondegenerate form yields a [[pseudo-Riemannian manifold]].\n\n==Infinite dimensions==\nNote that in an infinite dimensional space, we can have a bilinear form ƒ for which <math>v \\mapsto (x \\mapsto f(x,v))</math> is [[injective]] but not [[surjective]].  For example, on the space of [[continuous function]]s on a closed bounded interval, the form \n:<math> f(\\phi,\\psi) = \\int\\psi(x)\\phi(x) dx</math> \nis not surjective: for instance, the [[Dirac delta functional]] is in the dual space but not of the required form.  On the other hand, this bilinear form satisfies \n:<math>f(\\phi,\\psi)=0\\,</math> for all <math>\\,\\phi</math> implies that <math>\\psi=0.\\,</math>\nIn such a case where ƒ satisfies injectivity (but not necessarily surjectivity), ƒ is said to be ''weakly nondegenerate''.\nWhereas in finite dimensions every inner product is nondegenerate, in infinite dimensions inner products are (at least) weakly nondegenerate (this can be shown using positive-definiteness of inner products).\n\n==Terminology==\nIf ƒ vanishes identically on all vectors it is said to be ''' totally degenerate'''. Given any bilinear form ƒ on ''V'' the set of vectors\n\n:<math>\\{x\\in V \\mid f(x,y) = 0 \\mbox{ for all } y \\in V\\}</math>\n\nforms a totally degenerate [[linear subspace|subspace]] of ''V''. The map ƒ is nondegenerate [[if and only if]] this subspace is trivial.\n\nSometimes the words ''anisotropic'', ''isotropic'' and ''totally isotropic'' are used for nondegenerate, degenerate and totally degenerate respectively, although definitions of these latter can be a bit ambiguous: a vector <math>x\\in V</math> such that  <math>f(x,x)=0</math> is called isotropic for the [[isotropic quadratic form|quadratic form]] associated with the bilinear form <math>f</math>, but such vectors can arise even if the bilinear form has no nonzero isotropic vectors.\n\nGeometrically, an isotropic line of the ''quadratic'' form corresponds to a point of the associated [[quadric surface|quadric hypersurface]] in [[projective space]]. Such a line is additionally isotropic for the bilinear form if and only if the corresponding point is a [[singular variety|singularity]]. Hence, over an [[algebraically closed field]], [[Hilbert's nullstellensatz]] guarantees that the quadratic form always has isotropic lines, while the bilinear form has them if and only if the surface is singular.\n\n[[Category:Bilinear forms]]\n\n[[pl:Forma dwuliniowa#Własności]]"
    },
    {
      "title": "Inner product space",
      "url": "https://en.wikipedia.org/wiki/Inner_product_space",
      "text": "{{For|the scalar product or dot product of coordinate vectors|dot product}}\n{{See also|Parallelogram law|Polarization identity}}\n[[File:Inner-product-angle.png|thumb|300px|Geometric interpretation of the angle between two vectors defined using an inner product]]\n[[File:Product Spaces Drawing (1).png|alt=Scalar product spaces, inner product spaces, Hermitian product spaces.|thumb|300px|Scalar product spaces, over any field, have \"scalar products\" that are symmetrical and linear in the first argument. Hermitian product spaces are restricted to the field of complex numbers and have \"Hermitian products\" that are conjugate-symmetrical and linear in the first argument. Inner product spaces may be defined over any field, having \"inner products\" that are linear in the first argument, conjugate-symmetrical, and positive-definite. Unlike inner products, scalar products and Hermitian products need not be positive-definite.]]\n\nIn [[linear algebra]], an '''inner product space''' is a [[vector space]] with an additional [[Mathematical structure|structure]] called an '''inner product'''. This additional structure associates each pair of vectors in the space with a [[Scalar (mathematics)|scalar]] quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the [[angle]] between two vectors. They also provide the means of defining [[orthogonality]] between vectors (zero inner product). Inner product spaces generalize [[Euclidean space]]s (in which the inner product is the [[dot product]], also known as the scalar product) to vector spaces of any (possibly infinite) [[dimension (vector space)|dimension]], and are studied in [[functional analysis]]. The first usage of the concept of a vector space with an inner product is due to [[Giuseppe Peano]], in 1898.<ref>{{cite journal|last1=Moore|first1=Gregory H.|title=The axiomatization of linear algebra: 1875-1940|journal=Historia Mathematica|date=1995|volume=22|issue=3|pages=262–303|doi=10.1006/hmat.1995.1025}}</ref>\n\nAn inner product naturally induces an associated [[Norm (mathematics)|norm]], thus an inner product space is also a [[normed vector space]]. A [[complete space]] with an inner product is called a [[Hilbert space]]. An (incomplete) space with an inner product is called a '''pre-Hilbert space''', since its [[Complete space#Completion|completion]] with respect to the norm induced by the inner product is a [[Hilbert space]]. Inner product spaces over the field of complex numbers are sometimes referred to as '''unitary spaces'''.\n\n== Definition ==\nIn this article, the [[field (mathematics)|field]] of [[scalar (mathematics)|scalar]]s denoted {{math|''F''}} is either the field of [[real number]]s {{math|'''R'''}} or the field of [[complex number]]s {{math|'''C'''}}.\n\nFormally, an inner product space is a [[vector space]] {{math|''V''}} [[Algebra over a field|over the field]] {{math|''F''}} together with an ''inner product'', i.e., with a map\n:<math> \\langle \\cdot, \\cdot \\rangle : V \\times V \\to F </math>\n\nthat satisfies the following three properties for all vectors {{math|''x'', ''y'', ''z'' ∈ ''V''}} and all scalars {{math|''a'' ∈ ''F''}}:<ref name= Jain>{{cite book |title=Functional Analysis |first1=P. K. |last1=Jain |first2=Khalil |last2=Ahmad |chapter-url=https://books.google.com/books?id=yZ68h97pnAkC&pg=PA203 |page=203 |chapter=5.1 Definitions and basic properties of inner product spaces and Hilbert spaces |isbn=81-224-0801-X |year=1995 |edition=2nd |publisher=New Age International}}</ref><ref name=\"Prugovec̆ki\">{{cite book |title=Quantum Mechanics in Hilbert Space |first=Eduard |last=Prugovec̆ki |chapter-url=https://books.google.com/books?id=GxmQxn2PF3IC&pg=PA18 |chapter=Definition 2.1 |pages=18ff. |isbn=0-12-566060-X |year=1981 |publisher=Academic Press |edition =2nd}}</ref>\n\n* [[Complex conjugate|Conjugate]] symmetry:<ref group=\"Note\">A bar over an expression denotes complex conjugation; e.g., <math display=\"inline\">\\overline{x}</math> is the complex conjugation of <math display=\"inline\">x</math>. For real values, <math display=\"inline\">x = \\overline{x}</math> and conjugate symmetry is just symmetry.</ref>\n*: <math>\\langle x, y \\rangle = \\overline{\\langle y, x \\rangle}</math>\n* [[Linearity#In mathematics|Linearity]] in the first argument:\n*: <math>\\begin{align}\n     \\langle ax, y \\rangle &= a \\langle x, y \\rangle \\\\\n  \\langle x + y, z \\rangle &= \\langle x, z \\rangle + \\langle y, z \\rangle\n\\end{align}</math>\n* [[Definite bilinear form|Positive-definite]]:\n*: <math>\\langle x, x \\rangle > 0,\\quad x \\in V \\setminus \\{\\mathbf{0}\\}.</math>\n\n=== Elementary properties ===\nPositive-definiteness and linearity, respectively, ensure that:\n:<math>\\begin{align}\n                    \\langle x, x \\rangle &= 0 \\Rightarrow x = \\mathbf{0} \\\\\n  \\langle \\mathbf{0}, \\mathbf{0} \\rangle &= \\langle 0x, 0x \\rangle = 0 \\langle x, 0x \\rangle = 0\n\\end{align}</math>\n\nNotice that conjugate symmetry implies that {{math|{{angbr|''x'', ''x''}}}} is real for all {{math|''x''}}, since we have:\n: <math>\\langle x, x \\rangle = \\overline{\\langle x, x \\rangle} \\,.</math>\n\nConjugate symmetry and linearity in the first variable imply\n\n:<math>\\begin{align}\n    \\langle x, a y \\rangle &= \\overline{\\langle a y, x \\rangle} = \\overline{a} \\overline{\\langle y, x \\rangle} = \\overline{a} \\langle x, y \\rangle \\\\\n  \\langle x, y + z \\rangle &= \\overline{\\langle y + z, x \\rangle} = \\overline{\\langle y, x \\rangle} + \\overline{\\langle z, x \\rangle} = \\langle x, y \\rangle + \\langle x, z \\rangle \\,;\n\\end{align}</math>\n\nthat is, [[antilinear map|conjugate linearity]] in the second argument. So, an inner product is a [[sesquilinear form]]. Conjugate symmetry is also called Hermitian symmetry, and a conjugate-symmetric sesquilinear form is called a ''Hermitian form''. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a ''positive-definite Hermitian form''.\n\nThis important generalization of the familiar square expansion follows:\n: <math>\\langle x + y, x + y \\rangle = \\langle x, x \\rangle + \\langle x, y \\rangle + \\langle y, x \\rangle + \\langle y, y \\rangle \\,.</math>\n\nThese properties, constituents of the above linearity in the first and second argument:\n:<math>\\begin{align}\n  \\langle x + y, z \\rangle &= \\langle x, z \\rangle + \\langle y, z \\rangle \\,, \\\\\n  \\langle x, y + z \\rangle &= \\langle x, y\\rangle + \\langle x, z \\rangle\n\\end{align}</math>\n\nare otherwise known as [[additive map|''additivity'']].\n\nIn the case of {{math|''F'' {{=}} '''R'''}}, conjugate-symmetry reduces to symmetry, and sesquilinearity reduces to bilinearity. So, an inner product on a real vector space is a ''positive-definite symmetric bilinear form''. That is,\n\n:<math>\\begin{align}\n               \\langle x, y \\rangle &= \\langle y, x \\rangle \\\\\n  \\Rightarrow \\langle -x, x \\rangle &= \\langle x, -x \\rangle \\,,\n\\end{align}</math>\n\nand the [[binomial expansion]] becomes:\n: <math>\\langle x + y, x + y \\rangle = \\langle x, x \\rangle + 2\\langle x, y \\rangle + \\langle y, y \\rangle \\,.</math>\n\n=== Alternative definitions, notations and remarks===\nA common special case of the inner product, the scalar product or [[dot product]], is written with a centered dot <math>a \\cdot b</math>.<!-- This is more than just any special case; in fact, the article [[dot product]] currently says in the lede \"In Euclidean geometry, the dot product [...] is widely used and often called \"the\" '''inner product''' [...].\" -->\n\nSome authors, especially in [[physics]] and [[matrix algebra]], prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product {{math|{{angbr|''x'', ''y''}}}} as {{math|{{bra-ket|''y''{{nnbsp}}|{{nnbsp}}''x''}}}} (the [[bra–ket notation]] of [[quantum mechanics]]), respectively {{math|''y''<sup>†</sup>''x''}} (dot product as a case of the convention of forming the matrix product {{math|''AB''}} as the dot products of rows of {{math|''A''}} with columns of {{math|''B''}}). Here the kets and columns are identified with the vectors of {{math|''V''}} and the bras and rows with the [[linear functional]]s (covectors) of the [[dual space]] {{math|''V''{{isup|∗}}}}, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,<ref>{{cite book|last=Emch|first=Gerard G.|title=Algebraic Methods in Statistical Mechanics and Quantum Field Theory|year=1972|publisher=[[Wiley-Interscience]]|location=New York|isbn=978-0-471-23900-0}}</ref> taking {{math|{{angbr|''x'', ''y''}}}} to be conjugate linear in {{math|''x''}} rather than {{math|''y''}}. A few instead find a middle ground by recognizing both {{math|{{angbr|·, ·}}}} and {{math|{{bra-ket|·{{nnbsp}}|{{nnbsp}}·}}}} as distinct notations differing only in which argument is conjugate linear.\n\nThere are various technical reasons why it is necessary to restrict the [[basefield]] to {{math|'''R'''}} and {{math|'''C'''}} in the definition. Briefly, the basefield has to contain an [[ordered field|ordered subfield]] in order for non-negativity to make sense,<ref>{{citation|title=Introduction to Matrices and Linear Transformations|edition=3rd|series=Dover Books on Mathematics|first=Daniel T.|last=Finkbeiner|publisher=Courier Dover Publications|year=2013|url=https://books.google.com/books?id=-f_DAgAAQBAJ&pg=PA242|page=242|isbn=9780486279664}}.</ref> and therefore has to have [[characteristic (algebra)|characteristic]] equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of {{math|'''R'''}} or {{math|'''C'''}} will suffice for this purpose, e.g., the [[algebraic number]]s or the [[Constructible_number#Transformation_into_algebra|constructible number]]s. However in these cases when it is a proper subfield (i.e., neither {{math|'''R'''}} nor {{math|'''C'''}}) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over {{math|'''R'''}} or {{math|'''C'''}}, such as those used in [[quantum computation]], are automatically [[Complete metric space|metrically complete]] and hence [[Hilbert space]]s.\n\nIn some cases we need to consider non-negative ''semi-definite'' sesquilinear forms. This means that {{math|{{angbr|''x'', ''x''}}}} is only required to be non-negative. We show how to treat these below.\n\n== Examples ==\n===Real numbers===\n\nA simple example is the [[real numbers]] with the standard multiplication as the inner product\n:<math>\\langle x, y\\rangle := x y.</math>\n\n===Euclidean space===\n\nMore generally, the [[real coordinate space|real {{math|''n''}}-space]] {{math|'''R'''<sup>''n''</sup>}} with the [[dot product]] is an inner product space, an example of a [[Euclidean space|Euclidean {{math|''n''}}-space]].\n:<math>\n  \\left\\langle\n    \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix},\n    \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\n  \\right\\rangle :=\n  x^\\textsf{T} y = \\sum_{i=1}^n x_i y_i = x_1 y_1 + \\cdots + x_n y_n,\n</math>\n\nwhere {{math|''x''{{isup|{{font|T}}}}}} is the [[transpose]] of {{math|''x''}}.\n\n===Complex coordinate space===\n\nThe general form of an inner product on {{math|'''C'''<sup>''n''</sup>}} is known as the [[Hermitian form]] and is given by\n:<math>\\langle x, y \\rangle := y^\\dagger\\mathbf{M}x = \\overline{x^\\dagger\\mathbf{M}y},</math>\n\nwhere {{math|'''M'''}} is any [[Hermitian matrix|Hermitian]] [[positive-definite matrix]] and {{math|''y''<sup>†</sup>}} is the [[conjugate transpose]] of {{math|''y''}}. For the real case this corresponds to the dot product of the results of directionally different [[scaling (geometry)|scaling]] of the two vectors, with positive [[scale factor]]s and orthogonal directions of scaling. Up to an orthogonal transformation it is a [[weight function|weighted-sum]] version of the dot product, with positive weights.\n\n===Hilbert space===\n\nThe article on [[Hilbert space]] has several examples of inner product spaces wherein the metric induced by the inner product yields a [[complete space|complete]] metric space. An example of an inner product which induces an incomplete metric occurs with the space {{math|''C''([''a'', ''b''])}} of continuous complex valued functions ''f'' and ''g'' on the interval {{math|[''a'', ''b'']}}. The inner product is\n:<math> \\langle f, g \\rangle := \\int_a^b f(t) \\overline{g(t)} \\, \\mathrm{d}t. </math>\nThis space is not complete; consider for example, for the interval {{closed-closed|−1, 1}} the sequence of continuous \"step\" functions, {{math|{&thinsp;''f<sub>k</sub>''}<sub>''k''</sub>}}, defined by:\n:<math> f_k(t) = \\begin{cases} 0 & t \\in [-1, 0] \\\\ 1 & t \\in \\left [\\tfrac{1}{k}, 1 \\right] \\\\ kt & t \\in \\left (0, \\tfrac{1}{k} \\right) \\end{cases}</math>\n\nThis sequence is a [[Cauchy sequence]] for the norm induced by the preceding inner product, which does not converge to a ''continuous'' function.\n\n===Random variables===\n\nFor real [[random variable]]s {{math|''X''}} and {{math|''Y''}}, the [[expected value]] of their product\n:<math> \\langle X, Y \\rangle := \\operatorname{E}(XY) </math>\n\nis an inner product.<ref>{{cite web|last1=Ouwehand|first1=Peter|title=Spaces of Random Variables|url=http://users.aims.ac.za/~pouw/Lectures/Lecture_Spaces_Random_Variables.pdf|website=AIMS|accessdate=2017-09-05|date=November 2010}}</ref><ref>{{cite web|last1=Siegrist|first1=Kyle|title=Vector Spaces of Random Variables|url=http://www.math.uah.edu/stat/expect/Spaces.html|website=Random: Probability, Mathematical Statistics, Stochastic Processes|accessdate=2017-09-05|date=1997}}</ref><ref>{{cite thesis|last1=Bigoni|first1=Daniele|title=Uncertainty Quantification with Applications to Engineering Problems|date=2015|type=PhD |publisher=Technical University of Denmark|url=http://orbit.dtu.dk/files/106969507/phd359_Bigoni_D.pdf|accessdate=2017-09-05|chapter=Appendix B: Probability theory and functional spaces}}</ref> In this case, {{math|{{angbr|''X'', ''X''}} {{=}} 0}} if and only if {{math|[[probability|Pr]](''X'' {{=}} 0) {{=}} 1}} (i.e., {{math|''X'' {{=}} 0}} [[almost surely]]). This definition of expectation as inner product can be extended to [[random vector]]s as well.\n\n===Real matrices===\n\nFor real square matrices of the same size, {{math|{{angbr|''A'', ''B''}} :{{=}} tr(''AB''{{sup|{{font|T}}}})}} with transpose as conjugation\n:<math> \\left(\\langle A, B \\rangle = \\left\\langle B^\\textsf{T}, A^\\textsf{T} \\right\\rangle \\right)</math>\nis an inner product.\n\n===Vector spaces with forms===\nOn an inner product space, or more generally a vector space with a [[nondegenerate form]] (so an isomorphism {{math|''V'' → ''V''{{isup|∗}}}}) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.\n\n== Norms on inner product spaces ==<!-- This section is linked from [[Cauchy–Schwarz inequality]] -->\nA linear space with a norm such as:\n\n:<math>\\|x\\|_p = \\left( \\sum_{i=1}^{\\infty} \\left| \\xi_i \\right|^p \\right)^{\\frac{1}{p}} \\qquad x = \\left\\{ \\xi_i \\right\\} \\in l^p, \\quad p \\neq 2,</math>\n\nis a [[normed space]] but not an inner product space, because this norm does not satisfy the [[parallelogram equality#Normed vector spaces satisfying the parallelogram law|parallelogram equality]] required of a norm to have an inner product associated with it.<ref name=Ahmed>{{cite book |title=Functional Analysis |first1=P. K. |last1=Jain |first2=Khalil |last2=Ahmad | chapter-url=https://books.google.com/books?id=yZ68h97pnAkC&pg=PA209 |page=209 |chapter=Example 5 |isbn=81-224-0801-X |year=1995 |edition=2nd |publisher=New Age International}}</ref><ref name=Saxe>{{cite book |title=Beginning Functional Analysis |first=Karen |last=Saxe|authorlink= Karen Saxe |url=https://books.google.com/books?id=QALoZC64ea0C&pg=PA7 |page=7 |isbn=0-387-95224-1 |year=2002 |publisher=Springer}}</ref>\n\nHowever, inner product spaces have a naturally defined [[Norm (mathematics)|norm]] based upon the inner product of the space itself that does satisfy the parallelogram equality:\n\n:<math>\\|x\\| = \\sqrt{\\langle x, x \\rangle}.</math>\n\nThis is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector {{math|''x''}}. Directly from the axioms, we can prove the following:\n\n{{glossary}}\n{{term|[[Cauchy–Schwarz inequality]]}}{{defn|\nFor {{math|''x''}}, {{math|''y''}} elements of {{math|''V''}}\n: <math> |\\langle x, y \\rangle| \\leq \\|x\\| \\, \\|y\\| </math>\n\nwith equality if and only if {{math|''x''}} and {{math|''y''}} are [[linearly independent|linearly dependent]]. This is one of the most important inequalities in mathematics. It is also known in the Russian mathematical literature as the ''Cauchy–Bunyakovsky–Schwarz inequality''.\n}}\n{{term|[[Orthogonal]]ity}}{{defn|\nThe geometric interpretation of the inner product in terms of angle and length, motivates much of the geometric terminology we use in regard to these spaces. Indeed, an immediate consequence of the Cauchy–Schwarz inequality is that it justifies defining the [[angle]] between two non-zero vectors {{math|''x''}} and {{math|''y''}} (denoted {{math|∠}}) in the case {{math|''F'' {{=}} '''R'''}} by the identity\n: <math>\\angle(x, y) = \\arccos \\frac{\\langle x, y \\rangle}{\\|x\\| \\, \\|y\\|}.</math>\n\nWe assume the value of the angle is chosen to be in the interval {{closed-closed|0, π}}. This is in analogy to the situation in two-dimensional [[Euclidean space]].\n\nIn the case {{math|''F'' {{=}} '''C'''}}, the angle in the interval {{closed-closed|0, {{sfrac|π|2}}}} is typically defined by\n: <math>\\angle(x, y) = \\arccos \\frac{|\\langle x, y \\rangle|}{\\|x\\| \\, \\|y\\|}.</math>\n\nCorrespondingly, we will say that non-zero vectors {{math|''x''}} and {{math|''y''}} of {{math|''V''}} are orthogonal if and only if their inner product is zero.\n}}\n{{term|[[Homogeneous function|Homogeneity]]}}{{defn|\nFor {{math|''x''}} an element of {{math|''V''}} and {{math|''r''}} a scalar\n: <math> \\|rx\\| = \\|r\\| \\, \\| x\\|.</math>\n\nThe homogeneity property is completely trivial to prove.\n}}\n{{term|[[Triangle inequality]]}}{{defn|\nFor {{math|''x''}}, {{math|''y''}} elements of {{math|''V''}}\n: <math> \\|x + y\\| \\leq \\|x \\| + \\|y\\|. </math>\n\nThe last two properties show the function defined is indeed a norm.\n\nBecause of the triangle inequality and because of axiom 2, we see that {{math|{{norm|·}}}} is a norm which turns {{math|''V''}} into a [[normed vector space]] and hence also into a [[metric space]]. The most important inner product spaces are the ones which are [[completeness (topology)|complete]] with respect to this metric; they are called [[Hilbert space]]s. Every inner product {{math|''V''}} space is a [[Dense set|dense]] subspace of some Hilbert space. This Hilbert space is essentially uniquely determined by {{math|''V''}} and is constructed by completing {{math|''V''}}.\n}}\n{{term|[[Pythagorean theorem]]}}{{defn|\nWhenever {{math|''x''}}, {{math|''y''}} are in {{math|''V''}} and {{math|{{angbr|''x'', ''y''}} {{=}} 0}}, then\n: <math> \\|x\\|^2 + \\|y\\|^2 = \\|x + y\\|^2. </math>\n\nThe proof of the identity requires only expressing the definition of norm in terms of the inner product and multiplying out, using the property of additivity of each component.\n\nThe name ''Pythagorean theorem'' arises from the geometric interpretation of this result as an analogue of the theorem in [[synthetic geometry]]. Note that the proof of the Pythagorean theorem in synthetic geometry is considerably more elaborate because of the paucity of underlying structure. In this sense, the synthetic Pythagorean theorem, if correctly demonstrated, is deeper than the version given above.\n}}\n{{term|Parseval's identity}}{{defn|\nAn [[mathematical induction|induction]] on the Pythagorean theorem yields: if {{math|''x''<sub>1</sub>, …, ''x''<sub>''n''</sub>}} are [[orthogonal]] vectors, that is, {{math|{{angbr|''x<sub>j</sub>'', ''x<sub>k</sub>''}} {{=}} 0}} for distinct indices {{math|''j''}}, {{math|''k''}}, then\n: <math> \\sum_{i=1}^n \\|x_i\\|^2 = \\left\\|\\sum_{i=1}^n x_i \\right\\|^2. </math>\n\nIn view of the Cauchy-Schwarz inequality, we also note that {{math|{{angbr|·, ·}}}} is [[continuous function|continuous]] from {{math|''V'' × ''V''}} to {{math|''F''}}. This allows us to extend Pythagoras' theorem to infinitely many summands: suppose {{math|''V''}} is a ''complete'' inner product space. If {{math|{''x''<sub>''k''</sub><nowiki>}</nowiki>}} are mutually orthogonal vectors in {{math|''V''}} then\n: <math> \\sum_{i=1}^\\infty\\|x_i\\|^2 = \\left\\|\\sum_{i=1}^\\infty x_i\\right\\|^2, </math>\n\nprovided the infinite series on the left is [[convergence (series)|convergent]]. Completeness of the space is needed to ensure that the sequence of partial sums\n: <math> S_k = \\sum_{i=1}^k x_i, </math>\n\nwhich is easily shown to be a [[Cauchy sequence]], is convergent.\n}}\n{{term|[[Parallelogram law]]}}{{defn|\nFor {{math|''x''}}, {{math|''y''}} elements of {{math|''V''}},\n: <math> \\|x + y\\|^2 + \\|x - y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2. </math>\n\nThe parallelogram law is, in fact, a necessary and sufficient condition for the existence of a inner product corresponding to a given norm. If it holds, the inner product is defined by the [[polarization identity]]:\n: <math> \\|x + y\\|^2 = \\|x\\|^2 + \\|y\\|^2 + 2\\operatorname{Re}\\langle x, y \\rangle. </math>\n\nwhich is a form of the [[law of cosines]].\n}}\n{{term|[[Ptolemy's inequality]]}}{{defn|\nFor {{math|''x''}}, {{math|''y''}}, {{math|''z''}} elements of {{math|''V''}},\n: <math> \\|x - y\\| \\|z\\| + \\|y - z\\| \\|x\\| \\ge \\|x - z\\| \\|y\\|. </math>\n\nPtolemy's inequality is, in fact, a necessary and sufficient condition for the existence of a inner product corresponding to a given norm. In detail, [[Isaac Jacob Schoenberg]] proved in 1952 that, given any real, [[seminorm|seminormed]] space, if its seminorm is ptolemaic, then the seminorm is a norm which springs from an inner product.<ref>{{Cite journal|last=Apostol|first=Tom M.|date=1967|title=Ptolemy's Inequality and the Chordal Metric|url=https://www.tandfonline.com/doi/pdf/10.1080/0025570X.1967.11975804|journal=Mathematics Magazine|language=en|doi=10.2307/2688275}}</ref> \n\nAs in the case of parallelogram law, the inner product is defined by the [[polarization identity]].\n\n}}\n{{glossary end}}\n\n== Orthonormal sequences ==\nLet {{math|''V''}} be a finite dimensional inner product space of dimension {{math|''n''}}. Recall that every [[Basis (linear algebra)|basis]] of {{math|''V''}} consists of exactly {{math|''n''}} linearly independent vectors. Using the [[Gram–Schmidt process]] we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {{math|{''e''<sub>1</sub>, ..., ''e<sub>n</sub>''<nowiki>}</nowiki>}} is orthonormal if {{math|{{angbr|''e<sub>i</sub>'', ''e<sub>j</sub>''}} {{=}} 0}} for every {{math|''i'' ≠ ''j''}} and {{math|{{angbr|''e<sub>i</sub>'', ''e<sub>i</sub>''}} {{=}} {{norm|''e<sub>i</sub>''}} {{=}} 1}} for each {{math|''i''}}.\n\nThis definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let {{math|''V''}} be any inner product space. Then a collection\n\n:<math>E = \\left\\{ e_\\alpha \\right\\}_{\\alpha \\in A}</math>\n\nis a ''basis'' for {{math|''V''}} if the subspace of {{math|''V''}} generated by finite linear combinations of elements of {{math|''E''}} is dense in {{math|''V''}} (in the norm induced by the inner product). We say that {{math|''E''}} is an ''orthonormal basis'' for {{math|''V''}} if it is a basis and\n\n:<math>\\left \\langle e_{\\alpha}, e_{\\beta} \\right \\rangle = 0</math>\n\nif {{math|''α'' ≠ ''β''}} and {{math|{{angbr|''e<sub>α</sub>'', ''e<sub>α</sub>''}} {{=}} {{norm|''e<sub>α</sub>''}} {{=}} 1}} for all {{math|''α'', ''β'' ∈ ''A''}}.\n\nUsing an infinite-dimensional analog of the Gram-Schmidt process one may show:\n\n'''Theorem.''' Any [[separable space|separable]] inner product space {{math|''V''}} has an orthonormal basis.\n\nUsing the [[Hausdorff maximal principle]] and the fact that in a [[Hilbert space|complete inner product space]] orthogonal projection onto linear subspaces is well-defined, one may also show that\n\n'''Theorem.''' Any [[Hilbert space|complete inner product space]] {{math|''V''}} has an orthonormal basis.\n\nThe two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's ''A Hilbert Space Problem Book'' (see the references).{{citation needed|date=October 2017}}\n\n:{| class=\"toccolours collapsible collapsed\" width=\"90%\" style=\"text-align:left\"\n!Proof\n|-\n| Recall that the dimension of an inner product space is the [[cardinality]] of a maximal orthonormal system that it contains (by [[Zorn's lemma]] it contains at least one, and any two have the same cardinality). An orthonormal basis is certainly a maximal orthonormal system, but as we shall see, the converse need not hold. Observe that if {{math|''G''}} is a dense subspace of an inner product space {{math|''H''}}, then any orthonormal basis for {{math|''G''}} is automatically an orthonormal basis for {{math|''H''}}. Thus, it suffices to construct an inner product space {{math|''H''}} with a dense subspace {{math|''G''}} whose dimension is strictly smaller than that of {{math|''H''}}.\n\nLet {{math|''K''}} be a [[Hilbert space]] of dimension {{math|[[Aleph-null|ℵ<sub>0</sub>]]}} (for instance, {{math|''K'' {{=}} ''l''{{isup|2}}('''N''')}}). Let {{math|''E''}} be an orthonormal basis of {{math|''K''}}, so {{math|{{abs|''E''}} {{=}} ℵ<sub>0</sub>}}. Extend {{math|''E''}} to a [[Basis (linear algebra)#Related notions|Hamel basis]] {{math|''E'' ∪ ''F''}} for {{math|''K''}}, where {{math|''E'' ∩ ''F'' {{=}} ∅}}. Since it is known that the [[Hamel dimension]] of {{math|''K''}} is {{math|''c''}}, the cardinality of the continuum, it must be that {{math|{{abs|''F''}} {{=}} ''c''}}.\n\nLet {{math|''L''}} be a Hilbert space of dimension {{math|''c''}} (for instance, {{math|''L'' {{=}} ''l''{{isup|2}}('''R''')}}). Let {{math|''B''}} be an orthonormal basis for {{math|''L''}}, and let {{math|''φ'' : ''F'' → ''B''}} be a bijection. Then there is a linear transformation {{math|''T'' : ''K'' → ''L''}} such that {{math|''Tf'' {{=}} ''φ''(&thinsp;''f''&thinsp;)}} for {{math|''f'' ∈ ''F''}}, and {{math|''Te'' {{=}} 0}} for {{math|''e'' ∈ ''E''}}.\n\nLet {{math|''H'' {{=}} ''K'' ⊕ ''L''}} and let {{math|''G'' {{=}} {(''k'', ''Tk'') : ''k'' ∈ ''K'')<nowiki>}</nowiki>}} be the graph of {{math|''T''}}. Let {{math|''Ḡ''}} be the closure of {{math|''G''}} in {{math|''H''}}; we will show {{math|''Ḡ'' {{=}} H}}. Since for any {{math|''e'' ∈ ''E''}} we have {{math|(''e'', ''0'') ∈ ''G''}}, it follows that {{math|''K'' ⊕ 0 ⊂ ''Ḡ''}}.\n\nNext, if {{math|''b'' ∈ ''B''}}, then {{math|''b'' {{=}} ''Tf''}} for some {{math|''f'' ∈ ''F'' ⊂ ''K''}}, so {{math|(&thinsp;''f'', ''b'') ∈ ''G'' ⊂ ''Ḡ''}}; since {{math|(&thinsp;''f'', 0) ∈ ''Ḡ''}} as well, we also have {{math|(''0'', ''b'') ∈ ''Ḡ''}}. It follows that {{math|0 ⊕ ''L'' ⊂ ''Ḡ''}}, so {{math|''Ḡ'' {{=}} ''H''}}, and {{math|''G''}} is dense in {{math|''H''}}.\n\nFinally, {{math|{(''e'', 0) : ''e'' ∈ ''E''<nowiki>}</nowiki>}} is a maximal orthonormal set in {{math|''G''}}; if\n\n:<math>0 = \\langle (e, 0), (k, Tk) \\rangle = \\langle e, k \\rangle + \\langle 0, Tk \\rangle = \\langle e, k \\rangle</math>\n\nfor all {{math|''e'' ∈ ''E''}} then certainly {{math|''k'' {{=}} 0}}, so {{math|(''k'', ''Tk'') {{=}} (0, 0)}} is the zero vector in {{math|''G''}}. Hence the dimension of {{math|''G''}} is {{math|{{abs|''E''}} {{=}} ℵ<sub>0</sub>}}, whereas it is clear that the dimension of {{math|''H''}} is {{math|''c''}}. This completes the proof.\n|}\n\n[[Parseval's identity]] leads immediately to the following theorem:\n\n'''Theorem.''' Let {{math|''V''}} be a separable inner product space and {{math|{''e''<sub>''k''</sub>}<sub>''k''</sub>}} an orthonormal basis of&nbsp;{{math|''V''}}. Then the map\n:<math> x \\mapsto \\bigl\\{\\langle e_k, x\\rangle\\bigr\\}_{k \\in \\mathbb{N}} </math>\nis an isometric linear map {{math|''V'' → ''l''{{isup|2}}}} with a dense image.\n\nThis theorem can be regarded as an abstract form of [[Fourier series]], in which an arbitrary orthonormal basis plays the role of the sequence of [[trigonometric polynomial]]s. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided {{math|''l''{{isup|2}}}} is defined appropriately, as is explained in the article [[Hilbert space]]). In particular, we obtain the following result in the theory of Fourier series:\n\n'''Theorem.''' Let {{math|''V''}} be the inner product space {{math|''C''[−π, π]}}. Then the sequence (indexed on set of all integers) of continuous functions\n\n:<math>e_k(t) = \\frac{e^{i k t}}{\\sqrt{2 \\pi}}</math>\n\nis an orthonormal basis of the space {{math|''C''[−π, π]}} with the {{math|''L''<sup>2</sup>}} inner product. The mapping\n\n:<math> f \\mapsto \\frac{1}{\\sqrt{2 \\pi}} \\left\\{\\int_{-\\pi}^\\pi f(t) e^{-i k t} \\, \\mathrm{d}t \\right\\}_{k \\in \\mathbb{Z}} </math>\n\nis an isometric linear map with dense image.\n\nOrthogonality of the sequence {{math|{''e<sub>k</sub>''}<sub>''k''</sub>}} follows immediately from the fact that if {{math|''k'' ≠ ''j''}}, then\n\n:<math> \\int_{-\\pi}^\\pi e^{-i (j - k) t} \\, \\mathrm{d}t = 0. </math>\n\nNormality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the ''inner product norm'', follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on {{math|[−π, π]}} with the uniform norm. This is the content of the [[Weierstrass approximation theorem|Weierstrass theorem]] on the uniform density of trigonometric polynomials.\n\n==Operators on inner product spaces==\nSeveral types of [[linear]] maps {{math|''A''}} from an inner product space {{math|''V''}} to an inner product space {{math|''W''}} are of relevance:\n* [[Continuous linear operator|Continuous linear maps]], i.e., {{math|''A''}} is linear and continuous with respect to the metric defined above, or equivalently, {{math|''A''}} is linear and the set of non-negative reals {{math|{{{norm|''Ax''}}<nowiki>}</nowiki>}}, where {{math|''x''}} ranges over the closed unit ball of {{math|''V''}}, is bounded.\n* Symmetric linear operators, i.e., {{math|''A''}} is linear and {{math|{{angbr|''Ax'', ''y''}} {{=}} {{angbr|''x'', ''Ay''}}}} for all {{math|''x''}}, {{math|''y''}} in {{math|''V''}}.\n* Isometries, i.e., {{math|''A''}} is linear and {{math|{{angbr|''Ax'', ''Ay''}} {{=}} {{angbr|''x'', ''y''}}}} for all {{math|''x''}}, {{math|''y''}} in {{math|''V''}}, or equivalently, {{math|''A''}} is linear and {{math|{{norm|''Ax''}} {{=}} {{norm|''x''}}}} for all {{math|''x''}} in {{math|''V''}}. All isometries are [[injective]]. Isometries are [[morphism]]s between inner product spaces, and morphisms of real inner product spaces are orthogonal transformations (compare with [[orthogonal matrix]]).\n* Isometrical isomorphisms, i.e., {{math|''A''}} is an isometry which is [[surjective]] (and hence [[bijective]]). Isometrical isomorphisms are also known as unitary operators (compare with [[unitary matrix]]).\n\nFrom the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The [[spectral theorem]] provides a canonical form for symmetric, unitary and more generally [[normal operator]]s on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.\n\n== Generalizations ==\nAny of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.\n\n=== Degenerate inner products ===\nIf {{math|''V''}} is a vector space and {{math|{{angbr|·, ·}}}} a semi-definite sesquilinear form, then the function:\n\n:<math> \\|x\\| = \\sqrt{\\langle x, x\\rangle}</math>\n\nmakes sense and satisfies all the properties of norm except that {{math|{{norm|''x''}} {{=}} 0}} does not imply {{math|''x'' {{=}} 0}} (such a functional is then called a [[semi-norm]]). We can produce an inner product space by considering the quotient {{math|''W'' {{=}} ''V''/{''x'' : {{norm|''x''}} {{=}} 0}}}. The sesquilinear form {{math|{{angbr|·, ·}}}} factors through {{math|''W''}}.\n\nThis construction is used in numerous contexts. The [[Gelfand–Naimark–Segal construction]] is a particularly important example of the use of this technique. Another example is the representation of [[Mercer's theorem|semi-definite kernel]]s on arbitrary sets.\n\n=== Nondegenerate conjugate symmetric forms ===\n{{Main|Pseudo-Euclidean space}}\nAlternatively, one may require that the pairing be a [[nondegenerate form]], meaning that for all non-zero {{math|''x''}} there exists some {{math|''y''}} such that {{math|{{angbr|''x'', ''y''}} ≠ 0}}, though {{math|''y''}} need not equal {{math|''x''}}; in other words, the induced map to the dual space {{math|''V'' → ''V''{{isup|∗}}}} is injective. This generalization is important in [[differential geometry]]: a manifold whose tangent spaces have an inner product is a [[Riemannian manifold]], while if this is related to nondegenerate conjugate symmetric form the manifold is a [[pseudo-Riemannian manifold]]. By [[Sylvester's law of inertia]], just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with ''nonzero'' weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in [[Minkowski space]] is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four [[dimension (mathematics)|dimensions]] and indices 3 and 1 (assignment of [[sign (mathematics)|\"+\" and \"−\"]] to them [[sign convention#Metric signature|differs depending on conventions]]).\n\nPurely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism {{math|''V'' → ''V''{{isup|∗}}}}) and thus hold more generally.\n\n==Related products==\nThe term \"inner product\" is opposed to [[outer product]], which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a {{math|1 × ''n''}} ''covector'' with an {{math|''n'' × 1}} vector, yielding a 1&nbsp;×&nbsp;1 matrix (a scalar), while the outer product is the product of an {{math|''m'' × 1}} vector with a {{math|1 × ''n''}} covector, yielding an {{math|''m'' × ''n''}} matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the ''[[Trace (linear algebra)|trace]]'' of the outer product (trace only being properly defined for square matrices).  In a quip: \"inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out\".\n\nMore abstractly, the outer product is the bilinear map {{math|''W'' × ''V''{{isup|∗}} → Hom(''V'', ''W'')}} sending a vector and a covector to a rank 1 linear transformation ([[simple tensor]] of type (1, 1)), while the inner product is the bilinear evaluation map {{math|''V''{{isup|∗}} × ''V'' → ''F''}} given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.\n\nThe inner product and outer product should not be confused with the [[interior product]] and [[exterior product]], which are instead operations on [[vector field]]s and [[differential form]]s, or more generally on the [[exterior algebra]].\n\nAs a further complication, in [[geometric algebra]] the inner product and the ''exterior'' (Grassmann) product are combined in the geometric product (the Clifford product in a [[Clifford algebra]]) – the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) – and in this context the exterior product is usually called the ''outer product'' (alternatively, ''wedge product''). The inner product is more correctly called a ''scalar'' product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).\n\n==See also==\n* [[Space (mathematics)]]\n* [[Semi-inner-product]]\n* [[Normed vector space]]\n* [[Energetic space]]\n* [[Dual space]]\n* [[Biorthogonal system]]\n* [[Bilinear form]]\n\n== Notes ==\n{{Reflist|group=Note}}\n\n== References ==\n{{Reflist}}\n\n== Sources ==\n* {{Cite book | last1=Axler | first1=Sheldon | title=Linear Algebra Done Right | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-98258-8 | year=1997 | postscript=<!--None-->}}\n* {{Cite book | last1=Emch | first1=Gerard G. | title=Algebraic Methods in Statistical Mechanics and Quantum Field Theory | publisher=[[Wiley-Interscience]] | isbn=978-0-471-23900-0 | year=1972 | postscript=<!--None-->}}\n* {{Cite book | last1=Young | first1=Nicholas | title=An Introduction to Hilbert Space | publisher=[[Cambridge University Press]] | isbn=978-0-521-33717-5 | year=1988 | postscript=<!--None-->}}\n\n<!-- AWB bots, please leave this space alone. -->\n\n{{linear algebra}}\n{{Functional analysis}}\n\n{{reflist|group=Note}}\n\n[[Category:Normed spaces]]\n[[Category:Bilinear forms]]"
    },
    {
      "title": "Isotropic quadratic form",
      "url": "https://en.wikipedia.org/wiki/Isotropic_quadratic_form",
      "text": "In mathematics, a [[quadratic form]] over a [[field (mathematics)|field]] ''F'' is said to be '''isotropic''' if there is a non-zero vector on which the form evaluates to zero. Otherwise the quadratic form is '''anisotropic'''. More precisely, if ''q'' is a quadratic form on a [[vector space]] ''V'' over ''F'', then a non-zero vector ''v'' in ''V'' is said to be '''isotropic''' if {{nowrap|1=''q''(''v'') = 0}}. A quadratic form is isotropic if and only if there exists a non-zero isotropic vector (or [[null vector]]) for that quadratic form. \n\nSuppose that {{nowrap|(''V'', ''q'')}} is [[quadratic space]] and ''W'' is a [[linear subspace|subspace]]. Then ''W'' is called an '''isotropic subspace''' of ''V'' if ''some'' vector in it is isotropic, a '''totally isotropic subspace''' if ''all'' vectors in it are isotropic, and an '''anisotropic subspace''' if it does not contain ''any'' (non-zero) isotropic vectors. The '''{{visible anchor|isotropy index}}''' of a quadratic space is the maximum of the dimensions of the totally isotropic subspaces.<ref name=MH/>\n\nA quadratic form ''q'' on a finite-dimensional real vector space ''V'' is anisotropic if and only if ''q'' is a [[definite bilinear form|definite form]]:\n:* either ''q'' is ''positive definite'', i.e. {{nowrap|1=''q''(''v'') > 0}} for all non-zero ''v'' in ''V'' ; \n:* or ''q'' is ''negative definite'', i.e. {{nowrap|1=''q''(''v'') < 0}} for all non-zero ''v'' in ''V''. \n\nMore generally, if the quadratic form is non-degenerate and has the [[signature (quadratic form)|signature]] {{nowrap|(''a'', ''b'')}}, then its isotropy index is the minimum of ''a'' and ''b''.\n\n==Hyperbolic plane==\n{{hatnote|Not to be confused with the [[plane (geometry)|plane]] in [[hyperbolic geometry]].}}\nLet ''F'' be a field of characteristic not 2 and {{nowrap|1=''V'' = ''F''<sup>2</sup>}}.  If we consider the general element {{nowrap|(''x'', ''y'')}} of ''V'', then the quadratic forms {{nowrap|1=''q'' = ''xy''}} and {{nowrap|1=''r'' = ''x''<sup>2</sup> − ''y''<sup>2</sup>}} are equivalent since there is a [[linear transformation]] on ''V'' that makes ''q'' look like ''r'', and vice versa. Evidently, {{nowrap|(''V'', ''q'')}} and {{nowrap|(''V'', ''r'')}} are isotropic. This example is called the '''hyperbolic plane''' in the theory of [[quadratic form]]s. A common instance has ''F'' = [[real number]]s in which case {{nowrap|1={''x'' ∈ ''V'' : ''q''(''x'') = nonzero constant} }} and {{nowrap|1={''x'' ∈ ''V'' : ''r''(''x'') = nonzero constant} }} are [[hyperbola]]s. In particular, {{nowrap|1={''x'' ∈ ''V'' : ''r''(''x'') = 1} }} is the [[unit hyperbola]]. The notation {{nowrap|{{langle}}1{{rangle}} ⊕ {{langle}}−1{{rangle}}}} has been used by Milnor and Huseman<ref name=MH/>{{rp|9}} for the hyperbolic plane as the signs of the terms of the [[polynomial|bivariate polynomial]] ''r'' are exhibited.\n\nThe affine hyperbolic plane was described by [[Emil Artin]] as a quadratic space with basis {{nowrap|{''M'', ''N''} }} satisfying {{nowrap|1=''M''<sup>2</sup> = ''N''<sup>2</sup> = 0, ''NM'' = 1}}, where the products represent the quadratic  form.<ref>[[Emil Artin]] (1957) [[Geometric Algebra]], page 119</ref>\n\nThrough the [[polarization identity#Symmetric bilinear forms|polarization identity]] the quadratic form is related to a [[symmetric bilinear form]] {{nowrap|1=''B''(''u'', ''v'') = {{sfrac|1|4}}(''q''(''u'' + ''v'') − ''q''(''u'' − ''v''))}}.\n\nTwo vectors ''u'' and ''v'' are [[orthogonal]] when {{nowrap|1=''B''(''u'', ''v'') = 0}}. In the case of the hyperbolic plane, such ''u'' and ''v'' are [[hyperbolic-orthogonal]].\n\n==Split quadratic space==\nA space with quadratic form is '''split''' (or '''metabolic''') if there is a subspace which is equal to its own [[orthogonal complement]]; equivalently, the index of isotropy is equal to half the dimension.<ref name=MH>{{cite book | first1=J. | last1=Milnor | author1-link=John Milnor| first2=D. | last2=Husemoller | title=Symmetric Bilinear Forms | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 }}</ref>{{rp|57}} The hyperbolic plane is an example, and over a field of characteristic not equal to 2, every split space is a direct sum of hyperbolic planes.<ref name=MH/>{{rp|12,3}}\n\n== Relation with classification of quadratic forms ==\n\nFrom the point of view of classification of quadratic forms, anisotropic spaces are the basic building blocks for quadratic spaces of arbitrary dimensions. For a general field ''F'', classification of anisotropic quadratic forms is a nontrivial problem. By contrast, the isotropic forms are usually much easier to handle.  By [[Witt's decomposition theorem]], every inner product space over a field is an [[orthogonal direct sum]] of a split space and an anisotropic space.<ref name=MH/>{{rp|56}}\n\n==Field theory==\n* If ''F'' is an [[algebraically closed]] field,  for example, the field of [[complex numbers]], and {{nowrap|(''V'', ''q'')}} is a quadratic space of dimension at least two, then it is isotropic.\n* If ''F'' is a [[finite field]] and {{nowrap|(''V'', ''q'')}} is a quadratic space of dimension at least three, then it is isotropic (this is a consequence of the [[Chevalley–Warning_theorem#Statement_of_the_theorems|Chevalley-Warning theorem]]).\n* If ''F'' is the field ''Q''<sub>''p''</sub> of [[p-adic number|''p''-adic number]]s and {{nowrap|(''V'', ''q'')}} is a quadratic space of dimension at least five, then it is isotropic.\n\n== See also ==\n*[[Polar space]]\n*[[Witt group]]\n*[[Witt ring (forms)]]\n*[[Universal quadratic form]]\n\n== References ==\n{{reflist}}\n* Pete L. Clark, [http://www.math.miami.edu/~armstrong/685fa12/pete_clark.pdf Quadratic forms chapter I: Witts theory] from [[University of Miami]] in [[Coral Gables, Florida]].\n* [[Tsit Yuen Lam]] (1973) ''Algebraic Theory of Quadratic Forms'', §1.3 Hyperbolic plane and hyperbolic spaces, [[W. A. Benjamin]].\n* Tsit Yuen Lam (2005) ''Introduction to Quadratic Forms over Fields'', [[American Mathematical Society]] {{ISBN|0-8218-1095-2}} .\n* {{cite book | first=O.T | last=O'Meara | authorlink=O. Timothy O'Meara | year=1963 | title=Introduction to Quadratic Forms | page=94 §42D Isotropy | publisher=[[Springer-Verlag]] | isbn=3-540-66564-1 }}\n* {{cite book | first=Jean-Pierre | last=Serre | authorlink=Jean-Pierre Serre | title=A Course in Arithmetic | volume=7 | publisher=[[Springer-Verlag]] | year=2000 | origyear=1973 | edition=reprint of 3rd | series=[[Graduate Texts in Mathematics]]: Classics in mathematics | isbn=0-387-90040-3 | zbl=1034.11003  }}\n\n[[Category:Quadratic forms]]\n[[Category:Bilinear forms]]"
    },
    {
      "title": "Nondegenerate form",
      "url": "https://en.wikipedia.org/wiki/Nondegenerate_form",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Generalization of inner products}}\n{{Unreferenced|date=September 2016}}\nIn [[linear algebra]], a '''nondegenerate form''' or '''nonsingular form''' is a [[bilinear form]] that is not degenerate, meaning that <math>v \\mapsto (x \\mapsto f(x,v))</math>  is an [[isomorphism]], or equivalently in finite dimensions, if and only if\n:<math>f(x,y)=0 \\text{ for all } y \\in V \\text{ implies that } x = 0.</math>\n\n==Examples==\nThe most important examples of nondegenerate forms are [[inner product]]s and [[symplectic form]]s. Symmetric nondegenerate forms are important generalizations of inner products, in that often all that is required is that the map <math>V \\to V^*</math> be an isomorphism, not positivity. For example, a manifold with an inner product structure on its tangent spaces is a [[Riemannian manifold]], while relaxing this to a symmetric nondegenerate form yields a [[pseudo-Riemannian manifold]].\n\n==See also==\n*[[Degenerate form]]\n\n[[Category:Bilinear forms]]\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Symmetric bilinear form",
      "url": "https://en.wikipedia.org/wiki/Symmetric_bilinear_form",
      "text": "A '''symmetric bilinear form''' on a [[vector space]] is a [[bilinear map]] from two copies of the vector space to the field of [[Scalar (mathematics)|scalars]] such that the order of the two vectors does not affect the value of the map. In other words, it is a [[bilinear form|bilinear]] function <math>B</math> that maps every pair <math>(u,v)</math> of elements of the vector space <math>V</math> to the underlying field such that <math>B(u,v)=B(v,u)</math> for every <math>u</math> and <math>v</math> in <math>V</math>. They are also referred to more briefly as just '''symmetric forms''' when \"bilinear\" is understood.\n\nSymmetric bilinear forms on finite-dimensional vector spaces precisely correspond to [[symmetric matrix|symmetric matrices]] given a [[Basis (linear algebra)|basis]] for ''V''. Among bilinear forms, the symmetric ones are important because they are the ones for which the vector space admits a particularly simple kind of basis known as an orthogonal basis (at least when the characteristic of the field is not 2).\n\nGiven a symmetric bilinear form ''B'', the function {{nowrap|1=''q''(''x'') = ''B''(''x'', ''x'')}} is the associated [[quadratic form]] on the vector space. Moreover, if the characteristic of the field is not 2, ''B'' is the unique symmetric bilinear form associated with ''q''.\n\n== Formal definition ==\nLet '' V'' be a vector space of dimension ''n'' over a field ''K''.  A [[function (mathematics)|map]] <math>B : V\\times V\\rightarrow K</math> is a symmetric bilinear form on the space if:\n* <math>B(u,v)=B(v,u) \\ \\quad \\forall u,v \\in V</math>\n* <math>B(u+v,w)=B(u,w)+B(v,w)\\  \\quad \\forall u,v,w \\in V</math>\n* <math>B(\\lambda v,w)=\\lambda B(v,w)\\ \\quad \\forall \\lambda \\in K,\\forall v,w \\in V</math>\n\nThe last two axioms only imply linearity in the first argument, but the first axiom then immediately implies linearity in the second argument as well.\n\n== Examples ==\n\nLet {{nowrap|1=''V'' = '''R'''<sup>''n''</sup>}}, the ''n'' dimensional real vector space. Then the standard [[dot product]] is a symmetric bilinear form, {{nowrap|1=''B''(''x'', ''y'') = ''x'' ⋅ ''y''}}. The matrix corresponding to this bilinear form (see below) on a [[standard basis]] is the identity matrix.\n\nLet ''V'' be any vector space (including possibly infinite-dimensional), and assume ''T'' is a linear function from ''V'' to the field. Then the function defined by {{nowrap|1=''B''(''x'', ''y'') = ''T''(''x'')''T''(''y'')}} is a symmetric bilinear form.\n\nLet ''V'' be the vector space of continuous single-variable real functions. For <math>f,g \\in V</math> one can define <math>B(f,g)=\\int_0^1 f(t)g(t) dt</math>. By the properties of [[Integral|definite integrals]], this defines a symmetric bilinear form on ''V''. This is an example of a symmetric bilinear form which is not associated to any symmetric matrix (since the vector space is infinite-dimensional).\n\n==Matrix representation==\nLet <math>C=\\{e_{1},\\ldots,e_{n}\\}</math> be a basis for ''V''.  Define the {{nowrap|''n'' × ''n''}} matrix ''A'' by <math>A_{ij}=B(e_{i},e_{j})</math>.  The matrix ''A'' is a [[symmetric matrix]] exactly due to symmetry of the bilinear form.  If the ''n''×1 matrix ''x'' represents a vector ''v'' with respect to this basis, and analogously, ''y'' represents ''w'', then <math>B(v,w)</math> is given by :\n\n:<math>x^\\mathsf{T} A y=y^\\mathsf{T} A x.</math>\n\nSuppose '' C' '' is another basis for ''V'', with :\n<math>\\begin{bmatrix}e'_{1} & \\cdots & e'_{n}\\end{bmatrix} = \\begin{bmatrix}e_{1} & \\cdots & e_{n}\\end{bmatrix}S</math>\nwith ''S'' an invertible ''n''×''n'' matrix.\nNow the new matrix representation for the symmetric bilinear form is given by\n\n:<math>A' =S^\\mathsf{T} A S .</math>\n\n==Orthogonality and singularity==\nA symmetric bilinear form is always [[Reflexive bilinear form|reflexive]].  Two vectors ''v'' and ''w'' are defined to be orthogonal with respect to the bilinear form ''B'' if {{nowrap|1=''B''(''v'', ''w'') = 0}}, which is, due to reflexivity, equivalent to {{nowrap|1=''B''(''w'', ''v'') = 0}}.\n\nThe '''radical''' of a bilinear form ''B'' is the set of vectors orthogonal with every vector in ''V''.  That this is a subspace of ''V'' follows from the linearity of ''B'' in each of its arguments.  When working with a matrix representation ''A'' with respect to a certain basis, ''v'', represented by ''x'', is in the radical if and only if\n\n:<math>A x = 0 \\Longleftrightarrow x^\\mathsf{T} A = 0 .</math>\n\nThe matrix ''A'' is singular if and only if the radical is nontrivial.\n\nIf ''W'' is a subset of ''V'', then its ''[[orthogonal complement]]'' ''W''<sup>⊥</sup> is the set of all vectors in ''V'' that are orthogonal to every vector in ''W''; it is a subspace of ''V''.  When ''B'' is non-degenerate, the radical of ''B'' is trivial and the dimension of ''W''<sup>⊥</sup> is {{nowrap|1=dim(''W''<sup>⊥</sup>) = dim(''V'') − dim(''W'')}}.\n\n==Orthogonal basis==\nA basis <math>C=\\{e_{1},\\ldots,e_{n}\\}</math> is orthogonal with respect to ''B'' if and only if :\n\n:<math>B(e_{i},e_{j}) = 0\\ \\forall i \\neq j.</math>\n\nWhen the [[characteristic (algebra)|characteristic]] of the field is not two, ''V'' always has an orthogonal basis.  This can be proven by [[mathematical induction|induction]].\n\nA basis ''C'' is orthogonal if and only if the matrix representation ''A'' is a [[diagonal matrix]].\n\n===Signature and Sylvester's law of inertia===\nIn a more general form, [[Sylvester's law of inertia]] says that, when working over an [[ordered field]], the numbers of diagonal elements in the diagonalized form of a matrix that are positive, negative and zero respectively are independent of the chosen orthogonal basis.  These three numbers form the ''[[signature (quadratic form)|signature]]'' of the bilinear form.\n\n===Real case===\nWhen working in a space over the reals, one can go a bit a further.  Let <math>C=\\{e_{1},\\ldots,e_{n}\\}</math> be an orthogonal basis.\n\nWe define a new basis <math>C'=\\{e'_1,\\ldots,e'_n\\}</math>\n\n:<math>\ne'_i = \\begin{cases}\ne_i & \\text{if } B(e_i,e_i)=0  \\\\\n\\frac{e_i}{\\sqrt{B(e_i,e_i)}} & \\text{if } B(e_i,e_i) >0\\\\\n\\frac{e_i}{\\sqrt{-B(e_i,e_i)}}& \\text{if } B(e_i,e_i) <0\n\\end{cases}\n</math>\n\nNow, the new matrix representation ''A'' will be a diagonal matrix with only 0, 1 and −1 on the diagonal.  Zeroes will appear if and only if the radical is nontrivial.\n\n===Complex case===\nWhen working in a space over the complex numbers, one can go further as well and it is even easier.\nLet <math>C=\\{e_1,\\ldots,e_n\\}</math> be an orthogonal basis.\n\nWe define a new basis <math>C'=\\{e'_1,\\ldots,e'_n\\}</math> :\n\n:<math>\ne'_i = \\begin{cases}\ne_i & \\text{if }\\; B(e_i,e_i)=0  \\\\\ne_i/\\sqrt{B(e_i,e_i)} & \\text{if }\\; B(e_i,e_i) \\neq 0\\\\\n\\end{cases}\n</math>\n\nNow the new matrix representation ''A'' will be a diagonal matrix with only 0 and 1 on the diagonal.  Zeroes will appear if and only if the radical is nontrivial.\n\n==Orthogonal polarities==\nLet ''B'' be a symmetric bilinear form with a trivial radical on the space ''V'' over the field ''K'' with [[characteristic (algebra)|characteristic]] not 2.  One can now define a map from D(''V''), the set of all subspaces of ''V'', to itself:\n\n:<math>\\alpha:D(V)\\rightarrow D(V) :W\\mapsto W^{\\perp}.</math>\n\nThis map is an '''orthogonal polarity''' on the [[projective space]] PG(''W'').  Conversely, one can prove all orthogonal polarities are induced in this way, and that two symmetric bilinear forms with trivial radical induce the same polarity if and only if they are equal up to scalar multiplication.\n\n==References==\n* {{cite book | last1=Adkins | first1=William A. | last2=Weintraub | first2=Steven H. | title=Algebra: An Approach via Module Theory | series=[[Graduate Texts in Mathematics]] | volume=136 | publisher=[[Springer-Verlag]] | year=1992 | isbn=3-540-97839-9 | zbl=0768.00003  }}\n* {{cite book | first1=J. | last1=Milnor | author1-link=John Milnor| first2=D. | last2=Husemoller | title=Symmetric Bilinear Forms | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 }}\n* {{MathWorld|title=Symmetric Bilinear Form|urlname=SymmetricBilinearForm}}\n\n{{DEFAULTSORT:Symmetric Bilinear Form}}\n[[Category:Bilinear forms]]"
    },
    {
      "title": "Symplectic vector space",
      "url": "https://en.wikipedia.org/wiki/Symplectic_vector_space",
      "text": "In [[mathematics]], a '''symplectic vector space''' is a [[vector space]] ''V'' over a [[Field (mathematics)|field]] ''F'' (for example the real numbers '''R''') equipped with a symplectic [[bilinear form]].\n\nA '''symplectic bilinear form''' is a [[map (mathematics)|mapping]] {{nowrap|''ω'' : ''V'' × ''V'' → ''F''}} that is\n* ''[[bilinear form|bilinear]]'': [[linear map|linear]] in each argument separately,\n* ''[[alternating form|alternating]]'': {{nowrap|1=''ω''(''v'', ''v'') = 0}} holds for all {{nowrap|''v'' ∈ ''V''}}, and\n* ''[[Nondegenerate form|nondegenerate]]'': {{nowrap|1=''ω''(''u'', ''v'') = 0}} for all {{nowrap|''v'' ∈ ''V''}} implies that {{nowrap|1=''u''}} is zero.\n\nIf the underlying [[Field (mathematics)|field]] has [[characteristic (algebra)|characteristic]] not 2, alternation is equivalent to [[skew symmetry|skew-symmetry]]. If the characteristic is 2, the skew-symmetry is implied by, but does not imply alternation. In this case every symplectic form is a [[symmetric bilinear form|symmetric form]], but not vice versa. Working in a fixed [[basis (linear algebra)|basis]], ''ω'' can be represented by a [[matrix (mathematics)|matrix]]. The conditions above say that this matrix must be [[skew-symmetric matrix|skew-symmetric]], [[nonsingular matrix|nonsingular]], and [[Hollow matrix#Diagonal entries all zero|hollow]]. This is ''not'' the same thing as a [[symplectic matrix]], which represents a symplectic transformation of the space. If ''V'' is [[finite-dimensional]], then its dimension must necessarily be [[even number|even]] since every skew-symmetric, hollow matrix of odd size has [[determinant]] zero.  Notice the condition that the matrix be hollow is not redundant if the characteristic of the field is 2. A symplectic form behaves quite differently from a [[symmetric bilinear form|symmetric form]], for example, the scalar product on Euclidean vector spaces.\n\n==Standard symplectic space==\n{{Further|Symplectic matrix#Symplectic transformations}}\n\nThe standard symplectic space is '''R'''<sup>2''n''</sup> with the symplectic form given by a [[nonsingular matrix|nonsingular]], [[skew-symmetric matrix]]. Typically ''ω'' is chosen to be the [[block matrix]]\n\n:<math>\\omega = \\begin{bmatrix} 0 & I_n \\\\ -I_n & 0 \\end{bmatrix}</math>\n\nwhere ''I''<sub>''n''</sub> is the {{nowrap|''n'' × ''n''}} [[identity matrix]]. In terms of basis vectors {{nowrap|(''x''<sub>1</sub>, ..., ''x<sub>n</sub>'', ''y''<sub>1</sub>, ..., ''y<sub>n</sub>'')}}:\n\n:<math>\\omega(x_i, y_j) = -\\omega(y_j, x_i) = \\delta_{ij}\\,</math>\n:<math>\\omega(x_i, x_j) = \\omega(y_i, y_j) = 0.\\,</math>\n\nA modified version of the [[Gram–Schmidt process]] shows that any finite-dimensional symplectic vector space has a basis such that ''ω'' takes this form, often called a '''''Darboux basis''''', or [[symplectic basis]].\n\nThere is another way to interpret this standard symplectic form. Since the model space '''R'''<sup>''2n''</sup> used above carries much canonical structure which might easily lead to misinterpretation, we will use \"anonymous\" vector spaces instead. Let ''V'' be a real vector space of dimension ''n'' and ''V''<sup>∗</sup> its [[dual space]]. Now consider the [[direct sum of vector spaces|direct sum]] {{nowrap|1=''W'' = ''V'' ⊕ ''V''<sup>∗</sup>}} of these spaces equipped with the following form:\n\n:<math>\\omega(x \\oplus \\eta, y \\oplus \\xi) = \\xi(x) - \\eta(y).</math>\n\nNow choose any [[Basis (linear algebra)|basis]] {{nowrap|(''v''<sub>1</sub>, ..., ''v''<sub>''n''</sub>)}} of ''V'' and consider its  [[dual space|dual basis]]\n\n:<math>(v^*_1, \\ldots, v^*_n).</math>\n\nWe can interpret the basis vectors as lying in ''W'' if we write {{nowrap|1=''x''<sub>''i''</sub> = (''v''<sub>''i''</sub>, 0) and ''y''<sub>''i''</sub> = (0, ''v''<sub>''i''</sub><sup>∗</sup>)}}. Taken together, these form a complete basis of ''W'',\n\n:<math>(x_1, \\ldots, x_n, y_1, \\ldots, y_n).</math>\n\nThe form ''ω'' defined here can be shown to have the same properties as in the beginning of this section. On the other hand, every symplectic structure is isomorphic to one of the form {{nowrap|''V'' ⊕ ''V''<sup>∗</sup>}}. The subspace ''V'' is not unique, and a choice of subspace ''V'' is called a '''polarization.''' The subspaces that give such an isomorphism are called '''Lagrangian subspaces''' or simply '''Lagrangians.'''\n\nExplicitly, given a Lagrangian subspace (as defined below), then a choice of basis {{nowrap|(''x''<sub>1</sub>, ..., ''x<sub>n</sub>'')}} defines a dual basis for a complement, by {{nowrap|1=''ω''(''x''<sub>''i''</sub>, ''y''<sub>''j''</sub>) = ''δ''<sub>''ij''</sub>}}.\n\n===Analogy with complex structures===\nJust as every symplectic structure is isomorphic to one of the form {{nowrap|''V'' ⊕ ''V''<sup>∗</sup>}}, every [[linear complex structure|''complex'' structure]] on a vector space is isomorphic to one of the form {{nowrap|''V'' ⊕ ''V''}}. Using these structures, the [[tangent bundle]] of an ''n''-manifold, considered as a 2''n''-manifold, has an [[almost complex structure]], and the [[cotangent bundle|''co''tangent bundle]] of an ''n''-manifold, considered as a 2''n''-manifold, has a symplectic structure: {{nowrap|1=''T''<sub>∗</sub>(''T''<sup>∗</sup>''M'')<sub>''p''</sub> = ''T''<sub>''p''</sub>(''M'') ⊕ (''T''<sub>''p''</sub>(''M''))<sup>∗</sup>}}.\n\nThe complex analog to a Lagrangian subspace is a [[real subspace|''real'' subspace]], a subspace whose [[complexification]] is the whole space: {{nowrap|1=''W'' = ''V'' ⊕ ''J'' ''V''}}. As can be seen from the standard symplectic form above, every symplectic form on <math>\\mathbb{R}^{2n}</math> is isomorphic to the imaginary part of the standard complex (Hermitian) inner product on <math>\\mathbb{C}^{n}</math> (with the convention of the first argument being anti-linear).\n\n==Volume form==\nLet ''ω'' be an [[alternating bilinear form]] on an ''n''-dimensional real vector space ''V'', {{nowrap|''ω'' ∈ Λ<sup>2</sup>(''V'')}}. Then ''ω'' is non-degenerate if and only if ''n'' is even and {{nowrap|1=''ω''<sup>''n''/2</sup> = ''ω'' ∧ ... ∧ ''ω''}} is a [[volume form]].  A volume form on a ''n''-dimensional vector space ''V'' is a non-zero multiple of the ''n''-form {{nowrap|''e''<sub>1</sub><sup>∗</sup> ∧ ... ∧ ''e''<sub>''n''</sub><sup>∗</sup>}} where {{nowrap|''e''<sub>1</sub>, ''e''<sub>2</sub>, ..., ''e''<sub>''n''</sub>}} is a basis of ''V''.\n\nFor the standard basis defined in the previous section, we have\n\n:<math>\\omega^n=(-1)^{n/2} x^*_1\\wedge\\ldots \\wedge x^*_n \\wedge y^*_1\\wedge \\ldots \\wedge y^*_n.</math>\n\nBy reordering, one can write\n\n:<math>\\omega^n= x^*_1\\wedge y^*_1\\wedge \\ldots \\wedge x^*_n \\wedge y^*_n.</math>\n\nAuthors variously define ''ω''<sup>''n''</sup> or (−1)<sup>''n''/2</sup>''ω''<sup>''n''</sup> as the '''standard volume form'''. An occasional factor of ''n''! may also appear, depending on whether the definition of the [[alternating product]] contains a factor of ''n''! or not.  The volume form defines an [[orientation (mathematics)|orientation]] on the symplectic vector space {{nowrap|(''V'', ''ω'')}}.\n\n==Symplectic map==\nSuppose that {{nowrap|(''V'', ''ω'')}} and {{nowrap|(''W'', ''ρ'')}} are symplectic vector spaces. Then a [[linear map]] {{nowrap|1=''f'' : ''V'' → ''W''}} is called a '''symplectic map''' if the [[pullback (differential geometry)|pullback]] preserves the symplectic form, i.e. {{nowrap|1=''f''<sup>∗</sup>''ρ'' = ''ω''}}, where the pullback form is defined by {{nowrap|1=(''f''<sup>∗</sup>''ρ'')(''u'', ''v'') = ''ρ''(''f''(''u''), ''f''(''v''))}}. Symplectic maps are volume- and orientation-preserving.\n\n==Symplectic group==\nIf {{nowrap|1=''V'' = ''W''}}, then a symplectic map is called a '''linear symplectic transformation''' of ''V''. In particular, in this case one has that {{nowrap|1=''ω''(''f''(''u''), ''f''(''v'')) = ''ω''(''u'', ''v'')}}, and so the [[linear transformation]] ''f'' preserves the symplectic form.  The set of all symplectic transformations forms a [[group (mathematics)|group]] and in particular a [[Lie group]], called the [[symplectic group]] and denoted by Sp(''V'') or sometimes {{nowrap|Sp(''V'', ''ω'')}}.  In matrix form symplectic transformations are given by [[symplectic matrix|symplectic matrices]].\n\n==Subspaces==\nLet ''W'' be a [[linear subspace]] of ''V''. Define the '''symplectic complement''' of ''W'' to be the subspace\n:<math>W^\\perp = \\{v\\in V \\mid \\omega(v,w) = 0 \\mbox{ for all } w\\in W\\}.</math>\nThe symplectic complement satisfies:\n\n:<math>(W^\\perp)^\\perp = W</math>\n:<math>\\dim W + \\dim W^\\perp = \\dim V.</math>\n\nHowever, unlike [[orthogonal complement]]s, ''W''<sup>⊥</sup> ∩ ''W'' need not be 0. We distinguish four cases:\n\n*''W'' is '''symplectic''' if {{nowrap|1=''W''<sup>⊥</sup> ∩ ''W'' = {0}}}. This is true [[if and only if]] ''ω'' restricts to a nondegenerate form on ''W''. A symplectic subspace with the restricted form is a symplectic vector space in its own right.\n*''W'' is '''isotropic''' if {{nowrap|''W'' ⊆ ''W''<sup>⊥</sup>}}. This is true if and only if ''ω'' restricts to 0 on ''W''. Any one-dimensional subspace is isotropic.\n*''W'' is '''coisotropic''' if {{nowrap|''W''<sup>⊥</sup> ⊆ ''W''}}. ''W'' is coisotropic if and only if ''ω'' descends to a nondegenerate form on the [[Quotient space (linear algebra)|quotient space]] ''W''/''W''<sup>⊥</sup>. Equivalently ''W'' is coisotropic if and only if ''W''<sup>⊥</sup> is isotropic. Any [[codimension]]-one subspace is coisotropic.\n*''W'' is '''Lagrangian''' if {{nowrap|1=''W'' = ''W''<sup>⊥</sup>}}. A subspace is Lagrangian if and only if it is both isotropic and coisotropic. In a finite-dimensional vector space, a Lagrangian subspace is an isotropic one whose dimension is half that of ''V''. Every isotropic subspace can be extended to a Lagrangian one.\n\nReferring to the canonical vector space '''R'''<sup>2''n''</sup> above,\n* the subspace spanned by {''x''<sub>1</sub>, ''y''<sub>1</sub>} is symplectic\n* the subspace spanned by {''x''<sub>1</sub>, ''x''<sub>2</sub>} is isotropic\n* the subspace spanned by {''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x''<sub>''n''</sub>, ''y''<sub>1</sub>} is coisotropic\n* the subspace spanned by {''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x''<sub>''n''</sub>} is Lagrangian.\n\n==Heisenberg group==\n{{main|Heisenberg group}}\nA [[Heisenberg group]] can be defined for any symplectic vector space, and this is the typical way that [[Heisenberg group]]s arise.\n\nA vector space can be thought of as a commutative Lie group (under addition), or equivalently as a commutative [[Lie algebra]], meaning with trivial Lie bracket. The Heisenberg group is a [[central extension (mathematics)|central extension]] of such a commutative Lie group/algebra: the symplectic form defines the commutation, analogously to the [[canonical commutation relation]]s (CCR), and a Darboux basis corresponds to [[canonical coordinate]]s&nbsp;– in physics terms, to [[momentum operator]]s and [[position operator]]s.\n\nIndeed, by the [[Stone–von Neumann theorem]], every representation satisfying the CCR (every representation of the Heisenberg group) is of this form, or more properly unitarily conjugate to the standard one.\n\nFurther, the [[group algebra]] of (the dual to) a vector space is the [[symmetric algebra]], and the group algebra of the Heisenberg group (of the dual) is the [[Weyl algebra]]: one can think of the central extension as corresponding to quantization or [[Deformation quantization|deformation]].\n\nFormally, the symmetric algebra of ''V'' is the group algebra of the dual, {{nowrap|1=Sym(''V'') := ''K''[''V''<sup>∗</sup>]}}, and the Weyl algebra is the group algebra of the (dual) Heisenberg group {{nowrap|1=''W''(''V'') = ''K''[''H''(''V''<sup>∗</sup>)]}}. Since passing to group algebras is a [[contravariant functor]], the  central extension map {{nowrap|''H''(''V'') → ''V''}} becomes an inclusion {{nowrap|Sym(''V'') → ''W''(''V'')}}.\n\n==See also==\n\n* A [[symplectic manifold]] is a [[smooth manifold]] with a smoothly-varying ''closed'' symplectic form on each [[tangent space]]\n* [[Maslov index]]\n* A [[symplectic representation]] is a [[group representation]] where each group element acts as a symplectic transformation.\n\n==References==\n*{{cite book |authorlink=Ralph Abraham (mathematician) |first=Ralph |last=Abraham |first2=Jerrold E. |last2=Marsden |authorlink2=Jerrold E. Marsden |title=Foundations of Mechanics |year=1978 |publisher=Benjamin-Cummings |location=London |isbn=0-8053-0102-X |chapter=Hamiltonian and Lagrangian Systems |pages=161–252 }}\n\n[[Category:Linear algebra]]\n[[Category:Symplectic geometry]]\n[[Category:Bilinear forms]]"
    },
    {
      "title": "15 and 290 theorems",
      "url": "https://en.wikipedia.org/wiki/15_and_290_theorems",
      "text": "The '''15 theorem''' or '''Conway–Schneeberger Fifteen Theorem''', proved by [[John H. Conway]] and W. A. Schneeberger in 1993, states that if a positive [[definite quadratic form]] with [[integer matrix]] represents all [[positive integer]]s up to [[15 (number)|15]], then it represents all positive integers.<ref>{{cite book | authorlink=John Horton Conway | last=Conway | first=J.H. | url=http://www.fen.bilkent.edu.tr/~franz/mat/15.pdf | chapter=Universal quadratic forms and the fifteen theorem | title=Quadratic forms and their applications (Dublin, 1999) | pages=23–26 | series=Contemp. Math. | volume=272 | publisher=Amer. Math. Soc. | location=Providence, RI | year=2000 | isbn=0-8218-2779-0 | zbl=0987.11026}}</ref> The proof was complicated, and was never published. [[Manjul Bhargava]] found a much simpler proof which was published in 2000.<ref>{{cite book | last=Bhargava | first=Manjul | authorlink=Manjul Bhargava | url=http://www.maths.ed.ac.uk/~aar/books/dublin.pdf | chapter=On the Conway–Schneeberger fifteen theorem | title=Quadratic forms and their applications (Dublin, 1999) | pages=27–37 | series=Contemp. Math. | volume=272 | publisher=Amer. Math. Soc. | location=Providence, RI | year=2000 | isbn=0-8218-2779-0 | zbl=0987.11027|mr=1803359}}</ref>\n\nIn 2005, Bhargava and Jonathan P. Hanke announced a proof of Conway's conjecture that a similar [[theorem]] holds for [[integral quadratic form|integral]] quadratic forms, with the constant 15 replaced by [[290 (number)|290]]. The proof is to appear in ''Inventiones Mathematicae''.<ref>Bhargava, M., & Hanke, J., [http://www.wordpress.jonhanke.com/wp-content/uploads/2011/09/290-Theorem-preprint.pdf Universal quadratic forms and the 290-theorem]. ''Invent. Math.'', to appear.</ref>\n\n== Details ==\n\nIn a simple term, the results are as follows. Suppose <math>Q_{ij}</math> is a symmetric square matrix with real entries. For any vector <math>x</math> with integer components, define\n\n:<math>Q(x) = \\sum_{i,j} Q_{ij} x_i x_j</math>\n\nThis function is called a '''quadratic form'''. We say <math>Q</math> is '''positive definite''' if <math>Q(x) > 0 </math> whenever <math>x \\ne 0</math>. If <math>Q(x)</math> is always an integer, we call the function <math>Q</math> an '''integral quadratic form'''.\n\nWe get an integral quadratic form whenever the matrix entries <math>Q_{ij}</math> are integers; then <math>Q</math> is said to have '''integer matrix'''. However, <math>Q</math> will still be an integral quadratic form if the off-diagonal entries <math>Q_{ij}</math> are integers divided by 2, while the diagonal entries are integers. For example, ''x''<sup>2</sup> + ''xy'' + ''y''<sup>2</sup> is integral but does not have integral matrix.\n\nA positive integral quadratic form taking all positive integers as values is called '''[[Integral quadratic form#Universal quadratic forms|universal]]'''. The '''15 theorem''' says that a quadratic form with integer matrix is universal if it takes the numbers from 1 to 15 as values. A more precise version says that, if a positive definite quadratic form with integral matrix takes the values 1, 2, 3, 5, 6, 7, 10, 14, 15 {{OEIS|id=A030050}}, then it takes all positive integers as values. Moreover, for each of these 9 numbers, there is such a quadratic form taking all other 8 positive integers except for this number as values.\n\nFor example, the quadratic form\n\n:<math>w^2 + x^2 + y^2 + z^2</math>\n\nis universal, because every positive integer can be written as a sum of 4 squares, by [[Lagrange's four-square theorem]]. By the 15 theorem, to verify this, it is sufficient to check that every positive integer up to 15 is a sum of 4 squares. (This does not give an alternative proof of Lagrange's theorem, because Lagrange's theorem is used in the proof of the 15 theorem.)\n\nOn the other hand,\n\n:<math>w^2 + 2x^2 + 5y^2 + 5z^2,</math>\n\nis a positive definite quadratic form with integral matrix that takes as values all positive integers other than 15.\n\nThe '''290 theorem''' says a positive definite integral quadratic form is universal if it takes the numbers from 1 to 290 as values. A more precise version states that, if an integer valued integral quadratic form represents all the numbers 1, 2, 3, 5, 6, 7, 10, 13, 14, 15, 17, 19, 21, 22, 23, 26, 29, 30, 31, 34, 35, 37, 42, 58, 93, 110, 145, 203, 290 {{OEIS|id=A030051}}, then it represents all positive integers, and for each of these 29 numbers, there is such a quadratic form representing all other 28 positive integers with the exception of this one number.\n\nBhargava has found analogous criteria for a quadratic form with integral matrix to represent all primes (the set {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 67, 73} {{OEIS|id=A154363}}) and for such a quadratic form to represent all positive odd integers (the set {1, 3, 5, 7, 11, 15, 33} {{OEIS|id=A116582}}).\n\nExpository accounts of these result have been written by Hahn<ref>Alexander J. Hahn, [https://math.nd.edu/assets/20630/hahntoulouse.pdf Quadratic Forms over <math>\\mathbb{Z}</math> from Diophantus to the 290 Theorem], Advances in Applied Clifford Algebras, 2008, Volume 18, Issue 3-4, 665-676</ref> and Moon<ref>Yong Suk Moon, [https://web.archive.org/web/20140814082644/https://math.stanford.edu/theses/moon.pdf Universal quadratic forms and the 15-theorem and 290-theorem]</ref> (who provides proofs).\n\n==References==\n\n{{reflist}}\n\n[[Category:Additive number theory]]\n[[Category:Theorems in number theory]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Arf invariant",
      "url": "https://en.wikipedia.org/wiki/Arf_invariant",
      "text": "[[Image:10 Türk Lirası reverse.jpg|thumb|300px|right|Arf and a formula for the Arf invariant appear on the reverse side of the [[Turkish lira|2009 Turkish 10 Lira note]]]]\n\nIn [[mathematics]], the '''Arf invariant''' of a nonsingular [[quadratic form]] over a [[field (mathematics)|field]] of [[characteristic (algebra)|characteristic]] 2 was defined by [[Turkey|Turkish]] [[mathematician]] {{harvs|txt|authorlink=Cahit Arf|first=Cahit|last=Arf|year= 1941}} when he started the systematic study of quadratic forms over arbitrary fields of characteristic 2. The Arf invariant is the substitute, in characteristic 2, for the [[Discriminant#Discriminant of a quadratic form|discriminant for quadratic forms]] in characteristic not 2. Arf used his invariant, among others, in his endeavor to classify quadratic forms in characteristic 2.\n\nIn the special case of the 2-element field [[GF(2)|'''F'''<sub>2</sub>]] the Arf invariant can be described as the element of '''F'''<sub>2</sub> that occurs most often among the values of the form. Two nonsingular quadratic forms over '''F'''<sub>2</sub> are isomorphic if and only if they have the same dimension and the same Arf invariant. This fact was essentially known to {{harvs|txt|last=Dickson|first=Leonard|Leonard Eugene Dickson|year=1901}}, even for any finite field of characteristic 2, and Arf proved it for an arbitrary [[perfect field]]. An assessment of Arf's results in the framework of the theory of quadratic forms can be found in.<ref>Falko Lorenz and Peter Roquette. Cahit Arf and his invariant.</ref>\n\nThe Arf invariant is particularly [[#The Arf invariant in topology|applied]] in [[geometric topology]], where it is primarily used to define an invariant of {{nowrap|(4''k'' + 2)}}-dimensional manifolds ([[singly even]]-dimensional [[manifold]]s: surfaces (2-manifolds), 6-manifolds, 10-manifolds, etc.) with certain additional structure called a [[framed manifold|framing]], and thus the [[Arf–Kervaire invariant]] and the [[Arf invariant of a knot]]. The Arf invariant is analogous to the [[signature of a manifold]], which is defined for 4''k''-dimensional manifolds ([[doubly even]]-dimensional); this 4-fold periodicity corresponds to the 4-fold periodicity of [[L-theory]]. The Arf invariant can also be defined more generally for certain 2''k''-dimensional manifolds.\n\n==Definitions==\n\nThe Arf invariant is defined for a [[quadratic form]] ''q'' over a field ''K'' of characteristic 2 such that ''q'' is nonsingular, in the sense that the associated bilinear form <math>b(u,v)=q(u+v)-q(u)-q(v)</math> is [[nondegenerate bilinear form|nondegenerate]]. The form <math>b</math> is [[bilinear form#Symmetric, skew-symmetric and alternating forms|alternating]] since ''K'' has characteristic 2; it follows that a nonsingular quadratic form in characteristic 2 must have even dimension. Any binary (2-dimensional) nonsingular quadratic form over ''K'' is equivalent to a form <math>q(x,y)= ax^2 + xy +by^2</math> with <math>a, b</math> in ''K''. The Arf invariant is defined to be the product <math>ab</math>. If the form <math>q'(x,y)=a'x^2 + xy+b'y^2</math> is equivalent to <math>q(x,y)</math>, then the products <math>ab</math> and <math>a'b'</math> differ by an element of the form <math>u^2+u </math> with <math>u</math> in ''K''. These elements form an additive subgroup ''U'' of ''K''. Hence the coset of <math>ab</math> modulo U is an invariant of <math>q</math>, which means that it is not changed when <math>q</math> is replaced by an equivalent form.\n\nEvery nonsingular quadratic form <math>q</math> over ''K'' is equivalent to a direct sum <math>q = q_1 + \\ldots + q_r</math> of nonsingular binary forms. This was shown by Arf, but it had been earlier observed by Dickson in the case of finite fields of characteristic 2. The Arf invariant Arf(<math>q</math>) is defined to be the sum of the Arf invariants of the <math>q_i</math>. By definition, this is a coset of K modulo U. Arf<ref>Arf (1941)</ref> showed that indeed Arf(<math>q</math>) does not change if <math>q</math> is replaced by an equivalent quadratic form, which is to say that it is an invariant of <math>q</math>.\n\nThe Arf invariant is additive; in other words, the Arf invariant of an orthogonal sum of two quadratic forms is the sum of their Arf invariants.\n\nFor a field ''K'' of characteristic 2, [[Artin-Schreier theory]] identifies the quotient group of ''K'' by the subgroup ''U'' above with the [[Galois cohomology]] group ''H''<sup>1</sup>(''K'', '''F'''<sub>2</sub>). In other words, the nonzero elements of ''K''/''U'' are in one-to-one correspondence with the [[separable extension|separable]] quadratic extension fields of ''K''. So the Arf invariant of a nonsingular quadratic form over ''K'' is either zero or it describes a separable quadratic extension field of ''K''. This is analogous to the discriminant of a nonsingular quadratic form over a field ''F'' of characteristic not 2. In that case, the discriminant takes values in ''F''<sup>*</sup>/(''F''<sup>*</sup>)<sup>2</sup>, which can be identified with ''H''<sup>1</sup>(''F'', '''F'''<sub>2</sub>) by [[Kummer theory]].\n\n==Arf's main results==\n\nIf the field ''K'' is perfect, then every nonsingular quadratic form over ''K'' is uniquely determined (up to equivalence) by its dimension and its Arf invariant. In particular, this holds over the field '''F'''<sub>2</sub>. In this case, the subgroup ''U'' above is zero, and hence the Arf invariant is an element of the base field '''F'''<sub>2</sub>; it is either 0 or 1.\n\nIf the field ''K'' of characteristic 2 is not perfect (that is, ''K'' is different from its subfield ''K''<sup>2</sup> of squares), then the [[Clifford algebra]] is another important invariant of a quadratic form. A corrected version of Arf's original statement is that if the [[degree of a field extension|degree]] [''K'': ''K''<sup>2</sup>] is at most 2, then every quadratic form over ''K'' is completely characterized by its dimension, its Arf invariant and its Clifford algebra.<ref>Falko Lorenz and Peter Roquette. Cahit Arf and his invariant. Section 9.</ref> Examples of such fields are [[algebraic function field|function fields]] (or [[formal power series#formal Laurent series|power series fields]]) of one variable over perfect base fields.\n\n==Quadratic forms over F<sub>2</sub>==\n\nOver '''F'''<sub>2</sub>, the Arf invariant is 0 if the quadratic form is equivalent to a direct sum of copies of the binary form <math>xy</math>, and it is 1 if the form is a direct sum of <math>x^2+xy+y^2</math> with a number of copies of <math>xy</math>.\n\n[[William Browder (mathematician)|William Browder]] has called the Arf invariant the ''democratic invariant''<ref>Martino and Priddy, p.61</ref> because it is the value which is assumed most often by the quadratic form.<ref>Browder, Proposition III.1.8</ref> Another characterization: ''q'' has Arf invariant 0 if and only if the underlying 2''k''-dimensional vector space over the field '''F'''<sub>2</sub> has a ''k''-dimensional subspace on which ''q'' is identically 0 – that is, a [[totally isotropic]] subspace of half the dimension. In other words, a nonsingular quadratic form of dimension 2''k'' has Arf invariant 0 if and only if its [[isotropy index]] is ''k'' (this is the maximum dimension of a totally isotropic subspace of a nonsingular form).\n\n==The Arf invariant in topology==\n{{technical|date=August 2016}}\n\nLet ''M'' be a [[compact space|compact]], [[connected space|connected]] ''2k''-dimensional [[manifold]] with a boundary <math>\\partial M</math>\nsuch that the induced morphisms in <math>\\Z_2</math>-coefficient homology\n\n:<math>H_k(M,\\partial M;\\Z_2) \\to H_{k-1}(\\partial M;\\Z_2), \\quad H_k(\\partial M;\\Z_2) \\to H_k(M;\\Z_2)</math>\n\nare both zero (e.g. if <math>M</math> is closed). The [[intersection theory|intersection form]] \n\n:<math>\\lambda : H_k(M;\\Z_2)\\times H_k(M;\\Z_2)\\to \\Z_2</math>\n\nis non-singular. (Topologists usually write '''F'''<sub>2</sub> as <math>\\Z_2</math>.) A [[quadratic refinement]] for <math> \\lambda</math> is a function <math>\\mu : H_k(M;\\Z_2) \\to \\Z_2</math> which satisfies\n\n:<math>\\mu(x+y) + \\mu(x) + \\mu(y) \\equiv \\lambda(x,y) \\pmod 2 \\; \\forall \\,x,y \\in H_k(M;\\Z_2)</math>\n\nLet <math>\\{x,y\\}</math> be any 2-dimensional subspace of <math>H_k(M;\\Z_2)</math>, such that <math>\\lambda(x,y) = 1</math>. Then there are two possibilities. Either all of <math>\\mu(x+y), \\mu(x), \\mu(y)</math> are 1, or else just one of them is 1, and the other two are 0. Call the first case <math>H^{1,1}</math>, and the second case <math>H^{0,0}</math>. Since every form is equivalent to a symplectic form, we can always find subspaces <math>\\{x,y\\}</math> with ''x'' and ''y'' being <math>\\lambda</math>-dual. We can therefore split <math>H_k(M;\\Z_2)</math> into a direct sum of subspaces isomorphic to either <math>H^{0,0}</math> or <math>H^{1,1}</math>. Furthermore, by a clever change of basis, <math>H^{0,0} \\oplus H^{0,0} \\cong H^{1,1} \\oplus H^{1,1}.</math> We therefore define the Arf invariant \n\n:<math>Arf(H_k(M;\\Z_2);\\mu)</math> = (number of copies of <math>H^{1,1}</math> in a decomposition Mod 2) <math> \\in \\Z_2</math>.\n\n===Examples===\n\n* Let <math>M</math> be a compact, connected, [[oriented]] ''2''-dimensional [[manifold]], i.e. a [[Surface (topology)|surface]], of [[genus]] <math>g</math> such that the boundary <math>\\partial M</math> is either empty or is connected. [[Whitney embedding theorem|Embed]] <math>M</math> in <math>S^m</math>, where <math>m \\geq 4</math>. Choose a framing of ''M'', that is a trivialization of the normal ''(m-2)''-plane [[vector bundle]]. (This is possible for <math>m =3</math>, so is certainly possible for <math>m \\geq 4</math>). Choose a [[Symplectic vector space|symplectic basis]] <math>x_1, x_2, \\ldots, x_{2g-1},x_{2g}</math> for <math>H_1(M)=\\Z^{2g}</math>. Each basis element is represented by an embedded circle <math>x_i:S^1 \\subset M</math>. The normal ''(m-1)''-plane [[vector bundle]] of <math>S^1 \\subset M \\subset S^m</math> has two trivializations, one determined by a standard [[Parallelizable|framing]] of a standard embedding <math>S^1 \\subset S^m</math> and one determined by the framing of ''M'', which differ by a map <math>S^1 \\to SO(m-1)</math> i.e. an element of <math>\\pi_1(SO(m-1)) \\cong \\Z_2</math> for <math>m \\geq 4</math>. This can also be viewed as the framed cobordism class of <math>S^1</math> with this framing in the 1-dimensional framed cobordism group <math>\\Omega^{framed}_1 \\cong \\pi_m(S^{m-1}) \\, (m \\geq 4) \\cong \\Z_2</math>, which is generated by the circle <math>S^1</math> with the Lie group framing. The isomorphism here is via the [[Pontrjagin-Thom construction]]. Define <math>\\mu(x)\\in \\Z_2</math> to be this element. The Arf invariant of the framed surface is now defined\n\n::<math> \\Phi(M) = Arf(H_1(M,\\partial M;\\Z_2);\\mu) \\in \\Z_2 </math>\n\n:Note that <math>\\pi_1(SO(2)) \\cong \\Z,</math> so we had to stabilise, taking <math>m</math> to be at least 4, in order to get an element of <math>\\Z_2</math>. The case <math>m=3</math> is also admissible as long as we take the residue modulo 2 of the framing.\n\n* The Arf invariant <math>\\Phi(M)</math> of a framed surface detects whether there is a 3-manifold whose boundary is the given surface which extends the given framing. This is because <math>H^{1,1}</math> does not bound. <math>H^{1,1}</math> represents a torus <math>T^2</math> with a trivialisation on both generators of <math>H_1(T^2;\\Z_2)</math> which twists an odd number of times. The key fact is that up to homotopy there are two choices of trivialisation of a trivial 3-plane bundle over a circle, corresponding to the two elements of <math>\\pi_1(SO(3))</math>. An odd number of twists, known as the Lie group framing, does not extend across a disc, whilst an even number of twists does. (Note that this corresponds to putting a [[spin structure]] on our surface.) [[Lev Pontryagin|Pontrjagin]] used the Arf invariant of framed surfaces to compute the 2-dimensional framed [[cobordism]] group <math>\\Omega^{framed}_2 \\cong \\pi_m(S^{m-2}) \\, (m \\geq 4) \\cong \\Z_2</math>, which is generated by the [[torus]] <math>T^2</math> with the Lie group framing. The isomorphism here is via the [[Homotopy groups of spheres#Framed cobordism|Pontrjagin-Thom construction]].\n\n* Let <math>(M^2,\\partial M) \\subset S^3</math> be a [[Seifert surface]] for a knot, <math>\\partial M = K : S^1 \\hookrightarrow S^3</math>, which can be represented as a disc <math>D^2</math> with bands attached. The bands will typically be twisted and knotted. Each band corresponds to a generator <math>x \\in H_1(M;\\Z_2)</math>. <math>x</math> can be represented by a circle which traverses one of the bands. Define <math>\\mu(x)</math> to be the number of full twists in the band modulo 2. Suppose we let <math>S^3</math> bound <math>D^4</math>, and push the Seifert surface <math>M</math> into <math>D^4</math>, so that its boundary still resides in <math>S^3</math>. Around any generator <math>x \\in H_1(M,\\partial M)</math>, we now have a trivial normal 3-plane vector bundle. Trivialise it using the trivial framing of the normal bundle to the embedding <math>M \\hookrightarrow D^4</math> for 2 of the sections required. For the third, choose a section which remains normal to <math>x</math>, whilst always remaining tangent to <math>M</math>. This trivialisation again determines an element of <math>\\pi_1(SO(3))</math>, which we take to be <math>\\mu(x)</math>. Note that this coincides with the previous definition of <math>\\mu</math>.\n\n* The [[Arf invariant (knot)|Arf invariant of a knot]] is defined via its Seifert surface. It is independent of the choice of Seifert surface (The basic surgery change of S-equivalence, adding/removing a tube, adds/deletes a <math>H^{0,0}</math> direct summand), and so is a [[knot invariant]]. It is additive under [[connected sum]], and vanishes on [[slice knot]]s, so is a [[Link concordance|knot concordance]] invariant.\n\n* The [[Intersection form (4-manifold)|intersection form]] on the {{nowrap|(2''k'' + 1)}}-dimensional <math>\\Z_2</math>-coefficient homology <math>H_{2k+1}(M;\\Z_2)</math> of a [[parallelizable|framed]] {{nowrap|(4''k'' + 2)}}-dimensional manifold ''M'' has a quadratic refinement <math>\\mu</math>, which depends on the framing. For <math>k \\neq 0,1,3</math> and <math>x \\in H_{2k+1}(M;\\Z_2)</math> represented by an [[embedding]] <math>x:S^{2k+1}\\subset M</math> the value <math>\\mu(x)\\in \\Z_2</math> is 0 or 1, according as to the normal bundle of <math>x</math> is trivial or not. The [[Kervaire invariant]] of the framed {{nowrap|(4''k'' + 2)}}-dimensional manifold ''M'' is the Arf invariant of the quadratic refinement <math>\\mu</math> on <math>H_{2k+1}(M;\\Z_2)</math>. The Kervaire invariant is a homomorphism <math>\\pi_{4k+2}^S \\to \\Z_2</math> on the {{nowrap|(4''k'' + 2)}}-dimensional stable homotopy group of spheres. The Kervaire invariant can also be defined for a {{nowrap|(4''k'' + 2)}}-dimensional manifold ''M'' which is framed except at a point.\n\n* In [[surgery theory]], for any <math>4k+2</math>-dimensional normal map <math>(f,b):M \\to X</math> there is defined a nonsingular quadratic form <math>(K_{2k+1}(M;\\Z_2),\\mu)</math> on the <math>\\Z_2</math>-coefficient homology kernel\n\n::<math>K_{2k+1}(M;\\Z_2)=ker(f_*:H_{2k+1}(M;\\Z_2)\\to H_{2k+1}(X;\\Z_2))</math> \n\n:refining the homological [[Intersection theory|intersection form]] <math>\\lambda</math>. The Arf invariant of this form is the [[Kervaire invariant]] of ''(f,b)''. In the special case <math>X=S^{4k+2}</math> this is the [[Kervaire invariant]] of ''M''. The Kervaire invariant features in the classification of [[exotic sphere]]s by [[Kervaire]] and [[Milnor]], and more generally in the classification of manifolds by [[surgery theory]]. [[William Browder (mathematician)|Browder]] defined <math>\\mu</math> using functional [[Steenrod square]]s, and [[C.T.C. Wall|Wall]] defined <math>\\mu</math> using framed [[Immersion (mathematics)|immersion]]s. The quadratic enhancement <math>\\mu(x)</math> crucially provides more information than <math>\\lambda(x,x)</math> : it is possible to kill ''x'' by surgery if and only if <math>\\mu(x)=0</math>. The corresponding Kervaire invariant detects the surgery obstruction of <math>(f,b)</math> in the [[L-theory|L-group]] <math>L_{4k+2}(\\Z)=\\Z_2</math>.\n\n==See also==\n* [[de Rham invariant]], a mod 2 invariant of <math>(4k + 1)</math>-dimensional manifolds\n\n==Notes==\n{{reflist}}\n\n==References==\n* See Lickorish (1997) for the relation between the Arf invariant and the [[Jones polynomial]].\n* See Chapter 3 of Carter's book for another equivalent definition of the Arf invariant in terms of self-intersections of discs in 4-dimensional space.\n*{{citation | first = Cahit|last= Arf | authorlink = Cahit Arf | title = Untersuchungen über quadratische Formen in Körpern der Charakteristik 2, I| journal = J. Reine Angew. Math.| volume = 183| year = 1941 | pages = 148–167}}\n*[[Glen Bredon]]: ''Topology and Geometry'', 1993, {{ISBN|0-387-97926-3}}.\n*{{Citation | last1=Browder | first1=William |authorlink = William Browder (mathematician) |title=Surgery on simply-connected manifolds | publisher=[[Springer-Verlag]] | location=Berlin, New York | mr=0358813 | year=1972}}\n* J. Scott Carter: ''How Surfaces Intersect in Space'', Series on Knots and Everything, 1993, {{ISBN|981-02-1050-7}}.\n*{{springer|id=A/a013230|title=Arf invariant|author=A.V. Chernavskii}}\n*{{citation |mr=0104735 | last = Dickson |first = Leonard Eugene | authorlink = Leonard Eugene Dickson | title = Linear groups: With an exposition of the Galois field theory | publisher = Dover Publications| place = New York | year= 1901}} \n*{{citation |mr=1001966\n |last = Kirby|first= Robion\n |authorlink = Robion Kirby\n |year = 1989\n |title = The topology of 4-manifolds\n |series = Lecture Notes in Mathematics\n |volume=1374\n |publisher=Springer-Verlag\n |isbn=0-387-51148-2\n |doi=10.1007/BFb0089031\n}}\n* [[W. B. Raymond Lickorish]], ''An Introduction to Knot Theory'', Graduate Texts in Mathematics, Springer, 1997, {{ISBN|0-387-98254-X}}\n* {{Citation\n | last1 = Martino | first1 = J.\n | last2 = Priddy | first2 = S.\n | year = 2003\n | title = Group Extensions And Automorphism Group Rings\n | journal = Homology, Homotopy and Applications\n | volume = 5 | issue = 1 | pages = 53–70\n | arxiv = 0711.1536 | doi=10.4310/hha.2003.v5.n1.a3\n}}\n* [[Lev Pontryagin]], ''Smooth manifolds and their applications in homotopy theory'' American Mathematical Society Translations, Ser. 2, Vol. 11, pp.&nbsp;1–114 (1959)\n\n==Further reading==\n* {{citation|last1=Lorenz|first1=Falko|last2=Roquette|first2=Peter|author2link=Peter Roquette | title=Contributions to the history of number theory in the 20th century | mr=2934052 | zbl=1276.11001 | url=http://www.rzuser.uni-heidelberg.de/~ci3/arf3-withpicture.pdf|series=Heritage of European Mathematics | location=Zürich | publisher=[[European Mathematical Society]] | isbn=978-3-03719-113-2 | year=2013 | chapter=Cahit Arf and his invariant | pages=189–222 }}\n* {{citation | last=Knus | first=Max-Albert | title=Quadratic and Hermitian forms over rings | zbl=0756.11008 | mr=1096299| series=Grundlehren der Mathematischen Wissenschaften | volume=294 | location=Berlin | publisher=[[Springer-Verlag]] | year=1991 | isbn=3-540-52117-8 | pages=211–222 | doi=10.1007/978-3-642-75401-2}}\n\n{{DEFAULTSORT:Arf Invariant}}\n[[Category:Quadratic forms]]\n[[Category:Surgery theory]]"
    },
    {
      "title": "Barnes–Wall lattice",
      "url": "https://en.wikipedia.org/wiki/Barnes%E2%80%93Wall_lattice",
      "text": "In mathematics, the '''Barnes–Wall lattice''' Λ<sub>16</sub>, discovered by Eric Stephen Barnes and G. E. (Tim) Wall ({{harvtxt|Barnes|Wall|1959}}), is the 16-dimensional positive-definite even integral [[lattice (group)|lattice]] of discriminant 2<sup>8</sup> with no norm-2 vectors. It is the sublattice of the [[Leech lattice]] fixed by a certain automorphism of order 2, and is analogous to the [[Coxeter&ndash;Todd lattice]].\n\nThe automorphism group of the Barnes&ndash;Wall lattice has order 89181388800 = 2<sup>21</sup>&nbsp;3<sup>5</sup>&nbsp;5<sup>2</sup>&nbsp;7 and has structure 2<sup>1+8</sup>&nbsp;PSO<sub>8</sub><sup>+</sup>('''F'''<sub>2</sub>). \n\nThe [[Genus of a quadratic form|genus]] of the Barnes&ndash;Wall lattice was described by {{harvtxt|Scharlau|Venkov|1994}} and contains 24 lattices; all the elements other than the Barnes&ndash;Wall lattice have root system of maximal rank 16. \n\nThe Barnes&ndash;Wall lattice is described in detail in {{harv|Conway|Sloane|1999|loc=section 4.10}}.\n\n==References==\n*{{citation|mr=0106893 |last=Barnes|first= E. S.|last2= Wall|first2= G. E.|title= Some extreme forms defined in terms of Abelian groups|journal=  J. Austral. Math. Soc.|volume=  1 |year= 1959|issue=1|pages= 47–63|doi=10.1017/S1446788700025064}}\n*{{Citation | last1=Conway | first1=John Horton | author1-link=John Horton Conway | last2=Sloane | first2=Neil J. A. | author2-link=Neil Sloane | title=Sphere Packings, Lattices and Groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | series=Grundlehren der Mathematischen Wissenschaften | isbn=978-0-387-98585-5 | mr=0920369 | year=1999 | volume=290}}\n*{{citation\n |mr=1282375 \n |last=Scharlau \n |first=Rudolf \n |last2=Venkov \n |first2=Boris B. \n |title=The genus of the Barnes&ndash;Wall lattice. \n |journal=Comment. Math. Helv. \n |volume=69 \n |year=1994 \n |issue=2 \n |pages=322–333 \n |url=http://retro.seals.ch/digbib/view?did=c1:421661&sdid=c1:422358\n |doi=10.1007/BF02564490 \n|citeseerx=10.1.1.29.9284 \n }}{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n==External links==\n*[http://www.math.rwth-aachen.de/~Gabriele.Nebe/LATTICES/LAMBDA16.html Barnes&ndash;Wall lattice] at Sloane's lattice catalogue.\n\n{{DEFAULTSORT:Barnes-Wall lattice}}\n[[Category:Quadratic forms]]\n\n\n{{mathematics-stub}}"
    },
    {
      "title": "Bhargava cube",
      "url": "https://en.wikipedia.org/wiki/Bhargava_cube",
      "text": "[[File:Bhargava_cube.svg|thumb|right|Bhargava cube with the integers ''a'', ''b'', ''c'', ''d'', ''e'', ''f'', ''g'', ''h'' at the corners]]\n\nIn [[mathematics]], in [[number theory]], '''Bhargava cube''' (also called '''Bhargava's cube''') is a configuration consisting of eight [[integers]] placed at the eight corners of a [[cube]].<ref>{{cite book|last1=Mak Trifkovic|title=Algebraic Theory of Quadratic Numbers|date=2013|publisher=Springer|location=New York|isbn=978-1-4614-7716-7|page=175}}</ref> This configuration was extensively used by [[Manjul Bhargava]], a [[Canadian-American]] [[Fields Medal]] winning [[mathematician]], to study the composition laws of binary quadratic forms and other such forms. To each pair of opposite faces of a Bhargava cube one can associate an integer [[binary quadratic form]] thus getting three binary quadratic forms corresponding to the three pairs of opposite faces of the Bhargava cube.<ref name=Manjul1>{{cite book|last1=Manjul Bhargava|title=''Higher composition laws and applications'', in Proceedings of the International Congress of Mathematicians, Madrid, Spain, 2006|date=2006|publisher=European Mathematical Society}}</ref> These three quadratic forms all have the same [[discriminant]] and Manjul Bhargava proved that their [[binary function|composition]] in the sense of [[Carl Friedrich Gauss|Gauss]]<ref name=Gauss>{{cite book|last1=Carl Friedrich Gauss (translated by Arthur A Clarke)|title=[[Disquisitiones Arithmeticae]]|date=1986|publisher=Springer Verlag|pages=230–256}}</ref> is the [[identity element]] in the associated [[Group (mathematics)|group]] of [[equivalence class]]es of primitive binary quadratic forms. Using this property as the starting point for a theory of composition of binary quadratic forms Manjul Bhargava went on to define fourteen different composition laws using a cube.\n\n==Integer binary quadratic forms==\nAn expression of the form <math>Q(x,y)=ax^2+bxy+cy^2</math>, where ''a'', ''b'' and ''c'' are fixed integers and ''x'' and ''y'' are variable integers, is called an integer binary quadratic form. The discriminant of the form is defined as \n\n:<math>D = b^2 -4ac.</math>\n\nThe form is said to be primitive if the coefficients ''a'', ''b'', ''c'' are relatively prime. Two forms\n\n:<math>Q(x,y) = ax^2+bxy+cy^2, \\quad Q^\\prime(x,y)=a^\\prime x^2+b^\\prime xy + c^\\prime y^2</math>\n\nare said to be equivalent if there exists a transformation\n\n:<math>x\\mapsto \\alpha x + \\beta y,\\quad  y\\mapsto \\gamma x + \\delta y</math> \n\nwith integer coefficients satisfying <math>\\alpha\\delta - \\beta\\gamma =1</math> which transforms <math>Q(x,y)</math> to <math>Q^\\prime(x,y)</math>. This relation is indeed an equivalence relation in the set of integer binary quadratic forms and it preserves discriminants and primitivity.\n\n==Gauss composition of integer binary quadratic forms==\nLet <math>Q(x,y)</math>  and <math>Q^\\prime(x,y)</math> be two primitive binary quadratic forms having the same discriminant and let the corresponding equivalence classes of forms be <math>[Q(x,y)]</math>  and <math>[Q^\\prime(x,y)]</math>. One can find integers <math>p,q,r,s, p^\\prime, q^\\prime, r^\\prime, s^\\prime, a^{\\prime\\prime}, b^{\\prime\\prime}, c^{\\prime\\prime}</math>  such that\n\n:<math>X=px_1x_2+qx_1y_2+ry_1x_2+sy_1y_2</math>\n:<math>Y=p^\\prime x_1x_2 + q^\\prime x_1y_2 + r^\\prime y_1x_2 + s^\\prime y_1y_2</math>\n:<math>Q^{\\prime\\prime}(x,y) = a^{\\prime\\prime}x^2 + b^{\\prime\\prime}xy + c^{\\prime\\prime}y^2</math>\n: <math>Q^{\\prime\\prime}(X,Y)=Q(x_1,y_1)Q^\\prime(x_2,y_2)</math>\n\nThe class <math>[Q^{\\prime\\prime}(x,y)]</math>  is uniquely determined by the classes [''Q''(''x'', ''y'')] and [''Q''<sup>&prime;</sup>(''x'', ''y'')] and is called the composite of the classes  <math>[Q(x,y)]</math>  and <math>[Q^\\prime(x,y)]</math>.<ref name=Gauss/> This is indicated by writing\n\n:<math>[Q^{\\prime\\prime}(x,y)]= [Q(x,y)]\\ast [Q^\\prime(x,y)]</math>\n\nThe set of equivalence classes of primitive binary quadratic forms having a given discriminant ''D'' is a group under the composition law described above. The identity element of the group is the class determined by the following form:\n:<math>Q_{Id}^{(D)}(x,y)\n=\n\\begin{cases}\nx^2-\\frac{D}{4}y^2 & D \\equiv 0 \\pmod 4\\\\\nx^2 + xy + \\frac{1-D}{4}y^2 & D \\equiv 1 \\pmod 4\n\\end{cases}\n</math>\nThe inverse of the class <math>[a x^2 + h xy + b y^2]</math> is the class <math>[ a x^2 - h xy + b y^2]</math>.\n\n==Quadratic forms associated with the Bhargava cube==\nLet (''M'', ''N'') be the pair of 2 &times; 2 matrices associated with a pair of opposite sides of a Bhargava cube; the matrices are formed in such a way that their rows and columns correspond to the edges of the corresponding faces. The integer binary quadratic form associated with this pair of faces is defined as \n:<math>Q=-\\det (Mx+Ny)</math>\nThe quadratic form is also defined as \n:<math>Q =-\\det(Mx-Ny)</math>\nHowever, the former definition will be assumed in the sequel.\n\n===The three forms===\nLet the cube be formed by the integers ''a'', ''b'', ''c'', ''d'', ''e'', ''f'', ''g'', ''h''. The pairs of matrices associated with opposite edges are denoted by (''M''<sub>1</sub>, ''N''<sub>1</sub>), (''M''<sub>2</sub>, ''N''<sub>2</sub>), and (''M''<sub>3</sub>, ''N''<sub>3</sub>). The first rows of ''M''<sub>1</sub>, ''M''<sub>2</sub> and ''M''<sub>3</sub> are respectively [''a'' ''b''], [''a'' ''c''] and [''a'' ''e'']. The opposite edges in the same face are the second rows. The corresponding edges in the opposite faces form the rows of the matrices ''N''<sub>1</sub>, ''N''<sub>2</sub>, ''N''<sub>3</sub> (see figure).\n{|\n|-\n| [[File:BhargavaCubeShowingM1andN1.png|thumb|right|200px|Bhargava cube showing the pair of opposite faces  ''M''<sub>1</sub> and ''N''<sub>1</sub>.]] || [[File:BhargavaCubeShowingM2andN2.png|thumb|right|200px|Bhargava cube showing the pair of opposite faces  ''M''<sub>2</sub> and ''N''<sub>2</sub>.]] || [[File:BhargavaCubeShowingM3andN3.png|thumb|right|200px|Bhargava cube showing the pair of opposite faces  ''M''<sub>3</sub> and ''N''<sub>3</sub>.]]\n|}\n\nThe quadratic form associated with the faces defined by the matrices \n<math>M_1=\\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix},N_1=\\begin{bmatrix} e & f \\\\ g & h\\end{bmatrix} </math> (see figure) is\n:<math>Q_1=-\\det(M_1x+N_1y)= -(\\det(M_1)x^2 + ( ah+ed-bg-fc)xy +\\det(N_1)y^2)</math>\nThe discriminant of a quadratic form ''Q''<sub>1</sub> is\n:<math>D_1=(ah+ed-bg-fc)^2 - 4\\det(M_1) \\det(N_1).</math>\n\nThe quadratic form associated with the faces defined by the matrices \n<math>M_2=\\begin{bmatrix} a & c \\\\ e & g\\end{bmatrix},N_2=\\begin{bmatrix}  b & d \\\\ f & h\\end{bmatrix} </math> (see figure) is\n:<math>Q_2=-\\det(M_2x+N_2y)= -(\\det(M_2)x^2 + ( ah+bg-fc-ed)xy +\\det(N_2)y^2)</math>\nThe discriminant of a quadratic form  ''Q''<sub>2</sub> is\n:<math>D_2=(ah+bg-fc-ed)^2 - 4\\det(M_2) \\det(N_2).</math>\n\nThe quadratic form associated with the faces defined by the matrices \n<math>M_3=\\begin{bmatrix} a & e \\\\ b & f\\end{bmatrix},N_3=\\begin{bmatrix} c & g \\\\ d & h\\end{bmatrix} </math> (see figure) is\n:<math>Q_3=-\\det(M_3x+N_3y)= -(\\det(M_3)x^2 + ( ah+fc-ed-bg)xy +\\det(N_3)y^2)</math>\nThe discriminant of a quadratic form  ''Q''<sub>3</sub> is\n:<math>D_3=(ah+fc-ed-bg)^2 - 4\\det(M_3) \\det(N_3).</math>\n\nManjul Bhargava's surprising discovery can be summarised thus:<ref name=Manjul1/>\n\n:If a cube A gives rise to three primitive binary quadratic forms ''Q''<sub>1</sub>, ''Q''<sub>2</sub>, ''Q''<sub>3</sub>,  then ''Q''<sub>1</sub>, ''Q''<sub>2</sub>, ''Q''<sub>3</sub> have the same discriminant, and the product of these three forms is the identity in the group defined by Gauss composition. Conversely, if ''Q''<sub>1</sub>, ''Q''<sub>2</sub>, ''Q''<sub>3</sub> are any three primitive binary quadratic forms of the same discriminant whose product is the identity under Gauss composition, then there exists a cube A yielding ''Q''<sub>1</sub>, ''Q''<sub>2</sub>, ''Q''<sub>3</sub>.\n\n==Example==\n[[File:BhargavaCubeExample.jpg|thumb|right|150px|An example of Bhargava cube]]\n\nThe three quadratic forms associated with the numerical Bhargava cube shown in the figure are computed as follows.\n\n: <math>\n\\begin{align}\nQ_1(x,y)=-\\det(M_1x+N_1y) & = - \\det\\left( \\begin{bmatrix} 1 & 0 \\\\ 0 & -2 \\end{bmatrix} x + \\begin{bmatrix}0 & 3 \\\\ 4 & 5\\end{bmatrix}y\\right) \\\\\n& = -\\begin{vmatrix} x & 3y \\\\ 4y & -2x+5y\\end{vmatrix} = 2x^2-5xy+12y^2 \\\\  \\\\\nQ_2(x,y)=-\\det(M_2x+N_2y) & = - \\det\\left( \\begin{bmatrix} 1 & 0 \\\\ 0 & 4 \\end{bmatrix} x + \\begin{bmatrix}0 & 3 \\\\ -2 & 5\\end{bmatrix}y\\right) \\\\\n& = -\\begin{vmatrix} x & 3y \\\\ -2y & 4x+5y\\end{vmatrix} = -4x^2-5xy-6y^2 \\\\  \\\\\nQ_3(x,y)=-\\det(M_3x+N_3y) & = - \\det\\left( \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix} x + \\begin{bmatrix}0 & 4 \\\\ -2 & 5\\end{bmatrix}y\\right) \\\\\n& = -\\begin{vmatrix} x & 4y \\\\ -2y & 3x+5y\\end{vmatrix} = -3x^2-5xy - 8y^2 \\\\  {}\n\\end{align}\n</math>\n\nThe composition <math>[Q_1(x,y)]\\ast[ Q_2(x,y)]</math> is the form <math> [Q(x,y)]</math> where <math>Q(x,y) = -3x^2+ 5xy - 8y^2</math> because of the following:\n\n:<math>X = -2x_1x_2 + 4y_1x_2 + y_1y_2</math>\n:<math>Y=x_1x_2+3y_1y_2</math>\n:<math> Q(X,Y) = Q_1(x_1,y_1)Q_2(x_2,y_2)</math>\n\nAlso <math>[Q_3(x,y)]^{-1}=Q(x,y)</math>. Thus <math>[Q_1(x,y)]\\ast [Q_2(x,y)]\\ast [Q_3(x,y)]</math> is the identity element in the group defined by the Gauss composition.\n\n==Further composition laws on forms==\n\n===Composition of cubes===\nThe fact that the composition of the three binary quadratic forms associated with the Bhargava cube is the identity element in the group of such forms has been used by Manjul Bhargava to define a composition law for the cubes themselves.<ref name=Manjul1/>\n\n{|\n|-\n| [[File:BhargavaCubeOfCubiccForm.jpg|thumb|right|The Bhargava cube corresponding to the binary cubic form <math>px^3+3qx^2y+3rxy^2+sy^3</math>.]]|| [[File:BhargavaCubeOfPairOfQuadraticForms.jpg|thumb|right|The Bhargava cube corresponding to the pair of binary quadratic forms <math>(ax^2+2bxy+cy^2,</math> <math>dx^2+2exy+fy^2)</math>.]]\n|}\n\n===Composition of cubic forms===\n\nAn integer  binary cubic in the form <math>px^3 + 3qx^2y+3rxy^2+sy^3</math> can be represented by a triply symmetric Bhargava cube as in the figure. The law of composition of cubes can be used to define a law of composition for the binary cubic forms.<ref name=Manjul1/>\n\n===Composition of pairs of binary quadratic forms===\n\nThe pair of binary quadratic forms <math>(ax^2+2bxy+cy^2, dx^2+2exy+fy^2)</math> can be represented by a doubly symmetric Bhargava cube as in the figure. The law of composition of cubes is now used to define a composition law on pairs of binary quadratic forms.<ref name=Manjul1/>\n\n==References==\n{{reflist}}\n\n[[Category:Quadratic forms]]\n[[Category:Carl Friedrich Gauss]]\n[[Category:Number theory]]"
    },
    {
      "title": "Binary quadratic form",
      "url": "https://en.wikipedia.org/wiki/Binary_quadratic_form",
      "text": "{{About|binary quadratic forms with [[integer]] coefficients|binary quadratic forms with other coefficients|quadratic form}}\n{{more footnotes|date=July 2009}}\nIn [[mathematics]], a '''binary quadratic form''' is a quadratic [[homogeneous polynomial]] in two variables\n\n: <math> q(x,y)=ax^2+bxy+cy^2, \\, </math>\n\nwhere ''a'', ''b'', ''c'' are the '''coefficients'''. When the coefficients can be arbitrary [[complex number]]s, most results are not specific to the case of two variables, so they are described in [[quadratic form]].  A quadratic form with [[integer]] coefficients is called an '''integral binary quadratic form''', often abbreviated to ''binary quadratic form''.\n\nThis article is entirely devoted to integral binary quadratic forms.  This choice is motivated by their status as the driving force behind the development of [[algebraic number theory]].  Since the late nineteenth century, binary quadratic forms have given up their preeminence in algebraic number theory to [[quadratic field|quadratic]] and more general [[number field]]s, but advances specific to binary quadratic forms still occur on occasion.\n\nPierre Fermat stated that if p is an odd prime then the equation <math>p = x^2 + y^2</math> has a solution iff <math>p \\equiv 1 \\pmod{4}</math>, and he made similar statement about the equations <math>p = x^2 + 2y^2</math>, <math>p = x^2 + 3y^2</math>, <math>p = x^2 - 2y^2</math> and <math>p = x^2 - 3y^2</math>\n<math>x^2 + y^2, x^2 + 2y^2, x^2 - 3y^2</math> and so on are quadratic forms, and the theory of quadratic forms gives a unified way of looking at and proving these theorems\n\nAnother instance of quadratic forms is [[Pell's equation]] <math>x^2-ny^2=1</math>\n\nBinary quadratic forms are closely related to ideals in quadratic fields, this allows the class number of a quadratic field to be calculated by counting the number of reduced binary quadratic forms of a given discriminant\n\nThe classical theta function of 2 variables is <math> \\sum_{(m,n)\\in \\mathbb{Z}^2} q^{m^2 + n^2}</math>, if <math>f(x,y)</math> is a positive definite quadratic form then <math> \\sum_{(m,n)\\in \\mathbb{Z}^2} q^{f(m,n)}</math> is a theta function\n\n== Equivalence ==\n\nTwo forms ''f'' and ''g'' are called '''equivalent''' if there exist integers <math>\\alpha, \\beta, \\gamma, \\text{ and } \\delta</math> such that the following conditions hold:\n\n: <math>\\begin{align} f(\\alpha x + \\beta y, \\gamma x + \\delta y) &= g(x,y),\\\\\n                      \\alpha \\delta - \\beta \\gamma &= 1\\end{align}.</math>\n\nFor example, with <math>f= x^2 + 4xy + 2y^2</math> and <math>\\alpha = -3</math>, <math>\\beta = 2</math>, <math>\\gamma = 1</math>, and <math>\\delta = -1</math>, we find that  ''f'' is equivalent to <math>g = (-3x+2y)^2 + 4(-3x+2y)(x-y)+2(x-y)^2</math>, which simplifies to <math>-x^2+4xy-2y^2</math>.\n\nThe above equivalence conditions define an [[equivalence relation]] on the set of integral quadratic forms.  It follows that the quadratic forms are [[partition of a set|partition]]ed into equivalence classes, called '''classes''' of quadratic forms. A '''class invariant''' can mean either a function defined on equivalence classes of forms or a property shared by all forms in the same class.\n\nLagrange used a different notion of equivalence, in which the second condition is replaced by <math> \\alpha \\delta - \\beta \\gamma = \\pm 1</math>.  Since Gauss it has been recognized that this definition is inferior to that given above.  If there is a need to distinguish, sometimes forms are called '''properly equivalent''' using the definition above and  '''improperly equivalent''' if they are equivalent in Lagrange's sense.\n\nIn [[matrix (mathematics)|matrix]] terminology, which is used occasionally below, when\n\n:  <math> \\begin{pmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{pmatrix} </math>\n\nhas integer entries and determinant 1, the map <math> f(x,y) \\mapsto f(\\alpha x + \\beta y, \\gamma x + \\delta y)</math> is a (right) [[Group action (mathematics)|group action]] of <math>\\mathrm{SL}_2(\\mathbb{Z})</math> on the set of binary quadratic forms.  The equivalence relation above then arises from the general theory of group actions.\n\nIf <math>f=ax^2+bxy+cy^2</math>, then important invariants include\n\n* The [[discriminant]] <math>\\Delta=b^2-4ac. </math>.\n* The content, equal to the greatest common divisor of ''a'', ''b'', and ''c''.\n\nTerminology has arisen for classifying classes and their forms in terms of their invariants. A form of discriminant <math>\\Delta</math> is '''definite''' if <math>\\Delta < 0</math>, '''degenerate''' if <math>\\Delta</math> is a perfect square, and '''indefinite''' otherwise. A form is '''primitive''' if its content is 1, that is, if its coefficients are coprime.  If a form's discriminant is a [[fundamental discriminant]], then the form is primitive.<ref>{{harvnb|Cohen|1993|loc=§5.2}}</ref> Discriminants satisfy <math>\\Delta\\equiv 0,1 \\pmod 4. </math>\n\n=== Automorphisms ===\n\nIf ''f'' is a quadratic form, a matrix\n\n:  <math> \\begin{pmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{pmatrix} </math>\n\nin <math>\\mathrm{SL}_2(\\mathbb{Z})</math> is an '''automorphism''' of ''f'' if <math>f(\\alpha x + \\beta y, \\gamma x + \\delta y) = f(x,y)</math>.  For example, the matrix\n\n: <math> \\begin{pmatrix} 3 & -4 \\\\ -2 & 3 \\end{pmatrix} </math>\n\nis an automorphism of the form <math>f = x^2 - 2y^2</math>.  The automorphisms of a form form a [[subgroup]] of <math>\\mathrm{SL}_2(\\mathbb{Z})</math>.  When ''f'' is definite, the group is finite, and when ''f'' is indefinite, it is infinite and [[cyclic group|cyclic]].\n\n==Representations==\n\nWe say that a binary quadratic form <math>q(x,y)</math> '''represents''' an integer <math>n</math> if it is possible to find integers <math>x</math> and <math>y</math> satisfying the equation <math>n = f(x,y).</math>  Such an equation is a '''representation''' of ''n'' by ''f''.\n\n=== Examples ===\n\n[[Diophantus]] considered whether, for an odd integer <math>n</math>, it is possible to find integers <math>x</math> and <math>y</math> for which <math>n = x^2 + y^2</math>.<ref>{{harvnb|Weil|2001|p=30}}</ref> When <math>n=65</math>, we have\n: <math>\\begin{align} 65 &= 1^2 + 8^2,\\\\\n                      65 &= 4^2 + 7^2,\n\\end{align} </math>\n\nso we find pairs <math>(x,y) = (1,8) \\text{ and } (4,7)</math> that do the trick.  We obtain more pairs that work by switching the values of <math>x</math> and <math>y</math> and/or by changing the sign of one or both of <math>x</math> and <math>y</math>.  In all, there are sixteen different solution pairs.  On the other hand, when <math>n=3</math>, the equation\n\n: <math>3=x^2 + y^2</math>\n\ndoes not have integer solutions.  To see why, we note that <math>x^2 \\geq 4</math> unless <math>x = -1, 0</math> or <math>1</math>.  Thus, <math>x^2+y^2</math> will exceed 3 unless <math>(x,y)</math> is one of the nine pairs with <math>x</math> and <math>y</math> each equal to <math>-1, 0</math> or 1.  We can check these nine pairs directly to see that none of them satisfies <math>3 = x^2 + y^2</math>, so the equation does not have integer solutions.\n\nA similar argument shows that for each <math>n</math>, the equation <math>n =x^2+y^2</math> can have only a finite number of solutions since <math>x^2+y^2</math> will exceed <math>n</math> unless the absolute values <math>|x|</math> and <math>|y|</math> are both less than <math>\\sqrt{n}</math>.  There are only a finite number of pairs satisfying this constraint.\n\nAnother ancient problem involving quadratic forms asks us to solve [[Pell's equation]].  For instance, we may seek integers ''x'' and ''y'' so that <math>1 = x^2 - 2y^2</math>.  Changing signs of ''x'' and ''y'' in a solution gives another solution, so it is enough to seek just solutions in positive integers.  One solution is <math>(x,y) = (3,2)</math>, that is, there is an equality <math>1 = 3^2 - 2 \\cdot 2^2</math>. If <math>(x,y)</math> is any solution to <math>1 = x^2 - 2 y^2</math>, then <math>(3x+4y,2x+3y)</math> is another such pair.  For instance, from the pair <math>(3,2)</math>, we compute\n\n: <math>(3\\cdot 3 + 4 \\cdot 2, 2\\cdot 3 + 3 \\cdot 2) = (17,12)</math>,\n\nand we can check that this satisfies <math>1 = 17^2 - 2 \\cdot 12^2</math>.  Iterating this process, we find further pairs <math>(x,y)</math> with <math>1 = x^2 - 2y^2</math>:\n\n: <math>\\begin{align}\n             (3 \\cdot 17 + 4 \\cdot 12, 2 \\cdot 17 + 3 \\cdot 12) &= (99,70),\\\\\n             (3 \\cdot 99 + 4 \\cdot 70, 2 \\cdot 99 + 3 \\cdot 70) &= (477,408),\\\\\n                                                                &\\vdots \\end{align}\n</math> \nThese values will keep growing in size, so we see there are infinitely many ways to represent 1 by the form <math>x^2 - 2y^2</math>.  This recursive description was discussed in Theon of Smyrna's commentary on [[Euclid's Elements]].\n\n=== The representation problem ===\n\nThe oldest problem in the theory of binary quadratic forms is the '''representation problem''': describe the representations of a given number <math>n</math> by a given quadratic form ''f''.  \"Describe\" can mean various things: give an algorithm to generate all representations, a closed formula for the number of representations, or even just determine whether any representations exist.\n\nThe examples above discuss the representation problem for the numbers 3 and 65 by the form <math>x^2 + y^2</math> and for the number 1 by the form <math>x^2 - 2y^2</math>.  We see that 65 is represented by <math>x^2 + y^2</math> in sixteen different ways, while 1 is represented by <math>x^2 - 2y^2</math> in infinitely many ways and \n3 is not represented by <math>x^2+y^2</math> at all. In the first case, the sixteen representations were explicitly described. It was also shown that the number of representations of an integer by <math>x^2+y^2</math> is always finite.  The [[sum of squares function]] <math>r_2(n)</math> gives the number of representations of ''n'' by <math>x^2+y^2</math> as a function of ''n''.  There is a closed formula<ref>{{harvnb|Hardy|Wright|2008||loc=Thm. 278}}</ref>\n\n: <math> r_2(n) = 4(d_1(n) - d_3(n)), </math>\n\nwhere <math>d_1(n)</math> is the number of [[divisor]]s of ''n'' that are [[Modular arithmetic|congruent]] to 1 modulo 4 and <math>d_3(n)</math> is the number of divisors of ''n'' that are congruent to 3 modulo 4.\n\nThere are several class invariants relevant to the representation problem:\n\n* The set of integers represented by a class.  If an integer ''n'' is represented by a form in a class, then it is represented by all other forms in a class.\n* The minimum absolute value represented by a class.  This is the smallest nonnegative value in the set of integers represented by a class.\n* The congruence classes modulo the discriminant of a class represented by the class.\n\nThe minimum absolute value represented by a class is zero for degenerate classes and positive for definite and indefinite classes.  All numbers represented by a definite form <math>f = ax^2 + bxy + cy^2</math> have the same sign: positive if <math>a>0</math> and negative if <math>a<0</math>.  For this reason, the former are called '''positive definite''' forms and the latter are '''negative definite'''.\n\nThe number of representations of an integer ''n'' by a form ''f'' is finite if ''f'' is definite and infinite if ''f'' is indefinite.  We saw instances of this in the examples above:  <math>x^2+y^2</math> is positive definite and <math>x^2 - 2y^2</math> is indefinite.\n\n=== Equivalent representations ===\n\nThe notion of equivalence of forms can be extended to '''equivalent representations'''.  Representations <math>m = f(x_1,y_1)</math> and <math>n = g(x_2,y_2)</math> are equivalent if there exists a matrix\n\n: <math> \\begin{pmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{pmatrix} </math>\n\nwith integer entries and determinant 1 so that <math>f(\\alpha x + \\beta y, \\gamma x + \\delta y) = g(x,y)</math> and\n\n: <math>\\begin{pmatrix} \\delta& -\\beta \\\\ -\\gamma & \\alpha\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ y_1 \\end{pmatrix} = \\begin{pmatrix} x_2 \\\\ y_2 \\end{pmatrix}</math>\n\nThe above conditions give a (right) action of the group <math>\\mathrm{SL}_2(\\mathbb{Z})</math> on the set of representations of integers by binary quadratic forms.  It follows that equivalence defined this way is an equivalence relation and in particular that the forms in equivalent representations are equivalent forms.\n\nAs an example, let <math>f = x^2 - 2y^2</math> and consider a representation <math>1 = f(x_1,y_1)</math>.  Such a representation is a solution to the Pell equation described in the examples above. The matrix\n\n: <math> \\begin{pmatrix} 3 & -4 \\\\ -2 & 3 \\end{pmatrix} </math>\n\nhas determinant 1 and is an automorphism of ''f''.  Acting on the representation <math>1 = f(x_1,y_1)</math> by this matrix yields the equivalent representation <math>1 = f(3x_1 + 4y_1, 2x_1 + 3 y_1)</math>.  This is the recursion step in the process described above for generating infinitely many solutions to <math>1 = x^2 - 2y^2</math>.  Iterating this matrix action, we find that the infinite set of representations of 1 by ''f'' that were determined above are all equivalent.\n\nThere are generally finitely many equivalence classes of representations of an integer ''n'' by forms of given nonzero discriminant <math>\\Delta</math>.  A complete set of representatives for these classes can be given in terms of ''reduced forms'' defined in the section below.  When <math>\\Delta < 0</math>, every representation is equivalent to a unique representation by a reduced form, so a complete set of representatives is given by the finitely many representations of ''n'' by reduced forms of discriminant <math>\\Delta</math>.  When <math>\\Delta > 0</math>, Zagier proved that every representation of a positive integer ''n'' by a form of discriminant <math>\\Delta</math> is equivalent to a unique representation <math>n = f(x,y)</math> in which ''f'' is reduced in Zagier's sense and <math>x > 0</math>, <math>y \\geq 0</math>.<ref>{{harvnb|Zagier|1981||loc=}}</ref>  The set of all such representations constitutes a complete set of representatives for equivalence classes of representations.\n\n== Reduction and class numbers<!--'Class number (binary quadratic forms)' redirects here--> ==\n\nLagrange proved that for every value ''D'', there are only finitely many classes of binary quadratic forms with discriminant ''D''. Their number is the '''{{vanchor|class number}}'''<!--boldface per WP:R#PLA--> of discriminant ''D''. He described an algorithm, called '''reduction''', for constructing a canonical representative in each class, the '''reduced form''', whose coefficients are the smallest in a suitable sense.\n\nGauss gave a superior reduction algorithm in ''[[Disquisitiones Arithmeticae]]'', which has ever since the reduction algorithm most commonly given in textbooks.  In 1981, Zagier published an alternative reduction algorithm which has found several uses as an alternative to Gauss's.<ref>{{harvnb|Zagier|1981||loc=}}</ref>\n\n== Composition ==\n\n'''Composition''' most commonly refers to a [[binary operation]] on primitive equivalence classes of forms of the same discriminant, one of the deepest discoveries of Gauss, which makes this set into a finite [[abelian group]] called the '''form class group''' (or simply class group) of discriminant <math>\\Delta</math>.  [[Class group]]s have since become one of the central ideas in algebraic number theory.  From a modern perspective, the class group of a fundamental discriminant <math>\\Delta</math> is [[isomorphic]] to the [[narrow class group]] of the [[quadratic field]] <math>\\mathbf{Q}(\\sqrt{\\Delta})</math> of discriminant <math>\\Delta</math>.<ref>{{harvnb|Fröhlich|Taylor|1993|loc=Theorem 58}}</ref> For negative <math>\\Delta</math>, the narrow class group is the same as the [[ideal class group]], but for positive <math>\\Delta</math> it may be twice as big.\n\n\"Composition\" also sometimes refers to, roughly, a binary operation on binary quadratic forms.  The word \"roughly\" indicates two caveats:  only certain pairs of binary quadratic forms can be composed, and the resulting form is not well-defined (although its equivalence class is).  The composition operation on equivalence classes is defined by first defining composition of forms and then showing that this induces a well-defined operation on classes.\n\n\"Composition\" can also refer to a binary operation on representations of integers by forms.  This operation is substantially more complicated{{cn|date=March 2017}}<!--<ref>{{harvnb|Shanks|1989}}</ref> full citation not in article yet --> than composition of forms, but arose first historically.  We will consider such operations in a separate section below.\n\nComposition means taking 2 quadratic forms of the same discriminant and combining them to create a quadratic form of the same discriminant, it is a generalization of the 2-square identity <math>\\left(a^2 + b^2\\right)\\left(c^2 + d^2\\right) = \\left(ac-bd\\right)^2 + \\left(ad+bc\\right)^2</math>\n\n=== Composing forms and classes ===\n\nA variety of definitions of composition of forms has been given, often in an attempt to simplify the extremely technical and general definition of Gauss.  We present here Arndt's method, because it remains rather general while being simple enough to be amenable to computations by hand.  An alternative definition is described at [[Bhargava cube]]s.\n\nSuppose we wish to compose forms <math>f_1 = A_1 x^2 + B_1 xy + C_1 y^2</math> and <math>f_2 = A_2 x^2 + B_2 xy + C_2 y^2</math>, each primitive and of the same discriminant <math>\\Delta</math>.  We perform the following steps:\n\n# Compute <math>B_\\mu = \\tfrac{B_1 + B_2}{2}</math> and <math> e = \\gcd(A_1, A_2, B_\\mu)</math>, and <math>A = \\tfrac{A_1 A_2}{e^2}</math>\n# Solve the system of congruences <blockquote><math>\\begin{align} x &\\equiv B_1 \\pmod{2 \\tfrac{A_1}{e}}\\\\ x &\\equiv B_2 \\pmod{2 \\tfrac{A_2}{e}}\\\\ \\tfrac{B_\\mu}{e} x &\\equiv \\tfrac{\\Delta + B_1 B_2}{2e} \\pmod{2A} \\end{align} </math></blockquote>It can be shown that this system always has a unique integer solution modulo <math>2A</math>.  We arbitrarily choose such a solution and call it ''B''.\n# Compute ''C'' such that <math>\\Delta = B^2 - 4AC</math>.  It can be shown that ''C'' is an integer.\n\nThe form <math>Ax^2 + Bxy + Cy^2</math> is \"the\" composition of <math>f_1</math> and <math>f_2</math>.  We see that its first coefficient is well-defined, but the other two depend on the choice of ''B'' and ''C''.  One way to make this a well-defined operation is to make an arbitrary convention for how to choose ''B''—for instance, choose ''B'' to be the smallest positive solution to the system of congruences above.  Alternatively, we may view the result of composition, not as a form, but as an equivalence class of forms modulo the action of the group of matrices of the form\n\n: <math>\\begin{pmatrix} 1 & n\\\\ 0 & 1\\end{pmatrix}</math>,\n\nwhere ''n'' is an integer.  If we consider the class of <math>Ax^2 + Bxy + Cy^2</math> under this action, the middle coefficients of the forms in the class form a congruence class of integers modulo 2''A''.  Thus, composition gives a well-defined function from pairs of binary quadratic forms to such classes.\n\nIt can be shown that if <math>f_1</math> and <math>f_2</math> are equivalent to <math>g_1</math> and <math>g_2</math> respectively, then the composition of <math>f_1</math> and <math>f_2</math> is equivalent to the composition of <math>g_1</math> and <math>g_2</math>.  It follows that composition induces a well-defined operation on primitive  classes of discriminant <math>\\Delta</math>, and as mentioned above, Gauss showed these classes form a finite abelian group.  The [[identity element|identity]] class in the group is the unique class containing all forms <math>x^2 + Bxy + Cy^2</math>, i.e., with first coefficient 1.  (It can be shown that all such forms lie in a single class, and the restriction <math>\\Delta \\equiv 0 \\text{ or } 1 \\pmod{4}</math> implies that there exists such a form of every discriminant.)  To [[inverse element|invert]] a class, we take a representative <math>Ax^2 + Bxy + Cy^2</math> and form the class of <math>Ax^2 - Bxy + Cy^2</math>.  Alternatively, we can form the class of <math>Cx^2 + Bxy + Ay^2</math> since this and <math>Ax^2 - Bxy + Cy^2</math> are equivalent.\n\n== Genera of binary quadratic forms ==\n\nGauss also considered a coarser notion of equivalence, with each coarse class called a '''genus''' of forms.  Each genus is the union of a finite number of equivalence classes of the same discriminant, with the number of classes depending only on the discriminant. In the context of binary quadratic forms, genera can be defined either through congruence classes of numbers represented by forms or by '''genus characters''' defined on the set of forms.  A third definition is a special case of the [[genus of a quadratic form]] in n variables.  This states that forms are in the same genus if they are locally equivalent at all rational primes (including the [[Algebraic number field#Places|Archimedean place]]).\n\n== History ==\n\nThere is circumstantial evidence of protohistoric knowledge of algebraic identities involving binary quadratic forms.<ref>{{harvnb|Weil|2001|loc=Ch.I §§VI, VIII}}</ref>  The first problem concerning binary quadratic forms asks for the existence or construction of representations of integers by particular binary quadratic forms.  The prime examples are the solution of [[Pell's equation]] and the representation of integers as sums of two squares.  Pell's equation was already considered by the Indian mathematician [[Brahmagupta#Pell's equation|Brahmagupta]] in the 7th century CE. Several centuries later, his ideas were extended to a complete solution of Pell's equation known as the [[chakravala method]], attributed to either of the Indian mathematicians [[Jayadeva (mathematician)|Jayadeva]] or [[Bhāskara II]].<ref>{{harvnb|Weil|2001|loc=Ch.I §IX}}</ref> The problem of representing integers by sums of two squares was considered in the 3rd century by [[Diophantus]].<ref>{{harvnb|Weil|2001|loc=Ch.I §IX}}</ref>  In the 17th century, inspired while reading Diophantus's [[Arithmetica]], [[Fermat]] made several observations about representations by specific quadratic forms including that which is now known as [[Fermat's theorem on sums of two squares]].<ref>{{harvnb|Weil|2001|loc=Ch.II §§VIII-XI}}</ref> [[Euler]] provided the first proofs of Fermat's observations and added some new conjectures about representations by specific forms, without proof.<ref>{{harvnb|Weil|2001|loc=Ch.III §§VII-IX}}</ref>\n\nThe general theory of quadratic forms was initiated by [[Lagrange]] in 1775 in his ''[[List of important publications in mathematics#Recherches d'Arithmétique|Recherches d'Arithmétique]]''.  Lagrange was the first to realize that \"a coherent general theory required the simulatenous consideration of all forms.\"<ref>{{harvnb|Weil|2001|loc=p.318}}</ref>  He was the first to recognize the importance of the discriminant and to define the essential notions of equivalence and reduction, which, according to Weil, have \"dominated the whole subject of quadratic forms ever since\".<ref>{{harvnb|Weil|2001|loc=p.317}}</ref>  Lagrange showed that there are finitely many equivalence classes of given discriminant, thereby defining for the first time an arithmetic [[Ideal class group|class number]].  His introduction of reduction allowed the quick enumeration of the classes of given discriminant and foreshadowed the eventual development of [[infrastructure (number theory)|infrastructure]]. In 1798, [[Adrien-Marie Legendre|Legendre]] published ''Essai sur la théorie des nombres'', which summarized the work of Euler and Lagrange and added some of his own contributions, including the first glimpse of a composition operation on forms.\n\nThe theory was vastly extended and refined by [[Carl Friedrich Gauss|Gauss]] in Section V of ''[[List of important publications in mathematics#Disquisitiones Arithmeticae|Disquisitiones Arithmeticae]]''. Gauss introduced a very general version of a composition operator that allows composing even forms of different discriminants and imprimitive forms.  He replaced Lagrange's equivalence with the more precise notion of proper equivalence, and this enabled him to show that the primitive classes of given discriminant form a [[group (mathematics)|group]] under the composition operation. He introduced genus theory, which gives a powerful way to understand the quotient of the class group by the subgroup of squares.  (Gauss and many subsequent authors wrote 2''b'' in place of ''b''; the modern convention allowing the coefficient of ''xy'' to be odd is due to [[Gotthold Eisenstein|Eisenstein]]).\n\nThese investigations of Gauss strongly influenced both the arithmetical theory of quadratic forms in more than two variables and the subsequent development of algebraic number theory, where quadratic fields are replaced with more general [[number field]]s.  But the impact was not immediate.  Section V of ''Disquisitiones'' contains truly revolutionary ideas and involves very complicated computations, sometimes left to the reader.  Combined, the novelty and complexity made Section V notoriously difficult. [[Dirichlet]] published simplifications of the theory that made it accessible to a broader audience.  The culmination of this work is his text ''[[List of important publications in mathematics#Vorlesungen über Zahlentheorie|Vorlesungen über Zahlentheorie]]''.  The third edition of this work includes two supplements by [[Dedekind]].  Supplement XI introduces [[ring theory]], and from then on, especially after the 1897 publication of [[Hilbert|Hilbert's]] ''[[List of important publications in mathematics#Zahlbericht|Zahlbericht]]'', the theory of binary quadratic forms lost its preeminent position in [[algebraic number theory]] and became overshadowed by the more general theory of [[algebraic number fields]].\n\nEven so, work on binary quadratic forms with integer coefficients continues to the present.  This includes numerous results about quadratic number fields, which can often be translated into the language of binary quadratic forms, but also includes developments about forms themselves or that originated by thinking about forms, including [[Daniel Shanks|Shanks's]] infrastructure, [[Don Zagier|Zagier's]] reduction algorithm, [[John Horton Conway|Conway's]] topographs, and [[Manjul Bhargava|Bhargava's]] reinterpretation of composition through [[Bhargava cube]]s.\n\n==See also==\n* [[Bhargava cube]]\n*[[Fermat's theorem on sums of two squares]]\n* [[Legendre symbol]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n* Johannes Buchmann, Ulrich Vollmer: ''Binary Quadratic Forms'', Springer, Berlin 2007, {{ISBN|3-540-46367-4}}\n* Duncan A. Buell: ''Binary Quadratic Forms'', Springer, New York 1989\n* David A Cox, ''Primes of the form <math>x^2 + y^2</math>, Fermat, class field theory, and complex multiplication''\n* {{Citation\n| last=Cohen\n| first=Henri\n| author-link=Henri Cohen (number theorist)\n| title=A Course in Computational Algebraic Number Theory\n| publisher=[[Springer-Verlag]]\n| location=Berlin, New York\n| series=Graduate Texts in Mathematics\n| isbn=978-3-540-55640-4\n| mr=1228206\n| year=1993\n| volume=138\n}}\n* {{Citation\n| last= Fröhlich\n| first = Albrecht\n| authorlink= Albrecht Fröhlich\n| last2=Taylor\n| first2=Martin\n| authorlink2= Martin J. Taylor\n| title=Algebraic number theory\n| publisher=[[Cambridge University Press]]\n| series=Cambridge Studies in Advanced Mathematics\n| isbn=978-0-521-43834-6\n| mr=1215934\n| year=1993\n| volume=27\n}}\n* {{Citation\n| last1=Hardy\n| first1=G. H.\n| author1-link=G. H. Hardy\n| last2=Wright\n| first2=E. M.\n| author2-link=E. M. Wright\n| edition=6th\n| others=Revised by [[Roger Heath-Brown|D. R. Heath-Brown]] and [[Joseph H. Silverman|J. H. Silverman]].  Foreword by [[Andrew Wiles]].\n| title=An Introduction to the Theory of Numbers\n| publisher= Clarendon Press \n| location=Oxford\n| series=\n| isbn= 978-0-19-921986-5 \n| mr= 2445243\n| zbl=  1159.11001 \n| year=2008 \n| origyear=1938\n}}\n* {{Citation\n| last = Weil\n| first = André\n| authorlink= André Weil\n| title = Number Theory: An approach through history from Hammurapi to Legendre\n| publisher = Birkhäuser Boston\n| year = 2001\n}}\n* {{Citation\n| last = Zagier\n| first = Don\n| authorlink = Don Zagier\n| title = Zetafunktionen und quadratische Körper: eine Einführung in die höhere Zahlentheorie\n| publisher = Springer\n| year = 1981\n}}\n\n==External links==\n* [http://oeis.org/wiki/User:Peter_Luschny/BinaryQuadraticForms Peter Luschny, Positive numbers represented by a binary quadratic form]\n* {{eom|id=b/b016370|author=A. V. Malyshev|title=Binary quadratic form}}\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Büchi's problem",
      "url": "https://en.wikipedia.org/wiki/B%C3%BCchi%27s_problem",
      "text": "'''Büchi's problem''', also known as the '''''n'' squares' problem''', is an open problem from [[number theory]] named after the Swiss mathematician [[Julius Richard Büchi]]. It asks whether there is a positive integer ''M'' such that every sequence of ''M'' or more integer squares, whose second difference is constant and equal to 2, is necessarily a sequence of squares of the form (''x''&nbsp;+&nbsp;''i'')<sup>2</sup>, ''i''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;...,&nbsp;''M'',... for some integer&nbsp;''x''. In 1983, [[Douglas Hensley]] observed that Büchi's problem is equivalent to the following: Does there exist a positive integer ''M'' such that, for all integers ''x'' and ''a'', the quantity (''x''&nbsp;+&nbsp;''n'')<sup>2</sup>&nbsp;+&nbsp;''a'' cannot be a square for more than ''M'' consecutive values of&nbsp;''n'', unless&nbsp;''a''&nbsp;=&nbsp;0?\n\n==Statement of Büchi's problem==\n\nBüchi's problem can be stated in the following way: Does there exist a positive integer ''M'' such that the system of equations\n\n: <math>\n\\begin{cases}\nx_2^2-2x_1^2+x_0^2=2\\\\\nx_3^2-2x_2^2+x_1^2=2\\\\\n{}\\quad\\vdots\\\\\nx_{M-1}^2-2x_{M-2}^2+x_{M-3}^2=2\n\\end{cases}\n</math>\n\nhas only solutions satisfying <math>x_n^2=(x_0+n)^2.</math>\n\nSince the first difference of the sequence <math>\\sigma=(x_n^2)_{n=0,\\dots,M-1}</math> is the sequence <math>\\Delta^{(1)}(\\sigma)=(x_{n+1}^2-x_n^2)_{n=0,\\dots,M-2}</math>, the second difference of <math>\\sigma</math> is\n\n: <math>\n\\Delta^{(2)}(\\sigma)=((x_{n+2}^2-x_{n+1}^2)-(x_{n+1}^2-x_n^2))_{n=0,\\dots,M-3}=(x_{n+2}^2-2x_{n+1}^2+x_n^2)_{n=0,\\dots,M-3}.\n</math>\n\nTherefore, the above system of equations is equivalent to the single equation\n\n: <math>\\Delta^{(2)}(\\sigma)=(2)_{n=0,\\dots,M-3}</math>\n\nwhere the unknown is the sequence <math>\\sigma</math>.\n\n==Examples==\n\nObserve that for any integer ''x'' we have\n\n: <math>(\\star)\\qquad(x+2)^2-2(x+1)^2+x^2=2. </math>\n\nHence the equation <math> x_2^2-2x_1^2+x_0^2=2</math> has solutions, called ''trivial Büchi sequences of length three'', such that <math>x_2^2=(x_0+2)^2</math> and <math>x_1^2=(x_0+1)^2</math>. For example, the sequences (2,&nbsp;3,&nbsp;4) and (2,&nbsp;−3,&nbsp;4) are trivial Büchi sequences. A ''nontrivial Büchi sequence of length three'' is given for example by the sequence (0,&nbsp;7,&nbsp;10), as it satisfies 10<sup>2</sup>&nbsp;−&nbsp;2·7<sup>2</sup>&nbsp;+&nbsp;0<sup>2</sup>&nbsp;=&nbsp;2, while 0<sup>2</sup>, 7<sup>2</sup> and 10<sup>2</sup> are not consecutive squares.\n\nReplacing ''x'' by ''x''&nbsp;+&nbsp;1 in equation <math>(\\star)</math>, we obtain <math>(x+3)^2-2(x+2)^2+(x+1)^2=2</math>.  Hence the system of equations\n\n: <math>\n\\begin{cases}\nx_2^2-2x_1^2+x_0^2=2\\\\\nx_3^2-2x_2^2+x_1^2=2\n\\end{cases}\n</math>\n\nhas trivial Büchi solutions of length 4, namely the one satisfying <math>x_n^2=(x_0+n)^2</math> for ''n''&nbsp;=&nbsp;0,&nbsp;1,&nbsp;2,&nbsp;3. In 1983, D. Hensley showed that there are infinitely many nontrivial Büchi sequences of length four. It is not known whether there exist any non-trivial Büchi sequence of length five (Indeed, Büchi asked originally the question only for&nbsp;''M''&nbsp;=&nbsp;5.).\n\n==Original motivation==\nA positive answer to Büchi's problem would imply, using the negative answer to [[Hilbert's Tenth Problem]] by [[Yuri Matiyasevich]], that there is no algorithm to [[Decidability (logic)|decide]] whether a system of diagonal [[quadratic form]]s with integer coefficients represents an integer tuple. Indeed, Büchi observed that squaring, therefore multiplication, would be existentially definable in the integers over the [[first-order logic|first-order]] language having two symbols of constant for 0 and 1, a symbol of function for the sum, and a symbol of relation ''P'' to express that an integer is a square.\n\n==Some results==\n\n[[Paul Vojta]] proved in 1999 that a positive answer to Büchi's Problem would follow from a positive answer to a weak version of the [[Bombieri–Lang conjecture]]. In the same article, he proves that the analogue of Büchi's Problem for the field of meromorphic functions over the complex numbers has a positive answer. Positive answers to analogues of Büchi's Problem in various other rings of functions have been obtained since then (in the case of rings of functions, one adds the hypothesis that not all ''x''<sub>''n''</sub> are constant).\n\n==References==\n*[[Paul Vojta|Vojta, Paul]] (1999), ''Diagonal quadratic forms and Hilbert’s tenth problem'', pp.&nbsp;261–274 in ''Hilbert’s tenth problem: relations with arithmetic and algebraic geometry'' (Ghent, 1999), edited by J. Denef et al., Contemp. Math. 270, Amer. Math. Soc., Providence, RI, 2000.\n*[[Leonard Lipshitz|Lipshitz, Leonard]] (1990), \"Quadratic forms, the five square problem, and diophantine equations\" in ''Collected Papers of J. Richard Büchi''. Edited by [[Saunders Mac Lane]] and Dirk Siefkes. Springer, New York.\n*[[Douglas Hensley|Hensley, Douglas]] (1983), “Sequences of squares with second difference of two and a conjecture of Büchi”, unpublished.\n\n<!--- Categories --->\n\n{{DEFAULTSORT:Buchi's Problem}}\n[[Category:Number theory]]\n[[Category:Quadratic forms]]\n[[Category:Squares in number theory]]"
    },
    {
      "title": "Class number formula",
      "url": "https://en.wikipedia.org/wiki/Class_number_formula",
      "text": "In [[number theory]], the '''class number formula''' relates many important invariants of a [[number field]] to a special value of its [[Dedekind zeta function]].\n\n==General statement of the class number formula==\nWe start with the following data:\n\n* {{mvar|K}} is a number field. \n* {{math|[''K'' : '''Q'''] {{=}} ''n'' {{=}} ''r''<sub>1</sub> + 2''r''<sub>2</sub>}}, where {{math|''r''<sub>1</sub>}} denotes the number of [[real and complex embeddings|real embeddings]] of {{mvar|K}}, and {{math|2''r''<sub>2</sub>}} is the number of complex embeddings of {{mvar|K}}. \n* {{math|''ζ<sub>K</sub>''(''s'')}} is the [[Dedekind zeta function]] of {{mvar|K}}. \n* {{math|''h<sub>K</sub>''}} is the [[ideal class|class number]], the number of elements in the [[ideal class group]] of {{mvar|K}}.\n* {{math|Reg<sub>''K''</sub>}} is the [[regulator (mathematics)|regulator]] of {{mvar|K}}.\n* {{math|''w<sub>K</sub>''}} is the number of [[root of unity|roots of unity]] contained in {{mvar|K}}.\n* {{math|''D<sub>K</sub>''}} is the [[discriminant of an algebraic number field|discriminant]] of the [[Algebraic extension|extension]] {{math|''K''/'''Q'''}}.\n\nThen:\n\n:'''Theorem (Class Number Formula).''' {{math|''ζ<sub>K</sub>''(''s'')}} [[conditionally convergent|converges absolutely]] for {{math|Re(''s'') > 1}} and extends to a [[meromorphic]] [[function (mathematics)|function]] defined for all complex {{mvar|s}} with only one [[simple pole]] at {{math|''s'' {{=}} 1}}, with residue\n\n::<math> \\lim_{s \\to 1} (s-1) \\zeta_K(s) = \\frac{2^{r_1} \\cdot(2\\pi)^{r_2} \\cdot \\operatorname{Reg}_K \\cdot h_K}{w_K \\cdot \\sqrt{|D_K|}}</math>\n\nThis is the most general \"class number formula\". In particular cases, for example when {{mvar|K}} is a [[cyclotomic extension]] of {{math|'''Q'''}}, there are particular and more refined class number formulas.\n\n==Proof==\n\nThe idea of the proof of the class number formula is most easily seen when ''K'' = '''Q'''(i). In this case, the ring of integers in ''K'' is the [[Gaussian integer]]s.\n\nAn elementary manipulation shows that the residue of the Dedekind zeta function at ''s'' = 1 is the average of the coefficients of the [[Dirichlet series]] representation of the Dedekind zeta function. The ''n''-th coefficient of the Dirichlet series is essentially the number of representations of ''n'' as a sum of two squares of nonnegative integers. So one can compute the residue of the Dedekind zeta function at ''s'' = 1 by computing the average number of representations. As in the article on the [[Gauss circle problem]], one can compute this by approximating the number of lattice points inside of a quarter circle centered at the origin, concluding that the residue is one quarter of pi.\n\nThe proof when ''K'' is an arbitrary imaginary quadratic number field is very similar.<ref>https://www.math.umass.edu/~weston/oldpapers/cnf.pdf</ref>\n\nIn the general case, by [[Dirichlet's unit theorem]], the group of units in the ring of integers of ''K'' is infinite. One can nevertheless reduce the computation of the residue to a lattice point counting problem using the classical theory of real and complex embeddings<ref>http://planetmath.org/realandcomplexembeddings</ref> and approximate the number of lattice points in a region by the volume of the region, to complete the proof.\n\n==Dirichlet class number formula==\n\n[[Peter Gustav Lejeune Dirichlet]] published a proof of the class number formula for [[quadratic field]]s in 1839, but it was stated in the language of [[quadratic forms]] rather than classes of [[Ideal (ring theory)|ideal]]s. It appears that Gauss already knew this formula in 1801.<ref>http://mathoverflow.net/questions/109330/did-gauss-know-dirichlets-class-number-formula-in-1801</ref>\n\nThis exposition follows [[Harold Davenport|Davenport]].<ref name=Davenport>\n{{cite book |last1=Davenport |first1=Harold |authorlink1=Harold Davenport |editor1-first=Hugh L. |editor1-last=Montgomery |editor1-link=Hugh Montgomery (mathematician) |title=Multiplicative Number Theory |url=https://books.google.com/books?id=U91lsCaJJmsC |accessdate=2009-05-26 |edition=3rd |series=Graduate Texts in Mathematics |volume=74 |year=2000|publisher=Springer-Verlag |location=New York |isbn=978-0-387-95097-6 |pages=43–53 }}\n</ref>\n\nLet ''d'' be a [[fundamental discriminant]], and write ''h(d)'' for the number of equivalence classes of quadratic forms with discriminant ''d''. Let <math>\\chi = \\left(\\!\\frac{d}{m}\\!\\right)</math> be the [[Kronecker symbol]]. Then <math>\\chi</math> is a [[Dirichlet character]]. Write <math>L(s,\\chi)</math> for the [[Dirichlet L-series]] based on <math>\\chi</math>. For ''d > 0'', let ''t > 0'', ''u > 0'' be the solution to the [[Pell equation]] <math>t^2 - d u^2 = 4</math> for which ''u'' is smallest, and write\n:<math>\\epsilon = \\frac{1}{2}(t + u \\sqrt{d}).</math>\n(Then ε is either a [[fundamental unit (number theory)|fundamental unit]] of the [[real quadratic field]] <math>\\mathbb{Q}(\\sqrt{d})</math> or the square of a fundamental unit.)\nFor ''d'' < 0, write ''w'' for the number of automorphisms of quadratic forms of discriminant ''d''; that is,\n:<math>w =\n\\begin{cases}\n2, & d < -4; \\\\\n4, & d = -4; \\\\\n6, & d = -3.\n\\end{cases}\n</math>\nThen Dirichlet showed that\n:<math>h(d)=\n\\begin{cases}\n\\dfrac{w \\sqrt{|d|}}{2 \\pi} L(1,\\chi), & d < 0; \\\\\n\\dfrac{\\sqrt{d}}{\\ln \\epsilon} L(1,\\chi), & d > 0.\n\\end{cases}</math>\nThis is a special case of Theorem 1 above: for a [[quadratic field]] ''K'', the Dedekind zeta function is just <math>\\zeta_K(s) = \\zeta(s) L(s, \\chi)</math>, and the residue is <math>L(1,\\chi)</math>. Dirichlet also showed that the ''L''-series can be written in a finite form, which gives a finite form for the class number. Suppose <math>\\chi</math> is [[Dirichlet character#Primitive characters and conductor|primitive]] with prime [[Dirichlet character#Primitive characters and conductor|conductor]] <math>q</math>.  Then\n:<math> L(1, \\chi) =\n\\begin{cases}\n-\\dfrac{\\pi}{q^{3/2}}\\sum_{m=1}^{q-1} m \\left( \\dfrac{m}{q} \\right), & q \\equiv 3 \\mod 4; \\\\\n-\\dfrac{1}{q^{1/2}}\\sum_{m=1}^{q-1} \\left( \\dfrac{m}{q} \\right) \\ln 2\\sin \\dfrac{m\\pi}{q} , & q \\equiv 1 \\mod 4.\n\\end{cases}</math>\n\n==Galois extensions of the rationals==\nIf ''K'' is a [[Galois extension]] of '''Q''', the theory of [[Artin L-function]]s applies to <math> \\zeta_K(s)</math>. It has one factor of the [[Riemann zeta function]], which has a pole of residue one, and the quotient is regular at ''s'' = 1. This means that the right-hand side of the class number formula can be equated to a left-hand side\n\n:&Pi; ''L''(1,&rho;)<sup>dim &rho;</sup>\n\nwith ρ running over the classes of irreducible non-trivial complex [[linear representation]]s of Gal(''K''/'''Q''') of dimension dim(ρ). That is according to the standard decomposition of the [[regular representation]].\n\n==Abelian extensions of the rationals==\nThis is the case of the above, with Gal(''K''/'''Q''') an [[abelian group]], in which all the ρ can be replaced by [[Dirichlet character]]s (via [[class field theory]]) for some modulus ''f'' called the [[conductor of an abelian extension|conductor]]. Therefore all the ''L''(1) values occur for [[Dirichlet L-function]]s, for which there is a classical formula, involving logarithms.\n\nBy the [[Kronecker–Weber theorem]], all the values required for an '''analytic class number formula''' occur already when the cyclotomic fields are considered. In that case there is a further formulation possible, as shown by [[Ernst Kummer|Kummer]]. The [[Regulator (mathematics)|regulator]], a calculation of volume in 'logarithmic space' as divided by the logarithms of the units of the cyclotomic field, can be set against the quantities from the ''L''(1) recognisable as logarithms of [[cyclotomic unit]]s. There result formulae stating that the class number is determined by the index of the cyclotomic units in the whole group of units.\n\nIn [[Iwasawa theory]], these ideas are further combined with [[Stickelberger's theorem]].\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book | author=W. Narkiewicz | title=Elementary and analytic theory of algebraic numbers | edition=2nd | publisher=[[Springer-Verlag]]/[[Polish Scientific Publishers PWN]] | year=1990 | isbn=3-540-51250-0 | pages=324–355 }}\n\n{{PlanetMath attribution|id=4664|title=Class number formula}}\n\n{{L-functions-footer}}\n\n[[Category:Algebraic number theory]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Coxeter–Todd lattice",
      "url": "https://en.wikipedia.org/wiki/Coxeter%E2%80%93Todd_lattice",
      "text": "In mathematics, the '''Coxeter–Todd lattice''' K<sub>12</sub>, discovered by {{harvs|txt=yes|author1-link=H. S. M.  Coxeter|author2-link=J. A. Todd|last=Coxeter|last2=Todd|year=1953}}, is a 12-dimensional even integral [[lattice (group)|lattice]] of discriminant 3<sup>6</sup>  with no norm-2 vectors. It is the sublattice of the [[Leech lattice]] fixed by a certain automorphism of order 3, and is analogous to the [[Barnes–Wall lattice]].\n\n==Properties==\nThe Coxeter–Todd lattice can be made into a 6-dimensional lattice self dual over the Eisenstein integers. The automorphism group of this complex lattice has index 2 in the full automorphism group of the Coxeter–Todd lattice and is a [[complex reflection group]] (number 34 on the list) with structure 6.PSU<sub>4</sub>('''F'''<sub>3</sub>).2, called the [[Mitchell group]].\n\nThe [[Genus of a quadratic form|genus]] of the Coxeter–Todd lattice was described by {{harv|Scharlau|Venkov|1995}} and has 10 isometry classes: all of them other than the Coxeter–Todd lattice have a root system of maximal rank 12.\n\n== Construction ==\nBased on [http://www.math.rwth-aachen.de/~Gabriele.Nebe/LATTICES/K12.html Nebe] web page we can define K<sub>12</sub> using following 6 vectors in 6-dimensional complex coordinates. ω is complex number of order 3 i.e. ω<sup>3</sup>=1. \n\n(1,0,0,0,0,0), (0,1,0,0,0,0), (0,0,1,0,0,0),\n\n½(1,ω,ω,1,0,0),  ½(ω,1,ω,0,1,0), ½(ω,ω,1,0,0,1),\n\nBy adding vectors having scalar product -½ and multiplying by ω we can obtain all lattice vectors. We have 15 combinations of two zeros times 16 possible signs gives 240 vectors; plus 6 unit vectors times 2 for signs gives 240+12=252 vectors. Multiply it by 3 using multiplication by ω we obtain 756 unit vectors in K<sub>12</sub> lattice.\n\n==Further reading==\nThe Coxeter–Todd lattice is described in detail in {{harv|Conway|Sloane|1999|loc=section 4.9}} and {{harv|Conway|Sloane|1983}}.\n\n==References==\n* {{citation |last=Conway |first=J. H. |last2=Sloane |first2=N. J. A. |year=1983 |title=The Coxeter–Todd lattice, the Mitchell group, and related sphere  packings |journal=Mathematical Proceedings of the Cambridge Philosophical Society |volume=93 |issue=3 |pages=421–440 |doi=10.1017/S0305004100060746 |mr=0698347}}\n* {{Citation | last1=Conway | first1=John Horton | author1-link=John Horton Conway | last2=Sloane | first2=Neil J. A. | author2-link=Neil Sloane | year=1999 | title=Sphere Packings, Lattices and Groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | series=Grundlehren der Mathematischen Wissenschaften | volume=290 | isbn=978-0-387-98585-5 | mr=0920369 | doi=10.1007/978-1-4757-2016-7}}\n* {{citation\n|last=Coxeter |first=H. S. M. |last2=Todd |first2=J. A.\n|title=An extreme duodenary form\n|journal=Canadian Journal of Mathematics\n|volume=5\n|year=1953\n|pages=384–392\n|mr=0055381\n|doi=10.4153/CJM-1953-043-4}}\n* {{citation\n|url=http://www.matha.mathematik.uni-dortmund.de/preprints/95-07.html\n|title=The genus of the Coxeter-Todd lattice\n|first=Rudolf\n|last=Scharlau\n|first2=Boris B.\n|last2=Venkov\n|year=1995\n|journal=Preprint\n|deadurl=yes\n|archiveurl=https://web.archive.org/web/20070612070900/http://www.matha.mathematik.uni-dortmund.de/preprints/95-07.html\n|archivedate=2007-06-12\n|df=\n}}\n\n==External links==\n* [http://www.research.att.com/~njas/lattices/K12.html Coxeter–Todd lattice] in Sloane's lattice catalogue\n\n{{DEFAULTSORT:Coxeter-Todd lattice}}\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Definite quadratic form",
      "url": "https://en.wikipedia.org/wiki/Definite_quadratic_form",
      "text": "In [[mathematics]], a '''definite quadratic form''' is a [[quadratic form]] over some [[Real number|real]] [[vector space]] {{math|''V''}} that has the same [[positive and negative numbers|sign]] (always positive or always negative) for every nonzero vector of {{math|''V''}}.   According to that sign, the quadratic form is called '''positive-definite''' or '''negative-definite'''.\n\nA '''semidefinite''' (or '''semi-definite''') quadratic form is defined in the same way, except that \"positive\" and \"negative\" are replaced by \"not negative\" and \"not positive\", respectively.  An '''indefinite''' quadratic form is one that takes on both positive and negative values.\n\nMore generally, the definition applies to a vector space over an [[ordered field]].<ref>Milnor & Husemoller (1973) p.&nbsp;61</ref>\n\n==Associated symmetric bilinear form==\nQuadratic forms correspond one-to-one to [[symmetric bilinear form]]s over the same space.<ref>This is true only over a field of [[characteristic (algebra)|characteristic]] other than 2, but here we consider only [[ordered field]]s, which necessarily have characteristic 0.</ref> A symmetric bilinear form is also described as '''definite''', '''semidefinite''', etc. according to its associated quadratic form.  A quadratic form {{math|''Q''}} and its associated symmetric bilinear form {{math|''B''}} are related by the following equations:\n\n:<math>\\begin{align}\n    Q(x) &= B(x, x) \\\\\n  B(x,y) &= B(y,x) = \\frac{1}{2} (Q(x + y) - Q(x) - Q(y))\n\\end{align}</math>\n\nThe latter formula arises from expanding <math>Q(x+y) = B(x+y,x+y)</math>.\n\n==Examples==\nAs an example, let <math>V=\\mathbb{R}^2 </math>, and consider the quadratic form\n\n:<math>Q(x)=c_1{x_1}^2+c_2{x_2}^2 </math>\nwhere {{math|1=''x'' = (''x''<sub>1</sub>, ''x''<sub>2</sub>)}} <math> \\in V </math> and {{math|''c''<sub>1</sub>}} and {{math|''c''<sub>2</sub>}} are constants. If {{math|1=''c''<sub>1</sub> > 0}} and {{math|1=''c''<sub>2</sub> > 0}}, the quadratic form {{math|''Q''}} is positive-definite, so ''Q'' evaluates to a positive number whenever <math>(x_1,x_2)\\neq (0,0).</math> If one of the constants is positive and the other is 0, then {{math|''Q''}} is positive semidefinite and always evaluates to either 0 or a positive number. If {{math|1=''c''<sub>1</sub> > 0}} and {{math|1=''c''<sub>2</sub> < 0}}, or vice versa, then {{math|''Q''}} is indefinite and sometimes evaluates to a positive number and sometimes to a negative number. If {{math|1=''c''<sub>1</sub> < 0}} and {{math|1=''c''<sub>2</sub> < 0}}, the quadratic form is negative-definite and always evaluates to a negative number whenever <math>(x_1,x_2)\\neq (0,0).</math> And if one of the constants is negative and the other is 0, then {{math|''Q''}} is negative semidefinite and always evaluates to either 0 or a negative number. \n\nIn general a quadratic form in two variables will also involve a cross-product term in ''x''<sub>1</sub>''x''<sub>2</sub>:\n\n:<math>Q(x)=c_1{x_1}^2+c_2{x_2}^2+2c_3x_1x_2.</math>\n\nThis quadratic form is positive-definite if <math>c_1>0</math> and <math>c_1c_2-{c_3}^2>0,</math> negative-definite if <math>c_1<0</math> and <math>c_1c_2-{c_3}^2>0,</math> and indefinite if <math>c_1c_2-{c_3}^2<0.</math> It is positive or negative semidefinite if <math>c_1c_2-{c_3}^2=0,</math> with the sign of the semidefiniteness coinciding with the sign of <math>c_1.</math>\n\nThis bivariate quadratic form appears in the context of [[conic section]]s centered on the origin. If the general quadratic form above is equated to 0, the resulting equation is that of an [[ellipse]] if the quadratic form is positive or negative-definite, a [[hyperbola]] if it is indefinite, and a [[parabola]] if <math>c_1c_2-{c_3}^2=0.</math>\n\nThe square of the [[Euclidean norm]] in ''n''-dimensional space, the most commonly used measure of distance, is\n\n:<math>x_1^2+\\cdots+x_n^2.</math>\n\nIn two dimensions this means that the distance between two points is the square root of the sum of the squared distances along the <math>x_1</math> axis and the <math>x_2</math> axis.\n\n==Matrix form==\n\nA quadratic form can be written in terms of [[matrix (mathematics)|matrices]] as\n\n:<math>x^\\text{T} A x</math>\n\nwhere ''x'' is any ''n''×1 [[Euclidean vector#In Cartesian space|Cartesian vector]] <math>(x_1, \\cdots , x_n)^\\text{T} </math> in which not all elements are 0, superscript <sup>T</sup> denotes a [[transpose]], and ''A'' is an ''n''×''n'' [[symmetric matrix]]. If ''A'' is [[diagonal matrix|diagonal]] this is equivalent to a non-matrix form containing solely terms involving squared variables; but if ''A'' has any non-zero off-diagonal elements, the non-matrix form will also contain some terms involving products of two different variables.\n\nPositive or negative-definiteness or semi-definiteness, or indefiniteness, of this quadratic form is equivalent to [[positive-definite matrix|the same property of ''A'']], which can be checked by considering all [[eigenvalue]]s of ''A'' or by checking the signs of all of its [[principal minor]]s.\n\n==Optimization==\n\nDefinite quadratic forms lend themselves readily to [[optimization]] problems. Suppose the matrix quadratic form is augmented with linear terms, as\n\n:<math>x^\\text{T} A x + 2 b^\\text{T} x ,</math>\n\nwhere ''b'' is an ''n''×1 vector of constants. The [[first-order condition]]s for a maximum or minimum are found by setting the [[matrix derivative]] to the zero vector:\n\n:<math>2Ax+2b=0,</math>\n\ngiving \n\n:<math>x=-A^{-1}b</math>\n\nassuming ''A'' is [[nonsingular matrix|nonsingular]]. If the quadratic form, and hence ''A'', is positive-definite, the [[second partial derivative test|second-order condition]]s for a minimum are met at this point. If the quadratic form is negative-definite, the second-order conditions for a maximum are met.\n\nAn important example of such an optimization arises in [[multiple regression]], in which a vector of estimated parameters is sought which minimizes the sum of squared deviations from a perfect fit within the dataset.\n\n==See also==\n*[[Anisotropic quadratic form]]\n*[[Positive-definite function]]\n*[[Positive-definite matrix]]\n*[[Polarization identity]]\n\n==References==\n{{reflist}}\n* {{cite book | last=Kitaoka | first=Yoshiyuki | title=Arithmetic of quadratic forms | series=Cambridge Tracts in Mathematics | volume=106 | publisher=Cambridge University Press | year=1993 | isbn=0-521-40475-4 | zbl=0785.11021 }}\n* {{Lang Algebra | edition=3r2004|page=578}}\n* {{cite book | first1=J. | last1=Milnor | author1-link=John Milnor| first2=D. | last2=Husemoller | title=Symmetric Bilinear Forms | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 }}\n\n{{algebra-stub}}\n[[Category:Quadratic forms]]\n[[Category:Linear algebra]]"
    },
    {
      "title": "Discriminant",
      "url": "https://en.wikipedia.org/wiki/Discriminant",
      "text": "{{short description|Function of the coefficients of a polynomial that gives information on its roots}}\n{{distinguish|Discriminant analysis}}\n{{more citations needed|date=November 2011}}\nIn [[algebra]], the '''discriminant''' of a [[polynomial]] is a [[polynomial function]] of its coefficients, which allows deducing some properties of the [[root of a function|roots]] without computing them. For example, the discriminant of the [[quadratic polynomial]]\n:<math>ax^2+bx+c</math>\nis \n:<math>b^2-4ac,</math>\nwhich is zero if and only if the polynomial has a [[double root]], and (in the case of [[real number|real]] coefficients) is positive if and only if the polynomial has two real roots.\n\nFor a cubic polynomial with real coefficients, the discriminant is zero when two roots coincide, positive if the roots are three distinct real numbers, and negative if there is one real root and two distinct [[complex conjugate]] roots. For a real polynomial of [[degree of a polynomial|degree]] 4 or greater, the discriminant is zero if and only if it has a [[multiple root]], and positive if and only if the number of non-real roots is a multiple of 4.\n\nThe discriminant is widely used in [[number theory]], either directly or through its generalization as the [[discriminant of a number field]]. For [[polynomial factorization|factoring a polynomial]] with [[integer]] coefficients, the standard method consists in first factoring its [[modular arithmetic|reduction modulo]] a [[prime number]] not dividing the discriminant (nor dividing the leading coefficient). In [[algebraic geometry]], the discriminant with respect to one of the variables characterizes the points of a [[hypersurface]] where the [[implicit function theorem]] does not apply.\n\nThe term \"discriminant\" was coined in 1851 by the British mathematician [[James Joseph Sylvester]].<ref>{{cite journal|first=J. J.|last=Sylvester|date=1851|title=On a remarkable discovery in the theory of canonical forms and of hyperdeterminants|journal=Philosophical Magazine|series=4th series|volume=2|pages=391–410}}<br>Sylvester coins the word \"discriminant\" on [https://books.google.com/books?id=DBNDAQAAIAAJ&pg=PA406#v=onepage&q&f=false page 406].</ref>\n\n==Definition==\n\nLet \n:<math>A(x) = a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x+a_0</math>\nbe a polynomial of [[degree of a polynomial|degree]] {{math|''n''}} (this means <math>a_n\\ne 0</math>), such that the coefficients <math>a_0, \\ldots, a_n</math> belong to a [[field (mathematics)|field]], or, more generally, to a [[commutative ring]]. The [[resultant]] of {{math|''A''}} and its [[formal derivative|derivative]] <math>A'(x) = na_nx^{n-1}+(n-1)a_{n-1}x^{n-2}+\\cdots+a_1</math> is a polynomial in <math>a_0, \\ldots, a_n</math> with integer coefficients, which is the [[determinant]] of the [[Sylvester matrix]] of {{math|''A''}} and {{math|''A''{{void}}′}}. The nonzero entries of the first column of the Sylvester matrix are <math>a_n</math> and <math>na_n,</math> and the [[resultant]] is thus a [[multiple (mathematics)|multiple]] of <math>a_n.</math> So, the discriminant, up to its sign, is defined as the quotient of the resultant of {{math|''A''}} and {{math|''A'{{void}}''}} by <math>a_n:</math>\n:<math>\\operatorname{Disc}_x(A) = \\frac{(-1)^\\frac{n(n-1)}{2}}{a_n}\\operatorname{Res}_x(A,A')</math>\nThis sign has been historically chosen, in order that, over the reals, the discriminant is positive if all the roots of the polynomial are real.\nThe division by <math>a_n</math> may be not well defined if the [[ring (mathematics)|ring]] of the coefficients contains [[zero divisor]]s. Such a problem may be avoided by replacing <math>a_n</math> by 1 in the first column of the Sylvester matrix ''before'' computing the determinant. ''In any case, the discriminant is a polynomial in'' <math>a_0, \\ldots, a_n</math> ''with integer coefficients''.\n\n===Expression in terms of the roots===\nWhen the polynomial is defined over a [[field (mathematics)|field]], the [[fundamental theorem of algebra]] implies that it has {{math|''n''}} roots, {{math|''r''<sub>1</sub>, ''r''<sub>2</sub>, ..., ''r<sub>n</sub>''}}, not necessarily all distinct, in an [[algebraically closed extension]] of the field (for a polynomial with real coefficients, this algebraically closed extension is generally chosen as the field of [[complex number]]s).\n\nIn terms of the roots, the discriminant is equal to\n\n:<math>\\operatorname{Disc}_x(A) = {a_n}^{2n-2}\\prod_{i<j} \\left(r_i-r_j\\right)^2 = (-1)^{\\frac12 n(n-1)}{a_n}^{2n-2} \\prod_{i \\neq j} \\left(r_i-r_j\\right) </math>\n\nIt is thus the square of the [[Vandermonde polynomial]] times {{math|''a''<sub>''n''</sub><sup>2''n'' − 2</sup>}}.\n\nThis expression of the discriminant is often taken as a definition. It makes clear that if the polynomial has a [[multiple root]], then its discriminant is zero, and that if all the roots are real, then the discriminant is positive.\n\n==Low degrees==\nThe discriminant of a [[linear polynomial]] (degree 1) is rarely considered. If needed, it is commonly defined to be equal to 1 (using the usual conventions for the [[empty product]] and the [[determinant]] of the [[empty matrix]]). There is no common convention for the discriminant of a constant polynomial (degree 0).\n\nFor small degrees, the discriminant is rather simple (see below), but for higher degrees it becomes unwieldy. The discriminant of a [[generic polynomial|general]] [[Quartic function|quartic]] has 16 terms,<ref>{{cite book\n|title=Elimination practice: software tools and applications\n|first1=Dongming\n|last1=Wang\n|publisher=Imperial College Press\n|year=2004\n|isbn=1-86094-438-8\n|url=https://books.google.com/books?id=ucpk6oO5GN0C&pg=PA180\n|at=ch. 10 p. 180}}\n</ref> that of a [[Quintic function|quintic]] has 59 terms,<ref>{{cite book\n|title=Discriminants, resultants and multidimensional determinants\n|first1=I. M.\n|last1=Gelfand\n|first2=M. M.\n|last2=Kapranov\n|first3=A. V.\n|last3=Zelevinsky\n|publisher=Birkhäuser\n|year=1994\n|isbn=3-7643-3660-9\n|page=1\n|url=http://blms.oxfordjournals.org/cgi/reprint/28/1/96}}\n</ref> and that of a [[sextic equation|sextic]] has 246 terms.<ref>{{cite book\n|title=Solving polynomial equations: foundations, algorithms, and applications\n|first1=Alicia\n|last1=Dickenstein\n|author1-link=Alicia Dickenstein\n|first2=Ioannis Z.\n|last2=Emiris\n|publisher=Springer\n|year=2005\n|isbn=3-540-24326-7\n|url=https://books.google.com/books?id=rSs-pQNrO_YC&pg=PA26\n|at=ch. 1 p. 26}}\n</ref>\nThis is [[OEIS]] sequence {{OEIS link|A007878}}.\n<!-- Please don't add numbers of terms of higher degrees (like 7/1103, 8/5247 and others of http://oeis.org/A007878) without providing proper sources. Thanks -->\n\n===Degree 2===\nThe [[quadratic polynomial]]\n<math> ax^2+bx+c \\, </math>\nhas discriminant\n:<math>b^2-4ac\\,.</math>\n\nThe square root of the discriminant appears in the [[quadratic formula]] for the roots of the quadratic polynomial:\n:<math>x_{1,2}=\\frac{-b \\pm \\sqrt {b^2-4ac}}{2a}.</math>\n\nThe discriminant is zero if and only if the two roots are equal. If {{math|''a'', ''b'', ''c''}} are [[real number]]s, the polynomial has two distinct real roots if the discriminant is positive, and two [[complex conjugate]] roots if it is negative.\n<ref>{{cite book\n|title=Integers, polynomials, and rings\n|first1=Ronald S.\n|last1=Irving\n|publisher=Springer-Verlag New York, Inc.\n|year=2004\n|isbn=0-387-40397-3\n|url=https://books.google.com/books?id=B4k6ltaxm5YC&pg=PA154\n|at=ch. 10.3 pp. 153–154}}</ref>\n\nIf {{math|''a'', ''b'', ''c''}} are [[rational number]]s, then the discriminant is the square of a rational number if and only if the two roots are rational numbers.\n\n===Degree 3===\n[[File:Discriminant of cubic polynomials..png|thumb|The zero set of discriminant of the cubic {{math|''x''<sup>3</sup> + ''bx''<sup>2</sup> + ''cx'' + ''d''}}, i.e. points satisfying {{math|1=''b''<sup>2</sup>''c''<sup>2</sup> – 4''c''<sup>3</sup> – 4''b''<sup>3</sup>''d'' – 27''d''<sup>2</sup> + 18''bcd'' = 0}}.]]\nThe [[cubic polynomial]]\n<math> ax^3+bx^2+cx+d \\, </math>\nhas discriminant\n:<math>b^2c^2-4ac^3-4b^3d-27a^2d^2+18abcd\\,.</math>\n\nIn particular, the polynomial <math>x^3+px+q</math>\nhas discriminant\n:<math> -4p^3-27q^2\\,.</math>\n\nThe discriminant is zero if and only if at least two roots are equal. If the coefficients are [[real number]]s, and the discriminant is not zero, the discriminant is positive if the roots are three distinct real numbers, and negative if there is one real root and two [[complex conjugate]] roots.\n<ref>{{cite book\n|title=Integers, polynomials, and rings\n|first1=Ronald S.\n|last1=Irving\n|publisher=Springer-Verlag New York, Inc.\n|year=2004\n|isbn=0-387-40397-3\n|url=https://books.google.com/books?id=B4k6ltaxm5YC&pg=PA154\n|at=ch. 10 ex. 10.14.4 & 10.17.4, pp. 154–156}}</ref>\n\nThe [[square root]] of the product of the discriminant by {{math|−3}} (and possibly also by the square of a [[rational number]]) appears in the formulas for the roots of a cubic polynomial.\n\nIf the polynomial is irreducible and its coefficients are [[rational number]]s (or belong to a [[number field]]), then the discriminant is a square of a rational number (or a number from the number field) if and only if the [[Galois group]] of the cubic equation is the [[cyclic group]] of order three.\n\n===Degree 4===\n[[File:Quartic Discriminant.png|thumb|The discriminant of the quartic polynomial {{math|''x''<sup>4</sup> + ''cx''<sup>2</sup> + ''dx'' + ''e''}}. The surface represents points ({{math|''c'', ''d'', ''e''}}) where the polynomial has a repeated root. The cuspidal edge corresponds to the polynomials with a triple root, and the self-intersection corresponds to the polynomials with two different repeated roots.]]\nThe [[quartic polynomial]]\n<math> ax^4+bx^3+cx^2+dx+e\\,</math>\nhas discriminant\n:<math>\\begin{align}\n{} & 256a^3e^3-192a^2bde^2-128a^2c^2e^2+144a^2cd^2e \\\\[4pt]\n& {} -27a^2d^4+144ab^2ce^2-6ab^2d^2e-80abc^2de \\\\[4pt]\n& {} +18abcd^3+16ac^4e-4ac^3d^2-27b^4e^2+18b^3cde \\\\[4pt]\n& {} -4b^3d^3-4b^2c^3e+b^2c^2d^2\\,.\n\\end{align}</math>\n\nThe discriminant is zero if and only if at least two roots are equal. If the coefficients are [[real number]]s and the discriminant is non-zero, the discriminant is negative if there are two real roots and two [[complex conjugate]] roots, and it is positive if the roots are either all real or all non-real.\n\n==Properties==\n===Zero discriminant===\nThe discriminant of a polynomial over a [[field (mathematics)|field]] is zero if and only if the polynomial has a multiple root in some [[field extension]].\n\nThe discriminant of a polynomial over an integral domain is zero if and only if the polynomial and its [[formal derivative|derivative]] have a non-constant common divisor.\n\nIn [[characteristic of a ring|characteristic]] 0, this is equivalent to saying that the polynomial is not [[square-free polynomial|square-free]] (that is, the polynomial is divisible by the square of a non-constant polynomial).\n\nIn nonzero characteristic {{math|''p''}}, the discriminant is zero if and only if the polynomial is not square-free or it has an [[irreducible polynomial|irreducible factor]] which not separable (that is, the irreducible factor is a polynomial in <math>x^p</math>).\n\n===Invariance under change of the variable===\nThe discriminant of a polynomial is, [[up to]] a scaling, invariant under any [[projective transformation]] of the variable. As a projective transformation may be decomposed into a product of translations, homotheties and inversions, this results of the following formulas for simpler transformations, where {{math|''P''(''x'')}} denotes a polynomial in the variable {{mvar|''x''}} of degree {{math|''n''}}, with <math>a_n</math> as leading coefficient.\n\n* ''Invariance by translation'':\n::<math>\\operatorname{Disc}_x(P(x+\\alpha)) = \\operatorname{Disc}_x(P(x))</math>\n:This results from the expression of the discriminant in terms of the roots\n* ''Invariance by homothety'':\n::<math>\\operatorname{Disc}_x(P(\\alpha x)) = \\alpha^{n(n-1)}\\operatorname{Disc}_x(P(x))</math>\nThis results of the expression in terms of the roots, or of the quasi-homogeneity of the discriminant.\n* ''Invariance by inversion'':\n::<math>\\operatorname{Disc}_x(P^r(x)) =\\operatorname{Disc}_x(P(x))</math>\n:Here <math>P^r</math> denotes the [[reciprocal polynomial]] of {{math|''P''}}. That is, if <math>P(x)=a_nx^n + \\cdots +a_0,</math> then\n::<math>P^r(x) = x^nP(1/x) = a_0x^n +\\cdots +a_n.</math>\n\n===Invariance under ring homomorphisms===\nLet <math>\\varphi\\colon R\\to S </math> be a [[ring homomorphism|homomorphism]] of [[commutative ring]]. Given a polynomial \n:<math>A=a_nx^n+a_{n-1}x^{n-1}+ \\cdots+a_0</math>\nin {{math|''R''[''x'']}}, the homomorphism <math>\\varphi</math> acts on {{math|''A''}} for producing the polynomial\n:<math>A^\\varphi=\\varphi(a_n)x^n+\\varphi(a_{n-1})x^{n-1}+ \\cdots+\\varphi(a_0)</math>\nin {{math|''S''[''x'']}}.\n\nThe discriminant is invariant under <math>\\varphi</math> in the following sense. If <math>\\varphi(a_n)\\ne 0,</math> then \n:<math>\\operatorname{Disc}_x(A^\\varphi) = \\varphi(\\operatorname{Disc}_x(A)).</math>\nAs the discriminant is defined in terms of a determinant, this property results immediately from the similar property of determinants.\n\nIf <math>\\varphi(a_n)= 0,</math> then <math>\\varphi(\\operatorname{Disc}_x(A))</math> may be zero or not. One has, when <math>\\varphi(a_n)= 0,</math>\n:<math>\\varphi(\\operatorname{Disc}_x(A)) = \\varphi(a_{n-1})^2\\operatorname{Disc}_x(A^\\varphi).</math>\n\nWhen, as it is generally the case in [[algebraic geometry]], one is interested only to know if a discriminant is zero or not, these properties may be summarised into:\n:<math>\\varphi(\\operatorname{Disc}_x(A)) = 0</math> if and only either <math>\\operatorname{Disc}_x(A^\\varphi)=0</math> or <math>\\deg(A)-\\deg(A^\\varphi)\\ge 2.</math>\n\nThis is often interpreted by saying that <math>\\varphi(\\operatorname{Disc}_x(A)) = 0</math> if and only if <math>A^\\varphi</math> has a [[multiple root]], possibly [[point at infinity|at infinity]].\n\n===Product of polynomials===\nIf {{math|1=''P'' = ''QR''}} is a product of polynomials in {{math|''x''}}, then\n:<math>\\begin{align}\n\\operatorname{disc}_x(P) &= \\operatorname{disc}_x(Q)\\operatorname{Res}_x(Q,R)^2\\operatorname{disc}_x(R)\n\\\\[5pt]\n{}&=(-1)^{pq}\\operatorname{disc}_x(Q)\\operatorname{Res}_x(Q,R)\\operatorname{Res}_x(R,Q)\\operatorname{disc}_x(R),\n\\end{align}</math>\nwhere {{math|Res()}} denotes the [[resultant]], and {{math|''p''}} and {{math|''q''}} are the respective degrees of {{math|''P''}} and {{math|''Q''}}.\n\nThis property follows immediately by substituting the expression for the resultant, and the discriminant, in terms of the roots of the respective polynomials.\n\n===Homogeneity===\nThe discriminant is a [[homogeneous polynomial]] in the coefficients; it is also a homogeneous polynomial in the roots and thus [[quasi-homogeneous polynomial|quasi-homogeneous]] in the coefficients.\n\nThe discriminant of a polynomial of degree {{math|''n''}} is homogeneous of degree {{math|2''n'' − 2}} in the coefficients. This can be seen two ways. In terms of the roots-and-leading-term formula, multiplying all the coefficients by {{mvar|λ}} does not change the roots, but multiplies the leading term by {{mvar|λ}}. In terms of its expression as a determinant of a {{math|(2''n'' − 1) × (2''n'' − 1)}} matrix (the [[Sylvester matrix]]) divided by {{mvar|a<sub>n</sub>}}, the determinant is homogeneous of degree {{math|2''n'' − 1}} in the entries, and dividing by {{mvar|a<sub>n</sub>}} makes the degree {{math|2''n'' − 2}}.\n\nThe discriminant of a polynomial of degree {{math|''n''}} is homogeneous of degree {{math|''n''(''n'' − 1)}} in the roots. This follows from the expression of the discriminant in terms of the roots, which is the product of a constant and <math>\\binom{n}{2} = \\frac{n(n-1)}{2}</math> squared differences of roots.\n\nThe discriminant of a polynomial of degree {{math|''n''}} is quasi-homogeneous of degree {{math|''n''(''n'' − 1)}} in the coefficient, if, for every {{math|''i''}}, the coefficient of <math>x^i</math> is given the weight {{math|''n'' − ''i''}}. It is also quasi-homogeneous of the same degree, if, for every {{math|''i''}}, the coefficient of <math>x^i</math> is given the weight <math>''i''.</math> This is a consequence of the general fact that every polynomial which is homogeneous and [[symmetric polynomial|symmetric]] in the roots may be expressed as a quasi-homogeneous polynomial in the [[elementary symmetric function]]s of the roots.\n\nConsider the polynomial\n:<math> P=a_nx^n+a_{n-1}x^{n-1}+ \\cdots +a_0.</math>\nIt follows from what precedes that the exponents in every [[monomial]] {{math|''a''<sub>0</sub><sup>''i''<sub>0</sub></sup>. ..., ''a<sub>n</sub><sup>i<sub>n</sub></sup>''}} appearing in the discriminant satisfies the two equations\n:<math>i_0+i_1+\\cdots+i_n=2n-2</math>\nand\n:<math>i_1+2i_2 + \\cdots+n i_n=n(n-1),</math>\nand also the equation\n:<math>ni_0 +(n-1)i_1+ \\cdots+ i_{n-1}=n(n-1),</math>\nwhich is obtained by subtracting the second equation to the first one multiplied by {{math|''n''}}.\n\nThis restricts the possible terms in the discriminant. For the general quadratic polynomial there are only two possibilities and two terms in the discriminant, while the general homogeneous polynomial of degree two in three variables has 6 terms. For the general cubic polynomial, there are five possibilities and five terms in the discriminant, while the general homogeneous polynomial of degree 4 in 5 variables has 70 terms\n\nFor higher degrees, there may monomials which satisfy above equations and do not appear in the discriminant. The first example is for the quartic polynomial {{math|''ax''<sup>4</sup> + ''bx''<sup>3</sup> + ''cx''<sup>2</sup> + ''dx'' + ''e''}}, in which case the monomial {{math|''bc''<sup>4</sup>''d''}} satisfies the equations without appearing in the discriminant.\n\n==Real roots==\nIn this section, all polynomials have [[real number|real]] coefficients.\n\nIt has been seen in {{slink||Low degrees}} that the sign of the discriminant provides a full information on the nature of the roots for polynomials of degree 2 and 3. For higher degrees, the information provided by the discriminant is less complete, but still useful. More precisely, for a polynomial of degree {{math|''n''}}, one has:\n* The polynomial has a [[multiple root]] if and only if its discriminant is zero.\n* If the discriminant is positive, the number of non-real roots is a multiple of 4. That is, there is a nonnegative integer {{math|''k'' ≤ ''n''/4}} such there are {{math|2''k''}} pairs of [[complex conjugate]] roots and {{math|''n'' − 4''k''}} real roots.\n* If the discriminant is negative, the number of non-real roots is not a multiple of 4. That is, there is a nonnegative integer {{math|''k'' ≤ (''n'' − 2)/4}} such there are {{math|2''k'' + 1}} pairs of [[complex conjugate]] roots and {{math|''n'' − 4''k'' + 2}} real roots.\n\n==Homogeneous bivariate polynomial==\n\nLet\n:<math>A(x,y) = a_0x^n+ a_1 x^{n-1}y + \\cdots + a_n y^n=\\sum_{i=0}^n a_i x^{n-i}y^i</math>\nbe a [[homogeneous polynomial]] of degree {{math|''n''}} in two indeterminates.\n\nSupposing, for the moment, that <math>a_0</math> and <math>a_n</math> are both nonzero, one has\n:<math>\\operatorname{Disc}_x(A(x,1))=\\operatorname{Disc}_y(A(1,y)).</math>\nDenoting this quantity by <math>\\operatorname{Disc}^h (A),</math>\none has\n:<math>\\operatorname{Disc}_x (A) =y^{n(n-1)} \\operatorname{Disc}^h (A),</math>\nand\n:<math>\\operatorname{Disc}_y (A) =x^{n(n-1)} \\operatorname{Disc}^h (A).</math>\n\nBecause of these properties, the quantity <math>\\operatorname{Disc}^h (A)</math> is called the ''discriminant'' or the ''homogeneous discriminant'' of {{math|''A''}}.\n\nIf <math>a_0</math> and <math>a_n</math> may be zero, the polynomials {{math|''A''(''x'', 1)}} and {{math|''A''(1, ''y'')}} may have a degree smaller than {{math|''n''}}. In this case, above formulas and definition remain valid, if the discriminants are computed as if all polynomials would have the degree {{mvar|''n''}}. This means that the discriminants must be computed with <math>a_0</math> and <math>a_n</math> indeterminate, the substitution for them of their actual values being done ''after'' this computation. Equivalently, the formulas of {{slink||Invariance under ring homomorphisms}} must be used.\n\n==Use in algebraic geometry==\n\nThe typical use of discriminants in [[algebraic geometry]] is for studying [[algebraic curve]] and, more generally [[Hypersurface|algebraic hypersurface]]s. Let {{math|''V''}} be such a curve or hypersurface; {{math|''V''}} is defined as the zero set of a [[multivariate polynomial]]. This polynomial may be considered as a univariate polynomial in one of the indeterminates, with polynomials in the other indeterminates as coefficients. The discriminant with respect to the selected indeterminate defines a hypersurface {{math|''W''}} in the space of the other indeterminates. The points of {{math|''W''}} are exactly the projection of the points of {{math|''V''}} (including the [[points at infinity]]), which either are singular or have a [[tangent space|tangent hyperplane]] that is parallel to the axis of the selected indeterminate.\n\nFor example, let {{mvar|f}} be a bivariate polynomial in {{mvar|X}} and {{mvar|Y}} with real coefficients, such that {{math|''f'' {{=}} 0}} is the implicit equation of a plane [[algebraic curve]]. Viewing {{mvar|f}} as a univariate polynomial in {{mvar|Y}} with coefficients depending on {{mvar|X}}, then the discriminant is a polynomial in {{mvar|X}} whose roots are the {{mvar|X}}-coordinates of the singular points, of the points with a tangent parallel to the {{mvar|Y}}-axis and of some of the asymptotes parallel to the {{mvar|Y}}-axis. In other words, the computation of the roots of the {{mvar|Y}}-discriminant and the {{mvar|X}}-discriminant allows one to compute all of the remarkable points of the curve, except the [[inflection point]]s.\n\n==Generalizations==\nThere are two classes of the concept of discriminant. The first class is the [[discriminant of an algebraic number field]], which, in some cases including [[quadratic field]]s, is the discriminant of a polynomial defining the field.\n\nDiscriminants of the second class arise for problems depending on coefficients, when degenerate instances or singularities of the problem are characterized by the vanishing of a single polynomial in the coefficients. This is the case for the discriminant of a polynomial, which is zero when two roots collapse. Most of the cases, where such a generalized discriminant is defined, are instances of the following.\n\nLet {{math|''A''}} be a homogeneous polynomial in {{math|''n''}} indeterminates over a field of [[characteristic (algebra)|characteristic]] 0, or of a [[prime characteristic]] that does not divides the degree of the polynomial. The polynomial {{math|''A''}} defines a [[projective hypersurface]], which has [[singular point of an algebraic variety|singular points]] if and only the {{math|''n''}} [[partial derivative]]s of {{math|''A''}} have a nontrivial common zero. This is the case if and only if the [[multivariate resultant]] of these partial derivatives is zero, and this resultant may be considered as the discriminant of {{math|''A''}}. However, because of the integer coefficients resulting of the derivation, this multivariate resultant may be divisible by a power of {{math|''n''}}, and it is better to take, as a discriminant, the [[primitive part]] of the resultant, computed with generic coefficients. The restriction on the characteristic is needed, as, otherwise, a common zero of the partial derivative is not necessarily a zero of the polynomial (see [[Euler's identity for homogeneous polynomials]]).\n\nIn the case of a homogeneous bivariate polynomial of degree {{math|''d''}}, this general discriminant is <math>d^{d-2}</math> times the discriminant defined in {{slink||Homogeneous bivariate polynomial}}. Several other classical types of discriminants, that are instances of the general definition are described in next sections.\n\n===Quadratic forms===\nA [[quadratic form]] is a function over a [[vector space]], which is defined over some [[basis (vector space)|basis]] by a [[homogeneous polynomial]] of degree 2. In [[characteristic (algebra)|characteristic]] different of 2,<ref>In characteristic 2, the discriminant of a quadratic form is not defined, and is replaced by the [[Arf invariant]].</ref> it may be written, [[w.l.o.g.]]\n:<math>Q=\\sum_{i=1}^n a_{i,i} (x_i)^2+\\sum_{1\\le i <j\\le n}a_{i,j}x_i x_j,</math>\nor, in matrix form,\n:<math>Q=X^\\mathrm T A X,</math>\nwhere {{math|''A''}} is the [[symmetric matrix]] that has <math>a_{i,j}</math> as the coefficient of the {{math|''i''th}} row and {{math|''j''}}th column, and <math>X^\\mathrm T</math>and {{math|''X''}} are, respectively, the [[row vector]] and the [[column vector]] of the <math>x_i.</math>\n\nThe '''discriminant''' or '''determinant''' of {{math|''Q''}} is the [[determinant]] of {{math|''A''}}.<ref>{{cite book | first=J. W. S. | last=Cassels | authorlink=J. W. S. Cassels | title=Rational Quadratic Forms | series=London Mathematical Society Monographs | volume=13 | publisher=[[Academic Press]] | year=1978 | isbn=0-12-163260-1 | zbl=0395.10029 | page=6 }}</ref>\n\nThe [[Hessian determinant]] of {{math|''Q''}} is <math>2^n</math> times its discriminant. The [[multivariate resultant]] of the partial derivatives of {{math|''Q''}} is equal to its Hessian determinant. So, the discriminant of a quadratic form is a special case of the above general definition of a discriminant.\n\nThe discriminant of a quadratic form is invariant under linear changes of variables (that is a change of basis of the vector space on which the quadratic form is defined) in the following sense: a linear change of variables is defined by a [[nonsingular matrix]] {{math|''S''}}, changes the matrix {{math|''A''}} into <math>S^\\mathrm T A\\,S,</math> and thus multiplies the discriminant by the square of the determinant of {{math|''S''}}. Thus the discriminant is well defined only [[up to]] the multiplication by a square. In other words, the discriminant of a quadratic form over a field {{math|''K''}} is an element of {{math|''K''/(''K''<sup>×</sup>)<sup>2</sup>}}, the [[quotient monoid|quotient]] of the multiplicative [[monoid]] of {{math|''K''}} by the [[subgroup]] of the nonzero squares (that is, two elements of {{math|''K''}} are in the same [[equivalence class]] if one is the product of the other by a nonzero square). It follows that over the [[complex number]]s, a discriminant is equivalent to 0 or 1. Over the [[real number]]s, a discriminant is equivalent to −1, 0, or 1. Over the [[rational number]]s, a discriminant is equivalent to a unique [[square-free integer]].\n\nBy a theorem of [[Carl Gustav Jacob Jacobi|Jacobi]], a quadratic form over a field of characteristic different from 2 can be expressed, after a linear change of variables, in '''diagonal form''' as\n:<math>a_1x_1^2 + \\cdots + a_nx_n^2.</math>\nMore precisely, a quadratic forms on may be expressed as a sum\n:<math>\\sum_{i=1}^n a_i L_i^2</math>\nwhere the {{math|''L''<sub>''i''</sub>}} are independent linear forms and {{mvar|n}} is the number of the variables (some of the {{math|''a''<sub>''i''</sub>}} may be zero). Equivalently, for any symmetric matrix {{math|''A''}}, there is an [[elementary matrix]] {{math|''S''}} such that <math>S^\\mathrm T A\\,S</math> is a diagonal matrix.\nThen the discriminant is the product of the {{math|''a''<sub>''i''</sub>}}, which is well-defined as a class in {{math|''K''/(''K''<sup>×</sup>)<sup>2</sup>}}.\n\nGeometrically, the discriminant of a quadratic form in three variables is the equation of a [[projective curve|quadratic projective curve]]. The discriminant is zero if and only if the curve is decomposed in lines (possibly over an [[algebraically closed extension]] of the field).\n\nA quadratic form in four variable is the equation of a [[projective surface]]. The surface has a [[singular point of an algebraic variety|singular point]] if and only its discriminant is zero. In this case, either the surface may be decomposed in planes, or it has a unique singular point, and is a [[cone]] or a [[cylinder]]. Over the reals, if the discriminant is positive, then the surface either has no real point or has everywhere a negative [[Gaussian curvature]]. If the discriminant is negative, the surface has real points, and has a negative Gaussian curvature.\n\n===Conic sections===\nA [[conic section]] is a [[plane curve]] defined by an [[implicit equation]] of the form\n:<math>ax^2+ 2bxy + cy^2 + 2dx + 2ey + f = 0,</math>\nwhere {{math|''a'', ''b'', ''c'', ''d'', ''e'', ''f''}} are real numbers.\n\nTwo [[quadratic form]]s, and thus two discriminants may be associated to a conic section.\n\nThe first quadratic form is\n:<math>ax^2+ 2bxy + cy^2 + 2dxz + 2eyz + fz^2 = 0.</math>\nIts discriminant is the [[determinant]]\n:<math>\\begin{vmatrix} a & b & d\\\\b & c & e\\\\d & e & f \\end{vmatrix}. </math>\nIt is zero if the conic section degenerates into two lines, a double line or a single point.\n\nThe second discriminant, which is the only one that is considered in many elementary textbooks, is the discriminant of the homogeneous part of degree two of the equation. It is equal to<ref>{{cite book\n|title=Math refresher for scientists and engineers\n|first1=John R.\n|last1=Fanchi\n|publisher=John Wiley and Sons\n|year=2006\n|isbn=0-471-75715-2\n|url=https://books.google.com/books?id=75mAJPcAWT8C&pg=PA45\n|at=sec. 3.2, p. 45}}\n</ref>\n\n:<math>b^2 - ac,</math>\n\nand determines the [[shape]] of the conic section. If this discriminant is negative, the curve either has no real points, or is an [[ellipse]] or a [[circle]], or, if degenerated, is reduced to a single point. If the discriminant is zero, the curve is a [[parabola]], or, if degenerated, a double line or two parallel lines. If the discriminant is positive, the curve is a [[hyperbola]], or, if degenerated, a pair of intersecting lines.\n\n===Real quadric surfaces===\nA real [[quadric surface]] in the [[Euclidean space]] of dimension three is a surface that may be defined as the zeros of a polynomial of degree two in three variables. As for the conic sections there are two discriminants that may be naturally defined. Both are useful for getting information on the nature of a quadric surface.\n\nLet <math>P(x,y,z)</math> be a polynomial of degree two in three variables that defines a real quadric surface. The first associated quadratic form, <math>Q_4,</math> depends on four variables, and is obtained by [[homogenization of a polynomial|homogenizing]] {{math|''P''}}; that is\n:<math>Q_4(x,y,z,t)=t^2P(x/t,y/t, z/t).</math>\nLet us denote its discriminant by<math>\\Delta_4.</math>\n\nThe second quadratic form, <math>Q_3,</math> depends on three variables, and consists of the terms of degree two of {{math|''P''}}; that is \n:<math>Q_3(x,y,z)=Q_4(x, y,z,0).</math>\nLet us denote its discriminant by<math>\\Delta_3.</math>\n\nIf <math>\\Delta_4>0,</math> and the surface has real points, it is either a [[hyperbolic paraboloid]] or a [[one-sheet hyperboloid]]. In both cases, this is a [[ruled surface]] that has a negative [[Gaussian curvature]] at every point.\n\nIf <math>\\Delta_4<0,</math> the surface is either an [[ellipsoid]] or a [[two-sheet hyperboloid]] or an [[elliptic paraboloid]]. In all cases, it has a positive [[Gaussian curvature]] at every point.\n\nIf <math>\\Delta_4=0,</math> the surface has a [[singular point of an algebraic variety|singular point]], possibly [[point at infinity|at infinity]]. If there is only one singular point, the surface is a [[cylinder]] or a [[conic surface|cone]]. If there are several singular points the surface consists of two planes, a double plane or a single line.\n\nWhen <math>\\Delta_4\\ne 0,</math> the sign of <math>\\Delta_3,</math> if not 0, does not provide any useful information, as changing {{math|''P''}} into {{math|−''P''}} does not change the surface, but changes the sign of <math>\\Delta_3.</math> However, if <math>\\Delta_4\\ne 0</math> and <math>\\Delta_3 = 0,</math> the surface is a [[paraboloid]], which is elliptic of hyperbolic, depending on the sign of <math>\\Delta_4.</math>\n\n===Discriminant of an algebraic number field===\n{{main article|Discriminant of an algebraic number field}}\n\n==References==\n{{reflist|30em}}\n\n==External links==\n*[http://mathworld.wolfram.com/PolynomialDiscriminant.html Mathworld article]\n*[http://planetmath.org/discriminant Planetmath article]\n\n<!-- paying only -- *[http://www.jstor.org/stable/3619560 The geometry of the discriminant of a polynomial (1996)]-->\n\n[[Category:Polynomials]]\n[[Category:Conic sections]]\n[[Category:Quadratic forms]]\n[[Category:Determinants]]\n[[Category:Algebraic number theory]]"
    },
    {
      "title": "Donaldson's theorem",
      "url": "https://en.wikipedia.org/wiki/Donaldson%27s_theorem",
      "text": "In [[mathematics]], '''Donaldson's theorem''' states that a [[definite quadratic form|definite]] [[intersection form (4-manifold)|intersection form]] of a [[simply connected]] [[smooth manifold]] of [[dimension]] 4 is [[diagonalizable matrix|diagonalisable]]. If the intersection form is positive (negative) definite, it can be diagonalized to the [[identity matrix]] (negative identity matrix) over the {{em|integers}}.\n\n==History==\nThe theorem was proved by [[Simon Donaldson]]. This was a contribution cited for his [[Fields medal]] in 1986.\n\n==Extensions==\n[[Michael Freedman]] had previously shown that any [[unimodular symmetric bilinear form]] is realized as the intersection form of some closed, oriented [[four-manifold]]. Combining this result with the [[Serre classification theorem]] and Donaldson's theorem, several interesting results can be seen:\n\n1) Any non-diagonalizable intersection form gives rise to a four-dimensional [[topological manifold]] with no [[differentiable structure]] (so cannot be smoothed).\n\n2) Two smooth simply-connected 4-manifolds are [[homeomorphic]], if and only if, their intersection forms have the same [[rank (linear algebra)|rank]], [[signature of a quadratic form|signature]], and [[parity (mathematics)|parity]].\n\n==See also==\n*[[Unimodular lattice]]\n\n==References==\n*{{Citation | last1=Donaldson | first1=S. K. | title=An application of gauge theory to four-dimensional topology |doi=10.4310/jdg/1214437665 | mr=710056 | year=1983 | journal=[[Journal of Differential Geometry]] | volume=18 | issue=2 | pages=279–315 |zbl=0507.57010 }}\n*{{citation |first=S. K. |last=Donaldson |first2=P. B. |last2=Kronheimer |year=1990 |title=The Geometry of Four-Manifolds |series=Oxford Mathematical Monographs |isbn=0-19-850269-9 }}\n*{{citation |first=D. S. |last=Freed |first2=K. |last2=Uhlenbeck |authorlink2=Karen Uhlenbeck |year=1984 |title=Instantons and Four-Manifolds |publisher=Springer }}\n*{{citation |first=M. |last=Freedman |first2=F. |last2=Quinn |year=1990 |title=Topology of 4-Manifolds |publisher=Princeton University Press }}\n*{{citation |first=A. |last=Scorpan |year=2005 |title=The Wild World of 4-Manifolds |publisher=[[American Mathematical Society]] }}\n\n[[Category:Differential topology]]\n[[Category:Theorems in topology]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "E8 lattice",
      "url": "https://en.wikipedia.org/wiki/E8_lattice",
      "text": "{{Use dmy dates|date=July 2013}}\n{{DISPLAYTITLE:E<sub>8</sub> lattice}}\nIn [[mathematics]], the '''E<sub>8</sub> lattice''' is a special [[lattice (group)|lattice]] in '''R'''<sup>8</sup>. It can be characterized as the unique positive-definite, even, [[unimodular lattice]] of rank 8. The name derives from the fact that it is the [[root lattice]] of the [[E8 (mathematics)|E<sub>8</sub> root system]].\n\nThe norm<ref name=\"norm\">In this article, the ''norm'' of a vector refers to its length squared (the square of the ordinary [[norm (mathematics)|norm]]).</ref> of the E<sub>8</sub> lattice (divided by 2) is a positive definite even unimodular [[quadratic form]] in 8 variables, and conversely such a quadratic form can be used to construct a positive-definite, even, [[unimodular lattice]] of rank 8.\nThe existence of such a form was first shown by [[H. J. S. Smith]] in 1867,<ref name=\"Smith\">{{Cite journal| last=Smith | first=H. J. S. | title= On the orders and genera of quadratic forms containing more than three indeterminates | journal=Proceedings of the Royal Society | volume=16 | year=1867 | pages=197–208 | doi= 10.1098/rspl.1867.0036}}</ref> and the first explicit construction of this quadratic form was given by [[Aleksandr Korkin|A. Korkin]] and [[Yegor Ivanovich Zolotarev|G. Zolotarev]] in 1873.<ref>{{Cite journal| last1 = Korkine | first1 = A. | last2 = Zolotareff| first2 = G. | title=Sur les formes quadratiques |journal = Mathematische Annalen | volume = 6 | pages = 366–389 | year = 1873|doi=10.1007/BF01442795}}</ref>\n<!-- Conway claims this, but I can't find it in the paper.\nIn 1877 they constructed the corresponding E<sub>8</sub> lattice explicitly as part of a study of sphere packings.<ref>{{cite journal | last = Korkine and Zolotareff | first1 = A. | last2 = Zolotareff | first2 = G. | title=Sur les formes quadratique positives |journal = Mathematische Annalen | volume = 11 | pages = 242–292 | year = 1877|doi=10.1007/BF01442667}}</ref> \n-->\nThe E<sub>8</sub> lattice is also called the '''Gosset lattice''' after [[Thorold Gosset]] who was one of the first to study the geometry of the lattice itself around 1900.<ref name=\"gosset\">{{Cite journal| last=Gosset | first=Thorold | title = On the regular and semi-regular figures in space of ''n'' dimensions | journal = [[Messenger of Mathematics]] | volume = 29 | pages = 43–48 | year = 1900}}</ref>\n\n==Lattice points==\nThe '''E<sub>8</sub> lattice''' is a [[discrete subgroup]] of '''R'''<sup>8</sup> of full rank (i.e. it spans all of '''R'''<sup>8</sup>). It can be given explicitly by the set of points Γ<sub>8</sub> ⊂ '''R'''<sup>8</sup> such that\n*all the coordinates are [[integer]]s or all the coordinates are [[half-integer]]s (a mixture of integers and half-integers is not allowed), and\n*the sum of the eight coordinates is an [[even integer]].\nIn symbols,\n\n:<math>\\Gamma_8 = \\left\\{(x_i) \\in \\mathbb Z^8 \\cup (\\mathbb Z + \\tfrac{1}{2})^8 : {\\textstyle\\sum_i} x_i \\equiv 0\\;(\\mbox{mod }2)\\right\\}.</math>\nIt is not hard to check that the sum of two lattice points is another lattice point, so that Γ<sub>8</sub> is indeed a subgroup.\n\nAn alternative description of the E<sub>8</sub> lattice which is sometimes convenient is the set of all points in Γ&prime;<sub>8</sub> ⊂ '''R'''<sup>8</sup> such that\n*all the coordinates are integers and the sum of the coordinates is even, or\n*all the coordinates are half-integers and the sum of the coordinates is odd.\nIn symbols,\n:<math>\\Gamma_8' = \\left\\{(x_i) \\in \\mathbb Z^8 \\cup (\\mathbb Z + \\tfrac{1}{2})^8 : {{\\textstyle\\sum_i} x_i} \\equiv 2x_1 \\equiv 2x_2 \\equiv 2x_3 \\equiv 2x_4 \\equiv 2x_5 \\equiv 2x_6 \\equiv 2x_7 \\equiv 2x_8\\;(\\mbox{mod }2)\\right\\}.</math>\n\n:<math>\\Gamma_8' = \\left\\{(x_i) \\in \\mathbb Z^8 : {{\\textstyle\\sum_i} x_i} \\equiv 0(\\mbox{mod }2)\\right\\} \n\\cup \\left\\{(x_i) \\in (\\mathbb Z + \\tfrac{1}{2})^8 : {{\\textstyle\\sum_i} x_i} \\equiv 1(\\mbox{mod }2)\\right\\}.</math>\n\nThe lattices Γ<sub>8</sub> and Γ&prime;<sub>8</sub> are [[isomorphic]] and one may pass from one to the other by changing the signs of any odd number of coordinates. The lattice Γ<sub>8</sub> is sometimes called the ''even coordinate system'' for E<sub>8</sub> while the lattice Γ<sub>8</sub>' is called the ''odd coordinate system''. Unless we specify otherwise we shall work in the even coordinate system.\n\n==Properties==\nThe E<sub>8</sub> lattice Γ<sub>8</sub> can be characterized as the unique lattice in '''R'''<sup>8</sup> with the following properties:\n*It is ''integral'', meaning that all scalar products of lattice elements are integer numbers.\n*It is ''[[unimodular lattice|unimodular]]'', meaning that it is integral, and can be generated by the columns of an 8&times;8 matrix with [[determinant]] ±1 (i.e. the volume of the [[fundamental parallelotope]] of the lattice is 1). Equivalently, Γ<sub>8</sub> is ''self-dual'', meaning it is equal to its [[dual lattice]].\n*It is ''even'', meaning that the norm<ref name=\"norm\"/> of any lattice vector is even.\nEven unimodular lattices can occur only in dimensions divisible by 8. In dimension 16 there are two such lattices: Γ<sub>8</sub> ⊕ Γ<sub>8</sub> and Γ<sub>16</sub> (constructed in an analogous fashion to Γ<sub>8</sub>). In dimension 24 there are 24 such lattices, called [[Niemeier lattice]]s. The most important of these is the [[Leech lattice]].\n\nOne possible basis for Γ<sub>8</sub> is given by the columns of the ([[upper triangular matrix|upper triangular]]) matrix\n:<math>\\left[\\begin{smallmatrix}\n2 & -1 & 0 & 0 & 0 & 0 & 0 & 1/2 \\\\\n0 & 1 & -1 & 0 & 0 & 0 & 0 & 1/2 \\\\\n0 & 0 & 1 & -1 & 0 & 0 & 0 & 1/2 \\\\\n0 & 0 & 0 & 1 & -1 & 0 & 0 & 1/2 \\\\\n0 & 0 & 0 & 0 & 1 & -1 & 0 & 1/2 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & -1 & 1/2 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1  & 1/2 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0  & 1/2\n\\end{smallmatrix}\\right].</math>\nΓ<sub>8</sub> is then the integral span of these vectors. All other possible bases are obtained from this one by right multiplication by elements of GL(8,'''Z''').\n\nThe shortest nonzero vectors in Γ<sub>8</sub> have norm squared 2. There are 240 such vectors.\n\n*All half-integer: (can only be ±1/2)\n**All positive or all negative: 2\n**Four positive, four negative: (8*7*6*5)/(4*3*2*1)=70\n**Two of one, six of the other: 2*(8*7)/(2*1) = 56\n*All integer: (can only be 0, ±1)\n**Two ±1, six zeroes: 4*(8*7)/(2*1)=112\n\nThese form a [[root system]] of type [[E8 (mathematics)|E<sub>8</sub>]]. The lattice Γ<sub>8</sub> is equal to the E<sub>8</sub> root lattice, meaning that it is given by the integral span of the 240 roots. Any choice of 8 [[Simple root (root system)|simple root]]s gives a basis for Γ<sub>8</sub>.\n\n==Symmetry group==\nThe [[automorphism group]] (or [[symmetry group]]) of a lattice in '''R'''<sup>''n''</sup> is defined as the subgroup of the [[orthogonal group]] O(''n'') that preserves the lattice. The symmetry group of the E<sub>8</sub> lattice is the [[Weyl group|Weyl]]/[[Coxeter group]] of type E<sub>8</sub>. This is the group generated by [[reflection (mathematics)|reflection]]s in the hyperplanes orthogonal to the 240 roots of the lattice. Its [[order (group theory)|order]] is given by\n:<math>|W(\\mathrm{E}_8)| = 696729600 = 4!\\cdot 6!\\cdot 8!.</math>\n\nThe E<sub>8</sub> Weyl group contains a subgroup of order 128·8! consisting of all [[permutation]]s of the coordinates and all even sign changes. This subgroup is the Weyl group of type D<sub>8</sub>. The full E<sub>8</sub> Weyl group is generated by this subgroup and the [[block diagonal matrix]] ''H''<sub>4</sub>⊕''H''<sub>4</sub> where ''H''<sub>4</sub> is the [[Hadamard matrix]]\n:<math>H_4 = \\tfrac{1}{2}\\left[\\begin{smallmatrix}\n1 & 1 & 1 & 1\\\\\n1 & -1 & 1 & -1\\\\\n1 & 1 & -1 & -1\\\\\n1 & -1 & -1 & 1\\\\\n\\end{smallmatrix}\\right].</math>\n\n==Geometry==\n: See [[5 21 honeycomb|5<sub>21</sub> honeycomb]]\n\nThe E<sub>8</sub> lattice points are the vertices of the [[5 21 honeycomb|5<sub>21</sub>]] honeycomb, which is composed of regular [[8-simplex]] and [[8-orthoplex]] [[Facet (geometry)|facets]]. This honeycomb was first studied by Gosset who called it a ''9-ic semi-regular figure''<ref name=\"gosset\"/> (Gosset regarded honeycombs in ''n'' dimensions as degenerate ''n''+1 polytopes). In [[H. S. M. Coxeter|Coxeter's]] notation,<ref name=\"coxeter\">{{Cite book| first = H. S. M. | last = Coxeter | authorlink = H. S. M. Coxeter | year = 1973 | title = [[Regular Polytopes (book)|Regular Polytopes]] | edition = (3rd ed.) | publisher  = Dover Publications | location = New York | isbn = 0-486-61480-8}}</ref> Gosset's honeycomb is denoted by 5<sub>21</sub> and has the [[Coxeter-Dynkin diagram]]:\n:{{CDD|nodea_1|3a|nodea|3a|nodea|3a|nodea|3a|nodea|3a|branch|3a|nodea|3a|nodea}}\n\nThis honeycomb is highly regular in the sense that its symmetry group (the affine <math>{\\tilde{E}}_8</math> Weyl group) acts transitively on the [[face (geometry)|''k''-faces]] for ''k'' ≤ 6. All of the ''k''-faces for ''k'' ≤ 7 are simplices.\n\nThe [[vertex figure]] of Gosset's honeycomb is the semiregular [[E8 polytope|E<sub>8</sub> polytope]] (4<sub>21</sub> in Coxeter's notation) given by the [[convex hull]] of the 240 roots of the E<sub>8</sub> lattice.\n\nEach point of the E<sub>8</sub> lattice is surrounded by 2160 8-orthoplexes and 17280 8-simplices. The 2160 deep holes near the origin are exactly the halves of the norm 4 lattice points. The 17520 norm 8 lattice points fall into two classes (two [[orbit (group theory)|orbit]]s under the action of the E<sub>8</sub> automorphism group): 240 are twice the norm 2 lattice points while 17280 are 3 times the shallow holes surrounding the origin.\n\nA [[lattice hole|hole]] in a lattice is a point in the ambient Euclidean space whose distance to the nearest lattice point is a [[local maximum]]. (In a lattice defined as a [[uniform honeycomb]] these points correspond to the centers of the [[Facet (geometry)|facets]] volumes.) A deep hole is one whose distance to the lattice is a global maximum. There are two types of holes in the E<sub>8</sub> lattice:\n*''Deep holes'' such as the point (1,0,0,0,0,0,0,0) are at a distance of 1 from the nearest lattice points. There are 16 lattice points at this distance which form the vertices of an [[8-orthoplex]] centered at the hole (the [[Delaunay cell]] of the hole).\n*''Shallow holes'' such as the point <math>(\\tfrac{5}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6})</math> are at a distance of <math>\\tfrac{2\\sqrt 2}{3}</math> from the nearest lattice points. There are 9 lattice points at this distance forming the vertices of an [[8-simplex]] centered at the hole.\n\n==Sphere packings and kissing numbers==\nThe E<sub>8</sub> lattice is remarkable in that it gives optimal solutions to the [[sphere packing problem]] and the [[kissing number problem]] in 8 dimensions.\n\nThe [[sphere packing problem]] asks what is the densest way to pack (solid) ''n''-dimensional spheres of a fixed radius in '''R'''<sup>''n''</sup> so that no two spheres overlap. Lattice packings are special types of sphere packings where the spheres are centered at the points of a lattice. Placing spheres of radius 1/{{radic|2}} at the points of the E<sub>8</sub> lattice gives a lattice packing in '''R'''<sup>8</sup> with a density of\n:<math>\\frac{\\pi^4}{2^4 4!} \\cong 0.25367.</math>\nIt has long been known that this is the maximum density that can be achieved by a lattice packing in 8 dimensions.<ref>{{Cite journal| first = H. F. | last = Blichfeldt | authorlink=Hans Frederick Blichfeldt | year = 1935 | title = The minimum values of positive quadratic forms in six, seven and eight variables | journal = Mathematische Zeitschrift | volume = 39 | pages = 1–15 | doi = 10.1007/BF01201341 | zbl=0009.24403 }}</ref> Furthermore, the E<sub>8</sub> lattice is the unique lattice (up to isometries and rescalings) with this density.<ref>{{cite conference | first = N. M. | last = Vetčinkin | title = Uniqueness of classes of positive quadratic forms on which values of the Hermite constant are attained for 6 &le; ''n'' &le; 8 | booktitle = Geometry of positive quadratic forms | publisher = Trudy Math. Inst. Steklov | volume = 152 | year = 1980 | pages = 34&ndash;86}}</ref> The mathematician [[Maryna Viazovska]] has recently shown that this density is, in fact, optimal even among irregular packings.<ref name=\"quanta\">{{citation|last1=Klarreich|first1=Erica|authorlink1=Erica Klarreich|title=Sphere Packing Solved in Higher Dimensions|url=https://www.quantamagazine.org/20160330-sphere-packing-solved-in-higher-dimensions|magazine=Quanta Magazine|date=March 30, 2016}}</ref><ref>{{cite arXiv| first1 = Maryna | last1 = Viazovska |authorlink1 = Maryna Viazovska| year = 2016 | title = The sphere packing problem in dimension 8 | eprint = 1603.04246 }}</ref>\n\nThe [[kissing number problem]] asks what is the maximum number of spheres of a fixed radius that can touch (or \"kiss\") a central sphere of the same radius. In the E<sub>8</sub> lattice packing mentioned above any given sphere touches 240 neighboring spheres. This is because there are 240 lattice vectors of minimum nonzero norm (the roots of the E<sub>8</sub> lattice). It was shown in 1979 that this is the maximum possible number in 8 dimensions.<ref>{{Cite journal| first = V. I. | last = Levenshtein | title = On bounds for packing in ''n''-dimensional Euclidean space | journal = Soviet Mathematics - Doklady | volume = 20 | year = 1979 | pages = 417–421}}</ref><ref>{{Cite journal| first1 = A. M. | last1 = Odlyzko | author1-link=Andrew Odlyzko | last2=Sloane | first2=N. J. A. | author2-link=Neil Sloane | title = New bounds on the number of unit spheres that can touch a unit sphere in ''n'' dimensions | journal = Journal of Combinatorial Theory | volume = A26 | year = 1979 | pages = 210–214 | zbl=0408.52007 | doi = 10.1016/0097-3165(79)90074-8 }} This is also Chapter 13 of Conway and Sloane (1998).</ref>\n\nThe sphere packing problem and the kissing number problem are remarkably difficult and optimal solutions are only known in 1, 2, 3, 8, and 24 dimensions (plus dimension 4 for the kissing number problem). The fact that solutions are known in dimensions 8 and 24 follows in part from the special properties of the E<sub>8</sub> lattice and its 24-dimensional cousin, the [[Leech lattice]].\n\n==Theta function==\nOne can associate to any (positive-definite) lattice Λ a [[theta function]] given by\n:<math>\\Theta_\\Lambda(\\tau) = \\sum_{x\\in\\Lambda}e^{i\\pi\\tau\\|x\\|^2}\\qquad\\mathrm{Im}\\,\\tau > 0.</math>\nThe theta function of a lattice is then a [[holomorphic function]] on the [[upper half-plane]]. Furthermore, the theta function of an even unimodular lattice of rank ''n'' is actually a [[modular form]] of weight ''n''/2. The theta function of an integral lattice is often written as a power series in <math>q = e^{i\\pi\\tau}</math> so that the coefficient of ''q''<sup>''n''</sup> gives the number of lattice vectors of norm ''n''.\n\nUp to normalization, there is a unique modular form of weight 4: the [[Eisenstein series]] ''G''<sub>4</sub>(τ). The theta function for the E<sub>8</sub> lattice must then be proportional to ''G''<sub>4</sub>(τ). The normalization can be fixed by noting that there is a unique vector of norm 0. This gives\n:<math>\\Theta_{\\Gamma_8}(\\tau) = 1 + 240\\sum_{n=1}^\\infty \\sigma_3(n) q^{2n}</math>\nwhere σ<sub>3</sub>(''n'') is the [[divisor function]]. It follows that the number of E<sub>8</sub> lattice vectors of norm 2''n'' is 240 times the sum of the cubes of the divisors of ''n''. The first few terms of this series are given by {{OEIS|id=A004009}}:\n:<math>\\Theta_{\\Gamma_8}(\\tau) = 1 + 240\\,q^2 + 2160\\,q^4 + 6720\\,q^6 + 17520\\,q^8 + 30240\\, q^{10} + 60480\\,q^{12} + O(q^{14}).</math>\nThe E<sub>8</sub> theta function may be written in terms of the [[Jacobi theta function]]s  as follows:\n:<math>\\Theta_{\\Gamma_8}(\\tau) = \\frac{1}{2}\\left(\\theta_2(q)^8 + \\theta_3(q)^8 + \\theta_4(q)^8\\right)</math>\nwhere\n:<math>\n\\theta_2(q) = \\sum_{n=-\\infty}^{\\infty}q^{(n+\\frac{1}{2})^2}\\qquad\n\\theta_3(q) = \\sum_{n=-\\infty}^{\\infty}q^{n^2}\\qquad\n\\theta_4(q) = \\sum_{n=-\\infty}^{\\infty}(-1)^n q^{n^2}.\n</math>\n\n==Other constructions==\n\n===Hamming code===\nThe E<sub>8</sub> lattice is very closely related to the (extended) [[Hamming code]] ''H''(8,4) and can, in fact, be constructed from it. The Hamming code ''H''(8,4) is a [[linear code|binary code]] of length 8 and rank 4; that is, it is a 4-dimensional subspace of the finite vector space ('''F'''<sub>2</sub>)<sup>8</sup>. Writing elements of ('''F'''<sub>2</sub>)<sup>8</sup> as 8-bit integers in [[hexadecimal]], the code ''H''(8,4) can by given explicitly as the set\n:{00, 0F, 33, 3C, 55, 5A, 66, 69, 96, 99, A5, AA, C3, CC, F0, FF}.\nThe code ''H''(8,4) is significant partly because it is a [[Self-dual code|Type II self-dual code]]. It has a minimum [[Hamming weight]] 4, meaning that any two codewords differ by at least 4 bits. It is the largest length 8 binary code with this property.\n\nOne can construct a lattice Λ from a binary code ''C'' of length ''n'' by taking the set of all vectors ''x'' in '''Z'''<sup>''n''</sup> such that ''x'' is congruent (modulo 2) to a codeword of ''C''.<ref>This is the so-called \"Construction A\" in Conway and Sloane (1998). See &sect;2 of Ch. 5.</ref> It is often convenient to rescale Λ by a factor of 1/{{radic|2}},\n\n:<math>\\Lambda = \\tfrac{1}{\\sqrt 2}\\left\\{x \\in \\mathbb Z^n : x\\,\\bmod\\,2 \\in C\\right\\}.</math>\n\nApplying this construction a Type II self-dual code gives an even, unimodular lattice. In particular, applying it to the Hamming code ''H''(8,4) gives an E<sub>8</sub> lattice. It is not entirely trivial, however, to find an explicit isomorphism between this lattice and the lattice Γ<sub>8</sub> defined above.\n\n===Integral octonions===\nThe E<sub>8</sub> lattice is also closely related to the [[Algebra over a field#Non-associative algebras|nonassociative algebra]] of real [[octonion]]s '''O'''. It is possible to define the concept of an [[integral octonion]] analogous to that of an [[integral quaternion]]. The integral octonions naturally form a lattice inside '''O'''. This lattice is just a rescaled E<sub>8</sub> lattice. (The minimum norm in the integral octonion lattice is 1 rather than 2). Embedded in the octonions in this manner the E<sub>8</sub> lattice takes on the structure of a [[nonassociative ring]].\n\nFixing a basis (1, ''i'', ''j'', ''k'', ℓ, ℓ''i'', ℓ''j'', ℓ''k'')  of unit octonions,\none can define the integral octonions as a [[maximal order]] containing this basis. (One must, of course, extend the definitions of ''order'' and ''ring'' to include the nonassociative case). This amounts to finding the largest [[subring]] of '''O''' containing the units on which the expressions ''x''*''x'' (the norm of ''x'') and ''x'' + ''x''* (twice the real part of ''x'') are integer-valued. There are actually seven such maximal orders, one corresponding to each of the seven imaginary units. However, all seven maximal orders are isomorphic. One such maximal order is generated by the octonions ''i'', ''j'', and ½ (''i'' + ''j'' + ''k'' + ℓ).\n\nA detailed account of the integral octonions and their relation to the E<sub>8</sub> lattice can be found in Conway and Smith (2003).\n\n====Example definition of integral octonions====\nConsider octonion multiplication defined by triads: 137, 267, 457, 125, 243, 416, 356. Then integral octonions form vectors:\n\n1) <math>\\pm e_i</math>,  i=0, 1, ..., 7\n\n2) <math>\\pm e_0\\pm e_a\\pm e_b\\pm e_c</math>,  indexes abc run through the seven triads 124, 235, 346, 457, 561, 672, 713\n\n3) <math>\\pm e_p\\pm e_q\\pm e_r\\pm e_s</math>,  indexes pqrs run through the seven tetrads 3567, 1467, 1257, 1236, 2347, 1345, 2456.\n\nImaginary octonions in this set, namely 14 from 1) and 7*16=112 from 3), form the roots of the Lie algebra <math>E_7</math>. Along with the remaining 2+112 vectors we obtain 240 vectors that form roots of Lie algebra <math>E_8</math>. See the Koca work on this subject.<ref>Mehmet Koca, Ramazan Koc, Nazife O. Koca, The Chevalley group <math>G_{2}(2)</math> of order 12096 and the octonionic root system of <math>E_{7}</math>,Linear Algebra and its Applications Volume 422, Issues 2-3, 15 April 2007, Pages 808-823 [https://arxiv.org/abs/hep-th/0509189]</ref>\n\n==Applications==\nIn 1982 [[Michael Freedman]] produced an example of a topological [[4-manifold]], called the [[E8 manifold|E<sub>8</sub> manifold]], whose [[Intersection form (4-manifold)|intersection form]] is given by the E<sub>8</sub> lattice. This manifold is an example of a topological manifold which admits no [[smooth structure]] and is not even [[triangulation (topology)|triangulable]].\n\nIn [[string theory]], the [[heterotic string]] is a peculiar hybrid of a 26-dimensional [[bosonic string]] and a 10-dimensional [[superstring]]. In order for the theory to work correctly, the 16 mismatched dimensions must be compactified on an even, unimodular lattice of rank 16. There are two such lattices: Γ<sub>8</sub>⊕Γ<sub>8</sub> and Γ<sub>16</sub> (constructed in a fashion analogous to that of Γ<sub>8</sub>). These lead to two version of the heterotic string known as the E<sub>8</sub>&times;E<sub>8</sub> heterotic string and the SO(32) heterotic string.\n\n==See also==\n*[[Leech lattice]]\n*[[E8 (mathematics)|E<sub>8</sub> (mathematics)]]\n*[[Semiregular E-polytope]]\n\n==References==\n{{Reflist}}\n\n*{{Cite book |last1= Conway |first1= John H. |authorlink1=John Horton Conway |last2=Sloane |first2=Neil J. A. |authorlink2=Neil Sloane | year = 1998 | title = Sphere Packings, Lattices and Groups | edition=3rd | publisher = Springer-Verlag | location = New York | isbn = 0-387-98585-9}}\n*{{Cite book |last1= Conway |first1= John H. |authorlink1=John Horton Conway |last2=Smith |first2=Derek A. | title = On Quaternions and Octonions | publisher = AK Peters, Ltd | location = Natick, Massachusetts | year = 2003 | isbn = 1-56881-134-9}} Chapter 9 contains a discussion of the integral octonions and the E<sub>8</sub> lattice.\n\n{{DEFAULTSORT:E8 Lattice}}\n[[Category:Lattice points]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Ε-quadratic form",
      "url": "https://en.wikipedia.org/wiki/%CE%95-quadratic_form",
      "text": "{{Lowercase}}\n{{DISPLAYTITLE:ε-quadratic form}}\nIn [[mathematics]], specifically the theory of [[quadratic form]]s, an '''ε-quadratic form''' is a generalization of quadratic forms to skew-symmetric settings and to [[*-ring]]s; ε = ±1, accordingly for symmetric or skew-symmetric. They are also called <math>(-)^n</math>-quadratic forms, particularly in the context of [[surgery theory]].\n\nThere is the related notion of '''ε-symmetric forms''', which generalizes [[symmetric bilinear form|symmetric forms]], [[skew-symmetric form]]s (= [[symplectic form]]s), [[Hermitian form]]s, and [[skew-Hermitian form]]s. More briefly, one may refer to quadratic, skew-quadratic, symmetric, and skew-symmetric forms, where \"skew\" means (&minus;) and the * (involution) is implied.\n\nThe theory is 2-local: [[away from 2]], ε-quadratic forms are equivalent to ε-symmetric forms: half the symmetrization map (below) gives an explicit isomorphism.\n\n==Definition==\nε-symmetric forms and ε-quadratic forms are defined as follows.<ref>{{Cite arxiv |eprint = math/0111315|last1 = Ranicki|first1 = Andrew|title = Foundations of algebraic surgery|year = 2001}}</ref>\n\nGiven a module ''M'' over a [[*-ring]] ''R'', let ''B(M)'' be the space of [[bilinear form]]s on ''M'', and let ''T'': ''B(M)'' → ''B(M)'' be the \"[[conjugate transpose]]\" [[Involution (mathematics)|involution]] ''B(u,v)'' ↦ ''B(v,u)''*. Since multiplication by −1 is also an involution and commutes with linear maps, −''T'' is also an involution. Thus we can write ε = ±1 and ε''T'' is an involution, either ''T'' or −''T'' (ε can be more general than ±1; see below). Define the '''ε-symmetric forms''' as the [[invariant (mathematics)|invariants]] of ε''T'', and the '''ε-quadratic forms''' are the [[coinvariant]]s.\n\nAs an exact sequence,\n:<math>0 \\to Q^\\epsilon(M) \\to B(M) \\stackrel{1-\\epsilon T}{\\longrightarrow} B(M) \\to Q_\\epsilon(M) \\to 0 </math>\nAs [[kernel (algebra)|kernel]] and [[cokernel]],\n:<math>Q^\\epsilon(M) := \\mbox{ker}\\,(1-\\epsilon T)</math>\n:<math>Q_\\epsilon(M) := \\mbox{coker}\\,(1-\\epsilon T)</math>\n\nThe notation ''Q''<sup>ε</sup>(''M''), ''Q''<sub>ε</sub>(''M'') follows the standard notation ''M<sup>G</sup>'', ''M<sub>G</sub>'' for the invariants and coinvariants for a [[Group action (mathematics)|group action]], here of the order 2 group (an involution).\n\nComposition of the inclusion and quotient maps (but not 1 − ε''T'') as <math>Q^\\epsilon(M) \\to B(M) \\to Q_\\epsilon(M)</math> yields a map ''Q''<sup>ε</sup>(''M'') → ''Q''<sub>ε</sub>(''M''): every ε-symmetric form determines an ε-quadratic form.\n\n===Symmetrization===\nConversely, one can define a reverse homomorphism \"1 + ε''T''\": ''Q''<sub>ε</sub>(''M'') → ''Q''<sup>ε</sup>(''M''), called the '''symmetrization map''' (since it yields a symmetric form) by taking any lift of a quadratic form and multiplying it by 1 + ε''T''. This is a symmetric form because (1 − ε''T'')(1 + ε''T'') = 1 − ''T''<sup>2</sup> = 0, so it is in the kernel. More precisely, <math>(1 + \\epsilon T)B(M) < Q^\\epsilon(M)</math>. The map is well-defined by the same equation: choosing a different lift corresponds to adding a multiple of (1 − ε''T''), but this vanishes after multiplying by (1 + ε''T''). Thus every ε-quadratic form determines an ε-symmetric form.\n\nComposing these two maps either way: ''Q''<sup>ε</sup>(''M'') → ''Q''<sub>ε</sub>(''M'') → ''Q''<sup>ε</sup>(''M'') or ''Q''<sub>ε</sub>(''M'') → ''Q''<sup>ε</sup>(''M'') → ''Q''<sub>ε</sub>(''M'') yields multiplication by 2, and thus these maps are bijective if 2 is invertible in ''R'', with the inverse given by multiplication with 1/2.\n\nAn ε-quadratic form ψ ∈ ''Q''<sub>ε</sub>(''M'') is called '''non-degenerate''' if the associated ε-symmetric form (1 + ε''T'')(ψ) is non-degenerate.\n\n===Generalization from *===\nIf the * is trivial, then ε = ±1, and \"away from 2\" means that 2 is invertible: 1/2 ∈ ''R''.\n\nMore generally, one can take for ε ∈ ''R'' any element such that ε*ε =1. ε = ±1 always satisfy this, but so does any element of norm 1, such as complex numbers of unit norm.\n\nSimilarly, in the presence of a non-trivial *, ε-symmetric forms are equivalent to ε-quadratic forms if there is an element λ ∈ ''R'' such that λ* + λ = 1. If * is trivial, this is equivalent to 2λ = 1 or λ = 1/2, while if * is non-trivial there can be multiple possible λ; for example, over the complex numbers any number with real part 1/2 is such a λ.\n\nFor instance, in the ring <math>R=\\mathbf{Z}\\left[\\textstyle{\\frac{1+i}{2}}\\right]</math> (the integral lattice for the quadratic form 2''x''<sup>2</sup> − 2''x''+1), with complex conjugation, <math>\\lambda=\\textstyle{\\frac{1\\pm i}{2}}</math> are two such elements, though 1/2 ∉ ''R''.\n\n==Intuition==\n<!-- Fix for epsilon and * -->\nIn terms of matrices (we take ''V'' to be 2-dimensional), if * is trivial:\n* matrices <math>\\begin{pmatrix}a & b\\\\c & d\\end{pmatrix}</math> correspond to bilinear forms\n* the subspace of symmetric matrices <math>\\begin{pmatrix}a & b\\\\b & c\\end{pmatrix}</math> correspond to symmetric forms\n* the subspace of (&minus;1)-symmetric matrices <math>\\begin{pmatrix}0 & b\\\\-b & 0\\end{pmatrix}</math> correspond to [[symplectic form]]s\n* the bilinear form <math>\\begin{pmatrix}a & b\\\\c & d\\end{pmatrix}</math> yields the quadratic form\n::<math>ax^2 + bxy+cyx + dy^2 = ax^2 + (b+c)xy + dy^2\\, </math>,\n* the map 1 + T from quadratic forms to symmetric forms maps <math>ex^2 + fxy + gy^2</math>\nto <math>\\begin{pmatrix}2e & f\\\\f & 2g\\end{pmatrix}</math>, for example by lifting to <math>\\begin{pmatrix}e & f\\\\0 & g\\end{pmatrix}</math> and then adding to transpose. Mapping back to quadratic forms yields double the original: <math>2ex^2 + 2fxy + 2gy^2 = 2(ex^2 + fxy + gy^2)</math>.\n\nIf <math>\\bar{\\cdot } </math> is complex conjugation, then\n* the subspace of symmetric matrices are the [[Hermitian matrices]] <math>\\begin{pmatrix}a & z\\\\ \\bar z & c\\end{pmatrix}</math>\n* the subspace of skew-symmetric matrices are the [[skew-Hermitian matrices]] <math>\\begin{pmatrix}bi & z\\\\ -\\bar z & di\\end{pmatrix}</math>\n\n===Refinements===\nAn intuitive way to understand an ε-quadratic form is to think of it as a '''quadratic refinement''' of its associated ε-symmetric form.\n\nFor instance, in defining a [[Clifford algebra]] over a general field or ring, one quotients the [[tensor algebra]] by relations coming from the [[symmetric bilinear form|symmetric form]] ''and'' the quadratic form: ''vw'' + ''wv'' = 2''B(v,w)'' and <math>v^2=Q(v)</math>. If 2 is invertible, this second relation follows from the first (as the quadratic form can be recovered from the associated bilinear form), but at 2 this additional refinement is necessary.\n\n==Examples==\nAn easy example for an ε-quadratic form is the '''standard hyperbolic ε-quadratic form''' <math>H_\\epsilon(R) \\in Q_\\epsilon(R \\oplus R^*)</math>. (Here, ''R''* := Hom<sub>''R''</sub>(''R,R'') denotes the dual of the ''R''-module ''R''.) It is given by the bilinear form <math>((v_1,f_1),(v_2,f_2)) \\mapsto f_2(v_1)</math>. The standard hyperbolic ε-quadratic form is needed for the definition of [[L-theory]].\n\nFor the field of two elements ''R'' = '''F'''<sub>2</sub> there is no difference between (+1)-quadratic and (&minus;1)-quadratic forms, which are just called [[quadratic form]]s. The [[Arf invariant]] of a [[nonsingular]] quadratic form over '''F'''<sub>2</sub> is an '''F'''<sub>2</sub>-valued invariant with important applications in both algebra and topology, and plays a role similar to that played by the [[Discriminant#Discriminant of a quadratic form|discriminant of a quadratic form]] in characteristic not equal to two.\n\n=== Manifolds ===\n{{further|Intersection product}}\nThe free part of the middle [[homology group]] (with integer coefficients) of an oriented even-dimensional manifold has an ε-symmetric form, via [[Poincaré duality]], the [[intersection product|intersection form]]. In the case of [[singly even]] dimension <math>4k + 2,</math> this is skew-symmetric, while for [[doubly even]] dimension <math>4k,</math> this is symmetric. Geometrically this corresponds to intersection, where two ''n''/2-dimensional submanifolds in an ''n''-dimensional manifold generically intersect in a 0-dimensional submanifold (a set of points), by adding [[codimension]]. For singly even dimension the order switches sign, while for doubly even dimension order does not change sign, hence the ε-symmetry. The simplest cases are for the product of spheres, where the product <math>S^{2k} \\times S^{2k}</math> and <math>S^{2k+1} \\times S^{2k+1}</math> respectively give the symmetric form <math>\\left(\\begin{smallmatrix} 0 & 1\\\\ 1 & 0\\end{smallmatrix}\\right)</math> and skew-symmetric form <math>\\left(\\begin{smallmatrix} 0 & 1\\\\ -1 & 0\\end{smallmatrix}\\right).</math> In dimension two, this yields a torus, and taking the [[connected sum]] of ''g'' tori yields the surface of genus ''g,'' whose middle homology has the standard hyperbolic form.\n\nWith additional structure, this ε-symmetric form can be refined to an ε-quadratic form. For doubly even dimension this is integer valued, while for singly even dimension this is only defined up to parity, and takes values in '''Z'''/2. For example, given a [[framed manifold]], one can produce such a refinement. For singly even dimension, the Arf invariant of this skew-quadratic form is the [[Kervaire invariant]].\n\nGiven an oriented surface Σ embedded in '''R'''<sup>3</sup>, the middle homology group ''H''<sub>1</sub>(Σ) carries not only a skew-symmetric form (via intersection), but also a skew-quadratic form, which can be seen as a quadratic refinement, via self-linking. The skew-symmetric form is an invariant of the surface Σ, whereas the skew-quadratic form is an invariant of the embedding Σ ⊂ '''R'''<sup>3</sup>, e.g. for the [[Seifert surface]] of a [[knot (mathematics)|knot]]. The [[Arf invariant]] of the skew-quadratic form is a framed [[cobordism]] invariant generating the first stable [[homotopy group]] <math>\\pi^s_1</math>.\n\n[[File:Hopfkeyrings.jpg|thumb|In the standard embedding of the torus, a (1,1) curve self-links, thus <math>Q(1,1)=1</math>.]]\nFor the standard embedded [[torus]], the skew-symmetric form is given by <math>\\begin{pmatrix}0 & 1\\\\-1 & 0\\end{pmatrix}</math> (with respect to the standard [[symplectic basis]]), and the skew-quadratic refinement is given by ''xy'' with respect to this basis: ''Q''(1,0) = ''Q''(0,1)=0: the basis curves don't self-link; and ''Q''(1,1) = 1: a (1,1) self-links, as in the [[Hopf fibration]]. (This form has [[Arf invariant]] 0, and thus this embedded torus has [[Kervaire invariant]] 0.)\n\n==Applications==\nA key application is in algebraic [[surgery theory]], where even [[L-theory|L-groups]] are defined as [[Witt group]]s of ε-quadratic forms, by [[C.T.C. Wall|C.T.C.Wall]]\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Epsilon-Quadratic Form}}\n[[Category:Quadratic forms]]\n[[Category:Surgery theory]]"
    },
    {
      "title": "Eutactic lattice",
      "url": "https://en.wikipedia.org/wiki/Eutactic_lattice",
      "text": "In mathematics, a '''eutactic lattice''' (or '''eutactic form''') is a [[lattice (group)|lattice]] in [[Euclidean space]] whose minimal vectors form a [[eutactic star]]. This means they have a set of positive '''eutactic coefficients''' ''c''<sub>''i''</sub> such that (''v'',&nbsp;''v'')&nbsp;=&nbsp;Σ''c''<sub>''i''</sub>(''v'',&nbsp;''m''<sub>''i''</sub>)<sup>2</sup> where the sum is over the minimal vectors  ''m''<sub>''i''</sub>.  \"Eutactic\" is derived from the Greek language, and means \"well-situated\" or \"well-arranged\".\n\n{{harvtxt|Voronoi|1908}} proved that a lattice is extreme if and only if it is both [[perfect lattice|perfect]] and eutactic. \n\n{{harvtxt|Conway|Sloane|1988}} summarize the properties of eutactic lattices of dimension up to 7.\n\n==References==\n\n*{{Citation | last1=Conway | first1=John Horton | author1-link=John Horton Conway | last2=Sloane | first2=N. J. A. | author2-link=Neil Sloane | title=Low-dimensional lattices. III. Perfect forms | jstor=2398316 | mr=953277 | year=1988 | journal=Proceedings of the Royal Society. London. Series A. Mathematical, Physical and Engineering Sciences | issn=0962-8444 | volume=418 | issue=1854 | pages=43–80 | doi=10.1098/rspa.1988.0073}}\n**{{Citation |jstor=2398351 |pages=441 |last1=Conway |first1=J. H. |last2=Sloane |first2=N. J. A. |title=Errata: Low-Dimensional Lattices. III. Perfect Forms |volume=426 |issue=1871 |journal=Proceedings of the Royal Society of London |year=1989 |doi=10.1098/rspa.1989.0134 |postscript=.}}\n*{{Citation | last1=Coxeter | first1=Harold Scott MacDonald | author1-link=Harold Scott MacDonald Coxeter | title=Extreme forms | url=http://www.math.ca/10.4153/CJM-1951-045-8 | doi=10.4153/CJM-1951-045-8 | mr=0044580 | year=1951 | journal=[[Canadian Journal of Mathematics]] | issn=0008-414X | volume=3 | pages=391–441}}\n*{{Citation | last1=Korkine | last2=Zolotareff | title=Sur les formes quadratique positives | doi=10.1007/BF01442667 | year=1877 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=11 | pages=242–292 | first1=A. | first2=G. | issue=2}}\n*{{Citation | last1=Martinet | first1=Jacques | title=Perfect lattices in Euclidean spaces | url=https://books.google.com/books?id=gd9CcFclBRIC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] | isbn=978-3-540-44236-3 | mr=1957723 | year=2003 | volume=327}}\n*{{Citation | last1=Voronoi | first1=G. |authorlink=Georgy Voronoy | title=Nouvelles applications des paramètres continus à la théorie des formes quadratiques. Premier Mémoire: Sur quelques propriétés des formes quadratiques positives parfaites | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166534 | language=French | doi=10.1515/crll.1908.133.97 | year=1908 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=133 | pages=97–178 | issue=133}}\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Proofs of Fermat's theorem on sums of two squares",
      "url": "https://en.wikipedia.org/wiki/Proofs_of_Fermat%27s_theorem_on_sums_of_two_squares",
      "text": "{{Use dmy dates|date=July 2013}}\n[[Fermat's theorem on sums of two squares]] asserts that an odd [[prime number]] ''p'' can be expressed as\n\n: <math>p = x^2 + y^2</math>\n\nwith [[integer]] ''x'' and ''y'' if and only if ''p'' is [[Modular arithmetic|congruent]] to 1 (mod 4). The statement was announced by [[Albert Girard|Girard]] in 1625, and again by [[Pierre de Fermat|Fermat]] in 1640, but neither supplied a proof.\n\nThe \"only if\" clause is easy: a [[Square number|perfect square]] is congruent to 0 or 1 modulo 4, hence a sum of two squares is congruent to 0, 1, or 2. An odd prime number is congruent to either 1 or 3 modulo 4, and the second possibility has just been ruled out. The first proof that such a representation exists was given by [[Leonhard Euler]] in 1747 and was complicated. Since then, many different proofs have been found. Among them, the proof using [[Minkowski's theorem]] about [[convex set]]s<ref>See Goldman's book, §22.5</ref> and [[Don Zagier]]'s short proof based on involutions have appeared.\n\n==Euler's proof by infinite descent==\n[[Leonhard Euler|Euler]] succeeded in proving Fermat's theorem on sums of two squares in 1749, when he was forty-two years old. He communicated this in a letter to [[Christian Goldbach|Goldbach]] dated 12 April 1749.<ref>[http://www.math.dartmouth.edu/~euler/correspondence/letters/OO0852.pdf Euler à Goldbach, lettre CXXV]</ref> The proof relies on [[Proof by infinite descent|infinite descent]], and is only briefly sketched in the letter. The full proof consists in five steps and is published in two papers. The first four steps are Propositions 1 to 4 of the first paper<ref>De numerus qui sunt aggregata duorum quadratorum. (Novi commentarii academiae scientiarum Petropolitanae 4 (1752/3), 1758, 3-40) [http://www.math.dartmouth.edu/~euler/docs/originals/E228.pdf]</ref> and do not correspond exactly to the four steps below. The fifth step below is from the second paper.<ref>Demonstratio theorematis FERMATIANI omnem numerum primum formae 4n+1 esse summam duorum quadratorum. (Novi commentarii academiae scientiarum Petropolitanae 5 (1754/5), 1760, 3-13) [http://www.math.dartmouth.edu/~euler/docs/originals/E241.pdf]</ref> <ref>The summary is based on Edwards book, pages 45-48.</ref>\n\nFor the avoidance of ambiguity, zero will always be a valid possible constituent of \"sums of two squares\", so for example every square of an integer is trivially expressible as the sum of two squares by setting one of them to be zero.  \n\n1. ''The product of two numbers, each of which is a sum of two squares, is itself a sum of two squares.''\n\n::This is a well known property, based on the identity\n\n:::<math>(a^2+b^2)(p^2+q^2) = (ap+bq)^2 + (aq-bp)^2</math>\n\n::due to [[Brahmagupta–Fibonacci identity|Diophantus]].\n\n2. ''If a number which is a sum of two squares is divisible by a prime which is a sum of two squares, then the quotient is a sum of two squares.''\n(This is Euler's first Proposition).\n\n::Indeed, suppose for example that <math>a^2 + b^2</math> is divisible by <math>p^2+q^2</math> and that this latter is a prime. Then <math>p^2 + q^2</math> divides\n\n:::<math>(pb-aq)(pb+aq) = p^2b^2 - a^2q^2 = p^2(a^2+b^2) - a^2(p^2+q^2).</math>\n\n::Since <math>p^2+q^2</math> is a prime, it divides one of the two factors. Suppose that it divides <math>pb-aq</math>. Since\n\n:::<math>(a^2+b^2)(p^2+q^2) = (ap+bq)^2 + (aq-bp)^2</math>\n\n::(Diophantus's identity) it follows that <math>p^2+q^2</math> must divide <math>(ap+bq)^2</math>. So the equation can be divided by the square of <math>p^2+q^2</math>. Dividing the expression by <math>(p^2+q^2)^2</math> yields:\n\n:::<math>\\frac{a^2+b^2}{p^2+q^2} = \\left(\\frac{ap+bq}{p^2+q^2}\\right)^2 + \\left(\\frac{aq-bp}{p^2+q^2}\\right)^2</math>\n\n::and thus expresses the quotient as a sum of two squares, as claimed.\n\n::On the other hand if <math>p^2+q^2</math> divides <math>pb+aq</math>, a similar argument holds by using the following variant of Diophantus's identity: \n\n:::<math>(a^2+b^2)(q^2+p^2) = (aq+bp)^2 + (ap-bq)^2 . </math>\n\n3. ''If a number which can be written as a sum of two squares is divisible by a number which is not a sum of two squares, then the quotient has a factor which is not a sum of two squares.'' (This is Euler's second Proposition).\n\n::Suppose <math>q</math> is a number not expressible as a sum of two squares, which divides <math>a^2+b^2</math>.  Write the quotient, factored into its (possibly repeated) prime factors, as <math>p_1p_2\\cdots p_n</math> so that <math>a^2+b^2 = q p_1p_2\\cdots p_n</math>. If all factors <math>p_i</math> can be written as sums of two squares, then we can divide <math>a^2+b^2</math> successively by <math>p_1</math>, <math>p_2</math>, etc., and applying step (2.) above we deduce that each successive, smaller, quotient is a sum of two squares. If we get all the way down to <math>q</math> then <math>q</math> itself would have to be equal to the sum of two squares, which is a contradiction.  So at least one of the primes <math>p_i</math> is not the sum of two squares.\n\n4. ''If <math>a</math> and <math>b</math> are relatively prime positive integers then every factor of <math>a^2 + b^2</math> is a sum of two squares.''\n(This is the step that uses step (3.) to produce an 'infinite descent' and was Euler's Proposition 4. The proof sketched below also includes the proof of his Proposition 3).\n\n::Let <math>a,b</math> be relatively prime positive integers: wlog <math>a^2+b^2</math> is not itself prime, otherwise there is nothing to prove. Let <math>q</math> therefore be a ''proper'' factor of <math>a^2+b^2</math>, not necessarily prime: we wish to show that <math>q</math> is a sum of two squares.  Again, we lose nothing by assuming <math> q > 2 </math> since the case <math> q = 2 = 1^2 + 1^2 </math> is obvious.  \n\n::Let <math>m,n</math> be non-negative integers such that <math>mq,nq</math> are the closest multiples of <math>q</math> (in absolute value) to <math>a,b</math> respectively. Notice that the differences <math>c := a-mq</math> and <math>d := b-nq</math> are integers of absolute value strictly less than <math>q/2</math>: indeed, when <math> q > 2 </math> is even, gcd<math>(a,q/2)=1</math>; otherwise since gcd<math>(a,q/2) \\mid q/2 \\mid q \\mid a^2+b^2</math>, we would also have gcd<math>(a,q/2) \\mid b</math>.  Multiplying out we obtain\n:::<math>a^2 + b^2 = m^2q^2 + 2mqc + c^2 + n^2q^2 + 2nqd + d^2 = Aq + (c^2+d^2)</math>\n::uniquely defining a non-negative integer <math>A</math>.  Since <math>q</math> divides both ends of this equation sequence it follows that <math>c^2+d^2</math> must also be divisible by <math>q</math>: say <math>c^2+d^2 = qr</math>.  Let <math>g</math> be the gcd of <math>c</math> and <math>d</math> which by the co-primeness of <math>a,b</math> is relatively prime to <math>q</math>.  Thus <math>g^2</math> divides <math>r</math>, so writing <math>e = c/g</math>, <math>f = d/g</math> and <math>s = r/g^2</math>, we obtain the expression <math>e^2+f^2 = qs</math> for relatively prime <math>e</math> and <math>f</math>, and with <math> s < q/2 </math>, since\n\n:::<math>qs = e^2 + f^2 \\leq c^2+d^2 < \\left(\\frac{q}{2}\\right)^2 + \\left(\\frac{q}{2}\\right)^2 = q^2/2.</math>\n\n::Now finally, the ''descent'' step:  if <math>q</math> is not the sum of two squares, then by step (3.) there must be a factor <math>q_1</math> say of <math>s</math> which is not the sum of two squares.  But <math>q_1 \\leq s < q/2 < q </math> and so repeating these steps (initially with <math>e,f;q_1</math> in place of <math>a,b;q</math>, and so on ''ad infinitum'') we shall be able to find a strictly decreasing infinite sequence <math>q, q_1, q_2, \\ldots </math> of positive integers which are not themselves the sums of two squares but which divide into a sum of two relatively prime squares.  Since such an '''infinite descent''' is impossible, we conclude that <math>q</math> must be expressible as a sum of two squares, as claimed.\n\n5. ''Every prime of the form <math>4n+1</math> is a sum of two squares.''\n(This is the main result of Euler's second paper).\n\n::If <math>p=4n+1</math>, then by [[Fermat's Little Theorem]] each of the numbers <math>1, 2^{4n}, 3^{4n},\\dots, (4n)^{4n}</math> is congruent to one modulo <math>p</math>. The differences <math>2^{4n}-1, 3^{4n}-2^{4n},\\dots,(4n)^{4n}-(4n-1)^{4n}</math> are therefore all divisible by <math>p</math>. Each of these differences can be factored as\n:::<math>a^{4n}-b^{4n} = \\left(a^{2n}+b^{2n}\\right)\\left(a^{2n}-b^{2n}\\right).</math>\n::Since <math>p</math> is prime, it must divide one of the two factors. If in any of the <math>4n-1</math> cases it divides the first factor, then by  the previous step we conclude that <math>p</math> is itself a sum of two squares (since <math>a</math> and <math>b</math> differ by <math>1</math>, they are relatively prime). So it is enough to show that <math>p</math> cannot always divide the second factor. If it divides all <math>4n-1</math> differences <math>2^{2n}-1, 3^{2n}-2^{2n},\\dots,(4n)^{2n}-(4n-1)^{2n}</math>, then it would divide all <math>4n-2</math> differences of successive terms, all <math>4n-3</math> differences of the differences, and so forth. Since the <math>k</math>th differences of the sequence <math>1^k, 2^k, 3^k,\\dots</math> are all equal to <math>k!</math> ([[Finite difference#Calculus of finite differences|Finite difference]]), the <math>2n</math>th differences would all be constant and equal to <math>(2n)!</math>, which is certainly not divisible by <math>p</math>. Therefore, <math>p</math> cannot divide all the second factors which proves that <math>p</math> is indeed the sum of two squares.\n\n==Lagrange's proof through quadratic forms==\n[[Joseph Louis Lagrange|Lagrange]] completed a proof in 1775<ref>Nouv. Mém. Acad. Berlin, année 1771, 125; ibid. année 1773, 275; ibid année 1775, 351.</ref> based on his general theory of integral [[quadratic forms]]. The following presentation incorporates a slight simplification of his argument, due to [[Carl Friedrich Gauss|Gauss]], which appears in article 182 of the [[Disquisitiones Arithmeticae]].\n\nAn (integral [[binary quadratic form|binary) quadratic form]] is an expression of the form <math>ax^2 + bxy + cy^2</math> with <math>a,b,c</math> integers. A number <math>n</math> is said to be ''represented by the form'' if there exist integers <math>x,y</math> such that <math>n = ax^2 + bxy + cy^2</math>. Fermat's theorem on sums of two squares is then equivalent to the statement that a prime <math>p</math> is represented by the form <math>x^2 + y^2</math> (i.e., <math>a=c=1</math>, <math>b=0</math>) exactly when <math>p</math> is congruent to <math>1</math> modulo <math>4</math>.\n\nThe [[discriminant]] of the quadratic form is defined to be <math>b^2 - 4ac</math>. The discriminant of <math>x^2 + y^2</math> is then equal to <math>-4</math>.\n\nTwo forms <math> ax^2 + bxy + cy^2 </math> and <math> a'x'^2 + b'x'y' + c'y'^2 </math> are ''equivalent'' if and only if there exist substitutions with integer coefficients\n: <math> x = \\alpha x' + \\beta y'</math>\n: <math> y = \\gamma x' + \\delta y'</math>\nwith <math>\\alpha\\delta - \\beta\\gamma = \\pm 1</math> such that, when substituted into the first form, yield the second. Equivalent forms are readily seen to have the same discriminant, and hence also the same parity for the middle coefficient <math> b </math>, which coincides with the parity of the discriminant. Moreover, it is clear that equivalent forms will represent exactly the same integers, because these kind of substitutions can be reversed by substitutions of the same kind.\n\nLagrange proved that all positive definite forms of discriminant −4 are equivalent. Thus, to prove Fermat's theorem it is enough to find ''any'' positive definite form of discriminant −4 that represents <math>p</math>. For example, one can use a form\n:<math> px^2 + 2mxy + \\left(\\frac{m^2+1}{p}\\right)y^2,</math>\nwhere the first coefficient ''a''&nbsp;=&nbsp;<math>p</math> was chosen so that the form represents <math>p</math> by setting ''x''&nbsp;=&nbsp;1, and ''y''&nbsp;=&nbsp;0, the coefficient ''b''&nbsp;=&nbsp;2''m'' is an arbitrary even number (as it must be, to get an even discriminant), and finally <math> c=\\frac{m^2+1}{p} </math> is chosen so that the discriminant <math> b^2-4ac=4m^2-4pc </math> is equal to −4, which guarantees that the form is indeed equivalent to <math> x^2+y^2 </math>. Of course, the coefficient <math> c=\\frac{m^2+1}{p} </math> must be an integer, so the problem is reduced to finding some integer ''m'' such that <math>p</math> divides <math>m^2+1</math>:  or in other words, a '' 'square root of -1 modulo <math>p</math>' ''.  \n\nWe claim such a square root of <math>-1</math> is given by <math> K = \\prod_{k=1}^\\frac{p-1}{2} k </math>.  Firstly it follows from Euclid's [[Fundamental Theorem of Arithmetic]] that <math> ab \\equiv 0 \\pmod p \\iff a \\equiv 0 \\pmod p \\ \\ \\hbox{or}\\ \\  b \\equiv 0 \\pmod p </math>.  Consequently  <math> a^2 \\equiv 1 \\pmod p  \\iff a \\equiv \\pm 1 \\pmod p  </math>: that is, <math> \\pm 1 </math> are their own inverses modulo <math>p</math> and this property is unique to them.  It then follows from the validity of [[Euclidean division]] in the integers, and the fact that <math>p</math> is prime, that for every <math> 2 \\leq a \\leq p-2 </math> the gcd of <math>a</math> and <math>p</math> may be expressed via the [[Euclidean algorithm]] yielding a unique and ''distinct'' inverse <math> a^{-1} \\neq a </math> of  <math>a</math>  modulo <math>p</math>.  In particular therefore the product of ''all'' non-zero residues modulo <math>p</math> is <math>-1</math>.  Let <math> L = \\prod_{l=\\frac{p+1}{2}}^{p-1} l </math>: from what has just been observed, <math> KL \\equiv -1 \\pmod p </math>.  But by definition, since each term in <math>K</math> may be paired with its negative in <math>L</math>, <math> L = (-1)^\\frac{p-1}{2}K </math>, which since <math>p</math> is odd shows that <math> K^2 \\equiv -1 \\pmod p \\iff p \\equiv 1 \\pmod 4 </math>, as required.   \n\n\n<!-- \nThis is possible by [[Euler's criterion]], but we reproduce the argument below to finish the proof.\n\nAs said, it suffices to find a root ''m'' of the polynomial <math> P(x)=x^2+1 </math> modulo ''p''&nbsp;=&nbsp;4''n''+1. What we do know, by Fermat's Little Theorem, is that each ''z'' not congruent to 0 modulo ''p'' is a root of the polynomial <math> Q(z)=z^{p-1}-1 = z^{4n}-1 = (z^{2n}-1)(z^{2n}+1)</math>. Then it must be a root of either <math> z^{2n}-1 </math> or <math> z^{2n}+1 </math>, since the integers modulo ''p'' form a field. Moreover, by a theorem of Lagrange, the number of roots modulo ''p'' of a polynomial of degree ''d'' is at most ''d'' (this follows again since the integers modulo ''p'' form a field). So the 4''n'' nonzero classes 1, 2, …, ''p''&nbsp;−&nbsp;1 must split into exactly 2''n'' of them that are roots of <math>z^{2n} - 1 </math>, and the other 2''n'' that are roots of <math> z^{2n} + 1 </math>. Choosing any ''z'' of the second kind and setting <math>m=z^n</math> completes the proof. \n-->\n\n==Dedekind's two proofs using Gaussian integers==\n[[Richard Dedekind]] gave at least two proofs of Fermat's theorem on sums of two squares, both using the arithmetical properties of the [[Gaussian integers]], which are numbers of the form ''a''&nbsp;+&nbsp;''bi'', where ''a'' and ''b'' are integers, and ''i'' is the square root of −1. One appears in section 27 of his exposition of ideals published in 1877; the second appeared in Supplement XI to [[Peter Gustav Lejeune Dirichlet]]'s ''[[Vorlesungen über Zahlentheorie]]'', and was published in 1894.\n\n'''1. First proof.''' If <math>p</math> is an odd [[prime number]], then we have <math>i^{p-1} = (-1)^{\\frac{p-1}{2}}</math> in the Gaussian integers. Consequently, writing a Gaussian integer ω&nbsp;=&nbsp;''x''&nbsp;+&nbsp;''iy'' with ''x,y''&nbsp;∈&nbsp;'''Z''' and applying the [[Frobenius automorphism]] in '''Z'''[''i'']/(''p''), one finds\n:<math>\\omega^p = (x+yi)^p \\equiv x^p+y^pi^p \\equiv x + (-1)^{\\frac{p-1}{2}}yi \\pmod{p},</math>\nsince the automorphism fixes the elements of '''Z'''/(''p''). In the current case, <math>p=4n+1</math> for some integer n, and so in the above expression for ω<sup>p</sup>, the exponent (p-1)/2 of -1 is even. Hence the right hand side equals ω, so in this case the Frobenius endomorphism of '''Z'''[''i'']/(''p'') is the identity. \nKummer had already established that if {{nowrap|''f'' ∈ {1,2}}} is the [[order (group theory)|order]] of the Frobenius automorphism of '''Z'''[''i'']/(''p''), then the [[ideal (ring theory)|ideal]] <math>(p)</math> in '''Z'''[''i''] would be a product of 2/''f'' distinct [[prime ideal]]s. (In fact, Kummer had established a much more general result for any extension of '''Z''' obtained by adjoining a primitive ''m''-th [[root of unity]], where ''m'' was any positive integer; this is the case {{nowrap|''m'' {{=}} 4}} of that result.) Therefore, the ideal (''p'') is the product of two different prime ideals in '''Z'''[''i''].  Since the Gaussian integers are a [[Euclidean domain]] for the norm function <math>N(x + iy)=x^2+y^2</math>, every ideal is principal and generated by a nonzero element of the ideal of minimal norm. Since the norm is multiplicative, the norm of a generator <math>\\alpha</math> of one of the ideal factors of (''p'') must be a strict divisor of <math>N(p) = p^2</math>, so that we must have <math> p = N(\\alpha) = N(a+bi) = a^2 + b^2</math>, which gives Fermat's theorem.\n\n'''2. Second proof.''' This proof builds on Lagrange's result that if <math>p=4n+1</math> is a prime number, then there must be an integer ''m'' such that <math>m^2 + 1</math> is divisible by ''p'' (we can also see this by [[Euler's criterion]]); it also uses the fact that the Gaussian integers are a [[unique factorization domain]] (because they are a Euclidean domain). Since {{nowrap|''p'' ∈ '''Z'''}} does not divide either of the Gaussian integers <math>m + i</math> and <math>m-i</math> (as it does not divide their [[imaginary part]]s), but it does divide their product <math>m^2 + 1</math>, it follows that <math>p</math> cannot be a [[Integral domain#Divisibility, prime and irreducible elements|prime]] element in the Gaussian integers. We must therefore have a nontrivial factorization of ''p'' in the Gaussian integers, which in view of the norm can have only two factors (since the norm is multiplicative, and <math>p^2 = N(p)</math>, there can only be up to two factors of p), so it must be of the form <math>p = (x+yi)(x-yi)</math> for some integers <math>x</math> and <math>y</math>. This immediately yields that <math>p = x^2 + y^2</math>.\n\n==Proof by Minkowski's Theorem==\nFor <math>p</math> congruent to <math>1</math>  mod <math>4</math> a prime, <math>-1</math> is a [[quadratic residue]] mod <math>p</math> by [[Euler's criterion]]. Therefore, there exists an integer <math>m</math> such that <math>p</math> divides <math>m^2+1</math>. Let <math>\\hat{i}, \\hat{j}</math> be the [[standard basis]] elements for the [[vector space]] <math>\\mathbb{R}^2</math> and set <math>\\vec{u} = \\hat{i} + m\\hat{j}</math> and <math>\\vec{v} = 0\\hat{i} + p\\hat{j}</math>. Consider the [[lattice (group)|lattice]] <math>S = \\{a\\vec{u} + b\\vec{v} \\mid a, b \\in \\mathbb Z\\}</math>. If <math>\\vec{w} = a\\vec{u} + b\\vec{v} = a \\hat{i} + (am + bp)\\hat{j} \\in S</math> then <math>\\|\\vec{w}\\|^2 \\equiv a^2 + (am+bp)^2 \\equiv a^2(1 + m^2) \\equiv 0\\pmod{p}</math>. Thus <math>p</math> divides <math>\\|\\vec{w}\\|^2</math> for any <math>\\vec{w} \\in S</math>.\n\nThe area of the [[Primitive cell#Two dimensions|fundamental parallelogram]] of the lattice is <math>p</math>. The area of the open disk, <math>D</math>, of radius <math>\\sqrt{2p}</math> centered around the origin is <math>2 \\pi p > 4p</math>. Furthermore, <math>D</math> is convex and symmetrical about the origin. Therefore, by [[Minkowski's theorem]] there exists a nonzero vector <math>\\vec{w} \\in S</math> such that <math>\\vec{w} \\in D</math>. Both <math>\\|\\vec{w}\\|^2 < 2p</math> and <math>p \\mid \\|\\vec{w}\\|^2</math> so <math>p = \\|\\vec{w}\\|^2</math>. Hence <math>p</math> is the sum of the squares of the components of <math>\\vec{w}</math>.\n\n==Zagier's \"one-sentence proof\"==\nLet <math>p=4k+1</math> be prime, let <math>\\mathbb{N}</math> denote the [[natural number]]s (with or without zero), and consider the finite set <math>S=\\{(x,y,z)\\in\\mathbb{N}^3: x^2+4yz=p\\}</math> of triples of numbers.\nThen <math>S</math> has two [[involution (mathematics)|involution]]s: an obvious one <math>(x,y,z)\\mapsto (x,z,y)</math> whose fixed points <math>(x,y,y)</math> correspond to representations of <math>p</math> as a sum of two squares, and a more complicated one,\n: <math> (x,y,z)\\mapsto\n\\begin{cases}\n(x+2z,~z,~y-x-z),\\quad \\textrm{if}\\,\\,\\, x < y-z \\\\\n(2y-x,~y,~x-y+z),\\quad \\textrm{if}\\,\\,\\, y-z < x < 2y\\\\\n(x-2y,~x-y+z,~y),\\quad \\textrm{if}\\,\\,\\, x > 2y\n\\end{cases}\n</math>\n\nwhich has exactly one fixed point <math>(1,1,k)</math>. Two involutions over the same finite set must have sets of fixed points with the same [[Parity (mathematics)|parity]], and since the second involution has an odd number of fixed points, so does the first.\nZero is even, so the first involution has a nonzero number of fixed points, any one of which gives a representation of <math>p</math> as a sum of two squares.\n\nThis proof, due to [[Don Zagier|Zagier]], is a simplification of an earlier proof by [[Roger Heath-Brown|Heath-Brown]], which in turn was inspired by a proof of [[Joseph Liouville|Liouville]]. The technique of the proof is a combinatorial analogue of the topological principle that the [[Euler characteristic]]s of a [[topological space]] with an involution and of its [[Fixed point theorem|fixed point set]] have the same parity and is reminiscent of the use of ''sign-reversing involutions'' in the proofs of combinatorial bijections.\n\n==Proof with partition theory==\n\nIn 2016, A. David Christopher gave a [[Partition (number theory)|partition-theoretic]] proof by considering partitions of the odd prime <math>n</math> having exactly two sizes <math>a_i (i=1,2)</math>, each occurring exactly <math> a_i</math> times, and by showing that at least one such partition exists if <math>n</math> is congruent to 1 modulo 4.<ref>A. David Christopher, A partition-theoretic proof of Fermat’s Two Squares Theorem”, Discrete Mathematics, 339 (2016) 1410–1411.</ref>\n\n==References==\n{{Refbegin}}\n*Richard Dedekind, ''The theory of algebraic integers''.\n*Harold M. Edwards, ''Fermat's Last Theorem. A genetic introduction to algebraic number theory''. Graduate Texts in Mathematics no. 50, Springer-Verlag, NY, 1977.\n*C. F. Gauss, ''[[Disquisitiones Arithmeticae]]'' (English Edition). Transl. by Arthur A. Clarke. Springer-Verlag, 1986.\n*{{Citation|last = Goldman|first = Jay R.|year = 1998|title = The Queen of Mathematics: A historically motivated guide to Number Theory|publisher = [[A K Peters, Ltd.|A K Peters]]|isbn = 1-56881-006-7}}\n*D. R. Heath-Brown, ''Fermat's two squares theorem''. Invariant, 11 (1984) pp.&nbsp;3–5.\n*[[John Stillwell]], Introduction to ''Theory of Algebraic Integers'' by Richard Dedekind. Cambridge Mathematical Library, Cambridge University Press, 1996.\n* [[Don Zagier]], ''A one-sentence proof that every prime p ≡ 1 mod 4 is a sum of two squares''. Amer. Math. Monthly 97 (1990), no. 2, 144, {{DOI|10.2307/2323918}}\n{{Refend}}\n\n==Notes==\n{{Reflist}}\n\n==External links==\n* [https://web.archive.org/web/20121117120257/http://planetmath.org/encyclopedia/ProofOfThuesLemma.html Two more proofs at PlanetMath.org]\n* {{cite web|url = http://www.math.unh.edu/~dvf/532/Zagier|title=A one-sentence proof of the theorem|archiveurl=https://web.archive.org/web/20120205194801/http://www.math.unh.edu/~dvf/532/Zagier|archivedate=5 February 2012}}\n* [http://eprints.maths.ox.ac.uk/677/ reprint of Heath-Brown's proof, with commentary]\n\n{{DEFAULTSORT:Fermat's Theorem On Sums Of Two Squares, Proofs Of}}\n[[Category:Additive number theory]]\n[[Category:Algebraic number theory]]\n[[Category:Article proofs]]\n[[Category:Quadratic forms]]\n[[Category:Squares in number theory]]"
    },
    {
      "title": "Genus of a quadratic form",
      "url": "https://en.wikipedia.org/wiki/Genus_of_a_quadratic_form",
      "text": "In mathematics, the '''genus''' is a classification of quadratic forms and lattices over the ring of integers.\n\nAn [[integral quadratic form]] is a quadratic form on '''Z'''<sup>''n''</sup>, or equivalently a free '''Z'''-module of finite rank.  Two such forms are in the same ''genus'' if they are equivalent over the local rings '''Z'''<sub>''p''</sub> for each prime ''p'' and also equivalent over '''R'''.\n\nEquivalent forms are in the same genus, but the converse does not hold.  For example, ''x''<sup>2</sup> + 82''y''<sup>2</sup> and 2''x''<sup>2</sup> + 41''y''<sup>2</sup> are in the same genus but not equivalent over '''Z'''.\n\nForms in the same genus have equal [[Determinant of a quadratic form|discriminant]] and hence there are only finitely many equivalence classes in a genus.\n\nThe [[Smith–Minkowski–Siegel mass formula]] gives the ''weight'' or ''mass'' of the quadratic forms in a genus, the count of equivalence classes weighted by the reciprocals of the orders of their automorphism groups.\n\n==Binary quadratic forms==\nFor [[binary quadratic form]]s there is a group structure on the set ''C'' equivalence classes of forms with given [[Discriminant of a quadratic form|discriminant]].  The genera are defined by the ''generic characters''.  The principal genus, the genus containing the principal form, is precisely the subgroup ''C''<sup>2</sup> and the genera are the cosets of ''C''<sup>2</sup>: so in this case all genera contain the same number of classes of forms.  \n\n==See also==\n* [[Spinor genus]]\n\n==References==\n* {{cite book | first=J.W.S. | last=Cassels | authorlink=J. W. S. Cassels | title=Rational Quadratic Forms | series=London Mathematical Society Monographs | volume=13 | publisher=[[Academic Press]] | year=1978 | isbn=0-12-163260-1 | zbl=0395.10029 }}\n\n==External links==\n* {{SpringerEOM | title=Quadratic form }}\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Hasse invariant of a quadratic form",
      "url": "https://en.wikipedia.org/wiki/Hasse_invariant_of_a_quadratic_form",
      "text": "{{redirect|Hasse–Witt invariant|the invariant of algebraic curves|Hasse–Witt matrix}}\nIn [[mathematics]], the '''Hasse invariant''' (or '''Hasse–Witt invariant''') of a [[quadratic form]] ''Q'' over a [[field (mathematics)|field]] ''K'' takes values in the [[Brauer group]] Br(''K'').  The name \"Hasse–Witt\" comes from [[Helmut Hasse]] and [[Ernst Witt]].\n\nThe quadratic form ''Q''  may be taken as a [[diagonal form]]\n\n:Σ ''a''<sub>''i''</sub>''x''<sub>''i''</sub><sup>2</sup>.\n\nIts invariant is then defined as the product of the classes in the Brauer group of all the [[quaternion algebra]]s\n\n:(''a''<sub>''i''</sub>, ''a''<sub>''j''</sub>) for ''i'' < ''j''.\n\nThis is independent of the diagonal form chosen to compute it.<ref name=Lam118>Lam (2005) p.118</ref>\n\nIt may also be viewed as the second [[Stiefel–Whitney class]] of ''Q''.\n\n==Symbols==\nThe invariant may be computed for a specific [[Steinberg symbol|symbol]] φ  taking values ±1 in the group C<sub>2</sub>.<ref name=MH79>Milnor & Husemoller (1973) p.79</ref>\n\nIn the context of quadratic forms over a [[local field]], the Hasse invariant may be defined using the [[Hilbert symbol]], the unique symbol taking values in C<sub>2</sub>.<ref name=S36>Serre (1973) p.36</ref>  The invariants of a quadratic forms over a local field are precisely the dimension, [[Discriminant of a quadratic form|discriminant]] and Hasse invariant.<ref name=S39>Serre (1973) p.39</ref>\n\nFor quadratic forms over a [[number field]], there is a Hasse invariant ±1 for every [[finite place]].  The invariants of a form over a number field are precisely the dimension, discriminant, all local Hasse invariants and the [[Signature of a quadratic form|signature]]s coming from real embeddings.<ref name=CP16>Conner & Perlis (1984) p.16</ref>\n\n==References==\n{{reflist}}\n* {{cite book | first1=P.E. | last1=Conner | first2=R. | last2=Perlis | title=A Survey of Trace Forms of Algebraic Number Fields | series=Series in Pure Mathematics | volume=2 | publisher=World Scientific | year=1984 | isbn=9971-966-05-0 | zbl=0551.10017 }}\n* {{cite book | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | publisher=[[American Mathematical Society]] | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 | mr = 2104929 }}\n* {{cite book | first1=J. | last1=Milnor | author1-link=John Milnor| first2=D. | last2=Husemoller | title=Symmetric Bilinear Forms | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 }}\n* {{cite book | last=O'Meara | first=O.T. | title=Introduction to quadratic forms | series=Die Grundlehren der mathematischen Wissenschaften | volume=117 | publisher=[[Springer-Verlag]] | isbn=3-540-66564-1 | year=1973 | zbl=0259.10018 }}\n* {{cite book | first=Jean-Pierre | last=Serre | authorlink=Jean-Pierre Serre | title=A Course in Arithmetic | series=[[Graduate Texts in Mathematics]] | volume=7 | publisher=[[Springer-Verlag]] | year=1973 | isbn=0-387-90040-3 | zbl=0256.12001 }}\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Hasse–Minkowski theorem",
      "url": "https://en.wikipedia.org/wiki/Hasse%E2%80%93Minkowski_theorem",
      "text": "{{multiple image\n| direction = vertical\n| footer    = Two [[completion (ring theory)|completions]] of the rational numbers, the [[p-adic numbers|dyadic numbers]] (here, only the dyadic integers are shown) and the [[real numbers]]. The Hasse-Minkowski theorem gives a relationship between [[quadratic form]]s in a [[number field]] and in the completions of the number field.\n| width     = 300\n\n| image1    = 2adic12480.svg\n| alt1      = The [[p-adic numbers| 2-adic integers]]. Showing all of the 2-adic rationals would include an infinite sequence of clumps moving to the left of the figure.\n\n| image2    = Real number line.svg\n| alt2      = The [[real number]] line}}\n\nThe '''Hasse–Minkowski theorem''' is a fundamental result in [[number theory]] which states that two [[quadratic form]]s over a [[number field]] are equivalent if and only if they are equivalent ''locally at all places'', i.e. equivalent over every [[completion (ring theory)|completion]] of the field (which may be [[real number|real]], [[complex number|complex]], or [[p-adic number|p-adic]]). A related result is that a [[quadratic space]] over a number field is [[isotropic quadratic form|isotropic]] if and only if it is isotropic locally everywhere, or equivalently, that a quadratic form over a number field nontrivially represents zero if and only if this holds for all completions of the field. The theorem was proved in the case of the field of [[rational number]]s by [[Hermann Minkowski]] and generalized to number fields by [[Helmut Hasse]]. The same statement holds even more generally for all [[global field]]s.\n\n==Importance==\n\nThe importance of the Hasse–Minkowski theorem lies in the novel paradigm it presented for answering arithmetical questions: in order to determine whether an equation of a certain type has a solution in rational numbers, it is sufficient to test whether it has solutions over complete fields of real and ''p''-adic numbers, where analytic considerations, such as [[Newton's method]] and its ''p''-adic analogue, [[Hensel's lemma]], apply. This is encapsulated in the idea of a [[local-global principle]], which is one of the most fundamental techniques in [[arithmetic geometry]].\n\n== Application to the classification of quadratic forms ==\n\nThe Hasse–Minkowski theorem reduces the problem of classifying quadratic forms over a number field ''K'' up to equivalence to the set of analogous but much simpler questions over [[local field]]s. Basic invariants of a nonsingular quadratic form are its '''dimension''', which is a positive integer, and its '''discriminant''' modulo the squares in '''K''', which is an element of the multiplicative group  '''K'''<sup>*</sup>/'''K'''<sup>*2</sup>. In addition, for every [[place (mathematics)|place]] ''v'' of ''K'', there is an invariant coming from the completion '''K'''<sub>''v''</sub>. Depending on the choice of ''v'', this completion may be the [[real number]]s '''R''', the [[complex number]]s '''C''', or a [[p-adic number]] field, each of which has different kinds of invariants:\n* ''Case of'' '''R'''.  By [[Sylvester's law of inertia]], the signature (or, alternatively, the negative index of inertia) is a complete invariant.\n* ''Case of'' '''C'''.  All nonsingular quadratic forms of the same dimension are equivalent.\n* ''Case of'' '''Q'''<sub>''p''</sub> ''and its [[algebraic extension]]s''. Forms of the same dimension are classified up to equivalence by their [[Hasse invariant of a quadratic form|Hasse invariant]].\n\nThese invariants must satisfy some compatibility conditions: a parity relation (the sign of the discriminant must match the negative index of inertia) and a product formula (a local–global relation). Conversely, for every set of invariants satisfying these relations, there is a quadratic form over '''K''' with these invariants.\n\n== References ==\n* {{cite book | last=Kitaoka | first=Yoshiyuki | title=Arithmetic of quadratic forms | series=Cambridge Tracts in Mathematics | volume=106 | publisher=Cambridge University Press | year=1993 | isbn=0-521-40475-4 | zbl=0785.11021 }}\n* {{cite book | first=Jean-Pierre | last=Serre | authorlink=Jean-Pierre Serre | title=A Course in Arithmetic | series=[[Graduate Texts in Mathematics]] | volume=7 | publisher=[[Springer-Verlag]] | year=1973 | isbn=0-387-90040-3 | zbl=0256.12001 }}\n\n{{DEFAULTSORT:Hasse-Minkowski theorem}}\n[[Category:Quadratic forms]]\n[[Category:Theorems in number theory]]\n[[Category:Hermann Minkowski]]"
    },
    {
      "title": "Hilbert symbol",
      "url": "https://en.wikipedia.org/wiki/Hilbert_symbol",
      "text": "In [[mathematics]], the '''Hilbert symbol''' or '''norm-residue symbol''' is a function (–, –) from ''K''<sup>×</sup> × ''K''<sup>×</sup> to the group of ''n''th roots of unity in a [[local field]] ''K'' such as the fields of [[real number|reals]] or [[p-adic number]]s . It is related to [[reciprocity law (mathematics)|reciprocity laws]], and can be defined in terms of the [[Artin symbol]] of [[local class field theory]]. The Hilbert symbol was introduced by {{harvs|txt|first=David|last=Hilbert|authorlink=David Hilbert|year=1897|loc=sections 64, 131|year2=1998|loc2=English translation}} in his [[Zahlbericht]], with the slight difference that he defined it for elements of global fields rather than for the larger local fields.\n\nThe Hilbert symbol has been generalized to [[higher local field]]s.\n\n==Quadratic Hilbert symbol==\n\nOver a local field ''K'' whose [[multiplicative group]] of non-zero elements is ''K''<sup>×</sup>,\nthe quadratic Hilbert symbol is the [[function (mathematics)|function]] (–, –) from ''K''<sup>×</sup> × ''K''<sup>×</sup> to {−1,1} defined by\n\n:<math>(a,b)=\\begin{cases}+1,&\\mbox{ if }z^2=ax^2+by^2\\mbox{ has a non-zero solution }(x,y,z)\\in K^3;\\\\-1,&\\mbox{ otherwise.}\\end{cases}</math>\n\n===Properties===\nThe following three properties follow directly from the definition, by choosing suitable solutions of the diophantine equation above:\n*If ''a'' is a square, then (''a'', ''b'') = 1 for all ''b''.\n*For all ''a'',''b'' in ''K''<sup>×</sup>,  (''a'', ''b'') = (''b'', ''a'').\n*For any ''a'' in ''K''<sup>×</sup> such that ''a''−1 is also in ''K''<sup>×</sup>, we have (''a'', 1−''a'') = 1.\nThe (bi)multiplicativity, i.e.,\n:(''a'', ''b''<sub>1</sub>''b''<sub>2</sub>) = (''a'', ''b''<sub>1</sub>)·(''a'', ''b''<sub>2</sub>)\nfor any ''a'', ''b''<sub>1</sub> and ''b''<sub>2</sub> in ''K''<sup>×</sup> is, however, more difficult to prove, and requires the development of [[local class field theory]].\n\nThe third property shows that the Hilbert symbol is an example of a [[Steinberg symbol]] and thus factors over the second [[Milnor K-theory|Milnor K-group]] <math>K^M_2 (K)</math>, which is by definition\n:''K''<sup>×</sup> ⊗ ''K''<sup>×</sup> / (''a'' ⊗ (1−''a)'', ''a'' ∈ ''K''<sup>×</sup> \\ {1})\nBy the first property it even factors over <math>K^M_2 (K) / 2</math>. This is the first step towards the [[Milnor conjecture]].\n\n===Interpretation as an algebra===\n\nThe Hilbert symbol can also be used to denote the [[central simple algebra]] over ''K'' with basis 1,''i'',''j'',''k'' and multiplication rules <math>i^2=a</math>, <math>j^2=b</math>, <math>ij=-ji=k</math>.  In this case the algebra represents an element of order 2 in the [[Brauer group]] of ''K'', which is identified with -1 if it is a division algebra and +1 if it is isomorphic to the algebra of 2 by 2 matrices.\n\n===Hilbert symbols over the rationals===\nFor a [[valuation (algebra)|place]] ''v'' of the [[rational number field]] and rational numbers ''a'', ''b'' we let (''a'', ''b'')<sub>''v''</sub> denote the value of the Hilbert symbol in the corresponding [[complete space|completion]] '''Q'''<sub>''v''</sub>. As usual, if ''v'' is the valuation attached to a prime number ''p'' then the corresponding completion is the [[P-adic number|p-adic field]] and if ''v'' is the infinite place then the completion is the [[real number]] field.\n\nOver the reals, (''a'', ''b'')<sub>∞</sub> is +1 if at least one of ''a'' or ''b'' is positive, and −1 if both are negative.\n\nOver the p-adics with ''p'' odd, writing <math>a = p^{\\alpha} u</math> and <math>b = p^{\\beta} v</math>, where ''u'' and ''v'' are integers [[coprime]] to ''p'', we have\n\n:<math>(a,b)_p = (-1)^{\\alpha\\beta\\epsilon(p)} \\left(\\frac{u}{p}\\right)^\\beta \\left(\\frac{v}{p}\\right)^\\alpha</math>, where <math>\\epsilon(p) = (p-1)/2</math>\n\nand the expression involves two [[Legendre symbol]]s.\n\nOver the 2-adics, again writing <math>a = 2^\\alpha u</math> and <math>b = 2^\\beta v</math>, where ''u'' and ''v'' are [[odd number]]s, we have\n\n:<math>(a,b)_2 = (-1)^{\\epsilon(u)\\epsilon(v) + \\alpha\\omega(v) + \\beta\\omega(u)}</math>, where <math>\\omega(x) = (x^2-1)/8.</math>\n\nIt is known that if ''v'' ranges over all places, (''a'', ''b'')<sub>''v''</sub> is 1 for almost all places. Therefore, the following product formula\n\n:<math>\\prod_v (a,b)_v = 1</math>\n\nmakes sense. It is equivalent to the law of [[quadratic reciprocity]].\n\n===Kaplansky radical===\nThe Hilbert symbol on a field ''F'' defines a map\n\n:<math> (\\cdot,\\cdot) : F^*/F^{*2} \\times F^*/F^{*2} \\rightarrow \\mathop{Br}(F) </math>\n\nwhere Br(''F'') is the Brauer group of ''F''.  The kernel of this mapping, the elements ''a'' such that (''a'',''b'')=1 for all ''b'', is the '''Kaplansky radical''' of ''F''.<ref name=Lam4501>Lam (2005) pp.450–451</ref>\n\nThe radical is a subgroup of F<sup>*</sup>/F<sup>*2</sup>, identified with a subgroup of F<sup>*</sup>.  The radical is equal to F<sup>*</sup> if and only if ''F'' has [[u-invariant|''u''-invariant]] at most 2.<ref name=Lam451>Lam (2005) p.451</ref>  In the opposite direction, a field with radical F<sup>*2</sup> is termed a '''Hilbert field'''.<ref name=Lam455>Lam (2005) p.455</ref>\n\n==The general Hilbert symbol==\n\nIf ''K'' is a local field containing the group of ''n''th roots of unity for some positive integer ''n'' prime to the characteristic of ''K'', then the Hilbert symbol (,) is a function from ''K''*×''K''* to μ<sub>''n''</sub>.  In terms of the Artin symbol it can be defined by<ref name=Neu333>Neukirch (1999) p.333</ref>\n\n:<math> (a,b)\\sqrt[n]{b} = (a,K(\\sqrt[n]{b})/K)\\sqrt[n]{b}</math>\n\nHilbert originally defined the Hilbert symbol before the Artin symbol was discovered, and his definition (for ''n'' prime) used the power residue symbol when ''K'' has residue characteristic coprime to ''n'', and was rather complicated when ''K'' has residue characteristic dividing ''n''.\n\n===Properties===\n\nThe Hilbert symbol is (multiplicatively) bilinear:\n:(''ab'',''c'') = (''a'',''c'')(''b'',''c'')\n:(''a'',''bc'') = (''a'',''b'')(''a'',''c'')\nskew symmetric:\n:(''a'',''b'') = (''b'',''a'')<sup>−1</sup>\nnondegenerate:\n: (''a'',''b'')=1 for all ''b'' if and only if ''a'' is in ''K''*<sup>''n''</sup>\nIt detects norms (hence the name norm residue symbol):\n:(''a'',''b'')=1 if and only if ''a'' is a norm of an element in ''K''({{radic|''b''|''n''}})\nIt has the [[Steinberg symbol|\"symbol\" properties]]:\n:(''a'',1–''a'')=1, (''a'',–a)=1.\n\n===Hilbert's reciprocity law===\n\nHilbert's reciprocity law states that if ''a'' and ''b'' are in an algebraic number field containing the ''n''th roots of unity then<ref name=Neu334>Neukirch (1999) p.334</ref>\n\n:<math>\\prod_p (a,b)_p=1</math>\n\nwhere the product is over the finite and infinite primes ''p'' of the number field, and where (,)<sub>''p''</sub> is the Hilbert symbol of the completion at ''p''. Hilbert's reciprocity law follows from the [[Artin reciprocity law]] and the definition of the Hilbert symbol in terms of the Artin symbol.\n\n===Power residue symbol===\n\nIf ''K'' is a number field containing the ''n''th roots of unity, ''p'' is a prime ideal not dividing ''n'', π is a prime element of the local field of ''p'', and ''a'' is coprime to ''p'', then the [[power residue symbol]] ({{su|p=''a''|b=''p''}}) is related to the Hilbert symbol by<ref name=Neu336>Neukirch (1999) p.336</ref>\n:<math>\\binom{a}{p} = (\\pi,a)_p</math>\nThe power residue symbol is extended to fractional ideals by multiplicativity, and defined for elements of the number field\nby putting ({{su|p=''a''|b=''b''}})=({{su|p= ''a''|b=(''b'')}}) where (''b'') is the principal ideal generated by ''b''.\nHilbert's reciprocity law then implies the following reciprocity law for the residue symbol, for ''a'' and ''b'' prime to each other and to ''n'':\n:<math>\\binom{a}{b}=\\binom{b}{a}\\prod_{p|n,\\infty}(a,b)_p</math>\n\n==External links==\n*{{eom|id=Norm-residue_symbol|title=Norm-residue symbol}}\n*[http://mathworld.wolfram.com/HilbertSymbol.html HilbertSymbol] at [[Mathworld]]\n\n==References==\n{{reflist}}\n*{{Citation | first1=Z. I. | last1=Borevich | author1-link=Zenon Ivanovich Borevich | first2=I. R. | last2=Shafarevich | author2-link=Igor Shafarevich | title = Number theory | publisher = Academic Press | year = 1966 | isbn=0-12-117851-X | zbl=0145.04902 }}\n*{{Citation | last1=Hilbert | first1=David | author1-link=David Hilbert | title=Die Theorie der algebraischen Zahlkörper | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002115344 | language=German | year=1897 | journal=[[Jahresbericht der Deutschen Mathematiker-Vereinigung]] | issn=0012-0456 | volume=4 | pages=175–546 }}\n*{{Citation | last1=Hilbert | first1=David | author1-link=David Hilbert | title=The theory of algebraic number fields | url=https://books.google.com/books?id=_Q2h83Bm94cC | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-62779-1 |mr=1646901 | year=1998}}\n* {{citation | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | publisher=[[American Mathematical Society]] | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 }}\n* {{Citation | last1=Milnor | first1=John Willard | author1-link=John Milnor | title=Introduction to algebraic ''K''-theory | publisher=[[Princeton University Press]] | series=Annals of Mathematics Studies | mr = 0349811 | year=1971 | volume=72 | zbl=0237.18005 }}\n* {{citation | last=Neukirch | first=Jürgen | author-link=Jürgen Neukirch | title=Algebraic number theory | others=Translated from the German by Norbert Schappacher | series=Grundlehren der Mathematischen Wissenschaften | volume=322 | location=Berlin | publisher=[[Springer-Verlag]] | year=1999 | isbn=3-540-65399-6 | zbl=0956.11021 }}\n* {{Citation | last1=Serre | first1=Jean-Pierre | author1-link=Jean-Pierre Serre | title=A Course in Arithmetic | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Graduate Texts in Mathematics]] | isbn=978-3-540-90040-5 | year=1996 | volume=7 | zbl=0256.12001 }}\n* {{Citation | last1=Vostokov | first1=S. V. | last2=Fesenko | first2=I. B. | title=Local fields and their extensions | url=http://www.maths.nott.ac.uk/personal/ibf/book/book.html | series=Translations of Mathematical Monographs | volume=121 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-3259-2 | year=2002 | zbl=1156.11046 }}\n\n[[Category:Class field theory]]\n[[Category:Quadratic forms]]\n[[Category:David Hilbert]]"
    },
    {
      "title": "Hilbert's eleventh problem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%27s_eleventh_problem",
      "text": "'''Hilbert's eleventh problem''' is one of [[David Hilbert]]'s [[Hilbert problems|list of open mathematical problems]] posed at the Second International Congress of Mathematicians in Paris in 1900. A furthering of the theory of [[quadratic forms]], he stated the problem as follows:\n\n:''Our present knowledge of the theory of quadratic number fields puts us in a position to attack successfully the theory of quadratic forms with any number of variables and with any algebraic numerical coefficients. This leads in particular to the interesting problem: to solve a given quadratic equation with algebraic numerical coefficients in any number of variables by integral or fractional numbers belonging to the algebraic realm of rationality determined by the coefficients.''<ref>David Hilbert, {{cite web | title=Mathematical Problems | url=http://www.ams.org/journals/bull/1902-08-10/home.html}} ''[[Bulletin of the American Mathematical Society]]'', vol. 8, no. 10 (1902), pp. 437-479. Earlier publications (in the original German) appeared in ''Göttinger Nachrichten'', 1900, pp. 253–297, and ''Archiv der Mathematik und Physik'', 3rd series, vol. 1 (1901), pp. 44–63, 213–237.</ref>\n\nAs stated by Kaplansky, \"The 11th Problem is simply this: classify quadratic forms over algebraic number fields.\"  This is exactly what Minkowski did for quadratic form with fractional coefficients. A quadratic form (not quadratic equation) is any polynomial in which each term has variables appearing exactly twice. The general form of such an equation is ax^(2)+bxy+cy^(2). (All coefficients must be whole numbers.)\n\nA given quadratic form is said to represent a natural number if substituting specific numbers for the variables gives the number. Gauss and those who followed found that if we change variables in certain ways, the new quadratic form represented the same natural numbers as the old, but in a different, more easily interpreted form.  He used this theory of equivalent quadratic forms to prove whole number theory results.  Lagrange, for example, had shown that any natural number can be expressed as the sum of four squares.  Gauss proved this using his theory of equivalence relations by showing that the quadratic <math>w^2+x^2+y^2+z^2</math> represents all natural numbers. As mentioned earlier, Minkowski created and proved a similar theory for quadratic forms that had fractions as coefficients. Hilbert's eleventh problem asks for a similar theory. That is, a mode of classification so we can tell if one form is equivalent to another, but in the case where coefficients can be algebraic numbers. [[Helmut Hasse]]'s  accomplished this in a proof using his [[local-global principle]] and the fact that the theory is relatively simple for p-adic systems in October 1920. He published his work in 1923 and 1924. See [[Hasse principle]], [[Hasse-Minkowski theorem]]. The local-global principle says that a general result about a rational number or even all rational numbers can often be established by verifying that the result holds true for each of the p-adic number systems.\n\n== See also ==\n*[[Hilbert's problems]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* Yandell, Benjamin H. ''The Honors Class: Hilbert's Problems and Their Solvers.'' Natik: K Peters. Print.\n\n{{Hilbert's problems}}\n\n[[Category:Hilbert's problems|#11]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Hurwitz problem",
      "url": "https://en.wikipedia.org/wiki/Hurwitz_problem",
      "text": "In mathematics, the '''Hurwitz problem''', named after [[Adolf Hurwitz]], is the problem of finding multiplicative relations between [[quadratic form]]s which generalise those known to exist between sums of squares in certain numbers of variables.\n\nThere are well-known multiplicative relationships between sums of squares in two variables\n\n:<math> (x^2+y^2)(u^2+v^2) = (xu-yv)^2 + (xv+yu)^2 \\ , </math>\n\n(known as the [[Brahmagupta–Fibonacci identity]]), and also [[Euler's four-square identity]] and [[Degen's eight-square identity]].  These may be interpreted as multiplicativity for the norms on the [[complex number]]s, [[quaternion]]s and [[octonion]]s respectively.<ref name=Rajwade>{{cite book | title=Squares | volume=171 | series=London Mathematical Society Lecture Note Series | first=A. R. | last=Rajwade | publisher=[[Cambridge University Press]] | year=1993 | isbn=0-521-42668-5 | zbl=0785.11022 }}</ref>{{rp|1–3}}<ref>[[Charles W. Curtis]] (1963) \"The Four and Eight Square Problem and Division Algebras\" in ''Studies in Modern Algebra'' edited by A.A. Albert, pages 100–125, [[Mathematical Association of America]], Solution of Hurwitz’s Problem on page 115</ref>\n\nThe Hurwitz problem for the field ''K'' is to find general relations of the form\n\n:<math> (x_1^2+\\cdots+x_r^2) \\cdot (y_1^2+\\cdots+y_s^2) = (z_1^2 + \\cdots + z_n^2) \\ , </math>\n\nwith the ''z'' being bilinear forms in the ''x'' and ''y'': that is, each ''z'' is a ''K''-linear combination of terms of the form ''x''<sub>''i''</sub>''y''<sub>''j''</sub>.<ref name=Lam>{{cite book | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | author-link=Tsit Yuen Lam | publisher=[[American Mathematical Society]] | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 | mr = 2104929 }}</ref>{{rp|127}} We call a triple (''r'',&nbsp;''s'',&nbsp;''n'') ''admissible'' for ''K'' if such an identity exists.<ref name=Rajwade/>{{rp|125}} Trivial cases of admissible triples include (''r'',&nbsp;''s'',&nbsp;''rs'').  The problem is uninteresting for ''K'' of characteristic&nbsp;2, since over such fields every sum of squares is a square, and we exclude this case.  It is believed that otherwise admissibility is independent of the field of definition.<ref name=Rajwade/>{{rp|137}}\n\nHurwitz posed the problem in 1898 in the special case ''r''&nbsp;=&nbsp;''s''&nbsp;=&nbsp;''n'' and showed that, when coefficients are taken in '''C''', the only admissible values (''n'',&nbsp;''n'',&nbsp;''n'') were ''n''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;4,&nbsp;8:<ref name=Lam/>{{rp|130}} his proof extends to any field of characteristic not 2.<ref name=Rajwade/>{{rp|3}}\n\nThe \"Hurwitz–Radon\" problem is that of finding admissible triples of the form (''r'',&nbsp;''n'',&nbsp;''n'').  Obviously (1,&nbsp;''n'',&nbsp;''n'') is admissible.  The '''Hurwitz–Radon theorem''' states that (ρ(''n''),&nbsp;''n'',&nbsp;''n'') is admissible over any field where ρ(''n'') is the function defined for ''n''&nbsp;=&nbsp;2<sup>''u''</sup>''v'', ''v'' odd, ''u''&nbsp;=&nbsp;4''a''&nbsp;+&nbsp;''b'', 0&nbsp;≤&nbsp;''b''&nbsp;≤&nbsp;3, as ''ρ''(''n'')&nbsp;=&nbsp;8''a''&nbsp;+&nbsp;2<sup>''b''</sup>.<ref name=Rajwade/>{{rp|137}}<ref name=Lam/>{{rp|130}}\n\nOther admissible triples include (3,5,7)<ref name=Rajwade/>{{rp|138}} and (10,&nbsp;10,&nbsp;16).<ref name=Rajwade/>{{rp|137}}\n\n==See also==\n* [[Composition algebra]]\n* [[Hurwitz's theorem (normed division algebras)]]\n* [[Radon–Hurwitz number]]\n\n==References==\n{{reflist}}\n\n[[Category:Field theory]]\n[[Category:Quadratic forms]]\n[[Category:Mathematical problems]]"
    },
    {
      "title": "Isotropic line",
      "url": "https://en.wikipedia.org/wiki/Isotropic_line",
      "text": "{{For|isotropic lines in geology|Strain partitioning}}\n\nIn the geometry of [[quadratic form]]s, an '''isotropic line''' or '''null line''' is a [[Line (geometry)|line]] for which the quadratic form applied to the displacement vector between any pair of its points is zero. An isotropic line occurs only with an [[isotropic quadratic form]], and never with a [[definite quadratic form]].\n\nUsing [[complex geometry]], [[Edmond Laguerre]] first suggested the existence of two isotropic lines through the point {{nowrap|(''&alpha;'', ''&beta;'')}} that depend on the [[imaginary unit]] {{math|''i''}}:<ref name=Laguerre>[[Edmond Laguerre]] (1870) \"Sur l’emploi des imaginaires en la géométrie\", [https://archive.org/details/oeuvresdelaguer02fragoog  Oeuvres de Laguerre] 2: 89</ref>\n: First system: <math>(y - \\beta) = (x - \\alpha) i,</math>\n: Second system: <math>(y - \\beta) = -i (x - \\alpha) .</math>\nLaguerre then interpreted these lines as [[geodesic]]s:\n:An essential property of isotropic lines, and which can be used to define them, is the following: the distance between any two points of an isotropic line ''situated at a finite distance in the plane'' is zero. In other terms, these lines satisfy the [[differential equation]] {{nowrap|1=d''s''<sup>2</sup> = 0}}. On an arbitrary [[surface (differential geometry)|surface]] one can study curves that satisfy this differential equation; these curves are the geodesic lines of the surface, and we also call them ''isotropic lines''.<ref name=Laguerre/>{{rp|90}}\n\nIn the [[complex projective plane]], points are represented by [[homogeneous coordinates]] <math>(x_1, x_2, x_3)</math> and lines by homogeneous coordinates <math>(a_1, a_2, a_3)</math>. An '''isotropic line''' in the complex projective plane satisfies the equation:<ref>C. E. Springer (1964) ''Geometry and Analysis of Projective Spaces'', page 141, [[W. H. Freeman and Company]]</ref>\n:<math>a_3(x_2 \\pm i x_1) = (a_2 \\pm i a_1) x_2 .</math>\nIn terms of the affine subspace {{nowrap|1=''x''<sub>3</sub> = 1}}, an isotropic line through the origin is\n:<math>x_2 = \\pm i x_1 .</math>\n\nIn projective geometry, the isotropic lines are the ones passing through the [[circular points at infinity]].\n\nIn the real orthogonal geometry of [[Emil Artin]], isotropic lines occur in pairs:\n:A non-singular plane which contains an isotropic vector shall be called a [[isotropic quadratic form#Hyperbolic plane|hyperbolic plane]]. It can always be spanned by a pair ''N, M'' of vectors which satisfy <math>N^2 \\ =\\ M^2\\ =\\ 0, \\quad NM\\ =\\ 1\\ .</math>\n:We shall call any such ordered pair ''N, M'' a hyperbolic pair. If ''V'' is a non-singular plane with orthogonal geometry and ''N'' ≠ 0 is an isotropic vector of ''V'', then there exists precisely one ''M'' in ''V'' such that ''N, M'' is a hyperbolic pair. The vectors ''x N'' and ''y M'' are then the only isotropic vectors of ''V''.<ref>[[Emil Artin]] (1957) [[Geometric Algebra]],  page 119</ref>\n\n==Relativity==\nIsotropic lines have been used in cosmological writing to carry light. For example, in a mathematical encyclopedia, light consists of [[photon]]s: \"The [[worldline]] of a zero rest mass (such as a non-quantum model of a photon and other elementary particles of mass zero) is an isotropic line.\"<ref>[[Encyclopedia of Mathematics]] [https://www.encyclopediaofmath.org/index.php/World_line World line]</ref>\nFor isotropic lines through the origin, a particular point is a [[null vector]], and the collection of all such isotropic lines forms the [[light cone]] at the origin.\n\n[[Élie Cartan]] expanded the concept of isotropic lines to [[multivector]]s in his book on [[spinors in three dimensions]].<ref>{{Citation | last1=Cartan | first1=Élie | title=The theory of spinors | origyear=1938 | url=https://books.google.com/books?isbn=0486640701 |page=17| publisher=[[Dover Publications]] | location=New York | isbn=978-0-486-64070-9 | mr=631850 | year=1981}}</ref>\n\n==References==\n{{Reflist}}\n\n* Pete L. Clark, [http://www.math.miami.edu/~armstrong/685fa12/pete_clark.pdf Quadratic forms chapter I: Witts theory] from [[University of Miami]] in [[Coral Gables, Florida]].\n* [[O. Timothy O'Meara]] (1963,2000) ''Introduction to Quadratic Forms'', page 94\n \n{{DEFAULTSORT:Isotropic Line}}\n[[Category:Quadratic forms]]\n[[Category:Theory of relativity]]"
    },
    {
      "title": "Kaplansky's theorem on quadratic forms",
      "url": "https://en.wikipedia.org/wiki/Kaplansky%27s_theorem_on_quadratic_forms",
      "text": "In [[mathematics]], '''Kaplansky's theorem on quadratic forms''' is a result on simultaneous representation of [[Prime number|primes]] by [[quadratic forms]]. It was proved in 2003 by [[Irving Kaplansky]].<ref>{{citation\n | last = Kaplansky | first = Irving | authorlink = Irving Kaplansky\n | doi = 10.1090/S0002-9939-03-07022-9\n | issue = 7\n | journal = [[Proceedings of the American Mathematical Society]]\n | mr = 1963780\n | pages = 2299–2300 (electronic)\n | title = The forms {{math|''x'' + 32''y''<sup>2</sup> and ''x'' + 64''y''^2}} {{sic}}\n | volume = 131\n | year = 2003}}.</ref>\n\n==Statement of the theorem==\nKaplansky's theorem states that a prime ''p'' [[Modular arithmetic|congruent to 1 modulo 16]] is representable by both or none of ''x''<sup>2</sup>&nbsp;+&nbsp;32''y''<sup>2</sup> and ''x''<sup>2</sup>&nbsp;+&nbsp;64''y''<sup>2</sup>, whereas a prime ''p'' congruent to 9 modulo 16 is representable by exactly one of these quadratic forms.\n\nThis is remarkable since the primes represented by each of these forms individually are ''not'' describable by congruence conditions.<ref>{{citation\n | last = Cox | first = David A.\n | isbn = 0-471-50654-0\n | location = New York\n | mr = 1028322\n | publisher = John Wiley & Sons\n | title = Primes of the form {{math|x<sup>2</sup> + ny<sup>2</sup>}}\n | year = 1989}}.</ref>\n\n==Proof==\nKaplansky's proof uses the facts that 2 is a 4th power modulo ''p'' if and only if ''p'' is representable by ''x''<sup>2</sup>&nbsp;+&nbsp;64''y''<sup>2</sup>, and that &minus;4 is an 8th power modulo&nbsp;''p'' if and only if ''p'' is representable by ''x''<sup>2</sup>&nbsp;+&nbsp;32''y''<sup>2</sup>.\n\n==Examples==\n*The prime ''p''&nbsp;=&nbsp;17 is congruent to 1 modulo 16 and is representable by neither ''x''<sup>2</sup>&nbsp;+&nbsp;32''y''<sup>2</sup> nor ''x''<sup>2</sup>&nbsp;+&nbsp;64''y''<sup>2</sup>.\n*The prime ''p''=113 is congruent to 1 modulo 16 and is representable by both ''x''<sup>2</sup>&nbsp;+&nbsp;32''y''<sup>2</sup> and ''x''<sup>2</sup>+64''y''<sup>2</sup> (since 113&nbsp;=&nbsp;9<sup>2</sup>&nbsp;+&nbsp;32&times;1<sup>2</sup> and 113&nbsp;=&nbsp;7<sup>2</sup>&nbsp;+&nbsp;64&times;1<sup>2</sup>).\n*The prime ''p''&nbsp;=&nbsp;41 is congruent to 9 modulo 16 and is representable by ''x''<sup>2</sup>&nbsp;+&nbsp;32''y''<sup>2</sup> (since 41&nbsp;=&nbsp;3<sup>2</sup>&nbsp;+&nbsp;32&times;1<sup>2</sup>), but not by ''x''<sup>2</sup>&nbsp;+&nbsp;64''y''<sup>2</sup>.\n*The prime ''p''&nbsp;=&nbsp;73 is congruent to 9 modulo 16 and is representable by ''x''<sup>2</sup>&nbsp;+&nbsp;64''y''<sup>2</sup> (since 73&nbsp;=&nbsp;3<sup>2</sup>&nbsp;+&nbsp;64&times;1<sup>2</sup>), but not by ''x''<sup>2</sup>&nbsp;+&nbsp;32''y''<sup>2</sup>.\n\n==Similar results==\nFive results similar to Kaplansky's theorem are known:<ref>{{citation\n | last = Brink | first = David\n | doi = 10.1016/j.jnt.2008.04.007\n | issue = 2\n | journal = [[Journal of Number Theory]]\n | mr = 2473893\n | pages = 464–468\n | title = Five peculiar theorems on simultaneous representation of primes by quadratic forms\n | volume = 129\n | year = 2009}}.</ref>\n\n*A prime ''p'' congruent to 1 modulo 20 is representable by both or none of ''x''<sup>2</sup>&nbsp;+&nbsp;20''y''<sup>2</sup> and ''x''<sup>2</sup>&nbsp;+&nbsp;100''y''<sup>2</sup>, whereas a prime ''p'' congruent to 9 modulo 20 is representable by exactly one of these quadratic forms.\n*A prime ''p'' congruent to 1, 16 or 22 modulo 39 is representable by both or none of ''x''<sup>2</sup>&nbsp;+&nbsp;''xy''&nbsp;+&nbsp;10''y''<sup>2</sup> and ''x''<sup>2</sup>&nbsp;+&nbsp;''xy''&nbsp;+&nbsp;127''y''<sup>2</sup>, whereas a prime ''p'' congruent to 4, 10 or 25 modulo 39 is representable by exactly one of these quadratic forms.\n*A prime ''p'' congruent to 1, 16, 26, 31 or 36 modulo 55 is representable by both or none of ''x''<sup>2</sup>&nbsp;+&nbsp;''xy''&nbsp;+&nbsp;14''y''<sup>2</sup> and ''x''<sup>2</sup>&nbsp;+&nbsp;''xy''&nbsp;+&nbsp;69''y''<sup>2</sup>, whereas a prime ''p'' congruent to 4, 9, 14, 34 or 49 modulo 55 is representable by exactly one of these quadratic forms.\n*A prime ''p'' congruent to 1, 65 or 81 modulo 112 is representable by both or none of ''x''<sup>2</sup>&nbsp;+&nbsp;14''y''<sup>2</sup> and ''x''<sup>2</sup>&nbsp;+&nbsp;448''y''<sup>2</sup>, whereas a prime ''p'' congruent to 9, 25 or 57 modulo 112 is representable by exactly one of these quadratic forms.\n*A prime ''p'' congruent to 1 or 169 modulo 240 is representable by both or none of ''x''<sup>2</sup>&nbsp;+&nbsp;150''y''<sup>2</sup> and ''x''<sup>2</sup>&nbsp;+&nbsp;960''y''<sup>2</sup>, whereas a prime ''p'' congruent to 49 or 121 modulo 240 is representable by exactly one of these quadratic forms.\n\nIt is conjectured that there are no other similar results involving definite forms.\n\n==Notes==\n{{reflist}}\n\n[[Category:Theorems in number theory]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "L-theory",
      "url": "https://en.wikipedia.org/wiki/L-theory",
      "text": "{{DISPLAYTITLE:''L''-theory}}\nIn [[mathematics]] algebraic '''L-theory''' is the [[K-theory]] of [[quadratic form]]s; the term was coined by [[C. T. C. Wall]], \nwith ''L'' being used as the letter after ''K''. Algebraic ''L''-theory, also known as 'hermitian ''K''-theory',\nis important in [[surgery theory]].\n\n==Definition==\nOne can define ''L''-groups for any [[ring with involution]] ''R'': the quadratic ''L''-groups <math>L_*(R)</math> (Wall) and the symmetric ''L''-groups <math>L^*(R)</math> (Mishchenko, Ranicki).\n\n=== Even dimension ===\nThe even-dimensional ''L''-groups <math>L_{2k}(R)</math> are defined as the [[Witt group]]s of [[ε-quadratic forms]] over the ring ''R'' with <math>\\epsilon = (-1)^k</math>. More precisely, \n\n::<math>L_{2k}(R)</math> \n\nis the abelian group of equivalence classes <math>[\\psi]</math> of non-degenerate ε-quadratic forms <math>\\psi \\in Q_\\epsilon(F)</math> over R, where the underlying R-modules F are finitely generated free. The equivalence relation is given by stabilization with respect to [[hyperbolic ε-quadratic forms]]: \n\n:<math>[\\psi] = [\\psi'] \\Longleftrightarrow n, n' \\in {\\mathbb N}_0: \\psi \\oplus H_{(-1)^k}(R)^n \\cong \\psi' \\oplus H_{(-1)^k}(R)^{n'}</math>.\n\nThe addition in <math>L_{2k}(R)</math> is defined by \n\n:<math>[\\psi_1] + [\\psi_2] := [\\psi_1 \\oplus \\psi_2].</math>\n\nThe zero element is represented by <math>H_{(-1)^k}(R)^n</math> for any <math>n \\in {\\mathbb N}_0</math>. The inverse of <math>[\\psi]</math> is <math>[-\\psi]</math>.\n\n=== Odd dimension ===\nDefining odd-dimensional ''L''-groups is more complicated; further details and the definition of the odd-dimensional ''L''-groups can be found in the references mentioned below.\n\n==Examples and applications==\nThe ''L''-groups of a group <math>\\pi</math> are the ''L''-groups <math>L_*(\\mathbf{Z}[\\pi])</math> of the [[group ring]] <math>\\mathbf{Z}[\\pi]</math>. In the applications to topology  <math>\\pi</math> is the [[fundamental group]]\n<math>\\pi_1 (X)</math> of a space <math>X</math>. The quadratic ''L''-groups <math>L_*(\\mathbf{Z}[\\pi])</math>\nplay a central role in the surgery classification of the homotopy types of <math>n</math>-dimensional [[manifolds]] of dimension <math>n > 4</math>, and in the formulation of the [[Novikov conjecture]].\n\nThe distinction between symmetric ''L''-groups and quadratic ''L''-groups, indicated by upper and lower indices, reflects the usage in group homology and cohomology. The [[group cohomology]] <math>H^*</math> of the cyclic group <math>\\mathbf{Z}_2</math> deals with the fixed points of a <math>\\mathbf{Z}_2</math>-action, while the [[group homology]] <math>H_*</math> deals with the orbits of a <math>\\mathbf{Z}_2</math>-action; compare <math>X^G</math> (fixed points) and <math>X_G = X/G</math> (orbits, quotient) for upper/lower index notation.\n\nThe quadratic ''L''-groups: <math>L_n(R)</math> and the symmetric ''L''-groups: <math>L^n(R)</math> are related by \na symmetrization map <math>L_n(R) \\to L^n(R)</math> which is an isomorphism modulo 2-torsion, and which corresponds to the [[polarization identities]].\n\nThe quadratic and the symmetric ''L''-groups are 4-fold periodic (the comment of Ranicki, page 12, on the non-periodicity of the symmetric ''L''-groups refers to another type of ''L''-groups, defined using \"short complexes\").\n\nIn view of the applications to the [[classification of manifolds]] there are extensive calculations of\nthe quadratic <math>L</math>-groups <math>L_*(\\mathbf{Z}[\\pi])</math>. For finite <math>\\pi</math>\nalgebraic methods are used, and mostly geometric methods (e.g. controlled topology) are used for infinite <math>\\pi</math>. \n\nMore generally, one can define ''L''-groups for any [[additive category]] with a ''chain duality'', as in Ranicki (section 1).\n\n=== Integers ===\nThe '''simply connected ''L''-groups''' are also the ''L''-groups of the integers, as\n<math>L(e) := L(\\mathbf{Z}[e]) = L(\\mathbf{Z})</math> for both <math>L</math> = <math>L^*</math> or <math>L_*.</math> For quadratic ''L''-groups, these are the surgery obstructions to [[simply connected]] surgery.\n\nThe quadratic ''L''-groups of the integers are:\n:<math>\\begin{align}\nL_{4k}(\\mathbf{Z}) &= \\mathbf{Z}   && \\text{signature}/8\\\\\nL_{4k+1}(\\mathbf{Z}) &= 0\\\\\nL_{4k+2}(\\mathbf{Z}) &= \\mathbf{Z}/2 && \\text{Arf invariant}\\\\\nL_{4k+3}(\\mathbf{Z}) &= 0.\n\\end{align}</math>\nIn [[doubly even]] dimension (4''k''), the quadratic ''L''-groups detect the [[signature (topology)|signature]]; in [[singly even]] dimension (4''k''+2), the ''L''-groups detect the [[Arf invariant]] (topologically the [[Kervaire invariant]]).\n\nThe symmetric ''L''-groups of the integers are:\n:<math>\\begin{align}\nL^{4k}(\\mathbf{Z}) &= \\mathbf{Z} && \\text{signature}\\\\\nL^{4k+1}(\\mathbf{Z}) &= \\mathbf{Z}/2 && \\text{de Rham invariant}\\\\\nL^{4k+2}(\\mathbf{Z}) &= 0\\\\\nL^{4k+3}(\\mathbf{Z}) &= 0.\n\\end{align}</math>\nIn doubly even dimension (4''k''), the symmetric ''L''-groups, as with the quadratic ''L''-groups, detect the signature; in dimension (4''k''+1), the ''L''-groups detect the [[de Rham invariant]].\n\n==References==\n\n*{{Citation | last1=Lück | first1=Wolfgang | authorlink=Wolfgang Lück |title=Topology of high-dimensional manifolds, No. 1, 2 (Trieste, 2001) | url=http://www.math.uni-muenster.de/u/lueck/publ/lueck/ictp.pdf | publisher=Abdus Salam Int. Cent. Theoret. Phys., Trieste | series=ICTP Lect. Notes | mr=1937016 | year=2002 | volume=9 | chapter=A basic introduction to surgery theory | pages=1–224}}\n*{{Citation | last1=Ranicki | first1=Andrew A. |authorlink=Andrew Ranicki| title=Algebraic L-theory and topological manifolds | url=http://www.maths.ed.ac.uk/~aar/books/topman.pdf | publisher=[[Cambridge University Press]] | series=Cambridge Tracts in Mathematics | isbn=978-0-521-42024-2 | mr=1211640 | year=1992 | volume=102}}\n*{{Citation | last1=Wall | first1=C. T. C. |authorlink1=C. T. C. Wall| editor1-last=Ranicki | editor1-first=Andrew | editor1-link=Andrew Ranicki|title=Surgery on compact manifolds | origyear=1970 | url=http://www.maths.ed.ac.uk/~aar/books/scm.pdf | publisher=[[American Mathematical Society]] | location=Providence, R.I. | edition=2nd | series=Mathematical Surveys and Monographs | isbn=978-0-8218-0942-6 | mr=1687388 | year=1999 | volume=69}}\n\n[[Category:Geometric topology]]\n[[Category:Algebraic topology]]\n[[Category:Quadratic forms]]\n[[Category:Surgery theory]]"
    },
    {
      "title": "Markov spectrum",
      "url": "https://en.wikipedia.org/wiki/Markov_spectrum",
      "text": "In mathematics, the '''Markov spectrum''' devised by [[Andrey Markov]] is a complicated set of real numbers arising in [[Markov number|Markov Diophantine equation]] and also in the theory of [[Diophantine approximation]].\n\n== Quadratic form characterization ==\nConsider a [[quadratic form]] given by ''f''(''x'',''y'') = ''ax''<sup>2</sup> + ''bxy'' + ''cy''<sup>2</sup>  and suppose that its [[Discriminant#Quadratic forms|discriminant]] is fixed, say equal to −1/4. In other words, ''b''<sup>2</sup> − 4''ac'' = 1.\n\nOne can try to search what is the minimum integral value obtained by ''f'' when it is evaluated at non-zero vectors of the grid  <math>\\mathbb{Z}^2</math>, and if this minimum does not exist, what is the [[Infimum and supremum|infimum]].\n\nThe Markov spectrum ''M'' is the set obtained by repeating this search with different quadratic forms with discriminant fixed to −1/4:<math>M = \\left\\{ \\left(\\inf_{(x,y)\\in \\mathbb{Z}^2\\smallsetminus\\{(0,0)\\}} |f(x,y)| \\right)^{-1} : f(x,y) = ax^2 + bxy + cy^2,\\ b^2- 4ac = 1 \\right\\}</math>\n\n==Lagrange spectrum==\n{{details|Lagrange number}}\nStarting from [[Hurwitz's theorem (number theory)|Hurwitz's theorem]] on Diophantine approximation, that any real number <math>\\xi</math> has a sequence of rational approximations ''m''/''n'' tending to it with\n\n:<math>\\left |\\xi-\\frac{m}{n}\\right |<\\frac{1}{\\sqrt{5}\\, n^2},</math>\n\nit is possible to ask for each value of 1/''c'' with 1/''c'' &ge; {{radic|5}} about the existence of some <math>\\xi</math> for which\n\n:<math>\\left |\\xi-\\frac{m}{n}\\right |<\\frac{c} {n^2}</math>\n\nfor such a sequence, for which ''c'' is the best possible (maximal) value. Such 1/''c'' make up the '''Lagrange spectrum''' ''L'', a set of real numbers at least {{radic|5}} (which is the smallest value of the spectrum). The formulation with the reciprocal is awkward, but the traditional definition invites it; looking at the set of ''c'' instead allows a definition instead by means of an [[Limit superior and limit inferior|inferior limit]]. For that, consider\n:<math>\\liminf_{n \\to \\infty}n^2\\left |\\xi-\\frac{m}{n}\\right |,</math>\n\nwhere ''m'' is chosen as an integer function of ''n'' to make the difference minimal. This is a function of <math>\\xi</math>, and the reciprocal of the Lagrange spectrum is the range of values it takes on irrational numbers.\n\n=== Relation with Markov spectrum ===\nThe initial part of the Lagrange spectrum, namely the part lying in the interval [{{radic|5}}, 3), is equal to the Markov spectrum. The first few values are {{radic|5}}, {{radic|8}}, {{radic|221}}/5, {{radic|1517}}/13, ...<ref>Cassels (1957) p.18</ref> and the ''n''th number of this sequence (that is, the ''n''th [[Lagrange number]]) can be calculated from the ''n''th [[Markov number]] by the formula<blockquote><math>L_n = \\sqrt{9 - {4 \\over {m_n}^2}}.</math></blockquote>'''Freiman's constant''' is the name given to the end of the last gap in the Lagrange spectrum, namely:\n\n: <math> F = \\frac{2\\,221\\,564\\,096 + 283\\,748\\sqrt{462}}{491\\, 993\\, 569} = 4.5278295661\\dots</math> {{OEIS|A118472}}.\n\nReal numbers greater than ''F'' are also members of the Markov spectrum.<ref name=mathworld2>[http://mathworld.wolfram.com/FreimansConstant.html Freiman's Constant] Weisstein, Eric W. \"Freiman's Constant.\" From MathWorld&mdash;A Wolfram Web Resource), accessed 26 August 2008</ref>  Moreover, it is possible to prove that ''L'' is strictly contained in ''M''.<ref>{{Cite book|url=https://doi.org/10.1090/surv/030/03|title=The Markoff and Lagrange spectra compared|last=Cusick|first=Thomas|last2=Flahive|first2=Mary|author2-link= Mary Flahive |pages=35–45|doi=10.1090/surv/030/03}}</ref>\n\n== Geometry of Markov and Lagrange spectrum ==\nOn one hand, the initial part of the Markov and Lagrange spectrum lying in the interval [{{radic|5}}, 3) are both equal and they are a discrete set. On the other hand, the final part of these sets lying after Freiman's constant are also equal, but a continuous set. The geometry of the part between the initial part and final part has a fractal structure, and can be seen as a geometric transition between the discrete initial part and the continuous final part. This is stated precisely in the next theorem:<ref>{{Cite journal|last=Ibarra|first=Sergio Augusto Romaña|last2=Moreira|first2=Carlos Gustavo T. De A.|date=August 2017|title=On the Lagrange and Markov dynamical spectra|url=https://www.cambridge.org/core/journals/ergodic-theory-and-dynamical-systems/article/on-the-lagrange-and-markov-dynamical-spectra/444EC914B6639E5C515D77223CAA6ACC|journal=Ergodic Theory and Dynamical Systems|volume=37|issue=5|pages=1570–1591|doi=10.1017/etds.2015.121|issn=0143-3857}}</ref><blockquote>Given <math>t \\in \\mathbb{R}</math>, the [[Hausdorff dimension]] of <math>L\\cap(-\\infty,t)</math> is equal to the Hausdorff dimension of <math>M\\cap(-\\infty,t)</math>. Moreover, if ''d'' is the function defined as <math>d(t):=\\dim_{H}(M\\cup(-\\infty,t))</math>, where dim<sub>''H''</sub> denotes the Hausdorff dimension, then ''d'' is continuous and maps '''R''' onto [0,1].</blockquote>\n\n==See also==\n*[[Markov number]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*Conway, J. H. and Guy, R. K. The Book of Numbers. New York: Springer-Verlag, pp.&nbsp;188&ndash;189, 1996.\n*Cusick, T. W. and [[Mary Flahive|Flahive, M. E.]] The Markov and Lagrange Spectra. Providence, RI: Amer. Math. Soc., 1989.\n* {{cite book | first=J.W.S. | last=Cassels | authorlink=J. W. S. Cassels | title=An introduction to Diophantine approximation | series=Cambridge Tracts in Mathematics and Mathematical Physics | volume=45 | publisher=[[Cambridge University Press]] | year=1957 | zbl=0077.04801 }}\n\n==External links==\n*{{Springer|id=m/m062540|title=Markov spectrum problem}}\n\n[[Category:Diophantine approximation]]\n[[Category:Quadratic forms]]\n[[Category:Combinatorics]]"
    },
    {
      "title": "Meyer's theorem",
      "url": "https://en.wikipedia.org/wiki/Meyer%27s_theorem",
      "text": "{{For|the theorem in computational complexity theory|P/poly}}\n\nIn [[number theory]], '''Meyer's theorem''' on [[quadratic form]]s states that an [[indefinite quadratic form]] ''Q'' in five or more variables over the [[field (mathematics)|field]] of [[rational number]]s nontrivially represents zero. In other words, if the equation\n\n:''Q''(''x'') = 0\n\nhas a non-zero [[real number|real]] solution, then it has a non-zero rational solution (the converse is obvious). By clearing the denominators, an integral solution ''x'' may also be found.\n\nMeyer's theorem is usually deduced from the [[Hasse–Minkowski theorem]] (which was proved later) and the following statement: \n\n: A rational quadratic form in five or more variables represents zero over the field '''Q'''<sub>''p''</sub> of the [[p-adic number]]s  for all ''p''.\n\nMeyer's theorem is best possible with respect to the number of variables: there are indefinite rational quadratic forms ''Q'' in four variables which do not represent zero. One family of examples is given by \n\n:''Q''(''x''<sub>1</sub>,''x''<sub>2</sub>,''x''<sub>3</sub>,''x''<sub>4</sub>) = ''x''<sub>1</sub><sup>2</sup> + ''x''<sub>2</sub><sup>2</sup> &minus; ''p''(''x''<sub>3</sub><sup>2</sup> + ''x''<sub>4</sub><sup>2</sup>),\n\nwhere ''p'' is a [[prime number]] that is [[modular arithmetic|congruent]] to 3 modulo 4. This can be proved by the method of [[infinite descent]] using the fact that if the sum of two [[square number|perfect squares]] is divisible by such a ''p'' then each summand is divisible by ''p''.\n\n== See also ==\n\n* [[Lattice (group)]]\n* [[Oppenheim conjecture]]\n\n==References==\n* {{cite journal | first=A. | last=Meyer | title=Mathematische Mittheilungen | journal=Vierteljahrschrift der Naturforschenden Gesellschaft in Zürich | volume=29 | pages=209–222 | year=1884 }}\n* {{cite book | first1=J. | last1=Milnor | author1-link=John Milnor| first2=D. | last2=Husemoller | title=Symmetric Bilinear Forms | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 }}\n* {{cite book | first=Jean-Pierre | last=Serre | authorlink=Jean-Pierre Serre | title=A Course in Arithmetic | series=[[Graduate Texts in Mathematics]] | volume=7 | publisher=[[Springer-Verlag]] | year=1973 | isbn=0-387-90040-3 | zbl=0256.12001 }}\n* {{cite book | first=J.W.S. | last=Cassels | authorlink=J. W. S. Cassels | title=Rational Quadratic Forms | series=London Mathematical Society Monographs | volume=13 | publisher=[[Academic Press]] | year=1978 | isbn=0-12-163260-1 | zbl=0395.10029 }}\n\n[[Category:Quadratic forms]]\n[[Category:Theorems in number theory]]"
    },
    {
      "title": "Niemeier lattice",
      "url": "https://en.wikipedia.org/wiki/Niemeier_lattice",
      "text": "In [[mathematics]], a '''Niemeier lattice''' is one of the 24 \n[[Definite bilinear form|positive definite]] even [[unimodular lattice]]s of [[Rank of an abelian group|rank]] 24,\nwhich were classified by {{harvs|txt|authorlink=Hans-Volker Niemeier|first=Hans-Volker|last= Niemeier|year=1973}}. {{harvtxt|Venkov|1978}} gave a simplified proof of the classification. {{harvtxt|Witt|1941}} has a sentence mentioning that he found more than 10 such lattices, but gives no further details. One example of a Niemeier lattice is the [[Leech lattice]].\n\n== Classification ==\nNiemeier lattices are usually labeled by the [[Dynkin diagram]] of their\n[[root system]]s.  These Dynkin diagrams have rank either 0 or 24, and all of their components have the same [[Coxeter number]]. (The Coxeter number, at least in these cases,  is \nthe number of roots divided by the dimension.) There are exactly 24 Dynkin diagrams with these properties, and there turns out to be a unique Niemeier\nlattice for each of these Dynkin diagrams.\n\nThe complete list of Niemeier lattices is given in the following table.\nIn the table, \n:''G''<sub>0</sub> is the order of the group generated by reflections\n:''G''<sub>1</sub> is the order of the group of automorphisms fixing all components of the Dynkin diagram\n:''G''<sub>2</sub> is the order of the group of automorphisms of permutations of components of the Dynkin diagram\n:''G''<sub>&infin;</sub> is the index of the root lattice in the Niemeier lattice, in other words the order of the \"glue code\". It is the square root of the discriminant of the root lattice.\n:''G''<sub>0</sub>×''G''<sub>1</sub>×''G''<sub>2</sub> is the order of the automorphism group of the lattice\n:''G''<sub>&infin;</sub>×''G''<sub>1</sub>×''G''<sub>2</sub> is the order of the automorphism group of the corresponding deep hole.\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|-\n! Lattice root system!! Coxeter number !! ''G''<sub>0</sub>!! ''G''<sub>1</sub> !! ''G''<sub>2</sub>!! ''G''<sub>∞</sub> \n|-\n| [[Leech lattice]] (no roots)|| 0 || 1 ||[[Conway group|2Co<sub>1</sub>]] || 1|| '''Z'''<sup>24</sup>\n|-\n|A<sub>1</sub><sup>24</sup>|| 2 || 2<sup>24</sup> || 1 || [[Mathieu group|M<sub>24</sub>]]  || 2<sup>12</sup>\n|-\n|A<sub>2</sub><sup>12</sup>|| 3 || 3!<sup>12</sup> || 2 || [[Mathieu group|M<sub>12</sub>]] || 3<sup>6</sup>\n|-\n|A<sub>3</sub><sup>8</sup>|| 4 || 4!<sup>8</sup> || 2 || 1344 || 4<sup>4</sup>\n|-\n|A<sub>4</sub><sup>6</sup>|| 5 || 5!<sup>6</sup> || 2 || 120 || 5<sup>3</sup>\n|-\n|A<sub>5</sub><sup>4</sup>D<sub>4</sub>|| 6 || 6!<sup>4</sup>(2<sup>3</sup>4!) || 2 || 24 || 72\n|-\n|D<sub>4</sub><sup>6</sup>|| 6 || (2<sup>3</sup>4!)<sup>6</sup> || 3  || 720 || 4<sup>3</sup>\n|-\n|A<sub>6</sub><sup>4</sup>|| 7 || 7!<sup>4</sup> || 2 || 12 || 7<sup>2</sup>\n|-\n|A<sub>7</sub><sup>2</sup>D<sub>5</sub><sup>2</sup>|| 8 || 8!<sup>2</sup> (2<sup>4</sup>5!)<sup>2</sup>|| 2 || 4 || 32\n|-\n|A<sub>8</sub><sup>3</sup>|| 9 || 9!<sup>3</sup> || 2 || 6 || 27\n|-\n|A<sub>9</sub><sup>2</sup>D<sub>6</sub>|| 10 || 10!<sup>2</sup> (2<sup>5</sup>6!)|| 2 || 2 || 20\n|-\n|D<sub>6</sub><sup>4</sup>|| 10 || (2<sup>5</sup>6!)<sup>4</sup> || 1 || 24 || 16\n|-\n|E<sub>6</sub><sup>4</sup>|| 12 || (2<sup>7</sup>3<sup>4</sup>5)<sup>4</sup> || 2 || 24 || 9\n|-\n|A<sub>11</sub>D<sub>7</sub>E<sub>6</sub>|| 12 || 12!(2<sup>6</sup>7!)(2<sup>7</sup>3<sup>4</sup>5) || 2 || 1 || 12\n|-\n|A<sub>12</sub><sup>2</sup>|| 13 || (13!)<sup>2</sup> || 2 || 2 || 13\n|-\n|D<sub>8</sub><sup>3</sup>|| 14 || (2<sup>7</sup>8!)<sup>3</sup> || 1 || 6  || 8\n|-\n|A<sub>15</sub>D<sub>9</sub>|| 16 || 16!(2<sup>8</sup>9!) || 2 || 1 || 8\n|-\n|A<sub>17</sub>E<sub>7</sub>|| 18 || 18!(2<sup>10</sup>3<sup>4</sup>5.7) || 2 || 1 || 6\n|-\n|D<sub>10</sub>E<sub>7</sub><sup>2</sup>|| 18 || (2<sup>9</sup>10!)(2<sup>10</sup>3<sup>4</sup>5.7)<sup>2</sup> || 1 || 2  || 4\n|-\n|D<sub>12</sub><sup>2</sup>|| 22 ||(2<sup>11</sup>12!)<sup>2</sup>  || 1 || 2 || 4\n|-\n|A<sub>24</sub>|| 25 || 25!  || 2 || 1  || 5\n|-\n|D<sub>16</sub>E<sub>8</sub>|| 30 || (2<sup>15</sup>16!)(2<sup>14</sup>3<sup>5</sup>5<sup>2</sup>7) || 1 || 1 || 2\n|-\n|E<sub>8</sub><sup>3</sup>|| 30 ||(2<sup>14</sup>3<sup>5</sup>5<sup>2</sup>7)<sup>3</sup>  || 1 || 6 || 1\n|- \n|D<sub>24</sub>|| 46 || 2<sup>23</sup>24! || 1 || 1  || 2\n|}\n\n==The neighborhood graph of the Niemeier lattices==\nIf ''L'' is an odd unimodular lattice of dimension 8''n'' and ''M'' its sublattice of even vectors, then ''M'' is contained in exactly 3 unimodular lattices, one of which is ''L'' and the other two of which are even. (If ''L'' has a norm 1 vector then the two even lattices are [[isomorphic]].) The '''Kneser neighborhood graph''' in 8''n'' dimensions has a point for each even lattice, and a line joining two points for each odd 8''n'' dimensional lattice with no norm 1 vectors, where the vertices of each line are the two even lattices associated to the odd lattice. There may be several lines between the same pair of vertices, and there may be lines from a vertex to itself. Kneser proved that this graph  is always connected. In 8 dimensions it has one point and no lines, in 16 dimensions it has two points joined by one line, and in 24 dimensions it is the following graph:\n\n[[Image:Neighborhood graph of Niemeier lattices.svg]]\n\nEach point represents one of the 24 Niemeier lattices, and the lines joining them represent the 24 dimensional odd unimodular lattices with no norm 1 vectors. (Thick lines represent multiple lines.) The number on the right is the Coxeter number of the Niemeier lattice.\n\nIn 32 dimensions the neighborhood graph has more than a billion vertices.\n\n== Properties ==\nSome of the Niemeier lattices are related to [[sporadic simple group]]s. \nThe Leech lattice is acted  on by a [[Double covering group|double cover]] of the [[Conway group]], \nand the lattices A<sub>1</sub><sup>24</sup> and A<sub>2</sub><sup>12</sup>\nare acted on by the [[Mathieu group]]s M<sub>24</sub> and M<sub>12</sub>.\n\nThe Niemeier lattices, other than the Leech lattice, correspond to \nthe ''deep holes'' of the Leech lattice. This implies that the [[affine Dynkin diagram]]s of the Niemeier lattices can be seen inside the Leech lattice, when \ntwo points of the Leech lattice are joined by no lines when they have distance\n<math>\\sqrt 4</math>, by 1 line if they have distance <math>\\sqrt 6</math>,\nand by a double line if they have distance <math>\\sqrt 8</math>.\n\nNiemeier lattices also correspond to the 24 orbits of primitive norm zero vectors ''w'' of the even unimodular Lorentzian lattice [[II25,1|II<sub>25,1</sub>]], where the Niemeier lattice corresponding to ''w'' is ''w''<sup>&perp;</sup>/''w''.\n\n== References==\n\n*{{citation|title=Formes automorphes et voisins de Kneser des réseaux de Niemeier\n|first=Gaëtan |last=Chenevier |first2= Jean |last2=Lannes\n|year= 2014|arxiv=1409.7616|bibcode=2014arXiv1409.7616C}}\n*{{cite book\n |author1=[[John H. Conway|Conway, J. H.]] |author2=[[Neil Sloane|Sloane, N. J. A.]]\n | title = Sphere Packings, Lattices, and Groups\n | edition = 3rd\n | year = 1998\n | isbn = 0-387-98585-9\n | publisher = Springer-Verlag}}\n*{{Citation | last1=Ebeling | first1=Wolfgang | title=Lattices and codes | origyear=1994 | url=https://books.google.com/books?id=RVt5QgAACAAJ | publisher=Friedr. Vieweg & Sohn | location=Braunschweig | edition=revised | series=Advanced Lectures in Mathematics | isbn=978-3-528-16497-3 | mr=1938666 | year=2002}}\n*{{Cite journal\n | last = Niemeier|first= Hans-Volker\n | title = Definite quadratische Formen der Dimension 24 und Diskriminate 1.\n | format = In German\n | journal = [[Journal of Number Theory]]\n | volume = 5\n | year = 1973\n | pages = 142–178\n | mr = 0316384\n | doi = 10.1016/0022-314X(73)90068-1\n | ref = harv\n | postscript = <!--None-->\n | issue = 2| bibcode = 1973JNT.....5..142N\n }}\n*{{Citation | last1=Venkov | first1=B. B. | title=On the classification of integral even unimodular 24-dimensional quadratic forms | mr=558941 | year=1978 | journal=Akademiya Nauk Soyuza Sovetskikh Sotsialisticheskikh Respublik. Trudy Matematicheskogo Instituta imeni V. A. Steklova | issn=0371-9685 | volume=148 | pages=65–76}} English translation in {{harvtxt|Conway|Sloane|1998}}\n*{{Citation | last1=Witt | first1=Ernst | author1-link=Ernst Witt | title=Eine Identität zwischen Modulformen zweiten Grades | doi=10.1007/BF02940750 | mr=0005508 | year=1941 | journal=Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg | volume=14 | pages=323–337}}\n*{{Citation | last1=Witt | first1=Ernst | author1-link=Ernst Witt | title=Collected papers. Gesammelte Abhandlungen | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-57061-5 | mr=1643949 | year=1998}}\n\n== External links ==\n* [http://www.math.rwth-aachen.de/~Gabriele.Nebe/LATTICES/ Aachen University lattice catalogue]\n\n[[Category:Quadratic forms]]\n[[Category:Lattice points]]"
    },
    {
      "title": "Null vector",
      "url": "https://en.wikipedia.org/wiki/Null_vector",
      "text": "{{about||the additive identity of a vector space|zero element|null vector in Minkowski space|Minkowski space}}\n[[File:Conformalsphere.pdf|thumb|A null cone where <math>q(x,y,z) = x^2 + y^2 - z^2 .</math>]]\n\nIn [[mathematics]], given a [[vector space]] ''X'' with an associated [[quadratic form]] ''q'', written {{nowrap|(''X'', ''q'')}}, a '''null vector''' or '''isotropic vector''' is a non-zero element ''x'' of ''X'' for which {{nowrap|1=''q''(''x'') = 0}}.\n\nIn the theory of [[real number|real]] [[bilinear form]]s, [[definite quadratic form]]s and [[isotropic quadratic form]]s are distinct. They are distinguished in that ''only for the latter'' does there exists a nonzero null vector.\n\nA quadratic space {{nowrap|(''X'', ''q'')}} which has a null vector is called a [[pseudo-Euclidean space]].\n\nA pseudo-Euclidean vector space may be decomposed (non-uniquely) into [[orthogonal subspaces]] ''A'' and ''B'', {{nowrap|1=''X'' = ''A'' + ''B''}}, where ''q'' is positive-definite on ''A'' and negative-definite on ''B''. The '''null cone''', or '''isotropic cone''', of ''X'' consists of the union of balanced spheres:\n:<math>\\bigcup_{r\\ge0} \\{x = a + b : q(a) = -q(b) = r, a \\in A, b \\in B \\}.</math>\nThe null cone is also the union of the [[isotropic line]]s through the origin.\n\n==Examples==\nThe [[Minkowski space#Causal structure|light-like]] vectors of [[Minkowski space]] are null vectors.\n\nThe four [[linearly independent]] [[biquaternion]]s {{nowrap|1=''l'' = 1 + ''hi''}}, {{nowrap|1=''n'' = 1 + ''hj''}}, {{nowrap|1=''m'' = 1 + ''hk''}}, and {{nowrap|1=''m''<sup>∗</sup> = 1 – ''hk''}} are null vectors and {{nowrap|{ ''l'', ''n'', ''m'', ''m''<sup>∗</sup> }{{void}}}} can serve as a [[basis (linear algebra)|basis]] for the subspace used to represent [[spacetime]]. Null vectors are also used in the [[Newman–Penrose formalism]] approach to spacetime manifolds.<ref>Patrick Dolan (1968) [http://projecteuclid.org/euclid.cmp/1103840725 A Singularity-free solution of the Maxwell-Einstein Equations], [[Communications in Mathematical Physics]] 9(2):161–8, especially 166, link from [[Project Euclid]]</ref>\n\nA [[composition algebra]] ''splits'' when it has a null vector; otherwise it is a [[division algebra]].\n\nIn the [[Verma module]] of a [[Lie algebra]] there are null vectors.\n\n==References==\n{{Reflist}}\n\n* {{cite book |first=B. A. |last=Dubrovin |first2=A. T. |last2=Fomenko |authorlink2=Anatoly Fomenko |first3=S. P. |last3=Novikov |authorlink3=Sergei Novikov (mathematician) |year=1984 |title=Modern Geometry: Methods and Applications |translator-first=Robert G. |translator-last=Burns |page=50 |publisher=Springer |isbn=0-387-90872-2 |url=https://books.google.com/books?id=o5ePoAEACAAJ&pg=PA50 }}\n* {{cite book |first=Ronald |last=Shaw |year=1982 |title=Linear Algebra and Group Representations |volume=1 |page=151 |publisher=[[Academic Press]] |isbn=0-12-639201-3 |url=https://books.google.com/books?id=C6DgAAAAMAAJ }}\n* {{cite book | last = Neville | first = E. H. (Eric Harold) | author-link =Eric Harold Neville  | title =Prolegomena to Analytical Geometry in Anisotropic Euclidean Space of Three Dimensions | publisher =[[Cambridge University Press]] | date = 1922 |page=204| url =https://archive.org/details/prolegomenatoana00nevi }}\n\n\n[[Category:Linear algebra]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Perfect lattice",
      "url": "https://en.wikipedia.org/wiki/Perfect_lattice",
      "text": "In mathematics, a '''perfect lattice''' (or '''perfect form''') is a [[Lattice (group)|lattice]] in a [[Euclidean space|Euclidean vector space]], that is completely determined by the set ''S'' of its minimal vectors in the sense that there is only one positive definite quadratic form taking value 1 at all points of ''S''. Perfect lattices were introduced by {{harvtxt|Korkine|Zolotareff|1877}}. A '''strongly perfect lattice''' is one whose minimal vectors form a spherical 4-design. This notion was introduced by {{harvtxt|Venkov|2001}}.\n\n{{harvtxt|Voronoi|1908}} proved that a lattice is extreme if and only if it is both perfect and [[eutactic lattice|eutactic]].\n\nThe number of perfect lattices in dimensions 1, 2, 3, 4, 5, 6, 7, 8 is given by \n1, 1, 1, 2, 3, 7, 33, 10916 {{OEIS|id=A004026}}. {{harvtxt|Conway|Sloane|1988}} summarize the properties of perfect lattices of dimension up to 7.\n{{harvtxt|Sikirić|Schürmann|Vallentin|2007}} verified that the list of 10916 perfect lattices in dimension 8 found by Martinet and others is complete. It was proven by {{harvtxt|Riener|2006}} that only 2408 of these 10916 perfect lattices in dimension 8 are actually extreme lattices.\n\n==References==\n\n*{{Citation | last1=Conway | first1=John Horton | author1-link=John Horton Conway | last2=Sloane | first2=N. J. A. | author2-link=Neil Sloane | title=Low-dimensional lattices. III. Perfect forms | jstor=2398316 | mr=953277 | year=1988 | journal=Proceedings of the Royal Society. London. Series A. Mathematical, Physical and Engineering Sciences | issn=0962-8444 | volume=418 | issue=1854 | pages=43–80 | doi=10.1098/rspa.1988.0073| bibcode=1988RSPSA.418...43C }}\n**{{cite journal |title = Errata: Low-Dimensional Lattices. III. Perfect Forms| jstor = 2398351 | volume=426}}\n*{{Citation | last1=Korkine | last2=Zolotareff | title=Sur les formes quadratique positives | doi=10.1007/BF01442667 | year=1877 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=11 | pages=242–292}}\n*{{Citation | last1=Martinet | first1=Jacques | title=Perfect lattices in Euclidean spaces | url=https://books.google.com/books?id=gd9CcFclBRIC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] | isbn=978-3-540-44236-3 | mr=1957723 | year=2003 | volume=327}}\n*{{Citation | last1=Riener | first1=Cordian | title=On extreme forms in dimension 8 | url=https://eudml.org/doc/249637 | journal= Journal de théorie des nombres de Bordeaux | volume=18 |pages=677–682 | year=2006 | doi=10.5802/jtnb.565}}\n*{{Citation | last1=Sikirić | first1=Mathieu Dutour | last2=Schürmann | first2=Achill | last3=Vallentin | first3=Frank | title=Classification of eight-dimensional perfect forms | arxiv=math/0609388 | doi=10.1090/S1079-6762-07-00171-0 | mr=2300003 | year=2007 | journal=Electronic Research Announcements of the American Mathematical Society | issn=1079-6762 | volume=13 | pages=21–32}}\n*{{Citation |last1=Venkov |first1=Boris|title=Réseaux et designs sphériques, Réseaux euclidiens, designs sphériques et formes modulaires| journal= Monographie de l’Enseignement mathématique |volume=37 |year=2001|pages= 10–86}}\n*{{Citation | last1=Voronoi | first1=G. | title=Nouvelles applications des paramètres continus à la théorie des formes quadratiques. Premier Mémoire: Sur quelques propriétés des formes quadratiques positives parfaites | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166534 | language=French | doi=10.1515/crll.1908.133.97 | year=1908 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=133 | pages=97–178}}\n\n==External links==\n*[http://www.math.uni-magdeburg.de/lattice_geometry/ List of perfect lattices in dimension 8]\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Pfister form",
      "url": "https://en.wikipedia.org/wiki/Pfister_form",
      "text": "In [[mathematics]], a '''Pfister form''' is a particular kind of [[quadratic form]], introduced by [[Albrecht Pfister (mathematician)|Albrecht Pfister]] in 1965. In what follows, quadratic forms are considered over a [[field (mathematics)|field]] ''F'' of [[characteristic (algebra)|characteristic]] not 2.  For a natural number ''n'', an '''n-fold Pfister form''' over ''F'' is a quadratic form of dimension 2<sup>''n''</sup> that can be written as a [[tensor product of quadratic forms]]\n\n:<math>\\langle\\!\\langle a_1, a_2, \\ldots , a_n \\rangle\\!\\rangle \\cong \\langle 1, -a_1 \\rangle \\otimes \\langle 1, -a_2 \\rangle \\otimes \\cdots \\otimes \\langle 1, -a_n \\rangle,</math>\n\nfor some nonzero elements ''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub> of ''F''.<ref>Elman, Karpenko, Merkurjev (2008), section 9.B.</ref> (Some authors omit the signs in this definition; the notation here simplifies the relation to [[Milnor K-theory]], discussed below.) An ''n''-fold Pfister form can also be constructed inductively from an (''n''–1)-fold Pfister form ''q'' and a nonzero element ''a'' of ''F'', as <math>q \\oplus (-a)q</math>.\n\nSo the 1-fold and 2-fold Pfister forms look like:\n\n:<math>\\langle\\!\\langle a\\rangle\\!\\rangle\\cong \\langle 1, -a \\rangle = x^2 - ay^2</math>.\n:<math>\\langle\\!\\langle a,b\\rangle\\!\\rangle\\cong \\langle 1, -a, -b, ab \\rangle = x^2 - ay^2 - bz^2 + abw^2.</math>\n\nFor ''n'' ≤ 3, the ''n''-fold Pfister forms are norm forms of [[composition algebra]]s.<ref name=Lam316>Lam (2005) p. 316</ref>  In that case, two ''n''-fold Pfister forms are [[isomorphic]] if and only if the corresponding composition algebras are isomorphic. In particular, this gives the classification of [[octonion algebra]]s.\n\nThe ''n''-fold Pfister forms additively generate the ''n''-th power ''I''<sup>''n''</sup> of the fundamental ideal of the [[Witt group|Witt ring]] of ''F''.<ref name=Lam316>Lam (2005) p. 316</ref>\n\n==Characterizations==\nA quadratic form ''q'' over a field ''F'' is '''multiplicative''' if, for vectors of indeterminates '''x''' and '''y''', we can write ''q''('''x''').''q''('''y''') = ''q''('''z''') for some vector '''z''' of [[rational function]]s in the '''x''' and '''y''' over ''F''.  [[Isotropic quadratic form]]s are multiplicative.<ref name=Lam324>Lam (2005) p. 324</ref>  For [[anisotropic quadratic form]]s, Pfister forms are multiplicative, and conversely.<ref name=Lam325>Lam (2005) p. 325</ref>\n\nFor ''n''-fold Pfister forms with ''n'' ≤ 3, this had been known since the 19th century; in that case ''z'' can be taken to be bilinear in ''x'' and ''y'', by the properties of composition algebras. It was a remarkable discovery by Pfister that ''n''-fold Pfister forms for all ''n'' are multiplicative in the more general sense here, involving rational functions. For example, he deduced that for any field ''F'' and any natural number ''n'', the set of sums of 2<sup>''n''</sup> squares in ''F'' is closed under multiplication, using that\nthe quadratic form \n<math>x_1^2 +\\cdots + x_{2^n}^2</math>\nis an ''n''-fold Pfister form (namely, <math>\\langle\\!\\langle -1, \\ldots , -1 \\rangle\\!\\rangle</math>).<ref name=Lam319>Lam (2005) p. 319</ref>\n\nAnother striking feature of Pfister forms is that every isotropic Pfister form is in fact hyperbolic, that is, isomorphic to a direct sum of copies of the hyperbolic plane <math>\\langle 1, -1 \\rangle</math>. This property also characterizes Pfister forms, as follows. If ''q'' is an anisotropic quadratic form over a field ''F'', and if ''q'' becomes hyperbolic over every extension field ''E'' such that ''q'' becomes isotropic over ''E'', then ''q'' is isomorphic to ''a''φ for some nonzero ''a'' in ''F'' and some Pfister form φ over ''F''.<ref>Elman, Karpenko, Merkurjev (2008), Corollary 23.4.</ref>\n\n==Connection with ''K''-theory==\nLet ''k''<sub>''n''</sub>(''F'') be the ''n''-th [[Milnor K-theory|Milnor ''K''-group]] modulo 2.  There is a homomorphism from ''k''<sub>''n''</sub>(''F'') to the quotient ''I''<sup>''n''</sup>/''I''<sup>''n''+1</sup> in the Witt ring of ''F'', given by\n\n:<math> \\{a_1,\\ldots,a_n\\} \\mapsto \\langle\\!\\langle a_1, a_2, \\ldots , a_n \\rangle\\!\\rangle ,</math>\n\nwhere the image is an ''n''-fold Pfister form.<ref>Elman, Karpenko, Merkurjev (2008), section 5.</ref>  The homomorphism is surjective, since the Pfister forms additively generate ''I''<sup>''n''</sup>.  One part of the [[Milnor conjecture]], proved by Orlov, Vishik and [[Vladimir Voevodsky|Voevodsky]], states that this homomorphism is in fact an isomorphism ''k''<sub>''n''</sub>(''F'') ≅ ''I''<sup>''n''</sup>/''I''<sup>''n''+1</sup>.<ref>Orlov, Vishik, Voevodsky (2007).</ref> That gives an explicit description of the abelian group ''I''<sup>''n''</sup>/''I''<sup>''n''+1</sup> by generators and relations. The other part of the Milnor conjecture, proved by Voevodsky, says that ''k''<sub>''n''</sub>(''F'') (and hence ''I''<sup>''n''</sup>/''I''<sup>''n''+1</sup>) maps isomorphically to the [[Galois cohomology]] group ''H''<sup>''n''</sup>(''F'', '''F'''<sub>2</sub>).\n\n==Pfister neighbors==\nA '''Pfister neighbor''' is an anisotropic form σ which is isomorphic to a subform of ''a''φ for some nonzero ''a'' in ''F'' and some Pfister form φ with  dim φ < 2 dim σ.<ref>Elman, Karpenko, Merkurjev (2008), Definition 23.10.</ref>  The associated Pfister form φ is determined up to isomorphism by σ.  Every anisotropic form of dimension 3 is a Pfister neighbor; an anisotropic form of dimension 4 is a Pfister neighbor if and only if its [[discriminant of a quadratic form|discriminant]] in ''F''<sup>*</sup>/(''F''<sup>*</sup>)<sup>2</sup> is trivial.<ref name=Lam341>Lam (2005) p. 341</ref> A field ''F'' has the property that every 5-dimensional anisotropic form over ''F'' is a Pfister neighbor if and only if it is a [[linked field]].<ref name=Lam342>Lam (2005) p. 342</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{Citation | title=Algebraic and geometric theory of quadratic forms | author1-first=Richard | author1-last=Elman | author1-link=Richard Elman (mathematician) | author2-first=Nikita | author2-last=Karpenko | author3-first=Alexander | author3-last=Merkurjev | author3-link=Alexander Merkurjev | publisher=American Mathematical Society | year=2008 | isbn=978-0-8218-4329-1 | mr=2427530}}\n* {{Citation | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | publisher=American Mathematical Society | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 | mr = 2104929 }}, Ch. 10\n* {{Citation | title=An exact sequence for ''K''<sub>*</sub><sup>''M''</sup>/2 with applications to quadratic forms | author1-first=Dmitri | author1-last=Orlov | author2-first=Alexander | author2-last=Vishik | author3-first=Vladimir | author3-last=Voevodsky | author3-link=Vladimir Voevodsky | journal=Annals of Mathematics | volume=165 | year=2007 | pages=1–13 | doi=10.4007/annals.2007.165.1 | mr=2276765| arxiv=math/0101023 }}\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Positive definiteness",
      "url": "https://en.wikipedia.org/wiki/Positive_definiteness",
      "text": "In [[mathematics]], '''positive definiteness''' is a property of any object to which a [[bilinear form]] or a [[sesquilinear form]] may be naturally associated, which is [[positive definite bilinear form|positive definite]]. See, in particular:\n\n* [[Positive-definite bilinear form]]\n* [[Positive-definite quadratic form]]\n* [[Positive-definite matrix]]\n* [[Positive-definite function]]\n* [[Positive-definite kernel]]\n* [[Positive-definite function on a group]]\n\n==References==\n*{{citation\n | last = Fasshauer | first = Gregory E.\n | journal = Dolomites Research Notes on Approximation\n | pages = 21–63\n | title = Positive definite kernels: Past, present and future\n | url = http://www.math.iit.edu/~fass/PDKernels.pdf\n | volume = 4\n | year = 2011}}. \n*{{citation\n | last = Stewart | first = James\n | doi = 10.1216/RMJ-1976-6-3-409\n | issue = 3\n | journal = The Rocky Mountain Journal of Mathematics\n | mr = 0430674\n | pages = 409–434\n | title = Positive definite functions and generalizations, an historical survey\n | volume = 6\n | year = 1976}}.\n\n{{Set index article|mathematics}}\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Quadratic form",
      "url": "https://en.wikipedia.org/wiki/Quadratic_form",
      "text": "{{short description|Polynomial with all terms of degree two}}\n{{for|the usage in statistics|Quadratic form (statistics)}}\nIn [[mathematics]], a '''quadratic form''' is a [[polynomial]] with terms all of [[Degree of a polynomial|degree]] two. For example, \n\n<math>4x^2 + 2xy - 3y^2</math>\n\nis a quadratic form in the variables ''x'' and ''y''. The coefficients usually belong to a fixed field ''K'',\nsuch as the real or complex numbers, and we speak of a quadratic form over ''K''.\nQuadratic forms occupy a central place in various branches of mathematics, including [[number theory]], [[linear algebra]], [[group theory]] ([[orthogonal group]]), [[differential geometry]] ([[Riemannian metric]], [[second fundamental form]]), [[differential topology]] ([[intersection form (4-manifold)|intersection forms]] of [[four-manifold]]s), and [[Lie theory]] (the [[Killing form]]).\n\nQuadratic forms are not to be confused with a [[quadratic equation]] which has only one variable and includes terms of degree two or less. A quadratic form is one case of the more general concept of [[Homogeneous polynomial|homogeneous polynomials]].\n\n== Introduction ==\nQuadratic forms are homogeneous quadratic polynomials in ''n'' variables. In the cases of one, two, and three variables they are called '''unary''', '''[[binary quadratic form|binary]]''', and '''ternary''' and have the following explicit form:\n\n:<math>q(x) = ax^2\\quad \\textrm{(unary)} </math>\n:<math>q(x,y) = ax^2 + bxy + cy^2\\quad \\textrm{(binary)} </math>\n:<math>q(x,y,z) = ax^2 + by^2 + cz^2 + dxy + exz + fyz \\quad\\textrm{(ternary)} </math>\n\nwhere ''a'', ..., ''f'' are the '''coefficients'''.<ref>A tradition going back to [[Gauss]] dictates the use of manifestly even coefficients for the products of distinct variables, i.e. 2''b'' in place of ''b'' in binary forms and 2''d'', 2''e'', 2''f'' in place of ''d'', ''e'', ''f'' in ternary forms. Both conventions occur in the literature.</ref>\nThe notation <math>\\langle a_1,\\dots,a_n\\rangle</math> is often used for the quadratic form\n: <math>q(x)=a_1 x_1^2 + a_2 x_2^2+ \\ldots +a_n x_n^2.</math>\n\n\nThe theory of quadratic forms and methods used in their study depend in a large measure on the nature of the coefficients, which may be [[real number|real]] or [[complex number|complex]] numbers, [[rational number]]s, or [[integer]]s. In [[linear algebra]], [[analytic geometry]], and in the majority of applications of quadratic forms, the coefficients are real or complex numbers. In the algebraic theory of quadratic forms, the coefficients are elements of a certain [[field (algebra)|field]]. In the arithmetic theory of quadratic forms, the coefficients belong to a fixed [[commutative ring]], frequently the integers '''Z''' or the [[p-adic integer|''p''-adic integers]] '''Z'''<sub>''p''</sub>.<ref>[[Localization of a ring#Terminology|away from 2]], i. e. if 2 is invertible in the ring, quadratic forms are equivalent to [[symmetric bilinear form]]s (by the [[polarization identities]]), but at 2 they are different concepts; this distinction is particularly important for quadratic forms over the integers.</ref> Binary quadratic forms have been extensively studied in [[number theory]], in particular, in the theory of [[quadratic field]]s, [[continued fraction]]s, and [[modular forms]]. The theory of integral quadratic forms in ''n'' variables has important applications to [[algebraic topology]].\n\nUsing [[homogeneous coordinates]], a non-zero quadratic form in ''n'' variables defines an (''n''−2)-dimensional [[Quadric (projective geometry)|quadric]] in the (''n''−1)-dimensional [[projective space]]. This is a basic construction in [[projective geometry]]. In this way one may visualize 3-dimensional real quadratic forms as [[conic sections]].\nAn example is given by the three-dimensional [[Euclidean space]] and the square of the [[Euclidean norm]] expressing the [[distance]] between a point with coordinates {{nowrap|(''x'', ''y'', ''z'')}} and the origin:\n: <math> q(x,y,z)=d((x,y,z),(0,0,0))^2=\\|(x,y,z)\\|^2=x^2+y^2+z^2. </math>\n\nA closely related notion with geometric overtones is a '''quadratic space''', which is a pair {{nowrap|(''V'', ''q'')}}, with ''V'' a [[vector space]] over a field ''K'', and {{nowrap|''q'' : ''V'' → ''K''}} a quadratic form on ''V''. \n\n== History ==\nThe study of particular quadratic forms, in particular the question of whether a given integer can be the value of a quadratic form over the integers, dates back many centuries. One such case is [[Fermat's theorem on sums of two squares]], which determines when an integer may be expressed in the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup>}}, where ''x'', ''y'' are integers. This problem is related to the problem of finding [[Pythagorean triple]]s, which appeared in the second millennium B.C.<ref>[http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/Babylonian_Pythagoras.html Babylonian Pythagoras]</ref>\n\nIn 628, the Indian mathematician [[Brahmagupta]] wrote ''[[Brāhmasphuṭasiddhānta]]'' which includes, among many other things, a study of equations of the form {{nowrap|1=''x''<sup>2</sup> − ''ny''<sup>2</sup> = ''c''}}. In particular he considered what is now called [[Pell's equation]], {{nowrap|1=''x''<sup>2</sup> − ''ny''<sup>2</sup> = 1}}, and found a method for its solution.<ref>[http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Brahmagupta.html Brahmagupta biography]</ref> In Europe this problem was studied by [[William Brouncker, 2nd Viscount Brouncker|Brouncker]], [[Leonhard Euler|Euler]] and [[Joseph Louis Lagrange|Lagrange]].\n\nIn 1801 [[Carl Friedrich Gauss|Gauss]] published ''[[Disquisitiones Arithmeticae]],'' a major portion of which was devoted to a complete theory of [[binary quadratic form]]s over the [[integer]]s. Since then, the concept has been generalized, and the connections with [[quadratic number field]]s, the [[modular group]], and other areas of mathematics have been further elucidated.\n\n== Real quadratic forms ==\n{{see also|Sylvester's law of inertia|Definite quadratic form}}\n\nAny ''n''×''n'' real [[symmetric matrix]] ''A'' determines a quadratic form ''q''<sub>''A''</sub> in ''n'' variables by the formula\n\n: <math>q_A(x_1,\\ldots,x_n) = \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}{x_i}{x_j} = \\mathbf x^\\mathrm{T} A \\mathbf x. </math>\n\nConversely, given a quadratic form in ''n'' variables, its coefficients can be arranged into an {{nowrap|''n'' × ''n''}} symmetric matrix. \n\nAn important question in the theory of quadratic forms is how to simplify a quadratic form ''q'' by a homogeneous linear change of variables.  A fundamental theorem due to [[Carl Gustav Jacobi|Jacobi]] asserts that a real quadratic form ''q'' has an [[orthogonal diagonalization]].<ref>[[Maxime Bôcher]] (with E.P.R. DuVal)(1907) ''Introduction to Higher Algebra'', [https://babel.hathitrust.org/cgi/pt?id=uc1.b4248862;view=1up;seq=147 § 45 Reduction of a quadratic form to a sum of squares] via [[HathiTrust]]</ref>\n: <math> \\lambda_1 \\tilde x_1^2 + \\lambda_2 \\tilde x_2^2 + \\cdots + \\lambda_n \\tilde x_n^2, </math>\nso that the corresponding symmetric matrix is [[diagonal matrix|diagonal]], and this is accomplished with a change of variables given by an [[orthogonal matrix]] – in this case the coefficients ''λ''<sub>1</sub>, ''λ''<sub>2</sub>, ..., ''λ''<sub>''n''</sub> are determined uniquely up to a permutation.\n\nThere always exists a change of variables given by an invertible matrix, not necessarily orthogonal, such that the coefficients ''λ''<sub>''i''</sub> are 0, 1, and −1. [[Sylvester's law of inertia]] states that the numbers of each 1 and −1 are [[invariant (mathematics)|invariants]] of the quadratic form, in the sense that any other diagonalization will contain the same number of each. The '''signature''' of the quadratic form is the triple {{nowrap|(''n''<sub>0</sub>, ''n''<sub>+</sub>, ''n''<sub>−</sub>)}}, where ''n''<sub>0</sub> is the number of 0s and ''n''<sub>±</sub> is the number of ±1s. Sylvester's law of inertia shows that this is a well-defined quantity attached to the quadratic form. The case when all ''λ''<sub>''i''</sub> have the same sign is especially important: in this case the quadratic form is called '''[[positive definite form|positive definite]]''' (all 1) or '''negative definite''' (all −1).  If none of the terms are 0, then the form is called '''{{visible anchor|nondegenerate}}'''; this includes positive definite, negative definite, and indefinite (a mix of 1 and −1); equivalently, a nondegenerate quadratic form is one whose associated symmetric form is a [[nondegenerate form|nondegenerate ''bilinear'' form]]. A real vector space with an indefinite nondegenerate quadratic form of index {{nowrap|(''p'', ''q'')}} (denoting ''p'' 1s and ''q'' −1s) is often denoted as '''R'''<sup>''p'',''q''</sup> particularly in the physical theory of [[spacetime]].\n\nThe [[Discriminant#Discriminant of a quadratic form|discriminant of a quadratic form]], concretely the class of the determinant of a representing matrix in ''K''/(''K''<sup>×</sup>)<sup>2</sup> (up to non-zero squares) can also be defined, and for a real quadratic form is a cruder invariant than signature, taking values of only “positive, zero, or negative”. Zero corresponds to degenerate, while for a non-degenerate form it is the parity of the number of negative coefficients, <math>(-1)^{n_{-}}.</math>\n\nThese results are reformulated in a different way below.\n\nLet ''q'' be a quadratic form defined on an ''n''-dimensional [[real number|real]] vector space. Let ''A'' be the matrix of the quadratic form ''q'' in a given basis. This means that ''A'' is a symmetric {{nowrap|''n'' × ''n''}} matrix such that\n\n: <math>q(v)=x^\\mathrm{T} Ax,</math>\n\nwhere ''x'' is the column vector of coordinates of ''v'' in the chosen basis. Under a change of basis, the column ''x'' is multiplied on the left by an {{nowrap|''n'' × ''n''}} [[invertible matrix]] ''S'', and the symmetric square matrix ''A'' is transformed into another symmetric square matrix ''B'' of the same size according to the formula\n\n: <math> A\\to B=S^\\mathrm{T}AS.</math>\n\nAny symmetric matrix ''A'' can be transformed into a diagonal matrix\n\n: <math> B=\\begin{pmatrix}\n\\lambda_1 & 0 & \\cdots & 0\\\\\n0 & \\lambda_2 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & 0\\\\\n0 & 0 & \\cdots & \\lambda_n\n\\end{pmatrix}</math>\n\nby a suitable choice of an orthogonal matrix ''S'', and the diagonal entries of ''B'' are uniquely determined – this is Jacobi's theorem. If ''S'' is allowed to be any invertible matrix then ''B'' can be made to have only 0,1, and −1 on the diagonal, and the number of the entries of each type (''n''<sub>0</sub> for 0, ''n''<sub>+</sub> for 1, and ''n''<sub>−</sub> for −1) depends only on ''A''. This is one of the formulations of Sylvester's law of inertia and the numbers ''n''<sub>+</sub> and ''n''<sub>−</sub> are called the '''positive''' and '''negative''' '''indices of inertia'''. Although their definition involved a choice of basis and consideration of the corresponding real symmetric matrix ''A'', Sylvester's law of inertia means that they are invariants of the quadratic form ''q''.\n\nThe quadratic form ''q'' is positive definite (resp., negative definite) if {{nowrap|''q''(''v'') > 0}} (resp., {{nowrap|''q''(''v'') < 0}}) for every nonzero vector ''v''.<ref>If a non-strict inequality (with ≥ or ≤) holds then the quadratic form ''q'' is called semidefinite.</ref> When ''q''(''v'') assumes both positive and negative values, ''q'' is an '''indefinite''' quadratic form. The theorems of Jacobi and Sylvester show that any positive definite quadratic form in ''n'' variables can be brought to the sum of ''n'' squares by a suitable invertible linear transformation: geometrically, there is only ''one'' positive definite real quadratic form of every dimension. Its [[isometry group]] is a ''[[compact space|compact]]'' [[orthogonal group]] O(''n''). This stands in contrast with the case of indefinite forms, when the corresponding group, the [[indefinite orthogonal group]] O(''p'', ''q''), is non-compact. Further, the isometry groups of ''Q'' and −''Q'' are the same ({{nowrap|1=O(''p'', ''q'') ≈ O(''q'', ''p''))}}, but the associated [[Clifford algebra]]s (and hence [[pin group]]s) are different.\n\n== Definitions ==\nA '''quadratic form''' over a field ''K'' is a map <math>q: V \\to K</math> from a finite dimensional ''K'' vector space to ''K'' such that <math>q(av) = a^2q(v)</math> for all <math> a \\in K, v \\in V</math> and the function <math>q(u+v) - q(u) - q(v)</math> is bilinear.\n\nMore concretely, an ''n''-ary '''quadratic form''' over a field ''K'' is a [[homogeneous polynomial]] of degree 2 in ''n'' variables with coefficients in ''K'':\n\n: <math>q(x_1,\\ldots,x_n) = \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}{x_i}{x_j}, \\quad a_{ij}\\in K. </math>\n\nThis formula may be rewritten using matrices: let ''x'' be the [[column vector]] with components ''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub> and {{nowrap|1=''A'' = (''a''<sub>''ij''</sub>)}} be the ''n''×''n'' matrix over ''K'' whose entries are the coefficients of ''q''. Then\n\n: <math> q(x)=x^\\mathrm{T}Ax. </math>\n\nA vector <math>v = (x_1,\\ldots,x_n)</math> is a [[null vector]] if ''q''(''v'') = 0. \n\nTwo ''n''-ary quadratic forms ''φ'' and ''ψ'' over ''K'' are '''equivalent''' if there exists a nonsingular linear transformation {{nowrap|''C'' ∈ [[General linear group|GL]](''n'', ''K'')}} such that\n\n: <math> \\psi(x)=\\varphi(Cx). </math>\n\nLet the characteristic of ''K'' be different from 2.{{refn|The theory of quadratic forms over a field of characteristic 2 has important differences and many definitions and theorems must be modified.}} The coefficient matrix ''A'' of ''q'' may be replaced by the [[symmetric matrix]] {{nowrap|(''A'' + ''A''<sup>T</sup>)/2}} with the same quadratic form, so it may be assumed from the outset that ''A'' is symmetric. Moreover, a symmetric matrix ''A'' is uniquely determined by the corresponding quadratic form. Under an equivalence ''C'', the symmetric matrix ''A'' of ''φ'' and the symmetric matrix ''B'' of ''ψ'' are related as follows:\n\n: <math> B=C^\\mathrm{T}AC. </math>\n\nThe '''associated bilinear form''' of a quadratic form ''q'' is defined by\n\n: <math> b_q(x,y)=\\tfrac{1}{2}(q(x+y)-q(x)-q(y)) = x^\\mathrm{T}Ay = y^\\mathrm{T}Ax. </math>\n\nThus, ''b''<sub>''q''</sub> is a [[symmetric bilinear form]] over ''K'' with matrix ''A''. Conversely, any symmetric bilinear form ''b'' defines a quadratic form\n\n: <math> q(x)=b(x,x) </math>\n\nand these two processes are the inverses of one another.  As a consequence, over a field of characteristic not equal to 2, the theories of symmetric bilinear forms and of quadratic forms in ''n'' variables are essentially the same.\n\n=== Quadratic spaces ===\nA quadratic form ''q'' in ''n'' variables over ''K'' induces a map from the ''n''-dimensional coordinate space ''K''<sup>''n''</sup> into ''K'':\n\n: <math> Q(v)=q(v), \\quad v=[v_1,\\ldots,v_n]^\\mathrm{T}\\in K^n. </math>\n\nThe map ''Q'' is a [[homogeneous function]] of degree 2, which means that it has the property that, for all ''a'' in ''K'' and ''v'' in ''V'':\n:<math> Q(av) = a^2 Q(v). </math>\n\nWhen the characteristic of ''K'' is not 2, the bilinear map {{nowrap|''B'' : ''V'' × ''V'' → ''K''}} over ''K'' below is defined:\n:<math> B(v,w)= \\tfrac{1}{2}(Q(v+w)-Q(v)-Q(w)).</math>\nThis bilinear form ''B'' is symmetric, i.e. {{nowrap|1=''B''(''x'', ''y'') = ''B''(''y'', ''x'')}} for all ''x'', ''y'' in ''V'', and it determines ''Q'': {{nowrap|1=''Q''(''x'') = ''B''(''x'', ''x'')}}  for all ''x'' in ''V''.\n\nWhen the characteristic of ''K'' is 2, so that 2 is not a [[Unit (ring theory)|unit]], it is still possible to use a quadratic form to define a symmetric bilinear form {{nowrap|1=''B''′(''x'', ''y'') = ''Q''(''x'' + ''y'') − ''Q''(''x'') − ''Q''(''y'')}}. However, ''Q''(''x'') can no longer be recovered from this ''B''′ in the same way, since {{nowrap|1=''B''′(''x'', ''x'') = 0}} for all ''x'' (and is thus alternating<ref>This alternating form associated with a quadratic form in characteristic 2 is of interest related to the [[Arf invariant]] – {{cite|author=Irving Kaplansky|year=1974|title=Linear Algebra and Geometry|page=27}}.</ref>). Alternately, there always exists a bilinear form ''B''″ (not in general either unique or symmetric) such that {{nowrap|1=''B''″(''x'', ''x'') = ''Q''(''x'')}}.\n\nThe pair {{nowrap|(''V'', ''Q'')}} consisting of a finite-dimensional vector space ''V'' over ''K'' and a quadratic map ''Q'' from ''V'' to ''K'' is called a '''quadratic space''', and ''B'' as defined here is the associated symmetric bilinear form of ''Q''. The notion of a quadratic space is a coordinate-free version of the notion of quadratic form. Sometimes, ''Q'' is also called a quadratic form.\n\nTwo ''n''-dimensional quadratic spaces {{nowrap|(''V'', ''Q'')}} and {{nowrap|(''V''′, ''Q''′)}} are '''isometric''' if there exists an invertible linear transformation {{nowrap|''T'' : ''V'' → ''V''′}} ('''isometry''') such that\n\n: <math> Q(v) = Q'(Tv) \\text{ for all } v\\in V.</math>\n\nThe isometry classes of ''n''-dimensional quadratic spaces over ''K'' correspond to the equivalence classes of ''n''-ary quadratic forms over ''K''.\n\n===Generalization===\nLet ''R'' be a [[commutative ring]], ''M'' be an ''R''-[[Module (mathematics)|module]] and {{nowrap|''b'' : ''M'' × ''M'' → ''R''}} be an ''R''-bilinear form.{{refn|The bilinear form to which a quadratic form is associated is not restricted to being symmetric, which is of significance when 2 is not a unit in ''R''.}}  A mapping {{nowrap|''Q'' : ''M'' → ''R'' : ''v'' ↦ ''b''(''v'', ''v'')}} is the ''associated quadratic form'' of ''b'', and {{nowrap|''B'' : ''M'' × ''M'' → ''R'' : (''u'', ''v'') ↦ ''Q''(''u'' + ''v'') − ''Q''(''u'') − ''Q''(''v'')}} is the ''polar form'' of ''Q''.\n\nAlternatively, a quadratic form {{nowrap|''Q'' : ''M'' → ''R''}} may be characterized as follows:\n*There exists an ''R''-bilinear form {{nowrap|''b'' : ''M'' × ''M'' → ''R''}} such that {{nowrap|1=''Q''(''v'') = ''b''(''v'', ''v'')}} for all {{nowrap|''v'' ∈ ''M''}}.\n*{{nowrap|1=''Q''(''av'') = ''a''<sup>2</sup>''Q''(''v'')}} for all {{nowrap|''a'' ∈ ''R''}} and {{nowrap|''v'' ∈ ''M''}}, and the polar form of ''Q'' is ''R''-bilinear.\n\n=== Related concepts ===\n{{see also|Isotropic quadratic form}}\nTwo elements ''v'' and ''w'' of ''V'' are called '''[[orthogonal]]''' if {{nowrap|1=''B''(''v'', ''w'') = 0}}. The '''kernel''' of a bilinear form ''B'' consists of the elements that are orthogonal to every element of ''V''. ''Q'' is '''non-singular''' if the kernel of its associated bilinear form is {0}. If there exists a non-zero ''v'' in ''V'' such that {{nowrap|1=''Q''(''v'') = 0}}, the quadratic form ''Q'' is '''[[Isotropic quadratic form|isotropic]]''', otherwise it is '''anisotropic'''. This terminology also applies to vectors and subspaces of a quadratic space. If the restriction of ''Q'' to a subspace ''U'' of ''V'' is identically zero, ''U'' is '''totally singular'''.\n\nThe [[orthogonal group]] of a non-singular quadratic form ''Q'' is the group of the linear automorphisms of ''V'' that preserve ''Q'', i.e. the group of isometries of {{nowrap|(''V'', ''Q'')}} into itself.\n\nIf a quadratic space {{nowrap|(''A'', ''Q'')}} has a product so that ''A'' is an [[algebra over a field]], and satisfies\n:<math>\\forall x, y \\isin A \\quad Q(x y) = Q(x) Q(y) ,</math> then it is a [[composition algebra]].\n\n== Equivalence of forms ==\nEvery quadratic form ''q'' in ''n'' variables over a field of characteristic not equal to 2 is [[Matrix congruence|equivalent]] to a '''diagonal form'''\n\n: <math>q(x)=a_1 x_1^2 + a_2 x_2^2+ \\ldots +a_n x_n^2.</math>\n\nSuch a diagonal form is often denoted by <math>\\langle a_1,\\dots,a_n\\rangle.</math>\nClassification of all quadratic forms up to equivalence can thus be reduced to the case of diagonal forms.\n\n== Geometric meaning ==\nUsing [[Cartesian coordinates]] in three dimensions, let <math>\\mathbf{x} = (x,y,z)^\\text{T}</math>, and let <math>A</math> be a [[symmetric matrix|symmetric]] 3-by-3 matrix. Then the geometric nature of the [[solution set]] of the equation <math>\\mathbf{x}^\\text{T}A\\mathbf{x}+\\mathbf{b}^\\text{T}\\mathbf{x}=1</math> depends on the eigenvalues of the matrix <math>A</math>.\n\nIf all [[eigenvalue]]s of <math>A</math> are non-zero, then the solution set is an [[ellipsoid]] or a [[hyperboloid]]{{Citation needed|date=February 2017}}. If all the eigenvalues are positive, then it is an ellipsoid; if all the eigenvalues are negative, then it is an ''imaginary ellipsoid'' (we get the equation of an ellipsoid but with imaginary radii); if some eigenvalues are positive and some are negative, then it is a hyperboloid.\n\nIf there exist one or more eigenvalues <math>\\lambda_i = 0</math>, then the shape depends on the corresponding <math>b_i</math>. If the corresponding <math>b_i \\neq 0</math>, then the solution set is a [[paraboloid]] (either elliptic or hyperbolic); if the corresponding <math>b_i = 0</math>, then the dimension <math>i</math> degenerates and does not come into play, and the geometric meaning will be determined by other eigenvalues and other components of <math>\\mathbf{b}</math>. When the solution set is a paraboloid, whether it is elliptic or hyperbolic is determined by whether all other non-zero eigenvalues are of the same sign: if they are, then it is elliptic; otherwise, it is hyperbolic.\n\n== Integral quadratic forms ==\nQuadratic forms over the ring of integers are called '''integral quadratic forms''', whereas the corresponding modules are '''quadratic lattices''' (sometimes, simply [[lattice (group)|lattice]]s). They play an important role in [[number theory]] and [[topology]].\n\nAn integral quadratic form has integer coefficients, such as {{nowrap|''x''<sup>2</sup> + ''xy'' + ''y''<sup>2</sup>}}; equivalently, given a lattice Λ in a vector space ''V'' (over a field with characteristic 0, such as '''Q''' or '''R'''), a quadratic form ''Q'' is integral ''with respect to'' Λ if and only if it is integer-valued on Λ, meaning {{nowrap|''Q''(''x'', ''y'') ∈ '''Z'''}} if {{nowrap|''x'', ''y'' ∈ Λ}}.\n\nThis is the current use of the term; in the past it was sometimes used differently, as detailed below.\n\n=== Historical use ===\nHistorically there was some confusion and controversy over whether the notion of '''integral quadratic form''' should mean:\n;''twos in'': the quadratic form associated to a symmetric matrix with integer coefficients\n;''twos out'': a polynomial with integer coefficients (so the associated symmetric matrix may have half-integer coefficients off the diagonal)\nThis debate was due to the confusion of quadratic forms (represented by polynomials) and symmetric bilinear forms (represented by matrices), and \"twos out\" is now the accepted convention; \"twos in\" is instead the theory of integral symmetric bilinear forms (integral symmetric matrices).\n\nIn \"twos in\", binary quadratic forms are of the form <math>ax^2+2bxy+cy^2</math>, represented by the symmetric matrix\n:<math>\\begin{pmatrix}a & b\\\\ b&c\\end{pmatrix}</math>\nthis is the convention [[Gauss]] uses in [[Disquisitiones Arithmeticae]].\n\nIn \"twos out\", binary quadratic forms are of the form <math>ax^2+bxy+cy^2</math>, represented by the symmetric matrix\n:<math>\\begin{pmatrix}a & b/2\\\\ b/2&c\\end{pmatrix}.</math>\n\nSeveral points of view mean that ''twos out'' has been adopted as the standard convention. Those include:\n* better understanding of the 2-adic theory of quadratic forms, the 'local' source of the difficulty;\n* the [[lattice (group)|lattice]] point of view, which was generally adopted by the experts in the arithmetic of quadratic forms during the 1950s;\n* the actual needs for integral quadratic form theory in [[topology]] for [[intersection theory]];\n* the [[Lie group]] and [[algebraic group]] aspects.\n\n=== Universal quadratic forms ===\nAn integral quadratic form whose image consists of all the positive integers is sometimes called ''universal''.  [[Lagrange's four-square theorem]] shows that <math>w^2+x^2+y^2+z^2</math> is universal. [[Ramanujan]] generalized this to <math>aw^2+bx^2+cy^2+dz^2</math> and found 54 multisets {''a'',''b'',''c'',''d''} that can each generate all positive integers, namely,\n\n:{1,1,1,''d''}, 1 ≤ ''d'' ≤ 7\n:{1,1,2,''d''}, 2 ≤ ''d'' ≤ 14\n:{1,1,3,''d''}, 3 ≤ ''d'' ≤ 6\n:{1,2,2,''d''}, 2 ≤ ''d'' ≤ 7\n:{1,2,3,''d''}, 3 ≤ ''d'' ≤ 10\n:{1,2,4,''d''}, 4 ≤ ''d'' ≤ 14\n:{1,2,5,''d''}, 6 ≤ ''d'' ≤ 10\n\nThere are also forms whose image consists of all but one of the positive integers.  For example, {1,2,5,5} has 15 as the exception.  Recently, the [[15 and 290 theorems]] have completely characterized universal integral quadratic forms: if all coefficients are integers, then it represents all positive integers if and only if it represents all integers up through 290; if it has an integral matrix, it represents all positive integers if and only if it represents all integers up through 15.\n\n== See also ==\n*[[ε-quadratic form]]\n*[[Quadric]]\n*[[Discriminant#Discriminant of a quadratic form|Discriminant of a quadratic form]]\n*[[Cubic form]]\n*[[Witt group]]\n*[[Witt's theorem]]\n*[[Hasse–Minkowski theorem]]\n*[[Orthogonal group]]\n*[[Square class]]\n*[[Ramanujan's ternary quadratic form]]\n\n== Notes ==\n<references/>\n\n== References ==\n* {{Citation | last=O'Meara | first=O.T. | authorlink=O. Timothy O'Meara | title=Introduction to Quadratic Forms | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-66564-9 | year=2000}}\n* {{Citation | last1=Conway | first1=John Horton | author1-link= John Horton Conway | last2=Fung | first2=Francis Y. C. | title=The Sensual (Quadratic) Form | publisher=The Mathematical Association of America | series=Carus Mathematical Monographs | isbn=978-0-88385-030-5 | year=1997}}\n* {{cite book\n  | last = Shafarevich\n  | first = I. R.\n  | authorlink = Igor Shafarevich\n  |author2=A. O. Remizov\n   | title = Linear Algebra and Geometry\n  | publisher = [[Springer Science+Business Media|Springer]]\n  | year = 2012\n  | url = https://www.springer.com/mathematics/algebra/book/978-3-642-30993-9\n  | isbn = 978-3-642-30993-9}}\n\n== Further reading ==\n* {{cite book | first=J.W.S. | last=Cassels | authorlink=J. W. S. Cassels | title=Rational Quadratic Forms | series=London Mathematical Society Monographs | volume=13 | publisher=[[Academic Press]] | year=1978 | isbn=0-12-163260-1 | zbl=0395.10029 }}\n* {{cite book | last=Kitaoka | first=Yoshiyuki | title=Arithmetic of quadratic forms | series=Cambridge Tracts in Mathematics | volume=106 | publisher=Cambridge University Press | year=1993 | isbn=0-521-40475-4 | zbl=0785.11021 }}\n* {{cite book | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | publisher=[[American Mathematical Society]] | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 | mr = 2104929 }}\n* {{cite book | first1=J. | last1=Milnor | author1-link=John Milnor| first2=D. | last2=Husemoller | title=Symmetric Bilinear Forms | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 }}\n* {{cite book | last=O'Meara | first=O.T. | authorlink=O. Timothy O'Meara | title=Introduction to quadratic forms | series=Die Grundlehren der mathematischen Wissenschaften | volume=117 | publisher=[[Springer-Verlag]] | isbn=3-540-66564-1 | year=1973 | zbl=0259.10018 }}\n* {{cite book | last=Pfister | first=Albrecht | authorlink=Albrecht_Pfister_(mathematician) | title=Quadratic Forms with Applications to Algebraic Geometry and Topology | series=London Mathematical Society lecture note series | volume=217 | publisher=[[Cambridge University Press]] | year=1995 | isbn=0-521-46755-1 | zbl=0847.11014 }}\n\n== External links ==\n* {{eom|id=q/q076080|author=A.V.Malyshev|title=Quadratic form}}\n* {{eom|id=b/b016370|author=A.V.Malyshev|title=Binary quadratic form}}\n\n{{Authority control}}\n{{DEFAULTSORT:Quadratic Form}}\n[[Category:Linear algebra]]\n[[Category:Quadratic forms]]\n[[Category:Real algebraic geometry]]\n[[Category:Squares in number theory]]"
    },
    {
      "title": "Quadratic form (statistics)",
      "url": "https://en.wikipedia.org/wiki/Quadratic_form_%28statistics%29",
      "text": "{{Refimprove|date=December 2009}}\nIn [[multivariate statistics]], if <math>\\varepsilon</math> is a [[vector space|vector]] of <math>n</math> [[random variable]]s, and <math>\\Lambda</math> is an <math>n</math>-dimensional [[symmetric matrix]], then the [[scalar (mathematics)|scalar]] quantity <math>\\varepsilon^T\\Lambda\\varepsilon</math> is known as a '''quadratic form''' in <math>\\varepsilon</math>.\n\n==Expectation==\nIt can be shown that<ref>{{cite web|last=Douglas|first=Bates|title=Quadratic Forms of Random Variables|url=http://www.stat.wisc.edu/~st849-1/lectures/Ch02.pdf|work=STAT 849 lectures|accessdate=August 21, 2011}}</ref>\n\n:<math>\\operatorname{E}\\left[\\varepsilon^T\\Lambda\\varepsilon\\right]=\\operatorname{tr}\\left[\\Lambda \\Sigma\\right] + \\mu^T\\Lambda\\mu</math>\n\nwhere <math>\\mu</math> and <math>\\Sigma</math> are the [[expected value]] and [[covariance matrix|variance-covariance matrix]] of <math>\\varepsilon</math>, respectively, and tr denotes the [[Trace (linear algebra)|trace]] of a matrix. This result only depends on the existence of <math>\\mu</math> and <math>\\Sigma</math>; in particular, [[multivariate normal distribution|normality]] of <math>\\varepsilon</math> is ''not'' required.\n\nA book treatment of the topic of quadratic forms in random variables is that of Mathai and Provost.<ref name=\"Mathai\">{{cite book | title=Quadratic Forms in Random Variables | publisher=CRC Press |author1=Mathai, A. M. |author2=Provost, Serge B.  |lastauthoramp=yes | year=1992 | page=424 | isbn=978-0824786915}}</ref>\n\n=== Proof ===\nSince the quadratic form is a scalar quantity, <math> \\varepsilon^T\\Lambda\\varepsilon = \\operatorname{tr}(\\varepsilon^T\\Lambda\\varepsilon)</math>.\n\nNext, by the cyclic property of the [[Trace (linear algebra)#Properties|trace]] operator,\n\n:<math> \\operatorname{E}[\\operatorname{tr}(\\varepsilon^T\\Lambda\\varepsilon)] = \\operatorname{E}[\\operatorname{tr}(\\Lambda\\varepsilon\\varepsilon^T)]. </math>\n\nSince the trace operator is a linear combination of the components of the matrix, it therefore follows from the linearity of the expectation operator that\n\n: <math> \\operatorname{E}[\\operatorname{tr}(\\Lambda\\varepsilon\\varepsilon^T)] = \\operatorname{tr}(\\Lambda \\operatorname{E}(\\varepsilon\\varepsilon^T)). </math>\n\nA standard property of variances then tells us that this is\n\n: <math> \\operatorname{tr}(\\Lambda (\\Sigma + \\mu \\mu^T)). </math>\n\nApplying the cyclic property of the trace operator again, we get\n\n: <math> \\operatorname{tr}(\\Lambda\\Sigma) + \\operatorname{tr}(\\Lambda \\mu \\mu^T) = \\operatorname{tr}(\\Lambda\\Sigma) + \\operatorname{tr}(\\mu^T\\Lambda\\mu) = \\operatorname{tr}(\\Lambda\\Sigma) + \\mu^T\\Lambda\\mu.</math>\n\n==Variance==\nIn general, the variance of a quadratic form depends greatly on the distribution of <math>\\varepsilon</math>. However, if <math>\\varepsilon</math> ''does'' follow a multivariate normal distribution, the variance of the quadratic form becomes particularly tractable. Assume for the moment that <math>\\Lambda</math> is a symmetric matrix. Then,\n\n:<math>\\operatorname{var} \\left[\\varepsilon^T\\Lambda\\varepsilon\\right] = 2\\operatorname{tr}\\left[\\Lambda \\Sigma\\Lambda \\Sigma\\right] + 4\\mu^T\\Lambda\\Sigma\\Lambda\\mu</math> <ref>{{Cite book |title=Linear models in statistics |last=Rencher |first=Alvin C. |date=2008 |publisher=Wiley-Interscience |last2=Schaalje |first2=G. Bruce. |isbn=9780471754985 |edition=2nd |location=Hoboken, N.J. |oclc=212120778}}</ref>.  \n\nIn fact, this can be generalized to find the [[covariance]] between two quadratic forms on the same <math>\\varepsilon</math> (once again, <math>\\Lambda_1</math> and <math>\\Lambda_2</math> must both be symmetric):\n\n:<math>\\operatorname{cov}\\left[\\varepsilon^T\\Lambda_1\\varepsilon,\\varepsilon^T\\Lambda_2\\varepsilon\\right]=2\\operatorname{tr}\\left[\\Lambda _1\\Sigma\\Lambda_2 \\Sigma\\right] + 4\\mu^T\\Lambda_1\\Sigma\\Lambda_2\\mu</math>. \n\n===Computing the variance in the non-symmetric case===\nSome texts incorrectly{{citation needed|date=September 2018}} state that the above variance or covariance results hold without requiring <math>\\Lambda</math> to be symmetric. The case for general <math>\\Lambda</math> can be derived by noting that\n\n:<math>\\varepsilon^T\\Lambda^T\\varepsilon=\\varepsilon^T\\Lambda\\varepsilon</math>\n\nso\n\n:<math>\\varepsilon^T\\tilde{\\Lambda}\\varepsilon=\\varepsilon^T\\left(\\Lambda+\\Lambda^T\\right)\\varepsilon/2</math>\n\n'''is''' a quadratic form in the symmetric matrix <math>\\tilde{\\Lambda}=\\left(\\Lambda+\\Lambda^T\\right)/2</math>, so the mean and variance expressions are the same, provided <math>\\Lambda</math> is replaced by <math>\\tilde{\\Lambda}</math> therein.\n\n==Examples of quadratic forms==\nIn the setting where one has a set of observations <math>y</math> and an [[operator matrix]] <math>H</math>, then the  [[residual sum of squares]] can be written as a quadratic form in <math>y</math>:\n\n:<math>\\textrm{RSS}=y^T(I-H)^T (I-H)y.</math>\n\nFor procedures where the matrix <math>H</math> is [[symmetric matrix|symmetric]] and [[idempotent matrix|idempotent]], and the [[errors and residuals in statistics|errors]] are [[multivariate normal distribution|Gaussian]] with covariance matrix <math>\\sigma^2I</math>, <math>\\textrm{RSS}/\\sigma^2</math> has a [[chi-squared distribution]] with <math>k</math> degrees of freedom and noncentrality parameter <math>\\lambda</math>, where\n\n:<math>k=\\operatorname{tr}\\left[(I-H)^T(I-H)\\right]</math>\n:<math>\\lambda=\\mu^T(I-H)^T(I-H)\\mu/2</math>\n\nmay be found by matching the first two [[central moment]]s of a [[noncentral chi-squared distribution|noncentral chi-squared]] random variable to the expressions given in the first two sections. If <math>Hy</math> estimates <math>\\mu</math> with no [[bias of an estimator|bias]], then the noncentrality <math>\\lambda</math> is zero and <math>\\textrm{RSS}/\\sigma^2</math> follows a central chi-squared distribution.\n\n==See also==\n*[[Quadratic form]]\n*[[Covariance matrix]]\n*[[Matrix representation of conic sections]]\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Quadratic Form (Statistics)}}\n[[Category:Statistical theory]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Ramanujan's ternary quadratic form",
      "url": "https://en.wikipedia.org/wiki/Ramanujan%27s_ternary_quadratic_form",
      "text": "In [[mathematics]], in [[number theory]], '''Ramanujan's ternary quadratic form''' is the algebraic expression {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}} with integral values for ''x'', ''y'' and&nbsp;''z''.<ref name=Ono>{{cite journal|first1=Ken|last1=Ono|first2=Kannan | last2=Soundararajan |title=Ramanujan's ternary quadratic form|journal=[[Inventiones Mathematicae]]|year=1997|volume=130|issue=3|pages=415&ndash;454|url=http://mathcs.emory.edu/~ono/publications-cv/pdfs/025.pdf|doi=10.1007/s002220050191|mr=1483991|citeseerx=10.1.1.585.8840}}</ref><ref name=Jones>{{cite journal|first1=Burton W. | last1=Jones|first2=Gordon | last2=Pall |title=Regular and semi-regular positive ternary quadratic forms|journal=[[Acta Mathematica]]|year=1939|volume=70|issue=1|pages=165&ndash;191|doi=10.1007/bf02547347|mr=1555447}}</ref>  [[Srinivasa Ramanujan]] considered this expression in a footnote in a paper<ref name=Ramanujan>{{cite journal|last=S. Ramanujan|title=On the expression of a number in the form ''ax''<sup>2</sup> + ''by''<sup>2</sup> + ''cz''<sup>2</sup> + ''du''<sup>2</sup>|journal=Proc. Camb. Phil. Soc.|year=1916|volume=19|pages=11&ndash;21}}</ref>  published in 1916  and briefly discussed the representability of integers in this form. After giving necessary and sufficient conditions that an integer cannot be represented in the form {{nowrap|''ax''<sup>2</sup> + ''by''<sup>2</sup> + ''cz''<sup>2</sup>}} for certain specific values of ''a'', ''b'' and ''c'', Ramanujan observed in a footnote: \"(These) results may tempt us to suppose that there are similar simple results for the form {{nowrap|''ax''<sup>2</sup> + ''by''<sup>2</sup> + ''cz''<sup>2</sup>}} whatever are the values of ''a'', ''b'' and ''c''. It appears, however, that in most cases there are no such simple results.\"<ref name=Ramanujan/> To substantiate this observation, Ramanujan discussed the form which is now referred to as Ramanujan's ternary quadratic form.\n\n==Properties discovered by Ramanujan==\nIn his 1916 paper<ref name=Ramanujan/> Ramanujan made the following observations about the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}}.\n*The even numbers that are not of the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}} are 4<sup>λ</sup>(16''μ''&nbsp;+&nbsp;6).\n*The odd numbers that are not of the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2}}</sup>, viz. {{nowrap|3, 7, 21, 31, 33, 43, 67, 79, 87, 133, 217, 219, 223, 253, 307, 391, ...}} do not seem to obey any simple law.\n\n==Odd numbers beyond 391==\nBy putting an ellipsis at the end of the list of odd numbers not representable as ''x''<sup>2</sup>&nbsp;+&nbsp;''y''<sup>2</sup> + 10''z''<sup>2</sup>, Ramanujan indicated that his list was incomplete. It was not clear whether Ramanujan intended it to be a finite list or infinite list. This prompted others to look for such odd numbers. In 1927, Burton W. Jones and Gordon Pall<ref name=Jones/> discovered that the number 679 could not be expressed in the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}} and they also verified that there were no  other such numbers below 2000. This led to an early conjecture that the seventeen numbers - the sixteen numbers in Ramanujan's list and the number discovered by them – were the only odd numbers not representable as {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}}. However, in 1941, H Gupta<ref>{{cite journal|last=Gupta|first=Hansraj|title=Some idiosyncratic numbers of Ramanujan|journal=Proceedings of the Indian Academy of Sciences, Section A |year=1941|volume=13|issue=6|pages=519–520|url=http://www.ias.ac.in/jarch/proca/13/00000556.pdf|mr=0004816|doi=10.1007/BF03049015}}</ref> showed that the number  2719 could not be represented as {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}}.  He also verified that there were no other  such numbers below 20000. Further progress in this direction took place only after the development of modern computers. W. Galway wrote a computer programme to determine odd integers not expressible as {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}}. Galway verified that there are only eighteen numbers less than {{nowrap|2 &times; 10<sup>10</sup>}} not representable in the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}}.<ref name=Ono/> Based on Galway's computations, [[Ken Ono]] and K. Soundararajan formulated the following [[conjecture]]:<ref name=Ono/>\n\n:The odd positive integers which are not of the form ''x''<sup>2</sup> + {{nowrap|''y''<sup>2</sup> + 10''z''<sup>2</sup>}} are: {{nowrap|3, 7, 21, 31, 33, 43, 67, 79, 87, 133, 217, 219, 223, 253, 307, 391, 679, 2719}}.\n\n==Some known results==\nThe conjecture of Ken Ono and Soundararajan has not been fully resolved. However, besides the results enunciated by Ramanujan, a few more general results about the form have been established. The proofs of some of them are quite simple while those of the others involve quite complicated concepts and arguments.<ref name=Ono/>\n\n*Every integer of the form 10''n''&nbsp;+&nbsp;5 is represented by Ramanujan's ternary quadratic form.\n*If ''n'' is an odd integer which is not square-free then it can be represented in the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>}}.\n*There are only a finite number of odd integers which cannot be represented in the form ''x''<sup>2</sup> + ''y''<sup>2</sup> + 10''z''<sup>2</sup>.\n*If  the generalized Riemann hypothesis is true, then the conjecture of Ono and Soundararajan is also true.\n*Ramanujan's ternary quadratic form is not regular in the sense of [[Leonard Eugene Dickson|L.E. Dickson]].<ref name=Dickson>{{cite journal|last=L. E. Dickson|title=Ternary Quadratic Forms and Congruences|journal=Annals of Mathematics |series=Second Series|year=1926–1927|volume=28|issue=1/4|pages=333&ndash;341|doi=10.2307/1968378|mr=1502786|jstor=1968378}}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Srinivasa Ramanujan]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Signature (topology)",
      "url": "https://en.wikipedia.org/wiki/Signature_%28topology%29",
      "text": "{{refimprove|date=December 2009}}\nIn the [[mathematics|mathematical]] field of [[topology]], the '''signature''' is an integer [[Invariant (mathematics)|invariant]] which is defined for an oriented [[manifold]] ''M'' of dimension ''d''=4''k'' [[divisible by four]] ([[doubly even]]-dimensional).\n\nThis invariant of a manifold has been studied in detail, starting with  [[Rokhlin's theorem]] for 4-manifolds.\n\n== Definition ==\nGiven a [[connected space|connected]] and [[oriented]] manifold ''M'' of dimension 4''k'', the [[cup product]] gives rise to a [[quadratic form]] ''Q'' on the 'middle' real [[cohomology group]]\n\n:<math>H^{2k}(M,\\mathbf{R})</math>.\n\nThe basic identity for the cup product\n\n:<math>\\alpha^p \\smile \\beta^q = (-1)^{pq}(\\beta^q \\smile \\alpha^p)</math>\n\nshows that with ''p'' = ''q'' = 2''k'' the product is [[symmetric]]. It takes values in\n\n:<math>H^{4k}(M,\\mathbf{R})</math>.\n\nIf we assume also that ''M'' is [[compact manifold|compact]], [[Poincaré duality]] identifies this with\n\n:<math>H^{0}(M,\\mathbf{R})</math>\n\nwhich can be identified with <math>\\mathbf{R}</math>. Therefore cup product, under these hypotheses, does give rise to a [[symmetric bilinear form]] on ''H''<sup>2''k''</sup>(''M'',''R''); and therefore to a quadratic form ''Q''. The form ''Q'' is [[non-degenerate]] due to Poincaré duality, as it pairs non-degenerately with itself.<ref>{{cite book|last1=Hatcher|first1=Allen|title=Algebraic topology|date=2003|publisher=Cambridge Univ. Pr.|location=Cambridge|isbn=978-0521795401|page=250|edition=Repr.|url=https://www.math.cornell.edu/~hatcher/AT/AT.pdf|accessdate=8 January 2017|language=en}}</ref> More generally, the signature can be defined in this way for any general compact [[polyhedron]] with ''4n''-dimensional Poincaré duality.\n\nThe '''signature''' of ''M'' is by definition the [[signature (quadratic form)|signature]] of ''Q'', an ordered triple according to its definition. If ''M'' is not connected, its signature is defined to be the sum of the signatures of its connected components.\n\n== Other dimensions ==\n{{details|L-theory}}\nIf ''M'' has dimension not divisible by 4, its signature is usually defined to be 0. There are alternative generalization in [[L-theory]]: the signature can be interpreted as the 4''k''-dimensional (simply-connected) symmetric L-group <math>L^{4k},</math> or as the 4''k''-dimensional quadratic L-group <math>L_{4k},</math> and these invariants do not always vanish for other dimensions. The [[Kervaire invariant]] is a mod 2 (i.e., an element of <math>\\mathbf{Z}/2</math>) for framed manifolds of dimension 4''k''+2 (the quadratic L-group <math>L_{4k+2}</math>), while the [[de Rham invariant]] is a mod 2 invariant of manifolds of dimension 4''k''+1 (the symmetric L-group <math>L^{4k+1}</math>); the other dimensional L-groups vanish.\n\n=== Kervaire invariant ===\n{{main|Kervaire invariant}}\nWhen <math>d=4k+2=2(2k+1)</math> is twice an odd integer ([[singly even]]), the same construction gives rise to an [[antisymmetric bilinear form]]. Such forms do not have a signature invariant; if they are non-degenerate, any two such forms are equivalent. However, if one takes a [[quadratic refinement]] of the form, which occurs if one has a [[framed manifold]], then the resulting [[ε-quadratic form]]s need not be equivalent, being distinguished by the [[Arf invariant]]. The resulting invariant of a manifold is called the [[Kervaire invariant]].\n\n== Properties ==\n[[René Thom]] (1954) showed that the signature of a manifold is a cobordism invariant, and in particular is given by some linear combination of its [[Lev Semenovich Pontryagin|Pontryagin]] [[Pontryagin class|numbers]]. For example, in four dimensions, it is given by <math>\\frac{p_1}{3}</math>. [[Friedrich Hirzebruch]] (1954) found an explicit expression for this linear combination as the [[L genus]] of the manifold. [[William Browder (mathematician)|William Browder]] (1962) proved that a simply-connected compact [[polyhedron]] with 4''n''-dimensional [[Poincaré duality]] is homotopy equivalent to a manifold if and only if its signature satisfies the expression of the [[Hirzebruch signature theorem]].\n\n==See also==\n*[[Hirzebruch signature theorem]]\n*[[Genus of a multiplicative sequence]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Signature (Topology)}}\n[[Category:Geometric topology]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Smith–Minkowski–Siegel mass formula",
      "url": "https://en.wikipedia.org/wiki/Smith%E2%80%93Minkowski%E2%80%93Siegel_mass_formula",
      "text": "In mathematics, the '''Smith–Minkowski–Siegel mass formula''' (or '''Minkowski–Siegel mass formula''') is a formula for the sum of the weights of the lattices ([[quadratic form]]s) in a [[Genus of a quadratic form|genus]], weighted by the reciprocals of the orders of their automorphism groups. The mass formula is often given for integral quadratic forms, though it can be generalized to quadratic forms over any algebraic number field.\n\nIn 0 and 1 dimensions the mass formula is trivial, in 2 dimensions it is essentially equivalent to [[Peter Gustav Lejeune Dirichlet|Dirichlet's]] [[class number formula]]s for [[imaginary quadratic field]]s, and in 3 dimensions some partial results were given by [[Gotthold Eisenstein]]. \nThe mass formula in higher dimensions was first given by {{harvs|txt=yes|last=Smith|first=H. J. S.|authorlink=H. J. S. Smith|year=1867}}, though his results were forgotten for many years.\nIt was rediscovered by {{harvs|txt=yes|first=H. |last=Minkowski|authorlink=Hermann Minkowski|year=1885}}, and an error in Minkowski's paper was found and corrected by {{harvs|txt=yes|first=C. L. |last=Siegel|authorlink=C. L. Siegel|year=1935}}.\n\nMany published versions of the mass formula have errors; in particular the 2-adic densities are difficult to get right, and it is sometimes forgotten that the trivial cases of dimensions 0 and 1 are different from the cases of dimension at least 2. \n{{harvtxt|Conway|Sloane|1988}} give an expository account and precise statement of the mass formula for integral quadratic forms, which is reliable because they check it on a large number of explicit cases.\n\nFor recent proofs of the mass formula see {{harv|Kitaoka|1999}} and {{harv|Eskin|Rudnick|Sarnak|1991}}.\n\nThe Smith–Minkowski–Siegel mass formula is essentially the constant term of the [[Weil–Siegel formula]].\n\n==Statement of the mass formula==\nIf ''f'' is an ''n''-dimensional positive definite integral quadratic form (or lattice) then the '''mass'''\nof its genus is defined to be\n\n:<math>m(f) = \\sum_{\\Lambda}{1\\over|{\\operatorname{Aut}(\\Lambda)}|}</math>\n\nwhere the sum is over all integrally inequivalent forms in the same genus as ''f'', and Aut(Λ) is the automorphism group of Λ. \nThe form of the '''mass formula''' given by {{harvtxt|Conway|Sloane|1988}} states that for ''n''&nbsp;≥&nbsp;2 the mass is given by\n\n:<math>m(f) = 2\\pi^{-n(n+1)/4}\\prod_{j=1}^n\\Gamma(j/2)\\prod_{p\\text{ prime}}2m_p(f)</math>\n\nwhere ''m''<sub>''p''</sub>(''f'') is the ''p''-mass of ''f'', given by\n\n: <math>m_p(f) = {p^{(rn(n-1)+s(n+1))/2}\\over N(p^r)}</math>\n\nfor sufficiently large ''r'', where ''p''<sup>''s''</sup> is the highest power of ''p'' dividing the determinant of ''f''. The number ''N''(''p''<sup>''r''</sup>) is the number of ''n'' by ''n'' matrices\n''X'' with coefficients that are integers mod&nbsp;''p''<sup>&nbsp;''r''</sup> such that\n\n:<math>X^\\text{tr}AX \\equiv A\\  \\bmod\\  p^r</math>\n\nwhere ''A'' is the Gram matrix of ''f'', or in other words the order of the automorphism group of the form reduced mod&nbsp;''p''<sup>&nbsp;''r''</sup>.\n\nSome authors state the mass formula in terms of the ''p''-adic density\n\n:<math>\\alpha_p(f) = {N(p^r)\\over p^{rn(n-1)/2}} = {p^{s(n+1)/2}\\over m_p(f)}</math>\n\ninstead of the ''p''-mass. The ''p''-mass  is invariant under rescaling ''f'' but the ''p''-density is not.\n\nIn the (trivial) cases of dimension 0 or 1 the mass formula needs some modifications. The factor of 2 in front represents the Tamagawa number of the special orthogonal group, which is only 1 in dimensions 0 and 1. Also the factor of 2 in front of ''m''<sub>''p''</sub>(''f'') represents  the index of the special orthogonal group in the orthogonal group, which is only 1 in 0 dimensions.\n\n==Evaluation of the mass==\nThe mass formula gives the mass as an infinite product over all primes. This can be rewritten as a finite product as follows. For all but a finite number of primes (those not dividing 2&nbsp;det(''ƒ'')) the ''p''-mass ''m''<sub>''p''</sub>(''ƒ'') is equal to the '''standard p-mass''' std<sub>''p''</sub>(''ƒ''), given by\n\n:<math>\\operatorname{std}_p(f)= {1\\over 2(1-p^{-2})(1-p^{-4})\\dots(1-p^{2-n}) (1-{(-1)^{n/2}\\det(f)\\choose p}p^{-n/2})} \\quad</math> (for ''n''&nbsp;=&nbsp;dim(''&fnof;'') even)\n\n:<math>\\operatorname{std}_p(f)= {1\\over 2(1-p^{-2})(1-p^{-4})\\dots(1-p^{1-n}) } </math> (for ''n''&nbsp;=&nbsp;dim(''&fnof;'')&nbsp;odd)\n\nwhere the Legendre symbol in the second line is interpreted as 0 if ''p'' divides 2&nbsp;det(''ƒ'').\n\nIf all the ''p''-masses have their standard value, then the total mass is the\n'''standard mass'''\n\n:<math>\\operatorname{std}(f) = 2\\pi^{-n(n+1)/4}\\left(\\prod_{j=1}^n\\Gamma(j/2)\\right) \\zeta(2)\\zeta(4)\\dots \\zeta(n-1)</math> (For ''n''&nbsp;odd)\n\n:<math>\\operatorname{std}(f) = 2\\pi^{-n(n+1)/4}\\left(\\prod_{j=1}^n\\Gamma(j/2)\\right) \\zeta(2)\\zeta(4)\\dots \\zeta(n-2)\\zeta_D(n/2)</math> (For ''n''&nbsp;even)\n\nwhere\n\n:<math>\\zeta_D(s) = \\prod_p{1\\over 1-{\\big(\\frac{D}{p}\\big)}p^{-s}}</math>\n\n:''D'' = (&minus;1)<sup>''n''/2</sup>&nbsp;det(''&fnof;'')\n\nThe values of the [[Riemann zeta function]] for an even integers ''s''  are given in terms of [[Bernoulli number]]s by\n:<math>\\zeta(s) = {(2\\pi)^s\\over 2\\times s!}|B_s|.</math>\n\nSo the mass of ''ƒ'' is given as a finite product of rational numbers as\n\n:<math>m(f) = \\operatorname{std}(f)\\prod_{p|2\\det(f)}{m_p(f)\\over \\operatorname{std}_p(f)}.</math>\n\n==Evaluation of the ''p''-mass==\n\nIf the form ''f'' has a p-adic Jordan decomposition\n\n:<math>f=\\sum qf_q</math>\n\nwhere ''q'' runs through powers of ''p'' and ''f''<sub>''q''</sub> has determinant prime to ''p'' and dimension ''n''(''q''), \nthen the ''p''-mass is given by\n\n:<math>m_p(f) = \\prod_qM_p(f_q)\\times \\prod_{q<q'}(q'/q)^{n(q)n(q')/2}\\times 2^{n(I,I)-n(II)}</math>\n\nHere ''n''(II) is the sum of the dimensions of all Jordan components of type 2 and ''p''&nbsp;=&nbsp;2, and ''n''(I,I) is the total number of pairs of adjacent constituents ''f''<sub>''q''</sub>, ''f''<sub>2''q''</sub> that are both of type I.\n\nThe factor ''M''<sub>''p''</sub>(''f''<sub>''q''</sub>) is called a '''diagonal factor''' and is a power of ''p'' times the order of a certain orthogonal group over the field with ''p'' elements.\nFor odd ''p'' its value is given by\n\n:<math>{1\\over 2(1-p^{-2})(1-p^{-4})\\dots (1-p^{1-n})}</math>\n\nwhen ''n'' is odd, or\n\n:<math>{1\\over 2(1-p^{-2})(1-p^{-4})\\dots (1-p^{2-n})(1-p^{-n/2})}</math>\n\nwhen ''n'' is even and (&minus;1)<sup>''n''/2</sup>''d''<sub>''q''</sub> is a quadratic residue, or\n\n:<math>{1\\over 2(1-p^{-2})(1-p^{-4})\\dots (1-p^{2-n})(1+p^{-n/2})}</math>\n\nwhen ''n'' is even and (&minus;1)<sup>''n''/2</sup>''d''<sub>''q''</sub> is a quadratic nonresidue.\n\nFor ''p''&nbsp;=&nbsp;2 the diagonal factor ''M''<sub>''p''</sub>(''f''<sub>''q''</sub>) is notoriously tricky to calculate. (The notation is misleading as it depends not only on ''f''<sub>''q''</sub> but also on ''f''<sub>2''q''</sub> and ''f''<sub>''q''/2</sub>.) \n*We say that ''f''<sub>''q''</sub> is '''odd''' if it represents an odd 2-adic integer, and '''even''' otherwise.\n*The '''octane value''' of ''f''<sub>''q''</sub> is an integer mod 8; if ''f''<sub>''q''</sub> is even its octane value is 0 if the determinant is +1 or &minus;1 mod&nbsp;8, and is 4 if the determinant is +3 or &minus;3 mod&nbsp;8, while if ''f''<sub>''q''</sub> is odd it can be diagonalized and its octane value is then the number of diagonal entries that are 1 mod 4 minus the number that are 3 mod&nbsp;4.\n*We say that ''f''<sub>''q''</sub> is  '''bound''' if at least one of ''f''<sub>2''q''</sub> and ''f''<sub>''q''/2</sub> is odd, and say it is '''free''' otherwise.\n*The integer ''t'' is defined so that the dimension of ''f''<sub>''q''</sub>  is 2''t'' if ''f''<sub>''q''</sub> is even, and 2''t''&nbsp;+&nbsp;1 or 2''t''&nbsp;+&nbsp;2 if ''f''<sub>''q''</sub> is odd.\nThen the diagonal factor ''M''<sub>''p''</sub>(''f''<sub>''q''</sub>) is given as follows.\n\n:<math>{1\\over 2(1-p^{-2})(1-p^{-4})\\dots (1-p^{-2t})}</math>\n\nwhen the form is bound or has octane value +2 or &minus;2 mod&nbsp;8 or\n\n:<math>{1\\over 2(1-p^{-2})(1-p^{-4})\\dots (1-p^{2-2t})(1-p^{-t})}</math>\n\nwhen the form is free and has octane value &minus;1 or 0 or 1 mod&nbsp;8 or\n\n:<math>{1\\over 2(1-p^{-2})(1-p^{-4})\\dots (1-p^{2-2t})(1+p^{-t})}</math>\n\nwhen the form is free and has octane value &minus;3 or 3 or 4 mod&nbsp;8.\n\n==Evaluation of &zeta;<sub>''D''</sub>(''s'')==\nThe required values of the Dirichlet series ζ<sub>''D''</sub>(''s'') can be evaluated as follows.  We write χ for the [[Dirichlet character]] with χ(''m'') given by 0 if ''m'' is even, and the [[Jacobi symbol]] <math>{\\left(\\frac{D}{m}\\right)}</math> if ''m'' is odd. We write ''k'' for the modulus of this character and ''k''<sub>1</sub> for its conductor, and put χ&nbsp;=&nbsp;χ<sub>1</sub>ψ where χ<sub>1</sub> is the principal character mod ''k'' and ψ is a primitive character mod ''k''<sub>1</sub>. Then\n\n:<math>\\zeta_D(s) = L(s,\\chi) = L(s,\\psi)\\prod_{p|k}\\left(1 - {\\psi(p)\\over p^s}\\right)</math>\n\nThe functional equation for the L-series is\n\n:<math>L(1-s,\\psi)= {k_1^{s-1}\\Gamma(s)\\over (2\\pi)^s} (i^{-s}+\\psi(-1)i^s)G(\\psi)L(s,\\psi)</math>\n\nwhere ''G'' is the [[Gauss sum]]\n\n:<math>G(\\psi) = \\sum_{r=1}^{k_1}\\psi(r)e^{2\\pi i r/k_1}.</math>\n\nIf ''s'' is a positive integer then\n\n:<math>L(1-s,\\psi) = -{k_1^{s-1}\\over s} \\sum_{r=1}^{k_1}\\psi(r)B_s(r/k_1)</math>\n\nwhere ''B''<sub>''s''</sub>(''x'') is a [[Bernoulli polynomial]].\n\n==Examples==\nFor the case of even [[unimodular lattice]]s Λ of dimension ''n''&nbsp;&gt;&nbsp;0 divisible by 8 the mass formula is\n\n:<math>\\sum_{\\Lambda}{1\\over|\\operatorname{Aut}(\\Lambda)|} = {|B_{n/2}|\\over n}\\prod_{1\\le j< n/2}{|B_{2j}|\\over 4j}</math>\n\nwhere ''B''<sub>''k''</sub> is a [[Bernoulli number]].\n\n===Dimension ''n'' = 0===\nThe formula above fails for ''n'' = 0, and in general the mass formula needs to be modified in the trivial cases when the dimension is at most 1. For ''n'' = 0 there is just one lattice, the zero lattice, of weight 1, so the total mass is 1.\n\n===Dimension ''n'' = 8===\nThe mass formula gives the total mass as\n\n:<math>{|B_4|\\over 8}{|B_2|\\over 4}{|B_4|\\over 8}{|B_6|\\over 12} = {1/30\\over 8}\\;{1/6\\over 4}\\;{1/30\\over 8}\\;{1/42\\over 12} = {1\\over 696729600}.</math>\n\nThere is exactly one even unimodular lattice of dimension 8, the [[E8 lattice]], whose automorphism group is the Weyl group of ''E''<sub>8</sub> of order 696729600, so this verifies the mass formula in this case. \nSmith originally gave a nonconstructive proof of the existence of an even unimodular lattice of dimension 8 using the fact that the mass is non-zero.\n\n===Dimension ''n'' = 16===\nThe mass formula gives the total mass as\n:<math>{|B_8|\\over 16}{|B_2|\\over 4}{|B_4|\\over 8}{|B_6|\\over 12}{|B_8|\\over 16}{|B_{10}|\\over 20}{|B_{12}|\\over 24}{|B_{14}|\\over 28}  = {691\\over 277667181515243520000 }.</math>\nThere are two even unimodular lattices of dimension 16, one with root system ''E''<sub>8</sub><sup>2</sup>\nand automorphism group of order 2&times;696729600<sup>2</sup> = 970864271032320000, and one with root system ''D''<sub>16</sub> and automorphism group of order 2<sup>15</sup>16! = 685597979049984000.\n\nSo the mass formula is\n:<math>{1\\over 970864271032320000} + {1\\over 685597979049984000} = {691\\over 277667181515243520000 }.</math>\n\n===Dimension ''n'' = 24===\nThere are 24 even unimodular lattices of dimension 24, called the [[Niemeier lattice]]s. The mass formula for them is checked in {{harv|Conway|Sloane|1998|pp=410–413}}.\n\n===Dimension ''n'' = 32===\nThe mass in this case is large, more than 40 million. This implies that there are more than 80 million even\nunimodular lattices of dimension 32, as each has automorphism group of order at least 2 so contributes at most 1/2 to the mass. By refining this argument, {{harvtxt|King|2003}} showed that there are more than a billion such lattices. In higher dimensions the mass, and hence the number of lattices, increases very rapidly.\n\n==Generalizations==\nSiegel gave a more general formula that  counts the weighted number of  representations of one quadratic form by forms in some genus; the Smith–Minkowski–Siegel mass formula is the special case when one form is the zero form.\n\nTamagawa showed that the mass formula was equivalent to the statement that the [[Tamagawa number]] of \nthe orthogonal group is 2, which is equivalent to saying that the Tamagawa number of its simply connected cover the spin group is 1. [[André Weil]] conjectured more generally that [[Weil conjecture on Tamagawa numbers|the Tamagawa number of any simply connected semisimple group is 1]], and this conjecture was proved by Kottwitz in 1988.\n\n{{harvtxt|King|2003}} gave a mass formula for [[unimodular lattice]]s without roots (or with given root system).\n\n==See also==\n* [[Siegel identity]]\n\n==References==\n*{{citation|first1=  J. H.|last1= Conway | authorlink=John Horton Conway |first2= N. J. A.|last2= Sloane | author2-link=Neil Sloane | title=Sphere packings, lattices, and groups| year=1998| isbn=978-0-387-98585-5 |publisher=[[Springer-Verlag]] |location=  Berlin}}\n*{{citation|title=Low-Dimensional Lattices. IV. The Mass Formula|first1=  J. H.|last1= Conway|first2= N. J. A.|last2= Sloane |journal=    Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences|volume= 419|issue= 1857|year= 1988|pages=259–286| jstor=2398465 |doi=10.1098/rspa.1988.0107 |bibcode=1988RSPSA.419..259C|citeseerx=  10.1.1.24.2955}}\n*{{citation| mr = 1131433 |last=Eskin|first=Alex|last2= Rudnick|first2= Zeév|last3=Sarnak|first3= Peter |title=A proof of Siegel's weight formula. |journal=Internat. Math. Res. Notices |year=1991|issue= 5|pages= 65&ndash;69 | doi=10.1155/S1073792891000090| volume=1991}} \n*{{citation|arxiv=math.NT/0012231 |title=A mass formula for unimodular lattices with no roots |first= Oliver |last=King |journal=Mathematics of Computation|volume= 72|issue= 242 |year=2003|pages=839&ndash;863|doi=10.1090/S0025-5718-02-01455-2}}.\n*{{citation|last=Kitaoka|first=Yoshiyuki|isbn=978-0-521-64996-4 |title=Arithmetic of Quadratic Forms |series=Cambridge Tracts in Mathematics|year=1999|publisher=Cambridge Univ. Press|location=Cambridge}}\n*{{citation| title=Untersuchungen über quadratische Formen I. Bestimmung der Anzahl verschiedener Formen, welche ein gegebenes Genus enthält |doi=10.1007/BF02402203 |issue=1|volume =7|year= 1885 |pages=201&ndash;258 |journal=Acta Mathematica |first=Hermann|last= Minkowski|authorlink=Hermann Minkowski}}\n*{{citation|title= Uber Die Analytische Theorie Der Quadratischen Formen |first=Carl Ludwig |last=Siegel |authorlink=Carl Ludwig Siegel\n|journal=Annals of Mathematics |series=Second Series|volume=36|issue= 3|year= 1935|pages= 527&ndash;606 |jstor=1968644|doi= 10.2307/1968644}}\n*{{citation|title=On the Orders and Genera of Quadratic Forms Containing More than Three Indeterminates|first= H. J. Stephen|last= Smith |authorlink=H. J. S. Smith|journal= Proceedings of the Royal Society of London|volume= 16|year=1867 |pages= 197&ndash;208\n|jstor=112491|doi=10.1098/rspl.1867.0036 }}\n\n{{DEFAULTSORT:Smith-Minkowski-Siegel mass formula}}\n[[Category:Quadratic forms]]\n[[Category:Hermann Minkowski]]"
    },
    {
      "title": "Spinor genus",
      "url": "https://en.wikipedia.org/wiki/Spinor_genus",
      "text": "In mathematics, the '''spinor genus''' is a classification of [[quadratic form]]s and lattices over the [[ring of integers]], introduced by [[Martin Eichler]]. It refines the [[Genus of a quadratic form|genus]] but may be coarser than proper equivalence\n\n==Definitions==\nWe define two '''Z'''-lattices ''L'' and ''M'' in a [[quadratic space]] ''V'' over '''Q''' to be spinor equivalent if there exists a transformation ''g'' in the proper orthogonal group ''O''<sup>+</sup>(''V'') and for every prime ''p'' there exists a local transformation ''f''<sub>''p''</sub> of ''V''<sub>''p''</sub> of [[spinor norm]] 1 such that ''M'' = ''g'' ''f''<sub>''p''</sub>''L''<sub>''p''</sub>.\n\nA ''spinor genus'' is an equivalence class for this [[equivalence relation]].  Properly equivalent lattices are in the same spinor genus, and lattices in the same spinor genus are in the same genus.  The number of spinor genera in a genus is a power of two, and can be determined effectively.\n\n==Results==\nAn important result is that for [[indefinite form]]s of dimension at least three, each spinor genus contains exactly one proper equivalence class.\n\n==See also==\n*[[Genus of a quadratic form]]\n==References==\n* {{cite book | first=J. W. S. | last=Cassels | authorlink=J. W. S. Cassels | title=Rational Quadratic Forms | series=London Mathematical Society Monographs | volume=13 | publisher=[[Academic Press]] | year=1978 | isbn=0-12-163260-1 | zbl=0395.10029 }}\n* {{cite book | zbl=0915.52003 | last1=Conway | first1=J. H. | author1-link=John Horton Conway | last2=Sloane | first2=N. J. A. | author2-link=Neil Sloane | others=With contributions by Bannai, E.; [[Borcherds, R. E.]]; [[John Leech (mathematician)|Leech, J.]]; [[Simon P. Norton|Norton, S. P.]]; [[Odlyzko, A. M.]]; Parker, R. A.; Queen, L.; Venkov, B. B. | title=Sphere packings, lattices and groups | edition=3rd | series=Grundlehren der Mathematischen Wissenschaften | volume=290 | location=New York, NY | publisher=[[Springer-Verlag]] | isbn=0-387-98585-9 }}\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Surgery structure set",
      "url": "https://en.wikipedia.org/wiki/Surgery_structure_set",
      "text": "In [[mathematics]], the '''surgery structure set''' <math>\\mathcal{S} (X)</math> is the basic object in the study of [[manifold (mathematics)|manifold]]s which are [[homotopy]] equivalent to a [[closed manifold]] X. It is a concept which helps to answer the question whether two homotopy equivalent manifolds are [[diffeomorphic]] (or [[Piecewise linear manifold|PL-homeomorphic]] or [[homeomorphic]]). There are different versions of the structure set depending on the [[category (mathematics)|category]] (DIFF, PL or TOP) and whether [[Whitehead torsion]] is taken into account or not.\n\n== Definition ==\n\nLet X be a closed smooth (or PL- or topological) manifold of dimension n. We call two homotopy equivalences <math>f_i: M_i \\to X</math> from closed manifolds <math>M_i</math> of dimension <math>n</math> to <math>X</math> (<math>i=0,1</math>) equivalent if there exists a [[cobordism]] <math>\\mathcal{}(W;M_0,M_1)</math> together with a map <math>(F;f_0,f_1): (W;M_0,M_1) \\to (X \\times [0,1];X \\times \\{0\\},X \\times \\{1\\})</math> such that <math>F</math>, <math>f_0</math> and <math>f_1</math> are homotopy equivalences.\nThe '''structure set''' <math>\\mathcal{S}^h (X)</math> is the set of equivalence classes of homotopy equivalences <math>f: M \\to X</math> from closed manifolds of dimension n to X.\nThis set has a preferred base point: <math>id: X \\to X</math>.\n\nThere is also a version which takes Whitehead torsion into account. If we require in the definition above the homotopy equivalences F, <math>f_0</math> and <math>f_1</math> to be simple homotopy equivalences then we obtain the '''simple structure set''' <math>\\mathcal{S}^s (X)</math>.\n\n== Remarks ==\n\nNotice that <math>(W;M_0,M_1)</math> in the definition of <math>\\mathcal{S}^h (X)</math> resp. <math>\\mathcal{S}^s (X)</math> is an [[h-cobordism]] resp. an [[s-cobordism]]. Using the [[s-cobordism theorem]] we obtain another description for the simple structure set <math>\\mathcal{S}^s (X)</math>, provided that n>4: The simple structure set <math>\\mathcal{S}^s (X)</math> is the set of equivalence classes of homotopy equivalences <math>f: M \\to X</math> from closed manifolds <math>M</math> of dimension n to X with respect to the following equivalence relation. Two homotopy equivalences <math>f_i: M_i \\to X</math> (i=0,1) are equivalent if there exists a\ndiffeomorphism (or PL-homeomorphism or homeomorphism) <math>g: M_0 \\to M_1</math> such that <math>f_1 \\circ g</math> is homotopic to <math>f_0</math>.\n\nAs long as we are dealing with differential manifolds, there is in general no canonical group structure on <math>\\mathcal{S}^s (X)</math>. If we deal with topological manifolds, it is possible to endow <math>\\mathcal{S}^s (X)</math> with a preferred structure of an abelian group (see chapter 18 in the book of [http://www.maths.ed.ac.uk/~aar/books/topman.pdf  Ranicki]).\n\nNotice that a manifold M is diffeomorphic (or PL-homeomorphic or homeomorphic) to a closed manifold X if and only if there exists a simple homotopy equivalence <math>\\phi: M \\to X</math> whose equivalence class is the base point in <math>\\mathcal{S}^s (X)</math>. Some care is necessary because it may be possible that a given simple homotopy equivalence <math>\\phi: M \\to X</math> is not homotopic to a diffeomorphism (or PL-homeomorphism or homeomorphism) although M and X are diffeomorphic (or PL-homeomorphic or homeomorphic). Therefore, it is also necessary to study the operation of the group of homotopy classes of simple self-equivalences of X on <math>\\mathcal{S}^s (X)</math>.\n\nThe basic tool to compute the simple structure set is the [[surgery exact sequence]].\n\n== Examples ==\n\n'''Topological Spheres:''' The [[generalized Poincaré conjecture]] in the topological category says that <math>\\mathcal{S}^s (S^n)</math> only consists of the base point. This conjecture was proved by Smale (n > 4), Freedman (n = 4) and Perelman (n = 3).\n\n'''Exotic Spheres:''' The classification of [[exotic sphere]]s by Kervaire and Milnor gives <math>\\mathcal{S}^s (S^n) = \\theta_n = \\pi_n(PL/O)</math> for n > 4 (smooth category).\n\n==References==\n*{{Citation | last1=Browder | first1=William | authorlink1=William Browder (mathematician)| title=Surgery on simply-connected manifolds | publisher=[[Springer-Verlag]] | location=Berlin, New York |mr=0358813 | year=1972}}\n*{{Citation | last1=Ranicki | first1=Andrew |  authorlink1= Andrew Ranicki| title=Algebraic and Geometric Surgery | url=http://www.maths.ed.ac.uk/~aar/books/index.htm  | publisher=Oxford Mathematical Monographs, Clarendon Press | isbn=978-0-19-850924-0 |mr=2061749 | year=2002}}\n*{{Citation | last1=Wall | first1=C. T. C. | authorlink1=C. T. C. Wall |title=Surgery on compact manifolds | publisher=[[American Mathematical Society]] | location=Providence, R.I. | edition=2nd | series=Mathematical Surveys and Monographs | isbn=978-0-8218-0942-6 |mr=1687388 | year=1999 | volume=69}}\n*{{Citation | last1=Ranicki | first1=Andrew | authorlink1= Andrew Ranicki| title=Algebraic L-theory and topological manifolds | url=http://www.maths.ed.ac.uk/~aar/books/topman.pdf  | publisher=Cambridge Tracts in Mathematics 102, CUP | isbn= 0-521-42024-5|mr=1211640 | year=1992}}\n\n==External links==\n*[http://www.maths.ed.ac.uk/~aar/ Andrew Ranicki's homepage]\n*[http://www.math.uchicago.edu/~shmuel/ Shmuel Weinberger's homepage]\n\n[[Category:Geometric topology]]\n[[Category:Algebraic topology]]\n[[Category:Quadratic forms]]\n[[Category:Surgery theory]]"
    },
    {
      "title": "Tensor product of quadratic forms",
      "url": "https://en.wikipedia.org/wiki/Tensor_product_of_quadratic_forms",
      "text": "{{Unreferenced|date=February 2008}}\nThe '''[[tensor product]] of [[quadratic form]]s''' is most easily understood when one views the quadratic forms as ''[[quadratic space]]s''. If ''R'' is a commutative ring where ''2'' is invertible, and if <math>(V_1, q_1)</math> and <math>(V_2,q_2)</math> are two quadratic spaces over ''R'', then their tensor product <math>(V_1 \\otimes V_2, q_1 \\otimes q_2)</math> is the quadratic space whose underlying ''R''-module is the [[Tensor product of modules|tensor product]] <math>V_1 \\otimes V_2</math> of ''R''-modules and whose quadratic form is the quadratic form associated to the tensor product of the bilinear forms associated to <math>q_1</math> and <math>q_2</math>.\n\nIn particular, the form <math>q_1 \\otimes q_2</math> satisfies\n\n:<math> (q_1\\otimes q_2)(v_1 \\otimes v_2) = q_1(v_1) q_2(v_2) \\quad \\forall v_1 \\in V_1,\\ v_2 \\in V_2</math>\n\n(which does uniquely characterize it however). It follows from this that if the quadratic forms are diagonalizable (which is always possible if <math>2 \\in R</math> is invertible), i.e., \n\n:<math>q_1 \\cong \\langle a_1, ... , a_n \\rangle</math>\n:<math>q_2 \\cong \\langle b_1, ... , b_m \\rangle</math>\n\nthen the tensor product has diagonalization\n\n:<math>q_1 \\otimes q_2 \\cong \\langle a_1b_1, a_1b_2, ... a_1b_m, a_2b_1, ... , a_2b_m , ... , a_nb_1, ... a_nb_m \\rangle.</math>\n\n[[Category:Quadratic forms]]\n[[Category:Tensors]]\n\n{{algebra-stub}}"
    },
    {
      "title": "U-invariant",
      "url": "https://en.wikipedia.org/wiki/U-invariant",
      "text": "{{DISPLAYTITLE:''u''-invariant}}\nIn mathematics, the '''universal invariant''' or '''''u''-invariant''' of a [[field (mathematics)|field]] describes the structure of [[quadratic form]]s over the field.\n\nThe universal invariant ''u''(''F'') of a field ''F'' is the largest dimension of an [[anisotropic quadratic space]] over ''F'', or ∞ if this does not exist.  Since [[formally real field]]s have anisotropic quadratic forms (sums of squares) in every dimension, the invariant is only of interest for other fields.  An equivalent formulation is that ''u'' is the smallest number such that every form of dimension greater than ''u'' is [[Isotropic quadratic form|isotropic]], or that every form of dimension at least ''u'' is [[Universal quadratic form|universal]].\n\n==Examples==\n* For the complex numbers, ''u''('''C''') = 1.\n* If ''F'' is [[Quadratically closed field|quadratically closed]] then ''u''(''F'') = 1.\n* The function field of an [[algebraic curve]] over an [[algebraically closed field]] has ''u'' ≤ 2; this follows from [[Tsen's theorem]] that such a field is [[quasi-algebraically closed]].<ref name=Lam376>Lam (2005) p.376</ref>\n* If ''F'' is a nonreal [[global field|global]] or [[local field]], or more generally a [[linked field]], then ''u''(''F'') = 1,2,4 or 8.<ref name=Lam406>Lam (2005) p.406</ref>\n\n==Properties==\n* If ''F'' is not formally real then ''u''(''F'') is at most <math>q(F) = \\left|{F^\\star / F^{\\star2}}\\right|</math>, the index of the squares in the multiplicative group of ''F''.<ref name=Lam400>Lam (2005) p.&nbsp;400</ref>\n* ''u''(''F'') cannot take the values 3, 5, or 7.<ref name=Lam401>Lam (2005) p.&nbsp;401</ref>  Fields exist with ''u''&nbsp;=&nbsp;6<ref name=Lam484>Lam (2005) p.484</ref><ref name=Lam1989>{{cite book | last=Lam | first=T.Y. | authorlink=Tsit Yuen Lam | chapter=Fields of u-invariant 6 after A. Merkurjev | zbl=0683.10018 | title=Ring theory 1989. In honor of S. A. Amitsur, Proc. Symp. and Workshop, Jerusalem 1988/89 | series=Israel Math. Conf. Proc. | volume=1 | pages=12–30 | year=1989 }}</ref> and ''u''&nbsp;=&nbsp;9.<ref>{{cite journal | title=Fields of u-Invariant 9 | first=Oleg T. | last=Izhboldin | journal=Annals of Mathematics |series=Second Series | volume=154 | number=3 | year=2001 | pages=529–587 | jstor=3062141 | zbl=0998.11015 }}</ref>\n* [[Alexander Merkurjev|Merkurjev]] has shown that every even integer occurs as the value of ''u''(''F'') for some ''F''.<ref name=Lam402>Lam (2005) p.&nbsp;402</ref><ref name=ELM170>Elman, Karpenko, Merkurjev (2008) p.&nbsp;170</ref>\n* The ''u''-invariant is bounded under finite-degree [[field extension]]s.  If ''E''/''F'' is a field extension of degree ''n'' then\n:<math>u(E) \\le \\frac{n+1}{2} u(F) \\ . </math>\nIn the case of quadratic extensions, the ''u''-invariant is bounded by\n:<math>u(F) - 2 \\le u(E) \\le \\frac{3}{2} u(F) \\ </math>\nand all values in this range are achieved.<ref>{{cite book | last1=Mináč | first1=Ján | last2=Wadsworth | first2=Adrian R. | chapter=The u-invariant for algebraic extensions | zbl=0824.11018 | editor1-first=Alex  | editor1-last=Rosenberg|editor1-link=Alex F. T. W. Rosenberg | title=K-theory and algebraic geometry: connections with quadratic forms and division algebras. Summer Research Institute on quadratic forms and division algebras, July 6-24, 1992, University of California, Santa Barbara, CA (USA) | location=Providence, RI | publisher=[[American Mathematical Society]] | series=Proc. Symp. Pure Math. | volume=58.2 | pages=333–358 | year=1995 }}</ref>\n\n==The general ''u''-invariant==\nSince the ''u''-invariant is of little interest in the case of formally real fields, we define a '''general''' '''''u''-invariant''' to be the maximum dimension of an anisotropic form in the [[torsion subgroup]] of the [[Witt ring (forms)|Witt ring]] of '''F''', or ∞ if this does exist.<ref name=Lam409>Lam (2005) p.&nbsp;409</ref>  For non-formally real fields, the Witt ring is torsion, so this agrees with the previous definition.<ref name=Lam410>Lam (2005) p.&nbsp;410</ref>  For a formally real field, the general ''u''-invariant is either even or ∞.\n\n===Properties===\n* ''u''(''F'') ≤ 1 if and only if ''F'' is a [[Pythagorean field]].<ref name=Lam410/>\n\n==References==\n{{reflist}}\n* {{cite book | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | publisher=American Mathematical Society | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 | mr = 2104929 }}\n* {{cite book | title=Squares | volume=171 | series=London Mathematical Society Lecture Note Series | first=A. R. | last=Rajwade | publisher=[[Cambridge University Press]] | year=1993 | isbn=0-521-42668-5 | zbl=0785.11022 }}\n* {{cite book | title=The algebraic and geometric theory of quadratic forms | volume=56 | series=American Mathematical Society Colloquium Publications | first1=Richard | last1=Elman | first2=Nikita | last2=Karpenko | first3=Alexander | last3=Merkurjev | publisher=American Mathematical Society, Providence, RI | year=2008 | isbn=978-0-8218-4329-1 }}\n\n[[Category:Field theory]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Unimodular lattice",
      "url": "https://en.wikipedia.org/wiki/Unimodular_lattice",
      "text": "{{distinguish|modular lattice}}\nIn [[geometry]] and mathematical [[group theory]], a '''unimodular lattice''' is an integral [[Lattice (group)|lattice]] of [[Lattice (group)#Dividing space according to a lattice|determinant]]&nbsp;1&nbsp;or&nbsp;&minus;1. For a lattice in ''n''-dimensional Euclidean space, this is equivalent to requiring that the [[volume]] of any [[fundamental domain]] for the lattice be&nbsp;1.\n\nThe [[E8 lattice|''E''<sub>8</sub> lattice]] and the [[Leech lattice]] are two famous examples.\n\n== Definitions ==\n\n* A '''lattice''' is a [[free abelian group]] of finite [[rank of an abelian group|rank]] with a [[symmetric bilinear form]] (·,·).\n* The lattice is '''integral''' if (·,·) takes integer values.\n* The '''dimension''' of a lattice is the same as its rank (as a '''Z'''-[[module (mathematics)|module]]).\n* The '''norm''' of a lattice element ''a'' is (''a'', ''a'').\n* A lattice is '''positive definite''' if  the norm of all nonzero elements is positive.\n* The '''determinant''' of a lattice is the determinant of the [[Gram matrix]], a matrix with entries ''(a<sub>i</sub>, a<sub>j</sub>)'', where the elements ''a<sub>i</sub>'' form a basis for the lattice.\n* An integral lattice is '''unimodular''' if its determinant is 1 or &minus;1.\n* A unimodular lattice is '''even''' or '''type II''' if all norms are even, otherwise '''odd''' or '''type I'''.\n* The '''minimum''' of a positive definite lattice is the lowest nonzero norm.\n* Lattices are often embedded in a real vector space with a symmetric bilinear form. The lattice is '''positive definite''', '''Lorentzian''', and so on if its vector space is.\n* The '''signature''' of a lattice is the [[signature (quadratic form)|signature]] of the form on the vector space.\n\n== Examples ==\n\nThe three most important examples of unimodular lattices are:\n\n* The lattice '''Z''', in one dimension.\n* The [[E8 lattice|''E''<sub>8</sub> lattice]], an even 8-dimensional lattice,\n* The [[Leech lattice]], the  24-dimensional even unimodular lattice with no roots.\n\n== Properties ==\n\nA lattice is unimodular if and only if its [[reciprocal lattice|dual lattice]] is integral. Unimodular lattices are equal to their dual lattices, and for this reason, unimodular lattices are also known as self-dual.\n\nGiven a pair (''m'',''n'') of nonnegative integers, an even unimodular lattice of signature (''m'',''n'') exists if and only if ''m-n'' is divisible by 8, but an odd unimodular lattice of signature (''m'',''n'') always exists.  In particular, even unimodular definite lattices only exist in dimension divisible by 8.  Examples in all admissible signatures are given by the ''II<sub>m,n</sub>'' and ''I<sub>m,n</sub>'' constructions, respectively.\n\nThe [[theta function]] of a unimodular positive definite lattice is a [[modular form]] whose weight is one half the rank.  If the lattice is even, the form has [[level of a modular form|level]] 1, and if the lattice is odd the form has Γ<sub>0</sub>(4) structure (i.e., it is a modular form of level 4).  Due to the dimension bound on spaces of modular forms, the minimum norm of a nonzero vector of an even unimodular lattice is no greater than ⎣''n''/24⎦ + 1. An even unimodular lattice that achieves this bound is called extremal. Extremal even unimodular lattices are known in relevant dimensions up to 80,<ref>{{cite web |url=http://www.math.rwth-aachen.de/~Gabriele.Nebe/LATTICES/unimodular.html |title=Unimodular Lattices, Together With A Table of the Best Such Lattices |last1=Nebe |first1=Gabriele |last2=Sloane |first2=Neil |website=Online Catalogue of Lattices |accessdate=2015-05-30}}</ref> and their non-existence has been proven for dimensions above 163,264.<ref>{{cite book |last=Nebe |first=Gabriele |authorlink=Gabriele Nebe |arxiv=1201.1834 |chapter=Boris Venkov's Theory of Lattices and Spherical Designs |year=2013 | title=Diophantine methods, lattices, and arithmetic theory of quadratic forms | pages=1–19 | publisher=[[American Mathematical Society]] | location=Providence, RI | mr=3074799 | volume=587 | series=Contemporary Mathematics | editor1-last=Wan | editor1-first=Wai Kiu | editor2-last=Fukshansky | editor2-first=Lenny | editor3-last=Schulze-Pillot | editor3-first=Rainer | editor4-last=Vaaler | editor4-first=Jeffrey D.| display-editors = 3|bibcode=2012arXiv1201.1834N }}</ref>\n\n==Classification ==\n\nFor indefinite lattices, the classification is easy to describe.\nWrite '''R'''<sup>m,n</sup> for the ''m''&nbsp;+&nbsp;''n'' dimensional vector space\n'''R'''<sup>m+n</sup> with the inner product of \n(''a''<sub>1</sub>,&nbsp;...,&nbsp;''a''<sub>''m''+''n''</sub>) and (''b''<sub>1</sub>,&nbsp;...,&nbsp;''b''<sub>''m''+''n''</sub>) given by\n\n: <math> a_1 b_1 + \\cdots + a_m b_m - a_{m+1} b_{m+1} - \\cdots - a_{m+n} b_{m+n}. \\, </math>\n\nIn '''R'''<sup>m,n</sup> there is one odd indefinite unimodular lattice up to isomorphism, \ndenoted by\n\n:''I''<sub>''m'',''n''</sub>,\n\nwhich is given by all vectors (''a''<sub>1</sub>,...,''a''<sub>''m''+''n''</sub>)\nin '''R'''<sup>''m'',''n''</sup> with all the ''a''<sub>''i''</sub> integers.\n\nThere are no indefinite even unimodular lattices unless\n\n:''m'' &minus; ''n'' is divisible by 8,\n\nin which case there is a unique example up to isomorphism, denoted by\n\n:''II''<sub>''m'',''n''</sub>.\n\nThis is given by all vectors (''a''<sub>1</sub>,...,''a''<sub>''m''+''n''</sub>)\nin '''R'''<sup>''m'',''n''</sup> such that either all the ''a<sub>i</sub>''  are integers or they are all integers plus 1/2, and  their sum is even.  The lattice ''II''<sub>8,0</sub> is the same as the ''E''<sub>8</sub> lattice.\n\nPositive definite unimodular lattices have been classified up to dimension 25. There is a unique example ''I''<sub>''n'',0</sub> in each dimension ''n'' \nless than 8, and two examples (''I''<sub>8,0</sub> and ''II''<sub>8,0</sub>) in dimension 8. The number of lattices increases moderately up to dimension 25 (where there \nare 665 of them), but beyond dimension 25 the [[Smith-Minkowski-Siegel mass formula]] implies that the number increases very rapidly with the dimension; for example, there are more than 80,000,000,000,000,000 in dimension 32.\n\nIn some sense unimodular lattices up to dimension 9 are controlled by ''E''<sub>8</sub>, and up to dimension 25 they are controlled by the Leech lattice, and this accounts for their unusually good behavior in these dimensions. For example, the [[Dynkin diagram]] of the norm-2 vectors of unimodular lattices in dimension up to 25 can be naturally  identified with a configuration of vectors in the Leech lattice. The wild increase in numbers beyond 25 dimensions might be attributed to the fact that these lattices are no longer controlled by the Leech lattice.\n\nEven positive definite unimodular lattice exist only in dimensions divisible by 8.\nThere is one in dimension 8 (the ''E''<sub>8</sub> lattice), two in dimension 16 (''E''<sub>8</sub><sup>2</sup> and ''II''<sub>16,0</sub>), and 24 in dimension 24, called the [[Niemeier lattice]]s (examples: the [[Leech lattice]], ''II''<sub>24,0</sub>, ''II''<sub>16,0</sub>&nbsp;+&nbsp;''II''<sub>8,0</sub>, ''II''<sub>8,0</sub><sup>3</sup>). Beyond 24 dimensions the number increases very rapidly; in 32 dimensions there are more than a billion of them.\n\nUnimodular lattices with no ''roots'' (vectors of norm 1 or 2) have been classified up to dimension 28.  There are none of dimension less than 23 (other than the zero lattice!). \nThere is one in dimension 23 (called the '''short Leech lattice'''), two in dimension\n24 (the Leech lattice and the '''odd Leech lattice'''), and {{harvtxt|Bacher|Venkov|2001}} showed that there are 0, 1, 3, 38 in dimensions 25, 26, 27, 28. Beyond this the number increases very rapidly; there are at least 8000 in dimension 29. In sufficiently high dimensions most unimodular lattices have no roots.\n\nThe only non-zero example of even positive definite unimodular lattices with no roots in dimension less than 32 is the Leech lattice in dimension 24. In dimension 32 there are more than ten million examples, and above dimension 32 the number increases very rapidly.\n\nThe following table from {{harv|King|2003}} gives the numbers of (or lower bounds for) even or odd unimodular lattices in various dimensions, and shows the very rapid growth starting shortly after dimension 24.\n\n{| class=\"wikitable\" style=\"margin:auto; text-align:center;\"\n|-\n! Dimension\n! Odd lattices\n! Odd lattices<br />no roots\n! Even lattices\n! Even lattices<br />no roots\n|-\n| 0 || 0 || 0 || 1 || 1\n|-\n| 1 || 1 || 0 || ||\n|-\n| 2 || 1 || 0 || ||\n|-\n| 3 || 1 || 0 || ||\n|-\n| 4 || 1 || 0 || ||\n|-\n| 5 || 1 || 0 || ||\n|-\n| 6 || 1 || 0 || ||\n|-\n| 7 || 1 || 0 || ||\n|-\n| 8 || 1 || 0 || 1 (E<sub>8</sub> lattice) || 0\n|-\n| 9 || 2 || 0 || ||\n|-\n| 10 || 2 || 0 || ||\n|-\n| 11 || 2 || 0 || ||\n|-\n| 12 || 3 || 0 || ||\n|-\n| 13 || 3 || 0 || ||\n|-\n| 14 || 4 || 0 || ||\n|-\n| 15 || 5 || 0 || ||\n|-\n| 16 || 6 || 0 || 2 (''E''<sub>8</sub><sup>2</sup>, ''D''<sub>16</sub><sup>+</sup>) || 0\n|-\n| 17 || 9 || 0 || ||\n|-\n| 18 || 13 || 0 || ||\n|-\n| 19 || 16 || 0 || ||\n|-\n| 20 || 28 || 0 || ||\n|-\n| 21 || 40 || 0 || ||\n|-\n| 22 || 68 || 0 || ||\n|-\n| 23 || 117 || 1 (shorter Leech lattice) || ||\n|-\n| 24 || 273 || 1 (odd Leech lattice) || 24 (Niemeier lattices) || 1 (Leech lattice)\n|-\n| 25 || 665 || 0 || ||\n|-\n| 26 || ≥&nbsp;2307 || 1 || ||\n|-\n| 27 || ≥&nbsp;14179 || 3 || ||\n|-\n| 28 || ≥&nbsp;327972 || 38 || ||\n|-\n| 29 || ≥&nbsp;37938009 || ≥&nbsp;8900 || ||\n|-\n| 30 || ≥&nbsp;20169641025 || ≥&nbsp;82000000 || ||\n|-\n| 31 || ≥&nbsp;5000000000000 || ≥&nbsp;800000000000 || ||\n|-\n| 32 || ≥&nbsp;80000000000000000 || ≥&nbsp;10000000000000000 || ≥&nbsp;1160000000 || ≥&nbsp;10900000\n|}\n\nBeyond 32 dimensions, the numbers  increase even more rapidly.\n\n== Applications ==\nThe second [[cohomology group]] of a closed [[simply connected]] [[oriented]] topological [[4-manifold]] is a unimodular lattice.  [[Michael Freedman]] showed that this lattice almost determines the manifold: there is a unique such manifold for each even unimodular lattice, and exactly two for each odd unimodular lattice. In particular if we take the lattice to be 0, this implies the [[Poincaré conjecture]] for 4-dimensional topological manifolds. [[Donaldson's theorem]] states that if the manifold is [[Smooth manifold|smooth]] and the lattice is positive definite, then it must be a sum of copies of '''Z''', so most of these manifolds have no [[smooth structure]]. One such example is the [[E8 manifold]].\n\n== References ==\n{{Reflist}}\n*{{citation | last=Bacher | first=Roland | last2=Venkov | first2=Boris | chapter-url=http://www-fourier.ujf-grenoble.fr/PREP/html/a332/a332.html | chapter=Réseaux entiers unimodulaires sans racine en dimension 27 et 28 | trans-chapter=Unimodular integral lattices without roots in dimensions 27 and 28 | title=Réseaux euclidiens, designs sphériques et formes modulaires | trans-title=Euclidean lattices, spherical designs and modular forms | language=fr | editor-last=Martinet | editor-first=Jacques | pages=212–267 | series=Monogr. Enseign. Math. | volume=37 | publication-place=Geneva | year=2001 | isbn=2-940264-02-3 | mr=1878751 | zbl=1139.11319 | publisher=L'Enseignement Mathématique | deadurl=yes | archiveurl=https://web.archive.org/web/20070928015916/http://www-fourier.ujf-grenoble.fr/PREP/html/a332/a332.html | archivedate=2007-09-28 | df= }}\n*{{citation | zbl=0915.52003 | last1=Conway | first1=J.H. | author1-link=John Horton Conway | last2=Sloane | first2=N.J.A. | author2-link=Neil Sloane | others=With contributions by Bannai, E.; Borcherds, R.E.; Leech, J.; Norton, S.P.; Odlyzko, A.M.; Parker, R.A.; Queen, L.; Venkov, B.B. | title=Sphere packings, lattices and groups | edition=Third | series=Grundlehren der Mathematischen Wissenschaften | volume=290 | location=New York, NY | publisher=[[Springer-Verlag]] | isbn=0-387-98585-9 | mr=662447 | year=1999}}\n*{{Citation | last1=King | first1=Oliver D. | title=A mass formula for unimodular lattices with no roots | arxiv=math.NT/0012231 | doi=10.1090/S0025-5718-02-01455-2 | mr=1954971 | year=2003 | journal=[[Mathematics of Computation]]   | volume=72 | issue=242 | pages=839–863 | zbl=1099.11035 }}\n* {{citation | first1=John | last1=Milnor | author1-link=John Milnor| first2=Dale | last2=Husemoller | title=Symmetric Bilinear Forms | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 | mr=0506372 | location=New York-Heidelberg | doi=10.1007/978-3-642-88330-9}}\n* {{citation | first=Jean-Pierre | last=Serre | authorlink=Jean-Pierre Serre | title=A Course in Arithmetic | series=[[Graduate Texts in Mathematics]] | volume=7 | publisher=[[Springer-Verlag]] | year=1973 | isbn=0-387-90040-3 | zbl=0256.12001 | doi=10.1007/978-1-4684-9884-4 | mr=0344216}}\n\n==External links==\n*[[Neil Sloane]]'s [http://www.research.att.com/%7Enjas/lattices/unimodular.html catalogue] of unimodular lattices.\n*{{OEIS el|sequencenumber=A005134|name=Number of n-dimensional unimodular lattices|formalname=Number of n-dimensional unimodular lattice (or quadratic forms)}}\n\n[[Category:Quadratic forms]]\n[[Category:Lattice points]]"
    },
    {
      "title": "Universal quadratic form",
      "url": "https://en.wikipedia.org/wiki/Universal_quadratic_form",
      "text": "In mathematics, a '''universal quadratic form''' is a [[quadratic form]] over a [[ring (algebra)|ring]] that represents every element of the ring.<ref name=Lam10>Lam (2005) p.10</ref>  A non-singular form over a field which represents zero non-trivially is universal.<ref name=R146>Rajwade (1993) p.146</ref>\n\n==Examples==\n* Over the real numbers, the form ''x''<sup>2</sup> in one variable is not universal, as it cannot represent negative numbers: the two-variable form {{nowrap|''x''<sup>2</sup> − ''y''<sup>2</sup>}} over '''R''' is universal.\n* [[Lagrange's four-square theorem]] states that every positive integer is the sum of four squares.  Hence the form {{nowrap|''x''<sup>2</sup> + ''y''<sup>2</sup> + ''z''<sup>2</sup> + ''t''<sup>2</sup> − ''u''<sup>2</sup>}} over '''Z'''  is universal.\n* Over a [[finite field]], any non-singular quadratic form of dimension 2 or more is universal.<ref name=Lam36>Lam (2005) p.36</ref>\n\n==Forms over the rational numbers==\nThe [[Hasse–Minkowski theorem]] implies that a form is universal over '''Q''' if and only if it is universal over '''Q'''<sub>''p''</sub> for all ''p'' (where we include {{nowrap|1=''p'' = ∞}}, letting '''Q'''<sub>∞</sub> denote '''R''').<ref name=S43>Serre (1973) p.43</ref>  A form over '''R''' is universal if and only if it is not [[Definite quadratic form|definite]]; a form over '''Q'''<sub>''p''</sub> is universal if it has dimension at least 4.<ref name=S37>Serre (1973) p.37</ref>  One can conclude that all indefinite forms of dimension at least 4 over '''Q''' are universal.<ref name=S43/>\n\n==See also==\n* The [[15 and 290 theorems]] give conditions for a quadratic form to represent all positive integers.\n\n==References==\n{{reflist}}\n* {{cite book | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | publisher=American Mathematical Society | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 | mr = 2104929 }}\n* {{cite book | title=Squares | volume=171 | series=London Mathematical Society Lecture Note Series | first=A. R. | last=Rajwade | publisher=[[Cambridge University Press]] | year=1993 | isbn=0-521-42668-5 | zbl=0785.11022 }}\n* {{cite book | first=Jean-Pierre | last=Serre | authorlink=Jean-Pierre Serre | title=A Course in Arithmetic | series=[[Graduate Texts in Mathematics]] | volume=7 | publisher=[[Springer-Verlag]] | year=1973 | isbn=0-387-90040-3 | zbl=0256.12001 }}\n\n{{algebra-stub}}\n\n[[Category:Field theory]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Witt group",
      "url": "https://en.wikipedia.org/wiki/Witt_group",
      "text": "{{for|Witt groups in the theory of [[algebraic group]]s|Witt vector}}\nIn mathematics, a '''Witt group''' of a field, named after [[Ernst Witt]], is an [[abelian group]] whose elements are represented by symmetric bilinear forms over the field.\n\n==Definition==\nFix a field ''k'' of characteristic not two.  All [[vector space]]s will be assumed to be finite-dimensional. We say that two spaces equipped with [[symmetric bilinear form]]s are '''equivalent''' if one can be obtained from the other by adding a [[metabolic quadratic space]], that is, zero or more copies of a [[hyperbolic plane (quadratic forms)|hyperbolic plane]], the non-degenerate two-dimensional symmetric bilinear form with a norm 0 vector.<ref name=MB14>Milnor & Husemoller (1973) p.&nbsp;14</ref>  Each class is represented by the [[core form]] of a [[Witt decomposition]].<ref name=L30>Lorenz (2008) p.&nbsp;30</ref>\n\nThe '''Witt group of k''' is the abelian group ''W''(''k'') of equivalence classes of non-degenerate symmetric bilinear forms, with the group operation corresponding to the [[orthogonal direct sum]] of forms.  It is additively generated by the classes of one-dimensional forms.<ref name=MH65>Milnor & Husemoller (1973) p.&nbsp;65</ref>  Although classes may contain spaces of different dimension, the parity of the dimension is constant across a class and so rk : ''W''(''k'') → '''Z'''/2'''Z''' is a homomorphism.<ref name=MH66/>\n\nThe elements of finite order in the Witt group have [[Order (group theory)|order]] a power of 2;<ref name=L37>Lorenz (2008) p.&nbsp;37</ref><ref name=MB72>Milnor & Husemoller (1973) p.&nbsp;72</ref> the [[torsion subgroup]] is the [[Kernel (group theory)|kernel]] of the [[functorial]] map from ''W''(''k'') to ''W''(''k''<sup>py</sup>), where ''k''<sup>py</sup> is the [[Pythagorean closure]] of ''k'';<ref name=Lam260>Lam (2005) p.&nbsp;260</ref> it is generated by the [[Pfister form]]s.<ref name=Lam395>Lam (2005) p.&nbsp;395</ref>  If ''k'' is not formally real, then the Witt group is [[Torsion group|torsion]], with [[Exponent (group theory)|exponent]] a power of 2.<ref name=L35/>  The '''height''' of the field ''k'' is the exponent of the torsion in the Witt group, if this is finite, or ∞ otherwise.<ref name=Lam395>Lam (2005) p.&nbsp;395</ref>\n\n==Ring structure==\nThe Witt group of ''k'' can be given a [[commutative ring]] structure, by using the [[tensor product of quadratic forms]] to define the ring product. This is sometimes called the '''Witt ring''' ''W''(''k''), though the term \"Witt ring\" is often also used for a completely different ring of [[Witt vector]]s.\n\nTo discuss the structure of this ring we assume that ''k'' is of [[Characteristic (field)|characteristic]] not equal to 2, so that we may identify symmetric bilinear forms and quadratic forms.\n\nThe kernel of the rank mod 2 homomorphism is a [[prime ideal]], ''I'', of the Witt ring<ref name=MH66>Milnor & Husemoller (1973) p.&nbsp;66</ref> termed the ''fundamental ideal''.<ref name=L31/>  The ring homomorphisms from ''W''(''k'') to '''Z''' correspond to the [[field ordering]]s of ''k'', by taking [[Signature (quadratic form)|signature]] with respective to the ordering.<ref name=L31>Lorenz (2008) p.&nbsp;31</ref>  The Witt ring is a [[Jacobson ring]].<ref name=L35>Lorenz (2008) p.&nbsp;35</ref>  It is a [[Noetherian ring]] if and only if there are finitely many [[square class]]es; that is, if the squares in ''k'' form a subgroup of finite [[Index of a subgroup|index]] in the multiplicative group.<ref name=Lam32>Lam (2005) p.&nbsp;32</ref>\n\nIf ''k'' is not formally real, the fundamental ideal is the only prime ideal of ''W''<ref name=L33>Lorenz (2008) p.&nbsp;33</ref> and consists precisely of the [[nilpotent element]]s;<ref name=L35/> ''W'' is a [[local ring]] and has [[Krull dimension]] 0.<ref name=Lam280>Lam (2005) p.&nbsp;280</ref>\n\nIf ''k'' is real, then the nilpotent elements are precisely those of finite additive order, and these in turn are the forms all of whose signatures are zero;<ref name=L36>Lorenz (2008) p.&nbsp;36</ref> ''W'' has Krull dimension 1.<ref name=Lam280/>\n\nIf ''k'' is a real [[Pythagorean field]] then the [[zero-divisor]]s of ''W'' are the elements for which some signature is zero; otherwise, the zero-divisors are exactly the fundamental ideal.<ref name=L37/><ref name=Lam282>Lam (2005) p.&nbsp;282</ref>\n\nIf ''k'' is an ordered field with positive cone ''P'' then [[Sylvester's law of inertia]] holds for quadratic forms over ''k'' and the ''signature'' defines a ring homomorphism from ''W''(''k'') to '''Z''', with kernel a [[prime ideal]] ''K<sub>P</sub>''.  These prime ideals are in [[bijection]] with the orderings ''X<sub>k</sub>'' of ''k'' and constitute the minimal prime ideal [[Spectrum of a ring|spectrum]] MinSpec ''W''(''k'') of ''W''(''k'').  The bijection is a [[homeomorphism]] between MinSpec ''W''(''k'') with the [[Zariski topology]] and the set of orderings ''X<sub>k</sub>'' with the [[Harrison topology]].<ref>Lam (2005) pp. 277–280</ref>\n\nThe ''n''-th power of the fundamental ideal is additively generated by the ''n''-fold [[Pfister form]]s.<ref name=Lam316>Lam (2005) p.316</ref>\n\n==Examples==\n* The Witt ring of '''C''', and indeed any [[algebraically closed field]] or [[quadratically closed field]], is '''Z'''/2'''Z'''.<ref name=Lam34>Lam (2005) p.&nbsp;34</ref>\n* The Witt ring of '''R''' is '''Z'''.<ref name=Lam34/>\n* The Witt ring of a finite field '''F'''<sub>''q''</sub> with ''q'' odd is '''Z'''/4'''Z''' if ''q'' is 3 mod 4 and isomorphic to the group ring ('''Z'''/2'''Z''')[''F*''/''F*''<sup>2</sup>] if ''q'' = 1 mod 4.<ref name=Lam37>Lam (2005) p.37</ref>\n* The Witt ring of a [[local field]] with [[Maximal ideal of a valuation|maximal ideal]] of [[Ideal norm|norm]] congruent to 1 modulo 4 is isomorphic to the group ring ('''Z'''/2'''Z''')[''V''] where ''V'' is the [[Klein 4-group]].<ref name=Lam152>Lam (2005) p.152</ref>\n* The Witt ring of a local field with maximal ideal of norm congruent to 3 modulo 4 is ('''Z'''/4'''Z''')[''C''<sub>2</sub>] where ''C''<sub>2</sub> is a cyclic group of order 2.<ref name=Lam152>Lam (2005) p.152</ref>\n* The Witt ring of '''Q'''<sub>2</sub> is of order 32 and is given by<ref name=Lam166>Lam (2005) p.166</ref>\n::<math>\\mathbf{Z}_8[s,t]/\\langle 2s,2t,s^2,t^2,st-4 \\rangle \\ . </math>\n\n==Invariants==\nCertain invariants of a quadratic form can be regarded as functions on Witt classes.  We have seen that dimension mod 2 is a function on classes: the [[discriminant of a quadratic form|discriminant]] is also well-defined.  The [[Hasse invariant of a quadratic form]] is again a well-defined function on Witt classes with values in the [[Brauer group]] of the field of definition.<ref name=Lam119>Lam (2005) p.119</ref>\n\n===Rank and discriminant===\nWe define a ring over ''K'', ''Q''(''K''), as a set of pairs (''d'',''e'') with ''d'' in ''K*''/''K*''<sup>2</sup> and ''e'' in '''Z'''/2'''Z'''.  Addition and multiplication are defined by:\n\n:<math>(d_1,e_1) + (d_2,e_2) =  ((-1)^{e_1e_2}d_1d_2, e_1+e_2) </math>\n:<math>(d_1,e_1) \\cdot (d_2,e_2) = (d_1^{e_2}d_2^{e_1}, e_1e_2) \\ . </math>\n\nThen there is a surjective ring homomorphism from W(''K'') to this obtained by mapping a class to discriminant and rank mod 2.  The kernel is ''I''<sup>2</sup>.<ref name=CP12>Conner & Perlis (1984) p.12</ref>  The elements of ''Q'' may be regarded as classifying graded quadratic extensions of ''K''.<ref name=Lam113>Lam (2005) p.113</ref>\n\n===Brauer–Wall group===\nThe triple of discriminant, rank mod 2 and Hasse invariant defines a map from W(''K'') to the [[Brauer–Wall group]] BW(''K'').<ref name=Lam117>Lam (2005) p.117</ref>\n\n==Witt ring of a local field==\nLet ''K'' be a complete [[local field]] with valuation ''v'', uniformiser π and residue field ''k'' of characteristic not 2.  There is an injection ''W''(''k'') → ''W''(''K'') which lifts the diagonal form ⟨''a''<sub>1</sub>,...''a''<sub>''n''</sub>⟩ to ⟨''u''<sub>1</sub>,...''u''<sub>''n''</sub>⟩ where ''u''<sub>''i''</sub> is a unit of ''K'' with image ''a''<sub>''i''</sub> in ''k''.  This yields\n\n:<math> W(K) = W(k) \\oplus \\langle \\pi \\rangle \\cdot W(k) </math>\n\nidentifying ''W''(''k'') with its image in ''W''(''K'').<ref name=GMS64>Garibaldi, Merkurjev & Serre (2003) p.64</ref>\n\n==Witt ring of a number field==\nLet ''K'' be a [[number field]].  For quadratic forms over ''K'', there is a [[Hasse invariant of a quadratic form|Hasse invariant]] ±1 for every [[finite place]] corresponding to the [[Hilbert symbol]]s.  The invariants of a form over a number field are precisely the dimension, discriminant, all local Hasse invariants and the [[Signature of a quadratic form|signature]]s coming from real embeddings.<ref name=CP16>Conner & Perlis (1984) p.16</ref>\n\nWe define the '''symbol ring''' over ''K'', Sym(''K''), as a set of triples (''d'',''e'',''f'') with ''d'' in ''K*''/''K*''<sup>2</sup>, ''e'' in ''Z''/2 and ''f'' a sequence of elements ±1 indexed by the places of ''K'', subject to the condition that all but finitely many terms of ''f'' are +1, that the value on acomplex places is +1 and that the product of all the terms in ''f'' in +1.  Let [''a'', ''b''] be the sequence of Hilbert symbols: it satisfies the conditions on ''f'' just stated.<ref name=CP167>Conner & Perlis (1984) p.16-17</ref>\n\nWe define addition and multiplication as follows:\n\n:<math>(d_1,e_1,f_1) + (d_2,e_2,f_2) =             ((-1)^{e_1e_2}d_1d_2, e_1+e_2, [d_1,d_2][-d_1d_2,(-1)^{e_1e_2}]f_1f_2) </math>\n:<math>(d_1,e_1,f_1) \\cdot (d_2,e_2,f_2) =             (d_1^{e_2}d_2^{e_1}, e_1e_2, [d_1,d_2]^{1+e_1e_2}f_1^{e_2}f_2^{e_1}) \\ . </math>\n\nThen there is a surjective ring homomorphism from W(''K'') to Sym(''K'') obtained by mapping a class to discriminant, rank mod 2, and the sequence of Hasse invariants.  The kernel is ''I''<sup>3</sup>.<ref name=CP18>Conner & Perlis (1984) p.18</ref>\n\nThe symbol ring is a realisation of the Brauer-Wall group.<ref name=Lam116>Lam (2005) p.116</ref>\n\n===Witt ring of the rationals===\nThe [[Hasse–Minkowski theorem]] implies that there is an injection<ref name=Lam174>Lam (2005) p.174</ref>\n\n:<math> W(\\mathbf{Q}) \\rightarrow W(\\mathbf{R}) \\oplus \\prod_p W(\\mathbf{Q}_p) \\ . </math>\n\nWe make this concrete, and compute the image, by using the \"second residue homomorphism\" W('''Q'''<sub>''p''</sub>) → W('''F'''<sub>''p''</sub>).  Composed with the map W('''Q''') → W('''Q'''<sub>''p''</sub>) we obtain a group homomorphism ∂<sub>''p''</sub>: W('''Q''') → W('''F'''<sub>''p''</sub>) (for ''p''=2 we define ∂<sub>''2''</sub> to be the 2-adic valuation of the discriminant, taken mod 2).\n\nWe then have a split exact sequence<ref name=Lam175>Lam (2005) p.175</ref>\n\n:<math>\\mathbf{Z} \\rightarrow W(\\mathbf{Q}) \\rightarrow  \\mathbf{Z}/2 \\oplus \\prod_{p\\ne2} W(\\mathbf{F}_p) \\rightarrow 0 \\  </math>\n\nwhich can be written as an isomorphism\n\n:<math>W(\\mathbf{Q}) \\cong \\mathbf{Z} \\oplus \\mathbf{Z}/2 \\oplus \\prod_{p\\ne2} W(\\mathbf{F}_p) \\  </math>\n\nwhere the first component is the signature.<ref name=Lam178>Lam (2005) p.178</ref>\n\n\n==Witt ring and Milnor's K-theory==\n\nLet ''k'' be a field of characteristic not equal to 2. The powers of the ideal ''I'' of forms of even dimension (\"fundamental ideal\") in <math>W(k)</math> form a descending [[filtration]] and one may consider the associated graded ring, that is the direct sum of quotients <math>I^n/I^{n+1}</math>. Let <math>\\langle a\\rangle</math> be the quadratic form <math>ax^2</math> considered as an element of the Witt ring. Then <math>\\langle a\\rangle - \\langle 1\\rangle</math> is an element of ''I'' and correspondingly a product of the form\n\n:<math> \\langle\\langle a_1,\\ldots ,a_n\\rangle\\rangle = (\\langle a_1\\rangle - \\langle 1\\rangle)\\cdots (\\langle a_n\\rangle - \\langle 1\\rangle) </math>\n\nis an element of <math>I^n.</math> [[John Milnor]] in a 1970 paper <ref>{{Citation | last1=Milnor | first1=John Willard | author1-link=John Milnor | title=Algebraic K-theory and quadratic forms | doi=10.1007/BF01425486 | mr=0260844 | year=1970 | journal=[[Inventiones Mathematicae]] | issn=0020-9910 | volume=9 | pages=318–344 | issue=4}}</ref> proved that the mapping from <math>(k^*)^n</math> to <math>I^n/I^{n+1}</math> that sends <math>(a_1,\\ldots ,a_n)</math> to <math>\\langle\\langle a_1,\\ldots ,a_n\\rangle\\rangle</math> is multi-linear and maps Steinberg elements (elements such that for some <math>i</math> and <math>j</math> such that <math>i\\ne j</math> one has <math>a_i+a_j=1</math>) to zero. This means that this mapping defines a homomorphism from the [[Milnor ring]] of ''k'' to the graded Witt ring. Milnor showed also that this homomorphism sends elements divisible by 2 to zero and that it is surjective. In the same paper he made a conjecture that this homomorphism is an isomorphism for all fields ''k'' (of characteristic different from 2). This became known as the Milnor conjecture on quadratic forms.\n\nThe conjecture was proved by Dmitry Orlov, Alexander Vishik and [[Vladimir Voevodsky]]<ref>{{Citation | last1=Orlov | first1=Dmitry | last2 = Vishik | first2 = Alexander | last3 = Voevodsky | first3 = Vladimir | title=An exact sequence for ''K<sub>*</sub><sup>M</sup>/2'' with applications to quadratic forms | year=2007 | journal=[[Annals of Mathematics]] | volume=165 | pages=1–13 | issue=1 | doi=10.4007/annals.2007.165.1| arxiv=math/0101023 }}</ref> in 1996 (published in 2007) for the case <math>\\textrm{char}(k)=0</math>, leading to increased understanding of the structure of quadratic forms over arbitrary fields.\n\n==Grothendieck-Witt ring==\nThe '''Grothendieck-Witt ring''' ''GW'' is a related construction generated by isometry classes of nonsingular quadratic spaces with addition given by orthogonal sum and multiplication given by tensor product. Since two spaces that differ by a hyperbolic plane are not identified in ''GW'', the inverse for the addition needs to be introduced formally through the construction that was discovered by Grothendieck (see [[Grothendieck group]]).  There is a natural homomorphism ''GW'' → '''Z''' given by dimension: a field is [[quadratically closed field|quadratically closed]] if and only if this is an isomorphism.<ref name=Lam34>Lam (2005) p.&nbsp;34</ref> The hyperbolic spaces generate an ideal in ''GW'' and the Witt ring ''W'' is the quotient.<ref name=Lam28>Lam (2005) p.&nbsp;28</ref>  The [[exterior power]] gives the Grothendieck-Witt ring the additional structure of a [[λ-ring]].<ref name=GMS63>Garibaldi, Merkurjev & Serre (2003) p.63</ref>\n\n===Examples===\n* The Grothendieck-Witt ring of '''C''', and indeed any [[algebraically closed field]] or [[quadratically closed field]], is '''Z'''.<ref name=Lam34/>\n* The Grothendieck-Witt ring of '''R''' is isomorphic to the group ring '''Z'''[''C''<sub>2</sub>], where ''C''<sub>2</sub> is a cyclic group of order 2.<ref name=Lam34/>\n* The Grothendieck-Witt ring of any finite field of odd characteristic is '''Z''' ⊕ '''Z'''/2'''Z''' with trivial multiplication in the second component.<ref name=Lam36>Lam (2005) p.36, Theorem 3.5</ref> The element ''(1, 0)'' corresponds to the quadratic form ''⟨a⟩'' where ''a'' is not a square in the finite field.\n* The Grothendieck-Witt ring of a local field with maximal ideal of norm congruent to 1 modulo 4 is isomorphic to '''Z''' ⊕ ('''Z'''/2'''Z''')<sup>3</sup>.<ref name=Lam152/>\n* The Grothendieck-Witt ring of a local field with maximal ideal of norm congruent to 3 modulo 4 it is '''Z''' ⊕'''Z'''/4'''Z''' ⊕ '''Z'''/2'''Z'''.<ref name=Lam152/>\n\n==Grothendieck-Witt ring and motivic stable homotopy groups of spheres==\nIt was shown recently by [[Fabien Morel]] <ref>Fabien Morel, On the motivic stable π<sub>0</sub> of the sphere spectrum, In: Axiomatic, Enriched and Motivic Homotopy Theory, pp.&nbsp;219–260, J.P.C. Greenlees (ed.), 2004 Kluwer Academic Publishers.</ref><ref>Fabien Morel, '''A'''<sup>1</sup>-Algebraic topology over a field. Lecture Notes in Mathematics 2052, Springer Verlag, 2012.</ref> that the Grothendieck-Witt ring of a perfect field is isomorphic to the motivic stable homotopy group of spheres π<sub>0,0</sub>(S<sup>0,0</sup>) (see \"[[A¹ homotopy theory]]\").\n\n==Witt equivalence==\nTwo fields are said to be '''Witt equivalent''' if their Witt rings are isomorphic.\n\nFor global fields there is a local-to-global principle: two global fields are Witt equivalent if and only if there is a bijection between their places such that the corresponding local fields are Witt equivalent.<ref>{{cite book | last1=Perlis | first1=R. |last2=Szymiczek |first2=K. |last3=Conner |first3=P.E. |last4=Litherland |first4=R. |chapter=Matching Witts with global fields |editor1-last=Jacob |editor1-first=William B. |displayeditors=etal |title=Recent advances in real algebraic geometry and quadratic forms |location=Providence, RI |publisher=[[American Mathematical Society]] |series=Contemp. Math. |volume=155 |pages=365–387 |year=1994 |isbn=0-8218-5154-3 |zbl=0807.11024}}</ref>  In particular, two number fields ''K'' and ''L'' are Witt equivalent if and only if there is a  bijection ''T'' between the places of ''K'' and the places of ''L'' and a group isomorphism ''t'' between their [[square class|square-class groups]], preserving degree 2 Hilbert symbols. In this case the pair (''T'',''t'') is called a '''reciprocity equivalence''' or a '''degree 2 Hilbert symbol equivalence'''.<ref>{{cite journal | last=Szymiczek | first=Kazimierz | title=Hilbert-symbol equivalence of number fields | zbl=0978.11012 | journal=Tatra Mt. Math. Publ. | volume=11 | pages=7–16 | year=1997 }}</ref>  Some variations and extensions of this condition, such as \"tame degree ''l'' Hilbert symbol equivalence\", have also been studied.<ref>{{cite journal | last=Czogała | first=A. | title=Higher degree tame Hilbert-symbol equivalence of number fields | journal=Abh. Math. Sem. Univ. Hamburg | volume=69 | year=1999 | pages=175–185 | zbl=0968.11038 | doi=10.1007/bf02940871}}</ref>\n\n==Generalizations==\n{{main|L-theory}}\nWitt groups can also be defined in the same way for [[skew-symmetric form]]s, and for [[quadratic form]]s, and more generally [[ε-quadratic form]]s, over any [[*-ring]] ''R''.\n\nThe resulting groups (and generalizations thereof) are known as the even-dimensional symmetric [[L-theory|''L''-group]]s ''L''<sup>2''k''</sup>(''R'') and even-dimensional quadratic ''L''-groups  ''L''<sub>2''k''</sub>(''R''). The quadratic ''L''-groups are 4-periodic, with ''L''<sub>0</sub>(''R'') being the Witt group of (1)-quadratic forms (symmetric), and ''L''<sub>2</sub>(''R'') being the Witt group of (-1)-quadratic forms (skew-symmetric); symmetric ''L''-groups are not 4-periodic for all rings, hence they provide a less exact generalization.\n\n''L''-groups are central objects in [[surgery theory]], forming one of the three terms of the [[surgery exact sequence]].\n\n==Notes==\n{{reflist|2}}\n\n==References==\n* {{cite book | first1=Pierre E. | last1=Conner |author1-link=Pierre Conner| first2=Robert | last2=Perlis | title=A Survey of Trace Forms of Algebraic Number Fields | series=Series in Pure Mathematics | volume=2 | publisher=World Scientific | year=1984 | isbn=9971-966-05-0 | zbl=0551.10017 }}\n* {{cite book | last1=Garibaldi | first1=Skip | author1-link=Skip Garibaldi | last2=Merkurjev | first2=Alexander | author2-link=Alexander Merkurjev | last3=Serre | first3=Jean-Pierre | author3-link=Jean-Pierre Serre | title=Cohomological invariants in Galois cohomology | series=University Lecture Series | volume=28 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=2003 | isbn=0-8218-3287-5 | zbl=1159.12311 }}\n* {{cite book | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | publisher=American Mathematical Society | year=2005 | isbn=0-8218-1095-2 | zbl=1068.11023 | mr = 2104929 }}\n* {{Lang Algebra|edition=3r}}\n* {{cite book | first=Falko | last=Lorenz | title=Algebra. Volume II: Fields with Structure, Algebras and Advanced Topics | year=2008 | publisher=Springer | isbn=978-0-387-72487-4 | zbl=1130.12001 }}\n* {{cite book | first1=John | last1=Milnor | author1-link=John Milnor| first2=Dale | last2=Husemoller | author2-link=Dale Husemoller| title=Symmetric Bilinear Forms | series=Ergebnisse der Mathematik und ihrer Grenzgebiete | volume=73 | publisher=[[Springer-Verlag]] | year=1973 | isbn=3-540-06009-X | zbl=0292.10016 }}\n*{{citation|title =  Theorie der quadratischen Formen in beliebigen Korpern | year = 1936 | last=Witt | first=Ernst | authorlink=Ernst Witt| journal = [[Crelle's Journal|Journal für die reine und angewandte Mathematik]]| pages = 31–44 | volume = 176 | issue = 3 | zbl=0015.05701 }}\n\n==Further reading==\n* {{cite book | last=Balmer | first=Paul | chapter=Witt groups | editor1-last=Friedlander | editor1-first=Eric M. | editor2-last=Grayson | editor2-first=D.R. | title=Handbook of ''K''-theory | volume=2 | pages=539–579 | publisher=[[Springer-Verlag]] | year=2005 | isbn=3-540-23019-X | zbl=1115.19004 }}\n\n==External links==\n*[http://eom.springer.de/W/w098080.htm Witt rings] in the Springer encyclopedia of mathematics\n\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Hilbert space",
      "url": "https://en.wikipedia.org/wiki/Hilbert_space",
      "text": "{{short description|inner product space that is metrically complete; a Banach space whose norm induces an inner product (follows the parallelogram identity)}}\n{{Dablink|For the Hilbert space-filling curve, see [[Hilbert curve]].}}\n{{pp-move-indef}}\n[[Image:Standing waves on a string.gif|thumb|The state of a [[vibrating string]] can be modeled as a point in a Hilbert space. The decomposition of a vibrating string into its vibrations in distinct [[overtone]]s is given by the projection of the point onto the coordinate axes in the space.]]\n\nThe [[mathematics|mathematical]] concept of a '''Hilbert space''', named after [[David Hilbert]], generalizes the notion of [[Euclidean space]]. It extends the methods of [[linear algebra|vector algebra]] and [[calculus]] from the two-dimensional [[plane (geometry)|Euclidean plane]] and three-dimensional space to spaces with any finite or infinite number of [[Dimension|dimensions]]. A Hilbert space is an abstract [[vector space]] possessing the [[mathematical structure|structure]] of an [[inner product space|inner product]] that allows length and angle to be measured. Furthermore, Hilbert spaces are [[Complete metric space|complete]]: there are enough [[limit (mathematics)|limits]] in the space to allow the techniques of calculus to be used.\n\nHilbert spaces arise naturally and frequently in [[mathematics]] and [[physics]], typically as [[infinite-dimensional]] [[function space]]s. The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by [[David Hilbert]], [[Erhard Schmidt]], and [[Frigyes Riesz]]. They are indispensable tools in the theories of [[partial differential equation]]s, [[mathematical formulation of quantum mechanics|quantum mechanics]], [[Fourier analysis]] (which includes applications to [[signal processing]] and heat transfer), and [[ergodic theory]] (which forms the mathematical underpinning of [[thermodynamics]]). [[John von Neumann]] coined the term ''Hilbert space'' for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for [[functional analysis]]. Apart from the classical Euclidean spaces, examples of Hilbert spaces include [[Lp space|spaces of square-integrable functions]], [[Sequence space|spaces of sequences]], [[Sobolev space]]s consisting of [[generalized function]]s, and [[Hardy space]]s of [[holomorphic function]]s.\n\nGeometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the [[Pythagorean theorem]] and [[parallelogram law]] hold in a Hilbert space. At a deeper level, perpendicular projection onto a subspace (the analog of \"[[altitude (triangle)|dropping the altitude]]\" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to a set of [[coordinate axes]] (an [[orthonormal basis]]), in analogy with Cartesian coordinates in the plane. When that set of axes is [[countably infinite]], the Hilbert space can also be usefully thought of in terms of the space of [[infinite sequence]]s that are [[Lp norm|square-summable]]. The latter space is often in the older literature referred to as ''the'' Hilbert space. [[Linear operator]]s on a Hilbert space are likewise fairly concrete objects: in good cases, they are simply transformations that stretch the space by different factors in mutually perpendicular directions in a sense that is made precise by the study of their [[Spectrum_(functional_analysis)|spectrum]].\n\n==Definition and illustration==\n\n===Motivating example: Euclidean space===\nOne of the most familiar examples of a Hilbert space is the [[Euclidean space]] consisting of three-dimensional [[Euclidean vector|vectors]], denoted by {{math|ℝ<sup>3</sup>}}, and equipped with the [[dot product]]. The dot product takes two vectors {{math|'''x'''}} and {{math|'''y'''}}, and produces a real number {{math|'''x''' · '''y'''}}. If {{math|'''x'''}} and {{math|'''y'''}} are represented in [[Cartesian coordinates]], then the dot product is defined by\n\n:<math>\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix} \\cdot \\begin{pmatrix}y_1\\\\y_2\\\\y_3\\end{pmatrix} = x_1y_1+x_2y_2+x_3y_3 \\,.</math>\n\nThe dot product satisfies the properties:\n# It is symmetric in {{math|'''x'''}} and {{math|'''y'''}}: {{math|'''x''' · '''y''' {{=}} '''y''' · '''x'''}}.\n# It is [[linear function|linear]] in its first argument: {{math|(''a'''''x'''<sub>1</sub> + ''b'''''x'''<sub>2</sub>) · '''y''' {{=}} ''a'''''x'''<sub>1</sub> · '''y''' + ''b'''''x'''<sub>2</sub> · '''y'''}} for any scalars {{mvar|a}}, {{mvar|b}}, and vectors {{math|'''x'''<sub>1</sub>}}, {{math|'''x'''<sub>2</sub>}}, and {{math|'''y'''}}.\n# It is [[Definite bilinear form|positive definite]]: for all vectors {{math|'''x'''}}, {{math|'''x''' · '''x''' ≥ 0 }}, with equality [[if and only if]] {{math| '''x''' {{=}} 0}}.\n\nAn operation on pairs of vectors that, like the dot product, satisfies these three properties is known as a (real) [[inner product]]. A [[vector space]] equipped with such an inner product is known as a (real) [[inner product space]]. Every finite-dimensional inner product space is also a Hilbert space. The basic feature of the dot product that connects it with Euclidean geometry is that it is related to both the length (or [[norm (mathematics)|norm]]) of a vector, denoted {{math|{{norm|'''x'''}}}}, and to the angle {{mvar|θ}} between two vectors {{math|'''x'''}} and {{math|'''y'''}} by means of the formula\n\n:<math>\\mathbf{x}\\cdot\\mathbf{y} = \\|\\mathbf{x}\\|\\,\\|\\mathbf{y}\\|\\,\\cos\\theta \\,.</math>\n\n[[File:Completeness in Hilbert space.png|thumb|right|Completeness means that if a particle moves along the broken path (in blue) travelling a finite total distance, then the particle has a [[Well defined|well-defined]] net displacement (in orange).]]\n[[Multivariable calculus]] in Euclidean space relies on the ability to compute [[limit (mathematics)|limits]], and to have useful criteria for concluding that limits exist. A [[series (mathematics)|mathematical series]]\n: <math>\\sum_{n=0}^\\infty \\mathbf{x}_n</math>\n\nconsisting of vectors in {{math|ℝ<sup>3</sup>}} is [[absolute convergence|absolutely convergent]] provided that the sum of the lengths converges as an ordinary series of real numbers:<ref>{{harvnb|Marsden|1974|loc=§2.8}}</ref>\n: <math>\\sum_{k=0}^\\infty \\|\\mathbf{x}_k\\| < \\infty \\,.</math>\n\nJust as with a series of scalars, a series of vectors that converges absolutely also converges to some limit vector {{math|'''L'''}} in the Euclidean space, in the sense that\n: <math>\\left\\|\\mathbf{L}-\\sum_{k=0}^N\\mathbf{x}_k\\right\\|\\to 0\\quad\\text{as }N\\to\\infty \\,.</math>\n\nThis property expresses the ''completeness'' of Euclidean space: that a series that converges absolutely also converges in the ordinary sense.\n\nHilbert spaces are often taken over the [[complex number]]s. The [[complex plane]] denoted by {{math|ℂ}} is equipped with a notion of magnitude, the [[absolute value|complex modulus]] {{math|{{abs|''z''}}}} which is defined as the square root of the product of {{mvar|z}} with its [[complex conjugate]]:\n: <math>|z|^2 = z\\overline{z} \\,.</math>\n\nIf {{math|''z'' {{=}} ''x'' + ''iy''}} is a decomposition of {{mvar|z}} into its real and imaginary parts, then the modulus is the usual Euclidean two-dimensional length:\n\n: <math>|z| = \\sqrt{x^2 + y^2} \\,.</math>\n\nThe inner product of a pair of complex numbers {{mvar|z}} and {{mvar|w}} is the product of {{mvar|z}} with the complex conjugate of {{mvar|w}}:\n: <math>\\langle z, w\\rangle = z\\overline{w}\\,.</math>\n\nThis is complex-valued. The real part of {{math|{{angbr|''z'', ''w''}}}} gives the usual two-dimensional Euclidean [[dot product]].\n\nA second example is the space {{math|ℂ<sup>2</sup>}} whose elements are pairs of complex numbers {{math|''z'' {{=}} (''z''<sub>1</sub>, ''z''<sub>2</sub>)}}. Then the inner product of {{mvar|z}} with another such vector {{math|''w'' {{=}} (''w''<sub>1</sub>, ''w''<sub>2</sub>)}} is given by\n:<math>\\langle z, w\\rangle = z_1\\overline{w}_1 + z_2\\overline{w}_2\\,.</math>\n\nThe real part of {{math|{{angbr|''z'', ''w''}}}} is then the four-dimensional Euclidean dot product. This inner product is ''Hermitian'' symmetric, which means that the result of interchanging {{mvar|z}} and {{mvar|w}} is the complex conjugate:\n:<math>\\langle w, z\\rangle = \\overline{\\langle z, w\\rangle}\\,.</math>\n\n===Definition===\nA '''Hilbert space''' {{math|''H''}} is a [[real number|real]] or [[complex number|complex]] [[inner product space]] that is also a [[complete metric space]] with respect to the distance function induced by the inner product.<ref name=\"General\">The mathematical material in this section can be found in any good textbook on functional analysis, such as {{Harvtxt|Dieudonné|1960}}, {{Harvtxt|Hewitt|Stromberg|1965}}, {{Harvtxt|Reed|Simon|1980}} or {{Harvtxt|Rudin|1980}}.</ref>\n\nTo say that {{math|''H''}} is a complex '''inner product space''' means that {{math|''H''}} is a complex vector space on which there is an inner product {{math|{{angbr|''x'', ''y''}}}} associating a complex number to each pair of elements {{math|''x'', ''y''}} of {{math|''H''}} that satisfies the following properties:\n# The inner product is conjugate symmetric; that is, the inner product of a pair of elements is equal to the [[complex conjugate]] of the inner product of the swapped elements:\n#: <math>\\langle y, x\\rangle = \\overline{\\langle x, y\\rangle}\\,.</math>\n# The inner product is [[linear functional|linear]] in its first<ref group=nb>In some conventions, inner products are linear in their second arguments instead.</ref> argument. For all complex numbers {{math|''a''}} and {{math|''b''}},\n#: <math>\\langle ax_1 + bx_2, y\\rangle = a\\langle x_1, y\\rangle + b\\langle x_2, y\\rangle\\,.</math>\n# The inner product of an element with itself is [[definite bilinear form|positive definite]]:\n#: <math>\\begin{cases}\n  \\langle x, x\\rangle > 0 &x \\neq 0 \\\\\n  \\langle x, x\\rangle = 0 &x = 0\\,.\n\\end{cases}</math>\n\nIt follows from properties 1 and 2 that a complex inner product is [[antilinear map|conjugate linear]] in its second argument, meaning that\n:<math>\\langle x, ay_1 + by_2\\rangle = \\bar{a}\\langle x, y_1\\rangle + \\bar{b}\\langle x, y_2\\rangle\\,.</math>\n\nA real inner product space is defined in the same way, except that {{math|''H''}} is a real vector space and the inner product takes real values. Such an inner product will be bilinear: that is, linear in each argument.\n\nThe [[norm (mathematics)|norm]] is the real-valued function\n:<math>\\|x\\| = \\sqrt{\\langle x, x \\rangle}\\,,</math>\n\nand the distance {{math|''d''}} between two points {{math|''x'', ''y''}} in {{math|''H''}} is defined in terms of the norm by\n:<math>d(x, y) = \\|x - y\\| = \\sqrt{\\langle x - y, x - y \\rangle}\\,.</math>\n\nThat this function is a distance function means firstly that it is symmetric in {{math|''x''}} and {{math|''y''}}, secondly that the distance between {{math|''x''}} and itself is zero, and otherwise the distance between {{math|''x''}} and {{math|''y''}} must be positive, and lastly that the [[triangle inequality]] holds, meaning that the length of one leg of a triangle {{math|''xyz''}} cannot exceed the sum of the lengths of the other two legs:\n:<math>d(x, z) \\le d(x, y) + d(y, z)\\,.</math>\n:[[File:Triangle inequality in a metric space.svg|300px|none]]\n\nThis last property is ultimately a consequence of the more fundamental [[Cauchy–Schwarz inequality]], which asserts\n:<math>\\bigl|\\langle x, y\\rangle\\bigr| \\le \\|x\\|\\,\\|y\\|</math>\n\nwith equality if and only if {{math|''x''}} and {{math|''y''}} are [[linear independence|linearly dependent]].\n\nWith a distance function defined in this way, any inner product space is a [[metric space]], and sometimes is known as a '''pre-Hilbert space'''.<ref>{{harvnb|Dieudonné|1960|loc=§6.2}}</ref> Any pre-Hilbert space that is additionally also a [[complete space|complete]] space is a Hilbert space.\n\nThe '''completeness''' of {{math|''H''}} is expressed using a form of the [[Cauchy criterion]] for sequences in {{math|''H''}}: a pre-Hilbert space {{math|''H''}} is complete if every [[Cauchy sequence]] [[limit (mathematics)|converges with respect to this norm]] to an element in the space. Completeness can be characterized by the following equivalent condition: if a series of vectors\n: <math>\\sum_{k=0}^\\infty u_k</math>\n\n[[absolute convergence|converges absolutely]] in the sense that\n:<math>\\sum_{k=0}^\\infty\\|u_k\\| < \\infty\\,,</math>\n\nthen the series converges in {{math|''H''}}, in the sense that the partial sums converge to an element of {{math|''H''}}.\n\nAs a complete normed space, Hilbert spaces are by definition also [[Banach space]]s. As such they are [[topological vector space]]s, in which [[topology|topological]] notions like the [[open set|openness]] and [[closed set|closedness]] of subsets are well defined. Of special importance is the notion of a closed [[linear subspace]] of a Hilbert space that, with the inner product induced by restriction, is also complete (being a closed set in a complete metric space) and therefore a Hilbert space in its own right.\n\n===Second example: sequence spaces===\nThe [[sequence space]] {{math|''l''<sup>2</sup>}} consists of all [[sequence (mathematics)|infinite sequences]] {{math|'''z''' {{=}} (''z''<sub>1</sub>, ''z''<sub>2</sub>, …)}} of complex numbers such that the [[series (mathematics)|series]]\n: <math>\\sum_{n=1}^\\infty |z_n|^2</math>\n\n[[convergent series|converges]]. The inner product on {{math|''l''<sup>2</sup>}} is defined by\n: <math>\\langle \\mathbf{z}, \\mathbf{w}\\rangle = \\sum_{n=1}^\\infty z_n\\overline{w_n}\\,,</math>\n\nwith the latter series converging as a consequence of the Cauchy–Schwarz inequality.\n\nCompleteness of the space holds provided that whenever a series of elements from {{math|''l''<sup>2</sup>}} converges absolutely (in norm), then it converges to an element of {{math|''l''<sup>2</sup>}}. The proof is basic in [[mathematical analysis]], and permits mathematical series of elements of the space to be manipulated with the same ease as series of complex numbers (or vectors in a finite-dimensional Euclidean space).<ref>{{harvnb|Dieudonné|1960}}</ref>\n\n==History==\n[[File:Hilbert.jpg|thumb|right|[[David Hilbert]]]]\nPrior to the development of Hilbert spaces, other generalizations of Euclidean spaces were known to [[mathematician]]s and physicists. In particular, the idea of an [[vector space|abstract linear space]] had gained some traction towards the end of the 19th century:<ref>Largely from the work of [[Hermann Grassmann]], at the urging of [[August Ferdinand Möbius]] {{harv|Boyer|Merzbach|1991|pp=584–586}}. The first modern axiomatic account of abstract vector spaces ultimately appeared in [[Giuseppe Peano]]'s 1888 account ({{harvnb|Grattan-Guinness|2000|loc=§5.2.2}}; {{harvnb|O'Connor|Robertson|1996}}).</ref> this is a space whose elements can be added together and multiplied by scalars (such as [[real numbers|real]] or [[complex numbers]]) without necessarily identifying these elements with [[vector (geometric)|\"geometric\" vectors]], such as position and momentum vectors in physical systems. Other objects studied by mathematicians at the turn of the 20th century, in particular spaces of [[sequence (mathematics)|sequences]] (including [[series (mathematics)|series]]) and spaces of functions,<ref>A detailed account of the history of Hilbert spaces can be found in {{harvnb|Bourbaki|1987}}.</ref> can naturally be thought of as linear spaces. Functions, for instance, can be added together or multiplied by constant scalars, and these operations obey the algebraic laws satisfied by addition and scalar multiplication of spatial vectors.\n\nIn the first decade of the 20th century, parallel developments led to the introduction of Hilbert spaces. The first of these was the observation, which arose during [[David Hilbert]] and [[Erhard Schmidt]]'s study of [[integral equations]],<ref>{{harvnb|Schmidt|1908}}</ref> that two [[square-integrable]] real-valued functions {{mvar|f}} and {{mvar|g}} on an interval {{math|[''a'', ''b'']}} have an ''inner product''\n\n: <math>\\langle f, g \\rangle = \\int_a^b f(x)g(x)\\, \\mathrm{d}x</math>\n\nwhich has many of the familiar properties of the Euclidean dot product. In particular, the idea of an [[orthogonality|orthogonal]] family of functions has meaning. Schmidt exploited the similarity of this inner product with the usual dot product to prove an analog of the [[spectral decomposition]] for an operator of the form\n\n: <math>f(x) \\mapsto \\int_a^b K(x, y) f(y)\\, \\mathrm{d}y</math>\n\nwhere {{mvar|K}} is a continuous function symmetric in {{mvar|x}} and {{mvar|y}}. The resulting [[eigenfunction expansion]] expresses the function {{mvar|K}} as a series of the form\n\n: <math>K(x, y) = \\sum_n \\lambda_n\\varphi_n(x)\\varphi_n(y)</math>\n\nwhere the functions {{mvar|φ<sub>n</sub>}} are orthogonal in the sense that {{math|⟨''φ''<sub>''n''</sub>, ''φ''<sub>''m''</sub>⟩ {{=}} 0}} for all {{math|''n'' ≠ ''m''}}. The individual terms in this series are sometimes referred to as elementary product solutions. However, there are eigenfunction expansions that fail to converge in a suitable sense to a square-integrable function: the missing ingredient, which ensures convergence, is completeness.<ref>{{harvnb|Titchmarsh|1946|loc=§IX.1}}</ref>\n\nThe second development was the [[Lebesgue integral]], an alternative to the [[Riemann integral]] introduced by [[Henri Lebesgue]] in 1904.<ref>{{harvnb|Lebesgue|1904}}. Further details on the history of integration theory can be found in {{harvtxt|Bourbaki|1987}} and {{harvtxt|Saks|2005}}.</ref> The Lebesgue integral made it possible to integrate a much broader class of functions. In 1907, [[Frigyes Riesz]] and [[Ernst Sigismund Fischer]] independently proved that the space {{math|''L''<sup>2</sup>}} of square Lebesgue-integrable functions is a [[complete metric space]].<ref>{{harvnb|Bourbaki|1987}}.</ref> As a consequence of the interplay between geometry and completeness, the 19th century results of [[Joseph Fourier]], [[Friedrich Bessel]] and [[Marc-Antoine Parseval]] on [[trigonometric series]] easily carried over to these more general spaces, resulting in a geometrical and analytical apparatus now usually known as the [[Riesz–Fischer theorem]].<ref>{{harvnb|Dunford|Schwartz|1958|loc=§IV.16}}</ref>\n\nFurther basic results were proved in the early 20th century. For example, the [[Riesz representation theorem]] was independently established by [[Maurice Fréchet]] and [[Frigyes Riesz]] in 1907.<ref>In {{harvtxt|Dunford|Schwartz|1958|loc=§IV.16}}, the result that every linear functional on {{math|''L''<sup>2</sup>[0,1]}} is represented by integration is jointly attributed to {{harvtxt|Fréchet|1907}} and {{harvtxt|Riesz|1907}}. The general result, that the dual of a Hilbert space is identified with the Hilbert space itself, can be found in {{harvtxt|Riesz|1934}}.</ref> [[John von Neumann]] coined the term ''abstract Hilbert space'' in his work on unbounded [[Self-adjoint operator|Hermitian operators]].<ref>{{Harvnb|von Neumann|1929}}.</ref> Although other mathematicians such as [[Hermann Weyl]] and [[Norbert Wiener]] had already studied particular Hilbert spaces in great detail, often from a physically motivated point of view, von Neumann gave the first complete and axiomatic treatment of them.<ref>{{harvnb|Kline|1972|p=1092}}</ref> Von Neumann later used them in his seminal work on the foundations of quantum mechanics,<ref>{{Harvnb|Hilbert|Nordheim|von Neumann|1927}}</ref> and in his continued work with [[Eugene Wigner]]. The name \"Hilbert space\" was soon adopted by others, for example by Hermann Weyl in his book on quantum mechanics and the theory of groups.<ref name=\"Weyl31\">{{Harvnb|Weyl|1931}}.</ref>\n\nThe significance of the concept of a Hilbert space was underlined with the realization that it offers one of the best [[mathematical formulation of quantum mechanics|mathematical formulations of quantum mechanics]].<ref>{{harvnb|Prugovečki|1981|pp=1–10}}.</ref> In short, the states of a quantum mechanical system are vectors in a certain Hilbert space, the observables are [[hermitian operator]]s on that space, the [[symmetry|symmetries]] of the system are [[unitary operator]]s, and [[quantum measurement|measurements]] are [[orthogonal projection]]s. The relation between quantum mechanical symmetries and unitary operators provided an impetus for the development of the [[unitary representation|unitary]] [[representation theory]] of [[group (mathematics)|groups]], initiated in the 1928 work of Hermann Weyl.<ref name=\"Weyl31\" /> On the other hand, in the early 1930s it became clear that classical mechanics can be described in terms of Hilbert space ([[Koopman–von Neumann classical mechanics]]) and that certain properties of classical [[dynamical systems]] can be analyzed using Hilbert space techniques in the framework of [[ergodic theory]].<ref name=\"von Neumann 1932\">{{harvnb|von Neumann|1932}}</ref>\n\nThe algebra of [[observable]]s in quantum mechanics is naturally an algebra of operators defined on a Hilbert space, according to [[Werner Heisenberg]]'s [[matrix mechanics]] formulation of quantum theory. Von Neumann began investigating [[operator algebra]]s in the 1930s, as [[ring (mathematics)|rings]] of operators on a Hilbert space. The kind of algebras studied by von Neumann and his contemporaries are now known as [[von Neumann algebra]]s. In the 1940s, [[Israel Gelfand]], [[Mark Naimark]] and [[Irving Segal]] gave a definition of a kind of operator algebras called [[C*-algebra]]s that on the one hand made no reference to an underlying Hilbert space, and on the other extrapolated many of the useful features of the operator algebras that had previously been studied. The spectral theorem for self-adjoint operators in particular that underlies much of the existing Hilbert space theory was generalized to C*-algebras. These techniques are now basic in abstract harmonic analysis and representation theory.\n\n==Examples==\n\n===Lebesgue spaces===\n{{Main|Lp space|l1={{mvar|L<sup>p</sup>}} space}}\n\nLebesgue spaces are [[function space]]s associated to [[measure (mathematics)|measure spaces]] {{math|(''X'', ''M'', ''μ'')}}, where {{math|''X''}} is a set, {{math|''M''}} is a [[Sigma-algebra|σ-algebra]] of subsets of {{math|''X''}}, and {{math|''μ''}} is a [[countably additive measure]] on {{math|''M''}}. Let {{math|''L''<sup>2</sup>(''X'', ''μ'')}} be the space of those complex-valued measurable functions on {{math|''X''}} for which the [[Lebesgue integration|Lebesgue integral]] of the square of the [[absolute value]] of the function is finite, i.e., for a function {{math|''f''}} in {{math|''L''<sup>2</sup>(''X'', ''μ'')}},\n\n: <math> \\int_X |f|^2 \\mathrm{d} \\mu < \\infty \\,, </math>\n\nand where functions are identified if and only if they differ only on a [[null set|set of measure zero]].\n\nThe inner product of functions {{math|''f''}} and {{math|''g''}} in {{math|''L''<sup>2</sup>(''X'', ''μ'')}} is then defined as\n:<math>\\langle f, g\\rangle = \\int_X f(t) \\overline{g(t)} \\, \\mathrm{d} \\mu(t) \\,.</math>\n\nFor {{math|''f''}} and {{math|''g''}} in {{math|''L''<sup>2</sup>}}, this integral exists because of the Cauchy–Schwarz inequality, and defines an inner product on the space. Equipped with this inner product, {{math|''L''<sup>2</sup>}} is in fact complete.<ref>{{Harvnb|Halmos|1957|loc=Section 42}}.</ref> The Lebesgue integral is essential to ensure completeness: on domains of real numbers, for instance, not enough functions are [[Riemann integral|Riemann integrable]].<ref>{{Harvnb|Hewitt|Stromberg|1965}}.</ref>\n\nThe Lebesgue spaces appear in many natural settings. The spaces {{math|''L''<sup>2</sup>('''ℝ''')}} and {{math|''L''<sup>2</sup>([0,1])}} of square-integrable functions with respect to the [[Lebesgue measure]] on the real line and unit interval, respectively, are natural domains on which to define the Fourier transform and Fourier series. In other situations, the measure may be something other than the ordinary Lebesgue measure on the real line. For instance, if {{math|''w''}} is any positive measurable function, the space of all measurable functions {{math|''f''}} on the interval {{math|[0, 1]}} satisfying\n: <math>\\int_0^1 \\bigl|f(t)\\bigr|^2w(t)\\, \\mathrm{d}t < \\infty</math>\n\nis called the [[Lp space#Weighted Lp spaces|weighted {{math|''L''<sup>2</sup>}} space]] {{math|''L''{{su|p=2|b=''w''}}([0, 1])}}, and {{math|''w''}} is called the weight function. The inner product is defined by\n:<math>\\langle f, g\\rangle = \\int_0^1 f(t) \\overline{g(t)} w(t) \\, \\mathrm{d}t \\,.</math>\n\nThe weighted space {{math|''L''{{su|p=2|b=''w''}}([0, 1])}} is identical with the Hilbert space {{math|''L''<sup>2</sup>([0, 1], ''μ'')}} where the measure {{math|''μ''}} of a Lebesgue-measurable set {{math|''A''}} is defined by\n:<math>\\mu(A) = \\int_A w(t)\\,\\mathrm{d}t \\,.</math>\n\nWeighted {{math|''L''<sup>2</sup>}} spaces like this are frequently used to study orthogonal polynomials, because different families of orthogonal polynomials are orthogonal with respect to different weighting functions.\n\n===Sobolev spaces===\n[[Sobolev space]]s, denoted by {{math|''H''<sup>''s''</sup>}} or {{math|''W''<sup>''s'', 2</sup>}}, are Hilbert spaces. These are a special kind of [[function space]] in which [[derivative|differentiation]] may be performed, but that (unlike other [[Banach spaces]] such as the [[Hölder space]]s) support the structure of an inner product. Because differentiation is permitted, Sobolev spaces are a convenient setting for the theory of [[partial differential equations]].<ref name=\"BeJoSc81\" /> They also form the basis of the theory of [[Direct method in calculus of variations|direct methods in the calculus of variations]].<ref>{{Harvnb|Giusti|2003}}.<!--Find a reference more specific to the case p=2--></ref>\n\nFor {{math|''s''}} a non-negative integer and {{math|''Ω'' ⊂ '''ℝ'''<sup>''n''</sup>}}, the Sobolev space {{math|''H''<sup>''s''</sup>(''Ω'')}} contains {{math|''L''<sup>2</sup>}} functions whose [[weak derivative]]s of order up to {{math|''s''}} are also {{math|''L''<sup>2</sup>}}. The inner product in {{math|''H''<sup>''s''</sup>(''Ω'')}} is\n\n: <math>\\langle f, g\\rangle = \\int_\\Omega f(x)\\bar{g}(x)\\,\\mathrm{d}x + \\int_\\Omega D f(x)\\cdot D\\bar{g}(x)\\,\\mathrm{d}x + \\cdots + \\int_\\Omega D^s f(x)\\cdot D^s \\bar{g}(x)\\, \\mathrm{d}x</math>\n\nwhere the dot indicates the dot product in the Euclidean space of partial derivatives of each order. Sobolev spaces can also be defined when {{math|''s''}} is not an integer.\n\nSobolev spaces are also studied from the point of view of spectral theory, relying more specifically on the Hilbert space structure. If {{math|''Ω''}} is a suitable domain, then one can define the Sobolev space {{math|''H''<sup>''s''</sup>(''Ω'')}} as the space of [[Bessel potential]]s;<ref>{{harvnb|Stein|1970}}</ref> roughly,\n: <math>H^s(\\Omega) = \\left. \\left\\{ (1-\\Delta)^{-\\frac{s}{2}}f \\,\\right|\\, f\\in L^2(\\Omega)\\right\\} \\,.</math>\n\nHere {{math|Δ}} is the Laplacian and {{math|(1 − Δ)<sup>−{{sfrac|''s''|2}}</sup>}} is understood in terms of the [[spectral mapping theorem]]. Apart from providing a workable definition of Sobolev spaces for non-integer {{math|''s''}}, this definition also has particularly desirable properties under the [[Fourier transform]] that make it ideal for the study of [[pseudodifferential operator]]s. Using these methods on a [[compact space|compact]] [[Riemannian manifold]], one can obtain for instance the [[Hodge decomposition]], which is the basis of [[Hodge theory]].<ref>Details can be found in {{harvtxt|Warner|1983}}.</ref>\n\n===Spaces of holomorphic functions===\n====Hardy spaces====\nThe [[Hardy space]]s are function spaces, arising in [[complex analysis]] and [[harmonic analysis]], whose elements are certain [[holomorphic function]]s in a complex domain.<ref>A general reference on Hardy spaces is the book {{harvtxt|Duren|1970}}.</ref> Let {{math|''U''}} denote the [[unit disc]] in the complex plane. Then the Hardy space {{math|''H''<sup>2</sup>(''U'')}} is defined as the space of holomorphic functions {{math|''f''}} on {{math|''U''}} such that the means\n\n: <math>M_r(f) = \\frac{1}{2\\pi}\\int_0^{2\\pi}\\left|f\\left(re^{i\\theta}\\right)\\right|^2\\,\\mathrm{d}\\theta</math>\n\nremain bounded for {{math|''r'' < 1}}. The norm on this Hardy space is defined by\n\n: <math>\\left\\|f\\right\\|_2 = \\lim_{r \\to 1} \\sqrt{M_r(f)} \\,.</math>\n\nHardy spaces in the disc are related to Fourier series. A function {{math|''f''}} is in {{math|''H''<sup>2</sup>(''U'')}} if and only if\n\n: <math>f(z) = \\sum_{n=0}^\\infty a_n z^n</math>\n\nwhere\n\n: <math>\\sum_{n=0}^\\infty |a_n|^2 < \\infty \\,.</math>\n\nThus {{math|''H''<sup>2</sup>(''U'')}} consists of those functions that are L<sup>2</sup> on the circle, and whose negative frequency Fourier coefficients vanish.\n\n====Bergman spaces====\nThe [[Bergman space]]s are another family of Hilbert spaces of holomorphic functions.<ref>{{harvnb|Krantz|2002|loc=§1.4}}</ref> Let {{math|''D''}} be a bounded open set in the [[complex plane]] (or a higher-dimensional complex space) and let {{math|''L''<sup>2, ''h''</sup>(''D'')}} be the space of holomorphic functions {{math|''f''}} in {{math|''D''}} that are also in {{math|''L''<sup>2</sup>(''D'')}} in the sense that\n:<math>\\|f\\|^2 = \\int_D |f(z)|^2\\,\\mathrm{d}\\mu(z) < \\infty \\,,</math>\n\nwhere the integral is taken with respect to the Lebesgue measure in {{math|''D''}}. Clearly {{math|''L''<sup>2, ''h''</sup>(''D'')}} is a subspace of {{math|''L''<sup>2</sup>(''D'')}}; in fact, it is a [[closed set|closed]] subspace, and so a Hilbert space in its own right. This is a consequence of the estimate, valid on [[compact space|compact]] subsets {{math|''K''}} of {{math|''D''}}, that\n:<math>\\sup_{z\\in K} \\left|f(z)\\right| \\le C_K \\left\\|f\\right\\|_2 \\,,</math>\nwhich in turn follows from [[Cauchy's integral formula]]. Thus convergence of a sequence of holomorphic functions in {{math|''L''<sup>2</sup>(''D'')}} implies also [[compact convergence]], and so the limit function is also holomorphic. Another consequence of this inequality is that the linear functional that evaluates a function {{math|''f''}} at a point of {{math|''D''}} is actually continuous on {{math|''L''<sup>2, ''h''</sup>(''D'')}}. The Riesz representation theorem implies that the evaluation functional can be represented as an element of {{math|''L''<sup>2, ''h''</sup>(''D'')}}. Thus, for every {{math|''z'' ∈ ''D''}}, there is a function {{math|''η''<sub>''z''</sub> ∈ ''L''<sup>2, ''h''</sup>(''D'')}} such that\n: <math>f(z) = \\int_D f(\\zeta)\\overline{\\eta_z(\\zeta)}\\,\\mathrm{d}\\mu(\\zeta)</math>\n\nfor all {{math|''f'' ∈ ''L''<sup>2, ''h''</sup>(''D'')}}. The integrand\n: <math>K(\\zeta, z) = \\overline{\\eta_z(\\zeta)}</math>\n\nis known as the [[Bergman kernel]] of {{math|''D''}}. This [[integral kernel]] satisfies a reproducing property\n: <math>f(z) = \\int_D f(\\zeta)K(\\zeta, z)\\,\\mathrm{d}\\mu(\\zeta) \\,.</math>\n\nA Bergman space is an example of a [[reproducing kernel Hilbert space]], which is a Hilbert space of functions along with a kernel {{math|''K''(''ζ'', ''z'')}} that verifies a reproducing property analogous to this one. The Hardy space {{math|''H''<sup>2</sup>(''D'')}} also admits a reproducing kernel, known as the [[Szegő kernel]].<ref>{{harvnb|Krantz|2002|loc=§1.5}}</ref> Reproducing kernels are common in other areas of mathematics as well. For instance, in [[harmonic analysis]] the [[Poisson kernel]] is a reproducing kernel for the Hilbert space of square-integrable [[harmonic function]]s in the [[unit ball]]. That the latter is a Hilbert space at all is a consequence of the mean value theorem for harmonic functions.\n\n==Applications==\nMany of the applications of Hilbert spaces exploit the fact that Hilbert spaces support generalizations of simple geometric concepts like [[projection operator|projection]] and [[change of basis]] from their usual finite dimensional setting. In particular, the [[spectral theory]] of [[continuous function|continuous]] [[self-adjoint operator|self-adjoint]] [[linear operator]]s on a Hilbert space generalizes the usual [[spectral decomposition]] of a [[matrix (mathematics)|matrix]], and this often plays a major role in applications of the theory to other areas of mathematics and physics.\n\n===Sturm–Liouville theory===\n{{Main|Sturm–Liouville theory|Spectral theory of ordinary differential equations}}\n[[File:Harmonic partials on strings.svg|right|thumb|The [[overtone]]s of a vibrating string. These are [[eigenfunction]]s of an associated Sturm–Liouville problem. The eigenvalues 1, {{sfrac|1|2}}, {{sfrac|1|3}}, … form the (musical) [[harmonic series (music)|harmonic series]].]]\nIn the theory of [[ordinary differential equation]]s, spectral methods on a suitable Hilbert space are used to study the behavior of eigenvalues and eigenfunctions of differential equations. For example, the [[Sturm–Liouville theory|Sturm–Liouville problem]] arises in the study of the harmonics of waves in a violin string or a drum, and is a central problem in [[ordinary differential equations]].<ref>{{harvnb|Young|1988|loc=Chapter 9}}.</ref> The problem is a differential equation of the form\n: <math> -\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left[p(x)\\frac{\\mathrm{d}y}{\\mathrm{d}x}\\right] + q(x)y = \\lambda w(x)y</math>\n\nfor an unknown function {{math|''y''}} on an interval {{math|[''a'', ''b'']}}, satisfying general homogeneous [[Robin boundary conditions]]\n: <math>\\begin{cases}\n  \\alpha y(a)+\\alpha' y'(a) &= 0 \\\\\n  \\beta y(b) + \\beta' y'(b) &= 0 \\,.\n\\end{cases}</math>\nThe functions {{math|''p''}}, {{math|''q''}}, and {{math|''w''}} are given in advance, and the problem is to find the function {{math|''y''}} and constants {{math|''λ''}} for which the equation has a solution. The problem only has solutions for certain values of {{math|''λ''}}, called eigenvalues of the system, and this is a consequence of the spectral theorem for [[compact operator]]s applied to the [[integral operator]] defined by the [[Green's function]] for the system. Furthermore, another consequence of this general result is that the eigenvalues {{math|''λ''}} of the system can be arranged in an increasing sequence tending to infinity.<ref group=\"nb\">The eigenvalues of the Fredholm kernel are {{math|{{sfrac|1|''λ''}}}}, which tend to zero.</ref>\n\n===Partial differential equations===\nHilbert spaces form a basic tool in the study of [[partial differential equations]].<ref name=\"BeJoSc81\">{{harvnb|Bers|John|Schechter|1981}}.</ref> For many classes of partial differential equations, such as linear [[elliptic partial differential equation|elliptic equations]], it is possible to consider a generalized solution (known as a [[weak derivative|weak]] solution) by enlarging the class of functions. Many weak formulations involve the class of [[Sobolev space|Sobolev functions]], which is a Hilbert space. A suitable weak formulation reduces to a geometrical problem the analytic problem of finding a solution or, often what is more important, showing that a solution exists and is unique for given boundary data. For linear elliptic equations, one geometrical result that ensures unique solvability for a large class of problems is the [[Lax–Milgram theorem]]. This strategy forms the rudiment of the [[Galerkin method]] (a [[finite element method]]) for numerical solution of partial differential equations.<ref>More detail on finite element methods from this point of view can be found in {{harvtxt|Brenner|Scott|2005}}.</ref>\n\nA typical example is the [[Poisson equation]] {{math|−Δ''u'' {{=}} ''g''}} with [[Dirichlet boundary conditions]] in a bounded domain {{math|''Ω''}} in {{math|'''ℝ'''<sup>2</sup>}}. The weak formulation consists of finding a function {{math|''u''}} such that, for all continuously differentiable functions {{math|''v''}} in {{math|''Ω''}} vanishing on the boundary:\n: <math>\\int_\\Omega \\nabla u\\cdot\\nabla v = \\int_\\Omega gv\\,.</math>\n\nThis can be recast in terms of the Hilbert space {{math|''H''{{su|p=1|b=0}}(''Ω'')}} consisting of functions {{math|''u''}} such that {{math|''u''}}, along with its weak partial derivatives, are square integrable on {{math|''Ω''}}, and vanish on the boundary. The question then reduces to finding {{math|''u''}} in this space such that for all {{math|''v''}} in this space\n: <math>a(u, v) = b(v)</math>\n\nwhere {{math|''a''}} is a continuous [[bilinear form]], and {{math|''b''}} is a continuous [[linear functional]], given respectively by\n:<math>a(u, v) = \\int_\\Omega \\nabla u\\cdot\\nabla v,\\quad b(v)= \\int_\\Omega gv\\,.</math>\n\nSince the Poisson equation is [[elliptic partial differential equation|elliptic]], it follows from Poincaré's inequality that the bilinear form {{math|''a''}} is [[coercive function|coercive]]. The Lax–Milgram theorem then ensures the existence and uniqueness of solutions of this equation.\n\nHilbert spaces allow for many elliptic partial differential equations to be formulated in a similar way, and the Lax–Milgram theorem is then a basic tool in their analysis. With suitable modifications, similar techniques can be applied to [[parabolic partial differential equation]]s and certain [[hyperbolic partial differential equation]]s.\n\n===Ergodic theory===\n[[File:BunimovichStadium.svg|thumb|right|The path of a [[dynamical billiards|billiard]] ball in the [[Bunimovich stadium]] is described by an ergodic [[dynamical system]].]]\nThe field of [[ergodic theory]] is the study of the long-term behavior of [[chaos theory|chaotic]] [[dynamical system]]s. The protypical case of a field that ergodic theory applies to is [[thermodynamics]], in which—though the microscopic state of a system is extremely complicated (it is impossible to understand the ensemble of individual collisions between particles of matter)—the average behavior over sufficiently long time intervals is tractable. The [[laws of thermodynamics]] are assertions about such average behavior. In particular, one formulation of the [[zeroth law of thermodynamics]] asserts that over sufficiently long timescales, the only functionally independent measurement that one can make of a thermodynamic system in equilibrium is its total energy, in the form of [[temperature]].\n\nAn ergodic dynamical system is one for which, apart from the energy—measured by the [[Hamiltonian (quantum mechanics)|Hamiltonian]]—there are no other functionally independent [[conserved quantities]] on the [[phase space]]. More explicitly, suppose that the energy {{math|''E''}} is fixed, and let {{math|''Ω''<sub>''E''</sub>}} be the subset of the phase space consisting of all states of energy {{math|''E''}} (an energy surface), and let {{math|''T''<sub>''t''</sub>}} denote the evolution operator on the phase space. The dynamical system is ergodic if there are no continuous non-constant functions on {{math|''Ω''<sub>''E''</sub>}} such that\n: <math>f(T_tw) = f(w)</math>\n\nfor all {{math|''w''}} on {{math|''Ω''<sub>''E''</sub>}} and all time {{math|''t''}}. [[Liouville's theorem (Hamiltonian)|Liouville's theorem]] implies that there exists a [[measure theory|measure]] {{math|''μ''}} on the energy surface that is invariant under the [[time translation]]. As a result, time translation is a [[unitary transformation]] of the Hilbert space {{math|''L''<sup>2</sup>(''Ω''<sub>''E''</sub>, ''μ'')}} consisting of square-integrable functions on the energy surface {{math|''Ω''<sub>''E''</sub>}} with respect to the inner product\n: <math>\\left\\langle f, g\\right\\rangle_{L^2\\left(\\Omega_E, \\mu\\right)} = \\int_E f\\bar{g}\\,\\mathrm{d}\\mu\\,.</math>\n\nThe von Neumann mean ergodic theorem<ref name=\"von Neumann 1932\"/> states the following:\n* If {{math|''U''<sub>''t''</sub>}} is a (strongly continuous) one-parameter semigroup of unitary operators on a Hilbert space {{math|''H''}}, and {{math|''P''}} is the orthogonal projection onto the space of common fixed points of {{math|''U''<sub>''t''</sub>}}, {{math|{''x''  ∈''H'' {{!}} ''U''<sub>''t''</sub>''x'' {{=}} ''x'', ∀''t''&nbsp;>&nbsp;0<nowiki>}</nowiki>}}, then\n*: <math>Px = \\lim_{T\\to\\infty}\\frac{1}{T}\\int_0^T U_tx\\,\\mathrm{d}t\\,.</math>\n\nFor an ergodic system, the fixed set of the time evolution consists only of the constant functions, so the ergodic theorem implies the following:<ref>{{harvnb|Reed|Simon|1980}}</ref> for any function {{math|''f'' ∈ ''L''<sup>2</sup>(''Ω''<sub>''E''</sub>, ''μ'')}},\n: <math>\\underset{T\\to\\infty}{L^2 - \\lim} \\frac{1}{T}\\int_0^T f(T_tw)\\,\\mathrm{d}t = \\int_{\\Omega_E} f(y)\\,\\mathrm{d}\\mu(y)\\,.</math>\n\nThat is, the long time average of an observable {{math|''f''}} is equal to its expectation value over an energy surface.\n\n===Fourier analysis===\n[[File:Sawtooth Fourier Analysys.svg|thumb|right|Superposition of sinusoidal wave basis functions (bottom) to form a sawtooth wave (top)]]\n[[File:Harmoniki.png|thumb|right|[[Spherical harmonics]], an orthonormal basis for the Hilbert space of square-integrable functions on the sphere, shown graphed along the radial direction]]\nOne of the basic goals of [[Fourier analysis]] is to decompose a function into a (possibly infinite) [[linear combination]] of given basis functions: the associated [[Fourier series]]. The classical Fourier series associated to a function {{math|''f''}} defined on the interval {{math|[0, 1]}} is a series of the form\n: <math>\\sum_{n=-\\infty}^\\infty a_n e^{2\\pi in\\theta}</math>\n\nwhere\n: <math>a_n = \\int_0^1f(\\theta)e^{-2\\pi in\\theta}\\,\\mathrm{d}\\theta\\,.</math>\n\nThe example of adding up the first few terms in a Fourier series for a sawtooth function is shown in the figure. The basis functions are sine waves with wavelengths {{math|{{sfrac|''λ''|''n''}}}} (for integer {{math|''n''}}) shorter than the wavelength {{math|''λ''}} of the sawtooth itself (except for {{math|''n'' {{=}} 1}}, the ''fundamental'' wave). All basis functions have nodes at the nodes of the sawtooth, but all but the fundamental have additional nodes. The oscillation of the summed terms about the sawtooth is called the [[Gibbs phenomenon]].\n\nA significant problem in classical Fourier series asks in what sense the Fourier series converges, if at all, to the function {{math|''f''}}. Hilbert space methods provide one possible answer to this question.<ref>A treatment of Fourier series from this point of view is available, for instance, in {{harvtxt|Rudin|1987}} or {{harvtxt|Folland|2009}}.</ref> The functions {{math|''e<sub>n</sub>''(''θ'') {{=}} ''e''<sup>2π''inθ''</sup>}} form an orthogonal basis of the Hilbert space {{math|''L''<sup>2</sup>([0, 1])}}. Consequently, any square-integrable function can be expressed as a series\n:<math>f(\\theta) = \\sum_n a_n e_n(\\theta)\\,,\\quad a_n = \\langle f, e_n\\rangle</math>\n\nand, moreover, this series converges in the Hilbert space sense (that is, in the [[mean convergence|{{math|''L''<sup>2</sup>}} mean]]).\n\nThe problem can also be studied from the abstract point of view: every Hilbert space has an [[orthonormal basis]], and every element of the Hilbert space can be written in a unique way as a sum of multiples of these basis elements. The coefficients appearing on these basis elements are sometimes known abstractly as the Fourier coefficients of the element of the space.<ref>{{harvnb|Halmos|1957|loc=§5}}</ref> The abstraction is especially useful when it is more natural to use different basis functions for a space such as {{math|''L''<sup>2</sup>([0, 1])}}. In many circumstances, it is desirable not to decompose a function into trigonometric functions, but rather into [[orthogonal polynomials]] or [[wavelet]]s for instance,<ref>{{harvnb|Bachman|Narici|Beckenstein|2000}}</ref> and in higher dimensions into [[spherical harmonics]].<ref>{{harvnb|Stein|Weiss|1971|loc=§IV.2}}.</ref>\n\nFor instance, if {{math|''e''<sub>''n''</sub>}} are any orthonormal basis functions of {{math|''L''<sup>2</sup>[0, 1]}}, then a given function in {{math|''L''<sup>2</sup>[0, 1]}} can be approximated as a finite linear combination<ref>{{harvnb|Lancos|1988|pp=212–213}}</ref>\n: <math>f(x) \\approx f_n (x) = a_1 e_1 (x) + a_2 e_2(x) + \\cdots + a_n e_n (x)\\,.</math>\n\nThe coefficients {{math|{''a''<sub>''j''</sub><nowiki>}</nowiki>}} are selected to make the magnitude of the difference {{math|{{norm|''f'' − ''f''<sub>''n''</sub>}}<sup>2</sup>}} as small as possible. Geometrically, the [[#Best approximation|best approximation]] is the [[#Orthogonal complements and projections|orthogonal projection]] of {{math|''f''}} onto the subspace consisting of all linear combinations of the {{math|{''e''<sub>''j''</sub><nowiki>}</nowiki>}}, and can be calculated by<ref>{{harvnb|Lanczos|1988|loc=Equation 4-3.10}}</ref>\n: <math>a_j = \\int_0^1 \\overline{e_j(x)}f (x) \\, \\mathrm{d}x\\,.</math>\n\nThat this formula minimizes the difference {{math|{{norm|''f'' − ''f''<sub>''n''</sub>}}<sup>2</sup>}} is a consequence of [[#Bessel's inequality and Parseval's formula|Bessel's inequality and Parseval's formula]].\n\nIn various applications to physical problems, a function can be decomposed into physically meaningful [[eigenfunction]]s of a [[differential operator]] (typically the [[Laplace operator]]): this forms the foundation for the spectral study of functions, in reference to the [[spectral theorem|spectrum]] of the differential operator.<ref>The classic reference for spectral methods is {{harvnb|Courant|Hilbert|1953}}. A more up-to-date account is {{harvnb|Reed|Simon|1975}}.</ref> A concrete physical application involves the problem of [[hearing the shape of a drum]]: given the fundamental modes of vibration that a drumhead is capable of producing, can one infer the shape of the drum itself?<ref>{{harvnb|Kac|1966}}</ref> The mathematical formulation of this question involves the [[Dirichlet eigenvalue]]s of the Laplace equation in the plane, that represent the fundamental modes of vibration in direct analogy with the integers that represent the fundamental modes of vibration of the violin string.\n\n[[Spectral theory]] also underlies certain aspects of the [[Fourier transform]] of a function. Whereas Fourier analysis decomposes a function defined on a [[compact set]] into the discrete spectrum of the Laplacian (which corresponds to the vibrations of a violin string or drum), the Fourier transform of a function is the decomposition of a function defined on all of Euclidean space into its components in the [[continuous spectrum]] of the Laplacian. The Fourier transformation is also geometrical, in a sense made precise by the [[Plancherel theorem]], that asserts that it is an [[isometry]] of one Hilbert space (the \"time domain\") with another (the \"frequency domain\"). This isometry property of the Fourier transformation is a recurring theme in abstract [[harmonic analysis]], as evidenced for instance by the [[Plancherel theorem for spherical functions]] occurring in [[noncommutative harmonic analysis]].\n\n===Quantum mechanics===\n[[File:HAtomOrbitals.png|right|thumb|The [[atomic orbital|orbitals]] of an [[electron]] in a [[hydrogen atom]] are [[eigenfunction]]s of the [[energy (physics)|energy]].]]\nIn the mathematically rigorous formulation of [[quantum mechanics]], developed by [[John von Neumann]],<ref>{{harvnb|von Neumann|1955}}</ref> the possible states (more precisely, the [[pure state]]s) of a quantum mechanical system are represented by [[unit vector]]s (called ''state vectors'') residing in a complex separable Hilbert space, known as the [[State space (physics)|state space]], well defined up to a complex number of norm 1 (the [[phase factor]]). In other words, the possible states are points in the [[projective space|projectivization]] of a Hilbert space, usually called the [[complex projective space]]. The exact nature of this Hilbert space is dependent on the system; for example, the position and momentum states for a single non-relativistic spin zero particle is the space of all [[square-integrable]] functions, while the states for the spin of a single proton are unit elements of the two-dimensional complex Hilbert space of [[spinors in three dimensions|spinors]]. Each observable is represented by a [[self-adjoint operator|self-adjoint]] [[linear operator]] acting on the state space. Each eigenstate of an observable corresponds to an [[eigenvector]] of the operator, and the associated [[eigenvalue]] corresponds to the value of the observable in that eigenstate.\n\nThe inner product between two state vectors is a complex number known as a [[probability amplitude]]. During an ideal measurement of a quantum mechanical system, the probability that a system collapses from a given initial state to a particular eigenstate is given by the square of the [[absolute value]] of the probability amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.\n\nFor a general system, states are typically not pure, but instead are represented as statistical mixtures of pure states, or mixed states, given by [[density matrix|density matrices]]: self-adjoint operators of [[trace of a matrix|trace]] one on a Hilbert space. Moreover, for general quantum mechanical systems, the effects of a single measurement can influence other parts of a system in a manner that is described instead by a [[positive operator valued measure]]. Thus the structure both of the states and observables in the general theory is considerably more complicated than the idealization for pure states.\n\n===Color perception===\n{{Main|Color vision#Mathematics of color perception}}\nAny true physical color can be represented by a combination of pure [[spectral color]]s. As physical colors can be composed of any number of spectral colors, the space of physical colors may aptly be represented by a Hilbert space over spectral colors. Humans have [[Trichromacy|three types of cone cells]] for color perception, so the perceivable colors can be represented by 3-dimensional Euclidean space. The many-to-one linear mapping from the Hilbert space of physical colors to the Euclidean space of human perceivable colors explains why many distinct physical colors may be perceived by humans to be identical (e.g., pure yellow light versus a mix of red and green light, see [[metamerism (color)|metamerism]]).\n\n==Properties==\n\n===Pythagorean identity===\nTwo vectors {{math|''u''}} and {{math|''v''}} in a Hilbert space {{math|''H''}} are orthogonal when {{math|{{angbr|''u'', ''v''}} {{=}} 0}}. The notation for this is {{math|''u'' ⊥ ''v''}}. More generally, when {{math|''S''}} is a subset in {{math|''H''}}, the notation {{math|''u'' ⊥ ''S''}} means that {{math|''u''}} is orthogonal to every element from {{math|''S''}}.\n\nWhen {{math|''u''}} and {{math|''v''}} are orthogonal, one has\n: <math>\\|u + v\\|^2 = \\langle u + v, u + v \\rangle = \\langle u, u \\rangle + 2 \\, \\operatorname{Re} \\langle u, v \\rangle + \\langle v, v \\rangle= \\|u\\|^2 + \\|v\\|^2\\,.</math>\n\nBy induction on {{math|''n''}}, this is extended to any family {{math|''u''<sub>1</sub>, …, ''u<sub>n</sub>''}} of {{math|''n''}} orthogonal vectors,\n\n: <math>\\|u_1 + \\cdots + u_n\\|^2 = \\|u_1\\|^2 + \\cdots + \\|u_n\\|^2\\,.</math>\n\nWhereas the Pythagorean identity as stated is valid in any inner product space, completeness is required for the extension of the Pythagorean identity to series. A series {{math|∑''u<sub>k</sub>''}} of ''orthogonal'' vectors converges in {{math|''H''}} if and only if the series of squares of norms converges, and\n: <math>\\left\\|\\sum_{k=0}^\\infty u_k \\right\\|^2 = \\sum_{k=0}^\\infty \\left\\|u_k\\right\\|^2\\,.</math>\nFurthermore, the sum of a series of orthogonal vectors is independent of the order in which it is taken.\n\n===Parallelogram identity and polarization===\n[[File:Color parallelogram.svg|right|thumb|Geometrically, the parallelogram identity asserts that {{math|AC<sup>2</sup> + BD<sup>2</sup> {{=}} 2(AB<sup>2</sup> + AD<sup>2</sup>)}}. In words, the sum of the squares of the diagonals is twice the sum of the squares of any two adjacent sides.]]\nBy definition, every Hilbert space is also a [[Banach space]]. Furthermore, in every Hilbert space the following [[parallelogram identity]] holds:\n: <math>\\|u + v\\|^2 + \\|u - v\\|^2 = 2\\left(\\|u\\|^2 + \\|v\\|^2\\right)\\,.</math>\n\nConversely, every Banach space in which the parallelogram identity holds is a Hilbert space, and the inner product is uniquely determined by the norm by the [[polarization identity]].<ref>{{harvnb|Young|1988|p=23}}.</ref> For real Hilbert spaces, the polarization identity is\n: <math>\\langle u, v\\rangle = \\frac{1}{4}\\left(\\|u + v\\|^2 - \\|u - v\\|^2\\right)\\,.</math>\n\nFor complex Hilbert spaces, it is\n: <math>\\langle u, v\\rangle = \\tfrac{1}{4}\\left(\\|u + v\\|^2 - \\|u - v\\|^2 + i\\|u + iv\\|^2 - i\\|u - iv\\|^2\\right)\\,.</math>\n\nThe parallelogram law implies that any Hilbert space is a [[uniformly convex Banach space]].<ref>{{harvnb|Clarkson|1936}}.</ref>\n\n===Best approximation===\nThis subsection employs the [[Hilbert projection theorem]]. If {{math|''C''}} is a non-empty closed convex subset of a Hilbert space {{math|''H''}} and {{math|''x''}} a point in {{math|''H''}}, there exists a unique point {{math|''y'' ∈ ''C''}} that minimizes the distance between {{math|''x''}} and points in {{math|''C''}},<ref>{{harvnb|Rudin|1987|loc=Theorem 4.10}}</ref>\n\n: <math> y \\in C \\,, \\quad \\|x - y\\| = \\operatorname{dist}(x, C) = \\min \\{ \\|x - z\\| : z \\in C \\}\\,.</math>\n\nThis is equivalent to saying that there is a point with minimal norm in the translated convex set {{math|''D'' {{=}} ''C'' − ''x''}}. The proof consists in showing that every minimizing sequence {{math|(''d<sub>n</sub>'') ⊂ ''D''}} is Cauchy (using the parallelogram identity) hence converges (using completeness) to a point in {{math|''D''}} that has minimal norm. More generally, this holds in any uniformly convex Banach space.<ref>{{harvnb|Dunford|Schwartz|1958|loc=II.4.29}}</ref>\n\nWhen this result is applied to a closed subspace {{math|''F''}} of {{math|''H''}}, it can be shown that the point {{math|''y'' ∈ ''F''}} closest to {{math|''x''}} is characterized by<ref>{{harvnb|Rudin|1987|loc=Theorem 4.11}}</ref>\n\n: <math> y \\in F \\,, \\quad x - y \\perp F \\,.</math>\n\nThis point {{math|''y''}} is the ''orthogonal projection'' of {{math|''x''}} onto {{math|''F''}}, and the mapping {{math|''P<sub>F</sub>'' : ''x'' → ''y''}} is linear (see [[#Orthogonal complements and projections|Orthogonal complements and projections]]). This result is especially significant in [[applied mathematics]], especially [[numerical analysis]], where it forms the basis of [[least squares]] methods.<ref>{{cite book\n | last = Blanchet | first = Gérard | last2 = Charbit | first2 = Maurice\n | title = Digital Signal and Image Processing Using MATLAB\n | publisher = Wiley | series = Digital Signal and Image Processing\n | volume = 1 | edition = Second | date = 2014\n | location = New Jersey | pages = 349–360 | isbn = 978-1848216402\n}}</ref>\n\nIn particular, when {{math|''F''}} is not equal to {{math|''H''}}, one can find a nonzero vector {{math|''v''}} orthogonal to {{math|''F''}} (select {{math|''x'' ∉ ''F''}} and {{math|''v'' {{=}} ''x'' − ''y''}}). A very useful criterion is obtained by applying this observation to the closed subspace {{math|''F''}} generated by a subset {{math|''S''}} of {{math|''H''}}.\n: A subset {{math|''S''}} of {{math|''H''}} spans a dense vector subspace if (and only if) the vector 0 is the sole vector {{math|''v'' ∈ ''H''}} orthogonal to {{math|''S''}}.\n\n===Duality===\nThe [[continuous dual space|dual space]] {{math|''H''*}} is the space of all [[continuous function (topology)|continuous]] linear functions from the space {{math|''H''}} into the base field. It carries a natural norm, defined by\n: <math>\\|\\varphi\\| = \\sup_{\\|x\\|=1, x\\in H} |\\varphi(x)| \\,.</math>\nThis norm satisfies the [[parallelogram law]], and so the dual space is also an inner product space. The dual space is also complete, and so it is a Hilbert space in its own right.\n\nThe [[Riesz representation theorem]] affords a convenient description of the dual. To every element {{math|''u''}} of {{math|''H''}}, there is a unique element {{math|''φ<sub>u</sub>''}} of {{math|''H''*}}, defined by\n: <math>\\varphi_u(x) = \\langle x, u\\rangle \\,.</math>\n\nThe mapping {{math|''u'' ↦ ''φ<sub>u</sub>''}} is an [[antilinear map]]ping from {{math|''H''}} to {{math|''H''*}}. The Riesz representation theorem states that this mapping is an antilinear isomorphism.<ref>{{harvnb|Weidmann|1980|loc=Theorem 4.8}}</ref> Thus to every element {{math|''φ''}} of the dual {{math|''H''*}} there exists one and only one {{math|''u<sub>φ</sub>''}} in {{math|''H''}} such that\n: <math>\\langle x, u_\\varphi\\rangle = \\varphi(x)</math>\n\nfor all {{math|''x'' ∈ ''H''}}. The inner product on the dual space {{math|''H''*}} satisfies\n: <math> \\langle \\varphi, \\psi \\rangle = \\langle u_\\psi, u_\\varphi \\rangle \\,.</math>\n\nThe reversal of order on the right-hand side restores linearity in {{math|''φ''}} from the antilinearity of {{math|''u<sub>φ</sub>''}}. In the real case, the antilinear isomorphism from {{math|''H''}} to its dual is actually an isomorphism, and so real Hilbert spaces are naturally isomorphic to their own duals.\n\nThe representing vector {{math|''u<sub>φ</sub>''}} is obtained in the following way. When {{math|''φ'' ≠ 0}}, the [[Kernel (algebra)|kernel]] {{math|''F'' {{=}} Ker(''φ'')}} is a closed vector subspace of {{math|''H''}}, not equal to {{math|''H''}}, hence there exists a nonzero vector {{math|''v''}} orthogonal to {{math|''F''}}. The vector {{math|''u''}} is a suitable scalar multiple {{math|''λv''}} of {{math|''v''}}. The requirement that {{math|''φ''(''v'') {{=}} ⟨''v'', ''u''⟩}} yields\n: <math> u = \\langle v, v \\rangle^{-1} \\, \\overline{\\varphi (v)} \\, v \\,.</math>\n\nThis correspondence {{math|''φ'' ↔ ''u''}} is exploited by the [[bra–ket notation]] popular in [[physics]]. It is common in physics to assume that the inner product, denoted by {{math|{{bra-ket|''x''|''y''}}}}, is linear on the right,\n: <math>\\langle x | y \\rangle = \\langle y, x \\rangle \\,.</math>\n\nThe result {{math|{{bra-ket|''x''|''y''}}}} can be seen as the action of the linear functional {{math|{{bra|''x''}}}} (the ''bra'') on the vector {{math|{{ket|''y''}}}} (the ''ket'').\n\nThe Riesz representation theorem relies fundamentally not just on the presence of an inner product, but also on the completeness of the space. In fact, the theorem implies that the [[Banach space|topological dual]] of any inner product space can be identified with its completion. An immediate consequence of the Riesz representation theorem is also that a Hilbert space {{math|''H''}} is [[reflexive space|reflexive]], meaning that the natural map from {{math|''H''}} into its [[dual space|double dual space]] is an isomorphism.\n\n===Weakly-convergent sequences===\n{{main|Weak convergence (Hilbert space)}}\nIn a Hilbert space {{math|''H''}}, a sequence {{math|{''x<sub>n</sub>''<nowiki>}</nowiki>}} is [[weak topology#Weak convergence|weakly convergent]] to a vector {{math|''x'' ∈ ''H''}} when\n\n: <math>\\lim_n \\langle x_n, v \\rangle = \\langle x, v \\rangle</math>\n\nfor every {{math|''v'' ∈ ''H''}}.\n\nFor example, any orthonormal sequence {{math|{''f<sub>n</sub>''<nowiki>}</nowiki>}} converges weakly to&nbsp;0, as a consequence of [[#Bessel's inequality|Bessel's inequality]]. Every weakly convergent sequence {{math|{''x<sub>n</sub>''<nowiki>}</nowiki>}} is bounded, by the [[uniform boundedness principle]].\n\nConversely, every bounded sequence in a Hilbert space admits weakly convergent subsequences ([[Alaoglu's theorem]]).<ref>{{harvnb|Weidmann|1980|loc=§4.5}}</ref> This fact may be used to prove minimization results for continuous [[convex function]]als, in the same way that the [[Bolzano–Weierstrass theorem]] is used for continuous functions on {{math|'''ℝ'''<sup>''d''</sup>}}. Among several variants, one simple statement is as follows:<ref>{{harvnb|Buttazzo|Giaquinta|Hildebrandt|1998|loc=Theorem 5.17}}</ref>\n\n:If {{math|''f'' : ''H'' → '''ℝ'''}} is a convex continuous function such that {{math|''f''(''x'')}} tends to {{math|+∞}} when {{math|{{norm|''x''}}}} tends to {{math|∞}}, then {{math|''f''}} admits a minimum at some point {{math|''x''<sub>0</sub> ∈ ''H''}}.\n\nThis fact (and its various generalizations) are fundamental for [[direct method in the calculus of variations|direct method]]s in the [[calculus of variations]]. Minimization results for convex functionals are also a direct consequence of the slightly more abstract fact that closed bounded convex subsets in a Hilbert space {{math|''H''}} are [[Weak topology|weakly compact]], since {{math|''H''}} is reflexive. The existence of weakly convergent subsequences is a special case of the [[Eberlein–Šmulian theorem]].\n\n===Banach space properties===\nAny general property of [[Banach space]]s continues to hold for Hilbert spaces. The [[open mapping theorem (functional analysis)|open mapping theorem]] states that a [[continuous function|continuous]] [[surjective]] linear transformation from one Banach space to another is an [[open mapping]] meaning that it sends open sets to open sets. A corollary is the [[bounded inverse theorem]], that a continuous and [[bijective]] linear function from one Banach space to another is an isomorphism (that is, a continuous linear map whose inverse is also continuous). This theorem is considerably simpler to prove in the case of Hilbert spaces than in general Banach spaces.<ref>{{harvnb|Halmos|1982|loc=Problem 52, 58}}</ref> The open mapping theorem is equivalent to the [[closed graph theorem]], which asserts that a function from one Banach space to another is continuous if and only if its graph is a [[closed set]].<ref>{{harvnb|Rudin|1973}}</ref> In the case of Hilbert spaces, this is basic in the study of [[unbounded operator]]s (see [[closed operator]]).\n\nThe (geometrical) [[Hahn–Banach theorem]] asserts that a closed convex set can be separated from any point outside it by means of a [[hyperplane]] of the Hilbert space. This is an immediate consequence of the [[#Best approximation|best approximation]] property: if {{math|''y''}} is the element of a closed convex set {{math|''F''}} closest to {{math|''x''}}, then the separating hyperplane is the plane perpendicular to the segment {{math|''xy''}} passing through its midpoint.<ref>{{harvnb|Trèves|1967|loc=Chapter 18}}</ref>\n\n==Operators on Hilbert spaces==\n\n===Bounded operators===\nThe [[continuous function (topology)|continuous]] [[linear operator]]s {{math|''A'' : ''H''<sub>1</sub> → ''H''<sub>2</sub>}} from a Hilbert space {{math|''H''<sub>1</sub>}} to a second Hilbert space {{math|''H''<sub>2</sub>}} are ''bounded'' in the sense that they map [[bounded set]]s to bounded sets. Conversely, if an operator is bounded, then it is continuous. The space of such [[bounded linear operator]]s has a [[norm (mathematics)|norm]], the [[operator norm]] given by\n\n:<math>\\lVert A \\rVert = \\sup \\left\\{\\,\\lVert Ax \\rVert : \\lVert x \\rVert \\leq 1\\,\\right\\}\\,.</math>\n\nThe sum and the composite of two bounded linear operators is again bounded and linear. For ''y'' in ''H''<sub>2</sub>, the map that sends ''x''&nbsp;∈ ''H''<sub>1</sub> to ⟨''Ax'', ''y''⟩ is linear and continuous, and according to the [[Riesz representation theorem]] can therefore be represented in the form\n: <math>\\left\\langle x, A^* y \\right\\rangle = \\langle Ax, y \\rangle</math>\n\nfor some vector {{math|''A''*''y''}} in {{math|''H''<sub>1</sub>}}. This defines another bounded linear operator {{math|''A''* : ''H''<sub>2</sub> → ''H''<sub>1</sub>}}, the [[Hermitian adjoint|adjoint]] of {{mvar|''A''}}. One can see that {{math|''A''** {{=}} ''A''}}.\n\nThe set {{math|B(''H'')}} of all bounded linear operators on {{math|''H''}} (operators {{Math|''H'' → ''H''}}), together with the addition and composition operations, the norm and the adjoint operation, is a [[C*-algebra]], which is a type of [[operator algebra]].\n\nAn element {{math|''A''}} of {{math|B(''H'')}} is called 'self-adjoint' or 'Hermitian' if {{math|''A''* {{=}} ''A''}}. If {{math|''A''}} is Hermitian and {{math|⟨''Ax'', ''x''⟩ ≥ 0}} for every {{math|''x''}}, then {{math|''A''}} is called 'nonnegative', written {{math|''A'' ≥ 0}}; if equality holds only when {{math|''x'' {{=}} 0}}, then {{math|''A''}} is called 'positive'. The set of self adjoint operators admits a [[partial order]], in which {{math|''A'' ≥ ''B''}} if {{math|''A'' − ''B'' ≥ 0}}. If {{math|''A''}} has the form {{math|''B''*''B''}} for some {{math|''B''}}, then {{math|''A''}} is nonnegative; if {{math|''B''}} is invertible, then {{math|''A''}} is positive. A converse is also true in the sense that, for a non-negative operator {{math|''A''}}, there exists a unique non-negative [[Square root of a matrix|square root]] {{math|''B''}} such that\n: <math>A = B^2 = B^*B\\,.</math>\n\nIn a sense made precise by the [[#Spectral theorem|spectral theorem]], self-adjoint operators can usefully be thought of as operators that are \"real\". An element {{math|''A''}} of {{math|B(''H'')}} is called ''normal'' if {{math|''A''*''A'' {{=}} ''AA''*}}. Normal operators decompose into the sum of a self-adjoint operators and an imaginary multiple of a self adjoint operator\n: <math>A = \\frac{A + A^*}{2} + i\\frac{A - A^*}{2i}</math>\n\nthat commute with each other. Normal operators can also usefully be thought of in terms of their real and imaginary parts.\n\nAn element {{math|''U''}} of {{math|B(''H'')}} is called [[unitary operator|unitary]] if {{math|''U''}} is invertible and its inverse is given by {{math|''U''*}}. This can also be expressed by requiring that {{math|''U''}} be onto and {{math|⟨''Ux'', ''Uy''⟩ {{=}} ⟨''x'', ''y''⟩}} for all {{math|''x'', ''y'' ∈ ''H''}}. The unitary operators form a [[group (mathematics)|group]] under composition, which is the [[isometry group]] of {{math|''H''}}.\n\nAn element of {{math|B(''H'')}} is [[compact operator|compact]] if it sends bounded sets to [[relatively compact]] sets. Equivalently, a bounded operator {{math|''T''}} is compact if, for any bounded sequence {{math|{''x<sub>k</sub>''<nowiki>}</nowiki>}}, the sequence {{math|{''Tx<sub>k</sub>''<nowiki>}</nowiki>}} has a convergent subsequence. Many [[integral operator]]s are compact, and in fact define a special class of operators known as [[Hilbert–Schmidt operator]]s that are especially important in the study of [[integral equation]]s. [[Fredholm operator]]s differ from a compact operator by a multiple of the identity, and are equivalently characterized as operators with a finite dimensional [[kernel (linear operator)|kernel]] and [[cokernel]]. The index of a Fredholm operator {{math|''T''}} is defined by\n: <math>\\operatorname{index} T = \\dim\\ker T - \\dim\\operatorname{coker} T \\,.</math>\n\nThe index is [[homotopy]] invariant, and plays a deep role in [[differential geometry]] via the [[Atiyah–Singer index theorem]].\n\n===Unbounded operators===\n[[Unbounded operator]]s are also tractable in Hilbert spaces, and have important applications to [[quantum mechanics]].<ref>See {{harvtxt|Prugovečki|1981}}, {{harvtxt|Reed|Simon|1980|loc=Chapter VIII}} and {{harvtxt|Folland|1989}}.</ref> An unbounded operator {{math|''T''}} on a Hilbert space {{math|''H''}} is defined as a linear operator whose domain {{math|''D''(''T'')}} is a linear subspace of {{math|''H''}}. Often the domain {{math|''D''(''T'')}} is a dense subspace of {{math|''H''}}, in which case {{math|''T''}} is known as a [[densely defined operator]].\n\nThe adjoint of a densely defined unbounded operator is defined in essentially the same manner as for bounded operators. [[Self-adjoint operator|Self-adjoint unbounded operators]] play the role of the ''observables'' in the mathematical formulation of quantum mechanics. Examples of self-adjoint unbounded operators on the Hilbert space {{math|''L''<sup>2</sup>('''ℝ''')}} are:<ref>{{harvnb|Prugovečki|1981|loc=III, §1.4}}</ref>\n{{unordered list\n| A suitable extension of the differential operator\n: <math>(A f)(x) = -i \\frac{\\mathrm{d}}{\\mathrm{d}x} f(x) \\,,</math>\n\nwhere {{math|''i''}} is the imaginary unit and {{math|''f''}} is a differentiable function of compact support.\n| The multiplication-by-{{math|''x''}} operator:\n: <math>(B f) (x) = x f(x)\\,. </math>\n}}\n\nThese correspond to the [[momentum]] and [[position operator|position]] observables, respectively. Note that neither {{math|''A''}} nor {{math|''B''}} is defined on all of {{math|''H''}}, since in the case of {{math|''A''}} the derivative need not exist, and in the case of {{math|''B''}} the product function need not be square integrable. In both cases, the set of possible arguments form dense subspaces of {{math|''L''<sup>2</sup>('''ℝ''')}}.\n\n==Constructions==\n\n===Direct sums===\nTwo Hilbert spaces {{math|''H''<sub>1</sub>}} and {{math|''H''<sub>2</sub>}} can be combined into another Hilbert space, called the [[direct sum of modules#Direct sum of Hilbert spaces|(orthogonal) direct sum]],<ref>{{harvnb|Dunford|Schwartz|1958|loc=IV.4.17-18}}</ref> and denoted\n: <math>H_1 \\oplus H_2 \\,,</math>\n\nconsisting of the set of all [[ordered pair]]s {{math|(''x''<sub>1</sub>, ''x''<sub>2</sub>)}} where {{math|''x''<sub>''i''</sub> ∈ ''H''<sub>''i''</sub>}}, {{math|''i'' {{=}} 1, 2}}, and inner product defined by\n: <math>\\bigl\\langle (x_1, x_2), (y_1, y_2)\\bigr\\rangle_{H_1 \\oplus H_2} = \\left\\langle x_1, y_1\\right\\rangle_{H_1} + \\left\\langle x_2, y_2\\right\\rangle_{H_2} \\,.</math>\n\nMore generally, if {{math|''H''<sub>''i''</sub>}} is a family of Hilbert spaces indexed by {{nowrap|''i'' ∈ ''I''}}, then the direct sum of the {{math|''H''<sub>''i''</sub>}}, denoted\n: <math>\\bigoplus_{i \\in I}H_i</math>\n\nconsists of the set of all indexed families\n: <math>x = (x_i \\in H_i|i \\in I) \\in \\prod_{i \\in I}H_i</math>\n\nin the [[Cartesian product]] of the {{math|''H''<sub>''i''</sub>}} such that\n: <math>\\sum_{i \\in I} \\|x_i\\|^2 < \\infty \\,.</math>\n\nThe inner product is defined by\n: <math>\\langle x, y\\rangle = \\sum_{i \\in I} \\left\\langle x_i, y_i\\right\\rangle_{H_i} \\,.</math>\n\nEach of the {{math|''H''<sub>''i''</sub>}} is included as a closed subspace in the direct sum of all of the {{math|''H''<sub>''i''</sub>}}. Moreover, the {{math|''H''<sub>''i''</sub>}} are pairwise orthogonal. Conversely, if there is a system of closed subspaces, {{math|''V''<sub>''i''</sub>}}, {{math|''i'' ∈ ''I''}}, in a Hilbert space {{math|''H''}}, that are pairwise orthogonal and whose union is dense in {{math|''H''}}, then {{math|''H''}} is canonically isomorphic to the direct sum of {{math|''V<sub>i</sub>''}}. In this case, {{math|''H''}} is called the internal direct sum of the {{math|''V<sub>i</sub>''}}. A direct sum (internal or external) is also equipped with a family of orthogonal projections {{math|''E<sub>i</sub>''}} onto the {{math|''i''}}th direct summand {{math|''H<sub>i</sub>''}}. These projections are bounded, self-adjoint, [[idempotent]] operators that satisfy the orthogonality condition\n: <math>E_i E_j = 0,\\quad i \\neq j \\,.</math>\n\nThe [[spectral theorem]] for [[compact operator|compact]] self-adjoint operators on a Hilbert space {{math|''H''}} states that {{math|''H''}} splits into an orthogonal direct sum of the eigenspaces of an operator, and also gives an explicit decomposition of the operator as a sum of projections onto the eigenspaces. The direct sum of Hilbert spaces also appears in quantum mechanics as the [[Fock space]] of a system containing a variable number of particles, where each Hilbert space in the direct sum corresponds to an additional [[degrees of freedom (mechanics)|degree of freedom]] for the quantum mechanical system. In [[representation theory]], the [[Peter–Weyl theorem]] guarantees that any [[unitary representation]] of a [[compact group]] on a Hilbert space splits as the direct sum of finite-dimensional representations.\n\n===Tensor products===\n{{main|Tensor product of Hilbert spaces}}\nIf {{math|''x''<sub>1</sub>, ''y''<sub>1</sub> ∊ ''H''<sub>1</sub>}} and {{math|''x''<sub>2</sub>, ''y''<sub>2</sub> ∊ ''H''<sub>2</sub>}}, then one defines an inner product on the (ordinary) [[tensor product]] as follows. On [[simple tensor]]s, let\n\n: <math> \\langle x_1 \\otimes x_2, \\, y_1 \\otimes y_2 \\rangle = \\langle x_1, y_1 \\rangle \\, \\langle x_2, y_2 \\rangle \\,.</math>\n\nThis formula then extends by [[Sesquilinear form|sesquilinearity]] to an inner product on {{math|''H''<sub>1</sub> ⊗ ''H''<sub>2</sub>}}. The Hilbertian tensor product of {{math|''H''<sub>1</sub>}} and {{math|''H''<sub>2</sub>}}, sometimes denoted by {{math|''H''<sub>1</sub> <math>\\widehat{\\otimes}</math> ''H''<sub>2</sub>}}, is the Hilbert space obtained by completing {{math|''H''<sub>1</sub> ⊗ ''H''<sub>2</sub>}} for the metric associated to this inner product.<ref>{{harvnb|Weidmann|1980|loc=§3.4}}</ref>\n\nAn example is provided by the Hilbert space {{math|''L''<sup>2</sup>([0, 1])}}. The Hilbertian tensor product of two copies of {{math|''L''<sup>2</sup>([0, 1])}} is isometrically and linearly isomorphic to the space {{math|''L''<sup>2</sup>([0, 1]<sup>2</sup>)}} of square-integrable functions on the square {{math|[0, 1]<sup>2</sup>}}. This isomorphism sends a simple tensor {{math|''f''<sub>1</sub> ⊗ ''f''<sub>2</sub>}} to the function\n\n: <math>(s, t) \\mapsto f_1(s) \\, f_2(t)</math>\n\non the square.\n\nThis example is typical in the following sense.<ref>{{harvnb|Kadison|Ringrose|1983|loc=Theorem 2.6.4}}</ref> Associated to every simple tensor product {{math|''x''<sub>1</sub> ⊗ ''x''<sub>2</sub>}} is the rank one operator from {{math|''H''{{su|p=∗|b=1|lh=0.8em}}}} to {{math|''H''<sub>2</sub>}} that maps a given {{math|''x''* ∈ ''H''{{su|p=∗|b=1|lh=0.8em}}}} as\n\n: <math>x^* \\mapsto x^*(x_1) x_2 \\,.</math>\n\nThis mapping defined on simple tensors extends to a linear identification between {{math|''H''<sub>1</sub> ⊗ ''H''<sub>2</sub>}} and the space of finite rank operators from {{math|''H''{{su|p=∗|b=1|lh=0.8em}}}} to {{math|''H''<sub>2</sub>}}. This extends to a linear isometry of the Hilbertian tensor product {{math|''H''<sub>1</sub> <math>\\widehat{\\otimes}</math> ''H''<sub>2</sub>}} with the Hilbert space {{math|''HS''(''H''{{su|p=∗|b=1|lh=0.8em}}, ''H''<sub>2</sub>)}} of [[Hilbert–Schmidt operator]]s from {{math|''H''{{su|p=∗|b=1|lh=0.8em}}}} to {{math|''H''<sub>2</sub>}}.\n\n==Orthonormal bases==\nThe notion of an [[orthonormal basis]] from linear algebra generalizes over to the case of Hilbert spaces.<ref>{{harvnb|Dunford|Schwartz|1958|loc=§IV.4}}.</ref> In a Hilbert space {{math|''H''}}, an orthonormal basis is a family {{math|{''e''<sub>''k''</sub>}<sub>''k'' ∈ ''B''</sub>}} of elements of {{math|''H''}} satisfying the conditions:\n# ''Orthogonality'': Every two different elements of {{math|''B''}} are orthogonal: {{math|⟨''e<sub>k</sub>'', ''e<sub>j</sub>''⟩ {{=}} 0}} for all {{math|''k'', ''j'' ∈ ''B''}} with {{nowrap|''k'' ≠ ''j''}}.\n# ''Normalization'': Every element of the family has norm 1: {{math|{{norm|''e''<sub>''k''</sub>}} {{=}} 1}} for all {{math|''k'' ∈ ''B''}}.\n# ''Completeness'': The [[linear span]] of the family {{math|''e''<sub>''k''</sub>}}, {{math|''k'' ∈ ''B''}}, is [[dense set|dense]] in ''H''.\n\nA system of vectors satisfying the first two conditions basis is called an orthonormal system or an orthonormal set (or an orthonormal sequence if {{math|''B''}} is [[countable set|countable]]). Such a system is always [[linearly independent]]. Completeness of an orthonormal system of vectors of a Hilbert space can be equivalently restated as:\n\n: if {{math|⟨''v'', ''e''<sub>''k''</sub>⟩ {{=}} 0}} for all {{math|''k'' ∈ ''B''}} and some {{math|''v'' ∈ ''H''}} then {{math|''v'' {{=}} '''0'''}}.\n\nThis is related to the fact that the only vector orthogonal to a dense linear subspace is the zero vector, for if {{math|''S''}} is any orthonormal set and {{math|''v''}} is orthogonal to {{math|''S''}}, then {{math|''v''}} is orthogonal to the closure of the linear span of {{math|''S''}}, which is the whole space.\n\nExamples of orthonormal bases include:\n* the set {{math|{(1, 0, 0), (0, 1, 0), (0, 0, 1)<nowiki>}</nowiki>}} forms an orthonormal basis of {{math|'''ℝ'''<sup>3</sup>}} with the [[dot product]];\n* the sequence {{math|{''f''<sub>''n''</sub> : ''n'' ∈ '''ℤ'''<nowiki>}</nowiki>}} with {{math|''f''<sub>''n''</sub>(''x'') {{=}} [[exponential function|exp]](2π''inx'')}} forms an orthonormal basis of the complex space {{math|''L''<sup>2</sup>([0, 1])}};\n\nIn the infinite-dimensional case, an orthonormal basis will not be a basis in the sense of [[linear algebra]]; to distinguish the two, the latter basis is also called a [[Hamel basis]]. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique.\n\n===Sequence spaces===\nThe space {{math|''l''<sup>2</sup>}} of square-summable sequences of complex numbers is the set of infinite sequences\n\n: <math>(c_1, c_2, c_3, \\dots)</math>\n\nof complex numbers such that\n\n: <math>\\left|c_1\\right|^2 + \\left|c_2\\right|^2 + \\left|c_3\\right|^2 + \\cdots < \\infty \\,.</math>\n\nThis space has an orthonormal basis:\n\n: <math>\\begin{align}\n  e_1 &= (1, 0, 0, \\dots) \\\\\n  e_2 &= (0, 1, 0, \\dots) \\\\\n      & \\ \\ \\vdots\n\\end{align}</math>\n\nMore generally, if {{math|''B''}} is any set, then one can form a Hilbert space of sequences with index set {{math|''B''}}, defined by\n\n: <math>l^2(B) =\\left\\{ x : B \\xrightarrow{x} \\mathbb{C} \\, \\left| \\, \\sum_{b \\in B} \\left|x (b)\\right|^2 < \\infty \\right. \\right\\} \\,.</math>\n\nThe summation over ''B'' is here defined by\n\n: <math>\\sum_{b \\in B} \\left|x (b)\\right|^2 = \\sup \\sum_{n=1}^N \\left|x(b_n)\\right|^2</math>\n\nthe [[supremum]] being taken over all finite subsets of&nbsp;{{math|''B''}}. It follows that, for this sum to be finite, every element of {{math|''l''<sup>2</sup>(''B'')}} has only countably many nonzero terms. This space becomes a Hilbert space with the inner product\n:<math>\\langle x, y \\rangle = \\sum_{b \\in B} x(b)\\overline{y(b)}</math>\n\nfor all {{math|''x'', ''y'' ∈ ''l''<sup>2</sup>(''B'')}}. Here the sum also has only countably many nonzero terms, and is unconditionally convergent by the Cauchy–Schwarz inequality.\n\nAn orthonormal basis of {{math|''l''<sup>2</sup>(''B'')}} is indexed by the set {{math|''B''}}, given by\n:<math>e_b\\left(b'\\right) = \\begin{cases}\n  1 & \\text{if } b=b'\\\\\n  0 & \\text{otherwise.}\n\\end{cases}</math>\n\n{{Anchor|Bessel's inequality}}\n{{Anchor|Parseval's formula}}\n\n===Bessel's inequality and Parseval's formula===\nLet {{math|''f''<sub>1</sub>, ..., ''f''<sub>''n''</sub>}} be a finite orthonormal system in&nbsp;{{math|''H''}}. For an arbitrary vector {{math|''x'' ∈ ''H''}}, let\n\n: <math>y = \\sum_{j=1}^n \\langle x, f_j \\rangle \\, f_j \\,.</math>\n\nThen {{math|⟨''x'', ''f''<sub>''k''</sub>⟩ {{=}} ⟨''y'', ''f''<sub>''k''</sub>⟩}} for every {{math|''k'' {{=}} 1, …, ''n''}}. It follows that {{math|''x'' − ''y''}} is orthogonal to each {{math|''f''<sub>''k''</sub>}}, hence {{math|''x'' − ''y''}} is orthogonal to&nbsp;{{math|''y''}}. Using the Pythagorean identity twice, it follows that\n\n: <math>\\|x\\|^2 = \\|x - y\\|^2 + \\|y\\|^2 \\ge \\|y\\|^2 = \\sum_{j=1}^n\\bigl|\\langle x, f_j \\rangle\\bigr|^2 \\,.</math>\n\nLet {{math|{''f''<sub>''i''</sub>}, ''i'' ∈ ''I''}}, be an arbitrary orthonormal system in&nbsp;{{math|''H''}}. Applying the preceding inequality to every finite subset {{math|''J''}} of {{math|''I''}} gives the ''Bessel inequality''<ref>For the case of finite index sets, see, for instance, {{harvnb|Halmos|1957|loc=§5}}. For infinite index sets, see {{harvnb|Weidmann|1980|loc=Theorem 3.6}}.</ref>\n\n: <math>\\sum_{i \\in I}\\bigl|\\langle x, f_i \\rangle\\bigr|^2 \\le \\|x\\|^2, \\quad x \\in H</math>\n\n(according to the definition of the [[series (mathematics)#Summations over arbitrary index sets|sum of an arbitrary family]] of non-negative real numbers).\n\nGeometrically, Bessel's inequality implies that the orthogonal projection of {{math|''x''}} onto the linear subspace spanned by the {{math|''f<sub>i</sub>''}} has norm that does not exceed that of {{math|''x''}}. In two dimensions, this is the assertion that the length of the leg of a right triangle may not exceed the length of the hypotenuse.\n\nBessel's inequality is a stepping stone to the more powerful [[Parseval identity]], which governs the case when Bessel's inequality is actually an equality. If {{math|{''e''<sub>''k''</sub>}<sub>''k'' ∈ ''B''</sub>}} is an orthonormal basis of {{math|''H''}}, then every element {{math|''x''}} of {{math|''H''}} may be written as\n\n: <math>x = \\sum_{k \\in B} \\left\\langle x, e_k \\right\\rangle \\, e_k \\,.</math>\n\nEven if {{math|''B''}} is uncountable, Bessel's inequality guarantees that the expression is well-defined and consists only of countably many nonzero terms. This sum is called the Fourier expansion of {{math|''x''}}, and the individual coefficients {{math|⟨''x'', ''e''<sub>''k''</sub>⟩}} are the Fourier coefficients of {{math|''x''}}. Parseval's formula is then\n: <math>\\|x\\|^2 = \\sum_{k\\in B}|\\langle x, e_k\\rangle|^2 \\,.</math>\n\nConversely, if {{math|{''e''<sub>''k''</sub><nowiki>}</nowiki>}} is an orthonormal set such that Parseval's identity holds for every {{math|''x''}}, then {{math|{''e''<sub>''k''</sub><nowiki>}</nowiki>}} is an orthonormal basis.\n\n===Hilbert dimension===\nAs a consequence of [[Zorn's lemma]], ''every'' Hilbert space admits an orthonormal basis; furthermore, any two orthonormal bases of the same space have the same [[cardinal number|cardinality]], called the Hilbert dimension of the space.<ref>{{harvnb|Levitan|2001}}. Many authors, such as {{harvtxt|Dunford|Schwartz|1958|loc=§IV.4}}, refer to this just as the dimension. Unless the Hilbert space is finite dimensional, this is not the same thing as its dimension as a linear space (the cardinality of a Hamel basis).</ref> For instance, since {{math|''l''<sup>2</sup>(''B'')}} has an orthonormal basis indexed by {{math|''B''}}, its Hilbert dimension is the cardinality of {{math|''B''}} (which may be a finite integer, or a countable or uncountable [[cardinal number]]).\n\nAs a consequence of Parseval's identity, if {{math|{''e''<sub>''k''</sub>}<sub>''k'' ∈ ''B''</sub>}} is an orthonormal basis of {{math|''H''}}, then the map {{math|''Φ'' : ''H'' → ''l''<sup>2</sup>(''B'')}} defined by {{math|''Φ''(''x'') {{=}} ⟨x, ''e''<sub>''k''</sub>⟩<sub>''k''∈''B''</sub>}} is an isometric isomorphism of Hilbert spaces: it is a bijective linear mapping such that\n:<math>\\bigl\\langle \\Phi (x), \\Phi(y) \\bigr\\rangle_{l^2(B)} = \\left\\langle x, y \\right\\rangle_H</math>\nfor all {{math|''x'', ''y'' ∈ ''H''}}. The [[cardinal number]] of {{math|''B''}} is the Hilbert dimension of {{math|''H''}}. Thus every Hilbert space is isometrically isomorphic to a sequence space {{math|''l''<sup>2</sup>(''B'')}} for some set {{math|''B''}}.\n\n===Separable spaces===\nA Hilbert space is [[separable space|separable]] if and only if it admits a [[countable]] orthonormal basis. All infinite-dimensional separable Hilbert spaces are therefore isometrically isomorphic to {{math|''l''<sup>2</sup>}}.\n\nIn the past, Hilbert spaces were often required to be separable as part of the definition.<ref>{{harvnb|Prugovečki|1981|loc=I, §4.2}}</ref> Most spaces used in physics are separable, and since these are all isomorphic to each other, one often refers to any infinite-dimensional separable Hilbert space as \"''the'' Hilbert space\" or just \"Hilbert space\".<ref>{{harvtxt|von Neumann|1955}} defines a Hilbert space via a countable Hilbert basis, which amounts to an isometric isomorphism with ''l''<sup>2</sup>. The convention still persists in most rigorous treatments of quantum mechanics; see for instance {{harvnb|Sobrino|1996|loc=Appendix B}}.</ref> Even in [[quantum field theory]], most of the Hilbert spaces are in fact separable, as stipulated by the [[Wightman axioms]]. However, it is sometimes argued that non-separable Hilbert spaces are also important in quantum field theory, roughly because the systems in the theory possess an infinite number of [[degrees of freedom (mechanics)|degrees of freedom]] and any infinite [[Hilbert tensor product]] (of spaces of dimension greater than one) is non-separable.<ref name=\"Streater\">{{harvnb|Streater|Wightman|1964|pp=86–87}}</ref> For instance, a [[bosonic field]] can be naturally thought of as an element of a tensor product whose factors represent harmonic oscillators at each point of space. From this perspective, the natural state space of a boson might seem to be a non-separable space.<ref name=\"Streater\"/> However, it is only a small separable subspace of the full tensor product that can contain physically meaningful fields (on which the observables can be defined). Another non-separable Hilbert space models the state of an infinite collection of particles in an unbounded region of space. An orthonormal basis of the space is indexed by the density of the particles, a continuous parameter, and since the set of possible densities is uncountable, the basis is not countable.<ref name=\"Streater\"/>\n\n==Orthogonal complements and projections==\nIf {{math|''S''}} is a subset of a Hilbert space {{math|''H''}}, the set of vectors orthogonal to {{math|''S''}} is defined by\n: <math>S^\\perp = \\left\\{ x \\in H : \\langle x, s \\rangle = 0\\ \\forall s \\in S \\right\\} \\,.</math>\n\n{{math|''S''<sup>⊥</sup>}} is a [[closed set|closed]] subspace of {{math|''H''}} (can be proved easily using the linearity and continuity of the inner product) and so forms itself a Hilbert space. If {{math|''V''}} is a closed subspace of {{math|''H''}}, then {{math|''V''<sup>⊥</sup>}} is called the ''orthogonal complement'' of {{math|''V''}}. In fact, every {{math|''x'' ∈ ''H''}} can then be written uniquely as {{math|''x'' {{=}} ''v'' + ''w''}}, with {{math|''v'' ∈ ''V''}} and {{math|''w'' ∈ ''V''<sup>⊥</sup>}}. Therefore, {{math|''H''}} is the internal Hilbert direct sum of {{math|''V''}} and {{math|''V''<sup>⊥</sup>}}.\n\nThe linear operator {{math|''P<sub>V</sub>'' : ''H'' → ''H''}} that maps {{math|''x''}} to {{math|''v''}} is called the ''orthogonal projection'' onto {{math|''V''}}. There is a [[natural transformation|natural]] one-to-one correspondence between the set of all closed subspaces of {{math|''H''}} and the set of all bounded self-adjoint operators {{math|''P''}} such that {{math|''P''<sup>2</sup> {{=}} ''P''}}. Specifically,\n\n: '''Theorem'''. The orthogonal projection {{math|''P<sub>V</sub>''}} is a self-adjoint linear operator on {{math|''H''}} of norm ≤&nbsp;1 with the property {{math|''P''{{su|p=2|b=''V''}} {{=}} ''P<sub>V</sub>''}}. Moreover, any self-adjoint linear operator {{math|''E''}} such that {{math|''E''<sup>2</sup> {{=}} ''E''}} is of the form {{math|''P<sub>V</sub>''}}, where {{math|''V''}} is the range of {{math|''E''}}. For every {{math|''x''}} in {{math|''H''}}, {{math|''P<sub>V</sub>''(''x'')}} is the unique element {{math|''v''}} of {{math|''V''}} that minimizes the distance {{math|{{norm|''x'' − ''v''}}}}.\n\nThis provides the geometrical interpretation of {{math|''P<sub>V</sub>''(''x'')}}: it is the best approximation to ''x'' by elements of ''V''.<ref>{{harvnb|Young|1988|loc=Theorem 15.3}}</ref>\n\nProjections {{math|''P<sub>U</sub>''}} and {{math|''P<sub>V</sub>''}} are called mutually orthogonal if {{math|''P''<sub>''U''</sub>''P''<sub>''V''</sub> {{=}} 0}}. This is equivalent to {{math|''U''}} and {{math|''V''}} being orthogonal as subspaces of {{math|''H''}}. The sum of the two projections {{math|''P''<sub>''U''</sub>}} and {{math|''P''<sub>''V''</sub>}} is a projection only if {{math|''U''}} and {{math|''V''}} are orthogonal to each other, and in that case {{math|''P''<sub>''U''</sub> + ''P''<sub>''V''</sub> {{=}} ''P''<sub>''U''+''V''</sub>}}. The composite {{math|''P''<sub>''U''</sub>''P''<sub>''V''</sub>}} is generally not a projection; in fact, the composite is a projection if and only if the two projections commute, and in that case {{math|''P''<sub>''U''</sub>''P''<sub>''V''</sub> {{=}} ''P''<sub>''U''∩''V''</sub>}}.\n\nBy restricting the codomain to the Hilbert space {{math|''V''}}, the orthogonal projection {{math|''P''<sub>''V''</sub>}} gives rise to a projection mapping {{math|''π'' : ''H'' → ''V''}}; it is the adjoint of the [[inclusion mapping]]\n\n: <math>i : V \\to H \\,,</math>\n\nmeaning that\n\n: <math>\\left\\langle i x, y\\right\\rangle_H = \\left\\langle x, \\pi y \\right\\rangle_V</math>\n\nfor all {{math|''x'' ∈ ''V''}} and {{math|''y'' ∈ ''H''}}.\n\nThe operator norm of the orthogonal projection {{math|''P''<sub>''V''</sub>}} onto a nonzero closed subspace {{math|''V''}} is equal to 1:\n: <math>\\|P_V\\| = \\sup_{x \\in H, x\\not=0} \\frac{\\|P_V x\\|}{\\|x\\|} = 1 \\,.</math>\n\nEvery closed subspace ''V'' of a Hilbert space is therefore the image of an operator {{math|''P''}} of norm one such that {{math|''P''<sup>2</sup> {{=}} ''P''}}. The property of possessing appropriate projection operators characterizes Hilbert spaces:<ref>{{harvnb|Kakutani|1939}}</ref>\n\n* A Banach space of dimension higher than 2 is (isometrically) a Hilbert space if and only if, for every closed subspace {{math|''V''}}, there is an operator {{math|''P''<sub>''V''</sub>}} of norm one whose image is {{math|''V''}} such that {{math|''P''{{su|b=''V''|p=2}} {{=}} ''P<sub>V</sub>''}}.\n\nWhile this result characterizes the metric structure of a Hilbert space, the structure of a Hilbert space as a [[topological vector space]] can itself be characterized in terms of the presence of complementary subspaces:<ref>{{harvnb|Lindenstrauss|Tzafriri|1971}}</ref>\n* A Banach space {{math|''X''}} is topologically and linearly isomorphic to a Hilbert space if and only if, to every closed subspace {{math|''V''}}, there is a closed subspace {{math|''W''}} such that {{math|''X''}} is equal to the internal direct sum {{math|''V'' ⊕ ''W''}}.\n\nThe orthogonal complement satisfies some more elementary results. It is a [[monotone function]] in the sense that if {{math|''U'' ⊂ ''V''}}, then {{math|''V''<sup>⊥</sup> ⊆ ''U''<sup>⊥</sup>}} with equality holding if and only if {{math|''V''}} is contained in the [[closure (topology)|closure]] of {{math|''U''}}. This result is a special case of the [[Hahn–Banach theorem]]. The closure of a subspace can be completely characterized in terms of the orthogonal complement: if {{math|''V''}} is a subspace of {{math|''H''}}, then the closure of {{math|''V''}} is equal to {{math|''V''<sup>⊥⊥</sup>}}. The orthogonal complement is thus a [[Galois connection]] on the [[partial order]] of subspaces of a Hilbert space. In general, the orthogonal complement of a sum of subspaces is the intersection of the orthogonal complements:<ref>{{harvnb|Halmos|1957|loc=§12}}</ref>\n: <math>\\left(\\sum_i V_i\\right)^\\perp = \\bigcap_i V_i^\\perp \\,.</math>\n\nIf the {{math|''V''<sub>''i''</sub>}} are in addition closed, then\n: <math>\\overline{\\sum_i V_i^\\perp} = \\left(\\bigcap_i V_i\\right)^\\perp \\,.</math>\n\n==Spectral theory==\nThere is a well-developed [[spectral theory]] for self-adjoint operators in a Hilbert space, that is roughly analogous to the study of [[symmetric matrix|symmetric matrices]] over the reals or self-adjoint matrices over the complex numbers.<ref>A general account of spectral theory in Hilbert spaces can be found in {{harvtxt|Riesz|Sz Nagy|1990}}. A more sophisticated account in the language of C*-algebras is in {{harvtxt|Rudin|1973}} or {{harvtxt|Kadison|Ringrose|1997}}</ref> In the same sense, one can obtain a \"diagonalization\" of a self-adjoint operator as a suitable sum (actually an integral) of orthogonal projection operators.\n\nThe [[spectrum of an operator]] {{math|''T''}}, denoted {{math|''σ''(''T'')}}, is the set of complex numbers {{math|''λ''}} such that {{math|''T'' − ''λ''}} lacks a continuous inverse. If {{math|''T''}} is bounded, then the spectrum is always a [[compact set]] in the complex plane, and lies inside the disc {{math|{{abs|''z''}} ≤ {{norm|''T''}}}}. If {{math|''T''}} is self-adjoint, then the spectrum is real. In fact, it is contained in the interval {{math|[''m'', ''M'']}} where\n: <math>m = \\inf_{\\|x\\|=1}\\langle Tx, x\\rangle \\,,\\quad M = \\sup_{\\|x\\|=1}\\langle Tx, x\\rangle \\,.</math>\n\nMoreover, {{math|''m''}} and {{math|''M''}} are both actually contained within the spectrum.\n\nThe eigenspaces of an operator {{math|''T''}} are given by\n: <math>H_\\lambda = \\ker(T - \\lambda)\\,.</math>\n\nUnlike with finite matrices, not every element of the spectrum of {{math|''T''}} must be an eigenvalue: the linear operator {{math|''T'' − ''λ''}} may only lack an inverse because it is not surjective. Elements of the spectrum of an operator in the general sense are known as ''spectral values''. Since spectral values need not be eigenvalues, the spectral decomposition is often more subtle than in finite dimensions.\n\nHowever, the [[spectral theorem]] of a self-adjoint operator {{math|''T''}} takes a particularly simple form if, in addition, {{math|''T''}} is assumed to be a [[compact operator]]. The [[Compact operator on Hilbert space#Spectral theorem|spectral theorem for compact self-adjoint operators]] states:<ref>See, for instance, {{harvtxt|Riesz|Sz Nagy|1990|loc=Chapter VI}} or {{harvnb|Weidmann|1980|loc=Chapter 7}}. This result was already known to {{harvtxt|Schmidt|1907}} in the case of operators arising from integral kernels.</ref>\n* A compact self-adjoint operator {{math|''T''}} has only countably (or finitely) many spectral values. The spectrum of {{math|''T''}} has no [[limit point]] in the complex plane except possibly zero. The eigenspaces of {{math|''T''}} decompose {{math|''H''}} into an orthogonal direct sum:\n*: <math>H=\\bigoplus_{\\lambda\\in\\sigma(T)}H_\\lambda \\,.</math>\n: Moreover, if {{math|''E<sub>λ</sub>''}} denotes the orthogonal projection onto the eigenspace {{math|''H<sub>λ</sub>''}}, then\n:: <math>T = \\sum_{\\lambda\\in\\sigma(T)} \\lambda E_\\lambda \\,,</math>\n: where the sum converges with respect to the norm on {{math|B(''H'')}}.\n\nThis theorem plays a fundamental role in the theory of [[integral equation]]s, as many integral operators are compact, in particular those that arise from [[Hilbert–Schmidt operator]]s.\n\nThe general spectral theorem for self-adjoint operators involves a kind of operator-valued [[Riemann–Stieltjes integral]], rather than an infinite summation.<ref>{{harvnb|Riesz|Sz Nagy|1990|loc=§§107–108}}</ref> The ''spectral family'' associated to {{math|''T''}} associates to each real number λ an operator {{math|''E<sub>λ</sub>''}}, which is the projection onto the nullspace of the operator {{math|(''T'' − ''λ'')<sup>+</sup>}}, where the positive part of a self-adjoint operator is defined by\n: <math>A^+ = \\tfrac{1}{2}\\left(\\sqrt{A^2} + A\\right) \\,.</math>\n\nThe operators {{math|''E<sub>λ</sub>''}} are monotone increasing relative to the partial order defined on self-adjoint operators; the eigenvalues correspond precisely to the jump discontinuities. One has the spectral theorem, which asserts\n: <math>T = \\int_\\mathbb{R} \\lambda\\, \\mathrm{d}E_\\lambda \\,.</math>\n\nThe integral is understood as a Riemann–Stieltjes integral, convergent with respect to the norm on {{math|B(''H'')}}. In particular, one has the ordinary scalar-valued integral representation\n: <math>\\langle Tx, y\\rangle = \\int_{\\mathbb{R}} \\lambda\\,\\mathrm{d}\\langle E_\\lambda x, y\\rangle \\,.</math>\n\nA somewhat similar spectral decomposition holds for normal operators, although because the spectrum may now contain non-real complex numbers, the operator-valued Stieltjes measure {{math|d''E<sub>λ</sub>''}} must instead be replaced by a [[resolution of the identity]].\n\nA major application of spectral methods is the [[spectral mapping theorem]], which allows one to apply to a self-adjoint operator {{math|''T''}} any continuous complex function {{math|''f''}} defined on the spectrum of {{math|''T''}} by forming the integral\n: <math>f(T) = \\int_{\\sigma(T)} f(\\lambda)\\,\\mathrm{d}E_\\lambda \\,.</math>\n\nThe resulting [[continuous functional calculus]] has applications in particular to [[pseudodifferential operators]].<ref>{{harvnb|Shubin|1987}}</ref>\n\nThe spectral theory of ''unbounded'' self-adjoint operators is only marginally more difficult than for bounded operators. The spectrum of an unbounded operator is defined in precisely the same way as for bounded operators: {{math|''λ''}} is a spectral value if the [[resolvent operator]]\n: <math>R_\\lambda = (T - \\lambda)^{-1}</math>\n\nfails to be a well-defined continuous operator. The self-adjointness of {{math|''T''}} still guarantees that the spectrum is real. Thus the essential idea of working with unbounded operators is to look instead at the resolvent {{math|''R<sub>λ</sub>''}} where {{math|''λ''}} is nonreal. This is a ''bounded'' normal operator, which admits a spectral representation that can then be transferred to a spectral representation of {{math|''T''}} itself. A similar strategy is used, for instance, to study the spectrum of the Laplace operator: rather than address the operator directly, one instead looks as an associated resolvent such as a [[Riesz potential]] or [[Bessel potential]].\n\nA precise version of the spectral theorem in this case is:<ref>{{harvnb|Rudin|1973|loc=Theorem 13.30}}.</ref>\n: Given a densely defined self-adjoint operator {{math|''T''}} on a Hilbert space {{math|''H''}}, there corresponds a unique [[resolution of the identity]] {{math|''E''}} on the Borel sets of {{math|'''ℝ'''}}, such that\n:: <math>\\langle Tx, y\\rangle = \\int_\\mathbb{R} \\lambda\\,\\mathrm{d}E_{x,y}(\\lambda)</math>\n: for all {{math|''x'' ∈ ''D''(''T'')}} and {{math|''y'' ∈ ''H''}}. The spectral measure {{math|''E''}} is concentrated on the spectrum of {{math|''T''}}.\nThere is also a version of the spectral theorem that applies to unbounded normal operators.\n\n== In popular culture ==\n[[Thomas Pynchon]] introduced the fictional character, Sammy Hilbert-Spaess (a pun on \"Hilbert Space\"), in his 1973 novel, [[Gravity's Rainbow]].  Hilbert-Spaess is first described as a \"a ubiquitous double agent\" and later as \"at least a double agent\".  The novel had earlier referenced the work of fellow German mathematician [[Kurt Gödel]]'s [[Gödel's incompleteness theorems|Incompleteness Theorems]] which showed that [[Hilbert's program|Hilbert's Program]], Hilbert's formalized plan to unify mathematics into a single set of axioms, was not possible.<ref>{{Cite web|url=https://gravitys-rainbow.pynchonwiki.com/wiki/index.php?title=H&oldid=1593|title=H - Thomas Pynchon Wiki {{!}} Gravity's Rainbow|website=gravitys-rainbow.pynchonwiki.com|language=en|access-date=2018-10-23}}</ref><ref>{{Cite web|url=https://gravitys-rainbow.pynchonwiki.com/wiki/index.php?title=G|title=G - Thomas Pynchon Wiki {{!}} Gravity's Rainbow|website=gravitys-rainbow.pynchonwiki.com|language=en|access-date=2018-10-23}}</ref><ref>{{Cite book|title=Gravity's Rainbow|last=Thomas|first=Pynchon|publisher=Viking Press|year=1973|isbn=978-0143039945|location=|pages=217,275}}</ref>\n\n==See also==\n{{portal|Mathematics}}\n* [[Hadamard space]]\n* [[Hilbert algebra (disambiguation)|Hilbert algebra]]\n* [[Hilbert C*-module]]\n* [[Hilbert manifold]]\n* [[Operator theory]]\n* [[Operator topologies]]\n* [[Rigged Hilbert space]]\n\n==Remarks==\n{{reflist|group=nb}}\n\n==Notes==\n{{reflist|colwidth=30em}}\n\n==References==\n{{Refbegin|colwidth=30em}}\n*{{Citation | last1=Bachman | first1=George | last2=Narici | first2=Lawrence | last3=Beckenstein | first3=Edward | title=Fourier and wavelet analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Universitext | isbn=978-0-387-98899-3 | mr=1729490 | year=2000}}.\n* {{citation|title=Partial differential equations|first=Lipman|last= Bers|authorlink=Lipman Bers|first2=Fritz|last2= John|authorlink2=Fritz John|first3= Martin|last3= Schechter|publisher= American Mathematical Society|year=1981|isbn=978-0-8218-0049-2}}.\n* {{citation|first=Nicolas|last=Bourbaki|authorlink=Nicolas Bourbaki|title=Spectral theories|series=Elements of mathematics|publisher= Springer-Verlag|location=Berlin|year=1986|isbn=978-0-201-00767-1}}.\n* {{citation|first=Nicolas|last=Bourbaki|authorlink=Nicolas Bourbaki|title=Topological vector spaces|series=Elements of mathematics|publisher= Springer-Verlag|location=Berlin|year=1987|isbn=978-3-540-13627-9}}.\n*{{citation| authorlink1 = Carl Benjamin Boyer|last1=Boyer|first1=Carl Benjamin|last2=Merzbach|first2=Uta C|author2-link= Uta Merzbach | year = 1991| title = A History of Mathematics| edition= 2nd| publisher = John Wiley & Sons, Inc.|isbn=978-0-471-54397-8}}.\n* {{citation|first1=S.|last1=Brenner|first2=R. L.|last2=Scott|title=The Mathematical Theory of Finite Element Methods|edition=2nd|publisher=Springer|year=2005|isbn=978-0-387-95451-6}}.\n* {{Citation | last1=Buttazzo | first1=Giuseppe | last2=Giaquinta | first2=Mariano | last3=Hildebrandt | first3=Stefan | title=One-dimensional variational problems | publisher=The Clarendon Press Oxford University Press | series=Oxford Lecture Series in Mathematics and its Applications | isbn=978-0-19-850465-8 | mr=1694383 | year=1998 | volume=15}}.\n* {{citation|first=J. A.|last=Clarkson|title=Uniformly convex spaces|journal=Trans. Amer. Math. Soc.|volume=40|year=1936|pages=396–414|doi=10.2307/1989630|issue=3|jstor=1989630}}.\n* {{citation|first=Richard|last=Courant|authorlink=Richard Courant|first2=David|last2=Hilbert|authorlink2=David Hilbert|title=Methods of Mathematical Physics, Vol. I|publisher=Interscience|year=1953}}.\n* {{citation| first = Jean| last = Dieudonné| authorlink=Jean Dieudonné|title= Foundations of Modern Analysis|publisher = Academic Press| year= 1960}}.\n* {{citation|first=P.A.M.|last=Dirac|authorlink=Paul Dirac|title=Principles of Quantum Mechanics |publisher=Clarendon Press|location=Oxford|year=1930|title-link=Principles of Quantum Mechanics}}.\n* {{citation|first1=N.|last1=Dunford|first2=J.T.|last2=Schwartz|authorlink2=Jacob T. Schwartz|title=Linear operators, Parts I and II|publisher=Wiley-Interscience|year=1958}}.\n* {{citation|first=P.|last= Duren|title=Theory of H<sup>p</sup>-Spaces|year=1970|publisher= Academic Press|location= New York}}.\n<!--*{{Citation | last1=Feintuch | first1=Avraham | last2=Saeks | first2=Richard | title=System theory | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-251750-1 | mr=663906 | year=1982 | volume=102}}.-->\n*{{citation |title=Fourier analysis and its application |first=Gerald B.|last=Folland |url=https://books.google.com/books?as_isbn=0-8218-4790-2 |isbn=978-0-8218-4790-9 |publisher=American Mathematical Society Bookstore |year=2009 |edition=Reprint of Wadsworth and Brooks/Cole 1992}}.\n* {{citation|first=Gerald B.|last=Folland|title=Harmonic analysis in phase space|series=Annals of Mathematics Studies|volume= 122|publisher=Princeton University Press|year= 1989|isbn= 978-0-691-08527-2}}.\n* {{citation|first=Maurice|last=Fréchet|title=Sur les ensembles de fonctions et les opérations linéaires|journal=C. R. Acad. Sci. Paris|volume=144|pages=1414–1416|year=1907}}.\n* {{citation|first=Maurice|last=Fréchet|title=Sur les opérations linéaires|year=1904–1907}}.\n* {{citation|first=Enrico|last=Giusti|title=Direct Methods in the Calculus of Variations|publisher=World Scientific|year=2003|isbn=978-981-238-043-2}}.\n* {{Citation | last1=Grattan-Guinness | first1=Ivor | title=The search for mathematical roots, 1870–1940 | publisher=[[Princeton University Press]] | series=Princeton Paperbacks | isbn=978-0-691-05858-0 | mr=1807717 | year=2000}}.\n* {{citation| last =Halmos| first =Paul| authorlink=Paul Halmos|title=Introduction to Hilbert Space and the Theory of Spectral Multiplicity|year=1957|\npublisher=Chelsea Pub. Co}}\n* {{citation| last=Halmos|first=Paul|authorlink=Paul Halmos|title=A Hilbert Space Problem Book|year=1982|publisher=Springer-Verlag|isbn=978-0-387-90685-0}}.\n* {{citation| last = Hewitt| first = Edwin| last2 = Stromberg| first2 = Karl| title = Real and Abstract Analysis| year = 1965| publisher = Springer-Verlag| location = New York}}.\n* {{citation| last1=Hilbert| first1=David| authorlink1 = David Hilbert| last2=Nordheim| first2 = Lothar (Wolfgang)| authorlink2= Lothar Nordheim| last3= von Neumann| first3= John|authorlink3= John von Neumann| title = Über die Grundlagen der Quantenmechanik| url=http://dz-srv1.sub.uni-goettingen.de/sub/digbib/loader?ht=VIEW&did=D27779| journal = Mathematische Annalen| volume = 98| pages = 1–30| year = 1927| doi=10.1007/BF01451579}}{{dead link|date=July 2016}}.\n* {{citation|first=Mark|last=Kac|authorlink=Mark Kac|title=Can one hear the shape of a drum?|journal=[[American Mathematical Monthly]]|volume=73|issue=4, part 2|year=1966|pages=1–23|doi=10.2307/2313748|jstor=2313748}}.\n*{{Citation | last1=Kadison | first1=Richard V. | last2=Ringrose | first2=John R. | title=Fundamentals of the theory of operator algebras. Vol. I | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Graduate Studies in Mathematics | isbn=978-0-8218-0819-1 | mr=1468229 | year=1997 | volume=15}}.\n<!--* {{citation| title = Элементы теории функций и функционального анализа| last1 = Колмогоров| first1 = А. Н.| authorlink1 = Andrey Kolmogorov| last2= Фомин|first2= С. В.| authorlink2 = Sergei Fomin| year = 1989| edition = sixth Russian (with corrections)| publisher = \"Nauka\", Moscow| isbn= 5-02-013993-9}}.-->\n*{{Citation | last1=Kakutani | first1=Shizuo | author1-link=Shizuo Kakutani | title=Some characterizations of Euclidean space | mr=0000895 | year=1939 | journal=Japanese Journal of Mathematics | volume=16 | pages=93–97}}.\n*{{Citation | last1=Kline | first1=Morris | author1-link=Morris Kline | title=Mathematical thought from ancient to modern times, Volume 3 | year=1972 | publisher=[[Oxford University Press]] | edition=3rd | isbn=978-0-19-506137-6 | publication-date=1990}}.\n* {{citation| title = Introductory Real Analysis| last1 = Kolmogorov| first1 = Andrey| authorlink1 = Andrey Kolmogorov| last2= Fomin|first2= Sergei V.| authorlink2 = Sergei Fomin| year = 1970| edition = Revised English edition, trans. by Richard A. Silverman (1975)| publisher = Dover Press| isbn= 978-0-486-61226-3}}.\n* {{Citation | last1=Krantz | first1=Steven G. | authorlink=Steven Krantz|title=Function Theory of Several Complex Variables | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-2724-6 | year=2002}}.\n* {{citation |title=Applied analysis |first=Cornelius|last=Lanczos|isbn=978-0-486-65656-4 |publisher=Dover Publications |year=1988 |edition=Reprint of 1956 Prentice-Hall |url=https://books.google.com/books?as_isbn=0-486-65656-X}}.\n*{{Citation | last1=Lindenstrauss | first1=J. | last2=Tzafriri | first2=L. | title=On the complemented subspaces problem | mr=0276734 | year=1971 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=9 | pages=263–269 | doi=10.1007/BF02771592 | issue=2}}.\n*{{MacTutor|class=HistTopics|id=Abstract_linear_spaces|title=Abstract linear spaces|date=1996}}.\n*{{citation|first=Henri|last=Lebesgue|title=Leçons sur l'intégration et la recherche des fonctions primitives|url=https://books.google.com/?id=VfUKAAAAYAAJ&dq=%22Lebesgue%22%20%22Le%C3%A7ons%20sur%20l'int%C3%A9gration%20et%20la%20recherche%20des%20fonctions%20...%22&pg=PA1#v=onepage&q=|year=1904|publisher=Gauthier-Villars}}.\n* {{springer|id=H/h047380|title=Hilbert space|author=B.M. Levitan}}.\n* {{Citation | last1=Marsden | first1=Jerrold E. | author1-link=Jerrold E. Marsden | title=Elementary classical analysis | publisher=W. H. Freeman and Co. | mr=0357693 | year=1974}}.\n* {{citation| last=von Neumann| first=John| authorlink=John von Neumann| title=Allgemeine Eigenwerttheorie Hermitescher Funktionaloperatoren| journal=Mathematische Annalen| volume = 102| pages = 49–131| year = 1929| doi=10.1007/BF01782338}}.\n* {{citation|first=John|last=von Neumann|authorlink=John von Neumann|title=Physical Applications of the Ergodic Hypothesis|year=1932|journal=Proc Natl Acad Sci USA|volume=18|pages=263–266|doi=10.1073/pnas.18.3.263|pmid=16587674|issue=3|pmc=1076204|jstor=86260|bibcode = 1932PNAS...18..263N }}.\n*{{Citation | last1=von Neumann | first1=John | author1-link=John von Neumann | title=Mathematical Foundations of Quantum Mechanics | publisher=[[Princeton University Press]] | series=Princeton Landmarks in Mathematics | isbn=978-0-691-02893-4 | mr=1435976 | publication-date=1996|year=1932| title-link=Mathematical Foundations of Quantum Mechanics }}.\n* {{citation| last=Prugovečki|first=Eduard|title=Quantum mechanics in Hilbert space|publisher=Dover|edition=2nd|year=1981|publication-date=2006|isbn=978-0-486-45327-9}}.\n* {{citation|first=Michael|last=Reed|authorlink=Michael C. Reed|first2=Barry|last2=Simon|authorlink2=Barry Simon|series=Methods of Modern Mathematical Physics|title=Functional Analysis|publisher=Academic Press|year=1980|isbn= 978-0-12-585050-6}}.\n* {{citation|first=Michael|last=Reed|authorlink=Michael C. Reed|first2=Barry|last2=Simon|authorlink2=Barry Simon|title=Fourier Analysis, Self-Adjointness|series=Methods of Modern Mathematical Physics|publisher=Academic Press|year=1975|isbn=9780125850025 }}.\n* {{citation| first=Frigyes|last=Riesz|authorlink=Frigyes Riesz|title=Sur une espèce de Géométrie analytique des systèmes de fonctions sommables|journal=C. R. Acad. Sci. Paris|volume=144|pages=1409–1411|year=1907}}.\n* {{citation|first=Frigyes|last=Riesz|authorlink=Frigyes Riesz|title=Zur Theorie des Hilbertschen Raumes|journal=Acta Sci. Math. Szeged|volume=7|pages=34–38|year=1934}}.\n* {{citation|first=Frigyes|last=Riesz|authorlink=Frigyes Riesz|first2=Béla|last2=Sz.-Nagy|authorlink2=Béla Szőkefalvi-Nagy|title=Functional analysis|publisher=Dover|year=1990|isbn= 978-0-486-66289-3}}.\n* {{citation| first=Walter|last=Rudin|authorlink=Walter Rudin|title=Functional analysis|publisher=Tata MacGraw-Hill|year=1973}}.\n*{{citation|first=Walter|last=Rudin|authorlink=Walter Rudin|title=Real and Complex Analysis|year=1987|publisher=McGraw-Hill|isbn=978-0-07-100276-9}}.\n* {{citation|first=Stanisław|last=Saks|authorlink=Stanisław Saks|title=Theory of the integral|publisher=Dover|year=2005|edition=2nd Dover|isbn=978-0-486-44648-6}}; originally published ''Monografje Matematyczne'', vol. 7, Warszawa, 1937.\n* {{citation| last=Schmidt| first=Erhard|authorlink=Erhard Schmidt|title=Über die Auflösung linearer Gleichungen mit unendlich vielen Unbekannten|journal=Rend. Circ. Mat. Palermo|volume=25|pages=63–77|year=1908| doi=10.1007/BF03029116}}.\n*{{Citation | last1=Shubin | first1=M. A. | title=Pseudodifferential operators and spectral theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Springer Series in Soviet Mathematics | isbn=978-3-540-13621-7 | mr=883081 | year=1987}}.\n*{{Citation | last1=Sobrino | first1=Luis | title=Elements of non-relativistic quantum mechanics | journal=<!-- Citation bot--> | publisher=World Scientific Publishing Co. Inc. | location=River Edge, New Jersey | isbn=978-981-02-2386-1 | mr=1626401 | year=1996| bibcode=1996lnrq.book.....S | doi=10.1142/2865 }}.\n* {{citation|title=Calculus: Concepts and Contexts|edition=3rd|first=James|last=Stewart|publisher=Thomson/Brooks/Cole|year=2006}}.\n*{{citation|last=Stein|first=E|title= Singular Integrals and Differentiability Properties of Functions |publisher=Princeton Univ. Press|year=1970| isbn= 978-0-691-08079-6}}.\n*{{citation|last1=Stein|first1=Elias|authorlink1=Elias Stein|first2=Guido|last2=Weiss|authorlink2=Guido Weiss|title=Introduction to Fourier Analysis on Euclidean Spaces|publisher=Princeton University Press|year=1971|isbn=978-0-691-08078-9|location=Princeton, N.J.}}.\n* {{citation|last1=Streater|first1=Ray|authorlink1=Ray Streater|last2=Wightman|first2=Arthur|authorlink2=Arthur Wightman|title= PCT, Spin and Statistics and All That|year=1964|publisher=W. A. Benjamin, Inc}}.\n* {{cite book| last = Teschl| given = Gerald|authorlink=Gerald Teschl| title=Mathematical Methods in Quantum Mechanics; With Applications to Schrödinger Operators| publisher=[[American Mathematical Society]]| place = [[Providence, Rhode Island|Providence]]| year=2009 |url=http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/ |isbn=978-0-8218-4660-5 }}.\n* {{citation| last=Titchmarsh|first=Edward Charles|authorlink=Edward Charles Titchmarsh|title=Eigenfunction expansions, part 1|year=1946|publisher=Clarendon Press|location=Oxford University}}.\n*{{citation|first=François|last=Trèves|title=Topological Vector Spaces, Distributions and Kernels|publisher=Academic Press|year=1967}}.\n*{{Citation | last1=Warner | first1=Frank | title=Foundations of Differentiable Manifolds and Lie Groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-90894-6 | year=1983}}.\n*{{Citation | last1=Weidmann | first1=Joachim | title=Linear operators in Hilbert spaces | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics | isbn=978-0-387-90427-6 | mr=566954 | year=1980 | volume=68}}.\n* {{citation| last=Weyl| first=Hermann| authorlink=Hermann Weyl| title = The Theory of Groups and Quantum Mechanics| year = 1931| publisher = Dover Press| edition = English 1950| isbn= 978-0-486-60269-1}}.\n* {{citation| last=Young|first=Nicholas|title=An introduction to Hilbert space|publisher=Cambridge University Press|year=1988|zbl=0645.46024|isbn=978-0-521-33071-8}}.\n{{Refend}}\n\n==External links==\n{{Wikibooks|Functional Analysis/Hilbert spaces}}\n* {{springer|title=Hilbert space|id=p/h047380}}\n* [http://mathworld.wolfram.com/HilbertSpace.html Hilbert space at Mathworld]\n* [http://terrytao.wordpress.com/2009/01/17/254a-notes-5-hilbert-spaces/ 245B, notes 5: Hilbert spaces] by [[Terence Tao]]\n\n{{good article}}\n\n{{Functional Analysis}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Hilbert Space}}\n[[Category:Hilbert space|*]]\n[[Category:Linear algebra]]\n[[Category:Operator theory]]\n[[Category:Quantum mechanics]]\n[[Category:Functional analysis]]\n[[Category:David Hilbert]]"
    },
    {
      "title": "Bessel's inequality",
      "url": "https://en.wikipedia.org/wiki/Bessel%27s_inequality",
      "text": "In [[mathematics]], especially [[functional analysis]], '''Bessel's inequality''' is a statement about the coefficients of an element <math>x</math>  in a [[Hilbert space]] with respect to an [[orthonormal]] [[sequence]].\n\nLet <math>H</math> be a Hilbert space, and suppose that <math>e_1, e_2, ...</math> is an orthonormal sequence in <math>H</math>. Then, for any <math>x</math> in <math>H</math> one has\n:<math>\\sum_{k=1}^{\\infty}\\left\\vert\\left\\langle x,e_k\\right\\rangle \\right\\vert^2 \\le \\left\\Vert x\\right\\Vert^2,</math>\n\nwhere 〈•,•〉 denotes the [[inner product space|inner product]] in the Hilbert space <math>H</math>.<ref>{{Cite book|url=https://books.google.com/books?id=QALoZC64ea0C|title=Beginning Functional Analysis|last=Saxe|first=Karen|authorlink= Karen Saxe |date=2001-12-07|publisher=Springer Science & Business Media|isbn=9780387952246|pages=82|language=en}}</ref><ref>{{Cite book|url=https://books.google.com/books?id=XF8W9W-eyrgC|title=Mathematical Analysis II|last=Zorich|first=Vladimir A.|last2=Cooke|first2=R.|date=2004-01-22|publisher=Springer Science & Business Media|isbn=9783540406334|pages=508–509|language=en}}</ref><ref>{{Cite book|url=https://books.google.com/books?id=LBZEBAAAQBAJ|title=Foundations of Signal Processing|last=Vetterli|first=Martin|last2=Kovačević|first2=Jelena|last3=Goyal|first3=Vivek K.|date=2014-09-04|publisher=Cambridge University Press|isbn=9781139916578|pages=83|language=en}}</ref> If we define the infinite sum\n:<math>x' = \\sum_{k=1}^{\\infty}\\left\\langle x,e_k\\right\\rangle e_k, </math>\nconsisting of \"infinite sum\" of [[vector resolute]] <math>x</math> in direction <math>e_k</math>, Bessel's [[inequality (mathematics)|inequality]] tells us that this [[series (mathematics)|series]] [[Limit of a sequence|converges]]. One can think of it that there exists <math>x' \\in H</math> that can be described in terms of potential basis <math>e_1, e_2, \\dots</math>.\n\nFor a complete orthonormal sequence (that is, for an orthonormal sequence that is a [[Orthonormal basis|basis]]), we have [[Parseval's identity]], which replaces the inequality with an equality (and consequently <math>x'</math> with <math>x</math>).\n\nBessel's inequality follows from the identity\n:<math>0 \\le \\left\\| x - \\sum_{k=1}^n \\langle x, e_k \\rangle e_k\\right\\|^2 = \\|x\\|^2 - 2 \\sum_{k=1}^n |\\langle x, e_k \\rangle |^2 + \\sum_{k=1}^n | \\langle x, e_k \\rangle |^2 = \\|x\\|^2 - \\sum_{k=1}^n | \\langle x, e_k \\rangle |^2,</math>\nwhich holds for any natural ''n''.\n\n==See also==\n* [[Cauchy–Schwarz inequality]]\n* [[Parseval's theorem]]\n\n== Notes ==\n{{reflist}}\n\n==External links==\n* {{springer|title=Bessel inequality|id=p/b015850}}\n* [http://mathworld.wolfram.com/BesselsInequality.html Bessel's Inequality] the article on Bessel's Inequality on MathWorld.\n\n{{PlanetMath attribution|title=Bessel inequality|id=3089}}\n\n{{Functional Analysis}}\n\n[[Category:Hilbert space]]\n[[Category:Inequalities]]"
    },
    {
      "title": "Category of finite-dimensional Hilbert spaces",
      "url": "https://en.wikipedia.org/wiki/Category_of_finite-dimensional_Hilbert_spaces",
      "text": "In [[mathematics]], the [[category (mathematics)|category]] '''FdHilb''' has all finite-dimensional [[Hilbert spaces]] for [[object (category theory)|objects]] and the [[linear transformations]] between them as [[morphism]]s.\n\n==Properties==\n\nThis category\n* is [[monoidal category|monoidal]],\n* possesses finite [[biproduct]]s, and\n* is [[dagger compact category|dagger compact]].\n\nAccording to a theorem of Selinger, the category of finite-dimensional Hilbert spaces is complete in the [[dagger compact category]].<ref>P. Selinger, ''[http://www.mscs.dal.ca/~selinger/papers.html#finhilb  Finite dimensional Hilbert spaces are complete for dagger compact closed categories]'', Proceedings of the 5th International Workshop on Quantum Programming Languages, Reykjavik (2008).</ref><ref>M. Hasegawa, M. Hofmann and G. Plotkin, \"Finite dimensional vector spaces are complete for traced symmetric monoidal categories\", LNCS '''4800''', (2008), pp. 367–385.</ref> Many ideas from Hilbert spaces, such as the [[no-cloning theorem]], hold in general for dagger compact categories. See that article for additional details.\n\n==References==\n{{reflist}}\n\n[[Category:Monoidal categories]]\n[[Category:Dagger categories]]\n[[Category:Hilbert space]]\n\n\n{{categorytheory-stub}}"
    },
    {
      "title": "Céa's lemma",
      "url": "https://en.wikipedia.org/wiki/C%C3%A9a%27s_lemma",
      "text": "'''Céa's lemma''' is a [[lemma (mathematics)|lemma]] in [[mathematics]]. Introduced by [[Jean Céa]] in his [[Ph.D.]] dissertation, it is an important tool for proving error estimates for the [[finite element method]] applied to [[elliptic operator|elliptic]] [[partial differential equation]]s.\n\n==Lemma statement==\nLet <math>V</math> be a [[real number|real]] [[Hilbert space]] with the [[norm (mathematics)|norm]] <math>\\|\\cdot\\|.</math> Let <math>a:V\\times V\\to \\mathbb R</math> be a [[bilinear form]] with the  properties\n\n* <math>|a(v, w)| \\le \\gamma \\|v\\|\\,\\|w\\|</math> for some constant <math>\\gamma>0</math>  and all <math>v, w </math> in <math>V</math> ([[continuous function|continuity]])\n* <math>a(v, v) \\ge \\alpha \\|v\\|^2</math> for some constant <math>\\alpha>0</math> and all <math>v</math> in <math>V</math> ([[coercive bilinear form|coercivity]] or <math>V</math>-ellipticity).\n\nLet <math>L:V\\to \\mathbb R</math> be a [[bounded linear operator]]. Consider the problem of finding an element <math>u</math> in <math>V</math> such that\n\n: <math>a(u, v)=L(v)</math> for all <math>v</math> in <math>V.</math>\n\nConsider the same problem on a finite-dimensional subspace <math>V_h</math> of <math>V,</math> so, <math>u_h</math> in <math>V_h</math> satisfies\n\n: <math>a(u_h, v)=L(v)</math> for all <math>v</math> in <math>V_h.</math>\n\nBy the [[Lax–Milgram theorem]], each of these problems has exactly one solution. '''Céa's lemma''' states that\n\n: <math>\\|u-u_h\\|\\le \\frac{\\gamma}{\\alpha}\\|u-v\\|</math>  for all <math>v</math> in <math>V_h.</math>\n\nThat is to say, the subspace solution <math>u_h</math> is \"the best\" approximation of  <math>u</math> in <math>V_h,</math> [[up to]] the constant <math>\\gamma/\\alpha.</math>\n\nThe proof is straightforward \n: <math>\\alpha\\|u-u_h\\|^2 \\le a(u-u_h,u-u_h) = a(u-u_h,u-v) + a(u-u_h,v - u_h) = a(u-u_h,u-v)\n  \\le \\gamma\\|u-u_h\\|\\|u-v\\|</math>  for all <math>v</math> in <math>V_h.</math>\nWe used the <math>a</math>-orthogonality of <math>u-u_h</math> and <math>V_h</math>\n: <math>a(u-u_h,v) = 0, \\ \\forall \\ v</math> in <math>V_h</math>\nwhich follows directly from <math>V_h \\subset V</math> \n: <math>a(u, v) = L(v) = a(u_h, v)</math> for all <math>v</math> in <math>V_h</math>.\n\n'''Note:''' Céa's lemma holds on [[complex number|complex]] Hilbert spaces also, one then uses a  [[sesquilinear form]] <math>a(\\cdot, \\cdot)</math> instead of a bilinear one. The coercivity assumption then becomes <math>|a(v, v)| \\ge \\alpha \\|v\\|^2</math> for all <math>v</math> in <math>V</math> (notice the absolute value sign around <math>a(v, v)</math>).\n\n==Error estimate in the energy norm==\n[[File:Cea lemma illust.svg|right|thumb|The subspace solution <math>u_h</math> is the projection of <math>u</math> onto the subspace <math>V_h</math> in respect to the inner product <math>a(\\cdot, \\cdot)</math>.]]\nIn many applications, the bilinear form <math>a:V\\times V\\to \\mathbb R</math> is symmetric, so\n\n: <math>a(v, w) =a(w, v)</math> for all <math>v, w </math> in <math>V.</math>\n\nThis, together with the above properties of this form, implies that  <math>a(\\cdot, \\cdot)</math> is an [[inner product]] on <math>V.</math> The resulting norm\n\n: <math>\\|v\\|_a=\\sqrt{a(v, v)}</math>\n\nis called the '''[[energy norm]]''', since it corresponds to a [[energy (physics)|physical energy]] in many problems. This norm is equivalent to the original norm <math>\\|\\cdot\\|.</math>\n\nUsing the <math>a</math>-orthogonality of <math>u-u_h</math> and <math>V_h</math> and the [[Cauchy–Schwarz inequality]]\n: <math>\\|u-u_h\\|_a^2 = a(u-u_h,u-u_h) = a(u-u_h,u-v) \\le \\|u-u_h\\|_a \\cdot \\|u-v\\|_a</math> for all <math>v</math> in <math>V_h</math>.\n\nHence, in the energy norm, the inequality in Céa's lemma becomes\n\n: <math>\\|u-u_h\\|_a\\le \\|u-v\\|_a</math>  for all <math>v</math> in <math>V_h</math>\n\n(notice that the constant <math>\\gamma/\\alpha</math> on the right-hand side is no longer present).\n\nThis states that the subspace solution <math>u_h</math> is the best approximation to the full-space solution <math>u</math> in respect to the energy norm. Geometrically, this means that <math>u_h</math> is the [[projection (linear algebra)|projection]] of the solution <math>u</math> onto the subspace <math>V_h</math> in respect to the inner product <math>a(\\cdot, \\cdot)</math> (see the adjacent picture).\n\nUsing this result, one can also derive a sharper estimate in the norm <math>\\| \\cdot \\|</math>. Since\n: <math>\\alpha \\|u-u_h\\|^2 \\le a(u-u_h,u-u_h) = \\|u-u_h\\|_a^2 \\le \\|u - v\\|_a^2 \\le \\gamma \\|u-v\\|^2</math>  for all <math>v</math> in <math>V_h</math>,\nit follows that\n: <math>\\|u-u_h\\| \\le \\sqrt{\\frac{\\gamma}{\\alpha}} \\|u-v\\|</math>  for all <math>v</math> in <math>V_h</math>.\n\n==An application of Céa's lemma==\nWe will apply Céa's lemma to estimate the error of calculating the solution to an [[elliptic partial differential equation|elliptic differential equation]] by the [[finite element method]].\n\n[[File:String illust.svg|right|thumb|A string with fixed endpoints under the influence of a force pointing down.]]\nConsider the problem of finding a function <math>u:[a, b]\\to \\mathbb R</math> satisfying the conditions \n:<math>\\begin{cases} \n-u''=f \\mbox { in } [a, b]  \\\\\nu(a)=u(b)=0   \n\\end{cases}\n</math>\nwhere <math>f:[a, b]\\to \\mathbb R</math> is a given [[continuous function]].\n\nPhysically, the solution <math>u</math> to this two-point [[boundary value problem]] represents the shape taken by a [[rope|string]] under the influence of a force such that at every point <math>x</math>  between <math>a</math> and <math>b</math> the [[force density]] is <math>f(x)\\mathbf{e}</math> (where  <math>\\mathbf{e}</math> is a [[unit vector]] pointing vertically, while the endpoints of the string are on a horizontal line, see the adjacent picture). For example, that force may be the [[gravity]], when <math>f</math> is a constant function (since the gravitational force is the same at all points).\n\nLet the Hilbert space <math>V</math> be the [[Sobolev space]] <math>H^1_0(a, b),</math> which is the space of all [[square-integrable function]]s <math>v</math> defined on <math>[a, b]</math> that have a [[weak derivative]] on <math>[a, b]</math> with <math>v'</math> also being square integrable, and <math>v</math> satisfies the conditions <math>v(a)=v(b)=0.</math> The inner product on this space is\n\n: <math>(v, w)=\\int_a^b\\! v'(x) w'(x)\\,dx</math> for all <math>v</math> and <math>w</math> in <math>V.</math>\n\nAfter multiplying the original boundary value problem by <math>v</math> in this space and performing an [[integration by parts]], one obtains the equivalent problem\n\n: <math>a(u, v)=L(v)</math> for all <math>v</math> in <math>V,</math>\n\nwith\n\n: <math>a(u, v)=\\int_a^b\\! u'(x) v'(x)\\,dx</math>\n\n(here the bilinear form is given by the same expression as the inner product, this is not always the case), and\n\n:<math>L(v) = \\int_a^b\\! f(x) v(x) \\, dx.</math>\n\nIt can be shown that the bilinear form <math>a(\\cdot, \\cdot)</math> and the operator <math>L</math> satisfy the assumptions of Céa's lemma.\n\n[[File:Finite element method 1D illustration2.svg|right|thumb|A function in <math>V_h</math> (in red), and the typical collection of basis functions in <math>V_h</math> (in blue).]]\nIn order to determine a finite-dimensional subspace <math>V_h</math> of <math>V,</math> consider a [[partition of an interval|partition]]\n\n:<math>a=x_0< x_1 < \\cdots < x_{n-1} < x_n = b</math>\n\nof the interval <math>[a, b],</math> and let <math>V_h</math> be the space of all continuous functions that are [[affine function|affine]] on each subinterval in the partition (such functions are called [[piecewise-linear function|piecewise-linear]]). In addition, assume that any function in <math>V_h</math> takes the value 0 at the endpoints of <math>[a, b].</math> It follows that <math>V_h</math> is a vector subspace of <math>V</math> whose dimension is <math>n-1</math> (the number of points in the partition that are not endpoints).\n\nLet <math>u_h</math> be the solution to the subspace problem\n\n: <math>a(u_h, v)=L(v)</math> for all <math>v</math> in <math>V_h,</math>\n\nso one can think of <math>u_h</math> as of a piecewise-linear approximation to the exact solution <math>u.</math> By Céa's lemma, there exists a constant <math>C>0</math> dependent only on the bilinear form <math>a(\\cdot, \\cdot),</math> such that\n\n: <math>\\|u-u_h\\|\\le C \\|u-v\\|</math>  for all <math>v</math> in <math>V_h.</math>\n\nTo explicitly calculate the error between <math>u</math> and <math>u_h,</math> consider the function <math>\\pi u</math> in <math>V_h</math> that has the same values as <math>u</math> at the nodes of the partition (so <math>\\pi u</math> is obtained by linear interpolation on each interval <math>[x_i, x_{i+1}]</math> from the values of <math>u</math> at interval's endpoints). It can be shown using [[Taylor's theorem]] that there exists a constant <math>K</math> that depends only on the endpoints <math>a</math> and <math>b,</math> such that\n\n: <math>|u'(x)-(\\pi u)'(x)|\\le K h \\|u''\\|_{L^2(a, b)}</math>\n\nfor all <math>x</math> in <math>[a, b],</math> where <math>h</math> is the largest length of the subintervals <math>[x_i, x_{i+1}]</math> in the partition, and the norm on the right-hand side is the [[Lp space|L<sup>2</sup> norm]].\n\nThis inequality then yields an estimate for the error\n\n: <math>\\|u-\\pi u\\|.</math>\n\nThen, by substituting <math>v=\\pi u</math> in Céa's lemma it follows that\n\n: <math>\\|u-u_h\\|\\le C h \\|u''\\|_{L^2(a, b)}, </math>\n\nwhere <math>C</math> is a different constant from the above (it depends only on the bilinear form, which implicitly depends on the interval <math>[a, b]</math>).\n\nThis result is of a fundamental importance, as it states that the finite element method can be used to approximately calculate the solution of our problem, and that the error in the computed solution decreases proportionately to the partition size <math>h.</math>  Céa's lemma can be applied along the same lines to derive error estimates for finite element problems in higher dimensions (here the domain of <math>u</math> was in one dimension), and while using higher order [[polynomial]]s for the subspace <math>V_h.</math>\n\n==References==\n\n*{{cite book\n | last       = Céa\n | first      = Jean\n | title      = Approximation variationnelle des problèmes aux limites\n | type       = PhD thesis\n | series     = Annales de l'institut Fourier 14\n | volume     = 2\n | pages      = 345–444\n | year       = 1964\n | url        = http://archive.numdam.org/article/AIF_1964__14_2_345_0.pdf\n | format     = PDF\n | accessdate = 2010-11-27\n}} (Original work from J. Céa)\n\n*{{cite book\n | last       = Johnson\n | first      = Claes\n | title      = Numerical solution of partial differential equations by the finite element method\n | publisher  = Cambridge University Press\n | date       = 1987\n | pages      = \n | isbn       = 0-521-34514-6\n}}\n\n*{{cite book\n | last       = Monk\n | first      = Peter\n | title      = Finite element methods for Maxwell's equations\n | publisher  = Oxford University Press\n | date       = 2003\n | pages      = \n | isbn       = 0-19-850888-3\n}}\n\n*{{cite book\n | last       = Roos\n | first      = H.-G. |author2=Stynes, M. |author3=Tobiska, L. \n | title      = Numerical methods for singularly perturbed differential equations: convection-diffusion and flow problems\n | publisher  = Berlin; New York: Springer-Verlag\n | date       = 1996\n | pages      = \n | isbn       = 3-540-60718-8\n}}\n\n*{{cite book\n | last       = Eriksson\n | first      = K. |author2=Estep, D. |author3=Hansbo, P. |author4=Johnson, C.\n | title      = Computational differential equations\n | publisher  = Cambridge; New York: Cambridge University Press\n | date       = 1996\n | pages      = \n | isbn       = 0-521-56738-6\n}}\n\n*{{cite book\n | last       = Zeidler\n | first      = Eberhard\n | title      = Applied functional analysis: applications to mathematical physics\n | publisher  = New York: Springer-Verlag\n | date       = 1995\n | pages      = \n | isbn       = 0-387-94442-7\n}}\n\n* {{cite book | last=Brenner | first=Susanne C. | authorlink = Susanne Brenner |author2=L. Ridgeway Scott | title=The mathematical theory of finite element methods | year=2002 | edition=2nd | isbn=0-387-95451-1 | oclc=48892839}}\n* {{cite book | last=Ciarlet | first=Philippe G. | title=The finite element method for elliptic problems | year=2002 | edition=(SIAM Classics reprint) | isbn=0-89871-514-8 | oclc=48892573}}\n\n{{DEFAULTSORT:Ceas lemma}}\n[[Category:Numerical differential equations]]\n[[Category:Hilbert space]]\n[[Category:Lemmas]]"
    },
    {
      "title": "Compact operator on Hilbert space",
      "url": "https://en.wikipedia.org/wiki/Compact_operator_on_Hilbert_space",
      "text": "{{Cleanup rewrite|it is written like a maths textbook, not an encyclopedia article|article|date=September 2017}}\nIn [[functional analysis]], the concept of a '''[[compact operator]] on [[Hilbert space]]''' is an extension of the concept of a matrix acting on a finite-dimensional vector space; in Hilbert space, compact operators are precisely the closure of [[finite-rank operator]]s (representable by finite-dimensional matrices) in the [[topology]] induced by the [[operator norm]]. As such, results from matrix theory can sometimes be extended to compact operators using similar arguments. By contrast, the study of general operators on infinite-dimensional spaces often requires a genuinely different approach.\n\nFor example, the [[spectral theory of compact operators]] on [[Banach space]]s takes a form that is very similar to the [[Jordan canonical form]] of matrices. In the context of Hilbert spaces, a square matrix is unitarily diagonalizable if and only if it is [[normal operator|normal]]. A corresponding result holds for normal compact operators on Hilbert spaces. More generally, the compactness assumption can be dropped. But, as stated above, the techniques used to prove e.g. the [[spectral theorem]] are different, involving operator valued [[measure (mathematics)|measure]]s on the [[Spectrum (functional analysis)|spectrum]].\n\nSome results for compact operators on Hilbert space will be discussed, starting with general properties before considering subclasses of compact operators.\n\n== Definition ==\nLet <math>H</math> be a Hilbert space and <math>L(H)</math> be the set of [[bounded operator]]s on ''<math>H</math>''. Then, an operator <math>T\\in L(H)</math> is said to be a '''compact operator''' if the image of each bounded set under <math>T</math> is [[Relatively compact subspace|relatively compact]].\n\n== Some general properties ==\nWe list in this section some general properties of compact operators.\n\nIf ''X'' and ''Y'' are Hilbert spaces (in fact ''X'' Banach and ''Y'' normed will suffice), then ''T:'' ''X'' → ''Y'' is compact if and only if it is continuous when viewed as a map from ''X'' with the [[Weak convergence (Hilbert space)|weak topology]] to ''Y'' (with the norm topology). (See {{harv|Zhu|2007|loc=Theorem 1.14, p.11}}, and note in this reference that the uniform boundedness will apply in the situation where ''F'' ⊆ ''X'' satisfies (∀φ ∈ Hom(''X'', ''K'')) sup{''x**''(φ) = φ(''x''):''x''} < ∞, where ''K'' is the underlying field. The uniform boundedness principle applies since Hom(''X'', ''K'') with the norm topology will be a Banach space, and the maps ''x**'':Hom(''X'',''K'') → ''K'' are continuous homomorphisms with respect to this topology.)\n\nThe family of compact operators is a norm-closed, two-sided, *-ideal in ''L''(''H''). Consequently, a compact operator ''T'' cannot have a bounded inverse if ''H'' is infinite-dimensional. If ''ST'' = ''TS'' = ''I'', then the identity operator would be compact, a contradiction.\n\nIf a sequence of bounded operators ''S<sub>n</sub>'' → ''S'' in the [[strong operator topology]] and ''T'' is compact, then ''S<sub>n</sub>T'' converges to ''ST'' in norm{{Citation needed|date=May 2017}}. For example, consider the Hilbert space ''l''<sup>2</sup>('''N'''), with standard basis {''e<sub>n</sub>''}. Let ''P<sub>m</sub>'' be the orthogonal projection on the linear span of {''e''<sub>1</sub> ... ''e<sub>m</sub>''}. The sequence {''P<sub>m</sub>''} converges to the identity operator ''I'' strongly but not uniformly. Define ''T'' by ''Te<sub>n</sub>'' = (1/''n'')<sup>2</sup> · ''e<sub>n</sub>''. ''T'' is compact, and, as claimed above, ''P<sub>m</sub>T'' → ''I T'' = ''T'' in the uniform operator topology: for all ''x'',\n\n:<math>\\left\\| P_m T x - T x \\right \\| \\leq \\left( \\frac{1}{m+1}\\right)^2 \\| x \\|.</math>\n\nNotice each ''P<sub>m</sub>'' is a finite-rank operator. Similar reasoning shows that if ''T'' is compact, then ''T'' is the uniform limit of some sequence of finite-rank operators.\n\nBy the norm-closedness of the ideal of compact operators, the converse is also true.\n\nThe quotient C*-algebra of ''L''(''H'') modulo the compact operators is called the [[Calkin algebra]], in which one can consider properties of an operator up to compact perturbation.\n\n== Compact self adjoint operator ==\nA bounded operator ''T'' on a Hilbert space ''H'' is said to be [[self-adjoint operator|self-adjoint]] if ''T'' = ''T*'', or equivalently,\n\n:<math>\\langle T x, y \\rangle = \\langle x, T y \\rangle, \\quad x, y \\in H.</math>\n\nIt follows that <''Tx'', ''x''> is real for every ''x'' ∈ ''H'', thus eigenvalues of ''T'', when they exist, are real.  When a closed linear subspace ''L'' of ''H'' is invariant under ''T'', then the restriction of ''T'' to ''L'' is a self-adjoint operator on ''L'', and furthermore, the [[orthogonal complement]] ''L''<sup>&perp;</sup> of ''L'' is also invariant under ''T''.  For example, the space ''H'' can be decomposed as orthogonal [[direct sum]] of two ''T''&ndash;invariant closed linear subspaces: the [[Kernel (linear operator)|kernel]] of ''T'', and the orthogonal complement {{nowrap|(ker ''T'')<sup>&perp;</sup>}} of the kernel (which is equal to the closure of the range of ''T'', for any bounded self-adjoint operator).  These basic facts play an important role in the proof of the spectral theorem below.\n\nThe classification result for Hermitian {{nowrap|''n'' × ''n''}} matrices is the [[spectral theorem]]: If ''M'' = ''M*'', then ''M'' is unitarily diagonalizable and the diagonalization of ''M'' has real entries. Let ''T'' be a compact self adjoint operator on a Hilbert space ''H''. We will prove the same statement for ''T'': the operator ''T'' can be diagonalized by an orthonormal set of eigenvectors, each of which corresponds to a real eigenvalue.\n\n=== Spectral theorem ===\n'''Theorem''' For every compact self-adjoint operator ''T'' on a real or complex Hilbert space ''H'', there exists an [[orthonormal basis]] of ''H'' consisting of eigenvectors of ''T''.  More specifically, the orthogonal complement of the kernel of ''T'' admits, either a finite orthonormal basis of eigenvectors of ''T'', or a [[Countable set|countably infinite]] orthonormal basis {''e<sub>n</sub>''} of eigenvectors of ''T'', with corresponding eigenvalues {{nowrap|{''λ<sub>n</sub>''} ⊂ '''R'''}}, such that {{nowrap|''λ<sub>n</sub>'' → 0}}.\n\nIn other words, a compact self-adjoint operator can be unitarily diagonalized.  This is the spectral theorem.\n\nWhen ''H'' is [[Separable space|separable]], one can mix the basis {''e<sub>n</sub>''} with a [[Countable set|countable]] orthonormal basis for the kernel of ''T'', and obtain an orthonormal basis {''f''<sub>''n''</sub>} for ''H'', consisting of eigenvectors of ''T'' with real eigenvalues {μ<sub>''n''</sub>} such that {{nowrap|μ<sub>''n''</sub> → 0}}.\n\n'''Corollary''' For every compact self-adjoint operator ''T'' on a real or complex separable infinite-dimensional Hilbert space ''H'', there exists a countably infinite orthonormal basis {''f<sub>n</sub>''} of ''H'' consisting of eigenvectors of ''T'', with corresponding eigenvalues {{nowrap|{μ<sub>''n''</sub>} ⊂ '''R'''}}, such that {{nowrap|μ<sub>''n''</sub> → 0}}.\n\n==== The idea ====\nProving the spectral theorem for a Hermitian ''n'' × ''n'' matrix ''T'' hinges on showing the existence of one eigenvector ''x''. Once this is done, Hermiticity implies that both the linear span and orthogonal complement of ''x'' are invariant subspaces of ''T''. The desired result is then obtained by iteration. The existence of an eigenvector can be shown in at least two ways:\n\n#One can argue algebraically: The characteristic polynomial of ''T'' has a complex root, therefore ''T'' has an eigenvalue with a corresponding eigenvector. Or,\n#The eigenvalues can be characterized variationally: The largest eigenvalue is the maximum on the closed unit ''sphere'' of the function {{nowrap|''f'': '''R'''<sup>2''n''</sup> → '''R'''}} defined by ''f''(''x'') = ''x*Tx'' = <''Tx'', x>.\n\n'''Note.''' In the finite-dimensional case, part of the first approach works in much greater generality; any square matrix, not necessarily Hermitian, has an eigenvector. This is  simply not true for general  operators on Hilbert spaces.\n\nThe spectral theorem for the compact self adjoint case can be obtained analogously: one finds an eigenvector by extending the second finite-dimensional argument above, then apply induction.  We first sketch the argument for matrices.\n\nSince the closed unit sphere ''S'' in '''R'''<sup>2''n''</sup> is compact, and ''f'' is continuous, ''f''(''S'') is compact on the real line, therefore ''f'' attains a maximum on ''S'', at some unit vector ''y''. By [[Lagrange multipliers|Lagrange's multiplier]] theorem, ''y'' satisfies\n\n:<math>\\nabla f = \\nabla \\; y^* T y = \\lambda \\cdot \\nabla \\; y^* y</math>\n\nfor some λ. By Hermiticity, {{nowrap|''Ty'' {{=}} λ''y''}}.\n\nHowever, the Lagrange multipliers do not generalize easily to the infinite-dimensional case. Alternatively, let ''z'' ∈ '''C'''<sup>''n''</sup> be any vector. Notice that if a unit vector ''y'' maximizes <''Tx'', ''x''> on the unit sphere (or on the unit ball), it also maximizes the [[Rayleigh quotient]]:\n\n:<math>g(x) = \\frac{\\langle Tx, x \\rangle}{\\|x\\|^2}, \\qquad 0 \\ne x \\in \\mathbf{C}^n.</math>\n\nConsider the function:\n\n:<math>\\begin{cases} h : \\mathbf{R} \\to \\mathbf{R} \\\\ h(t) = g(y+tz) \\end{cases}</math>\n\nBy calculus, {{nowrap|''h''′(0) {{=}} 0}}, ''i.e.'',\n\n:<math>\\begin{align}\nh'(0) &= \\lim_{t \\to 0} \\frac{h(t) - h(0)}{t - 0} \\\\\n&= \\lim_{t \\to 0} \\frac{g(y+tz) - g(y)}{t} \\\\\n&= \\lim_{t \\to 0} \\frac{1}{t} \\left (\\frac{\\langle T(y+tz), y+tz \\rangle}{\\|y+tz\\|^2} - \\frac{\\langle Ty, y \\rangle}{\\|y\\|^2} \\right ) \\\\\n&= \\lim_{t \\to 0} \\frac{1}{t} \\left (\\frac{\\langle T(y+tz), y+tz \\rangle - \\langle Ty, y \\rangle}{\\|y\\|^2} \\right ) \\\\\n&= \\frac{1}{\\|y\\|^2} \\lim_{t \\to 0}  \\frac{\\langle T(y+tz), y+tz \\rangle - \\langle Ty, y \\rangle}{t} \\\\\n&= \\frac{1}{\\|y\\|^2} \\left (\\frac{d}{dt} \\frac{\\langle T (y + t z), y + tz \\rangle}{\\langle y + tz, y + tz \\rangle} \\right)(0) \\\\\n&= 0.\n\\end{align}</math>\n\nDefine:\n\n:<math>m=\\frac{\\langle Ty, y \\rangle}{\\langle y, y \\rangle}</math>\n\nAfter some algebra the above expression becomes (''Re'' denotes the real part of a complex number)\n\n:<math>\\Re \\left (\\langle T y - m y, z \\rangle \\right) = 0.</math>\n\nBut ''z'' is arbitrary, therefore {{nowrap|''Ty'' − ''my'' {{=}} 0}}. This is the crux of proof for spectral theorem in the matricial case.\n\n==== Details ====\n'''Claim'''&nbsp;  If ''T'' is a compact self-adjoint operator on a non-zero Hilbert space ''H'' and\n\n:<math>m(T) := \\sup \\bigl\\{ |\\langle T x, x \\rangle| : x \\in H, \\, \\|x\\| \\le 1 \\bigr\\},</math>\n\nthen ''m''(''T'') or −''m''(''T'') is an eigenvalue of ''T''.\n\nIf {{nowrap|''m''(''T'') {{=}} 0}}, then ''T'' = 0 by the [[polarization identity]], and this case is clear.  Consider the function\n\n:<math>\\begin{cases} f : H \\to \\mathbf{R} \\\\ f(x) = \\langle T x, x \\rangle \\end{cases}</math>\n\nReplacing ''T'' by −''T'' if necessary, one may assume that the supremum of ''f'' on the closed unit ball ''B'' ⊂ ''H'' is equal to {{nowrap|''m''(''T'') > 0}}.  If ''f'' attains its maximum ''m''(''T'') on ''B'' at some unit vector ''y'', then, by the same argument used for matrices, ''y'' is an eigenvector of ''T'', with corresponding eigenvalue {{nowrap|λ {{=}} < λ''y'', ''y''>}} = {{nowrap|<''Ty'', ''y''> {{=}} ''f''(''y'') {{=}} ''m''(''T'')}}.\n\nBy the [[Banach–Alaoglu theorem]] and the reflexivity of ''H'', the closed unit ball ''B'' is weakly compact.  Also, the compactness of ''T'' means (see above) that ''T'' : ''X'' with the weak topology → ''X'' with the norm topology, is continuous. These two facts imply that ''f'' is continuous on ''B'' equipped with the weak topology, and ''f'' attains therefore its maximum ''m'' on ''B'' at some {{nowrap|''y'' ∈ ''B''}}.  By maximality, ||''y''|| = 1, which in turn implies that ''y'' also maximizes the Rayleigh quotient ''g''(''x'') (see above).  This shows that ''y'' is an eigenvector of ''T'', and ends the proof of the claim.\n\n'''Note.''' The compactness of ''T'' is crucial. In general, ''f'' need not be continuous for the weak topology on the unit ball ''B''. For example, let ''T'' be the identity operator, which is not compact when ''H'' is infinite-dimensional.  Take any orthonormal sequence {''y<sub>n</sub>''}. Then ''y<sub>n</sub>'' converges to 0 weakly, but lim ''f''(''y<sub>n</sub>'') = 1 ≠ 0 = ''f''(0).\n\nLet ''T'' be a compact operator on a Hilbert space ''H''.  A finite (possibly empty) or countably infinite orthonormal sequence  {''e<sub>n</sub>''} of eigenvectors of ''T'', with corresponding non-zero eigenvalues, is constructed by induction as follows. Let ''H''<sub>0</sub> = ''H'' and ''T''<sub>0</sub> = ''T''.  If ''m''(''T''<sub>0</sub>) = 0, then ''T'' = 0 and the construction stops without producing any eigenvector ''e<sub>n</sub>''.  Suppose that orthonormal eigenvectors {{nowrap|''e''<sub>0</sub>, &hellip;, ''e''<sub>''n'' − 1</sub>}} of ''T'' have been found.  Then  {{nowrap|''E<sub>n</sub>'':{{=}} span(''e''<sub>0</sub>, &hellip;, ''e''<sub>''n'' − 1</sub>)}} is invariant under ''T'', and by self-adjointness,  the orthogonal  complement ''H<sub>n</sub>'' of ''E''<sub>''n''</sub> is an invariant subspace of ''T''.  Let ''T<sub>n</sub>'' denote the restriction of ''T'' to ''H<sub>n</sub>''. If ''m''(''T<sub>n</sub>'') = 0, then ''T<sub>n</sub>'' = 0, and the construction stops. Otherwise, by the ''claim'' applied to ''T<sub>n</sub>'', there is a norm one eigenvector ''e<sub>n</sub>'' of ''T'' in ''H''<sub>''n''</sub>, with corresponding non-zero eigenvalue λ<sub>''n''</sub> = {{nowrap|&plusmn; ''m''(''T<sub>n</sub>'')}}.\n\nLet ''F'' =  (span{''e<sub>n</sub>''})<sup>&perp;</sup>, where {''e<sub>n</sub>''} is the finite or infinite sequence constructed by the inductive process; by self-adjointness, ''F'' is  invariant under ''T''.  Let ''S'' denote the restriction of ''T'' to ''F''.  If the process was stopped after finitely many steps, with a last vector ''e''<sub>''m''−1</sub>, then  ''F''= ''H<sub>m</sub>'' and ''S'' = ''T<sub>m</sub>'' = 0 by construction.  In the infinite case, compactness of ''T'' and the weak-convergence of ''e<sub>n</sub>'' to 0 imply that {{nowrap|''Te<sub>n</sub>'' {{=}} λ<sub>''n''</sub>''e<sub>n</sub>'' → 0}},  therefore {{nowrap|λ<sub>''n''</sub> → 0}}. Since ''F'' is contained in ''H<sub>n</sub>'' for every ''n'', it  follows that ''m''(''S'') ≤ ''m''({''T<sub>n</sub>''}) = |λ<sub>''n''</sub>| for every ''n'', hence ''m''(''S'') = 0.  This implies again that  {{nowrap|''S'' {{=}} 0}}.\n\nThe fact that ''S'' = 0 means that ''F'' is contained in the kernel of ''T''.  Conversely, if ''x'' ∈ ker(''T''), then by  self-adjointness, ''x'' is orthogonal to every eigenvector {''e<sub>n</sub>''} with non-zero eigenvalue.  It follows that {{nowrap|''F'' {{=}} ker(''T'')}}, and that {''e<sub>n</sub>''} is an orthonormal basis for the orthogonal complement of the kernel of ''T''.  One can complete the  diagonalization of ''T'' by selecting an orthonormal basis of the kernel.  This proves the spectral theorem.\n\nA shorter but more abstract proof goes as follows: by [[Zorn's lemma]], select ''U'' to be a maximal subset of ''H'' with the following three properties: all elements of ''U'' are eigenvectors of ''T'', they have norm one, and any two distinct elements of ''U'' are orthogonal.  Let ''F'' be the orthogonal complement of the linear span of ''U''.  If ''F'' ≠ {0}, it is a non-trivial invariant subspace of ''T'', and by the initial claim there must exist a norm one eigenvector ''y'' of ''T'' in ''F''. But then ''U'' &cup; {''y''} contradicts the maximality of ''U''.  It follows that ''F'' = {0}, hence span(''U'') is dense in ''H''.  This shows that ''U'' is an orthonormal basis of ''H'' consisting of eigenvectors of ''T''.\n\n=== Functional calculus ===\nIf ''T'' is compact on an infinite-dimensional Hilbert space ''H'', then ''T'' is not invertible, hence σ(''T''), the spectrum of ''T'', always contains 0.  The spectral theorem shows that σ(''T'') consists of the eigenvalues {λ<sub>''n''</sub>} of ''T'', and of 0 (if 0 is not already an eigenvalue). The set σ(''T'') is a compact subset of the real line, and the eigenvalues are dense in σ(''T'').\n\nAny spectral theorem can be reformulated in terms of a [[functional calculus]]. In the present context we have:\n\n'''Theorem.''' Let ''C''(σ(''T'')) denote the [[C*-algebra]] of continuous functions on σ(''T''). There exists a unique isometric homomorphism {{nowrap|Φ : ''C''(σ(''T'')) → ''L''(''H'')}} such that Φ(1) = ''I'' and, if ''f'' is the identity function ''f''(λ) = λ, then {{nowrap|Φ(''f'') {{=}} ''T''}}. Moreover, {{nowrap|σ(''f''(''T'')) {{=}} ''f''(σ(''T''))}}.\n\nThe functional calculus map Φ is defined in a natural way: let {''e<sub>n</sub>''} be an orthonormal basis of eigenvectors for ''H'', with corresponding eigenvalues {λ<sub>''n''</sub>}; for {{nowrap|''f'' ∈ ''C''(σ(''T''))}}, the operator Φ(''f''), diagonal with respect to the orthonormal basis {''e<sub>n</sub>''}, is defined by setting\n\n:<math>\\Phi(f)(e_n) = f(\\lambda_n) e_n</math>\n\nfor every ''n''.  Since  Φ(''f'') is diagonal with respect to an orthonormal basis, its norm is equal to the supremum of the modulus of diagonal coefficients,\n\n:<math>\\|\\Phi(f)\\| = \\sup_{\\lambda_n \\in \\sigma(T)} |f(\\lambda_n)| = \\|f\\|_{C(\\sigma(T))}.</math>\n\nThe other properties of Φ can be readily verified.  Conversely, any homomorphism Ψ satisfying the requirements of the theorem must coincide with Φ when ''f'' is a polynomial. By the [[Stone–Weierstrass theorem|Weierstrass approximation theorem]], polynomial functions are dense in ''C''(σ(''T'')), and it follows that {{nowrap|Ψ {{=}} Φ}}.  This shows that Φ is unique.\n\nThe more general [[continuous functional calculus]] can be defined for any self-adjoint (or even normal, in the complex case) bounded linear operator on a Hilbert space.  The compact case, described here, is a particularly simple instance of this functional calculus.\n\n=== Simultaneous diagonalisation ===\nConsider an Hilbert space ''H'' (e.g. the finite-dimensional '''C'''<sup>''n''</sup>), and a commuting set <math>\\mathcal{F}\\subseteq\\operatorname{Hom}(H,H)</math> of self-adjoint operators. Then under suitable conditions, can be simultaneously (unitarily) diagonalised. ''Viz.'', there exists an orthonormal basis ''Q'' consisting of common eigenvectors for the operators — ''i.e.''\n\n:<math>(\\forall{q\\in Q,T\\in\\mathcal{F}})~(\\exists{\\sigma\\in\\mathbf{C}})~(T-\\sigma)q=0</math>\n\n'''Lemma.''' Suppose all the operators in <math>\\mathcal{F}</math> are compact. Then every closed non-zero <math>\\mathcal{F}</math>-invariant sub-space ''S'' ⊆ ''H'' has a common eigenvector for <math>\\mathcal{F}</math>.\n\n'''Proof.''' ''Case I:'' all the operators have each exactly one eigenvalue. Then take any <math>s\\in S</math> of unit length. This is a common eigenvector.\n\n''Case II:'' there is some operator <math>T\\in\\mathcal{F}</math> with at least 2 eigenvalues and let <math>0 \\neq \\alpha \\in \\sigma(T\\upharpoonright S)</math>. Since ''T'' is compact and α is non-zero, we have <math>S':=\\ker(T\\upharpoonright S-\\alpha)</math> is a finite-dimensional (and therefore closed) non-zero <math>\\mathcal{F}</math>-invariant sub-space (because the operators all commute with ''T'', we have for <math>T'\\in\\mathcal{F}</math> and <math>x\\in\\ker(T\\upharpoonright S-\\alpha)</math>, that <math>(T-\\alpha)(T'x)=(T'(T~x)-\\alpha T'x)=0</math>). In particular we definitely have <math>\\dim~S'<\\dim~S</math>. Thus we could in principle argue by induction over dimension, yielding that <math>S'\\subseteq S</math> has a common eigenvector for <math>\\mathcal{F}</math>.\n\n'''Theorem 1.''' If all the operators in <math>\\mathcal{F}</math> are compact then the operators can be simultaneously (unitarily) diagonalised.\n\n'''Proof.''' The following set\n\n:<math>\\mathbf{P}=\\{ A \\subseteq H : A \\text{ is an orthonormal set of common eigenvectors for } \\mathcal{F}\\},</math>\n\nis partially ordered by inclusion. This clearly has the Zorn property. So taking ''Q'' a maximal member, if ''Q'' is a basis for the whole hilbert space ''H'', we are done. If this were not the case, then letting <math>S={\\langle Q\\rangle}^{\\bot}</math>, it is easy to see that this would be an <math>\\mathcal{F}</math>-invariant non-trivial closed subspace; and thus by the lemma above, therein would lie a common eigenvector for the operators (necessarily orthogonal to ''Q''). But then there would then be a proper extension of ''Q'' within '''P'''; a contradiction to its maximality.\n\n'''Theorem 2.''' If there is an injective compact operator in <math>\\mathcal{F}</math>; then the operators can be simultaneously (unitarily) diagonalised.\n\n'''Proof.''' Fix <math>T_0\\in\\mathcal{F}</math> compact injective. Then we have, by the spectral theory of compact symmetric operators on hilbert spaces:\n\n:<math>H=\\overline{\\bigoplus_{\\lambda\\in\\sigma(T_0)} \\ker(T_0-\\sigma)},</math>\n\nwhere <math>\\sigma(T_0)</math> is a discrete, countable subset of positive real numbers, and all the eigenspaces are finite-dimensional. Since <math>\\mathcal{F}</math> a commuting set, we have all the eigenspaces are invariant. Since the operators restricted to the eigenspaces (which are finite-dimensional) are automatically all compact, we can apply Theorem 1 to each of these, and find orthonormal bases ''Q''<sub>σ</sub> for the <math>\\ker(T_0-\\sigma)</math>. Since ''T''<sub>0</sub> is symmetric, we have that\n\n:<math>Q:=\\bigcup_{\\sigma\\in\\sigma(T_0)} Q_{\\sigma}</math>\n\nis a (countable) orthonormal set. It is also, by the decomposition we first stated, a basis for ''H''.\n\n'''Theorem 3.''' If ''H'' a finite-dimensional Hilbert space, and <math>\\mathcal{F}\\subseteq\\operatorname{Hom}(H,H)</math> a commutative set of operators, each of which is diagonalisable; then the operators can be simultaneously diagonalised.\n\n'''Proof.''' ''Case I:'' all operators have exactly one eigenvalue. Then any basis for ''H'' will do.\n\n''Case II:'' Fix <math>T_0\\in\\mathcal{F}</math> an operator with at least two eigenvalues, and let <math>P\\in\\operatorname{Hom}(H,H)^{\\times}</math> so that <math>P^{-1}~T_0~P</math> is a symmetric operator. Now let α be an eigenvalue of <math>P^{-1}T_0P</math>. Then it is easy to see that both:\n\n:<math>\\ker\\left(P^{-1}~T_0(P-\\alpha)\\right), \\quad \\ker\\left(P^{-1}~T_0(P-\\alpha) \\right)^{\\bot}</math>\n\nare non-trivial <math>P^{-1}\\mathcal{F}P</math>-invariant subspaces. By induction over dimension we have that there are linearly independent bases ''Q''<sub>1</sub>, ''Q''<sub>2</sub> for the subspaces, which demonstrate that the operators in <math>P^{-1}\\mathcal{F}P</math> can be simultaneously diagonalisable on the subspaces. Clearly then <math>P(Q_1\\cup Q_2)</math> demonstrates that the operators in <math>\\mathcal{F}</math> can be simultaneously diagonalised.\n\nNotice we did not have to directly use the machinery of matrices at all in this proof. There are other versions which do.\n\nWe can strengthen the above to the case where all the operators merely commute with their adjoint; in this case we remove the term \"orthogonal\" from the diagonalisation. There are weaker results for operators arising from representations due to Weyl–Peter. Let ''G'' be a fixed locally compact hausdorff group, and <math>H=L^2(G)</math> (the space of square integrable measurable functions with respect to the unique-up-to-scale Haar measure on ''G''). Consider the continuous shift action:\n\n:<math>\\begin{cases} G\\times H\\to H \\\\ (gf)(x)=f(g^{-1}x) \\end{cases}</math>\n\nThen if ''G'' were compact then there is a unique decomposition of ''H'' into a countable direct sum of finite-dimensional, irreducible, invariant subspaces (this is essentially diagonalisation of the family of operators <math>G\\subseteq U(H)</math>). If ''G'' were not compact, but were abelian, then diagonalisation is not achieved, but we get a unique ''continuous'' decomposition of ''H'' into 1-dimensional invariant subspaces.\n\n== Compact normal operator ==\nThe family of Hermitian matrices is a proper subset of matrices that are unitarily diagonalizable. A matrix ''M'' is unitarily diagonalizable if and only if it is normal, i.e. ''M*M'' = ''MM*''. Similar statements hold for compact normal operators.\n\nLet ''T'' be compact and ''T*T'' = ''TT*''. Apply the ''Cartesian decomposition'' to ''T'': define\n\n:<math>R = \\frac{T + T^*}{2}, \\quad J = \\frac{T - T^*}{2i}.</math>\n\nThe self adjoint compact operators ''R'' and ''J'' are called the real and imaginary parts of ''T'' respectively. ''T'' is compact means ''T*'', consequently ''R'' and ''J'', are compact. Furthermore, the normality of ''T'' implies ''R'' and ''J'' commute. Therefore they can be simultaneously diagonalized, from which follows the claim.\n\nA [[hyponormal operator|hyponormal compact operator]] (in particular, a [[subnormal operator]]) is normal.\n\n== Unitary operator ==\nThe spectrum of a [[unitary operator]] ''U'' lies on the unit circle in the complex plane; it could be the entire unit circle. However, if ''U'' is identity plus a compact perturbation, ''U'' has only countable spectrum, containing 1 and possibly, a finite set or a sequence tending to 1 on the unit circle. More precisely, suppose {{nowrap|''U'' {{=}} ''I'' + ''C''}} where ''C'' is compact.  The equations {{nowrap|''UU*'' {{=}} ''U*U'' {{=}} ''I''}} and {{nowrap|''C'' {{=}} ''U'' − ''I''}} show that ''C'' is normal.  The spectrum of ''C'' contains 0, and possibly, a finite set or a sequence tending to 0.  Since {{nowrap|''U'' {{=}} ''I'' + ''C''}}, the spectrum of ''U'' is obtained by shifting the spectrum of ''C'' by 1.\n\n== Examples ==\n* Let ''H'' = [[Lp space|''L''<sup>2</sup>([0, 1])]]. The multiplication operator ''M'' defined by\n\n:: <math>(M f)(x) = x f(x), \\quad f \\in H, \\, \\, x \\in [0, 1]</math>\n\n:is a bounded self-adjoint operator on ''H'' that has no eigenvector and hence, by the spectral theorem, cannot be compact.\n\n* Let ''K''(''x'', ''y'') be square integrable on [0, 1]<sup>2</sup> and define ''T''<sub>''K''</sub> on ''H'' by\n\n::<math>(T_K f)(x) = \\int_0^1 K(x, y) f(y) \\, \\mathrm{d} y.</math>\n\n:Then ''T<sub>K</sub>'' is compact on ''H''; it is a [[Hilbert–Schmidt operator]].\n\n* Suppose that the kernel ''K''(''x'', ''y'') satisfies the Hermiticity condition\n\n::<math>K(y, x) = \\overline{K(x, y)}, \\quad x, y \\in [0, 1].</math>\n\n:Then ''T<sub>K</sub>'' is compact and self-adjoint on ''H''; if {φ<sub>''n''</sub>} is an orthonormal basis of eigenvectors, with eigenvalues {λ<sub>''n''</sub>}, it can be proved that\n\n::<math>\\sum \\lambda_n^2 < \\infty, \\ \\ K(x, y) \\sim \\sum \\lambda_n \\varphi_n(x) \\overline{\\varphi_n(y)},</math>\n\n:where the sum of the series of functions is understood as ''L''<sup>2</sup> convergence for the Lebesgue measure {{nowrap|on [0, 1]<sup>2</sup>}}. [[Mercer's theorem]] gives conditions under which the series converges to ''K''(''x'', ''y'') pointwise, and uniformly {{nowrap|on [0, 1]<sup>2</sup>}}.\n\n== See also ==\n*[[Singular value decomposition#Bounded operators on Hilbert spaces]]. The notion of singular values can be extended from matrices to compact operators.\n*[[Decomposition of spectrum (functional analysis)]]. If the compactness assumption is removed, operators need not have countable spectrum in general.\n*[[Calkin algebra]]\n\n== References ==\n*J. Blank, P. Exner, and M. Havlicek, ''Hilbert Space Operators in Quantum Physics'', American Institute of Physics, 1994. \n*M. Reed and B. Simon, ''Methods of Modern Mathematical Physics I: Functional Analysis'', Academic Press, 1972.\n*{{citation|last=Zhu | first= Kehe | title=Operator Theory in Function Spaces | series=Mathematical surveys and monographs | volume=Vol. 138 | publisher=American Mathematical Society | year=2007 | isbn=978-0-8218-3965-2}}\n\n{{Functional analysis}}\n\n[[Category:Operator theory]]\n[[Category:Hilbert space]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Coorbit theory",
      "url": "https://en.wikipedia.org/wiki/Coorbit_theory",
      "text": "{{refimprove|date=June 2011}}\nIn mathematics, '''coorbit theory''' was developed by [[Hans Georg Feichtinger]] and Karlheinz Gröchenig around 1990.<ref>H. G. Feichtinger and K. Gröchenig. \"A unified approach to atomic decompositions via integrable group representations\" Lect. Notes in Math. 1302:52—73, 1988.</ref><ref>H. G. Feichtinger and K. Gröchenig. \"Banach spaces related to integrable group representations and their atomic decompositions, I\" J. Funct. Anal. 86(2):307–340, 1989.</ref><ref>H. G. Feichtinger and K. Gröchenig. \"Banach spaces related to integrable group representations and their atomic decompositions, II\" Monatsh. Math. 108(2-3):129–148, 1989.</ref>  It provides theory for atomic decomposition of a range of [[Banach space]]s of [[distribution (mathematics)|distributions]]. Among others the well established [[wavelet transform]] and the [[short-time Fourier transform]] are covered by the theory.\n\nThe starting point is a [[Representation theory|square integrable representation]] <math>\\pi</math> of a [[locally compact group]] <math>\\mathcal G</math> on a [[Hilbert space]] <math>\\mathcal H</math>, with which one can define a transform of a function <math>f \\in \\mathcal H</math> with respect to <math>g\\in \\mathcal H</math> by <math>V_g f (x) = \\langle f, \\pi(x)g \\rangle</math>. Many important transforms are special cases of the transform, e.g. the short-time Fourier transform and the wavelet transform for the [[Heisenberg group]] and the [[affine group]] respectively. Representation theory yields the reproducing formula <math>V_g f = V_g f * V_g g</math>. By discretization of this continuous [[convolution]] integral it can be shown that by sufficiently dense sampling in phase space the corresponding functions will span a frame for the Hilbert space.\n\nAn important aspect of the theory is the derivation of atomic decompositions for Banach spaces. One of the key steps is to define the voice transform for distributions in a natural way. For a given Banach space <math>Y</math>, the corresponding coorbit space is defined as the set of all distributions such that <math>V_g f \\in Y</math>. The reproducing formula is true also in this case and therefore it is possible to obtain atomic decompositions for coorbit spaces.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Coorbit Theory}}\n[[Category:Hilbert space]]"
    },
    {
      "title": "Cotlar–Stein lemma",
      "url": "https://en.wikipedia.org/wiki/Cotlar%E2%80%93Stein_lemma",
      "text": "In [[mathematics]], in the field of [[functional analysis]], the '''Cotlar–Stein almost orthogonality lemma''' is named after mathematicians [[Mischa Cotlar]]\nand [[Elias Stein]]. It may be used to obtain information on the [[operator norm]] on an operator, acting from one [[Hilbert space]] into another\nwhen the operator can be decomposed into ''almost orthogonal'' pieces.\nThe original version of this lemma\n(for [[self-adjoint]] and mutually commuting operators)\nwas proved by Mischa Cotlar in 1955<ref>{{harvnb|Cotlar|1955}}</ref> and allowed him to conclude that the [[Hilbert transform]]\nis a [[continuous linear operator]] in <math>L^2</math>\nwithout using the [[Fourier transform]].\nA more general version was proved by Elias Stein.<ref>{{harvnb|Stein|1993}}</ref>\n\n==Cotlar–Stein almost orthogonality lemma==\nLet <math>E,\\,F</math> be two [[Hilbert spaces]].\nConsider a family of operators\n<math>T_j</math>, <math> j\\geq 1</math>,\nwith each <math>T_j</math>\na [[bounded linear operator]] from <math>E</math> to <math>F</math>.\n\nDenote\n\n: <math>a_{jk}=\\Vert T_j T_k^\\ast\\Vert,\n\\qquad b_{jk}=\\Vert T_j^\\ast T_k\\Vert.</math>\n\nThe family of operators\n<math>T_j:\\;E\\to F</math>, <math>j\\ge 1,</math>\nis ''almost orthogonal'' if\n\n:<math>A=\\sup_{j}\\sum_{k}\\sqrt{a_{jk}}<\\infty,\n\\qquad B=\\sup_{j}\\sum_{k}\\sqrt{b_{jk}}<\\infty.</math>\n\nThe Cotlar–Stein lemma states that if <math>T_j</math>\nare almost orthogonal,\nthen the series\n<math>\\sum_{j}T_j</math>\nconverges in the [[strong operator topology]],\nand that\n\n:<math>\\Vert \\sum_{j}T_j\\Vert \\le\\sqrt{AB}.</math>\n\n==Proof==\nIf ''R''<sub>1</sub>, ..., ''R''<sub>''n''</sub> is a finite collection of bounded operators, then<ref>{{harvnb|Hörmander|1994}}</ref>\n\n:<math>\\displaystyle{\\sum_{i,j} |(R_i v,R_jv)| \\le \\left(\\max_i \\sum_j \\|R_i^*R_j\\|^{1\\over 2}\\right)\\left(\\max_i \\sum_j \\|R_iR_j^*\\|^{1\\over 2}\\right)\\|v\\|^2.}</math>\n\nSo under the hypotheses of the lemma,\n\n:<math>\\displaystyle{\\sum_{i,j} |(T_i v,T_jv)| \\le AB\\|v\\|^2.}</math>\n\nIt follows that \n\n:<math>\\displaystyle{\\|\\sum_{i=1}^n T_iv\\|^2 \\le AB \\|v\\|^2,}</math>\n\nand that\n\n:<math>\\displaystyle{\\|\\sum_{i=m}^n T_iv\\|^2 \\le \\sum_{i,j\\ge m} |(T_iv,T_jv)|.}</math>\n\nHence the partial sums \n\n:<math>\\displaystyle{s_n=\\sum_{i=1}^n T_iv}</math>\n\nform a [[Cauchy sequence]]. \n\nThe sum is therefore absolutely convergent with limit satisfying the stated inequality.\n\nTo prove the inequality above set\n\n:<math>\\displaystyle{R=\\sum a_{ij}R_i^*R_j}</math>\n\nwith |''a''<sub>''ij''</sub>| ≤ 1 chosen so that\n\n:<math>\\displaystyle{(Rv,v)=|(Rv,v)|=\\sum |(R_iv,R_jv)|.}</math>\n\nThen\n\n:<math>\\displaystyle{\\|R\\|^{2m} =\\|(R^*R)^m\\|\\le \\sum \\|R_{i_1}^* R_{i_2} R_{i_3}^* R_{i_4} \\cdots R_{i_{2m}}\\| \\le \\sum \\left(\\|R_{i_1}^*\\|\\|R_{i_1}^*R_{i_2}\\|\\|R_{i_2}R_{i_3}^*\\|\\cdots \\|R_{i_{2m-1}}^* R_{i_{2m}}\\|\\|R_{i_{2m}}\\|\\right)^{1\\over 2}.}</math>\n\nHence\n\n:<math>\\displaystyle{\\|R\\|^{2m} \\le n \\cdot \\max \\|R_i\\| \\left(\\max_i \\sum_j \\|R_i^*R_j\\|^{1\\over 2}\\right)^{2m}\\left(\\max_i \\sum_j \\|R_iR_j^*\\|^{1\\over 2}\\right)^{2m-1}.}</math>\n\nTaking 2''m''th roots and letting ''m'' tend to ∞,\n\n:<math>\\displaystyle{\\|R\\|\\le  \\left(\\max_i \\sum_j \\|R_i^*R_j\\|^{1\\over 2}\\right)\\left(\\max_i \\sum_j \\|R_iR_j^*\\|^{1\\over 2}\\right),}</math>\n\nwhich immediately implies the inequality.\n\n==Generalization==\nThere is a generalization of the Cotlar–Stein lemma with sums replaced by integrals.<ref>{{harvnb|Knapp|Stein|1971}}</ref><ref>{{cite journal|last1=Calderon|first1=Alberto|last2=Vaillancourt|first2=Remi|title=On the boundedness of pseudo-differential operators|journal=Journal of the Mathematical Society of Japan|date=1971|volume=23|issue=2|pages=374–378|doi=10.2969/jmsj/02320374}}</ref>Let ''X'' be a locally compact space and μ a Borel measure on ''X''. Let ''T''(''x'') be a map from ''X'' into bounded operators from ''E'' to ''F'' which is uniformly bounded and continuous in the strong operator topology. If \n\n:<math>\\displaystyle{A= \\sup_x \\int_X \\|T(x)^*T(y)\\|^{1\\over 2} \\, d\\mu(y),\\,\\,\\, B= \\sup_x \\int_X \\|T(y)T(x)^*\\|^{1\\over 2}\\, d\\mu(y),}</math>\n\nare finite, then the function ''T''(''x'')''v'' is integrable for each ''v'' in ''E'' with\n\n:<math>\\displaystyle{\\|\\int_X T(x)v\\, d\\mu(x)\\| \\le \\sqrt{AB}  \\cdot \\|v\\|.}</math>\n\nThe result can be proved by replacing sums by integrals in the previous proof or by using Riemann sums to approximate the integrals.\n\n==Example==\nHere is an example of an ''orthogonal'' family of operators.  Consider the inifite-dimensional matrices\n\n:<math>\nT=\\left[\n\\begin{array}{cccc}\n1&0&0&\\vdots\\\\0&1&0&\\vdots\\\\0&0&1&\\vdots\\\\\\cdots&\\cdots&\\cdots&\\ddots\\end{array}\n\\right]\n</math>\n\nand also\n\n:<math>\n\\qquad\nT_1=\\left[\n\\begin{array}{cccc}\n1&0&0&\\vdots\\\\0&0&0&\\vdots\\\\0&0&0&\\vdots\\\\\\cdots&\\cdots&\\cdots&\\ddots\\end{array}\n\\right],\n\\qquad\nT_2=\\left[\n\\begin{array}{cccc}\n0&0&0&\\vdots\\\\0&1&0&\\vdots\\\\0&0&0&\\vdots\\\\\\cdots&\\cdots&\\cdots&\\ddots\\end{array}\n\\right],\n\\qquad\nT_3=\\left[\n\\begin{array}{cccc}\n0&0&0&\\vdots\\\\0&0&0&\\vdots\\\\0&0&1&\\vdots\\\\\\cdots&\\cdots&\\cdots&\\ddots\\end{array}\n\\right],\n\\qquad\n\\dots.\n</math>\n\nThen\n<math>\\Vert T_j\\Vert=1</math> for each <math>j</math>,\nhence the series <math>\\sum_{j\\in\\mathbb{N}}T_j</math>\ndoes not converge in the [[uniform operator topology]].\n\nYet, since\n<math>\\Vert T_j T_k^\\ast\\Vert=0</math>\nand \n<math>\\Vert T_j^\\ast T_k\\Vert=0</math>\nfor <math>j\\ne k</math>,\nthe Cotlar–Stein almost orthogonality lemma tells us that \n\n:<math>T=\\sum_{j\\in\\mathbb{N}}T_j</math>\n\nconverges in the [[strong operator topology]] and is bounded by 1.\n\n==Notes==\n{{reflist|2}}\n\n==References==\n*{{citation|first=Mischa|last= Cotlar|title=A combinatorial inequality and its application to L<sup>2</sup> spaces|journal= Math. Cuyana|volume= 1|year=1955|pages= 41–55}}\n*{{citation|first=Lars|last=Hörmander|title=Analysis of Partial Differential Operators III: Pseudodifferential Operators|publisher=Springer-Verlag|year=1994|edition=2nd|isbn=978-3-540-49937-4|pages=165–166}}\n*{{citation|first=Anthony W.|last=Knapp|first2=Elias|last2=Stein|title=Intertwining operators for semisimple Lie groups|journal=Ann. Math.|volume=93|year=1971|pages=489–579}}\n*{{citation|first=Elias|last= Stein|title=Harmonic Analysis: Real-variable Methods, Orthogonality and Oscillatory Integrals|publisher= Princeton University Press|year= 1993|isbn= 0-691-03216-5}}\n\n{{DEFAULTSORT:Cotlar-Stein lemma}}\n[[Category:Hilbert space]]\n[[Category:Harmonic analysis]]\n[[Category:Operator theory]]\n[[Category:Inequalities]]\n[[Category:Theorems in functional analysis]]\n[[Category:Lemmas]]"
    },
    {
      "title": "Crinkled arc",
      "url": "https://en.wikipedia.org/wiki/Crinkled_arc",
      "text": "In [[mathematics]], and in particular the study of [[Hilbert space|Hilbert spaces]], a '''crinkled arc''' is a type of [[continuity (mathematics)|continuous]] curve.  The concept is usually credited to [[Paul Halmos]].\n\nSpecifically, consider <math>f\\colon\\left[0,1\\right]\\longrightarrow X</math>, where <math>X</math> is a Hilbert space with inner product <math>\\langle\\cdot,\\cdot\\rangle</math>.  We say that <math>f</math> is a crinkled arc if it is [[continuity (mathematics)|continuous]] and possesses the ''crinkly'' property: if <math>0\\leqslant a<b\\leqslant c<d\\leqslant 1<</math> then <math>\\langle f(b)-f(a),f(d)-f(c)\\rangle=0</math>, that is, the [[chord (geometry)|chords]] <math>f(b)-f(a)</math> and <math>f(d)-f(c)</math> are [[orthogonal]] whenever the [[interval (mathematics)|intervals]] <math>\\left[a,b\\right]</math> and <math>\\left[c,d\\right]</math> are [[set intersection|nonoverlapping]].\n\nHalmos points out that, if two nonoverlapping chords are orthogonal, then \"the curve makes a [[right-angle]] turn during the passage between the chords' farthest end-points\" and observes that such as curve would \"seem to be making a sudden right angle turn at each point\" which would justify the choice of terminology.  Halmos deduces that such a curve could not have a [[tangent]] at any point, and uses the concept to justify his statement that an infinite-dimensional Hilbert space is \"even roomier than it looks\".\n\nWriting in 1975, Richard Vitale considers Halmos's empirical observation that every attempt to construct a crinkled arc results in essentially the same solution and proves that <math>f(t)</math> is a crinkled arc [[if and only if]], after appropriate scaling,\n\n:<math>\nf(t) = \\sqrt{2}\\sum_{n=1}^\\infty x_n\n\\frac{\\sin\\left(n-1/2\\right)\\pi t}{\\left(n-1/2\\right)\\pi}\n</math>\nwhere <math>\\left\\{x_n\\right\\}</math> is an [[orthonormal set]].\n\n==References==\n\n* Paul R. Halmos 1982. ''A Hilbert Space Problem Book'' Springer-Verlag\n* R. A. Vitale 1975.  \"A representation of a crinkled arc\".  ''Proceedings of the American Mathematical Society'', Volume 52, pages 303–4.\n\n[[Category:Hilbert space]]"
    },
    {
      "title": "Energetic space",
      "url": "https://en.wikipedia.org/wiki/Energetic_space",
      "text": "In [[mathematics]], more precisely in [[functional analysis]], an '''energetic space''' is, intuitively, a subspace of a given [[real number|real]] [[Hilbert space]] equipped with a new \"energetic\" [[Inner product space|inner product]]. The motivation for the name comes from [[physics]], as in many physical problems the [[energy]] of a system can be expressed in terms of the energetic inner product. An example of this will be given later in the article. \n\n==Energetic space==\nFormally, consider a real Hilbert space <math>X</math> with the [[Inner product space|inner product]] <math>(\\cdot|\\cdot)</math> and the [[norm (mathematics)|norm]] <math>\\|\\cdot\\|</math>. Let <math>Y</math> be a linear subspace of <math>X</math> and <math>B:Y\\to X</math> be a [[strongly monotone]] [[symmetric operator|symmetric]] [[linear operator]], that is, a linear operator satisfying \n\n* <math>(Bu|v)=(u|Bv)\\, </math> for all <math>u, v</math> in <math>Y</math>\n* <math>(Bu|u) \\ge c\\|u\\|^2</math> for some constant <math>c>0</math> and all <math>u</math> in <math>Y.</math>\n\nThe '''energetic inner product''' is defined as \n:<math>(u|v)_E =(Bu|v)\\,</math> for all <math>u,v</math> in <math>Y</math>\nand the '''energetic norm'''{{anchor|energetic norm}} is\n:<math>\\|u\\|_E=(u|u)^\\frac{1}{2}_E \\, </math> for all <math>u</math> in <math>Y.</math>\n\nThe set <math>Y</math> together with the energetic inner product is a [[pre-Hilbert space]]. The '''energetic space''' <math>X_E</math> is defined as the [[complete metric space|completion]] of <math>Y</math> in the energetic norm. <math>X_E</math> can be considered a subset of the original Hilbert space <math>X,</math> since any [[Cauchy sequence]] in the energetic norm is also Cauchy in the norm of <math>X</math> (this follows from the strong monotonicity property of <math>B</math>). \n\nThe energetic inner product is extended from <math>Y</math> to <math>X_E</math> by\n: <math> (u|v)_E = \\lim_{n\\to\\infty} (u_n|v_n)_E</math>\nwhere <math>(u_n)</math> and <math>(v_n)</math> are sequences in ''Y'' that converge to points in <math>X_E</math> in the energetic norm.\n\n==Energetic extension==\nThe operator <math>B</math> admits an '''energetic extension''' <math>B_E</math> \n\n:<math>B_E:X_E\\to X^*_E</math>\n\ndefined on <math>X_E</math> with values in the [[dual space]] <math>X^*_E</math> that is given by the formula \n\n:<math>\\langle B_E u | v \\rangle_E = (u|v)_E</math> for all <math>u,v</math> in <math>X_E.</math>\n\nHere, <math>\\langle \\cdot |\\cdot \\rangle_E</math> denotes the duality bracket between <math>X^*_E</math> and <math>X_E,</math> so <math>\\langle B_E u | v \\rangle_E</math> actually denotes  <math>(B_E u)(v).</math>\n\nIf <math>u</math> and <math>v</math> are elements in the original subspace <math>Y,</math> then\n\n:<math>\\langle B_E u | v \\rangle_E = (u|v)_E = (Bu|v) = \\langle u|B|v\\rangle</math>\n\nby the definition of the energetic inner product.  If one views <math>Bu,</math> which is an element in <math>X,</math> as an element in the dual <math>X^*</math> via the [[Riesz representation theorem]], then <math>Bu</math> will also be in the dual <math>X_E^*</math> (by the strong monotonicity property of <math>B</math>). Via these identifications, it follows from the above formula that <math>B_E u= Bu.</math> In different words, the original operator <math>B:Y\\to X</math> can be viewed as an operator <math>B:Y\\to X_E^*,</math> and then <math>B_E:X_E\\to X^*_E</math> is simply the function extension of <math>B</math> from <math>Y</math> to <math>X_E.</math> <!--- \n\nI commented out the below text, since it is not clear what norm one uses to talk about convergence and boundedness. I will think more about it. \n\nThat is, <math>B_E</math> is that [[linear functional]] which acts like ''B'' but has a domain of <math>X_E</math>—that is, its domain includes all limit points, ''u'', of the domain of ''B'' for which ''Bu<sub>n</sub>'' is bounded as <math>u_n\\to u</math>.\n\n--->\n\n==An example from physics==\n[[File:String illust.svg|right|thumb|A string with fixed endpoints under the influence of a force pointing down.]]\nConsider a [[rope|string]]<!-- a piece of wire, so the link to [[rope|string]] is not ambiguous--> whose endpoints are fixed at two points <math>a<b</math> on the real line   (here viewed as a horizontal line). Let the vertical outer [[force density]] at each point <math>x</math> <math>(a\\le x \\le b)</math> on the string be <math>f(x)\\mathbf{e}</math>, where  <math>\\mathbf{e}</math> is a [[unit vector]] pointing vertically and <math>f:[a, b]\\to \\mathbb R.</math> Let <math>u(x)</math> be the [[Deflection (engineering)|deflection]] of the string at the point <math>x</math> under the influence of the force. Assuming that the deflection is small, the [[elastic energy]] of the string is \n\n: <math>\\frac{1}{2} \\int_a^b\\! u'(x)^2\\, dx</math>\n\nand the total [[potential energy]] of the string is\n\n: <math>F(u) = \\frac{1}{2} \\int_a^b\\! u'(x)^2\\,dx - \\int_a^b\\! u(x)f(x)\\,dx.</math>\n\nThe deflection <math>u(x)</math> minimizing the potential energy will satisfy the [[differential equation]]\n\n: <math>-u''=f\\,</math>\n\nwith [[boundary conditions]]\n\n:<math>u(a)=u(b)=0.\\,</math>\n\nTo study this equation, consider the space <math>X=L^2(a, b), </math> that is, the [[Lp space]] of all [[square-integrable function]]s <math>u:[a, b]\\to \\mathbb R</math> in respect to the [[Lebesgue measure]]. This space is Hilbert in respect to the inner product\n\n: <math>(u|v)=\\int_a^b\\! u(x)v(x)\\,dx,</math>\n\nwith the norm being given by \n\n: <math>\\|u\\|=\\sqrt{(u|u)}.</math>\n\nLet <math>Y</math> be the set of all [[smooth function|twice continuously differentiable functions]] <math>u:[a, b]\\to \\mathbb R</math> with the [[boundary conditions]] <math>u(a)=u(b)=0.</math> Then <math>Y</math> is a linear subspace of <math>X.</math>\n\nConsider the operator <math>B:Y\\to X</math> given by the formula\n\n: <math>Bu = -u'',\\,</math>\n\nso the deflection satisfies the equation <math>Bu=f.</math> Using  [[integration by parts]] and the boundary conditions, one can see that \n\n: <math>(Bu|v)=-\\int_a^b\\! u''(x)v(x)\\, dx=\\int_a^b u'(x)v'(x) = (u|Bv) </math>\n\nfor any <math>u</math> and <math>v</math> in <math>Y.</math> Therefore, <math>B</math> is a symmetric linear operator. \n\n<math>B</math> is also strongly monotone, since, by the [[Friedrichs's inequality]] \n\n: <math>\\|u\\|^2 = \\int_a^b u^2(x)\\, dx \\le C \\int_a^b u'(x)^2\\, dx = C\\,(Bu|u)</math>\n\nfor some <math>C>0.</math>\n\nThe energetic space in respect to the operator <math>B</math> is then the [[Sobolev space]] <math>H^1_0(a, b).</math> We see that the elastic energy of the string which motivated this study is \n\n: <math>\\frac{1}{2} \\int_a^b\\! u'(x)^2\\, dx = \\frac{1}{2} (u|u)_E,</math>\n\nso it is half of the energetic inner product of <math>u</math> with itself. \n\nTo calculate the deflection <math>u</math> minimizing the total potential energy <math>F(u)</math> of the string, one writes this problem in the form \n\n:<math>(u|v)_E=(f|v)\\,</math> for all <math>v</math> in <math>X_E</math>.\n\nNext, one usually approximates <math>u</math> by some <math>u_h</math>, a function in a finite-dimensional subspace of the true solution space. For example, one might let <math>u_h</math> be a continuous [[piecewise linear function]] in the energetic space, which gives the [[finite element method]]. The approximation <math>u_h</math> can be computed by solving a [[system of linear equations]].\n\nThe energetic norm turns out to be the natural norm in which to measure the error between  <math>u</math> and <math>u_h</math>, see [[Céa's lemma]].\n\n==See also==\n* [[Inner product space]]\n* [[Positive-definite kernel]]\n\n==References==\n*{{cite book\n | last       = Zeidler\n | first      = Eberhard\n | title      = Applied functional analysis: applications to mathematical physics\n | publisher  = New York: Springer-Verlag\n | date       = 1995\n | pages      = \n | isbn       = 0-387-94442-7\n}}\n\n*{{cite book\n | last       = Johnson\n | first      = Claes\n | title      = Numerical solution of partial differential equations by the finite element method\n | publisher  = Cambridge University Press\n | date       = 1987\n | pages      = \n | isbn       = 0-521-34514-6\n}}\n\n[[Category:Functional analysis]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Gleason's theorem",
      "url": "https://en.wikipedia.org/wiki/Gleason%27s_theorem",
      "text": "'''Gleason's theorem''' (named after [[Andrew M. Gleason]]) is a mathematical result which shows that the rule one uses to calculate [[probability|probabilities]] in [[quantum physics]] follows logically from particular assumptions about how measurements are represented mathematically.  More specifically, it proves that the [[Born rule]] for the probability of obtaining specific results for a given measurement follows naturally from the structure formed by the [[Lattice (order)|lattice]] of events in a [[Real number|real]] or [[Complex number|complex]] [[Hilbert space]].  This result is of particular importance for the field of [[quantum logic]].  Furthermore, it was historically significant for the role it played in showing that [[local hidden variable theory|local hidden variable theories]] are inconsistent with quantum physics.  The theorem states:\n\n:'''Theorem'''. Suppose ''H'' is a [[separable Hilbert space]]. A ''measure'' on ''H'' is a function ''f'' that assigns a nonnegative real number to each closed subspace of ''H'' in such a way that, if <math display=\"inline\">\\{A_i\\}</math> is a countable collection of mutually orthogonal subspaces of ''H,'' and the closed [[linear span]] of this collection is ''B,'' then <math display=\"inline\">f(B) = \\sum_i f(A_i)</math>. If the Hilbert space ''H'' has dimension at least three, then every measure ''f'' can be written in the form <math display=\"inline\">f(A) = \\mathrm{Tr}(W P_A)</math>, where ''W'' is a [[Positive-definite matrix|positive semidefinite]] [[trace class]] operator and <math display=\"inline\">P_A</math> is the orthogonal projection onto ''A.''\n\nThe trace-class operator ''W'' can be interpreted as the [[density matrix]] of a [[quantum state]]. Effectively, the theorem says that any legitimate probability measure on the space of measurement outcomes is generated by some quantum state.\n\n== Overview ==\nConsider a quantum system with a Hilbert space of dimension 3 or larger, and suppose that there exists some function that assigns a probability to each outcome of any possible measurement upon that system.  The probability of any such outcome must be a real number between 0 and 1 inclusive, and in order to be consistent, for any individual measurement the probabilities of the different possible outcomes must add up to 1.  Gleason's theorem shows that any such function&mdash;that is, any consistent assignment of probabilities to measurement outcomes&mdash;must be expressible in terms of a quantum-mechanical [[density operator]] and the [[Born rule]].  In other words, given that each quantum system is associated with a Hilbert space, and given that measurements are described by particular mathematical entities defined on that Hilbert space, both the structure of quantum state space and the rule for calculating probabilities from a quantum state then follow.\n\nFor simplicity, we can assume that the dimension of the Hilbert space is finite. A quantum-mechanical [[observable]] is a [[self-adjoint operator]] on that Hilbert space. Equivalently, we can say that a measurement is defined by an [[orthonormal basis]], with each possible outcome of that measurement corresponding to one of the vectors comprising the basis. A density operator is a [[Positive-definite matrix|positive-semidefinite operator]] whose [[Trace (linear algebra)|trace]] is equal to 1. In the language of [[Carl Friedrich von Weizsäcker|von Weizsäcker]], a density operator is a \"catalogue of probabilities\": for each measurement that can be defined, we can compute the probability distribution over the outcomes of that measurement from the density operator.<ref>Dreischner, Görnitz and von Weizsäcker (1988)</ref> We do so by applying the Born rule, which states that<math display=\"block\">P(x_i) = \\mathrm{Tr}(\\Pi_i W),</math>where <math display=\"inline\">W</math> is the density operator and <math display=\"inline\">\\Pi_i</math> is the [[Projection (linear algebra)|projection operator]] onto the basis vector associated with the measurement outcome <math display=\"inline\">x_i</math>.\n\nLet <math display=\"inline\">f</math> be a function from projection operators to the [[unit interval]] with the property that, if a set <math display=\"inline\">\\{\\Pi_i\\}</math> of projection operators sum to the [[identity matrix]]—that is, if they correspond to an orthonormal basis—then<math display=\"block\">\\sum_i f(\\Pi_i) = 1.</math>Such a function expresses an assignment of probability values to the outcomes of measurements, an assignment that is \"noncontextual\" in the sense that the probability for an outcome does not depend upon which measurement that outcome is embedded within, but only upon the mathematical representation of that specific outcome, i.e., its projection operator.<ref>Barnum ''et al.'' (2000); Pitowsky (2003), §1.3; Pitowsky (2006), §2.1; Kunjwal and Spekkens (2015)</ref> Gleason's theorem states that for any such function <math display=\"inline\">f</math>, there exists a positive semidefinite operator with unit trace <math display=\"inline\">W</math> such that<math display=\"block\">f(\\Pi_i) = \\mathrm{Tr}(\\Pi_i W).</math>Both the Born rule and the fact that \"catalogues of probability\" are positive semidefinite operators of unit trace follow from the assumptions that measurements are represented by orthonormal bases, and that probability assignments are \"noncontextual\".  In order for Gleason's theorem to be applicable, the space on which measurements are defined must be a real or complex Hilbert space, or a quaternionic [[Module (mathematics)|module]].<ref>Piron (1972), §6; Drisch (1979); Horwitz ''et al.'' (1984); Razon ''et al.'' (1991); Varadarajan (2007), pp. 83 ff.; Cassinelli and Lahti (2017), §2; Moretti and Oppio (2018) </ref>  (Gleason's argument is inapplicable if, for example, one tries to construct an [[p-adic quantum mechanics|analogue of quantum mechanics using ''p''-adic numbers]].)\n\nAnother way of phrasing the theorem uses the terminology of [[quantum logic]], which makes heavy use of [[lattice theory]].  Quantum logic treats quantum events (or measurement outcomes) as [[propositional logic|logical propositions]], and studies the relationships and structures formed by these events, with specific emphasis on [[Measurement in quantum mechanics|quantum measurement]].  In quantum logic, the logical propositions that describe events are organized into a [[lattice (order)|lattice]] in which the [[distributive law]], valid in classical logic, is weakened, to reflect the fact that in quantum physics, not all pairs of quantities can be [[uncertainty principle|measured simultaneously]].<ref>Dvurecenskij (1992)</ref> The ''representation theorem'' in quantum logic shows that such a lattice is [[Isomorphism|isomorphic]] to the lattice of [[Space (mathematics)|subspaces]] of a [[vector space]] with a [[scalar product]].<ref>Pitowsky (2006), §2</ref>  It remains an open problem in quantum logic to constrain the [[field (mathematics)|field]] ''K'' over which the vector space is defined.  [[Solèr's theorem]] implies that, granting certain hypotheses, the field ''K'' must be either the [[real number]]s, [[complex number]]s, or the [[quaternion]]s.<ref>Baez (2010); Cassinelli and Lahti (2017), §3; Moretti and Oppio (2019) </ref>\n\nWe let ''A'' represent an observable with finitely many potential outcomes: the eigenvalues of the [[Hermitian operator]] ''A'', i.e. <math>\\alpha_1, \\alpha_2, \\alpha_3, \\ldots, \\alpha_n </math>.  An \"event\", then, is a proposition <math>x_i</math>, which in natural language can be rendered \"the outcome of measuring ''A'' on the system is <math>\\alpha_i</math>\".  Let ''H'' denote the Hilbert space associated with the physical system, and let ''L'' denote the lattice of subspaces of ''H.'' The events <math>x_i</math> generate a sublattice of ''L'' which is a finite [[Boolean algebra]], and if ''n'' is the dimension of the Hilbert space, then each event is an [[Atom (order theory)|atom]] of the lattice ''L.''\n\nA ''quantum probability function'' over ''H'' is a [[real function]] ''P'' on the atoms in ''L'' that has the following properties:\n# <math>P(0) = 0</math>, and <math>P(y) \\ge 0</math> for all <math>y \\in L</math>\n# <math>\\sum_{j=1}^n P(x_j) = 1</math>, if <math>x_1, x_2, x_3, \\ldots, x_n</math> are orthogonal atoms\n\nThis means for every lattice element ''y'', the probability of obtaining ''y'' as a measurement outcome is known, since it may be expressed as the union of the atoms under ''y'': <math display=\"block\">P(y) = \\sum \\{ P(x_j) \\mid x_j \\le y \\}. </math>\n\nIn this context, Gleason's theorem states:\n\n:''Given a quantum probability function P over a space of dimension <math>\\ge 3 </math>, there is an Hermitian, non-negative operator W on H, whose trace is unity, such that <math>P(x) = \\langle \\mathbf{x}, W \\mathbf{x} \\rangle </math> for all atoms <math>x \\in L</math>, where <math> \\langle\\,  ,\\, \\rangle</math> is the inner product, and <math>\\mathbf{x}</math> is a unit vector along <math>x</math>.\n\nAs one consequence: if some <math>x_0</math> satisfies <math>P(x_0) = 1</math>, then ''W'' is the projection onto the complex line spanned by <math>x_0</math> and <math> P(x) = \\left| \\langle\\mathbf{x_0}, \\mathbf{x} \\rangle \\right|^2</math> for all <math>x \\in L</math>.\n\n== Implications ==\nGleason's theorem highlights a number of fundamental issues in quantum measurement theory. [[QBism|Fuchs]] argues that the theorem \"is an extremely powerful result,\" because \"it indicates the extent to which the Born probability rule and even the state-space structure of density operators are ''dependent'' upon the theory's other postulates.\"  As a consequence, quantum theory is \"a tighter package than one might have first thought.\"<ref>Fuchs (2011), pp.&nbsp;94&ndash;95</ref>\n\nThe theorem is often taken to rule out the possibility of [[Hidden variable theory|hidden variables]] in quantum mechanics. This is because the theorem implies that there can be no [[Bivalence|bivalent]] probability measures, i.e. probability measures having only the values 1 and 0. To see this, note that the mapping <math>u \\rightarrow \\langle Wu, u \\rangle </math> is continuous on the [[unit sphere]] of the Hilbert space for any density operator ''W''. Since this unit sphere is [[Connected (topology)|connected]], no continuous function on it can take only the values of 0 and 1.<ref>Wilce (2017), §1.3</ref> But, a hidden variable theory which is [[determinism|deterministic]] implies that the probability of a given outcome is ''always'' either 0 or 1: either the electron's spin is up, or it isn't (which accords with [[Classical mechanics|classical]] intuitions). Gleason's theorem therefore seems to hint that quantum theory represents a deep and fundamental departure from the classical way of looking at the world. (This has been argued to support a variety of philosophical [[perspectivism]].<ref>Edwards (1979)</ref>)\n\nGleason's theorem motivated later work by [[John Stuart Bell]], [[Ernst Specker]] and [[Simon Kochen]] that led to the result often called the [[Kochen–Specker theorem]], which rules out a broad class of hidden-variable models. As noted above, Gleason's theorem shows that there is no bivalent probability measure over the rays of a Hilbert space (as long as the dimension of that space exceeds 2). The Kochen–Specker theorem refines this statement by constructing a specific finite subset of rays on which no bivalent probability measure can be defined.<ref>Peres (1991); Mermin (1993)</ref>\n\nA density operator that is a rank-1 projection is known as a ''pure'' quantum state, and all quantum states that are not pure are designated ''mixed.'' Assigning a pure state to a quantum system implies certainty about the outcome of some measurement on that system (i.e., <math>P(x) = 1</math> for some outcome ''x''). Any mixed state can be written as a [[convex combination]] of pure states, though not in a unique way. Because Gleason's theorem yields the set of all quantum states, pure and mixed, it can be taken as an argument that pure and mixed states should be treated on the same conceptual footing, rather than viewing pure states as more fundamental conceptions.<ref>Wallace (2017)</ref>\n\nTo some researchers, such as Pitowsky, the result is convincing enough to conclude that quantum mechanics represents a new theory of probability. Alternatively, such approaches as [[relational quantum mechanics]] and some versions of [[Quantum Bayesianism]] employ Gleason's theorem as an essential step in deriving the quantum formalism from [[Information theory|information-theoretic]] postulates.<ref>Barnum ''et al.'' (2000); Wilce (2017), §1.4; Cassinelli and Lahti (2017), §2</ref>\n\n== Outline of Gleason's proof ==\n\nGleason's original proof proceeds in three stages.<ref>Hrushovski and Pitowsky (2004), §2</ref> In Gleason's terminology, a frame function that is derived in the standard way&mdash;i.e., by the Born rule from a quantum state&mdash;is ''regular.'' Gleason derives a sequence of [[Lemma (mathematics)|lemma]]s concerning when a frame function is necessarily regular, culminating in the final theorem. First, he establishes that every frame function on the Hilbert space <math>\\mathbb{R}^3</math> is continuous. Then, he proves the theorem for the special case of <math>\\mathbb{R}^3</math>. Finally, he shows that the general problem can be reduced to this special case. Gleason credits one lemma used in this last stage of the proof to his doctoral student [[Richard Palais]].<ref>Gleason (1957), footnote 3</ref>\n\n== Generalizations ==\n\nGleason originally proved the theorem assuming that the measurements applied to the system are of the [[John von Neumann|von Neumann]] type, i.e., that each possible measurement corresponds to an [[orthonormal basis]] of the Hilbert space. Later, Busch, and independently [[Carlton M. Caves|Caves]] ''et al.,'' proved an analogous result for a more general class of measurements, known as [[POVM|positive operator valued measures]] (POVMs).  The proof of this result is simpler than that of Gleason's, and unlike the original theorem of Gleason, the generalized version using POVMs also applies to the case of a single [[qubit]], for which the dimension of the Hilbert space equals 2.<ref>Busch (2003); Caves ''et al.'' (2004); Fuchs (2011), p. 116</ref> This has been interpreted as showing that the probabilities for outcomes of measurements upon a single qubit cannot be explained in terms of hidden variables, provided that the class of allowed measurements is sufficiently broad.<ref>Spekkens (2005)</ref>\n\nGleason's theorem, in its original version, does not hold if the Hilbert space is defined over the [[rational number]]s, i.e., if the components of vectors in the Hilbert space are restricted to be rational numbers, or complex numbers with rational parts. However, when the set of allowed measurements is the set of all POVMs, the theorem holds.<ref>Caves ''et al.'' (2004), §3.D</ref>\n\nThe original proof by Gleason was not [[Constructive proof|constructive]]: one of the ideas on which it depends is the fact that every continuous function defined on a [[compact space]] obtains its [[minimum]]. Because one cannot in all cases explicitly show where the minimum occurs, a proof that relies upon this principle will not be a constructive proof. However, the theorem can be reformulated in such a way that a constructive proof can be found.<ref>Richman and Bridges (1999); Hrushovski and Pitowsky (2004)</ref>\n\nGleason's theorem can be extended to some cases where the observables of the theory form a [[von Neumann algebra]]. Specifically, an analogue of Gleason's result can be shown to hold if the algebra of observables has no [[direct sum]]mand that is representable as the algebra of two-by-two matrices over a commutative von Neumann algebra (i.e., no direct summand of type I<sub>2</sub>). In essence, the only barrier to proving the theorem is the fact that Gleason's original result does not hold when the Hilbert space is that of a qubit.<ref>Hamhalter (2003)</ref>\n\n== References ==\n{{Reflist}}\n* {{Cite web|url=https://golem.ph.utexas.edu/category/2010/12/solers_theorem.html|title=Solèr's Theorem|last=Baez|first=John C.|authorlink=John C. Baez|date=2010-12-01|website=[[nLab|The n-Category Café]]|archive-url=|archive-date=|dead-url=|access-date=2017-04-24}}\n* {{Cite journal|last=Barnum|first=H.|last2=Caves|first2=C. M.|authorlink2=Carlton M. Caves |last3=Finkelstein|first3=J.|last4=Fuchs|first4=C. A.|last5=Schack|first5=R.|date=2000-05-08|title=Quantum probability from decision theory?|url=http://rspa.royalsocietypublishing.org/content/456/1997/1175|journal=[[Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences]]|language=en|volume=456|issue=1997|pages=1175–1182|arxiv=quant-ph/9907024|doi=10.1098/rspa.2000.0557|issn=1364-5021|via=|bibcode=2000RSPSA.456.1175B|citeseerx=10.1.1.769.8732}}\n* {{Cite journal|last=Busch|first=Paul|date=2003|title=Quantum States and Generalized Observables: A Simple Proof of Gleason's Theorem|url=|journal=[[Physical Review Letters]]|volume=91|issue=12|pages=120403|arxiv=quant-ph/9909073|doi=10.1103/PhysRevLett.91.120403|pmid=14525351|bibcode=2003PhRvL..91l0403B}}\n* {{Cite journal|last=Cassinelli|first=G.|last2=Lahti|first2=P.|date=2017-11-13|title=Quantum mechanics: why complex Hilbert space?|url=http://rsta.royalsocietypublishing.org/content/375/2106/20160393|journal=[[Philosophical Transactions of the Royal Society A]]|language=en|volume=375|issue=2106|pages=20160393|doi=10.1098/rsta.2016.0393|issn=1364-503X|pmid=28971945|bibcode=2017RSPTA.37560393C}}\n* {{Cite journal|last=Caves|first=Carlton M.|author-link=Carlton M. Caves|last2=Fuchs|first2=Christopher A.|last3=Manne|first3=Kiran K.|last4=Renes|first4=Joseph M.|date=2004|title=Gleason-Type Derivations of the Quantum Probability Rule for Generalized Measurements|url=|journal=[[Foundations of Physics]]|volume=34|issue=2|pages=193–209|arxiv=quant-ph/0306179|doi=10.1023/B:FOOP.0000019581.00318.a5|bibcode=2004FoPh...34..193C}}\n* {{Cite journal|last=Drieschner|first=M.|last2=Görnitz|first2=Th.|last3=von Weizsäcker|first3=C. F.|authorlink3=Carl Friedrich von Weizsäcker |date=1988-03-01|title=Reconstruction of abstract quantum theory|journal=[[International Journal of Theoretical Physics]]|language=en|volume=27|issue=3|pages=289–306|doi=10.1007/bf00668895|issn=0020-7748|bibcode=1988IJTP...27..289D}}\n*{{Cite journal|last=Drisch|first=Thomas|date=1979-04-01|title=Generalization of Gleason's theorem|journal=[[International Journal of Theoretical Physics]]|language=en|volume=18|issue=4|pages=239–243|doi=10.1007/bf00671760|issn=0020-7748|bibcode=1979IJTP...18..239D}}\n* {{cite book|title = Gleason's Theorem and Its Applications|publisher = [[Wolters Kluwer|Kluwer Acad. Publ.]]|year = 1992|isbn = 978-0-7923-1990-0|series = Mathematics and its Applications, Vol. 60|location = Dordrecht|pages = 348|author = Dvurecenskij, Anatolij |oclc=751579618}}\n* {{cite journal\n |  author = Edwards, David\n |  title = The Mathematical Foundations of Quantum Mechanics\n |  journal = [[Synthese]]\n |  volume = 42\n |  year = 1979\n |  pages = 1–70\n |  url = http://www.springerlink.com/content/g047367766j30t07/\n | doi=10.1007/BF00413704\n }}\n* {{cite book|first=Christopher A.|last=Fuchs|title=Coming of Age with Quantum Information: Notes on a Paulian Idea|year = 2011|publisher = [[Cambridge University Press]]|isbn = 978-0-521-19926-1|location = Cambridge |oclc=535491156}}\n* {{cite journal|first=Andrew M.|author-link=Andrew M. Gleason|year = 1957|title = Measures on the closed subspaces of a Hilbert space|url = http://www.iumj.indiana.edu/IUMJ/FULLTEXT/1957/6/56050|journal = [[Indiana University Mathematics Journal]]|volume = 6|issue=4|pages = 885–893|doi=10.1512/iumj.1957.6.56050|mr=0096113|author = Gleason}}\n* {{Cite book|url=https://books.google.com/books?id=jU7jzm4PylUC|title=Quantum Measure Theory|last=Hamhalter|first=Jan|date=2003-10-31|publisher=[[Springer Science+Business Media|Springer Science & Business Media]]|isbn=9781402017148|language=en|mr=2015280|zbl=1038.81003 |oclc=928681664}}\n*{{Cite journal|last=Horwitz|first=L.P|last2=Biedenharn|first2=L.C|title=Quaternion quantum mechanics: Second quantization and gauge fields|url=http://linkinghub.elsevier.com/retrieve/pii/000349168490068X|year=1984|journal=[[Annals of Physics]]|volume=157|issue=2|pages=432–488|doi=10.1016/0003-4916(84)90068-x|bibcode=1984AnPhy.157..432H}}\n* {{Cite journal|last=Hrushovski|first=Ehud|last2=Pitowsky|first2=Itamar|date=2004-06-01|title=Generalizations of Kochen and Specker's theorem and the effectiveness of Gleason's theorem|url=http://www.sciencedirect.com/science/article/pii/S1355219804000024|journal=[[Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics]]|volume=35|issue=2|pages=177–194|doi=10.1016/j.shpsb.2003.10.002|arxiv=quant-ph/0307139|bibcode=2004SHPMP..35..177H}}\n* {{Cite journal|last1=Kunjwal|first1=Ravi|last2=Spekkens|first2=Rob W.|date=2015-09-09|title=From the Kochen&ndash;Specker theorem to noncontextuality inequalities without assuming determinism|url=|journal=[[Physical Review Letters]]|volume=115|issue=11|pages=110403|arxiv=1506.04150|doi=10.1103/PhysRevLett.115.110403|pmid=26406812|via=|bibcode=2015PhRvL.115k0403K}}\n* {{Cite journal|last=Mermin|first=N. David|authorlink=David Mermin|date=1993-07-01|title=Hidden variables and the two theorems of John Bell|journal=[[Reviews of Modern Physics]]|volume=65|issue=3|pages=803–815|doi=10.1103/RevModPhys.65.803|bibcode=1993RvMP...65..803M|arxiv=1802.10119}}\n*  {{Cite journal|last1=Moretti|first1=Valter|last2=Oppio|first2=Marco|date=2018-10-16|title=The Correct Formulation of Gleason’s Theorem in Quaternionic Hilbert Spaces|journal=[[Annales Henri Poincare]]|volume=19|issue=11|pages=3321–3355|doi=10.1007/s00023-018-0729-8|arxiv=1803.06882}}\n*  {{Cite journal|last1=Moretti|first1=Valter|last2=Oppio|first2=Marco|date=2019-06-01|title=Quantum theory in quaternionic Hilbert space: How Poincaré symmetry reduces the theory to the standard complex one|journal=[[Reviews in Mathematical Physics]]|volume=31|issue=4|pages=1950013|doi=10.1142/S0129055X19500132|arxiv=1709.09246}}\n* {{Cite journal|last=Peres|first=Asher|authorlink=Asher Peres|date=1991|title=Two simple proofs of the Kochen-Specker theorem|url=http://stacks.iop.org/0305-4470/24/i=4/a=003|journal=[[Journal of Physics A: Mathematical and General]]|language=en|volume=24|issue=4|pages=L175–L178|doi=10.1088/0305-4470/24/4/003|issn=0305-4470|bibcode=1991JPhA...24L.175P}}\n*{{Cite journal|last=Piron|first=C.|authorlink=Constantin Piron |date=1972-10-01|title=Survey of general quantum physics|journal=[[Foundations of Physics]]|language=en|volume=2|issue=4|pages=287–314|doi=10.1007/bf00708413|issn=0015-9018|bibcode=1972FoPh....2..287P}}\n* {{Cite journal|last=Pitowsky|first=Itamar|date=2013|title=Betting on the outcomes of measurements: a Bayesian theory of quantum probability|url=|journal=[[Studies in History and Philosophy of Science|Studies in History and Philosophy of Modern Physics]]|volume=34|issue=3|pages=395&ndash;414|arxiv=quant-ph/0208121|doi=10.1016/S1355-2198(03)00035-2|via=|bibcode=2003SHPMP..34..395P}}\n* {{Cite book|title=Physical Theory and its Interpretation: Essays in Honor of Jeffrey Bub|last=Pitowsky|first=Itamar|publisher=[[Springer Science+Business Media|Springer]]|year=2006|isbn=9781402048760|editor-last=Demopoulos|editor-first=William|location=|pages=213|chapter=Quantum mechanics as a theory of probability|arxiv=quant-ph/0510095|editor-last2=Pitowsky|editor-first2=Itamar|bibcode=2005quant.ph.10095P |oclc=917845122}}\n*{{Cite journal|last=Razon|first=Aharon|last2=Horwitz|first2=L. P.|date=1991-08-01|title=Projection operators and states in the tensor product of quaternion Hilbert modules|journal=[[Acta Applicandae Mathematicae]]| language=en|volume=24|issue=2|pages=179–194|doi=10.1007/bf00046891|issn=0167-8019}}\n* {{Cite journal|last=Richman|first=Fred|last2=Bridges|first2=Douglas|date=1999-03-10|title=A Constructive Proof of Gleason's Theorem|url=http://www.sciencedirect.com/science/article/pii/S0022123698933729|journal=[[Journal of Functional Analysis]]|volume=162|issue=2|pages=287–312|doi=10.1006/jfan.1998.3372}}\n* {{Cite journal|last=Spekkens|first=R. W.|date=2005-05-31|title=Contextuality for preparations, transformations, and unsharp measurements|journal=[[Physical Review A]]|volume=71|issue=5|pages=052108|arxiv=quant-ph/0406166|doi=10.1103/PhysRevA.71.052108|bibcode=2005PhRvA..71e2108S}}\n* {{Cite book |last=Varadarajan |first=Veeravalli S. |author-link=Veeravalli S. Varadarajan |title=Geometry of Quantum Theory |edition=2nd |publisher=[[Springer Science+Business Media]] |year=2007 |isbn=978-0-387-96124-8 |oclc=764647569}}\n* {{Cite book|last=Wallace|first=David|date=2017|title=What is Quantum Information?|chapter=Inferential versus Dynamical Conceptions of Physics|publisher=[[Cambridge University Press]]|pages=179–206|isbn=978-1-107-14211-4|editor-first1=Olimpia|editor-last1=Lombardi|editor-first2=Sebastian|editor-last2=Fortin|editor-first3=Federico|editor-last3=Holik|editor-first4=Cristian|editor-last4=López|location = Cambridge |oclc=965759965}}\n* Wilce, A. (2017). \"[https://plato.stanford.edu/entries/qt-quantlog/ Quantum Logic and Probability Theory]\". In [[Stanford Encyclopedia of Philosophy|''The Stanford Encyclopedia of Philosophy'']] (Spring 2017 Edition), Edward N. Zalta (ed.).\n\n{{DEFAULTSORT:Gleasons theorem}}\n[[Category:Hilbert space]]\n[[Category:Quantum measurement]]\n[[Category:Probability theorems]]"
    },
    {
      "title": "Jacobi operator",
      "url": "https://en.wikipedia.org/wiki/Jacobi_operator",
      "text": "A '''Jacobi operator''', also known as '''Jacobi matrix''', is a symmetric [[linear operator]] acting on [[sequence]]s which is given by an infinite [[tridiagonal matrix]]. It is commonly used to specify systems of [[orthogonal polynomials|orthonormal polynomials]] over a finite, positive [[Borel measure]]. This operator is named after [[Carl Gustav Jacob Jacobi]].\n\nThe name derives from a theorem from Jacobi, dating to 1848, stating that every [[symmetric matrix]] over a [[principal ideal domain]] is congruent to a tridiagonal matrix.\n\n== Self-adjoint Jacobi operators ==\nThe most important case is the one of self-adjoint Jacobi operators acting on the [[Hilbert space]] of square summable sequences over the [[positive integers]] <math>\\ell^2(\\mathbb{N})</math>. In this case it is given by\n\n:<math>Jf_0 = a_0 f_1 + b_0 f_0, \\quad Jf_n =  a_n f_{n+1} + b_n f_n + a_{n-1} f_{n-1}, \\quad n>0,</math>\n\nwhere the coefficients are assumed to satisfy\n\n:<math>a_n >0, \\quad b_n \\in \\mathbb{R}.</math>\n\nThe operator will be bounded if and only if the coefficients are bounded.\n\nThere are close connections with the theory of [[orthogonal polynomials]]. In fact, the solution <math>p_n(x)</math> of the [[recurrence relation]]\n\n:<math> J\\, p_n(x) = x\\, p_n(x), \\qquad p_0(x)=1 \\text{ and } p_{-1} (x)=0,</math>\n\nis a polynomial of degree ''n'' and these polynomials are [[orthonormal]] with respect to the [[spectral measure]] corresponding to the first basis vector <math>\\delta_{1,n}</math>.\n\nThis recurrence relation is also commonly written as\n:<math>xp_n(x)=a_{n+1}p_{n+1}(x) + b_n p_n(x) + a_np_{n-1}(x)</math>\n\n== Applications ==\nIt arises in many areas of mathematics and physics. The case ''a''(''n'')&nbsp;=&nbsp;1 is known as the discrete one-dimensional [[Schrödinger operator]]. It also arises in:\n\n* The [[Lax pair]] of the [[Toda lattice]].\n* The three-term recurrence relationship of [[orthogonal polynomials]], orthogonal over a positive and finite [[Borel measure]].\n* Algorithms devised to calculate [[Gaussian quadrature|Gaussian quadrature rules]], derived from systems of orthogonal polynomials.<ref>[http://www.math.unipd.it/~alvise/PAPERS/meurant_sommariva12.pdf Fast variants of the Golub and Welsch algorithm for symmetric weight functions – Gérard Meurant]</ref>\n\n== Generalizations ==\nWhen one considers [[Bergman space]], namely the space of [[square-integrable]] [[holomorphic functions]] over some domain, then, under general circumstances, one can give that space a basis of orthogonal polynomials, the [[Bergman polynomial]]s. In this case, the analog of the tridiagonal Jacobi operator is a [[Hessenberg operator]] – an infinite-dimensional [[Hessenberg matrix]]. The system of orthogonal polynomials is given by\n\n:<math>zp_n(z)=\\sum_{k=0}^{n+1} D_{kn} p_k(z)</math>\n\nand <math>p_0(z)=1</math>. Here, ''D'' is the Hessenberg operator that generalizes the tridiagonal Jacobi operator ''J'' for this situation.<ref>V. Tomeo, E. Torrano, \"Two applications of the subnormality of the Hessenberg matrix related to general orthogonal polynomials\", ''Linear Algebra and its Applications'' (2011) Volume 435, Issue 9, Pages 2314-2320, DOI=https://doi.org/10.1016/j.laa.2011.04.027 URL=http://oa.upm.es/id/eprint/8725/contents</ref><ref>Edward B. Saff and Nikos Stylianopoulos, \"Asymptotics for Hessenberg matrices for the Bergman shift operator on Jordan region\", (2012) arXiv:1205.4183 [math.CV] URL=https://arxiv.org/abs/1205.4183</ref><ref>Carmen Escribano and Antonio Giraldo and M. Asunción Sastre and Emilio Torrano, \"The Hessenberg matrix and the Riemann mapping\" (2011){arXiv:1107.603 [math.SP] URL=https://arxiv.org/abs/1107.6036</ref>  Note that ''D'' is the right-[[shift operator]] on the Bergman space: that is, it is given by\n\n:<math>[Df](z) = zf(z)</math>\n\nThe zeros of the Bergman polynomial <math>p_n(z)</math> correspond to the [[eigenvalue]]s of the principle <math>n\\times n</math> submatrix of ''D''.  That is, The Bergman polynomials are the [[characteristic polynomial]]s for the principle submatrixes of the shift operator.\n\n==References==\n{{reflist}}\n*{{citation|title=Jacobi Operators and Completely Integrable Nonlinear Lattices\n|first= Gerald|last=Teschl|authorlink=Gerald Teschl|publisher=Amer. Math. Soc.|location=Providence|year=2000|url=http://www.mat.univie.ac.at/~gerald/ftp/book-jac/|isbn=0-8218-1940-2}}\n==External links==\n* {{eom|title=Jacobi matrix|id=Jacobi_matrix}}\n\n{{DEFAULTSORT:Jacobi Operator}}\n[[Category:Operator theory]]\n[[Category:Hilbert space]]\n[[Category:Recurrence relations]]"
    },
    {
      "title": "Kirszbraun theorem",
      "url": "https://en.wikipedia.org/wiki/Kirszbraun_theorem",
      "text": "In [[mathematics]], specifically [[real analysis]] and [[functional analysis]], the '''Kirszbraun theorem''' states that if ''U'' is a [[subset]] of some [[Hilbert space]] ''H''<sub>1</sub>, and ''H''<sub>2</sub> is another Hilbert space, and \n\n:''f'' : ''U'' → ''H''<sub>2</sub>\n\nis a [[Lipschitz continuity|Lipschitz-continuous]] map, then there is a Lipschitz-continuous map \n\n:''F'': ''H''<sub>1</sub> → ''H''<sub>2</sub>\n\nthat extends ''f'' and has the same Lipschitz constant as ''f''.\n\nNote that this result in particular applies to [[Euclidean space]]s '''E'''<sup>''n''</sup> and '''E'''<sup>''m''</sup>, and it was in this form that Kirszbraun originally formulated and proved the theorem.<ref>{{cite journal |first=M. D. |last=Kirszbraun |title=Über die zusammenziehende und Lipschitzsche Transformationen |journal=Fund. Math. |volume=22 |issue= |pages=77–108 |year=1934 |doi= }}</ref> The version for Hilbert spaces can for example be found in (Schwartz 1969, p.&nbsp;21).<ref name=\"Schwartz1969\">{{cite book |authorlink=Jack Schwartz |first=J. T. |last=Schwartz |title=Nonlinear functional analysis |publisher=Gordon and Breach Science |location=New York |year=1969 }}</ref> If ''H''<sub>1</sub> is a [[separable space]] (in particular, if it is a Euclidean space) the result is true in [[Zermelo–Fraenkel set theory]];  for the fully general case, it appears to need some form of the axiom of choice;  the [[Boolean prime ideal theorem]] is known to be sufficient.<ref>{{cite paper |first=D. H. |last=Fremlin |year=2011 |title=Kirszbraun's theorem |work=Preprint |url=http://www.essex.ac.uk/maths/people/fremlin/n11706.pdf }}</ref>\n\nThe proof of the theorem uses geometric features of Hilbert spaces; the corresponding statement for [[Banach space]]s is not true in general, not even for finite-dimensional Banach spaces. It is for instance possible to construct counterexamples where the domain is a subset of '''R'''<sup>''n''</sup> with the [[Uniform norm|maximum norm]] and '''R'''<sup>''m''</sup> carries the Euclidean norm.<ref>{{cite book |first=H. |last=Federer |title=Geometric Measure Theory |publisher=Springer |location=Berlin |year=1969 |page=202 |isbn= }}</ref> More generally, the theorem fails for <math> \\mathbb{R}^m </math> equipped with any <math> \\ell_p</math> norm (<math> p \\neq 2</math>) (Schwartz 1969, p.&nbsp;20).<ref name=\"Schwartz1969\" />\n\nFor an '''R'''-valued function the extension is provided by <math>\\tilde f(x):=\\inf_{u\\in U}\\big(f(u)+\\text{Lip}(f)\\cdot d(x,u)\\big),</math> where <math>\\text{Lip}(f)</math> is f's Lipschitz constant on U.\n\n==History==\n\nThe theorem was proved by [[Mojżesz David Kirszbraun]], and later it was reproved by [[Frederick Valentine]],<ref>{{cite journal |first=F. A. |last=Valentine |title=A Lipschitz Condition Preserving Extension for a Vector Function |journal=[[American Journal of Mathematics]] |volume=67 |issue=1 |year=1945 |pages=83–93 |doi=10.2307/2371917 }}</ref> who first proved it for the Euclidean plane.<ref>{{cite journal |first=F. A. |last=Valentine |title=On the extension of a vector function so as to preserve a Lipschitz condition |journal=Bulletin of the American Mathematical Society |volume=49 |issue= |pages=100–108 |year=1943 |mr=0008251 |doi=10.1090/s0002-9904-1943-07859-7}}</ref> Sometimes this theorem is also called '''Kirszbraun–Valentine theorem'''.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.encyclopediaofmath.org/index.php/Kirszbraun_theorem ''Kirszbraun theorem''] at [[Encyclopedia of Mathematics]].\n\n{{DEFAULTSORT:Kirszbraun Theorem}}\n[[Category:Lipschitz maps]]\n[[Category:Metric geometry]]\n[[Category:Theorems in real analysis]]\n[[Category:Theorems in functional analysis]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Liouville space",
      "url": "https://en.wikipedia.org/wiki/Liouville_space",
      "text": "In mathematics, a '''Liouville space''', also known as a '''line space''' or an '''extended Hilbert space''' is [[Cartesian product]] of two [[Hilbert space]]s<ref>{{cite web|title=Hilbert space and liouville space|url=https://www.chemie.uni-hamburg.de/nmr/insensitive/tutorial/en.lproj/hilbert_space.html}}</ref>.\n\n==References==\n<references />\n\n[[Category:Hilbert space]]\n[[Category:Linear algebra]]\n[[Category:Operator theory]]\n[[Category:Functional analysis]]\n\n\n{{Abstract-algebra-stub}}"
    },
    {
      "title": "Moment problem",
      "url": "https://en.wikipedia.org/wiki/Moment_problem",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Trying to map moments to a measure that generates them}}\nIn [[mathematics]], a '''moment problem''' arises as the result of trying to invert the mapping that takes a [[measure (mathematics)|measure]] &mu; to the sequences of [[Moment (mathematics)|moment]]s\n\n:<math>m_n = \\int_{-\\infty}^\\infty x^n \\,d\\mu(x)\\,.</math>\n\nMore generally, one may consider\n:<math>m_n = \\int_{-\\infty}^\\infty M_n(x) \\,d\\mu(x)\\,.</math>\nfor an arbitrary sequence of functions ''M''<sub>''n''</sub>.\n\n== Introduction ==\n\nIn the classical setting, &mu; is a measure on the [[real line]], and ''M'' is the sequence { ''x''<sup>''n''</sup> : ''n'' = 0, 1, 2, ... }. In this form the question appears in [[probability theory]], asking whether there is a  [[probability measure]] having specified [[mean]], [[variance]] and so on, and whether it is unique.\n\nThere are three named classical moment problems: the [[Hamburger moment problem]] in which the [[support (mathematics)|support]] of &mu; is allowed to be the whole real line; the [[Stieltjes moment problem]], for <nowiki>[0, +&infin;)</nowiki>; and the [[Hausdorff moment problem]] for a bounded interval, which [[without loss of generality]] may be taken as [0,&nbsp;1].\n\n==Existence==\n\nA sequence of numbers ''m''<sub>''n''</sub> is the sequence of moments of a measure ''&mu;'' if and only if a certain positivity condition is fulfilled; namely, the [[Hankel matrices]] ''H''<sub>''n''</sub>,\n\n:<math>(H_n)_{ij} = m_{i+j}\\,,</math>\n\nshould be [[positive-definite matrix|positive semi-definite]]. This is because a positive-semidefinite Hankel matrix corresponds to a linear functional <math> \\Lambda</math> such that <math>\\Lambda(x^n) = m_n</math> and  <math> \\Lambda(f^2) \\geq 0 </math> (non-negative for sum of squares of polynomials). Assume <math> \\Lambda</math> can be  extended to <math> \\mathbb{R}[x]^*</math>. In the univariate case, a non-negative polynomial can always be written as a sum of squares. So the linear functional <math> \\Lambda</math> is positive for all the non-negative polynomials in the univariate case. By Haviland's theorem, the linear functional has a measure form, that is <math> \\Lambda(x^n) = \\int_{-\\infty}^{\\infty} x^n d \\mu</math>. A condition of similar form is necessary and sufficient for the existence of a measure <math>\\mu</math> supported on a given interval [''a'',&nbsp;''b''].\n\nOne way to prove these results is to consider the linear functional <math>\\scriptstyle\\varphi</math> that sends a polynomial\n\n: <math>P(x) = \\sum_k a_k x^k </math>\n\nto\n\n: <math>\\sum_k a_k m_k.</math>\n\nIf ''m''<sub>''kn''</sub> are the moments of some measure ''&mu;'' supported on [''a'',&nbsp;''b''], then evidently\n\n:{{NumBlk|:|''&phi;''(''P'')&nbsp;&ge;&nbsp;0 for any polynomial ''P'' that is non-negative on [''a'',&nbsp;''b''].|{{EquationRef|1}}}}\n\nVice versa, if ({{EquationNote|1}}) holds, one can apply the [[M. Riesz extension theorem]] and extend <math>\\phi</math> to a functional on the space of continuous functions with compact support ''C''<sub>0</sub>([''a'',&nbsp;''b'']), so that\n\n:{{NumBlk|:|<math>\\qquad \\varphi(f) \\ge 0\\text{ for any } f \\in C_0([a,b]),\\;f\\ge 0</math>.|{{EquationRef|2}}}}\n\nBy the [[Riesz representation theorem#The representation theorem for linear functionals on Cc.28X.29|Riesz representation theorem]], ({{EquationNote|2}}) holds iff there exists a measure ''&mu;'' supported on [''a'',&nbsp;''b''], such that\n\n: <math> \\varphi(f) = \\int f \\, d\\mu</math>\n\nfor every ''&fnof;''&nbsp;&isin;&nbsp;''C''<sub>0</sub>([''a'',&nbsp;''b'']).\n\nThus the existence of the measure <math>\\mu</math> is equivalent to ({{EquationNote|1}}). Using a representation theorem for positive polynomials on [''a'',&nbsp;''b''], <!-- This is due to Riesz or Fejer (or maybe both); a ref. is needed (maybe Szego's book?) --> one can reformulate ({{EquationNote|1}}) as a condition on [[Hankel matrices]].\n\nSee {{harvnb|Shohat|Tamarkin|1943}} and {{harvnb|Krein|Nudelman|1977}} for more details.\n\n== Uniqueness (or determinacy) ==\n\nThe uniqueness of ''&mu;'' in the Hausdorff moment problem follows from the [[Weierstrass approximation theorem]], which states that [[polynomial]]s are [[dense set|dense]] under the [[uniform norm]] in the space of [[continuous functions]] on [0,&nbsp;1]. For the problem on an infinite interval, uniqueness is a more delicate question; see [[Carleman's condition]], [[Krein's condition]] and {{harvtxt|Akhiezer|1965}}.\n\n== Variations ==\n\nAn important variation is the [[truncated moment problem]], which studies the properties of measures with fixed first ''k'' moments (for a finite ''k''). Results on the truncated moment problem have numerous applications to [[extremal problems]], optimisation and limit theorems in [[probability theory]]. See also: [[Chebyshev–Markov–Stieltjes inequalities]] and {{harvnb|Krein|Nudelman|1977}}.\n\n== See also ==\n*[[Stieltjes moment problem]]\n*[[Hamburger moment problem]]\n*[[Hausdorff moment problem]]\n*[[Moment (mathematics)]]\n*[[Carleman's condition]]\n*[[Hankel matrix]]\n\n==References==\n*{{cite book | last1 =  Shohat | first1 = James Alexander | first2 = Jacob D. | last2 = Tamarkin | authorlink2 = Jacob Tamarkin | title = The Problem of Moments | publisher = American mathematical society | location = New York | year = 1943 | ref = harv}}\n*{{cite book | last1 = Akhiezer | first1 = Naum I. | authorlink1 = Naum Akhiezer | title = The classical moment problem and some related questions in analysis | publisher = Hafner Publishing Co. |location = New York | year = 1965 | ref = harv}} (translated from the Russian by N. Kemmer)\n*{{cite book | last1 = Krein | first1 = M. G. | last2 = Nudelman |first2 = A. A. |title = The Markov moment problem and extremal problems.  Ideas and problems of P. L. Chebyshev and A. A. Markov and their further development | volume = Translations of Mathematical Monographs, Vol. 50 | publisher = American Mathematical Society, Providence, R.I. | year = 1977 | ref = harv}} (Translated from the Russian by D. Louvish)\n*{{cite book | last1 = Schmüdgen | first1 = Konrad | title = The moment problem  | publisher = Springer International Publishing | year = 2017 | ref = harv}} \n\n[[Category:Mathematical analysis]]\n[[Category:Hilbert space]]\n[[Category:Probability problems]]\n[[Category:Moment (mathematics)]]\n[[Category:Mathematical problems]]\n[[Category:Real algebraic geometry]]\n[[Category:Optimization in vector spaces]]"
    },
    {
      "title": "Morse–Palais lemma",
      "url": "https://en.wikipedia.org/wiki/Morse%E2%80%93Palais_lemma",
      "text": "In [[mathematics]], the '''Morse–Palais lemma''' is a result in the [[calculus of variations]] and theory of [[Hilbert spaces]]. Roughly speaking, it states that a [[smooth function|smooth]] enough [[function (mathematics)|function]] near a critical point can be expressed as a [[quadratic form]] after a suitable change of coordinates.\n\nThe Morse–Palais lemma was originally proved in the finite-dimensional case by the [[United States|American]] [[mathematician]] [[Marston Morse]], using the [[Gram–Schmidt process|Gram–Schmidt orthogonalization process]]. This result plays a crucial role in [[Morse theory]].  The generalization to Hilbert spaces is due to [[Richard Palais]] and [[Stephen Smale]].\n\n==Statement of the lemma==\n\nLet (''H'',&nbsp;〈&nbsp;,&nbsp;〉) be a [[real number|real]] Hilbert space, and let ''U'' be an [[open set|open neighbourhood]] of 0 in ''H''. Let ''f''&nbsp;:&nbsp;''U''&nbsp;→&nbsp;'''R''' be a (''k''&nbsp;+&nbsp;2)-times continuously [[differentiable function]] with ''k''&nbsp;≥&nbsp;1, i.e. ''f''&nbsp;∈&nbsp;''C''<sup>''k''+2</sup>(''U'';&nbsp;'''R'''). Assume that ''f''(0)&nbsp;=&nbsp;0 and that 0 is a non-degenerate [[critical point (mathematics)|critical point]] of ''f'', i.e. the second derivative D<sup>2</sup>''f''(0) defines an [[isomorphism]] of ''H'' with its [[continuous dual space]] ''H''<sup>∗</sup> by\n\n:<math>H \\ni x \\mapsto \\mathrm{D}^{2} f(0) ( x, - ) \\in H^{*}. </math>\n\nThen there exists a subneighbourhood ''V'' of 0 in ''U'', a [[diffeomorphism]] ''φ''&nbsp;:&nbsp;''V''&nbsp;→&nbsp;''V'' that is ''C''<sup>''k''</sup> with ''C''<sup>''k''</sup> inverse, and an [[invertible function|invertible]] [[symmetric operator]] ''A''&nbsp;:&nbsp;''H''&nbsp;→&nbsp;''H'', such that\n\n:<math>f(x) = \\langle A \\varphi(x), \\varphi(x) \\rangle</math>\n\nfor all ''x''&nbsp;∈&nbsp;''V''.\n\n==Corollary==\n\nLet ''f''&nbsp;:&nbsp;''U''&nbsp;→&nbsp;'''R''' be ''C''<sup>''k''+2</sup> such that 0 is a non-degenerate critical point. Then there exists a ''C''<sup>''k''</sup>-with-''C''<sup>''k''</sup>-inverse diffeomorphism ''ψ''&nbsp;:&nbsp;''V''&nbsp;→&nbsp;''V'' and an orthogonal decomposition\n\n:<math>H = G \\oplus G^{\\perp},</math>\n\nsuch that, if one writes\n\n:<math>\\psi (x) = y + z \\mbox{ with } y \\in G, z \\in G^{\\perp},</math>\n\nthen\n\n:<math>f (\\psi(x)) = \\langle y, y \\rangle - \\langle z, z \\rangle</math>\n\nfor all ''x''&nbsp;∈&nbsp;''V''.\n\n==References==\n\n* {{cite book | last=Lang | first=Serge | authorlink= Serge Lang | title=Differential manifolds | publisher=Addison–Wesley Publishing Co., Inc. | location=Reading, Mass.&ndash;London&ndash;Don Mills, Ont. | year=1972 }}\n\n{{DEFAULTSORT:Morse-Palais lemma}}\n[[Category:Calculus of variations]]\n[[Category:Hilbert space]]\n[[Category:Lemmas]]"
    },
    {
      "title": "Positive-definite kernel",
      "url": "https://en.wikipedia.org/wiki/Positive-definite_kernel",
      "text": "In [[operator theory]], a branch of mathematics, a '''positive-definite kernel''' is a generalization of a [[positive-definite function]] or a [[positive-definite matrix]]. It was first introduced by [[James Mercer (mathematician)|James Mercer]] in the early 20th century, in the context of solving [[integral equation|integral operator equations]]. Since then positive definite functions and their various analogues and generalizations have arisen in diverse parts of mathematics. They occur naturally in [[Fourier analysis]], [[probability theory]], [[operator theory]], [[complex analysis|complex function-theory]], [[moment problem]]s, [[integral equation]]s, [[boundary value problem|boundary-value problems]] for [[partial differential equation]]s, [[machine learning]], [[embedding problem]], [[information theory]], and other areas.\n\nThis article will discuss some of the historical and current developments of the theory of positive definite kernels, starting with the general idea and properties before considering practical applications.\n\n== Definition ==\n\nLet <math> \\mathcal X </math> be a nonempty set, sometimes referred to as the index set. A [[symmetric function]] <math> K: \\mathcal X \\times \\mathcal X \\to \\mathbb{R}</math> is called a positive definite (p.d.) kernel on <math>\\mathcal X</math> if\n:<math>\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j K(x_i, x_j) \\ge 0\\quad\\quad\\quad\\quad(1.1)</math>\nholds for any <math>n\\in \\mathbb{N}, x_1, \\dots, x_n\\in \\mathcal X, c_1, \\dots, c_n \\in \\mathbb{R}</math>. \n\nIn machine learning and probability theory, a distinction is sometimes made between positive definite kernels, for which equality in (1.1) implies <math>c_i = 0\\; (\\forall i)</math>, and positive semi-definite (p.s.d.) kernels, which do not impose this condition. Note that this is equivalent to requiring that any finite matrix constructed by pairwise evaluation, <math>\\mathbf{K}_{ij} = K(x_i, x_j)</math>, has either entirely positive (p.d.) or nonnegative (p.s.d.) eigenvalues.\n\nIn mathematical literature, kernels are usually complex valued functions, but in this article we assume real-valued functions, which is the common practice in machine learning and other applications of p.d. kernels.\n\n=== Some general properties === \n* For a family of p.d. kernels <math> (K_i)_{i\\in\\mathbb{N}},\\ \\  K_i:\\mathcal X\\times \\mathcal X\\to R</math>\n** The sum <math> \\sum_{i=1}^n \\lambda_i K_i</math>  is p.d., given <math> \\lambda_1,\\dots,\\lambda_n \\ge 0</math>\n** The product <math> K_1^{a_1}\\dots K_n^{a_n}</math> is p.d., given <math> a_1,\\dots,a_n\\in \\mathbb{N}</math>\n** The limit <math> K = \\lim_{n\\to\\infty}K_n</math> is p.d. if the limit exists.\n* If <math> (\\mathcal X_i)_{i=1}^n </math>  is a sequence of sets, and <math> (K_i)_{i=1}^n,\\ \\  K_i:\\mathcal X_i\\times \\mathcal X_i\\to R</math> a sequence of p.d. kernels, then both\n: <math> K((x_1,\\dots,x_n),(y_1,\\dots,y_n)) =\\prod_{i=1}^n K_i(x_i,y_i) </math>  and\n: <math> K((x_1,\\dots,x_n),(y_1,\\dots,y_n)) =\\sum_{i=1}^n K_i(x_i,y_i) </math>\n: are p.d. kernels on <math> \\mathcal X=\\mathcal X_1\\times\\dots\\times \\mathcal X_n</math>.\n* Let <math> \\mathcal X_0\\subset \\mathcal X</math>. Then the restriction <math> K_0</math> of <math> K</math> to <math> \\mathcal X_0\\times \\mathcal X_0</math> is also a p.d. kernel.\n\n=== Examples of p.d. kernels ===\n\n* Common examples of p.d. kernels defined on Euclidean space <math>\\mathbb{R}^d</math> include:\n** Linear kernel: <math> K(x,y) = x^Ty, \\quad x,y \\in\\mathbb{R}^d</math>.\n** [[Polynomial kernel]]: <math> K(x,y) =(x^Ty+r)^n, \\quad x,y\\in\\mathbb{R}^d, r>=0 </math>.\n** Gaussian kernel ([[Radial basis function kernel|RBF Kernel]]): <math> K(x,y) =e^{-\\frac{\\|x-y\\|^2}{2\\sigma^2}}, \\quad x,y\\in\\mathbb{R}^d, \\sigma>0</math>.\n** Laplacian kernel: <math> K(x,y) =e^{-\\alpha\\|x-y\\|}, \\quad x,y\\in\\mathbb{R}^d, \\alpha>0</math>.\n** Abel kernel: <math> K(x,y) =e^{-\\alpha|x-y|}, x,y \\quad \\in\\mathbb{R}, \\alpha>0</math>.\n** kernel generating [[Sobolev space]]s <math>W^k_2(\\mathbb{R}^d)</math>: <math> K(x,y) = \\|x-y\\|_2^{k-\\frac d2}B_{k-\\frac d2}(\\|x-y\\|_2)</math>, where <math>B_\\nu</math> is the Bessel function of third kind.\n** kernel generating Paley-Wiener space: <math> K(x,y) = \\mbox{sinc}(\\alpha(x-y)), x,y\\in\\mathbb{R}, \\alpha>0</math>.\n* If <math>H</math> is a Hilbert space, then its corresponding inner product <math>(\\cdot,\\cdot)_H:H\\times H\\to \\mathbb{R}</math> is a p.d. kernel. Indeed, we have\n::<math> \\sum_{i,j=1}^n c_i c_j (x_i, x_j)_H = \\left(\\sum_{i=1}^n c_i x_i, \\sum_{j=1}^n c_j x_j\\right)_H= \\left\\|\\sum_{i=1}^n c_ix_i\\right\\|_H^2\\ge 0 </math>\n\n* Kernels defined on <math>\\mathbb{R}_+^d</math>  and histograms: Histograms are frequently encountered in applications of machine learning to real-life problems. Most observations are usually available under the form of nonnegative vectors of counts, which, if normalized, yield histograms of frequencies. It has been shown <ref>Hein, M. and Bousquet, O. (2005). \"[https://pdfs.semanticscholar.org/39eb/fbb53b041b97332cd351886749c0395037fb.pdf#page=145 Hilbertian metrics and positive definite kernels on probability measures]\". In Ghahramani, Z. and Cowell, R., editors, Proceedings of AISTATS 2005.</ref> that the following family of squared metrics, respectively Jensen divergence, the <math>\\chi</math>-square, Total Variation, and two variations of the Hellinger distance:\n::<math> \\psi_{JD}= H\\left(\\frac{\\theta+\\theta'}2\\right)-\\frac{H(\\theta)+H(\\theta')}2,</math>\n::<math> \\psi_{\\chi^2}= \\sum_i\\frac{(\\theta_i-\\theta_i')^2}{\\theta_i+\\theta_i'},\\quad \\psi_{TV}= \\sum_i |\\theta_i-\\theta_i'|,</math>\n::<math> \\psi_{H_1}= \\sum_i |\\sqrt{\\theta_i}-\\sqrt{\\theta_i'}|,\\psi_{H_2}= \\sum_i |\\sqrt{\\theta_i}-\\sqrt{\\theta_i'}|^2,</math>\ncan be used to define p.d. kernels using the following formula\n:: <math> K(\\theta,\\theta') = e^{-\\alpha \\psi(\\theta,\\theta')}, \\alpha>0. </math>\n\n== History ==\n\nPD kernels, as defined in (1.1), have arisen first in 1909 in a paper on integral equations by James Mercer.<ref>Mercer, J. (1909). “Functions of positive and negative type and their connection with the theory of integral equations”. Philosophical Transactions of the Royal Society of London, Series A 209, pp. 415-446.</ref> Several other authors made use of this concept in the following two decades, but none of them explicitly used kernels <math>K(x,y) = f(x-y)</math>, i.e. p.d. functions (indeed M. Mathias and [[Salomon Bochner|S. Bochner]] seem not to have been aware of the study of p.d. kernels). Mercer’s work arose from Hilbert’s paper of 1904 <ref>Hilbert, D. (1904). \"Grundzuge einer allgemeinen Theorie der linearen Integralgleichungen I\", Gott. Nachrichten, math.-phys. K1 (1904), pp. 49-91.</ref> on [[Fredholm integral equation]]s of the second kind:\n\n:<math> f(s) = \\phi(s) - \\lambda \\int_a^b K(s,t)\\phi(t)dt.\\qquad\\qquad(1.2)</math>\n\nIn particular, Hilbert had shown that\n\n:<math> \\int_a^b \\int_a^b K(s,t)x(s)x(t)dsdt = \\sum \\frac1{\\lambda_n}\\left[\\int_a^b \\psi_n(s)x(s)ds\\right]^2,\\qquad\\qquad (1.3)</math> \t\t\n\nwhere <math>K</math> is a continuous real symmetric kernel, <math>x</math> is continuous, <math>\\{\\psi_n\\}</math>  is a complete system of [[eigenfunction|orthonormal eigenfunctions]], and <math>\\lambda_n</math>’s are the corresponding [[eigenvalues and eigenvectors|eigenvalues]] of (1.2). Hilbert defined a “definite” kernel as one for which the double integral \n\n:<math>J(x) =\\int_a^b \\int_a^b K(s,t)x(s)x(t)dsdt</math> \n\nsatisfies <math>J(x)>0</math> except for <math>x(t) = 0</math>. The original object of Mercer’s paper was to characterize the kernels which are definite in the sense of Hilbert, but Mercer soon found that the class of such functions was too restrictive to characterize in terms of determinants. He therefore defined a continuous real symmetric kernel <math>K(s,t)</math> to be of positive type (i.e. positive definite) if <math>J(x)\\ge 0</math> for all real continuous functions <math>x</math> on <math>[a,b]</math>, and he proved that (1.1) is a necessary and sufficient condition for a kernel to be of positive type. Mercer then proved that for any continuous p.d. kernel the expansion\n\n:<math> K(s,t)=\\sum \\frac{\\psi_n(s)\\psi_n(t)}{\\lambda_n} </math>\n\nholds absolutely and uniformly.\n\nAt about the same time W. H. Young,<ref>Young, W. H. (1909). \"A note on a class of symmetric functions and on a theorem required in the theory of integral equations\", Philos. Trans. Roy.Soc. London, Ser. A, 209, pp. 415-446.</ref> motivated by a different question in the theory of integral equations, showed that for continuous kernels condition (1.1) is equivalent to <math>J(x)\\ge 0</math> for all <math>x\\in L^1[a,b]</math>.\n\nE.H. Moore <ref>Moore, E.H. (1916). \"On properly positive Hermitian matrices\", Bull. Amer. Math. Soc. 23, 59, pp. 66-67.</ref><ref>Moore, E.H. (1935). \"General Analysis, Part I\", Memoirs Amer. Philos. Soc. 1, Philadelphia.</ref> initiated the study of a very general kind of p.d. kernel. If <math>E</math> is an abstract set, he calls functions <math>K(x,y)</math> defined on <math>E\\times E</math> “positive Hermitian matrices” if they satisfy (1.1) for all <math>x_i\\in E</math>. Moore was interested in generalization of integral equations and showed that to each  such <math>K</math> there is a Hilbert space <math>H</math> of functions such that, for each <math>f\\in H, f(y) =(f,K(\\cdot,y))_H</math>. This property is called the reproducing property of the kernel and turns out to have importance in the solution of boundary-value problems for elliptic partial differential equations, and is the main reason for the success of kernel methods in machine learning. More details on this will be presented in the following section.\n\nAnother line of development in which p.d. kernels played a large role was the theory of harmonics on homogeneous spaces as begun by [[Élie Cartan|E. Cartan]] in 1929, and continued by [[Hermann Weyl|H. Weyl]] and S. Ito. The most comprehensive theory of p.d. kernels in homogeneous spaces is that of [[Mark Krein|M. Krein]]<ref>Krein. M (1949/1950). \"Hermitian-positive kernels on homogeneous spaces I and II\" (in Russian), Ukrain. Mat. Z. 1(1949), pp. 64-98, and 2(1950), pp. 10-59. English translation: Amer. Math. Soc. Translations Ser. 2, 34 (1963), pp. 69-164.</ref> which includes as special cases the work on p.d. functions and irreducible [[Positive-definite function on a group|unitary representations]] of locally compact groups.\n\nIn probability theory p.d. kernels arise as covariance kernels of stochastic processes.<ref>Loève, M. (1960). \"Probability theory\", 2nd ed., Van Nostrand, Princeton, N.J.</ref>\n\n== Connection with reproducing kernel Hilbert spaces and feature maps ==\n\n{{further|Reproducing kernel Hilbert space}}\n\nPositive definite kernels provide a framework that encompasses some basic Hilbert space constructions. In the following we present a tight relationship between positive definite kernels and two mathematical objects, namely reproducing Hilbert spaces and feature maps.\n\nLet <math>X</math> be a set, <math>H</math> a Hilbert space of functions <math>f:X\\to \\mathbb{R}</math>, and <math>(\\cdot,\\cdot)_H:H\\times H\\to \\mathbb{R}</math> the corresponding inner product on <math>H</math>. For any <math>x\\in X</math> the evaluation functional <math>e_x:H\\to \\mathbb{R}</math> is defined by <math>f\\mapsto e_x(f) =f(x)</math>.\nWe first define a reproducing kernel Hilbert space (RKHS):\n\n<blockquote>'''Definition''': Space <math>H</math> is called a reproducing kernel Hilbert space if the evaluation functionals are continuous. </blockquote>\n\nEvery RKHS has a special function associated to it, namely the reproducing kernel:\n\n<blockquote>'''Definition''': Reproducing kernel is a function <math>K:X\\times X \\to \\mathbb{R}</math> such that\n:: 1) <math>K_x(\\cdot)\\in H, \\forall x\\in X</math>, and\n:: 2) <math>(f,K_x) =f(x)</math>, for all <math>f\\in H</math> and <math>x\\in X</math>.\nThe latter property is called the reproducing property. </blockquote>\n\nThe following result shows equivalence between RKHS and reproducing kernels:\n\n<blockquote>'''Theorem''': Every reproducing kernel <math>K</math> induces a unique RKHS, and every RKHS has a unique reproducing kernel. </blockquote>\n\nNow the connection between p.d. kernels and RKHS is given by the following theorem\n\n<blockquote>'''Theorem''': Every reproducing kernel is positive definite, and every p.d. kernel defines a unique RKHS, of which it is the unique reproducing kernel. </blockquote>\n\nThus given a positive definite kernel <math>K</math>, it is possible to build an associated RKHS with <math>K</math> as a reproducing kernel.\n\nAs stated earlier, p.d. kernels can be constructed from inner products. This fact can be used to connect p.d. kernels with another interesting object that arises in machine learning applications, namely the feature map. Let <math>F</math> be a Hilbert space, and <math>(\\cdot,\\cdot)_F</math> the corresponding inner product. Any map <math>\\Phi: X\\to F</math> is called a feature map. In this case we call <math>F</math> the feature space. It is easy to see <ref>Rosasco, L. and Poggio, T. (2015).  \"A Regularization Tour of Machine Learning - MIT 9.520 Lecture Notes\" Manuscript.</ref> that every feature map defines a unique p.d. kernel by\n:<math>K(x,y) =(\\Phi(x),\\Phi(y))_F.</math>\nIndeed, positive definiteness of <math>K</math> follows from the p.d. property of the inner product. On the other hand, every p.d. kernel, and its corresponding RKHS, have many associated feature maps. For example: Let <math>F=H</math>, and <math>\\Phi(x) =K_x</math> for all <math>x\\in X</math>. Then <math>(\\Phi(x),\\Phi(y))_F = (K_x,K_y)_H = K(x,y)</math>, by the reproducing property.\nThis suggests a new look at p.d. kernels as inner products in appropriate Hilbert spaces, or in other words p.d. kernels can be viewed as similarity maps which quantify effectively how similar two points <math>x</math>  and <math>y</math> are through the value <math>K(x,y)</math>. Moreover, through the equivalence of p.d. kernels and its corresponding RKHS, every feature map can be used to construct a RKHS.\n\n== Kernels and distances ==\n\nKernel methods, which are very popular machine learning applications of p.d. kernels, are often compared to distance based methods such as [[k-nearest neighbors algorithm|nearest neighbors]]. In this section we discuss parallels between their two respective ingredients, namely kernels <math>K</math> and distances <math>d</math>.\n\nHere by a distance function between each pair of elements of some set <math>X</math>, we mean a [[Metric (mathematics)|metric]] defined on that set, i.e. any nonnegative-valued function <math>d</math> on <math>\\mathcal X\\times \\mathcal X</math> which satisfies\n\n* <math> d(x,y)\\ge 0</math>, and <math> d(x,y)=0</math> if and only if <math>x=y</math>,\n* <math> d(x,y)=d(y,x) </math>,\n* <math> d(x,z)\\le d(x,y)+d(y,z)</math>.\n\nOne link between distances and p.d. kernels is given by a particular kind of kernel, called a negative definite kernel, and defined as follows\n\n<blockquote>'''Definition''': A symmetric function <math>\\psi:\\mathcal X\\times \\mathcal X\\to \\mathbb{R}</math> is called a negative definite (n.d.) kernel on <math>\\mathcal X</math> if\n:<math>\\sum_{i,j=1}^n c_i c_j \\psi(x_i, x_j) \\le 0\\quad\\quad\\quad\\quad(1.4)</math>\nholds for any <math>n\\in \\mathbb{N}, x_1, \\dots, x_n\\in \\mathcal X,</math> and <math>c_1, \\dots, c_n \\in \\mathbb{R}</math> such that\n<math>\\sum_{i=1}^n c_i=0</math>.\n</blockquote>\nThe parallel between n.d. kernels and distances is in the following: whenever a n.d. kernel vanishes on the set <math>\\{(x,x):x\\in\\mathcal X\\}</math>, and is zero only on this set, then its square root is a distance for <math>\\mathcal X</math>.<ref>Berg, C., Christensen, J. P. R., and Ressel, P. (1984). \"Harmonic Analysis on Semigroups\". Number 100 in Graduate Texts in Mathematics, Springer Verlag.</ref> At the same time each distance does not correspond necessarily to a n.d. kernel. This is only true for Hilbertian distances, where distance <math>d</math> is called Hilbertian if one can embed the metric space <math>(\\mathcal X,d)</math> [[isometry|isometrically]] into some Hilbert space.\n\nOn the other hand, n.d. kernels can be identified with a subfamily of p.d. kernels known as infinitely divisible kernels. A nonnegative-valued kernel <math>K</math> is said to be infinitely divisible if for every <math>n\\in\\mathbb{N}</math> there exists a positive definite kernel <math>K_n</math> such that <math>K=(K_n)^n</math>.\n\nAnother link is that a p.d. kernel induces a [[pseudometric space|pseudometric]], where the first constraint on the distance function is loosened to allow <math> d(x,y) = 0 </math> for <math> x \\neq y </math>.  Given a positive definite kernel <math> K </math>, we can define a distance function as:\n\n: <math> d(x,y) = \\sqrt{ K(x,x) - 2 K(x,y) + K(y,y) } </math>\n\n== Some applications ==\n\n=== Kernels in machine learning ===\n\n{{further|Kernel method}}\n\nPositive definite kernels, through their equivalence with reproducing kernel Hilbert spaces, are particularly important in the field of [[statistical learning theory]] because of the celebrated [[representer theorem]] which states that every minimizer function in an RKHS can be written as a linear combination of the kernel function evaluated at the training points. This is a practically useful result as it effectively simplifies the empirical risk minimization problem from an infinite dimensional to a finite dimensional optimization problem.\n\n=== Kernels in probabilistic models ===\nThere are several different ways in which kernels arise in probability theory.\n\n* Nondeterministic recovery problems: Assume that we want to find the response <math>f(x)</math> of an unknown model function <math>f</math> at a new point <math>x</math> of a set <math>\\mathcal X</math>, provided that we have a sample of input-response pairs <math>(x_i,f_i)=(x_i,f(x_i))</math> given by observation or experiment. The response <math>f_i</math> at <math>x_i</math> is not a fixed function of <math>x_i</math> but rather a realization of a real-valued random variable <math>Z(x_i)</math>. The goal is to get information about the function <math>E[Z(x_i)]</math> which replaces <math>f</math> in the deterministic setting. For two elements <math>x,y\\in\\mathcal X</math> the random variables <math>Z(x)</math> and <math>Z(y)</math> will not be uncorrelated, because if <math>x</math> is too close to <math>y</math> the random experiments described by <math>Z(x)</math> and <math>Z(y)</math> will often show similar behaviour. This is described by a covariance kernel <math> K(x,y)=E[Z(x)\\cdot Z(y)] </math>. Such a kernel exists and is positive definite under weak additional assumptions. Now a good estimate for <math>Z(x)</math> can be obtained by using kernel interpolation with the covariance kernel, ignoring the probabilistic background completely.\n\nAssume now that a noise variable <math>\\epsilon(x)</math>, with zero mean and variance <math>\\sigma^2</math>, is added to <math>x</math>, such that the noise is independent for different <math>x</math> and independent of <math>Z</math> there, then the problem of finding a good estimate for <math>f</math> is identical to the above one, but with a modified kernel given by <math>K(x,y)=E[Z(x)\\cdot Z(y)] + \\sigma^2\\delta_{xy}</math>.\n* Density estimation by kernels: The problem is to recover the density <math>f</math> of a multivariate distribution over a domain <math>\\mathcal X</math>, from a large sample <math>x_1,\\dots,x_n\\in\\mathcal X</math> including repetitions. Where sampling points lie dense, the true density function must take large values. A simple density estimate is possible by counting the number of samples in each cell of a grid, and plotting the resulting histogram, which yields a piecewise constant density estimate. A better estimate can be obtained by using a nonnegative translation invariant kernel <math>K</math>, with total integral equal to one, and define\n::<math>f(x)=\\frac1n \\sum_{i=1}^n K\\left(\\frac{x-x_i}h\\right)</math>\nas a smooth estimate.\n\n=== Numerical solution of partial differential equations ===\n\n{{further|Meshfree methods}}\n\nOne of the greatest application areas of so-called [[meshfree methods]] is in the numerical solution of [[Partial differential equation|PDEs]]. Some of the popular meshfree methods are closely related to positive definite kernels (such as [[Meshless local Petrov Galerkin|meshless local Petrov Galerkin (MLPG)]], [[Reproducing kernel particle method|Reproducing kernel particle method (RKPM)]] and [[Smoothed-particle hydrodynamics|smoothed-particle hydrodynamics (SPH)]]). These methods use radial basis kernel for [[Collocation method|collocation]]<ref>Schabak, R. and Wendland, H. (2006). \"Kernel Techniques: From Machine Learning to Meshless Methods\", Cambridge University Press, Acta Numerica (2006), pp. 1-97.</ref>\n\n=== Stinespring dilation theorem ===\n\n{{further|Stinespring dilation theorem}}\n\n=== Other applications ===\nIn the literature on computer experiments <ref>Haaland, B. and Qian, P. Z. G. (2010). \"Accurate emulators for large-scale computer experiments\", Ann. Stat.</ref> and other engineering experiments one increasingly encounters models based on p.d. kernels, RBFs or [[kriging]]. One such topic is [[Optimus platform#Response Surface Modeling|response surface modeling]]. Other types of applications that boil down to data fitting are [[rapid prototyping]] and [[Computer graphics (computer science)|computer graphics]]. Here one often uses implicit surface models to approximate or interpolate point cloud data.\n\nApplications of p.d. kernels in various other branches of mathematics are in multivariate integration, multivariate optimization, and in numerical analysis and scientific computing, where one studies fast, accurate and adaptive algorithms ideally implemented in high-performance computing environments.<ref>Gumerov, N. A. and Duraiswami, R. (2007). \"[https://www.academia.edu/download/30742577/Gumerov_Duraiswami_SISC_29_1876_1899_2007.pdf Fast radial basis function interpolation via preconditioned Krylov iteration]\". SIAM J. Scient. Computing 29/5, pp. 1876-1899.</ref>\n\n== See also ==\n*[[Integral equation]]\n*[[Integral transform]]\n*[[Positive definite function on a group]]\n*[[Reproducing kernel Hilbert space]]\n*[[Kernel method]]\n\n== References ==\n{{Reflist|30em}}\n\n{{DEFAULTSORT:Positive Definite Kernel}}\n[[Category:Operator theory]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Projective Hilbert space",
      "url": "https://en.wikipedia.org/wiki/Projective_Hilbert_space",
      "text": "In [[mathematics]] and the foundations of [[quantum mechanics]], the '''projective Hilbert space''' <math>P(H)</math> of a complex [[Hilbert space]] <math>H</math> is the set of [[equivalence class]]es of vectors <math>v</math> in <math>H</math>, with <math>v \\ne 0</math>, for the relation <math>\\sim</math> given by\n\n:<math>v \\sim w</math> when <math>v = \\lambda w</math> for some non-zero complex number <math>\\lambda</math>.\n\nThe equivalence classes for the relation <math>\\sim</math> are also called '''rays''' or '''projective rays'''.\n\nThis is the usual construction of [[projectivization]], applied to a [[complex number|complex]] Hilbert space.\n\n==Overview==\nThe physical significance of the projective Hilbert space is that in [[Quantum mechanics|quantum theory]], the [[wave function]]s <math>\\psi</math> and <math>\\lambda \\psi</math> represent the same ''physical state'', for any <math>\\lambda \\ne 0</math>. It is conventional to choose a <math>\\psi</math> from the ray so that it has unit [[normed vector space|norm]], <math>\\langle\\psi|\\psi\\rangle = 1</math>, in which case it is called a [[normalized wavefunction]]. The unit norm constraint does not completely determine <math>\\psi</math> within the ray, since <math>\\psi</math> could be multiplied by any <math>\\lambda</math> with [[absolute value]] 1 (the [[U(1)]] action) and retain its normalization. Such a <math>\\lambda</math> can be written as <math>\\lambda = e^{i\\phi}</math> with <math>\\phi</math> called the global [[Phase factor|phase]].\n\nRays that differ by such a <math>\\lambda</math> correspond to the same state (cf. [[Quantum state#Mathematical generalizations|quantum state (algebraic definition)]], given a [[C*-algebra]] of observables and a representation on <math>H</math>). No measurement can recover the phase of a ray, it is not observable. One says that <math>U(1)</math> is a gauge group of the first kind.\n\nIf <math>H</math> is an irreducible representation of the algebra of observables then the rays induce pure states. Convex linear combinations of rays naturally give rise to density matrix which (still in case of an irreducible representation) correspond to mixed states.\n\nThe same construction can be applied also to real Hilbert spaces.\n\nIn the case <math>H</math> is finite-dimensional, that is, <math>H=H_n</math>, the set of projective rays may be treated just as any other projective space; it is a [[homogeneous space]] for a [[unitary group]] <math>\\mathrm{U}(n)</math> or [[orthogonal group]] <math>\\mathrm{O}(n)</math>, in the complex and real cases respectively. For the finite-dimensional complex Hilbert space, one writes\n\n:<math>P(H_{n})=\\mathbb{C}P^{n-1}</math>\n\nso that, for example, the projectivization of two-dimensional complex Hilbert space (the space describing one [[qubit]]) is the [[complex projective line]] <math>\\mathbb{C}P^{1}</math>. This is known as the [[Bloch sphere]]. See [[Hopf fibration]] for details of the projectivization construction in this case.\n\nComplex projective Hilbert space may be given a natural metric, the [[Fubini–Study metric]], derived from the Hilbert space's norm.\n\n==Product==\nThe [[Cartesian product]]s of projective Hilbert spaces is not a projective space. [[Segre mapping]] is an embedding of the Cartesian product of two projective spaces into their tensor product. In quantum theory, it describes how to make states of the composite system from states of its constituents. It is only an [[embedding]] not a surjection; most of the tensor product space does not lie in its [[range of a function|range]] and represents ''[[quantum entanglement|entangled]] states''.\n\n==See also==\n* [[Projective space]], for the concept in general\n* [[Complex projective space]]\n* [[Projective representation]]\n\n==References==\n\n{{cite arXiv|eprint=gr-qc/9706069|last1=Ashtekar|first1=Abhay|title=Geometrical Formulation of Quantum Mechanics|last2=Schilling|first2=Troy A|year=1997}}\n\n{{DEFAULTSORT:Projective Hilbert Space}}\n[[Category:Hilbert space]]"
    },
    {
      "title": "Representer theorem",
      "url": "https://en.wikipedia.org/wiki/Representer_theorem",
      "text": "{{context|date=June 2012}}\n\nIn [[Computational learning theory|statistical learning theory]], a '''representer theorem''' is any of several related results stating that a minimizer <math>f^{*}</math> of a regularized [[Empirical risk minimization|empirical risk functional]] defined over a [[reproducing kernel Hilbert space]] can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.\n\n==Formal statement==\nThe following Representer Theorem and its proof are due to [[Bernhard Schölkopf|Schölkopf]], Herbrich, and Smola:\n\n'''Theorem:''' Consider a positive-definite real-valued kernel <math>k : \\mathcal{X} \\times \\mathcal{X} \\to \\R</math> on a non-empty set <math>\\mathcal{X}</math> with a corresponding reproducing kernel Hilbert space <math>H_k</math>.  Let there be given\n* a training sample <math>(x_1, y_1), \\dotsc, (x_n, y_n) \\in \\mathcal{X} \\times \\R</math>,\n* a strictly monotonically increasing real-valued function <math>g \\colon [0, \\infty) \\to \\R</math>, and\n* an arbitrary error function <math>E \\colon (\\mathcal{X} \\times \\R^2)^n \\to \\R \\cup \\lbrace \\infty \\rbrace</math>, \nwhich together define the following regularized empirical risk functional on <math>H_k</math>:\n\n:<math>\n f \\mapsto E\\left( (x_1, y_1, f(x_1)), ..., (x_n, y_n, f(x_n)) \\right) + g\\left( \\lVert f \\rVert \\right).\n</math>\n\nThen, any minimizer of the empirical risk\n\n:<math>\n f^{*} = \\operatorname{arg min}_{f \\in H_k} \\left\\lbrace E\\left( (x_1, y_1, f(x_1)), ..., (x_n, y_n, f(x_n)) \\right) + g\\left( \\lVert f \\rVert \\right) \\right \\rbrace, \\quad (*)\n</math>\n\nadmits a representation of the form:\n\n:<math>\n f^{*}(\\cdot) = \\sum_{i = 1}^n \\alpha_i k(\\cdot, x_i),\n</math>\n\nwhere <math>\\alpha_i \\in \\R</math> for all <math>1 \\le i \\le n</math>.\n\n'''Proof:'''\nDefine a mapping\n\n:<math>\n\\begin{align}\n \\varphi \\colon \\mathcal{X} &\\to \\R^{\\mathcal{X} }\\\\\n\\varphi(x) &= k(\\cdot, x)\n\\end{align}\n</math>\n\n(so that <math>\\varphi(x) = k(\\cdot, x)</math> is itself a map <math>\\mathcal{X} \\to \\R</math>).  Since <math>k</math> is a reproducing kernel, then\n\n:<math>\n \\varphi(x)(x') = k(x', x) = \\langle \\varphi(x'), \\varphi(x) \\rangle,\n</math>\nwhere <math>\\langle \\cdot, \\cdot \\rangle</math> is the inner product on <math>H_k</math>.\n\nGiven any <math>x_1, ..., x_n</math>, one can use orthogonal projection to decompose any <math>f \\in H_k</math> into a sum of two functions, one lying in <math>\\operatorname{span} \\left \\lbrace \\varphi(x_1), ..., \\varphi(x_n) \\right \\rbrace</math>, and the other lying in the orthogonal complement:\n\n:<math>\n f = \\sum_{i = 1}^n \\alpha_i \\varphi(x_i) + v,\n</math>\nwhere <math>\\langle v, \\varphi(x_i) \\rangle = 0</math> for all <math>i</math>.\n\nThe above orthogonal decomposition and the [[Reproducing kernel Hilbert space#The Reproducing Property|reproducing property]] together show that applying <math>f</math> to any training point <math>x_j</math> produces\n\n:<math>\n f(x_j) = \\left \\langle \\sum_{i = 1}^n \\alpha_i \\varphi(x_i) + v, \\varphi(x_j) \\right \\rangle = \\sum_{i = 1}^n \\alpha_i \\langle \\varphi(x_i), \\varphi(x_j) \\rangle,\n</math>\n\nwhich we observe is independent of <math>v</math>.  Consequently, the value of the error function <math>E</math> in (*) is likewise independent of <math>v</math>.  For the second term (the regularization term), since <math>v</math> is orthogonal to <math>\\sum_{i = 1}^n \\alpha_i \\varphi(x_i)</math> and <math>g</math> is strictly monotonic, we have\n\n:<math>\n\\begin{align}\n g\\left( \\lVert f \\rVert \\right) &= g \\left(  \\lVert \\sum_{i = 1}^n \\alpha_i \\varphi(x_i) + v \\rVert \\right) \\\\\n&= g \\left( \\sqrt{  \\lVert \\sum_{i = 1}^n \\alpha_i \\varphi(x_i)  \\rVert^2 + \\lVert v \\rVert^2} \\right) \\\\\n&\\ge g \\left(  \\lVert \\sum_{i = 1}^n \\alpha_i \\varphi(x_i) \\rVert \\right).\n\\end{align}\n</math>\n\nTherefore setting <math>v = 0</math> does not affect the first term of (*), while it strictly decreasing the second term.  Consequently, any minimizer <math>f^{*}</math> in (*) must have <math>v = 0</math>, i.e., it must be of the form\n\n:<math>\n f^{*}(\\cdot) = \\sum_{i = 1}^n \\alpha_i \\varphi(x_i) = \\sum_{i = 1}^n \\alpha_i k(\\cdot, x_i),\n</math>\n\nwhich is the desired result.\n\n==Generalizations==\nThe Theorem stated above is a particular example of a family of results that are collectively referred to as \"representer theorems\"; here we describe several such.\n\nThe first statement of a representer theorem was due to Kimeldorf and Wahba for the special case in which\n\n:<math>\n\\begin{align}\nE\\left( (x_1, y_1, f(x_1)), ...,  (x_n, y_n, f(x_n)) \\right) &= \\frac{1}{n} \\sum_{i = 1}^n (f(x_i) - y_i)^2, \\\\\ng(\\lVert f \\rVert) &= \\lambda \\lVert f \\rVert^2\n\\end{align}\n</math>\n\nfor <math>\\lambda > 0</math>.  Schölkopf, Herbrich, and Smola generalized this result by relaxing the assumption of the squared-loss cost and allowing the regularizer to be any strictly monotonically increasing function <math>g(\\cdot)</math> of the Hilbert space norm.\n\nIt is possible to generalize further by augmenting the regularized empirical risk functional through the addition of unpenalized offset terms.  For example, Schölkopf, Herbrich, and Smola also consider the minimization\n\n:<math>\n \\tilde{f}^{*} = \\operatorname{arg min} \\left\\lbrace E\\left( (x_1, y_1, \\tilde{f}(x_1)),  ...,  (x_n, y_n, \\tilde{f}(x_n)) \\right) + g\\left( \\lVert f \\rVert \\right) \\mid \\tilde{f} = f  + h \\in H_k \\oplus  \\operatorname{span} \\lbrace \\psi_p \\mid 1 \\le p \\le M \\rbrace  \\right \\rbrace, \\quad (\\dagger)\n</math>\n\ni.e., we consider functions of the form <math>\\tilde{f} = f + h</math>, where <math>f \\in H_k</math> and <math>h</math> is an unpenalized function lying in the span of a finite set of real-valued functions <math>\\lbrace \\psi_p \\colon \\mathcal{X} \\to \\R \\mid 1 \\le p \\le M \\rbrace</math>.  Under the assumption that the <math>m \\times M</math> matrix <math>\\left( \\psi_p(x_i) \\right)_{ip}</math> has rank <math>M</math>, they show that the minimizer <math>\\tilde{f}^{*}</math> in <math>(\\dagger)</math>\nadmits a representation of the form\n\n:<math>\n \\tilde{f}^{*}(\\cdot) = \\sum_{i = 1}^n \\alpha_i k(\\cdot, x_i) + \\sum_{p = 1}^M \\beta_p \\psi_p(\\cdot)\n</math>\n\nwhere <math>\\alpha_i, \\beta_p \\in \\R</math> and the <math>\\beta_p</math> are all uniquely determined.\n\nThe conditions under which a representer theorem exists were investigated by Argyriou, Micchelli, and Pontil, who proved the following:\n\n'''Theorem:''' Let <math>\\mathcal{X}</math> be a nonempty set, <math>k</math> a positive-definite real-valued kernel on <math>\\mathcal{X} \\times \\mathcal{X}</math> with corresponding reproducing kernel Hilbert space <math>H_k</math>, and let <math>R \\colon H_k \\to \\R</math> be a differentiable regularization function.  Then given a training sample <math>(x_1, y_1), ..., (x_n, y_n) \\in \\mathcal{X} \\times \\R</math> and an arbitrary error function <math>E \\colon (\\mathcal{X} \\times \\R^2)^m \\to \\R \\cup \\lbrace \\infty \\rbrace</math>, a minimizer\n\n:<math>\nf^{*} =  \\operatorname{arg min}_{f \\in H_k} \\left\\lbrace E\\left( (x_1, y_1, f(x_1)), ...,  (x_n, y_n, f(x_n)) \\right) + R(f) \\right \\rbrace \\quad (\\ddagger)\n</math>\n\nof the regularized empirical risk admits a representation of the form\n\n:<math>\n f^{*}(\\cdot) = \\sum_{i = 1}^n \\alpha_i k(\\cdot, x_i),\n</math>\n\nwhere <math>\\alpha_i \\in \\R</math> for all <math>1 \\le i \\le n</math>, if and only if there exists a nondecreasing function <math>h \\colon [0, \\infty) \\to \\R</math> for which\n\n:<math>\nR(f) = h(\\lVert f \\rVert).\n</math>\n\nEffectively, this result provides a necessary and sufficient condition on a differentiable regularizer <math>R(\\cdot)</math> under which the corresponding regularized empirical risk minimization <math>(\\ddagger)</math> will have a representer theorem.  In particular, this shows that a broad class of regularized risk minimizations (much broader than those originally considered by Kimeldorf and Wahba) have representer theorems.\n\n==Applications==\nRepresenter theorems are useful from a practical standpoint because they dramatically simplify the regularized empirical risk minimization problem <math>(\\ddagger)</math>.  In most interesting applications, the search domain <math>H_k</math> for the minimization will be an infinite-dimensional subspace of <math>L^2(\\mathcal{X})</math>, and therefore the search (as written) does not admit implementation on finite-memory and finite-precision computers.  In contrast, the representation of <math>f^{*}(\\cdot)</math> afforded by a representer theorem reduces the original (infinite-dimensional) minimization problem to a search for the optimal <math>n</math>-dimensional vector of coefficients <math>\\alpha = (\\alpha_1, ..., \\alpha_n) \\in \\R^n</math>; <math>\\alpha</math> can then be obtained by applying any standard function minimization algorithm.  Consequently, representer theorems provide the theoretical basis for the reduction of the general machine learning problem to algorithms that can actually be implemented on computers in practice.\n\n{{no footnotes|date=June 2012}}\n\n==See also==\n* [[Mercer's theorem]]\n* [[Kernel methods]]\n\n==References==\n{{reflist}}\n<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\n*{{cite journal\n |first1=Andreas |last1=Argyriou\n |first2=Charles A. |last2=Micchelli\n |first3=Massimiliano |last3=Pontil\n |title=When Is There a Representer Theorem? Vector Versus Matrix Regularizers\n |journal=Journal of Machine Learning Research\n |volume=10 |issue=Dec |pages=2507–2529  |year=2009\n}}\n*{{cite journal\n |first1=Felipe |last1=Cucker\n |first2=Steve |last2=Smale\n |title=On the Mathematical Foundations of Learning\n |journal=[[Bulletin of the American Mathematical Society]]\n |volume=39 |issue=1 |pages=1&ndash;49 |year=2002\n |doi=10.1090/S0273-0979-01-00923-5\n |mr=1864085\n}}\n*{{cite journal\n |first1=George S. |last1=Kimeldorf\n |first2=Grace |last2=Wahba\n |title=A correspondence between Bayesian estimation on stochastic processes and smoothing by splines\n |journal=The Annals of Mathematical Statistics\n |volume=41 |issue=2 |pages=495&ndash;502 |year=1970\n |doi=10.1214/aoms/1177697089\n}}\n*{{cite book\n |first1=Bernhard |last1=Schölkopf\n |first2=Ralf |last2=Herbrich\n |first3=Alex J. |last3=Smola\n |title=A Generalized Representer Theorem\n |journal=Computational Learning Theory\n |volume=2111 |pages=416&ndash;426 |year=2001\n |doi=10.1007/3-540-44581-1_27\n |series=Lecture Notes in Computer Science\n |isbn=978-3-540-42343-0\n|citeseerx=10.1.1.42.8617\n }}\n\n[[Category:Computational learning theory]]\n[[Category:Theoretical computer science]]\n[[Category:Machine learning]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Reproducing kernel Hilbert space",
      "url": "https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space",
      "text": "[[File:Different Views on RKHS.png|thumb|right|Figure illustrates related but varying approaches to viewing RKHS]]\n\nIn [[functional analysis]] (a branch of [[mathematics]]), a '''reproducing kernel Hilbert space (RKHS)''' is a [[Hilbert space]] of functions in which point evaluation is a continuous linear [[Functional (mathematics)|functional]]. Roughly speaking, this means that if two functions <math>f</math> and <math>g</math> in the RKHS are close in norm, i.e., <math>\\|f-g\\|</math> is small, then <math>f</math> and <math>g</math> are also pointwise close, i.e., <math>|f(x)-g(x)|</math> is small for all <math>x</math>. The reverse need not be true.\n\nIt is not entirely straightforward to construct a Hilbert space of functions which is not an RKHS.<ref>Alpay, D., and T. M. Mills. \"A family of Hilbert spaces which are not reproducing kernel Hilbert spaces.\" J. Anal. Appl. 1.2 (2003): 107–111.</ref> Note that [[Square-integrable function|''L''<sup>''2''</sup> spaces]] are not  Hilbert spaces of functions (and hence not RKHSs), but rather Hilbert spaces of equivalence classes of functions (for example, the functions <math>f</math> and <math>g</math> defined by <math>f(x)=0</math> and <math>g(x)=1_{\\mathbb{Q}}</math> are equivalent in ''L''<sup>''2''</sup>). However, there are RKHSs in which the norm is an ''L''<sup>''2''</sup>-norm, such as the space of band-limited functions (see the example below).\n\nAn RKHS is associated with a kernel that reproduces every function in the space in the sense that for any <math>x</math> in the set on which the functions are defined, \"evaluation at <math>x</math>\" can be performed by taking an inner product with a function determined by the kernel. Such a ''reproducing kernel'' exists if and only if every evaluation functional is continuous.\n\nThe reproducing kernel was first introduced in the 1907 work of [[Stanisław Zaremba (mathematician)|Stanisław Zaremba]] concerning [[boundary value problem]]s for [[Harmonic function|harmonic]] and [[Biharmonic equation|biharmonic functions]].  [[James Mercer (mathematician)|James Mercer]] simultaneously examined [[Positive-definite kernel|functions]] which satisfy the reproducing property in the theory of [[integral equation]]s. The idea of the reproducing kernel remained untouched for nearly twenty years until it appeared in the dissertations of [[Gábor Szegő]], [[Stefan Bergman]], and [[Salomon Bochner]].  The subject was eventually systematically developed in the early 1950s by [[Nachman Aronszajn]] and Stefan Bergman.<ref>Okutmustur</ref>\n\nThese spaces have wide applications, including [[complex analysis]], [[harmonic analysis]], and [[quantum mechanics]].  Reproducing kernel Hilbert spaces are particularly important in the field of [[statistical learning theory]] because of the celebrated [[representer theorem]] which states that every function in an RKHS that minimises an empirical risk functional can be written as a [[linear combination]] of the kernel function evaluated at the training points.  This is a practically useful result as it effectively simplifies the [[empirical risk minimization]] problem from an infinite dimensional to a finite dimensional optimization problem.\n\nFor ease of understanding, we provide the framework for real-valued Hilbert spaces.  The theory can be easily extended to spaces of complex-valued functions and hence include the many important examples of reproducing kernel Hilbert spaces that are spaces of [[analytic functions]].<ref>Paulson</ref>\n\n==Definition==\nLet <math>X</math> be an arbitrary [[Set (mathematics)|set]] and <math>H</math> a [[Hilbert space]] of [[real-valued function]]s on <math>X</math>.  The [[Cartesian closed category#Evaluation|evaluation]] functional over the Hilbert space of functions <math>H</math> is a linear functional that evaluates each function at a point <math>x</math>,\n\n:<math> L_{x} : f \\mapsto f(x)  \\text{   } \\forall f \\in H. </math>\n\nWe say that ''H'' is a '''reproducing kernel Hilbert space''' if, for all <math>x</math> in <math>X</math>, <math> L_{x} </math> is [[continuous function (topology)|continuous]] at any <math>f</math> in <math>H</math> or, equivalently, if <math> L_x </math> is a [[bounded operator]] on <math>H</math>, i.e. there exists some ''M > 0'' such that\n\n{{NumBlk|:|<math> |L_{x}(f)| := |f(x)| \\le M \\|f\\|_H \\text{   } \\forall f \\in H. \\,</math>|{{EquationRef|1}}}}\n\nWhile property ({{EquationNote|1}}) is the weakest condition that ensures both the existence of an inner product and the evaluation of every function in <math>H</math> at every point in the domain, it does not lend itself to easy application in practice.  A more intuitive definition of the RKHS can be obtained by observing that this property guarantees that the evaluation functional can be represented by taking the inner product of <math> f </math> with a function <math> K_x </math> in <math>H</math> .  This function is the so-called '''reproducing kernel''' for the Hilbert space <math>H</math> from which the RKHS takes its name.  More formally, the [[Riesz representation theorem]] implies that for all <math>x</math> in <math>X</math> there exists a unique element <math> K_x </math> of <math>H</math> with the reproducing property,\n\n{{NumBlk|:|<math>  f(x) = L_{x}(f) = \\langle f,\\ K_x \\rangle \\quad \\forall f \\in H.</math>|{{EquationRef|2}}}}\n\nSince <math> K_x </math> is itself a function in <math>H</math>, it holds that for every <math>y</math> in <math>X</math> there exist a <math>K_y\\in H</math> such that\n\n:<math> K_x(y) = \\langle K_x,\\ K_y \\rangle. </math>\n\nThis allows us to define the reproducing kernel of <math>H</math> as a function <math> K: X \\times X \\to \\mathbb{R} </math> by\n\n:<math> K(x,y) = \\langle K_x,\\ K_y \\rangle. </math>\n\nFrom this definition it is easy to see that <math> K: X \\times X \\to \\mathbb{R} </math> is both symmetric and [[positive definite]], i.e.\n\n:<math> \\sum_{i,j =1}^n c_i c_j K(x_i, x_j) \\ge  0 </math>\n\nfor any <math> n \\in \\mathbb{N}, x_1, \\dots, x_n \\in X, \\text{ and } c_1, \\dots, c_n \\in \\mathbb{R}. </math><ref>Durrett</ref> The Moore–Aronszajn theorem (see below) is a sort of converse to this: if a function <math>K</math> satisfies these conditions then there is a Hilbert space of functions on <math>X</math> for which it is a reproducing kernel.\n\n==Example==\nThe space of [[Bandlimiting|bandlimited]] [[continuous function]]s <math>H</math> is a RKHS, as we now show.  Formally, fix some [[cutoff frequency]] <math> a < \\infty </math> and define the Hilbert space\n\n:<math> H = \\{ f \\in C(\\mathbb{R}) | \\operatorname{supp}(F) \\subset [-a,a] \\} </math>\n\nwhere <math>C(\\mathbb{R})</math> is the set of continuous functions, and <math> F(\\omega) = \\int f(t) e^{-i\\omega t} dt </math> is the [[Fourier transform]] of <math> f</math>.\n\nFrom the [[Fourier inversion theorem]], we have\n\n:<math> f(x) = \\frac{1}{2 \\pi} \\int_{-a}^{a} F(\\omega) e^{ix \\omega} d\\omega .</math>\n\nIt then follows by the [[Cauchy–Schwarz inequality]] and [[Parseval's theorem]] that, for all <math>x</math>,\n\n:<math> |f(x)| \\le \n\\frac{1}{2 \\pi} \\sqrt{ \\int_{-a}^{a} 2a |F(\\omega)| ^2 d\\omega} \n= \\sqrt{\\frac{a}{\\pi}} \\|f \\|.  </math>\n\nThis inequality shows that the evaluation functional is bounded, proving that <math> H </math> is indeed a RKHS.\n\nThe kernel function <math>K_x</math> in this case is given by\n\n:<math>K_x(y) = \\frac{a}{\\pi} \\operatorname{sinc}(a(y-x))=\\frac{\\sin(a(y-x))}{\\pi(y-x)}.</math>\n\nTo see this, we first note that the Fourier transform of <math>K_x(y)</math> defined above is given by\n\n:<math>\\int_{-\\infty}^{\\infty} K_x(y)e^{-i \\omega y} dy = \n\\begin{cases}\ne^{-i \\omega x} &\\text{if } \\omega \\in [-a, a], \\\\\n0 &\\text{if } \\textrm{otherwise}\n\\end{cases}\n </math>\n\nwhich is a consequence of the [[Fourier transform#Basic properties|time-shifting property of the Fourier transform]]. Consequently, using [[Plancherel's theorem]], we have\n\n:<math> \\langle f, K_x\\rangle = \\int_{-\\infty}^{\\infty} f(y) \\cdot \\overline{K_x(y)}dy \n= \\frac{1}{2\\pi} \\int_{-a}^{a} F(\\omega) \\cdot e^{i\\omega x}  d\\omega = f(x) .</math>\n\nThus we obtain the reproducing property of the kernel.\n\nNote that <math>K_x</math> in this case is the \"bandlimited version\" of the [[Dirac delta function]], and that <math>K_x(y)</math> converges to <math>\\delta(y-x)</math> in the weak sense as the cutoff frequency <math>a</math> tends to infinity.\n\n== Moore–Aronszajn theorem ==\nWe have seen how a reproducing kernel Hilbert space defines a reproducing kernel function that is both symmetric and [[positive definite kernel|positive definite]]. The Moore–Aronszajn theorem goes in the other direction; it states that every symmetric, positive definite kernel defines a unique reproducing kernel Hilbert space. The theorem first appeared in Aronszajn's ''Theory of Reproducing Kernels'', although he attributes it to [[E. H. Moore]].\n\n:'''Theorem'''. Suppose ''K'' is a symmetric, [[positive definite kernel]] on a set ''X''. Then there is a unique Hilbert space of functions on ''X'' for which ''K'' is a reproducing kernel.\n\n'''Proof'''. For all ''x'' in ''X'', define ''K<sub>x</sub>'' = ''K''(''x'', ⋅ ). Let ''H''<sub>0</sub> be the linear span of {''K<sub>x</sub>'' : ''x'' ∈ ''X''}. Define an inner product on ''H''<sub>0</sub> by\n\n:<math> \\left\\langle \\sum_{j=1}^n b_j K_{y_j}, \\sum_{i=1}^m a_i K_{x_i} \\right \\rangle = \\sum_{i=1}^m \\sum_{j=1}^n {a_i} b_j K(y_j, x_i).</math>\n\nThe symmetry of this inner product follows from the symmetry of ''K'' and the non-degeneracy follows from the fact that ''K'' is positive definite.\n\nLet ''H'' be the [[Completion (metric space)|completion]] of ''H''<sub>0</sub> with respect to this inner product. Then ''H'' consists of functions of the form\n\n:<math> f(x) = \\sum_{i=1}^\\infty a_i K_{x_i} (x) \\quad \\text{where} \\quad \\lim_{n \\to \\infty}\\sup_{p\\geq0} \\sum_{i,j=n}^{n+p} a_ia_j K(x_i,x_j) = 0.</math>\n\nNow we can check the reproducing property ({{EquationNote|2}}):\n\n:<math>\\langle f, K_x \\rangle = \\left \\langle \\sum_{i=1}^\\infty a_i K_{x_i}, K_x \\right \\rangle= \\sum_{i=1}^\\infty a_i K (x_i, x) = f(x).</math>\n\nTo prove uniqueness, let ''G'' be another Hilbert space of functions for which ''K'' is a reproducing kernel. For any ''x'' and ''y'' in ''X'', ({{EquationNote|2}}) implies that\n\n:<math>\\langle K_x, K_y \\rangle_H = K(x, y) = \\langle K_x, K_y \\rangle_G.</math>\n\nBy linearity, <math>\\langle \\cdot, \\cdot \\rangle_H = \\langle \\cdot, \\cdot \\rangle_G</math> on the span of {''K<sub>x</sub>'' : ''x'' ∈ ''X''}. Then <math>H \\subset G</math> because ''G'' is complete and contains ''H''<sub>0</sub> and hence contains its completion.\n\nNow we need to prove that every element of ''G'' is in ''H''. Let <math> f </math> be an element of ''G''. Since ''H'' is a closed subspace of ''G'', we can write <math> f=f_H + f_{H^{\\bot}} </math> where <math> f_H \\in H </math> and <math> f_{H^{\\bot}} \\in H^{\\bot} </math>. Now if <math> x \\in X </math> then, since ''K'' is a reproducing kernel of ''G'' :\n\n:<math>f(x) = \\langle K_x , f \\rangle =  \\langle K_x , f_H \\rangle = f_H(x)   </math>\n\nWhich shows that <math> f_{H^{\\bot}} = 0 </math> in ''G'' and concludes the proof.\n\n==Integral operators and Mercer's theorem==\nWe may characterize a symmetric positive definite kernel <math>K</math> via the integral operator using [[Mercer's theorem]] and obtain an additional view of the RKHS. Let <math>X</math> be a compact space equipped with a strictly positive finite [[Borel measure]] <math>\\mu</math> and <math>K: X \\times X \\to \\R</math> a continuous, symmetric, and positive definite function. Define the integral operator <math>T_K: L_2(X) \\to L_2(X)</math> as\n\n:<math> [T_K f](\\cdot) =\\int_X  K(\\cdot,t) f(t)\\, d\\mu(t) </math>\n\nwhere <math>L_2(X)</math> is the space of square integrable functions with respect to <math> \\mu </math>.\n\nMercer's theorem states that the spectral decomposition of the integral operator <math>T_K</math> of <math>K</math> yields a series representation of <math>K</math> in terms of the eigenvalues and eigenfunctions of <math> T_K </math>. This then implies that <math>K</math> is a reproducing kernel so that the corresponding RKHS can be defined in terms of these eigenvalues and eigenfunctions.  We provide the details below.\n\nUnder these assumptions <math>T_K</math> is a compact, continuous, self-adjoint, and positive operator.  The [[spectral theorem]] for self-adjoint operators implies that there is an at most countable decreasing sequence <math>(\\sigma_i)_i \\geq 0 </math> such that <math>\\lim_{i \\to \\infty}\\sigma_i = 0</math> and\n<math>T_K\\phi_i(x) = \\sigma_i\\phi_i(x)</math>, where the <math>\\{\\phi_i\\}</math> form an orthonormal basis of <math>L_2(X)</math>. By the positivity of <math>T_K, \\sigma_i > 0</math> for all <math>i.</math> One can also show that <math>T_K </math> maps continuously into the space of continuous functions <math>C(X)</math> and therefore we may choose continuous functions as the eigenvectors, that is, <math>\\phi_i \\in C(X)</math> for all <math>i.</math> Then by Mercer's theorem  <math> K </math> may be written in terms of the eigenvalues and continuous eigenfunctions as\n\n:<math> K(x,y) = \\sum_{j=1}^\\infty \\sigma_j \\, \\phi_j(x) \\, \\phi_j(y) </math>\n\nfor all <math>x, y \\in X</math> such that\n\n:<math> \\lim_{n \\to \\infty}\\sup_{u,v} \\left |K(u,v) - \\sum_{j=1}^n \\sigma_j \\, \\phi_j(u) \\, \\phi_j(v) \\right | = 0. </math>\n\nThis above series representation is referred to as a Mercer kernel or Mercer representation of <math> K </math>.\n\nFurthermore, it can be shown that the RKHS <math> H </math> of <math> K </math> is given by\n\n:<math> H = \\left \\{ f \\in L_2(X) \\left | \\sum_{i=1}^\\infty \\frac{\\left\\langle f,\\phi_i \\right \\rangle^2}{\\sigma_i} < \\infty \\right. \\right\\} </math>\n\nwhere the inner product of <math> H </math> given by\n\n:<math> \\left\\langle f,g \\right\\rangle_H = \\sum_{i=1}^\\infty \\frac{\\left\\langle f,\\phi_i \\right\\rangle_{L_2}\\left\\langle g,\\phi_i \\right\\rangle_{L_2}}{\\sigma_i}. </math>\n\nThis representation of the RKHS has application in probability and statistics, for example  to the [[Karhunen–Loève theorem|Karhunen-Loève representation]] for stochastic processes and [[kernel PCA]].\n\n==Feature maps==\nA '''feature map''' is a map <math> \\varphi: X \\rightarrow F </math>, where <math> F </math> is a Hilbert space which we will call the feature space.  The first sections presented the connection between bounded/continuous evaluation functions, positive definite functions, and integral operators and in this section we provide another representation of the RKHS in terms of feature maps.\n\nWe first note that every feature map defines a kernel via\n\n{{NumBlk|:|<math> K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle. </math> |{{EquationRef|3}}}}\n\nClearly <math> K </math> is symmetric and positive definiteness follows from the properties of inner product in <math> F </math>.  Conversely, every positive definite function and corresponding reproducing kernel Hilbert space has infinitely many associated feature maps such that ({{EquationNote|3}}) holds.\n\nFor example, we can trivially take <math> F = H </math> and <math> \\varphi(x) = K_x </math> for all <math> x \\in X </math>.  Then ({{EquationNote|3}}) is satisfied by the reproducing property. Another classical example of a feature map relates to the previous section regarding integral operators by taking <math> F = \\ell^2 </math> and <math> \\varphi(x) = (\\sqrt{\\sigma_i}\\phi_i(x))_i </math>.\n\nThis connection between kernels and feature maps provides us with a new way to understand positive definite functions and hence reproducing kernels as inner products in <math> H </math>. Moreover, every feature map can naturally define a RKHS by means of the definition of a positive definite function.\n\nLastly, feature maps allow us to construct function spaces that reveal another perspective on the RKHS.  Consider the linear space\n\n:<math> H_{\\varphi} = \\{ f: X \\to \\mathbb{R} | \\exists w \\in F, f(x) = \\langle w, \\varphi(x) \\rangle_{F}, \\forall \\text{  } x \\in X \\} . </math>\n\nWe can define a norm on <math> H_{\\varphi} </math>  by\n\n:<math> \\|f\\|_{\\varphi} = \\text{inf} \\{\\|w\\|_F : w \\in F, f(x) = \\langle w, \\varphi(x)\\rangle_F, \\forall \\text{  } x \\in X \\} .</math>\n\nIt can be shown that <math> H_{\\varphi} </math> is a RKHS with kernel defined by <math> K(x,y) = \\langle\\varphi(x), \\varphi(y)\\rangle </math>.  This representation implies that the elements of the RKHS are inner products of elements in the feature space and can accordingly be seen as hyperplanes.  This view of the RKHS is related to the [[kernel trick]] in machine learning.<ref>Rosasco</ref>\n\n==Properties==\n\nThe following properties of RKHSs may be useful to readers.\n\n* Let <math>(X_i)_{i=1}^p</math> be a sequence of sets and <math>(K_i)_{i=1}^p</math> be a collection of corresponding positive definite functions on <math> (X_i)_{i=1}^p.</math> It then follows that\n::<math>K((x_1,\\ldots ,x_p),(y_1,\\ldots,y_p)) = K_1(x_1,y_1)\\cdots K_p(x_p,y_p)</math>\n:is a kernel on <math> X = X_1 \\times \\dots \\times X_p.</math>\n\n* Let <math>X_0 \\subset X,</math> then the restriction of <math> K </math> to <math>X_0 \\times X_0 </math> is also a reproducing kernel.\n* Consider a normalized kernel <math>K</math> such that <math> K(x, x) = 1 </math> for all <math>x \\in X </math>. Define a pseudo-metric on X as\n::<math> d_K(x,y) = \\|K_x - K_y\\|_H^2 = 2(1-K(x,y)) \\qquad \\forall x \\in X </math>.\n:By the [[Cauchy–Schwarz inequality]],\n::<math> K(x,y)^2 \\le K(x, x)K(y, y) \\qquad \\forall x,y \\in X.</math>\n:This inequality allows us to view <math>K</math> as a [[Similarity measure|measure of similarity]] between inputs. If <math>x,y \\in X</math> are similar then <math>K(x,y)</math> will be closer to 1 while if <math>x,y \\in X</math> are dissimilar then <math>K(x,y)</math> will be closer to 0.\n\n*The closure of the span of <math> \\{ K_x | x \\in X \\} </math> coincides with <math> H </math>.<ref>Rosasco</ref>\n\n== Common Examples ==\n\n===Bilinear kernels===\n:<math> K(x,y) = \\langle x,y\\rangle </math>\nThe RKHS <math>H</math> corresponding to this kernel is the dual space, consisting of functions <math>f(x)=\\langle x,\\beta\\rangle</math> satisfying <math>\\|f\\|_H^2=\\|\\beta\\|^2.</math>\n\n===Polynomial kernels===\n:<math> K(x,y) = (\\alpha\\langle x,y \\rangle + 1)^d, \\qquad \\alpha \\in \\R, d \\in \\N </math>\n\n===[[Radial basis function kernel]]s=== \nThese are another common class of kernels which satisfy <math> K(x,y) = K(\\|x - y\\|).</math> Some examples include:\n\n*'''Gaussian''' or '''squared exponential kernel''':\n::<math> K(x,y) = e^{-\\frac{\\|x - y\\|^2}{2\\sigma^2}},  \\qquad \\sigma > 0 </math>\n\n* '''Laplacian Kernel''':\n::<math> K(x,y) = e^{-\\frac{\\|x - y\\|}{\\sigma}}, \\qquad \\sigma > 0 </math>\n:The squared norm of a function <math>f</math> in the RKHS <math>H</math> with this kernel is:<ref>Berlinet, Alain and Thomas, Christine. ''Reproducing kernel Hilbert spaces in Probability and Statistics'', Kluwer Academic Publishers, 2004</ref>\n::<math>\\|f\\|_H^2=\\int f(x)^2dx+\\int f'(x)^2dx</math>.\n\n===[[Bergman kernel]]s===\n\nWe also provide examples of [[Bergman kernel]]s. Let ''X'' be finite and let ''H'' consist of all complex-valued functions on ''X''.  Then an element of ''H'' can be represented as an array of complex numbers. If the usual [[inner product]] is used, then ''K<sub>x</sub>'' is the function whose value is 1 at ''x'' and 0 everywhere else, and <math>K(x,y)</math> can be thought of as an identity matrix since\n\n:<math>K(x,y)=\\begin{cases} 1 & x=y \\\\ 0 & x \\neq y \\end{cases}</math>\n\nIn this case, ''H'' is isomorphic to <math>\\Complex^n.</math>\n\nThe case of <math>X= \\mathbb{D}</math> (where <math>\\mathbb{D}</math> denotes the [[unit disc]]) is more sophisticated. Here the [[Bergman space]] [[H square|<math>H^2(\\mathbb{D})</math>]] is the space of [[square-integrable function|square-integrable]] [[holomorphic function]]s on <math>\\mathbb{D}.</math> It can be shown that the reproducing kernel for <math>H^2(\\mathbb{D})</math> is\n\n:<math>K(x,y)=\\frac{1}{\\pi}\\frac{1}{(1-x\\overline{y})^2}.</math>\n\nLastly, the space of band limited functions in <math> L^2(\\R) </math> with bandwidth <math>2a</math> are a RKHS with reproducing kernel\n\n:<math>K(x,y)=\\frac{\\sin a (x - y)}{\\pi (x-y)}.</math>\n\n== Extension to vector-valued functions==\nIn this section we extend the definition of the RKHS to spaces of vector-valued functions as this extension is particularly important in [[multi-task learning]] and [[manifold regularization]].  The main difference is that the reproducing kernel <math> \\Gamma </math> is a symmetric function that is now a positive semi-definite ''matrix'' for any <math> x,y </math> in <math> X </math>.  More formally, we define a vector-valued RKHS (vvRKHS) as a Hilbert space of functions <math> f: X \\to \\mathbb{R}^T </math> such that for all <math> c \\in \\mathbb{R}^T </math> and <math> x \\in X </math>\n\n:<math> \\Gamma_xc(y) = \\Gamma(x, y)c \\in H  \\text{ for } y \\in X </math>\n\nand\n\n:<math> \\langle f, \\Gamma_x c \\rangle_H = f(x)^\\intercal c.  </math>\n\nThis second property parallels the reproducing property for the scalar-valued case.   We note that this definition can also be connected to integral operators, bounded evaluation functions, and feature maps as we saw for the scalar-valued RKHS.  We can equivalently define the vvRKHS as a vector-valued Hilbert space with a bounded evaluation functional and show that this implies the existence of a unique reproducing kernel by the Riesz Representation theorem.  Mercer's theorem can also be extended to address the vector-valued setting and we can therefore obtain a feature map view of the vvRKHS. Lastly, it can also be shown that the closure of the span of <math> \\{ \\Gamma_xc : x \\in X, c \\in \\mathbb{R}^T \\} </math> coincides with <math> H </math>, another property similar to the scalar-valued case.\n\nWe can gain intuition for the vvRKHS by taking a component-wise perspective on these spaces.  In particular, we find that every vvRKHS is isometrically [[isomorphic]] to a scalar-valued RKHS on a particular input space. Let <math>\\Lambda = \\{1, \\dots, T \\} </math>.  Consider the space <math> X \\times \\Lambda </math> and the corresponding reproducing kernel\n\n{{NumBlk|:|<math> \\gamma: X \\times \\Lambda \\times X \\times \\Lambda \\to \\mathbb{R}. </math>|{{EquationRef|4}}}}\n\nAs noted above, the RKHS associated to this reproducing kernel is given by the closure of the span of <math>\\{ \\gamma_{(x,t)} : x \\in X, t \\in \\Lambda \\} </math> where \n<math> \\ \\gamma_{(x,t)} (y,s)  = \\gamma( (x,t), (y,s)) </math> for every set of pairs <math> (x,t), (y,s) \\in  X \\times \\Lambda </math>.\n\nThe connection to the scalar-valued RKHS can then be made by the fact that every matrix-valued kernel can be identified with a kernel of the form of ({{EquationNote|4}}) via\n\n:<math> \\Gamma(x,y)_{(t,s)} = \\gamma((x,t), (y,s)). </math>\n\nMoreover, every kernel with the form of ({{EquationNote|4}}) defines a matrix-valued kernel with the above expression.  Now letting the map <math> D: H_{\\Gamma} \\to H_{\\gamma} </math> be defined as\n\n:<math> (Df)(x,t) = \\langle f(x), e_t \\rangle_{\\mathbb{R}^T} </math>\n\nwhere <math> e_t </math> is the <math> t^{th} </math> component of the canonical basis for <math> \\mathbb{R}^T </math>, one can show that <math> D </math> is bijective and an isometry between <math> H_{\\Gamma} </math> and <math> H_{\\gamma} </math>.\n\nWhile this view of the vvRKHS can be quite useful in multi-task learning, it should be noted that this isometry does not reduce the study of the vector-valued case to that of the scalar-valued case.  In fact, this isometry procedure can make both the scalar-valued kernel and the input space too difficult to work with in practice as properties of the original kernels are often lost.<ref>De Vito</ref><ref>Zhang</ref><ref>Alvarez</ref>\n\nAn important class of matrix-valued reproducing kernels are ''separable'' kernels which can factorized as the product of a scalar valued kernel and a <math>T</math>-dimensional symmetric positive semi-definite matrix.  In light of our previous discussion these kernels are of the form\n\n:<math> \\gamma((x,t),(y,s)) =  K(x,y) K_T(t,s) </math>\n\nfor all <math>x,y </math> in <math> X </math> and <math>t,s</math> in <math> T </math>.  As the scalar-valued kernel encodes dependencies between the inputs, we can observe that the matrix-valued kernel encodes dependencies among both the inputs and the outputs.\n \nWe lastly remark that the above theory can be further extended to spaces of functions with values in function spaces but obtaining kernels for these spaces is a more difficult task.<ref>Rosasco</ref>\n\n== See also ==\n*[[Positive definite kernel]]\n*[[Mercer's theorem]]\n*[[Kernel trick]]\n*[[Kernel embedding of distributions]]\n* [[Representer theorem]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*Alvarez, Mauricio, Rosasco, Lorenzo and Lawrence, Neil, “Kernels for Vector-Valued Functions: a Review,” https://arxiv.org/abs/1106.6251, June 2011.\n* {{cite journal\n |first=Nachman |last=Aronszajn |authorlink=Nachman Aronszajn\n |title=Theory of Reproducing Kernels\n |journal=[[Transactions of the American Mathematical Society]]\n |volume=68 |issue=3 |pages=337–404 |year=1950\n |jstor=1990404\n |mr=51437\n |doi=10.1090/S0002-9947-1950-0051437-7\n}}\n* Berlinet, Alain and Thomas, Christine. ''Reproducing kernel Hilbert spaces in Probability and Statistics'', Kluwer Academic Publishers, 2004.\n* {{cite journal\n |first1=Felipe |last1=Cucker\n |first2=Steve |last2=Smale\n |title=On the Mathematical Foundations of Learning\n |journal=[[Bulletin of the American Mathematical Society]]\n |volume=39 |issue=1 |pages=1–49 |year=2002\n |doi=10.1090/S0273-0979-01-00923-5\n |mr=1864085\n}}\n*De Vito, Ernest, Umanita, Veronica, and Villa, Silvia. \"An extension of Mercer theorem to vector-valued measurable kernels,\" https://arxiv.org/pdf/1110.4017.pdf, June 2013.\n*Durrett, Greg.  9.520 Course Notes, Massachusetts Institute of Technology, http://www.mit.edu/~9.520/scribe-notes/class03_gdurett.pdf, February 2010.\n* {{cite journal\n |first=George |last=Kimeldorf \n |first2=Grace |last2=Wahba |authorlink2=Grace Wahba\n |url=http://www.stat.wisc.edu/~wahba/ftp1/oldie/kw71.pdf \n |title=Some results on Tchebycheffian Spline Functions\n |journal=Journal of Mathematical Analysis and Applications\n |volume=33 |issue=1 |year=1971 |pages=82–95 |doi=10.1016/0022-247X(71)90184-3\n |mr=290013\n}}\n*Okutmustur, Baver.   “Reproducing Kernel Hilbert Spaces,” M.S. dissertation, Bilkent University, http://www.thesis.bilkent.edu.tr/0002953.pdf, August 2005.\n*Paulsen, Vern. “An introduction to the theory of reproducing kernel Hilbert spaces,” http://www.math.uh.edu/~vern/rkhs.pdf.\n* {{cite journal\n |first=Ingo |last=Steinwart \n |first2=Clint |last2=Scovel \n |title= Mercer's theorem on general domains: On the interaction between measures, kernels, and RKHSs\n |journal=Constr. Approx.\n |volume=35 |issue=3|year=2012 |pages=363–417\n |mr=2914365 |doi=10.1007/s00365-012-9153-3\n}}\n* Rosasco, Lorenzo and Poggio, Thomas.  \"A Regularization Tour of Machine Learning – MIT 9.520 Lecture Notes\" Manuscript, Dec. 2014.\n* [[Grace Wahba|Wahba, Grace]],  ''Spline Models for Observational Data'', [http://www.siam.org/books/ SIAM], 1990.\n*{{cite journal | last1 = Zhang | first1 = Haizhang | last2 = Xu | first2 = Yuesheng | last3 = Zhang | first3 = Qinghui | year = 2012 | title = Refinement of Operator-valued Reproducing Kernels | url = | journal = Journal of Machine Learning Research | volume = 13 | issue = | pages = 91–136 }}\n\n[[Category:Hilbert space]]"
    },
    {
      "title": "Scattering theory",
      "url": "https://en.wikipedia.org/wiki/Scattering_theory",
      "text": "{{Quantum mechanics|cTopic=Advanced topics}}[[Image:Scattering theory illust.png|right|thumb|Top: the [[real part]] of a [[plane wave]] travelling upwards. Bottom: The real part of the field after inserting in the path of the plane wave a small transparent disk of [[index of refraction]] higher than the index of the surrounding medium. This object scatters part of the wave field, although at any individual point, the wave's frequency and wavelength remain intact.]]\nIn [[mathematics]] and [[physics]], '''scattering theory''' is a framework for studying and understanding the [[scattering]] of [[wave|waves]] and [[Elementary particle|particles]]. Wave scattering corresponds to the collision and scattering of a wave with some material object, for instance [[sunlight]] scattered by [[rain drop]]s to form a [[rainbow]]. Scattering also includes the interaction of [[billiard balls]] on a table, the [[Rutherford scattering]] (or angle change) of [[alpha particle]]s by [[gold]] [[atomic nucleus|nuclei]], the Bragg scattering (or diffraction) of electrons and X-rays by a cluster of atoms, and the [[inelastic scattering]] of a fission fragment as it traverses a thin foil. More precisely, scattering consists of the study of how solutions of [[partial differential equations]], propagating freely \"in the distant past\", come together and interact with one another or with a [[boundary condition]], and then propagate away \"to the distant future\".  The '''direct scattering problem''' is the problem of determining the distribution of scattered radiation/particle flux basing on the characteristics of the [[scattering|scatterer]]. The [[inverse scattering problem]] is the problem of determining the characteristics of an object (e.g., its shape, internal constitution) from measurement data of radiation or particles scattered from the object.\n\nSince its early statement for [[radar|radiolocation]], the problem has found vast number of applications, such as [[acoustic location|echolocation]], [[geophysical]] survey, [[nondestructive testing]], [[medical imaging]] and [[quantum field theory]], to name just a few.\n\n==Conceptual underpinnings==\n\nThe concepts used in scattering theory go by different names in \ndifferent fields.  The object of this section is to point the reader \nto common threads.\n\n===Composite targets and range equations===\n\n[[Image:Xsection2.png|288px|thumb|left|Equivalent quantities used in the theory of scattering from composite specimens, but with a variety of units.]]\n\nWhen the target is a set of many scattering centers whose relative position varies unpredictably, it is customary to think of a range equation whose arguments take different forms in different application areas.  In the simplest case consider an interaction that removes particles from the \"unscattered beam\" at a uniform rate that is proportional to the incident flux <math>I</math> of particles per unit area per unit time, i.e. that\n\n:<math> \\frac{dI}{dx}=-QI \\,\\!</math>\n\nwhere ''Q'' is an interaction coefficient and ''x'' is the distance traveled in the target.\n\nThe above ordinary first-order [[differential equation]] has solutions of the form:\n\n: <math>I = I_o e^{-Q \\Delta x} = I_o e^{-\\frac{\\Delta x}{\\lambda}} = I_o e^{-\\sigma (\\eta \\Delta x)} = I_o e^{-\\frac{\\rho \\Delta x}{\\tau}} ,</math>\n\nwhere ''I''<sub>o</sub> is the initial flux, path length Δx&nbsp;≡&nbsp;''x''&nbsp;&minus;&nbsp;''x''<sub>o</sub>, the second equality defines an interaction [[mean free path]] λ, the third uses the number of targets per unit volume η to define an area [[cross section (physics)|cross-section]] σ, and the last uses the target mass density ρ to define a density mean free path τ.   Hence one converts between these quantities via ''Q'' =&nbsp;1/''&lambda;'' =&nbsp;''&eta;&sigma;'' =&nbsp;''&rho;/&tau;'', as shown in the figure at left.\n\nIn electromagnetic absorption spectroscopy, for example, interaction coefficient (e.g. Q in cm<sup>&minus;1</sup>) is variously called [[opacity (optics)|opacity]], [[absorption coefficient]], and [[attenuation coefficient]].  In nuclear physics, area cross-sections (e.g. σ in [[barn (unit)|barn]]s or units of 10<sup>&minus;24</sup> cm<sup>2</sup>), density mean free path (e.g. τ in grams/cm<sup>2</sup>), and its reciprocal the [[mass attenuation coefficient]] (e.g. in cm<sup>2</sup>/gram) or ''area per nucleon'' are all popular, while in electron microscopy the [[inelastic mean free path]]<ref>R. F. Egerton (1996) ''Electron energy-loss spectroscopy in the electron microscope'' (Second Edition, Plenum Press, NY) {{ISBN|0-306-45223-5}}</ref> (e.g. λ in nanometers) is often discussed<ref>Ludwig Reimer (1997) ''Transmission electron microscopy: Physics of image formation and microanalysis'' (Fourth Edition, Springer, Berlin) {{ISBN|3-540-62568-2}}</ref> instead.\n\n==In theoretical physics==\nIn [[mathematical physics]], '''scattering theory''' is a framework for studying and understanding the interaction or [[scattering]] of solutions to [[partial differential equation]]s. In [[acoustics]], the differential equation is the [[wave equation]], and scattering studies how its solutions, the [[sound wave]]s, scatter from solid objects or propagate through non-uniform media (such as sound waves, in [[sea water]], coming from a [[submarine]]). In the case of classical [[electrodynamics]], the differential equation is again the wave equation, and the scattering of [[light]] or [[radio wave]]s is studied. In [[particle physics]], the equations are those of [[Quantum electrodynamics]], [[Quantum chromodynamics]] and the [[Standard Model]], the solutions of which correspond to  [[fundamental particle]]s.\n\nIn regular [[quantum mechanics]], which includes [[quantum chemistry]], the relevant equation is the [[Schrödinger equation]], although equivalent formulations, such as the [[Lippmann-Schwinger equation]] and the [[Faddeev equation]]s, are also largely used. The solutions of interest describe the long-term motion of free atoms, molecules, photons, electrons, and protons. The scenario is that several particles come together from an infinite distance away. These reagents then collide, optionally reacting, getting destroyed or creating new particles. The products and unused reagents then fly away to infinity again. (The atoms and molecules are effectively particles for our purposes. Also, under everyday circumstances, only photons are being created and destroyed.) The solutions reveal which directions the products are most likely to fly off to and how quickly. They also reveal the probability of various reactions, creations, and decays occurring. There are two predominant techniques of finding solutions to scattering problems: [[partial wave analysis]], and the [[Born approximation]].\n\n==Elastic and inelastic scattering==\n\nThe term \"elastic scattering\" implies that the internal states of the scattered particles do not change, and hence they emerge unchanged from the scattering process.  In inelastic scattering, by contrast, the particles' internal state is changed, which may amount to exciting some of the electrons of a scattering atom, or the complete annihilation of a scattering particle and the creation of entirely new particles.\n\nThe example of scattering in [[quantum chemistry]] is particularly instructive, as the theory is reasonably complex while still having a good foundation on which to build an intuitive understanding. When two atoms are scattered off one another, one can understand them as being the [[bound state]] solutions of some differential equation.  Thus, for example, the [[hydrogen atom]] corresponds to a solution to the [[Schrödinger equation]] with a negative inverse-power (i.e., attractive Coulombic) [[central potential]].  The scattering of two hydrogen atoms will disturb the state of each atom, resulting in one or both becoming excited, or even [[ionization|ionized]], representing an inelastic scattering process.\n\nThe term \"[[deep inelastic scattering]]\" refers to a special kind of scattering experiment in particle physics.\n\n==The mathematical framework==\nIn [[mathematics]], scattering theory deals with a more abstract formulation of the same set of concepts. For example, if a [[differential equation]] is known to have some simple, localized solutions, and the solutions are a function of a single parameter, that parameter can take the conceptual role of [[time]].  One then asks what might happen if two such solutions are set up far away from each other, in the \"distant past\", and are made  to move towards each other, interact (under the constraint of the differential equation) and then move apart in the \"future\".  The scattering matrix then pairs solutions in the \"distant past\" to those in the \"distant future\".\n\nSolutions to differential equations are often posed on [[manifold]]s. Frequently, the means to the solution requires the study of the [[Spectrum (functional analysis)|spectrum]] of an [[operator theory|operator]] on the manifold. As a result, the solutions often have a spectrum that can be identified with a [[Hilbert space]], and scattering is described by a certain map, the [[S matrix]], on Hilbert spaces.  Spaces with a [[discrete spectrum (physics)|discrete spectrum]] correspond to [[bound state]]s in quantum mechanics, while a [[continuous spectrum]] is associated with scattering states.  The study of inelastic scattering then asks how discrete and continuous spectra are mixed together.\n\nAn important, notable development is the [[inverse scattering transform]], central to the solution of many [[exactly solvable model]]s.\n\n==See also==\n{{div col}}\n*[[Backscattering]]\n*[[Brillouin scattering]]\n*[[Compton scattering]]\n*[[Diffuse sky radiation]]\n*[[Extinction (astronomy)|Extinction]]\n*[[Haag–Ruelle scattering theory]]\n*[[Linewidth]]\n*[[Mie theory]]\n*[[Molecular scattering]]\n*[[Raman scattering]]\n*[[Rayleigh scattering]]\n*[[S-matrix]]\n*[[Scattering from rough surfaces]]\n*[[Scintillation (physics)]]\n{{div col end}}\n\n==Footnotes==\n{{reflist}}\n\n==References==\n* [http://www.mif.pg.gda.pl/homepages/slawek/epic/sem.htm Lectures of the European school on theoretical methods for electron and positron induced chemistry, Prague, Feb. 2005]\n* [http://www.math.ru.nl/~koelink/edu/LM-dictaat-scattering.pdf E. Koelink, Lectures on scattering theory, Delft the Netherlands 2006]\n<!--\n{{Author = Koelink, E. | Title = Introduction to Scattering Theory | Url = http://www.pa.msu.edu/~mmoore/852scattering.pdf| Year = Spring 2008 }}\n-->\n\n==External links==\n*[http://www.opticsinfobase.org/submit/ocis/OCIS_2007.pdf Optics Classification and Indexing Scheme (OCIS)], [[Optical Society of America]], 1997\n\n{{Quantum mechanics topics|state=collapsed}}\n\n[[Category:Scattering theory| ]]\n[[Category:Scattering, absorption and radiative transfer (optics)| ]]\n[[Category:Quantum mechanics]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Self-adjoint operator",
      "url": "https://en.wikipedia.org/wiki/Self-adjoint_operator",
      "text": "In [[mathematics]], a '''self-adjoint operator''' (or '''Hermitian operator''') on a finite-dimensional [[complex vector space]] ''V'' with [[inner product]] <math>\\langle\\cdot,\\cdot\\rangle</math> is a [[linear map]] ''A'' (from ''V'' to itself) that is its own [[Adjoint of an operator|adjoint]]: <math>\\langle Av,w\\rangle=\\langle v,Aw\\rangle</math> for all vectors v and w. If ''V'' is finite-dimensional with a given [[orthonormal basis]], this is equivalent to the condition that the [[matrix (mathematics)|matrix]] of ''A'' is a [[Hermitian matrix]], i.e., equal to its [[conjugate transpose]] ''A''{{sup|∗}}. By the finite-dimensional [[spectral theorem]], ''V'' has an [[orthonormal basis]] such that the matrix of ''A'' relative to this basis is a [[diagonal matrix]] with entries in the [[real number]]s. In this article, we consider [[generalization]]s of this [[concept]] to operators on [[Hilbert space]]s of arbitrary dimension.\n\nSelf-adjoint operators are used in [[functional analysis]] and [[quantum mechanics]].  In quantum mechanics their importance lies in the [[Dirac–von Neumann axioms|Dirac–von Neumann formulation]] of quantum mechanics, in which physical [[observable]]s such as position, [[momentum]], [[angular momentum]] and [[spin (physics)|spin]] are represented by self-adjoint operators on a Hilbert space. Of particular significance is the [[Hamiltonian (quantum mechanics)|Hamiltonian]] operator <math>\\hat{H}</math> defined by\n:<math>\\hat{H} \\psi = -\\frac{\\hbar^2}{2m} \\nabla^2 \\psi + V \\psi,</math>\n\nwhich as an observable corresponds to the total energy of a particle of mass ''m'' in a real potential field ''V''. Differential operators are an important class of [[unbounded operator]]s.\n\nThe structure of self-adjoint operators on infinite-dimensional Hilbert spaces essentially resembles the finite-dimensional case. That is to say, operators are self-adjoint if and only if they are unitarily equivalent to real-valued multiplication operators. With suitable modifications, this result can be extended to possibly unbounded operators on infinite-dimensional spaces. Since an everywhere-defined self-adjoint operator is necessarily bounded, one needs be more attentive to the domain issue in the unbounded case. This is explained below in more detail.\n\n==Bounded self-adjoint operators==\nSuppose <math>A</math> is a [[bounded operator|bounded]] linear operator from a Hilbert space ''H'' to itself. Then there is a unique bounded operator <math>A^*</math>, called the '''adjoint''' of <math>A</math> such that (in [[bracket notation]])\n:<math>\\langle Ax|y\\rangle = \\left\\langle x|A^*y\\right\\rangle</math>\n\nfor all <math>x,y</math> in ''H''.<ref>{{harvnb|Hall|2013}} Proposition A.53</ref> We say that ''A'' is '''self-adjoint''' (physicists use the term \"Hermitian\") if <math>A^* = A</math>. Equivalently, a bounded operator ''A'' is self-adjoint if \n:<math>\\langle Ax|y\\rangle = \\langle x|Ay\\rangle</math>\n\nfor all ''x'' and ''y'' in ''H''.\n\n==Symmetric operators ==\n{{see also|Extensions of symmetric operators}}\n\n===Subtleties of the unbounded case===\nIn many applications, we are led to consider operators that are unbounded; examples include the position, momentum, and Hamiltonian operators in quantum mechanics, as well as many differential operators. In the unbounded case, there are a number of subtle technical issues that have to be dealt with. In particular, there is a crucial distinction between operators that are merely \"symmetric\" (defined in this section) and those that are \"self-adjoint\" (defined in the next section). In the case of differential operators defined on bounded domains, these technical issues have to do with making an appropriate choice of boundary conditions.\n\n===Definition of a symmetric operator===\n\nWe now consider an [[unbounded operator]] ''A'' on a Hilbert space ''H''. This means ''A'' is a linear map from a subspace of ''H''—the \"domain\" of ''A'', denoted <math>\\operatorname{Dom}(A)</math>—to ''H'' itself. We typically assume that <math>\\operatorname{Dom}(A)</math> is a dense subspace of ''H''. Such an operator is called '''symmetric''' if, in [[bracket notation]],\n\n: <math> \\langle Ax | y \\rangle =  \\lang x | Ay \\rangle </math>\n\nfor all elements ''x'' and ''y'' in the domain of ''A''.\n\nIf ''A'' is symmetric and <math>\\mathrm{Dom}(A)=H</math>, then ''A'' is necessarily bounded.<ref>{{harvnb|Hall|2013}} Corollary 9.9</ref> That is to say, an unbounded symmetric operator cannot be defined on the whole Hilbert space. Since the operators considered in quantum mechanics are unbounded, it is impossible to define them as symmetric operators on the whole Hilbert space.\n\n'''In the physics''' literature, the term '''[[Hermitian matrix|Hermitian]]''' is used in place of the term symmetric. It should be noted, however, that the physics literature generally glosses over the distinction between operators that are merely symmetric and operators that are actually self-adjoint (as defined in the next section).\n\nAlthough the notion of a symmetric operator is easy to understand, it is not the \"right\" notion in the general unbounded case. Specifically, the [[spectral theorem]] applies only to operators that are self-adjoint (defined in the next section) and not to most operators that are merely symmetric. In particular, although the eigenvalues of a symmetric operator are necessarily real, a symmetric operator need not have any eigenvectors, let alone an orthonormal basis of them.\n\nMore generally, a partially defined linear operator ''A'' from a [[topological vector space]] ''E'' into its [[continuous dual space]] ''E''<sup>∗</sup> is said to be '''symmetric''' if\n:<math> \\langle Ax | y \\rangle =  \\lang x | Ay \\rang </math>\n\nfor all elements ''x'' and ''y'' in the domain of ''A''.  This usage is fairly standard in the functional analysis literature.\n\n===A simple example===\n\nAs noted above, the [[spectral theorem]] applies only to self-adjoint operators, and not in general to symmetric operators. Nevertheless, we can at this point give a simple example of a symmetric operator that has an orthonormal basis of eigenvectors. (This operator is actually \"essentially self-adjoint.\") The operator ''A'' below can be seen to have a [[compact operator on Hilbert space|compact]] inverse, meaning that the corresponding differential equation ''Af'' = ''g'' is solved by some integral, therefore compact, operator ''G''. The compact symmetric operator ''G'' then has a countable family of eigenvectors which are complete in {{math|''L''<sup>2</sup>}}. The same can then be said for ''A''.\n\nConsider the complex Hilbert space L<sup>2</sup>[0,1] and the [[differential operator]]\n: <math>A = -\\frac{d^2}{dx^2}</math>\n\nwith <math>\\mathrm{Dom}(A)</math> consisting of all complex-valued infinitely [[differentiable function|differentiable]] functions ''f'' on [0, 1]  satisfying the boundary conditions \n:<math>f(0) = f(1) = 0.</math> \n\nThen [[integration by parts]] of the inner product shows that ''A'' is symmetric. The reader is invited to perform integration by parts twice and verify that the given boundary conditions for <math>\\operatorname{Dom}(A)</math> ensure that the boundary terms in the integration by parts vanish.\n\nThe eigenfunctions of ''A'' are the sinusoids\n: <math>f_n(x) =  \\sin(n \\pi x) \\qquad  n= 1, 2, \\ldots</math>\n\nwith the real eigenvalues ''n''<sup>2</sup>π<sup>2</sup>; the well-known orthogonality of the sine functions follows as a consequence of the property of being symmetric.\n\nWe consider generalizations of this operator below.\n\n== Self-adjoint operators ==\n===Definition of a self-adjoint operator===\nBriefly, a densely defined linear operator ''A'' on a Hilbert space is '''self-adjoint''' if it equals its adjoint. That is to say, ''A'' is self-adjoint if (1) the domain of ''A'' coincides with the domain of the adjoint, and (2) the operator ''A'' agrees with its adjoint on this common domain.\n\nWe now elaborate on the above definition. Given a densely defined linear operator ''A'' on ''H'', its adjoint ''A''{{sup|∗}} is defined as follows:\n\n* The domain of ''A''{{sup|∗}} consists of vectors ''x'' in ''H'' such that\n*:<math> y \\mapsto \\langle x | A y \\rangle </math>\n: (which is a densely defined ''linear'' map) is a continuous linear functional.  By continuity and density of the domain of ''A'',  it extends to a unique continuous linear functional on all of ''H''.\n* By the [[Riesz representation theorem]] for linear functionals, if ''x'' is in the domain of ''A''{{sup|∗}}, there is a unique vector ''z'' in ''H'' such that\n*:<math id=\"capitalised Dom for consistency with previous section\">\\langle x  | A y \\rangle  = \\langle z |  y \\rangle \\qquad \\forall y \\in \\operatorname{Dom} A</math>\n:This  vector ''z'' is defined to be ''A''{{sup|∗}} ''x''.  It can be shown that the dependence of ''z'' on ''x'' is linear.\n\nNotice that it is the denseness of the domain of the operator, along with the uniqueness part of Riesz representation, that ensures the adjoint operator is well defined.\n\nA result of Hellinger-Toeplitz type says that an operator having an everywhere-defined bounded adjoint is bounded.\n\nThe condition for a linear operator on a Hilbert space to be ''self-adjoint'' is stronger than to be ''symmetric''. Although this distinction is technical, it is very important; the spectral theorem applies only to operators that are self-adjoint and not to operators that are merely symmetric. For an extensive discussion of the distinction, see Chapter 9 of Hall (2013).\n\nFor any densely defined operator ''A'' on Hilbert space one can define its adjoint operator ''A''{{sup|∗}}. For a symmetric operator ''A'', the domain of the operator ''A''{{sup|∗}} contains the domain of the operator ''A'', and the restriction of the operator ''A''{{sup|∗}} on the domain of ''A'' coincides with the operator ''A'', i.e. {{nowrap|1=''A'' ⊆ ''A''{{sup|∗}}}}, in other words ''A''{{sup|∗}} is extension of ''A''. For a self-adjoint operator ''A'' the domain of ''A''{{sup|∗}} is the same as the domain of ''A'',  and {{nowrap|1=''A'' = ''A''{{sup|∗}}}}. See also [[Extensions of symmetric operators]] and [[unbounded operator]].\n\n===Essential self-adjointness===\nA symmetric operator ''A'' is always closable; that is, the closure of the graph of ''A'' is the graph of an operator. A symmetric operator ''A'' is said to be '''essentially self-adjoint''' if the closure of ''A'' is self-adjoint. Equivalently, ''A'' is essentially self-adjoint if it has a ''unique'' self-adjoint extension. In practical terms, having an essentially self-adjoint operator is almost as good as having a self-adjoint operator, since we merely need to take the closure to obtain self-adjoint operator.\n\n===Geometric interpretation===\nThere is a useful [[geometric]] way of looking at the adjoint of an operator ''A'' on ''H'' as follows: we consider the graph G(''A'') of ''A'' defined by\n:<math>\\operatorname{G}(A) = \\{(\\xi, A \\xi): \\xi \\in \\operatorname{Dom}A\\} \\subseteq H \\oplus H .</math>\n\n:'''Theorem'''.  Let J be the [[symplectic matrix|symplectic mapping]]\n:: <math> \\begin{cases} H \\oplus H \\to H \\oplus H \\\\ \\operatorname{J}: (\\xi, \\eta) \\mapsto (-\\eta, \\xi) \\end{cases}</math>\n\n:Then the graph of ''A''{{sup|∗}} is the [[orthogonal complement]] of JG(''A''):\n::<math>\\operatorname{G}(A^*) = (\\operatorname{J}\\operatorname{G}(A))^\\perp = \\{ (x, y) \\in H \\oplus H : \\langle (x, y)|(-A\\xi, \\xi) \\rangle = 0\\;\\;\\forall \\xi \\in \\operatorname{Dom}A\\}</math>\n\nA densely defined operator ''A'' is symmetric [[if and only if]] {{nowrap|''A'' ⊆ ''A''{{sup|∗}}}}, where the subset notation {{nowrap|''A'' ⊆ ''A''{{sup|∗}}}} is understood to mean {{nowrap|G(''A'') ⊆ G(''A''{{sup|∗}})}}. An operator ''A'' is '''self-adjoint''' if and only if {{nowrap|1=''A'' = ''A''{{sup|∗}}}}; that is, if and only if {{nowrap|1= G(''A'') = G(''A''{{sup|∗}})}}.\n\n===An example===\nConsider the complex Hilbert space ''L''<sup>2</sup>('''R'''), and the operator which multiplies a given function by ''x'':\n:<math>A f(x) = xf(x)</math>\n\nThe domain of ''A'' is  the space of all ''L''<sup>2</sup> functions <math>f(x)</math> for which <math>xf(x)</math> is also square-integrable. Then ''A'' is self-adjoint.<ref>{{harvnb|Hall|2013}} Proposition 9.30</ref> On the other hand, ''A'' does not have any eigenfunctions. (More precisely, ''A'' does not have any ''normalizable'' eigenvectors, that is, eigenvectors that are actually in the Hilbert space on which ''A'' is defined.)\n\nAs we will see later, self-adjoint operators have very important spectral properties; they are in fact multiplication operators on general measure spaces.\n\n==The distinction between symmetric and self-adjoint operators==\nAs has been discussed above, although the distinction between a symmetric operator and a self-adjoint (or essentially self-adjoint) operator is a subtle one, it is important since self-adjointness is the hypothesis in the spectral theorem. Here we discuss some concrete examples of the distinction; see the section below on extensions of symmetric operators for the general theory.\n\n===Boundary conditions===\nIn the case where the Hilbert space is a space of functions on a bounded domain, these distinctions have to do with a familiar issue in quantum physics: One cannot define an operator—such as the momentum or Hamiltonian operator—on a bounded domain without specifying ''boundary conditions''. In mathematical terms, choosing the boundary conditions amounts to choosing an appropriate domain for the operator. Consider, for example, the Hilbert space <math>L^2([0, 1])</math> (the space of square-integrable functions on the interval [0,1]). Let us define a \"momentum\" operator ''A'' on this space by the usual formula, setting Planck's constant equal to 1:\n: <math>Af = -i\\frac{df}{dx}</math>.\n\nWe must now specify a domain for ''A'', which amounts to choosing boundary conditions. If we choose\n: <math>\\operatorname{Dom}(A) = \\left\\{\\text{smooth functions}\\right\\}</math>,\n\nthen ''A'' is not symmetric (because the boundary terms in the integration by parts do not vanish).\n\nIf we choose \n: <math>\\operatorname{Dom}(A) = \\left\\{\\text{smooth functions}\\,f|f(0) = f(1) = 0\\right\\}</math>,\n\nthen using integration by parts, one can easily verify that ''A'' is symmetric. This operator is not essentially self-adjoint,<ref>{{harvnb|Hall|2013}} Proposition 9.27</ref> however, basically because we have specified too many boundary conditions on the domain of ''A'', which makes the domain of the adjoint too big. (This example is discussed also in the \"Examples\" section below.)\n\nSpecifically, with the above choice of domain for ''A'', the domain of the closure <math>A^{\\mathrm{cl}}</math> of ''A'' is\n\n:<math>\\operatorname{Dom}\\left(A^{\\mathrm{cl}}\\right) = \\left\\{\\text{functions } f \\text{ with two derivatives in }L^2|f(0) = f(1) = 0\\right\\}</math>,\n\nwhereas the domain of the adjoint <math>A^*</math> of ''A'' is\n:<math>\\operatorname{Dom}\\left(A^*\\right) = \\left\\{\\text{functions } f \\text{ with two derivatives in }L^2\\right\\}</math>.\n\nThat is to say, the domain of the closure has the same boundary conditions as the domain of ''A'' itself, just a less stringent smoothness assumption. Meanwhile, since there are \"too many\" boundary conditions on ''A'', there are \"too few\" (actually, none at all in this case) for <math>A^*</math>. If we compute <math>\\langle g, Af\\rangle</math> for <math>f \\in \\operatorname{Dom}(A)</math> using integration by parts, then since <math>f</math> vanishes at both ends of the interval, no boundary conditions on <math>g</math> are needed to cancel out the boundary terms in the integration by parts. Thus, any sufficiently smooth function <math>g</math> is in the domain of <math>A^*</math>, with <math>A^*g = -i\\,dg/dx</math>.<ref>{{harvnb|Hall|2013}} Proposition 9.28</ref>\n\nSince the domain of the closure and the domain of the adjoint do not agree, ''A'' is not essentially self-adjoint. After all, a general result says that the domain of the adjoint of <math>A^\\mathrm{cl}</math> is the same as the domain of the adjoint of ''A''. Thus, in this case, the domain of the adjoint of <math>A^\\mathrm{cl}</math> is bigger than the domain of <math>A^\\mathrm{cl}</math> itself, showing that <math>A^\\mathrm{cl}</math> is not self-adjoint, which by definition means that ''A'' is not essentially self-adjoint.\n\nThe problem with the preceding example is that we imposed too many boundary conditions on the domain of ''A''. A better choice of domain would be to use periodic boundary conditions:\n:<math>\\operatorname{Dom}(A) = \\{\\text{smooth functions}\\,f|f(0) = f(1)\\}</math>.\n\nWith this domain, ''A'' is essentially self-adjoint.<ref>{{harvnb|Hall|2013}} Example 9.25</ref>\n\nIn this case, we can understand the implications of the domain issues for the spectral theorem. If we use the first choice of domain (with no boundary conditions), all functions <math>f_\\beta(x) = e^{\\beta x}</math> for <math>\\beta \\in \\mathbb C</math> are eigenvectors, with eigenvalues <math>-i \\beta</math>, and so the spectrum is the whole complex plane. If we use the second choice of domain (with Dirichlet boundary conditions), ''A'' has no eigenvectors at all. If we use the third choice of domain (with periodic boundary conditions), we can find an orthonormal basis of eigenvectors for ''A'', the functions <math>f_n(x) := e^{2\\pi inx}</math>. Thus, in this case finding a domain such that ''A'' is self-adjoint is a compromise: the domain has to be small enough so that ''A'' is symmetric, but large enough so that <math>D(A^*)=D(A)</math>.\n\n===Schrödinger operators with singular potentials===\nA more subtle example of the distinction between symmetric and (essentially) self-adjoint operators comes from [[Schrödinger equation|Schrödinger operators]] in quantum mechanics. If the potential energy is singular—particularly if the potential is unbounded below—the associated Schrödinger operator may fail to be essentially self-adjoint. In one dimension, for example, the operator\n:<math>\\hat{H} := \\frac{P^2}{2m} - X^4</math>\n\nis not essentially self-adjoint on the space of smooth, rapidly decaying functions.<ref>{{harvnb|Hall|2013}} Theorem 9.41</ref> In this case, the failure of essential self-adjointness reflects a pathology in the underlying classical system: A classical particle with a <math>-x^4</math> potential escapes to infinity in finite time. This operator does not have a ''unique'' self-adjoint, but it does admit self-adjoint extensions obtained by specifying \"boundary conditions at infinity\". (Since <math>\\hat{H}</math> is a real operator, it commutes with complex conjugation. Thus, the deficiency indices are automatically equal, which is the condition for having a self-adjoint extension. See the discussion of extensions of symmetric operators below.)\n\nIn this case, if we initially define <math>\\hat{H}</math> on the space of smooth, rapidly decaying functions, the adjoint will be \"the same\" operator (i.e., given by the same formula) but on the largest possible domain, namely\n:<math>\\operatorname{Dom}\\left(\\hat{H}^*\\right) = \\left\\{ \\text{twice differentiable functions }f \\in L^2(\\mathbb{R})\\left|\\left( -\\frac{\\hbar^2}{2m}\\frac{d^2f}{dx^2} - x^4f(x)\\right) \\in L^2(\\mathbb{R}) \\right. \\right\\}. </math>\n\nIt is then possible to show that <math>\\hat{H}^*</math> is not a symmetric operator, which certainly implies that <math>\\hat{H}</math> is not essentially self adjoint. Indeed, <math>\\hat{H}^*</math> has eigenvectors with pure imaginary eigenvalues,<ref>{{harvnb|Berezin|1991}} p. 85</ref><ref>{{harvnb|Hall|2013}} Section 9.10</ref> which is impossible for a symmetric operator. This strange occurrence is possible because of a cancellation between the two terms in <math>\\hat{H}^*</math>: There are functions <math>f</math> in the domain of <math>\\hat{H}^*</math> for which neither <math>d^2 f/dx^2</math> nor <math>x^4f(x)</math> is separately in <math>L^2(\\mathbb{R})</math>, but the combination of them occurring in <math>\\hat{H}^*</math> is in <math>L^2(\\mathbb{R})</math>. This allows for <math>\\hat{H}^*</math> to be nonsymmetric, even though both <math>d^2/dx^2</math> and <math>X^4</math> are symmetric operators. This sort of cancellation does not occur if we replace the repelling potential <math>-x^4</math> with the confining potential <math>x^4</math>.\n\nConditions for Schrödinger operators to be self-adjoint or essentially self-adjoint can be found in various textbooks, such as those by Berezin and Schubin, Hall, and Reed and Simon listed in the references.\n\n== Spectral theorem ==\n{{Main|Spectral theorem}}\n\nIn the physics literature, the spectral theorem is often stated by saying that a self-adjoint operator has an orthonormal basis of eigenvectors. Physicists are well aware, however, of the phenomenon of \"continuous spectrum\"; thus, when they speak of an \"orthonormal basis\" they mean either an orthonormal basis in the classic sense ''or'' some continuous analog thereof. In the case of the momentum operator <math>P = -i\\,d/dx</math>, for example, physicists would say that the eigenvectors are the functions <math>f_p(x) := e^{ipx}</math>, which are clearly not in the Hilbert space <math>L^2(\\mathbb{R})</math>. (Physicists would say that the eigenvectors are \"non-normalizable.\") Physicists would then go on to say that these \"eigenvectors\" are orthonormal in a continuous sense, where the usual Kronecker delta <math>\\delta_{i,j}</math> is replaced by a Dirac delta function <math>\\delta\\left(p - p'\\right)</math>.\n\nAlthough these statements may seem disconcerting to mathematicians, they can be made rigorous by use of the Fourier transform, which allows a general <math>L^2</math> function to be expressed as a \"superposition\" (i.e., integral) of the functions <math>e^{ipx}</math>, even though these functions are not in <math>L^2</math>. The Fourier transform \"diagonalizes\" the momentum operator; that is, it converts it into the operator of multiplication by <math>p</math>, where <math>p</math> is the variable of the Fourier transform.\n\nThe spectral theorem in general can be expressed similarly as the possibility of \"diagonalizing\" an operator by showing it is unitarily equivalent to a multiplication operator. Other versions of the spectral theorem are similarly intended to capture the idea that a self-adjoint operator can have \"eigenvectors\" that are not actually in the Hilbert space in question.\n\n===Statement of the spectral theorem===\n\nPartially defined operators ''A'', ''B'' on Hilbert spaces ''H'', ''K'' are '''unitarily equivalent''' if and only if there is a [[unitary transformation]] ''U'' : ''H'' → ''K'' such that\n\n* ''U'' maps dom ''A'' [[bijective]]ly onto  dom ''B'',\n* <math> B U \\xi = U A \\xi ,\\qquad \\forall \\xi \\in \\operatorname{dom}A. </math>\n\nA [[multiplication operator]] is defined as follows:  Let (''X'', Σ, μ) be a countably additive [[measure space]] and ''f'' a real-valued measurable function on ''X''. An operator ''T'' of the form\n\n:<math>[T \\psi] (x) = f(x) \\psi(x)</math>\n\nwhose domain is the space of ψ for which the right-hand side above is in ''L''<sup>2</sup> is called a multiplication operator.\n\nOne version of the spectral theorem can be stated as follows.\n\n:'''Theorem.''' Any  multiplication operator is a (densely defined) self-adjoint operator. Any self-adjoint operator is unitarily equivalent to a multiplication operator.<ref>{{harvnb|Hall|2013}} Theorems 7.20 and 10.10</ref>\n\nOther versions of the spectral theorem can be found in the spectral theorem article linked to above.\n\nThe spectral theorem for unbounded self-adjoint operators can be proved by reduction to the spectral theorem for unitary (hence bounded) operators.<ref>{{harvnb|Hall|2013}} Section 10.4</ref>  This reduction uses the ''[[Cayley transform]]'' for self-adjoint operators which is defined in the next section. We might note that if T is multiplication by f, then the spectrum of T is just the [[essential range]] of f.\n\n=== Functional calculus ===\nOne important application of the spectral theorem is to define a \"[[Borel functional calculus|functional calculus]].\" That is to say, if <math>h</math> is a function on the real line and <math>T</math> is a self-adjoint operator, we wish to define the operator <math>h(T)</math>. If <math>T</math> has a true orthonormal basis of eigenvectors <math>e_j</math> with eigenvalues <math>\\lambda_j</math>, then <math>h(T)</math> is the operator with eigenvectors <math>e_j</math> and eigenvalues <math>h\\left(\\lambda_j\\right)</math>. The goal of functional calculus is to extend this idea to the case where <math>T</math> has continuous spectrum.\n\nOf particular importance in quantum physics is the case in which <math>T</math> is the Hamiltonian operator <math>\\hat{H}</math> and <math>h(x) := e^{-itx/\\hbar}</math> is an exponential. In this case, the functional calculus should allow us to define the operator\n:<math>U(t) := h\\left(\\hat{H}\\right) = e^\\frac{-it\\hat{H}}{\\hbar},</math>\n\nwhich is the operator defining the time-evolution in quantum mechanics.\n\nGiven the representation of ''T'' as the operator of multiplication by <math>f</math>—as guaranteed by the spectral theorem—it is easy to characterize the functional calculus:  If ''h'' is a bounded real-valued Borel function on '''R''', then ''h''(''T'') is the operator of multiplication by the composition <math>h \\circ f</math>.\n\n=== Resolution of the identity ===\nIt has been customary to introduce the following notation\n:<math>\\operatorname{E}_T(\\lambda) = \\mathbf{1}_{(-\\infty, \\lambda]} (T)</math>\n\nwhere <math>\\mathbf{1}_{(-\\infty, \\lambda]}</math> is the characteristic function of the interval <math>(-\\infty, \\lambda]</math>.  The family of projection operators E<sub>''T''</sub>(λ) is called '''resolution of the identity''' for ''T''.  Moreover, the following [[Stieltjes integral]] representation for ''T'' can be proved:\n:<math> T = \\int_{-\\infty}^{+\\infty} \\lambda d \\operatorname{E}_T(\\lambda).</math>\n\nThe definition of the operator integral above can be reduced to that of a scalar valued Stieltjes integral using the weak operator topology. In more modern treatments however, this representation is usually avoided, since most technical problems can be dealt with by the functional calculus.\n\n=== Formulation in the physics literature ===\nIn physics, particularly in quantum mechanics, the spectral theorem is expressed in a way which combines the spectral theorem as stated above and the [[Borel functional calculus]] using [[Dirac notation]] as follows:\n\nIf ''H'' is self-adjoint and ''f'' is a [[Borel function]],\n:<math>f(H) = \\int dE \\left| \\Psi_E \\rangle f(E) \\langle \\Psi_E \\right|</math>\n\nwith\n:<math>H \\left|\\Psi_E\\right\\rangle = E \\left|\\Psi_E\\right\\rangle</math>\n\nwhere the integral runs over the whole spectrum of ''H''. The notation suggests that ''H'' is diagonalized by the eigenvectors Ψ<sub>''E''</sub>. Such a notation is purely [[Formal calculation|formal]]. One can see the similarity between Dirac's notation and the previous section. The resolution of the identity (sometimes called projection valued measures) formally resembles the rank-1 projections <math>\\left|\\Psi_E\\right\\rangle \\left\\langle\\Psi_E\\right|</math>. In the Dirac notation, (projective) measurements are described via [[eigenvalues]] and [[eigenstates]], both purely formal objects. As one would expect, this does not survive passage to the resolution of the identity. In the latter formulation, measurements are described using the [[spectral measure]] of <math>|\\Psi \\rangle</math>, if the system is prepared in <math>|\\Psi \\rangle</math> prior to the measurement. Alternatively, if one would like to preserve the notion of eigenstates and make it rigorous, rather than merely formal, one can replace the state space by a suitable [[rigged Hilbert space]].\n\nIf {{nowrap|''f'' {{=}} 1}}, the theorem is referred to as resolution of unity:\n\n:<math>I = \\int dE \\left|\\Psi_E\\right\\rangle \\left\\langle\\Psi_E\\right|</math>\n\nIn the case <math>H_\\text{eff} = H - i\\Gamma</math> is the sum of an Hermitian ''H'' and a skew-Hermitian  (see [[skew-Hermitian matrix]]) operator <math> -i\\Gamma</math>, one defines the [[biorthogonal system|biorthogonal]] basis set\n\n:<math>H^*_\\text{eff} \\left|\\Psi_E^*\\right\\rangle = E^* \\left|\\Psi_E^*\\right\\rangle</math>\n\nand write the spectral theorem as:\n\n:<math>f\\left(H_\\text{eff}\\right) = \\int dE \\left|\\Psi_E\\right\\rangle f(E) \\left\\langle\\Psi_E^*\\right|</math>\n\n(See [[Feshbach–Fano partitioning]] method for the context where such operators appear in [[scattering theory]]).\n\n== Extensions of symmetric operators ==\n{{further|Extensions of symmetric operators|Unbounded operator}}\n\nThe following question arises in several contexts: if an operator ''A'' on the Hilbert space ''H'' is symmetric, when does it have self-adjoint extensions?  An operator that has a unique self-adjoint extension is said to be '''essentially self-adjoint'''; equivalently, an operator is essentially self-adjoint if its closure (the operator whose graph is the closure of the graph of ''A'') is self-adjoint. In general, a symmetric operator could have many self-adjoint extensions or none at all. Thus, we would like a classification of its self-adjoint extensions.\n\nThe first basic criterion for essential self-adjointness is the following:<ref>{{harvnb|Hall|2013}} Theorem 9.21</ref>\n:'''Theorem''': If ''A'' is a symmetric operator on ''H'', then ''A'' is essentially self-adjoint if and only if the range of the operators <math>A-i</math> and <math>A+i</math> are dense in ''H''. \nEquivalently, ''A'' is essentially self-adjoint if and only if the operators <math>A^* - i</math> and <math>A^* + i</math> have trivial kernels.<ref>{{harvnb|Hall|2013}} Corollary 9.22</ref> That is to say, ''A'' ''fails to be'' self-adjoint if and only if <math>A^*</math> has an eigenvector with eigenvalue <math>i</math> or <math>-i</math>.\n\nAnother way of looking at the issue is provided by the '''[[Cayley transform]]''' of a self-adjoint operator and the deficiency indices. (We should note here that it is often of technical convenience to deal with [[closed operator]]s. In the symmetric case, the closedness requirement poses no obstacles, since it is known that all symmetric operators are [[closable operator|closable]].)\n\n: '''Theorem'''. Suppose ''A'' is a symmetric operator. Then there is a unique partially defined linear operator\n::<math>\\operatorname{W}(A) : \\operatorname{ran}(A + i) \\to \\operatorname{ran}(A - i)</math>\n: such that\n::<math> \\operatorname{W}(A)(Ax + ix) = Ax - ix, \\qquad  x \\in \\operatorname{dom}(A). </math>\n\nHere, ''ran'' and ''dom'' denote the [[image (mathematics)|image]] (in other words, range) and the [[domain (mathematics)|domain]], respectively. W(''A'') is [[isometry|isometric]] on its domain.  Moreover, the range of 1&nbsp;−&nbsp;W(''A'') is [[dense set|dense]] in ''H''.\n\nConversely, given any partially defined operator ''U'' which is isometric on its domain (which is not necessarily closed) and such that 1&nbsp;−&nbsp;''U'' is dense, there is a (unique) operator S(''U'')\n: <math>\\operatorname{S}(U) : \\operatorname{ran}(1 - U) \\to \\operatorname{ran}(1 + U)</math>\n\nsuch that\n: <math>\\operatorname{S}(U)(x - Ux) = i(x + U x) \\qquad x \\in \\operatorname{dom}(U).</math>\n\nThe operator S(''U'')  is densely defined and symmetric.\n\nThe mappings W and S are inverses of each other.{{clarify|reason=Is this really true? We did not require the domain of A to be dense, yet S(U) is densely defined. It appears that S(W(A)) has acquired a property that A did not possess.|date=September 2015}}\n\nThe mapping W is called the '''Cayley transform'''.  It associates a [[partial isometry|partially defined isometry]] to any symmetric densely defined operator.  Note that the mappings W and S are [[monotone convergence theorem|monotone]]: This means that if ''B'' is a symmetric operator that extends the densely defined symmetric operator ''A'', then W(''B'') extends W(''A''), and similarly for S.\n\n:'''Theorem'''. A necessary and sufficient condition for ''A'' to be self-adjoint is that its Cayley transform W(''A'') be unitary.\n\nThis immediately gives us a necessary and sufficient condition for ''A'' to have a self-adjoint extension, as follows:\n\n:'''Theorem'''. A necessary and sufficient condition for ''A'' to have a self-adjoint extension is that W(''A'') have a unitary extension.\n\nA partially defined isometric operator ''V'' on a Hilbert space ''H'' has a unique isometric extension to the norm closure of dom(''V''). A partially defined isometric operator with closed domain is called a [[partial isometry]].\n\nGiven a partial isometry ''V'', the '''deficiency indices''' of ''V'' are defined as the dimension of the [[orthogonal complement]]s of the domain and range:\n\n:<math>\\begin{align}\n  n_+(V) &= \\operatorname{dim}\\ \\operatorname{dom}(V)^\\perp \\\\\n  n_-(V) &= \\operatorname{dim}\\ \\operatorname{ran}(V)^\\perp\n\\end{align}</math>\n\n:'''Theorem'''. A partial isometry ''V'' has a unitary extension if and only if the deficiency indices are identical. Moreover, ''V'' has a ''unique'' unitary extension if and only if the deficiency indices are both zero.\n\nWe see that there is a bijection between symmetric extensions of an operator and isometric extensions of its Cayley transform. The symmetric extension is self-adjoint if and only if the corresponding isometric extension is unitary.\n\nA symmetric operator has a unique self-adjoint extension if and only if both its deficiency indices are zero.  Such an operator is said to be '''essentially self-adjoint'''. Symmetric operators which are not essentially self-adjoint may still have a [[canonical form|canonical]] self-adjoint extension.  Such is the case for ''non-negative'' symmetric operators (or more generally, operators which are bounded below).  These operators always have a canonically defined [[Friedrichs extension]] and for these operators we can define a canonical functional calculus. Many operators that occur in  analysis are bounded below (such as the negative of the [[Laplacian]] operator), so the issue of essential adjointness for these operators is less critical.\n\n===Self-adjoint extensions in quantum mechanics===\nIn quantum mechanics, observables correspond to self-adjoint operators. By [[Stone's theorem on one-parameter unitary groups]], self-adjoint operators are precisely the infinitesimal generators of unitary groups of [[time evolution]] operators. However, many physical problems are formulated as a time-evolution equation involving differential operators for which the Hamiltonian is only symmetric.  In such cases, either the Hamiltonian is essentially self-adjoint, in which case the physical problem has unique solutions or one attempts to find self-adjoint extensions of the Hamiltonian corresponding to different types of boundary conditions or conditions at infinity.\n\n'''Example.''' The one-dimensional Schrödinger operator with the potential <math>V(x) = -(1 + |x|)^\\alpha</math>, defined initially on smooth compactly supported functions, is essentially self-adjoint (that is, has a self-adjoint closure) for {{math|0 < ''α'' ≤ 2}} but not for {{math|''α'' > 2}}. See Berezin and Schubin, pages 55 and 86, or Section 9.10 in Hall.\n\nThe failure of essential self-adjointness for <math>\\alpha > 2</math> has a counterpart in the classical dynamics of a particle with potential <math>V(x)</math>: The classical particle escapes to infinity in finite time.<ref>{{harvnb|Hall|2013}} Chapter 2, Exercise 4</ref>\n\n'''Example.''' There is no self-adjoint momentum operator ''p'' for a particle moving on a half-line. Nevertheless, the Hamiltonian <math>p^2</math> of a \"free\" particle on a half-line has several self-adjoint extensions corresponding to different types of boundary conditions. Physically, these boundary conditions are related to reflections of the particle at the origin (see Reed and Simon, vol.2).\n\n== Von Neumann's formulas ==\nSuppose ''A'' is symmetric densely defined. Then any symmetric extension of ''A'' is a restriction of ''A''*.  Indeed, ''A'' ⊆ ''B'' and ''B'' symmetric yields ''B'' ⊆ ''A''* by applying the definition of dom(''A''*).\n\n: '''Theorem.''' Suppose ''A'' is a densely defined symmetric operator. Let\n:: <math> N_\\pm = \\operatorname{ran}(A \\pm i)^\\perp,</math>\n: Then\n:: <math> N_\\pm = \\operatorname{ker}(A^* \\mp i),</math>\n: and\n:: <math> \\operatorname{dom}\\left(A^*\\right) = \\operatorname{dom}\\left(\\overline{A}\\right) \\oplus N_+ \\oplus N_-,</math>\n: where the decomposition is orthogonal relative to the graph inner product of dom(''A''*):\n:: <math>\\langle \\xi | \\eta \\rangle_\\text{graph} = \\langle \\xi | \\eta \\rangle + \\left\\langle A^* \\xi | A^* \\eta \\right\\rangle</math>.\n\nThese are referred to as von Neumann's formulas in the Akhiezer and Glazman reference.\n\n== Examples ==\n===A symmetric operator that is not essentially self-adjoint===\nWe first consider the Hilbert space <math>L^2[0, 1]</math> and the differential operator\n\n: <math>D: \\phi \\mapsto \\frac{1}{i} \\phi'</math>\n\ndefined on the space of continuously differentiable complex-valued functions on [0,1], satisfying the boundary conditions\n:<math>\\phi(0) = \\phi(1) = 0.</math>\n\nThen ''D'' is a symmetric operator as can be shown by [[integration by parts]].  The spaces ''N''<sub>+</sub>, ''N''<sub>−</sub> (defined below) are given respectively by the [[distribution (mathematics)|distribution]]al solutions to the equation\n\n:<math>\\begin{align}\n  -i u' &=  i u \\\\\n  -i u' &= -i u\n\\end{align}</math>\n\nwhich are in ''L''<sup>2</sup>[0, 1].  One can show that each one of these solution spaces is 1-dimensional, generated by the functions ''x'' → ''e''<sup>''−x''</sup> and ''x'' → ''e''<sup>''x''</sup> respectively.  This shows that ''D'' is not essentially self-adjoint,<ref>{{harvnb|Hall|2013}} Section 9.6</ref> but does have self-adjoint extensions.  These self-adjoint extensions are parametrized by the space of unitary mappings ''N''<sub>+</sub> → ''N''<sub>−</sub>, which in this case happens to be the unit circle '''T'''.\n\nIn this case, the failure of essential self-adjointenss is due to an \"incorrect\" choice of boundary conditions in the definition of the domain of <math>D</math>. Since <math>D</math> is a first-order operator, only one boundary condition is needed to ensure that <math>D</math> is symmetric. If we replaced the boundary conditions given above by the single boundary condition\n: <math>\\phi(0) = \\phi(1)</math>,\n\nthen ''D'' would still be symmetric and would now, in fact, be essentially self-adjoint. This change of boundary conditions gives one particular essentially self-adjoint extension of ''D''. Other essentially self-adjoint extensions come from imposing boundary conditions of the form <math>\\phi(1) = e^{i\\theta}\\phi(0)</math>.\n\nThis simple example illustrates a general fact about self-adjoint extensions of symmetric differential operators ''P'' on an open set ''M''. They are determined by the unitary maps between the eigenvalue spaces          \n: <math> N_\\pm = \\left\\{u \\in L^2(M): P_\\operatorname{dist} u =  \\pm i u\\right\\} </math>\n\nwhere ''P''<sub>dist</sub> is the distributional extension of ''P''.\n\n===Constant-coefficient operators===\nWe next give the example of differential operators with [[constant coefficient]]s. Let\n:<math>P\\left(\\vec{x}\\right) = \\sum_\\alpha c_\\alpha x^\\alpha </math>\n\nbe a polynomial on '''R'''<sup>''n''</sup> with ''real'' coefficients, where α ranges over a (finite) set of [[multi-index|multi-indices]].  Thus\n: <math> \\alpha = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_n)</math>\n\nand\n: <math>x^\\alpha = x_1^{\\alpha_1} x_2^{\\alpha_2} \\cdots x_n^{\\alpha_n}.</math>\n\nWe also use the notation\n\n:<math>D^\\alpha = \\frac{1}{i^{|\\alpha|}} \\partial_{x_1}^{\\alpha_1}\\partial_{x_2}^{\\alpha_2} \\cdots \\partial_{x_n}^{\\alpha_n}. </math>\n\nThen the operator ''P''(D) defined on the space of infinitely differentiable functions of compact support on '''R'''<sup>''n''</sup> by\n: <math> P(\\operatorname{D}) \\phi = \\sum_\\alpha c_\\alpha \\operatorname{D}^\\alpha \\phi</math>\n\nis essentially self-adjoint on ''L''<sup>2</sup>('''R'''<sup>''n''</sup>).\n\n:'''Theorem'''. Let ''P'' a polynomial function on '''R'''<sup>''n''</sup> with real coefficients, '''F''' the Fourier transform considered as a unitary map ''L''<sup>2</sup>('''R'''<sup>''n''</sup>) → ''L''<sup>2</sup>('''R'''<sup>''n''</sup>).  Then '''F'''*''P''(D)'''F''' is essentially self-adjoint and its unique self-adjoint extension is the operator of multiplication by the function ''P''.\n\nMore generally, consider linear differential operators acting on infinitely differentiable complex-valued functions of compact support.  If ''M'' is an open subset of '''R'''<sup>''n''</sup>\n:<math>P \\phi(x) = \\sum_\\alpha a_\\alpha (x) \\left[D^\\alpha \\phi\\right](x)</math>\n\nwhere ''a''<sub>α</sub> are (not necessarily constant) infinitely differentiable functions. ''P'' is a linear operator\n:<math> C_0^\\infty(M) \\to C_0^\\infty(M).</math>\n\nCorresponding to ''P'' there is another differential operator, the '''[[formal adjoint]]''' of ''P''\n:<math> P^\\mathrm{*form} \\phi = \\sum_\\alpha D^\\alpha \\left(\\overline{a_\\alpha} \\phi\\right)</math>\n\n:'''Theorem'''.  The adjoint ''P''* of ''P'' is a restriction of the distributional extension of the formal adjoint to an appropriate subspace of <math>L^2</math>.  Specifically:\n::<math>\\operatorname{dom} P^* = \\left\\{u \\in L^2(M): P^{\\mathrm{*form}}u \\in L^2(M) \\right\\}.</math>\n\n== Spectral multiplicity theory ==\nThe multiplication representation of a self-adjoint operator, though extremely useful, is not a canonical representation.  This suggests that it is not easy to extract from this representation a criterion to determine when self-adjoint operators ''A'' and ''B'' are unitarily equivalent.   The finest grained representation which we now discuss involves spectral multiplicity.  This circle of results is called the ''[[Hans Hahn (mathematician)|Hahn]]-[[Ernst Hellinger|Hellinger]] theory of spectral multiplicity''.\n\n===Uniform multiplicity===\nWe first define ''uniform multiplicity'':\n\n'''Definition'''. A self-adjoint operator ''A'' has uniform multiplicity ''n'' where ''n'' is such that 1 ≤ ''n'' ≤ ω if and only if ''A'' is unitarily equivalent  to the operator M<sub>''f''</sub> of multiplication by the function ''f''(λ) = λ on\n: <math>L^2_\\mu\\left(\\mathbf{R}, \\mathbf{H}_n\\right) = \\left\\{\\psi: \\mathbf{R} \\to \\mathbf{H}_n: \\psi \\mbox{ measurable and } \\int_{\\mathbf{R}} \\|\\psi(t)\\|^2 d\\mu(t) < \\infty\\right\\}</math>\n\nwhere '''H'''<sub>''n''</sub> is a Hilbert space of dimension ''n''.  The domain of M<sub>''f''</sub> consists of vector-valued functions ψ on '''R''' such that\n: <math>\\int_\\mathbf{R} |\\lambda|^2\\ \\|\\psi(\\lambda)\\|^2 \\, d\\mu(\\lambda) < \\infty.</math>\n\nNon-negative countably additive measures μ, ν are '''mutually singular''' if and only if they are supported on disjoint Borel sets.\n\n: '''Theorem'''.  Let ''A'' be a self-adjoint operator on a ''separable'' Hilbert space ''H''.  Then there is an ω sequence of countably additive finite measures on '''R''' (some of which may be identically 0)\n:: <math>\\left\\{\\mu_\\ell\\right\\}_{1 \\leq \\ell \\leq \\omega}</math>\n: such that the measures are pairwise singular and ''A'' is unitarily equivalent to the operator of multiplication by the function ''f''(λ) = λ on\n:: <math>\\bigoplus_{1 \\leq \\ell \\leq \\omega} L^2_{\\mu_\\ell} \\left(\\mathbf{R}, \\mathbf{H}_\\ell \\right).</math>\n\nThis representation is unique in the following sense: For any two such representations of the same ''A'', the corresponding measures are equivalent in the sense that they have the same sets of measure 0.\n\n===Direct integrals===\nThe spectral multiplicity theorem can be reformulated using the language of [[direct integral]]s of Hilbert spaces:\n\n:'''Theorem'''.<ref>{{harvnb|Hall|2013}} Theorems 7.19 and 10.9</ref> Any self-adjoint operator on a separable Hilbert space is unitarily equivalent to multiplication by the function λ ↦ λ on\n::<math>\\int_\\mathbf{R}^\\oplus H_\\lambda\\, d \\mu(\\lambda).</math>\n\nUnlike the multiplication-operator version of the spectral theorem, the direct-integral version is unique in the sense that the measure equivalence class of μ (or equivalently its sets of measure 0) is uniquely determined and the measurable function <math>\\lambda\\mapsto\\mathrm{dim}(H_{\\lambda})</math> is determined almost everywhere with respect to μ.<ref>{{harvnb|Hall|2013}} Proposition 7.22</ref> The function <math>\\lambda \\mapsto \\operatorname{dim}\\left(H_\\lambda\\right)</math> is the '''spectral multiplicity function''' of the operator.\n\nWe may now state the classification result for self-adjoint operators: Two self-adjoint operators are unitarily equivalent if and only if (1) their spectra agree as sets, (2) the measures appearing in their direct-integral representations have the same sets of measure zero, and (3) their spectral multiplicity functions agree almost everywhere with respect to the measure in the direct integral.<ref>{{harvnb|Hall|2013}} Proposition 7.24</ref>\n\n=== Example: structure of the Laplacian ===\nThe Laplacian on '''R'''<sup>''n''</sup> is the operator\n:<math>\\Delta = \\sum_{i=1}^n \\partial_{x_i}^2.</math>\n\nAs remarked above, the Laplacian is diagonalized by the Fourier transform.  Actually it is more natural to consider the ''negative'' of the Laplacian −Δ since as an operator it is non-negative; (see [[elliptic operator]]).\n\n'''Theorem'''.  If ''n'' = 1, then −Δ has uniform multiplicity <math>\\text{mult} = 2</math>, otherwise −Δ has uniform multiplicity <math>\\text{mult} = \\omega</math>.  Moreover, the measure μ<sub>'''mult'''</sub> may be taken to be Lebesgue measure on [0, ∞).\n\n== Pure point spectrum ==\nA self-adjoint operator ''A'' on ''H'' has pure point spectrum if and only if ''H'' has an orthonormal basis {''e<sub>i</sub>''}<sub>''i'' ∈ I</sub> consisting of eigenvectors for ''A''.\n\n'''Example'''.  The Hamiltonian for the harmonic oscillator has a quadratic potential ''V'', that is\n:<math>-\\Delta  + |x|^2.</math>\n\nThis Hamiltonian has pure point spectrum; this is typical for bound state [[Hamiltonian (quantum mechanics)|Hamiltonians]] in quantum mechanics. As was pointed out in a previous example, a sufficient condition that an unbounded symmetric operator has eigenvectors which form a Hilbert space basis is that it has a compact inverse.\n\n== See also ==\n*[[Compact operator on Hilbert space]]\n*[[Theoretical and experimental justification for the Schrödinger equation]]\n*[[Unbounded operator]]\n\n== Citations ==\n{{Reflist}}\n\n== References ==\n*{{citation |authorlink=Naum Akhiezer |first=N. I. |last=Akhiezer |first2=I. M. |last2=Glazman |title=Theory of Linear Operators in Hilbert Space |others=Two volumes |location= |publisher=Pitman |year=1981 |isbn= }}\n*{{citation |first=F. A. |last=Berezin |first2=M. A. |last2=Shubin |title=The Schrödinger Equation |location= |publisher=Kluwer |year=1991 }}\n*{{citation |first=B. C. |last=Hall |title=Quantum Theory for Mathematicians |publisher=Springer |series=Graduate Texts in Mathematics|volume=267 |year=2013 |isbn=978-1461471158}}\n*{{citation |authorlink=Tosio Kato |first=T. |last=Kato |title=Perturbation Theory for Linear Operators |publisher=Springer |location=New York |year=1966 |isbn= }}\n*{{citation |authorlink=Michael C. Reed |first=M. |last=Reed |authorlink2=Barry Simon |first2=B. |last2=Simon |title=Methods of Mathematical Physics |others=Vol 2 |publisher=Academic Press |year=1972 |isbn= }}\n*{{citation |authorlink=Gerald Teschl |first=G. |last=Teschl |title=Mathematical Methods in Quantum Mechanics; With Applications to Schrödinger Operators |publisher=American Mathematical Society |location=Providence |year=2009 |url=http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/ }}\n*{{citation |authorlink=Kōsaku Yosida |first=K. |last=Yosida |title=Functional Analysis |location= |publisher=Academic Press |year=1965 |isbn= }}\n*{{citation |authorlink=Valter Moretti |first=V. |last=Moretti |title=Spectral Theory and Quantum Mechanics:Mathematical Foundations of Quantum Theories, Symmetries and Introduction to the Algebraic Formulation |location= |publisher=Springer-Verlag |year=2018 |isbn=978-3-319-70706-8 }}\n\n{{Functional Analysis}}\n\n{{DEFAULTSORT:Self-Adjoint Operator}}\n[[Category:Operator theory]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Solèr's theorem",
      "url": "https://en.wikipedia.org/wiki/Sol%C3%A8r%27s_theorem",
      "text": "In [[mathematics]], '''Solèr's theorem''' is a result concerning certain infinite-dimensional [[Vector space|vector spaces]]. It states that any [[Orthomodular lattice|orthomodular]] form that has an infinite orthonormal sequence is a [[Hilbert space]] over the [[real number]]s, [[complex number]]s or [[quaternion]]s.<ref>{{Cite journal|last=Solèr|first=M. P.|date=1995-01-01|title=Characterization of hilbert spaces by orthomodular spaces|journal=Communications in Algebra|volume=23|issue=1|pages=219–243|doi=10.1080/00927879508825218|issn=0092-7872}}</ref><ref>{{Cite journal|last=Prestel|first=Alexander|date=1995-12-01|title=On Solèr's characterization of Hilbert spaces|journal=Manuscripta Mathematica|language=en|volume=86|issue=1|pages=225–238|doi=10.1007/bf02567991|issn=0025-2611}}</ref> Originally proved by [[Maria Pia Solèr]], the result is significant for [[quantum logic]]<ref>{{Cite book|title=Current Research in Operational Quantum Logic|last=Coecke|first=Bob|last2=Moore|first2=David|last3=Wilce|first3=Alexander|date=2000|publisher=Springer, Dordrecht|isbn=978-90-481-5437-1|location=|pages=1–36|language=en|chapter=Operational Quantum Logic: An Overview|arxiv=quant-ph/0008019|doi=10.1007/978-94-017-1201-9_1}}</ref><ref>{{Cite journal|last=Aerts|first=Diederik|last2=Van Steirteghem|first2=Bart|date=2000-03-01|title=Quantum Axiomatics and a Theorem of M. P. Solèr|journal=[[International Journal of Theoretical Physics]]|language=en|volume=39|issue=3|pages=497–502|arxiv=quant-ph/0105107|doi=10.1023/a:1003661015110|issn=0020-7748}}</ref> and the foundations of [[quantum mechanics]].<ref name=\":0\">{{Cite journal|last=Holland|first=Samuel S.|date=1995|title=Orthomodularity in infinite dimensions; a theorem of M. Solèr|url=http://www.ams.org/bull/1995-32-02/S0273-0979-1995-00593-8/|journal=Bulletin of the American Mathematical Society|volume=32|issue=2|pages=205–234|arxiv=math/9504224|doi=10.1090/s0273-0979-1995-00593-8|issn=0273-0979|via=}}</ref><ref name=\":1\">{{Cite web|url=https://golem.ph.utexas.edu/category/2010/12/solers_theorem.html|title=Solèr's Theorem|last=Baez|first=John C.|authorlink=John C. Baez|date=1 December 2010|website=[[nLab|The n-Category Café]]|archive-url=|archive-date=|dead-url=|access-date=2017-07-22}}</ref> In particular, Solèr's theorem helps to fill a gap in the effort to use [[Gleason's theorem]] to rederive quantum mechanics from [[Information theory|information-theoretic]] postulates.<ref name=\":2\">{{Cite book|title=Physical Theory and its Interpretation|volume = 72|last=Pitowsky|first=Itamar|date=2006|publisher=Springer, Dordrecht|isbn=978-1-4020-4875-3|location=|pages=213–240|language=en|chapter=Quantum Mechanics as a Theory of Probability|arxiv=quant-ph/0510095|doi=10.1007/1-4020-4876-9_10|series = The Western Ontario Series in Philosophy of Science}}</ref><ref>{{Cite journal|last=Grinbaum|first=Alexei|date=2007-09-01|title=Reconstruction of Quantum Theory|url=http://philsci-archive.pitt.edu/2703/1/reconstruction2.pdf|journal=The British Journal for the Philosophy of Science|volume=58|issue=3|pages=387–408|doi=10.1093/bjps/axm028|issn=0007-0882|via=}}<br/>{{Cite journal|last=Cassinelli|first=G.|last2=Lahti|first2=P.|date=2017-11-13|title=Quantum mechanics: why complex Hilbert space?|url=http://rsta.royalsocietypublishing.org/content/375/2106/20160393|journal=[[Philosophical Transactions of the Royal Society A]]|language=en|volume=375|issue=2106|pages=20160393|doi=10.1098/rsta.2016.0393|issn=1364-503X|pmid=28971945|bibcode=2017RSPTA.37560393C}}</ref>\n\nPhysicist [[John C. Baez]] notes,<blockquote>Nothing in the assumptions mentions the continuum: the hypotheses are purely algebraic. It therefore seems quite magical that [the [[division ring]] over which the Hilbert space is defined] is forced to be the real numbers, complex numbers or quaternions.<ref name=\":1\" /></blockquote>Writing a decade after Solèr's original publication, Pitowsky calls her theorem \"celebrated\".<ref name=\":2\" />\n\n== Statement ==\n\nLet <math> \\mathbb K</math> be a [[division ring]]. That means it is a [[Ring (mathematics)|ring]] in which one can add, subtract, multiply, and divide but in which the multiplication need not be [[Commutative property|commutative]]. Suppose this ring has a conjugation, i.e. an operation <math> x \\mapsto x^* </math> for which\n\n: <math>\n\\begin{align}\n& (x+y)^* = x^* + y^*, \\\\\n& (xy)^* = y^* x^* \\text{ (the order of multiplication is inverted), and } \\\\\n& (x^*)^* = x.\n\\end{align}\n</math>\n\nConsider a vector space ''V'' with scalars in <math> \\mathbb K</math>, and a mapping\n\n: <math> (u,v) \\mapsto \\langle u,v\\rangle </math>\n\nwhich is  <math>\\mathbb K </math> -linear  in left (or in the right) entry, satisfying the identity\n\n: <math> \\langle u,v\\rangle = \\langle v,u\\rangle^*. </math>\n\nThis is called a Hermitian form. Suppose this form is non-degenerate in the sense that\n\n: <math> \\langle u,v\\rangle = 0 \\text{ for all values of } u \\text{ only if } v=0.  </math>\n\nFor any subspace ''S'' let <math> S^\\bot</math> be the orthogonal complement of&nbsp;''S''. Call the subspace \"closed\" if <math> S^{\\bot\\bot} = S.</math>\n\nCall this whole vector space, and the Hermitian form, \"orthomodular\" if for every closed subspace ''S'' we have that <math> S + S^\\bot </math> is the entire space. (The term \"orthomodular\" derives from the study of quantum logic. In quantum logic, the [[Distributive property|distributive law]] is taken to fail due to the [[uncertainty principle]], and it is replaced with the \"modular law,\" or in the case of infinite-dimensional Hilbert spaces, the \"orthomodular law.\"<ref name=\":1\" />)\n\nA set of vectors <math display=\"inline\">u_i \\in V</math> is called \"orthonormal\" if <math display=\"block\">\\langle u_i, u_j \\rangle = \\delta_{ij}.</math>The result is this:\n\n: If this space has an infinite orthonormal set, then the division ring of scalars is either the field of real numbers, the field of complex numbers, or the ring of [[quaternion]]s.\n\n==References==\n\n{{Reflist}}\n\n[[Category:Hilbert space]]\n[[Category:Mathematical logic]]\n[[Category:Theorems in quantum physics]]"
    },
    {
      "title": "State space (physics)",
      "url": "https://en.wikipedia.org/wiki/State_space_%28physics%29",
      "text": "{{no footnotes|date=February 2014}}\nIn [[physics]], a '''state space''' is an abstract [[space]] in which different \"positions\" represent, not literal locations, but rather [[state (disambiguation)#Physics|states]] of some physical system. This makes it a type of [[phase space]].\n\nSpecifically, in [[quantum mechanics]] a state space is a [[complex number|complex]] [[Hilbert space]] in which the possible instantaneous states of the system may be described by [[unit vector]]s. These [[quantum state|state vectors]], using [[Paul Dirac|Dirac's]] [[bra–ket notation]], can often be treated like [[coordinate vector]]s and operated on using the rules of [[linear algebra]]. This [[Bra–ket notation|Dirac]] [[Mathematical formulation of quantum mechanics|formalism of quantum mechanics]] can replace calculation of complicated [[integrals]] with simpler vector operations.\n\n==See also==\n*[[Configuration space (physics)]] for the space of possible positions that a physical system may attain\n*[[Configuration space (mathematics)]] for the space of positions of particles in a topological space\n*[[State space (controls)]] for information about '''state space''' in control engineering\n*[[State space]] for information about discrete state space in computer science\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite book | author=Claude Cohen-Tannoudji |title=Quantum Mechanics | publisher=John Wiley & Sons. Inc. |year=1977|isbn=0-471-16433-X}}\n*{{cite book | author=[[David J. Griffiths]] |title=Introduction to Quantum Mechanics | publisher=Prentice Hall |year=1995|isbn=0-13-124405-1}}\n\n[[Category:Concepts in physics]]\n[[Category:Hilbert space]]\n\n\n{{physics-stub}}"
    },
    {
      "title": "Strongly monotone",
      "url": "https://en.wikipedia.org/wiki/Strongly_monotone",
      "text": "{{Expert-subject|mathematics|date=November 2008}}\nThis article is bollocks. See [[coercive bilinear form|coercivity]] instead.\nIn [[functional analysis]], an operator <math>A:X\\to X^*</math> where ''X'' is a real [[Hilbert space]] is said to be '''strongly monotone''' if\n:<math>\\exists\\,c>0 \\mbox{ s.t. } \\langle Au-Av , u-v \\rangle\\geq c \\|u-v\\|^2 \\quad \\forall u,v\\in X.</math>\nThis is analogous to the notion of [[strictly increasing]] for scalar-valued functions of one scalar argument.\n\n==See also==\n* [[Monotonic function]]\n\n==References==\n* Zeidler. ''Applied Functional Analysis'' (AMS 108) p.&nbsp;173\n\n\n[[Category:Hilbert space]]\n\n\n{{Mathanalysis-stub}}"
    },
    {
      "title": "Tensor product of Hilbert spaces",
      "url": "https://en.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces",
      "text": "{{Short description|Tensor product space endowed with a special inner product}}In [[mathematics]], and in particular [[functional analysis]], the '''tensor product of [[Hilbert space]]s''' is a way to extend the [[tensor product]] construction so that the result of taking a tensor product of two Hilbert spaces is another Hilbert space. Roughly speaking, the tensor product is the metric space [[complete metric space|completion]] of the ordinary tensor product. This is an example of a [[topological tensor product]]. The tensor product allows Hilbert spaces to be collected into a [[symmetric monoidal category]].<ref>B. Coecke and E. O. Paquette, Categories for the practising physicist, in: New Structures for Physics, B. Coecke (ed.), Springer Lecture Notes in Physics, 2009. [https://arxiv.org/abs/0905.3010 arXiv:0905.3010]</ref>\n\n==Definition==\n\nSince Hilbert spaces have [[Dot product|inner products]], one would like to introduce an inner product, and therefore a topology, on the tensor product that arise naturally from those of the factors. Let&nbsp;''H''<sub>1</sub> and&nbsp;''H''<sub>2</sub> be two Hilbert spaces with inner products <math>\\langle \\cdot,\\cdot\\rangle_1</math> and <math>\\langle \\cdot,\\cdot\\rangle_2</math>, respectively. Construct the tensor product of&nbsp;''H''<sub>1</sub> and&nbsp;''H''<sub>2</sub> as vector spaces as explained in the article on [[tensor product]]s. We can turn this vector space tensor product into an [[inner product space]] by defining\n\n:<math> \\langle\\phi_1\\otimes\\phi_2,\\psi_1\\otimes\\psi_2\\rangle = \\langle\\phi_1,\\psi_1\\rangle_1 \\, \\langle\\phi_2,\\psi_2\\rangle_2 \\quad \\mbox{for all } \\phi_1,\\psi_1 \\in H_1 \\mbox{ and } \\phi_2,\\psi_2 \\in H_2 </math>\n\nand extending by linearity. That this inner product is the natural one is justified by the identification of scalar-valued bilinear maps on ''H''<sub>1</sub> × ''H''<sub>2</sub> and linear functionals on their vector space tensor product. Finally, take the [[complete space#Completion|completion]] under this inner product. The resulting Hilbert space is the tensor product of &nbsp;''H''<sub>1</sub> and&nbsp;''H''<sub>2</sub>.\n\n===Explicit construction===\n\nThe tensor product can also be defined without appealing to the metric space completion. If ''H''<sub>1</sub> and ''H''<sub>2</sub> are two Hilbert spaces, one associates to every [[simple tensor]] product <math>x_1 \\otimes x_2</math> the rank one operator from <math>H_1^*</math> to ''H''<sub>2</sub> that maps a given <math>x^*\\in H^*_1</math> as\n\n:<math> x^* \\mapsto x^*(x_1) \\, x_2</math>\n\nThis extends to a linear identification between <math>H_1 \\otimes H_2</math> and the space of finite rank operators from <math>H_1^*</math> to ''H''<sub>2</sub>. The finite rank operators are embedded in the Hilbert space <math>HS(H_1^*, H_2)</math> of [[Hilbert–Schmidt operator]]s from <math>H_1^*</math> to ''H''<sub>2</sub>. The scalar product in <math>HS(H_1^*, H_2)</math> is given by\n\n:<math>\\langle T_1, T_2 \\rangle = \\sum_n \\left \\langle T_1 e_n^*, T_2 e_n^* \\right \\rangle, </math>\n\nwhere <math>(e_n^*)</math> is an arbitrary orthonormal basis of <math>H_1^*.</math>\n\nUnder the preceding identification, one can define the Hilbertian tensor product of ''H''<sub>1</sub> and ''H''<sub>2</sub>, that is isometrically and linearly isomorphic to <math>HS(H_1^*, H_2).</math>\n\n===Universal property===\n\nThe Hilbert tensor product <math>H=H_1\\otimes H_2</math> is characterized by the following [[universal property]] {{harv|Kadison|Ringrose|1983|loc=Theorem 2.6.4}}:\n\n:There is a weakly Hilbert–Schmidt mapping ''p''&nbsp;:&nbsp;''H''<sub>1</sub>&nbsp;×&nbsp;''H''<sub>2</sub>&nbsp;→&nbsp;''H'' such that, given any weakly Hilbert–Schmidt mapping ''L''&nbsp;:&nbsp;''H''<sub>1</sub>&nbsp;×&nbsp;''H''<sub>2</sub>&nbsp;→&nbsp;''K'' to a Hilbert space ''K'', there is a unique bounded operator ''T''&nbsp;:&nbsp;''H''&nbsp;→&nbsp;''K'' such that ''L''&nbsp;=&nbsp;''Tp''.\n\nA weakly Hilbert-Schmidt mapping ''L''&nbsp;:&nbsp;''H''<sub>1</sub>&nbsp;×&nbsp;''H''<sub>2</sub>&nbsp;→&nbsp;''K'' is defined as a bilinear map for which a real number ''d'' exists, such that \n\n:<math>\\sum_{i,j=1}^\\infty \\big| \\left \\langle L(e_i, f_j), u  \\right \\rangle\\big|^2 \\le d^2\\,\\|u\\|^2 </math> \n\nfor all <math>u \\in K</math> and one (hence all) orthonormal basis ''e''<sub>1</sub>, ''e''<sub>2</sub>, ... of ''H''<sub>1</sub> and ''f''<sub>1</sub>, ''f''<sub>2</sub>, ... of ''H''<sub>2</sub>.\n\nAs with any universal property, this characterizes the tensor product ''H'' uniquely, up to isomorphism. The same universal property, with obvious modifications, also applies for the tensor product of any finite number of Hilbert spaces. It is essentially the same universal property shared by all definitions of tensor products, irrespective of the spaces being tensored: this implies that any space with a tensor product is a [[symmetric monoidal category]], and Hilbert spaces are a particular example thereof.\n\n===Infinite tensor products===\n\nIf <math>H_n</math> is a collection of Hilbert spaces and <math>\\xi_n</math> is a collection of unit vectors in these Hilbert spaces then the incomplete tensor product (or Guichardet tensor product) is the <math>L^2</math> completion of the set of all finite linear combinations of simple tensor vectors <math>\\otimes_{n=1}^{\\infty} \\psi_n</math> where all but finitely many of the <math>\\psi_n</math>'s equal the corresponding <math>\\xi_n</math>.<ref name=\"BR\">Bratteli, O. and Robinson, D: ''Operator Algebras and Quantum Statistical Mechanics v.1, 2nd ed.'', page 144. Springer-Verlag, 2002.</ref>\n\n===Operator algebras===\n\nLet <math>\\mathfrak{A}_i</math> be the [[von Neumann algebra]] of bounded operators on <math>H_i</math> for <math>i=1,2.</math> Then the von Neumann tensor product of the von Neumann algebras is the strong completion of the set of all finite linear combinations of simple tensor products <math>A_1\\otimes A_2</math> where <math>A_i \\in \\mathfrak{A}_i</math> for <math>i=1,2.</math> This is exactly equal to the von Neumann algebra of bounded operators of <math>H_1\\otimes H_2.</math> Unlike for Hilbert spaces, one may take infinite tensor products of von Neumann algebras, and for that matter [[C* algebra|C*-algebras]] of operators, without defining reference states.<ref name=\"BR\"/> This is one advantage of the \"algebraic\" method in quantum statistical mechanics.\n\n==Properties==\n\nIf <math>H_1</math> and <math>H_2</math> have [[orthonormal basis|orthonormal bases]] <math>\\{\\phi_k\\}</math> and <math>\\{\\psi_l\\},</math> respectively, then <math>\\{\\phi_k\\otimes\\psi_l\\}</math> is an orthonormal basis for <math>H_1\\otimes H_2.</math> In particular, the Hilbert dimension of the tensor product is the product (as [[cardinal number]]s) of the Hilbert dimensions.\n\n==Examples and applications==\n\nThe following examples show how tensor products arise naturally.\n\nGiven two [[measure space]]s <math>X</math> and <math>Y</math>, with measures <math>\\mu</math> and <math>\\nu</math> respectively, one may look at <math>L^2(X\\times Y)</math>, the space of functions on <math>X\\times Y</math> that are square integrable with respect to the product measure <math>\\mu\\times\\nu.</math> If <math>f</math> is a square integrable function on <math>X,</math> and <math>g</math> is a square integrable function on <math>Y,</math> then we can define a function <math>h</math> on <math>X\\times Y</math> by <math>h(x,y)=f(x)g(y).</math> The definition of the product measure ensures that all functions of this form are square integrable, so this defines a [[bilinear map]]ping <math>L^2(X)\\times L^2(Y)\\to L^2(X\\times Y).</math> [[Linear combination]]s of functions of the form <math>f(x)g(y)</math> are also in <math>L^2(X\\times Y)</math>. It turns out that the set of linear combinations is in fact dense in <math>L^2(X\\times Y),</math> if <math>L^2(X)</math> and <math>L^2(Y)</math> are separable.{{cn|date=October 2014}} This shows that <math>L^2(X)\\otimes L^2(Y)</math> is [[isomorphic]] to <math>L^2(X\\times Y),</math> and it also explains why we need to take the completion in the construction of the Hilbert space tensor product.\n\nSimilarly, we can show that <math>L^2(X;H)</math>, denoting the space of square integrable functions <math>X\\to H</math>, is isomorphic to <math>L^2(X)\\otimes H</math> if this space is separable. The isomorphism maps <math>f(x)\\otimes\\phi\\in L^2(X)\\otimes H</math> to <math>f(x)\\phi\\in L^2(X;H)</math> We can combine this with the previous example and conclude that <math>L^2(X)\\otimes L^2(Y)</math> and <math>L^2(X\\times Y)</math> are both isomorphic to <math>L^2(X;L^2(Y)).</math>\n\nTensor products of Hilbert spaces arise often in [[quantum mechanics]]. If some particle is described by the Hilbert space <math>H_1,</math> and another particle is described by <math>H_2,</math> then the system consisting of both particles is described by the tensor product of <math>H_1</math> and <math>H_2.</math> For example, the state space of a [[quantum harmonic oscillator]] is <math>L^2(\\R),</math> so the state space of two oscillators is <math>L^2(\\R)\\otimes L^2(\\R),</math> which is isomorphic to <math>L^2(\\R^2)</math>. Therefore, the two-particle system is described by wave functions of the form <math>\\psi(x_1,x_2).</math> A more intricate example is provided by the [[Fock space]]s, which describe a variable number of particles.\n\n==References==\n{{reflist}}\n\n*{{Citation | last1=Kadison | first1=Richard V. | last2=Ringrose | first2=John R. | title=Fundamentals of the theory of operator algebras. Vol. I | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=[[Graduate Studies in Mathematics]] | isbn=978-0-8218-0819-1 | mr= 1468229 | year=1997 | volume=15}}.\n*{{Citation | last1=Weidmann | first1=Joachim | title=Linear operators in Hilbert spaces | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Graduate Texts in Mathematics]] | isbn=978-0-387-90427-6 |mr=566954 | year=1980 | volume=68}}.\n\n{{Functional Analysis}}\n\n[[Category:Functional analysis]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Topological tensor product",
      "url": "https://en.wikipedia.org/wiki/Topological_tensor_product",
      "text": "{{Short description|Tensor product constructions for topological vector spaces}}In [[mathematics]], there are usually many different ways to construct a '''topological tensor product''' of two [[topological vector space]]s. For [[Hilbert space]]s or [[nuclear space]]s there is a simple [[well-behaved]] theory of [[tensor product]]s (see [[Tensor product of Hilbert spaces]]), but for general [[Banach space]]s or [[locally convex topological vector space]]s the theory is notoriously subtle.\n\n== Motivation ==\nOne of the original motivations for topological tensor products <math>\\hat{\\otimes}</math> is the fact that tensor products of the spaces of smooth functions on <math>\\R^n</math> do not behave as expected. There is an injection\n\n:<math>C^\\infty(\\R^n) \\otimes C^\\infty(\\R^m) \\hookrightarrow C^\\infty(\\R^{n+m})</math>\n\nbut this is not an isomorphism. For example, the function <math>f(x,y) = e^{xy}</math> cannot be expressed as a finite linear combination of smooth functions in <math>C^\\infty(\\R_x)\\otimes C^\\infty(\\R_y).</math><ref>{{Cite web| url= https://math.stackexchange.com/a/2244646/251222|title=What is an example of a smooth function in C∞(R2) which is not contained in C∞(R)⊗C∞(R) |last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref> We only get an isomorphism after constructing the topological tensor product; i.e.,\n\n:<math>C^\\infty(\\R^n) \\mathop{\\hat{\\otimes}} C^\\infty(\\R^m) \\cong C^\\infty(\\R^{n+m})</math>\n\nThis article first details the construction in the Banach space case. <math>C^\\infty(\\R^n)</math> is not a Banach space and further cases are discussed at the end.\n\n==Tensor products of Hilbert spaces==\n:{{Main|Tensor product of Hilbert spaces}}\nThe algebraic tensor product of two Hilbert spaces ''A'' and ''B'' has a natural positive definite [[sesquilinear form]] (scalar product) induced by the sesquilinear forms of ''A'' and ''B''. So in particular it has a natural [[positive definite quadratic form]], and the corresponding completion is a Hilbert space ''A'' ⊗ ''B'', called the (Hilbert space) tensor product of ''A'' and ''B''.\n\nIf the vectors ''a<sub>i</sub>'' and ''b<sub>j</sub>'' run through [[Orthonormal basis|orthonormal bases]] of ''A'' and ''B'', then the vectors ''a<sub>i</sub>''⊗''b<sub>j</sub>'' form an orthonormal basis of ''A'' ⊗ ''B''.\n\n==Cross norms and tensor products of Banach spaces==\nWe shall use the notation from {{harv|Ryan|2002}} in this section. The obvious way to define the tensor product of two Banach spaces ''A'' and ''B'' is to copy the method for Hilbert spaces: define a norm on the algebraic tensor product, then take the completion in this norm. The problem is that there is more than one natural way to define a norm on the tensor product.\n\nIf ''A'' and ''B'' are Banach spaces the algebraic tensor product of ''A'' and ''B'' means the [[tensor product]] of ''A'' and ''B'' as vector spaces and is denoted by <math>A \\otimes B</math>. The algebraic tensor product <math>A \\otimes B</math> consists of all finite sums\n\n:<math>x = \\sum_{i=1}^n a_i \\otimes b_i</math>\n\nwhere <math>n</math> is a natural number depending on <math>x</math> and <math>a_i \\in A</math> and <math>b_i \\in B</math> for\n<math>i = 1, \\ldots, n</math>.\n\nWhen ''A'' and ''B'' are Banach spaces, a '''cross norm''' ''p'' on the algebraic tensor product <math>A \\otimes B</math> is a norm satisfying the conditions\n\n:<math>p(a \\otimes b) = \\|a\\| \\|b\\|,</math>\n:<math>p'(a' \\otimes b') = \\|a'\\| \\|b'\\|.</math>\n\nHere ''a''′ and ''b''′ are in the [[Dual space#Continuous dual space|topological dual spaces]] of ''A'' and ''B'', respectively, and ''p''′ is the [[dual norm]] of ''p''. The term '''reasonable crossnorm''' is also used for the definition above.\n\nThere is a cross norm <math>\\pi</math> called the projective cross norm, given by\n\n:<math>\\pi(x) = \\inf \\left\\{ \\sum_{i=1}^n \\|a_i\\| \\|b_i\\| : x = \\sum_{i} a_i \\otimes b_i \\right\\}</math>\n\nwhere <math>x \\in A \\otimes B</math>.\n\nIt turns out that the projective cross norm agrees with the largest cross norm ({{harv|Ryan|2002}}, proposition 2.1).\n\nThere is a cross norm <math>\\varepsilon</math> called the injective cross norm, given by\n\n:<math>\\varepsilon(x) = \\sup \\{ |(a'\\otimes b')(x)| : a' \\in A', b' \\in B', \\|a'\\| = \\|b'\\| = 1 \\}</math>\n\nwhere <math>x \\in A \\otimes B</math>. Here ''A''′ and ''B''′ mean the topological duals of ''A'' and ''B'', respectively.\n\nNote hereby that the injective cross norm is only in some reasonable sense the \"smallest\".\n\nThe completions of the algebraic tensor product in these two norms are called the projective and injective tensor products, and are denoted by <math>A \\operatorname{\\hat{\\otimes}}_\\pi B</math> and <math>A \\operatorname{\\hat{\\otimes}}_\\varepsilon B.</math>\n\nWhen ''A'' and ''B'' are Hilbert spaces, the norm used for their Hilbert space tensor product is not equal to either of these norms in general. Some authors denote it by σ, so the Hilbert space tensor product in the section above would be <math>A \\operatorname{\\hat{\\otimes}}_\\sigma B.</math>\n\nA '''uniform crossnorm''' α is an assignment to each pair <math>(X, Y)</math> of Banach spaces of a reasonable crossnorm on <math>X \\otimes Y</math> so that if <math>X, W, Y, Z</math> are arbitrary Banach spaces then for all (continuous linear) operators <math>S: X \\to W</math> and <math>T: Y \\to Z</math> the operator <math>S \\otimes T : X \\otimes_\\alpha Y \\to W \\otimes_\\alpha Z</math> is continuous and <math>\\|S \\otimes T\\| \\leq \\|S\\| \\|T\\|.</math> If ''A'' and ''B'' are two Banach spaces and α is a uniform cross norm then α defines a reasonable cross norm on the algebraic tensor product <math>A \\otimes B.</math> The normed linear space obtained by equipping <math>A \\otimes B</math> with that norm is denoted by <math>A \\otimes_\\alpha B.</math> The completion of <math>A \\otimes_\\alpha B,</math> which is a Banach space, is denoted by <math>A \\operatorname{\\hat{\\otimes}}_\\alpha B.</math> The value of the norm given by α on <math>A \\otimes B</math> and on the completed tensor product <math>A \\operatorname{\\hat{\\otimes}}_\\alpha B</math> for an element ''x'' in <math>A \\operatorname{\\hat{\\otimes}}_\\alpha B</math> (or <math>A \\otimes_\\alpha B</math>) is denoted by <math>\\alpha_{A,B}(x)</math> or <math>\\alpha(x).</math>\n\nA uniform crossnorm <math>\\alpha</math> is said to be '''finitely generated''' if, for every pair <math>(X, Y)</math> of Banach spaces and every <math>u \\in X \\otimes Y</math>,\n\n:<math>\\alpha(u; X \\otimes Y) = \\inf \\{ \\alpha(u ; M \\otimes N) : \\dim M, \\dim N < \\infty \\}.</math>\n\nA uniform crossnorm <math>\\alpha</math> is '''cofinitely generated''' if, for every pair <math>(X, Y)</math> of Banach spaces and every <math>u \\in X \\otimes Y</math>,\n\n:<math>\\alpha(u) = \\sup \\{ \\alpha((Q_E \\otimes Q_F)u; (X/E) \\otimes (Y/F)) : \\dim X/E, \\dim Y/F < \\infty \\}.</math>\n\nA '''tensor norm''' is defined to be a finitely generated uniform crossnorm. The projective cross norm <math>\\pi</math> and the injective cross norm <math>\\varepsilon</math> defined above are tensor norms and they are called the projective tensor norm and the injective tensor norm, respectively.\n\nIf ''A'' and ''B'' are arbitrary Banach spaces and ''α'' is an arbitrary uniform cross norm then\n\n:<math>\\varepsilon_{A,B}(x) \\leq \\alpha_{A,B}(x) \\leq \\pi_{A,B}(x).</math>\n\n==Tensor products of locally convex topological vector spaces==\n\nThe topologies of locally convex topological vector spaces <math>A</math> and <math>B</math> are given by families of [[seminorm]]s. For each choice of seminorm on <math>A</math> and on <math>B</math> we can define the corresponding family of cross norms on the algebraic tensor product <math>A\\otimes B,</math> and by choosing one cross norm from each family we get some cross norms on <math>A\\otimes B,</math> defining a topology. There are in general an enormous number of ways to do this. The two most important ways are to take all the projective cross norms, or all the injective cross norms. The completions of the resulting topologies on <math>A\\otimes B</math> are called the projective and injective tensor products, and denoted by <math>A\\otimes_{\\gamma} B</math> and <math>A\\otimes_{\\lambda} B.</math> There is a natural map from <math>A\\otimes_{\\gamma} B</math> to <math>A\\otimes_{\\lambda} B.</math>\n\nIf <math>A</math> or <math>B</math> is a [[nuclear space]] then the natural map from <math>A\\otimes_{\\gamma} B</math> to <math>A\\otimes_{\\lambda} B</math> is an [[isomorphism]]. Roughly speaking, this means that if <math>A</math> or <math>B</math> is nuclear, then there is only one sensible tensor product of <math>A</math> and <math>B</math>.\nThis property characterizes nuclear spaces.\n\n==See also==\n*[[Hilbert space]]\n*[[Banach space]]\n*[[Fréchet space]]\n*[[locally convex topological vector space]]\n*[[Nuclear space]]\n*[[Tensor product of Hilbert spaces]]\n*[[Fredholm kernel]]\n*[[Projective topology]]\n\n==References==\n{{Reflist}}\n\n*{{citation|last=Ryan|first=R.A.|title=Introduction to Tensor Products of Banach Spaces|publisher=Springer|publication-place=New York| year=2002}}.\n*{{citation|first=A.|last=Grothendieck|authorlink=Alexander Grothendieck|title=Produits tensoriels topologiques et espaces nucléaires| journal=Memoirs of the American Mathematical Society|volume=16|year=1955}}.\n\n{{Functional Analysis}}\n\n[[Category:Operator theory]]\n[[Category:Topological vector spaces]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Tree kernel",
      "url": "https://en.wikipedia.org/wiki/Tree_kernel",
      "text": "In [[machine learning]], '''tree kernels''' are the application of the more general concept of [[positive-definite kernel]] to tree structures. They find applications in [[natural language processing]], where they can be used for [[machine-learned]] [[parsing]] or classification of sentences.\n\n== Motivation ==\nIn natural language processing, it is often necessary to compare tree structures (e.g. [[parse tree]]s) for similarity. Such  comparisons can be performed by computing [[dot product]]s of vectors of features of the trees, but these vectors tend to be very large: NLP techniques have come to a point where a simple dependency relation over two words is encoded with a vector of several millions of features.<ref>{{cite conference |first1=Ryan |last1=McDonald |first2=Fernando |last2=Pereira |first3=Kiril |last3=Ribarov |first4=Jan |last4=Hajič |title=Non-projective Dependency Parsing using Spanning Tree Algorithms |conference=HLT–EMNLP |year=2005 |url=http://www.aclweb.org/anthology/H/H05/H05-1.pdf#page=559}}</ref> It can be impractical to represent complex structures such as trees with features vectors. Well-designed kernels allow computing similarity over trees without explicitly computing the feature vectors of these trees. Moreover, [[kernel methods]] have been widely used in machine learning tasks ( e.g. [[Support vector machine|SVM]] ), and thus plenty of algorithms are working natively with kernels, or have an extension that handles [[Kernel trick|kernelization]].\n\nAn example application is classification of sentences, such as different types of questions.<ref name=\"zhang\">{{cite conference |last1=Zhang |first1=Dell |first2=Wee Sun |last2=Lee |title=Question classification using support vector machines |conference=SIGIR |year=2003}}</ref>\n\n== Examples ==\n[[File:Acat.svg|thumb|Constituency parse tree for the sentence : \"A cat eats a mouse.\"]]\n[[File:Amouse.svg|thumb|Same as above, for the sentence : \"A mouse eats a cat.\"]]\n\nHere are presented two examples of tree kernel applied to the constituency trees of the sentences \"A cat eats a mouse.\" and \"A mouse eats a cat.\". In this example \"A\" and \"a\" are the same words, and in most of the NLP applications they would be represented with the same token.\n\nThe interest of these two kernels is that they show very different granularity (the subset tree kernel being far more fine-grained than the subtree kernel), for the same computation complexity. Both can be computed recursively in time ''O(|T<sub>1</sub>|.|T<sub>2</sub>|)''.<ref name=\"collins\">{{cite conference |authorlink1=Michael Collins (computational linguist) |last1=Collins |first1=Michael |first2=Nigel |last2=Duffy |title=Convolution kernels for natural language |conference=[[Conference on Neural Information Processing Systems|Advances in Neural Information Processing Systems]] |year=2001}}</ref>\n\n=== Subtree kernel ===\n\nIn the case of constituency tree, a subtree is defined as a node and all its children (e.g., [NP [D [A]] [N [mouse]]] is a subtree of the two trees). Terminals are not considered subtree (e.g. [a] is not a subtree). The subtree kernel count the number of common subtrees between two given trees.\n\nIn this example, there are seven common subtrees:\n:[NP [D [a]] [N [cat]]],\n:[NP [D [a]] [N [mouse]]],\n:[N [mouse]],\n:[N [cat]],\n:[V [eats]],\n:[D [a]] (counted twice as it appears twice).\n\n=== Subset tree kernel ===\n\nA subset tree is a more general structure than a subtree. The basic definition is the same, but in the case of subset trees, leaves need not be terminals (e.g., [VP [V] [NP]] is a subset tree of both trees), but here too single nodes are not considered as trees. Because of this more general definition, there are more subset trees than subtrees, and more common subset trees than common subtrees.\n\nIn this example, there are 54 common subset trees. The seven common subtrees plus among others:\n:[NP [D] [N]] (counted twice),\n:[VP [V [eats]] [NP]]...\n\n== See also ==\n*[[Graph kernel]]\n*[[Parse tree]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n*Jun Sun, Min Zhang and Chew Lim Tan. ''Tree Sequence Kernel for Natural Language''\n*Alessandro Moschitti. ''Making Tree Kernels practical for Natural Language Learning''\n\n== External links ==\n*[http://disi.unitn.it/moschitti/Tree-Kernel.htm http://disi.unitn.it/moschitti/Tree-Kernel.htm] -- Application of tree kernel to SVM, on Alessandro Moschitti web-page.\n\n[[Category:Operator theory]]\n[[Category:Hilbert space]]"
    },
    {
      "title": "Weak convergence (Hilbert space)",
      "url": "https://en.wikipedia.org/wiki/Weak_convergence_%28Hilbert_space%29",
      "text": "{{Unreferenced|date=December 2009}}\nIn [[mathematics]], '''weak convergence''' in a [[Hilbert space]] is [[Limit of a sequence|convergence]] of a [[sequence]] of points in the [[weak topology]].\n\n==Definition==\nA [[sequence (mathematics)|sequence]] of points <math>(x_n)</math> in a Hilbert space ''H'' is said to '''converge weakly''' to a point ''x'' in ''H'' if\n\n:<math>\\langle x_n,y \\rangle \\to \\langle x,y \\rangle</math>\n\nfor all ''y'' in ''H''.  Here, <math>\\langle \\cdot, \\cdot \\rangle</math> is understood to be the [[inner product]] on the Hilbert space. The notation\n\n:<math>x_n \\rightharpoonup x</math>\n\nis sometimes used to denote this kind of convergence.\n\n==Properties==\n*If a sequence converges strongly, then it converges weakly as well.\n*Since every closed and bounded set is weakly [[Relatively compact subspace|relatively compact]] (its closure in the weak topology is compact), every [[bounded sequence]] <math>x_n</math> in a Hilbert space ''H'' contains a weakly convergent subsequence. Note that closed and bounded sets are not in general weakly compact in Hilbert spaces (consider the set consisting of an [[orthonormal basis]] in an infinitely dimensional Hilbert space which is closed and bounded but not weakly compact since it doesn't contain 0). However, bounded and weakly closed sets are weakly compact so as a consequence every convex bounded closed set is weakly compact.\n*As a consequence of the [[Banach-Steinhaus Theorem|principle of uniform boundedness]], every weakly convergent sequence is bounded.\n*The norm is (sequentially) weakly [[Lower semicontinuous|lower-semicontinuous]]: if <math>x_n</math> converges weakly to ''x'', then\n\n::<math>\\Vert x\\Vert \\le \\liminf_{n\\to\\infty} \\Vert x_n \\Vert, </math>\n\n:and this inequality is strict whenever the convergence is not strong. For example, infinite orthonormal sequences converge weakly to zero, as demonstrated below.\n\n*If <math>x_n</math> converges weakly to <math>x</math> and we have the additional assumption that <math>\\lVert x_n \\rVert \\to \\lVert x \\rVert</math>, then <math> x_n</math> converges to <math>x</math> strongly:\n\n::<math>\\langle x - x_n, x - x_n \\rangle = \\langle x, x \\rangle + \\langle x_n, x_n \\rangle - \\langle x_n, x \\rangle - \\langle x, x_n \\rangle \\rightarrow 0.</math>\n\n*If the Hilbert space is finite-dimensional, i.e. a [[Euclidean space]], then the concepts of weak convergence and strong convergence are the same.\n\n=== Example ===\n[[Image:Sinfrequency.jpg|alt=The first 3 curves in the sequence fn=sin(nx)|thumb|350px|The first 3 functions in the sequence <math>f_n(x) = \\sin(n x)</math> on <math>[0, 2 \\pi]</math>. As <math>n \\rightarrow \\infty</math> <math>f_n</math> converges weakly to <math>f =0</math>.]]\n\nThe Hilbert space <math>L^2[0, 2\\pi]</math> is the space of the [[square-integrable function]]s on the interval <math>[0, 2\\pi]</math> equipped with the inner product defined by\n:<math>\\langle f,g \\rangle = \\int_0^{2\\pi} f(x)\\cdot g(x)\\,dx,</math>\n(see [[Lp space|L<sup>''p''</sup> space]]). The sequence of functions <math>f_1, f_2, \\ldots</math> defined by\n:<math>f_n(x) = \\sin(n x)</math>\nconverges weakly to the zero function in <math>L^2[0, 2\\pi]</math>, as the integral\n:<math>\\int_0^{2\\pi} \\sin(n x)\\cdot g(x)\\,dx.</math>\ntends to zero for any square-integrable function <math>g</math> on <math>[0, 2\\pi]</math> when <math>n</math> goes to infinity, which is by [[Riemann–Lebesgue lemma]],  i.e.\n:<math>\\langle f_n,g \\rangle \\to \\langle 0,g \\rangle = 0.</math>\nAlthough <math>f_n</math> has an increasing number of 0's in <math>[0,2 \\pi]</math> as <math>n</math> goes to infinity, it is of course not equal to the zero function for any <math>n</math>. Note that <math>f_n</math> does not converge to 0 in the <math>L_\\infty</math> or <math>L_2</math> norms. This dissimilarity is one of the reasons why this type of convergence is considered to be \"weak.\"\n\n===Weak convergence of orthonormal sequences===\nConsider a sequence <math>e_n</math> which was constructed to be orthonormal, that is,\n\n:<math>\\langle e_n, e_m \\rangle = \\delta_{mn}</math>\n\nwhere <math>\\delta_{mn}</math> equals one if ''m'' = ''n'' and zero otherwise.  We claim that if the sequence is infinite, then it converges weakly to zero. A simple proof is as follows. For ''x'' ∈ ''H'', we have\n\n:<math> \\sum_n | \\langle e_n, x \\rangle |^2 \\leq \\| x \\|^2</math> ([[Bessel's inequality]])\n\nwhere equality holds when {''e''<sub>''n''</sub>} is a Hilbert space basis. Therefore\n\n:<math> | \\langle e_n, x \\rangle |^2 \\rightarrow 0</math> (since the series above converges, its corresponding sequence must go to zero)\n\ni.e.\n\n:<math> \\langle e_n, x \\rangle  \\rightarrow 0 .</math>\n\n==Banach–Saks theorem==\nThe '''Banach–Saks theorem''' states that every bounded sequence <math>x_n</math> contains a subsequence <math>x_{n_k}</math> and a point ''x'' such that\n\n:<math>\\frac{1}{N}\\sum_{k=1}^N x_{n_k}</math>\n\nconverges strongly to ''x'' as ''N'' goes to infinity.\n\n==Generalizations==\n{{See also|Weak topology|Weak topology (polar topology)}}\n\nThe definition of weak convergence can be extended to [[Banach space]]s. A sequence of points <math>(x_n)</math> in a Banach space ''B'' is said to '''converge weakly''' to a point ''x'' in ''B'' if\n\n:<math>f(x_n) \\to f(x)</math>\n\nfor any bounded linear [[functional (mathematics)|functional]] <math>f</math> defined on <math>B</math>, that is, for any <math>f</math> in the [[dual space]] <math>B'</math>. If <math>B</math> is an [[Lp space]] on <math>\\Omega</math>, and <math>p<\\infty</math> then, any such <math>f</math> has the form\n\n:<math>f(x)=\\int_{\\Omega} x\\,y\\,d\\mu</math>   \n\nFor some <math>y\\in\\,L^q(B)</math> where <math>\\frac{1}{p}+\\frac{1}{q}=1</math> and <math>\\mu</math> is the [[Measure (mathematics)|measure]] on <math>\\Omega</math>.\n\nIn the case where <math>B</math> is a Hilbert space, then, by the [[Riesz representation theorem]],\n\n:<math>f(\\cdot)=\\langle \\cdot,y \\rangle</math>\n\nfor some <math>y</math> in <math>B</math>, so one obtains the Hilbert space definition of weak convergence.\n\n{{Functional Analysis}}\n\n{{DEFAULTSORT:Weak Convergence (Hilbert Space)}}\n[[Category:Hilbert space]]"
    },
    {
      "title": "Wigner's theorem",
      "url": "https://en.wikipedia.org/wiki/Wigner%27s_theorem",
      "text": "[[Image:Wigner.jpg|220px|thumb|right|[[Eugene Wigner|E.P. Wigner]] (1902–1995), [[Foreign Member of the Royal Society|ForMemRS]], first proved the theorem bearing his name. It was a key step towards the modern classification scheme of particle types, according to which particle types are partly characterized by which [[representation theory of the Lorentz group|representation]] of the [[Lorentz group]] under which it transforms. The Lorentz group is a symmetry group of every relativistic quantum field theory.\n\nWigner's early work laid the ground for what many physicists came to call the '''group theory disease'''<ref>{{harvnb|Seitz|Vogt|Weinberg|2000}}</ref> in quantum mechanics &ndash; or as [[Hermann Weyl]] (co-responsible) puts it in his [https://www.amazon.co.uk/Theory-Groups-Quantum-Mechanics-Mathematics/dp/0486602699/ref=sr_1_1?s=books&ie=UTF8&qid=1423285616&sr=1-1&keywords=9780486602691 ''The Theory of Groups and Quantum Mechanics''] (preface to 2nd ed.), \"It has been rumored that the '''group pest''' is gradually being cut out from quantum mechanics. This is certainly not true…\"\n]]\n\n'''Wigner's theorem''', proved by [[Eugene Wigner]] in 1931,<ref>{{harvnb|Wigner|1931|pp=251–254}} (in German),<br/>{{harvnb|Wigner|1959|pp=233–236}} (English translation).</ref>  is a cornerstone of the [[mathematical formulation of quantum mechanics]]. The theorem specifies how physical [[symmetries]] such as rotations, translations, and [[CPT symmetry|CPT]] are represented on the [[Hilbert space]] of [[Quantum state|states]].\n\nAccording to the theorem, any '''symmetry transformation''' of '''ray space''' is represented by a [[unitary transformation|linear and unitary]] or [[antiunitary operator|antilinear and antiunitary]] transformation of Hilbert space. The representation of a '''symmetry group''' on Hilbert space is either an ordinary [[representation (group theory)|representation]] or a [[projective representation]].\n\n==Rays and ray space==\nIt is a [[Postulates of quantum mechanics|postulate of quantum mechanics]] that vectors in [[Hilbert space]] that are scalar nonzero multiples of each other represent the same [[quantum state|pure state]]. A '''ray''' is a set<ref>{{harvnb|Weinberg|2002|p=49}}</ref><ref name=Bauerle_de_Kerf_1999_p341>{{harvnb|Bäuerle|de Kerf|1999|p=341}}</ref>\n\n:<math>\\underline{\\Psi} = \\left\\{e^{i\\alpha}\\Psi : \\alpha \\in \\mathbb{R}, \\emptyset \\ne \\Psi \\in \\mathcal{H}\\right\\}, </math>\n\nand a ray whose vectors have unit norm is called a '''unit ray'''. If {{math|Φ ∈ {{underline|Ψ}}}}, then {{math|Φ}} is a '''representative''' of {{math|{{underline|Ψ}}}}. There is a one-to-one correspondence between physical pure states and unit rays.<ref group=nb>Here the possibility of [[superselection rule]]s is ignored. It may be the case that a system cannot be prepared in specific states. For instance, superposition of states with different spin is generally believed impossible. Likewise, states being superpositions of states with different charge are considered impossible. Minor complications due to those issues are treated in {{harvtxt|Bogoliubov|Logunov|Todorov|1975}}</ref> The space of all rays is called '''ray space'''.\n\nFormally,<ref name=Simon_2008>{{harvnb|Simon|Mukunda|Chaturvedi|Srinivasan|2008}}</ref> if {{mvar|H}} is a complex Hilbert space, then let {{mvar|B}} be the subset\n:<math>B = \\{\\Psi \\in \\mathcal{H}: \\lVert\\Psi\\rVert = 1\\}</math>\n\nof vectors with unit norm. If {{mvar|H}} is finite-dimensional with complex dimension {{mvar|N}}, then {{mvar|B}} has real dimension {{math|2''N'' − 1}}. Define a relation ≅ on {{mvar|B}} by\n:<math>\\Psi \\cong \\Phi \\Leftrightarrow \\Psi = e^{i\\alpha}\\Phi,\\quad \\alpha \\in \\mathbb{R}.</math>\n\nThe relation ≅ is an [[equivalence relation]] on the set {{mvar|B}}. '''Unit ray space''', {{mvar|S}}, is defined as the set of equivalence classes\n:<math>S = B / {\\cong}.</math>\n\nIf {{mvar|N}} is finite, {{mvar|S}} has real dimension {{math|2''N'' − 2}} hence complex dimension {{math|''N'' − 1}}. Equivalently for these purposes, one may define ≈ on {{mvar|H}} by\n:<math>\\Psi \\approx \\Phi \\Leftrightarrow \\Psi = z\\Phi,\\quad z \\in \\mathbb{C} \\smallsetminus \\{0\\},</math>\n\nwhere {{math|ℂ \\ {0} }} is the set of nonzero complex numbers, and set\n\n:<math>S^\\prime = \\mathcal{H} \\smallsetminus \\emptyset / {\\approx}.</math>\n\nThis definition makes it clear that unit ray space is a [[projective Hilbert space]]. It is also possible to skip the normalization and take '''ray space''' as<ref>This approach is used in {{harvnb|Bargmann|1964}}, which serves as a basis reference for the proof outline to be given below.</ref>\n\n:<math>R = \\mathcal{H} \\smallsetminus \\emptyset / {\\cong},</math>\n\nwhere ≅ is now defined on all of {{mvar|H}} by the same formula. The real dimension of {{mvar|R}} is {{math|2''N'' − 1}} if {{mvar|N}} is finite. This approach is used in the sequel. The difference between {{mvar|R}} and {{mvar|S}} is rather trivial, and passage between the two is effected by multiplication of the rays by a nonzero ''real'' number, defined as the ray generated by any representative of the ray multiplied by the real number.\n\nRay space is sometimes awkward to work with. It is, for instance, not a vector space with well-defined linear combinations of rays. But a transformation of a physical system is a transformation of states, hence mathematically a transformation of ray space. In quantum mechanics, a transformation of a physical system gives rise to a [[bijection|bijective]] '''unit ray transformation''' {{math|''T''}} of unit ray space,\n:<math>T: S \\ni \\underline{\\Psi} \\subset \\mathcal{H} \\mapsto S' \\ni \\underline{\\Psi'} = T\\underline{\\Psi} \\subset \\mathcal{H}'.</math>\n\nThe set of all unit ray transformations is thus the [[permutation group]] on {{mvar|S}}. Not all of these transformations are permissible as symmetry transformations to be described next. A unit ray transformation may be extended to {{mvar|R}} by means of the multiplication with reals described above according to<ref>{{harvnb|Bauerle|de Kerf|1999|p=341}} defines general ray transformations on {{mvar|R}} to begin with, which means that it is not necessarily bijective on {{mvar|S}} (i.e. not necessarily norm preserving). This is not important since only symmetry transformations are of interest anyway.</ref>\n:<math>T: R \\rightarrow R'; T\\left(\\lambda \\underline{\\Psi}\\right) \\equiv \\lambda T \\underline{\\Psi},\\quad \\underline{\\Psi} \\in S, \\lambda \\in \\mathbb{R}.</math>\n\nTo keep the notation uniform, call this a '''ray transformation'''. This terminological distinction is not made in the literature, but is necessary here since both possibilities are covered while in the literature one possibility is chosen.\n\n==Symmetry transformations==\nLoosely speaking, a symmetry transformation is a change in which \"nothing happens\"<ref>{{harvnb|de Kerf|Bäuerle|1999}}</ref> or a \"change of our view\"<ref>{{harvnb|Weinberg|2002|p=50}}</ref> that does not change the outcomes of possible experiments. For example, translating a system in a [[Homogeneity (physics)|homogeneous]] environment should have no qualitative effect on the outcomes of experiments made on the system. Likewise for rotating a system in an [[isotropic]] environment. This becomes even clearer when one considers the mathematically equivalent [[active and passive transformation|passive transformations]], i.e. simply changes of coordinates and let the system be. Usually, the domain and range Hilbert spaces are the same. An exception would be (in a non-relativistic theory) the Hilbert space of electron states that is subjected to a [[charge conjugation]] transformation. In this case the electron states are mapped to the Hilbert space of positron states and vice versa. To make this precise, introduce the '''ray product''',\n:<math>\\underline{\\Psi} \\cdot \\underline{\\Phi} = \\left|\\left(\\Psi, \\Phi\\right)\\right|,</math>\n\nwhere {{math|(,)}} is the Hilbert space [[inner product]], and {{math|Ψ,Φ}} are normalized elements of this space. A surjective ray transformation {{math|''T'': ''R'' → ''R<nowiki>'</nowiki>''}} is called a '''symmetry transformation''' if<ref>{{harvnb|de Kerf|Van Groesen|1999|p=342}}</ref>\n:<math>T \\underline{\\Psi} \\cdot T\\underline{\\Phi} = \\underline{\\Psi} \\cdot \\underline{\\Phi},\\quad \\forall \\Psi, \\Phi \\in \\mathcal{H}.</math>\n\nIt can also be defined in terms of unit ray space; i.e., {{math|''T'': ''S'' → ''S<nowiki>'</nowiki>''}} with no other changes.<ref name=Bargmann_1964>{{harvnb|Bargmann|1964}}</ref><ref name=Wigner_1931>{{harvnb|Wigner|1931}}</ref> In this case it is sometimes called a '''Wigner automorphism'''. It can then be extended to {{mvar|R}} by means of multiplication by reals as described earlier. In particular, unit rays are taken to unit rays. The significance of this definition is that [[transition probability|transition probabilities]] are preserved. In particular the [[Born rule]], another postulate of quantum mechanics, will predict the same probabilities in the transformed and untransformed systems,\n:<math>P(\\Psi \\rightarrow \\Phi) = |(\\Psi, \\Phi)|^2 = \\left[\\underline{\\Psi} \\cdot \\underline{\\Phi}\\right]^2 = \\left[T\\underline{\\Psi} \\cdot T\\underline{\\Phi}\\right]^2 = \\left|\\left(\\Psi', \\Phi'\\right)\\right|^2 = P\\left(\\Psi' \\rightarrow \\Phi'\\right), \\quad \\Psi' \\in T\\underline{\\Psi}, \\Phi' \\in T\\underline{\\Phi}.</math>\n\nIt is clear from the definitions that this is independent of the representatives of the rays chosen.\n\n==Symmetry groups==\nSome facts about symmetry transformations that can be verified using the definition:\n* The product of two symmetry transformations, i.e. two symmetry transformations applied in succession, is a symmetry transformation.\n* Any symmetry transformation has an inverse.\n* The identity transformation is a symmetry transformation.\n* Multiplication of symmetry transformations is associative.\n\nThe set of symmetry transformations thus forms a [[group (mathematics)|group]], the '''symmetry group''' of the system. Some important frequently occurring [[subgroup]]s in the symmetry group of a system are [[presentation (group theory)|realization]]s of\n* The [[permutation group|symmetric group]] with its subgroups. This is important on the exchange of particle labels.\n* The [[Poincaré group]]. It encodes the fundamental symmetries of [[spacetime]].\n* Internal symmetry groups like [[SU(2)]] and [[SU(3)]]. They describe so called [[internal symmetry|internal symmetries]], like [[isospin]] and [[color charge]] peculiar to quantum mechanical systems.\n\nThese groups are also referred to as symmetry groups of the system.\n\n==Statement of Wigner's theorem==\n\n===Preliminaries===\nSome preliminary definitions are needed to state the theorem. A transformation {{mvar|U}} of Hilbert space is [[unitary transformation|unitary]] if\n:<math>(U \\Psi, U \\Phi) = (\\Psi, \\Phi),</math>\n\nand a transformation {{mvar|A}} is [[antiunitary transformation|antiunitary]] if\n:<math>(A \\Psi, A \\Phi) = (\\Psi, \\Phi)^* = (\\Phi, \\Psi).</math>\n\nA unitary operator is automatically [[linear transformation|linear]]. Likewise an antiunitary transformation is necessarily [[antilinear map|antilinear]].<ref group=nb>{{harvtxt|Bäurle|de Kerf|1999|p=342}} This is stated but not proved.</ref> Both variants are real linear and additive.\n\nGiven a unitary transformation {{mvar|U}} of Hilbert space, define\n:<math>T: \\underline{\\Psi} = \\left\\{e^{i\\alpha}\\Psi \\mid \\alpha \\in \\mathbb{R}\\right\\} \\mapsto \\underline{\\Psi'} = \\left\\{e^{i\\beta} U\\Psi \\mid \\beta \\in \\mathbb{R}\\right\\}.</math>\n\nThis is a symmetry transformation since\n:<math>\n  T\\underline{\\Psi} \\cdot T\\underline{\\Phi} = \\underline{\\Psi'} \\cdot \\underline{\\Phi'} =\n  \\left|\\left(e^{i\\alpha}U\\Psi, e^{i\\beta}U\\Phi\\right)\\right| =\n  |(\\Psi, \\Phi)| = \\underline{\\Psi} \\cdot \\underline{\\Phi}.\n</math>\n\nIn the same way an antiunitary transformation of Hilbert space induces a symmetry transformation. One says that a transformation {{math|''U''}} of Hilbert space is '''compatible''' with the transformation {{mvar|T}} of ray space if for all {{math|Ψ}},<ref name=Bargmann_1964/>\n:<math>T\\underline{\\Psi} = \\left\\{e^{i\\alpha}U\\Psi \\mid \\alpha \\in \\mathbb{R}\\right\\},</math>\n\nor equivalently\n:<math>U\\Psi \\in T \\underline \\Psi.</math>\n\nTransformations of Hilbert space by either a unitary linear transformation or an antiunitary antilinear operator are obviously then compatible with the transformations or ray space they induce as described.\n\n===Statement===\nWigner's theorem states a converse of the above:<ref>{{harvnb|de Kerf|Van Groesen|1999|p=343}}</ref>\n: '''''Wigner's theorem (1931):''''' ''If {{mvar|H}} and {{mvar|K}} are Hilbert spaces and if''\n:: <math>T:\\underline{\\Psi} \\subset \\mathcal{H} \\mapsto \\underline{\\Psi'} \\subset \\mathcal{K}</math>\n: ''is a symmetry transformation, then there exists a transformation'' {{math|''V'':''H'' → ''K''}} ''which is compatible with'' {{mvar|T}} ''and such that'' {{mvar|V}} ''is either unitary or antiunitary if'' {{math|dim ''H'' ≥ 2}}. ''If'' {{math|dim ''H'' {{=}} 1}} ''there exists a unitary transformation'' {{math|''U'':''H'' → ''K''}} ''and an antiunitary transformation'' {{math|''A'':''H'' → ''K''}}, ''both compatible with'' {{mvar|T}}''.\n\nProofs can be found in {{harvard citations|txt|last=Wigner|year=1931|year2=1959}}, {{harvtxt|Bargmann|1964}} and {{harvtxt|Weinberg|2002}}.\n\nAntiunitary and antilinear transformations are less prominent in physics. They are all related to a reversal of the direction of the flow of time.<ref>{{harvnb|Weinberg|2002|p=51}}</ref>\n\n==Representations and projective representations==\nA transformation compatible with a symmetry transformation is not unique. One has the following (additive transformations include both linear and antilinear transformations).<ref>This is proved in detail in {{harvnb|Bargmann|1964}}.</ref><ref>{{harvnb|de Kerf|Van Groesen|1999|p=344}} This is stated but not proved.</ref>\n: '''''Theorem:''''' ''If'' {{mvar|U}} ''and'' {{mvar|V}} ''are two additive transformations of'' {{mvar|H}} ''onto'' {{mvar|K}}, ''both compatible with the ray transformation'' {{mvar|T}} ''with'' {{math|dim ''H'' ≥ 2}}, ''then''\n:: <math>V = Ue^{i\\alpha}, \\alpha \\in \\mathbb{R}.</math>\n\nThe significance of this theorem is that it specifies the degree of uniqueness of the representation on {{mvar|H}}. On the face of it, one might believe that\n: <math>Vh = Ue^{i\\alpha(h)}h, \\alpha \\in \\mathbb{R}, h \\in \\mathcal{H} \\quad (\\text{wrong unless }  \\alpha(h) \\text{ is const.})</math>\n\nwould be admissible, with {{math|α(''h'') ≠ α(''k'')}} for {{math|⟨h{{!}}k⟩ {{=}} 0}}, but this is not the case according to the theorem.<ref group=nb>There is an exception to this. If a superselection rule is in effect, then the phase ''may'' depend on in which sector of {{mvar|H}} {{mvar|h}} resides, see {{harvnb|Weinberg|2002|p=53}}</ref> If {{mvar|G}} is a symmetry group (in this latter sense of being embedded as a subgroup of the symmetry group of the system acting on ray space), and if {{math|''f'', ''g'', ''h'' ∈ ''G''}} with {{math|''fg'' {{=}} ''h''}}, then\n:<math>T(f)T(g) = T(h),</math>\n\nwhere the {{mvar|T}} are ray transformations. From the last theorem, one has for the compatible representatives {{mvar|U}},\n:<math>U(f)U(g) = \\omega(f, g)U(fg) = e^{i\\xi(f, g)}U(fg),</math>\n\nwhere {{math|''ω''(''f'', ''g'')}} is a phase factor.<ref group=nb>Again there is an exception. If a superselection rule is in effect, then the phase ''may'' depend on in which sector of {{mvar|H}} {{mvar|h}} resides on which the operators act, see {{harvnb|Weinberg|2002|p=53}}</ref>\n\nThe function {{mvar|ω}} is called a '''{{math|2}}-cocycle''' or [[Schur multiplier]]. A map {{math|''U'':''G'' → GL(''V'')}} satisfying the above relation for some vector space {{mvar|V}} is called a [[projective representation]] or a '''ray representation'''. If {{math|''ω''(''f'', ''g'') {{=}} 1}}, then it is called a [[representation (group theory)|representation]].\n\nOne should note that the terminology differs between mathematics and physics. In the linked article, term ''projective representation'' has a slightly different meaning, but the term as presented here enters as an ingredient and the mathematics per se is of course the same. If the realization of the symmetry group, {{math|''g'' → ''T''(''g'')}}, is given in terms of action on the space of unit rays {{math|''S'' {{=}} ''PH''}}, then it is a projective representation {{math|''G'' → PGL(''H'')}} in the mathematical sense, while its representative on Hilbert space is a projective representation {{math|''G'' → GL(''H'')}} in the physical sense.\n\nApplying the last relation (several times) to the product {{mvar|fgh}} and appealing to the known associativity of multiplication of operators on {{mvar|H}}, one finds\n:<math>\\begin{align}\n  \\omega(f, g)\\omega(fg, h) &= \\omega(g, h)\\omega(f, gh), \\\\\n     \\xi(f, g) + \\xi(fg, h) &= \\xi(g, h) + \\xi(f, gh) \\quad (\\operatorname{mod} 2\\pi).\n\\end{align}</math>\n\nThy also satisfy\n:<math>\\begin{align}\n                  \\omega(g, e) &= \\omega(e, g) = 1, \\\\\n                     \\xi(g, e) &= \\xi(e, g) = 0 \\quad (\\operatorname{mod} 2\\pi), \\\\\n  \\omega\\left(g, g^{-1}\\right) &= \\omega(g^{-1}, g), \\\\\n     \\xi\\left(g, g^{-1}\\right) &= \\xi(g^{-1}, g) = 0 \\quad (\\operatorname{mod} 2\\pi). \\\\\n\\end{align}</math>\n\nUpon redefinition of the phases,\n:<math>U(g) \\mapsto \\hat{U}(g) = \\eta(g)U(g) = e^{i\\zeta(g)}U(g),</math>\n\nwhich is allowed by last theorem, one finds<ref>{{harvnb|de Kerf|Van Groesen|1999|p=346}} There is an error in this formula in the book.</ref><ref name=Weinberg_2002_p82>{{harvnb|Weinberg|2002|p=82}}</ref>\n:<math>\\begin{align}\n  \\hat{\\omega}(g, h) &= \\omega(g, h)\\eta(g)\\eta(h)\\eta(gh)^{-1},\\\\\n     \\hat{\\xi}(g, h) &= \\xi(g, h) + \\zeta(g) + \\zeta(h) - \\zeta(gh) \\quad (\\operatorname{mod} 2\\pi),\\end{align}</math>\n\nwhere the hatted quantities are defined by\n:<math>\\hat{U}(f)\\hat{U}(g) = \\hat{\\omega}(f, g)\\hat{U}(fg) = e^{i\\hat{\\xi}(f,g)}\\hat{U}(fg).</math>\n\n===Utility of phase freedom===\nThe following rather technical theorems and many more can be found, with accessible proofs, in {{harvtxt|Bargmann|1954}}.\n\nThe freedom of choice of phases can be used to simplify the phase factors. For some groups the phase can be eliminated altogether.\n* ''Theorem: If'' {{math|G}} ''is semisimple and simply connected, then'' {{math|''ω''(''g'', ''h'') {{=}} 1}} ''is possible''.<ref>{{harvnb|Weinberg|2002|loc=Appendix B, Chapter 2}}</ref>\nIn the case of the [[Lorentz group]] and its subgroup the [[rotation group SO(3)]], phases can, for projective representations, be chosen such that {{math|''ω''(''g'', ''h'') {{=}} ± 1}}. For their respective [[universal covering group]]s, [[SL(2,C)]] and [[Spin(3)]], it is according to the theorem possible to have {{math|''ω''(''g'', ''h'') {{=}} 1}}, i.e. they are proper representations.\n\nThe study of redefinition of phases involves [[group cohomology]]. Two functions related as the hatted and non-hatted versions of {{math|''ω''}} above are said to be '''cohomologous'''. They belong to the same '''second cohomology class''', i.e. they are represented by the same element in {{math|''H''<sup>2</sup>(''G'')}}, the '''second cohomology group''' of {{math|G}}. If an element of {{math|''H''<sup>2</sup>(''G'')}} contains the trivial function {{math|''ω'' {{=}} 0}}, then it is said to be '''trivial'''.<ref name=Weinberg_2002_p82/> The topic can be studied at the level of [[Lie algebra]]s and [[Lie algebra cohomology]] as well.<ref>{{harvnb|Bäurle|de Kerf|1999|pp=347–349}}</ref><ref>{{harvnb|Weinberg|2002|loc=Section 2.7.}}</ref>\n\nAssuming the projective representation {{math|''g'' → ''T''(''g'')}} is weakly continuous, two relevant theorems can be stated. An immediate consequence of (weak) continuity is that the identity component is represented by unitary operators.<ref group=nb>This is made plausible as follows. In an open neighborhood in the vicinity of the identity all operators can be expressed as squares. Whether an operator is unitary or antiunitary its square is unitary. Hence they are all unitary in a sufficiently small neighborhood. Such a neighborhood generates the identity.</ref>\n\n* ''Theorem: (Wigner 1939). The phase freedom can be used such that in a some neighborhood of the identity the map'' {{math|''g'' → ''U''(''g'')}} ''is strongly continuous.''<ref name=Straumann_2014>{{harvnb|Straumann|2014}}</ref>\n* ''Theorem (Bargmann). In a sufficiently small neighborhood of e, the choice'' {{math|''ω''(''g''<sub>1</sub>, ''g''<sub>2</sub>) ≡ 1}} ''is possible for semisimple Lie groups (such as'' {{math|SO(''n'')}}, SO(3,1) ''and affine linear groups, (in particular the Poincaré group). More precisely, this is exactly the case when the second cohomology group'' {{math|''H''<sup>2</sup>({{math|'''g'''}}, ℝ)}} ''of the Lie algebra'' {{math|'''g'''}} ''of'' {{mvar|G}} ''is trivial''.<ref name=Straumann_2014/>\n\n==See also==\n* [[Particle physics and representation theory]]\n\n==Remarks==\n{{Reflist|group=nb}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{cite journal|ref=harv|first=V.|last=Bargmann|title=On unitary ray representations of continuous groups|journal=Ann. of Math.|volume=59|issue=1|year=1954|pages=1&ndash;46|doi=10.2307/1969831|jstor=1969831}}\n*{{cite journal|ref=harv|authorlink=Valentine Bargmann|last=Bargmann|first=V.|title=Note on Wigner's Theorem on Symmetry Operations|journal=Journal of Mathematical Physics|volume=5|issue=7|year=1964|doi=10.1063/1.1704188|pages=862–868|bibcode = 1964JMP.....5..862B }}\n*{{cite book|ref=harv|last1=Bogoliubov|first1=N. N.|authorlink1=Nikolay Bogoliubov|last2=Logunov|first2=A.A.|last3=Todorov|first3=I. T.|title=Introduction to axiomatic quantum field theory|publisher=Benjamin|location=New York|year=1975|others=Translated to English by  Stephan A. Fulling and Ludmila G. Popova|asin=B000IM4HLS|series=Mathematical Physics Monograph Series|volume=18}}\n*{{cite book|ref=harv|editor1=E.A. Van Groesen|editor2=E. M. De Jager|last1=Bäurle|first1=C. G. A.|last2=de Kerf|first2=E.A.|title=Lie algebras Part 1:Finite and infinite dimensional Lie algebras and their applications in physics|series=Studies in mathematical physics|volume=1|publisher=North-Holland|location=Amsterdam|year=1999|isbn=0 444 88776 8|edition=2nd}}\n*{{cite journal|journal=Biogr. Mem. Fellows R. Soc.|year=2000|first1=F.|last1=Seitz|first2=E.|last2=Vogt|first3=A. M.|last3=Weinberg|title=Eugene Paul Wigner. 17 November 1902 -- 1 January 1995|volume=46|pages=577–592|doi=10.1098/rsbm.1999.0102}}\n*{{cite journal|ref=harv|last1=Simon|first1=R.|last2=Mukunda|first2=N.|authorlink2=N. Mukunda|last3=Chaturvedi|first3=S.|last4=Srinivasan|first4=V.|first5=J.|last5=Hamhalter|year=2008|title=Two elementary proofs of the Wigner theorem on symmetry in quantum mechanics|journal=Phys. Lett. A|volume=372|issue=46|pages=6847–6852|doi=10.1016/j.physleta.2008.09.052 |arxiv = 0808.0779 |bibcode = 2008PhLA..372.6847S }}\n*{{Cite book |ref=harv|last=Straumann|first=N.|title=Springer Handbook of Spacetime|journal=<!-- -->|pages=265–278 |chapter=Unitary Representations of the inhomogeneous Lorentz Group and their Significance in Quantum Physics|year=2014|isbn=978-3-642-41991-1|editor1=A. Ashtekar|editor2=V. Petkov|doi=10.1007/978-3-642-41992-8_14|arxiv=0809.4942|bibcode=2014shst.book..265S|citeseerx=10.1.1.312.401}}\n*{{citation|last=Weinberg|first=S.|year=2002|title=The Quantum Theory of Fields|volume=I|isbn=978-0-521-55001-7|authorlink=Steven Weinberg|publisher=[[Cambridge University Press]]}}\n*{{cite book|ref=harv|first=E. P.|last=Wigner|authorlink=Eugene Wigner|title=Gruppentheorie und ihre Anwendung auf die Quanten mechanik der Atomspektren|publisher=Friedrich Vieweg und Sohn|location=Braunschweig, Germany|year=1931|pages=251–254|asin=B000K1MPEI|language=German}}\n*{{cite book|ref=harv|first=E. P.|last=Wigner|title=Group Theory and its Application to the Quantum Mechanics of Atomic Spectra|publisher=Academic Press|location=New York|year=1959|pages=233–236|others=translation from German by J. J. Griffin|isbn=978-0-1275-0550-3}}\n\n==Further reading==\n*{{cite journal|last=Mouchet|first=Amaury|title=An alternative proof of Wigner theorem on quantum transformations based on elementary complex analysis|journal=Physics Letters A|volume=377|issue=39|year=2013|pages=2709–2711|doi=10.1016/j.physleta.2013.08.017|arxiv = 1304.1376 |bibcode = 2013PhLA..377.2709M }}\n*{{cite journal|last=Molnar|first=Lajos|journal=J. Austral. Math. Soc. (Series A)|volume=65|issue=3|year=1999|pages=354–369|title=An Algebraic Approach to Wigner's Unitary-Antiunitary Theorem|url=http://www.austms.org.au/Publ/Jamsa/V65P3/pdf/p93.pdf|format=pdf|arxiv=math/9808033|bibcode = 1998math......8033M|doi=10.1017/s144678870003593x}}\n\n[[Category:Quantum mechanics]]\n[[Category:Hilbert space]]\n[[Category:Theorems in quantum physics]]"
    }
  ]
}